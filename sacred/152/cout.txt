INFO - Siam-FC - Running command 'main'
INFO - Siam-FC - Started run with ID "152"
INFO - root - Creating training directory: /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-momentum-lr0.001-defgroup-1
INFO - root - preproces -- siamese_fc_color
WARNING - root - root is not explicitly specified, using default value: None
INFO - root - For training, we use instance size 255 - 2 * 8 ...
WARNING - root - init_method is not explicitly specified, using default value: None
INFO - root - Model moving average is disabled since decay factor is 0
sdiufhasudf Tensor("siamese_fc/conv5/split:0", shape=(8, 8, 8, 192), dtype=float32)
Tensor("siamese_fc/conv5/def/transpose:0", shape=(8, 192, 8, 8), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/b1/weights:0' shape=(128, 192, 3, 3) dtype=float32_ref> Tensor("siamese_fc/conv5/def/transpose_1:0", shape=(8, 18, 8, 8), dtype=float32)
ddd Tensor("siamese_fc/conv5/def/b1/transpose:0", shape=(8, 6, 6, 128), dtype=float32)
Tensor("siamese_fc/conv5/def/transpose_2:0", shape=(8, 192, 8, 8), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/b2/weights:0' shape=(128, 192, 3, 3) dtype=float32_ref> Tensor("siamese_fc/conv5/def/transpose_3:0", shape=(8, 18, 8, 8), dtype=float32)
sdiufhasudf Tensor("siamese_fc_1/conv5/split:0", shape=(8, 22, 22, 192), dtype=float32)
Tensor("siamese_fc_1/conv5/def/transpose:0", shape=(8, 192, 22, 22), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/b1/weights:0' shape=(128, 192, 3, 3) dtype=float32_ref> Tensor("siamese_fc_1/conv5/def/transpose_1:0", shape=(8, 18, 22, 22), dtype=float32)
ddd Tensor("siamese_fc_1/conv5/def/b1/transpose:0", shape=(8, 20, 20, 128), dtype=float32)
Tensor("siamese_fc_1/conv5/def/transpose_2:0", shape=(8, 192, 22, 22), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/b2/weights:0' shape=(128, 192, 3, 3) dtype=float32_ref> Tensor("siamese_fc_1/conv5/def/transpose_3:0", shape=(8, 18, 22, 22), dtype=float32)
Tensor("detection/add:0", shape=(8, 15, 15), dtype=float32)
2017-12-08 04:52:22.961234: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-08 04:52:22.961273: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-08 04:52:22.961280: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2017-12-08 04:52:22.961284: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-08 04:52:22.961288: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
2017-12-08 04:52:23.374637: I tensorflow/core/common_runtime/gpu/gpu_device.cc:887] Found device 0 with properties: 
name: Tesla K80
major: 3 minor: 7 memoryClockRate (GHz) 0.8235
pciBusID 6bee:00:00.0
Total memory: 11.17GiB
Free memory: 11.10GiB
2017-12-08 04:52:23.374675: I tensorflow/core/common_runtime/gpu/gpu_device.cc:908] DMA: 0 
2017-12-08 04:52:23.374681: I tensorflow/core/common_runtime/gpu/gpu_device.cc:918] 0:   Y 
2017-12-08 04:52:23.374689: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla K80, pci bus id: 6bee:00:00.0)
INFO:tensorflow:Restoring parameters from /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-only/model.ckpt-150000
INFO - tensorflow - Restoring parameters from /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-only/model.ckpt-150000
INFO - root - Starting threads 0 ...

INFO - root - training for 332500 steps
INFO - root - 2017-12-08 04:52:26.288660: step 0, loss = 2.28, batch loss = 2.23 (3.6 examples/sec; 2.199 sec/batch; 203h:05m:28s remains)
2017-12-08 04:52:26.663274: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289622 -4.4289603 -4.4289622 -4.4289651 -4.4289589 -4.4289403 -4.4289088 -4.4288678 -4.4288235 -4.4288063 -4.4288173 -4.4288445 -4.4288826 -4.428925 -4.4289556][-4.4289603 -4.428957 -4.4289594 -4.4289579 -4.4289412 -4.4289041 -4.4288483 -4.4287829 -4.4287257 -4.4287162 -4.4287453 -4.4287891 -4.4288416 -4.428896 -4.4289346][-4.42895 -4.428947 -4.42895 -4.42894 -4.4288988 -4.42883 -4.4287376 -4.428638 -4.4285665 -4.4285684 -4.4286256 -4.428699 -4.4287796 -4.4288545 -4.4289079][-4.4289279 -4.4289284 -4.4289346 -4.4289131 -4.4288387 -4.4287305 -4.4285927 -4.4284487 -4.4283571 -4.4283781 -4.4284749 -4.428586 -4.4287076 -4.4288092 -4.4288821][-4.4288936 -4.4289007 -4.4289103 -4.4288797 -4.4287777 -4.4286346 -4.4284515 -4.4282517 -4.4281325 -4.4281912 -4.4283342 -4.4284854 -4.4286456 -4.4287724 -4.428864][-4.4288559 -4.42887 -4.4288778 -4.4288359 -4.4287076 -4.4285269 -4.428288 -4.4280233 -4.4278841 -4.4280038 -4.4282012 -4.4283957 -4.4285922 -4.4287429 -4.4288535][-4.4288225 -4.4288354 -4.428834 -4.4287782 -4.4286361 -4.4284258 -4.4281387 -4.4278126 -4.4276824 -4.4278922 -4.4281454 -4.4283714 -4.4285808 -4.4287386 -4.428854][-4.4287996 -4.4288006 -4.4287844 -4.42872 -4.4285827 -4.4283729 -4.4280825 -4.4277611 -4.4277034 -4.4279752 -4.4282346 -4.428443 -4.4286237 -4.4287653 -4.4288707][-4.42878 -4.4287653 -4.4287419 -4.4286857 -4.428586 -4.4284291 -4.4282155 -4.4280014 -4.4280152 -4.42824 -4.4284258 -4.4285645 -4.4286919 -4.4288039 -4.4288921][-4.4287643 -4.4287391 -4.4287157 -4.4286733 -4.4286146 -4.4285183 -4.4283962 -4.4282804 -4.4283171 -4.4284654 -4.4285784 -4.4286542 -4.42874 -4.4288306 -4.4289083][-4.4287496 -4.4287224 -4.4287009 -4.428669 -4.4286385 -4.4285927 -4.4285336 -4.4284716 -4.4285016 -4.4285922 -4.4286571 -4.4286952 -4.42876 -4.4288445 -4.4289193][-4.4287462 -4.4287219 -4.4287024 -4.42868 -4.4286647 -4.42865 -4.4286284 -4.4285979 -4.4286165 -4.4286647 -4.4287009 -4.4287219 -4.4287806 -4.4288626 -4.4289317][-4.4287624 -4.4287515 -4.4287419 -4.4287314 -4.4287276 -4.4287286 -4.4287229 -4.4287009 -4.4287 -4.4287071 -4.4287205 -4.4287333 -4.4287934 -4.4288769 -4.4289408][-4.4287977 -4.4288034 -4.4288092 -4.4288096 -4.4288092 -4.4288116 -4.4287987 -4.4287658 -4.4287438 -4.4287248 -4.4287219 -4.4287314 -4.4287939 -4.4288797 -4.4289422][-4.4288068 -4.428822 -4.4288416 -4.4288516 -4.4288526 -4.4288526 -4.4288273 -4.4287777 -4.4287362 -4.4287076 -4.4287028 -4.4287195 -4.4287877 -4.4288769 -4.4289389]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-momentum-lr0.001-defgroup-1/model.ckpt-0 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-momentum-lr0.001-defgroup-1/model.ckpt-0 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-08 04:52:29.319407: step 10, loss = 2.28, batch loss = 2.23 (36.7 examples/sec; 0.218 sec/batch; 20h:07m:55s remains)
INFO - root - 2017-12-08 04:52:31.485041: step 20, loss = 2.28, batch loss = 2.23 (36.6 examples/sec; 0.219 sec/batch; 20h:12m:08s remains)
[<tf.Variable 'siamese_fc/conv1/weights:0' shape=(11, 11, 3, 96) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv1/BatchNorm/beta:0' shape=(96,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv1/BatchNorm/gamma:0' shape=(96,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv1/BatchNorm/moving_mean:0' shape=(96,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv1/BatchNorm/moving_variance:0' shape=(96,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv2/b1/weights:0' shape=(5, 5, 48, 128) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv2/b1/BatchNorm/beta:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv2/b1/BatchNorm/gamma:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv2/b1/BatchNorm/moving_mean:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv2/b1/BatchNorm/moving_variance:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv2/b2/weights:0' shape=(5, 5, 48, 128) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv2/b2/BatchNorm/beta:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv2/b2/BatchNorm/gamma:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv2/b2/BatchNorm/moving_mean:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv2/b2/BatchNorm/moving_variance:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv3/weights:0' shape=(3, 3, 256, 384) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv3/BatchNorm/beta:0' shape=(384,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv3/BatchNorm/gamma:0' shape=(384,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv3/BatchNorm/moving_mean:0' shape=(384,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv3/BatchNorm/moving_variance:0' shape=(384,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b1/weights:0' shape=(3, 3, 192, 192) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b1/BatchNorm/beta:0' shape=(192,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b1/BatchNorm/gamma:0' shape=(192,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b1/BatchNorm/moving_mean:0' shape=(192,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b1/BatchNorm/moving_variance:0' shape=(192,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b2/weights:0' shape=(3, 3, 192, 192) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b2/BatchNorm/beta:0' shape=(192,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b2/BatchNorm/gamma:0' shape=(192,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b2/BatchNorm/moving_mean:0' shape=(192,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv4/b2/BatchNorm/moving_variance:0' shape=(192,) dtype=float32_ref>, <tf.Variable 'detection/biases:0' shape=(1,) dtype=float32_ref>, <tf.Variable 'global_step:0' shape=() dtype=int32_ref>]
INFO - root - 2017-12-08 04:52:33.625958: step 30, loss = 2.28, batch loss = 2.23 (38.5 examples/sec; 0.208 sec/batch; 19h:10m:23s remains)
INFO - root - 2017-12-08 04:52:35.763775: step 40, loss = 2.28, batch loss = 2.23 (38.7 examples/sec; 0.207 sec/batch; 19h:05m:58s remains)
INFO - root - 2017-12-08 04:52:37.923356: step 50, loss = 2.28, batch loss = 2.23 (37.2 examples/sec; 0.215 sec/batch; 19h:51m:06s remains)
INFO - root - 2017-12-08 04:52:40.097017: step 60, loss = 2.28, batch loss = 2.23 (38.0 examples/sec; 0.211 sec/batch; 19h:27m:53s remains)
INFO - root - 2017-12-08 04:52:42.238060: step 70, loss = 2.28, batch loss = 2.23 (37.3 examples/sec; 0.214 sec/batch; 19h:47m:43s remains)
INFO - root - 2017-12-08 04:52:44.403057: step 80, loss = 2.28, batch loss = 2.23 (36.8 examples/sec; 0.217 sec/batch; 20h:03m:34s remains)
INFO - root - 2017-12-08 04:52:46.560872: step 90, loss = 2.28, batch loss = 2.23 (38.3 examples/sec; 0.209 sec/batch; 19h:17m:47s remains)
INFO - root - 2017-12-08 04:52:48.703150: step 100, loss = 2.28, batch loss = 2.23 (37.6 examples/sec; 0.212 sec/batch; 19h:37m:12s remains)
2017-12-08 04:52:48.986795: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4287977 -4.4287491 -4.4287038 -4.4286828 -4.4286909 -4.4287176 -4.4287457 -4.4287653 -4.4287829 -4.4287806 -4.4287682 -4.428771 -4.4287596 -4.4287348 -4.4287176][-4.428813 -4.4287486 -4.4286785 -4.4286308 -4.4286208 -4.4286413 -4.4286733 -4.4286923 -4.42871 -4.4287171 -4.4287171 -4.4287305 -4.4287238 -4.4287024 -4.4286947][-4.4288211 -4.4287481 -4.4286652 -4.4286041 -4.4285827 -4.4285979 -4.4286294 -4.4286475 -4.4286675 -4.4286857 -4.4286947 -4.4287071 -4.4287 -4.428678 -4.4286733][-4.4288206 -4.4287453 -4.4286628 -4.4286027 -4.4285793 -4.4285913 -4.4286194 -4.4286356 -4.4286618 -4.4286909 -4.4287047 -4.4287095 -4.4286995 -4.4286757 -4.4286666][-4.4288154 -4.4287405 -4.4286623 -4.4286027 -4.4285736 -4.428575 -4.4285946 -4.428607 -4.428637 -4.4286747 -4.4286947 -4.4286971 -4.428689 -4.4286695 -4.4286618][-4.4288092 -4.4287376 -4.4286642 -4.4286017 -4.4285603 -4.428545 -4.4285445 -4.4285393 -4.4285617 -4.4286079 -4.4286447 -4.4286566 -4.4286551 -4.4286447 -4.4286456][-4.4287972 -4.4287243 -4.428647 -4.4285755 -4.4285235 -4.4284983 -4.4284821 -4.4284563 -4.4284625 -4.4285178 -4.4285741 -4.4286022 -4.4286118 -4.4286132 -4.42862][-4.4287896 -4.4287148 -4.4286327 -4.4285555 -4.4285035 -4.4284811 -4.4284606 -4.428422 -4.4284134 -4.4284697 -4.4285293 -4.4285612 -4.4285779 -4.4285917 -4.4286008][-4.4287934 -4.428721 -4.4286385 -4.4285626 -4.4285207 -4.4285107 -4.4284964 -4.4284596 -4.4284468 -4.4284897 -4.4285307 -4.4285474 -4.4285593 -4.4285784 -4.4285889][-4.4288034 -4.4287343 -4.4286528 -4.42858 -4.428546 -4.4285421 -4.4285316 -4.428504 -4.4285026 -4.4285359 -4.428556 -4.4285522 -4.428555 -4.4285707 -4.4285793][-4.428822 -4.4287553 -4.428678 -4.4286122 -4.4285879 -4.4285884 -4.4285769 -4.428555 -4.4285612 -4.4285932 -4.4286036 -4.4285889 -4.4285836 -4.4285932 -4.4286][-4.4288464 -4.4287829 -4.4287128 -4.4286575 -4.4286442 -4.4286532 -4.4286509 -4.4286389 -4.4286466 -4.428669 -4.4286733 -4.4286566 -4.4286509 -4.4286556 -4.428659][-4.4288754 -4.4288192 -4.4287596 -4.4287148 -4.4287071 -4.4287229 -4.4287348 -4.4287405 -4.4287486 -4.4287586 -4.4287558 -4.4287405 -4.4287338 -4.4287357 -4.4287372][-4.4289145 -4.428874 -4.4288282 -4.428792 -4.4287825 -4.4287958 -4.4288158 -4.428834 -4.4288454 -4.4288492 -4.4288411 -4.4288249 -4.4288163 -4.4288182 -4.4288216][-4.4289546 -4.4289322 -4.428905 -4.4288797 -4.42887 -4.4288774 -4.4288931 -4.42891 -4.4289193 -4.4289212 -4.4289136 -4.4289021 -4.4288969 -4.4289 -4.428905]]...]
INFO - root - 2017-12-08 04:52:51.120134: step 110, loss = 2.28, batch loss = 2.23 (37.3 examples/sec; 0.214 sec/batch; 19h:48m:09s remains)
INFO - root - 2017-12-08 04:52:53.287132: step 120, loss = 2.28, batch loss = 2.23 (35.0 examples/sec; 0.229 sec/batch; 21h:06m:10s remains)
INFO - root - 2017-12-08 04:52:55.433085: step 130, loss = 2.28, batch loss = 2.23 (36.8 examples/sec; 0.217 sec/batch; 20h:03m:44s remains)
INFO - root - 2017-12-08 04:52:57.575989: step 140, loss = 2.28, batch loss = 2.23 (37.6 examples/sec; 0.213 sec/batch; 19h:38m:32s remains)
INFO - root - 2017-12-08 04:52:59.746166: step 150, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 20h:35m:11s remains)
INFO - root - 2017-12-08 04:53:01.911156: step 160, loss = 2.28, batch loss = 2.23 (35.8 examples/sec; 0.223 sec/batch; 20h:36m:40s remains)
INFO - root - 2017-12-08 04:53:04.106981: step 170, loss = 2.28, batch loss = 2.23 (37.4 examples/sec; 0.214 sec/batch; 19h:43m:25s remains)
INFO - root - 2017-12-08 04:53:06.304879: step 180, loss = 2.28, batch loss = 2.23 (36.0 examples/sec; 0.222 sec/batch; 20h:31m:22s remains)
INFO - root - 2017-12-08 04:53:08.504818: step 190, loss = 2.28, batch loss = 2.23 (36.8 examples/sec; 0.217 sec/batch; 20h:04m:01s remains)
INFO - root - 2017-12-08 04:53:10.697070: step 200, loss = 2.28, batch loss = 2.23 (37.0 examples/sec; 0.216 sec/batch; 19h:56m:34s remains)
2017-12-08 04:53:11.014908: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288893 -4.4288459 -4.4288244 -4.4288311 -4.4288278 -4.4288225 -4.4288197 -4.4288092 -4.4288006 -4.4288054 -4.4288197 -4.4288268 -4.4288173 -4.4288197 -4.4288306][-4.4288945 -4.4288564 -4.4288335 -4.428833 -4.4288249 -4.4288154 -4.4288096 -4.4287992 -4.4287872 -4.4287915 -4.4288039 -4.4288049 -4.4287977 -4.4288039 -4.4288139][-4.4288535 -4.4288363 -4.4288287 -4.4288192 -4.4288044 -4.42879 -4.4287796 -4.4287758 -4.4287763 -4.4287853 -4.4287868 -4.4287715 -4.4287591 -4.428761 -4.4287629][-4.4287963 -4.4288054 -4.4288106 -4.4287944 -4.4287806 -4.4287581 -4.4287376 -4.4287434 -4.428771 -4.4287939 -4.428782 -4.4287453 -4.428719 -4.4287086 -4.4286962][-4.4287448 -4.4287648 -4.4287829 -4.4287696 -4.4287543 -4.4287095 -4.4286637 -4.4286785 -4.4287391 -4.4287834 -4.4287753 -4.4287314 -4.4286895 -4.428659 -4.4286337][-4.4287038 -4.4287157 -4.4287381 -4.4287314 -4.42871 -4.4286304 -4.4285364 -4.4285522 -4.4286532 -4.4287314 -4.4287415 -4.428709 -4.4286642 -4.4286284 -4.4286065][-4.4286904 -4.4286804 -4.4286962 -4.4286914 -4.4286489 -4.4284997 -4.428318 -4.4283319 -4.4285064 -4.4286437 -4.428689 -4.4286737 -4.4286313 -4.4286022 -4.4286113][-4.4286747 -4.4286466 -4.4286757 -4.428689 -4.4286332 -4.4284177 -4.4281406 -4.4281373 -4.4283876 -4.4285917 -4.428668 -4.4286585 -4.4286027 -4.4285693 -4.4286094][-4.4286618 -4.4286423 -4.4286976 -4.4287357 -4.4286962 -4.4285011 -4.428257 -4.4282393 -4.4284511 -4.4286451 -4.4287243 -4.4287224 -4.4286666 -4.4286308 -4.4286671][-4.4286942 -4.4286857 -4.4287472 -4.4287858 -4.4287591 -4.4286346 -4.4284844 -4.4284692 -4.4285994 -4.4287367 -4.4288015 -4.4288039 -4.4287615 -4.4287262 -4.4287462][-4.4287686 -4.4287581 -4.4288025 -4.4288297 -4.4288144 -4.4287333 -4.4286366 -4.4286184 -4.4286895 -4.4287877 -4.4288492 -4.4288583 -4.4288254 -4.4287891 -4.4287906][-4.4288182 -4.4287968 -4.4288268 -4.4288425 -4.4288359 -4.4287758 -4.4287081 -4.4286933 -4.4287314 -4.428803 -4.4288568 -4.4288683 -4.4288483 -4.4288135 -4.4288058][-4.4288344 -4.4288168 -4.4288406 -4.4288478 -4.4288487 -4.4288154 -4.42877 -4.4287586 -4.4287748 -4.4288206 -4.428865 -4.428875 -4.428864 -4.4288383 -4.4288263][-4.4288468 -4.4288363 -4.4288564 -4.4288549 -4.4288578 -4.42884 -4.4288092 -4.428802 -4.4288106 -4.4288383 -4.4288735 -4.42888 -4.4288692 -4.4288568 -4.4288511][-4.4288521 -4.4288507 -4.4288621 -4.4288621 -4.4288678 -4.4288588 -4.4288354 -4.4288278 -4.428833 -4.4288507 -4.4288769 -4.4288831 -4.42887 -4.4288583 -4.4288492]]...]
INFO - root - 2017-12-08 04:53:13.202946: step 210, loss = 2.28, batch loss = 2.23 (37.2 examples/sec; 0.215 sec/batch; 19h:50m:08s remains)
INFO - root - 2017-12-08 04:53:15.378845: step 220, loss = 2.28, batch loss = 2.23 (35.7 examples/sec; 0.224 sec/batch; 20h:42m:02s remains)
INFO - root - 2017-12-08 04:53:17.573527: step 230, loss = 2.28, batch loss = 2.23 (36.8 examples/sec; 0.218 sec/batch; 20h:05m:07s remains)
INFO - root - 2017-12-08 04:53:19.727153: step 240, loss = 2.28, batch loss = 2.23 (36.9 examples/sec; 0.217 sec/batch; 20h:01m:39s remains)
INFO - root - 2017-12-08 04:53:21.877326: step 250, loss = 2.28, batch loss = 2.23 (37.3 examples/sec; 0.215 sec/batch; 19h:49m:04s remains)
INFO - root - 2017-12-08 04:53:24.078240: step 260, loss = 2.28, batch loss = 2.23 (35.8 examples/sec; 0.224 sec/batch; 20h:37m:51s remains)
INFO - root - 2017-12-08 04:53:26.227254: step 270, loss = 2.28, batch loss = 2.23 (37.0 examples/sec; 0.216 sec/batch; 19h:57m:10s remains)
INFO - root - 2017-12-08 04:53:28.396501: step 280, loss = 2.28, batch loss = 2.23 (37.5 examples/sec; 0.213 sec/batch; 19h:41m:44s remains)
INFO - root - 2017-12-08 04:53:30.561493: step 290, loss = 2.28, batch loss = 2.23 (37.2 examples/sec; 0.215 sec/batch; 19h:52m:01s remains)
INFO - root - 2017-12-08 04:53:32.756082: step 300, loss = 2.28, batch loss = 2.23 (35.3 examples/sec; 0.227 sec/batch; 20h:54m:55s remains)
2017-12-08 04:53:33.037692: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4286618 -4.4286289 -4.4285984 -4.4286027 -4.4286618 -4.4287472 -4.4288049 -4.4288011 -4.4287663 -4.4287267 -4.4287062 -4.4287128 -4.4287081 -4.4286766 -4.4286494][-4.4287825 -4.4287419 -4.4286757 -4.428628 -4.4286456 -4.4287181 -4.4287839 -4.4287977 -4.4287877 -4.4287791 -4.4287753 -4.4287758 -4.4287553 -4.4286981 -4.428638][-4.4288378 -4.4287972 -4.4287148 -4.4286327 -4.4286137 -4.4286675 -4.4287295 -4.4287553 -4.4287677 -4.4287839 -4.4287887 -4.4287791 -4.4287438 -4.4286733 -4.4285965][-4.4288244 -4.4287891 -4.4287066 -4.4286122 -4.4285684 -4.4285965 -4.4286447 -4.4286766 -4.4287071 -4.4287415 -4.4287548 -4.4287343 -4.4286842 -4.4286084 -4.428535][-4.428772 -4.4287391 -4.4286671 -4.428587 -4.4285393 -4.4285417 -4.4285684 -4.4285994 -4.4286351 -4.4286709 -4.4286838 -4.4286528 -4.42859 -4.4285116 -4.4284525][-4.4287329 -4.4287038 -4.4286427 -4.428586 -4.4285488 -4.4285345 -4.4285455 -4.4285746 -4.4286079 -4.428627 -4.4286227 -4.4285784 -4.428503 -4.4284153 -4.4283671][-4.4287167 -4.4286909 -4.42864 -4.4286036 -4.4285817 -4.4285622 -4.4285569 -4.4285817 -4.4286156 -4.4286194 -4.4285979 -4.4285426 -4.4284582 -4.4283566 -4.4283118][-4.4287162 -4.4286995 -4.4286685 -4.4286537 -4.42864 -4.4286194 -4.4286008 -4.42861 -4.4286361 -4.4286304 -4.4286032 -4.4285522 -4.4284692 -4.4283695 -4.4283357][-4.4287305 -4.4287333 -4.4287319 -4.4287376 -4.4287314 -4.4287105 -4.4286823 -4.4286809 -4.4286938 -4.4286842 -4.4286613 -4.4286222 -4.4285531 -4.4284663 -4.428443][-4.4287262 -4.4287577 -4.4287777 -4.4287958 -4.4287968 -4.4287763 -4.4287429 -4.4287295 -4.4287238 -4.4287119 -4.4287019 -4.4286895 -4.4286456 -4.4285865 -4.4285755][-4.4287004 -4.4287624 -4.4287992 -4.428823 -4.428833 -4.42882 -4.4287891 -4.4287648 -4.428741 -4.4287181 -4.4287109 -4.4287257 -4.4287138 -4.4286852 -4.4286876][-4.428659 -4.4287367 -4.4287806 -4.4288116 -4.4288349 -4.428834 -4.42881 -4.4287848 -4.42875 -4.4287128 -4.4287033 -4.4287367 -4.4287481 -4.4287391 -4.4287457][-4.4286351 -4.4287028 -4.4287438 -4.4287763 -4.4288025 -4.428812 -4.4288034 -4.4287839 -4.4287424 -4.4286976 -4.4286866 -4.4287267 -4.4287524 -4.4287548 -4.4287667][-4.4286509 -4.4286871 -4.4287119 -4.4287391 -4.4287686 -4.4287882 -4.4287944 -4.4287863 -4.4287491 -4.4287004 -4.4286842 -4.4287186 -4.4287434 -4.4287472 -4.4287553][-4.428689 -4.4286928 -4.4286985 -4.4287124 -4.4287357 -4.4287567 -4.4287724 -4.4287763 -4.4287477 -4.4287071 -4.4286933 -4.4287181 -4.4287329 -4.4287267 -4.4287229]]...]
INFO - root - 2017-12-08 04:53:35.196502: step 310, loss = 2.28, batch loss = 2.23 (38.0 examples/sec; 0.211 sec/batch; 19h:26m:17s remains)
INFO - root - 2017-12-08 04:53:37.386279: step 320, loss = 2.28, batch loss = 2.23 (36.7 examples/sec; 0.218 sec/batch; 20h:07m:40s remains)
INFO - root - 2017-12-08 04:53:39.624434: step 330, loss = 2.28, batch loss = 2.23 (36.8 examples/sec; 0.217 sec/batch; 20h:03m:22s remains)
INFO - root - 2017-12-08 04:53:41.806557: step 340, loss = 2.28, batch loss = 2.23 (37.0 examples/sec; 0.216 sec/batch; 19h:58m:03s remains)
INFO - root - 2017-12-08 04:53:43.999867: step 350, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.222 sec/batch; 20h:28m:09s remains)
INFO - root - 2017-12-08 04:53:46.228701: step 360, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 20h:13m:26s remains)
INFO - root - 2017-12-08 04:53:48.437884: step 370, loss = 2.28, batch loss = 2.23 (37.2 examples/sec; 0.215 sec/batch; 19h:50m:53s remains)
INFO - root - 2017-12-08 04:53:50.627527: step 380, loss = 2.28, batch loss = 2.23 (37.2 examples/sec; 0.215 sec/batch; 19h:51m:33s remains)
INFO - root - 2017-12-08 04:53:52.808864: step 390, loss = 2.28, batch loss = 2.23 (36.6 examples/sec; 0.218 sec/batch; 20h:08m:40s remains)
INFO - root - 2017-12-08 04:53:54.991821: step 400, loss = 2.28, batch loss = 2.23 (37.0 examples/sec; 0.216 sec/batch; 19h:56m:23s remains)
2017-12-08 04:53:55.306732: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288392 -4.4288373 -4.4288459 -4.4288721 -4.4289017 -4.428905 -4.4288898 -4.4288645 -4.4288545 -4.4288397 -4.4288216 -4.4288015 -4.4288177 -4.4288449 -4.4288697][-4.4288392 -4.4288383 -4.4288425 -4.4288712 -4.4289036 -4.4289041 -4.4288831 -4.4288478 -4.4288287 -4.4288106 -4.4287834 -4.4287477 -4.4287543 -4.428772 -4.4288025][-4.4288378 -4.4288292 -4.4288263 -4.4288521 -4.4288826 -4.4288764 -4.42885 -4.428812 -4.428792 -4.4287777 -4.4287534 -4.4287109 -4.428709 -4.4287114 -4.42874][-4.4287934 -4.4287658 -4.4287543 -4.4287777 -4.4288044 -4.4287934 -4.4287682 -4.4287405 -4.4287333 -4.4287319 -4.4287252 -4.4286909 -4.4286847 -4.4286766 -4.4287004][-4.4287481 -4.4286928 -4.4286623 -4.4286733 -4.4286909 -4.4286728 -4.4286509 -4.4286413 -4.4286523 -4.4286671 -4.4286804 -4.4286642 -4.4286633 -4.4286571 -4.4286804][-4.4287481 -4.4286747 -4.4286203 -4.4285989 -4.4285874 -4.4285464 -4.4285188 -4.4285245 -4.4285593 -4.4285927 -4.428628 -4.4286332 -4.4286461 -4.4286485 -4.4286742][-4.4287882 -4.4287276 -4.4286728 -4.4286404 -4.428616 -4.4285469 -4.4284763 -4.4284539 -4.428483 -4.4285226 -4.4285674 -4.4285917 -4.4286227 -4.4286432 -4.4286771][-4.4287853 -4.4287443 -4.4287095 -4.4287057 -4.4287157 -4.4286594 -4.428566 -4.4285016 -4.4284897 -4.4284968 -4.4285226 -4.4285502 -4.4285927 -4.4286313 -4.4286752][-4.4287276 -4.4286942 -4.4286833 -4.4287157 -4.4287734 -4.4287562 -4.4286866 -4.4286184 -4.4285727 -4.4285383 -4.4285283 -4.4285364 -4.4285722 -4.428617 -4.4286714][-4.4286427 -4.4286041 -4.4286079 -4.4286733 -4.4287696 -4.4287887 -4.4287553 -4.4287133 -4.4286666 -4.428618 -4.4285865 -4.4285741 -4.4285927 -4.4286294 -4.428688][-4.4285955 -4.428545 -4.428544 -4.428618 -4.4287338 -4.4287829 -4.4287815 -4.4287672 -4.4287367 -4.4287014 -4.4286737 -4.4286542 -4.4286571 -4.428678 -4.4287267][-4.4286323 -4.4285684 -4.4285469 -4.4286056 -4.4287138 -4.4287744 -4.428793 -4.4287987 -4.4287858 -4.4287682 -4.4287539 -4.42874 -4.4287376 -4.4287457 -4.4287767][-4.4287324 -4.4286733 -4.4286346 -4.4286623 -4.4287338 -4.4287777 -4.4287992 -4.4288163 -4.4288192 -4.4288168 -4.4288154 -4.4288111 -4.4288096 -4.4288116 -4.4288292][-4.428834 -4.4287906 -4.4287539 -4.4287581 -4.428793 -4.4288158 -4.4288311 -4.4288478 -4.4288573 -4.428863 -4.4288678 -4.4288669 -4.4288635 -4.4288626 -4.4288745][-4.4289021 -4.4288745 -4.4288487 -4.4288459 -4.4288616 -4.4288697 -4.4288769 -4.4288878 -4.4288969 -4.4289041 -4.4289079 -4.428905 -4.4289012 -4.4288988 -4.4289083]]...]
INFO - root - 2017-12-08 04:53:57.492256: step 410, loss = 2.28, batch loss = 2.23 (37.2 examples/sec; 0.215 sec/batch; 19h:50m:44s remains)
INFO - root - 2017-12-08 04:53:59.667078: step 420, loss = 2.28, batch loss = 2.23 (37.7 examples/sec; 0.212 sec/batch; 19h:35m:03s remains)
INFO - root - 2017-12-08 04:54:01.846015: step 430, loss = 2.28, batch loss = 2.23 (35.0 examples/sec; 0.229 sec/batch; 21h:04m:57s remains)
INFO - root - 2017-12-08 04:54:04.043806: step 440, loss = 2.28, batch loss = 2.23 (34.9 examples/sec; 0.229 sec/batch; 21h:10m:04s remains)
INFO - root - 2017-12-08 04:54:06.240989: step 450, loss = 2.28, batch loss = 2.23 (36.6 examples/sec; 0.218 sec/batch; 20h:08m:43s remains)
INFO - root - 2017-12-08 04:54:08.455947: step 460, loss = 2.28, batch loss = 2.23 (35.1 examples/sec; 0.228 sec/batch; 20h:59m:39s remains)
INFO - root - 2017-12-08 04:54:10.630827: step 470, loss = 2.28, batch loss = 2.23 (37.3 examples/sec; 0.214 sec/batch; 19h:46m:09s remains)
INFO - root - 2017-12-08 04:54:12.879109: step 480, loss = 2.28, batch loss = 2.23 (35.0 examples/sec; 0.229 sec/batch; 21h:04m:31s remains)
INFO - root - 2017-12-08 04:54:15.103829: step 490, loss = 2.28, batch loss = 2.23 (35.4 examples/sec; 0.226 sec/batch; 20h:51m:26s remains)
INFO - root - 2017-12-08 04:54:17.331281: step 500, loss = 2.28, batch loss = 2.23 (36.0 examples/sec; 0.222 sec/batch; 20h:29m:14s remains)
2017-12-08 04:54:17.612121: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288425 -4.4288273 -4.4288163 -4.4287972 -4.42877 -4.4287672 -4.4287806 -4.4287939 -4.4288 -4.4287906 -4.4287629 -4.4287195 -4.4286695 -4.4286222 -4.4285688][-4.4287949 -4.4287767 -4.4287615 -4.4287348 -4.4287024 -4.4286933 -4.4287105 -4.4287448 -4.4287739 -4.428772 -4.4287372 -4.4286823 -4.4286175 -4.4285655 -4.4285154][-4.4287376 -4.4287157 -4.4286857 -4.4286404 -4.4286013 -4.4285965 -4.4286284 -4.4286928 -4.4287453 -4.4287391 -4.4286861 -4.4286213 -4.4285555 -4.4285064 -4.4284735][-4.4287143 -4.4286795 -4.4286194 -4.4285488 -4.4285016 -4.428504 -4.428555 -4.4286423 -4.4287138 -4.4287062 -4.4286404 -4.4285765 -4.4285169 -4.4284735 -4.4284592][-4.4286909 -4.4286404 -4.4285545 -4.4284678 -4.4284182 -4.42842 -4.428473 -4.4285707 -4.4286561 -4.4286485 -4.4285893 -4.428545 -4.4285007 -4.4284706 -4.4284844][-4.428669 -4.4286118 -4.4285178 -4.4284182 -4.4283504 -4.4283237 -4.4283447 -4.4284291 -4.42852 -4.4285235 -4.4284863 -4.4284835 -4.4284754 -4.4284778 -4.4285145][-4.4286518 -4.4286051 -4.428525 -4.42842 -4.428308 -4.4282107 -4.4281521 -4.4282007 -4.4283061 -4.4283485 -4.4283614 -4.4284158 -4.4284468 -4.4284778 -4.4285283][-4.4286122 -4.4285808 -4.4285307 -4.4284334 -4.4282975 -4.4281349 -4.4279814 -4.4279833 -4.4281254 -4.4282336 -4.4283109 -4.4284062 -4.4284592 -4.4284916 -4.428535][-4.428565 -4.4285326 -4.4284978 -4.4284253 -4.4283166 -4.4281759 -4.428019 -4.4279995 -4.428133 -4.4282513 -4.4283481 -4.4284549 -4.4285135 -4.4285345 -4.4285579][-4.4285212 -4.4284925 -4.428472 -4.4284377 -4.4283881 -4.4283156 -4.4282227 -4.4282136 -4.4282908 -4.4283566 -4.4284244 -4.4285173 -4.4285717 -4.42858 -4.4285803][-4.4285011 -4.4285059 -4.4285145 -4.4285135 -4.4284992 -4.4284692 -4.4284263 -4.4284191 -4.428441 -4.4284492 -4.4284697 -4.4285369 -4.4285846 -4.4285913 -4.4285817][-4.4285502 -4.4285851 -4.4286084 -4.4286079 -4.4285922 -4.4285684 -4.4285374 -4.4285197 -4.4285121 -4.4284897 -4.4284844 -4.4285326 -4.4285774 -4.4285946 -4.4286][-4.4286218 -4.4286561 -4.4286742 -4.4286613 -4.4286356 -4.4286094 -4.4285765 -4.4285483 -4.428535 -4.4285188 -4.42852 -4.4285593 -4.4285955 -4.4286203 -4.42864][-4.4286742 -4.4286952 -4.4286995 -4.42868 -4.4286537 -4.4286342 -4.4286084 -4.428587 -4.428586 -4.4285893 -4.4285965 -4.428616 -4.4286323 -4.4286509 -4.4286733][-4.4286942 -4.4287071 -4.4287128 -4.4286995 -4.4286866 -4.4286771 -4.4286575 -4.4286518 -4.4286685 -4.4286838 -4.4286857 -4.42868 -4.4286747 -4.42868 -4.4286923]]...]
INFO - root - 2017-12-08 04:54:19.826379: step 510, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 20h:11m:48s remains)
INFO - root - 2017-12-08 04:54:22.033055: step 520, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 20h:15m:40s remains)
INFO - root - 2017-12-08 04:54:24.270965: step 530, loss = 2.28, batch loss = 2.23 (36.8 examples/sec; 0.217 sec/batch; 20h:01m:44s remains)
INFO - root - 2017-12-08 04:54:26.495472: step 540, loss = 2.28, batch loss = 2.23 (37.4 examples/sec; 0.214 sec/batch; 19h:42m:14s remains)
INFO - root - 2017-12-08 04:54:28.719552: step 550, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 20h:21m:18s remains)
INFO - root - 2017-12-08 04:54:30.896374: step 560, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 20h:13m:59s remains)
INFO - root - 2017-12-08 04:54:33.085825: step 570, loss = 2.28, batch loss = 2.23 (35.8 examples/sec; 0.224 sec/batch; 20h:36m:49s remains)
INFO - root - 2017-12-08 04:54:35.330957: step 580, loss = 2.28, batch loss = 2.23 (35.7 examples/sec; 0.224 sec/batch; 20h:39m:27s remains)
INFO - root - 2017-12-08 04:54:37.523072: step 590, loss = 2.28, batch loss = 2.23 (34.7 examples/sec; 0.231 sec/batch; 21h:15m:40s remains)
INFO - root - 2017-12-08 04:54:39.723322: step 600, loss = 2.28, batch loss = 2.23 (36.9 examples/sec; 0.217 sec/batch; 19h:58m:33s remains)
2017-12-08 04:54:40.026899: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.42874 -4.4287214 -4.4287066 -4.428689 -4.428678 -4.4286761 -4.4287004 -4.42876 -4.4288225 -4.4288568 -4.4288726 -4.4288669 -4.42883 -4.4287992 -4.4288034][-4.4287338 -4.4287219 -4.4287162 -4.4287105 -4.4287038 -4.4287038 -4.4287291 -4.4287868 -4.4288397 -4.4288611 -4.4288673 -4.4288592 -4.4288344 -4.4288173 -4.4288306][-4.428709 -4.4286923 -4.4287 -4.4287167 -4.428721 -4.4287248 -4.4287481 -4.4287987 -4.4288335 -4.4288392 -4.4288354 -4.428823 -4.4288039 -4.4287949 -4.4288173][-4.4286838 -4.4286633 -4.4286723 -4.4286938 -4.4286933 -4.4286828 -4.4286895 -4.42873 -4.4287539 -4.4287577 -4.4287596 -4.4287615 -4.428751 -4.428741 -4.4287653][-4.4286532 -4.4286451 -4.4286528 -4.4286728 -4.4286647 -4.4286284 -4.4286146 -4.428648 -4.4286747 -4.42869 -4.42871 -4.4287238 -4.4287119 -4.4286895 -4.4286962][-4.4286132 -4.4286189 -4.428618 -4.428618 -4.4285865 -4.4285235 -4.4284921 -4.4285297 -4.4285836 -4.4286332 -4.4286742 -4.4286861 -4.4286571 -4.4286165 -4.4286065][-4.4285917 -4.428606 -4.428587 -4.428546 -4.4284706 -4.4283743 -4.428328 -4.4283772 -4.4284706 -4.4285488 -4.4285913 -4.4285913 -4.4285464 -4.4284887 -4.4284768][-4.4285836 -4.4286003 -4.4285736 -4.4285049 -4.4283957 -4.4282808 -4.4282284 -4.4282808 -4.42838 -4.4284458 -4.4284644 -4.4284539 -4.4284024 -4.4283438 -4.4283538][-4.4286003 -4.42861 -4.4285831 -4.4285131 -4.4284096 -4.4283085 -4.4282608 -4.4282913 -4.42835 -4.428371 -4.4283576 -4.4283361 -4.4282885 -4.4282537 -4.4283066][-4.4286566 -4.4286585 -4.42864 -4.4285884 -4.428514 -4.4284439 -4.4284081 -4.4284143 -4.4284325 -4.4284182 -4.4283848 -4.4283509 -4.4283013 -4.4282851 -4.4283586][-4.4287109 -4.4287105 -4.4287038 -4.4286742 -4.4286323 -4.4285979 -4.428587 -4.4285874 -4.428587 -4.4285607 -4.4285197 -4.4284821 -4.4284372 -4.42843 -4.4284887][-4.4287581 -4.4287457 -4.4287357 -4.4287219 -4.4287081 -4.4287062 -4.4287219 -4.4287314 -4.4287314 -4.4287086 -4.4286757 -4.4286437 -4.4286094 -4.4286027 -4.4286366][-4.4287729 -4.4287524 -4.4287314 -4.4287276 -4.4287376 -4.4287629 -4.4287963 -4.4288173 -4.428823 -4.428812 -4.428792 -4.42877 -4.4287477 -4.4287376 -4.4287505][-4.4287639 -4.4287457 -4.4287152 -4.4287114 -4.4287381 -4.4287806 -4.428822 -4.4288478 -4.4288588 -4.4288554 -4.4288421 -4.4288278 -4.4288135 -4.4288039 -4.4288077][-4.4287639 -4.4287496 -4.4287148 -4.4287109 -4.4287438 -4.4287877 -4.4288259 -4.4288483 -4.4288564 -4.4288507 -4.4288373 -4.4288244 -4.4288149 -4.4288092 -4.4288149]]...]
INFO - root - 2017-12-08 04:54:42.243806: step 610, loss = 2.28, batch loss = 2.23 (37.5 examples/sec; 0.213 sec/batch; 19h:39m:34s remains)
INFO - root - 2017-12-08 04:54:44.435660: step 620, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.222 sec/batch; 20h:27m:08s remains)
INFO - root - 2017-12-08 04:54:46.671226: step 630, loss = 2.28, batch loss = 2.23 (34.8 examples/sec; 0.230 sec/batch; 21h:12m:29s remains)
INFO - root - 2017-12-08 04:54:48.895676: step 640, loss = 2.28, batch loss = 2.23 (37.0 examples/sec; 0.216 sec/batch; 19h:55m:48s remains)
INFO - root - 2017-12-08 04:54:51.131818: step 650, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 20h:23m:11s remains)
INFO - root - 2017-12-08 04:54:53.343152: step 660, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 20h:23m:10s remains)
INFO - root - 2017-12-08 04:54:55.613517: step 670, loss = 2.28, batch loss = 2.23 (35.7 examples/sec; 0.224 sec/batch; 20h:38m:09s remains)
INFO - root - 2017-12-08 04:54:57.809644: step 680, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.220 sec/batch; 20h:18m:17s remains)
INFO - root - 2017-12-08 04:55:00.039585: step 690, loss = 2.28, batch loss = 2.23 (34.2 examples/sec; 0.234 sec/batch; 21h:31m:59s remains)
INFO - root - 2017-12-08 04:55:02.282535: step 700, loss = 2.28, batch loss = 2.23 (35.0 examples/sec; 0.229 sec/batch; 21h:03m:56s remains)
2017-12-08 04:55:02.591622: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4287438 -4.4287395 -4.4287329 -4.4287271 -4.4287233 -4.4287219 -4.42872 -4.4287152 -4.4287128 -4.4287148 -4.4287205 -4.4287224 -4.4287066 -4.4286737 -4.4286375][-4.4287691 -4.4287624 -4.4287519 -4.4287391 -4.4287262 -4.4287152 -4.4287043 -4.428689 -4.4286766 -4.4286742 -4.4286795 -4.4286819 -4.4286675 -4.4286389 -4.4286075][-4.4288116 -4.4288054 -4.4287925 -4.4287763 -4.428761 -4.4287481 -4.4287314 -4.428709 -4.4286914 -4.4286852 -4.4286895 -4.4286909 -4.4286804 -4.4286594 -4.4286337][-4.4288235 -4.4288163 -4.4287996 -4.4287806 -4.4287682 -4.4287634 -4.4287519 -4.4287324 -4.4287162 -4.4287148 -4.4287238 -4.4287286 -4.4287229 -4.4287086 -4.4286857][-4.428782 -4.4287658 -4.4287395 -4.4287157 -4.4287086 -4.4287148 -4.4287157 -4.4287052 -4.428699 -4.4287128 -4.4287372 -4.4287519 -4.4287567 -4.4287477 -4.4287248][-4.4287181 -4.4286847 -4.4286442 -4.4286156 -4.428617 -4.4286342 -4.4286394 -4.42863 -4.4286289 -4.4286613 -4.428709 -4.4287448 -4.4287667 -4.4287663 -4.4287481][-4.4286752 -4.428627 -4.4285769 -4.4285483 -4.4285564 -4.428575 -4.4285707 -4.4285488 -4.428545 -4.4285879 -4.4286571 -4.4287171 -4.4287596 -4.428771 -4.42876][-4.4286613 -4.42862 -4.4285779 -4.4285569 -4.428566 -4.4285765 -4.428556 -4.428514 -4.4284949 -4.4285336 -4.428607 -4.4286809 -4.4287381 -4.42876 -4.4287558][-4.4286647 -4.4286442 -4.4286203 -4.4286122 -4.4286227 -4.428627 -4.4286046 -4.4285612 -4.4285359 -4.4285593 -4.4286165 -4.4286809 -4.428731 -4.4287496 -4.4287438][-4.4286919 -4.4286876 -4.4286747 -4.42867 -4.4286771 -4.4286766 -4.4286609 -4.4286327 -4.42862 -4.42864 -4.4286814 -4.4287295 -4.4287624 -4.4287643 -4.4287448][-4.4287405 -4.4287386 -4.428721 -4.4287066 -4.4287009 -4.42869 -4.4286742 -4.4286609 -4.4286652 -4.4286909 -4.42873 -4.4287705 -4.4287925 -4.4287825 -4.4287505][-4.4287896 -4.4287829 -4.4287586 -4.4287348 -4.4287214 -4.4287052 -4.4286871 -4.4286776 -4.428688 -4.4287143 -4.4287505 -4.4287844 -4.4288006 -4.4287858 -4.42875][-4.4288111 -4.428802 -4.4287772 -4.4287558 -4.4287477 -4.4287391 -4.4287267 -4.428719 -4.4287248 -4.4287395 -4.4287629 -4.4287844 -4.42879 -4.4287734 -4.4287424][-4.4288 -4.4287887 -4.4287672 -4.4287529 -4.4287529 -4.4287553 -4.4287548 -4.4287548 -4.4287581 -4.428762 -4.4287705 -4.4287777 -4.4287724 -4.4287524 -4.4287252][-4.4287882 -4.4287763 -4.4287562 -4.4287434 -4.4287424 -4.4287481 -4.4287558 -4.4287634 -4.4287682 -4.4287677 -4.4287677 -4.4287677 -4.4287596 -4.428741 -4.4287186]]...]
INFO - root - 2017-12-08 04:55:04.801103: step 710, loss = 2.28, batch loss = 2.23 (35.7 examples/sec; 0.224 sec/batch; 20h:40m:01s remains)
INFO - root - 2017-12-08 04:55:07.004690: step 720, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 20h:11m:59s remains)
INFO - root - 2017-12-08 04:55:09.247437: step 730, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.225 sec/batch; 20h:46m:30s remains)
INFO - root - 2017-12-08 04:55:11.469816: step 740, loss = 2.28, batch loss = 2.23 (35.4 examples/sec; 0.226 sec/batch; 20h:48m:22s remains)
INFO - root - 2017-12-08 04:55:13.694683: step 750, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 20h:15m:31s remains)
INFO - root - 2017-12-08 04:55:15.895854: step 760, loss = 2.28, batch loss = 2.23 (36.9 examples/sec; 0.217 sec/batch; 19h:57m:10s remains)
INFO - root - 2017-12-08 04:55:18.127147: step 770, loss = 2.28, batch loss = 2.23 (33.6 examples/sec; 0.238 sec/batch; 21h:55m:57s remains)
INFO - root - 2017-12-08 04:55:20.320802: step 780, loss = 2.28, batch loss = 2.23 (37.4 examples/sec; 0.214 sec/batch; 19h:42m:01s remains)
INFO - root - 2017-12-08 04:55:22.527296: step 790, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 20h:21m:05s remains)
INFO - root - 2017-12-08 04:55:24.780313: step 800, loss = 2.28, batch loss = 2.23 (37.2 examples/sec; 0.215 sec/batch; 19h:48m:12s remains)
2017-12-08 04:55:25.046359: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288788 -4.428865 -4.4288583 -4.428833 -4.428781 -4.4287248 -4.42869 -4.4286795 -4.428689 -4.428709 -4.4287238 -4.4287291 -4.4287052 -4.4286766 -4.428668][-4.4288845 -4.4288707 -4.428863 -4.4288392 -4.4287896 -4.428731 -4.4286866 -4.4286613 -4.4286585 -4.428669 -4.4286785 -4.4286819 -4.4286609 -4.428638 -4.42864][-4.42885 -4.4288325 -4.4288225 -4.4287982 -4.4287519 -4.4286976 -4.4286528 -4.4286208 -4.4286122 -4.4286203 -4.4286337 -4.4286423 -4.4286318 -4.4286208 -4.4286313][-4.4287777 -4.4287543 -4.4287386 -4.4287148 -4.4286752 -4.4286332 -4.4286008 -4.4285765 -4.4285741 -4.4285865 -4.4286022 -4.428607 -4.4285994 -4.4285979 -4.4286141][-4.4287014 -4.4286718 -4.4286504 -4.4286261 -4.4285903 -4.4285579 -4.4285321 -4.428515 -4.4285245 -4.4285507 -4.4285741 -4.4285803 -4.4285808 -4.4285946 -4.4286222][-4.4286323 -4.4285975 -4.428566 -4.4285326 -4.4284916 -4.4284525 -4.4284148 -4.4284005 -4.4284348 -4.4284978 -4.4285455 -4.4285641 -4.4285688 -4.4285846 -4.4286084][-4.42861 -4.4285765 -4.4285359 -4.4284873 -4.4284258 -4.4283533 -4.4282746 -4.4282446 -4.4283028 -4.4284029 -4.4284773 -4.4285116 -4.428514 -4.4285169 -4.4285293][-4.4286261 -4.4286008 -4.4285564 -4.4284964 -4.4284253 -4.4283457 -4.4282589 -4.42822 -4.4282737 -4.4283757 -4.4284515 -4.4284844 -4.42848 -4.4284687 -4.4284644][-4.42868 -4.4286604 -4.4286194 -4.428565 -4.4285111 -4.4284563 -4.4283967 -4.4283648 -4.4283957 -4.4284663 -4.4285183 -4.4285336 -4.4285212 -4.4285026 -4.42849][-4.4287534 -4.4287372 -4.4287019 -4.4286561 -4.428616 -4.4285784 -4.4285336 -4.4285064 -4.4285207 -4.4285641 -4.428596 -4.4286008 -4.4285789 -4.4285536 -4.4285431][-4.4288335 -4.42882 -4.4287953 -4.4287605 -4.4287286 -4.4286985 -4.42866 -4.4286327 -4.4286351 -4.4286585 -4.428678 -4.428679 -4.4286542 -4.4286222 -4.4286118][-4.4289 -4.4288898 -4.4288745 -4.4288511 -4.4288254 -4.4287996 -4.4287696 -4.4287496 -4.428751 -4.4287648 -4.4287777 -4.4287772 -4.4287553 -4.4287233 -4.428709][-4.4289317 -4.4289265 -4.4289179 -4.4289041 -4.4288864 -4.4288707 -4.4288535 -4.4288445 -4.4288478 -4.428854 -4.4288597 -4.4288588 -4.428843 -4.4288187 -4.4288054][-4.428937 -4.4289341 -4.4289284 -4.4289193 -4.4289083 -4.4289017 -4.4288964 -4.428896 -4.4289 -4.4289007 -4.4288979 -4.4288926 -4.4288826 -4.4288664 -4.4288568][-4.4289412 -4.42894 -4.4289355 -4.4289289 -4.4289236 -4.4289231 -4.4289231 -4.4289236 -4.428925 -4.4289203 -4.4289112 -4.4288988 -4.4288859 -4.4288692 -4.4288545]]...]
INFO - root - 2017-12-08 04:55:27.271800: step 810, loss = 2.28, batch loss = 2.23 (36.0 examples/sec; 0.222 sec/batch; 20h:29m:56s remains)
INFO - root - 2017-12-08 04:55:29.501084: step 820, loss = 2.28, batch loss = 2.23 (35.2 examples/sec; 0.227 sec/batch; 20h:54m:54s remains)
INFO - root - 2017-12-08 04:55:31.725343: step 830, loss = 2.28, batch loss = 2.23 (36.0 examples/sec; 0.222 sec/batch; 20h:27m:58s remains)
INFO - root - 2017-12-08 04:55:33.947815: step 840, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 20h:20m:36s remains)
INFO - root - 2017-12-08 04:55:36.210487: step 850, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 20h:21m:38s remains)
INFO - root - 2017-12-08 04:55:38.449681: step 860, loss = 2.28, batch loss = 2.23 (36.8 examples/sec; 0.217 sec/batch; 20h:01m:16s remains)
INFO - root - 2017-12-08 04:55:40.676028: step 870, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 20h:15m:59s remains)
INFO - root - 2017-12-08 04:55:42.874462: step 880, loss = 2.28, batch loss = 2.23 (37.0 examples/sec; 0.216 sec/batch; 19h:54m:37s remains)
INFO - root - 2017-12-08 04:55:45.129293: step 890, loss = 2.28, batch loss = 2.23 (35.7 examples/sec; 0.224 sec/batch; 20h:39m:32s remains)
INFO - root - 2017-12-08 04:55:47.356330: step 900, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 20h:11m:28s remains)
2017-12-08 04:55:47.674758: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289188 -4.4289074 -4.4289007 -4.4289026 -4.42891 -4.4289184 -4.4289274 -4.4289312 -4.4289312 -4.4289289 -4.4289289 -4.4289336 -4.4289412 -4.4289484 -4.4289541][-4.4289031 -4.4288788 -4.428863 -4.4288626 -4.4288712 -4.4288864 -4.4289093 -4.4289279 -4.4289389 -4.4289403 -4.4289384 -4.4289384 -4.4289408 -4.4289436 -4.4289479][-4.4288874 -4.4288507 -4.4288206 -4.4288077 -4.4288135 -4.4288325 -4.4288664 -4.4288979 -4.428926 -4.4289408 -4.4289465 -4.4289494 -4.4289489 -4.4289436 -4.4289408][-4.4288669 -4.4288182 -4.4287705 -4.4287424 -4.4287405 -4.428762 -4.4288063 -4.42885 -4.4288945 -4.428925 -4.428947 -4.4289618 -4.4289656 -4.4289532 -4.4289393][-4.4288383 -4.4287796 -4.4287081 -4.428659 -4.4286408 -4.4286571 -4.4287143 -4.4287796 -4.4288459 -4.42889 -4.4289241 -4.4289541 -4.4289718 -4.42896 -4.4289389][-4.4288044 -4.4287333 -4.4286375 -4.4285612 -4.4285121 -4.4285069 -4.4285679 -4.42866 -4.4287643 -4.4288325 -4.4288774 -4.4289169 -4.428947 -4.4289489 -4.4289317][-4.4287705 -4.428688 -4.4285703 -4.4284649 -4.4283743 -4.4283137 -4.4283481 -4.4284678 -4.4286289 -4.4287372 -4.4288025 -4.4288535 -4.4288926 -4.4289107 -4.4289107][-4.428762 -4.4286776 -4.4285564 -4.4284358 -4.4283047 -4.4281621 -4.4281135 -4.4282303 -4.4284449 -4.4286056 -4.4287043 -4.4287758 -4.4288297 -4.4288597 -4.4288788][-4.4287872 -4.4287176 -4.4286141 -4.4285045 -4.4283662 -4.428175 -4.4280319 -4.428071 -4.4282746 -4.428462 -4.4285979 -4.4287004 -4.4287806 -4.4288273 -4.4288559][-4.4288225 -4.4287734 -4.4287028 -4.42862 -4.4285064 -4.4283423 -4.4281836 -4.4281335 -4.4282379 -4.4283829 -4.4285231 -4.428648 -4.4287529 -4.4288182 -4.4288549][-4.4288516 -4.4288263 -4.4287934 -4.4287419 -4.4286618 -4.4285507 -4.4284391 -4.4283733 -4.4283886 -4.4284496 -4.4285436 -4.4286561 -4.428762 -4.4288325 -4.4288716][-4.4288759 -4.4288659 -4.4288554 -4.4288306 -4.4287848 -4.4287233 -4.4286656 -4.4286265 -4.4286184 -4.428628 -4.4286642 -4.4287353 -4.4288158 -4.4288716 -4.4289036][-4.4288988 -4.4288936 -4.42889 -4.4288812 -4.4288616 -4.428834 -4.4288125 -4.4287992 -4.4287996 -4.4288025 -4.4288073 -4.4288383 -4.4288845 -4.4289188 -4.4289379][-4.4289236 -4.4289217 -4.4289184 -4.4289131 -4.428906 -4.4288979 -4.4288926 -4.4288883 -4.4288921 -4.4288983 -4.4288993 -4.4289126 -4.4289341 -4.4289517 -4.4289603][-4.4289432 -4.4289441 -4.4289412 -4.428937 -4.4289317 -4.42893 -4.4289308 -4.4289308 -4.4289341 -4.4289393 -4.4289408 -4.4289489 -4.4289589 -4.4289675 -4.4289703]]...]
INFO - root - 2017-12-08 04:55:49.900282: step 910, loss = 2.28, batch loss = 2.23 (35.8 examples/sec; 0.224 sec/batch; 20h:36m:17s remains)
INFO - root - 2017-12-08 04:55:52.131521: step 920, loss = 2.28, batch loss = 2.23 (34.1 examples/sec; 0.235 sec/batch; 21h:37m:03s remains)
INFO - root - 2017-12-08 04:55:54.356409: step 930, loss = 2.28, batch loss = 2.23 (37.3 examples/sec; 0.215 sec/batch; 19h:45m:57s remains)
INFO - root - 2017-12-08 04:55:56.578177: step 940, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 20h:12m:00s remains)
INFO - root - 2017-12-08 04:55:58.818080: step 950, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.225 sec/batch; 20h:45m:36s remains)
INFO - root - 2017-12-08 04:56:01.071368: step 960, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 20h:12m:56s remains)
INFO - root - 2017-12-08 04:56:03.268521: step 970, loss = 2.28, batch loss = 2.23 (36.6 examples/sec; 0.219 sec/batch; 20h:07m:31s remains)
INFO - root - 2017-12-08 04:56:05.503090: step 980, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.221 sec/batch; 20h:19m:12s remains)
INFO - root - 2017-12-08 04:56:07.740779: step 990, loss = 2.28, batch loss = 2.23 (36.6 examples/sec; 0.218 sec/batch; 20h:06m:08s remains)
INFO - root - 2017-12-08 04:56:09.982052: step 1000, loss = 2.28, batch loss = 2.23 (35.7 examples/sec; 0.224 sec/batch; 20h:36m:27s remains)
2017-12-08 04:56:10.275698: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4286232 -4.4286432 -4.4286671 -4.4287028 -4.4287281 -4.4287276 -4.4286938 -4.4286461 -4.4286184 -4.4286122 -4.4286308 -4.4286675 -4.4287181 -4.4287853 -4.4288487][-4.4286051 -4.428628 -4.4286528 -4.4286947 -4.4287252 -4.4287286 -4.4286909 -4.4286366 -4.4286175 -4.428628 -4.4286585 -4.4287004 -4.4287505 -4.4288139 -4.4288726][-4.4285688 -4.4285941 -4.4286242 -4.428668 -4.4287004 -4.4287047 -4.4286671 -4.4286089 -4.4285955 -4.4286227 -4.4286675 -4.4287186 -4.4287682 -4.4288244 -4.4288764][-4.4285531 -4.4285789 -4.4286113 -4.4286537 -4.4286857 -4.4286904 -4.4286547 -4.4285941 -4.4285822 -4.428618 -4.4286733 -4.4287271 -4.4287739 -4.4288249 -4.4288721][-4.4285688 -4.4285893 -4.4286118 -4.4286408 -4.4286633 -4.4286637 -4.428628 -4.4285693 -4.4285569 -4.428597 -4.4286537 -4.4287071 -4.4287567 -4.4288096 -4.4288592][-4.4286079 -4.428616 -4.4286208 -4.4286294 -4.4286332 -4.428617 -4.4285674 -4.4284983 -4.4284868 -4.428545 -4.4286141 -4.4286728 -4.4287333 -4.4287972 -4.4288526][-4.428678 -4.428669 -4.4286504 -4.4286332 -4.4286113 -4.4285684 -4.4284935 -4.4284043 -4.4283962 -4.4284892 -4.4285879 -4.4286618 -4.4287329 -4.4288034 -4.4288611][-4.4287114 -4.428689 -4.4286556 -4.4286265 -4.4285927 -4.428535 -4.4284377 -4.4283223 -4.4283113 -4.4284387 -4.4285717 -4.4286671 -4.4287453 -4.4288173 -4.4288745][-4.4287152 -4.4286861 -4.4286509 -4.4286232 -4.4285927 -4.4285393 -4.4284439 -4.4283237 -4.4283133 -4.4284468 -4.4285836 -4.4286814 -4.4287586 -4.4288282 -4.4288845][-4.42871 -4.4286776 -4.4286461 -4.4286275 -4.4286122 -4.4285817 -4.4285188 -4.4284315 -4.4284215 -4.4285164 -4.4286156 -4.4286928 -4.4287591 -4.4288235 -4.42888][-4.4287038 -4.4286752 -4.4286489 -4.4286356 -4.42863 -4.4286213 -4.4285941 -4.4285378 -4.4285221 -4.4285741 -4.428637 -4.4286914 -4.4287481 -4.4288096 -4.42887][-4.4286976 -4.4286737 -4.4286509 -4.4286394 -4.4286404 -4.4286485 -4.428647 -4.4286122 -4.4285922 -4.428618 -4.42866 -4.4287009 -4.4287524 -4.428812 -4.4288726][-4.4287095 -4.4286895 -4.4286666 -4.4286585 -4.4286656 -4.4286823 -4.4286938 -4.4286714 -4.4286513 -4.4286647 -4.4286933 -4.4287276 -4.4287791 -4.4288387 -4.4288955][-4.4287462 -4.4287286 -4.4287105 -4.428709 -4.428721 -4.4287381 -4.4287486 -4.4287286 -4.4287105 -4.4287176 -4.4287419 -4.4287748 -4.4288259 -4.4288821 -4.4289303][-4.4288082 -4.4287958 -4.4287853 -4.4287868 -4.4287972 -4.4288087 -4.42881 -4.4287891 -4.4287739 -4.4287815 -4.4288058 -4.4288373 -4.4288807 -4.428926 -4.4289622]]...]
INFO - root - 2017-12-08 04:56:12.491437: step 1010, loss = 2.28, batch loss = 2.23 (35.0 examples/sec; 0.228 sec/batch; 21h:01m:01s remains)
INFO - root - 2017-12-08 04:56:14.719003: step 1020, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 20h:11m:45s remains)
INFO - root - 2017-12-08 04:56:16.956378: step 1030, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.225 sec/batch; 20h:45m:35s remains)
INFO - root - 2017-12-08 04:56:19.165022: step 1040, loss = 2.28, batch loss = 2.23 (35.6 examples/sec; 0.225 sec/batch; 20h:41m:55s remains)
INFO - root - 2017-12-08 04:56:21.414718: step 1050, loss = 2.28, batch loss = 2.23 (36.7 examples/sec; 0.218 sec/batch; 20h:04m:53s remains)
INFO - root - 2017-12-08 04:56:23.614322: step 1060, loss = 2.28, batch loss = 2.23 (35.4 examples/sec; 0.226 sec/batch; 20h:47m:29s remains)
INFO - root - 2017-12-08 04:56:25.827329: step 1070, loss = 2.28, batch loss = 2.23 (35.2 examples/sec; 0.227 sec/batch; 20h:54m:19s remains)
INFO - root - 2017-12-08 04:56:28.046892: step 1080, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 20h:32m:34s remains)
INFO - root - 2017-12-08 04:56:30.312893: step 1090, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 20h:14m:17s remains)
INFO - root - 2017-12-08 04:56:32.538311: step 1100, loss = 2.28, batch loss = 2.23 (36.8 examples/sec; 0.218 sec/batch; 20h:01m:55s remains)
2017-12-08 04:56:32.818327: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4287238 -4.428668 -4.4285941 -4.4285479 -4.428535 -4.42856 -4.4286289 -4.4287071 -4.4287639 -4.4288006 -4.4288306 -4.428844 -4.4288673 -4.4288926 -4.4289031][-4.4287248 -4.4286704 -4.4285831 -4.4284883 -4.4284134 -4.42841 -4.4285035 -4.4286256 -4.42871 -4.4287629 -4.4288054 -4.4288249 -4.4288526 -4.4288826 -4.4289002][-4.4287419 -4.4286928 -4.4286041 -4.4284883 -4.4283743 -4.4283338 -4.4284229 -4.4285564 -4.4286537 -4.4287186 -4.4287677 -4.4287977 -4.4288373 -4.4288716 -4.4288931][-4.4287605 -4.4287243 -4.4286571 -4.4285679 -4.428463 -4.4284015 -4.4284511 -4.4285483 -4.4286332 -4.4286895 -4.4287395 -4.4287782 -4.4288263 -4.4288692 -4.4288926][-4.42874 -4.4287109 -4.4286537 -4.4285889 -4.4285097 -4.4284506 -4.4284682 -4.4285235 -4.4285936 -4.4286518 -4.4287062 -4.4287534 -4.4288092 -4.42886 -4.4288898][-4.4286928 -4.428668 -4.4286137 -4.4285603 -4.4284968 -4.4284339 -4.4284158 -4.4284334 -4.4284916 -4.4285631 -4.4286366 -4.4287 -4.4287682 -4.428834 -4.4288778][-4.4286613 -4.4286346 -4.428587 -4.4285407 -4.4284811 -4.4284058 -4.4283528 -4.4283304 -4.4283695 -4.4284568 -4.4285493 -4.4286308 -4.4287109 -4.428792 -4.4288559][-4.4286928 -4.4286604 -4.4286146 -4.4285731 -4.4285221 -4.428452 -4.4283905 -4.428349 -4.4283638 -4.4284372 -4.4285245 -4.428606 -4.4286876 -4.428772 -4.4288473][-4.4287539 -4.4287243 -4.4286795 -4.4286313 -4.4285803 -4.4285221 -4.4284749 -4.428443 -4.4284463 -4.4284949 -4.4285631 -4.4286327 -4.428709 -4.428791 -4.4288645][-4.4287791 -4.4287629 -4.4287286 -4.4286776 -4.42862 -4.4285574 -4.428514 -4.4284935 -4.4284954 -4.4285278 -4.4285917 -4.4286618 -4.4287338 -4.4288125 -4.4288831][-4.4287572 -4.4287615 -4.4287491 -4.4287152 -4.4286637 -4.4286027 -4.4285541 -4.4285278 -4.4285178 -4.4285359 -4.4285965 -4.4286704 -4.4287415 -4.4288173 -4.4288845][-4.4287405 -4.4287605 -4.4287734 -4.4287648 -4.4287333 -4.4286852 -4.4286366 -4.4285941 -4.4285612 -4.4285569 -4.4286027 -4.428669 -4.4287376 -4.4288106 -4.4288707][-4.4287682 -4.4287858 -4.4288039 -4.4288106 -4.4288015 -4.4287758 -4.4287376 -4.4286876 -4.4286356 -4.42861 -4.4286351 -4.4286852 -4.4287462 -4.4288116 -4.4288611][-4.4288015 -4.4288082 -4.4288177 -4.4288263 -4.4288335 -4.4288278 -4.4288068 -4.428761 -4.4287028 -4.4286623 -4.42867 -4.4287066 -4.42876 -4.4288139 -4.4288535][-4.4288139 -4.428812 -4.428813 -4.4288182 -4.4288263 -4.4288268 -4.4288168 -4.4287767 -4.4287205 -4.428679 -4.42868 -4.4287071 -4.4287553 -4.4288025 -4.4288363]]...]
INFO - root - 2017-12-08 04:56:35.031722: step 1110, loss = 2.28, batch loss = 2.23 (34.7 examples/sec; 0.231 sec/batch; 21h:13m:23s remains)
INFO - root - 2017-12-08 04:56:37.229455: step 1120, loss = 2.28, batch loss = 2.23 (35.1 examples/sec; 0.228 sec/batch; 20h:59m:00s remains)
INFO - root - 2017-12-08 04:56:39.436501: step 1130, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 20h:30m:53s remains)
INFO - root - 2017-12-08 04:56:41.666769: step 1140, loss = 2.28, batch loss = 2.23 (36.7 examples/sec; 0.218 sec/batch; 20h:02m:52s remains)
INFO - root - 2017-12-08 04:56:43.914325: step 1150, loss = 2.28, batch loss = 2.23 (35.2 examples/sec; 0.227 sec/batch; 20h:56m:01s remains)
INFO - root - 2017-12-08 04:56:46.140529: step 1160, loss = 2.28, batch loss = 2.23 (36.7 examples/sec; 0.218 sec/batch; 20h:04m:44s remains)
INFO - root - 2017-12-08 04:56:48.391232: step 1170, loss = 2.28, batch loss = 2.23 (35.3 examples/sec; 0.227 sec/batch; 20h:50m:47s remains)
INFO - root - 2017-12-08 04:56:50.613116: step 1180, loss = 2.28, batch loss = 2.23 (35.7 examples/sec; 0.224 sec/batch; 20h:38m:11s remains)
INFO - root - 2017-12-08 04:56:52.845527: step 1190, loss = 2.28, batch loss = 2.23 (34.8 examples/sec; 0.230 sec/batch; 21h:08m:02s remains)
INFO - root - 2017-12-08 04:56:55.074643: step 1200, loss = 2.28, batch loss = 2.23 (34.5 examples/sec; 0.232 sec/batch; 21h:19m:26s remains)
2017-12-08 04:56:55.356512: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288411 -4.428834 -4.428854 -4.4288878 -4.4289079 -4.4289036 -4.4288983 -4.428906 -4.4289317 -4.4289403 -4.4289341 -4.4289145 -4.428894 -4.4288745 -4.4288621][-4.42878 -4.4287624 -4.428792 -4.4288521 -4.4288926 -4.4289007 -4.428905 -4.4289236 -4.42896 -4.4289761 -4.4289727 -4.4289408 -4.4288979 -4.42886 -4.4288321][-4.4287047 -4.428689 -4.428731 -4.4288116 -4.42886 -4.4288692 -4.4288754 -4.4288988 -4.4289412 -4.4289675 -4.4289761 -4.4289522 -4.42891 -4.428863 -4.4288187][-4.4286823 -4.4286861 -4.4287319 -4.4288092 -4.4288497 -4.4288406 -4.4288254 -4.42884 -4.4288845 -4.4289188 -4.4289393 -4.428937 -4.4289184 -4.4288826 -4.42883][-4.4287095 -4.4287291 -4.4287729 -4.4288278 -4.4288387 -4.4287939 -4.4287353 -4.4287229 -4.4287763 -4.4288311 -4.4288731 -4.4289 -4.4289126 -4.4288988 -4.4288564][-4.4287148 -4.4287448 -4.4287796 -4.428802 -4.4287744 -4.4286857 -4.428565 -4.4284992 -4.4285746 -4.4286895 -4.4287767 -4.4288368 -4.4288807 -4.4288955 -4.4288726][-4.4287105 -4.4287462 -4.4287691 -4.4287543 -4.4286833 -4.4285488 -4.4283552 -4.4282026 -4.4283128 -4.4285173 -4.4286718 -4.4287767 -4.4288464 -4.4288807 -4.4288716][-4.42871 -4.4287362 -4.4287534 -4.4287171 -4.4286218 -4.4284673 -4.4282503 -4.4280653 -4.42819 -4.4284391 -4.4286242 -4.4287477 -4.4288278 -4.4288716 -4.4288769][-4.428699 -4.4287195 -4.4287391 -4.4287086 -4.4286323 -4.4285173 -4.4283738 -4.4282618 -4.4283452 -4.428524 -4.428668 -4.4287715 -4.428844 -4.4288869 -4.4288993][-4.4287133 -4.4287267 -4.4287515 -4.4287462 -4.4287124 -4.4286528 -4.4285927 -4.4285526 -4.4285941 -4.4286842 -4.428762 -4.4288182 -4.428864 -4.4288955 -4.42891][-4.4287672 -4.4287715 -4.4287906 -4.4288011 -4.4287868 -4.4287639 -4.4287567 -4.4287639 -4.4287915 -4.4288249 -4.4288487 -4.428863 -4.4288764 -4.4288874 -4.4289007][-4.4288487 -4.4288473 -4.428853 -4.4288526 -4.4288387 -4.4288335 -4.428854 -4.4288769 -4.4288912 -4.4288907 -4.428884 -4.4288778 -4.4288735 -4.4288683 -4.4288745][-4.428905 -4.4288955 -4.42889 -4.4288807 -4.4288621 -4.4288573 -4.4288864 -4.428906 -4.4289026 -4.4288764 -4.4288554 -4.4288454 -4.4288406 -4.4288363 -4.4288387][-4.4289112 -4.4288983 -4.4288898 -4.4288812 -4.428865 -4.42886 -4.4288807 -4.4288878 -4.4288716 -4.428834 -4.4288106 -4.4288063 -4.428802 -4.428803 -4.4288139][-4.428896 -4.428884 -4.4288797 -4.4288759 -4.4288688 -4.4288716 -4.42888 -4.428875 -4.4288564 -4.42882 -4.4287996 -4.4287982 -4.4287953 -4.4288049 -4.4288297]]...]
INFO - root - 2017-12-08 04:56:57.594240: step 1210, loss = 2.28, batch loss = 2.23 (35.6 examples/sec; 0.225 sec/batch; 20h:42m:16s remains)
INFO - root - 2017-12-08 04:56:59.822073: step 1220, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 20h:21m:14s remains)
INFO - root - 2017-12-08 04:57:02.046171: step 1230, loss = 2.28, batch loss = 2.23 (35.2 examples/sec; 0.227 sec/batch; 20h:55m:39s remains)
INFO - root - 2017-12-08 04:57:04.291032: step 1240, loss = 2.28, batch loss = 2.23 (35.4 examples/sec; 0.226 sec/batch; 20h:46m:50s remains)
INFO - root - 2017-12-08 04:57:06.524404: step 1250, loss = 2.28, batch loss = 2.23 (34.1 examples/sec; 0.234 sec/batch; 21h:34m:27s remains)
INFO - root - 2017-12-08 04:57:08.736508: step 1260, loss = 2.28, batch loss = 2.23 (34.3 examples/sec; 0.233 sec/batch; 21h:26m:53s remains)
INFO - root - 2017-12-08 04:57:10.965652: step 1270, loss = 2.28, batch loss = 2.23 (37.1 examples/sec; 0.216 sec/batch; 19h:51m:20s remains)
INFO - root - 2017-12-08 04:57:13.213487: step 1280, loss = 2.28, batch loss = 2.23 (35.4 examples/sec; 0.226 sec/batch; 20h:45m:55s remains)
INFO - root - 2017-12-08 04:57:15.434488: step 1290, loss = 2.28, batch loss = 2.23 (36.9 examples/sec; 0.217 sec/batch; 19h:56m:07s remains)
INFO - root - 2017-12-08 04:57:17.711074: step 1300, loss = 2.28, batch loss = 2.23 (34.4 examples/sec; 0.233 sec/batch; 21h:23m:27s remains)
2017-12-08 04:57:18.016301: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4287624 -4.4287996 -4.4287958 -4.4287758 -4.4287539 -4.428709 -4.42867 -4.428699 -4.4287262 -4.4287429 -4.4287858 -4.4287825 -4.4287615 -4.4287314 -4.4286804][-4.4287624 -4.4288106 -4.4288163 -4.4287915 -4.4287724 -4.4287224 -4.4286571 -4.4286723 -4.42871 -4.4287324 -4.4287829 -4.428802 -4.4287906 -4.4287653 -4.428721][-4.42876 -4.4288054 -4.4288144 -4.428782 -4.4287629 -4.4287243 -4.4286551 -4.4286571 -4.4287057 -4.42873 -4.4287825 -4.4288254 -4.4288239 -4.4288006 -4.42876][-4.428741 -4.4287739 -4.4287739 -4.4287281 -4.4286919 -4.4286547 -4.4285874 -4.42857 -4.4286251 -4.42867 -4.428731 -4.4287891 -4.4287972 -4.4287724 -4.4287381][-4.4286742 -4.4286761 -4.4286575 -4.4286003 -4.4285493 -4.4285116 -4.4284477 -4.4284015 -4.4284496 -4.4285235 -4.428597 -4.4286776 -4.4287105 -4.4286957 -4.4286733][-4.428575 -4.42853 -4.4284854 -4.4284258 -4.4283714 -4.4283309 -4.4282589 -4.4281673 -4.4282031 -4.4283175 -4.4284191 -4.42852 -4.4285812 -4.428586 -4.4285779][-4.4285345 -4.4284592 -4.4283891 -4.4283271 -4.4282718 -4.4282141 -4.4281235 -4.4279804 -4.4279876 -4.428143 -4.4282775 -4.4283795 -4.428452 -4.4284749 -4.4284739][-4.4285583 -4.4284887 -4.428421 -4.4283681 -4.4283128 -4.428246 -4.4281721 -4.4280558 -4.4280419 -4.4281697 -4.4282851 -4.4283524 -4.4284077 -4.42843 -4.4284229][-4.4285922 -4.4285393 -4.4284873 -4.4284506 -4.4284039 -4.4283414 -4.4282928 -4.4282236 -4.4282107 -4.4282928 -4.4283714 -4.4284143 -4.428452 -4.4284725 -4.428452][-4.4286151 -4.4285769 -4.4285417 -4.4285192 -4.4284906 -4.4284477 -4.4284191 -4.4283819 -4.4283724 -4.4284148 -4.4284596 -4.42848 -4.4285021 -4.4285216 -4.4285011][-4.4286394 -4.4286041 -4.4285769 -4.4285622 -4.4285502 -4.4285345 -4.4285245 -4.4285026 -4.428483 -4.4284873 -4.4285054 -4.4285131 -4.428525 -4.4285493 -4.4285398][-4.4286609 -4.4286242 -4.4285922 -4.4285755 -4.4285727 -4.4285736 -4.428575 -4.4285612 -4.4285374 -4.4285245 -4.4285321 -4.4285383 -4.4285507 -4.4285774 -4.4285793][-4.4286876 -4.4286504 -4.4286108 -4.428587 -4.4285831 -4.4285884 -4.4285941 -4.4285851 -4.4285665 -4.4285555 -4.4285626 -4.4285712 -4.4285812 -4.428606 -4.4286132][-4.4287353 -4.4287019 -4.4286604 -4.4286284 -4.4286141 -4.4286156 -4.4286227 -4.4286156 -4.4285989 -4.4285874 -4.4285893 -4.4285936 -4.4286032 -4.4286256 -4.428638][-4.4287925 -4.4287667 -4.428731 -4.4286985 -4.4286809 -4.4286819 -4.4286904 -4.4286861 -4.4286723 -4.4286575 -4.4286432 -4.4286323 -4.4286308 -4.4286418 -4.4286509]]...]
INFO - root - 2017-12-08 04:57:20.237940: step 1310, loss = 2.28, batch loss = 2.23 (35.4 examples/sec; 0.226 sec/batch; 20h:47m:30s remains)
INFO - root - 2017-12-08 04:57:22.476100: step 1320, loss = 2.28, batch loss = 2.23 (35.8 examples/sec; 0.223 sec/batch; 20h:32m:42s remains)
INFO - root - 2017-12-08 04:57:24.732610: step 1330, loss = 2.28, batch loss = 2.23 (37.7 examples/sec; 0.212 sec/batch; 19h:32m:07s remains)
INFO - root - 2017-12-08 04:57:26.971471: step 1340, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.225 sec/batch; 20h:42m:33s remains)
INFO - root - 2017-12-08 04:57:29.224555: step 1350, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.220 sec/batch; 20h:16m:26s remains)
INFO - root - 2017-12-08 04:57:31.467553: step 1360, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.225 sec/batch; 20h:43m:34s remains)
INFO - root - 2017-12-08 04:57:33.701808: step 1370, loss = 2.28, batch loss = 2.23 (34.9 examples/sec; 0.229 sec/batch; 21h:05m:16s remains)
INFO - root - 2017-12-08 04:57:35.925549: step 1380, loss = 2.28, batch loss = 2.23 (35.1 examples/sec; 0.228 sec/batch; 20h:58m:02s remains)
INFO - root - 2017-12-08 04:57:38.169989: step 1390, loss = 2.28, batch loss = 2.23 (34.9 examples/sec; 0.229 sec/batch; 21h:03m:13s remains)
INFO - root - 2017-12-08 04:57:40.402423: step 1400, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 20h:18m:05s remains)
2017-12-08 04:57:40.697993: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4287434 -4.4287562 -4.4287572 -4.4287462 -4.4287424 -4.4287424 -4.4287357 -4.4287195 -4.4287038 -4.4286985 -4.4287009 -4.4286995 -4.4287009 -4.4287114 -4.4287324][-4.4287252 -4.4287415 -4.4287472 -4.4287372 -4.4287348 -4.42874 -4.4287362 -4.4287229 -4.4287128 -4.4287138 -4.4287205 -4.4287167 -4.4287133 -4.428719 -4.4287319][-4.4287128 -4.4287262 -4.4287319 -4.4287243 -4.42872 -4.4287243 -4.4287224 -4.4287138 -4.4287071 -4.4287114 -4.4287248 -4.4287252 -4.4287233 -4.4287262 -4.4287357][-4.4286733 -4.4286828 -4.428689 -4.4286823 -4.4286761 -4.4286771 -4.4286795 -4.4286785 -4.4286776 -4.4286852 -4.4287047 -4.4287148 -4.4287124 -4.4287133 -4.4287229][-4.4286137 -4.4286156 -4.4286232 -4.4286165 -4.4286027 -4.4285951 -4.4285927 -4.4285989 -4.4286118 -4.4286242 -4.4286494 -4.4286718 -4.428678 -4.4286795 -4.4286866][-4.4285769 -4.42856 -4.4285526 -4.4285326 -4.4285011 -4.4284754 -4.4284697 -4.42849 -4.4285321 -4.428565 -4.4285994 -4.4286294 -4.4286413 -4.428647 -4.4286494][-4.4286137 -4.4285641 -4.428525 -4.4284778 -4.4284139 -4.4283586 -4.4283428 -4.4283748 -4.4284496 -4.4285173 -4.4285746 -4.4286227 -4.4286513 -4.4286656 -4.4286671][-4.4286814 -4.4286122 -4.4285541 -4.42849 -4.428401 -4.4283209 -4.4282942 -4.4283276 -4.4284215 -4.4285197 -4.4286032 -4.4286709 -4.4287171 -4.4287434 -4.4287486][-4.4287477 -4.4286823 -4.4286294 -4.4285717 -4.4284911 -4.4284196 -4.4283962 -4.4284205 -4.4285011 -4.4285865 -4.42866 -4.4287181 -4.4287639 -4.4287953 -4.4288082][-4.4287982 -4.4287438 -4.4287019 -4.4286571 -4.4286008 -4.4285579 -4.4285536 -4.4285789 -4.4286342 -4.4286847 -4.4287252 -4.4287505 -4.428771 -4.4287887 -4.4287996][-4.4288325 -4.4287863 -4.4287491 -4.4287143 -4.4286761 -4.4286489 -4.4286456 -4.4286609 -4.4286947 -4.4287205 -4.4287381 -4.4287372 -4.4287367 -4.428741 -4.4287496][-4.4288554 -4.4288225 -4.4287925 -4.4287634 -4.4287305 -4.428699 -4.4286757 -4.4286671 -4.428679 -4.4286914 -4.4286947 -4.4286914 -4.42869 -4.4287009 -4.4287205][-4.4288597 -4.4288454 -4.4288263 -4.4288063 -4.4287791 -4.4287405 -4.4286966 -4.4286647 -4.4286532 -4.42865 -4.4286532 -4.4286671 -4.4286866 -4.4287205 -4.4287577][-4.4288449 -4.428853 -4.4288473 -4.4288359 -4.4288163 -4.428781 -4.4287357 -4.4286966 -4.4286728 -4.4286604 -4.4286628 -4.4286852 -4.4287186 -4.4287705 -4.4288206][-4.42879 -4.4288144 -4.4288254 -4.4288282 -4.4288239 -4.4288049 -4.4287734 -4.428741 -4.4287162 -4.4287 -4.4287024 -4.4287257 -4.428761 -4.4288106 -4.4288554]]...]
INFO - root - 2017-12-08 04:57:42.915236: step 1410, loss = 2.28, batch loss = 2.23 (34.9 examples/sec; 0.229 sec/batch; 21h:05m:07s remains)
INFO - root - 2017-12-08 04:57:45.152979: step 1420, loss = 2.28, batch loss = 2.23 (34.2 examples/sec; 0.234 sec/batch; 21h:32m:18s remains)
INFO - root - 2017-12-08 04:57:47.380030: step 1430, loss = 2.28, batch loss = 2.23 (35.6 examples/sec; 0.225 sec/batch; 20h:39m:15s remains)
INFO - root - 2017-12-08 04:57:49.617723: step 1440, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 20h:12m:44s remains)
INFO - root - 2017-12-08 04:57:51.845206: step 1450, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 20h:11m:27s remains)
INFO - root - 2017-12-08 04:57:54.074914: step 1460, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 20h:28m:02s remains)
INFO - root - 2017-12-08 04:57:56.289907: step 1470, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 20h:11m:52s remains)
INFO - root - 2017-12-08 04:57:58.524094: step 1480, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 20h:28m:29s remains)
INFO - root - 2017-12-08 04:58:00.762535: step 1490, loss = 2.28, batch loss = 2.23 (35.7 examples/sec; 0.224 sec/batch; 20h:36m:41s remains)
INFO - root - 2017-12-08 04:58:03.019270: step 1500, loss = 2.28, batch loss = 2.23 (36.9 examples/sec; 0.217 sec/batch; 19h:55m:40s remains)
2017-12-08 04:58:03.329535: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289126 -4.428915 -4.4289212 -4.4289317 -4.4289389 -4.4289293 -4.4288979 -4.4288521 -4.4288173 -4.4288073 -4.428822 -4.4288239 -4.4287944 -4.4287357 -4.4286952][-4.4289055 -4.4289079 -4.428905 -4.4289012 -4.4289 -4.4288921 -4.4288721 -4.4288416 -4.4288154 -4.4288073 -4.4288192 -4.4288144 -4.4287715 -4.4286928 -4.4286375][-4.4288826 -4.4288812 -4.4288692 -4.4288573 -4.4288554 -4.4288549 -4.4288354 -4.4288054 -4.4287767 -4.4287705 -4.428792 -4.428792 -4.4287372 -4.4286389 -4.4285727][-4.4288692 -4.4288626 -4.4288435 -4.4288278 -4.4288235 -4.4288263 -4.4288087 -4.4287786 -4.4287529 -4.4287524 -4.428781 -4.4287777 -4.4287095 -4.4286051 -4.4285455][-4.42887 -4.4288535 -4.4288278 -4.4288054 -4.4287953 -4.4287949 -4.42878 -4.4287572 -4.4287505 -4.4287667 -4.428792 -4.4287748 -4.4286919 -4.4285951 -4.4285622][-4.42887 -4.4288387 -4.4288015 -4.4287691 -4.4287491 -4.4287376 -4.4287181 -4.4287138 -4.4287424 -4.428781 -4.4288006 -4.4287653 -4.4286828 -4.428616 -4.428618][-4.4288688 -4.4288297 -4.4287858 -4.4287453 -4.4287071 -4.4286695 -4.4286346 -4.4286537 -4.4287186 -4.4287729 -4.4287829 -4.4287434 -4.4286857 -4.4286566 -4.4286819][-4.4288821 -4.4288445 -4.4288 -4.4287534 -4.4286952 -4.428627 -4.4285831 -4.4286218 -4.4287014 -4.4287496 -4.4287558 -4.4287314 -4.4287052 -4.4286981 -4.4287291][-4.4289002 -4.4288683 -4.4288321 -4.4287887 -4.4287229 -4.4286442 -4.4286022 -4.428638 -4.4286942 -4.4287267 -4.4287429 -4.4287472 -4.4287448 -4.4287443 -4.4287667][-4.4288936 -4.4288712 -4.4288483 -4.428812 -4.4287457 -4.428679 -4.4286442 -4.4286556 -4.4286804 -4.4287047 -4.4287405 -4.42877 -4.4287796 -4.4287806 -4.4287863][-4.4288573 -4.4288483 -4.4288397 -4.4288068 -4.42874 -4.4286804 -4.4286509 -4.42865 -4.4286618 -4.4286904 -4.4287324 -4.4287734 -4.4287872 -4.4287839 -4.4287748][-4.428813 -4.4288144 -4.4288139 -4.428782 -4.4287176 -4.4286661 -4.4286404 -4.4286351 -4.4286532 -4.42869 -4.4287295 -4.4287653 -4.4287729 -4.4287663 -4.428751][-4.4287753 -4.4287896 -4.4287891 -4.4287562 -4.4287019 -4.4286666 -4.4286456 -4.4286437 -4.4286656 -4.4286947 -4.4287276 -4.4287543 -4.4287596 -4.4287572 -4.4287524][-4.4287505 -4.4287758 -4.4287734 -4.4287424 -4.4287024 -4.4286842 -4.4286718 -4.4286747 -4.428689 -4.4287052 -4.4287324 -4.4287553 -4.4287577 -4.4287648 -4.4287763][-4.4287267 -4.4287553 -4.4287481 -4.4287243 -4.4287033 -4.4287004 -4.4287057 -4.4287114 -4.4287124 -4.4287186 -4.4287391 -4.4287515 -4.4287519 -4.4287686 -4.4287815]]...]
INFO - root - 2017-12-08 04:58:05.568340: step 1510, loss = 2.28, batch loss = 2.23 (35.1 examples/sec; 0.228 sec/batch; 20h:58m:21s remains)
INFO - root - 2017-12-08 04:58:07.792311: step 1520, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 20h:09m:45s remains)
INFO - root - 2017-12-08 04:58:10.016834: step 1530, loss = 2.28, batch loss = 2.23 (35.3 examples/sec; 0.226 sec/batch; 20h:49m:02s remains)
INFO - root - 2017-12-08 04:58:12.286259: step 1540, loss = 2.28, batch loss = 2.23 (32.2 examples/sec; 0.249 sec/batch; 22h:51m:04s remains)
INFO - root - 2017-12-08 04:58:14.523623: step 1550, loss = 2.28, batch loss = 2.23 (35.2 examples/sec; 0.227 sec/batch; 20h:52m:28s remains)
INFO - root - 2017-12-08 04:58:16.754973: step 1560, loss = 2.28, batch loss = 2.23 (34.9 examples/sec; 0.229 sec/batch; 21h:03m:40s remains)
INFO - root - 2017-12-08 04:58:18.982451: step 1570, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 20h:27m:57s remains)
INFO - root - 2017-12-08 04:58:21.221571: step 1580, loss = 2.28, batch loss = 2.23 (35.2 examples/sec; 0.227 sec/batch; 20h:52m:17s remains)
INFO - root - 2017-12-08 04:58:23.455664: step 1590, loss = 2.28, batch loss = 2.23 (34.1 examples/sec; 0.235 sec/batch; 21h:33m:26s remains)
INFO - root - 2017-12-08 04:58:25.697075: step 1600, loss = 2.28, batch loss = 2.23 (37.7 examples/sec; 0.212 sec/batch; 19h:30m:12s remains)
2017-12-08 04:58:25.986535: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289684 -4.4289713 -4.4289703 -4.4289689 -4.4289618 -4.4289479 -4.4289355 -4.4289231 -4.4289217 -4.4289289 -4.4289389 -4.4289541 -4.4289742 -4.42899 -4.428997][-4.4289422 -4.4289451 -4.4289479 -4.4289532 -4.428947 -4.4289327 -4.42892 -4.4289069 -4.4289079 -4.4289188 -4.4289327 -4.4289522 -4.4289718 -4.4289784 -4.4289718][-4.428905 -4.4289045 -4.4289107 -4.4289203 -4.428915 -4.4288993 -4.4288859 -4.4288735 -4.4288769 -4.4288893 -4.4289064 -4.4289308 -4.4289513 -4.42895 -4.42893][-4.42887 -4.4288664 -4.4288754 -4.4288831 -4.4288726 -4.428853 -4.4288378 -4.4288282 -4.4288383 -4.4288554 -4.4288831 -4.4289179 -4.4289408 -4.4289365 -4.4289083][-4.4288459 -4.4288344 -4.4288359 -4.4288287 -4.4288044 -4.428771 -4.4287443 -4.42873 -4.428741 -4.4287744 -4.4288268 -4.4288812 -4.4289126 -4.4289174 -4.4288974][-4.4288282 -4.4288058 -4.4287848 -4.4287548 -4.4287162 -4.4286656 -4.4286184 -4.4285827 -4.4285851 -4.4286456 -4.4287376 -4.4288197 -4.4288669 -4.4288864 -4.4288836][-4.4288311 -4.4288011 -4.42876 -4.4287062 -4.4286389 -4.4285483 -4.4284515 -4.4283609 -4.4283442 -4.4284515 -4.4286013 -4.4287281 -4.4288096 -4.4288535 -4.4288669][-4.4288454 -4.428813 -4.4287615 -4.4286976 -4.4286108 -4.4284873 -4.4283319 -4.428164 -4.4281144 -4.42826 -4.4284525 -4.4286213 -4.4287429 -4.428813 -4.4288464][-4.4288568 -4.4288378 -4.4288082 -4.4287763 -4.4287214 -4.4286251 -4.4284873 -4.4283218 -4.428257 -4.4283595 -4.4285054 -4.4286513 -4.4287629 -4.4288259 -4.4288568][-4.4288616 -4.4288607 -4.42886 -4.4288611 -4.4288359 -4.4287629 -4.4286513 -4.4285231 -4.4284682 -4.4285212 -4.4286022 -4.4287052 -4.428791 -4.4288392 -4.4288611][-4.4288311 -4.4288297 -4.42884 -4.4288583 -4.4288473 -4.4287863 -4.4286966 -4.4286046 -4.4285712 -4.4286022 -4.4286447 -4.4287167 -4.4287829 -4.4288211 -4.4288392][-4.4287949 -4.4287868 -4.4287992 -4.4288263 -4.4288268 -4.428781 -4.4287224 -4.4286661 -4.4286532 -4.4286804 -4.4287114 -4.4287605 -4.4288049 -4.428833 -4.4288445][-4.4287696 -4.428751 -4.4287553 -4.4287739 -4.4287758 -4.4287419 -4.4287052 -4.4286723 -4.4286723 -4.4287014 -4.4287314 -4.4287744 -4.4288144 -4.4288449 -4.4288578][-4.4287572 -4.428721 -4.4287128 -4.4287176 -4.4287152 -4.4286923 -4.4286718 -4.4286513 -4.4286571 -4.4286942 -4.4287314 -4.4287763 -4.4288197 -4.428854 -4.4288692][-4.4287724 -4.4287267 -4.4287086 -4.4287066 -4.4287047 -4.4286923 -4.4286814 -4.4286685 -4.4286776 -4.4287109 -4.4287467 -4.4287887 -4.428834 -4.4288707 -4.4288874]]...]
INFO - root - 2017-12-08 04:58:28.194261: step 1610, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 20h:29m:10s remains)
INFO - root - 2017-12-08 04:58:30.445443: step 1620, loss = 2.28, batch loss = 2.23 (36.8 examples/sec; 0.217 sec/batch; 19h:58m:20s remains)
INFO - root - 2017-12-08 04:58:32.711772: step 1630, loss = 2.28, batch loss = 2.23 (36.6 examples/sec; 0.219 sec/batch; 20h:06m:19s remains)
INFO - root - 2017-12-08 04:58:34.956144: step 1640, loss = 2.28, batch loss = 2.23 (35.0 examples/sec; 0.229 sec/batch; 21h:00m:20s remains)
INFO - root - 2017-12-08 04:58:37.208254: step 1650, loss = 2.28, batch loss = 2.23 (35.7 examples/sec; 0.224 sec/batch; 20h:34m:51s remains)
INFO - root - 2017-12-08 04:58:39.449394: step 1660, loss = 2.28, batch loss = 2.23 (35.3 examples/sec; 0.226 sec/batch; 20h:47m:55s remains)
INFO - root - 2017-12-08 04:58:41.680692: step 1670, loss = 2.28, batch loss = 2.23 (35.2 examples/sec; 0.227 sec/batch; 20h:53m:37s remains)
INFO - root - 2017-12-08 04:58:43.899374: step 1680, loss = 2.28, batch loss = 2.23 (37.0 examples/sec; 0.216 sec/batch; 19h:51m:42s remains)
INFO - root - 2017-12-08 04:58:46.136888: step 1690, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 20h:11m:58s remains)
INFO - root - 2017-12-08 04:58:48.360448: step 1700, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.225 sec/batch; 20h:40m:42s remains)
2017-12-08 04:58:48.637320: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4286375 -4.4285774 -4.4286261 -4.4287324 -4.4288449 -4.4289312 -4.4289527 -4.4289346 -4.4289184 -4.4289007 -4.4289012 -4.4289246 -4.4289455 -4.4289231 -4.4288745][-4.4285388 -4.4284625 -4.4285407 -4.428688 -4.4288216 -4.4289007 -4.4289103 -4.428884 -4.4288712 -4.4288669 -4.4288864 -4.4289222 -4.4289494 -4.4289179 -4.4288588][-4.4284887 -4.42841 -4.4285069 -4.4286771 -4.4288168 -4.4288769 -4.4288683 -4.4288325 -4.4288216 -4.428833 -4.4288731 -4.4289293 -4.4289565 -4.4289145 -4.4288454][-4.4285297 -4.4284859 -4.4285874 -4.42874 -4.428853 -4.42888 -4.4288363 -4.4287767 -4.4287653 -4.4287934 -4.4288583 -4.4289346 -4.4289584 -4.4289055 -4.4288292][-4.4286466 -4.4286566 -4.4287367 -4.4288335 -4.4288917 -4.4288707 -4.4287748 -4.4286804 -4.4286714 -4.4287281 -4.4288235 -4.4289165 -4.4289384 -4.4288821 -4.4288111][-4.4287534 -4.428793 -4.428843 -4.428875 -4.4288621 -4.4287724 -4.4286094 -4.4284744 -4.4284935 -4.428616 -4.4287605 -4.4288669 -4.4288855 -4.4288335 -4.4287782][-4.4288177 -4.4288578 -4.4288745 -4.428853 -4.428772 -4.4285936 -4.4283357 -4.4281654 -4.4282522 -4.4284654 -4.4286647 -4.4287868 -4.4288139 -4.4287744 -4.4287353][-4.4288588 -4.4288888 -4.4288778 -4.4288163 -4.4286847 -4.4284463 -4.4281225 -4.4279509 -4.4281139 -4.42839 -4.4286103 -4.4287386 -4.4287748 -4.4287438 -4.4287124][-4.4288855 -4.4288936 -4.4288583 -4.4287863 -4.4286618 -4.4284544 -4.428205 -4.4281182 -4.4282751 -4.4284997 -4.4286747 -4.428771 -4.4287882 -4.4287505 -4.42872][-4.4288974 -4.4288888 -4.4288511 -4.4287906 -4.4287 -4.4285641 -4.4284325 -4.4284205 -4.4285369 -4.4286866 -4.4287992 -4.4288483 -4.4288278 -4.4287744 -4.4287443][-4.4289136 -4.4289031 -4.4288712 -4.42883 -4.4287753 -4.4286962 -4.4286356 -4.4286542 -4.4287419 -4.4288359 -4.4288974 -4.4289083 -4.4288721 -4.4288206 -4.4287944][-4.4289236 -4.42892 -4.4288979 -4.4288759 -4.4288487 -4.4288044 -4.4287729 -4.4287944 -4.4288564 -4.4289136 -4.4289436 -4.4289379 -4.4289074 -4.4288692 -4.4288445][-4.4289303 -4.4289231 -4.4289083 -4.4289002 -4.4288926 -4.4288716 -4.4288568 -4.42887 -4.4289021 -4.4289289 -4.42894 -4.4289322 -4.4289155 -4.4288936 -4.4288716][-4.4289427 -4.4289312 -4.4289193 -4.4289169 -4.4289203 -4.4289207 -4.4289212 -4.428926 -4.4289346 -4.428936 -4.4289317 -4.4289246 -4.4289188 -4.4289093 -4.4288926][-4.4289508 -4.4289408 -4.4289312 -4.4289303 -4.428937 -4.428947 -4.4289551 -4.4289575 -4.4289532 -4.4289432 -4.4289341 -4.4289308 -4.4289312 -4.4289279 -4.4289193]]...]
INFO - root - 2017-12-08 04:58:50.850534: step 1710, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.221 sec/batch; 20h:16m:38s remains)
INFO - root - 2017-12-08 04:58:53.108669: step 1720, loss = 2.28, batch loss = 2.23 (35.2 examples/sec; 0.227 sec/batch; 20h:51m:53s remains)
INFO - root - 2017-12-08 04:58:55.343161: step 1730, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 20h:28m:38s remains)
INFO - root - 2017-12-08 04:58:57.576129: step 1740, loss = 2.28, batch loss = 2.23 (35.6 examples/sec; 0.225 sec/batch; 20h:39m:34s remains)
INFO - root - 2017-12-08 04:58:59.808144: step 1750, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 20h:16m:34s remains)
INFO - root - 2017-12-08 04:59:02.044431: step 1760, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 20h:29m:38s remains)
INFO - root - 2017-12-08 04:59:04.295450: step 1770, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.225 sec/batch; 20h:41m:53s remains)
INFO - root - 2017-12-08 04:59:06.522296: step 1780, loss = 2.28, batch loss = 2.23 (35.8 examples/sec; 0.224 sec/batch; 20h:32m:55s remains)
INFO - root - 2017-12-08 04:59:08.780669: step 1790, loss = 2.28, batch loss = 2.23 (36.8 examples/sec; 0.217 sec/batch; 19h:57m:14s remains)
INFO - root - 2017-12-08 04:59:11.012428: step 1800, loss = 2.28, batch loss = 2.23 (37.2 examples/sec; 0.215 sec/batch; 19h:45m:26s remains)
2017-12-08 04:59:11.307939: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4287667 -4.4287443 -4.4287295 -4.4287171 -4.4287167 -4.4287391 -4.4287663 -4.4287839 -4.428803 -4.4288263 -4.4288363 -4.428833 -4.42883 -4.4288225 -4.4288039][-4.4287214 -4.4286871 -4.4286675 -4.4286585 -4.4286742 -4.4287138 -4.4287486 -4.4287686 -4.4287882 -4.4288044 -4.428803 -4.4287968 -4.4287925 -4.4287882 -4.4287772][-4.42867 -4.42861 -4.4285746 -4.4285693 -4.428597 -4.4286537 -4.4287014 -4.4287324 -4.4287605 -4.4287691 -4.4287581 -4.4287462 -4.4287415 -4.4287481 -4.4287543][-4.4286571 -4.4286003 -4.4285564 -4.4285364 -4.4285522 -4.4286065 -4.4286618 -4.4287109 -4.4287519 -4.4287496 -4.4287257 -4.4287066 -4.4286962 -4.4287109 -4.4287381][-4.4287095 -4.4286695 -4.4286313 -4.4285841 -4.428565 -4.4285865 -4.4286318 -4.4286942 -4.4287481 -4.4287477 -4.4287176 -4.4286933 -4.4286695 -4.4286661 -4.4286952][-4.4287248 -4.4287124 -4.4286962 -4.4286351 -4.4285688 -4.4285331 -4.4285541 -4.428627 -4.4287081 -4.4287381 -4.4287138 -4.4286761 -4.4286318 -4.4285908 -4.4286013][-4.4286876 -4.4287076 -4.4287186 -4.4286709 -4.4285736 -4.42847 -4.428443 -4.4285131 -4.428628 -4.4287043 -4.4286933 -4.4286356 -4.428556 -4.4284658 -4.4284482][-4.4286404 -4.4286852 -4.4287133 -4.4286885 -4.4285679 -4.4283919 -4.4282956 -4.4283509 -4.4285078 -4.4286323 -4.4286447 -4.4285812 -4.4284735 -4.4283271 -4.4282713][-4.4285727 -4.428628 -4.4286652 -4.4286551 -4.428534 -4.4283023 -4.4281182 -4.4281607 -4.4283752 -4.4285493 -4.4285979 -4.4285555 -4.4284415 -4.4282732 -4.42819][-4.4285212 -4.4285746 -4.4286156 -4.4286156 -4.428525 -4.4283156 -4.4280906 -4.4280839 -4.4283023 -4.4284983 -4.4285784 -4.42857 -4.4285011 -4.4283752 -4.428298][-4.4285274 -4.4285703 -4.4286146 -4.4286318 -4.4285994 -4.4284945 -4.4283462 -4.4282885 -4.4284015 -4.4285483 -4.4286275 -4.4286442 -4.42863 -4.4285617 -4.4284954][-4.4285917 -4.4286036 -4.4286408 -4.428668 -4.4286809 -4.4286771 -4.4286351 -4.4285846 -4.4286032 -4.4286675 -4.4287114 -4.4287214 -4.4287291 -4.4286976 -4.4286475][-4.4287138 -4.4286976 -4.42871 -4.4287271 -4.4287419 -4.428781 -4.4288 -4.4287825 -4.4287748 -4.4287848 -4.428793 -4.42879 -4.4288006 -4.4287949 -4.428761][-4.4288526 -4.4288297 -4.4288225 -4.4288244 -4.428833 -4.4288697 -4.4288969 -4.4289 -4.4288936 -4.4288912 -4.4288912 -4.4288816 -4.4288864 -4.4288921 -4.4288731][-4.428936 -4.4289217 -4.4289122 -4.4289045 -4.4289064 -4.4289308 -4.4289489 -4.4289565 -4.4289556 -4.4289556 -4.4289565 -4.4289513 -4.4289541 -4.4289603 -4.4289503]]...]
INFO - root - 2017-12-08 04:59:13.525923: step 1810, loss = 2.28, batch loss = 2.23 (37.4 examples/sec; 0.214 sec/batch; 19h:40m:05s remains)
INFO - root - 2017-12-08 04:59:15.765276: step 1820, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.221 sec/batch; 20h:20m:27s remains)
INFO - root - 2017-12-08 04:59:17.991711: step 1830, loss = 2.28, batch loss = 2.23 (36.9 examples/sec; 0.217 sec/batch; 19h:54m:45s remains)
INFO - root - 2017-12-08 04:59:20.266197: step 1840, loss = 2.28, batch loss = 2.23 (35.2 examples/sec; 0.227 sec/batch; 20h:52m:58s remains)
INFO - root - 2017-12-08 04:59:22.506063: step 1850, loss = 2.28, batch loss = 2.23 (34.0 examples/sec; 0.235 sec/batch; 21h:36m:42s remains)
INFO - root - 2017-12-08 04:59:24.748070: step 1860, loss = 2.28, batch loss = 2.23 (35.3 examples/sec; 0.227 sec/batch; 20h:49m:04s remains)
INFO - root - 2017-12-08 04:59:26.984562: step 1870, loss = 2.28, batch loss = 2.23 (35.8 examples/sec; 0.224 sec/batch; 20h:32m:28s remains)
INFO - root - 2017-12-08 04:59:29.204086: step 1880, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 20h:11m:10s remains)
INFO - root - 2017-12-08 04:59:31.441975: step 1890, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 20h:27m:28s remains)
INFO - root - 2017-12-08 04:59:33.702228: step 1900, loss = 2.28, batch loss = 2.23 (35.7 examples/sec; 0.224 sec/batch; 20h:34m:45s remains)
2017-12-08 04:59:33.998526: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4287634 -4.4287267 -4.4287434 -4.4287839 -4.4288063 -4.4288249 -4.4288507 -4.4288759 -4.4288979 -4.4288974 -4.4288583 -4.4288197 -4.4287815 -4.4287748 -4.4287939][-4.4287119 -4.4286356 -4.42863 -4.4286861 -4.4287477 -4.428792 -4.4288125 -4.42884 -4.4288726 -4.4288793 -4.428843 -4.4288111 -4.4287777 -4.428772 -4.4287777][-4.4286604 -4.4285612 -4.4285226 -4.4285722 -4.4286647 -4.4287415 -4.4287653 -4.428793 -4.4288254 -4.4288349 -4.4288092 -4.4287996 -4.4287834 -4.4287763 -4.4287682][-4.4286389 -4.428524 -4.4284492 -4.4284673 -4.4285622 -4.4286618 -4.4287119 -4.4287481 -4.4287825 -4.4287977 -4.4287872 -4.4287949 -4.4287944 -4.428792 -4.4287705][-4.4286466 -4.4285378 -4.4284439 -4.4284215 -4.4284873 -4.4285774 -4.4286423 -4.4286962 -4.4287591 -4.4287915 -4.4287906 -4.4288006 -4.4287977 -4.42879 -4.428761][-4.4286447 -4.4285555 -4.4284596 -4.4284005 -4.4284205 -4.4284749 -4.4285345 -4.4286041 -4.4286871 -4.4287367 -4.4287419 -4.4287572 -4.4287539 -4.4287457 -4.4287286][-4.428607 -4.4285369 -4.42845 -4.4283757 -4.4283586 -4.42838 -4.4284296 -4.4285045 -4.4286 -4.4286623 -4.428659 -4.4286704 -4.4286737 -4.4286876 -4.4286919][-4.4285421 -4.4284906 -4.4284267 -4.4283595 -4.4283233 -4.4283133 -4.4283404 -4.428401 -4.4284978 -4.4285669 -4.4285703 -4.4285846 -4.4286122 -4.428658 -4.428678][-4.4284792 -4.428431 -4.4283795 -4.4283247 -4.428288 -4.428267 -4.4282751 -4.4283185 -4.4283957 -4.4284611 -4.4284825 -4.4285216 -4.4285727 -4.4286346 -4.4286661][-4.428441 -4.4284015 -4.428359 -4.4283166 -4.428287 -4.4282656 -4.4282618 -4.4282827 -4.4283338 -4.4283915 -4.4284282 -4.4284873 -4.4285517 -4.4286237 -4.4286637][-4.4284797 -4.4284482 -4.4284153 -4.428381 -4.4283552 -4.4283333 -4.4283223 -4.4283276 -4.4283514 -4.4283872 -4.4284148 -4.4284744 -4.428544 -4.4286213 -4.4286642][-4.4285951 -4.4285693 -4.4285431 -4.4285169 -4.4284964 -4.4284797 -4.4284678 -4.4284644 -4.4284697 -4.4284821 -4.4284844 -4.4285197 -4.4285746 -4.4286413 -4.4286809][-4.4287605 -4.4287429 -4.4287238 -4.4287033 -4.4286871 -4.4286757 -4.4286666 -4.4286613 -4.4286594 -4.4286575 -4.4286394 -4.4286432 -4.4286685 -4.4287081 -4.4287357][-4.4288964 -4.4288883 -4.4288774 -4.4288659 -4.4288564 -4.4288487 -4.4288406 -4.4288359 -4.428833 -4.4288263 -4.4288025 -4.4287896 -4.42879 -4.4288034 -4.4288177][-4.4289732 -4.4289718 -4.4289665 -4.4289618 -4.4289594 -4.4289575 -4.4289556 -4.4289556 -4.4289551 -4.42895 -4.42893 -4.4289131 -4.4289036 -4.4289031 -4.4289074]]...]
INFO - root - 2017-12-08 04:59:36.224185: step 1910, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.225 sec/batch; 20h:40m:08s remains)
INFO - root - 2017-12-08 04:59:38.453726: step 1920, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 20h:26m:42s remains)
INFO - root - 2017-12-08 04:59:40.729709: step 1930, loss = 2.28, batch loss = 2.23 (35.2 examples/sec; 0.227 sec/batch; 20h:51m:41s remains)
INFO - root - 2017-12-08 04:59:42.968697: step 1940, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 20h:09m:25s remains)
INFO - root - 2017-12-08 04:59:45.190801: step 1950, loss = 2.28, batch loss = 2.23 (37.4 examples/sec; 0.214 sec/batch; 19h:38m:10s remains)
INFO - root - 2017-12-08 04:59:47.424603: step 1960, loss = 2.28, batch loss = 2.23 (37.0 examples/sec; 0.216 sec/batch; 19h:51m:47s remains)
INFO - root - 2017-12-08 04:59:49.662651: step 1970, loss = 2.28, batch loss = 2.23 (35.6 examples/sec; 0.225 sec/batch; 20h:39m:13s remains)
INFO - root - 2017-12-08 04:59:51.912816: step 1980, loss = 2.28, batch loss = 2.23 (35.1 examples/sec; 0.228 sec/batch; 20h:54m:30s remains)
INFO - root - 2017-12-08 04:59:54.153632: step 1990, loss = 2.28, batch loss = 2.23 (35.1 examples/sec; 0.228 sec/batch; 20h:55m:25s remains)
INFO - root - 2017-12-08 04:59:56.400478: step 2000, loss = 2.28, batch loss = 2.23 (36.0 examples/sec; 0.222 sec/batch; 20h:24m:14s remains)
2017-12-08 04:59:56.664209: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4290276 -4.4290247 -4.4290223 -4.4290228 -4.4290209 -4.4290223 -4.4290261 -4.429028 -4.4290242 -4.4290166 -4.4290137 -4.4290166 -4.429019 -4.4290261 -4.429029][-4.4290142 -4.4290023 -4.4289932 -4.4289889 -4.4289837 -4.428987 -4.4289894 -4.4289827 -4.4289665 -4.4289474 -4.4289365 -4.4289389 -4.4289508 -4.4289761 -4.4289961][-4.4289937 -4.4289722 -4.4289546 -4.4289422 -4.4289312 -4.4289341 -4.4289312 -4.42891 -4.4288745 -4.428833 -4.4288044 -4.4288039 -4.4288325 -4.428884 -4.42893][-4.4289665 -4.4289327 -4.4289036 -4.4288812 -4.4288626 -4.4288583 -4.4288478 -4.4288063 -4.4287457 -4.4286714 -4.428618 -4.4286137 -4.4286623 -4.4287524 -4.4288368][-4.4289269 -4.4288807 -4.4288387 -4.4288087 -4.4287848 -4.4287729 -4.4287505 -4.428688 -4.4286056 -4.4285073 -4.4284415 -4.428442 -4.4285111 -4.428638 -4.4287581][-4.4288578 -4.4287949 -4.4287419 -4.4287057 -4.4286838 -4.4286723 -4.4286404 -4.4285674 -4.428484 -4.4283991 -4.4283562 -4.4283786 -4.428463 -4.4286032 -4.428731][-4.4287686 -4.4286833 -4.428607 -4.4285531 -4.4285274 -4.4285355 -4.4285097 -4.4284372 -4.4283829 -4.4283547 -4.4283605 -4.4284115 -4.4285054 -4.4286404 -4.42876][-4.4286661 -4.4285445 -4.42842 -4.4283428 -4.4283366 -4.4283872 -4.4283929 -4.4283476 -4.4283519 -4.4283876 -4.4284296 -4.4284978 -4.4285913 -4.4287133 -4.4288173][-4.4285822 -4.4284234 -4.4282517 -4.4281669 -4.428206 -4.4283195 -4.4283733 -4.4283772 -4.4284225 -4.4284825 -4.4285412 -4.428618 -4.4287004 -4.4287982 -4.4288764][-4.4285774 -4.4284263 -4.4282751 -4.4282146 -4.4282784 -4.4284077 -4.4284883 -4.428525 -4.4285779 -4.4286284 -4.4286814 -4.4287462 -4.4288092 -4.4288769 -4.4289265][-4.4286776 -4.4285793 -4.4284968 -4.4284678 -4.4285145 -4.4286127 -4.4286857 -4.4287186 -4.4287548 -4.4287853 -4.4288197 -4.4288616 -4.4289031 -4.4289441 -4.4289689][-4.4288116 -4.428762 -4.4287338 -4.4287353 -4.4287663 -4.4288249 -4.4288697 -4.4288831 -4.4288945 -4.4289031 -4.4289193 -4.4289465 -4.4289722 -4.4289932 -4.4290004][-4.4289174 -4.4288955 -4.4288955 -4.4289088 -4.4289289 -4.4289613 -4.4289827 -4.4289832 -4.4289827 -4.4289804 -4.4289861 -4.429 -4.4290128 -4.4290204 -4.4290209][-4.4289808 -4.4289722 -4.4289775 -4.428988 -4.4290013 -4.4290195 -4.4290309 -4.4290314 -4.429028 -4.4290247 -4.4290261 -4.4290309 -4.4290342 -4.4290352 -4.4290328][-4.4290133 -4.4290133 -4.4290171 -4.4290223 -4.4290314 -4.4290423 -4.4290466 -4.4290462 -4.4290428 -4.4290404 -4.4290404 -4.4290409 -4.4290414 -4.4290395 -4.4290366]]...]
INFO - root - 2017-12-08 04:59:58.921406: step 2010, loss = 2.28, batch loss = 2.23 (35.0 examples/sec; 0.229 sec/batch; 20h:59m:53s remains)
INFO - root - 2017-12-08 05:00:01.151676: step 2020, loss = 2.28, batch loss = 2.23 (36.7 examples/sec; 0.218 sec/batch; 20h:01m:52s remains)
INFO - root - 2017-12-08 05:00:03.384701: step 2030, loss = 2.28, batch loss = 2.23 (34.5 examples/sec; 0.232 sec/batch; 21h:18m:43s remains)
INFO - root - 2017-12-08 05:00:05.616491: step 2040, loss = 2.28, batch loss = 2.23 (36.7 examples/sec; 0.218 sec/batch; 20h:00m:25s remains)
INFO - root - 2017-12-08 05:00:07.860284: step 2050, loss = 2.28, batch loss = 2.23 (36.7 examples/sec; 0.218 sec/batch; 20h:01m:38s remains)
INFO - root - 2017-12-08 05:00:10.096978: step 2060, loss = 2.28, batch loss = 2.23 (35.6 examples/sec; 0.225 sec/batch; 20h:36m:48s remains)
INFO - root - 2017-12-08 05:00:12.381214: step 2070, loss = 2.28, batch loss = 2.23 (34.5 examples/sec; 0.232 sec/batch; 21h:17m:08s remains)
INFO - root - 2017-12-08 05:00:14.605469: step 2080, loss = 2.28, batch loss = 2.23 (36.7 examples/sec; 0.218 sec/batch; 20h:00m:09s remains)
INFO - root - 2017-12-08 05:00:16.849226: step 2090, loss = 2.28, batch loss = 2.23 (36.7 examples/sec; 0.218 sec/batch; 20h:00m:14s remains)
INFO - root - 2017-12-08 05:00:19.083477: step 2100, loss = 2.28, batch loss = 2.23 (36.0 examples/sec; 0.222 sec/batch; 20h:23m:10s remains)
2017-12-08 05:00:19.451023: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4286079 -4.4286256 -4.4286613 -4.428709 -4.4287271 -4.4287381 -4.4287663 -4.4287963 -4.428822 -4.4288597 -4.4289036 -4.4289379 -4.4289618 -4.4289718 -4.428957][-4.4286304 -4.4286575 -4.4286938 -4.4287333 -4.4287453 -4.42876 -4.4287915 -4.4288292 -4.42887 -4.4289184 -4.4289584 -4.4289775 -4.4289894 -4.4289918 -4.4289722][-4.4286571 -4.4286747 -4.4287057 -4.4287376 -4.4287481 -4.4287629 -4.4287915 -4.4288259 -4.4288692 -4.4289227 -4.4289641 -4.4289813 -4.4289961 -4.428997 -4.4289718][-4.4286842 -4.4286847 -4.4287181 -4.4287515 -4.4287519 -4.4287424 -4.4287477 -4.4287667 -4.4288135 -4.4288812 -4.4289355 -4.4289603 -4.4289808 -4.4289789 -4.4289532][-4.4287109 -4.4287024 -4.4287305 -4.4287562 -4.4287314 -4.4286761 -4.4286456 -4.4286566 -4.4287157 -4.4288044 -4.4288845 -4.4289284 -4.4289608 -4.4289613 -4.4289393][-4.42875 -4.4287424 -4.4287581 -4.4287615 -4.4287043 -4.42859 -4.4284997 -4.4284925 -4.4285755 -4.4286976 -4.4288087 -4.42888 -4.4289336 -4.4289494 -4.4289379][-4.428792 -4.4287977 -4.4288058 -4.4287882 -4.4287095 -4.4285555 -4.4284005 -4.4283481 -4.4284425 -4.4286022 -4.4287415 -4.4288416 -4.4289122 -4.4289432 -4.4289412][-4.4288263 -4.4288507 -4.4288554 -4.4288273 -4.4287438 -4.4285908 -4.428411 -4.4283257 -4.4283996 -4.4285612 -4.4287043 -4.4288197 -4.4288993 -4.4289327 -4.4289293][-4.4288368 -4.4288688 -4.4288673 -4.4288344 -4.4287586 -4.4286284 -4.4284596 -4.4283667 -4.4284096 -4.4285536 -4.4286909 -4.42881 -4.4288936 -4.4289212 -4.4289112][-4.4288244 -4.4288554 -4.4288445 -4.4288106 -4.42875 -4.4286466 -4.4285059 -4.4284234 -4.4284477 -4.4285688 -4.4286885 -4.4288034 -4.42889 -4.4289193 -4.4289026][-4.4288158 -4.4288373 -4.4288239 -4.4287968 -4.4287496 -4.4286652 -4.4285464 -4.4284797 -4.4285059 -4.4286079 -4.4287062 -4.4288039 -4.42888 -4.4289069 -4.4288921][-4.4288373 -4.4288421 -4.4288182 -4.4287963 -4.4287634 -4.4287009 -4.4286079 -4.4285555 -4.4285841 -4.4286714 -4.4287491 -4.4288206 -4.42887 -4.4288807 -4.428874][-4.4288788 -4.42887 -4.4288363 -4.4288073 -4.4287791 -4.4287353 -4.4286723 -4.4286418 -4.4286742 -4.4287467 -4.4287992 -4.4288354 -4.4288411 -4.428823 -4.4288206][-4.428915 -4.4289088 -4.428875 -4.42884 -4.4288135 -4.4287839 -4.4287477 -4.4287353 -4.4287672 -4.4288244 -4.4288554 -4.4288568 -4.4288106 -4.4287496 -4.4287386][-4.4289141 -4.428916 -4.4288983 -4.4288764 -4.4288583 -4.428843 -4.4288268 -4.4288278 -4.4288554 -4.4288945 -4.4289074 -4.4288826 -4.4288044 -4.4287086 -4.428678]]...]
INFO - root - 2017-12-08 05:00:21.672152: step 2110, loss = 2.28, batch loss = 2.23 (34.7 examples/sec; 0.230 sec/batch; 21h:08m:30s remains)
INFO - root - 2017-12-08 05:00:23.936615: step 2120, loss = 2.28, batch loss = 2.23 (35.1 examples/sec; 0.228 sec/batch; 20h:53m:52s remains)
INFO - root - 2017-12-08 05:00:26.177197: step 2130, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 20h:26m:00s remains)
INFO - root - 2017-12-08 05:00:28.421258: step 2140, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 20h:10m:04s remains)
INFO - root - 2017-12-08 05:00:30.636904: step 2150, loss = 2.28, batch loss = 2.23 (36.7 examples/sec; 0.218 sec/batch; 20h:00m:05s remains)
INFO - root - 2017-12-08 05:00:32.857539: step 2160, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 20h:16m:25s remains)
INFO - root - 2017-12-08 05:00:35.078365: step 2170, loss = 2.28, batch loss = 2.23 (36.8 examples/sec; 0.217 sec/batch; 19h:56m:02s remains)
INFO - root - 2017-12-08 05:00:37.303412: step 2180, loss = 2.28, batch loss = 2.23 (36.9 examples/sec; 0.217 sec/batch; 19h:52m:34s remains)
INFO - root - 2017-12-08 05:00:39.567673: step 2190, loss = 2.28, batch loss = 2.23 (35.7 examples/sec; 0.224 sec/batch; 20h:34m:21s remains)
INFO - root - 2017-12-08 05:00:41.768311: step 2200, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.226 sec/batch; 20h:41m:23s remains)
2017-12-08 05:00:42.061559: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4286809 -4.4287105 -4.4287162 -4.4287205 -4.4287257 -4.4287329 -4.4287419 -4.428751 -4.4287577 -4.4287581 -4.4287548 -4.4287543 -4.4287558 -4.4287529 -4.4287496][-4.4286981 -4.4287305 -4.4287372 -4.428741 -4.4287443 -4.4287505 -4.4287586 -4.4287658 -4.42877 -4.428772 -4.4287715 -4.4287715 -4.4287705 -4.4287639 -4.4287543][-4.4287863 -4.4288216 -4.4288297 -4.4288282 -4.4288235 -4.4288206 -4.42882 -4.4288192 -4.4288192 -4.4288244 -4.4288297 -4.4288349 -4.4288383 -4.4288359 -4.4288273][-4.428864 -4.4288874 -4.4288855 -4.428874 -4.4288611 -4.4288483 -4.4288383 -4.4288282 -4.428823 -4.428833 -4.4288464 -4.4288554 -4.4288635 -4.4288707 -4.428875][-4.4288697 -4.4288769 -4.4288611 -4.4288383 -4.4288139 -4.4287868 -4.42876 -4.4287357 -4.4287271 -4.4287515 -4.4287815 -4.4287982 -4.4288125 -4.4288306 -4.4288535][-4.4287982 -4.4287829 -4.4287467 -4.4287038 -4.4286561 -4.4286046 -4.428556 -4.4285107 -4.4285069 -4.42856 -4.428618 -4.4286523 -4.4286785 -4.4287124 -4.4287572][-4.4287033 -4.4286656 -4.4286127 -4.4285517 -4.4284778 -4.4283962 -4.428319 -4.4282508 -4.428256 -4.4283381 -4.4284225 -4.4284778 -4.428524 -4.4285746 -4.4286366][-4.4286828 -4.4286389 -4.42859 -4.4285307 -4.4284539 -4.428369 -4.4282918 -4.428226 -4.4282331 -4.428309 -4.4283838 -4.4284339 -4.4284773 -4.4285259 -4.4285913][-4.4287567 -4.4287286 -4.4287024 -4.4286695 -4.4286213 -4.4285655 -4.4285126 -4.4284687 -4.4284673 -4.4285064 -4.4285445 -4.4285674 -4.4285851 -4.4286103 -4.4286575][-4.4288726 -4.4288726 -4.42887 -4.4288611 -4.4288368 -4.428802 -4.4287677 -4.4287405 -4.4287367 -4.4287519 -4.4287658 -4.428772 -4.4287744 -4.4287825 -4.4288044][-4.4289427 -4.4289565 -4.428967 -4.428968 -4.4289556 -4.4289346 -4.4289155 -4.4289017 -4.4289021 -4.4289117 -4.4289193 -4.4289227 -4.4289241 -4.4289293 -4.4289341][-4.4289618 -4.4289708 -4.428977 -4.4289765 -4.4289703 -4.4289603 -4.4289513 -4.4289446 -4.428946 -4.4289546 -4.4289627 -4.428967 -4.4289713 -4.4289765 -4.4289784][-4.4289775 -4.4289789 -4.4289789 -4.428977 -4.4289727 -4.4289656 -4.42896 -4.428956 -4.428956 -4.4289613 -4.4289684 -4.4289713 -4.4289751 -4.4289813 -4.4289875][-4.4290037 -4.4290056 -4.4290042 -4.4290004 -4.4289956 -4.4289904 -4.4289856 -4.4289837 -4.4289832 -4.4289837 -4.4289842 -4.4289818 -4.4289823 -4.4289875 -4.4289961][-4.4290242 -4.4290314 -4.4290309 -4.4290266 -4.4290185 -4.4290061 -4.4289923 -4.4289851 -4.4289784 -4.4289713 -4.4289637 -4.4289575 -4.4289556 -4.4289632 -4.428977]]...]
INFO - root - 2017-12-08 05:00:44.285520: step 2210, loss = 2.28, batch loss = 2.23 (36.9 examples/sec; 0.217 sec/batch; 19h:53m:47s remains)
INFO - root - 2017-12-08 05:00:46.522343: step 2220, loss = 2.28, batch loss = 2.23 (36.7 examples/sec; 0.218 sec/batch; 19h:58m:33s remains)
INFO - root - 2017-12-08 05:00:48.747712: step 2230, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 20h:09m:47s remains)
INFO - root - 2017-12-08 05:00:50.992637: step 2240, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 20h:10m:30s remains)
INFO - root - 2017-12-08 05:00:53.233157: step 2250, loss = 2.28, batch loss = 2.23 (36.9 examples/sec; 0.217 sec/batch; 19h:52m:34s remains)
INFO - root - 2017-12-08 05:00:55.508104: step 2260, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.221 sec/batch; 20h:18m:16s remains)
INFO - root - 2017-12-08 05:00:57.768122: step 2270, loss = 2.28, batch loss = 2.23 (35.3 examples/sec; 0.226 sec/batch; 20h:45m:57s remains)
INFO - root - 2017-12-08 05:01:00.011254: step 2280, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.221 sec/batch; 20h:13m:51s remains)
INFO - root - 2017-12-08 05:01:02.242820: step 2290, loss = 2.28, batch loss = 2.23 (34.8 examples/sec; 0.230 sec/batch; 21h:04m:11s remains)
INFO - root - 2017-12-08 05:01:04.484242: step 2300, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 20h:16m:32s remains)
2017-12-08 05:01:04.762778: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289274 -4.4289107 -4.4288912 -4.4288797 -4.4288769 -4.4288774 -4.4288774 -4.4288778 -4.42888 -4.4288912 -4.4289122 -4.4289336 -4.4289541 -4.4289722 -4.4289823][-4.4289122 -4.4288855 -4.4288635 -4.4288559 -4.4288621 -4.4288721 -4.4288745 -4.4288712 -4.4288621 -4.4288654 -4.4288869 -4.4289079 -4.42893 -4.4289532 -4.4289665][-4.4289083 -4.428874 -4.4288483 -4.4288354 -4.4288392 -4.4288449 -4.428833 -4.4288116 -4.4287844 -4.4287882 -4.4288197 -4.4288468 -4.4288783 -4.4289122 -4.4289293][-4.4289017 -4.4288568 -4.4288154 -4.428781 -4.4287724 -4.4287705 -4.4287415 -4.4286971 -4.4286489 -4.4286642 -4.4287119 -4.4287491 -4.42879 -4.4288383 -4.4288673][-4.4288716 -4.4288125 -4.4287453 -4.4286709 -4.4286213 -4.4285965 -4.4285545 -4.4285011 -4.4284439 -4.4284868 -4.4285679 -4.4286256 -4.42868 -4.4287477 -4.4287972][-4.4288249 -4.4287424 -4.428638 -4.4285154 -4.4284 -4.4283209 -4.4282618 -4.4282155 -4.4281688 -4.4282584 -4.4283943 -4.4284863 -4.4285636 -4.4286561 -4.4287286][-4.4287534 -4.4286385 -4.428493 -4.4283233 -4.42814 -4.42799 -4.42791 -4.4278913 -4.4278731 -4.4280281 -4.4282413 -4.4283848 -4.428484 -4.4285889 -4.428678][-4.4287114 -4.4285789 -4.4284191 -4.4282365 -4.4280324 -4.4278564 -4.4277806 -4.4277945 -4.4277987 -4.4279757 -4.428216 -4.4283834 -4.4284906 -4.4285927 -4.4286785][-4.4287438 -4.4286385 -4.428515 -4.4283781 -4.4282274 -4.4281068 -4.428062 -4.4280572 -4.4280334 -4.4281507 -4.4283423 -4.4284863 -4.4285822 -4.4286671 -4.4287329][-4.4288077 -4.4287477 -4.428678 -4.428607 -4.4285274 -4.4284654 -4.4284406 -4.4284081 -4.4283519 -4.428401 -4.4285235 -4.4286275 -4.4287043 -4.4287677 -4.4288125][-4.428874 -4.4288473 -4.4288149 -4.4287858 -4.4287543 -4.428731 -4.4287248 -4.4286966 -4.4286394 -4.4286528 -4.428719 -4.4287763 -4.428823 -4.4288654 -4.4288936][-4.4289036 -4.4288907 -4.4288754 -4.428864 -4.4288597 -4.4288654 -4.4288769 -4.428865 -4.4288278 -4.4288282 -4.4288578 -4.4288793 -4.4289026 -4.4289284 -4.4289465][-4.4289041 -4.4288979 -4.4288926 -4.4288888 -4.4288945 -4.4289122 -4.4289322 -4.4289336 -4.4289179 -4.4289155 -4.4289279 -4.4289346 -4.4289489 -4.428968 -4.4289804][-4.4289103 -4.4289093 -4.4289103 -4.4289122 -4.4289174 -4.4289327 -4.4289503 -4.4289556 -4.4289508 -4.4289484 -4.4289536 -4.4289589 -4.4289708 -4.4289865 -4.4289975][-4.428946 -4.4289508 -4.4289565 -4.42896 -4.4289613 -4.428967 -4.4289756 -4.4289789 -4.4289761 -4.4289713 -4.4289722 -4.4289765 -4.4289861 -4.4289989 -4.4290094]]...]
INFO - root - 2017-12-08 05:01:06.974189: step 2310, loss = 2.28, batch loss = 2.23 (35.4 examples/sec; 0.226 sec/batch; 20h:42m:00s remains)
INFO - root - 2017-12-08 05:01:09.245084: step 2320, loss = 2.28, batch loss = 2.23 (32.0 examples/sec; 0.250 sec/batch; 22h:53m:47s remains)
INFO - root - 2017-12-08 05:01:11.499188: step 2330, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 20h:04m:27s remains)
INFO - root - 2017-12-08 05:01:13.728766: step 2340, loss = 2.28, batch loss = 2.23 (35.8 examples/sec; 0.224 sec/batch; 20h:31m:03s remains)
INFO - root - 2017-12-08 05:01:15.966729: step 2350, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 20h:27m:04s remains)
INFO - root - 2017-12-08 05:01:18.177875: step 2360, loss = 2.28, batch loss = 2.23 (37.6 examples/sec; 0.213 sec/batch; 19h:31m:18s remains)
INFO - root - 2017-12-08 05:01:20.418376: step 2370, loss = 2.28, batch loss = 2.23 (35.0 examples/sec; 0.228 sec/batch; 20h:56m:52s remains)
INFO - root - 2017-12-08 05:01:22.638150: step 2380, loss = 2.28, batch loss = 2.23 (35.0 examples/sec; 0.228 sec/batch; 20h:56m:43s remains)
INFO - root - 2017-12-08 05:01:24.896824: step 2390, loss = 2.28, batch loss = 2.23 (34.6 examples/sec; 0.231 sec/batch; 21h:11m:51s remains)
INFO - root - 2017-12-08 05:01:27.126389: step 2400, loss = 2.28, batch loss = 2.23 (37.2 examples/sec; 0.215 sec/batch; 19h:42m:45s remains)
2017-12-08 05:01:27.396685: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4287009 -4.4287105 -4.4287491 -4.4287786 -4.4288025 -4.4288349 -4.4288464 -4.428813 -4.4287577 -4.4287395 -4.4287539 -4.4287848 -4.4288187 -4.4288397 -4.4288669][-4.42867 -4.4287019 -4.4287524 -4.4287972 -4.4288306 -4.4288573 -4.4288545 -4.4288039 -4.4287372 -4.4287133 -4.4287271 -4.4287643 -4.4288111 -4.4288387 -4.428864][-4.4286423 -4.4286842 -4.4287434 -4.4288068 -4.4288492 -4.4288626 -4.428834 -4.4287653 -4.4286923 -4.4286637 -4.4286847 -4.4287338 -4.4287882 -4.4288177 -4.4288464][-4.4286742 -4.4287086 -4.4287634 -4.4288249 -4.4288497 -4.4288321 -4.4287791 -4.4287043 -4.428638 -4.4286213 -4.4286585 -4.4287238 -4.4287796 -4.4288087 -4.4288421][-4.4287329 -4.4287558 -4.4287934 -4.428823 -4.4288068 -4.4287519 -4.4286776 -4.4286027 -4.4285502 -4.428566 -4.4286351 -4.4287162 -4.42877 -4.4288034 -4.4288449][-4.4287529 -4.4287667 -4.4287739 -4.4287624 -4.4287057 -4.4286232 -4.4285278 -4.4284372 -4.4283857 -4.4284382 -4.4285455 -4.42865 -4.428721 -4.4287777 -4.4288335][-4.4287124 -4.4287138 -4.4286976 -4.4286575 -4.4285831 -4.4284911 -4.4283848 -4.42827 -4.4281931 -4.42826 -4.4283924 -4.4285235 -4.4286313 -4.4287229 -4.428803][-4.4286447 -4.4286432 -4.428627 -4.4285855 -4.4285293 -4.4284687 -4.4283862 -4.4282751 -4.4281797 -4.4282117 -4.428319 -4.4284496 -4.4285769 -4.4286852 -4.4287791][-4.4286304 -4.4286313 -4.4286227 -4.428607 -4.4285913 -4.4285765 -4.4285374 -4.42846 -4.4283776 -4.428371 -4.4284196 -4.4285116 -4.4286146 -4.4287028 -4.4287839][-4.4286842 -4.4286819 -4.4286833 -4.4286919 -4.4287047 -4.4287133 -4.4286976 -4.4286513 -4.4285917 -4.4285717 -4.4285827 -4.4286323 -4.42869 -4.4287462 -4.4288082][-4.4287887 -4.4287782 -4.4287806 -4.428792 -4.4288054 -4.4288135 -4.4288096 -4.4287896 -4.4287539 -4.4287376 -4.4287348 -4.4287543 -4.4287734 -4.4287982 -4.4288368][-4.4288816 -4.428874 -4.4288745 -4.42888 -4.4288874 -4.4288921 -4.4288926 -4.4288898 -4.4288735 -4.428863 -4.4288545 -4.4288568 -4.4288564 -4.4288592 -4.428875][-4.4289346 -4.42893 -4.4289308 -4.4289351 -4.4289393 -4.4289451 -4.4289503 -4.4289546 -4.428947 -4.42894 -4.4289312 -4.4289322 -4.4289289 -4.4289207 -4.4289207][-4.428966 -4.4289632 -4.4289641 -4.428966 -4.428968 -4.4289751 -4.4289827 -4.4289875 -4.4289837 -4.4289784 -4.4289737 -4.4289765 -4.4289765 -4.4289684 -4.4289622][-4.4289727 -4.4289703 -4.4289689 -4.4289689 -4.4289694 -4.4289737 -4.428978 -4.4289794 -4.428978 -4.428977 -4.428977 -4.4289818 -4.4289861 -4.4289846 -4.4289818]]...]
INFO - root - 2017-12-08 05:01:29.607732: step 2410, loss = 2.28, batch loss = 2.23 (34.9 examples/sec; 0.229 sec/batch; 20h:59m:47s remains)
INFO - root - 2017-12-08 05:01:31.848076: step 2420, loss = 2.28, batch loss = 2.23 (34.6 examples/sec; 0.231 sec/batch; 21h:13m:24s remains)
INFO - root - 2017-12-08 05:01:34.107099: step 2430, loss = 2.28, batch loss = 2.23 (33.9 examples/sec; 0.236 sec/batch; 21h:37m:39s remains)
INFO - root - 2017-12-08 05:01:36.336519: step 2440, loss = 2.28, batch loss = 2.23 (36.6 examples/sec; 0.219 sec/batch; 20h:03m:18s remains)
INFO - root - 2017-12-08 05:01:38.572902: step 2450, loss = 2.28, batch loss = 2.23 (35.0 examples/sec; 0.229 sec/batch; 20h:57m:34s remains)
INFO - root - 2017-12-08 05:01:40.846292: step 2460, loss = 2.28, batch loss = 2.23 (34.6 examples/sec; 0.231 sec/batch; 21h:13m:20s remains)
INFO - root - 2017-12-08 05:01:43.093872: step 2470, loss = 2.28, batch loss = 2.23 (35.4 examples/sec; 0.226 sec/batch; 20h:43m:21s remains)
INFO - root - 2017-12-08 05:01:45.316555: step 2480, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 20h:04m:40s remains)
INFO - root - 2017-12-08 05:01:47.628355: step 2490, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.220 sec/batch; 20h:11m:40s remains)
INFO - root - 2017-12-08 05:01:49.855434: step 2500, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 20h:05m:26s remains)
2017-12-08 05:01:50.145946: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289145 -4.4289088 -4.4288993 -4.4288898 -4.4288793 -4.4288712 -4.4288712 -4.4288921 -4.428915 -4.428925 -4.4289203 -4.4289055 -4.4288945 -4.4288945 -4.4289007][-4.4288988 -4.4288864 -4.4288664 -4.4288483 -4.4288297 -4.428813 -4.42881 -4.4288397 -4.4288745 -4.4288964 -4.4289017 -4.4288893 -4.4288726 -4.4288626 -4.4288616][-4.4288487 -4.4288244 -4.4287853 -4.4287596 -4.4287424 -4.428721 -4.4287186 -4.428761 -4.428812 -4.4288449 -4.4288621 -4.4288611 -4.4288464 -4.42883 -4.4288254][-4.4287868 -4.4287438 -4.428688 -4.4286604 -4.4286394 -4.428617 -4.4286089 -4.4286551 -4.4287205 -4.4287629 -4.4287934 -4.4288025 -4.428793 -4.4287744 -4.4287696][-4.4287615 -4.4287019 -4.4286323 -4.428596 -4.4285617 -4.4285212 -4.428493 -4.4285336 -4.428618 -4.4286785 -4.4287162 -4.4287333 -4.4287372 -4.4287324 -4.4287305][-4.4287319 -4.4286656 -4.4285884 -4.4285254 -4.4284663 -4.4283934 -4.4283218 -4.4283371 -4.4284587 -4.428565 -4.428628 -4.4286618 -4.4286823 -4.4287038 -4.4287195][-4.4287186 -4.4286623 -4.4285893 -4.4284921 -4.4283862 -4.428266 -4.4281397 -4.428113 -4.4282689 -4.4284358 -4.4285388 -4.4285932 -4.4286208 -4.4286561 -4.4286962][-4.4287453 -4.4286971 -4.4286275 -4.4285083 -4.4283814 -4.4282641 -4.4281425 -4.4281082 -4.4282479 -4.4284029 -4.428494 -4.4285436 -4.4285769 -4.4286222 -4.4286752][-4.42877 -4.4287333 -4.4286685 -4.4285631 -4.42845 -4.4283609 -4.42828 -4.428266 -4.4283695 -4.4284687 -4.4285259 -4.42855 -4.4285645 -4.4285965 -4.4286494][-4.4287648 -4.4287491 -4.4287052 -4.4286342 -4.4285464 -4.4284744 -4.4284058 -4.4283853 -4.428442 -4.4284973 -4.4285364 -4.4285555 -4.4285626 -4.4285793 -4.4286323][-4.4287343 -4.4287286 -4.4287133 -4.4286761 -4.4286261 -4.4285746 -4.4285212 -4.4284992 -4.4285197 -4.42854 -4.4285526 -4.42857 -4.4285884 -4.428607 -4.4286523][-4.4287357 -4.4287333 -4.4287367 -4.4287124 -4.4286842 -4.4286523 -4.4286165 -4.4285979 -4.428607 -4.4286046 -4.4285941 -4.4286118 -4.428647 -4.4286675 -4.4287052][-4.4287524 -4.4287586 -4.428771 -4.4287524 -4.4287343 -4.4287171 -4.4286947 -4.4286819 -4.4286904 -4.4286819 -4.4286647 -4.4286742 -4.4287052 -4.4287291 -4.4287653][-4.4287944 -4.4288068 -4.4288259 -4.4288177 -4.42881 -4.4288096 -4.4288 -4.4287953 -4.4288096 -4.4288039 -4.4287868 -4.4287896 -4.4288073 -4.42882 -4.4288435][-4.4288568 -4.42887 -4.4288883 -4.4288874 -4.4288874 -4.4288931 -4.4288907 -4.4288921 -4.4289036 -4.4289017 -4.4288921 -4.4288936 -4.4289026 -4.42891 -4.4289227]]...]
INFO - root - 2017-12-08 05:01:52.361288: step 2510, loss = 2.28, batch loss = 2.23 (36.6 examples/sec; 0.218 sec/batch; 20h:01m:06s remains)
INFO - root - 2017-12-08 05:01:54.607965: step 2520, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 20h:05m:52s remains)
INFO - root - 2017-12-08 05:01:56.830436: step 2530, loss = 2.28, batch loss = 2.23 (35.6 examples/sec; 0.224 sec/batch; 20h:34m:10s remains)
INFO - root - 2017-12-08 05:01:59.069016: step 2540, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.225 sec/batch; 20h:38m:49s remains)
INFO - root - 2017-12-08 05:02:01.316861: step 2550, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 20h:14m:28s remains)
INFO - root - 2017-12-08 05:02:03.646847: step 2560, loss = 2.28, batch loss = 2.23 (34.8 examples/sec; 0.230 sec/batch; 21h:03m:11s remains)
INFO - root - 2017-12-08 05:02:05.892363: step 2570, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 20h:24m:54s remains)
INFO - root - 2017-12-08 05:02:08.145503: step 2580, loss = 2.28, batch loss = 2.23 (32.6 examples/sec; 0.245 sec/batch; 22h:28m:52s remains)
INFO - root - 2017-12-08 05:02:10.402537: step 2590, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 20h:24m:04s remains)
INFO - root - 2017-12-08 05:02:12.629018: step 2600, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 20h:08m:35s remains)
2017-12-08 05:02:12.923090: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4287314 -4.4287276 -4.4287381 -4.4287491 -4.4287462 -4.4287438 -4.4287367 -4.4287252 -4.4287024 -4.4286737 -4.4286489 -4.42863 -4.4286275 -4.4286485 -4.4286828][-4.4287114 -4.4286923 -4.4286962 -4.4287114 -4.4287195 -4.4287338 -4.4287472 -4.4287505 -4.4287262 -4.4286766 -4.4286222 -4.4285827 -4.428576 -4.4285975 -4.4286389][-4.4286933 -4.4286785 -4.4286866 -4.4287109 -4.4287291 -4.4287491 -4.4287634 -4.428762 -4.4287329 -4.4286718 -4.4285979 -4.4285426 -4.4285216 -4.4285254 -4.4285574][-4.4286852 -4.4286866 -4.4287024 -4.4287305 -4.42875 -4.428762 -4.4287658 -4.4287543 -4.4287262 -4.428668 -4.4285917 -4.4285259 -4.4284811 -4.4284554 -4.4284697][-4.4286895 -4.4287057 -4.4287271 -4.4287529 -4.4287663 -4.4287624 -4.4287496 -4.4287248 -4.4286952 -4.4286513 -4.4285913 -4.4285264 -4.428462 -4.4284105 -4.4284048][-4.4286904 -4.4287148 -4.4287333 -4.4287543 -4.4287653 -4.4287515 -4.42872 -4.4286737 -4.4286361 -4.428606 -4.4285693 -4.4285164 -4.428453 -4.4283996 -4.4283962][-4.4286604 -4.4286933 -4.4287186 -4.4287429 -4.4287539 -4.4287338 -4.4286842 -4.4286141 -4.428566 -4.4285502 -4.4285388 -4.4285111 -4.4284725 -4.4284363 -4.4284444][-4.4286361 -4.428688 -4.4287305 -4.4287543 -4.4287558 -4.4287243 -4.4286585 -4.4285779 -4.4285312 -4.4285235 -4.4285264 -4.4285235 -4.4285121 -4.4284897 -4.4284973][-4.4286237 -4.4286942 -4.4287519 -4.4287753 -4.42877 -4.4287386 -4.4286685 -4.42859 -4.4285474 -4.428544 -4.4285531 -4.4285722 -4.4285855 -4.4285765 -4.4285817][-4.4286203 -4.4286952 -4.4287629 -4.4287906 -4.4287906 -4.4287658 -4.4287019 -4.4286318 -4.4285975 -4.4286051 -4.4286242 -4.4286442 -4.4286523 -4.4286442 -4.4286485][-4.4286513 -4.4287171 -4.4287806 -4.4288096 -4.4288173 -4.4287968 -4.4287405 -4.4286828 -4.4286633 -4.4286814 -4.4287028 -4.428709 -4.4286966 -4.4286814 -4.4286914][-4.4286909 -4.4287424 -4.4287968 -4.4288273 -4.4288425 -4.4288273 -4.4287796 -4.4287353 -4.4287291 -4.428751 -4.42877 -4.4287672 -4.4287448 -4.4287291 -4.42874][-4.4287319 -4.4287653 -4.4288106 -4.428844 -4.428864 -4.4288588 -4.4288306 -4.4288044 -4.4288044 -4.4288206 -4.4288306 -4.4288192 -4.4287944 -4.4287815 -4.4287882][-4.4287972 -4.4288082 -4.4288359 -4.428863 -4.428884 -4.4288883 -4.4288774 -4.4288654 -4.4288678 -4.4288764 -4.42888 -4.4288692 -4.4288516 -4.4288449 -4.4288483][-4.4288673 -4.4288621 -4.4288735 -4.4288869 -4.4288993 -4.4289055 -4.4289041 -4.4289012 -4.428905 -4.4289136 -4.4289188 -4.4289155 -4.428906 -4.4289031 -4.4289026]]...]
INFO - root - 2017-12-08 05:02:15.169745: step 2610, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 20h:09m:01s remains)
INFO - root - 2017-12-08 05:02:17.433191: step 2620, loss = 2.28, batch loss = 2.23 (34.9 examples/sec; 0.229 sec/batch; 20h:58m:44s remains)
INFO - root - 2017-12-08 05:02:19.676304: step 2630, loss = 2.28, batch loss = 2.23 (34.9 examples/sec; 0.229 sec/batch; 20h:59m:11s remains)
INFO - root - 2017-12-08 05:02:21.904169: step 2640, loss = 2.28, batch loss = 2.23 (35.1 examples/sec; 0.228 sec/batch; 20h:54m:14s remains)
INFO - root - 2017-12-08 05:02:24.180765: step 2650, loss = 2.28, batch loss = 2.23 (36.6 examples/sec; 0.218 sec/batch; 20h:00m:55s remains)
INFO - root - 2017-12-08 05:02:26.418972: step 2660, loss = 2.28, batch loss = 2.23 (36.9 examples/sec; 0.217 sec/batch; 19h:52m:47s remains)
INFO - root - 2017-12-08 05:02:28.655723: step 2670, loss = 2.28, batch loss = 2.23 (34.5 examples/sec; 0.232 sec/batch; 21h:13m:44s remains)
INFO - root - 2017-12-08 05:02:30.878957: step 2680, loss = 2.28, batch loss = 2.23 (34.8 examples/sec; 0.230 sec/batch; 21h:04m:17s remains)
INFO - root - 2017-12-08 05:02:33.109483: step 2690, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.225 sec/batch; 20h:37m:36s remains)
INFO - root - 2017-12-08 05:02:35.377506: step 2700, loss = 2.28, batch loss = 2.23 (33.2 examples/sec; 0.241 sec/batch; 22h:02m:49s remains)
2017-12-08 05:02:35.651847: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4287553 -4.4287958 -4.4288487 -4.4288907 -4.4289064 -4.4289103 -4.4289165 -4.4289255 -4.4289269 -4.4289222 -4.4289174 -4.4289136 -4.4289126 -4.4289126 -4.4289117][-4.4287667 -4.4288011 -4.4288445 -4.4288845 -4.4289026 -4.4289088 -4.4289174 -4.4289255 -4.428926 -4.4289207 -4.4289141 -4.4289074 -4.4289036 -4.4289021 -4.4289007][-4.4288225 -4.4288416 -4.4288573 -4.4288754 -4.4288816 -4.4288793 -4.4288859 -4.428894 -4.4288969 -4.4288974 -4.4288983 -4.4288988 -4.4289 -4.4288983 -4.428896][-4.4288611 -4.4288611 -4.4288545 -4.4288507 -4.428844 -4.4288316 -4.4288354 -4.4288492 -4.4288607 -4.4288712 -4.4288826 -4.4288917 -4.4288983 -4.4288964 -4.4288893][-4.4288363 -4.4288216 -4.4288006 -4.4287825 -4.4287696 -4.4287553 -4.4287581 -4.42878 -4.428802 -4.4288197 -4.4288378 -4.4288526 -4.4288654 -4.4288611 -4.4288449][-4.4287558 -4.4287353 -4.4287119 -4.4286933 -4.4286828 -4.4286728 -4.4286776 -4.4287105 -4.4287491 -4.4287772 -4.428802 -4.4288239 -4.4288406 -4.42883 -4.4287949][-4.4286618 -4.4286308 -4.4285984 -4.428576 -4.4285636 -4.4285507 -4.4285555 -4.4285941 -4.4286513 -4.4287086 -4.4287539 -4.4287906 -4.4288135 -4.4287996 -4.4287534][-4.4285569 -4.4285126 -4.4284668 -4.42844 -4.4284205 -4.4284 -4.4283957 -4.4284358 -4.4285178 -4.4286094 -4.4286757 -4.4287171 -4.4287353 -4.4287128 -4.4286671][-4.4284563 -4.4284053 -4.4283571 -4.4283328 -4.428308 -4.42827 -4.4282446 -4.4282742 -4.4283662 -4.4284811 -4.4285679 -4.428618 -4.428638 -4.4286175 -4.4285808][-4.42847 -4.428431 -4.4284039 -4.4284019 -4.4283891 -4.4283562 -4.4283266 -4.4283442 -4.4284205 -4.4285221 -4.4286041 -4.428647 -4.4286547 -4.4286304 -4.4285922][-4.4286027 -4.4285769 -4.4285603 -4.428565 -4.4285626 -4.428546 -4.4285316 -4.4285512 -4.4286032 -4.4286695 -4.4287224 -4.4287496 -4.4287457 -4.4287267 -4.4286966][-4.4287329 -4.4287162 -4.4286981 -4.4286933 -4.4286904 -4.4286819 -4.4286776 -4.4286942 -4.4287248 -4.4287596 -4.4287858 -4.4287949 -4.4287853 -4.428771 -4.4287505][-4.4287658 -4.4287529 -4.4287276 -4.4287076 -4.4287014 -4.4286981 -4.4286966 -4.4287057 -4.4287205 -4.4287395 -4.4287486 -4.4287453 -4.4287362 -4.4287324 -4.4287219][-4.4287319 -4.4287133 -4.4286766 -4.4286432 -4.4286346 -4.428638 -4.4286418 -4.4286442 -4.42865 -4.4286594 -4.4286613 -4.4286609 -4.4286661 -4.428679 -4.42869][-4.4287086 -4.4286718 -4.428627 -4.428596 -4.4285922 -4.4286032 -4.4286137 -4.4286137 -4.428618 -4.4286265 -4.4286313 -4.4286394 -4.4286585 -4.4286871 -4.4287128]]...]
INFO - root - 2017-12-08 05:02:37.892736: step 2710, loss = 2.28, batch loss = 2.23 (34.8 examples/sec; 0.230 sec/batch; 21h:02m:37s remains)
INFO - root - 2017-12-08 05:02:40.126841: step 2720, loss = 2.28, batch loss = 2.23 (33.9 examples/sec; 0.236 sec/batch; 21h:38m:05s remains)
INFO - root - 2017-12-08 05:02:42.359267: step 2730, loss = 2.28, batch loss = 2.23 (32.0 examples/sec; 0.250 sec/batch; 22h:54m:36s remains)
INFO - root - 2017-12-08 05:02:44.588015: step 2740, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 20h:13m:33s remains)
INFO - root - 2017-12-08 05:02:46.854644: step 2750, loss = 2.28, batch loss = 2.23 (35.0 examples/sec; 0.228 sec/batch; 20h:55m:42s remains)
INFO - root - 2017-12-08 05:02:49.104197: step 2760, loss = 2.28, batch loss = 2.23 (34.4 examples/sec; 0.232 sec/batch; 21h:16m:43s remains)
INFO - root - 2017-12-08 05:02:51.333971: step 2770, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.222 sec/batch; 20h:19m:16s remains)
INFO - root - 2017-12-08 05:02:53.572386: step 2780, loss = 2.28, batch loss = 2.23 (34.9 examples/sec; 0.230 sec/batch; 21h:01m:12s remains)
INFO - root - 2017-12-08 05:02:55.790163: step 2790, loss = 2.28, batch loss = 2.23 (36.6 examples/sec; 0.219 sec/batch; 20h:02m:30s remains)
INFO - root - 2017-12-08 05:02:58.029297: step 2800, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.225 sec/batch; 20h:37m:22s remains)
2017-12-08 05:02:58.322940: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4286432 -4.4286385 -4.428638 -4.4286456 -4.428658 -4.4286261 -4.4285479 -4.4284883 -4.428514 -4.4286137 -4.428731 -4.4288197 -4.4288859 -4.4289346 -4.4289551][-4.4286 -4.4285746 -4.4285684 -4.4285822 -4.4286056 -4.4285903 -4.4285288 -4.42848 -4.4285111 -4.4286156 -4.4287338 -4.4288177 -4.4288759 -4.4289207 -4.4289408][-4.4286313 -4.4285741 -4.4285421 -4.4285426 -4.4285707 -4.4285836 -4.4285522 -4.4285169 -4.4285474 -4.4286523 -4.4287591 -4.4288287 -4.4288778 -4.4289184 -4.4289365][-4.4286885 -4.4286108 -4.4285555 -4.428525 -4.4285254 -4.428544 -4.4285359 -4.4285145 -4.4285603 -4.4286814 -4.4287906 -4.4288487 -4.42889 -4.42893 -4.4289451][-4.4287348 -4.4286423 -4.428566 -4.4285 -4.4284549 -4.4284582 -4.4284549 -4.4284472 -4.4285121 -4.4286585 -4.428781 -4.4288373 -4.4288797 -4.428926 -4.4289474][-4.4287653 -4.4286709 -4.4285703 -4.4284582 -4.4283719 -4.4283581 -4.4283428 -4.4283381 -4.4284239 -4.4286027 -4.4287457 -4.4288096 -4.4288573 -4.4289145 -4.428946][-4.4287763 -4.4286828 -4.4285617 -4.4284129 -4.4282985 -4.4282765 -4.42825 -4.4282475 -4.4283538 -4.4285507 -4.4287076 -4.4287844 -4.4288421 -4.4289055 -4.4289427][-4.4287572 -4.4286685 -4.42855 -4.4283891 -4.428267 -4.4282451 -4.4282117 -4.4282126 -4.4283338 -4.4285293 -4.4286871 -4.4287753 -4.428843 -4.4289074 -4.4289451][-4.4287357 -4.4286532 -4.4285669 -4.428462 -4.4283743 -4.4283376 -4.4282918 -4.4282746 -4.4283714 -4.4285417 -4.4286904 -4.4287777 -4.4288483 -4.4289112 -4.4289455][-4.4287033 -4.4286351 -4.4285965 -4.428575 -4.4285445 -4.4285131 -4.4284563 -4.4284091 -4.4284587 -4.42859 -4.4287205 -4.4287972 -4.4288588 -4.4289145 -4.4289441][-4.4286542 -4.428597 -4.4285955 -4.4286242 -4.428628 -4.4286075 -4.4285588 -4.4285188 -4.42855 -4.4286518 -4.4287596 -4.4288216 -4.4288683 -4.4289179 -4.4289427][-4.4286056 -4.4285622 -4.4285784 -4.42861 -4.42862 -4.4286103 -4.4285836 -4.4285755 -4.4286132 -4.4286966 -4.4287829 -4.4288335 -4.4288778 -4.4289269 -4.4289479][-4.4286036 -4.4285812 -4.4286075 -4.4286284 -4.4286323 -4.4286203 -4.428607 -4.4286232 -4.4286637 -4.4287291 -4.4287977 -4.4288378 -4.4288816 -4.4289351 -4.4289565][-4.4286318 -4.428618 -4.4286613 -4.4286828 -4.4286733 -4.4286537 -4.4286413 -4.4286566 -4.4286919 -4.4287438 -4.4288006 -4.4288383 -4.4288821 -4.42894 -4.4289641][-4.4286432 -4.4286208 -4.4286618 -4.4286962 -4.4287 -4.4286909 -4.4286814 -4.4286971 -4.4287267 -4.4287648 -4.4288044 -4.4288373 -4.4288797 -4.4289403 -4.4289656]]...]
INFO - root - 2017-12-08 05:03:00.549927: step 2810, loss = 2.28, batch loss = 2.23 (35.1 examples/sec; 0.228 sec/batch; 20h:50m:38s remains)
INFO - root - 2017-12-08 05:03:02.803426: step 2820, loss = 2.28, batch loss = 2.23 (36.7 examples/sec; 0.218 sec/batch; 19h:58m:08s remains)
INFO - root - 2017-12-08 05:03:05.065551: step 2830, loss = 2.28, batch loss = 2.23 (35.6 examples/sec; 0.224 sec/batch; 20h:33m:18s remains)
INFO - root - 2017-12-08 05:03:07.302733: step 2840, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 20h:06m:03s remains)
INFO - root - 2017-12-08 05:03:09.557380: step 2850, loss = 2.28, batch loss = 2.23 (35.3 examples/sec; 0.227 sec/batch; 20h:44m:37s remains)
INFO - root - 2017-12-08 05:03:11.795233: step 2860, loss = 2.28, batch loss = 2.23 (37.0 examples/sec; 0.216 sec/batch; 19h:47m:03s remains)
INFO - root - 2017-12-08 05:03:14.062222: step 2870, loss = 2.28, batch loss = 2.23 (36.6 examples/sec; 0.218 sec/batch; 19h:59m:55s remains)
INFO - root - 2017-12-08 05:03:16.306761: step 2880, loss = 2.28, batch loss = 2.23 (35.3 examples/sec; 0.226 sec/batch; 20h:43m:39s remains)
INFO - root - 2017-12-08 05:03:18.542888: step 2890, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.222 sec/batch; 20h:16m:52s remains)
INFO - root - 2017-12-08 05:03:20.776388: step 2900, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.222 sec/batch; 20h:17m:51s remains)
2017-12-08 05:03:21.069924: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288883 -4.4289112 -4.4289241 -4.4289255 -4.428906 -4.4288578 -4.4287987 -4.4287577 -4.4287443 -4.4287672 -4.4288116 -4.428843 -4.4288435 -4.4288254 -4.4288068][-4.428916 -4.4289293 -4.4289269 -4.4289174 -4.4288926 -4.4288387 -4.4287767 -4.4287367 -4.42873 -4.4287667 -4.4288311 -4.4288797 -4.4288912 -4.428875 -4.4288473][-4.4289179 -4.4289112 -4.4288893 -4.4288759 -4.428865 -4.4288316 -4.4287829 -4.4287505 -4.4287472 -4.4287891 -4.428863 -4.4289207 -4.4289374 -4.4289212 -4.4288864][-4.4289074 -4.4288807 -4.4288459 -4.4288373 -4.4288511 -4.4288487 -4.4288192 -4.4287868 -4.428772 -4.4288073 -4.4288783 -4.4289331 -4.42895 -4.428937 -4.4289][-4.4289069 -4.4288659 -4.4288268 -4.4288268 -4.4288626 -4.4288855 -4.4288721 -4.4288354 -4.4288054 -4.4288225 -4.4288774 -4.42892 -4.42893 -4.428915 -4.4288797][-4.4289074 -4.4288678 -4.428833 -4.4288392 -4.4288754 -4.4289045 -4.4289012 -4.4288678 -4.4288306 -4.4288287 -4.4288621 -4.4288883 -4.4288864 -4.4288669 -4.4288359][-4.4289088 -4.4288807 -4.4288568 -4.4288621 -4.4288898 -4.4289126 -4.428915 -4.4288931 -4.4288568 -4.4288411 -4.4288554 -4.428864 -4.4288473 -4.4288182 -4.4287891][-4.4289069 -4.4288936 -4.4288836 -4.42889 -4.4289069 -4.42892 -4.428925 -4.4289131 -4.4288878 -4.4288721 -4.4288788 -4.4288764 -4.4288473 -4.4288063 -4.4287734][-4.4288979 -4.4289 -4.4289012 -4.428905 -4.4289074 -4.4289093 -4.4289155 -4.4289112 -4.4289 -4.4288969 -4.4289074 -4.428906 -4.4288735 -4.4288316 -4.4287982][-4.4288797 -4.4289021 -4.4289088 -4.4289002 -4.4288826 -4.428874 -4.4288793 -4.4288774 -4.4288745 -4.4288826 -4.4288979 -4.4288979 -4.4288721 -4.428844 -4.428823][-4.42885 -4.428894 -4.4289002 -4.428874 -4.4288373 -4.4288225 -4.4288306 -4.4288292 -4.4288278 -4.4288387 -4.4288511 -4.4288545 -4.4288406 -4.4288316 -4.4288306][-4.4288287 -4.4288869 -4.4288898 -4.4288473 -4.4287963 -4.4287786 -4.4287949 -4.4287963 -4.4287877 -4.4287872 -4.4287887 -4.4287877 -4.4287806 -4.4287896 -4.4288125][-4.4288416 -4.4288983 -4.4288936 -4.4288449 -4.42879 -4.4287739 -4.4287949 -4.4287977 -4.4287782 -4.4287577 -4.4287367 -4.4287171 -4.4287076 -4.4287314 -4.4287744][-4.4288826 -4.4289193 -4.4289 -4.42885 -4.428803 -4.4287953 -4.4288192 -4.4288235 -4.4287992 -4.4287634 -4.4287176 -4.4286737 -4.4286585 -4.4286919 -4.4287405][-4.4289117 -4.4289212 -4.4288898 -4.4288483 -4.4288197 -4.428823 -4.4288449 -4.42885 -4.4288321 -4.4287934 -4.4287348 -4.42868 -4.4286666 -4.4287014 -4.4287333]]...]
INFO - root - 2017-12-08 05:03:23.302805: step 2910, loss = 2.28, batch loss = 2.23 (35.0 examples/sec; 0.229 sec/batch; 20h:55m:38s remains)
INFO - root - 2017-12-08 05:03:25.548420: step 2920, loss = 2.28, batch loss = 2.23 (35.3 examples/sec; 0.227 sec/batch; 20h:44m:56s remains)
INFO - root - 2017-12-08 05:03:27.773228: step 2930, loss = 2.28, batch loss = 2.23 (34.4 examples/sec; 0.233 sec/batch; 21h:18m:30s remains)
INFO - root - 2017-12-08 05:03:30.011554: step 2940, loss = 2.28, batch loss = 2.23 (33.8 examples/sec; 0.237 sec/batch; 21h:39m:14s remains)
INFO - root - 2017-12-08 05:03:32.261778: step 2950, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 20h:05m:56s remains)
INFO - root - 2017-12-08 05:03:34.485952: step 2960, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 20h:03m:02s remains)
INFO - root - 2017-12-08 05:03:36.728990: step 2970, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 20h:14m:32s remains)
INFO - root - 2017-12-08 05:03:38.965896: step 2980, loss = 2.28, batch loss = 2.23 (35.2 examples/sec; 0.227 sec/batch; 20h:46m:31s remains)
INFO - root - 2017-12-08 05:03:41.238453: step 2990, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.225 sec/batch; 20h:36m:38s remains)
INFO - root - 2017-12-08 05:03:43.445043: step 3000, loss = 2.28, batch loss = 2.23 (34.3 examples/sec; 0.233 sec/batch; 21h:21m:40s remains)
2017-12-08 05:03:43.732349: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4287095 -4.4287171 -4.4287629 -4.4288273 -4.4288716 -4.42887 -4.4288158 -4.4287539 -4.4287248 -4.4287348 -4.4287643 -4.4287758 -4.4288 -4.4287963 -4.4287686][-4.4287 -4.4287109 -4.42876 -4.4288259 -4.4288697 -4.428863 -4.4287939 -4.4287195 -4.4286861 -4.4287024 -4.4287338 -4.4287462 -4.428771 -4.4287658 -4.4287381][-4.4286733 -4.4286847 -4.4287324 -4.4288015 -4.4288487 -4.42884 -4.4287658 -4.4286842 -4.4286523 -4.428679 -4.4287157 -4.4287219 -4.428741 -4.4287367 -4.4287143][-4.428638 -4.4286528 -4.4286928 -4.4287643 -4.4288211 -4.4288239 -4.4287691 -4.428709 -4.4286985 -4.4287372 -4.4287724 -4.4287739 -4.4287834 -4.4287739 -4.4287519][-4.4286127 -4.4286318 -4.42865 -4.4287009 -4.428751 -4.428761 -4.4287333 -4.4287081 -4.4287295 -4.4287868 -4.428833 -4.4288416 -4.4288478 -4.4288363 -4.4288163][-4.4285917 -4.4286094 -4.4285941 -4.4286113 -4.428648 -4.4286609 -4.4286366 -4.4286132 -4.4286447 -4.428719 -4.4287839 -4.4288082 -4.42882 -4.4288168 -4.4288182][-4.428566 -4.4285703 -4.4285159 -4.4284859 -4.428503 -4.428514 -4.4284644 -4.4284015 -4.4284244 -4.428546 -4.4286551 -4.4287086 -4.4287305 -4.4287395 -4.4287677][-4.4285741 -4.42854 -4.4284315 -4.4283361 -4.4283147 -4.4283013 -4.4282022 -4.4280653 -4.4280772 -4.4282913 -4.4284768 -4.4285641 -4.4285865 -4.4286094 -4.4286652][-4.42865 -4.4285874 -4.4284544 -4.4283218 -4.4282684 -4.4282312 -4.4280939 -4.4278903 -4.4278946 -4.4281759 -4.4284053 -4.4284954 -4.4284978 -4.4285159 -4.42858][-4.4287639 -4.4287105 -4.4286084 -4.4285021 -4.4284506 -4.4284191 -4.4283214 -4.428184 -4.4281907 -4.4283919 -4.42856 -4.4286051 -4.4285684 -4.4285512 -4.428586][-4.4288692 -4.4288363 -4.428781 -4.4287186 -4.4286809 -4.4286547 -4.4285975 -4.4285183 -4.4285169 -4.4286232 -4.428709 -4.4287143 -4.4286633 -4.4286304 -4.4286389][-4.4289241 -4.4289074 -4.4288826 -4.4288554 -4.4288359 -4.4288197 -4.4287834 -4.428731 -4.4287148 -4.4287472 -4.428762 -4.4287205 -4.4286551 -4.4286122 -4.4285913][-4.4289436 -4.428936 -4.4289246 -4.428916 -4.4289079 -4.4288955 -4.4288664 -4.4288235 -4.4287977 -4.4287868 -4.4287519 -4.42867 -4.4285841 -4.428524 -4.4284739][-4.4289613 -4.4289556 -4.428947 -4.4289427 -4.4289393 -4.4289308 -4.4289055 -4.4288673 -4.4288359 -4.4288039 -4.4287419 -4.4286366 -4.42854 -4.4284735 -4.4284163][-4.4289742 -4.428968 -4.4289594 -4.4289536 -4.4289522 -4.4289489 -4.4289322 -4.4289031 -4.42887 -4.4288292 -4.4287658 -4.4286742 -4.4285917 -4.4285355 -4.4284849]]...]
INFO - root - 2017-12-08 05:03:45.966127: step 3010, loss = 2.28, batch loss = 2.23 (35.7 examples/sec; 0.224 sec/batch; 20h:29m:34s remains)
INFO - root - 2017-12-08 05:03:48.207514: step 3020, loss = 2.28, batch loss = 2.23 (37.1 examples/sec; 0.216 sec/batch; 19h:43m:32s remains)
INFO - root - 2017-12-08 05:03:50.442953: step 3030, loss = 2.28, batch loss = 2.23 (37.9 examples/sec; 0.211 sec/batch; 19h:17m:33s remains)
INFO - root - 2017-12-08 05:03:52.636463: step 3040, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.220 sec/batch; 20h:09m:26s remains)
INFO - root - 2017-12-08 05:03:54.903393: step 3050, loss = 2.28, batch loss = 2.23 (36.7 examples/sec; 0.218 sec/batch; 19h:57m:05s remains)
INFO - root - 2017-12-08 05:03:57.154312: step 3060, loss = 2.28, batch loss = 2.23 (35.8 examples/sec; 0.223 sec/batch; 20h:26m:17s remains)
INFO - root - 2017-12-08 05:03:59.396543: step 3070, loss = 2.28, batch loss = 2.23 (35.1 examples/sec; 0.228 sec/batch; 20h:49m:57s remains)
INFO - root - 2017-12-08 05:04:01.653262: step 3080, loss = 2.28, batch loss = 2.23 (36.0 examples/sec; 0.222 sec/batch; 20h:21m:20s remains)
INFO - root - 2017-12-08 05:04:03.892891: step 3090, loss = 2.28, batch loss = 2.23 (36.9 examples/sec; 0.217 sec/batch; 19h:50m:16s remains)
INFO - root - 2017-12-08 05:04:06.113945: step 3100, loss = 2.28, batch loss = 2.23 (36.7 examples/sec; 0.218 sec/batch; 19h:56m:33s remains)
2017-12-08 05:04:06.390728: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4287705 -4.4287834 -4.4287562 -4.4287019 -4.4286742 -4.428689 -4.4286966 -4.4286528 -4.4285927 -4.428596 -4.4286637 -4.4287276 -4.4287405 -4.4286957 -4.4286504][-4.428721 -4.4287391 -4.4287224 -4.4286823 -4.4286714 -4.4286947 -4.4287014 -4.4286618 -4.4286127 -4.4286265 -4.4287024 -4.4287739 -4.4287949 -4.4287558 -4.4287081][-4.4286871 -4.428721 -4.428731 -4.4287243 -4.4287386 -4.4287639 -4.4287648 -4.4287305 -4.4286976 -4.42872 -4.4287925 -4.428854 -4.4288716 -4.42884 -4.4287891][-4.428721 -4.4287591 -4.4287853 -4.4288015 -4.42882 -4.4288287 -4.4288116 -4.4287782 -4.4287624 -4.4287887 -4.4288392 -4.4288731 -4.4288831 -4.4288659 -4.4288287][-4.4287715 -4.4287877 -4.4288039 -4.428822 -4.4288316 -4.4288149 -4.4287715 -4.4287252 -4.428721 -4.4287553 -4.4287925 -4.4288082 -4.4288177 -4.4288225 -4.4288082][-4.4287891 -4.4287696 -4.4287577 -4.4287643 -4.4287624 -4.4287171 -4.4286356 -4.4285603 -4.4285588 -4.4286184 -4.4286747 -4.4286981 -4.4287143 -4.4287386 -4.4287529][-4.42877 -4.4287138 -4.4286685 -4.4286604 -4.428648 -4.4285803 -4.428463 -4.4283586 -4.4283667 -4.4284668 -4.4285626 -4.4286103 -4.4286408 -4.4286871 -4.4287243][-4.4287496 -4.4286776 -4.4286184 -4.4286065 -4.428596 -4.4285288 -4.428411 -4.4283113 -4.4283314 -4.4284511 -4.4285674 -4.4286332 -4.4286728 -4.4287224 -4.42876][-4.4287477 -4.4286971 -4.42866 -4.428668 -4.4286761 -4.4286351 -4.428555 -4.4284868 -4.4285035 -4.428596 -4.4286852 -4.4287343 -4.4287562 -4.4287839 -4.4288044][-4.4287686 -4.4287519 -4.4287467 -4.4287748 -4.4287996 -4.4287891 -4.4287486 -4.4287076 -4.4287138 -4.4287672 -4.4288135 -4.4288311 -4.4288249 -4.4288211 -4.4288216][-4.4288096 -4.4288111 -4.4288254 -4.4288583 -4.428885 -4.4288878 -4.4288712 -4.4288459 -4.4288411 -4.428865 -4.4288816 -4.4288769 -4.428854 -4.4288316 -4.428822][-4.4288654 -4.4288669 -4.4288826 -4.428905 -4.4289217 -4.4289274 -4.4289203 -4.4289031 -4.4288893 -4.4288936 -4.4288921 -4.4288812 -4.4288597 -4.4288363 -4.4288278][-4.4289083 -4.4289002 -4.4289021 -4.4289026 -4.4289007 -4.4288993 -4.42889 -4.4288726 -4.4288507 -4.4288449 -4.4288473 -4.4288511 -4.4288445 -4.428833 -4.4288344][-4.4289 -4.4288821 -4.4288712 -4.4288521 -4.4288306 -4.42882 -4.4288096 -4.4287939 -4.4287739 -4.4287682 -4.4287853 -4.428812 -4.4288216 -4.4288168 -4.428822][-4.4288588 -4.4288421 -4.4288297 -4.4287972 -4.4287591 -4.4287448 -4.4287415 -4.4287348 -4.4287214 -4.4287233 -4.4287558 -4.4287982 -4.4288163 -4.4288106 -4.42881]]...]
INFO - root - 2017-12-08 05:04:08.634860: step 3110, loss = 2.28, batch loss = 2.23 (37.0 examples/sec; 0.216 sec/batch; 19h:46m:36s remains)
INFO - root - 2017-12-08 05:04:10.885596: step 3120, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 20h:12m:59s remains)
INFO - root - 2017-12-08 05:04:13.109323: step 3130, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 20h:07m:11s remains)
INFO - root - 2017-12-08 05:04:15.376496: step 3140, loss = 2.28, batch loss = 2.23 (34.8 examples/sec; 0.230 sec/batch; 21h:01m:55s remains)
INFO - root - 2017-12-08 05:04:17.640567: step 3150, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.222 sec/batch; 20h:17m:56s remains)
INFO - root - 2017-12-08 05:04:19.879090: step 3160, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 20h:22m:03s remains)
INFO - root - 2017-12-08 05:04:22.104332: step 3170, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 20h:12m:14s remains)
INFO - root - 2017-12-08 05:04:24.339191: step 3180, loss = 2.28, batch loss = 2.23 (36.9 examples/sec; 0.217 sec/batch; 19h:51m:08s remains)
INFO - root - 2017-12-08 05:04:26.622681: step 3190, loss = 2.28, batch loss = 2.23 (33.9 examples/sec; 0.236 sec/batch; 21h:36m:08s remains)
INFO - root - 2017-12-08 05:04:28.850442: step 3200, loss = 2.28, batch loss = 2.23 (34.8 examples/sec; 0.230 sec/batch; 21h:02m:29s remains)
2017-12-08 05:04:29.127988: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289045 -4.428906 -4.4289017 -4.4288845 -4.4288707 -4.4288654 -4.4288611 -4.4288578 -4.4288554 -4.4288521 -4.4288564 -4.4288731 -4.4288926 -4.4289026 -4.4288936][-4.4288654 -4.4288607 -4.4288492 -4.4288249 -4.4288087 -4.4287996 -4.4287944 -4.428793 -4.4287963 -4.4287944 -4.4288 -4.4288168 -4.4288373 -4.4288387 -4.428813][-4.4288106 -4.4287877 -4.4287586 -4.42872 -4.4286895 -4.4286718 -4.4286704 -4.4286904 -4.4287105 -4.4287076 -4.4287109 -4.4287219 -4.4287457 -4.4287505 -4.4287186][-4.4287643 -4.428721 -4.4286718 -4.4286156 -4.4285588 -4.4285088 -4.4284935 -4.4285326 -4.428575 -4.4285774 -4.428586 -4.428607 -4.4286528 -4.4286823 -4.4286652][-4.4287376 -4.42868 -4.4286194 -4.4285507 -4.42846 -4.4283581 -4.428299 -4.4283423 -4.4284091 -4.4284372 -4.4284782 -4.4285331 -4.4286103 -4.4286685 -4.428668][-4.4287333 -4.4286733 -4.4286065 -4.4285293 -4.4284 -4.428225 -4.4281011 -4.428122 -4.4281969 -4.42826 -4.4283609 -4.428473 -4.4285827 -4.4286661 -4.4286947][-4.4287405 -4.4286838 -4.4286208 -4.4285603 -4.4284382 -4.428237 -4.4280782 -4.4280729 -4.4281363 -4.4282055 -4.4283309 -4.4284649 -4.42857 -4.4286451 -4.4286861][-4.4287472 -4.4286923 -4.4286437 -4.4286122 -4.4285278 -4.4283619 -4.4282231 -4.4282203 -4.42828 -4.4283338 -4.428432 -4.4285393 -4.4286132 -4.4286585 -4.428688][-4.4287539 -4.4287081 -4.42868 -4.4286733 -4.4286222 -4.4285121 -4.4284019 -4.42839 -4.4284344 -4.4284649 -4.4285316 -4.4286175 -4.4286714 -4.4286866 -4.4286909][-4.4287548 -4.4287238 -4.4287224 -4.428741 -4.4287219 -4.4286523 -4.4285583 -4.4285288 -4.4285512 -4.428565 -4.4286079 -4.4286766 -4.4287119 -4.4287071 -4.4286966][-4.4287348 -4.4287114 -4.42873 -4.4287696 -4.4287724 -4.4287243 -4.4286475 -4.4286103 -4.4286261 -4.4286404 -4.4286747 -4.4287248 -4.4287505 -4.4287395 -4.4287229][-4.4287281 -4.4287014 -4.4287267 -4.4287748 -4.4287977 -4.4287748 -4.428719 -4.4286857 -4.4286962 -4.4287071 -4.4287314 -4.4287663 -4.4287877 -4.4287796 -4.4287667][-4.4287581 -4.4287252 -4.428741 -4.4287858 -4.4288163 -4.4288135 -4.4287786 -4.4287529 -4.428761 -4.4287658 -4.4287863 -4.4288149 -4.4288354 -4.4288383 -4.4288411][-4.4288106 -4.4287777 -4.428782 -4.428812 -4.4288325 -4.4288297 -4.4288044 -4.4287815 -4.4287858 -4.4287939 -4.4288206 -4.4288573 -4.4288845 -4.4289002 -4.428915][-4.4288659 -4.428834 -4.4288235 -4.4288316 -4.4288349 -4.4288254 -4.4288058 -4.4287915 -4.4287968 -4.4288106 -4.4288406 -4.4288821 -4.428916 -4.4289408 -4.4289632]]...]
INFO - root - 2017-12-08 05:04:31.363014: step 3210, loss = 2.28, batch loss = 2.23 (35.1 examples/sec; 0.228 sec/batch; 20h:52m:23s remains)
INFO - root - 2017-12-08 05:04:33.594898: step 3220, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.225 sec/batch; 20h:37m:20s remains)
INFO - root - 2017-12-08 05:04:35.827401: step 3230, loss = 2.28, batch loss = 2.23 (35.0 examples/sec; 0.229 sec/batch; 20h:54m:37s remains)
INFO - root - 2017-12-08 05:04:38.078025: step 3240, loss = 2.28, batch loss = 2.23 (37.5 examples/sec; 0.213 sec/batch; 19h:31m:13s remains)
INFO - root - 2017-12-08 05:04:40.321641: step 3250, loss = 2.28, batch loss = 2.23 (34.9 examples/sec; 0.229 sec/batch; 20h:59m:16s remains)
INFO - root - 2017-12-08 05:04:42.608117: step 3260, loss = 2.28, batch loss = 2.23 (35.8 examples/sec; 0.224 sec/batch; 20h:27m:52s remains)
INFO - root - 2017-12-08 05:04:44.875310: step 3270, loss = 2.28, batch loss = 2.23 (37.7 examples/sec; 0.212 sec/batch; 19h:25m:49s remains)
INFO - root - 2017-12-08 05:04:47.088798: step 3280, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.222 sec/batch; 20h:17m:02s remains)
INFO - root - 2017-12-08 05:04:49.330381: step 3290, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.220 sec/batch; 20h:08m:14s remains)
INFO - root - 2017-12-08 05:04:51.555846: step 3300, loss = 2.28, batch loss = 2.23 (36.7 examples/sec; 0.218 sec/batch; 19h:55m:09s remains)
2017-12-08 05:04:51.841492: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289694 -4.4289527 -4.4289417 -4.4289379 -4.4289408 -4.4289393 -4.4289308 -4.4289169 -4.4289036 -4.4289 -4.4289112 -4.4289312 -4.4289451 -4.4289408 -4.428925][-4.4289465 -4.4289284 -4.4289184 -4.42892 -4.4289241 -4.4289231 -4.4289145 -4.4289031 -4.4288907 -4.4288812 -4.4288826 -4.4289012 -4.428915 -4.4289045 -4.4288726][-4.42894 -4.428925 -4.4289184 -4.4289207 -4.4289222 -4.4289145 -4.4288983 -4.4288859 -4.4288831 -4.4288721 -4.4288621 -4.428875 -4.4288888 -4.4288688 -4.4288168][-4.4289374 -4.4289255 -4.428925 -4.4289236 -4.428916 -4.4288993 -4.4288712 -4.4288578 -4.4288607 -4.4288487 -4.4288321 -4.428843 -4.4288497 -4.4288163 -4.4287562][-4.42894 -4.4289293 -4.4289269 -4.4289083 -4.4288869 -4.428854 -4.4288025 -4.4287724 -4.428781 -4.4287848 -4.4287853 -4.4288011 -4.4288006 -4.428761 -4.4287033][-4.4289422 -4.4289217 -4.4289012 -4.4288616 -4.428822 -4.4287615 -4.4286695 -4.4286137 -4.4286408 -4.42869 -4.4287343 -4.4287663 -4.4287562 -4.4287081 -4.4286575][-4.4289227 -4.4288883 -4.4288516 -4.4287977 -4.428731 -4.4286237 -4.428462 -4.4283571 -4.4284205 -4.428556 -4.4286642 -4.4287238 -4.4287214 -4.4286737 -4.4286313][-4.4288874 -4.4288416 -4.4287891 -4.4287181 -4.428628 -4.4284759 -4.4282255 -4.4280357 -4.4281669 -4.4284096 -4.4285812 -4.4286728 -4.4286852 -4.4286413 -4.4286022][-4.428863 -4.4288111 -4.4287419 -4.4286537 -4.428556 -4.4284105 -4.4281754 -4.4279923 -4.4281359 -4.4283905 -4.4285693 -4.4286637 -4.4286752 -4.4286275 -4.4285817][-4.4288692 -4.4288225 -4.4287472 -4.4286509 -4.4285588 -4.4284716 -4.4283638 -4.4282985 -4.4283929 -4.4285445 -4.4286528 -4.42871 -4.4287086 -4.4286566 -4.4286079][-4.4288783 -4.4288411 -4.4287734 -4.4286842 -4.42861 -4.4285707 -4.4285502 -4.4285588 -4.4286227 -4.428699 -4.4287481 -4.4287724 -4.4287543 -4.4286981 -4.4286509][-4.4288917 -4.4288673 -4.42882 -4.4287539 -4.4287014 -4.42869 -4.4287028 -4.4287338 -4.4287772 -4.4288111 -4.428822 -4.4288158 -4.4287877 -4.4287381 -4.4287038][-4.4289236 -4.4289107 -4.4288869 -4.4288464 -4.4288168 -4.4288225 -4.4288416 -4.4288683 -4.42889 -4.4289021 -4.4288955 -4.4288769 -4.4288492 -4.428812 -4.42879][-4.4289632 -4.4289551 -4.4289436 -4.4289241 -4.4289145 -4.4289246 -4.4289384 -4.4289532 -4.4289632 -4.4289646 -4.4289532 -4.4289374 -4.4289184 -4.4288969 -4.4288859][-4.4289875 -4.428978 -4.42897 -4.4289637 -4.4289632 -4.4289684 -4.4289722 -4.428977 -4.4289804 -4.4289784 -4.4289713 -4.4289646 -4.4289565 -4.4289465 -4.4289422]]...]
INFO - root - 2017-12-08 05:04:54.077290: step 3310, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.222 sec/batch; 20h:16m:07s remains)
INFO - root - 2017-12-08 05:04:56.310166: step 3320, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.222 sec/batch; 20h:15m:21s remains)
INFO - root - 2017-12-08 05:04:58.570171: step 3330, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 20h:04m:48s remains)
INFO - root - 2017-12-08 05:05:00.820264: step 3340, loss = 2.28, batch loss = 2.23 (35.7 examples/sec; 0.224 sec/batch; 20h:28m:42s remains)
INFO - root - 2017-12-08 05:05:03.069430: step 3350, loss = 2.28, batch loss = 2.23 (35.1 examples/sec; 0.228 sec/batch; 20h:50m:07s remains)
INFO - root - 2017-12-08 05:05:05.293159: step 3360, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 20h:11m:37s remains)
INFO - root - 2017-12-08 05:05:07.570058: step 3370, loss = 2.28, batch loss = 2.23 (35.7 examples/sec; 0.224 sec/batch; 20h:29m:47s remains)
INFO - root - 2017-12-08 05:05:09.860771: step 3380, loss = 2.28, batch loss = 2.23 (35.1 examples/sec; 0.228 sec/batch; 20h:50m:47s remains)
INFO - root - 2017-12-08 05:05:12.115728: step 3390, loss = 2.28, batch loss = 2.23 (36.8 examples/sec; 0.217 sec/batch; 19h:51m:15s remains)
INFO - root - 2017-12-08 05:05:14.365306: step 3400, loss = 2.28, batch loss = 2.23 (35.1 examples/sec; 0.228 sec/batch; 20h:48m:28s remains)
2017-12-08 05:05:14.641378: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4285612 -4.4286 -4.4286637 -4.4287276 -4.4287534 -4.4287505 -4.4287434 -4.4287429 -4.4287539 -4.4287834 -4.428812 -4.4288206 -4.4288235 -4.4288278 -4.4288349][-4.4284945 -4.4285722 -4.4286613 -4.4287395 -4.4287705 -4.4287715 -4.4287667 -4.4287553 -4.4287543 -4.4287739 -4.4287972 -4.4288044 -4.4288034 -4.4287925 -4.4287772][-4.4285131 -4.4286141 -4.4287186 -4.4287939 -4.42882 -4.4288139 -4.4288025 -4.4287763 -4.4287596 -4.4287658 -4.4287782 -4.4287868 -4.4287848 -4.4287529 -4.4287148][-4.4285951 -4.4286928 -4.4287868 -4.4288411 -4.4288483 -4.4288282 -4.428803 -4.4287672 -4.4287419 -4.4287519 -4.42877 -4.4287829 -4.4287815 -4.4287362 -4.4286833][-4.4286757 -4.42875 -4.4288187 -4.4288521 -4.4288435 -4.4288087 -4.428762 -4.4287086 -4.4286852 -4.4287119 -4.4287443 -4.4287686 -4.4287758 -4.4287395 -4.4286952][-4.4287133 -4.428761 -4.4288015 -4.428812 -4.4287872 -4.4287319 -4.4286518 -4.428566 -4.4285412 -4.4286017 -4.4286685 -4.42872 -4.4287515 -4.4287472 -4.4287295][-4.4287066 -4.42873 -4.428741 -4.4287257 -4.4286776 -4.428597 -4.4284735 -4.4283457 -4.4283257 -4.4284492 -4.4285831 -4.4286861 -4.4287486 -4.4287724 -4.4287763][-4.4286904 -4.4286828 -4.4286656 -4.4286332 -4.4285727 -4.4284744 -4.4283257 -4.4281754 -4.4281921 -4.4284015 -4.4285955 -4.4287224 -4.4287891 -4.4288206 -4.4288249][-4.4287057 -4.428678 -4.4286494 -4.4286194 -4.4285736 -4.4284983 -4.4283962 -4.4283204 -4.4283791 -4.4285593 -4.4287128 -4.4287958 -4.42883 -4.4288425 -4.4288349][-4.4287238 -4.4286938 -4.4286757 -4.4286704 -4.4286566 -4.4286261 -4.4285955 -4.4285855 -4.4286275 -4.4287276 -4.4288044 -4.4288249 -4.4288149 -4.4288087 -4.4288][-4.4287367 -4.4287 -4.4286904 -4.4287128 -4.4287286 -4.42873 -4.4287314 -4.4287395 -4.4287577 -4.4288039 -4.4288278 -4.4288058 -4.4287615 -4.4287386 -4.4287338][-4.4287472 -4.4287014 -4.4286895 -4.4287152 -4.4287448 -4.4287596 -4.4287677 -4.4287777 -4.428792 -4.4288168 -4.4288144 -4.4287663 -4.4286995 -4.4286675 -4.4286714][-4.4287453 -4.4286971 -4.4286761 -4.4286919 -4.4287162 -4.42873 -4.4287419 -4.4287596 -4.4287782 -4.4287915 -4.4287772 -4.4287219 -4.4286504 -4.4286132 -4.4286242][-4.4287243 -4.4286842 -4.428669 -4.4286733 -4.4286814 -4.4286857 -4.4286942 -4.4287086 -4.4287281 -4.4287405 -4.4287286 -4.4286847 -4.4286251 -4.4285951 -4.4286108][-4.4287362 -4.4287105 -4.4287028 -4.4286919 -4.4286675 -4.4286551 -4.4286556 -4.428659 -4.428678 -4.4286985 -4.4286985 -4.4286733 -4.4286304 -4.4286165 -4.4286323]]...]
INFO - root - 2017-12-08 05:05:16.878114: step 3410, loss = 2.28, batch loss = 2.23 (36.7 examples/sec; 0.218 sec/batch; 19h:56m:55s remains)
INFO - root - 2017-12-08 05:05:19.139628: step 3420, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.222 sec/batch; 20h:17m:06s remains)
INFO - root - 2017-12-08 05:05:21.370264: step 3430, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.220 sec/batch; 20h:09m:11s remains)
INFO - root - 2017-12-08 05:05:23.651850: step 3440, loss = 2.28, batch loss = 2.23 (35.3 examples/sec; 0.227 sec/batch; 20h:42m:48s remains)
INFO - root - 2017-12-08 05:05:25.889523: step 3450, loss = 2.28, batch loss = 2.23 (36.6 examples/sec; 0.218 sec/batch; 19h:57m:46s remains)
INFO - root - 2017-12-08 05:05:28.133973: step 3460, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 20h:05m:26s remains)
INFO - root - 2017-12-08 05:05:30.380694: step 3470, loss = 2.28, batch loss = 2.23 (35.3 examples/sec; 0.227 sec/batch; 20h:43m:07s remains)
INFO - root - 2017-12-08 05:05:32.614609: step 3480, loss = 2.28, batch loss = 2.23 (35.6 examples/sec; 0.225 sec/batch; 20h:32m:46s remains)
INFO - root - 2017-12-08 05:05:34.848276: step 3490, loss = 2.28, batch loss = 2.23 (35.8 examples/sec; 0.224 sec/batch; 20h:26m:42s remains)
INFO - root - 2017-12-08 05:05:37.077618: step 3500, loss = 2.28, batch loss = 2.23 (35.6 examples/sec; 0.225 sec/batch; 20h:33m:44s remains)
2017-12-08 05:05:37.393184: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289794 -4.428978 -4.4289761 -4.4289746 -4.4289718 -4.4289703 -4.4289727 -4.4289756 -4.4289818 -4.4289875 -4.4289947 -4.4290004 -4.428998 -4.4289904 -4.4289908][-4.4289722 -4.4289637 -4.4289556 -4.42895 -4.4289422 -4.4289374 -4.4289355 -4.4289374 -4.4289465 -4.4289532 -4.4289584 -4.4289651 -4.4289646 -4.4289622 -4.4289727][-4.4289641 -4.4289489 -4.4289317 -4.4289174 -4.4289 -4.4288826 -4.4288707 -4.4288659 -4.4288759 -4.4288859 -4.4288969 -4.428905 -4.4289083 -4.4289131 -4.4289351][-4.4289517 -4.4289279 -4.4288969 -4.4288654 -4.428833 -4.428803 -4.4287815 -4.4287729 -4.4287825 -4.4287977 -4.4288192 -4.4288373 -4.4288483 -4.42886 -4.4288931][-4.4289312 -4.4289007 -4.4288564 -4.4288163 -4.4287724 -4.4287333 -4.4287062 -4.4287043 -4.4287148 -4.4287271 -4.4287543 -4.4287829 -4.4288039 -4.4288216 -4.4288607][-4.4289012 -4.4288688 -4.4288287 -4.4287915 -4.4287386 -4.4286809 -4.4286504 -4.428658 -4.4286723 -4.4286852 -4.4287176 -4.4287553 -4.4287782 -4.4287963 -4.4288373][-4.4288754 -4.4288516 -4.4288239 -4.4287853 -4.4287109 -4.4286275 -4.4285936 -4.4286141 -4.4286542 -4.4286847 -4.4287248 -4.4287615 -4.4287772 -4.4287872 -4.4288225][-4.4288545 -4.4288464 -4.4288259 -4.4287753 -4.4286785 -4.4285851 -4.4285684 -4.4286065 -4.4286718 -4.4287219 -4.4287615 -4.4287896 -4.4287968 -4.4287939 -4.4288158][-4.4288468 -4.4288592 -4.4288406 -4.4287753 -4.4286723 -4.4285927 -4.4286032 -4.4286585 -4.4287291 -4.4287772 -4.4288049 -4.4288235 -4.4288182 -4.428802 -4.4288168][-4.4288745 -4.4289 -4.4288859 -4.4288273 -4.4287429 -4.428689 -4.4287109 -4.4287572 -4.428803 -4.4288349 -4.4288459 -4.4288568 -4.4288487 -4.4288273 -4.4288373][-4.4289055 -4.4289393 -4.4289365 -4.4289045 -4.4288549 -4.4288263 -4.4288449 -4.4288707 -4.4288926 -4.4289002 -4.4288921 -4.428894 -4.4288812 -4.428854 -4.428854][-4.4289036 -4.4289403 -4.4289513 -4.4289432 -4.4289193 -4.4289036 -4.4289131 -4.4289284 -4.4289432 -4.4289393 -4.4289236 -4.4289131 -4.4288921 -4.4288578 -4.4288507][-4.4288869 -4.4289203 -4.428937 -4.4289403 -4.4289236 -4.428906 -4.428905 -4.4289193 -4.4289365 -4.4289331 -4.4289212 -4.4289041 -4.4288774 -4.4288406 -4.4288359][-4.4288597 -4.4288836 -4.4288988 -4.4289036 -4.4288907 -4.4288712 -4.4288592 -4.4288683 -4.4288869 -4.4288883 -4.4288797 -4.4288626 -4.4288392 -4.4288116 -4.4288168][-4.4288325 -4.42885 -4.42886 -4.42886 -4.4288487 -4.4288297 -4.4288144 -4.4288192 -4.4288383 -4.4288411 -4.428833 -4.4288168 -4.4288 -4.4287834 -4.4287996]]...]
INFO - root - 2017-12-08 05:05:39.597043: step 3510, loss = 2.28, batch loss = 2.23 (35.2 examples/sec; 0.227 sec/batch; 20h:46m:37s remains)
INFO - root - 2017-12-08 05:05:41.864127: step 3520, loss = 2.28, batch loss = 2.23 (35.4 examples/sec; 0.226 sec/batch; 20h:40m:05s remains)
INFO - root - 2017-12-08 05:05:44.086185: step 3530, loss = 2.28, batch loss = 2.23 (37.4 examples/sec; 0.214 sec/batch; 19h:32m:51s remains)
INFO - root - 2017-12-08 05:05:46.290150: step 3540, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 20h:20m:49s remains)
INFO - root - 2017-12-08 05:05:48.570401: step 3550, loss = 2.28, batch loss = 2.23 (35.2 examples/sec; 0.228 sec/batch; 20h:47m:26s remains)
INFO - root - 2017-12-08 05:05:50.801065: step 3560, loss = 2.28, batch loss = 2.23 (36.8 examples/sec; 0.217 sec/batch; 19h:50m:31s remains)
INFO - root - 2017-12-08 05:05:53.066463: step 3570, loss = 2.28, batch loss = 2.23 (34.9 examples/sec; 0.229 sec/batch; 20h:57m:34s remains)
INFO - root - 2017-12-08 05:05:55.295700: step 3580, loss = 2.28, batch loss = 2.23 (35.4 examples/sec; 0.226 sec/batch; 20h:37m:27s remains)
INFO - root - 2017-12-08 05:05:57.534312: step 3590, loss = 2.28, batch loss = 2.23 (35.3 examples/sec; 0.226 sec/batch; 20h:41m:08s remains)
INFO - root - 2017-12-08 05:05:59.762776: step 3600, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 20h:01m:56s remains)
2017-12-08 05:06:00.029358: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288282 -4.428803 -4.4287624 -4.4286971 -4.4286447 -4.4286366 -4.4286537 -4.428678 -4.4286957 -4.4287276 -4.4287605 -4.4287615 -4.4287176 -4.4286489 -4.4285913][-4.4288106 -4.4288015 -4.4287844 -4.4287324 -4.4286661 -4.4286308 -4.4286366 -4.428668 -4.4286938 -4.4287295 -4.4287677 -4.4287753 -4.4287367 -4.4286718 -4.428607][-4.4287977 -4.4288106 -4.42882 -4.428792 -4.4287224 -4.42866 -4.4286466 -4.4286809 -4.4287205 -4.4287624 -4.4288073 -4.4288239 -4.4287949 -4.4287386 -4.428678][-4.4287848 -4.428812 -4.4288368 -4.4288311 -4.4287696 -4.428688 -4.428659 -4.4286914 -4.4287434 -4.428793 -4.4288445 -4.4288712 -4.4288549 -4.4288111 -4.4287577][-4.4287596 -4.4287939 -4.4288197 -4.4288278 -4.4287796 -4.4286952 -4.4286575 -4.428689 -4.4287438 -4.4287958 -4.4288521 -4.4288907 -4.4288898 -4.4288559 -4.4288111][-4.4287128 -4.4287472 -4.4287663 -4.42878 -4.4287491 -4.4286795 -4.4286456 -4.428679 -4.4287295 -4.4287724 -4.4288254 -4.4288721 -4.428884 -4.4288621 -4.4288263][-4.4286528 -4.4286785 -4.4286981 -4.42872 -4.4287052 -4.4286618 -4.4286418 -4.4286776 -4.4287167 -4.4287405 -4.4287753 -4.4288197 -4.4288425 -4.4288354 -4.4288111][-4.4286013 -4.428616 -4.4286442 -4.4286819 -4.4286842 -4.4286628 -4.428659 -4.42869 -4.4287095 -4.4287071 -4.4287138 -4.4287443 -4.4287691 -4.42878 -4.4287763][-4.4285994 -4.4285975 -4.4286308 -4.4286814 -4.4286981 -4.4286909 -4.428699 -4.4287214 -4.4287167 -4.4286895 -4.4286695 -4.4286809 -4.4287019 -4.4287238 -4.4287429][-4.4286532 -4.4286361 -4.4286575 -4.4287052 -4.4287252 -4.4287281 -4.4287462 -4.4287591 -4.4287395 -4.4286981 -4.4286695 -4.4286675 -4.4286747 -4.4286962 -4.4287276][-4.4287214 -4.4286971 -4.4287009 -4.4287314 -4.4287467 -4.4287543 -4.4287724 -4.4287758 -4.4287505 -4.4287114 -4.428689 -4.4286866 -4.4286847 -4.4287 -4.4287338][-4.4287634 -4.4287367 -4.4287271 -4.4287395 -4.4287486 -4.4287534 -4.4287643 -4.4287572 -4.4287324 -4.4287066 -4.4287014 -4.4287105 -4.4287105 -4.4287167 -4.4287419][-4.4287767 -4.428751 -4.4287386 -4.4287457 -4.428751 -4.4287477 -4.4287462 -4.428731 -4.4287052 -4.428689 -4.4286976 -4.4287171 -4.4287229 -4.4287252 -4.4287434][-4.4287882 -4.428771 -4.4287605 -4.4287672 -4.4287724 -4.4287658 -4.4287534 -4.4287291 -4.4287004 -4.4286866 -4.4287014 -4.4287248 -4.4287343 -4.4287362 -4.4287505][-4.4288149 -4.4288073 -4.4288025 -4.4288106 -4.4288158 -4.4288106 -4.4287972 -4.4287729 -4.4287448 -4.4287295 -4.4287415 -4.4287629 -4.4287744 -4.4287753 -4.4287825]]...]
INFO - root - 2017-12-08 05:06:02.243024: step 3610, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.225 sec/batch; 20h:34m:41s remains)
INFO - root - 2017-12-08 05:06:04.511967: step 3620, loss = 2.28, batch loss = 2.23 (35.0 examples/sec; 0.228 sec/batch; 20h:51m:50s remains)
INFO - root - 2017-12-08 05:06:06.734165: step 3630, loss = 2.28, batch loss = 2.23 (35.8 examples/sec; 0.223 sec/batch; 20h:24m:02s remains)
INFO - root - 2017-12-08 05:06:08.996736: step 3640, loss = 2.28, batch loss = 2.23 (36.7 examples/sec; 0.218 sec/batch; 19h:54m:00s remains)
INFO - root - 2017-12-08 05:06:11.269149: step 3650, loss = 2.28, batch loss = 2.23 (34.6 examples/sec; 0.231 sec/batch; 21h:07m:43s remains)
INFO - root - 2017-12-08 05:06:13.497295: step 3660, loss = 2.28, batch loss = 2.23 (34.6 examples/sec; 0.231 sec/batch; 21h:06m:52s remains)
INFO - root - 2017-12-08 05:06:15.740613: step 3670, loss = 2.28, batch loss = 2.23 (35.3 examples/sec; 0.227 sec/batch; 20h:42m:01s remains)
INFO - root - 2017-12-08 05:06:17.965003: step 3680, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 20h:21m:14s remains)
INFO - root - 2017-12-08 05:06:20.257849: step 3690, loss = 2.28, batch loss = 2.23 (36.0 examples/sec; 0.222 sec/batch; 20h:16m:17s remains)
INFO - root - 2017-12-08 05:06:22.494816: step 3700, loss = 2.28, batch loss = 2.23 (34.4 examples/sec; 0.232 sec/batch; 21h:13m:06s remains)
2017-12-08 05:06:22.805507: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.428823 -4.428822 -4.4287825 -4.4287286 -4.4286909 -4.4286866 -4.4286914 -4.4286704 -4.428638 -4.4286113 -4.4286256 -4.42867 -4.4287033 -4.4287233 -4.4287405][-4.4287724 -4.4287758 -4.4287457 -4.4287024 -4.4286757 -4.4286866 -4.4286971 -4.4286637 -4.4286094 -4.4285684 -4.4285889 -4.4286518 -4.4287052 -4.4287353 -4.4287505][-4.4286976 -4.4287033 -4.4286828 -4.4286585 -4.4286532 -4.4286804 -4.4286952 -4.4286613 -4.4286094 -4.4285822 -4.4286189 -4.4286909 -4.4287448 -4.4287581 -4.4287462][-4.4286232 -4.4286237 -4.4286113 -4.4286122 -4.42863 -4.4286647 -4.4286785 -4.4286556 -4.4286275 -4.42863 -4.4286771 -4.4287419 -4.4287744 -4.4287581 -4.4287162][-4.4285836 -4.4285674 -4.4285522 -4.4285545 -4.4285755 -4.4285984 -4.4285975 -4.4285831 -4.42859 -4.4286242 -4.4286809 -4.4287357 -4.4287529 -4.4287181 -4.4286575][-4.4285355 -4.4284959 -4.4284625 -4.4284463 -4.4284439 -4.4284344 -4.4284124 -4.4284196 -4.4284692 -4.4285374 -4.428606 -4.4286556 -4.4286675 -4.428638 -4.4285908][-4.4284167 -4.4283466 -4.4282794 -4.4282327 -4.4282093 -4.4281721 -4.4281378 -4.4281769 -4.4282827 -4.4283929 -4.4284868 -4.4285383 -4.4285522 -4.428546 -4.4285359][-4.4282994 -4.428185 -4.428071 -4.4279966 -4.4279709 -4.4279385 -4.427917 -4.4279919 -4.4281397 -4.4282775 -4.4283838 -4.428442 -4.4284625 -4.4284883 -4.4285159][-4.4283948 -4.428298 -4.4282069 -4.4281583 -4.4281592 -4.4281678 -4.4281807 -4.4282341 -4.42833 -4.4284196 -4.4284859 -4.4285121 -4.4285111 -4.42853 -4.4285569][-4.4285932 -4.4285359 -4.4284878 -4.4284678 -4.4284787 -4.428503 -4.4285226 -4.4285436 -4.4285808 -4.428617 -4.4286389 -4.4286361 -4.4286218 -4.4286218 -4.4286351][-4.4287615 -4.4287333 -4.4287105 -4.4286995 -4.4287052 -4.428721 -4.4287305 -4.4287329 -4.428741 -4.4287524 -4.4287562 -4.4287472 -4.4287367 -4.4287319 -4.4287362][-4.4288654 -4.4288473 -4.4288292 -4.4288177 -4.4288177 -4.4288235 -4.4288273 -4.4288254 -4.4288268 -4.4288325 -4.4288306 -4.4288244 -4.4288206 -4.4288249 -4.4288354][-4.4289212 -4.4289088 -4.4288955 -4.428884 -4.4288788 -4.4288783 -4.42888 -4.4288816 -4.428885 -4.4288888 -4.4288898 -4.4288898 -4.428894 -4.4289026 -4.4289112][-4.4289427 -4.4289351 -4.4289269 -4.4289188 -4.4289145 -4.4289131 -4.4289145 -4.4289174 -4.4289174 -4.4289169 -4.4289188 -4.4289236 -4.4289322 -4.42894 -4.4289441][-4.42895 -4.4289422 -4.428937 -4.4289317 -4.4289293 -4.4289308 -4.4289351 -4.4289393 -4.4289374 -4.4289317 -4.4289308 -4.4289327 -4.4289374 -4.4289408 -4.4289412]]...]
INFO - root - 2017-12-08 05:06:25.073480: step 3710, loss = 2.28, batch loss = 2.23 (34.5 examples/sec; 0.232 sec/batch; 21h:09m:54s remains)
INFO - root - 2017-12-08 05:06:27.319225: step 3720, loss = 2.28, batch loss = 2.23 (34.9 examples/sec; 0.229 sec/batch; 20h:57m:17s remains)
INFO - root - 2017-12-08 05:06:29.546364: step 3730, loss = 2.28, batch loss = 2.23 (34.8 examples/sec; 0.230 sec/batch; 21h:00m:52s remains)
INFO - root - 2017-12-08 05:06:31.764746: step 3740, loss = 2.28, batch loss = 2.23 (36.8 examples/sec; 0.217 sec/batch; 19h:51m:42s remains)
INFO - root - 2017-12-08 05:06:34.004559: step 3750, loss = 2.28, batch loss = 2.23 (37.3 examples/sec; 0.215 sec/batch; 19h:36m:17s remains)
INFO - root - 2017-12-08 05:06:36.245812: step 3760, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.225 sec/batch; 20h:34m:28s remains)
INFO - root - 2017-12-08 05:06:38.519756: step 3770, loss = 2.28, batch loss = 2.23 (35.1 examples/sec; 0.228 sec/batch; 20h:47m:29s remains)
INFO - root - 2017-12-08 05:06:40.768806: step 3780, loss = 2.28, batch loss = 2.23 (34.6 examples/sec; 0.231 sec/batch; 21h:05m:52s remains)
INFO - root - 2017-12-08 05:06:43.017089: step 3790, loss = 2.28, batch loss = 2.23 (34.6 examples/sec; 0.231 sec/batch; 21h:07m:21s remains)
INFO - root - 2017-12-08 05:06:45.267051: step 3800, loss = 2.28, batch loss = 2.23 (35.2 examples/sec; 0.228 sec/batch; 20h:46m:38s remains)
2017-12-08 05:06:45.546474: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289842 -4.4289694 -4.4289494 -4.4289341 -4.4289274 -4.428926 -4.4289289 -4.4289336 -4.4289284 -4.4289217 -4.4289131 -4.4289069 -4.4289 -4.4288964 -4.4288979][-4.4289827 -4.4289608 -4.4289331 -4.4289136 -4.4289055 -4.4289045 -4.4289069 -4.4289155 -4.4289093 -4.4288993 -4.4288888 -4.4288826 -4.428874 -4.4288692 -4.4288697][-4.428967 -4.4289336 -4.428896 -4.4288726 -4.4288683 -4.4288745 -4.4288878 -4.4289045 -4.4289012 -4.4288931 -4.4288816 -4.4288716 -4.4288626 -4.4288573 -4.428853][-4.4289417 -4.4288893 -4.4288325 -4.4287992 -4.4287987 -4.4288177 -4.4288468 -4.4288769 -4.4288845 -4.4288907 -4.4288845 -4.4288712 -4.4288621 -4.4288526 -4.4288397][-4.4289045 -4.428834 -4.4287529 -4.4286919 -4.4286656 -4.4286685 -4.428699 -4.4287505 -4.42879 -4.4288297 -4.428843 -4.4288344 -4.4288344 -4.4288311 -4.4288144][-4.428874 -4.42879 -4.428688 -4.4285932 -4.428524 -4.4284806 -4.428483 -4.4285321 -4.4286103 -4.4287062 -4.4287581 -4.4287739 -4.4287925 -4.4288058 -4.4287987][-4.4288588 -4.4287729 -4.4286633 -4.4285445 -4.4284277 -4.4283128 -4.4282284 -4.42822 -4.4283304 -4.4284959 -4.4286008 -4.4286585 -4.42871 -4.4287515 -4.428772][-4.4288564 -4.4287815 -4.428679 -4.4285636 -4.4284267 -4.4282627 -4.4280705 -4.427949 -4.428051 -4.4282603 -4.4284143 -4.4285207 -4.4286113 -4.4286823 -4.4287381][-4.4288816 -4.4288363 -4.4287677 -4.428688 -4.428575 -4.4284244 -4.4282289 -4.4280725 -4.4280891 -4.4282331 -4.4283695 -4.4284863 -4.4285884 -4.428668 -4.4287405][-4.428915 -4.4288988 -4.4288683 -4.4288325 -4.4287558 -4.4286513 -4.42852 -4.4284191 -4.4283967 -4.42845 -4.42852 -4.428586 -4.4286551 -4.4287167 -4.4287815][-4.428937 -4.4289351 -4.4289265 -4.4289217 -4.4288812 -4.4288187 -4.4287515 -4.4287143 -4.4287019 -4.4287143 -4.4287333 -4.4287438 -4.4287677 -4.428802 -4.4288511][-4.4289379 -4.4289403 -4.42894 -4.4289532 -4.4289446 -4.4289184 -4.4288926 -4.4288955 -4.4289002 -4.428906 -4.428905 -4.4288893 -4.428884 -4.428894 -4.4289227][-4.4289312 -4.4289336 -4.4289351 -4.4289532 -4.4289651 -4.4289656 -4.4289646 -4.4289813 -4.4289885 -4.4289913 -4.42899 -4.4289756 -4.4289651 -4.4289637 -4.4289756][-4.4289379 -4.4289403 -4.4289432 -4.428957 -4.4289703 -4.4289784 -4.4289861 -4.4290013 -4.4290071 -4.4290075 -4.4290066 -4.4289989 -4.4289923 -4.4289885 -4.4289927][-4.4289541 -4.4289551 -4.4289556 -4.4289613 -4.428968 -4.4289737 -4.4289765 -4.4289813 -4.4289832 -4.4289827 -4.4289837 -4.4289832 -4.4289823 -4.4289827 -4.4289875]]...]
INFO - root - 2017-12-08 05:06:47.759127: step 3810, loss = 2.28, batch loss = 2.23 (35.1 examples/sec; 0.228 sec/batch; 20h:50m:10s remains)
INFO - root - 2017-12-08 05:06:49.982171: step 3820, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.222 sec/batch; 20h:14m:41s remains)
INFO - root - 2017-12-08 05:06:52.236253: step 3830, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 20h:01m:57s remains)
INFO - root - 2017-12-08 05:06:54.522396: step 3840, loss = 2.28, batch loss = 2.23 (35.8 examples/sec; 0.223 sec/batch; 20h:23m:41s remains)
INFO - root - 2017-12-08 05:06:56.744483: step 3850, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 20h:01m:22s remains)
INFO - root - 2017-12-08 05:06:58.974383: step 3860, loss = 2.28, batch loss = 2.23 (35.1 examples/sec; 0.228 sec/batch; 20h:48m:27s remains)
INFO - root - 2017-12-08 05:07:01.199926: step 3870, loss = 2.28, batch loss = 2.23 (36.8 examples/sec; 0.217 sec/batch; 19h:50m:06s remains)
INFO - root - 2017-12-08 05:07:03.421138: step 3880, loss = 2.28, batch loss = 2.23 (36.9 examples/sec; 0.217 sec/batch; 19h:46m:20s remains)
INFO - root - 2017-12-08 05:07:05.665325: step 3890, loss = 2.28, batch loss = 2.23 (36.9 examples/sec; 0.217 sec/batch; 19h:46m:07s remains)
INFO - root - 2017-12-08 05:07:07.894991: step 3900, loss = 2.28, batch loss = 2.23 (36.6 examples/sec; 0.219 sec/batch; 19h:57m:57s remains)
2017-12-08 05:07:08.258602: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288955 -4.4288077 -4.4287 -4.4286108 -4.4285626 -4.4285779 -4.4286418 -4.4287086 -4.4287372 -4.4287391 -4.4287314 -4.4287167 -4.4287062 -4.4286885 -4.4286766][-4.4288979 -4.4288015 -4.4286747 -4.4285612 -4.428504 -4.4285393 -4.4286284 -4.4287004 -4.4287276 -4.4287281 -4.4287138 -4.4286971 -4.4286957 -4.4286914 -4.4286823][-4.4289012 -4.4288 -4.428659 -4.4285221 -4.4284611 -4.4285207 -4.4286361 -4.4287138 -4.4287357 -4.4287362 -4.4287176 -4.4287014 -4.4287019 -4.4286962 -4.4286795][-4.42891 -4.4288077 -4.4286571 -4.4285021 -4.4284396 -4.4285269 -4.4286656 -4.4287443 -4.4287577 -4.4287467 -4.4287195 -4.4287028 -4.4287009 -4.4286947 -4.4286819][-4.4289212 -4.4288187 -4.4286637 -4.4284945 -4.428421 -4.428525 -4.4286838 -4.4287663 -4.4287739 -4.4287558 -4.4287248 -4.4287009 -4.4286909 -4.4286885 -4.4286933][-4.4289384 -4.4288459 -4.4287019 -4.4285307 -4.4284396 -4.4285321 -4.4286876 -4.4287667 -4.4287691 -4.42875 -4.4287248 -4.4286971 -4.4286857 -4.4286914 -4.4287086][-4.4289479 -4.4288683 -4.4287429 -4.4285851 -4.4284906 -4.4285617 -4.4286981 -4.4287643 -4.42876 -4.4287405 -4.4287238 -4.4286919 -4.4286761 -4.4286814 -4.4287057][-4.4289479 -4.4288764 -4.4287639 -4.4286251 -4.428544 -4.428607 -4.4287295 -4.4287925 -4.428791 -4.4287663 -4.4287429 -4.4286981 -4.4286633 -4.4286585 -4.4286828][-4.4289455 -4.4288816 -4.42878 -4.4286628 -4.4286017 -4.4286609 -4.4287667 -4.4288306 -4.428833 -4.4288044 -4.4287791 -4.4287357 -4.4286933 -4.4286718 -4.4286776][-4.4289403 -4.42888 -4.4287868 -4.4286852 -4.4286366 -4.428688 -4.428781 -4.4288492 -4.4288578 -4.4288325 -4.4288163 -4.4287853 -4.4287481 -4.4287176 -4.4286995][-4.4289255 -4.428863 -4.428772 -4.4286833 -4.4286489 -4.4286938 -4.4287763 -4.4288507 -4.4288688 -4.4288492 -4.4288411 -4.4288235 -4.4287944 -4.4287624 -4.4287381][-4.4289131 -4.4288497 -4.4287648 -4.4286923 -4.4286733 -4.4287119 -4.4287815 -4.4288516 -4.4288692 -4.428853 -4.4288468 -4.4288373 -4.4288149 -4.4287872 -4.4287663][-4.4289031 -4.4288454 -4.4287715 -4.4287176 -4.4287105 -4.4287429 -4.428793 -4.4288492 -4.4288626 -4.4288511 -4.4288468 -4.4288425 -4.4288239 -4.4287968 -4.4287796][-4.42889 -4.4288425 -4.4287848 -4.4287462 -4.4287443 -4.4287705 -4.4288063 -4.4288492 -4.4288592 -4.4288497 -4.4288449 -4.4288425 -4.4288297 -4.4288068 -4.4287958][-4.4288912 -4.428854 -4.4288125 -4.4287872 -4.4287882 -4.42881 -4.4288363 -4.4288654 -4.4288692 -4.4288583 -4.42885 -4.4288459 -4.4288378 -4.4288263 -4.4288244]]...]
INFO - root - 2017-12-08 05:07:10.479363: step 3910, loss = 2.28, batch loss = 2.23 (34.9 examples/sec; 0.229 sec/batch; 20h:55m:11s remains)
INFO - root - 2017-12-08 05:07:12.708998: step 3920, loss = 2.28, batch loss = 2.23 (35.6 examples/sec; 0.225 sec/batch; 20h:31m:07s remains)
INFO - root - 2017-12-08 05:07:14.941807: step 3930, loss = 2.28, batch loss = 2.23 (34.9 examples/sec; 0.229 sec/batch; 20h:53m:45s remains)
INFO - root - 2017-12-08 05:07:17.179003: step 3940, loss = 2.28, batch loss = 2.23 (35.4 examples/sec; 0.226 sec/batch; 20h:38m:37s remains)
INFO - root - 2017-12-08 05:07:19.405479: step 3950, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 20h:00m:12s remains)
INFO - root - 2017-12-08 05:07:21.638319: step 3960, loss = 2.28, batch loss = 2.23 (35.1 examples/sec; 0.228 sec/batch; 20h:48m:49s remains)
INFO - root - 2017-12-08 05:07:23.887945: step 3970, loss = 2.28, batch loss = 2.23 (35.7 examples/sec; 0.224 sec/batch; 20h:27m:01s remains)
INFO - root - 2017-12-08 05:07:26.160135: step 3980, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 20h:01m:01s remains)
INFO - root - 2017-12-08 05:07:28.377167: step 3990, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.221 sec/batch; 20h:12m:08s remains)
INFO - root - 2017-12-08 05:07:30.643857: step 4000, loss = 2.28, batch loss = 2.23 (35.0 examples/sec; 0.229 sec/batch; 20h:52m:26s remains)
2017-12-08 05:07:30.907522: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289832 -4.4289818 -4.4289722 -4.4289584 -4.4289436 -4.4289308 -4.4289169 -4.42891 -4.428916 -4.4289279 -4.4289341 -4.42894 -4.4289489 -4.4289551 -4.4289575][-4.428957 -4.4289651 -4.4289565 -4.4289284 -4.4288931 -4.4288559 -4.4288177 -4.4287996 -4.4288125 -4.42884 -4.4288592 -4.42887 -4.4288845 -4.4289021 -4.4289169][-4.4289093 -4.4289293 -4.4289212 -4.4288783 -4.4288116 -4.428741 -4.4286695 -4.428627 -4.4286461 -4.4286966 -4.4287391 -4.4287648 -4.4287844 -4.4288187 -4.4288564][-4.4288526 -4.428885 -4.4288778 -4.4288149 -4.4287086 -4.4285917 -4.4284835 -4.4284182 -4.428452 -4.4285464 -4.4286208 -4.4286623 -4.4286919 -4.4287429 -4.4287996][-4.4287915 -4.4288311 -4.4288321 -4.4287534 -4.4286008 -4.4284167 -4.4282537 -4.4281769 -4.4282575 -4.428432 -4.4285617 -4.4286232 -4.428659 -4.4287181 -4.4287758][-4.4286928 -4.428741 -4.4287653 -4.4286871 -4.4284935 -4.4282155 -4.4279537 -4.4278746 -4.4280567 -4.4283438 -4.428545 -4.4286327 -4.428679 -4.428731 -4.4287734][-4.4285522 -4.4286122 -4.4286704 -4.42861 -4.4284 -4.4280434 -4.4276495 -4.4275513 -4.4278674 -4.4282765 -4.4285479 -4.4286795 -4.4287348 -4.4287786 -4.4287906][-4.4284148 -4.4284821 -4.42857 -4.4285502 -4.4283719 -4.4280238 -4.427587 -4.4274411 -4.4278045 -4.4282575 -4.4285522 -4.4287024 -4.4287686 -4.4288063 -4.4287925][-4.4283323 -4.4283891 -4.4284997 -4.4285455 -4.4284539 -4.42822 -4.4279232 -4.4277987 -4.428019 -4.4283524 -4.4285841 -4.42871 -4.4287663 -4.4287939 -4.4287667][-4.4283485 -4.4283843 -4.428504 -4.4286041 -4.4286017 -4.4284906 -4.4283442 -4.4282708 -4.4283614 -4.428525 -4.428647 -4.4287224 -4.4287486 -4.428762 -4.4287348][-4.428422 -4.4284487 -4.4285645 -4.4286776 -4.4287162 -4.4286742 -4.4286022 -4.428555 -4.428565 -4.4286151 -4.4286652 -4.4287066 -4.4287171 -4.428731 -4.4287114][-4.4284883 -4.4285145 -4.4286146 -4.4287057 -4.4287357 -4.4287167 -4.42868 -4.4286366 -4.4285984 -4.4285874 -4.4286189 -4.4286709 -4.4287086 -4.4287405 -4.4287281][-4.428544 -4.4285665 -4.4286456 -4.4287004 -4.4287009 -4.4286857 -4.4286642 -4.4286213 -4.4285688 -4.4285412 -4.428565 -4.4286323 -4.4287038 -4.4287605 -4.4287639][-4.4286246 -4.428638 -4.4286852 -4.4287 -4.4286728 -4.4286518 -4.428638 -4.42861 -4.4285717 -4.4285603 -4.4285836 -4.4286466 -4.4287224 -4.4287992 -4.4288168][-4.4287171 -4.4287257 -4.428751 -4.4287391 -4.4286895 -4.428658 -4.4286475 -4.4286437 -4.4286256 -4.4286289 -4.4286566 -4.42871 -4.4287868 -4.4288592 -4.4288697]]...]
INFO - root - 2017-12-08 05:07:33.144067: step 4010, loss = 2.28, batch loss = 2.23 (34.9 examples/sec; 0.230 sec/batch; 20h:56m:38s remains)
INFO - root - 2017-12-08 05:07:35.360910: step 4020, loss = 2.28, batch loss = 2.23 (34.7 examples/sec; 0.230 sec/batch; 21h:01m:39s remains)
INFO - root - 2017-12-08 05:07:37.629304: step 4030, loss = 2.28, batch loss = 2.23 (36.6 examples/sec; 0.219 sec/batch; 19h:57m:59s remains)
INFO - root - 2017-12-08 05:07:39.853819: step 4040, loss = 2.28, batch loss = 2.23 (36.8 examples/sec; 0.217 sec/batch; 19h:49m:33s remains)
INFO - root - 2017-12-08 05:07:42.098029: step 4050, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.222 sec/batch; 20h:12m:38s remains)
INFO - root - 2017-12-08 05:07:44.336460: step 4060, loss = 2.28, batch loss = 2.23 (35.4 examples/sec; 0.226 sec/batch; 20h:35m:56s remains)
INFO - root - 2017-12-08 05:07:46.552576: step 4070, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 20h:09m:47s remains)
INFO - root - 2017-12-08 05:07:48.814255: step 4080, loss = 2.28, batch loss = 2.23 (34.4 examples/sec; 0.232 sec/batch; 21h:12m:11s remains)
INFO - root - 2017-12-08 05:07:51.092384: step 4090, loss = 2.28, batch loss = 2.23 (35.3 examples/sec; 0.227 sec/batch; 20h:41m:00s remains)
INFO - root - 2017-12-08 05:07:53.340985: step 4100, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 20h:08m:51s remains)
2017-12-08 05:07:53.629004: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289651 -4.4289618 -4.4289603 -4.4289627 -4.428967 -4.428968 -4.4289632 -4.4289675 -4.4289765 -4.4289713 -4.4289575 -4.4289441 -4.4289355 -4.4289336 -4.4289427][-4.4289093 -4.4288974 -4.4288964 -4.4289093 -4.4289184 -4.4289246 -4.4289308 -4.4289441 -4.4289513 -4.4289346 -4.4289074 -4.4288855 -4.4288778 -4.428884 -4.4289021][-4.42884 -4.4288282 -4.4288363 -4.4288573 -4.4288654 -4.4288731 -4.4288869 -4.4289036 -4.4289036 -4.4288735 -4.4288435 -4.4288268 -4.4288297 -4.4288478 -4.4288721][-4.4287796 -4.428782 -4.4288058 -4.42883 -4.4288316 -4.4288297 -4.4288335 -4.4288411 -4.4288335 -4.428803 -4.4287887 -4.4287925 -4.4288092 -4.4288335 -4.4288483][-4.4287553 -4.4287677 -4.4287934 -4.428812 -4.428802 -4.4287744 -4.4287438 -4.428731 -4.42873 -4.4287267 -4.4287496 -4.42878 -4.428803 -4.4288177 -4.4288082][-4.4287715 -4.4287734 -4.4287844 -4.4287839 -4.4287448 -4.4286704 -4.4285941 -4.4285746 -4.4286222 -4.4286828 -4.4287505 -4.4288 -4.428813 -4.4288044 -4.4287748][-4.4287858 -4.4287643 -4.4287486 -4.4287195 -4.4286385 -4.4285059 -4.4283733 -4.4283729 -4.4285154 -4.4286594 -4.428761 -4.4288063 -4.4287982 -4.4287658 -4.4287233][-4.4287705 -4.4287295 -4.4287033 -4.4286633 -4.4285655 -4.4284143 -4.4282703 -4.4283133 -4.4285178 -4.42869 -4.4287786 -4.4287939 -4.4287543 -4.4287033 -4.4286613][-4.42876 -4.428721 -4.4287028 -4.4286666 -4.4285989 -4.4285064 -4.4284425 -4.428514 -4.428669 -4.428782 -4.4288149 -4.4287844 -4.4287148 -4.4286642 -4.4286442][-4.4287591 -4.4287333 -4.42872 -4.4286885 -4.4286509 -4.4286156 -4.4286141 -4.4286819 -4.4287686 -4.4288135 -4.4287953 -4.4287295 -4.4286542 -4.4286356 -4.4286661][-4.4287581 -4.4287519 -4.4287434 -4.428719 -4.4286981 -4.4286828 -4.4287004 -4.42875 -4.4287877 -4.4287868 -4.4287386 -4.4286623 -4.4286156 -4.4286447 -4.4287152][-4.4287505 -4.4287539 -4.4287491 -4.4287276 -4.4287105 -4.4287024 -4.4287286 -4.4287682 -4.4287777 -4.4287496 -4.4286876 -4.4286222 -4.4286146 -4.4286804 -4.4287653][-4.4286976 -4.4287066 -4.4287105 -4.4286971 -4.428689 -4.4286914 -4.4287224 -4.4287558 -4.4287539 -4.4287186 -4.4286613 -4.4286189 -4.4286289 -4.4286976 -4.4287796][-4.4286141 -4.428618 -4.4286284 -4.4286256 -4.4286284 -4.4286389 -4.4286709 -4.4287004 -4.428699 -4.4286742 -4.4286456 -4.4286208 -4.4286275 -4.4286828 -4.4287567][-4.4285626 -4.428556 -4.4285631 -4.428565 -4.4285707 -4.428587 -4.4286208 -4.4286485 -4.4286637 -4.4286628 -4.42865 -4.4286289 -4.4286251 -4.42867 -4.4287372]]...]
INFO - root - 2017-12-08 05:07:55.854256: step 4110, loss = 2.28, batch loss = 2.23 (36.8 examples/sec; 0.218 sec/batch; 19h:50m:46s remains)
INFO - root - 2017-12-08 05:07:58.090488: step 4120, loss = 2.28, batch loss = 2.23 (36.9 examples/sec; 0.217 sec/batch; 19h:45m:17s remains)
INFO - root - 2017-12-08 05:08:00.341425: step 4130, loss = 2.28, batch loss = 2.23 (34.9 examples/sec; 0.229 sec/batch; 20h:53m:18s remains)
INFO - root - 2017-12-08 05:08:02.584974: step 4140, loss = 2.28, batch loss = 2.23 (36.9 examples/sec; 0.217 sec/batch; 19h:47m:14s remains)
INFO - root - 2017-12-08 05:08:04.824740: step 4150, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 20h:19m:18s remains)
INFO - root - 2017-12-08 05:08:07.050003: step 4160, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.221 sec/batch; 20h:11m:40s remains)
INFO - root - 2017-12-08 05:08:09.307822: step 4170, loss = 2.28, batch loss = 2.23 (33.6 examples/sec; 0.238 sec/batch; 21h:44m:17s remains)
INFO - root - 2017-12-08 05:08:11.562663: step 4180, loss = 2.28, batch loss = 2.23 (35.3 examples/sec; 0.227 sec/batch; 20h:41m:35s remains)
INFO - root - 2017-12-08 05:08:13.778021: step 4190, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 20h:07m:50s remains)
INFO - root - 2017-12-08 05:08:16.016040: step 4200, loss = 2.28, batch loss = 2.23 (35.4 examples/sec; 0.226 sec/batch; 20h:36m:56s remains)
2017-12-08 05:08:16.293204: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288387 -4.4288254 -4.4288163 -4.4288235 -4.4288368 -4.4288349 -4.4288168 -4.42879 -4.4287596 -4.4287248 -4.4286885 -4.428659 -4.42864 -4.4286571 -4.4287114][-4.4287906 -4.4287877 -4.4287858 -4.42879 -4.42879 -4.4287634 -4.4287171 -4.4286675 -4.4286189 -4.4285727 -4.4285321 -4.4285 -4.4284778 -4.4284987 -4.4285674][-4.4287724 -4.4287825 -4.4287858 -4.4287791 -4.4287567 -4.4287038 -4.4286346 -4.4285703 -4.4285135 -4.4284668 -4.428431 -4.4284024 -4.4283824 -4.4284062 -4.4284806][-4.4287477 -4.4287691 -4.4287724 -4.4287539 -4.4287152 -4.4286494 -4.4285717 -4.4285064 -4.4284568 -4.4284291 -4.4284172 -4.4284062 -4.4284024 -4.4284344 -4.4284997][-4.4286728 -4.4287052 -4.4287157 -4.4286995 -4.4286585 -4.4285917 -4.42852 -4.4284706 -4.4284458 -4.42845 -4.428472 -4.428493 -4.428515 -4.4285512 -4.4285951][-4.4285579 -4.4286079 -4.4286332 -4.4286251 -4.428587 -4.4285293 -4.4284744 -4.4284525 -4.428463 -4.4284959 -4.4285378 -4.4285779 -4.4286146 -4.4286494 -4.4286757][-4.4284496 -4.4285188 -4.4285593 -4.4285569 -4.4285245 -4.42848 -4.4284406 -4.4284353 -4.4284663 -4.4285183 -4.4285712 -4.4286184 -4.4286585 -4.4286919 -4.4287167][-4.4283791 -4.4284592 -4.4285088 -4.428514 -4.4284959 -4.4284782 -4.4284668 -4.4284792 -4.4285178 -4.4285674 -4.4286103 -4.4286456 -4.4286776 -4.4287128 -4.4287505][-4.4283862 -4.4284678 -4.428524 -4.4285426 -4.4285469 -4.4285645 -4.4285836 -4.428607 -4.4286284 -4.4286447 -4.4286551 -4.428668 -4.4286895 -4.4287276 -4.428782][-4.4284778 -4.4285455 -4.4286013 -4.4286323 -4.4286532 -4.4286842 -4.4287105 -4.4287286 -4.4287262 -4.4287033 -4.428678 -4.4286671 -4.4286757 -4.4287124 -4.4287767][-4.4285641 -4.4286127 -4.4286623 -4.4286995 -4.4287291 -4.4287624 -4.4287915 -4.42881 -4.4287953 -4.4287429 -4.4286804 -4.428638 -4.4286237 -4.42865 -4.42872][-4.4286227 -4.428658 -4.4287 -4.4287367 -4.4287696 -4.4288049 -4.4288373 -4.4288578 -4.42884 -4.4287734 -4.4286861 -4.4286146 -4.4285779 -4.4285946 -4.4286666][-4.4287043 -4.4287271 -4.4287572 -4.4287882 -4.4288197 -4.4288511 -4.4288788 -4.4288969 -4.4288845 -4.4288282 -4.4287486 -4.4286761 -4.4286366 -4.428648 -4.4287062][-4.42881 -4.4288225 -4.4288378 -4.4288578 -4.4288816 -4.428905 -4.4289246 -4.4289384 -4.4289331 -4.4289002 -4.4288511 -4.4288015 -4.42877 -4.4287696 -4.428793][-4.4289145 -4.4289188 -4.4289217 -4.4289289 -4.4289412 -4.4289527 -4.4289603 -4.4289665 -4.4289646 -4.4289484 -4.4289212 -4.4288907 -4.4288645 -4.4288454 -4.4288278]]...]
INFO - root - 2017-12-08 05:08:18.535327: step 4210, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 20h:08m:27s remains)
INFO - root - 2017-12-08 05:08:20.754690: step 4220, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.222 sec/batch; 20h:13m:35s remains)
INFO - root - 2017-12-08 05:08:22.989717: step 4230, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 20h:09m:30s remains)
INFO - root - 2017-12-08 05:08:25.246052: step 4240, loss = 2.28, batch loss = 2.23 (35.2 examples/sec; 0.228 sec/batch; 20h:44m:48s remains)
INFO - root - 2017-12-08 05:08:27.493185: step 4250, loss = 2.28, batch loss = 2.23 (34.9 examples/sec; 0.229 sec/batch; 20h:55m:18s remains)
INFO - root - 2017-12-08 05:08:29.714103: step 4260, loss = 2.28, batch loss = 2.23 (34.8 examples/sec; 0.230 sec/batch; 20h:55m:53s remains)
INFO - root - 2017-12-08 05:08:31.974285: step 4270, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 20h:03m:31s remains)
INFO - root - 2017-12-08 05:08:34.194642: step 4280, loss = 2.28, batch loss = 2.23 (36.9 examples/sec; 0.217 sec/batch; 19h:47m:31s remains)
INFO - root - 2017-12-08 05:08:36.461745: step 4290, loss = 2.28, batch loss = 2.23 (35.6 examples/sec; 0.224 sec/batch; 20h:27m:46s remains)
INFO - root - 2017-12-08 05:08:38.694835: step 4300, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 20h:10m:19s remains)
2017-12-08 05:08:38.981188: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4286661 -4.4286656 -4.4286528 -4.4286208 -4.4285727 -4.4285274 -4.4285197 -4.4285541 -4.4285975 -4.4286366 -4.4286666 -4.428679 -4.4286728 -4.4286547 -4.4286394][-4.428647 -4.4286466 -4.4286232 -4.4285707 -4.4284987 -4.42844 -4.428441 -4.428494 -4.4285512 -4.4286041 -4.428648 -4.4286714 -4.428668 -4.4286456 -4.4286242][-4.4286346 -4.428628 -4.4285865 -4.4285097 -4.4284163 -4.4283519 -4.4283671 -4.4284387 -4.4285111 -4.4285789 -4.4286337 -4.4286609 -4.4286571 -4.428628 -4.4285951][-4.42863 -4.4286118 -4.4285517 -4.428452 -4.4283433 -4.4282784 -4.42831 -4.4284029 -4.4284968 -4.4285817 -4.4286408 -4.4286656 -4.4286551 -4.4286108 -4.4285555][-4.4286308 -4.4285951 -4.4285183 -4.428401 -4.4282842 -4.4282212 -4.4282727 -4.4283948 -4.4285116 -4.42861 -4.4286714 -4.4286909 -4.4286642 -4.4285917 -4.4285049][-4.4286337 -4.4285803 -4.4284883 -4.4283595 -4.4282374 -4.4281712 -4.42824 -4.4283886 -4.4285259 -4.4286346 -4.4286995 -4.4287148 -4.4286718 -4.4285703 -4.4284534][-4.4286284 -4.4285655 -4.4284682 -4.4283342 -4.4282041 -4.4281311 -4.4282141 -4.4283857 -4.4285398 -4.4286532 -4.4287214 -4.4287372 -4.4286847 -4.4285655 -4.4284291][-4.4286089 -4.4285507 -4.4284706 -4.4283547 -4.4282293 -4.4281583 -4.4282432 -4.4284172 -4.428575 -4.4286833 -4.4287486 -4.4287677 -4.4287186 -4.4286003 -4.4284592][-4.4285827 -4.4285426 -4.4284983 -4.4284239 -4.4283271 -4.4282746 -4.4283514 -4.4285045 -4.4286427 -4.4287281 -4.4287806 -4.4287968 -4.4287577 -4.4286561 -4.4285264][-4.428556 -4.4285355 -4.4285283 -4.4284983 -4.4284353 -4.4284015 -4.4284687 -4.428596 -4.428709 -4.4287691 -4.4288049 -4.4288135 -4.4287839 -4.4287014 -4.4285836][-4.4285288 -4.4285374 -4.4285679 -4.4285779 -4.428544 -4.4285231 -4.4285755 -4.42867 -4.4287519 -4.428791 -4.4288125 -4.4288173 -4.4287958 -4.4287248 -4.4286122][-4.4285126 -4.4285483 -4.4286089 -4.4286466 -4.428638 -4.4286284 -4.4286647 -4.4287286 -4.4287877 -4.4288139 -4.4288254 -4.4288239 -4.4287992 -4.4287262 -4.4286103][-4.4285169 -4.4285703 -4.4286437 -4.4286928 -4.4287004 -4.4287009 -4.4287248 -4.4287682 -4.428813 -4.4288363 -4.4288454 -4.4288373 -4.4288025 -4.4287252 -4.4286118][-4.428534 -4.4285936 -4.4286652 -4.4287109 -4.4287224 -4.4287233 -4.4287348 -4.4287653 -4.4288049 -4.4288311 -4.4288421 -4.4288316 -4.4287915 -4.42872 -4.4286261][-4.4285512 -4.4286079 -4.428668 -4.4286971 -4.4287009 -4.4286962 -4.4286904 -4.42871 -4.4287519 -4.4287844 -4.4288015 -4.4287949 -4.4287558 -4.4287009 -4.4286375]]...]
INFO - root - 2017-12-08 05:08:41.199350: step 4310, loss = 2.28, batch loss = 2.23 (36.8 examples/sec; 0.217 sec/batch; 19h:49m:09s remains)
INFO - root - 2017-12-08 05:08:43.455736: step 4320, loss = 2.28, batch loss = 2.23 (35.1 examples/sec; 0.228 sec/batch; 20h:47m:25s remains)
INFO - root - 2017-12-08 05:08:45.715378: step 4330, loss = 2.28, batch loss = 2.23 (34.6 examples/sec; 0.231 sec/batch; 21h:04m:28s remains)
INFO - root - 2017-12-08 05:08:47.941996: step 4340, loss = 2.28, batch loss = 2.23 (34.1 examples/sec; 0.235 sec/batch; 21h:23m:02s remains)
INFO - root - 2017-12-08 05:08:50.182640: step 4350, loss = 2.28, batch loss = 2.23 (37.7 examples/sec; 0.212 sec/batch; 19h:21m:13s remains)
INFO - root - 2017-12-08 05:08:52.420028: step 4360, loss = 2.28, batch loss = 2.23 (34.8 examples/sec; 0.230 sec/batch; 20h:58m:17s remains)
INFO - root - 2017-12-08 05:08:54.641969: step 4370, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 20h:01m:24s remains)
INFO - root - 2017-12-08 05:08:56.878533: step 4380, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.225 sec/batch; 20h:31m:01s remains)
INFO - root - 2017-12-08 05:08:59.078436: step 4390, loss = 2.28, batch loss = 2.23 (37.2 examples/sec; 0.215 sec/batch; 19h:34m:30s remains)
INFO - root - 2017-12-08 05:09:01.309902: step 4400, loss = 2.28, batch loss = 2.23 (35.6 examples/sec; 0.225 sec/batch; 20h:28m:25s remains)
2017-12-08 05:09:01.604002: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.428833 -4.42881 -4.4288135 -4.4288239 -4.4288321 -4.4288335 -4.4288268 -4.4288211 -4.428823 -4.4288306 -4.4288373 -4.4288135 -4.4287796 -4.4287324 -4.4286819][-4.4288135 -4.4287968 -4.4288077 -4.4288192 -4.4288211 -4.4288111 -4.4287934 -4.4287825 -4.4287896 -4.4288039 -4.4288111 -4.428791 -4.4287529 -4.4287038 -4.4286571][-4.4287996 -4.428791 -4.4288058 -4.4288154 -4.4288073 -4.4287863 -4.4287586 -4.4287443 -4.4287567 -4.4287825 -4.4287949 -4.4287844 -4.4287524 -4.4287071 -4.4286656][-4.4287848 -4.428782 -4.4287958 -4.428802 -4.4287848 -4.4287524 -4.4287095 -4.4286833 -4.4286938 -4.4287324 -4.4287591 -4.4287715 -4.4287586 -4.4287271 -4.4286942][-4.4287486 -4.4287581 -4.4287753 -4.4287763 -4.428751 -4.4287095 -4.4286485 -4.428607 -4.4286132 -4.4286623 -4.42871 -4.4287405 -4.42875 -4.4287424 -4.4287295][-4.4287062 -4.4287276 -4.4287438 -4.4287376 -4.4287009 -4.4286451 -4.428566 -4.4285097 -4.4285178 -4.4285879 -4.4286604 -4.4287057 -4.4287329 -4.4287472 -4.4287505][-4.4286733 -4.4286947 -4.4287052 -4.4286966 -4.4286532 -4.428587 -4.4284868 -4.42842 -4.4284372 -4.4285312 -4.4286313 -4.4286885 -4.4287224 -4.4287457 -4.4287562][-4.4286466 -4.428659 -4.4286642 -4.4286575 -4.42862 -4.4285502 -4.4284291 -4.4283462 -4.4283738 -4.428493 -4.4286165 -4.4286857 -4.4287186 -4.4287419 -4.4287519][-4.4286184 -4.4286313 -4.4286389 -4.42864 -4.4286208 -4.4285583 -4.4284291 -4.4283195 -4.4283333 -4.42846 -4.428597 -4.428679 -4.4287114 -4.4287333 -4.4287386][-4.4285927 -4.4286118 -4.4286308 -4.4286442 -4.4286456 -4.428597 -4.4284678 -4.4283328 -4.4283166 -4.4284353 -4.4285727 -4.4286628 -4.4286938 -4.4287124 -4.4287157][-4.4285908 -4.4286132 -4.4286351 -4.4286537 -4.4286575 -4.4286122 -4.428483 -4.4283385 -4.4283047 -4.4284205 -4.4285612 -4.42865 -4.4286733 -4.4286842 -4.4286909][-4.4285908 -4.4285994 -4.4286113 -4.4286304 -4.4286327 -4.42859 -4.4284639 -4.4283185 -4.428277 -4.4283943 -4.4285412 -4.428627 -4.4286427 -4.4286461 -4.4286633][-4.4285874 -4.4285722 -4.4285722 -4.428597 -4.4286146 -4.4285879 -4.4284739 -4.4283352 -4.4282885 -4.4283895 -4.4285264 -4.428607 -4.4286232 -4.4286304 -4.4286556][-4.4285917 -4.42855 -4.428544 -4.4285736 -4.4286041 -4.4286036 -4.4285269 -4.4284172 -4.428369 -4.4284353 -4.4285455 -4.4286189 -4.4286466 -4.4286628 -4.4286795][-4.4286108 -4.4285522 -4.42854 -4.428565 -4.4286013 -4.4286218 -4.42858 -4.4284945 -4.4284391 -4.4284792 -4.4285722 -4.42865 -4.4286909 -4.4287081 -4.4287119]]...]
INFO - root - 2017-12-08 05:09:03.832315: step 4410, loss = 2.28, batch loss = 2.23 (37.2 examples/sec; 0.215 sec/batch; 19h:37m:19s remains)
INFO - root - 2017-12-08 05:09:06.110336: step 4420, loss = 2.28, batch loss = 2.23 (34.3 examples/sec; 0.233 sec/batch; 21h:13m:44s remains)
INFO - root - 2017-12-08 05:09:08.349371: step 4430, loss = 2.28, batch loss = 2.23 (34.8 examples/sec; 0.230 sec/batch; 20h:55m:36s remains)
INFO - root - 2017-12-08 05:09:10.585211: step 4440, loss = 2.28, batch loss = 2.23 (35.2 examples/sec; 0.228 sec/batch; 20h:44m:07s remains)
INFO - root - 2017-12-08 05:09:12.810341: step 4450, loss = 2.28, batch loss = 2.23 (37.7 examples/sec; 0.212 sec/batch; 19h:18m:41s remains)
INFO - root - 2017-12-08 05:09:15.045909: step 4460, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 20h:17m:17s remains)
INFO - root - 2017-12-08 05:09:17.267819: step 4470, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.220 sec/batch; 20h:03m:42s remains)
INFO - root - 2017-12-08 05:09:19.499313: step 4480, loss = 2.28, batch loss = 2.23 (37.3 examples/sec; 0.214 sec/batch; 19h:32m:30s remains)
INFO - root - 2017-12-08 05:09:21.694986: step 4490, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 19h:57m:25s remains)
INFO - root - 2017-12-08 05:09:23.959282: step 4500, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 20h:17m:43s remains)
2017-12-08 05:09:24.247754: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289684 -4.4289703 -4.4289684 -4.4289536 -4.4289322 -4.4289031 -4.4288635 -4.428833 -4.4288354 -4.4288373 -4.4288492 -4.4288836 -4.4289136 -4.4289322 -4.4289351][-4.4289775 -4.4289765 -4.4289675 -4.4289384 -4.4289007 -4.4288554 -4.4287853 -4.4287367 -4.4287558 -4.4287753 -4.4287996 -4.4288535 -4.4289021 -4.4289303 -4.4289346][-4.428977 -4.4289756 -4.4289613 -4.4289203 -4.4288597 -4.4287877 -4.4286809 -4.4286275 -4.4286895 -4.4287415 -4.4287758 -4.4288363 -4.428896 -4.4289351 -4.4289403][-4.4289641 -4.4289565 -4.42894 -4.4288878 -4.42881 -4.4287033 -4.4285593 -4.4285173 -4.428628 -4.42871 -4.4287505 -4.4288144 -4.42888 -4.4289308 -4.4289384][-4.4289412 -4.4289279 -4.4289031 -4.4288378 -4.4287386 -4.4285903 -4.4284105 -4.4283943 -4.4285569 -4.4286733 -4.4287276 -4.4288011 -4.4288716 -4.4289284 -4.4289351][-4.4289193 -4.428906 -4.42887 -4.4287896 -4.4286637 -4.4284477 -4.4282069 -4.4282255 -4.4284563 -4.4286156 -4.4286942 -4.4287848 -4.4288654 -4.428925 -4.4289346][-4.4289074 -4.4289031 -4.4288669 -4.4287834 -4.4286451 -4.428371 -4.4280548 -4.4280958 -4.4283924 -4.4285841 -4.42868 -4.428781 -4.4288731 -4.4289336 -4.4289441][-4.4289041 -4.4289174 -4.4288874 -4.4288163 -4.4286962 -4.4284472 -4.4281535 -4.4281807 -4.4284506 -4.4286246 -4.4287133 -4.4288087 -4.428895 -4.4289436 -4.4289527][-4.4288945 -4.4289155 -4.4288964 -4.4288483 -4.4287634 -4.4285874 -4.4283957 -4.4284186 -4.4285941 -4.4287043 -4.4287672 -4.428843 -4.42891 -4.4289417 -4.4289513][-4.4288731 -4.4288821 -4.4288673 -4.428844 -4.4287925 -4.42868 -4.4285707 -4.4286017 -4.4287162 -4.4287825 -4.4288182 -4.4288688 -4.4289165 -4.4289346 -4.4289427][-4.4288454 -4.42884 -4.4288292 -4.42882 -4.4287863 -4.4287162 -4.428658 -4.4286919 -4.4287767 -4.42883 -4.4288511 -4.4288754 -4.4289 -4.4289069 -4.428915][-4.428813 -4.4288116 -4.4288082 -4.4288015 -4.4287786 -4.4287405 -4.4287157 -4.4287443 -4.4287987 -4.4288354 -4.4288459 -4.4288497 -4.4288578 -4.4288607 -4.428865][-4.4287891 -4.4287992 -4.4288015 -4.4287906 -4.4287782 -4.4287653 -4.4287643 -4.4287844 -4.4288087 -4.428822 -4.4288211 -4.4288144 -4.4288168 -4.4288182 -4.4288182][-4.4287953 -4.4288139 -4.4288154 -4.428803 -4.4287963 -4.4287915 -4.4287996 -4.428813 -4.428822 -4.42882 -4.4288106 -4.4287963 -4.4287891 -4.4287815 -4.4287744][-4.4288239 -4.4288492 -4.4288516 -4.4288378 -4.4288273 -4.4288144 -4.4288106 -4.4288116 -4.4288116 -4.4288049 -4.4287982 -4.4287872 -4.4287748 -4.4287624 -4.4287519]]...]
INFO - root - 2017-12-08 05:09:26.472411: step 4510, loss = 2.28, batch loss = 2.23 (36.7 examples/sec; 0.218 sec/batch; 19h:50m:56s remains)
INFO - root - 2017-12-08 05:09:28.702075: step 4520, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 19h:58m:17s remains)
INFO - root - 2017-12-08 05:09:30.926078: step 4530, loss = 2.28, batch loss = 2.23 (35.0 examples/sec; 0.229 sec/batch; 20h:50m:07s remains)
INFO - root - 2017-12-08 05:09:33.173897: step 4540, loss = 2.28, batch loss = 2.23 (35.0 examples/sec; 0.229 sec/batch; 20h:50m:20s remains)
INFO - root - 2017-12-08 05:09:35.426930: step 4550, loss = 2.28, batch loss = 2.23 (34.2 examples/sec; 0.234 sec/batch; 21h:20m:05s remains)
INFO - root - 2017-12-08 05:09:37.646379: step 4560, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.220 sec/batch; 20h:04m:42s remains)
INFO - root - 2017-12-08 05:09:39.874238: step 4570, loss = 2.28, batch loss = 2.23 (37.6 examples/sec; 0.213 sec/batch; 19h:22m:10s remains)
INFO - root - 2017-12-08 05:09:42.108539: step 4580, loss = 2.28, batch loss = 2.23 (35.7 examples/sec; 0.224 sec/batch; 20h:26m:01s remains)
INFO - root - 2017-12-08 05:09:44.371891: step 4590, loss = 2.28, batch loss = 2.23 (35.2 examples/sec; 0.227 sec/batch; 20h:42m:17s remains)
INFO - root - 2017-12-08 05:09:46.604597: step 4600, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 20h:06m:32s remains)
2017-12-08 05:09:46.878606: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289479 -4.4289289 -4.4289107 -4.4289021 -4.4289007 -4.4289 -4.4289021 -4.4289117 -4.4289269 -4.4289384 -4.4289403 -4.4289355 -4.4289331 -4.4289341 -4.4289312][-4.4289336 -4.4289055 -4.4288816 -4.4288745 -4.4288735 -4.4288678 -4.4288626 -4.4288712 -4.4288917 -4.42891 -4.428915 -4.4289103 -4.4289064 -4.4289045 -4.4288974][-4.4289083 -4.4288712 -4.4288421 -4.4288363 -4.4288387 -4.4288297 -4.4288139 -4.4288192 -4.4288468 -4.4288726 -4.4288816 -4.4288836 -4.4288831 -4.4288793 -4.4288645][-4.4288845 -4.4288392 -4.4287987 -4.4287858 -4.4287896 -4.4287782 -4.4287524 -4.4287539 -4.4287925 -4.4288321 -4.42885 -4.42886 -4.4288688 -4.4288669 -4.4288468][-4.42887 -4.4288216 -4.428772 -4.4287496 -4.4287472 -4.4287271 -4.4286819 -4.4286685 -4.4287176 -4.4287791 -4.4288149 -4.4288392 -4.428863 -4.4288721 -4.4288526][-4.4288721 -4.4288259 -4.4287734 -4.4287419 -4.4287238 -4.4286795 -4.4285955 -4.4285512 -4.42861 -4.4287033 -4.4287648 -4.428812 -4.4288607 -4.4288907 -4.428884][-4.4288845 -4.4288421 -4.4287891 -4.428751 -4.4287128 -4.4286308 -4.4284906 -4.4283934 -4.4284539 -4.4285889 -4.4286885 -4.4287629 -4.4288383 -4.4288931 -4.4289083][-4.4289045 -4.4288692 -4.4288197 -4.4287806 -4.4287367 -4.4286313 -4.4284444 -4.42828 -4.42831 -4.428463 -4.4285941 -4.4286938 -4.4287872 -4.4288568 -4.428894][-4.4289217 -4.4288983 -4.4288592 -4.4288254 -4.42879 -4.428699 -4.4285216 -4.4283357 -4.4283061 -4.4284139 -4.4285336 -4.428638 -4.4287329 -4.4288034 -4.4288473][-4.4289289 -4.4289179 -4.4288936 -4.4288678 -4.4288445 -4.4287829 -4.4286489 -4.4284945 -4.4284334 -4.428473 -4.4285445 -4.4286208 -4.4286962 -4.428751 -4.4287868][-4.4289222 -4.4289188 -4.42891 -4.4288969 -4.4288864 -4.4288507 -4.4287596 -4.42865 -4.4285917 -4.4285932 -4.4286213 -4.4286561 -4.4286966 -4.4287257 -4.4287443][-4.4289012 -4.4288974 -4.4288983 -4.4288993 -4.4289021 -4.4288893 -4.4288368 -4.42877 -4.428731 -4.4287186 -4.428721 -4.4287233 -4.4287267 -4.4287314 -4.4287357][-4.4288807 -4.428874 -4.4288788 -4.4288907 -4.4289017 -4.4289002 -4.4288731 -4.4288416 -4.4288268 -4.4288158 -4.4288077 -4.4287939 -4.4287748 -4.4287639 -4.4287615][-4.4288769 -4.4288678 -4.4288726 -4.4288888 -4.4288993 -4.4288979 -4.4288816 -4.4288721 -4.4288783 -4.4288759 -4.4288688 -4.4288511 -4.4288249 -4.42881 -4.4288068][-4.4288878 -4.4288774 -4.42888 -4.4288926 -4.4288964 -4.4288888 -4.428874 -4.4288745 -4.4288917 -4.428896 -4.4288926 -4.4288774 -4.4288497 -4.428834 -4.428834]]...]
INFO - root - 2017-12-08 05:09:49.107923: step 4610, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 19h:57m:24s remains)
INFO - root - 2017-12-08 05:09:51.343902: step 4620, loss = 2.28, batch loss = 2.23 (36.0 examples/sec; 0.222 sec/batch; 20h:14m:42s remains)
INFO - root - 2017-12-08 05:09:53.595738: step 4630, loss = 2.28, batch loss = 2.23 (34.8 examples/sec; 0.230 sec/batch; 20h:56m:28s remains)
INFO - root - 2017-12-08 05:09:55.840029: step 4640, loss = 2.28, batch loss = 2.23 (34.7 examples/sec; 0.230 sec/batch; 20h:58m:43s remains)
INFO - root - 2017-12-08 05:09:58.071182: step 4650, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 20h:00m:26s remains)
INFO - root - 2017-12-08 05:10:00.323438: step 4660, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 19h:56m:17s remains)
INFO - root - 2017-12-08 05:10:02.551243: step 4670, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 19h:58m:48s remains)
INFO - root - 2017-12-08 05:10:04.777679: step 4680, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.222 sec/batch; 20h:10m:26s remains)
INFO - root - 2017-12-08 05:10:07.050554: step 4690, loss = 2.28, batch loss = 2.23 (34.4 examples/sec; 0.233 sec/batch; 21h:10m:41s remains)
INFO - root - 2017-12-08 05:10:09.274608: step 4700, loss = 2.28, batch loss = 2.23 (35.7 examples/sec; 0.224 sec/batch; 20h:25m:42s remains)
2017-12-08 05:10:09.562258: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.428936 -4.4289513 -4.4289613 -4.4289665 -4.4289608 -4.4289484 -4.4289446 -4.4289451 -4.4289403 -4.4289246 -4.4289002 -4.4288664 -4.4288244 -4.4287786 -4.4287472][-4.4289436 -4.428967 -4.4289842 -4.4289913 -4.4289818 -4.4289632 -4.4289503 -4.4289403 -4.428925 -4.4288964 -4.4288564 -4.4288111 -4.4287648 -4.4287229 -4.4287047][-4.4289265 -4.42896 -4.4289808 -4.4289823 -4.4289622 -4.4289351 -4.428915 -4.428894 -4.42887 -4.4288335 -4.4287834 -4.4287314 -4.4286871 -4.4286537 -4.4286504][-4.4288826 -4.4289188 -4.4289389 -4.4289322 -4.4288955 -4.428854 -4.4288278 -4.4288077 -4.4287863 -4.4287515 -4.4287076 -4.4286594 -4.4286227 -4.4286027 -4.4286113][-4.4288058 -4.4288287 -4.4288392 -4.4288168 -4.4287596 -4.4287057 -4.42869 -4.4286942 -4.4286928 -4.4286776 -4.4286523 -4.4286237 -4.4286056 -4.4286017 -4.4286184][-4.4287004 -4.4286985 -4.4286928 -4.4286518 -4.42857 -4.4285069 -4.4285221 -4.4285779 -4.4286218 -4.4286337 -4.4286237 -4.4286089 -4.4286084 -4.4286289 -4.4286637][-4.4286337 -4.4286032 -4.4285765 -4.4285088 -4.428391 -4.428308 -4.4283528 -4.4284649 -4.4285574 -4.4285975 -4.4285979 -4.4285884 -4.4286013 -4.4286485 -4.4287076][-4.4286566 -4.4286156 -4.4285803 -4.4285073 -4.4283786 -4.4282789 -4.4283161 -4.4284363 -4.4285407 -4.4285903 -4.4285936 -4.4285817 -4.4285889 -4.4286437 -4.4287167][-4.4287148 -4.4286852 -4.4286637 -4.4286132 -4.4285173 -4.4284377 -4.4284511 -4.4285297 -4.4286065 -4.4286423 -4.42864 -4.4286218 -4.428606 -4.4286313 -4.4286842][-4.4287658 -4.4287529 -4.4287434 -4.4287186 -4.4286628 -4.4286137 -4.4286156 -4.4286575 -4.4287 -4.4287186 -4.4287066 -4.4286809 -4.4286456 -4.4286332 -4.4286532][-4.4287863 -4.4287858 -4.4287887 -4.4287872 -4.4287663 -4.4287438 -4.4287453 -4.4287667 -4.428791 -4.4287996 -4.4287796 -4.4287381 -4.4286795 -4.4286351 -4.4286251][-4.4287887 -4.4287992 -4.4288192 -4.4288363 -4.4288297 -4.4288111 -4.4288054 -4.4288125 -4.4288287 -4.4288387 -4.4288182 -4.428771 -4.4287062 -4.428647 -4.4286132][-4.4288187 -4.4288335 -4.4288607 -4.4288774 -4.4288592 -4.4288244 -4.4288058 -4.4288058 -4.4288249 -4.4288397 -4.4288216 -4.4287777 -4.4287214 -4.4286666 -4.4286251][-4.4288831 -4.4288945 -4.4289112 -4.4289069 -4.428864 -4.4288039 -4.4287682 -4.428762 -4.4287815 -4.4288063 -4.4287996 -4.4287677 -4.4287252 -4.4286857 -4.4286509][-4.4289393 -4.4289441 -4.4289422 -4.4289136 -4.4288516 -4.4287777 -4.4287314 -4.42872 -4.4287419 -4.4287786 -4.4287906 -4.4287758 -4.4287443 -4.4287143 -4.42869]]...]
INFO - root - 2017-12-08 05:10:11.799214: step 4710, loss = 2.28, batch loss = 2.23 (35.4 examples/sec; 0.226 sec/batch; 20h:33m:56s remains)
INFO - root - 2017-12-08 05:10:14.045843: step 4720, loss = 2.28, batch loss = 2.23 (35.4 examples/sec; 0.226 sec/batch; 20h:34m:40s remains)
INFO - root - 2017-12-08 05:10:16.287647: step 4730, loss = 2.28, batch loss = 2.23 (33.6 examples/sec; 0.238 sec/batch; 21h:40m:18s remains)
INFO - root - 2017-12-08 05:10:18.504326: step 4740, loss = 2.28, batch loss = 2.23 (37.3 examples/sec; 0.215 sec/batch; 19h:32m:15s remains)
INFO - root - 2017-12-08 05:10:20.735429: step 4750, loss = 2.28, batch loss = 2.23 (36.7 examples/sec; 0.218 sec/batch; 19h:49m:52s remains)
INFO - root - 2017-12-08 05:10:23.003847: step 4760, loss = 2.28, batch loss = 2.23 (36.8 examples/sec; 0.217 sec/batch; 19h:47m:25s remains)
INFO - root - 2017-12-08 05:10:25.303617: step 4770, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.225 sec/batch; 20h:30m:11s remains)
INFO - root - 2017-12-08 05:10:27.532470: step 4780, loss = 2.28, batch loss = 2.23 (36.7 examples/sec; 0.218 sec/batch; 19h:50m:33s remains)
INFO - root - 2017-12-08 05:10:29.795788: step 4790, loss = 2.28, batch loss = 2.23 (36.0 examples/sec; 0.222 sec/batch; 20h:12m:10s remains)
INFO - root - 2017-12-08 05:10:32.016405: step 4800, loss = 2.28, batch loss = 2.23 (37.1 examples/sec; 0.216 sec/batch; 19h:39m:06s remains)
2017-12-08 05:10:32.323341: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288325 -4.42883 -4.4288282 -4.4288278 -4.4288344 -4.4288411 -4.428853 -4.4288788 -4.4289012 -4.4289126 -4.4289036 -4.4288783 -4.428865 -4.4288769 -4.4289007][-4.4287539 -4.4287529 -4.4287562 -4.4287572 -4.4287624 -4.4287715 -4.4287868 -4.4288206 -4.4288578 -4.4288878 -4.4289007 -4.4288936 -4.4288845 -4.4288917 -4.4289117][-4.4286861 -4.4286885 -4.4286928 -4.4286909 -4.4286895 -4.4286942 -4.4287086 -4.4287472 -4.4287953 -4.4288411 -4.4288716 -4.4288821 -4.428884 -4.4288945 -4.4289136][-4.4286547 -4.4286637 -4.428668 -4.4286585 -4.4286447 -4.4286385 -4.4286447 -4.4286776 -4.4287305 -4.4287848 -4.4288263 -4.4288511 -4.4288669 -4.428885 -4.4289079][-4.4286337 -4.4286447 -4.4286447 -4.4286294 -4.4286056 -4.42859 -4.4285879 -4.4286175 -4.4286709 -4.4287271 -4.4287715 -4.4288025 -4.4288273 -4.4288511 -4.4288778][-4.4286289 -4.428628 -4.4286084 -4.4285722 -4.4285245 -4.4284883 -4.4284749 -4.4285111 -4.4285874 -4.4286613 -4.4287109 -4.4287457 -4.4287724 -4.4287958 -4.4288197][-4.4286551 -4.4286308 -4.4285789 -4.428503 -4.4284172 -4.4283423 -4.4282942 -4.4283452 -4.4284725 -4.428587 -4.4286542 -4.4286914 -4.4287133 -4.4287319 -4.4287519][-4.4287143 -4.4286733 -4.4285874 -4.428472 -4.4283466 -4.4282227 -4.4281244 -4.4281712 -4.4283452 -4.4285078 -4.4286003 -4.4286451 -4.4286642 -4.4286785 -4.4286966][-4.4287791 -4.4287362 -4.4286375 -4.4285035 -4.4283581 -4.42821 -4.4280853 -4.4281087 -4.4282737 -4.42844 -4.428545 -4.4285975 -4.4286237 -4.4286432 -4.4286675][-4.4288297 -4.4287987 -4.4287124 -4.4285908 -4.4284549 -4.4283228 -4.4282126 -4.4282079 -4.4283147 -4.4284344 -4.4285192 -4.4285674 -4.4285989 -4.4286256 -4.4286575][-4.4288573 -4.42885 -4.4287939 -4.4287028 -4.428596 -4.4284997 -4.42842 -4.428401 -4.42845 -4.4285107 -4.4285622 -4.4285932 -4.4286141 -4.4286327 -4.4286528][-4.42886 -4.4288759 -4.4288507 -4.4287896 -4.4287138 -4.4286518 -4.4286027 -4.4285874 -4.4286089 -4.4286356 -4.428659 -4.4286718 -4.428678 -4.4286752 -4.4286656][-4.4288416 -4.4288707 -4.4288669 -4.42883 -4.4287825 -4.4287467 -4.4287267 -4.4287243 -4.4287376 -4.42875 -4.4287577 -4.4287596 -4.4287586 -4.4287429 -4.428709][-4.4288034 -4.4288292 -4.4288392 -4.4288259 -4.4288039 -4.428791 -4.428793 -4.428803 -4.4288149 -4.4288211 -4.4288206 -4.4288182 -4.428823 -4.4288158 -4.4287825][-4.4287586 -4.4287767 -4.428791 -4.4287915 -4.4287858 -4.4287891 -4.4288082 -4.4288273 -4.4288421 -4.4288511 -4.4288497 -4.4288507 -4.4288654 -4.4288731 -4.4288549]]...]
INFO - root - 2017-12-08 05:10:34.565495: step 4810, loss = 2.28, batch loss = 2.23 (36.7 examples/sec; 0.218 sec/batch; 19h:50m:42s remains)
INFO - root - 2017-12-08 05:10:36.795081: step 4820, loss = 2.28, batch loss = 2.23 (35.1 examples/sec; 0.228 sec/batch; 20h:46m:00s remains)
INFO - root - 2017-12-08 05:10:39.030947: step 4830, loss = 2.28, batch loss = 2.23 (35.4 examples/sec; 0.226 sec/batch; 20h:33m:06s remains)
INFO - root - 2017-12-08 05:10:41.264945: step 4840, loss = 2.28, batch loss = 2.23 (34.5 examples/sec; 0.232 sec/batch; 21h:05m:16s remains)
INFO - root - 2017-12-08 05:10:43.489474: step 4850, loss = 2.28, batch loss = 2.23 (35.3 examples/sec; 0.226 sec/batch; 20h:36m:16s remains)
INFO - root - 2017-12-08 05:10:45.717661: step 4860, loss = 2.28, batch loss = 2.23 (34.9 examples/sec; 0.229 sec/batch; 20h:51m:20s remains)
INFO - root - 2017-12-08 05:10:47.943052: step 4870, loss = 2.28, batch loss = 2.23 (37.2 examples/sec; 0.215 sec/batch; 19h:35m:27s remains)
INFO - root - 2017-12-08 05:10:50.166382: step 4880, loss = 2.28, batch loss = 2.23 (37.2 examples/sec; 0.215 sec/batch; 19h:33m:59s remains)
INFO - root - 2017-12-08 05:10:52.421030: step 4890, loss = 2.28, batch loss = 2.23 (34.5 examples/sec; 0.232 sec/batch; 21h:06m:16s remains)
INFO - root - 2017-12-08 05:10:54.697394: step 4900, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.225 sec/batch; 20h:29m:42s remains)
2017-12-08 05:10:54.980187: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4286404 -4.4287395 -4.428812 -4.4288287 -4.4287829 -4.428721 -4.4287004 -4.4287243 -4.42876 -4.4288154 -4.4288816 -4.4289241 -4.4289379 -4.4289117 -4.4288549][-4.428647 -4.4287567 -4.4288383 -4.4288588 -4.4288154 -4.4287515 -4.4287224 -4.4287372 -4.4287796 -4.428844 -4.4289126 -4.4289441 -4.4289527 -4.4289308 -4.4288712][-4.4286232 -4.4287376 -4.4288268 -4.4288535 -4.4288158 -4.42875 -4.4287086 -4.4287162 -4.42877 -4.4288487 -4.4289217 -4.4289446 -4.4289451 -4.4289269 -4.4288707][-4.4285774 -4.4286919 -4.4287863 -4.4288206 -4.4287872 -4.4287162 -4.428659 -4.428659 -4.4287271 -4.4288259 -4.4289064 -4.4289303 -4.4289279 -4.4289107 -4.4288568][-4.4285488 -4.4286504 -4.4287405 -4.428782 -4.4287534 -4.4286737 -4.428596 -4.4285765 -4.4286494 -4.4287677 -4.4288626 -4.4288993 -4.4289012 -4.4288745 -4.42881][-4.4285612 -4.4286451 -4.4287229 -4.428762 -4.4287386 -4.4286537 -4.4285545 -4.428493 -4.4285531 -4.4286938 -4.4288092 -4.4288621 -4.4288721 -4.4288349 -4.4287529][-4.4286156 -4.42868 -4.4287357 -4.4287596 -4.4287386 -4.428659 -4.4285517 -4.4284563 -4.4284983 -4.4286504 -4.4287806 -4.4288483 -4.428863 -4.42882 -4.4287295][-4.428679 -4.428719 -4.4287438 -4.4287395 -4.428709 -4.4286423 -4.4285564 -4.4284754 -4.4285183 -4.4286618 -4.4287882 -4.4288635 -4.428885 -4.4288464 -4.4287562][-4.428721 -4.4287329 -4.428721 -4.4286857 -4.42864 -4.4285855 -4.4285336 -4.4285016 -4.4285645 -4.4286957 -4.4288106 -4.4288826 -4.4289055 -4.4288793 -4.4288068][-4.4287271 -4.4287133 -4.4286747 -4.4286213 -4.4285674 -4.4285254 -4.4285131 -4.4285412 -4.42862 -4.4287324 -4.4288282 -4.4288836 -4.4289007 -4.428884 -4.4288378][-4.4287143 -4.4286871 -4.4286408 -4.4285855 -4.4285345 -4.4285083 -4.4285297 -4.4285917 -4.4286761 -4.42877 -4.4288406 -4.4288745 -4.4288821 -4.4288726 -4.428854][-4.4287281 -4.4287004 -4.4286566 -4.4286065 -4.4285626 -4.4285502 -4.4285908 -4.4286656 -4.4287443 -4.4288173 -4.428863 -4.4288754 -4.4288731 -4.4288678 -4.42887][-4.4287758 -4.4287529 -4.4287152 -4.4286752 -4.4286408 -4.4286356 -4.4286823 -4.4287558 -4.4288192 -4.4288735 -4.4288988 -4.428894 -4.4288821 -4.428874 -4.4288831][-4.4288392 -4.4288268 -4.4288011 -4.4287763 -4.4287548 -4.4287515 -4.4287891 -4.4288468 -4.4288888 -4.4289203 -4.4289312 -4.4289184 -4.4289041 -4.4288969 -4.428906][-4.428884 -4.4288845 -4.4288735 -4.428863 -4.4288549 -4.4288511 -4.4288712 -4.428905 -4.428925 -4.4289351 -4.428936 -4.42892 -4.4289041 -4.4288974 -4.428905]]...]
INFO - root - 2017-12-08 05:10:57.215340: step 4910, loss = 2.28, batch loss = 2.23 (36.0 examples/sec; 0.222 sec/batch; 20h:13m:23s remains)
INFO - root - 2017-12-08 05:10:59.440114: step 4920, loss = 2.28, batch loss = 2.23 (35.1 examples/sec; 0.228 sec/batch; 20h:42m:50s remains)
INFO - root - 2017-12-08 05:11:01.656395: step 4930, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.225 sec/batch; 20h:30m:29s remains)
INFO - root - 2017-12-08 05:11:03.861720: step 4940, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.225 sec/batch; 20h:31m:02s remains)
INFO - root - 2017-12-08 05:11:06.132590: step 4950, loss = 2.28, batch loss = 2.23 (34.7 examples/sec; 0.230 sec/batch; 20h:58m:18s remains)
INFO - root - 2017-12-08 05:11:08.370744: step 4960, loss = 2.28, batch loss = 2.23 (34.3 examples/sec; 0.233 sec/batch; 21h:13m:10s remains)
INFO - root - 2017-12-08 05:11:10.585899: step 4970, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 20h:06m:04s remains)
INFO - root - 2017-12-08 05:11:12.807401: step 4980, loss = 2.28, batch loss = 2.23 (36.7 examples/sec; 0.218 sec/batch; 19h:51m:19s remains)
INFO - root - 2017-12-08 05:11:15.042742: step 4990, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 20h:01m:14s remains)
INFO - root - 2017-12-08 05:11:17.281495: step 5000, loss = 2.28, batch loss = 2.23 (35.4 examples/sec; 0.226 sec/batch; 20h:33m:21s remains)
2017-12-08 05:11:17.565021: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289637 -4.4289684 -4.42897 -4.4289718 -4.4289746 -4.4289756 -4.4289694 -4.4289584 -4.4289513 -4.4289489 -4.42895 -4.4289589 -4.4289694 -4.4289751 -4.4289804][-4.4289007 -4.4289083 -4.4289117 -4.4289184 -4.4289236 -4.4289255 -4.4289165 -4.4288993 -4.4288864 -4.4288793 -4.4288778 -4.4288888 -4.4289007 -4.4289064 -4.4289184][-4.428771 -4.4287825 -4.428792 -4.4288058 -4.4288168 -4.4288177 -4.4288025 -4.4287696 -4.4287505 -4.4287419 -4.4287424 -4.4287586 -4.4287786 -4.4287915 -4.4288149][-4.4286046 -4.428616 -4.4286275 -4.4286408 -4.4286413 -4.4286313 -4.4286003 -4.4285541 -4.4285417 -4.4285469 -4.42856 -4.428596 -4.4286346 -4.4286561 -4.4286885][-4.4284725 -4.4284716 -4.4284744 -4.4284711 -4.4284449 -4.4283986 -4.4283361 -4.4282842 -4.428298 -4.4283371 -4.4283757 -4.4284348 -4.4284983 -4.4285421 -4.428596][-4.428421 -4.4283996 -4.4283738 -4.4283271 -4.4282403 -4.4281149 -4.4279938 -4.4279566 -4.4280372 -4.4281397 -4.4282241 -4.428308 -4.428381 -4.4284515 -4.4285412][-4.4284487 -4.4284143 -4.4283595 -4.428268 -4.4281063 -4.4278746 -4.4276628 -4.4276547 -4.4278355 -4.4280143 -4.4281483 -4.4282532 -4.4283285 -4.4284186 -4.4285288][-4.4285064 -4.4284744 -4.4284172 -4.4283271 -4.428175 -4.4279389 -4.4277196 -4.4277272 -4.4279237 -4.42811 -4.4282446 -4.4283385 -4.4283986 -4.4284797 -4.4285755][-4.4285645 -4.4285245 -4.4284773 -4.4284277 -4.4283528 -4.4282193 -4.4280896 -4.4281015 -4.4282279 -4.4283423 -4.4284239 -4.4284706 -4.4284983 -4.4285536 -4.4286237][-4.4286385 -4.428597 -4.4285588 -4.4285336 -4.4285121 -4.4284663 -4.4284129 -4.428421 -4.4284825 -4.4285336 -4.4285669 -4.4285769 -4.4285812 -4.4286141 -4.4286618][-4.4286833 -4.4286628 -4.4286432 -4.4286366 -4.4286427 -4.4286332 -4.4286036 -4.4286008 -4.4286323 -4.4286642 -4.4286795 -4.428678 -4.42867 -4.4286814 -4.4287105][-4.4287033 -4.4286876 -4.4286647 -4.4286556 -4.4286709 -4.428679 -4.4286642 -4.4286647 -4.4286861 -4.4287143 -4.4287324 -4.4287353 -4.4287224 -4.4287233 -4.428741][-4.42873 -4.4286957 -4.4286504 -4.4286261 -4.4286337 -4.4286451 -4.4286427 -4.4286475 -4.4286642 -4.4286933 -4.4287167 -4.4287286 -4.4287257 -4.4287281 -4.4287381][-4.4287243 -4.428688 -4.428637 -4.42861 -4.4286146 -4.4286227 -4.4286156 -4.4286146 -4.4286351 -4.4286723 -4.4286962 -4.4287138 -4.4287195 -4.4287186 -4.4287224][-4.4287014 -4.4286661 -4.4286208 -4.4286051 -4.4286227 -4.42863 -4.4286089 -4.4285994 -4.4286261 -4.4286728 -4.4286914 -4.4287024 -4.4287086 -4.4287157 -4.4287243]]...]
INFO - root - 2017-12-08 05:11:19.802565: step 5010, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 20h:00m:46s remains)
INFO - root - 2017-12-08 05:11:22.045446: step 5020, loss = 2.28, batch loss = 2.23 (37.1 examples/sec; 0.216 sec/batch; 19h:37m:03s remains)
INFO - root - 2017-12-08 05:11:24.355027: step 5030, loss = 2.28, batch loss = 2.23 (32.4 examples/sec; 0.247 sec/batch; 22h:27m:55s remains)
INFO - root - 2017-12-08 05:11:26.600901: step 5040, loss = 2.28, batch loss = 2.23 (36.6 examples/sec; 0.219 sec/batch; 19h:54m:32s remains)
INFO - root - 2017-12-08 05:11:28.823721: step 5050, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.225 sec/batch; 20h:29m:12s remains)
INFO - root - 2017-12-08 05:11:31.069634: step 5060, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.220 sec/batch; 20h:02m:56s remains)
INFO - root - 2017-12-08 05:11:33.336164: step 5070, loss = 2.28, batch loss = 2.23 (35.0 examples/sec; 0.229 sec/batch; 20h:48m:53s remains)
INFO - root - 2017-12-08 05:11:35.579687: step 5080, loss = 2.28, batch loss = 2.23 (36.8 examples/sec; 0.217 sec/batch; 19h:46m:49s remains)
INFO - root - 2017-12-08 05:11:37.841914: step 5090, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.226 sec/batch; 20h:30m:56s remains)
INFO - root - 2017-12-08 05:11:40.105937: step 5100, loss = 2.28, batch loss = 2.23 (35.3 examples/sec; 0.227 sec/batch; 20h:37m:27s remains)
2017-12-08 05:11:40.395478: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288354 -4.4288459 -4.4288578 -4.4288735 -4.4288669 -4.4288421 -4.4288058 -4.4287691 -4.4287243 -4.428688 -4.4286585 -4.4286466 -4.4286451 -4.4286537 -4.4286938][-4.4288387 -4.428853 -4.4288712 -4.4288898 -4.4288855 -4.4288673 -4.4288421 -4.4288039 -4.4287443 -4.4286828 -4.4286304 -4.428607 -4.4286122 -4.4286389 -4.4286942][-4.4288239 -4.4288445 -4.4288673 -4.4288816 -4.4288778 -4.4288669 -4.4288549 -4.428813 -4.4287381 -4.4286571 -4.4285955 -4.4285703 -4.4285913 -4.4286413 -4.4287105][-4.4287949 -4.4288235 -4.428853 -4.4288707 -4.4288692 -4.428865 -4.4288559 -4.4288063 -4.4287195 -4.4286361 -4.4285836 -4.4285641 -4.4285989 -4.4286661 -4.428741][-4.4287734 -4.4288049 -4.4288383 -4.428854 -4.4288554 -4.428854 -4.4288344 -4.4287796 -4.4287033 -4.4286323 -4.4285941 -4.4285803 -4.4286203 -4.4286928 -4.428762][-4.4287586 -4.4287877 -4.4288144 -4.4288306 -4.4288397 -4.4288321 -4.428793 -4.4287262 -4.4286613 -4.4286175 -4.4286036 -4.4286046 -4.4286394 -4.4287014 -4.4287577][-4.4287372 -4.428772 -4.4287906 -4.428802 -4.4287996 -4.4287686 -4.4287043 -4.428628 -4.4285779 -4.4285688 -4.4285917 -4.428617 -4.4286542 -4.4286995 -4.4287395][-4.4287252 -4.4287734 -4.4287882 -4.428781 -4.4287443 -4.4286771 -4.4285736 -4.4284725 -4.428432 -4.4284697 -4.4285426 -4.4286051 -4.4286547 -4.4286885 -4.4287143][-4.4287224 -4.4287834 -4.4287939 -4.4287643 -4.42869 -4.4285674 -4.4284124 -4.4282756 -4.4282622 -4.4283619 -4.4284863 -4.4285803 -4.4286404 -4.4286671 -4.428679][-4.4287415 -4.4287982 -4.4287934 -4.4287405 -4.4286351 -4.4284759 -4.4282961 -4.4281673 -4.4282088 -4.4283562 -4.428493 -4.4285865 -4.4286461 -4.4286675 -4.428669][-4.42878 -4.4288144 -4.428791 -4.4287138 -4.4286003 -4.4284472 -4.4283018 -4.4282479 -4.4283476 -4.4284859 -4.4285812 -4.4286437 -4.4286866 -4.4287019 -4.4286985][-4.4288316 -4.4288387 -4.4287891 -4.4286957 -4.4285846 -4.4284678 -4.4283895 -4.4284086 -4.4285264 -4.4286404 -4.4287038 -4.4287395 -4.4287648 -4.4287729 -4.4287691][-4.4288888 -4.4288707 -4.428793 -4.428679 -4.42856 -4.428472 -4.4284477 -4.428514 -4.4286308 -4.4287257 -4.4287758 -4.428803 -4.4288249 -4.4288368 -4.4288435][-4.4289193 -4.4288816 -4.42878 -4.4286404 -4.4285054 -4.428441 -4.4284649 -4.4285655 -4.4286785 -4.4287553 -4.4287987 -4.4288254 -4.4288492 -4.4288692 -4.4288869][-4.4289041 -4.428854 -4.4287338 -4.4285736 -4.4284344 -4.4283962 -4.4284582 -4.4285846 -4.4286947 -4.4287586 -4.4287963 -4.428823 -4.4288516 -4.4288774 -4.4289036]]...]
INFO - root - 2017-12-08 05:11:42.626471: step 5110, loss = 2.28, batch loss = 2.23 (36.7 examples/sec; 0.218 sec/batch; 19h:49m:29s remains)
INFO - root - 2017-12-08 05:11:44.883248: step 5120, loss = 2.28, batch loss = 2.23 (33.4 examples/sec; 0.239 sec/batch; 21h:45m:56s remains)
INFO - root - 2017-12-08 05:11:47.119247: step 5130, loss = 2.28, batch loss = 2.23 (35.0 examples/sec; 0.229 sec/batch; 20h:48m:43s remains)
INFO - root - 2017-12-08 05:11:49.358088: step 5140, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 19h:56m:52s remains)
INFO - root - 2017-12-08 05:11:51.596802: step 5150, loss = 2.28, batch loss = 2.23 (37.1 examples/sec; 0.215 sec/batch; 19h:35m:17s remains)
INFO - root - 2017-12-08 05:11:53.872773: step 5160, loss = 2.28, batch loss = 2.23 (35.3 examples/sec; 0.227 sec/batch; 20h:37m:39s remains)
INFO - root - 2017-12-08 05:11:56.119489: step 5170, loss = 2.28, batch loss = 2.23 (34.9 examples/sec; 0.229 sec/batch; 20h:50m:13s remains)
INFO - root - 2017-12-08 05:11:58.362139: step 5180, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.221 sec/batch; 20h:07m:24s remains)
INFO - root - 2017-12-08 05:12:00.587179: step 5190, loss = 2.28, batch loss = 2.23 (35.8 examples/sec; 0.223 sec/batch; 20h:18m:02s remains)
INFO - root - 2017-12-08 05:12:02.822138: step 5200, loss = 2.28, batch loss = 2.23 (36.7 examples/sec; 0.218 sec/batch; 19h:48m:02s remains)
2017-12-08 05:12:03.162110: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289289 -4.4289093 -4.4288883 -4.4288754 -4.4288669 -4.4288497 -4.4288 -4.4287486 -4.4287181 -4.4287214 -4.4287348 -4.4287686 -4.4287906 -4.4287934 -4.4287744][-4.4289269 -4.428906 -4.4288859 -4.4288754 -4.4288697 -4.4288568 -4.4288073 -4.4287562 -4.4287286 -4.4287305 -4.4287362 -4.4287524 -4.4287534 -4.42875 -4.4287267][-4.4289222 -4.4289026 -4.428884 -4.4288754 -4.4288735 -4.4288588 -4.4288034 -4.4287453 -4.42872 -4.4287243 -4.4287243 -4.4287305 -4.4287148 -4.4287076 -4.4286919][-4.4289203 -4.4289036 -4.428885 -4.4288707 -4.428863 -4.4288449 -4.4287782 -4.4287047 -4.4286747 -4.4286866 -4.4286909 -4.4287 -4.42868 -4.4286733 -4.4286709][-4.4289179 -4.4289045 -4.4288845 -4.4288588 -4.4288363 -4.42881 -4.4287372 -4.4286509 -4.4286151 -4.428637 -4.4286656 -4.4286895 -4.4286728 -4.4286623 -4.4286637][-4.4289141 -4.4289002 -4.4288754 -4.4288392 -4.4288044 -4.4287724 -4.4287033 -4.4286146 -4.4285703 -4.4286013 -4.428659 -4.428699 -4.4286895 -4.4286776 -4.4286761][-4.4288993 -4.4288797 -4.4288445 -4.4287958 -4.4287505 -4.4287162 -4.4286628 -4.4285855 -4.428535 -4.4285688 -4.4286432 -4.4286861 -4.4286733 -4.4286618 -4.4286628][-4.4288688 -4.4288349 -4.428781 -4.4287143 -4.4286637 -4.4286418 -4.428618 -4.4285636 -4.4285073 -4.4285336 -4.4286156 -4.4286571 -4.4286308 -4.4286056 -4.4285955][-4.4288254 -4.4287748 -4.4287019 -4.4286184 -4.4285703 -4.4285755 -4.4285965 -4.4285784 -4.4285212 -4.4285264 -4.4285975 -4.4286346 -4.4285994 -4.4285583 -4.42854][-4.428793 -4.4287376 -4.4286604 -4.4285731 -4.4285274 -4.4285464 -4.428597 -4.4286132 -4.4285717 -4.4285579 -4.4286065 -4.4286413 -4.428618 -4.4285946 -4.4285903][-4.4287758 -4.4287372 -4.428689 -4.428628 -4.4285951 -4.4286113 -4.4286604 -4.4286962 -4.4286857 -4.4286761 -4.4287076 -4.4287362 -4.4287252 -4.4287148 -4.4287128][-4.4287648 -4.4287567 -4.4287467 -4.4287243 -4.42872 -4.4287453 -4.4287853 -4.4288235 -4.42883 -4.4288239 -4.4288344 -4.4288468 -4.4288368 -4.4288249 -4.428813][-4.4287758 -4.4287844 -4.4287949 -4.428792 -4.4287996 -4.4288244 -4.4288526 -4.4288845 -4.428894 -4.4288888 -4.4288912 -4.4288993 -4.4288936 -4.42888 -4.428864][-4.4288116 -4.4288082 -4.4288144 -4.4288225 -4.4288373 -4.4288592 -4.4288797 -4.428905 -4.4289136 -4.4289088 -4.428906 -4.4289055 -4.4288988 -4.42889 -4.4288774][-4.4288583 -4.428833 -4.4288225 -4.428833 -4.4288549 -4.4288731 -4.428884 -4.4288988 -4.4289031 -4.4289002 -4.428896 -4.4288888 -4.4288797 -4.428874 -4.4288688]]...]
INFO - root - 2017-12-08 05:12:05.406404: step 5210, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 20h:05m:34s remains)
INFO - root - 2017-12-08 05:12:07.630920: step 5220, loss = 2.28, batch loss = 2.23 (37.2 examples/sec; 0.215 sec/batch; 19h:32m:34s remains)
INFO - root - 2017-12-08 05:12:09.923836: step 5230, loss = 2.28, batch loss = 2.23 (33.9 examples/sec; 0.236 sec/batch; 21h:25m:27s remains)
INFO - root - 2017-12-08 05:12:12.189571: step 5240, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 19h:56m:18s remains)
INFO - root - 2017-12-08 05:12:14.420944: step 5250, loss = 2.28, batch loss = 2.23 (36.6 examples/sec; 0.219 sec/batch; 19h:52m:01s remains)
INFO - root - 2017-12-08 05:12:16.658821: step 5260, loss = 2.28, batch loss = 2.23 (35.6 examples/sec; 0.225 sec/batch; 20h:27m:15s remains)
INFO - root - 2017-12-08 05:12:18.875153: step 5270, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.222 sec/batch; 20h:09m:48s remains)
INFO - root - 2017-12-08 05:12:21.179145: step 5280, loss = 2.28, batch loss = 2.23 (34.9 examples/sec; 0.229 sec/batch; 20h:49m:44s remains)
INFO - root - 2017-12-08 05:12:23.417582: step 5290, loss = 2.28, batch loss = 2.23 (34.9 examples/sec; 0.229 sec/batch; 20h:51m:26s remains)
INFO - root - 2017-12-08 05:12:25.639813: step 5300, loss = 2.28, batch loss = 2.23 (36.8 examples/sec; 0.217 sec/batch; 19h:44m:07s remains)
2017-12-08 05:12:25.923747: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289174 -4.4289289 -4.4289389 -4.4289374 -4.428906 -4.42888 -4.4288549 -4.4288335 -4.42883 -4.4288263 -4.4288249 -4.4288392 -4.4288645 -4.4288969 -4.4289303][-4.4288635 -4.4288745 -4.428884 -4.4288783 -4.4288354 -4.4287992 -4.4287682 -4.4287481 -4.4287558 -4.4287534 -4.4287457 -4.4287586 -4.4287863 -4.4288278 -4.4288697][-4.428771 -4.428782 -4.4287949 -4.4287872 -4.4287353 -4.4286871 -4.42865 -4.4286413 -4.4286709 -4.4286766 -4.4286633 -4.4286742 -4.4286985 -4.4287405 -4.4287844][-4.4286714 -4.4286823 -4.4287014 -4.4287033 -4.4286475 -4.4285822 -4.4285331 -4.4285316 -4.4285827 -4.4286022 -4.4285879 -4.4285979 -4.4286222 -4.428668 -4.4287095][-4.4285932 -4.4286008 -4.4286242 -4.4286346 -4.4285755 -4.4284897 -4.4284253 -4.4284253 -4.4284906 -4.4285336 -4.4285369 -4.4285526 -4.4285808 -4.428628 -4.4286604][-4.4285545 -4.4285665 -4.4285946 -4.4286036 -4.428534 -4.4284205 -4.4283323 -4.4283271 -4.4284019 -4.4284768 -4.4285116 -4.42854 -4.4285717 -4.4286141 -4.4286346][-4.4285717 -4.4286003 -4.4286346 -4.4286361 -4.4285517 -4.428412 -4.4283056 -4.4282961 -4.4283748 -4.428473 -4.428534 -4.4285717 -4.4286027 -4.4286361 -4.4286513][-4.4286056 -4.4286466 -4.4286819 -4.4286742 -4.4285865 -4.42845 -4.4283533 -4.4283543 -4.4284282 -4.4285207 -4.4285808 -4.4286103 -4.4286313 -4.428658 -4.4286785][-4.4286151 -4.4286613 -4.4286957 -4.4286866 -4.4286151 -4.4285059 -4.4284358 -4.4284482 -4.4285049 -4.4285679 -4.4286027 -4.4286118 -4.4286208 -4.4286461 -4.428678][-4.4286156 -4.4286618 -4.4286957 -4.4286923 -4.4286389 -4.4285588 -4.4285088 -4.4285173 -4.4285545 -4.4285913 -4.4286056 -4.4286056 -4.4286122 -4.4286427 -4.4286838][-4.4286079 -4.4286509 -4.4286861 -4.4286938 -4.4286585 -4.4286041 -4.4285684 -4.4285727 -4.4285951 -4.4286122 -4.4286151 -4.428617 -4.4286294 -4.4286656 -4.4287071][-4.4285979 -4.4286366 -4.4286704 -4.4286866 -4.4286666 -4.4286327 -4.4286127 -4.4286222 -4.4286408 -4.4286475 -4.4286427 -4.4286466 -4.4286642 -4.4287014 -4.4287367][-4.4286041 -4.4286346 -4.4286623 -4.4286795 -4.428668 -4.4286475 -4.4286389 -4.4286556 -4.4286776 -4.4286809 -4.4286737 -4.4286795 -4.4286981 -4.4287329 -4.4287577][-4.4286175 -4.4286408 -4.4286594 -4.4286776 -4.4286761 -4.4286675 -4.428668 -4.4286904 -4.4287148 -4.4287167 -4.4287081 -4.4287128 -4.4287267 -4.4287553 -4.4287739][-4.4286351 -4.4286528 -4.4286666 -4.428688 -4.4286947 -4.4286914 -4.4286933 -4.4287143 -4.4287367 -4.4287395 -4.4287357 -4.4287438 -4.4287562 -4.4287782 -4.4287915]]...]
INFO - root - 2017-12-08 05:12:28.164765: step 5310, loss = 2.28, batch loss = 2.23 (34.6 examples/sec; 0.231 sec/batch; 20h:59m:05s remains)
INFO - root - 2017-12-08 05:12:30.396427: step 5320, loss = 2.28, batch loss = 2.23 (33.8 examples/sec; 0.237 sec/batch; 21h:32m:22s remains)
INFO - root - 2017-12-08 05:12:32.614139: step 5330, loss = 2.28, batch loss = 2.23 (37.9 examples/sec; 0.211 sec/batch; 19h:10m:19s remains)
INFO - root - 2017-12-08 05:12:34.846385: step 5340, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.220 sec/batch; 20h:00m:33s remains)
INFO - root - 2017-12-08 05:12:37.086944: step 5350, loss = 2.28, batch loss = 2.23 (34.6 examples/sec; 0.231 sec/batch; 21h:01m:51s remains)
INFO - root - 2017-12-08 05:12:39.315412: step 5360, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.225 sec/batch; 20h:28m:45s remains)
INFO - root - 2017-12-08 05:12:41.538819: step 5370, loss = 2.28, batch loss = 2.23 (35.4 examples/sec; 0.226 sec/batch; 20h:32m:47s remains)
INFO - root - 2017-12-08 05:12:43.764078: step 5380, loss = 2.28, batch loss = 2.23 (34.7 examples/sec; 0.230 sec/batch; 20h:56m:04s remains)
INFO - root - 2017-12-08 05:12:45.993473: step 5390, loss = 2.28, batch loss = 2.23 (35.6 examples/sec; 0.225 sec/batch; 20h:24m:04s remains)
INFO - root - 2017-12-08 05:12:48.252135: step 5400, loss = 2.28, batch loss = 2.23 (34.9 examples/sec; 0.229 sec/batch; 20h:48m:03s remains)
2017-12-08 05:12:48.541635: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289455 -4.4289465 -4.4289489 -4.42895 -4.4289379 -4.4289312 -4.4289322 -4.4289346 -4.4289341 -4.4289212 -4.4289069 -4.4288964 -4.4288917 -4.4288936 -4.4289007][-4.428946 -4.4289532 -4.428966 -4.4289718 -4.4289575 -4.4289494 -4.4289517 -4.4289575 -4.428956 -4.4289355 -4.4289126 -4.4288936 -4.4288788 -4.428875 -4.4288769][-4.4289212 -4.428925 -4.4289379 -4.4289393 -4.4289203 -4.4289079 -4.4289131 -4.4289284 -4.428936 -4.4289193 -4.4288931 -4.4288673 -4.4288507 -4.4288468 -4.4288464][-4.4288907 -4.4288869 -4.4288936 -4.4288888 -4.4288597 -4.428833 -4.4288311 -4.4288554 -4.4288864 -4.4288878 -4.4288673 -4.4288435 -4.4288259 -4.4288244 -4.4288287][-4.4288664 -4.428844 -4.4288325 -4.4288116 -4.4287682 -4.4287138 -4.428688 -4.4287224 -4.4287915 -4.4288292 -4.4288292 -4.4288197 -4.428812 -4.4288163 -4.4288235][-4.4288459 -4.4288073 -4.428771 -4.4287295 -4.4286642 -4.4285688 -4.4284954 -4.4285254 -4.4286437 -4.4287386 -4.4287806 -4.428793 -4.4288 -4.4288158 -4.4288278][-4.4288387 -4.4287987 -4.4287505 -4.4286842 -4.4285779 -4.4284229 -4.4282751 -4.4282732 -4.4284286 -4.4285817 -4.4286814 -4.4287376 -4.4287767 -4.4288073 -4.4288273][-4.4288349 -4.4288025 -4.4287581 -4.4286742 -4.4285331 -4.4283352 -4.4281363 -4.4280877 -4.4282479 -4.4284396 -4.4285889 -4.4286923 -4.4287567 -4.4287906 -4.4288082][-4.4288507 -4.4288392 -4.4288239 -4.4287663 -4.4286518 -4.428493 -4.4283233 -4.4282479 -4.4283452 -4.4284863 -4.4286084 -4.4287009 -4.4287529 -4.4287758 -4.4287839][-4.4288635 -4.4288678 -4.4288745 -4.4288497 -4.4287868 -4.4287028 -4.4285908 -4.4285121 -4.4285336 -4.42859 -4.4286466 -4.4286962 -4.4287243 -4.4287381 -4.4287481][-4.4288516 -4.4288578 -4.4288707 -4.428864 -4.4288397 -4.4288116 -4.4287562 -4.4286995 -4.4286847 -4.4286847 -4.4286895 -4.4286995 -4.4287062 -4.4287171 -4.4287333][-4.4288292 -4.4288287 -4.4288449 -4.4288487 -4.42884 -4.4288406 -4.4288259 -4.4288011 -4.4287853 -4.428762 -4.4287419 -4.4287343 -4.4287319 -4.4287462 -4.4287682][-4.4288278 -4.4288197 -4.4288282 -4.4288306 -4.4288292 -4.4288459 -4.4288588 -4.4288635 -4.428863 -4.4288406 -4.4288187 -4.4288049 -4.4288 -4.4288116 -4.4288282][-4.4288516 -4.4288383 -4.4288411 -4.4288435 -4.4288473 -4.4288692 -4.4288864 -4.4288988 -4.4289107 -4.4289036 -4.4288883 -4.4288769 -4.4288669 -4.4288659 -4.4288712][-4.428874 -4.4288597 -4.4288592 -4.428865 -4.4288707 -4.4288859 -4.4288964 -4.4289021 -4.4289126 -4.4289145 -4.4289064 -4.4288931 -4.4288836 -4.428884 -4.4288917]]...]
INFO - root - 2017-12-08 05:12:50.811216: step 5410, loss = 2.28, batch loss = 2.23 (34.2 examples/sec; 0.234 sec/batch; 21h:13m:28s remains)
INFO - root - 2017-12-08 05:12:53.028192: step 5420, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.226 sec/batch; 20h:29m:56s remains)
INFO - root - 2017-12-08 05:12:55.249362: step 5430, loss = 2.28, batch loss = 2.23 (36.6 examples/sec; 0.219 sec/batch; 19h:52m:14s remains)
INFO - root - 2017-12-08 05:12:57.495975: step 5440, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 19h:53m:33s remains)
INFO - root - 2017-12-08 05:12:59.716173: step 5450, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.221 sec/batch; 20h:01m:59s remains)
INFO - root - 2017-12-08 05:13:01.940885: step 5460, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.219 sec/batch; 19h:56m:20s remains)
INFO - root - 2017-12-08 05:13:04.184084: step 5470, loss = 2.28, batch loss = 2.23 (35.3 examples/sec; 0.227 sec/batch; 20h:36m:17s remains)
INFO - root - 2017-12-08 05:13:06.418439: step 5480, loss = 2.28, batch loss = 2.23 (35.7 examples/sec; 0.224 sec/batch; 20h:20m:12s remains)
INFO - root - 2017-12-08 05:13:08.656050: step 5490, loss = 2.28, batch loss = 2.23 (34.8 examples/sec; 0.230 sec/batch; 20h:54m:29s remains)
INFO - root - 2017-12-08 05:13:10.904331: step 5500, loss = 2.28, batch loss = 2.23 (35.2 examples/sec; 0.227 sec/batch; 20h:37m:53s remains)
2017-12-08 05:13:11.189044: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4287405 -4.4287419 -4.4287372 -4.4287319 -4.4287353 -4.4287481 -4.4287739 -4.4287992 -4.4288192 -4.4288187 -4.4287977 -4.4287848 -4.4288025 -4.4287963 -4.4288073][-4.4287176 -4.4287233 -4.4287157 -4.4287028 -4.4287024 -4.4287133 -4.4287333 -4.4287481 -4.428761 -4.4287653 -4.428751 -4.4287434 -4.428772 -4.4287763 -4.4287906][-4.4286647 -4.4286795 -4.4286761 -4.4286656 -4.4286776 -4.4286952 -4.4287238 -4.4287395 -4.4287481 -4.4287539 -4.4287333 -4.4287152 -4.4287443 -4.4287591 -4.4287767][-4.4286518 -4.4286637 -4.4286585 -4.4286537 -4.4286661 -4.428679 -4.4287152 -4.4287343 -4.4287457 -4.4287515 -4.4287214 -4.4286876 -4.4287105 -4.4287395 -4.4287691][-4.4286432 -4.4286466 -4.4286423 -4.4286456 -4.4286547 -4.4286509 -4.42868 -4.4286957 -4.4287124 -4.42872 -4.42869 -4.4286427 -4.4286571 -4.4286971 -4.4287505][-4.4286103 -4.4285994 -4.4285965 -4.4286065 -4.4286108 -4.4285846 -4.4285893 -4.4286036 -4.4286356 -4.4286594 -4.4286394 -4.4285812 -4.4285822 -4.4286184 -4.4286833][-4.4285989 -4.4285612 -4.42853 -4.4285278 -4.4285045 -4.4284182 -4.4283619 -4.4284067 -4.4285045 -4.4285746 -4.4285789 -4.4285235 -4.4285178 -4.4285555 -4.42862][-4.42855 -4.4284925 -4.4284253 -4.4283915 -4.4283252 -4.4281468 -4.4279723 -4.428061 -4.4282742 -4.4284186 -4.4284658 -4.4284368 -4.4284596 -4.4285216 -4.4285917][-4.4284959 -4.4284363 -4.4283676 -4.4283309 -4.4282722 -4.4280715 -4.4278097 -4.4278655 -4.42813 -4.42832 -4.4283938 -4.4283991 -4.4284606 -4.4285493 -4.4286203][-4.4284859 -4.428452 -4.4284286 -4.4284277 -4.4284234 -4.428318 -4.4281564 -4.4281454 -4.4283085 -4.4284453 -4.4285049 -4.4285259 -4.4285889 -4.4286566 -4.4287024][-4.4284892 -4.4284892 -4.4285107 -4.4285426 -4.4285774 -4.42855 -4.428483 -4.428452 -4.4285245 -4.4286003 -4.4286342 -4.4286513 -4.4286957 -4.4287324 -4.428761][-4.4285493 -4.4285774 -4.4286227 -4.428668 -4.4287105 -4.4287181 -4.4286952 -4.4286656 -4.4286742 -4.4286962 -4.4286914 -4.4286814 -4.4287076 -4.4287434 -4.4287734][-4.4286332 -4.428679 -4.4287348 -4.4287734 -4.4288049 -4.4288158 -4.4288068 -4.4287772 -4.4287491 -4.4287314 -4.4286838 -4.4286327 -4.4286394 -4.4286909 -4.42874][-4.4286804 -4.4287305 -4.428782 -4.4288068 -4.4288249 -4.4288335 -4.4288273 -4.4288044 -4.4287643 -4.4287257 -4.4286656 -4.4286056 -4.4286208 -4.4286914 -4.4287486][-4.4287066 -4.428741 -4.4287744 -4.4287872 -4.4287996 -4.4288111 -4.4288158 -4.4288054 -4.4287786 -4.4287515 -4.4287162 -4.4286842 -4.4287119 -4.428782 -4.42882]]...]
INFO - root - 2017-12-08 05:13:13.425421: step 5510, loss = 2.28, batch loss = 2.23 (35.6 examples/sec; 0.224 sec/batch; 20h:23m:16s remains)
INFO - root - 2017-12-08 05:13:15.655133: step 5520, loss = 2.28, batch loss = 2.23 (35.1 examples/sec; 0.228 sec/batch; 20h:42m:03s remains)
INFO - root - 2017-12-08 05:13:17.887957: step 5530, loss = 2.28, batch loss = 2.23 (34.2 examples/sec; 0.234 sec/batch; 21h:14m:46s remains)
INFO - root - 2017-12-08 05:13:20.164102: step 5540, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.220 sec/batch; 20h:01m:03s remains)
INFO - root - 2017-12-08 05:13:22.395865: step 5550, loss = 2.28, batch loss = 2.23 (36.8 examples/sec; 0.217 sec/batch; 19h:43m:41s remains)
INFO - root - 2017-12-08 05:13:24.623307: step 5560, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 20h:14m:33s remains)
INFO - root - 2017-12-08 05:13:26.864752: step 5570, loss = 2.28, batch loss = 2.23 (34.8 examples/sec; 0.230 sec/batch; 20h:51m:37s remains)
INFO - root - 2017-12-08 05:13:29.093473: step 5580, loss = 2.28, batch loss = 2.23 (35.1 examples/sec; 0.228 sec/batch; 20h:41m:05s remains)
INFO - root - 2017-12-08 05:13:31.318364: step 5590, loss = 2.28, batch loss = 2.23 (36.6 examples/sec; 0.219 sec/batch; 19h:50m:48s remains)
INFO - root - 2017-12-08 05:13:33.554749: step 5600, loss = 2.28, batch loss = 2.23 (35.2 examples/sec; 0.227 sec/batch; 20h:36m:55s remains)
2017-12-08 05:13:33.842746: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289021 -4.4288893 -4.4288745 -4.4288588 -4.428843 -4.4288344 -4.4288363 -4.4288454 -4.4288607 -4.4288797 -4.4288964 -4.4289055 -4.4289074 -4.4289122 -4.4289227][-4.4288754 -4.4288535 -4.4288316 -4.4288073 -4.428782 -4.4287672 -4.428761 -4.42876 -4.4287744 -4.4288025 -4.4288368 -4.4288592 -4.4288683 -4.4288788 -4.4288926][-4.4288483 -4.4288163 -4.4287925 -4.4287686 -4.4287353 -4.4287028 -4.4286709 -4.4286489 -4.428659 -4.4287033 -4.4287615 -4.4288034 -4.4288254 -4.4288383 -4.4288507][-4.4288316 -4.4287887 -4.4287634 -4.4287395 -4.4286914 -4.428627 -4.4285507 -4.4284983 -4.428504 -4.4285712 -4.4286633 -4.4287376 -4.4287796 -4.4287987 -4.4288082][-4.4288154 -4.4287667 -4.4287486 -4.4287329 -4.428669 -4.428566 -4.4284372 -4.4283433 -4.4283352 -4.4284225 -4.4285564 -4.4286671 -4.4287333 -4.4287639 -4.4287753][-4.4287896 -4.4287481 -4.4287443 -4.4287376 -4.4286685 -4.4285417 -4.428369 -4.4282246 -4.4281836 -4.4282765 -4.4284449 -4.4285913 -4.4286857 -4.428731 -4.4287481][-4.4287434 -4.4287066 -4.428719 -4.4287305 -4.4286771 -4.4285517 -4.4283547 -4.4281468 -4.4280305 -4.4281058 -4.428319 -4.4285097 -4.4286275 -4.4286866 -4.4287157][-4.4287038 -4.4286675 -4.4286885 -4.4287205 -4.4286952 -4.4285908 -4.4283862 -4.4281235 -4.4279318 -4.427989 -4.4282427 -4.4284563 -4.4285808 -4.4286413 -4.4286771][-4.4287062 -4.4286675 -4.4286895 -4.4287362 -4.4287434 -4.4286757 -4.4284935 -4.4282341 -4.4280367 -4.4280791 -4.4283047 -4.4284821 -4.4285712 -4.4286146 -4.4286461][-4.4287419 -4.4287 -4.4287162 -4.4287682 -4.4288006 -4.4287705 -4.428627 -4.4284034 -4.4282312 -4.4282475 -4.428411 -4.4285321 -4.4285769 -4.4285975 -4.4286237][-4.4288106 -4.4287715 -4.4287729 -4.4288106 -4.4288483 -4.4288449 -4.4287486 -4.4285769 -4.4284387 -4.4284291 -4.4285316 -4.428606 -4.4286222 -4.428627 -4.4286485][-4.4288893 -4.4288578 -4.4288464 -4.4288626 -4.4288936 -4.4289079 -4.4288554 -4.4287372 -4.428627 -4.428597 -4.4286542 -4.4286995 -4.4287033 -4.4287028 -4.4287167][-4.4289403 -4.4289188 -4.4289045 -4.4289083 -4.4289293 -4.428946 -4.4289141 -4.4288297 -4.4287429 -4.4287124 -4.4287419 -4.4287734 -4.4287825 -4.4287853 -4.4287953][-4.4289551 -4.4289451 -4.4289351 -4.4289365 -4.4289503 -4.4289608 -4.4289417 -4.4288869 -4.4288244 -4.4287968 -4.4288096 -4.4288311 -4.4288454 -4.428854 -4.4288635][-4.42895 -4.428947 -4.4289432 -4.4289441 -4.428947 -4.428947 -4.4289341 -4.428905 -4.4288688 -4.4288497 -4.428853 -4.4288659 -4.4288826 -4.4288983 -4.4289083]]...]
INFO - root - 2017-12-08 05:13:36.069383: step 5610, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 20h:13m:36s remains)
INFO - root - 2017-12-08 05:13:38.288043: step 5620, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.226 sec/batch; 20h:28m:42s remains)
INFO - root - 2017-12-08 05:13:40.517279: step 5630, loss = 2.28, batch loss = 2.23 (36.6 examples/sec; 0.219 sec/batch; 19h:51m:04s remains)
INFO - root - 2017-12-08 05:13:42.794139: step 5640, loss = 2.28, batch loss = 2.23 (37.0 examples/sec; 0.216 sec/batch; 19h:39m:12s remains)
INFO - root - 2017-12-08 05:13:45.013864: step 5650, loss = 2.28, batch loss = 2.23 (36.7 examples/sec; 0.218 sec/batch; 19h:47m:46s remains)
INFO - root - 2017-12-08 05:13:47.257068: step 5660, loss = 2.28, batch loss = 2.23 (34.4 examples/sec; 0.233 sec/batch; 21h:06m:54s remains)
INFO - root - 2017-12-08 05:13:49.482782: step 5670, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 20h:02m:31s remains)
INFO - root - 2017-12-08 05:13:51.739189: step 5680, loss = 2.28, batch loss = 2.23 (31.8 examples/sec; 0.252 sec/batch; 22h:50m:16s remains)
INFO - root - 2017-12-08 05:13:54.002725: step 5690, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 19h:53m:54s remains)
INFO - root - 2017-12-08 05:13:56.239500: step 5700, loss = 2.28, batch loss = 2.23 (35.7 examples/sec; 0.224 sec/batch; 20h:22m:03s remains)
2017-12-08 05:13:56.534144: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288363 -4.42882 -4.428772 -4.4287071 -4.4286571 -4.4286294 -4.428638 -4.4286327 -4.4286137 -4.4286041 -4.4285917 -4.428575 -4.4285588 -4.4285259 -4.4284754][-4.4288406 -4.4288197 -4.4287591 -4.4286742 -4.428596 -4.4285417 -4.4285355 -4.4285259 -4.4285049 -4.4284859 -4.4284673 -4.4284592 -4.428452 -4.4284167 -4.4283624][-4.42885 -4.4288278 -4.428762 -4.4286647 -4.4285722 -4.4285045 -4.4284906 -4.4284806 -4.4284606 -4.4284368 -4.4284153 -4.4284124 -4.4284072 -4.4283743 -4.4283323][-4.4288521 -4.4288325 -4.4287748 -4.4286871 -4.4286046 -4.4285507 -4.4285426 -4.4285398 -4.4285197 -4.4284935 -4.4284763 -4.42847 -4.4284544 -4.4284191 -4.4283857][-4.4288449 -4.428833 -4.4287944 -4.4287343 -4.4286819 -4.4286547 -4.4286571 -4.4286652 -4.4286532 -4.4286323 -4.4286175 -4.4286041 -4.4285779 -4.4285412 -4.4285116][-4.4288378 -4.4288316 -4.4288116 -4.4287763 -4.4287477 -4.4287391 -4.4287467 -4.4287658 -4.4287667 -4.4287577 -4.4287424 -4.4287267 -4.4287038 -4.4286742 -4.42865][-4.4288321 -4.4288273 -4.4288149 -4.4287872 -4.4287634 -4.4287562 -4.4287605 -4.4287825 -4.4287949 -4.4287987 -4.42879 -4.4287839 -4.4287791 -4.4287672 -4.4287581][-4.4288282 -4.4288192 -4.4288025 -4.4287677 -4.4287353 -4.4287229 -4.4287229 -4.4287434 -4.4287591 -4.4287667 -4.4287648 -4.4287734 -4.4287915 -4.4288034 -4.4288154][-4.4288235 -4.4288096 -4.4287834 -4.4287376 -4.4286952 -4.4286733 -4.4286714 -4.42869 -4.4286995 -4.4287014 -4.4287024 -4.4287248 -4.428762 -4.4287953 -4.4288254][-4.4288168 -4.4287996 -4.4287663 -4.4287167 -4.42867 -4.4286423 -4.4286389 -4.4286518 -4.4286509 -4.4286461 -4.4286523 -4.4286842 -4.4287319 -4.4287786 -4.4288173][-4.4288096 -4.4287925 -4.42876 -4.4287157 -4.4286723 -4.4286404 -4.4286332 -4.4286447 -4.4286413 -4.428638 -4.4286485 -4.4286828 -4.4287314 -4.4287815 -4.4288182][-4.4288058 -4.428791 -4.4287663 -4.4287314 -4.4286952 -4.4286647 -4.4286561 -4.4286723 -4.4286771 -4.4286776 -4.42869 -4.4287186 -4.4287615 -4.4288044 -4.4288306][-4.4288139 -4.428803 -4.4287844 -4.4287586 -4.4287291 -4.4287047 -4.428699 -4.4287229 -4.4287367 -4.4287405 -4.4287515 -4.4287739 -4.4288054 -4.4288344 -4.4288487][-4.4288311 -4.4288254 -4.4288139 -4.4287944 -4.4287715 -4.4287567 -4.4287553 -4.4287853 -4.4288054 -4.4288125 -4.4288216 -4.428834 -4.4288511 -4.428865 -4.4288692][-4.4288478 -4.4288473 -4.428844 -4.4288287 -4.428812 -4.4288054 -4.4288092 -4.4288397 -4.4288621 -4.4288707 -4.4288764 -4.4288812 -4.4288869 -4.4288907 -4.4288888]]...]
INFO - root - 2017-12-08 05:13:58.746890: step 5710, loss = 2.28, batch loss = 2.23 (37.0 examples/sec; 0.216 sec/batch; 19h:36m:58s remains)
INFO - root - 2017-12-08 05:14:00.952169: step 5720, loss = 2.28, batch loss = 2.23 (35.6 examples/sec; 0.225 sec/batch; 20h:25m:06s remains)
INFO - root - 2017-12-08 05:14:03.204167: step 5730, loss = 2.28, batch loss = 2.23 (36.7 examples/sec; 0.218 sec/batch; 19h:48m:38s remains)
INFO - root - 2017-12-08 05:14:05.453601: step 5740, loss = 2.28, batch loss = 2.23 (34.7 examples/sec; 0.231 sec/batch; 20h:57m:03s remains)
INFO - root - 2017-12-08 05:14:07.670196: step 5750, loss = 2.28, batch loss = 2.23 (36.7 examples/sec; 0.218 sec/batch; 19h:46m:23s remains)
INFO - root - 2017-12-08 05:14:09.942624: step 5760, loss = 2.28, batch loss = 2.23 (35.1 examples/sec; 0.228 sec/batch; 20h:41m:59s remains)
INFO - root - 2017-12-08 05:14:12.196892: step 5770, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.225 sec/batch; 20h:27m:08s remains)
INFO - root - 2017-12-08 05:14:14.447371: step 5780, loss = 2.28, batch loss = 2.23 (35.1 examples/sec; 0.228 sec/batch; 20h:39m:30s remains)
INFO - root - 2017-12-08 05:14:16.689733: step 5790, loss = 2.28, batch loss = 2.23 (35.4 examples/sec; 0.226 sec/batch; 20h:32m:10s remains)
INFO - root - 2017-12-08 05:14:18.914935: step 5800, loss = 2.28, batch loss = 2.23 (36.0 examples/sec; 0.222 sec/batch; 20h:08m:28s remains)
2017-12-08 05:14:19.245878: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4286838 -4.4286947 -4.4287105 -4.4287362 -4.4287772 -4.4288187 -4.4288483 -4.4288421 -4.4287915 -4.4287128 -4.42865 -4.4286408 -4.4286671 -4.4286866 -4.428669][-4.428607 -4.428648 -4.4286995 -4.4287405 -4.428781 -4.42881 -4.428823 -4.4288077 -4.4287663 -4.4287062 -4.4286571 -4.4286485 -4.4286752 -4.4286966 -4.4286833][-4.4285808 -4.4286442 -4.4287257 -4.4287715 -4.4287915 -4.428792 -4.4287853 -4.4287629 -4.4287348 -4.4287038 -4.4286718 -4.4286623 -4.4286814 -4.4286952 -4.4286857][-4.4286103 -4.428669 -4.428741 -4.4287629 -4.4287434 -4.4287186 -4.4287105 -4.4287014 -4.4286909 -4.4286819 -4.4286747 -4.428669 -4.4286737 -4.4286685 -4.4286618][-4.4285913 -4.4286284 -4.4286637 -4.4286447 -4.4285746 -4.4285169 -4.4285216 -4.4285522 -4.4285712 -4.4285884 -4.4286194 -4.4286408 -4.4286423 -4.4286132 -4.4285913][-4.4285717 -4.4285684 -4.4285488 -4.4284725 -4.4283462 -4.428247 -4.4282627 -4.4283519 -4.4284167 -4.4284711 -4.4285455 -4.4286103 -4.4286275 -4.4285889 -4.4285436][-4.4285727 -4.4285188 -4.4284348 -4.4282923 -4.4281011 -4.4279242 -4.4279213 -4.4280882 -4.428237 -4.4283419 -4.4284596 -4.4285736 -4.4286213 -4.428597 -4.4285502][-4.4286 -4.428515 -4.4284019 -4.4282446 -4.4280629 -4.4278803 -4.4278436 -4.4280114 -4.4281869 -4.4283023 -4.4284244 -4.4285455 -4.428606 -4.4286127 -4.4285913][-4.4286709 -4.4286 -4.4285097 -4.4283953 -4.4282932 -4.4281926 -4.4281521 -4.4282465 -4.4283676 -4.4284477 -4.4285283 -4.4285984 -4.4286356 -4.428647 -4.4286413][-4.4287505 -4.4287028 -4.4286442 -4.4285727 -4.4285231 -4.4284897 -4.4284773 -4.4285288 -4.4286027 -4.4286532 -4.4286995 -4.4287271 -4.4287291 -4.4287186 -4.4287019][-4.4288154 -4.4287853 -4.4287577 -4.4287167 -4.4286933 -4.4286942 -4.4287024 -4.4287353 -4.428781 -4.428812 -4.4288363 -4.4288483 -4.4288363 -4.4288087 -4.428772][-4.4288716 -4.4288473 -4.428834 -4.42882 -4.4288168 -4.4288321 -4.4288468 -4.4288616 -4.4288816 -4.4288979 -4.4289112 -4.4289174 -4.428906 -4.4288793 -4.4288416][-4.4289064 -4.4288898 -4.4288821 -4.42888 -4.4288816 -4.428894 -4.4289079 -4.428916 -4.4289236 -4.4289303 -4.4289341 -4.4289355 -4.4289341 -4.4289231 -4.4289][-4.4289155 -4.4289083 -4.428905 -4.4289083 -4.428915 -4.428925 -4.4289384 -4.4289446 -4.4289455 -4.4289446 -4.4289403 -4.4289365 -4.4289393 -4.428937 -4.4289236][-4.4289165 -4.4289117 -4.42891 -4.4289112 -4.428915 -4.4289212 -4.4289303 -4.4289355 -4.428936 -4.4289346 -4.4289312 -4.4289293 -4.4289308 -4.4289322 -4.4289241]]...]
INFO - root - 2017-12-08 05:14:21.464404: step 5810, loss = 2.28, batch loss = 2.23 (35.2 examples/sec; 0.227 sec/batch; 20h:38m:16s remains)
INFO - root - 2017-12-08 05:14:23.684401: step 5820, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.222 sec/batch; 20h:07m:37s remains)
INFO - root - 2017-12-08 05:14:25.912554: step 5830, loss = 2.28, batch loss = 2.23 (36.0 examples/sec; 0.222 sec/batch; 20h:10m:39s remains)
INFO - root - 2017-12-08 05:14:28.171789: step 5840, loss = 2.28, batch loss = 2.23 (35.0 examples/sec; 0.229 sec/batch; 20h:44m:44s remains)
INFO - root - 2017-12-08 05:14:30.411962: step 5850, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 20h:13m:30s remains)
INFO - root - 2017-12-08 05:14:32.639883: step 5860, loss = 2.28, batch loss = 2.23 (36.7 examples/sec; 0.218 sec/batch; 19h:47m:03s remains)
INFO - root - 2017-12-08 05:14:34.858523: step 5870, loss = 2.28, batch loss = 2.23 (36.9 examples/sec; 0.217 sec/batch; 19h:41m:29s remains)
INFO - root - 2017-12-08 05:14:37.083875: step 5880, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 19h:54m:07s remains)
INFO - root - 2017-12-08 05:14:39.376377: step 5890, loss = 2.28, batch loss = 2.23 (34.9 examples/sec; 0.229 sec/batch; 20h:46m:45s remains)
INFO - root - 2017-12-08 05:14:41.615819: step 5900, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.221 sec/batch; 20h:04m:40s remains)
2017-12-08 05:14:41.884433: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288239 -4.4288306 -4.4288554 -4.4288783 -4.4288921 -4.4288917 -4.4288759 -4.4288611 -4.4288626 -4.428875 -4.42888 -4.428885 -4.4288726 -4.4288459 -4.4288163][-4.4287648 -4.4287748 -4.4288116 -4.4288406 -4.4288588 -4.4288611 -4.4288378 -4.4288182 -4.428823 -4.4288349 -4.4288435 -4.4288516 -4.4288325 -4.4287906 -4.4287443][-4.4287062 -4.4287176 -4.4287663 -4.4288025 -4.42882 -4.4288096 -4.4287667 -4.4287415 -4.4287539 -4.4287682 -4.4287848 -4.428803 -4.4287858 -4.4287271 -4.4286613][-4.4286723 -4.4286804 -4.4287276 -4.4287605 -4.4287696 -4.4287405 -4.4286718 -4.4286351 -4.4286594 -4.4286904 -4.4287186 -4.4287539 -4.42875 -4.4286861 -4.4286036][-4.4286323 -4.4286442 -4.4286923 -4.4287119 -4.4286947 -4.4286318 -4.4285278 -4.428483 -4.4285364 -4.4285989 -4.4286475 -4.4286957 -4.4287095 -4.4286494 -4.4285517][-4.4285979 -4.4286237 -4.4286733 -4.4286757 -4.4286127 -4.4284945 -4.4283419 -4.4282908 -4.4283872 -4.428504 -4.4285917 -4.4286618 -4.4286942 -4.4286332 -4.4285145][-4.4285984 -4.42863 -4.4286752 -4.42866 -4.4285488 -4.4283705 -4.4281621 -4.4281015 -4.4282312 -4.4284072 -4.4285469 -4.4286485 -4.4286933 -4.428637 -4.4285097][-4.4286547 -4.42868 -4.4287119 -4.428679 -4.4285388 -4.4283242 -4.428102 -4.4280396 -4.4281735 -4.4283719 -4.4285374 -4.4286609 -4.4287143 -4.4286666 -4.4285536][-4.4287314 -4.42875 -4.428771 -4.4287391 -4.4286156 -4.4284339 -4.4282522 -4.4282002 -4.4283056 -4.4284606 -4.4285936 -4.4287138 -4.4287629 -4.4287148 -4.4286165][-4.4287663 -4.4287763 -4.4287925 -4.4287767 -4.4287004 -4.4285874 -4.4284611 -4.4284124 -4.4284859 -4.4285893 -4.4286757 -4.4287686 -4.4288049 -4.4287467 -4.4286475][-4.4287844 -4.4287944 -4.428813 -4.4288135 -4.4287834 -4.4287295 -4.4286489 -4.4285975 -4.4286351 -4.4287014 -4.4287496 -4.4288182 -4.42884 -4.4287739 -4.428669][-4.4288068 -4.4288225 -4.4288507 -4.4288645 -4.4288621 -4.4288454 -4.4287987 -4.4287534 -4.428771 -4.428813 -4.42883 -4.4288726 -4.4288788 -4.428813 -4.4287119][-4.4288397 -4.4288688 -4.4289074 -4.4289246 -4.4289231 -4.4289141 -4.4288855 -4.4288521 -4.42887 -4.4289083 -4.428916 -4.4289355 -4.4289222 -4.4288578 -4.4287729][-4.4288826 -4.4289279 -4.4289761 -4.4289889 -4.4289746 -4.4289536 -4.428925 -4.4288983 -4.4289222 -4.4289656 -4.4289789 -4.4289813 -4.4289536 -4.428894 -4.42883][-4.4289279 -4.4289751 -4.4290152 -4.4290209 -4.4289985 -4.4289665 -4.4289336 -4.4289122 -4.4289346 -4.428977 -4.4289932 -4.4289918 -4.4289646 -4.4289207 -4.4288783]]...]
INFO - root - 2017-12-08 05:14:44.109468: step 5910, loss = 2.28, batch loss = 2.23 (35.7 examples/sec; 0.224 sec/batch; 20h:21m:19s remains)
INFO - root - 2017-12-08 05:14:46.346215: step 5920, loss = 2.28, batch loss = 2.23 (35.3 examples/sec; 0.227 sec/batch; 20h:34m:24s remains)
INFO - root - 2017-12-08 05:14:48.570672: step 5930, loss = 2.28, batch loss = 2.23 (35.2 examples/sec; 0.227 sec/batch; 20h:35m:33s remains)
INFO - root - 2017-12-08 05:14:50.799159: step 5940, loss = 2.28, batch loss = 2.23 (35.1 examples/sec; 0.228 sec/batch; 20h:41m:36s remains)
INFO - root - 2017-12-08 05:14:53.040621: step 5950, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.220 sec/batch; 19h:58m:45s remains)
INFO - root - 2017-12-08 05:14:55.279684: step 5960, loss = 2.28, batch loss = 2.23 (36.6 examples/sec; 0.219 sec/batch; 19h:51m:00s remains)
INFO - root - 2017-12-08 05:14:57.545299: step 5970, loss = 2.28, batch loss = 2.23 (32.1 examples/sec; 0.249 sec/batch; 22h:36m:31s remains)
INFO - root - 2017-12-08 05:14:59.777706: step 5980, loss = 2.28, batch loss = 2.23 (35.2 examples/sec; 0.227 sec/batch; 20h:35m:37s remains)
INFO - root - 2017-12-08 05:15:02.033673: step 5990, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 20h:03m:26s remains)
INFO - root - 2017-12-08 05:15:04.274918: step 6000, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.226 sec/batch; 20h:27m:46s remains)
2017-12-08 05:15:04.557383: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4286981 -4.4286904 -4.4286823 -4.4286613 -4.4286451 -4.4286489 -4.4286742 -4.4286971 -4.4287105 -4.4287133 -4.4287119 -4.4287062 -4.428709 -4.4287415 -4.4287896][-4.4286852 -4.4286742 -4.4286733 -4.4286575 -4.42864 -4.428638 -4.4286695 -4.4287105 -4.4287305 -4.4287324 -4.4287257 -4.4287162 -4.428719 -4.428751 -4.428792][-4.4286847 -4.4286661 -4.4286618 -4.4286432 -4.4286051 -4.4285889 -4.4286227 -4.4286885 -4.42874 -4.4287605 -4.4287686 -4.4287624 -4.428761 -4.428782 -4.4288054][-4.4286833 -4.4286513 -4.4286356 -4.4286013 -4.4285493 -4.4285116 -4.42852 -4.4285889 -4.428678 -4.4287319 -4.4287686 -4.4287853 -4.4287896 -4.4287982 -4.4288058][-4.4286828 -4.428638 -4.4285979 -4.4285388 -4.4284596 -4.4283719 -4.4283061 -4.4283524 -4.4284925 -4.4286003 -4.4286757 -4.4287343 -4.4287682 -4.4287825 -4.4287887][-4.428678 -4.4286232 -4.4285522 -4.428453 -4.4283228 -4.4281373 -4.4279342 -4.427928 -4.4281554 -4.42837 -4.428525 -4.4286447 -4.42872 -4.4287548 -4.4287748][-4.4286451 -4.4285965 -4.4285021 -4.428371 -4.4281936 -4.4279137 -4.4275694 -4.4275126 -4.427865 -4.4281974 -4.4284186 -4.4285774 -4.4286852 -4.4287434 -4.4287753][-4.4286284 -4.4286 -4.4285097 -4.4283834 -4.428205 -4.4279284 -4.42759 -4.4275293 -4.4278617 -4.4281859 -4.4284 -4.4285531 -4.4286695 -4.4287386 -4.4287729][-4.4286547 -4.4286542 -4.4285946 -4.4285107 -4.428391 -4.4282122 -4.4279971 -4.4279265 -4.4280992 -4.4283128 -4.4284596 -4.4285746 -4.428678 -4.4287367 -4.4287539][-4.4286766 -4.4286914 -4.4286594 -4.4286203 -4.428576 -4.4284878 -4.4283609 -4.4282727 -4.4283171 -4.4284244 -4.4285145 -4.4285936 -4.428678 -4.4287248 -4.4287252][-4.4286466 -4.4286618 -4.4286494 -4.4286494 -4.4286647 -4.4286375 -4.4285612 -4.4284716 -4.4284496 -4.4284925 -4.428545 -4.428606 -4.4286747 -4.4287152 -4.4287105][-4.4285665 -4.4285736 -4.4285827 -4.4286108 -4.4286585 -4.4286737 -4.4286242 -4.4285531 -4.4285235 -4.4285479 -4.428575 -4.4286113 -4.4286551 -4.4286857 -4.4286823][-4.4284921 -4.4284954 -4.4285259 -4.42857 -4.4286165 -4.4286432 -4.428617 -4.4285736 -4.428565 -4.4285827 -4.4285917 -4.4286046 -4.4286156 -4.4286294 -4.4286332][-4.428493 -4.4284945 -4.4285283 -4.4285612 -4.4285822 -4.4286056 -4.4285941 -4.4285684 -4.4285669 -4.4285722 -4.4285789 -4.4285693 -4.4285364 -4.4285269 -4.4285436][-4.4285288 -4.4285011 -4.4285078 -4.428514 -4.428503 -4.4285121 -4.4285083 -4.4285007 -4.4284997 -4.4285035 -4.4285264 -4.4285216 -4.4284735 -4.4284544 -4.42848]]...]
INFO - root - 2017-12-08 05:15:06.784324: step 6010, loss = 2.28, batch loss = 2.23 (34.4 examples/sec; 0.233 sec/batch; 21h:06m:49s remains)
INFO - root - 2017-12-08 05:15:09.039318: step 6020, loss = 2.28, batch loss = 2.23 (36.0 examples/sec; 0.222 sec/batch; 20h:10m:11s remains)
INFO - root - 2017-12-08 05:15:11.318401: step 6030, loss = 2.28, batch loss = 2.23 (35.3 examples/sec; 0.227 sec/batch; 20h:33m:21s remains)
INFO - root - 2017-12-08 05:15:13.563392: step 6040, loss = 2.28, batch loss = 2.23 (34.5 examples/sec; 0.232 sec/batch; 21h:02m:57s remains)
INFO - root - 2017-12-08 05:15:15.798135: step 6050, loss = 2.28, batch loss = 2.23 (34.5 examples/sec; 0.232 sec/batch; 21h:01m:11s remains)
INFO - root - 2017-12-08 05:15:18.037599: step 6060, loss = 2.28, batch loss = 2.23 (33.9 examples/sec; 0.236 sec/batch; 21h:23m:08s remains)
INFO - root - 2017-12-08 05:15:20.255653: step 6070, loss = 2.28, batch loss = 2.23 (35.7 examples/sec; 0.224 sec/batch; 20h:18m:33s remains)
INFO - root - 2017-12-08 05:15:22.500944: step 6080, loss = 2.28, batch loss = 2.23 (34.3 examples/sec; 0.233 sec/batch; 21h:09m:10s remains)
INFO - root - 2017-12-08 05:15:24.749300: step 6090, loss = 2.28, batch loss = 2.23 (36.0 examples/sec; 0.222 sec/batch; 20h:09m:37s remains)
INFO - root - 2017-12-08 05:15:26.968555: step 6100, loss = 2.28, batch loss = 2.23 (36.7 examples/sec; 0.218 sec/batch; 19h:47m:18s remains)
2017-12-08 05:15:27.268216: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289632 -4.4289784 -4.4289851 -4.4289756 -4.4289517 -4.4289293 -4.4289169 -4.4289169 -4.4289184 -4.428936 -4.4289608 -4.4289751 -4.4289718 -4.4289565 -4.4289441][-4.4289465 -4.4289575 -4.4289603 -4.4289479 -4.4289193 -4.4288883 -4.4288712 -4.4288745 -4.4288745 -4.428895 -4.4289308 -4.4289575 -4.4289551 -4.4289341 -4.428925][-4.4289384 -4.4289432 -4.4289412 -4.4289269 -4.4288898 -4.4288492 -4.4288235 -4.4288316 -4.428833 -4.4288516 -4.428896 -4.4289393 -4.4289393 -4.4289107 -4.4289017][-4.4289289 -4.4289269 -4.4289165 -4.4288979 -4.4288578 -4.4288077 -4.428772 -4.4287758 -4.428781 -4.4287996 -4.4288468 -4.4288983 -4.4289026 -4.4288697 -4.4288621][-4.4289088 -4.4288964 -4.4288807 -4.4288654 -4.428834 -4.4287806 -4.4287329 -4.4287314 -4.4287477 -4.4287663 -4.4288015 -4.4288449 -4.4288406 -4.4288 -4.428792][-4.4288955 -4.4288745 -4.428853 -4.4288387 -4.4288135 -4.4287591 -4.4287062 -4.4287086 -4.4287429 -4.4287715 -4.4287944 -4.4288187 -4.4287953 -4.42874 -4.4287257][-4.4288869 -4.4288578 -4.42883 -4.4288096 -4.4287777 -4.4287262 -4.4286747 -4.428679 -4.4287214 -4.428762 -4.4287939 -4.4288116 -4.4287767 -4.4287128 -4.428689][-4.4288826 -4.4288468 -4.4288096 -4.4287786 -4.42874 -4.428688 -4.428638 -4.4286337 -4.428669 -4.4287233 -4.4287791 -4.4288068 -4.4287772 -4.4287224 -4.4286971][-4.4288855 -4.4288516 -4.428812 -4.4287724 -4.4287257 -4.4286714 -4.4286218 -4.4286056 -4.4286308 -4.4286966 -4.4287705 -4.4288044 -4.4287882 -4.4287553 -4.428741][-4.4288955 -4.4288688 -4.4288325 -4.4287887 -4.4287362 -4.4286776 -4.4286251 -4.4286094 -4.4286304 -4.4286914 -4.428761 -4.4287944 -4.4287915 -4.4287763 -4.4287696][-4.4288983 -4.4288745 -4.4288416 -4.4287972 -4.4287438 -4.4286847 -4.4286284 -4.4286132 -4.4286375 -4.4286942 -4.4287558 -4.4287877 -4.4287949 -4.4287877 -4.4287806][-4.428895 -4.4288692 -4.428833 -4.428792 -4.4287481 -4.4287 -4.4286528 -4.4286442 -4.4286728 -4.4287233 -4.4287715 -4.4287977 -4.4288058 -4.4288044 -4.4288][-4.4288945 -4.4288712 -4.4288406 -4.4288125 -4.428793 -4.4287663 -4.4287362 -4.4287329 -4.4287572 -4.428791 -4.4288168 -4.42883 -4.4288316 -4.4288306 -4.42883][-4.4288979 -4.4288831 -4.4288683 -4.4288568 -4.4288568 -4.4288507 -4.4288383 -4.4288383 -4.428853 -4.4288697 -4.4288774 -4.4288783 -4.428875 -4.428874 -4.4288764][-4.4289041 -4.4288955 -4.4288907 -4.4288883 -4.4288974 -4.4289017 -4.4289002 -4.4289026 -4.4289107 -4.428916 -4.4289136 -4.42891 -4.4289055 -4.4289045 -4.428905]]...]
INFO - root - 2017-12-08 05:15:29.491739: step 6110, loss = 2.28, batch loss = 2.23 (36.6 examples/sec; 0.218 sec/batch; 19h:47m:49s remains)
INFO - root - 2017-12-08 05:15:31.732982: step 6120, loss = 2.28, batch loss = 2.23 (35.8 examples/sec; 0.223 sec/batch; 20h:15m:28s remains)
INFO - root - 2017-12-08 05:15:33.981391: step 6130, loss = 2.28, batch loss = 2.23 (35.0 examples/sec; 0.229 sec/batch; 20h:43m:00s remains)
INFO - root - 2017-12-08 05:15:36.247621: step 6140, loss = 2.28, batch loss = 2.23 (36.6 examples/sec; 0.219 sec/batch; 19h:49m:52s remains)
INFO - root - 2017-12-08 05:15:38.489764: step 6150, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 20h:03m:37s remains)
INFO - root - 2017-12-08 05:15:40.704380: step 6160, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.221 sec/batch; 19h:59m:28s remains)
INFO - root - 2017-12-08 05:15:42.971483: step 6170, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 19h:54m:42s remains)
INFO - root - 2017-12-08 05:15:45.199195: step 6180, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 20h:00m:48s remains)
INFO - root - 2017-12-08 05:15:47.418905: step 6190, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 19h:56m:15s remains)
INFO - root - 2017-12-08 05:15:49.646406: step 6200, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.226 sec/batch; 20h:26m:22s remains)
2017-12-08 05:15:49.925977: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4287515 -4.4287291 -4.4287329 -4.428771 -4.4287887 -4.4287786 -4.4287362 -4.4286637 -4.4285879 -4.4285493 -4.4285831 -4.4286485 -4.4286995 -4.4287391 -4.4287963][-4.4287496 -4.4287372 -4.4287577 -4.4288049 -4.4288197 -4.428792 -4.4287281 -4.4286489 -4.4285917 -4.428566 -4.4285941 -4.4286585 -4.4287243 -4.4287844 -4.4288568][-4.4287643 -4.4287705 -4.428802 -4.428844 -4.4288487 -4.4288054 -4.4287281 -4.4286466 -4.4286013 -4.4285765 -4.4285932 -4.4286609 -4.4287386 -4.4288187 -4.4288993][-4.4287958 -4.4288216 -4.4288588 -4.4288921 -4.4288788 -4.428813 -4.4287205 -4.42863 -4.4285846 -4.4285669 -4.4285855 -4.4286656 -4.4287586 -4.4288545 -4.4289308][-4.4288282 -4.4288669 -4.4289074 -4.4289279 -4.4288874 -4.4288015 -4.4286938 -4.428587 -4.4285331 -4.428535 -4.4285822 -4.4286866 -4.4288006 -4.4289017 -4.42896][-4.4288726 -4.4289136 -4.4289451 -4.42894 -4.4288731 -4.4287663 -4.4286361 -4.4285073 -4.4284568 -4.4285007 -4.4285955 -4.4287262 -4.4288497 -4.4289393 -4.4289641][-4.4288983 -4.4289317 -4.4289508 -4.4289227 -4.428833 -4.4286966 -4.4285378 -4.4284062 -4.4283848 -4.4284844 -4.4286208 -4.4287682 -4.4288888 -4.4289551 -4.4289455][-4.4288845 -4.4289083 -4.428915 -4.4288683 -4.4287596 -4.4285975 -4.4284253 -4.428318 -4.4283557 -4.4284968 -4.42865 -4.4288006 -4.428906 -4.428946 -4.428905][-4.4288473 -4.4288692 -4.4288707 -4.428812 -4.4286857 -4.4285135 -4.428359 -4.4283152 -4.4284096 -4.4285545 -4.4286981 -4.4288316 -4.4289112 -4.4289174 -4.4288516][-4.4288015 -4.4288292 -4.4288292 -4.4287663 -4.4286327 -4.4284725 -4.4283662 -4.4283881 -4.4285011 -4.4286232 -4.4287467 -4.4288588 -4.4289112 -4.4288888 -4.4288077][-4.4287519 -4.4287877 -4.4287848 -4.4287186 -4.4285922 -4.4284644 -4.4284177 -4.4284782 -4.4285789 -4.4286757 -4.4287853 -4.4288783 -4.4289069 -4.4288669 -4.4287744][-4.4287071 -4.4287386 -4.4287257 -4.4286661 -4.4285617 -4.4284782 -4.4284811 -4.4285603 -4.4286494 -4.4287262 -4.4288211 -4.4288945 -4.4289041 -4.4288516 -4.428751][-4.428679 -4.4287019 -4.4286842 -4.4286389 -4.4285688 -4.4285278 -4.4285569 -4.4286308 -4.428709 -4.4287758 -4.4288588 -4.4289126 -4.4289045 -4.4288354 -4.4287262][-4.4286857 -4.4287 -4.4286795 -4.42865 -4.4286165 -4.4286041 -4.4286332 -4.4286909 -4.4287591 -4.4288259 -4.4288921 -4.4289207 -4.4288936 -4.4288125 -4.4287009][-4.4287148 -4.4287238 -4.4287052 -4.4286871 -4.428678 -4.4286804 -4.4287024 -4.4287424 -4.4287925 -4.42885 -4.428894 -4.4289079 -4.4288707 -4.4287882 -4.428679]]...]
INFO - root - 2017-12-08 05:15:52.167323: step 6210, loss = 2.28, batch loss = 2.23 (35.1 examples/sec; 0.228 sec/batch; 20h:40m:21s remains)
INFO - root - 2017-12-08 05:15:54.387372: step 6220, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 20h:03m:03s remains)
INFO - root - 2017-12-08 05:15:56.603102: step 6230, loss = 2.28, batch loss = 2.23 (35.6 examples/sec; 0.225 sec/batch; 20h:21m:32s remains)
INFO - root - 2017-12-08 05:15:58.825162: step 6240, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.222 sec/batch; 20h:06m:38s remains)
INFO - root - 2017-12-08 05:16:01.068040: step 6250, loss = 2.28, batch loss = 2.23 (35.6 examples/sec; 0.225 sec/batch; 20h:22m:09s remains)
INFO - root - 2017-12-08 05:16:03.319279: step 6260, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 19h:53m:13s remains)
INFO - root - 2017-12-08 05:16:05.554124: step 6270, loss = 2.28, batch loss = 2.23 (37.3 examples/sec; 0.214 sec/batch; 19h:25m:52s remains)
INFO - root - 2017-12-08 05:16:07.815260: step 6280, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.221 sec/batch; 20h:03m:15s remains)
INFO - root - 2017-12-08 05:16:10.094209: step 6290, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 19h:55m:59s remains)
INFO - root - 2017-12-08 05:16:12.321130: step 6300, loss = 2.28, batch loss = 2.23 (34.9 examples/sec; 0.229 sec/batch; 20h:46m:36s remains)
2017-12-08 05:16:12.608845: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289622 -4.4289055 -4.4288206 -4.428719 -4.4286022 -4.4285312 -4.4285631 -4.4286304 -4.428659 -4.4286637 -4.428669 -4.42868 -4.4286904 -4.4287105 -4.4287381][-4.4289708 -4.4289126 -4.428823 -4.4287105 -4.4285731 -4.4284692 -4.4284887 -4.428587 -4.4286475 -4.42866 -4.4286609 -4.4286695 -4.4286847 -4.4287157 -4.4287562][-4.42898 -4.4289265 -4.4288359 -4.4287124 -4.4285545 -4.4284105 -4.4284005 -4.428524 -4.428618 -4.4286432 -4.4286427 -4.428648 -4.4286661 -4.4287081 -4.428762][-4.4289875 -4.4289436 -4.4288592 -4.4287343 -4.428566 -4.4283876 -4.4283328 -4.4284592 -4.4285808 -4.4286213 -4.428616 -4.4286132 -4.4286323 -4.4286861 -4.4287505][-4.4289889 -4.4289556 -4.42888 -4.4287634 -4.4286 -4.4284019 -4.4282985 -4.4284081 -4.42855 -4.4286075 -4.4286051 -4.4285927 -4.4286084 -4.4286642 -4.42873][-4.4289837 -4.4289613 -4.4289002 -4.428803 -4.4286571 -4.428452 -4.42831 -4.4283824 -4.4285264 -4.4286 -4.4286108 -4.4285951 -4.4286017 -4.4286489 -4.4287024][-4.4289775 -4.4289632 -4.4289174 -4.4288387 -4.4287109 -4.4285064 -4.4283404 -4.4283667 -4.428494 -4.4285851 -4.4286146 -4.428607 -4.4286027 -4.4286304 -4.4286604][-4.4289713 -4.4289618 -4.4289274 -4.4288654 -4.4287624 -4.4285769 -4.4284077 -4.4283934 -4.4284854 -4.428587 -4.4286389 -4.4286475 -4.4286366 -4.428638 -4.4286389][-4.4289694 -4.4289613 -4.4289346 -4.42889 -4.4288168 -4.4286642 -4.4285107 -4.4284692 -4.4285173 -4.4286056 -4.4286771 -4.4287109 -4.4287086 -4.4286923 -4.4286675][-4.4289708 -4.4289637 -4.4289422 -4.4289093 -4.4288616 -4.4287457 -4.4286189 -4.4285684 -4.42858 -4.4286408 -4.4287186 -4.4287758 -4.42879 -4.4287686 -4.4287276][-4.4289732 -4.428966 -4.428947 -4.4289193 -4.4288931 -4.4288168 -4.4287205 -4.42867 -4.4286551 -4.4286876 -4.4287615 -4.4288406 -4.4288769 -4.4288607 -4.4288135][-4.4289765 -4.4289675 -4.428947 -4.4289231 -4.4289107 -4.4288735 -4.4288092 -4.4287596 -4.4287257 -4.4287391 -4.4288082 -4.4289017 -4.4289532 -4.4289422 -4.4288974][-4.4289827 -4.4289742 -4.4289551 -4.4289346 -4.4289284 -4.428916 -4.4288769 -4.42883 -4.4287872 -4.4287963 -4.4288564 -4.42895 -4.4290085 -4.4290042 -4.4289675][-4.4289894 -4.42898 -4.4289622 -4.4289441 -4.42894 -4.42894 -4.4289207 -4.4288831 -4.4288421 -4.4288483 -4.4288955 -4.4289746 -4.42903 -4.4290318 -4.4290071][-4.4289904 -4.4289784 -4.4289608 -4.4289474 -4.4289489 -4.428956 -4.4289551 -4.4289269 -4.4288864 -4.4288821 -4.42891 -4.4289708 -4.42902 -4.4290285 -4.4290147]]...]
INFO - root - 2017-12-08 05:16:14.827758: step 6310, loss = 2.28, batch loss = 2.23 (34.6 examples/sec; 0.231 sec/batch; 20h:56m:09s remains)
INFO - root - 2017-12-08 05:16:17.050483: step 6320, loss = 2.28, batch loss = 2.23 (35.8 examples/sec; 0.223 sec/batch; 20h:13m:51s remains)
INFO - root - 2017-12-08 05:16:19.342504: step 6330, loss = 2.28, batch loss = 2.23 (34.3 examples/sec; 0.233 sec/batch; 21h:09m:19s remains)
INFO - root - 2017-12-08 05:16:21.566203: step 6340, loss = 2.28, batch loss = 2.23 (36.0 examples/sec; 0.222 sec/batch; 20h:06m:30s remains)
INFO - root - 2017-12-08 05:16:23.793441: step 6350, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 20h:01m:36s remains)
INFO - root - 2017-12-08 05:16:26.049502: step 6360, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.222 sec/batch; 20h:05m:02s remains)
INFO - root - 2017-12-08 05:16:28.326474: step 6370, loss = 2.28, batch loss = 2.23 (33.6 examples/sec; 0.238 sec/batch; 21h:35m:11s remains)
INFO - root - 2017-12-08 05:16:30.562230: step 6380, loss = 2.28, batch loss = 2.23 (37.1 examples/sec; 0.215 sec/batch; 19h:30m:46s remains)
INFO - root - 2017-12-08 05:16:32.798548: step 6390, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 19h:53m:30s remains)
INFO - root - 2017-12-08 05:16:35.028432: step 6400, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 19h:54m:14s remains)
2017-12-08 05:16:35.341462: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4287887 -4.4287934 -4.42879 -4.4287658 -4.4287457 -4.4287229 -4.4287148 -4.4287086 -4.4287138 -4.4287271 -4.4287524 -4.4287663 -4.4287572 -4.4287233 -4.4286871][-4.428791 -4.4287772 -4.4287639 -4.4287362 -4.4287138 -4.4286957 -4.4286966 -4.4286966 -4.4287047 -4.4287252 -4.4287543 -4.4287739 -4.4287729 -4.4287429 -4.42871][-4.4287777 -4.428761 -4.4287467 -4.4287162 -4.4286847 -4.428659 -4.4286571 -4.4286647 -4.4286838 -4.4287114 -4.4287386 -4.4287567 -4.4287586 -4.428731 -4.4286985][-4.4287724 -4.42876 -4.4287486 -4.4287148 -4.4286623 -4.4286113 -4.4286013 -4.4286284 -4.4286661 -4.4287004 -4.428719 -4.4287233 -4.4287176 -4.4286942 -4.428668][-4.4287658 -4.4287596 -4.4287505 -4.4287128 -4.4286456 -4.4285755 -4.4285564 -4.4285941 -4.4286475 -4.4286833 -4.4286866 -4.4286704 -4.4286532 -4.4286394 -4.4286213][-4.4287467 -4.4287477 -4.4287419 -4.4287033 -4.42863 -4.4285507 -4.4285216 -4.4285645 -4.4286304 -4.4286637 -4.4286566 -4.4286294 -4.428606 -4.4286046 -4.4286046][-4.4287086 -4.4287057 -4.4287033 -4.4286714 -4.4286027 -4.4285226 -4.4284854 -4.4285293 -4.4286017 -4.4286408 -4.4286451 -4.4286342 -4.4286218 -4.428638 -4.4286566][-4.4286671 -4.4286523 -4.4286561 -4.428638 -4.4285817 -4.4285083 -4.4284644 -4.428503 -4.4285746 -4.4286218 -4.4286385 -4.4286561 -4.4286718 -4.42871 -4.4287367][-4.4286537 -4.4286275 -4.4286313 -4.4286284 -4.4285893 -4.4285312 -4.4284868 -4.428515 -4.4285731 -4.4286094 -4.4286256 -4.4286637 -4.4287128 -4.4287724 -4.4288039][-4.42867 -4.4286337 -4.4286366 -4.4286528 -4.4286361 -4.4285975 -4.42856 -4.4285679 -4.4286013 -4.4286237 -4.4286461 -4.4286966 -4.4287539 -4.4288116 -4.4288311][-4.4286947 -4.4286537 -4.4286513 -4.42868 -4.4286909 -4.4286795 -4.4286561 -4.4286489 -4.4286575 -4.4286656 -4.428689 -4.4287319 -4.4287739 -4.4288092 -4.42881][-4.4287348 -4.4286933 -4.4286823 -4.4287081 -4.4287314 -4.428741 -4.4287329 -4.4287224 -4.4287229 -4.4287229 -4.4287329 -4.4287486 -4.4287605 -4.4287739 -4.4287648][-4.4287729 -4.4287338 -4.4287152 -4.4287281 -4.4287534 -4.4287744 -4.4287753 -4.4287734 -4.4287796 -4.4287791 -4.4287724 -4.4287577 -4.428741 -4.4287376 -4.4287271][-4.4288015 -4.4287667 -4.4287453 -4.4287467 -4.4287686 -4.4287958 -4.4288034 -4.4288087 -4.4288168 -4.428813 -4.4287896 -4.42876 -4.4287324 -4.4287243 -4.4287181][-4.4288325 -4.4288015 -4.4287796 -4.4287715 -4.4287858 -4.42881 -4.4288182 -4.4288263 -4.4288378 -4.4288311 -4.4288015 -4.428772 -4.4287529 -4.4287524 -4.4287491]]...]
INFO - root - 2017-12-08 05:16:37.570971: step 6410, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 20h:01m:33s remains)
INFO - root - 2017-12-08 05:16:39.794612: step 6420, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 20h:10m:36s remains)
INFO - root - 2017-12-08 05:16:42.039093: step 6430, loss = 2.28, batch loss = 2.23 (36.6 examples/sec; 0.219 sec/batch; 19h:49m:09s remains)
INFO - root - 2017-12-08 05:16:44.260564: step 6440, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 19h:50m:47s remains)
INFO - root - 2017-12-08 05:16:46.491360: step 6450, loss = 2.28, batch loss = 2.23 (35.3 examples/sec; 0.227 sec/batch; 20h:30m:52s remains)
INFO - root - 2017-12-08 05:16:48.734489: step 6460, loss = 2.28, batch loss = 2.23 (34.1 examples/sec; 0.234 sec/batch; 21h:13m:40s remains)
INFO - root - 2017-12-08 05:16:50.978267: step 6470, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.220 sec/batch; 19h:56m:38s remains)
INFO - root - 2017-12-08 05:16:53.211518: step 6480, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 19h:52m:15s remains)
INFO - root - 2017-12-08 05:16:55.463829: step 6490, loss = 2.28, batch loss = 2.23 (35.6 examples/sec; 0.224 sec/batch; 20h:19m:39s remains)
INFO - root - 2017-12-08 05:16:57.712135: step 6500, loss = 2.28, batch loss = 2.23 (34.9 examples/sec; 0.229 sec/batch; 20h:44m:13s remains)
2017-12-08 05:16:57.999107: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289789 -4.429009 -4.4290214 -4.4290223 -4.4289966 -4.4289393 -4.4288654 -4.4288058 -4.4287539 -4.4287047 -4.4286613 -4.4286633 -4.428719 -4.4287906 -4.4288549][-4.4289885 -4.429029 -4.4290414 -4.4290357 -4.428987 -4.4289093 -4.42882 -4.4287491 -4.4286981 -4.4286733 -4.428658 -4.4286618 -4.4287062 -4.4287791 -4.4288464][-4.4289947 -4.4290357 -4.4290409 -4.4290252 -4.4289579 -4.4288607 -4.4287629 -4.4286866 -4.428637 -4.4286265 -4.4286351 -4.4286513 -4.4286828 -4.4287553 -4.428833][-4.4290023 -4.4290395 -4.4290371 -4.4290056 -4.4289241 -4.4288106 -4.4287052 -4.4286304 -4.4285841 -4.4285712 -4.4285889 -4.4286137 -4.428648 -4.4287362 -4.4288282][-4.429008 -4.4290423 -4.4290357 -4.4289894 -4.4288921 -4.4287634 -4.42865 -4.4285765 -4.4285278 -4.4285121 -4.428534 -4.4285679 -4.4286189 -4.42873 -4.4288316][-4.4290161 -4.4290504 -4.4290404 -4.4289775 -4.428853 -4.4287019 -4.4285684 -4.4284811 -4.428441 -4.4284544 -4.4284978 -4.4285574 -4.4286375 -4.4287605 -4.4288592][-4.4290257 -4.4290576 -4.4290423 -4.4289632 -4.4288068 -4.4286218 -4.4284582 -4.428339 -4.4283266 -4.4283981 -4.4284725 -4.4285641 -4.4286714 -4.428802 -4.4288988][-4.429029 -4.4290571 -4.4290347 -4.4289365 -4.4287457 -4.4285135 -4.428308 -4.4281864 -4.4282303 -4.4283576 -4.428473 -4.428597 -4.4287205 -4.428844 -4.4289269][-4.4290318 -4.4290447 -4.42901 -4.4288926 -4.4286704 -4.4283919 -4.4281535 -4.4280577 -4.4281588 -4.4283195 -4.4284668 -4.4286213 -4.4287586 -4.4288697 -4.4289322][-4.4290295 -4.429019 -4.4289665 -4.42883 -4.4286065 -4.4283247 -4.4280858 -4.4280119 -4.4281349 -4.4282985 -4.4284587 -4.428628 -4.4287739 -4.4288769 -4.4289236][-4.429019 -4.4289846 -4.42891 -4.4287643 -4.4285722 -4.4283528 -4.4281721 -4.4281168 -4.4282165 -4.4283528 -4.4284964 -4.4286556 -4.4287944 -4.4288807 -4.428915][-4.4290066 -4.4289556 -4.4288635 -4.4287224 -4.4285831 -4.4284477 -4.4283452 -4.4283113 -4.4283757 -4.428472 -4.4285812 -4.4287081 -4.4288168 -4.42888 -4.4289002][-4.4289951 -4.4289412 -4.4288454 -4.4287243 -4.4286356 -4.4285583 -4.4285078 -4.4284973 -4.4285483 -4.428616 -4.4286895 -4.4287763 -4.4288487 -4.4288893 -4.4288969][-4.4289861 -4.4289412 -4.4288611 -4.4287643 -4.4286981 -4.4286489 -4.4286203 -4.4286251 -4.4286723 -4.4287229 -4.4287777 -4.4288406 -4.4288869 -4.4289088 -4.42891][-4.4289861 -4.4289627 -4.4289083 -4.428834 -4.4287806 -4.4287457 -4.4287291 -4.4287376 -4.4287729 -4.4288111 -4.4288521 -4.428894 -4.4289193 -4.42893 -4.42893]]...]
INFO - root - 2017-12-08 05:17:00.223799: step 6510, loss = 2.28, batch loss = 2.23 (36.8 examples/sec; 0.217 sec/batch; 19h:41m:31s remains)
INFO - root - 2017-12-08 05:17:02.487629: step 6520, loss = 2.28, batch loss = 2.23 (34.7 examples/sec; 0.230 sec/batch; 20h:51m:45s remains)
INFO - root - 2017-12-08 05:17:04.753673: step 6530, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.222 sec/batch; 20h:05m:22s remains)
INFO - root - 2017-12-08 05:17:07.000600: step 6540, loss = 2.28, batch loss = 2.23 (35.2 examples/sec; 0.228 sec/batch; 20h:35m:59s remains)
INFO - root - 2017-12-08 05:17:09.250056: step 6550, loss = 2.28, batch loss = 2.23 (36.7 examples/sec; 0.218 sec/batch; 19h:44m:42s remains)
INFO - root - 2017-12-08 05:17:11.499190: step 6560, loss = 2.28, batch loss = 2.23 (34.7 examples/sec; 0.231 sec/batch; 20h:52m:34s remains)
INFO - root - 2017-12-08 05:17:13.793617: step 6570, loss = 2.28, batch loss = 2.23 (34.6 examples/sec; 0.232 sec/batch; 20h:57m:46s remains)
INFO - root - 2017-12-08 05:17:16.031281: step 6580, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.222 sec/batch; 20h:03m:32s remains)
INFO - root - 2017-12-08 05:17:18.254589: step 6590, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 19h:51m:14s remains)
INFO - root - 2017-12-08 05:17:20.513183: step 6600, loss = 2.28, batch loss = 2.23 (37.7 examples/sec; 0.212 sec/batch; 19h:13m:59s remains)
2017-12-08 05:17:20.802750: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289627 -4.4289331 -4.4288731 -4.4287953 -4.4287376 -4.4286919 -4.4286447 -4.4286046 -4.4285665 -4.428544 -4.4285541 -4.4285841 -4.428607 -4.4286413 -4.4286494][-4.428966 -4.4289389 -4.4288807 -4.4288068 -4.4287448 -4.4286928 -4.4286532 -4.4286122 -4.4285703 -4.4285407 -4.4285455 -4.4285922 -4.4286356 -4.4286814 -4.4286885][-4.428966 -4.4289384 -4.4288831 -4.428813 -4.4287515 -4.4286981 -4.4286556 -4.4286189 -4.428596 -4.4285769 -4.4285746 -4.428616 -4.4286661 -4.4287071 -4.4287138][-4.4289613 -4.42893 -4.4288712 -4.4287972 -4.4287291 -4.4286766 -4.4286337 -4.4286103 -4.4286141 -4.4286108 -4.4286051 -4.428637 -4.428678 -4.4287143 -4.4287271][-4.4289508 -4.4289136 -4.4288435 -4.4287643 -4.4286928 -4.4286423 -4.4285893 -4.4285574 -4.4285712 -4.4285932 -4.428597 -4.4286137 -4.4286418 -4.4286838 -4.4287109][-4.4289279 -4.4288793 -4.4287896 -4.4286962 -4.4286246 -4.4285679 -4.4284921 -4.4284382 -4.4284625 -4.4285173 -4.4285488 -4.4285655 -4.428587 -4.4286366 -4.42868][-4.4289007 -4.4288392 -4.4287267 -4.4286108 -4.4285274 -4.4284515 -4.4283481 -4.4282804 -4.4283214 -4.4284158 -4.4284787 -4.4285126 -4.4285378 -4.4285917 -4.4286451][-4.4288831 -4.4288158 -4.428699 -4.4285774 -4.4284844 -4.4283986 -4.4282804 -4.4282041 -4.4282436 -4.4283442 -4.4284205 -4.428472 -4.4284973 -4.4285407 -4.4285855][-4.4288731 -4.4288082 -4.4287095 -4.4286127 -4.428544 -4.4284725 -4.4283619 -4.4282789 -4.4282966 -4.4283781 -4.4284487 -4.4284935 -4.4284983 -4.4284997 -4.4285178][-4.428874 -4.4288139 -4.4287367 -4.428668 -4.4286275 -4.4285793 -4.4284921 -4.4284167 -4.4284229 -4.4284821 -4.4285312 -4.4285474 -4.4285192 -4.4284873 -4.428484][-4.4288821 -4.4288225 -4.4287567 -4.4287028 -4.4286761 -4.428647 -4.4285955 -4.4285507 -4.4285555 -4.4285874 -4.4286075 -4.4286008 -4.4285545 -4.4285049 -4.4284954][-4.4288926 -4.4288368 -4.4287782 -4.4287248 -4.4286981 -4.4286833 -4.4286628 -4.4286461 -4.4286537 -4.4286604 -4.4286537 -4.4286308 -4.4285803 -4.428524 -4.4285188][-4.4289131 -4.4288678 -4.4288144 -4.4287615 -4.4287343 -4.4287262 -4.4287167 -4.4287057 -4.4287114 -4.4287066 -4.4286857 -4.4286637 -4.428628 -4.4285779 -4.4285741][-4.4289422 -4.4289093 -4.4288654 -4.42882 -4.4288011 -4.4287992 -4.4287968 -4.4287872 -4.4287858 -4.4287777 -4.42876 -4.4287424 -4.4287176 -4.4286737 -4.4286623][-4.4289694 -4.4289484 -4.4289179 -4.4288888 -4.4288836 -4.4288912 -4.42889 -4.4288769 -4.4288707 -4.42886 -4.4288468 -4.4288354 -4.4288177 -4.4287858 -4.428772]]...]
INFO - root - 2017-12-08 05:17:23.010867: step 6610, loss = 2.28, batch loss = 2.23 (36.7 examples/sec; 0.218 sec/batch; 19h:44m:33s remains)
INFO - root - 2017-12-08 05:17:25.292392: step 6620, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.221 sec/batch; 20h:02m:29s remains)
INFO - root - 2017-12-08 05:17:27.524194: step 6630, loss = 2.28, batch loss = 2.23 (34.8 examples/sec; 0.230 sec/batch; 20h:50m:17s remains)
INFO - root - 2017-12-08 05:17:29.756578: step 6640, loss = 2.28, batch loss = 2.23 (35.6 examples/sec; 0.225 sec/batch; 20h:22m:08s remains)
INFO - root - 2017-12-08 05:17:31.979894: step 6650, loss = 2.28, batch loss = 2.23 (36.0 examples/sec; 0.222 sec/batch; 20h:08m:03s remains)
INFO - root - 2017-12-08 05:17:34.210035: step 6660, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.226 sec/batch; 20h:25m:05s remains)
INFO - root - 2017-12-08 05:17:36.433610: step 6670, loss = 2.28, batch loss = 2.23 (36.8 examples/sec; 0.217 sec/batch; 19h:40m:22s remains)
INFO - root - 2017-12-08 05:17:38.722812: step 6680, loss = 2.28, batch loss = 2.23 (34.2 examples/sec; 0.234 sec/batch; 21h:08m:57s remains)
INFO - root - 2017-12-08 05:17:40.969130: step 6690, loss = 2.28, batch loss = 2.23 (33.7 examples/sec; 0.237 sec/batch; 21h:27m:49s remains)
INFO - root - 2017-12-08 05:17:43.169880: step 6700, loss = 2.28, batch loss = 2.23 (35.3 examples/sec; 0.227 sec/batch; 20h:30m:23s remains)
2017-12-08 05:17:43.467742: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4286332 -4.4286728 -4.4286766 -4.4286275 -4.4285469 -4.4284482 -4.4283719 -4.4283886 -4.4284968 -4.4286079 -4.4287024 -4.4287658 -4.4288044 -4.4288096 -4.4287705][-4.4286351 -4.4286737 -4.4286919 -4.4286642 -4.4285951 -4.4284997 -4.4284115 -4.4283924 -4.4284568 -4.428534 -4.4286089 -4.4286771 -4.4287333 -4.4287667 -4.4287558][-4.4286695 -4.4287148 -4.428741 -4.4287224 -4.4286551 -4.4285703 -4.4284806 -4.4284329 -4.4284434 -4.4284778 -4.4285216 -4.4285793 -4.4286447 -4.4287052 -4.4287248][-4.4286866 -4.4287505 -4.4287858 -4.4287677 -4.4287014 -4.4286242 -4.4285455 -4.4284983 -4.4284873 -4.42849 -4.4285059 -4.42854 -4.4285917 -4.42865 -4.4286861][-4.4286461 -4.4287362 -4.4287848 -4.4287663 -4.4286933 -4.4286 -4.4285259 -4.4284997 -4.4285026 -4.4285192 -4.4285388 -4.4285645 -4.4285946 -4.4286346 -4.4286695][-4.4285665 -4.42868 -4.4287415 -4.4287171 -4.42863 -4.4285049 -4.4284072 -4.4283924 -4.4284449 -4.4285154 -4.4285731 -4.428616 -4.4286385 -4.4286671 -4.4286962][-4.4284916 -4.428606 -4.4286714 -4.4286485 -4.4285603 -4.4284182 -4.4283 -4.4282775 -4.4283667 -4.4284878 -4.4285908 -4.4286585 -4.4286861 -4.4287138 -4.4287391][-4.4284377 -4.4285355 -4.4285927 -4.4285703 -4.4284973 -4.4283786 -4.4282851 -4.4282694 -4.4283514 -4.428472 -4.4285784 -4.4286475 -4.4286795 -4.4287057 -4.4287314][-4.4284196 -4.428493 -4.4285388 -4.4285231 -4.4284744 -4.4283943 -4.4283366 -4.4283366 -4.4283938 -4.4284797 -4.4285674 -4.4286227 -4.4286447 -4.4286571 -4.4286771][-4.428463 -4.4285288 -4.4285631 -4.4285407 -4.4285059 -4.42846 -4.4284277 -4.42844 -4.4284825 -4.4285455 -4.4286122 -4.4286485 -4.428647 -4.4286366 -4.428648][-4.4285192 -4.4285841 -4.4286151 -4.4285927 -4.4285645 -4.4285398 -4.4285216 -4.42854 -4.4285765 -4.4286265 -4.4286838 -4.4287071 -4.4286895 -4.4286594 -4.4286561][-4.428586 -4.4286504 -4.4286819 -4.4286618 -4.4286385 -4.4286275 -4.4286156 -4.4286404 -4.428678 -4.4287195 -4.4287672 -4.428781 -4.4287581 -4.4287181 -4.4286981][-4.4286823 -4.4287338 -4.428762 -4.4287505 -4.4287357 -4.4287314 -4.428721 -4.4287372 -4.4287682 -4.4288025 -4.42884 -4.4288526 -4.4288397 -4.4288034 -4.4287734][-4.4287815 -4.4288163 -4.4288425 -4.4288425 -4.4288359 -4.4288321 -4.428822 -4.4288282 -4.4288492 -4.4288793 -4.4289083 -4.4289217 -4.4289174 -4.4288907 -4.4288621][-4.4288626 -4.4288869 -4.4289074 -4.4289122 -4.4289064 -4.4289 -4.4288912 -4.4288931 -4.4289069 -4.4289308 -4.4289541 -4.428967 -4.4289651 -4.4289479 -4.4289303]]...]
INFO - root - 2017-12-08 05:17:45.678327: step 6710, loss = 2.28, batch loss = 2.23 (35.7 examples/sec; 0.224 sec/batch; 20h:15m:28s remains)
INFO - root - 2017-12-08 05:17:47.899818: step 6720, loss = 2.28, batch loss = 2.23 (35.2 examples/sec; 0.227 sec/batch; 20h:32m:19s remains)
INFO - root - 2017-12-08 05:17:50.125447: step 6730, loss = 2.28, batch loss = 2.23 (35.4 examples/sec; 0.226 sec/batch; 20h:26m:26s remains)
INFO - root - 2017-12-08 05:17:52.354748: step 6740, loss = 2.28, batch loss = 2.23 (35.4 examples/sec; 0.226 sec/batch; 20h:26m:07s remains)
INFO - root - 2017-12-08 05:17:54.621009: step 6750, loss = 2.28, batch loss = 2.23 (37.0 examples/sec; 0.216 sec/batch; 19h:34m:48s remains)
INFO - root - 2017-12-08 05:17:56.844451: step 6760, loss = 2.28, batch loss = 2.23 (37.7 examples/sec; 0.212 sec/batch; 19h:10m:58s remains)
INFO - root - 2017-12-08 05:17:59.087499: step 6770, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.225 sec/batch; 20h:22m:20s remains)
INFO - root - 2017-12-08 05:18:01.308766: step 6780, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 19h:58m:44s remains)
INFO - root - 2017-12-08 05:18:03.549716: step 6790, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.219 sec/batch; 19h:51m:33s remains)
INFO - root - 2017-12-08 05:18:05.774424: step 6800, loss = 2.28, batch loss = 2.23 (35.8 examples/sec; 0.224 sec/batch; 20h:14m:23s remains)
2017-12-08 05:18:06.062738: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289155 -4.428863 -4.428803 -4.4287596 -4.4287338 -4.4287739 -4.4288158 -4.4288211 -4.4287977 -4.4288 -4.4287949 -4.4287844 -4.4287829 -4.428781 -4.4287791][-4.4289141 -4.4288592 -4.4287896 -4.4287291 -4.4286838 -4.4287109 -4.4287415 -4.4287386 -4.4287124 -4.4287262 -4.428751 -4.4287529 -4.4287448 -4.4287348 -4.42874][-4.4289155 -4.428864 -4.4287906 -4.4287233 -4.4286628 -4.42867 -4.4286838 -4.4286666 -4.4286318 -4.4286509 -4.4287033 -4.4287195 -4.4286985 -4.4286737 -4.4286757][-4.4289155 -4.4288692 -4.4287963 -4.4287252 -4.4286509 -4.42863 -4.4286275 -4.4286017 -4.4285684 -4.4285946 -4.4286642 -4.428689 -4.428659 -4.4286218 -4.428616][-4.4289165 -4.428874 -4.4288034 -4.4287381 -4.4286652 -4.4286323 -4.4286251 -4.4285984 -4.4285622 -4.4285836 -4.4286585 -4.4286842 -4.4286509 -4.4286194 -4.4286089][-4.4289169 -4.4288745 -4.4288039 -4.4287472 -4.4286866 -4.4286556 -4.4286504 -4.4286256 -4.4285951 -4.4286122 -4.4286695 -4.4286981 -4.4286814 -4.4286594 -4.4286504][-4.4289041 -4.4288526 -4.4287786 -4.428721 -4.4286747 -4.4286466 -4.4286337 -4.4286065 -4.4285932 -4.4286156 -4.4286618 -4.4286971 -4.4287019 -4.4286966 -4.4286919][-4.4288883 -4.4288225 -4.4287386 -4.42868 -4.4286427 -4.4286213 -4.4285846 -4.4285374 -4.4285212 -4.4285474 -4.42859 -4.42863 -4.4286628 -4.4286776 -4.4286847][-4.4288716 -4.4287887 -4.4286909 -4.42862 -4.4285808 -4.4285626 -4.4285092 -4.4284387 -4.4284067 -4.4284306 -4.428472 -4.4285188 -4.4285779 -4.4286203 -4.4286475][-4.428853 -4.4287586 -4.4286513 -4.428566 -4.4285173 -4.4285011 -4.428452 -4.4283834 -4.4283509 -4.4283752 -4.428411 -4.4284568 -4.4285274 -4.4285913 -4.4286404][-4.4288473 -4.428762 -4.42866 -4.4285731 -4.4285192 -4.428504 -4.4284697 -4.4284258 -4.428411 -4.4284315 -4.4284587 -4.4284964 -4.4285607 -4.4286265 -4.428688][-4.4288678 -4.4288 -4.4287219 -4.4286556 -4.4286137 -4.428606 -4.4285879 -4.4285684 -4.4285645 -4.4285731 -4.42859 -4.4286189 -4.4286656 -4.4287176 -4.4287686][-4.42891 -4.4288664 -4.4288158 -4.4287772 -4.4287543 -4.4287562 -4.4287515 -4.4287467 -4.4287395 -4.42873 -4.4287319 -4.4287457 -4.4287705 -4.4288015 -4.4288363][-4.4289432 -4.4289174 -4.4288826 -4.4288592 -4.428853 -4.4288597 -4.428863 -4.428863 -4.428854 -4.4288387 -4.4288344 -4.4288392 -4.42885 -4.4288678 -4.4288921][-4.4289556 -4.42894 -4.4289165 -4.4289012 -4.4289041 -4.428915 -4.4289222 -4.428925 -4.4289174 -4.4289036 -4.4288969 -4.4288979 -4.4289036 -4.4289155 -4.4289331]]...]
INFO - root - 2017-12-08 05:18:08.267690: step 6810, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.222 sec/batch; 20h:04m:32s remains)
INFO - root - 2017-12-08 05:18:10.501200: step 6820, loss = 2.28, batch loss = 2.23 (36.7 examples/sec; 0.218 sec/batch; 19h:44m:05s remains)
INFO - root - 2017-12-08 05:18:12.737809: step 6830, loss = 2.28, batch loss = 2.23 (35.7 examples/sec; 0.224 sec/batch; 20h:15m:44s remains)
INFO - root - 2017-12-08 05:18:14.970521: step 6840, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 19h:58m:39s remains)
INFO - root - 2017-12-08 05:18:17.197720: step 6850, loss = 2.28, batch loss = 2.23 (36.0 examples/sec; 0.222 sec/batch; 20h:07m:24s remains)
INFO - root - 2017-12-08 05:18:19.431463: step 6860, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.221 sec/batch; 20h:01m:59s remains)
INFO - root - 2017-12-08 05:18:21.663083: step 6870, loss = 2.28, batch loss = 2.23 (34.9 examples/sec; 0.229 sec/batch; 20h:43m:35s remains)
INFO - root - 2017-12-08 05:18:23.883329: step 6880, loss = 2.28, batch loss = 2.23 (35.6 examples/sec; 0.225 sec/batch; 20h:18m:57s remains)
INFO - root - 2017-12-08 05:18:26.109500: step 6890, loss = 2.28, batch loss = 2.23 (35.0 examples/sec; 0.228 sec/batch; 20h:39m:58s remains)
INFO - root - 2017-12-08 05:18:28.336522: step 6900, loss = 2.28, batch loss = 2.23 (36.8 examples/sec; 0.217 sec/batch; 19h:39m:32s remains)
2017-12-08 05:18:28.600967: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288149 -4.4287786 -4.428751 -4.4287314 -4.4287081 -4.428669 -4.4286408 -4.4286432 -4.4286957 -4.4287448 -4.4287748 -4.4287853 -4.4287863 -4.4287605 -4.4287066][-4.4288177 -4.4287691 -4.4287314 -4.4287186 -4.4287038 -4.4286613 -4.4286156 -4.4286075 -4.4286752 -4.42874 -4.4287772 -4.4287882 -4.4287891 -4.428761 -4.4286923][-4.4288144 -4.4287524 -4.428709 -4.4287 -4.4286952 -4.428658 -4.4286032 -4.4285851 -4.4286585 -4.4287338 -4.4287729 -4.4287882 -4.4287963 -4.4287705 -4.4286933][-4.4288092 -4.4287481 -4.4287062 -4.4286966 -4.4286861 -4.4286442 -4.4285679 -4.4285259 -4.4286017 -4.4286952 -4.4287434 -4.4287677 -4.4287858 -4.4287729 -4.4287024][-4.4288154 -4.4287629 -4.4287171 -4.4286895 -4.4286633 -4.4286041 -4.4284759 -4.4283834 -4.4284811 -4.428627 -4.4287009 -4.4287443 -4.4287777 -4.4287868 -4.4287324][-4.42883 -4.4287839 -4.4287281 -4.4286637 -4.4285965 -4.4284873 -4.4282703 -4.42811 -4.4282694 -4.4285135 -4.4286513 -4.4287314 -4.4287934 -4.428822 -4.4287896][-4.4288368 -4.4287963 -4.4287405 -4.4286642 -4.4285622 -4.4283714 -4.4280324 -4.4278111 -4.4280715 -4.4284096 -4.4286084 -4.4287329 -4.4288206 -4.428865 -4.4288478][-4.4288416 -4.4288034 -4.4287615 -4.4287119 -4.4286165 -4.4284163 -4.4280972 -4.4279256 -4.4281611 -4.4284468 -4.4286246 -4.4287467 -4.4288359 -4.4288821 -4.4288726][-4.4288406 -4.4287987 -4.4287653 -4.4287438 -4.4286776 -4.4285388 -4.4283566 -4.4282784 -4.4284105 -4.4285746 -4.4286823 -4.4287581 -4.4288158 -4.4288473 -4.4288397][-4.4288511 -4.4288096 -4.42878 -4.4287615 -4.4287043 -4.4286056 -4.4285111 -4.428484 -4.428566 -4.4286551 -4.4287143 -4.428762 -4.4287891 -4.4287977 -4.4287887][-4.4288754 -4.4288421 -4.4288168 -4.4287982 -4.4287429 -4.4286604 -4.4286013 -4.428596 -4.428659 -4.4287047 -4.4287367 -4.4287691 -4.4287791 -4.4287691 -4.4287548][-4.4288955 -4.4288735 -4.4288545 -4.4288383 -4.428793 -4.4287333 -4.4286919 -4.4286952 -4.4287438 -4.4287658 -4.4287825 -4.4288015 -4.4287996 -4.4287829 -4.4287639][-4.4289064 -4.4288979 -4.4288907 -4.4288774 -4.4288478 -4.4288068 -4.4287672 -4.4287748 -4.4288187 -4.4288263 -4.4288354 -4.4288464 -4.4288335 -4.4288177 -4.4288015][-4.4289107 -4.4289126 -4.4289169 -4.4289093 -4.4288874 -4.4288564 -4.4288177 -4.4288297 -4.4288769 -4.4288845 -4.4288898 -4.4288993 -4.4288898 -4.4288821 -4.4288683][-4.4289165 -4.428926 -4.4289379 -4.4289346 -4.4289188 -4.4288979 -4.4288692 -4.4288759 -4.4289107 -4.4289207 -4.4289346 -4.4289527 -4.4289527 -4.4289536 -4.4289455]]...]
INFO - root - 2017-12-08 05:18:30.863048: step 6910, loss = 2.28, batch loss = 2.23 (33.9 examples/sec; 0.236 sec/batch; 21h:20m:50s remains)
INFO - root - 2017-12-08 05:18:33.091666: step 6920, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 20h:10m:03s remains)
INFO - root - 2017-12-08 05:18:35.355298: step 6930, loss = 2.28, batch loss = 2.23 (37.0 examples/sec; 0.216 sec/batch; 19h:31m:42s remains)
INFO - root - 2017-12-08 05:18:37.602625: step 6940, loss = 2.28, batch loss = 2.23 (34.6 examples/sec; 0.231 sec/batch; 20h:53m:59s remains)
INFO - root - 2017-12-08 05:18:39.828649: step 6950, loss = 2.28, batch loss = 2.23 (34.3 examples/sec; 0.233 sec/batch; 21h:04m:32s remains)
INFO - root - 2017-12-08 05:18:42.055359: step 6960, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.220 sec/batch; 19h:54m:58s remains)
INFO - root - 2017-12-08 05:18:44.285192: step 6970, loss = 2.28, batch loss = 2.23 (35.4 examples/sec; 0.226 sec/batch; 20h:25m:27s remains)
INFO - root - 2017-12-08 05:18:46.513846: step 6980, loss = 2.28, batch loss = 2.23 (35.8 examples/sec; 0.224 sec/batch; 20h:12m:43s remains)
INFO - root - 2017-12-08 05:18:48.756179: step 6990, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.220 sec/batch; 19h:55m:05s remains)
INFO - root - 2017-12-08 05:18:51.041734: step 7000, loss = 2.28, batch loss = 2.23 (35.7 examples/sec; 0.224 sec/batch; 20h:15m:51s remains)
2017-12-08 05:18:51.324128: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4286666 -4.4286575 -4.4286141 -4.4285707 -4.42854 -4.4285536 -4.428596 -4.4286261 -4.4286165 -4.4285803 -4.4285583 -4.428566 -4.4285645 -4.4285588 -4.4285769][-4.428637 -4.4286327 -4.4286008 -4.4285769 -4.4285617 -4.428576 -4.4286051 -4.4286227 -4.4286118 -4.4285846 -4.4285645 -4.4285665 -4.4285603 -4.4285502 -4.4285641][-4.4286022 -4.428596 -4.4285722 -4.4285541 -4.4285407 -4.42856 -4.4285831 -4.4285903 -4.4285831 -4.4285841 -4.4285774 -4.4285679 -4.4285541 -4.4285398 -4.4285426][-4.4285884 -4.4285889 -4.4285789 -4.4285579 -4.4285316 -4.428534 -4.4285407 -4.4285378 -4.4285455 -4.4285774 -4.4285879 -4.4285684 -4.428545 -4.4285307 -4.4285316][-4.4285922 -4.4286046 -4.4286 -4.4285731 -4.4285431 -4.428524 -4.4284949 -4.4284759 -4.4285078 -4.4285769 -4.4286094 -4.4285884 -4.4285674 -4.4285693 -4.4285727][-4.4285855 -4.4286113 -4.4286141 -4.4285884 -4.4285622 -4.4285293 -4.4284539 -4.4284086 -4.4284692 -4.4285808 -4.4286375 -4.428627 -4.4286242 -4.4286361 -4.4286394][-4.4285455 -4.4285765 -4.42859 -4.428576 -4.4285579 -4.4284983 -4.4283643 -4.4282928 -4.4284024 -4.4285669 -4.4286494 -4.4286571 -4.4286656 -4.4286733 -4.4286656][-4.4284692 -4.4284968 -4.4285245 -4.4285226 -4.4285026 -4.428411 -4.4282265 -4.4281588 -4.4283485 -4.4285555 -4.4286489 -4.4286752 -4.4286904 -4.4286785 -4.4286628][-4.4284139 -4.4284444 -4.4284863 -4.4284925 -4.4284697 -4.4283857 -4.4282341 -4.4282265 -4.4284172 -4.4285817 -4.4286442 -4.4286604 -4.4286623 -4.4286461 -4.4286408][-4.4284191 -4.428463 -4.4285059 -4.4285126 -4.4284997 -4.4284596 -4.42839 -4.4284177 -4.4285383 -4.4286251 -4.4286437 -4.4286389 -4.4286308 -4.4286156 -4.4286175][-4.4284773 -4.4285316 -4.4285631 -4.4285641 -4.428565 -4.4285483 -4.42852 -4.4285412 -4.428607 -4.4286427 -4.4286418 -4.4286318 -4.4286194 -4.4285927 -4.4285893][-4.428556 -4.4286184 -4.4286385 -4.42863 -4.428627 -4.4286089 -4.4285812 -4.4285884 -4.428618 -4.4286323 -4.4286404 -4.4286432 -4.4286323 -4.4286027 -4.4285951][-4.4286251 -4.4286728 -4.4286766 -4.4286685 -4.4286723 -4.4286485 -4.4286094 -4.42861 -4.42863 -4.4286427 -4.4286566 -4.4286671 -4.4286509 -4.4286246 -4.4286294][-4.4286404 -4.4286752 -4.4286642 -4.4286604 -4.428678 -4.4286628 -4.4286218 -4.4286237 -4.4286442 -4.4286494 -4.4286532 -4.42865 -4.4286222 -4.4286 -4.4286294][-4.42863 -4.4286551 -4.4286437 -4.4286489 -4.4286728 -4.4286609 -4.4286203 -4.4286122 -4.4286208 -4.4286189 -4.4286127 -4.4286013 -4.4285712 -4.4285545 -4.4285941]]...]
INFO - root - 2017-12-08 05:18:53.578143: step 7010, loss = 2.28, batch loss = 2.23 (35.4 examples/sec; 0.226 sec/batch; 20h:24m:18s remains)
INFO - root - 2017-12-08 05:18:55.788112: step 7020, loss = 2.28, batch loss = 2.23 (35.3 examples/sec; 0.227 sec/batch; 20h:31m:07s remains)
INFO - root - 2017-12-08 05:18:58.012819: step 7030, loss = 2.28, batch loss = 2.23 (35.6 examples/sec; 0.225 sec/batch; 20h:18m:49s remains)
INFO - root - 2017-12-08 05:19:00.255911: step 7040, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.225 sec/batch; 20h:22m:16s remains)
INFO - root - 2017-12-08 05:19:02.509373: step 7050, loss = 2.28, batch loss = 2.23 (34.8 examples/sec; 0.230 sec/batch; 20h:47m:22s remains)
INFO - root - 2017-12-08 05:19:04.749119: step 7060, loss = 2.28, batch loss = 2.23 (35.6 examples/sec; 0.225 sec/batch; 20h:18m:27s remains)
INFO - root - 2017-12-08 05:19:06.961219: step 7070, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.221 sec/batch; 19h:56m:48s remains)
INFO - root - 2017-12-08 05:19:09.212797: step 7080, loss = 2.28, batch loss = 2.23 (34.3 examples/sec; 0.234 sec/batch; 21h:06m:49s remains)
INFO - root - 2017-12-08 05:19:11.428446: step 7090, loss = 2.28, batch loss = 2.23 (36.6 examples/sec; 0.218 sec/batch; 19h:44m:31s remains)
INFO - root - 2017-12-08 05:19:13.650026: step 7100, loss = 2.28, batch loss = 2.23 (36.6 examples/sec; 0.219 sec/batch; 19h:46m:34s remains)
2017-12-08 05:19:13.932502: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288626 -4.4288764 -4.4288845 -4.4288883 -4.4288816 -4.42886 -4.4288254 -4.4287939 -4.42878 -4.428792 -4.4288163 -4.4288459 -4.4288611 -4.4288507 -4.4288125][-4.4289041 -4.4289136 -4.4289188 -4.4289155 -4.4288945 -4.4288483 -4.4287891 -4.428741 -4.4287214 -4.4287457 -4.428792 -4.4288378 -4.4288626 -4.4288578 -4.428823][-4.4289136 -4.4289217 -4.4289284 -4.4289203 -4.42888 -4.4288068 -4.4287219 -4.428659 -4.4286408 -4.4286857 -4.4287572 -4.428823 -4.4288588 -4.4288626 -4.4288325][-4.4289074 -4.4289107 -4.4289169 -4.4289017 -4.4288445 -4.4287505 -4.428648 -4.4285712 -4.4285603 -4.428628 -4.4287229 -4.4288077 -4.4288568 -4.4288716 -4.4288511][-4.4289155 -4.4289169 -4.4289212 -4.4288945 -4.4288139 -4.4286957 -4.4285688 -4.4284763 -4.4284716 -4.4285583 -4.4286733 -4.4287758 -4.4288354 -4.4288592 -4.4288468][-4.4289432 -4.4289441 -4.4289408 -4.4288955 -4.4287882 -4.42864 -4.428493 -4.4284024 -4.4284067 -4.4284997 -4.4286208 -4.42873 -4.4287963 -4.4288282 -4.4288282][-4.4289465 -4.4289455 -4.4289246 -4.4288573 -4.4287243 -4.4285555 -4.4284053 -4.4283319 -4.4283543 -4.4284506 -4.428566 -4.4286723 -4.42875 -4.4287992 -4.4288235][-4.4289289 -4.4289217 -4.4288812 -4.4287896 -4.4286366 -4.4284573 -4.4283133 -4.4282656 -4.42831 -4.428412 -4.4285169 -4.4286122 -4.4286933 -4.428762 -4.4288092][-4.4289107 -4.4289 -4.4288421 -4.4287357 -4.4285808 -4.4284115 -4.4282851 -4.4282551 -4.4283137 -4.4284215 -4.4285183 -4.4286013 -4.4286828 -4.4287553 -4.4288054][-4.4288878 -4.4288888 -4.4288259 -4.4287143 -4.4285736 -4.4284472 -4.4283586 -4.4283357 -4.4283853 -4.4284849 -4.4285755 -4.4286532 -4.428731 -4.4287887 -4.4288106][-4.4288273 -4.4288459 -4.4287939 -4.4286985 -4.4285927 -4.4285135 -4.4284663 -4.4284515 -4.4284816 -4.4285526 -4.4286237 -4.4286957 -4.4287567 -4.4287877 -4.428772][-4.4287786 -4.4288058 -4.4287705 -4.4287004 -4.4286366 -4.4285955 -4.428575 -4.4285755 -4.4285989 -4.4286413 -4.4286885 -4.4287391 -4.42877 -4.4287672 -4.4287171][-4.4287615 -4.4287863 -4.4287653 -4.428721 -4.4286933 -4.4286785 -4.4286723 -4.4286814 -4.4286985 -4.428719 -4.4287462 -4.4287748 -4.4287786 -4.4287496 -4.4286747][-4.4287939 -4.4288158 -4.4288077 -4.4287863 -4.4287753 -4.4287729 -4.4287753 -4.4287839 -4.4287934 -4.4287987 -4.4288087 -4.4288187 -4.4288063 -4.4287653 -4.4286885][-4.4288507 -4.4288669 -4.4288673 -4.428863 -4.4288597 -4.4288607 -4.4288659 -4.4288731 -4.4288788 -4.4288788 -4.4288778 -4.428875 -4.4288568 -4.428823 -4.4287639]]...]
INFO - root - 2017-12-08 05:19:16.170314: step 7110, loss = 2.28, batch loss = 2.23 (37.5 examples/sec; 0.213 sec/batch; 19h:16m:55s remains)
INFO - root - 2017-12-08 05:19:18.397038: step 7120, loss = 2.28, batch loss = 2.23 (36.9 examples/sec; 0.217 sec/batch; 19h:35m:07s remains)
INFO - root - 2017-12-08 05:19:20.621185: step 7130, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 19h:50m:57s remains)
INFO - root - 2017-12-08 05:19:22.869429: step 7140, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.226 sec/batch; 20h:23m:13s remains)
INFO - root - 2017-12-08 05:19:25.138320: step 7150, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 20h:07m:41s remains)
INFO - root - 2017-12-08 05:19:27.361137: step 7160, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 19h:58m:44s remains)
INFO - root - 2017-12-08 05:19:29.594015: step 7170, loss = 2.28, batch loss = 2.23 (35.8 examples/sec; 0.223 sec/batch; 20h:10m:42s remains)
INFO - root - 2017-12-08 05:19:31.813880: step 7180, loss = 2.28, batch loss = 2.23 (37.5 examples/sec; 0.213 sec/batch; 19h:16m:54s remains)
INFO - root - 2017-12-08 05:19:34.066728: step 7190, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 20h:07m:23s remains)
INFO - root - 2017-12-08 05:19:36.295937: step 7200, loss = 2.28, batch loss = 2.23 (35.4 examples/sec; 0.226 sec/batch; 20h:24m:04s remains)
2017-12-08 05:19:36.578223: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289627 -4.4289522 -4.4289374 -4.428926 -4.428916 -4.4289069 -4.4289026 -4.4289002 -4.4289055 -4.4289184 -4.4289255 -4.4289341 -4.4289479 -4.428956 -4.4289656][-4.4289393 -4.4289193 -4.4288945 -4.4288731 -4.4288588 -4.4288468 -4.4288387 -4.4288349 -4.4288435 -4.428864 -4.4288712 -4.4288769 -4.4289007 -4.428916 -4.4289303][-4.4288931 -4.4288535 -4.4288135 -4.4287806 -4.428761 -4.4287443 -4.4287319 -4.4287238 -4.42874 -4.4287696 -4.4287853 -4.4287968 -4.4288316 -4.42886 -4.4288869][-4.4288306 -4.4287696 -4.4287157 -4.4286747 -4.42865 -4.428628 -4.4286113 -4.4286094 -4.4286361 -4.428668 -4.4286914 -4.4287105 -4.4287534 -4.4287906 -4.4288292][-4.4287877 -4.4287109 -4.4286447 -4.4285927 -4.4285574 -4.4285188 -4.4284964 -4.4285145 -4.4285545 -4.4285803 -4.4286122 -4.4286413 -4.4286895 -4.4287338 -4.4287815][-4.4287543 -4.4286585 -4.4285731 -4.4285016 -4.4284415 -4.4283733 -4.4283457 -4.4283924 -4.4284492 -4.4284649 -4.4285026 -4.4285502 -4.4286141 -4.4286766 -4.4287391][-4.4287267 -4.4286079 -4.4284992 -4.4284105 -4.4283314 -4.4282393 -4.4282055 -4.4282703 -4.4283371 -4.4283552 -4.4284029 -4.4284639 -4.428545 -4.4286246 -4.4287009][-4.4286985 -4.4285727 -4.4284549 -4.4283652 -4.4282966 -4.4282122 -4.4281816 -4.4282303 -4.4282827 -4.4283056 -4.4283547 -4.4284134 -4.4285007 -4.4285855 -4.4286709][-4.4286847 -4.428565 -4.4284625 -4.4283895 -4.4283409 -4.4282856 -4.4282613 -4.428277 -4.4283042 -4.4283276 -4.4283743 -4.4284248 -4.4285054 -4.4285855 -4.4286742][-4.4286962 -4.428586 -4.4285064 -4.4284515 -4.4284105 -4.4283805 -4.4283729 -4.428369 -4.4283686 -4.4283876 -4.4284344 -4.428483 -4.4285526 -4.428627 -4.4287148][-4.4287357 -4.428638 -4.4285812 -4.4285445 -4.4285088 -4.4284959 -4.4285197 -4.4285278 -4.4285135 -4.428525 -4.4285693 -4.4286146 -4.4286656 -4.428721 -4.4287925][-4.4288254 -4.4287624 -4.428731 -4.4287114 -4.4286819 -4.4286761 -4.42872 -4.4287462 -4.4287276 -4.4287257 -4.4287596 -4.428802 -4.4288349 -4.4288635 -4.4289055][-4.4289141 -4.428895 -4.4288883 -4.4288783 -4.4288568 -4.4288535 -4.4288969 -4.4289279 -4.4289131 -4.4289007 -4.4289222 -4.4289589 -4.4289784 -4.4289842 -4.4289989][-4.4289637 -4.4289713 -4.4289775 -4.4289713 -4.4289603 -4.4289613 -4.4289913 -4.429009 -4.4289966 -4.4289823 -4.4289975 -4.4290247 -4.4290342 -4.4290309 -4.4290338][-4.4289775 -4.4289937 -4.4290037 -4.4290018 -4.4289937 -4.4289942 -4.4290104 -4.4290161 -4.4290056 -4.4289937 -4.4290032 -4.4290237 -4.4290333 -4.4290323 -4.4290304]]...]
INFO - root - 2017-12-08 05:19:38.824426: step 7210, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 19h:57m:07s remains)
INFO - root - 2017-12-08 05:19:41.065352: step 7220, loss = 2.28, batch loss = 2.23 (36.7 examples/sec; 0.218 sec/batch; 19h:43m:08s remains)
INFO - root - 2017-12-08 05:19:43.317788: step 7230, loss = 2.28, batch loss = 2.23 (35.1 examples/sec; 0.228 sec/batch; 20h:37m:16s remains)
INFO - root - 2017-12-08 05:19:45.580169: step 7240, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.225 sec/batch; 20h:21m:15s remains)
INFO - root - 2017-12-08 05:19:47.809975: step 7250, loss = 2.28, batch loss = 2.23 (37.1 examples/sec; 0.216 sec/batch; 19h:29m:04s remains)
INFO - root - 2017-12-08 05:19:50.045627: step 7260, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.220 sec/batch; 19h:55m:04s remains)
INFO - root - 2017-12-08 05:19:52.274346: step 7270, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 19h:59m:07s remains)
INFO - root - 2017-12-08 05:19:54.495340: step 7280, loss = 2.28, batch loss = 2.23 (36.6 examples/sec; 0.219 sec/batch; 19h:45m:18s remains)
INFO - root - 2017-12-08 05:19:56.725119: step 7290, loss = 2.28, batch loss = 2.23 (37.1 examples/sec; 0.216 sec/batch; 19h:30m:18s remains)
INFO - root - 2017-12-08 05:19:58.983054: step 7300, loss = 2.28, batch loss = 2.23 (35.3 examples/sec; 0.227 sec/batch; 20h:29m:31s remains)
2017-12-08 05:19:59.266474: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288936 -4.4288955 -4.4289131 -4.4289293 -4.4289312 -4.4289222 -4.4289193 -4.4289246 -4.4289289 -4.4289222 -4.4289007 -4.4288826 -4.4288821 -4.4288974 -4.4289222][-4.4287968 -4.4287896 -4.4288116 -4.4288344 -4.4288406 -4.4288349 -4.4288325 -4.4288349 -4.4288368 -4.4288287 -4.4288058 -4.4287915 -4.4287996 -4.4288244 -4.4288573][-4.4287086 -4.4286919 -4.4287162 -4.4287481 -4.4287629 -4.428762 -4.42876 -4.4287581 -4.4287539 -4.4287419 -4.4287257 -4.4287257 -4.4287496 -4.4287815 -4.4288144][-4.4286842 -4.4286637 -4.428688 -4.4287219 -4.4287333 -4.4287281 -4.4287262 -4.4287238 -4.4287243 -4.4287162 -4.4287167 -4.4287362 -4.4287634 -4.4287868 -4.4288073][-4.4286952 -4.4286804 -4.4286966 -4.4287124 -4.4287052 -4.42868 -4.4286637 -4.4286609 -4.4286895 -4.4287162 -4.4287415 -4.4287677 -4.4287791 -4.428782 -4.4287868][-4.4287162 -4.4287086 -4.428719 -4.4287176 -4.428688 -4.42862 -4.428546 -4.4285254 -4.4286065 -4.4286995 -4.4287548 -4.4287782 -4.4287767 -4.4287667 -4.4287605][-4.4287515 -4.4287486 -4.4287596 -4.4287467 -4.42869 -4.4285717 -4.4284077 -4.4283457 -4.4284964 -4.4286666 -4.4287477 -4.4287758 -4.428782 -4.4287739 -4.4287629][-4.4287753 -4.4287724 -4.4287834 -4.4287753 -4.4287171 -4.428575 -4.4283686 -4.4282727 -4.42845 -4.4286509 -4.4287429 -4.4287682 -4.4287853 -4.4287896 -4.4287777][-4.4287944 -4.4288025 -4.4288259 -4.4288321 -4.4287915 -4.4286857 -4.42854 -4.4284787 -4.4285808 -4.4287105 -4.42877 -4.4287791 -4.4287982 -4.4288054 -4.4287934][-4.4287925 -4.4288092 -4.4288421 -4.428863 -4.4288478 -4.4287953 -4.4287267 -4.4287 -4.4287481 -4.4287996 -4.4288077 -4.4287872 -4.4287877 -4.4287868 -4.4287786][-4.428782 -4.4287987 -4.428834 -4.4288583 -4.4288554 -4.428843 -4.4288297 -4.4288278 -4.4288478 -4.4288564 -4.4288468 -4.4288216 -4.4288054 -4.4287872 -4.4287667][-4.4287834 -4.4287934 -4.4288239 -4.42885 -4.4288535 -4.4288583 -4.4288683 -4.4288812 -4.428895 -4.4288979 -4.4288931 -4.428884 -4.4288716 -4.4288464 -4.4288087][-4.4288354 -4.4288435 -4.4288635 -4.4288735 -4.4288754 -4.4288816 -4.4288936 -4.4289083 -4.4289269 -4.4289379 -4.4289436 -4.428947 -4.4289389 -4.4289141 -4.4288697][-4.4288888 -4.4288979 -4.4289117 -4.4289093 -4.428906 -4.4289055 -4.4289136 -4.4289303 -4.4289546 -4.4289689 -4.4289751 -4.4289761 -4.4289684 -4.4289451 -4.4289036][-4.4289131 -4.4289212 -4.42893 -4.4289269 -4.4289274 -4.4289255 -4.4289284 -4.4289441 -4.4289694 -4.4289851 -4.4289918 -4.4289846 -4.4289756 -4.4289551 -4.4289207]]...]
INFO - root - 2017-12-08 05:20:01.518505: step 7310, loss = 2.28, batch loss = 2.23 (35.3 examples/sec; 0.226 sec/batch; 20h:26m:50s remains)
INFO - root - 2017-12-08 05:20:03.764944: step 7320, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.225 sec/batch; 20h:20m:07s remains)
INFO - root - 2017-12-08 05:20:05.981465: step 7330, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 20h:07m:19s remains)
INFO - root - 2017-12-08 05:20:08.254739: step 7340, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 19h:46m:16s remains)
INFO - root - 2017-12-08 05:20:10.464573: step 7350, loss = 2.28, batch loss = 2.23 (37.2 examples/sec; 0.215 sec/batch; 19h:25m:33s remains)
INFO - root - 2017-12-08 05:20:12.727538: step 7360, loss = 2.28, batch loss = 2.23 (35.1 examples/sec; 0.228 sec/batch; 20h:34m:33s remains)
INFO - root - 2017-12-08 05:20:14.977001: step 7370, loss = 2.28, batch loss = 2.23 (36.9 examples/sec; 0.217 sec/batch; 19h:35m:18s remains)
INFO - root - 2017-12-08 05:20:17.225087: step 7380, loss = 2.28, batch loss = 2.23 (35.2 examples/sec; 0.227 sec/batch; 20h:30m:08s remains)
INFO - root - 2017-12-08 05:20:19.455838: step 7390, loss = 2.28, batch loss = 2.23 (34.0 examples/sec; 0.235 sec/batch; 21h:14m:35s remains)
INFO - root - 2017-12-08 05:20:21.676371: step 7400, loss = 2.28, batch loss = 2.23 (36.8 examples/sec; 0.218 sec/batch; 19h:38m:45s remains)
2017-12-08 05:20:21.961885: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.42888 -4.4289246 -4.42894 -4.4289174 -4.4288712 -4.428793 -4.4287095 -4.4286337 -4.428606 -4.4286556 -4.4287267 -4.428782 -4.428812 -4.428833 -4.428843][-4.4289289 -4.4289522 -4.4289408 -4.428906 -4.4288507 -4.428772 -4.4286838 -4.4286084 -4.4285989 -4.4286532 -4.4287157 -4.4287648 -4.4288034 -4.4288325 -4.428853][-4.4289408 -4.4289551 -4.4289351 -4.4289041 -4.4288607 -4.4287987 -4.4287148 -4.4286389 -4.42863 -4.4286809 -4.4287286 -4.4287663 -4.4288096 -4.4288425 -4.4288669][-4.428926 -4.4289322 -4.4289155 -4.4289017 -4.4288864 -4.4288516 -4.4287782 -4.4286938 -4.4286661 -4.4287047 -4.4287448 -4.4287806 -4.4288287 -4.4288664 -4.428895][-4.4289351 -4.4289312 -4.4289136 -4.4289 -4.4288864 -4.4288588 -4.4287748 -4.4286776 -4.4286394 -4.4286838 -4.4287343 -4.4287882 -4.4288445 -4.4288883 -4.42892][-4.428968 -4.428956 -4.4289312 -4.4288917 -4.4288392 -4.4287672 -4.4286418 -4.4285235 -4.4285173 -4.4286075 -4.4286923 -4.428771 -4.4288406 -4.4288979 -4.4289331][-4.4289913 -4.42898 -4.428947 -4.4288774 -4.4287596 -4.4286036 -4.4283934 -4.4282484 -4.4283252 -4.4285045 -4.4286366 -4.4287419 -4.428834 -4.4289041 -4.4289417][-4.4289889 -4.4289827 -4.4289479 -4.428854 -4.4286833 -4.4284406 -4.4281321 -4.4279513 -4.4281235 -4.42839 -4.4285717 -4.4287109 -4.4288239 -4.4289069 -4.4289503][-4.4289722 -4.428968 -4.4289355 -4.4288378 -4.4286523 -4.4283977 -4.4280834 -4.4278984 -4.4280653 -4.4283328 -4.4285321 -4.4286952 -4.42882 -4.4289064 -4.4289532][-4.4289594 -4.4289532 -4.428926 -4.4288459 -4.428699 -4.4285135 -4.4282951 -4.428154 -4.428225 -4.4283938 -4.4285564 -4.4287124 -4.4288325 -4.4289136 -4.4289579][-4.4289584 -4.4289508 -4.4289255 -4.4288669 -4.4287677 -4.4286532 -4.4285274 -4.4284315 -4.4284415 -4.4285269 -4.4286404 -4.4287715 -4.4288745 -4.4289384 -4.4289742][-4.428967 -4.4289594 -4.4289336 -4.4288898 -4.4288259 -4.4287572 -4.4286823 -4.4286189 -4.4286165 -4.4286647 -4.4287472 -4.4288507 -4.4289265 -4.42897 -4.4289894][-4.4289765 -4.4289713 -4.4289465 -4.4289093 -4.4288688 -4.4288268 -4.4287777 -4.4287381 -4.4287357 -4.428772 -4.4288416 -4.4289193 -4.428968 -4.4289908 -4.4289951][-4.4289718 -4.4289746 -4.4289613 -4.4289346 -4.4289045 -4.4288707 -4.428833 -4.4288077 -4.4288068 -4.4288383 -4.4288979 -4.4289551 -4.4289827 -4.4289913 -4.4289875][-4.4289527 -4.428966 -4.4289656 -4.4289513 -4.4289293 -4.4288993 -4.4288664 -4.4288459 -4.4288473 -4.4288745 -4.42892 -4.4289589 -4.4289746 -4.4289765 -4.4289708]]...]
INFO - root - 2017-12-08 05:20:24.199637: step 7410, loss = 2.28, batch loss = 2.23 (34.5 examples/sec; 0.232 sec/batch; 20h:55m:07s remains)
INFO - root - 2017-12-08 05:20:26.442574: step 7420, loss = 2.28, batch loss = 2.23 (36.0 examples/sec; 0.222 sec/batch; 20h:02m:20s remains)
INFO - root - 2017-12-08 05:20:28.682222: step 7430, loss = 2.28, batch loss = 2.23 (35.2 examples/sec; 0.227 sec/batch; 20h:32m:27s remains)
INFO - root - 2017-12-08 05:20:30.922226: step 7440, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 20h:08m:32s remains)
INFO - root - 2017-12-08 05:20:33.153254: step 7450, loss = 2.28, batch loss = 2.23 (37.3 examples/sec; 0.215 sec/batch; 19h:22m:50s remains)
INFO - root - 2017-12-08 05:20:35.405012: step 7460, loss = 2.28, batch loss = 2.23 (34.7 examples/sec; 0.231 sec/batch; 20h:50m:20s remains)
INFO - root - 2017-12-08 05:20:37.642337: step 7470, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.222 sec/batch; 19h:59m:56s remains)
INFO - root - 2017-12-08 05:20:39.907739: step 7480, loss = 2.28, batch loss = 2.23 (34.8 examples/sec; 0.230 sec/batch; 20h:45m:22s remains)
INFO - root - 2017-12-08 05:20:42.135607: step 7490, loss = 2.28, batch loss = 2.23 (36.9 examples/sec; 0.217 sec/batch; 19h:35m:00s remains)
INFO - root - 2017-12-08 05:20:44.358518: step 7500, loss = 2.28, batch loss = 2.23 (36.9 examples/sec; 0.217 sec/batch; 19h:34m:37s remains)
2017-12-08 05:20:44.662778: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289937 -4.42899 -4.4289894 -4.4289913 -4.4289775 -4.4289474 -4.4289203 -4.4289217 -4.4289341 -4.4289379 -4.428936 -4.4289327 -4.4289222 -4.4289002 -4.42889][-4.4289994 -4.4289918 -4.4289856 -4.4289746 -4.428937 -4.4288831 -4.4288363 -4.4288435 -4.4288731 -4.4288926 -4.4288964 -4.4288926 -4.4288759 -4.4288468 -4.4288268][-4.4289908 -4.4289842 -4.428977 -4.428947 -4.4288754 -4.4287848 -4.4287181 -4.4287372 -4.4288 -4.4288549 -4.428874 -4.4288712 -4.428843 -4.4287939 -4.4287553][-4.4289846 -4.4289818 -4.4289732 -4.4289193 -4.4288111 -4.4286771 -4.4285827 -4.4286232 -4.4287372 -4.4288263 -4.4288626 -4.4288669 -4.4288297 -4.4287567 -4.4286981][-4.4289804 -4.4289823 -4.4289656 -4.4288931 -4.428751 -4.4285688 -4.4284344 -4.4284825 -4.4286466 -4.4287782 -4.428834 -4.4288435 -4.4287953 -4.4287062 -4.4286385][-4.428966 -4.4289675 -4.4289365 -4.4288421 -4.4286685 -4.4284396 -4.428257 -4.4283028 -4.4285154 -4.4287009 -4.4287972 -4.4288225 -4.428772 -4.4286709 -4.4286118][-4.4289455 -4.4289346 -4.4288878 -4.428782 -4.4285951 -4.4283395 -4.4280972 -4.42812 -4.4283929 -4.4286394 -4.4287653 -4.4288015 -4.428751 -4.4286556 -4.4286256][-4.4289141 -4.4288831 -4.4288163 -4.4287 -4.4285355 -4.4283028 -4.4280305 -4.427999 -4.428297 -4.428575 -4.4287105 -4.4287457 -4.4286914 -4.4286137 -4.4286237][-4.4288621 -4.4288187 -4.428751 -4.4286604 -4.428565 -4.4284286 -4.4282484 -4.4281983 -4.428391 -4.4285927 -4.4286876 -4.4286995 -4.4286351 -4.4285712 -4.4286184][-4.428812 -4.4287715 -4.4287171 -4.4286695 -4.4286447 -4.4285936 -4.4284978 -4.4284568 -4.4285541 -4.4286652 -4.4287186 -4.4287195 -4.4286504 -4.4285851 -4.4286337][-4.4287729 -4.428741 -4.4287019 -4.4286823 -4.4286942 -4.4286871 -4.4286432 -4.4286132 -4.4286613 -4.4287214 -4.4287481 -4.4287424 -4.4286776 -4.4286137 -4.4286518][-4.42874 -4.428719 -4.4287 -4.4287014 -4.4287276 -4.4287486 -4.4287415 -4.4287219 -4.4287453 -4.4287786 -4.4287925 -4.4287896 -4.42874 -4.4286933 -4.4287224][-4.4287348 -4.428721 -4.4287114 -4.428721 -4.4287543 -4.42879 -4.4288039 -4.4287972 -4.4288082 -4.4288278 -4.4288392 -4.4288435 -4.4288182 -4.428793 -4.42881][-4.4287767 -4.4287748 -4.4287739 -4.4287853 -4.4288163 -4.4288511 -4.4288707 -4.4288707 -4.4288745 -4.4288859 -4.4288988 -4.4289079 -4.4289026 -4.428894 -4.4289041][-4.42885 -4.4288549 -4.4288616 -4.4288692 -4.4288917 -4.4289227 -4.4289446 -4.4289527 -4.4289546 -4.4289575 -4.4289656 -4.4289703 -4.4289694 -4.4289689 -4.4289732]]...]
INFO - root - 2017-12-08 05:20:46.886630: step 7510, loss = 2.28, batch loss = 2.23 (37.0 examples/sec; 0.216 sec/batch; 19h:30m:15s remains)
INFO - root - 2017-12-08 05:20:49.118768: step 7520, loss = 2.28, batch loss = 2.23 (37.0 examples/sec; 0.216 sec/batch; 19h:29m:36s remains)
INFO - root - 2017-12-08 05:20:51.328490: step 7530, loss = 2.28, batch loss = 2.23 (36.8 examples/sec; 0.217 sec/batch; 19h:36m:35s remains)
INFO - root - 2017-12-08 05:20:53.611958: step 7540, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.220 sec/batch; 19h:53m:50s remains)
INFO - root - 2017-12-08 05:20:55.841724: step 7550, loss = 2.28, batch loss = 2.23 (36.6 examples/sec; 0.219 sec/batch; 19h:45m:17s remains)
INFO - root - 2017-12-08 05:20:58.066813: step 7560, loss = 2.28, batch loss = 2.23 (35.4 examples/sec; 0.226 sec/batch; 20h:24m:23s remains)
INFO - root - 2017-12-08 05:21:00.305062: step 7570, loss = 2.28, batch loss = 2.23 (35.8 examples/sec; 0.224 sec/batch; 20h:11m:21s remains)
INFO - root - 2017-12-08 05:21:02.543893: step 7580, loss = 2.28, batch loss = 2.23 (33.3 examples/sec; 0.240 sec/batch; 21h:41m:44s remains)
INFO - root - 2017-12-08 05:21:04.782599: step 7590, loss = 2.28, batch loss = 2.23 (36.9 examples/sec; 0.217 sec/batch; 19h:35m:19s remains)
INFO - root - 2017-12-08 05:21:07.011352: step 7600, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.222 sec/batch; 20h:01m:13s remains)
2017-12-08 05:21:07.317600: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4287324 -4.4287686 -4.4287782 -4.4287696 -4.4287753 -4.428802 -4.4287963 -4.4287605 -4.4287596 -4.4287968 -4.4288125 -4.4288311 -4.4288554 -4.4288583 -4.4288449][-4.4287305 -4.4287896 -4.42881 -4.4288044 -4.4288054 -4.4288263 -4.4288111 -4.4287658 -4.4287634 -4.4287944 -4.4288011 -4.4288154 -4.4288459 -4.4288521 -4.4288349][-4.4287724 -4.4288392 -4.4288597 -4.428844 -4.428822 -4.4288173 -4.4287834 -4.4287295 -4.4287324 -4.4287686 -4.42878 -4.4287972 -4.428833 -4.4288487 -4.4288335][-4.4288268 -4.4288907 -4.4289012 -4.4288645 -4.4288177 -4.42878 -4.4287248 -4.4286675 -4.42868 -4.4287233 -4.4287448 -4.4287724 -4.4288206 -4.4288483 -4.4288416][-4.4288287 -4.4288878 -4.4288983 -4.428854 -4.4287882 -4.4287133 -4.4286222 -4.4285564 -4.4285817 -4.4286361 -4.4286737 -4.4287162 -4.4287829 -4.4288321 -4.428844][-4.428793 -4.4288349 -4.4288373 -4.4287777 -4.4286847 -4.4285636 -4.4284263 -4.4283566 -4.4284143 -4.4285026 -4.4285679 -4.4286289 -4.4287138 -4.4287934 -4.4288325][-4.4287353 -4.4287562 -4.4287362 -4.4286556 -4.4285383 -4.4283786 -4.4282103 -4.4281569 -4.4282632 -4.4283948 -4.4284892 -4.4285636 -4.4286613 -4.4287591 -4.4288182][-4.4286838 -4.4286795 -4.4286385 -4.4285474 -4.4284248 -4.4282665 -4.4281268 -4.4281187 -4.4282532 -4.428401 -4.4285016 -4.4285717 -4.428658 -4.4287539 -4.4288177][-4.4286728 -4.4286513 -4.4286003 -4.4285121 -4.4284105 -4.4283094 -4.428247 -4.42827 -4.4283876 -4.4285107 -4.42859 -4.4286418 -4.4287043 -4.428782 -4.4288387][-4.4287066 -4.4286809 -4.4286304 -4.4285536 -4.4284797 -4.4284382 -4.4284272 -4.428452 -4.4285321 -4.4286156 -4.4286728 -4.4287119 -4.4287605 -4.4288206 -4.4288664][-4.4287543 -4.4287286 -4.4286852 -4.4286256 -4.4285784 -4.42857 -4.4285793 -4.4286 -4.4286523 -4.4287043 -4.4287438 -4.4287753 -4.4288125 -4.4288564 -4.4288931][-4.4288163 -4.4287915 -4.4287591 -4.4287238 -4.4287024 -4.4287047 -4.4287133 -4.4287262 -4.4287553 -4.4287863 -4.4288144 -4.4288387 -4.4288683 -4.4288988 -4.4289241][-4.4288964 -4.4288764 -4.428854 -4.4288354 -4.4288263 -4.4288287 -4.4288325 -4.4288397 -4.4288535 -4.4288716 -4.4288912 -4.4289112 -4.4289322 -4.4289479 -4.4289584][-4.4289594 -4.4289465 -4.4289341 -4.4289246 -4.4289188 -4.4289184 -4.4289203 -4.4289236 -4.42893 -4.42894 -4.4289513 -4.4289637 -4.4289751 -4.4289789 -4.4289794][-4.4289756 -4.4289703 -4.4289675 -4.4289656 -4.4289637 -4.4289622 -4.4289637 -4.4289656 -4.4289694 -4.4289742 -4.4289804 -4.428987 -4.4289913 -4.42899 -4.428987]]...]
INFO - root - 2017-12-08 05:21:09.582944: step 7610, loss = 2.28, batch loss = 2.23 (34.8 examples/sec; 0.230 sec/batch; 20h:45m:24s remains)
INFO - root - 2017-12-08 05:21:11.784205: step 7620, loss = 2.28, batch loss = 2.23 (36.8 examples/sec; 0.217 sec/batch; 19h:37m:10s remains)
INFO - root - 2017-12-08 05:21:14.017224: step 7630, loss = 2.28, batch loss = 2.23 (37.2 examples/sec; 0.215 sec/batch; 19h:25m:09s remains)
INFO - root - 2017-12-08 05:21:16.303035: step 7640, loss = 2.28, batch loss = 2.23 (35.2 examples/sec; 0.227 sec/batch; 20h:28m:47s remains)
INFO - root - 2017-12-08 05:21:18.512228: step 7650, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.225 sec/batch; 20h:19m:31s remains)
INFO - root - 2017-12-08 05:21:20.743682: step 7660, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 19h:50m:10s remains)
INFO - root - 2017-12-08 05:21:23.005714: step 7670, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 19h:49m:46s remains)
INFO - root - 2017-12-08 05:21:25.265418: step 7680, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.222 sec/batch; 20h:00m:56s remains)
INFO - root - 2017-12-08 05:21:27.495872: step 7690, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 20h:07m:32s remains)
INFO - root - 2017-12-08 05:21:29.750890: step 7700, loss = 2.28, batch loss = 2.23 (34.4 examples/sec; 0.232 sec/batch; 20h:58m:12s remains)
2017-12-08 05:21:30.011123: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4286942 -4.4287009 -4.4287143 -4.4287248 -4.4287319 -4.4287367 -4.428731 -4.4287252 -4.42872 -4.4287214 -4.4287314 -4.4287653 -4.4287972 -4.4288025 -4.4287958][-4.4287319 -4.4287167 -4.4287105 -4.4287128 -4.4287181 -4.4287229 -4.4287238 -4.4287357 -4.4287457 -4.4287496 -4.4287553 -4.4287815 -4.4288135 -4.42882 -4.4288139][-4.4287815 -4.4287562 -4.42874 -4.4287305 -4.4287205 -4.4287052 -4.4286995 -4.4287224 -4.4287486 -4.4287629 -4.4287667 -4.4287853 -4.428812 -4.428823 -4.4288263][-4.4288177 -4.4287958 -4.4287834 -4.4287653 -4.4287338 -4.4286814 -4.4286571 -4.4286833 -4.4287252 -4.4287539 -4.4287682 -4.4287806 -4.428792 -4.4288025 -4.4288182][-4.4288206 -4.4288197 -4.4288239 -4.4288 -4.4287333 -4.4286394 -4.4285851 -4.428606 -4.4286666 -4.4287267 -4.4287663 -4.4287782 -4.428771 -4.4287658 -4.4287763][-4.4287925 -4.4288034 -4.4288173 -4.428781 -4.4286804 -4.4285493 -4.42846 -4.4284759 -4.4285736 -4.4286823 -4.4287558 -4.4287763 -4.4287596 -4.4287348 -4.4287281][-4.4287548 -4.4287539 -4.4287605 -4.4287152 -4.4286 -4.4284482 -4.4283314 -4.4283438 -4.4284873 -4.4286423 -4.4287453 -4.428782 -4.4287677 -4.4287343 -4.4287119][-4.4287467 -4.4287243 -4.4287152 -4.4286685 -4.4285645 -4.4284244 -4.4282947 -4.4282918 -4.4284525 -4.4286261 -4.4287443 -4.4287987 -4.4288006 -4.4287715 -4.4287419][-4.4287753 -4.4287333 -4.4287081 -4.4286709 -4.4286075 -4.4285169 -4.428421 -4.4284034 -4.428514 -4.4286566 -4.4287672 -4.4288297 -4.4288473 -4.4288316 -4.428802][-4.4287977 -4.4287496 -4.4287133 -4.4286938 -4.4286852 -4.4286613 -4.4286232 -4.4286094 -4.4286575 -4.4287343 -4.4288106 -4.4288669 -4.4288883 -4.4288783 -4.4288468][-4.428792 -4.4287534 -4.428719 -4.4287138 -4.4287467 -4.4287715 -4.4287758 -4.4287767 -4.4287896 -4.4288063 -4.4288368 -4.428875 -4.428896 -4.4288893 -4.4288568][-4.4287887 -4.4287705 -4.4287481 -4.4287434 -4.4287853 -4.4288287 -4.428854 -4.4288635 -4.4288588 -4.42884 -4.4288325 -4.4288464 -4.4288607 -4.4288549 -4.4288244][-4.4287915 -4.428791 -4.4287858 -4.428782 -4.4288154 -4.4288568 -4.4288821 -4.428885 -4.4288626 -4.42883 -4.4288054 -4.4287958 -4.4287982 -4.4287906 -4.4287672][-4.4287844 -4.4287953 -4.4288077 -4.428813 -4.4288359 -4.4288659 -4.4288826 -4.4288759 -4.4288368 -4.4287925 -4.4287677 -4.4287615 -4.4287672 -4.4287677 -4.4287562][-4.4287915 -4.428803 -4.4288187 -4.4288239 -4.42884 -4.4288588 -4.428863 -4.4288516 -4.4288092 -4.4287634 -4.4287477 -4.4287562 -4.4287834 -4.4287968 -4.428792]]...]
INFO - root - 2017-12-08 05:21:32.240304: step 7710, loss = 2.28, batch loss = 2.23 (35.6 examples/sec; 0.225 sec/batch; 20h:17m:15s remains)
INFO - root - 2017-12-08 05:21:34.460241: step 7720, loss = 2.28, batch loss = 2.23 (35.3 examples/sec; 0.227 sec/batch; 20h:27m:09s remains)
INFO - root - 2017-12-08 05:21:36.710473: step 7730, loss = 2.28, batch loss = 2.23 (34.4 examples/sec; 0.233 sec/batch; 21h:00m:01s remains)
INFO - root - 2017-12-08 05:21:38.962350: step 7740, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.221 sec/batch; 19h:58m:16s remains)
INFO - root - 2017-12-08 05:21:41.243411: step 7750, loss = 2.28, batch loss = 2.23 (35.8 examples/sec; 0.223 sec/batch; 20h:08m:29s remains)
INFO - root - 2017-12-08 05:21:43.478992: step 7760, loss = 2.28, batch loss = 2.23 (36.0 examples/sec; 0.222 sec/batch; 20h:01m:56s remains)
INFO - root - 2017-12-08 05:21:45.705304: step 7770, loss = 2.28, batch loss = 2.23 (35.6 examples/sec; 0.225 sec/batch; 20h:17m:43s remains)
INFO - root - 2017-12-08 05:21:47.940887: step 7780, loss = 2.28, batch loss = 2.23 (35.8 examples/sec; 0.223 sec/batch; 20h:09m:13s remains)
INFO - root - 2017-12-08 05:21:50.208602: step 7790, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 19h:55m:04s remains)
INFO - root - 2017-12-08 05:21:52.458700: step 7800, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 19h:55m:30s remains)
2017-12-08 05:21:52.741540: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4287467 -4.4287596 -4.4287734 -4.4287877 -4.4287992 -4.42881 -4.4288235 -4.4288392 -4.4288588 -4.4288778 -4.4288931 -4.4289017 -4.4289045 -4.4289007 -4.4288921][-4.4287519 -4.4287591 -4.4287629 -4.4287658 -4.4287677 -4.4287715 -4.42878 -4.4287939 -4.4288168 -4.428844 -4.4288697 -4.4288907 -4.4289045 -4.4289074 -4.4289][-4.4287643 -4.4287648 -4.4287548 -4.428741 -4.42873 -4.4287205 -4.42872 -4.4287329 -4.4287605 -4.4287953 -4.42883 -4.428864 -4.4288888 -4.4288955 -4.4288864][-4.4287882 -4.4287839 -4.428761 -4.4287319 -4.4287062 -4.4286804 -4.4286685 -4.4286804 -4.4287143 -4.4287539 -4.428793 -4.4288363 -4.42887 -4.4288788 -4.4288669][-4.428812 -4.4288011 -4.4287663 -4.4287229 -4.4286838 -4.4286423 -4.4286141 -4.4286246 -4.4286714 -4.42872 -4.4287639 -4.4288149 -4.4288564 -4.4288664 -4.4288516][-4.4288211 -4.4287992 -4.4287505 -4.4286957 -4.4286456 -4.4285884 -4.4285359 -4.4285421 -4.4286108 -4.428679 -4.4287324 -4.428791 -4.4288416 -4.4288564 -4.4288425][-4.42881 -4.4287729 -4.4287086 -4.4286418 -4.4285822 -4.4285073 -4.4284244 -4.4284229 -4.4285216 -4.4286237 -4.4286957 -4.4287648 -4.4288249 -4.4288487 -4.4288411][-4.4287724 -4.4287205 -4.4286475 -4.428576 -4.4285135 -4.42843 -4.42833 -4.4283271 -4.4284515 -4.4285822 -4.4286695 -4.4287486 -4.4288149 -4.4288459 -4.4288454][-4.4287338 -4.4286828 -4.4286208 -4.428565 -4.4285183 -4.4284549 -4.4283772 -4.4283767 -4.4284883 -4.4286132 -4.4286976 -4.4287724 -4.4288335 -4.4288621 -4.4288621][-4.4287276 -4.4286895 -4.4286489 -4.4286175 -4.4285932 -4.4285574 -4.4285088 -4.4285059 -4.4285851 -4.4286847 -4.4287548 -4.4288144 -4.4288626 -4.4288845 -4.4288821][-4.4287615 -4.4287391 -4.4287205 -4.4287105 -4.4287038 -4.4286847 -4.4286537 -4.4286475 -4.4286981 -4.4287672 -4.4288182 -4.4288588 -4.4288888 -4.4289017 -4.4288955][-4.4288063 -4.4287949 -4.4287896 -4.428792 -4.4287958 -4.4287882 -4.4287686 -4.4287634 -4.428792 -4.4288325 -4.4288621 -4.428884 -4.4289002 -4.4289045 -4.428896][-4.4288373 -4.428833 -4.428834 -4.4288416 -4.4288507 -4.4288507 -4.4288411 -4.4288383 -4.428853 -4.4288745 -4.4288893 -4.4288988 -4.4289036 -4.4289 -4.4288874][-4.4288507 -4.4288511 -4.4288573 -4.4288673 -4.4288778 -4.4288812 -4.4288774 -4.4288759 -4.4288816 -4.4288917 -4.4288993 -4.4289012 -4.4288983 -4.4288883 -4.4288735][-4.428853 -4.4288564 -4.428863 -4.4288712 -4.4288783 -4.4288812 -4.4288797 -4.4288783 -4.4288793 -4.4288831 -4.4288883 -4.4288893 -4.428884 -4.428874 -4.4288607]]...]
INFO - root - 2017-12-08 05:21:54.951663: step 7810, loss = 2.28, batch loss = 2.23 (34.6 examples/sec; 0.231 sec/batch; 20h:52m:25s remains)
INFO - root - 2017-12-08 05:21:57.176955: step 7820, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.222 sec/batch; 19h:59m:23s remains)
INFO - root - 2017-12-08 05:21:59.415964: step 7830, loss = 2.28, batch loss = 2.23 (35.0 examples/sec; 0.229 sec/batch; 20h:37m:16s remains)
INFO - root - 2017-12-08 05:22:01.643508: step 7840, loss = 2.28, batch loss = 2.23 (35.6 examples/sec; 0.225 sec/batch; 20h:15m:29s remains)
INFO - root - 2017-12-08 05:22:03.905630: step 7850, loss = 2.28, batch loss = 2.23 (34.9 examples/sec; 0.229 sec/batch; 20h:39m:14s remains)
INFO - root - 2017-12-08 05:22:06.162363: step 7860, loss = 2.28, batch loss = 2.23 (34.7 examples/sec; 0.231 sec/batch; 20h:47m:33s remains)
INFO - root - 2017-12-08 05:22:08.493301: step 7870, loss = 2.28, batch loss = 2.23 (34.4 examples/sec; 0.232 sec/batch; 20h:56m:59s remains)
INFO - root - 2017-12-08 05:22:10.714859: step 7880, loss = 2.28, batch loss = 2.23 (34.7 examples/sec; 0.230 sec/batch; 20h:45m:45s remains)
INFO - root - 2017-12-08 05:22:12.941164: step 7890, loss = 2.28, batch loss = 2.23 (34.5 examples/sec; 0.232 sec/batch; 20h:55m:16s remains)
INFO - root - 2017-12-08 05:22:15.216523: step 7900, loss = 2.28, batch loss = 2.23 (34.0 examples/sec; 0.235 sec/batch; 21h:11m:59s remains)
2017-12-08 05:22:15.498195: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288168 -4.4287915 -4.4287844 -4.4287953 -4.4288063 -4.4288187 -4.4288268 -4.4288287 -4.4288306 -4.4288459 -4.4288616 -4.428874 -4.428875 -4.4288678 -4.4288621][-4.4287376 -4.4287066 -4.4287066 -4.4287262 -4.4287357 -4.4287467 -4.4287548 -4.4287572 -4.4287634 -4.4287877 -4.4288144 -4.4288335 -4.428834 -4.4288192 -4.4288049][-4.4286714 -4.4286447 -4.4286604 -4.4286876 -4.4286909 -4.4286928 -4.4286942 -4.4286962 -4.4287062 -4.4287372 -4.4287729 -4.4287972 -4.4287953 -4.4287724 -4.4287519][-4.4286571 -4.4286475 -4.4286785 -4.4287052 -4.4286957 -4.42868 -4.4286661 -4.4286618 -4.4286718 -4.42871 -4.4287539 -4.4287829 -4.4287806 -4.4287548 -4.4287357][-4.4286718 -4.4286752 -4.4287033 -4.4287195 -4.4286966 -4.4286685 -4.4286456 -4.4286361 -4.4286404 -4.4286766 -4.4287248 -4.4287586 -4.4287615 -4.4287434 -4.4287376][-4.4286895 -4.4287004 -4.4287176 -4.4287205 -4.42869 -4.4286566 -4.4286308 -4.428617 -4.4286165 -4.4286466 -4.4286928 -4.4287271 -4.4287343 -4.4287319 -4.4287448][-4.4287124 -4.4287267 -4.4287314 -4.428719 -4.4286809 -4.4286456 -4.4286232 -4.428607 -4.4285979 -4.428618 -4.4286547 -4.4286838 -4.4286962 -4.4287114 -4.4287438][-4.4287109 -4.4287205 -4.4287152 -4.4286971 -4.4286633 -4.428637 -4.4286275 -4.428617 -4.4286017 -4.4286094 -4.428637 -4.4286594 -4.4286709 -4.4286933 -4.4287281][-4.42869 -4.4286947 -4.428689 -4.4286771 -4.428659 -4.4286494 -4.4286575 -4.4286518 -4.428627 -4.4286232 -4.4286404 -4.4286528 -4.4286537 -4.4286714 -4.4286971][-4.4286633 -4.428668 -4.4286728 -4.4286795 -4.42868 -4.4286838 -4.4286981 -4.4286895 -4.4286509 -4.428637 -4.4286418 -4.4286423 -4.4286265 -4.4286342 -4.4286537][-4.42864 -4.4286528 -4.4286728 -4.4286985 -4.42871 -4.4287176 -4.4287329 -4.428721 -4.4286742 -4.4286494 -4.4286427 -4.4286323 -4.4285955 -4.4285908 -4.4286094][-4.4286394 -4.4286551 -4.4286842 -4.4287176 -4.4287333 -4.4287386 -4.4287496 -4.4287353 -4.4286909 -4.4286656 -4.4286566 -4.4286408 -4.4285965 -4.4285841 -4.4286046][-4.4286456 -4.4286609 -4.4286952 -4.4287319 -4.4287515 -4.4287539 -4.4287548 -4.4287324 -4.4286919 -4.4286695 -4.4286628 -4.4286542 -4.4286251 -4.4286141 -4.4286332][-4.4286642 -4.4286771 -4.428709 -4.4287448 -4.4287696 -4.4287763 -4.4287724 -4.4287453 -4.42871 -4.4286947 -4.4286933 -4.4286933 -4.4286833 -4.4286776 -4.4286928][-4.4287148 -4.4287233 -4.4287424 -4.4287672 -4.4287896 -4.4287996 -4.428792 -4.4287653 -4.4287367 -4.4287291 -4.4287329 -4.4287362 -4.4287353 -4.4287319 -4.4287467]]...]
INFO - root - 2017-12-08 05:22:17.731786: step 7910, loss = 2.28, batch loss = 2.23 (35.4 examples/sec; 0.226 sec/batch; 20h:22m:53s remains)
INFO - root - 2017-12-08 05:22:19.958878: step 7920, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 19h:55m:10s remains)
INFO - root - 2017-12-08 05:22:22.202388: step 7930, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 19h:56m:44s remains)
INFO - root - 2017-12-08 05:22:24.440324: step 7940, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 19h:54m:57s remains)
INFO - root - 2017-12-08 05:22:26.653153: step 7950, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.221 sec/batch; 19h:58m:00s remains)
INFO - root - 2017-12-08 05:22:28.887928: step 7960, loss = 2.28, batch loss = 2.23 (36.8 examples/sec; 0.217 sec/batch; 19h:36m:18s remains)
INFO - root - 2017-12-08 05:22:31.126649: step 7970, loss = 2.28, batch loss = 2.23 (35.0 examples/sec; 0.228 sec/batch; 20h:34m:59s remains)
INFO - root - 2017-12-08 05:22:33.383165: step 7980, loss = 2.28, batch loss = 2.23 (35.0 examples/sec; 0.228 sec/batch; 20h:35m:26s remains)
INFO - root - 2017-12-08 05:22:35.607213: step 7990, loss = 2.28, batch loss = 2.23 (36.6 examples/sec; 0.218 sec/batch; 19h:41m:12s remains)
INFO - root - 2017-12-08 05:22:37.848191: step 8000, loss = 2.28, batch loss = 2.23 (34.8 examples/sec; 0.230 sec/batch; 20h:42m:43s remains)
2017-12-08 05:22:38.189906: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4287195 -4.428721 -4.4287286 -4.4287357 -4.4287291 -4.4287314 -4.4287472 -4.4287496 -4.4287434 -4.4287391 -4.4287496 -4.4287558 -4.4287539 -4.4287553 -4.42875][-4.4287109 -4.4287114 -4.4287181 -4.4287305 -4.4287262 -4.4287257 -4.4287367 -4.4287329 -4.4287167 -4.4287047 -4.4287171 -4.4287262 -4.4287224 -4.4287128 -4.4286962][-4.4287372 -4.4287434 -4.4287524 -4.4287648 -4.4287581 -4.4287472 -4.4287434 -4.4287291 -4.4287043 -4.428688 -4.4286952 -4.4287047 -4.4286981 -4.4286695 -4.428628][-4.4287858 -4.4287834 -4.4287872 -4.428791 -4.428772 -4.4287415 -4.4287186 -4.4286923 -4.4286633 -4.428647 -4.4286513 -4.4286594 -4.4286494 -4.4286 -4.4285297][-4.4287939 -4.4287758 -4.4287663 -4.4287567 -4.4287248 -4.4286714 -4.4286222 -4.42858 -4.4285526 -4.4285469 -4.4285622 -4.4285793 -4.4285669 -4.4285049 -4.42842][-4.4287419 -4.4287114 -4.4286852 -4.4286618 -4.4286165 -4.4285336 -4.4284587 -4.4284077 -4.4283919 -4.4284148 -4.4284663 -4.4285069 -4.4284987 -4.4284444 -4.4283772][-4.4286394 -4.4286036 -4.4285655 -4.4285336 -4.4284811 -4.428381 -4.4283028 -4.4282675 -4.4282818 -4.4283419 -4.4284248 -4.4284773 -4.4284735 -4.428442 -4.4284072][-4.4285407 -4.4285135 -4.428484 -4.4284625 -4.428421 -4.4283323 -4.42827 -4.4282641 -4.4282918 -4.428349 -4.4284248 -4.4284687 -4.4284673 -4.4284525 -4.428442][-4.4285183 -4.4285159 -4.4285169 -4.42852 -4.4285 -4.4284325 -4.4283824 -4.4283838 -4.4283981 -4.4284153 -4.4284511 -4.4284744 -4.4284706 -4.4284682 -4.428472][-4.4285593 -4.4285741 -4.4286013 -4.4286284 -4.4286289 -4.428586 -4.4285407 -4.4285321 -4.4285264 -4.4285083 -4.4285021 -4.4284959 -4.4284825 -4.4284921 -4.4285154][-4.4286342 -4.4286585 -4.4287014 -4.4287415 -4.4287567 -4.4287314 -4.4286842 -4.4286518 -4.4286227 -4.4285846 -4.4285583 -4.4285307 -4.4285131 -4.4285336 -4.428566][-4.4287086 -4.4287291 -4.4287686 -4.4288068 -4.4288278 -4.4288149 -4.4287739 -4.4287291 -4.4286823 -4.428637 -4.4286046 -4.4285669 -4.4285483 -4.4285717 -4.4286003][-4.4287724 -4.42878 -4.428803 -4.4288263 -4.4288421 -4.4288349 -4.4288015 -4.4287534 -4.4287004 -4.4286566 -4.4286294 -4.4285984 -4.4285812 -4.4285975 -4.4286184][-4.4288106 -4.4288006 -4.4288044 -4.4288135 -4.4288239 -4.4288216 -4.4287972 -4.4287539 -4.4287057 -4.4286666 -4.4286461 -4.4286251 -4.4286056 -4.4286065 -4.428617][-4.4288292 -4.4288063 -4.4287939 -4.4287896 -4.4287939 -4.4287987 -4.428791 -4.4287682 -4.4287381 -4.4287066 -4.4286866 -4.4286633 -4.4286366 -4.4286165 -4.4286108]]...]
INFO - root - 2017-12-08 05:22:40.437005: step 8010, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 19h:56m:27s remains)
INFO - root - 2017-12-08 05:22:42.663632: step 8020, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.220 sec/batch; 19h:50m:27s remains)
INFO - root - 2017-12-08 05:22:44.873279: step 8030, loss = 2.28, batch loss = 2.23 (36.7 examples/sec; 0.218 sec/batch; 19h:39m:53s remains)
INFO - root - 2017-12-08 05:22:47.120436: step 8040, loss = 2.28, batch loss = 2.23 (35.3 examples/sec; 0.226 sec/batch; 20h:24m:32s remains)
INFO - root - 2017-12-08 05:22:49.312587: step 8050, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 19h:45m:26s remains)
INFO - root - 2017-12-08 05:22:51.548072: step 8060, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.220 sec/batch; 19h:50m:28s remains)
INFO - root - 2017-12-08 05:22:53.816491: step 8070, loss = 2.28, batch loss = 2.23 (34.7 examples/sec; 0.231 sec/batch; 20h:46m:33s remains)
INFO - root - 2017-12-08 05:22:56.076255: step 8080, loss = 2.28, batch loss = 2.23 (33.5 examples/sec; 0.239 sec/batch; 21h:32m:14s remains)
INFO - root - 2017-12-08 05:22:58.321937: step 8090, loss = 2.28, batch loss = 2.23 (36.0 examples/sec; 0.222 sec/batch; 20h:00m:41s remains)
INFO - root - 2017-12-08 05:23:00.570512: step 8100, loss = 2.28, batch loss = 2.23 (35.4 examples/sec; 0.226 sec/batch; 20h:21m:29s remains)
2017-12-08 05:23:00.853666: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289565 -4.42894 -4.4289117 -4.4288964 -4.4289007 -4.4289117 -4.4289212 -4.4289289 -4.4289303 -4.4289279 -4.4289231 -4.428915 -4.428906 -4.4289045 -4.4289117][-4.4289494 -4.4289284 -4.4288983 -4.4288807 -4.4288869 -4.4289041 -4.42892 -4.42893 -4.4289312 -4.4289279 -4.4289188 -4.4289074 -4.4288945 -4.4288907 -4.428905][-4.4288869 -4.4288707 -4.4288435 -4.428822 -4.4288206 -4.4288325 -4.4288516 -4.4288664 -4.4288716 -4.4288692 -4.4288635 -4.4288554 -4.428843 -4.4288387 -4.4288597][-4.42879 -4.4287848 -4.428761 -4.4287291 -4.428709 -4.4287062 -4.4287276 -4.4287553 -4.4287648 -4.4287653 -4.4287691 -4.4287705 -4.428762 -4.4287553 -4.4287782][-4.4287114 -4.4287171 -4.4286919 -4.42864 -4.4285817 -4.4285483 -4.4285765 -4.4286304 -4.4286575 -4.4286695 -4.4286842 -4.4286923 -4.4286842 -4.4286742 -4.4286895][-4.4286609 -4.4286718 -4.4286361 -4.4285536 -4.4284439 -4.4283657 -4.4284 -4.428494 -4.4285583 -4.4286027 -4.4286423 -4.42866 -4.4286523 -4.4286332 -4.4286289][-4.4286094 -4.4286265 -4.4286017 -4.4285221 -4.4283895 -4.4282718 -4.4282856 -4.4283891 -4.4284782 -4.4285531 -4.42862 -4.42865 -4.4286447 -4.4286165 -4.4285932][-4.4285808 -4.4286084 -4.4286203 -4.4285927 -4.4285045 -4.4284091 -4.4283986 -4.4284592 -4.428525 -4.4285917 -4.4286537 -4.4286785 -4.4286675 -4.4286346 -4.4286013][-4.4285936 -4.428627 -4.4286652 -4.4286828 -4.4286489 -4.4285955 -4.4285836 -4.4286103 -4.4286442 -4.4286842 -4.4287219 -4.4287353 -4.4287205 -4.4286876 -4.4286504][-4.4286327 -4.4286695 -4.4287128 -4.4287462 -4.4287481 -4.4287276 -4.4287229 -4.4287367 -4.4287548 -4.4287748 -4.4287944 -4.4287982 -4.4287848 -4.42876 -4.4287276][-4.4286842 -4.428721 -4.42876 -4.42879 -4.4288006 -4.4287891 -4.4287882 -4.4288034 -4.428823 -4.4288435 -4.4288588 -4.428863 -4.4288588 -4.4288483 -4.4288297][-4.4287243 -4.4287586 -4.4287939 -4.4288197 -4.4288249 -4.4288125 -4.4288125 -4.4288359 -4.4288712 -4.4289041 -4.4289246 -4.4289322 -4.4289346 -4.4289322 -4.4289207][-4.42876 -4.42879 -4.4288244 -4.4288468 -4.4288454 -4.4288316 -4.4288359 -4.4288678 -4.4289155 -4.4289584 -4.42898 -4.4289856 -4.4289865 -4.42898 -4.428967][-4.4287877 -4.4288111 -4.4288416 -4.4288607 -4.4288559 -4.4288454 -4.4288554 -4.4288917 -4.4289403 -4.4289808 -4.4289985 -4.428997 -4.4289894 -4.4289756 -4.4289603][-4.4288197 -4.4288368 -4.4288592 -4.4288721 -4.4288664 -4.4288611 -4.4288754 -4.42891 -4.42895 -4.4289784 -4.4289861 -4.4289756 -4.4289565 -4.4289346 -4.4289184]]...]
INFO - root - 2017-12-08 05:23:03.078924: step 8110, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 19h:46m:04s remains)
INFO - root - 2017-12-08 05:23:05.310267: step 8120, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.222 sec/batch; 19h:59m:34s remains)
INFO - root - 2017-12-08 05:23:07.533689: step 8130, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 19h:54m:26s remains)
INFO - root - 2017-12-08 05:23:09.814207: step 8140, loss = 2.28, batch loss = 2.23 (31.4 examples/sec; 0.254 sec/batch; 22h:55m:11s remains)
INFO - root - 2017-12-08 05:23:12.062567: step 8150, loss = 2.28, batch loss = 2.23 (35.6 examples/sec; 0.225 sec/batch; 20h:15m:13s remains)
INFO - root - 2017-12-08 05:23:14.294957: step 8160, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.225 sec/batch; 20h:16m:58s remains)
INFO - root - 2017-12-08 05:23:16.519618: step 8170, loss = 2.28, batch loss = 2.23 (36.8 examples/sec; 0.217 sec/batch; 19h:34m:32s remains)
INFO - root - 2017-12-08 05:23:18.749721: step 8180, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.221 sec/batch; 19h:56m:52s remains)
INFO - root - 2017-12-08 05:23:20.998557: step 8190, loss = 2.28, batch loss = 2.23 (37.0 examples/sec; 0.216 sec/batch; 19h:29m:09s remains)
INFO - root - 2017-12-08 05:23:23.226993: step 8200, loss = 2.28, batch loss = 2.23 (35.0 examples/sec; 0.229 sec/batch; 20h:36m:46s remains)
2017-12-08 05:23:23.519991: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288573 -4.4288435 -4.428843 -4.4288492 -4.4288626 -4.4288635 -4.4288578 -4.4288669 -4.4288878 -4.4289021 -4.4288969 -4.4288836 -4.4288716 -4.42887 -4.428874][-4.4288249 -4.4287982 -4.4288039 -4.4288263 -4.4288535 -4.4288545 -4.4288454 -4.4288478 -4.428865 -4.4288783 -4.4288607 -4.4288363 -4.4288259 -4.4288316 -4.4288397][-4.4288063 -4.4287682 -4.4287672 -4.4287953 -4.4288244 -4.4288139 -4.4287896 -4.4287767 -4.4287872 -4.42881 -4.4287977 -4.4287667 -4.42876 -4.4287696 -4.428781][-4.4288144 -4.4287791 -4.4287682 -4.4287868 -4.428803 -4.428781 -4.4287338 -4.4287052 -4.4287219 -4.4287672 -4.4287639 -4.4287329 -4.4287214 -4.4287248 -4.4287314][-4.4288211 -4.428792 -4.4287739 -4.4287758 -4.4287734 -4.4287372 -4.4286752 -4.4286451 -4.4286737 -4.4287362 -4.4287438 -4.4287171 -4.4287033 -4.4287014 -4.4287024][-4.4288144 -4.4287791 -4.4287529 -4.4287462 -4.4287348 -4.4286833 -4.4286094 -4.4285789 -4.4286208 -4.4286971 -4.4287205 -4.4287133 -4.42871 -4.4287076 -4.4287066][-4.42876 -4.4287105 -4.4286814 -4.4286742 -4.4286551 -4.4285741 -4.428463 -4.4284334 -4.4285064 -4.4286032 -4.4286575 -4.4286895 -4.4287095 -4.4287167 -4.428719][-4.4286542 -4.4285884 -4.4285674 -4.4285831 -4.4285727 -4.4284711 -4.4283233 -4.4282832 -4.4283819 -4.4285131 -4.428607 -4.4286771 -4.4287119 -4.4287171 -4.4287171][-4.4285855 -4.4285164 -4.4285188 -4.4285731 -4.42859 -4.4285131 -4.4283814 -4.4283285 -4.4284129 -4.4285431 -4.4286451 -4.4287128 -4.4287324 -4.4287224 -4.4287148][-4.4285951 -4.4285345 -4.4285617 -4.4286394 -4.4286747 -4.4286327 -4.4285369 -4.4284763 -4.4285235 -4.4286213 -4.4287071 -4.4287519 -4.4287558 -4.42874 -4.4287295][-4.4286971 -4.4286489 -4.4286761 -4.4287472 -4.42878 -4.4287515 -4.428679 -4.4286227 -4.428638 -4.4287009 -4.428762 -4.4287939 -4.4287992 -4.4287858 -4.4287767][-4.428802 -4.4287686 -4.4287891 -4.4288373 -4.4288583 -4.4288311 -4.4287758 -4.4287424 -4.4287558 -4.428793 -4.42882 -4.4288344 -4.4288387 -4.4288311 -4.4288244][-4.4288464 -4.4288249 -4.4288464 -4.4288788 -4.4288864 -4.4288597 -4.428822 -4.4288092 -4.4288249 -4.4288445 -4.428854 -4.4288554 -4.4288554 -4.4288511 -4.4288497][-4.4288673 -4.4288607 -4.4288869 -4.4289093 -4.4289088 -4.4288778 -4.4288507 -4.4288478 -4.4288611 -4.42887 -4.428874 -4.4288754 -4.4288812 -4.4288878 -4.4288907][-4.4289045 -4.4289093 -4.4289289 -4.4289374 -4.42893 -4.4289064 -4.4288874 -4.4288907 -4.4289036 -4.4289126 -4.4289169 -4.4289212 -4.4289289 -4.4289351 -4.4289336]]...]
INFO - root - 2017-12-08 05:23:25.782752: step 8210, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 19h:49m:14s remains)
INFO - root - 2017-12-08 05:23:28.005752: step 8220, loss = 2.28, batch loss = 2.23 (35.4 examples/sec; 0.226 sec/batch; 20h:20m:30s remains)
INFO - root - 2017-12-08 05:23:30.246704: step 8230, loss = 2.28, batch loss = 2.23 (35.6 examples/sec; 0.225 sec/batch; 20h:15m:44s remains)
INFO - root - 2017-12-08 05:23:32.484114: step 8240, loss = 2.28, batch loss = 2.23 (35.8 examples/sec; 0.223 sec/batch; 20h:07m:33s remains)
INFO - root - 2017-12-08 05:23:34.742277: step 8250, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.222 sec/batch; 19h:59m:07s remains)
INFO - root - 2017-12-08 05:23:36.959357: step 8260, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 19h:43m:50s remains)
INFO - root - 2017-12-08 05:23:39.233017: step 8270, loss = 2.28, batch loss = 2.23 (35.0 examples/sec; 0.229 sec/batch; 20h:35m:56s remains)
INFO - root - 2017-12-08 05:23:41.465750: step 8280, loss = 2.28, batch loss = 2.23 (34.6 examples/sec; 0.231 sec/batch; 20h:49m:46s remains)
INFO - root - 2017-12-08 05:23:43.712819: step 8290, loss = 2.28, batch loss = 2.23 (34.8 examples/sec; 0.230 sec/batch; 20h:40m:25s remains)
INFO - root - 2017-12-08 05:23:45.935859: step 8300, loss = 2.28, batch loss = 2.23 (36.0 examples/sec; 0.222 sec/batch; 19h:59m:52s remains)
2017-12-08 05:23:46.205484: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4290195 -4.4290166 -4.4290109 -4.4290047 -4.4290004 -4.4289975 -4.4289951 -4.4289904 -4.4289856 -4.4289865 -4.4289923 -4.4289966 -4.4289966 -4.4289966 -4.429][-4.429019 -4.4290156 -4.429008 -4.4289994 -4.4289918 -4.4289865 -4.4289813 -4.4289727 -4.428966 -4.428968 -4.4289742 -4.4289756 -4.428968 -4.4289594 -4.4289584][-4.4290037 -4.4289932 -4.4289784 -4.4289637 -4.4289508 -4.4289389 -4.4289227 -4.4289026 -4.42889 -4.4288931 -4.4289017 -4.4289 -4.4288878 -4.4288788 -4.42888][-4.428968 -4.4289432 -4.428915 -4.4288898 -4.4288654 -4.4288378 -4.4288011 -4.4287591 -4.4287376 -4.4287481 -4.4287653 -4.4287653 -4.4287539 -4.4287539 -4.4287691][-4.4289055 -4.4288611 -4.4288177 -4.4287829 -4.4287477 -4.4287009 -4.4286375 -4.4285645 -4.4285297 -4.4285626 -4.4286065 -4.4286165 -4.4286103 -4.4286232 -4.4286561][-4.4288316 -4.4287691 -4.4287167 -4.428678 -4.4286323 -4.4285612 -4.428463 -4.4283476 -4.4283009 -4.4283814 -4.4284782 -4.4285088 -4.4285073 -4.4285288 -4.4285722][-4.4287786 -4.4287081 -4.4286542 -4.4286151 -4.4285531 -4.4284492 -4.4283175 -4.428165 -4.4281125 -4.4282508 -4.4284043 -4.4284616 -4.4284649 -4.4284863 -4.4285383][-4.4287796 -4.4287167 -4.4286675 -4.4286242 -4.4285417 -4.4284177 -4.4282832 -4.4281421 -4.4281092 -4.4282646 -4.4284325 -4.4285007 -4.4285011 -4.4285154 -4.4285622][-4.4288249 -4.4287763 -4.4287329 -4.4286842 -4.4285984 -4.4284868 -4.4283805 -4.4282851 -4.4282727 -4.4283977 -4.4285388 -4.4285994 -4.4286013 -4.4286113 -4.4286408][-4.4288592 -4.4288197 -4.42878 -4.4287319 -4.4286609 -4.4285808 -4.4285111 -4.4284544 -4.4284515 -4.4285369 -4.4286385 -4.4286909 -4.4287043 -4.4287186 -4.4287305][-4.4288797 -4.4288497 -4.4288206 -4.4287834 -4.4287367 -4.4286866 -4.4286461 -4.4286132 -4.4286075 -4.4286504 -4.4287133 -4.4287581 -4.4287844 -4.428803 -4.4288044][-4.428916 -4.4288988 -4.4288793 -4.4288549 -4.4288239 -4.4287949 -4.4287744 -4.4287605 -4.4287505 -4.4287581 -4.4287896 -4.4288235 -4.428854 -4.4288764 -4.4288745][-4.4289541 -4.428947 -4.428937 -4.4289289 -4.428915 -4.4289026 -4.4288983 -4.4288969 -4.4288836 -4.4288697 -4.4288716 -4.4288859 -4.4289079 -4.4289317 -4.4289365][-4.4289894 -4.4289885 -4.4289846 -4.4289861 -4.4289865 -4.428988 -4.4289942 -4.4289994 -4.4289865 -4.4289665 -4.42895 -4.4289436 -4.4289551 -4.4289775 -4.428988][-4.429019 -4.429019 -4.4290128 -4.4290128 -4.4290175 -4.4290252 -4.4290366 -4.4290452 -4.4290385 -4.4290218 -4.4290013 -4.4289885 -4.4289908 -4.4290047 -4.4290161]]...]
INFO - root - 2017-12-08 05:23:48.437578: step 8310, loss = 2.28, batch loss = 2.23 (34.7 examples/sec; 0.231 sec/batch; 20h:45m:40s remains)
INFO - root - 2017-12-08 05:23:50.681411: step 8320, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.226 sec/batch; 20h:19m:09s remains)
INFO - root - 2017-12-08 05:23:52.917474: step 8330, loss = 2.28, batch loss = 2.23 (35.2 examples/sec; 0.228 sec/batch; 20h:29m:12s remains)
INFO - root - 2017-12-08 05:23:55.138406: step 8340, loss = 2.28, batch loss = 2.23 (35.1 examples/sec; 0.228 sec/batch; 20h:32m:11s remains)
INFO - root - 2017-12-08 05:23:57.362127: step 8350, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.220 sec/batch; 19h:50m:04s remains)
INFO - root - 2017-12-08 05:23:59.606877: step 8360, loss = 2.28, batch loss = 2.23 (35.7 examples/sec; 0.224 sec/batch; 20h:11m:36s remains)
INFO - root - 2017-12-08 05:24:01.836110: step 8370, loss = 2.28, batch loss = 2.23 (36.0 examples/sec; 0.222 sec/batch; 19h:59m:52s remains)
INFO - root - 2017-12-08 05:24:04.062227: step 8380, loss = 2.28, batch loss = 2.23 (36.0 examples/sec; 0.222 sec/batch; 20h:00m:55s remains)
INFO - root - 2017-12-08 05:24:06.320338: step 8390, loss = 2.28, batch loss = 2.23 (34.2 examples/sec; 0.234 sec/batch; 21h:03m:43s remains)
INFO - root - 2017-12-08 05:24:08.570775: step 8400, loss = 2.28, batch loss = 2.23 (35.8 examples/sec; 0.224 sec/batch; 20h:07m:20s remains)
2017-12-08 05:24:08.846752: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4290142 -4.42904 -4.4290509 -4.4290113 -4.4289107 -4.42878 -4.4286795 -4.4286408 -4.4286661 -4.4287319 -4.4288096 -4.4288731 -4.4289093 -4.4289169 -4.4289093][-4.4290338 -4.4290638 -4.4290824 -4.4290566 -4.4289808 -4.4288659 -4.4287577 -4.4286871 -4.428669 -4.4286976 -4.4287581 -4.4288268 -4.4288859 -4.4289188 -4.4289284][-4.4290509 -4.4290829 -4.4291034 -4.4290829 -4.4290175 -4.4289083 -4.4287868 -4.42869 -4.428638 -4.4286389 -4.4286942 -4.4287729 -4.4288564 -4.4289179 -4.4289522][-4.4290614 -4.4290919 -4.4291115 -4.4290967 -4.4290347 -4.4289255 -4.4287939 -4.4286776 -4.4285975 -4.4285746 -4.428628 -4.4287133 -4.4288158 -4.4289064 -4.4289684][-4.4290671 -4.4290953 -4.4291115 -4.4290986 -4.4290366 -4.4289274 -4.4287906 -4.4286542 -4.4285417 -4.4284925 -4.428546 -4.4286418 -4.42876 -4.4288721 -4.428957][-4.42907 -4.4290948 -4.429101 -4.4290776 -4.4290042 -4.428884 -4.4287362 -4.4285707 -4.4284196 -4.4283524 -4.4284272 -4.4285507 -4.4286909 -4.4288239 -4.4289269][-4.4290667 -4.4290853 -4.4290776 -4.4290318 -4.4289341 -4.4287934 -4.4286289 -4.4284344 -4.4282465 -4.4281774 -4.4282923 -4.428452 -4.4286118 -4.428762 -4.4288855][-4.4290586 -4.4290676 -4.4290423 -4.4289703 -4.4288468 -4.4286914 -4.4285259 -4.428329 -4.4281354 -4.428081 -4.4282227 -4.4283953 -4.428555 -4.4287071 -4.428843][-4.4290466 -4.4290419 -4.4290004 -4.42891 -4.4287739 -4.4286227 -4.4284792 -4.4283156 -4.4281654 -4.428144 -4.4282703 -4.4284158 -4.4285522 -4.4286933 -4.42883][-4.4290338 -4.4290171 -4.428967 -4.4288712 -4.4287381 -4.4286017 -4.4284906 -4.4283786 -4.4282908 -4.4282994 -4.4283953 -4.4285016 -4.4286108 -4.4287353 -4.4288588][-4.4290237 -4.4290032 -4.4289551 -4.4288669 -4.428751 -4.4286375 -4.4285574 -4.4284954 -4.428463 -4.4284883 -4.4285522 -4.4286261 -4.4287128 -4.4288173 -4.4289165][-4.4290195 -4.4290028 -4.4289675 -4.4289031 -4.4288168 -4.4287319 -4.428678 -4.4286532 -4.4286551 -4.4286814 -4.4287176 -4.4287648 -4.4288273 -4.4289055 -4.4289737][-4.4290161 -4.4290071 -4.4289885 -4.4289513 -4.4288974 -4.428843 -4.4288135 -4.4288092 -4.428823 -4.4288454 -4.4288683 -4.4288993 -4.4289374 -4.4289832 -4.4290161][-4.4290137 -4.4290109 -4.4290056 -4.4289913 -4.428966 -4.42894 -4.4289269 -4.4289308 -4.4289436 -4.42896 -4.4289761 -4.4289975 -4.4290161 -4.4290304 -4.42903][-4.4290109 -4.4290118 -4.4290123 -4.4290104 -4.4290042 -4.4289966 -4.4289947 -4.4289994 -4.4290085 -4.429019 -4.4290304 -4.4290414 -4.4290419 -4.4290314 -4.4290085]]...]
INFO - root - 2017-12-08 05:24:11.069021: step 8410, loss = 2.28, batch loss = 2.23 (35.8 examples/sec; 0.224 sec/batch; 20h:07m:30s remains)
INFO - root - 2017-12-08 05:24:13.340626: step 8420, loss = 2.28, batch loss = 2.23 (34.1 examples/sec; 0.235 sec/batch; 21h:06m:43s remains)
INFO - root - 2017-12-08 05:24:15.562915: step 8430, loss = 2.28, batch loss = 2.23 (37.8 examples/sec; 0.212 sec/batch; 19h:03m:58s remains)
INFO - root - 2017-12-08 05:24:17.798480: step 8440, loss = 2.28, batch loss = 2.23 (36.8 examples/sec; 0.218 sec/batch; 19h:35m:06s remains)
INFO - root - 2017-12-08 05:24:20.043930: step 8450, loss = 2.28, batch loss = 2.23 (35.2 examples/sec; 0.227 sec/batch; 20h:26m:08s remains)
INFO - root - 2017-12-08 05:24:22.318378: step 8460, loss = 2.28, batch loss = 2.23 (34.3 examples/sec; 0.233 sec/batch; 20h:59m:43s remains)
INFO - root - 2017-12-08 05:24:24.566570: step 8470, loss = 2.28, batch loss = 2.23 (34.1 examples/sec; 0.235 sec/batch; 21h:06m:54s remains)
INFO - root - 2017-12-08 05:24:26.826530: step 8480, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.220 sec/batch; 19h:50m:30s remains)
INFO - root - 2017-12-08 05:24:29.056899: step 8490, loss = 2.28, batch loss = 2.23 (35.8 examples/sec; 0.223 sec/batch; 20h:05m:43s remains)
INFO - root - 2017-12-08 05:24:31.278850: step 8500, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 19h:51m:52s remains)
2017-12-08 05:24:31.605966: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289594 -4.4289618 -4.428957 -4.4289551 -4.428957 -4.4289613 -4.4289632 -4.4289622 -4.42896 -4.4289618 -4.428966 -4.4289684 -4.4289694 -4.428968 -4.4289632][-4.428956 -4.4289608 -4.4289546 -4.4289508 -4.4289508 -4.4289541 -4.4289546 -4.4289513 -4.428947 -4.4289465 -4.42895 -4.4289532 -4.4289565 -4.4289589 -4.428956][-4.4289346 -4.4289374 -4.4289293 -4.4289241 -4.4289246 -4.4289284 -4.42893 -4.428926 -4.4289227 -4.4289231 -4.4289269 -4.4289341 -4.4289427 -4.4289489 -4.4289479][-4.4289165 -4.4289141 -4.4288979 -4.4288859 -4.4288831 -4.428885 -4.4288855 -4.4288812 -4.4288793 -4.4288831 -4.4288926 -4.4289064 -4.4289207 -4.4289303 -4.42893][-4.4288783 -4.4288659 -4.4288368 -4.428813 -4.4288025 -4.4287996 -4.4287949 -4.4287853 -4.4287829 -4.4287949 -4.4288187 -4.428844 -4.4288645 -4.428875 -4.4288721][-4.4288373 -4.4288154 -4.4287705 -4.4287267 -4.4286962 -4.4286761 -4.4286561 -4.4286356 -4.4286284 -4.4286504 -4.4286947 -4.4287376 -4.4287677 -4.4287786 -4.4287739][-4.4288211 -4.4288 -4.4287457 -4.4286804 -4.4286237 -4.4285779 -4.428535 -4.4284945 -4.4284754 -4.4285026 -4.4285617 -4.428618 -4.4286542 -4.4286661 -4.4286613][-4.4288087 -4.428803 -4.4287572 -4.4286914 -4.42863 -4.4285803 -4.4285307 -4.4284849 -4.4284592 -4.4284778 -4.4285293 -4.4285774 -4.4286056 -4.4286118 -4.4286036][-4.428772 -4.4287791 -4.4287472 -4.4286957 -4.4286537 -4.4286294 -4.4286075 -4.42859 -4.4285812 -4.428597 -4.4286323 -4.4286637 -4.4286804 -4.4286852 -4.4286776][-4.4287915 -4.4288044 -4.4287834 -4.4287477 -4.4287248 -4.42872 -4.42872 -4.4287252 -4.428731 -4.4287457 -4.4287663 -4.4287839 -4.428793 -4.4287968 -4.428791][-4.4288545 -4.4288688 -4.4288592 -4.4288406 -4.4288316 -4.428833 -4.4288406 -4.4288526 -4.428864 -4.4288759 -4.4288864 -4.4288945 -4.4288993 -4.4289007 -4.4288964][-4.4289203 -4.4289317 -4.4289269 -4.4289179 -4.4289126 -4.4289126 -4.4289169 -4.4289274 -4.4289384 -4.4289484 -4.4289536 -4.4289584 -4.4289613 -4.4289627 -4.4289608][-4.4289646 -4.4289703 -4.4289665 -4.42896 -4.4289536 -4.4289474 -4.4289441 -4.4289465 -4.4289522 -4.4289579 -4.4289608 -4.4289641 -4.428966 -4.4289656 -4.4289656][-4.4289646 -4.4289646 -4.4289584 -4.4289536 -4.42895 -4.4289432 -4.4289374 -4.4289355 -4.428937 -4.428937 -4.4289365 -4.428937 -4.428937 -4.428937 -4.4289393][-4.428937 -4.428937 -4.4289312 -4.4289303 -4.4289312 -4.4289293 -4.428926 -4.4289246 -4.428925 -4.4289241 -4.4289203 -4.4289169 -4.4289141 -4.4289131 -4.4289155]]...]
INFO - root - 2017-12-08 05:24:33.811534: step 8510, loss = 2.28, batch loss = 2.23 (36.7 examples/sec; 0.218 sec/batch; 19h:37m:34s remains)
INFO - root - 2017-12-08 05:24:36.023697: step 8520, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.221 sec/batch; 19h:51m:02s remains)
INFO - root - 2017-12-08 05:24:38.260015: step 8530, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.225 sec/batch; 20h:15m:23s remains)
INFO - root - 2017-12-08 05:24:40.477135: step 8540, loss = 2.28, batch loss = 2.23 (37.8 examples/sec; 0.212 sec/batch; 19h:03m:01s remains)
INFO - root - 2017-12-08 05:24:42.716443: step 8550, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 20h:02m:24s remains)
INFO - root - 2017-12-08 05:24:44.932502: step 8560, loss = 2.28, batch loss = 2.23 (35.2 examples/sec; 0.227 sec/batch; 20h:25m:41s remains)
INFO - root - 2017-12-08 05:24:47.187468: step 8570, loss = 2.28, batch loss = 2.23 (33.7 examples/sec; 0.237 sec/batch; 21h:20m:08s remains)
INFO - root - 2017-12-08 05:24:49.447327: step 8580, loss = 2.28, batch loss = 2.23 (37.2 examples/sec; 0.215 sec/batch; 19h:20m:42s remains)
INFO - root - 2017-12-08 05:24:51.678652: step 8590, loss = 2.28, batch loss = 2.23 (36.6 examples/sec; 0.218 sec/batch; 19h:38m:29s remains)
INFO - root - 2017-12-08 05:24:53.921236: step 8600, loss = 2.28, batch loss = 2.23 (35.2 examples/sec; 0.228 sec/batch; 20h:28m:29s remains)
2017-12-08 05:24:54.244626: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288969 -4.4288955 -4.4288931 -4.4288926 -4.4288898 -4.4288807 -4.4288697 -4.4288611 -4.42886 -4.428865 -4.4288721 -4.4288759 -4.4288726 -4.4288607 -4.42884][-4.4289165 -4.428915 -4.4289169 -4.4289227 -4.4289255 -4.4289155 -4.4288983 -4.428884 -4.4288812 -4.4288893 -4.4289007 -4.4289079 -4.4289036 -4.4288883 -4.4288635][-4.4289112 -4.4289112 -4.4289207 -4.428936 -4.4289474 -4.428937 -4.4289122 -4.4288921 -4.4288921 -4.4289069 -4.4289236 -4.4289341 -4.4289289 -4.4289103 -4.4288845][-4.4288816 -4.4288754 -4.4288855 -4.4289036 -4.4289136 -4.4288988 -4.4288673 -4.4288449 -4.4288597 -4.4288936 -4.4289169 -4.4289279 -4.4289207 -4.4288993 -4.4288745][-4.4288239 -4.4288135 -4.4288235 -4.4288397 -4.4288354 -4.4287996 -4.4287429 -4.4287076 -4.4287405 -4.4288063 -4.4288497 -4.42887 -4.4288716 -4.428853 -4.4288354][-4.4287376 -4.4287319 -4.4287548 -4.4287767 -4.4287643 -4.4287014 -4.4286003 -4.4285135 -4.4285436 -4.4286504 -4.4287252 -4.4287648 -4.42878 -4.4287729 -4.4287667][-4.428679 -4.4286718 -4.4286942 -4.4287157 -4.4286985 -4.4286122 -4.4284573 -4.4283004 -4.4283094 -4.4284515 -4.4285574 -4.4286175 -4.4286504 -4.4286594 -4.428669][-4.4286785 -4.4286609 -4.4286718 -4.4286838 -4.4286561 -4.4285617 -4.4283977 -4.4282322 -4.4282322 -4.4283724 -4.4284854 -4.428556 -4.4285941 -4.4286079 -4.4286194][-4.428679 -4.4286585 -4.4286671 -4.42868 -4.4286652 -4.4286051 -4.4285026 -4.4283996 -4.4283977 -4.4284859 -4.4285674 -4.4286208 -4.4286418 -4.4286418 -4.428637][-4.4286675 -4.4286556 -4.4286637 -4.428679 -4.4286737 -4.4286427 -4.4286017 -4.428555 -4.4285493 -4.4285917 -4.4286437 -4.4286871 -4.4287009 -4.4286923 -4.4286737][-4.4286366 -4.4286346 -4.4286394 -4.4286513 -4.4286547 -4.4286466 -4.4286427 -4.4286351 -4.4286308 -4.4286542 -4.4286962 -4.4287372 -4.4287505 -4.4287386 -4.4287128][-4.4286208 -4.4286256 -4.4286318 -4.4286432 -4.4286575 -4.42867 -4.4286809 -4.4286876 -4.4286814 -4.4286957 -4.4287333 -4.428772 -4.4287844 -4.4287758 -4.4287553][-4.42867 -4.4286695 -4.4286757 -4.4286852 -4.4287024 -4.428719 -4.4287286 -4.4287372 -4.4287362 -4.4287462 -4.4287729 -4.4288006 -4.4288087 -4.4288054 -4.4287972][-4.428751 -4.4287472 -4.4287519 -4.4287596 -4.428772 -4.4287791 -4.4287839 -4.4287882 -4.4287872 -4.4287906 -4.4288 -4.4288144 -4.4288197 -4.4288211 -4.4288187][-4.4288244 -4.4288182 -4.4288158 -4.4288163 -4.4288197 -4.4288177 -4.4288197 -4.4288197 -4.4288125 -4.4288092 -4.4288125 -4.4288197 -4.428822 -4.4288239 -4.4288225]]...]
INFO - root - 2017-12-08 05:24:56.474363: step 8610, loss = 2.28, batch loss = 2.23 (35.6 examples/sec; 0.225 sec/batch; 20h:12m:29s remains)
INFO - root - 2017-12-08 05:24:58.694773: step 8620, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 20h:02m:17s remains)
INFO - root - 2017-12-08 05:25:00.922752: step 8630, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 20h:02m:27s remains)
INFO - root - 2017-12-08 05:25:03.169809: step 8640, loss = 2.28, batch loss = 2.23 (34.1 examples/sec; 0.235 sec/batch; 21h:08m:05s remains)
INFO - root - 2017-12-08 05:25:05.440236: step 8650, loss = 2.28, batch loss = 2.23 (35.1 examples/sec; 0.228 sec/batch; 20h:30m:21s remains)
INFO - root - 2017-12-08 05:25:07.684276: step 8660, loss = 2.28, batch loss = 2.23 (35.6 examples/sec; 0.225 sec/batch; 20h:13m:11s remains)
INFO - root - 2017-12-08 05:25:09.948126: step 8670, loss = 2.28, batch loss = 2.23 (35.3 examples/sec; 0.227 sec/batch; 20h:24m:44s remains)
INFO - root - 2017-12-08 05:25:12.155591: step 8680, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 19h:41m:20s remains)
INFO - root - 2017-12-08 05:25:14.378093: step 8690, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.225 sec/batch; 20h:15m:55s remains)
INFO - root - 2017-12-08 05:25:16.622167: step 8700, loss = 2.28, batch loss = 2.23 (34.8 examples/sec; 0.230 sec/batch; 20h:40m:44s remains)
2017-12-08 05:25:16.942753: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289207 -4.4289131 -4.4289055 -4.4289002 -4.4289088 -4.4289274 -4.4289241 -4.4289093 -4.4288821 -4.4288554 -4.4288468 -4.4288406 -4.428843 -4.4288564 -4.4288769][-4.428916 -4.4289055 -4.4288974 -4.4288812 -4.4288821 -4.4289021 -4.4289007 -4.4288783 -4.4288487 -4.4288321 -4.4288349 -4.4288287 -4.4288368 -4.4288559 -4.4288745][-4.4288936 -4.4288807 -4.4288697 -4.4288473 -4.42884 -4.4288535 -4.4288373 -4.4288 -4.4287739 -4.4287777 -4.428802 -4.4288044 -4.4288254 -4.4288535 -4.4288769][-4.428864 -4.4288459 -4.4288263 -4.4288073 -4.4287968 -4.4287891 -4.4287391 -4.4286757 -4.4286618 -4.4286962 -4.4287529 -4.4287786 -4.4288154 -4.4288516 -4.4288793][-4.4288397 -4.4288187 -4.4287858 -4.4287524 -4.4287162 -4.4286628 -4.4285574 -4.4284635 -4.4284763 -4.4285631 -4.4286604 -4.42872 -4.4287782 -4.428823 -4.4288554][-4.4288177 -4.428791 -4.4287453 -4.4286871 -4.4286132 -4.428493 -4.4283133 -4.4281869 -4.4282556 -4.4284053 -4.4285336 -4.4286308 -4.4287181 -4.428782 -4.4288268][-4.428803 -4.4287705 -4.4287124 -4.4286418 -4.4285331 -4.4283414 -4.4280858 -4.4279404 -4.4280939 -4.4283023 -4.4284439 -4.4285636 -4.4286671 -4.4287481 -4.4288077][-4.4288039 -4.4287763 -4.4287257 -4.4286528 -4.4285259 -4.4283032 -4.4280229 -4.4279056 -4.4281192 -4.4283466 -4.4284697 -4.4285722 -4.4286618 -4.4287434 -4.4288068][-4.4288235 -4.4288058 -4.4287643 -4.4286919 -4.4285769 -4.4283915 -4.4281821 -4.4281306 -4.4283204 -4.4285064 -4.4285946 -4.428658 -4.428709 -4.428772 -4.4288297][-4.4288645 -4.4288549 -4.4288192 -4.4287577 -4.4286747 -4.4285541 -4.428442 -4.4284339 -4.4285736 -4.4287004 -4.4287486 -4.4287853 -4.4288039 -4.4288383 -4.4288754][-4.4289074 -4.4289103 -4.428885 -4.4288459 -4.4287896 -4.4287276 -4.4286876 -4.4287028 -4.428792 -4.4288635 -4.4288874 -4.4289041 -4.4289041 -4.4289141 -4.4289312][-4.4289441 -4.4289522 -4.4289417 -4.4289279 -4.4289064 -4.4288826 -4.4288731 -4.4288926 -4.4289384 -4.4289727 -4.4289775 -4.4289751 -4.4289651 -4.428966 -4.4289751][-4.4289684 -4.4289756 -4.4289737 -4.428977 -4.4289856 -4.4289842 -4.4289908 -4.4290047 -4.429018 -4.4290257 -4.4290166 -4.4290028 -4.4289856 -4.4289827 -4.4289889][-4.4289632 -4.4289656 -4.428966 -4.4289742 -4.4289942 -4.4290075 -4.4290223 -4.429028 -4.4290237 -4.4290175 -4.4290028 -4.4289856 -4.4289713 -4.428968 -4.4289746][-4.42894 -4.4289408 -4.4289422 -4.4289465 -4.4289656 -4.42898 -4.4289865 -4.4289861 -4.4289789 -4.4289708 -4.4289627 -4.4289522 -4.4289422 -4.4289432 -4.4289513]]...]
INFO - root - 2017-12-08 05:25:19.206818: step 8710, loss = 2.28, batch loss = 2.23 (33.6 examples/sec; 0.238 sec/batch; 21h:25m:15s remains)
INFO - root - 2017-12-08 05:25:21.456396: step 8720, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 19h:44m:07s remains)
INFO - root - 2017-12-08 05:25:23.739846: step 8730, loss = 2.28, batch loss = 2.23 (34.9 examples/sec; 0.229 sec/batch; 20h:37m:23s remains)
INFO - root - 2017-12-08 05:25:25.983796: step 8740, loss = 2.28, batch loss = 2.23 (35.0 examples/sec; 0.228 sec/batch; 20h:32m:45s remains)
INFO - root - 2017-12-08 05:25:28.226951: step 8750, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 19h:42m:45s remains)
INFO - root - 2017-12-08 05:25:30.475125: step 8760, loss = 2.28, batch loss = 2.23 (35.7 examples/sec; 0.224 sec/batch; 20h:08m:19s remains)
INFO - root - 2017-12-08 05:25:32.714637: step 8770, loss = 2.28, batch loss = 2.23 (36.6 examples/sec; 0.219 sec/batch; 19h:40m:02s remains)
INFO - root - 2017-12-08 05:25:34.977720: step 8780, loss = 2.28, batch loss = 2.23 (35.1 examples/sec; 0.228 sec/batch; 20h:30m:19s remains)
INFO - root - 2017-12-08 05:25:37.238069: step 8790, loss = 2.28, batch loss = 2.23 (32.9 examples/sec; 0.243 sec/batch; 21h:51m:14s remains)
INFO - root - 2017-12-08 05:25:39.484553: step 8800, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 19h:42m:15s remains)
2017-12-08 05:25:39.756251: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4287572 -4.4287724 -4.4287648 -4.4287424 -4.4287157 -4.428699 -4.4287004 -4.4287291 -4.4287772 -4.4288239 -4.428874 -4.4289155 -4.4289417 -4.4289455 -4.4289112][-4.4287348 -4.428771 -4.428782 -4.4287758 -4.4287581 -4.4287395 -4.4287357 -4.4287529 -4.4287891 -4.4288278 -4.4288716 -4.42891 -4.4289379 -4.4289417 -4.4289088][-4.4287248 -4.4287834 -4.4288025 -4.4288006 -4.4287877 -4.4287663 -4.4287539 -4.4287562 -4.4287791 -4.4288116 -4.4288473 -4.4288793 -4.4289064 -4.4289136 -4.4288845][-4.4287391 -4.4288092 -4.42883 -4.42882 -4.4287953 -4.428762 -4.4287314 -4.4287167 -4.4287291 -4.4287667 -4.4288034 -4.4288416 -4.428874 -4.4288831 -4.4288549][-4.42878 -4.4288478 -4.4288616 -4.4288325 -4.42878 -4.4287109 -4.4286389 -4.4286032 -4.4286318 -4.4287 -4.4287605 -4.4288197 -4.4288583 -4.428865 -4.42883][-4.4288387 -4.4288964 -4.4288955 -4.428844 -4.4287553 -4.4286256 -4.4284806 -4.4284124 -4.4284878 -4.4286289 -4.4287395 -4.4288149 -4.4288526 -4.4288516 -4.428812][-4.4288826 -4.428925 -4.4289122 -4.4288468 -4.4287233 -4.4285245 -4.4282851 -4.428185 -4.4283366 -4.42857 -4.4287314 -4.4288197 -4.4288516 -4.42884 -4.4287958][-4.4288969 -4.4289222 -4.4289055 -4.4288416 -4.4287081 -4.4284658 -4.4281616 -4.4280519 -4.4282746 -4.4285574 -4.4287353 -4.428823 -4.4288492 -4.4288287 -4.4287863][-4.4288878 -4.4289069 -4.4289012 -4.4288478 -4.4287276 -4.4284964 -4.4282231 -4.4281435 -4.4283528 -4.4285979 -4.4287467 -4.42882 -4.4288373 -4.4288211 -4.4287891][-4.4288659 -4.4288869 -4.4289012 -4.4288735 -4.4287848 -4.4286032 -4.4284077 -4.4283619 -4.4285007 -4.4286637 -4.4287663 -4.4288177 -4.4288259 -4.42882 -4.4288058][-4.4288354 -4.428854 -4.4288936 -4.4289083 -4.428863 -4.4287405 -4.4286118 -4.4285727 -4.4286356 -4.4287276 -4.4287944 -4.4288249 -4.4288263 -4.4288292 -4.4288325][-4.4288068 -4.4288297 -4.4288955 -4.428946 -4.428937 -4.42886 -4.4287715 -4.4287224 -4.4287314 -4.4287786 -4.4288254 -4.4288416 -4.4288359 -4.4288406 -4.4288535][-4.4287949 -4.4288268 -4.4289031 -4.4289703 -4.4289804 -4.4289327 -4.4288626 -4.4288044 -4.4287868 -4.4288125 -4.4288459 -4.4288535 -4.4288487 -4.428853 -4.4288645][-4.4288092 -4.4288459 -4.42892 -4.428977 -4.4289894 -4.42896 -4.4289021 -4.4288378 -4.428803 -4.4288149 -4.4288416 -4.4288468 -4.4288383 -4.4288425 -4.4288549][-4.4288411 -4.4288769 -4.428936 -4.4289742 -4.4289804 -4.4289527 -4.4288974 -4.4288268 -4.4287767 -4.42878 -4.4288068 -4.4288149 -4.4287992 -4.4287949 -4.428802]]...]
INFO - root - 2017-12-08 05:25:42.016741: step 8810, loss = 2.28, batch loss = 2.23 (32.7 examples/sec; 0.245 sec/batch; 22h:01m:07s remains)
INFO - root - 2017-12-08 05:25:44.274233: step 8820, loss = 2.28, batch loss = 2.23 (35.7 examples/sec; 0.224 sec/batch; 20h:07m:21s remains)
INFO - root - 2017-12-08 05:25:46.493715: step 8830, loss = 2.28, batch loss = 2.23 (36.6 examples/sec; 0.219 sec/batch; 19h:40m:09s remains)
INFO - root - 2017-12-08 05:25:48.724497: step 8840, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.222 sec/batch; 19h:55m:06s remains)
INFO - root - 2017-12-08 05:25:50.970924: step 8850, loss = 2.28, batch loss = 2.23 (34.8 examples/sec; 0.230 sec/batch; 20h:41m:28s remains)
INFO - root - 2017-12-08 05:25:53.217963: step 8860, loss = 2.28, batch loss = 2.23 (33.9 examples/sec; 0.236 sec/batch; 21h:11m:07s remains)
INFO - root - 2017-12-08 05:25:55.479114: step 8870, loss = 2.28, batch loss = 2.23 (36.7 examples/sec; 0.218 sec/batch; 19h:36m:16s remains)
INFO - root - 2017-12-08 05:25:57.708374: step 8880, loss = 2.28, batch loss = 2.23 (35.8 examples/sec; 0.224 sec/batch; 20h:06m:13s remains)
INFO - root - 2017-12-08 05:25:59.931382: step 8890, loss = 2.28, batch loss = 2.23 (37.3 examples/sec; 0.215 sec/batch; 19h:18m:09s remains)
INFO - root - 2017-12-08 05:26:02.176035: step 8900, loss = 2.28, batch loss = 2.23 (34.6 examples/sec; 0.231 sec/batch; 20h:48m:12s remains)
2017-12-08 05:26:02.486082: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4287338 -4.42872 -4.4287353 -4.4287643 -4.4287858 -4.4287748 -4.4287739 -4.4287715 -4.4287815 -4.4288025 -4.4288106 -4.428791 -4.4287457 -4.4286833 -4.428648][-4.4286275 -4.4286456 -4.4286919 -4.42875 -4.4287996 -4.4288034 -4.428802 -4.4287915 -4.4287968 -4.4288187 -4.4288268 -4.4287896 -4.4287128 -4.4286256 -4.4285746][-4.4285192 -4.42859 -4.4286809 -4.4287658 -4.4288192 -4.42882 -4.4288006 -4.4287748 -4.4287691 -4.4287767 -4.4287724 -4.4287186 -4.4286194 -4.4285212 -4.4284678][-4.4284115 -4.4285431 -4.4286814 -4.4287834 -4.4288259 -4.4288006 -4.4287529 -4.4287105 -4.4286938 -4.4286861 -4.428678 -4.4286308 -4.4285417 -4.4284663 -4.4284368][-4.428411 -4.42858 -4.4287233 -4.4288006 -4.4288082 -4.4287457 -4.42866 -4.4286 -4.4285827 -4.42858 -4.4285836 -4.428565 -4.4285197 -4.4284949 -4.4285016][-4.4285808 -4.4287186 -4.4288068 -4.4288177 -4.4287763 -4.4286766 -4.4285483 -4.4284711 -4.4284725 -4.4284973 -4.4285293 -4.4285517 -4.4285574 -4.42857 -4.4285917][-4.4287338 -4.4288106 -4.4288311 -4.4287734 -4.4286819 -4.42855 -4.4283786 -4.4282861 -4.4283447 -4.4284358 -4.4285164 -4.4285865 -4.4286332 -4.4286575 -4.4286623][-4.42877 -4.4288011 -4.4287667 -4.4286542 -4.4285111 -4.4283442 -4.4281297 -4.428031 -4.4281826 -4.4283514 -4.4284778 -4.4285879 -4.4286623 -4.4286823 -4.4286561][-4.4287696 -4.4287581 -4.4286757 -4.4285212 -4.4283471 -4.4281769 -4.4279742 -4.4279051 -4.4281244 -4.4283347 -4.4284639 -4.4285774 -4.4286604 -4.4286675 -4.4286175][-4.4287591 -4.4287205 -4.4286213 -4.4284692 -4.4283185 -4.4282012 -4.4280963 -4.4280906 -4.4282727 -4.428442 -4.4285297 -4.4286137 -4.4286876 -4.4286866 -4.4286327][-4.4287419 -4.4286942 -4.4285927 -4.4284568 -4.4283447 -4.4282866 -4.4282613 -4.4282889 -4.4284139 -4.4285359 -4.4285965 -4.4286547 -4.4287109 -4.4287105 -4.4286704][-4.4287453 -4.4286914 -4.4285951 -4.4284873 -4.4284172 -4.4284067 -4.4284258 -4.4284635 -4.4285436 -4.4286237 -4.4286661 -4.4287024 -4.4287386 -4.4287391 -4.4287109][-4.4288073 -4.428762 -4.4286938 -4.4286294 -4.4285955 -4.4286013 -4.4286284 -4.4286547 -4.4286933 -4.4287348 -4.428762 -4.4287858 -4.4288087 -4.42881 -4.4287963][-4.428884 -4.428854 -4.4288182 -4.4287925 -4.4287777 -4.4287848 -4.4288082 -4.4288225 -4.4288344 -4.4288511 -4.4288683 -4.428885 -4.4288964 -4.428896 -4.4288907][-4.4289417 -4.4289236 -4.4289083 -4.4289045 -4.4289026 -4.42891 -4.428925 -4.4289351 -4.4289365 -4.4289412 -4.4289513 -4.42896 -4.4289632 -4.4289603 -4.428956]]...]
INFO - root - 2017-12-08 05:26:04.699531: step 8910, loss = 2.28, batch loss = 2.23 (37.3 examples/sec; 0.215 sec/batch; 19h:17m:09s remains)
INFO - root - 2017-12-08 05:26:06.946990: step 8920, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 19h:50m:16s remains)
INFO - root - 2017-12-08 05:26:09.177113: step 8930, loss = 2.28, batch loss = 2.23 (36.0 examples/sec; 0.222 sec/batch; 19h:59m:29s remains)
INFO - root - 2017-12-08 05:26:11.443549: step 8940, loss = 2.28, batch loss = 2.23 (36.0 examples/sec; 0.222 sec/batch; 19h:59m:00s remains)
INFO - root - 2017-12-08 05:26:13.696796: step 8950, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 19h:42m:07s remains)
INFO - root - 2017-12-08 05:26:15.948286: step 8960, loss = 2.28, batch loss = 2.23 (35.2 examples/sec; 0.227 sec/batch; 20h:26m:37s remains)
INFO - root - 2017-12-08 05:26:18.220062: step 8970, loss = 2.28, batch loss = 2.23 (34.8 examples/sec; 0.230 sec/batch; 20h:39m:54s remains)
INFO - root - 2017-12-08 05:26:20.458500: step 8980, loss = 2.28, batch loss = 2.23 (35.8 examples/sec; 0.223 sec/batch; 20h:04m:16s remains)
INFO - root - 2017-12-08 05:26:22.687741: step 8990, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 19h:51m:23s remains)
INFO - root - 2017-12-08 05:26:24.951038: step 9000, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 19h:45m:57s remains)
2017-12-08 05:26:25.231025: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.428658 -4.4286718 -4.4286695 -4.4286361 -4.4285975 -4.4286075 -4.4286556 -4.4287219 -4.4287629 -4.4287596 -4.4287219 -4.4286795 -4.4286671 -4.4286823 -4.4286923][-4.4286742 -4.428679 -4.4286718 -4.428648 -4.4286261 -4.4286423 -4.4286885 -4.4287486 -4.4287877 -4.4287839 -4.4287415 -4.4286876 -4.4286575 -4.42866 -4.4286723][-4.4287004 -4.4286828 -4.4286647 -4.4286504 -4.4286427 -4.4286594 -4.428699 -4.4287472 -4.4287896 -4.4287934 -4.42875 -4.4286804 -4.4286284 -4.4286222 -4.4286494][-4.4287066 -4.4286718 -4.4286408 -4.4286289 -4.42863 -4.4286466 -4.4286728 -4.4287 -4.4287496 -4.4287806 -4.4287462 -4.4286652 -4.4285841 -4.4285674 -4.4286118][-4.4286942 -4.4286494 -4.4286027 -4.4285965 -4.428606 -4.4286065 -4.4285979 -4.4285979 -4.4286623 -4.4287343 -4.4287329 -4.42866 -4.4285674 -4.4285407 -4.4285865][-4.4286757 -4.4286227 -4.428565 -4.4285655 -4.4285784 -4.4285526 -4.4284854 -4.42844 -4.4285293 -4.4286523 -4.4286947 -4.4286461 -4.4285555 -4.4285154 -4.4285479][-4.4286747 -4.4286213 -4.4285674 -4.4285622 -4.4285626 -4.42851 -4.428371 -4.4282546 -4.4283533 -4.4285331 -4.4286256 -4.4286146 -4.4285388 -4.4284911 -4.4285083][-4.4286962 -4.4286442 -4.4285917 -4.4285703 -4.4285574 -4.4285035 -4.4283495 -4.4282088 -4.4282832 -4.4284682 -4.4285851 -4.4286032 -4.4285512 -4.4285092 -4.4285135][-4.4287333 -4.42868 -4.4286304 -4.4285984 -4.428587 -4.4285583 -4.4284577 -4.4283452 -4.4283662 -4.4284925 -4.4285979 -4.4286246 -4.4285908 -4.4285541 -4.4285545][-4.4287772 -4.4287372 -4.4286947 -4.4286571 -4.42864 -4.4286222 -4.4285607 -4.4284773 -4.4284697 -4.4285483 -4.4286351 -4.4286666 -4.4286532 -4.4286246 -4.4286251][-4.4288206 -4.4287963 -4.4287677 -4.4287305 -4.4287066 -4.4286914 -4.4286509 -4.4285903 -4.4285665 -4.4286108 -4.4286804 -4.4287229 -4.4287281 -4.4287047 -4.4287019][-4.4288626 -4.4288487 -4.4288278 -4.4287996 -4.4287782 -4.4287705 -4.4287477 -4.4286971 -4.4286575 -4.4286752 -4.4287329 -4.4287806 -4.4287887 -4.4287605 -4.4287477][-4.4288821 -4.4288783 -4.4288692 -4.4288526 -4.42884 -4.4288397 -4.4288316 -4.4287872 -4.4287386 -4.42874 -4.4287786 -4.4288149 -4.4288163 -4.4287829 -4.4287624][-4.4288712 -4.4288764 -4.4288831 -4.428884 -4.4288812 -4.4288883 -4.4288888 -4.4288507 -4.4288015 -4.42879 -4.4288139 -4.4288406 -4.4288411 -4.4288116 -4.4287834][-4.428844 -4.428853 -4.4288721 -4.4288898 -4.4288988 -4.4289093 -4.4289126 -4.4288831 -4.428844 -4.4288335 -4.4288511 -4.428875 -4.4288764 -4.4288483 -4.4288111]]...]
INFO - root - 2017-12-08 05:26:27.433541: step 9010, loss = 2.28, batch loss = 2.23 (35.3 examples/sec; 0.227 sec/batch; 20h:21m:59s remains)
INFO - root - 2017-12-08 05:26:29.680724: step 9020, loss = 2.28, batch loss = 2.23 (36.0 examples/sec; 0.222 sec/batch; 19h:57m:17s remains)
INFO - root - 2017-12-08 05:26:31.935322: step 9030, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.220 sec/batch; 19h:47m:57s remains)
INFO - root - 2017-12-08 05:26:34.176360: step 9040, loss = 2.28, batch loss = 2.23 (35.4 examples/sec; 0.226 sec/batch; 20h:19m:58s remains)
INFO - root - 2017-12-08 05:26:36.397135: step 9050, loss = 2.28, batch loss = 2.23 (34.8 examples/sec; 0.230 sec/batch; 20h:37m:44s remains)
INFO - root - 2017-12-08 05:26:38.637704: step 9060, loss = 2.28, batch loss = 2.23 (35.4 examples/sec; 0.226 sec/batch; 20h:17m:27s remains)
INFO - root - 2017-12-08 05:26:40.884413: step 9070, loss = 2.28, batch loss = 2.23 (36.6 examples/sec; 0.218 sec/batch; 19h:37m:33s remains)
INFO - root - 2017-12-08 05:26:43.109481: step 9080, loss = 2.28, batch loss = 2.23 (36.7 examples/sec; 0.218 sec/batch; 19h:36m:08s remains)
INFO - root - 2017-12-08 05:26:45.343091: step 9090, loss = 2.28, batch loss = 2.23 (37.0 examples/sec; 0.216 sec/batch; 19h:24m:58s remains)
INFO - root - 2017-12-08 05:26:47.579420: step 9100, loss = 2.28, batch loss = 2.23 (35.7 examples/sec; 0.224 sec/batch; 20h:07m:38s remains)
2017-12-08 05:26:47.860972: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289517 -4.4289083 -4.4288321 -4.4287539 -4.4287233 -4.4287276 -4.4287295 -4.4287286 -4.4287472 -4.4287543 -4.428772 -4.4288011 -4.4288397 -4.4288616 -4.4288392][-4.4289532 -4.4289045 -4.4288268 -4.4287534 -4.4287257 -4.4287329 -4.4287448 -4.4287543 -4.4287724 -4.4287729 -4.4287777 -4.4288025 -4.428844 -4.4288692 -4.42885][-4.4289565 -4.4289031 -4.428823 -4.4287496 -4.4287229 -4.4287286 -4.4287472 -4.4287724 -4.4287992 -4.4288039 -4.4287968 -4.4288039 -4.4288321 -4.428854 -4.4288411][-4.42897 -4.4289145 -4.428833 -4.4287491 -4.4287109 -4.4287114 -4.4287319 -4.4287729 -4.428813 -4.4288211 -4.4288068 -4.4287992 -4.4288068 -4.4288182 -4.42881][-4.4289851 -4.4289284 -4.4288411 -4.428741 -4.4286828 -4.428669 -4.4286861 -4.4287357 -4.4287896 -4.428813 -4.428803 -4.4287882 -4.4287848 -4.4287844 -4.4287796][-4.4289985 -4.4289455 -4.4288545 -4.4287391 -4.4286489 -4.4286003 -4.4285941 -4.4286342 -4.4287095 -4.428772 -4.4287949 -4.428792 -4.4287806 -4.4287672 -4.4287615][-4.429008 -4.4289646 -4.4288759 -4.4287519 -4.4286275 -4.4285274 -4.4284692 -4.4284749 -4.4285803 -4.4287014 -4.4287758 -4.4288044 -4.4288 -4.428782 -4.4287705][-4.4290156 -4.4289827 -4.4289036 -4.4287834 -4.4286404 -4.4284935 -4.4283543 -4.4282861 -4.428412 -4.4285941 -4.4287276 -4.4288006 -4.4288216 -4.4288168 -4.4288077][-4.4290214 -4.4289961 -4.4289284 -4.428823 -4.4286876 -4.4285312 -4.428349 -4.4282246 -4.4283323 -4.4285221 -4.4286771 -4.42878 -4.4288297 -4.4288464 -4.4288411][-4.4290242 -4.4290047 -4.4289479 -4.428863 -4.4287553 -4.428637 -4.428494 -4.4283957 -4.4284425 -4.4285479 -4.4286594 -4.4287558 -4.4288158 -4.428844 -4.4288445][-4.4290223 -4.4290032 -4.4289536 -4.4288797 -4.4287915 -4.4287143 -4.4286275 -4.4285789 -4.4285932 -4.4286218 -4.4286795 -4.4287543 -4.428813 -4.4288435 -4.4288473][-4.4290171 -4.4289966 -4.428947 -4.4288707 -4.4287915 -4.4287434 -4.428699 -4.4286814 -4.4286895 -4.4286952 -4.4287267 -4.4287777 -4.428823 -4.4288435 -4.4288397][-4.42901 -4.4289861 -4.4289289 -4.4288464 -4.4287744 -4.4287391 -4.4287105 -4.4287009 -4.4287119 -4.4287324 -4.4287696 -4.4288068 -4.4288316 -4.4288306 -4.4288096][-4.4290042 -4.428978 -4.4289141 -4.4288254 -4.4287543 -4.428719 -4.4286914 -4.4286857 -4.4287109 -4.4287519 -4.4287982 -4.4288263 -4.428834 -4.4288139 -4.4287772][-4.4290018 -4.4289732 -4.4289069 -4.4288211 -4.4287581 -4.4287257 -4.4287024 -4.4287047 -4.4287453 -4.4287882 -4.4288163 -4.42883 -4.4288249 -4.4287953 -4.4287543]]...]
INFO - root - 2017-12-08 05:26:50.098906: step 9110, loss = 2.28, batch loss = 2.23 (32.5 examples/sec; 0.246 sec/batch; 22h:08m:33s remains)
INFO - root - 2017-12-08 05:26:52.333108: step 9120, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 19h:59m:29s remains)
INFO - root - 2017-12-08 05:26:54.572019: step 9130, loss = 2.28, batch loss = 2.23 (35.6 examples/sec; 0.225 sec/batch; 20h:11m:27s remains)
INFO - root - 2017-12-08 05:26:56.801931: step 9140, loss = 2.28, batch loss = 2.23 (35.7 examples/sec; 0.224 sec/batch; 20h:08m:54s remains)
INFO - root - 2017-12-08 05:26:59.025739: step 9150, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 19h:44m:20s remains)
INFO - root - 2017-12-08 05:27:01.243860: step 9160, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 19h:50m:37s remains)
INFO - root - 2017-12-08 05:27:03.493455: step 9170, loss = 2.28, batch loss = 2.23 (33.8 examples/sec; 0.237 sec/batch; 21h:14m:39s remains)
INFO - root - 2017-12-08 05:27:05.730759: step 9180, loss = 2.28, batch loss = 2.23 (36.6 examples/sec; 0.218 sec/batch; 19h:36m:55s remains)
INFO - root - 2017-12-08 05:27:07.941854: step 9190, loss = 2.28, batch loss = 2.23 (36.0 examples/sec; 0.222 sec/batch; 19h:58m:15s remains)
INFO - root - 2017-12-08 05:27:10.183770: step 9200, loss = 2.28, batch loss = 2.23 (34.8 examples/sec; 0.230 sec/batch; 20h:38m:37s remains)
2017-12-08 05:27:10.462074: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289594 -4.4289451 -4.4289265 -4.4289165 -4.4289021 -4.4288845 -4.428874 -4.4288797 -4.4288878 -4.428896 -4.4289002 -4.428906 -4.4289141 -4.428916 -4.4289184][-4.4289489 -4.4289341 -4.4289122 -4.4288864 -4.4288487 -4.4288106 -4.4287949 -4.4288044 -4.4288177 -4.4288306 -4.4288435 -4.4288554 -4.4288664 -4.428863 -4.4288611][-4.4289427 -4.42893 -4.4288993 -4.42885 -4.4287844 -4.4287267 -4.4287028 -4.4287071 -4.4287238 -4.4287481 -4.4287758 -4.4287934 -4.4288044 -4.4288 -4.428792][-4.4289403 -4.4289279 -4.4288883 -4.4288206 -4.4287405 -4.4286747 -4.4286423 -4.4286222 -4.4286304 -4.4286671 -4.4287114 -4.4287357 -4.4287434 -4.4287376 -4.4287252][-4.4289412 -4.4289331 -4.4288874 -4.4288096 -4.42872 -4.4286509 -4.4285879 -4.42851 -4.4284935 -4.428556 -4.4286313 -4.4286714 -4.428678 -4.4286685 -4.4286556][-4.4289412 -4.4289374 -4.4288898 -4.4288054 -4.42871 -4.4286284 -4.4285207 -4.4283648 -4.4283042 -4.4284043 -4.4285254 -4.4285908 -4.4286094 -4.428607 -4.4285979][-4.4289312 -4.4289279 -4.4288788 -4.4287839 -4.4286766 -4.4285679 -4.4284096 -4.4281597 -4.4280252 -4.4281812 -4.4283857 -4.4284973 -4.4285383 -4.4285502 -4.4285526][-4.428915 -4.4289131 -4.4288645 -4.42876 -4.428638 -4.4285049 -4.4282985 -4.4279804 -4.4277954 -4.4280257 -4.4283113 -4.4284511 -4.4285 -4.4285197 -4.42853][-4.4288993 -4.428895 -4.4288454 -4.4287434 -4.4286265 -4.4284987 -4.4283113 -4.4280643 -4.4279537 -4.428154 -4.4283848 -4.4284925 -4.42852 -4.4285264 -4.4285374][-4.4288869 -4.4288812 -4.4288425 -4.4287653 -4.4286804 -4.428587 -4.4284477 -4.4282918 -4.4282408 -4.4283628 -4.4284954 -4.4285574 -4.4285679 -4.4285626 -4.428565][-4.428884 -4.42888 -4.428854 -4.4288096 -4.428761 -4.4286909 -4.428575 -4.4284625 -4.4284425 -4.4285059 -4.4285645 -4.4285927 -4.428607 -4.4286075 -4.4286094][-4.428895 -4.4288888 -4.4288735 -4.4288559 -4.4288187 -4.4287529 -4.428659 -4.4285822 -4.4285812 -4.4286051 -4.4286175 -4.4286318 -4.4286542 -4.4286714 -4.4286776][-4.428916 -4.4289074 -4.4288988 -4.4288859 -4.4288583 -4.4288082 -4.4287457 -4.4287024 -4.4287071 -4.4287076 -4.4286909 -4.4286928 -4.42872 -4.4287438 -4.428751][-4.4289465 -4.42894 -4.4289322 -4.4289246 -4.4289107 -4.4288793 -4.4288468 -4.4288259 -4.4288273 -4.4288182 -4.4287963 -4.428791 -4.4288096 -4.4288297 -4.4288297][-4.4289713 -4.428966 -4.4289589 -4.4289546 -4.4289465 -4.4289284 -4.428915 -4.4289117 -4.4289174 -4.4289088 -4.4288888 -4.4288793 -4.4288888 -4.4289031 -4.4289]]...]
INFO - root - 2017-12-08 05:27:12.695005: step 9210, loss = 2.28, batch loss = 2.23 (35.1 examples/sec; 0.228 sec/batch; 20h:28m:07s remains)
INFO - root - 2017-12-08 05:27:15.147496: step 9220, loss = 2.28, batch loss = 2.23 (34.8 examples/sec; 0.230 sec/batch; 20h:38m:36s remains)
INFO - root - 2017-12-08 05:27:17.397504: step 9230, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.222 sec/batch; 19h:54m:16s remains)
INFO - root - 2017-12-08 05:27:19.653589: step 9240, loss = 2.28, batch loss = 2.23 (34.9 examples/sec; 0.229 sec/batch; 20h:34m:32s remains)
INFO - root - 2017-12-08 05:27:21.867729: step 9250, loss = 2.28, batch loss = 2.23 (34.5 examples/sec; 0.232 sec/batch; 20h:48m:47s remains)
INFO - root - 2017-12-08 05:27:24.116142: step 9260, loss = 2.28, batch loss = 2.23 (36.0 examples/sec; 0.222 sec/batch; 19h:56m:59s remains)
INFO - root - 2017-12-08 05:27:26.353002: step 9270, loss = 2.28, batch loss = 2.23 (37.8 examples/sec; 0.212 sec/batch; 18h:59m:40s remains)
INFO - root - 2017-12-08 05:27:28.588771: step 9280, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.221 sec/batch; 19h:47m:50s remains)
INFO - root - 2017-12-08 05:27:30.818871: step 9290, loss = 2.28, batch loss = 2.23 (37.4 examples/sec; 0.214 sec/batch; 19h:12m:22s remains)
INFO - root - 2017-12-08 05:27:33.058705: step 9300, loss = 2.28, batch loss = 2.23 (34.5 examples/sec; 0.232 sec/batch; 20h:49m:58s remains)
2017-12-08 05:27:33.350680: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.42872 -4.4286861 -4.4286561 -4.4286289 -4.42862 -4.4286289 -4.42865 -4.4286938 -4.4287367 -4.4287539 -4.4287443 -4.4287095 -4.4286675 -4.4286451 -4.4286752][-4.4286714 -4.4286323 -4.4285946 -4.4285603 -4.4285541 -4.4285736 -4.4286017 -4.4286489 -4.4286938 -4.428709 -4.4287047 -4.4286842 -4.428658 -4.4286489 -4.4286866][-4.4286027 -4.4285626 -4.428525 -4.4284911 -4.4284954 -4.4285264 -4.4285579 -4.4286 -4.4286423 -4.4286594 -4.4286633 -4.4286556 -4.4286427 -4.4286461 -4.4286909][-4.4285493 -4.4285083 -4.4284739 -4.428452 -4.4284697 -4.4285045 -4.4285331 -4.4285693 -4.4286122 -4.4286361 -4.4286466 -4.4286466 -4.4286404 -4.428647 -4.4286914][-4.4285336 -4.4284987 -4.4284735 -4.4284625 -4.4284816 -4.4285073 -4.4285254 -4.4285526 -4.4285946 -4.4286232 -4.42864 -4.42865 -4.4286504 -4.4286547 -4.4286938][-4.428524 -4.4284911 -4.4284682 -4.4284563 -4.4284625 -4.4284768 -4.428484 -4.4285083 -4.4285569 -4.4285932 -4.4286175 -4.4286394 -4.4286485 -4.4286537 -4.4286923][-4.4284792 -4.4284372 -4.4284086 -4.42839 -4.428391 -4.4284086 -4.4284239 -4.4284592 -4.4285183 -4.4285626 -4.4285922 -4.4286184 -4.4286318 -4.4286423 -4.4286876][-4.4284244 -4.4283757 -4.4283352 -4.4283094 -4.4283128 -4.428339 -4.4283733 -4.4284286 -4.4284973 -4.4285479 -4.4285793 -4.4286027 -4.428618 -4.4286351 -4.428688][-4.4284039 -4.4283657 -4.4283261 -4.4282932 -4.4282928 -4.4283214 -4.4283724 -4.4284453 -4.42852 -4.4285736 -4.4286046 -4.4286194 -4.42863 -4.4286466 -4.428699][-4.4284358 -4.4284115 -4.4283795 -4.4283452 -4.4283419 -4.4283676 -4.4284215 -4.4285007 -4.428575 -4.4286318 -4.4286637 -4.4286709 -4.4286704 -4.4286766 -4.4287181][-4.4284759 -4.4284577 -4.4284244 -4.4283876 -4.4283824 -4.4284043 -4.4284549 -4.4285326 -4.428607 -4.4286714 -4.4287114 -4.4287167 -4.4287071 -4.4287062 -4.4287372][-4.4285121 -4.4285007 -4.428473 -4.4284396 -4.4284353 -4.428453 -4.4284935 -4.42856 -4.4286284 -4.4286928 -4.4287353 -4.4287395 -4.4287243 -4.4287205 -4.4287477][-4.4285212 -4.4285154 -4.4284987 -4.4284763 -4.4284725 -4.4284787 -4.4284987 -4.4285469 -4.4286079 -4.428669 -4.4287138 -4.428721 -4.4287081 -4.4287076 -4.4287381][-4.4285078 -4.4284992 -4.4284868 -4.4284711 -4.4284644 -4.4284549 -4.4284539 -4.4284868 -4.4285407 -4.4285989 -4.4286513 -4.4286704 -4.4286675 -4.4286757 -4.4287143][-4.4285126 -4.4284968 -4.42848 -4.4284644 -4.4284554 -4.4284439 -4.4284358 -4.4284582 -4.4285045 -4.4285579 -4.4286118 -4.4286361 -4.4286385 -4.428648 -4.4286895]]...]
INFO - root - 2017-12-08 05:27:35.578255: step 9310, loss = 2.28, batch loss = 2.23 (35.0 examples/sec; 0.228 sec/batch; 20h:29m:58s remains)
INFO - root - 2017-12-08 05:27:37.790747: step 9320, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 19h:39m:41s remains)
INFO - root - 2017-12-08 05:27:40.056556: step 9330, loss = 2.28, batch loss = 2.23 (35.7 examples/sec; 0.224 sec/batch; 20h:07m:21s remains)
INFO - root - 2017-12-08 05:27:42.275468: step 9340, loss = 2.28, batch loss = 2.23 (36.6 examples/sec; 0.219 sec/batch; 19h:38m:11s remains)
INFO - root - 2017-12-08 05:27:44.517349: step 9350, loss = 2.28, batch loss = 2.23 (34.9 examples/sec; 0.229 sec/batch; 20h:33m:40s remains)
INFO - root - 2017-12-08 05:27:46.751630: step 9360, loss = 2.28, batch loss = 2.23 (36.8 examples/sec; 0.217 sec/batch; 19h:30m:28s remains)
INFO - root - 2017-12-08 05:27:48.973538: step 9370, loss = 2.28, batch loss = 2.23 (37.2 examples/sec; 0.215 sec/batch; 19h:17m:34s remains)
INFO - root - 2017-12-08 05:27:51.255797: step 9380, loss = 2.28, batch loss = 2.23 (36.0 examples/sec; 0.222 sec/batch; 19h:55m:44s remains)
INFO - root - 2017-12-08 05:27:53.512591: step 9390, loss = 2.28, batch loss = 2.23 (35.8 examples/sec; 0.223 sec/batch; 20h:01m:52s remains)
INFO - root - 2017-12-08 05:27:55.773510: step 9400, loss = 2.28, batch loss = 2.23 (34.8 examples/sec; 0.230 sec/batch; 20h:37m:59s remains)
2017-12-08 05:27:56.064868: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4287591 -4.4287572 -4.4287596 -4.4287682 -4.4287772 -4.4287896 -4.4288116 -4.4288144 -4.4287791 -4.4286761 -4.4285812 -4.4285827 -4.4286327 -4.4286904 -4.4287333][-4.4287715 -4.4287806 -4.4288063 -4.4288239 -4.42882 -4.4288182 -4.4288349 -4.4288545 -4.4288321 -4.42872 -4.4286108 -4.4285922 -4.4286413 -4.4286923 -4.4287224][-4.4287772 -4.428793 -4.4288325 -4.4288497 -4.428833 -4.4288235 -4.4288416 -4.4288678 -4.4288487 -4.4287324 -4.4286094 -4.4285808 -4.4286342 -4.4286885 -4.4287167][-4.4288116 -4.4288154 -4.4288464 -4.4288492 -4.4288273 -4.4288225 -4.4288416 -4.4288635 -4.4288406 -4.4287305 -4.4286051 -4.4285855 -4.4286437 -4.4286942 -4.4287133][-4.42884 -4.4288325 -4.4288392 -4.4288182 -4.4287977 -4.4288058 -4.4288235 -4.4288235 -4.4287915 -4.4286861 -4.4285688 -4.4285793 -4.4286547 -4.4287024 -4.4287148][-4.4288459 -4.428822 -4.4287953 -4.4287462 -4.4287257 -4.4287477 -4.4287639 -4.4287496 -4.42871 -4.4286156 -4.4285169 -4.4285536 -4.4286342 -4.4286704 -4.4286833][-4.428792 -4.4287467 -4.4286785 -4.4285827 -4.4285426 -4.4285707 -4.4285836 -4.4285455 -4.4285035 -4.4284277 -4.4283614 -4.4284387 -4.4285417 -4.428587 -4.4286242][-4.4287043 -4.4286456 -4.4285417 -4.4284053 -4.4283442 -4.4283776 -4.4283876 -4.4283214 -4.4282608 -4.428206 -4.4281859 -4.4283071 -4.4284439 -4.4285126 -4.4285779][-4.4286256 -4.4285655 -4.4284511 -4.4283113 -4.42825 -4.4282918 -4.4283128 -4.4282384 -4.4281759 -4.4281592 -4.4281759 -4.4282932 -4.4284272 -4.4285016 -4.4285769][-4.4285874 -4.428546 -4.4284544 -4.4283328 -4.4282794 -4.4283333 -4.428371 -4.4282904 -4.428225 -4.4282475 -4.4282994 -4.4284048 -4.4285216 -4.42859 -4.4286542][-4.4286113 -4.4285865 -4.4285307 -4.4284463 -4.4283977 -4.4284348 -4.4284544 -4.4283738 -4.4283242 -4.4283738 -4.4284515 -4.4285464 -4.4286547 -4.4287233 -4.4287753][-4.4286728 -4.4286685 -4.42865 -4.4286113 -4.4285789 -4.4285879 -4.4285817 -4.42852 -4.4284921 -4.4285388 -4.4286075 -4.428688 -4.4287763 -4.4288378 -4.4288764][-4.4287519 -4.4287639 -4.428771 -4.4287663 -4.4287539 -4.4287486 -4.4287362 -4.4286923 -4.4286704 -4.4286866 -4.428719 -4.4287748 -4.4288435 -4.428894 -4.4289203][-4.428834 -4.4288425 -4.4288549 -4.4288616 -4.4288621 -4.4288573 -4.4288492 -4.428822 -4.4287982 -4.4287786 -4.4287691 -4.4288034 -4.4288564 -4.4288969 -4.4289212][-4.4288888 -4.428885 -4.4288917 -4.428895 -4.4288974 -4.4288936 -4.4288869 -4.4288683 -4.42884 -4.4287906 -4.4287505 -4.4287763 -4.4288263 -4.4288669 -4.4288974]]...]
INFO - root - 2017-12-08 05:27:58.278803: step 9410, loss = 2.28, batch loss = 2.23 (35.7 examples/sec; 0.224 sec/batch; 20h:06m:50s remains)
INFO - root - 2017-12-08 05:28:00.527881: step 9420, loss = 2.28, batch loss = 2.23 (35.4 examples/sec; 0.226 sec/batch; 20h:16m:13s remains)
INFO - root - 2017-12-08 05:28:02.753296: step 9430, loss = 2.28, batch loss = 2.23 (35.4 examples/sec; 0.226 sec/batch; 20h:18m:09s remains)
INFO - root - 2017-12-08 05:28:04.991311: step 9440, loss = 2.28, batch loss = 2.23 (35.4 examples/sec; 0.226 sec/batch; 20h:17m:27s remains)
INFO - root - 2017-12-08 05:28:07.233962: step 9450, loss = 2.28, batch loss = 2.23 (35.1 examples/sec; 0.228 sec/batch; 20h:27m:15s remains)
INFO - root - 2017-12-08 05:28:09.548824: step 9460, loss = 2.28, batch loss = 2.23 (35.3 examples/sec; 0.227 sec/batch; 20h:21m:53s remains)
INFO - root - 2017-12-08 05:28:11.773502: step 9470, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.225 sec/batch; 20h:11m:57s remains)
INFO - root - 2017-12-08 05:28:14.006934: step 9480, loss = 2.28, batch loss = 2.23 (34.6 examples/sec; 0.231 sec/batch; 20h:44m:55s remains)
INFO - root - 2017-12-08 05:28:16.228264: step 9490, loss = 2.28, batch loss = 2.23 (37.0 examples/sec; 0.216 sec/batch; 19h:24m:30s remains)
INFO - root - 2017-12-08 05:28:18.488465: step 9500, loss = 2.28, batch loss = 2.23 (34.8 examples/sec; 0.230 sec/batch; 20h:37m:14s remains)
2017-12-08 05:28:18.772830: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4287529 -4.4288192 -4.4288583 -4.4288073 -4.428658 -4.42849 -4.4284182 -4.428472 -4.4286494 -4.4288096 -4.4288793 -4.4289026 -4.4289126 -4.4289174 -4.4289064][-4.4287891 -4.4288363 -4.4288597 -4.4288039 -4.4286504 -4.4284816 -4.4284134 -4.4284635 -4.4286203 -4.4287753 -4.4288578 -4.4289017 -4.4289255 -4.4289355 -4.4289303][-4.42881 -4.4288306 -4.4288363 -4.42878 -4.428637 -4.4284897 -4.4284258 -4.42847 -4.4286027 -4.4287376 -4.4288168 -4.4288726 -4.4289165 -4.4289374 -4.4289451][-4.4287972 -4.4287992 -4.4287791 -4.428709 -4.4285851 -4.4284744 -4.4284229 -4.4284616 -4.42857 -4.4286785 -4.4287543 -4.428823 -4.4288874 -4.4289236 -4.4289436][-4.4287548 -4.4287457 -4.4286971 -4.4286008 -4.4284873 -4.4284072 -4.428369 -4.4284072 -4.428504 -4.4286003 -4.4286842 -4.4287658 -4.4288406 -4.4288898 -4.4289155][-4.4287062 -4.4286919 -4.4286261 -4.4285159 -4.4284182 -4.4283705 -4.42835 -4.4283786 -4.4284549 -4.4285388 -4.4286323 -4.4287219 -4.4288039 -4.4288597 -4.4288912][-4.4286857 -4.4286723 -4.4286132 -4.4285126 -4.428422 -4.428391 -4.4283891 -4.4284148 -4.4284744 -4.4285483 -4.4286413 -4.4287329 -4.4288087 -4.4288597 -4.4288917][-4.4287086 -4.4286995 -4.4286618 -4.4285841 -4.4284935 -4.428463 -4.4284706 -4.4284935 -4.428544 -4.4286113 -4.4286938 -4.4287853 -4.42885 -4.4288816 -4.4289012][-4.4287624 -4.4287648 -4.4287457 -4.4286909 -4.4286151 -4.4285908 -4.4286046 -4.42862 -4.4286494 -4.4286995 -4.4287643 -4.4288445 -4.4288988 -4.4289203 -4.428926][-4.4288173 -4.428833 -4.4288249 -4.428781 -4.4287195 -4.4287114 -4.4287353 -4.4287434 -4.4287567 -4.4287896 -4.4288368 -4.4288931 -4.4289317 -4.428947 -4.4289494][-4.4288416 -4.42886 -4.4288588 -4.4288249 -4.4287806 -4.4287829 -4.4288192 -4.4288363 -4.428843 -4.4288626 -4.4288874 -4.428916 -4.4289403 -4.4289517 -4.428957][-4.4288459 -4.4288626 -4.4288664 -4.4288459 -4.4288187 -4.4288263 -4.4288621 -4.4288797 -4.4288793 -4.4288883 -4.4289041 -4.4289188 -4.4289336 -4.4289441 -4.42895][-4.42886 -4.4288731 -4.4288754 -4.4288573 -4.428833 -4.4288363 -4.4288611 -4.4288764 -4.4288797 -4.428885 -4.4289002 -4.4289145 -4.4289308 -4.4289422 -4.4289494][-4.4288917 -4.4289 -4.4288993 -4.4288855 -4.4288654 -4.428863 -4.4288754 -4.4288869 -4.4288926 -4.4289012 -4.4289131 -4.4289227 -4.4289355 -4.428946 -4.4289532][-4.428916 -4.4289207 -4.4289188 -4.4289093 -4.4288955 -4.4288893 -4.4288931 -4.4289007 -4.4289088 -4.4289193 -4.42893 -4.4289374 -4.4289436 -4.4289508 -4.428956]]...]
INFO - root - 2017-12-08 05:28:20.978576: step 9510, loss = 2.28, batch loss = 2.23 (37.3 examples/sec; 0.214 sec/batch; 19h:13m:29s remains)
INFO - root - 2017-12-08 05:28:23.228139: step 9520, loss = 2.28, batch loss = 2.23 (34.7 examples/sec; 0.230 sec/batch; 20h:40m:06s remains)
INFO - root - 2017-12-08 05:28:25.459860: step 9530, loss = 2.28, batch loss = 2.23 (36.6 examples/sec; 0.219 sec/batch; 19h:37m:58s remains)
INFO - root - 2017-12-08 05:28:27.685006: step 9540, loss = 2.28, batch loss = 2.23 (37.1 examples/sec; 0.216 sec/batch; 19h:21m:34s remains)
INFO - root - 2017-12-08 05:28:29.927403: step 9550, loss = 2.28, batch loss = 2.23 (34.9 examples/sec; 0.229 sec/batch; 20h:32m:56s remains)
INFO - root - 2017-12-08 05:28:32.193552: step 9560, loss = 2.28, batch loss = 2.23 (35.6 examples/sec; 0.225 sec/batch; 20h:10m:55s remains)
INFO - root - 2017-12-08 05:28:34.416586: step 9570, loss = 2.28, batch loss = 2.23 (36.7 examples/sec; 0.218 sec/batch; 19h:32m:04s remains)
INFO - root - 2017-12-08 05:28:36.660954: step 9580, loss = 2.28, batch loss = 2.23 (35.7 examples/sec; 0.224 sec/batch; 20h:05m:43s remains)
INFO - root - 2017-12-08 05:28:38.907243: step 9590, loss = 2.28, batch loss = 2.23 (37.1 examples/sec; 0.216 sec/batch; 19h:20m:06s remains)
INFO - root - 2017-12-08 05:28:41.145091: step 9600, loss = 2.28, batch loss = 2.23 (35.4 examples/sec; 0.226 sec/batch; 20h:14m:55s remains)
2017-12-08 05:28:41.458923: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289241 -4.4289255 -4.42893 -4.4289274 -4.4289041 -4.4288616 -4.4288034 -4.4287524 -4.4287305 -4.4287105 -4.4286857 -4.4286528 -4.428627 -4.4285679 -4.4285021][-4.4288921 -4.4288931 -4.4289055 -4.4289103 -4.4288874 -4.4288378 -4.4287724 -4.4287205 -4.428709 -4.4286976 -4.4286804 -4.4286551 -4.4286389 -4.4285755 -4.4284968][-4.4288468 -4.428853 -4.4288759 -4.4288907 -4.4288712 -4.4288216 -4.42876 -4.428721 -4.4287252 -4.4287252 -4.4287114 -4.4286995 -4.4287028 -4.428658 -4.4285913][-4.4287629 -4.4287767 -4.4288144 -4.4288478 -4.4288406 -4.4287992 -4.4287529 -4.4287348 -4.4287605 -4.4287729 -4.4287634 -4.4287624 -4.428782 -4.4287624 -4.42872][-4.4286637 -4.4286766 -4.4287267 -4.428782 -4.4287934 -4.4287667 -4.4287391 -4.4287343 -4.4287696 -4.4287949 -4.4287953 -4.4288073 -4.4288363 -4.4288387 -4.4288211][-4.4285727 -4.4285822 -4.4286389 -4.4287062 -4.4287348 -4.4287195 -4.4287009 -4.4287004 -4.4287357 -4.4287729 -4.42879 -4.4288163 -4.4288568 -4.42888 -4.4288807][-4.4285417 -4.4285522 -4.4285965 -4.4286447 -4.4286675 -4.4286532 -4.4286337 -4.42863 -4.4286613 -4.4287114 -4.428751 -4.4287953 -4.4288487 -4.428884 -4.4288974][-4.4285789 -4.428587 -4.4285979 -4.4286041 -4.428597 -4.4285645 -4.4285326 -4.4285197 -4.4285488 -4.4286175 -4.4286776 -4.4287429 -4.4288182 -4.4288654 -4.4288936][-4.4286137 -4.4286103 -4.4285884 -4.4285493 -4.428494 -4.4284277 -4.4283781 -4.4283586 -4.4283915 -4.4284906 -4.4285836 -4.4286671 -4.4287634 -4.4288268 -4.4288707][-4.4286551 -4.4286351 -4.4285893 -4.4285183 -4.4284325 -4.42835 -4.4283066 -4.4283004 -4.4283352 -4.4284468 -4.4285617 -4.4286437 -4.4287295 -4.428791 -4.4288411][-4.4287028 -4.4286766 -4.4286184 -4.4285426 -4.42847 -4.4284124 -4.4283962 -4.4284105 -4.4284358 -4.4285226 -4.4286265 -4.4286909 -4.4287453 -4.42879 -4.428833][-4.4287367 -4.4287086 -4.4286547 -4.4285965 -4.4285564 -4.4285364 -4.4285398 -4.4285593 -4.4285817 -4.4286385 -4.4287219 -4.4287705 -4.4287972 -4.4288182 -4.4288449][-4.4287786 -4.4287448 -4.4287086 -4.42868 -4.4286733 -4.4286866 -4.4287076 -4.4287271 -4.4287443 -4.4287767 -4.4288297 -4.4288588 -4.4288654 -4.4288692 -4.428874][-4.428844 -4.4288096 -4.4287987 -4.4288 -4.428813 -4.4288397 -4.4288669 -4.4288816 -4.4288907 -4.4289036 -4.4289255 -4.4289308 -4.4289227 -4.4289117 -4.4288969][-4.4289122 -4.4288855 -4.4288912 -4.4289069 -4.4289279 -4.4289541 -4.4289694 -4.4289713 -4.428967 -4.4289627 -4.4289665 -4.4289613 -4.4289489 -4.4289331 -4.428906]]...]
INFO - root - 2017-12-08 05:28:43.682262: step 9610, loss = 2.28, batch loss = 2.23 (37.3 examples/sec; 0.215 sec/batch; 19h:15m:36s remains)
INFO - root - 2017-12-08 05:28:45.910535: step 9620, loss = 2.28, batch loss = 2.23 (37.0 examples/sec; 0.216 sec/batch; 19h:23m:25s remains)
INFO - root - 2017-12-08 05:28:48.130634: step 9630, loss = 2.28, batch loss = 2.23 (34.9 examples/sec; 0.229 sec/batch; 20h:34m:08s remains)
INFO - root - 2017-12-08 05:28:50.376888: step 9640, loss = 2.28, batch loss = 2.23 (34.3 examples/sec; 0.233 sec/batch; 20h:56m:17s remains)
INFO - root - 2017-12-08 05:28:52.633527: step 9650, loss = 2.28, batch loss = 2.23 (34.0 examples/sec; 0.235 sec/batch; 21h:04m:51s remains)
INFO - root - 2017-12-08 05:28:54.884286: step 9660, loss = 2.28, batch loss = 2.23 (36.6 examples/sec; 0.218 sec/batch; 19h:35m:20s remains)
INFO - root - 2017-12-08 05:28:57.116181: step 9670, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.225 sec/batch; 20h:12m:52s remains)
INFO - root - 2017-12-08 05:28:59.399389: step 9680, loss = 2.28, batch loss = 2.23 (33.3 examples/sec; 0.240 sec/batch; 21h:32m:04s remains)
INFO - root - 2017-12-08 05:29:01.624110: step 9690, loss = 2.28, batch loss = 2.23 (35.8 examples/sec; 0.223 sec/batch; 20h:01m:09s remains)
INFO - root - 2017-12-08 05:29:03.858738: step 9700, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 19h:58m:32s remains)
2017-12-08 05:29:04.144839: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288173 -4.4288359 -4.4288421 -4.4288316 -4.4288144 -4.4287753 -4.4287481 -4.428772 -4.4288058 -4.428833 -4.4288645 -4.4288554 -4.4288015 -4.42876 -4.428772][-4.428802 -4.4288206 -4.4288282 -4.4288139 -4.4288006 -4.4287744 -4.4287543 -4.4287868 -4.4288368 -4.4288678 -4.4288964 -4.4288855 -4.4288163 -4.428751 -4.4287519][-4.4287882 -4.4288006 -4.4287992 -4.4287739 -4.4287572 -4.4287405 -4.4287314 -4.4287834 -4.428854 -4.4288945 -4.4289188 -4.4289079 -4.428834 -4.42876 -4.4287481][-4.4287882 -4.4287963 -4.4287868 -4.4287539 -4.4287205 -4.4286942 -4.4286857 -4.42875 -4.4288387 -4.4288883 -4.4289083 -4.4289007 -4.4288387 -4.4287648 -4.42874][-4.4288135 -4.4288163 -4.4287996 -4.4287529 -4.4286928 -4.4286389 -4.4286213 -4.428699 -4.42881 -4.4288759 -4.4288988 -4.4288969 -4.4288497 -4.4287791 -4.4287429][-4.4288483 -4.4288373 -4.4288058 -4.4287419 -4.4286551 -4.4285612 -4.4285216 -4.42862 -4.4287691 -4.4288654 -4.4289002 -4.4289036 -4.4288673 -4.428803 -4.4287591][-4.4288473 -4.42883 -4.4287872 -4.4287124 -4.4285979 -4.428453 -4.428359 -4.4284816 -4.4286962 -4.4288316 -4.4288859 -4.4288955 -4.4288683 -4.42881 -4.4287653][-4.4288015 -4.4287868 -4.4287496 -4.4286776 -4.4285493 -4.4283586 -4.4281836 -4.4283037 -4.4285789 -4.4287591 -4.4288392 -4.4288559 -4.4288383 -4.42879 -4.4287505][-4.4287634 -4.4287534 -4.4287362 -4.4286914 -4.4285893 -4.428421 -4.428237 -4.4282908 -4.4285378 -4.4287176 -4.4288011 -4.4288263 -4.4288197 -4.4287844 -4.4287481][-4.4287572 -4.4287624 -4.4287658 -4.4287548 -4.4287 -4.428587 -4.4284492 -4.428442 -4.42859 -4.4287214 -4.4287858 -4.4288135 -4.4288144 -4.4287953 -4.42877][-4.428772 -4.4287925 -4.4288106 -4.4288187 -4.4287896 -4.4287105 -4.4286041 -4.4285765 -4.4286509 -4.42873 -4.4287782 -4.4288054 -4.4288054 -4.4287968 -4.4287829][-4.428792 -4.4288144 -4.4288335 -4.4288497 -4.4288316 -4.42876 -4.4286642 -4.4286261 -4.4286656 -4.4287214 -4.4287634 -4.4287968 -4.4288015 -4.4287949 -4.42878][-4.4288263 -4.4288416 -4.4288559 -4.4288735 -4.4288621 -4.4287987 -4.4287076 -4.4286609 -4.428679 -4.4287205 -4.4287629 -4.4287987 -4.4288054 -4.4287972 -4.4287796][-4.4288654 -4.428874 -4.4288812 -4.4288936 -4.4288888 -4.4288411 -4.42877 -4.42873 -4.4287333 -4.4287591 -4.4287872 -4.4288116 -4.42882 -4.428812 -4.4288073][-4.4289045 -4.428906 -4.42891 -4.4289155 -4.4289174 -4.4288893 -4.4288445 -4.4288187 -4.428812 -4.42882 -4.4288325 -4.4288449 -4.4288526 -4.4288507 -4.4288583]]...]
INFO - root - 2017-12-08 05:29:06.373779: step 9710, loss = 2.28, batch loss = 2.23 (35.2 examples/sec; 0.227 sec/batch; 20h:22m:21s remains)
INFO - root - 2017-12-08 05:29:08.616832: step 9720, loss = 2.28, batch loss = 2.23 (35.8 examples/sec; 0.224 sec/batch; 20h:03m:33s remains)
INFO - root - 2017-12-08 05:29:10.831402: step 9730, loss = 2.28, batch loss = 2.23 (36.7 examples/sec; 0.218 sec/batch; 19h:32m:19s remains)
INFO - root - 2017-12-08 05:29:13.093843: step 9740, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.221 sec/batch; 19h:51m:01s remains)
INFO - root - 2017-12-08 05:29:15.330184: step 9750, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.225 sec/batch; 20h:12m:58s remains)
INFO - root - 2017-12-08 05:29:17.585904: step 9760, loss = 2.28, batch loss = 2.23 (35.7 examples/sec; 0.224 sec/batch; 20h:05m:42s remains)
INFO - root - 2017-12-08 05:29:19.853997: step 9770, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 19h:47m:34s remains)
INFO - root - 2017-12-08 05:29:22.105156: step 9780, loss = 2.28, batch loss = 2.23 (36.7 examples/sec; 0.218 sec/batch; 19h:32m:56s remains)
INFO - root - 2017-12-08 05:29:24.358570: step 9790, loss = 2.28, batch loss = 2.23 (35.3 examples/sec; 0.226 sec/batch; 20h:17m:38s remains)
INFO - root - 2017-12-08 05:29:26.618826: step 9800, loss = 2.28, batch loss = 2.23 (35.3 examples/sec; 0.227 sec/batch; 20h:19m:00s remains)
2017-12-08 05:29:26.920096: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289408 -4.4289393 -4.4289355 -4.4289312 -4.4289284 -4.4289284 -4.4289317 -4.428937 -4.4289427 -4.4289451 -4.4289374 -4.4289212 -4.4289141 -4.428915 -4.42892][-4.4289107 -4.428905 -4.4288974 -4.4288874 -4.4288807 -4.4288831 -4.4288926 -4.4289064 -4.4289217 -4.4289312 -4.4289255 -4.4289093 -4.4289045 -4.4289031 -4.4288993][-4.4288955 -4.4288864 -4.4288712 -4.4288535 -4.428843 -4.4288478 -4.428865 -4.4288893 -4.4289165 -4.4289365 -4.428937 -4.4289293 -4.4289312 -4.4289274 -4.4289103][-4.4288549 -4.428844 -4.42882 -4.4287906 -4.4287724 -4.4287758 -4.4287977 -4.4288316 -4.4288707 -4.4289021 -4.4289131 -4.4289184 -4.42893 -4.4289308 -4.4289103][-4.4287615 -4.4287429 -4.4287062 -4.428659 -4.4286237 -4.4286184 -4.4286423 -4.4286838 -4.4287338 -4.4287772 -4.4288034 -4.4288263 -4.428854 -4.42887 -4.4288597][-4.4286571 -4.4286375 -4.4285927 -4.4285269 -4.4284673 -4.428442 -4.428452 -4.428484 -4.4285283 -4.4285746 -4.428617 -4.4286656 -4.4287176 -4.4287567 -4.4287705][-4.4286809 -4.4286652 -4.4286232 -4.428556 -4.4284854 -4.4284415 -4.4284244 -4.4284267 -4.4284444 -4.4284759 -4.4285269 -4.4285932 -4.428658 -4.4287052 -4.4287324][-4.4288039 -4.428793 -4.4287663 -4.428721 -4.428668 -4.4286294 -4.4285994 -4.4285769 -4.4285655 -4.4285684 -4.4286017 -4.4286561 -4.4287114 -4.4287462 -4.4287648][-4.4288621 -4.4288564 -4.4288445 -4.428822 -4.4287944 -4.4287748 -4.4287519 -4.4287243 -4.4287004 -4.4286814 -4.4286861 -4.4287157 -4.4287558 -4.428781 -4.428791][-4.4288611 -4.428863 -4.428864 -4.4288583 -4.4288478 -4.4288406 -4.4288268 -4.4288006 -4.4287744 -4.4287462 -4.4287286 -4.4287333 -4.428761 -4.4287825 -4.4287872][-4.4288568 -4.4288588 -4.4288626 -4.4288611 -4.4288559 -4.4288545 -4.4288473 -4.4288263 -4.4288068 -4.4287848 -4.428761 -4.4287515 -4.4287658 -4.4287806 -4.428781][-4.4288683 -4.428863 -4.42886 -4.4288487 -4.4288349 -4.4288278 -4.428823 -4.4288111 -4.428803 -4.4287958 -4.428781 -4.4287696 -4.428771 -4.4287744 -4.4287715][-4.4288931 -4.4288812 -4.42887 -4.4288478 -4.4288216 -4.4287987 -4.4287858 -4.4287767 -4.4287786 -4.4287839 -4.4287772 -4.4287672 -4.4287586 -4.4287519 -4.4287472][-4.4289269 -4.428915 -4.4289017 -4.4288754 -4.4288397 -4.4288006 -4.4287715 -4.4287553 -4.4287572 -4.4287663 -4.4287663 -4.4287596 -4.4287462 -4.428731 -4.4287233][-4.4289575 -4.4289517 -4.4289436 -4.4289227 -4.4288874 -4.4288411 -4.4287992 -4.4287705 -4.4287648 -4.4287705 -4.428771 -4.4287663 -4.4287496 -4.42873 -4.428721]]...]
INFO - root - 2017-12-08 05:29:29.139343: step 9810, loss = 2.28, batch loss = 2.23 (35.7 examples/sec; 0.224 sec/batch; 20h:04m:23s remains)
INFO - root - 2017-12-08 05:29:31.363038: step 9820, loss = 2.28, batch loss = 2.23 (35.3 examples/sec; 0.227 sec/batch; 20h:18m:55s remains)
INFO - root - 2017-12-08 05:29:33.591468: step 9830, loss = 2.28, batch loss = 2.23 (35.8 examples/sec; 0.223 sec/batch; 20h:00m:32s remains)
INFO - root - 2017-12-08 05:29:35.826362: step 9840, loss = 2.28, batch loss = 2.23 (35.3 examples/sec; 0.227 sec/batch; 20h:19m:40s remains)
INFO - root - 2017-12-08 05:29:38.051387: step 9850, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 19h:42m:46s remains)
INFO - root - 2017-12-08 05:29:40.287607: step 9860, loss = 2.28, batch loss = 2.23 (34.3 examples/sec; 0.233 sec/batch; 20h:55m:20s remains)
INFO - root - 2017-12-08 05:29:42.512255: step 9870, loss = 2.28, batch loss = 2.23 (37.2 examples/sec; 0.215 sec/batch; 19h:15m:02s remains)
INFO - root - 2017-12-08 05:29:44.737836: step 9880, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 19h:38m:51s remains)
INFO - root - 2017-12-08 05:29:46.972154: step 9890, loss = 2.28, batch loss = 2.23 (36.6 examples/sec; 0.218 sec/batch; 19h:33m:54s remains)
INFO - root - 2017-12-08 05:29:49.210924: step 9900, loss = 2.28, batch loss = 2.23 (34.9 examples/sec; 0.229 sec/batch; 20h:33m:00s remains)
2017-12-08 05:29:49.511494: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288116 -4.42882 -4.428843 -4.4288573 -4.4288664 -4.428875 -4.42888 -4.4288712 -4.4288607 -4.4288592 -4.4288478 -4.4288287 -4.4288063 -4.4287748 -4.4287381][-4.428802 -4.428822 -4.42885 -4.4288554 -4.4288516 -4.42885 -4.4288454 -4.4288297 -4.4288116 -4.4288073 -4.4287972 -4.4287753 -4.4287448 -4.4287038 -4.4286618][-4.428791 -4.4288225 -4.428844 -4.4288363 -4.4288187 -4.428803 -4.4287872 -4.428761 -4.4287448 -4.4287438 -4.4287429 -4.4287271 -4.4286919 -4.428637 -4.4285936][-4.4287939 -4.428822 -4.4288239 -4.4288011 -4.4287734 -4.4287524 -4.4287257 -4.4286919 -4.42868 -4.4286933 -4.4287071 -4.4287014 -4.428668 -4.4286075 -4.4285617][-4.4288106 -4.4288325 -4.4288168 -4.428771 -4.4287281 -4.4286976 -4.4286652 -4.42863 -4.4286232 -4.4286594 -4.4286938 -4.4287024 -4.4286776 -4.428617 -4.4285588][-4.4288192 -4.4288392 -4.4288087 -4.428741 -4.4286804 -4.4286356 -4.4285994 -4.4285631 -4.4285631 -4.4286194 -4.4286737 -4.4287019 -4.4286919 -4.4286413 -4.4285707][-4.4287963 -4.4288106 -4.42877 -4.4286923 -4.4286208 -4.4285665 -4.4285307 -4.4285045 -4.4285192 -4.428586 -4.4286475 -4.4286847 -4.4286828 -4.4286494 -4.4285874][-4.4287343 -4.4287434 -4.4287038 -4.4286308 -4.4285579 -4.4284978 -4.4284673 -4.4284606 -4.4284925 -4.4285612 -4.4286242 -4.4286547 -4.4286485 -4.42863 -4.4285927][-4.4286795 -4.4286747 -4.4286294 -4.4285612 -4.4284949 -4.4284339 -4.4284081 -4.4284134 -4.4284554 -4.4285307 -4.4285994 -4.4286294 -4.428616 -4.428596 -4.4285703][-4.4286485 -4.4286261 -4.4285712 -4.4284987 -4.4284391 -4.42839 -4.4283781 -4.4283953 -4.4284482 -4.4285259 -4.4285879 -4.4286141 -4.4286017 -4.428575 -4.4285455][-4.4286456 -4.4286108 -4.4285479 -4.4284759 -4.4284253 -4.4284034 -4.4284177 -4.4284477 -4.4284921 -4.4285474 -4.4285946 -4.42862 -4.4286108 -4.4285803 -4.4285398][-4.4286518 -4.4286227 -4.4285712 -4.4285107 -4.4284682 -4.4284687 -4.4285035 -4.4285336 -4.4285564 -4.4285831 -4.4286242 -4.4286566 -4.42865 -4.4286122 -4.4285579][-4.428659 -4.4286346 -4.4285946 -4.4285541 -4.4285307 -4.4285474 -4.4285913 -4.4286218 -4.4286389 -4.4286575 -4.4286904 -4.4287143 -4.4287066 -4.4286671 -4.428606][-4.4287252 -4.4287028 -4.4286728 -4.4286456 -4.4286375 -4.42866 -4.4286976 -4.4287276 -4.4287496 -4.428772 -4.4287896 -4.4287987 -4.4287877 -4.4287562 -4.4287105][-4.42882 -4.4288068 -4.42879 -4.4287767 -4.4287763 -4.4287944 -4.4288177 -4.4288363 -4.4288549 -4.4288707 -4.4288778 -4.4288764 -4.4288673 -4.4288487 -4.4288239]]...]
INFO - root - 2017-12-08 05:29:51.747547: step 9910, loss = 2.28, batch loss = 2.23 (36.7 examples/sec; 0.218 sec/batch; 19h:31m:41s remains)
INFO - root - 2017-12-08 05:29:53.967177: step 9920, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.221 sec/batch; 19h:50m:05s remains)
INFO - root - 2017-12-08 05:29:56.205195: step 9930, loss = 2.28, batch loss = 2.23 (36.0 examples/sec; 0.222 sec/batch; 19h:55m:34s remains)
INFO - root - 2017-12-08 05:29:58.430931: step 9940, loss = 2.28, batch loss = 2.23 (35.2 examples/sec; 0.227 sec/batch; 20h:21m:26s remains)
INFO - root - 2017-12-08 05:30:00.657853: step 9950, loss = 2.28, batch loss = 2.23 (35.1 examples/sec; 0.228 sec/batch; 20h:24m:47s remains)
INFO - root - 2017-12-08 05:30:02.931919: step 9960, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 19h:48m:42s remains)
INFO - root - 2017-12-08 05:30:05.183774: step 9970, loss = 2.28, batch loss = 2.23 (35.8 examples/sec; 0.223 sec/batch; 19h:59m:35s remains)
INFO - root - 2017-12-08 05:30:07.433921: step 9980, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 19h:57m:36s remains)
INFO - root - 2017-12-08 05:30:09.684891: step 9990, loss = 2.28, batch loss = 2.23 (35.8 examples/sec; 0.224 sec/batch; 20h:02m:10s remains)
INFO - root - 2017-12-08 05:30:11.905321: step 10000, loss = 2.28, batch loss = 2.23 (35.4 examples/sec; 0.226 sec/batch; 20h:14m:35s remains)
2017-12-08 05:30:12.215013: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289465 -4.4289365 -4.4289188 -4.4289045 -4.428894 -4.4288836 -4.4288778 -4.428885 -4.4289007 -4.428915 -4.4289241 -4.4289308 -4.4289336 -4.4289379 -4.4289384][-4.4289451 -4.4289317 -4.428905 -4.4288831 -4.4288683 -4.4288516 -4.42884 -4.4288459 -4.4288611 -4.4288764 -4.4288888 -4.4289031 -4.4289145 -4.4289246 -4.4289279][-4.4289279 -4.4289074 -4.428865 -4.4288311 -4.4288058 -4.4287825 -4.4287691 -4.4287777 -4.4287949 -4.4288068 -4.42882 -4.428844 -4.4288673 -4.4288878 -4.4289][-4.4289045 -4.4288712 -4.4288111 -4.42876 -4.4287181 -4.428688 -4.42868 -4.4286981 -4.4287233 -4.4287395 -4.4287529 -4.4287815 -4.4288087 -4.4288354 -4.4288549][-4.4288969 -4.4288526 -4.4287748 -4.4287043 -4.428637 -4.4285879 -4.4285727 -4.4285989 -4.4286427 -4.4286747 -4.4286985 -4.4287295 -4.4287567 -4.4287863 -4.4288154][-4.4288845 -4.4288359 -4.4287424 -4.4286451 -4.42854 -4.4284444 -4.4283905 -4.42842 -4.4285097 -4.4285922 -4.4286532 -4.4286985 -4.4287267 -4.4287562 -4.4287953][-4.4288588 -4.4288063 -4.4287004 -4.42858 -4.4284368 -4.4282761 -4.4281445 -4.4281626 -4.428318 -4.4284749 -4.4285951 -4.428669 -4.4287057 -4.4287391 -4.428781][-4.4288378 -4.428792 -4.428689 -4.428556 -4.4283748 -4.4281387 -4.4278922 -4.4278584 -4.4280891 -4.4283371 -4.4285221 -4.4286332 -4.4286914 -4.4287319 -4.4287739][-4.4288306 -4.4287915 -4.428709 -4.4285922 -4.4284158 -4.4281707 -4.4279027 -4.427844 -4.4280748 -4.4283352 -4.4285326 -4.42865 -4.4287162 -4.4287548 -4.4287872][-4.4288254 -4.4287939 -4.4287324 -4.4286532 -4.4285388 -4.4283762 -4.4282002 -4.4281545 -4.4283037 -4.4284897 -4.4286389 -4.4287329 -4.428782 -4.4287972 -4.4288158][-4.4288211 -4.4287915 -4.4287405 -4.4286866 -4.4286323 -4.4285493 -4.4284563 -4.4284391 -4.4285426 -4.4286709 -4.4287767 -4.4288459 -4.4288788 -4.4288731 -4.4288721][-4.428844 -4.42883 -4.4287972 -4.4287639 -4.4287477 -4.4287133 -4.4286742 -4.4286814 -4.4287572 -4.428843 -4.4289117 -4.4289594 -4.4289756 -4.4289522 -4.4289341][-4.4288859 -4.4288931 -4.4288831 -4.4288726 -4.4288745 -4.428863 -4.4288468 -4.428854 -4.4288964 -4.4289436 -4.4289823 -4.4290113 -4.4290166 -4.4289885 -4.4289656][-4.428916 -4.4289322 -4.428937 -4.4289393 -4.4289465 -4.4289451 -4.4289393 -4.4289403 -4.42896 -4.4289832 -4.4290061 -4.4290237 -4.4290223 -4.4289994 -4.4289784][-4.4289355 -4.4289494 -4.4289551 -4.42896 -4.4289641 -4.4289627 -4.4289618 -4.4289637 -4.4289765 -4.4289913 -4.4290042 -4.4290137 -4.429008 -4.4289927 -4.4289784]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-momentum-lr0.001-defgroup-1/model.ckpt-10000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-momentum-lr0.001-defgroup-1/model.ckpt-10000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-08 05:30:14.717904: step 10010, loss = 2.28, batch loss = 2.23 (37.1 examples/sec; 0.216 sec/batch; 19h:20m:15s remains)
INFO - root - 2017-12-08 05:30:16.995989: step 10020, loss = 2.28, batch loss = 2.23 (33.6 examples/sec; 0.238 sec/batch; 21h:18m:49s remains)
INFO - root - 2017-12-08 05:30:19.278689: step 10030, loss = 2.28, batch loss = 2.23 (33.1 examples/sec; 0.241 sec/batch; 21h:37m:46s remains)
INFO - root - 2017-12-08 05:30:21.546163: step 10040, loss = 2.28, batch loss = 2.23 (35.1 examples/sec; 0.228 sec/batch; 20h:24m:23s remains)
INFO - root - 2017-12-08 05:30:23.815958: step 10050, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 19h:48m:11s remains)
INFO - root - 2017-12-08 05:30:26.039050: step 10060, loss = 2.28, batch loss = 2.23 (35.2 examples/sec; 0.227 sec/batch; 20h:20m:35s remains)
INFO - root - 2017-12-08 05:30:28.263921: step 10070, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 19h:47m:47s remains)
INFO - root - 2017-12-08 05:30:30.496040: step 10080, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.221 sec/batch; 19h:45m:07s remains)
INFO - root - 2017-12-08 05:30:32.737754: step 10090, loss = 2.28, batch loss = 2.23 (35.7 examples/sec; 0.224 sec/batch; 20h:05m:40s remains)
INFO - root - 2017-12-08 05:30:34.990772: step 10100, loss = 2.28, batch loss = 2.23 (35.6 examples/sec; 0.225 sec/batch; 20h:08m:11s remains)
2017-12-08 05:30:35.290574: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.428947 -4.4289236 -4.4289312 -4.4289536 -4.4289541 -4.4289322 -4.428906 -4.4288907 -4.4288926 -4.4289074 -4.4289093 -4.4288912 -4.428874 -4.428875 -4.4288774][-4.4289336 -4.4289007 -4.4289045 -4.42893 -4.4289336 -4.4289117 -4.4288745 -4.4288497 -4.4288454 -4.4288559 -4.4288564 -4.4288349 -4.4288163 -4.42882 -4.4288244][-4.4289246 -4.4288807 -4.4288816 -4.4289002 -4.4288955 -4.4288688 -4.4288292 -4.4288087 -4.4288092 -4.4288073 -4.4287891 -4.428762 -4.4287534 -4.4287648 -4.4287677][-4.4289136 -4.4288545 -4.428844 -4.4288468 -4.4288306 -4.4287996 -4.428771 -4.4287705 -4.4287815 -4.4287686 -4.4287357 -4.4287128 -4.4287176 -4.4287329 -4.42873][-4.4288869 -4.428803 -4.4287663 -4.4287496 -4.42872 -4.4286823 -4.4286709 -4.4287066 -4.4287457 -4.4287443 -4.4287214 -4.4287233 -4.4287348 -4.4287419 -4.4287276][-4.4288459 -4.4287429 -4.4286866 -4.42866 -4.4286056 -4.4285383 -4.4285245 -4.4285865 -4.4286594 -4.4286966 -4.4287114 -4.428741 -4.4287605 -4.4287615 -4.4287376][-4.4287834 -4.4286737 -4.4286165 -4.4285913 -4.4285159 -4.4283919 -4.4283481 -4.4284191 -4.4285212 -4.4286065 -4.4286647 -4.4287171 -4.4287481 -4.4287553 -4.4287367][-4.42872 -4.4286089 -4.4285541 -4.4285278 -4.4284515 -4.4283066 -4.4282422 -4.4283166 -4.4284277 -4.4285336 -4.4286065 -4.4286618 -4.4286981 -4.4287195 -4.4287219][-4.4286966 -4.4286 -4.4285526 -4.4285274 -4.4284825 -4.42838 -4.4283352 -4.4284029 -4.4284854 -4.4285526 -4.4285874 -4.4286184 -4.4286566 -4.4286957 -4.428719][-4.4287171 -4.4286404 -4.4286056 -4.4285889 -4.4285774 -4.4285307 -4.4285131 -4.4285636 -4.4286036 -4.4286222 -4.4286222 -4.4286323 -4.4286737 -4.4287248 -4.4287581][-4.428781 -4.4287257 -4.4287043 -4.4286995 -4.4287076 -4.4286942 -4.4286861 -4.4287138 -4.4287267 -4.4287152 -4.4287033 -4.4287081 -4.4287415 -4.4287858 -4.4288154][-4.428864 -4.4288354 -4.4288273 -4.4288282 -4.4288378 -4.4288335 -4.4288263 -4.428833 -4.4288287 -4.42881 -4.4287972 -4.4287982 -4.4288206 -4.428854 -4.4288774][-4.4289207 -4.4289122 -4.4289131 -4.4289131 -4.428916 -4.4289188 -4.4289193 -4.4289217 -4.4289136 -4.428895 -4.4288797 -4.4288712 -4.4288831 -4.4289107 -4.4289274][-4.428925 -4.4289246 -4.4289265 -4.4289274 -4.4289269 -4.4289379 -4.4289455 -4.4289465 -4.4289389 -4.428926 -4.4289131 -4.4288993 -4.4289036 -4.4289231 -4.4289336][-4.4289122 -4.4289093 -4.42891 -4.4289122 -4.4289117 -4.4289207 -4.4289284 -4.4289274 -4.4289203 -4.4289107 -4.4289055 -4.4289031 -4.4289083 -4.4289207 -4.428926]]...]
INFO - root - 2017-12-08 05:30:37.510185: step 10110, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.222 sec/batch; 19h:50m:30s remains)
INFO - root - 2017-12-08 05:30:39.745648: step 10120, loss = 2.28, batch loss = 2.23 (35.1 examples/sec; 0.228 sec/batch; 20h:23m:25s remains)
INFO - root - 2017-12-08 05:30:41.997661: step 10130, loss = 2.28, batch loss = 2.23 (35.1 examples/sec; 0.228 sec/batch; 20h:23m:41s remains)
INFO - root - 2017-12-08 05:30:44.258501: step 10140, loss = 2.28, batch loss = 2.23 (36.8 examples/sec; 0.217 sec/batch; 19h:27m:54s remains)
INFO - root - 2017-12-08 05:30:46.532473: step 10150, loss = 2.28, batch loss = 2.23 (36.0 examples/sec; 0.222 sec/batch; 19h:55m:18s remains)
INFO - root - 2017-12-08 05:30:48.756284: step 10160, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 19h:46m:49s remains)
INFO - root - 2017-12-08 05:30:50.971311: step 10170, loss = 2.28, batch loss = 2.23 (37.2 examples/sec; 0.215 sec/batch; 19h:14m:50s remains)
INFO - root - 2017-12-08 05:30:53.257729: step 10180, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.225 sec/batch; 20h:09m:29s remains)
INFO - root - 2017-12-08 05:30:55.483878: step 10190, loss = 2.28, batch loss = 2.23 (35.6 examples/sec; 0.224 sec/batch; 20h:05m:52s remains)
INFO - root - 2017-12-08 05:30:57.713563: step 10200, loss = 2.28, batch loss = 2.23 (34.8 examples/sec; 0.230 sec/batch; 20h:33m:12s remains)
2017-12-08 05:30:58.017114: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288726 -4.428864 -4.4288492 -4.4288306 -4.4288158 -4.4288011 -4.4287839 -4.428762 -4.4287477 -4.4287477 -4.4287615 -4.4287825 -4.4287858 -4.4287882 -4.4287882][-4.4288745 -4.4288568 -4.4288378 -4.4288163 -4.4287996 -4.4287872 -4.4287782 -4.4287591 -4.42874 -4.4287281 -4.4287233 -4.4287276 -4.428721 -4.4287181 -4.4287119][-4.4288511 -4.4288206 -4.4287906 -4.4287643 -4.4287415 -4.4287314 -4.4287305 -4.4287176 -4.428699 -4.4286766 -4.428659 -4.4286542 -4.4286475 -4.4286494 -4.4286489][-4.42881 -4.4287734 -4.4287367 -4.4287114 -4.4286847 -4.4286747 -4.4286776 -4.4286652 -4.4286447 -4.4286208 -4.4286003 -4.428597 -4.4286017 -4.4286141 -4.4286261][-4.4287648 -4.4287357 -4.4287124 -4.4286962 -4.4286747 -4.4286637 -4.4286652 -4.4286389 -4.4286022 -4.4285769 -4.4285679 -4.4285703 -4.4285817 -4.4285994 -4.4286218][-4.428731 -4.4287095 -4.4286995 -4.4286962 -4.4286847 -4.42868 -4.4286737 -4.428618 -4.4285507 -4.4285264 -4.4285269 -4.4285274 -4.4285355 -4.4285593 -4.4285908][-4.4287057 -4.4286871 -4.4286833 -4.428689 -4.4286795 -4.4286718 -4.4286423 -4.4285412 -4.4284396 -4.4284282 -4.4284549 -4.4284668 -4.4284778 -4.4285131 -4.4285493][-4.42867 -4.4286532 -4.4286461 -4.42865 -4.428628 -4.428596 -4.4285278 -4.428381 -4.428257 -4.4282875 -4.4283628 -4.4284024 -4.4284363 -4.4284883 -4.4285321][-4.4285727 -4.4285445 -4.42852 -4.4285059 -4.4284654 -4.4284177 -4.4283395 -4.4281964 -4.4281063 -4.4281979 -4.4283147 -4.4283819 -4.4284415 -4.4285026 -4.4285436][-4.4284525 -4.4283972 -4.4283438 -4.4283137 -4.4282784 -4.4282527 -4.4282126 -4.4281378 -4.4281216 -4.4282308 -4.4283404 -4.4284058 -4.428463 -4.4285107 -4.4285421][-4.4283876 -4.42832 -4.4282556 -4.4282236 -4.4282122 -4.4282174 -4.4282141 -4.4281974 -4.4282227 -4.4283075 -4.4283781 -4.4284182 -4.4284573 -4.4284887 -4.4285164][-4.42845 -4.4283876 -4.4283209 -4.4282842 -4.4282756 -4.428288 -4.4282975 -4.4283009 -4.4283333 -4.4283895 -4.4284286 -4.428453 -4.4284821 -4.42851 -4.4285369][-4.4285812 -4.4285312 -4.428472 -4.4284306 -4.4284139 -4.4284229 -4.4284329 -4.4284425 -4.42847 -4.4285059 -4.4285274 -4.4285431 -4.4285679 -4.4285951 -4.4286213][-4.4287157 -4.4286833 -4.4286366 -4.4286008 -4.4285817 -4.4285831 -4.4285879 -4.428597 -4.4286203 -4.4286456 -4.4286628 -4.4286761 -4.4286971 -4.4287219 -4.4287434][-4.4288287 -4.428812 -4.428781 -4.4287543 -4.4287405 -4.4287434 -4.4287505 -4.428762 -4.4287796 -4.4287958 -4.4288068 -4.4288154 -4.4288273 -4.4288406 -4.4288516]]...]
INFO - root - 2017-12-08 05:31:00.257383: step 10210, loss = 2.28, batch loss = 2.23 (36.6 examples/sec; 0.219 sec/batch; 19h:33m:51s remains)
INFO - root - 2017-12-08 05:31:02.490352: step 10220, loss = 2.28, batch loss = 2.23 (34.0 examples/sec; 0.235 sec/batch; 21h:03m:08s remains)
INFO - root - 2017-12-08 05:31:04.714228: step 10230, loss = 2.28, batch loss = 2.23 (36.9 examples/sec; 0.217 sec/batch; 19h:25m:36s remains)
INFO - root - 2017-12-08 05:31:06.940921: step 10240, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 19h:41m:06s remains)
INFO - root - 2017-12-08 05:31:09.245230: step 10250, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.225 sec/batch; 20h:09m:17s remains)
INFO - root - 2017-12-08 05:31:11.476514: step 10260, loss = 2.28, batch loss = 2.23 (35.6 examples/sec; 0.225 sec/batch; 20h:07m:49s remains)
INFO - root - 2017-12-08 05:31:13.706846: step 10270, loss = 2.28, batch loss = 2.23 (34.3 examples/sec; 0.233 sec/batch; 20h:51m:03s remains)
INFO - root - 2017-12-08 05:31:15.932118: step 10280, loss = 2.28, batch loss = 2.23 (37.4 examples/sec; 0.214 sec/batch; 19h:09m:54s remains)
INFO - root - 2017-12-08 05:31:18.184159: step 10290, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 19h:57m:29s remains)
INFO - root - 2017-12-08 05:31:20.413724: step 10300, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.222 sec/batch; 19h:51m:21s remains)
2017-12-08 05:31:20.734498: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289093 -4.4289203 -4.4289284 -4.4289351 -4.428937 -4.428936 -4.4289351 -4.4289308 -4.4289222 -4.4289055 -4.4288793 -4.4288592 -4.4288526 -4.4288673 -4.428894][-4.4288683 -4.4288797 -4.4288869 -4.428894 -4.428896 -4.4288936 -4.4288926 -4.4288921 -4.4288874 -4.42887 -4.4288445 -4.4288297 -4.42883 -4.428853 -4.4288893][-4.4288521 -4.42886 -4.4288654 -4.4288745 -4.4288812 -4.4288797 -4.4288783 -4.4288793 -4.4288774 -4.4288645 -4.4288445 -4.4288406 -4.42885 -4.428875 -4.4289041][-4.4288425 -4.4288473 -4.4288526 -4.4288621 -4.4288669 -4.42886 -4.428853 -4.4288568 -4.4288621 -4.4288597 -4.4288487 -4.4288478 -4.4288559 -4.4288774 -4.4289026][-4.4288116 -4.428812 -4.4288168 -4.4288235 -4.4288239 -4.4288077 -4.42879 -4.4287934 -4.4288182 -4.4288411 -4.428844 -4.428843 -4.42884 -4.428854 -4.4288759][-4.4287329 -4.4287362 -4.4287515 -4.4287624 -4.4287581 -4.4287186 -4.42868 -4.4286871 -4.4287429 -4.4288049 -4.4288325 -4.4288349 -4.4288135 -4.4288111 -4.4288316][-4.4286265 -4.4286509 -4.42869 -4.4287086 -4.4286838 -4.4285979 -4.428515 -4.4285083 -4.4286 -4.4287167 -4.4287839 -4.4287996 -4.4287724 -4.4287629 -4.428792][-4.4285278 -4.428587 -4.4286666 -4.4287028 -4.4286575 -4.4285216 -4.4283781 -4.4283357 -4.4284496 -4.4286084 -4.4287143 -4.4287539 -4.4287391 -4.4287386 -4.4287815][-4.42846 -4.4285479 -4.4286661 -4.428731 -4.4286947 -4.4285541 -4.4283948 -4.4283214 -4.4284081 -4.4285512 -4.4286637 -4.4287233 -4.4287357 -4.428762 -4.4288182][-4.4284782 -4.4285746 -4.4287028 -4.4287839 -4.4287705 -4.4286656 -4.4285493 -4.4284878 -4.4285259 -4.4286094 -4.4286947 -4.4287581 -4.428791 -4.4288321 -4.4288859][-4.4285536 -4.4286418 -4.4287562 -4.428834 -4.4288387 -4.42877 -4.4286952 -4.4286513 -4.4286594 -4.4286914 -4.4287429 -4.4287968 -4.4288344 -4.4288797 -4.4289317][-4.4286261 -4.4287038 -4.4288011 -4.428874 -4.4288912 -4.4288535 -4.4288073 -4.4287791 -4.4287682 -4.4287553 -4.4287629 -4.4287944 -4.4288311 -4.4288793 -4.4289312][-4.4286513 -4.4287233 -4.4288135 -4.4288893 -4.4289203 -4.4289036 -4.4288774 -4.4288597 -4.4288425 -4.4288073 -4.4287858 -4.4287982 -4.42883 -4.4288726 -4.4289207][-4.4286489 -4.4287152 -4.4288039 -4.4288917 -4.4289412 -4.4289379 -4.4289227 -4.428916 -4.4289021 -4.4288673 -4.4288359 -4.4288297 -4.4288464 -4.4288759 -4.4289155][-4.4286432 -4.4286995 -4.4287868 -4.4288793 -4.4289408 -4.4289479 -4.42894 -4.4289322 -4.4289179 -4.4288883 -4.4288564 -4.4288397 -4.4288416 -4.428865 -4.4289064]]...]
INFO - root - 2017-12-08 05:31:22.958897: step 10310, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 19h:55m:41s remains)
INFO - root - 2017-12-08 05:31:25.216368: step 10320, loss = 2.28, batch loss = 2.23 (37.2 examples/sec; 0.215 sec/batch; 19h:15m:31s remains)
INFO - root - 2017-12-08 05:31:27.473563: step 10330, loss = 2.28, batch loss = 2.23 (35.6 examples/sec; 0.225 sec/batch; 20h:07m:27s remains)
INFO - root - 2017-12-08 05:31:29.696720: step 10340, loss = 2.28, batch loss = 2.23 (36.9 examples/sec; 0.217 sec/batch; 19h:23m:52s remains)
INFO - root - 2017-12-08 05:31:31.931834: step 10350, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.222 sec/batch; 19h:50m:49s remains)
INFO - root - 2017-12-08 05:31:34.152103: step 10360, loss = 2.28, batch loss = 2.23 (36.8 examples/sec; 0.217 sec/batch; 19h:27m:00s remains)
INFO - root - 2017-12-08 05:31:36.387586: step 10370, loss = 2.28, batch loss = 2.23 (35.2 examples/sec; 0.227 sec/batch; 20h:21m:00s remains)
INFO - root - 2017-12-08 05:31:38.671290: step 10380, loss = 2.28, batch loss = 2.23 (35.1 examples/sec; 0.228 sec/batch; 20h:24m:22s remains)
INFO - root - 2017-12-08 05:31:40.912781: step 10390, loss = 2.28, batch loss = 2.23 (35.2 examples/sec; 0.227 sec/batch; 20h:21m:16s remains)
INFO - root - 2017-12-08 05:31:43.143702: step 10400, loss = 2.28, batch loss = 2.23 (34.8 examples/sec; 0.230 sec/batch; 20h:32m:48s remains)
2017-12-08 05:31:43.419582: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289875 -4.4289918 -4.428987 -4.4289651 -4.4289222 -4.428874 -4.4288387 -4.4288359 -4.4288731 -4.4289246 -4.4289603 -4.4289808 -4.4289889 -4.4289889 -4.4289861][-4.4289923 -4.4289966 -4.428987 -4.4289575 -4.4289055 -4.4288492 -4.4288111 -4.4288116 -4.4288564 -4.4289155 -4.4289579 -4.4289832 -4.4289927 -4.4289923 -4.4289889][-4.4289942 -4.4289966 -4.4289813 -4.4289427 -4.4288816 -4.4288149 -4.4287734 -4.4287782 -4.4288321 -4.4289007 -4.42895 -4.42898 -4.4289927 -4.4289932 -4.42899][-4.4289856 -4.4289851 -4.4289627 -4.4289131 -4.4288387 -4.428762 -4.4287205 -4.4287338 -4.4287972 -4.4288759 -4.4289341 -4.4289718 -4.42899 -4.4289932 -4.4289918][-4.4289508 -4.428946 -4.4289112 -4.4288411 -4.4287472 -4.4286637 -4.4286284 -4.4286528 -4.4287248 -4.4288139 -4.4288878 -4.4289417 -4.4289751 -4.4289889 -4.4289927][-4.4288874 -4.4288745 -4.428822 -4.4287281 -4.428617 -4.4285355 -4.4285121 -4.4285445 -4.428618 -4.42871 -4.4288011 -4.4288793 -4.4289355 -4.428968 -4.4289827][-4.4288125 -4.4287877 -4.4287171 -4.4286051 -4.4284911 -4.4284239 -4.4284167 -4.4284515 -4.428514 -4.4285951 -4.4286966 -4.4288006 -4.4288845 -4.4289379 -4.428967][-4.4287581 -4.4287214 -4.4286413 -4.4285288 -4.4284258 -4.4283772 -4.4283814 -4.4284096 -4.4284492 -4.4285049 -4.428606 -4.4287243 -4.4288297 -4.4289045 -4.4289494][-4.4287367 -4.4286962 -4.4286222 -4.4285269 -4.4284453 -4.4284105 -4.4284196 -4.4284325 -4.4284453 -4.4284778 -4.4285707 -4.4286876 -4.4288011 -4.4288869 -4.42894][-4.4287372 -4.4286966 -4.4286346 -4.4285631 -4.4285049 -4.428483 -4.4284916 -4.4284925 -4.4284854 -4.4285016 -4.4285827 -4.428688 -4.4287982 -4.4288869 -4.4289412][-4.4287605 -4.4287233 -4.4286771 -4.4286289 -4.4285917 -4.4285803 -4.4285855 -4.4285774 -4.4285579 -4.4285631 -4.4286261 -4.4287128 -4.4288096 -4.4288931 -4.4289446][-4.4288054 -4.4287806 -4.4287519 -4.4287257 -4.4287095 -4.4287071 -4.4287119 -4.4287009 -4.4286757 -4.4286695 -4.428709 -4.4287715 -4.4288459 -4.4289122 -4.4289522][-4.4288473 -4.4288349 -4.4288225 -4.4288154 -4.4288163 -4.428823 -4.4288287 -4.42882 -4.4287977 -4.4287844 -4.4288044 -4.4288449 -4.428895 -4.4289393 -4.4289641][-4.428884 -4.4288769 -4.42887 -4.4288726 -4.4288826 -4.4288945 -4.4289021 -4.4288993 -4.4288845 -4.42887 -4.4288769 -4.4289026 -4.4289346 -4.4289613 -4.4289722][-4.4289222 -4.4289169 -4.4289117 -4.4289141 -4.4289222 -4.4289308 -4.428936 -4.4289351 -4.4289274 -4.4289184 -4.42892 -4.4289331 -4.4289522 -4.4289665 -4.4289689]]...]
INFO - root - 2017-12-08 05:31:45.652555: step 10410, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.220 sec/batch; 19h:42m:16s remains)
INFO - root - 2017-12-08 05:31:47.877922: step 10420, loss = 2.28, batch loss = 2.23 (34.8 examples/sec; 0.230 sec/batch; 20h:35m:32s remains)
INFO - root - 2017-12-08 05:31:50.138893: step 10430, loss = 2.28, batch loss = 2.23 (34.8 examples/sec; 0.230 sec/batch; 20h:33m:13s remains)
INFO - root - 2017-12-08 05:31:52.352096: step 10440, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 19h:46m:11s remains)
INFO - root - 2017-12-08 05:31:54.588976: step 10450, loss = 2.28, batch loss = 2.23 (36.9 examples/sec; 0.217 sec/batch; 19h:24m:03s remains)
INFO - root - 2017-12-08 05:31:56.832920: step 10460, loss = 2.28, batch loss = 2.23 (35.7 examples/sec; 0.224 sec/batch; 20h:01m:08s remains)
INFO - root - 2017-12-08 05:31:59.088954: step 10470, loss = 2.28, batch loss = 2.23 (35.6 examples/sec; 0.225 sec/batch; 20h:07m:48s remains)
INFO - root - 2017-12-08 05:32:01.328729: step 10480, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.221 sec/batch; 19h:47m:56s remains)
INFO - root - 2017-12-08 05:32:03.564829: step 10490, loss = 2.28, batch loss = 2.23 (37.0 examples/sec; 0.216 sec/batch; 19h:20m:58s remains)
INFO - root - 2017-12-08 05:32:05.790161: step 10500, loss = 2.28, batch loss = 2.23 (36.7 examples/sec; 0.218 sec/batch; 19h:29m:24s remains)
2017-12-08 05:32:06.077182: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4287786 -4.428771 -4.4287572 -4.4287395 -4.4287291 -4.4287229 -4.4287157 -4.4287028 -4.4286709 -4.4286427 -4.4286523 -4.4287009 -4.428762 -4.4288044 -4.4288197][-4.4287972 -4.4288 -4.4287891 -4.4287663 -4.4287457 -4.4287305 -4.42872 -4.4287095 -4.4286895 -4.4286714 -4.4286814 -4.4287233 -4.4287782 -4.4288168 -4.42883][-4.428812 -4.4288206 -4.4288111 -4.4287887 -4.4287639 -4.4287376 -4.4287152 -4.4286962 -4.4286819 -4.4286747 -4.42869 -4.428731 -4.4287829 -4.4288187 -4.4288344][-4.4287958 -4.4288034 -4.4287996 -4.4287815 -4.4287553 -4.4287214 -4.4286838 -4.4286509 -4.4286366 -4.4286385 -4.4286628 -4.4287114 -4.4287658 -4.4287992 -4.4288177][-4.4287677 -4.428781 -4.4287891 -4.4287767 -4.4287448 -4.4286947 -4.4286308 -4.4285755 -4.4285583 -4.4285717 -4.428606 -4.4286566 -4.4287043 -4.4287305 -4.4287472][-4.4287281 -4.428751 -4.4287744 -4.4287682 -4.4287252 -4.428648 -4.4285479 -4.4284596 -4.4284325 -4.4284592 -4.4285088 -4.4285645 -4.4286079 -4.4286308 -4.4286456][-4.428669 -4.4286971 -4.4287243 -4.4287128 -4.4286551 -4.4285536 -4.4284196 -4.4282994 -4.4282651 -4.4283118 -4.4283838 -4.4284515 -4.4284997 -4.4285316 -4.428555][-4.4285693 -4.4285994 -4.4286275 -4.4286118 -4.4285531 -4.4284539 -4.4283295 -4.4282207 -4.4282 -4.4282584 -4.42833 -4.4283934 -4.4284458 -4.428493 -4.4285345][-4.4285016 -4.4285321 -4.4285607 -4.4285517 -4.4285207 -4.4284749 -4.4284253 -4.4283824 -4.4283762 -4.4284105 -4.4284487 -4.428483 -4.4285207 -4.4285669 -4.4286094][-4.4285688 -4.42859 -4.4286079 -4.4286089 -4.4286065 -4.4286065 -4.4286151 -4.428617 -4.4286132 -4.42862 -4.4286265 -4.4286342 -4.428648 -4.4286733 -4.4286952][-4.4286857 -4.4286942 -4.4287033 -4.4287128 -4.4287276 -4.428751 -4.4287767 -4.4287848 -4.4287715 -4.4287577 -4.4287415 -4.4287219 -4.4287114 -4.4287186 -4.4287248][-4.4287853 -4.4287806 -4.4287872 -4.4288077 -4.4288354 -4.4288654 -4.4288869 -4.4288793 -4.4288473 -4.428813 -4.4287791 -4.4287367 -4.4287047 -4.4286947 -4.4286947][-4.4288387 -4.4288206 -4.4288311 -4.428865 -4.4289007 -4.4289293 -4.4289389 -4.428916 -4.4288726 -4.4288287 -4.4287829 -4.4287252 -4.4286785 -4.428659 -4.4286613][-4.4288611 -4.4288368 -4.4288487 -4.4288888 -4.4289203 -4.4289408 -4.4289412 -4.4289083 -4.4288559 -4.4288082 -4.4287605 -4.4287052 -4.4286628 -4.4286518 -4.4286647][-4.4288659 -4.4288344 -4.4288421 -4.4288783 -4.4289007 -4.4289131 -4.4289074 -4.42887 -4.428812 -4.4287615 -4.4287195 -4.4286747 -4.4286542 -4.4286652 -4.4286876]]...]
INFO - root - 2017-12-08 05:32:08.311339: step 10510, loss = 2.28, batch loss = 2.23 (34.5 examples/sec; 0.232 sec/batch; 20h:44m:10s remains)
INFO - root - 2017-12-08 05:32:10.560159: step 10520, loss = 2.28, batch loss = 2.23 (37.4 examples/sec; 0.214 sec/batch; 19h:07m:57s remains)
INFO - root - 2017-12-08 05:32:12.808816: step 10530, loss = 2.28, batch loss = 2.23 (35.7 examples/sec; 0.224 sec/batch; 20h:02m:19s remains)
INFO - root - 2017-12-08 05:32:15.068017: step 10540, loss = 2.28, batch loss = 2.23 (36.0 examples/sec; 0.222 sec/batch; 19h:52m:01s remains)
INFO - root - 2017-12-08 05:32:17.309469: step 10550, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.225 sec/batch; 20h:08m:57s remains)
INFO - root - 2017-12-08 05:32:19.536978: step 10560, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.222 sec/batch; 19h:50m:24s remains)
INFO - root - 2017-12-08 05:32:21.775462: step 10570, loss = 2.28, batch loss = 2.23 (34.4 examples/sec; 0.232 sec/batch; 20h:46m:12s remains)
INFO - root - 2017-12-08 05:32:24.081841: step 10580, loss = 2.28, batch loss = 2.23 (34.7 examples/sec; 0.231 sec/batch; 20h:37m:23s remains)
INFO - root - 2017-12-08 05:32:26.297303: step 10590, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 19h:46m:50s remains)
INFO - root - 2017-12-08 05:32:28.552699: step 10600, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.225 sec/batch; 20h:08m:19s remains)
2017-12-08 05:32:28.819308: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4287095 -4.4285822 -4.4284635 -4.4284115 -4.4284949 -4.4285827 -4.4286113 -4.4285965 -4.4285555 -4.4285359 -4.42859 -4.4286423 -4.4286852 -4.4287152 -4.4287252][-4.4287119 -4.42861 -4.4285221 -4.4284859 -4.4285436 -4.4285879 -4.428576 -4.4285455 -4.4285045 -4.4285011 -4.4285827 -4.4286618 -4.428719 -4.4287558 -4.4287767][-4.4287505 -4.4286847 -4.4286375 -4.42862 -4.4286485 -4.4286513 -4.428618 -4.4285831 -4.4285479 -4.4285507 -4.4286332 -4.4287157 -4.4287667 -4.4287925 -4.4288092][-4.4288092 -4.4287677 -4.4287453 -4.4287362 -4.4287429 -4.42873 -4.4287128 -4.4287095 -4.4286962 -4.4286909 -4.4287291 -4.4287739 -4.4287977 -4.4288116 -4.4288216][-4.4288516 -4.4288144 -4.4287906 -4.4287767 -4.4287682 -4.4287472 -4.42875 -4.4287858 -4.4288158 -4.4288173 -4.4288049 -4.4287934 -4.4287858 -4.4287868 -4.4287992][-4.4288735 -4.4288211 -4.4287724 -4.4287415 -4.4287133 -4.42867 -4.4286704 -4.4287252 -4.4287953 -4.42882 -4.4287944 -4.4287567 -4.4287376 -4.4287462 -4.4287825][-4.4288549 -4.42878 -4.4287066 -4.4286447 -4.4285865 -4.4285226 -4.4285059 -4.428555 -4.428647 -4.428688 -4.4286666 -4.4286451 -4.4286547 -4.4287038 -4.4287786][-4.4287977 -4.4287028 -4.4286184 -4.42854 -4.4284616 -4.4283848 -4.42835 -4.4283757 -4.4284697 -4.4285121 -4.4284916 -4.4285026 -4.4285741 -4.42868 -4.4287872][-4.4287524 -4.4286456 -4.4285545 -4.428473 -4.4283924 -4.4283237 -4.4282818 -4.4282818 -4.4283519 -4.4283757 -4.4283466 -4.4284015 -4.4285493 -4.4286962 -4.4288096][-4.4287405 -4.4286351 -4.4285445 -4.4284639 -4.4283967 -4.428359 -4.4283414 -4.4283366 -4.4283695 -4.4283552 -4.428319 -4.4284115 -4.4286094 -4.4287672 -4.428853][-4.4287548 -4.4286652 -4.4285975 -4.4285345 -4.428493 -4.4284916 -4.4285073 -4.4285088 -4.4285183 -4.428494 -4.4284644 -4.4285469 -4.42872 -4.428843 -4.4288774][-4.4287753 -4.4287052 -4.4286661 -4.4286394 -4.4286237 -4.4286385 -4.428659 -4.4286518 -4.4286561 -4.4286609 -4.4286642 -4.4287214 -4.4288287 -4.4288888 -4.4288735][-4.4288011 -4.4287472 -4.4287291 -4.42873 -4.4287348 -4.4287515 -4.428762 -4.4287496 -4.4287553 -4.4287758 -4.4288025 -4.4288449 -4.4288936 -4.4289021 -4.4288549][-4.4288216 -4.42878 -4.4287734 -4.4287896 -4.4288063 -4.4288187 -4.4288211 -4.4288182 -4.4288321 -4.4288535 -4.4288797 -4.4289093 -4.4289227 -4.4289036 -4.4288473][-4.428822 -4.4287829 -4.428782 -4.428803 -4.4288268 -4.4288344 -4.4288325 -4.4288425 -4.4288683 -4.4288917 -4.42891 -4.4289289 -4.4289312 -4.428906 -4.4288526]]...]
INFO - root - 2017-12-08 05:32:31.059767: step 10610, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 19h:56m:19s remains)
INFO - root - 2017-12-08 05:32:33.281931: step 10620, loss = 2.28, batch loss = 2.23 (34.7 examples/sec; 0.231 sec/batch; 20h:37m:57s remains)
INFO - root - 2017-12-08 05:32:35.513784: step 10630, loss = 2.28, batch loss = 2.23 (34.7 examples/sec; 0.230 sec/batch; 20h:35m:38s remains)
INFO - root - 2017-12-08 05:32:37.751723: step 10640, loss = 2.28, batch loss = 2.23 (37.2 examples/sec; 0.215 sec/batch; 19h:13m:14s remains)
INFO - root - 2017-12-08 05:32:40.007227: step 10650, loss = 2.28, batch loss = 2.23 (36.7 examples/sec; 0.218 sec/batch; 19h:30m:14s remains)
INFO - root - 2017-12-08 05:32:42.257890: step 10660, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 19h:37m:14s remains)
INFO - root - 2017-12-08 05:32:44.518414: step 10670, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 19h:35m:10s remains)
INFO - root - 2017-12-08 05:32:46.771075: step 10680, loss = 2.28, batch loss = 2.23 (34.0 examples/sec; 0.235 sec/batch; 21h:02m:41s remains)
INFO - root - 2017-12-08 05:32:49.052285: step 10690, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 19h:40m:21s remains)
INFO - root - 2017-12-08 05:32:51.288827: step 10700, loss = 2.28, batch loss = 2.23 (35.8 examples/sec; 0.223 sec/batch; 19h:57m:05s remains)
2017-12-08 05:32:51.585235: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289875 -4.4290061 -4.4290071 -4.4289904 -4.4289508 -4.4288797 -4.4288154 -4.4287658 -4.4287434 -4.428782 -4.4288459 -4.4289074 -4.4289532 -4.4289546 -4.4289031][-4.4290018 -4.4290295 -4.4290357 -4.4290051 -4.4289336 -4.42883 -4.4287477 -4.4286814 -4.4286575 -4.4287219 -4.428813 -4.4288878 -4.4289336 -4.4289365 -4.4288754][-4.429008 -4.4290447 -4.4290557 -4.42902 -4.4289269 -4.4287939 -4.4286904 -4.4286065 -4.428596 -4.4286861 -4.4287958 -4.4288735 -4.428915 -4.4289074 -4.4288287][-4.429008 -4.4290476 -4.4290528 -4.429008 -4.4288874 -4.4287171 -4.4285808 -4.4284887 -4.4285131 -4.4286418 -4.4287758 -4.4288535 -4.4288826 -4.4288549 -4.4287481][-4.4290066 -4.429049 -4.4290428 -4.4289808 -4.4288344 -4.4286313 -4.4284596 -4.4283481 -4.4284019 -4.4285731 -4.4287314 -4.428813 -4.4288378 -4.4287848 -4.428638][-4.4290047 -4.4290462 -4.4290309 -4.4289522 -4.4287844 -4.4285617 -4.4283628 -4.4282079 -4.428257 -4.42846 -4.42865 -4.4287486 -4.4287744 -4.4287033 -4.428514][-4.4289961 -4.4290328 -4.4290104 -4.4289203 -4.4287348 -4.4284983 -4.4282885 -4.4280906 -4.428112 -4.4283185 -4.4285269 -4.4286518 -4.4286976 -4.4286289 -4.42843][-4.4289842 -4.4290128 -4.4289818 -4.4288845 -4.4286866 -4.4284544 -4.4282522 -4.4280457 -4.4280357 -4.4282103 -4.42841 -4.4285474 -4.428617 -4.4285707 -4.4284015][-4.4289656 -4.4289842 -4.4289465 -4.4288454 -4.4286513 -4.4284391 -4.428266 -4.4280891 -4.4280729 -4.4281974 -4.42837 -4.4284997 -4.4285707 -4.4285502 -4.4284363][-4.4289503 -4.4289575 -4.4289179 -4.4288268 -4.4286551 -4.4284735 -4.4283328 -4.4281983 -4.4281874 -4.4282713 -4.4284081 -4.4285226 -4.4285917 -4.4285936 -4.4285221][-4.4289465 -4.4289451 -4.4289074 -4.4288344 -4.4287019 -4.4285684 -4.4284725 -4.4283853 -4.4283838 -4.4284353 -4.4285336 -4.4286218 -4.4286737 -4.428679 -4.4286366][-4.4289546 -4.4289513 -4.4289174 -4.4288611 -4.4287672 -4.4286814 -4.4286256 -4.4285822 -4.4285927 -4.4286275 -4.4286928 -4.4287496 -4.4287791 -4.4287777 -4.4287596][-4.4289665 -4.4289651 -4.4289374 -4.4288964 -4.4288368 -4.4287896 -4.4287639 -4.4287548 -4.428771 -4.4287906 -4.4288306 -4.4288621 -4.4288735 -4.428864 -4.4288573][-4.4289765 -4.4289751 -4.4289556 -4.4289312 -4.4289002 -4.4288793 -4.4288769 -4.4288864 -4.4289007 -4.4289074 -4.4289222 -4.4289336 -4.4289331 -4.4289217 -4.4289188][-4.4289856 -4.4289875 -4.4289751 -4.4289618 -4.4289465 -4.4289393 -4.4289455 -4.42896 -4.4289722 -4.4289746 -4.4289775 -4.4289775 -4.4289713 -4.4289618 -4.4289613]]...]
INFO - root - 2017-12-08 05:32:53.882528: step 10710, loss = 2.28, batch loss = 2.23 (34.5 examples/sec; 0.232 sec/batch; 20h:43m:37s remains)
INFO - root - 2017-12-08 05:32:56.126834: step 10720, loss = 2.28, batch loss = 2.23 (36.7 examples/sec; 0.218 sec/batch; 19h:29m:09s remains)
INFO - root - 2017-12-08 05:32:58.371616: step 10730, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 19h:34m:38s remains)
INFO - root - 2017-12-08 05:33:00.621237: step 10740, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 19h:35m:04s remains)
INFO - root - 2017-12-08 05:33:02.837109: step 10750, loss = 2.28, batch loss = 2.23 (37.0 examples/sec; 0.216 sec/batch; 19h:20m:36s remains)
INFO - root - 2017-12-08 05:33:05.104185: step 10760, loss = 2.28, batch loss = 2.23 (34.9 examples/sec; 0.230 sec/batch; 20h:30m:53s remains)
INFO - root - 2017-12-08 05:33:07.337822: step 10770, loss = 2.28, batch loss = 2.23 (34.4 examples/sec; 0.233 sec/batch; 20h:47m:36s remains)
INFO - root - 2017-12-08 05:33:09.596106: step 10780, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 19h:44m:53s remains)
INFO - root - 2017-12-08 05:33:11.820291: step 10790, loss = 2.28, batch loss = 2.23 (36.7 examples/sec; 0.218 sec/batch; 19h:28m:03s remains)
INFO - root - 2017-12-08 05:33:14.045952: step 10800, loss = 2.28, batch loss = 2.23 (36.7 examples/sec; 0.218 sec/batch; 19h:28m:14s remains)
2017-12-08 05:33:14.321773: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.428936 -4.4289303 -4.4289188 -4.428905 -4.4288855 -4.4288645 -4.4288454 -4.42883 -4.4288259 -4.4288344 -4.4288478 -4.4288435 -4.4288206 -4.4287772 -4.4287095][-4.4289265 -4.4289155 -4.428894 -4.4288688 -4.4288473 -4.4288325 -4.4288173 -4.4288011 -4.4288 -4.4288192 -4.4288416 -4.4288354 -4.4287987 -4.4287381 -4.4286518][-4.4289041 -4.4288859 -4.4288607 -4.42884 -4.4288306 -4.428822 -4.4288058 -4.4287882 -4.4287872 -4.4288092 -4.4288297 -4.4288259 -4.4287915 -4.42873 -4.4286561][-4.4288816 -4.4288645 -4.4288421 -4.4288225 -4.4288068 -4.4287777 -4.4287395 -4.4287095 -4.4287086 -4.4287338 -4.4287663 -4.4287858 -4.4287796 -4.4287481 -4.4287019][-4.4288788 -4.4288635 -4.4288368 -4.4288025 -4.4287572 -4.4286814 -4.4286003 -4.4285688 -4.4285874 -4.4286289 -4.4286766 -4.4287167 -4.4287367 -4.4287372 -4.4287176][-4.428905 -4.4288845 -4.4288387 -4.4287758 -4.4286823 -4.4285369 -4.4283876 -4.42836 -4.4284286 -4.4285049 -4.4285727 -4.4286323 -4.4286809 -4.4287124 -4.428719][-4.42893 -4.428896 -4.4288216 -4.428719 -4.4285793 -4.428381 -4.428174 -4.4281726 -4.4283147 -4.4284244 -4.428493 -4.4285607 -4.4286366 -4.4287066 -4.4287496][-4.4289293 -4.4288836 -4.4287858 -4.428658 -4.4285069 -4.4283285 -4.428164 -4.4282012 -4.4283605 -4.4284558 -4.4285 -4.4285617 -4.428648 -4.42874 -4.4288058][-4.4288836 -4.4288487 -4.4287677 -4.4286585 -4.4285464 -4.4284563 -4.4283991 -4.428441 -4.4285355 -4.4285793 -4.42859 -4.4286389 -4.4287162 -4.4287963 -4.4288511][-4.4287996 -4.4287915 -4.4287572 -4.4287004 -4.4286461 -4.4286323 -4.4286456 -4.42868 -4.4287095 -4.4287047 -4.4286938 -4.4287305 -4.4287844 -4.4288354 -4.4288635][-4.4287052 -4.4287419 -4.4287639 -4.4287562 -4.4287405 -4.4287558 -4.4287877 -4.4288058 -4.4287963 -4.4287634 -4.4287376 -4.4287543 -4.428791 -4.4288278 -4.4288435][-4.4286723 -4.428741 -4.4287767 -4.428781 -4.4287748 -4.4287868 -4.4288168 -4.4288268 -4.4288049 -4.4287648 -4.4287348 -4.4287343 -4.4287596 -4.4287968 -4.4288168][-4.4286942 -4.42875 -4.4287696 -4.4287691 -4.4287615 -4.4287672 -4.428793 -4.4288025 -4.4287834 -4.4287558 -4.4287353 -4.4287338 -4.4287624 -4.4288096 -4.4288425][-4.4287157 -4.4287395 -4.4287467 -4.4287367 -4.4287186 -4.4287171 -4.4287477 -4.4287682 -4.42877 -4.4287691 -4.4287686 -4.4287791 -4.4288173 -4.4288697 -4.4289012][-4.4287052 -4.4287043 -4.4287033 -4.4286971 -4.4286904 -4.4287057 -4.4287548 -4.4287934 -4.4288154 -4.428833 -4.428844 -4.4288578 -4.4288888 -4.428925 -4.4289441]]...]
INFO - root - 2017-12-08 05:33:16.561168: step 10810, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 19h:54m:40s remains)
INFO - root - 2017-12-08 05:33:18.796945: step 10820, loss = 2.28, batch loss = 2.23 (36.8 examples/sec; 0.218 sec/batch; 19h:26m:19s remains)
INFO - root - 2017-12-08 05:33:21.063420: step 10830, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 19h:43m:09s remains)
INFO - root - 2017-12-08 05:33:23.298797: step 10840, loss = 2.28, batch loss = 2.23 (35.0 examples/sec; 0.229 sec/batch; 20h:26m:24s remains)
INFO - root - 2017-12-08 05:33:25.495001: step 10850, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.222 sec/batch; 19h:49m:07s remains)
INFO - root - 2017-12-08 05:33:27.749414: step 10860, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 19h:53m:59s remains)
INFO - root - 2017-12-08 05:33:30.015236: step 10870, loss = 2.28, batch loss = 2.23 (35.1 examples/sec; 0.228 sec/batch; 20h:22m:19s remains)
INFO - root - 2017-12-08 05:33:32.244956: step 10880, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.225 sec/batch; 20h:07m:07s remains)
INFO - root - 2017-12-08 05:33:34.477179: step 10890, loss = 2.28, batch loss = 2.23 (35.4 examples/sec; 0.226 sec/batch; 20h:09m:45s remains)
INFO - root - 2017-12-08 05:33:36.718771: step 10900, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 19h:43m:08s remains)
2017-12-08 05:33:37.019791: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.428822 -4.4287934 -4.4287915 -4.4288392 -4.4289155 -4.4289708 -4.4289937 -4.4290004 -4.4289794 -4.42892 -4.4288416 -4.4287748 -4.4287119 -4.4286685 -4.4286737][-4.4287729 -4.4287381 -4.42874 -4.4287963 -4.4288754 -4.4289336 -4.428968 -4.4289918 -4.4289789 -4.4289179 -4.4288321 -4.4287562 -4.4286914 -4.4286618 -4.4286842][-4.4287081 -4.4286647 -4.4286709 -4.4287271 -4.4287992 -4.4288545 -4.4289002 -4.4289436 -4.4289489 -4.4288988 -4.428823 -4.4287505 -4.4286876 -4.4286656 -4.4286995][-4.4286504 -4.428617 -4.4286389 -4.428689 -4.4287391 -4.4287777 -4.428823 -4.4288793 -4.4288983 -4.4288621 -4.4288068 -4.4287534 -4.4287081 -4.4286981 -4.4287338][-4.4285989 -4.4285884 -4.4286318 -4.4286785 -4.42871 -4.4287353 -4.4287791 -4.4288335 -4.4288497 -4.4288187 -4.4287834 -4.4287586 -4.4287367 -4.4287386 -4.4287739][-4.4285526 -4.428566 -4.4286237 -4.4286652 -4.4286761 -4.4286942 -4.4287381 -4.4287763 -4.42877 -4.4287357 -4.4287257 -4.4287353 -4.4287415 -4.4287539 -4.4287915][-4.4285488 -4.4285755 -4.4286337 -4.4286666 -4.4286675 -4.4286728 -4.4286957 -4.4286981 -4.4286528 -4.4286046 -4.4286175 -4.42866 -4.4286914 -4.4287152 -4.4287605][-4.428576 -4.4286036 -4.4286556 -4.4286852 -4.4286928 -4.4286866 -4.4286761 -4.4286351 -4.4285474 -4.4284773 -4.4285049 -4.428576 -4.4286261 -4.4286551 -4.4287086][-4.4286156 -4.4286451 -4.4286985 -4.4287357 -4.4287462 -4.4287229 -4.428679 -4.4286036 -4.4284816 -4.4283872 -4.428422 -4.4285216 -4.4285927 -4.4286275 -4.4286857][-4.4286437 -4.4286847 -4.4287415 -4.4287848 -4.4287891 -4.4287586 -4.4287062 -4.4286308 -4.4285069 -4.4284067 -4.4284468 -4.4285526 -4.4286208 -4.4286504 -4.4287057][-4.4286776 -4.4287167 -4.4287691 -4.4288092 -4.4288144 -4.4287877 -4.4287457 -4.42869 -4.428596 -4.4285226 -4.4285607 -4.4286437 -4.4286842 -4.4286985 -4.4287443][-4.4287038 -4.4287372 -4.4287796 -4.42881 -4.4288168 -4.428802 -4.4287763 -4.4287381 -4.4286771 -4.42864 -4.4286838 -4.4287424 -4.4287529 -4.4287472 -4.4287758][-4.4287109 -4.4287453 -4.4287839 -4.4288106 -4.4288163 -4.4288092 -4.4287863 -4.4287486 -4.4287047 -4.4286962 -4.4287529 -4.4288077 -4.4287987 -4.4287648 -4.4287734][-4.4287176 -4.4287553 -4.4287949 -4.4288235 -4.4288292 -4.4288135 -4.4287786 -4.4287357 -4.4286995 -4.4287095 -4.4287724 -4.4288149 -4.42878 -4.4287109 -4.4287043][-4.4287062 -4.4287539 -4.4287968 -4.4288244 -4.4288235 -4.4287968 -4.4287534 -4.4287105 -4.42868 -4.4287033 -4.428762 -4.4287815 -4.4287171 -4.4286184 -4.428606]]...]
INFO - root - 2017-12-08 05:33:39.222035: step 10910, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.221 sec/batch; 19h:46m:47s remains)
INFO - root - 2017-12-08 05:33:41.464409: step 10920, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 19h:39m:14s remains)
INFO - root - 2017-12-08 05:33:43.723777: step 10930, loss = 2.28, batch loss = 2.23 (35.1 examples/sec; 0.228 sec/batch; 20h:22m:18s remains)
INFO - root - 2017-12-08 05:33:45.950566: step 10940, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 19h:45m:38s remains)
INFO - root - 2017-12-08 05:33:48.176480: step 10950, loss = 2.28, batch loss = 2.23 (35.7 examples/sec; 0.224 sec/batch; 20h:02m:27s remains)
INFO - root - 2017-12-08 05:33:50.427460: step 10960, loss = 2.28, batch loss = 2.23 (35.7 examples/sec; 0.224 sec/batch; 20h:00m:02s remains)
INFO - root - 2017-12-08 05:33:52.677390: step 10970, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.222 sec/batch; 19h:48m:20s remains)
INFO - root - 2017-12-08 05:33:54.955697: step 10980, loss = 2.28, batch loss = 2.23 (35.1 examples/sec; 0.228 sec/batch; 20h:22m:00s remains)
INFO - root - 2017-12-08 05:33:57.183427: step 10990, loss = 2.28, batch loss = 2.23 (36.0 examples/sec; 0.222 sec/batch; 19h:50m:40s remains)
INFO - root - 2017-12-08 05:33:59.422048: step 11000, loss = 2.28, batch loss = 2.23 (35.8 examples/sec; 0.224 sec/batch; 19h:57m:46s remains)
2017-12-08 05:33:59.725461: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289865 -4.4289718 -4.428957 -4.428947 -4.4289513 -4.428967 -4.42898 -4.4289804 -4.4289742 -4.428968 -4.4289608 -4.4289551 -4.4289532 -4.4289584 -4.4289703][-4.4289622 -4.4289365 -4.4289203 -4.428915 -4.4289241 -4.4289451 -4.42896 -4.4289513 -4.4289327 -4.4289203 -4.4289145 -4.4289126 -4.42892 -4.4289322 -4.4289556][-4.4289141 -4.428874 -4.4288597 -4.4288645 -4.4288745 -4.4288931 -4.4289041 -4.4288855 -4.4288573 -4.4288492 -4.4288459 -4.4288435 -4.4288583 -4.4288845 -4.4289246][-4.4288445 -4.4287815 -4.4287629 -4.4287744 -4.428781 -4.428792 -4.4287953 -4.4287643 -4.4287405 -4.4287524 -4.4287605 -4.4287515 -4.4287653 -4.4288025 -4.4288621][-4.4287729 -4.42869 -4.4286609 -4.4286714 -4.4286714 -4.4286594 -4.42863 -4.4285789 -4.4285846 -4.4286466 -4.4286866 -4.428678 -4.4286838 -4.4287262 -4.4287968][-4.4287205 -4.4286284 -4.4285841 -4.42858 -4.4285569 -4.4284968 -4.4283991 -4.4283113 -4.4283681 -4.4285164 -4.4286213 -4.4286346 -4.4286394 -4.4286771 -4.4287415][-4.4287004 -4.4286118 -4.4285593 -4.4285326 -4.4284782 -4.4283638 -4.428185 -4.4280539 -4.42817 -4.4284048 -4.4285746 -4.4286218 -4.4286313 -4.4286585 -4.4287066][-4.4287071 -4.4286351 -4.4285936 -4.4285593 -4.4284935 -4.4283652 -4.42817 -4.4280481 -4.4281778 -4.4284024 -4.428575 -4.4286351 -4.42865 -4.4286733 -4.4287081][-4.4287333 -4.4286823 -4.4286571 -4.4286304 -4.4285779 -4.4284821 -4.4283442 -4.4282708 -4.4283633 -4.4285092 -4.4286308 -4.4286742 -4.4286852 -4.4287052 -4.4287338][-4.4287658 -4.4287314 -4.4287243 -4.4287133 -4.4286852 -4.4286289 -4.4285522 -4.4285154 -4.4285712 -4.4286518 -4.4287195 -4.4287319 -4.42873 -4.4287453 -4.4287667][-4.428803 -4.428781 -4.4287877 -4.428793 -4.4287872 -4.4287648 -4.4287324 -4.428721 -4.4287496 -4.4287882 -4.4288135 -4.4287977 -4.4287853 -4.4287972 -4.4288135][-4.4288464 -4.4288297 -4.4288406 -4.4288554 -4.4288635 -4.428864 -4.428864 -4.4288716 -4.4288893 -4.428906 -4.428905 -4.428875 -4.4288554 -4.4288588 -4.4288673][-4.4288883 -4.4288754 -4.4288845 -4.4289021 -4.4289131 -4.4289193 -4.42893 -4.4289455 -4.4289618 -4.4289722 -4.4289641 -4.4289317 -4.4289079 -4.4289045 -4.4289074][-4.4289165 -4.4289045 -4.42891 -4.4289255 -4.428937 -4.4289455 -4.4289556 -4.4289694 -4.4289813 -4.4289875 -4.4289813 -4.4289584 -4.4289389 -4.4289346 -4.428937][-4.4289446 -4.4289331 -4.4289327 -4.4289408 -4.4289489 -4.4289579 -4.4289675 -4.428978 -4.4289851 -4.42899 -4.4289856 -4.4289718 -4.4289594 -4.428956 -4.4289579]]...]
INFO - root - 2017-12-08 05:34:01.987309: step 11010, loss = 2.28, batch loss = 2.23 (34.3 examples/sec; 0.233 sec/batch; 20h:48m:34s remains)
INFO - root - 2017-12-08 05:34:04.228065: step 11020, loss = 2.28, batch loss = 2.23 (33.1 examples/sec; 0.241 sec/batch; 21h:33m:51s remains)
INFO - root - 2017-12-08 05:34:06.502736: step 11030, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.221 sec/batch; 19h:45m:54s remains)
INFO - root - 2017-12-08 05:34:08.727958: step 11040, loss = 2.28, batch loss = 2.23 (35.6 examples/sec; 0.225 sec/batch; 20h:04m:54s remains)
INFO - root - 2017-12-08 05:34:10.980022: step 11050, loss = 2.28, batch loss = 2.23 (34.7 examples/sec; 0.230 sec/batch; 20h:34m:03s remains)
INFO - root - 2017-12-08 05:34:13.213456: step 11060, loss = 2.28, batch loss = 2.23 (35.8 examples/sec; 0.223 sec/batch; 19h:57m:09s remains)
INFO - root - 2017-12-08 05:34:15.436601: step 11070, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 19h:38m:45s remains)
INFO - root - 2017-12-08 05:34:17.677095: step 11080, loss = 2.28, batch loss = 2.23 (34.8 examples/sec; 0.230 sec/batch; 20h:32m:24s remains)
INFO - root - 2017-12-08 05:34:19.896954: step 11090, loss = 2.28, batch loss = 2.23 (37.2 examples/sec; 0.215 sec/batch; 19h:11m:13s remains)
INFO - root - 2017-12-08 05:34:22.132512: step 11100, loss = 2.28, batch loss = 2.23 (36.6 examples/sec; 0.218 sec/batch; 19h:30m:23s remains)
2017-12-08 05:34:22.413762: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288039 -4.4287767 -4.4287329 -4.4287181 -4.4287453 -4.4287825 -4.4288182 -4.428844 -4.4288487 -4.42883 -4.4288054 -4.4288063 -4.4288177 -4.4288225 -4.4288216][-4.4287686 -4.4287291 -4.4286671 -4.4286551 -4.4286914 -4.4287372 -4.428781 -4.42881 -4.4288192 -4.4288139 -4.4288096 -4.4288154 -4.428823 -4.4288177 -4.4288111][-4.4287663 -4.4287167 -4.4286385 -4.4286213 -4.4286528 -4.4286942 -4.4287391 -4.4287658 -4.4287887 -4.428813 -4.4288254 -4.4288235 -4.4288125 -4.4287882 -4.428781][-4.42876 -4.4287019 -4.4286156 -4.4285822 -4.4285989 -4.4286342 -4.42868 -4.4287109 -4.4287491 -4.4287915 -4.4288044 -4.4287906 -4.4287682 -4.4287467 -4.4287558][-4.4287467 -4.4286819 -4.4285889 -4.428535 -4.4285345 -4.4285607 -4.4286041 -4.4286437 -4.4287057 -4.428771 -4.4287777 -4.4287543 -4.4287362 -4.42873 -4.4287581][-4.4287252 -4.4286623 -4.4285636 -4.4284945 -4.428473 -4.4284806 -4.4285131 -4.4285564 -4.4286404 -4.4287252 -4.4287472 -4.428741 -4.4287243 -4.4287152 -4.4287372][-4.4286752 -4.428627 -4.4285355 -4.42846 -4.4284086 -4.4283676 -4.4283643 -4.4284062 -4.4285231 -4.4286394 -4.4286909 -4.4286995 -4.4286742 -4.4286509 -4.4286633][-4.4286094 -4.428575 -4.4285064 -4.4284286 -4.4283309 -4.4282284 -4.4281769 -4.4282146 -4.4283781 -4.4285431 -4.4286232 -4.4286318 -4.4286008 -4.4285765 -4.4285941][-4.4285641 -4.4285407 -4.4284854 -4.4284048 -4.4282818 -4.4281521 -4.4280796 -4.4281192 -4.428299 -4.4284697 -4.4285522 -4.4285612 -4.4285398 -4.4285274 -4.4285507][-4.4285455 -4.4285393 -4.4285035 -4.4284406 -4.4283371 -4.4282346 -4.42818 -4.4282203 -4.4283552 -4.4284716 -4.4285245 -4.4285345 -4.4285197 -4.428515 -4.428544][-4.4285588 -4.4285741 -4.428566 -4.42853 -4.4284649 -4.4283957 -4.4283504 -4.4283743 -4.4284453 -4.4284997 -4.4285274 -4.4285364 -4.4285178 -4.4285154 -4.42856][-4.4286323 -4.42866 -4.42867 -4.4286542 -4.4286175 -4.4285707 -4.4285316 -4.4285374 -4.4285593 -4.4285707 -4.4285793 -4.4285779 -4.4285574 -4.4285631 -4.4286151][-4.4287243 -4.4287539 -4.4287696 -4.4287682 -4.4287505 -4.4287195 -4.4286928 -4.4286904 -4.428688 -4.428689 -4.4286957 -4.4286919 -4.4286737 -4.4286828 -4.4287248][-4.4288144 -4.4288411 -4.4288549 -4.4288578 -4.4288516 -4.428833 -4.4288177 -4.4288135 -4.4288068 -4.4288096 -4.4288158 -4.4288125 -4.4287968 -4.4287987 -4.428823][-4.4288969 -4.4289165 -4.428925 -4.4289241 -4.4289165 -4.4289021 -4.4288926 -4.4288921 -4.4288893 -4.4288931 -4.4288983 -4.4288964 -4.4288855 -4.428884 -4.4289002]]...]
INFO - root - 2017-12-08 05:34:24.673195: step 11110, loss = 2.28, batch loss = 2.23 (35.8 examples/sec; 0.224 sec/batch; 19h:57m:56s remains)
INFO - root - 2017-12-08 05:34:26.934793: step 11120, loss = 2.28, batch loss = 2.23 (35.4 examples/sec; 0.226 sec/batch; 20h:09m:10s remains)
INFO - root - 2017-12-08 05:34:29.176688: step 11130, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 19h:45m:06s remains)
INFO - root - 2017-12-08 05:34:31.401207: step 11140, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 19h:34m:12s remains)
INFO - root - 2017-12-08 05:34:33.643208: step 11150, loss = 2.28, batch loss = 2.23 (35.0 examples/sec; 0.228 sec/batch; 20h:22m:28s remains)
INFO - root - 2017-12-08 05:34:35.862593: step 11160, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 19h:43m:37s remains)
INFO - root - 2017-12-08 05:34:38.087932: step 11170, loss = 2.28, batch loss = 2.23 (34.1 examples/sec; 0.234 sec/batch; 20h:54m:47s remains)
INFO - root - 2017-12-08 05:34:40.326937: step 11180, loss = 2.28, batch loss = 2.23 (34.6 examples/sec; 0.231 sec/batch; 20h:37m:02s remains)
INFO - root - 2017-12-08 05:34:42.582478: step 11190, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 19h:33m:26s remains)
INFO - root - 2017-12-08 05:34:44.809232: step 11200, loss = 2.28, batch loss = 2.23 (36.0 examples/sec; 0.222 sec/batch; 19h:48m:54s remains)
2017-12-08 05:34:45.094735: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288387 -4.428863 -4.4288812 -4.4288826 -4.428884 -4.4288917 -4.4289107 -4.428925 -4.4289279 -4.428915 -4.4288855 -4.4288526 -4.428834 -4.428833 -4.428844][-4.4288177 -4.428844 -4.428865 -4.4288688 -4.4288697 -4.4288783 -4.4289021 -4.428926 -4.4289393 -4.42893 -4.4289002 -4.428864 -4.4288411 -4.4288383 -4.4288526][-4.4287944 -4.4288187 -4.428844 -4.4288435 -4.4288354 -4.4288363 -4.4288607 -4.4288931 -4.428916 -4.4289136 -4.428894 -4.4288683 -4.428853 -4.4288568 -4.4288754][-4.428793 -4.4288116 -4.4288278 -4.4288125 -4.42879 -4.42878 -4.4288054 -4.4288492 -4.4288836 -4.4288912 -4.4288845 -4.4288774 -4.4288812 -4.4288955 -4.42892][-4.42882 -4.4288344 -4.4288316 -4.4287868 -4.4287386 -4.4287062 -4.4287128 -4.4287577 -4.4288135 -4.4288507 -4.4288678 -4.42888 -4.4289017 -4.4289289 -4.4289551][-4.4288554 -4.42887 -4.428854 -4.4287806 -4.4286966 -4.4286227 -4.428576 -4.4285975 -4.4286814 -4.4287691 -4.4288282 -4.428865 -4.4289 -4.428936 -4.4289637][-4.428874 -4.42889 -4.4288726 -4.4287853 -4.4286666 -4.4285417 -4.4284105 -4.428369 -4.4284806 -4.4286427 -4.42876 -4.4288268 -4.4288735 -4.4289136 -4.4289446][-4.4288831 -4.4289041 -4.4288926 -4.4288111 -4.4286742 -4.4285064 -4.42829 -4.4281564 -4.428277 -4.4285097 -4.4286866 -4.4287853 -4.4288344 -4.4288707 -4.4289041][-4.4288874 -4.4289174 -4.4289274 -4.4288673 -4.4287367 -4.4285669 -4.4283433 -4.4281754 -4.4282584 -4.4284868 -4.4286656 -4.4287567 -4.4287853 -4.4288034 -4.4288406][-4.428894 -4.428937 -4.428968 -4.4289351 -4.4288344 -4.4287076 -4.4285512 -4.4284282 -4.4284539 -4.4285874 -4.4286995 -4.4287381 -4.42871 -4.4286804 -4.4287233][-4.428894 -4.4289479 -4.4289961 -4.4289842 -4.4289112 -4.4288259 -4.4287329 -4.4286618 -4.4286618 -4.4287086 -4.4287357 -4.4286962 -4.4285712 -4.4284687 -4.428525][-4.4288898 -4.4289465 -4.4290009 -4.4290013 -4.4289489 -4.4288912 -4.428843 -4.4288106 -4.4288039 -4.4288015 -4.4287639 -4.4286523 -4.4284258 -4.4282212 -4.4282813][-4.4288883 -4.4289412 -4.4289937 -4.4290018 -4.42897 -4.4289317 -4.4289079 -4.4288993 -4.4289012 -4.4288826 -4.4288139 -4.4286742 -4.4284272 -4.4281864 -4.4282088][-4.4288874 -4.428937 -4.4289861 -4.429 -4.4289856 -4.4289641 -4.4289565 -4.4289565 -4.4289627 -4.4289474 -4.4288821 -4.4287682 -4.4285874 -4.4284167 -4.4283977][-4.428884 -4.4289303 -4.4289775 -4.4289937 -4.4289865 -4.4289751 -4.428977 -4.4289832 -4.4289951 -4.4289865 -4.4289327 -4.4288526 -4.4287481 -4.42866 -4.4286327]]...]
INFO - root - 2017-12-08 05:34:47.312374: step 11210, loss = 2.28, batch loss = 2.23 (34.0 examples/sec; 0.236 sec/batch; 21h:01m:23s remains)
INFO - root - 2017-12-08 05:34:49.537880: step 11220, loss = 2.28, batch loss = 2.23 (37.6 examples/sec; 0.213 sec/batch; 18h:59m:51s remains)
INFO - root - 2017-12-08 05:34:51.771257: step 11230, loss = 2.28, batch loss = 2.23 (36.6 examples/sec; 0.218 sec/batch; 19h:29m:45s remains)
INFO - root - 2017-12-08 05:34:53.997264: step 11240, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 19h:52m:34s remains)
INFO - root - 2017-12-08 05:34:56.252331: step 11250, loss = 2.28, batch loss = 2.23 (35.7 examples/sec; 0.224 sec/batch; 19h:58m:09s remains)
INFO - root - 2017-12-08 05:34:58.477776: step 11260, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.222 sec/batch; 19h:48m:00s remains)
INFO - root - 2017-12-08 05:35:00.708535: step 11270, loss = 2.28, batch loss = 2.23 (35.7 examples/sec; 0.224 sec/batch; 20h:01m:18s remains)
INFO - root - 2017-12-08 05:35:02.966119: step 11280, loss = 2.28, batch loss = 2.23 (36.7 examples/sec; 0.218 sec/batch; 19h:28m:25s remains)
INFO - root - 2017-12-08 05:35:05.207902: step 11290, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.220 sec/batch; 19h:38m:48s remains)
INFO - root - 2017-12-08 05:35:07.427196: step 11300, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.219 sec/batch; 19h:35m:00s remains)
2017-12-08 05:35:07.692299: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4287267 -4.4287362 -4.428719 -4.4287124 -4.4287052 -4.4286838 -4.4286776 -4.4287009 -4.4287171 -4.4286842 -4.428617 -4.4285669 -4.4285789 -4.428627 -4.4286857][-4.4286976 -4.4287081 -4.4286933 -4.4286876 -4.4286737 -4.4286494 -4.4286494 -4.4286804 -4.4287033 -4.4286723 -4.4285979 -4.4285331 -4.4285502 -4.4286017 -4.4286561][-4.4286342 -4.4286489 -4.4286418 -4.4286394 -4.428628 -4.4286103 -4.4286203 -4.4286523 -4.428679 -4.42866 -4.42859 -4.4285264 -4.428556 -4.4286108 -4.4286637][-4.4285722 -4.4285913 -4.4286 -4.4286056 -4.4285979 -4.4285865 -4.4285936 -4.4286132 -4.4286342 -4.428628 -4.4285789 -4.4285407 -4.4285827 -4.4286394 -4.4286919][-4.4285488 -4.4285665 -4.4285841 -4.4285903 -4.428575 -4.4285588 -4.4285512 -4.4285541 -4.4285679 -4.42858 -4.4285669 -4.4285626 -4.4286175 -4.4286742 -4.4287262][-4.42854 -4.4285607 -4.4285865 -4.4285936 -4.4285645 -4.4285297 -4.4285011 -4.428483 -4.4284883 -4.4285116 -4.4285254 -4.4285469 -4.4286194 -4.4286823 -4.4287324][-4.4285426 -4.4285746 -4.4286075 -4.4286065 -4.4285588 -4.4284968 -4.428441 -4.4284029 -4.4283986 -4.4284177 -4.4284277 -4.4284477 -4.4285378 -4.4286146 -4.4286704][-4.4285722 -4.4286151 -4.4286413 -4.4286208 -4.428555 -4.4284725 -4.4284 -4.4283519 -4.4283381 -4.4283452 -4.4283381 -4.4283423 -4.4284463 -4.4285378 -4.4285922][-4.4286127 -4.4286537 -4.4286642 -4.4286284 -4.4285569 -4.4284759 -4.4284024 -4.428359 -4.4283504 -4.4283624 -4.4283619 -4.4283719 -4.4284635 -4.4285455 -4.4285927][-4.4286385 -4.4286685 -4.4286671 -4.4286242 -4.4285641 -4.4285011 -4.42844 -4.4284158 -4.4284253 -4.4284463 -4.428453 -4.4284616 -4.4285383 -4.4286127 -4.4286571][-4.428637 -4.428669 -4.4286661 -4.4286304 -4.4285889 -4.428544 -4.4285 -4.428483 -4.4284968 -4.4285116 -4.4285164 -4.4285226 -4.4285927 -4.4286637 -4.4287105][-4.4286275 -4.4286604 -4.4286571 -4.4286323 -4.4286151 -4.4285946 -4.428565 -4.42854 -4.4285393 -4.4285359 -4.4285312 -4.4285398 -4.4286075 -4.4286761 -4.42873][-4.4286375 -4.4286628 -4.4286523 -4.4286361 -4.4286432 -4.4286447 -4.4286222 -4.4285884 -4.4285741 -4.4285564 -4.428535 -4.42854 -4.4286075 -4.4286771 -4.4287271][-4.4286418 -4.4286652 -4.4286566 -4.4286466 -4.4286733 -4.428689 -4.4286656 -4.4286308 -4.4286151 -4.428596 -4.4285722 -4.4285789 -4.4286408 -4.4286981 -4.4287281][-4.4286408 -4.4286647 -4.4286652 -4.4286642 -4.4286995 -4.4287167 -4.4286895 -4.4286523 -4.4286408 -4.428638 -4.428637 -4.4286566 -4.4287124 -4.4287481 -4.42875]]...]
INFO - root - 2017-12-08 05:35:09.923424: step 11310, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 19h:53m:24s remains)
INFO - root - 2017-12-08 05:35:12.153528: step 11320, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 19h:33m:30s remains)
INFO - root - 2017-12-08 05:35:14.367379: step 11330, loss = 2.28, batch loss = 2.23 (36.8 examples/sec; 0.217 sec/batch; 19h:24m:00s remains)
INFO - root - 2017-12-08 05:35:16.627969: step 11340, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 19h:35m:31s remains)
INFO - root - 2017-12-08 05:35:18.871035: step 11350, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.220 sec/batch; 19h:38m:56s remains)
INFO - root - 2017-12-08 05:35:21.181298: step 11360, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 19h:37m:37s remains)
INFO - root - 2017-12-08 05:35:23.428802: step 11370, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.225 sec/batch; 20h:06m:41s remains)
INFO - root - 2017-12-08 05:35:25.665719: step 11380, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 19h:51m:33s remains)
INFO - root - 2017-12-08 05:35:27.886147: step 11390, loss = 2.28, batch loss = 2.23 (37.4 examples/sec; 0.214 sec/batch; 19h:06m:00s remains)
INFO - root - 2017-12-08 05:35:30.117183: step 11400, loss = 2.28, batch loss = 2.23 (35.4 examples/sec; 0.226 sec/batch; 20h:10m:36s remains)
2017-12-08 05:35:30.410364: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289508 -4.4289474 -4.4289527 -4.4289556 -4.4289546 -4.4289541 -4.4289508 -4.428946 -4.4289451 -4.4289527 -4.4289637 -4.4289746 -4.4289851 -4.428997 -4.4290094][-4.4289184 -4.4289212 -4.4289308 -4.4289331 -4.4289308 -4.4289289 -4.4289265 -4.4289217 -4.4289179 -4.4289231 -4.4289336 -4.4289465 -4.4289622 -4.42898 -4.4289975][-4.4288774 -4.4288845 -4.428895 -4.428894 -4.4288878 -4.4288793 -4.4288726 -4.42887 -4.4288721 -4.4288826 -4.4288974 -4.4289131 -4.4289355 -4.4289608 -4.4289842][-4.4288187 -4.4288197 -4.4288244 -4.4288187 -4.4288073 -4.4287891 -4.4287724 -4.4287744 -4.4287872 -4.4288068 -4.4288287 -4.4288549 -4.4288898 -4.4289274 -4.4289613][-4.4287505 -4.4287381 -4.4287276 -4.4287105 -4.4286966 -4.4286776 -4.4286609 -4.4286766 -4.4287 -4.4287214 -4.4287462 -4.428781 -4.4288282 -4.4288788 -4.4289236][-4.4287028 -4.4286747 -4.4286461 -4.428617 -4.4285984 -4.4285865 -4.4285855 -4.4286141 -4.4286337 -4.4286408 -4.4286523 -4.4286866 -4.4287434 -4.4288063 -4.4288673][-4.4286795 -4.4286447 -4.4286065 -4.4285645 -4.4285393 -4.4285464 -4.4285707 -4.4286027 -4.4286036 -4.4285846 -4.4285731 -4.4285994 -4.4286561 -4.4287233 -4.4287972][-4.4286427 -4.4286065 -4.4285617 -4.4285221 -4.4285169 -4.428556 -4.4285994 -4.4286203 -4.428606 -4.4285803 -4.4285583 -4.4285693 -4.4286127 -4.4286714 -4.4287438][-4.4286036 -4.4285707 -4.4285216 -4.4284897 -4.4285111 -4.4285769 -4.4286246 -4.4286389 -4.4286237 -4.4286075 -4.4285927 -4.4285994 -4.428628 -4.4286714 -4.4287257][-4.4285774 -4.4285583 -4.4285197 -4.4284992 -4.4285278 -4.428587 -4.4286246 -4.4286389 -4.4286356 -4.4286375 -4.4286466 -4.4286633 -4.4286871 -4.4287162 -4.4287519][-4.428586 -4.4285803 -4.4285617 -4.428555 -4.4285769 -4.428607 -4.4286194 -4.4286289 -4.4286385 -4.4286652 -4.4286981 -4.4287214 -4.4287429 -4.4287658 -4.428792][-4.4286318 -4.4286346 -4.4286284 -4.4286318 -4.4286447 -4.4286451 -4.4286284 -4.4286308 -4.4286518 -4.4287019 -4.4287481 -4.4287744 -4.4287977 -4.42882 -4.4288468][-4.4287062 -4.42871 -4.4287071 -4.4287148 -4.4287229 -4.4287028 -4.4286652 -4.4286547 -4.4286761 -4.4287362 -4.4287906 -4.4288239 -4.428854 -4.4288812 -4.4289088][-4.4287992 -4.4288025 -4.4287958 -4.4287996 -4.4288 -4.4287744 -4.4287329 -4.4287152 -4.428731 -4.4287882 -4.4288416 -4.4288797 -4.4289155 -4.428946 -4.42897][-4.4288926 -4.4288955 -4.4288845 -4.4288774 -4.4288721 -4.4288511 -4.4288154 -4.4287915 -4.4287968 -4.4288406 -4.4288874 -4.4289269 -4.4289641 -4.4289927 -4.4290109]]...]
INFO - root - 2017-12-08 05:35:32.651574: step 11410, loss = 2.28, batch loss = 2.23 (35.4 examples/sec; 0.226 sec/batch; 20h:07m:48s remains)
INFO - root - 2017-12-08 05:35:34.885070: step 11420, loss = 2.28, batch loss = 2.23 (35.4 examples/sec; 0.226 sec/batch; 20h:10m:47s remains)
INFO - root - 2017-12-08 05:35:37.146861: step 11430, loss = 2.28, batch loss = 2.23 (35.8 examples/sec; 0.224 sec/batch; 19h:56m:16s remains)
INFO - root - 2017-12-08 05:35:39.378033: step 11440, loss = 2.28, batch loss = 2.23 (35.2 examples/sec; 0.227 sec/batch; 20h:15m:40s remains)
INFO - root - 2017-12-08 05:35:41.604153: step 11450, loss = 2.28, batch loss = 2.23 (34.7 examples/sec; 0.231 sec/batch; 20h:35m:21s remains)
INFO - root - 2017-12-08 05:35:43.861233: step 11460, loss = 2.28, batch loss = 2.23 (35.1 examples/sec; 0.228 sec/batch; 20h:19m:38s remains)
INFO - root - 2017-12-08 05:35:46.093703: step 11470, loss = 2.28, batch loss = 2.23 (35.6 examples/sec; 0.225 sec/batch; 20h:02m:50s remains)
INFO - root - 2017-12-08 05:35:48.335587: step 11480, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 19h:43m:56s remains)
INFO - root - 2017-12-08 05:35:50.557591: step 11490, loss = 2.28, batch loss = 2.23 (34.2 examples/sec; 0.234 sec/batch; 20h:51m:11s remains)
INFO - root - 2017-12-08 05:35:52.810269: step 11500, loss = 2.28, batch loss = 2.23 (35.4 examples/sec; 0.226 sec/batch; 20h:09m:27s remains)
2017-12-08 05:35:53.114714: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289541 -4.4289532 -4.4289408 -4.4289217 -4.4288969 -4.4288707 -4.4288545 -4.4288449 -4.4288478 -4.4288497 -4.4288487 -4.4288492 -4.4288578 -4.4288721 -4.4288912][-4.4289513 -4.428946 -4.4289293 -4.4289055 -4.4288759 -4.42884 -4.4288139 -4.4288 -4.4288082 -4.428823 -4.4288387 -4.4288564 -4.4288735 -4.4288874 -4.4288974][-4.4289474 -4.4289393 -4.4289179 -4.4288898 -4.4288573 -4.4288139 -4.4287767 -4.4287529 -4.428761 -4.428791 -4.4288273 -4.4288659 -4.428896 -4.4289112 -4.4289122][-4.4289479 -4.4289422 -4.4289212 -4.4288907 -4.4288507 -4.4287891 -4.4287276 -4.4286876 -4.4286914 -4.4287357 -4.4287968 -4.4288578 -4.4289045 -4.4289222 -4.4289203][-4.4289489 -4.4289412 -4.4289145 -4.4288745 -4.4288163 -4.4287271 -4.4286394 -4.428586 -4.428596 -4.4286604 -4.4287453 -4.428822 -4.4288774 -4.4289017 -4.4289045][-4.428936 -4.4289169 -4.428875 -4.4288163 -4.4287362 -4.428618 -4.4284935 -4.4284163 -4.4284515 -4.4285588 -4.4286747 -4.4287639 -4.4288244 -4.4288583 -4.4288645][-4.428895 -4.4288535 -4.4287848 -4.4287047 -4.428606 -4.4284558 -4.4282794 -4.4281526 -4.4282131 -4.4283905 -4.4285502 -4.4286623 -4.4287434 -4.4287963 -4.4288116][-4.4288416 -4.4287734 -4.4286752 -4.4285703 -4.4284511 -4.4282832 -4.4280758 -4.4279089 -4.427999 -4.428237 -4.4284329 -4.4285636 -4.4286656 -4.4287434 -4.4287758][-4.4288025 -4.4287224 -4.4286127 -4.4285021 -4.4283886 -4.4282527 -4.4280949 -4.4279838 -4.4280696 -4.4282703 -4.4284425 -4.4285631 -4.4286652 -4.4287519 -4.4287987][-4.4288177 -4.4287419 -4.4286427 -4.4285483 -4.4284625 -4.4283805 -4.4283023 -4.4282684 -4.4283304 -4.4284525 -4.4285693 -4.4286652 -4.4287519 -4.4288249 -4.42887][-4.4288716 -4.4288116 -4.42873 -4.4286561 -4.4286027 -4.4285669 -4.4285474 -4.4285545 -4.4285965 -4.4286571 -4.4287224 -4.4287891 -4.4288559 -4.4289112 -4.4289441][-4.4289222 -4.4288826 -4.4288225 -4.42877 -4.428741 -4.4287333 -4.4287438 -4.4287658 -4.4287949 -4.4288216 -4.4288478 -4.4288874 -4.4289341 -4.4289713 -4.4289904][-4.4289522 -4.4289346 -4.4289021 -4.4288726 -4.4288621 -4.4288664 -4.4288807 -4.4289017 -4.4289179 -4.4289265 -4.4289331 -4.4289532 -4.4289813 -4.4290018 -4.42901][-4.42897 -4.4289713 -4.42896 -4.4289446 -4.42894 -4.4289446 -4.428957 -4.4289708 -4.4289784 -4.4289789 -4.428978 -4.4289851 -4.4289985 -4.4290075 -4.4290085][-4.4289775 -4.4289856 -4.4289823 -4.4289737 -4.4289684 -4.4289703 -4.4289775 -4.4289856 -4.4289889 -4.428988 -4.4289865 -4.4289889 -4.4289942 -4.4289985 -4.4289989]]...]
INFO - root - 2017-12-08 05:35:55.340641: step 11510, loss = 2.28, batch loss = 2.23 (36.0 examples/sec; 0.222 sec/batch; 19h:49m:04s remains)
INFO - root - 2017-12-08 05:35:57.609609: step 11520, loss = 2.28, batch loss = 2.23 (36.8 examples/sec; 0.217 sec/batch; 19h:22m:41s remains)
INFO - root - 2017-12-08 05:35:59.841410: step 11530, loss = 2.28, batch loss = 2.23 (37.7 examples/sec; 0.212 sec/batch; 18h:54m:25s remains)
INFO - root - 2017-12-08 05:36:02.077838: step 11540, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 19h:51m:16s remains)
INFO - root - 2017-12-08 05:36:04.316071: step 11550, loss = 2.28, batch loss = 2.23 (35.4 examples/sec; 0.226 sec/batch; 20h:07m:59s remains)
INFO - root - 2017-12-08 05:36:06.532054: step 11560, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 19h:32m:51s remains)
INFO - root - 2017-12-08 05:36:08.787465: step 11570, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 19h:50m:41s remains)
INFO - root - 2017-12-08 05:36:11.038491: step 11580, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 19h:33m:28s remains)
INFO - root - 2017-12-08 05:36:13.269106: step 11590, loss = 2.28, batch loss = 2.23 (37.9 examples/sec; 0.211 sec/batch; 18h:48m:06s remains)
INFO - root - 2017-12-08 05:36:15.525557: step 11600, loss = 2.28, batch loss = 2.23 (34.7 examples/sec; 0.230 sec/batch; 20h:31m:44s remains)
2017-12-08 05:36:15.818812: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4287443 -4.4287748 -4.4288063 -4.4288325 -4.4288464 -4.42885 -4.4288521 -4.4288554 -4.4288583 -4.4288583 -4.4288597 -4.428853 -4.4288383 -4.428834 -4.4288287][-4.4286675 -4.4287009 -4.4287405 -4.4287748 -4.4287982 -4.4288139 -4.4288273 -4.4288392 -4.4288487 -4.4288545 -4.4288597 -4.428854 -4.4288397 -4.4288354 -4.4288387][-4.4286294 -4.4286394 -4.4286618 -4.4286895 -4.4287152 -4.4287429 -4.4287729 -4.4287996 -4.4288225 -4.4288378 -4.4288473 -4.428843 -4.4288292 -4.4288254 -4.4288359][-4.4286389 -4.4286289 -4.4286294 -4.4286351 -4.4286423 -4.4286647 -4.4286995 -4.4287362 -4.4287729 -4.428803 -4.4288239 -4.4288263 -4.4288158 -4.4288139 -4.4288273][-4.4286613 -4.4286432 -4.4286404 -4.4286375 -4.428628 -4.428627 -4.4286408 -4.4286642 -4.4286942 -4.4287262 -4.4287562 -4.4287705 -4.4287705 -4.4287744 -4.4287915][-4.4287033 -4.4286776 -4.4286675 -4.4286656 -4.4286528 -4.4286323 -4.4286237 -4.4286275 -4.4286361 -4.4286451 -4.4286671 -4.4286804 -4.4286776 -4.4286819 -4.4287052][-4.4287405 -4.4287105 -4.4286947 -4.4286971 -4.4286861 -4.4286594 -4.4286418 -4.4286346 -4.428627 -4.428616 -4.4286261 -4.4286208 -4.428586 -4.4285588 -4.4285712][-4.4287443 -4.4287167 -4.4287047 -4.4287162 -4.4287128 -4.4286876 -4.42867 -4.4286628 -4.4286528 -4.4286294 -4.4286251 -4.4286075 -4.4285512 -4.4284987 -4.4284854][-4.4287648 -4.4287391 -4.4287248 -4.4287348 -4.4287329 -4.4287114 -4.4287009 -4.4287047 -4.4287028 -4.4286785 -4.428659 -4.4286332 -4.4285755 -4.4285235 -4.4285045][-4.4288144 -4.4287949 -4.4287753 -4.428772 -4.4287643 -4.4287457 -4.42874 -4.4287519 -4.4287543 -4.4287376 -4.428721 -4.4287019 -4.4286575 -4.4286165 -4.4285994][-4.4288492 -4.4288321 -4.4288135 -4.4288034 -4.4287968 -4.4287844 -4.428781 -4.4287963 -4.428803 -4.4287934 -4.4287829 -4.4287744 -4.4287472 -4.4287219 -4.4287157][-4.4288831 -4.4288726 -4.4288626 -4.4288578 -4.4288559 -4.428853 -4.428854 -4.4288673 -4.4288707 -4.4288573 -4.428843 -4.4288359 -4.4288235 -4.4288163 -4.428823][-4.4289155 -4.4289145 -4.4289136 -4.4289179 -4.4289184 -4.4289184 -4.42892 -4.4289279 -4.4289293 -4.4289155 -4.4289031 -4.4289002 -4.4288983 -4.4289036 -4.4289155][-4.4289303 -4.4289346 -4.4289379 -4.4289412 -4.4289403 -4.4289403 -4.4289436 -4.42895 -4.4289513 -4.4289455 -4.4289393 -4.4289412 -4.4289465 -4.4289546 -4.4289646][-4.4289508 -4.4289551 -4.4289579 -4.4289603 -4.4289603 -4.4289618 -4.4289641 -4.4289651 -4.4289627 -4.4289579 -4.4289532 -4.4289546 -4.4289603 -4.4289684 -4.4289751]]...]
INFO - root - 2017-12-08 05:36:18.051608: step 11610, loss = 2.28, batch loss = 2.23 (36.6 examples/sec; 0.219 sec/batch; 19h:29m:25s remains)
INFO - root - 2017-12-08 05:36:20.292098: step 11620, loss = 2.28, batch loss = 2.23 (35.1 examples/sec; 0.228 sec/batch; 20h:19m:06s remains)
INFO - root - 2017-12-08 05:36:22.547581: step 11630, loss = 2.28, batch loss = 2.23 (36.0 examples/sec; 0.222 sec/batch; 19h:49m:51s remains)
INFO - root - 2017-12-08 05:36:24.776388: step 11640, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.220 sec/batch; 19h:38m:09s remains)
INFO - root - 2017-12-08 05:36:26.998737: step 11650, loss = 2.28, batch loss = 2.23 (35.4 examples/sec; 0.226 sec/batch; 20h:09m:30s remains)
INFO - root - 2017-12-08 05:36:29.267708: step 11660, loss = 2.28, batch loss = 2.23 (34.1 examples/sec; 0.235 sec/batch; 20h:55m:59s remains)
INFO - root - 2017-12-08 05:36:31.489974: step 11670, loss = 2.28, batch loss = 2.23 (37.4 examples/sec; 0.214 sec/batch; 19h:05m:05s remains)
INFO - root - 2017-12-08 05:36:33.720005: step 11680, loss = 2.28, batch loss = 2.23 (36.7 examples/sec; 0.218 sec/batch; 19h:25m:10s remains)
INFO - root - 2017-12-08 05:36:35.967374: step 11690, loss = 2.28, batch loss = 2.23 (34.9 examples/sec; 0.229 sec/batch; 20h:25m:46s remains)
INFO - root - 2017-12-08 05:36:38.225409: step 11700, loss = 2.28, batch loss = 2.23 (35.1 examples/sec; 0.228 sec/batch; 20h:18m:25s remains)
2017-12-08 05:36:38.557521: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.428936 -4.4289207 -4.4289103 -4.4288926 -4.4288816 -4.4288855 -4.4288864 -4.4288774 -4.428853 -4.4288287 -4.4288187 -4.4288268 -4.4288492 -4.428874 -4.4288893][-4.4289122 -4.4288898 -4.4288754 -4.4288535 -4.4288359 -4.4288349 -4.4288325 -4.4288216 -4.4287906 -4.4287524 -4.4287324 -4.4287376 -4.4287682 -4.4288015 -4.4288182][-4.4288974 -4.4288673 -4.428843 -4.4288111 -4.4287844 -4.42878 -4.4287777 -4.42877 -4.4287457 -4.4287105 -4.4286819 -4.4286728 -4.4286981 -4.4287286 -4.4287467][-4.4288797 -4.4288387 -4.4287992 -4.4287543 -4.4287214 -4.4287143 -4.4287128 -4.4287124 -4.4287066 -4.428689 -4.4286585 -4.4286413 -4.4286585 -4.4286814 -4.4286981][-4.4288507 -4.4288025 -4.4287558 -4.4287081 -4.4286642 -4.4286427 -4.4286366 -4.4286437 -4.428658 -4.4286566 -4.4286404 -4.4286318 -4.4286485 -4.4286661 -4.428688][-4.4288297 -4.4287786 -4.4287329 -4.428679 -4.428618 -4.4285665 -4.4285288 -4.4285283 -4.4285612 -4.428586 -4.4285979 -4.4286118 -4.4286346 -4.4286561 -4.4286861][-4.4288235 -4.4287672 -4.4287181 -4.4286489 -4.4285579 -4.4284549 -4.4283576 -4.4283409 -4.4284158 -4.4284878 -4.4285431 -4.4285889 -4.4286237 -4.4286528 -4.4286876][-4.42882 -4.4287591 -4.4287028 -4.4286189 -4.4285021 -4.4283547 -4.4281898 -4.4281554 -4.42828 -4.4284048 -4.4285045 -4.4285803 -4.4286232 -4.4286571 -4.4286919][-4.4288244 -4.428762 -4.4286981 -4.4286113 -4.4285011 -4.4283657 -4.4282022 -4.4281654 -4.4282889 -4.4284229 -4.4285307 -4.4286089 -4.4286447 -4.4286685 -4.4286947][-4.4288354 -4.4287744 -4.4287143 -4.4286427 -4.4285684 -4.4284906 -4.4283943 -4.4283648 -4.4284334 -4.4285235 -4.4286013 -4.4286618 -4.4286838 -4.428689 -4.4286971][-4.4288511 -4.4287982 -4.4287481 -4.4286976 -4.4286575 -4.4286256 -4.4285827 -4.4285545 -4.4285707 -4.428617 -4.4286637 -4.4287024 -4.428709 -4.4287004 -4.4286942][-4.42887 -4.4288259 -4.42879 -4.4287629 -4.4287477 -4.4287367 -4.4287128 -4.4286761 -4.4286547 -4.4286709 -4.4287 -4.4287233 -4.4287229 -4.4287109 -4.4287028][-4.4288874 -4.4288507 -4.4288254 -4.4288139 -4.42881 -4.4288039 -4.4287777 -4.4287286 -4.4286861 -4.4286847 -4.4287047 -4.4287205 -4.42873 -4.4287329 -4.4287333][-4.4289093 -4.4288788 -4.4288559 -4.4288454 -4.4288383 -4.4288263 -4.4287958 -4.4287434 -4.4286966 -4.4286895 -4.4287043 -4.428719 -4.4287434 -4.428762 -4.4287667][-4.4289365 -4.4289131 -4.4288883 -4.4288721 -4.4288592 -4.4288421 -4.428813 -4.4287682 -4.4287267 -4.4287181 -4.4287281 -4.4287395 -4.4287729 -4.4287996 -4.428802]]...]
INFO - root - 2017-12-08 05:36:40.824663: step 11710, loss = 2.28, batch loss = 2.23 (37.0 examples/sec; 0.216 sec/batch; 19h:15m:00s remains)
INFO - root - 2017-12-08 05:36:43.069347: step 11720, loss = 2.28, batch loss = 2.23 (35.6 examples/sec; 0.225 sec/batch; 20h:02m:24s remains)
INFO - root - 2017-12-08 05:36:45.280649: step 11730, loss = 2.28, batch loss = 2.23 (37.4 examples/sec; 0.214 sec/batch; 19h:03m:20s remains)
INFO - root - 2017-12-08 05:36:47.506606: step 11740, loss = 2.28, batch loss = 2.23 (37.0 examples/sec; 0.216 sec/batch; 19h:15m:35s remains)
INFO - root - 2017-12-08 05:36:49.736720: step 11750, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 19h:32m:08s remains)
INFO - root - 2017-12-08 05:36:51.989042: step 11760, loss = 2.28, batch loss = 2.23 (34.0 examples/sec; 0.236 sec/batch; 20h:59m:27s remains)
INFO - root - 2017-12-08 05:36:54.230210: step 11770, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 19h:51m:34s remains)
INFO - root - 2017-12-08 05:36:56.468971: step 11780, loss = 2.28, batch loss = 2.23 (35.1 examples/sec; 0.228 sec/batch; 20h:18m:39s remains)
INFO - root - 2017-12-08 05:36:58.733158: step 11790, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.220 sec/batch; 19h:36m:33s remains)
INFO - root - 2017-12-08 05:37:00.971683: step 11800, loss = 2.28, batch loss = 2.23 (36.6 examples/sec; 0.219 sec/batch; 19h:28m:05s remains)
2017-12-08 05:37:01.267577: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.428751 -4.4287152 -4.4287009 -4.4287286 -4.4287453 -4.4287457 -4.4287467 -4.4287505 -4.4287581 -4.4287734 -4.4287958 -4.4288206 -4.4288316 -4.4288278 -4.4288344][-4.428721 -4.428689 -4.4286866 -4.4287243 -4.4287438 -4.42875 -4.4287591 -4.4287682 -4.4287734 -4.4287863 -4.4287996 -4.4288068 -4.4288092 -4.4288025 -4.4288087][-4.4287043 -4.4286656 -4.4286628 -4.4287009 -4.4287186 -4.4287224 -4.4287367 -4.4287629 -4.4287806 -4.4288 -4.4288135 -4.4288187 -4.4288192 -4.4288092 -4.4288063][-4.4287038 -4.4286461 -4.4286232 -4.428647 -4.4286571 -4.4286637 -4.4286838 -4.4287229 -4.42876 -4.4287915 -4.4288106 -4.4288206 -4.4288249 -4.4288158 -4.4288111][-4.4286895 -4.4286137 -4.428566 -4.4285693 -4.4285674 -4.4285727 -4.4285989 -4.4286504 -4.4287076 -4.4287591 -4.4287891 -4.4288063 -4.4288168 -4.4288144 -4.4288158][-4.4286394 -4.42857 -4.4285192 -4.4285092 -4.4284949 -4.4284887 -4.4285026 -4.4285474 -4.4286132 -4.4286814 -4.4287224 -4.4287534 -4.4287767 -4.4287868 -4.428802][-4.4285693 -4.428525 -4.42849 -4.42848 -4.4284716 -4.4284568 -4.4284506 -4.4284711 -4.4285221 -4.428587 -4.4286327 -4.428679 -4.4287114 -4.4287376 -4.4287715][-4.4284692 -4.4284363 -4.428411 -4.4284096 -4.4284167 -4.4284077 -4.4284024 -4.4284186 -4.428463 -4.4285169 -4.4285488 -4.4285893 -4.4286218 -4.4286571 -4.428709][-4.4284019 -4.4283667 -4.4283452 -4.4283557 -4.4283752 -4.4283729 -4.4283776 -4.4284058 -4.4284492 -4.428494 -4.4285126 -4.42854 -4.4285636 -4.4285979 -4.4286571][-4.4284086 -4.428371 -4.4283557 -4.4283767 -4.4284019 -4.4284019 -4.4284177 -4.4284525 -4.4284868 -4.4285226 -4.4285369 -4.4285583 -4.4285769 -4.4286036 -4.4286551][-4.4284673 -4.4284296 -4.4284196 -4.428443 -4.4284654 -4.4284663 -4.4284849 -4.4285173 -4.428544 -4.4285669 -4.4285808 -4.4286017 -4.4286184 -4.428637 -4.4286804][-4.4285517 -4.4285183 -4.4285054 -4.42852 -4.4285355 -4.4285393 -4.4285574 -4.4285913 -4.4286165 -4.4286342 -4.4286456 -4.428659 -4.428668 -4.4286785 -4.4287128][-4.4286337 -4.4286146 -4.428607 -4.4286137 -4.4286208 -4.4286232 -4.428637 -4.4286666 -4.4286895 -4.4287043 -4.4287148 -4.4287181 -4.4287157 -4.4287224 -4.4287491][-4.4287338 -4.4287248 -4.4287229 -4.4287276 -4.4287324 -4.4287348 -4.4287424 -4.4287586 -4.4287729 -4.428782 -4.4287891 -4.4287853 -4.4287758 -4.4287767 -4.4287953][-4.4288287 -4.4288249 -4.4288244 -4.4288273 -4.42883 -4.4288306 -4.428833 -4.4288387 -4.4288449 -4.4288516 -4.4288573 -4.4288559 -4.4288483 -4.4288468 -4.4288573]]...]
INFO - root - 2017-12-08 05:37:03.503092: step 11810, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.221 sec/batch; 19h:42m:58s remains)
INFO - root - 2017-12-08 05:37:05.785101: step 11820, loss = 2.28, batch loss = 2.23 (35.2 examples/sec; 0.227 sec/batch; 20h:14m:52s remains)
INFO - root - 2017-12-08 05:37:08.057136: step 11830, loss = 2.28, batch loss = 2.23 (33.1 examples/sec; 0.242 sec/batch; 21h:31m:15s remains)
INFO - root - 2017-12-08 05:37:10.291800: step 11840, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 19h:40m:52s remains)
INFO - root - 2017-12-08 05:37:12.514760: step 11850, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.220 sec/batch; 19h:38m:16s remains)
INFO - root - 2017-12-08 05:37:14.754326: step 11860, loss = 2.28, batch loss = 2.23 (35.6 examples/sec; 0.225 sec/batch; 20h:01m:22s remains)
INFO - root - 2017-12-08 05:37:16.975908: step 11870, loss = 2.28, batch loss = 2.23 (37.0 examples/sec; 0.217 sec/batch; 19h:16m:56s remains)
INFO - root - 2017-12-08 05:37:19.241649: step 11880, loss = 2.28, batch loss = 2.23 (36.7 examples/sec; 0.218 sec/batch; 19h:23m:58s remains)
INFO - root - 2017-12-08 05:37:21.514470: step 11890, loss = 2.28, batch loss = 2.23 (35.4 examples/sec; 0.226 sec/batch; 20h:07m:59s remains)
INFO - root - 2017-12-08 05:37:23.763612: step 11900, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 19h:33m:32s remains)
2017-12-08 05:37:24.037006: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289327 -4.4289212 -4.4289 -4.4288926 -4.4288955 -4.4288883 -4.42887 -4.4288521 -4.428812 -4.4287591 -4.4287276 -4.4287329 -4.4287724 -4.4288173 -4.4288597][-4.4289341 -4.4289289 -4.4289083 -4.4288912 -4.4288812 -4.428864 -4.4288397 -4.4288259 -4.4287992 -4.4287534 -4.4287314 -4.4287391 -4.428772 -4.4288182 -4.4288664][-4.4289145 -4.4289169 -4.4289017 -4.42888 -4.42886 -4.4288321 -4.4287906 -4.4287696 -4.4287539 -4.4287243 -4.4287171 -4.4287267 -4.4287586 -4.4288111 -4.4288659][-4.42888 -4.428884 -4.428874 -4.4288597 -4.4288421 -4.4288077 -4.4287486 -4.4287105 -4.428689 -4.4286714 -4.4286876 -4.4287114 -4.4287491 -4.4288054 -4.4288588][-4.4288425 -4.4288349 -4.4288197 -4.4288096 -4.4287963 -4.4287615 -4.4286919 -4.4286332 -4.4285979 -4.4285955 -4.4286389 -4.4286833 -4.4287281 -4.42879 -4.4288425][-4.4288111 -4.4287882 -4.4287653 -4.4287486 -4.4287291 -4.428679 -4.4285851 -4.428493 -4.4284396 -4.4284725 -4.4285626 -4.42864 -4.4287043 -4.4287753 -4.4288268][-4.4287915 -4.4287596 -4.4287372 -4.4287148 -4.428679 -4.4286008 -4.4284639 -4.428308 -4.4282155 -4.4282994 -4.4284573 -4.428576 -4.428668 -4.4287505 -4.4288116][-4.4287772 -4.4287496 -4.4287376 -4.4287167 -4.4286618 -4.4285626 -4.4284019 -4.428195 -4.4280696 -4.4282 -4.4284039 -4.4285412 -4.4286451 -4.4287276 -4.4288015][-4.4287872 -4.42877 -4.4287744 -4.4287605 -4.4286933 -4.4285874 -4.4284334 -4.42823 -4.42811 -4.4282537 -4.4284463 -4.4285617 -4.4286513 -4.4287205 -4.4287953][-4.42881 -4.4288116 -4.4288321 -4.4288235 -4.4287562 -4.4286566 -4.4285307 -4.4283638 -4.428267 -4.4283843 -4.4285293 -4.4286103 -4.4286776 -4.4287329 -4.4288][-4.4288492 -4.4288568 -4.4288788 -4.4288721 -4.4288158 -4.4287434 -4.42866 -4.4285436 -4.428472 -4.4285512 -4.4286427 -4.4286904 -4.4287348 -4.4287748 -4.4288249][-4.4289064 -4.42891 -4.4289184 -4.428905 -4.4288611 -4.4288135 -4.4287629 -4.428688 -4.4286427 -4.4287009 -4.4287615 -4.4287925 -4.4288149 -4.4288359 -4.4288635][-4.4289656 -4.4289618 -4.4289546 -4.4289351 -4.4289041 -4.428874 -4.4288425 -4.4287939 -4.4287643 -4.4288049 -4.4288464 -4.428864 -4.4288683 -4.4288778 -4.4288907][-4.4289851 -4.4289694 -4.4289503 -4.4289327 -4.4289193 -4.42891 -4.4288955 -4.4288712 -4.4288559 -4.428874 -4.4288888 -4.4288869 -4.42888 -4.4288898 -4.4289002][-4.4289656 -4.4289484 -4.42893 -4.4289174 -4.428916 -4.42892 -4.4289184 -4.4289074 -4.4288974 -4.4288936 -4.42888 -4.4288645 -4.4288573 -4.4288764 -4.4288926]]...]
INFO - root - 2017-12-08 05:37:26.259648: step 11910, loss = 2.28, batch loss = 2.23 (34.5 examples/sec; 0.232 sec/batch; 20h:39m:23s remains)
INFO - root - 2017-12-08 05:37:28.479555: step 11920, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.221 sec/batch; 19h:39m:06s remains)
INFO - root - 2017-12-08 05:37:30.780821: step 11930, loss = 2.28, batch loss = 2.23 (31.9 examples/sec; 0.251 sec/batch; 22h:21m:11s remains)
INFO - root - 2017-12-08 05:37:33.012133: step 11940, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.222 sec/batch; 19h:44m:59s remains)
INFO - root - 2017-12-08 05:37:35.245858: step 11950, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.220 sec/batch; 19h:36m:39s remains)
INFO - root - 2017-12-08 05:37:37.481516: step 11960, loss = 2.28, batch loss = 2.23 (34.9 examples/sec; 0.229 sec/batch; 20h:23m:23s remains)
INFO - root - 2017-12-08 05:37:39.699069: step 11970, loss = 2.28, batch loss = 2.23 (35.2 examples/sec; 0.227 sec/batch; 20h:15m:15s remains)
INFO - root - 2017-12-08 05:37:41.943653: step 11980, loss = 2.28, batch loss = 2.23 (33.5 examples/sec; 0.239 sec/batch; 21h:16m:07s remains)
INFO - root - 2017-12-08 05:37:44.189517: step 11990, loss = 2.28, batch loss = 2.23 (35.3 examples/sec; 0.227 sec/batch; 20h:11m:06s remains)
INFO - root - 2017-12-08 05:37:46.415794: step 12000, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 19h:34m:09s remains)
2017-12-08 05:37:46.750436: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.428843 -4.4288244 -4.4288077 -4.428812 -4.4288268 -4.4288187 -4.4287939 -4.4287691 -4.4287648 -4.4287715 -4.4287887 -4.4288054 -4.428803 -4.4287934 -4.4287934][-4.4288106 -4.4287934 -4.4287739 -4.4287777 -4.4287877 -4.4287672 -4.4287238 -4.4286895 -4.4286938 -4.4287157 -4.428741 -4.4287581 -4.4287491 -4.42873 -4.4287348][-4.4287758 -4.4287567 -4.4287357 -4.428741 -4.4287453 -4.4287124 -4.4286551 -4.4286189 -4.4286332 -4.4286656 -4.4286923 -4.4287076 -4.4286947 -4.4286737 -4.4286885][-4.4287596 -4.4287324 -4.4287052 -4.428709 -4.4287076 -4.428669 -4.4286094 -4.4285793 -4.4286008 -4.4286203 -4.4286361 -4.428658 -4.428658 -4.4286442 -4.4286647][-4.4287667 -4.4287257 -4.4286828 -4.4286742 -4.4286609 -4.4286203 -4.4285674 -4.428544 -4.4285517 -4.4285469 -4.4285483 -4.4285879 -4.4286208 -4.4286304 -4.4286551][-4.4287806 -4.42873 -4.428669 -4.4286413 -4.4286156 -4.4285779 -4.4285283 -4.4284992 -4.4284768 -4.4284334 -4.4284225 -4.4284964 -4.4285741 -4.42862 -4.4286513][-4.4288015 -4.42875 -4.4286842 -4.4286461 -4.428618 -4.4285817 -4.4285274 -4.42848 -4.4284253 -4.4283457 -4.4283271 -4.428431 -4.4285479 -4.4286227 -4.4286685][-4.428813 -4.4287739 -4.4287229 -4.4286861 -4.4286585 -4.4286122 -4.4285507 -4.4284987 -4.4284325 -4.4283471 -4.4283481 -4.4284549 -4.4285679 -4.4286423 -4.428699][-4.4288168 -4.4287806 -4.4287329 -4.4286976 -4.4286747 -4.4286308 -4.4285793 -4.42854 -4.4284854 -4.4284277 -4.4284558 -4.42854 -4.4286065 -4.4286509 -4.42871][-4.4288249 -4.4287782 -4.4287138 -4.4286733 -4.4286613 -4.4286451 -4.4286256 -4.4286113 -4.42858 -4.4285474 -4.4285727 -4.4286132 -4.4286227 -4.428637 -4.4286895][-4.4288363 -4.4287739 -4.4286928 -4.4286528 -4.4286547 -4.4286685 -4.428679 -4.4286852 -4.4286709 -4.428648 -4.4286547 -4.4286547 -4.4286337 -4.4286375 -4.4286733][-4.4288731 -4.4287963 -4.4287081 -4.428668 -4.4286747 -4.4286995 -4.4287171 -4.4287271 -4.4287152 -4.4286914 -4.4286871 -4.4286661 -4.4286366 -4.4286456 -4.4286747][-4.4289126 -4.4288421 -4.4287567 -4.4287095 -4.4287066 -4.4287305 -4.428741 -4.4287462 -4.4287324 -4.4287114 -4.4287043 -4.42868 -4.4286585 -4.4286728 -4.4286966][-4.4289536 -4.4289012 -4.4288249 -4.4287734 -4.4287586 -4.4287667 -4.4287572 -4.4287515 -4.4287438 -4.4287424 -4.4287505 -4.4287467 -4.428741 -4.4287553 -4.4287648][-4.428988 -4.4289503 -4.4288855 -4.4288363 -4.4288139 -4.4287996 -4.4287763 -4.4287691 -4.4287744 -4.4287963 -4.4288297 -4.4288416 -4.4288387 -4.42885 -4.4288473]]...]
INFO - root - 2017-12-08 05:37:48.973484: step 12010, loss = 2.28, batch loss = 2.23 (35.1 examples/sec; 0.228 sec/batch; 20h:18m:15s remains)
INFO - root - 2017-12-08 05:37:51.204287: step 12020, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.225 sec/batch; 20h:04m:26s remains)
INFO - root - 2017-12-08 05:37:53.496172: step 12030, loss = 2.28, batch loss = 2.23 (32.7 examples/sec; 0.245 sec/batch; 21h:48m:30s remains)
INFO - root - 2017-12-08 05:37:55.766305: step 12040, loss = 2.28, batch loss = 2.23 (35.6 examples/sec; 0.225 sec/batch; 19h:59m:56s remains)
INFO - root - 2017-12-08 05:37:58.013187: step 12050, loss = 2.28, batch loss = 2.23 (35.2 examples/sec; 0.227 sec/batch; 20h:14m:25s remains)
INFO - root - 2017-12-08 05:38:00.238227: step 12060, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.221 sec/batch; 19h:42m:28s remains)
INFO - root - 2017-12-08 05:38:02.482784: step 12070, loss = 2.28, batch loss = 2.23 (35.1 examples/sec; 0.228 sec/batch; 20h:17m:39s remains)
INFO - root - 2017-12-08 05:38:04.735380: step 12080, loss = 2.28, batch loss = 2.23 (35.1 examples/sec; 0.228 sec/batch; 20h:16m:09s remains)
INFO - root - 2017-12-08 05:38:06.960495: step 12090, loss = 2.28, batch loss = 2.23 (37.2 examples/sec; 0.215 sec/batch; 19h:09m:31s remains)
INFO - root - 2017-12-08 05:38:09.212918: step 12100, loss = 2.28, batch loss = 2.23 (37.5 examples/sec; 0.213 sec/batch; 18h:59m:51s remains)
2017-12-08 05:38:09.532678: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4287577 -4.4287596 -4.4287524 -4.4287267 -4.4286938 -4.42868 -4.4287105 -4.4287672 -4.4288096 -4.4288259 -4.4288187 -4.4288073 -4.4288135 -4.4288292 -4.4288521][-4.4287353 -4.4287438 -4.4287381 -4.428719 -4.4286952 -4.4286895 -4.4287267 -4.428791 -4.4288406 -4.4288526 -4.4288392 -4.4288211 -4.4288168 -4.4288206 -4.4288278][-4.4287305 -4.4287395 -4.4287324 -4.428719 -4.4286981 -4.4286895 -4.4287233 -4.4287958 -4.4288588 -4.4288807 -4.4288721 -4.42885 -4.4288306 -4.4288168 -4.4288054][-4.4287691 -4.4287715 -4.428761 -4.4287424 -4.428709 -4.4286718 -4.4286747 -4.4287419 -4.4288254 -4.4288754 -4.4288869 -4.4288716 -4.4288507 -4.4288287 -4.4288092][-4.4288259 -4.4288239 -4.4288096 -4.428782 -4.4287305 -4.4286528 -4.4285989 -4.4286323 -4.4287291 -4.4288206 -4.4288645 -4.4288669 -4.4288597 -4.4288473 -4.4288354][-4.4288745 -4.4288712 -4.4288564 -4.4288177 -4.4287434 -4.42862 -4.4284973 -4.4284658 -4.4285622 -4.4287066 -4.4287968 -4.428833 -4.4288535 -4.42886 -4.4288564][-4.4289131 -4.4289165 -4.4289045 -4.428865 -4.4287853 -4.4286389 -4.4284558 -4.4283366 -4.4283886 -4.4285607 -4.4287024 -4.428782 -4.4288335 -4.4288573 -4.428863][-4.4289289 -4.4289494 -4.4289446 -4.4289088 -4.428844 -4.4287286 -4.4285607 -4.4283938 -4.428349 -4.4284568 -4.4285979 -4.4287071 -4.4287853 -4.42883 -4.4288511][-4.4289212 -4.4289556 -4.4289632 -4.4289422 -4.4288974 -4.4288249 -4.4287143 -4.4285755 -4.4284906 -4.4285059 -4.4285765 -4.4286556 -4.4287267 -4.4287763 -4.4288139][-4.4288955 -4.4289351 -4.4289565 -4.4289584 -4.4289351 -4.4288917 -4.4288254 -4.4287314 -4.4286585 -4.4286413 -4.4286628 -4.4286942 -4.4287224 -4.4287438 -4.4287772][-4.4288692 -4.4289074 -4.428936 -4.4289546 -4.4289517 -4.4289241 -4.428875 -4.428812 -4.4287577 -4.4287429 -4.4287534 -4.4287715 -4.4287767 -4.4287686 -4.4287763][-4.4288678 -4.4288974 -4.4289193 -4.4289384 -4.4289432 -4.4289188 -4.4288712 -4.4288168 -4.4287744 -4.428772 -4.4287968 -4.4288254 -4.4288306 -4.4288163 -4.4288082][-4.4288864 -4.4289007 -4.4289093 -4.4289179 -4.428916 -4.4288864 -4.428833 -4.4287724 -4.4287291 -4.4287357 -4.4287791 -4.4288282 -4.4288454 -4.4288397 -4.4288325][-4.4289 -4.4288993 -4.4288988 -4.4289 -4.428895 -4.4288626 -4.4288054 -4.4287367 -4.4286871 -4.4286985 -4.42875 -4.4288058 -4.4288268 -4.4288278 -4.4288278][-4.4288964 -4.4288855 -4.4288836 -4.4288845 -4.4288816 -4.428853 -4.4287992 -4.4287372 -4.4286938 -4.4287066 -4.4287529 -4.4287949 -4.4288054 -4.4288015 -4.4288034]]...]
INFO - root - 2017-12-08 05:38:11.746045: step 12110, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 19h:38m:31s remains)
INFO - root - 2017-12-08 05:38:13.987155: step 12120, loss = 2.28, batch loss = 2.23 (35.0 examples/sec; 0.228 sec/batch; 20h:19m:02s remains)
INFO - root - 2017-12-08 05:38:16.240717: step 12130, loss = 2.28, batch loss = 2.23 (35.7 examples/sec; 0.224 sec/batch; 19h:57m:06s remains)
INFO - root - 2017-12-08 05:38:18.459096: step 12140, loss = 2.28, batch loss = 2.23 (35.3 examples/sec; 0.227 sec/batch; 20h:11m:41s remains)
INFO - root - 2017-12-08 05:38:20.705309: step 12150, loss = 2.28, batch loss = 2.23 (35.0 examples/sec; 0.228 sec/batch; 20h:19m:21s remains)
INFO - root - 2017-12-08 05:38:22.957149: step 12160, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.222 sec/batch; 19h:44m:08s remains)
INFO - root - 2017-12-08 05:38:25.195882: step 12170, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 19h:33m:45s remains)
INFO - root - 2017-12-08 05:38:27.415414: step 12180, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 19h:33m:59s remains)
INFO - root - 2017-12-08 05:38:29.652821: step 12190, loss = 2.28, batch loss = 2.23 (35.6 examples/sec; 0.225 sec/batch; 19h:59m:33s remains)
INFO - root - 2017-12-08 05:38:31.878980: step 12200, loss = 2.28, batch loss = 2.23 (35.3 examples/sec; 0.227 sec/batch; 20h:11m:31s remains)
2017-12-08 05:38:32.181216: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289079 -4.4289188 -4.4289217 -4.4289217 -4.4289227 -4.4289231 -4.4289231 -4.4289227 -4.4289188 -4.4289122 -4.4289122 -4.4289212 -4.4289346 -4.428946 -4.42895][-4.4288673 -4.4288821 -4.4288888 -4.4288864 -4.42888 -4.4288721 -4.4288697 -4.4288764 -4.4288836 -4.4288878 -4.4288983 -4.4289155 -4.4289346 -4.4289455 -4.428946][-4.4287972 -4.428813 -4.4288225 -4.4288168 -4.4287996 -4.4287782 -4.428772 -4.428793 -4.428823 -4.4288464 -4.4288745 -4.428905 -4.4289303 -4.4289422 -4.4289374][-4.428709 -4.42872 -4.4287305 -4.4287214 -4.4286952 -4.4286575 -4.4286418 -4.4286723 -4.4287243 -4.4287691 -4.4288154 -4.4288621 -4.4288993 -4.428916 -4.4289093][-4.4286227 -4.4286289 -4.4286418 -4.4286318 -4.428596 -4.4285345 -4.4284897 -4.4285126 -4.4285893 -4.4286656 -4.4287372 -4.4288034 -4.4288516 -4.4288664 -4.4288511][-4.4285388 -4.4285421 -4.4285541 -4.4285383 -4.4284811 -4.4283752 -4.4282722 -4.4282765 -4.4283929 -4.42852 -4.4286308 -4.4287252 -4.4287877 -4.4288049 -4.4287829][-4.4284654 -4.4284554 -4.4284496 -4.4284153 -4.4283295 -4.4281726 -4.4280019 -4.4279876 -4.4281588 -4.4283538 -4.4285092 -4.4286261 -4.4287043 -4.428731 -4.4287162][-4.4284143 -4.42838 -4.4283504 -4.4283023 -4.4282174 -4.4280663 -4.42789 -4.4278688 -4.4280581 -4.4282789 -4.4284468 -4.4285574 -4.4286289 -4.4286661 -4.4286704][-4.4284024 -4.4283528 -4.4283161 -4.428278 -4.4282346 -4.4281588 -4.4280572 -4.4280429 -4.4281716 -4.42834 -4.4284687 -4.4285445 -4.4285922 -4.4286284 -4.4286489][-4.428441 -4.4283943 -4.4283686 -4.4283552 -4.4283519 -4.4283352 -4.4282947 -4.4282832 -4.4283447 -4.4284387 -4.4285188 -4.42857 -4.4286008 -4.428628 -4.4286532][-4.4285054 -4.428473 -4.4284625 -4.4284663 -4.4284806 -4.4284878 -4.4284744 -4.428462 -4.4284868 -4.428534 -4.4285836 -4.4286242 -4.4286518 -4.4286742 -4.4286933][-4.42857 -4.4285512 -4.4285531 -4.4285665 -4.428587 -4.4286022 -4.4285989 -4.4285917 -4.4286041 -4.4286318 -4.4286642 -4.4286985 -4.4287238 -4.4287429 -4.4287543][-4.4286304 -4.428618 -4.4286275 -4.4286447 -4.4286652 -4.4286795 -4.42868 -4.4286776 -4.4286866 -4.4287071 -4.4287329 -4.4287624 -4.4287868 -4.4288039 -4.4288111][-4.4287062 -4.4286976 -4.4287081 -4.4287257 -4.4287419 -4.42875 -4.4287491 -4.4287477 -4.4287529 -4.4287667 -4.4287868 -4.4288096 -4.4288292 -4.4288435 -4.4288497][-4.4287891 -4.4287848 -4.4287925 -4.4288034 -4.4288116 -4.4288139 -4.4288116 -4.4288096 -4.4288125 -4.4288211 -4.428833 -4.4288468 -4.4288597 -4.42887 -4.428875]]...]
INFO - root - 2017-12-08 05:38:34.394018: step 12210, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.221 sec/batch; 19h:37m:47s remains)
INFO - root - 2017-12-08 05:38:36.660968: step 12220, loss = 2.28, batch loss = 2.23 (35.2 examples/sec; 0.228 sec/batch; 20h:14m:43s remains)
INFO - root - 2017-12-08 05:38:38.884257: step 12230, loss = 2.28, batch loss = 2.23 (35.0 examples/sec; 0.228 sec/batch; 20h:18m:22s remains)
INFO - root - 2017-12-08 05:38:41.139380: step 12240, loss = 2.28, batch loss = 2.23 (37.4 examples/sec; 0.214 sec/batch; 19h:00m:28s remains)
INFO - root - 2017-12-08 05:38:43.385137: step 12250, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 19h:30m:11s remains)
INFO - root - 2017-12-08 05:38:45.601032: step 12260, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 19h:48m:52s remains)
INFO - root - 2017-12-08 05:38:47.829459: step 12270, loss = 2.28, batch loss = 2.23 (36.8 examples/sec; 0.217 sec/batch; 19h:19m:32s remains)
INFO - root - 2017-12-08 05:38:50.062079: step 12280, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.222 sec/batch; 19h:42m:22s remains)
INFO - root - 2017-12-08 05:38:52.299450: step 12290, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.225 sec/batch; 20h:02m:04s remains)
INFO - root - 2017-12-08 05:38:54.577432: step 12300, loss = 2.28, batch loss = 2.23 (35.1 examples/sec; 0.228 sec/batch; 20h:16m:28s remains)
2017-12-08 05:38:54.881708: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4290075 -4.4289212 -4.4288011 -4.4286752 -4.4286275 -4.4286785 -4.4287739 -4.4288545 -4.4289031 -4.4289012 -4.4288583 -4.4288235 -4.4288287 -4.4288793 -4.4289384][-4.4289956 -4.4289007 -4.4287682 -4.4286275 -4.4285836 -4.428659 -4.4287639 -4.4288378 -4.428874 -4.4288664 -4.4288297 -4.428813 -4.4288373 -4.4288917 -4.428947][-4.4289646 -4.428874 -4.4287534 -4.4286242 -4.4285884 -4.4286661 -4.42875 -4.4287944 -4.4288006 -4.4287839 -4.4287624 -4.4287748 -4.4288244 -4.4288888 -4.4289436][-4.4289365 -4.42885 -4.4287486 -4.4286432 -4.4286146 -4.428669 -4.4287133 -4.4287257 -4.4287004 -4.4286752 -4.4286718 -4.4287152 -4.4287839 -4.42886 -4.4289212][-4.42893 -4.42885 -4.4287686 -4.428689 -4.4286556 -4.4286613 -4.4286509 -4.4286308 -4.42859 -4.4285722 -4.4286056 -4.4286857 -4.4287715 -4.42885 -4.42891][-4.4289637 -4.4289055 -4.4288321 -4.4287515 -4.4286828 -4.4286175 -4.4285393 -4.428504 -4.4284868 -4.4285069 -4.4285836 -4.4286976 -4.4287963 -4.4288707 -4.4289203][-4.4290175 -4.4289856 -4.4289093 -4.4287996 -4.4286652 -4.42851 -4.4283619 -4.4283457 -4.4284115 -4.4285007 -4.428618 -4.4287415 -4.4288344 -4.4288983 -4.428936][-4.4290581 -4.4290404 -4.4289589 -4.4288177 -4.4286304 -4.4284048 -4.4282184 -4.4282384 -4.42839 -4.4285421 -4.4286761 -4.4287934 -4.4288726 -4.4289222 -4.42895][-4.4290824 -4.4290686 -4.4289837 -4.4288306 -4.4286261 -4.4283886 -4.428215 -4.428268 -4.4284511 -4.4286222 -4.4287496 -4.4288468 -4.428906 -4.4289417 -4.4289589][-4.4291043 -4.4290848 -4.4289908 -4.4288387 -4.4286528 -4.4284558 -4.4283357 -4.4284086 -4.4285727 -4.4287224 -4.42883 -4.4288988 -4.4289351 -4.4289541 -4.4289618][-4.4291153 -4.429091 -4.4289961 -4.4288507 -4.4286895 -4.4285479 -4.4284925 -4.4285736 -4.4287024 -4.4288154 -4.428894 -4.4289355 -4.42895 -4.4289546 -4.4289613][-4.4291124 -4.4290915 -4.4290118 -4.4288878 -4.4287553 -4.4286613 -4.4286466 -4.428719 -4.428812 -4.4288878 -4.428937 -4.4289484 -4.4289441 -4.4289441 -4.4289584][-4.4290924 -4.4290752 -4.4290195 -4.4289269 -4.4288273 -4.42877 -4.4287777 -4.4288325 -4.4288955 -4.4289417 -4.4289575 -4.42894 -4.4289184 -4.4289169 -4.4289408][-4.4290605 -4.429049 -4.4290094 -4.4289508 -4.428884 -4.4288516 -4.4288673 -4.42891 -4.428956 -4.4289813 -4.4289694 -4.4289303 -4.4288983 -4.4288993 -4.4289336][-4.4290347 -4.429029 -4.4290004 -4.4289646 -4.4289246 -4.4289074 -4.4289207 -4.4289551 -4.4289837 -4.4289856 -4.4289546 -4.4289064 -4.428874 -4.4288831 -4.4289293]]...]
INFO - root - 2017-12-08 05:38:57.109815: step 12310, loss = 2.28, batch loss = 2.23 (34.6 examples/sec; 0.231 sec/batch; 20h:33m:19s remains)
INFO - root - 2017-12-08 05:38:59.385867: step 12320, loss = 2.28, batch loss = 2.23 (32.0 examples/sec; 0.250 sec/batch; 22h:13m:07s remains)
INFO - root - 2017-12-08 05:39:01.638096: step 12330, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.220 sec/batch; 19h:34m:57s remains)
INFO - root - 2017-12-08 05:39:03.880002: step 12340, loss = 2.28, batch loss = 2.23 (35.0 examples/sec; 0.229 sec/batch; 20h:19m:30s remains)
INFO - root - 2017-12-08 05:39:06.119783: step 12350, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 19h:48m:00s remains)
INFO - root - 2017-12-08 05:39:08.394934: step 12360, loss = 2.28, batch loss = 2.23 (32.9 examples/sec; 0.243 sec/batch; 21h:37m:12s remains)
INFO - root - 2017-12-08 05:39:10.623818: step 12370, loss = 2.28, batch loss = 2.23 (37.6 examples/sec; 0.213 sec/batch; 18h:54m:51s remains)
INFO - root - 2017-12-08 05:39:12.873588: step 12380, loss = 2.28, batch loss = 2.23 (35.8 examples/sec; 0.223 sec/batch; 19h:51m:08s remains)
INFO - root - 2017-12-08 05:39:15.118562: step 12390, loss = 2.28, batch loss = 2.23 (34.5 examples/sec; 0.232 sec/batch; 20h:36m:46s remains)
INFO - root - 2017-12-08 05:39:17.340050: step 12400, loss = 2.28, batch loss = 2.23 (35.8 examples/sec; 0.223 sec/batch; 19h:50m:55s remains)
2017-12-08 05:39:17.640401: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4286904 -4.4287381 -4.4287977 -4.4288349 -4.428843 -4.4288383 -4.4288282 -4.4287782 -4.4287152 -4.4286838 -4.428699 -4.4287109 -4.4286966 -4.4286385 -4.4285388][-4.4285712 -4.4286537 -4.4287519 -4.4288111 -4.428823 -4.4288111 -4.4287877 -4.4287343 -4.42868 -4.4286609 -4.4286795 -4.4286952 -4.4286852 -4.428627 -4.4285164][-4.4284635 -4.4285903 -4.4287257 -4.428792 -4.428793 -4.4287653 -4.4287372 -4.4287033 -4.4286785 -4.4286804 -4.4287052 -4.4287219 -4.4287143 -4.4286604 -4.4285436][-4.4283924 -4.4285665 -4.4287219 -4.4287767 -4.4287539 -4.4287066 -4.4286747 -4.42867 -4.4286804 -4.4287033 -4.4287295 -4.4287491 -4.4287457 -4.4286942 -4.4285808][-4.4283853 -4.4285774 -4.4287291 -4.4287663 -4.4287233 -4.4286666 -4.428638 -4.4286518 -4.4286776 -4.4287076 -4.4287343 -4.4287605 -4.4287682 -4.4287229 -4.4286056][-4.4284368 -4.4286175 -4.4287515 -4.4287629 -4.4286952 -4.42863 -4.4286046 -4.4286284 -4.4286604 -4.4286866 -4.4287109 -4.4287457 -4.428771 -4.4287386 -4.4286175][-4.4285626 -4.4286962 -4.4287882 -4.4287734 -4.42868 -4.4286003 -4.4285703 -4.4285913 -4.4286165 -4.4286394 -4.428668 -4.4287157 -4.428761 -4.4287424 -4.4286356][-4.4287047 -4.4287753 -4.4288197 -4.428793 -4.4286952 -4.4286189 -4.4285808 -4.4285827 -4.428586 -4.4285893 -4.4286175 -4.42868 -4.4287381 -4.4287333 -4.4286494][-4.4288168 -4.4288397 -4.4288568 -4.4288363 -4.4287581 -4.4286952 -4.4286518 -4.4286304 -4.428607 -4.4285913 -4.4286089 -4.4286723 -4.4287333 -4.4287324 -4.4286618][-4.4288845 -4.4288812 -4.4288893 -4.42888 -4.4288282 -4.4287858 -4.428751 -4.4287186 -4.4286747 -4.4286418 -4.4286461 -4.4286985 -4.4287519 -4.4287467 -4.4286842][-4.4289083 -4.4288988 -4.4289036 -4.4289021 -4.4288735 -4.4288507 -4.4288316 -4.4288073 -4.428771 -4.4287372 -4.4287257 -4.4287529 -4.4287896 -4.4287815 -4.4287295][-4.4289255 -4.4289165 -4.428916 -4.4289165 -4.4289 -4.4288874 -4.4288764 -4.4288673 -4.4288497 -4.4288259 -4.4288106 -4.4288197 -4.4288368 -4.4288292 -4.428791][-4.428947 -4.4289417 -4.4289412 -4.4289365 -4.4289231 -4.4289122 -4.4289026 -4.4288993 -4.4288926 -4.4288812 -4.4288731 -4.4288774 -4.4288836 -4.4288769 -4.428853][-4.4289622 -4.4289603 -4.4289608 -4.4289556 -4.4289441 -4.4289351 -4.4289274 -4.428925 -4.4289236 -4.4289217 -4.4289193 -4.4289231 -4.428925 -4.4289217 -4.4289093][-4.4289703 -4.4289708 -4.4289722 -4.4289718 -4.428968 -4.4289646 -4.4289608 -4.4289579 -4.428957 -4.4289579 -4.4289579 -4.4289594 -4.4289589 -4.4289575 -4.4289541]]...]
INFO - root - 2017-12-08 05:39:19.898714: step 12410, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 19h:30m:03s remains)
INFO - root - 2017-12-08 05:39:22.163177: step 12420, loss = 2.28, batch loss = 2.23 (35.0 examples/sec; 0.229 sec/batch; 20h:19m:57s remains)
INFO - root - 2017-12-08 05:39:24.368254: step 12430, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 19h:32m:43s remains)
INFO - root - 2017-12-08 05:39:26.593565: step 12440, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 19h:47m:28s remains)
INFO - root - 2017-12-08 05:39:28.832375: step 12450, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 19h:30m:41s remains)
INFO - root - 2017-12-08 05:39:31.061829: step 12460, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 19h:37m:41s remains)
INFO - root - 2017-12-08 05:39:33.276420: step 12470, loss = 2.28, batch loss = 2.23 (36.7 examples/sec; 0.218 sec/batch; 19h:23m:23s remains)
INFO - root - 2017-12-08 05:39:35.552384: step 12480, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.222 sec/batch; 19h:42m:18s remains)
INFO - root - 2017-12-08 05:39:37.773067: step 12490, loss = 2.28, batch loss = 2.23 (34.7 examples/sec; 0.231 sec/batch; 20h:29m:30s remains)
INFO - root - 2017-12-08 05:39:40.014795: step 12500, loss = 2.28, batch loss = 2.23 (35.1 examples/sec; 0.228 sec/batch; 20h:14m:27s remains)
2017-12-08 05:39:40.317893: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4286361 -4.4287128 -4.4287596 -4.4287772 -4.4287667 -4.4287286 -4.4286876 -4.4286828 -4.4287229 -4.4287624 -4.4287868 -4.4288158 -4.4288368 -4.4288268 -4.4288015][-4.4285908 -4.4286604 -4.4287066 -4.4287424 -4.4287562 -4.4287391 -4.4287095 -4.4287086 -4.428741 -4.4287715 -4.4287839 -4.4288015 -4.4288111 -4.4287992 -4.42877][-4.4285932 -4.4286356 -4.4286714 -4.4287171 -4.4287577 -4.4287663 -4.4287467 -4.4287472 -4.4287772 -4.428802 -4.4288092 -4.4288149 -4.428813 -4.4287944 -4.4287553][-4.42865 -4.4286604 -4.428679 -4.4287124 -4.42876 -4.428781 -4.4287629 -4.4287586 -4.4287949 -4.4288297 -4.4288354 -4.4288316 -4.4288239 -4.428802 -4.4287543][-4.4287367 -4.4287295 -4.4287291 -4.4287362 -4.4287567 -4.4287586 -4.4287219 -4.4286981 -4.4287419 -4.4288 -4.4288235 -4.4288292 -4.4288259 -4.4288092 -4.4287682][-4.4288034 -4.428791 -4.4287796 -4.4287624 -4.4287319 -4.4286785 -4.4285951 -4.4285359 -4.4286003 -4.4287086 -4.4287786 -4.4288192 -4.4288354 -4.4288321 -4.4288039][-4.428793 -4.4287734 -4.4287586 -4.4287243 -4.4286461 -4.4285274 -4.4283738 -4.4282675 -4.4283781 -4.4285727 -4.4287128 -4.4288073 -4.4288564 -4.4288678 -4.4288449][-4.4287314 -4.4287052 -4.4286857 -4.428638 -4.4285355 -4.4283772 -4.42817 -4.428019 -4.4281726 -4.428441 -4.428637 -4.4287739 -4.4288449 -4.4288616 -4.4288435][-4.4286871 -4.4286618 -4.4286447 -4.428606 -4.4285216 -4.4283829 -4.4281917 -4.4280252 -4.4281554 -4.4284177 -4.4286132 -4.4287505 -4.4288206 -4.4288344 -4.428823][-4.4286985 -4.4286804 -4.4286728 -4.4286637 -4.4286242 -4.4285393 -4.4284124 -4.4282851 -4.4283504 -4.4285264 -4.4286628 -4.4287581 -4.4288082 -4.4288092 -4.4287996][-4.4287581 -4.4287448 -4.4287438 -4.4287577 -4.4287524 -4.4287062 -4.4286284 -4.4285316 -4.4285512 -4.4286494 -4.4287252 -4.4287767 -4.4288054 -4.4287882 -4.4287696][-4.4288239 -4.4288044 -4.4287992 -4.4288192 -4.4288311 -4.4288025 -4.4287453 -4.4286723 -4.4286737 -4.4287314 -4.4287715 -4.4287896 -4.4287996 -4.4287634 -4.4287305][-4.4288597 -4.4288325 -4.42882 -4.4288354 -4.4288435 -4.4288144 -4.4287686 -4.4287252 -4.4287333 -4.4287777 -4.4287982 -4.4287953 -4.428792 -4.4287462 -4.4287038][-4.4288745 -4.4288406 -4.4288168 -4.4288168 -4.4288206 -4.4287968 -4.4287615 -4.4287415 -4.4287581 -4.4287968 -4.428812 -4.4288073 -4.4287958 -4.4287505 -4.4287086][-4.4288759 -4.4288397 -4.4288068 -4.42879 -4.428792 -4.428782 -4.4287591 -4.4287529 -4.4287691 -4.428802 -4.4288173 -4.4288144 -4.4288015 -4.4287658 -4.4287338]]...]
INFO - root - 2017-12-08 05:39:42.513092: step 12510, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 19h:48m:50s remains)
INFO - root - 2017-12-08 05:39:44.761682: step 12520, loss = 2.28, batch loss = 2.23 (35.1 examples/sec; 0.228 sec/batch; 20h:16m:47s remains)
INFO - root - 2017-12-08 05:39:47.024901: step 12530, loss = 2.28, batch loss = 2.23 (36.0 examples/sec; 0.222 sec/batch; 19h:44m:46s remains)
INFO - root - 2017-12-08 05:39:49.287756: step 12540, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 19h:37m:06s remains)
INFO - root - 2017-12-08 05:39:51.510470: step 12550, loss = 2.28, batch loss = 2.23 (36.9 examples/sec; 0.217 sec/batch; 19h:14m:33s remains)
INFO - root - 2017-12-08 05:39:53.750821: step 12560, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 19h:32m:50s remains)
INFO - root - 2017-12-08 05:39:55.973675: step 12570, loss = 2.28, batch loss = 2.23 (35.6 examples/sec; 0.225 sec/batch; 19h:57m:43s remains)
INFO - root - 2017-12-08 05:39:58.243743: step 12580, loss = 2.28, batch loss = 2.23 (34.7 examples/sec; 0.230 sec/batch; 20h:28m:20s remains)
INFO - root - 2017-12-08 05:40:00.486854: step 12590, loss = 2.28, batch loss = 2.23 (35.2 examples/sec; 0.227 sec/batch; 20h:10m:09s remains)
INFO - root - 2017-12-08 05:40:02.727774: step 12600, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 19h:32m:52s remains)
2017-12-08 05:40:03.013437: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288788 -4.4288754 -4.4288826 -4.4288931 -4.4289002 -4.4288955 -4.4288831 -4.4288716 -4.4288611 -4.4288616 -4.4288621 -4.4288549 -4.428853 -4.4288516 -4.4288588][-4.428874 -4.42887 -4.428885 -4.4289017 -4.4289141 -4.4289155 -4.4289064 -4.4288907 -4.4288712 -4.4288669 -4.42887 -4.4288688 -4.4288654 -4.4288588 -4.4288564][-4.4288497 -4.4288487 -4.4288678 -4.42889 -4.4289103 -4.4289174 -4.4289041 -4.42888 -4.4288607 -4.4288678 -4.4288898 -4.4289 -4.4288936 -4.4288774 -4.4288659][-4.4287968 -4.4288034 -4.4288254 -4.4288445 -4.4288635 -4.4288716 -4.4288635 -4.4288421 -4.428834 -4.4288573 -4.4288993 -4.4289217 -4.4289165 -4.4288983 -4.4288816][-4.4287062 -4.4287271 -4.4287548 -4.4287686 -4.4287758 -4.4287705 -4.4287658 -4.428761 -4.4287815 -4.4288297 -4.4288869 -4.4289227 -4.42893 -4.4289184 -4.4289002][-4.4285908 -4.4286222 -4.4286547 -4.4286642 -4.4286518 -4.4286289 -4.4286256 -4.4286389 -4.42869 -4.428761 -4.4288325 -4.4288831 -4.4289141 -4.4289207 -4.4289093][-4.4285069 -4.42853 -4.4285545 -4.4285474 -4.4285069 -4.4284596 -4.4284554 -4.4284825 -4.4285545 -4.4286442 -4.4287314 -4.4288039 -4.4288616 -4.4288936 -4.4289][-4.4285221 -4.4285207 -4.42852 -4.4284949 -4.4284329 -4.4283671 -4.4283442 -4.4283571 -4.4284282 -4.4285269 -4.4286261 -4.4287224 -4.4288054 -4.42886 -4.4288845][-4.4286189 -4.4285922 -4.4285665 -4.4285364 -4.4284854 -4.4284258 -4.4283814 -4.4283562 -4.4283962 -4.4284854 -4.4285889 -4.4286985 -4.4287925 -4.4288535 -4.42888][-4.4287281 -4.4286876 -4.4286509 -4.4286284 -4.4285979 -4.4285612 -4.4285212 -4.42848 -4.4284925 -4.4285612 -4.4286523 -4.4287486 -4.4288259 -4.428875 -4.4288893][-4.4288316 -4.4287934 -4.4287567 -4.4287415 -4.4287229 -4.4287019 -4.4286733 -4.4286342 -4.4286356 -4.4286895 -4.42876 -4.4288263 -4.4288731 -4.4289007 -4.4288988][-4.4289093 -4.4288883 -4.428863 -4.4288588 -4.4288507 -4.4288368 -4.4288139 -4.428781 -4.4287744 -4.4288073 -4.4288507 -4.428885 -4.428905 -4.4289141 -4.4289007][-4.4289594 -4.4289527 -4.4289393 -4.4289441 -4.4289422 -4.4289312 -4.428915 -4.4288912 -4.4288807 -4.4288921 -4.4289055 -4.42891 -4.4289107 -4.4289074 -4.4288907][-4.4289804 -4.4289813 -4.4289756 -4.4289832 -4.4289842 -4.4289761 -4.428968 -4.4289556 -4.4289489 -4.428947 -4.4289379 -4.4289207 -4.4289079 -4.4288955 -4.4288783][-4.4289784 -4.4289856 -4.4289832 -4.428987 -4.4289875 -4.4289856 -4.4289851 -4.4289827 -4.4289823 -4.4289732 -4.4289508 -4.4289227 -4.4289036 -4.4288883 -4.4288707]]...]
INFO - root - 2017-12-08 05:40:05.244683: step 12610, loss = 2.28, batch loss = 2.23 (37.2 examples/sec; 0.215 sec/batch; 19h:06m:49s remains)
INFO - root - 2017-12-08 05:40:07.508016: step 12620, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 19h:49m:01s remains)
INFO - root - 2017-12-08 05:40:09.767819: step 12630, loss = 2.28, batch loss = 2.23 (35.8 examples/sec; 0.223 sec/batch; 19h:50m:40s remains)
INFO - root - 2017-12-08 05:40:12.036530: step 12640, loss = 2.28, batch loss = 2.23 (34.1 examples/sec; 0.235 sec/batch; 20h:52m:07s remains)
INFO - root - 2017-12-08 05:40:14.290684: step 12650, loss = 2.28, batch loss = 2.23 (36.6 examples/sec; 0.218 sec/batch; 19h:24m:40s remains)
INFO - root - 2017-12-08 05:40:16.533733: step 12660, loss = 2.28, batch loss = 2.23 (35.2 examples/sec; 0.227 sec/batch; 20h:10m:09s remains)
INFO - root - 2017-12-08 05:40:18.761489: step 12670, loss = 2.28, batch loss = 2.23 (34.9 examples/sec; 0.229 sec/batch; 20h:23m:18s remains)
INFO - root - 2017-12-08 05:40:20.995359: step 12680, loss = 2.28, batch loss = 2.23 (34.2 examples/sec; 0.234 sec/batch; 20h:46m:24s remains)
INFO - root - 2017-12-08 05:40:23.235492: step 12690, loss = 2.28, batch loss = 2.23 (34.6 examples/sec; 0.231 sec/batch; 20h:31m:33s remains)
INFO - root - 2017-12-08 05:40:25.472252: step 12700, loss = 2.28, batch loss = 2.23 (37.6 examples/sec; 0.213 sec/batch; 18h:52m:41s remains)
2017-12-08 05:40:25.763505: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289241 -4.4289422 -4.4289484 -4.4289494 -4.4289436 -4.4289422 -4.4289513 -4.4289613 -4.428978 -4.4289889 -4.4289932 -4.4289813 -4.4289389 -4.4288759 -4.4288259][-4.4289293 -4.4289513 -4.4289622 -4.428967 -4.4289637 -4.4289651 -4.4289684 -4.4289651 -4.4289675 -4.4289684 -4.4289627 -4.4289527 -4.4289136 -4.428863 -4.4288177][-4.4289188 -4.4289508 -4.4289742 -4.4289851 -4.4289761 -4.4289675 -4.4289551 -4.4289365 -4.4289269 -4.4289179 -4.4289064 -4.4289007 -4.4288716 -4.4288368 -4.428802][-4.4289236 -4.4289589 -4.4289818 -4.4289846 -4.428957 -4.4289246 -4.4288869 -4.4288478 -4.4288311 -4.4288211 -4.4288106 -4.4288106 -4.4287963 -4.428781 -4.4287672][-4.4289322 -4.4289546 -4.428956 -4.4289331 -4.4288783 -4.4288168 -4.4287534 -4.4286895 -4.428679 -4.4286938 -4.4286962 -4.4287043 -4.4287086 -4.4287095 -4.4287114][-4.4289117 -4.4289122 -4.4288883 -4.4288368 -4.4287567 -4.428679 -4.4285812 -4.4284749 -4.42848 -4.4285331 -4.4285617 -4.4285865 -4.4286103 -4.42862 -4.4286337][-4.4288445 -4.4288096 -4.4287505 -4.4286695 -4.4285717 -4.4284797 -4.4283338 -4.4281731 -4.4282246 -4.4283495 -4.4284163 -4.4284716 -4.4285216 -4.42854 -4.4285517][-4.42874 -4.428658 -4.428566 -4.4284725 -4.4283686 -4.428267 -4.4280987 -4.4279237 -4.4280648 -4.4282622 -4.4283614 -4.4284348 -4.4285059 -4.4285259 -4.4285131][-4.4286323 -4.4285264 -4.4284353 -4.4283519 -4.4282703 -4.4282193 -4.4281521 -4.4281054 -4.4282513 -4.4284067 -4.4284792 -4.4285426 -4.4286103 -4.4286208 -4.4285855][-4.4285903 -4.4285111 -4.4284587 -4.4284129 -4.4283729 -4.4283876 -4.4284158 -4.4284444 -4.4285393 -4.4286242 -4.4286618 -4.4287081 -4.428762 -4.4287634 -4.4287314][-4.4286408 -4.4286184 -4.4286156 -4.4286 -4.4285841 -4.4286165 -4.4286647 -4.4287 -4.42875 -4.4287896 -4.4288087 -4.4288373 -4.4288731 -4.4288721 -4.428854][-4.4287186 -4.4287415 -4.4287758 -4.42878 -4.4287777 -4.4288054 -4.4288421 -4.4288588 -4.42888 -4.4288988 -4.4289021 -4.4289107 -4.4289289 -4.4289303 -4.4289246][-4.4287424 -4.4287925 -4.4288478 -4.4288721 -4.4288774 -4.4288964 -4.4289184 -4.4289217 -4.4289274 -4.4289308 -4.4289222 -4.4289174 -4.4289188 -4.4289169 -4.4289169][-4.4286871 -4.4287562 -4.4288292 -4.4288678 -4.4288793 -4.4288874 -4.4288964 -4.4288921 -4.4288888 -4.4288821 -4.4288669 -4.428854 -4.428844 -4.4288392 -4.4288454][-4.4285645 -4.4286366 -4.4287138 -4.4287586 -4.4287734 -4.4287758 -4.4287772 -4.4287696 -4.4287639 -4.4287539 -4.4287362 -4.4287176 -4.4287024 -4.4287009 -4.4287152]]...]
INFO - root - 2017-12-08 05:40:28.006905: step 12710, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 19h:39m:12s remains)
INFO - root - 2017-12-08 05:40:30.236922: step 12720, loss = 2.28, batch loss = 2.23 (35.4 examples/sec; 0.226 sec/batch; 20h:04m:11s remains)
INFO - root - 2017-12-08 05:40:32.466248: step 12730, loss = 2.28, batch loss = 2.23 (36.9 examples/sec; 0.217 sec/batch; 19h:15m:15s remains)
INFO - root - 2017-12-08 05:40:34.720046: step 12740, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.222 sec/batch; 19h:41m:34s remains)
INFO - root - 2017-12-08 05:40:36.971898: step 12750, loss = 2.28, batch loss = 2.23 (35.3 examples/sec; 0.227 sec/batch; 20h:07m:57s remains)
INFO - root - 2017-12-08 05:40:39.246286: step 12760, loss = 2.28, batch loss = 2.23 (36.6 examples/sec; 0.219 sec/batch; 19h:26m:07s remains)
INFO - root - 2017-12-08 05:40:41.469311: step 12770, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 19h:30m:30s remains)
INFO - root - 2017-12-08 05:40:43.716507: step 12780, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.221 sec/batch; 19h:39m:50s remains)
INFO - root - 2017-12-08 05:40:45.945486: step 12790, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 19h:26m:47s remains)
INFO - root - 2017-12-08 05:40:48.232673: step 12800, loss = 2.28, batch loss = 2.23 (34.1 examples/sec; 0.235 sec/batch; 20h:51m:33s remains)
2017-12-08 05:40:48.512600: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.428895 -4.428895 -4.4288759 -4.4288464 -4.428822 -4.428812 -4.42881 -4.4288163 -4.4288292 -4.4288416 -4.4288497 -4.4288568 -4.4288621 -4.4288607 -4.4288497][-4.4289112 -4.4289074 -4.428874 -4.4288239 -4.4287906 -4.4287791 -4.4287763 -4.4287887 -4.428813 -4.4288359 -4.4288521 -4.428865 -4.428875 -4.4288788 -4.428874][-4.4289083 -4.4289012 -4.4288487 -4.4287758 -4.4287281 -4.4287114 -4.4287071 -4.42873 -4.428772 -4.4288073 -4.4288306 -4.42885 -4.4288673 -4.4288788 -4.4288845][-4.428884 -4.42887 -4.4288034 -4.4287133 -4.4286432 -4.4286046 -4.4285922 -4.4286318 -4.4286962 -4.4287434 -4.4287734 -4.4287963 -4.4288158 -4.4288373 -4.4288554][-4.4288459 -4.4288263 -4.4287543 -4.4286571 -4.4285669 -4.4284921 -4.4284577 -4.4285173 -4.4286141 -4.4286828 -4.428721 -4.4287438 -4.428762 -4.4287868 -4.4288149][-4.4287639 -4.4287357 -4.4286633 -4.4285636 -4.4284482 -4.4283156 -4.428237 -4.4283276 -4.4284835 -4.4285927 -4.4286466 -4.4286757 -4.4286976 -4.4287267 -4.4287648][-4.4286804 -4.4286389 -4.4285665 -4.42847 -4.4283395 -4.4281516 -4.4280052 -4.4281268 -4.4283485 -4.4284916 -4.4285631 -4.4285979 -4.4286232 -4.4286623 -4.4287152][-4.4286904 -4.4286356 -4.4285636 -4.4284792 -4.4283648 -4.4281759 -4.4280071 -4.4281173 -4.4283404 -4.4284782 -4.42855 -4.4285827 -4.4286103 -4.4286594 -4.4287205][-4.4287081 -4.42865 -4.4285922 -4.4285345 -4.428462 -4.428329 -4.4281988 -4.4282646 -4.4284182 -4.4285088 -4.4285569 -4.4285755 -4.4285989 -4.4286518 -4.4287167][-4.4287148 -4.428669 -4.4286327 -4.4286036 -4.4285665 -4.4284778 -4.4283786 -4.4284205 -4.4285278 -4.4285808 -4.4286032 -4.4286041 -4.4286151 -4.4286604 -4.4287186][-4.4287357 -4.4287109 -4.4286957 -4.4286847 -4.4286695 -4.4286051 -4.4285235 -4.4285512 -4.4286332 -4.4286671 -4.4286718 -4.4286551 -4.4286528 -4.4286871 -4.4287324][-4.4287906 -4.4287777 -4.4287744 -4.4287763 -4.4287715 -4.4287205 -4.4286489 -4.4286609 -4.42872 -4.4287434 -4.4287419 -4.4287157 -4.4287038 -4.4287257 -4.4287558][-4.4288611 -4.428853 -4.42885 -4.4288554 -4.4288559 -4.4288211 -4.4287629 -4.4287648 -4.4288049 -4.4288225 -4.4288177 -4.4287896 -4.4287729 -4.4287815 -4.4287906][-4.4288969 -4.4288926 -4.4288893 -4.4288988 -4.4289079 -4.4288921 -4.4288564 -4.4288535 -4.4288731 -4.4288754 -4.4288654 -4.4288425 -4.42883 -4.42883 -4.4288235][-4.4289107 -4.4289074 -4.4289045 -4.4289136 -4.4289255 -4.428925 -4.4289141 -4.4289179 -4.4289289 -4.4289231 -4.4289083 -4.4288893 -4.4288774 -4.4288712 -4.4288578]]...]
INFO - root - 2017-12-08 05:40:50.728655: step 12810, loss = 2.28, batch loss = 2.23 (35.7 examples/sec; 0.224 sec/batch; 19h:54m:48s remains)
INFO - root - 2017-12-08 05:40:53.010842: step 12820, loss = 2.28, batch loss = 2.23 (33.0 examples/sec; 0.243 sec/batch; 21h:33m:30s remains)
INFO - root - 2017-12-08 05:40:55.207738: step 12830, loss = 2.28, batch loss = 2.23 (35.8 examples/sec; 0.223 sec/batch; 19h:48m:57s remains)
INFO - root - 2017-12-08 05:40:57.442449: step 12840, loss = 2.28, batch loss = 2.23 (35.7 examples/sec; 0.224 sec/batch; 19h:53m:22s remains)
INFO - root - 2017-12-08 05:40:59.665316: step 12850, loss = 2.28, batch loss = 2.23 (35.3 examples/sec; 0.227 sec/batch; 20h:08m:02s remains)
INFO - root - 2017-12-08 05:41:01.898574: step 12860, loss = 2.28, batch loss = 2.23 (36.8 examples/sec; 0.218 sec/batch; 19h:18m:55s remains)
INFO - root - 2017-12-08 05:41:04.154825: step 12870, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.221 sec/batch; 19h:34m:59s remains)
INFO - root - 2017-12-08 05:41:06.380998: step 12880, loss = 2.28, batch loss = 2.23 (36.7 examples/sec; 0.218 sec/batch; 19h:22m:30s remains)
INFO - root - 2017-12-08 05:41:08.630887: step 12890, loss = 2.28, batch loss = 2.23 (35.0 examples/sec; 0.229 sec/batch; 20h:17m:46s remains)
INFO - root - 2017-12-08 05:41:10.875727: step 12900, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.220 sec/batch; 19h:32m:37s remains)
2017-12-08 05:41:11.161230: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.428905 -4.428926 -4.4289384 -4.4289408 -4.4289379 -4.4289436 -4.4289522 -4.4289584 -4.4289584 -4.4289508 -4.4289274 -4.4288893 -4.4288573 -4.4288473 -4.4288597][-4.4288707 -4.428906 -4.4289241 -4.4289217 -4.4289093 -4.4289131 -4.4289203 -4.4289265 -4.4289317 -4.4289269 -4.4288964 -4.4288416 -4.4287858 -4.4287648 -4.4287829][-4.4288244 -4.428875 -4.4289031 -4.4288979 -4.4288731 -4.4288664 -4.4288592 -4.4288573 -4.4288745 -4.428884 -4.4288554 -4.4287915 -4.4287248 -4.4287028 -4.4287252][-4.4287605 -4.4288225 -4.4288568 -4.4288492 -4.4288135 -4.4287877 -4.4287543 -4.4287434 -4.4287839 -4.4288211 -4.4288139 -4.4287591 -4.4286952 -4.4286737 -4.4286866][-4.4287062 -4.4287682 -4.4287972 -4.4287758 -4.4287167 -4.4286494 -4.4285755 -4.42857 -4.4286585 -4.428741 -4.4287677 -4.4287338 -4.4286828 -4.4286618 -4.4286556][-4.4286795 -4.4287367 -4.4287548 -4.4287152 -4.4286242 -4.428493 -4.4283619 -4.4283695 -4.4285274 -4.4286628 -4.4287224 -4.4287105 -4.4286733 -4.4286561 -4.4286337][-4.4286809 -4.4287348 -4.428751 -4.4287004 -4.4285822 -4.4283915 -4.4281979 -4.4282227 -4.4284444 -4.4286151 -4.428699 -4.4287086 -4.4286828 -4.4286647 -4.4286308][-4.428689 -4.4287496 -4.4287806 -4.4287415 -4.4286208 -4.4284153 -4.4282088 -4.4282374 -4.4284592 -4.4286265 -4.4287195 -4.428741 -4.4287243 -4.4287076 -4.4286652][-4.4286933 -4.42876 -4.4288087 -4.4287949 -4.4287009 -4.4285345 -4.42838 -4.4284029 -4.4285626 -4.4286904 -4.4287667 -4.4287858 -4.4287767 -4.428762 -4.428719][-4.4286957 -4.4287558 -4.4288054 -4.4288154 -4.428761 -4.4286547 -4.4285655 -4.4285831 -4.4286761 -4.4287505 -4.4287982 -4.4288116 -4.4288135 -4.4288025 -4.4287696][-4.4287019 -4.4287438 -4.428772 -4.42878 -4.4287591 -4.4287095 -4.4286695 -4.4286833 -4.4287319 -4.4287715 -4.4287968 -4.4288054 -4.4288106 -4.428803 -4.4287834][-4.4287071 -4.4287262 -4.4287276 -4.4287262 -4.4287262 -4.4287162 -4.4287095 -4.428721 -4.4287395 -4.4287524 -4.4287596 -4.4287605 -4.4287653 -4.4287643 -4.4287553][-4.4287066 -4.4287109 -4.4287148 -4.4287176 -4.4287286 -4.42874 -4.4287515 -4.4287581 -4.4287505 -4.4287372 -4.42873 -4.4287291 -4.4287386 -4.4287477 -4.4287496][-4.4286909 -4.4286885 -4.4287148 -4.4287467 -4.4287729 -4.4287891 -4.4287982 -4.4287982 -4.4287729 -4.428741 -4.4287267 -4.42873 -4.4287443 -4.4287577 -4.4287667][-4.4286575 -4.4286389 -4.4286847 -4.4287472 -4.4287896 -4.4288054 -4.42881 -4.4288106 -4.4287872 -4.4287548 -4.4287424 -4.4287496 -4.4287643 -4.4287753 -4.4287858]]...]
INFO - root - 2017-12-08 05:41:13.385314: step 12910, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.220 sec/batch; 19h:33m:55s remains)
INFO - root - 2017-12-08 05:41:15.608155: step 12920, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 19h:48m:06s remains)
INFO - root - 2017-12-08 05:41:17.843856: step 12930, loss = 2.28, batch loss = 2.23 (36.0 examples/sec; 0.222 sec/batch; 19h:42m:10s remains)
INFO - root - 2017-12-08 05:41:20.109867: step 12940, loss = 2.28, batch loss = 2.23 (33.7 examples/sec; 0.238 sec/batch; 21h:05m:56s remains)
INFO - root - 2017-12-08 05:41:22.333105: step 12950, loss = 2.28, batch loss = 2.23 (37.3 examples/sec; 0.215 sec/batch; 19h:03m:08s remains)
INFO - root - 2017-12-08 05:41:24.561682: step 12960, loss = 2.28, batch loss = 2.23 (37.0 examples/sec; 0.216 sec/batch; 19h:10m:09s remains)
INFO - root - 2017-12-08 05:41:26.840621: step 12970, loss = 2.28, batch loss = 2.23 (36.0 examples/sec; 0.222 sec/batch; 19h:43m:09s remains)
INFO - root - 2017-12-08 05:41:29.095355: step 12980, loss = 2.28, batch loss = 2.23 (35.8 examples/sec; 0.224 sec/batch; 19h:50m:47s remains)
INFO - root - 2017-12-08 05:41:31.325614: step 12990, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 19h:30m:35s remains)
INFO - root - 2017-12-08 05:41:33.606417: step 13000, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 19h:47m:17s remains)
2017-12-08 05:41:33.905687: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288764 -4.42883 -4.4287763 -4.4287238 -4.4286904 -4.4286866 -4.4287214 -4.4287434 -4.4287267 -4.4286919 -4.4286718 -4.4286714 -4.4286838 -4.4287052 -4.4287224][-4.4288635 -4.4288082 -4.42875 -4.4286981 -4.4286647 -4.4286647 -4.4287066 -4.4287391 -4.4287267 -4.428678 -4.4286356 -4.4286337 -4.4286556 -4.4286766 -4.4286866][-4.4288454 -4.4287825 -4.428719 -4.4286613 -4.4286261 -4.4286308 -4.4286647 -4.4286933 -4.4286871 -4.4286294 -4.4285707 -4.4285717 -4.4286094 -4.4286375 -4.4286356][-4.4288054 -4.42873 -4.428659 -4.4285936 -4.42855 -4.4285502 -4.4285712 -4.4285903 -4.4285908 -4.428544 -4.4284921 -4.4284973 -4.4285512 -4.4285984 -4.4285989][-4.4287567 -4.4286666 -4.4285889 -4.4285231 -4.4284744 -4.4284639 -4.4284825 -4.4284992 -4.4285064 -4.4284854 -4.4284558 -4.4284682 -4.4285431 -4.4286213 -4.428628][-4.4287252 -4.4286337 -4.4285574 -4.4284892 -4.4284339 -4.4284115 -4.4284239 -4.4284315 -4.4284415 -4.4284415 -4.4284291 -4.4284506 -4.4285369 -4.4286313 -4.4286442][-4.4287062 -4.4286194 -4.4285522 -4.4284787 -4.4284058 -4.4283562 -4.4283514 -4.4283433 -4.4283395 -4.42834 -4.428349 -4.4283977 -4.4284816 -4.4285588 -4.4285631][-4.4287062 -4.4286308 -4.4285769 -4.4285059 -4.4284182 -4.4283366 -4.4283066 -4.4282923 -4.4282856 -4.4282918 -4.4283218 -4.4283819 -4.4284406 -4.4284825 -4.4284749][-4.4287319 -4.4286823 -4.4286523 -4.4286008 -4.42852 -4.4284263 -4.42839 -4.4283843 -4.4283862 -4.4283895 -4.428412 -4.4284635 -4.4284959 -4.42851 -4.4285088][-4.42879 -4.4287724 -4.4287672 -4.4287391 -4.4286804 -4.4286017 -4.4285703 -4.4285769 -4.4285827 -4.4285851 -4.4285936 -4.42862 -4.428628 -4.4286284 -4.4286442][-4.4288425 -4.4288464 -4.4288564 -4.4288445 -4.428803 -4.4287515 -4.4287291 -4.4287357 -4.42874 -4.4287353 -4.4287372 -4.4287515 -4.4287496 -4.4287453 -4.4287672][-4.4288692 -4.4288735 -4.428885 -4.428884 -4.4288597 -4.4288254 -4.4288034 -4.4288034 -4.4288044 -4.4287992 -4.4288044 -4.4288187 -4.4288139 -4.4288087 -4.4288321][-4.4289002 -4.4288988 -4.4289069 -4.4289093 -4.4289 -4.4288797 -4.42886 -4.4288549 -4.4288511 -4.4288511 -4.4288626 -4.4288769 -4.4288731 -4.4288664 -4.4288836][-4.4289532 -4.4289503 -4.4289541 -4.4289579 -4.428957 -4.4289484 -4.428937 -4.4289308 -4.4289274 -4.4289269 -4.4289336 -4.4289436 -4.4289422 -4.42894 -4.4289508][-4.428998 -4.4289985 -4.4290004 -4.4290042 -4.4290075 -4.4290061 -4.4290028 -4.4290013 -4.429 -4.4289994 -4.4290013 -4.4290032 -4.4290013 -4.4290004 -4.4290051]]...]
INFO - root - 2017-12-08 05:41:36.122975: step 13010, loss = 2.28, batch loss = 2.23 (37.2 examples/sec; 0.215 sec/batch; 19h:04m:56s remains)
INFO - root - 2017-12-08 05:41:38.377738: step 13020, loss = 2.28, batch loss = 2.23 (35.0 examples/sec; 0.229 sec/batch; 20h:16m:46s remains)
INFO - root - 2017-12-08 05:41:40.646664: step 13030, loss = 2.28, batch loss = 2.23 (34.9 examples/sec; 0.229 sec/batch; 20h:21m:41s remains)
INFO - root - 2017-12-08 05:41:42.886647: step 13040, loss = 2.28, batch loss = 2.23 (34.8 examples/sec; 0.230 sec/batch; 20h:22m:36s remains)
INFO - root - 2017-12-08 05:41:45.116640: step 13050, loss = 2.28, batch loss = 2.23 (34.7 examples/sec; 0.231 sec/batch; 20h:28m:10s remains)
INFO - root - 2017-12-08 05:41:47.349251: step 13060, loss = 2.28, batch loss = 2.23 (35.6 examples/sec; 0.225 sec/batch; 19h:56m:31s remains)
INFO - root - 2017-12-08 05:41:49.554211: step 13070, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 19h:31m:14s remains)
INFO - root - 2017-12-08 05:41:51.777931: step 13080, loss = 2.28, batch loss = 2.23 (35.7 examples/sec; 0.224 sec/batch; 19h:53m:41s remains)
INFO - root - 2017-12-08 05:41:54.074105: step 13090, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.226 sec/batch; 20h:00m:52s remains)
INFO - root - 2017-12-08 05:41:56.299869: step 13100, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 19h:46m:21s remains)
2017-12-08 05:41:56.587422: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289927 -4.4289675 -4.428937 -4.4289122 -4.4289007 -4.4288945 -4.428885 -4.4288778 -4.4288716 -4.4288626 -4.4288497 -4.4288473 -4.428853 -4.4288621 -4.4288721][-4.4289675 -4.4289289 -4.4288883 -4.4288521 -4.4288325 -4.42882 -4.4288015 -4.4287906 -4.4287906 -4.4287891 -4.4287744 -4.4287734 -4.4287829 -4.4287944 -4.4288][-4.4289284 -4.4288721 -4.4288158 -4.4287615 -4.4287238 -4.4286957 -4.4286661 -4.4286556 -4.4286709 -4.4286847 -4.4286714 -4.4286704 -4.4286895 -4.4287076 -4.4287138][-4.4288797 -4.4288054 -4.42873 -4.4286571 -4.4286003 -4.4285517 -4.4285159 -4.428514 -4.4285417 -4.4285612 -4.4285545 -4.4285583 -4.42859 -4.4286232 -4.428637][-4.4288282 -4.4287381 -4.42864 -4.4285469 -4.4284706 -4.4283996 -4.42835 -4.4283552 -4.4283934 -4.4284225 -4.4284148 -4.4284167 -4.4284616 -4.4285131 -4.4285412][-4.4287796 -4.4286795 -4.4285612 -4.4284463 -4.428349 -4.4282532 -4.4281821 -4.4281783 -4.4282284 -4.4282684 -4.4282656 -4.4282804 -4.4283495 -4.428421 -4.42846][-4.4287419 -4.428627 -4.4284849 -4.4283481 -4.4282317 -4.428112 -4.4279938 -4.4279642 -4.4280381 -4.4281073 -4.4281344 -4.4281931 -4.4282932 -4.4283738 -4.4284158][-4.4287109 -4.428576 -4.4284048 -4.42825 -4.4281206 -4.427979 -4.4278297 -4.4278026 -4.4279294 -4.4280562 -4.4281254 -4.4282069 -4.4283104 -4.4283876 -4.4284258][-4.4286871 -4.428545 -4.4283676 -4.4282207 -4.4281135 -4.4280062 -4.4278994 -4.4279032 -4.42805 -4.42818 -4.4282541 -4.4283185 -4.4284086 -4.428484 -4.4285169][-4.4286919 -4.4285722 -4.4284334 -4.4283271 -4.4282594 -4.4281979 -4.4281368 -4.4281421 -4.4282436 -4.4283304 -4.4283853 -4.4284372 -4.4285278 -4.4286118 -4.4286575][-4.42873 -4.42865 -4.4285612 -4.4284949 -4.4284496 -4.4284072 -4.4283614 -4.4283557 -4.4284124 -4.4284606 -4.4284887 -4.4285369 -4.42863 -4.4287181 -4.4287744][-4.4287949 -4.4287496 -4.4287019 -4.4286618 -4.4286265 -4.4285955 -4.4285688 -4.428565 -4.4286027 -4.4286404 -4.4286628 -4.4287028 -4.4287729 -4.4288363 -4.428874][-4.4288588 -4.4288321 -4.428802 -4.4287758 -4.4287519 -4.4287391 -4.4287357 -4.4287457 -4.4287844 -4.4288182 -4.4288354 -4.4288621 -4.4288983 -4.4289255 -4.4289384][-4.4288979 -4.4288759 -4.4288545 -4.4288435 -4.4288359 -4.4288383 -4.428854 -4.4288797 -4.4289145 -4.4289346 -4.4289355 -4.4289389 -4.4289484 -4.4289556 -4.4289637][-4.4289308 -4.428906 -4.4288855 -4.4288783 -4.4288778 -4.4288869 -4.4289045 -4.4289274 -4.4289546 -4.4289684 -4.428966 -4.4289627 -4.4289618 -4.4289637 -4.4289722]]...]
INFO - root - 2017-12-08 05:41:58.812544: step 13110, loss = 2.28, batch loss = 2.23 (36.0 examples/sec; 0.222 sec/batch; 19h:43m:01s remains)
INFO - root - 2017-12-08 05:42:01.074746: step 13120, loss = 2.28, batch loss = 2.23 (33.8 examples/sec; 0.237 sec/batch; 20h:59m:20s remains)
INFO - root - 2017-12-08 05:42:03.295968: step 13130, loss = 2.28, batch loss = 2.23 (35.4 examples/sec; 0.226 sec/batch; 20h:01m:14s remains)
INFO - root - 2017-12-08 05:42:05.550704: step 13140, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.221 sec/batch; 19h:38m:09s remains)
INFO - root - 2017-12-08 05:42:07.806090: step 13150, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.225 sec/batch; 19h:58m:13s remains)
INFO - root - 2017-12-08 05:42:10.072042: step 13160, loss = 2.28, batch loss = 2.23 (35.8 examples/sec; 0.223 sec/batch; 19h:48m:07s remains)
INFO - root - 2017-12-08 05:42:12.354281: step 13170, loss = 2.28, batch loss = 2.23 (35.1 examples/sec; 0.228 sec/batch; 20h:14m:28s remains)
INFO - root - 2017-12-08 05:42:14.572640: step 13180, loss = 2.28, batch loss = 2.23 (37.0 examples/sec; 0.216 sec/batch; 19h:09m:53s remains)
INFO - root - 2017-12-08 05:42:16.805160: step 13190, loss = 2.28, batch loss = 2.23 (37.0 examples/sec; 0.216 sec/batch; 19h:09m:43s remains)
INFO - root - 2017-12-08 05:42:19.034834: step 13200, loss = 2.28, batch loss = 2.23 (35.3 examples/sec; 0.227 sec/batch; 20h:07m:32s remains)
2017-12-08 05:42:19.315637: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288788 -4.4288716 -4.4288678 -4.4288573 -4.4288521 -4.4288559 -4.4288669 -4.4288864 -4.4288926 -4.4288836 -4.4288688 -4.428863 -4.4288626 -4.4288511 -4.4288459][-4.4288573 -4.4288468 -4.4288359 -4.4288158 -4.4288063 -4.4288111 -4.4288239 -4.4288435 -4.4288478 -4.4288478 -4.4288392 -4.4288368 -4.4288363 -4.428812 -4.4288015][-4.428812 -4.428792 -4.4287686 -4.4287386 -4.4287248 -4.4287281 -4.428751 -4.4287734 -4.4287739 -4.4287877 -4.4287963 -4.4287996 -4.4287915 -4.428741 -4.4287233][-4.4287634 -4.4287319 -4.4286985 -4.4286585 -4.4286361 -4.4286361 -4.4286695 -4.4286952 -4.4286966 -4.4287214 -4.4287558 -4.4287715 -4.4287596 -4.4286819 -4.428648][-4.4287214 -4.4286809 -4.4286404 -4.4285936 -4.4285531 -4.4285359 -4.4285693 -4.4286051 -4.4286122 -4.4286413 -4.4286985 -4.42873 -4.42872 -4.4286227 -4.4285784][-4.4286995 -4.4286551 -4.4286041 -4.4285512 -4.4284878 -4.4284415 -4.4284768 -4.4285274 -4.4285407 -4.4285645 -4.4286308 -4.4286733 -4.4286618 -4.42855 -4.4285064][-4.428709 -4.4286714 -4.4286156 -4.4285507 -4.428463 -4.4283991 -4.428431 -4.428484 -4.4285121 -4.4285326 -4.4285936 -4.4286289 -4.4285951 -4.428463 -4.4284368][-4.4287381 -4.4287167 -4.4286723 -4.4286022 -4.4285078 -4.4284492 -4.4284782 -4.4285178 -4.4285469 -4.4285483 -4.4285808 -4.4286041 -4.4285502 -4.4284048 -4.4284005][-4.4287562 -4.4287543 -4.42873 -4.4286804 -4.4286075 -4.4285631 -4.4285793 -4.4285984 -4.4286127 -4.428596 -4.4286041 -4.4286132 -4.4285412 -4.4284005 -4.4284163][-4.4287548 -4.4287667 -4.4287653 -4.42875 -4.4287033 -4.4286628 -4.4286613 -4.4286571 -4.4286566 -4.4286323 -4.4286332 -4.4286356 -4.4285727 -4.4284625 -4.4284906][-4.4287519 -4.4287672 -4.4287729 -4.4287806 -4.4287629 -4.4287229 -4.4287014 -4.4286733 -4.4286613 -4.4286513 -4.4286594 -4.4286623 -4.4286218 -4.4285374 -4.4285593][-4.4287672 -4.4287767 -4.4287815 -4.4287877 -4.428782 -4.4287391 -4.4286952 -4.4286461 -4.4286385 -4.4286585 -4.4286785 -4.4286861 -4.4286613 -4.42859 -4.4285908][-4.4287863 -4.4287934 -4.4287949 -4.4287887 -4.4287825 -4.4287391 -4.4286871 -4.4286437 -4.428647 -4.42868 -4.4287047 -4.428719 -4.4287004 -4.4286308 -4.4286008][-4.4288044 -4.4288187 -4.4288268 -4.4288106 -4.428793 -4.428751 -4.4287071 -4.4286914 -4.4287086 -4.4287386 -4.4287591 -4.428771 -4.4287519 -4.428688 -4.4286432][-4.4288225 -4.4288449 -4.4288616 -4.4288535 -4.4288349 -4.4288025 -4.4287724 -4.4287763 -4.4288006 -4.4288259 -4.4288392 -4.4288478 -4.4288316 -4.4287829 -4.4287395]]...]
INFO - root - 2017-12-08 05:42:21.556160: step 13210, loss = 2.28, batch loss = 2.23 (35.6 examples/sec; 0.225 sec/batch; 19h:55m:25s remains)
INFO - root - 2017-12-08 05:42:23.791853: step 13220, loss = 2.28, batch loss = 2.23 (35.6 examples/sec; 0.225 sec/batch; 19h:55m:57s remains)
INFO - root - 2017-12-08 05:42:26.012955: step 13230, loss = 2.28, batch loss = 2.23 (36.6 examples/sec; 0.218 sec/batch; 19h:22m:00s remains)
INFO - root - 2017-12-08 05:42:28.281342: step 13240, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 19h:46m:46s remains)
INFO - root - 2017-12-08 05:42:30.524760: step 13250, loss = 2.28, batch loss = 2.23 (35.0 examples/sec; 0.228 sec/batch; 20h:14m:45s remains)
INFO - root - 2017-12-08 05:42:32.753040: step 13260, loss = 2.28, batch loss = 2.23 (34.9 examples/sec; 0.229 sec/batch; 20h:20m:52s remains)
INFO - root - 2017-12-08 05:42:35.012097: step 13270, loss = 2.28, batch loss = 2.23 (33.7 examples/sec; 0.237 sec/batch; 21h:01m:39s remains)
INFO - root - 2017-12-08 05:42:37.239704: step 13280, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.222 sec/batch; 19h:38m:31s remains)
INFO - root - 2017-12-08 05:42:39.526879: step 13290, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 19h:45m:33s remains)
INFO - root - 2017-12-08 05:42:41.772625: step 13300, loss = 2.28, batch loss = 2.23 (35.3 examples/sec; 0.226 sec/batch; 20h:04m:34s remains)
2017-12-08 05:42:42.101073: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4287825 -4.4287848 -4.4287872 -4.4287691 -4.4287376 -4.4287052 -4.4286489 -4.4285502 -4.4284315 -4.4283624 -4.4283967 -4.4284759 -4.4285307 -4.4285336 -4.4284787][-4.4288163 -4.4288225 -4.4288211 -4.4287958 -4.4287591 -4.4287233 -4.4286819 -4.4286118 -4.4285226 -4.4284778 -4.4284949 -4.4285297 -4.4285507 -4.4285493 -4.4285135][-4.4288216 -4.4288263 -4.42882 -4.4287915 -4.4287462 -4.4287071 -4.4286814 -4.4286337 -4.4285712 -4.4285412 -4.4285426 -4.4285474 -4.4285445 -4.4285426 -4.4285278][-4.4287939 -4.4287992 -4.428792 -4.4287577 -4.4287004 -4.42866 -4.4286375 -4.4285903 -4.4285345 -4.4285126 -4.4285111 -4.4285207 -4.4285316 -4.4285421 -4.428544][-4.4287677 -4.4287767 -4.4287724 -4.4287329 -4.4286561 -4.4286065 -4.4285817 -4.4285235 -4.428462 -4.4284353 -4.4284387 -4.4284763 -4.4285231 -4.4285531 -4.4285636][-4.4287467 -4.4287496 -4.4287429 -4.4286909 -4.4285989 -4.4285564 -4.4285417 -4.4284792 -4.4284134 -4.4283752 -4.42838 -4.4284463 -4.4285226 -4.4285703 -4.4285879][-4.4286876 -4.4286852 -4.4286704 -4.428606 -4.4285111 -4.4284863 -4.4284973 -4.428453 -4.4283876 -4.4283371 -4.4283471 -4.4284277 -4.428503 -4.4285488 -4.4285693][-4.4286265 -4.4286175 -4.4285984 -4.4285355 -4.4284592 -4.4284573 -4.4284883 -4.4284668 -4.4284158 -4.4283762 -4.4283886 -4.4284387 -4.4284787 -4.4285054 -4.4285316][-4.4286165 -4.42861 -4.42859 -4.42853 -4.4284735 -4.4284925 -4.4285321 -4.4285288 -4.4284983 -4.4284816 -4.4284863 -4.428493 -4.4284849 -4.4284778 -4.4285026][-4.4286609 -4.4286542 -4.4286232 -4.4285583 -4.4285154 -4.4285445 -4.42858 -4.4285808 -4.4285703 -4.4285789 -4.4285831 -4.4285569 -4.4285111 -4.4284749 -4.4284883][-4.4286795 -4.4286666 -4.4286304 -4.4285722 -4.428546 -4.4285674 -4.4285812 -4.4285779 -4.4285827 -4.4286175 -4.428628 -4.4285941 -4.4285407 -4.4284978 -4.428504][-4.4287076 -4.4286923 -4.4286513 -4.4285946 -4.42856 -4.4285536 -4.428545 -4.4285431 -4.4285707 -4.4286194 -4.4286342 -4.428606 -4.428566 -4.4285345 -4.4285383][-4.4287491 -4.4287281 -4.4286804 -4.4286208 -4.4285645 -4.4285297 -4.4285116 -4.4285183 -4.4285622 -4.4286146 -4.428627 -4.4286051 -4.4285817 -4.4285665 -4.4285679][-4.4287519 -4.4287281 -4.428688 -4.4286356 -4.428576 -4.4285297 -4.4285045 -4.4285145 -4.428566 -4.4286146 -4.4286261 -4.428617 -4.4286056 -4.4286 -4.4286017][-4.4287405 -4.4287157 -4.4286819 -4.42864 -4.428587 -4.4285388 -4.4285169 -4.4285316 -4.4285793 -4.4286256 -4.4286456 -4.42865 -4.4286485 -4.428647 -4.4286442]]...]
INFO - root - 2017-12-08 05:42:44.339892: step 13310, loss = 2.28, batch loss = 2.23 (37.4 examples/sec; 0.214 sec/batch; 18h:56m:30s remains)
INFO - root - 2017-12-08 05:42:46.564891: step 13320, loss = 2.28, batch loss = 2.23 (36.8 examples/sec; 0.217 sec/batch; 19h:15m:49s remains)
INFO - root - 2017-12-08 05:42:48.798927: step 13330, loss = 2.28, batch loss = 2.23 (37.2 examples/sec; 0.215 sec/batch; 19h:03m:29s remains)
INFO - root - 2017-12-08 05:42:51.031636: step 13340, loss = 2.28, batch loss = 2.23 (36.7 examples/sec; 0.218 sec/batch; 19h:20m:06s remains)
INFO - root - 2017-12-08 05:42:53.260382: step 13350, loss = 2.28, batch loss = 2.23 (35.8 examples/sec; 0.223 sec/batch; 19h:47m:51s remains)
INFO - root - 2017-12-08 05:42:55.513722: step 13360, loss = 2.28, batch loss = 2.23 (36.9 examples/sec; 0.217 sec/batch; 19h:13m:04s remains)
INFO - root - 2017-12-08 05:42:57.809552: step 13370, loss = 2.28, batch loss = 2.23 (36.7 examples/sec; 0.218 sec/batch; 19h:20m:50s remains)
INFO - root - 2017-12-08 05:43:00.060491: step 13380, loss = 2.28, batch loss = 2.23 (35.8 examples/sec; 0.224 sec/batch; 19h:49m:10s remains)
INFO - root - 2017-12-08 05:43:02.284632: step 13390, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 19h:25m:55s remains)
INFO - root - 2017-12-08 05:43:04.551459: step 13400, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.222 sec/batch; 19h:40m:00s remains)
2017-12-08 05:43:04.843937: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4290104 -4.4290118 -4.4289813 -4.4289317 -4.4288607 -4.4287739 -4.4286833 -4.4286318 -4.4285936 -4.4286027 -4.4286432 -4.4286962 -4.4287496 -4.4288349 -4.4289193][-4.4290118 -4.4290051 -4.4289656 -4.4289036 -4.4288168 -4.4287162 -4.4286304 -4.4285913 -4.428576 -4.4286222 -4.4286995 -4.428761 -4.4288111 -4.428885 -4.4289508][-4.4290113 -4.428997 -4.4289503 -4.4288759 -4.4287753 -4.4286647 -4.4285874 -4.428565 -4.428576 -4.4286432 -4.4287338 -4.4288006 -4.4288554 -4.4289269 -4.4289813][-4.4290085 -4.428987 -4.4289331 -4.4288445 -4.4287271 -4.4286122 -4.4285445 -4.4285393 -4.4285769 -4.4286628 -4.4287496 -4.4288168 -4.4288888 -4.4289632 -4.4290013][-4.4290028 -4.428977 -4.4289179 -4.428813 -4.4286728 -4.4285436 -4.4284692 -4.4284687 -4.4285374 -4.4286509 -4.4287381 -4.42881 -4.4289017 -4.4289784 -4.4290037][-4.4289947 -4.428966 -4.4288983 -4.428772 -4.428597 -4.4284348 -4.4283366 -4.4283419 -4.4284663 -4.4286184 -4.4287128 -4.428793 -4.4288955 -4.4289713 -4.4289966][-4.4289851 -4.4289517 -4.4288712 -4.4287181 -4.4285021 -4.428298 -4.4281754 -4.4282041 -4.428401 -4.4285951 -4.4287062 -4.4287925 -4.4288874 -4.4289579 -4.4289889][-4.4289842 -4.4289489 -4.4288616 -4.4286914 -4.4284525 -4.4282203 -4.4280849 -4.4281406 -4.4283795 -4.4285989 -4.428721 -4.4288073 -4.4288912 -4.4289541 -4.4289875][-4.4289942 -4.4289656 -4.4288812 -4.4287138 -4.4284773 -4.4282422 -4.428113 -4.428185 -4.4284191 -4.4286289 -4.4287491 -4.4288316 -4.4289055 -4.4289603 -4.4289942][-4.4290061 -4.428987 -4.428915 -4.4287629 -4.4285464 -4.4283266 -4.4282188 -4.4282956 -4.4284983 -4.428678 -4.428793 -4.4288735 -4.428937 -4.4289784 -4.4290066][-4.4290171 -4.4290085 -4.4289503 -4.4288225 -4.4286361 -4.4284458 -4.428369 -4.428443 -4.4285965 -4.428741 -4.428853 -4.4289322 -4.4289846 -4.4290128 -4.4290295][-4.4290261 -4.4290209 -4.4289756 -4.4288735 -4.42872 -4.4285693 -4.4285221 -4.42859 -4.4287014 -4.4288135 -4.4289141 -4.4289856 -4.429028 -4.4290466 -4.4290543][-4.4290261 -4.4290195 -4.4289804 -4.4288993 -4.428782 -4.4286766 -4.42866 -4.4287262 -4.4288149 -4.4288988 -4.4289746 -4.4290271 -4.4290552 -4.4290662 -4.4290662][-4.42902 -4.4290128 -4.428978 -4.4289131 -4.4288263 -4.42876 -4.4287639 -4.4288297 -4.4289088 -4.4289718 -4.4290204 -4.4290519 -4.4290681 -4.4290709 -4.4290624][-4.4290171 -4.4290137 -4.4289875 -4.4289384 -4.4288754 -4.4288321 -4.4288435 -4.4289021 -4.4289689 -4.4290156 -4.4290447 -4.4290624 -4.4290681 -4.4290643 -4.4290533]]...]
INFO - root - 2017-12-08 05:43:07.116556: step 13410, loss = 2.28, batch loss = 2.23 (35.4 examples/sec; 0.226 sec/batch; 20h:02m:38s remains)
INFO - root - 2017-12-08 05:43:09.353701: step 13420, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 19h:29m:34s remains)
INFO - root - 2017-12-08 05:43:11.576078: step 13430, loss = 2.28, batch loss = 2.23 (37.1 examples/sec; 0.216 sec/batch; 19h:06m:29s remains)
INFO - root - 2017-12-08 05:43:13.815325: step 13440, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 19h:24m:01s remains)
INFO - root - 2017-12-08 05:43:16.067184: step 13450, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 19h:25m:30s remains)
INFO - root - 2017-12-08 05:43:18.300039: step 13460, loss = 2.28, batch loss = 2.23 (37.5 examples/sec; 0.213 sec/batch; 18h:52m:55s remains)
INFO - root - 2017-12-08 05:43:20.578552: step 13470, loss = 2.28, batch loss = 2.23 (35.2 examples/sec; 0.227 sec/batch; 20h:07m:43s remains)
INFO - root - 2017-12-08 05:43:22.843148: step 13480, loss = 2.28, batch loss = 2.23 (34.7 examples/sec; 0.231 sec/batch; 20h:27m:02s remains)
INFO - root - 2017-12-08 05:43:25.093525: step 13490, loss = 2.28, batch loss = 2.23 (37.0 examples/sec; 0.216 sec/batch; 19h:09m:53s remains)
INFO - root - 2017-12-08 05:43:27.353867: step 13500, loss = 2.28, batch loss = 2.23 (35.1 examples/sec; 0.228 sec/batch; 20h:10m:32s remains)
2017-12-08 05:43:27.640116: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.428875 -4.4288836 -4.428885 -4.4288888 -4.4288931 -4.4288964 -4.4289 -4.4289045 -4.4289083 -4.42891 -4.4289088 -4.428906 -4.4289007 -4.428894 -4.4288883][-4.4288869 -4.4288974 -4.4289012 -4.428906 -4.4289103 -4.4289131 -4.4289155 -4.4289188 -4.4289222 -4.4289246 -4.4289246 -4.4289217 -4.428915 -4.428906 -4.4288979][-4.42889 -4.4289026 -4.428906 -4.4289079 -4.4289069 -4.4289026 -4.4288993 -4.4288988 -4.4289021 -4.42891 -4.4289179 -4.4289222 -4.4289193 -4.4289136 -4.428906][-4.4288812 -4.4288888 -4.428885 -4.4288759 -4.428863 -4.4288492 -4.4288378 -4.4288335 -4.4288392 -4.4288559 -4.4288774 -4.4288955 -4.4289064 -4.4289103 -4.4289093][-4.4288549 -4.4288535 -4.42884 -4.4288211 -4.428802 -4.4287844 -4.4287729 -4.4287686 -4.4287758 -4.4287953 -4.428823 -4.42885 -4.4288716 -4.4288869 -4.4288955][-4.4288158 -4.4288096 -4.4287949 -4.4287772 -4.4287624 -4.4287505 -4.4287419 -4.4287362 -4.4287376 -4.4287467 -4.4287634 -4.4287853 -4.4288092 -4.4288316 -4.4288487][-4.4287667 -4.428762 -4.42875 -4.4287343 -4.4287233 -4.4287186 -4.4287171 -4.4287133 -4.4287076 -4.4287009 -4.4286981 -4.4287028 -4.4287181 -4.4287415 -4.4287639][-4.4287081 -4.4287062 -4.4286966 -4.4286809 -4.4286695 -4.4286666 -4.4286704 -4.42867 -4.4286585 -4.4286385 -4.4286175 -4.4286036 -4.428607 -4.428627 -4.42865][-4.4286427 -4.4286408 -4.4286318 -4.4286156 -4.4286065 -4.42861 -4.4286189 -4.4286208 -4.428607 -4.4285784 -4.4285431 -4.4285131 -4.4285069 -4.4285235 -4.4285436][-4.4286079 -4.4286051 -4.4285965 -4.4285846 -4.4285846 -4.4285975 -4.4286113 -4.4286113 -4.4285932 -4.42856 -4.4285178 -4.42848 -4.4284711 -4.4284854 -4.4285026][-4.4286327 -4.4286332 -4.4286275 -4.4286175 -4.42862 -4.4286323 -4.4286385 -4.4286289 -4.4286075 -4.4285779 -4.4285417 -4.4285107 -4.4285078 -4.4285245 -4.4285407][-4.4286923 -4.4286952 -4.4286885 -4.4286766 -4.4286747 -4.4286771 -4.42867 -4.4286485 -4.4286208 -4.4285936 -4.4285703 -4.4285555 -4.4285655 -4.4285893 -4.42861][-4.4287472 -4.4287543 -4.4287491 -4.4287391 -4.4287343 -4.4287286 -4.4287105 -4.4286785 -4.4286437 -4.4286165 -4.4286017 -4.4286003 -4.4286194 -4.4286466 -4.428669][-4.4287786 -4.42879 -4.4287906 -4.4287863 -4.428782 -4.428772 -4.428751 -4.4287176 -4.4286833 -4.4286566 -4.4286442 -4.4286461 -4.4286647 -4.4286885 -4.4287081][-4.4287944 -4.4288096 -4.4288144 -4.4288158 -4.4288149 -4.4288068 -4.4287891 -4.428762 -4.4287338 -4.4287105 -4.428699 -4.4287009 -4.4287133 -4.4287276 -4.4287391]]...]
INFO - root - 2017-12-08 05:43:29.885188: step 13510, loss = 2.28, batch loss = 2.23 (35.7 examples/sec; 0.224 sec/batch; 19h:52m:36s remains)
INFO - root - 2017-12-08 05:43:32.111417: step 13520, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 19h:29m:01s remains)
INFO - root - 2017-12-08 05:43:34.354739: step 13530, loss = 2.28, batch loss = 2.23 (34.8 examples/sec; 0.230 sec/batch; 20h:23m:28s remains)
INFO - root - 2017-12-08 05:43:36.570570: step 13540, loss = 2.28, batch loss = 2.23 (35.1 examples/sec; 0.228 sec/batch; 20h:11m:04s remains)
INFO - root - 2017-12-08 05:43:38.767339: step 13550, loss = 2.28, batch loss = 2.23 (37.5 examples/sec; 0.213 sec/batch; 18h:53m:11s remains)
INFO - root - 2017-12-08 05:43:41.025442: step 13560, loss = 2.28, batch loss = 2.23 (34.9 examples/sec; 0.229 sec/batch; 20h:17m:12s remains)
INFO - root - 2017-12-08 05:43:43.253228: step 13570, loss = 2.28, batch loss = 2.23 (37.2 examples/sec; 0.215 sec/batch; 19h:04m:21s remains)
INFO - root - 2017-12-08 05:43:45.492890: step 13580, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.220 sec/batch; 19h:30m:23s remains)
INFO - root - 2017-12-08 05:43:47.749426: step 13590, loss = 2.28, batch loss = 2.23 (35.3 examples/sec; 0.227 sec/batch; 20h:05m:15s remains)
INFO - root - 2017-12-08 05:43:49.975308: step 13600, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 19h:46m:00s remains)
2017-12-08 05:43:50.258629: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288177 -4.4288225 -4.4288006 -4.4287496 -4.42867 -4.4286046 -4.4285555 -4.428544 -4.4285245 -4.4284821 -4.4284797 -4.4285216 -4.4285359 -4.428546 -4.4285631][-4.42882 -4.4288321 -4.4288063 -4.4287481 -4.4286551 -4.4285722 -4.4285145 -4.4284954 -4.428493 -4.4284916 -4.4285169 -4.4285455 -4.428515 -4.42848 -4.4284844][-4.4288106 -4.4288225 -4.4287977 -4.42873 -4.42862 -4.4285173 -4.4284492 -4.4284244 -4.4284482 -4.4284992 -4.4285507 -4.4285669 -4.4285011 -4.4284353 -4.4284325][-4.4288025 -4.4288125 -4.4287872 -4.4287109 -4.4285893 -4.4284654 -4.4283729 -4.4283385 -4.4283977 -4.4284945 -4.4285717 -4.4285827 -4.4285007 -4.4284124 -4.4284024][-4.428803 -4.428813 -4.4287877 -4.4287148 -4.4285827 -4.4284225 -4.42828 -4.4282255 -4.4283352 -4.4284806 -4.4285793 -4.4286036 -4.4285307 -4.4284277 -4.4284043][-4.4288 -4.4288063 -4.4287839 -4.4287128 -4.42857 -4.4283648 -4.42814 -4.4280453 -4.4282293 -4.42845 -4.4285774 -4.428617 -4.4285679 -4.4284806 -4.4284639][-4.4287915 -4.4287944 -4.4287724 -4.428699 -4.4285479 -4.4283137 -4.4280252 -4.4278865 -4.428134 -4.4284058 -4.4285564 -4.428617 -4.4286041 -4.4285574 -4.42855][-4.4287891 -4.4287829 -4.4287558 -4.4286785 -4.4285374 -4.42833 -4.4280791 -4.4279795 -4.4281836 -4.4284105 -4.4285464 -4.4286218 -4.4286523 -4.4286489 -4.4286442][-4.42878 -4.428762 -4.4287271 -4.4286556 -4.4285393 -4.4283938 -4.42824 -4.4282036 -4.4283314 -4.4284611 -4.4285603 -4.4286413 -4.4287033 -4.4287324 -4.4287319][-4.4287591 -4.4287276 -4.428688 -4.4286337 -4.4285522 -4.42846 -4.4283786 -4.4283733 -4.4284439 -4.4285126 -4.4285817 -4.4286571 -4.4287314 -4.42878 -4.4287791][-4.4287286 -4.4286971 -4.4286571 -4.42861 -4.4285636 -4.4285178 -4.428484 -4.4284925 -4.4285288 -4.4285665 -4.428617 -4.4286733 -4.4287329 -4.428782 -4.4287834][-4.4286852 -4.4286656 -4.4286313 -4.4285903 -4.4285688 -4.4285603 -4.4285626 -4.428576 -4.4285841 -4.4286079 -4.4286461 -4.4286747 -4.4287062 -4.4287443 -4.4287438][-4.4286275 -4.4286222 -4.4286 -4.4285679 -4.4285583 -4.4285717 -4.4285874 -4.4286036 -4.4286008 -4.4286222 -4.4286609 -4.4286723 -4.42868 -4.4286952 -4.428688][-4.4285851 -4.4285936 -4.4285817 -4.4285603 -4.4285541 -4.4285655 -4.4285769 -4.4285951 -4.4285932 -4.4286265 -4.4286714 -4.42868 -4.4286714 -4.4286714 -4.4286623][-4.4285793 -4.4286022 -4.4286051 -4.4285917 -4.4285688 -4.4285531 -4.428544 -4.428545 -4.4285526 -4.4285989 -4.428659 -4.4286828 -4.4286819 -4.4286766 -4.4286666]]...]
INFO - root - 2017-12-08 05:43:52.485649: step 13610, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.222 sec/batch; 19h:38m:45s remains)
INFO - root - 2017-12-08 05:43:54.745871: step 13620, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 19h:45m:43s remains)
INFO - root - 2017-12-08 05:43:56.992159: step 13630, loss = 2.28, batch loss = 2.23 (34.0 examples/sec; 0.235 sec/batch; 20h:49m:57s remains)
INFO - root - 2017-12-08 05:43:59.250107: step 13640, loss = 2.28, batch loss = 2.23 (34.2 examples/sec; 0.234 sec/batch; 20h:43m:23s remains)
INFO - root - 2017-12-08 05:44:01.453741: step 13650, loss = 2.28, batch loss = 2.23 (37.0 examples/sec; 0.216 sec/batch; 19h:07m:31s remains)
INFO - root - 2017-12-08 05:44:03.697309: step 13660, loss = 2.28, batch loss = 2.23 (35.8 examples/sec; 0.223 sec/batch; 19h:46m:28s remains)
INFO - root - 2017-12-08 05:44:05.928656: step 13670, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.222 sec/batch; 19h:37m:22s remains)
INFO - root - 2017-12-08 05:44:08.205081: step 13680, loss = 2.28, batch loss = 2.23 (34.4 examples/sec; 0.233 sec/batch; 20h:36m:18s remains)
INFO - root - 2017-12-08 05:44:10.451824: step 13690, loss = 2.28, batch loss = 2.23 (34.9 examples/sec; 0.229 sec/batch; 20h:16m:20s remains)
INFO - root - 2017-12-08 05:44:12.685802: step 13700, loss = 2.28, batch loss = 2.23 (33.4 examples/sec; 0.240 sec/batch; 21h:12m:37s remains)
2017-12-08 05:44:12.955475: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4286308 -4.428637 -4.4286661 -4.4287047 -4.4287353 -4.4287472 -4.4287572 -4.42876 -4.4287519 -4.4287562 -4.4287767 -4.4287848 -4.4287734 -4.4287548 -4.4287477][-4.428627 -4.42864 -4.4286776 -4.4287262 -4.4287496 -4.4287472 -4.4287438 -4.4287391 -4.4287186 -4.4287009 -4.4287014 -4.4287038 -4.4287062 -4.4287033 -4.4287062][-4.4286246 -4.4286585 -4.4287062 -4.4287577 -4.4287767 -4.4287572 -4.4287362 -4.4287224 -4.4286971 -4.428669 -4.4286461 -4.4286366 -4.42864 -4.428648 -4.4286647][-4.4286256 -4.4286785 -4.4287324 -4.4287753 -4.4287872 -4.4287643 -4.4287271 -4.4286962 -4.4286704 -4.4286456 -4.428618 -4.4286003 -4.4286008 -4.428618 -4.428648][-4.4286337 -4.4286871 -4.4287372 -4.4287682 -4.4287686 -4.4287319 -4.4286785 -4.4286327 -4.428616 -4.4286 -4.4285736 -4.4285645 -4.4285774 -4.4286046 -4.4286423][-4.4286618 -4.4287057 -4.4287472 -4.4287615 -4.4287386 -4.4286675 -4.4285727 -4.4285073 -4.4285169 -4.4285355 -4.4285245 -4.4285345 -4.4285617 -4.4286027 -4.4286513][-4.4286933 -4.4287176 -4.4287477 -4.4287443 -4.4286814 -4.4285502 -4.4283786 -4.428287 -4.4283624 -4.428462 -4.4284992 -4.4285269 -4.4285541 -4.4285889 -4.4286313][-4.4286914 -4.4286976 -4.4287262 -4.4287105 -4.428616 -4.428441 -4.4282136 -4.428103 -4.4282565 -4.4284334 -4.428514 -4.4285426 -4.4285445 -4.428555 -4.4285855][-4.428688 -4.4286852 -4.4287028 -4.4286766 -4.4285841 -4.4284544 -4.4283004 -4.4282403 -4.4283752 -4.4285254 -4.4285932 -4.4285946 -4.4285665 -4.428555 -4.4285717][-4.42871 -4.4287047 -4.42872 -4.4286976 -4.4286332 -4.4285707 -4.4285 -4.4284811 -4.4285612 -4.4286537 -4.4286885 -4.4286742 -4.42863 -4.4286003 -4.4286065][-4.4287357 -4.4287434 -4.4287639 -4.4287515 -4.4287124 -4.42868 -4.4286442 -4.4286356 -4.4286776 -4.4287281 -4.4287462 -4.4287281 -4.4286785 -4.4286437 -4.428647][-4.4287634 -4.428771 -4.428781 -4.4287744 -4.4287505 -4.4287267 -4.4287033 -4.4286947 -4.4287105 -4.4287333 -4.4287486 -4.4287448 -4.4287081 -4.4286761 -4.4286704][-4.4287882 -4.4287791 -4.4287639 -4.4287486 -4.4287281 -4.4287033 -4.4286861 -4.42868 -4.4286871 -4.4287095 -4.4287443 -4.4287596 -4.4287391 -4.4287138 -4.4287019][-4.4287939 -4.4287667 -4.428731 -4.4287014 -4.4286752 -4.4286537 -4.4286532 -4.42866 -4.4286761 -4.4287081 -4.4287524 -4.4287829 -4.4287806 -4.4287653 -4.4287496][-4.4287748 -4.4287395 -4.4287 -4.42867 -4.428659 -4.4286556 -4.4286642 -4.4286747 -4.4286971 -4.4287252 -4.4287653 -4.4288015 -4.4288111 -4.4288063 -4.4287953]]...]
INFO - root - 2017-12-08 05:44:15.187016: step 13710, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.226 sec/batch; 19h:58m:33s remains)
INFO - root - 2017-12-08 05:44:17.426705: step 13720, loss = 2.28, batch loss = 2.23 (34.7 examples/sec; 0.231 sec/batch; 20h:25m:16s remains)
INFO - root - 2017-12-08 05:44:19.653573: step 13730, loss = 2.28, batch loss = 2.23 (35.1 examples/sec; 0.228 sec/batch; 20h:11m:42s remains)
INFO - root - 2017-12-08 05:44:21.907617: step 13740, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 19h:24m:14s remains)
INFO - root - 2017-12-08 05:44:24.164198: step 13750, loss = 2.28, batch loss = 2.23 (35.3 examples/sec; 0.227 sec/batch; 20h:05m:35s remains)
INFO - root - 2017-12-08 05:44:26.410931: step 13760, loss = 2.28, batch loss = 2.23 (35.1 examples/sec; 0.228 sec/batch; 20h:11m:24s remains)
INFO - root - 2017-12-08 05:44:28.624032: step 13770, loss = 2.28, batch loss = 2.23 (37.3 examples/sec; 0.214 sec/batch; 18h:58m:39s remains)
INFO - root - 2017-12-08 05:44:30.879348: step 13780, loss = 2.28, batch loss = 2.23 (34.3 examples/sec; 0.233 sec/batch; 20h:39m:10s remains)
INFO - root - 2017-12-08 05:44:33.112286: step 13790, loss = 2.28, batch loss = 2.23 (36.6 examples/sec; 0.218 sec/batch; 19h:20m:35s remains)
INFO - root - 2017-12-08 05:44:35.338941: step 13800, loss = 2.28, batch loss = 2.23 (36.6 examples/sec; 0.219 sec/batch; 19h:22m:16s remains)
2017-12-08 05:44:35.603587: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288354 -4.428844 -4.4288559 -4.4288583 -4.4288483 -4.4288387 -4.4288378 -4.4288445 -4.4288664 -4.4288969 -4.4289284 -4.4289408 -4.4289289 -4.4289079 -4.4288878][-4.428802 -4.42881 -4.4288297 -4.4288387 -4.4288235 -4.4288015 -4.428792 -4.428802 -4.4288349 -4.4288783 -4.4289188 -4.4289346 -4.4289212 -4.4288964 -4.4288726][-4.4287658 -4.4287605 -4.428782 -4.428802 -4.4287877 -4.4287562 -4.4287343 -4.4287324 -4.4287691 -4.4288249 -4.4288712 -4.4288893 -4.4288855 -4.4288726 -4.4288521][-4.4287415 -4.4287238 -4.4287395 -4.4287567 -4.4287438 -4.4287028 -4.4286628 -4.4286342 -4.4286628 -4.428741 -4.4288135 -4.4288397 -4.4288449 -4.4288454 -4.4288383][-4.4287357 -4.4287143 -4.4287124 -4.4287086 -4.4286933 -4.4286528 -4.4285917 -4.4285207 -4.4285316 -4.4286451 -4.4287624 -4.4288068 -4.4288244 -4.4288363 -4.4288363][-4.4287348 -4.4287171 -4.4287071 -4.4286838 -4.4286637 -4.4286103 -4.4285083 -4.4283605 -4.4283638 -4.4285336 -4.4287019 -4.4287739 -4.4288111 -4.4288292 -4.4288321][-4.4287448 -4.4287419 -4.4287271 -4.4286857 -4.4286513 -4.4285755 -4.4284024 -4.4281583 -4.428153 -4.4284143 -4.4286475 -4.4287577 -4.4288106 -4.4288259 -4.4288177][-4.4287829 -4.428791 -4.428761 -4.4286904 -4.42863 -4.4285407 -4.4283242 -4.4280176 -4.4279823 -4.4283175 -4.4285994 -4.4287367 -4.4287992 -4.42881 -4.4287906][-4.428803 -4.4288082 -4.4287672 -4.4286819 -4.4286113 -4.4285369 -4.4283447 -4.4280729 -4.4280267 -4.4283214 -4.428575 -4.4287095 -4.4287734 -4.4287767 -4.428751][-4.4288187 -4.4288249 -4.4287815 -4.4286919 -4.4286356 -4.428596 -4.4284759 -4.4283047 -4.4282889 -4.4284711 -4.4286308 -4.4287252 -4.4287677 -4.42876 -4.4287353][-4.4288254 -4.4288435 -4.4288058 -4.4287176 -4.4286776 -4.4286747 -4.4286318 -4.4285488 -4.428556 -4.4286432 -4.4287167 -4.4287596 -4.4287744 -4.4287653 -4.4287472][-4.4288411 -4.4288712 -4.4288535 -4.4287858 -4.4287577 -4.4287763 -4.428791 -4.4287686 -4.4287782 -4.4288044 -4.4288216 -4.4288206 -4.4288025 -4.4287934 -4.4287891][-4.4288745 -4.4289031 -4.4289041 -4.4288635 -4.4288483 -4.4288654 -4.4289002 -4.4289064 -4.4288988 -4.4288931 -4.4288788 -4.4288535 -4.4288254 -4.4288249 -4.42884][-4.4289069 -4.4289241 -4.4289236 -4.4288993 -4.4288931 -4.4289083 -4.4289422 -4.428956 -4.4289432 -4.4289303 -4.4288993 -4.4288611 -4.4288349 -4.4288564 -4.428894][-4.428925 -4.428925 -4.4289165 -4.428905 -4.4289141 -4.4289365 -4.4289675 -4.4289856 -4.4289842 -4.4289727 -4.4289384 -4.4289031 -4.4288859 -4.4289126 -4.4289494]]...]
INFO - root - 2017-12-08 05:44:37.852426: step 13810, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.226 sec/batch; 19h:57m:58s remains)
INFO - root - 2017-12-08 05:44:40.115532: step 13820, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.221 sec/batch; 19h:32m:05s remains)
INFO - root - 2017-12-08 05:44:42.357113: step 13830, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 19h:24m:12s remains)
INFO - root - 2017-12-08 05:44:44.646172: step 13840, loss = 2.28, batch loss = 2.23 (34.6 examples/sec; 0.231 sec/batch; 20h:27m:52s remains)
INFO - root - 2017-12-08 05:44:46.864097: step 13850, loss = 2.28, batch loss = 2.23 (34.7 examples/sec; 0.230 sec/batch; 20h:22m:47s remains)
INFO - root - 2017-12-08 05:44:49.098258: step 13860, loss = 2.28, batch loss = 2.23 (36.8 examples/sec; 0.218 sec/batch; 19h:15m:51s remains)
INFO - root - 2017-12-08 05:44:51.341366: step 13870, loss = 2.28, batch loss = 2.23 (36.8 examples/sec; 0.218 sec/batch; 19h:15m:29s remains)
INFO - root - 2017-12-08 05:44:53.618387: step 13880, loss = 2.28, batch loss = 2.23 (35.7 examples/sec; 0.224 sec/batch; 19h:48m:37s remains)
INFO - root - 2017-12-08 05:44:55.849818: step 13890, loss = 2.28, batch loss = 2.23 (34.9 examples/sec; 0.229 sec/batch; 20h:18m:28s remains)
INFO - root - 2017-12-08 05:44:58.091338: step 13900, loss = 2.28, batch loss = 2.23 (35.2 examples/sec; 0.227 sec/batch; 20h:07m:31s remains)
2017-12-08 05:44:58.352792: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288759 -4.4288797 -4.4288769 -4.4288797 -4.4288907 -4.4289036 -4.4289217 -4.4289317 -4.4289284 -4.4289165 -4.4289026 -4.4289045 -4.4289155 -4.4289212 -4.428915][-4.4288526 -4.4288568 -4.4288559 -4.4288554 -4.4288621 -4.4288697 -4.428884 -4.428894 -4.4288921 -4.4288816 -4.4288769 -4.4288869 -4.4289 -4.4289055 -4.4288983][-4.4288292 -4.4288282 -4.4288187 -4.4288087 -4.4288111 -4.4288192 -4.428834 -4.428853 -4.4288592 -4.4288583 -4.4288659 -4.4288807 -4.4288931 -4.4288969 -4.4288883][-4.4288173 -4.4288063 -4.428781 -4.4287548 -4.4287486 -4.4287591 -4.4287848 -4.4288273 -4.4288521 -4.4288654 -4.4288845 -4.428896 -4.4289012 -4.4288983 -4.4288878][-4.4288073 -4.4287829 -4.4287367 -4.4286876 -4.4286609 -4.4286652 -4.4286976 -4.4287639 -4.4288139 -4.4288516 -4.4288874 -4.4289041 -4.4289064 -4.4289017 -4.4288921][-4.4288135 -4.4287748 -4.4287004 -4.428617 -4.4285512 -4.4285245 -4.4285417 -4.4286122 -4.428688 -4.4287663 -4.42884 -4.4288821 -4.4288969 -4.4289007 -4.4288936][-4.4288273 -4.42878 -4.4286904 -4.4285784 -4.4284725 -4.4283981 -4.4283748 -4.4284253 -4.4285169 -4.4286394 -4.4287596 -4.4288344 -4.4288688 -4.4288859 -4.4288816][-4.4288387 -4.4288006 -4.4287219 -4.4286146 -4.4284973 -4.4283919 -4.4283223 -4.4283304 -4.4284086 -4.4285426 -4.4286857 -4.4287796 -4.4288249 -4.4288521 -4.4288549][-4.4288387 -4.4288182 -4.4287667 -4.4286928 -4.4286051 -4.428514 -4.4284329 -4.4284058 -4.4284444 -4.4285445 -4.4286618 -4.4287453 -4.4287868 -4.4288135 -4.4288235][-4.4288244 -4.4288239 -4.4288015 -4.4287686 -4.4287291 -4.4286804 -4.4286213 -4.4285865 -4.4285908 -4.4286423 -4.42871 -4.4287624 -4.428792 -4.4288111 -4.4288187][-4.428793 -4.4288049 -4.428803 -4.4287949 -4.4287906 -4.4287777 -4.4287472 -4.4287257 -4.4287286 -4.4287558 -4.4287863 -4.428813 -4.4288278 -4.4288316 -4.4288273][-4.4287634 -4.42877 -4.4287615 -4.4287567 -4.4287677 -4.4287782 -4.4287777 -4.4287777 -4.42879 -4.4288106 -4.4288244 -4.4288368 -4.4288416 -4.4288321 -4.4288173][-4.4287462 -4.42874 -4.4287157 -4.4287019 -4.4287138 -4.4287357 -4.4287548 -4.4287691 -4.4287863 -4.4288015 -4.4288063 -4.4288116 -4.428812 -4.4287972 -4.428781][-4.4287567 -4.4287438 -4.4287095 -4.4286876 -4.4286904 -4.4287095 -4.4287338 -4.42875 -4.4287663 -4.4287786 -4.42878 -4.428782 -4.4287791 -4.4287653 -4.4287524][-4.4288177 -4.428812 -4.4287844 -4.4287629 -4.4287543 -4.42876 -4.4287753 -4.4287863 -4.4287987 -4.4288087 -4.428812 -4.428813 -4.4288096 -4.4288 -4.428791]]...]
INFO - root - 2017-12-08 05:45:00.551848: step 13910, loss = 2.28, batch loss = 2.23 (37.1 examples/sec; 0.216 sec/batch; 19h:04m:34s remains)
INFO - root - 2017-12-08 05:45:02.789039: step 13920, loss = 2.28, batch loss = 2.23 (35.3 examples/sec; 0.227 sec/batch; 20h:03m:14s remains)
INFO - root - 2017-12-08 05:45:05.019718: step 13930, loss = 2.28, batch loss = 2.23 (35.8 examples/sec; 0.223 sec/batch; 19h:44m:52s remains)
INFO - root - 2017-12-08 05:45:07.264940: step 13940, loss = 2.28, batch loss = 2.23 (33.7 examples/sec; 0.237 sec/batch; 20h:59m:18s remains)
INFO - root - 2017-12-08 05:45:09.507377: step 13950, loss = 2.28, batch loss = 2.23 (35.1 examples/sec; 0.228 sec/batch; 20h:08m:36s remains)
INFO - root - 2017-12-08 05:45:11.758313: step 13960, loss = 2.28, batch loss = 2.23 (36.9 examples/sec; 0.217 sec/batch; 19h:11m:21s remains)
INFO - root - 2017-12-08 05:45:13.993683: step 13970, loss = 2.28, batch loss = 2.23 (35.2 examples/sec; 0.228 sec/batch; 20h:08m:06s remains)
INFO - root - 2017-12-08 05:45:16.235678: step 13980, loss = 2.28, batch loss = 2.23 (35.1 examples/sec; 0.228 sec/batch; 20h:10m:48s remains)
INFO - root - 2017-12-08 05:45:18.460398: step 13990, loss = 2.28, batch loss = 2.23 (36.7 examples/sec; 0.218 sec/batch; 19h:17m:21s remains)
INFO - root - 2017-12-08 05:45:20.750261: step 14000, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.225 sec/batch; 19h:55m:21s remains)
2017-12-08 05:45:21.018495: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288187 -4.4288244 -4.428822 -4.42882 -4.4288087 -4.4287982 -4.428813 -4.4288445 -4.4288673 -4.4288635 -4.4288263 -4.428772 -4.4287424 -4.4287381 -4.42875][-4.4288173 -4.4288278 -4.4288263 -4.4288225 -4.4288149 -4.428813 -4.4288335 -4.4288626 -4.4288707 -4.4288349 -4.4287629 -4.4286809 -4.4286394 -4.42865 -4.4286866][-4.4287963 -4.4288096 -4.4288111 -4.4288073 -4.4288068 -4.4288144 -4.428834 -4.4288492 -4.4288273 -4.4287515 -4.4286361 -4.4285283 -4.4284987 -4.4285464 -4.4286227][-4.4287734 -4.4287906 -4.4287996 -4.4288006 -4.4288073 -4.4288177 -4.4288263 -4.4288125 -4.4287534 -4.4286332 -4.4284897 -4.4283938 -4.4284096 -4.4285045 -4.4286079][-4.4287705 -4.4287944 -4.4288073 -4.4288087 -4.4288182 -4.4288259 -4.4288049 -4.4287419 -4.4286361 -4.4284916 -4.428371 -4.4283481 -4.4284368 -4.4285622 -4.4286594][-4.4287758 -4.4288006 -4.4288034 -4.4287977 -4.4288 -4.4287825 -4.4287109 -4.4285836 -4.428431 -4.4282994 -4.4282813 -4.428381 -4.4285221 -4.4286375 -4.4287009][-4.4287858 -4.4288073 -4.4287963 -4.4287682 -4.4287367 -4.42867 -4.4285259 -4.4283247 -4.4281549 -4.4281006 -4.4282217 -4.4284086 -4.4285579 -4.4286532 -4.4286861][-4.4287834 -4.4287953 -4.4287667 -4.4287081 -4.4286284 -4.42851 -4.4283137 -4.4280844 -4.4279861 -4.4280591 -4.4282532 -4.4284391 -4.4285612 -4.4286203 -4.4286227][-4.4287987 -4.4288092 -4.428781 -4.4287114 -4.4286041 -4.4284744 -4.428287 -4.428113 -4.4281187 -4.4282422 -4.4284029 -4.4285321 -4.4286013 -4.4286194 -4.42861][-4.4288421 -4.4288568 -4.428833 -4.4287658 -4.4286532 -4.4285378 -4.4284029 -4.4283257 -4.4283924 -4.4284945 -4.4285889 -4.4286585 -4.42868 -4.4286771 -4.42867][-4.4288907 -4.4289088 -4.4288878 -4.4288278 -4.4287295 -4.4286466 -4.4285803 -4.4285784 -4.4286613 -4.4287214 -4.4287581 -4.4287786 -4.4287672 -4.4287562 -4.4287524][-4.4289322 -4.42895 -4.4289379 -4.428896 -4.4288239 -4.4287677 -4.4287405 -4.4287624 -4.4288263 -4.4288559 -4.4288621 -4.4288578 -4.4288406 -4.4288297 -4.4288268][-4.4289584 -4.4289708 -4.428968 -4.428947 -4.4289 -4.4288597 -4.42885 -4.428874 -4.4289188 -4.4289346 -4.4289284 -4.4289174 -4.4289017 -4.4288874 -4.4288793][-4.4289589 -4.4289627 -4.4289641 -4.4289536 -4.4289227 -4.4288974 -4.4288936 -4.42891 -4.4289403 -4.42895 -4.4289393 -4.4289293 -4.4289169 -4.4289021 -4.4288917][-4.4289403 -4.4289341 -4.42893 -4.4289203 -4.4289031 -4.42889 -4.4288898 -4.4288969 -4.4289136 -4.4289203 -4.428916 -4.4289107 -4.4289021 -4.4288898 -4.42888]]...]
INFO - root - 2017-12-08 05:45:23.250186: step 14010, loss = 2.28, batch loss = 2.23 (35.4 examples/sec; 0.226 sec/batch; 20h:00m:28s remains)
INFO - root - 2017-12-08 05:45:25.495626: step 14020, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 19h:34m:03s remains)
INFO - root - 2017-12-08 05:45:27.724489: step 14030, loss = 2.28, batch loss = 2.23 (36.7 examples/sec; 0.218 sec/batch; 19h:16m:30s remains)
INFO - root - 2017-12-08 05:45:29.978355: step 14040, loss = 2.28, batch loss = 2.23 (37.1 examples/sec; 0.216 sec/batch; 19h:05m:58s remains)
INFO - root - 2017-12-08 05:45:32.228119: step 14050, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 19h:42m:23s remains)
INFO - root - 2017-12-08 05:45:34.489092: step 14060, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 19h:42m:51s remains)
INFO - root - 2017-12-08 05:45:36.718420: step 14070, loss = 2.28, batch loss = 2.23 (35.4 examples/sec; 0.226 sec/batch; 20h:00m:38s remains)
INFO - root - 2017-12-08 05:45:38.992205: step 14080, loss = 2.28, batch loss = 2.23 (36.0 examples/sec; 0.222 sec/batch; 19h:38m:49s remains)
INFO - root - 2017-12-08 05:45:41.238577: step 14090, loss = 2.28, batch loss = 2.23 (37.0 examples/sec; 0.216 sec/batch; 19h:07m:14s remains)
INFO - root - 2017-12-08 05:45:43.533465: step 14100, loss = 2.28, batch loss = 2.23 (35.6 examples/sec; 0.225 sec/batch; 19h:52m:56s remains)
2017-12-08 05:45:43.795014: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4287691 -4.4287572 -4.4287496 -4.4287467 -4.4287472 -4.4287462 -4.4287434 -4.4287276 -4.4287057 -4.42869 -4.4286876 -4.4287133 -4.4287581 -4.4287944 -4.4288249][-4.42869 -4.4286709 -4.428658 -4.4286623 -4.4286733 -4.4286795 -4.4286804 -4.428668 -4.4286485 -4.4286318 -4.4286313 -4.42866 -4.4287057 -4.4287448 -4.428782][-4.428607 -4.4285851 -4.4285688 -4.4285822 -4.4286036 -4.428618 -4.4286294 -4.4286242 -4.4286132 -4.4286051 -4.4286122 -4.4286389 -4.4286752 -4.42871 -4.4287472][-4.4285455 -4.4285178 -4.4284949 -4.4285083 -4.4285254 -4.4285336 -4.428546 -4.4285436 -4.4285455 -4.4285545 -4.4285812 -4.4286175 -4.4286551 -4.4286933 -4.4287329][-4.4285178 -4.4284811 -4.4284482 -4.4284477 -4.4284482 -4.4284387 -4.4284372 -4.4284225 -4.4284306 -4.4284682 -4.4285254 -4.4285855 -4.428638 -4.4286857 -4.4287286][-4.4284987 -4.4284482 -4.4283991 -4.4283776 -4.4283566 -4.4283271 -4.4282966 -4.4282541 -4.4282651 -4.4283376 -4.4284334 -4.4285259 -4.4286008 -4.4286656 -4.4287186][-4.4284444 -4.4283867 -4.4283218 -4.4282713 -4.428226 -4.4281855 -4.4281292 -4.4280562 -4.42807 -4.4281859 -4.4283247 -4.4284439 -4.428544 -4.428638 -4.4287109][-4.42845 -4.4284058 -4.4283395 -4.428267 -4.4282074 -4.428165 -4.4281039 -4.4280338 -4.4280567 -4.4281831 -4.428319 -4.4284229 -4.42852 -4.4286227 -4.4287052][-4.4285254 -4.4285073 -4.4284558 -4.42839 -4.4283419 -4.428309 -4.4282651 -4.4282203 -4.4282432 -4.4283409 -4.428432 -4.428493 -4.4285631 -4.4286489 -4.4287148][-4.4286509 -4.4286547 -4.4286256 -4.4285822 -4.4285522 -4.4285259 -4.4284878 -4.4284568 -4.4284711 -4.4285312 -4.4285827 -4.4286079 -4.4286509 -4.4287105 -4.4287534][-4.42877 -4.4287848 -4.4287748 -4.4287524 -4.4287367 -4.428719 -4.4286866 -4.4286642 -4.4286685 -4.4286971 -4.4287248 -4.4287324 -4.4287581 -4.4287934 -4.4288149][-4.428853 -4.42887 -4.4288697 -4.4288588 -4.428853 -4.4288425 -4.4288197 -4.4288039 -4.4288015 -4.4288139 -4.4288316 -4.4288335 -4.4288459 -4.4288659 -4.428875][-4.4288712 -4.4288831 -4.42888 -4.4288726 -4.4288731 -4.4288735 -4.4288611 -4.42885 -4.4288454 -4.4288545 -4.4288726 -4.4288778 -4.428885 -4.4289007 -4.4289088][-4.42884 -4.4288473 -4.4288459 -4.4288416 -4.428843 -4.4288478 -4.4288392 -4.4288273 -4.4288192 -4.4288282 -4.4288511 -4.428865 -4.4288759 -4.4288964 -4.4289126][-4.4288335 -4.4288392 -4.4288397 -4.4288387 -4.4288416 -4.4288445 -4.4288383 -4.4288282 -4.428822 -4.4288273 -4.4288435 -4.4288588 -4.4288726 -4.4288936 -4.4289136]]...]
INFO - root - 2017-12-08 05:45:46.035770: step 14110, loss = 2.28, batch loss = 2.23 (35.7 examples/sec; 0.224 sec/batch; 19h:49m:54s remains)
INFO - root - 2017-12-08 05:45:48.278854: step 14120, loss = 2.28, batch loss = 2.23 (35.1 examples/sec; 0.228 sec/batch; 20h:09m:23s remains)
INFO - root - 2017-12-08 05:45:50.509092: step 14130, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 19h:32m:39s remains)
INFO - root - 2017-12-08 05:45:52.733391: step 14140, loss = 2.28, batch loss = 2.23 (33.5 examples/sec; 0.239 sec/batch; 21h:07m:02s remains)
INFO - root - 2017-12-08 05:45:54.978425: step 14150, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.225 sec/batch; 19h:56m:10s remains)
INFO - root - 2017-12-08 05:45:57.212379: step 14160, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.221 sec/batch; 19h:34m:12s remains)
INFO - root - 2017-12-08 05:45:59.436098: step 14170, loss = 2.28, batch loss = 2.23 (36.6 examples/sec; 0.218 sec/batch; 19h:18m:30s remains)
INFO - root - 2017-12-08 05:46:01.665620: step 14180, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 19h:41m:02s remains)
INFO - root - 2017-12-08 05:46:03.918644: step 14190, loss = 2.28, batch loss = 2.23 (36.0 examples/sec; 0.222 sec/batch; 19h:38m:42s remains)
INFO - root - 2017-12-08 05:46:06.162735: step 14200, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.225 sec/batch; 19h:55m:03s remains)
2017-12-08 05:46:06.423472: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.428925 -4.4289141 -4.4289079 -4.4289055 -4.428894 -4.428874 -4.4288588 -4.42887 -4.4288974 -4.4289136 -4.4289165 -4.4289117 -4.428906 -4.4289041 -4.4289041][-4.4289207 -4.428916 -4.4289064 -4.4288898 -4.4288607 -4.4288206 -4.42879 -4.4288011 -4.4288511 -4.4288926 -4.4289155 -4.4289222 -4.4289193 -4.4289141 -4.4289083][-4.42892 -4.4289188 -4.4289083 -4.4288759 -4.4288216 -4.4287539 -4.4286928 -4.4286942 -4.4287663 -4.4288383 -4.4288878 -4.4289145 -4.4289279 -4.4289293 -4.4289217][-4.4289293 -4.4289317 -4.4289169 -4.4288697 -4.4287872 -4.4286852 -4.4285817 -4.428555 -4.428648 -4.4287562 -4.4288349 -4.42888 -4.4289145 -4.4289303 -4.4289303][-4.4289389 -4.4289474 -4.4289303 -4.4288774 -4.428771 -4.4286284 -4.4284668 -4.4283934 -4.4284992 -4.4286489 -4.4287615 -4.4288235 -4.4288735 -4.4289088 -4.4289222][-4.4289455 -4.4289603 -4.428946 -4.4288964 -4.4287925 -4.4286337 -4.4284124 -4.4282551 -4.4283361 -4.4285107 -4.4286613 -4.4287462 -4.4288096 -4.4288564 -4.4288845][-4.4289436 -4.4289589 -4.4289465 -4.4289041 -4.4288244 -4.4286971 -4.4284692 -4.4282393 -4.4282408 -4.4283905 -4.4285479 -4.428648 -4.4287229 -4.4287739 -4.4288096][-4.4289351 -4.4289441 -4.4289289 -4.4288907 -4.4288349 -4.4287586 -4.428586 -4.42838 -4.4283338 -4.4283981 -4.4285 -4.4285803 -4.42865 -4.4286962 -4.4287248][-4.4289274 -4.4289303 -4.4289107 -4.428874 -4.4288282 -4.4287863 -4.4286785 -4.4285421 -4.4285088 -4.4285231 -4.4285555 -4.4285908 -4.4286332 -4.428658 -4.4286661][-4.42892 -4.4289222 -4.4289045 -4.4288716 -4.4288278 -4.4287953 -4.4287271 -4.4286542 -4.4286537 -4.428659 -4.4286575 -4.4286628 -4.4286847 -4.4286847 -4.4286618][-4.428916 -4.4289174 -4.4289093 -4.42889 -4.4288507 -4.4288192 -4.4287634 -4.4287186 -4.4287338 -4.4287415 -4.4287291 -4.4287248 -4.4287477 -4.4287438 -4.4287071][-4.4289141 -4.42891 -4.4289079 -4.4289031 -4.4288783 -4.428853 -4.4287996 -4.4287586 -4.4287658 -4.4287682 -4.428751 -4.4287496 -4.4287758 -4.4287786 -4.4287481][-4.4289145 -4.4289007 -4.4288974 -4.4289 -4.4288893 -4.4288754 -4.4288316 -4.4287939 -4.4287891 -4.4287858 -4.4287658 -4.4287586 -4.4287834 -4.428793 -4.4287753][-4.4289265 -4.428905 -4.428894 -4.4288945 -4.4288921 -4.4288907 -4.4288611 -4.4288321 -4.4288244 -4.4288177 -4.4288 -4.428791 -4.4288063 -4.4288149 -4.4288006][-4.428947 -4.4289284 -4.4289136 -4.4289088 -4.4289074 -4.42891 -4.428894 -4.4288793 -4.4288788 -4.428874 -4.4288611 -4.4288559 -4.4288678 -4.4288731 -4.4288578]]...]
INFO - root - 2017-12-08 05:46:08.679059: step 14210, loss = 2.28, batch loss = 2.23 (35.8 examples/sec; 0.224 sec/batch; 19h:47m:02s remains)
INFO - root - 2017-12-08 05:46:10.941577: step 14220, loss = 2.28, batch loss = 2.23 (36.8 examples/sec; 0.217 sec/batch; 19h:12m:28s remains)
INFO - root - 2017-12-08 05:46:13.172686: step 14230, loss = 2.28, batch loss = 2.23 (34.2 examples/sec; 0.234 sec/batch; 20h:41m:55s remains)
INFO - root - 2017-12-08 05:46:15.425345: step 14240, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 19h:41m:18s remains)
INFO - root - 2017-12-08 05:46:17.648244: step 14250, loss = 2.28, batch loss = 2.23 (35.6 examples/sec; 0.225 sec/batch; 19h:51m:25s remains)
INFO - root - 2017-12-08 05:46:19.893427: step 14260, loss = 2.28, batch loss = 2.23 (35.0 examples/sec; 0.229 sec/batch; 20h:12m:27s remains)
INFO - root - 2017-12-08 05:46:22.117308: step 14270, loss = 2.28, batch loss = 2.23 (36.0 examples/sec; 0.222 sec/batch; 19h:38m:11s remains)
INFO - root - 2017-12-08 05:46:24.355578: step 14280, loss = 2.28, batch loss = 2.23 (34.8 examples/sec; 0.230 sec/batch; 20h:17m:47s remains)
INFO - root - 2017-12-08 05:46:26.597065: step 14290, loss = 2.28, batch loss = 2.23 (34.7 examples/sec; 0.230 sec/batch; 20h:22m:22s remains)
INFO - root - 2017-12-08 05:46:28.848399: step 14300, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 19h:30m:37s remains)
2017-12-08 05:46:29.118599: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289117 -4.4289031 -4.4289069 -4.4288979 -4.428884 -4.4288845 -4.4288869 -4.4288816 -4.4288621 -4.4288392 -4.4288149 -4.4287896 -4.4287777 -4.4287796 -4.4287739][-4.4289241 -4.4289117 -4.4289041 -4.4288855 -4.4288683 -4.428864 -4.4288578 -4.4288492 -4.428833 -4.4288187 -4.4288015 -4.4287682 -4.4287424 -4.4287391 -4.4287338][-4.4289289 -4.42891 -4.4288859 -4.4288521 -4.4288259 -4.4288168 -4.4288044 -4.4287949 -4.428782 -4.4287806 -4.4287763 -4.428741 -4.4287148 -4.4287186 -4.4287186][-4.4289255 -4.4288988 -4.42886 -4.4288192 -4.4287839 -4.42876 -4.42873 -4.4287157 -4.4287167 -4.42874 -4.4287577 -4.42874 -4.4287324 -4.4287486 -4.4287534][-4.4289136 -4.4288807 -4.428833 -4.4287844 -4.4287353 -4.4286714 -4.4285913 -4.428565 -4.4286094 -4.4286942 -4.428762 -4.4287825 -4.4287949 -4.4288082 -4.4288044][-4.428906 -4.4288745 -4.4288244 -4.4287672 -4.4286904 -4.4285541 -4.4283834 -4.4283357 -4.428463 -4.4286394 -4.4287663 -4.4288211 -4.4288473 -4.428853 -4.4288368][-4.4289117 -4.4288845 -4.4288378 -4.4287744 -4.4286642 -4.4284458 -4.4281669 -4.4280987 -4.4283223 -4.4285879 -4.4287562 -4.4288354 -4.4288721 -4.428874 -4.4288468][-4.4289231 -4.4289007 -4.4288621 -4.4288049 -4.4286914 -4.4284539 -4.428144 -4.4280624 -4.4283166 -4.428597 -4.4287586 -4.4288383 -4.4288754 -4.4288654 -4.4288297][-4.4289303 -4.4289126 -4.4288826 -4.4288368 -4.4287519 -4.4285793 -4.42836 -4.4282913 -4.4284682 -4.4286671 -4.4287834 -4.428844 -4.4288664 -4.4288406 -4.4287987][-4.4289279 -4.4289055 -4.4288745 -4.4288335 -4.42878 -4.4286823 -4.4285579 -4.4285083 -4.428607 -4.4287238 -4.4288044 -4.4288578 -4.4288712 -4.4288425 -4.428791][-4.4289174 -4.4288831 -4.42884 -4.4287977 -4.428762 -4.42871 -4.4286342 -4.4285951 -4.4286628 -4.4287505 -4.4288168 -4.4288707 -4.4288874 -4.4288621 -4.4288006][-4.4289079 -4.4288645 -4.428812 -4.42877 -4.4287472 -4.4287148 -4.4286566 -4.4286246 -4.428679 -4.4287572 -4.4288197 -4.4288712 -4.4288855 -4.4288559 -4.4287891][-4.4289064 -4.4288664 -4.4288177 -4.4287782 -4.4287548 -4.4287233 -4.4286675 -4.4286404 -4.4286876 -4.428761 -4.4288216 -4.4288635 -4.4288664 -4.4288363 -4.428782][-4.4289103 -4.4288831 -4.4288454 -4.4288058 -4.4287729 -4.4287305 -4.4286728 -4.4286518 -4.4286962 -4.4287653 -4.4288206 -4.4288487 -4.428844 -4.4288187 -4.4287868][-4.4289069 -4.4288869 -4.4288559 -4.4288158 -4.4287772 -4.4287267 -4.42867 -4.4286637 -4.4287176 -4.4287834 -4.4288292 -4.4288468 -4.4288363 -4.4288139 -4.4287958]]...]
INFO - root - 2017-12-08 05:46:31.350366: step 14310, loss = 2.28, batch loss = 2.23 (34.9 examples/sec; 0.229 sec/batch; 20h:15m:16s remains)
INFO - root - 2017-12-08 05:46:33.581915: step 14320, loss = 2.28, batch loss = 2.23 (34.8 examples/sec; 0.230 sec/batch; 20h:19m:59s remains)
INFO - root - 2017-12-08 05:46:35.800249: step 14330, loss = 2.28, batch loss = 2.23 (35.7 examples/sec; 0.224 sec/batch; 19h:48m:59s remains)
INFO - root - 2017-12-08 05:46:38.026603: step 14340, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.222 sec/batch; 19h:35m:57s remains)
INFO - root - 2017-12-08 05:46:40.290689: step 14350, loss = 2.28, batch loss = 2.23 (36.6 examples/sec; 0.219 sec/batch; 19h:18m:50s remains)
INFO - root - 2017-12-08 05:46:42.517018: step 14360, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.221 sec/batch; 19h:33m:34s remains)
INFO - root - 2017-12-08 05:46:44.750155: step 14370, loss = 2.28, batch loss = 2.23 (35.2 examples/sec; 0.227 sec/batch; 20h:03m:32s remains)
INFO - root - 2017-12-08 05:46:46.998174: step 14380, loss = 2.28, batch loss = 2.23 (34.3 examples/sec; 0.233 sec/batch; 20h:37m:55s remains)
INFO - root - 2017-12-08 05:46:49.253503: step 14390, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 19h:23m:07s remains)
INFO - root - 2017-12-08 05:46:51.474287: step 14400, loss = 2.28, batch loss = 2.23 (36.6 examples/sec; 0.218 sec/batch; 19h:18m:01s remains)
2017-12-08 05:46:51.738092: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289241 -4.428874 -4.4288354 -4.4287925 -4.4287524 -4.4287353 -4.42876 -4.4287977 -4.428833 -4.4288564 -4.4288712 -4.4288764 -4.4288664 -4.4288521 -4.4288311][-4.4289312 -4.4288716 -4.4288216 -4.4287663 -4.4287081 -4.4286757 -4.4286928 -4.4287357 -4.4287934 -4.4288392 -4.4288654 -4.4288716 -4.4288588 -4.4288354 -4.4288015][-4.4289136 -4.4288445 -4.4287829 -4.4287176 -4.42865 -4.4286056 -4.4286089 -4.4286551 -4.4287419 -4.4288116 -4.4288416 -4.42884 -4.4288211 -4.4287887 -4.4287434][-4.4288969 -4.4288216 -4.4287577 -4.4286962 -4.4286294 -4.4285717 -4.4285555 -4.4286032 -4.42872 -4.428802 -4.4288197 -4.4288006 -4.4287696 -4.4287286 -4.4286737][-4.4288969 -4.4288177 -4.4287539 -4.4286971 -4.4286289 -4.4285507 -4.4284949 -4.42853 -4.4286752 -4.4287705 -4.428781 -4.4287438 -4.4286971 -4.4286489 -4.4285941][-4.428885 -4.4288006 -4.4287271 -4.4286537 -4.4285655 -4.4284425 -4.4283161 -4.4283376 -4.4285288 -4.428659 -4.428689 -4.4286551 -4.4286032 -4.4285574 -4.42851][-4.4288621 -4.4287677 -4.4286771 -4.4285812 -4.4284687 -4.4282937 -4.4280853 -4.4280877 -4.4283242 -4.4285021 -4.4285741 -4.4285684 -4.428524 -4.428484 -4.4284439][-4.42886 -4.428772 -4.4286928 -4.4286141 -4.4285259 -4.4283791 -4.4281907 -4.4281869 -4.4283824 -4.4285364 -4.428607 -4.4286041 -4.428555 -4.4285088 -4.4284616][-4.4288821 -4.4288125 -4.4287529 -4.4287019 -4.4286489 -4.4285569 -4.4284344 -4.42844 -4.4285703 -4.4286737 -4.4287243 -4.4287133 -4.4286537 -4.4285941 -4.4285345][-4.4288964 -4.4288397 -4.4287915 -4.42875 -4.4287128 -4.4286556 -4.4285755 -4.42859 -4.4286795 -4.4287486 -4.4287839 -4.4287772 -4.4287229 -4.4286594 -4.4286008][-4.4288988 -4.4288421 -4.42879 -4.4287472 -4.4287152 -4.4286752 -4.4286222 -4.4286361 -4.4287028 -4.4287558 -4.4287782 -4.4287734 -4.4287348 -4.4286828 -4.4286323][-4.4288979 -4.4288349 -4.4287734 -4.4287291 -4.4287057 -4.4286838 -4.4286585 -4.4286761 -4.4287295 -4.4287734 -4.4287882 -4.4287825 -4.4287539 -4.4287152 -4.4286771][-4.4288898 -4.4288235 -4.4287658 -4.4287338 -4.4287295 -4.4287357 -4.4287415 -4.4287658 -4.4288044 -4.428833 -4.4288311 -4.4288149 -4.4287863 -4.4287558 -4.4287271][-4.428885 -4.4288197 -4.4287648 -4.42874 -4.4287453 -4.4287715 -4.4287963 -4.4288192 -4.4288421 -4.4288564 -4.4288483 -4.4288297 -4.4288082 -4.4287896 -4.42877][-4.4288969 -4.4288354 -4.4287806 -4.4287539 -4.4287567 -4.4287844 -4.428813 -4.4288292 -4.4288411 -4.4288478 -4.428843 -4.4288335 -4.4288249 -4.4288163 -4.4288054]]...]
INFO - root - 2017-12-08 05:46:53.954180: step 14410, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.221 sec/batch; 19h:33m:46s remains)
INFO - root - 2017-12-08 05:46:56.180790: step 14420, loss = 2.28, batch loss = 2.23 (34.6 examples/sec; 0.231 sec/batch; 20h:25m:29s remains)
INFO - root - 2017-12-08 05:46:58.416934: step 14430, loss = 2.28, batch loss = 2.23 (35.4 examples/sec; 0.226 sec/batch; 19h:59m:27s remains)
INFO - root - 2017-12-08 05:47:00.651485: step 14440, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.220 sec/batch; 19h:27m:24s remains)
INFO - root - 2017-12-08 05:47:02.909469: step 14450, loss = 2.28, batch loss = 2.23 (36.0 examples/sec; 0.222 sec/batch; 19h:36m:36s remains)
INFO - root - 2017-12-08 05:47:05.168263: step 14460, loss = 2.28, batch loss = 2.23 (37.2 examples/sec; 0.215 sec/batch; 19h:01m:24s remains)
INFO - root - 2017-12-08 05:47:07.412565: step 14470, loss = 2.28, batch loss = 2.23 (34.9 examples/sec; 0.229 sec/batch; 20h:14m:33s remains)
INFO - root - 2017-12-08 05:47:09.676805: step 14480, loss = 2.28, batch loss = 2.23 (33.9 examples/sec; 0.236 sec/batch; 20h:48m:59s remains)
INFO - root - 2017-12-08 05:47:11.929723: step 14490, loss = 2.28, batch loss = 2.23 (34.8 examples/sec; 0.230 sec/batch; 20h:17m:28s remains)
INFO - root - 2017-12-08 05:47:14.153183: step 14500, loss = 2.28, batch loss = 2.23 (37.0 examples/sec; 0.216 sec/batch; 19h:04m:37s remains)
2017-12-08 05:47:14.452540: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.428834 -4.428833 -4.428812 -4.4287591 -4.4286933 -4.4286246 -4.4285874 -4.4286027 -4.4286418 -4.4286995 -4.4287405 -4.4287748 -4.4288 -4.4287896 -4.4287238][-4.4288821 -4.4288669 -4.428843 -4.4288011 -4.4287343 -4.4286509 -4.4285774 -4.428555 -4.4285731 -4.4286246 -4.4286723 -4.4287286 -4.4287829 -4.4287858 -4.4287224][-4.4289021 -4.4288778 -4.428853 -4.4288216 -4.4287577 -4.4286594 -4.4285536 -4.4285054 -4.428524 -4.42858 -4.4286304 -4.4287 -4.4287729 -4.4287872 -4.4287338][-4.4288936 -4.4288688 -4.4288454 -4.4288254 -4.4287705 -4.4286647 -4.4285264 -4.4284587 -4.4284968 -4.42856 -4.4286013 -4.42867 -4.4287472 -4.4287777 -4.4287529][-4.4288669 -4.4288521 -4.428834 -4.4288163 -4.4287658 -4.42865 -4.42848 -4.4284029 -4.428483 -4.4285669 -4.4285913 -4.4286351 -4.4287024 -4.4287438 -4.4287486][-4.428823 -4.4288177 -4.4288044 -4.4287868 -4.4287305 -4.4286032 -4.4284105 -4.4283319 -4.4284697 -4.4285932 -4.42861 -4.4286094 -4.4286456 -4.428679 -4.428699][-4.4287724 -4.4287748 -4.4287682 -4.4287481 -4.4286928 -4.4285636 -4.4283576 -4.42827 -4.4284449 -4.42861 -4.4286284 -4.4285889 -4.428587 -4.4286032 -4.4286246][-4.4287496 -4.4287596 -4.4287605 -4.4287415 -4.4286923 -4.4285645 -4.4283495 -4.4282379 -4.4284086 -4.4285946 -4.4286275 -4.42857 -4.42854 -4.4285355 -4.4285507][-4.4287634 -4.4287758 -4.4287853 -4.4287682 -4.4287205 -4.4286017 -4.4283957 -4.4282608 -4.4283876 -4.4285655 -4.4286089 -4.4285464 -4.42851 -4.4285016 -4.4285178][-4.4287982 -4.42881 -4.4288197 -4.4288068 -4.42876 -4.428658 -4.4284816 -4.4283433 -4.428411 -4.42855 -4.4285908 -4.4285355 -4.4285111 -4.428515 -4.4285378][-4.4288387 -4.4288406 -4.4288392 -4.4288249 -4.428782 -4.4286985 -4.4285564 -4.4284286 -4.4284525 -4.4285526 -4.4285927 -4.428555 -4.4285493 -4.4285583 -4.4285808][-4.4288707 -4.42886 -4.42884 -4.4288158 -4.4287763 -4.4287105 -4.4286013 -4.4284968 -4.4285011 -4.4285665 -4.4285965 -4.4285765 -4.4285865 -4.4285979 -4.4286137][-4.4289031 -4.4288788 -4.4288397 -4.4287987 -4.4287558 -4.4287095 -4.4286385 -4.4285655 -4.4285564 -4.428587 -4.4286017 -4.4285951 -4.4286189 -4.428637 -4.4286456][-4.42893 -4.428896 -4.4288445 -4.4287915 -4.4287481 -4.4287181 -4.4286828 -4.4286404 -4.4286203 -4.428618 -4.428617 -4.4286122 -4.4286418 -4.4286704 -4.4286795][-4.42895 -4.4289193 -4.4288678 -4.4288096 -4.4287648 -4.4287415 -4.4287224 -4.428699 -4.4286714 -4.4286413 -4.4286218 -4.4286089 -4.4286366 -4.428678 -4.4286981]]...]
INFO - root - 2017-12-08 05:47:16.657754: step 14510, loss = 2.28, batch loss = 2.23 (33.1 examples/sec; 0.241 sec/batch; 21h:19m:23s remains)
INFO - root - 2017-12-08 05:47:18.880262: step 14520, loss = 2.28, batch loss = 2.23 (38.1 examples/sec; 0.210 sec/batch; 18h:31m:53s remains)
INFO - root - 2017-12-08 05:47:21.126936: step 14530, loss = 2.28, batch loss = 2.23 (36.8 examples/sec; 0.217 sec/batch; 19h:10m:42s remains)
INFO - root - 2017-12-08 05:47:23.426056: step 14540, loss = 2.28, batch loss = 2.23 (34.1 examples/sec; 0.235 sec/batch; 20h:44m:33s remains)
INFO - root - 2017-12-08 05:47:25.701080: step 14550, loss = 2.28, batch loss = 2.23 (36.6 examples/sec; 0.219 sec/batch; 19h:19m:20s remains)
INFO - root - 2017-12-08 05:47:27.934524: step 14560, loss = 2.28, batch loss = 2.23 (36.0 examples/sec; 0.222 sec/batch; 19h:36m:07s remains)
INFO - root - 2017-12-08 05:47:30.193854: step 14570, loss = 2.28, batch loss = 2.23 (35.2 examples/sec; 0.227 sec/batch; 20h:04m:23s remains)
INFO - root - 2017-12-08 05:47:32.421267: step 14580, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.225 sec/batch; 19h:53m:15s remains)
INFO - root - 2017-12-08 05:47:34.701678: step 14590, loss = 2.28, batch loss = 2.23 (35.7 examples/sec; 0.224 sec/batch; 19h:48m:13s remains)
INFO - root - 2017-12-08 05:47:36.906078: step 14600, loss = 2.28, batch loss = 2.23 (36.8 examples/sec; 0.217 sec/batch; 19h:11m:26s remains)
2017-12-08 05:47:37.191019: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4287128 -4.4287767 -4.4288349 -4.4288659 -4.4288421 -4.42875 -4.4286475 -4.4285626 -4.4285355 -4.4285865 -4.4286571 -4.4287105 -4.428741 -4.4287286 -4.428699][-4.4287581 -4.4288306 -4.4288936 -4.4289265 -4.4289007 -4.4288158 -4.4287195 -4.428628 -4.4285617 -4.4285631 -4.4286027 -4.4286408 -4.428668 -4.42867 -4.4286628][-4.428803 -4.4288669 -4.4289212 -4.4289503 -4.4289274 -4.4288478 -4.4287558 -4.4286623 -4.4285808 -4.4285493 -4.4285579 -4.4285808 -4.4286003 -4.4286146 -4.428637][-4.4288311 -4.4288812 -4.428925 -4.4289503 -4.4289193 -4.4288287 -4.4287238 -4.4286294 -4.4285688 -4.4285583 -4.4285669 -4.428576 -4.4285769 -4.4285879 -4.4286308][-4.42885 -4.4288921 -4.4289231 -4.4289303 -4.4288745 -4.4287548 -4.428628 -4.4285321 -4.4285097 -4.4285564 -4.4285955 -4.4286141 -4.4286165 -4.4286251 -4.4286618][-4.4288754 -4.4289088 -4.4289293 -4.4289117 -4.4288116 -4.4286451 -4.4284849 -4.4283748 -4.4283834 -4.4284978 -4.4285893 -4.4286418 -4.4286771 -4.4287019 -4.4287252][-4.4288988 -4.4289193 -4.428925 -4.4288797 -4.4287381 -4.4285235 -4.42832 -4.4281778 -4.4281993 -4.4283814 -4.4285288 -4.428618 -4.4286952 -4.4287491 -4.42878][-4.4289064 -4.4289145 -4.4289088 -4.4288406 -4.4286819 -4.4284472 -4.42822 -4.4280481 -4.4280491 -4.4282584 -4.4284539 -4.4285812 -4.4286928 -4.4287691 -4.4288163][-4.4288425 -4.4288526 -4.4288626 -4.42882 -4.4286985 -4.4285011 -4.428308 -4.4281645 -4.4281569 -4.4283223 -4.4285016 -4.42863 -4.4287386 -4.4288168 -4.4288688][-4.4287243 -4.4287643 -4.4288216 -4.4288387 -4.4287863 -4.4286561 -4.4285197 -4.4284143 -4.428401 -4.4285007 -4.4286275 -4.4287257 -4.4288063 -4.428865 -4.4289074][-4.4286113 -4.4286866 -4.428792 -4.4288626 -4.4288712 -4.4288106 -4.4287291 -4.4286575 -4.428638 -4.4286861 -4.4287663 -4.4288311 -4.4288774 -4.4289107 -4.428936][-4.4285512 -4.4286513 -4.4287939 -4.4288917 -4.4289317 -4.4289188 -4.4288845 -4.4288416 -4.4288206 -4.4288349 -4.4288769 -4.4289145 -4.428937 -4.4289527 -4.4289656][-4.4285612 -4.4286675 -4.428813 -4.4289074 -4.4289589 -4.4289742 -4.4289684 -4.4289451 -4.4289274 -4.4289255 -4.4289384 -4.4289532 -4.4289622 -4.428968 -4.4289742][-4.4286404 -4.4287305 -4.4288473 -4.428915 -4.4289613 -4.428987 -4.4289923 -4.4289827 -4.4289713 -4.4289646 -4.428967 -4.4289694 -4.4289727 -4.428978 -4.4289823][-4.4287515 -4.4288187 -4.4288926 -4.4289279 -4.4289541 -4.4289727 -4.4289784 -4.428977 -4.4289713 -4.428967 -4.428967 -4.428966 -4.428968 -4.4289732 -4.4289761]]...]
INFO - root - 2017-12-08 05:47:39.443823: step 14610, loss = 2.28, batch loss = 2.23 (34.9 examples/sec; 0.229 sec/batch; 20h:13m:59s remains)
INFO - root - 2017-12-08 05:47:41.718763: step 14620, loss = 2.28, batch loss = 2.23 (35.8 examples/sec; 0.223 sec/batch; 19h:43m:51s remains)
INFO - root - 2017-12-08 05:47:43.963277: step 14630, loss = 2.28, batch loss = 2.23 (36.0 examples/sec; 0.222 sec/batch; 19h:38m:03s remains)
INFO - root - 2017-12-08 05:47:46.215578: step 14640, loss = 2.28, batch loss = 2.23 (34.4 examples/sec; 0.232 sec/batch; 20h:30m:15s remains)
INFO - root - 2017-12-08 05:47:48.457890: step 14650, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.222 sec/batch; 19h:34m:20s remains)
INFO - root - 2017-12-08 05:47:50.692435: step 14660, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 19h:25m:12s remains)
INFO - root - 2017-12-08 05:47:52.969445: step 14670, loss = 2.28, batch loss = 2.23 (35.3 examples/sec; 0.227 sec/batch; 20h:00m:19s remains)
INFO - root - 2017-12-08 05:47:55.273686: step 14680, loss = 2.28, batch loss = 2.23 (33.6 examples/sec; 0.238 sec/batch; 21h:00m:06s remains)
INFO - root - 2017-12-08 05:47:57.503200: step 14690, loss = 2.28, batch loss = 2.23 (37.8 examples/sec; 0.211 sec/batch; 18h:40m:11s remains)
INFO - root - 2017-12-08 05:47:59.748015: step 14700, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 19h:23m:29s remains)
2017-12-08 05:48:00.056519: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288993 -4.4288993 -4.4289126 -4.4289265 -4.4289312 -4.4289317 -4.4289379 -4.428936 -4.428925 -4.4289155 -4.4289145 -4.4289179 -4.4289236 -4.42893 -4.4289336][-4.4288921 -4.4288883 -4.4288979 -4.4289117 -4.4289207 -4.4289255 -4.4289312 -4.4289303 -4.4289203 -4.4289083 -4.4289041 -4.4289055 -4.4289093 -4.4289112 -4.4289122][-4.4288173 -4.4288034 -4.4288082 -4.4288249 -4.4288387 -4.4288449 -4.4288492 -4.4288445 -4.4288321 -4.4288154 -4.4288082 -4.4288044 -4.4288015 -4.4287992 -4.4288006][-4.4286995 -4.4286652 -4.4286585 -4.4286747 -4.4286952 -4.4287062 -4.4287095 -4.4287033 -4.4286895 -4.4286675 -4.4286509 -4.428638 -4.4286261 -4.428618 -4.4286156][-4.4285603 -4.4284949 -4.4284616 -4.4284635 -4.4284863 -4.428504 -4.4285121 -4.4285083 -4.428494 -4.4284725 -4.4284487 -4.428421 -4.4283934 -4.4283748 -4.4283648][-4.4284606 -4.4283686 -4.4283071 -4.4282894 -4.4283032 -4.4283118 -4.4283137 -4.428308 -4.428298 -4.4282904 -4.4282751 -4.4282479 -4.4282203 -4.4282022 -4.42819][-4.4285135 -4.4284258 -4.4283557 -4.4283228 -4.428319 -4.4283042 -4.4282846 -4.4282722 -4.4282813 -4.4283085 -4.4283266 -4.4283237 -4.4283123 -4.4283056 -4.428297][-4.4286518 -4.4285874 -4.4285283 -4.4284925 -4.4284744 -4.42844 -4.4283991 -4.4283772 -4.428401 -4.4284573 -4.4285083 -4.4285345 -4.4285445 -4.42855 -4.4285431][-4.4287348 -4.4286852 -4.428637 -4.4286065 -4.4285903 -4.4285579 -4.4285178 -4.428494 -4.4285197 -4.4285779 -4.4286346 -4.4286718 -4.4286914 -4.428699 -4.4286904][-4.4287543 -4.4287066 -4.4286637 -4.4286423 -4.4286389 -4.4286232 -4.4286003 -4.4285822 -4.4285965 -4.4286356 -4.4286776 -4.4287076 -4.4287238 -4.4287305 -4.4287243][-4.4287529 -4.4287043 -4.4286613 -4.4286418 -4.4286461 -4.428648 -4.4286442 -4.4286327 -4.4286294 -4.4286413 -4.4286628 -4.4286814 -4.4286942 -4.4287009 -4.4286976][-4.4287672 -4.42872 -4.4286752 -4.4286509 -4.4286561 -4.428669 -4.42868 -4.4286737 -4.4286528 -4.4286394 -4.42864 -4.428647 -4.4286575 -4.4286642 -4.4286656][-4.4287987 -4.4287519 -4.4287043 -4.4286766 -4.4286761 -4.4286866 -4.4286995 -4.4286919 -4.4286618 -4.4286332 -4.4286189 -4.4286151 -4.4286218 -4.4286289 -4.4286337][-4.4288216 -4.4287753 -4.42873 -4.4287043 -4.428699 -4.4287028 -4.4287143 -4.4287081 -4.4286809 -4.4286532 -4.4286342 -4.4286242 -4.4286251 -4.4286294 -4.4286318][-4.4288349 -4.4287868 -4.4287424 -4.42872 -4.4287133 -4.4287133 -4.4287257 -4.4287262 -4.4287105 -4.42869 -4.4286718 -4.4286618 -4.4286623 -4.4286642 -4.4286671]]...]
INFO - root - 2017-12-08 05:48:02.290347: step 14710, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 19h:29m:25s remains)
INFO - root - 2017-12-08 05:48:04.553052: step 14720, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 19h:20m:40s remains)
INFO - root - 2017-12-08 05:48:06.795737: step 14730, loss = 2.28, batch loss = 2.23 (35.6 examples/sec; 0.225 sec/batch; 19h:51m:43s remains)
INFO - root - 2017-12-08 05:48:09.045951: step 14740, loss = 2.28, batch loss = 2.23 (34.9 examples/sec; 0.229 sec/batch; 20h:13m:58s remains)
INFO - root - 2017-12-08 05:48:11.273912: step 14750, loss = 2.28, batch loss = 2.23 (34.8 examples/sec; 0.230 sec/batch; 20h:16m:36s remains)
INFO - root - 2017-12-08 05:48:13.500163: step 14760, loss = 2.28, batch loss = 2.23 (36.7 examples/sec; 0.218 sec/batch; 19h:15m:10s remains)
INFO - root - 2017-12-08 05:48:15.728932: step 14770, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.221 sec/batch; 19h:32m:12s remains)
INFO - root - 2017-12-08 05:48:17.963862: step 14780, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 19h:19m:44s remains)
INFO - root - 2017-12-08 05:48:20.216651: step 14790, loss = 2.28, batch loss = 2.23 (34.7 examples/sec; 0.231 sec/batch; 20h:20m:56s remains)
INFO - root - 2017-12-08 05:48:22.439437: step 14800, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 19h:30m:05s remains)
2017-12-08 05:48:22.728291: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288025 -4.4287925 -4.4287949 -4.4287915 -4.4287872 -4.4287834 -4.4287715 -4.4287539 -4.4287677 -4.4288111 -4.4288373 -4.428844 -4.4288511 -4.4288678 -4.4288859][-4.4287424 -4.4287353 -4.4287324 -4.4287148 -4.4286952 -4.4286857 -4.4286723 -4.4286661 -4.4287038 -4.4287739 -4.428823 -4.4288406 -4.4288459 -4.4288621 -4.4288836][-4.4286814 -4.4286847 -4.4286776 -4.4286437 -4.4286056 -4.4285755 -4.4285545 -4.4285636 -4.4286289 -4.4287248 -4.4287963 -4.4288325 -4.4288464 -4.4288573 -4.4288669][-4.42863 -4.4286532 -4.4286427 -4.4285893 -4.4285307 -4.42848 -4.4284534 -4.4284811 -4.4285688 -4.4286795 -4.4287591 -4.4288096 -4.428843 -4.4288607 -4.4288573][-4.4286208 -4.4286547 -4.4286351 -4.4285522 -4.4284606 -4.4283891 -4.4283667 -4.4284215 -4.4285235 -4.4286356 -4.4287152 -4.4287696 -4.4288149 -4.4288392 -4.4288297][-4.4286623 -4.4286785 -4.4286337 -4.4285164 -4.4283848 -4.4282808 -4.4282537 -4.4283233 -4.4284348 -4.4285526 -4.428638 -4.4286923 -4.4287434 -4.4287744 -4.4287705][-4.4287052 -4.4286833 -4.4286175 -4.4284892 -4.4283338 -4.4281945 -4.4281473 -4.4282093 -4.4283133 -4.4284272 -4.4285169 -4.428576 -4.4286389 -4.4286838 -4.4286933][-4.4286981 -4.428638 -4.4285645 -4.4284582 -4.428329 -4.4282069 -4.4281664 -4.428215 -4.428287 -4.428371 -4.4284372 -4.428494 -4.4285712 -4.4286318 -4.4286556][-4.4286642 -4.4285827 -4.4285107 -4.4284396 -4.4283686 -4.4283094 -4.4283094 -4.4283466 -4.4283733 -4.4284115 -4.42845 -4.4285069 -4.4285965 -4.428669 -4.4287066][-4.4286404 -4.4285569 -4.4284987 -4.4284739 -4.4284711 -4.42848 -4.4285131 -4.4285355 -4.4285269 -4.4285331 -4.4285569 -4.4286032 -4.428679 -4.4287443 -4.4287887][-4.428648 -4.4285789 -4.4285455 -4.4285631 -4.428607 -4.4286518 -4.4286942 -4.4287062 -4.4286909 -4.4286847 -4.4286985 -4.4287305 -4.4287734 -4.4288068 -4.4288363][-4.4286928 -4.4286504 -4.4286447 -4.4286857 -4.4287429 -4.4287925 -4.4288335 -4.42884 -4.4288268 -4.4288206 -4.4288263 -4.4288378 -4.4288473 -4.4288459 -4.428854][-4.428751 -4.4287357 -4.4287519 -4.428803 -4.4288578 -4.4288964 -4.428925 -4.4289274 -4.4289169 -4.4289145 -4.42892 -4.4289212 -4.4289136 -4.4288893 -4.428874][-4.4288063 -4.4288096 -4.4288344 -4.4288821 -4.4289255 -4.4289546 -4.4289713 -4.4289694 -4.4289575 -4.4289522 -4.4289532 -4.4289484 -4.4289355 -4.428905 -4.4288812][-4.4288635 -4.4288764 -4.4288974 -4.4289289 -4.4289575 -4.4289737 -4.4289784 -4.4289708 -4.428957 -4.4289432 -4.4289379 -4.42893 -4.4289165 -4.4288888 -4.4288611]]...]
INFO - root - 2017-12-08 05:48:25.006156: step 14810, loss = 2.28, batch loss = 2.23 (35.4 examples/sec; 0.226 sec/batch; 19h:57m:24s remains)
INFO - root - 2017-12-08 05:48:27.263873: step 14820, loss = 2.28, batch loss = 2.23 (34.8 examples/sec; 0.230 sec/batch; 20h:17m:16s remains)
INFO - root - 2017-12-08 05:48:29.554507: step 14830, loss = 2.28, batch loss = 2.23 (35.2 examples/sec; 0.227 sec/batch; 20h:03m:00s remains)
INFO - root - 2017-12-08 05:48:31.787789: step 14840, loss = 2.28, batch loss = 2.23 (35.0 examples/sec; 0.229 sec/batch; 20h:09m:48s remains)
INFO - root - 2017-12-08 05:48:34.052076: step 14850, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 19h:22m:18s remains)
INFO - root - 2017-12-08 05:48:36.300733: step 14860, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.222 sec/batch; 19h:32m:54s remains)
INFO - root - 2017-12-08 05:48:38.580744: step 14870, loss = 2.28, batch loss = 2.23 (37.0 examples/sec; 0.216 sec/batch; 19h:03m:44s remains)
INFO - root - 2017-12-08 05:48:40.816892: step 14880, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 19h:25m:01s remains)
INFO - root - 2017-12-08 05:48:43.107063: step 14890, loss = 2.28, batch loss = 2.23 (34.2 examples/sec; 0.234 sec/batch; 20h:38m:09s remains)
INFO - root - 2017-12-08 05:48:45.377602: step 14900, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 19h:22m:40s remains)
2017-12-08 05:48:45.675394: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288325 -4.4287763 -4.4287152 -4.4286771 -4.4286528 -4.4286757 -4.4287095 -4.4287043 -4.4287043 -4.4287276 -4.4287405 -4.4287596 -4.4287982 -4.4288359 -4.4288549][-4.4288197 -4.4287715 -4.42872 -4.4287143 -4.4287271 -4.428762 -4.4287624 -4.4287024 -4.4286656 -4.428699 -4.4287324 -4.4287534 -4.4287882 -4.4288225 -4.4288392][-4.4287915 -4.4287486 -4.4287114 -4.4287324 -4.4287705 -4.4288125 -4.4288058 -4.4287286 -4.4286709 -4.4286981 -4.4287386 -4.4287605 -4.4287896 -4.4288144 -4.4288197][-4.4287391 -4.42869 -4.4286652 -4.4287095 -4.4287677 -4.4288135 -4.4288163 -4.4287639 -4.4287214 -4.4287372 -4.4287643 -4.4287796 -4.4288006 -4.4288106 -4.4287906][-4.428688 -4.4286346 -4.4286146 -4.4286594 -4.4287128 -4.4287553 -4.4287763 -4.4287667 -4.428762 -4.428782 -4.4287992 -4.4288106 -4.4288235 -4.4288135 -4.428762][-4.4286656 -4.428616 -4.4285865 -4.4285903 -4.4285955 -4.4286108 -4.4286437 -4.4286947 -4.42875 -4.4287972 -4.4288268 -4.4288392 -4.4288478 -4.4288239 -4.42875][-4.428721 -4.4286757 -4.4286137 -4.4285426 -4.42845 -4.4283953 -4.42844 -4.4285669 -4.4287 -4.428792 -4.4288397 -4.428865 -4.4288821 -4.4288588 -4.4287791][-4.4288197 -4.4287739 -4.42869 -4.4285631 -4.4283786 -4.4282293 -4.4282632 -4.428453 -4.428648 -4.42877 -4.42883 -4.4288716 -4.4289055 -4.4288883 -4.4288197][-4.4288869 -4.4288487 -4.4287724 -4.4286532 -4.428463 -4.4282656 -4.428236 -4.428412 -4.4286156 -4.42874 -4.4288054 -4.4288597 -4.4289002 -4.4289017 -4.4288583][-4.428915 -4.4288907 -4.428844 -4.428762 -4.4286165 -4.42843 -4.4283285 -4.428421 -4.428597 -4.4287162 -4.428782 -4.4288392 -4.4288774 -4.4288869 -4.4288731][-4.4289112 -4.4288955 -4.4288712 -4.4288225 -4.42873 -4.4285879 -4.428462 -4.4284749 -4.4286056 -4.4287119 -4.4287734 -4.42882 -4.4288445 -4.4288521 -4.428853][-4.4288774 -4.4288654 -4.4288597 -4.4288373 -4.42879 -4.4286995 -4.4285922 -4.4285655 -4.4286389 -4.428719 -4.4287705 -4.428803 -4.4288125 -4.4288144 -4.42881][-4.4288325 -4.4288163 -4.4288154 -4.4288068 -4.4287891 -4.4287496 -4.4286866 -4.428658 -4.4286904 -4.4287376 -4.4287758 -4.4287949 -4.428792 -4.4287839 -4.4287696][-4.4288116 -4.4287758 -4.4287586 -4.4287448 -4.4287343 -4.4287333 -4.4287219 -4.4287143 -4.4287305 -4.4287596 -4.4287887 -4.428792 -4.4287748 -4.4287605 -4.4287457][-4.4288173 -4.4287629 -4.4287138 -4.4286766 -4.428659 -4.4286695 -4.4286895 -4.4287081 -4.428731 -4.4287558 -4.4287834 -4.4287748 -4.4287443 -4.4287224 -4.4287162]]...]
INFO - root - 2017-12-08 05:48:47.901454: step 14910, loss = 2.28, batch loss = 2.23 (35.6 examples/sec; 0.225 sec/batch; 19h:49m:10s remains)
INFO - root - 2017-12-08 05:48:50.144470: step 14920, loss = 2.28, batch loss = 2.23 (35.0 examples/sec; 0.229 sec/batch; 20h:10m:54s remains)
INFO - root - 2017-12-08 05:48:52.404744: step 14930, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 19h:19m:29s remains)
INFO - root - 2017-12-08 05:48:54.665137: step 14940, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 19h:38m:04s remains)
INFO - root - 2017-12-08 05:48:56.927161: step 14950, loss = 2.28, batch loss = 2.23 (34.7 examples/sec; 0.230 sec/batch; 20h:19m:02s remains)
INFO - root - 2017-12-08 05:48:59.166910: step 14960, loss = 2.28, batch loss = 2.23 (35.4 examples/sec; 0.226 sec/batch; 19h:56m:38s remains)
INFO - root - 2017-12-08 05:49:01.413998: step 14970, loss = 2.28, batch loss = 2.23 (35.6 examples/sec; 0.225 sec/batch; 19h:49m:24s remains)
INFO - root - 2017-12-08 05:49:03.644847: step 14980, loss = 2.28, batch loss = 2.23 (35.3 examples/sec; 0.227 sec/batch; 19h:59m:02s remains)
INFO - root - 2017-12-08 05:49:05.908064: step 14990, loss = 2.28, batch loss = 2.23 (35.8 examples/sec; 0.223 sec/batch; 19h:41m:58s remains)
INFO - root - 2017-12-08 05:49:08.195754: step 15000, loss = 2.28, batch loss = 2.23 (34.2 examples/sec; 0.234 sec/batch; 20h:36m:46s remains)
2017-12-08 05:49:08.509134: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289775 -4.4289818 -4.4289737 -4.4289584 -4.4289346 -4.4288969 -4.42886 -4.428833 -4.4288206 -4.4288158 -4.4288268 -4.4288473 -4.4288788 -4.4289179 -4.4289517][-4.4289823 -4.4289885 -4.4289823 -4.4289646 -4.42893 -4.4288731 -4.4288216 -4.42878 -4.428762 -4.4287591 -4.4287777 -4.428802 -4.428833 -4.4288754 -4.4289231][-4.4289842 -4.4289904 -4.428988 -4.4289694 -4.4289212 -4.4288449 -4.4287834 -4.4287386 -4.4287214 -4.4287281 -4.428762 -4.4287863 -4.4288068 -4.4288411 -4.4288917][-4.4289875 -4.4289932 -4.4289951 -4.42897 -4.4289021 -4.4288044 -4.4287357 -4.4287009 -4.4287028 -4.4287224 -4.4287648 -4.428791 -4.4288034 -4.4288268 -4.4288716][-4.4289789 -4.4289837 -4.4289818 -4.4289408 -4.42885 -4.4287281 -4.4286427 -4.4286208 -4.4286623 -4.4287019 -4.4287477 -4.4287753 -4.42879 -4.428812 -4.4288497][-4.428947 -4.4289365 -4.4289079 -4.4288421 -4.4287319 -4.4285831 -4.4284616 -4.428443 -4.4285431 -4.4286237 -4.4286866 -4.4287186 -4.4287467 -4.4287825 -4.4288177][-4.4288731 -4.4288235 -4.4287519 -4.4286556 -4.4285226 -4.4283438 -4.4281735 -4.4281416 -4.4283123 -4.4284649 -4.4285641 -4.4286146 -4.4286695 -4.4287262 -4.428772][-4.42875 -4.42865 -4.4285359 -4.4284096 -4.4282603 -4.4280715 -4.4278779 -4.4278426 -4.4280763 -4.4282923 -4.4284229 -4.428503 -4.4285874 -4.428658 -4.4287186][-4.4286418 -4.4285083 -4.4283781 -4.4282522 -4.4281306 -4.4279933 -4.4278593 -4.4278407 -4.4280424 -4.4282341 -4.4283681 -4.4284658 -4.4285631 -4.42864 -4.4287109][-4.4286213 -4.4284906 -4.4283819 -4.4282947 -4.4282379 -4.4281926 -4.4281454 -4.4281316 -4.4282279 -4.4283371 -4.428441 -4.4285374 -4.4286327 -4.4287062 -4.4287691][-4.4287 -4.4286022 -4.4285331 -4.4284973 -4.428494 -4.4285054 -4.428503 -4.428484 -4.4285 -4.4285407 -4.4286103 -4.4286928 -4.4287682 -4.4288225 -4.4288678][-4.4288073 -4.4287457 -4.4287138 -4.4287148 -4.4287376 -4.4287605 -4.4287686 -4.4287539 -4.428741 -4.4287472 -4.428793 -4.4288554 -4.4289055 -4.4289374 -4.4289632][-4.4288907 -4.4288573 -4.4288454 -4.4288583 -4.4288845 -4.4289017 -4.4289079 -4.4289012 -4.4288907 -4.428884 -4.4289083 -4.4289503 -4.4289837 -4.4290023 -4.4290147][-4.42894 -4.4289265 -4.4289246 -4.4289355 -4.4289522 -4.4289622 -4.4289656 -4.4289622 -4.4289536 -4.4289436 -4.4289536 -4.42898 -4.4290066 -4.4290228 -4.4290285][-4.4289684 -4.4289641 -4.4289637 -4.42897 -4.4289784 -4.4289837 -4.4289851 -4.4289818 -4.4289756 -4.42897 -4.4289732 -4.428987 -4.4290056 -4.429018 -4.4290204]]...]
INFO - root - 2017-12-08 05:49:10.763282: step 15010, loss = 2.28, batch loss = 2.23 (34.9 examples/sec; 0.229 sec/batch; 20h:12m:32s remains)
INFO - root - 2017-12-08 05:49:13.020294: step 15020, loss = 2.28, batch loss = 2.23 (33.5 examples/sec; 0.239 sec/batch; 21h:03m:58s remains)
INFO - root - 2017-12-08 05:49:15.261924: step 15030, loss = 2.28, batch loss = 2.23 (35.3 examples/sec; 0.227 sec/batch; 19h:58m:26s remains)
INFO - root - 2017-12-08 05:49:17.520504: step 15040, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.222 sec/batch; 19h:32m:40s remains)
INFO - root - 2017-12-08 05:49:19.782702: step 15050, loss = 2.28, batch loss = 2.23 (36.0 examples/sec; 0.222 sec/batch; 19h:34m:43s remains)
INFO - root - 2017-12-08 05:49:22.056410: step 15060, loss = 2.28, batch loss = 2.23 (35.0 examples/sec; 0.228 sec/batch; 20h:08m:45s remains)
INFO - root - 2017-12-08 05:49:24.339464: step 15070, loss = 2.28, batch loss = 2.23 (34.9 examples/sec; 0.229 sec/batch; 20h:12m:43s remains)
INFO - root - 2017-12-08 05:49:26.606350: step 15080, loss = 2.28, batch loss = 2.23 (36.0 examples/sec; 0.222 sec/batch; 19h:36m:36s remains)
INFO - root - 2017-12-08 05:49:28.851415: step 15090, loss = 2.28, batch loss = 2.23 (35.8 examples/sec; 0.223 sec/batch; 19h:40m:46s remains)
INFO - root - 2017-12-08 05:49:31.095743: step 15100, loss = 2.28, batch loss = 2.23 (35.3 examples/sec; 0.226 sec/batch; 19h:57m:39s remains)
2017-12-08 05:49:31.373487: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4290028 -4.4290152 -4.42902 -4.4290257 -4.429029 -4.4290252 -4.4290147 -4.4290047 -4.4289994 -4.4289994 -4.4290094 -4.4290261 -4.4290395 -4.42905 -4.4290614][-4.4289613 -4.4289761 -4.4289823 -4.4289885 -4.4289927 -4.4289875 -4.4289727 -4.4289556 -4.4289451 -4.4289422 -4.4289565 -4.42898 -4.4289994 -4.429019 -4.4290409][-4.4288979 -4.4289103 -4.4289117 -4.4289107 -4.4289117 -4.4289064 -4.4288921 -4.4288754 -4.4288592 -4.4288497 -4.4288626 -4.42889 -4.4289179 -4.4289513 -4.4289923][-4.4288216 -4.428822 -4.4288149 -4.4288011 -4.428791 -4.4287863 -4.4287825 -4.4287772 -4.4287634 -4.4287496 -4.4287572 -4.4287863 -4.428822 -4.42887 -4.4289312][-4.4287424 -4.4287295 -4.4287109 -4.4286804 -4.4286542 -4.42865 -4.4286642 -4.4286814 -4.4286795 -4.4286661 -4.4286685 -4.4286933 -4.4287295 -4.4287848 -4.4288607][-4.4286752 -4.4286561 -4.4286313 -4.4285889 -4.4285469 -4.42854 -4.4285717 -4.4286141 -4.42862 -4.428606 -4.4286041 -4.4286227 -4.4286547 -4.4287095 -4.4287934][-4.4286451 -4.428627 -4.4286051 -4.4285693 -4.42853 -4.4285212 -4.428555 -4.428606 -4.4286137 -4.428597 -4.428586 -4.4285946 -4.4286165 -4.4286666 -4.4287515][-4.4286633 -4.428648 -4.4286342 -4.4286132 -4.4285846 -4.4285684 -4.4285827 -4.4286237 -4.4286313 -4.428618 -4.4286027 -4.4286036 -4.428617 -4.4286618 -4.4287405][-4.4287009 -4.428689 -4.4286842 -4.4286766 -4.428659 -4.4286346 -4.4286156 -4.4286351 -4.4286442 -4.4286423 -4.4286289 -4.4286294 -4.4286442 -4.4286857 -4.4287534][-4.4287419 -4.4287276 -4.4287243 -4.4287257 -4.4287167 -4.4286866 -4.4286385 -4.4286284 -4.4286313 -4.4286389 -4.4286437 -4.4286604 -4.4286833 -4.4287219 -4.4287782][-4.4287605 -4.4287376 -4.4287295 -4.4287386 -4.4287391 -4.4287119 -4.4286551 -4.428628 -4.4286246 -4.4286408 -4.428668 -4.4287014 -4.4287295 -4.4287639 -4.4288092][-4.4287872 -4.4287648 -4.4287519 -4.428761 -4.4287639 -4.4287395 -4.4286942 -4.4286723 -4.4286776 -4.4287009 -4.4287333 -4.428762 -4.4287791 -4.4288049 -4.4288425][-4.42884 -4.4288211 -4.4288015 -4.428803 -4.4288025 -4.4287829 -4.4287567 -4.4287515 -4.4287705 -4.4287968 -4.428822 -4.4288344 -4.4288368 -4.4288492 -4.4288807][-4.4289069 -4.4288988 -4.4288788 -4.4288697 -4.428863 -4.4288507 -4.42884 -4.4288445 -4.428865 -4.4288921 -4.4289117 -4.4289145 -4.4289069 -4.4289074 -4.4289312][-4.4289818 -4.4289827 -4.4289675 -4.4289536 -4.4289432 -4.4289355 -4.4289331 -4.4289389 -4.4289556 -4.4289789 -4.4289932 -4.4289913 -4.4289808 -4.4289765 -4.4289918]]...]
INFO - root - 2017-12-08 05:49:33.642551: step 15110, loss = 2.28, batch loss = 2.23 (34.6 examples/sec; 0.231 sec/batch; 20h:22m:36s remains)
INFO - root - 2017-12-08 05:49:35.880746: step 15120, loss = 2.28, batch loss = 2.23 (36.6 examples/sec; 0.219 sec/batch; 19h:15m:56s remains)
INFO - root - 2017-12-08 05:49:38.108226: step 15130, loss = 2.28, batch loss = 2.23 (35.2 examples/sec; 0.227 sec/batch; 20h:00m:36s remains)
INFO - root - 2017-12-08 05:49:40.330161: step 15140, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 19h:17m:49s remains)
INFO - root - 2017-12-08 05:49:42.579089: step 15150, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.221 sec/batch; 19h:27m:14s remains)
INFO - root - 2017-12-08 05:49:44.829626: step 15160, loss = 2.28, batch loss = 2.23 (35.0 examples/sec; 0.228 sec/batch; 20h:07m:48s remains)
INFO - root - 2017-12-08 05:49:47.029996: step 15170, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.225 sec/batch; 19h:51m:47s remains)
INFO - root - 2017-12-08 05:49:49.269929: step 15180, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.222 sec/batch; 19h:33m:25s remains)
INFO - root - 2017-12-08 05:49:51.493669: step 15190, loss = 2.28, batch loss = 2.23 (35.8 examples/sec; 0.223 sec/batch; 19h:40m:34s remains)
INFO - root - 2017-12-08 05:49:53.743082: step 15200, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 19h:36m:50s remains)
2017-12-08 05:49:54.023897: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288368 -4.4288368 -4.4288263 -4.428822 -4.4288182 -4.4288239 -4.4288287 -4.4288173 -4.4288292 -4.4288607 -4.428894 -4.428915 -4.4289274 -4.4289289 -4.4289274][-4.4288707 -4.4288697 -4.4288526 -4.4288378 -4.4288211 -4.4288182 -4.4288235 -4.4288158 -4.42883 -4.4288678 -4.4288969 -4.4289083 -4.4289165 -4.4289212 -4.428926][-4.4288697 -4.4288759 -4.4288626 -4.428844 -4.4288173 -4.4288039 -4.4288039 -4.4287934 -4.4288 -4.4288344 -4.4288616 -4.4288707 -4.4288869 -4.4289036 -4.4289184][-4.4288311 -4.4288478 -4.4288464 -4.42883 -4.4287987 -4.4287763 -4.4287634 -4.4287405 -4.4287324 -4.42876 -4.42879 -4.4288096 -4.4288435 -4.4288793 -4.42891][-4.4287777 -4.4287982 -4.4288044 -4.428793 -4.4287634 -4.4287319 -4.4287033 -4.42866 -4.4286332 -4.4286528 -4.4286923 -4.42874 -4.4288044 -4.428863 -4.4289074][-4.4287162 -4.4287324 -4.428741 -4.4287348 -4.4287076 -4.428669 -4.4286251 -4.4285688 -4.4285288 -4.428544 -4.4286075 -4.4286976 -4.4287906 -4.428865 -4.4289126][-4.4286575 -4.4286761 -4.4286923 -4.4286938 -4.4286728 -4.4286246 -4.4285622 -4.4284925 -4.4284525 -4.4284849 -4.4285851 -4.4287057 -4.42881 -4.4288826 -4.4289222][-4.4286175 -4.4286366 -4.4286613 -4.42867 -4.4286542 -4.4286 -4.4285269 -4.4284487 -4.4284177 -4.4284778 -4.4286036 -4.4287324 -4.4288363 -4.4289021 -4.4289341][-4.42861 -4.4286265 -4.428648 -4.4286523 -4.4286304 -4.428576 -4.4285088 -4.4284492 -4.4284611 -4.4285483 -4.4286718 -4.428782 -4.4288654 -4.4289203 -4.4289446][-4.4286413 -4.4286542 -4.4286652 -4.4286509 -4.42862 -4.428576 -4.4285364 -4.4285326 -4.4285908 -4.42868 -4.4287724 -4.4288445 -4.4289007 -4.4289379 -4.4289527][-4.4286957 -4.4287033 -4.4287028 -4.4286766 -4.4286461 -4.4286165 -4.4286113 -4.4286494 -4.42872 -4.4287944 -4.4288535 -4.428895 -4.4289317 -4.4289556 -4.4289603][-4.4287667 -4.4287639 -4.4287505 -4.4287214 -4.4286981 -4.4286904 -4.4287086 -4.4287524 -4.4288154 -4.428875 -4.4289074 -4.4289341 -4.4289618 -4.4289756 -4.4289694][-4.4288478 -4.4288354 -4.4288158 -4.4287877 -4.4287724 -4.4287791 -4.4288044 -4.4288387 -4.4288893 -4.4289355 -4.428957 -4.4289737 -4.4289908 -4.4289927 -4.4289775][-4.4289169 -4.4289 -4.4288754 -4.4288487 -4.42884 -4.4288526 -4.4288769 -4.4289064 -4.4289494 -4.4289904 -4.4290066 -4.4290118 -4.4290133 -4.4290028 -4.4289837][-4.4289627 -4.4289441 -4.4289174 -4.4288878 -4.4288774 -4.4288898 -4.4289145 -4.428947 -4.4289865 -4.4290228 -4.4290352 -4.4290328 -4.4290233 -4.4290056 -4.4289856]]...]
INFO - root - 2017-12-08 05:49:56.279956: step 15210, loss = 2.28, batch loss = 2.23 (35.0 examples/sec; 0.228 sec/batch; 20h:07m:08s remains)
INFO - root - 2017-12-08 05:49:58.533186: step 15220, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.222 sec/batch; 19h:32m:05s remains)
INFO - root - 2017-12-08 05:50:00.763510: step 15230, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 19h:37m:22s remains)
INFO - root - 2017-12-08 05:50:03.020765: step 15240, loss = 2.28, batch loss = 2.23 (35.2 examples/sec; 0.227 sec/batch; 20h:00m:12s remains)
INFO - root - 2017-12-08 05:50:05.253863: step 15250, loss = 2.28, batch loss = 2.23 (35.4 examples/sec; 0.226 sec/batch; 19h:53m:23s remains)
INFO - root - 2017-12-08 05:50:07.518951: step 15260, loss = 2.28, batch loss = 2.23 (34.1 examples/sec; 0.234 sec/batch; 20h:39m:41s remains)
INFO - root - 2017-12-08 05:50:09.775491: step 15270, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.221 sec/batch; 19h:26m:14s remains)
INFO - root - 2017-12-08 05:50:12.004540: step 15280, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 19h:27m:36s remains)
INFO - root - 2017-12-08 05:50:14.261013: step 15290, loss = 2.28, batch loss = 2.23 (37.3 examples/sec; 0.214 sec/batch; 18h:53m:52s remains)
INFO - root - 2017-12-08 05:50:16.513890: step 15300, loss = 2.28, batch loss = 2.23 (34.8 examples/sec; 0.230 sec/batch; 20h:16m:32s remains)
2017-12-08 05:50:16.807602: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4287558 -4.4287634 -4.4287815 -4.4288206 -4.4288354 -4.4288116 -4.4287629 -4.4287095 -4.4286795 -4.4286361 -4.428576 -4.428565 -4.4286318 -4.428689 -4.4286966][-4.4287319 -4.4287505 -4.4287677 -4.4287882 -4.4287934 -4.4287643 -4.4287076 -4.4286652 -4.4286432 -4.4285946 -4.4285407 -4.4285526 -4.42865 -4.4287372 -4.4287715][-4.4287186 -4.4287372 -4.4287395 -4.4287443 -4.4287539 -4.4287324 -4.4286842 -4.4286437 -4.4286184 -4.4285703 -4.4285178 -4.4285288 -4.4286194 -4.42871 -4.4287634][-4.4286828 -4.42869 -4.4286728 -4.4286804 -4.4287119 -4.4287057 -4.4286709 -4.4286366 -4.4286222 -4.4285879 -4.4285388 -4.4285331 -4.4285984 -4.4286795 -4.4287419][-4.4286175 -4.4286184 -4.4285908 -4.4286046 -4.4286528 -4.42865 -4.4286108 -4.4285865 -4.4285932 -4.42859 -4.42856 -4.4285474 -4.4285975 -4.4286776 -4.4287486][-4.4285569 -4.42856 -4.4285231 -4.42853 -4.4285722 -4.4285564 -4.4285021 -4.4284835 -4.428514 -4.4285493 -4.42856 -4.4285622 -4.4286094 -4.428689 -4.4287615][-4.4285307 -4.4285436 -4.4284973 -4.4284854 -4.4285131 -4.4284725 -4.4283986 -4.4283781 -4.4284325 -4.4285121 -4.4285736 -4.4285917 -4.4286251 -4.4286895 -4.4287548][-4.4285235 -4.4285383 -4.4284739 -4.4284391 -4.42846 -4.4284148 -4.4283409 -4.4283271 -4.4283948 -4.4285016 -4.428596 -4.428627 -4.4286375 -4.4286752 -4.4287276][-4.4285264 -4.4285231 -4.4284463 -4.4284043 -4.4284263 -4.4283934 -4.4283433 -4.4283519 -4.4284229 -4.428535 -4.4286308 -4.4286613 -4.4286513 -4.4286633 -4.4286981][-4.4285669 -4.4285531 -4.4284811 -4.4284463 -4.428462 -4.4284415 -4.42842 -4.4284487 -4.4285111 -4.4286041 -4.4286771 -4.4286842 -4.4286571 -4.4286532 -4.4286718][-4.4286504 -4.4286389 -4.4285846 -4.428566 -4.4285793 -4.4285665 -4.4285655 -4.4285975 -4.4286385 -4.4286962 -4.4287333 -4.4287081 -4.4286661 -4.4286537 -4.4286604][-4.4287205 -4.4287214 -4.4286971 -4.4286976 -4.4287152 -4.4287114 -4.4287138 -4.4287381 -4.4287581 -4.4287834 -4.4287915 -4.4287534 -4.42871 -4.4286938 -4.4286957][-4.4287529 -4.4287672 -4.4287667 -4.4287786 -4.4287992 -4.4288025 -4.4288087 -4.42883 -4.4288459 -4.4288526 -4.4288483 -4.4288135 -4.4287772 -4.4287629 -4.4287643][-4.42875 -4.4287763 -4.4287925 -4.4288144 -4.4288378 -4.4288492 -4.42886 -4.4288826 -4.4289031 -4.428906 -4.428895 -4.4288688 -4.428844 -4.4288335 -4.4288354][-4.4287357 -4.4287677 -4.4287944 -4.4288211 -4.4288483 -4.4288688 -4.4288874 -4.42891 -4.4289274 -4.4289289 -4.4289141 -4.428894 -4.4288812 -4.4288764 -4.4288778]]...]
INFO - root - 2017-12-08 05:50:19.017247: step 15310, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.225 sec/batch; 19h:51m:40s remains)
INFO - root - 2017-12-08 05:50:21.295195: step 15320, loss = 2.28, batch loss = 2.23 (34.8 examples/sec; 0.230 sec/batch; 20h:13m:51s remains)
INFO - root - 2017-12-08 05:50:23.506907: step 15330, loss = 2.28, batch loss = 2.23 (35.3 examples/sec; 0.227 sec/batch; 19h:58m:25s remains)
INFO - root - 2017-12-08 05:50:25.734172: step 15340, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.222 sec/batch; 19h:31m:42s remains)
INFO - root - 2017-12-08 05:50:27.962856: step 15350, loss = 2.28, batch loss = 2.23 (36.7 examples/sec; 0.218 sec/batch; 19h:13m:22s remains)
INFO - root - 2017-12-08 05:50:30.248658: step 15360, loss = 2.28, batch loss = 2.23 (34.0 examples/sec; 0.235 sec/batch; 20h:43m:26s remains)
INFO - root - 2017-12-08 05:50:32.463177: step 15370, loss = 2.28, batch loss = 2.23 (36.6 examples/sec; 0.218 sec/batch; 19h:14m:41s remains)
INFO - root - 2017-12-08 05:50:34.690622: step 15380, loss = 2.28, batch loss = 2.23 (35.6 examples/sec; 0.224 sec/batch; 19h:46m:17s remains)
INFO - root - 2017-12-08 05:50:36.915361: step 15390, loss = 2.28, batch loss = 2.23 (35.0 examples/sec; 0.229 sec/batch; 20h:09m:16s remains)
INFO - root - 2017-12-08 05:50:39.190114: step 15400, loss = 2.28, batch loss = 2.23 (33.4 examples/sec; 0.239 sec/batch; 21h:04m:42s remains)
2017-12-08 05:50:39.478956: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289842 -4.428988 -4.4289846 -4.42898 -4.428967 -4.4289503 -4.4289265 -4.4289055 -4.4289103 -4.4289346 -4.4289613 -4.4289632 -4.42895 -4.4289155 -4.42884][-4.428988 -4.4289894 -4.4289818 -4.428968 -4.428946 -4.42892 -4.4288816 -4.4288516 -4.4288573 -4.4288888 -4.4289274 -4.4289393 -4.4289265 -4.4288754 -4.4287663][-4.4289966 -4.4289956 -4.4289789 -4.4289541 -4.4289193 -4.4288707 -4.4288063 -4.4287643 -4.4287729 -4.4288249 -4.4288907 -4.4289169 -4.4289 -4.4288235 -4.4286618][-4.4290009 -4.4289951 -4.42897 -4.428937 -4.4288845 -4.4288011 -4.4286938 -4.4286242 -4.4286461 -4.4287405 -4.4288459 -4.4288912 -4.4288712 -4.4287672 -4.4285631][-4.4289947 -4.4289846 -4.4289513 -4.428905 -4.4288325 -4.4287105 -4.4285431 -4.4284348 -4.428493 -4.4286547 -4.4288044 -4.4288712 -4.4288516 -4.4287457 -4.4285641][-4.42898 -4.4289651 -4.42892 -4.4288568 -4.4287562 -4.4285808 -4.4283352 -4.4281673 -4.4282842 -4.428535 -4.4287329 -4.4288392 -4.4288454 -4.4287777 -4.4286637][-4.428966 -4.428946 -4.4288855 -4.4288049 -4.428679 -4.4284687 -4.4281726 -4.4279633 -4.428134 -4.4284525 -4.4286852 -4.4288187 -4.428854 -4.4288206 -4.4287643][-4.4289594 -4.428937 -4.4288692 -4.4287777 -4.428659 -4.4284854 -4.4282589 -4.4281235 -4.4282632 -4.4285355 -4.4287376 -4.4288487 -4.4288797 -4.428865 -4.4288354][-4.4289579 -4.4289417 -4.4288874 -4.4288082 -4.428719 -4.4286127 -4.4284964 -4.428453 -4.4285383 -4.4286971 -4.4288173 -4.4288845 -4.4288993 -4.428885 -4.4288473][-4.4289627 -4.4289589 -4.4289289 -4.4288721 -4.4288034 -4.4287472 -4.4287095 -4.4287133 -4.4287505 -4.4288116 -4.4288568 -4.4288888 -4.4288931 -4.4288688 -4.4288068][-4.4289751 -4.428978 -4.4289637 -4.4289289 -4.4288783 -4.4288425 -4.4288411 -4.4288611 -4.4288788 -4.4288926 -4.4288993 -4.4289031 -4.4288812 -4.42882 -4.4287128][-4.4289818 -4.4289837 -4.428978 -4.4289522 -4.4289021 -4.428863 -4.4288659 -4.4288993 -4.4289255 -4.4289389 -4.4289455 -4.4289279 -4.4288592 -4.4287238 -4.4285421][-4.4289746 -4.4289746 -4.428968 -4.4289331 -4.4288645 -4.4288063 -4.4288006 -4.4288449 -4.4288988 -4.4289393 -4.4289527 -4.4289112 -4.4287896 -4.4285765 -4.4283066][-4.428956 -4.4289508 -4.4289265 -4.4288645 -4.4287624 -4.4286685 -4.4286571 -4.4287333 -4.4288359 -4.428905 -4.4289169 -4.4288554 -4.4287114 -4.4284816 -4.428206][-4.428936 -4.4289231 -4.4288735 -4.4287615 -4.4285941 -4.4284406 -4.4284353 -4.4285784 -4.4287438 -4.4288516 -4.4288831 -4.4288292 -4.4287119 -4.4285479 -4.4283819]]...]
INFO - root - 2017-12-08 05:50:41.732755: step 15410, loss = 2.28, batch loss = 2.23 (33.7 examples/sec; 0.237 sec/batch; 20h:55m:06s remains)
INFO - root - 2017-12-08 05:50:43.978858: step 15420, loss = 2.28, batch loss = 2.23 (36.7 examples/sec; 0.218 sec/batch; 19h:12m:09s remains)
INFO - root - 2017-12-08 05:50:46.224665: step 15430, loss = 2.28, batch loss = 2.23 (35.0 examples/sec; 0.228 sec/batch; 20h:06m:22s remains)
INFO - root - 2017-12-08 05:50:48.450142: step 15440, loss = 2.28, batch loss = 2.23 (35.6 examples/sec; 0.225 sec/batch; 19h:47m:38s remains)
INFO - root - 2017-12-08 05:50:50.685036: step 15450, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 19h:36m:34s remains)
INFO - root - 2017-12-08 05:50:52.939864: step 15460, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.221 sec/batch; 19h:25m:55s remains)
INFO - root - 2017-12-08 05:50:55.174774: step 15470, loss = 2.28, batch loss = 2.23 (35.4 examples/sec; 0.226 sec/batch; 19h:53m:13s remains)
INFO - root - 2017-12-08 05:50:57.402938: step 15480, loss = 2.28, batch loss = 2.23 (36.9 examples/sec; 0.217 sec/batch; 19h:04m:48s remains)
INFO - root - 2017-12-08 05:50:59.642325: step 15490, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 19h:26m:48s remains)
INFO - root - 2017-12-08 05:51:01.856533: step 15500, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 19h:26m:15s remains)
2017-12-08 05:51:02.136282: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.428751 -4.4287605 -4.4287753 -4.4287667 -4.4287696 -4.428803 -4.4288445 -4.4288721 -4.4288568 -4.4287882 -4.4286842 -4.4285965 -4.4285874 -4.4286432 -4.42874][-4.4287519 -4.4287577 -4.4287477 -4.4287152 -4.4287062 -4.4287453 -4.4288073 -4.428853 -4.4288464 -4.4287763 -4.4286704 -4.4285774 -4.4285684 -4.4286227 -4.42872][-4.4287734 -4.4287672 -4.42873 -4.4286733 -4.4286547 -4.4286933 -4.4287658 -4.4288254 -4.4288354 -4.4287829 -4.4286933 -4.4286113 -4.4286013 -4.42865 -4.4287386][-4.4288025 -4.4287848 -4.4287267 -4.4286504 -4.4286156 -4.4286432 -4.4287057 -4.4287691 -4.4288063 -4.428802 -4.428761 -4.4287167 -4.4287105 -4.4287448 -4.4288144][-4.4288411 -4.4288239 -4.4287586 -4.4286695 -4.4286118 -4.4286051 -4.4286256 -4.4286675 -4.4287295 -4.4287891 -4.4288154 -4.428822 -4.4288306 -4.4288483 -4.4288883][-4.4288917 -4.4288898 -4.4288292 -4.4287281 -4.4286313 -4.4285574 -4.428494 -4.428484 -4.4285674 -4.4286942 -4.4287915 -4.4288507 -4.4288774 -4.4288893 -4.4289117][-4.4289331 -4.4289484 -4.4289002 -4.4287853 -4.4286389 -4.4284735 -4.4282942 -4.4282026 -4.428297 -4.4284921 -4.4286656 -4.4287882 -4.4288487 -4.4288712 -4.4288917][-4.4289665 -4.4289908 -4.4289556 -4.4288392 -4.4286609 -4.4284296 -4.4281573 -4.4279866 -4.4280672 -4.4283032 -4.4285278 -4.428699 -4.4287882 -4.4288244 -4.4288497][-4.428978 -4.4290051 -4.4289818 -4.4288778 -4.4287105 -4.4284778 -4.428184 -4.4279733 -4.428009 -4.4282231 -4.4284434 -4.428627 -4.4287262 -4.4287734 -4.4288034][-4.4289522 -4.4289789 -4.4289775 -4.4289036 -4.4287724 -4.4285879 -4.4283433 -4.4281449 -4.4281306 -4.428277 -4.4284458 -4.4286046 -4.4286947 -4.4287419 -4.42877][-4.4289193 -4.428947 -4.4289665 -4.4289317 -4.4288449 -4.42872 -4.4285512 -4.4283929 -4.4283509 -4.4284253 -4.4285164 -4.4286218 -4.42869 -4.4287343 -4.4287634][-4.4289069 -4.4289341 -4.428967 -4.4289613 -4.428916 -4.4288445 -4.4287486 -4.4286389 -4.4285836 -4.4286032 -4.428627 -4.4286733 -4.4287157 -4.4287524 -4.4287839][-4.4289045 -4.4289289 -4.428966 -4.4289765 -4.4289603 -4.4289279 -4.4288769 -4.4288092 -4.4287586 -4.4287467 -4.4287353 -4.4287386 -4.4287562 -4.428782 -4.4288135][-4.4289064 -4.4289269 -4.4289603 -4.4289784 -4.428977 -4.4289608 -4.4289327 -4.4288945 -4.4288578 -4.428833 -4.4288106 -4.4287968 -4.4287992 -4.4288158 -4.4288435][-4.4289374 -4.4289513 -4.4289737 -4.4289875 -4.4289861 -4.4289765 -4.4289608 -4.42894 -4.4289188 -4.4288988 -4.4288812 -4.428864 -4.4288607 -4.4288697 -4.4288926]]...]
INFO - root - 2017-12-08 05:51:04.365789: step 15510, loss = 2.28, batch loss = 2.23 (33.6 examples/sec; 0.238 sec/batch; 20h:58m:45s remains)
INFO - root - 2017-12-08 05:51:06.607861: step 15520, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 19h:18m:43s remains)
INFO - root - 2017-12-08 05:51:08.844557: step 15530, loss = 2.28, batch loss = 2.23 (36.0 examples/sec; 0.222 sec/batch; 19h:33m:50s remains)
INFO - root - 2017-12-08 05:51:11.073996: step 15540, loss = 2.28, batch loss = 2.23 (36.6 examples/sec; 0.219 sec/batch; 19h:15m:24s remains)
INFO - root - 2017-12-08 05:51:13.309309: step 15550, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 19h:26m:49s remains)
INFO - root - 2017-12-08 05:51:15.573471: step 15560, loss = 2.28, batch loss = 2.23 (34.9 examples/sec; 0.229 sec/batch; 20h:09m:46s remains)
INFO - root - 2017-12-08 05:51:17.804971: step 15570, loss = 2.28, batch loss = 2.23 (34.9 examples/sec; 0.230 sec/batch; 20h:12m:15s remains)
INFO - root - 2017-12-08 05:51:20.041169: step 15580, loss = 2.28, batch loss = 2.23 (35.3 examples/sec; 0.226 sec/batch; 19h:56m:17s remains)
INFO - root - 2017-12-08 05:51:22.257416: step 15590, loss = 2.28, batch loss = 2.23 (36.8 examples/sec; 0.218 sec/batch; 19h:08m:49s remains)
INFO - root - 2017-12-08 05:51:24.485438: step 15600, loss = 2.28, batch loss = 2.23 (37.0 examples/sec; 0.216 sec/batch; 19h:03m:03s remains)
2017-12-08 05:51:24.771292: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4286423 -4.4287028 -4.4287782 -4.4288216 -4.4288282 -4.428792 -4.4287443 -4.4287128 -4.4286895 -4.4286656 -4.428659 -4.4286876 -4.4287362 -4.428771 -4.4287772][-4.4286394 -4.4286976 -4.4287753 -4.428822 -4.4288259 -4.4287782 -4.4287095 -4.4286675 -4.4286523 -4.4286504 -4.4286718 -4.4287157 -4.4287653 -4.428793 -4.4287944][-4.4286833 -4.4287214 -4.428781 -4.4288087 -4.4287877 -4.4287238 -4.4286442 -4.4286094 -4.4286275 -4.4286685 -4.4287138 -4.4287581 -4.4287977 -4.4288225 -4.4288211][-4.4287448 -4.4287577 -4.428782 -4.4287686 -4.4287109 -4.4286256 -4.4285469 -4.428546 -4.4286184 -4.4287014 -4.4287586 -4.428791 -4.4288154 -4.4288335 -4.4288311][-4.4287786 -4.4287696 -4.4287558 -4.4286985 -4.4286003 -4.4284987 -4.428442 -4.4284968 -4.4286289 -4.4287405 -4.4287963 -4.4288139 -4.4288173 -4.4288116 -4.4287958][-4.428772 -4.4287505 -4.4287014 -4.4286036 -4.4284773 -4.4283748 -4.4283586 -4.4284725 -4.4286451 -4.428771 -4.4288278 -4.4288282 -4.428802 -4.4287634 -4.4287333][-4.4287319 -4.4286923 -4.4286203 -4.428503 -4.4283733 -4.4282932 -4.4283175 -4.4284625 -4.428647 -4.4287772 -4.4288368 -4.4288249 -4.4287686 -4.4287057 -4.4286642][-4.4286704 -4.4286156 -4.4285369 -4.4284272 -4.4283209 -4.4282761 -4.4283137 -4.4284453 -4.4286165 -4.4287434 -4.4287972 -4.4287672 -4.4286914 -4.4286246 -4.4285855][-4.4286342 -4.4285693 -4.4285007 -4.4284186 -4.4283557 -4.4283428 -4.4283752 -4.4284706 -4.4286041 -4.4287066 -4.4287348 -4.4286804 -4.4285908 -4.4285316 -4.4285088][-4.4286609 -4.4285975 -4.428546 -4.4284959 -4.4284687 -4.4284673 -4.4284906 -4.4285474 -4.428628 -4.4286909 -4.4286876 -4.4286127 -4.42852 -4.4284682 -4.4284582][-4.4287257 -4.4286733 -4.4286423 -4.42862 -4.4286079 -4.428606 -4.4286141 -4.42864 -4.4286771 -4.4287066 -4.4286833 -4.4286079 -4.4285264 -4.4284739 -4.4284515][-4.4287853 -4.4287558 -4.4287477 -4.4287457 -4.4287453 -4.4287415 -4.4287348 -4.4287376 -4.4287472 -4.4287558 -4.4287262 -4.4286637 -4.4286003 -4.4285517 -4.4285178][-4.4288378 -4.4288306 -4.4288306 -4.4288306 -4.4288316 -4.4288273 -4.4288216 -4.4288192 -4.4288187 -4.4288177 -4.428792 -4.4287448 -4.4286985 -4.4286642 -4.4286304][-4.4288826 -4.4288836 -4.4288788 -4.4288721 -4.4288707 -4.4288721 -4.4288764 -4.428875 -4.4288721 -4.4288664 -4.428843 -4.4288025 -4.4287682 -4.4287486 -4.4287219][-4.4288921 -4.4288917 -4.4288807 -4.4288745 -4.4288797 -4.42889 -4.4288979 -4.4288936 -4.4288888 -4.4288778 -4.4288592 -4.4288282 -4.4288025 -4.4287825 -4.4287577]]...]
INFO - root - 2017-12-08 05:51:27.018614: step 15610, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.221 sec/batch; 19h:24m:58s remains)
INFO - root - 2017-12-08 05:51:29.238984: step 15620, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.225 sec/batch; 19h:49m:30s remains)
INFO - root - 2017-12-08 05:51:31.491744: step 15630, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.225 sec/batch; 19h:48m:36s remains)
INFO - root - 2017-12-08 05:51:33.737953: step 15640, loss = 2.28, batch loss = 2.23 (35.1 examples/sec; 0.228 sec/batch; 20h:02m:57s remains)
INFO - root - 2017-12-08 05:51:35.949282: step 15650, loss = 2.28, batch loss = 2.23 (37.4 examples/sec; 0.214 sec/batch; 18h:50m:13s remains)
INFO - root - 2017-12-08 05:51:38.196803: step 15660, loss = 2.28, batch loss = 2.23 (34.1 examples/sec; 0.235 sec/batch; 20h:38m:48s remains)
INFO - root - 2017-12-08 05:51:40.425931: step 15670, loss = 2.28, batch loss = 2.23 (34.1 examples/sec; 0.235 sec/batch; 20h:40m:20s remains)
INFO - root - 2017-12-08 05:51:42.651070: step 15680, loss = 2.28, batch loss = 2.23 (35.0 examples/sec; 0.228 sec/batch; 20h:05m:22s remains)
INFO - root - 2017-12-08 05:51:44.865058: step 15690, loss = 2.28, batch loss = 2.23 (35.8 examples/sec; 0.223 sec/batch; 19h:39m:35s remains)
INFO - root - 2017-12-08 05:51:47.102025: step 15700, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 19h:26m:17s remains)
2017-12-08 05:51:47.370079: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289923 -4.4289918 -4.4289913 -4.4289875 -4.4289823 -4.428968 -4.4289474 -4.4289289 -4.4289165 -4.4289112 -4.4289136 -4.42892 -4.4289269 -4.428937 -4.4289508][-4.4289913 -4.4289927 -4.4289908 -4.4289789 -4.4289641 -4.4289374 -4.4289045 -4.4288769 -4.4288626 -4.4288564 -4.4288588 -4.428865 -4.4288754 -4.4288936 -4.4289155][-4.4289804 -4.4289794 -4.4289694 -4.4289455 -4.4289145 -4.4288759 -4.4288368 -4.4288068 -4.4287972 -4.4287949 -4.428791 -4.4287891 -4.4288006 -4.4288292 -4.4288621][-4.42896 -4.4289451 -4.4289217 -4.428884 -4.4288368 -4.4287839 -4.4287424 -4.4287128 -4.4287171 -4.4287324 -4.4287343 -4.42873 -4.4287415 -4.428772 -4.4288068][-4.4289441 -4.42891 -4.4288712 -4.4288211 -4.4287534 -4.4286771 -4.4286084 -4.4285645 -4.4285989 -4.4286637 -4.4286995 -4.4287076 -4.4287176 -4.428741 -4.4287643][-4.4289169 -4.42887 -4.4288216 -4.4287572 -4.4286513 -4.4285264 -4.4283905 -4.4283047 -4.4283772 -4.4285173 -4.4286113 -4.4286509 -4.4286718 -4.4286981 -4.428721][-4.428875 -4.4288173 -4.4287581 -4.4286723 -4.4285188 -4.4283276 -4.4280972 -4.4279528 -4.4280725 -4.4282961 -4.4284625 -4.4285512 -4.428597 -4.4286385 -4.4286709][-4.4288087 -4.42874 -4.4286752 -4.42858 -4.4284124 -4.4281883 -4.427896 -4.4277015 -4.4278488 -4.428123 -4.4283228 -4.4284415 -4.4285145 -4.4285789 -4.4286261][-4.4287362 -4.4286618 -4.4286013 -4.4285254 -4.4283991 -4.4282303 -4.4280114 -4.4278584 -4.427938 -4.4281435 -4.4282994 -4.4284015 -4.4284768 -4.4285545 -4.428618][-4.4286661 -4.4285941 -4.4285526 -4.4285183 -4.4284649 -4.4283805 -4.4282737 -4.4281926 -4.4282103 -4.4283104 -4.4283924 -4.428452 -4.42851 -4.4285836 -4.42865][-4.4286227 -4.428556 -4.428535 -4.4285407 -4.4285417 -4.4285154 -4.4284768 -4.4284415 -4.4284372 -4.4284744 -4.4285054 -4.4285326 -4.4285755 -4.4286423 -4.42871][-4.4286265 -4.4285731 -4.4285679 -4.4286008 -4.4286289 -4.4286304 -4.4286265 -4.4286165 -4.4286051 -4.428616 -4.4286323 -4.4286523 -4.4286838 -4.4287376 -4.4287972][-4.4287009 -4.4286714 -4.428678 -4.4287148 -4.4287429 -4.4287477 -4.4287529 -4.4287539 -4.4287448 -4.4287481 -4.4287658 -4.4287872 -4.4288139 -4.4288535 -4.4288955][-4.4288054 -4.428793 -4.4288044 -4.428834 -4.4288559 -4.4288616 -4.4288659 -4.4288712 -4.4288654 -4.428865 -4.4288769 -4.428894 -4.428916 -4.4289436 -4.4289703][-4.4288921 -4.4288883 -4.4289012 -4.4289227 -4.4289379 -4.4289436 -4.428946 -4.42895 -4.4289474 -4.4289451 -4.4289503 -4.4289603 -4.4289751 -4.4289923 -4.4290071]]...]
INFO - root - 2017-12-08 05:51:49.616837: step 15710, loss = 2.28, batch loss = 2.23 (35.3 examples/sec; 0.227 sec/batch; 19h:56m:46s remains)
INFO - root - 2017-12-08 05:51:51.845596: step 15720, loss = 2.28, batch loss = 2.23 (36.6 examples/sec; 0.218 sec/batch; 19h:13m:11s remains)
INFO - root - 2017-12-08 05:51:54.112560: step 15730, loss = 2.28, batch loss = 2.23 (35.8 examples/sec; 0.224 sec/batch; 19h:40m:27s remains)
INFO - root - 2017-12-08 05:51:56.339879: step 15740, loss = 2.28, batch loss = 2.23 (35.6 examples/sec; 0.225 sec/batch; 19h:47m:55s remains)
INFO - root - 2017-12-08 05:51:58.643923: step 15750, loss = 2.28, batch loss = 2.23 (36.7 examples/sec; 0.218 sec/batch; 19h:10m:00s remains)
INFO - root - 2017-12-08 05:52:00.906770: step 15760, loss = 2.28, batch loss = 2.23 (35.8 examples/sec; 0.223 sec/batch; 19h:38m:51s remains)
INFO - root - 2017-12-08 05:52:03.131870: step 15770, loss = 2.28, batch loss = 2.23 (34.7 examples/sec; 0.231 sec/batch; 20h:18m:07s remains)
INFO - root - 2017-12-08 05:52:05.348370: step 15780, loss = 2.28, batch loss = 2.23 (36.7 examples/sec; 0.218 sec/batch; 19h:10m:53s remains)
INFO - root - 2017-12-08 05:52:07.606732: step 15790, loss = 2.28, batch loss = 2.23 (34.4 examples/sec; 0.232 sec/batch; 20h:27m:04s remains)
INFO - root - 2017-12-08 05:52:09.859990: step 15800, loss = 2.28, batch loss = 2.23 (34.9 examples/sec; 0.229 sec/batch; 20h:11m:18s remains)
2017-12-08 05:52:10.173210: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4286547 -4.4286833 -4.4287214 -4.4287682 -4.4288058 -4.4288349 -4.4288483 -4.4288411 -4.4288106 -4.428762 -4.4286966 -4.4286451 -4.4286423 -4.4286761 -4.4287176][-4.4286785 -4.4286976 -4.4287257 -4.4287672 -4.428812 -4.4288535 -4.4288764 -4.4288616 -4.4288173 -4.4287629 -4.4287038 -4.4286647 -4.4286747 -4.4287205 -4.4287696][-4.4286985 -4.4286947 -4.4287057 -4.428741 -4.4287863 -4.4288254 -4.4288406 -4.428822 -4.4287758 -4.4287281 -4.4286866 -4.4286661 -4.42869 -4.4287491 -4.4288025][-4.4287214 -4.4287028 -4.4287043 -4.4287281 -4.4287562 -4.4287729 -4.428771 -4.4287491 -4.4287095 -4.4286795 -4.4286656 -4.4286714 -4.4287081 -4.4287667 -4.428813][-4.4287615 -4.4287443 -4.4287376 -4.4287395 -4.4287338 -4.4287176 -4.4286923 -4.4286675 -4.4286437 -4.4286451 -4.4286623 -4.4286861 -4.4287257 -4.4287705 -4.4287968][-4.4287887 -4.428781 -4.4287696 -4.4287462 -4.4287024 -4.4286504 -4.4286017 -4.4285741 -4.4285765 -4.4286213 -4.4286737 -4.4287133 -4.4287457 -4.4287734 -4.4287796][-4.4287624 -4.4287605 -4.4287491 -4.4287124 -4.4286447 -4.4285636 -4.4284906 -4.4284658 -4.4285107 -4.4286036 -4.4286852 -4.4287372 -4.4287682 -4.4287825 -4.428772][-4.4287076 -4.4287081 -4.4286981 -4.4286604 -4.4285817 -4.428483 -4.4284005 -4.4283977 -4.4284906 -4.4286194 -4.42872 -4.4287782 -4.4288039 -4.4287982 -4.4287691][-4.4286528 -4.4286394 -4.4286246 -4.4285865 -4.428514 -4.42843 -4.4283824 -4.4284205 -4.4285369 -4.4286666 -4.4287639 -4.428813 -4.4288211 -4.4287977 -4.4287572][-4.4286108 -4.4285703 -4.4285388 -4.4284983 -4.4284482 -4.42841 -4.4284186 -4.4284859 -4.4285913 -4.4286947 -4.4287691 -4.4288 -4.428792 -4.428761 -4.4287186][-4.4286041 -4.4285455 -4.4285016 -4.428463 -4.4284334 -4.4284267 -4.4284606 -4.428525 -4.428606 -4.4286842 -4.42874 -4.4287596 -4.4287472 -4.4287157 -4.4286771][-4.4286242 -4.4285793 -4.4285493 -4.428525 -4.4285059 -4.4285069 -4.4285336 -4.4285736 -4.4286242 -4.4286752 -4.4287143 -4.4287238 -4.4287066 -4.4286737 -4.4286351][-4.4286523 -4.4286375 -4.4286284 -4.4286165 -4.4286041 -4.4286036 -4.428617 -4.4286385 -4.4286671 -4.428699 -4.4287195 -4.4287152 -4.4286871 -4.4286466 -4.4286013][-4.4287052 -4.4287143 -4.42871 -4.4286923 -4.4286747 -4.428668 -4.4286747 -4.4286928 -4.4287162 -4.4287362 -4.428741 -4.4287233 -4.4286895 -4.428647 -4.4285989][-4.4287715 -4.4287829 -4.4287705 -4.4287462 -4.4287271 -4.428719 -4.4287219 -4.4287367 -4.4287529 -4.4287581 -4.4287467 -4.4287229 -4.4286933 -4.4286618 -4.4286242]]...]
INFO - root - 2017-12-08 05:52:12.431197: step 15810, loss = 2.28, batch loss = 2.23 (36.0 examples/sec; 0.222 sec/batch; 19h:33m:05s remains)
INFO - root - 2017-12-08 05:52:14.704573: step 15820, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.222 sec/batch; 19h:30m:56s remains)
INFO - root - 2017-12-08 05:52:16.924872: step 15830, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.221 sec/batch; 19h:23m:46s remains)
INFO - root - 2017-12-08 05:52:19.168660: step 15840, loss = 2.28, batch loss = 2.23 (34.3 examples/sec; 0.233 sec/batch; 20h:29m:32s remains)
INFO - root - 2017-12-08 05:52:21.393565: step 15850, loss = 2.28, batch loss = 2.23 (36.0 examples/sec; 0.222 sec/batch; 19h:33m:26s remains)
INFO - root - 2017-12-08 05:52:23.652448: step 15860, loss = 2.28, batch loss = 2.23 (35.4 examples/sec; 0.226 sec/batch; 19h:52m:49s remains)
INFO - root - 2017-12-08 05:52:25.881252: step 15870, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.220 sec/batch; 19h:21m:25s remains)
INFO - root - 2017-12-08 05:52:28.140824: step 15880, loss = 2.28, batch loss = 2.23 (35.7 examples/sec; 0.224 sec/batch; 19h:41m:22s remains)
INFO - root - 2017-12-08 05:52:30.378332: step 15890, loss = 2.28, batch loss = 2.23 (35.2 examples/sec; 0.227 sec/batch; 19h:57m:43s remains)
INFO - root - 2017-12-08 05:52:32.633021: step 15900, loss = 2.28, batch loss = 2.23 (34.5 examples/sec; 0.232 sec/batch; 20h:23m:14s remains)
2017-12-08 05:52:32.925348: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288087 -4.4288011 -4.4287915 -4.4288 -4.4288096 -4.4288135 -4.4288149 -4.428822 -4.4288368 -4.4288449 -4.4288344 -4.4288273 -4.4288292 -4.4288368 -4.4288559][-4.4287386 -4.4287348 -4.4287524 -4.4287968 -4.4288325 -4.4288321 -4.4288116 -4.4288011 -4.4288092 -4.4288139 -4.4288039 -4.4288049 -4.428812 -4.4288273 -4.4288554][-4.4286671 -4.4286594 -4.4286962 -4.4287682 -4.4288197 -4.4288182 -4.4287868 -4.4287696 -4.4287796 -4.4287786 -4.4287677 -4.4287639 -4.428771 -4.4287958 -4.4288297][-4.4286423 -4.4286232 -4.4286566 -4.4287238 -4.4287724 -4.4287682 -4.4287348 -4.4287138 -4.4287267 -4.4287305 -4.4287248 -4.4287167 -4.4287124 -4.42874 -4.4287815][-4.428678 -4.4286509 -4.4286652 -4.4286928 -4.428709 -4.4286933 -4.4286547 -4.4286227 -4.4286456 -4.4286728 -4.4286833 -4.4286795 -4.42867 -4.4287009 -4.4287448][-4.4287472 -4.4287167 -4.4287086 -4.4286885 -4.4286485 -4.4285922 -4.4285145 -4.4284687 -4.4285393 -4.4286227 -4.4286618 -4.4286718 -4.4286685 -4.4287019 -4.4287457][-4.4288082 -4.4287748 -4.4287472 -4.4286861 -4.4285817 -4.4284472 -4.4282894 -4.4282136 -4.4283738 -4.4285555 -4.4286528 -4.4286971 -4.4287148 -4.4287424 -4.4287643][-4.4288416 -4.4288025 -4.4287591 -4.4286742 -4.4285269 -4.4283228 -4.4280868 -4.4279776 -4.4282274 -4.4284997 -4.4286571 -4.42874 -4.4287758 -4.4287877 -4.4287753][-4.4288287 -4.4287891 -4.4287481 -4.4286685 -4.4285235 -4.428319 -4.4281034 -4.4280338 -4.428278 -4.4285326 -4.4286857 -4.4287782 -4.428822 -4.4288154 -4.4287639][-4.4287844 -4.4287481 -4.4287291 -4.4286847 -4.4285855 -4.4284296 -4.428277 -4.4282522 -4.4284396 -4.42863 -4.4287434 -4.4288173 -4.428853 -4.4288297 -4.4287572][-4.4287367 -4.4287004 -4.4287038 -4.4287047 -4.4286609 -4.4285674 -4.4284763 -4.4284773 -4.4286075 -4.4287353 -4.4288111 -4.4288564 -4.4288664 -4.4288306 -4.4287534][-4.4287243 -4.4286909 -4.4287047 -4.4287262 -4.4287214 -4.4286776 -4.4286351 -4.4286475 -4.4287295 -4.4288144 -4.428863 -4.42888 -4.4288683 -4.4288297 -4.4287658][-4.4287658 -4.4287438 -4.4287558 -4.4287753 -4.4287786 -4.4287572 -4.4287438 -4.4287562 -4.4288044 -4.4288592 -4.4288869 -4.428885 -4.428865 -4.4288321 -4.4287853][-4.4288311 -4.4288259 -4.4288368 -4.4288425 -4.4288335 -4.4288154 -4.4288092 -4.4288144 -4.4288416 -4.4288754 -4.4288912 -4.4288797 -4.4288573 -4.4288278 -4.4287949][-4.4288778 -4.4288878 -4.4289002 -4.4288964 -4.4288716 -4.4288449 -4.4288282 -4.4288259 -4.4288464 -4.428875 -4.4288874 -4.4288774 -4.4288526 -4.4288206 -4.428792]]...]
INFO - root - 2017-12-08 05:52:35.110215: step 15910, loss = 2.28, batch loss = 2.23 (36.7 examples/sec; 0.218 sec/batch; 19h:10m:53s remains)
INFO - root - 2017-12-08 05:52:37.404508: step 15920, loss = 2.28, batch loss = 2.23 (35.0 examples/sec; 0.229 sec/batch; 20h:06m:40s remains)
INFO - root - 2017-12-08 05:52:39.646639: step 15930, loss = 2.28, batch loss = 2.23 (34.9 examples/sec; 0.230 sec/batch; 20h:10m:55s remains)
INFO - root - 2017-12-08 05:52:41.869133: step 15940, loss = 2.28, batch loss = 2.23 (37.1 examples/sec; 0.216 sec/batch; 18h:58m:45s remains)
INFO - root - 2017-12-08 05:52:44.118167: step 15950, loss = 2.28, batch loss = 2.23 (37.2 examples/sec; 0.215 sec/batch; 18h:55m:17s remains)
INFO - root - 2017-12-08 05:52:46.356779: step 15960, loss = 2.28, batch loss = 2.23 (36.9 examples/sec; 0.217 sec/batch; 19h:02m:25s remains)
INFO - root - 2017-12-08 05:52:48.590298: step 15970, loss = 2.28, batch loss = 2.23 (37.1 examples/sec; 0.216 sec/batch; 18h:58m:55s remains)
INFO - root - 2017-12-08 05:52:50.839901: step 15980, loss = 2.28, batch loss = 2.23 (33.9 examples/sec; 0.236 sec/batch; 20h:44m:53s remains)
INFO - root - 2017-12-08 05:52:53.101168: step 15990, loss = 2.28, batch loss = 2.23 (36.6 examples/sec; 0.219 sec/batch; 19h:12m:47s remains)
INFO - root - 2017-12-08 05:52:55.338858: step 16000, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 19h:17m:21s remains)
2017-12-08 05:52:55.613323: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4287014 -4.4287362 -4.4287515 -4.4287167 -4.4286571 -4.4286261 -4.4286203 -4.4286289 -4.4286466 -4.42867 -4.4286904 -4.4286928 -4.4286857 -4.4286819 -4.4286933][-4.4286952 -4.4287453 -4.4287663 -4.4287281 -4.4286442 -4.4285717 -4.4285378 -4.428556 -4.4286075 -4.4286652 -4.4287114 -4.4287291 -4.4287333 -4.4287472 -4.4287639][-4.4286942 -4.4287562 -4.4287796 -4.4287415 -4.4286456 -4.4285426 -4.4284768 -4.4284978 -4.42857 -4.4286513 -4.4287152 -4.4287519 -4.4287691 -4.4287863 -4.428793][-4.4286962 -4.4287653 -4.428791 -4.4287548 -4.4286642 -4.4285536 -4.4284711 -4.4284883 -4.4285617 -4.4286408 -4.4287014 -4.4287505 -4.4287839 -4.4287987 -4.4287868][-4.4287148 -4.4287758 -4.4287987 -4.4287624 -4.4286814 -4.4285722 -4.4284792 -4.4284806 -4.4285388 -4.4286017 -4.4286532 -4.4287162 -4.4287586 -4.4287663 -4.4287405][-4.4287324 -4.4287796 -4.4287853 -4.4287381 -4.4286537 -4.4285407 -4.4284425 -4.4284339 -4.4284835 -4.4285388 -4.4285917 -4.4286585 -4.4286971 -4.4287 -4.4286723][-4.42873 -4.4287524 -4.4287267 -4.4286585 -4.4285665 -4.4284573 -4.4283633 -4.4283614 -4.4284291 -4.4285097 -4.4285841 -4.4286604 -4.4286985 -4.4287 -4.42868][-4.4287252 -4.4287071 -4.4286432 -4.4285583 -4.4284749 -4.4283962 -4.4283447 -4.4283767 -4.4284816 -4.4285884 -4.428668 -4.428731 -4.4287548 -4.4287486 -4.4287319][-4.4287429 -4.42869 -4.4286027 -4.428524 -4.4284697 -4.428443 -4.42845 -4.4285083 -4.4286108 -4.4287071 -4.4287577 -4.428781 -4.428771 -4.4287472 -4.4287362][-4.4287729 -4.4287052 -4.4286218 -4.4285617 -4.4285355 -4.4285426 -4.4285774 -4.4286375 -4.4287171 -4.4287806 -4.428791 -4.4287734 -4.4287362 -4.4287014 -4.4286938][-4.4287744 -4.4287128 -4.4286437 -4.4286008 -4.428596 -4.4286175 -4.4286537 -4.4286981 -4.4287496 -4.4287829 -4.4287672 -4.4287262 -4.42868 -4.4286418 -4.4286394][-4.4287472 -4.4287057 -4.4286551 -4.4286337 -4.4286451 -4.4286642 -4.4286857 -4.4287047 -4.4287281 -4.4287372 -4.4287081 -4.4286623 -4.4286242 -4.4286032 -4.4286103][-4.4286909 -4.4286771 -4.4286551 -4.42866 -4.4286809 -4.42869 -4.4286895 -4.4286804 -4.4286804 -4.4286828 -4.4286675 -4.4286313 -4.42861 -4.4286084 -4.4286294][-4.4286323 -4.4286413 -4.4286461 -4.4286733 -4.4287024 -4.4287062 -4.4286928 -4.4286742 -4.4286633 -4.4286647 -4.4286671 -4.4286537 -4.4286513 -4.4286623 -4.4286852][-4.428597 -4.4286213 -4.428647 -4.42869 -4.4287286 -4.428731 -4.4287076 -4.4286861 -4.4286838 -4.4287004 -4.428721 -4.4287219 -4.428721 -4.4287262 -4.4287338]]...]
INFO - root - 2017-12-08 05:52:57.855876: step 16010, loss = 2.28, batch loss = 2.23 (35.2 examples/sec; 0.227 sec/batch; 19h:57m:49s remains)
INFO - root - 2017-12-08 05:53:00.073373: step 16020, loss = 2.28, batch loss = 2.23 (37.0 examples/sec; 0.216 sec/batch; 19h:01m:17s remains)
INFO - root - 2017-12-08 05:53:02.306299: step 16030, loss = 2.28, batch loss = 2.23 (36.8 examples/sec; 0.218 sec/batch; 19h:07m:56s remains)
INFO - root - 2017-12-08 05:53:04.537257: step 16040, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 19h:26m:08s remains)
INFO - root - 2017-12-08 05:53:06.778056: step 16050, loss = 2.28, batch loss = 2.23 (35.4 examples/sec; 0.226 sec/batch; 19h:52m:44s remains)
INFO - root - 2017-12-08 05:53:09.055333: step 16060, loss = 2.28, batch loss = 2.23 (35.2 examples/sec; 0.227 sec/batch; 19h:57m:56s remains)
INFO - root - 2017-12-08 05:53:11.287512: step 16070, loss = 2.28, batch loss = 2.23 (34.7 examples/sec; 0.230 sec/batch; 20h:14m:59s remains)
INFO - root - 2017-12-08 05:53:13.512318: step 16080, loss = 2.28, batch loss = 2.23 (35.2 examples/sec; 0.227 sec/batch; 19h:59m:08s remains)
INFO - root - 2017-12-08 05:53:15.748963: step 16090, loss = 2.28, batch loss = 2.23 (34.0 examples/sec; 0.235 sec/batch; 20h:41m:26s remains)
INFO - root - 2017-12-08 05:53:18.002113: step 16100, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.220 sec/batch; 19h:22m:14s remains)
2017-12-08 05:53:18.285807: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289546 -4.4289632 -4.4289637 -4.4289613 -4.4289579 -4.4289551 -4.4289546 -4.4289541 -4.4289532 -4.4289536 -4.4289551 -4.4289579 -4.4289651 -4.428978 -4.4289894][-4.42892 -4.4289317 -4.428936 -4.4289379 -4.4289384 -4.4289379 -4.4289389 -4.4289393 -4.4289379 -4.428936 -4.4289355 -4.428936 -4.4289422 -4.4289579 -4.4289756][-4.4288821 -4.4288945 -4.4289012 -4.4289064 -4.4289122 -4.428916 -4.4289207 -4.4289227 -4.4289217 -4.4289193 -4.4289188 -4.4289165 -4.4289188 -4.4289336 -4.4289551][-4.428844 -4.4288497 -4.4288483 -4.4288468 -4.42885 -4.4288549 -4.42886 -4.428864 -4.4288673 -4.4288712 -4.4288788 -4.4288788 -4.4288774 -4.4288907 -4.4289165][-4.428793 -4.4287868 -4.4287705 -4.4287543 -4.4287472 -4.4287443 -4.428741 -4.4287438 -4.4287586 -4.4287786 -4.428803 -4.4288111 -4.4288068 -4.4288177 -4.4288507][-4.4287233 -4.4287024 -4.4286685 -4.4286318 -4.4286056 -4.4285831 -4.428556 -4.4285483 -4.4285827 -4.4286337 -4.4286861 -4.4287133 -4.4287138 -4.4287276 -4.42877][-4.4286685 -4.4286346 -4.4285827 -4.4285231 -4.4284754 -4.4284244 -4.4283519 -4.4283152 -4.4283743 -4.4284644 -4.428546 -4.4286008 -4.4286227 -4.4286489 -4.4287033][-4.4286647 -4.4286218 -4.428555 -4.4284697 -4.4283938 -4.4283085 -4.4281812 -4.4281087 -4.4281988 -4.4283347 -4.428441 -4.428514 -4.4285588 -4.4286 -4.4286633][-4.42871 -4.4286752 -4.4286156 -4.428524 -4.4284258 -4.4283118 -4.42815 -4.4280462 -4.4281349 -4.4282827 -4.4283957 -4.428484 -4.4285522 -4.428606 -4.42867][-4.4287577 -4.4287496 -4.4287233 -4.4286628 -4.4285822 -4.4284821 -4.4283485 -4.4282613 -4.4283042 -4.4283915 -4.4284668 -4.4285412 -4.42861 -4.4286628 -4.4287181][-4.4287763 -4.4287982 -4.4288087 -4.42879 -4.4287477 -4.4286857 -4.4286041 -4.4285488 -4.4285541 -4.4285793 -4.4286017 -4.4286385 -4.4286842 -4.4287257 -4.4287705][-4.4287415 -4.4287877 -4.4288306 -4.4288492 -4.4288478 -4.4288282 -4.428791 -4.42876 -4.4287472 -4.4287362 -4.4287214 -4.428721 -4.4287391 -4.4287663 -4.4288039][-4.428688 -4.4287477 -4.4288087 -4.428853 -4.4288816 -4.4288974 -4.4288912 -4.4288769 -4.4288592 -4.4288321 -4.4287992 -4.4287767 -4.4287739 -4.4287896 -4.428822][-4.4287047 -4.4287558 -4.4288068 -4.4288492 -4.4288898 -4.4289246 -4.4289365 -4.4289336 -4.4289203 -4.428895 -4.4288611 -4.4288325 -4.4288206 -4.4288287 -4.4288549][-4.4288025 -4.428833 -4.42886 -4.4288855 -4.4289174 -4.4289489 -4.4289627 -4.4289656 -4.4289603 -4.428947 -4.428926 -4.4289055 -4.4288936 -4.4288945 -4.4289083]]...]
INFO - root - 2017-12-08 05:53:20.503478: step 16110, loss = 2.28, batch loss = 2.23 (36.9 examples/sec; 0.217 sec/batch; 19h:02m:38s remains)
INFO - root - 2017-12-08 05:53:22.780505: step 16120, loss = 2.28, batch loss = 2.23 (35.3 examples/sec; 0.227 sec/batch; 19h:56m:24s remains)
INFO - root - 2017-12-08 05:53:25.023727: step 16130, loss = 2.28, batch loss = 2.23 (35.4 examples/sec; 0.226 sec/batch; 19h:50m:50s remains)
INFO - root - 2017-12-08 05:53:27.262629: step 16140, loss = 2.28, batch loss = 2.23 (35.8 examples/sec; 0.223 sec/batch; 19h:37m:12s remains)
INFO - root - 2017-12-08 05:53:29.525796: step 16150, loss = 2.28, batch loss = 2.23 (34.7 examples/sec; 0.230 sec/batch; 20h:15m:07s remains)
INFO - root - 2017-12-08 05:53:31.750897: step 16160, loss = 2.28, batch loss = 2.23 (37.2 examples/sec; 0.215 sec/batch; 18h:53m:35s remains)
INFO - root - 2017-12-08 05:53:33.993183: step 16170, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 19h:34m:58s remains)
INFO - root - 2017-12-08 05:53:36.176531: step 16180, loss = 2.28, batch loss = 2.23 (37.8 examples/sec; 0.212 sec/batch; 18h:35m:37s remains)
INFO - root - 2017-12-08 05:53:38.416210: step 16190, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 19h:14m:21s remains)
INFO - root - 2017-12-08 05:53:40.638908: step 16200, loss = 2.28, batch loss = 2.23 (36.8 examples/sec; 0.218 sec/batch; 19h:06m:36s remains)
2017-12-08 05:53:40.929424: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288921 -4.4288712 -4.4288716 -4.4288855 -4.428875 -4.4288526 -4.4288297 -4.4288096 -4.4288063 -4.4288049 -4.4287972 -4.4288011 -4.4288096 -4.4288211 -4.4288354][-4.4289031 -4.4288836 -4.4288774 -4.4288754 -4.4288445 -4.4287915 -4.4287348 -4.4287114 -4.4287243 -4.4287362 -4.4287372 -4.4287658 -4.4288077 -4.4288387 -4.4288564][-4.4289107 -4.4288893 -4.4288654 -4.4288397 -4.4287758 -4.4286871 -4.4285913 -4.4285583 -4.4286 -4.4286423 -4.4286594 -4.4287143 -4.4287858 -4.4288335 -4.4288573][-4.4289136 -4.4288869 -4.4288473 -4.4288006 -4.4287195 -4.4286151 -4.4284964 -4.428443 -4.428484 -4.4285469 -4.4285884 -4.4286618 -4.4287472 -4.4288049 -4.4288373][-4.4288936 -4.4288669 -4.42882 -4.4287639 -4.4286823 -4.4285879 -4.4284697 -4.428391 -4.4283957 -4.4284596 -4.4285245 -4.4286118 -4.4287009 -4.4287624 -4.4288011][-4.428874 -4.42885 -4.4288011 -4.4287372 -4.4286594 -4.428575 -4.4284635 -4.4283586 -4.4283195 -4.42839 -4.42848 -4.42858 -4.4286604 -4.4287252 -4.42877][-4.4288716 -4.4288564 -4.4288092 -4.4287438 -4.4286728 -4.4285994 -4.4285016 -4.4283938 -4.4283361 -4.4284019 -4.4284949 -4.428576 -4.4286332 -4.4286904 -4.4287376][-4.4288731 -4.4288707 -4.42884 -4.428791 -4.4287324 -4.4286714 -4.4286 -4.4285359 -4.4284983 -4.4285388 -4.4285874 -4.4286165 -4.4286337 -4.4286695 -4.4287028][-4.4288764 -4.4288826 -4.4288712 -4.428843 -4.4288006 -4.4287462 -4.4287 -4.4286857 -4.42868 -4.4286919 -4.4286938 -4.4286861 -4.4286737 -4.4286866 -4.4287009][-4.4288898 -4.4289007 -4.4288988 -4.4288845 -4.4288535 -4.4288063 -4.4287705 -4.4287829 -4.4287863 -4.4287891 -4.4287839 -4.4287691 -4.4287496 -4.4287491 -4.4287391][-4.428895 -4.4289112 -4.428916 -4.4289107 -4.4288945 -4.4288654 -4.4288397 -4.4288492 -4.4288473 -4.4288459 -4.4288425 -4.4288359 -4.4288182 -4.4288087 -4.4287834][-4.4288807 -4.4289055 -4.4289188 -4.4289255 -4.428926 -4.4289145 -4.4289 -4.428896 -4.4288845 -4.4288797 -4.4288716 -4.4288707 -4.4288578 -4.4288487 -4.4288216][-4.4288611 -4.4288893 -4.4289007 -4.4289107 -4.42892 -4.4289165 -4.428906 -4.4288964 -4.4288869 -4.4288878 -4.4288836 -4.4288874 -4.4288769 -4.4288731 -4.4288588][-4.4288573 -4.4288826 -4.4288874 -4.4288983 -4.4289155 -4.4289227 -4.4289193 -4.4289122 -4.428906 -4.4289055 -4.4289041 -4.4289055 -4.4288926 -4.4288859 -4.4288855][-4.428864 -4.4288812 -4.428885 -4.4288964 -4.4289165 -4.4289308 -4.4289303 -4.4289255 -4.4289236 -4.4289222 -4.4289184 -4.4289107 -4.4288864 -4.4288735 -4.4288793]]...]
INFO - root - 2017-12-08 05:53:43.137470: step 16210, loss = 2.28, batch loss = 2.23 (34.9 examples/sec; 0.230 sec/batch; 20h:09m:54s remains)
INFO - root - 2017-12-08 05:53:45.359844: step 16220, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 19h:33m:24s remains)
INFO - root - 2017-12-08 05:53:47.598204: step 16230, loss = 2.28, batch loss = 2.23 (36.0 examples/sec; 0.222 sec/batch; 19h:31m:45s remains)
INFO - root - 2017-12-08 05:53:49.819401: step 16240, loss = 2.28, batch loss = 2.23 (36.6 examples/sec; 0.218 sec/batch; 19h:10m:35s remains)
INFO - root - 2017-12-08 05:53:52.048237: step 16250, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 19h:35m:43s remains)
INFO - root - 2017-12-08 05:53:54.273098: step 16260, loss = 2.28, batch loss = 2.23 (36.0 examples/sec; 0.222 sec/batch; 19h:32m:23s remains)
INFO - root - 2017-12-08 05:53:56.508793: step 16270, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.222 sec/batch; 19h:27m:48s remains)
INFO - root - 2017-12-08 05:53:58.746319: step 16280, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 19h:35m:37s remains)
INFO - root - 2017-12-08 05:54:00.975990: step 16290, loss = 2.28, batch loss = 2.23 (35.3 examples/sec; 0.227 sec/batch; 19h:53m:56s remains)
INFO - root - 2017-12-08 05:54:03.203413: step 16300, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.222 sec/batch; 19h:27m:23s remains)
2017-12-08 05:54:03.505836: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289293 -4.4289246 -4.428916 -4.4289012 -4.4288864 -4.4288793 -4.4288826 -4.4288926 -4.4289069 -4.4289188 -4.4289131 -4.4288878 -4.4288545 -4.4288311 -4.4288359][-4.4288945 -4.4288926 -4.4288864 -4.4288683 -4.4288421 -4.4288206 -4.4288135 -4.4288225 -4.428844 -4.4288688 -4.4288697 -4.4288354 -4.4287853 -4.4287519 -4.4287581][-4.4288578 -4.42887 -4.4288778 -4.4288635 -4.4288225 -4.4287753 -4.4287472 -4.428751 -4.4287858 -4.4288278 -4.4288387 -4.4287963 -4.4287229 -4.4286695 -4.4286695][-4.428843 -4.428874 -4.4289 -4.4288883 -4.4288311 -4.428751 -4.4286904 -4.4286833 -4.4287357 -4.4288034 -4.428833 -4.4287953 -4.428709 -4.4286284 -4.4286084][-4.4288354 -4.4288774 -4.4289131 -4.4289012 -4.4288282 -4.4287186 -4.4286189 -4.4285913 -4.4286652 -4.4287734 -4.428833 -4.42882 -4.4287424 -4.4286432 -4.4285951][-4.4288177 -4.4288659 -4.4289069 -4.4288945 -4.4288073 -4.4286714 -4.42853 -4.4284711 -4.4285617 -4.4287157 -4.4288206 -4.4288468 -4.4287925 -4.4286876 -4.4286141][-4.4287882 -4.4288392 -4.4288778 -4.4288578 -4.4287534 -4.4285946 -4.4284225 -4.4283323 -4.4284353 -4.4286342 -4.428792 -4.4288621 -4.4288368 -4.4287362 -4.4286413][-4.4287353 -4.4287896 -4.4288325 -4.4288092 -4.4286995 -4.4285331 -4.4283452 -4.4282293 -4.4283204 -4.4285355 -4.4287357 -4.4288507 -4.4288568 -4.428771 -4.4286652][-4.4286709 -4.4287353 -4.4287963 -4.4287968 -4.4287114 -4.428566 -4.4283905 -4.4282589 -4.4283028 -4.4284835 -4.4286909 -4.4288325 -4.4288673 -4.4288006 -4.4286847][-4.4286222 -4.4286857 -4.4287577 -4.4287906 -4.4287524 -4.4286547 -4.4285245 -4.4284067 -4.4283981 -4.4285092 -4.4286761 -4.42881 -4.42885 -4.428791 -4.4286718][-4.4286227 -4.4286752 -4.4287448 -4.4287953 -4.4287968 -4.4287505 -4.4286704 -4.4285803 -4.428545 -4.4285951 -4.4287066 -4.428812 -4.4288492 -4.4288015 -4.4286947][-4.428679 -4.4287157 -4.4287758 -4.4288311 -4.4288578 -4.4288507 -4.4288073 -4.4287434 -4.4287024 -4.4287143 -4.4287744 -4.4288487 -4.4288807 -4.4288478 -4.4287639][-4.4287777 -4.4287977 -4.4288459 -4.4288979 -4.4289317 -4.4289441 -4.4289274 -4.428885 -4.4288478 -4.42884 -4.4288673 -4.4289122 -4.428937 -4.4289145 -4.4288521][-4.42887 -4.4288783 -4.4289107 -4.4289508 -4.4289761 -4.4289913 -4.4289908 -4.4289694 -4.4289436 -4.4289274 -4.4289336 -4.4289579 -4.4289756 -4.4289594 -4.4289155][-4.4289255 -4.4289279 -4.4289436 -4.4289646 -4.4289765 -4.4289861 -4.4289937 -4.4289885 -4.4289756 -4.4289613 -4.4289589 -4.42897 -4.4289789 -4.4289656 -4.428936]]...]
INFO - root - 2017-12-08 05:54:05.748161: step 16310, loss = 2.28, batch loss = 2.23 (34.3 examples/sec; 0.234 sec/batch; 20h:30m:31s remains)
INFO - root - 2017-12-08 05:54:07.977737: step 16320, loss = 2.28, batch loss = 2.23 (35.3 examples/sec; 0.227 sec/batch; 19h:54m:07s remains)
INFO - root - 2017-12-08 05:54:10.217208: step 16330, loss = 2.28, batch loss = 2.23 (37.4 examples/sec; 0.214 sec/batch; 18h:48m:04s remains)
INFO - root - 2017-12-08 05:54:12.448484: step 16340, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 19h:18m:30s remains)
INFO - root - 2017-12-08 05:54:14.672387: step 16350, loss = 2.28, batch loss = 2.23 (36.7 examples/sec; 0.218 sec/batch; 19h:07m:26s remains)
INFO - root - 2017-12-08 05:54:16.906549: step 16360, loss = 2.28, batch loss = 2.23 (36.0 examples/sec; 0.222 sec/batch; 19h:32m:11s remains)
INFO - root - 2017-12-08 05:54:19.136408: step 16370, loss = 2.28, batch loss = 2.23 (35.7 examples/sec; 0.224 sec/batch; 19h:40m:28s remains)
INFO - root - 2017-12-08 05:54:21.398155: step 16380, loss = 2.28, batch loss = 2.23 (35.4 examples/sec; 0.226 sec/batch; 19h:51m:33s remains)
INFO - root - 2017-12-08 05:54:23.626384: step 16390, loss = 2.28, batch loss = 2.23 (35.4 examples/sec; 0.226 sec/batch; 19h:51m:36s remains)
INFO - root - 2017-12-08 05:54:25.854802: step 16400, loss = 2.28, batch loss = 2.23 (35.2 examples/sec; 0.227 sec/batch; 19h:58m:05s remains)
2017-12-08 05:54:26.125568: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288321 -4.4288082 -4.4287791 -4.4287348 -4.428659 -4.428575 -4.4285197 -4.4285049 -4.4285407 -4.4286008 -4.4286733 -4.42873 -4.4287653 -4.4288 -4.4288473][-4.4288282 -4.4288144 -4.4287996 -4.4287643 -4.4286904 -4.4286036 -4.4285407 -4.4285264 -4.4285603 -4.4286094 -4.428668 -4.4287105 -4.428741 -4.4287777 -4.42883][-4.4288015 -4.4287987 -4.428802 -4.4288006 -4.4287572 -4.4286804 -4.4286027 -4.4285703 -4.4285874 -4.428617 -4.428668 -4.4287038 -4.4287271 -4.4287634 -4.428822][-4.4287515 -4.4287548 -4.4287748 -4.4288087 -4.4288039 -4.4287333 -4.4286151 -4.4285431 -4.4285522 -4.4285984 -4.4286757 -4.4287267 -4.4287453 -4.4287748 -4.4288297][-4.4286861 -4.4286947 -4.4287295 -4.4287777 -4.428782 -4.4286981 -4.4285235 -4.4284015 -4.4284267 -4.4285374 -4.4286671 -4.4287453 -4.4287682 -4.4287949 -4.4288445][-4.4286213 -4.4286451 -4.4286904 -4.4287353 -4.4287171 -4.4285984 -4.4283428 -4.4281359 -4.428194 -4.4284024 -4.4286075 -4.4287286 -4.4287696 -4.4288034 -4.4288526][-4.4285736 -4.4286137 -4.4286613 -4.4286885 -4.4286337 -4.4284644 -4.4281363 -4.4278607 -4.4279866 -4.4283009 -4.4285603 -4.4287105 -4.4287663 -4.4288 -4.4288497][-4.428586 -4.428628 -4.4286671 -4.4286733 -4.4285984 -4.4284234 -4.4281292 -4.4279127 -4.4280753 -4.4283757 -4.428607 -4.4287367 -4.4287782 -4.4288 -4.4288435][-4.4286852 -4.4287171 -4.4287391 -4.428731 -4.4286571 -4.4285278 -4.4283466 -4.4282451 -4.4283605 -4.4285436 -4.4286928 -4.4287786 -4.4287963 -4.4288116 -4.4288511][-4.4287853 -4.4288054 -4.4288154 -4.4288025 -4.4287291 -4.4286323 -4.4285297 -4.4284921 -4.4285693 -4.4286671 -4.4287534 -4.4288044 -4.4288149 -4.4288321 -4.4288664][-4.4288282 -4.4288464 -4.4288526 -4.4288406 -4.4287786 -4.4287038 -4.4286442 -4.4286313 -4.4286771 -4.4287233 -4.4287739 -4.4288116 -4.4288249 -4.4288459 -4.4288769][-4.4288125 -4.4288316 -4.4288335 -4.4288192 -4.4287682 -4.4287071 -4.428668 -4.4286666 -4.4286976 -4.4287214 -4.4287596 -4.4288 -4.4288244 -4.42885 -4.4288836][-4.4287777 -4.4287915 -4.4287939 -4.4287825 -4.4287376 -4.4286852 -4.4286489 -4.428638 -4.4286642 -4.4286847 -4.4287305 -4.4287887 -4.4288278 -4.4288554 -4.4288917][-4.4287572 -4.4287615 -4.4287572 -4.4287548 -4.4287281 -4.4286914 -4.4286556 -4.428638 -4.428659 -4.428688 -4.42875 -4.4288254 -4.4288697 -4.4288893 -4.4289174][-4.4287481 -4.4287395 -4.4287305 -4.4287438 -4.428741 -4.428721 -4.4286971 -4.42869 -4.4287128 -4.4287467 -4.4288192 -4.4289 -4.4289346 -4.4289393 -4.4289517]]...]
INFO - root - 2017-12-08 05:54:28.337212: step 16410, loss = 2.28, batch loss = 2.23 (35.8 examples/sec; 0.224 sec/batch; 19h:38m:21s remains)
INFO - root - 2017-12-08 05:54:30.585866: step 16420, loss = 2.28, batch loss = 2.23 (34.8 examples/sec; 0.230 sec/batch; 20h:11m:51s remains)
INFO - root - 2017-12-08 05:54:32.826585: step 16430, loss = 2.28, batch loss = 2.23 (35.7 examples/sec; 0.224 sec/batch; 19h:41m:16s remains)
INFO - root - 2017-12-08 05:54:35.029595: step 16440, loss = 2.28, batch loss = 2.23 (35.1 examples/sec; 0.228 sec/batch; 20h:00m:53s remains)
INFO - root - 2017-12-08 05:54:37.275565: step 16450, loss = 2.28, batch loss = 2.23 (35.4 examples/sec; 0.226 sec/batch; 19h:51m:37s remains)
INFO - root - 2017-12-08 05:54:39.551267: step 16460, loss = 2.28, batch loss = 2.23 (36.7 examples/sec; 0.218 sec/batch; 19h:09m:23s remains)
INFO - root - 2017-12-08 05:54:41.781128: step 16470, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 19h:16m:17s remains)
INFO - root - 2017-12-08 05:54:44.026197: step 16480, loss = 2.28, batch loss = 2.23 (34.7 examples/sec; 0.231 sec/batch; 20h:14m:36s remains)
INFO - root - 2017-12-08 05:54:46.271108: step 16490, loss = 2.28, batch loss = 2.23 (36.7 examples/sec; 0.218 sec/batch; 19h:09m:20s remains)
INFO - root - 2017-12-08 05:54:48.496416: step 16500, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 19h:16m:52s remains)
2017-12-08 05:54:48.772911: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288387 -4.4288425 -4.428844 -4.4288478 -4.4288549 -4.428863 -4.4288726 -4.4288778 -4.4288754 -4.4288673 -4.4288611 -4.4288573 -4.4288473 -4.4288282 -4.428813][-4.4288316 -4.4288335 -4.4288254 -4.4288211 -4.4288249 -4.428834 -4.4288497 -4.4288635 -4.42887 -4.4288678 -4.4288678 -4.4288688 -4.4288626 -4.4288454 -4.4288316][-4.4288025 -4.4288068 -4.4287925 -4.4287796 -4.4287744 -4.4287734 -4.4287839 -4.4288006 -4.4288135 -4.4288177 -4.4288282 -4.428844 -4.4288521 -4.4288421 -4.4288268][-4.4287386 -4.4287424 -4.4287224 -4.4287052 -4.4286928 -4.4286814 -4.4286814 -4.4286895 -4.4287009 -4.4287086 -4.4287295 -4.4287596 -4.4287825 -4.4287791 -4.4287605][-4.4286427 -4.428647 -4.42863 -4.4286222 -4.428618 -4.428606 -4.4285979 -4.4285879 -4.4285817 -4.428576 -4.4285936 -4.428627 -4.428659 -4.4286675 -4.4286604][-4.4285626 -4.4285669 -4.4285593 -4.4285684 -4.4285793 -4.428575 -4.4285612 -4.428524 -4.4284859 -4.42846 -4.4284663 -4.428493 -4.428525 -4.4285464 -4.4285636][-4.4285455 -4.4285474 -4.4285469 -4.428566 -4.4285831 -4.4285765 -4.428555 -4.428494 -4.428422 -4.4283752 -4.4283657 -4.4283772 -4.4284048 -4.428443 -4.4284873][-4.4285641 -4.4285574 -4.4285569 -4.4285812 -4.4285984 -4.4285893 -4.4285641 -4.4284954 -4.4284163 -4.4283652 -4.42834 -4.4283266 -4.4283447 -4.4283953 -4.4284558][-4.4285889 -4.4285684 -4.4285579 -4.4285779 -4.4285927 -4.4285908 -4.4285731 -4.4285197 -4.4284654 -4.4284415 -4.4284344 -4.4284239 -4.4284248 -4.4284449 -4.4284792][-4.4286237 -4.4285989 -4.4285789 -4.4285846 -4.4285808 -4.4285736 -4.42856 -4.4285316 -4.4285159 -4.4285264 -4.4285483 -4.4285583 -4.4285536 -4.4285355 -4.4285235][-4.4286628 -4.428637 -4.428617 -4.4286065 -4.42858 -4.428555 -4.4285345 -4.4285245 -4.4285393 -4.4285746 -4.428616 -4.4286423 -4.4286408 -4.4286003 -4.428565][-4.4286718 -4.4286542 -4.4286485 -4.4286361 -4.4286008 -4.4285626 -4.4285312 -4.4285245 -4.4285507 -4.428597 -4.4286461 -4.4286833 -4.4286861 -4.4286475 -4.4286294][-4.4286714 -4.4286714 -4.4286847 -4.4286795 -4.4286418 -4.4285989 -4.4285617 -4.4285541 -4.4285784 -4.4286208 -4.4286671 -4.4287095 -4.4287229 -4.4287081 -4.4287229][-4.4286752 -4.4286861 -4.4287066 -4.4287066 -4.4286685 -4.4286237 -4.4285932 -4.4285917 -4.4286146 -4.4286489 -4.4286928 -4.4287395 -4.4287629 -4.4287786 -4.4288177][-4.4287038 -4.4287076 -4.4287143 -4.4287052 -4.4286671 -4.428627 -4.4286103 -4.4286218 -4.42865 -4.428679 -4.4287176 -4.4287634 -4.4287882 -4.4288197 -4.4288697]]...]
INFO - root - 2017-12-08 05:54:51.031750: step 16510, loss = 2.28, batch loss = 2.23 (33.2 examples/sec; 0.241 sec/batch; 21h:10m:16s remains)
INFO - root - 2017-12-08 05:54:53.315620: step 16520, loss = 2.28, batch loss = 2.23 (35.6 examples/sec; 0.225 sec/batch; 19h:43m:56s remains)
INFO - root - 2017-12-08 05:54:55.552025: step 16530, loss = 2.28, batch loss = 2.23 (37.0 examples/sec; 0.216 sec/batch; 18h:57m:32s remains)
INFO - root - 2017-12-08 05:54:57.771771: step 16540, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 19h:16m:28s remains)
INFO - root - 2017-12-08 05:55:00.005002: step 16550, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 19h:14m:13s remains)
INFO - root - 2017-12-08 05:55:02.228039: step 16560, loss = 2.28, batch loss = 2.23 (35.7 examples/sec; 0.224 sec/batch; 19h:40m:20s remains)
INFO - root - 2017-12-08 05:55:04.463874: step 16570, loss = 2.28, batch loss = 2.23 (34.8 examples/sec; 0.230 sec/batch; 20h:08m:55s remains)
INFO - root - 2017-12-08 05:55:06.689394: step 16580, loss = 2.28, batch loss = 2.23 (35.6 examples/sec; 0.225 sec/batch; 19h:44m:10s remains)
INFO - root - 2017-12-08 05:55:08.940247: step 16590, loss = 2.28, batch loss = 2.23 (36.6 examples/sec; 0.219 sec/batch; 19h:10m:38s remains)
INFO - root - 2017-12-08 05:55:11.152082: step 16600, loss = 2.28, batch loss = 2.23 (37.0 examples/sec; 0.216 sec/batch; 18h:58m:34s remains)
2017-12-08 05:55:11.439470: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4285455 -4.4285393 -4.428546 -4.4284978 -4.4284992 -4.4285893 -4.4286628 -4.4286876 -4.428699 -4.4286942 -4.4286566 -4.4286532 -4.4286876 -4.4286909 -4.4286876][-4.4285889 -4.428587 -4.4285951 -4.4285522 -4.4285531 -4.4286423 -4.4286933 -4.4287195 -4.4287462 -4.4287386 -4.4286923 -4.4286985 -4.4287372 -4.4287324 -4.428719][-4.4286308 -4.4286308 -4.4286437 -4.4285979 -4.428597 -4.4286771 -4.4286971 -4.4287248 -4.4287691 -4.4287667 -4.4287295 -4.4287462 -4.4287834 -4.4287667 -4.4287477][-4.4286489 -4.4286427 -4.4286475 -4.4285946 -4.4285946 -4.428668 -4.4286666 -4.4286919 -4.4287438 -4.42875 -4.4287295 -4.4287534 -4.4287748 -4.4287386 -4.4287162][-4.4286618 -4.4286389 -4.4286251 -4.428566 -4.4285688 -4.4286265 -4.4285989 -4.4286146 -4.4286695 -4.4286847 -4.4286838 -4.4287219 -4.4287434 -4.4286976 -4.4286714][-4.42865 -4.4286118 -4.4285822 -4.4285121 -4.428514 -4.42854 -4.4284472 -4.4284368 -4.4285307 -4.4285874 -4.4286108 -4.4286728 -4.4287152 -4.4286871 -4.4286695][-4.4286189 -4.4285522 -4.4284935 -4.4283977 -4.4283791 -4.4283457 -4.428133 -4.4280791 -4.4282827 -4.4284396 -4.4285159 -4.4286184 -4.4287 -4.4287124 -4.4287171][-4.4286132 -4.4285264 -4.4284358 -4.428318 -4.4282694 -4.42817 -4.42783 -4.4277086 -4.4280434 -4.4283185 -4.4284544 -4.4285917 -4.4287109 -4.42876 -4.42879][-4.4285941 -4.4285107 -4.4284267 -4.4283223 -4.42828 -4.4282064 -4.4279189 -4.4277911 -4.428082 -4.428349 -4.4284854 -4.4286165 -4.4287505 -4.4288139 -4.4288416][-4.4285612 -4.4284968 -4.42844 -4.4283719 -4.4283681 -4.4283772 -4.4282336 -4.4281554 -4.4283195 -4.4284897 -4.4285789 -4.4286747 -4.4287858 -4.4288397 -4.4288554][-4.428535 -4.4284921 -4.428473 -4.428443 -4.4284816 -4.4285388 -4.4284806 -4.4284387 -4.4285283 -4.4286141 -4.4286585 -4.4287219 -4.4288006 -4.42884 -4.4288468][-4.4285116 -4.4284821 -4.4284954 -4.4285 -4.4285622 -4.4286423 -4.4286289 -4.4286108 -4.4286628 -4.4286981 -4.42872 -4.4287639 -4.4288092 -4.4288211 -4.4288182][-4.4285135 -4.4284964 -4.428534 -4.4285607 -4.4286275 -4.4287205 -4.4287357 -4.4287233 -4.4287481 -4.4287586 -4.4287629 -4.4287863 -4.4288025 -4.4287891 -4.4287791][-4.4285688 -4.42857 -4.428618 -4.4286451 -4.42869 -4.428771 -4.4288006 -4.4287848 -4.4287887 -4.4287882 -4.4287806 -4.4287848 -4.4287786 -4.428751 -4.4287438][-4.4286671 -4.4286857 -4.4287214 -4.4287338 -4.4287591 -4.4288154 -4.4288392 -4.4288216 -4.4288187 -4.4288182 -4.4288063 -4.4287987 -4.4287844 -4.4287558 -4.428751]]...]
INFO - root - 2017-12-08 05:55:13.693085: step 16610, loss = 2.28, batch loss = 2.23 (34.0 examples/sec; 0.236 sec/batch; 20h:40m:34s remains)
INFO - root - 2017-12-08 05:55:15.940265: step 16620, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.220 sec/batch; 19h:19m:49s remains)
INFO - root - 2017-12-08 05:55:18.162990: step 16630, loss = 2.28, batch loss = 2.23 (36.6 examples/sec; 0.219 sec/batch; 19h:11m:06s remains)
INFO - root - 2017-12-08 05:55:20.419825: step 16640, loss = 2.28, batch loss = 2.23 (35.8 examples/sec; 0.223 sec/batch; 19h:36m:08s remains)
INFO - root - 2017-12-08 05:55:22.675166: step 16650, loss = 2.28, batch loss = 2.23 (35.3 examples/sec; 0.227 sec/batch; 19h:53m:24s remains)
INFO - root - 2017-12-08 05:55:24.926245: step 16660, loss = 2.28, batch loss = 2.23 (34.1 examples/sec; 0.234 sec/batch; 20h:33m:36s remains)
INFO - root - 2017-12-08 05:55:27.149493: step 16670, loss = 2.28, batch loss = 2.23 (37.1 examples/sec; 0.215 sec/batch; 18h:53m:37s remains)
INFO - root - 2017-12-08 05:55:29.371840: step 16680, loss = 2.28, batch loss = 2.23 (37.1 examples/sec; 0.215 sec/batch; 18h:53m:49s remains)
INFO - root - 2017-12-08 05:55:31.647020: step 16690, loss = 2.28, batch loss = 2.23 (35.6 examples/sec; 0.225 sec/batch; 19h:43m:27s remains)
INFO - root - 2017-12-08 05:55:33.876680: step 16700, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.225 sec/batch; 19h:45m:30s remains)
2017-12-08 05:55:34.182701: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4290156 -4.4290166 -4.4290204 -4.4290209 -4.4290152 -4.4290071 -4.4289989 -4.4289975 -4.4289961 -4.42899 -4.4289794 -4.4289637 -4.4289355 -4.4289021 -4.4288831][-4.429008 -4.4290032 -4.4290056 -4.4290104 -4.4290113 -4.4290085 -4.4290023 -4.4290018 -4.4290004 -4.4289937 -4.4289818 -4.4289651 -4.4289384 -4.4289055 -4.428884][-4.4290051 -4.4289942 -4.4289923 -4.4289947 -4.4289966 -4.4289942 -4.4289894 -4.4289894 -4.4289885 -4.4289823 -4.4289708 -4.4289575 -4.4289393 -4.4289136 -4.4288888][-4.4290195 -4.4290032 -4.4289923 -4.4289875 -4.4289851 -4.4289813 -4.428977 -4.42898 -4.4289794 -4.4289722 -4.4289622 -4.4289513 -4.42894 -4.4289231 -4.4288988][-4.4290342 -4.4290109 -4.4289861 -4.4289689 -4.4289556 -4.4289393 -4.428926 -4.4289241 -4.4289265 -4.4289231 -4.4289193 -4.4289131 -4.4289131 -4.4289103 -4.42889][-4.429029 -4.4289904 -4.428946 -4.4289093 -4.4288759 -4.4288354 -4.428792 -4.4287677 -4.4287763 -4.4287925 -4.428803 -4.4288034 -4.4288192 -4.4288425 -4.4288406][-4.4289804 -4.4289136 -4.4288449 -4.4287863 -4.4287314 -4.4286513 -4.4285436 -4.4284644 -4.4284925 -4.4285574 -4.4286013 -4.4286256 -4.4286618 -4.4287128 -4.4287376][-4.4288983 -4.4288096 -4.4287295 -4.4286675 -4.428596 -4.4284625 -4.4282575 -4.4281049 -4.4281869 -4.4283328 -4.4284315 -4.4284921 -4.4285407 -4.4285994 -4.4286418][-4.4288383 -4.4287524 -4.4286842 -4.4286375 -4.4285741 -4.4284286 -4.4281888 -4.4280171 -4.42814 -4.42832 -4.42843 -4.4284916 -4.4285221 -4.4285502 -4.4285808][-4.4288054 -4.4287314 -4.4286795 -4.4286547 -4.4286261 -4.4285293 -4.4283509 -4.4282346 -4.4283323 -4.4284568 -4.4285254 -4.4285522 -4.428546 -4.4285345 -4.4285417][-4.4287887 -4.4287252 -4.4286857 -4.428678 -4.4286809 -4.4286361 -4.4285331 -4.4284682 -4.4285259 -4.4285917 -4.4286237 -4.4286275 -4.4286089 -4.4285884 -4.4285917][-4.4287977 -4.4287429 -4.4287157 -4.42872 -4.4287343 -4.4287181 -4.4286623 -4.428627 -4.4286613 -4.4287009 -4.4287167 -4.4287157 -4.4287062 -4.4286971 -4.428709][-4.4288464 -4.4288044 -4.4287896 -4.4287977 -4.4288125 -4.4288082 -4.4287853 -4.428772 -4.428793 -4.4288163 -4.4288235 -4.4288216 -4.4288206 -4.4288192 -4.4288316][-4.4289131 -4.4288874 -4.4288816 -4.4288888 -4.4289036 -4.42891 -4.4289079 -4.4289093 -4.4289246 -4.42894 -4.4289436 -4.42894 -4.428937 -4.4289341 -4.4289389][-4.4289703 -4.4289565 -4.4289556 -4.4289656 -4.4289832 -4.428997 -4.4290051 -4.4290109 -4.4290209 -4.4290304 -4.4290338 -4.4290314 -4.4290271 -4.4290228 -4.4290218]]...]
INFO - root - 2017-12-08 05:55:36.417631: step 16710, loss = 2.28, batch loss = 2.23 (35.0 examples/sec; 0.229 sec/batch; 20h:03m:30s remains)
INFO - root - 2017-12-08 05:55:38.659012: step 16720, loss = 2.28, batch loss = 2.23 (34.8 examples/sec; 0.230 sec/batch; 20h:08m:28s remains)
INFO - root - 2017-12-08 05:55:40.902372: step 16730, loss = 2.28, batch loss = 2.23 (36.6 examples/sec; 0.218 sec/batch; 19h:09m:05s remains)
INFO - root - 2017-12-08 05:55:43.137796: step 16740, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.221 sec/batch; 19h:21m:03s remains)
INFO - root - 2017-12-08 05:55:45.368682: step 16750, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 19h:12m:28s remains)
INFO - root - 2017-12-08 05:55:47.606327: step 16760, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 19h:32m:14s remains)
INFO - root - 2017-12-08 05:55:49.816664: step 16770, loss = 2.28, batch loss = 2.23 (36.7 examples/sec; 0.218 sec/batch; 19h:06m:27s remains)
INFO - root - 2017-12-08 05:55:52.052109: step 16780, loss = 2.28, batch loss = 2.23 (36.6 examples/sec; 0.218 sec/batch; 19h:09m:13s remains)
INFO - root - 2017-12-08 05:55:54.314708: step 16790, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 19h:31m:13s remains)
INFO - root - 2017-12-08 05:55:56.586372: step 16800, loss = 2.28, batch loss = 2.23 (34.6 examples/sec; 0.232 sec/batch; 20h:18m:17s remains)
2017-12-08 05:55:56.882868: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288831 -4.4288692 -4.4288521 -4.4288363 -4.4288087 -4.4287577 -4.4287028 -4.4286947 -4.4287376 -4.4287648 -4.4287839 -4.428803 -4.4288197 -4.4288335 -4.4288363][-4.4288731 -4.4288545 -4.4288378 -4.4288154 -4.4287658 -4.4286871 -4.4286208 -4.4286251 -4.4286971 -4.4287481 -4.4287767 -4.4287934 -4.4288039 -4.4288111 -4.42881][-4.4288449 -4.4288197 -4.4288077 -4.4287958 -4.4287405 -4.4286451 -4.4285736 -4.4285932 -4.4286895 -4.4287543 -4.4287949 -4.428813 -4.4288158 -4.4288063 -4.4287944][-4.4287992 -4.4287639 -4.428761 -4.4287677 -4.4287229 -4.4286232 -4.4285531 -4.4285884 -4.4286966 -4.42876 -4.4287949 -4.42881 -4.4288125 -4.428792 -4.4287729][-4.4287429 -4.4286981 -4.4287038 -4.4287205 -4.4286747 -4.4285612 -4.4284883 -4.4285469 -4.4286728 -4.4287314 -4.4287515 -4.4287577 -4.4287596 -4.4287415 -4.4287262][-4.4286785 -4.4286227 -4.4286327 -4.4286456 -4.4285755 -4.428411 -4.4282894 -4.4283686 -4.4285431 -4.4286337 -4.4286628 -4.4286747 -4.4286833 -4.4286709 -4.4286628][-4.4286361 -4.4285707 -4.4285769 -4.4285769 -4.4284692 -4.42822 -4.4280148 -4.4281182 -4.4283781 -4.428535 -4.4285979 -4.4286284 -4.4286513 -4.4286485 -4.4286437][-4.4286332 -4.4285755 -4.4285808 -4.4285674 -4.4284472 -4.4281788 -4.4279532 -4.428082 -4.4283757 -4.4285502 -4.4286103 -4.4286366 -4.4286685 -4.4286785 -4.4286833][-4.4286671 -4.4286332 -4.4286437 -4.4286284 -4.428535 -4.4283428 -4.4282107 -4.4283271 -4.42854 -4.4286633 -4.4286904 -4.4286942 -4.4287214 -4.4287415 -4.4287529][-4.4287024 -4.4286938 -4.4287152 -4.4287076 -4.4286375 -4.428515 -4.4284568 -4.428545 -4.4286842 -4.4287648 -4.428771 -4.4287663 -4.4287882 -4.4288025 -4.4288116][-4.4287419 -4.4287562 -4.428791 -4.4287958 -4.4287419 -4.4286523 -4.4286218 -4.4286795 -4.4287691 -4.4288263 -4.4288349 -4.428834 -4.4288487 -4.4288559 -4.4288611][-4.4287939 -4.42882 -4.4288616 -4.4288659 -4.4288173 -4.4287367 -4.4287095 -4.4287453 -4.4288077 -4.4288578 -4.4288769 -4.4288836 -4.4288945 -4.4289 -4.428906][-4.4288363 -4.4288673 -4.4289036 -4.4289002 -4.4288559 -4.42879 -4.4287682 -4.4287891 -4.428834 -4.4288845 -4.4289107 -4.4289184 -4.4289227 -4.4289231 -4.4289269][-4.4288459 -4.4288731 -4.4289021 -4.4289036 -4.4288712 -4.428823 -4.4288092 -4.4288225 -4.4288559 -4.4288988 -4.4289274 -4.4289384 -4.4289393 -4.4289346 -4.4289351][-4.42886 -4.4288745 -4.4288955 -4.4289007 -4.4288816 -4.4288511 -4.4288411 -4.4288526 -4.4288783 -4.4289093 -4.4289312 -4.4289441 -4.4289455 -4.4289403 -4.4289379]]...]
INFO - root - 2017-12-08 05:55:59.103709: step 16810, loss = 2.28, batch loss = 2.23 (35.3 examples/sec; 0.227 sec/batch; 19h:52m:09s remains)
INFO - root - 2017-12-08 05:56:01.326911: step 16820, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.226 sec/batch; 19h:46m:26s remains)
INFO - root - 2017-12-08 05:56:03.565125: step 16830, loss = 2.28, batch loss = 2.23 (35.2 examples/sec; 0.227 sec/batch; 19h:54m:48s remains)
INFO - root - 2017-12-08 05:56:05.822709: step 16840, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.222 sec/batch; 19h:27m:12s remains)
INFO - root - 2017-12-08 05:56:08.074594: step 16850, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 19h:32m:01s remains)
INFO - root - 2017-12-08 05:56:10.332764: step 16860, loss = 2.28, batch loss = 2.23 (34.9 examples/sec; 0.229 sec/batch; 20h:07m:18s remains)
INFO - root - 2017-12-08 05:56:12.572756: step 16870, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.220 sec/batch; 19h:19m:11s remains)
INFO - root - 2017-12-08 05:56:14.809804: step 16880, loss = 2.28, batch loss = 2.23 (36.6 examples/sec; 0.218 sec/batch; 19h:08m:14s remains)
INFO - root - 2017-12-08 05:56:17.060218: step 16890, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 19h:22m:34s remains)
INFO - root - 2017-12-08 05:56:19.304363: step 16900, loss = 2.28, batch loss = 2.23 (35.3 examples/sec; 0.226 sec/batch; 19h:51m:16s remains)
2017-12-08 05:56:19.592610: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.428741 -4.4287248 -4.4287353 -4.4287734 -4.4288068 -4.4288211 -4.4288244 -4.4288235 -4.4288244 -4.42883 -4.4288278 -4.4288087 -4.428791 -4.4287953 -4.428822][-4.4288664 -4.4288578 -4.4288645 -4.4288893 -4.4289093 -4.428916 -4.4289184 -4.4289184 -4.4289207 -4.4289241 -4.4289165 -4.4289002 -4.4288917 -4.4289026 -4.4289274][-4.4289494 -4.428946 -4.4289494 -4.4289632 -4.4289732 -4.4289732 -4.42897 -4.4289656 -4.42896 -4.4289541 -4.4289379 -4.4289227 -4.4289236 -4.4289412 -4.428968][-4.4289865 -4.4289842 -4.4289842 -4.4289889 -4.4289889 -4.428978 -4.4289665 -4.428957 -4.4289408 -4.4289236 -4.4288988 -4.4288797 -4.4288845 -4.4289069 -4.4289331][-4.4289889 -4.4289818 -4.4289761 -4.4289737 -4.4289641 -4.42894 -4.4289155 -4.428895 -4.4288664 -4.4288392 -4.428812 -4.4287958 -4.4288034 -4.428823 -4.4288464][-4.4289727 -4.428956 -4.4289446 -4.428936 -4.4289131 -4.4288707 -4.4288287 -4.4287944 -4.42876 -4.4287338 -4.4287148 -4.428709 -4.4287186 -4.4287238 -4.4287357][-4.4289484 -4.4289203 -4.4289031 -4.4288859 -4.4288492 -4.4287891 -4.4287271 -4.4286852 -4.4286633 -4.4286547 -4.4286504 -4.4286566 -4.4286633 -4.4286489 -4.4286442][-4.4289174 -4.4288778 -4.4288535 -4.4288292 -4.4287825 -4.4287148 -4.4286494 -4.4286132 -4.4286141 -4.42863 -4.4286413 -4.4286532 -4.4286528 -4.42863 -4.4286208][-4.4288836 -4.4288406 -4.4288173 -4.428792 -4.42874 -4.4286761 -4.4286246 -4.4286075 -4.4286337 -4.4286718 -4.4286914 -4.4286938 -4.4286857 -4.4286623 -4.4286618][-4.4288578 -4.4288297 -4.4288154 -4.4287925 -4.428741 -4.428689 -4.4286585 -4.4286652 -4.4287119 -4.4287643 -4.4287782 -4.428761 -4.4287395 -4.4287105 -4.4287167][-4.4288425 -4.428843 -4.4288468 -4.4288321 -4.4287887 -4.4287462 -4.428731 -4.4287539 -4.4288087 -4.4288588 -4.4288588 -4.4288225 -4.4287806 -4.4287391 -4.4287491][-4.4288287 -4.4288621 -4.4288893 -4.4288907 -4.4288654 -4.4288306 -4.428823 -4.42885 -4.4288974 -4.4289327 -4.428916 -4.428865 -4.4288011 -4.4287419 -4.42875][-4.4288239 -4.4288778 -4.4289217 -4.4289379 -4.4289308 -4.4289103 -4.4289117 -4.42894 -4.4289713 -4.4289856 -4.42896 -4.4288988 -4.4288144 -4.4287362 -4.4287381][-4.42884 -4.4288926 -4.4289374 -4.4289589 -4.4289646 -4.4289594 -4.4289708 -4.4290004 -4.4290195 -4.4290218 -4.428997 -4.4289346 -4.4288378 -4.4287491 -4.4287462][-4.4288716 -4.4289074 -4.4289389 -4.4289575 -4.4289689 -4.4289732 -4.4289904 -4.4290214 -4.4290371 -4.4290357 -4.4290166 -4.4289608 -4.4288664 -4.4287777 -4.4287729]]...]
INFO - root - 2017-12-08 05:56:21.904094: step 16910, loss = 2.28, batch loss = 2.23 (33.2 examples/sec; 0.241 sec/batch; 21h:08m:02s remains)
INFO - root - 2017-12-08 05:56:24.141116: step 16920, loss = 2.28, batch loss = 2.23 (34.9 examples/sec; 0.229 sec/batch; 20h:07m:03s remains)
INFO - root - 2017-12-08 05:56:26.369850: step 16930, loss = 2.28, batch loss = 2.23 (34.4 examples/sec; 0.232 sec/batch; 20h:22m:44s remains)
INFO - root - 2017-12-08 05:56:28.590311: step 16940, loss = 2.28, batch loss = 2.23 (38.1 examples/sec; 0.210 sec/batch; 18h:24m:39s remains)
INFO - root - 2017-12-08 05:56:30.827061: step 16950, loss = 2.28, batch loss = 2.23 (36.0 examples/sec; 0.222 sec/batch; 19h:27m:06s remains)
INFO - root - 2017-12-08 05:56:33.045497: step 16960, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 19h:32m:22s remains)
INFO - root - 2017-12-08 05:56:35.332660: step 16970, loss = 2.28, batch loss = 2.23 (35.4 examples/sec; 0.226 sec/batch; 19h:48m:39s remains)
INFO - root - 2017-12-08 05:56:37.560464: step 16980, loss = 2.28, batch loss = 2.23 (35.3 examples/sec; 0.226 sec/batch; 19h:50m:35s remains)
INFO - root - 2017-12-08 05:56:39.796055: step 16990, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 19h:12m:39s remains)
INFO - root - 2017-12-08 05:56:42.054470: step 17000, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.221 sec/batch; 19h:24m:13s remains)
2017-12-08 05:56:42.342577: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289083 -4.4289 -4.428894 -4.4288869 -4.428885 -4.4288874 -4.428895 -4.4288969 -4.4289041 -4.4289131 -4.4289179 -4.4289279 -4.42894 -4.4289465 -4.4289546][-4.4289212 -4.428906 -4.4288921 -4.4288754 -4.4288688 -4.4288707 -4.4288816 -4.4288874 -4.4289036 -4.4289212 -4.4289279 -4.4289331 -4.4289412 -4.4289417 -4.4289451][-4.4289217 -4.4288964 -4.4288683 -4.4288368 -4.4288192 -4.4288187 -4.4288335 -4.4288492 -4.4288788 -4.4289079 -4.42892 -4.4289241 -4.4289322 -4.4289312 -4.4289317][-4.4289017 -4.4288607 -4.4288111 -4.4287634 -4.4287386 -4.4287338 -4.4287505 -4.4287834 -4.4288368 -4.428875 -4.4288907 -4.4289007 -4.4289169 -4.4289231 -4.4289217][-4.4288445 -4.4287863 -4.4287162 -4.4286494 -4.4286113 -4.4285908 -4.4285946 -4.4286513 -4.4287496 -4.428812 -4.4288354 -4.4288564 -4.4288874 -4.4289036 -4.4289031][-4.4287705 -4.4286976 -4.428607 -4.4285107 -4.4284363 -4.428369 -4.4283156 -4.4283805 -4.428546 -4.4286666 -4.4287305 -4.4287848 -4.4288435 -4.4288716 -4.4288759][-4.428699 -4.4286232 -4.4285207 -4.4284005 -4.4282804 -4.4281373 -4.4279923 -4.4280434 -4.42827 -4.4284592 -4.42858 -4.4286737 -4.4287643 -4.4288149 -4.428843][-4.4286838 -4.4286256 -4.4285421 -4.4284258 -4.4282956 -4.4281192 -4.4279194 -4.4279251 -4.4281425 -4.4283504 -4.4284987 -4.4286122 -4.4287195 -4.4287839 -4.4288316][-4.4287381 -4.4286928 -4.4286394 -4.4285579 -4.4284606 -4.4283233 -4.4281597 -4.4281292 -4.428266 -4.4284258 -4.4285545 -4.4286561 -4.428751 -4.4288073 -4.4288545][-4.4287968 -4.428761 -4.4287314 -4.4286828 -4.4286246 -4.4285326 -4.4284124 -4.42837 -4.4284472 -4.4285645 -4.4286718 -4.4287539 -4.4288216 -4.4288573 -4.4288883][-4.4288588 -4.4288259 -4.4288044 -4.4287753 -4.4287376 -4.4286737 -4.4285865 -4.428545 -4.4285865 -4.4286704 -4.4287596 -4.4288273 -4.4288745 -4.4288988 -4.428915][-4.4289212 -4.4289017 -4.4288869 -4.4288688 -4.4288435 -4.4287949 -4.428731 -4.4286957 -4.4287152 -4.4287724 -4.4288416 -4.4288907 -4.4289212 -4.4289355 -4.4289436][-4.4289393 -4.4289308 -4.4289246 -4.4289169 -4.4289055 -4.4288774 -4.4288392 -4.4288182 -4.42883 -4.4288697 -4.4289179 -4.4289474 -4.4289618 -4.428967 -4.4289722][-4.4289169 -4.4289126 -4.4289112 -4.4289122 -4.4289165 -4.4289112 -4.4289007 -4.4288988 -4.4289112 -4.4289365 -4.4289656 -4.4289813 -4.428987 -4.4289894 -4.4289927][-4.4289165 -4.4289141 -4.4289122 -4.4289126 -4.4289188 -4.4289236 -4.4289274 -4.428936 -4.4289479 -4.4289646 -4.4289789 -4.4289865 -4.4289908 -4.4289947 -4.4289994]]...]
INFO - root - 2017-12-08 05:56:44.566820: step 17010, loss = 2.28, batch loss = 2.23 (37.3 examples/sec; 0.214 sec/batch; 18h:46m:20s remains)
INFO - root - 2017-12-08 05:56:46.806160: step 17020, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 19h:16m:13s remains)
INFO - root - 2017-12-08 05:56:49.042279: step 17030, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.225 sec/batch; 19h:45m:00s remains)
INFO - root - 2017-12-08 05:56:51.259192: step 17040, loss = 2.28, batch loss = 2.23 (36.6 examples/sec; 0.219 sec/batch; 19h:08m:54s remains)
INFO - root - 2017-12-08 05:56:53.492369: step 17050, loss = 2.28, batch loss = 2.23 (35.0 examples/sec; 0.228 sec/batch; 20h:00m:40s remains)
INFO - root - 2017-12-08 05:56:55.721549: step 17060, loss = 2.28, batch loss = 2.23 (34.5 examples/sec; 0.232 sec/batch; 20h:18m:12s remains)
INFO - root - 2017-12-08 05:56:57.953372: step 17070, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 19h:30m:32s remains)
INFO - root - 2017-12-08 05:57:00.188378: step 17080, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.220 sec/batch; 19h:17m:38s remains)
INFO - root - 2017-12-08 05:57:02.390640: step 17090, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 19h:15m:20s remains)
INFO - root - 2017-12-08 05:57:04.618209: step 17100, loss = 2.28, batch loss = 2.23 (35.4 examples/sec; 0.226 sec/batch; 19h:47m:03s remains)
2017-12-08 05:57:04.919501: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288249 -4.4288349 -4.4288387 -4.4288464 -4.428854 -4.4288607 -4.4288769 -4.428895 -4.4289002 -4.4288864 -4.428875 -4.428874 -4.4288859 -4.4288988 -4.4288988][-4.4287953 -4.428793 -4.4287777 -4.4287734 -4.4287777 -4.4287839 -4.4288125 -4.428853 -4.4288769 -4.4288769 -4.4288745 -4.4288712 -4.4288797 -4.4288936 -4.4288912][-4.4287739 -4.428762 -4.4287248 -4.4286904 -4.4286656 -4.4286523 -4.428689 -4.428762 -4.4288096 -4.4288306 -4.4288459 -4.4288487 -4.4288497 -4.4288559 -4.428853][-4.4287829 -4.4287696 -4.4287171 -4.4286437 -4.4285674 -4.4285111 -4.4285421 -4.4286408 -4.4287186 -4.4287672 -4.4287953 -4.4288006 -4.42879 -4.4287758 -4.4287672][-4.4287858 -4.4287753 -4.4287167 -4.4286036 -4.42846 -4.4283433 -4.4283643 -4.4285007 -4.4286151 -4.4286847 -4.4287195 -4.4287171 -4.4286914 -4.4286647 -4.4286695][-4.42877 -4.4287605 -4.4287019 -4.4285727 -4.4283843 -4.4282012 -4.42819 -4.4283681 -4.4285369 -4.4286375 -4.4286747 -4.4286647 -4.4286294 -4.4285994 -4.4286318][-4.4287462 -4.4287438 -4.4286895 -4.4285707 -4.4283867 -4.4281735 -4.4281106 -4.428297 -4.4285121 -4.4286366 -4.4286809 -4.4286737 -4.428638 -4.4286089 -4.4286509][-4.4287577 -4.4287586 -4.4287148 -4.4286294 -4.4284987 -4.4283185 -4.4282069 -4.4283404 -4.4285555 -4.4286809 -4.4287167 -4.4287019 -4.4286637 -4.4286304 -4.4286528][-4.4287829 -4.4287896 -4.4287643 -4.4287181 -4.4286504 -4.428544 -4.4284472 -4.4284987 -4.4286475 -4.4287486 -4.4287586 -4.4287233 -4.4286747 -4.4286332 -4.4286308][-4.4287944 -4.4287987 -4.4287891 -4.4287672 -4.4287391 -4.4286957 -4.4286366 -4.4286466 -4.4287348 -4.4287987 -4.4287887 -4.4287324 -4.428668 -4.4286189 -4.4285984][-4.4287753 -4.4287758 -4.4287724 -4.4287648 -4.4287629 -4.4287572 -4.4287291 -4.4287362 -4.4287896 -4.42883 -4.428823 -4.4287624 -4.4286895 -4.4286404 -4.428606][-4.4287529 -4.4287539 -4.4287553 -4.4287591 -4.4287772 -4.4287958 -4.4287863 -4.4287872 -4.4288125 -4.4288416 -4.4288478 -4.4288077 -4.4287524 -4.4287148 -4.4286823][-4.4287605 -4.4287786 -4.428792 -4.4287949 -4.4288125 -4.4288349 -4.4288354 -4.4288282 -4.4288378 -4.4288597 -4.42887 -4.4288511 -4.4288149 -4.4287848 -4.4287529][-4.4287839 -4.4288177 -4.4288449 -4.4288464 -4.4288545 -4.42887 -4.4288678 -4.4288568 -4.4288588 -4.4288735 -4.4288783 -4.428863 -4.4288416 -4.4288249 -4.4288039][-4.4288378 -4.428865 -4.4288859 -4.4288821 -4.428884 -4.428895 -4.428894 -4.4288869 -4.4288926 -4.4289012 -4.4289021 -4.4288893 -4.42888 -4.4288754 -4.4288654]]...]
INFO - root - 2017-12-08 05:57:07.156813: step 17110, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 19h:32m:32s remains)
INFO - root - 2017-12-08 05:57:09.415046: step 17120, loss = 2.28, batch loss = 2.23 (35.6 examples/sec; 0.225 sec/batch; 19h:42m:34s remains)
INFO - root - 2017-12-08 05:57:11.646562: step 17130, loss = 2.28, batch loss = 2.23 (36.0 examples/sec; 0.222 sec/batch; 19h:27m:51s remains)
INFO - root - 2017-12-08 05:57:13.883551: step 17140, loss = 2.28, batch loss = 2.23 (36.7 examples/sec; 0.218 sec/batch; 19h:07m:06s remains)
INFO - root - 2017-12-08 05:57:16.139219: step 17150, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.225 sec/batch; 19h:44m:19s remains)
INFO - root - 2017-12-08 05:57:18.374176: step 17160, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.226 sec/batch; 19h:45m:18s remains)
INFO - root - 2017-12-08 05:57:20.634504: step 17170, loss = 2.28, batch loss = 2.23 (35.4 examples/sec; 0.226 sec/batch; 19h:46m:22s remains)
INFO - root - 2017-12-08 05:57:22.880515: step 17180, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.221 sec/batch; 19h:23m:58s remains)
INFO - root - 2017-12-08 05:57:25.135780: step 17190, loss = 2.28, batch loss = 2.23 (35.1 examples/sec; 0.228 sec/batch; 19h:57m:15s remains)
INFO - root - 2017-12-08 05:57:27.430225: step 17200, loss = 2.28, batch loss = 2.23 (34.8 examples/sec; 0.230 sec/batch; 20h:06m:45s remains)
2017-12-08 05:57:27.742119: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4287786 -4.4287858 -4.4287868 -4.42878 -4.4287639 -4.4287419 -4.4287262 -4.42873 -4.4287553 -4.4287891 -4.4288135 -4.4288211 -4.4288273 -4.4288344 -4.4288411][-4.4287863 -4.4287925 -4.42879 -4.4287758 -4.4287467 -4.4287086 -4.4286776 -4.4286771 -4.4287143 -4.42877 -4.4288177 -4.4288406 -4.4288507 -4.4288549 -4.4288549][-4.4287868 -4.4287915 -4.4287877 -4.4287672 -4.4287252 -4.4286685 -4.428616 -4.4286022 -4.4286456 -4.4287252 -4.4288 -4.4288416 -4.42886 -4.4288654 -4.4288621][-4.4287858 -4.4287896 -4.4287863 -4.4287643 -4.4287138 -4.4286418 -4.4285669 -4.4285316 -4.4285712 -4.4286685 -4.42877 -4.4288321 -4.4288597 -4.4288683 -4.428865][-4.4287839 -4.4287868 -4.4287844 -4.4287634 -4.4287076 -4.4286242 -4.4285316 -4.428472 -4.4284987 -4.4286084 -4.4287343 -4.4288168 -4.4288554 -4.4288678 -4.4288654][-4.4287772 -4.4287796 -4.4287786 -4.4287586 -4.4287019 -4.4286165 -4.4285164 -4.4284387 -4.4284492 -4.4285641 -4.4287066 -4.4288049 -4.428853 -4.4288678 -4.4288659][-4.4287815 -4.428781 -4.4287763 -4.4287562 -4.4287028 -4.4286237 -4.4285274 -4.4284415 -4.4284334 -4.42854 -4.4286871 -4.4287977 -4.4288535 -4.4288697 -4.4288669][-4.428802 -4.4287958 -4.4287829 -4.4287581 -4.4287076 -4.4286361 -4.42855 -4.4284687 -4.4284477 -4.4285359 -4.4286747 -4.42879 -4.4288526 -4.4288716 -4.4288688][-4.4288249 -4.4288149 -4.4287949 -4.4287667 -4.428721 -4.4286571 -4.4285774 -4.4285049 -4.4284806 -4.4285469 -4.4286656 -4.4287767 -4.4288449 -4.4288697 -4.4288688][-4.4288363 -4.4288273 -4.4288058 -4.4287791 -4.4287424 -4.4286895 -4.4286151 -4.4285474 -4.42852 -4.4285626 -4.4286528 -4.4287496 -4.4288216 -4.4288578 -4.428864][-4.4288316 -4.4288268 -4.42881 -4.428792 -4.4287696 -4.4287324 -4.428668 -4.4286013 -4.4285641 -4.42858 -4.42864 -4.428719 -4.4287915 -4.4288383 -4.4288545][-4.428812 -4.428812 -4.4288034 -4.4287982 -4.4287925 -4.4287753 -4.4287281 -4.4286647 -4.428616 -4.4286065 -4.4286394 -4.4286985 -4.4287648 -4.4288177 -4.428843][-4.4287806 -4.4287853 -4.4287858 -4.4287915 -4.4287996 -4.4288011 -4.4287758 -4.4287257 -4.4286752 -4.4286513 -4.4286642 -4.4287028 -4.4287562 -4.4288054 -4.4288354][-4.4287524 -4.4287624 -4.4287691 -4.42878 -4.4287949 -4.4288092 -4.4288054 -4.4287744 -4.4287343 -4.42871 -4.4287114 -4.4287324 -4.4287691 -4.4288073 -4.4288335][-4.42875 -4.428761 -4.4287663 -4.4287744 -4.4287868 -4.4288034 -4.4288135 -4.4288015 -4.4287786 -4.4287629 -4.428762 -4.4287729 -4.4287944 -4.42882 -4.4288383]]...]
INFO - root - 2017-12-08 05:57:29.975564: step 17210, loss = 2.28, batch loss = 2.23 (34.7 examples/sec; 0.230 sec/batch; 20h:10m:34s remains)
INFO - root - 2017-12-08 05:57:32.204823: step 17220, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 19h:19m:53s remains)
INFO - root - 2017-12-08 05:57:34.439758: step 17230, loss = 2.28, batch loss = 2.23 (34.8 examples/sec; 0.230 sec/batch; 20h:07m:25s remains)
INFO - root - 2017-12-08 05:57:36.660080: step 17240, loss = 2.28, batch loss = 2.23 (37.2 examples/sec; 0.215 sec/batch; 18h:51m:17s remains)
INFO - root - 2017-12-08 05:57:38.893954: step 17250, loss = 2.28, batch loss = 2.23 (36.8 examples/sec; 0.217 sec/batch; 19h:01m:11s remains)
INFO - root - 2017-12-08 05:57:41.132063: step 17260, loss = 2.28, batch loss = 2.23 (35.6 examples/sec; 0.225 sec/batch; 19h:40m:08s remains)
INFO - root - 2017-12-08 05:57:43.408322: step 17270, loss = 2.28, batch loss = 2.23 (35.0 examples/sec; 0.229 sec/batch; 20h:02m:29s remains)
INFO - root - 2017-12-08 05:57:45.643224: step 17280, loss = 2.28, batch loss = 2.23 (34.9 examples/sec; 0.229 sec/batch; 20h:03m:36s remains)
INFO - root - 2017-12-08 05:57:47.871003: step 17290, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 19h:15m:16s remains)
INFO - root - 2017-12-08 05:57:50.118024: step 17300, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.220 sec/batch; 19h:18m:14s remains)
2017-12-08 05:57:50.382691: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288263 -4.4288125 -4.4287996 -4.4287825 -4.4287615 -4.4287634 -4.4287891 -4.4288182 -4.42884 -4.4288483 -4.4288449 -4.428834 -4.4288263 -4.4288235 -4.4288077][-4.4288516 -4.4288278 -4.4288077 -4.42879 -4.42877 -4.4287729 -4.4287992 -4.428823 -4.4288344 -4.4288282 -4.4288106 -4.4287863 -4.4287658 -4.4287534 -4.4287319][-4.428864 -4.4288392 -4.4288125 -4.4287915 -4.4287663 -4.4287567 -4.4287758 -4.4288 -4.4288106 -4.4287992 -4.4287753 -4.4287453 -4.4287176 -4.4287009 -4.428679][-4.4288545 -4.4288306 -4.4288006 -4.42877 -4.4287372 -4.4287162 -4.4287214 -4.4287443 -4.4287615 -4.4287572 -4.4287381 -4.4287095 -4.4286828 -4.4286661 -4.4286489][-4.4288154 -4.42879 -4.4287572 -4.4287219 -4.4286919 -4.4286675 -4.4286628 -4.4286852 -4.4287062 -4.4287128 -4.4287095 -4.4286962 -4.42868 -4.4286695 -4.428658][-4.4287639 -4.4287276 -4.428679 -4.4286251 -4.4285946 -4.4285731 -4.4285688 -4.4285936 -4.4286289 -4.4286547 -4.4286747 -4.4286866 -4.4286909 -4.4286923 -4.4286923][-4.4287343 -4.4286842 -4.4286103 -4.4285254 -4.4284739 -4.4284453 -4.4284415 -4.4284697 -4.42852 -4.4285731 -4.4286251 -4.4286671 -4.4286985 -4.4287176 -4.4287324][-4.4287267 -4.4286613 -4.42857 -4.4284625 -4.4283919 -4.4283586 -4.42835 -4.4283686 -4.428421 -4.4285 -4.4285889 -4.4286642 -4.4287243 -4.4287648 -4.4287934][-4.4287286 -4.4286609 -4.4285755 -4.4284763 -4.4284158 -4.4283972 -4.4283953 -4.4283996 -4.428432 -4.4285016 -4.4285965 -4.4286838 -4.4287577 -4.4288073 -4.4288416][-4.4287333 -4.4286866 -4.4286351 -4.4285722 -4.42854 -4.4285431 -4.4285502 -4.4285474 -4.4285545 -4.4285827 -4.4286432 -4.4287119 -4.4287782 -4.4288268 -4.42886][-4.4287372 -4.4287181 -4.4286971 -4.4286685 -4.4286604 -4.4286757 -4.4286819 -4.428679 -4.4286747 -4.428669 -4.42869 -4.4287257 -4.4287682 -4.4288049 -4.4288349][-4.4287171 -4.428719 -4.4287186 -4.4287114 -4.4287167 -4.4287386 -4.42875 -4.4287586 -4.4287562 -4.4287362 -4.4287271 -4.42873 -4.428741 -4.4287553 -4.428772][-4.4286523 -4.4286737 -4.4287028 -4.4287152 -4.4287262 -4.4287486 -4.4287705 -4.428793 -4.4287972 -4.4287782 -4.4287553 -4.4287343 -4.42872 -4.4287095 -4.4287076][-4.4285679 -4.4286051 -4.4286537 -4.4286757 -4.4286923 -4.4287167 -4.4287457 -4.4287758 -4.4287882 -4.4287777 -4.4287539 -4.4287257 -4.4286995 -4.4286728 -4.4286542][-4.4284854 -4.4285231 -4.428576 -4.4286089 -4.4286389 -4.4286757 -4.4287143 -4.4287477 -4.428761 -4.4287505 -4.4287243 -4.4286952 -4.428669 -4.4286442 -4.4286251]]...]
INFO - root - 2017-12-08 05:57:52.616189: step 17310, loss = 2.28, batch loss = 2.23 (35.3 examples/sec; 0.226 sec/batch; 19h:49m:19s remains)
INFO - root - 2017-12-08 05:57:54.865517: step 17320, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.220 sec/batch; 19h:16m:28s remains)
INFO - root - 2017-12-08 05:57:57.081222: step 17330, loss = 2.28, batch loss = 2.23 (38.0 examples/sec; 0.211 sec/batch; 18h:26m:55s remains)
INFO - root - 2017-12-08 05:57:59.308808: step 17340, loss = 2.28, batch loss = 2.23 (37.1 examples/sec; 0.216 sec/batch; 18h:53m:27s remains)
INFO - root - 2017-12-08 05:58:01.539368: step 17350, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.220 sec/batch; 19h:17m:38s remains)
INFO - root - 2017-12-08 05:58:03.768443: step 17360, loss = 2.28, batch loss = 2.23 (36.6 examples/sec; 0.218 sec/batch; 19h:06m:42s remains)
INFO - root - 2017-12-08 05:58:06.008027: step 17370, loss = 2.28, batch loss = 2.23 (35.4 examples/sec; 0.226 sec/batch; 19h:47m:45s remains)
INFO - root - 2017-12-08 05:58:08.283884: step 17380, loss = 2.28, batch loss = 2.23 (33.6 examples/sec; 0.238 sec/batch; 20h:49m:14s remains)
INFO - root - 2017-12-08 05:58:10.523947: step 17390, loss = 2.28, batch loss = 2.23 (35.4 examples/sec; 0.226 sec/batch; 19h:47m:26s remains)
INFO - root - 2017-12-08 05:58:12.799952: step 17400, loss = 2.28, batch loss = 2.23 (33.2 examples/sec; 0.241 sec/batch; 21h:07m:14s remains)
2017-12-08 05:58:13.116307: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288034 -4.4286857 -4.4285979 -4.42858 -4.4286513 -4.4287395 -4.4288039 -4.4288454 -4.4288678 -4.4288683 -4.4288459 -4.4288177 -4.428782 -4.4287457 -4.4287305][-4.4287777 -4.4286432 -4.4285331 -4.4284964 -4.4285607 -4.4286647 -4.4287496 -4.4288092 -4.4288454 -4.4288573 -4.4288435 -4.4288163 -4.4287772 -4.4287286 -4.4287047][-4.4287543 -4.4286041 -4.4284797 -4.4284225 -4.428463 -4.4285774 -4.428688 -4.4287672 -4.42881 -4.4288249 -4.4288149 -4.4287863 -4.4287419 -4.4286814 -4.4286489][-4.4287295 -4.428566 -4.4284344 -4.4283648 -4.428391 -4.4285188 -4.4286542 -4.4287438 -4.4287939 -4.4288139 -4.4288068 -4.4287682 -4.4287133 -4.4286413 -4.428596][-4.4287038 -4.42853 -4.4283891 -4.4283056 -4.4283228 -4.42846 -4.4286127 -4.4286995 -4.4287539 -4.4287777 -4.4287648 -4.4287148 -4.428659 -4.4285984 -4.4285555][-4.4286823 -4.4285026 -4.4283485 -4.4282451 -4.4282465 -4.4283857 -4.4285493 -4.4286284 -4.4286733 -4.4286933 -4.4286771 -4.4286227 -4.428586 -4.42856 -4.4285431][-4.4286561 -4.4284663 -4.4282951 -4.4281621 -4.428134 -4.4282608 -4.4284177 -4.4284849 -4.4285164 -4.428525 -4.4285054 -4.4284639 -4.4284682 -4.4284949 -4.4285226][-4.4286265 -4.4284191 -4.428225 -4.428061 -4.4280005 -4.4281082 -4.4282484 -4.4283056 -4.4283304 -4.4283466 -4.4283333 -4.4283166 -4.4283648 -4.4284348 -4.4284949][-4.4286146 -4.4283991 -4.4281926 -4.4280167 -4.4279442 -4.4280372 -4.4281583 -4.428206 -4.428237 -4.4282761 -4.4282928 -4.4283104 -4.4283776 -4.4284468 -4.4284968][-4.4286461 -4.4284477 -4.4282565 -4.4281 -4.4280419 -4.4281168 -4.4282088 -4.4282455 -4.4282756 -4.4283242 -4.4283614 -4.4283953 -4.428452 -4.4284868 -4.4285045][-4.4287233 -4.4285655 -4.4284105 -4.4282861 -4.4282408 -4.4282913 -4.4283547 -4.4283834 -4.4284039 -4.428441 -4.4284797 -4.4285097 -4.4285417 -4.4285421 -4.4285274][-4.4288154 -4.4287081 -4.4286022 -4.4285197 -4.4284863 -4.4285126 -4.42855 -4.4285717 -4.428586 -4.4286127 -4.4286432 -4.4286575 -4.4286628 -4.4286461 -4.428617][-4.4288878 -4.4288216 -4.4287567 -4.4287066 -4.4286838 -4.4286962 -4.4287176 -4.4287348 -4.4287481 -4.4287663 -4.4287858 -4.428791 -4.4287872 -4.4287691 -4.4287415][-4.4289308 -4.4288898 -4.4288478 -4.4288177 -4.4288039 -4.42881 -4.4288235 -4.4288349 -4.4288459 -4.4288573 -4.4288712 -4.4288764 -4.4288716 -4.4288592 -4.42884][-4.4289541 -4.4289279 -4.4289 -4.4288812 -4.4288731 -4.4288764 -4.428884 -4.42889 -4.4288974 -4.428906 -4.4289145 -4.4289184 -4.4289155 -4.4289064 -4.4288936]]...]
INFO - root - 2017-12-08 05:58:15.328356: step 17410, loss = 2.28, batch loss = 2.23 (36.9 examples/sec; 0.217 sec/batch; 18h:59m:14s remains)
INFO - root - 2017-12-08 05:58:17.556712: step 17420, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 19h:10m:43s remains)
INFO - root - 2017-12-08 05:58:19.805983: step 17430, loss = 2.28, batch loss = 2.23 (33.2 examples/sec; 0.241 sec/batch; 21h:03m:38s remains)
INFO - root - 2017-12-08 05:58:22.064634: step 17440, loss = 2.28, batch loss = 2.23 (35.0 examples/sec; 0.229 sec/batch; 20h:00m:15s remains)
INFO - root - 2017-12-08 05:58:24.307542: step 17450, loss = 2.28, batch loss = 2.23 (34.3 examples/sec; 0.233 sec/batch; 20h:23m:48s remains)
INFO - root - 2017-12-08 05:58:26.529819: step 17460, loss = 2.28, batch loss = 2.23 (35.7 examples/sec; 0.224 sec/batch; 19h:37m:09s remains)
INFO - root - 2017-12-08 05:58:28.764006: step 17470, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.220 sec/batch; 19h:16m:51s remains)
INFO - root - 2017-12-08 05:58:30.995764: step 17480, loss = 2.28, batch loss = 2.23 (36.0 examples/sec; 0.222 sec/batch; 19h:26m:24s remains)
INFO - root - 2017-12-08 05:58:33.231591: step 17490, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 19h:29m:01s remains)
INFO - root - 2017-12-08 05:58:35.456292: step 17500, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 19h:12m:27s remains)
2017-12-08 05:58:35.744930: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4287224 -4.4287367 -4.4287477 -4.428771 -4.4287906 -4.4287896 -4.4287825 -4.4287868 -4.4287906 -4.428791 -4.42879 -4.4287934 -4.4287815 -4.4287586 -4.4287462][-4.4286628 -4.4286971 -4.42872 -4.4287486 -4.4287605 -4.42874 -4.4287148 -4.4287095 -4.4287119 -4.4287171 -4.4287219 -4.4287248 -4.4287057 -4.4286876 -4.4286919][-4.4286218 -4.4286785 -4.4287224 -4.4287572 -4.4287529 -4.4287024 -4.4286456 -4.4286208 -4.4286251 -4.4286447 -4.428659 -4.4286485 -4.4286194 -4.4286013 -4.428618][-4.4285622 -4.428647 -4.4287262 -4.4287653 -4.4287353 -4.4286475 -4.4285531 -4.4285049 -4.4285221 -4.4285765 -4.4286084 -4.4285879 -4.428544 -4.428514 -4.4285321][-4.4285264 -4.4286175 -4.4287186 -4.4287581 -4.4287009 -4.4285645 -4.428412 -4.4283328 -4.4283924 -4.4285216 -4.4285913 -4.4285755 -4.4285197 -4.4284692 -4.42848][-4.4285269 -4.4285994 -4.4286933 -4.4287214 -4.4286351 -4.4284387 -4.42819 -4.4280696 -4.4282174 -4.4284487 -4.4285746 -4.4285874 -4.4285307 -4.4284554 -4.4284525][-4.4285765 -4.4286284 -4.4286966 -4.4287004 -4.4285812 -4.4283056 -4.4279308 -4.4277754 -4.4280558 -4.4283895 -4.4285679 -4.4286132 -4.4285617 -4.4284635 -4.4284463][-4.4286456 -4.4286847 -4.4287167 -4.4286919 -4.4285388 -4.4281945 -4.4277315 -4.4275908 -4.4279933 -4.4283762 -4.4285727 -4.428627 -4.4285793 -4.4284754 -4.4284587][-4.42867 -4.4287143 -4.4287233 -4.4286847 -4.4285359 -4.4282155 -4.4278154 -4.4277549 -4.4281168 -4.4284382 -4.4286089 -4.4286494 -4.428587 -4.4284835 -4.4284763][-4.4286652 -4.4287214 -4.4287314 -4.4286928 -4.4285703 -4.4283428 -4.4281077 -4.4281211 -4.4283485 -4.4285555 -4.4286728 -4.4286866 -4.4286056 -4.4285064 -4.4284997][-4.4286413 -4.4287033 -4.4287152 -4.4286785 -4.4285908 -4.4284534 -4.428349 -4.4283977 -4.4285321 -4.4286556 -4.4287243 -4.4287095 -4.4286165 -4.4285221 -4.4285145][-4.428606 -4.4286747 -4.4286819 -4.4286413 -4.4285851 -4.4285212 -4.4285069 -4.4285707 -4.4286504 -4.4287119 -4.4287343 -4.4286971 -4.4286022 -4.4285169 -4.428514][-4.4285893 -4.4286613 -4.4286656 -4.428628 -4.428597 -4.4285874 -4.4286222 -4.4286876 -4.4287248 -4.4287319 -4.4287176 -4.428678 -4.4286017 -4.4285297 -4.4285312][-4.4286242 -4.4286947 -4.4286928 -4.4286647 -4.4286489 -4.4286733 -4.4287229 -4.4287696 -4.4287691 -4.4287381 -4.4287076 -4.4286809 -4.4286275 -4.4285703 -4.4285841][-4.4287024 -4.4287691 -4.4287744 -4.4287572 -4.4287505 -4.4287796 -4.4288187 -4.4288416 -4.4288163 -4.4287663 -4.4287343 -4.4287219 -4.4286904 -4.4286561 -4.4286866]]...]
INFO - root - 2017-12-08 05:58:37.959844: step 17510, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.220 sec/batch; 19h:15m:38s remains)
INFO - root - 2017-12-08 05:58:40.220791: step 17520, loss = 2.28, batch loss = 2.23 (35.2 examples/sec; 0.228 sec/batch; 19h:54m:34s remains)
INFO - root - 2017-12-08 05:58:42.447468: step 17530, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.222 sec/batch; 19h:24m:28s remains)
INFO - root - 2017-12-08 05:58:44.677102: step 17540, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.225 sec/batch; 19h:42m:36s remains)
INFO - root - 2017-12-08 05:58:46.940623: step 17550, loss = 2.28, batch loss = 2.23 (33.9 examples/sec; 0.236 sec/batch; 20h:37m:34s remains)
INFO - root - 2017-12-08 05:58:49.210710: step 17560, loss = 2.28, batch loss = 2.23 (35.3 examples/sec; 0.227 sec/batch; 19h:48m:54s remains)
INFO - root - 2017-12-08 05:58:51.422462: step 17570, loss = 2.28, batch loss = 2.23 (36.8 examples/sec; 0.217 sec/batch; 19h:01m:29s remains)
INFO - root - 2017-12-08 05:58:53.665209: step 17580, loss = 2.28, batch loss = 2.23 (35.7 examples/sec; 0.224 sec/batch; 19h:34m:54s remains)
INFO - root - 2017-12-08 05:58:55.886764: step 17590, loss = 2.28, batch loss = 2.23 (36.6 examples/sec; 0.219 sec/batch; 19h:07m:29s remains)
INFO - root - 2017-12-08 05:58:58.123938: step 17600, loss = 2.28, batch loss = 2.23 (35.6 examples/sec; 0.225 sec/batch; 19h:39m:26s remains)
2017-12-08 05:58:58.411403: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288092 -4.4288292 -4.4288459 -4.4288559 -4.4288626 -4.4288659 -4.4288688 -4.4288754 -4.4288807 -4.4288831 -4.4288874 -4.428895 -4.4288969 -4.4288917 -4.4288888][-4.4287791 -4.42881 -4.4288363 -4.42885 -4.4288549 -4.4288583 -4.428863 -4.4288716 -4.4288783 -4.4288797 -4.4288812 -4.4288812 -4.4288807 -4.4288797 -4.4288816][-4.428803 -4.4288344 -4.4288568 -4.428863 -4.428863 -4.4288659 -4.4288712 -4.42888 -4.4288898 -4.4288926 -4.4288917 -4.428884 -4.4288783 -4.4288783 -4.4288816][-4.4288526 -4.4288716 -4.4288759 -4.4288673 -4.4288573 -4.4288473 -4.4288425 -4.4288459 -4.4288611 -4.4288764 -4.4288788 -4.4288673 -4.4288592 -4.4288616 -4.4288635][-4.4288917 -4.4288878 -4.42887 -4.428843 -4.4288187 -4.4287882 -4.4287586 -4.4287567 -4.4287872 -4.4288225 -4.4288359 -4.4288225 -4.428813 -4.428813 -4.4288082][-4.428863 -4.4288278 -4.42878 -4.4287262 -4.4286733 -4.4286027 -4.4285326 -4.4285398 -4.42861 -4.4286857 -4.4287267 -4.4287229 -4.4287124 -4.4287086 -4.42869][-4.4287624 -4.4286904 -4.4286051 -4.4285054 -4.4283972 -4.4282689 -4.4281592 -4.4281993 -4.4283414 -4.4284759 -4.4285583 -4.4285688 -4.4285603 -4.4285593 -4.4285297][-4.4286413 -4.4285431 -4.4284339 -4.4283047 -4.4281611 -4.42801 -4.4278975 -4.4279814 -4.4281688 -4.42832 -4.428412 -4.4284306 -4.4284325 -4.4284468 -4.4284291][-4.4285836 -4.4284987 -4.4284139 -4.4283214 -4.4282284 -4.4281478 -4.4281011 -4.428175 -4.4283071 -4.4283996 -4.4284549 -4.4284592 -4.4284573 -4.4284744 -4.4284706][-4.4286132 -4.4285665 -4.4285269 -4.42849 -4.4284606 -4.4284444 -4.4284372 -4.428484 -4.4285541 -4.4285975 -4.4286189 -4.4286113 -4.4286056 -4.4286146 -4.4286137][-4.4287138 -4.4286995 -4.4286928 -4.4286914 -4.4286942 -4.4287038 -4.4287066 -4.4287281 -4.428761 -4.42878 -4.4287896 -4.4287834 -4.4287782 -4.4287758 -4.4287696][-4.4288106 -4.4288082 -4.4288158 -4.4288282 -4.42884 -4.4288526 -4.4288545 -4.4288611 -4.4288726 -4.4288816 -4.4288893 -4.4288874 -4.4288831 -4.4288726 -4.428863][-4.428864 -4.4288635 -4.4288726 -4.4288855 -4.428894 -4.4289002 -4.4289007 -4.4289026 -4.428906 -4.4289103 -4.4289145 -4.4289122 -4.4289069 -4.4288979 -4.4288907][-4.4288921 -4.428895 -4.4289031 -4.4289103 -4.4289145 -4.4289155 -4.4289126 -4.4289103 -4.4289112 -4.4289131 -4.4289136 -4.4289112 -4.4289088 -4.4289064 -4.4289041][-4.4288516 -4.428853 -4.4288559 -4.4288578 -4.4288564 -4.4288549 -4.4288526 -4.4288521 -4.4288545 -4.4288573 -4.4288583 -4.4288578 -4.4288568 -4.4288568 -4.4288573]]...]
INFO - root - 2017-12-08 05:59:00.659879: step 17610, loss = 2.28, batch loss = 2.23 (35.7 examples/sec; 0.224 sec/batch; 19h:34m:44s remains)
INFO - root - 2017-12-08 05:59:02.902683: step 17620, loss = 2.28, batch loss = 2.23 (35.1 examples/sec; 0.228 sec/batch; 19h:55m:55s remains)
INFO - root - 2017-12-08 05:59:05.131345: step 17630, loss = 2.28, batch loss = 2.23 (34.4 examples/sec; 0.233 sec/batch; 20h:21m:00s remains)
INFO - root - 2017-12-08 05:59:07.362513: step 17640, loss = 2.28, batch loss = 2.23 (35.7 examples/sec; 0.224 sec/batch; 19h:37m:01s remains)
INFO - root - 2017-12-08 05:59:09.581640: step 17650, loss = 2.28, batch loss = 2.23 (38.1 examples/sec; 0.210 sec/batch; 18h:20m:25s remains)
INFO - root - 2017-12-08 05:59:11.814009: step 17660, loss = 2.28, batch loss = 2.23 (36.8 examples/sec; 0.217 sec/batch; 18h:59m:15s remains)
INFO - root - 2017-12-08 05:59:14.057799: step 17670, loss = 2.28, batch loss = 2.23 (36.8 examples/sec; 0.217 sec/batch; 18h:59m:41s remains)
INFO - root - 2017-12-08 05:59:16.283868: step 17680, loss = 2.28, batch loss = 2.23 (36.0 examples/sec; 0.222 sec/batch; 19h:25m:33s remains)
INFO - root - 2017-12-08 05:59:18.526274: step 17690, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 19h:09m:25s remains)
INFO - root - 2017-12-08 05:59:20.765594: step 17700, loss = 2.28, batch loss = 2.23 (33.7 examples/sec; 0.237 sec/batch; 20h:43m:58s remains)
2017-12-08 05:59:21.048937: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4287195 -4.4287238 -4.4287481 -4.4287667 -4.4287825 -4.4288163 -4.4288354 -4.4288383 -4.4288096 -4.4287505 -4.4286714 -4.4286141 -4.4286232 -4.4286656 -4.4286757][-4.4287195 -4.4287343 -4.4287591 -4.4287796 -4.4287925 -4.42881 -4.4288249 -4.4288254 -4.428792 -4.4287252 -4.4286423 -4.4285855 -4.4286008 -4.4286475 -4.4286594][-4.4287381 -4.4287548 -4.42877 -4.4287882 -4.4287992 -4.4287977 -4.4287972 -4.4287982 -4.4287734 -4.4287095 -4.4286246 -4.428566 -4.4285793 -4.4286323 -4.4286604][-4.42875 -4.4287591 -4.428762 -4.4287758 -4.4287891 -4.4287748 -4.4287634 -4.4287663 -4.4287558 -4.4287019 -4.4286318 -4.4285808 -4.42859 -4.4286408 -4.4286733][-4.428762 -4.4287629 -4.4287562 -4.42876 -4.428762 -4.4287481 -4.42874 -4.4287491 -4.4287448 -4.4286957 -4.4286432 -4.4286079 -4.42861 -4.4286385 -4.4286551][-4.4287796 -4.4287786 -4.4287591 -4.4287357 -4.4287119 -4.4286995 -4.4287047 -4.428719 -4.4287128 -4.42866 -4.4286056 -4.4285793 -4.4285846 -4.4286032 -4.4286103][-4.42877 -4.42876 -4.4287243 -4.4286761 -4.428618 -4.4285994 -4.4286151 -4.4286418 -4.4286346 -4.428576 -4.4285164 -4.4284983 -4.4285212 -4.4285483 -4.4285564][-4.4287324 -4.4287057 -4.4286466 -4.4285612 -4.4284544 -4.4284263 -4.4284625 -4.428515 -4.4285097 -4.4284339 -4.4283738 -4.4283781 -4.4284396 -4.4284878 -4.4284964][-4.4286819 -4.4286408 -4.4285603 -4.4284282 -4.4282718 -4.4282479 -4.4283209 -4.4284043 -4.4284062 -4.4283261 -4.4282784 -4.4283142 -4.4284115 -4.4284778 -4.4284878][-4.4286618 -4.4286337 -4.428565 -4.4284372 -4.4282856 -4.428268 -4.4283533 -4.4284482 -4.4284711 -4.4284286 -4.4283972 -4.4284258 -4.4285111 -4.4285722 -4.4285879][-4.4287109 -4.4287062 -4.4286671 -4.4285817 -4.4284806 -4.4284682 -4.4285312 -4.428607 -4.4286366 -4.42862 -4.428596 -4.4286041 -4.4286566 -4.4286995 -4.4287167][-4.4287319 -4.4287372 -4.4287171 -4.4286628 -4.4286075 -4.4286132 -4.4286723 -4.4287367 -4.42876 -4.4287472 -4.4287291 -4.4287257 -4.4287534 -4.4287815 -4.4287972][-4.428647 -4.4286551 -4.42864 -4.4286156 -4.4286141 -4.4286623 -4.42874 -4.4288068 -4.4288268 -4.4288135 -4.4287982 -4.4287934 -4.4288092 -4.4288244 -4.4288321][-4.4284983 -4.4285064 -4.4284925 -4.4284992 -4.4285545 -4.4286523 -4.4287539 -4.4288249 -4.4288468 -4.4288378 -4.4288292 -4.4288263 -4.4288306 -4.428833 -4.4288378][-4.4284 -4.428421 -4.4284258 -4.4284573 -4.4285364 -4.428648 -4.4287539 -4.4288254 -4.428854 -4.4288559 -4.4288535 -4.4288516 -4.4288464 -4.4288373 -4.4288392]]...]
INFO - root - 2017-12-08 05:59:23.289302: step 17710, loss = 2.28, batch loss = 2.23 (34.5 examples/sec; 0.232 sec/batch; 20h:15m:56s remains)
INFO - root - 2017-12-08 05:59:25.519633: step 17720, loss = 2.28, batch loss = 2.23 (37.0 examples/sec; 0.216 sec/batch; 18h:54m:20s remains)
INFO - root - 2017-12-08 05:59:27.766639: step 17730, loss = 2.28, batch loss = 2.23 (35.8 examples/sec; 0.223 sec/batch; 19h:31m:02s remains)
INFO - root - 2017-12-08 05:59:30.002489: step 17740, loss = 2.28, batch loss = 2.23 (35.1 examples/sec; 0.228 sec/batch; 19h:54m:27s remains)
INFO - root - 2017-12-08 05:59:32.252771: step 17750, loss = 2.28, batch loss = 2.23 (36.6 examples/sec; 0.219 sec/batch; 19h:07m:18s remains)
INFO - root - 2017-12-08 05:59:34.473208: step 17760, loss = 2.28, batch loss = 2.23 (36.8 examples/sec; 0.217 sec/batch; 18h:59m:10s remains)
INFO - root - 2017-12-08 05:59:36.744583: step 17770, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 19h:28m:20s remains)
INFO - root - 2017-12-08 05:59:39.004521: step 17780, loss = 2.28, batch loss = 2.23 (36.7 examples/sec; 0.218 sec/batch; 19h:03m:37s remains)
INFO - root - 2017-12-08 05:59:41.249885: step 17790, loss = 2.28, batch loss = 2.23 (37.0 examples/sec; 0.216 sec/batch; 18h:54m:41s remains)
INFO - root - 2017-12-08 05:59:43.493197: step 17800, loss = 2.28, batch loss = 2.23 (37.1 examples/sec; 0.215 sec/batch; 18h:50m:04s remains)
2017-12-08 05:59:43.790445: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289026 -4.4288464 -4.4287949 -4.4287686 -4.428772 -4.4287887 -4.4288077 -4.4288273 -4.428844 -4.4288554 -4.4288683 -4.4288812 -4.4288821 -4.4288745 -4.4288688][-4.4288859 -4.4288187 -4.4287672 -4.4287558 -4.4287734 -4.428792 -4.4288049 -4.4288154 -4.4288259 -4.4288344 -4.428844 -4.4288497 -4.4288397 -4.428822 -4.4288154][-4.4288721 -4.4288049 -4.4287663 -4.428772 -4.4287939 -4.4288025 -4.4287977 -4.428792 -4.428791 -4.4287958 -4.428803 -4.428802 -4.4287848 -4.4287677 -4.42877][-4.4288683 -4.4288125 -4.4287925 -4.4288054 -4.4288168 -4.4288039 -4.428772 -4.4287405 -4.4287271 -4.4287338 -4.4287491 -4.4287505 -4.4287381 -4.4287343 -4.4287534][-4.42888 -4.4288392 -4.4288311 -4.4288387 -4.42883 -4.4287887 -4.4287219 -4.4286604 -4.4286389 -4.4286623 -4.4286971 -4.4287152 -4.4287219 -4.4287381 -4.428772][-4.4288945 -4.4288654 -4.4288592 -4.4288564 -4.4288249 -4.4287519 -4.4286489 -4.4285583 -4.4285469 -4.4286065 -4.4286685 -4.4287086 -4.4287353 -4.4287639 -4.4288006][-4.4288936 -4.4288683 -4.4288607 -4.4288435 -4.4287839 -4.4286809 -4.42855 -4.4284534 -4.4284849 -4.4285884 -4.4286695 -4.4287181 -4.4287477 -4.4287748 -4.4288063][-4.4288716 -4.4288449 -4.4288316 -4.4287982 -4.4287143 -4.4285927 -4.4284611 -4.4284039 -4.4284863 -4.4286079 -4.4286861 -4.4287238 -4.4287415 -4.428761 -4.4287887][-4.4288554 -4.4288254 -4.4288034 -4.4287577 -4.4286642 -4.4285507 -4.4284663 -4.4284716 -4.4285626 -4.4286566 -4.4287095 -4.4287267 -4.4287243 -4.4287329 -4.4287605][-4.4288511 -4.4288163 -4.4287829 -4.4287291 -4.4286461 -4.4285679 -4.4285426 -4.4285846 -4.4286542 -4.4287086 -4.428731 -4.4287286 -4.42871 -4.4287062 -4.4287348][-4.4288454 -4.4288034 -4.4287581 -4.4287043 -4.4286437 -4.4286046 -4.4286175 -4.4286671 -4.4287138 -4.4287367 -4.4287367 -4.4287252 -4.4287 -4.4286876 -4.4287162][-4.4288297 -4.4287796 -4.4287276 -4.4286776 -4.4286366 -4.4286222 -4.4286489 -4.4286971 -4.4287329 -4.4287395 -4.4287281 -4.4287124 -4.4286885 -4.4286747 -4.4287][-4.4288 -4.4287472 -4.4286971 -4.4286523 -4.4286175 -4.4286103 -4.428637 -4.4286819 -4.4287143 -4.4287176 -4.4287047 -4.4286885 -4.4286685 -4.4286575 -4.4286771][-4.4287643 -4.4287167 -4.4286733 -4.4286351 -4.4286022 -4.4285927 -4.4286156 -4.4286542 -4.4286842 -4.4286885 -4.428679 -4.428668 -4.4286528 -4.4286442 -4.4286604][-4.428751 -4.4287095 -4.4286737 -4.4286404 -4.4286103 -4.4286003 -4.428617 -4.428648 -4.4286733 -4.428678 -4.4286714 -4.4286647 -4.4286542 -4.4286494 -4.4286623]]...]
INFO - root - 2017-12-08 05:59:45.993583: step 17810, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.221 sec/batch; 19h:21m:30s remains)
INFO - root - 2017-12-08 05:59:48.235722: step 17820, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.226 sec/batch; 19h:43m:10s remains)
INFO - root - 2017-12-08 05:59:50.481228: step 17830, loss = 2.28, batch loss = 2.23 (36.0 examples/sec; 0.222 sec/batch; 19h:24m:03s remains)
INFO - root - 2017-12-08 05:59:52.695480: step 17840, loss = 2.28, batch loss = 2.23 (36.9 examples/sec; 0.217 sec/batch; 18h:57m:56s remains)
INFO - root - 2017-12-08 05:59:54.931087: step 17850, loss = 2.28, batch loss = 2.23 (35.7 examples/sec; 0.224 sec/batch; 19h:34m:20s remains)
INFO - root - 2017-12-08 05:59:57.154948: step 17860, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 19h:12m:35s remains)
INFO - root - 2017-12-08 05:59:59.385087: step 17870, loss = 2.28, batch loss = 2.23 (35.6 examples/sec; 0.225 sec/batch; 19h:39m:59s remains)
INFO - root - 2017-12-08 06:00:01.620739: step 17880, loss = 2.28, batch loss = 2.23 (34.9 examples/sec; 0.229 sec/batch; 20h:00m:34s remains)
INFO - root - 2017-12-08 06:00:03.862663: step 17890, loss = 2.28, batch loss = 2.23 (36.0 examples/sec; 0.222 sec/batch; 19h:26m:33s remains)
INFO - root - 2017-12-08 06:00:06.100912: step 17900, loss = 2.28, batch loss = 2.23 (37.0 examples/sec; 0.216 sec/batch; 18h:52m:42s remains)
2017-12-08 06:00:06.379612: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4287519 -4.4287639 -4.4287996 -4.4288344 -4.4288359 -4.4288139 -4.4287825 -4.4287496 -4.4287243 -4.428699 -4.4286671 -4.4286628 -4.4286938 -4.4287419 -4.4287739][-4.4287696 -4.428772 -4.4287968 -4.4288268 -4.4288311 -4.4288106 -4.428781 -4.4287572 -4.4287424 -4.4287291 -4.4287095 -4.4287028 -4.4287229 -4.4287643 -4.4287882][-4.4288211 -4.4288182 -4.4288249 -4.4288378 -4.428833 -4.4288177 -4.4287968 -4.4287839 -4.4287834 -4.4287872 -4.4287887 -4.428782 -4.4287887 -4.428813 -4.4288182][-4.42886 -4.4288664 -4.428865 -4.4288616 -4.4288473 -4.4288292 -4.4288049 -4.428793 -4.428802 -4.4288216 -4.428843 -4.4288416 -4.4288416 -4.428853 -4.4288492][-4.4288321 -4.4288554 -4.42886 -4.4288549 -4.4288349 -4.4288149 -4.4287896 -4.4287753 -4.4287848 -4.428812 -4.4288459 -4.4288568 -4.4288616 -4.42887 -4.4288673][-4.4287415 -4.4287853 -4.4288096 -4.4288025 -4.4287806 -4.4287715 -4.4287558 -4.4287477 -4.4287567 -4.4287844 -4.42882 -4.4288373 -4.4288487 -4.4288592 -4.4288607][-4.4286294 -4.4286971 -4.4287429 -4.4287329 -4.4287086 -4.4287095 -4.428709 -4.4287143 -4.4287348 -4.4287624 -4.4287963 -4.4288206 -4.4288349 -4.4288411 -4.4288468][-4.4285851 -4.428647 -4.428688 -4.4286661 -4.4286308 -4.42864 -4.4286509 -4.4286728 -4.4287152 -4.4287472 -4.4287739 -4.4287996 -4.4288149 -4.4288182 -4.4288259][-4.4286284 -4.4286604 -4.4286742 -4.4286418 -4.4286084 -4.4286165 -4.4286284 -4.4286537 -4.4286981 -4.4287219 -4.4287386 -4.4287572 -4.4287705 -4.4287763 -4.42879][-4.4287076 -4.4287171 -4.4287167 -4.4286909 -4.4286723 -4.4286833 -4.4286876 -4.4286995 -4.4287238 -4.4287262 -4.4287148 -4.4287047 -4.4287024 -4.4287124 -4.4287381][-4.4287295 -4.4287348 -4.4287357 -4.4287276 -4.4287329 -4.4287505 -4.4287515 -4.4287481 -4.4287515 -4.4287343 -4.4286895 -4.4286432 -4.4286156 -4.4286237 -4.4286571][-4.4286828 -4.4287009 -4.4287124 -4.4287214 -4.4287386 -4.4287548 -4.4287519 -4.4287419 -4.4287343 -4.428709 -4.4286451 -4.4285717 -4.4285297 -4.4285398 -4.4285822][-4.4287024 -4.4287243 -4.4287362 -4.4287434 -4.4287524 -4.4287572 -4.4287486 -4.4287362 -4.4287281 -4.4287043 -4.4286413 -4.4285622 -4.42852 -4.4285331 -4.428575][-4.4287653 -4.4287829 -4.4287925 -4.4287953 -4.4287987 -4.4287953 -4.4287829 -4.4287715 -4.4287658 -4.4287457 -4.428688 -4.4286189 -4.4285893 -4.428606 -4.4286356][-4.4287896 -4.4288006 -4.4288054 -4.4288077 -4.4288087 -4.428803 -4.428793 -4.4287896 -4.4287896 -4.4287739 -4.4287257 -4.4286709 -4.4286542 -4.4286766 -4.4286962]]...]
INFO - root - 2017-12-08 06:00:08.624167: step 17910, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 19h:18m:17s remains)
INFO - root - 2017-12-08 06:00:10.869090: step 17920, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 19h:19m:00s remains)
INFO - root - 2017-12-08 06:00:13.086468: step 17930, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 19h:12m:49s remains)
INFO - root - 2017-12-08 06:00:15.329950: step 17940, loss = 2.28, batch loss = 2.23 (35.4 examples/sec; 0.226 sec/batch; 19h:44m:12s remains)
INFO - root - 2017-12-08 06:00:17.591542: step 17950, loss = 2.28, batch loss = 2.23 (34.0 examples/sec; 0.235 sec/batch; 20h:33m:49s remains)
INFO - root - 2017-12-08 06:00:19.855186: step 17960, loss = 2.28, batch loss = 2.23 (35.7 examples/sec; 0.224 sec/batch; 19h:35m:24s remains)
INFO - root - 2017-12-08 06:00:22.093905: step 17970, loss = 2.28, batch loss = 2.23 (35.4 examples/sec; 0.226 sec/batch; 19h:43m:52s remains)
INFO - root - 2017-12-08 06:00:24.338405: step 17980, loss = 2.28, batch loss = 2.23 (35.8 examples/sec; 0.223 sec/batch; 19h:30m:41s remains)
INFO - root - 2017-12-08 06:00:26.588083: step 17990, loss = 2.28, batch loss = 2.23 (35.6 examples/sec; 0.225 sec/batch; 19h:39m:04s remains)
INFO - root - 2017-12-08 06:00:28.811615: step 18000, loss = 2.28, batch loss = 2.23 (34.6 examples/sec; 0.231 sec/batch; 20h:12m:13s remains)
2017-12-08 06:00:29.105113: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4286847 -4.4287043 -4.4287333 -4.4287534 -4.428762 -4.428762 -4.4287696 -4.4287858 -4.4288006 -4.4288106 -4.4288206 -4.4288344 -4.4288392 -4.4288292 -4.4288077][-4.4286246 -4.4286456 -4.4286666 -4.4286823 -4.428699 -4.4287033 -4.4287124 -4.4287333 -4.4287572 -4.428772 -4.4287806 -4.4287891 -4.4287891 -4.4287763 -4.4287462][-4.4285984 -4.4286118 -4.4286165 -4.4286246 -4.4286485 -4.428658 -4.4286704 -4.4286942 -4.428719 -4.4287338 -4.4287405 -4.4287429 -4.4287434 -4.4287362 -4.4287071][-4.4285965 -4.4285903 -4.4285717 -4.4285669 -4.428587 -4.4286008 -4.4286232 -4.4286504 -4.4286742 -4.4286852 -4.4286871 -4.4286895 -4.428699 -4.4287095 -4.4286971][-4.428628 -4.4285817 -4.4285316 -4.4285064 -4.4285078 -4.4285226 -4.4285631 -4.4286022 -4.4286265 -4.4286294 -4.4286222 -4.4286227 -4.4286408 -4.4286695 -4.4286752][-4.4286556 -4.4285693 -4.4284906 -4.4284372 -4.4284148 -4.4284239 -4.4284863 -4.4285493 -4.4285808 -4.4285769 -4.4285522 -4.4285378 -4.428545 -4.4285765 -4.4285951][-4.4286594 -4.4285512 -4.4284506 -4.428371 -4.4283218 -4.4283218 -4.4283924 -4.4284744 -4.4285183 -4.428514 -4.4284744 -4.4284425 -4.428432 -4.4284568 -4.4284787][-4.4286647 -4.4285612 -4.4284568 -4.4283667 -4.4282994 -4.4282813 -4.4283357 -4.4284124 -4.4284596 -4.4284649 -4.4284277 -4.4283886 -4.4283657 -4.4283767 -4.4283919][-4.4287195 -4.4286447 -4.4285626 -4.4284859 -4.428421 -4.4283881 -4.4284096 -4.4284606 -4.4284987 -4.428503 -4.4284668 -4.4284244 -4.4283957 -4.4283924 -4.4283996][-4.4287887 -4.4287448 -4.4286928 -4.4286427 -4.4285955 -4.4285579 -4.4285536 -4.4285774 -4.4285994 -4.428597 -4.4285636 -4.428525 -4.4284964 -4.4284816 -4.4284854][-4.4288335 -4.4288096 -4.428782 -4.4287553 -4.42873 -4.4286971 -4.428678 -4.4286876 -4.4287038 -4.4287014 -4.428678 -4.4286485 -4.4286218 -4.4285951 -4.4285936][-4.4288473 -4.4288349 -4.4288244 -4.4288177 -4.4288111 -4.4287896 -4.4287691 -4.4287691 -4.428782 -4.4287858 -4.4287772 -4.4287615 -4.4287438 -4.4287181 -4.4287124][-4.4288425 -4.428834 -4.4288316 -4.4288354 -4.4288392 -4.42883 -4.4288135 -4.4288082 -4.4288187 -4.4288282 -4.42883 -4.4288297 -4.4288263 -4.4288096 -4.4288034][-4.4288449 -4.4288335 -4.4288306 -4.4288344 -4.4288387 -4.428834 -4.4288206 -4.4288149 -4.4288239 -4.4288335 -4.4288383 -4.428844 -4.4288492 -4.4288435 -4.4288425][-4.428865 -4.4288564 -4.428853 -4.4288526 -4.428853 -4.4288507 -4.4288421 -4.4288354 -4.4288373 -4.4288387 -4.4288378 -4.4288411 -4.4288478 -4.4288497 -4.4288559]]...]
INFO - root - 2017-12-08 06:00:31.338359: step 18010, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 19h:29m:12s remains)
INFO - root - 2017-12-08 06:00:33.550696: step 18020, loss = 2.28, batch loss = 2.23 (36.8 examples/sec; 0.217 sec/batch; 18h:58m:03s remains)
INFO - root - 2017-12-08 06:00:35.785457: step 18030, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 19h:18m:14s remains)
INFO - root - 2017-12-08 06:00:38.047630: step 18040, loss = 2.28, batch loss = 2.23 (33.6 examples/sec; 0.238 sec/batch; 20h:47m:52s remains)
INFO - root - 2017-12-08 06:00:40.304776: step 18050, loss = 2.28, batch loss = 2.23 (37.4 examples/sec; 0.214 sec/batch; 18h:39m:51s remains)
INFO - root - 2017-12-08 06:00:42.532003: step 18060, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 19h:19m:43s remains)
INFO - root - 2017-12-08 06:00:44.746105: step 18070, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 19h:16m:42s remains)
INFO - root - 2017-12-08 06:00:46.977975: step 18080, loss = 2.28, batch loss = 2.23 (35.7 examples/sec; 0.224 sec/batch; 19h:34m:08s remains)
INFO - root - 2017-12-08 06:00:49.227839: step 18090, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.222 sec/batch; 19h:22m:22s remains)
INFO - root - 2017-12-08 06:00:51.449636: step 18100, loss = 2.28, batch loss = 2.23 (35.7 examples/sec; 0.224 sec/batch; 19h:33m:02s remains)
2017-12-08 06:00:51.733436: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288774 -4.4288726 -4.4288726 -4.4288678 -4.4288692 -4.4288731 -4.4288745 -4.4288812 -4.4288831 -4.4288688 -4.4288573 -4.4288549 -4.4288573 -4.4288592 -4.4288578][-4.4288116 -4.4288025 -4.4287968 -4.4287887 -4.4287958 -4.4288015 -4.4288044 -4.4288144 -4.4288182 -4.4288034 -4.4287868 -4.4287786 -4.4287815 -4.4287848 -4.42878][-4.4287772 -4.4287648 -4.4287567 -4.428751 -4.428761 -4.42876 -4.4287543 -4.4287639 -4.428772 -4.4287658 -4.4287529 -4.4287477 -4.428751 -4.4287534 -4.42874][-4.4287724 -4.4287624 -4.428762 -4.4287639 -4.4287734 -4.4287596 -4.4287405 -4.4287529 -4.4287653 -4.4287658 -4.42876 -4.428761 -4.4287667 -4.4287686 -4.4287505][-4.4287777 -4.4287682 -4.428772 -4.4287763 -4.4287806 -4.4287615 -4.4287405 -4.428751 -4.4287534 -4.4287477 -4.4287457 -4.4287505 -4.4287581 -4.4287643 -4.4287529][-4.4287777 -4.4287605 -4.4287529 -4.42875 -4.4287457 -4.4287238 -4.4287028 -4.4287 -4.4286876 -4.4286771 -4.4286776 -4.4286838 -4.4286919 -4.4287028 -4.4287095][-4.42873 -4.4287028 -4.4286847 -4.4286718 -4.4286551 -4.4286313 -4.4286079 -4.4285951 -4.428576 -4.4285684 -4.4285712 -4.42857 -4.4285626 -4.4285645 -4.4285851][-4.4286566 -4.4286318 -4.4286094 -4.4285836 -4.4285526 -4.4285197 -4.4284892 -4.4284787 -4.4284744 -4.4284773 -4.4284816 -4.4284759 -4.428462 -4.4284539 -4.4284773][-4.4286184 -4.4286036 -4.4285779 -4.4285455 -4.4285164 -4.4284873 -4.4284639 -4.4284639 -4.42847 -4.4284778 -4.4284816 -4.4284778 -4.4284668 -4.42846 -4.4284778][-4.4286346 -4.428628 -4.428606 -4.4285827 -4.4285674 -4.4285469 -4.4285307 -4.428535 -4.4285383 -4.4285431 -4.42855 -4.4285483 -4.4285393 -4.4285331 -4.4285455][-4.4286919 -4.4286976 -4.4286928 -4.428688 -4.4286885 -4.428678 -4.4286637 -4.4286647 -4.4286604 -4.4286561 -4.4286528 -4.4286442 -4.428627 -4.4286194 -4.4286275][-4.4287019 -4.4287214 -4.4287386 -4.4287591 -4.42878 -4.4287863 -4.428782 -4.4287796 -4.4287696 -4.4287562 -4.4287381 -4.4287152 -4.4286804 -4.4286647 -4.4286742][-4.42866 -4.4286928 -4.4287262 -4.4287596 -4.42879 -4.4288073 -4.4288139 -4.4288125 -4.4288 -4.4287815 -4.4287577 -4.4287276 -4.4286819 -4.428658 -4.4286628][-4.4286437 -4.4286747 -4.42871 -4.4287424 -4.4287677 -4.4287806 -4.4287844 -4.4287829 -4.4287729 -4.4287629 -4.4287453 -4.4287152 -4.428668 -4.42864 -4.4286389][-4.4286847 -4.4286981 -4.4287181 -4.4287372 -4.42875 -4.4287581 -4.4287629 -4.4287672 -4.4287653 -4.4287672 -4.4287629 -4.4287415 -4.4287014 -4.4286766 -4.4286709]]...]
INFO - root - 2017-12-08 06:00:54.013903: step 18110, loss = 2.28, batch loss = 2.23 (34.7 examples/sec; 0.230 sec/batch; 20h:07m:44s remains)
INFO - root - 2017-12-08 06:00:56.298127: step 18120, loss = 2.28, batch loss = 2.23 (32.2 examples/sec; 0.248 sec/batch; 21h:41m:20s remains)
INFO - root - 2017-12-08 06:00:58.553090: step 18130, loss = 2.28, batch loss = 2.23 (36.8 examples/sec; 0.217 sec/batch; 18h:58m:31s remains)
INFO - root - 2017-12-08 06:01:00.852737: step 18140, loss = 2.28, batch loss = 2.23 (33.7 examples/sec; 0.237 sec/batch; 20h:43m:21s remains)
INFO - root - 2017-12-08 06:01:03.085659: step 18150, loss = 2.28, batch loss = 2.23 (36.9 examples/sec; 0.217 sec/batch; 18h:55m:18s remains)
INFO - root - 2017-12-08 06:01:05.357253: step 18160, loss = 2.28, batch loss = 2.23 (36.0 examples/sec; 0.222 sec/batch; 19h:24m:23s remains)
INFO - root - 2017-12-08 06:01:07.641445: step 18170, loss = 2.28, batch loss = 2.23 (34.7 examples/sec; 0.230 sec/batch; 20h:06m:14s remains)
INFO - root - 2017-12-08 06:01:09.899256: step 18180, loss = 2.28, batch loss = 2.23 (33.9 examples/sec; 0.236 sec/batch; 20h:37m:47s remains)
INFO - root - 2017-12-08 06:01:12.134279: step 18190, loss = 2.28, batch loss = 2.23 (37.1 examples/sec; 0.216 sec/batch; 18h:49m:10s remains)
INFO - root - 2017-12-08 06:01:14.367935: step 18200, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.221 sec/batch; 19h:15m:07s remains)
2017-12-08 06:01:14.664061: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4286165 -4.4286194 -4.4286175 -4.428627 -4.4286394 -4.4286461 -4.428678 -4.4286685 -4.4286313 -4.4286056 -4.428555 -4.4285135 -4.428483 -4.428463 -4.4284587][-4.4286103 -4.4286389 -4.4286509 -4.4286613 -4.4286647 -4.42867 -4.4286971 -4.428678 -4.4286385 -4.4286017 -4.4285531 -4.4285197 -4.4284859 -4.4284582 -4.4284577][-4.4286122 -4.428648 -4.4286585 -4.428659 -4.4286537 -4.4286561 -4.4286804 -4.4286652 -4.4286356 -4.4285893 -4.4285407 -4.4285164 -4.4284916 -4.4284682 -4.4284735][-4.4286041 -4.4286356 -4.42864 -4.4286227 -4.4286022 -4.4285979 -4.4286132 -4.42861 -4.4286027 -4.4285593 -4.4285121 -4.4284973 -4.4284878 -4.4284797 -4.4284916][-4.4286036 -4.4286222 -4.4286141 -4.42858 -4.4285326 -4.4285069 -4.4285107 -4.428525 -4.4285417 -4.4285035 -4.42846 -4.4284644 -4.428473 -4.4284868 -4.4285011][-4.4286084 -4.428618 -4.4286003 -4.4285483 -4.4284649 -4.4284091 -4.4284039 -4.4284406 -4.4284787 -4.4284449 -4.4284158 -4.4284387 -4.4284577 -4.4284897 -4.428503][-4.4286265 -4.4286203 -4.4285922 -4.4285216 -4.4284053 -4.4283147 -4.4282889 -4.4283452 -4.4284205 -4.4284096 -4.4284072 -4.4284353 -4.428443 -4.4284763 -4.4284906][-4.4286318 -4.4286075 -4.4285727 -4.4284968 -4.4283648 -4.428247 -4.4281993 -4.4282746 -4.4283886 -4.4284072 -4.4284267 -4.428442 -4.4284267 -4.4284468 -4.4284678][-4.4286356 -4.428596 -4.42856 -4.428504 -4.4283962 -4.4282942 -4.428256 -4.42833 -4.4284291 -4.4284425 -4.4284635 -4.4284692 -4.4284415 -4.428452 -4.428484][-4.4286537 -4.4286146 -4.4285874 -4.4285607 -4.4284911 -4.4284286 -4.4284139 -4.42847 -4.4285283 -4.4285245 -4.4285407 -4.4285407 -4.4285121 -4.4285154 -4.4285541][-4.4286628 -4.428637 -4.428617 -4.4286094 -4.4285736 -4.4285474 -4.4285479 -4.4285846 -4.42862 -4.4286189 -4.4286366 -4.4286346 -4.4286056 -4.4286022 -4.4286375][-4.4286551 -4.4286509 -4.4286447 -4.4286537 -4.4286447 -4.428638 -4.4286432 -4.4286695 -4.4286933 -4.4287019 -4.4287243 -4.4287276 -4.4287014 -4.4286942 -4.4287229][-4.4286833 -4.4286904 -4.4286909 -4.4287009 -4.4286952 -4.4286933 -4.4287024 -4.4287229 -4.4287381 -4.4287543 -4.4287786 -4.4287858 -4.42877 -4.4287653 -4.4287863][-4.4287205 -4.4287262 -4.4287243 -4.4287229 -4.4287109 -4.4287062 -4.4287176 -4.42874 -4.4287562 -4.4287815 -4.4288111 -4.42882 -4.4288063 -4.4288039 -4.4288206][-4.4287629 -4.4287653 -4.42876 -4.4287496 -4.4287362 -4.4287286 -4.4287343 -4.4287562 -4.428781 -4.4288096 -4.4288359 -4.4288445 -4.4288335 -4.4288287 -4.4288359]]...]
INFO - root - 2017-12-08 06:01:16.886188: step 18210, loss = 2.28, batch loss = 2.23 (36.9 examples/sec; 0.217 sec/batch; 18h:56m:56s remains)
INFO - root - 2017-12-08 06:01:19.149732: step 18220, loss = 2.28, batch loss = 2.23 (35.7 examples/sec; 0.224 sec/batch; 19h:33m:43s remains)
INFO - root - 2017-12-08 06:01:21.416635: step 18230, loss = 2.28, batch loss = 2.23 (35.7 examples/sec; 0.224 sec/batch; 19h:32m:36s remains)
INFO - root - 2017-12-08 06:01:23.658506: step 18240, loss = 2.28, batch loss = 2.23 (34.9 examples/sec; 0.229 sec/batch; 19h:59m:42s remains)
INFO - root - 2017-12-08 06:01:25.901739: step 18250, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.221 sec/batch; 19h:14m:54s remains)
INFO - root - 2017-12-08 06:01:28.140215: step 18260, loss = 2.28, batch loss = 2.23 (37.0 examples/sec; 0.216 sec/batch; 18h:53m:02s remains)
INFO - root - 2017-12-08 06:01:30.374771: step 18270, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 19h:09m:03s remains)
INFO - root - 2017-12-08 06:01:32.636523: step 18280, loss = 2.28, batch loss = 2.23 (35.1 examples/sec; 0.228 sec/batch; 19h:54m:07s remains)
INFO - root - 2017-12-08 06:01:34.908397: step 18290, loss = 2.28, batch loss = 2.23 (35.3 examples/sec; 0.227 sec/batch; 19h:47m:15s remains)
INFO - root - 2017-12-08 06:01:37.132196: step 18300, loss = 2.28, batch loss = 2.23 (34.6 examples/sec; 0.231 sec/batch; 20h:10m:59s remains)
2017-12-08 06:01:37.471033: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288549 -4.4288311 -4.4288054 -4.4287896 -4.4287953 -4.4288154 -4.4288449 -4.4288669 -4.428875 -4.4288707 -4.4288678 -4.4288764 -4.428895 -4.4289126 -4.4289308][-4.428833 -4.4287887 -4.4287438 -4.4287105 -4.4287038 -4.428719 -4.4287496 -4.4287806 -4.4287992 -4.4288015 -4.4288011 -4.4288197 -4.4288464 -4.4288712 -4.4288964][-4.4288473 -4.428802 -4.4287467 -4.4286971 -4.428668 -4.4286633 -4.4286766 -4.4286952 -4.4287152 -4.4287252 -4.4287314 -4.4287624 -4.428793 -4.4288244 -4.4288592][-4.4288392 -4.4288034 -4.4287496 -4.4286952 -4.42865 -4.42862 -4.4286094 -4.428606 -4.428617 -4.4286361 -4.4286532 -4.4286919 -4.4287243 -4.4287629 -4.4288092][-4.4287877 -4.4287629 -4.4287162 -4.4286647 -4.4286141 -4.4285669 -4.4285288 -4.4284868 -4.4284859 -4.4285321 -4.4285722 -4.42862 -4.4286566 -4.428699 -4.4287572][-4.4287143 -4.4286914 -4.428658 -4.4286194 -4.4285622 -4.4284844 -4.4283996 -4.4282956 -4.428288 -4.4283862 -4.428472 -4.4285479 -4.428597 -4.4286447 -4.4287138][-4.4286318 -4.4286022 -4.4285765 -4.42854 -4.4284611 -4.4283385 -4.42819 -4.427999 -4.4279742 -4.4281507 -4.4283032 -4.4284296 -4.4285159 -4.42859 -4.42868][-4.4286132 -4.4285855 -4.4285669 -4.4285378 -4.4284587 -4.4283295 -4.428165 -4.4279494 -4.4279022 -4.4280953 -4.4282641 -4.428401 -4.4285092 -4.4285979 -4.4286909][-4.4286771 -4.4286695 -4.4286637 -4.4286466 -4.4285913 -4.4285021 -4.4283895 -4.4282603 -4.4282355 -4.4283528 -4.4284577 -4.4285417 -4.4286146 -4.4286795 -4.4287467][-4.4287748 -4.4287891 -4.4287977 -4.4287896 -4.4287581 -4.4287052 -4.428638 -4.4285755 -4.4285645 -4.42862 -4.42867 -4.4287086 -4.4287443 -4.4287796 -4.4288158][-4.4288664 -4.4288869 -4.4289012 -4.4288983 -4.42888 -4.4288545 -4.4288187 -4.428791 -4.4287882 -4.428812 -4.4288287 -4.4288397 -4.4288554 -4.42887 -4.4288816][-4.4289188 -4.4289346 -4.4289441 -4.42894 -4.4289284 -4.4289165 -4.4289002 -4.4288912 -4.4288974 -4.4289093 -4.4289131 -4.4289155 -4.4289231 -4.4289269 -4.4289212][-4.4289389 -4.4289446 -4.4289479 -4.4289455 -4.4289408 -4.4289341 -4.4289269 -4.4289274 -4.4289355 -4.4289446 -4.4289479 -4.4289479 -4.4289494 -4.428947 -4.4289379][-4.4289465 -4.4289489 -4.4289484 -4.4289489 -4.4289479 -4.4289432 -4.4289403 -4.4289417 -4.4289479 -4.4289546 -4.428956 -4.4289556 -4.4289551 -4.4289513 -4.4289455][-4.4289508 -4.4289513 -4.4289474 -4.4289451 -4.4289436 -4.4289422 -4.4289417 -4.4289422 -4.4289455 -4.42895 -4.4289527 -4.4289536 -4.428956 -4.428957 -4.4289579]]...]
INFO - root - 2017-12-08 06:01:39.704833: step 18310, loss = 2.28, batch loss = 2.23 (37.7 examples/sec; 0.212 sec/batch; 18h:32m:28s remains)
INFO - root - 2017-12-08 06:01:41.957502: step 18320, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 19h:06m:35s remains)
INFO - root - 2017-12-08 06:01:44.236258: step 18330, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.225 sec/batch; 19h:40m:42s remains)
INFO - root - 2017-12-08 06:01:46.474194: step 18340, loss = 2.28, batch loss = 2.23 (34.9 examples/sec; 0.229 sec/batch; 19h:59m:22s remains)
INFO - root - 2017-12-08 06:01:48.699405: step 18350, loss = 2.28, batch loss = 2.23 (34.4 examples/sec; 0.232 sec/batch; 20h:16m:54s remains)
INFO - root - 2017-12-08 06:01:50.942264: step 18360, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.220 sec/batch; 19h:13m:37s remains)
INFO - root - 2017-12-08 06:01:53.202165: step 18370, loss = 2.28, batch loss = 2.23 (33.4 examples/sec; 0.240 sec/batch; 20h:55m:13s remains)
INFO - root - 2017-12-08 06:01:55.412762: step 18380, loss = 2.28, batch loss = 2.23 (36.8 examples/sec; 0.217 sec/batch; 18h:58m:30s remains)
INFO - root - 2017-12-08 06:01:57.645665: step 18390, loss = 2.28, batch loss = 2.23 (35.0 examples/sec; 0.229 sec/batch; 19h:57m:42s remains)
INFO - root - 2017-12-08 06:01:59.879877: step 18400, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 19h:27m:04s remains)
2017-12-08 06:02:00.168771: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4287086 -4.4287257 -4.4287271 -4.4287629 -4.4287934 -4.4288015 -4.4287834 -4.4287543 -4.4287443 -4.4287162 -4.4286652 -4.4285955 -4.4285216 -4.4285283 -4.4286218][-4.4287128 -4.4287286 -4.4287395 -4.4287863 -4.4288311 -4.4288573 -4.4288559 -4.4288406 -4.428823 -4.428772 -4.428688 -4.4285793 -4.4284883 -4.4285092 -4.4286251][-4.4287057 -4.4287167 -4.4287362 -4.4287868 -4.4288392 -4.428875 -4.4288783 -4.4288683 -4.4288478 -4.4287968 -4.4287152 -4.4286141 -4.4285412 -4.428575 -4.4286842][-4.4286909 -4.4287066 -4.428731 -4.4287677 -4.4288139 -4.4288411 -4.428833 -4.4288163 -4.4287987 -4.4287739 -4.4287262 -4.4286642 -4.4286323 -4.4286613 -4.4287324][-4.4286475 -4.4286795 -4.42871 -4.428731 -4.4287534 -4.4287515 -4.4287148 -4.4286776 -4.4286737 -4.4286919 -4.4286938 -4.4286819 -4.4286757 -4.4286766 -4.4287019][-4.4286 -4.4286408 -4.4286728 -4.4286771 -4.4286532 -4.4285994 -4.4285073 -4.428432 -4.4284673 -4.428544 -4.4286075 -4.4286451 -4.4286423 -4.4286232 -4.4286261][-4.4285741 -4.4286094 -4.4286246 -4.4286051 -4.4285336 -4.42842 -4.4282374 -4.4281006 -4.4282155 -4.4283791 -4.4285107 -4.42859 -4.4285855 -4.4285645 -4.4285641][-4.4285765 -4.4286032 -4.4285979 -4.4285517 -4.4284554 -4.4283123 -4.4280829 -4.427927 -4.4281077 -4.4283228 -4.4284859 -4.4285741 -4.4285641 -4.4285369 -4.4285235][-4.4286008 -4.428617 -4.4286089 -4.4285574 -4.4284825 -4.4283848 -4.4282331 -4.4281554 -4.4282808 -4.42842 -4.4285293 -4.4285841 -4.4285712 -4.4285412 -4.4285288][-4.4286313 -4.4286532 -4.42866 -4.4286284 -4.4285808 -4.4285221 -4.4284425 -4.4284215 -4.4284868 -4.4285493 -4.4285927 -4.4286141 -4.428606 -4.42859 -4.4285765][-4.4286304 -4.42867 -4.4287004 -4.4286947 -4.4286695 -4.4286356 -4.428597 -4.4285922 -4.4286184 -4.4286323 -4.4286394 -4.4286451 -4.4286485 -4.4286385 -4.4286113][-4.4286203 -4.4286718 -4.4287233 -4.4287424 -4.4287353 -4.4287086 -4.4286795 -4.4286671 -4.4286704 -4.4286714 -4.428669 -4.4286709 -4.4286823 -4.4286742 -4.428637][-4.4286432 -4.4286866 -4.4287519 -4.4287896 -4.4287906 -4.4287682 -4.4287362 -4.42871 -4.4287057 -4.4287066 -4.4287086 -4.4287167 -4.4287338 -4.42872 -4.4286795][-4.4286962 -4.4287229 -4.42878 -4.4288197 -4.4288268 -4.42881 -4.4287772 -4.4287405 -4.4287324 -4.4287348 -4.4287419 -4.4287591 -4.428782 -4.4287758 -4.4287486][-4.4287457 -4.4287539 -4.4287896 -4.4288158 -4.4288197 -4.4288087 -4.4287858 -4.4287462 -4.42873 -4.4287324 -4.4287491 -4.4287791 -4.4288144 -4.4288225 -4.4288096]]...]
INFO - root - 2017-12-08 06:02:02.394188: step 18410, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 19h:27m:16s remains)
INFO - root - 2017-12-08 06:02:04.634629: step 18420, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 19h:18m:17s remains)
INFO - root - 2017-12-08 06:02:06.892791: step 18430, loss = 2.28, batch loss = 2.23 (35.6 examples/sec; 0.225 sec/batch; 19h:37m:40s remains)
INFO - root - 2017-12-08 06:02:09.127821: step 18440, loss = 2.28, batch loss = 2.23 (35.0 examples/sec; 0.228 sec/batch; 19h:55m:50s remains)
INFO - root - 2017-12-08 06:02:11.345804: step 18450, loss = 2.28, batch loss = 2.23 (37.3 examples/sec; 0.214 sec/batch; 18h:41m:13s remains)
INFO - root - 2017-12-08 06:02:13.632745: step 18460, loss = 2.28, batch loss = 2.23 (37.0 examples/sec; 0.217 sec/batch; 18h:53m:10s remains)
INFO - root - 2017-12-08 06:02:15.887259: step 18470, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.222 sec/batch; 19h:20m:23s remains)
INFO - root - 2017-12-08 06:02:18.154087: step 18480, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 19h:07m:49s remains)
INFO - root - 2017-12-08 06:02:20.423677: step 18490, loss = 2.28, batch loss = 2.23 (36.0 examples/sec; 0.222 sec/batch; 19h:23m:22s remains)
INFO - root - 2017-12-08 06:02:22.674580: step 18500, loss = 2.28, batch loss = 2.23 (35.0 examples/sec; 0.229 sec/batch; 19h:57m:50s remains)
2017-12-08 06:02:22.994098: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288707 -4.4288964 -4.42892 -4.4289389 -4.4289532 -4.4289618 -4.428967 -4.4289722 -4.4289742 -4.4289703 -4.4289618 -4.4289474 -4.4289222 -4.4288836 -4.4288487][-4.4288607 -4.4288821 -4.4289 -4.4289165 -4.4289308 -4.4289412 -4.4289532 -4.4289703 -4.4289851 -4.4289913 -4.4289885 -4.4289732 -4.4289436 -4.4288993 -4.4288573][-4.4288397 -4.4288454 -4.4288487 -4.4288559 -4.4288673 -4.4288774 -4.428896 -4.4289279 -4.42896 -4.42898 -4.4289865 -4.4289761 -4.4289474 -4.4289017 -4.4288578][-4.4288063 -4.4287853 -4.4287653 -4.4287581 -4.4287634 -4.4287691 -4.4287882 -4.4288321 -4.428884 -4.4289212 -4.4289412 -4.4289412 -4.4289217 -4.4288836 -4.4288445][-4.4287682 -4.428719 -4.4286737 -4.4286489 -4.4286432 -4.4286337 -4.4286356 -4.428679 -4.4287486 -4.4288054 -4.4288416 -4.42886 -4.42886 -4.4288397 -4.4288144][-4.428731 -4.4286656 -4.4286036 -4.4285622 -4.4285378 -4.4285007 -4.4284673 -4.4284892 -4.4285688 -4.428648 -4.4287047 -4.4287448 -4.4287705 -4.4287744 -4.42877][-4.4286933 -4.4286256 -4.4285569 -4.4285026 -4.4284582 -4.4283943 -4.4283228 -4.42831 -4.4283915 -4.4284916 -4.4285693 -4.4286284 -4.4286785 -4.4287076 -4.4287257][-4.4286547 -4.4285955 -4.4285307 -4.4284735 -4.4284182 -4.4283404 -4.4282503 -4.42821 -4.4282804 -4.4283915 -4.428483 -4.4285522 -4.428616 -4.4286633 -4.4286976][-4.4286261 -4.4285765 -4.4285216 -4.428473 -4.4284239 -4.4283543 -4.4282742 -4.428225 -4.4282718 -4.4283743 -4.4284673 -4.4285359 -4.4286 -4.4286523 -4.4286938][-4.4286232 -4.4285774 -4.428534 -4.4284997 -4.42847 -4.4284239 -4.4283671 -4.4283247 -4.4283495 -4.42843 -4.4285121 -4.4285722 -4.4286242 -4.4286704 -4.4287095][-4.4286656 -4.4286208 -4.428586 -4.4285655 -4.428555 -4.428534 -4.4285011 -4.4284706 -4.4284797 -4.428534 -4.428596 -4.4286413 -4.4286747 -4.4287057 -4.4287348][-4.4287539 -4.4287148 -4.4286852 -4.4286709 -4.4286695 -4.4286647 -4.4286475 -4.4286256 -4.4286227 -4.4286504 -4.428689 -4.4287171 -4.4287338 -4.42875 -4.4287639][-4.4288478 -4.4288268 -4.4288073 -4.4287949 -4.428793 -4.428793 -4.428782 -4.4287629 -4.4287515 -4.4287581 -4.4287753 -4.4287868 -4.428791 -4.4287915 -4.4287853][-4.428894 -4.4289026 -4.4289031 -4.4288993 -4.4288974 -4.4288931 -4.4288783 -4.4288564 -4.4288383 -4.4288316 -4.428833 -4.4288335 -4.4288287 -4.4288144 -4.4287848][-4.428864 -4.4289036 -4.4289303 -4.4289436 -4.4289494 -4.428947 -4.4289327 -4.4289112 -4.42889 -4.428874 -4.4288626 -4.428853 -4.4288383 -4.4288087 -4.4287548]]...]
INFO - root - 2017-12-08 06:02:25.207205: step 18510, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 19h:18m:05s remains)
INFO - root - 2017-12-08 06:02:27.462346: step 18520, loss = 2.28, batch loss = 2.23 (35.0 examples/sec; 0.229 sec/batch; 19h:55m:57s remains)
INFO - root - 2017-12-08 06:02:29.721769: step 18530, loss = 2.28, batch loss = 2.23 (35.6 examples/sec; 0.225 sec/batch; 19h:35m:21s remains)
INFO - root - 2017-12-08 06:02:31.958467: step 18540, loss = 2.28, batch loss = 2.23 (37.0 examples/sec; 0.216 sec/batch; 18h:50m:11s remains)
INFO - root - 2017-12-08 06:02:34.187713: step 18550, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.222 sec/batch; 19h:19m:27s remains)
INFO - root - 2017-12-08 06:02:36.435464: step 18560, loss = 2.28, batch loss = 2.23 (35.6 examples/sec; 0.224 sec/batch; 19h:34m:16s remains)
INFO - root - 2017-12-08 06:02:38.688224: step 18570, loss = 2.28, batch loss = 2.23 (35.2 examples/sec; 0.227 sec/batch; 19h:48m:54s remains)
INFO - root - 2017-12-08 06:02:40.938902: step 18580, loss = 2.28, batch loss = 2.23 (34.3 examples/sec; 0.233 sec/batch; 20h:20m:50s remains)
INFO - root - 2017-12-08 06:02:43.197946: step 18590, loss = 2.28, batch loss = 2.23 (33.6 examples/sec; 0.238 sec/batch; 20h:46m:45s remains)
INFO - root - 2017-12-08 06:02:45.427034: step 18600, loss = 2.28, batch loss = 2.23 (34.6 examples/sec; 0.231 sec/batch; 20h:09m:59s remains)
2017-12-08 06:02:45.748030: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.428771 -4.4287686 -4.4287925 -4.4288077 -4.4287834 -4.4287553 -4.4287424 -4.4287286 -4.4286842 -4.4286184 -4.4285836 -4.4285936 -4.4286466 -4.4287019 -4.4287758][-4.4287567 -4.4287496 -4.4287724 -4.4287982 -4.4287877 -4.428772 -4.42876 -4.4287367 -4.4286919 -4.428637 -4.4286075 -4.4286194 -4.4286566 -4.4286871 -4.4287415][-4.4287415 -4.4287457 -4.4287715 -4.4287992 -4.4287982 -4.4287906 -4.4287782 -4.4287534 -4.4287214 -4.4286942 -4.4286656 -4.4286609 -4.4286661 -4.4286609 -4.4286995][-4.4287338 -4.428761 -4.4287925 -4.4288158 -4.42882 -4.4288106 -4.4287863 -4.428751 -4.4287295 -4.4287372 -4.4287243 -4.4287043 -4.4286771 -4.4286489 -4.4286833][-4.4287395 -4.4287829 -4.4288139 -4.4288311 -4.4288321 -4.4288044 -4.4287505 -4.4286771 -4.428648 -4.4287066 -4.42874 -4.4287281 -4.4286919 -4.4286594 -4.4286942][-4.428762 -4.4288039 -4.4288306 -4.42884 -4.4288278 -4.4287724 -4.4286671 -4.428525 -4.4284639 -4.4285846 -4.4286838 -4.4287028 -4.4286823 -4.4286628 -4.4287095][-4.4287853 -4.4288144 -4.4288273 -4.4288263 -4.4288034 -4.4287248 -4.4285707 -4.4283519 -4.428246 -4.428431 -4.42859 -4.4286408 -4.4286518 -4.4286566 -4.4287128][-4.4287829 -4.4287934 -4.428793 -4.4287872 -4.4287715 -4.4286938 -4.4285197 -4.4282551 -4.4281368 -4.4283471 -4.4285169 -4.4285855 -4.4286232 -4.4286537 -4.4287133][-4.4287734 -4.4287667 -4.4287539 -4.4287524 -4.4287572 -4.4287052 -4.4285669 -4.4283543 -4.4282746 -4.4284177 -4.428524 -4.4285722 -4.4286151 -4.4286542 -4.428709][-4.4287677 -4.4287553 -4.4287419 -4.4287524 -4.4287724 -4.4287496 -4.42866 -4.4285216 -4.4284811 -4.428555 -4.4285932 -4.4286127 -4.4286413 -4.4286685 -4.4287105][-4.4287715 -4.4287577 -4.4287515 -4.4287724 -4.428802 -4.4287963 -4.4287357 -4.4286427 -4.4286275 -4.4286633 -4.4286709 -4.4286704 -4.4286752 -4.4286809 -4.4287109][-4.4287887 -4.4287705 -4.4287686 -4.428793 -4.4288282 -4.4288335 -4.4287882 -4.4287281 -4.4287291 -4.4287405 -4.4287319 -4.4287281 -4.4287243 -4.4287167 -4.4287372][-4.4288173 -4.428793 -4.428793 -4.4288158 -4.4288468 -4.4288559 -4.42882 -4.4287815 -4.42879 -4.4287868 -4.4287815 -4.4287906 -4.4287872 -4.4287767 -4.4287944][-4.42885 -4.4288244 -4.42882 -4.4288387 -4.428865 -4.4288683 -4.4288421 -4.4288263 -4.4288354 -4.42883 -4.428833 -4.4288478 -4.4288492 -4.428844 -4.42886][-4.428875 -4.4288507 -4.4288392 -4.4288549 -4.4288764 -4.4288774 -4.4288583 -4.428853 -4.4288583 -4.4288549 -4.4288597 -4.4288735 -4.4288759 -4.4288731 -4.4288883]]...]
INFO - root - 2017-12-08 06:02:47.987782: step 18610, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 19h:10m:03s remains)
INFO - root - 2017-12-08 06:02:50.208519: step 18620, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 19h:16m:33s remains)
INFO - root - 2017-12-08 06:02:52.449268: step 18630, loss = 2.28, batch loss = 2.23 (35.7 examples/sec; 0.224 sec/batch; 19h:30m:40s remains)
INFO - root - 2017-12-08 06:02:54.759522: step 18640, loss = 2.28, batch loss = 2.23 (35.4 examples/sec; 0.226 sec/batch; 19h:42m:02s remains)
INFO - root - 2017-12-08 06:02:57.036968: step 18650, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 19h:08m:53s remains)
INFO - root - 2017-12-08 06:02:59.300109: step 18660, loss = 2.28, batch loss = 2.23 (35.4 examples/sec; 0.226 sec/batch; 19h:42m:18s remains)
INFO - root - 2017-12-08 06:03:01.537270: step 18670, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.225 sec/batch; 19h:37m:14s remains)
INFO - root - 2017-12-08 06:03:03.767518: step 18680, loss = 2.28, batch loss = 2.23 (34.2 examples/sec; 0.234 sec/batch; 20h:24m:03s remains)
INFO - root - 2017-12-08 06:03:05.995442: step 18690, loss = 2.28, batch loss = 2.23 (37.2 examples/sec; 0.215 sec/batch; 18h:44m:41s remains)
INFO - root - 2017-12-08 06:03:08.253080: step 18700, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 19h:25m:08s remains)
2017-12-08 06:03:08.540955: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288206 -4.4287691 -4.4287238 -4.4286518 -4.4285941 -4.4286284 -4.4286942 -4.4287605 -4.4287972 -4.4287858 -4.4287477 -4.4287114 -4.4286733 -4.4286609 -4.4286609][-4.4287906 -4.4287319 -4.4286866 -4.4286118 -4.4285388 -4.4285488 -4.42861 -4.4286814 -4.4287343 -4.428751 -4.4287434 -4.4287238 -4.4286895 -4.4286671 -4.428658][-4.4287691 -4.4287109 -4.4286623 -4.4285808 -4.4284897 -4.4284687 -4.42851 -4.4285846 -4.4286571 -4.4287004 -4.4287267 -4.4287276 -4.4286976 -4.428669 -4.428659][-4.4287605 -4.4287105 -4.4286575 -4.4285741 -4.4284735 -4.4284239 -4.4284396 -4.4285116 -4.4286008 -4.4286704 -4.4287257 -4.4287486 -4.4287248 -4.4286928 -4.428678][-4.4287691 -4.4287238 -4.4286747 -4.4286075 -4.428514 -4.4284358 -4.4284124 -4.4284635 -4.4285622 -4.4286461 -4.4287186 -4.4287629 -4.4287577 -4.4287305 -4.4287128][-4.4287853 -4.4287324 -4.42868 -4.4286232 -4.42855 -4.4284682 -4.428421 -4.4284549 -4.4285469 -4.4286203 -4.4286852 -4.4287405 -4.4287581 -4.4287505 -4.4287415][-4.4288054 -4.428751 -4.4286895 -4.4286261 -4.4285603 -4.4284945 -4.4284511 -4.4284668 -4.42852 -4.4285536 -4.4285941 -4.4286604 -4.4287124 -4.4287395 -4.4287519][-4.4288163 -4.4287753 -4.4287271 -4.4286671 -4.4286122 -4.4285626 -4.4285188 -4.4284887 -4.4284759 -4.4284492 -4.4284492 -4.4285164 -4.4286079 -4.4286833 -4.4287233][-4.4288216 -4.4288 -4.4287786 -4.4287467 -4.4287195 -4.4286857 -4.428628 -4.428545 -4.4284539 -4.4283628 -4.4283214 -4.4283867 -4.4285097 -4.4286237 -4.428688][-4.428844 -4.4288378 -4.4288354 -4.4288321 -4.4288316 -4.42881 -4.4287477 -4.4286427 -4.4285212 -4.4284096 -4.4283595 -4.4284105 -4.4285212 -4.4286294 -4.4286861][-4.4288731 -4.4288735 -4.4288812 -4.428895 -4.4289074 -4.4288926 -4.4288411 -4.4287529 -4.428647 -4.4285536 -4.4285173 -4.4285455 -4.42862 -4.4286866 -4.4287128][-4.428905 -4.4289064 -4.4289117 -4.4289222 -4.4289384 -4.4289336 -4.4288983 -4.4288344 -4.4287567 -4.4286928 -4.4286733 -4.4286923 -4.4287348 -4.4287677 -4.4287686][-4.4289269 -4.42893 -4.4289303 -4.428936 -4.4289503 -4.4289522 -4.4289317 -4.4288955 -4.4288483 -4.4288044 -4.428791 -4.4287992 -4.4288192 -4.4288311 -4.42882][-4.4289327 -4.4289384 -4.4289355 -4.4289351 -4.4289441 -4.4289494 -4.4289408 -4.4289284 -4.428906 -4.4288774 -4.4288669 -4.4288669 -4.4288697 -4.428865 -4.4288473][-4.4289227 -4.4289317 -4.4289322 -4.4289303 -4.4289341 -4.428937 -4.4289341 -4.4289322 -4.4289241 -4.4289079 -4.4288979 -4.4288945 -4.4288917 -4.4288826 -4.4288683]]...]
INFO - root - 2017-12-08 06:03:10.751705: step 18710, loss = 2.28, batch loss = 2.23 (34.7 examples/sec; 0.230 sec/batch; 20h:04m:01s remains)
INFO - root - 2017-12-08 06:03:12.970605: step 18720, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.221 sec/batch; 19h:13m:31s remains)
INFO - root - 2017-12-08 06:03:15.256525: step 18730, loss = 2.28, batch loss = 2.23 (34.8 examples/sec; 0.230 sec/batch; 20h:02m:36s remains)
INFO - root - 2017-12-08 06:03:17.538697: step 18740, loss = 2.28, batch loss = 2.23 (35.6 examples/sec; 0.225 sec/batch; 19h:34m:31s remains)
INFO - root - 2017-12-08 06:03:19.781801: step 18750, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 19h:05m:40s remains)
INFO - root - 2017-12-08 06:03:22.024210: step 18760, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 19h:15m:34s remains)
INFO - root - 2017-12-08 06:03:24.325044: step 18770, loss = 2.28, batch loss = 2.23 (35.0 examples/sec; 0.228 sec/batch; 19h:53m:57s remains)
INFO - root - 2017-12-08 06:03:26.586312: step 18780, loss = 2.28, batch loss = 2.23 (34.3 examples/sec; 0.233 sec/batch; 20h:18m:29s remains)
INFO - root - 2017-12-08 06:03:28.804996: step 18790, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.222 sec/batch; 19h:19m:51s remains)
INFO - root - 2017-12-08 06:03:31.038521: step 18800, loss = 2.28, batch loss = 2.23 (36.9 examples/sec; 0.217 sec/batch; 18h:53m:55s remains)
2017-12-08 06:03:31.309077: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288373 -4.4288421 -4.4288387 -4.4288077 -4.4287663 -4.4287271 -4.4286942 -4.42865 -4.4285975 -4.4285612 -4.42856 -4.42857 -4.4285793 -4.4286094 -4.4286156][-4.4288421 -4.4288516 -4.42885 -4.4288063 -4.4287543 -4.4286995 -4.4286423 -4.4285774 -4.4285316 -4.4285088 -4.42851 -4.4285212 -4.4285231 -4.428545 -4.4285665][-4.4288445 -4.4288516 -4.428853 -4.4288058 -4.4287462 -4.4286923 -4.4286284 -4.4285531 -4.4285135 -4.4285131 -4.4285274 -4.4285331 -4.4285192 -4.4285345 -4.4285626][-4.4288344 -4.4288435 -4.4288559 -4.4288187 -4.4287581 -4.4287066 -4.4286427 -4.4285703 -4.4285345 -4.4285531 -4.4285846 -4.4285841 -4.4285536 -4.4285607 -4.4285951][-4.4287839 -4.4287868 -4.4288106 -4.4287887 -4.4287171 -4.4286509 -4.4285789 -4.4285054 -4.428483 -4.4285398 -4.428597 -4.4286 -4.4285674 -4.4285741 -4.4286079][-4.4286919 -4.4286771 -4.4287047 -4.4287081 -4.4286385 -4.4285355 -4.4284043 -4.428278 -4.4282775 -4.4284053 -4.4285135 -4.4285417 -4.428534 -4.4285469 -4.4285674][-4.4286084 -4.4285612 -4.4285707 -4.4285712 -4.4285021 -4.4283533 -4.4281363 -4.4279375 -4.4279928 -4.4282227 -4.4283924 -4.4284649 -4.4284887 -4.4285016 -4.4285145][-4.4285574 -4.4284835 -4.4284754 -4.4284821 -4.4284368 -4.4282761 -4.4280176 -4.4277759 -4.4278822 -4.42817 -4.4283652 -4.4284568 -4.4284849 -4.4284821 -4.4284697][-4.4285398 -4.4284639 -4.428462 -4.4285026 -4.4285131 -4.4284186 -4.4282379 -4.4280715 -4.4281411 -4.428339 -4.4284739 -4.4285426 -4.4285488 -4.4285135 -4.4284678][-4.428556 -4.4285126 -4.4285336 -4.428606 -4.42866 -4.4286227 -4.4285154 -4.4284024 -4.4284234 -4.4285407 -4.4286208 -4.4286604 -4.4286389 -4.4285784 -4.4285121][-4.4285703 -4.4285512 -4.428586 -4.4286747 -4.428751 -4.4287457 -4.4286785 -4.4285946 -4.4285932 -4.4286656 -4.4287162 -4.4287443 -4.428719 -4.4286609 -4.4285903][-4.4285789 -4.4285703 -4.4286036 -4.428689 -4.4287629 -4.4287667 -4.42872 -4.4286633 -4.4286642 -4.4287181 -4.4287615 -4.4287934 -4.4287882 -4.4287534 -4.4286847][-4.4286227 -4.4286165 -4.4286475 -4.4287143 -4.4287729 -4.42878 -4.4287505 -4.4287162 -4.4287167 -4.4287534 -4.4287891 -4.4288235 -4.4288311 -4.4288049 -4.4287357][-4.4287066 -4.4286995 -4.4287224 -4.4287639 -4.4288034 -4.42881 -4.4287972 -4.4287796 -4.4287748 -4.4287896 -4.4288044 -4.428823 -4.4288292 -4.4288011 -4.4287381][-4.4287806 -4.4287715 -4.4287844 -4.4288058 -4.4288268 -4.4288325 -4.4288321 -4.4288263 -4.428823 -4.4288287 -4.4288282 -4.4288306 -4.4288268 -4.428793 -4.4287362]]...]
INFO - root - 2017-12-08 06:03:33.550587: step 18810, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 19h:23m:45s remains)
INFO - root - 2017-12-08 06:03:35.791022: step 18820, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.220 sec/batch; 19h:11m:03s remains)
INFO - root - 2017-12-08 06:03:38.026445: step 18830, loss = 2.28, batch loss = 2.23 (36.7 examples/sec; 0.218 sec/batch; 18h:59m:25s remains)
INFO - root - 2017-12-08 06:03:40.260937: step 18840, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 19h:14m:41s remains)
INFO - root - 2017-12-08 06:03:42.521851: step 18850, loss = 2.28, batch loss = 2.23 (34.0 examples/sec; 0.235 sec/batch; 20h:30m:38s remains)
INFO - root - 2017-12-08 06:03:44.811891: step 18860, loss = 2.28, batch loss = 2.23 (34.3 examples/sec; 0.233 sec/batch; 20h:19m:41s remains)
INFO - root - 2017-12-08 06:03:47.062074: step 18870, loss = 2.28, batch loss = 2.23 (35.3 examples/sec; 0.226 sec/batch; 19h:43m:24s remains)
INFO - root - 2017-12-08 06:03:49.310685: step 18880, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 19h:06m:12s remains)
INFO - root - 2017-12-08 06:03:51.557533: step 18890, loss = 2.28, batch loss = 2.23 (34.2 examples/sec; 0.234 sec/batch; 20h:23m:16s remains)
INFO - root - 2017-12-08 06:03:53.813978: step 18900, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 19h:15m:34s remains)
2017-12-08 06:03:54.088352: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288926 -4.4288807 -4.4288688 -4.4288592 -4.4288549 -4.4288497 -4.4288383 -4.4288235 -4.4287906 -4.42876 -4.4287357 -4.428741 -4.4287829 -4.4288392 -4.4288812][-4.4288821 -4.4288716 -4.4288621 -4.4288564 -4.4288511 -4.42885 -4.4288425 -4.4288287 -4.4288015 -4.4287729 -4.4287515 -4.4287515 -4.4287858 -4.42884 -4.4288807][-4.42888 -4.428875 -4.4288635 -4.4288549 -4.4288425 -4.4288321 -4.4288106 -4.4287868 -4.4287629 -4.4287486 -4.4287467 -4.4287553 -4.4287829 -4.4288325 -4.428875][-4.428874 -4.4288759 -4.4288621 -4.4288487 -4.4288144 -4.4287715 -4.4287171 -4.4286761 -4.428669 -4.4286885 -4.4287109 -4.4287252 -4.42876 -4.4288177 -4.4288697][-4.4288535 -4.4288621 -4.4288435 -4.428812 -4.4287443 -4.4286547 -4.4285526 -4.4284763 -4.4284925 -4.4285746 -4.4286437 -4.4286904 -4.4287486 -4.428812 -4.4288621][-4.4288297 -4.4288316 -4.4288096 -4.4287572 -4.4286532 -4.4285111 -4.428329 -4.4281878 -4.4282517 -4.4284415 -4.4285884 -4.4286828 -4.428762 -4.4288216 -4.4288597][-4.4288049 -4.4288049 -4.4287844 -4.4287152 -4.4285851 -4.4284005 -4.42816 -4.4279833 -4.4281 -4.4283671 -4.428566 -4.428688 -4.4287772 -4.4288363 -4.4288678][-4.42879 -4.4287949 -4.4287724 -4.4286947 -4.4285641 -4.4284015 -4.4282117 -4.4280939 -4.4282055 -4.4284353 -4.4286151 -4.4287295 -4.4288111 -4.428864 -4.4288859][-4.4287853 -4.4287972 -4.4287782 -4.4287162 -4.4286208 -4.42852 -4.4284186 -4.4283719 -4.4284549 -4.428597 -4.4287148 -4.4287972 -4.4288611 -4.4289021 -4.4289126][-4.4287887 -4.4288158 -4.4288168 -4.4287882 -4.4287343 -4.4286804 -4.4286413 -4.428638 -4.4286909 -4.4287596 -4.4288182 -4.428863 -4.4289026 -4.4289312 -4.4289351][-4.4288192 -4.4288568 -4.4288726 -4.4288621 -4.4288306 -4.4288044 -4.4288006 -4.4288206 -4.4288573 -4.4288859 -4.4289079 -4.428926 -4.4289408 -4.4289513 -4.4289455][-4.4288731 -4.4289041 -4.4289174 -4.4289064 -4.4288864 -4.4288826 -4.4288969 -4.428925 -4.4289522 -4.4289632 -4.4289665 -4.4289694 -4.4289703 -4.4289641 -4.4289508][-4.4289069 -4.4289246 -4.4289284 -4.4289155 -4.4289002 -4.4289069 -4.4289269 -4.4289527 -4.4289708 -4.4289713 -4.4289646 -4.4289665 -4.428967 -4.4289579 -4.4289455][-4.42891 -4.4289155 -4.42891 -4.428894 -4.42888 -4.428884 -4.428894 -4.4289107 -4.4289231 -4.4289165 -4.4289103 -4.4289246 -4.4289374 -4.428936 -4.4289346][-4.4289055 -4.4289007 -4.4288855 -4.4288669 -4.4288507 -4.4288421 -4.4288349 -4.4288387 -4.428843 -4.428833 -4.4288383 -4.4288712 -4.4289012 -4.4289131 -4.4289231]]...]
INFO - root - 2017-12-08 06:03:56.303871: step 18910, loss = 2.28, batch loss = 2.23 (34.4 examples/sec; 0.232 sec/batch; 20h:14m:37s remains)
INFO - root - 2017-12-08 06:03:58.550183: step 18920, loss = 2.28, batch loss = 2.23 (33.4 examples/sec; 0.240 sec/batch; 20h:52m:42s remains)
INFO - root - 2017-12-08 06:04:00.792605: step 18930, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 19h:16m:16s remains)
INFO - root - 2017-12-08 06:04:03.041258: step 18940, loss = 2.28, batch loss = 2.23 (36.7 examples/sec; 0.218 sec/batch; 18h:58m:57s remains)
INFO - root - 2017-12-08 06:04:05.287615: step 18950, loss = 2.28, batch loss = 2.23 (34.6 examples/sec; 0.231 sec/batch; 20h:08m:25s remains)
INFO - root - 2017-12-08 06:04:07.549970: step 18960, loss = 2.28, batch loss = 2.23 (34.9 examples/sec; 0.229 sec/batch; 19h:57m:38s remains)
INFO - root - 2017-12-08 06:04:09.811731: step 18970, loss = 2.28, batch loss = 2.23 (37.1 examples/sec; 0.216 sec/batch; 18h:47m:09s remains)
INFO - root - 2017-12-08 06:04:12.064936: step 18980, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 19h:15m:21s remains)
INFO - root - 2017-12-08 06:04:14.305386: step 18990, loss = 2.28, batch loss = 2.23 (35.8 examples/sec; 0.224 sec/batch; 19h:28m:54s remains)
INFO - root - 2017-12-08 06:04:16.522197: step 19000, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.220 sec/batch; 19h:10m:07s remains)
2017-12-08 06:04:16.814145: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288979 -4.4288917 -4.4288526 -4.428791 -4.4287171 -4.4286518 -4.4285927 -4.4285407 -4.4285502 -4.4286194 -4.4286695 -4.4287062 -4.4287381 -4.428771 -4.4288049][-4.428896 -4.4288816 -4.42884 -4.4287767 -4.4286923 -4.4286194 -4.4285555 -4.4284992 -4.4284964 -4.4285493 -4.4285946 -4.4286432 -4.4286981 -4.4287519 -4.4288][-4.4289141 -4.4289002 -4.42886 -4.428791 -4.4286981 -4.4286165 -4.4285531 -4.428493 -4.4284735 -4.4285073 -4.4285469 -4.4286103 -4.4286819 -4.4287486 -4.428802][-4.4289317 -4.4289236 -4.4288888 -4.4288216 -4.42873 -4.4286427 -4.4285612 -4.4284825 -4.428442 -4.4284644 -4.4285078 -4.42859 -4.4286761 -4.4287443 -4.4287953][-4.42894 -4.4289379 -4.42891 -4.4288416 -4.4287438 -4.428637 -4.4285183 -4.4284058 -4.4283538 -4.4283891 -4.4284596 -4.4285731 -4.4286771 -4.4287448 -4.4287896][-4.4289389 -4.4289351 -4.4289 -4.4288182 -4.4287024 -4.4285679 -4.4283977 -4.4282341 -4.4281883 -4.4282818 -4.4284191 -4.4285727 -4.4286895 -4.4287548 -4.428791][-4.4289274 -4.4289193 -4.4288716 -4.4287696 -4.4286256 -4.4284511 -4.4282203 -4.4279943 -4.4279733 -4.428185 -4.4284105 -4.4285903 -4.4287028 -4.4287558 -4.4287834][-4.4289184 -4.4289036 -4.4288392 -4.4287214 -4.4285626 -4.4283638 -4.4281025 -4.4278483 -4.427897 -4.4282017 -4.4284606 -4.4286246 -4.4287105 -4.4287457 -4.428771][-4.4289155 -4.4288898 -4.4288168 -4.4287062 -4.4285579 -4.4283848 -4.4281845 -4.428031 -4.4281244 -4.42837 -4.42856 -4.4286704 -4.4287243 -4.4287496 -4.4287791][-4.4289122 -4.428875 -4.4288039 -4.4287171 -4.4286118 -4.4284997 -4.42839 -4.428318 -4.4283962 -4.4285512 -4.4286613 -4.4287248 -4.4287577 -4.4287839 -4.4288163][-4.4289045 -4.4288578 -4.428793 -4.42873 -4.4286642 -4.4286008 -4.428546 -4.428504 -4.4285588 -4.4286604 -4.4287286 -4.4287696 -4.4287982 -4.4288273 -4.4288568][-4.4288969 -4.4288406 -4.4287753 -4.42872 -4.4286761 -4.4286432 -4.428616 -4.4285874 -4.4286294 -4.42871 -4.4287658 -4.4288006 -4.4288282 -4.4288559 -4.4288788][-4.4288907 -4.4288259 -4.4287553 -4.4287009 -4.4286671 -4.4286547 -4.4286485 -4.4286356 -4.4286761 -4.4287457 -4.4287953 -4.4288273 -4.428854 -4.4288754 -4.4288921][-4.42889 -4.4288235 -4.4287534 -4.4287033 -4.428678 -4.4286766 -4.4286885 -4.4286933 -4.4287319 -4.4287882 -4.42883 -4.4288568 -4.4288774 -4.4288931 -4.4289083][-4.4288983 -4.4288344 -4.4287705 -4.4287286 -4.4287124 -4.4287176 -4.42874 -4.4287529 -4.4287829 -4.4288249 -4.4288573 -4.4288778 -4.428896 -4.4289126 -4.42893]]...]
INFO - root - 2017-12-08 06:04:19.070071: step 19010, loss = 2.28, batch loss = 2.23 (36.6 examples/sec; 0.219 sec/batch; 19h:02m:11s remains)
INFO - root - 2017-12-08 06:04:21.306900: step 19020, loss = 2.28, batch loss = 2.23 (35.8 examples/sec; 0.224 sec/batch; 19h:28m:24s remains)
INFO - root - 2017-12-08 06:04:23.550848: step 19030, loss = 2.28, batch loss = 2.23 (35.7 examples/sec; 0.224 sec/batch; 19h:30m:32s remains)
INFO - root - 2017-12-08 06:04:25.791368: step 19040, loss = 2.28, batch loss = 2.23 (35.3 examples/sec; 0.227 sec/batch; 19h:43m:41s remains)
INFO - root - 2017-12-08 06:04:28.023678: step 19050, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.225 sec/batch; 19h:37m:09s remains)
INFO - root - 2017-12-08 06:04:30.269425: step 19060, loss = 2.28, batch loss = 2.23 (36.0 examples/sec; 0.222 sec/batch; 19h:20m:16s remains)
INFO - root - 2017-12-08 06:04:32.503202: step 19070, loss = 2.28, batch loss = 2.23 (35.8 examples/sec; 0.223 sec/batch; 19h:27m:25s remains)
INFO - root - 2017-12-08 06:04:34.729643: step 19080, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.221 sec/batch; 19h:12m:22s remains)
INFO - root - 2017-12-08 06:04:36.966002: step 19090, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 19h:07m:41s remains)
INFO - root - 2017-12-08 06:04:39.194976: step 19100, loss = 2.28, batch loss = 2.23 (34.8 examples/sec; 0.230 sec/batch; 20h:00m:53s remains)
2017-12-08 06:04:39.475455: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289427 -4.4289365 -4.428926 -4.4289217 -4.4289207 -4.4289207 -4.4289193 -4.4289169 -4.4289155 -4.4289246 -4.4289389 -4.4289541 -4.4289727 -4.4289851 -4.4289923][-4.4289303 -4.4289241 -4.4289074 -4.428896 -4.4288907 -4.4288855 -4.4288778 -4.4288645 -4.4288573 -4.42887 -4.4288888 -4.4289126 -4.428947 -4.4289722 -4.4289827][-4.4289074 -4.4288921 -4.4288664 -4.4288507 -4.4288425 -4.4288383 -4.4288263 -4.4287977 -4.4287791 -4.4287963 -4.4288149 -4.4288487 -4.4289036 -4.4289474 -4.4289637][-4.4288611 -4.428834 -4.4287977 -4.4287758 -4.4287643 -4.4287596 -4.4287429 -4.4286966 -4.4286647 -4.4286871 -4.4287081 -4.4287567 -4.4288316 -4.4288979 -4.4289303][-4.4287982 -4.4287629 -4.428719 -4.4286957 -4.42869 -4.4286814 -4.4286532 -4.4285846 -4.4285336 -4.4285603 -4.4285903 -4.4286623 -4.4287586 -4.4288425 -4.42889][-4.4287291 -4.4286852 -4.4286304 -4.4286017 -4.428597 -4.4285827 -4.4285417 -4.4284515 -4.4283895 -4.42843 -4.4284892 -4.4285932 -4.4287109 -4.4288011 -4.4288526][-4.4286666 -4.4286036 -4.42853 -4.4284883 -4.4284778 -4.4284511 -4.4283962 -4.4283032 -4.428256 -4.4283347 -4.4284344 -4.4285622 -4.4286871 -4.4287786 -4.4288311][-4.4286628 -4.4285827 -4.4284792 -4.4284058 -4.4283738 -4.4283314 -4.4282761 -4.4282217 -4.4282217 -4.4283371 -4.4284606 -4.4285917 -4.4287038 -4.4287858 -4.4288378][-4.4287252 -4.4286418 -4.4285169 -4.4284215 -4.4283786 -4.4283295 -4.42829 -4.4282761 -4.4283085 -4.428432 -4.4285522 -4.4286609 -4.4287491 -4.4288187 -4.428865][-4.4287777 -4.4287043 -4.42859 -4.4285131 -4.428483 -4.4284453 -4.4284177 -4.4284134 -4.4284377 -4.4285407 -4.4286423 -4.4287281 -4.428802 -4.428865 -4.4289002][-4.4288154 -4.4287634 -4.4286842 -4.4286513 -4.4286513 -4.4286294 -4.4286051 -4.4285879 -4.4285884 -4.4286633 -4.4287386 -4.4288096 -4.4288712 -4.4289193 -4.4289393][-4.4288621 -4.4288368 -4.4287992 -4.4288 -4.4288158 -4.4288011 -4.4287767 -4.4287462 -4.4287262 -4.4287696 -4.4288259 -4.428885 -4.4289312 -4.4289651 -4.4289765][-4.428915 -4.4289155 -4.4289079 -4.4289203 -4.4289355 -4.4289279 -4.4289126 -4.4288831 -4.428853 -4.4288659 -4.428894 -4.4289317 -4.4289637 -4.4289889 -4.4290004][-4.4289346 -4.4289446 -4.4289489 -4.4289622 -4.4289761 -4.428977 -4.4289727 -4.4289541 -4.4289284 -4.4289236 -4.4289303 -4.42895 -4.4289727 -4.4289966 -4.4290104][-4.4289408 -4.4289484 -4.4289541 -4.4289656 -4.4289756 -4.42898 -4.4289808 -4.4289722 -4.4289579 -4.4289517 -4.4289527 -4.4289637 -4.42898 -4.428998 -4.4290104]]...]
INFO - root - 2017-12-08 06:04:41.701182: step 19110, loss = 2.28, batch loss = 2.23 (36.6 examples/sec; 0.219 sec/batch; 19h:01m:34s remains)
INFO - root - 2017-12-08 06:04:43.938718: step 19120, loss = 2.28, batch loss = 2.23 (35.4 examples/sec; 0.226 sec/batch; 19h:40m:20s remains)
INFO - root - 2017-12-08 06:04:46.178852: step 19130, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.225 sec/batch; 19h:35m:37s remains)
INFO - root - 2017-12-08 06:04:48.384779: step 19140, loss = 2.28, batch loss = 2.23 (37.3 examples/sec; 0.214 sec/batch; 18h:38m:53s remains)
INFO - root - 2017-12-08 06:04:50.640056: step 19150, loss = 2.28, batch loss = 2.23 (36.8 examples/sec; 0.218 sec/batch; 18h:56m:04s remains)
INFO - root - 2017-12-08 06:04:52.869801: step 19160, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.221 sec/batch; 19h:16m:29s remains)
INFO - root - 2017-12-08 06:04:55.100407: step 19170, loss = 2.28, batch loss = 2.23 (35.6 examples/sec; 0.224 sec/batch; 19h:32m:11s remains)
INFO - root - 2017-12-08 06:04:57.363793: step 19180, loss = 2.28, batch loss = 2.23 (35.6 examples/sec; 0.225 sec/batch; 19h:33m:01s remains)
INFO - root - 2017-12-08 06:04:59.630985: step 19190, loss = 2.28, batch loss = 2.23 (35.3 examples/sec; 0.227 sec/batch; 19h:44m:42s remains)
INFO - root - 2017-12-08 06:05:01.909341: step 19200, loss = 2.28, batch loss = 2.23 (32.3 examples/sec; 0.247 sec/batch; 21h:32m:15s remains)
2017-12-08 06:05:02.238521: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289474 -4.4289374 -4.428916 -4.4288878 -4.4288578 -4.4288144 -4.4287629 -4.4287329 -4.4287348 -4.4287548 -4.4287834 -4.4288263 -4.4288721 -4.4289107 -4.4289365][-4.4289336 -4.428916 -4.4288878 -4.4288526 -4.428813 -4.4287705 -4.4287267 -4.4286981 -4.4287043 -4.4287376 -4.4287853 -4.4288354 -4.4288845 -4.4289207 -4.4289432][-4.4289069 -4.4288878 -4.428865 -4.4288282 -4.4287834 -4.4287424 -4.4287 -4.4286637 -4.4286642 -4.4287086 -4.4287715 -4.42883 -4.4288831 -4.4289193 -4.4289422][-4.4288583 -4.4288435 -4.4288344 -4.4288058 -4.4287596 -4.4287252 -4.4286861 -4.4286447 -4.4286389 -4.4286876 -4.4287615 -4.4288282 -4.4288816 -4.428916 -4.4289403][-4.4288092 -4.4288063 -4.4288177 -4.4288073 -4.4287663 -4.4287338 -4.428688 -4.42864 -4.4286284 -4.4286766 -4.4287553 -4.4288292 -4.4288845 -4.4289179 -4.4289436][-4.4287643 -4.42878 -4.428812 -4.4288206 -4.428793 -4.4287558 -4.4286866 -4.4286194 -4.4286051 -4.4286551 -4.4287391 -4.4288225 -4.4288869 -4.4289265 -4.4289517][-4.4287319 -4.4287596 -4.4288054 -4.4288273 -4.4288082 -4.4287663 -4.4286785 -4.4285855 -4.4285641 -4.4286218 -4.4287214 -4.4288177 -4.4288917 -4.428937 -4.4289603][-4.4286895 -4.4287329 -4.4287915 -4.4288177 -4.4288054 -4.4287686 -4.4286733 -4.4285545 -4.428524 -4.4285884 -4.4287052 -4.42881 -4.4288926 -4.4289451 -4.4289675][-4.4286394 -4.4287028 -4.4287724 -4.428802 -4.4287972 -4.4287672 -4.4286647 -4.4285297 -4.4284987 -4.4285669 -4.4286981 -4.4288082 -4.428896 -4.4289517 -4.4289732][-4.4286036 -4.4286795 -4.4287548 -4.4287806 -4.4287782 -4.4287596 -4.4286618 -4.4285312 -4.4285092 -4.4285822 -4.4287114 -4.4288168 -4.4289036 -4.4289589 -4.428978][-4.4286084 -4.4286709 -4.4287329 -4.4287534 -4.4287553 -4.428751 -4.4286709 -4.4285727 -4.4285746 -4.4286385 -4.4287405 -4.4288297 -4.4289112 -4.4289646 -4.4289813][-4.4286394 -4.4286857 -4.4287362 -4.4287586 -4.4287663 -4.4287691 -4.4287128 -4.4286561 -4.4286747 -4.4287119 -4.4287763 -4.4288473 -4.4289203 -4.4289689 -4.4289837][-4.4286952 -4.4287281 -4.4287648 -4.4287825 -4.4287844 -4.4287829 -4.4287448 -4.4287214 -4.4287524 -4.4287715 -4.42881 -4.4288692 -4.4289312 -4.4289732 -4.4289865][-4.4287372 -4.428762 -4.4287863 -4.428792 -4.4287834 -4.4287744 -4.4287505 -4.4287534 -4.428793 -4.4288054 -4.4288335 -4.4288859 -4.42894 -4.4289765 -4.428988][-4.4287648 -4.4287963 -4.4288177 -4.4288106 -4.4287882 -4.4287715 -4.4287519 -4.4287615 -4.4287977 -4.4288087 -4.4288378 -4.4288907 -4.4289408 -4.4289737 -4.4289842]]...]
INFO - root - 2017-12-08 06:05:04.520002: step 19210, loss = 2.28, batch loss = 2.23 (36.0 examples/sec; 0.222 sec/batch; 19h:21m:08s remains)
INFO - root - 2017-12-08 06:05:06.751352: step 19220, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 19h:14m:05s remains)
INFO - root - 2017-12-08 06:05:08.982006: step 19230, loss = 2.28, batch loss = 2.23 (35.8 examples/sec; 0.223 sec/batch; 19h:26m:24s remains)
INFO - root - 2017-12-08 06:05:11.215716: step 19240, loss = 2.28, batch loss = 2.23 (35.3 examples/sec; 0.226 sec/batch; 19h:41m:36s remains)
INFO - root - 2017-12-08 06:05:13.486101: step 19250, loss = 2.28, batch loss = 2.23 (35.6 examples/sec; 0.225 sec/batch; 19h:34m:32s remains)
INFO - root - 2017-12-08 06:05:15.737247: step 19260, loss = 2.28, batch loss = 2.23 (35.8 examples/sec; 0.223 sec/batch; 19h:25m:47s remains)
INFO - root - 2017-12-08 06:05:17.964774: step 19270, loss = 2.28, batch loss = 2.23 (36.9 examples/sec; 0.217 sec/batch; 18h:52m:45s remains)
INFO - root - 2017-12-08 06:05:20.220768: step 19280, loss = 2.28, batch loss = 2.23 (34.9 examples/sec; 0.229 sec/batch; 19h:55m:17s remains)
INFO - root - 2017-12-08 06:05:22.497205: step 19290, loss = 2.28, batch loss = 2.23 (32.7 examples/sec; 0.244 sec/batch; 21h:15m:17s remains)
INFO - root - 2017-12-08 06:05:24.731291: step 19300, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.221 sec/batch; 19h:15m:29s remains)
2017-12-08 06:05:25.013438: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289355 -4.42892 -4.4289112 -4.4289103 -4.4289002 -4.428875 -4.4288483 -4.4288483 -4.428863 -4.4288645 -4.4288692 -4.4288821 -4.4288883 -4.4288759 -4.4288483][-4.4289351 -4.4289179 -4.4289079 -4.4289007 -4.428884 -4.4288507 -4.428812 -4.4288096 -4.4288363 -4.4288492 -4.4288578 -4.4288664 -4.4288645 -4.4288363 -4.4287872][-4.4289351 -4.4289207 -4.4289083 -4.4288907 -4.4288616 -4.4288177 -4.4287739 -4.4287729 -4.4288092 -4.4288359 -4.4288597 -4.4288716 -4.4288588 -4.4288058 -4.4287248][-4.428937 -4.4289207 -4.4289055 -4.4288807 -4.42884 -4.4287858 -4.4287391 -4.42874 -4.4287868 -4.4288287 -4.4288688 -4.4288931 -4.42888 -4.4288077 -4.4286938][-4.42893 -4.4289088 -4.42889 -4.4288611 -4.428812 -4.4287524 -4.4287028 -4.4286966 -4.4287386 -4.4287925 -4.4288478 -4.4288917 -4.4288993 -4.4288325 -4.4287076][-4.4289236 -4.4288993 -4.4288778 -4.428844 -4.4287848 -4.42871 -4.4286342 -4.4285984 -4.4286294 -4.4286962 -4.4287744 -4.4288526 -4.428896 -4.4288607 -4.428751][-4.4289207 -4.4288969 -4.4288783 -4.4288411 -4.4287691 -4.4286704 -4.4285531 -4.428473 -4.4284844 -4.4285665 -4.4286771 -4.428791 -4.4288673 -4.4288635 -4.4287891][-4.4289117 -4.4288907 -4.4288745 -4.4288354 -4.4287581 -4.4286456 -4.4285021 -4.4283872 -4.4283814 -4.4284711 -4.4286084 -4.4287405 -4.4288244 -4.4288435 -4.4288139][-4.4288859 -4.4288592 -4.4288411 -4.428803 -4.4287376 -4.4286466 -4.4285188 -4.4284053 -4.4283934 -4.4284763 -4.428607 -4.428721 -4.4287829 -4.4287996 -4.4287972][-4.4288559 -4.428823 -4.4288073 -4.4287767 -4.4287329 -4.4286771 -4.4285922 -4.42851 -4.4285097 -4.4285841 -4.4286838 -4.42875 -4.4287658 -4.4287591 -4.4287548][-4.4288464 -4.4288158 -4.428802 -4.4287782 -4.4287481 -4.4287133 -4.4286585 -4.4286013 -4.428617 -4.4286914 -4.4287667 -4.4287939 -4.4287782 -4.4287519 -4.4287319][-4.4288487 -4.4288287 -4.4288273 -4.4288177 -4.4287944 -4.4287658 -4.428721 -4.428678 -4.4287028 -4.4287705 -4.4288311 -4.4288387 -4.4288125 -4.4287772 -4.4287472][-4.42887 -4.4288592 -4.4288716 -4.4288812 -4.4288716 -4.4288526 -4.4288154 -4.4287834 -4.4288092 -4.4288564 -4.4288912 -4.4288807 -4.428853 -4.428822 -4.4287925][-4.4289002 -4.4288974 -4.4289207 -4.4289451 -4.4289479 -4.428936 -4.4289064 -4.4288769 -4.4288912 -4.4289141 -4.428925 -4.4289069 -4.4288826 -4.4288597 -4.4288387][-4.4289193 -4.4289174 -4.4289408 -4.4289689 -4.4289789 -4.4289746 -4.428956 -4.4289322 -4.4289308 -4.4289346 -4.4289317 -4.4289112 -4.4288874 -4.4288721 -4.4288611]]...]
INFO - root - 2017-12-08 06:05:27.240275: step 19310, loss = 2.28, batch loss = 2.23 (36.0 examples/sec; 0.222 sec/batch; 19h:20m:54s remains)
INFO - root - 2017-12-08 06:05:29.500887: step 19320, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 19h:04m:26s remains)
INFO - root - 2017-12-08 06:05:31.725228: step 19330, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.220 sec/batch; 19h:10m:22s remains)
INFO - root - 2017-12-08 06:05:33.961178: step 19340, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 19h:02m:45s remains)
INFO - root - 2017-12-08 06:05:36.188066: step 19350, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.220 sec/batch; 19h:10m:12s remains)
INFO - root - 2017-12-08 06:05:38.430309: step 19360, loss = 2.28, batch loss = 2.23 (35.1 examples/sec; 0.228 sec/batch; 19h:48m:50s remains)
INFO - root - 2017-12-08 06:05:40.692168: step 19370, loss = 2.28, batch loss = 2.23 (34.0 examples/sec; 0.235 sec/batch; 20h:26m:51s remains)
INFO - root - 2017-12-08 06:05:42.956214: step 19380, loss = 2.28, batch loss = 2.23 (35.8 examples/sec; 0.223 sec/batch; 19h:25m:22s remains)
INFO - root - 2017-12-08 06:05:45.224413: step 19390, loss = 2.28, batch loss = 2.23 (33.6 examples/sec; 0.238 sec/batch; 20h:43m:53s remains)
INFO - root - 2017-12-08 06:05:47.456798: step 19400, loss = 2.28, batch loss = 2.23 (34.8 examples/sec; 0.230 sec/batch; 19h:58m:36s remains)
2017-12-08 06:05:47.747934: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4287858 -4.4287786 -4.4287925 -4.4288116 -4.4288011 -4.4287744 -4.4287529 -4.4287338 -4.4287205 -4.4287262 -4.4287271 -4.4287171 -4.4287186 -4.4287295 -4.4287462][-4.4287634 -4.4287624 -4.42878 -4.4288068 -4.42881 -4.4287939 -4.4287763 -4.4287534 -4.4287386 -4.4287391 -4.4287286 -4.4287162 -4.4287157 -4.4287171 -4.4287186][-4.4287362 -4.4287438 -4.4287696 -4.4287977 -4.4288058 -4.4287949 -4.4287748 -4.4287534 -4.4287424 -4.4287457 -4.4287405 -4.4287343 -4.4287257 -4.42871 -4.4286895][-4.4287014 -4.4287229 -4.4287624 -4.4287925 -4.4288034 -4.4287915 -4.4287605 -4.4287367 -4.428731 -4.4287391 -4.42875 -4.4287624 -4.428751 -4.4287176 -4.42867][-4.4286819 -4.4287009 -4.4287419 -4.4287734 -4.4287806 -4.4287639 -4.4287257 -4.4286995 -4.428699 -4.4287076 -4.4287367 -4.4287791 -4.4287868 -4.42876 -4.4287052][-4.4287157 -4.4287171 -4.4287367 -4.4287477 -4.4287391 -4.4287086 -4.4286642 -4.4286447 -4.4286523 -4.428668 -4.4287105 -4.4287739 -4.4288073 -4.4288063 -4.4287782][-4.4287653 -4.4287467 -4.4287338 -4.4287167 -4.4286828 -4.428628 -4.4285703 -4.4285541 -4.4285793 -4.4286132 -4.4286695 -4.4287496 -4.4288068 -4.4288387 -4.4288425][-4.4287939 -4.4287629 -4.4287271 -4.4286857 -4.4286284 -4.4285507 -4.4284678 -4.4284415 -4.4284825 -4.4285421 -4.428618 -4.4287114 -4.4287891 -4.4288459 -4.4288745][-4.428803 -4.4287663 -4.4287224 -4.4286714 -4.4285979 -4.4284954 -4.4283791 -4.4283242 -4.4283667 -4.4284515 -4.4285502 -4.4286542 -4.4287467 -4.4288168 -4.4288573][-4.4288177 -4.4287796 -4.4287372 -4.4286923 -4.4286251 -4.4285231 -4.4283981 -4.4283185 -4.4283419 -4.4284296 -4.4285355 -4.4286427 -4.4287357 -4.4288039 -4.4288406][-4.4288769 -4.42884 -4.4288039 -4.4287772 -4.4287372 -4.4286695 -4.4285827 -4.4285221 -4.4285254 -4.4285784 -4.4286523 -4.4287367 -4.4288111 -4.4288621 -4.4288816][-4.4289412 -4.4289117 -4.428885 -4.4288735 -4.4288588 -4.4288278 -4.4287848 -4.4287562 -4.4287562 -4.4287791 -4.42882 -4.4288735 -4.4289184 -4.4289432 -4.4289451][-4.4289746 -4.4289536 -4.4289365 -4.4289341 -4.4289293 -4.428915 -4.4288988 -4.4288931 -4.428896 -4.428906 -4.4289255 -4.4289541 -4.428977 -4.4289861 -4.4289813][-4.428988 -4.4289746 -4.4289637 -4.4289618 -4.4289579 -4.4289508 -4.4289441 -4.4289451 -4.4289489 -4.4289532 -4.4289622 -4.428978 -4.4289908 -4.4289951 -4.4289937][-4.4289961 -4.42899 -4.4289827 -4.4289794 -4.4289751 -4.4289703 -4.428968 -4.4289694 -4.4289718 -4.4289742 -4.4289794 -4.4289889 -4.4289966 -4.4290004 -4.4290018]]...]
INFO - root - 2017-12-08 06:05:49.974361: step 19410, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 19h:07m:06s remains)
INFO - root - 2017-12-08 06:05:52.194976: step 19420, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 19h:14m:31s remains)
INFO - root - 2017-12-08 06:05:54.466539: step 19430, loss = 2.28, batch loss = 2.23 (34.9 examples/sec; 0.229 sec/batch; 19h:56m:05s remains)
INFO - root - 2017-12-08 06:05:56.766753: step 19440, loss = 2.28, batch loss = 2.23 (33.2 examples/sec; 0.241 sec/batch; 20h:57m:35s remains)
INFO - root - 2017-12-08 06:05:59.009407: step 19450, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.225 sec/batch; 19h:35m:11s remains)
INFO - root - 2017-12-08 06:06:01.257339: step 19460, loss = 2.28, batch loss = 2.23 (35.7 examples/sec; 0.224 sec/batch; 19h:30m:32s remains)
INFO - root - 2017-12-08 06:06:03.484406: step 19470, loss = 2.28, batch loss = 2.23 (35.3 examples/sec; 0.227 sec/batch; 19h:43m:06s remains)
INFO - root - 2017-12-08 06:06:05.703285: step 19480, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 19h:13m:52s remains)
INFO - root - 2017-12-08 06:06:07.949601: step 19490, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 19h:23m:03s remains)
INFO - root - 2017-12-08 06:06:10.202328: step 19500, loss = 2.28, batch loss = 2.23 (37.1 examples/sec; 0.215 sec/batch; 18h:44m:08s remains)
2017-12-08 06:06:10.482856: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4287462 -4.4287314 -4.428721 -4.4287038 -4.4286852 -4.4286909 -4.4287195 -4.4287438 -4.4287615 -4.4287519 -4.4287229 -4.4286842 -4.4286747 -4.4286728 -4.4286652][-4.4287558 -4.4287481 -4.4287486 -4.4287419 -4.4287281 -4.42873 -4.4287519 -4.4287691 -4.4287739 -4.4287596 -4.4287305 -4.4287004 -4.4286942 -4.4286947 -4.4286776][-4.4287939 -4.4288058 -4.4288182 -4.428822 -4.4288087 -4.4287992 -4.4288034 -4.4287972 -4.42878 -4.4287605 -4.4287381 -4.4287176 -4.4287124 -4.4287095 -4.4286766][-4.4288449 -4.4288707 -4.4288878 -4.4288931 -4.428874 -4.4288511 -4.4288344 -4.4288025 -4.4287643 -4.4287434 -4.428731 -4.42872 -4.4287114 -4.4286928 -4.4286466][-4.4288774 -4.4289045 -4.4289155 -4.428916 -4.4288926 -4.4288568 -4.4288173 -4.4287634 -4.4287148 -4.4286976 -4.4287028 -4.4287066 -4.4286962 -4.4286666 -4.4286113][-4.428874 -4.428894 -4.428894 -4.428885 -4.4288578 -4.42881 -4.4287496 -4.4286785 -4.4286289 -4.4286251 -4.4286532 -4.4286819 -4.4286866 -4.4286628 -4.4286113][-4.4288387 -4.4288445 -4.4288297 -4.4288068 -4.428771 -4.4287143 -4.4286451 -4.4285722 -4.428535 -4.4285574 -4.4286137 -4.4286637 -4.428688 -4.4286809 -4.4286485][-4.4287844 -4.428772 -4.4287462 -4.4287119 -4.4286714 -4.428618 -4.428556 -4.4284978 -4.4284825 -4.4285283 -4.4286008 -4.4286561 -4.4286852 -4.4286938 -4.4286852][-4.4287219 -4.4286876 -4.4286537 -4.4286208 -4.4285917 -4.4285631 -4.4285321 -4.4285064 -4.4285116 -4.4285569 -4.4286175 -4.4286585 -4.4286718 -4.4286823 -4.4286923][-4.4286594 -4.4286113 -4.4285855 -4.4285707 -4.4285674 -4.4285669 -4.4285645 -4.4285631 -4.4285779 -4.4286132 -4.4286551 -4.4286728 -4.4286628 -4.4286628 -4.428678][-4.4286308 -4.4285884 -4.4285793 -4.4285822 -4.4285941 -4.4286056 -4.428607 -4.4286079 -4.4286242 -4.4286551 -4.4286861 -4.4286914 -4.4286704 -4.4286623 -4.4286742][-4.4286537 -4.42862 -4.4286141 -4.4286175 -4.4286261 -4.4286351 -4.428637 -4.4286366 -4.4286532 -4.4286861 -4.4287124 -4.4287138 -4.4286976 -4.4286919 -4.4287009][-4.428709 -4.4286771 -4.428659 -4.4286504 -4.4286494 -4.4286575 -4.4286637 -4.4286661 -4.4286838 -4.4287148 -4.4287381 -4.4287415 -4.4287329 -4.4287314 -4.4287386][-4.4287553 -4.428721 -4.4286876 -4.4286661 -4.4286594 -4.428668 -4.428678 -4.4286866 -4.4287071 -4.4287362 -4.4287605 -4.4287677 -4.4287643 -4.4287634 -4.4287682][-4.4287724 -4.4287372 -4.4286923 -4.4286671 -4.4286671 -4.42868 -4.4286942 -4.4287081 -4.4287243 -4.4287457 -4.4287663 -4.4287744 -4.4287729 -4.4287748 -4.4287791]]...]
INFO - root - 2017-12-08 06:06:12.721578: step 19510, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 19h:12m:49s remains)
INFO - root - 2017-12-08 06:06:14.944994: step 19520, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 19h:04m:11s remains)
INFO - root - 2017-12-08 06:06:17.179057: step 19530, loss = 2.28, batch loss = 2.23 (35.6 examples/sec; 0.224 sec/batch; 19h:30m:58s remains)
INFO - root - 2017-12-08 06:06:19.410610: step 19540, loss = 2.28, batch loss = 2.23 (36.8 examples/sec; 0.218 sec/batch; 18h:55m:13s remains)
INFO - root - 2017-12-08 06:06:21.690971: step 19550, loss = 2.28, batch loss = 2.23 (32.7 examples/sec; 0.245 sec/batch; 21h:15m:50s remains)
INFO - root - 2017-12-08 06:06:23.963523: step 19560, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.222 sec/batch; 19h:16m:15s remains)
INFO - root - 2017-12-08 06:06:26.188703: step 19570, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.221 sec/batch; 19h:14m:15s remains)
INFO - root - 2017-12-08 06:06:28.462802: step 19580, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 19h:11m:39s remains)
INFO - root - 2017-12-08 06:06:30.673448: step 19590, loss = 2.28, batch loss = 2.23 (35.7 examples/sec; 0.224 sec/batch; 19h:29m:56s remains)
INFO - root - 2017-12-08 06:06:32.920206: step 19600, loss = 2.28, batch loss = 2.23 (35.3 examples/sec; 0.227 sec/batch; 19h:42m:00s remains)
2017-12-08 06:06:33.191776: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289517 -4.4289513 -4.42895 -4.4289546 -4.428956 -4.4289432 -4.428915 -4.4288774 -4.4288363 -4.4287996 -4.4287887 -4.4288087 -4.4288297 -4.4288487 -4.428875][-4.4289503 -4.4289436 -4.4289393 -4.4289465 -4.42895 -4.42893 -4.4288855 -4.42883 -4.4287815 -4.4287429 -4.4287276 -4.4287467 -4.428772 -4.4287963 -4.4288297][-4.4289355 -4.4289212 -4.4289103 -4.4289131 -4.4289107 -4.4288788 -4.4288158 -4.4287477 -4.4287024 -4.428679 -4.4286737 -4.4286942 -4.4287195 -4.4287467 -4.428781][-4.428894 -4.4288669 -4.4288473 -4.4288421 -4.4288287 -4.4287891 -4.4287219 -4.4286451 -4.4286175 -4.4286304 -4.4286456 -4.4286647 -4.4286752 -4.4286995 -4.4287329][-4.4288316 -4.4287853 -4.42875 -4.4287348 -4.4287086 -4.4286623 -4.4285851 -4.4284983 -4.4285111 -4.4285941 -4.428647 -4.4286637 -4.4286613 -4.4286776 -4.4287][-4.4287586 -4.4286885 -4.428627 -4.4285975 -4.4285583 -4.4284968 -4.4283881 -4.4282727 -4.4283586 -4.4285469 -4.4286523 -4.4286742 -4.4286747 -4.428688 -4.4286914][-4.4286895 -4.428597 -4.4285007 -4.4284468 -4.4283862 -4.4282937 -4.4281435 -4.428 -4.4281845 -4.428483 -4.4286361 -4.428668 -4.4286914 -4.4287095 -4.4286971][-4.4286489 -4.4285536 -4.4284406 -4.4283705 -4.4282875 -4.4281621 -4.4279761 -4.4278235 -4.4280548 -4.4283924 -4.4285693 -4.428616 -4.4286604 -4.4286885 -4.4286852][-4.4286523 -4.4285727 -4.4284763 -4.4284153 -4.4283385 -4.4282217 -4.4280481 -4.427918 -4.4280834 -4.428349 -4.428503 -4.4285526 -4.4286003 -4.4286394 -4.4286671][-4.4287148 -4.4286566 -4.4285893 -4.428555 -4.4284973 -4.4284029 -4.4282527 -4.428123 -4.428184 -4.4283376 -4.4284549 -4.4284997 -4.42855 -4.4286108 -4.4286728][-4.4288077 -4.4287691 -4.4287271 -4.428709 -4.4286718 -4.4286008 -4.4284821 -4.4283652 -4.42836 -4.4284306 -4.428515 -4.4285531 -4.4286051 -4.4286728 -4.4287381][-4.4288979 -4.4288731 -4.4288492 -4.428844 -4.4288187 -4.4287653 -4.4286842 -4.4286013 -4.4285836 -4.4286194 -4.42868 -4.4287176 -4.4287639 -4.4288087 -4.4288454][-4.4289632 -4.42895 -4.4289379 -4.4289365 -4.4289184 -4.4288783 -4.4288235 -4.4287786 -4.4287767 -4.4288082 -4.4288549 -4.4288898 -4.4289212 -4.4289384 -4.4289455][-4.4289923 -4.4289861 -4.4289794 -4.428977 -4.4289618 -4.42893 -4.4288945 -4.4288759 -4.4288917 -4.4289231 -4.42896 -4.4289856 -4.4290028 -4.4290018 -4.4289951][-4.4289942 -4.4289932 -4.4289889 -4.4289856 -4.4289737 -4.4289508 -4.428926 -4.4289174 -4.4289341 -4.4289608 -4.4289894 -4.4290071 -4.4290156 -4.4290104 -4.4290023]]...]
INFO - root - 2017-12-08 06:06:35.377045: step 19610, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 19h:01m:58s remains)
INFO - root - 2017-12-08 06:06:37.652470: step 19620, loss = 2.28, batch loss = 2.23 (35.1 examples/sec; 0.228 sec/batch; 19h:46m:53s remains)
INFO - root - 2017-12-08 06:06:39.911173: step 19630, loss = 2.28, batch loss = 2.23 (36.9 examples/sec; 0.217 sec/batch; 18h:51m:19s remains)
INFO - root - 2017-12-08 06:06:42.147372: step 19640, loss = 2.28, batch loss = 2.23 (37.3 examples/sec; 0.215 sec/batch; 18h:39m:30s remains)
INFO - root - 2017-12-08 06:06:44.379641: step 19650, loss = 2.28, batch loss = 2.23 (35.8 examples/sec; 0.224 sec/batch; 19h:25m:27s remains)
INFO - root - 2017-12-08 06:06:46.597219: step 19660, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 19h:06m:31s remains)
INFO - root - 2017-12-08 06:06:48.841063: step 19670, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.225 sec/batch; 19h:34m:26s remains)
INFO - root - 2017-12-08 06:06:51.102040: step 19680, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.222 sec/batch; 19h:16m:35s remains)
INFO - root - 2017-12-08 06:06:53.317331: step 19690, loss = 2.28, batch loss = 2.23 (35.6 examples/sec; 0.224 sec/batch; 19h:30m:02s remains)
INFO - root - 2017-12-08 06:06:55.554880: step 19700, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 19h:07m:14s remains)
2017-12-08 06:06:55.862180: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289017 -4.428865 -4.4288044 -4.4287314 -4.4286809 -4.4286838 -4.4287167 -4.4287276 -4.4286947 -4.4286451 -4.4286432 -4.4286947 -4.4287386 -4.4287648 -4.4287925][-4.4289126 -4.4288874 -4.4288397 -4.4287724 -4.4287105 -4.428688 -4.4286976 -4.4286919 -4.42865 -4.4286032 -4.4286232 -4.4286947 -4.4287515 -4.4287863 -4.4288173][-4.4289222 -4.4289093 -4.42887 -4.428803 -4.4287295 -4.4286823 -4.4286675 -4.4286466 -4.4286013 -4.4285588 -4.4285889 -4.4286695 -4.4287438 -4.4287949 -4.4288316][-4.4289255 -4.4289217 -4.4288859 -4.42882 -4.428741 -4.4286747 -4.4286423 -4.4286189 -4.4285803 -4.4285464 -4.4285769 -4.4286528 -4.4287353 -4.4287987 -4.42884][-4.4289179 -4.4289165 -4.428884 -4.428823 -4.4287457 -4.42867 -4.4286327 -4.4286246 -4.4286089 -4.4285927 -4.4286208 -4.428678 -4.4287462 -4.4288063 -4.4288473][-4.4288945 -4.4288878 -4.4288521 -4.428791 -4.4287124 -4.4286404 -4.4286132 -4.428628 -4.4286475 -4.42866 -4.4286938 -4.428731 -4.4287763 -4.4288292 -4.4288659][-4.4288588 -4.4288397 -4.4287939 -4.4287243 -4.4286394 -4.4285741 -4.4285645 -4.4286051 -4.4286489 -4.4286914 -4.4287415 -4.4287734 -4.4288092 -4.4288583 -4.4288893][-4.428822 -4.4287868 -4.4287252 -4.4286456 -4.4285617 -4.4285054 -4.4285126 -4.4285774 -4.4286427 -4.4287057 -4.4287705 -4.4288111 -4.4288449 -4.4288826 -4.4289036][-4.4287877 -4.42874 -4.428669 -4.4285932 -4.42852 -4.4284782 -4.4285069 -4.4285879 -4.4286604 -4.4287291 -4.4288006 -4.4288459 -4.4288764 -4.4288993 -4.428906][-4.4287648 -4.4287133 -4.4286461 -4.4285855 -4.4285326 -4.4285078 -4.4285488 -4.4286275 -4.4287 -4.4287696 -4.4288354 -4.4288716 -4.4288898 -4.428895 -4.4288921][-4.4287653 -4.4287224 -4.4286642 -4.4286146 -4.428575 -4.4285607 -4.4286013 -4.4286737 -4.4287448 -4.4288125 -4.4288635 -4.4288821 -4.4288821 -4.4288721 -4.4288654][-4.4287815 -4.4287519 -4.4287071 -4.4286647 -4.4286313 -4.4286165 -4.4286456 -4.4287119 -4.4287844 -4.4288459 -4.4288793 -4.42888 -4.4288683 -4.4288521 -4.4288468][-4.4287996 -4.4287825 -4.4287539 -4.4287243 -4.4286933 -4.4286757 -4.4286919 -4.4287453 -4.4288082 -4.4288568 -4.4288769 -4.4288716 -4.4288616 -4.4288487 -4.4288464][-4.428812 -4.4288054 -4.4287939 -4.42878 -4.42876 -4.4287453 -4.42875 -4.4287791 -4.4288197 -4.4288487 -4.4288588 -4.4288564 -4.4288549 -4.4288511 -4.4288521][-4.4288168 -4.4288111 -4.42881 -4.4288125 -4.42881 -4.4288073 -4.4288054 -4.428813 -4.4288268 -4.4288373 -4.4288406 -4.4288411 -4.4288445 -4.4288468 -4.4288535]]...]
INFO - root - 2017-12-08 06:06:58.111282: step 19710, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.225 sec/batch; 19h:34m:56s remains)
INFO - root - 2017-12-08 06:07:00.363989: step 19720, loss = 2.28, batch loss = 2.23 (32.0 examples/sec; 0.250 sec/batch; 21h:41m:31s remains)
INFO - root - 2017-12-08 06:07:02.599292: step 19730, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 19h:01m:21s remains)
INFO - root - 2017-12-08 06:07:04.836302: step 19740, loss = 2.28, batch loss = 2.23 (35.0 examples/sec; 0.229 sec/batch; 19h:53m:05s remains)
INFO - root - 2017-12-08 06:07:07.082081: step 19750, loss = 2.28, batch loss = 2.23 (34.1 examples/sec; 0.235 sec/batch; 20h:24m:29s remains)
INFO - root - 2017-12-08 06:07:09.329285: step 19760, loss = 2.28, batch loss = 2.23 (35.0 examples/sec; 0.228 sec/batch; 19h:50m:53s remains)
INFO - root - 2017-12-08 06:07:11.577130: step 19770, loss = 2.28, batch loss = 2.23 (34.8 examples/sec; 0.230 sec/batch; 19h:57m:23s remains)
INFO - root - 2017-12-08 06:07:13.831963: step 19780, loss = 2.28, batch loss = 2.23 (35.1 examples/sec; 0.228 sec/batch; 19h:48m:36s remains)
INFO - root - 2017-12-08 06:07:16.068411: step 19790, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 19h:22m:29s remains)
INFO - root - 2017-12-08 06:07:18.311047: step 19800, loss = 2.28, batch loss = 2.23 (34.8 examples/sec; 0.230 sec/batch; 19h:58m:05s remains)
2017-12-08 06:07:18.602443: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4287157 -4.4287524 -4.4287825 -4.428782 -4.4287887 -4.4287953 -4.4287992 -4.4288216 -4.4288435 -4.4288425 -4.4288144 -4.4287515 -4.4286742 -4.4286165 -4.4286194][-4.4287243 -4.4287515 -4.4287758 -4.4287744 -4.4287777 -4.4287829 -4.4287739 -4.4287753 -4.428793 -4.4288006 -4.4287758 -4.4287133 -4.4286146 -4.4285245 -4.4285183][-4.428781 -4.4287925 -4.4288006 -4.4287882 -4.4287767 -4.4287724 -4.4287438 -4.4287119 -4.4287157 -4.4287338 -4.4287286 -4.4286804 -4.4285626 -4.4284472 -4.4284329][-4.4288392 -4.4288282 -4.4288158 -4.4287906 -4.4287653 -4.4287486 -4.4286995 -4.4286265 -4.4286084 -4.4286408 -4.4286704 -4.4286575 -4.4285488 -4.428421 -4.428391][-4.42887 -4.4288344 -4.4287977 -4.4287581 -4.4287229 -4.4287033 -4.428647 -4.4285436 -4.4284921 -4.4285293 -4.4285932 -4.4286256 -4.4285564 -4.4284377 -4.4283843][-4.4288831 -4.4288268 -4.4287677 -4.4287124 -4.4286771 -4.428659 -4.4285936 -4.4284678 -4.4283862 -4.4284191 -4.4285173 -4.428597 -4.4285789 -4.4284782 -4.4283991][-4.4288955 -4.4288297 -4.4287539 -4.42869 -4.4286504 -4.4286284 -4.4285583 -4.4284306 -4.4283409 -4.4283671 -4.4284854 -4.4285932 -4.42861 -4.4285259 -4.428431][-4.4288921 -4.42883 -4.4287586 -4.4286985 -4.4286456 -4.4286065 -4.4285388 -4.4284267 -4.4283447 -4.4283624 -4.4284897 -4.4286079 -4.4286418 -4.4285731 -4.4284697][-4.4288826 -4.4288311 -4.4287663 -4.4287105 -4.4286461 -4.4285884 -4.4285221 -4.4284339 -4.4283757 -4.4283862 -4.4285088 -4.4286346 -4.4286861 -4.428637 -4.428534][-4.4288816 -4.4288392 -4.42878 -4.4287295 -4.428659 -4.4285846 -4.4285188 -4.4284644 -4.428442 -4.428453 -4.4285583 -4.4286718 -4.4287252 -4.4286857 -4.42859][-4.4288812 -4.4288549 -4.4288087 -4.4287715 -4.4287052 -4.4286213 -4.4285526 -4.4285145 -4.4285173 -4.4285393 -4.4286261 -4.4287243 -4.4287581 -4.4287062 -4.4286094][-4.4288659 -4.4288621 -4.4288359 -4.4288177 -4.4287653 -4.4286838 -4.4286132 -4.4285793 -4.4285932 -4.4286308 -4.4287095 -4.4287863 -4.4288034 -4.4287324 -4.428627][-4.4288406 -4.4288526 -4.4288425 -4.4288411 -4.4288106 -4.4287443 -4.4286819 -4.4286513 -4.4286623 -4.4287004 -4.4287648 -4.4288254 -4.4288297 -4.428741 -4.4286284][-4.4288225 -4.4288464 -4.4288521 -4.4288635 -4.4288526 -4.4288073 -4.4287634 -4.4287324 -4.4287281 -4.4287486 -4.4287858 -4.4288239 -4.4288163 -4.4287186 -4.4286041][-4.4288039 -4.4288273 -4.4288368 -4.4288545 -4.4288573 -4.428833 -4.4288111 -4.4287891 -4.4287767 -4.428781 -4.4287925 -4.4288116 -4.4287992 -4.4287014 -4.4285812]]...]
INFO - root - 2017-12-08 06:07:20.884074: step 19810, loss = 2.28, batch loss = 2.23 (35.4 examples/sec; 0.226 sec/batch; 19h:38m:03s remains)
INFO - root - 2017-12-08 06:07:23.168685: step 19820, loss = 2.28, batch loss = 2.23 (30.8 examples/sec; 0.260 sec/batch; 22h:35m:21s remains)
INFO - root - 2017-12-08 06:07:25.416432: step 19830, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 19h:11m:29s remains)
INFO - root - 2017-12-08 06:07:27.663316: step 19840, loss = 2.28, batch loss = 2.23 (35.7 examples/sec; 0.224 sec/batch; 19h:26m:58s remains)
INFO - root - 2017-12-08 06:07:29.907071: step 19850, loss = 2.28, batch loss = 2.23 (35.3 examples/sec; 0.227 sec/batch; 19h:41m:38s remains)
INFO - root - 2017-12-08 06:07:32.108584: step 19860, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 19h:10m:07s remains)
INFO - root - 2017-12-08 06:07:34.369069: step 19870, loss = 2.28, batch loss = 2.23 (33.7 examples/sec; 0.238 sec/batch; 20h:38m:25s remains)
INFO - root - 2017-12-08 06:07:36.603206: step 19880, loss = 2.28, batch loss = 2.23 (37.3 examples/sec; 0.215 sec/batch; 18h:38m:06s remains)
INFO - root - 2017-12-08 06:07:38.831410: step 19890, loss = 2.28, batch loss = 2.23 (37.0 examples/sec; 0.216 sec/batch; 18h:47m:30s remains)
INFO - root - 2017-12-08 06:07:41.125223: step 19900, loss = 2.28, batch loss = 2.23 (35.0 examples/sec; 0.229 sec/batch; 19h:51m:37s remains)
2017-12-08 06:07:41.416010: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289374 -4.4289618 -4.4289436 -4.428894 -4.4288235 -4.4287577 -4.428688 -4.4286594 -4.4286957 -4.4287047 -4.4286814 -4.4286332 -4.4286089 -4.4286208 -4.4286451][-4.4289308 -4.4289546 -4.4289384 -4.4288931 -4.4288287 -4.4287658 -4.4287028 -4.4286809 -4.4287124 -4.42871 -4.4286723 -4.4286137 -4.4285851 -4.4286 -4.4286323][-4.4289222 -4.4289474 -4.4289365 -4.4288988 -4.4288411 -4.4287839 -4.4287257 -4.4287033 -4.4287267 -4.4287095 -4.4286642 -4.4286032 -4.4285727 -4.4285793 -4.4286051][-4.4289117 -4.4289389 -4.4289341 -4.4289045 -4.4288578 -4.4288058 -4.4287467 -4.4287186 -4.4287343 -4.4287071 -4.4286628 -4.4286108 -4.428575 -4.42856 -4.428566][-4.428905 -4.4289331 -4.4289303 -4.428906 -4.4288678 -4.4288158 -4.4287462 -4.4287066 -4.4287214 -4.4286938 -4.4286604 -4.4286261 -4.4285913 -4.4285583 -4.4285522][-4.4289031 -4.4289293 -4.4289184 -4.4288898 -4.4288392 -4.428762 -4.4286585 -4.4286089 -4.4286423 -4.4286442 -4.428647 -4.4286442 -4.4286237 -4.4285932 -4.42859][-4.4289026 -4.4289236 -4.4289002 -4.4288516 -4.428762 -4.4286213 -4.428452 -4.4283981 -4.4284868 -4.42856 -4.4286232 -4.4286618 -4.4286685 -4.4286556 -4.4286637][-4.428896 -4.4289103 -4.4288754 -4.4288015 -4.4286666 -4.4284511 -4.4282088 -4.4281597 -4.4283285 -4.4284859 -4.4286022 -4.4286718 -4.4286976 -4.4287014 -4.4287128][-4.4288836 -4.4288926 -4.4288545 -4.4287729 -4.4286242 -4.4283929 -4.4281383 -4.4281015 -4.4282942 -4.4284668 -4.4285865 -4.4286575 -4.4286871 -4.4286981 -4.4287086][-4.4288697 -4.4288745 -4.4288421 -4.4287777 -4.4286609 -4.4284887 -4.4283042 -4.4282804 -4.428401 -4.4284992 -4.4285755 -4.4286294 -4.4286561 -4.4286723 -4.4286757][-4.4288611 -4.4288635 -4.428843 -4.4288063 -4.4287367 -4.4286294 -4.428514 -4.4284973 -4.4285355 -4.4285483 -4.4285722 -4.4286108 -4.4286418 -4.4286618 -4.4286489][-4.42886 -4.4288588 -4.428844 -4.4288282 -4.4287958 -4.4287314 -4.4286566 -4.4286437 -4.4286423 -4.4286132 -4.428606 -4.4286366 -4.428668 -4.4286819 -4.4286551][-4.4288607 -4.4288573 -4.4288435 -4.4288363 -4.4288263 -4.4287939 -4.428751 -4.4287424 -4.4287324 -4.4286952 -4.4286771 -4.4287004 -4.4287314 -4.4287415 -4.428709][-4.4288635 -4.4288588 -4.4288449 -4.4288344 -4.4288282 -4.428813 -4.4287939 -4.4287982 -4.4287953 -4.4287653 -4.4287457 -4.4287715 -4.4288049 -4.4288149 -4.4287853][-4.4288769 -4.4288712 -4.4288564 -4.4288397 -4.4288263 -4.4288149 -4.4288087 -4.4288273 -4.428834 -4.4288111 -4.4287944 -4.4288225 -4.4288597 -4.4288793 -4.428864]]...]
INFO - root - 2017-12-08 06:07:43.638630: step 19910, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.221 sec/batch; 19h:13m:28s remains)
INFO - root - 2017-12-08 06:07:45.904357: step 19920, loss = 2.28, batch loss = 2.23 (33.9 examples/sec; 0.236 sec/batch; 20h:31m:00s remains)
INFO - root - 2017-12-08 06:07:48.142638: step 19930, loss = 2.28, batch loss = 2.23 (34.8 examples/sec; 0.230 sec/batch; 19h:56m:11s remains)
INFO - root - 2017-12-08 06:07:50.367202: step 19940, loss = 2.28, batch loss = 2.23 (37.2 examples/sec; 0.215 sec/batch; 18h:39m:50s remains)
INFO - root - 2017-12-08 06:07:52.600865: step 19950, loss = 2.28, batch loss = 2.23 (37.7 examples/sec; 0.212 sec/batch; 18h:26m:46s remains)
INFO - root - 2017-12-08 06:07:54.835743: step 19960, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.221 sec/batch; 19h:13m:43s remains)
INFO - root - 2017-12-08 06:07:57.106473: step 19970, loss = 2.28, batch loss = 2.23 (35.0 examples/sec; 0.228 sec/batch; 19h:50m:07s remains)
INFO - root - 2017-12-08 06:07:59.325693: step 19980, loss = 2.28, batch loss = 2.23 (35.3 examples/sec; 0.227 sec/batch; 19h:40m:39s remains)
INFO - root - 2017-12-08 06:08:01.593429: step 19990, loss = 2.28, batch loss = 2.23 (33.6 examples/sec; 0.238 sec/batch; 20h:40m:45s remains)
INFO - root - 2017-12-08 06:08:03.819104: step 20000, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 19h:06m:08s remains)
2017-12-08 06:08:04.095544: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289207 -4.4289107 -4.4289169 -4.4289236 -4.4289317 -4.4289379 -4.428936 -4.4289355 -4.4289341 -4.4289227 -4.4289055 -4.4288869 -4.4288874 -4.4289007 -4.4289145][-4.4288754 -4.4288597 -4.4288664 -4.4288783 -4.4288964 -4.4289064 -4.4288974 -4.4288926 -4.4288898 -4.4288692 -4.4288445 -4.428822 -4.4288263 -4.4288449 -4.4288635][-4.4288177 -4.4287944 -4.4287972 -4.4288135 -4.4288387 -4.4288459 -4.4288225 -4.4288058 -4.4287992 -4.4287753 -4.42875 -4.428731 -4.4287395 -4.4287663 -4.4287958][-4.4287667 -4.4287348 -4.4287314 -4.4287424 -4.4287581 -4.4287543 -4.4287062 -4.4286723 -4.42867 -4.4286561 -4.4286437 -4.4286389 -4.4286628 -4.4287009 -4.4287438][-4.4287238 -4.4286871 -4.42868 -4.42868 -4.4286728 -4.4286456 -4.4285593 -4.4285007 -4.4285183 -4.4285364 -4.4285483 -4.4285603 -4.4286065 -4.4286556 -4.4287071][-4.4286923 -4.4286528 -4.4286366 -4.4286108 -4.4285645 -4.4284968 -4.4283462 -4.42823 -4.428288 -4.4283662 -4.4284077 -4.4284468 -4.4285293 -4.4285989 -4.428668][-4.4286909 -4.4286408 -4.4286027 -4.4285297 -4.4284306 -4.4283051 -4.4280534 -4.4278493 -4.427999 -4.4281745 -4.4282408 -4.4283066 -4.4284334 -4.4285364 -4.4286289][-4.4287319 -4.4286709 -4.4286003 -4.4284787 -4.4283476 -4.4281936 -4.427875 -4.4276252 -4.4278717 -4.4281025 -4.4281631 -4.4282293 -4.4283776 -4.4285007 -4.4286065][-4.4287896 -4.4287262 -4.428648 -4.4285331 -4.4284325 -4.4283266 -4.4280891 -4.4279146 -4.4281068 -4.4282708 -4.42829 -4.4283152 -4.4284272 -4.4285336 -4.4286261][-4.4288445 -4.4287939 -4.428731 -4.4286423 -4.4285889 -4.428544 -4.4284019 -4.4282975 -4.4284062 -4.4284949 -4.4284892 -4.4284911 -4.4285593 -4.4286342 -4.4286976][-4.4288788 -4.4288421 -4.4287987 -4.4287415 -4.428719 -4.4287105 -4.4286413 -4.4285841 -4.4286318 -4.4286714 -4.4286637 -4.4286537 -4.4286947 -4.4287448 -4.4287815][-4.4289055 -4.4288783 -4.4288535 -4.4288244 -4.4288187 -4.4288278 -4.4287992 -4.4287567 -4.4287663 -4.4287767 -4.428771 -4.4287615 -4.4287853 -4.4288173 -4.4288397][-4.4289217 -4.4289045 -4.4288883 -4.428875 -4.4288745 -4.4288869 -4.4288774 -4.4288468 -4.4288383 -4.4288368 -4.4288316 -4.4288259 -4.4288387 -4.42886 -4.4288726][-4.4289203 -4.4289069 -4.4288878 -4.428874 -4.4288726 -4.42888 -4.4288797 -4.4288673 -4.428864 -4.4288645 -4.428865 -4.428865 -4.4288735 -4.4288898 -4.4288969][-4.4289274 -4.4289145 -4.4288955 -4.4288797 -4.4288754 -4.4288783 -4.42888 -4.4288797 -4.4288836 -4.4288859 -4.4288893 -4.4288936 -4.4289031 -4.4289184 -4.4289236]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-momentum-lr0.001-defgroup-1/model.ckpt-20000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-momentum-lr0.001-defgroup-1/model.ckpt-20000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-08 06:08:06.814733: step 20010, loss = 2.28, batch loss = 2.23 (34.7 examples/sec; 0.231 sec/batch; 20h:01m:50s remains)
INFO - root - 2017-12-08 06:08:09.052098: step 20020, loss = 2.28, batch loss = 2.23 (35.4 examples/sec; 0.226 sec/batch; 19h:35m:41s remains)
INFO - root - 2017-12-08 06:08:11.288220: step 20030, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 19h:20m:16s remains)
INFO - root - 2017-12-08 06:08:13.516864: step 20040, loss = 2.28, batch loss = 2.23 (34.9 examples/sec; 0.229 sec/batch; 19h:53m:34s remains)
INFO - root - 2017-12-08 06:08:15.739396: step 20050, loss = 2.28, batch loss = 2.23 (37.4 examples/sec; 0.214 sec/batch; 18h:33m:47s remains)
INFO - root - 2017-12-08 06:08:17.975411: step 20060, loss = 2.28, batch loss = 2.23 (35.1 examples/sec; 0.228 sec/batch; 19h:45m:17s remains)
INFO - root - 2017-12-08 06:08:20.283547: step 20070, loss = 2.28, batch loss = 2.23 (35.3 examples/sec; 0.226 sec/batch; 19h:38m:31s remains)
INFO - root - 2017-12-08 06:08:22.492070: step 20080, loss = 2.28, batch loss = 2.23 (35.4 examples/sec; 0.226 sec/batch; 19h:37m:34s remains)
INFO - root - 2017-12-08 06:08:24.754063: step 20090, loss = 2.28, batch loss = 2.23 (35.2 examples/sec; 0.227 sec/batch; 19h:42m:38s remains)
INFO - root - 2017-12-08 06:08:26.992745: step 20100, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 19h:19m:03s remains)
2017-12-08 06:08:27.262892: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4286952 -4.4286861 -4.4286923 -4.4287057 -4.4287262 -4.4287467 -4.4287486 -4.4287271 -4.4287109 -4.4287229 -4.4287524 -4.4287519 -4.4287448 -4.4287515 -4.4287605][-4.4287019 -4.4286995 -4.428719 -4.4287419 -4.4287653 -4.4287863 -4.4288011 -4.4287863 -4.4287739 -4.4287882 -4.4287982 -4.4287724 -4.42875 -4.4287586 -4.4287686][-4.4287066 -4.4287062 -4.4287267 -4.4287534 -4.4287753 -4.4287996 -4.4288249 -4.4288197 -4.4288149 -4.4288368 -4.4288383 -4.4287982 -4.4287643 -4.4287653 -4.428771][-4.4286757 -4.428669 -4.4286752 -4.4287004 -4.4287314 -4.4287667 -4.4287982 -4.428802 -4.4288111 -4.4288487 -4.4288573 -4.428823 -4.4287944 -4.4287863 -4.4287815][-4.4285913 -4.4285655 -4.4285469 -4.4285707 -4.4286151 -4.4286661 -4.4287038 -4.4287171 -4.4287419 -4.4287977 -4.4288244 -4.4288158 -4.428812 -4.4288106 -4.428803][-4.4284797 -4.4284163 -4.4283628 -4.4283819 -4.4284377 -4.4285088 -4.4285574 -4.4285831 -4.4286251 -4.4286976 -4.4287491 -4.4287715 -4.4287906 -4.4288054 -4.4288111][-4.42841 -4.4283104 -4.4282255 -4.4282403 -4.4282956 -4.4283628 -4.4284 -4.4284229 -4.4284749 -4.4285655 -4.4286494 -4.4287043 -4.4287434 -4.4287724 -4.4287958][-4.4285455 -4.4284515 -4.4283748 -4.4283643 -4.4283757 -4.4283934 -4.4283943 -4.4284024 -4.4284506 -4.4285336 -4.4286127 -4.4286695 -4.4287114 -4.4287415 -4.428772][-4.4286876 -4.4286222 -4.4285803 -4.4285679 -4.4285555 -4.4285364 -4.4285169 -4.4285269 -4.4285741 -4.4286423 -4.4286985 -4.4287281 -4.4287477 -4.4287596 -4.4287777][-4.4287577 -4.4287229 -4.4287119 -4.4287086 -4.4287014 -4.4286752 -4.4286518 -4.4286613 -4.4287071 -4.4287596 -4.4287939 -4.4288011 -4.4288015 -4.428803 -4.4288116][-4.4287353 -4.4287415 -4.4287615 -4.428772 -4.4287748 -4.428751 -4.4287286 -4.42874 -4.4287815 -4.4288225 -4.4288416 -4.4288311 -4.4288168 -4.4288168 -4.4288244][-4.4286518 -4.4286728 -4.4287033 -4.4287276 -4.4287415 -4.4287271 -4.4287009 -4.4287119 -4.4287529 -4.4287963 -4.4288197 -4.4288068 -4.4287934 -4.4287972 -4.4288068][-4.42863 -4.4286342 -4.4286423 -4.4286518 -4.4286623 -4.42865 -4.4286184 -4.428617 -4.4286585 -4.42872 -4.4287653 -4.428771 -4.4287729 -4.4287815 -4.4287968][-4.4286423 -4.4286294 -4.4286113 -4.4285932 -4.4285874 -4.4285736 -4.4285493 -4.4285464 -4.4285917 -4.4286685 -4.4287353 -4.4287639 -4.4287786 -4.4287934 -4.4288082][-4.4286642 -4.4286456 -4.42861 -4.4285612 -4.4285412 -4.4285378 -4.42854 -4.4285445 -4.428587 -4.42867 -4.4287462 -4.4287853 -4.4288054 -4.4288182 -4.4288259]]...]
INFO - root - 2017-12-08 06:08:29.511931: step 20110, loss = 2.28, batch loss = 2.23 (35.2 examples/sec; 0.227 sec/batch; 19h:41m:47s remains)
INFO - root - 2017-12-08 06:08:31.749687: step 20120, loss = 2.28, batch loss = 2.23 (34.7 examples/sec; 0.230 sec/batch; 19h:58m:59s remains)
INFO - root - 2017-12-08 06:08:34.007493: step 20130, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.220 sec/batch; 19h:05m:54s remains)
INFO - root - 2017-12-08 06:08:36.242051: step 20140, loss = 2.28, batch loss = 2.23 (34.7 examples/sec; 0.231 sec/batch; 20h:01m:50s remains)
INFO - root - 2017-12-08 06:08:38.514943: step 20150, loss = 2.28, batch loss = 2.23 (34.7 examples/sec; 0.230 sec/batch; 19h:59m:51s remains)
INFO - root - 2017-12-08 06:08:40.749879: step 20160, loss = 2.28, batch loss = 2.23 (34.4 examples/sec; 0.233 sec/batch; 20h:11m:53s remains)
INFO - root - 2017-12-08 06:08:42.984229: step 20170, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 19h:00m:03s remains)
INFO - root - 2017-12-08 06:08:45.195062: step 20180, loss = 2.28, batch loss = 2.23 (37.9 examples/sec; 0.211 sec/batch; 18h:17m:58s remains)
INFO - root - 2017-12-08 06:08:47.428770: step 20190, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 19h:11m:20s remains)
INFO - root - 2017-12-08 06:08:49.687212: step 20200, loss = 2.28, batch loss = 2.23 (36.6 examples/sec; 0.218 sec/batch; 18h:56m:18s remains)
2017-12-08 06:08:49.987941: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289083 -4.4289012 -4.4288945 -4.4288921 -4.4288931 -4.4288964 -4.4289045 -4.4289155 -4.4289284 -4.4289355 -4.4289346 -4.4289293 -4.4289217 -4.4289122 -4.4289045][-4.42892 -4.4289131 -4.4289064 -4.4289041 -4.4289045 -4.4289069 -4.4289131 -4.42892 -4.4289265 -4.4289303 -4.4289303 -4.4289274 -4.4289222 -4.4289145 -4.4289088][-4.4289289 -4.4289231 -4.4289165 -4.4289136 -4.4289103 -4.4289021 -4.4288926 -4.4288821 -4.428875 -4.4288731 -4.4288745 -4.4288783 -4.4288845 -4.4288883 -4.4288936][-4.4289169 -4.4289093 -4.4288993 -4.4288898 -4.4288731 -4.4288435 -4.4288077 -4.4287729 -4.4287491 -4.4287486 -4.42876 -4.4287758 -4.4287977 -4.4288192 -4.42884][-4.4288645 -4.4288492 -4.4288311 -4.4288125 -4.42878 -4.4287271 -4.4286675 -4.42861 -4.4285722 -4.4285808 -4.4286151 -4.4286485 -4.4286857 -4.4287229 -4.4287567][-4.4287658 -4.4287357 -4.4287028 -4.4286819 -4.42865 -4.428587 -4.4285121 -4.4284315 -4.4283776 -4.4284039 -4.4284825 -4.4285493 -4.4286022 -4.428648 -4.4286814][-4.4286456 -4.4286013 -4.4285574 -4.4285436 -4.4285274 -4.4284682 -4.4283895 -4.4282928 -4.4282217 -4.4282651 -4.4283929 -4.4284964 -4.4285645 -4.4286113 -4.428638][-4.4285517 -4.4285011 -4.4284549 -4.4284496 -4.4284592 -4.42843 -4.4283843 -4.4283223 -4.4282765 -4.4283242 -4.4284415 -4.4285316 -4.4285884 -4.4286337 -4.4286656][-4.4285312 -4.428483 -4.4284396 -4.42844 -4.4284739 -4.4284883 -4.4284983 -4.4284987 -4.4284997 -4.4285412 -4.4286089 -4.4286542 -4.4286838 -4.4287229 -4.4287596][-4.4285936 -4.4285569 -4.4285245 -4.4285307 -4.4285779 -4.42862 -4.4286637 -4.4287004 -4.4287248 -4.4287539 -4.4287839 -4.4287982 -4.4288077 -4.4288335 -4.4288626][-4.4287195 -4.4286981 -4.428678 -4.4286871 -4.4287281 -4.42877 -4.428813 -4.4288487 -4.4288697 -4.428885 -4.428895 -4.4288955 -4.428896 -4.42891 -4.428926][-4.428843 -4.4288325 -4.4288192 -4.428823 -4.4288478 -4.428874 -4.4288979 -4.4289136 -4.4289184 -4.4289188 -4.4289184 -4.4289169 -4.4289184 -4.4289265 -4.428936][-4.4289036 -4.4288974 -4.4288845 -4.4288807 -4.4288883 -4.4288983 -4.428906 -4.4289069 -4.4289002 -4.4288917 -4.4288878 -4.4288912 -4.4288979 -4.428905 -4.4289122][-4.4288969 -4.428894 -4.4288788 -4.4288645 -4.4288568 -4.4288545 -4.4288559 -4.4288568 -4.4288554 -4.4288516 -4.428854 -4.4288654 -4.4288759 -4.4288807 -4.4288883][-4.4288621 -4.4288616 -4.4288483 -4.42883 -4.4288135 -4.428803 -4.4288054 -4.4288163 -4.4288292 -4.4288392 -4.4288483 -4.428863 -4.4288726 -4.428874 -4.42888]]...]
INFO - root - 2017-12-08 06:08:52.193116: step 20210, loss = 2.28, batch loss = 2.23 (37.2 examples/sec; 0.215 sec/batch; 18h:39m:19s remains)
INFO - root - 2017-12-08 06:08:54.416607: step 20220, loss = 2.28, batch loss = 2.23 (36.8 examples/sec; 0.217 sec/batch; 18h:50m:55s remains)
INFO - root - 2017-12-08 06:08:56.706204: step 20230, loss = 2.28, batch loss = 2.23 (35.3 examples/sec; 0.227 sec/batch; 19h:40m:44s remains)
INFO - root - 2017-12-08 06:08:58.951460: step 20240, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 19h:19m:23s remains)
INFO - root - 2017-12-08 06:09:01.223714: step 20250, loss = 2.28, batch loss = 2.23 (35.7 examples/sec; 0.224 sec/batch; 19h:27m:05s remains)
INFO - root - 2017-12-08 06:09:03.434844: step 20260, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.225 sec/batch; 19h:31m:55s remains)
INFO - root - 2017-12-08 06:09:05.663245: step 20270, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.220 sec/batch; 19h:06m:30s remains)
INFO - root - 2017-12-08 06:09:07.924714: step 20280, loss = 2.28, batch loss = 2.23 (35.3 examples/sec; 0.226 sec/batch; 19h:37m:53s remains)
INFO - root - 2017-12-08 06:09:10.156758: step 20290, loss = 2.28, batch loss = 2.23 (37.3 examples/sec; 0.214 sec/batch; 18h:34m:39s remains)
INFO - root - 2017-12-08 06:09:12.412917: step 20300, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.225 sec/batch; 19h:32m:13s remains)
2017-12-08 06:09:12.688020: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289365 -4.4289093 -4.4288888 -4.4288759 -4.4288726 -4.4288821 -4.4288983 -4.4289055 -4.4289141 -4.4289212 -4.4289141 -4.4288964 -4.4288669 -4.428844 -4.4288597][-4.4289231 -4.4288917 -4.428865 -4.4288445 -4.428833 -4.4288392 -4.4288583 -4.4288774 -4.4288988 -4.4289064 -4.4288936 -4.4288692 -4.4288359 -4.4288096 -4.428822][-4.428905 -4.4288683 -4.4288306 -4.428793 -4.4287715 -4.4287829 -4.428813 -4.4288526 -4.4288912 -4.4289055 -4.4288874 -4.4288526 -4.4288096 -4.428771 -4.4287763][-4.4288874 -4.4288487 -4.4288 -4.42874 -4.4286952 -4.4287024 -4.4287496 -4.4288168 -4.4288764 -4.4289074 -4.4289036 -4.428865 -4.4288082 -4.4287524 -4.4287415][-4.4288678 -4.4288263 -4.4287682 -4.4286895 -4.4286218 -4.4286051 -4.4286542 -4.4287496 -4.4288392 -4.4288936 -4.4289112 -4.4288883 -4.4288363 -4.4287724 -4.4287429][-4.4288635 -4.4288163 -4.4287477 -4.4286518 -4.4285522 -4.4284911 -4.4285097 -4.4286103 -4.4287419 -4.4288368 -4.42888 -4.4288797 -4.4288535 -4.428813 -4.4287839][-4.4288683 -4.4288192 -4.4287496 -4.428647 -4.4285212 -4.4284015 -4.4283428 -4.4283962 -4.4285603 -4.4287205 -4.4288139 -4.4288516 -4.428864 -4.4288559 -4.4288392][-4.4288783 -4.4288392 -4.4287868 -4.4287062 -4.4285836 -4.4284368 -4.4283009 -4.42826 -4.4283948 -4.4285793 -4.4287148 -4.4287982 -4.4288511 -4.4288788 -4.42888][-4.428875 -4.4288526 -4.428823 -4.4287758 -4.4286966 -4.4285889 -4.4284673 -4.4283824 -4.4284134 -4.4285216 -4.4286337 -4.4287343 -4.428812 -4.4288664 -4.4288979][-4.4288473 -4.4288378 -4.4288268 -4.4288063 -4.428772 -4.4287224 -4.4286623 -4.4286046 -4.4285812 -4.4286008 -4.4286385 -4.4286947 -4.4287562 -4.42882 -4.4288859][-4.4288154 -4.4288039 -4.428793 -4.4287844 -4.4287772 -4.4287753 -4.4287705 -4.4287586 -4.4287457 -4.4287415 -4.4287305 -4.4287138 -4.4287176 -4.4287577 -4.42883][-4.4288 -4.4287758 -4.4287443 -4.42873 -4.4287319 -4.4287524 -4.4287815 -4.4288082 -4.4288216 -4.4288254 -4.4288158 -4.4287844 -4.4287438 -4.4287343 -4.4287748][-4.4287748 -4.4287534 -4.4287105 -4.428678 -4.4286661 -4.4286771 -4.4287167 -4.4287729 -4.4288111 -4.4288273 -4.4288297 -4.428813 -4.4287767 -4.4287419 -4.4287395][-4.4287205 -4.4287176 -4.4286838 -4.4286489 -4.4286218 -4.4286103 -4.4286361 -4.4286966 -4.4287486 -4.4287777 -4.428791 -4.4287949 -4.4287806 -4.4287505 -4.4287262][-4.4286427 -4.4286528 -4.4286408 -4.4286265 -4.4286103 -4.4285913 -4.4285965 -4.4286337 -4.4286675 -4.4286904 -4.4287148 -4.4287381 -4.4287524 -4.4287481 -4.4287333]]...]
INFO - root - 2017-12-08 06:09:14.936284: step 20310, loss = 2.28, batch loss = 2.23 (35.3 examples/sec; 0.227 sec/batch; 19h:39m:34s remains)
INFO - root - 2017-12-08 06:09:17.178637: step 20320, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 19h:21m:02s remains)
INFO - root - 2017-12-08 06:09:19.457392: step 20330, loss = 2.28, batch loss = 2.23 (35.7 examples/sec; 0.224 sec/batch; 19h:25m:13s remains)
INFO - root - 2017-12-08 06:09:21.686873: step 20340, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 19h:01m:37s remains)
INFO - root - 2017-12-08 06:09:23.919587: step 20350, loss = 2.28, batch loss = 2.23 (35.3 examples/sec; 0.227 sec/batch; 19h:40m:30s remains)
INFO - root - 2017-12-08 06:09:26.135524: step 20360, loss = 2.28, batch loss = 2.23 (35.1 examples/sec; 0.228 sec/batch; 19h:46m:15s remains)
INFO - root - 2017-12-08 06:09:28.404801: step 20370, loss = 2.28, batch loss = 2.23 (35.6 examples/sec; 0.225 sec/batch; 19h:29m:42s remains)
INFO - root - 2017-12-08 06:09:30.631729: step 20380, loss = 2.28, batch loss = 2.23 (37.5 examples/sec; 0.213 sec/batch; 18h:28m:41s remains)
INFO - root - 2017-12-08 06:09:32.869312: step 20390, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 19h:08m:12s remains)
INFO - root - 2017-12-08 06:09:35.090054: step 20400, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 19h:00m:34s remains)
2017-12-08 06:09:35.385252: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4286666 -4.4286962 -4.4287195 -4.4287295 -4.4287229 -4.4287071 -4.4287024 -4.4287086 -4.4287167 -4.4287276 -4.4287519 -4.4287872 -4.4288111 -4.4288158 -4.4288135][-4.42862 -4.428627 -4.4286437 -4.42866 -4.4286528 -4.4286356 -4.4286308 -4.4286366 -4.4286404 -4.4286432 -4.428659 -4.4286909 -4.4287238 -4.4287529 -4.4287863][-4.4285965 -4.4285817 -4.428587 -4.4286084 -4.4286118 -4.4286 -4.4285989 -4.4286065 -4.4286041 -4.4285922 -4.428587 -4.428597 -4.4286194 -4.4286561 -4.4287138][-4.4286065 -4.4285822 -4.4285827 -4.4286127 -4.4286318 -4.4286323 -4.4286323 -4.4286304 -4.4286075 -4.4285736 -4.4285455 -4.4285393 -4.4285574 -4.4286003 -4.4286728][-4.4286723 -4.428638 -4.4286318 -4.4286628 -4.4286919 -4.4287024 -4.4286966 -4.4286742 -4.4286213 -4.428555 -4.4285026 -4.428494 -4.4285331 -4.4285984 -4.4286852][-4.4287844 -4.4287481 -4.4287381 -4.4287615 -4.4287848 -4.4287868 -4.428761 -4.4287081 -4.4286218 -4.428525 -4.4284573 -4.4284635 -4.4285436 -4.4286413 -4.42874][-4.4289036 -4.4288764 -4.4288673 -4.42888 -4.4288912 -4.4288764 -4.4288273 -4.4287486 -4.4286442 -4.4285369 -4.428472 -4.4284997 -4.4286056 -4.4287171 -4.42881][-4.4289775 -4.4289656 -4.4289632 -4.4289718 -4.4289742 -4.4289489 -4.42889 -4.4288058 -4.4287057 -4.42861 -4.4285579 -4.4285965 -4.4287038 -4.4288049 -4.4288778][-4.4289832 -4.4289894 -4.4289966 -4.4290061 -4.4290061 -4.4289823 -4.4289346 -4.4288669 -4.428791 -4.4287229 -4.4286842 -4.4287157 -4.4287977 -4.4288716 -4.4289179][-4.4289274 -4.4289503 -4.4289718 -4.4289913 -4.4290023 -4.4289942 -4.4289694 -4.4289308 -4.4288874 -4.4288445 -4.428812 -4.4288206 -4.4288645 -4.4289036 -4.4289255][-4.4288445 -4.4288812 -4.4289188 -4.4289532 -4.4289784 -4.428987 -4.4289827 -4.4289684 -4.4289489 -4.4289231 -4.4288945 -4.4288826 -4.4288945 -4.4289083 -4.4289155][-4.4287896 -4.4288321 -4.42888 -4.4289222 -4.4289522 -4.4289665 -4.4289713 -4.4289694 -4.4289641 -4.4289513 -4.42893 -4.4289122 -4.4289079 -4.4289069 -4.4289064][-4.42878 -4.4288182 -4.428865 -4.4289041 -4.4289284 -4.4289379 -4.42894 -4.4289384 -4.4289351 -4.4289265 -4.4289117 -4.4289 -4.428895 -4.4288931 -4.428895][-4.4288011 -4.42883 -4.4288697 -4.4289012 -4.4289169 -4.4289174 -4.4289107 -4.4289007 -4.4288907 -4.4288807 -4.4288707 -4.4288669 -4.4288678 -4.4288721 -4.4288826][-4.4288425 -4.4288616 -4.4288898 -4.4289122 -4.4289217 -4.4289184 -4.4289064 -4.4288883 -4.4288688 -4.4288521 -4.4288411 -4.4288397 -4.4288454 -4.4288568 -4.428875]]...]
INFO - root - 2017-12-08 06:09:37.614244: step 20410, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.225 sec/batch; 19h:32m:11s remains)
INFO - root - 2017-12-08 06:09:39.886918: step 20420, loss = 2.28, batch loss = 2.23 (35.4 examples/sec; 0.226 sec/batch; 19h:35m:36s remains)
INFO - root - 2017-12-08 06:09:42.113537: step 20430, loss = 2.28, batch loss = 2.23 (35.3 examples/sec; 0.226 sec/batch; 19h:37m:46s remains)
INFO - root - 2017-12-08 06:09:44.339317: step 20440, loss = 2.28, batch loss = 2.23 (36.0 examples/sec; 0.222 sec/batch; 19h:16m:10s remains)
INFO - root - 2017-12-08 06:09:46.576370: step 20450, loss = 2.28, batch loss = 2.23 (36.0 examples/sec; 0.222 sec/batch; 19h:15m:04s remains)
INFO - root - 2017-12-08 06:09:48.814456: step 20460, loss = 2.28, batch loss = 2.23 (34.6 examples/sec; 0.231 sec/batch; 20h:00m:46s remains)
INFO - root - 2017-12-08 06:09:51.036167: step 20470, loss = 2.28, batch loss = 2.23 (35.8 examples/sec; 0.223 sec/batch; 19h:21m:45s remains)
INFO - root - 2017-12-08 06:09:53.295266: step 20480, loss = 2.28, batch loss = 2.23 (34.0 examples/sec; 0.236 sec/batch; 20h:25m:05s remains)
INFO - root - 2017-12-08 06:09:55.521528: step 20490, loss = 2.28, batch loss = 2.23 (36.8 examples/sec; 0.217 sec/batch; 18h:49m:07s remains)
INFO - root - 2017-12-08 06:09:57.756522: step 20500, loss = 2.28, batch loss = 2.23 (36.8 examples/sec; 0.218 sec/batch; 18h:51m:20s remains)
2017-12-08 06:09:58.051548: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4286928 -4.4287133 -4.4287148 -4.4287157 -4.4287405 -4.428761 -4.4287405 -4.4287138 -4.428688 -4.4286876 -4.4287095 -4.4287362 -4.4287677 -4.4287758 -4.4287791][-4.428627 -4.4286566 -4.428669 -4.4286909 -4.4287405 -4.4287791 -4.4287815 -4.428771 -4.4287539 -4.4287505 -4.4287639 -4.4287677 -4.4287796 -4.4287672 -4.4287639][-4.428596 -4.428618 -4.4286323 -4.4286718 -4.4287376 -4.4287782 -4.4287839 -4.4287829 -4.4287767 -4.4287705 -4.4287672 -4.4287524 -4.4287505 -4.4287329 -4.4287267][-4.4286337 -4.4286361 -4.4286432 -4.4286861 -4.4287448 -4.4287615 -4.4287443 -4.4287319 -4.4287333 -4.4287348 -4.4287281 -4.4286966 -4.4286757 -4.4286585 -4.4286637][-4.42869 -4.4286838 -4.4286838 -4.4287095 -4.4287405 -4.4287267 -4.428679 -4.4286523 -4.4286642 -4.4286876 -4.4286861 -4.4286418 -4.4285979 -4.4285755 -4.4285903][-4.4287028 -4.4287024 -4.4287062 -4.4287195 -4.4287176 -4.428669 -4.4285827 -4.4285197 -4.4285488 -4.428616 -4.4286432 -4.4286079 -4.4285583 -4.4285321 -4.4285479][-4.4286923 -4.4287009 -4.4287114 -4.4287095 -4.4286809 -4.4285946 -4.4284449 -4.428299 -4.4283276 -4.4284616 -4.4285488 -4.4285574 -4.4285269 -4.428503 -4.4285226][-4.4287043 -4.4287214 -4.4287362 -4.428721 -4.4286709 -4.4285445 -4.4283242 -4.4280624 -4.4280629 -4.4282656 -4.4284253 -4.4284887 -4.4284821 -4.4284587 -4.4284725][-4.4287176 -4.4287429 -4.4287643 -4.4287477 -4.4287062 -4.4286008 -4.4283934 -4.4281216 -4.4280548 -4.4282017 -4.4283566 -4.42844 -4.4284463 -4.4284263 -4.4284372][-4.4287014 -4.428731 -4.4287615 -4.4287667 -4.4287629 -4.4287138 -4.4285851 -4.4283967 -4.4282928 -4.4283257 -4.4284148 -4.4284854 -4.4284911 -4.4284725 -4.4284787][-4.42868 -4.4287 -4.4287348 -4.42876 -4.4287934 -4.4287915 -4.4287333 -4.4286208 -4.4285126 -4.4284716 -4.4284992 -4.4285507 -4.4285564 -4.4285455 -4.4285502][-4.4286838 -4.428678 -4.4287024 -4.4287429 -4.4287953 -4.4288244 -4.4288149 -4.4287686 -4.4286938 -4.4286304 -4.4286151 -4.4286346 -4.4286375 -4.4286304 -4.4286256][-4.42871 -4.4286752 -4.4286804 -4.4287205 -4.4287796 -4.4288273 -4.4288387 -4.4288211 -4.4287853 -4.4287376 -4.4287133 -4.4287271 -4.4287415 -4.4287391 -4.4287262][-4.42874 -4.4286909 -4.4286761 -4.4287043 -4.428761 -4.428813 -4.4288268 -4.4288173 -4.4288106 -4.4287915 -4.4287763 -4.4287925 -4.4288216 -4.4288344 -4.4288244][-4.4287834 -4.4287381 -4.4287095 -4.4287119 -4.4287419 -4.4287744 -4.4287777 -4.4287748 -4.4287872 -4.428793 -4.4287992 -4.4288206 -4.4288521 -4.4288754 -4.4288778]]...]
INFO - root - 2017-12-08 06:10:00.287826: step 20510, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.219 sec/batch; 19h:01m:17s remains)
INFO - root - 2017-12-08 06:10:02.550007: step 20520, loss = 2.28, batch loss = 2.23 (35.2 examples/sec; 0.227 sec/batch; 19h:40m:27s remains)
INFO - root - 2017-12-08 06:10:04.782459: step 20530, loss = 2.28, batch loss = 2.23 (35.8 examples/sec; 0.223 sec/batch; 19h:21m:59s remains)
INFO - root - 2017-12-08 06:10:07.039252: step 20540, loss = 2.28, batch loss = 2.23 (35.1 examples/sec; 0.228 sec/batch; 19h:45m:07s remains)
INFO - root - 2017-12-08 06:10:09.280406: step 20550, loss = 2.28, batch loss = 2.23 (31.8 examples/sec; 0.251 sec/batch; 21h:45m:59s remains)
INFO - root - 2017-12-08 06:10:11.541438: step 20560, loss = 2.28, batch loss = 2.23 (37.1 examples/sec; 0.216 sec/batch; 18h:42m:17s remains)
INFO - root - 2017-12-08 06:10:13.754909: step 20570, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.220 sec/batch; 19h:04m:27s remains)
INFO - root - 2017-12-08 06:10:16.014755: step 20580, loss = 2.28, batch loss = 2.23 (34.6 examples/sec; 0.231 sec/batch; 20h:00m:49s remains)
INFO - root - 2017-12-08 06:10:18.273256: step 20590, loss = 2.28, batch loss = 2.23 (34.3 examples/sec; 0.233 sec/batch; 20h:10m:49s remains)
INFO - root - 2017-12-08 06:10:20.494230: step 20600, loss = 2.28, batch loss = 2.23 (35.4 examples/sec; 0.226 sec/batch; 19h:34m:55s remains)
2017-12-08 06:10:20.799257: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288063 -4.4287953 -4.4288082 -4.42883 -4.4288411 -4.4288325 -4.42881 -4.4288025 -4.4288249 -4.4288621 -4.4288888 -4.4288836 -4.4288321 -4.4287696 -4.4287543][-4.4287949 -4.4287763 -4.4287972 -4.4288397 -4.4288669 -4.4288549 -4.428813 -4.4287858 -4.4287992 -4.4288392 -4.4288731 -4.42887 -4.4288154 -4.4287529 -4.42874][-4.4287281 -4.4286985 -4.4287362 -4.428813 -4.428874 -4.4288697 -4.428812 -4.4287624 -4.4287648 -4.4288125 -4.428864 -4.4288735 -4.4288211 -4.4287553 -4.4287343][-4.4285979 -4.4285555 -4.42862 -4.4287457 -4.4288425 -4.4288473 -4.4287739 -4.4287024 -4.4286971 -4.4287639 -4.4288435 -4.4288712 -4.4288263 -4.4287567 -4.42872][-4.4284234 -4.4283738 -4.4284759 -4.4286528 -4.4287758 -4.4287763 -4.4286795 -4.4285789 -4.4285727 -4.4286675 -4.42878 -4.428834 -4.4288073 -4.4287429 -4.4286938][-4.428287 -4.4282503 -4.4283872 -4.428587 -4.4287004 -4.4286695 -4.428535 -4.4284034 -4.4284067 -4.4285393 -4.4286914 -4.4287791 -4.4287796 -4.4287271 -4.4286733][-4.4283185 -4.428319 -4.4284496 -4.4286017 -4.4286537 -4.4285717 -4.4283991 -4.4282551 -4.4282823 -4.4284492 -4.4286308 -4.4287419 -4.4287643 -4.4287262 -4.4286742][-4.4284892 -4.4285121 -4.4285917 -4.4286585 -4.4286418 -4.4285345 -4.4283752 -4.4282737 -4.4283204 -4.428473 -4.4286356 -4.4287405 -4.4287729 -4.4287462 -4.4286962][-4.4286265 -4.4286427 -4.4286757 -4.4286847 -4.4286394 -4.428555 -4.4284649 -4.4284267 -4.4284682 -4.428566 -4.428679 -4.4287605 -4.428791 -4.4287729 -4.4287267][-4.428688 -4.428689 -4.4286895 -4.4286675 -4.4286194 -4.4285703 -4.4285378 -4.4285417 -4.42858 -4.4286413 -4.4287186 -4.4287815 -4.4288092 -4.4287925 -4.4287467][-4.428721 -4.4287052 -4.4286809 -4.4286385 -4.42859 -4.4285626 -4.4285636 -4.4285922 -4.4286351 -4.4286909 -4.4287562 -4.4288116 -4.428834 -4.4288106 -4.4287591][-4.4287558 -4.4287271 -4.4286847 -4.4286332 -4.4285884 -4.428575 -4.4285879 -4.4286222 -4.4286666 -4.428721 -4.4287806 -4.4288297 -4.4288473 -4.4288211 -4.4287667][-4.428781 -4.4287524 -4.4287119 -4.4286661 -4.4286327 -4.428628 -4.4286389 -4.4286633 -4.4286995 -4.4287467 -4.4288006 -4.4288435 -4.4288578 -4.428834 -4.428782][-4.4287906 -4.4287682 -4.4287424 -4.4287162 -4.4287 -4.4287004 -4.4287086 -4.4287271 -4.4287567 -4.428793 -4.4288335 -4.4288669 -4.4288769 -4.4288549 -4.4288044][-4.4287858 -4.428771 -4.4287634 -4.428762 -4.4287672 -4.4287739 -4.42878 -4.4287949 -4.4288168 -4.4288387 -4.42886 -4.4288783 -4.4288845 -4.4288659 -4.4288177]]...]
INFO - root - 2017-12-08 06:10:23.103743: step 20610, loss = 2.28, batch loss = 2.23 (35.6 examples/sec; 0.224 sec/batch; 19h:26m:42s remains)
INFO - root - 2017-12-08 06:10:25.346546: step 20620, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.226 sec/batch; 19h:32m:21s remains)
INFO - root - 2017-12-08 06:10:27.590662: step 20630, loss = 2.28, batch loss = 2.23 (34.4 examples/sec; 0.233 sec/batch; 20h:10m:04s remains)
INFO - root - 2017-12-08 06:10:29.832989: step 20640, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 19h:18m:36s remains)
INFO - root - 2017-12-08 06:10:32.067380: step 20650, loss = 2.28, batch loss = 2.23 (37.0 examples/sec; 0.216 sec/batch; 18h:42m:19s remains)
INFO - root - 2017-12-08 06:10:34.316370: step 20660, loss = 2.28, batch loss = 2.23 (36.8 examples/sec; 0.217 sec/batch; 18h:49m:53s remains)
INFO - root - 2017-12-08 06:10:36.536094: step 20670, loss = 2.28, batch loss = 2.23 (37.0 examples/sec; 0.217 sec/batch; 18h:45m:13s remains)
INFO - root - 2017-12-08 06:10:38.806020: step 20680, loss = 2.28, batch loss = 2.23 (35.3 examples/sec; 0.226 sec/batch; 19h:36m:22s remains)
INFO - root - 2017-12-08 06:10:41.033685: step 20690, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.225 sec/batch; 19h:30m:52s remains)
INFO - root - 2017-12-08 06:10:43.272116: step 20700, loss = 2.28, batch loss = 2.23 (34.8 examples/sec; 0.230 sec/batch; 19h:52m:55s remains)
2017-12-08 06:10:43.572439: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288983 -4.4288549 -4.4288216 -4.428812 -4.4288325 -4.428874 -4.4289241 -4.4289613 -4.4289808 -4.4289808 -4.4289694 -4.4289556 -4.428956 -4.4289722 -4.4289918][-4.4289179 -4.428854 -4.4287949 -4.4287648 -4.4287729 -4.4288106 -4.4288626 -4.4289045 -4.4289284 -4.4289336 -4.4289379 -4.42894 -4.4289436 -4.4289603 -4.4289823][-4.4288912 -4.4287915 -4.4286952 -4.4286385 -4.4286318 -4.4286747 -4.4287415 -4.428802 -4.4288363 -4.42885 -4.4288692 -4.4288917 -4.4289045 -4.4289212 -4.4289441][-4.4288254 -4.4286828 -4.4285374 -4.4284339 -4.4283996 -4.4284506 -4.4285483 -4.4286418 -4.4286995 -4.4287286 -4.4287672 -4.4288144 -4.4288492 -4.4288764 -4.4289021][-4.4287567 -4.4285746 -4.4283762 -4.428215 -4.4281411 -4.4281874 -4.4283161 -4.4284458 -4.4285364 -4.4285903 -4.4286571 -4.4287362 -4.4287996 -4.4288449 -4.4288745][-4.4287186 -4.4285216 -4.4282937 -4.4280915 -4.4279714 -4.4279895 -4.4281178 -4.4282708 -4.4283886 -4.4284682 -4.4285645 -4.428669 -4.4287553 -4.4288206 -4.4288597][-4.4287367 -4.4285607 -4.4283495 -4.4281464 -4.4280028 -4.4279757 -4.4280543 -4.428194 -4.4283156 -4.4284086 -4.428524 -4.4286461 -4.4287467 -4.4288259 -4.4288731][-4.4288034 -4.4286742 -4.4285078 -4.4283381 -4.4282069 -4.4281425 -4.4281569 -4.4282565 -4.428359 -4.4284472 -4.4285617 -4.4286814 -4.4287825 -4.4288588 -4.428905][-4.4288969 -4.4288092 -4.4286833 -4.4285526 -4.4284463 -4.4283762 -4.4283595 -4.4284215 -4.428494 -4.4285579 -4.4286513 -4.4287539 -4.4288425 -4.4289093 -4.4289494][-4.428967 -4.4289088 -4.4288149 -4.4287138 -4.4286327 -4.4285789 -4.4285593 -4.4285965 -4.4286442 -4.4286928 -4.42876 -4.4288316 -4.4288917 -4.4289379 -4.4289632][-4.42899 -4.4289556 -4.4288898 -4.4288139 -4.4287529 -4.4287195 -4.4287128 -4.4287367 -4.4287648 -4.4287982 -4.42884 -4.4288783 -4.4289074 -4.4289312 -4.4289393][-4.4289827 -4.4289618 -4.42892 -4.4288664 -4.4288259 -4.428812 -4.4288197 -4.4288349 -4.4288468 -4.4288635 -4.4288836 -4.4288969 -4.4288993 -4.4289026 -4.4288993][-4.4289608 -4.4289422 -4.4289174 -4.4288907 -4.4288745 -4.4288788 -4.4288936 -4.4289064 -4.4289083 -4.4289112 -4.4289136 -4.4289103 -4.4288988 -4.4288893 -4.4288764][-4.428926 -4.4289036 -4.4288945 -4.4288931 -4.4288993 -4.4289169 -4.4289389 -4.4289551 -4.4289541 -4.4289465 -4.4289351 -4.4289227 -4.4289041 -4.428885 -4.4288616][-4.4288869 -4.4288545 -4.4288564 -4.4288797 -4.428906 -4.4289336 -4.4289618 -4.4289808 -4.4289804 -4.4289665 -4.4289484 -4.4289331 -4.4289141 -4.4288898 -4.4288578]]...]
INFO - root - 2017-12-08 06:10:45.801027: step 20710, loss = 2.28, batch loss = 2.23 (34.4 examples/sec; 0.233 sec/batch; 20h:09m:59s remains)
INFO - root - 2017-12-08 06:10:48.074282: step 20720, loss = 2.28, batch loss = 2.23 (34.9 examples/sec; 0.229 sec/batch; 19h:51m:17s remains)
INFO - root - 2017-12-08 06:10:50.305233: step 20730, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 18h:58m:26s remains)
INFO - root - 2017-12-08 06:10:52.531249: step 20740, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 19h:01m:34s remains)
INFO - root - 2017-12-08 06:10:54.756693: step 20750, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.221 sec/batch; 19h:09m:58s remains)
INFO - root - 2017-12-08 06:10:56.995391: step 20760, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.220 sec/batch; 19h:04m:21s remains)
INFO - root - 2017-12-08 06:10:59.253432: step 20770, loss = 2.28, batch loss = 2.23 (33.3 examples/sec; 0.240 sec/batch; 20h:47m:51s remains)
INFO - root - 2017-12-08 06:11:01.517282: step 20780, loss = 2.28, batch loss = 2.23 (35.3 examples/sec; 0.227 sec/batch; 19h:38m:24s remains)
INFO - root - 2017-12-08 06:11:03.749914: step 20790, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.221 sec/batch; 19h:05m:41s remains)
INFO - root - 2017-12-08 06:11:05.989325: step 20800, loss = 2.28, batch loss = 2.23 (36.7 examples/sec; 0.218 sec/batch; 18h:50m:58s remains)
2017-12-08 06:11:06.278148: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289064 -4.4289064 -4.4289012 -4.4288988 -4.4288955 -4.4288907 -4.4288912 -4.4289002 -4.428915 -4.4289274 -4.4289317 -4.4289289 -4.4289227 -4.4289184 -4.4289193][-4.4289246 -4.4289269 -4.4289184 -4.428906 -4.4288883 -4.4288678 -4.4288597 -4.4288721 -4.4288993 -4.4289255 -4.4289379 -4.4289355 -4.4289269 -4.4289179 -4.4289155][-4.42892 -4.4289265 -4.4289126 -4.4288836 -4.4288416 -4.4287968 -4.4287753 -4.4287915 -4.4288397 -4.4288936 -4.4289274 -4.428936 -4.4289312 -4.4289193 -4.4289136][-4.4288774 -4.4288917 -4.4288731 -4.428823 -4.4287472 -4.4286685 -4.4286275 -4.4286485 -4.4287252 -4.42882 -4.4288893 -4.4289207 -4.4289265 -4.428916 -4.4289074][-4.4288068 -4.4288325 -4.4288135 -4.4287434 -4.4286284 -4.4285064 -4.4284382 -4.4284616 -4.4285693 -4.4287124 -4.4288235 -4.428885 -4.4289055 -4.4289002 -4.4288907][-4.4287415 -4.42878 -4.4287658 -4.4286838 -4.4285359 -4.42837 -4.4282703 -4.4282889 -4.4284205 -4.4286051 -4.4287529 -4.4288406 -4.4288754 -4.4288759 -4.4288683][-4.4287319 -4.428772 -4.42876 -4.4286733 -4.4285073 -4.4283128 -4.42819 -4.4281988 -4.4283371 -4.4285417 -4.4287086 -4.4288092 -4.4288507 -4.4288549 -4.4288478][-4.4287877 -4.4288154 -4.4288006 -4.4287195 -4.428565 -4.4283791 -4.428256 -4.4282551 -4.4283752 -4.4285626 -4.42872 -4.4288139 -4.4288535 -4.42886 -4.428853][-4.4288578 -4.4288716 -4.4288559 -4.4287958 -4.4286819 -4.4285398 -4.428442 -4.4284344 -4.4285188 -4.428658 -4.4287791 -4.4288516 -4.4288816 -4.4288874 -4.4288812][-4.4289069 -4.4289145 -4.4289036 -4.42887 -4.4288073 -4.4287219 -4.4286604 -4.4286494 -4.4286933 -4.4287758 -4.4288497 -4.428894 -4.4289103 -4.4289122 -4.4289074][-4.4289241 -4.4289322 -4.4289279 -4.4289179 -4.428894 -4.4288554 -4.4288249 -4.4288135 -4.4288287 -4.4288654 -4.4289002 -4.4289207 -4.4289255 -4.4289241 -4.4289212][-4.4289179 -4.4289284 -4.42893 -4.4289322 -4.4289289 -4.4289169 -4.4289055 -4.4288969 -4.4288979 -4.42891 -4.4289222 -4.4289284 -4.4289274 -4.428926 -4.428925][-4.4289064 -4.4289184 -4.4289222 -4.4289279 -4.4289327 -4.4289322 -4.4289303 -4.4289274 -4.428926 -4.4289279 -4.4289308 -4.4289303 -4.4289279 -4.4289274 -4.4289274][-4.4289026 -4.4289136 -4.4289174 -4.4289227 -4.4289274 -4.4289308 -4.4289331 -4.4289336 -4.4289336 -4.4289331 -4.4289327 -4.4289308 -4.42893 -4.42893 -4.4289308][-4.428906 -4.4289136 -4.428915 -4.4289169 -4.4289203 -4.4289241 -4.4289279 -4.4289303 -4.4289327 -4.4289327 -4.4289317 -4.4289303 -4.4289289 -4.4289289 -4.4289293]]...]
INFO - root - 2017-12-08 06:11:08.523966: step 20810, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 18h:58m:09s remains)
INFO - root - 2017-12-08 06:11:10.744550: step 20820, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 19h:07m:16s remains)
INFO - root - 2017-12-08 06:11:12.975916: step 20830, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 19h:02m:48s remains)
INFO - root - 2017-12-08 06:11:15.218017: step 20840, loss = 2.28, batch loss = 2.23 (35.4 examples/sec; 0.226 sec/batch; 19h:34m:57s remains)
INFO - root - 2017-12-08 06:11:17.460647: step 20850, loss = 2.28, batch loss = 2.23 (35.1 examples/sec; 0.228 sec/batch; 19h:45m:29s remains)
INFO - root - 2017-12-08 06:11:19.737792: step 20860, loss = 2.28, batch loss = 2.23 (35.4 examples/sec; 0.226 sec/batch; 19h:32m:57s remains)
INFO - root - 2017-12-08 06:11:22.011832: step 20870, loss = 2.28, batch loss = 2.23 (34.7 examples/sec; 0.230 sec/batch; 19h:55m:47s remains)
INFO - root - 2017-12-08 06:11:24.291660: step 20880, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.225 sec/batch; 19h:29m:16s remains)
INFO - root - 2017-12-08 06:11:26.487850: step 20890, loss = 2.28, batch loss = 2.23 (35.6 examples/sec; 0.224 sec/batch; 19h:25m:33s remains)
INFO - root - 2017-12-08 06:11:28.717842: step 20900, loss = 2.28, batch loss = 2.23 (34.6 examples/sec; 0.231 sec/batch; 20h:01m:09s remains)
2017-12-08 06:11:28.987139: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.428884 -4.4288912 -4.4288917 -4.4288878 -4.4288797 -4.4288688 -4.4288578 -4.4288497 -4.4288454 -4.4288454 -4.4288445 -4.4288416 -4.4288411 -4.428843 -4.4288464][-4.4288893 -4.4288955 -4.428895 -4.4288917 -4.4288845 -4.4288759 -4.4288664 -4.4288578 -4.4288511 -4.4288473 -4.4288449 -4.428844 -4.4288473 -4.4288526 -4.4288564][-4.4288912 -4.4288964 -4.428896 -4.428894 -4.4288893 -4.428884 -4.4288769 -4.4288664 -4.4288549 -4.4288445 -4.4288387 -4.4288411 -4.4288487 -4.4288573 -4.4288607][-4.42889 -4.428896 -4.4288974 -4.4288983 -4.4288988 -4.4288979 -4.4288921 -4.4288788 -4.4288568 -4.4288354 -4.4288254 -4.4288306 -4.4288445 -4.4288559 -4.42886][-4.4288869 -4.4288936 -4.4288988 -4.4289064 -4.4289131 -4.4289165 -4.4289107 -4.4288898 -4.4288526 -4.4288158 -4.4287996 -4.4288092 -4.4288311 -4.4288487 -4.4288549][-4.4288831 -4.4288931 -4.4289041 -4.4289188 -4.4289317 -4.428937 -4.4289274 -4.4288926 -4.4288344 -4.428781 -4.4287615 -4.4287777 -4.4288106 -4.4288368 -4.4288473][-4.4288821 -4.4288983 -4.428916 -4.4289379 -4.4289532 -4.4289575 -4.4289379 -4.428884 -4.4288049 -4.4287405 -4.4287214 -4.4287481 -4.428793 -4.4288273 -4.4288445][-4.4288874 -4.42891 -4.4289331 -4.4289589 -4.4289737 -4.4289722 -4.4289384 -4.4288645 -4.428771 -4.4287038 -4.4286938 -4.4287338 -4.4287887 -4.4288278 -4.4288497][-4.4288969 -4.4289236 -4.42895 -4.4289756 -4.428987 -4.4289765 -4.4289274 -4.4288378 -4.4287391 -4.42868 -4.4286861 -4.4287395 -4.4287992 -4.4288383 -4.4288616][-4.428906 -4.4289331 -4.4289589 -4.4289818 -4.428988 -4.4289684 -4.4289079 -4.428812 -4.4287195 -4.4286771 -4.4287004 -4.42876 -4.4288173 -4.4288535 -4.4288754][-4.4289093 -4.4289336 -4.4289551 -4.4289742 -4.4289765 -4.4289513 -4.4288878 -4.4287968 -4.4287205 -4.4286985 -4.4287338 -4.428791 -4.4288397 -4.4288688 -4.4288869][-4.4289055 -4.428926 -4.4289436 -4.4289603 -4.4289622 -4.4289374 -4.42888 -4.4288034 -4.4287486 -4.4287434 -4.42878 -4.4288268 -4.4288616 -4.4288821 -4.4288926][-4.4288979 -4.4289155 -4.4289331 -4.4289513 -4.428957 -4.4289384 -4.4288926 -4.4288344 -4.4287968 -4.4287972 -4.4288254 -4.4288564 -4.4288778 -4.4288893 -4.4288921][-4.4288917 -4.4289093 -4.4289274 -4.4289484 -4.4289584 -4.4289479 -4.4289145 -4.4288712 -4.4288421 -4.42884 -4.4288564 -4.428875 -4.4288864 -4.4288907 -4.4288864][-4.4288893 -4.428906 -4.4289246 -4.4289451 -4.428957 -4.4289527 -4.42893 -4.4288964 -4.4288712 -4.4288645 -4.4288716 -4.4288816 -4.4288869 -4.428885 -4.4288759]]...]
INFO - root - 2017-12-08 06:11:31.205239: step 20910, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 18h:58m:28s remains)
INFO - root - 2017-12-08 06:11:33.438650: step 20920, loss = 2.28, batch loss = 2.23 (35.6 examples/sec; 0.225 sec/batch; 19h:26m:44s remains)
INFO - root - 2017-12-08 06:11:35.699887: step 20930, loss = 2.28, batch loss = 2.23 (34.6 examples/sec; 0.231 sec/batch; 20h:00m:41s remains)
INFO - root - 2017-12-08 06:11:37.924801: step 20940, loss = 2.28, batch loss = 2.23 (35.8 examples/sec; 0.223 sec/batch; 19h:19m:28s remains)
INFO - root - 2017-12-08 06:11:40.137279: step 20950, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 18h:58m:46s remains)
INFO - root - 2017-12-08 06:11:42.372020: step 20960, loss = 2.28, batch loss = 2.23 (37.1 examples/sec; 0.216 sec/batch; 18h:40m:58s remains)
INFO - root - 2017-12-08 06:11:44.638617: step 20970, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.220 sec/batch; 19h:03m:09s remains)
INFO - root - 2017-12-08 06:11:46.907222: step 20980, loss = 2.28, batch loss = 2.23 (35.0 examples/sec; 0.228 sec/batch; 19h:46m:09s remains)
INFO - root - 2017-12-08 06:11:49.133425: step 20990, loss = 2.28, batch loss = 2.23 (35.4 examples/sec; 0.226 sec/batch; 19h:33m:04s remains)
INFO - root - 2017-12-08 06:11:51.397138: step 21000, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 19h:05m:50s remains)
2017-12-08 06:11:51.665136: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289722 -4.4289894 -4.4290113 -4.4290171 -4.4290032 -4.428957 -4.4288859 -4.4288297 -4.4288149 -4.4288349 -4.4288707 -4.4289088 -4.4289236 -4.4289274 -4.4289355][-4.4289417 -4.4289641 -4.4289994 -4.4290104 -4.428987 -4.4289179 -4.42882 -4.4287453 -4.4287338 -4.428772 -4.4288216 -4.4288583 -4.4288816 -4.428894 -4.4289055][-4.4288816 -4.4289136 -4.4289565 -4.428978 -4.4289522 -4.4288621 -4.4287329 -4.4286327 -4.4286332 -4.4287133 -4.4287887 -4.4288268 -4.428854 -4.428865 -4.4288731][-4.4287939 -4.4288373 -4.4288878 -4.4289145 -4.428885 -4.4287763 -4.4286079 -4.4284744 -4.4285016 -4.4286461 -4.4287596 -4.428813 -4.428844 -4.4288483 -4.4288478][-4.4287219 -4.4287672 -4.4288149 -4.4288387 -4.428802 -4.4286761 -4.4284606 -4.428288 -4.428359 -4.4285755 -4.428731 -4.4288058 -4.428843 -4.428843 -4.4288363][-4.4286866 -4.4287295 -4.4287663 -4.42878 -4.4287348 -4.4285846 -4.4283133 -4.4280915 -4.4282131 -4.428494 -4.4286842 -4.4287782 -4.4288235 -4.4288287 -4.4288225][-4.4286957 -4.4287295 -4.4287486 -4.4287548 -4.4287062 -4.4285388 -4.4282331 -4.4279742 -4.4281287 -4.4284396 -4.4286361 -4.4287376 -4.428793 -4.4288073 -4.4288092][-4.4287305 -4.4287505 -4.4287567 -4.4287672 -4.4287362 -4.4286013 -4.4283438 -4.4281249 -4.4282427 -4.4284887 -4.428637 -4.4287167 -4.4287658 -4.4287829 -4.428791][-4.4287477 -4.4287586 -4.4287686 -4.428791 -4.4287906 -4.428719 -4.4285612 -4.4284239 -4.4284787 -4.4286175 -4.4286942 -4.4287295 -4.4287515 -4.4287543 -4.4287553][-4.42877 -4.4287748 -4.428792 -4.4288187 -4.4288321 -4.4288039 -4.4287233 -4.428648 -4.4286737 -4.4287448 -4.4287753 -4.4287806 -4.4287739 -4.4287529 -4.4287357][-4.4287968 -4.4287958 -4.4288077 -4.4288268 -4.4288387 -4.4288211 -4.4287753 -4.4287357 -4.4287548 -4.4287987 -4.4288163 -4.4288192 -4.4288054 -4.4287691 -4.4287362][-4.4288149 -4.4288 -4.4288044 -4.4288144 -4.4288111 -4.4287772 -4.4287333 -4.4287057 -4.4287286 -4.4287691 -4.4287906 -4.4288106 -4.428812 -4.4287786 -4.4287424][-4.4288192 -4.428791 -4.4287934 -4.4288011 -4.4287782 -4.4287214 -4.4286671 -4.4286404 -4.4286613 -4.4287119 -4.4287429 -4.4287782 -4.4288011 -4.428782 -4.4287491][-4.4288006 -4.4287677 -4.4287791 -4.4287939 -4.428761 -4.4286938 -4.4286361 -4.4286041 -4.4286237 -4.4286737 -4.4287024 -4.4287348 -4.4287686 -4.4287682 -4.4287467][-4.4287872 -4.4287605 -4.4287829 -4.4288106 -4.4287834 -4.42873 -4.4286852 -4.4286566 -4.4286675 -4.4286933 -4.4286995 -4.4287138 -4.4287477 -4.4287624 -4.4287543]]...]
INFO - root - 2017-12-08 06:11:53.933649: step 21010, loss = 2.28, batch loss = 2.23 (36.8 examples/sec; 0.217 sec/batch; 18h:47m:57s remains)
INFO - root - 2017-12-08 06:11:56.168069: step 21020, loss = 2.28, batch loss = 2.23 (36.0 examples/sec; 0.222 sec/batch; 19h:13m:55s remains)
INFO - root - 2017-12-08 06:11:58.405461: step 21030, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 19h:05m:49s remains)
INFO - root - 2017-12-08 06:12:00.673655: step 21040, loss = 2.28, batch loss = 2.23 (34.7 examples/sec; 0.231 sec/batch; 19h:58m:14s remains)
INFO - root - 2017-12-08 06:12:02.924602: step 21050, loss = 2.28, batch loss = 2.23 (33.7 examples/sec; 0.237 sec/batch; 20h:32m:24s remains)
INFO - root - 2017-12-08 06:12:05.162097: step 21060, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.225 sec/batch; 19h:29m:03s remains)
INFO - root - 2017-12-08 06:12:07.417081: step 21070, loss = 2.28, batch loss = 2.23 (30.7 examples/sec; 0.261 sec/batch; 22h:34m:42s remains)
INFO - root - 2017-12-08 06:12:09.688683: step 21080, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 19h:15m:34s remains)
INFO - root - 2017-12-08 06:12:11.966664: step 21090, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.226 sec/batch; 19h:31m:07s remains)
INFO - root - 2017-12-08 06:12:14.204065: step 21100, loss = 2.28, batch loss = 2.23 (37.0 examples/sec; 0.216 sec/batch; 18h:41m:19s remains)
2017-12-08 06:12:14.478612: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288354 -4.4288468 -4.4288507 -4.4288492 -4.4288406 -4.4288211 -4.4287963 -4.4287758 -4.428762 -4.428762 -4.428771 -4.4287777 -4.4287815 -4.4287896 -4.4287953][-4.4288797 -4.428894 -4.4288955 -4.4288883 -4.4288735 -4.4288483 -4.4288173 -4.4287868 -4.4287586 -4.4287424 -4.4287391 -4.428741 -4.4287424 -4.4287548 -4.428771][-4.4289336 -4.4289484 -4.4289484 -4.4289408 -4.4289279 -4.4289064 -4.4288721 -4.4288306 -4.4287906 -4.4287591 -4.4287419 -4.4287338 -4.4287262 -4.4287314 -4.4287505][-4.4289751 -4.4289856 -4.4289823 -4.4289708 -4.4289522 -4.4289255 -4.4288859 -4.4288449 -4.4288154 -4.4287982 -4.4287868 -4.4287748 -4.4287586 -4.4287472 -4.42875][-4.4289908 -4.42899 -4.4289732 -4.428947 -4.4289093 -4.4288568 -4.4288015 -4.428762 -4.4287519 -4.4287624 -4.4287772 -4.4287896 -4.4287887 -4.4287734 -4.42876][-4.4289808 -4.4289694 -4.4289384 -4.4288921 -4.428812 -4.4287152 -4.4286366 -4.4285908 -4.4285932 -4.4286323 -4.4286861 -4.4287386 -4.428772 -4.4287786 -4.4287705][-4.4289618 -4.428937 -4.4288807 -4.4287891 -4.4286404 -4.4284739 -4.4283566 -4.4283137 -4.4283414 -4.4284182 -4.4285183 -4.4286208 -4.4287009 -4.4287477 -4.42877][-4.4289379 -4.4288893 -4.4287891 -4.4286327 -4.42841 -4.4281697 -4.4280043 -4.4279661 -4.4280272 -4.42814 -4.4282913 -4.4284458 -4.4285769 -4.428679 -4.4287457][-4.4289312 -4.4288712 -4.42875 -4.4285631 -4.42831 -4.4280577 -4.4279079 -4.4278946 -4.4279633 -4.4280715 -4.4282155 -4.4283667 -4.4285126 -4.4286432 -4.4287329][-4.4289527 -4.4289083 -4.4288139 -4.4286656 -4.4284663 -4.428287 -4.4281983 -4.4282131 -4.4282637 -4.4283233 -4.4284019 -4.4284949 -4.4286013 -4.4286995 -4.4287682][-4.4289722 -4.4289503 -4.4288931 -4.4288034 -4.4286885 -4.4285893 -4.4285479 -4.4285631 -4.4285893 -4.4286113 -4.4286418 -4.4286823 -4.4287386 -4.4287915 -4.4288254][-4.4289665 -4.428956 -4.4289222 -4.4288721 -4.4288135 -4.4287677 -4.4287496 -4.4287586 -4.42876 -4.4287615 -4.4287724 -4.4287891 -4.4288116 -4.4288378 -4.4288483][-4.4289393 -4.4289279 -4.4289007 -4.4288721 -4.4288435 -4.4288182 -4.4288 -4.4287958 -4.4287806 -4.4287677 -4.4287658 -4.4287758 -4.4287877 -4.4288 -4.4288049][-4.4288921 -4.4288731 -4.4288459 -4.4288239 -4.4288063 -4.4287858 -4.42876 -4.4287405 -4.4287124 -4.4286928 -4.4286947 -4.4287071 -4.4287171 -4.4287176 -4.4287271][-4.4288449 -4.4288211 -4.4287944 -4.4287729 -4.4287496 -4.428719 -4.4286809 -4.4286504 -4.428618 -4.4286003 -4.4286036 -4.4286222 -4.4286323 -4.428628 -4.4286389]]...]
INFO - root - 2017-12-08 06:12:16.719857: step 21110, loss = 2.28, batch loss = 2.23 (34.9 examples/sec; 0.229 sec/batch; 19h:50m:56s remains)
INFO - root - 2017-12-08 06:12:18.948330: step 21120, loss = 2.28, batch loss = 2.23 (35.8 examples/sec; 0.223 sec/batch; 19h:18m:53s remains)
INFO - root - 2017-12-08 06:12:21.203166: step 21130, loss = 2.28, batch loss = 2.23 (35.6 examples/sec; 0.225 sec/batch; 19h:26m:02s remains)
INFO - root - 2017-12-08 06:12:23.439182: step 21140, loss = 2.28, batch loss = 2.23 (35.6 examples/sec; 0.225 sec/batch; 19h:26m:24s remains)
INFO - root - 2017-12-08 06:12:25.669066: step 21150, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 19h:17m:55s remains)
INFO - root - 2017-12-08 06:12:27.898317: step 21160, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 19h:16m:30s remains)
INFO - root - 2017-12-08 06:12:30.161954: step 21170, loss = 2.28, batch loss = 2.23 (35.3 examples/sec; 0.227 sec/batch; 19h:36m:39s remains)
INFO - root - 2017-12-08 06:12:32.387903: step 21180, loss = 2.28, batch loss = 2.23 (35.0 examples/sec; 0.228 sec/batch; 19h:45m:29s remains)
INFO - root - 2017-12-08 06:12:34.625547: step 21190, loss = 2.28, batch loss = 2.23 (35.8 examples/sec; 0.223 sec/batch; 19h:19m:31s remains)
INFO - root - 2017-12-08 06:12:36.877299: step 21200, loss = 2.28, batch loss = 2.23 (35.6 examples/sec; 0.225 sec/batch; 19h:25m:49s remains)
2017-12-08 06:12:37.171130: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4286537 -4.4285984 -4.428555 -4.4285607 -4.4286141 -4.4286642 -4.42869 -4.4286985 -4.4286966 -4.4286933 -4.4287205 -4.4287663 -4.4287868 -4.4287863 -4.4287767][-4.4286232 -4.428586 -4.4285641 -4.4285803 -4.428637 -4.4286895 -4.4287133 -4.4287248 -4.4287219 -4.4287248 -4.4287443 -4.4287767 -4.4287877 -4.4287767 -4.42877][-4.4286027 -4.4286046 -4.4286165 -4.4286394 -4.4286766 -4.4287038 -4.4286985 -4.4286919 -4.4286919 -4.428709 -4.4287305 -4.4287572 -4.4287615 -4.4287424 -4.4287291][-4.4286051 -4.4286547 -4.4286933 -4.4287038 -4.4286976 -4.4286709 -4.42861 -4.42857 -4.4285841 -4.4286432 -4.4286942 -4.4287243 -4.4287233 -4.4286938 -4.4286571][-4.4286571 -4.4287252 -4.428762 -4.4287429 -4.4286838 -4.4285874 -4.4284596 -4.42839 -4.4284334 -4.4285603 -4.42867 -4.4287124 -4.4287076 -4.4286637 -4.4286003][-4.4287491 -4.4288125 -4.4288259 -4.4287691 -4.4286456 -4.428462 -4.4282575 -4.4281592 -4.4282446 -4.4284563 -4.4286351 -4.4286985 -4.4286909 -4.4286304 -4.4285474][-4.4288349 -4.42889 -4.42889 -4.4288054 -4.4286308 -4.4283848 -4.4281349 -4.42803 -4.4281492 -4.4284081 -4.4286242 -4.428699 -4.4286809 -4.4286013 -4.4285021][-4.4288826 -4.4289322 -4.4289341 -4.428854 -4.42869 -4.4284635 -4.4282575 -4.4281845 -4.4282861 -4.428494 -4.4286737 -4.4287333 -4.4287024 -4.428617 -4.4285073][-4.4288955 -4.42894 -4.4289436 -4.428884 -4.4287672 -4.4286017 -4.4284749 -4.4284496 -4.428515 -4.4286323 -4.4287395 -4.4287724 -4.4287276 -4.4286442 -4.428544][-4.428884 -4.4289222 -4.4289265 -4.4288816 -4.4288011 -4.4286861 -4.42862 -4.4286275 -4.428668 -4.42872 -4.4287686 -4.4287763 -4.4287252 -4.4286432 -4.428545][-4.4288688 -4.4289007 -4.4289017 -4.42886 -4.4287944 -4.4287033 -4.4286737 -4.4287081 -4.4287486 -4.4287724 -4.4287825 -4.428771 -4.4287186 -4.4286361 -4.4285464][-4.4288774 -4.4289045 -4.4288964 -4.4288545 -4.4287982 -4.4287271 -4.428719 -4.4287686 -4.4288068 -4.4288177 -4.4288068 -4.4287729 -4.4287205 -4.428647 -4.4285812][-4.4288964 -4.4289169 -4.4288993 -4.4288626 -4.4288206 -4.4287739 -4.4287767 -4.4288316 -4.4288664 -4.4288688 -4.4288464 -4.4288006 -4.4287453 -4.4286828 -4.428637][-4.4288955 -4.4289083 -4.4288859 -4.4288564 -4.4288321 -4.4288106 -4.4288225 -4.4288754 -4.4289079 -4.4289112 -4.428885 -4.4288344 -4.4287639 -4.4286971 -4.4286575][-4.4288816 -4.4288912 -4.4288716 -4.4288507 -4.42884 -4.4288359 -4.428853 -4.4289 -4.4289341 -4.4289436 -4.4289165 -4.42885 -4.4287524 -4.4286647 -4.4286084]]...]
INFO - root - 2017-12-08 06:12:39.432824: step 21210, loss = 2.28, batch loss = 2.23 (35.4 examples/sec; 0.226 sec/batch; 19h:33m:27s remains)
INFO - root - 2017-12-08 06:12:41.644916: step 21220, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.222 sec/batch; 19h:11m:13s remains)
INFO - root - 2017-12-08 06:12:43.918678: step 21230, loss = 2.28, batch loss = 2.23 (34.9 examples/sec; 0.229 sec/batch; 19h:50m:17s remains)
INFO - root - 2017-12-08 06:12:46.194127: step 21240, loss = 2.28, batch loss = 2.23 (32.4 examples/sec; 0.247 sec/batch; 21h:21m:01s remains)
INFO - root - 2017-12-08 06:12:48.435915: step 21250, loss = 2.28, batch loss = 2.23 (36.6 examples/sec; 0.219 sec/batch; 18h:53m:50s remains)
INFO - root - 2017-12-08 06:12:50.677137: step 21260, loss = 2.28, batch loss = 2.23 (37.2 examples/sec; 0.215 sec/batch; 18h:34m:53s remains)
INFO - root - 2017-12-08 06:12:52.920258: step 21270, loss = 2.28, batch loss = 2.23 (33.7 examples/sec; 0.238 sec/batch; 20h:32m:30s remains)
INFO - root - 2017-12-08 06:12:55.177217: step 21280, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.221 sec/batch; 19h:04m:27s remains)
INFO - root - 2017-12-08 06:12:57.411017: step 21290, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.225 sec/batch; 19h:27m:53s remains)
INFO - root - 2017-12-08 06:12:59.648567: step 21300, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.220 sec/batch; 19h:01m:41s remains)
2017-12-08 06:12:59.929023: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4286294 -4.4286709 -4.4286966 -4.4286952 -4.4286776 -4.4286532 -4.4286289 -4.4285941 -4.4285703 -4.4285626 -4.42858 -4.4285946 -4.4285946 -4.4286108 -4.4286408][-4.4286451 -4.4286976 -4.4287224 -4.4287062 -4.4286647 -4.4286189 -4.4285803 -4.4285417 -4.4285145 -4.4285083 -4.4285364 -4.4285641 -4.4285731 -4.4285936 -4.4286246][-4.4286709 -4.4287214 -4.4287367 -4.4287014 -4.428638 -4.4285655 -4.4285021 -4.4284463 -4.4284177 -4.4284325 -4.42849 -4.4285421 -4.4285684 -4.4285965 -4.4286275][-4.4286947 -4.4287276 -4.4287224 -4.4286704 -4.4285889 -4.4284883 -4.4283977 -4.4283261 -4.4283118 -4.4283671 -4.4284611 -4.4285412 -4.4285889 -4.4286284 -4.4286628][-4.4287171 -4.4287295 -4.4287038 -4.428638 -4.4285421 -4.4284186 -4.4282994 -4.4282184 -4.4282346 -4.4283385 -4.4284625 -4.4285607 -4.4286203 -4.4286642 -4.4286995][-4.4287119 -4.4287076 -4.4286742 -4.4286013 -4.4284959 -4.428359 -4.4282222 -4.4281449 -4.4282036 -4.4283543 -4.4285 -4.4286022 -4.4286528 -4.4286866 -4.4287162][-4.428678 -4.4286695 -4.4286375 -4.4285603 -4.4284487 -4.4283094 -4.4281769 -4.4281235 -4.4282179 -4.4283905 -4.428544 -4.4286418 -4.4286823 -4.4286981 -4.4287114][-4.428638 -4.4286361 -4.4286103 -4.428535 -4.4284215 -4.4282875 -4.428174 -4.428153 -4.4282651 -4.428432 -4.4285736 -4.4286628 -4.4286957 -4.4286909 -4.4286828][-4.4285975 -4.428607 -4.4285908 -4.42852 -4.4284081 -4.4282804 -4.4281912 -4.4282069 -4.4283257 -4.4284706 -4.4285903 -4.428668 -4.4286914 -4.4286656 -4.4286413][-4.428566 -4.428587 -4.4285822 -4.428514 -4.4283986 -4.4282851 -4.4282336 -4.42828 -4.428391 -4.428503 -4.4285994 -4.4286666 -4.4286819 -4.4286475 -4.4286103][-4.4285474 -4.42858 -4.4285827 -4.4285154 -4.4284029 -4.42831 -4.4282913 -4.4283533 -4.4284468 -4.4285331 -4.4286113 -4.42867 -4.4286833 -4.4286513 -4.428607][-4.4285502 -4.4285808 -4.4285784 -4.4285126 -4.4284172 -4.4283566 -4.4283667 -4.4284339 -4.4285135 -4.4285865 -4.4286518 -4.4286923 -4.4286957 -4.4286656 -4.4286175][-4.4285612 -4.42858 -4.4285655 -4.4285078 -4.4284482 -4.4284368 -4.428472 -4.4285326 -4.4285927 -4.4286485 -4.4286962 -4.4287157 -4.4287033 -4.4286709 -4.4286242][-4.4285665 -4.4285736 -4.4285569 -4.4285183 -4.4285 -4.4285316 -4.4285789 -4.4286232 -4.4286609 -4.42869 -4.4287171 -4.4287195 -4.4286942 -4.4286609 -4.4286151][-4.4285641 -4.4285622 -4.42855 -4.4285336 -4.4285431 -4.4285908 -4.4286351 -4.4286652 -4.428688 -4.4286995 -4.4287095 -4.4287019 -4.4286666 -4.4286232 -4.4285755]]...]
INFO - root - 2017-12-08 06:13:02.161132: step 21310, loss = 2.28, batch loss = 2.23 (35.0 examples/sec; 0.228 sec/batch; 19h:44m:43s remains)
INFO - root - 2017-12-08 06:13:04.376362: step 21320, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 19h:06m:42s remains)
INFO - root - 2017-12-08 06:13:06.616512: step 21330, loss = 2.28, batch loss = 2.23 (36.8 examples/sec; 0.218 sec/batch; 18h:48m:47s remains)
INFO - root - 2017-12-08 06:13:08.858673: step 21340, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 19h:14m:39s remains)
INFO - root - 2017-12-08 06:13:11.142173: step 21350, loss = 2.28, batch loss = 2.23 (35.8 examples/sec; 0.224 sec/batch; 19h:20m:19s remains)
INFO - root - 2017-12-08 06:13:13.383314: step 21360, loss = 2.28, batch loss = 2.23 (35.2 examples/sec; 0.227 sec/batch; 19h:38m:23s remains)
INFO - root - 2017-12-08 06:13:15.603822: step 21370, loss = 2.28, batch loss = 2.23 (35.7 examples/sec; 0.224 sec/batch; 19h:21m:03s remains)
INFO - root - 2017-12-08 06:13:17.841084: step 21380, loss = 2.28, batch loss = 2.23 (36.0 examples/sec; 0.222 sec/batch; 19h:13m:10s remains)
INFO - root - 2017-12-08 06:13:20.098299: step 21390, loss = 2.28, batch loss = 2.23 (35.3 examples/sec; 0.227 sec/batch; 19h:36m:33s remains)
INFO - root - 2017-12-08 06:13:22.367357: step 21400, loss = 2.28, batch loss = 2.23 (35.1 examples/sec; 0.228 sec/batch; 19h:41m:14s remains)
2017-12-08 06:13:22.696816: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289432 -4.4289422 -4.4289389 -4.4289317 -4.4289289 -4.4289308 -4.428936 -4.4289365 -4.4289393 -4.428936 -4.4289269 -4.4289303 -4.4289432 -4.4289494 -4.4289565][-4.42893 -4.4289207 -4.42892 -4.4289126 -4.4289069 -4.4289041 -4.4289079 -4.42891 -4.4289179 -4.4289141 -4.4288993 -4.4288945 -4.4289036 -4.4289088 -4.4289179][-4.4289141 -4.4288945 -4.428896 -4.4288898 -4.428874 -4.4288597 -4.4288578 -4.428863 -4.4288783 -4.4288797 -4.4288626 -4.4288511 -4.4288516 -4.4288516 -4.428863][-4.4288816 -4.428853 -4.4288549 -4.4288549 -4.4288316 -4.4288039 -4.4287939 -4.4287992 -4.4288235 -4.4288378 -4.4288249 -4.428812 -4.428803 -4.428792 -4.4288006][-4.4288278 -4.4287963 -4.4288015 -4.4288087 -4.4287825 -4.428741 -4.4287086 -4.4286938 -4.42873 -4.4287734 -4.4287777 -4.428771 -4.4287562 -4.4287353 -4.4287372][-4.4287515 -4.4287262 -4.4287372 -4.4287438 -4.4287076 -4.4286408 -4.4285669 -4.4285207 -4.4285736 -4.428659 -4.4286885 -4.4286947 -4.428689 -4.4286709 -4.4286737][-4.4286928 -4.4286723 -4.4286809 -4.4286742 -4.4286232 -4.4285212 -4.4283876 -4.4282951 -4.4283686 -4.4285097 -4.4285741 -4.4286027 -4.4286237 -4.4286232 -4.4286356][-4.4286823 -4.4286609 -4.4286652 -4.4286518 -4.4285841 -4.4284458 -4.4282589 -4.428112 -4.4281912 -4.4283838 -4.4284921 -4.4285536 -4.4286094 -4.4286304 -4.4286485][-4.4287124 -4.4286914 -4.428688 -4.4286737 -4.4286046 -4.4284711 -4.4282951 -4.4281559 -4.4282122 -4.4283819 -4.42849 -4.42856 -4.4286294 -4.4286661 -4.428688][-4.4287348 -4.4287086 -4.4287028 -4.4286861 -4.4286308 -4.4285445 -4.428431 -4.4283519 -4.4283862 -4.4284763 -4.4285359 -4.4285851 -4.4286442 -4.4286847 -4.4287109][-4.4287281 -4.4286966 -4.4286876 -4.4286733 -4.428638 -4.428607 -4.4285612 -4.4285264 -4.4285374 -4.4285612 -4.4285731 -4.4285908 -4.4286327 -4.4286752 -4.428709][-4.4287229 -4.4286885 -4.4286723 -4.4286561 -4.4286332 -4.4286418 -4.4286475 -4.4286327 -4.4286222 -4.4286118 -4.4285975 -4.4285903 -4.42862 -4.4286661 -4.4287071][-4.4287167 -4.4286885 -4.4286766 -4.4286704 -4.4286561 -4.428679 -4.4287047 -4.4287014 -4.4286852 -4.4286656 -4.4286432 -4.4286194 -4.4286318 -4.4286704 -4.4287148][-4.4287295 -4.42872 -4.4287276 -4.4287391 -4.4287367 -4.4287519 -4.42877 -4.4287682 -4.4287548 -4.4287405 -4.4287176 -4.4286847 -4.428678 -4.428699 -4.4287415][-4.4287977 -4.4288025 -4.4288206 -4.4288383 -4.42884 -4.4288449 -4.4288511 -4.4288468 -4.4288359 -4.4288244 -4.4288039 -4.4287767 -4.4287653 -4.4287758 -4.4288092]]...]
INFO - root - 2017-12-08 06:13:24.934325: step 21410, loss = 2.28, batch loss = 2.23 (34.4 examples/sec; 0.233 sec/batch; 20h:05m:36s remains)
INFO - root - 2017-12-08 06:13:27.189080: step 21420, loss = 2.28, batch loss = 2.23 (33.3 examples/sec; 0.240 sec/batch; 20h:44m:17s remains)
INFO - root - 2017-12-08 06:13:29.452275: step 21430, loss = 2.28, batch loss = 2.23 (34.9 examples/sec; 0.230 sec/batch; 19h:50m:01s remains)
INFO - root - 2017-12-08 06:13:31.700632: step 21440, loss = 2.28, batch loss = 2.23 (35.2 examples/sec; 0.227 sec/batch; 19h:37m:48s remains)
INFO - root - 2017-12-08 06:13:33.921906: step 21450, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.225 sec/batch; 19h:28m:32s remains)
INFO - root - 2017-12-08 06:13:36.188535: step 21460, loss = 2.28, batch loss = 2.23 (34.9 examples/sec; 0.229 sec/batch; 19h:48m:11s remains)
INFO - root - 2017-12-08 06:13:38.414671: step 21470, loss = 2.28, batch loss = 2.23 (35.0 examples/sec; 0.228 sec/batch; 19h:43m:11s remains)
INFO - root - 2017-12-08 06:13:40.645147: step 21480, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.221 sec/batch; 19h:07m:52s remains)
INFO - root - 2017-12-08 06:13:42.861786: step 21490, loss = 2.28, batch loss = 2.23 (37.5 examples/sec; 0.214 sec/batch; 18h:27m:03s remains)
INFO - root - 2017-12-08 06:13:45.104510: step 21500, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 19h:07m:02s remains)
2017-12-08 06:13:45.407327: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.428793 -4.4288211 -4.428833 -4.4288521 -4.4288545 -4.4288392 -4.4288325 -4.4288306 -4.4288292 -4.4288259 -4.4288177 -4.4288087 -4.4287968 -4.4287858 -4.4287777][-4.42886 -4.4288845 -4.428894 -4.4289131 -4.4289141 -4.4288993 -4.4288888 -4.4288759 -4.4288607 -4.4288435 -4.428822 -4.428803 -4.4287858 -4.4287763 -4.4287786][-4.4289312 -4.4289603 -4.4289718 -4.4289794 -4.4289637 -4.4289289 -4.4288964 -4.4288669 -4.4288435 -4.4288263 -4.4288092 -4.4287934 -4.4287734 -4.4287624 -4.4287691][-4.4289417 -4.4289689 -4.4289756 -4.4289675 -4.4289317 -4.4288759 -4.4288182 -4.4287748 -4.4287519 -4.4287448 -4.4287486 -4.4287519 -4.428741 -4.4287338 -4.4287453][-4.4289017 -4.4289079 -4.4288907 -4.42886 -4.4288011 -4.42872 -4.4286408 -4.4285965 -4.4286003 -4.4286294 -4.4286704 -4.4286966 -4.4286995 -4.4286957 -4.4287014][-4.4288139 -4.4287848 -4.4287372 -4.4286809 -4.4286017 -4.4285054 -4.4284225 -4.4284034 -4.42846 -4.42854 -4.4286156 -4.4286642 -4.428679 -4.4286776 -4.4286766][-4.4286761 -4.4286256 -4.4285612 -4.428484 -4.42838 -4.428268 -4.4281988 -4.4282312 -4.4283562 -4.4284854 -4.4285803 -4.4286389 -4.4286623 -4.4286618 -4.4286609][-4.4285784 -4.4285121 -4.4284234 -4.4283118 -4.4281797 -4.4280705 -4.4280419 -4.4281349 -4.4283071 -4.4284592 -4.4285641 -4.4286261 -4.4286532 -4.4286561 -4.4286571][-4.4285989 -4.4285269 -4.428412 -4.4282622 -4.4281082 -4.4280305 -4.4280567 -4.4281807 -4.4283495 -4.4284906 -4.4285889 -4.4286427 -4.4286613 -4.42866 -4.4286609][-4.4286571 -4.428597 -4.4284897 -4.4283504 -4.4282303 -4.4281936 -4.4282351 -4.42834 -4.4284687 -4.4285731 -4.4286418 -4.4286761 -4.4286857 -4.4286804 -4.4286757][-4.4287081 -4.4286733 -4.4285874 -4.4284778 -4.4283938 -4.4283714 -4.4284067 -4.4284849 -4.4285793 -4.4286494 -4.4286909 -4.4287081 -4.4287076 -4.4287028 -4.4286947][-4.42877 -4.4287562 -4.4286909 -4.4286103 -4.4285502 -4.4285359 -4.4285665 -4.4286294 -4.4287019 -4.4287438 -4.4287539 -4.4287448 -4.4287295 -4.428721 -4.42871][-4.4288292 -4.4288297 -4.4287863 -4.4287276 -4.4286819 -4.4286737 -4.4287004 -4.4287515 -4.4288034 -4.4288225 -4.428812 -4.4287896 -4.4287691 -4.4287543 -4.428731][-4.4288511 -4.4288588 -4.4288292 -4.4287891 -4.4287553 -4.4287515 -4.42878 -4.4288225 -4.4288526 -4.42885 -4.4288259 -4.4288039 -4.428791 -4.4287767 -4.4287462][-4.4288497 -4.4288611 -4.428843 -4.4288158 -4.4287915 -4.4287925 -4.4288192 -4.4288464 -4.4288573 -4.4288435 -4.4288149 -4.4287992 -4.4287939 -4.4287786 -4.4287438]]...]
INFO - root - 2017-12-08 06:13:47.615972: step 21510, loss = 2.28, batch loss = 2.23 (37.0 examples/sec; 0.216 sec/batch; 18h:40m:06s remains)
INFO - root - 2017-12-08 06:13:49.855965: step 21520, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 19h:03m:56s remains)
INFO - root - 2017-12-08 06:13:52.070551: step 21530, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.220 sec/batch; 19h:02m:01s remains)
INFO - root - 2017-12-08 06:13:54.304218: step 21540, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 18h:56m:23s remains)
INFO - root - 2017-12-08 06:13:56.531419: step 21550, loss = 2.28, batch loss = 2.23 (37.2 examples/sec; 0.215 sec/batch; 18h:34m:20s remains)
INFO - root - 2017-12-08 06:13:58.766472: step 21560, loss = 2.28, batch loss = 2.23 (36.0 examples/sec; 0.222 sec/batch; 19h:12m:14s remains)
INFO - root - 2017-12-08 06:14:00.999886: step 21570, loss = 2.28, batch loss = 2.23 (35.3 examples/sec; 0.227 sec/batch; 19h:34m:42s remains)
INFO - root - 2017-12-08 06:14:03.243181: step 21580, loss = 2.28, batch loss = 2.23 (34.1 examples/sec; 0.235 sec/batch; 20h:17m:00s remains)
INFO - root - 2017-12-08 06:14:05.498655: step 21590, loss = 2.28, batch loss = 2.23 (34.4 examples/sec; 0.233 sec/batch; 20h:05m:55s remains)
INFO - root - 2017-12-08 06:14:07.739727: step 21600, loss = 2.28, batch loss = 2.23 (32.6 examples/sec; 0.246 sec/batch; 21h:12m:09s remains)
2017-12-08 06:14:08.062916: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4287648 -4.42878 -4.4287934 -4.4287987 -4.4287825 -4.4287448 -4.4287119 -4.4287047 -4.4287152 -4.4287267 -4.4287491 -4.4287777 -4.4288025 -4.4288239 -4.42885][-4.428772 -4.4287829 -4.4287891 -4.4287877 -4.4287639 -4.4287233 -4.4286866 -4.4286752 -4.4286819 -4.4286957 -4.4287286 -4.4287705 -4.4288006 -4.4288206 -4.4288435][-4.4287181 -4.42872 -4.4287148 -4.4287014 -4.4286685 -4.4286246 -4.4285865 -4.4285812 -4.4286013 -4.4286313 -4.4286838 -4.4287453 -4.4287887 -4.4288116 -4.428833][-4.4286246 -4.4286194 -4.4286065 -4.428587 -4.4285507 -4.4285069 -4.4284739 -4.4284873 -4.4285321 -4.428587 -4.428658 -4.4287324 -4.4287834 -4.4288082 -4.4288278][-4.428515 -4.4285069 -4.4284959 -4.428483 -4.4284534 -4.4284158 -4.428391 -4.4284215 -4.4284911 -4.4285674 -4.4286523 -4.4287305 -4.4287834 -4.4288087 -4.4288278][-4.428432 -4.4284225 -4.4284139 -4.4284048 -4.4283795 -4.4283457 -4.4283247 -4.4283643 -4.4284468 -4.4285378 -4.428637 -4.4287233 -4.428781 -4.4288092 -4.42883][-4.4283891 -4.428381 -4.4283729 -4.42836 -4.4283285 -4.42829 -4.4282637 -4.4283023 -4.4283867 -4.4284844 -4.4285965 -4.4286947 -4.4287639 -4.4288006 -4.4288278][-4.4283848 -4.4283795 -4.4283686 -4.4283462 -4.4283032 -4.4282513 -4.4282131 -4.4282403 -4.42832 -4.428422 -4.4285431 -4.4286528 -4.4287343 -4.4287839 -4.4288206][-4.42838 -4.4283743 -4.428359 -4.4283304 -4.4282813 -4.4282207 -4.4281764 -4.428194 -4.4282675 -4.4283719 -4.428503 -4.4286222 -4.4287124 -4.4287705 -4.4288154][-4.4283934 -4.4283915 -4.428381 -4.4283581 -4.4283142 -4.4282537 -4.4282074 -4.4282141 -4.4282727 -4.4283671 -4.428493 -4.4286108 -4.4287019 -4.4287634 -4.4288139][-4.4284492 -4.4284511 -4.42845 -4.4284444 -4.4284153 -4.4283657 -4.4283185 -4.428308 -4.4283438 -4.4284158 -4.4285197 -4.4286218 -4.4287038 -4.4287648 -4.4288182][-4.4285526 -4.428555 -4.4285583 -4.4285626 -4.4285507 -4.42852 -4.4284844 -4.4284678 -4.4284835 -4.4285288 -4.4286 -4.4286723 -4.4287343 -4.4287853 -4.4288325][-4.4286757 -4.428679 -4.4286833 -4.428689 -4.4286876 -4.4286757 -4.4286575 -4.4286456 -4.4286504 -4.4286718 -4.4287095 -4.4287496 -4.4287848 -4.4288192 -4.4288535][-4.4287314 -4.4287391 -4.4287457 -4.4287529 -4.4287553 -4.4287496 -4.4287419 -4.4287395 -4.4287462 -4.428762 -4.4287863 -4.4288096 -4.4288306 -4.428853 -4.4288754][-4.4286919 -4.4287019 -4.4287105 -4.4287171 -4.4287167 -4.42871 -4.4287081 -4.4287176 -4.4287405 -4.4287682 -4.428803 -4.428834 -4.4288568 -4.4288764 -4.4288926]]...]
INFO - root - 2017-12-08 06:14:10.256872: step 21610, loss = 2.28, batch loss = 2.23 (38.0 examples/sec; 0.211 sec/batch; 18h:11m:58s remains)
INFO - root - 2017-12-08 06:14:12.482881: step 21620, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.220 sec/batch; 19h:01m:51s remains)
INFO - root - 2017-12-08 06:14:14.722627: step 21630, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 18h:54m:52s remains)
INFO - root - 2017-12-08 06:14:16.962396: step 21640, loss = 2.28, batch loss = 2.23 (35.2 examples/sec; 0.227 sec/batch; 19h:37m:14s remains)
INFO - root - 2017-12-08 06:14:19.233020: step 21650, loss = 2.28, batch loss = 2.23 (34.9 examples/sec; 0.229 sec/batch; 19h:46m:55s remains)
INFO - root - 2017-12-08 06:14:21.463354: step 21660, loss = 2.28, batch loss = 2.23 (34.5 examples/sec; 0.232 sec/batch; 20h:01m:23s remains)
INFO - root - 2017-12-08 06:14:23.702353: step 21670, loss = 2.28, batch loss = 2.23 (37.0 examples/sec; 0.216 sec/batch; 18h:40m:22s remains)
INFO - root - 2017-12-08 06:14:25.958011: step 21680, loss = 2.28, batch loss = 2.23 (35.1 examples/sec; 0.228 sec/batch; 19h:41m:55s remains)
INFO - root - 2017-12-08 06:14:28.228581: step 21690, loss = 2.28, batch loss = 2.23 (34.5 examples/sec; 0.232 sec/batch; 20h:00m:07s remains)
INFO - root - 2017-12-08 06:14:30.471775: step 21700, loss = 2.28, batch loss = 2.23 (35.1 examples/sec; 0.228 sec/batch; 19h:40m:08s remains)
2017-12-08 06:14:30.744425: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4287558 -4.4287748 -4.4287853 -4.4287853 -4.4287825 -4.4287658 -4.4287224 -4.4286747 -4.428658 -4.4286895 -4.4287324 -4.428761 -4.4287996 -4.4288249 -4.428844][-4.4287338 -4.4287496 -4.4287519 -4.4287467 -4.4287367 -4.4287162 -4.4286628 -4.4285984 -4.4285779 -4.428617 -4.4286675 -4.4287038 -4.428751 -4.428792 -4.4288225][-4.4287186 -4.4287333 -4.4287372 -4.42873 -4.4287124 -4.4286928 -4.4286318 -4.4285526 -4.4285231 -4.4285712 -4.4286313 -4.428679 -4.4287386 -4.4287868 -4.4288182][-4.4287081 -4.42872 -4.4287333 -4.4287224 -4.4286938 -4.4286666 -4.4286036 -4.4285207 -4.428493 -4.4285564 -4.4286327 -4.4286904 -4.4287558 -4.4287992 -4.4288268][-4.4287062 -4.4287038 -4.428709 -4.4286895 -4.4286413 -4.4285884 -4.4285 -4.4284005 -4.4283881 -4.4285016 -4.428618 -4.428699 -4.4287682 -4.42881 -4.4288359][-4.4287128 -4.4286966 -4.4286857 -4.4286542 -4.4285822 -4.4284739 -4.4283137 -4.4281516 -4.4281573 -4.4283557 -4.428545 -4.4286718 -4.4287648 -4.4288177 -4.4288449][-4.4287219 -4.4287081 -4.42869 -4.4286532 -4.4285665 -4.4284148 -4.4281898 -4.4279709 -4.4279985 -4.428256 -4.4284921 -4.4286542 -4.4287748 -4.4288378 -4.428864][-4.4287376 -4.4287348 -4.4287295 -4.4287138 -4.4286518 -4.4285173 -4.4283113 -4.4281297 -4.4281526 -4.42836 -4.4285641 -4.4287143 -4.4288287 -4.4288797 -4.4288921][-4.4287658 -4.4287748 -4.4287853 -4.4287848 -4.4287477 -4.4286551 -4.4285007 -4.4283781 -4.4283872 -4.428524 -4.4286804 -4.4288054 -4.4288936 -4.4289222 -4.42892][-4.4288011 -4.4288239 -4.4288449 -4.4288445 -4.4288125 -4.4287424 -4.4286151 -4.42852 -4.4285307 -4.4286189 -4.428741 -4.428854 -4.4289284 -4.4289422 -4.4289312][-4.4288249 -4.4288573 -4.4288855 -4.4288769 -4.4288383 -4.4287677 -4.4286504 -4.4285655 -4.4285793 -4.4286489 -4.4287562 -4.4288664 -4.4289355 -4.4289474 -4.428936][-4.4288273 -4.4288535 -4.4288788 -4.4288683 -4.4288321 -4.4287634 -4.42866 -4.4285941 -4.4286089 -4.4286718 -4.428771 -4.428875 -4.428936 -4.4289465 -4.4289393][-4.4288278 -4.4288392 -4.4288554 -4.4288564 -4.4288344 -4.4287806 -4.4287057 -4.4286623 -4.4286752 -4.4287262 -4.4288054 -4.4288898 -4.428937 -4.4289446 -4.4289403][-4.4288039 -4.4288011 -4.4288168 -4.4288249 -4.428813 -4.4287782 -4.4287314 -4.4287095 -4.428721 -4.4287629 -4.4288254 -4.4288878 -4.4289255 -4.4289331 -4.4289351][-4.4287772 -4.4287686 -4.4287791 -4.428781 -4.4287682 -4.4287453 -4.4287186 -4.4287057 -4.4287167 -4.4287519 -4.4288092 -4.4288621 -4.4288979 -4.4289165 -4.4289317]]...]
INFO - root - 2017-12-08 06:14:32.972872: step 21710, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 19h:13m:05s remains)
INFO - root - 2017-12-08 06:14:35.210255: step 21720, loss = 2.28, batch loss = 2.23 (35.6 examples/sec; 0.225 sec/batch; 19h:25m:12s remains)
INFO - root - 2017-12-08 06:14:37.474788: step 21730, loss = 2.28, batch loss = 2.23 (33.3 examples/sec; 0.240 sec/batch; 20h:44m:32s remains)
INFO - root - 2017-12-08 06:14:39.729729: step 21740, loss = 2.28, batch loss = 2.23 (36.0 examples/sec; 0.222 sec/batch; 19h:11m:59s remains)
INFO - root - 2017-12-08 06:14:41.951520: step 21750, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 18h:57m:33s remains)
INFO - root - 2017-12-08 06:14:44.182438: step 21760, loss = 2.28, batch loss = 2.23 (35.6 examples/sec; 0.225 sec/batch; 19h:23m:03s remains)
INFO - root - 2017-12-08 06:14:46.416652: step 21770, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.222 sec/batch; 19h:07m:45s remains)
INFO - root - 2017-12-08 06:14:48.648548: step 21780, loss = 2.28, batch loss = 2.23 (35.6 examples/sec; 0.225 sec/batch; 19h:23m:25s remains)
INFO - root - 2017-12-08 06:14:50.934844: step 21790, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.220 sec/batch; 19h:00m:38s remains)
INFO - root - 2017-12-08 06:14:53.166562: step 21800, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 18h:59m:28s remains)
2017-12-08 06:14:53.478479: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4287009 -4.4286909 -4.4286928 -4.4286828 -4.428659 -4.4286118 -4.4285579 -4.428534 -4.42857 -4.4286513 -4.4287186 -4.4287825 -4.4288259 -4.4288406 -4.4288487][-4.4286489 -4.4286485 -4.4286628 -4.4286647 -4.4286547 -4.4286141 -4.4285636 -4.4285383 -4.428576 -4.4286633 -4.4287434 -4.4288149 -4.4288635 -4.4288845 -4.4288993][-4.4285812 -4.4285932 -4.4286194 -4.4286389 -4.4286394 -4.4286113 -4.4285817 -4.4285731 -4.4286189 -4.4287076 -4.4287839 -4.4288421 -4.4288807 -4.4289012 -4.4289117][-4.4285283 -4.428546 -4.4285879 -4.4286213 -4.4286222 -4.4286032 -4.4285984 -4.4286122 -4.428668 -4.4287524 -4.4288187 -4.4288692 -4.4288988 -4.4289107 -4.4289088][-4.4285326 -4.4285545 -4.4286089 -4.428648 -4.4286366 -4.4286122 -4.4286132 -4.4286342 -4.4286919 -4.4287729 -4.428843 -4.4288936 -4.4289188 -4.42892 -4.4288993][-4.4285936 -4.4286103 -4.4286671 -4.4287014 -4.4286718 -4.4286304 -4.4286108 -4.4286165 -4.4286733 -4.4287629 -4.4288392 -4.4288893 -4.4289145 -4.4289131 -4.4288788][-4.4286823 -4.4286814 -4.4287171 -4.4287348 -4.4286985 -4.4286451 -4.4286003 -4.4285955 -4.4286566 -4.4287462 -4.4288211 -4.4288669 -4.4288917 -4.4288869 -4.4288449][-4.4287596 -4.4287314 -4.4287271 -4.4287133 -4.4286742 -4.4286218 -4.4285746 -4.428576 -4.4286356 -4.4287086 -4.4287729 -4.4288154 -4.4288363 -4.4288263 -4.4287925][-4.4287825 -4.4287343 -4.428689 -4.4286418 -4.4285936 -4.4285517 -4.4285336 -4.4285631 -4.42862 -4.4286733 -4.4287219 -4.4287553 -4.4287686 -4.42876 -4.4287472][-4.4287648 -4.4287105 -4.4286461 -4.4285817 -4.4285297 -4.4285045 -4.4285221 -4.4285846 -4.428647 -4.42869 -4.4287229 -4.4287386 -4.4287395 -4.428731 -4.4287324][-4.4287171 -4.4286747 -4.4286232 -4.4285779 -4.4285445 -4.4285383 -4.4285765 -4.4286489 -4.4287138 -4.4287558 -4.42878 -4.4287844 -4.4287763 -4.4287672 -4.4287672][-4.42864 -4.42862 -4.4286032 -4.4286017 -4.4286122 -4.4286337 -4.4286771 -4.4287429 -4.4288015 -4.4288454 -4.4288707 -4.4288774 -4.42887 -4.4288568 -4.4288421][-4.4285851 -4.4285755 -4.428586 -4.4286242 -4.4286728 -4.4287176 -4.4287686 -4.4288278 -4.4288783 -4.4289188 -4.4289465 -4.4289584 -4.4289541 -4.4289379 -4.4289103][-4.4285665 -4.4285641 -4.4285917 -4.4286489 -4.4287171 -4.4287772 -4.4288363 -4.4288836 -4.4289169 -4.428946 -4.4289703 -4.4289837 -4.4289842 -4.4289737 -4.4289494][-4.4285846 -4.4285932 -4.4286289 -4.428689 -4.4287605 -4.428823 -4.428874 -4.4289026 -4.4289122 -4.4289236 -4.4289379 -4.4289489 -4.4289565 -4.4289594 -4.4289513]]...]
INFO - root - 2017-12-08 06:14:55.709372: step 21810, loss = 2.28, batch loss = 2.23 (36.9 examples/sec; 0.217 sec/batch; 18h:42m:29s remains)
INFO - root - 2017-12-08 06:14:57.952261: step 21820, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.225 sec/batch; 19h:25m:23s remains)
INFO - root - 2017-12-08 06:15:00.199074: step 21830, loss = 2.28, batch loss = 2.23 (35.8 examples/sec; 0.223 sec/batch; 19h:16m:14s remains)
INFO - root - 2017-12-08 06:15:02.464529: step 21840, loss = 2.28, batch loss = 2.23 (33.6 examples/sec; 0.238 sec/batch; 20h:34m:14s remains)
INFO - root - 2017-12-08 06:15:04.749582: step 21850, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 19h:02m:59s remains)
INFO - root - 2017-12-08 06:15:06.994985: step 21860, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 19h:13m:42s remains)
INFO - root - 2017-12-08 06:15:09.255130: step 21870, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 19h:12m:58s remains)
INFO - root - 2017-12-08 06:15:11.486622: step 21880, loss = 2.28, batch loss = 2.23 (35.2 examples/sec; 0.227 sec/batch; 19h:34m:57s remains)
INFO - root - 2017-12-08 06:15:13.711069: step 21890, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 19h:13m:06s remains)
INFO - root - 2017-12-08 06:15:15.938337: step 21900, loss = 2.28, batch loss = 2.23 (37.7 examples/sec; 0.212 sec/batch; 18h:19m:24s remains)
2017-12-08 06:15:16.238285: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4286819 -4.4286284 -4.4286208 -4.4286437 -4.42867 -4.4287019 -4.4287176 -4.4287052 -4.4286904 -4.4287066 -4.4287243 -4.4287181 -4.4287028 -4.4286833 -4.42865][-4.4287052 -4.4286523 -4.428638 -4.4286604 -4.4286885 -4.4287267 -4.4287496 -4.4287481 -4.4287486 -4.4287724 -4.4287882 -4.4287906 -4.4287853 -4.4287829 -4.4287791][-4.4287238 -4.4286895 -4.4286819 -4.4287052 -4.4287314 -4.4287643 -4.4287744 -4.4287653 -4.4287615 -4.4287777 -4.428791 -4.4287996 -4.4288135 -4.4288349 -4.428853][-4.4286985 -4.4286909 -4.4287057 -4.4287295 -4.4287467 -4.428771 -4.4287705 -4.4287472 -4.4287257 -4.4287248 -4.4287419 -4.4287658 -4.4287996 -4.4288373 -4.42887][-4.4286051 -4.4286118 -4.4286442 -4.4286642 -4.4286838 -4.4287167 -4.4287143 -4.42868 -4.4286437 -4.4286265 -4.4286571 -4.4287124 -4.4287729 -4.42882 -4.428854][-4.4285216 -4.4285145 -4.428534 -4.4285474 -4.4285803 -4.4286327 -4.428628 -4.4285774 -4.4285283 -4.4285045 -4.4285531 -4.4286532 -4.42875 -4.4287958 -4.4288111][-4.428462 -4.4284358 -4.4284229 -4.4284086 -4.4284348 -4.4284987 -4.4285049 -4.4284539 -4.4284077 -4.4283929 -4.4284558 -4.4285941 -4.4287181 -4.4287586 -4.4287467][-4.4283991 -4.4283524 -4.4283137 -4.4282804 -4.4283028 -4.4283786 -4.4284167 -4.4283981 -4.4283819 -4.4283881 -4.4284558 -4.4285975 -4.4287267 -4.4287534 -4.4287105][-4.4284024 -4.4283452 -4.428297 -4.4282751 -4.428309 -4.4283915 -4.4284616 -4.4284778 -4.4284792 -4.4284921 -4.4285421 -4.4286518 -4.4287643 -4.4287739 -4.4287124][-4.4285111 -4.4284668 -4.4284177 -4.4283986 -4.428422 -4.4284835 -4.4285588 -4.4285831 -4.4285731 -4.4285803 -4.4286127 -4.4286895 -4.4287844 -4.4287939 -4.4287391][-4.4286304 -4.4286089 -4.4285674 -4.4285474 -4.4285545 -4.4285827 -4.4286304 -4.4286432 -4.4286189 -4.4286137 -4.4286394 -4.4286985 -4.428782 -4.4288025 -4.4287672][-4.4287057 -4.4287024 -4.4286618 -4.4286327 -4.4286304 -4.4286351 -4.42865 -4.4286451 -4.4286165 -4.428606 -4.4286366 -4.428688 -4.4287591 -4.4287863 -4.4287663][-4.4287415 -4.4287453 -4.4287081 -4.4286785 -4.4286737 -4.4286652 -4.4286542 -4.4286313 -4.4285917 -4.4285741 -4.4286056 -4.4286542 -4.4287066 -4.4287291 -4.428721][-4.4287457 -4.4287486 -4.4287281 -4.4287124 -4.4287133 -4.4287019 -4.428679 -4.4286461 -4.4285989 -4.428566 -4.428586 -4.428627 -4.4286585 -4.4286737 -4.4286766][-4.4287667 -4.4287691 -4.4287677 -4.4287686 -4.4287772 -4.4287748 -4.4287548 -4.4287248 -4.42868 -4.428637 -4.4286389 -4.4286666 -4.4286761 -4.4286785 -4.4286842]]...]
INFO - root - 2017-12-08 06:15:18.493633: step 21910, loss = 2.28, batch loss = 2.23 (33.9 examples/sec; 0.236 sec/batch; 20h:20m:23s remains)
INFO - root - 2017-12-08 06:15:20.728040: step 21920, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.219 sec/batch; 18h:56m:05s remains)
INFO - root - 2017-12-08 06:15:22.957348: step 21930, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 18h:53m:38s remains)
INFO - root - 2017-12-08 06:15:25.254976: step 21940, loss = 2.28, batch loss = 2.23 (34.4 examples/sec; 0.232 sec/batch; 20h:03m:23s remains)
INFO - root - 2017-12-08 06:15:27.484826: step 21950, loss = 2.28, batch loss = 2.23 (35.0 examples/sec; 0.229 sec/batch; 19h:43m:25s remains)
INFO - root - 2017-12-08 06:15:29.710737: step 21960, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 19h:05m:03s remains)
INFO - root - 2017-12-08 06:15:31.939024: step 21970, loss = 2.28, batch loss = 2.23 (37.4 examples/sec; 0.214 sec/batch; 18h:26m:32s remains)
INFO - root - 2017-12-08 06:15:34.169083: step 21980, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 19h:02m:54s remains)
INFO - root - 2017-12-08 06:15:36.398355: step 21990, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 18h:58m:23s remains)
INFO - root - 2017-12-08 06:15:38.637576: step 22000, loss = 2.28, batch loss = 2.23 (37.0 examples/sec; 0.216 sec/batch; 18h:39m:29s remains)
2017-12-08 06:15:38.976162: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289708 -4.4289474 -4.4289069 -4.4288654 -4.4288416 -4.4288263 -4.4288244 -4.4288263 -4.4288387 -4.4288664 -4.4288898 -4.4289036 -4.4289007 -4.4288874 -4.4288654][-4.4289665 -4.4289284 -4.4288707 -4.4288168 -4.4287958 -4.4287777 -4.4287691 -4.4287729 -4.4287958 -4.4288392 -4.4288707 -4.4288859 -4.428885 -4.428874 -4.4288497][-4.428966 -4.4289227 -4.428854 -4.428791 -4.428771 -4.428751 -4.428719 -4.4287033 -4.4287257 -4.4287825 -4.4288235 -4.4288449 -4.4288483 -4.4288344 -4.4288039][-4.428968 -4.4289265 -4.4288516 -4.4287834 -4.4287624 -4.4287333 -4.4286733 -4.4286246 -4.4286385 -4.4287043 -4.4287558 -4.4287887 -4.4287972 -4.42878 -4.4287395][-4.42897 -4.4289303 -4.4288507 -4.4287744 -4.4287324 -4.4286757 -4.4285741 -4.4285016 -4.4285312 -4.4286323 -4.4287043 -4.4287453 -4.4287553 -4.4287295 -4.428678][-4.4289618 -4.4289222 -4.4288349 -4.4287405 -4.428659 -4.4285488 -4.4283953 -4.42832 -4.4283938 -4.4285502 -4.4286571 -4.4287133 -4.4287329 -4.4287138 -4.4286709][-4.4289541 -4.4289188 -4.4288349 -4.42873 -4.4286122 -4.4284439 -4.4282408 -4.4281545 -4.42825 -4.4284472 -4.4285903 -4.428669 -4.4287004 -4.4287024 -4.4286847][-4.4289613 -4.4289365 -4.4288712 -4.4287748 -4.4286566 -4.4284854 -4.428299 -4.4282084 -4.4282694 -4.4284482 -4.4285903 -4.4286656 -4.4287009 -4.4287252 -4.4287362][-4.428966 -4.4289522 -4.4289064 -4.4288354 -4.4287472 -4.4286242 -4.4284992 -4.4284282 -4.4284534 -4.4285774 -4.4286804 -4.4287381 -4.4287596 -4.4287796 -4.4288006][-4.4289627 -4.4289522 -4.42892 -4.4288754 -4.4288197 -4.4287453 -4.4286776 -4.4286256 -4.4286227 -4.4286895 -4.4287515 -4.4287953 -4.4288077 -4.4288144 -4.4288268][-4.4289575 -4.4289474 -4.4289312 -4.4289141 -4.4288869 -4.428843 -4.4288077 -4.4287677 -4.4287477 -4.4287758 -4.428813 -4.4288521 -4.4288611 -4.4288611 -4.428863][-4.4289513 -4.4289436 -4.4289436 -4.4289522 -4.4289513 -4.4289322 -4.4289188 -4.4288912 -4.4288683 -4.4288783 -4.4289007 -4.4289346 -4.4289379 -4.4289269 -4.4289236][-4.4289513 -4.4289479 -4.4289551 -4.4289761 -4.4289947 -4.428997 -4.4289989 -4.4289846 -4.4289694 -4.4289765 -4.4289894 -4.4290147 -4.4290142 -4.4289989 -4.4289937][-4.4289556 -4.4289551 -4.4289613 -4.4289789 -4.429 -4.4290094 -4.4290137 -4.4290075 -4.428998 -4.4290009 -4.4290075 -4.4290218 -4.4290204 -4.4290071 -4.429][-4.4289641 -4.4289622 -4.4289651 -4.4289737 -4.4289842 -4.4289908 -4.4289894 -4.4289818 -4.4289746 -4.4289732 -4.4289708 -4.4289742 -4.4289746 -4.428966 -4.4289589]]...]
INFO - root - 2017-12-08 06:15:41.219500: step 22010, loss = 2.28, batch loss = 2.23 (35.0 examples/sec; 0.229 sec/batch; 19h:42m:36s remains)
INFO - root - 2017-12-08 06:15:43.461836: step 22020, loss = 2.28, batch loss = 2.23 (35.1 examples/sec; 0.228 sec/batch; 19h:39m:26s remains)
INFO - root - 2017-12-08 06:15:45.720068: step 22030, loss = 2.28, batch loss = 2.23 (35.4 examples/sec; 0.226 sec/batch; 19h:27m:55s remains)
INFO - root - 2017-12-08 06:15:47.955775: step 22040, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.222 sec/batch; 19h:07m:26s remains)
INFO - root - 2017-12-08 06:15:50.162931: step 22050, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 19h:11m:50s remains)
INFO - root - 2017-12-08 06:15:52.413722: step 22060, loss = 2.28, batch loss = 2.23 (34.0 examples/sec; 0.235 sec/batch; 20h:16m:44s remains)
INFO - root - 2017-12-08 06:15:54.677101: step 22070, loss = 2.28, batch loss = 2.23 (34.7 examples/sec; 0.231 sec/batch; 19h:53m:15s remains)
INFO - root - 2017-12-08 06:15:56.907311: step 22080, loss = 2.28, batch loss = 2.23 (34.5 examples/sec; 0.232 sec/batch; 20h:01m:20s remains)
INFO - root - 2017-12-08 06:15:59.151658: step 22090, loss = 2.28, batch loss = 2.23 (35.0 examples/sec; 0.228 sec/batch; 19h:41m:05s remains)
INFO - root - 2017-12-08 06:16:01.380696: step 22100, loss = 2.28, batch loss = 2.23 (35.1 examples/sec; 0.228 sec/batch; 19h:40m:18s remains)
2017-12-08 06:16:01.673836: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289479 -4.4289594 -4.4289632 -4.4289641 -4.4289565 -4.4289441 -4.4289341 -4.4289269 -4.42893 -4.4289389 -4.4289489 -4.4289613 -4.42897 -4.4289732 -4.4289708][-4.4289379 -4.4289403 -4.4289436 -4.428946 -4.4289284 -4.4289079 -4.4288945 -4.428884 -4.4288893 -4.4289031 -4.428925 -4.4289474 -4.4289665 -4.4289794 -4.4289823][-4.4289122 -4.4289064 -4.4289021 -4.4288878 -4.4288435 -4.42881 -4.428793 -4.4287834 -4.4287977 -4.4288292 -4.4288697 -4.4289021 -4.4289265 -4.4289422 -4.428947][-4.4288859 -4.42887 -4.4288468 -4.4288015 -4.4287291 -4.428689 -4.4286861 -4.4286971 -4.4287343 -4.4287777 -4.428822 -4.4288464 -4.4288535 -4.4288545 -4.4288487][-4.428875 -4.4288416 -4.4287868 -4.4287009 -4.428596 -4.4285583 -4.4285917 -4.4286361 -4.4286904 -4.4287353 -4.4287686 -4.4287753 -4.4287667 -4.4287548 -4.4287438][-4.4288592 -4.4287992 -4.4287119 -4.4285793 -4.428431 -4.428391 -4.4284773 -4.4285641 -4.4286308 -4.4286709 -4.4286895 -4.4286861 -4.4286737 -4.4286642 -4.428647][-4.4288177 -4.4287286 -4.4286141 -4.4284587 -4.4282985 -4.42826 -4.428381 -4.4284921 -4.4285636 -4.4285975 -4.4285946 -4.4285836 -4.4285688 -4.4285579 -4.4285378][-4.4287577 -4.4286475 -4.4285212 -4.42839 -4.4282808 -4.4282656 -4.4283767 -4.4284744 -4.4285316 -4.4285569 -4.4285326 -4.4284973 -4.4284635 -4.4284358 -4.4284024][-4.4287057 -4.428597 -4.4284878 -4.4284 -4.4283547 -4.428371 -4.4284606 -4.4285293 -4.4285564 -4.4285665 -4.428545 -4.4284992 -4.4284487 -4.4283991 -4.4283495][-4.4286842 -4.4285975 -4.4285264 -4.4284854 -4.4284883 -4.4285293 -4.4286041 -4.4286442 -4.4286475 -4.4286513 -4.4286361 -4.428596 -4.4285393 -4.428473 -4.4284186][-4.4287095 -4.428658 -4.4286318 -4.4286289 -4.4286528 -4.4287 -4.4287639 -4.4287891 -4.4287791 -4.4287753 -4.4287663 -4.4287438 -4.4287004 -4.4286385 -4.4285941][-4.42875 -4.4287271 -4.428731 -4.4287591 -4.4287977 -4.4288459 -4.4289012 -4.4289145 -4.4288898 -4.4288716 -4.4288664 -4.4288597 -4.4288368 -4.4287963 -4.4287705][-4.4287877 -4.42878 -4.4287949 -4.4288406 -4.4288898 -4.4289365 -4.4289784 -4.4289765 -4.4289403 -4.4289184 -4.4289174 -4.4289188 -4.4289093 -4.4288878 -4.4288793][-4.4288325 -4.4288354 -4.4288511 -4.4288955 -4.4289441 -4.4289823 -4.4290051 -4.4289865 -4.4289522 -4.4289374 -4.428937 -4.4289389 -4.4289341 -4.4289227 -4.4289188][-4.42888 -4.4288931 -4.4289083 -4.4289417 -4.4289722 -4.4289923 -4.4289966 -4.4289727 -4.4289455 -4.4289346 -4.4289322 -4.4289327 -4.42893 -4.4289241 -4.4289212]]...]
INFO - root - 2017-12-08 06:16:03.906692: step 22110, loss = 2.28, batch loss = 2.23 (33.4 examples/sec; 0.240 sec/batch; 20h:39m:30s remains)
INFO - root - 2017-12-08 06:16:06.157553: step 22120, loss = 2.28, batch loss = 2.23 (34.0 examples/sec; 0.236 sec/batch; 20h:18m:25s remains)
INFO - root - 2017-12-08 06:16:08.452630: step 22130, loss = 2.28, batch loss = 2.23 (34.1 examples/sec; 0.235 sec/batch; 20h:13m:13s remains)
INFO - root - 2017-12-08 06:16:10.674430: step 22140, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.225 sec/batch; 19h:26m:10s remains)
INFO - root - 2017-12-08 06:16:12.908664: step 22150, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 19h:12m:20s remains)
INFO - root - 2017-12-08 06:16:15.139049: step 22160, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.221 sec/batch; 19h:04m:41s remains)
INFO - root - 2017-12-08 06:16:17.374656: step 22170, loss = 2.28, batch loss = 2.23 (35.3 examples/sec; 0.226 sec/batch; 19h:30m:51s remains)
INFO - root - 2017-12-08 06:16:19.606141: step 22180, loss = 2.28, batch loss = 2.23 (36.9 examples/sec; 0.217 sec/batch; 18h:39m:53s remains)
INFO - root - 2017-12-08 06:16:21.836316: step 22190, loss = 2.28, batch loss = 2.23 (36.0 examples/sec; 0.222 sec/batch; 19h:09m:56s remains)
INFO - root - 2017-12-08 06:16:24.109301: step 22200, loss = 2.28, batch loss = 2.23 (35.8 examples/sec; 0.223 sec/batch; 19h:15m:48s remains)
2017-12-08 06:16:24.392944: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4286942 -4.4286866 -4.4286437 -4.4286084 -4.4286203 -4.428668 -4.4287295 -4.428791 -4.4288425 -4.4288783 -4.4288955 -4.428885 -4.4288645 -4.4288554 -4.42886][-4.4286561 -4.428669 -4.4286323 -4.4285922 -4.4286103 -4.428668 -4.4287343 -4.4287939 -4.4288478 -4.428894 -4.4289184 -4.4289112 -4.4288955 -4.4288893 -4.4288917][-4.4286232 -4.4286528 -4.4286242 -4.4285836 -4.4286075 -4.4286695 -4.4287343 -4.4287891 -4.42884 -4.4288936 -4.4289227 -4.4289203 -4.4289074 -4.4288988 -4.4288964][-4.4286189 -4.4286451 -4.42862 -4.428587 -4.4286127 -4.4286675 -4.4287205 -4.428762 -4.4288054 -4.42886 -4.428894 -4.4288979 -4.4288859 -4.4288731 -4.4288673][-4.428658 -4.4286613 -4.4286313 -4.4286041 -4.428627 -4.4286647 -4.4286952 -4.4287133 -4.4287424 -4.4288006 -4.428843 -4.4288578 -4.4288473 -4.4288292 -4.4288182][-4.4287171 -4.4287009 -4.4286642 -4.4286342 -4.4286423 -4.4286523 -4.428648 -4.4286346 -4.428648 -4.4287157 -4.4287758 -4.4288096 -4.4288068 -4.4287848 -4.4287672][-4.42877 -4.4287438 -4.4287004 -4.4286609 -4.428648 -4.4286294 -4.4285874 -4.4285307 -4.4285212 -4.4286032 -4.4286885 -4.4287462 -4.4287586 -4.428741 -4.42872][-4.4288034 -4.4287777 -4.4287386 -4.428699 -4.4286728 -4.4286265 -4.4285445 -4.4284387 -4.4283972 -4.4284897 -4.4285965 -4.4286752 -4.4287033 -4.4286962 -4.428679][-4.4288135 -4.4287925 -4.4287677 -4.4287453 -4.4287233 -4.4286642 -4.4285564 -4.4284186 -4.4283476 -4.4284291 -4.4285374 -4.4286218 -4.4286637 -4.4286704 -4.4286618][-4.4287915 -4.4287853 -4.4287791 -4.4287806 -4.4287772 -4.4287314 -4.4286308 -4.4284973 -4.4284158 -4.4284654 -4.4285483 -4.428618 -4.4286647 -4.4286838 -4.4286828][-4.4287782 -4.42879 -4.428802 -4.428822 -4.4288368 -4.4288158 -4.4287415 -4.4286342 -4.4285622 -4.42858 -4.428627 -4.4286718 -4.4287148 -4.4287343 -4.4287291][-4.42881 -4.4288335 -4.4288568 -4.4288831 -4.4289002 -4.428894 -4.4288464 -4.4287691 -4.4287148 -4.4287152 -4.428731 -4.4287496 -4.4287791 -4.4287887 -4.4287682][-4.4288716 -4.4288945 -4.4289193 -4.4289403 -4.4289532 -4.4289546 -4.4289269 -4.4288735 -4.4288335 -4.4288254 -4.4288235 -4.4288244 -4.4288435 -4.4288445 -4.4288135][-4.4289312 -4.4289422 -4.4289556 -4.4289646 -4.428977 -4.42899 -4.4289818 -4.4289479 -4.4289169 -4.4289031 -4.4288936 -4.4288883 -4.4289 -4.4288969 -4.428865][-4.4289722 -4.4289684 -4.428967 -4.4289651 -4.4289761 -4.4289966 -4.4290061 -4.4289908 -4.4289694 -4.4289546 -4.4289432 -4.4289389 -4.4289432 -4.4289374 -4.4289079]]...]
INFO - root - 2017-12-08 06:16:26.619992: step 22210, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.221 sec/batch; 19h:00m:44s remains)
INFO - root - 2017-12-08 06:16:28.861857: step 22220, loss = 2.28, batch loss = 2.23 (35.6 examples/sec; 0.225 sec/batch; 19h:21m:10s remains)
INFO - root - 2017-12-08 06:16:31.093812: step 22230, loss = 2.28, batch loss = 2.23 (35.3 examples/sec; 0.227 sec/batch; 19h:31m:59s remains)
INFO - root - 2017-12-08 06:16:33.369317: step 22240, loss = 2.28, batch loss = 2.23 (34.2 examples/sec; 0.234 sec/batch; 20h:08m:11s remains)
INFO - root - 2017-12-08 06:16:35.612094: step 22250, loss = 2.28, batch loss = 2.23 (37.3 examples/sec; 0.215 sec/batch; 18h:29m:44s remains)
INFO - root - 2017-12-08 06:16:37.855913: step 22260, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.222 sec/batch; 19h:06m:50s remains)
INFO - root - 2017-12-08 06:16:40.108573: step 22270, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.222 sec/batch; 19h:06m:21s remains)
INFO - root - 2017-12-08 06:16:42.350552: step 22280, loss = 2.28, batch loss = 2.23 (35.4 examples/sec; 0.226 sec/batch; 19h:29m:09s remains)
INFO - root - 2017-12-08 06:16:44.600155: step 22290, loss = 2.28, batch loss = 2.23 (35.0 examples/sec; 0.228 sec/batch; 19h:41m:13s remains)
INFO - root - 2017-12-08 06:16:46.837020: step 22300, loss = 2.28, batch loss = 2.23 (35.2 examples/sec; 0.227 sec/batch; 19h:35m:17s remains)
2017-12-08 06:16:47.144775: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289265 -4.4289017 -4.4288931 -4.4288907 -4.4288683 -4.4288106 -4.428721 -4.4286447 -4.4286189 -4.4286265 -4.4286442 -4.4286723 -4.4287195 -4.4287739 -4.4288182][-4.4289002 -4.4288783 -4.4288678 -4.4288564 -4.428812 -4.4287133 -4.428586 -4.4284911 -4.4284782 -4.4285173 -4.4285603 -4.428606 -4.4286752 -4.4287481 -4.4288039][-4.4288807 -4.4288635 -4.4288492 -4.4288192 -4.4287367 -4.42859 -4.42843 -4.4283395 -4.4283705 -4.4284587 -4.4285221 -4.4285727 -4.4286528 -4.42874 -4.4287977][-4.428865 -4.42885 -4.4288263 -4.4287682 -4.4286408 -4.4284444 -4.428247 -4.428175 -4.4282866 -4.4284329 -4.4285059 -4.42855 -4.4286284 -4.4287186 -4.4287772][-4.428863 -4.4288416 -4.4288006 -4.4287076 -4.4285278 -4.4282689 -4.4280214 -4.4279871 -4.4282269 -4.4284554 -4.4285398 -4.42858 -4.4286461 -4.4287186 -4.4287696][-4.4288769 -4.4288492 -4.4287915 -4.4286547 -4.4284081 -4.4280648 -4.427743 -4.4277577 -4.4281564 -4.4284687 -4.4285684 -4.4286151 -4.4286761 -4.428741 -4.4287853][-4.4289117 -4.4288883 -4.4288206 -4.4286494 -4.4283481 -4.4279318 -4.4275403 -4.4275851 -4.4280849 -4.4284468 -4.42856 -4.4286141 -4.4286833 -4.4287567 -4.4287972][-4.4289384 -4.4289193 -4.4288507 -4.4286733 -4.4283738 -4.4279709 -4.4276075 -4.4276657 -4.428134 -4.4284778 -4.4285889 -4.4286456 -4.4287143 -4.4287829 -4.4288116][-4.4289441 -4.4289308 -4.4288759 -4.4287257 -4.4284768 -4.4281516 -4.4278712 -4.4279108 -4.4282665 -4.428546 -4.4286466 -4.4287019 -4.42877 -4.4288244 -4.4288321][-4.428946 -4.4289379 -4.428895 -4.4287772 -4.4285922 -4.4283586 -4.4281511 -4.4281573 -4.4283996 -4.4286141 -4.4287043 -4.42876 -4.428833 -4.4288692 -4.4288564][-4.4289432 -4.428937 -4.4289074 -4.428823 -4.4286985 -4.428546 -4.4283919 -4.4283762 -4.4285421 -4.4287014 -4.4287715 -4.4288268 -4.4289036 -4.4289241 -4.428895][-4.4289246 -4.4289145 -4.4288955 -4.4288445 -4.4287724 -4.4286828 -4.4285746 -4.4285541 -4.428669 -4.4287806 -4.4288268 -4.4288778 -4.4289489 -4.4289641 -4.4289351][-4.4289031 -4.4288874 -4.4288764 -4.4288516 -4.4288149 -4.4287615 -4.42869 -4.4286809 -4.4287682 -4.4288487 -4.4288769 -4.4289103 -4.4289632 -4.4289813 -4.4289641][-4.42889 -4.4288707 -4.4288678 -4.4288611 -4.4288411 -4.4288058 -4.4287596 -4.4287643 -4.428833 -4.4288955 -4.4289207 -4.4289379 -4.4289689 -4.4289804 -4.4289751][-4.4288874 -4.428874 -4.4288816 -4.4288878 -4.4288754 -4.4288449 -4.4288087 -4.4288116 -4.4288573 -4.4289026 -4.4289255 -4.4289351 -4.4289551 -4.4289684 -4.428978]]...]
INFO - root - 2017-12-08 06:16:49.373737: step 22310, loss = 2.28, batch loss = 2.23 (36.0 examples/sec; 0.222 sec/batch; 19h:07m:38s remains)
INFO - root - 2017-12-08 06:16:51.640509: step 22320, loss = 2.28, batch loss = 2.23 (34.7 examples/sec; 0.230 sec/batch; 19h:50m:34s remains)
INFO - root - 2017-12-08 06:16:53.879882: step 22330, loss = 2.28, batch loss = 2.23 (35.4 examples/sec; 0.226 sec/batch; 19h:29m:47s remains)
INFO - root - 2017-12-08 06:16:56.082009: step 22340, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 18h:53m:05s remains)
INFO - root - 2017-12-08 06:16:58.323603: step 22350, loss = 2.28, batch loss = 2.23 (35.4 examples/sec; 0.226 sec/batch; 19h:29m:34s remains)
INFO - root - 2017-12-08 06:17:00.559149: step 22360, loss = 2.28, batch loss = 2.23 (34.5 examples/sec; 0.232 sec/batch; 19h:58m:34s remains)
INFO - root - 2017-12-08 06:17:02.807584: step 22370, loss = 2.28, batch loss = 2.23 (33.8 examples/sec; 0.237 sec/batch; 20h:22m:39s remains)
INFO - root - 2017-12-08 06:17:05.051118: step 22380, loss = 2.28, batch loss = 2.23 (37.3 examples/sec; 0.214 sec/batch; 18h:28m:31s remains)
INFO - root - 2017-12-08 06:17:07.283344: step 22390, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 18h:55m:50s remains)
INFO - root - 2017-12-08 06:17:09.516203: step 22400, loss = 2.28, batch loss = 2.23 (35.2 examples/sec; 0.228 sec/batch; 19h:36m:13s remains)
2017-12-08 06:17:09.809793: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288769 -4.4288664 -4.4288578 -4.4288549 -4.4288568 -4.4288583 -4.4288559 -4.4288487 -4.4288282 -4.4288077 -4.4287982 -4.4288082 -4.4288173 -4.4288149 -4.4288][-4.4288568 -4.4288421 -4.428834 -4.42883 -4.4288278 -4.4288211 -4.4288106 -4.4287977 -4.4287686 -4.4287367 -4.428719 -4.4287343 -4.4287519 -4.4287553 -4.4287434][-4.4288359 -4.4288187 -4.4288058 -4.4287887 -4.428771 -4.4287524 -4.4287262 -4.428699 -4.4286594 -4.4286213 -4.4286046 -4.4286404 -4.4286704 -4.4286866 -4.4286871][-4.4288139 -4.4287896 -4.4287639 -4.4287257 -4.4286871 -4.4286518 -4.4286051 -4.4285612 -4.42853 -4.4284978 -4.4284916 -4.4285469 -4.4285936 -4.4286389 -4.4286637][-4.4287767 -4.4287372 -4.4286919 -4.4286327 -4.4285727 -4.4285183 -4.428462 -4.4284296 -4.428432 -4.4284248 -4.4284353 -4.4284978 -4.4285531 -4.428627 -4.4286795][-4.4287052 -4.4286389 -4.4285688 -4.4284868 -4.4284143 -4.4283571 -4.4283028 -4.4283123 -4.428371 -4.4284058 -4.42844 -4.4284978 -4.4285569 -4.4286437 -4.4287143][-4.4285908 -4.4284959 -4.4283948 -4.4282913 -4.4282236 -4.428206 -4.4281936 -4.4282708 -4.4283857 -4.4284577 -4.4285097 -4.4285588 -4.4286113 -4.4286933 -4.4287629][-4.4284444 -4.4283304 -4.4282165 -4.4281111 -4.4280696 -4.4281321 -4.4282036 -4.4283543 -4.4284954 -4.4285765 -4.4286284 -4.4286613 -4.4287057 -4.4287758 -4.4288273][-4.4283018 -4.4282031 -4.4281359 -4.428103 -4.4281316 -4.4282384 -4.4283452 -4.4284997 -4.4286275 -4.428689 -4.4287281 -4.42876 -4.4288015 -4.4288516 -4.428884][-4.42819 -4.4281368 -4.4281721 -4.4282489 -4.4283276 -4.4284172 -4.4284987 -4.4286222 -4.4287229 -4.4287658 -4.428792 -4.4288254 -4.4288578 -4.4288898 -4.4289112][-4.4281631 -4.4281683 -4.4282756 -4.4284039 -4.4284906 -4.4285464 -4.4285946 -4.4286823 -4.4287615 -4.4287982 -4.4288263 -4.4288549 -4.4288807 -4.4289017 -4.4289255][-4.428256 -4.428298 -4.4284091 -4.4285183 -4.428587 -4.428627 -4.4286633 -4.4287295 -4.428793 -4.4288297 -4.428853 -4.4288788 -4.4289093 -4.4289279 -4.4289532][-4.4284043 -4.42846 -4.428545 -4.4286108 -4.4286537 -4.4286876 -4.4287262 -4.4287806 -4.4288316 -4.4288635 -4.428885 -4.4289083 -4.4289417 -4.4289641 -4.4289851][-4.4285445 -4.428597 -4.4286537 -4.4286857 -4.4287181 -4.4287543 -4.4287868 -4.4288282 -4.428864 -4.4288917 -4.4289117 -4.4289346 -4.4289627 -4.4289827 -4.4289947][-4.42865 -4.4286857 -4.4287195 -4.4287376 -4.4287677 -4.4288054 -4.4288378 -4.428864 -4.4288907 -4.4289136 -4.42893 -4.4289465 -4.428966 -4.4289818 -4.4289861]]...]
INFO - root - 2017-12-08 06:17:12.028767: step 22410, loss = 2.28, batch loss = 2.23 (37.4 examples/sec; 0.214 sec/batch; 18h:25m:28s remains)
INFO - root - 2017-12-08 06:17:14.284173: step 22420, loss = 2.28, batch loss = 2.23 (35.6 examples/sec; 0.225 sec/batch; 19h:21m:46s remains)
INFO - root - 2017-12-08 06:17:16.552918: step 22430, loss = 2.28, batch loss = 2.23 (35.8 examples/sec; 0.223 sec/batch; 19h:13m:26s remains)
INFO - root - 2017-12-08 06:17:18.794431: step 22440, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 19h:11m:16s remains)
INFO - root - 2017-12-08 06:17:21.044676: step 22450, loss = 2.28, batch loss = 2.23 (35.7 examples/sec; 0.224 sec/batch; 19h:16m:38s remains)
INFO - root - 2017-12-08 06:17:23.281652: step 22460, loss = 2.28, batch loss = 2.23 (36.9 examples/sec; 0.217 sec/batch; 18h:39m:02s remains)
INFO - root - 2017-12-08 06:17:25.490428: step 22470, loss = 2.28, batch loss = 2.23 (36.6 examples/sec; 0.219 sec/batch; 18h:49m:27s remains)
INFO - root - 2017-12-08 06:17:27.719448: step 22480, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 18h:54m:30s remains)
INFO - root - 2017-12-08 06:17:29.942372: step 22490, loss = 2.28, batch loss = 2.23 (35.8 examples/sec; 0.224 sec/batch; 19h:15m:10s remains)
INFO - root - 2017-12-08 06:17:32.186433: step 22500, loss = 2.28, batch loss = 2.23 (35.7 examples/sec; 0.224 sec/batch; 19h:19m:17s remains)
2017-12-08 06:17:32.475309: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288988 -4.4289 -4.4288964 -4.4288912 -4.4288845 -4.42888 -4.4288826 -4.428895 -4.4289093 -4.4289136 -4.4289 -4.4288726 -4.4288392 -4.4288168 -4.4288111][-4.4288936 -4.4288945 -4.4288898 -4.4288797 -4.428864 -4.4288521 -4.4288497 -4.4288607 -4.4288774 -4.4288826 -4.42887 -4.4288507 -4.4288306 -4.4288235 -4.4288273][-4.4288983 -4.4289002 -4.4288931 -4.4288778 -4.4288521 -4.4288297 -4.4288192 -4.4288235 -4.4288325 -4.4288316 -4.4288225 -4.4288158 -4.4288149 -4.4288254 -4.4288373][-4.4289131 -4.428915 -4.4289083 -4.4288893 -4.4288554 -4.428822 -4.428802 -4.4287953 -4.42879 -4.4287763 -4.4287653 -4.4287663 -4.4287763 -4.4287987 -4.42882][-4.4289227 -4.428926 -4.4289212 -4.4289017 -4.4288621 -4.4288216 -4.4287925 -4.4287744 -4.4287543 -4.4287276 -4.42871 -4.4287086 -4.4287195 -4.4287457 -4.4287777][-4.4289279 -4.4289331 -4.42893 -4.4289112 -4.42887 -4.4288287 -4.4287958 -4.4287658 -4.428731 -4.4286923 -4.4286609 -4.428647 -4.42865 -4.428678 -4.42872][-4.4289274 -4.4289346 -4.4289322 -4.4289141 -4.428874 -4.4288349 -4.428802 -4.4287653 -4.4287214 -4.4286737 -4.4286265 -4.4285908 -4.4285808 -4.42861 -4.4286613][-4.4289222 -4.4289312 -4.4289274 -4.4289083 -4.4288697 -4.4288321 -4.4288011 -4.4287624 -4.4287119 -4.4286547 -4.4285951 -4.42854 -4.4285254 -4.4285631 -4.4286232][-4.4289141 -4.428925 -4.4289193 -4.4288979 -4.4288578 -4.4288173 -4.4287844 -4.4287434 -4.4286871 -4.4286194 -4.4285479 -4.4284825 -4.4284739 -4.4285283 -4.4285979][-4.4289041 -4.4289179 -4.4289103 -4.4288831 -4.4288383 -4.4287915 -4.4287562 -4.4287167 -4.42866 -4.4285865 -4.4285049 -4.4284339 -4.4284334 -4.4285035 -4.4285765][-4.4288936 -4.4289107 -4.4289026 -4.4288726 -4.4288282 -4.428781 -4.4287491 -4.428721 -4.42867 -4.4285936 -4.4285045 -4.4284267 -4.4284215 -4.4284911 -4.4285583][-4.4288759 -4.4288979 -4.4288931 -4.4288621 -4.4288163 -4.4287677 -4.4287405 -4.4287271 -4.4286876 -4.4286146 -4.42853 -4.4284554 -4.4284363 -4.4284897 -4.4285469][-4.4288507 -4.4288778 -4.4288797 -4.4288507 -4.4288063 -4.4287605 -4.4287395 -4.4287386 -4.4287176 -4.4286633 -4.4286003 -4.4285374 -4.428504 -4.42853 -4.4285688][-4.4288225 -4.4288468 -4.4288526 -4.4288306 -4.428792 -4.4287519 -4.4287415 -4.4287581 -4.4287634 -4.4287314 -4.4286885 -4.4286346 -4.4285855 -4.42858 -4.428597][-4.4287853 -4.4287992 -4.4288073 -4.4287972 -4.4287729 -4.4287457 -4.4287481 -4.4287782 -4.4288 -4.4287896 -4.428762 -4.4287143 -4.4286518 -4.428618 -4.4286194]]...]
INFO - root - 2017-12-08 06:17:34.690468: step 22510, loss = 2.28, batch loss = 2.23 (36.8 examples/sec; 0.217 sec/batch; 18h:42m:30s remains)
INFO - root - 2017-12-08 06:17:36.913810: step 22520, loss = 2.28, batch loss = 2.23 (36.7 examples/sec; 0.218 sec/batch; 18h:45m:51s remains)
INFO - root - 2017-12-08 06:17:39.185956: step 22530, loss = 2.28, batch loss = 2.23 (34.8 examples/sec; 0.230 sec/batch; 19h:47m:52s remains)
INFO - root - 2017-12-08 06:17:41.421616: step 22540, loss = 2.28, batch loss = 2.23 (35.1 examples/sec; 0.228 sec/batch; 19h:37m:07s remains)
INFO - root - 2017-12-08 06:17:43.679273: step 22550, loss = 2.28, batch loss = 2.23 (35.3 examples/sec; 0.226 sec/batch; 19h:29m:10s remains)
INFO - root - 2017-12-08 06:17:45.903550: step 22560, loss = 2.28, batch loss = 2.23 (37.7 examples/sec; 0.212 sec/batch; 18h:16m:55s remains)
INFO - root - 2017-12-08 06:17:48.131625: step 22570, loss = 2.28, batch loss = 2.23 (37.2 examples/sec; 0.215 sec/batch; 18h:31m:59s remains)
INFO - root - 2017-12-08 06:17:50.363029: step 22580, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.220 sec/batch; 18h:57m:06s remains)
INFO - root - 2017-12-08 06:17:52.629723: step 22590, loss = 2.28, batch loss = 2.23 (34.6 examples/sec; 0.231 sec/batch; 19h:54m:00s remains)
INFO - root - 2017-12-08 06:17:54.873709: step 22600, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.222 sec/batch; 19h:05m:10s remains)
2017-12-08 06:17:55.177411: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289351 -4.42892 -4.4289036 -4.4288955 -4.4288912 -4.428894 -4.4288955 -4.4288979 -4.4289012 -4.4288993 -4.4289017 -4.4289088 -4.4289184 -4.4289265 -4.4289331][-4.4289093 -4.4288797 -4.428853 -4.4288373 -4.4288311 -4.4288406 -4.4288478 -4.4288564 -4.4288645 -4.4288616 -4.4288669 -4.4288816 -4.4288936 -4.4288964 -4.4288993][-4.428875 -4.4288311 -4.4287925 -4.4287667 -4.4287572 -4.4287643 -4.4287667 -4.4287786 -4.4287972 -4.4288096 -4.4288254 -4.4288487 -4.4288683 -4.4288669 -4.4288607][-4.4288368 -4.4287796 -4.42873 -4.4286904 -4.4286718 -4.4286747 -4.4286613 -4.4286661 -4.4287014 -4.42873 -4.4287515 -4.4287772 -4.4288073 -4.4288139 -4.4288039][-4.428812 -4.4287434 -4.4286809 -4.4286227 -4.4285855 -4.4285712 -4.4285364 -4.4285378 -4.428606 -4.428658 -4.4286785 -4.4287038 -4.4287419 -4.4287581 -4.4287515][-4.4287891 -4.4287081 -4.4286337 -4.4285641 -4.4285107 -4.4284606 -4.4283814 -4.4283566 -4.4284697 -4.4285588 -4.4285874 -4.4286103 -4.4286571 -4.4286928 -4.4286957][-4.4287491 -4.4286485 -4.4285564 -4.4284797 -4.4284115 -4.4283233 -4.4281759 -4.4281125 -4.4282956 -4.4284387 -4.4284849 -4.4285016 -4.4285588 -4.4286203 -4.4286427][-4.4286952 -4.4285626 -4.428442 -4.4283538 -4.4282689 -4.4281292 -4.4278703 -4.4277415 -4.4280376 -4.4282637 -4.4283261 -4.4283433 -4.4284182 -4.4285178 -4.4285803][-4.4286571 -4.4285054 -4.4283581 -4.428266 -4.428184 -4.4280124 -4.4276605 -4.4274955 -4.4278851 -4.4281645 -4.4282465 -4.4282665 -4.4283524 -4.428463 -4.428545][-4.4286675 -4.4285479 -4.4284391 -4.4283881 -4.4283442 -4.4282227 -4.4279652 -4.4278641 -4.4281273 -4.42833 -4.4283915 -4.4284024 -4.4284558 -4.4285359 -4.4286027][-4.4286962 -4.4286151 -4.4285474 -4.4285173 -4.428494 -4.4284253 -4.428277 -4.4282351 -4.4283819 -4.4284983 -4.4285431 -4.428546 -4.428566 -4.4286108 -4.4286628][-4.4287405 -4.42869 -4.4286437 -4.4286127 -4.4285803 -4.4285254 -4.4284377 -4.428431 -4.4285145 -4.4285746 -4.428607 -4.4286151 -4.4286323 -4.4286685 -4.4287124][-4.4288015 -4.4287758 -4.4287481 -4.4287195 -4.4286866 -4.4286427 -4.4285932 -4.4285951 -4.4286432 -4.4286776 -4.4287 -4.4287105 -4.4287338 -4.4287639 -4.4287887][-4.4288707 -4.4288616 -4.4288554 -4.4288464 -4.428834 -4.42881 -4.4287839 -4.4287887 -4.4288173 -4.4288344 -4.4288445 -4.4288564 -4.428874 -4.4288912 -4.4289031][-4.4289212 -4.4289212 -4.4289241 -4.4289265 -4.4289269 -4.4289203 -4.4289122 -4.4289203 -4.4289374 -4.4289451 -4.4289465 -4.4289489 -4.428956 -4.4289641 -4.4289722]]...]
INFO - root - 2017-12-08 06:17:57.453203: step 22610, loss = 2.28, batch loss = 2.23 (35.1 examples/sec; 0.228 sec/batch; 19h:36m:00s remains)
INFO - root - 2017-12-08 06:17:59.696280: step 22620, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 18h:51m:06s remains)
INFO - root - 2017-12-08 06:18:01.934360: step 22630, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 18h:56m:10s remains)
INFO - root - 2017-12-08 06:18:04.189908: step 22640, loss = 2.28, batch loss = 2.23 (35.2 examples/sec; 0.227 sec/batch; 19h:33m:33s remains)
INFO - root - 2017-12-08 06:18:06.434377: step 22650, loss = 2.28, batch loss = 2.23 (33.9 examples/sec; 0.236 sec/batch; 20h:17m:41s remains)
INFO - root - 2017-12-08 06:18:08.709139: step 22660, loss = 2.28, batch loss = 2.23 (32.6 examples/sec; 0.245 sec/batch; 21h:06m:43s remains)
INFO - root - 2017-12-08 06:18:10.934634: step 22670, loss = 2.28, batch loss = 2.23 (36.8 examples/sec; 0.218 sec/batch; 18h:43m:52s remains)
INFO - root - 2017-12-08 06:18:13.201788: step 22680, loss = 2.28, batch loss = 2.23 (35.2 examples/sec; 0.227 sec/batch; 19h:33m:39s remains)
INFO - root - 2017-12-08 06:18:15.494637: step 22690, loss = 2.28, batch loss = 2.23 (35.8 examples/sec; 0.223 sec/batch; 19h:13m:55s remains)
INFO - root - 2017-12-08 06:18:17.728652: step 22700, loss = 2.28, batch loss = 2.23 (33.7 examples/sec; 0.237 sec/batch; 20h:24m:09s remains)
2017-12-08 06:18:18.014594: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289355 -4.4289136 -4.4288821 -4.4288459 -4.4288068 -4.4287782 -4.42876 -4.4287477 -4.4287343 -4.4287257 -4.4287386 -4.4287658 -4.4287934 -4.4288077 -4.4288058][-4.4289231 -4.4288964 -4.4288588 -4.4288111 -4.42876 -4.4287291 -4.4287133 -4.428699 -4.428679 -4.428669 -4.4286757 -4.4286942 -4.4287229 -4.4287381 -4.428731][-4.4289093 -4.4288774 -4.4288306 -4.428771 -4.4287062 -4.4286666 -4.4286447 -4.4286261 -4.4286118 -4.4286165 -4.4286366 -4.4286609 -4.4286923 -4.4287109 -4.4286928][-4.4288988 -4.4288578 -4.4287963 -4.4287233 -4.4286418 -4.4285817 -4.42853 -4.4285035 -4.4285178 -4.4285688 -4.4286222 -4.4286647 -4.4286981 -4.4287171 -4.4286909][-4.4288912 -4.42884 -4.4287667 -4.4286766 -4.4285736 -4.4284811 -4.4283872 -4.4283333 -4.4283781 -4.4285026 -4.428607 -4.4286685 -4.4287109 -4.4287295 -4.4287047][-4.4288874 -4.4288306 -4.4287457 -4.4286385 -4.4285131 -4.4283838 -4.4282403 -4.4281244 -4.4281678 -4.4283557 -4.4285197 -4.42861 -4.4286737 -4.4287009 -4.4286823][-4.4288917 -4.4288321 -4.4287381 -4.4286127 -4.42847 -4.4283233 -4.4281473 -4.4279437 -4.4279065 -4.4281292 -4.4283581 -4.4284811 -4.4285579 -4.4286 -4.4286036][-4.428895 -4.4288335 -4.4287372 -4.42861 -4.4284625 -4.4283228 -4.4281516 -4.4279079 -4.4277787 -4.42798 -4.4282303 -4.4283614 -4.4284377 -4.4284863 -4.42851][-4.4288964 -4.4288416 -4.4287548 -4.4286432 -4.4285164 -4.4284091 -4.4282885 -4.42811 -4.4280081 -4.4281054 -4.4282622 -4.4283495 -4.4284062 -4.4284453 -4.4284663][-4.4289012 -4.4288597 -4.4287925 -4.4287071 -4.4286127 -4.4285464 -4.4284859 -4.4283986 -4.4283471 -4.4283538 -4.4283957 -4.4284391 -4.4284811 -4.4285083 -4.4285111][-4.4289141 -4.4288859 -4.4288373 -4.4287772 -4.4287181 -4.4286819 -4.4286585 -4.4286232 -4.4285879 -4.4285545 -4.4285383 -4.4285636 -4.4286003 -4.4286175 -4.4286036][-4.4289351 -4.42892 -4.4288845 -4.428843 -4.4288082 -4.4287872 -4.4287763 -4.4287643 -4.4287429 -4.4287124 -4.4286876 -4.4286962 -4.4287205 -4.4287333 -4.4287157][-4.4289527 -4.4289441 -4.4289174 -4.4288888 -4.4288669 -4.4288549 -4.4288497 -4.4288511 -4.4288497 -4.4288368 -4.4288192 -4.4288135 -4.4288211 -4.42883 -4.4288168][-4.4289641 -4.42896 -4.4289417 -4.4289227 -4.4289069 -4.4288964 -4.428894 -4.4288993 -4.4289031 -4.4289021 -4.4288931 -4.4288845 -4.4288831 -4.42889 -4.42889][-4.4289722 -4.4289722 -4.4289608 -4.428947 -4.4289365 -4.4289293 -4.4289274 -4.4289303 -4.4289341 -4.4289365 -4.4289341 -4.4289284 -4.428926 -4.4289322 -4.4289389]]...]
INFO - root - 2017-12-08 06:18:20.273661: step 22710, loss = 2.28, batch loss = 2.23 (36.6 examples/sec; 0.219 sec/batch; 18h:48m:13s remains)
INFO - root - 2017-12-08 06:18:22.489450: step 22720, loss = 2.28, batch loss = 2.23 (34.9 examples/sec; 0.229 sec/batch; 19h:44m:40s remains)
INFO - root - 2017-12-08 06:18:24.756067: step 22730, loss = 2.28, batch loss = 2.23 (37.3 examples/sec; 0.215 sec/batch; 18h:28m:31s remains)
INFO - root - 2017-12-08 06:18:26.977652: step 22740, loss = 2.28, batch loss = 2.23 (36.0 examples/sec; 0.222 sec/batch; 19h:07m:47s remains)
INFO - root - 2017-12-08 06:18:29.237918: step 22750, loss = 2.28, batch loss = 2.23 (34.8 examples/sec; 0.230 sec/batch; 19h:47m:34s remains)
INFO - root - 2017-12-08 06:18:31.498904: step 22760, loss = 2.28, batch loss = 2.23 (35.6 examples/sec; 0.225 sec/batch; 19h:21m:04s remains)
INFO - root - 2017-12-08 06:18:33.723116: step 22770, loss = 2.28, batch loss = 2.23 (37.1 examples/sec; 0.216 sec/batch; 18h:34m:32s remains)
INFO - root - 2017-12-08 06:18:35.989956: step 22780, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 19h:01m:34s remains)
INFO - root - 2017-12-08 06:18:38.233372: step 22790, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.225 sec/batch; 19h:23m:34s remains)
INFO - root - 2017-12-08 06:18:40.486803: step 22800, loss = 2.28, batch loss = 2.23 (33.5 examples/sec; 0.239 sec/batch; 20h:33m:14s remains)
2017-12-08 06:18:40.777951: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.42886 -4.4288979 -4.4289274 -4.4289389 -4.428926 -4.4288926 -4.4288659 -4.4288588 -4.4288754 -4.4288893 -4.4289045 -4.4289074 -4.428896 -4.4288826 -4.4288759][-4.4288611 -4.4289021 -4.4289336 -4.4289432 -4.4289188 -4.4288688 -4.4288349 -4.4288321 -4.4288607 -4.4288874 -4.4289107 -4.4289188 -4.4289007 -4.4288778 -4.4288707][-4.4288864 -4.4289179 -4.4289346 -4.4289279 -4.4288807 -4.4288144 -4.4287734 -4.4287696 -4.428802 -4.4288435 -4.4288807 -4.4288979 -4.4288845 -4.4288664 -4.4288678][-4.4289165 -4.4289317 -4.4289231 -4.4288898 -4.4288168 -4.4287338 -4.4286828 -4.4286737 -4.4287043 -4.4287672 -4.4288278 -4.4288573 -4.4288568 -4.4288487 -4.4288597][-4.428925 -4.4289265 -4.4288888 -4.428822 -4.4287181 -4.4286113 -4.4285378 -4.4285178 -4.4285536 -4.4286451 -4.4287395 -4.4287925 -4.4288139 -4.4288263 -4.4288483][-4.4289069 -4.4288979 -4.4288297 -4.42873 -4.428596 -4.4284549 -4.4283576 -4.4283347 -4.42839 -4.4285088 -4.4286356 -4.4287167 -4.4287648 -4.4287968 -4.4288306][-4.4288712 -4.4288464 -4.4287558 -4.428637 -4.4284911 -4.4283309 -4.4282155 -4.4281888 -4.42826 -4.4283981 -4.4285474 -4.4286504 -4.4287109 -4.4287519 -4.4287939][-4.4288263 -4.4287882 -4.428689 -4.4285712 -4.4284396 -4.4282942 -4.428174 -4.428133 -4.4281964 -4.4283361 -4.4284925 -4.4286065 -4.4286747 -4.428721 -4.4287653][-4.4288025 -4.4287667 -4.428679 -4.4285851 -4.4284897 -4.4283767 -4.4282613 -4.4281993 -4.4282379 -4.4283543 -4.4284959 -4.4286079 -4.4286795 -4.42873 -4.4287667][-4.4288197 -4.4287953 -4.4287329 -4.4286704 -4.4286151 -4.4285407 -4.4284468 -4.42838 -4.4283929 -4.4284778 -4.428586 -4.4286747 -4.4287343 -4.4287729 -4.4287896][-4.4288568 -4.4288425 -4.4288058 -4.428771 -4.42875 -4.4287152 -4.4286556 -4.4285975 -4.42859 -4.4286423 -4.4287124 -4.4287624 -4.4287953 -4.4288116 -4.428803][-4.4288683 -4.428864 -4.4288492 -4.4288387 -4.4288406 -4.4288359 -4.4288068 -4.4287658 -4.4287457 -4.4287691 -4.4288006 -4.4288111 -4.4288206 -4.4288187 -4.4287972][-4.4288554 -4.4288588 -4.4288559 -4.4288592 -4.4288688 -4.4288735 -4.4288673 -4.4288497 -4.4288349 -4.4288445 -4.428843 -4.428823 -4.4288154 -4.4288092 -4.4287863][-4.4288535 -4.4288568 -4.4288521 -4.4288559 -4.4288578 -4.428863 -4.428874 -4.4288778 -4.4288754 -4.4288826 -4.4288654 -4.4288268 -4.4288111 -4.4288077 -4.4287896][-4.4288669 -4.4288592 -4.4288435 -4.4288383 -4.4288292 -4.4288359 -4.4288616 -4.4288869 -4.4289041 -4.4289112 -4.428885 -4.4288383 -4.4288192 -4.428813 -4.4287944]]...]
INFO - root - 2017-12-08 06:18:43.001854: step 22810, loss = 2.28, batch loss = 2.23 (36.7 examples/sec; 0.218 sec/batch; 18h:43m:41s remains)
INFO - root - 2017-12-08 06:18:45.268229: step 22820, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.225 sec/batch; 19h:21m:45s remains)
INFO - root - 2017-12-08 06:18:47.511500: step 22830, loss = 2.28, batch loss = 2.23 (32.8 examples/sec; 0.244 sec/batch; 20h:57m:27s remains)
INFO - root - 2017-12-08 06:18:49.769699: step 22840, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 18h:52m:04s remains)
INFO - root - 2017-12-08 06:18:52.001117: step 22850, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.225 sec/batch; 19h:23m:41s remains)
INFO - root - 2017-12-08 06:18:54.239718: step 22860, loss = 2.28, batch loss = 2.23 (34.8 examples/sec; 0.230 sec/batch; 19h:45m:40s remains)
INFO - root - 2017-12-08 06:18:56.464166: step 22870, loss = 2.28, batch loss = 2.23 (35.8 examples/sec; 0.224 sec/batch; 19h:13m:38s remains)
INFO - root - 2017-12-08 06:18:58.703157: step 22880, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 19h:11m:04s remains)
INFO - root - 2017-12-08 06:19:00.935171: step 22890, loss = 2.28, batch loss = 2.23 (35.2 examples/sec; 0.227 sec/batch; 19h:31m:51s remains)
INFO - root - 2017-12-08 06:19:03.188864: step 22900, loss = 2.28, batch loss = 2.23 (34.6 examples/sec; 0.231 sec/batch; 19h:54m:25s remains)
2017-12-08 06:19:03.487916: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288626 -4.4288716 -4.4288406 -4.4287915 -4.4287682 -4.4287734 -4.4287872 -4.4287891 -4.4288015 -4.4288421 -4.4289 -4.4289432 -4.4289374 -4.4289179 -4.4289193][-4.42879 -4.4288082 -4.4287767 -4.4287224 -4.4286966 -4.4287057 -4.4287229 -4.4287229 -4.4287419 -4.4287949 -4.4288568 -4.4288974 -4.4288826 -4.4288573 -4.4288516][-4.42877 -4.428802 -4.4287786 -4.428731 -4.4287052 -4.4287124 -4.4287233 -4.4287186 -4.4287386 -4.428791 -4.4288387 -4.4288616 -4.4288344 -4.4288011 -4.4287877][-4.428771 -4.4288249 -4.4288197 -4.4287882 -4.4287653 -4.4287615 -4.4287629 -4.4287639 -4.4287977 -4.4288478 -4.4288726 -4.4288712 -4.4288297 -4.4287872 -4.4287663][-4.42874 -4.4288177 -4.4288359 -4.4288116 -4.4287853 -4.428772 -4.428772 -4.4287944 -4.4288511 -4.4288993 -4.4289079 -4.4288893 -4.4288392 -4.4287839 -4.428741][-4.4286795 -4.4287677 -4.4288006 -4.4287734 -4.428731 -4.428699 -4.4286847 -4.4287319 -4.4288287 -4.4288883 -4.428895 -4.42888 -4.4288359 -4.42877 -4.4287014][-4.4286313 -4.4286947 -4.4287057 -4.4286528 -4.4285774 -4.4284997 -4.4284439 -4.4285188 -4.4286895 -4.4288011 -4.4288259 -4.4288254 -4.4287934 -4.4287281 -4.4286518][-4.4286633 -4.4286723 -4.4286304 -4.4285274 -4.4283853 -4.4282222 -4.4280863 -4.4281654 -4.4284167 -4.4285989 -4.4286704 -4.4287057 -4.4287076 -4.4286737 -4.4286246][-4.4287343 -4.428709 -4.428617 -4.4284658 -4.4282622 -4.4280114 -4.4277754 -4.4278207 -4.4281125 -4.4283481 -4.4284773 -4.4285645 -4.4286218 -4.4286385 -4.428628][-4.4287882 -4.4287591 -4.42866 -4.4285169 -4.4283319 -4.4280996 -4.4278622 -4.4278526 -4.4280729 -4.428287 -4.4284353 -4.4285426 -4.4286189 -4.4286623 -4.4286718][-4.4288363 -4.4288125 -4.4287291 -4.4286284 -4.4285131 -4.4283652 -4.4282045 -4.4281716 -4.428287 -4.4284248 -4.4285421 -4.4286265 -4.4286966 -4.4287443 -4.428761][-4.4288797 -4.4288568 -4.4287958 -4.4287357 -4.4286785 -4.4286103 -4.4285283 -4.4285016 -4.4285507 -4.4286218 -4.428689 -4.4287415 -4.4287972 -4.4288349 -4.4288487][-4.4289155 -4.4288969 -4.4288592 -4.4288263 -4.4288015 -4.4287777 -4.4287438 -4.4287276 -4.4287434 -4.4287686 -4.4287934 -4.428823 -4.4288621 -4.42889 -4.4289017][-4.4289532 -4.4289336 -4.4289093 -4.4288936 -4.4288855 -4.4288797 -4.4288669 -4.4288573 -4.4288545 -4.4288554 -4.4288626 -4.4288759 -4.4288988 -4.4289193 -4.4289322][-4.4289851 -4.428966 -4.4289479 -4.42894 -4.428936 -4.4289351 -4.4289317 -4.4289274 -4.4289207 -4.4289145 -4.4289131 -4.428916 -4.4289303 -4.428947 -4.4289613]]...]
INFO - root - 2017-12-08 06:19:05.767732: step 22910, loss = 2.28, batch loss = 2.23 (35.4 examples/sec; 0.226 sec/batch; 19h:24m:46s remains)
INFO - root - 2017-12-08 06:19:07.982695: step 22920, loss = 2.28, batch loss = 2.23 (36.0 examples/sec; 0.222 sec/batch; 19h:06m:18s remains)
INFO - root - 2017-12-08 06:19:10.221360: step 22930, loss = 2.28, batch loss = 2.23 (35.3 examples/sec; 0.226 sec/batch; 19h:27m:44s remains)
INFO - root - 2017-12-08 06:19:12.452492: step 22940, loss = 2.28, batch loss = 2.23 (35.2 examples/sec; 0.228 sec/batch; 19h:33m:51s remains)
INFO - root - 2017-12-08 06:19:14.683695: step 22950, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 19h:00m:13s remains)
INFO - root - 2017-12-08 06:19:16.946040: step 22960, loss = 2.28, batch loss = 2.23 (35.2 examples/sec; 0.227 sec/batch; 19h:33m:05s remains)
INFO - root - 2017-12-08 06:19:19.185054: step 22970, loss = 2.28, batch loss = 2.23 (34.6 examples/sec; 0.231 sec/batch; 19h:52m:33s remains)
INFO - root - 2017-12-08 06:19:21.428311: step 22980, loss = 2.28, batch loss = 2.23 (36.6 examples/sec; 0.218 sec/batch; 18h:46m:42s remains)
INFO - root - 2017-12-08 06:19:23.673580: step 22990, loss = 2.28, batch loss = 2.23 (36.0 examples/sec; 0.223 sec/batch; 19h:07m:49s remains)
INFO - root - 2017-12-08 06:19:25.896578: step 23000, loss = 2.28, batch loss = 2.23 (35.4 examples/sec; 0.226 sec/batch; 19h:26m:14s remains)
2017-12-08 06:19:26.202340: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4286904 -4.4287395 -4.4287877 -4.4288273 -4.428843 -4.4288588 -4.4288793 -4.42887 -4.4288373 -4.4287629 -4.4286661 -4.4285488 -4.4284525 -4.4284635 -4.4285865][-4.4287653 -4.4288054 -4.4288416 -4.4288735 -4.4288912 -4.4289155 -4.4289365 -4.4289222 -4.4288769 -4.4288 -4.4287143 -4.428606 -4.4285288 -4.4285517 -4.4286737][-4.4287896 -4.428833 -4.4288688 -4.4288964 -4.4289145 -4.4289308 -4.4289303 -4.4288936 -4.4288387 -4.4287839 -4.4287291 -4.4286509 -4.4286017 -4.4286356 -4.4287457][-4.4287744 -4.4288249 -4.4288645 -4.428894 -4.4289093 -4.4289002 -4.4288621 -4.428793 -4.4287333 -4.4287128 -4.4287066 -4.4286757 -4.428658 -4.4286985 -4.4287896][-4.4287181 -4.428782 -4.428844 -4.4288845 -4.4288874 -4.4288492 -4.4287791 -4.428688 -4.4286251 -4.42863 -4.4286618 -4.4286723 -4.4286804 -4.4287314 -4.428812][-4.4286628 -4.4287357 -4.4288149 -4.4288621 -4.4288516 -4.4287972 -4.4287086 -4.4286127 -4.428555 -4.4285693 -4.42862 -4.4286561 -4.4286847 -4.4287424 -4.42882][-4.4286556 -4.4287362 -4.42881 -4.4288416 -4.4288168 -4.4287543 -4.4286609 -4.42857 -4.4285188 -4.4285321 -4.4285831 -4.4286294 -4.4286704 -4.4287305 -4.4288073][-4.4286814 -4.4287562 -4.4288054 -4.4288054 -4.4287653 -4.4287047 -4.4286189 -4.4285417 -4.4285088 -4.428524 -4.4285693 -4.4286137 -4.4286609 -4.4287214 -4.4287944][-4.4287195 -4.4287691 -4.4287887 -4.42877 -4.4287252 -4.4286714 -4.4286027 -4.428544 -4.428525 -4.4285407 -4.4285789 -4.4286146 -4.4286623 -4.428721 -4.4287896][-4.4287524 -4.4287591 -4.4287562 -4.4287381 -4.4287024 -4.4286556 -4.4285994 -4.4285531 -4.4285393 -4.4285564 -4.4285932 -4.4286222 -4.4286666 -4.4287219 -4.4287896][-4.428761 -4.4287438 -4.4287305 -4.4287157 -4.4286971 -4.4286766 -4.4286551 -4.4286327 -4.4286165 -4.4286227 -4.4286461 -4.4286566 -4.4286838 -4.4287319 -4.4288][-4.4287386 -4.4287109 -4.4286966 -4.4286904 -4.4286952 -4.4287109 -4.4287338 -4.4287353 -4.4287167 -4.4287109 -4.4287162 -4.4287109 -4.4287181 -4.4287548 -4.4288168][-4.4287324 -4.4286995 -4.4286833 -4.428679 -4.4287033 -4.4287491 -4.4288015 -4.4288135 -4.428791 -4.4287767 -4.428772 -4.4287572 -4.4287553 -4.4287858 -4.4288387][-4.4287658 -4.4287348 -4.4287214 -4.4287233 -4.4287519 -4.4288025 -4.4288564 -4.4288626 -4.428834 -4.4288168 -4.4288034 -4.4287834 -4.4287815 -4.4288106 -4.4288554][-4.4288507 -4.4288282 -4.4288187 -4.428823 -4.42884 -4.4288707 -4.4288964 -4.4288807 -4.4288478 -4.4288249 -4.4288054 -4.4287891 -4.4287977 -4.4288287 -4.4288659]]...]
INFO - root - 2017-12-08 06:19:28.421135: step 23010, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 19h:00m:24s remains)
INFO - root - 2017-12-08 06:19:30.672880: step 23020, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.220 sec/batch; 18h:57m:18s remains)
INFO - root - 2017-12-08 06:19:32.914136: step 23030, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.219 sec/batch; 18h:52m:03s remains)
INFO - root - 2017-12-08 06:19:35.142748: step 23040, loss = 2.28, batch loss = 2.23 (35.2 examples/sec; 0.227 sec/batch; 19h:32m:51s remains)
INFO - root - 2017-12-08 06:19:37.373290: step 23050, loss = 2.28, batch loss = 2.23 (35.3 examples/sec; 0.226 sec/batch; 19h:28m:04s remains)
INFO - root - 2017-12-08 06:19:39.606521: step 23060, loss = 2.28, batch loss = 2.23 (36.9 examples/sec; 0.217 sec/batch; 18h:37m:01s remains)
INFO - root - 2017-12-08 06:19:41.812848: step 23070, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 18h:53m:53s remains)
INFO - root - 2017-12-08 06:19:44.063926: step 23080, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.222 sec/batch; 19h:02m:22s remains)
INFO - root - 2017-12-08 06:19:46.301528: step 23090, loss = 2.28, batch loss = 2.23 (35.4 examples/sec; 0.226 sec/batch; 19h:26m:00s remains)
INFO - root - 2017-12-08 06:19:48.575092: step 23100, loss = 2.28, batch loss = 2.23 (35.8 examples/sec; 0.223 sec/batch; 19h:11m:52s remains)
2017-12-08 06:19:48.865504: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4287 -4.4287248 -4.4287286 -4.428719 -4.4286952 -4.4286652 -4.428647 -4.428658 -4.4286957 -4.4287443 -4.4288058 -4.4288588 -4.4288917 -4.4289117 -4.4289227][-4.4286551 -4.4287024 -4.4287224 -4.4287176 -4.4286904 -4.4286509 -4.4286146 -4.4286089 -4.4286427 -4.428699 -4.4287829 -4.4288511 -4.4288874 -4.4289064 -4.4289188][-4.4286046 -4.4286604 -4.4286852 -4.4286823 -4.4286604 -4.4286265 -4.4285922 -4.4285822 -4.428616 -4.4286785 -4.4287696 -4.428834 -4.4288611 -4.4288754 -4.4288883][-4.4285426 -4.42859 -4.4286127 -4.42862 -4.4286189 -4.4286084 -4.4285884 -4.4285727 -4.4285927 -4.428658 -4.4287395 -4.4287844 -4.4287944 -4.4288034 -4.4288182][-4.4285078 -4.428534 -4.4285564 -4.4285765 -4.4285994 -4.4285984 -4.4285684 -4.4285207 -4.4285159 -4.42859 -4.4286757 -4.4287124 -4.4287205 -4.42873 -4.4287481][-4.4285145 -4.4285254 -4.4285421 -4.4285645 -4.4285908 -4.4285736 -4.4284916 -4.4283686 -4.4283152 -4.4284129 -4.4285398 -4.4286094 -4.4286427 -4.4286647 -4.4286871][-4.4285502 -4.4285474 -4.4285474 -4.4285617 -4.4285769 -4.4285135 -4.4283533 -4.4281211 -4.4280066 -4.4281673 -4.4283814 -4.4285126 -4.4285827 -4.4286237 -4.4286575][-4.428587 -4.4285741 -4.4285579 -4.4285545 -4.4285522 -4.4284635 -4.4282689 -4.4280019 -4.4278708 -4.4280915 -4.4283667 -4.4285312 -4.4286127 -4.4286556 -4.4286876][-4.428596 -4.4285769 -4.4285493 -4.4285474 -4.4285569 -4.4284911 -4.4283414 -4.4281631 -4.4280858 -4.42827 -4.4284987 -4.4286289 -4.4286823 -4.4287038 -4.4287181][-4.428576 -4.4285593 -4.4285369 -4.4285502 -4.4285803 -4.4285564 -4.4284754 -4.4283729 -4.4283228 -4.4284434 -4.4286108 -4.4287062 -4.4287343 -4.4287343 -4.4287324][-4.4285836 -4.428565 -4.4285383 -4.4285541 -4.4285951 -4.4286075 -4.4285674 -4.4284968 -4.4284515 -4.4285293 -4.4286594 -4.4287391 -4.428751 -4.4287357 -4.4287243][-4.4286275 -4.4285946 -4.4285464 -4.4285431 -4.4285793 -4.4286165 -4.4286075 -4.4285455 -4.4284911 -4.428535 -4.4286494 -4.428731 -4.4287462 -4.4287338 -4.4287243][-4.4286723 -4.4286332 -4.4285631 -4.4285316 -4.4285588 -4.4286075 -4.4286108 -4.4285617 -4.4285111 -4.4285235 -4.4286189 -4.4287038 -4.4287319 -4.4287262 -4.428719][-4.4287148 -4.4286838 -4.4286118 -4.4285588 -4.4285674 -4.4286079 -4.4286032 -4.4285731 -4.428546 -4.4285417 -4.4286051 -4.4286776 -4.4287086 -4.4287138 -4.4287148][-4.4287343 -4.4287124 -4.4286542 -4.4286036 -4.4286084 -4.4286408 -4.42863 -4.428616 -4.4286022 -4.428575 -4.428597 -4.4286518 -4.4286962 -4.4287205 -4.4287238]]...]
INFO - root - 2017-12-08 06:19:51.095200: step 23110, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 19h:08m:27s remains)
INFO - root - 2017-12-08 06:19:53.373845: step 23120, loss = 2.28, batch loss = 2.23 (33.2 examples/sec; 0.241 sec/batch; 20h:44m:11s remains)
INFO - root - 2017-12-08 06:19:55.645720: step 23130, loss = 2.28, batch loss = 2.23 (35.1 examples/sec; 0.228 sec/batch; 19h:33m:44s remains)
INFO - root - 2017-12-08 06:19:57.931576: step 23140, loss = 2.28, batch loss = 2.23 (34.6 examples/sec; 0.231 sec/batch; 19h:52m:02s remains)
INFO - root - 2017-12-08 06:20:00.213093: step 23150, loss = 2.28, batch loss = 2.23 (32.2 examples/sec; 0.249 sec/batch; 21h:22m:24s remains)
INFO - root - 2017-12-08 06:20:02.462496: step 23160, loss = 2.28, batch loss = 2.23 (35.7 examples/sec; 0.224 sec/batch; 19h:16m:11s remains)
INFO - root - 2017-12-08 06:20:04.697551: step 23170, loss = 2.28, batch loss = 2.23 (35.1 examples/sec; 0.228 sec/batch; 19h:33m:48s remains)
INFO - root - 2017-12-08 06:20:06.927724: step 23180, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.225 sec/batch; 19h:22m:11s remains)
INFO - root - 2017-12-08 06:20:09.148859: step 23190, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 19h:08m:57s remains)
INFO - root - 2017-12-08 06:20:11.398655: step 23200, loss = 2.28, batch loss = 2.23 (37.2 examples/sec; 0.215 sec/batch; 18h:29m:31s remains)
2017-12-08 06:20:11.675118: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.42899 -4.4289865 -4.4289804 -4.4289765 -4.428978 -4.4289846 -4.4289927 -4.428997 -4.428998 -4.428998 -4.4289951 -4.4289885 -4.4289804 -4.4289742 -4.4289732][-4.4289908 -4.4289894 -4.4289861 -4.4289846 -4.4289846 -4.4289856 -4.4289865 -4.4289875 -4.42899 -4.4289951 -4.428998 -4.4289937 -4.4289856 -4.428978 -4.4289765][-4.4289856 -4.4289856 -4.4289846 -4.4289827 -4.428978 -4.4289689 -4.4289589 -4.4289565 -4.4289627 -4.4289775 -4.4289932 -4.428998 -4.4289927 -4.4289827 -4.428978][-4.4289775 -4.428978 -4.4289765 -4.4289694 -4.42895 -4.4289203 -4.4288912 -4.4288812 -4.4288945 -4.4289284 -4.4289665 -4.4289894 -4.4289918 -4.4289832 -4.4289756][-4.4289632 -4.4289637 -4.4289579 -4.428937 -4.4288921 -4.4288263 -4.4287629 -4.4287386 -4.4287667 -4.42883 -4.4289026 -4.4289541 -4.4289746 -4.4289732 -4.428966][-4.4289455 -4.428946 -4.4289317 -4.4288888 -4.4288034 -4.4286771 -4.4285545 -4.4285078 -4.4285617 -4.428678 -4.4288044 -4.4288964 -4.4289446 -4.4289556 -4.4289532][-4.42893 -4.4289312 -4.4289064 -4.4288416 -4.4287095 -4.4285088 -4.42831 -4.428237 -4.4283285 -4.4285049 -4.4286914 -4.4288297 -4.4289112 -4.4289412 -4.428947][-4.428926 -4.4289269 -4.4288974 -4.4288187 -4.4286551 -4.428401 -4.4281459 -4.428062 -4.4281936 -4.4284124 -4.4286342 -4.4288 -4.4289041 -4.4289479 -4.428957][-4.4289432 -4.4289436 -4.4289169 -4.4288411 -4.4286819 -4.4284377 -4.4281993 -4.4281263 -4.4282513 -4.42845 -4.4286551 -4.4288125 -4.4289188 -4.428966 -4.4289732][-4.4289665 -4.4289713 -4.4289546 -4.4288988 -4.42878 -4.4286022 -4.4284325 -4.4283762 -4.428453 -4.4285855 -4.4287362 -4.4288592 -4.428946 -4.4289842 -4.4289865][-4.42898 -4.4289918 -4.42899 -4.4289608 -4.42889 -4.4287848 -4.4286833 -4.4286456 -4.4286823 -4.4287553 -4.4288473 -4.4289236 -4.4289784 -4.4290004 -4.4289956][-4.4289846 -4.4290028 -4.4290123 -4.4290066 -4.4289746 -4.4289227 -4.4288692 -4.4288449 -4.4288554 -4.4288883 -4.4289341 -4.4289727 -4.4290037 -4.4290128 -4.4290009][-4.4289813 -4.4290037 -4.4290209 -4.429029 -4.429019 -4.4289951 -4.4289641 -4.4289422 -4.4289346 -4.4289455 -4.4289675 -4.4289889 -4.429008 -4.4290128 -4.4290004][-4.4289732 -4.4289937 -4.4290142 -4.4290304 -4.4290295 -4.4290128 -4.4289851 -4.428956 -4.428937 -4.4289384 -4.4289556 -4.4289765 -4.4289956 -4.4290051 -4.429][-4.428957 -4.4289737 -4.4289951 -4.4290142 -4.4290152 -4.4289966 -4.4289632 -4.4289269 -4.428905 -4.4289093 -4.4289331 -4.4289627 -4.428987 -4.4290028 -4.4290037]]...]
INFO - root - 2017-12-08 06:20:13.890526: step 23210, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.220 sec/batch; 18h:55m:39s remains)
INFO - root - 2017-12-08 06:20:16.115247: step 23220, loss = 2.28, batch loss = 2.23 (35.1 examples/sec; 0.228 sec/batch; 19h:36m:18s remains)
INFO - root - 2017-12-08 06:20:18.343917: step 23230, loss = 2.28, batch loss = 2.23 (35.1 examples/sec; 0.228 sec/batch; 19h:36m:24s remains)
INFO - root - 2017-12-08 06:20:20.582549: step 23240, loss = 2.28, batch loss = 2.23 (35.7 examples/sec; 0.224 sec/batch; 19h:15m:07s remains)
INFO - root - 2017-12-08 06:20:22.828720: step 23250, loss = 2.28, batch loss = 2.23 (36.8 examples/sec; 0.218 sec/batch; 18h:41m:32s remains)
INFO - root - 2017-12-08 06:20:25.072798: step 23260, loss = 2.28, batch loss = 2.23 (36.0 examples/sec; 0.222 sec/batch; 19h:04m:01s remains)
INFO - root - 2017-12-08 06:20:27.291159: step 23270, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.222 sec/batch; 19h:03m:41s remains)
INFO - root - 2017-12-08 06:20:29.513547: step 23280, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 18h:49m:12s remains)
INFO - root - 2017-12-08 06:20:31.811751: step 23290, loss = 2.28, batch loss = 2.23 (33.8 examples/sec; 0.237 sec/batch; 20h:21m:32s remains)
INFO - root - 2017-12-08 06:20:34.044741: step 23300, loss = 2.28, batch loss = 2.23 (35.0 examples/sec; 0.229 sec/batch; 19h:38m:04s remains)
2017-12-08 06:20:34.336377: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288769 -4.428884 -4.4288988 -4.428906 -4.4289031 -4.4288607 -4.4288368 -4.4288692 -4.4289155 -4.42893 -4.4289317 -4.4289069 -4.4288559 -4.428844 -4.4288774][-4.4288239 -4.4288068 -4.4288173 -4.4288354 -4.4288492 -4.4288263 -4.4288177 -4.4288626 -4.4289107 -4.4289217 -4.428915 -4.4288788 -4.428822 -4.428813 -4.4288592][-4.4288087 -4.4287758 -4.4287877 -4.4288135 -4.428834 -4.4288206 -4.4288116 -4.4288507 -4.4288969 -4.4289103 -4.4288983 -4.4288549 -4.4287968 -4.42879 -4.4288435][-4.4288354 -4.4288068 -4.4288297 -4.4288578 -4.4288697 -4.4288507 -4.4288268 -4.4288449 -4.4288778 -4.4288907 -4.4288797 -4.4288378 -4.4287863 -4.4287839 -4.4288354][-4.4288783 -4.4288597 -4.4288831 -4.4288898 -4.4288735 -4.4288259 -4.4287739 -4.4287677 -4.42881 -4.4288483 -4.4288559 -4.42883 -4.4287906 -4.4287925 -4.4288368][-4.4289136 -4.4289045 -4.4289055 -4.4288697 -4.42881 -4.428721 -4.4286275 -4.4286017 -4.4286757 -4.4287758 -4.4288239 -4.4288192 -4.4287853 -4.4287868 -4.4288249][-4.4289188 -4.4289045 -4.4288716 -4.4287891 -4.4286761 -4.428524 -4.4283681 -4.4283252 -4.4284534 -4.4286361 -4.428741 -4.42876 -4.4287305 -4.4287386 -4.4287858][-4.4288926 -4.4288688 -4.428812 -4.4287004 -4.4285512 -4.4283462 -4.4281316 -4.42808 -4.4282508 -4.4284835 -4.4286275 -4.4286652 -4.4286504 -4.4286838 -4.4287529][-4.4288812 -4.4288526 -4.4287915 -4.4286904 -4.4285645 -4.4283962 -4.4282279 -4.4282041 -4.4283428 -4.4285269 -4.4286423 -4.4286728 -4.4286642 -4.428709 -4.4287806][-4.428894 -4.4288788 -4.4288378 -4.4287715 -4.4286933 -4.428596 -4.4285054 -4.4284992 -4.4285774 -4.4286814 -4.4287424 -4.4287496 -4.4287343 -4.4287744 -4.428833][-4.4289169 -4.4289131 -4.4288888 -4.4288492 -4.4288063 -4.4287539 -4.4287119 -4.4287238 -4.4287696 -4.428823 -4.4288492 -4.42884 -4.4288168 -4.4288449 -4.4288878][-4.4289303 -4.4289293 -4.4289188 -4.4289045 -4.4288864 -4.4288607 -4.428843 -4.4288611 -4.4288869 -4.4289112 -4.4289141 -4.4288912 -4.42886 -4.4288754 -4.4289045][-4.4289236 -4.4289222 -4.428916 -4.4289079 -4.4288955 -4.42888 -4.4288716 -4.4288859 -4.4289041 -4.4289222 -4.4289155 -4.4288859 -4.4288521 -4.428865 -4.4288907][-4.4289179 -4.4289126 -4.4289079 -4.4289026 -4.4288931 -4.4288788 -4.4288688 -4.4288759 -4.4288874 -4.4288983 -4.4288878 -4.4288535 -4.4288187 -4.4288378 -4.4288707][-4.4289522 -4.4289389 -4.4289241 -4.4289074 -4.4288893 -4.4288678 -4.4288516 -4.4288483 -4.428854 -4.4288645 -4.4288597 -4.4288278 -4.4287953 -4.4288249 -4.4288669]]...]
INFO - root - 2017-12-08 06:20:36.594640: step 23310, loss = 2.28, batch loss = 2.23 (34.8 examples/sec; 0.230 sec/batch; 19h:43m:57s remains)
INFO - root - 2017-12-08 06:20:38.881977: step 23320, loss = 2.28, batch loss = 2.23 (35.4 examples/sec; 0.226 sec/batch; 19h:23m:23s remains)
INFO - root - 2017-12-08 06:20:41.145317: step 23330, loss = 2.28, batch loss = 2.23 (34.7 examples/sec; 0.231 sec/batch; 19h:49m:21s remains)
INFO - root - 2017-12-08 06:20:43.406156: step 23340, loss = 2.28, batch loss = 2.23 (33.2 examples/sec; 0.241 sec/batch; 20h:43m:03s remains)
INFO - root - 2017-12-08 06:20:45.641434: step 23350, loss = 2.28, batch loss = 2.23 (36.6 examples/sec; 0.219 sec/batch; 18h:46m:22s remains)
INFO - root - 2017-12-08 06:20:47.866772: step 23360, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 18h:51m:50s remains)
INFO - root - 2017-12-08 06:20:50.151780: step 23370, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.225 sec/batch; 19h:21m:27s remains)
INFO - root - 2017-12-08 06:20:52.433153: step 23380, loss = 2.28, batch loss = 2.23 (33.4 examples/sec; 0.240 sec/batch; 20h:35m:40s remains)
INFO - root - 2017-12-08 06:20:54.684826: step 23390, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 18h:58m:53s remains)
INFO - root - 2017-12-08 06:20:56.925302: step 23400, loss = 2.28, batch loss = 2.23 (35.7 examples/sec; 0.224 sec/batch; 19h:13m:12s remains)
2017-12-08 06:20:57.225471: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.428966 -4.4289756 -4.4289684 -4.428926 -4.4288421 -4.428731 -4.4286451 -4.4286313 -4.4286709 -4.4287167 -4.4287534 -4.4288063 -4.428863 -4.4289083 -4.4289341][-4.4289579 -4.4289665 -4.4289551 -4.428906 -4.4288139 -4.4286962 -4.4286132 -4.4286137 -4.4286652 -4.4287148 -4.4287462 -4.4288044 -4.4288821 -4.4289412 -4.4289651][-4.4289374 -4.4289393 -4.4289236 -4.428865 -4.4287643 -4.4286337 -4.4285483 -4.4285636 -4.4286318 -4.42869 -4.4287229 -4.4287858 -4.4288754 -4.4289417 -4.4289713][-4.4288974 -4.428896 -4.428885 -4.4288254 -4.4287143 -4.4285707 -4.428483 -4.4285049 -4.4285841 -4.4286509 -4.4286952 -4.4287605 -4.4288545 -4.4289308 -4.4289689][-4.4288526 -4.4288507 -4.4288449 -4.4287863 -4.4286704 -4.4285192 -4.4284277 -4.4284425 -4.4285135 -4.4285793 -4.4286447 -4.4287167 -4.4288177 -4.4289093 -4.4289603][-4.4287996 -4.4287987 -4.42879 -4.4287305 -4.4286213 -4.4284778 -4.4283843 -4.4283948 -4.4284463 -4.4285073 -4.4285946 -4.4286761 -4.4287806 -4.428884 -4.4289451][-4.428751 -4.4287472 -4.4287391 -4.428689 -4.4285975 -4.4284787 -4.4284015 -4.4284039 -4.4284191 -4.4284635 -4.42856 -4.4286528 -4.4287553 -4.4288611 -4.42893][-4.4287286 -4.428721 -4.4287062 -4.4286675 -4.4286027 -4.4285226 -4.4284635 -4.4284167 -4.4283748 -4.4283953 -4.4284964 -4.428607 -4.4287124 -4.4288216 -4.4289012][-4.428751 -4.4287353 -4.4287086 -4.4286776 -4.4286408 -4.4285893 -4.4285321 -4.4284511 -4.4283824 -4.4283834 -4.4284844 -4.4285989 -4.4286976 -4.4287982 -4.4288845][-4.4288125 -4.4287987 -4.42877 -4.4287333 -4.4287028 -4.4286675 -4.4286127 -4.4285331 -4.4284682 -4.4284568 -4.428534 -4.4286418 -4.4287348 -4.4288239 -4.4289021][-4.4288888 -4.4288821 -4.4288578 -4.4288216 -4.4287949 -4.4287677 -4.4287248 -4.4286628 -4.4286036 -4.42858 -4.4286227 -4.4287124 -4.4287982 -4.4288726 -4.4289322][-4.4289541 -4.428956 -4.4289408 -4.4289165 -4.4288974 -4.4288754 -4.4288435 -4.4287972 -4.4287543 -4.4287319 -4.4287543 -4.4288168 -4.4288759 -4.428926 -4.4289632][-4.4289932 -4.4290004 -4.4289942 -4.4289808 -4.42897 -4.4289532 -4.4289317 -4.4289041 -4.4288826 -4.4288712 -4.4288816 -4.4289179 -4.4289508 -4.4289761 -4.4289932][-4.4290094 -4.4290166 -4.4290128 -4.4290075 -4.4290032 -4.4289937 -4.4289837 -4.4289732 -4.4289656 -4.4289618 -4.428967 -4.4289842 -4.4289985 -4.4290094 -4.4290152][-4.4290042 -4.4290094 -4.4290071 -4.4290037 -4.4290013 -4.428998 -4.428997 -4.4289961 -4.4289947 -4.4289927 -4.4289932 -4.429 -4.4290056 -4.4290118 -4.4290161]]...]
INFO - root - 2017-12-08 06:20:59.460663: step 23410, loss = 2.28, batch loss = 2.23 (37.3 examples/sec; 0.214 sec/batch; 18h:24m:16s remains)
INFO - root - 2017-12-08 06:21:01.717330: step 23420, loss = 2.28, batch loss = 2.23 (35.4 examples/sec; 0.226 sec/batch; 19h:24m:24s remains)
INFO - root - 2017-12-08 06:21:03.960352: step 23430, loss = 2.28, batch loss = 2.23 (34.6 examples/sec; 0.231 sec/batch; 19h:51m:06s remains)
INFO - root - 2017-12-08 06:21:06.192486: step 23440, loss = 2.28, batch loss = 2.23 (33.5 examples/sec; 0.239 sec/batch; 20h:29m:37s remains)
INFO - root - 2017-12-08 06:21:08.499997: step 23450, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.226 sec/batch; 19h:21m:52s remains)
INFO - root - 2017-12-08 06:21:10.734177: step 23460, loss = 2.28, batch loss = 2.23 (34.6 examples/sec; 0.231 sec/batch; 19h:50m:00s remains)
INFO - root - 2017-12-08 06:21:12.961055: step 23470, loss = 2.28, batch loss = 2.23 (35.7 examples/sec; 0.224 sec/batch; 19h:12m:50s remains)
INFO - root - 2017-12-08 06:21:15.201463: step 23480, loss = 2.28, batch loss = 2.23 (35.3 examples/sec; 0.227 sec/batch; 19h:26m:40s remains)
INFO - root - 2017-12-08 06:21:17.441446: step 23490, loss = 2.28, batch loss = 2.23 (35.2 examples/sec; 0.227 sec/batch; 19h:31m:12s remains)
INFO - root - 2017-12-08 06:21:19.690312: step 23500, loss = 2.28, batch loss = 2.23 (36.6 examples/sec; 0.219 sec/batch; 18h:45m:44s remains)
2017-12-08 06:21:19.999096: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4285655 -4.4286108 -4.4286714 -4.4287086 -4.4287047 -4.4286704 -4.4286327 -4.4286094 -4.4286122 -4.42862 -4.4285965 -4.4285488 -4.4284987 -4.4284887 -4.4285264][-4.4285536 -4.4285946 -4.4286623 -4.4287105 -4.4287238 -4.4287066 -4.42868 -4.4286547 -4.428647 -4.4286427 -4.4286141 -4.4285641 -4.4285073 -4.4284844 -4.4285011][-4.4286118 -4.4286375 -4.42869 -4.4287314 -4.4287496 -4.4287448 -4.4287233 -4.4287057 -4.4287019 -4.4286942 -4.4286642 -4.42862 -4.4285626 -4.42852 -4.4285054][-4.4286404 -4.4286523 -4.4286838 -4.4287119 -4.4287324 -4.4287386 -4.4287281 -4.4287238 -4.428731 -4.4287286 -4.4287076 -4.4286761 -4.4286294 -4.4285774 -4.4285417][-4.4286232 -4.4286275 -4.4286494 -4.4286704 -4.4286871 -4.4286885 -4.4286733 -4.4286737 -4.4286919 -4.4287043 -4.4287019 -4.4286928 -4.428678 -4.4286528 -4.4286208][-4.4285526 -4.4285531 -4.4285679 -4.4285889 -4.4285984 -4.4285841 -4.4285493 -4.4285431 -4.4285831 -4.4286261 -4.4286513 -4.4286733 -4.4286914 -4.4286876 -4.4286618][-4.428412 -4.42841 -4.42843 -4.4284654 -4.4284763 -4.42845 -4.4283805 -4.4283347 -4.4283824 -4.4284706 -4.4285378 -4.4285946 -4.4286427 -4.4286551 -4.4286389][-4.4282818 -4.4282651 -4.4282823 -4.4283171 -4.4283237 -4.42829 -4.4281812 -4.4280672 -4.428102 -4.4282517 -4.4283891 -4.4284973 -4.4285755 -4.428607 -4.4286051][-4.4283075 -4.428278 -4.4282756 -4.428287 -4.4282794 -4.4282451 -4.4281282 -4.4279776 -4.4279857 -4.4281564 -4.4283323 -4.4284635 -4.4285541 -4.4286013 -4.428616][-4.428493 -4.4284668 -4.4284477 -4.4284396 -4.4284306 -4.4284115 -4.4283347 -4.4282188 -4.4281893 -4.4282842 -4.4284129 -4.4285221 -4.4286027 -4.4286513 -4.4286671][-4.4286823 -4.4286671 -4.4286442 -4.4286261 -4.4286175 -4.4286094 -4.428565 -4.4284883 -4.4284396 -4.428463 -4.4285331 -4.4286051 -4.4286613 -4.4286952 -4.4287019][-4.4287777 -4.4287791 -4.4287667 -4.4287543 -4.4287562 -4.4287605 -4.4287415 -4.4286933 -4.4286418 -4.4286222 -4.4286413 -4.4286737 -4.4287 -4.4287124 -4.4287076][-4.428782 -4.4287858 -4.4287796 -4.4287744 -4.4287844 -4.4287968 -4.4287891 -4.4287586 -4.4287114 -4.4286776 -4.4286723 -4.4286838 -4.4286952 -4.4286947 -4.4286804][-4.4287424 -4.4287457 -4.4287415 -4.4287362 -4.4287467 -4.4287605 -4.42876 -4.4287386 -4.4286976 -4.42866 -4.4286442 -4.4286475 -4.4286532 -4.4286466 -4.4286294][-4.4287038 -4.4287052 -4.4287028 -4.428699 -4.4287076 -4.4287229 -4.4287281 -4.4287152 -4.4286838 -4.42865 -4.4286351 -4.42864 -4.4286489 -4.4286461 -4.4286313]]...]
INFO - root - 2017-12-08 06:21:22.229700: step 23510, loss = 2.28, batch loss = 2.23 (37.1 examples/sec; 0.216 sec/batch; 18h:30m:18s remains)
INFO - root - 2017-12-08 06:21:24.485981: step 23520, loss = 2.28, batch loss = 2.23 (36.6 examples/sec; 0.218 sec/batch; 18h:44m:09s remains)
INFO - root - 2017-12-08 06:21:26.732765: step 23530, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 18h:48m:39s remains)
INFO - root - 2017-12-08 06:21:28.963277: step 23540, loss = 2.28, batch loss = 2.23 (35.7 examples/sec; 0.224 sec/batch; 19h:15m:15s remains)
INFO - root - 2017-12-08 06:21:31.198372: step 23550, loss = 2.28, batch loss = 2.23 (34.3 examples/sec; 0.233 sec/batch; 19h:59m:55s remains)
INFO - root - 2017-12-08 06:21:33.428759: step 23560, loss = 2.28, batch loss = 2.23 (35.2 examples/sec; 0.227 sec/batch; 19h:30m:51s remains)
INFO - root - 2017-12-08 06:21:35.649215: step 23570, loss = 2.28, batch loss = 2.23 (37.0 examples/sec; 0.216 sec/batch; 18h:33m:56s remains)
INFO - root - 2017-12-08 06:21:37.891548: step 23580, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.222 sec/batch; 19h:01m:02s remains)
INFO - root - 2017-12-08 06:21:40.131894: step 23590, loss = 2.28, batch loss = 2.23 (36.7 examples/sec; 0.218 sec/batch; 18h:42m:39s remains)
INFO - root - 2017-12-08 06:21:42.378952: step 23600, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.222 sec/batch; 19h:01m:16s remains)
2017-12-08 06:21:42.677729: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.428793 -4.4287963 -4.4288015 -4.4288106 -4.4288235 -4.4288244 -4.4288158 -4.4287992 -4.4287753 -4.428751 -4.4287386 -4.42873 -4.4287295 -4.42874 -4.4287434][-4.4288025 -4.4288011 -4.4288011 -4.42881 -4.4288235 -4.4288259 -4.4288144 -4.42879 -4.4287591 -4.4287357 -4.4287291 -4.4287305 -4.4287419 -4.428762 -4.4287629][-4.4288135 -4.4288015 -4.428792 -4.4287972 -4.4288106 -4.4288125 -4.4287968 -4.4287677 -4.4287367 -4.42872 -4.4287262 -4.4287424 -4.4287663 -4.4287882 -4.4287853][-4.42882 -4.4287992 -4.4287777 -4.428771 -4.4287734 -4.4287672 -4.4287462 -4.4287176 -4.4286938 -4.4286914 -4.4287114 -4.428741 -4.4287729 -4.4287949 -4.4287906][-4.4288168 -4.4287863 -4.4287524 -4.4287281 -4.4287138 -4.4286923 -4.4286594 -4.4286256 -4.4286127 -4.4286294 -4.428669 -4.4287148 -4.428761 -4.4287906 -4.4287906][-4.4288077 -4.4287672 -4.4287205 -4.4286742 -4.4286332 -4.4285855 -4.4285226 -4.428473 -4.4284787 -4.4285297 -4.4286 -4.42867 -4.4287391 -4.4287848 -4.4287934][-4.4287996 -4.4287515 -4.4286919 -4.4286246 -4.4285555 -4.428472 -4.4283686 -4.4282956 -4.4283247 -4.4284234 -4.4285312 -4.42863 -4.4287214 -4.428782 -4.4287977][-4.4287877 -4.4287438 -4.4286847 -4.4286108 -4.4285288 -4.4284263 -4.4283047 -4.4282217 -4.428266 -4.4283948 -4.4285226 -4.4286323 -4.4287276 -4.4287891 -4.4288044][-4.4287982 -4.4287629 -4.428709 -4.428638 -4.4285636 -4.4284792 -4.4283881 -4.4283266 -4.4283614 -4.428472 -4.4285846 -4.4286814 -4.4287653 -4.4288182 -4.4288254][-4.4288192 -4.4287925 -4.4287491 -4.4286852 -4.4286246 -4.4285688 -4.4285178 -4.428484 -4.4285045 -4.4285822 -4.428668 -4.4287443 -4.4288125 -4.4288535 -4.428853][-4.428843 -4.4288225 -4.428791 -4.4287376 -4.428689 -4.4286518 -4.4286237 -4.428606 -4.4286208 -4.4286757 -4.4287376 -4.4287939 -4.4288459 -4.4288783 -4.4288735][-4.4288464 -4.4288287 -4.4288063 -4.4287667 -4.42873 -4.4287038 -4.4286847 -4.4286723 -4.4286852 -4.4287257 -4.428772 -4.4288144 -4.4288535 -4.42888 -4.4288764][-4.4288378 -4.4288216 -4.4288039 -4.4287748 -4.4287472 -4.4287276 -4.4287114 -4.4287047 -4.4287181 -4.4287486 -4.4287848 -4.4288192 -4.42885 -4.4288721 -4.4288716][-4.4288239 -4.4288087 -4.428793 -4.4287696 -4.4287491 -4.4287319 -4.4287186 -4.4287162 -4.4287286 -4.4287524 -4.4287825 -4.4288125 -4.4288383 -4.4288578 -4.4288616][-4.4288082 -4.4287939 -4.4287815 -4.4287653 -4.4287505 -4.4287386 -4.4287305 -4.4287324 -4.4287438 -4.4287615 -4.4287848 -4.4288082 -4.4288306 -4.4288483 -4.428854]]...]
INFO - root - 2017-12-08 06:21:44.904602: step 23610, loss = 2.28, batch loss = 2.23 (35.4 examples/sec; 0.226 sec/batch; 19h:24m:30s remains)
INFO - root - 2017-12-08 06:21:47.154492: step 23620, loss = 2.28, batch loss = 2.23 (36.0 examples/sec; 0.222 sec/batch; 19h:04m:17s remains)
INFO - root - 2017-12-08 06:21:49.364814: step 23630, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.220 sec/batch; 18h:53m:28s remains)
INFO - root - 2017-12-08 06:21:51.591461: step 23640, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.225 sec/batch; 19h:18m:41s remains)
INFO - root - 2017-12-08 06:21:53.824184: step 23650, loss = 2.28, batch loss = 2.23 (37.3 examples/sec; 0.214 sec/batch; 18h:23m:55s remains)
INFO - root - 2017-12-08 06:21:56.057604: step 23660, loss = 2.28, batch loss = 2.23 (34.8 examples/sec; 0.230 sec/batch; 19h:42m:25s remains)
INFO - root - 2017-12-08 06:21:58.292309: step 23670, loss = 2.28, batch loss = 2.23 (35.7 examples/sec; 0.224 sec/batch; 19h:12m:09s remains)
INFO - root - 2017-12-08 06:22:00.495455: step 23680, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.226 sec/batch; 19h:21m:24s remains)
INFO - root - 2017-12-08 06:22:02.727019: step 23690, loss = 2.28, batch loss = 2.23 (36.6 examples/sec; 0.219 sec/batch; 18h:45m:30s remains)
INFO - root - 2017-12-08 06:22:04.940603: step 23700, loss = 2.28, batch loss = 2.23 (35.8 examples/sec; 0.223 sec/batch; 19h:08m:45s remains)
2017-12-08 06:22:05.207553: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4286318 -4.4286032 -4.4285455 -4.4284859 -4.4284911 -4.4285564 -4.4286447 -4.4287276 -4.4287729 -4.4287734 -4.4287362 -4.4286757 -4.4286327 -4.4286208 -4.4286537][-4.428679 -4.4286518 -4.4285865 -4.4285207 -4.4285145 -4.4285579 -4.4286213 -4.4286857 -4.4287214 -4.4287219 -4.4287033 -4.4286809 -4.4286752 -4.4286857 -4.42872][-4.4287343 -4.42871 -4.4286389 -4.4285645 -4.4285378 -4.4285531 -4.4285917 -4.428638 -4.4286661 -4.42867 -4.42867 -4.4286771 -4.4286942 -4.428721 -4.4287643][-4.428791 -4.4287682 -4.4286971 -4.4286141 -4.4285665 -4.4285545 -4.428566 -4.428587 -4.4286008 -4.428606 -4.4286194 -4.4286437 -4.4286737 -4.4287043 -4.4287467][-4.4288297 -4.4287972 -4.4287133 -4.4286127 -4.428546 -4.4285164 -4.4285073 -4.4285073 -4.4285183 -4.4285388 -4.4285722 -4.4286184 -4.4286618 -4.4286957 -4.4287348][-4.4288168 -4.4287634 -4.4286575 -4.4285355 -4.428452 -4.428411 -4.4283967 -4.4283986 -4.4284234 -4.428473 -4.428545 -4.4286222 -4.4286809 -4.4287133 -4.4287453][-4.428771 -4.4287033 -4.4285817 -4.4284477 -4.4283528 -4.4283009 -4.4282808 -4.4282804 -4.4283018 -4.4283714 -4.4284854 -4.4285965 -4.428669 -4.4287066 -4.4287381][-4.4287324 -4.4286618 -4.4285393 -4.4284029 -4.4283028 -4.4282393 -4.4282036 -4.4281831 -4.4281764 -4.4282517 -4.4283977 -4.4285274 -4.42861 -4.4286566 -4.42869][-4.4287043 -4.4286432 -4.4285369 -4.4284225 -4.4283428 -4.4282966 -4.4282684 -4.4282475 -4.4282303 -4.4282832 -4.4284005 -4.4284973 -4.428565 -4.4286132 -4.4286475][-4.4286838 -4.4286346 -4.4285479 -4.4284706 -4.4284296 -4.4284172 -4.4284105 -4.4283962 -4.4283686 -4.42838 -4.4284453 -4.4285021 -4.4285564 -4.4286056 -4.4286437][-4.4286795 -4.4286423 -4.4285784 -4.4285283 -4.4285097 -4.4285097 -4.4285131 -4.428494 -4.4284539 -4.4284449 -4.4284825 -4.428525 -4.4285717 -4.4286222 -4.4286628][-4.4286942 -4.42868 -4.4286432 -4.4286118 -4.4285946 -4.4285865 -4.4285789 -4.428545 -4.4284968 -4.4284859 -4.4285178 -4.4285626 -4.4286084 -4.428658 -4.4286985][-4.4287224 -4.4287319 -4.4287219 -4.4287071 -4.4286876 -4.4286618 -4.4286289 -4.4285707 -4.4285107 -4.4284983 -4.4285321 -4.4285831 -4.4286327 -4.4286823 -4.4287205][-4.4287596 -4.4287834 -4.4287944 -4.4287963 -4.4287767 -4.4287329 -4.4286747 -4.4285965 -4.4285288 -4.4285164 -4.4285502 -4.4286084 -4.4286628 -4.4287028 -4.4287262][-4.42879 -4.4288192 -4.4288416 -4.4288564 -4.42884 -4.428793 -4.4287286 -4.4286532 -4.4285922 -4.4285808 -4.4286127 -4.4286652 -4.4287086 -4.42873 -4.4287338]]...]
INFO - root - 2017-12-08 06:22:07.425215: step 23710, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 18h:57m:41s remains)
INFO - root - 2017-12-08 06:22:09.676801: step 23720, loss = 2.28, batch loss = 2.23 (35.4 examples/sec; 0.226 sec/batch; 19h:23m:09s remains)
INFO - root - 2017-12-08 06:22:11.922933: step 23730, loss = 2.28, batch loss = 2.23 (33.6 examples/sec; 0.238 sec/batch; 20h:23m:55s remains)
INFO - root - 2017-12-08 06:22:14.135702: step 23740, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 18h:47m:22s remains)
INFO - root - 2017-12-08 06:22:16.391978: step 23750, loss = 2.28, batch loss = 2.23 (35.2 examples/sec; 0.227 sec/batch; 19h:28m:45s remains)
INFO - root - 2017-12-08 06:22:18.634924: step 23760, loss = 2.28, batch loss = 2.23 (35.2 examples/sec; 0.227 sec/batch; 19h:28m:41s remains)
INFO - root - 2017-12-08 06:22:20.903161: step 23770, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.222 sec/batch; 19h:01m:45s remains)
INFO - root - 2017-12-08 06:22:23.145327: step 23780, loss = 2.28, batch loss = 2.23 (36.0 examples/sec; 0.222 sec/batch; 19h:02m:10s remains)
INFO - root - 2017-12-08 06:22:25.388528: step 23790, loss = 2.28, batch loss = 2.23 (36.6 examples/sec; 0.218 sec/batch; 18h:43m:24s remains)
INFO - root - 2017-12-08 06:22:27.604386: step 23800, loss = 2.28, batch loss = 2.23 (37.2 examples/sec; 0.215 sec/batch; 18h:25m:20s remains)
2017-12-08 06:22:27.869300: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288421 -4.4288268 -4.428812 -4.4287949 -4.4287772 -4.4287648 -4.4287663 -4.4287724 -4.42878 -4.4287848 -4.4287839 -4.42877 -4.4287481 -4.4287357 -4.4287434][-4.4288254 -4.4288044 -4.4287782 -4.4287472 -4.4287162 -4.4286952 -4.4286971 -4.4287176 -4.4287462 -4.42877 -4.4287844 -4.4287877 -4.4287786 -4.4287686 -4.4287705][-4.4288039 -4.4287815 -4.4287462 -4.4287033 -4.4286623 -4.4286346 -4.4286375 -4.4286704 -4.4287224 -4.4287686 -4.4288006 -4.4288239 -4.42883 -4.4288254 -4.4288197][-4.4287839 -4.4287682 -4.4287357 -4.4286914 -4.4286451 -4.4286113 -4.4286094 -4.4286437 -4.42871 -4.4287744 -4.428823 -4.4288621 -4.4288826 -4.4288836 -4.42887][-4.4287562 -4.4287529 -4.4287286 -4.4286833 -4.4286327 -4.4285908 -4.4285779 -4.428607 -4.4286814 -4.42876 -4.4288211 -4.4288673 -4.4288917 -4.4288931 -4.4288716][-4.4286876 -4.4287109 -4.4287024 -4.428658 -4.4286013 -4.4285488 -4.4285235 -4.42854 -4.4286094 -4.4286976 -4.428771 -4.4288211 -4.4288416 -4.428844 -4.4288282][-4.4285622 -4.428618 -4.4286385 -4.4286151 -4.4285612 -4.4285035 -4.4284697 -4.4284725 -4.4285216 -4.428606 -4.4286876 -4.4287429 -4.4287639 -4.4287691 -4.42877][-4.42846 -4.4285355 -4.42858 -4.4285827 -4.4285502 -4.428504 -4.4284706 -4.4284649 -4.4284954 -4.4285684 -4.4286461 -4.4287043 -4.4287305 -4.4287462 -4.4287624][-4.4284759 -4.428545 -4.428597 -4.4286156 -4.4286065 -4.4285765 -4.4285455 -4.4285336 -4.4285493 -4.4286032 -4.4286652 -4.4287119 -4.4287362 -4.4287562 -4.4287815][-4.4285884 -4.4286318 -4.4286728 -4.4287019 -4.4287152 -4.4287009 -4.4286671 -4.4286466 -4.4286456 -4.4286747 -4.4287124 -4.42874 -4.4287562 -4.428772 -4.4287977][-4.4287186 -4.4287348 -4.4287548 -4.428781 -4.4288082 -4.4288077 -4.4287753 -4.4287477 -4.4287333 -4.428741 -4.4287567 -4.4287677 -4.4287763 -4.4287858 -4.4287996][-4.4288225 -4.4288187 -4.4288211 -4.4288335 -4.4288597 -4.4288678 -4.4288492 -4.4288239 -4.4288054 -4.4288 -4.4287992 -4.4287949 -4.428792 -4.4287934 -4.4287882][-4.4288926 -4.4288845 -4.42888 -4.428884 -4.4289021 -4.428915 -4.4289122 -4.428896 -4.4288793 -4.4288664 -4.4288492 -4.4288263 -4.4288011 -4.4287868 -4.4287715][-4.428894 -4.4289026 -4.428915 -4.4289255 -4.4289427 -4.4289584 -4.4289627 -4.4289489 -4.4289303 -4.4289107 -4.4288859 -4.4288583 -4.4288268 -4.4288068 -4.4287896][-4.4288383 -4.4288716 -4.4289103 -4.4289374 -4.4289627 -4.4289794 -4.4289832 -4.4289694 -4.4289503 -4.42893 -4.4289083 -4.4288883 -4.42886 -4.4288378 -4.4288206]]...]
INFO - root - 2017-12-08 06:22:30.108742: step 23810, loss = 2.28, batch loss = 2.23 (35.1 examples/sec; 0.228 sec/batch; 19h:33m:07s remains)
INFO - root - 2017-12-08 06:22:32.362770: step 23820, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 18h:57m:55s remains)
INFO - root - 2017-12-08 06:22:34.605276: step 23830, loss = 2.28, batch loss = 2.23 (35.2 examples/sec; 0.227 sec/batch; 19h:27m:56s remains)
INFO - root - 2017-12-08 06:22:36.860204: step 23840, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.225 sec/batch; 19h:19m:31s remains)
INFO - root - 2017-12-08 06:22:39.126428: step 23850, loss = 2.28, batch loss = 2.23 (36.0 examples/sec; 0.222 sec/batch; 19h:03m:50s remains)
INFO - root - 2017-12-08 06:22:41.363658: step 23860, loss = 2.28, batch loss = 2.23 (34.7 examples/sec; 0.230 sec/batch; 19h:44m:33s remains)
INFO - root - 2017-12-08 06:22:43.626203: step 23870, loss = 2.28, batch loss = 2.23 (35.3 examples/sec; 0.227 sec/batch; 19h:25m:49s remains)
INFO - root - 2017-12-08 06:22:45.907054: step 23880, loss = 2.28, batch loss = 2.23 (36.0 examples/sec; 0.222 sec/batch; 19h:03m:08s remains)
INFO - root - 2017-12-08 06:22:48.139398: step 23890, loss = 2.28, batch loss = 2.23 (34.4 examples/sec; 0.232 sec/batch; 19h:55m:37s remains)
INFO - root - 2017-12-08 06:22:50.363969: step 23900, loss = 2.28, batch loss = 2.23 (34.9 examples/sec; 0.229 sec/batch; 19h:39m:27s remains)
2017-12-08 06:22:50.628862: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288621 -4.4288497 -4.428843 -4.4288468 -4.4288554 -4.4288626 -4.4288592 -4.4288425 -4.4288425 -4.4288583 -4.4288726 -4.428884 -4.4288874 -4.4288945 -4.4289103][-4.4287825 -4.4287596 -4.4287467 -4.4287548 -4.4287686 -4.4287753 -4.4287591 -4.4287171 -4.42872 -4.4287691 -4.4288206 -4.4288583 -4.4288745 -4.4288878 -4.428905][-4.4287186 -4.42868 -4.4286547 -4.4286633 -4.428679 -4.4286714 -4.4286141 -4.4285178 -4.4285235 -4.42862 -4.4287176 -4.4287915 -4.4288311 -4.4288559 -4.4288797][-4.4286871 -4.4286318 -4.4285927 -4.428596 -4.4286118 -4.4285889 -4.4284887 -4.4283333 -4.4283423 -4.4284892 -4.4286265 -4.42872 -4.4287772 -4.4288111 -4.4288435][-4.4286494 -4.4285784 -4.428534 -4.4285388 -4.4285583 -4.4285264 -4.4283986 -4.4282064 -4.428225 -4.4284182 -4.42858 -4.4286747 -4.428741 -4.4287834 -4.4288211][-4.4286137 -4.4285355 -4.4284964 -4.4284987 -4.4284945 -4.42843 -4.428257 -4.4280066 -4.4280477 -4.4283228 -4.428524 -4.4286308 -4.4287124 -4.4287677 -4.4288116][-4.4286046 -4.4285378 -4.4285169 -4.4285064 -4.4284573 -4.4283271 -4.4280772 -4.4277611 -4.4278374 -4.4281912 -4.4284329 -4.4285645 -4.4286709 -4.4287419 -4.4287939][-4.4286294 -4.4285808 -4.4285884 -4.4285855 -4.4285197 -4.4283652 -4.4281082 -4.4278212 -4.42791 -4.4282327 -4.4284382 -4.4285555 -4.4286571 -4.4287219 -4.4287758][-4.4286537 -4.4286203 -4.42866 -4.4286861 -4.4286294 -4.4284897 -4.4282908 -4.4280915 -4.4281716 -4.42841 -4.4285474 -4.4286261 -4.4286995 -4.4287453 -4.4287844][-4.4286809 -4.4286513 -4.4287004 -4.4287405 -4.4286985 -4.4285841 -4.4284296 -4.4282885 -4.4283547 -4.428544 -4.4286504 -4.4287066 -4.4287586 -4.4287906 -4.4288154][-4.4287248 -4.4286966 -4.4287329 -4.4287639 -4.42872 -4.4286189 -4.4284868 -4.4283652 -4.4284029 -4.4285622 -4.4286709 -4.4287376 -4.4287868 -4.4288144 -4.4288354][-4.4287815 -4.4287629 -4.4287877 -4.4288073 -4.4287624 -4.4286809 -4.4285836 -4.4284816 -4.428484 -4.4285927 -4.4286833 -4.42875 -4.4287958 -4.4288244 -4.4288497][-4.4288392 -4.4288287 -4.4288454 -4.4288535 -4.4288211 -4.42877 -4.4287071 -4.4286242 -4.4286017 -4.4286613 -4.4287248 -4.4287777 -4.4288158 -4.4288468 -4.4288735][-4.4289055 -4.4289012 -4.4289036 -4.4288983 -4.4288778 -4.4288478 -4.428802 -4.4287291 -4.4286947 -4.4287233 -4.4287696 -4.4288163 -4.4288492 -4.4288793 -4.4289041][-4.4289527 -4.4289517 -4.4289474 -4.4289379 -4.4289217 -4.4288969 -4.4288549 -4.4287896 -4.428751 -4.428762 -4.4287953 -4.4288344 -4.428864 -4.4288931 -4.4289184]]...]
INFO - root - 2017-12-08 06:22:52.869421: step 23910, loss = 2.28, batch loss = 2.23 (34.8 examples/sec; 0.230 sec/batch; 19h:41m:53s remains)
INFO - root - 2017-12-08 06:22:55.164922: step 23920, loss = 2.28, batch loss = 2.23 (32.8 examples/sec; 0.244 sec/batch; 20h:52m:41s remains)
INFO - root - 2017-12-08 06:22:57.407024: step 23930, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 18h:46m:40s remains)
INFO - root - 2017-12-08 06:22:59.679265: step 23940, loss = 2.28, batch loss = 2.23 (35.1 examples/sec; 0.228 sec/batch; 19h:32m:59s remains)
INFO - root - 2017-12-08 06:23:01.923016: step 23950, loss = 2.28, batch loss = 2.23 (34.7 examples/sec; 0.231 sec/batch; 19h:47m:05s remains)
INFO - root - 2017-12-08 06:23:04.180673: step 23960, loss = 2.28, batch loss = 2.23 (34.4 examples/sec; 0.233 sec/batch; 19h:57m:19s remains)
INFO - root - 2017-12-08 06:23:06.410803: step 23970, loss = 2.28, batch loss = 2.23 (36.8 examples/sec; 0.217 sec/batch; 18h:36m:51s remains)
INFO - root - 2017-12-08 06:23:08.652569: step 23980, loss = 2.28, batch loss = 2.23 (35.0 examples/sec; 0.228 sec/batch; 19h:33m:48s remains)
INFO - root - 2017-12-08 06:23:10.923923: step 23990, loss = 2.28, batch loss = 2.23 (35.1 examples/sec; 0.228 sec/batch; 19h:32m:24s remains)
INFO - root - 2017-12-08 06:23:13.168381: step 24000, loss = 2.28, batch loss = 2.23 (35.0 examples/sec; 0.229 sec/batch; 19h:35m:48s remains)
2017-12-08 06:23:13.456433: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.428792 -4.4287496 -4.428719 -4.4287095 -4.4287186 -4.428721 -4.4287181 -4.4287271 -4.4287448 -4.4287734 -4.4288039 -4.4288187 -4.4288392 -4.428874 -4.4288917][-4.4287667 -4.428731 -4.4287047 -4.4286947 -4.4287047 -4.4287119 -4.4287181 -4.4287457 -4.4287724 -4.428802 -4.4288254 -4.4288316 -4.4288459 -4.4288726 -4.4288845][-4.4287586 -4.4287324 -4.4287081 -4.4286923 -4.428699 -4.4287124 -4.428721 -4.4287567 -4.428793 -4.4288297 -4.4288497 -4.4288459 -4.4288535 -4.4288707 -4.4288716][-4.4287596 -4.4287376 -4.4287148 -4.4286942 -4.4286852 -4.4286823 -4.4286747 -4.4287066 -4.4287629 -4.4288187 -4.4288445 -4.428844 -4.4288559 -4.4288697 -4.4288645][-4.4287333 -4.4287095 -4.4286947 -4.4286804 -4.4286575 -4.4286256 -4.42858 -4.4285927 -4.4286609 -4.4287362 -4.4287829 -4.4288049 -4.4288316 -4.4288583 -4.4288611][-4.4287071 -4.4286919 -4.4286923 -4.4286923 -4.428669 -4.428616 -4.4285383 -4.4285178 -4.4285712 -4.4286432 -4.4287004 -4.4287496 -4.4287958 -4.4288359 -4.4288516][-4.4287176 -4.4287152 -4.4287257 -4.4287338 -4.4287229 -4.4286828 -4.4286194 -4.4285979 -4.4286318 -4.4286695 -4.4286985 -4.4287429 -4.4287853 -4.4288163 -4.4288316][-4.4287448 -4.4287457 -4.4287562 -4.4287667 -4.4287696 -4.4287543 -4.4287276 -4.4287295 -4.42876 -4.4287724 -4.4287739 -4.4287987 -4.42882 -4.4288287 -4.42883][-4.4287782 -4.4287815 -4.4287891 -4.4287963 -4.4287996 -4.4287963 -4.4287953 -4.4288149 -4.42884 -4.428843 -4.4288421 -4.4288588 -4.4288721 -4.4288621 -4.4288478][-4.4287992 -4.4288025 -4.428813 -4.4288206 -4.4288244 -4.4288239 -4.4288282 -4.4288397 -4.4288478 -4.4288507 -4.4288549 -4.4288712 -4.4288898 -4.4288788 -4.4288607][-4.4288154 -4.4288116 -4.4288197 -4.4288292 -4.4288325 -4.4288321 -4.4288292 -4.428823 -4.4288192 -4.4288206 -4.4288235 -4.4288383 -4.4288669 -4.4288683 -4.4288578][-4.428834 -4.42882 -4.4288211 -4.4288254 -4.4288235 -4.4288125 -4.4287953 -4.4287786 -4.4287753 -4.4287786 -4.42878 -4.4287982 -4.4288354 -4.4288478 -4.4288473][-4.4288454 -4.4288259 -4.42882 -4.42882 -4.42881 -4.4287925 -4.4287696 -4.4287539 -4.4287591 -4.4287682 -4.4287705 -4.4287887 -4.4288235 -4.4288397 -4.4288406][-4.4288526 -4.4288359 -4.4288306 -4.4288287 -4.4288197 -4.4288068 -4.4287896 -4.4287844 -4.4288011 -4.4288144 -4.4288177 -4.4288292 -4.4288492 -4.4288568 -4.4288516][-4.428863 -4.42885 -4.4288464 -4.4288454 -4.4288397 -4.428833 -4.4288254 -4.4288282 -4.4288459 -4.4288597 -4.428863 -4.4288692 -4.4288774 -4.4288774 -4.4288654]]...]
INFO - root - 2017-12-08 06:23:15.698571: step 24010, loss = 2.28, batch loss = 2.23 (34.6 examples/sec; 0.231 sec/batch; 19h:47m:14s remains)
INFO - root - 2017-12-08 06:23:17.906558: step 24020, loss = 2.28, batch loss = 2.23 (35.8 examples/sec; 0.223 sec/batch; 19h:08m:40s remains)
INFO - root - 2017-12-08 06:23:20.126965: step 24030, loss = 2.28, batch loss = 2.23 (35.6 examples/sec; 0.225 sec/batch; 19h:14m:34s remains)
INFO - root - 2017-12-08 06:23:22.383359: step 24040, loss = 2.28, batch loss = 2.23 (34.0 examples/sec; 0.236 sec/batch; 20h:11m:17s remains)
INFO - root - 2017-12-08 06:23:24.627493: step 24050, loss = 2.28, batch loss = 2.23 (36.7 examples/sec; 0.218 sec/batch; 18h:40m:19s remains)
INFO - root - 2017-12-08 06:23:26.853738: step 24060, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 18h:50m:32s remains)
INFO - root - 2017-12-08 06:23:29.100672: step 24070, loss = 2.28, batch loss = 2.23 (35.2 examples/sec; 0.227 sec/batch; 19h:26m:43s remains)
INFO - root - 2017-12-08 06:23:31.323730: step 24080, loss = 2.28, batch loss = 2.23 (35.7 examples/sec; 0.224 sec/batch; 19h:10m:54s remains)
INFO - root - 2017-12-08 06:23:33.545989: step 24090, loss = 2.28, batch loss = 2.23 (35.0 examples/sec; 0.228 sec/batch; 19h:33m:50s remains)
INFO - root - 2017-12-08 06:23:35.818371: step 24100, loss = 2.28, batch loss = 2.23 (34.7 examples/sec; 0.230 sec/batch; 19h:44m:02s remains)
2017-12-08 06:23:36.111748: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289036 -4.4289045 -4.4289079 -4.4289165 -4.42893 -4.4289289 -4.4289117 -4.42889 -4.4288907 -4.4289069 -4.4289274 -4.4289489 -4.4289646 -4.4289622 -4.4289355][-4.428915 -4.4289193 -4.428926 -4.4289355 -4.4289417 -4.4289231 -4.4288807 -4.4288406 -4.4288311 -4.4288454 -4.4288754 -4.4289122 -4.4289403 -4.4289455 -4.4289279][-4.4289145 -4.428925 -4.428936 -4.4289474 -4.4289422 -4.4289002 -4.4288273 -4.4287653 -4.4287453 -4.428762 -4.4288068 -4.428865 -4.4289093 -4.4289207 -4.42891][-4.4288988 -4.428916 -4.4289322 -4.4289412 -4.4289274 -4.428865 -4.4287634 -4.428679 -4.4286489 -4.4286656 -4.4287229 -4.4288063 -4.4288716 -4.428894 -4.4288921][-4.428865 -4.4288936 -4.4289155 -4.4289246 -4.4289012 -4.4288216 -4.428699 -4.4286017 -4.4285631 -4.4285774 -4.4286427 -4.4287448 -4.4288282 -4.4288616 -4.428874][-4.4288206 -4.4288568 -4.4288812 -4.4288869 -4.42886 -4.4287753 -4.4286489 -4.4285512 -4.428515 -4.4285326 -4.4286022 -4.4287047 -4.4287858 -4.4288192 -4.4288425][-4.4287896 -4.4288211 -4.4288411 -4.4288483 -4.4288273 -4.4287548 -4.4286413 -4.4285555 -4.42853 -4.4285541 -4.428617 -4.4286962 -4.4287515 -4.4287734 -4.4287982][-4.4288073 -4.4288259 -4.4288359 -4.4288425 -4.4288287 -4.4287663 -4.4286714 -4.4286008 -4.4285855 -4.4286103 -4.428658 -4.4287038 -4.4287295 -4.4287391 -4.428762][-4.4288549 -4.4288607 -4.4288588 -4.4288568 -4.4288435 -4.428792 -4.4287171 -4.4286618 -4.4286532 -4.4286761 -4.42871 -4.42873 -4.4287348 -4.4287362 -4.4287562][-4.4288945 -4.4288921 -4.4288807 -4.42887 -4.4288559 -4.4288182 -4.4287677 -4.4287305 -4.4287295 -4.4287534 -4.428782 -4.428793 -4.4287896 -4.4287863 -4.4287987][-4.4289188 -4.4289122 -4.4288969 -4.428884 -4.428874 -4.428853 -4.4288239 -4.4288 -4.428802 -4.4288206 -4.4288435 -4.4288573 -4.4288588 -4.4288573 -4.428864][-4.4289327 -4.4289284 -4.4289145 -4.4289041 -4.4289017 -4.428895 -4.4288807 -4.4288645 -4.4288607 -4.4288683 -4.4288836 -4.4289 -4.4289103 -4.4289145 -4.4289207][-4.428946 -4.4289451 -4.4289351 -4.4289289 -4.4289317 -4.4289317 -4.4289241 -4.4289112 -4.4289002 -4.428896 -4.4289021 -4.4289174 -4.4289346 -4.4289484 -4.4289579][-4.4289522 -4.4289556 -4.4289517 -4.4289494 -4.4289536 -4.4289546 -4.4289465 -4.4289312 -4.428915 -4.4289055 -4.4289069 -4.4289174 -4.4289346 -4.4289541 -4.4289684][-4.4289484 -4.4289536 -4.4289541 -4.4289556 -4.4289594 -4.4289565 -4.4289451 -4.4289293 -4.4289136 -4.4289041 -4.4289031 -4.4289103 -4.4289241 -4.4289427 -4.42896]]...]
INFO - root - 2017-12-08 06:23:38.373383: step 24110, loss = 2.28, batch loss = 2.23 (32.8 examples/sec; 0.244 sec/batch; 20h:53m:44s remains)
INFO - root - 2017-12-08 06:23:40.623434: step 24120, loss = 2.28, batch loss = 2.23 (36.6 examples/sec; 0.218 sec/batch; 18h:42m:53s remains)
INFO - root - 2017-12-08 06:23:42.856334: step 24130, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.221 sec/batch; 18h:53m:30s remains)
INFO - root - 2017-12-08 06:23:45.082105: step 24140, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 18h:54m:39s remains)
INFO - root - 2017-12-08 06:23:47.274204: step 24150, loss = 2.28, batch loss = 2.23 (36.8 examples/sec; 0.217 sec/batch; 18h:36m:37s remains)
INFO - root - 2017-12-08 06:23:49.503303: step 24160, loss = 2.28, batch loss = 2.23 (36.9 examples/sec; 0.217 sec/batch; 18h:34m:37s remains)
INFO - root - 2017-12-08 06:23:51.747850: step 24170, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 18h:50m:52s remains)
INFO - root - 2017-12-08 06:23:53.994007: step 24180, loss = 2.28, batch loss = 2.23 (35.0 examples/sec; 0.228 sec/batch; 19h:33m:27s remains)
INFO - root - 2017-12-08 06:23:56.240768: step 24190, loss = 2.28, batch loss = 2.23 (35.1 examples/sec; 0.228 sec/batch; 19h:32m:21s remains)
INFO - root - 2017-12-08 06:23:58.477829: step 24200, loss = 2.28, batch loss = 2.23 (31.9 examples/sec; 0.251 sec/batch; 21h:30m:02s remains)
2017-12-08 06:23:58.775377: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.428813 -4.4287753 -4.4287343 -4.4286938 -4.4286962 -4.42873 -4.4287677 -4.4287992 -4.4288216 -4.4288349 -4.42884 -4.4288306 -4.4287891 -4.4287686 -4.4287391][-4.4287782 -4.4287286 -4.428688 -4.4286451 -4.4286594 -4.4287138 -4.4287524 -4.4287887 -4.4288168 -4.4288297 -4.4288268 -4.4287972 -4.4287353 -4.4287066 -4.4286709][-4.4287572 -4.4286985 -4.4286551 -4.4286108 -4.4286437 -4.4287152 -4.4287543 -4.4287977 -4.4288282 -4.4288421 -4.4288344 -4.428782 -4.4286966 -4.4286466 -4.4286022][-4.4287496 -4.4286742 -4.4286137 -4.4285746 -4.4286184 -4.4286876 -4.4287291 -4.4287834 -4.428823 -4.4288383 -4.4288282 -4.4287691 -4.4286757 -4.4286013 -4.428544][-4.4287553 -4.4286571 -4.4285817 -4.4285502 -4.4285836 -4.4286237 -4.4286523 -4.4287081 -4.42877 -4.4288058 -4.4288077 -4.4287567 -4.4286757 -4.4286036 -4.4285412][-4.4287705 -4.4286542 -4.428566 -4.4285283 -4.4285336 -4.4285407 -4.4285369 -4.42856 -4.42864 -4.4287267 -4.4287763 -4.4287634 -4.4287162 -4.4286633 -4.4285927][-4.4287806 -4.4286737 -4.4285827 -4.42853 -4.4285045 -4.42847 -4.4284005 -4.4283328 -4.4283991 -4.428566 -4.4286942 -4.4287467 -4.4287581 -4.4287353 -4.4286728][-4.4287462 -4.4286642 -4.4285865 -4.42853 -4.4284854 -4.4284143 -4.4282575 -4.4280486 -4.4280868 -4.4283414 -4.42855 -4.4286647 -4.4287281 -4.4287467 -4.4287109][-4.4286556 -4.4286032 -4.4285588 -4.4285183 -4.4284744 -4.4283953 -4.4281788 -4.4278669 -4.4278903 -4.4281764 -4.4284015 -4.428546 -4.428647 -4.4286966 -4.4286895][-4.42855 -4.4285197 -4.4285107 -4.4285088 -4.4284968 -4.4284444 -4.42825 -4.4279871 -4.4279938 -4.4281888 -4.4283419 -4.4284606 -4.4285679 -4.4286304 -4.4286342][-4.4285359 -4.4285035 -4.4285121 -4.4285455 -4.428566 -4.4285574 -4.4284453 -4.4282808 -4.4282684 -4.4283485 -4.4284172 -4.4284964 -4.4285831 -4.4286346 -4.42863][-4.4286342 -4.428597 -4.4286079 -4.4286432 -4.4286675 -4.4286871 -4.4286413 -4.4285574 -4.4285474 -4.4285736 -4.4286013 -4.4286523 -4.4287057 -4.4287271 -4.4287009][-4.4287791 -4.4287543 -4.4287667 -4.4287953 -4.4288058 -4.4288235 -4.4288163 -4.4287891 -4.4287925 -4.4288054 -4.4288149 -4.4288363 -4.4288545 -4.4288487 -4.4288177][-4.4289203 -4.4289112 -4.4289222 -4.4289422 -4.4289417 -4.428947 -4.4289508 -4.4289508 -4.4289703 -4.4289837 -4.4289837 -4.4289761 -4.428966 -4.4289489 -4.4289241][-4.4290051 -4.4290047 -4.4290142 -4.4290266 -4.4290214 -4.42902 -4.4290242 -4.4290333 -4.4290538 -4.4290638 -4.429059 -4.4290366 -4.4290137 -4.4289918 -4.4289784]]...]
INFO - root - 2017-12-08 06:24:01.007558: step 24210, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.221 sec/batch; 18h:58m:06s remains)
INFO - root - 2017-12-08 06:24:03.233189: step 24220, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 18h:48m:52s remains)
INFO - root - 2017-12-08 06:24:05.469437: step 24230, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.219 sec/batch; 18h:47m:41s remains)
INFO - root - 2017-12-08 06:24:07.739406: step 24240, loss = 2.28, batch loss = 2.23 (34.9 examples/sec; 0.229 sec/batch; 19h:36m:26s remains)
INFO - root - 2017-12-08 06:24:09.964725: step 24250, loss = 2.28, batch loss = 2.23 (36.6 examples/sec; 0.218 sec/batch; 18h:41m:37s remains)
INFO - root - 2017-12-08 06:24:12.189859: step 24260, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 19h:03m:32s remains)
INFO - root - 2017-12-08 06:24:14.431504: step 24270, loss = 2.28, batch loss = 2.23 (35.0 examples/sec; 0.228 sec/batch; 19h:33m:18s remains)
INFO - root - 2017-12-08 06:24:16.651221: step 24280, loss = 2.28, batch loss = 2.23 (35.4 examples/sec; 0.226 sec/batch; 19h:21m:28s remains)
INFO - root - 2017-12-08 06:24:18.889632: step 24290, loss = 2.28, batch loss = 2.23 (35.7 examples/sec; 0.224 sec/batch; 19h:10m:11s remains)
INFO - root - 2017-12-08 06:24:21.138952: step 24300, loss = 2.28, batch loss = 2.23 (35.3 examples/sec; 0.226 sec/batch; 19h:22m:35s remains)
2017-12-08 06:24:21.414667: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.428874 -4.4288468 -4.4288015 -4.4287591 -4.4287057 -4.4286537 -4.428628 -4.4286413 -4.4286814 -4.4287386 -4.4287853 -4.4287848 -4.42875 -4.4287109 -4.4286914][-4.4288855 -4.4288626 -4.4288244 -4.4287872 -4.4287343 -4.428689 -4.4286747 -4.4286909 -4.4287181 -4.4287605 -4.4287887 -4.4287786 -4.4287448 -4.4287167 -4.4287109][-4.4288864 -4.428863 -4.4288278 -4.4287963 -4.4287548 -4.4287262 -4.4287238 -4.4287477 -4.4287663 -4.4287839 -4.428782 -4.4287519 -4.4287219 -4.4287114 -4.4287319][-4.4288726 -4.4288459 -4.4288058 -4.4287777 -4.4287434 -4.4287186 -4.428721 -4.4287491 -4.4287724 -4.4287705 -4.4287405 -4.42869 -4.4286585 -4.4286695 -4.4287148][-4.4288483 -4.428803 -4.4287434 -4.4287038 -4.4286537 -4.428618 -4.4286261 -4.4286747 -4.4287181 -4.4287148 -4.4286642 -4.4286032 -4.4285674 -4.4285841 -4.4286408][-4.4288278 -4.428762 -4.4286757 -4.4286132 -4.4285488 -4.4285 -4.4285016 -4.4285555 -4.4286137 -4.4286213 -4.4285669 -4.4285164 -4.4284897 -4.428493 -4.4285297][-4.4288287 -4.4287539 -4.4286504 -4.4285707 -4.4284911 -4.42843 -4.4284067 -4.4284325 -4.4284792 -4.4284959 -4.4284725 -4.4284592 -4.4284616 -4.4284577 -4.4284635][-4.4288292 -4.4287529 -4.4286432 -4.4285517 -4.4284616 -4.4283853 -4.4283414 -4.4283404 -4.4283705 -4.4283967 -4.4284124 -4.4284492 -4.428494 -4.4285126 -4.4285183][-4.42882 -4.4287467 -4.4286461 -4.42857 -4.4284973 -4.4284339 -4.42839 -4.4283805 -4.4283943 -4.4284186 -4.4284539 -4.4285259 -4.428597 -4.4286284 -4.4286408][-4.4288206 -4.4287567 -4.4286819 -4.4286413 -4.4286046 -4.4285712 -4.428545 -4.4285464 -4.4285555 -4.4285765 -4.4286103 -4.4286742 -4.4287353 -4.4287658 -4.4287753][-4.4288282 -4.4287796 -4.4287357 -4.4287281 -4.42872 -4.4287057 -4.4286976 -4.4287052 -4.4287157 -4.4287343 -4.42876 -4.4287996 -4.4288349 -4.4288492 -4.4288473][-4.4288363 -4.4288034 -4.4287844 -4.4287958 -4.4288039 -4.4288015 -4.4287977 -4.4287996 -4.428802 -4.4288158 -4.4288368 -4.4288597 -4.4288759 -4.42888 -4.4288764][-4.4288435 -4.4288206 -4.4288149 -4.4288363 -4.4288559 -4.42886 -4.4288568 -4.428853 -4.4288483 -4.4288626 -4.4288821 -4.4288983 -4.4289074 -4.4289079 -4.428905][-4.4288559 -4.4288368 -4.4288321 -4.4288526 -4.4288731 -4.4288812 -4.4288845 -4.4288826 -4.4288821 -4.428896 -4.4289136 -4.4289246 -4.4289289 -4.4289255 -4.4289203][-4.4288735 -4.4288592 -4.4288545 -4.4288683 -4.428885 -4.4288917 -4.428895 -4.428895 -4.4288945 -4.4289031 -4.4289145 -4.4289207 -4.4289231 -4.4289188 -4.4289117]]...]
INFO - root - 2017-12-08 06:24:23.685726: step 24310, loss = 2.28, batch loss = 2.23 (35.2 examples/sec; 0.227 sec/batch; 19h:27m:50s remains)
INFO - root - 2017-12-08 06:24:25.912035: step 24320, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 18h:45m:37s remains)
INFO - root - 2017-12-08 06:24:28.152000: step 24330, loss = 2.28, batch loss = 2.23 (37.3 examples/sec; 0.214 sec/batch; 18h:20m:34s remains)
INFO - root - 2017-12-08 06:24:30.385487: step 24340, loss = 2.28, batch loss = 2.23 (36.0 examples/sec; 0.222 sec/batch; 19h:01m:39s remains)
INFO - root - 2017-12-08 06:24:32.633011: step 24350, loss = 2.28, batch loss = 2.23 (37.1 examples/sec; 0.216 sec/batch; 18h:28m:46s remains)
INFO - root - 2017-12-08 06:24:34.889118: step 24360, loss = 2.28, batch loss = 2.23 (34.9 examples/sec; 0.229 sec/batch; 19h:36m:36s remains)
INFO - root - 2017-12-08 06:24:37.110049: step 24370, loss = 2.28, batch loss = 2.23 (36.8 examples/sec; 0.217 sec/batch; 18h:35m:32s remains)
INFO - root - 2017-12-08 06:24:39.351961: step 24380, loss = 2.28, batch loss = 2.23 (36.6 examples/sec; 0.219 sec/batch; 18h:42m:06s remains)
INFO - root - 2017-12-08 06:24:41.568491: step 24390, loss = 2.28, batch loss = 2.23 (37.0 examples/sec; 0.216 sec/batch; 18h:31m:36s remains)
INFO - root - 2017-12-08 06:24:43.826736: step 24400, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.221 sec/batch; 18h:57m:01s remains)
2017-12-08 06:24:44.119273: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289441 -4.4289412 -4.4289379 -4.4289393 -4.4289432 -4.4289422 -4.4289374 -4.4289336 -4.4289327 -4.4289293 -4.4289227 -4.4289169 -4.4289126 -4.4289041 -4.4288917][-4.4289455 -4.4289441 -4.4289403 -4.4289403 -4.428946 -4.4289517 -4.4289532 -4.4289556 -4.4289584 -4.4289556 -4.4289451 -4.4289351 -4.42893 -4.4289231 -4.4289107][-4.4289112 -4.4289103 -4.4289069 -4.4289026 -4.4289026 -4.4289069 -4.4289074 -4.4289103 -4.4289174 -4.4289193 -4.4289126 -4.42891 -4.4289141 -4.4289184 -4.4289141][-4.4288344 -4.4288344 -4.4288344 -4.4288263 -4.4288154 -4.4288068 -4.4288025 -4.4288063 -4.4288206 -4.4288325 -4.4288344 -4.4288406 -4.4288578 -4.4288764 -4.4288883][-4.4287424 -4.4287424 -4.4287372 -4.4287186 -4.4286933 -4.4286685 -4.4286566 -4.4286723 -4.428709 -4.4287391 -4.4287505 -4.4287572 -4.4287748 -4.428803 -4.428834][-4.4286532 -4.4286528 -4.4286385 -4.4286056 -4.4285522 -4.4284878 -4.4284525 -4.4284964 -4.4285822 -4.4286442 -4.4286704 -4.4286771 -4.4286919 -4.4287181 -4.4287505][-4.4285927 -4.4285903 -4.428566 -4.42851 -4.4284096 -4.4282813 -4.4282007 -4.4282808 -4.4284434 -4.4285593 -4.4286032 -4.4286127 -4.428627 -4.4286466 -4.4286685][-4.4285469 -4.4285564 -4.4285374 -4.4284716 -4.4283495 -4.4281845 -4.4280605 -4.4281459 -4.4283543 -4.4285045 -4.4285669 -4.4285827 -4.428597 -4.4286141 -4.4286294][-4.4285049 -4.4285474 -4.4285626 -4.4285192 -4.4284267 -4.42831 -4.4282193 -4.4282651 -4.4284048 -4.4285192 -4.4285755 -4.42859 -4.4286032 -4.4286227 -4.4286432][-4.4284682 -4.4285407 -4.4285889 -4.4285793 -4.4285331 -4.4284906 -4.4284639 -4.4284873 -4.4285522 -4.42861 -4.4286385 -4.4286447 -4.4286509 -4.4286647 -4.4286776][-4.4284806 -4.4285512 -4.428606 -4.4286208 -4.4286132 -4.428617 -4.4286251 -4.4286327 -4.428659 -4.4286928 -4.4287162 -4.4287271 -4.42873 -4.4287267 -4.4287138][-4.4285707 -4.4286113 -4.4286461 -4.4286633 -4.4286728 -4.428689 -4.4286923 -4.4286809 -4.4286861 -4.4287171 -4.428751 -4.428772 -4.4287815 -4.4287767 -4.4287562][-4.4287052 -4.4287181 -4.4287286 -4.4287314 -4.4287319 -4.4287257 -4.4287024 -4.4286695 -4.4286647 -4.4287019 -4.4287491 -4.4287825 -4.4288068 -4.4288163 -4.4288082][-4.4288297 -4.4288335 -4.428834 -4.4288235 -4.4288063 -4.4287734 -4.4287205 -4.428669 -4.4286585 -4.4286952 -4.4287438 -4.4287839 -4.4288192 -4.4288492 -4.428864][-4.4289002 -4.4289021 -4.4288974 -4.42888 -4.4288545 -4.4288106 -4.4287529 -4.4287014 -4.4286885 -4.4287157 -4.4287572 -4.4287963 -4.4288273 -4.4288545 -4.4288774]]...]
INFO - root - 2017-12-08 06:24:46.326734: step 24410, loss = 2.28, batch loss = 2.23 (35.4 examples/sec; 0.226 sec/batch; 19h:18m:57s remains)
INFO - root - 2017-12-08 06:24:48.552125: step 24420, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 19h:03m:04s remains)
INFO - root - 2017-12-08 06:24:50.779305: step 24430, loss = 2.28, batch loss = 2.23 (37.3 examples/sec; 0.214 sec/batch; 18h:21m:20s remains)
INFO - root - 2017-12-08 06:24:53.022107: step 24440, loss = 2.28, batch loss = 2.23 (36.6 examples/sec; 0.219 sec/batch; 18h:42m:49s remains)
INFO - root - 2017-12-08 06:24:55.279556: step 24450, loss = 2.28, batch loss = 2.23 (34.8 examples/sec; 0.230 sec/batch; 19h:40m:09s remains)
INFO - root - 2017-12-08 06:24:57.515121: step 24460, loss = 2.28, batch loss = 2.23 (35.4 examples/sec; 0.226 sec/batch; 19h:20m:21s remains)
INFO - root - 2017-12-08 06:24:59.746276: step 24470, loss = 2.28, batch loss = 2.23 (34.7 examples/sec; 0.230 sec/batch; 19h:42m:18s remains)
INFO - root - 2017-12-08 06:25:01.995550: step 24480, loss = 2.28, batch loss = 2.23 (35.0 examples/sec; 0.229 sec/batch; 19h:34m:08s remains)
INFO - root - 2017-12-08 06:25:04.233241: step 24490, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 18h:55m:24s remains)
INFO - root - 2017-12-08 06:25:06.514519: step 24500, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 18h:52m:54s remains)
2017-12-08 06:25:06.817581: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289804 -4.4289808 -4.4289737 -4.4289641 -4.4289527 -4.4289408 -4.4289336 -4.4289308 -4.4289322 -4.4289393 -4.4289513 -4.4289613 -4.4289575 -4.4289427 -4.4289322][-4.4289622 -4.4289646 -4.4289603 -4.4289546 -4.428946 -4.4289341 -4.4289269 -4.4289241 -4.4289284 -4.4289355 -4.4289446 -4.4289508 -4.4289446 -4.42893 -4.4289122][-4.428946 -4.4289536 -4.4289565 -4.4289556 -4.428947 -4.4289284 -4.4289141 -4.4289112 -4.4289184 -4.4289207 -4.4289188 -4.4289169 -4.4289131 -4.4289055 -4.4288864][-4.4289322 -4.4289455 -4.4289513 -4.428947 -4.4289279 -4.428894 -4.428865 -4.4288568 -4.4288673 -4.42887 -4.4288673 -4.4288626 -4.4288659 -4.4288654 -4.4288573][-4.4289212 -4.4289365 -4.4289384 -4.42892 -4.4288821 -4.42883 -4.4287767 -4.42875 -4.4287543 -4.4287634 -4.4287739 -4.4287858 -4.4288025 -4.4288068 -4.4288092][-4.428915 -4.4289303 -4.428926 -4.4288936 -4.428843 -4.4287744 -4.4286895 -4.428618 -4.4285927 -4.4286122 -4.4286623 -4.428709 -4.4287391 -4.4287367 -4.4287338][-4.4289169 -4.4289293 -4.42892 -4.4288783 -4.4288173 -4.4287305 -4.4286132 -4.4284916 -4.4284286 -4.4284687 -4.4285669 -4.4286432 -4.4286766 -4.4286647 -4.4286509][-4.4289336 -4.428947 -4.428937 -4.428894 -4.4288173 -4.4287081 -4.4285655 -4.4284215 -4.428359 -4.4284248 -4.4285426 -4.4286242 -4.4286489 -4.4286237 -4.4286][-4.428957 -4.4289708 -4.4289618 -4.4289217 -4.4288392 -4.4287229 -4.4285817 -4.4284563 -4.4284172 -4.4284878 -4.4285841 -4.4286456 -4.4286618 -4.4286323 -4.4286075][-4.428977 -4.4289861 -4.4289756 -4.4289455 -4.4288845 -4.4287963 -4.4286962 -4.4286137 -4.4285865 -4.4286218 -4.4286737 -4.4287043 -4.4287162 -4.4286919 -4.428668][-4.4289942 -4.4289994 -4.4289918 -4.4289784 -4.4289517 -4.4289117 -4.4288626 -4.4288158 -4.4287915 -4.4287853 -4.42879 -4.4287949 -4.4287977 -4.4287834 -4.4287686][-4.429008 -4.4290133 -4.4290133 -4.4290152 -4.4290104 -4.428998 -4.4289756 -4.4289484 -4.4289236 -4.4289045 -4.4288983 -4.4288955 -4.4288931 -4.4288864 -4.4288797][-4.4290171 -4.4290218 -4.4290261 -4.4290338 -4.4290357 -4.42903 -4.4290171 -4.4290023 -4.4289827 -4.4289665 -4.4289637 -4.4289603 -4.428957 -4.4289536 -4.4289494][-4.4290247 -4.4290295 -4.4290323 -4.4290361 -4.4290376 -4.4290342 -4.4290252 -4.4290147 -4.4290023 -4.4289932 -4.4289908 -4.4289875 -4.4289861 -4.4289827 -4.4289808][-4.4290237 -4.4290295 -4.42903 -4.4290309 -4.4290304 -4.429028 -4.4290223 -4.4290156 -4.4290075 -4.4290018 -4.4289989 -4.4289961 -4.4289942 -4.4289923 -4.4289923]]...]
INFO - root - 2017-12-08 06:25:09.047979: step 24510, loss = 2.28, batch loss = 2.23 (35.7 examples/sec; 0.224 sec/batch; 19h:09m:10s remains)
INFO - root - 2017-12-08 06:25:11.291485: step 24520, loss = 2.28, batch loss = 2.23 (34.9 examples/sec; 0.229 sec/batch; 19h:37m:41s remains)
INFO - root - 2017-12-08 06:25:13.543226: step 24530, loss = 2.28, batch loss = 2.23 (37.9 examples/sec; 0.211 sec/batch; 18h:02m:13s remains)
INFO - root - 2017-12-08 06:25:15.769184: step 24540, loss = 2.28, batch loss = 2.23 (37.4 examples/sec; 0.214 sec/batch; 18h:19m:04s remains)
INFO - root - 2017-12-08 06:25:18.001132: step 24550, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 18h:53m:00s remains)
INFO - root - 2017-12-08 06:25:20.275776: step 24560, loss = 2.28, batch loss = 2.23 (35.7 examples/sec; 0.224 sec/batch; 19h:08m:55s remains)
INFO - root - 2017-12-08 06:25:22.507133: step 24570, loss = 2.28, batch loss = 2.23 (35.3 examples/sec; 0.227 sec/batch; 19h:23m:58s remains)
INFO - root - 2017-12-08 06:25:24.793667: step 24580, loss = 2.28, batch loss = 2.23 (36.6 examples/sec; 0.219 sec/batch; 18h:42m:49s remains)
INFO - root - 2017-12-08 06:25:27.031863: step 24590, loss = 2.28, batch loss = 2.23 (35.6 examples/sec; 0.225 sec/batch; 19h:13m:43s remains)
INFO - root - 2017-12-08 06:25:29.285026: step 24600, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.220 sec/batch; 18h:49m:47s remains)
2017-12-08 06:25:29.579336: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289241 -4.4288988 -4.4288726 -4.4288483 -4.4288435 -4.4288559 -4.4288664 -4.4288759 -4.4288979 -4.4289083 -4.4289021 -4.428895 -4.4288807 -4.4288588 -4.4288435][-4.4289017 -4.4288583 -4.4288135 -4.4287715 -4.4287562 -4.4287677 -4.4287758 -4.4287815 -4.4288111 -4.4288411 -4.4288554 -4.4288669 -4.42886 -4.4288287 -4.4287906][-4.4288764 -4.4288111 -4.4287367 -4.4286709 -4.4286475 -4.4286547 -4.4286537 -4.428659 -4.4287033 -4.4287515 -4.4287853 -4.4288125 -4.4288168 -4.4287839 -4.4287329][-4.4288583 -4.4287767 -4.4286766 -4.4285879 -4.42856 -4.4285555 -4.4285297 -4.4285164 -4.4285741 -4.4286466 -4.4287 -4.4287314 -4.4287457 -4.4287229 -4.4286771][-4.4288497 -4.4287643 -4.42865 -4.4285488 -4.4285121 -4.42848 -4.4284048 -4.4283514 -4.4284143 -4.4285145 -4.4285893 -4.4286251 -4.4286537 -4.428658 -4.42864][-4.4288468 -4.4287653 -4.4286475 -4.428534 -4.4284697 -4.4283886 -4.42825 -4.4281535 -4.428236 -4.4283824 -4.4284825 -4.4285197 -4.4285574 -4.4285975 -4.4286146][-4.428843 -4.4287677 -4.4286556 -4.428534 -4.4284406 -4.4283128 -4.4281287 -4.4280219 -4.4281325 -4.4283142 -4.4284167 -4.4284282 -4.4284372 -4.4284945 -4.42855][-4.4288373 -4.4287744 -4.4286804 -4.4285703 -4.4284739 -4.4283423 -4.4281783 -4.4281011 -4.4282041 -4.4283562 -4.428412 -4.4283605 -4.4283185 -4.4283662 -4.4284372][-4.428834 -4.4287858 -4.4287167 -4.428637 -4.4285612 -4.42846 -4.4283519 -4.4283061 -4.4283662 -4.42845 -4.4284496 -4.4283528 -4.4282684 -4.4282808 -4.428328][-4.4288344 -4.4287968 -4.428751 -4.4287028 -4.4286561 -4.4285955 -4.42853 -4.4284925 -4.428504 -4.4285364 -4.4285169 -4.4284306 -4.4283519 -4.4283361 -4.42834][-4.4288359 -4.4287982 -4.4287672 -4.4287453 -4.4287305 -4.4287071 -4.4286723 -4.4286375 -4.428616 -4.4286208 -4.4286108 -4.4285693 -4.4285383 -4.428525 -4.4284978][-4.4288411 -4.4288015 -4.4287744 -4.4287682 -4.4287796 -4.4287906 -4.428782 -4.4287539 -4.4287252 -4.4287167 -4.4287119 -4.4287009 -4.4287071 -4.4286995 -4.4286604][-4.4288511 -4.4288058 -4.4287786 -4.4287796 -4.4288087 -4.428844 -4.42885 -4.4288292 -4.4288034 -4.4287953 -4.4287968 -4.4288 -4.4288154 -4.4288111 -4.4287791][-4.4288616 -4.428813 -4.4287834 -4.4287863 -4.4288239 -4.4288673 -4.4288797 -4.4288659 -4.42885 -4.4288464 -4.4288468 -4.4288526 -4.4288707 -4.4288726 -4.4288568][-4.4288812 -4.4288344 -4.4288039 -4.4288087 -4.4288454 -4.428884 -4.4288964 -4.4288874 -4.42888 -4.4288812 -4.4288769 -4.4288797 -4.4288945 -4.4289026 -4.4289002]]...]
INFO - root - 2017-12-08 06:25:31.800787: step 24610, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 18h:46m:35s remains)
INFO - root - 2017-12-08 06:25:34.062836: step 24620, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 19h:04m:34s remains)
INFO - root - 2017-12-08 06:25:36.288678: step 24630, loss = 2.28, batch loss = 2.23 (34.5 examples/sec; 0.232 sec/batch; 19h:50m:23s remains)
INFO - root - 2017-12-08 06:25:38.519539: step 24640, loss = 2.28, batch loss = 2.23 (36.6 examples/sec; 0.218 sec/batch; 18h:40m:28s remains)
INFO - root - 2017-12-08 06:25:40.743849: step 24650, loss = 2.28, batch loss = 2.23 (34.7 examples/sec; 0.230 sec/batch; 19h:41m:28s remains)
INFO - root - 2017-12-08 06:25:42.982040: step 24660, loss = 2.28, batch loss = 2.23 (34.2 examples/sec; 0.234 sec/batch; 20h:01m:34s remains)
INFO - root - 2017-12-08 06:25:45.235946: step 24670, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 18h:47m:37s remains)
INFO - root - 2017-12-08 06:25:47.462827: step 24680, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.221 sec/batch; 18h:56m:02s remains)
INFO - root - 2017-12-08 06:25:49.697940: step 24690, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.226 sec/batch; 19h:17m:00s remains)
INFO - root - 2017-12-08 06:25:51.926735: step 24700, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.220 sec/batch; 18h:50m:40s remains)
2017-12-08 06:25:52.212470: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289417 -4.4289451 -4.4289484 -4.42895 -4.4289517 -4.4289522 -4.42895 -4.428946 -4.4289403 -4.4289341 -4.4289293 -4.4289289 -4.4289327 -4.42894 -4.428947][-4.4289708 -4.4289713 -4.42897 -4.4289675 -4.4289675 -4.428967 -4.4289622 -4.428957 -4.4289494 -4.4289412 -4.428936 -4.4289355 -4.4289389 -4.4289465 -4.4289551][-4.4289718 -4.4289665 -4.4289584 -4.4289494 -4.4289403 -4.42893 -4.4289122 -4.4288926 -4.428874 -4.428864 -4.428864 -4.4288692 -4.4288812 -4.4288955 -4.4289069][-4.4289527 -4.4289384 -4.4289165 -4.4288917 -4.4288673 -4.4288454 -4.428813 -4.4287772 -4.4287453 -4.4287381 -4.4287486 -4.4287663 -4.4287949 -4.4288206 -4.4288373][-4.4289145 -4.4288869 -4.4288473 -4.4288025 -4.4287596 -4.4287262 -4.4286823 -4.4286261 -4.4285812 -4.4285841 -4.4286141 -4.4286566 -4.4287148 -4.4287524 -4.4287686][-4.4288669 -4.4288225 -4.4287567 -4.4286828 -4.4286165 -4.4285712 -4.4285192 -4.4284515 -4.4284029 -4.4284248 -4.4284844 -4.428556 -4.4286385 -4.4286814 -4.4286942][-4.4288239 -4.4287548 -4.4286604 -4.4285545 -4.4284673 -4.4284124 -4.4283504 -4.4282713 -4.4282308 -4.4282923 -4.4283919 -4.428494 -4.4285917 -4.4286342 -4.4286437][-4.4287977 -4.4287138 -4.4286089 -4.4285016 -4.4284244 -4.4283857 -4.4283414 -4.4282808 -4.4282641 -4.4283452 -4.4284511 -4.4285555 -4.4286423 -4.4286766 -4.4286828][-4.4288182 -4.4287434 -4.4286566 -4.4285769 -4.4285288 -4.4285069 -4.4284863 -4.4284544 -4.42845 -4.4285064 -4.4285769 -4.4286566 -4.4287157 -4.4287348 -4.4287429][-4.4288626 -4.4288063 -4.4287453 -4.4286938 -4.4286652 -4.4286509 -4.4286432 -4.4286313 -4.4286337 -4.4286642 -4.4287038 -4.4287586 -4.4287953 -4.4288025 -4.4288058][-4.4289126 -4.4288769 -4.42884 -4.4288149 -4.4288068 -4.4288015 -4.4288006 -4.4287982 -4.4288011 -4.4288116 -4.4288311 -4.4288621 -4.4288778 -4.4288807 -4.4288812][-4.428957 -4.4289351 -4.4289169 -4.4289069 -4.4289069 -4.428906 -4.4289064 -4.4289074 -4.4289093 -4.4289136 -4.4289241 -4.4289427 -4.4289556 -4.4289575 -4.4289556][-4.4289913 -4.4289794 -4.4289718 -4.4289684 -4.4289703 -4.4289718 -4.4289746 -4.4289775 -4.4289794 -4.4289823 -4.4289861 -4.4289942 -4.4289989 -4.428998 -4.4289913][-4.4290094 -4.4290018 -4.4289956 -4.4289913 -4.4289885 -4.428988 -4.4289904 -4.4289932 -4.4289947 -4.4289951 -4.4289942 -4.4289913 -4.428987 -4.4289818 -4.4289756][-4.4290037 -4.4289923 -4.4289804 -4.428968 -4.4289589 -4.4289556 -4.4289575 -4.4289618 -4.428966 -4.4289684 -4.428968 -4.4289622 -4.4289556 -4.4289508 -4.4289494]]...]
INFO - root - 2017-12-08 06:25:54.450097: step 24710, loss = 2.28, batch loss = 2.23 (36.6 examples/sec; 0.219 sec/batch; 18h:40m:54s remains)
INFO - root - 2017-12-08 06:25:56.685557: step 24720, loss = 2.28, batch loss = 2.23 (35.3 examples/sec; 0.227 sec/batch; 19h:23m:24s remains)
INFO - root - 2017-12-08 06:25:58.922477: step 24730, loss = 2.28, batch loss = 2.23 (35.3 examples/sec; 0.227 sec/batch; 19h:23m:47s remains)
INFO - root - 2017-12-08 06:26:01.176990: step 24740, loss = 2.28, batch loss = 2.23 (36.0 examples/sec; 0.222 sec/batch; 18h:59m:17s remains)
INFO - root - 2017-12-08 06:26:03.412519: step 24750, loss = 2.28, batch loss = 2.23 (34.0 examples/sec; 0.235 sec/batch; 20h:07m:19s remains)
INFO - root - 2017-12-08 06:26:05.665802: step 24760, loss = 2.28, batch loss = 2.23 (36.7 examples/sec; 0.218 sec/batch; 18h:37m:22s remains)
INFO - root - 2017-12-08 06:26:07.935692: step 24770, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 19h:02m:12s remains)
INFO - root - 2017-12-08 06:26:10.202092: step 24780, loss = 2.28, batch loss = 2.23 (34.8 examples/sec; 0.230 sec/batch; 19h:40m:19s remains)
INFO - root - 2017-12-08 06:26:12.428959: step 24790, loss = 2.28, batch loss = 2.23 (34.3 examples/sec; 0.233 sec/batch; 19h:56m:30s remains)
INFO - root - 2017-12-08 06:26:14.648725: step 24800, loss = 2.28, batch loss = 2.23 (38.1 examples/sec; 0.210 sec/batch; 17h:57m:09s remains)
2017-12-08 06:26:14.932021: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4287882 -4.4288034 -4.4288268 -4.4288449 -4.4288492 -4.4288411 -4.4288263 -4.4288163 -4.4288139 -4.4288177 -4.4288225 -4.4288182 -4.4287944 -4.4287653 -4.4287529][-4.4287577 -4.4287815 -4.4288154 -4.4288416 -4.428844 -4.4288249 -4.4287982 -4.4287786 -4.4287772 -4.4287868 -4.4287982 -4.4288044 -4.428793 -4.428772 -4.4287715][-4.4287333 -4.4287653 -4.428803 -4.4288321 -4.4288306 -4.4287968 -4.4287572 -4.4287343 -4.4287419 -4.4287653 -4.428792 -4.4288211 -4.42883 -4.4288249 -4.4288316][-4.4287353 -4.4287581 -4.4287872 -4.4288125 -4.42881 -4.4287767 -4.4287376 -4.4287181 -4.4287343 -4.4287729 -4.4288163 -4.4288626 -4.4288816 -4.4288783 -4.428874][-4.4287987 -4.4287992 -4.4288087 -4.4288206 -4.4288082 -4.4287705 -4.4287305 -4.4287086 -4.4287214 -4.4287658 -4.4288235 -4.4288764 -4.4288926 -4.4288764 -4.4288526][-4.4288492 -4.4288373 -4.4288363 -4.4288383 -4.4288096 -4.4287615 -4.4287086 -4.4286757 -4.4286842 -4.4287286 -4.4287872 -4.4288282 -4.4288325 -4.4288049 -4.4287643][-4.4288516 -4.4288287 -4.4288177 -4.4288087 -4.4287634 -4.4286957 -4.4286203 -4.4285669 -4.4285712 -4.4286251 -4.428689 -4.4287238 -4.4287252 -4.4286885 -4.4286404][-4.42882 -4.4287977 -4.428771 -4.4287438 -4.4286871 -4.4286017 -4.4284844 -4.4283805 -4.4283757 -4.428453 -4.4285378 -4.4285846 -4.4285932 -4.4285665 -4.4285312][-4.4287524 -4.4287491 -4.4287276 -4.4286947 -4.4286318 -4.4285593 -4.4284482 -4.4283228 -4.4283166 -4.4284005 -4.42849 -4.4285378 -4.4285579 -4.4285588 -4.4285507][-4.4287081 -4.4287233 -4.4287181 -4.4286885 -4.4286346 -4.4285965 -4.4285517 -4.4284897 -4.4284868 -4.4285312 -4.4285827 -4.4286256 -4.4286594 -4.4286804 -4.4286733][-4.4287186 -4.4287295 -4.4287286 -4.42871 -4.4286747 -4.4286737 -4.4286838 -4.428678 -4.428678 -4.4286909 -4.4287186 -4.4287496 -4.42878 -4.428792 -4.428771][-4.428721 -4.4287353 -4.4287481 -4.4287448 -4.4287295 -4.4287419 -4.4287672 -4.4287748 -4.4287739 -4.4287715 -4.4287777 -4.4287896 -4.4288116 -4.4288154 -4.4287853][-4.4286771 -4.4286857 -4.4287057 -4.4287109 -4.4287105 -4.4287291 -4.428751 -4.4287577 -4.4287553 -4.4287467 -4.4287429 -4.4287472 -4.4287682 -4.4287682 -4.4287395][-4.428606 -4.4286041 -4.4286156 -4.4286118 -4.4286079 -4.4286323 -4.4286571 -4.4286666 -4.4286685 -4.4286666 -4.428659 -4.42866 -4.4286766 -4.4286757 -4.428658][-4.4285631 -4.4285479 -4.4285445 -4.4285288 -4.4285107 -4.428524 -4.4285507 -4.4285741 -4.4285822 -4.4285827 -4.4285655 -4.428556 -4.4285669 -4.4285736 -4.4285831]]...]
INFO - root - 2017-12-08 06:26:17.170173: step 24810, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 19h:03m:16s remains)
INFO - root - 2017-12-08 06:26:19.398670: step 24820, loss = 2.28, batch loss = 2.23 (35.0 examples/sec; 0.229 sec/batch; 19h:32m:22s remains)
INFO - root - 2017-12-08 06:26:21.629651: step 24830, loss = 2.28, batch loss = 2.23 (36.6 examples/sec; 0.218 sec/batch; 18h:39m:58s remains)
INFO - root - 2017-12-08 06:26:23.881215: step 24840, loss = 2.28, batch loss = 2.23 (35.4 examples/sec; 0.226 sec/batch; 19h:18m:18s remains)
INFO - root - 2017-12-08 06:26:26.118778: step 24850, loss = 2.28, batch loss = 2.23 (35.8 examples/sec; 0.223 sec/batch; 19h:04m:47s remains)
INFO - root - 2017-12-08 06:26:28.351030: step 24860, loss = 2.28, batch loss = 2.23 (34.8 examples/sec; 0.230 sec/batch; 19h:38m:08s remains)
INFO - root - 2017-12-08 06:26:30.582749: step 24870, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 19h:01m:06s remains)
INFO - root - 2017-12-08 06:26:32.812656: step 24880, loss = 2.28, batch loss = 2.23 (35.1 examples/sec; 0.228 sec/batch; 19h:28m:06s remains)
INFO - root - 2017-12-08 06:26:35.046310: step 24890, loss = 2.28, batch loss = 2.23 (35.2 examples/sec; 0.228 sec/batch; 19h:26m:23s remains)
INFO - root - 2017-12-08 06:26:37.280522: step 24900, loss = 2.28, batch loss = 2.23 (34.4 examples/sec; 0.233 sec/batch; 19h:52m:37s remains)
2017-12-08 06:26:37.602228: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289141 -4.4289036 -4.4288993 -4.42889 -4.4288845 -4.428885 -4.4288821 -4.4288721 -4.4288583 -4.4288521 -4.4288449 -4.4288392 -4.4288316 -4.4288287 -4.4288368][-4.4289155 -4.4289045 -4.4288993 -4.4288912 -4.4288888 -4.4288912 -4.4288859 -4.4288735 -4.4288654 -4.4288678 -4.428875 -4.4288754 -4.4288669 -4.4288597 -4.4288616][-4.4289093 -4.428894 -4.4288864 -4.4288821 -4.4288812 -4.4288797 -4.428864 -4.4288483 -4.428853 -4.4288721 -4.4288993 -4.4289136 -4.4289088 -4.4288979 -4.4288907][-4.4288788 -4.4288554 -4.4288392 -4.4288321 -4.4288287 -4.4288158 -4.4287829 -4.4287653 -4.428793 -4.42884 -4.4288936 -4.4289222 -4.428925 -4.4289193 -4.4289131][-4.4288325 -4.4288077 -4.428782 -4.4287562 -4.4287291 -4.4286819 -4.4286089 -4.4285846 -4.4286561 -4.4287462 -4.4288306 -4.428875 -4.4288821 -4.4288859 -4.4288878][-4.4287858 -4.4287663 -4.4287314 -4.4286842 -4.4286261 -4.428525 -4.42838 -4.4283376 -4.428463 -4.4286013 -4.4287105 -4.4287715 -4.4287891 -4.4288044 -4.4288139][-4.4287519 -4.4287472 -4.4287138 -4.4286551 -4.4285822 -4.4284592 -4.4282823 -4.4282289 -4.4283705 -4.4285116 -4.428617 -4.428678 -4.4286976 -4.4287176 -4.4287367][-4.42876 -4.4287691 -4.4287438 -4.4286909 -4.4286242 -4.4285245 -4.4283977 -4.4283786 -4.4284773 -4.4285641 -4.4286213 -4.4286571 -4.4286647 -4.428679 -4.4287081][-4.4288058 -4.4288177 -4.4287996 -4.42876 -4.42871 -4.4286389 -4.4285693 -4.4285812 -4.4286456 -4.4286857 -4.4287062 -4.428719 -4.4287124 -4.4287186 -4.4287481][-4.428863 -4.4288669 -4.4288516 -4.4288282 -4.4287958 -4.4287558 -4.4287276 -4.4287519 -4.4287968 -4.42881 -4.4288087 -4.4288058 -4.428793 -4.42879 -4.4288125][-4.4289041 -4.4289002 -4.4288874 -4.4288759 -4.4288616 -4.4288478 -4.4288454 -4.4288716 -4.4289021 -4.4289012 -4.4288864 -4.428875 -4.4288597 -4.4288526 -4.4288692][-4.4289184 -4.4289155 -4.4289064 -4.4289007 -4.4288983 -4.4289012 -4.4289079 -4.42893 -4.42895 -4.4289422 -4.4289241 -4.4289141 -4.4289002 -4.4288931 -4.428905][-4.4289126 -4.4289088 -4.4289026 -4.4289002 -4.428905 -4.4289136 -4.42892 -4.4289327 -4.4289412 -4.4289317 -4.4289193 -4.4289155 -4.4289107 -4.4289079 -4.4289184][-4.428896 -4.4288912 -4.4288869 -4.428885 -4.4288907 -4.4289017 -4.4289064 -4.4289103 -4.42891 -4.4289021 -4.4288974 -4.4289002 -4.4289036 -4.4289088 -4.4289203][-4.4288807 -4.4288731 -4.4288683 -4.428864 -4.4288707 -4.4288855 -4.4288945 -4.4288969 -4.428895 -4.4288888 -4.428885 -4.4288874 -4.4288931 -4.4289036 -4.4289179]]...]
INFO - root - 2017-12-08 06:26:39.872375: step 24910, loss = 2.28, batch loss = 2.23 (36.0 examples/sec; 0.223 sec/batch; 19h:00m:39s remains)
INFO - root - 2017-12-08 06:26:42.103491: step 24920, loss = 2.28, batch loss = 2.23 (35.6 examples/sec; 0.225 sec/batch; 19h:11m:29s remains)
INFO - root - 2017-12-08 06:26:44.375324: step 24930, loss = 2.28, batch loss = 2.23 (33.9 examples/sec; 0.236 sec/batch; 20h:10m:34s remains)
INFO - root - 2017-12-08 06:26:46.615199: step 24940, loss = 2.28, batch loss = 2.23 (35.7 examples/sec; 0.224 sec/batch; 19h:08m:43s remains)
INFO - root - 2017-12-08 06:26:48.838355: step 24950, loss = 2.28, batch loss = 2.23 (34.3 examples/sec; 0.233 sec/batch; 19h:55m:14s remains)
INFO - root - 2017-12-08 06:26:51.074835: step 24960, loss = 2.28, batch loss = 2.23 (34.3 examples/sec; 0.233 sec/batch; 19h:54m:37s remains)
INFO - root - 2017-12-08 06:26:53.313135: step 24970, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 18h:41m:59s remains)
INFO - root - 2017-12-08 06:26:55.524691: step 24980, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 18h:45m:22s remains)
INFO - root - 2017-12-08 06:26:57.752140: step 24990, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 18h:53m:12s remains)
INFO - root - 2017-12-08 06:26:59.990876: step 25000, loss = 2.28, batch loss = 2.23 (35.8 examples/sec; 0.223 sec/batch; 19h:03m:50s remains)
2017-12-08 06:27:00.273150: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288754 -4.4288759 -4.4288859 -4.4288964 -4.4289026 -4.4289064 -4.4289141 -4.4289241 -4.4289241 -4.428915 -4.4289074 -4.4288855 -4.4288278 -4.4287429 -4.4286594][-4.4288778 -4.428875 -4.428884 -4.4288926 -4.4288945 -4.4288945 -4.4289026 -4.4289136 -4.4289188 -4.428916 -4.4289074 -4.42888 -4.4288077 -4.4287033 -4.4286008][-4.4288759 -4.428864 -4.4288597 -4.4288521 -4.4288383 -4.4288311 -4.4288349 -4.4288411 -4.4288568 -4.4288764 -4.4288778 -4.428854 -4.4287877 -4.428689 -4.428587][-4.4288735 -4.4288425 -4.4288158 -4.428782 -4.4287415 -4.4287133 -4.4287014 -4.4286947 -4.4287295 -4.4287915 -4.4288273 -4.4288244 -4.4287791 -4.4286952 -4.4285984][-4.4288678 -4.4288158 -4.42877 -4.4287114 -4.4286442 -4.4285903 -4.4285445 -4.4285097 -4.4285746 -4.4286966 -4.4287758 -4.4287944 -4.4287658 -4.4286952 -4.4286017][-4.4288607 -4.428793 -4.4287229 -4.428637 -4.428544 -4.42845 -4.4283409 -4.4282708 -4.4283867 -4.4285827 -4.4287062 -4.4287443 -4.4287257 -4.428659 -4.4285684][-4.4288449 -4.4287629 -4.4286695 -4.4285636 -4.4284434 -4.4282994 -4.4280977 -4.4279695 -4.4281592 -4.4284439 -4.4286065 -4.4286618 -4.4286566 -4.4285989 -4.4285278][-4.4288125 -4.4287276 -4.4286246 -4.4285164 -4.4283853 -4.4282088 -4.4279261 -4.427732 -4.4279819 -4.4283175 -4.4285069 -4.4285893 -4.4286056 -4.4285731 -4.42854][-4.4287734 -4.4287071 -4.4286203 -4.4285493 -4.4284668 -4.4283328 -4.4280949 -4.4279318 -4.428124 -4.4283872 -4.4285359 -4.4286113 -4.4286284 -4.4286165 -4.4286156][-4.4287453 -4.4287033 -4.4286466 -4.428617 -4.4285975 -4.42853 -4.4283848 -4.4282894 -4.4284039 -4.4285674 -4.4286485 -4.428688 -4.4286866 -4.4286833 -4.4286938][-4.4287167 -4.4286909 -4.4286652 -4.4286776 -4.4287062 -4.4286861 -4.4286103 -4.4285583 -4.4286156 -4.4287043 -4.4287453 -4.4287605 -4.4287567 -4.428762 -4.4287715][-4.4286866 -4.4286661 -4.4286671 -4.4287105 -4.4287572 -4.4287686 -4.42875 -4.4287295 -4.4287534 -4.4287977 -4.4288163 -4.4288139 -4.4288135 -4.428823 -4.4288292][-4.4287004 -4.4286776 -4.4286938 -4.4287486 -4.4287939 -4.4288225 -4.4288416 -4.4288483 -4.428864 -4.4288874 -4.4288945 -4.4288869 -4.4288836 -4.4288855 -4.4288778][-4.4287648 -4.4287314 -4.4287386 -4.4287848 -4.4288254 -4.4288626 -4.4289012 -4.4289265 -4.4289432 -4.4289532 -4.4289522 -4.4289412 -4.4289293 -4.4289188 -4.4289036][-4.4288259 -4.4287958 -4.4287992 -4.4288373 -4.4288754 -4.42891 -4.4289441 -4.4289684 -4.4289732 -4.428967 -4.4289575 -4.428946 -4.4289351 -4.4289265 -4.42892]]...]
INFO - root - 2017-12-08 06:27:02.516214: step 25010, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.225 sec/batch; 19h:14m:40s remains)
INFO - root - 2017-12-08 06:27:04.784144: step 25020, loss = 2.28, batch loss = 2.23 (36.7 examples/sec; 0.218 sec/batch; 18h:37m:05s remains)
INFO - root - 2017-12-08 06:27:07.034233: step 25030, loss = 2.28, batch loss = 2.23 (35.2 examples/sec; 0.228 sec/batch; 19h:26m:03s remains)
INFO - root - 2017-12-08 06:27:09.267303: step 25040, loss = 2.28, batch loss = 2.23 (36.7 examples/sec; 0.218 sec/batch; 18h:35m:59s remains)
INFO - root - 2017-12-08 06:27:11.518893: step 25050, loss = 2.28, batch loss = 2.23 (35.3 examples/sec; 0.227 sec/batch; 19h:21m:04s remains)
INFO - root - 2017-12-08 06:27:13.798724: step 25060, loss = 2.28, batch loss = 2.23 (34.4 examples/sec; 0.233 sec/batch; 19h:52m:47s remains)
INFO - root - 2017-12-08 06:27:16.027767: step 25070, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 19h:02m:03s remains)
INFO - root - 2017-12-08 06:27:18.266677: step 25080, loss = 2.28, batch loss = 2.23 (35.8 examples/sec; 0.223 sec/batch; 19h:04m:53s remains)
INFO - root - 2017-12-08 06:27:20.506910: step 25090, loss = 2.28, batch loss = 2.23 (36.0 examples/sec; 0.222 sec/batch; 18h:59m:01s remains)
INFO - root - 2017-12-08 06:27:22.750589: step 25100, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.221 sec/batch; 18h:54m:03s remains)
2017-12-08 06:27:23.048902: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288068 -4.4288082 -4.4287863 -4.4287629 -4.4287462 -4.4287024 -4.4286242 -4.428596 -4.4286828 -4.4287715 -4.4288306 -4.4288588 -4.4288783 -4.4289002 -4.4289207][-4.4288116 -4.4288077 -4.4287777 -4.4287486 -4.4287286 -4.4286804 -4.4285965 -4.4285684 -4.4286609 -4.4287553 -4.4288211 -4.4288535 -4.4288769 -4.4289007 -4.4289193][-4.4288158 -4.4288077 -4.4287748 -4.4287462 -4.42873 -4.4286857 -4.4286056 -4.4285769 -4.4286656 -4.4287586 -4.4288259 -4.4288568 -4.4288793 -4.4289017 -4.4289165][-4.4288211 -4.4288149 -4.4287839 -4.4287591 -4.4287496 -4.4287119 -4.4286351 -4.4286017 -4.4286809 -4.42877 -4.4288349 -4.4288626 -4.4288816 -4.4289017 -4.4289136][-4.4288158 -4.4288116 -4.4287825 -4.4287615 -4.4287562 -4.428721 -4.4286408 -4.4286017 -4.4286757 -4.4287648 -4.4288316 -4.42886 -4.4288797 -4.4289002 -4.42891][-4.428803 -4.4287968 -4.4287658 -4.4287424 -4.4287353 -4.4286923 -4.4286 -4.4285555 -4.4286351 -4.4287357 -4.4288106 -4.4288454 -4.4288697 -4.4288926 -4.4289021][-4.4287891 -4.4287744 -4.4287333 -4.428699 -4.4286838 -4.4286251 -4.4285154 -4.4284706 -4.4285717 -4.4286928 -4.4287815 -4.4288239 -4.4288554 -4.4288831 -4.428896][-4.4287786 -4.428751 -4.4286966 -4.4286509 -4.4286265 -4.4285545 -4.4284329 -4.4283948 -4.4285235 -4.4286652 -4.4287643 -4.4288096 -4.4288421 -4.4288678 -4.42888][-4.4287724 -4.4287343 -4.4286742 -4.428627 -4.4286036 -4.4285316 -4.4284148 -4.4283905 -4.4285312 -4.4286747 -4.4287682 -4.4288011 -4.4288216 -4.4288406 -4.4288554][-4.42876 -4.4287248 -4.4286761 -4.4286447 -4.4286346 -4.4285784 -4.4284816 -4.4284668 -4.4285884 -4.4287076 -4.4287796 -4.4287977 -4.4288077 -4.4288225 -4.42884][-4.4287496 -4.4287267 -4.4286952 -4.4286814 -4.428689 -4.4286561 -4.4285812 -4.428566 -4.4286537 -4.4287405 -4.428791 -4.4288 -4.4288058 -4.4288158 -4.4288297][-4.4287386 -4.4287281 -4.4287119 -4.4287119 -4.4287305 -4.4287133 -4.4286509 -4.428627 -4.4286838 -4.4287467 -4.4287829 -4.4287906 -4.4287987 -4.4288049 -4.4288125][-4.428719 -4.4287176 -4.4287109 -4.4287176 -4.4287395 -4.4287286 -4.4286675 -4.4286289 -4.4286675 -4.4287224 -4.4287577 -4.4287729 -4.4287858 -4.4287906 -4.428793][-4.4286933 -4.4287014 -4.4287047 -4.4287167 -4.4287367 -4.4287248 -4.4286623 -4.4286084 -4.4286366 -4.4286914 -4.4287295 -4.4287529 -4.428771 -4.42878 -4.4287844][-4.4286718 -4.4286933 -4.4287086 -4.4287248 -4.4287405 -4.4287224 -4.4286547 -4.4285941 -4.428616 -4.4286695 -4.42871 -4.4287381 -4.4287605 -4.4287767 -4.42879]]...]
INFO - root - 2017-12-08 06:27:25.335980: step 25110, loss = 2.28, batch loss = 2.23 (35.6 examples/sec; 0.225 sec/batch; 19h:11m:59s remains)
INFO - root - 2017-12-08 06:27:27.584696: step 25120, loss = 2.28, batch loss = 2.23 (34.9 examples/sec; 0.230 sec/batch; 19h:35m:58s remains)
INFO - root - 2017-12-08 06:27:29.852949: step 25130, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 18h:52m:42s remains)
INFO - root - 2017-12-08 06:27:32.086506: step 25140, loss = 2.28, batch loss = 2.23 (36.8 examples/sec; 0.217 sec/batch; 18h:33m:48s remains)
INFO - root - 2017-12-08 06:27:34.337426: step 25150, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.221 sec/batch; 18h:50m:02s remains)
INFO - root - 2017-12-08 06:27:36.616709: step 25160, loss = 2.28, batch loss = 2.23 (35.2 examples/sec; 0.227 sec/batch; 19h:24m:55s remains)
INFO - root - 2017-12-08 06:27:38.913578: step 25170, loss = 2.28, batch loss = 2.23 (31.7 examples/sec; 0.252 sec/batch; 21h:32m:12s remains)
INFO - root - 2017-12-08 06:27:41.174306: step 25180, loss = 2.28, batch loss = 2.23 (35.4 examples/sec; 0.226 sec/batch; 19h:16m:19s remains)
INFO - root - 2017-12-08 06:27:43.435576: step 25190, loss = 2.28, batch loss = 2.23 (35.4 examples/sec; 0.226 sec/batch; 19h:18m:01s remains)
INFO - root - 2017-12-08 06:27:45.691311: step 25200, loss = 2.28, batch loss = 2.23 (36.7 examples/sec; 0.218 sec/batch; 18h:35m:52s remains)
2017-12-08 06:27:46.000456: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288774 -4.428853 -4.4288297 -4.4288034 -4.428772 -4.428719 -4.4286766 -4.4286976 -4.4287419 -4.4287562 -4.4287486 -4.4287653 -4.4287844 -4.4287906 -4.428782][-4.4288754 -4.4288568 -4.4288363 -4.4288011 -4.4287534 -4.4286776 -4.4286194 -4.4286432 -4.4287 -4.4287319 -4.428731 -4.4287457 -4.4287553 -4.4287558 -4.4287472][-4.4288731 -4.4288483 -4.4288197 -4.4287791 -4.4287286 -4.4286342 -4.4285507 -4.4285607 -4.42863 -4.4286852 -4.4286966 -4.4287162 -4.4287376 -4.4287424 -4.4287338][-4.4288678 -4.4288287 -4.4287915 -4.4287562 -4.4287248 -4.4286284 -4.4285293 -4.4285235 -4.4285946 -4.4286566 -4.4286757 -4.4287076 -4.428751 -4.42876 -4.4287443][-4.4288645 -4.4288192 -4.4287844 -4.4287648 -4.4287548 -4.4286723 -4.4285655 -4.4285412 -4.428606 -4.4286613 -4.4286871 -4.4287338 -4.4287858 -4.4287887 -4.428761][-4.42886 -4.4288187 -4.4287858 -4.4287739 -4.4287739 -4.4286981 -4.4285769 -4.4285197 -4.428587 -4.428668 -4.4287124 -4.4287639 -4.4288039 -4.4287891 -4.4287424][-4.4288511 -4.4288011 -4.4287562 -4.4287386 -4.4287281 -4.4286342 -4.4284563 -4.4283361 -4.4284344 -4.4285951 -4.4286742 -4.4287238 -4.4287524 -4.4287243 -4.4286728][-4.428844 -4.4287863 -4.4287338 -4.4287043 -4.42867 -4.4285417 -4.428287 -4.4280882 -4.428246 -4.4284968 -4.4286084 -4.4286456 -4.4286532 -4.4286089 -4.4285445][-4.4288588 -4.4288054 -4.4287505 -4.4287047 -4.4286442 -4.4285121 -4.4282727 -4.4280987 -4.4282651 -4.4285 -4.4285927 -4.4285965 -4.4285731 -4.428504 -4.4284258][-4.4288898 -4.42885 -4.4287996 -4.4287496 -4.4286857 -4.4285865 -4.4284239 -4.4283228 -4.428431 -4.4285722 -4.4286132 -4.4285903 -4.4285536 -4.428484 -4.4284153][-4.4289155 -4.4288874 -4.4288473 -4.4288025 -4.4287477 -4.428678 -4.4285746 -4.428515 -4.4285731 -4.4286385 -4.4286413 -4.42862 -4.4286003 -4.4285579 -4.4285164][-4.4289241 -4.4288988 -4.4288669 -4.4288354 -4.4287953 -4.4287453 -4.4286814 -4.4286466 -4.4286714 -4.42869 -4.4286785 -4.4286761 -4.4286752 -4.4286575 -4.4286308][-4.4289274 -4.4289055 -4.4288821 -4.4288626 -4.4288359 -4.4287977 -4.4287605 -4.4287438 -4.428751 -4.4287491 -4.4287376 -4.4287486 -4.4287496 -4.42873 -4.4287071][-4.4289293 -4.4289117 -4.4289 -4.4288917 -4.428875 -4.4288459 -4.4288211 -4.4288125 -4.4288154 -4.428803 -4.4287925 -4.4288011 -4.428793 -4.4287634 -4.4287386][-4.4289279 -4.4289174 -4.4289141 -4.4289174 -4.4289093 -4.4288859 -4.4288621 -4.4288559 -4.4288611 -4.4288425 -4.4288282 -4.4288244 -4.4288082 -4.428772 -4.4287424]]...]
INFO - root - 2017-12-08 06:27:48.220908: step 25210, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.222 sec/batch; 18h:55m:51s remains)
INFO - root - 2017-12-08 06:27:50.443403: step 25220, loss = 2.28, batch loss = 2.23 (36.0 examples/sec; 0.222 sec/batch; 18h:57m:13s remains)
INFO - root - 2017-12-08 06:27:52.691811: step 25230, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 18h:41m:41s remains)
INFO - root - 2017-12-08 06:27:54.994384: step 25240, loss = 2.28, batch loss = 2.23 (34.8 examples/sec; 0.230 sec/batch; 19h:38m:38s remains)
INFO - root - 2017-12-08 06:27:57.224867: step 25250, loss = 2.28, batch loss = 2.23 (35.7 examples/sec; 0.224 sec/batch; 19h:08m:53s remains)
INFO - root - 2017-12-08 06:27:59.454042: step 25260, loss = 2.28, batch loss = 2.23 (33.7 examples/sec; 0.238 sec/batch; 20h:16m:38s remains)
INFO - root - 2017-12-08 06:28:01.730651: step 25270, loss = 2.28, batch loss = 2.23 (35.8 examples/sec; 0.223 sec/batch; 19h:03m:12s remains)
INFO - root - 2017-12-08 06:28:03.953136: step 25280, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 18h:59m:36s remains)
INFO - root - 2017-12-08 06:28:06.200194: step 25290, loss = 2.28, batch loss = 2.23 (35.6 examples/sec; 0.225 sec/batch; 19h:09m:49s remains)
INFO - root - 2017-12-08 06:28:08.459988: step 25300, loss = 2.28, batch loss = 2.23 (35.7 examples/sec; 0.224 sec/batch; 19h:06m:30s remains)
2017-12-08 06:28:08.757366: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289761 -4.4289942 -4.4290128 -4.4290223 -4.4290166 -4.4289851 -4.4289379 -4.4288597 -4.4287682 -4.4286814 -4.4286 -4.4285831 -4.4286065 -4.4286156 -4.4285626][-4.429 -4.42903 -4.4290514 -4.4290514 -4.4290156 -4.4289474 -4.4288621 -4.428761 -4.42867 -4.4285984 -4.4285388 -4.4285426 -4.4285874 -4.4286284 -4.4286284][-4.429008 -4.4290504 -4.4290752 -4.4290624 -4.4289875 -4.4288692 -4.4287338 -4.4286122 -4.4285393 -4.4285126 -4.4285035 -4.4285312 -4.4285932 -4.4286504 -4.4286785][-4.4289994 -4.4290423 -4.4290628 -4.4290271 -4.4289088 -4.4287405 -4.4285641 -4.4284434 -4.4284296 -4.42849 -4.4285545 -4.4286 -4.4286547 -4.4286966 -4.4287171][-4.4290013 -4.4290361 -4.4290471 -4.4289837 -4.4288187 -4.4286008 -4.4283848 -4.428278 -4.4283404 -4.428504 -4.4286389 -4.4287033 -4.4287586 -4.428781 -4.4287777][-4.4290156 -4.4290395 -4.4290371 -4.4289412 -4.4287271 -4.4284444 -4.4281712 -4.4280844 -4.4282441 -4.4285059 -4.4286923 -4.4287782 -4.428843 -4.4288645 -4.4288526][-4.4290104 -4.4290247 -4.4290104 -4.4288859 -4.4286208 -4.428256 -4.4279079 -4.4278431 -4.4281077 -4.4284558 -4.428688 -4.4288054 -4.4288855 -4.4289184 -4.428916][-4.4290071 -4.4290128 -4.428987 -4.4288406 -4.4285398 -4.4281135 -4.4277096 -4.4276786 -4.4280295 -4.4284253 -4.4286819 -4.4288254 -4.4289126 -4.4289513 -4.4289627][-4.4290137 -4.4290094 -4.4289746 -4.4288297 -4.4285483 -4.4281554 -4.4278016 -4.4277978 -4.4281278 -4.4284844 -4.4287219 -4.4288626 -4.4289489 -4.4289851 -4.4290013][-4.4290094 -4.4289889 -4.4289384 -4.4288068 -4.4285917 -4.4283094 -4.4280891 -4.4281173 -4.4283638 -4.428627 -4.4288054 -4.4289136 -4.4289818 -4.4290075 -4.429019][-4.4289846 -4.4289393 -4.4288607 -4.42874 -4.4285913 -4.4284163 -4.4283237 -4.4283996 -4.4285831 -4.4287682 -4.4288912 -4.4289646 -4.429008 -4.4290175 -4.4290175][-4.4289546 -4.4288816 -4.4287815 -4.4286785 -4.4285827 -4.42849 -4.42849 -4.4286008 -4.4287496 -4.4288826 -4.4289651 -4.4290156 -4.4290361 -4.4290285 -4.4290156][-4.4289412 -4.4288526 -4.4287453 -4.4286613 -4.4286022 -4.4285684 -4.4286237 -4.4287505 -4.4288745 -4.4289665 -4.4290233 -4.4290538 -4.4290552 -4.429038 -4.4290195][-4.4289341 -4.42884 -4.4287343 -4.4286628 -4.4286375 -4.4286542 -4.42874 -4.4288564 -4.4289513 -4.4290161 -4.4290538 -4.4290648 -4.4290524 -4.4290304 -4.4290113][-4.4289289 -4.4288421 -4.4287467 -4.428688 -4.4286909 -4.428741 -4.4288273 -4.4289155 -4.4289837 -4.4290304 -4.4290547 -4.4290538 -4.4290376 -4.4290166 -4.4289994]]...]
INFO - root - 2017-12-08 06:28:10.998015: step 25310, loss = 2.28, batch loss = 2.23 (33.1 examples/sec; 0.242 sec/batch; 20h:37m:10s remains)
INFO - root - 2017-12-08 06:28:13.222823: step 25320, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 18h:40m:37s remains)
INFO - root - 2017-12-08 06:28:15.460335: step 25330, loss = 2.28, batch loss = 2.23 (35.3 examples/sec; 0.226 sec/batch; 19h:18m:50s remains)
INFO - root - 2017-12-08 06:28:17.715314: step 25340, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.220 sec/batch; 18h:48m:42s remains)
INFO - root - 2017-12-08 06:28:19.968064: step 25350, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.220 sec/batch; 18h:48m:31s remains)
INFO - root - 2017-12-08 06:28:22.199386: step 25360, loss = 2.28, batch loss = 2.23 (35.8 examples/sec; 0.224 sec/batch; 19h:04m:50s remains)
INFO - root - 2017-12-08 06:28:24.497812: step 25370, loss = 2.28, batch loss = 2.23 (35.4 examples/sec; 0.226 sec/batch; 19h:15m:59s remains)
INFO - root - 2017-12-08 06:28:26.715133: step 25380, loss = 2.28, batch loss = 2.23 (37.8 examples/sec; 0.212 sec/batch; 18h:04m:13s remains)
INFO - root - 2017-12-08 06:28:28.945929: step 25390, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 18h:51m:08s remains)
INFO - root - 2017-12-08 06:28:31.178679: step 25400, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.221 sec/batch; 18h:48m:53s remains)
2017-12-08 06:28:31.472490: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288254 -4.4287877 -4.4287624 -4.4287505 -4.4287519 -4.4287467 -4.4287062 -4.4286728 -4.4286723 -4.4287024 -4.4287667 -4.4288325 -4.4288845 -4.4289255 -4.4289565][-4.4287648 -4.4287162 -4.4286861 -4.4286647 -4.4286666 -4.4286633 -4.4286008 -4.428544 -4.4285383 -4.4285822 -4.428668 -4.4287615 -4.4288387 -4.4288974 -4.4289412][-4.4287057 -4.4286551 -4.4286337 -4.4286094 -4.4286084 -4.4286022 -4.4285121 -4.4284286 -4.4284272 -4.4284873 -4.4285841 -4.42869 -4.4287882 -4.428865 -4.4289217][-4.4286327 -4.4285841 -4.4285741 -4.4285421 -4.4285283 -4.4285059 -4.42839 -4.4282856 -4.4283009 -4.4283972 -4.4285078 -4.4286203 -4.4287372 -4.428833 -4.428905][-4.4285641 -4.4285178 -4.4285111 -4.4284697 -4.4284453 -4.4283977 -4.4282594 -4.4281297 -4.4281635 -4.4283128 -4.4284511 -4.4285679 -4.4286981 -4.4288087 -4.4288945][-4.4285264 -4.4284806 -4.4284687 -4.428422 -4.4283891 -4.4283133 -4.4281526 -4.4279842 -4.42802 -4.4282246 -4.4284077 -4.4285383 -4.42868 -4.4287968 -4.4288898][-4.4284925 -4.4284606 -4.4284406 -4.4283915 -4.4283681 -4.4282827 -4.4281073 -4.4278946 -4.427907 -4.4281545 -4.4283733 -4.4285183 -4.4286685 -4.4287872 -4.4288859][-4.4285054 -4.4285054 -4.4284911 -4.4284482 -4.4284492 -4.4283834 -4.4282126 -4.4279971 -4.4279857 -4.4282012 -4.4284024 -4.4285321 -4.4286728 -4.4287839 -4.4288831][-4.4285975 -4.4286275 -4.4286327 -4.4286027 -4.4286141 -4.4285693 -4.4284096 -4.4282146 -4.4281893 -4.4283319 -4.4284816 -4.4285827 -4.4286966 -4.42879 -4.4288812][-4.42871 -4.4287381 -4.4287486 -4.4287348 -4.4287467 -4.4287128 -4.4285703 -4.4284005 -4.4283714 -4.4284639 -4.4285703 -4.4286461 -4.4287319 -4.4288044 -4.4288831][-4.4288158 -4.4288287 -4.4288383 -4.4288344 -4.428843 -4.4288106 -4.4286909 -4.4285455 -4.4285159 -4.4285769 -4.4286523 -4.4287095 -4.4287724 -4.4288306 -4.4288974][-4.4288836 -4.4288912 -4.428906 -4.4289126 -4.4289169 -4.4288893 -4.4287996 -4.4286747 -4.4286361 -4.4286752 -4.4287295 -4.4287705 -4.4288192 -4.4288692 -4.4289231][-4.4289169 -4.4289188 -4.4289384 -4.428956 -4.4289637 -4.4289446 -4.4288855 -4.4287872 -4.4287448 -4.4287672 -4.4288073 -4.4288368 -4.4288759 -4.428916 -4.4289541][-4.4289436 -4.4289451 -4.428966 -4.428988 -4.428997 -4.4289851 -4.4289451 -4.4288769 -4.4288411 -4.4288516 -4.4288826 -4.4289017 -4.4289279 -4.4289556 -4.4289804][-4.42897 -4.4289713 -4.4289865 -4.4290018 -4.4290071 -4.4290013 -4.4289765 -4.4289322 -4.4289036 -4.4289088 -4.4289322 -4.4289455 -4.42896 -4.428978 -4.4289942]]...]
INFO - root - 2017-12-08 06:28:33.703373: step 25410, loss = 2.28, batch loss = 2.23 (37.0 examples/sec; 0.216 sec/batch; 18h:25m:37s remains)
INFO - root - 2017-12-08 06:28:35.967567: step 25420, loss = 2.28, batch loss = 2.23 (34.9 examples/sec; 0.229 sec/batch; 19h:32m:43s remains)
INFO - root - 2017-12-08 06:28:38.216543: step 25430, loss = 2.28, batch loss = 2.23 (35.2 examples/sec; 0.227 sec/batch; 19h:23m:58s remains)
INFO - root - 2017-12-08 06:28:40.460972: step 25440, loss = 2.28, batch loss = 2.23 (35.6 examples/sec; 0.225 sec/batch; 19h:10m:49s remains)
INFO - root - 2017-12-08 06:28:42.705878: step 25450, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 18h:41m:24s remains)
INFO - root - 2017-12-08 06:28:44.973005: step 25460, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.221 sec/batch; 18h:48m:47s remains)
INFO - root - 2017-12-08 06:28:47.206644: step 25470, loss = 2.28, batch loss = 2.23 (36.0 examples/sec; 0.222 sec/batch; 18h:58m:06s remains)
INFO - root - 2017-12-08 06:28:49.439478: step 25480, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.222 sec/batch; 18h:54m:18s remains)
INFO - root - 2017-12-08 06:28:51.688557: step 25490, loss = 2.28, batch loss = 2.23 (34.6 examples/sec; 0.231 sec/batch; 19h:42m:39s remains)
INFO - root - 2017-12-08 06:28:53.929931: step 25500, loss = 2.28, batch loss = 2.23 (35.8 examples/sec; 0.224 sec/batch; 19h:03m:49s remains)
2017-12-08 06:28:54.225263: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289742 -4.4289846 -4.4289794 -4.4289546 -4.4289255 -4.4289169 -4.428936 -4.428956 -4.428967 -4.4289713 -4.4289665 -4.4289517 -4.4289317 -4.4289217 -4.4289327][-4.4289641 -4.4289737 -4.428967 -4.42894 -4.4289026 -4.4288874 -4.4289136 -4.4289436 -4.4289594 -4.4289627 -4.4289489 -4.4289222 -4.4288955 -4.42889 -4.4289141][-4.4289365 -4.4289455 -4.4289393 -4.4289069 -4.4288554 -4.4288268 -4.4288616 -4.4289041 -4.4289279 -4.4289322 -4.42891 -4.4288721 -4.428843 -4.4288492 -4.4288883][-4.4289021 -4.4289083 -4.4289012 -4.4288611 -4.4287891 -4.428741 -4.428781 -4.4288383 -4.4288764 -4.4288855 -4.4288597 -4.4288177 -4.428791 -4.4288092 -4.4288559][-4.4288735 -4.4288745 -4.4288645 -4.4288173 -4.4287262 -4.4286585 -4.4287038 -4.4287748 -4.4288239 -4.4288411 -4.4288192 -4.4287806 -4.4287581 -4.4287839 -4.4288349][-4.4288454 -4.4288421 -4.4288282 -4.4287753 -4.4286723 -4.4285917 -4.4286432 -4.4287195 -4.4287658 -4.428782 -4.4287658 -4.4287362 -4.4287238 -4.4287562 -4.4288149][-4.4288311 -4.42882 -4.4287982 -4.4287353 -4.4286208 -4.428535 -4.428596 -4.428669 -4.4286985 -4.4287033 -4.4286904 -4.4286714 -4.42867 -4.4287138 -4.4287834][-4.4288206 -4.4288044 -4.42877 -4.4286928 -4.4285679 -4.4284854 -4.4285579 -4.4286275 -4.4286466 -4.4286432 -4.4286289 -4.4286103 -4.4286156 -4.4286685 -4.4287486][-4.4287968 -4.428781 -4.4287381 -4.4286523 -4.4285288 -4.4284592 -4.428546 -4.4286137 -4.4286222 -4.4286065 -4.42858 -4.4285545 -4.4285574 -4.428618 -4.4287066][-4.428782 -4.4287744 -4.4287267 -4.4286361 -4.4285178 -4.4284616 -4.4285612 -4.4286313 -4.4286375 -4.4286075 -4.4285569 -4.4285164 -4.4285088 -4.4285703 -4.4286656][-4.4287848 -4.4287858 -4.4287357 -4.4286442 -4.42853 -4.4284849 -4.428587 -4.4286604 -4.428669 -4.4286242 -4.4285545 -4.428503 -4.4284883 -4.4285479 -4.4286451][-4.428813 -4.428812 -4.42876 -4.4286675 -4.4285526 -4.4285083 -4.4286051 -4.4286833 -4.428699 -4.428648 -4.4285693 -4.4285169 -4.4285045 -4.4285641 -4.4286613][-4.4288507 -4.4288454 -4.4287939 -4.4286966 -4.4285831 -4.4285417 -4.4286351 -4.4287181 -4.4287362 -4.4286833 -4.4286089 -4.4285626 -4.42856 -4.4286218 -4.4287157][-4.4288793 -4.4288707 -4.4288254 -4.4287305 -4.4286251 -4.4285951 -4.4286871 -4.4287658 -4.4287782 -4.4287324 -4.4286723 -4.428637 -4.4286437 -4.4287014 -4.4287791][-4.4288898 -4.4288845 -4.4288483 -4.4287639 -4.4286771 -4.4286594 -4.4287405 -4.428803 -4.4288096 -4.4287782 -4.4287395 -4.4287162 -4.4287262 -4.42877 -4.4288254]]...]
INFO - root - 2017-12-08 06:28:56.456949: step 25510, loss = 2.28, batch loss = 2.23 (35.0 examples/sec; 0.228 sec/batch; 19h:28m:29s remains)
INFO - root - 2017-12-08 06:28:58.713322: step 25520, loss = 2.28, batch loss = 2.23 (34.8 examples/sec; 0.230 sec/batch; 19h:36m:26s remains)
INFO - root - 2017-12-08 06:29:00.982573: step 25530, loss = 2.28, batch loss = 2.23 (35.4 examples/sec; 0.226 sec/batch; 19h:15m:25s remains)
INFO - root - 2017-12-08 06:29:03.233255: step 25540, loss = 2.28, batch loss = 2.23 (35.7 examples/sec; 0.224 sec/batch; 19h:07m:27s remains)
INFO - root - 2017-12-08 06:29:05.496859: step 25550, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 19h:00m:26s remains)
INFO - root - 2017-12-08 06:29:07.717295: step 25560, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 18h:49m:21s remains)
INFO - root - 2017-12-08 06:29:09.979556: step 25570, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.222 sec/batch; 18h:53m:17s remains)
INFO - root - 2017-12-08 06:29:12.215491: step 25580, loss = 2.28, batch loss = 2.23 (35.8 examples/sec; 0.224 sec/batch; 19h:03m:22s remains)
INFO - root - 2017-12-08 06:29:14.445813: step 25590, loss = 2.28, batch loss = 2.23 (35.3 examples/sec; 0.226 sec/batch; 19h:17m:41s remains)
INFO - root - 2017-12-08 06:29:16.700974: step 25600, loss = 2.28, batch loss = 2.23 (33.9 examples/sec; 0.236 sec/batch; 20h:05m:19s remains)
2017-12-08 06:29:17.017258: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288383 -4.4288044 -4.4287982 -4.4287949 -4.428771 -4.428721 -4.428647 -4.4285655 -4.4285288 -4.4285846 -4.4287171 -4.428843 -4.4289284 -4.4289737 -4.4289913][-4.4288106 -4.4287682 -4.4287577 -4.428761 -4.4287505 -4.4287109 -4.4286427 -4.4285545 -4.4285126 -4.4285736 -4.4287076 -4.4288378 -4.4289279 -4.4289742 -4.4289923][-4.4287972 -4.4287515 -4.4287381 -4.4287419 -4.4287348 -4.4286971 -4.4286237 -4.428524 -4.428484 -4.428555 -4.4286938 -4.4288297 -4.4289265 -4.4289737 -4.4289904][-4.4288116 -4.4287629 -4.4287395 -4.4287319 -4.4287119 -4.4286666 -4.4285822 -4.4284744 -4.4284439 -4.4285326 -4.4286819 -4.4288254 -4.4289289 -4.4289761 -4.42899][-4.4288483 -4.4288006 -4.4287672 -4.428741 -4.4286957 -4.4286265 -4.4285245 -4.4284134 -4.4283972 -4.4285064 -4.4286671 -4.4288187 -4.4289284 -4.4289775 -4.4289908][-4.428894 -4.4288559 -4.42882 -4.4287796 -4.4287062 -4.4286046 -4.4284811 -4.4283695 -4.4283633 -4.4284854 -4.4286561 -4.428812 -4.4289255 -4.428977 -4.4289923][-4.4289384 -4.4289141 -4.4288874 -4.4288459 -4.4287562 -4.4286284 -4.428484 -4.4283662 -4.4283533 -4.4284744 -4.4286485 -4.4288058 -4.4289217 -4.4289751 -4.4289927][-4.4289708 -4.4289646 -4.4289527 -4.4289222 -4.4288359 -4.4286952 -4.4285312 -4.4283967 -4.4283595 -4.4284682 -4.4286394 -4.4288 -4.4289188 -4.4289737 -4.4289932][-4.4289865 -4.4289927 -4.4289942 -4.42898 -4.4289122 -4.4287763 -4.4285979 -4.4284358 -4.4283628 -4.4284515 -4.4286208 -4.4287887 -4.4289131 -4.4289703 -4.4289918][-4.428987 -4.4289975 -4.4290066 -4.4290075 -4.4289622 -4.42884 -4.4286566 -4.4284692 -4.4283633 -4.4284306 -4.4285979 -4.4287748 -4.428905 -4.4289651 -4.4289889][-4.4289804 -4.4289923 -4.4290051 -4.4290175 -4.4289923 -4.4288878 -4.4287114 -4.4285131 -4.4283838 -4.4284267 -4.4285851 -4.4287667 -4.4289 -4.4289613 -4.4289865][-4.4289737 -4.4289851 -4.4289994 -4.4290175 -4.4290056 -4.4289222 -4.4287615 -4.4285655 -4.428422 -4.4284377 -4.4285831 -4.4287615 -4.428895 -4.4289603 -4.4289861][-4.4289675 -4.4289765 -4.4289885 -4.429009 -4.4290071 -4.4289465 -4.4288092 -4.4286256 -4.4284739 -4.4284644 -4.4285936 -4.4287643 -4.4288955 -4.4289613 -4.428987][-4.428967 -4.4289703 -4.4289761 -4.4289942 -4.4290023 -4.4289641 -4.4288535 -4.4286938 -4.4285522 -4.4285269 -4.4286313 -4.428782 -4.4289021 -4.4289656 -4.4289908][-4.4289637 -4.4289618 -4.4289627 -4.4289784 -4.4289932 -4.4289737 -4.4288859 -4.4287543 -4.4286346 -4.4286041 -4.4286819 -4.4288068 -4.4289112 -4.4289703 -4.4289956]]...]
INFO - root - 2017-12-08 06:29:19.265735: step 25610, loss = 2.28, batch loss = 2.23 (34.6 examples/sec; 0.231 sec/batch; 19h:43m:27s remains)
INFO - root - 2017-12-08 06:29:21.515923: step 25620, loss = 2.28, batch loss = 2.23 (35.8 examples/sec; 0.223 sec/batch; 19h:02m:36s remains)
INFO - root - 2017-12-08 06:29:23.777435: step 25630, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.220 sec/batch; 18h:46m:29s remains)
INFO - root - 2017-12-08 06:29:26.009395: step 25640, loss = 2.28, batch loss = 2.23 (36.8 examples/sec; 0.217 sec/batch; 18h:30m:20s remains)
INFO - root - 2017-12-08 06:29:28.216044: step 25650, loss = 2.28, batch loss = 2.23 (37.2 examples/sec; 0.215 sec/batch; 18h:19m:43s remains)
INFO - root - 2017-12-08 06:29:30.467125: step 25660, loss = 2.28, batch loss = 2.23 (35.0 examples/sec; 0.228 sec/batch; 19h:28m:03s remains)
INFO - root - 2017-12-08 06:29:32.694681: step 25670, loss = 2.28, batch loss = 2.23 (35.7 examples/sec; 0.224 sec/batch; 19h:06m:43s remains)
INFO - root - 2017-12-08 06:29:34.957663: step 25680, loss = 2.28, batch loss = 2.23 (35.0 examples/sec; 0.229 sec/batch; 19h:29m:17s remains)
INFO - root - 2017-12-08 06:29:37.188818: step 25690, loss = 2.28, batch loss = 2.23 (36.8 examples/sec; 0.218 sec/batch; 18h:33m:06s remains)
INFO - root - 2017-12-08 06:29:39.418815: step 25700, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 18h:40m:48s remains)
2017-12-08 06:29:39.687350: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289093 -4.4289007 -4.4288769 -4.4288554 -4.42886 -4.4288788 -4.4288974 -4.4289088 -4.4289045 -4.4288673 -4.4288 -4.4287128 -4.4286413 -4.4286256 -4.4286642][-4.428875 -4.4288626 -4.4288349 -4.4288049 -4.4287996 -4.4288092 -4.42883 -4.4288626 -4.428885 -4.4288731 -4.4288216 -4.4287415 -4.4286642 -4.4286327 -4.4286566][-4.4288387 -4.428812 -4.4287686 -4.428721 -4.4286962 -4.4286923 -4.4287167 -4.4287748 -4.4288287 -4.4288516 -4.4288359 -4.428782 -4.4287105 -4.428659 -4.4286528][-4.4287968 -4.4287496 -4.4286857 -4.4286213 -4.4285769 -4.4285526 -4.4285769 -4.4286613 -4.4287519 -4.4288144 -4.4288421 -4.4288192 -4.4287577 -4.4286866 -4.4286447][-4.4287424 -4.4286785 -4.4286041 -4.4285407 -4.4284868 -4.4284396 -4.428452 -4.4285488 -4.4286752 -4.4287744 -4.4288349 -4.428834 -4.4287748 -4.42869 -4.4286218][-4.4286828 -4.4285994 -4.4285212 -4.4284725 -4.4284244 -4.4283581 -4.4283423 -4.4284286 -4.4285827 -4.42872 -4.4288092 -4.4288268 -4.4287748 -4.4286909 -4.4286132][-4.4286489 -4.4285474 -4.4284687 -4.4284444 -4.4284167 -4.4283433 -4.4282889 -4.428339 -4.428493 -4.4286528 -4.4287672 -4.428812 -4.4287858 -4.4287243 -4.4286571][-4.4286728 -4.4285688 -4.4284987 -4.4284897 -4.4284778 -4.4284091 -4.428329 -4.4283385 -4.4284592 -4.4286118 -4.4287357 -4.4288034 -4.4288096 -4.4287782 -4.428731][-4.4287472 -4.4286609 -4.4286056 -4.4286013 -4.4285951 -4.428535 -4.42845 -4.4284334 -4.4285126 -4.4286346 -4.4287477 -4.42882 -4.4288473 -4.4288373 -4.4288058][-4.4288173 -4.4287591 -4.4287224 -4.4287252 -4.4287281 -4.4286804 -4.4286046 -4.4285789 -4.4286242 -4.4287081 -4.4287968 -4.4288573 -4.4288821 -4.4288783 -4.4288578][-4.4288397 -4.428812 -4.4287958 -4.4288077 -4.4288197 -4.4287872 -4.4287319 -4.4287095 -4.4287333 -4.4287863 -4.4288473 -4.428885 -4.4288969 -4.4288912 -4.4288759][-4.42881 -4.4288049 -4.4288082 -4.4288325 -4.4288526 -4.4288287 -4.4287887 -4.4287734 -4.4287872 -4.4288144 -4.4288473 -4.4288669 -4.4288721 -4.4288659 -4.4288535][-4.4287486 -4.4287558 -4.4287753 -4.4288092 -4.428833 -4.4288111 -4.4287739 -4.4287596 -4.4287686 -4.4287858 -4.428802 -4.42881 -4.4288139 -4.4288106 -4.4288025][-4.4287033 -4.4287157 -4.4287405 -4.4287739 -4.428792 -4.4287648 -4.4287248 -4.4287086 -4.4287167 -4.4287357 -4.42875 -4.4287548 -4.4287591 -4.4287634 -4.4287624][-4.4287295 -4.4287429 -4.4287634 -4.4287834 -4.4287882 -4.4287548 -4.4287095 -4.4286866 -4.4286909 -4.4287128 -4.4287338 -4.4287443 -4.4287548 -4.4287682 -4.4287744]]...]
INFO - root - 2017-12-08 06:29:41.942728: step 25710, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.221 sec/batch; 18h:52m:21s remains)
INFO - root - 2017-12-08 06:29:44.213453: step 25720, loss = 2.28, batch loss = 2.23 (36.7 examples/sec; 0.218 sec/batch; 18h:35m:04s remains)
INFO - root - 2017-12-08 06:29:46.489084: step 25730, loss = 2.28, batch loss = 2.23 (34.2 examples/sec; 0.234 sec/batch; 19h:56m:11s remains)
INFO - root - 2017-12-08 06:29:48.765933: step 25740, loss = 2.28, batch loss = 2.23 (32.0 examples/sec; 0.250 sec/batch; 21h:16m:36s remains)
INFO - root - 2017-12-08 06:29:50.984125: step 25750, loss = 2.28, batch loss = 2.23 (37.5 examples/sec; 0.213 sec/batch; 18h:10m:20s remains)
INFO - root - 2017-12-08 06:29:53.243793: step 25760, loss = 2.28, batch loss = 2.23 (35.7 examples/sec; 0.224 sec/batch; 19h:06m:52s remains)
INFO - root - 2017-12-08 06:29:55.482729: step 25770, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.222 sec/batch; 18h:53m:41s remains)
INFO - root - 2017-12-08 06:29:57.727284: step 25780, loss = 2.28, batch loss = 2.23 (35.1 examples/sec; 0.228 sec/batch; 19h:26m:40s remains)
INFO - root - 2017-12-08 06:29:59.938132: step 25790, loss = 2.28, batch loss = 2.23 (35.6 examples/sec; 0.225 sec/batch; 19h:08m:08s remains)
INFO - root - 2017-12-08 06:30:02.202829: step 25800, loss = 2.28, batch loss = 2.23 (34.1 examples/sec; 0.235 sec/batch; 20h:00m:34s remains)
2017-12-08 06:30:02.510186: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288 -4.4287496 -4.4287233 -4.4287276 -4.428771 -4.4288206 -4.4288435 -4.4288378 -4.4288135 -4.4287906 -4.4287739 -4.4287653 -4.4287558 -4.4287438 -4.4287391][-4.4287672 -4.4287109 -4.4286814 -4.428688 -4.4287338 -4.42878 -4.4287949 -4.4287815 -4.4287515 -4.4287267 -4.42871 -4.4286957 -4.4286761 -4.4286566 -4.4286518][-4.4287333 -4.4286637 -4.4286308 -4.42864 -4.4286823 -4.4287214 -4.4287376 -4.4287295 -4.428709 -4.4286962 -4.4286861 -4.4286709 -4.4286332 -4.4285994 -4.4285908][-4.4286947 -4.4286132 -4.4285803 -4.4286013 -4.4286504 -4.4286842 -4.4286938 -4.4286847 -4.4286761 -4.42868 -4.4286857 -4.4286723 -4.42862 -4.428565 -4.4285388][-4.4286346 -4.4285622 -4.4285507 -4.4285927 -4.4286427 -4.4286609 -4.4286466 -4.4286184 -4.4286127 -4.4286351 -4.42866 -4.4286528 -4.4285884 -4.4285121 -4.4284692][-4.4285793 -4.4285393 -4.4285541 -4.4286 -4.4286304 -4.4286189 -4.428566 -4.4285111 -4.4285083 -4.42855 -4.428597 -4.4286027 -4.4285383 -4.4284611 -4.4284258][-4.4285827 -4.4285741 -4.4285946 -4.4286189 -4.4286184 -4.4285636 -4.4284596 -4.42837 -4.4283719 -4.4284296 -4.4284964 -4.4285235 -4.4284854 -4.428442 -4.4284391][-4.4286103 -4.4286118 -4.42862 -4.4286165 -4.4285855 -4.4284782 -4.4282985 -4.4281573 -4.4281964 -4.4283018 -4.4284072 -4.4284806 -4.4284978 -4.4285083 -4.4285297][-4.4285965 -4.4286017 -4.4285975 -4.4285746 -4.4285154 -4.4283657 -4.4281406 -4.4280133 -4.4281368 -4.4283037 -4.4284458 -4.4285507 -4.4286003 -4.4286165 -4.428617][-4.42857 -4.4285808 -4.4285827 -4.4285541 -4.4284897 -4.4283638 -4.4282079 -4.4281712 -4.4282994 -4.4284382 -4.4285536 -4.4286361 -4.4286718 -4.4286633 -4.4286366][-4.4285922 -4.4286022 -4.4286022 -4.428576 -4.4285207 -4.4284353 -4.4283595 -4.428381 -4.4284811 -4.4285707 -4.4286337 -4.4286771 -4.4286928 -4.4286652 -4.4286156][-4.4286361 -4.4286189 -4.4285941 -4.4285622 -4.4285245 -4.42848 -4.4284654 -4.4285235 -4.4286137 -4.4286723 -4.4286971 -4.4287109 -4.42871 -4.4286695 -4.428606][-4.4286718 -4.4286337 -4.4285903 -4.4285569 -4.4285369 -4.4285293 -4.4285603 -4.428637 -4.428721 -4.4287605 -4.4287682 -4.4287682 -4.4287539 -4.4287033 -4.4286385][-4.4287324 -4.4286966 -4.4286575 -4.428627 -4.4286122 -4.4286237 -4.4286723 -4.4287429 -4.4288073 -4.4288335 -4.4288363 -4.4288363 -4.4288211 -4.4287791 -4.4287276][-4.4288168 -4.428793 -4.4287705 -4.4287472 -4.428731 -4.428741 -4.4287839 -4.4288354 -4.428875 -4.4288888 -4.4288936 -4.428895 -4.428884 -4.4288545 -4.4288173]]...]
INFO - root - 2017-12-08 06:30:04.762877: step 25810, loss = 2.28, batch loss = 2.23 (33.9 examples/sec; 0.236 sec/batch; 20h:07m:49s remains)
INFO - root - 2017-12-08 06:30:06.986015: step 25820, loss = 2.28, batch loss = 2.23 (36.6 examples/sec; 0.218 sec/batch; 18h:36m:36s remains)
INFO - root - 2017-12-08 06:30:09.250960: step 25830, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.225 sec/batch; 19h:11m:41s remains)
INFO - root - 2017-12-08 06:30:11.500713: step 25840, loss = 2.28, batch loss = 2.23 (34.6 examples/sec; 0.231 sec/batch; 19h:41m:03s remains)
INFO - root - 2017-12-08 06:30:13.773137: step 25850, loss = 2.28, batch loss = 2.23 (35.8 examples/sec; 0.223 sec/batch; 19h:02m:14s remains)
INFO - root - 2017-12-08 06:30:16.001006: step 25860, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 18h:44m:28s remains)
INFO - root - 2017-12-08 06:30:18.250267: step 25870, loss = 2.28, batch loss = 2.23 (35.2 examples/sec; 0.227 sec/batch; 19h:22m:19s remains)
INFO - root - 2017-12-08 06:30:20.489552: step 25880, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 18h:41m:00s remains)
INFO - root - 2017-12-08 06:30:22.753533: step 25890, loss = 2.28, batch loss = 2.23 (35.2 examples/sec; 0.228 sec/batch; 19h:22m:36s remains)
INFO - root - 2017-12-08 06:30:24.976788: step 25900, loss = 2.28, batch loss = 2.23 (34.5 examples/sec; 0.232 sec/batch; 19h:46m:11s remains)
2017-12-08 06:30:25.295644: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.428956 -4.4289308 -4.4289036 -4.4288793 -4.4288692 -4.4288754 -4.428885 -4.4288931 -4.4288993 -4.4289055 -4.4289103 -4.4289184 -4.4289293 -4.4289451 -4.4289637][-4.4289203 -4.4288812 -4.4288392 -4.4288049 -4.4287882 -4.4287882 -4.4287996 -4.428812 -4.428823 -4.4288297 -4.4288392 -4.4288483 -4.4288616 -4.4288845 -4.4289136][-4.4289112 -4.4288626 -4.428812 -4.4287686 -4.4287376 -4.4287233 -4.4287305 -4.4287405 -4.4287467 -4.4287457 -4.4287558 -4.4287643 -4.4287786 -4.4288096 -4.4288492][-4.4289112 -4.428863 -4.4288154 -4.4287682 -4.42872 -4.4286933 -4.4286952 -4.4286933 -4.4286838 -4.4286675 -4.4286661 -4.4286718 -4.4286833 -4.4287181 -4.4287639][-4.4289136 -4.428874 -4.4288335 -4.4287848 -4.4287267 -4.4286904 -4.4286728 -4.4286461 -4.4286103 -4.4285674 -4.4285483 -4.4285483 -4.428555 -4.4285913 -4.4286485][-4.4289041 -4.4288731 -4.4288392 -4.428792 -4.4287276 -4.4286771 -4.4286308 -4.4285707 -4.4285035 -4.4284286 -4.4284029 -4.4284086 -4.4284205 -4.4284668 -4.4285421][-4.4288669 -4.4288354 -4.4287982 -4.428741 -4.4286628 -4.4285874 -4.42851 -4.428422 -4.4283366 -4.4282422 -4.4282188 -4.428237 -4.4282675 -4.4283457 -4.4284549][-4.4288034 -4.4287529 -4.4286957 -4.4286146 -4.4285145 -4.4284091 -4.4283071 -4.4282122 -4.4281316 -4.4280477 -4.4280438 -4.4280949 -4.4281607 -4.4282823 -4.4284253][-4.4287271 -4.4286456 -4.4285636 -4.4284663 -4.4283605 -4.4282527 -4.4281621 -4.4280977 -4.4280548 -4.4280124 -4.4280429 -4.4281182 -4.4282031 -4.4283419 -4.42849][-4.4287229 -4.4286342 -4.4285541 -4.4284782 -4.4284077 -4.4283361 -4.4282885 -4.4282656 -4.4282484 -4.4282308 -4.4282684 -4.4283381 -4.4284167 -4.4285331 -4.4286489][-4.4288154 -4.428761 -4.4287186 -4.4286804 -4.4286485 -4.4286103 -4.4285879 -4.4285774 -4.4285636 -4.4285555 -4.4285812 -4.4286318 -4.4286914 -4.4287682 -4.4288378][-4.4289174 -4.4288979 -4.4288893 -4.42888 -4.4288721 -4.4288559 -4.428843 -4.428834 -4.4288239 -4.428822 -4.4288383 -4.4288692 -4.4289045 -4.4289422 -4.4289718][-4.4289875 -4.4289818 -4.4289827 -4.4289823 -4.4289818 -4.428977 -4.4289708 -4.428966 -4.4289603 -4.4289594 -4.4289675 -4.4289837 -4.428997 -4.4290123 -4.4290237][-4.4290266 -4.4290247 -4.4290237 -4.4290252 -4.4290266 -4.4290276 -4.4290261 -4.4290247 -4.4290214 -4.42902 -4.4290218 -4.4290271 -4.4290295 -4.4290352 -4.4290409][-4.4290471 -4.4290485 -4.4290471 -4.4290476 -4.429049 -4.42905 -4.42905 -4.4290495 -4.4290481 -4.4290466 -4.4290466 -4.4290476 -4.4290476 -4.4290495 -4.4290519]]...]
INFO - root - 2017-12-08 06:30:27.530578: step 25910, loss = 2.28, batch loss = 2.23 (34.9 examples/sec; 0.229 sec/batch; 19h:31m:17s remains)
INFO - root - 2017-12-08 06:30:29.779657: step 25920, loss = 2.28, batch loss = 2.23 (37.1 examples/sec; 0.215 sec/batch; 18h:20m:25s remains)
INFO - root - 2017-12-08 06:30:32.013430: step 25930, loss = 2.28, batch loss = 2.23 (36.7 examples/sec; 0.218 sec/batch; 18h:33m:21s remains)
INFO - root - 2017-12-08 06:30:34.233732: step 25940, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.222 sec/batch; 18h:52m:04s remains)
INFO - root - 2017-12-08 06:30:36.449971: step 25950, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 18h:40m:03s remains)
INFO - root - 2017-12-08 06:30:38.665116: step 25960, loss = 2.28, batch loss = 2.23 (36.7 examples/sec; 0.218 sec/batch; 18h:32m:51s remains)
INFO - root - 2017-12-08 06:30:40.936292: step 25970, loss = 2.28, batch loss = 2.23 (34.8 examples/sec; 0.230 sec/batch; 19h:34m:21s remains)
INFO - root - 2017-12-08 06:30:43.214863: step 25980, loss = 2.28, batch loss = 2.23 (35.7 examples/sec; 0.224 sec/batch; 19h:03m:24s remains)
INFO - root - 2017-12-08 06:30:45.464454: step 25990, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.221 sec/batch; 18h:51m:16s remains)
INFO - root - 2017-12-08 06:30:47.686656: step 26000, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 18h:58m:23s remains)
2017-12-08 06:30:48.014752: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4287105 -4.4287171 -4.42872 -4.4287066 -4.428688 -4.4286723 -4.4286714 -4.4286833 -4.4286995 -4.4287128 -4.4287214 -4.4287143 -4.4286828 -4.4286385 -4.4286022][-4.4288034 -4.4288034 -4.4288044 -4.4287972 -4.428782 -4.42876 -4.4287519 -4.4287567 -4.4287715 -4.4287844 -4.428792 -4.4287834 -4.4287558 -4.4287205 -4.4286919][-4.4288654 -4.4288592 -4.4288588 -4.4288607 -4.4288521 -4.4288297 -4.4288139 -4.428813 -4.4288282 -4.4288445 -4.4288573 -4.4288549 -4.4288383 -4.4288116 -4.4287882][-4.4288678 -4.428853 -4.4288478 -4.4288507 -4.4288449 -4.4288254 -4.4288058 -4.428802 -4.42882 -4.4288492 -4.4288788 -4.4288926 -4.4288921 -4.4288759 -4.4288573][-4.4288287 -4.4287925 -4.4287691 -4.4287591 -4.42875 -4.4287305 -4.4287038 -4.4286966 -4.4287229 -4.4287758 -4.4288325 -4.4288721 -4.4288874 -4.4288793 -4.4288673][-4.42878 -4.428719 -4.4286666 -4.4286246 -4.428587 -4.42855 -4.428514 -4.4285054 -4.4285455 -4.4286289 -4.42872 -4.428792 -4.4288282 -4.4288325 -4.4288316][-4.4287481 -4.4286704 -4.4285936 -4.4285235 -4.4284458 -4.4283662 -4.4283009 -4.4282765 -4.4283242 -4.4284358 -4.4285617 -4.4286709 -4.4287357 -4.428761 -4.4287753][-4.42876 -4.4286757 -4.4285917 -4.4285154 -4.4284229 -4.4283128 -4.4282174 -4.4281573 -4.4281836 -4.4282956 -4.4284339 -4.4285588 -4.4286404 -4.428689 -4.4287248][-4.4288058 -4.4287262 -4.4286566 -4.4286041 -4.4285283 -4.428432 -4.4283504 -4.4282846 -4.4282789 -4.4283433 -4.4284468 -4.4285426 -4.428596 -4.4286366 -4.4286814][-4.4288645 -4.4288044 -4.4287581 -4.4287338 -4.428688 -4.4286218 -4.4285789 -4.4285398 -4.4285197 -4.428534 -4.4285789 -4.4286189 -4.4286227 -4.4286284 -4.4286618][-4.428905 -4.4288697 -4.4288473 -4.4288449 -4.4288254 -4.4287877 -4.428772 -4.4287577 -4.4287324 -4.4287167 -4.4287181 -4.4287119 -4.4286795 -4.4286537 -4.4286737][-4.4289241 -4.4289064 -4.4289064 -4.42892 -4.4289145 -4.4288917 -4.4288855 -4.4288788 -4.4288583 -4.4288363 -4.4288239 -4.4287939 -4.4287419 -4.4287028 -4.4287152][-4.4289331 -4.4289322 -4.4289494 -4.4289684 -4.4289684 -4.4289508 -4.4289451 -4.428937 -4.4289227 -4.42891 -4.4289 -4.4288621 -4.4288025 -4.4287581 -4.4287677][-4.4289279 -4.4289351 -4.4289618 -4.4289856 -4.428988 -4.4289742 -4.4289694 -4.428957 -4.4289432 -4.4289341 -4.4289303 -4.4288979 -4.4288464 -4.4288082 -4.4288163][-4.4288926 -4.4289002 -4.4289284 -4.4289522 -4.4289594 -4.4289551 -4.4289522 -4.42894 -4.4289236 -4.4289155 -4.4289165 -4.4288979 -4.4288678 -4.4288464 -4.4288549]]...]
INFO - root - 2017-12-08 06:30:50.225278: step 26010, loss = 2.28, batch loss = 2.23 (36.8 examples/sec; 0.217 sec/batch; 18h:29m:20s remains)
INFO - root - 2017-12-08 06:30:52.474354: step 26020, loss = 2.28, batch loss = 2.23 (36.0 examples/sec; 0.222 sec/batch; 18h:55m:56s remains)
INFO - root - 2017-12-08 06:30:54.714894: step 26030, loss = 2.28, batch loss = 2.23 (36.0 examples/sec; 0.222 sec/batch; 18h:54m:51s remains)
INFO - root - 2017-12-08 06:30:56.941453: step 26040, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.222 sec/batch; 18h:52m:06s remains)
INFO - root - 2017-12-08 06:30:59.167626: step 26050, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.221 sec/batch; 18h:50m:42s remains)
INFO - root - 2017-12-08 06:31:01.387414: step 26060, loss = 2.28, batch loss = 2.23 (35.1 examples/sec; 0.228 sec/batch; 19h:25m:30s remains)
INFO - root - 2017-12-08 06:31:03.642856: step 26070, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 18h:41m:21s remains)
INFO - root - 2017-12-08 06:31:05.883579: step 26080, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.222 sec/batch; 18h:52m:50s remains)
INFO - root - 2017-12-08 06:31:08.115194: step 26090, loss = 2.28, batch loss = 2.23 (36.7 examples/sec; 0.218 sec/batch; 18h:34m:17s remains)
INFO - root - 2017-12-08 06:31:10.347525: step 26100, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 18h:37m:56s remains)
2017-12-08 06:31:10.640794: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.42867 -4.4286761 -4.4286809 -4.428689 -4.4287381 -4.4288058 -4.42884 -4.4288363 -4.42878 -4.4287295 -4.4287004 -4.428679 -4.4286623 -4.4286284 -4.4285936][-4.4286489 -4.4286509 -4.4286575 -4.428679 -4.4287372 -4.4288073 -4.4288411 -4.4288325 -4.4287753 -4.4287281 -4.4287033 -4.4286747 -4.4286504 -4.4286046 -4.4285588][-4.4286547 -4.4286547 -4.4286618 -4.428689 -4.4287391 -4.4287968 -4.4288244 -4.428812 -4.4287634 -4.4287219 -4.4287014 -4.4286747 -4.428648 -4.4286051 -4.428565][-4.4286985 -4.4286904 -4.4286957 -4.4287248 -4.4287577 -4.4287887 -4.4287992 -4.4287834 -4.4287505 -4.4287171 -4.4286985 -4.4286737 -4.42865 -4.4286151 -4.4285903][-4.428741 -4.4287105 -4.4287033 -4.4287353 -4.4287467 -4.4287505 -4.4287424 -4.4287329 -4.4287167 -4.4286971 -4.4286957 -4.4286809 -4.4286671 -4.4286404 -4.4286175][-4.4287333 -4.428689 -4.428669 -4.4286995 -4.4286928 -4.4286737 -4.4286575 -4.4286556 -4.428647 -4.4286442 -4.4286623 -4.4286647 -4.4286666 -4.4286504 -4.4286294][-4.4286923 -4.428648 -4.4286113 -4.4286385 -4.4286323 -4.4286075 -4.4285913 -4.4285831 -4.4285703 -4.4285755 -4.4286075 -4.4286227 -4.4286427 -4.4286351 -4.4286127][-4.4286795 -4.4286313 -4.4285841 -4.4285994 -4.4285951 -4.4285789 -4.428566 -4.4285522 -4.4285359 -4.4285479 -4.4285741 -4.4285812 -4.428606 -4.428607 -4.4285855][-4.4286795 -4.42865 -4.4286056 -4.4286065 -4.4286027 -4.4285979 -4.4286013 -4.4285965 -4.4285951 -4.4286041 -4.4286041 -4.4285836 -4.4285917 -4.4285917 -4.4285693][-4.428627 -4.4286284 -4.4286065 -4.4286022 -4.4286008 -4.4286184 -4.4286461 -4.4286423 -4.4286408 -4.4286461 -4.428617 -4.4285703 -4.4285579 -4.4285555 -4.4285417][-4.428555 -4.428575 -4.4285765 -4.42857 -4.4285645 -4.4285941 -4.4286356 -4.42863 -4.4286222 -4.4286222 -4.4285765 -4.4285169 -4.4284911 -4.4284892 -4.4284782][-4.4285231 -4.4285507 -4.4285636 -4.4285526 -4.4285426 -4.428566 -4.4285955 -4.4285769 -4.4285626 -4.4285631 -4.4285192 -4.4284596 -4.4284234 -4.4284143 -4.4284081][-4.4285541 -4.4285731 -4.4285722 -4.428544 -4.42852 -4.4285307 -4.428545 -4.4285245 -4.4285159 -4.4285259 -4.4284983 -4.4284549 -4.4284148 -4.4283996 -4.4283996][-4.4286385 -4.4286227 -4.4285975 -4.4285669 -4.4285493 -4.4285612 -4.428575 -4.4285679 -4.4285703 -4.4285893 -4.4285779 -4.4285507 -4.4285164 -4.4284987 -4.4284992][-4.4287791 -4.4287462 -4.4287143 -4.4286976 -4.4286947 -4.4287076 -4.428721 -4.4287205 -4.4287248 -4.4287395 -4.4287372 -4.4287229 -4.4286981 -4.4286861 -4.4286876]]...]
INFO - root - 2017-12-08 06:31:12.863325: step 26110, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 18h:43m:04s remains)
INFO - root - 2017-12-08 06:31:15.097981: step 26120, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.220 sec/batch; 18h:44m:14s remains)
INFO - root - 2017-12-08 06:31:17.329183: step 26130, loss = 2.28, batch loss = 2.23 (35.6 examples/sec; 0.224 sec/batch; 19h:06m:02s remains)
INFO - root - 2017-12-08 06:31:19.560956: step 26140, loss = 2.28, batch loss = 2.23 (35.2 examples/sec; 0.227 sec/batch; 19h:19m:33s remains)
INFO - root - 2017-12-08 06:31:21.818258: step 26150, loss = 2.28, batch loss = 2.23 (35.0 examples/sec; 0.228 sec/batch; 19h:26m:17s remains)
INFO - root - 2017-12-08 06:31:24.067484: step 26160, loss = 2.28, batch loss = 2.23 (37.5 examples/sec; 0.213 sec/batch; 18h:09m:19s remains)
INFO - root - 2017-12-08 06:31:26.311262: step 26170, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 18h:47m:33s remains)
INFO - root - 2017-12-08 06:31:28.544961: step 26180, loss = 2.28, batch loss = 2.23 (36.8 examples/sec; 0.217 sec/batch; 18h:28m:44s remains)
INFO - root - 2017-12-08 06:31:30.813000: step 26190, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.220 sec/batch; 18h:43m:57s remains)
INFO - root - 2017-12-08 06:31:33.053762: step 26200, loss = 2.28, batch loss = 2.23 (34.8 examples/sec; 0.230 sec/batch; 19h:33m:36s remains)
2017-12-08 06:31:33.343093: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.42901 -4.4290071 -4.4289861 -4.4289632 -4.4289455 -4.4289427 -4.4289522 -4.4289637 -4.428968 -4.428966 -4.428957 -4.428947 -4.4289446 -4.4289517 -4.4289694][-4.4290161 -4.4290056 -4.4289684 -4.4289269 -4.4288931 -4.4288826 -4.4288917 -4.4289126 -4.428926 -4.4289303 -4.4289203 -4.4289074 -4.4289055 -4.4289155 -4.4289417][-4.4290137 -4.4289961 -4.4289441 -4.428885 -4.4288363 -4.4288144 -4.4288163 -4.4288411 -4.4288611 -4.4288735 -4.428863 -4.4288521 -4.4288573 -4.4288716 -4.4289021][-4.4289994 -4.4289746 -4.4289126 -4.42884 -4.4287772 -4.4287391 -4.4287319 -4.4287553 -4.4287834 -4.428812 -4.4288077 -4.4288015 -4.4288125 -4.4288239 -4.4288483][-4.4289746 -4.42894 -4.4288688 -4.4287829 -4.4287043 -4.4286389 -4.4286089 -4.4286113 -4.4286489 -4.4287138 -4.4287324 -4.4287415 -4.4287543 -4.4287572 -4.4287748][-4.4289541 -4.4289064 -4.4288254 -4.4287314 -4.4286313 -4.4285183 -4.4284377 -4.4283919 -4.4284277 -4.4285469 -4.4286137 -4.4286594 -4.42868 -4.4286814 -4.4286971][-4.4289351 -4.4288754 -4.4287844 -4.4286809 -4.4285574 -4.4283895 -4.4282255 -4.4280906 -4.4280939 -4.4282789 -4.4284277 -4.4285378 -4.4285927 -4.4286089 -4.4286323][-4.4289236 -4.4288578 -4.4287596 -4.4286518 -4.42852 -4.4283352 -4.4281235 -4.4279041 -4.4278336 -4.4280434 -4.4282603 -4.4284282 -4.4285173 -4.4285574 -4.4285927][-4.428926 -4.4288664 -4.4287796 -4.4286861 -4.428576 -4.4284329 -4.4282675 -4.4280782 -4.4279866 -4.4281168 -4.4282923 -4.42845 -4.4285412 -4.4285879 -4.4286251][-4.428936 -4.4288936 -4.4288273 -4.4287605 -4.4286828 -4.428587 -4.4284821 -4.4283628 -4.4282937 -4.4283504 -4.4284496 -4.4285512 -4.4286203 -4.4286647 -4.4286966][-4.4289465 -4.4289188 -4.4288721 -4.4288287 -4.4287777 -4.4287171 -4.4286542 -4.4285812 -4.4285369 -4.4285593 -4.42861 -4.4286652 -4.4287128 -4.4287486 -4.4287715][-4.4289622 -4.428947 -4.4289184 -4.428894 -4.428865 -4.4288325 -4.4287996 -4.4287572 -4.4287267 -4.4287291 -4.4287481 -4.4287753 -4.4288073 -4.4288344 -4.4288487][-4.428978 -4.4289732 -4.4289589 -4.4289484 -4.428936 -4.4289246 -4.4289131 -4.4288912 -4.4288697 -4.4288607 -4.4288635 -4.428874 -4.4288917 -4.4289055 -4.4289126][-4.4289856 -4.4289851 -4.4289756 -4.4289689 -4.4289627 -4.4289594 -4.4289584 -4.4289513 -4.4289427 -4.4289393 -4.4289393 -4.4289441 -4.4289527 -4.4289575 -4.4289613][-4.4289908 -4.4289918 -4.4289856 -4.4289808 -4.428977 -4.4289742 -4.4289746 -4.4289737 -4.4289727 -4.4289746 -4.4289784 -4.4289837 -4.4289885 -4.4289894 -4.4289908]]...]
INFO - root - 2017-12-08 06:31:35.559480: step 26210, loss = 2.28, batch loss = 2.23 (36.0 examples/sec; 0.222 sec/batch; 18h:54m:47s remains)
INFO - root - 2017-12-08 06:31:37.790959: step 26220, loss = 2.28, batch loss = 2.23 (34.8 examples/sec; 0.230 sec/batch; 19h:33m:59s remains)
INFO - root - 2017-12-08 06:31:40.020500: step 26230, loss = 2.28, batch loss = 2.23 (35.3 examples/sec; 0.226 sec/batch; 19h:15m:58s remains)
INFO - root - 2017-12-08 06:31:42.265887: step 26240, loss = 2.28, batch loss = 2.23 (34.4 examples/sec; 0.232 sec/batch; 19h:45m:49s remains)
INFO - root - 2017-12-08 06:31:44.538418: step 26250, loss = 2.28, batch loss = 2.23 (35.8 examples/sec; 0.224 sec/batch; 19h:01m:59s remains)
INFO - root - 2017-12-08 06:31:46.794331: step 26260, loss = 2.28, batch loss = 2.23 (35.6 examples/sec; 0.225 sec/batch; 19h:08m:26s remains)
INFO - root - 2017-12-08 06:31:49.046649: step 26270, loss = 2.28, batch loss = 2.23 (35.3 examples/sec; 0.227 sec/batch; 19h:17m:23s remains)
INFO - root - 2017-12-08 06:31:51.268375: step 26280, loss = 2.28, batch loss = 2.23 (34.4 examples/sec; 0.232 sec/batch; 19h:45m:52s remains)
INFO - root - 2017-12-08 06:31:53.515320: step 26290, loss = 2.28, batch loss = 2.23 (37.6 examples/sec; 0.213 sec/batch; 18h:06m:39s remains)
INFO - root - 2017-12-08 06:31:55.739186: step 26300, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.220 sec/batch; 18h:45m:04s remains)
2017-12-08 06:31:56.021835: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.42857 -4.4285574 -4.4285731 -4.4286222 -4.4287038 -4.4287467 -4.4286966 -4.4286356 -4.4286141 -4.4285703 -4.4285207 -4.4284768 -4.4284387 -4.4284534 -4.428545][-4.4285359 -4.428545 -4.4285755 -4.4286294 -4.4287009 -4.4287372 -4.4286838 -4.4286251 -4.4285946 -4.4285254 -4.4284444 -4.4283667 -4.4283161 -4.4283471 -4.4284663][-4.4285741 -4.4285989 -4.428627 -4.4286671 -4.4287186 -4.4287291 -4.4286594 -4.4285965 -4.4285893 -4.4285388 -4.4284544 -4.4283614 -4.4283009 -4.428339 -4.4284625][-4.4286108 -4.42863 -4.4286389 -4.4286585 -4.4286957 -4.428689 -4.42862 -4.428575 -4.428597 -4.4285831 -4.4285278 -4.4284558 -4.4284062 -4.4284539 -4.4285541][-4.42867 -4.4286547 -4.4286323 -4.4286256 -4.4286408 -4.4286218 -4.4285426 -4.4284959 -4.4285493 -4.428586 -4.4285789 -4.4285593 -4.4285541 -4.4285994 -4.4286618][-4.4287453 -4.4287009 -4.42864 -4.4285874 -4.4285436 -4.4284558 -4.4283071 -4.428206 -4.4283094 -4.4284592 -4.4285259 -4.4285722 -4.4286189 -4.4286571 -4.4286957][-4.4288187 -4.4287543 -4.4286575 -4.4285579 -4.4284339 -4.4282427 -4.4279714 -4.4277287 -4.4278808 -4.4281878 -4.4283628 -4.4284749 -4.4285593 -4.4286032 -4.428638][-4.4288745 -4.4288015 -4.4286838 -4.4285569 -4.4283957 -4.4281726 -4.42786 -4.4275289 -4.4276586 -4.4280252 -4.4282527 -4.4283886 -4.428483 -4.4285421 -4.428586][-4.4288988 -4.4288263 -4.4287186 -4.4286051 -4.4284697 -4.4283156 -4.4281111 -4.4279041 -4.4279637 -4.4281878 -4.4283314 -4.4284167 -4.4284763 -4.4285269 -4.428566][-4.4288945 -4.4288282 -4.4287386 -4.4286556 -4.4285703 -4.4285026 -4.4284105 -4.4283094 -4.428329 -4.4284205 -4.4284654 -4.4284916 -4.4285178 -4.4285507 -4.428576][-4.428854 -4.4288063 -4.4287429 -4.4286952 -4.4286575 -4.4286427 -4.4286103 -4.4285541 -4.42855 -4.4285789 -4.4285841 -4.4285817 -4.4285922 -4.4286094 -4.4286232][-4.4287949 -4.4287744 -4.4287424 -4.4287267 -4.4287214 -4.4287243 -4.4287162 -4.4286838 -4.4286671 -4.4286666 -4.4286666 -4.4286585 -4.4286628 -4.4286737 -4.4286861][-4.4287777 -4.4287786 -4.428771 -4.4287772 -4.4287825 -4.4287868 -4.4287825 -4.4287605 -4.4287376 -4.4287238 -4.4287214 -4.4287138 -4.4287124 -4.4287243 -4.4287486][-4.4288082 -4.4288177 -4.4288225 -4.4288387 -4.4288468 -4.4288487 -4.4288464 -4.4288373 -4.4288192 -4.4288049 -4.4287968 -4.428781 -4.4287705 -4.4287782 -4.4288039][-4.42886 -4.4288707 -4.4288807 -4.428894 -4.4289 -4.4288983 -4.4288931 -4.4288878 -4.4288793 -4.4288716 -4.428864 -4.42885 -4.4288359 -4.4288416 -4.428863]]...]
INFO - root - 2017-12-08 06:31:58.259870: step 26310, loss = 2.28, batch loss = 2.23 (34.1 examples/sec; 0.234 sec/batch; 19h:55m:33s remains)
INFO - root - 2017-12-08 06:32:00.518093: step 26320, loss = 2.28, batch loss = 2.23 (36.7 examples/sec; 0.218 sec/batch; 18h:33m:25s remains)
INFO - root - 2017-12-08 06:32:02.779747: step 26330, loss = 2.28, batch loss = 2.23 (35.6 examples/sec; 0.224 sec/batch; 19h:05m:08s remains)
INFO - root - 2017-12-08 06:32:05.041472: step 26340, loss = 2.28, batch loss = 2.23 (33.3 examples/sec; 0.240 sec/batch; 20h:26m:59s remains)
INFO - root - 2017-12-08 06:32:07.273097: step 26350, loss = 2.28, batch loss = 2.23 (35.2 examples/sec; 0.227 sec/batch; 19h:18m:47s remains)
INFO - root - 2017-12-08 06:32:09.495002: step 26360, loss = 2.28, batch loss = 2.23 (37.0 examples/sec; 0.216 sec/batch; 18h:23m:31s remains)
INFO - root - 2017-12-08 06:32:11.758298: step 26370, loss = 2.28, batch loss = 2.23 (36.7 examples/sec; 0.218 sec/batch; 18h:33m:17s remains)
INFO - root - 2017-12-08 06:32:14.011808: step 26380, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 18h:48m:05s remains)
INFO - root - 2017-12-08 06:32:16.230313: step 26390, loss = 2.28, batch loss = 2.23 (37.6 examples/sec; 0.213 sec/batch; 18h:05m:14s remains)
INFO - root - 2017-12-08 06:32:18.539456: step 26400, loss = 2.28, batch loss = 2.23 (32.3 examples/sec; 0.248 sec/batch; 21h:02m:44s remains)
2017-12-08 06:32:18.838934: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289589 -4.428956 -4.4289479 -4.4289451 -4.4289532 -4.4289708 -4.4289904 -4.4290018 -4.4290032 -4.4289975 -4.4289827 -4.4289646 -4.4289479 -4.4289346 -4.42893][-4.4289546 -4.4289336 -4.4289064 -4.4288945 -4.4289055 -4.4289327 -4.4289651 -4.4289908 -4.4290066 -4.4290142 -4.4290104 -4.428998 -4.4289827 -4.4289665 -4.4289551][-4.428937 -4.4288893 -4.4288335 -4.428803 -4.428812 -4.42885 -4.4289002 -4.4289451 -4.4289794 -4.4290056 -4.4290185 -4.429018 -4.4290085 -4.4289927 -4.4289765][-4.4289131 -4.4288378 -4.428751 -4.4286962 -4.4286966 -4.4287405 -4.4288044 -4.4288654 -4.4289207 -4.4289713 -4.4290085 -4.4290271 -4.4290276 -4.4290152 -4.4289961][-4.4288955 -4.4288039 -4.4286923 -4.4286094 -4.4285908 -4.4286251 -4.4286866 -4.4287539 -4.4288282 -4.4289041 -4.4289684 -4.4290133 -4.4290318 -4.429028 -4.429009][-4.4288883 -4.4287958 -4.4286728 -4.428565 -4.428514 -4.4285164 -4.4285522 -4.4286137 -4.4287047 -4.4288049 -4.4288936 -4.4289641 -4.4290071 -4.4290195 -4.429008][-4.4288907 -4.428812 -4.4286933 -4.4285717 -4.42848 -4.4284263 -4.4284072 -4.4284458 -4.4285531 -4.428678 -4.4287891 -4.4288855 -4.4289556 -4.4289913 -4.4289942][-4.4289007 -4.4288421 -4.42874 -4.428617 -4.4284911 -4.4283757 -4.4282851 -4.4282832 -4.4283938 -4.4285369 -4.4286647 -4.4287825 -4.4288812 -4.4289451 -4.4289708][-4.4289145 -4.428874 -4.4287949 -4.4286861 -4.4285522 -4.4284029 -4.4282608 -4.4282069 -4.4282885 -4.4284225 -4.4285493 -4.4286752 -4.4287953 -4.4288864 -4.4289365][-4.428926 -4.4289021 -4.4288487 -4.4287677 -4.4286528 -4.4285088 -4.4283576 -4.4282684 -4.42829 -4.4283776 -4.4284797 -4.428596 -4.4287219 -4.4288297 -4.4289][-4.4289322 -4.4289203 -4.42889 -4.4288411 -4.42876 -4.4286494 -4.4285226 -4.4284229 -4.4283881 -4.4284153 -4.4284768 -4.428566 -4.428678 -4.4287863 -4.4288645][-4.4289374 -4.4289331 -4.4289203 -4.4288955 -4.4288497 -4.4287829 -4.4286985 -4.4286122 -4.4285488 -4.4285288 -4.4285474 -4.428597 -4.4286766 -4.4287672 -4.4288425][-4.4289427 -4.4289432 -4.4289389 -4.4289289 -4.4289093 -4.42888 -4.4288392 -4.4287825 -4.4287229 -4.4286828 -4.428669 -4.42868 -4.4287195 -4.428781 -4.4288416][-4.4289494 -4.4289551 -4.4289541 -4.4289503 -4.4289451 -4.4289384 -4.428925 -4.4288979 -4.42886 -4.4288268 -4.428803 -4.42879 -4.4287982 -4.4288278 -4.4288659][-4.4289527 -4.4289637 -4.4289632 -4.42896 -4.4289594 -4.4289603 -4.42896 -4.4289532 -4.4289389 -4.4289217 -4.4289041 -4.4288874 -4.4288816 -4.4288898 -4.428906]]...]
INFO - root - 2017-12-08 06:32:21.129359: step 26410, loss = 2.28, batch loss = 2.23 (36.9 examples/sec; 0.217 sec/batch; 18h:25m:32s remains)
INFO - root - 2017-12-08 06:32:23.385712: step 26420, loss = 2.28, batch loss = 2.23 (35.7 examples/sec; 0.224 sec/batch; 19h:04m:39s remains)
INFO - root - 2017-12-08 06:32:25.614832: step 26430, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.225 sec/batch; 19h:09m:51s remains)
INFO - root - 2017-12-08 06:32:27.849549: step 26440, loss = 2.28, batch loss = 2.23 (35.7 examples/sec; 0.224 sec/batch; 19h:02m:21s remains)
INFO - root - 2017-12-08 06:32:30.070575: step 26450, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 18h:38m:28s remains)
INFO - root - 2017-12-08 06:32:32.293857: step 26460, loss = 2.28, batch loss = 2.23 (36.7 examples/sec; 0.218 sec/batch; 18h:32m:01s remains)
INFO - root - 2017-12-08 06:32:34.542731: step 26470, loss = 2.28, batch loss = 2.23 (34.6 examples/sec; 0.231 sec/batch; 19h:40m:30s remains)
INFO - root - 2017-12-08 06:32:36.802022: step 26480, loss = 2.28, batch loss = 2.23 (35.8 examples/sec; 0.223 sec/batch; 18h:59m:25s remains)
INFO - root - 2017-12-08 06:32:39.058573: step 26490, loss = 2.28, batch loss = 2.23 (34.3 examples/sec; 0.233 sec/batch; 19h:48m:07s remains)
INFO - root - 2017-12-08 06:32:41.302939: step 26500, loss = 2.28, batch loss = 2.23 (35.3 examples/sec; 0.227 sec/batch; 19h:15m:34s remains)
2017-12-08 06:32:41.591787: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.428885 -4.4288931 -4.4288845 -4.4288521 -4.4288135 -4.4287844 -4.4287596 -4.42872 -4.4286895 -4.4287109 -4.4287634 -4.42883 -4.4288888 -4.428905 -4.4288697][-4.4289393 -4.4289365 -4.4289 -4.4288387 -4.4287863 -4.4287553 -4.4287448 -4.4287248 -4.4287124 -4.428751 -4.4288068 -4.4288697 -4.42891 -4.4289074 -4.4288568][-4.4289708 -4.4289556 -4.4288917 -4.4288063 -4.4287457 -4.4287367 -4.4287491 -4.4287434 -4.4287405 -4.4287686 -4.4288111 -4.4288578 -4.4288864 -4.428874 -4.4288068][-4.4289784 -4.428947 -4.4288564 -4.4287553 -4.4287024 -4.4287195 -4.4287491 -4.4287515 -4.4287424 -4.4287529 -4.4287806 -4.4288225 -4.4288454 -4.4288168 -4.4287367][-4.4289489 -4.4289017 -4.4287977 -4.4287028 -4.4286647 -4.42869 -4.4287119 -4.4286947 -4.4286666 -4.4286671 -4.4287033 -4.4287562 -4.4287782 -4.4287405 -4.4286575][-4.4288807 -4.4288287 -4.42874 -4.4286666 -4.4286389 -4.4286385 -4.4286022 -4.4285231 -4.4284806 -4.4285097 -4.4285846 -4.428647 -4.428658 -4.4286127 -4.4285455][-4.4288111 -4.4287705 -4.4287071 -4.4286532 -4.4286056 -4.4285359 -4.4283891 -4.4282165 -4.4281955 -4.4283204 -4.4284525 -4.4285235 -4.4285278 -4.4284987 -4.428472][-4.42876 -4.4287319 -4.4286857 -4.4286337 -4.42855 -4.4284043 -4.428175 -4.4279819 -4.4280715 -4.4283056 -4.4284835 -4.4285622 -4.4285765 -4.4285812 -4.4285984][-4.4287276 -4.4287138 -4.4286952 -4.4286623 -4.4285903 -4.4284673 -4.4283071 -4.4282327 -4.4283605 -4.4285522 -4.4286876 -4.4287429 -4.4287529 -4.4287624 -4.42878][-4.4287658 -4.4287572 -4.4287581 -4.4287477 -4.4287047 -4.4286427 -4.4285736 -4.4285693 -4.4286528 -4.4287586 -4.4288311 -4.4288573 -4.4288669 -4.4288788 -4.4288874][-4.428813 -4.4288011 -4.4288049 -4.428803 -4.4287767 -4.4287395 -4.4287014 -4.4287076 -4.4287643 -4.4288278 -4.4288678 -4.4288821 -4.4289002 -4.42892 -4.4289269][-4.4288082 -4.4288006 -4.4288015 -4.4287972 -4.4287672 -4.4287271 -4.4286928 -4.428699 -4.4287448 -4.4287958 -4.4288306 -4.428854 -4.428885 -4.4289122 -4.4289188][-4.4287786 -4.4287677 -4.4287615 -4.4287534 -4.4287181 -4.4286714 -4.4286404 -4.4286566 -4.4287081 -4.428761 -4.4287987 -4.4288311 -4.42887 -4.4288988 -4.4289002][-4.4287839 -4.4287581 -4.4287357 -4.4287167 -4.4286804 -4.4286375 -4.428618 -4.4286432 -4.4286966 -4.4287477 -4.4287872 -4.4288235 -4.42886 -4.4288836 -4.4288816][-4.4287996 -4.4287605 -4.4287276 -4.4287076 -4.4286876 -4.4286618 -4.4286532 -4.4286728 -4.4287128 -4.4287529 -4.428782 -4.4288058 -4.4288287 -4.428843 -4.4288359]]...]
INFO - root - 2017-12-08 06:32:43.842200: step 26510, loss = 2.28, batch loss = 2.23 (34.8 examples/sec; 0.230 sec/batch; 19h:33m:22s remains)
INFO - root - 2017-12-08 06:32:46.076499: step 26520, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.225 sec/batch; 19h:08m:18s remains)
INFO - root - 2017-12-08 06:32:48.324596: step 26530, loss = 2.28, batch loss = 2.23 (34.4 examples/sec; 0.232 sec/batch; 19h:44m:24s remains)
INFO - root - 2017-12-08 06:32:50.542390: step 26540, loss = 2.28, batch loss = 2.23 (36.6 examples/sec; 0.218 sec/batch; 18h:33m:24s remains)
INFO - root - 2017-12-08 06:32:52.774113: step 26550, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.220 sec/batch; 18h:42m:30s remains)
INFO - root - 2017-12-08 06:32:55.037585: step 26560, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 18h:42m:10s remains)
INFO - root - 2017-12-08 06:32:57.267788: step 26570, loss = 2.28, batch loss = 2.23 (35.3 examples/sec; 0.226 sec/batch; 19h:14m:17s remains)
INFO - root - 2017-12-08 06:32:59.521753: step 26580, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.225 sec/batch; 19h:09m:02s remains)
INFO - root - 2017-12-08 06:33:01.754813: step 26590, loss = 2.28, batch loss = 2.23 (35.2 examples/sec; 0.227 sec/batch; 19h:18m:22s remains)
INFO - root - 2017-12-08 06:33:03.984592: step 26600, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 18h:37m:04s remains)
2017-12-08 06:33:04.255975: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289269 -4.4289231 -4.4289184 -4.4289141 -4.4289136 -4.4289141 -4.4289093 -4.4288988 -4.4288907 -4.4288912 -4.4289017 -4.4289193 -4.4289327 -4.4289346 -4.4289217][-4.4289513 -4.42894 -4.4289231 -4.4289079 -4.4288993 -4.4288988 -4.4289 -4.428894 -4.4288864 -4.4288826 -4.4288874 -4.4289045 -4.4289241 -4.4289331 -4.4289241][-4.4289565 -4.428937 -4.4289107 -4.4288831 -4.4288597 -4.4288497 -4.4288497 -4.4288521 -4.4288526 -4.4288545 -4.4288535 -4.428864 -4.428885 -4.428905 -4.4289103][-4.4289308 -4.4288969 -4.4288564 -4.4288158 -4.4287791 -4.4287524 -4.4287424 -4.4287481 -4.4287629 -4.4287806 -4.4287868 -4.4287972 -4.4288154 -4.4288406 -4.4288597][-4.4288945 -4.4288406 -4.4287839 -4.4287319 -4.4286804 -4.428627 -4.4285975 -4.4286113 -4.4286489 -4.4286909 -4.4287124 -4.4287229 -4.4287386 -4.4287677 -4.4287915][-4.4288507 -4.42877 -4.4286966 -4.4286361 -4.42857 -4.4284778 -4.4284153 -4.4284406 -4.4285192 -4.4285946 -4.4286289 -4.4286251 -4.4286246 -4.4286566 -4.4286952][-4.4287944 -4.4286866 -4.4286008 -4.4285297 -4.4284372 -4.4282994 -4.42818 -4.4282002 -4.4283323 -4.4284496 -4.4285007 -4.4284883 -4.4284744 -4.4285054 -4.4285579][-4.4287438 -4.4286308 -4.4285522 -4.4284954 -4.4284115 -4.4282684 -4.4281216 -4.4281096 -4.4282532 -4.4283967 -4.4284649 -4.42845 -4.4284134 -4.4284239 -4.4284706][-4.4287219 -4.42865 -4.4286108 -4.428586 -4.4285393 -4.4284506 -4.4283547 -4.428339 -4.4284191 -4.4285216 -4.4285817 -4.4285836 -4.4285507 -4.4285345 -4.4285421][-4.4287019 -4.4286861 -4.4286938 -4.4287028 -4.4286866 -4.4286432 -4.4285946 -4.4285831 -4.4286137 -4.4286656 -4.4287043 -4.4287157 -4.4287019 -4.4286866 -4.4286795][-4.428668 -4.4286976 -4.4287467 -4.4287882 -4.4288006 -4.4287868 -4.4287682 -4.4287624 -4.4287682 -4.4287877 -4.4288111 -4.4288239 -4.4288211 -4.4288092 -4.4287982][-4.4286661 -4.4287062 -4.428762 -4.4288139 -4.42884 -4.4288449 -4.4288421 -4.4288416 -4.4288406 -4.4288478 -4.4288669 -4.428884 -4.4288888 -4.428885 -4.4288726][-4.4287367 -4.4287643 -4.428802 -4.4288397 -4.4288635 -4.4288735 -4.428875 -4.4288764 -4.42888 -4.4288898 -4.4289074 -4.4289231 -4.4289312 -4.4289289 -4.4289179][-4.4288106 -4.4288235 -4.4288397 -4.4288583 -4.4288735 -4.4288855 -4.4288931 -4.4289002 -4.4289083 -4.42892 -4.4289336 -4.4289455 -4.4289541 -4.4289536 -4.4289436][-4.4288673 -4.4288731 -4.4288759 -4.4288778 -4.4288836 -4.42889 -4.4288969 -4.4289055 -4.4289165 -4.4289289 -4.4289393 -4.4289451 -4.4289474 -4.4289436 -4.4289331]]...]
INFO - root - 2017-12-08 06:33:06.474954: step 26610, loss = 2.28, batch loss = 2.23 (35.8 examples/sec; 0.223 sec/batch; 18h:57m:46s remains)
INFO - root - 2017-12-08 06:33:08.721237: step 26620, loss = 2.28, batch loss = 2.23 (36.6 examples/sec; 0.219 sec/batch; 18h:35m:26s remains)
INFO - root - 2017-12-08 06:33:10.949746: step 26630, loss = 2.28, batch loss = 2.23 (37.3 examples/sec; 0.215 sec/batch; 18h:13m:35s remains)
INFO - root - 2017-12-08 06:33:13.166325: step 26640, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 18h:54m:37s remains)
INFO - root - 2017-12-08 06:33:15.404960: step 26650, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 18h:45m:01s remains)
INFO - root - 2017-12-08 06:33:17.648605: step 26660, loss = 2.28, batch loss = 2.23 (35.7 examples/sec; 0.224 sec/batch; 19h:01m:16s remains)
INFO - root - 2017-12-08 06:33:19.878199: step 26670, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 18h:36m:13s remains)
INFO - root - 2017-12-08 06:33:22.143078: step 26680, loss = 2.28, batch loss = 2.23 (35.0 examples/sec; 0.229 sec/batch; 19h:25m:12s remains)
INFO - root - 2017-12-08 06:33:24.395845: step 26690, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 18h:39m:00s remains)
INFO - root - 2017-12-08 06:33:26.637432: step 26700, loss = 2.28, batch loss = 2.23 (35.6 examples/sec; 0.225 sec/batch; 19h:04m:35s remains)
2017-12-08 06:33:26.942272: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289207 -4.4289236 -4.4289169 -4.4289045 -4.4288969 -4.4289 -4.4288945 -4.4288874 -4.4288888 -4.4288883 -4.4288831 -4.4288754 -4.4288664 -4.4288545 -4.428854][-4.42889 -4.4288926 -4.4288864 -4.4288726 -4.4288597 -4.4288573 -4.428844 -4.4288239 -4.4288139 -4.4288135 -4.4288111 -4.4288015 -4.428791 -4.4287858 -4.4287982][-4.4288549 -4.4288597 -4.4288507 -4.4288363 -4.4288182 -4.428802 -4.4287682 -4.4287267 -4.4287062 -4.4287128 -4.4287205 -4.4287152 -4.4287143 -4.4287257 -4.4287505][-4.4288263 -4.4288297 -4.4288135 -4.4287934 -4.4287643 -4.4287238 -4.4286652 -4.4286051 -4.4285889 -4.4286156 -4.4286447 -4.42865 -4.4286623 -4.4286947 -4.4287286][-4.4287968 -4.4287977 -4.4287763 -4.4287491 -4.4287047 -4.4286304 -4.428534 -4.42846 -4.4284635 -4.4285307 -4.4285946 -4.4286184 -4.4286427 -4.4286871 -4.4287229][-4.4287553 -4.4287553 -4.4287248 -4.4286866 -4.4286332 -4.4285235 -4.4283643 -4.4282546 -4.428299 -4.4284377 -4.4285507 -4.4286065 -4.4286475 -4.4286947 -4.4287286][-4.428709 -4.4287052 -4.4286642 -4.4286184 -4.4285617 -4.4284196 -4.4281816 -4.4280148 -4.4281287 -4.42836 -4.4285221 -4.4286103 -4.4286675 -4.4287086 -4.4287276][-4.4286895 -4.4286795 -4.4286346 -4.4285975 -4.4285493 -4.4284015 -4.42815 -4.4279876 -4.428144 -4.4283986 -4.4285626 -4.4286523 -4.4287009 -4.428721 -4.4287171][-4.4286971 -4.4286947 -4.4286575 -4.4286346 -4.4286036 -4.428493 -4.4283152 -4.4282341 -4.4283657 -4.4285479 -4.428659 -4.4287219 -4.4287405 -4.4287291 -4.4287028][-4.4287281 -4.4287338 -4.4287081 -4.4286909 -4.4286714 -4.4285951 -4.4284844 -4.4284558 -4.4285593 -4.4286838 -4.4287524 -4.4287844 -4.4287672 -4.4287343 -4.4286995][-4.4287648 -4.4287744 -4.4287605 -4.4287529 -4.4287443 -4.4286904 -4.4286127 -4.428606 -4.4286981 -4.4287934 -4.428833 -4.4288445 -4.4288106 -4.4287634 -4.4287286][-4.4287915 -4.4288068 -4.4288049 -4.428812 -4.4288092 -4.4287696 -4.4287205 -4.4287233 -4.4287963 -4.4288621 -4.4288864 -4.428895 -4.4288626 -4.4288082 -4.4287724][-4.4288263 -4.4288521 -4.4288611 -4.42887 -4.4288611 -4.4288292 -4.428803 -4.428813 -4.4288611 -4.4288988 -4.4289093 -4.4289117 -4.4288826 -4.4288325 -4.4288034][-4.42886 -4.4288893 -4.4289021 -4.4289069 -4.4288917 -4.4288659 -4.4288507 -4.42886 -4.4288912 -4.4289088 -4.4289126 -4.4289055 -4.42888 -4.428844 -4.4288292][-4.428884 -4.4289074 -4.4289179 -4.4289184 -4.4289007 -4.428875 -4.428863 -4.4288678 -4.4288859 -4.4288979 -4.4289055 -4.4289021 -4.4288864 -4.428865 -4.4288597]]...]
INFO - root - 2017-12-08 06:33:29.169120: step 26710, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 18h:37m:41s remains)
INFO - root - 2017-12-08 06:33:31.392612: step 26720, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 18h:36m:00s remains)
INFO - root - 2017-12-08 06:33:33.633508: step 26730, loss = 2.28, batch loss = 2.23 (36.8 examples/sec; 0.217 sec/batch; 18h:27m:04s remains)
INFO - root - 2017-12-08 06:33:35.867127: step 26740, loss = 2.28, batch loss = 2.23 (35.2 examples/sec; 0.228 sec/batch; 19h:19m:43s remains)
INFO - root - 2017-12-08 06:33:38.104905: step 26750, loss = 2.28, batch loss = 2.23 (36.0 examples/sec; 0.222 sec/batch; 18h:51m:46s remains)
INFO - root - 2017-12-08 06:33:40.353440: step 26760, loss = 2.28, batch loss = 2.23 (35.1 examples/sec; 0.228 sec/batch; 19h:19m:59s remains)
INFO - root - 2017-12-08 06:33:42.586583: step 26770, loss = 2.28, batch loss = 2.23 (35.1 examples/sec; 0.228 sec/batch; 19h:20m:38s remains)
INFO - root - 2017-12-08 06:33:44.820688: step 26780, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 18h:47m:23s remains)
INFO - root - 2017-12-08 06:33:47.046010: step 26790, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.220 sec/batch; 18h:42m:16s remains)
INFO - root - 2017-12-08 06:33:49.303193: step 26800, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.225 sec/batch; 19h:07m:03s remains)
2017-12-08 06:33:49.600104: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.428936 -4.4289122 -4.4288793 -4.428844 -4.4288 -4.4287348 -4.42868 -4.4286275 -4.4286327 -4.4287052 -4.4287524 -4.4287529 -4.4287157 -4.4286518 -4.4285674][-4.4289312 -4.4289064 -4.4288745 -4.4288363 -4.4287844 -4.4287071 -4.4286413 -4.4285884 -4.4286013 -4.4286771 -4.4287262 -4.4287152 -4.4286609 -4.4285922 -4.4285197][-4.4289255 -4.4289 -4.4288645 -4.4288182 -4.4287596 -4.42868 -4.4286075 -4.4285507 -4.4285617 -4.4286246 -4.4286723 -4.4286623 -4.4286094 -4.428556 -4.4285059][-4.4289193 -4.4288921 -4.428853 -4.4287949 -4.4287281 -4.428648 -4.4285636 -4.4284954 -4.4285 -4.4285455 -4.4285951 -4.4286041 -4.4285722 -4.4285431 -4.4285178][-4.4289141 -4.4288831 -4.4288363 -4.4287663 -4.4286909 -4.4286041 -4.4284878 -4.4283924 -4.4283986 -4.4284487 -4.4285111 -4.42855 -4.4285369 -4.4285245 -4.4285235][-4.4289083 -4.428874 -4.4288197 -4.4287372 -4.4286418 -4.4285336 -4.4283643 -4.4282112 -4.4282455 -4.4283538 -4.42846 -4.4285369 -4.4285388 -4.4285316 -4.4285412][-4.4289021 -4.4288645 -4.4288044 -4.4287138 -4.4285965 -4.4284468 -4.428196 -4.4279552 -4.4280577 -4.4282947 -4.4284668 -4.4285679 -4.4285827 -4.4285817 -4.428587][-4.4288979 -4.4288568 -4.4287939 -4.4287014 -4.4285693 -4.428391 -4.4281106 -4.42785 -4.4280076 -4.4283075 -4.4285083 -4.4286118 -4.428637 -4.4286423 -4.4286427][-4.428896 -4.428853 -4.4287877 -4.4286981 -4.4285707 -4.4284148 -4.4282169 -4.4280534 -4.42817 -4.4284163 -4.4285946 -4.4286852 -4.4287081 -4.4287086 -4.4287009][-4.428894 -4.4288516 -4.4287915 -4.4287128 -4.4286065 -4.4284892 -4.4283733 -4.4282889 -4.4283671 -4.4285512 -4.4286885 -4.4287524 -4.4287596 -4.4287548 -4.4287481][-4.4288917 -4.428853 -4.4288077 -4.4287457 -4.42866 -4.4285665 -4.4284863 -4.4284444 -4.4285145 -4.4286604 -4.4287553 -4.4287939 -4.4288011 -4.4288011 -4.428793][-4.4288878 -4.4288521 -4.4288211 -4.4287753 -4.428709 -4.4286294 -4.4285707 -4.4285483 -4.4286118 -4.4287214 -4.4287786 -4.4288034 -4.4288111 -4.4288092 -4.4287958][-4.4288831 -4.4288497 -4.4288268 -4.4287949 -4.4287472 -4.428678 -4.4286232 -4.4285893 -4.4286356 -4.4287148 -4.4287562 -4.4287791 -4.4287872 -4.428782 -4.4287686][-4.4288816 -4.4288487 -4.4288254 -4.4287996 -4.4287524 -4.428679 -4.4286103 -4.4285603 -4.4285965 -4.4286742 -4.4287176 -4.4287391 -4.428751 -4.4287539 -4.4287443][-4.4288831 -4.4288473 -4.4288163 -4.4287887 -4.4287353 -4.4286561 -4.4285817 -4.4285278 -4.4285617 -4.4286408 -4.4286823 -4.4287004 -4.4287167 -4.4287286 -4.4287233]]...]
INFO - root - 2017-12-08 06:33:51.811267: step 26810, loss = 2.28, batch loss = 2.23 (35.8 examples/sec; 0.224 sec/batch; 18h:59m:50s remains)
INFO - root - 2017-12-08 06:33:54.047953: step 26820, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.221 sec/batch; 18h:47m:34s remains)
INFO - root - 2017-12-08 06:33:56.280116: step 26830, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.225 sec/batch; 19h:07m:12s remains)
INFO - root - 2017-12-08 06:33:58.535303: step 26840, loss = 2.28, batch loss = 2.23 (34.7 examples/sec; 0.231 sec/batch; 19h:36m:10s remains)
INFO - root - 2017-12-08 06:34:00.830821: step 26850, loss = 2.28, batch loss = 2.23 (35.4 examples/sec; 0.226 sec/batch; 19h:12m:22s remains)
INFO - root - 2017-12-08 06:34:03.068505: step 26860, loss = 2.28, batch loss = 2.23 (34.3 examples/sec; 0.233 sec/batch; 19h:47m:58s remains)
INFO - root - 2017-12-08 06:34:05.332335: step 26870, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 18h:35m:05s remains)
INFO - root - 2017-12-08 06:34:07.614110: step 26880, loss = 2.28, batch loss = 2.23 (32.5 examples/sec; 0.246 sec/batch; 20h:53m:42s remains)
INFO - root - 2017-12-08 06:34:09.875236: step 26890, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.222 sec/batch; 18h:48m:19s remains)
INFO - root - 2017-12-08 06:34:12.111999: step 26900, loss = 2.28, batch loss = 2.23 (36.0 examples/sec; 0.222 sec/batch; 18h:50m:48s remains)
2017-12-08 06:34:12.424536: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289131 -4.4289126 -4.4289122 -4.4289112 -4.4289093 -4.4289069 -4.4289031 -4.4289 -4.4288988 -4.4288988 -4.4288979 -4.428896 -4.4288917 -4.4288869 -4.4288816][-4.42896 -4.4289637 -4.4289637 -4.4289622 -4.4289589 -4.4289551 -4.4289503 -4.4289479 -4.4289489 -4.4289517 -4.4289541 -4.4289551 -4.4289536 -4.4289494 -4.4289436][-4.4289856 -4.42899 -4.4289885 -4.4289832 -4.428977 -4.4289708 -4.4289665 -4.428966 -4.4289708 -4.428977 -4.4289846 -4.4289927 -4.4289947 -4.4289918 -4.428987][-4.4289746 -4.4289775 -4.4289708 -4.428957 -4.4289432 -4.4289331 -4.4289279 -4.4289327 -4.4289446 -4.4289584 -4.4289732 -4.428987 -4.4289947 -4.4289932 -4.428988][-4.4289308 -4.4289351 -4.4289246 -4.4289002 -4.428875 -4.4288564 -4.4288483 -4.4288611 -4.428885 -4.4289079 -4.4289289 -4.4289484 -4.4289608 -4.4289618 -4.4289556][-4.4288311 -4.4288383 -4.428823 -4.4287858 -4.4287391 -4.4287 -4.4286852 -4.4287114 -4.4287591 -4.428802 -4.4288321 -4.4288549 -4.4288688 -4.42887 -4.4288654][-4.42867 -4.4286828 -4.4286623 -4.428606 -4.428525 -4.4284492 -4.4284163 -4.4284587 -4.4285421 -4.42862 -4.4286704 -4.4287009 -4.4287195 -4.4287248 -4.4287238][-4.4285064 -4.4285245 -4.4285026 -4.4284296 -4.4283142 -4.4281936 -4.4281316 -4.4281893 -4.428309 -4.4284153 -4.4284821 -4.4285183 -4.4285398 -4.4285517 -4.4285579][-4.4284711 -4.4284973 -4.42849 -4.4284272 -4.4283147 -4.4281907 -4.428124 -4.4281793 -4.4282918 -4.4283853 -4.4284339 -4.4284449 -4.4284463 -4.428452 -4.4284639][-4.4285874 -4.4286189 -4.4286318 -4.4286017 -4.4285369 -4.4284649 -4.4284296 -4.4284663 -4.4285331 -4.4285784 -4.4285841 -4.428556 -4.4285254 -4.4285169 -4.4285274][-4.42871 -4.4287276 -4.4287457 -4.4287419 -4.4287229 -4.4287009 -4.4286952 -4.4287162 -4.4287453 -4.4287539 -4.428731 -4.4286828 -4.4286389 -4.4286242 -4.4286346][-4.42879 -4.4287882 -4.428793 -4.4287963 -4.4288 -4.4288044 -4.4288139 -4.4288287 -4.4288445 -4.4288454 -4.428822 -4.428781 -4.4287467 -4.4287362 -4.4287443][-4.4288158 -4.4288011 -4.4287915 -4.4287915 -4.4287982 -4.4288087 -4.4288244 -4.4288359 -4.4288464 -4.4288511 -4.428843 -4.428822 -4.4288039 -4.4288011 -4.4288087][-4.4288378 -4.4288135 -4.4287877 -4.4287734 -4.4287691 -4.4287734 -4.4287858 -4.4287915 -4.4287968 -4.4288077 -4.4288211 -4.4288297 -4.4288344 -4.428844 -4.4288549][-4.4288759 -4.4288468 -4.4288092 -4.4287748 -4.428751 -4.4287391 -4.4287376 -4.42873 -4.4287252 -4.4287405 -4.4287748 -4.4288149 -4.4288492 -4.4288783 -4.4288988]]...]
INFO - root - 2017-12-08 06:34:14.664013: step 26910, loss = 2.28, batch loss = 2.23 (36.0 examples/sec; 0.223 sec/batch; 18h:53m:22s remains)
INFO - root - 2017-12-08 06:34:16.894562: step 26920, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 18h:55m:05s remains)
INFO - root - 2017-12-08 06:34:19.147728: step 26930, loss = 2.28, batch loss = 2.23 (35.2 examples/sec; 0.227 sec/batch; 19h:17m:09s remains)
INFO - root - 2017-12-08 06:34:21.388214: step 26940, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.220 sec/batch; 18h:42m:33s remains)
INFO - root - 2017-12-08 06:34:23.650959: step 26950, loss = 2.28, batch loss = 2.23 (36.8 examples/sec; 0.217 sec/batch; 18h:25m:34s remains)
INFO - root - 2017-12-08 06:34:25.893087: step 26960, loss = 2.28, batch loss = 2.23 (34.7 examples/sec; 0.230 sec/batch; 19h:32m:59s remains)
INFO - root - 2017-12-08 06:34:28.123735: step 26970, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 18h:40m:08s remains)
INFO - root - 2017-12-08 06:34:30.392930: step 26980, loss = 2.28, batch loss = 2.23 (34.9 examples/sec; 0.229 sec/batch; 19h:26m:24s remains)
INFO - root - 2017-12-08 06:34:32.620925: step 26990, loss = 2.28, batch loss = 2.23 (32.4 examples/sec; 0.247 sec/batch; 20h:57m:55s remains)
INFO - root - 2017-12-08 06:34:34.890316: step 27000, loss = 2.28, batch loss = 2.23 (35.8 examples/sec; 0.224 sec/batch; 18h:59m:04s remains)
2017-12-08 06:34:35.193241: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4286933 -4.428731 -4.4287333 -4.4287028 -4.4286661 -4.42864 -4.4286103 -4.4285703 -4.428544 -4.42856 -4.4285936 -4.4286246 -4.4286609 -4.428709 -4.4287763][-4.428689 -4.4287305 -4.428731 -4.4286971 -4.4286575 -4.4286332 -4.4286127 -4.42858 -4.4285555 -4.4285631 -4.4285913 -4.4286327 -4.4286814 -4.4287262 -4.4287868][-4.4286914 -4.4287353 -4.4287424 -4.4287224 -4.4286938 -4.4286733 -4.4286547 -4.4286308 -4.4286141 -4.4286237 -4.4286518 -4.4286933 -4.4287424 -4.4287791 -4.428823][-4.4286852 -4.4287329 -4.4287486 -4.4287381 -4.4287124 -4.4286861 -4.4286537 -4.4286242 -4.4286208 -4.4286466 -4.4286923 -4.4287329 -4.4287782 -4.4288163 -4.4288521][-4.4286833 -4.4287262 -4.4287348 -4.42871 -4.4286656 -4.4286146 -4.4285545 -4.4285049 -4.4285049 -4.4285645 -4.4286466 -4.4286962 -4.4287462 -4.4287982 -4.4288387][-4.4286566 -4.4286914 -4.4286814 -4.4286208 -4.4285364 -4.4284482 -4.4283428 -4.428246 -4.4282389 -4.4283519 -4.4284863 -4.4285736 -4.4286547 -4.4287357 -4.428793][-4.4286218 -4.4286327 -4.4285903 -4.4284878 -4.4283624 -4.4282413 -4.4280906 -4.4279332 -4.4279189 -4.4281068 -4.4283075 -4.428443 -4.4285631 -4.42867 -4.4287443][-4.4286146 -4.4286113 -4.4285593 -4.428453 -4.4283285 -4.4282146 -4.428071 -4.4279127 -4.4278984 -4.4280829 -4.4282827 -4.4284258 -4.4285583 -4.428668 -4.4287448][-4.4286461 -4.428638 -4.428597 -4.4285197 -4.428441 -4.4283729 -4.4282837 -4.4281864 -4.4281774 -4.4282961 -4.4284306 -4.4285364 -4.4286447 -4.4287329 -4.428792][-4.4287333 -4.4287143 -4.4286737 -4.4286213 -4.4285851 -4.42855 -4.4284987 -4.4284563 -4.4284616 -4.4285283 -4.428606 -4.4286804 -4.4287519 -4.4288087 -4.4288454][-4.4288397 -4.4288216 -4.428793 -4.4287605 -4.4287424 -4.42872 -4.4286885 -4.4286671 -4.4286733 -4.4287047 -4.4287467 -4.4287968 -4.4288383 -4.4288669 -4.4288836][-4.4289131 -4.4289055 -4.4288931 -4.4288764 -4.4288678 -4.4288526 -4.4288311 -4.4288154 -4.4288177 -4.4288273 -4.4288425 -4.4288673 -4.4288869 -4.4288931 -4.4288964][-4.42895 -4.428947 -4.4289374 -4.4289289 -4.428926 -4.4289169 -4.428906 -4.4288979 -4.4289026 -4.4289021 -4.4288964 -4.4288993 -4.4289031 -4.4288955 -4.4288912][-4.4289594 -4.4289584 -4.4289489 -4.4289427 -4.4289412 -4.4289355 -4.4289284 -4.4289269 -4.428937 -4.4289336 -4.4289188 -4.4289069 -4.4288974 -4.4288859 -4.4288812][-4.4289608 -4.42896 -4.4289522 -4.4289479 -4.4289474 -4.4289441 -4.4289379 -4.4289379 -4.428947 -4.4289422 -4.4289184 -4.4288921 -4.4288754 -4.4288678 -4.4288683]]...]
INFO - root - 2017-12-08 06:34:37.412455: step 27010, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 18h:37m:17s remains)
INFO - root - 2017-12-08 06:34:39.647184: step 27020, loss = 2.28, batch loss = 2.23 (35.8 examples/sec; 0.223 sec/batch; 18h:56m:34s remains)
INFO - root - 2017-12-08 06:34:41.906443: step 27030, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.225 sec/batch; 19h:07m:46s remains)
INFO - root - 2017-12-08 06:34:44.133297: step 27040, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.225 sec/batch; 19h:07m:30s remains)
INFO - root - 2017-12-08 06:34:46.357477: step 27050, loss = 2.28, batch loss = 2.23 (34.2 examples/sec; 0.234 sec/batch; 19h:51m:13s remains)
INFO - root - 2017-12-08 06:34:48.629866: step 27060, loss = 2.28, batch loss = 2.23 (34.6 examples/sec; 0.231 sec/batch; 19h:37m:29s remains)
INFO - root - 2017-12-08 06:34:50.882862: step 27070, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.220 sec/batch; 18h:42m:01s remains)
INFO - root - 2017-12-08 06:34:53.118844: step 27080, loss = 2.28, batch loss = 2.23 (37.1 examples/sec; 0.215 sec/batch; 18h:16m:50s remains)
INFO - root - 2017-12-08 06:34:55.342265: step 27090, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.226 sec/batch; 19h:08m:05s remains)
INFO - root - 2017-12-08 06:34:57.579269: step 27100, loss = 2.28, batch loss = 2.23 (34.6 examples/sec; 0.231 sec/batch; 19h:37m:16s remains)
2017-12-08 06:34:57.861404: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289021 -4.4289026 -4.4288993 -4.4288926 -4.4288845 -4.4288855 -4.4288964 -4.428906 -4.4289103 -4.4289083 -4.4289079 -4.4289131 -4.4289207 -4.4289222 -4.4289155][-4.4288788 -4.428884 -4.42888 -4.428863 -4.4288435 -4.4288435 -4.42886 -4.428875 -4.42888 -4.4288759 -4.4288745 -4.4288845 -4.4289012 -4.4289107 -4.4289041][-4.4288692 -4.4288783 -4.4288721 -4.4288406 -4.4288044 -4.4287972 -4.4288187 -4.4288363 -4.4288397 -4.42883 -4.4288278 -4.4288425 -4.4288683 -4.4288878 -4.42889][-4.4288712 -4.4288783 -4.4288692 -4.4288292 -4.4287758 -4.4287562 -4.4287806 -4.4288039 -4.4288073 -4.4287925 -4.4287844 -4.428802 -4.4288387 -4.428874 -4.4288917][-4.4288678 -4.4288716 -4.4288597 -4.4288125 -4.4287524 -4.4287171 -4.4287333 -4.42876 -4.4287658 -4.428751 -4.428741 -4.4287615 -4.4288087 -4.4288583 -4.4288917][-4.4288607 -4.4288635 -4.42885 -4.4287992 -4.4287353 -4.428689 -4.4286904 -4.4287167 -4.4287286 -4.42872 -4.4287095 -4.42873 -4.4287839 -4.4288397 -4.4288816][-4.4288535 -4.4288521 -4.4288392 -4.4287934 -4.4287319 -4.4286795 -4.4286675 -4.4286962 -4.428721 -4.4287181 -4.4287014 -4.4287167 -4.4287724 -4.4288273 -4.4288654][-4.4288316 -4.4288268 -4.4288206 -4.4287839 -4.4287314 -4.428688 -4.4286766 -4.4287109 -4.4287524 -4.4287553 -4.4287291 -4.4287271 -4.4287715 -4.4288187 -4.4288473][-4.4288058 -4.4287992 -4.4288011 -4.4287806 -4.4287491 -4.4287252 -4.4287195 -4.42875 -4.4287953 -4.4288125 -4.4287949 -4.4287825 -4.4288044 -4.4288321 -4.4288492][-4.4287939 -4.4287887 -4.42879 -4.4287748 -4.4287624 -4.4287586 -4.4287624 -4.4287815 -4.4288192 -4.4288478 -4.4288507 -4.4288383 -4.4288416 -4.4288459 -4.4288468][-4.4287982 -4.428791 -4.428792 -4.428772 -4.4287653 -4.428772 -4.4287772 -4.428792 -4.4288235 -4.428853 -4.4288692 -4.4288635 -4.4288559 -4.4288449 -4.4288316][-4.4288225 -4.4288073 -4.4287992 -4.4287782 -4.4287643 -4.4287634 -4.4287591 -4.4287686 -4.4287977 -4.4288278 -4.4288516 -4.4288497 -4.42884 -4.4288287 -4.4288149][-4.4288425 -4.4288273 -4.4288106 -4.4287872 -4.4287667 -4.4287543 -4.4287429 -4.4287438 -4.428761 -4.4287863 -4.4288182 -4.4288268 -4.4288263 -4.4288273 -4.4288192][-4.428843 -4.4288335 -4.4288278 -4.4288116 -4.4287863 -4.4287634 -4.4287424 -4.4287271 -4.4287276 -4.4287395 -4.4287682 -4.4287877 -4.4288073 -4.4288321 -4.4288349][-4.4288411 -4.4288349 -4.4288445 -4.4288473 -4.4288273 -4.4287915 -4.4287534 -4.4287233 -4.4287195 -4.4287219 -4.4287324 -4.4287467 -4.4287767 -4.4288211 -4.4288411]]...]
INFO - root - 2017-12-08 06:35:00.093724: step 27110, loss = 2.28, batch loss = 2.23 (35.4 examples/sec; 0.226 sec/batch; 19h:09m:06s remains)
INFO - root - 2017-12-08 06:35:02.354766: step 27120, loss = 2.28, batch loss = 2.23 (34.6 examples/sec; 0.231 sec/batch; 19h:38m:08s remains)
INFO - root - 2017-12-08 06:35:04.596222: step 27130, loss = 2.28, batch loss = 2.23 (35.7 examples/sec; 0.224 sec/batch; 18h:59m:18s remains)
INFO - root - 2017-12-08 06:35:06.834679: step 27140, loss = 2.28, batch loss = 2.23 (35.1 examples/sec; 0.228 sec/batch; 19h:18m:25s remains)
INFO - root - 2017-12-08 06:35:09.122438: step 27150, loss = 2.28, batch loss = 2.23 (35.4 examples/sec; 0.226 sec/batch; 19h:11m:16s remains)
INFO - root - 2017-12-08 06:35:11.385491: step 27160, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.225 sec/batch; 19h:07m:07s remains)
INFO - root - 2017-12-08 06:35:13.614109: step 27170, loss = 2.28, batch loss = 2.23 (34.9 examples/sec; 0.229 sec/batch; 19h:27m:30s remains)
INFO - root - 2017-12-08 06:35:15.838234: step 27180, loss = 2.28, batch loss = 2.23 (35.1 examples/sec; 0.228 sec/batch; 19h:18m:49s remains)
INFO - root - 2017-12-08 06:35:18.052449: step 27190, loss = 2.28, batch loss = 2.23 (36.6 examples/sec; 0.218 sec/batch; 18h:31m:49s remains)
INFO - root - 2017-12-08 06:35:20.310278: step 27200, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 18h:43m:31s remains)
2017-12-08 06:35:20.616288: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289079 -4.4289083 -4.4289179 -4.4289322 -4.4289393 -4.4289436 -4.428947 -4.4289484 -4.4289479 -4.4289494 -4.4289513 -4.4289536 -4.4289546 -4.4289522 -4.4289479][-4.4288607 -4.4288731 -4.4288964 -4.4289174 -4.4289222 -4.4289222 -4.4289217 -4.4289203 -4.4289246 -4.4289365 -4.428947 -4.4289536 -4.4289551 -4.42895 -4.4289408][-4.4288163 -4.4288487 -4.428885 -4.4289017 -4.4288917 -4.428874 -4.4288573 -4.4288459 -4.4288588 -4.4288898 -4.4289184 -4.4289341 -4.4289384 -4.4289312 -4.4289193][-4.4287934 -4.4288435 -4.4288778 -4.4288754 -4.4288373 -4.4287877 -4.4287381 -4.4287071 -4.4287314 -4.4287958 -4.4288568 -4.4288898 -4.4289012 -4.4288964 -4.4288864][-4.4287863 -4.428843 -4.4288616 -4.4288325 -4.4287591 -4.4286637 -4.4285593 -4.4284859 -4.4285169 -4.4286323 -4.4287429 -4.4288087 -4.4288363 -4.4288392 -4.4288387][-4.4287944 -4.4288473 -4.4288468 -4.4287915 -4.428688 -4.4285412 -4.4283571 -4.4282079 -4.4282355 -4.4284229 -4.4286022 -4.428709 -4.4287581 -4.4287696 -4.4287791][-4.4288044 -4.42885 -4.4288383 -4.4287691 -4.4286509 -4.4284687 -4.4282103 -4.4279761 -4.4279933 -4.4282575 -4.4284964 -4.4286284 -4.4286819 -4.4286928 -4.4287062][-4.4288197 -4.42886 -4.428843 -4.4287729 -4.4286585 -4.4284763 -4.4282107 -4.4279628 -4.4279814 -4.4282594 -4.4284973 -4.4286184 -4.4286485 -4.4286389 -4.4286408][-4.4288373 -4.42887 -4.42885 -4.4287915 -4.4287057 -4.4285722 -4.4283781 -4.428205 -4.4282236 -4.4284229 -4.4285903 -4.4286556 -4.4286356 -4.4285893 -4.4285727][-4.4288368 -4.4288607 -4.4288425 -4.4287963 -4.4287448 -4.4286723 -4.4285636 -4.4284658 -4.42848 -4.42859 -4.4286633 -4.4286566 -4.4285827 -4.4284949 -4.4284639][-4.4288259 -4.4288459 -4.428833 -4.4287982 -4.4287672 -4.4287343 -4.4286838 -4.42864 -4.4286489 -4.4286895 -4.4286766 -4.4286013 -4.4284797 -4.4283562 -4.4283218][-4.4288087 -4.4288278 -4.428822 -4.4287968 -4.4287782 -4.4287667 -4.4287448 -4.428721 -4.42872 -4.4287243 -4.4286656 -4.4285517 -4.4283943 -4.4282436 -4.4282036][-4.428802 -4.4288173 -4.428812 -4.4287963 -4.4287953 -4.4288039 -4.4287915 -4.4287639 -4.4287481 -4.4287276 -4.4286513 -4.428534 -4.4283838 -4.4282455 -4.4282093][-4.4288154 -4.4288239 -4.4288125 -4.428803 -4.4288154 -4.4288316 -4.4288192 -4.4287872 -4.4287634 -4.4287324 -4.4286585 -4.4285612 -4.4284487 -4.4283581 -4.4283414][-4.4288354 -4.428834 -4.4288173 -4.4288197 -4.428843 -4.4288564 -4.4288421 -4.4288015 -4.4287744 -4.4287486 -4.4286885 -4.42862 -4.4285464 -4.4284973 -4.4284968]]...]
INFO - root - 2017-12-08 06:35:22.869509: step 27210, loss = 2.28, batch loss = 2.23 (35.2 examples/sec; 0.227 sec/batch; 19h:16m:40s remains)
INFO - root - 2017-12-08 06:35:25.102063: step 27220, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 18h:45m:14s remains)
INFO - root - 2017-12-08 06:35:27.337318: step 27230, loss = 2.28, batch loss = 2.23 (35.1 examples/sec; 0.228 sec/batch; 19h:21m:15s remains)
INFO - root - 2017-12-08 06:35:29.595089: step 27240, loss = 2.28, batch loss = 2.23 (33.8 examples/sec; 0.237 sec/batch; 20h:04m:43s remains)
INFO - root - 2017-12-08 06:35:31.831518: step 27250, loss = 2.28, batch loss = 2.23 (35.1 examples/sec; 0.228 sec/batch; 19h:20m:00s remains)
INFO - root - 2017-12-08 06:35:34.072404: step 27260, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 18h:52m:17s remains)
INFO - root - 2017-12-08 06:35:36.286385: step 27270, loss = 2.28, batch loss = 2.23 (37.5 examples/sec; 0.213 sec/batch; 18h:05m:00s remains)
INFO - root - 2017-12-08 06:35:38.583962: step 27280, loss = 2.28, batch loss = 2.23 (33.7 examples/sec; 0.238 sec/batch; 20h:08m:37s remains)
INFO - root - 2017-12-08 06:35:40.847901: step 27290, loss = 2.28, batch loss = 2.23 (35.1 examples/sec; 0.228 sec/batch; 19h:20m:32s remains)
INFO - root - 2017-12-08 06:35:43.094743: step 27300, loss = 2.28, batch loss = 2.23 (35.8 examples/sec; 0.224 sec/batch; 18h:57m:39s remains)
2017-12-08 06:35:43.370007: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289575 -4.4289517 -4.428946 -4.4289289 -4.4289055 -4.4288812 -4.4288607 -4.4288511 -4.4288549 -4.4288697 -4.4288869 -4.4289017 -4.42891 -4.4289126 -4.4289269][-4.4289637 -4.4289608 -4.4289632 -4.4289556 -4.4289331 -4.4289112 -4.4289002 -4.4289 -4.4289103 -4.4289231 -4.4289289 -4.4289351 -4.42894 -4.4289331 -4.4289317][-4.4289641 -4.4289579 -4.4289594 -4.4289541 -4.4289317 -4.4289174 -4.428916 -4.4289246 -4.42894 -4.4289517 -4.4289513 -4.4289494 -4.4289489 -4.42893 -4.4289155][-4.4289103 -4.4289055 -4.4289107 -4.428906 -4.4288864 -4.4288769 -4.4288735 -4.428875 -4.4288912 -4.4289103 -4.4289107 -4.4289112 -4.4289136 -4.4288898 -4.4288597][-4.4287987 -4.4287987 -4.4288049 -4.4287987 -4.4287767 -4.4287586 -4.42873 -4.4287171 -4.4287486 -4.4287925 -4.4288082 -4.4288168 -4.4288292 -4.4288049 -4.4287629][-4.4286637 -4.4286547 -4.4286423 -4.4286079 -4.4285622 -4.4285159 -4.4284363 -4.4283838 -4.4284472 -4.4285431 -4.4285951 -4.4286222 -4.4286547 -4.4286509 -4.42863][-4.4286013 -4.4285626 -4.4284978 -4.4284086 -4.4283242 -4.4282317 -4.4280829 -4.42797 -4.4280763 -4.4282475 -4.428349 -4.4284139 -4.4284816 -4.428515 -4.42853][-4.4286718 -4.4286232 -4.4285307 -4.4284205 -4.4283309 -4.4282446 -4.4281149 -4.4280167 -4.4281135 -4.4282708 -4.4283667 -4.4284329 -4.4284992 -4.4285417 -4.4285669][-4.4287734 -4.4287457 -4.4286876 -4.4286156 -4.4285612 -4.42851 -4.4284444 -4.4283938 -4.4284406 -4.4285188 -4.4285636 -4.4285994 -4.4286366 -4.4286566 -4.4286585][-4.4288135 -4.4288077 -4.4287896 -4.4287562 -4.4287338 -4.4287105 -4.4286838 -4.428658 -4.4286652 -4.4286804 -4.4286823 -4.428689 -4.4287019 -4.4286976 -4.4286766][-4.4287496 -4.4287643 -4.4287896 -4.4287949 -4.4287968 -4.428792 -4.4287839 -4.4287739 -4.4287715 -4.4287624 -4.4287491 -4.4287443 -4.428741 -4.4287267 -4.4286995][-4.4286203 -4.4286528 -4.4287119 -4.4287572 -4.4287877 -4.4288011 -4.4288073 -4.4288111 -4.4288116 -4.4287996 -4.42879 -4.428791 -4.4287896 -4.4287786 -4.4287572][-4.4285421 -4.4285712 -4.42864 -4.4287028 -4.4287453 -4.4287791 -4.4288073 -4.4288297 -4.4288449 -4.4288411 -4.4288425 -4.4288545 -4.428863 -4.4288573 -4.4288397][-4.42857 -4.4285874 -4.4286427 -4.4286971 -4.428731 -4.4287686 -4.4288087 -4.4288416 -4.4288669 -4.4288731 -4.4288845 -4.428905 -4.4289236 -4.4289203 -4.4288974][-4.4286609 -4.4286675 -4.4287047 -4.4287472 -4.4287734 -4.4288049 -4.4288416 -4.4288735 -4.4289041 -4.4289169 -4.4289308 -4.4289551 -4.4289708 -4.428956 -4.4289231]]...]
INFO - root - 2017-12-08 06:35:45.597737: step 27310, loss = 2.28, batch loss = 2.23 (34.9 examples/sec; 0.229 sec/batch; 19h:26m:07s remains)
INFO - root - 2017-12-08 06:35:47.851524: step 27320, loss = 2.28, batch loss = 2.23 (35.1 examples/sec; 0.228 sec/batch; 19h:20m:13s remains)
INFO - root - 2017-12-08 06:35:50.083670: step 27330, loss = 2.28, batch loss = 2.23 (37.1 examples/sec; 0.216 sec/batch; 18h:16m:29s remains)
INFO - root - 2017-12-08 06:35:52.321115: step 27340, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 18h:38m:45s remains)
INFO - root - 2017-12-08 06:35:54.586249: step 27350, loss = 2.28, batch loss = 2.23 (36.0 examples/sec; 0.222 sec/batch; 18h:50m:35s remains)
INFO - root - 2017-12-08 06:35:56.811645: step 27360, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.222 sec/batch; 18h:46m:34s remains)
INFO - root - 2017-12-08 06:35:59.045292: step 27370, loss = 2.28, batch loss = 2.23 (35.8 examples/sec; 0.224 sec/batch; 18h:57m:47s remains)
INFO - root - 2017-12-08 06:36:01.269312: step 27380, loss = 2.28, batch loss = 2.23 (35.8 examples/sec; 0.223 sec/batch; 18h:56m:31s remains)
INFO - root - 2017-12-08 06:36:03.506919: step 27390, loss = 2.28, batch loss = 2.23 (34.8 examples/sec; 0.230 sec/batch; 19h:30m:06s remains)
INFO - root - 2017-12-08 06:36:05.746978: step 27400, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.220 sec/batch; 18h:39m:12s remains)
2017-12-08 06:36:06.056432: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289122 -4.4288993 -4.4288864 -4.4288559 -4.4288235 -4.4287877 -4.4287739 -4.4287734 -4.4287987 -4.4288235 -4.4288335 -4.4288163 -4.4287572 -4.4286985 -4.42866][-4.4288931 -4.4288688 -4.428853 -4.428822 -4.4287872 -4.4287486 -4.4287353 -4.4287405 -4.4287772 -4.4288111 -4.4288292 -4.4288211 -4.4287496 -4.4286547 -4.4285793][-4.4289026 -4.4288688 -4.4288468 -4.4288154 -4.428781 -4.428741 -4.4287243 -4.4287376 -4.4287791 -4.4288087 -4.4288297 -4.4288368 -4.4287758 -4.4286685 -4.4285703][-4.428925 -4.4288855 -4.4288521 -4.4288163 -4.428781 -4.4287424 -4.4287076 -4.4287138 -4.4287515 -4.4287806 -4.4288034 -4.4288211 -4.428793 -4.42872 -4.4286304][-4.4289484 -4.4289103 -4.4288707 -4.4288282 -4.4287887 -4.4287443 -4.4286919 -4.4286723 -4.4286866 -4.4287109 -4.4287424 -4.4287777 -4.4287915 -4.4287734 -4.4287214][-4.4289532 -4.4289141 -4.4288669 -4.4288149 -4.4287648 -4.4287152 -4.4286513 -4.4285989 -4.4285803 -4.4286003 -4.4286485 -4.4287047 -4.4287624 -4.4287891 -4.4287705][-4.4289207 -4.4288697 -4.4288077 -4.4287395 -4.4286766 -4.4286122 -4.4285231 -4.4284248 -4.4283643 -4.4283915 -4.4284792 -4.4285827 -4.4286757 -4.4287271 -4.4287353][-4.4288626 -4.42879 -4.4287057 -4.42862 -4.42855 -4.4284706 -4.428349 -4.4281936 -4.4280906 -4.4281478 -4.4283004 -4.4284525 -4.4285603 -4.4286175 -4.4286466][-4.4287977 -4.4287043 -4.4286051 -4.4285173 -4.4284582 -4.4283843 -4.4282513 -4.4280705 -4.4279494 -4.4280362 -4.4282255 -4.4283886 -4.4284787 -4.42853 -4.4285679][-4.4287691 -4.4286723 -4.4285879 -4.4285312 -4.4285007 -4.4284434 -4.4283309 -4.4281936 -4.4281163 -4.4281917 -4.42833 -4.4284434 -4.428503 -4.4285507 -4.4285808][-4.4287963 -4.428719 -4.4286671 -4.4286494 -4.4286461 -4.4286075 -4.4285307 -4.4284511 -4.4284191 -4.4284644 -4.428535 -4.4285955 -4.4286327 -4.428669 -4.4286795][-4.4288573 -4.4288111 -4.428792 -4.4287982 -4.4288111 -4.4287953 -4.4287524 -4.4287114 -4.4287019 -4.4287262 -4.4287524 -4.42878 -4.4287992 -4.4288168 -4.4288177][-4.4289227 -4.4289074 -4.4289088 -4.4289222 -4.428936 -4.4289322 -4.4289112 -4.42889 -4.4288859 -4.4288993 -4.4289041 -4.428915 -4.428926 -4.4289312 -4.42893][-4.4289603 -4.4289641 -4.4289761 -4.4289894 -4.4289956 -4.4289923 -4.4289842 -4.4289746 -4.4289746 -4.4289813 -4.42898 -4.4289808 -4.428988 -4.4289923 -4.428988][-4.4289789 -4.4289846 -4.4289956 -4.4290066 -4.4290085 -4.4290056 -4.4290028 -4.4290013 -4.4290018 -4.4290047 -4.4290028 -4.4290004 -4.4290028 -4.4290047 -4.4289989]]...]
INFO - root - 2017-12-08 06:36:08.299381: step 27410, loss = 2.28, batch loss = 2.23 (35.3 examples/sec; 0.227 sec/batch; 19h:12m:08s remains)
INFO - root - 2017-12-08 06:36:10.517793: step 27420, loss = 2.28, batch loss = 2.23 (36.9 examples/sec; 0.217 sec/batch; 18h:22m:54s remains)
INFO - root - 2017-12-08 06:36:12.770217: step 27430, loss = 2.28, batch loss = 2.23 (35.0 examples/sec; 0.228 sec/batch; 19h:20m:52s remains)
INFO - root - 2017-12-08 06:36:14.993603: step 27440, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 18h:52m:06s remains)
INFO - root - 2017-12-08 06:36:17.233238: step 27450, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.225 sec/batch; 19h:04m:37s remains)
INFO - root - 2017-12-08 06:36:19.458900: step 27460, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 18h:35m:01s remains)
INFO - root - 2017-12-08 06:36:21.693083: step 27470, loss = 2.28, batch loss = 2.23 (34.5 examples/sec; 0.232 sec/batch; 19h:39m:10s remains)
INFO - root - 2017-12-08 06:36:23.952436: step 27480, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 18h:36m:41s remains)
INFO - root - 2017-12-08 06:36:26.214019: step 27490, loss = 2.28, batch loss = 2.23 (35.2 examples/sec; 0.228 sec/batch; 19h:16m:41s remains)
INFO - root - 2017-12-08 06:36:28.437307: step 27500, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 18h:37m:39s remains)
2017-12-08 06:36:28.741753: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289365 -4.4289217 -4.4288821 -4.4288192 -4.4287519 -4.4287186 -4.428731 -4.4287643 -4.4288063 -4.4288363 -4.4288478 -4.4288478 -4.4288516 -4.4288545 -4.4288459][-4.4289312 -4.4289212 -4.4288831 -4.4288154 -4.4287415 -4.428699 -4.4287052 -4.4287333 -4.4287767 -4.4288163 -4.4288316 -4.4288297 -4.4288354 -4.4288416 -4.4288297][-4.4289188 -4.428916 -4.4288859 -4.42882 -4.4287467 -4.4286981 -4.4286861 -4.4286947 -4.4287305 -4.4287782 -4.4287982 -4.4287949 -4.4287977 -4.4288073 -4.4287896][-4.4289207 -4.4289265 -4.4289064 -4.4288454 -4.4287748 -4.4287152 -4.4286695 -4.4286408 -4.4286575 -4.4287148 -4.4287505 -4.42876 -4.4287777 -4.4287963 -4.4287772][-4.4289284 -4.4289417 -4.4289289 -4.4288721 -4.4288011 -4.428719 -4.4286175 -4.4285326 -4.4285283 -4.4286122 -4.4286861 -4.4287286 -4.4287705 -4.4288049 -4.4287944][-4.4289336 -4.4289532 -4.4289441 -4.4288845 -4.4288054 -4.4286938 -4.4285278 -4.4283638 -4.4283385 -4.4284787 -4.4286194 -4.4287128 -4.4287844 -4.4288263 -4.4288187][-4.4289374 -4.4289565 -4.4289441 -4.4288754 -4.4287806 -4.4286394 -4.4284091 -4.4281511 -4.4281125 -4.4283457 -4.4285693 -4.4287124 -4.4287982 -4.4288344 -4.4288206][-4.4289446 -4.42896 -4.4289417 -4.4288673 -4.4287663 -4.4286065 -4.4283361 -4.4280128 -4.427969 -4.4282808 -4.4285574 -4.4287205 -4.4288 -4.428822 -4.4288039][-4.4289536 -4.4289637 -4.42894 -4.4288664 -4.4287748 -4.4286313 -4.4283886 -4.4281087 -4.4280734 -4.4283485 -4.4285975 -4.4287386 -4.428792 -4.4287987 -4.4287858][-4.4289589 -4.4289675 -4.4289432 -4.4288807 -4.4288111 -4.4287043 -4.428534 -4.4283452 -4.4283137 -4.4284854 -4.4286537 -4.4287496 -4.4287767 -4.4287744 -4.4287767][-4.4289589 -4.4289737 -4.428957 -4.4289122 -4.4288716 -4.4288063 -4.4287014 -4.4285827 -4.42854 -4.428618 -4.4287148 -4.4287739 -4.4287877 -4.4287887 -4.428802][-4.428957 -4.428978 -4.4289675 -4.4289432 -4.42893 -4.4289002 -4.428843 -4.428761 -4.4287081 -4.4287314 -4.428782 -4.428822 -4.4288325 -4.4288368 -4.4288473][-4.4289546 -4.4289794 -4.428978 -4.4289684 -4.4289746 -4.42897 -4.4289389 -4.4288774 -4.4288268 -4.4288263 -4.4288559 -4.4288869 -4.4289 -4.4289064 -4.42891][-4.4289589 -4.4289856 -4.4289918 -4.4289908 -4.4290066 -4.4290113 -4.4289918 -4.4289427 -4.4289031 -4.4288983 -4.428915 -4.4289365 -4.42895 -4.4289575 -4.4289575][-4.4289656 -4.4289889 -4.428997 -4.4289989 -4.4290123 -4.4290171 -4.4290037 -4.4289665 -4.428936 -4.4289293 -4.4289346 -4.4289446 -4.4289546 -4.4289627 -4.42896]]...]
INFO - root - 2017-12-08 06:36:31.014796: step 27510, loss = 2.28, batch loss = 2.23 (35.1 examples/sec; 0.228 sec/batch; 19h:19m:25s remains)
INFO - root - 2017-12-08 06:36:33.252143: step 27520, loss = 2.28, batch loss = 2.23 (34.1 examples/sec; 0.235 sec/batch; 19h:52m:47s remains)
INFO - root - 2017-12-08 06:36:35.497603: step 27530, loss = 2.28, batch loss = 2.23 (35.7 examples/sec; 0.224 sec/batch; 18h:59m:40s remains)
INFO - root - 2017-12-08 06:36:37.736346: step 27540, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 18h:44m:08s remains)
INFO - root - 2017-12-08 06:36:40.024997: step 27550, loss = 2.28, batch loss = 2.23 (34.6 examples/sec; 0.232 sec/batch; 19h:36m:50s remains)
INFO - root - 2017-12-08 06:36:42.258709: step 27560, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.225 sec/batch; 19h:05m:49s remains)
INFO - root - 2017-12-08 06:36:44.494933: step 27570, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.220 sec/batch; 18h:39m:10s remains)
INFO - root - 2017-12-08 06:36:46.725176: step 27580, loss = 2.28, batch loss = 2.23 (34.6 examples/sec; 0.231 sec/batch; 19h:34m:14s remains)
INFO - root - 2017-12-08 06:36:48.942041: step 27590, loss = 2.28, batch loss = 2.23 (36.7 examples/sec; 0.218 sec/batch; 18h:29m:07s remains)
INFO - root - 2017-12-08 06:36:51.200385: step 27600, loss = 2.28, batch loss = 2.23 (35.4 examples/sec; 0.226 sec/batch; 19h:08m:38s remains)
2017-12-08 06:36:51.507576: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4287596 -4.4287753 -4.4287758 -4.4287777 -4.4288015 -4.4288325 -4.4288564 -4.4288659 -4.4288135 -4.42873 -4.4287004 -4.4287324 -4.4287729 -4.4288082 -4.4287958][-4.4287615 -4.4287715 -4.4287653 -4.4287667 -4.428793 -4.4288263 -4.4288554 -4.428884 -4.4288683 -4.428813 -4.4287953 -4.428823 -4.428854 -4.4288764 -4.428865][-4.4287438 -4.4287448 -4.4287314 -4.4287324 -4.4287577 -4.4287968 -4.4288344 -4.4288769 -4.4288912 -4.4288669 -4.4288568 -4.4288731 -4.4288869 -4.4288912 -4.4288859][-4.4287229 -4.4287152 -4.4287033 -4.4287024 -4.428721 -4.4287596 -4.4288015 -4.4288435 -4.4288669 -4.4288568 -4.4288383 -4.4288359 -4.4288378 -4.4288416 -4.4288535][-4.4287066 -4.4286909 -4.4286819 -4.4286776 -4.4286838 -4.42872 -4.4287577 -4.4287891 -4.4288006 -4.42878 -4.4287391 -4.4287186 -4.42872 -4.4287386 -4.4287758][-4.4286904 -4.4286718 -4.4286556 -4.4286284 -4.4286242 -4.4286623 -4.4287057 -4.4287248 -4.4287143 -4.4286671 -4.4286032 -4.4285736 -4.4285884 -4.4286351 -4.4286966][-4.4286594 -4.4286466 -4.4286184 -4.4285707 -4.4285655 -4.4286227 -4.4286709 -4.4286661 -4.4286275 -4.4285479 -4.4284573 -4.428422 -4.42846 -4.428544 -4.4286356][-4.4286218 -4.4286232 -4.4286 -4.4285626 -4.4285707 -4.4286389 -4.4286785 -4.4286528 -4.4285903 -4.4284811 -4.4283671 -4.4283333 -4.428401 -4.4285154 -4.4286246][-4.428576 -4.4286127 -4.42862 -4.4286065 -4.4286289 -4.4286819 -4.4287052 -4.4286714 -4.4286041 -4.4285011 -4.4283924 -4.428371 -4.4284453 -4.428556 -4.428658][-4.4285684 -4.4286294 -4.4286566 -4.4286652 -4.4286947 -4.4287252 -4.428731 -4.4287057 -4.4286628 -4.4285975 -4.4285297 -4.4285192 -4.4285755 -4.4286523 -4.428721][-4.4286451 -4.4286971 -4.42872 -4.4287281 -4.4287481 -4.428761 -4.4287581 -4.4287519 -4.4287381 -4.4287162 -4.4286895 -4.4286857 -4.4287148 -4.42875 -4.428782][-4.4287443 -4.4287705 -4.4287791 -4.4287844 -4.4288044 -4.4288211 -4.4288225 -4.4288235 -4.4288235 -4.4288249 -4.428823 -4.4288177 -4.4288211 -4.4288282 -4.428834][-4.4288378 -4.4288449 -4.4288473 -4.4288511 -4.42887 -4.428894 -4.4289041 -4.4289045 -4.4289031 -4.4289026 -4.4289036 -4.428895 -4.428885 -4.4288797 -4.428874][-4.4288921 -4.428896 -4.4289002 -4.428905 -4.4289136 -4.4289308 -4.4289412 -4.42894 -4.4289379 -4.4289379 -4.428936 -4.4289289 -4.4289188 -4.4289122 -4.4289055][-4.4289179 -4.4289236 -4.428925 -4.4289241 -4.4289236 -4.4289327 -4.4289441 -4.4289479 -4.4289479 -4.4289446 -4.42894 -4.4289336 -4.4289274 -4.4289236 -4.4289222]]...]
INFO - root - 2017-12-08 06:36:53.731771: step 27610, loss = 2.28, batch loss = 2.23 (37.4 examples/sec; 0.214 sec/batch; 18h:06m:54s remains)
INFO - root - 2017-12-08 06:36:55.982834: step 27620, loss = 2.28, batch loss = 2.23 (34.7 examples/sec; 0.230 sec/batch; 19h:30m:25s remains)
INFO - root - 2017-12-08 06:36:58.224324: step 27630, loss = 2.28, batch loss = 2.23 (35.0 examples/sec; 0.229 sec/batch; 19h:22m:21s remains)
INFO - root - 2017-12-08 06:37:00.460311: step 27640, loss = 2.28, batch loss = 2.23 (35.0 examples/sec; 0.228 sec/batch; 19h:20m:07s remains)
INFO - root - 2017-12-08 06:37:02.734736: step 27650, loss = 2.28, batch loss = 2.23 (36.7 examples/sec; 0.218 sec/batch; 18h:29m:01s remains)
INFO - root - 2017-12-08 06:37:04.980415: step 27660, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.222 sec/batch; 18h:46m:21s remains)
INFO - root - 2017-12-08 06:37:07.213698: step 27670, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 18h:36m:27s remains)
INFO - root - 2017-12-08 06:37:09.451689: step 27680, loss = 2.28, batch loss = 2.23 (35.7 examples/sec; 0.224 sec/batch; 18h:59m:42s remains)
INFO - root - 2017-12-08 06:37:11.675126: step 27690, loss = 2.28, batch loss = 2.23 (35.7 examples/sec; 0.224 sec/batch; 18h:58m:08s remains)
INFO - root - 2017-12-08 06:37:13.932395: step 27700, loss = 2.28, batch loss = 2.23 (34.0 examples/sec; 0.235 sec/batch; 19h:54m:15s remains)
2017-12-08 06:37:14.213320: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288836 -4.4288259 -4.428781 -4.4287562 -4.4287429 -4.4287052 -4.42867 -4.42867 -4.4287128 -4.4287519 -4.4287848 -4.4288044 -4.428833 -4.428854 -4.4288263][-4.428895 -4.4288445 -4.4288006 -4.4287739 -4.4287586 -4.4287195 -4.4286866 -4.428699 -4.4287481 -4.4287934 -4.42883 -4.4288526 -4.4288726 -4.4288774 -4.4288344][-4.4289093 -4.428863 -4.4288158 -4.4287777 -4.4287496 -4.4287033 -4.42867 -4.4286876 -4.4287429 -4.4288054 -4.4288578 -4.4288888 -4.4289117 -4.4289126 -4.428863][-4.4289265 -4.4288774 -4.4288225 -4.4287682 -4.4287205 -4.4286613 -4.428627 -4.4286408 -4.4286952 -4.4287729 -4.428844 -4.4288898 -4.4289222 -4.42893 -4.4288869][-4.4289336 -4.428875 -4.4288077 -4.4287372 -4.428669 -4.4285951 -4.4285417 -4.428535 -4.4285827 -4.4286728 -4.4287672 -4.4288354 -4.4288836 -4.4289007 -4.4288735][-4.4289227 -4.4288497 -4.4287696 -4.4286857 -4.428597 -4.4284968 -4.4284034 -4.4283662 -4.42842 -4.4285283 -4.4286466 -4.4287419 -4.428813 -4.428844 -4.4288259][-4.4288964 -4.4288049 -4.4287109 -4.4286141 -4.4285073 -4.4283729 -4.4282184 -4.428133 -4.4282026 -4.4283462 -4.4284911 -4.4286089 -4.4287014 -4.4287477 -4.4287386][-4.428854 -4.4287419 -4.428627 -4.4285116 -4.4283867 -4.4282303 -4.4280381 -4.4279137 -4.4280162 -4.4282126 -4.4283872 -4.4285135 -4.4286065 -4.428658 -4.4286618][-4.4288125 -4.4286847 -4.4285512 -4.4284258 -4.4283109 -4.4281907 -4.4280577 -4.4279833 -4.4280877 -4.4282756 -4.4284372 -4.428545 -4.4286208 -4.42866 -4.428669][-4.4288063 -4.4286871 -4.4285626 -4.4284511 -4.4283638 -4.428297 -4.4282489 -4.4282432 -4.4283304 -4.4284658 -4.4285827 -4.428659 -4.4287133 -4.42874 -4.4287477][-4.4288287 -4.4287362 -4.4286408 -4.4285612 -4.4285073 -4.4284725 -4.428473 -4.428503 -4.4285741 -4.4286613 -4.4287357 -4.4287839 -4.4288235 -4.4288449 -4.4288511][-4.4288521 -4.4287896 -4.4287305 -4.4286919 -4.428678 -4.4286723 -4.4286938 -4.4287329 -4.4287829 -4.4288349 -4.428874 -4.4288979 -4.4289222 -4.428937 -4.4289379][-4.4288712 -4.4288349 -4.4288096 -4.4288096 -4.4288263 -4.42884 -4.4288626 -4.42889 -4.4289136 -4.4289384 -4.4289532 -4.4289603 -4.4289727 -4.4289832 -4.4289818][-4.4288836 -4.4288597 -4.4288478 -4.428864 -4.428896 -4.4289236 -4.428946 -4.4289603 -4.428966 -4.4289751 -4.4289789 -4.4289784 -4.4289813 -4.4289842 -4.4289832][-4.4288826 -4.428864 -4.4288526 -4.4288692 -4.4289002 -4.42893 -4.4289551 -4.4289684 -4.4289703 -4.4289751 -4.4289751 -4.4289703 -4.4289675 -4.4289665 -4.4289641]]...]
INFO - root - 2017-12-08 06:37:16.411174: step 27710, loss = 2.28, batch loss = 2.23 (36.9 examples/sec; 0.217 sec/batch; 18h:20m:20s remains)
INFO - root - 2017-12-08 06:37:18.672272: step 27720, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 18h:43m:06s remains)
INFO - root - 2017-12-08 06:37:20.895062: step 27730, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.226 sec/batch; 19h:05m:35s remains)
INFO - root - 2017-12-08 06:37:23.188586: step 27740, loss = 2.28, batch loss = 2.23 (34.8 examples/sec; 0.230 sec/batch; 19h:27m:33s remains)
INFO - root - 2017-12-08 06:37:25.408645: step 27750, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 18h:33m:51s remains)
INFO - root - 2017-12-08 06:37:27.642280: step 27760, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 18h:51m:12s remains)
INFO - root - 2017-12-08 06:37:29.880063: step 27770, loss = 2.28, batch loss = 2.23 (35.3 examples/sec; 0.227 sec/batch; 19h:10m:31s remains)
INFO - root - 2017-12-08 06:37:32.106646: step 27780, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 18h:51m:49s remains)
INFO - root - 2017-12-08 06:37:34.344424: step 27790, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 18h:52m:21s remains)
INFO - root - 2017-12-08 06:37:36.597022: step 27800, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.225 sec/batch; 19h:03m:16s remains)
2017-12-08 06:37:36.873305: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288454 -4.4288483 -4.4288607 -4.4288874 -4.4289103 -4.42889 -4.4288683 -4.4288645 -4.4288721 -4.4288826 -4.4288983 -4.4289026 -4.4288874 -4.4288526 -4.4287953][-4.428854 -4.428854 -4.4288554 -4.4288845 -4.4289203 -4.4289155 -4.428905 -4.4289083 -4.42892 -4.4289317 -4.4289455 -4.4289422 -4.4289112 -4.4288678 -4.428813][-4.42881 -4.4288106 -4.4288073 -4.4288335 -4.4288769 -4.4288816 -4.4288864 -4.4289036 -4.4289241 -4.4289422 -4.4289579 -4.4289632 -4.428925 -4.4288745 -4.4288135][-4.4287343 -4.4287491 -4.4287653 -4.4288006 -4.4288445 -4.428844 -4.428853 -4.4288778 -4.4289036 -4.4289222 -4.428937 -4.4289551 -4.4289279 -4.42888 -4.4288154][-4.4286661 -4.4287109 -4.4287524 -4.4288 -4.4288192 -4.4287872 -4.4287848 -4.4288192 -4.4288507 -4.4288707 -4.4288893 -4.4289136 -4.428916 -4.4288898 -4.428823][-4.4286375 -4.4287086 -4.4287596 -4.4287839 -4.4287481 -4.4286551 -4.4286232 -4.4286804 -4.4287443 -4.4287953 -4.4288311 -4.4288659 -4.4289012 -4.4289002 -4.4288373][-4.4286275 -4.4286895 -4.4287291 -4.4287238 -4.4286189 -4.4284325 -4.4283462 -4.4284472 -4.4285913 -4.4287062 -4.4287806 -4.4288378 -4.4288912 -4.4289074 -4.4288526][-4.4286437 -4.4286709 -4.4286823 -4.4286323 -4.4284611 -4.428184 -4.4280438 -4.4282141 -4.4284477 -4.4286323 -4.4287472 -4.4288235 -4.4288793 -4.4289017 -4.42886][-4.4286704 -4.4286804 -4.4286675 -4.4285922 -4.4284186 -4.4281611 -4.4280324 -4.4282026 -4.4284368 -4.4286342 -4.4287543 -4.4288211 -4.4288616 -4.4288788 -4.4288545][-4.4286928 -4.4287076 -4.428688 -4.4286261 -4.4285212 -4.4283743 -4.428297 -4.4283934 -4.4285464 -4.428699 -4.428791 -4.4288325 -4.4288487 -4.4288564 -4.4288478][-4.4287038 -4.4287152 -4.4287004 -4.4286604 -4.428617 -4.4285655 -4.4285455 -4.428597 -4.428679 -4.4287744 -4.4288306 -4.4288478 -4.4288449 -4.4288387 -4.4288411][-4.42874 -4.4287429 -4.4287338 -4.4287033 -4.4286814 -4.4286842 -4.4287028 -4.4287386 -4.42878 -4.4288311 -4.4288588 -4.428854 -4.4288268 -4.4288039 -4.4288135][-4.4287972 -4.428791 -4.4287772 -4.4287424 -4.42873 -4.4287615 -4.4287939 -4.4288192 -4.4288406 -4.4288616 -4.4288621 -4.42884 -4.4287977 -4.4287643 -4.4287791][-4.42885 -4.4288387 -4.4288168 -4.4287786 -4.4287691 -4.428812 -4.4288468 -4.4288588 -4.4288659 -4.42887 -4.4288564 -4.4288306 -4.42879 -4.4287558 -4.4287624][-4.4288874 -4.4288783 -4.4288568 -4.4288216 -4.4288225 -4.4288688 -4.428896 -4.42889 -4.4288812 -4.4288726 -4.4288568 -4.4288354 -4.4288073 -4.4287848 -4.4287825]]...]
INFO - root - 2017-12-08 06:37:39.089374: step 27810, loss = 2.28, batch loss = 2.23 (35.0 examples/sec; 0.228 sec/batch; 19h:19m:57s remains)
INFO - root - 2017-12-08 06:37:41.313819: step 27820, loss = 2.28, batch loss = 2.23 (35.6 examples/sec; 0.225 sec/batch; 19h:01m:40s remains)
INFO - root - 2017-12-08 06:37:43.576730: step 27830, loss = 2.28, batch loss = 2.23 (35.4 examples/sec; 0.226 sec/batch; 19h:08m:23s remains)
INFO - root - 2017-12-08 06:37:45.806520: step 27840, loss = 2.28, batch loss = 2.23 (37.5 examples/sec; 0.213 sec/batch; 18h:02m:18s remains)
INFO - root - 2017-12-08 06:37:48.038551: step 27850, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.220 sec/batch; 18h:39m:05s remains)
INFO - root - 2017-12-08 06:37:50.270273: step 27860, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 18h:32m:52s remains)
INFO - root - 2017-12-08 06:37:52.498031: step 27870, loss = 2.28, batch loss = 2.23 (36.0 examples/sec; 0.222 sec/batch; 18h:46m:42s remains)
INFO - root - 2017-12-08 06:37:54.748794: step 27880, loss = 2.28, batch loss = 2.23 (34.8 examples/sec; 0.230 sec/batch; 19h:27m:27s remains)
INFO - root - 2017-12-08 06:37:57.008138: step 27890, loss = 2.28, batch loss = 2.23 (34.0 examples/sec; 0.236 sec/batch; 19h:56m:16s remains)
INFO - root - 2017-12-08 06:37:59.236650: step 27900, loss = 2.28, batch loss = 2.23 (36.9 examples/sec; 0.217 sec/batch; 18h:22m:04s remains)
2017-12-08 06:37:59.537972: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.42859 -4.4286122 -4.4286261 -4.4286017 -4.4285736 -4.4285731 -4.4285927 -4.4286408 -4.4286895 -4.4287028 -4.4286814 -4.4286423 -4.4286056 -4.4285603 -4.4284968][-4.4285903 -4.4286265 -4.4286561 -4.4286504 -4.4286342 -4.4286342 -4.4286437 -4.4286857 -4.4287434 -4.4287696 -4.4287491 -4.4286995 -4.42865 -4.4285994 -4.4285393][-4.4285851 -4.4286337 -4.4286814 -4.4287024 -4.4287043 -4.4286981 -4.4286871 -4.4287086 -4.4287648 -4.4288049 -4.4287972 -4.4287548 -4.4287052 -4.4286561 -4.4286046][-4.4285555 -4.4286017 -4.4286623 -4.4287076 -4.4287243 -4.4287114 -4.4286823 -4.4286847 -4.4287372 -4.428792 -4.4288096 -4.4287858 -4.428741 -4.4286938 -4.42865][-4.4285259 -4.4285469 -4.4286046 -4.428659 -4.4286814 -4.4286594 -4.428618 -4.428606 -4.4286623 -4.4287362 -4.4287825 -4.4287844 -4.4287519 -4.4287148 -4.4286866][-4.4285216 -4.4285231 -4.4285674 -4.4286089 -4.4286079 -4.4285612 -4.4284964 -4.4284668 -4.4285307 -4.4286337 -4.4287105 -4.4287395 -4.4287276 -4.4287081 -4.4287004][-4.4285297 -4.4285274 -4.4285588 -4.4285765 -4.4285426 -4.4284663 -4.4283652 -4.4283032 -4.4283733 -4.4285083 -4.428616 -4.4286652 -4.4286685 -4.4286609 -4.4286761][-4.4285245 -4.4285293 -4.4285569 -4.4285588 -4.4285116 -4.4284253 -4.4283056 -4.4282222 -4.4282885 -4.4284325 -4.4285445 -4.4285908 -4.4285951 -4.4285965 -4.4286342][-4.4285336 -4.4285445 -4.428565 -4.42856 -4.4285307 -4.4284744 -4.4283881 -4.4283166 -4.4283495 -4.428443 -4.4285145 -4.4285355 -4.4285307 -4.4285407 -4.4285984][-4.4285507 -4.4285603 -4.4285636 -4.4285507 -4.428545 -4.4285345 -4.428493 -4.4284372 -4.42843 -4.42846 -4.4284892 -4.4284883 -4.4284768 -4.4284997 -4.4285746][-4.4285531 -4.4285502 -4.4285364 -4.4285192 -4.4285326 -4.4285603 -4.4285541 -4.4285121 -4.4284797 -4.4284668 -4.4284678 -4.4284625 -4.4284563 -4.4284911 -4.428576][-4.4285464 -4.4285283 -4.4285097 -4.428503 -4.4285364 -4.4285932 -4.4286122 -4.4285822 -4.4285378 -4.4285021 -4.4284873 -4.4284844 -4.4284835 -4.428515 -4.4286036][-4.4285731 -4.4285469 -4.4285254 -4.428524 -4.4285679 -4.4286342 -4.4286652 -4.4286466 -4.4286041 -4.4285626 -4.4285474 -4.4285512 -4.4285522 -4.4285755 -4.4286633][-4.4286232 -4.4285874 -4.4285531 -4.42854 -4.4285817 -4.42865 -4.4286895 -4.4286838 -4.428647 -4.4286113 -4.4286041 -4.4286194 -4.4286275 -4.4286466 -4.428719][-4.4286537 -4.4286075 -4.4285541 -4.4285235 -4.4285612 -4.4286385 -4.4286942 -4.4287052 -4.4286838 -4.4286604 -4.4286556 -4.4286709 -4.4286838 -4.4286981 -4.4287539]]...]
INFO - root - 2017-12-08 06:38:01.760843: step 27910, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 18h:41m:38s remains)
INFO - root - 2017-12-08 06:38:03.990222: step 27920, loss = 2.28, batch loss = 2.23 (36.6 examples/sec; 0.218 sec/batch; 18h:28m:11s remains)
INFO - root - 2017-12-08 06:38:06.222353: step 27930, loss = 2.28, batch loss = 2.23 (37.2 examples/sec; 0.215 sec/batch; 18h:12m:43s remains)
INFO - root - 2017-12-08 06:38:08.490777: step 27940, loss = 2.28, batch loss = 2.23 (34.7 examples/sec; 0.231 sec/batch; 19h:31m:54s remains)
INFO - root - 2017-12-08 06:38:10.719887: step 27950, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 18h:33m:14s remains)
INFO - root - 2017-12-08 06:38:12.965695: step 27960, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 18h:41m:32s remains)
INFO - root - 2017-12-08 06:38:15.194771: step 27970, loss = 2.28, batch loss = 2.23 (34.5 examples/sec; 0.232 sec/batch; 19h:35m:22s remains)
INFO - root - 2017-12-08 06:38:17.437990: step 27980, loss = 2.28, batch loss = 2.23 (35.1 examples/sec; 0.228 sec/batch; 19h:18m:19s remains)
INFO - root - 2017-12-08 06:38:19.695936: step 27990, loss = 2.28, batch loss = 2.23 (33.2 examples/sec; 0.241 sec/batch; 20h:21m:23s remains)
INFO - root - 2017-12-08 06:38:21.998570: step 28000, loss = 2.28, batch loss = 2.23 (32.8 examples/sec; 0.244 sec/batch; 20h:36m:30s remains)
2017-12-08 06:38:22.301960: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4286947 -4.4286852 -4.428669 -4.4286737 -4.4287014 -4.4287086 -4.4286933 -4.4286504 -4.4285822 -4.4285383 -4.4285626 -4.4286289 -4.4286861 -4.4287343 -4.4287772][-4.4287133 -4.4287133 -4.4286981 -4.428699 -4.4287124 -4.4286895 -4.4286423 -4.4285669 -4.4284863 -4.4284592 -4.4285131 -4.428597 -4.4286737 -4.4287386 -4.4287825][-4.4287424 -4.4287505 -4.4287429 -4.4287429 -4.4287424 -4.4286923 -4.4286165 -4.428515 -4.4284372 -4.4284453 -4.428525 -4.4286261 -4.4287047 -4.4287672 -4.4287925][-4.4287863 -4.4287934 -4.4287872 -4.4287815 -4.4287648 -4.42869 -4.4285908 -4.4284787 -4.428421 -4.4284649 -4.4285626 -4.4286742 -4.4287539 -4.4288044 -4.4288082][-4.4288058 -4.4288011 -4.4287906 -4.4287806 -4.4287534 -4.4286594 -4.4285307 -4.42841 -4.42839 -4.428472 -4.4285884 -4.4287076 -4.428792 -4.42883 -4.4288087][-4.4287877 -4.4287682 -4.42875 -4.4287405 -4.4287014 -4.4285831 -4.4283981 -4.4282522 -4.4282842 -4.4284286 -4.4285722 -4.4287019 -4.4287982 -4.4288316 -4.4287934][-4.4287348 -4.4287024 -4.4286804 -4.4286757 -4.4286256 -4.4284868 -4.4282489 -4.4280591 -4.4281483 -4.4283624 -4.4285369 -4.4286852 -4.428792 -4.4288349 -4.4287968][-4.428679 -4.4286542 -4.4286408 -4.4286456 -4.4286051 -4.4284811 -4.4282427 -4.4280353 -4.4281397 -4.4283729 -4.4285541 -4.4287028 -4.4288068 -4.4288487 -4.4288158][-4.4286475 -4.4286504 -4.4286547 -4.4286633 -4.428638 -4.4285583 -4.4283719 -4.4281864 -4.4282584 -4.4284549 -4.4286227 -4.4287534 -4.428833 -4.4288616 -4.4288225][-4.428628 -4.4286604 -4.4286828 -4.4286876 -4.4286733 -4.4286218 -4.4284816 -4.4283257 -4.4283738 -4.428544 -4.4286914 -4.4287896 -4.4288368 -4.4288421 -4.4287896][-4.4286032 -4.4286489 -4.428679 -4.4286761 -4.428669 -4.4286351 -4.4285321 -4.4284 -4.4284306 -4.4285836 -4.4287128 -4.4287858 -4.4288144 -4.4288011 -4.4287386][-4.4285617 -4.4286127 -4.4286466 -4.4286418 -4.4286466 -4.4286284 -4.4285541 -4.4284387 -4.4284406 -4.4285679 -4.4286785 -4.4287424 -4.4287682 -4.4287558 -4.4286885][-4.4285183 -4.4285693 -4.4286056 -4.4286113 -4.4286289 -4.4286227 -4.4285717 -4.4284725 -4.4284425 -4.4285412 -4.4286394 -4.428699 -4.4287281 -4.428721 -4.4286613][-4.4285216 -4.4285536 -4.4285874 -4.4286003 -4.428618 -4.4286242 -4.4286065 -4.4285359 -4.4284968 -4.4285612 -4.4286346 -4.4286809 -4.4287114 -4.4287152 -4.42868][-4.4285755 -4.4285707 -4.4285827 -4.428586 -4.428596 -4.4286151 -4.4286327 -4.4286027 -4.4285731 -4.4286041 -4.4286456 -4.4286766 -4.4287052 -4.4287233 -4.4287219]]...]
INFO - root - 2017-12-08 06:38:24.552415: step 28010, loss = 2.28, batch loss = 2.23 (35.0 examples/sec; 0.229 sec/batch; 19h:19m:45s remains)
INFO - root - 2017-12-08 06:38:26.821469: step 28020, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 18h:36m:08s remains)
INFO - root - 2017-12-08 06:38:29.052986: step 28030, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 18h:33m:38s remains)
INFO - root - 2017-12-08 06:38:31.267503: step 28040, loss = 2.28, batch loss = 2.23 (36.7 examples/sec; 0.218 sec/batch; 18h:24m:55s remains)
INFO - root - 2017-12-08 06:38:33.509019: step 28050, loss = 2.28, batch loss = 2.23 (36.0 examples/sec; 0.222 sec/batch; 18h:47m:08s remains)
INFO - root - 2017-12-08 06:38:35.764151: step 28060, loss = 2.28, batch loss = 2.23 (35.1 examples/sec; 0.228 sec/batch; 19h:17m:27s remains)
INFO - root - 2017-12-08 06:38:38.001647: step 28070, loss = 2.28, batch loss = 2.23 (35.4 examples/sec; 0.226 sec/batch; 19h:06m:18s remains)
INFO - root - 2017-12-08 06:38:40.227739: step 28080, loss = 2.28, batch loss = 2.23 (35.7 examples/sec; 0.224 sec/batch; 18h:58m:12s remains)
INFO - root - 2017-12-08 06:38:42.488325: step 28090, loss = 2.28, batch loss = 2.23 (34.7 examples/sec; 0.231 sec/batch; 19h:30m:15s remains)
INFO - root - 2017-12-08 06:38:44.715571: step 28100, loss = 2.28, batch loss = 2.23 (35.3 examples/sec; 0.227 sec/batch; 19h:10m:50s remains)
2017-12-08 06:38:44.992145: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288545 -4.4288416 -4.4288449 -4.4288626 -4.4288783 -4.4288836 -4.4288769 -4.4288688 -4.428864 -4.4288607 -4.4288588 -4.4288583 -4.4288583 -4.4288592 -4.428864][-4.4288344 -4.4288206 -4.4288216 -4.428844 -4.428864 -4.4288683 -4.4288573 -4.4288468 -4.4288387 -4.4288321 -4.4288287 -4.4288282 -4.4288321 -4.4288359 -4.4288435][-4.4288244 -4.4288058 -4.4288034 -4.4288268 -4.4288516 -4.4288545 -4.4288397 -4.4288244 -4.4288139 -4.4288049 -4.4288011 -4.4288044 -4.4288125 -4.42882 -4.42883][-4.4288149 -4.4287906 -4.4287829 -4.4288011 -4.42883 -4.4288335 -4.4288135 -4.4287858 -4.428761 -4.4287438 -4.4287419 -4.4287567 -4.4287791 -4.4287968 -4.428813][-4.4287829 -4.4287605 -4.4287548 -4.4287672 -4.4287944 -4.4287982 -4.428772 -4.4287243 -4.4286718 -4.4286337 -4.4286237 -4.4286489 -4.42869 -4.4287262 -4.4287572][-4.4287305 -4.4287128 -4.4287133 -4.4287238 -4.4287457 -4.4287486 -4.4287186 -4.4286585 -4.4285851 -4.4285254 -4.4285007 -4.4285245 -4.4285769 -4.4286294 -4.4286761][-4.428669 -4.42865 -4.4286532 -4.4286671 -4.4286933 -4.4287024 -4.4286785 -4.4286232 -4.4285536 -4.4284921 -4.4284592 -4.4284739 -4.42852 -4.428565 -4.4286089][-4.4286261 -4.4285975 -4.4285979 -4.428617 -4.4286537 -4.4286728 -4.428659 -4.4286132 -4.4285555 -4.4285078 -4.4284854 -4.4285021 -4.42854 -4.4285707 -4.428597][-4.4286256 -4.428587 -4.4285803 -4.4286013 -4.4286432 -4.4286704 -4.4286623 -4.4286208 -4.4285679 -4.4285278 -4.4285178 -4.4285436 -4.4285803 -4.428607 -4.4286275][-4.428658 -4.4286141 -4.4286 -4.42862 -4.4286585 -4.4286857 -4.4286795 -4.428637 -4.428586 -4.4285483 -4.4285431 -4.4285727 -4.4286113 -4.4286418 -4.4286685][-4.4286847 -4.4286408 -4.4286246 -4.4286489 -4.428689 -4.4287143 -4.4287076 -4.4286685 -4.4286251 -4.4285874 -4.42858 -4.4286075 -4.4286447 -4.4286742 -4.4287043][-4.4287009 -4.4286566 -4.4286413 -4.4286771 -4.4287257 -4.4287586 -4.428762 -4.4287319 -4.4286895 -4.428648 -4.4286385 -4.428659 -4.4286866 -4.42871 -4.4287362][-4.4287157 -4.4286728 -4.4286556 -4.4286928 -4.4287434 -4.4287844 -4.4288068 -4.4287939 -4.4287553 -4.4287109 -4.4286947 -4.4287057 -4.4287205 -4.4287367 -4.4287581][-4.4287252 -4.4286885 -4.4286704 -4.4286976 -4.4287367 -4.4287777 -4.4288135 -4.4288158 -4.4287877 -4.4287486 -4.4287276 -4.4287267 -4.42873 -4.4287395 -4.4287558][-4.4287386 -4.4287157 -4.4287047 -4.4287205 -4.4287438 -4.4287734 -4.4288073 -4.4288168 -4.4288 -4.428772 -4.4287534 -4.4287453 -4.4287438 -4.4287462 -4.4287567]]...]
INFO - root - 2017-12-08 06:38:47.228524: step 28110, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 18h:35m:52s remains)
INFO - root - 2017-12-08 06:38:49.452310: step 28120, loss = 2.28, batch loss = 2.23 (36.7 examples/sec; 0.218 sec/batch; 18h:27m:01s remains)
INFO - root - 2017-12-08 06:38:51.698556: step 28130, loss = 2.28, batch loss = 2.23 (36.8 examples/sec; 0.218 sec/batch; 18h:23m:54s remains)
INFO - root - 2017-12-08 06:38:53.922855: step 28140, loss = 2.28, batch loss = 2.23 (35.4 examples/sec; 0.226 sec/batch; 19h:07m:06s remains)
INFO - root - 2017-12-08 06:38:56.139615: step 28150, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.220 sec/batch; 18h:36m:56s remains)
INFO - root - 2017-12-08 06:38:58.407966: step 28160, loss = 2.28, batch loss = 2.23 (34.6 examples/sec; 0.231 sec/batch; 19h:33m:40s remains)
INFO - root - 2017-12-08 06:39:00.667872: step 28170, loss = 2.28, batch loss = 2.23 (37.1 examples/sec; 0.216 sec/batch; 18h:14m:40s remains)
INFO - root - 2017-12-08 06:39:02.907762: step 28180, loss = 2.28, batch loss = 2.23 (35.3 examples/sec; 0.227 sec/batch; 19h:09m:10s remains)
INFO - root - 2017-12-08 06:39:05.149897: step 28190, loss = 2.28, batch loss = 2.23 (35.7 examples/sec; 0.224 sec/batch; 18h:55m:45s remains)
INFO - root - 2017-12-08 06:39:07.387881: step 28200, loss = 2.28, batch loss = 2.23 (35.2 examples/sec; 0.227 sec/batch; 19h:12m:19s remains)
2017-12-08 06:39:07.683479: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4286208 -4.4286294 -4.4286861 -4.4287653 -4.4288492 -4.4289002 -4.4289303 -4.4289112 -4.4288287 -4.4287462 -4.4286747 -4.4286094 -4.4286375 -4.4287338 -4.4288082][-4.4286127 -4.4286537 -4.4287295 -4.4288111 -4.428884 -4.4289322 -4.4289589 -4.4289341 -4.4288559 -4.4287772 -4.4287086 -4.428647 -4.4286795 -4.4287748 -4.4288392][-4.4286828 -4.4287395 -4.4288111 -4.4288735 -4.4289165 -4.4289417 -4.428956 -4.4289327 -4.4288669 -4.428813 -4.4287748 -4.4287429 -4.42877 -4.4288387 -4.4288845][-4.4287581 -4.4288158 -4.4288731 -4.428905 -4.4289083 -4.428906 -4.4288988 -4.428874 -4.4288397 -4.4288235 -4.4288135 -4.4288125 -4.4288378 -4.4288754 -4.428896][-4.4287891 -4.4288497 -4.4288936 -4.4288878 -4.42885 -4.4288135 -4.4287767 -4.4287515 -4.428762 -4.4287863 -4.4288144 -4.428844 -4.4288645 -4.4288712 -4.42886][-4.4287715 -4.4288387 -4.4288673 -4.4288225 -4.4287353 -4.4286394 -4.4285579 -4.4285369 -4.4286165 -4.4287195 -4.4287996 -4.4288549 -4.4288597 -4.428833 -4.4287972][-4.4287386 -4.4288111 -4.4288249 -4.4287467 -4.4286089 -4.4284387 -4.4282804 -4.4282408 -4.4284234 -4.42864 -4.4287815 -4.4288597 -4.4288549 -4.428802 -4.4287491][-4.4287329 -4.428802 -4.4288025 -4.4287043 -4.428535 -4.4282956 -4.4280243 -4.4279313 -4.4282255 -4.42854 -4.4287238 -4.4288249 -4.4288254 -4.4287605 -4.4286914][-4.4287839 -4.428844 -4.4288416 -4.4287457 -4.4285803 -4.428329 -4.42803 -4.4279304 -4.4282222 -4.428515 -4.428678 -4.4287767 -4.4287724 -4.4286942 -4.4286103][-4.4288716 -4.4289241 -4.4289279 -4.4288487 -4.4287162 -4.428525 -4.4283347 -4.4282885 -4.4284554 -4.4286175 -4.4287028 -4.4287596 -4.428721 -4.4286175 -4.4285274][-4.42895 -4.4289913 -4.4289989 -4.4289432 -4.428854 -4.4287338 -4.4286332 -4.4286108 -4.428678 -4.4287348 -4.4287395 -4.4287419 -4.4286604 -4.428514 -4.4284139][-4.4289889 -4.4290166 -4.4290242 -4.4289961 -4.4289527 -4.4288898 -4.4288354 -4.4288149 -4.4288259 -4.4288139 -4.428762 -4.4287248 -4.4286175 -4.4284306 -4.4283118][-4.4289894 -4.4290056 -4.4290075 -4.4290004 -4.4289918 -4.4289694 -4.4289412 -4.428925 -4.428916 -4.4288788 -4.4288092 -4.4287581 -4.4286537 -4.4284673 -4.4283633][-4.4289703 -4.4289808 -4.4289808 -4.4289856 -4.4289937 -4.4289908 -4.4289832 -4.42898 -4.4289742 -4.4289441 -4.4288869 -4.4288464 -4.4287682 -4.4286284 -4.4285674][-4.4289608 -4.4289684 -4.428968 -4.4289756 -4.428988 -4.4289932 -4.4289942 -4.4289975 -4.4289966 -4.4289804 -4.428946 -4.4289193 -4.428875 -4.4288 -4.4287777]]...]
INFO - root - 2017-12-08 06:39:09.918981: step 28210, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.222 sec/batch; 18h:44m:06s remains)
INFO - root - 2017-12-08 06:39:12.152577: step 28220, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 18h:31m:25s remains)
INFO - root - 2017-12-08 06:39:14.393726: step 28230, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 18h:41m:54s remains)
INFO - root - 2017-12-08 06:39:16.646789: step 28240, loss = 2.28, batch loss = 2.23 (35.1 examples/sec; 0.228 sec/batch; 19h:15m:57s remains)
INFO - root - 2017-12-08 06:39:18.922703: step 28250, loss = 2.28, batch loss = 2.23 (34.9 examples/sec; 0.229 sec/batch; 19h:23m:28s remains)
INFO - root - 2017-12-08 06:39:21.165128: step 28260, loss = 2.28, batch loss = 2.23 (37.4 examples/sec; 0.214 sec/batch; 18h:04m:44s remains)
INFO - root - 2017-12-08 06:39:23.417978: step 28270, loss = 2.28, batch loss = 2.23 (37.1 examples/sec; 0.216 sec/batch; 18h:12m:57s remains)
INFO - root - 2017-12-08 06:39:25.694574: step 28280, loss = 2.28, batch loss = 2.23 (36.0 examples/sec; 0.222 sec/batch; 18h:47m:55s remains)
INFO - root - 2017-12-08 06:39:27.922960: step 28290, loss = 2.28, batch loss = 2.23 (37.1 examples/sec; 0.216 sec/batch; 18h:13m:18s remains)
INFO - root - 2017-12-08 06:39:30.182334: step 28300, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 18h:49m:47s remains)
2017-12-08 06:39:30.480053: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289913 -4.4290152 -4.4290156 -4.4289989 -4.4289675 -4.4289165 -4.4288535 -4.4288144 -4.428791 -4.4287934 -4.428812 -4.4288292 -4.4288254 -4.428792 -4.42875][-4.428987 -4.4290056 -4.4290018 -4.4289813 -4.4289422 -4.4288759 -4.4288015 -4.4287534 -4.4287257 -4.4287419 -4.4287872 -4.4288216 -4.4288192 -4.4287896 -4.4287472][-4.4289913 -4.4290037 -4.428988 -4.4289517 -4.428905 -4.4288316 -4.4287486 -4.4286857 -4.4286528 -4.4286876 -4.4287543 -4.4287992 -4.4287987 -4.4287806 -4.4287453][-4.4289904 -4.4289913 -4.42896 -4.4289083 -4.4288521 -4.4287724 -4.4286838 -4.4286056 -4.4285784 -4.4286394 -4.4287119 -4.4287605 -4.4287658 -4.4287577 -4.4287291][-4.4289861 -4.4289761 -4.4289336 -4.4288726 -4.4288077 -4.428719 -4.428616 -4.4285231 -4.4285183 -4.4286122 -4.4286923 -4.4287448 -4.4287529 -4.4287529 -4.42873][-4.4289861 -4.4289684 -4.4289193 -4.4288516 -4.4287734 -4.4286675 -4.428546 -4.4284372 -4.4284625 -4.4285879 -4.4286842 -4.4287572 -4.42878 -4.4287853 -4.4287691][-4.4289804 -4.4289589 -4.4289074 -4.4288316 -4.42873 -4.4285955 -4.4284377 -4.4282956 -4.4283581 -4.4285364 -4.4286742 -4.4287829 -4.4288249 -4.4288363 -4.4288259][-4.4289646 -4.4289465 -4.4288936 -4.428802 -4.4286714 -4.4284949 -4.428278 -4.4280996 -4.4282289 -4.4284797 -4.4286628 -4.428802 -4.428863 -4.4288797 -4.4288797][-4.4289513 -4.4289293 -4.4288678 -4.4287586 -4.4286051 -4.4284062 -4.4281716 -4.428021 -4.4282022 -4.4284635 -4.4286509 -4.428791 -4.428864 -4.4288974 -4.4289145][-4.4289408 -4.4289203 -4.4288564 -4.4287424 -4.4285979 -4.4284167 -4.4282169 -4.4281359 -4.4283004 -4.4285045 -4.4286609 -4.42878 -4.42885 -4.428894 -4.4289351][-4.4289379 -4.4289227 -4.4288664 -4.4287667 -4.4286509 -4.4285016 -4.42834 -4.4283028 -4.428432 -4.4285784 -4.4287014 -4.4287982 -4.4288559 -4.428896 -4.4289422][-4.4289393 -4.4289355 -4.4288988 -4.4288216 -4.4287362 -4.4286242 -4.4285088 -4.4284964 -4.4285841 -4.428679 -4.4287682 -4.4288378 -4.4288778 -4.4289064 -4.42894][-4.4289393 -4.428936 -4.4289107 -4.4288549 -4.4287992 -4.4287295 -4.4286633 -4.4286666 -4.4287167 -4.4287691 -4.42882 -4.428864 -4.428895 -4.4289145 -4.4289317][-4.4289327 -4.4289255 -4.4289074 -4.428874 -4.4288435 -4.4288096 -4.4287796 -4.4287882 -4.42881 -4.4288278 -4.4288492 -4.428874 -4.428894 -4.4289069 -4.428915][-4.4289289 -4.4289179 -4.4289026 -4.4288812 -4.428864 -4.4288497 -4.4288397 -4.4288464 -4.4288521 -4.4288549 -4.428864 -4.4288778 -4.4288878 -4.4288969 -4.4289026]]...]
INFO - root - 2017-12-08 06:39:32.701706: step 28310, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.225 sec/batch; 19h:01m:50s remains)
INFO - root - 2017-12-08 06:39:34.931702: step 28320, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 18h:32m:57s remains)
INFO - root - 2017-12-08 06:39:37.168082: step 28330, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 18h:33m:27s remains)
INFO - root - 2017-12-08 06:39:39.441728: step 28340, loss = 2.28, batch loss = 2.23 (35.4 examples/sec; 0.226 sec/batch; 19h:07m:03s remains)
INFO - root - 2017-12-08 06:39:41.678711: step 28350, loss = 2.28, batch loss = 2.23 (33.5 examples/sec; 0.239 sec/batch; 20h:09m:21s remains)
INFO - root - 2017-12-08 06:39:43.914369: step 28360, loss = 2.28, batch loss = 2.23 (33.9 examples/sec; 0.236 sec/batch; 19h:54m:43s remains)
INFO - root - 2017-12-08 06:39:46.168589: step 28370, loss = 2.28, batch loss = 2.23 (35.7 examples/sec; 0.224 sec/batch; 18h:54m:49s remains)
INFO - root - 2017-12-08 06:39:48.397188: step 28380, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 18h:33m:23s remains)
INFO - root - 2017-12-08 06:39:50.617855: step 28390, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 18h:30m:47s remains)
INFO - root - 2017-12-08 06:39:52.859303: step 28400, loss = 2.28, batch loss = 2.23 (35.2 examples/sec; 0.227 sec/batch; 19h:10m:35s remains)
2017-12-08 06:39:53.188106: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4290032 -4.4290042 -4.4289908 -4.4289703 -4.428947 -4.4289346 -4.42894 -4.4289508 -4.4289527 -4.4289446 -4.4289212 -4.42889 -4.4288645 -4.4288511 -4.4288473][-4.4289975 -4.4290013 -4.4289861 -4.428956 -4.4289217 -4.4289045 -4.4289145 -4.428937 -4.4289527 -4.4289594 -4.4289484 -4.4289203 -4.4288921 -4.4288697 -4.4288526][-4.4289761 -4.4289746 -4.42895 -4.4289031 -4.42885 -4.4288211 -4.4288316 -4.4288645 -4.4288988 -4.4289269 -4.4289379 -4.4289269 -4.4289107 -4.4288955 -4.4288807][-4.4289508 -4.4289341 -4.4288888 -4.4288163 -4.4287381 -4.42869 -4.4286928 -4.428731 -4.4287848 -4.4288363 -4.4288716 -4.428885 -4.4288917 -4.4288993 -4.4289045][-4.4289293 -4.4288993 -4.428833 -4.4287367 -4.4286361 -4.4285636 -4.4285431 -4.4285717 -4.4286327 -4.4286966 -4.428751 -4.428791 -4.4288273 -4.428863 -4.4288955][-4.4289188 -4.4288893 -4.4288158 -4.4287114 -4.4286 -4.4285 -4.4284363 -4.4284296 -4.4284773 -4.4285374 -4.428597 -4.4286585 -4.4287248 -4.4287891 -4.4288478][-4.4289064 -4.4288869 -4.4288187 -4.428719 -4.428606 -4.4284878 -4.4283848 -4.428339 -4.4283643 -4.4284115 -4.4284658 -4.4285398 -4.4286275 -4.4287105 -4.4287882][-4.4288874 -4.4288869 -4.4288354 -4.4287496 -4.4286456 -4.4285326 -4.4284286 -4.4283748 -4.4283881 -4.4284306 -4.4284787 -4.4285536 -4.4286356 -4.4287062 -4.4287748][-4.42888 -4.4289002 -4.4288726 -4.4288087 -4.4287286 -4.4286466 -4.4285731 -4.4285355 -4.428544 -4.428576 -4.4286094 -4.4286647 -4.42872 -4.4287629 -4.4288082][-4.4288926 -4.4289279 -4.4289241 -4.4288878 -4.4288406 -4.4287953 -4.4287472 -4.4287138 -4.428709 -4.428721 -4.4287319 -4.4287605 -4.4287915 -4.4288149 -4.4288425][-4.4289174 -4.4289527 -4.428966 -4.42896 -4.428947 -4.4289331 -4.4289055 -4.4288716 -4.4288483 -4.4288325 -4.4288177 -4.4288192 -4.4288321 -4.4288478 -4.4288669][-4.4289403 -4.4289656 -4.4289827 -4.4289923 -4.429 -4.4290071 -4.4289985 -4.4289708 -4.4289374 -4.4289017 -4.428864 -4.4288368 -4.4288306 -4.4288416 -4.4288621][-4.4289584 -4.42897 -4.4289818 -4.4289942 -4.4290075 -4.4290214 -4.4290218 -4.4290047 -4.4289718 -4.428926 -4.428874 -4.4288249 -4.4287944 -4.4287953 -4.4288158][-4.428968 -4.428966 -4.428966 -4.4289708 -4.4289794 -4.428988 -4.428987 -4.4289765 -4.4289513 -4.4289045 -4.4288468 -4.4287906 -4.4287486 -4.4287391 -4.4287577][-4.4289622 -4.428947 -4.4289317 -4.4289212 -4.4289165 -4.4289126 -4.4289041 -4.4288936 -4.4288759 -4.4288411 -4.4287939 -4.4287515 -4.4287219 -4.4287148 -4.4287329]]...]
INFO - root - 2017-12-08 06:39:55.409378: step 28410, loss = 2.28, batch loss = 2.23 (34.7 examples/sec; 0.230 sec/batch; 19h:27m:20s remains)
INFO - root - 2017-12-08 06:39:57.640899: step 28420, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.221 sec/batch; 18h:41m:46s remains)
INFO - root - 2017-12-08 06:39:59.857918: step 28430, loss = 2.28, batch loss = 2.23 (35.7 examples/sec; 0.224 sec/batch; 18h:56m:53s remains)
INFO - root - 2017-12-08 06:40:02.090876: step 28440, loss = 2.28, batch loss = 2.23 (35.3 examples/sec; 0.227 sec/batch; 19h:08m:13s remains)
INFO - root - 2017-12-08 06:40:04.323608: step 28450, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 18h:40m:02s remains)
INFO - root - 2017-12-08 06:40:06.555201: step 28460, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.225 sec/batch; 19h:01m:09s remains)
INFO - root - 2017-12-08 06:40:08.787608: step 28470, loss = 2.28, batch loss = 2.23 (37.0 examples/sec; 0.216 sec/batch; 18h:15m:43s remains)
INFO - root - 2017-12-08 06:40:11.031834: step 28480, loss = 2.28, batch loss = 2.23 (35.6 examples/sec; 0.225 sec/batch; 18h:58m:14s remains)
INFO - root - 2017-12-08 06:40:13.241521: step 28490, loss = 2.28, batch loss = 2.23 (37.4 examples/sec; 0.214 sec/batch; 18h:05m:15s remains)
INFO - root - 2017-12-08 06:40:15.512287: step 28500, loss = 2.28, batch loss = 2.23 (34.9 examples/sec; 0.229 sec/batch; 19h:21m:18s remains)
2017-12-08 06:40:15.812384: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.428956 -4.42894 -4.4289236 -4.4289212 -4.4289284 -4.4289312 -4.4289255 -4.4289174 -4.4289045 -4.4288893 -4.4288855 -4.4289064 -4.4289188 -4.4289141 -4.4289212][-4.4289379 -4.4289184 -4.4288983 -4.4288921 -4.4288964 -4.428894 -4.4288812 -4.42887 -4.4288535 -4.4288383 -4.4288368 -4.4288688 -4.4288936 -4.4288993 -4.4289122][-4.4288979 -4.4288769 -4.428853 -4.4288425 -4.42884 -4.428822 -4.4287939 -4.4287786 -4.4287658 -4.4287639 -4.4287605 -4.4287944 -4.4288325 -4.4288597 -4.42889][-4.4288468 -4.4288182 -4.42878 -4.4287567 -4.4287438 -4.4287043 -4.4286456 -4.4286194 -4.4286246 -4.4286494 -4.4286537 -4.4286933 -4.4287405 -4.4287915 -4.4288449][-4.4287848 -4.4287424 -4.4286938 -4.4286566 -4.4286265 -4.4285617 -4.4284616 -4.4284191 -4.4284511 -4.4285131 -4.4285326 -4.428576 -4.4286346 -4.4287105 -4.4287891][-4.4287133 -4.4286523 -4.4285917 -4.4285455 -4.4284925 -4.4283948 -4.4282527 -4.42819 -4.4282584 -4.4283729 -4.4284382 -4.4285169 -4.4286036 -4.4286976 -4.42878][-4.4286857 -4.4286022 -4.4285283 -4.4284749 -4.4284158 -4.4283094 -4.4281631 -4.428113 -4.4282217 -4.4283862 -4.4284997 -4.4286003 -4.4286909 -4.4287772 -4.428844][-4.4287214 -4.4286351 -4.42856 -4.4285121 -4.4284787 -4.4284096 -4.4283161 -4.428299 -4.4284015 -4.4285469 -4.4286561 -4.42874 -4.4288111 -4.4288793 -4.4289231][-4.4287882 -4.4287176 -4.4286647 -4.4286346 -4.4286132 -4.4285727 -4.4285259 -4.428525 -4.4285989 -4.4287028 -4.4287868 -4.4288492 -4.4289055 -4.4289551 -4.4289804][-4.428833 -4.4287782 -4.4287491 -4.4287362 -4.4287205 -4.4287004 -4.4286833 -4.4286942 -4.4287481 -4.4288173 -4.4288745 -4.42892 -4.4289603 -4.4289894 -4.4290009][-4.428853 -4.4288096 -4.4287944 -4.4287958 -4.4287949 -4.4287939 -4.4287987 -4.4288154 -4.4288507 -4.4288917 -4.4289269 -4.4289527 -4.4289765 -4.4289932 -4.428998][-4.4288774 -4.4288416 -4.4288263 -4.4288311 -4.4288411 -4.4288549 -4.4288731 -4.4288912 -4.4289122 -4.4289346 -4.428957 -4.4289737 -4.4289851 -4.4289937 -4.4289956][-4.4289179 -4.4288912 -4.4288764 -4.4288797 -4.428896 -4.4289188 -4.4289384 -4.4289513 -4.42896 -4.4289694 -4.4289784 -4.4289851 -4.4289894 -4.4289932 -4.4289966][-4.4289651 -4.4289479 -4.4289346 -4.4289351 -4.4289455 -4.4289589 -4.4289694 -4.4289756 -4.42898 -4.4289856 -4.4289927 -4.428997 -4.4290009 -4.4290056 -4.4290109][-4.4289985 -4.4289927 -4.4289851 -4.4289832 -4.428987 -4.4289923 -4.4289961 -4.4289989 -4.4290013 -4.4290056 -4.4290109 -4.4290152 -4.429019 -4.4290237 -4.4290276]]...]
INFO - root - 2017-12-08 06:40:18.031648: step 28510, loss = 2.28, batch loss = 2.23 (36.7 examples/sec; 0.218 sec/batch; 18h:24m:06s remains)
INFO - root - 2017-12-08 06:40:20.279342: step 28520, loss = 2.28, batch loss = 2.23 (35.1 examples/sec; 0.228 sec/batch; 19h:15m:04s remains)
INFO - root - 2017-12-08 06:40:22.519879: step 28530, loss = 2.28, batch loss = 2.23 (35.1 examples/sec; 0.228 sec/batch; 19h:15m:03s remains)
INFO - root - 2017-12-08 06:40:24.771204: step 28540, loss = 2.28, batch loss = 2.23 (37.0 examples/sec; 0.216 sec/batch; 18h:15m:02s remains)
INFO - root - 2017-12-08 06:40:26.996468: step 28550, loss = 2.28, batch loss = 2.23 (36.6 examples/sec; 0.219 sec/batch; 18h:28m:20s remains)
INFO - root - 2017-12-08 06:40:29.223137: step 28560, loss = 2.28, batch loss = 2.23 (35.8 examples/sec; 0.224 sec/batch; 18h:52m:18s remains)
INFO - root - 2017-12-08 06:40:31.459350: step 28570, loss = 2.28, batch loss = 2.23 (36.8 examples/sec; 0.217 sec/batch; 18h:21m:15s remains)
INFO - root - 2017-12-08 06:40:33.690840: step 28580, loss = 2.28, batch loss = 2.23 (36.8 examples/sec; 0.218 sec/batch; 18h:22m:38s remains)
INFO - root - 2017-12-08 06:40:35.917596: step 28590, loss = 2.28, batch loss = 2.23 (35.4 examples/sec; 0.226 sec/batch; 19h:03m:04s remains)
INFO - root - 2017-12-08 06:40:38.156913: step 28600, loss = 2.28, batch loss = 2.23 (33.9 examples/sec; 0.236 sec/batch; 19h:54m:58s remains)
2017-12-08 06:40:38.460078: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.428874 -4.428864 -4.4288082 -4.42873 -4.4286265 -4.4285507 -4.428534 -4.4285822 -4.42863 -4.4287033 -4.42879 -4.4288411 -4.4288492 -4.4288344 -4.428822][-4.4288712 -4.4288545 -4.4287987 -4.4287162 -4.4286079 -4.4285159 -4.4284806 -4.428525 -4.4285893 -4.4286833 -4.4287839 -4.42884 -4.4288449 -4.428823 -4.4288025][-4.4288621 -4.428854 -4.4288087 -4.42872 -4.4285913 -4.4284668 -4.4284081 -4.428442 -4.4285283 -4.4286513 -4.4287734 -4.4288397 -4.4288416 -4.4288125 -4.4287858][-4.4288568 -4.4288616 -4.4288292 -4.4287357 -4.4285841 -4.4284282 -4.4283395 -4.4283605 -4.4284706 -4.428618 -4.4287548 -4.4288249 -4.4288263 -4.4287996 -4.4287744][-4.4288697 -4.4288797 -4.4288545 -4.4287572 -4.428586 -4.4283929 -4.4282613 -4.4282641 -4.4284058 -4.4285812 -4.4287267 -4.4287968 -4.4287996 -4.4287806 -4.4287629][-4.4288878 -4.428905 -4.4288864 -4.4287896 -4.4286056 -4.4283743 -4.4281864 -4.4281688 -4.4283419 -4.4285479 -4.4286966 -4.4287672 -4.428772 -4.4287605 -4.4287467][-4.428915 -4.4289403 -4.4289293 -4.4288359 -4.4286489 -4.428391 -4.4281578 -4.4281244 -4.4283166 -4.4285345 -4.4286776 -4.4287453 -4.4287543 -4.4287477 -4.4287267][-4.428947 -4.4289827 -4.4289818 -4.4288921 -4.4287004 -4.4284353 -4.428196 -4.4281564 -4.4283414 -4.4285483 -4.4286752 -4.4287381 -4.4287496 -4.4287415 -4.4287138][-4.4289622 -4.4290013 -4.4290123 -4.4289317 -4.4287429 -4.4284897 -4.428268 -4.42822 -4.4283671 -4.4285512 -4.4286733 -4.4287324 -4.4287462 -4.4287381 -4.4287043][-4.4289474 -4.4289827 -4.4290051 -4.4289436 -4.428781 -4.4285607 -4.4283681 -4.4283109 -4.42841 -4.4285603 -4.4286737 -4.4287348 -4.4287577 -4.4287505 -4.4287157][-4.4289117 -4.428946 -4.4289794 -4.4289446 -4.4288211 -4.4286366 -4.4284625 -4.4283915 -4.4284492 -4.4285669 -4.4286718 -4.4287353 -4.4287615 -4.4287548 -4.4287238][-4.4288831 -4.4289174 -4.4289522 -4.4289384 -4.4288497 -4.4286995 -4.4285445 -4.4284711 -4.4285045 -4.428587 -4.4286747 -4.4287343 -4.42876 -4.4287577 -4.4287362][-4.4288664 -4.4289079 -4.4289432 -4.428946 -4.4288859 -4.4287682 -4.4286461 -4.4285827 -4.4285932 -4.4286394 -4.4286995 -4.4287519 -4.4287758 -4.4287839 -4.428782][-4.4288507 -4.4289069 -4.4289479 -4.4289641 -4.4289289 -4.4288416 -4.4287453 -4.4286866 -4.4286795 -4.4287009 -4.4287415 -4.428791 -4.428822 -4.4288421 -4.4288597][-4.4288244 -4.4288993 -4.4289474 -4.4289737 -4.4289589 -4.4288983 -4.4288206 -4.4287663 -4.42875 -4.4287596 -4.428792 -4.4288411 -4.4288797 -4.4289117 -4.4289427]]...]
INFO - root - 2017-12-08 06:40:40.669904: step 28610, loss = 2.28, batch loss = 2.23 (36.8 examples/sec; 0.218 sec/batch; 18h:22m:10s remains)
INFO - root - 2017-12-08 06:40:42.919907: step 28620, loss = 2.28, batch loss = 2.23 (33.6 examples/sec; 0.238 sec/batch; 20h:05m:45s remains)
INFO - root - 2017-12-08 06:40:45.165290: step 28630, loss = 2.28, batch loss = 2.23 (34.9 examples/sec; 0.229 sec/batch; 19h:19m:22s remains)
INFO - root - 2017-12-08 06:40:47.386556: step 28640, loss = 2.28, batch loss = 2.23 (35.7 examples/sec; 0.224 sec/batch; 18h:53m:46s remains)
INFO - root - 2017-12-08 06:40:49.614029: step 28650, loss = 2.28, batch loss = 2.23 (37.1 examples/sec; 0.215 sec/batch; 18h:10m:47s remains)
INFO - root - 2017-12-08 06:40:51.846085: step 28660, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 18h:39m:50s remains)
INFO - root - 2017-12-08 06:40:54.085842: step 28670, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 18h:33m:14s remains)
INFO - root - 2017-12-08 06:40:56.321293: step 28680, loss = 2.28, batch loss = 2.23 (36.0 examples/sec; 0.222 sec/batch; 18h:46m:36s remains)
INFO - root - 2017-12-08 06:40:58.557851: step 28690, loss = 2.28, batch loss = 2.23 (35.1 examples/sec; 0.228 sec/batch; 19h:12m:30s remains)
INFO - root - 2017-12-08 06:41:00.796051: step 28700, loss = 2.28, batch loss = 2.23 (34.2 examples/sec; 0.234 sec/batch; 19h:43m:50s remains)
2017-12-08 06:41:01.097721: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289408 -4.4289379 -4.4289322 -4.4289317 -4.4289336 -4.4289336 -4.4289336 -4.4289331 -4.4289312 -4.4289308 -4.4289341 -4.4289365 -4.4289379 -4.4289432 -4.4289494][-4.42892 -4.4289117 -4.4289045 -4.4289074 -4.4289131 -4.4289131 -4.4289122 -4.4289103 -4.428906 -4.4289031 -4.4289055 -4.4289069 -4.4289103 -4.4289184 -4.4289269][-4.4288816 -4.4288697 -4.4288626 -4.4288712 -4.428884 -4.4288821 -4.4288754 -4.4288692 -4.4288568 -4.4288464 -4.4288445 -4.4288478 -4.4288583 -4.4288731 -4.428885][-4.4288292 -4.4288149 -4.4288068 -4.42882 -4.4288392 -4.4288292 -4.4288087 -4.428792 -4.4287715 -4.4287515 -4.4287467 -4.4287572 -4.42878 -4.4288058 -4.4288249][-4.428761 -4.42874 -4.4287233 -4.4287348 -4.4287505 -4.4287219 -4.4286828 -4.4286661 -4.4286513 -4.4286342 -4.4286337 -4.4286628 -4.4287038 -4.4287434 -4.4287696][-4.4286685 -4.4286308 -4.4285913 -4.428587 -4.4285922 -4.4285393 -4.4284859 -4.4284959 -4.4285212 -4.4285259 -4.4285388 -4.4285884 -4.4286451 -4.4286923 -4.4287238][-4.42854 -4.4284663 -4.4283843 -4.4283605 -4.4283586 -4.4282775 -4.4282088 -4.4282751 -4.4283671 -4.4284062 -4.4284315 -4.4284964 -4.4285731 -4.4286356 -4.4286804][-4.4283948 -4.4282889 -4.4281797 -4.428153 -4.4281659 -4.4280758 -4.4279943 -4.4281068 -4.4282541 -4.4283209 -4.4283433 -4.4284077 -4.4285035 -4.4285927 -4.4286528][-4.4283304 -4.4282289 -4.4281368 -4.4281387 -4.4281912 -4.4281435 -4.4280939 -4.4282002 -4.4283285 -4.4283805 -4.4283853 -4.4284277 -4.4285188 -4.428606 -4.42866][-4.4283805 -4.4283123 -4.4282584 -4.428287 -4.4283576 -4.4283519 -4.4283381 -4.428411 -4.4284806 -4.4285059 -4.4284992 -4.42852 -4.4285893 -4.4286585 -4.4287][-4.4284716 -4.4284339 -4.4284148 -4.4284539 -4.42852 -4.4285336 -4.4285388 -4.4285841 -4.4286137 -4.42862 -4.4286027 -4.4286079 -4.4286633 -4.4287233 -4.428762][-4.4285984 -4.4285851 -4.42859 -4.4286289 -4.4286766 -4.4286909 -4.4287052 -4.4287324 -4.4287372 -4.4287276 -4.4287086 -4.4287128 -4.4287605 -4.4288135 -4.4288449][-4.4287405 -4.4287472 -4.428761 -4.4287925 -4.428822 -4.4288321 -4.4288445 -4.4288592 -4.4288545 -4.4288445 -4.4288306 -4.4288363 -4.4288721 -4.4289117 -4.4289322][-4.4288416 -4.4288592 -4.4288764 -4.4289 -4.4289174 -4.4289231 -4.4289303 -4.428936 -4.4289346 -4.4289303 -4.4289207 -4.4289236 -4.4289465 -4.4289727 -4.4289846][-4.4288993 -4.4289203 -4.4289389 -4.4289551 -4.4289627 -4.4289618 -4.4289627 -4.4289646 -4.4289656 -4.4289646 -4.4289589 -4.4289594 -4.4289737 -4.4289942 -4.4290042]]...]
INFO - root - 2017-12-08 06:41:03.331178: step 28710, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.226 sec/batch; 19h:02m:23s remains)
INFO - root - 2017-12-08 06:41:05.555921: step 28720, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 18h:47m:48s remains)
INFO - root - 2017-12-08 06:41:07.797543: step 28730, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 18h:48m:09s remains)
INFO - root - 2017-12-08 06:41:10.060373: step 28740, loss = 2.28, batch loss = 2.23 (36.0 examples/sec; 0.222 sec/batch; 18h:43m:52s remains)
INFO - root - 2017-12-08 06:41:12.282093: step 28750, loss = 2.28, batch loss = 2.23 (35.4 examples/sec; 0.226 sec/batch; 19h:02m:46s remains)
INFO - root - 2017-12-08 06:41:14.524280: step 28760, loss = 2.28, batch loss = 2.23 (35.7 examples/sec; 0.224 sec/batch; 18h:55m:52s remains)
INFO - root - 2017-12-08 06:41:16.788853: step 28770, loss = 2.28, batch loss = 2.23 (35.4 examples/sec; 0.226 sec/batch; 19h:04m:27s remains)
INFO - root - 2017-12-08 06:41:19.045879: step 28780, loss = 2.28, batch loss = 2.23 (36.9 examples/sec; 0.217 sec/batch; 18h:18m:00s remains)
INFO - root - 2017-12-08 06:41:21.324305: step 28790, loss = 2.28, batch loss = 2.23 (35.3 examples/sec; 0.227 sec/batch; 19h:07m:51s remains)
INFO - root - 2017-12-08 06:41:23.588526: step 28800, loss = 2.28, batch loss = 2.23 (34.9 examples/sec; 0.229 sec/batch; 19h:19m:08s remains)
2017-12-08 06:41:23.889876: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.428834 -4.4288206 -4.4288321 -4.4288564 -4.4288774 -4.4288993 -4.4289064 -4.42889 -4.4288521 -4.4288235 -4.4288244 -4.4288354 -4.4288211 -4.4287877 -4.4287715][-4.4288349 -4.4288268 -4.4288445 -4.4288735 -4.428894 -4.42891 -4.4289107 -4.428895 -4.4288592 -4.4288335 -4.42883 -4.4288368 -4.4288206 -4.4287825 -4.4287677][-4.4288273 -4.4288268 -4.428853 -4.428884 -4.4289 -4.428906 -4.4288964 -4.4288807 -4.4288478 -4.4288249 -4.4288239 -4.4288378 -4.4288216 -4.4287767 -4.4287581][-4.4288158 -4.4288182 -4.428843 -4.4288621 -4.4288645 -4.4288573 -4.4288406 -4.4288297 -4.4288054 -4.4287891 -4.4287968 -4.4288259 -4.4288149 -4.4287634 -4.4287386][-4.4288054 -4.4288 -4.4288158 -4.4288244 -4.4288116 -4.42879 -4.4287639 -4.4287515 -4.4287314 -4.4287291 -4.4287548 -4.4288034 -4.4288087 -4.4287586 -4.4287291][-4.4288015 -4.4287848 -4.4287786 -4.4287734 -4.4287467 -4.4287119 -4.4286757 -4.4286594 -4.4286466 -4.4286556 -4.4287043 -4.4287763 -4.428803 -4.4287653 -4.4287343][-4.4288058 -4.4287834 -4.4287648 -4.4287429 -4.4287028 -4.4286466 -4.4285965 -4.4285755 -4.428566 -4.4285831 -4.4286485 -4.4287424 -4.4287958 -4.4287772 -4.4287491][-4.4287996 -4.4287825 -4.4287653 -4.4287381 -4.4286971 -4.4286284 -4.4285631 -4.4285407 -4.4285307 -4.4285417 -4.4286051 -4.428709 -4.4287782 -4.4287777 -4.4287524][-4.4287782 -4.428771 -4.4287653 -4.4287548 -4.4287262 -4.4286704 -4.4286146 -4.4285927 -4.4285831 -4.4285793 -4.4286184 -4.4287047 -4.4287658 -4.4287672 -4.4287362][-4.4287658 -4.4287624 -4.428771 -4.4287815 -4.4287705 -4.4287343 -4.4287028 -4.4286861 -4.4286771 -4.4286604 -4.4286695 -4.42872 -4.4287529 -4.4287376 -4.4286947][-4.42877 -4.4287567 -4.4287696 -4.4287891 -4.4287887 -4.4287629 -4.4287481 -4.4287381 -4.4287319 -4.4287109 -4.4287071 -4.4287319 -4.4287424 -4.4287133 -4.42867][-4.428782 -4.4287553 -4.4287663 -4.4287786 -4.4287677 -4.4287472 -4.4287453 -4.4287457 -4.4287391 -4.4287167 -4.4287114 -4.4287243 -4.42873 -4.4287047 -4.4286776][-4.4287982 -4.4287686 -4.4287724 -4.4287663 -4.4287362 -4.4287157 -4.4287176 -4.4287243 -4.4287262 -4.4287086 -4.4287028 -4.4287095 -4.4287143 -4.4286962 -4.4286909][-4.4288392 -4.4288192 -4.4288187 -4.4287925 -4.4287496 -4.4287257 -4.4287262 -4.4287333 -4.4287376 -4.4287224 -4.42872 -4.4287238 -4.4287233 -4.4287095 -4.4287186][-4.4288812 -4.4288774 -4.4288812 -4.4288545 -4.4288154 -4.4287963 -4.4287958 -4.4287992 -4.4287987 -4.4287868 -4.4287834 -4.4287806 -4.4287715 -4.4287543 -4.4287639]]...]
INFO - root - 2017-12-08 06:41:26.139208: step 28810, loss = 2.28, batch loss = 2.23 (34.7 examples/sec; 0.231 sec/batch; 19h:26m:51s remains)
INFO - root - 2017-12-08 06:41:28.371238: step 28820, loss = 2.28, batch loss = 2.23 (35.3 examples/sec; 0.227 sec/batch; 19h:07m:29s remains)
INFO - root - 2017-12-08 06:41:30.575770: step 28830, loss = 2.28, batch loss = 2.23 (35.8 examples/sec; 0.224 sec/batch; 18h:52m:30s remains)
INFO - root - 2017-12-08 06:41:32.813301: step 28840, loss = 2.28, batch loss = 2.23 (36.0 examples/sec; 0.222 sec/batch; 18h:45m:50s remains)
INFO - root - 2017-12-08 06:41:35.035217: step 28850, loss = 2.28, batch loss = 2.23 (35.7 examples/sec; 0.224 sec/batch; 18h:54m:49s remains)
INFO - root - 2017-12-08 06:41:37.262907: step 28860, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.222 sec/batch; 18h:41m:46s remains)
INFO - root - 2017-12-08 06:41:39.499349: step 28870, loss = 2.28, batch loss = 2.23 (37.0 examples/sec; 0.216 sec/batch; 18h:12m:48s remains)
INFO - root - 2017-12-08 06:41:41.763316: step 28880, loss = 2.28, batch loss = 2.23 (35.3 examples/sec; 0.227 sec/batch; 19h:07m:47s remains)
INFO - root - 2017-12-08 06:41:43.981944: step 28890, loss = 2.28, batch loss = 2.23 (36.7 examples/sec; 0.218 sec/batch; 18h:23m:37s remains)
INFO - root - 2017-12-08 06:41:46.242568: step 28900, loss = 2.28, batch loss = 2.23 (34.5 examples/sec; 0.232 sec/batch; 19h:34m:03s remains)
2017-12-08 06:41:46.532278: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4290152 -4.429019 -4.4290166 -4.4290133 -4.4290104 -4.429008 -4.4290051 -4.4290047 -4.4290066 -4.4290109 -4.4290142 -4.4290171 -4.4290185 -4.4290195 -4.42902][-4.4290152 -4.4290195 -4.4290175 -4.4290128 -4.4290071 -4.4290009 -4.4289966 -4.428997 -4.4290032 -4.4290123 -4.4290214 -4.4290285 -4.4290318 -4.4290333 -4.4290314][-4.4290118 -4.4290152 -4.4290094 -4.4289966 -4.4289794 -4.4289608 -4.428946 -4.4289403 -4.428946 -4.42896 -4.4289784 -4.4289994 -4.4290166 -4.4290271 -4.42903][-4.42901 -4.4290085 -4.4289908 -4.4289579 -4.428915 -4.4288716 -4.4288378 -4.4288239 -4.4288325 -4.42886 -4.4289002 -4.4289465 -4.4289861 -4.4290118 -4.4290233][-4.4290047 -4.4289908 -4.42895 -4.4288821 -4.4287972 -4.4287157 -4.4286571 -4.4286342 -4.4286523 -4.4287076 -4.4287863 -4.4288731 -4.4289441 -4.4289889 -4.4290113][-4.4289885 -4.4289527 -4.4288769 -4.4287586 -4.428616 -4.4284859 -4.4283967 -4.4283648 -4.4283996 -4.4284959 -4.428627 -4.4287639 -4.4288769 -4.4289527 -4.4289908][-4.4289689 -4.4289117 -4.4287987 -4.4286313 -4.4284344 -4.4282622 -4.4281521 -4.4281254 -4.4281912 -4.4283342 -4.4285097 -4.4286823 -4.4288235 -4.4289203 -4.4289727][-4.42896 -4.4288936 -4.4287639 -4.4285712 -4.428349 -4.4281631 -4.4280591 -4.4280529 -4.4281445 -4.4283175 -4.4285131 -4.4286919 -4.4288316 -4.428925 -4.4289756][-4.4289675 -4.4289169 -4.4288073 -4.4286408 -4.428453 -4.4283032 -4.4282308 -4.4282374 -4.4283237 -4.4284792 -4.4286475 -4.428793 -4.4288983 -4.4289637 -4.4289956][-4.4289756 -4.4289513 -4.4288869 -4.4287815 -4.4286633 -4.4285727 -4.4285374 -4.4285526 -4.4286141 -4.4287157 -4.4288187 -4.4289041 -4.4289646 -4.4289985 -4.4290128][-4.4289637 -4.4289637 -4.4289408 -4.4288926 -4.4288368 -4.4287972 -4.4287877 -4.4287996 -4.4288306 -4.42888 -4.4289289 -4.428968 -4.4289956 -4.42901 -4.4290152][-4.4289269 -4.4289446 -4.4289479 -4.428937 -4.4289193 -4.4289079 -4.4289103 -4.42892 -4.4289346 -4.4289556 -4.4289756 -4.4289927 -4.4290037 -4.429008 -4.429008][-4.42886 -4.4288931 -4.4289155 -4.4289303 -4.4289365 -4.4289393 -4.4289465 -4.4289551 -4.4289632 -4.4289751 -4.4289875 -4.4289985 -4.4290042 -4.4290032 -4.429][-4.4287682 -4.4288096 -4.428843 -4.4288769 -4.4288993 -4.4289155 -4.4289336 -4.4289474 -4.4289584 -4.4289708 -4.4289832 -4.4289908 -4.4289913 -4.428988 -4.4289856][-4.4287 -4.428731 -4.428761 -4.4288015 -4.4288316 -4.4288592 -4.4288888 -4.4289112 -4.4289269 -4.4289427 -4.4289546 -4.4289603 -4.4289584 -4.428956 -4.4289579]]...]
INFO - root - 2017-12-08 06:41:48.761883: step 28910, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 18h:29m:54s remains)
INFO - root - 2017-12-08 06:41:51.004599: step 28920, loss = 2.28, batch loss = 2.23 (35.7 examples/sec; 0.224 sec/batch; 18h:53m:08s remains)
INFO - root - 2017-12-08 06:41:53.252977: step 28930, loss = 2.28, batch loss = 2.23 (34.6 examples/sec; 0.231 sec/batch; 19h:28m:18s remains)
INFO - root - 2017-12-08 06:41:55.504999: step 28940, loss = 2.28, batch loss = 2.23 (36.9 examples/sec; 0.217 sec/batch; 18h:15m:53s remains)
INFO - root - 2017-12-08 06:41:57.776828: step 28950, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 18h:27m:26s remains)
INFO - root - 2017-12-08 06:42:00.017928: step 28960, loss = 2.28, batch loss = 2.23 (34.7 examples/sec; 0.230 sec/batch; 19h:25m:54s remains)
INFO - root - 2017-12-08 06:42:02.290071: step 28970, loss = 2.28, batch loss = 2.23 (34.3 examples/sec; 0.234 sec/batch; 19h:41m:18s remains)
INFO - root - 2017-12-08 06:42:04.545381: step 28980, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.225 sec/batch; 18h:59m:34s remains)
INFO - root - 2017-12-08 06:42:06.810113: step 28990, loss = 2.28, batch loss = 2.23 (35.0 examples/sec; 0.228 sec/batch; 19h:14m:51s remains)
INFO - root - 2017-12-08 06:42:09.067368: step 29000, loss = 2.28, batch loss = 2.23 (37.3 examples/sec; 0.214 sec/batch; 18h:04m:19s remains)
2017-12-08 06:42:09.375268: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288568 -4.428875 -4.4288874 -4.4288912 -4.4288888 -4.4288845 -4.4288821 -4.428885 -4.4288883 -4.4288859 -4.4288697 -4.4288368 -4.428771 -4.4286718 -4.4285674][-4.4288626 -4.4288778 -4.4288859 -4.4288859 -4.4288816 -4.4288759 -4.4288735 -4.4288774 -4.4288807 -4.4288735 -4.4288387 -4.4287758 -4.4286642 -4.4285078 -4.428349][-4.4288745 -4.4288821 -4.4288831 -4.4288807 -4.4288764 -4.4288678 -4.4288607 -4.4288583 -4.428854 -4.4288397 -4.4287939 -4.4287133 -4.428586 -4.4284215 -4.4282513][-4.428885 -4.4288855 -4.4288793 -4.4288735 -4.4288669 -4.4288535 -4.4288387 -4.4288273 -4.428812 -4.4287925 -4.4287605 -4.4287038 -4.4286127 -4.4285092 -4.4284091][-4.4288893 -4.428884 -4.4288707 -4.4288564 -4.4288397 -4.428813 -4.4287815 -4.428751 -4.4287229 -4.4287066 -4.4287095 -4.4287109 -4.4286914 -4.4286633 -4.4286356][-4.4288859 -4.4288712 -4.42884 -4.4288044 -4.4287629 -4.4287086 -4.4286542 -4.428597 -4.4285507 -4.4285464 -4.4286 -4.4286737 -4.4287329 -4.428771 -4.428792][-4.4288673 -4.4288363 -4.4287796 -4.4287157 -4.4286423 -4.4285526 -4.4284668 -4.4283786 -4.4283252 -4.4283566 -4.4284639 -4.428597 -4.42872 -4.42881 -4.4288673][-4.4288373 -4.4287896 -4.4287086 -4.4286184 -4.4285231 -4.4284158 -4.4283147 -4.428216 -4.4281845 -4.4282703 -4.4284163 -4.4285727 -4.4287128 -4.4288173 -4.4288745][-4.428812 -4.4287572 -4.4286747 -4.4285874 -4.4285026 -4.4284291 -4.4283714 -4.4283156 -4.4283214 -4.428412 -4.4285369 -4.4286623 -4.4287653 -4.428823 -4.428834][-4.428781 -4.4287295 -4.4286752 -4.4286256 -4.4285841 -4.4285622 -4.4285588 -4.4285522 -4.4285746 -4.4286323 -4.4287 -4.4287605 -4.4288015 -4.4288068 -4.4287643][-4.4287467 -4.4287043 -4.4286852 -4.4286847 -4.4286928 -4.4287114 -4.4287357 -4.4287477 -4.4287639 -4.4287844 -4.4287949 -4.4287968 -4.4287915 -4.4287705 -4.4287195][-4.4287419 -4.4287176 -4.4287252 -4.4287548 -4.4287891 -4.4288263 -4.4288578 -4.4288712 -4.4288697 -4.4288497 -4.4288087 -4.4287591 -4.4287138 -4.4286861 -4.4286661][-4.4287796 -4.4287791 -4.4287987 -4.4288321 -4.4288678 -4.4289002 -4.428916 -4.4289083 -4.4288726 -4.4288073 -4.428719 -4.4286203 -4.4285378 -4.4285226 -4.4285631][-4.4288249 -4.4288311 -4.4288464 -4.4288669 -4.4288864 -4.428896 -4.4288826 -4.428844 -4.428771 -4.4286633 -4.4285321 -4.4283853 -4.4282861 -4.4283175 -4.4284277][-4.4288611 -4.4288588 -4.4288564 -4.4288535 -4.4288454 -4.4288263 -4.4287815 -4.4287133 -4.4286203 -4.4285035 -4.4283714 -4.4282284 -4.4281607 -4.428246 -4.4283905]]...]
INFO - root - 2017-12-08 06:42:11.640093: step 29010, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.219 sec/batch; 18h:30m:15s remains)
INFO - root - 2017-12-08 06:42:13.883270: step 29020, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.222 sec/batch; 18h:41m:24s remains)
INFO - root - 2017-12-08 06:42:16.134437: step 29030, loss = 2.28, batch loss = 2.23 (33.9 examples/sec; 0.236 sec/batch; 19h:52m:47s remains)
INFO - root - 2017-12-08 06:42:18.381085: step 29040, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.222 sec/batch; 18h:41m:52s remains)
INFO - root - 2017-12-08 06:42:20.634839: step 29050, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.221 sec/batch; 18h:39m:23s remains)
INFO - root - 2017-12-08 06:42:22.868959: step 29060, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 18h:27m:53s remains)
INFO - root - 2017-12-08 06:42:25.132513: step 29070, loss = 2.28, batch loss = 2.23 (36.9 examples/sec; 0.217 sec/batch; 18h:17m:09s remains)
INFO - root - 2017-12-08 06:42:27.346938: step 29080, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 18h:27m:09s remains)
INFO - root - 2017-12-08 06:42:29.604013: step 29090, loss = 2.28, batch loss = 2.23 (35.3 examples/sec; 0.227 sec/batch; 19h:06m:46s remains)
INFO - root - 2017-12-08 06:42:31.872917: step 29100, loss = 2.28, batch loss = 2.23 (34.9 examples/sec; 0.229 sec/batch; 19h:18m:41s remains)
2017-12-08 06:42:32.164727: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288654 -4.4288678 -4.4288697 -4.428874 -4.4288735 -4.4288678 -4.4288597 -4.4288568 -4.4288678 -4.4288912 -4.4289107 -4.4289141 -4.4289026 -4.4288921 -4.4288859][-4.428844 -4.4288416 -4.4288421 -4.4288425 -4.4288378 -4.4288254 -4.4288149 -4.4288163 -4.4288392 -4.4288774 -4.4289088 -4.42892 -4.4289165 -4.4289131 -4.4289045][-4.4288034 -4.428791 -4.4287815 -4.4287686 -4.4287519 -4.428731 -4.4287195 -4.4287281 -4.4287624 -4.4288144 -4.4288616 -4.4288907 -4.4289088 -4.4289231 -4.4289207][-4.428751 -4.428721 -4.4286914 -4.4286609 -4.4286313 -4.4286036 -4.4285932 -4.4286065 -4.4286485 -4.4287133 -4.4287796 -4.4288349 -4.4288793 -4.4289169 -4.4289284][-4.4287066 -4.4286628 -4.4286127 -4.4285641 -4.4285192 -4.4284849 -4.4284697 -4.4284739 -4.4285069 -4.4285765 -4.4286666 -4.4287543 -4.4288278 -4.4288855 -4.4289122][-4.4286728 -4.4286308 -4.4285779 -4.4285231 -4.4284682 -4.428421 -4.428381 -4.428349 -4.4283504 -4.4284267 -4.4285479 -4.4286695 -4.4287663 -4.4288354 -4.4288716][-4.4286475 -4.4286141 -4.4285707 -4.4285231 -4.42847 -4.4284077 -4.4283218 -4.4282269 -4.4281921 -4.4282794 -4.4284282 -4.428575 -4.4286866 -4.4287605 -4.4288044][-4.4286556 -4.4286284 -4.4285946 -4.4285603 -4.4285188 -4.428453 -4.4283276 -4.4281611 -4.4280787 -4.428165 -4.4283237 -4.428483 -4.4286027 -4.4286776 -4.4287205][-4.4286861 -4.4286761 -4.4286633 -4.4286442 -4.4286141 -4.4285545 -4.4284296 -4.4282584 -4.4281473 -4.4281797 -4.4283032 -4.4284439 -4.4285564 -4.4286323 -4.4286757][-4.4287243 -4.4287286 -4.4287362 -4.4287319 -4.4287143 -4.4286723 -4.4285851 -4.4284658 -4.4283733 -4.4283533 -4.4284067 -4.4284925 -4.4285736 -4.4286456 -4.4286909][-4.4287648 -4.4287772 -4.428791 -4.4287972 -4.4287887 -4.4287634 -4.4287086 -4.4286356 -4.4285665 -4.4285231 -4.4285288 -4.4285674 -4.4286194 -4.4286876 -4.42874][-4.428793 -4.4288049 -4.4288125 -4.4288163 -4.4288125 -4.4288049 -4.4287782 -4.4287319 -4.4286737 -4.4286218 -4.4286013 -4.4286051 -4.4286408 -4.428709 -4.4287724][-4.4287815 -4.428782 -4.42878 -4.4287844 -4.428791 -4.4288 -4.42879 -4.4287586 -4.4287086 -4.4286652 -4.4286442 -4.4286366 -4.428668 -4.4287372 -4.4288063][-4.4287534 -4.4287395 -4.4287262 -4.4287324 -4.4287505 -4.4287696 -4.4287663 -4.428741 -4.4287071 -4.4286852 -4.4286733 -4.4286728 -4.4287114 -4.4287829 -4.4288554][-4.4287319 -4.4287128 -4.4286857 -4.4286852 -4.4287062 -4.4287162 -4.4287086 -4.4287028 -4.4286971 -4.4287 -4.428699 -4.4287071 -4.4287496 -4.428812 -4.4288769]]...]
INFO - root - 2017-12-08 06:42:34.430708: step 29110, loss = 2.28, batch loss = 2.23 (35.3 examples/sec; 0.227 sec/batch; 19h:06m:00s remains)
INFO - root - 2017-12-08 06:42:36.675457: step 29120, loss = 2.28, batch loss = 2.23 (35.6 examples/sec; 0.224 sec/batch; 18h:55m:00s remains)
INFO - root - 2017-12-08 06:42:38.946033: step 29130, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 18h:47m:09s remains)
INFO - root - 2017-12-08 06:42:41.176010: step 29140, loss = 2.28, batch loss = 2.23 (35.3 examples/sec; 0.226 sec/batch; 19h:04m:33s remains)
INFO - root - 2017-12-08 06:42:43.441234: step 29150, loss = 2.28, batch loss = 2.23 (36.0 examples/sec; 0.223 sec/batch; 18h:44m:59s remains)
INFO - root - 2017-12-08 06:42:45.656131: step 29160, loss = 2.28, batch loss = 2.23 (36.8 examples/sec; 0.218 sec/batch; 18h:20m:11s remains)
INFO - root - 2017-12-08 06:42:47.900452: step 29170, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.221 sec/batch; 18h:39m:40s remains)
INFO - root - 2017-12-08 06:42:50.124281: step 29180, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 18h:27m:59s remains)
INFO - root - 2017-12-08 06:42:52.357016: step 29190, loss = 2.28, batch loss = 2.23 (35.7 examples/sec; 0.224 sec/batch; 18h:53m:49s remains)
INFO - root - 2017-12-08 06:42:54.614908: step 29200, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 18h:46m:50s remains)
2017-12-08 06:42:54.920804: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4290576 -4.4290514 -4.429018 -4.428978 -4.4289269 -4.4288836 -4.4288259 -4.4287682 -4.428793 -4.4288492 -4.4288545 -4.4288058 -4.4287243 -4.4286556 -4.4286356][-4.4290566 -4.4290504 -4.4290166 -4.4289703 -4.4289141 -4.4288669 -4.4288034 -4.4287424 -4.42877 -4.4288383 -4.428853 -4.4288049 -4.4287205 -4.4286528 -4.4286366][-4.4290462 -4.4290376 -4.428998 -4.4289389 -4.4288735 -4.4288225 -4.4287558 -4.4287004 -4.4287434 -4.42883 -4.4288554 -4.4288092 -4.42872 -4.4286489 -4.4286237][-4.4290352 -4.429018 -4.4289665 -4.4288883 -4.4288044 -4.4287338 -4.4286532 -4.4286113 -4.4286895 -4.4288116 -4.4288659 -4.4288368 -4.4287539 -4.4286804 -4.4286418][-4.42903 -4.4290056 -4.4289412 -4.4288421 -4.4287343 -4.428627 -4.4285097 -4.428473 -4.4286032 -4.4287691 -4.428865 -4.4288726 -4.4288158 -4.42876 -4.4287233][-4.4290285 -4.4290004 -4.4289265 -4.42881 -4.4286785 -4.4285226 -4.4283347 -4.42827 -4.4284463 -4.4286675 -4.4288135 -4.4288716 -4.4288607 -4.428844 -4.4288297][-4.429029 -4.4289975 -4.4289136 -4.4287796 -4.4286242 -4.4284244 -4.4281583 -4.4280305 -4.428236 -4.4285097 -4.4287052 -4.4288225 -4.42887 -4.4288926 -4.4288917][-4.4290261 -4.4289885 -4.4288974 -4.4287543 -4.4285851 -4.4283643 -4.42805 -4.4278426 -4.428031 -4.4283285 -4.4285607 -4.4287338 -4.4288392 -4.4288921 -4.4288888][-4.4290204 -4.4289751 -4.4288788 -4.4287357 -4.4285717 -4.4283676 -4.428071 -4.4278259 -4.4279456 -4.4282174 -4.4284568 -4.4286642 -4.4288073 -4.4288759 -4.4288716][-4.4290204 -4.4289765 -4.428885 -4.4287519 -4.4286094 -4.4284468 -4.4282136 -4.4280014 -4.4280524 -4.4282551 -4.4284635 -4.4286661 -4.42881 -4.42888 -4.4288793][-4.4290276 -4.4289904 -4.42891 -4.4287906 -4.42867 -4.4285474 -4.4283762 -4.4282155 -4.4282351 -4.4283733 -4.4285388 -4.4287195 -4.4288507 -4.4289136 -4.4289188][-4.4290409 -4.4290152 -4.4289494 -4.4288526 -4.4287558 -4.42867 -4.4285517 -4.4284368 -4.4284387 -4.428524 -4.42864 -4.4287877 -4.4289026 -4.4289613 -4.4289751][-4.4290562 -4.4290442 -4.4289985 -4.4289279 -4.4288554 -4.4288006 -4.4287276 -4.4286513 -4.4286442 -4.4286847 -4.4287496 -4.428853 -4.4289441 -4.4289932 -4.4290128][-4.4290628 -4.429059 -4.4290266 -4.4289722 -4.4289155 -4.42888 -4.4288406 -4.4287963 -4.4287863 -4.4287963 -4.4288177 -4.4288812 -4.428947 -4.4289823 -4.4290028][-4.429059 -4.4290509 -4.4290156 -4.4289603 -4.4289007 -4.4288712 -4.4288497 -4.4288292 -4.4288282 -4.4288287 -4.4288254 -4.4288611 -4.428905 -4.4289265 -4.428947]]...]
INFO - root - 2017-12-08 06:42:57.169933: step 29210, loss = 2.28, batch loss = 2.23 (35.6 examples/sec; 0.225 sec/batch; 18h:56m:53s remains)
INFO - root - 2017-12-08 06:42:59.445367: step 29220, loss = 2.28, batch loss = 2.23 (33.9 examples/sec; 0.236 sec/batch; 19h:52m:33s remains)
INFO - root - 2017-12-08 06:43:01.724539: step 29230, loss = 2.28, batch loss = 2.23 (34.8 examples/sec; 0.230 sec/batch; 19h:21m:42s remains)
INFO - root - 2017-12-08 06:43:03.956514: step 29240, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 18h:46m:06s remains)
INFO - root - 2017-12-08 06:43:06.205920: step 29250, loss = 2.28, batch loss = 2.23 (33.8 examples/sec; 0.236 sec/batch; 19h:54m:36s remains)
INFO - root - 2017-12-08 06:43:08.443023: step 29260, loss = 2.28, batch loss = 2.23 (35.4 examples/sec; 0.226 sec/batch; 19h:02m:24s remains)
INFO - root - 2017-12-08 06:43:10.709462: step 29270, loss = 2.28, batch loss = 2.23 (34.0 examples/sec; 0.235 sec/batch; 19h:49m:27s remains)
INFO - root - 2017-12-08 06:43:12.956222: step 29280, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.222 sec/batch; 18h:39m:41s remains)
INFO - root - 2017-12-08 06:43:15.189750: step 29290, loss = 2.28, batch loss = 2.23 (35.3 examples/sec; 0.226 sec/batch; 19h:04m:13s remains)
INFO - root - 2017-12-08 06:43:17.403684: step 29300, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 18h:36m:15s remains)
2017-12-08 06:43:17.707965: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288969 -4.4288974 -4.4289026 -4.4289093 -4.4289103 -4.428906 -4.4288874 -4.42886 -4.4288473 -4.4288535 -4.4288669 -4.4288783 -4.42889 -4.4289093 -4.4289379][-4.4288392 -4.4288492 -4.4288664 -4.4288774 -4.4288769 -4.4288692 -4.4288406 -4.4287992 -4.4287848 -4.4288 -4.4288216 -4.4288363 -4.4288535 -4.428885 -4.4289279][-4.4287658 -4.4287872 -4.4288158 -4.4288311 -4.4288292 -4.4288139 -4.4287696 -4.4287138 -4.4287062 -4.4287419 -4.4287763 -4.4287949 -4.4288177 -4.4288597 -4.4289141][-4.4287033 -4.4287376 -4.4287758 -4.4287925 -4.4287839 -4.4287496 -4.4286761 -4.4285955 -4.4286013 -4.428668 -4.42872 -4.4287467 -4.4287753 -4.4288316 -4.4288969][-4.4286585 -4.4287014 -4.42874 -4.4287505 -4.4287267 -4.4286618 -4.4285464 -4.4284334 -4.428453 -4.428556 -4.4286308 -4.4286733 -4.4287133 -4.4287877 -4.4288688][-4.4286423 -4.428689 -4.4287171 -4.4287124 -4.4286647 -4.4285636 -4.4284039 -4.4282641 -4.4283032 -4.4284406 -4.4285455 -4.4286079 -4.4286613 -4.4287548 -4.4288487][-4.4286575 -4.4287128 -4.4287329 -4.4287066 -4.4286265 -4.4284945 -4.4283185 -4.4281893 -4.4282522 -4.4284053 -4.4285235 -4.4285955 -4.4286575 -4.4287586 -4.4288559][-4.4287176 -4.4287815 -4.4287934 -4.4287429 -4.4286313 -4.4284887 -4.4283452 -4.4282722 -4.4283552 -4.4284964 -4.4286032 -4.4286685 -4.4287224 -4.4288087 -4.4288926][-4.4287863 -4.4288626 -4.4288759 -4.4288144 -4.4286904 -4.4285655 -4.4284778 -4.4284658 -4.4285536 -4.4286642 -4.4287467 -4.4287972 -4.4288344 -4.4288921 -4.4289455][-4.4288116 -4.4288931 -4.4289093 -4.4288492 -4.4287343 -4.4286437 -4.4286103 -4.4286404 -4.428719 -4.4287958 -4.4288507 -4.4288859 -4.4289103 -4.4289446 -4.4289756][-4.4288073 -4.4288859 -4.4289021 -4.4288511 -4.4287629 -4.4287143 -4.4287243 -4.4287724 -4.428834 -4.4288745 -4.4289026 -4.4289265 -4.4289393 -4.4289579 -4.4289765][-4.4288063 -4.4288759 -4.4288893 -4.428844 -4.4287834 -4.4287729 -4.4288154 -4.4288692 -4.42891 -4.4289231 -4.4289317 -4.4289455 -4.4289489 -4.428957 -4.4289675][-4.4288311 -4.4288859 -4.4288907 -4.4288492 -4.4288082 -4.4288187 -4.4288731 -4.4289236 -4.4289479 -4.4289489 -4.4289489 -4.4289532 -4.4289479 -4.4289503 -4.4289584][-4.42888 -4.4289231 -4.4289222 -4.4288845 -4.4288521 -4.4288645 -4.4289122 -4.4289527 -4.4289689 -4.4289656 -4.42896 -4.4289551 -4.4289446 -4.4289441 -4.4289532][-4.4289227 -4.42896 -4.42896 -4.4289327 -4.4289064 -4.4289136 -4.4289441 -4.4289694 -4.4289808 -4.4289775 -4.4289718 -4.4289622 -4.4289479 -4.4289465 -4.4289551]]...]
INFO - root - 2017-12-08 06:43:19.984363: step 29310, loss = 2.28, batch loss = 2.23 (34.9 examples/sec; 0.229 sec/batch; 19h:17m:12s remains)
INFO - root - 2017-12-08 06:43:22.226711: step 29320, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.222 sec/batch; 18h:41m:01s remains)
INFO - root - 2017-12-08 06:43:24.452736: step 29330, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 18h:27m:58s remains)
INFO - root - 2017-12-08 06:43:26.671354: step 29340, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 18h:26m:15s remains)
INFO - root - 2017-12-08 06:43:28.909862: step 29350, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 18h:30m:15s remains)
INFO - root - 2017-12-08 06:43:31.155063: step 29360, loss = 2.28, batch loss = 2.23 (35.4 examples/sec; 0.226 sec/batch; 19h:02m:31s remains)
INFO - root - 2017-12-08 06:43:33.412922: step 29370, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 18h:45m:08s remains)
INFO - root - 2017-12-08 06:43:35.693394: step 29380, loss = 2.28, batch loss = 2.23 (34.3 examples/sec; 0.234 sec/batch; 19h:39m:59s remains)
INFO - root - 2017-12-08 06:43:37.945747: step 29390, loss = 2.28, batch loss = 2.23 (35.1 examples/sec; 0.228 sec/batch; 19h:11m:22s remains)
INFO - root - 2017-12-08 06:43:40.190578: step 29400, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.226 sec/batch; 18h:59m:26s remains)
2017-12-08 06:43:40.492828: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288168 -4.428833 -4.42885 -4.428865 -4.4288731 -4.4288607 -4.42884 -4.428822 -4.4288139 -4.4288087 -4.4287896 -4.4287605 -4.4287138 -4.4286742 -4.4286823][-4.4287639 -4.4287863 -4.4288206 -4.4288487 -4.4288616 -4.4288406 -4.4288054 -4.42878 -4.4287758 -4.4287782 -4.4287724 -4.4287586 -4.4287267 -4.4286933 -4.4287038][-4.4287338 -4.4287691 -4.4288177 -4.4288497 -4.428853 -4.4288144 -4.4287577 -4.4287276 -4.4287329 -4.4287496 -4.4287624 -4.4287639 -4.428751 -4.4287262 -4.4287329][-4.4287047 -4.4287591 -4.4288192 -4.4288449 -4.4288268 -4.4287558 -4.428669 -4.4286437 -4.428678 -4.42872 -4.4287472 -4.4287505 -4.4287486 -4.4287372 -4.4287477][-4.4286923 -4.4287543 -4.4288158 -4.4288287 -4.4287796 -4.4286671 -4.4285431 -4.4285297 -4.4286013 -4.4286847 -4.4287367 -4.42875 -4.4287562 -4.4287591 -4.4287724][-4.4287395 -4.4287953 -4.4288378 -4.4288173 -4.4287157 -4.4285483 -4.4283724 -4.4283681 -4.4284945 -4.4286408 -4.4287343 -4.4287758 -4.4287949 -4.4288058 -4.4288154][-4.4288106 -4.4288449 -4.4288535 -4.4287848 -4.428628 -4.4284096 -4.4281874 -4.428194 -4.4283824 -4.4285922 -4.4287353 -4.4288116 -4.4288487 -4.4288645 -4.4288692][-4.4288917 -4.4288983 -4.4288678 -4.4287477 -4.4285355 -4.4282751 -4.4280314 -4.428062 -4.4283104 -4.4285731 -4.42876 -4.4288635 -4.4289126 -4.4289274 -4.4289255][-4.4289503 -4.4289374 -4.4288778 -4.4287252 -4.4284863 -4.4282088 -4.4279666 -4.4280195 -4.4283004 -4.4285917 -4.4287987 -4.42891 -4.4289589 -4.4289675 -4.4289579][-4.4289813 -4.4289622 -4.428894 -4.4287381 -4.4285078 -4.4282479 -4.4280329 -4.428091 -4.4283552 -4.4286356 -4.4288368 -4.4289393 -4.4289842 -4.4289894 -4.4289804][-4.4289994 -4.428978 -4.4289122 -4.428772 -4.4285674 -4.4283395 -4.4281597 -4.4282131 -4.4284468 -4.4287004 -4.4288845 -4.4289761 -4.4290152 -4.42902 -4.4290123][-4.4290152 -4.4289913 -4.4289303 -4.4288111 -4.4286389 -4.4284468 -4.4283051 -4.4283528 -4.4285502 -4.4287672 -4.4289284 -4.429008 -4.4290414 -4.4290452 -4.4290376][-4.4290233 -4.4290018 -4.4289503 -4.4288573 -4.4287271 -4.4285836 -4.4284878 -4.4285316 -4.4286857 -4.428853 -4.4289756 -4.4290361 -4.4290571 -4.4290562 -4.4290466][-4.429018 -4.4290018 -4.4289641 -4.4289021 -4.4288168 -4.4287224 -4.4286695 -4.42871 -4.4288206 -4.4289336 -4.4290137 -4.42905 -4.4290605 -4.4290562 -4.4290471][-4.4290166 -4.4290042 -4.4289784 -4.4289427 -4.4288955 -4.4288449 -4.4288211 -4.4288545 -4.428925 -4.4289913 -4.4290342 -4.4290524 -4.4290581 -4.4290533 -4.4290471]]...]
INFO - root - 2017-12-08 06:43:42.706153: step 29410, loss = 2.28, batch loss = 2.23 (36.7 examples/sec; 0.218 sec/batch; 18h:20m:09s remains)
INFO - root - 2017-12-08 06:43:44.969631: step 29420, loss = 2.28, batch loss = 2.23 (35.7 examples/sec; 0.224 sec/batch; 18h:53m:31s remains)
INFO - root - 2017-12-08 06:43:47.193166: step 29430, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 18h:37m:48s remains)
INFO - root - 2017-12-08 06:43:49.419923: step 29440, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.222 sec/batch; 18h:40m:01s remains)
INFO - root - 2017-12-08 06:43:51.651909: step 29450, loss = 2.28, batch loss = 2.23 (35.0 examples/sec; 0.229 sec/batch; 19h:15m:39s remains)
INFO - root - 2017-12-08 06:43:53.917287: step 29460, loss = 2.28, batch loss = 2.23 (36.8 examples/sec; 0.217 sec/batch; 18h:18m:18s remains)
INFO - root - 2017-12-08 06:43:56.151322: step 29470, loss = 2.28, batch loss = 2.23 (36.7 examples/sec; 0.218 sec/batch; 18h:21m:56s remains)
INFO - root - 2017-12-08 06:43:58.381525: step 29480, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 18h:30m:34s remains)
INFO - root - 2017-12-08 06:44:00.611891: step 29490, loss = 2.28, batch loss = 2.23 (35.4 examples/sec; 0.226 sec/batch; 19h:02m:29s remains)
INFO - root - 2017-12-08 06:44:02.850583: step 29500, loss = 2.28, batch loss = 2.23 (34.5 examples/sec; 0.232 sec/batch; 19h:30m:10s remains)
2017-12-08 06:44:03.149964: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288383 -4.4288454 -4.4288268 -4.4288082 -4.4288039 -4.4288077 -4.4287825 -4.4287066 -4.4286284 -4.4286032 -4.4286389 -4.4287086 -4.4287663 -4.4288177 -4.4288383][-4.4288564 -4.4288588 -4.4288316 -4.4288063 -4.4287944 -4.4287829 -4.4287481 -4.4286623 -4.42858 -4.4285655 -4.4286265 -4.4287143 -4.4287744 -4.4288249 -4.428853][-4.4288778 -4.4288745 -4.4288468 -4.4288182 -4.4287906 -4.4287543 -4.4287038 -4.428618 -4.4285464 -4.4285407 -4.4286251 -4.4287281 -4.4287844 -4.4288239 -4.4288592][-4.4289012 -4.4288945 -4.4288669 -4.4288378 -4.4287872 -4.4287167 -4.4286427 -4.4285622 -4.4285078 -4.4285059 -4.428597 -4.4287143 -4.4287744 -4.4288135 -4.4288526][-4.4288974 -4.4288907 -4.42887 -4.4288363 -4.4287577 -4.4286518 -4.4285545 -4.428484 -4.42845 -4.4284549 -4.4285536 -4.4286728 -4.4287338 -4.4287796 -4.4288197][-4.4288726 -4.4288697 -4.4288583 -4.4288125 -4.4287009 -4.4285483 -4.4284163 -4.4283504 -4.4283419 -4.4283791 -4.4285045 -4.428616 -4.4286742 -4.4287305 -4.4287715][-4.4288435 -4.4288492 -4.4288373 -4.4287777 -4.4286366 -4.4284358 -4.4282494 -4.42817 -4.4281974 -4.4282975 -4.4284573 -4.4285669 -4.4286261 -4.4286876 -4.4287343][-4.4288044 -4.4288187 -4.4288054 -4.42874 -4.4285989 -4.4283881 -4.4281683 -4.4280748 -4.428123 -4.4282684 -4.4284449 -4.4285541 -4.4286261 -4.4286909 -4.4287405][-4.4287672 -4.4287872 -4.4287686 -4.4287081 -4.42859 -4.4284172 -4.428225 -4.4281478 -4.4281816 -4.4283042 -4.4284611 -4.428575 -4.4286766 -4.42875 -4.428792][-4.42875 -4.4287682 -4.4287534 -4.4287043 -4.4286146 -4.4284973 -4.4283781 -4.4283452 -4.4283538 -4.4284186 -4.4285297 -4.42864 -4.4287491 -4.4288211 -4.4288554][-4.4287734 -4.4287853 -4.4287772 -4.4287443 -4.4286861 -4.428616 -4.4285622 -4.4285545 -4.4285464 -4.4285717 -4.4286451 -4.4287395 -4.4288325 -4.4288917 -4.4289117][-4.42882 -4.4288282 -4.4288244 -4.4288044 -4.4287767 -4.4287467 -4.4287291 -4.4287305 -4.4287138 -4.4287229 -4.4287682 -4.4288421 -4.428905 -4.4289384 -4.4289474][-4.4288688 -4.4288716 -4.4288664 -4.4288559 -4.4288464 -4.42884 -4.4288421 -4.4288526 -4.4288449 -4.4288468 -4.4288712 -4.4289179 -4.4289527 -4.428968 -4.428968][-4.4289031 -4.4289 -4.4288888 -4.4288812 -4.4288783 -4.4288821 -4.4288917 -4.428906 -4.4289074 -4.42891 -4.4289212 -4.4289446 -4.4289594 -4.4289641 -4.4289622][-4.4289207 -4.4289131 -4.4288988 -4.4288888 -4.4288826 -4.4288855 -4.4288945 -4.4289036 -4.4289069 -4.4289093 -4.4289117 -4.4289193 -4.4289265 -4.4289341 -4.428936]]...]
INFO - root - 2017-12-08 06:44:05.419143: step 29510, loss = 2.28, batch loss = 2.23 (34.5 examples/sec; 0.232 sec/batch; 19h:31m:52s remains)
INFO - root - 2017-12-08 06:44:07.656077: step 29520, loss = 2.28, batch loss = 2.23 (35.1 examples/sec; 0.228 sec/batch; 19h:11m:19s remains)
INFO - root - 2017-12-08 06:44:09.908975: step 29530, loss = 2.28, batch loss = 2.23 (35.6 examples/sec; 0.225 sec/batch; 18h:55m:30s remains)
INFO - root - 2017-12-08 06:44:12.147810: step 29540, loss = 2.28, batch loss = 2.23 (36.0 examples/sec; 0.222 sec/batch; 18h:41m:39s remains)
INFO - root - 2017-12-08 06:44:14.399219: step 29550, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 18h:28m:45s remains)
INFO - root - 2017-12-08 06:44:16.666307: step 29560, loss = 2.28, batch loss = 2.23 (33.5 examples/sec; 0.239 sec/batch; 20h:07m:20s remains)
INFO - root - 2017-12-08 06:44:18.903065: step 29570, loss = 2.28, batch loss = 2.23 (35.1 examples/sec; 0.228 sec/batch; 19h:11m:43s remains)
INFO - root - 2017-12-08 06:44:21.124795: step 29580, loss = 2.28, batch loss = 2.23 (36.9 examples/sec; 0.217 sec/batch; 18h:14m:27s remains)
INFO - root - 2017-12-08 06:44:23.398464: step 29590, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 18h:44m:12s remains)
INFO - root - 2017-12-08 06:44:25.670125: step 29600, loss = 2.28, batch loss = 2.23 (34.8 examples/sec; 0.230 sec/batch; 19h:19m:29s remains)
2017-12-08 06:44:25.971132: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.42874 -4.4287605 -4.4287825 -4.4287992 -4.428823 -4.4288406 -4.4288464 -4.4288564 -4.4288759 -4.4288788 -4.4288654 -4.4288321 -4.4287753 -4.42872 -4.4287047][-4.4287958 -4.4288235 -4.4288487 -4.4288578 -4.428863 -4.4288554 -4.4288411 -4.428843 -4.4288654 -4.4288774 -4.4288721 -4.4288459 -4.4287891 -4.4287267 -4.4287086][-4.4288526 -4.428875 -4.4288921 -4.4288878 -4.4288707 -4.4288392 -4.4288096 -4.4288058 -4.4288282 -4.4288464 -4.4288526 -4.4288445 -4.4288073 -4.4287543 -4.428741][-4.4288669 -4.4288917 -4.4289093 -4.4288912 -4.4288492 -4.4287925 -4.4287477 -4.4287405 -4.4287667 -4.4287944 -4.428822 -4.4288473 -4.428844 -4.4288111 -4.4287972][-4.4288731 -4.4288974 -4.428905 -4.4288588 -4.4287777 -4.4286852 -4.428616 -4.4285932 -4.4286332 -4.4287009 -4.4287734 -4.4288421 -4.4288735 -4.4288645 -4.4288521][-4.428854 -4.4288721 -4.4288654 -4.428792 -4.4286666 -4.4285173 -4.42839 -4.4283295 -4.4284086 -4.4285545 -4.4286914 -4.4288006 -4.4288635 -4.42888 -4.428874][-4.42881 -4.4288063 -4.4287682 -4.4286575 -4.4284773 -4.428246 -4.4280176 -4.427887 -4.4280305 -4.4282942 -4.4285111 -4.428668 -4.4287782 -4.4288354 -4.4288473][-4.4288063 -4.4287758 -4.4286957 -4.4285378 -4.4283018 -4.4279981 -4.4276662 -4.4274483 -4.4276371 -4.4280019 -4.428288 -4.428493 -4.4286466 -4.4287462 -4.4287839][-4.4288454 -4.4288082 -4.4287338 -4.4285979 -4.4284034 -4.4281554 -4.4278846 -4.4276934 -4.4278121 -4.4280825 -4.4283056 -4.4284697 -4.4286022 -4.4286962 -4.4287434][-4.4289083 -4.4288764 -4.4288259 -4.4287424 -4.428627 -4.42848 -4.4283257 -4.4282165 -4.4282746 -4.4284177 -4.4285383 -4.4286213 -4.4286919 -4.4287386 -4.4287591][-4.4289255 -4.4288945 -4.428864 -4.428823 -4.4287567 -4.4286671 -4.4285827 -4.4285388 -4.4285812 -4.428658 -4.4287057 -4.4287324 -4.428762 -4.4287782 -4.4287739][-4.4289007 -4.4288692 -4.4288445 -4.4288206 -4.4287758 -4.4287205 -4.4286823 -4.4286871 -4.4287353 -4.4287868 -4.42879 -4.4287772 -4.4287753 -4.4287748 -4.42876][-4.4288445 -4.4288092 -4.4287839 -4.4287677 -4.4287372 -4.4287062 -4.4287004 -4.4287372 -4.428791 -4.4288282 -4.4288158 -4.4287882 -4.4287734 -4.4287767 -4.4287682][-4.4288282 -4.42879 -4.4287639 -4.4287519 -4.4287286 -4.4287143 -4.4287343 -4.4287887 -4.4288449 -4.4288754 -4.4288568 -4.4288249 -4.4287958 -4.4287853 -4.4287839][-4.4288497 -4.4288197 -4.4287987 -4.4287839 -4.4287586 -4.4287448 -4.4287648 -4.4288182 -4.4288683 -4.4288921 -4.4288712 -4.4288435 -4.4288135 -4.4287882 -4.42878]]...]
INFO - root - 2017-12-08 06:44:28.197561: step 29610, loss = 2.28, batch loss = 2.23 (35.1 examples/sec; 0.228 sec/batch; 19h:09m:59s remains)
INFO - root - 2017-12-08 06:44:30.421263: step 29620, loss = 2.28, batch loss = 2.23 (34.9 examples/sec; 0.229 sec/batch; 19h:18m:30s remains)
INFO - root - 2017-12-08 06:44:32.653729: step 29630, loss = 2.28, batch loss = 2.23 (35.8 examples/sec; 0.223 sec/batch; 18h:47m:17s remains)
INFO - root - 2017-12-08 06:44:34.886753: step 29640, loss = 2.28, batch loss = 2.23 (34.7 examples/sec; 0.230 sec/batch; 19h:22m:43s remains)
INFO - root - 2017-12-08 06:44:37.148301: step 29650, loss = 2.28, batch loss = 2.23 (33.3 examples/sec; 0.240 sec/batch; 20h:13m:40s remains)
INFO - root - 2017-12-08 06:44:39.438041: step 29660, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.226 sec/batch; 18h:58m:37s remains)
INFO - root - 2017-12-08 06:44:41.667557: step 29670, loss = 2.28, batch loss = 2.23 (35.8 examples/sec; 0.224 sec/batch; 18h:49m:15s remains)
INFO - root - 2017-12-08 06:44:43.882048: step 29680, loss = 2.28, batch loss = 2.23 (34.5 examples/sec; 0.232 sec/batch; 19h:28m:47s remains)
INFO - root - 2017-12-08 06:44:46.121559: step 29690, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.225 sec/batch; 18h:57m:51s remains)
INFO - root - 2017-12-08 06:44:48.348325: step 29700, loss = 2.28, batch loss = 2.23 (34.8 examples/sec; 0.230 sec/batch; 19h:20m:08s remains)
2017-12-08 06:44:48.616680: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.42867 -4.4287767 -4.4288492 -4.4289079 -4.4289365 -4.4289269 -4.4288816 -4.4288244 -4.4287839 -4.4287338 -4.4286733 -4.4286208 -4.4286203 -4.4287043 -4.4288154][-4.4286137 -4.4287415 -4.4288244 -4.4288855 -4.4289231 -4.4289265 -4.4288945 -4.4288468 -4.4288011 -4.4287376 -4.4286623 -4.4285846 -4.428556 -4.4286246 -4.4287367][-4.428596 -4.42873 -4.4288135 -4.4288607 -4.4288888 -4.4288874 -4.4288578 -4.4288192 -4.4287806 -4.4287243 -4.4286575 -4.4285789 -4.4285369 -4.4285855 -4.4286752][-4.4285994 -4.4287205 -4.4287949 -4.4288263 -4.4288378 -4.4288244 -4.42879 -4.4287634 -4.4287391 -4.4287043 -4.4286733 -4.4286237 -4.4285865 -4.4286127 -4.428659][-4.4286561 -4.4287438 -4.4287939 -4.4288039 -4.4287949 -4.4287624 -4.4287167 -4.4286981 -4.4287028 -4.4287038 -4.4287109 -4.4286976 -4.4286723 -4.4286728 -4.4286761][-4.4287434 -4.4287958 -4.4288135 -4.4288015 -4.4287734 -4.4287181 -4.428648 -4.4286294 -4.4286585 -4.4287047 -4.4287586 -4.4287806 -4.4287782 -4.4287667 -4.4287415][-4.4288192 -4.4288359 -4.4288259 -4.4287972 -4.4287605 -4.4286909 -4.4285965 -4.4285631 -4.4285979 -4.4286776 -4.4287734 -4.4288363 -4.4288621 -4.4288521 -4.4288206][-4.4288259 -4.4288225 -4.4287987 -4.4287658 -4.4287295 -4.4286542 -4.428546 -4.42849 -4.4285131 -4.4286065 -4.4287295 -4.4288368 -4.4289007 -4.4289141 -4.4288969][-4.4288015 -4.4287925 -4.4287648 -4.4287353 -4.4287071 -4.4286361 -4.4285197 -4.4284363 -4.4284325 -4.428525 -4.4286613 -4.4287958 -4.4288983 -4.4289479 -4.428956][-4.4287791 -4.4287763 -4.4287524 -4.4287229 -4.4286966 -4.4286308 -4.4285069 -4.4283991 -4.4283738 -4.428462 -4.4285946 -4.4287343 -4.4288554 -4.4289279 -4.4289632][-4.4287815 -4.42878 -4.428761 -4.4287324 -4.4287024 -4.4286442 -4.4285245 -4.4284081 -4.4283724 -4.4284472 -4.4285617 -4.4286857 -4.428802 -4.428884 -4.4289327][-4.4287648 -4.4287705 -4.4287667 -4.4287429 -4.4287109 -4.4286623 -4.4285603 -4.4284563 -4.4284172 -4.428473 -4.4285645 -4.4286685 -4.428771 -4.4288483 -4.4288955][-4.4287114 -4.4287252 -4.4287395 -4.4287295 -4.4287019 -4.428659 -4.4285841 -4.42851 -4.42848 -4.42852 -4.4285946 -4.4286852 -4.428771 -4.4288344 -4.4288745][-4.4286427 -4.4286647 -4.4286952 -4.4286947 -4.4286695 -4.4286356 -4.4285855 -4.4285464 -4.4285364 -4.4285665 -4.42863 -4.4287143 -4.4287815 -4.4288254 -4.4288507][-4.4285831 -4.42861 -4.4286461 -4.4286485 -4.4286251 -4.428597 -4.4285612 -4.4285493 -4.4285579 -4.4285932 -4.4286604 -4.42874 -4.4287906 -4.4288135 -4.4288173]]...]
INFO - root - 2017-12-08 06:44:50.860089: step 29710, loss = 2.28, batch loss = 2.23 (34.1 examples/sec; 0.234 sec/batch; 19h:43m:04s remains)
INFO - root - 2017-12-08 06:44:53.103444: step 29720, loss = 2.28, batch loss = 2.23 (33.2 examples/sec; 0.241 sec/batch; 20h:16m:08s remains)
INFO - root - 2017-12-08 06:44:55.327591: step 29730, loss = 2.28, batch loss = 2.23 (34.9 examples/sec; 0.229 sec/batch; 19h:16m:14s remains)
INFO - root - 2017-12-08 06:44:57.553025: step 29740, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 18h:43m:51s remains)
INFO - root - 2017-12-08 06:44:59.795199: step 29750, loss = 2.28, batch loss = 2.23 (35.7 examples/sec; 0.224 sec/batch; 18h:50m:32s remains)
INFO - root - 2017-12-08 06:45:02.014250: step 29760, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.220 sec/batch; 18h:31m:00s remains)
INFO - root - 2017-12-08 06:45:04.283481: step 29770, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.225 sec/batch; 18h:56m:24s remains)
INFO - root - 2017-12-08 06:45:06.524974: step 29780, loss = 2.28, batch loss = 2.23 (34.9 examples/sec; 0.229 sec/batch; 19h:15m:54s remains)
INFO - root - 2017-12-08 06:45:08.813511: step 29790, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 18h:43m:15s remains)
INFO - root - 2017-12-08 06:45:11.046004: step 29800, loss = 2.28, batch loss = 2.23 (35.3 examples/sec; 0.226 sec/batch; 19h:02m:38s remains)
2017-12-08 06:45:11.367941: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289117 -4.4289031 -4.4288955 -4.4288878 -4.428885 -4.4288874 -4.428894 -4.4289017 -4.4289031 -4.428895 -4.4288759 -4.428854 -4.4288306 -4.4288111 -4.4287992][-4.4288659 -4.4288588 -4.4288507 -4.4288378 -4.4288321 -4.4288354 -4.4288449 -4.4288592 -4.4288678 -4.4288664 -4.4288559 -4.4288397 -4.4288187 -4.4287968 -4.4287758][-4.4288292 -4.42882 -4.4288096 -4.4287939 -4.4287896 -4.4287925 -4.4288006 -4.4288139 -4.4288259 -4.4288335 -4.4288335 -4.4288254 -4.42881 -4.428791 -4.4287686][-4.4288125 -4.428791 -4.428771 -4.4287534 -4.4287491 -4.4287486 -4.4287448 -4.4287438 -4.4287567 -4.4287806 -4.4288 -4.4288092 -4.4288077 -4.4288015 -4.4287906][-4.4288225 -4.4287767 -4.4287338 -4.4287052 -4.4286962 -4.4286838 -4.428659 -4.4286408 -4.42865 -4.4286904 -4.4287276 -4.428751 -4.4287691 -4.4287877 -4.4288049][-4.4288263 -4.428751 -4.4286757 -4.4286251 -4.4285941 -4.428546 -4.4284759 -4.4284387 -4.4284663 -4.4285426 -4.4286103 -4.4286504 -4.4286928 -4.4287434 -4.4287872][-4.4288096 -4.42872 -4.428618 -4.4285297 -4.4284487 -4.4283414 -4.4282255 -4.42818 -4.4282403 -4.4283514 -4.4284558 -4.428524 -4.4286003 -4.4286847 -4.4287467][-4.4287982 -4.4287248 -4.4286237 -4.4285192 -4.4284062 -4.4282651 -4.42814 -4.4281149 -4.4281969 -4.4283018 -4.4283957 -4.428473 -4.4285712 -4.4286714 -4.4287348][-4.4287829 -4.4287558 -4.4286914 -4.4286146 -4.42852 -4.4283938 -4.4282975 -4.4282966 -4.4283638 -4.4284248 -4.4284787 -4.4285464 -4.42864 -4.4287205 -4.4287629][-4.428751 -4.42877 -4.428751 -4.4287157 -4.4286623 -4.4285841 -4.4285278 -4.4285355 -4.4285712 -4.4285917 -4.4286227 -4.4286752 -4.4287438 -4.4287939 -4.42881][-4.4287248 -4.4287791 -4.4287944 -4.4287934 -4.4287815 -4.428751 -4.4287238 -4.4287252 -4.4287367 -4.4287429 -4.4287672 -4.428803 -4.4288473 -4.4288721 -4.4288716][-4.4287477 -4.4288096 -4.428833 -4.4288521 -4.4288659 -4.42887 -4.4288616 -4.4288616 -4.4288707 -4.4288816 -4.4289031 -4.4289246 -4.4289455 -4.4289489 -4.4289384][-4.4288187 -4.4288607 -4.42888 -4.4289079 -4.4289327 -4.4289479 -4.4289446 -4.4289436 -4.4289513 -4.4289613 -4.4289756 -4.4289842 -4.4289908 -4.4289875 -4.4289813][-4.4288859 -4.428905 -4.4289174 -4.4289465 -4.4289756 -4.4289908 -4.4289908 -4.4289885 -4.4289918 -4.428997 -4.4290018 -4.4290028 -4.4290004 -4.4289966 -4.4289994][-4.4289436 -4.428946 -4.4289508 -4.4289713 -4.4289975 -4.42901 -4.42901 -4.4290075 -4.4290061 -4.4290075 -4.429009 -4.4290085 -4.4290071 -4.4290066 -4.4290109]]...]
INFO - root - 2017-12-08 06:45:13.659994: step 29810, loss = 2.28, batch loss = 2.23 (35.8 examples/sec; 0.223 sec/batch; 18h:47m:29s remains)
INFO - root - 2017-12-08 06:45:15.892509: step 29820, loss = 2.28, batch loss = 2.23 (34.8 examples/sec; 0.230 sec/batch; 19h:20m:00s remains)
INFO - root - 2017-12-08 06:45:18.123293: step 29830, loss = 2.28, batch loss = 2.23 (33.9 examples/sec; 0.236 sec/batch; 19h:49m:35s remains)
INFO - root - 2017-12-08 06:45:20.351920: step 29840, loss = 2.28, batch loss = 2.23 (34.2 examples/sec; 0.234 sec/batch; 19h:39m:17s remains)
INFO - root - 2017-12-08 06:45:22.611200: step 29850, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 18h:25m:39s remains)
INFO - root - 2017-12-08 06:45:24.884288: step 29860, loss = 2.28, batch loss = 2.23 (35.8 examples/sec; 0.223 sec/batch; 18h:46m:47s remains)
INFO - root - 2017-12-08 06:45:27.114997: step 29870, loss = 2.28, batch loss = 2.23 (35.0 examples/sec; 0.229 sec/batch; 19h:14m:28s remains)
INFO - root - 2017-12-08 06:45:29.338501: step 29880, loss = 2.28, batch loss = 2.23 (37.0 examples/sec; 0.216 sec/batch; 18h:09m:48s remains)
INFO - root - 2017-12-08 06:45:31.595299: step 29890, loss = 2.28, batch loss = 2.23 (35.2 examples/sec; 0.227 sec/batch; 19h:06m:57s remains)
INFO - root - 2017-12-08 06:45:33.856030: step 29900, loss = 2.28, batch loss = 2.23 (36.6 examples/sec; 0.219 sec/batch; 18h:22m:23s remains)
2017-12-08 06:45:34.170247: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.428792 -4.4287763 -4.4287925 -4.4288092 -4.4288139 -4.4288039 -4.4287715 -4.4287262 -4.4286895 -4.4287167 -4.4287744 -4.4288187 -4.4288373 -4.4288521 -4.4288735][-4.4287615 -4.4287419 -4.4287519 -4.4287758 -4.4287977 -4.4288116 -4.4287934 -4.4287515 -4.4287143 -4.4287319 -4.4287796 -4.42881 -4.4288144 -4.4288135 -4.4288244][-4.4287071 -4.4286804 -4.4286833 -4.42871 -4.4287462 -4.4287734 -4.428772 -4.4287543 -4.4287467 -4.4287672 -4.4288006 -4.4288154 -4.4287987 -4.4287739 -4.4287558][-4.4286528 -4.4286237 -4.4286327 -4.4286652 -4.4286938 -4.4287076 -4.4286995 -4.4286966 -4.4287271 -4.42877 -4.4288025 -4.4288116 -4.4287839 -4.4287348 -4.4286923][-4.4286127 -4.4285965 -4.4286246 -4.4286571 -4.4286594 -4.4286389 -4.4285946 -4.4285769 -4.4286337 -4.4287114 -4.4287572 -4.4287782 -4.4287524 -4.4286923 -4.4286327][-4.4286084 -4.4285831 -4.4286127 -4.4286394 -4.428617 -4.4285483 -4.4284348 -4.4283733 -4.4284678 -4.4286008 -4.4286704 -4.4287114 -4.4287043 -4.4286456 -4.428575][-4.4286127 -4.4285507 -4.4285645 -4.4285846 -4.4285421 -4.4284148 -4.428205 -4.4280558 -4.4281964 -4.4284034 -4.4285007 -4.4285665 -4.4286084 -4.428586 -4.4285336][-4.4286127 -4.4285073 -4.4284921 -4.4284968 -4.42843 -4.4282742 -4.42801 -4.4277873 -4.427968 -4.4282389 -4.4283609 -4.4284382 -4.4285192 -4.4285426 -4.42852][-4.4286618 -4.4285522 -4.4285221 -4.4285221 -4.4284558 -4.4283209 -4.4281249 -4.427979 -4.4280996 -4.4283123 -4.42841 -4.4284606 -4.4285345 -4.428575 -4.4285803][-4.4287553 -4.4286795 -4.4286504 -4.4286408 -4.4285831 -4.4284868 -4.4283729 -4.4283113 -4.4283686 -4.4284925 -4.428555 -4.4285822 -4.42863 -4.4286671 -4.4286809][-4.4288273 -4.4287949 -4.4287767 -4.4287496 -4.4287019 -4.4286342 -4.4285736 -4.42856 -4.4285913 -4.4286528 -4.4287009 -4.4287271 -4.4287472 -4.4287653 -4.4287782][-4.4288454 -4.4288549 -4.4288654 -4.4288464 -4.4288015 -4.4287524 -4.4287171 -4.4287238 -4.42875 -4.4287853 -4.4288154 -4.4288354 -4.4288368 -4.4288325 -4.4288325][-4.4288173 -4.42885 -4.4288783 -4.4288821 -4.4288521 -4.4288173 -4.4288034 -4.4288125 -4.4288354 -4.4288578 -4.4288692 -4.4288721 -4.4288635 -4.4288464 -4.4288335][-4.4287767 -4.4288139 -4.4288468 -4.4288673 -4.4288535 -4.4288278 -4.4288163 -4.4288235 -4.4288425 -4.4288511 -4.428853 -4.4288468 -4.4288378 -4.4288192 -4.428802][-4.4287705 -4.4288006 -4.4288249 -4.42884 -4.4288306 -4.4288073 -4.4287915 -4.4287915 -4.4288039 -4.4288082 -4.4288092 -4.4288058 -4.4288034 -4.4287968 -4.4287858]]...]
INFO - root - 2017-12-08 06:45:36.392021: step 29910, loss = 2.28, batch loss = 2.23 (36.6 examples/sec; 0.218 sec/batch; 18h:21m:51s remains)
INFO - root - 2017-12-08 06:45:38.642575: step 29920, loss = 2.28, batch loss = 2.23 (34.9 examples/sec; 0.229 sec/batch; 19h:14m:39s remains)
INFO - root - 2017-12-08 06:45:40.872711: step 29930, loss = 2.28, batch loss = 2.23 (36.0 examples/sec; 0.222 sec/batch; 18h:39m:06s remains)
INFO - root - 2017-12-08 06:45:43.107969: step 29940, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.225 sec/batch; 18h:56m:30s remains)
INFO - root - 2017-12-08 06:45:45.366424: step 29950, loss = 2.28, batch loss = 2.23 (34.1 examples/sec; 0.235 sec/batch; 19h:43m:52s remains)
INFO - root - 2017-12-08 06:45:47.612446: step 29960, loss = 2.28, batch loss = 2.23 (33.9 examples/sec; 0.236 sec/batch; 19h:49m:11s remains)
INFO - root - 2017-12-08 06:45:49.859615: step 29970, loss = 2.28, batch loss = 2.23 (35.1 examples/sec; 0.228 sec/batch; 19h:09m:50s remains)
INFO - root - 2017-12-08 06:45:52.126538: step 29980, loss = 2.28, batch loss = 2.23 (34.6 examples/sec; 0.231 sec/batch; 19h:26m:44s remains)
INFO - root - 2017-12-08 06:45:54.398095: step 29990, loss = 2.28, batch loss = 2.23 (34.2 examples/sec; 0.234 sec/batch; 19h:37m:45s remains)
INFO - root - 2017-12-08 06:45:56.616097: step 30000, loss = 2.28, batch loss = 2.23 (36.0 examples/sec; 0.222 sec/batch; 18h:41m:22s remains)
2017-12-08 06:45:56.895937: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4287095 -4.4287491 -4.4287868 -4.4288187 -4.428854 -4.4288793 -4.4288778 -4.4288545 -4.4288206 -4.4288039 -4.428823 -4.4288521 -4.4288745 -4.4288726 -4.4288545][-4.4287434 -4.4288011 -4.4288559 -4.4288988 -4.4289365 -4.4289665 -4.4289694 -4.4289484 -4.428915 -4.4288964 -4.4289088 -4.4289269 -4.428937 -4.4289265 -4.4289069][-4.4287562 -4.4288254 -4.4288855 -4.4289212 -4.4289408 -4.4289551 -4.4289551 -4.428947 -4.4289408 -4.4289446 -4.4289584 -4.4289637 -4.4289575 -4.4289308 -4.4289021][-4.4287581 -4.4288216 -4.428864 -4.4288793 -4.4288764 -4.4288716 -4.4288707 -4.4288859 -4.4289145 -4.4289451 -4.4289622 -4.4289579 -4.4289365 -4.4288936 -4.4288487][-4.428771 -4.4288044 -4.4288096 -4.4287891 -4.4287615 -4.4287491 -4.4287562 -4.4287915 -4.4288473 -4.4289017 -4.4289293 -4.428926 -4.4288974 -4.428843 -4.42879][-4.4287906 -4.4287767 -4.4287348 -4.4286737 -4.4286189 -4.4285946 -4.4286041 -4.4286609 -4.4287419 -4.4288149 -4.4288487 -4.4288516 -4.4288306 -4.4287829 -4.4287381][-4.4288011 -4.4287481 -4.4286704 -4.4285741 -4.4284811 -4.4284229 -4.4284191 -4.428493 -4.4286036 -4.4286914 -4.4287329 -4.4287477 -4.4287467 -4.4287248 -4.4286981][-4.428802 -4.4287262 -4.4286318 -4.4285154 -4.42839 -4.4283061 -4.4282994 -4.4283867 -4.4285049 -4.42858 -4.4286094 -4.4286356 -4.4286671 -4.4286857 -4.4286942][-4.4288015 -4.4287267 -4.4286361 -4.4285164 -4.4283814 -4.4283013 -4.4283109 -4.4283929 -4.4284859 -4.4285326 -4.4285436 -4.4285684 -4.4286137 -4.428659 -4.4286981][-4.4287844 -4.4287109 -4.4286256 -4.4285107 -4.4283872 -4.4283333 -4.4283719 -4.4284577 -4.428535 -4.4285626 -4.4285526 -4.4285541 -4.4285784 -4.428617 -4.4286594][-4.4287491 -4.4286747 -4.4285951 -4.4284935 -4.4284024 -4.4283853 -4.4284525 -4.4285464 -4.4286132 -4.4286189 -4.4285841 -4.4285445 -4.4285278 -4.4285526 -4.4286017][-4.42871 -4.4286513 -4.4286003 -4.4285436 -4.428503 -4.428514 -4.4285793 -4.4286551 -4.4287 -4.4286785 -4.42861 -4.4285269 -4.42848 -4.4285097 -4.4285765][-4.4287353 -4.4287167 -4.428699 -4.4286761 -4.4286594 -4.4286747 -4.4287219 -4.4287715 -4.4287877 -4.4287491 -4.4286737 -4.4285893 -4.4285502 -4.4285831 -4.428647][-4.428813 -4.4288197 -4.4288187 -4.4288087 -4.4288025 -4.4288197 -4.4288521 -4.4288826 -4.4288855 -4.4288459 -4.4287782 -4.428709 -4.42868 -4.4286947 -4.4287314][-4.4288721 -4.4288888 -4.4288936 -4.4288836 -4.4288759 -4.4288874 -4.4289064 -4.4289246 -4.4289293 -4.4289055 -4.42886 -4.428803 -4.4287639 -4.4287472 -4.4287515]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-momentum-lr0.001-defgroup-1/model.ckpt-30000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-momentum-lr0.001-defgroup-1/model.ckpt-30000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-08 06:45:59.480395: step 30010, loss = 2.28, batch loss = 2.23 (33.8 examples/sec; 0.236 sec/batch; 19h:51m:55s remains)
INFO - root - 2017-12-08 06:46:01.711157: step 30020, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.221 sec/batch; 18h:32m:25s remains)
INFO - root - 2017-12-08 06:46:03.924902: step 30030, loss = 2.28, batch loss = 2.23 (36.0 examples/sec; 0.222 sec/batch; 18h:38m:42s remains)
INFO - root - 2017-12-08 06:46:06.174587: step 30040, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.222 sec/batch; 18h:37m:51s remains)
INFO - root - 2017-12-08 06:46:08.437987: step 30050, loss = 2.28, batch loss = 2.23 (37.1 examples/sec; 0.216 sec/batch; 18h:06m:46s remains)
INFO - root - 2017-12-08 06:46:10.676164: step 30060, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.222 sec/batch; 18h:37m:37s remains)
INFO - root - 2017-12-08 06:46:12.916803: step 30070, loss = 2.28, batch loss = 2.23 (34.8 examples/sec; 0.230 sec/batch; 19h:18m:39s remains)
INFO - root - 2017-12-08 06:46:15.154851: step 30080, loss = 2.28, batch loss = 2.23 (37.0 examples/sec; 0.216 sec/batch; 18h:08m:43s remains)
INFO - root - 2017-12-08 06:46:17.403548: step 30090, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 18h:25m:07s remains)
INFO - root - 2017-12-08 06:46:19.644091: step 30100, loss = 2.28, batch loss = 2.23 (35.6 examples/sec; 0.225 sec/batch; 18h:53m:14s remains)
2017-12-08 06:46:19.948196: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288516 -4.4288678 -4.4288754 -4.4288769 -4.4288912 -4.4289169 -4.4289465 -4.428966 -4.42897 -4.42895 -4.4289165 -4.428884 -4.4288578 -4.4288449 -4.4288392][-4.4288168 -4.4288459 -4.42887 -4.4288836 -4.4289055 -4.4289384 -4.4289722 -4.4289885 -4.4289861 -4.4289622 -4.4289274 -4.428894 -4.4288592 -4.4288411 -4.4288373][-4.42877 -4.4287834 -4.4287949 -4.4288044 -4.4288363 -4.428884 -4.4289355 -4.4289637 -4.4289689 -4.4289508 -4.42892 -4.4288869 -4.4288464 -4.4288106 -4.428791][-4.428719 -4.4286838 -4.4286532 -4.4286489 -4.4286909 -4.4287629 -4.4288421 -4.42889 -4.4289103 -4.4289083 -4.4288936 -4.428875 -4.428843 -4.428791 -4.42875][-4.4287095 -4.4286261 -4.4285383 -4.428494 -4.4285274 -4.4286194 -4.4287229 -4.4287934 -4.4288316 -4.4288454 -4.4288454 -4.4288406 -4.4288254 -4.4287753 -4.4287205][-4.428762 -4.4286642 -4.4285474 -4.4284554 -4.4284396 -4.4285064 -4.4285989 -4.428678 -4.4287353 -4.4287667 -4.4287829 -4.4287915 -4.4287868 -4.4287395 -4.4286737][-4.4288425 -4.4287696 -4.4286661 -4.42856 -4.4284983 -4.4284983 -4.4285259 -4.4285703 -4.42862 -4.4286609 -4.4287009 -4.4287295 -4.4287329 -4.42869 -4.4286222][-4.4289131 -4.428875 -4.4288111 -4.4287276 -4.4286575 -4.4286113 -4.4285808 -4.4285645 -4.428576 -4.4286056 -4.4286504 -4.4286914 -4.428709 -4.4286814 -4.42862][-4.4289412 -4.4289279 -4.4289036 -4.4288573 -4.428802 -4.4287519 -4.42871 -4.4286647 -4.4286413 -4.4286532 -4.4286933 -4.4287314 -4.4287553 -4.4287457 -4.4286985][-4.4289207 -4.4289184 -4.4289165 -4.4288983 -4.4288673 -4.4288454 -4.4288287 -4.428792 -4.4287648 -4.4287648 -4.4287972 -4.4288278 -4.4288421 -4.428834 -4.4287982][-4.4288774 -4.4288745 -4.428874 -4.4288688 -4.42886 -4.4288697 -4.4288855 -4.4288783 -4.4288645 -4.4288611 -4.4288845 -4.4289069 -4.4289103 -4.4288931 -4.428864][-4.4288273 -4.4288163 -4.4287987 -4.4287915 -4.4287987 -4.4288282 -4.4288626 -4.4288745 -4.42888 -4.4288855 -4.4289055 -4.4289289 -4.4289341 -4.4289227 -4.4289074][-4.4287963 -4.4287691 -4.4287267 -4.4287043 -4.42871 -4.4287429 -4.4287858 -4.4288106 -4.4288306 -4.4288549 -4.428884 -4.4289074 -4.4289145 -4.428916 -4.4289203][-4.4287968 -4.4287505 -4.428689 -4.4286456 -4.4286375 -4.4286661 -4.428719 -4.4287529 -4.4287834 -4.4288263 -4.4288669 -4.4288917 -4.428896 -4.4288969 -4.428906][-4.428822 -4.4287691 -4.4287047 -4.4286585 -4.4286466 -4.4286752 -4.428731 -4.4287667 -4.4287934 -4.4288263 -4.4288559 -4.4288788 -4.4288883 -4.4288936 -4.4289036]]...]
INFO - root - 2017-12-08 06:46:22.179720: step 30110, loss = 2.28, batch loss = 2.23 (36.0 examples/sec; 0.222 sec/batch; 18h:39m:25s remains)
INFO - root - 2017-12-08 06:46:24.451376: step 30120, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.221 sec/batch; 18h:35m:53s remains)
INFO - root - 2017-12-08 06:46:26.704155: step 30130, loss = 2.28, batch loss = 2.23 (34.9 examples/sec; 0.229 sec/batch; 19h:15m:00s remains)
INFO - root - 2017-12-08 06:46:28.931409: step 30140, loss = 2.28, batch loss = 2.23 (35.4 examples/sec; 0.226 sec/batch; 18h:59m:03s remains)
INFO - root - 2017-12-08 06:46:31.158468: step 30150, loss = 2.28, batch loss = 2.23 (34.8 examples/sec; 0.230 sec/batch; 19h:18m:13s remains)
INFO - root - 2017-12-08 06:46:33.415800: step 30160, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.220 sec/batch; 18h:30m:30s remains)
INFO - root - 2017-12-08 06:46:35.638368: step 30170, loss = 2.28, batch loss = 2.23 (36.6 examples/sec; 0.218 sec/batch; 18h:20m:56s remains)
INFO - root - 2017-12-08 06:46:37.880072: step 30180, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 18h:25m:43s remains)
INFO - root - 2017-12-08 06:46:40.113940: step 30190, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 18h:42m:52s remains)
INFO - root - 2017-12-08 06:46:42.338993: step 30200, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.222 sec/batch; 18h:37m:55s remains)
2017-12-08 06:46:42.659763: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289665 -4.4289584 -4.4289584 -4.4289656 -4.4289732 -4.428977 -4.4289756 -4.4289732 -4.4289727 -4.4289708 -4.4289656 -4.4289594 -4.4289527 -4.42895 -4.4289522][-4.4289503 -4.4289508 -4.4289618 -4.4289756 -4.4289794 -4.4289722 -4.4289584 -4.42895 -4.4289474 -4.4289494 -4.4289494 -4.4289451 -4.4289379 -4.4289336 -4.428936][-4.4289446 -4.4289522 -4.4289632 -4.4289689 -4.4289589 -4.4289351 -4.4289069 -4.42889 -4.4288893 -4.4289017 -4.4289131 -4.4289188 -4.428916 -4.4289136 -4.4289174][-4.4289589 -4.4289627 -4.4289618 -4.4289489 -4.4289193 -4.4288754 -4.42883 -4.4288034 -4.4288006 -4.4288235 -4.4288497 -4.42887 -4.4288831 -4.428894 -4.4289122][-4.4289784 -4.42897 -4.4289513 -4.42892 -4.4288731 -4.4288068 -4.4287305 -4.4286733 -4.42866 -4.4286985 -4.4287477 -4.428793 -4.4288373 -4.4288797 -4.4289241][-4.4289737 -4.4289408 -4.4289012 -4.4288545 -4.4287868 -4.4286804 -4.4285398 -4.4284244 -4.4284072 -4.4284987 -4.4286084 -4.4287038 -4.4287891 -4.4288592 -4.4289184][-4.42895 -4.4288874 -4.4288182 -4.4287457 -4.4286447 -4.4284849 -4.4282684 -4.428091 -4.4281092 -4.4283013 -4.4284992 -4.4286485 -4.4287577 -4.428833 -4.4288912][-4.4289145 -4.4288273 -4.4287357 -4.428648 -4.4285336 -4.4283581 -4.4281383 -4.4279737 -4.4280543 -4.4283056 -4.4285288 -4.4286776 -4.4287691 -4.4288216 -4.4288616][-4.4288974 -4.4288006 -4.4287066 -4.4286294 -4.4285555 -4.4284577 -4.4283438 -4.4282722 -4.4283471 -4.4285278 -4.42868 -4.4287705 -4.4288197 -4.4288435 -4.428853][-4.4289093 -4.4288211 -4.4287434 -4.4286995 -4.428689 -4.4286757 -4.4286489 -4.4286256 -4.4286556 -4.4287372 -4.4288073 -4.428844 -4.4288597 -4.4288573 -4.4288435][-4.4289303 -4.4288549 -4.4287939 -4.4287806 -4.4288087 -4.4288344 -4.4288406 -4.4288187 -4.4288092 -4.4288311 -4.428853 -4.4288607 -4.4288535 -4.4288354 -4.4288139][-4.428947 -4.4288764 -4.4288173 -4.4288096 -4.4288454 -4.4288788 -4.4288869 -4.4288526 -4.4288158 -4.4288092 -4.4288106 -4.4288054 -4.4287896 -4.4287691 -4.4287577][-4.4289603 -4.4288931 -4.4288378 -4.4288263 -4.4288507 -4.4288654 -4.4288526 -4.4287982 -4.42874 -4.4287109 -4.4287071 -4.428709 -4.4287076 -4.4287066 -4.4287171][-4.4289665 -4.4289012 -4.4288511 -4.4288354 -4.4288435 -4.4288392 -4.4288073 -4.4287319 -4.4286485 -4.4285936 -4.4285874 -4.4286208 -4.4286575 -4.4286857 -4.4287186][-4.4289536 -4.42889 -4.4288397 -4.4288216 -4.428823 -4.4288168 -4.4287858 -4.4287062 -4.4286036 -4.4285231 -4.4285188 -4.428586 -4.4286613 -4.4287095 -4.4287477]]...]
INFO - root - 2017-12-08 06:46:44.912167: step 30210, loss = 2.28, batch loss = 2.23 (33.5 examples/sec; 0.239 sec/batch; 20h:01m:56s remains)
INFO - root - 2017-12-08 06:46:47.146062: step 30220, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.220 sec/batch; 18h:30m:41s remains)
INFO - root - 2017-12-08 06:46:49.379616: step 30230, loss = 2.28, batch loss = 2.23 (35.7 examples/sec; 0.224 sec/batch; 18h:49m:15s remains)
INFO - root - 2017-12-08 06:46:51.641853: step 30240, loss = 2.28, batch loss = 2.23 (36.0 examples/sec; 0.222 sec/batch; 18h:39m:01s remains)
INFO - root - 2017-12-08 06:46:53.905142: step 30250, loss = 2.28, batch loss = 2.23 (35.4 examples/sec; 0.226 sec/batch; 18h:59m:45s remains)
INFO - root - 2017-12-08 06:46:56.165016: step 30260, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 18h:42m:11s remains)
INFO - root - 2017-12-08 06:46:58.385854: step 30270, loss = 2.28, batch loss = 2.23 (37.2 examples/sec; 0.215 sec/batch; 18h:03m:19s remains)
INFO - root - 2017-12-08 06:47:00.622178: step 30280, loss = 2.28, batch loss = 2.23 (37.2 examples/sec; 0.215 sec/batch; 18h:03m:10s remains)
INFO - root - 2017-12-08 06:47:02.855603: step 30290, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.221 sec/batch; 18h:31m:30s remains)
INFO - root - 2017-12-08 06:47:05.080201: step 30300, loss = 2.28, batch loss = 2.23 (37.3 examples/sec; 0.214 sec/batch; 17h:59m:03s remains)
2017-12-08 06:47:05.371896: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4287987 -4.4287581 -4.42875 -4.4287472 -4.4287419 -4.4287453 -4.4287806 -4.4288168 -4.4288325 -4.4288363 -4.4288206 -4.4287796 -4.4287066 -4.428647 -4.4286208][-4.4287958 -4.4287496 -4.4287338 -4.4287252 -4.4287186 -4.4287205 -4.4287529 -4.4287844 -4.4288006 -4.4288116 -4.428812 -4.42879 -4.4287353 -4.4286866 -4.4286613][-4.4287682 -4.4287229 -4.4287062 -4.4286957 -4.4286952 -4.4287028 -4.4287314 -4.4287543 -4.4287667 -4.428781 -4.4287958 -4.4287977 -4.4287696 -4.4287291 -4.4287024][-4.4287462 -4.4287028 -4.4286814 -4.4286594 -4.428658 -4.4286642 -4.4286828 -4.4286981 -4.428709 -4.4287305 -4.4287624 -4.4287863 -4.4287834 -4.4287567 -4.4287271][-4.4287539 -4.4287109 -4.4286814 -4.4286346 -4.4286089 -4.4285984 -4.4285994 -4.4286046 -4.4286189 -4.4286523 -4.4287095 -4.4287653 -4.428792 -4.4287834 -4.4287515][-4.4287429 -4.4287066 -4.4286737 -4.4286146 -4.4285722 -4.42854 -4.4285116 -4.42851 -4.4285398 -4.4285979 -4.4286819 -4.4287577 -4.4287963 -4.4287906 -4.4287524][-4.4287186 -4.4286828 -4.4286432 -4.4285746 -4.4285192 -4.4284658 -4.4284039 -4.4283924 -4.4284525 -4.4285479 -4.4286613 -4.4287486 -4.42879 -4.4287834 -4.4287419][-4.4286819 -4.4286523 -4.4286046 -4.4285192 -4.4284396 -4.4283462 -4.4282227 -4.4281888 -4.4282813 -4.4284291 -4.4285831 -4.428699 -4.4287596 -4.4287672 -4.4287291][-4.4286685 -4.4286528 -4.4286103 -4.4285254 -4.4284377 -4.4283171 -4.428154 -4.4280825 -4.4281654 -4.4283366 -4.4285173 -4.4286561 -4.4287343 -4.4287543 -4.4287257][-4.4287119 -4.4287105 -4.4286823 -4.428618 -4.4285526 -4.4284635 -4.42834 -4.4282808 -4.4283204 -4.4284263 -4.4285469 -4.4286513 -4.4287248 -4.4287543 -4.4287429][-4.4287734 -4.4287782 -4.4287591 -4.4287081 -4.4286575 -4.4285955 -4.4285135 -4.4284759 -4.4284887 -4.4285469 -4.4286141 -4.4286709 -4.4287214 -4.4287491 -4.4287472][-4.4288049 -4.4288087 -4.4287934 -4.4287529 -4.4287162 -4.4286795 -4.428628 -4.4286008 -4.4285955 -4.4286184 -4.42865 -4.4286752 -4.4287019 -4.42872 -4.4287324][-4.4288039 -4.4287982 -4.4287863 -4.4287577 -4.4287372 -4.4287219 -4.4286957 -4.4286737 -4.4286523 -4.4286432 -4.4286489 -4.4286647 -4.4286828 -4.4286909 -4.4287052][-4.428802 -4.4287858 -4.4287753 -4.4287591 -4.4287548 -4.4287586 -4.4287515 -4.4287319 -4.4286952 -4.4286642 -4.4286561 -4.4286675 -4.428679 -4.4286828 -4.4286938][-4.4287949 -4.42877 -4.4287558 -4.4287486 -4.4287562 -4.4287739 -4.4287791 -4.4287639 -4.4287219 -4.4286804 -4.4286609 -4.4286656 -4.42867 -4.428679 -4.4287019]]...]
INFO - root - 2017-12-08 06:47:07.614041: step 30310, loss = 2.28, batch loss = 2.23 (35.7 examples/sec; 0.224 sec/batch; 18h:48m:44s remains)
INFO - root - 2017-12-08 06:47:09.838815: step 30320, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 18h:26m:47s remains)
INFO - root - 2017-12-08 06:47:12.063694: step 30330, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 18h:28m:06s remains)
INFO - root - 2017-12-08 06:47:14.312767: step 30340, loss = 2.28, batch loss = 2.23 (35.2 examples/sec; 0.227 sec/batch; 19h:05m:31s remains)
INFO - root - 2017-12-08 06:47:16.566634: step 30350, loss = 2.28, batch loss = 2.23 (35.0 examples/sec; 0.229 sec/batch; 19h:10m:54s remains)
INFO - root - 2017-12-08 06:47:18.768435: step 30360, loss = 2.28, batch loss = 2.23 (37.6 examples/sec; 0.213 sec/batch; 17h:52m:40s remains)
INFO - root - 2017-12-08 06:47:21.004843: step 30370, loss = 2.28, batch loss = 2.23 (36.0 examples/sec; 0.222 sec/batch; 18h:38m:09s remains)
INFO - root - 2017-12-08 06:47:23.277447: step 30380, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.221 sec/batch; 18h:34m:20s remains)
INFO - root - 2017-12-08 06:47:25.506004: step 30390, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.225 sec/batch; 18h:54m:07s remains)
INFO - root - 2017-12-08 06:47:27.778921: step 30400, loss = 2.28, batch loss = 2.23 (34.0 examples/sec; 0.235 sec/batch; 19h:43m:30s remains)
2017-12-08 06:47:28.067342: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288306 -4.4288368 -4.4288411 -4.4288425 -4.4288425 -4.428843 -4.428843 -4.4288411 -4.4288383 -4.428834 -4.4288287 -4.4288239 -4.4288192 -4.4288158 -4.4288139][-4.4288688 -4.4288688 -4.4288683 -4.428865 -4.4288588 -4.4288545 -4.4288549 -4.4288583 -4.4288616 -4.4288635 -4.4288626 -4.4288607 -4.4288588 -4.4288568 -4.4288568][-4.4288726 -4.4288678 -4.4288597 -4.428843 -4.428823 -4.428813 -4.4288173 -4.4288321 -4.4288483 -4.428863 -4.4288735 -4.4288793 -4.4288821 -4.428884 -4.4288869][-4.4288306 -4.4288216 -4.4288054 -4.4287729 -4.4287372 -4.4287176 -4.4287233 -4.4287491 -4.428781 -4.428813 -4.4288397 -4.4288568 -4.4288654 -4.42887 -4.428875][-4.42876 -4.4287472 -4.4287257 -4.4286876 -4.4286447 -4.4286194 -4.42862 -4.428647 -4.4286885 -4.4287348 -4.4287734 -4.4287972 -4.42881 -4.4288163 -4.428822][-4.4287095 -4.428689 -4.4286618 -4.4286237 -4.4285812 -4.4285483 -4.4285316 -4.4285493 -4.4285951 -4.4286513 -4.4286957 -4.4287233 -4.4287381 -4.4287453 -4.4287534][-4.4287152 -4.42869 -4.428659 -4.4286242 -4.4285817 -4.4285336 -4.4284887 -4.4284821 -4.4285235 -4.4285822 -4.4286261 -4.42865 -4.4286609 -4.428668 -4.4286828][-4.4287691 -4.428741 -4.4287095 -4.4286814 -4.4286437 -4.4285889 -4.4285264 -4.4284983 -4.4285235 -4.4285674 -4.4285979 -4.4286075 -4.4286032 -4.4286032 -4.4286218][-4.4288278 -4.4288 -4.4287672 -4.4287424 -4.4287124 -4.4286656 -4.4286103 -4.4285803 -4.4285936 -4.4286203 -4.4286375 -4.4286346 -4.4286189 -4.4286084 -4.4286227][-4.42887 -4.4288478 -4.4288182 -4.4287977 -4.4287772 -4.4287434 -4.4287043 -4.4286809 -4.4286861 -4.4286981 -4.4287038 -4.4286952 -4.4286752 -4.42866 -4.4286685][-4.4288931 -4.4288812 -4.42886 -4.4288473 -4.4288363 -4.4288173 -4.4287944 -4.42878 -4.42878 -4.4287825 -4.4287815 -4.4287715 -4.4287524 -4.4287362 -4.4287386][-4.4288878 -4.4288878 -4.4288812 -4.4288793 -4.4288788 -4.4288731 -4.4288635 -4.428854 -4.4288497 -4.4288473 -4.4288459 -4.4288378 -4.4288197 -4.4288011 -4.4287977][-4.4288549 -4.4288616 -4.428865 -4.4288688 -4.4288745 -4.4288783 -4.4288754 -4.4288683 -4.4288611 -4.4288583 -4.4288607 -4.4288554 -4.4288378 -4.4288206 -4.4288173][-4.4288206 -4.4288292 -4.4288354 -4.4288406 -4.4288445 -4.4288468 -4.42884 -4.4288278 -4.4288177 -4.4288182 -4.4288268 -4.4288263 -4.4288139 -4.4288015 -4.428803][-4.4288025 -4.42881 -4.4288154 -4.4288163 -4.4288111 -4.4288039 -4.4287877 -4.4287682 -4.4287581 -4.4287634 -4.4287758 -4.428781 -4.4287753 -4.4287682 -4.4287763]]...]
INFO - root - 2017-12-08 06:47:30.300117: step 30410, loss = 2.28, batch loss = 2.23 (34.5 examples/sec; 0.232 sec/batch; 19h:26m:54s remains)
INFO - root - 2017-12-08 06:47:32.524964: step 30420, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.221 sec/batch; 18h:30m:08s remains)
INFO - root - 2017-12-08 06:47:34.761218: step 30430, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.225 sec/batch; 18h:53m:15s remains)
INFO - root - 2017-12-08 06:47:36.985554: step 30440, loss = 2.28, batch loss = 2.23 (35.7 examples/sec; 0.224 sec/batch; 18h:48m:22s remains)
INFO - root - 2017-12-08 06:47:39.211517: step 30450, loss = 2.28, batch loss = 2.23 (35.6 examples/sec; 0.225 sec/batch; 18h:51m:31s remains)
INFO - root - 2017-12-08 06:47:41.444199: step 30460, loss = 2.28, batch loss = 2.23 (33.6 examples/sec; 0.238 sec/batch; 19h:58m:57s remains)
INFO - root - 2017-12-08 06:47:43.694268: step 30470, loss = 2.28, batch loss = 2.23 (35.8 examples/sec; 0.224 sec/batch; 18h:46m:25s remains)
INFO - root - 2017-12-08 06:47:45.913017: step 30480, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 18h:26m:42s remains)
INFO - root - 2017-12-08 06:47:48.179841: step 30490, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.220 sec/batch; 18h:28m:50s remains)
INFO - root - 2017-12-08 06:47:50.428809: step 30500, loss = 2.28, batch loss = 2.23 (35.1 examples/sec; 0.228 sec/batch; 19h:08m:10s remains)
2017-12-08 06:47:50.758175: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.42893 -4.4289317 -4.4289351 -4.4289389 -4.4289412 -4.4289312 -4.4288979 -4.4288468 -4.4288192 -4.4288292 -4.4288411 -4.4287667 -4.4286652 -4.4286175 -4.4286284][-4.4289389 -4.4289331 -4.4289351 -4.4289312 -4.4289179 -4.4288955 -4.4288535 -4.4287958 -4.428782 -4.4288282 -4.4288712 -4.4288197 -4.42873 -4.4286776 -4.4286752][-4.4289365 -4.4289231 -4.4289207 -4.4288974 -4.4288578 -4.4288135 -4.4287457 -4.4286494 -4.4286518 -4.4287524 -4.4288354 -4.4288254 -4.428771 -4.4287324 -4.4287233][-4.4289317 -4.4289117 -4.4288907 -4.4288321 -4.4287567 -4.4286904 -4.4285917 -4.4284444 -4.4284649 -4.4286408 -4.428772 -4.4288116 -4.4287958 -4.4287658 -4.4287386][-4.4289274 -4.4288988 -4.4288573 -4.428761 -4.4286408 -4.4285355 -4.4283867 -4.4281731 -4.428225 -4.4284983 -4.428689 -4.4287767 -4.42879 -4.4287491 -4.42869][-4.4289131 -4.4288778 -4.4288211 -4.4287014 -4.4285378 -4.4283838 -4.4281583 -4.4278374 -4.4279246 -4.4283195 -4.4285846 -4.4287138 -4.4287453 -4.4286861 -4.4285908][-4.4288812 -4.4288406 -4.4287734 -4.4286427 -4.4284534 -4.4282708 -4.4279981 -4.427597 -4.4277115 -4.4281874 -4.4284968 -4.4286456 -4.4286814 -4.4285903 -4.4284511][-4.4288383 -4.4288058 -4.4287462 -4.4286275 -4.4284539 -4.4283013 -4.4280925 -4.42778 -4.4278965 -4.4283066 -4.4285626 -4.428668 -4.4286723 -4.4285307 -4.4283338][-4.4287858 -4.4287758 -4.4287405 -4.4286656 -4.4285383 -4.4284272 -4.4282918 -4.428093 -4.4282131 -4.4285269 -4.4287081 -4.428762 -4.42873 -4.42855 -4.4283147][-4.4287357 -4.4287519 -4.4287477 -4.4287152 -4.42863 -4.4285407 -4.4284415 -4.4283061 -4.428412 -4.428658 -4.4288034 -4.4288487 -4.4288177 -4.42863 -4.4283996][-4.4287062 -4.4287338 -4.4287448 -4.4287372 -4.428678 -4.4286132 -4.4285431 -4.4284444 -4.4285126 -4.4286919 -4.4288187 -4.428885 -4.4288759 -4.428721 -4.4285312][-4.4287024 -4.4287229 -4.4287376 -4.4287424 -4.42871 -4.4286771 -4.4286451 -4.4285865 -4.4286284 -4.4287391 -4.4288268 -4.4288874 -4.4288859 -4.428772 -4.4286318][-4.4287162 -4.4287248 -4.4287395 -4.4287577 -4.4287515 -4.4287348 -4.4287214 -4.4287024 -4.4287348 -4.4287992 -4.4288526 -4.4288936 -4.428884 -4.4288039 -4.4287043][-4.4287314 -4.4287429 -4.428771 -4.4288058 -4.4288173 -4.4287968 -4.4287739 -4.4287629 -4.4287829 -4.4288268 -4.4288688 -4.4288979 -4.4288893 -4.4288349 -4.42877][-4.428751 -4.4287648 -4.4288015 -4.428844 -4.4288573 -4.4288311 -4.4287958 -4.4287791 -4.428792 -4.4288292 -4.4288712 -4.4289031 -4.4289026 -4.4288716 -4.4288392]]...]
INFO - root - 2017-12-08 06:47:53.003117: step 30510, loss = 2.28, batch loss = 2.23 (35.4 examples/sec; 0.226 sec/batch; 18h:57m:51s remains)
INFO - root - 2017-12-08 06:47:55.223712: step 30520, loss = 2.28, batch loss = 2.23 (37.2 examples/sec; 0.215 sec/batch; 18h:01m:32s remains)
INFO - root - 2017-12-08 06:47:57.492010: step 30530, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.220 sec/batch; 18h:28m:04s remains)
INFO - root - 2017-12-08 06:47:59.716776: step 30540, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.221 sec/batch; 18h:30m:04s remains)
INFO - root - 2017-12-08 06:48:01.954072: step 30550, loss = 2.28, batch loss = 2.23 (35.2 examples/sec; 0.227 sec/batch; 19h:04m:00s remains)
INFO - root - 2017-12-08 06:48:04.202142: step 30560, loss = 2.28, batch loss = 2.23 (34.9 examples/sec; 0.229 sec/batch; 19h:12m:26s remains)
INFO - root - 2017-12-08 06:48:06.447230: step 30570, loss = 2.28, batch loss = 2.23 (34.5 examples/sec; 0.232 sec/batch; 19h:27m:21s remains)
INFO - root - 2017-12-08 06:48:08.687318: step 30580, loss = 2.28, batch loss = 2.23 (35.1 examples/sec; 0.228 sec/batch; 19h:06m:30s remains)
INFO - root - 2017-12-08 06:48:10.954576: step 30590, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.222 sec/batch; 18h:35m:47s remains)
INFO - root - 2017-12-08 06:48:13.162349: step 30600, loss = 2.28, batch loss = 2.23 (35.6 examples/sec; 0.225 sec/batch; 18h:50m:14s remains)
2017-12-08 06:48:13.467006: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288511 -4.4288607 -4.428884 -4.4289131 -4.4289227 -4.4289193 -4.4289193 -4.428915 -4.4289145 -4.4289112 -4.4289107 -4.4289303 -4.4289484 -4.4289432 -4.4289293][-4.4289222 -4.4289389 -4.4289603 -4.4289851 -4.4289927 -4.428988 -4.4289837 -4.4289784 -4.4289775 -4.4289718 -4.4289689 -4.4289784 -4.4289889 -4.4289813 -4.4289641][-4.4289517 -4.4289665 -4.4289818 -4.4289961 -4.4289947 -4.4289842 -4.4289746 -4.4289694 -4.4289684 -4.4289618 -4.4289556 -4.4289575 -4.4289641 -4.4289565 -4.428937][-4.4289384 -4.428947 -4.4289536 -4.4289556 -4.4289436 -4.4289241 -4.4289088 -4.4289069 -4.4289064 -4.4288979 -4.4288859 -4.4288783 -4.42888 -4.4288721 -4.428854][-4.4288821 -4.428874 -4.428863 -4.4288478 -4.428823 -4.4287891 -4.4287596 -4.4287615 -4.4287672 -4.4287615 -4.428751 -4.4287419 -4.4287453 -4.4287438 -4.4287362][-4.4287958 -4.428762 -4.4287329 -4.4287004 -4.4286604 -4.4286022 -4.4285483 -4.4285579 -4.4285851 -4.4285951 -4.4285946 -4.42859 -4.4286008 -4.4286122 -4.4286165][-4.4287024 -4.4286509 -4.4286127 -4.4285722 -4.4285235 -4.428443 -4.428359 -4.4283762 -4.4284339 -4.4284635 -4.4284773 -4.4284811 -4.4284987 -4.4285207 -4.4285355][-4.4286823 -4.4286394 -4.4286146 -4.4285893 -4.4285545 -4.4284897 -4.4284158 -4.4284215 -4.428472 -4.4285045 -4.428524 -4.4285345 -4.42854 -4.4285417 -4.428546][-4.4287243 -4.4287105 -4.4287128 -4.4287133 -4.4286985 -4.4286637 -4.42862 -4.4286108 -4.4286308 -4.4286466 -4.4286556 -4.4286604 -4.4286566 -4.4286408 -4.4286323][-4.4287667 -4.4287739 -4.428793 -4.4288054 -4.4288 -4.4287815 -4.4287615 -4.4287486 -4.4287477 -4.4287467 -4.4287491 -4.4287586 -4.4287615 -4.4287481 -4.4287395][-4.4288158 -4.4288316 -4.4288568 -4.4288754 -4.4288735 -4.428863 -4.4288511 -4.4288373 -4.4288325 -4.42883 -4.4288321 -4.4288468 -4.4288573 -4.4288483 -4.4288406][-4.4288759 -4.4288988 -4.428926 -4.428947 -4.428947 -4.4289374 -4.4289279 -4.4289165 -4.4289136 -4.428915 -4.4289174 -4.4289312 -4.4289427 -4.4289389 -4.4289312][-4.4289417 -4.4289656 -4.4289846 -4.4290013 -4.4290009 -4.4289913 -4.4289818 -4.4289742 -4.4289718 -4.4289737 -4.428978 -4.4289913 -4.4290051 -4.429008 -4.4290056][-4.4289813 -4.4290028 -4.4290147 -4.4290266 -4.4290271 -4.429019 -4.4290113 -4.4290056 -4.4290032 -4.4290032 -4.4290051 -4.4290156 -4.42903 -4.4290376 -4.4290428][-4.4289918 -4.4290085 -4.4290156 -4.4290214 -4.4290218 -4.4290166 -4.4290113 -4.429008 -4.4290071 -4.4290066 -4.4290075 -4.4290137 -4.4290261 -4.4290357 -4.4290433]]...]
INFO - root - 2017-12-08 06:48:15.677645: step 30610, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 18h:41m:55s remains)
INFO - root - 2017-12-08 06:48:17.894967: step 30620, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.222 sec/batch; 18h:35m:35s remains)
INFO - root - 2017-12-08 06:48:20.120380: step 30630, loss = 2.28, batch loss = 2.23 (34.7 examples/sec; 0.230 sec/batch; 19h:19m:17s remains)
INFO - root - 2017-12-08 06:48:22.396696: step 30640, loss = 2.28, batch loss = 2.23 (33.2 examples/sec; 0.241 sec/batch; 20h:12m:00s remains)
INFO - root - 2017-12-08 06:48:24.676130: step 30650, loss = 2.28, batch loss = 2.23 (32.6 examples/sec; 0.245 sec/batch; 20h:34m:13s remains)
INFO - root - 2017-12-08 06:48:26.876112: step 30660, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 18h:25m:58s remains)
INFO - root - 2017-12-08 06:48:29.109123: step 30670, loss = 2.28, batch loss = 2.23 (36.7 examples/sec; 0.218 sec/batch; 18h:16m:48s remains)
INFO - root - 2017-12-08 06:48:31.343069: step 30680, loss = 2.28, batch loss = 2.23 (36.7 examples/sec; 0.218 sec/batch; 18h:15m:02s remains)
INFO - root - 2017-12-08 06:48:33.572362: step 30690, loss = 2.28, batch loss = 2.23 (35.8 examples/sec; 0.223 sec/batch; 18h:43m:44s remains)
INFO - root - 2017-12-08 06:48:35.796216: step 30700, loss = 2.28, batch loss = 2.23 (37.2 examples/sec; 0.215 sec/batch; 18h:01m:22s remains)
2017-12-08 06:48:36.100798: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288988 -4.4288993 -4.4289026 -4.4288964 -4.4288807 -4.4288731 -4.4288707 -4.4288721 -4.4288774 -4.4288793 -4.4288797 -4.4288778 -4.4288783 -4.4288874 -4.428895][-4.4288745 -4.4288797 -4.4288754 -4.42885 -4.4288144 -4.4287972 -4.4287925 -4.4288034 -4.4288225 -4.4288363 -4.42885 -4.4288549 -4.4288564 -4.4288645 -4.4288754][-4.4288173 -4.428822 -4.4288049 -4.428761 -4.4287109 -4.4286861 -4.4286804 -4.4287043 -4.4287429 -4.4287772 -4.4288087 -4.4288268 -4.4288292 -4.4288344 -4.4288511][-4.4287162 -4.428721 -4.4287043 -4.4286647 -4.4286213 -4.4285979 -4.4285927 -4.4286256 -4.428679 -4.4287271 -4.4287605 -4.4287796 -4.4287806 -4.4287834 -4.4288073][-4.4286356 -4.4286504 -4.4286509 -4.4286251 -4.4285846 -4.4285421 -4.4285092 -4.4285336 -4.4286008 -4.4286551 -4.4286785 -4.4286904 -4.4286947 -4.4287071 -4.4287496][-4.4286346 -4.4286504 -4.4286542 -4.4286175 -4.4285493 -4.4284587 -4.4283752 -4.4283881 -4.4284844 -4.4285636 -4.4285936 -4.4286084 -4.4286251 -4.4286556 -4.4287124][-4.4286561 -4.428659 -4.4286432 -4.4285879 -4.4284949 -4.4283566 -4.4282188 -4.4282346 -4.4283957 -4.4285245 -4.4285707 -4.4285841 -4.4286075 -4.4286528 -4.4287157][-4.4286537 -4.42864 -4.4286094 -4.4285583 -4.4284816 -4.4283686 -4.428277 -4.4283247 -4.4284873 -4.4286127 -4.4286461 -4.4286427 -4.4286594 -4.4287047 -4.428761][-4.4286251 -4.428616 -4.4286089 -4.4285941 -4.4285622 -4.4285078 -4.4284763 -4.4285278 -4.4286413 -4.4287291 -4.4287386 -4.4287167 -4.4287181 -4.4287515 -4.4287963][-4.4286122 -4.4286346 -4.4286628 -4.4286656 -4.4286442 -4.4286113 -4.4286003 -4.428637 -4.4287128 -4.4287753 -4.4287715 -4.4287376 -4.4287338 -4.4287658 -4.4288116][-4.4286337 -4.42867 -4.4287114 -4.4287233 -4.4287047 -4.4286771 -4.4286628 -4.4286876 -4.4287429 -4.428792 -4.4287872 -4.4287529 -4.4287448 -4.4287791 -4.4288273][-4.4286952 -4.4287214 -4.4287562 -4.428771 -4.4287577 -4.4287314 -4.42872 -4.4287353 -4.42877 -4.4288054 -4.4288087 -4.4287872 -4.4287844 -4.4288158 -4.4288578][-4.4287786 -4.4287949 -4.4288177 -4.4288263 -4.4288192 -4.4288063 -4.4288049 -4.4288144 -4.4288297 -4.4288473 -4.42885 -4.4288392 -4.4288411 -4.4288659 -4.4288955][-4.4288497 -4.4288611 -4.4288797 -4.4288893 -4.4288874 -4.428884 -4.4288893 -4.4288969 -4.4289017 -4.4289074 -4.428906 -4.4288979 -4.4288988 -4.4289117 -4.4289289][-4.4289045 -4.4289103 -4.4289222 -4.4289312 -4.4289346 -4.4289365 -4.4289432 -4.4289503 -4.4289532 -4.4289536 -4.4289503 -4.4289427 -4.42894 -4.4289432 -4.4289527]]...]
INFO - root - 2017-12-08 06:48:38.326676: step 30710, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 18h:40m:16s remains)
INFO - root - 2017-12-08 06:48:40.605987: step 30720, loss = 2.28, batch loss = 2.23 (35.4 examples/sec; 0.226 sec/batch; 18h:56m:56s remains)
INFO - root - 2017-12-08 06:48:42.795069: step 30730, loss = 2.28, batch loss = 2.23 (37.0 examples/sec; 0.216 sec/batch; 18h:06m:12s remains)
INFO - root - 2017-12-08 06:48:45.064338: step 30740, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 18h:21m:54s remains)
INFO - root - 2017-12-08 06:48:47.302910: step 30750, loss = 2.28, batch loss = 2.23 (35.7 examples/sec; 0.224 sec/batch; 18h:46m:07s remains)
INFO - root - 2017-12-08 06:48:49.524187: step 30760, loss = 2.28, batch loss = 2.23 (37.4 examples/sec; 0.214 sec/batch; 17h:54m:48s remains)
INFO - root - 2017-12-08 06:48:51.733190: step 30770, loss = 2.28, batch loss = 2.23 (35.3 examples/sec; 0.227 sec/batch; 18h:59m:02s remains)
INFO - root - 2017-12-08 06:48:53.979262: step 30780, loss = 2.28, batch loss = 2.23 (35.7 examples/sec; 0.224 sec/batch; 18h:48m:02s remains)
INFO - root - 2017-12-08 06:48:56.230677: step 30790, loss = 2.28, batch loss = 2.23 (34.9 examples/sec; 0.229 sec/batch; 19h:13m:38s remains)
INFO - root - 2017-12-08 06:48:58.453291: step 30800, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 18h:25m:43s remains)
2017-12-08 06:48:58.752106: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.42893 -4.4289184 -4.4289083 -4.4289017 -4.4288955 -4.4288864 -4.4288721 -4.4288559 -4.428844 -4.4288416 -4.4288411 -4.4288359 -4.4288363 -4.4288449 -4.42886][-4.4289055 -4.4288898 -4.4288778 -4.4288673 -4.4288521 -4.4288349 -4.4288106 -4.4287839 -4.4287696 -4.4287715 -4.4287796 -4.42878 -4.4287791 -4.4287863 -4.4288077][-4.4288745 -4.4288564 -4.428844 -4.4288239 -4.428791 -4.4287586 -4.4287152 -4.4286752 -4.42866 -4.4286714 -4.4286962 -4.4287086 -4.428709 -4.4287143 -4.4287386][-4.4288511 -4.4288321 -4.4288177 -4.4287868 -4.4287357 -4.4286828 -4.4286222 -4.4285765 -4.4285712 -4.4285979 -4.4286351 -4.4286532 -4.4286447 -4.4286385 -4.4286523][-4.4288359 -4.428812 -4.4287915 -4.4287486 -4.4286838 -4.4286132 -4.4285307 -4.4284682 -4.4284849 -4.4285426 -4.428596 -4.4286184 -4.4286017 -4.4285812 -4.4285817][-4.4288325 -4.428803 -4.4287748 -4.4287248 -4.4286489 -4.4285541 -4.428421 -4.4283085 -4.4283404 -4.4284496 -4.4285398 -4.4285765 -4.4285612 -4.42853 -4.4285221][-4.42884 -4.4288139 -4.4287858 -4.4287305 -4.4286408 -4.4285083 -4.4283075 -4.4281168 -4.4281535 -4.428329 -4.4284673 -4.4285274 -4.4285197 -4.428484 -4.4284658][-4.4288349 -4.4288149 -4.4287882 -4.4287338 -4.4286432 -4.4285092 -4.4283018 -4.4280868 -4.4281163 -4.428319 -4.4284687 -4.4285421 -4.4285407 -4.428504 -4.428472][-4.42881 -4.4287958 -4.4287786 -4.4287305 -4.4286561 -4.4285622 -4.4284272 -4.4282813 -4.4283051 -4.4284544 -4.4285626 -4.4286189 -4.428617 -4.4285803 -4.428544][-4.4287996 -4.4287934 -4.4287815 -4.4287419 -4.4286852 -4.4286275 -4.4285517 -4.4284706 -4.4284949 -4.4285851 -4.4286494 -4.4286814 -4.4286718 -4.4286375 -4.4286156][-4.4288144 -4.4288144 -4.4288058 -4.4287739 -4.4287271 -4.4286809 -4.4286304 -4.4285793 -4.4286013 -4.428659 -4.4286981 -4.4287167 -4.4287062 -4.4286804 -4.4286723][-4.4288435 -4.4288425 -4.4288354 -4.4288082 -4.4287739 -4.4287357 -4.4286962 -4.4286633 -4.4286804 -4.4287238 -4.4287519 -4.4287615 -4.4287505 -4.4287233 -4.4287195][-4.4288754 -4.428875 -4.42887 -4.4288435 -4.4288111 -4.4287786 -4.4287486 -4.4287281 -4.4287438 -4.4287839 -4.4288116 -4.4288259 -4.4288158 -4.4287891 -4.428781][-4.42889 -4.4288907 -4.4288945 -4.4288769 -4.4288535 -4.4288359 -4.4288249 -4.4288235 -4.4288373 -4.4288692 -4.428894 -4.4289041 -4.4288936 -4.4288697 -4.4288568][-4.4289045 -4.4289079 -4.4289169 -4.4289107 -4.4289 -4.428896 -4.4288988 -4.4289012 -4.4289079 -4.4289246 -4.4289389 -4.428946 -4.4289379 -4.4289227 -4.4289136]]...]
INFO - root - 2017-12-08 06:49:00.949857: step 30810, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.220 sec/batch; 18h:28m:00s remains)
INFO - root - 2017-12-08 06:49:03.164091: step 30820, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.220 sec/batch; 18h:28m:11s remains)
INFO - root - 2017-12-08 06:49:05.410995: step 30830, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 18h:25m:45s remains)
INFO - root - 2017-12-08 06:49:07.675885: step 30840, loss = 2.28, batch loss = 2.23 (34.0 examples/sec; 0.235 sec/batch; 19h:42m:46s remains)
INFO - root - 2017-12-08 06:49:09.922548: step 30850, loss = 2.28, batch loss = 2.23 (34.9 examples/sec; 0.229 sec/batch; 19h:13m:31s remains)
INFO - root - 2017-12-08 06:49:12.150938: step 30860, loss = 2.28, batch loss = 2.23 (34.6 examples/sec; 0.231 sec/batch; 19h:22m:22s remains)
INFO - root - 2017-12-08 06:49:14.402240: step 30870, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 18h:31m:08s remains)
INFO - root - 2017-12-08 06:49:16.647859: step 30880, loss = 2.28, batch loss = 2.23 (34.8 examples/sec; 0.230 sec/batch; 19h:13m:58s remains)
INFO - root - 2017-12-08 06:49:18.900505: step 30890, loss = 2.28, batch loss = 2.23 (35.7 examples/sec; 0.224 sec/batch; 18h:47m:21s remains)
INFO - root - 2017-12-08 06:49:21.134544: step 30900, loss = 2.28, batch loss = 2.23 (34.9 examples/sec; 0.229 sec/batch; 19h:13m:33s remains)
2017-12-08 06:49:21.453826: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288664 -4.4288716 -4.4288778 -4.4288511 -4.4288015 -4.4287314 -4.4286766 -4.4286675 -4.4286866 -4.4286842 -4.4286394 -4.4285679 -4.4284763 -4.4283853 -4.4283686][-4.428894 -4.4289069 -4.4289203 -4.4288979 -4.4288483 -4.4287786 -4.4287148 -4.4286809 -4.4286823 -4.4286766 -4.4286251 -4.4285374 -4.4284477 -4.4283671 -4.4283471][-4.4288859 -4.4288979 -4.4289117 -4.4288964 -4.4288588 -4.4287958 -4.4287362 -4.4287 -4.4286904 -4.4286871 -4.428647 -4.4285736 -4.4285083 -4.4284444 -4.4284191][-4.4288664 -4.4288712 -4.4288745 -4.4288607 -4.4288292 -4.4287634 -4.4287004 -4.42867 -4.4286628 -4.4286742 -4.4286647 -4.4286313 -4.4285889 -4.4285355 -4.4284978][-4.4288368 -4.4288287 -4.428812 -4.428793 -4.4287553 -4.4286857 -4.4286246 -4.4286008 -4.4286046 -4.4286385 -4.4286633 -4.4286685 -4.4286537 -4.4286051 -4.4285488][-4.4287357 -4.4287128 -4.4286723 -4.4286418 -4.4285917 -4.4285221 -4.428472 -4.4284682 -4.4284987 -4.4285669 -4.4286327 -4.4286671 -4.4286618 -4.4286089 -4.4285388][-4.4285631 -4.4285316 -4.4284768 -4.4284277 -4.4283643 -4.4282951 -4.4282446 -4.4282489 -4.4283028 -4.428412 -4.4285116 -4.4285607 -4.4285622 -4.4285097 -4.4284329][-4.4283977 -4.42836 -4.4283018 -4.4282417 -4.4281793 -4.4281273 -4.4280844 -4.4280853 -4.4281397 -4.4282565 -4.4283576 -4.4283977 -4.4284 -4.4283566 -4.4282813][-4.4283905 -4.4283547 -4.428298 -4.428236 -4.428184 -4.4281521 -4.4281235 -4.4281168 -4.4281483 -4.4282312 -4.4283056 -4.428329 -4.428329 -4.4282918 -4.4282331][-4.4285135 -4.4284925 -4.4284582 -4.4284134 -4.4283752 -4.42835 -4.4283319 -4.4283257 -4.4283328 -4.4283714 -4.4284105 -4.4284239 -4.4284163 -4.4283838 -4.4283481][-4.4286718 -4.4286647 -4.4286513 -4.4286284 -4.4286022 -4.4285812 -4.4285731 -4.4285765 -4.4285774 -4.4285917 -4.4286041 -4.4286065 -4.4285855 -4.428556 -4.4285374][-4.4288111 -4.42881 -4.4288111 -4.428802 -4.4287868 -4.4287724 -4.428771 -4.4287734 -4.4287691 -4.4287734 -4.4287763 -4.4287729 -4.4287515 -4.4287271 -4.4287171][-4.4289203 -4.4289212 -4.4289269 -4.4289255 -4.4289174 -4.4289117 -4.4289122 -4.4289112 -4.4289064 -4.4289064 -4.42891 -4.4289093 -4.428895 -4.4288778 -4.4288688][-4.4289947 -4.4289961 -4.4290004 -4.429 -4.4289937 -4.4289918 -4.4289927 -4.4289927 -4.4289918 -4.4289927 -4.4289947 -4.4289947 -4.428988 -4.4289804 -4.4289761][-4.4290247 -4.4290261 -4.429028 -4.4290285 -4.4290257 -4.4290237 -4.4290237 -4.4290237 -4.4290237 -4.4290237 -4.4290237 -4.4290218 -4.42902 -4.4290185 -4.429018]]...]
INFO - root - 2017-12-08 06:49:23.653097: step 30910, loss = 2.28, batch loss = 2.23 (35.7 examples/sec; 0.224 sec/batch; 18h:45m:15s remains)
INFO - root - 2017-12-08 06:49:25.903810: step 30920, loss = 2.28, batch loss = 2.23 (35.0 examples/sec; 0.229 sec/batch; 19h:10m:06s remains)
INFO - root - 2017-12-08 06:49:28.122028: step 30930, loss = 2.28, batch loss = 2.23 (35.4 examples/sec; 0.226 sec/batch; 18h:56m:55s remains)
INFO - root - 2017-12-08 06:49:30.344753: step 30940, loss = 2.28, batch loss = 2.23 (36.8 examples/sec; 0.217 sec/batch; 18h:11m:50s remains)
INFO - root - 2017-12-08 06:49:32.583559: step 30950, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 18h:21m:56s remains)
INFO - root - 2017-12-08 06:49:34.815490: step 30960, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 18h:30m:12s remains)
INFO - root - 2017-12-08 06:49:37.036055: step 30970, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 18h:22m:00s remains)
INFO - root - 2017-12-08 06:49:39.275708: step 30980, loss = 2.28, batch loss = 2.23 (35.7 examples/sec; 0.224 sec/batch; 18h:44m:35s remains)
INFO - root - 2017-12-08 06:49:41.500130: step 30990, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 18h:39m:58s remains)
INFO - root - 2017-12-08 06:49:43.733105: step 31000, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.225 sec/batch; 18h:51m:10s remains)
2017-12-08 06:49:44.074956: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289546 -4.4289608 -4.4289613 -4.4289584 -4.4289556 -4.4289532 -4.4289532 -4.4289541 -4.4289532 -4.4289508 -4.4289474 -4.4289451 -4.4289436 -4.4289427 -4.4289427][-4.4289713 -4.4289761 -4.4289775 -4.4289732 -4.4289675 -4.4289618 -4.4289608 -4.42896 -4.4289532 -4.4289436 -4.4289336 -4.4289265 -4.428925 -4.4289274 -4.4289289][-4.4289536 -4.4289594 -4.4289622 -4.4289517 -4.42894 -4.4289274 -4.4289222 -4.4289165 -4.4289055 -4.42889 -4.4288764 -4.4288683 -4.4288731 -4.4288855 -4.4288969][-4.4289069 -4.4289241 -4.4289246 -4.4289007 -4.4288716 -4.4288416 -4.4288197 -4.4288077 -4.4287992 -4.4287877 -4.4287782 -4.4287767 -4.4287958 -4.4288273 -4.4288549][-4.4288321 -4.4288721 -4.4288754 -4.4288349 -4.4287767 -4.4287114 -4.4286523 -4.4286251 -4.4286304 -4.4286361 -4.4286408 -4.4286609 -4.4287062 -4.4287658 -4.4288144][-4.4287343 -4.4288063 -4.4288216 -4.4287705 -4.428679 -4.4285574 -4.4284372 -4.4283948 -4.428431 -4.4284692 -4.4285045 -4.4285626 -4.4286437 -4.4287276 -4.42879][-4.428616 -4.4287257 -4.4287615 -4.4287114 -4.42859 -4.4284039 -4.4282117 -4.4281578 -4.428246 -4.4283414 -4.4284315 -4.4285398 -4.4286485 -4.4287381 -4.4287949][-4.4285417 -4.4286819 -4.4287314 -4.4286852 -4.4285512 -4.4283319 -4.4281116 -4.4280691 -4.4281974 -4.4283414 -4.4284754 -4.4286094 -4.428719 -4.4287915 -4.4288268][-4.4285703 -4.4287019 -4.4287496 -4.4287128 -4.4285913 -4.4283991 -4.4282274 -4.4282093 -4.4283252 -4.4284611 -4.4285889 -4.4287119 -4.4288 -4.4288459 -4.4288583][-4.4286571 -4.4287643 -4.4288054 -4.4287825 -4.4286852 -4.4285436 -4.4284425 -4.42844 -4.4285159 -4.4286118 -4.4287052 -4.4287944 -4.4288516 -4.4288745 -4.4288745][-4.4287477 -4.4288321 -4.4288678 -4.4288568 -4.4287887 -4.4286947 -4.42864 -4.4286385 -4.4286761 -4.4287329 -4.4287925 -4.4288454 -4.4288788 -4.4288888 -4.428885][-4.428822 -4.4288793 -4.4289021 -4.4288936 -4.4288516 -4.4287982 -4.4287724 -4.4287705 -4.4287844 -4.4288135 -4.4288468 -4.4288754 -4.4288926 -4.4288969 -4.4288921][-4.4288716 -4.428906 -4.428916 -4.4289069 -4.4288831 -4.4288578 -4.4288487 -4.4288483 -4.4288526 -4.4288654 -4.4288812 -4.428895 -4.4289026 -4.4289026 -4.4288979][-4.4289 -4.4289165 -4.4289169 -4.42891 -4.4288988 -4.4288893 -4.4288869 -4.4288855 -4.4288845 -4.4288874 -4.4288931 -4.4289 -4.4289031 -4.4289017 -4.428896][-4.42891 -4.428916 -4.4289112 -4.428906 -4.4289012 -4.4288983 -4.4288974 -4.4288955 -4.4288926 -4.4288912 -4.4288926 -4.428896 -4.4288974 -4.4288945 -4.4288888]]...]
INFO - root - 2017-12-08 06:49:46.310049: step 31010, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.225 sec/batch; 18h:50m:57s remains)
INFO - root - 2017-12-08 06:49:48.547092: step 31020, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.225 sec/batch; 18h:52m:25s remains)
INFO - root - 2017-12-08 06:49:50.769019: step 31030, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 18h:31m:17s remains)
INFO - root - 2017-12-08 06:49:53.005323: step 31040, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 18h:21m:32s remains)
INFO - root - 2017-12-08 06:49:55.253375: step 31050, loss = 2.28, batch loss = 2.23 (36.7 examples/sec; 0.218 sec/batch; 18h:16m:10s remains)
INFO - root - 2017-12-08 06:49:57.507870: step 31060, loss = 2.28, batch loss = 2.23 (35.2 examples/sec; 0.227 sec/batch; 19h:00m:13s remains)
INFO - root - 2017-12-08 06:49:59.763108: step 31070, loss = 2.28, batch loss = 2.23 (34.9 examples/sec; 0.229 sec/batch; 19h:12m:41s remains)
INFO - root - 2017-12-08 06:50:02.005037: step 31080, loss = 2.28, batch loss = 2.23 (36.0 examples/sec; 0.222 sec/batch; 18h:35m:56s remains)
INFO - root - 2017-12-08 06:50:04.235309: step 31090, loss = 2.28, batch loss = 2.23 (37.7 examples/sec; 0.212 sec/batch; 17h:46m:12s remains)
INFO - root - 2017-12-08 06:50:06.464233: step 31100, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 18h:21m:53s remains)
2017-12-08 06:50:06.750033: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4287477 -4.4287 -4.42869 -4.4286723 -4.428637 -4.4286451 -4.4286933 -4.4287395 -4.428781 -4.42882 -4.4288507 -4.428854 -4.4288268 -4.4287672 -4.4287224][-4.4287305 -4.4286733 -4.4286618 -4.4286556 -4.4286447 -4.4286728 -4.4287338 -4.4287777 -4.4287992 -4.4288254 -4.4288626 -4.4288821 -4.4288607 -4.4288039 -4.4287591][-4.4287434 -4.4286847 -4.4286757 -4.4286761 -4.4286833 -4.4287214 -4.42878 -4.4288116 -4.428822 -4.4288363 -4.4288697 -4.4288964 -4.4288788 -4.4288244 -4.4287796][-4.428772 -4.4287214 -4.428721 -4.4287248 -4.4287357 -4.4287605 -4.4287958 -4.428813 -4.4288244 -4.4288411 -4.4288774 -4.4289079 -4.4288888 -4.4288278 -4.42878][-4.4287796 -4.4287276 -4.4287243 -4.4287248 -4.4287176 -4.4287057 -4.4287004 -4.4287033 -4.4287262 -4.4287739 -4.428843 -4.4288955 -4.4288783 -4.4288192 -4.4287777][-4.4287705 -4.428719 -4.4287057 -4.428688 -4.4286442 -4.4285765 -4.4285192 -4.4284992 -4.4285474 -4.4286547 -4.4287844 -4.4288712 -4.4288707 -4.4288235 -4.4287887][-4.4287643 -4.4287086 -4.4286833 -4.4286432 -4.4285588 -4.428431 -4.4283128 -4.42827 -4.4283586 -4.4285407 -4.4287338 -4.4288487 -4.4288678 -4.4288273 -4.4287891][-4.4287529 -4.4286933 -4.4286704 -4.4286242 -4.4285154 -4.4283543 -4.4282026 -4.4281592 -4.4282818 -4.4285073 -4.4287262 -4.4288473 -4.4288726 -4.4288359 -4.4287944][-4.4287496 -4.4286923 -4.4286785 -4.428647 -4.4285512 -4.4284043 -4.4282708 -4.4282427 -4.4283667 -4.4285812 -4.4287777 -4.4288731 -4.428885 -4.4288473 -4.4288006][-4.4287581 -4.4287181 -4.4287181 -4.4287095 -4.4286404 -4.4285345 -4.4284577 -4.4284511 -4.4285517 -4.4287186 -4.4288568 -4.4289064 -4.428895 -4.4288492 -4.4287992][-4.4287753 -4.4287505 -4.4287572 -4.42877 -4.4287391 -4.4286876 -4.4286613 -4.4286714 -4.4287448 -4.4288516 -4.428926 -4.4289274 -4.4288931 -4.4288459 -4.428802][-4.4287839 -4.4287529 -4.4287548 -4.4287834 -4.42879 -4.4287839 -4.4287953 -4.4288154 -4.4288678 -4.4289274 -4.42895 -4.4289193 -4.4288764 -4.4288363 -4.4288092][-4.428812 -4.4287691 -4.4287596 -4.4287953 -4.4288263 -4.4288492 -4.4288764 -4.4289017 -4.428936 -4.4289565 -4.4289436 -4.428905 -4.4288635 -4.4288316 -4.4288058][-4.4288239 -4.4287868 -4.4287829 -4.4288206 -4.4288573 -4.428885 -4.4289131 -4.4289351 -4.4289517 -4.4289494 -4.4289222 -4.4288878 -4.4288554 -4.428823 -4.4287882][-4.428822 -4.4288015 -4.4288197 -4.4288597 -4.4288912 -4.4289122 -4.4289322 -4.428947 -4.4289517 -4.4289389 -4.4289145 -4.4288898 -4.428864 -4.4288268 -4.4287839]]...]
INFO - root - 2017-12-08 06:50:09.007774: step 31110, loss = 2.28, batch loss = 2.23 (36.8 examples/sec; 0.217 sec/batch; 18h:11m:47s remains)
INFO - root - 2017-12-08 06:50:11.232881: step 31120, loss = 2.28, batch loss = 2.23 (36.7 examples/sec; 0.218 sec/batch; 18h:14m:54s remains)
INFO - root - 2017-12-08 06:50:13.461445: step 31130, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 18h:38m:17s remains)
INFO - root - 2017-12-08 06:50:15.726598: step 31140, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 18h:39m:58s remains)
INFO - root - 2017-12-08 06:50:17.954450: step 31150, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 18h:38m:58s remains)
INFO - root - 2017-12-08 06:50:20.206726: step 31160, loss = 2.28, batch loss = 2.23 (34.9 examples/sec; 0.229 sec/batch; 19h:10m:57s remains)
INFO - root - 2017-12-08 06:50:22.439265: step 31170, loss = 2.28, batch loss = 2.23 (34.6 examples/sec; 0.231 sec/batch; 19h:22m:14s remains)
INFO - root - 2017-12-08 06:50:24.650560: step 31180, loss = 2.28, batch loss = 2.23 (37.0 examples/sec; 0.216 sec/batch; 18h:06m:19s remains)
INFO - root - 2017-12-08 06:50:26.887077: step 31190, loss = 2.28, batch loss = 2.23 (36.8 examples/sec; 0.217 sec/batch; 18h:10m:18s remains)
INFO - root - 2017-12-08 06:50:29.104442: step 31200, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.220 sec/batch; 18h:25m:34s remains)
2017-12-08 06:50:29.401032: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4287133 -4.4286766 -4.4286561 -4.4286528 -4.4286566 -4.428659 -4.4286833 -4.4287152 -4.4287524 -4.42878 -4.4288106 -4.4288397 -4.428844 -4.4288287 -4.4287977][-4.4287114 -4.4286771 -4.4286633 -4.4286513 -4.4286356 -4.4286156 -4.42862 -4.4286456 -4.4286885 -4.4287372 -4.4287853 -4.4288211 -4.4288135 -4.4287815 -4.4287338][-4.4287243 -4.4287052 -4.4287038 -4.4286823 -4.4286442 -4.4285941 -4.4285703 -4.4285803 -4.4286275 -4.4286971 -4.4287524 -4.4287844 -4.4287639 -4.4287138 -4.4286609][-4.4287229 -4.428719 -4.4287286 -4.4287086 -4.428658 -4.4285803 -4.4285207 -4.4285049 -4.42855 -4.428637 -4.4286909 -4.428719 -4.4286957 -4.4286494 -4.4286122][-4.4287057 -4.4287148 -4.4287319 -4.4287109 -4.42865 -4.4285336 -4.4284105 -4.4283352 -4.428381 -4.4285097 -4.4285851 -4.4286237 -4.4286151 -4.4285984 -4.4285913][-4.4286942 -4.4287105 -4.4287171 -4.4286714 -4.4285727 -4.4283934 -4.4281659 -4.4279943 -4.4280505 -4.4282732 -4.4284258 -4.4284997 -4.4285169 -4.4285398 -4.4285645][-4.4287081 -4.4287224 -4.4287019 -4.4286108 -4.4284525 -4.4281983 -4.42787 -4.4276023 -4.4277072 -4.4280686 -4.4283218 -4.428432 -4.42848 -4.4285307 -4.4285722][-4.4287443 -4.4287467 -4.4287133 -4.4285975 -4.4284081 -4.4281421 -4.4278312 -4.427609 -4.4277668 -4.4281397 -4.4283915 -4.428493 -4.4285455 -4.4285989 -4.42864][-4.4287996 -4.4287934 -4.4287648 -4.4286637 -4.4285016 -4.4282918 -4.4280906 -4.4279866 -4.4281244 -4.4283772 -4.4285507 -4.42862 -4.4286551 -4.4286928 -4.4287224][-4.4288526 -4.4288435 -4.4288321 -4.4287744 -4.4286776 -4.4285469 -4.4284296 -4.428391 -4.4284706 -4.4286036 -4.4287019 -4.4287329 -4.4287472 -4.4287663 -4.4287858][-4.4288926 -4.4288836 -4.428894 -4.4288812 -4.4288416 -4.428771 -4.4287047 -4.42868 -4.4287066 -4.4287581 -4.4287996 -4.4288092 -4.4288092 -4.4288063 -4.428812][-4.428896 -4.428896 -4.4289236 -4.428937 -4.4289274 -4.428894 -4.42886 -4.4288411 -4.4288392 -4.4288492 -4.4288592 -4.428854 -4.4288363 -4.4288111 -4.4287992][-4.428853 -4.4288726 -4.4289141 -4.4289446 -4.428956 -4.4289474 -4.4289355 -4.4289203 -4.428906 -4.4288931 -4.4288836 -4.42887 -4.4288368 -4.428792 -4.4287643][-4.428761 -4.428802 -4.4288592 -4.4289083 -4.4289379 -4.4289489 -4.4289484 -4.428925 -4.4288964 -4.4288678 -4.4288454 -4.4288244 -4.4287848 -4.4287319 -4.4286942][-4.4286447 -4.4286952 -4.4287691 -4.4288344 -4.4288797 -4.4289026 -4.4288993 -4.428863 -4.4288206 -4.4287834 -4.4287515 -4.4287305 -4.4287004 -4.4286571 -4.4286289]]...]
INFO - root - 2017-12-08 06:50:31.626357: step 31210, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 18h:23m:15s remains)
INFO - root - 2017-12-08 06:50:33.870783: step 31220, loss = 2.28, batch loss = 2.23 (35.1 examples/sec; 0.228 sec/batch; 19h:04m:26s remains)
INFO - root - 2017-12-08 06:50:36.118501: step 31230, loss = 2.28, batch loss = 2.23 (35.6 examples/sec; 0.225 sec/batch; 18h:49m:23s remains)
INFO - root - 2017-12-08 06:50:38.353855: step 31240, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.220 sec/batch; 18h:25m:59s remains)
INFO - root - 2017-12-08 06:50:40.564366: step 31250, loss = 2.28, batch loss = 2.23 (34.7 examples/sec; 0.231 sec/batch; 19h:17m:48s remains)
INFO - root - 2017-12-08 06:50:42.805760: step 31260, loss = 2.28, batch loss = 2.23 (35.0 examples/sec; 0.229 sec/batch; 19h:07m:59s remains)
INFO - root - 2017-12-08 06:50:45.066003: step 31270, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 18h:38m:19s remains)
INFO - root - 2017-12-08 06:50:47.315061: step 31280, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 18h:29m:46s remains)
INFO - root - 2017-12-08 06:50:49.536822: step 31290, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 18h:30m:43s remains)
INFO - root - 2017-12-08 06:50:51.801166: step 31300, loss = 2.28, batch loss = 2.23 (36.0 examples/sec; 0.222 sec/batch; 18h:34m:55s remains)
2017-12-08 06:50:52.088307: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288754 -4.428812 -4.4287405 -4.4286571 -4.4285851 -4.4286056 -4.42866 -4.4286728 -4.4286737 -4.4286766 -4.4286861 -4.4286957 -4.4287286 -4.4287672 -4.4287853][-4.4288964 -4.4288554 -4.4288144 -4.4287548 -4.428699 -4.4287038 -4.4287291 -4.4287353 -4.4287291 -4.4287257 -4.4287324 -4.4287262 -4.4287381 -4.42876 -4.4287672][-4.4289031 -4.4288793 -4.4288635 -4.4288268 -4.4287863 -4.4287744 -4.428772 -4.4287677 -4.4287553 -4.4287491 -4.4287605 -4.4287577 -4.4287624 -4.4287639 -4.4287472][-4.4288912 -4.42886 -4.428844 -4.4288158 -4.4287868 -4.4287739 -4.4287686 -4.4287653 -4.4287505 -4.4287405 -4.4287553 -4.4287658 -4.4287744 -4.4287724 -4.4287505][-4.4288754 -4.4288349 -4.4288049 -4.4287691 -4.4287324 -4.4287233 -4.4287186 -4.4287133 -4.4287052 -4.4287028 -4.4287243 -4.4287438 -4.42875 -4.4287567 -4.4287438][-4.4288759 -4.4288311 -4.4287949 -4.4287519 -4.4286947 -4.4286742 -4.4286623 -4.4286475 -4.4286366 -4.4286423 -4.4286785 -4.4287095 -4.428721 -4.42874 -4.4287386][-4.4288673 -4.4288154 -4.4287763 -4.4287386 -4.4286776 -4.4286523 -4.4286366 -4.4286132 -4.4285913 -4.428597 -4.4286413 -4.42868 -4.4287052 -4.428741 -4.4287491][-4.428854 -4.4287968 -4.42876 -4.4287357 -4.4286833 -4.4286623 -4.4286504 -4.4286246 -4.4285955 -4.4285984 -4.4286413 -4.4286871 -4.4287238 -4.4287686 -4.4287825][-4.4288626 -4.4288082 -4.4287696 -4.4287486 -4.4287009 -4.4286866 -4.4286852 -4.4286675 -4.4286418 -4.4286418 -4.4286714 -4.4287128 -4.4287572 -4.4288011 -4.4288163][-4.4288807 -4.4288406 -4.4288154 -4.4287949 -4.4287496 -4.4287353 -4.4287448 -4.4287419 -4.4287281 -4.4287295 -4.4287472 -4.4287777 -4.4288116 -4.4288416 -4.4288526][-4.4289012 -4.4288831 -4.4288807 -4.4288793 -4.4288468 -4.428834 -4.428843 -4.4288487 -4.4288497 -4.4288578 -4.428865 -4.428874 -4.428884 -4.4288921 -4.428895][-4.4289174 -4.4289174 -4.4289308 -4.4289412 -4.4289236 -4.4289165 -4.4289227 -4.4289289 -4.4289355 -4.4289451 -4.428947 -4.4289422 -4.4289374 -4.4289331 -4.4289312][-4.4289217 -4.4289312 -4.4289494 -4.4289627 -4.4289589 -4.4289575 -4.4289627 -4.4289665 -4.42897 -4.4289756 -4.4289756 -4.4289708 -4.4289627 -4.4289541 -4.4289479][-4.4289241 -4.4289322 -4.428946 -4.428956 -4.4289584 -4.4289608 -4.4289641 -4.4289641 -4.4289646 -4.428966 -4.4289651 -4.4289632 -4.4289579 -4.4289517 -4.4289455][-4.4289327 -4.4289336 -4.4289389 -4.4289427 -4.4289446 -4.428947 -4.4289489 -4.4289484 -4.4289474 -4.428947 -4.4289465 -4.428946 -4.4289446 -4.4289432 -4.42894]]...]
INFO - root - 2017-12-08 06:50:54.315194: step 31310, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 18h:18m:45s remains)
INFO - root - 2017-12-08 06:50:56.554559: step 31320, loss = 2.28, batch loss = 2.23 (35.6 examples/sec; 0.225 sec/batch; 18h:48m:38s remains)
INFO - root - 2017-12-08 06:50:58.853216: step 31330, loss = 2.28, batch loss = 2.23 (34.3 examples/sec; 0.234 sec/batch; 19h:32m:26s remains)
INFO - root - 2017-12-08 06:51:01.081709: step 31340, loss = 2.28, batch loss = 2.23 (37.0 examples/sec; 0.216 sec/batch; 18h:04m:16s remains)
INFO - root - 2017-12-08 06:51:03.329422: step 31350, loss = 2.28, batch loss = 2.23 (35.6 examples/sec; 0.225 sec/batch; 18h:47m:48s remains)
INFO - root - 2017-12-08 06:51:05.562920: step 31360, loss = 2.28, batch loss = 2.23 (35.7 examples/sec; 0.224 sec/batch; 18h:45m:57s remains)
INFO - root - 2017-12-08 06:51:07.793969: step 31370, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 18h:30m:15s remains)
INFO - root - 2017-12-08 06:51:10.022975: step 31380, loss = 2.28, batch loss = 2.23 (36.0 examples/sec; 0.222 sec/batch; 18h:36m:37s remains)
INFO - root - 2017-12-08 06:51:12.256091: step 31390, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 18h:28m:32s remains)
INFO - root - 2017-12-08 06:51:14.512264: step 31400, loss = 2.28, batch loss = 2.23 (36.7 examples/sec; 0.218 sec/batch; 18h:14m:58s remains)
2017-12-08 06:51:14.803813: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289465 -4.4289403 -4.4289446 -4.4289503 -4.4289408 -4.4289193 -4.4288912 -4.4288716 -4.4288659 -4.4288683 -4.4288836 -4.4289141 -4.4289427 -4.4289503 -4.4289093][-4.4289522 -4.428947 -4.4289484 -4.428947 -4.4289336 -4.4289103 -4.4288797 -4.4288578 -4.4288511 -4.4288583 -4.4288816 -4.4289193 -4.4289513 -4.4289536 -4.42891][-4.4289703 -4.428967 -4.4289627 -4.4289484 -4.42892 -4.4288855 -4.4288511 -4.4288273 -4.4288235 -4.428833 -4.4288588 -4.4288983 -4.4289284 -4.4289269 -4.4288912][-4.4289913 -4.4289885 -4.42898 -4.4289513 -4.4289079 -4.4288673 -4.4288325 -4.42881 -4.4288096 -4.428822 -4.4288478 -4.4288869 -4.42891 -4.428895 -4.4288549][-4.4289989 -4.4289894 -4.4289727 -4.4289308 -4.4288764 -4.4288316 -4.4287958 -4.4287734 -4.4287729 -4.4287891 -4.428813 -4.428853 -4.4288659 -4.4288373 -4.4287963][-4.4289913 -4.4289656 -4.4289303 -4.4288759 -4.4288173 -4.4287734 -4.428731 -4.428699 -4.4286985 -4.4287252 -4.4287539 -4.4287963 -4.428802 -4.4287653 -4.4287343][-4.428978 -4.4289336 -4.4288797 -4.4288096 -4.4287338 -4.42868 -4.4286242 -4.42858 -4.4285884 -4.4286461 -4.4286962 -4.4287529 -4.4287686 -4.4287343 -4.4287114][-4.4289527 -4.4288964 -4.4288321 -4.4287472 -4.4286447 -4.4285703 -4.4285 -4.4284511 -4.4284763 -4.4285784 -4.4286623 -4.4287281 -4.4287496 -4.428721 -4.4286938][-4.4289289 -4.4288688 -4.4288039 -4.428719 -4.4286127 -4.4285221 -4.4284492 -4.4284034 -4.4284339 -4.4285493 -4.4286423 -4.4286985 -4.42871 -4.4286761 -4.428638][-4.4289117 -4.4288487 -4.428791 -4.4287281 -4.4286485 -4.4285626 -4.4284987 -4.4284687 -4.4284945 -4.4285822 -4.4286447 -4.4286728 -4.4286566 -4.4286084 -4.4285655][-4.4288926 -4.4288239 -4.4287767 -4.4287391 -4.4286909 -4.4286227 -4.4285765 -4.428556 -4.4285722 -4.4286222 -4.4286466 -4.4286413 -4.4286027 -4.4285412 -4.4284935][-4.4288754 -4.4288049 -4.4287639 -4.42874 -4.4287105 -4.4286656 -4.4286313 -4.4286056 -4.4286036 -4.4286208 -4.4286184 -4.428597 -4.4285493 -4.428484 -4.4284377][-4.4288731 -4.4288073 -4.4287744 -4.4287605 -4.4287486 -4.4287233 -4.4286947 -4.4286561 -4.428637 -4.4286284 -4.4286032 -4.4285736 -4.4285288 -4.4284716 -4.4284272][-4.4288859 -4.4288316 -4.4288125 -4.4288135 -4.428812 -4.4287944 -4.4287653 -4.4287219 -4.4286971 -4.428679 -4.4286456 -4.4286127 -4.4285731 -4.428535 -4.4285016][-4.4289074 -4.428864 -4.4288535 -4.428863 -4.4288645 -4.428843 -4.4288149 -4.4287777 -4.4287519 -4.4287348 -4.4287105 -4.428688 -4.4286675 -4.4286542 -4.4286404]]...]
INFO - root - 2017-12-08 06:51:17.035190: step 31410, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 18h:37m:55s remains)
INFO - root - 2017-12-08 06:51:19.260728: step 31420, loss = 2.28, batch loss = 2.23 (34.8 examples/sec; 0.230 sec/batch; 19h:13m:21s remains)
INFO - root - 2017-12-08 06:51:21.496984: step 31430, loss = 2.28, batch loss = 2.23 (35.0 examples/sec; 0.229 sec/batch; 19h:08m:15s remains)
INFO - root - 2017-12-08 06:51:23.733321: step 31440, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.225 sec/batch; 18h:49m:57s remains)
INFO - root - 2017-12-08 06:51:25.974604: step 31450, loss = 2.28, batch loss = 2.23 (34.9 examples/sec; 0.229 sec/batch; 19h:10m:33s remains)
INFO - root - 2017-12-08 06:51:28.227302: step 31460, loss = 2.28, batch loss = 2.23 (35.3 examples/sec; 0.226 sec/batch; 18h:55m:51s remains)
INFO - root - 2017-12-08 06:51:30.455094: step 31470, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.221 sec/batch; 18h:26m:46s remains)
INFO - root - 2017-12-08 06:51:32.712122: step 31480, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.221 sec/batch; 18h:26m:20s remains)
INFO - root - 2017-12-08 06:51:34.936880: step 31490, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.220 sec/batch; 18h:24m:52s remains)
INFO - root - 2017-12-08 06:51:37.190148: step 31500, loss = 2.28, batch loss = 2.23 (35.3 examples/sec; 0.227 sec/batch; 18h:58m:20s remains)
2017-12-08 06:51:37.517829: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4287438 -4.4287858 -4.4288216 -4.4288311 -4.4288154 -4.428781 -4.4287372 -4.4286985 -4.4286904 -4.4287038 -4.4287496 -4.4288139 -4.4288549 -4.4288759 -4.4288754][-4.4288125 -4.4288425 -4.428865 -4.4288611 -4.428822 -4.4287577 -4.4286871 -4.4286261 -4.428628 -4.4286685 -4.4287477 -4.4288445 -4.4289045 -4.4289355 -4.4289403][-4.4288826 -4.4289 -4.4289079 -4.4288883 -4.4288216 -4.4287248 -4.4286189 -4.4285231 -4.428514 -4.4285793 -4.4287024 -4.42884 -4.4289222 -4.4289641 -4.4289761][-4.4289432 -4.4289494 -4.4289408 -4.428905 -4.4288144 -4.4286866 -4.4285469 -4.4284105 -4.4283786 -4.4284573 -4.4286194 -4.428793 -4.4288993 -4.428957 -4.428978][-4.4289894 -4.4289842 -4.4289608 -4.428915 -4.42881 -4.428659 -4.4284883 -4.4283118 -4.4282537 -4.4283338 -4.4285192 -4.4287167 -4.4288487 -4.4289217 -4.4289489][-4.4289989 -4.4289851 -4.4289556 -4.4289079 -4.4287934 -4.4286251 -4.4284291 -4.4282222 -4.4281363 -4.4282193 -4.4284282 -4.42865 -4.4288015 -4.4288874 -4.4289179][-4.4289846 -4.4289665 -4.4289351 -4.4288907 -4.4287777 -4.428606 -4.4284015 -4.4281878 -4.4280882 -4.4281678 -4.4283862 -4.4286237 -4.4287887 -4.4288797 -4.4289122][-4.4289641 -4.428947 -4.4289136 -4.4288678 -4.4287629 -4.4286065 -4.4284172 -4.4282279 -4.4281383 -4.4282055 -4.4284062 -4.4286342 -4.4288006 -4.4288907 -4.4289231][-4.4289432 -4.4289293 -4.4288964 -4.4288492 -4.4287577 -4.428627 -4.4284639 -4.4283104 -4.4282489 -4.4283128 -4.4284825 -4.4286804 -4.4288321 -4.4289117 -4.4289389][-4.4289231 -4.4289165 -4.4288907 -4.4288449 -4.4287658 -4.4286585 -4.4285269 -4.4284153 -4.4283857 -4.4284487 -4.4285908 -4.4287486 -4.4288731 -4.428936 -4.4289565][-4.428915 -4.4289188 -4.4289012 -4.4288592 -4.4287896 -4.4287052 -4.4286103 -4.428545 -4.428545 -4.428607 -4.4287181 -4.4288363 -4.4289279 -4.4289689 -4.428978][-4.4289227 -4.4289336 -4.428925 -4.428895 -4.4288435 -4.4287848 -4.4287219 -4.4286914 -4.4287057 -4.4287562 -4.4288349 -4.4289165 -4.428977 -4.428998 -4.4289956][-4.4289389 -4.4289536 -4.4289536 -4.4289374 -4.4289079 -4.4288754 -4.4288387 -4.428823 -4.4288383 -4.4288754 -4.4289255 -4.4289756 -4.4290113 -4.4290161 -4.4290071][-4.428956 -4.42897 -4.4289751 -4.4289694 -4.428957 -4.4289451 -4.4289303 -4.4289246 -4.4289379 -4.4289618 -4.4289904 -4.4290133 -4.4290285 -4.4290252 -4.4290137][-4.4289703 -4.4289804 -4.428987 -4.428988 -4.4289856 -4.4289861 -4.4289856 -4.4289861 -4.4289937 -4.4290042 -4.4290161 -4.4290228 -4.4290261 -4.4290214 -4.4290128]]...]
INFO - root - 2017-12-08 06:51:39.747167: step 31510, loss = 2.28, batch loss = 2.23 (35.8 examples/sec; 0.223 sec/batch; 18h:39m:43s remains)
INFO - root - 2017-12-08 06:51:41.976049: step 31520, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 18h:18m:59s remains)
INFO - root - 2017-12-08 06:51:44.218612: step 31530, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 18h:36m:21s remains)
INFO - root - 2017-12-08 06:51:46.470809: step 31540, loss = 2.28, batch loss = 2.23 (33.9 examples/sec; 0.236 sec/batch; 19h:42m:32s remains)
INFO - root - 2017-12-08 06:51:48.738561: step 31550, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.226 sec/batch; 18h:51m:27s remains)
INFO - root - 2017-12-08 06:51:51.003765: step 31560, loss = 2.28, batch loss = 2.23 (35.6 examples/sec; 0.225 sec/batch; 18h:46m:54s remains)
INFO - root - 2017-12-08 06:51:53.271277: step 31570, loss = 2.28, batch loss = 2.23 (33.6 examples/sec; 0.238 sec/batch; 19h:52m:54s remains)
INFO - root - 2017-12-08 06:51:55.524912: step 31580, loss = 2.28, batch loss = 2.23 (34.4 examples/sec; 0.232 sec/batch; 19h:24m:56s remains)
INFO - root - 2017-12-08 06:51:57.788700: step 31590, loss = 2.28, batch loss = 2.23 (35.2 examples/sec; 0.227 sec/batch; 18h:58m:11s remains)
INFO - root - 2017-12-08 06:52:00.074487: step 31600, loss = 2.28, batch loss = 2.23 (35.8 examples/sec; 0.224 sec/batch; 18h:41m:49s remains)
2017-12-08 06:52:00.377960: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288187 -4.4288411 -4.428843 -4.4288454 -4.4288621 -4.4288845 -4.4288993 -4.4289141 -4.428915 -4.4288888 -4.4288449 -4.4288034 -4.428782 -4.4287729 -4.4287496][-4.4287796 -4.4288082 -4.4288116 -4.4288082 -4.4288177 -4.4288368 -4.4288487 -4.4288568 -4.4288478 -4.4288015 -4.4287348 -4.428689 -4.4286833 -4.4286923 -4.428678][-4.4287333 -4.4287572 -4.4287524 -4.4287391 -4.4287338 -4.42873 -4.4287224 -4.4287138 -4.4286885 -4.4286294 -4.4285588 -4.4285293 -4.4285522 -4.4285903 -4.4286][-4.4287043 -4.4287119 -4.4286876 -4.4286537 -4.4286227 -4.4285846 -4.4285398 -4.4285016 -4.4284692 -4.4284315 -4.4283986 -4.4284062 -4.4284625 -4.4285307 -4.4285607][-4.4287057 -4.4286919 -4.4286385 -4.4285755 -4.428514 -4.4284372 -4.4283466 -4.4282732 -4.4282575 -4.4282937 -4.4283452 -4.428412 -4.4284935 -4.4285679 -4.4285951][-4.4287171 -4.4286766 -4.4285975 -4.4285159 -4.4284434 -4.4283538 -4.4282289 -4.4281216 -4.4281311 -4.4282475 -4.4283791 -4.4284906 -4.4285851 -4.4286518 -4.4286661][-4.4287286 -4.4286685 -4.4285784 -4.4285035 -4.4284554 -4.4283938 -4.4282775 -4.4281759 -4.428205 -4.4283447 -4.4284925 -4.4286065 -4.4286914 -4.4287434 -4.4287438][-4.4287319 -4.4286709 -4.4285941 -4.4285488 -4.4285431 -4.4285254 -4.4284496 -4.4283853 -4.428422 -4.4285355 -4.4286494 -4.4287367 -4.4287972 -4.4288268 -4.4288087][-4.4287233 -4.4286814 -4.4286385 -4.4286361 -4.4286675 -4.4286823 -4.428648 -4.4286194 -4.4286532 -4.4287252 -4.4287977 -4.4288516 -4.4288816 -4.4288797 -4.4288383][-4.428721 -4.4287133 -4.4287119 -4.428741 -4.4287872 -4.4288135 -4.4288034 -4.4287949 -4.4288187 -4.4288597 -4.4289021 -4.4289303 -4.4289312 -4.4288988 -4.4288349][-4.4287262 -4.4287481 -4.4287767 -4.4288163 -4.4288588 -4.4288816 -4.4288812 -4.4288826 -4.4289012 -4.428926 -4.4289513 -4.4289589 -4.4289346 -4.428875 -4.428793][-4.4287276 -4.4287653 -4.4288073 -4.4288468 -4.4288821 -4.4289017 -4.4289069 -4.4289112 -4.4289241 -4.4289384 -4.428946 -4.4289346 -4.42889 -4.428812 -4.428721][-4.4287372 -4.4287796 -4.428823 -4.4288607 -4.4288888 -4.4289045 -4.4289083 -4.4289079 -4.4289136 -4.4289174 -4.4289064 -4.4288754 -4.4288177 -4.4287324 -4.4286432][-4.4287853 -4.4288197 -4.4288478 -4.4288707 -4.4288869 -4.4288921 -4.4288878 -4.4288793 -4.4288774 -4.4288716 -4.4288468 -4.4288034 -4.4287438 -4.428668 -4.4285927][-4.4288526 -4.4288673 -4.4288716 -4.4288697 -4.428864 -4.4288535 -4.4288392 -4.4288254 -4.4288197 -4.42881 -4.4287782 -4.4287314 -4.4286833 -4.4286275 -4.4285731]]...]
INFO - root - 2017-12-08 06:52:02.595333: step 31610, loss = 2.28, batch loss = 2.23 (37.0 examples/sec; 0.216 sec/batch; 18h:03m:38s remains)
INFO - root - 2017-12-08 06:52:04.881769: step 31620, loss = 2.28, batch loss = 2.23 (34.8 examples/sec; 0.230 sec/batch; 19h:11m:59s remains)
INFO - root - 2017-12-08 06:52:07.112863: step 31630, loss = 2.28, batch loss = 2.23 (32.2 examples/sec; 0.248 sec/batch; 20h:45m:04s remains)
INFO - root - 2017-12-08 06:52:09.363403: step 31640, loss = 2.28, batch loss = 2.23 (35.6 examples/sec; 0.225 sec/batch; 18h:47m:28s remains)
INFO - root - 2017-12-08 06:52:11.596922: step 31650, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 18h:17m:34s remains)
INFO - root - 2017-12-08 06:52:13.837653: step 31660, loss = 2.28, batch loss = 2.23 (34.2 examples/sec; 0.234 sec/batch; 19h:32m:51s remains)
INFO - root - 2017-12-08 06:52:16.073243: step 31670, loss = 2.28, batch loss = 2.23 (35.7 examples/sec; 0.224 sec/batch; 18h:44m:30s remains)
INFO - root - 2017-12-08 06:52:18.350285: step 31680, loss = 2.28, batch loss = 2.23 (37.5 examples/sec; 0.213 sec/batch; 17h:48m:34s remains)
INFO - root - 2017-12-08 06:52:20.590448: step 31690, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 18h:36m:20s remains)
INFO - root - 2017-12-08 06:52:22.826579: step 31700, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.222 sec/batch; 18h:31m:14s remains)
2017-12-08 06:52:23.152536: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288411 -4.4288397 -4.4288211 -4.4287877 -4.4287333 -4.4286532 -4.4285865 -4.4285979 -4.4286489 -4.4286971 -4.4287457 -4.428812 -4.428875 -4.428896 -4.4288955][-4.4288526 -4.4288659 -4.4288487 -4.4287925 -4.4287138 -4.4285965 -4.428484 -4.4284878 -4.4285755 -4.4286709 -4.4287453 -4.4288154 -4.4288821 -4.4288979 -4.428894][-4.4288368 -4.4288726 -4.428864 -4.4287992 -4.428709 -4.4285707 -4.4284186 -4.4283977 -4.4285021 -4.4286361 -4.4287405 -4.4288192 -4.4288812 -4.428894 -4.4288807][-4.4287996 -4.4288621 -4.4288759 -4.4288249 -4.4287372 -4.4286056 -4.428452 -4.4284096 -4.4284959 -4.4286323 -4.4287443 -4.4288239 -4.42888 -4.4288883 -4.4288688][-4.4287524 -4.4288287 -4.4288678 -4.4288378 -4.4287686 -4.428658 -4.4285398 -4.4285049 -4.4285688 -4.4286785 -4.4287739 -4.4288421 -4.4288869 -4.4288893 -4.42887][-4.4287248 -4.4287987 -4.4288464 -4.4288344 -4.4287896 -4.4287028 -4.4286218 -4.4286041 -4.4286594 -4.4287419 -4.428812 -4.4288611 -4.428896 -4.4288969 -4.428884][-4.4287143 -4.4287758 -4.428823 -4.4288259 -4.4288006 -4.4287343 -4.4286795 -4.4286728 -4.4287243 -4.4287868 -4.4288368 -4.4288726 -4.4288993 -4.428906 -4.4289012][-4.4287071 -4.42876 -4.4288106 -4.4288225 -4.4288044 -4.4287491 -4.4286995 -4.4286976 -4.4287505 -4.428803 -4.428843 -4.4288759 -4.4288988 -4.4289064 -4.4289093][-4.4287009 -4.4287481 -4.4287987 -4.4288135 -4.4287939 -4.4287405 -4.4286833 -4.428678 -4.4287405 -4.4287987 -4.4288445 -4.428885 -4.4289045 -4.428906 -4.4289069][-4.4286985 -4.4287519 -4.4288058 -4.4288216 -4.4288006 -4.4287486 -4.4286814 -4.4286675 -4.4287353 -4.428802 -4.4288549 -4.4288983 -4.4289145 -4.4289074 -4.4288969][-4.4287133 -4.4287734 -4.42883 -4.4288497 -4.428833 -4.4287863 -4.4287186 -4.4286947 -4.4287548 -4.4288216 -4.4288731 -4.4289145 -4.4289303 -4.4289145 -4.4288936][-4.4287481 -4.4288011 -4.4288497 -4.4288726 -4.4288626 -4.4288263 -4.42877 -4.4287424 -4.428781 -4.4288363 -4.4288797 -4.428916 -4.4289279 -4.4289112 -4.4288869][-4.428792 -4.4288325 -4.4288707 -4.4288926 -4.4288878 -4.4288578 -4.4288158 -4.4287925 -4.4288111 -4.4288487 -4.4288812 -4.4289088 -4.4289131 -4.4288974 -4.4288778][-4.428843 -4.4288697 -4.4288983 -4.4289174 -4.428915 -4.4288912 -4.4288616 -4.428844 -4.428853 -4.4288769 -4.4288945 -4.4289103 -4.42891 -4.428896 -4.4288845][-4.4288974 -4.4289112 -4.4289269 -4.4289389 -4.4289393 -4.4289274 -4.4289122 -4.4289007 -4.428905 -4.4289193 -4.4289246 -4.4289289 -4.4289269 -4.4289174 -4.4289117]]...]
INFO - root - 2017-12-08 06:52:25.385881: step 31710, loss = 2.28, batch loss = 2.23 (34.3 examples/sec; 0.233 sec/batch; 19h:29m:53s remains)
INFO - root - 2017-12-08 06:52:27.621462: step 31720, loss = 2.28, batch loss = 2.23 (35.3 examples/sec; 0.227 sec/batch; 18h:55m:32s remains)
INFO - root - 2017-12-08 06:52:29.867190: step 31730, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.225 sec/batch; 18h:50m:21s remains)
INFO - root - 2017-12-08 06:52:32.114890: step 31740, loss = 2.28, batch loss = 2.23 (34.2 examples/sec; 0.234 sec/batch; 19h:33m:20s remains)
INFO - root - 2017-12-08 06:52:34.367653: step 31750, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 18h:29m:06s remains)
INFO - root - 2017-12-08 06:52:36.650750: step 31760, loss = 2.28, batch loss = 2.23 (34.9 examples/sec; 0.229 sec/batch; 19h:09m:19s remains)
INFO - root - 2017-12-08 06:52:38.868321: step 31770, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 18h:22m:31s remains)
INFO - root - 2017-12-08 06:52:41.131987: step 31780, loss = 2.28, batch loss = 2.23 (35.2 examples/sec; 0.227 sec/batch; 18h:59m:10s remains)
INFO - root - 2017-12-08 06:52:43.392499: step 31790, loss = 2.28, batch loss = 2.23 (35.4 examples/sec; 0.226 sec/batch; 18h:53m:51s remains)
INFO - root - 2017-12-08 06:52:45.632915: step 31800, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.221 sec/batch; 18h:29m:10s remains)
2017-12-08 06:52:45.936719: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.428896 -4.4288859 -4.4288721 -4.4288273 -4.428772 -4.4287262 -4.4287019 -4.4286942 -4.4287062 -4.42874 -4.4287605 -4.4287624 -4.4287467 -4.4286861 -4.42861][-4.4288445 -4.4288473 -4.4288483 -4.4288149 -4.4287567 -4.4287033 -4.4286547 -4.4286256 -4.4286413 -4.4287052 -4.42875 -4.4287596 -4.4287291 -4.4286451 -4.4285469][-4.4287987 -4.4288063 -4.4288173 -4.4287972 -4.4287424 -4.4286842 -4.4286146 -4.4285555 -4.4285674 -4.4286604 -4.4287357 -4.4287591 -4.4287305 -4.4286485 -4.4285479][-4.4287672 -4.4287691 -4.4287796 -4.4287682 -4.4287233 -4.4286628 -4.4285746 -4.4284778 -4.4284759 -4.4285927 -4.4286985 -4.428751 -4.4287467 -4.4286885 -4.4286032][-4.4287467 -4.4287477 -4.4287477 -4.4287329 -4.4286895 -4.4286189 -4.428494 -4.4283442 -4.4283261 -4.4284763 -4.4286242 -4.4287181 -4.42875 -4.4287276 -4.4286666][-4.4287333 -4.428741 -4.4287367 -4.4287171 -4.4286757 -4.4285975 -4.428441 -4.4282441 -4.4282117 -4.428381 -4.42856 -4.4286909 -4.4287467 -4.4287453 -4.428699][-4.4287291 -4.4287386 -4.4287376 -4.428721 -4.4286871 -4.4286189 -4.42846 -4.4282517 -4.4282103 -4.4283528 -4.4285164 -4.4286618 -4.4287376 -4.4287486 -4.4287076][-4.428721 -4.4287391 -4.4287457 -4.4287376 -4.4287176 -4.4286704 -4.4285321 -4.4283471 -4.428308 -4.4284058 -4.4285097 -4.4286342 -4.4287224 -4.4287438 -4.4287124][-4.4286904 -4.4287267 -4.4287548 -4.4287667 -4.4287634 -4.4287381 -4.4286289 -4.428483 -4.4284558 -4.4285111 -4.428555 -4.42864 -4.4287229 -4.4287529 -4.4287329][-4.4287086 -4.4287453 -4.4287848 -4.4288077 -4.4288154 -4.4288068 -4.4287286 -4.4286122 -4.428586 -4.428607 -4.4286146 -4.4286666 -4.4287381 -4.4287739 -4.4287686][-4.4287696 -4.4287906 -4.4288316 -4.428853 -4.4288588 -4.428854 -4.4287996 -4.4287133 -4.4286919 -4.4286976 -4.4286981 -4.4287319 -4.4287863 -4.4288182 -4.42882][-4.428843 -4.4288416 -4.4288688 -4.4288783 -4.4288712 -4.4288645 -4.4288321 -4.4287844 -4.4287815 -4.428792 -4.4287944 -4.4288135 -4.4288454 -4.4288626 -4.428864][-4.4289136 -4.4289021 -4.4289126 -4.4289131 -4.4289017 -4.428894 -4.428874 -4.4288521 -4.4288597 -4.4288688 -4.42887 -4.4288764 -4.4288869 -4.4288907 -4.4288878][-4.4289541 -4.4289436 -4.428946 -4.428936 -4.4289174 -4.4289117 -4.4289021 -4.4288974 -4.4289093 -4.428916 -4.428915 -4.4289103 -4.4289036 -4.4288974 -4.4288907][-4.4289632 -4.4289541 -4.4289536 -4.4289374 -4.4289207 -4.4289227 -4.428925 -4.4289336 -4.4289541 -4.428967 -4.4289689 -4.42896 -4.4289417 -4.4289217 -4.4289036]]...]
INFO - root - 2017-12-08 06:52:48.172976: step 31810, loss = 2.28, batch loss = 2.23 (36.6 examples/sec; 0.219 sec/batch; 18h:15m:55s remains)
INFO - root - 2017-12-08 06:52:50.416462: step 31820, loss = 2.28, batch loss = 2.23 (35.2 examples/sec; 0.227 sec/batch; 18h:58m:19s remains)
INFO - root - 2017-12-08 06:52:52.644298: step 31830, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 18h:35m:27s remains)
INFO - root - 2017-12-08 06:52:54.864431: step 31840, loss = 2.28, batch loss = 2.23 (35.7 examples/sec; 0.224 sec/batch; 18h:41m:45s remains)
INFO - root - 2017-12-08 06:52:57.103558: step 31850, loss = 2.28, batch loss = 2.23 (36.0 examples/sec; 0.222 sec/batch; 18h:33m:56s remains)
INFO - root - 2017-12-08 06:52:59.329954: step 31860, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.225 sec/batch; 18h:47m:58s remains)
INFO - root - 2017-12-08 06:53:01.554324: step 31870, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.220 sec/batch; 18h:24m:22s remains)
INFO - root - 2017-12-08 06:53:03.781577: step 31880, loss = 2.28, batch loss = 2.23 (35.6 examples/sec; 0.225 sec/batch; 18h:47m:09s remains)
INFO - root - 2017-12-08 06:53:06.020659: step 31890, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.222 sec/batch; 18h:30m:57s remains)
INFO - root - 2017-12-08 06:53:08.263306: step 31900, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.226 sec/batch; 18h:49m:51s remains)
2017-12-08 06:53:08.552687: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4287276 -4.4286542 -4.4286375 -4.4286618 -4.4287281 -4.4287977 -4.4288244 -4.4288106 -4.4287982 -4.4287753 -4.428731 -4.4286995 -4.4287081 -4.4287496 -4.4287953][-4.4287729 -4.4286981 -4.4286661 -4.4286795 -4.4287362 -4.4287887 -4.4287963 -4.4287791 -4.4287777 -4.4287777 -4.4287553 -4.4287348 -4.4287496 -4.4287944 -4.4288368][-4.4288168 -4.4287419 -4.4286952 -4.4286919 -4.4287229 -4.4287472 -4.4287338 -4.4287214 -4.4287424 -4.4287691 -4.428772 -4.4287682 -4.4287915 -4.4288392 -4.4288783][-4.4288468 -4.4287705 -4.4287086 -4.4286857 -4.4286838 -4.4286728 -4.4286342 -4.4286318 -4.4286876 -4.4287477 -4.42878 -4.4287996 -4.428834 -4.4288812 -4.4289145][-4.4288511 -4.42877 -4.4286942 -4.4286556 -4.4286237 -4.4285722 -4.4284987 -4.4285016 -4.4285932 -4.4286847 -4.42875 -4.4288015 -4.4288521 -4.428896 -4.4289212][-4.4288268 -4.4287367 -4.428648 -4.4285922 -4.4285331 -4.42844 -4.428329 -4.4283433 -4.4284787 -4.4286 -4.42869 -4.428771 -4.4288406 -4.4288821 -4.4289002][-4.428781 -4.4286842 -4.4285879 -4.428524 -4.4284434 -4.4283185 -4.4281788 -4.4282136 -4.4283843 -4.4285183 -4.4286151 -4.4287171 -4.4288063 -4.4288492 -4.428864][-4.4287186 -4.4286203 -4.4285197 -4.4284511 -4.4283609 -4.4282126 -4.4280467 -4.4280858 -4.4282746 -4.4284153 -4.4285135 -4.4286342 -4.4287481 -4.4288015 -4.4288182][-4.428699 -4.4286032 -4.4285035 -4.4284263 -4.4283385 -4.4281936 -4.4280219 -4.4280396 -4.4282084 -4.4283428 -4.4284358 -4.4285617 -4.4286919 -4.4287553 -4.4287767][-4.4287205 -4.4286489 -4.4285746 -4.4285064 -4.4284296 -4.4283185 -4.4281893 -4.4281855 -4.4282937 -4.4283919 -4.4284678 -4.4285765 -4.4286938 -4.4287539 -4.4287777][-4.4287505 -4.4287047 -4.4286609 -4.42861 -4.4285464 -4.4284682 -4.428381 -4.4283624 -4.4284182 -4.428484 -4.4285426 -4.42863 -4.4287238 -4.4287729 -4.428792][-4.4287915 -4.4287624 -4.4287391 -4.4287043 -4.4286594 -4.4286118 -4.428556 -4.4285297 -4.428555 -4.4286008 -4.428647 -4.4287124 -4.4287777 -4.42881 -4.4288173][-4.4288335 -4.428823 -4.4288168 -4.4287996 -4.4287758 -4.4287524 -4.4287214 -4.4286957 -4.4287024 -4.428731 -4.4287653 -4.4288082 -4.4288468 -4.4288616 -4.4288588][-4.4288926 -4.4288921 -4.4288921 -4.4288864 -4.4288769 -4.4288659 -4.4288464 -4.4288225 -4.4288163 -4.4288292 -4.4288492 -4.4288707 -4.4288869 -4.4288893 -4.4288778][-4.428937 -4.4289403 -4.4289427 -4.4289412 -4.4289365 -4.4289312 -4.42892 -4.4289007 -4.4288893 -4.4288898 -4.4288983 -4.4289055 -4.4289064 -4.4288988 -4.4288821]]...]
INFO - root - 2017-12-08 06:53:10.811983: step 31910, loss = 2.28, batch loss = 2.23 (34.0 examples/sec; 0.235 sec/batch; 19h:37m:48s remains)
INFO - root - 2017-12-08 06:53:13.050783: step 31920, loss = 2.28, batch loss = 2.23 (35.6 examples/sec; 0.225 sec/batch; 18h:45m:48s remains)
INFO - root - 2017-12-08 06:53:15.309219: step 31930, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 18h:35m:19s remains)
INFO - root - 2017-12-08 06:53:17.553514: step 31940, loss = 2.28, batch loss = 2.23 (34.7 examples/sec; 0.231 sec/batch; 19h:15m:58s remains)
INFO - root - 2017-12-08 06:53:19.779249: step 31950, loss = 2.28, batch loss = 2.23 (33.3 examples/sec; 0.240 sec/batch; 20h:03m:00s remains)
INFO - root - 2017-12-08 06:53:22.054677: step 31960, loss = 2.28, batch loss = 2.23 (35.4 examples/sec; 0.226 sec/batch; 18h:52m:28s remains)
INFO - root - 2017-12-08 06:53:24.309298: step 31970, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.221 sec/batch; 18h:28m:29s remains)
INFO - root - 2017-12-08 06:53:26.524535: step 31980, loss = 2.28, batch loss = 2.23 (34.9 examples/sec; 0.229 sec/batch; 19h:06m:58s remains)
INFO - root - 2017-12-08 06:53:28.759032: step 31990, loss = 2.28, batch loss = 2.23 (35.6 examples/sec; 0.225 sec/batch; 18h:46m:59s remains)
INFO - root - 2017-12-08 06:53:31.004977: step 32000, loss = 2.28, batch loss = 2.23 (35.8 examples/sec; 0.224 sec/batch; 18h:40m:36s remains)
2017-12-08 06:53:31.278627: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289608 -4.4289441 -4.428926 -4.4289103 -4.4288912 -4.428865 -4.4288378 -4.4288135 -4.4288 -4.4287939 -4.4287944 -4.428791 -4.4287992 -4.4288197 -4.4288416][-4.4289513 -4.4289355 -4.4289184 -4.4288988 -4.4288726 -4.4288387 -4.4288044 -4.4287744 -4.4287634 -4.4287543 -4.4287477 -4.4287434 -4.4287577 -4.4287829 -4.4288068][-4.4289412 -4.428925 -4.4289069 -4.42888 -4.4288387 -4.4287963 -4.4287558 -4.4287295 -4.4287324 -4.42872 -4.4287066 -4.4286981 -4.4287167 -4.4287519 -4.42878][-4.428925 -4.4289074 -4.4288907 -4.4288626 -4.428812 -4.4287643 -4.4287224 -4.4287062 -4.4287181 -4.4286947 -4.42867 -4.4286513 -4.4286685 -4.428709 -4.4287405][-4.4289064 -4.4288797 -4.4288583 -4.4288254 -4.4287715 -4.4287267 -4.4286909 -4.4286871 -4.4287019 -4.4286728 -4.4286385 -4.4286156 -4.42863 -4.4286714 -4.4287095][-4.4288969 -4.42886 -4.4288239 -4.4287786 -4.428721 -4.4286771 -4.4286532 -4.4286628 -4.4286819 -4.4286466 -4.4286079 -4.4285922 -4.4286118 -4.4286575 -4.4287047][-4.4289012 -4.4288588 -4.4288058 -4.4287491 -4.4286866 -4.4286394 -4.4286165 -4.4286308 -4.4286494 -4.4286137 -4.4285779 -4.4285769 -4.4286051 -4.4286613 -4.4287195][-4.4289207 -4.4288807 -4.428823 -4.42876 -4.4286919 -4.4286361 -4.4285994 -4.4286027 -4.4286189 -4.4285922 -4.428565 -4.4285812 -4.4286208 -4.4286838 -4.4287391][-4.42894 -4.428906 -4.4288549 -4.4287987 -4.4287415 -4.42869 -4.4286418 -4.4286203 -4.4286175 -4.4286003 -4.4285817 -4.428607 -4.4286523 -4.4287138 -4.4287591][-4.4289389 -4.4289069 -4.4288621 -4.42882 -4.4287839 -4.4287457 -4.4286923 -4.4286475 -4.4286289 -4.4286141 -4.4286017 -4.4286242 -4.4286737 -4.4287329 -4.4287705][-4.4289289 -4.4288974 -4.4288578 -4.4288278 -4.4287982 -4.4287553 -4.4286842 -4.428618 -4.4285989 -4.4286003 -4.4286032 -4.4286232 -4.4286737 -4.4287291 -4.4287667][-4.4289231 -4.4288893 -4.4288492 -4.4288206 -4.4287896 -4.4287391 -4.4286542 -4.4285769 -4.4285717 -4.4285994 -4.4286203 -4.4286356 -4.4286723 -4.4287143 -4.4287472][-4.4289193 -4.4288788 -4.4288273 -4.4287863 -4.4287543 -4.4287095 -4.4286408 -4.4285846 -4.4285979 -4.4286389 -4.428659 -4.4286633 -4.4286704 -4.4286914 -4.4287186][-4.4289174 -4.4288721 -4.4288106 -4.4287581 -4.4287281 -4.4286966 -4.4286561 -4.4286408 -4.4286685 -4.4287047 -4.4287138 -4.4287038 -4.4286852 -4.4286823 -4.4287028][-4.4289174 -4.4288726 -4.4288034 -4.4287429 -4.4287133 -4.4286904 -4.4286637 -4.4286704 -4.4287047 -4.4287362 -4.4287438 -4.4287305 -4.4287033 -4.4286942 -4.4287148]]...]
INFO - root - 2017-12-08 06:53:33.498814: step 32010, loss = 2.28, batch loss = 2.23 (36.9 examples/sec; 0.217 sec/batch; 18h:05m:27s remains)
INFO - root - 2017-12-08 06:53:35.726359: step 32020, loss = 2.28, batch loss = 2.23 (34.5 examples/sec; 0.232 sec/batch; 19h:20m:05s remains)
INFO - root - 2017-12-08 06:53:37.989730: step 32030, loss = 2.28, batch loss = 2.23 (35.7 examples/sec; 0.224 sec/batch; 18h:40m:48s remains)
INFO - root - 2017-12-08 06:53:40.216573: step 32040, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 18h:17m:04s remains)
INFO - root - 2017-12-08 06:53:42.476314: step 32050, loss = 2.28, batch loss = 2.23 (35.8 examples/sec; 0.224 sec/batch; 18h:40m:22s remains)
INFO - root - 2017-12-08 06:53:44.715529: step 32060, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 18h:27m:58s remains)
INFO - root - 2017-12-08 06:53:46.944023: step 32070, loss = 2.28, batch loss = 2.23 (34.8 examples/sec; 0.230 sec/batch; 19h:11m:32s remains)
INFO - root - 2017-12-08 06:53:49.166325: step 32080, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.219 sec/batch; 18h:18m:57s remains)
INFO - root - 2017-12-08 06:53:51.403638: step 32090, loss = 2.28, batch loss = 2.23 (34.7 examples/sec; 0.231 sec/batch; 19h:14m:58s remains)
INFO - root - 2017-12-08 06:53:53.628759: step 32100, loss = 2.28, batch loss = 2.23 (35.8 examples/sec; 0.224 sec/batch; 18h:39m:06s remains)
2017-12-08 06:53:53.915575: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4286952 -4.4287033 -4.4287171 -4.4287281 -4.4287367 -4.42873 -4.4287047 -4.4286857 -4.4286938 -4.4287195 -4.4287496 -4.42878 -4.428802 -4.4288173 -4.4288239][-4.4286761 -4.4286785 -4.4286885 -4.42869 -4.4286852 -4.4286637 -4.428628 -4.4286108 -4.4286304 -4.4286666 -4.4287086 -4.4287505 -4.4287786 -4.4287963 -4.4288049][-4.4286585 -4.4286547 -4.4286566 -4.428648 -4.4286389 -4.4286184 -4.428587 -4.4285769 -4.4286032 -4.4286361 -4.4286776 -4.428721 -4.4287481 -4.4287648 -4.4287782][-4.4286594 -4.4286566 -4.4286575 -4.4286456 -4.4286408 -4.4286342 -4.4286118 -4.4286056 -4.42863 -4.4286556 -4.4286861 -4.42872 -4.4287338 -4.4287372 -4.4287448][-4.4286628 -4.4286575 -4.4286571 -4.4286442 -4.428638 -4.4286408 -4.4286313 -4.428628 -4.42865 -4.4286695 -4.4286895 -4.4287105 -4.4287133 -4.4287071 -4.4287076][-4.4286423 -4.4286342 -4.4286251 -4.428597 -4.4285841 -4.4286013 -4.4286113 -4.4286132 -4.42863 -4.4286494 -4.4286685 -4.4286771 -4.4286709 -4.4286623 -4.4286547][-4.4285889 -4.4285808 -4.4285588 -4.4285059 -4.4284825 -4.4285169 -4.4285488 -4.4285588 -4.4285774 -4.4286046 -4.428627 -4.42863 -4.428617 -4.4286132 -4.4286132][-4.4285369 -4.4285278 -4.4285059 -4.4284425 -4.4284134 -4.42846 -4.4285073 -4.4285207 -4.4285369 -4.4285684 -4.4285965 -4.4285989 -4.4285865 -4.4285936 -4.428616][-4.428534 -4.4285355 -4.4285412 -4.4285016 -4.4284773 -4.4285126 -4.4285531 -4.4285541 -4.4285545 -4.4285784 -4.4286013 -4.4286017 -4.4285932 -4.4286089 -4.4286432][-4.4286032 -4.4286203 -4.4286475 -4.4286389 -4.42862 -4.4286261 -4.4286408 -4.4286337 -4.4286251 -4.4286413 -4.4286504 -4.4286385 -4.42863 -4.4286456 -4.4286809][-4.4286914 -4.4287181 -4.4287519 -4.4287572 -4.4287438 -4.4287319 -4.4287229 -4.4287076 -4.4286985 -4.42871 -4.4287119 -4.428689 -4.4286737 -4.4286866 -4.428721][-4.4287519 -4.4287734 -4.4288058 -4.4288163 -4.4288096 -4.4287968 -4.4287834 -4.4287696 -4.4287605 -4.4287643 -4.4287696 -4.4287543 -4.4287395 -4.4287453 -4.4287691][-4.4288287 -4.4288406 -4.4288654 -4.4288783 -4.4288788 -4.4288697 -4.4288588 -4.4288487 -4.4288387 -4.4288359 -4.4288387 -4.4288368 -4.4288306 -4.4288316 -4.4288435][-4.4288936 -4.428896 -4.4289074 -4.4289179 -4.4289222 -4.4289193 -4.4289145 -4.4289103 -4.4289036 -4.4288993 -4.4289002 -4.4289036 -4.4289041 -4.4289031 -4.4289055][-4.4289451 -4.4289432 -4.4289446 -4.428947 -4.42895 -4.4289503 -4.4289503 -4.4289494 -4.4289465 -4.4289451 -4.4289465 -4.4289494 -4.4289517 -4.4289513 -4.4289508]]...]
INFO - root - 2017-12-08 06:53:56.150891: step 32110, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 18h:35m:32s remains)
INFO - root - 2017-12-08 06:53:58.384292: step 32120, loss = 2.28, batch loss = 2.23 (35.2 examples/sec; 0.228 sec/batch; 18h:59m:25s remains)
INFO - root - 2017-12-08 06:54:00.615553: step 32130, loss = 2.28, batch loss = 2.23 (35.2 examples/sec; 0.227 sec/batch; 18h:56m:28s remains)
INFO - root - 2017-12-08 06:54:02.868301: step 32140, loss = 2.28, batch loss = 2.23 (35.0 examples/sec; 0.229 sec/batch; 19h:05m:22s remains)
INFO - root - 2017-12-08 06:54:05.090215: step 32150, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 18h:20m:57s remains)
INFO - root - 2017-12-08 06:54:07.317262: step 32160, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 18h:19m:32s remains)
INFO - root - 2017-12-08 06:54:09.584172: step 32170, loss = 2.28, batch loss = 2.23 (34.3 examples/sec; 0.233 sec/batch; 19h:28m:23s remains)
INFO - root - 2017-12-08 06:54:11.805517: step 32180, loss = 2.28, batch loss = 2.23 (35.2 examples/sec; 0.227 sec/batch; 18h:58m:34s remains)
INFO - root - 2017-12-08 06:54:14.049993: step 32190, loss = 2.28, batch loss = 2.23 (35.1 examples/sec; 0.228 sec/batch; 19h:01m:47s remains)
INFO - root - 2017-12-08 06:54:16.306784: step 32200, loss = 2.28, batch loss = 2.23 (36.7 examples/sec; 0.218 sec/batch; 18h:09m:54s remains)
2017-12-08 06:54:16.583532: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289351 -4.4289412 -4.4289417 -4.4289389 -4.4289355 -4.4289317 -4.4289284 -4.4289265 -4.428926 -4.42893 -4.4289355 -4.4289422 -4.42895 -4.428956 -4.42896][-4.428936 -4.4289436 -4.4289441 -4.4289408 -4.4289355 -4.42893 -4.4289255 -4.4289241 -4.4289274 -4.4289365 -4.4289474 -4.428957 -4.4289641 -4.4289665 -4.4289665][-4.4289365 -4.4289417 -4.4289365 -4.4289246 -4.4289088 -4.4288931 -4.4288855 -4.4288888 -4.4289031 -4.428925 -4.428947 -4.4289656 -4.4289751 -4.4289732 -4.4289675][-4.4289336 -4.4289317 -4.4289131 -4.4288797 -4.4288373 -4.4287987 -4.4287806 -4.4287882 -4.4288197 -4.4288654 -4.4289122 -4.4289503 -4.4289722 -4.4289732 -4.4289641][-4.4289203 -4.428906 -4.4288669 -4.4288 -4.4287152 -4.4286385 -4.4286 -4.4286118 -4.4286685 -4.428751 -4.4288354 -4.4289036 -4.428946 -4.4289584 -4.4289517][-4.4289 -4.428875 -4.4288168 -4.4287157 -4.4285827 -4.4284573 -4.4283876 -4.4283986 -4.4284844 -4.4286108 -4.4287367 -4.4288354 -4.4288993 -4.4289265 -4.4289265][-4.4288893 -4.4288607 -4.4287972 -4.4286814 -4.4285197 -4.4283538 -4.42825 -4.4282541 -4.4283628 -4.4285207 -4.4286737 -4.42879 -4.428863 -4.428896 -4.4289][-4.4288969 -4.4288735 -4.4288206 -4.428721 -4.4285722 -4.4284067 -4.4282923 -4.4282894 -4.4283929 -4.4285417 -4.4286838 -4.428791 -4.4288554 -4.4288826 -4.4288845][-4.4289126 -4.4289002 -4.4288683 -4.4288044 -4.4287009 -4.4285746 -4.4284773 -4.4284635 -4.428534 -4.4286394 -4.4287419 -4.4288173 -4.4288588 -4.4288721 -4.4288697][-4.4289303 -4.4289284 -4.4289155 -4.428884 -4.4288292 -4.4287543 -4.428689 -4.4286728 -4.4287081 -4.4287663 -4.4288206 -4.4288535 -4.4288635 -4.4288592 -4.4288492][-4.4289432 -4.4289432 -4.4289341 -4.428915 -4.4288893 -4.4288559 -4.4288268 -4.428823 -4.4288425 -4.4288707 -4.4288931 -4.428895 -4.4288797 -4.4288573 -4.4288363][-4.4289465 -4.4289374 -4.4289131 -4.4288807 -4.428854 -4.4288416 -4.428844 -4.428864 -4.4288888 -4.4289103 -4.4289222 -4.428916 -4.4288931 -4.4288645 -4.4288392][-4.4289312 -4.4289103 -4.4288583 -4.4287896 -4.4287372 -4.4287281 -4.4287558 -4.4288073 -4.4288592 -4.4288983 -4.4289193 -4.4289203 -4.4289055 -4.4288826 -4.42886][-4.42889 -4.4288659 -4.428792 -4.4286866 -4.4286017 -4.428587 -4.428637 -4.428719 -4.4288025 -4.4288645 -4.4289031 -4.4289184 -4.4289179 -4.4289074 -4.4288936][-4.4288454 -4.4288363 -4.4287648 -4.4286489 -4.4285502 -4.4285364 -4.4286017 -4.4287 -4.4287939 -4.4288611 -4.4289055 -4.4289284 -4.4289365 -4.4289346 -4.4289284]]...]
INFO - root - 2017-12-08 06:54:18.789188: step 32210, loss = 2.28, batch loss = 2.23 (35.7 examples/sec; 0.224 sec/batch; 18h:40m:00s remains)
INFO - root - 2017-12-08 06:54:21.050152: step 32220, loss = 2.28, batch loss = 2.23 (34.0 examples/sec; 0.235 sec/batch; 19h:38m:28s remains)
INFO - root - 2017-12-08 06:54:23.279061: step 32230, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 18h:25m:25s remains)
INFO - root - 2017-12-08 06:54:25.528351: step 32240, loss = 2.28, batch loss = 2.23 (36.6 examples/sec; 0.219 sec/batch; 18h:13m:53s remains)
INFO - root - 2017-12-08 06:54:27.772042: step 32250, loss = 2.28, batch loss = 2.23 (36.8 examples/sec; 0.218 sec/batch; 18h:08m:46s remains)
INFO - root - 2017-12-08 06:54:30.006786: step 32260, loss = 2.28, batch loss = 2.23 (35.1 examples/sec; 0.228 sec/batch; 19h:01m:28s remains)
INFO - root - 2017-12-08 06:54:32.231315: step 32270, loss = 2.28, batch loss = 2.23 (35.1 examples/sec; 0.228 sec/batch; 18h:59m:11s remains)
INFO - root - 2017-12-08 06:54:34.456681: step 32280, loss = 2.28, batch loss = 2.23 (35.7 examples/sec; 0.224 sec/batch; 18h:40m:36s remains)
INFO - root - 2017-12-08 06:54:36.697150: step 32290, loss = 2.28, batch loss = 2.23 (35.4 examples/sec; 0.226 sec/batch; 18h:51m:01s remains)
INFO - root - 2017-12-08 06:54:38.964835: step 32300, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 18h:15m:51s remains)
2017-12-08 06:54:39.252699: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.428823 -4.4288464 -4.4288583 -4.4288578 -4.42884 -4.428812 -4.4287839 -4.4287596 -4.42875 -4.4287658 -4.4288025 -4.4288425 -4.4288735 -4.4288936 -4.4288964][-4.4288092 -4.4288292 -4.4288211 -4.428793 -4.4287605 -4.4287357 -4.4287186 -4.4287052 -4.4287047 -4.42873 -4.4287758 -4.4288263 -4.428864 -4.4288912 -4.4288988][-4.4288025 -4.4288192 -4.42879 -4.4287343 -4.4286861 -4.4286642 -4.4286604 -4.4286637 -4.4286752 -4.4287052 -4.4287529 -4.4288077 -4.4288483 -4.4288788 -4.4288917][-4.4287777 -4.4287863 -4.4287405 -4.4286609 -4.4286046 -4.4285884 -4.4285965 -4.4286089 -4.4286265 -4.4286604 -4.4287138 -4.4287758 -4.428823 -4.4288592 -4.4288821][-4.4287591 -4.4287486 -4.4286857 -4.4285817 -4.42852 -4.4285192 -4.4285474 -4.4285707 -4.4285936 -4.42863 -4.4286885 -4.4287534 -4.4288049 -4.4288473 -4.428874][-4.4287605 -4.4287119 -4.4286261 -4.4285045 -4.4284387 -4.4284496 -4.4284983 -4.4285407 -4.4285779 -4.4286151 -4.42867 -4.4287333 -4.4287887 -4.4288335 -4.4288611][-4.4287624 -4.4286804 -4.4285717 -4.4284425 -4.4283714 -4.4283776 -4.4284244 -4.4284725 -4.4285231 -4.4285707 -4.4286332 -4.4287076 -4.4287748 -4.428823 -4.4288487][-4.4287548 -4.4286518 -4.4285331 -4.4284062 -4.4283237 -4.4283013 -4.4283257 -4.4283743 -4.4284458 -4.4285164 -4.428597 -4.4286904 -4.428772 -4.4288197 -4.4288359][-4.4287691 -4.4286637 -4.4285522 -4.42844 -4.4283471 -4.4282942 -4.428298 -4.428349 -4.4284291 -4.4285092 -4.4285979 -4.428699 -4.4287834 -4.4288206 -4.4288139][-4.4287863 -4.4286947 -4.4286013 -4.4285135 -4.4284377 -4.4283881 -4.4283953 -4.4284468 -4.4285231 -4.428596 -4.4286742 -4.4287577 -4.4288154 -4.4288235 -4.428782][-4.4287996 -4.4287353 -4.4286704 -4.4286132 -4.4285684 -4.4285436 -4.4285645 -4.4286208 -4.4286928 -4.4287553 -4.4288096 -4.4288521 -4.4288678 -4.4288349 -4.4287553][-4.4288139 -4.4287806 -4.4287505 -4.4287262 -4.4287152 -4.4287167 -4.4287443 -4.4287939 -4.4288507 -4.4288907 -4.4289131 -4.428916 -4.4288955 -4.4288292 -4.4287224][-4.4288449 -4.4288392 -4.4288306 -4.428822 -4.4288206 -4.4288249 -4.4288406 -4.428874 -4.4289136 -4.4289351 -4.4289379 -4.428925 -4.428895 -4.4288235 -4.4287124][-4.4288774 -4.4288855 -4.4288855 -4.4288797 -4.4288721 -4.4288673 -4.4288678 -4.4288821 -4.4289064 -4.4289193 -4.4289231 -4.428915 -4.428896 -4.4288397 -4.4287491][-4.4289002 -4.4289079 -4.4289041 -4.42889 -4.4288721 -4.428853 -4.42884 -4.4288397 -4.4288535 -4.4288659 -4.4288778 -4.4288831 -4.4288783 -4.4288397 -4.4287791]]...]
INFO - root - 2017-12-08 06:54:41.488856: step 32310, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 18h:34m:14s remains)
INFO - root - 2017-12-08 06:54:43.706435: step 32320, loss = 2.28, batch loss = 2.23 (37.3 examples/sec; 0.214 sec/batch; 17h:51m:46s remains)
INFO - root - 2017-12-08 06:54:45.945012: step 32330, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 18h:24m:31s remains)
INFO - root - 2017-12-08 06:54:48.161996: step 32340, loss = 2.28, batch loss = 2.23 (36.7 examples/sec; 0.218 sec/batch; 18h:11m:34s remains)
INFO - root - 2017-12-08 06:54:50.399894: step 32350, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.225 sec/batch; 18h:46m:28s remains)
INFO - root - 2017-12-08 06:54:52.621829: step 32360, loss = 2.28, batch loss = 2.23 (36.8 examples/sec; 0.218 sec/batch; 18h:08m:31s remains)
INFO - root - 2017-12-08 06:54:54.866041: step 32370, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 18h:33m:47s remains)
INFO - root - 2017-12-08 06:54:57.103441: step 32380, loss = 2.28, batch loss = 2.23 (33.4 examples/sec; 0.239 sec/batch; 19h:57m:43s remains)
INFO - root - 2017-12-08 06:54:59.342418: step 32390, loss = 2.28, batch loss = 2.23 (33.9 examples/sec; 0.236 sec/batch; 19h:38m:49s remains)
INFO - root - 2017-12-08 06:55:01.597452: step 32400, loss = 2.28, batch loss = 2.23 (36.0 examples/sec; 0.222 sec/batch; 18h:30m:04s remains)
2017-12-08 06:55:01.896350: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4287424 -4.4287634 -4.4287777 -4.4288063 -4.4288378 -4.4288559 -4.4288721 -4.4288931 -4.4289179 -4.4289279 -4.4289236 -4.4289207 -4.428925 -4.4289308 -4.428936][-4.4286575 -4.4286704 -4.428678 -4.4287128 -4.428751 -4.4287734 -4.4287915 -4.4288244 -4.4288578 -4.4288669 -4.4288697 -4.4288783 -4.4288888 -4.4289 -4.42891][-4.4286361 -4.428638 -4.4286323 -4.4286571 -4.4286819 -4.428689 -4.4286981 -4.4287286 -4.4287548 -4.4287572 -4.4287739 -4.428802 -4.4288259 -4.4288487 -4.4288635][-4.4286752 -4.4286709 -4.4286585 -4.4286652 -4.4286523 -4.42863 -4.4286127 -4.4286256 -4.4286447 -4.4286575 -4.4286976 -4.4287405 -4.4287734 -4.4288039 -4.428822][-4.4286971 -4.4286962 -4.4286904 -4.4286823 -4.4286304 -4.4285707 -4.4285207 -4.4284992 -4.4285183 -4.4285703 -4.4286451 -4.4287033 -4.4287357 -4.4287648 -4.4287844][-4.4286904 -4.4287038 -4.4287157 -4.4287033 -4.4286237 -4.4285154 -4.4283996 -4.4283195 -4.4283562 -4.4284759 -4.4285917 -4.4286628 -4.4286985 -4.4287219 -4.4287434][-4.4286656 -4.4286938 -4.4287229 -4.4287114 -4.4286175 -4.4284611 -4.4282508 -4.4280891 -4.4281716 -4.4283872 -4.4285364 -4.428617 -4.4286594 -4.4286804 -4.4287019][-4.4286404 -4.4286776 -4.4287229 -4.4287186 -4.4286203 -4.4284353 -4.4281816 -4.4279971 -4.4281311 -4.4283609 -4.4285059 -4.4285855 -4.4286304 -4.4286456 -4.4286671][-4.4286633 -4.4287 -4.4287491 -4.42875 -4.4286556 -4.4284916 -4.4283166 -4.4282355 -4.42832 -4.4284453 -4.4285321 -4.4285836 -4.4286218 -4.4286327 -4.428659][-4.4287305 -4.428762 -4.4288054 -4.4288096 -4.4287314 -4.4286122 -4.4285259 -4.4285049 -4.428534 -4.4285774 -4.428606 -4.4286213 -4.4286437 -4.4286566 -4.4286895][-4.42881 -4.4288292 -4.4288626 -4.428875 -4.4288268 -4.4287572 -4.428721 -4.4287205 -4.4287219 -4.4287314 -4.4287195 -4.4286995 -4.4287028 -4.4287157 -4.4287491][-4.4288688 -4.4288716 -4.4288979 -4.4289179 -4.4288988 -4.4288635 -4.4288497 -4.4288564 -4.4288535 -4.4288478 -4.4288163 -4.4287715 -4.4287496 -4.4287543 -4.4287949][-4.4289069 -4.4288983 -4.4289126 -4.42893 -4.4289293 -4.428916 -4.42891 -4.4289255 -4.4289279 -4.428915 -4.4288816 -4.4288273 -4.4287839 -4.4287829 -4.4288273][-4.4289355 -4.428926 -4.4289308 -4.4289365 -4.4289379 -4.4289322 -4.4289255 -4.4289403 -4.4289474 -4.4289432 -4.4289155 -4.4288659 -4.4288249 -4.4288263 -4.4288664][-4.4289293 -4.4289255 -4.428926 -4.4289212 -4.428916 -4.4289074 -4.4289026 -4.4289155 -4.4289236 -4.4289212 -4.4289002 -4.4288721 -4.4288492 -4.4288549 -4.4288855]]...]
INFO - root - 2017-12-08 06:55:04.164824: step 32410, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 18h:19m:55s remains)
INFO - root - 2017-12-08 06:55:06.407135: step 32420, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 18h:16m:17s remains)
INFO - root - 2017-12-08 06:55:08.638525: step 32430, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.225 sec/batch; 18h:46m:33s remains)
INFO - root - 2017-12-08 06:55:10.864639: step 32440, loss = 2.28, batch loss = 2.23 (35.0 examples/sec; 0.229 sec/batch; 19h:03m:36s remains)
INFO - root - 2017-12-08 06:55:13.106138: step 32450, loss = 2.28, batch loss = 2.23 (34.6 examples/sec; 0.231 sec/batch; 19h:16m:48s remains)
INFO - root - 2017-12-08 06:55:15.335853: step 32460, loss = 2.28, batch loss = 2.23 (33.7 examples/sec; 0.237 sec/batch; 19h:45m:59s remains)
INFO - root - 2017-12-08 06:55:17.583477: step 32470, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 18h:16m:27s remains)
INFO - root - 2017-12-08 06:55:19.837655: step 32480, loss = 2.28, batch loss = 2.23 (34.8 examples/sec; 0.230 sec/batch; 19h:08m:11s remains)
INFO - root - 2017-12-08 06:55:22.091139: step 32490, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 18h:33m:36s remains)
INFO - root - 2017-12-08 06:55:24.324483: step 32500, loss = 2.28, batch loss = 2.23 (35.1 examples/sec; 0.228 sec/batch; 18h:59m:29s remains)
2017-12-08 06:55:24.624136: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.428864 -4.4287887 -4.4286604 -4.4285274 -4.4284172 -4.4283633 -4.4283752 -4.4284062 -4.4284239 -4.4284234 -4.4283805 -4.4283257 -4.4283195 -4.4283886 -4.428483][-4.4288712 -4.4288082 -4.4287033 -4.4285903 -4.4285035 -4.42846 -4.4284625 -4.428483 -4.4284863 -4.4284596 -4.4283986 -4.4283414 -4.4283471 -4.4284086 -4.4284911][-4.4288826 -4.4288349 -4.4287562 -4.428668 -4.4286051 -4.4285645 -4.4285512 -4.4285607 -4.4285684 -4.4285398 -4.42848 -4.4284387 -4.4284554 -4.4284973 -4.428544][-4.4288931 -4.4288559 -4.4287934 -4.4287143 -4.4286528 -4.4286008 -4.4285617 -4.428556 -4.428575 -4.4285684 -4.4285288 -4.4285088 -4.4285364 -4.428555 -4.4285693][-4.4289079 -4.4288731 -4.4288077 -4.4287162 -4.4286346 -4.428556 -4.4284973 -4.4284854 -4.4285054 -4.4285183 -4.4285126 -4.4285178 -4.4285493 -4.4285531 -4.4285555][-4.4289193 -4.4288821 -4.4288 -4.4286804 -4.4285536 -4.4284315 -4.4283562 -4.4283452 -4.42838 -4.4284339 -4.4284754 -4.4285131 -4.4285531 -4.4285603 -4.4285645][-4.4289145 -4.4288621 -4.4287491 -4.4285822 -4.42839 -4.4282331 -4.4281645 -4.4281688 -4.4282217 -4.428299 -4.4283581 -4.4284143 -4.428473 -4.4285049 -4.4285283][-4.4288917 -4.4288177 -4.4286771 -4.4284697 -4.4282317 -4.4280772 -4.4280438 -4.4280782 -4.4281511 -4.4282379 -4.4282947 -4.4283509 -4.4284225 -4.4284663 -4.4285059][-4.4288759 -4.4288025 -4.4286747 -4.4284959 -4.4283152 -4.4282265 -4.4282265 -4.4282684 -4.4283476 -4.4284272 -4.4284582 -4.4284768 -4.4285116 -4.4285293 -4.4285493][-4.4288807 -4.4288216 -4.42872 -4.428596 -4.4284878 -4.4284472 -4.4284472 -4.4284735 -4.4285426 -4.4286056 -4.4286146 -4.4286027 -4.4286051 -4.4285994 -4.4285994][-4.4288988 -4.4288592 -4.4287791 -4.4286833 -4.4286017 -4.4285703 -4.4285564 -4.4285641 -4.4286118 -4.4286513 -4.4286461 -4.4286242 -4.4286146 -4.4286022 -4.428606][-4.4289064 -4.4288778 -4.4288054 -4.4287181 -4.4286418 -4.4286041 -4.4285707 -4.4285617 -4.4285936 -4.4286232 -4.4286232 -4.4286051 -4.428587 -4.4285769 -4.4285946][-4.4288969 -4.4288568 -4.4287663 -4.4286642 -4.4285765 -4.4285355 -4.4284945 -4.4284821 -4.4285173 -4.4285612 -4.4285865 -4.4285831 -4.4285636 -4.4285583 -4.42859][-4.4288793 -4.4288187 -4.428709 -4.4285917 -4.4284978 -4.4284592 -4.4284239 -4.4284167 -4.4284606 -4.4285216 -4.4285569 -4.4285474 -4.4285207 -4.4285121 -4.4285507][-4.42887 -4.4287953 -4.4286757 -4.4285593 -4.4284759 -4.4284353 -4.4284058 -4.4283996 -4.4284434 -4.42851 -4.4285355 -4.4284983 -4.428462 -4.4284472 -4.4284911]]...]
INFO - root - 2017-12-08 06:55:26.860759: step 32510, loss = 2.28, batch loss = 2.23 (35.0 examples/sec; 0.229 sec/batch; 19h:03m:41s remains)
INFO - root - 2017-12-08 06:55:29.099390: step 32520, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 18h:24m:22s remains)
INFO - root - 2017-12-08 06:55:31.331526: step 32530, loss = 2.28, batch loss = 2.23 (33.0 examples/sec; 0.243 sec/batch; 20h:12m:59s remains)
INFO - root - 2017-12-08 06:55:33.605779: step 32540, loss = 2.28, batch loss = 2.23 (35.1 examples/sec; 0.228 sec/batch; 18h:58m:19s remains)
INFO - root - 2017-12-08 06:55:35.848509: step 32550, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.225 sec/batch; 18h:46m:40s remains)
INFO - root - 2017-12-08 06:55:38.069710: step 32560, loss = 2.28, batch loss = 2.23 (35.7 examples/sec; 0.224 sec/batch; 18h:39m:13s remains)
INFO - root - 2017-12-08 06:55:40.363278: step 32570, loss = 2.28, batch loss = 2.23 (35.1 examples/sec; 0.228 sec/batch; 18h:57m:46s remains)
INFO - root - 2017-12-08 06:55:42.597530: step 32580, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 18h:23m:13s remains)
INFO - root - 2017-12-08 06:55:44.827445: step 32590, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.222 sec/batch; 18h:28m:10s remains)
INFO - root - 2017-12-08 06:55:47.059088: step 32600, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 18h:35m:07s remains)
2017-12-08 06:55:47.364463: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.42873 -4.428771 -4.428834 -4.428865 -4.4288654 -4.4288411 -4.42877 -4.4286942 -4.4287076 -4.4287524 -4.4287677 -4.4287744 -4.4287753 -4.4287758 -4.4287972][-4.4287252 -4.4287658 -4.4288254 -4.4288454 -4.4288282 -4.4287944 -4.4287224 -4.428658 -4.4286852 -4.4287276 -4.4287405 -4.4287572 -4.42877 -4.428781 -4.428812][-4.4286904 -4.4287248 -4.4287744 -4.4287972 -4.4287882 -4.4287543 -4.4286952 -4.4286513 -4.4286776 -4.4287028 -4.4287195 -4.4287357 -4.4287572 -4.4287834 -4.4288273][-4.42866 -4.428679 -4.4287128 -4.4287429 -4.4287491 -4.4287238 -4.4286823 -4.4286504 -4.4286633 -4.4286642 -4.4286895 -4.4287276 -4.4287686 -4.4288063 -4.4288521][-4.4286351 -4.4286389 -4.42867 -4.42871 -4.4287376 -4.4287267 -4.428699 -4.4286633 -4.428648 -4.4286203 -4.4286618 -4.4287395 -4.4288068 -4.428854 -4.4288826][-4.4286308 -4.428616 -4.4286375 -4.4286685 -4.4286952 -4.4286861 -4.42867 -4.4286447 -4.4286213 -4.4285908 -4.4286532 -4.4287548 -4.4288325 -4.428884 -4.428896][-4.428659 -4.4286232 -4.4286156 -4.4286175 -4.4286213 -4.4285979 -4.4285765 -4.4285822 -4.4285851 -4.4285746 -4.4286423 -4.4287319 -4.4288068 -4.4288678 -4.4288836][-4.4287004 -4.4286537 -4.4286261 -4.4285975 -4.4285684 -4.428515 -4.4284844 -4.4285278 -4.4285607 -4.4285722 -4.4286404 -4.4287176 -4.4287791 -4.4288344 -4.4288507][-4.4287195 -4.4286828 -4.4286566 -4.4286146 -4.4285564 -4.4284792 -4.4284449 -4.428515 -4.42858 -4.4286137 -4.4286742 -4.428741 -4.4287786 -4.4288049 -4.4288058][-4.4287148 -4.4287009 -4.428689 -4.4286413 -4.4285731 -4.4284964 -4.428472 -4.4285541 -4.4286337 -4.4286723 -4.4287167 -4.4287639 -4.428771 -4.4287624 -4.4287562][-4.4287329 -4.4287367 -4.4287424 -4.4287019 -4.4286432 -4.4285965 -4.4285803 -4.4286327 -4.4286919 -4.4287238 -4.4287496 -4.4287748 -4.4287667 -4.4287395 -4.4287281][-4.4287724 -4.4287848 -4.4287977 -4.4287691 -4.4287257 -4.4287105 -4.4287062 -4.4287281 -4.4287577 -4.4287744 -4.4287825 -4.4287934 -4.4287839 -4.4287539 -4.4287338][-4.4287987 -4.428803 -4.4288087 -4.4287891 -4.428762 -4.4287682 -4.4287767 -4.4287796 -4.4287772 -4.428772 -4.4287667 -4.4287729 -4.4287739 -4.4287596 -4.4287486][-4.4287925 -4.4287906 -4.4287963 -4.4287853 -4.4287696 -4.428782 -4.4287953 -4.4287915 -4.4287744 -4.4287524 -4.4287481 -4.4287591 -4.4287734 -4.4287834 -4.4287863][-4.4287786 -4.4287724 -4.428781 -4.4287777 -4.4287634 -4.4287658 -4.4287763 -4.4287734 -4.4287634 -4.4287457 -4.4287534 -4.4287715 -4.4287896 -4.428813 -4.4288239]]...]
INFO - root - 2017-12-08 06:55:49.633438: step 32610, loss = 2.28, batch loss = 2.23 (35.2 examples/sec; 0.227 sec/batch; 18h:54m:54s remains)
INFO - root - 2017-12-08 06:55:51.863114: step 32620, loss = 2.28, batch loss = 2.23 (34.7 examples/sec; 0.231 sec/batch; 19h:13m:05s remains)
INFO - root - 2017-12-08 06:55:54.090014: step 32630, loss = 2.28, batch loss = 2.23 (35.8 examples/sec; 0.224 sec/batch; 18h:37m:42s remains)
INFO - root - 2017-12-08 06:55:56.322258: step 32640, loss = 2.28, batch loss = 2.23 (35.7 examples/sec; 0.224 sec/batch; 18h:40m:33s remains)
INFO - root - 2017-12-08 06:55:58.563540: step 32650, loss = 2.28, batch loss = 2.23 (36.6 examples/sec; 0.218 sec/batch; 18h:10m:59s remains)
INFO - root - 2017-12-08 06:56:00.818648: step 32660, loss = 2.28, batch loss = 2.23 (36.7 examples/sec; 0.218 sec/batch; 18h:08m:24s remains)
INFO - root - 2017-12-08 06:56:03.046021: step 32670, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 18h:18m:05s remains)
INFO - root - 2017-12-08 06:56:05.284624: step 32680, loss = 2.28, batch loss = 2.23 (35.6 examples/sec; 0.225 sec/batch; 18h:43m:18s remains)
INFO - root - 2017-12-08 06:56:07.501735: step 32690, loss = 2.28, batch loss = 2.23 (35.7 examples/sec; 0.224 sec/batch; 18h:41m:08s remains)
INFO - root - 2017-12-08 06:56:09.771178: step 32700, loss = 2.28, batch loss = 2.23 (34.6 examples/sec; 0.231 sec/batch; 19h:15m:09s remains)
2017-12-08 06:56:10.090434: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.42861 -4.4286809 -4.4287739 -4.4288206 -4.428803 -4.428751 -4.4287186 -4.4287329 -4.4287357 -4.4286861 -4.4286075 -4.428546 -4.4285526 -4.4286494 -4.4287763][-4.4286633 -4.4287362 -4.4288244 -4.4288626 -4.4288392 -4.4287786 -4.4287286 -4.4287138 -4.4286914 -4.4286342 -4.428575 -4.4285607 -4.4286184 -4.4287224 -4.4288273][-4.4287634 -4.4288244 -4.4288893 -4.4289041 -4.4288611 -4.428782 -4.42871 -4.4286661 -4.4286237 -4.428576 -4.4285579 -4.4285989 -4.4286914 -4.4287868 -4.4288683][-4.4288764 -4.4289112 -4.4289379 -4.4289207 -4.4288554 -4.4287539 -4.4286513 -4.4285736 -4.4285331 -4.4285355 -4.4285746 -4.4286523 -4.4287543 -4.4288373 -4.4289002][-4.4289575 -4.428968 -4.4289594 -4.428916 -4.4288268 -4.4286985 -4.4285579 -4.4284482 -4.4284339 -4.42851 -4.4286137 -4.4287162 -4.4288139 -4.4288816 -4.4289222][-4.4289942 -4.428987 -4.4289522 -4.4288855 -4.4287724 -4.4286132 -4.4284415 -4.4283276 -4.4283562 -4.4284973 -4.4286528 -4.4287772 -4.4288688 -4.42892 -4.4289436][-4.4290128 -4.4289994 -4.4289432 -4.4288549 -4.4287205 -4.4285445 -4.428369 -4.428287 -4.4283543 -4.4285154 -4.4286861 -4.4288192 -4.4289026 -4.4289432 -4.4289589][-4.4290175 -4.4289966 -4.4289269 -4.4288273 -4.4286947 -4.4285288 -4.4283781 -4.4283333 -4.4284182 -4.4285688 -4.4287248 -4.4288473 -4.4289179 -4.4289532 -4.428966][-4.4290152 -4.4289885 -4.4289174 -4.42882 -4.4286985 -4.42855 -4.4284205 -4.4284 -4.4284978 -4.4286356 -4.4287686 -4.4288712 -4.4289284 -4.4289579 -4.42897][-4.4290118 -4.4289837 -4.4289207 -4.4288316 -4.4287152 -4.4285769 -4.4284658 -4.4284654 -4.4285727 -4.4287071 -4.4288182 -4.4288964 -4.4289408 -4.4289637 -4.4289727][-4.4290075 -4.4289842 -4.4289341 -4.4288554 -4.4287434 -4.4286213 -4.42854 -4.4285583 -4.42866 -4.4287734 -4.4288568 -4.4289131 -4.4289455 -4.4289627 -4.42897][-4.4289994 -4.4289823 -4.4289432 -4.4288797 -4.4287896 -4.4287019 -4.4286561 -4.4286776 -4.4287453 -4.4288206 -4.4288764 -4.4289145 -4.4289393 -4.428956 -4.4289646][-4.4289894 -4.4289765 -4.4289455 -4.4288969 -4.4288349 -4.42878 -4.4287572 -4.4287739 -4.4288116 -4.4288545 -4.4288893 -4.428916 -4.428937 -4.4289532 -4.4289618][-4.4289837 -4.4289742 -4.4289508 -4.4289169 -4.4288764 -4.4288435 -4.4288311 -4.428844 -4.428865 -4.4288883 -4.4289093 -4.4289279 -4.4289446 -4.4289584 -4.4289641][-4.4289846 -4.4289765 -4.4289594 -4.4289384 -4.4289141 -4.4288921 -4.4288821 -4.42889 -4.4289045 -4.4289179 -4.4289317 -4.428946 -4.4289594 -4.4289694 -4.4289708]]...]
INFO - root - 2017-12-08 06:56:12.323877: step 32710, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.220 sec/batch; 18h:19m:56s remains)
INFO - root - 2017-12-08 06:56:14.553366: step 32720, loss = 2.28, batch loss = 2.23 (35.0 examples/sec; 0.228 sec/batch; 19h:00m:49s remains)
INFO - root - 2017-12-08 06:56:16.801523: step 32730, loss = 2.28, batch loss = 2.23 (35.4 examples/sec; 0.226 sec/batch; 18h:48m:15s remains)
INFO - root - 2017-12-08 06:56:19.033701: step 32740, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 18h:16m:30s remains)
INFO - root - 2017-12-08 06:56:21.273356: step 32750, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.221 sec/batch; 18h:25m:58s remains)
INFO - root - 2017-12-08 06:56:23.515501: step 32760, loss = 2.28, batch loss = 2.23 (36.6 examples/sec; 0.218 sec/batch; 18h:11m:33s remains)
INFO - root - 2017-12-08 06:56:25.763665: step 32770, loss = 2.28, batch loss = 2.23 (34.7 examples/sec; 0.231 sec/batch; 19h:13m:13s remains)
INFO - root - 2017-12-08 06:56:28.001811: step 32780, loss = 2.28, batch loss = 2.23 (34.8 examples/sec; 0.230 sec/batch; 19h:08m:31s remains)
INFO - root - 2017-12-08 06:56:30.229409: step 32790, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.222 sec/batch; 18h:28m:28s remains)
INFO - root - 2017-12-08 06:56:32.464662: step 32800, loss = 2.28, batch loss = 2.23 (37.3 examples/sec; 0.215 sec/batch; 17h:52m:17s remains)
2017-12-08 06:56:32.754519: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288135 -4.4288073 -4.428781 -4.4287424 -4.428719 -4.4287095 -4.4286962 -4.4286685 -4.4286308 -4.428617 -4.4286571 -4.4287319 -4.4287839 -4.42879 -4.4287767][-4.4287653 -4.4287653 -4.4287381 -4.4286981 -4.4286809 -4.4286823 -4.428678 -4.4286561 -4.4286251 -4.4286189 -4.428659 -4.4287229 -4.4287605 -4.4287577 -4.428741][-4.428731 -4.4287367 -4.4287109 -4.4286776 -4.4286733 -4.4286885 -4.4286976 -4.4286919 -4.4286747 -4.4286728 -4.4286966 -4.4287348 -4.4287567 -4.4287548 -4.4287467][-4.4287028 -4.4287052 -4.4286819 -4.4286566 -4.4286656 -4.4286938 -4.4287176 -4.4287338 -4.4287319 -4.4287319 -4.4287343 -4.4287438 -4.428751 -4.4287548 -4.4287529][-4.4286718 -4.4286661 -4.4286385 -4.4286122 -4.42862 -4.4286451 -4.4286752 -4.4287066 -4.4287148 -4.4287095 -4.428699 -4.428699 -4.4287043 -4.4287219 -4.4287324][-4.4286551 -4.428648 -4.4286151 -4.4285817 -4.4285688 -4.4285703 -4.428587 -4.42863 -4.4286532 -4.4286518 -4.4286418 -4.4286427 -4.4286523 -4.4286747 -4.428689][-4.4286284 -4.4286218 -4.4285836 -4.4285336 -4.4284811 -4.4284406 -4.4284525 -4.428534 -4.4286003 -4.4286261 -4.4286356 -4.4286466 -4.4286504 -4.4286523 -4.4286489][-4.428596 -4.4285827 -4.4285259 -4.4284382 -4.4283347 -4.4282608 -4.4282937 -4.4284344 -4.4285579 -4.4286251 -4.4286604 -4.4286761 -4.4286666 -4.4286427 -4.4286141][-4.4285927 -4.4285765 -4.4285126 -4.4284153 -4.4283042 -4.4282274 -4.428266 -4.428412 -4.4285417 -4.4286237 -4.4286718 -4.4286923 -4.4286804 -4.4286489 -4.428617][-4.4286323 -4.428628 -4.4285855 -4.428515 -4.4284363 -4.4283733 -4.4283776 -4.4284582 -4.4285388 -4.4286008 -4.4286466 -4.4286723 -4.4286695 -4.4286537 -4.4286427][-4.4286871 -4.42869 -4.4286642 -4.428618 -4.4285669 -4.4285107 -4.4284825 -4.4285021 -4.4285364 -4.4285774 -4.42862 -4.4286494 -4.4286594 -4.4286628 -4.4286695][-4.4287405 -4.4287457 -4.4287286 -4.4287019 -4.4286728 -4.4286275 -4.42859 -4.428575 -4.4285779 -4.4286 -4.4286337 -4.4286642 -4.4286833 -4.4286966 -4.4287081][-4.4288049 -4.4288039 -4.4287877 -4.4287724 -4.428762 -4.4287367 -4.4287071 -4.4286838 -4.4286718 -4.4286752 -4.4286928 -4.4287143 -4.4287329 -4.4287472 -4.42876][-4.42887 -4.4288626 -4.4288473 -4.4288392 -4.42884 -4.4288335 -4.4288192 -4.4288015 -4.4287839 -4.4287758 -4.4287772 -4.4287825 -4.4287896 -4.4287963 -4.4288034][-4.4289274 -4.4289184 -4.428906 -4.4288993 -4.4289012 -4.4289007 -4.4288964 -4.4288883 -4.4288759 -4.4288669 -4.4288611 -4.4288559 -4.4288521 -4.4288487 -4.4288468]]...]
INFO - root - 2017-12-08 06:56:35.018425: step 32810, loss = 2.28, batch loss = 2.23 (31.9 examples/sec; 0.251 sec/batch; 20h:53m:27s remains)
INFO - root - 2017-12-08 06:56:37.267167: step 32820, loss = 2.28, batch loss = 2.23 (36.7 examples/sec; 0.218 sec/batch; 18h:10m:14s remains)
INFO - root - 2017-12-08 06:56:39.540265: step 32830, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.222 sec/batch; 18h:27m:34s remains)
INFO - root - 2017-12-08 06:56:41.825138: step 32840, loss = 2.28, batch loss = 2.23 (34.0 examples/sec; 0.235 sec/batch; 19h:35m:56s remains)
INFO - root - 2017-12-08 06:56:44.077345: step 32850, loss = 2.28, batch loss = 2.23 (35.0 examples/sec; 0.228 sec/batch; 19h:00m:04s remains)
INFO - root - 2017-12-08 06:56:46.337101: step 32860, loss = 2.28, batch loss = 2.23 (35.3 examples/sec; 0.227 sec/batch; 18h:52m:38s remains)
INFO - root - 2017-12-08 06:56:48.581265: step 32870, loss = 2.28, batch loss = 2.23 (35.0 examples/sec; 0.228 sec/batch; 18h:59m:49s remains)
INFO - root - 2017-12-08 06:56:50.803385: step 32880, loss = 2.28, batch loss = 2.23 (34.7 examples/sec; 0.230 sec/batch; 19h:10m:08s remains)
INFO - root - 2017-12-08 06:56:53.039761: step 32890, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 18h:14m:54s remains)
INFO - root - 2017-12-08 06:56:55.272259: step 32900, loss = 2.28, batch loss = 2.23 (36.6 examples/sec; 0.219 sec/batch; 18h:12m:14s remains)
2017-12-08 06:56:55.546028: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4287195 -4.4287057 -4.4287157 -4.4287472 -4.4287786 -4.4288149 -4.4288468 -4.428863 -4.4288549 -4.42883 -4.4288168 -4.4288259 -4.4288359 -4.42884 -4.4288397][-4.4286776 -4.4286575 -4.4286704 -4.4287186 -4.4287624 -4.4288015 -4.4288397 -4.4288683 -4.4288683 -4.4288478 -4.4288325 -4.4288306 -4.4288316 -4.428833 -4.4288349][-4.4286284 -4.4286137 -4.4286289 -4.4286795 -4.4287195 -4.4287505 -4.4287963 -4.42884 -4.4288545 -4.4288483 -4.4288411 -4.4288387 -4.4288344 -4.4288268 -4.42882][-4.4285855 -4.4285827 -4.4286084 -4.4286513 -4.4286761 -4.4286885 -4.4287195 -4.428761 -4.4287896 -4.4288068 -4.4288139 -4.4288187 -4.4288197 -4.4288096 -4.4287958][-4.4285583 -4.4285617 -4.4285908 -4.428617 -4.4286151 -4.4285965 -4.4285889 -4.4286075 -4.4286571 -4.4287119 -4.4287405 -4.4287486 -4.4287534 -4.4287577 -4.4287534][-4.4285421 -4.4285336 -4.4285517 -4.4285574 -4.4285264 -4.4284625 -4.4283862 -4.428359 -4.4284506 -4.4285669 -4.428628 -4.4286432 -4.4286509 -4.4286761 -4.4286985][-4.428535 -4.4285111 -4.42851 -4.4284987 -4.4284496 -4.4283466 -4.4281888 -4.4280949 -4.4282303 -4.4284105 -4.4285021 -4.4285212 -4.4285426 -4.428596 -4.4286547][-4.4285684 -4.428546 -4.428545 -4.4285369 -4.428494 -4.4283953 -4.4282222 -4.4280972 -4.4282007 -4.4283557 -4.4284344 -4.4284515 -4.4284849 -4.4285517 -4.4286256][-4.4286385 -4.428617 -4.4286232 -4.4286246 -4.4286027 -4.4285464 -4.4284267 -4.4283261 -4.428359 -4.4284344 -4.428483 -4.4284964 -4.428524 -4.4285727 -4.4286318][-4.4286885 -4.4286642 -4.4286652 -4.4286752 -4.428679 -4.4286618 -4.4285984 -4.428535 -4.4285278 -4.4285517 -4.4285865 -4.428596 -4.428606 -4.4286213 -4.4286561][-4.4287167 -4.4286914 -4.4286866 -4.4286985 -4.4287148 -4.4287214 -4.4286919 -4.4286513 -4.4286318 -4.4286404 -4.4286652 -4.428668 -4.4286671 -4.42867 -4.42869][-4.4287524 -4.428731 -4.4287281 -4.4287429 -4.4287639 -4.4287744 -4.4287615 -4.4287405 -4.4287219 -4.4287167 -4.4287224 -4.4287219 -4.4287243 -4.4287319 -4.4287419][-4.4288011 -4.4287868 -4.4287872 -4.4287982 -4.4288139 -4.428823 -4.4288206 -4.4288116 -4.4287949 -4.428782 -4.4287796 -4.4287806 -4.4287896 -4.4288025 -4.4288063][-4.4288611 -4.4288535 -4.428853 -4.4288564 -4.4288635 -4.4288697 -4.4288707 -4.4288659 -4.4288521 -4.4288378 -4.4288359 -4.4288445 -4.4288616 -4.4288774 -4.4288797][-4.4289165 -4.4289117 -4.4289083 -4.4289064 -4.4289069 -4.4289088 -4.4289103 -4.4289088 -4.4289017 -4.4288936 -4.4288931 -4.4289041 -4.4289193 -4.4289303 -4.4289322]]...]
INFO - root - 2017-12-08 06:56:57.788943: step 32910, loss = 2.28, batch loss = 2.23 (33.6 examples/sec; 0.238 sec/batch; 19h:49m:26s remains)
INFO - root - 2017-12-08 06:57:00.031342: step 32920, loss = 2.28, batch loss = 2.23 (33.6 examples/sec; 0.238 sec/batch; 19h:48m:31s remains)
INFO - root - 2017-12-08 06:57:02.273824: step 32930, loss = 2.28, batch loss = 2.23 (36.6 examples/sec; 0.219 sec/batch; 18h:11m:31s remains)
INFO - root - 2017-12-08 06:57:04.522076: step 32940, loss = 2.28, batch loss = 2.23 (36.6 examples/sec; 0.218 sec/batch; 18h:10m:28s remains)
INFO - root - 2017-12-08 06:57:06.760335: step 32950, loss = 2.28, batch loss = 2.23 (34.8 examples/sec; 0.230 sec/batch; 19h:06m:53s remains)
INFO - root - 2017-12-08 06:57:09.030428: step 32960, loss = 2.28, batch loss = 2.23 (37.1 examples/sec; 0.216 sec/batch; 17h:57m:17s remains)
INFO - root - 2017-12-08 06:57:11.279899: step 32970, loss = 2.28, batch loss = 2.23 (35.3 examples/sec; 0.227 sec/batch; 18h:52m:40s remains)
INFO - root - 2017-12-08 06:57:13.506690: step 32980, loss = 2.28, batch loss = 2.23 (36.8 examples/sec; 0.217 sec/batch; 18h:03m:55s remains)
INFO - root - 2017-12-08 06:57:15.728878: step 32990, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 18h:23m:25s remains)
INFO - root - 2017-12-08 06:57:17.995782: step 33000, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 18h:24m:06s remains)
2017-12-08 06:57:18.297169: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4287968 -4.4287953 -4.4287686 -4.4287086 -4.4286089 -4.4284658 -4.4283576 -4.4283366 -4.4283772 -4.42848 -4.4286094 -4.4287534 -4.4288735 -4.4289594 -4.4290009][-4.4288344 -4.4288192 -4.4287863 -4.4287276 -4.4286447 -4.4285221 -4.4284205 -4.42839 -4.4284081 -4.4284754 -4.4285922 -4.4287376 -4.4288659 -4.4289503 -4.4289942][-4.4288568 -4.4288344 -4.4287992 -4.4287477 -4.4286747 -4.4285622 -4.4284625 -4.4284177 -4.4284177 -4.4284616 -4.4285722 -4.4287186 -4.4288487 -4.4289365 -4.4289827][-4.4288836 -4.4288635 -4.4288263 -4.4287696 -4.4286838 -4.4285603 -4.4284506 -4.4283991 -4.4283986 -4.4284472 -4.4285655 -4.4287167 -4.4288406 -4.4289269 -4.4289727][-4.4289079 -4.4288931 -4.4288516 -4.4287758 -4.428658 -4.4285111 -4.4283819 -4.4283328 -4.4283566 -4.4284306 -4.4285636 -4.4287205 -4.4288397 -4.4289236 -4.428966][-4.4289174 -4.4289007 -4.4288549 -4.4287505 -4.42859 -4.4284153 -4.4282761 -4.4282541 -4.4283309 -4.428442 -4.4285922 -4.428751 -4.4288635 -4.4289355 -4.4289713][-4.4289193 -4.4288921 -4.4288383 -4.4287076 -4.4284987 -4.4282832 -4.4281249 -4.4281449 -4.4283051 -4.4284692 -4.4286413 -4.4288011 -4.4288979 -4.4289536 -4.4289842][-4.42892 -4.4288797 -4.4288158 -4.4286728 -4.4284425 -4.428205 -4.4280252 -4.4280643 -4.4282837 -4.4284887 -4.4286804 -4.4288406 -4.428925 -4.428967 -4.4289956][-4.4289064 -4.4288564 -4.4287887 -4.4286523 -4.4284463 -4.4282355 -4.4280763 -4.428123 -4.4283309 -4.4285312 -4.4287181 -4.4288611 -4.4289365 -4.4289737 -4.4290013][-4.4288807 -4.4288316 -4.4287744 -4.4286594 -4.428493 -4.4283209 -4.4281988 -4.4282556 -4.4284286 -4.4285979 -4.4287577 -4.4288731 -4.428937 -4.4289751 -4.4290023][-4.42886 -4.428813 -4.4287705 -4.4286895 -4.4285665 -4.4284244 -4.4283257 -4.4283686 -4.4285011 -4.4286451 -4.4287858 -4.428884 -4.4289427 -4.42898 -4.4290032][-4.4288468 -4.4288034 -4.4287734 -4.4287267 -4.4286456 -4.4285288 -4.4284377 -4.4284506 -4.4285321 -4.428647 -4.4287758 -4.4288778 -4.4289432 -4.4289808 -4.429][-4.428844 -4.4288049 -4.4287829 -4.428761 -4.4287205 -4.4286385 -4.428565 -4.4285517 -4.4285831 -4.428658 -4.4287663 -4.42887 -4.4289374 -4.4289746 -4.4289937][-4.4288688 -4.428833 -4.4288125 -4.4287977 -4.4287786 -4.4287338 -4.4286871 -4.42866 -4.4286518 -4.4286957 -4.4287825 -4.4288721 -4.4289317 -4.4289675 -4.4289908][-4.4289141 -4.4288855 -4.4288621 -4.428843 -4.4288211 -4.428792 -4.4287653 -4.4287343 -4.4287105 -4.4287405 -4.4288111 -4.4288797 -4.4289279 -4.4289608 -4.4289865]]...]
INFO - root - 2017-12-08 06:57:20.546484: step 33010, loss = 2.28, batch loss = 2.23 (34.1 examples/sec; 0.234 sec/batch; 19h:30m:00s remains)
INFO - root - 2017-12-08 06:57:22.774667: step 33020, loss = 2.28, batch loss = 2.23 (36.9 examples/sec; 0.217 sec/batch; 18h:01m:09s remains)
INFO - root - 2017-12-08 06:57:25.005391: step 33030, loss = 2.28, batch loss = 2.23 (35.1 examples/sec; 0.228 sec/batch; 18h:58m:45s remains)
INFO - root - 2017-12-08 06:57:27.280933: step 33040, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.220 sec/batch; 18h:18m:43s remains)
INFO - root - 2017-12-08 06:57:29.514035: step 33050, loss = 2.28, batch loss = 2.23 (36.6 examples/sec; 0.218 sec/batch; 18h:09m:48s remains)
INFO - root - 2017-12-08 06:57:31.786682: step 33060, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.226 sec/batch; 18h:46m:09s remains)
INFO - root - 2017-12-08 06:57:34.052192: step 33070, loss = 2.28, batch loss = 2.23 (36.6 examples/sec; 0.219 sec/batch; 18h:11m:34s remains)
INFO - root - 2017-12-08 06:57:36.318738: step 33080, loss = 2.28, batch loss = 2.23 (34.9 examples/sec; 0.229 sec/batch; 19h:04m:56s remains)
INFO - root - 2017-12-08 06:57:38.551825: step 33090, loss = 2.28, batch loss = 2.23 (36.6 examples/sec; 0.219 sec/batch; 18h:11m:17s remains)
INFO - root - 2017-12-08 06:57:40.791176: step 33100, loss = 2.28, batch loss = 2.23 (35.8 examples/sec; 0.223 sec/batch; 18h:33m:41s remains)
2017-12-08 06:57:41.087169: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288759 -4.428843 -4.4288483 -4.4288654 -4.4288692 -4.428865 -4.4288497 -4.4288015 -4.4287376 -4.4286885 -4.4286642 -4.4286928 -4.4287434 -4.4287944 -4.4288239][-4.4288282 -4.428793 -4.4288034 -4.4288344 -4.4288535 -4.4288511 -4.4288144 -4.4287395 -4.4286647 -4.4286389 -4.4286456 -4.4286871 -4.4287381 -4.4287777 -4.4287953][-4.4287934 -4.4287496 -4.4287639 -4.42881 -4.4288383 -4.4288278 -4.4287682 -4.428669 -4.4286013 -4.4286108 -4.4286551 -4.428709 -4.42875 -4.4287639 -4.4287677][-4.4287877 -4.4287267 -4.4287295 -4.4287786 -4.4288077 -4.4287868 -4.428709 -4.4286003 -4.4285588 -4.4286075 -4.4286823 -4.4287376 -4.4287653 -4.4287596 -4.4287639][-4.4288011 -4.4287243 -4.4287152 -4.4287515 -4.4287653 -4.4287271 -4.4286413 -4.4285517 -4.42855 -4.4286261 -4.4287086 -4.4287562 -4.4287739 -4.4287629 -4.4287581][-4.4288106 -4.4287333 -4.4287214 -4.4287415 -4.4287348 -4.428688 -4.4286094 -4.428545 -4.4285736 -4.4286642 -4.4287443 -4.4287839 -4.428791 -4.4287772 -4.4287605][-4.4287939 -4.4287224 -4.4287186 -4.4287357 -4.4287238 -4.4286795 -4.4286189 -4.4285703 -4.428596 -4.4286728 -4.4287434 -4.4287791 -4.4287949 -4.4287939 -4.4287734][-4.4287744 -4.4287157 -4.4287248 -4.4287424 -4.4287372 -4.4287066 -4.428659 -4.4286089 -4.4286017 -4.4286451 -4.4287028 -4.4287457 -4.4287763 -4.4287834 -4.4287658][-4.4287486 -4.4286971 -4.4287162 -4.4287419 -4.4287491 -4.42874 -4.4287038 -4.4286451 -4.4286065 -4.4286275 -4.428679 -4.4287252 -4.4287548 -4.4287529 -4.428731][-4.4287128 -4.4286647 -4.4286947 -4.4287362 -4.4287581 -4.4287658 -4.4287305 -4.4286633 -4.4286132 -4.4286237 -4.4286671 -4.4287057 -4.4287219 -4.4287114 -4.4286942][-4.4286861 -4.4286342 -4.42866 -4.4286966 -4.4287252 -4.4287443 -4.4287195 -4.4286623 -4.4286275 -4.4286413 -4.4286733 -4.4286962 -4.4286981 -4.4286847 -4.428668][-4.4286628 -4.4286065 -4.4286156 -4.4286265 -4.4286427 -4.428668 -4.428668 -4.4286451 -4.4286394 -4.4286671 -4.4286957 -4.4287028 -4.4286976 -4.4286866 -4.4286752][-4.4286747 -4.42863 -4.4286289 -4.4286051 -4.4285741 -4.4285731 -4.4285984 -4.4286165 -4.4286432 -4.4286833 -4.4287052 -4.4287081 -4.4287219 -4.4287276 -4.428719][-4.4287238 -4.4287009 -4.4287081 -4.4286742 -4.4286079 -4.4285669 -4.4285927 -4.4286351 -4.4286728 -4.4287057 -4.428709 -4.4287066 -4.4287353 -4.42876 -4.4287534][-4.4287663 -4.4287558 -4.4287696 -4.4287415 -4.428679 -4.4286337 -4.4286456 -4.4286847 -4.4287128 -4.4287319 -4.428719 -4.4287071 -4.4287338 -4.4287639 -4.4287567]]...]
INFO - root - 2017-12-08 06:57:43.309583: step 33110, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.225 sec/batch; 18h:43m:08s remains)
INFO - root - 2017-12-08 06:57:45.567263: step 33120, loss = 2.28, batch loss = 2.23 (36.0 examples/sec; 0.222 sec/batch; 18h:30m:05s remains)
INFO - root - 2017-12-08 06:57:47.828810: step 33130, loss = 2.28, batch loss = 2.23 (35.2 examples/sec; 0.228 sec/batch; 18h:55m:26s remains)
INFO - root - 2017-12-08 06:57:50.056559: step 33140, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.220 sec/batch; 18h:19m:37s remains)
INFO - root - 2017-12-08 06:57:52.274999: step 33150, loss = 2.28, batch loss = 2.23 (37.8 examples/sec; 0.212 sec/batch; 17h:37m:16s remains)
INFO - root - 2017-12-08 06:57:54.522224: step 33160, loss = 2.28, batch loss = 2.23 (35.1 examples/sec; 0.228 sec/batch; 18h:55m:37s remains)
INFO - root - 2017-12-08 06:57:56.796345: step 33170, loss = 2.28, batch loss = 2.23 (35.7 examples/sec; 0.224 sec/batch; 18h:36m:48s remains)
INFO - root - 2017-12-08 06:57:59.059403: step 33180, loss = 2.28, batch loss = 2.23 (34.3 examples/sec; 0.233 sec/batch; 19h:22m:25s remains)
INFO - root - 2017-12-08 06:58:01.321706: step 33190, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 18h:15m:43s remains)
INFO - root - 2017-12-08 06:58:03.543072: step 33200, loss = 2.28, batch loss = 2.23 (35.7 examples/sec; 0.224 sec/batch; 18h:37m:00s remains)
2017-12-08 06:58:03.850926: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289618 -4.4289684 -4.428968 -4.42896 -4.428946 -4.4289312 -4.4289184 -4.4289141 -4.4289188 -4.4289289 -4.4289422 -4.4289508 -4.4289556 -4.4289575 -4.42896][-4.4289632 -4.4289694 -4.4289651 -4.4289484 -4.4289227 -4.428896 -4.4288721 -4.4288621 -4.4288721 -4.4288955 -4.4289241 -4.4289432 -4.4289536 -4.4289575 -4.4289613][-4.4289637 -4.4289665 -4.4289546 -4.428925 -4.428884 -4.4288425 -4.4287996 -4.4287782 -4.4287949 -4.4288387 -4.4288893 -4.428925 -4.4289451 -4.4289551 -4.4289603][-4.4289618 -4.4289603 -4.42894 -4.4289 -4.4288425 -4.4287744 -4.4287047 -4.4286695 -4.4287014 -4.4287705 -4.4288445 -4.4289002 -4.4289341 -4.4289522 -4.4289608][-4.4289565 -4.42895 -4.42892 -4.4288726 -4.4287944 -4.4286823 -4.4285645 -4.4285131 -4.4285765 -4.4286861 -4.4287896 -4.4288678 -4.4289179 -4.4289441 -4.4289589][-4.4289355 -4.4289188 -4.4288774 -4.4288163 -4.4287124 -4.4285493 -4.428371 -4.4282975 -4.4284 -4.4285707 -4.4287205 -4.4288235 -4.4288874 -4.4289207 -4.4289408][-4.42888 -4.4288564 -4.4288077 -4.4287362 -4.4286036 -4.4283948 -4.4281511 -4.4280376 -4.4281912 -4.4284425 -4.4286447 -4.4287748 -4.4288416 -4.4288716 -4.4288917][-4.4288073 -4.4287839 -4.4287367 -4.428658 -4.4285049 -4.4282589 -4.4279466 -4.4277687 -4.4279823 -4.4283147 -4.4285607 -4.4287109 -4.4287739 -4.4287982 -4.4288206][-4.4287143 -4.4286985 -4.428659 -4.4285808 -4.428431 -4.42819 -4.4278622 -4.4276466 -4.4278874 -4.4282508 -4.4285083 -4.4286518 -4.4286995 -4.4287119 -4.4287329][-4.4286528 -4.42865 -4.4286251 -4.4285626 -4.4284477 -4.428268 -4.4280233 -4.4278688 -4.42804 -4.428318 -4.4285269 -4.4286337 -4.428647 -4.428637 -4.4286456][-4.4286394 -4.4286518 -4.4286485 -4.428616 -4.4285464 -4.4284444 -4.4283032 -4.4282093 -4.4282961 -4.4284658 -4.4286075 -4.4286661 -4.4286437 -4.4286051 -4.4285908][-4.4286575 -4.4286919 -4.4287171 -4.4287119 -4.4286757 -4.428627 -4.4285522 -4.4284859 -4.4285135 -4.4286013 -4.4286942 -4.4287252 -4.4286842 -4.4286256 -4.428586][-4.4286737 -4.4287248 -4.428771 -4.4287844 -4.4287682 -4.4287558 -4.4287257 -4.4286761 -4.4286771 -4.4287167 -4.4287739 -4.4287925 -4.4287448 -4.4286757 -4.4286218][-4.4286885 -4.4287434 -4.4287982 -4.4288206 -4.4288182 -4.4288259 -4.4288235 -4.4288 -4.4287972 -4.4288125 -4.428843 -4.4288526 -4.4288063 -4.4287362 -4.4286785][-4.4287372 -4.428782 -4.4288273 -4.4288464 -4.4288483 -4.4288568 -4.4288683 -4.428865 -4.428865 -4.4288673 -4.4288774 -4.4288793 -4.4288425 -4.4287872 -4.42874]]...]
INFO - root - 2017-12-08 06:58:06.088272: step 33210, loss = 2.28, batch loss = 2.23 (36.8 examples/sec; 0.217 sec/batch; 18h:03m:58s remains)
INFO - root - 2017-12-08 06:58:08.339532: step 33220, loss = 2.28, batch loss = 2.23 (35.4 examples/sec; 0.226 sec/batch; 18h:46m:21s remains)
INFO - root - 2017-12-08 06:58:10.558647: step 33230, loss = 2.28, batch loss = 2.23 (36.8 examples/sec; 0.217 sec/batch; 18h:03m:05s remains)
INFO - root - 2017-12-08 06:58:12.777157: step 33240, loss = 2.28, batch loss = 2.23 (35.1 examples/sec; 0.228 sec/batch; 18h:56m:37s remains)
INFO - root - 2017-12-08 06:58:15.024927: step 33250, loss = 2.28, batch loss = 2.23 (35.2 examples/sec; 0.227 sec/batch; 18h:53m:25s remains)
INFO - root - 2017-12-08 06:58:17.281403: step 33260, loss = 2.28, batch loss = 2.23 (35.3 examples/sec; 0.227 sec/batch; 18h:49m:56s remains)
INFO - root - 2017-12-08 06:58:19.548021: step 33270, loss = 2.28, batch loss = 2.23 (35.4 examples/sec; 0.226 sec/batch; 18h:46m:39s remains)
INFO - root - 2017-12-08 06:58:21.789411: step 33280, loss = 2.28, batch loss = 2.23 (37.2 examples/sec; 0.215 sec/batch; 17h:53m:18s remains)
INFO - root - 2017-12-08 06:58:24.013617: step 33290, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 18h:30m:35s remains)
INFO - root - 2017-12-08 06:58:26.236428: step 33300, loss = 2.28, batch loss = 2.23 (34.2 examples/sec; 0.234 sec/batch; 19h:28m:03s remains)
2017-12-08 06:58:26.532526: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288921 -4.4289126 -4.4288888 -4.4288292 -4.4287724 -4.4287119 -4.4286404 -4.4285913 -4.4286041 -4.4286747 -4.4287572 -4.428843 -4.4289112 -4.4289494 -4.4289727][-4.4289041 -4.4289312 -4.4289036 -4.4288349 -4.4287653 -4.4286823 -4.4285784 -4.4285107 -4.4285393 -4.4286313 -4.4287252 -4.4288278 -4.428906 -4.428946 -4.4289722][-4.4288893 -4.4289241 -4.4288993 -4.4288249 -4.4287419 -4.4286289 -4.4284992 -4.4284267 -4.4284773 -4.42859 -4.4286971 -4.4288154 -4.4288979 -4.4289389 -4.4289675][-4.4288535 -4.42889 -4.4288664 -4.4287868 -4.428688 -4.4285502 -4.4284115 -4.4283509 -4.4284277 -4.4285631 -4.4286861 -4.4288154 -4.4288988 -4.4289365 -4.428966][-4.4288192 -4.4288521 -4.4288249 -4.4287376 -4.4286189 -4.4284596 -4.4283295 -4.428299 -4.4284058 -4.4285579 -4.4287 -4.4288359 -4.4289141 -4.4289465 -4.4289713][-4.4288 -4.4288149 -4.4287772 -4.4286776 -4.4285312 -4.4283566 -4.4282646 -4.4282837 -4.4284143 -4.4285722 -4.4287214 -4.4288578 -4.4289303 -4.4289594 -4.428977][-4.42877 -4.4287663 -4.4287162 -4.4285989 -4.4284172 -4.4282317 -4.4282112 -4.4283004 -4.4284506 -4.4286027 -4.4287477 -4.4288769 -4.4289441 -4.4289718 -4.4289832][-4.4287481 -4.4287357 -4.4286861 -4.4285674 -4.4283729 -4.4281926 -4.4282203 -4.4283471 -4.4284992 -4.4286332 -4.4287672 -4.4288859 -4.4289522 -4.4289823 -4.42899][-4.4287615 -4.4287462 -4.4287024 -4.4285984 -4.4284205 -4.4282718 -4.4283051 -4.4284077 -4.4285293 -4.4286423 -4.4287663 -4.4288797 -4.4289522 -4.4289861 -4.4289932][-4.4287829 -4.4287691 -4.4287281 -4.4286394 -4.4284892 -4.4283757 -4.4283967 -4.4284563 -4.4285445 -4.4286442 -4.4287586 -4.4288721 -4.42895 -4.4289846 -4.4289918][-4.4287953 -4.42878 -4.4287448 -4.4286761 -4.4285564 -4.4284739 -4.428483 -4.4285145 -4.4285784 -4.4286642 -4.4287615 -4.4288688 -4.428947 -4.4289832 -4.4289913][-4.4288049 -4.4287825 -4.4287519 -4.4287038 -4.428616 -4.4285607 -4.4285607 -4.4285722 -4.4286218 -4.4286947 -4.4287767 -4.4288731 -4.428947 -4.4289823 -4.4289913][-4.4288216 -4.4287958 -4.428771 -4.4287381 -4.4286757 -4.4286366 -4.4286261 -4.4286203 -4.4286585 -4.4287257 -4.4287963 -4.4288812 -4.4289489 -4.4289823 -4.4289913][-4.42885 -4.4288182 -4.428792 -4.4287648 -4.4287224 -4.4286966 -4.4286795 -4.4286642 -4.428689 -4.4287457 -4.42881 -4.4288864 -4.4289494 -4.4289823 -4.4289908][-4.4288588 -4.4288292 -4.4288077 -4.4287844 -4.4287548 -4.4287348 -4.4287143 -4.4286952 -4.42871 -4.4287596 -4.4288235 -4.428896 -4.4289541 -4.4289832 -4.4289904]]...]
INFO - root - 2017-12-08 06:58:28.847978: step 33310, loss = 2.28, batch loss = 2.23 (34.9 examples/sec; 0.229 sec/batch; 19h:03m:04s remains)
INFO - root - 2017-12-08 06:58:31.100191: step 33320, loss = 2.28, batch loss = 2.23 (35.2 examples/sec; 0.228 sec/batch; 18h:54m:25s remains)
INFO - root - 2017-12-08 06:58:33.336148: step 33330, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 18h:13m:11s remains)
INFO - root - 2017-12-08 06:58:35.554450: step 33340, loss = 2.28, batch loss = 2.23 (36.0 examples/sec; 0.222 sec/batch; 18h:27m:11s remains)
INFO - root - 2017-12-08 06:58:37.795084: step 33350, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 18h:12m:39s remains)
INFO - root - 2017-12-08 06:58:40.084424: step 33360, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 18h:31m:45s remains)
INFO - root - 2017-12-08 06:58:42.313566: step 33370, loss = 2.28, batch loss = 2.23 (37.9 examples/sec; 0.211 sec/batch; 17h:30m:58s remains)
INFO - root - 2017-12-08 06:58:44.554143: step 33380, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.221 sec/batch; 18h:23m:29s remains)
INFO - root - 2017-12-08 06:58:46.776621: step 33390, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.220 sec/batch; 18h:18m:44s remains)
INFO - root - 2017-12-08 06:58:49.004297: step 33400, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.220 sec/batch; 18h:17m:59s remains)
2017-12-08 06:58:49.294994: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4286566 -4.4286375 -4.4286594 -4.4286928 -4.4287047 -4.4287286 -4.4287705 -4.4287949 -4.4288011 -4.4288173 -4.4288387 -4.4288368 -4.4288244 -4.4288087 -4.4287949][-4.428628 -4.4286194 -4.4286647 -4.428721 -4.4287572 -4.4287949 -4.4288406 -4.4288592 -4.4288464 -4.4288406 -4.4288573 -4.428864 -4.4288549 -4.4288383 -4.4288268][-4.4286842 -4.4286962 -4.428751 -4.428803 -4.4288421 -4.4288712 -4.4288945 -4.4288945 -4.428864 -4.4288435 -4.4288564 -4.4288759 -4.4288774 -4.428863 -4.4288487][-4.4287844 -4.4288058 -4.4288487 -4.4288754 -4.4288888 -4.4288878 -4.4288688 -4.4288316 -4.428791 -4.4287791 -4.4287996 -4.428834 -4.4288521 -4.4288492 -4.4288416][-4.4288683 -4.4288764 -4.4288888 -4.428884 -4.428864 -4.4288192 -4.428741 -4.4286528 -4.4286237 -4.4286551 -4.4287009 -4.4287577 -4.428802 -4.4288235 -4.4288378][-4.4289045 -4.4288878 -4.4288707 -4.4288406 -4.4287825 -4.4286847 -4.4285288 -4.4283738 -4.428381 -4.4284906 -4.4285874 -4.4286776 -4.4287529 -4.4288044 -4.4288526][-4.4289336 -4.4288979 -4.4288492 -4.4287844 -4.4286923 -4.4285555 -4.4283428 -4.4281425 -4.4281979 -4.4283834 -4.4285235 -4.4286389 -4.4287267 -4.4287963 -4.428865][-4.4289436 -4.4288983 -4.4288316 -4.4287453 -4.4286423 -4.4285231 -4.4283509 -4.4282222 -4.4283 -4.4284596 -4.4285841 -4.4286971 -4.4287763 -4.4288378 -4.4288993][-4.4289274 -4.4288759 -4.4288096 -4.4287324 -4.4286566 -4.4285784 -4.4284797 -4.4284382 -4.4285135 -4.4286175 -4.4287009 -4.4288015 -4.4288712 -4.4289179 -4.4289579][-4.4289007 -4.4288397 -4.4287777 -4.4287186 -4.4286766 -4.4286437 -4.428616 -4.4286327 -4.4287019 -4.42877 -4.4288287 -4.4289021 -4.4289565 -4.4289894 -4.429008][-4.4288635 -4.428782 -4.4287143 -4.4286695 -4.428658 -4.4286723 -4.4287081 -4.4287686 -4.4288325 -4.4288759 -4.428906 -4.428947 -4.4289765 -4.4289932 -4.4290009][-4.4287934 -4.4286814 -4.4285979 -4.428565 -4.4285913 -4.428659 -4.4287443 -4.4288297 -4.4288907 -4.4289184 -4.4289269 -4.42894 -4.4289503 -4.4289589 -4.4289551][-4.4287114 -4.4285817 -4.4284859 -4.428472 -4.4285398 -4.4286551 -4.4287739 -4.428865 -4.4289107 -4.4289165 -4.4289088 -4.4289036 -4.4289007 -4.4289045 -4.428896][-4.4286752 -4.4285636 -4.4284873 -4.4284897 -4.428576 -4.4287004 -4.4288092 -4.4288764 -4.4288964 -4.4288831 -4.4288654 -4.4288435 -4.4288225 -4.4288268 -4.4288211][-4.4287114 -4.42864 -4.4285979 -4.4286089 -4.428678 -4.4287767 -4.4288535 -4.428884 -4.4288754 -4.4288449 -4.4288144 -4.4287791 -4.4287457 -4.4287534 -4.4287596]]...]
INFO - root - 2017-12-08 06:58:51.552451: step 33410, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 18h:14m:25s remains)
INFO - root - 2017-12-08 06:58:53.857797: step 33420, loss = 2.28, batch loss = 2.23 (35.4 examples/sec; 0.226 sec/batch; 18h:45m:40s remains)
INFO - root - 2017-12-08 06:58:56.111423: step 33430, loss = 2.28, batch loss = 2.23 (36.8 examples/sec; 0.217 sec/batch; 18h:03m:23s remains)
INFO - root - 2017-12-08 06:58:58.432352: step 33440, loss = 2.28, batch loss = 2.23 (35.4 examples/sec; 0.226 sec/batch; 18h:46m:56s remains)
INFO - root - 2017-12-08 06:59:00.688083: step 33450, loss = 2.28, batch loss = 2.23 (33.9 examples/sec; 0.236 sec/batch; 19h:36m:26s remains)
INFO - root - 2017-12-08 06:59:02.917236: step 33460, loss = 2.28, batch loss = 2.23 (36.0 examples/sec; 0.222 sec/batch; 18h:26m:29s remains)
INFO - root - 2017-12-08 06:59:05.147678: step 33470, loss = 2.28, batch loss = 2.23 (36.7 examples/sec; 0.218 sec/batch; 18h:07m:13s remains)
INFO - root - 2017-12-08 06:59:07.399043: step 33480, loss = 2.28, batch loss = 2.23 (34.3 examples/sec; 0.233 sec/batch; 19h:21m:53s remains)
INFO - root - 2017-12-08 06:59:09.679705: step 33490, loss = 2.28, batch loss = 2.23 (33.3 examples/sec; 0.240 sec/batch; 19h:57m:21s remains)
INFO - root - 2017-12-08 06:59:11.938613: step 33500, loss = 2.28, batch loss = 2.23 (37.1 examples/sec; 0.216 sec/batch; 17h:55m:31s remains)
2017-12-08 06:59:12.248206: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289837 -4.4289637 -4.4289212 -4.4288592 -4.4287825 -4.4287062 -4.42866 -4.4286709 -4.4287214 -4.4287667 -4.4287968 -4.4288087 -4.42881 -4.4288044 -4.4287934][-4.4290028 -4.4289756 -4.4289179 -4.4288321 -4.4287286 -4.4286251 -4.4285522 -4.4285512 -4.4286122 -4.428678 -4.4287276 -4.4287553 -4.4287744 -4.4287882 -4.4287887][-4.429008 -4.4289746 -4.428906 -4.4288039 -4.4286838 -4.4285631 -4.42847 -4.4284649 -4.4285431 -4.4286313 -4.4286938 -4.4287186 -4.4287357 -4.4287524 -4.4287567][-4.4290032 -4.4289637 -4.4288845 -4.4287677 -4.4286327 -4.4284978 -4.4283934 -4.4283981 -4.4285045 -4.4286232 -4.4286947 -4.42871 -4.4287152 -4.4287233 -4.4287276][-4.4289918 -4.4289436 -4.4288526 -4.428721 -4.4285731 -4.4284296 -4.4283214 -4.4283452 -4.4284816 -4.4286304 -4.4287162 -4.428731 -4.4287257 -4.4287248 -4.4287329][-4.4289751 -4.428916 -4.428813 -4.4286695 -4.4285183 -4.4283853 -4.4282932 -4.42834 -4.4284968 -4.4286633 -4.4287572 -4.4287729 -4.4287624 -4.4287577 -4.4287734][-4.4289608 -4.4288955 -4.4287939 -4.4286566 -4.4285207 -4.4284182 -4.4283562 -4.4284205 -4.4285765 -4.4287314 -4.4288154 -4.4288282 -4.4288177 -4.4288135 -4.4288325][-4.4289536 -4.4288898 -4.4288 -4.4286852 -4.4285803 -4.42851 -4.4284763 -4.4285512 -4.4286914 -4.4288158 -4.4288793 -4.4288888 -4.4288793 -4.428874 -4.4288883][-4.42895 -4.4288888 -4.4288135 -4.4287252 -4.4286547 -4.4286146 -4.4286046 -4.4286819 -4.4287963 -4.4288859 -4.4289246 -4.4289308 -4.4289231 -4.4289193 -4.4289279][-4.4289412 -4.4288855 -4.4288249 -4.4287624 -4.4287243 -4.428709 -4.4287157 -4.4287887 -4.4288769 -4.4289322 -4.4289503 -4.4289508 -4.4289451 -4.4289441 -4.4289479][-4.4289303 -4.4288812 -4.428833 -4.4287934 -4.4287834 -4.4287934 -4.4288154 -4.4288797 -4.4289441 -4.4289694 -4.4289656 -4.4289551 -4.4289455 -4.4289432 -4.4289422][-4.428926 -4.4288888 -4.4288554 -4.4288383 -4.4288526 -4.4288759 -4.4288993 -4.4289441 -4.4289818 -4.4289837 -4.4289622 -4.4289422 -4.4289279 -4.4289222 -4.4289122][-4.4289236 -4.4288979 -4.4288797 -4.428884 -4.4289131 -4.428937 -4.4289489 -4.4289675 -4.42898 -4.4289684 -4.4289412 -4.4289222 -4.4289069 -4.4288921 -4.4288659][-4.428926 -4.42891 -4.4289031 -4.4289165 -4.428947 -4.4289646 -4.4289622 -4.4289579 -4.4289532 -4.4289365 -4.4289145 -4.4289 -4.4288826 -4.4288564 -4.4288116][-4.4289322 -4.4289246 -4.428925 -4.4289365 -4.42896 -4.4289713 -4.4289613 -4.4289427 -4.4289289 -4.428915 -4.4289012 -4.42889 -4.4288683 -4.4288321 -4.428771]]...]
INFO - root - 2017-12-08 06:59:14.443266: step 33510, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 18h:21m:59s remains)
INFO - root - 2017-12-08 06:59:16.690326: step 33520, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.225 sec/batch; 18h:41m:32s remains)
INFO - root - 2017-12-08 06:59:18.916122: step 33530, loss = 2.28, batch loss = 2.23 (37.6 examples/sec; 0.213 sec/batch; 17h:41m:14s remains)
INFO - root - 2017-12-08 06:59:21.194938: step 33540, loss = 2.28, batch loss = 2.23 (35.2 examples/sec; 0.227 sec/batch; 18h:52m:09s remains)
INFO - root - 2017-12-08 06:59:23.425684: step 33550, loss = 2.28, batch loss = 2.23 (34.8 examples/sec; 0.230 sec/batch; 19h:05m:25s remains)
INFO - root - 2017-12-08 06:59:25.700706: step 33560, loss = 2.28, batch loss = 2.23 (33.9 examples/sec; 0.236 sec/batch; 19h:37m:22s remains)
INFO - root - 2017-12-08 06:59:27.950703: step 33570, loss = 2.28, batch loss = 2.23 (35.7 examples/sec; 0.224 sec/batch; 18h:35m:55s remains)
INFO - root - 2017-12-08 06:59:30.233958: step 33580, loss = 2.28, batch loss = 2.23 (36.0 examples/sec; 0.222 sec/batch; 18h:26m:02s remains)
INFO - root - 2017-12-08 06:59:32.500233: step 33590, loss = 2.28, batch loss = 2.23 (35.7 examples/sec; 0.224 sec/batch; 18h:37m:14s remains)
INFO - root - 2017-12-08 06:59:34.733667: step 33600, loss = 2.28, batch loss = 2.23 (34.0 examples/sec; 0.235 sec/batch; 19h:31m:48s remains)
2017-12-08 06:59:35.016865: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289432 -4.4289336 -4.4289217 -4.4289126 -4.4289112 -4.42891 -4.4288969 -4.4288874 -4.42889 -4.4288964 -4.4289026 -4.42891 -4.42892 -4.4289279 -4.4289341][-4.4289169 -4.4289069 -4.4288974 -4.4288883 -4.4288816 -4.4288731 -4.4288564 -4.42885 -4.4288616 -4.42887 -4.4288688 -4.4288697 -4.4288769 -4.428885 -4.4288955][-4.4288774 -4.4288683 -4.4288692 -4.4288616 -4.4288435 -4.428822 -4.4287992 -4.4287987 -4.4288216 -4.4288378 -4.4288349 -4.4288259 -4.4288273 -4.4288335 -4.4288435][-4.4288287 -4.4288249 -4.4288335 -4.4288235 -4.4287906 -4.4287505 -4.4287152 -4.4287133 -4.4287486 -4.4287863 -4.4287958 -4.4287844 -4.4287829 -4.4287848 -4.428791][-4.4287705 -4.4287796 -4.4287949 -4.4287777 -4.42873 -4.4286685 -4.4286003 -4.4285789 -4.428638 -4.4287186 -4.428751 -4.4287448 -4.4287429 -4.4287443 -4.4287486][-4.4287133 -4.4287391 -4.4287591 -4.4287391 -4.4286828 -4.4285913 -4.4284654 -4.4283972 -4.428494 -4.4286356 -4.4286938 -4.4286976 -4.4287043 -4.4287138 -4.428719][-4.4286647 -4.4286985 -4.4287238 -4.4287071 -4.4286461 -4.42852 -4.4283123 -4.4281793 -4.4283309 -4.4285421 -4.4286342 -4.42866 -4.42868 -4.428699 -4.4287][-4.428627 -4.4286675 -4.4286976 -4.4286852 -4.4286256 -4.4284763 -4.4281974 -4.428 -4.4281964 -4.42846 -4.428587 -4.4286442 -4.4286737 -4.4286909 -4.4286804][-4.4286184 -4.4286656 -4.4287043 -4.4287028 -4.428658 -4.4285188 -4.4282441 -4.4280457 -4.4282212 -4.4284668 -4.4286036 -4.4286757 -4.4287095 -4.4287171 -4.4286895][-4.4286337 -4.4286838 -4.4287271 -4.4287362 -4.4287105 -4.4286079 -4.4284072 -4.4282718 -4.4283957 -4.4285712 -4.4286814 -4.4287391 -4.4287629 -4.428761 -4.4287181][-4.42866 -4.4287076 -4.4287519 -4.4287629 -4.4287448 -4.4286766 -4.42855 -4.4284706 -4.4285541 -4.4286661 -4.4287348 -4.4287705 -4.4287858 -4.42878 -4.4287376][-4.4286885 -4.4287291 -4.4287715 -4.4287786 -4.428761 -4.4287262 -4.4286547 -4.4286094 -4.4286642 -4.4287295 -4.4287586 -4.4287772 -4.4287915 -4.4287877 -4.4287539][-4.4287171 -4.4287472 -4.4287839 -4.4287891 -4.4287763 -4.4287577 -4.4287143 -4.4286914 -4.4287286 -4.4287667 -4.4287786 -4.4287839 -4.4287939 -4.4287972 -4.4287744][-4.4287577 -4.4287758 -4.4288015 -4.4288058 -4.4287972 -4.4287848 -4.4287581 -4.4287486 -4.4287758 -4.4287972 -4.4288077 -4.428813 -4.4288177 -4.4288225 -4.4288125][-4.4288282 -4.4288349 -4.428844 -4.4288383 -4.4288282 -4.4288197 -4.4288096 -4.4288116 -4.4288287 -4.4288406 -4.4288492 -4.4288526 -4.4288545 -4.4288597 -4.4288626]]...]
INFO - root - 2017-12-08 06:59:37.241793: step 33610, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.225 sec/batch; 18h:41m:07s remains)
INFO - root - 2017-12-08 06:59:39.478225: step 33620, loss = 2.28, batch loss = 2.23 (35.1 examples/sec; 0.228 sec/batch; 18h:56m:37s remains)
INFO - root - 2017-12-08 06:59:41.676549: step 33630, loss = 2.28, batch loss = 2.23 (35.8 examples/sec; 0.224 sec/batch; 18h:34m:14s remains)
INFO - root - 2017-12-08 06:59:43.922342: step 33640, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 18h:14m:35s remains)
INFO - root - 2017-12-08 06:59:46.170436: step 33650, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 18h:29m:48s remains)
INFO - root - 2017-12-08 06:59:48.417360: step 33660, loss = 2.28, batch loss = 2.23 (34.1 examples/sec; 0.235 sec/batch; 19h:29m:24s remains)
INFO - root - 2017-12-08 06:59:50.642858: step 33670, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 18h:20m:21s remains)
INFO - root - 2017-12-08 06:59:52.884164: step 33680, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 18h:19m:55s remains)
INFO - root - 2017-12-08 06:59:55.125174: step 33690, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.221 sec/batch; 18h:22m:56s remains)
INFO - root - 2017-12-08 06:59:57.345016: step 33700, loss = 2.28, batch loss = 2.23 (34.9 examples/sec; 0.229 sec/batch; 19h:01m:13s remains)
2017-12-08 06:59:57.641790: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288521 -4.4288592 -4.4288621 -4.4288654 -4.4288688 -4.4288712 -4.4288716 -4.42887 -4.4288688 -4.4288683 -4.4288697 -4.4288716 -4.4288726 -4.4288716 -4.42887][-4.4288535 -4.428864 -4.4288697 -4.4288759 -4.42888 -4.4288826 -4.4288821 -4.42888 -4.4288793 -4.4288793 -4.4288793 -4.4288797 -4.4288793 -4.4288769 -4.428874][-4.428854 -4.4288659 -4.4288735 -4.42888 -4.428884 -4.428885 -4.4288836 -4.4288826 -4.4288845 -4.4288874 -4.4288883 -4.4288878 -4.4288859 -4.4288821 -4.4288774][-4.4288588 -4.4288692 -4.428875 -4.4288797 -4.4288797 -4.4288764 -4.4288721 -4.4288721 -4.4288793 -4.4288888 -4.428895 -4.4288964 -4.428895 -4.4288907 -4.428885][-4.4288583 -4.428865 -4.4288659 -4.428863 -4.4288554 -4.4288435 -4.4288321 -4.4288311 -4.4288449 -4.4288654 -4.4288826 -4.4288921 -4.4288964 -4.428895 -4.4288898][-4.4288497 -4.4288511 -4.428844 -4.4288292 -4.4288068 -4.42878 -4.4287572 -4.428751 -4.428771 -4.4288058 -4.4288392 -4.4288611 -4.4288745 -4.4288793 -4.428875][-4.4288435 -4.4288411 -4.4288263 -4.4287996 -4.4287629 -4.4287224 -4.4286852 -4.4286666 -4.4286842 -4.4287276 -4.4287734 -4.4288063 -4.4288287 -4.42884 -4.4288406][-4.4288478 -4.428843 -4.4288225 -4.4287868 -4.42874 -4.4286895 -4.4286375 -4.4286032 -4.4286108 -4.4286561 -4.4287105 -4.4287534 -4.428782 -4.4287996 -4.4288044][-4.42885 -4.4288445 -4.42882 -4.4287806 -4.428731 -4.4286761 -4.4286146 -4.4285669 -4.4285655 -4.4286113 -4.4286728 -4.4287248 -4.4287577 -4.4287753 -4.4287806][-4.4288435 -4.4288387 -4.4288177 -4.4287844 -4.4287429 -4.4286957 -4.42864 -4.4285936 -4.4285874 -4.4286284 -4.4286871 -4.42874 -4.428772 -4.4287825 -4.4287829][-4.4288445 -4.428843 -4.4288297 -4.4288073 -4.4287796 -4.4287496 -4.42871 -4.4286742 -4.4286661 -4.4286966 -4.4287434 -4.4287868 -4.4288092 -4.4288082 -4.4288015][-4.4288526 -4.428853 -4.4288454 -4.4288316 -4.4288158 -4.4287996 -4.4287772 -4.4287543 -4.4287462 -4.4287648 -4.4287963 -4.4288249 -4.4288363 -4.4288297 -4.4288216][-4.4288616 -4.4288626 -4.4288568 -4.4288487 -4.4288397 -4.4288321 -4.4288216 -4.42881 -4.4288054 -4.4288154 -4.428834 -4.4288487 -4.4288526 -4.4288445 -4.4288383][-4.4288659 -4.4288678 -4.4288635 -4.4288588 -4.4288549 -4.4288535 -4.4288516 -4.4288492 -4.4288492 -4.4288545 -4.4288621 -4.4288664 -4.4288654 -4.4288588 -4.4288545][-4.4288688 -4.4288721 -4.4288697 -4.4288678 -4.4288669 -4.4288673 -4.4288688 -4.42887 -4.4288707 -4.4288721 -4.4288721 -4.4288712 -4.4288683 -4.4288645 -4.4288626]]...]
INFO - root - 2017-12-08 06:59:59.851468: step 33710, loss = 2.28, batch loss = 2.23 (35.0 examples/sec; 0.229 sec/batch; 18h:58m:06s remains)
INFO - root - 2017-12-08 07:00:02.091167: step 33720, loss = 2.28, batch loss = 2.23 (35.1 examples/sec; 0.228 sec/batch; 18h:54m:35s remains)
INFO - root - 2017-12-08 07:00:04.312947: step 33730, loss = 2.28, batch loss = 2.23 (35.2 examples/sec; 0.227 sec/batch; 18h:51m:14s remains)
INFO - root - 2017-12-08 07:00:06.563173: step 33740, loss = 2.28, batch loss = 2.23 (34.9 examples/sec; 0.229 sec/batch; 19h:01m:46s remains)
INFO - root - 2017-12-08 07:00:08.807217: step 33750, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.221 sec/batch; 18h:22m:44s remains)
INFO - root - 2017-12-08 07:00:11.061250: step 33760, loss = 2.28, batch loss = 2.23 (37.7 examples/sec; 0.212 sec/batch; 17h:36m:01s remains)
INFO - root - 2017-12-08 07:00:13.307684: step 33770, loss = 2.28, batch loss = 2.23 (35.4 examples/sec; 0.226 sec/batch; 18h:45m:21s remains)
INFO - root - 2017-12-08 07:00:15.538508: step 33780, loss = 2.28, batch loss = 2.23 (36.6 examples/sec; 0.219 sec/batch; 18h:09m:12s remains)
INFO - root - 2017-12-08 07:00:17.759313: step 33790, loss = 2.28, batch loss = 2.23 (34.5 examples/sec; 0.232 sec/batch; 19h:14m:14s remains)
INFO - root - 2017-12-08 07:00:20.001580: step 33800, loss = 2.28, batch loss = 2.23 (36.0 examples/sec; 0.222 sec/batch; 18h:26m:59s remains)
2017-12-08 07:00:20.273105: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288616 -4.4288244 -4.4287963 -4.4287648 -4.4287071 -4.4286466 -4.4286356 -4.4286757 -4.4287267 -4.4287763 -4.4288254 -4.4288478 -4.4288397 -4.4288268 -4.4288354][-4.4288921 -4.4288654 -4.4288292 -4.4287772 -4.4287062 -4.4286308 -4.4286051 -4.4286504 -4.4287195 -4.4287763 -4.428823 -4.4288397 -4.4288316 -4.4288192 -4.428822][-4.4289093 -4.428885 -4.4288464 -4.4287868 -4.4287095 -4.428618 -4.4285746 -4.4286184 -4.4287062 -4.4287739 -4.428823 -4.4288411 -4.4288387 -4.428833 -4.4288359][-4.428895 -4.4288726 -4.4288335 -4.42878 -4.4287105 -4.4286203 -4.4285684 -4.4286013 -4.4286885 -4.4287648 -4.42882 -4.4288383 -4.4288392 -4.4288449 -4.4288573][-4.4288688 -4.428853 -4.4288111 -4.4287562 -4.4286914 -4.4286046 -4.428544 -4.4285555 -4.4286327 -4.4287252 -4.4287934 -4.4288187 -4.428822 -4.428834 -4.42886][-4.4288592 -4.42885 -4.428803 -4.4287362 -4.4286566 -4.4285536 -4.4284563 -4.4284134 -4.4284897 -4.4286313 -4.4287353 -4.4287858 -4.428802 -4.42882 -4.4288545][-4.4288659 -4.4288626 -4.428813 -4.4287353 -4.4286404 -4.42851 -4.428349 -4.428225 -4.4283128 -4.4285264 -4.4286747 -4.4287496 -4.428782 -4.4288073 -4.42885][-4.4288831 -4.4288931 -4.4288468 -4.4287643 -4.4286675 -4.4285336 -4.4283509 -4.4281993 -4.4282928 -4.4285212 -4.4286618 -4.4287319 -4.4287634 -4.428792 -4.4288383][-4.4289045 -4.4289222 -4.4288774 -4.4287872 -4.4287 -4.4285893 -4.4284415 -4.4283423 -4.4284363 -4.428607 -4.4286971 -4.4287405 -4.4287572 -4.428782 -4.428822][-4.4289165 -4.4289403 -4.4289007 -4.4288154 -4.4287405 -4.4286556 -4.4285326 -4.4284811 -4.4285784 -4.4286952 -4.4287462 -4.4287686 -4.4287682 -4.4287829 -4.428812][-4.4289212 -4.4289513 -4.4289212 -4.4288459 -4.4287825 -4.4287252 -4.42862 -4.4285831 -4.4286709 -4.4287543 -4.4287848 -4.4288011 -4.4287982 -4.4288025 -4.428813][-4.4289279 -4.428956 -4.4289355 -4.4288735 -4.4288211 -4.4287882 -4.4287052 -4.4286704 -4.4287391 -4.4287977 -4.4288187 -4.4288368 -4.4288387 -4.4288387 -4.4288311][-4.4289379 -4.4289575 -4.4289432 -4.4288983 -4.4288688 -4.4288554 -4.4288044 -4.4287748 -4.4288149 -4.4288464 -4.42886 -4.4288764 -4.42888 -4.4288764 -4.4288616][-4.4289508 -4.42896 -4.4289494 -4.4289193 -4.4288983 -4.4288907 -4.4288597 -4.4288387 -4.4288583 -4.4288754 -4.4288888 -4.4289064 -4.4289136 -4.4289107 -4.4288993][-4.4289589 -4.4289618 -4.4289532 -4.4289317 -4.4289136 -4.4289031 -4.4288855 -4.4288707 -4.4288783 -4.4288917 -4.4289083 -4.4289255 -4.4289346 -4.428937 -4.4289327]]...]
INFO - root - 2017-12-08 07:00:22.480749: step 33810, loss = 2.28, batch loss = 2.23 (35.3 examples/sec; 0.227 sec/batch; 18h:48m:38s remains)
INFO - root - 2017-12-08 07:00:24.757694: step 33820, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 18h:14m:26s remains)
INFO - root - 2017-12-08 07:00:26.979191: step 33830, loss = 2.28, batch loss = 2.23 (33.6 examples/sec; 0.238 sec/batch; 19h:45m:10s remains)
INFO - root - 2017-12-08 07:00:29.209987: step 33840, loss = 2.28, batch loss = 2.23 (37.1 examples/sec; 0.215 sec/batch; 17h:52m:24s remains)
INFO - root - 2017-12-08 07:00:31.475717: step 33850, loss = 2.28, batch loss = 2.23 (35.6 examples/sec; 0.225 sec/batch; 18h:39m:04s remains)
INFO - root - 2017-12-08 07:00:33.698921: step 33860, loss = 2.28, batch loss = 2.23 (35.6 examples/sec; 0.224 sec/batch; 18h:37m:07s remains)
INFO - root - 2017-12-08 07:00:35.940560: step 33870, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.222 sec/batch; 18h:23m:10s remains)
INFO - root - 2017-12-08 07:00:38.205304: step 33880, loss = 2.28, batch loss = 2.23 (37.3 examples/sec; 0.214 sec/batch; 17h:46m:30s remains)
INFO - root - 2017-12-08 07:00:40.459071: step 33890, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 18h:14m:39s remains)
INFO - root - 2017-12-08 07:00:42.681946: step 33900, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 18h:18m:24s remains)
2017-12-08 07:00:42.979167: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4287953 -4.4287896 -4.4287825 -4.4287663 -4.4287624 -4.4287515 -4.4287238 -4.428699 -4.4286804 -4.4286942 -4.42873 -4.4287534 -4.4287748 -4.4288058 -4.4288421][-4.4287381 -4.4287233 -4.4287167 -4.4286995 -4.4286966 -4.4286838 -4.4286413 -4.4286089 -4.4285903 -4.4286137 -4.4286556 -4.428678 -4.4287004 -4.4287362 -4.4287834][-4.4286962 -4.4286718 -4.4286704 -4.42866 -4.428658 -4.428648 -4.4286036 -4.4285765 -4.4285545 -4.4285741 -4.428617 -4.42864 -4.4286623 -4.4286976 -4.4287448][-4.4286408 -4.4286184 -4.4286227 -4.4286184 -4.4286137 -4.4286041 -4.4285693 -4.4285455 -4.4285183 -4.4285331 -4.4285836 -4.4286218 -4.4286494 -4.4286776 -4.4287171][-4.4285789 -4.428555 -4.428556 -4.4285445 -4.4285283 -4.428514 -4.4284887 -4.4284673 -4.4284325 -4.4284492 -4.4285207 -4.4285827 -4.4286227 -4.4286523 -4.4286914][-4.428503 -4.4284639 -4.4284492 -4.4284163 -4.4283834 -4.4283695 -4.4283557 -4.42834 -4.4283075 -4.4283333 -4.4284239 -4.4285064 -4.4285612 -4.4286089 -4.4286642][-4.42849 -4.4284277 -4.4283805 -4.4283047 -4.4282422 -4.4282279 -4.4282284 -4.4282317 -4.428216 -4.4282575 -4.428359 -4.4284625 -4.42853 -4.4285941 -4.4286561][-4.4285674 -4.4284911 -4.4284263 -4.4283242 -4.428237 -4.4282064 -4.4282064 -4.4282136 -4.4282074 -4.4282556 -4.4283643 -4.4284811 -4.4285555 -4.4286208 -4.4286747][-4.4286733 -4.4286032 -4.42855 -4.4284649 -4.4283834 -4.4283409 -4.428319 -4.4283118 -4.4282966 -4.42833 -4.428422 -4.4285312 -4.4285984 -4.4286556 -4.4287014][-4.4287624 -4.4287124 -4.4286842 -4.4286346 -4.4285779 -4.4285374 -4.428503 -4.4284749 -4.4284382 -4.4284425 -4.4285026 -4.4285851 -4.4286418 -4.4286952 -4.4287429][-4.4288111 -4.4287829 -4.4287705 -4.4287496 -4.4287219 -4.4286985 -4.4286723 -4.4286423 -4.4286041 -4.4285946 -4.4286246 -4.4286728 -4.4287124 -4.4287558 -4.4288][-4.4288611 -4.4288497 -4.4288468 -4.4288378 -4.428823 -4.4288111 -4.4288006 -4.4287848 -4.4287591 -4.4287477 -4.4287615 -4.4287891 -4.4288139 -4.4288383 -4.4288664][-4.4289069 -4.428905 -4.4289083 -4.4289055 -4.4288979 -4.4288936 -4.42889 -4.4288831 -4.4288669 -4.4288588 -4.4288678 -4.4288859 -4.4288983 -4.4289055 -4.4289184][-4.4289346 -4.4289365 -4.4289403 -4.4289408 -4.4289384 -4.4289393 -4.4289408 -4.4289384 -4.4289274 -4.4289212 -4.4289289 -4.4289393 -4.4289436 -4.4289436 -4.4289479][-4.428936 -4.4289412 -4.4289451 -4.428946 -4.428947 -4.4289489 -4.4289508 -4.4289489 -4.4289408 -4.4289355 -4.4289384 -4.4289441 -4.4289484 -4.4289503 -4.4289556]]...]
INFO - root - 2017-12-08 07:00:45.194048: step 33910, loss = 2.28, batch loss = 2.23 (35.6 examples/sec; 0.224 sec/batch; 18h:37m:07s remains)
INFO - root - 2017-12-08 07:00:47.449464: step 33920, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 18h:20m:34s remains)
INFO - root - 2017-12-08 07:00:49.696931: step 33930, loss = 2.28, batch loss = 2.23 (35.8 examples/sec; 0.224 sec/batch; 18h:32m:29s remains)
INFO - root - 2017-12-08 07:00:51.918393: step 33940, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.226 sec/batch; 18h:42m:38s remains)
INFO - root - 2017-12-08 07:00:54.149757: step 33950, loss = 2.28, batch loss = 2.23 (34.6 examples/sec; 0.231 sec/batch; 19h:11m:10s remains)
INFO - root - 2017-12-08 07:00:56.417291: step 33960, loss = 2.28, batch loss = 2.23 (35.7 examples/sec; 0.224 sec/batch; 18h:34m:16s remains)
INFO - root - 2017-12-08 07:00:58.655600: step 33970, loss = 2.28, batch loss = 2.23 (35.4 examples/sec; 0.226 sec/batch; 18h:45m:49s remains)
INFO - root - 2017-12-08 07:01:00.917178: step 33980, loss = 2.28, batch loss = 2.23 (34.5 examples/sec; 0.232 sec/batch; 19h:12m:59s remains)
INFO - root - 2017-12-08 07:01:03.167156: step 33990, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.222 sec/batch; 18h:22m:42s remains)
INFO - root - 2017-12-08 07:01:05.391413: step 34000, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.221 sec/batch; 18h:21m:50s remains)
2017-12-08 07:01:05.689571: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.42861 -4.4285989 -4.4286208 -4.4286389 -4.4286418 -4.4286261 -4.4286051 -4.4285841 -4.428555 -4.4285417 -4.4285779 -4.4286337 -4.4286704 -4.4286804 -4.4287019][-4.4285989 -4.4285975 -4.4286256 -4.4286361 -4.4286237 -4.428607 -4.4285951 -4.4285836 -4.4285617 -4.4285507 -4.4285846 -4.4286389 -4.4286737 -4.4286819 -4.4287009][-4.4285989 -4.4286036 -4.42863 -4.4286232 -4.4285922 -4.4285712 -4.42856 -4.4285493 -4.4285264 -4.4285121 -4.4285479 -4.4286065 -4.4286566 -4.4286795 -4.4286995][-4.428627 -4.4286203 -4.428618 -4.4285889 -4.4285522 -4.4285283 -4.4285197 -4.4285188 -4.4284921 -4.4284697 -4.4285059 -4.4285765 -4.4286413 -4.4286709 -4.4286933][-4.4286652 -4.4286394 -4.428607 -4.4285536 -4.4285059 -4.42847 -4.4284592 -4.4284768 -4.4284625 -4.4284439 -4.4284835 -4.42857 -4.4286485 -4.42868 -4.4286971][-4.4287138 -4.4286823 -4.4286251 -4.4285378 -4.4284458 -4.4283595 -4.4283509 -4.4284167 -4.4284534 -4.4284573 -4.4284983 -4.428586 -4.4286647 -4.4287009 -4.428719][-4.4287562 -4.4287314 -4.4286532 -4.4285312 -4.4283714 -4.4282131 -4.4282093 -4.4283414 -4.4284315 -4.4284544 -4.4285011 -4.4285979 -4.4286823 -4.4287243 -4.4287443][-4.42876 -4.4287448 -4.4286642 -4.4285293 -4.428328 -4.4281421 -4.4281597 -4.4283237 -4.428421 -4.4284363 -4.4284854 -4.4285946 -4.4286904 -4.4287415 -4.4287663][-4.4287367 -4.4287381 -4.4286766 -4.4285693 -4.4284072 -4.4282708 -4.428298 -4.4284191 -4.4284782 -4.4284706 -4.4285078 -4.428607 -4.4287009 -4.4287548 -4.4287839][-4.4287128 -4.4287238 -4.4286952 -4.4286413 -4.42855 -4.4284697 -4.4284811 -4.4285541 -4.4285769 -4.4285488 -4.4285669 -4.4286404 -4.4287171 -4.428762 -4.4287896][-4.4287148 -4.4287291 -4.4287176 -4.4287057 -4.4286666 -4.4286189 -4.42862 -4.4286656 -4.4286685 -4.4286208 -4.4286165 -4.4286633 -4.4287229 -4.428761 -4.4287844][-4.4287405 -4.4287586 -4.4287562 -4.4287567 -4.4287457 -4.4287205 -4.4287205 -4.42875 -4.4287438 -4.4286861 -4.428659 -4.4286761 -4.428721 -4.4287558 -4.4287767][-4.4287658 -4.4287786 -4.4287806 -4.4287806 -4.4287748 -4.4287591 -4.4287639 -4.4287839 -4.4287658 -4.4287009 -4.428668 -4.4286785 -4.4287148 -4.4287477 -4.42877][-4.4287853 -4.4287915 -4.428793 -4.4287891 -4.4287767 -4.4287624 -4.42877 -4.4287815 -4.428762 -4.4286995 -4.4286666 -4.4286776 -4.4287143 -4.4287467 -4.4287724][-4.4287868 -4.4287844 -4.42878 -4.4287724 -4.4287605 -4.4287496 -4.4287572 -4.4287686 -4.4287548 -4.4287019 -4.4286766 -4.4286909 -4.428731 -4.4287596 -4.428781]]...]
INFO - root - 2017-12-08 07:01:07.941836: step 34010, loss = 2.28, batch loss = 2.23 (34.7 examples/sec; 0.230 sec/batch; 19h:05m:28s remains)
INFO - root - 2017-12-08 07:01:10.200907: step 34020, loss = 2.28, batch loss = 2.23 (34.9 examples/sec; 0.229 sec/batch; 18h:58m:54s remains)
INFO - root - 2017-12-08 07:01:12.449774: step 34030, loss = 2.28, batch loss = 2.23 (36.7 examples/sec; 0.218 sec/batch; 18h:04m:57s remains)
INFO - root - 2017-12-08 07:01:14.676683: step 34040, loss = 2.28, batch loss = 2.23 (36.6 examples/sec; 0.219 sec/batch; 18h:08m:14s remains)
INFO - root - 2017-12-08 07:01:16.918898: step 34050, loss = 2.28, batch loss = 2.23 (37.2 examples/sec; 0.215 sec/batch; 17h:49m:31s remains)
INFO - root - 2017-12-08 07:01:19.135007: step 34060, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 18h:08m:58s remains)
INFO - root - 2017-12-08 07:01:21.384550: step 34070, loss = 2.28, batch loss = 2.23 (35.8 examples/sec; 0.224 sec/batch; 18h:31m:41s remains)
INFO - root - 2017-12-08 07:01:23.659574: step 34080, loss = 2.28, batch loss = 2.23 (35.8 examples/sec; 0.223 sec/batch; 18h:31m:28s remains)
INFO - root - 2017-12-08 07:01:25.920955: step 34090, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 18h:18m:52s remains)
INFO - root - 2017-12-08 07:01:28.166041: step 34100, loss = 2.28, batch loss = 2.23 (35.7 examples/sec; 0.224 sec/batch; 18h:33m:33s remains)
2017-12-08 07:01:28.466927: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289694 -4.428968 -4.4289637 -4.4289527 -4.4289365 -4.4289207 -4.4289055 -4.4289002 -4.4289088 -4.4289289 -4.4289494 -4.4289637 -4.4289689 -4.4289694 -4.428968][-4.4289804 -4.4289775 -4.428968 -4.4289474 -4.4289207 -4.4288926 -4.4288678 -4.4288597 -4.4288759 -4.4289093 -4.4289436 -4.4289656 -4.4289737 -4.4289732 -4.4289718][-4.4289813 -4.4289742 -4.428956 -4.4289231 -4.428874 -4.4288211 -4.4287758 -4.4287634 -4.4287934 -4.4288516 -4.4289083 -4.4289474 -4.4289632 -4.4289656 -4.4289656][-4.4289784 -4.42897 -4.4289403 -4.4288869 -4.4288111 -4.4287252 -4.4286442 -4.4286242 -4.4286776 -4.42877 -4.4288568 -4.4289179 -4.4289494 -4.4289608 -4.4289622][-4.4289784 -4.4289637 -4.428915 -4.4288354 -4.4287291 -4.4286046 -4.42848 -4.4284539 -4.4285383 -4.4286671 -4.4287844 -4.4288726 -4.4289269 -4.4289527 -4.4289618][-4.4289737 -4.428947 -4.4288788 -4.4287767 -4.4286456 -4.4284787 -4.4283075 -4.4282765 -4.4283943 -4.4285536 -4.4286947 -4.4288116 -4.4288931 -4.4289374 -4.4289594][-4.4289465 -4.4289079 -4.4288239 -4.42871 -4.4285579 -4.4283447 -4.4281197 -4.428092 -4.4282532 -4.4284425 -4.4286003 -4.4287367 -4.4288383 -4.4289007 -4.4289384][-4.4289103 -4.4288621 -4.428772 -4.428659 -4.4285035 -4.428268 -4.4280138 -4.4279962 -4.428194 -4.4283972 -4.4285531 -4.4286861 -4.4287906 -4.4288616 -4.42891][-4.428874 -4.4288244 -4.4287486 -4.428659 -4.4285426 -4.4283638 -4.4281831 -4.4281697 -4.4283133 -4.4284635 -4.4285803 -4.4286828 -4.4287705 -4.4288335 -4.4288816][-4.4288473 -4.4288058 -4.4287591 -4.4287095 -4.4286408 -4.4285374 -4.4284477 -4.428441 -4.4285073 -4.4285817 -4.42865 -4.4287205 -4.4287839 -4.4288268 -4.4288621][-4.4288373 -4.4288087 -4.4287944 -4.4287815 -4.4287496 -4.4286942 -4.4286604 -4.4286547 -4.42867 -4.4286861 -4.428721 -4.4287724 -4.4288144 -4.4288359 -4.428853][-4.4288387 -4.4288197 -4.4288235 -4.4288321 -4.4288173 -4.4287887 -4.4287791 -4.4287724 -4.4287653 -4.4287558 -4.4287763 -4.4288173 -4.4288392 -4.4288421 -4.4288445][-4.4288325 -4.4288149 -4.428822 -4.4288421 -4.4288416 -4.4288321 -4.4288344 -4.4288278 -4.4288182 -4.4288087 -4.4288278 -4.4288559 -4.4288611 -4.4288497 -4.4288454][-4.4288397 -4.4288235 -4.4288321 -4.42885 -4.4288535 -4.4288516 -4.4288568 -4.4288516 -4.4288416 -4.4288378 -4.42886 -4.4288836 -4.4288826 -4.4288669 -4.4288621][-4.4288621 -4.4288592 -4.4288716 -4.4288821 -4.4288797 -4.4288788 -4.4288793 -4.4288735 -4.4288664 -4.4288673 -4.4288836 -4.4288979 -4.4288926 -4.4288807 -4.4288826]]...]
INFO - root - 2017-12-08 07:01:30.659151: step 34110, loss = 2.28, batch loss = 2.23 (37.7 examples/sec; 0.212 sec/batch; 17h:36m:32s remains)
INFO - root - 2017-12-08 07:01:32.904962: step 34120, loss = 2.28, batch loss = 2.23 (36.8 examples/sec; 0.218 sec/batch; 18h:02m:21s remains)
INFO - root - 2017-12-08 07:01:35.131700: step 34130, loss = 2.28, batch loss = 2.23 (36.6 examples/sec; 0.219 sec/batch; 18h:06m:49s remains)
INFO - root - 2017-12-08 07:01:37.367404: step 34140, loss = 2.28, batch loss = 2.23 (35.3 examples/sec; 0.227 sec/batch; 18h:48m:04s remains)
INFO - root - 2017-12-08 07:01:39.635752: step 34150, loss = 2.28, batch loss = 2.23 (36.0 examples/sec; 0.222 sec/batch; 18h:25m:03s remains)
INFO - root - 2017-12-08 07:01:41.860523: step 34160, loss = 2.28, batch loss = 2.23 (36.0 examples/sec; 0.222 sec/batch; 18h:25m:42s remains)
INFO - root - 2017-12-08 07:01:44.093666: step 34170, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 18h:13m:43s remains)
INFO - root - 2017-12-08 07:01:46.321910: step 34180, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 18h:12m:24s remains)
INFO - root - 2017-12-08 07:01:48.538689: step 34190, loss = 2.28, batch loss = 2.23 (36.8 examples/sec; 0.218 sec/batch; 18h:02m:08s remains)
INFO - root - 2017-12-08 07:01:50.784461: step 34200, loss = 2.28, batch loss = 2.23 (35.0 examples/sec; 0.229 sec/batch; 18h:56m:48s remains)
2017-12-08 07:01:51.082389: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288263 -4.428843 -4.4288607 -4.4288654 -4.428844 -4.4288 -4.4287643 -4.4287629 -4.4287806 -4.4287915 -4.4287853 -4.4287782 -4.4287858 -4.428792 -4.428792][-4.4287972 -4.4288077 -4.428823 -4.4288363 -4.4288325 -4.4288073 -4.4287786 -4.4287806 -4.4288073 -4.42883 -4.4288344 -4.4288297 -4.4288259 -4.4288192 -4.4288106][-4.4287939 -4.428803 -4.4288125 -4.4288187 -4.4288173 -4.4287996 -4.4287767 -4.4287782 -4.4288034 -4.4288287 -4.4288416 -4.4288454 -4.4288435 -4.4288349 -4.4288268][-4.4288034 -4.4288158 -4.4288192 -4.4288116 -4.4287953 -4.428771 -4.4287472 -4.4287438 -4.4287653 -4.428793 -4.4288187 -4.42885 -4.4288578 -4.4288464 -4.4288397][-4.4287777 -4.4287958 -4.4287972 -4.4287767 -4.4287362 -4.4286938 -4.428669 -4.4286718 -4.4287033 -4.4287415 -4.4287872 -4.4288435 -4.4288611 -4.4288449 -4.42883][-4.4286962 -4.4287162 -4.4287143 -4.4286847 -4.428616 -4.4285479 -4.4285245 -4.428556 -4.4286227 -4.4286871 -4.4287534 -4.4288263 -4.4288535 -4.4288297 -4.4287992][-4.4286041 -4.42862 -4.4286084 -4.4285607 -4.4284649 -4.4283733 -4.4283471 -4.4284143 -4.4285259 -4.4286218 -4.4287076 -4.4287896 -4.4288239 -4.4287987 -4.4287562][-4.4285674 -4.4285731 -4.42856 -4.4285073 -4.4283967 -4.4282794 -4.4282351 -4.4283128 -4.42845 -4.4285617 -4.4286494 -4.4287257 -4.4287634 -4.4287472 -4.4287095][-4.4286094 -4.4286089 -4.4286022 -4.4285645 -4.4284663 -4.4283509 -4.4282913 -4.4283438 -4.4284625 -4.4285583 -4.4286213 -4.4286714 -4.4287024 -4.4287004 -4.4286838][-4.4286895 -4.4286914 -4.4287009 -4.4286914 -4.4286256 -4.4285364 -4.4284778 -4.4284921 -4.4285574 -4.4286127 -4.4286442 -4.4286656 -4.4286823 -4.4286919 -4.4287028][-4.4287453 -4.4287567 -4.4287782 -4.4287939 -4.4287691 -4.4287162 -4.4286752 -4.4286656 -4.4286866 -4.4287071 -4.4287138 -4.4287133 -4.4287181 -4.4287305 -4.4287586][-4.4287596 -4.4287667 -4.4287891 -4.428822 -4.4288316 -4.4288173 -4.4288039 -4.4287882 -4.4287891 -4.4287906 -4.4287853 -4.4287715 -4.4287624 -4.4287682 -4.4287982][-4.4287629 -4.4287562 -4.4287663 -4.4287958 -4.4288211 -4.4288354 -4.4288425 -4.4288335 -4.4288316 -4.4288287 -4.4288149 -4.428792 -4.4287739 -4.4287748 -4.4288034][-4.4287934 -4.4287739 -4.4287682 -4.4287796 -4.4288015 -4.4288263 -4.4288459 -4.42885 -4.4288492 -4.4288383 -4.4288096 -4.4287691 -4.4287424 -4.4287395 -4.4287686][-4.4288325 -4.4288025 -4.4287815 -4.4287796 -4.4287949 -4.4288206 -4.4288507 -4.4288692 -4.4288707 -4.4288483 -4.4288044 -4.4287491 -4.4287095 -4.4286957 -4.4287214]]...]
INFO - root - 2017-12-08 07:01:53.260967: step 34210, loss = 2.28, batch loss = 2.23 (35.0 examples/sec; 0.229 sec/batch; 18h:56m:57s remains)
INFO - root - 2017-12-08 07:01:55.486988: step 34220, loss = 2.28, batch loss = 2.23 (37.5 examples/sec; 0.213 sec/batch; 17h:39m:36s remains)
INFO - root - 2017-12-08 07:01:57.696032: step 34230, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 18h:19m:13s remains)
INFO - root - 2017-12-08 07:01:59.915448: step 34240, loss = 2.28, batch loss = 2.23 (37.5 examples/sec; 0.213 sec/batch; 17h:40m:39s remains)
INFO - root - 2017-12-08 07:02:02.154718: step 34250, loss = 2.28, batch loss = 2.23 (36.0 examples/sec; 0.222 sec/batch; 18h:23m:19s remains)
INFO - root - 2017-12-08 07:02:04.423760: step 34260, loss = 2.28, batch loss = 2.23 (34.8 examples/sec; 0.230 sec/batch; 19h:01m:52s remains)
INFO - root - 2017-12-08 07:02:06.665029: step 34270, loss = 2.28, batch loss = 2.23 (35.2 examples/sec; 0.227 sec/batch; 18h:48m:06s remains)
INFO - root - 2017-12-08 07:02:08.895752: step 34280, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 18h:26m:09s remains)
INFO - root - 2017-12-08 07:02:11.156545: step 34290, loss = 2.28, batch loss = 2.23 (35.2 examples/sec; 0.227 sec/batch; 18h:49m:04s remains)
INFO - root - 2017-12-08 07:02:13.383927: step 34300, loss = 2.28, batch loss = 2.23 (36.7 examples/sec; 0.218 sec/batch; 18h:04m:37s remains)
2017-12-08 07:02:13.685359: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4287663 -4.428791 -4.4288211 -4.4288535 -4.4288912 -4.4289269 -4.4289427 -4.428946 -4.4289494 -4.4289627 -4.4289789 -4.4289918 -4.4289951 -4.4289865 -4.4289622][-4.42869 -4.4287281 -4.4287744 -4.4288235 -4.4288716 -4.4289122 -4.4289308 -4.428936 -4.4289412 -4.4289575 -4.4289742 -4.42898 -4.428967 -4.428946 -4.4289184][-4.4286494 -4.4286861 -4.4287438 -4.4288039 -4.4288578 -4.4288969 -4.4289184 -4.4289265 -4.428936 -4.4289551 -4.428968 -4.4289651 -4.4289412 -4.4289 -4.42886][-4.4286609 -4.4286833 -4.4287415 -4.428803 -4.4288449 -4.4288721 -4.4288898 -4.4288931 -4.4289041 -4.4289308 -4.4289489 -4.4289508 -4.4289255 -4.4288721 -4.4288149][-4.4286733 -4.4286857 -4.42874 -4.4288 -4.428822 -4.4288187 -4.4288015 -4.4287829 -4.4287944 -4.428834 -4.428875 -4.4288993 -4.4288888 -4.4288435 -4.428782][-4.4286747 -4.4286795 -4.4287171 -4.4287581 -4.4287539 -4.4287105 -4.428637 -4.4285793 -4.428596 -4.4286642 -4.4287438 -4.4288034 -4.4288216 -4.4288 -4.4287443][-4.4286456 -4.4286485 -4.428659 -4.4286647 -4.428627 -4.4285417 -4.4284115 -4.4283152 -4.4283524 -4.4284692 -4.4285932 -4.4286895 -4.4287457 -4.4287567 -4.4287224][-4.4285975 -4.4285893 -4.4285655 -4.4285383 -4.4284754 -4.4283595 -4.428194 -4.4280872 -4.4281635 -4.4283314 -4.4284916 -4.4286184 -4.4287014 -4.4287324 -4.428721][-4.4285808 -4.428545 -4.4284887 -4.4284406 -4.4283824 -4.4282894 -4.4281578 -4.428081 -4.4281645 -4.42833 -4.4284816 -4.428606 -4.4286933 -4.4287286 -4.4287267][-4.428606 -4.4285526 -4.4284892 -4.4284487 -4.4284225 -4.4283872 -4.4283223 -4.428277 -4.428318 -4.4284239 -4.4285283 -4.4286251 -4.4287009 -4.4287214 -4.4287043][-4.4286571 -4.4286041 -4.4285555 -4.4285336 -4.42853 -4.4285269 -4.4285 -4.4284682 -4.4284649 -4.4285107 -4.4285717 -4.428628 -4.4286666 -4.4286494 -4.428596][-4.4287229 -4.4286795 -4.4286447 -4.4286313 -4.4286304 -4.4286251 -4.4286032 -4.4285769 -4.4285555 -4.4285622 -4.4285851 -4.4285955 -4.4285889 -4.4285364 -4.4284582][-4.4287767 -4.4287429 -4.428719 -4.4287081 -4.4287033 -4.4286871 -4.4286571 -4.4286323 -4.42861 -4.4285994 -4.428597 -4.4285736 -4.4285307 -4.4284611 -4.4283834][-4.4287987 -4.4287753 -4.428761 -4.4287524 -4.4287472 -4.4287295 -4.4287009 -4.4286795 -4.4286594 -4.4286427 -4.4286232 -4.4285779 -4.428524 -4.4284668 -4.4284163][-4.4288049 -4.4287934 -4.4287896 -4.4287844 -4.4287825 -4.4287744 -4.4287524 -4.4287357 -4.428721 -4.4287028 -4.4286757 -4.4286275 -4.42858 -4.4285464 -4.4285293]]...]
INFO - root - 2017-12-08 07:02:15.915317: step 34310, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.220 sec/batch; 18h:15m:30s remains)
INFO - root - 2017-12-08 07:02:18.153760: step 34320, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 18h:12m:39s remains)
INFO - root - 2017-12-08 07:02:20.382342: step 34330, loss = 2.28, batch loss = 2.23 (37.0 examples/sec; 0.216 sec/batch; 17h:53m:16s remains)
INFO - root - 2017-12-08 07:02:22.629731: step 34340, loss = 2.28, batch loss = 2.23 (34.9 examples/sec; 0.229 sec/batch; 18h:57m:58s remains)
INFO - root - 2017-12-08 07:02:24.858996: step 34350, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.226 sec/batch; 18h:40m:41s remains)
INFO - root - 2017-12-08 07:02:27.113918: step 34360, loss = 2.28, batch loss = 2.23 (35.3 examples/sec; 0.227 sec/batch; 18h:46m:46s remains)
INFO - root - 2017-12-08 07:02:29.384252: step 34370, loss = 2.28, batch loss = 2.23 (36.6 examples/sec; 0.219 sec/batch; 18h:06m:07s remains)
INFO - root - 2017-12-08 07:02:31.623524: step 34380, loss = 2.28, batch loss = 2.23 (34.8 examples/sec; 0.230 sec/batch; 19h:02m:59s remains)
INFO - root - 2017-12-08 07:02:33.884008: step 34390, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 18h:19m:06s remains)
INFO - root - 2017-12-08 07:02:36.117514: step 34400, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 18h:12m:52s remains)
2017-12-08 07:02:36.432865: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.428802 -4.4287863 -4.4287405 -4.4286895 -4.4286556 -4.4286265 -4.4286 -4.4285722 -4.4285479 -4.4285455 -4.4285369 -4.428544 -4.4286008 -4.428669 -4.4287171][-4.4288 -4.4287782 -4.4287176 -4.4286571 -4.4286218 -4.4285989 -4.4285722 -4.4285517 -4.4285288 -4.428534 -4.4285321 -4.42853 -4.4285793 -4.428638 -4.4286838][-4.4287519 -4.428731 -4.4286623 -4.428606 -4.4285927 -4.428587 -4.4285583 -4.4285316 -4.4285116 -4.4285231 -4.4285369 -4.4285421 -4.4285822 -4.4286342 -4.4286838][-4.4286876 -4.4286718 -4.4286151 -4.4285803 -4.4285927 -4.4285932 -4.4285507 -4.4285188 -4.4285049 -4.4285131 -4.4285312 -4.4285455 -4.4285846 -4.4286427 -4.4287047][-4.42865 -4.4286451 -4.428607 -4.4285889 -4.4286041 -4.4285831 -4.4285154 -4.4284859 -4.4284978 -4.4285178 -4.4285383 -4.42855 -4.4285893 -4.4286532 -4.4287257][-4.4286408 -4.428647 -4.4286256 -4.4286075 -4.4286017 -4.428545 -4.4284348 -4.428411 -4.4284735 -4.4285274 -4.4285507 -4.4285564 -4.4285884 -4.4286451 -4.4287233][-4.4286418 -4.4286556 -4.4286418 -4.42862 -4.4285846 -4.4284816 -4.4283371 -4.4283218 -4.4284487 -4.4285483 -4.4285779 -4.4285746 -4.4285874 -4.4286342 -4.4287109][-4.4286432 -4.428669 -4.4286666 -4.4286375 -4.4285736 -4.4284468 -4.4283037 -4.4283061 -4.4284515 -4.428565 -4.4285831 -4.428566 -4.4285784 -4.4286337 -4.4287105][-4.4286566 -4.4286914 -4.4286966 -4.4286675 -4.4286017 -4.428503 -4.4284177 -4.42842 -4.428503 -4.4285655 -4.4285607 -4.4285517 -4.4285846 -4.4286561 -4.4287333][-4.4287033 -4.4287338 -4.4287481 -4.4287329 -4.428679 -4.428607 -4.4285522 -4.4285431 -4.4285622 -4.4285684 -4.4285522 -4.4285645 -4.4286251 -4.4287081 -4.4287772][-4.4287496 -4.42878 -4.4288006 -4.4287963 -4.4287553 -4.4286933 -4.4286461 -4.4286289 -4.4286218 -4.4285965 -4.4285779 -4.4286103 -4.4286885 -4.4287705 -4.4288263][-4.42879 -4.4288163 -4.4288349 -4.4288354 -4.4288049 -4.4287486 -4.4287076 -4.4287 -4.4286971 -4.4286718 -4.4286561 -4.4286923 -4.4287672 -4.4288354 -4.4288745][-4.4288259 -4.4288487 -4.4288616 -4.4288549 -4.4288287 -4.4287853 -4.4287586 -4.4287643 -4.4287682 -4.4287491 -4.4287391 -4.4287663 -4.4288235 -4.4288731 -4.4288974][-4.4288688 -4.428885 -4.4288855 -4.4288683 -4.4288487 -4.4288249 -4.4288058 -4.4288096 -4.4288106 -4.4287925 -4.4287872 -4.4288144 -4.4288597 -4.42889 -4.4289021][-4.4289079 -4.4289165 -4.4289041 -4.4288826 -4.4288664 -4.428853 -4.42884 -4.4288344 -4.4288263 -4.428813 -4.4288163 -4.4288392 -4.4288731 -4.4288917 -4.428894]]...]
INFO - root - 2017-12-08 07:02:38.690011: step 34410, loss = 2.28, batch loss = 2.23 (33.1 examples/sec; 0.241 sec/batch; 19h:59m:12s remains)
INFO - root - 2017-12-08 07:02:40.926669: step 34420, loss = 2.28, batch loss = 2.23 (33.5 examples/sec; 0.239 sec/batch; 19h:46m:30s remains)
INFO - root - 2017-12-08 07:02:43.186565: step 34430, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 18h:09m:27s remains)
INFO - root - 2017-12-08 07:02:45.409412: step 34440, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.220 sec/batch; 18h:13m:29s remains)
INFO - root - 2017-12-08 07:02:47.638342: step 34450, loss = 2.28, batch loss = 2.23 (35.8 examples/sec; 0.223 sec/batch; 18h:29m:42s remains)
INFO - root - 2017-12-08 07:02:49.899202: step 34460, loss = 2.28, batch loss = 2.23 (33.1 examples/sec; 0.242 sec/batch; 20h:01m:24s remains)
INFO - root - 2017-12-08 07:02:52.161346: step 34470, loss = 2.28, batch loss = 2.23 (35.3 examples/sec; 0.227 sec/batch; 18h:45m:56s remains)
INFO - root - 2017-12-08 07:02:54.417293: step 34480, loss = 2.28, batch loss = 2.23 (36.0 examples/sec; 0.222 sec/batch; 18h:22m:49s remains)
INFO - root - 2017-12-08 07:02:56.652602: step 34490, loss = 2.28, batch loss = 2.23 (36.0 examples/sec; 0.222 sec/batch; 18h:22m:22s remains)
INFO - root - 2017-12-08 07:02:58.886893: step 34500, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.222 sec/batch; 18h:21m:51s remains)
2017-12-08 07:02:59.180727: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289393 -4.4288764 -4.4287977 -4.4287448 -4.4287105 -4.42868 -4.4286904 -4.4287386 -4.4287767 -4.4287987 -4.4287939 -4.4287615 -4.4287004 -4.4286408 -4.4286022][-4.4289513 -4.4288836 -4.4287949 -4.4287262 -4.4286766 -4.42864 -4.4286385 -4.4286695 -4.4287095 -4.4287567 -4.4287848 -4.4287891 -4.4287653 -4.4287291 -4.4286947][-4.4289589 -4.4288917 -4.4287949 -4.4287167 -4.4286561 -4.4286222 -4.4286122 -4.4286127 -4.4286385 -4.4286885 -4.4287386 -4.4287753 -4.4287896 -4.428782 -4.4287639][-4.4289594 -4.4288921 -4.4287915 -4.4287062 -4.4286413 -4.4286127 -4.4286032 -4.4285827 -4.4285851 -4.4286237 -4.4286852 -4.4287395 -4.4287782 -4.42879 -4.4287906][-4.4289484 -4.4288745 -4.4287667 -4.4286704 -4.4285922 -4.4285622 -4.4285603 -4.4285421 -4.4285421 -4.4285803 -4.4286466 -4.4287043 -4.4287524 -4.4287782 -4.428793][-4.4289274 -4.4288421 -4.428721 -4.4285989 -4.4284859 -4.4284306 -4.4284239 -4.4284253 -4.4284592 -4.4285312 -4.4286132 -4.428678 -4.428731 -4.4287591 -4.4287772][-4.4288969 -4.4287968 -4.4286571 -4.4284997 -4.42833 -4.4282122 -4.4281754 -4.4282017 -4.4282918 -4.4284182 -4.4285331 -4.4286203 -4.4286828 -4.4287152 -4.4287357][-4.4288712 -4.42876 -4.4286027 -4.4284163 -4.4281969 -4.4280152 -4.4279456 -4.4279962 -4.4281425 -4.4283185 -4.4284711 -4.4285865 -4.4286566 -4.4286885 -4.4287076][-4.4288716 -4.4287634 -4.4286094 -4.4284215 -4.428195 -4.4280019 -4.4279313 -4.4280028 -4.4281673 -4.4283519 -4.4285111 -4.4286318 -4.4286976 -4.42872 -4.4287281][-4.4288955 -4.42881 -4.4286852 -4.4285288 -4.4283414 -4.4281859 -4.42814 -4.428215 -4.4283576 -4.4285097 -4.4286423 -4.4287391 -4.42879 -4.4288034 -4.4288006][-4.4289351 -4.4288764 -4.4287839 -4.4286714 -4.4285417 -4.4284439 -4.42843 -4.4284949 -4.428597 -4.4287028 -4.4287953 -4.4288611 -4.428895 -4.4288979 -4.4288845][-4.4289622 -4.4289269 -4.428865 -4.4287953 -4.4287267 -4.4286857 -4.4286904 -4.428731 -4.4287887 -4.4288497 -4.428905 -4.42894 -4.4289546 -4.4289494 -4.4289336][-4.4289684 -4.4289489 -4.4289131 -4.4288774 -4.4288497 -4.4288406 -4.4288535 -4.4288721 -4.4288921 -4.4289126 -4.4289365 -4.42895 -4.4289575 -4.4289541 -4.4289446][-4.4289589 -4.4289484 -4.4289269 -4.4289088 -4.4289 -4.4289021 -4.4289126 -4.4289174 -4.428915 -4.4289131 -4.4289203 -4.4289274 -4.4289355 -4.4289393 -4.4289374][-4.4289389 -4.4289308 -4.428915 -4.4289021 -4.4289002 -4.4289036 -4.4289122 -4.4289117 -4.4289017 -4.4288912 -4.4288921 -4.4288969 -4.4289041 -4.4289122 -4.4289179]]...]
INFO - root - 2017-12-08 07:03:01.390922: step 34510, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 18h:07m:09s remains)
INFO - root - 2017-12-08 07:03:03.662281: step 34520, loss = 2.28, batch loss = 2.23 (35.4 examples/sec; 0.226 sec/batch; 18h:42m:59s remains)
INFO - root - 2017-12-08 07:03:05.908667: step 34530, loss = 2.28, batch loss = 2.23 (35.3 examples/sec; 0.227 sec/batch; 18h:46m:16s remains)
INFO - root - 2017-12-08 07:03:08.132437: step 34540, loss = 2.28, batch loss = 2.23 (35.2 examples/sec; 0.227 sec/batch; 18h:47m:23s remains)
INFO - root - 2017-12-08 07:03:10.343769: step 34550, loss = 2.28, batch loss = 2.23 (37.1 examples/sec; 0.216 sec/batch; 17h:52m:09s remains)
INFO - root - 2017-12-08 07:03:12.562944: step 34560, loss = 2.28, batch loss = 2.23 (35.2 examples/sec; 0.228 sec/batch; 18h:49m:59s remains)
INFO - root - 2017-12-08 07:03:14.790215: step 34570, loss = 2.28, batch loss = 2.23 (35.1 examples/sec; 0.228 sec/batch; 18h:50m:44s remains)
INFO - root - 2017-12-08 07:03:17.034788: step 34580, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.220 sec/batch; 18h:13m:01s remains)
INFO - root - 2017-12-08 07:03:19.276272: step 34590, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.221 sec/batch; 18h:15m:24s remains)
INFO - root - 2017-12-08 07:03:21.578240: step 34600, loss = 2.28, batch loss = 2.23 (35.8 examples/sec; 0.224 sec/batch; 18h:30m:07s remains)
2017-12-08 07:03:21.886714: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288836 -4.4288912 -4.4288988 -4.428906 -4.4289041 -4.4289026 -4.4289055 -4.4289155 -4.42894 -4.4289613 -4.4289584 -4.4289365 -4.4289131 -4.4288969 -4.4288855][-4.428895 -4.42891 -4.4289203 -4.4289308 -4.4289279 -4.42892 -4.4289083 -4.4289069 -4.42892 -4.4289303 -4.4289246 -4.4289188 -4.42891 -4.4288883 -4.4288554][-4.428916 -4.4289408 -4.4289532 -4.42896 -4.4289494 -4.4289279 -4.4288931 -4.4288707 -4.428875 -4.4288836 -4.4288821 -4.4288888 -4.4288864 -4.4288511 -4.4287834][-4.42895 -4.4289737 -4.4289794 -4.4289756 -4.4289536 -4.4289117 -4.4288478 -4.4287987 -4.4287949 -4.4288082 -4.4288087 -4.4288259 -4.428833 -4.4287829 -4.4286742][-4.42898 -4.4289994 -4.4289961 -4.4289742 -4.4289231 -4.4288435 -4.4287314 -4.4286427 -4.42863 -4.4286585 -4.4286828 -4.4287314 -4.4287643 -4.4287205 -4.4285932][-4.4289947 -4.429008 -4.4289894 -4.4289331 -4.4288368 -4.4287033 -4.4285231 -4.4283752 -4.4283609 -4.4284506 -4.428546 -4.4286513 -4.4287205 -4.4287066 -4.4286146][-4.4289794 -4.4289832 -4.4289503 -4.4288669 -4.4287281 -4.4285431 -4.4283018 -4.4281125 -4.4281321 -4.4283233 -4.4285035 -4.428647 -4.4287381 -4.42876 -4.4287229][-4.4289322 -4.4289289 -4.428895 -4.4288077 -4.4286594 -4.4284668 -4.4282374 -4.4280825 -4.42815 -4.4283857 -4.4285865 -4.4287181 -4.4287925 -4.4288182 -4.4288087][-4.4288559 -4.4288459 -4.4288254 -4.42877 -4.428669 -4.4285474 -4.4284143 -4.4283319 -4.4283953 -4.4285612 -4.4287009 -4.4287853 -4.4288259 -4.4288359 -4.428833][-4.4287772 -4.4287596 -4.4287596 -4.4287534 -4.4287229 -4.4286847 -4.4286337 -4.4285893 -4.4286151 -4.4286966 -4.4287729 -4.4288182 -4.428833 -4.4288349 -4.4288416][-4.4287357 -4.4287028 -4.4287038 -4.4287271 -4.4287415 -4.4287567 -4.4287496 -4.42873 -4.4287381 -4.4287744 -4.4288154 -4.4288416 -4.4288492 -4.4288516 -4.4288659][-4.4287419 -4.428689 -4.4286766 -4.4287019 -4.4287381 -4.4287834 -4.4288158 -4.4288263 -4.4288349 -4.4288535 -4.4288754 -4.4288936 -4.4289017 -4.428905 -4.4289155][-4.4287777 -4.4287171 -4.4286962 -4.4287157 -4.4287672 -4.428834 -4.4288878 -4.428915 -4.4289269 -4.4289331 -4.4289365 -4.4289412 -4.4289403 -4.428936 -4.4289384][-4.4288406 -4.4287968 -4.428782 -4.4287944 -4.4288363 -4.4288898 -4.428936 -4.428967 -4.4289751 -4.428967 -4.4289546 -4.4289479 -4.4289389 -4.4289269 -4.4289203][-4.4289126 -4.4288912 -4.4288821 -4.4288774 -4.4288878 -4.4289103 -4.4289374 -4.4289575 -4.4289565 -4.4289384 -4.4289169 -4.4289055 -4.4288964 -4.4288859 -4.4288769]]...]
INFO - root - 2017-12-08 07:03:24.160133: step 34610, loss = 2.28, batch loss = 2.23 (33.3 examples/sec; 0.240 sec/batch; 19h:50m:58s remains)
INFO - root - 2017-12-08 07:03:26.383446: step 34620, loss = 2.28, batch loss = 2.23 (37.3 examples/sec; 0.214 sec/batch; 17h:44m:28s remains)
INFO - root - 2017-12-08 07:03:28.635269: step 34630, loss = 2.28, batch loss = 2.23 (35.3 examples/sec; 0.226 sec/batch; 18h:44m:06s remains)
INFO - root - 2017-12-08 07:03:30.902442: step 34640, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 18h:26m:20s remains)
INFO - root - 2017-12-08 07:03:33.141227: step 34650, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.225 sec/batch; 18h:38m:07s remains)
INFO - root - 2017-12-08 07:03:35.417083: step 34660, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.222 sec/batch; 18h:20m:31s remains)
INFO - root - 2017-12-08 07:03:37.680211: step 34670, loss = 2.28, batch loss = 2.23 (35.4 examples/sec; 0.226 sec/batch; 18h:40m:24s remains)
INFO - root - 2017-12-08 07:03:39.926515: step 34680, loss = 2.28, batch loss = 2.23 (35.7 examples/sec; 0.224 sec/batch; 18h:32m:37s remains)
INFO - root - 2017-12-08 07:03:42.157730: step 34690, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.226 sec/batch; 18h:39m:18s remains)
INFO - root - 2017-12-08 07:03:44.389693: step 34700, loss = 2.28, batch loss = 2.23 (34.9 examples/sec; 0.230 sec/batch; 18h:59m:13s remains)
2017-12-08 07:03:44.705476: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288425 -4.4287658 -4.4287066 -4.4286871 -4.4286838 -4.4286819 -4.4286981 -4.4287472 -4.4287982 -4.4288278 -4.4288411 -4.4288397 -4.4288349 -4.4288392 -4.4288521][-4.4288054 -4.4287043 -4.4286275 -4.4286036 -4.4286022 -4.4286032 -4.4286232 -4.428689 -4.4287658 -4.4288135 -4.4288349 -4.4288316 -4.4288216 -4.4288244 -4.4288373][-4.4287724 -4.4286556 -4.4285631 -4.4285355 -4.4285398 -4.428545 -4.4285707 -4.4286556 -4.4287434 -4.428793 -4.4288182 -4.4288211 -4.42882 -4.4288316 -4.4288435][-4.4287386 -4.428607 -4.4285021 -4.4284716 -4.42849 -4.4285083 -4.42854 -4.4286351 -4.42872 -4.4287519 -4.4287639 -4.4287648 -4.4287648 -4.4287739 -4.4287848][-4.4287233 -4.4285841 -4.4284759 -4.4284434 -4.4284658 -4.4284754 -4.4284868 -4.4285812 -4.4286656 -4.4286838 -4.4286828 -4.4286761 -4.4286666 -4.428668 -4.4286828][-4.4287252 -4.4285841 -4.4284716 -4.4284277 -4.4284296 -4.4284024 -4.428371 -4.4284582 -4.4285536 -4.4285793 -4.4285793 -4.4285693 -4.4285555 -4.428555 -4.4285727][-4.4287148 -4.4285669 -4.4284391 -4.4283624 -4.428318 -4.4282293 -4.4281349 -4.4282351 -4.4283786 -4.4284434 -4.4284596 -4.4284506 -4.42843 -4.4284253 -4.4284487][-4.4286942 -4.4285378 -4.4283967 -4.4282951 -4.4282055 -4.4280467 -4.4278727 -4.4279852 -4.428194 -4.4283075 -4.42834 -4.4283414 -4.4283261 -4.428329 -4.4283619][-4.4286814 -4.4285235 -4.4283905 -4.428298 -4.4282093 -4.4280539 -4.4278789 -4.4279809 -4.42818 -4.4282856 -4.4283133 -4.4283252 -4.4283252 -4.4283419 -4.4283776][-4.4286847 -4.4285445 -4.4284334 -4.4283686 -4.4283142 -4.4282131 -4.4281039 -4.4281864 -4.4283252 -4.4283824 -4.4283862 -4.4284043 -4.4284215 -4.4284463 -4.4284763][-4.4287124 -4.4285994 -4.4285169 -4.4284763 -4.42845 -4.4283948 -4.4283357 -4.4283886 -4.4284673 -4.428483 -4.4284744 -4.4285021 -4.4285316 -4.4285541 -4.4285722][-4.4287591 -4.4286714 -4.428616 -4.4285922 -4.4285851 -4.4285631 -4.4285378 -4.4285669 -4.4286008 -4.4285908 -4.4285746 -4.4285984 -4.428627 -4.428638 -4.4286413][-4.428822 -4.428761 -4.4287257 -4.4287109 -4.4287119 -4.4287071 -4.4286966 -4.4287124 -4.4287248 -4.4287086 -4.4286914 -4.4287071 -4.4287243 -4.4287276 -4.4287262][-4.428885 -4.4288449 -4.4288206 -4.4288096 -4.4288116 -4.4288135 -4.4288106 -4.428822 -4.4288268 -4.4288163 -4.4288063 -4.4288111 -4.4288168 -4.4288177 -4.4288177][-4.4289351 -4.4289103 -4.428894 -4.4288855 -4.4288855 -4.4288874 -4.42889 -4.4289012 -4.4289045 -4.4288983 -4.428894 -4.428895 -4.428895 -4.4288926 -4.4288912]]...]
INFO - root - 2017-12-08 07:03:46.942677: step 34710, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.222 sec/batch; 18h:19m:55s remains)
INFO - root - 2017-12-08 07:03:49.193127: step 34720, loss = 2.28, batch loss = 2.23 (35.3 examples/sec; 0.227 sec/batch; 18h:45m:35s remains)
INFO - root - 2017-12-08 07:03:51.446932: step 34730, loss = 2.28, batch loss = 2.23 (35.0 examples/sec; 0.229 sec/batch; 18h:54m:11s remains)
INFO - root - 2017-12-08 07:03:53.704051: step 34740, loss = 2.28, batch loss = 2.23 (36.0 examples/sec; 0.222 sec/batch; 18h:21m:35s remains)
INFO - root - 2017-12-08 07:03:55.939308: step 34750, loss = 2.28, batch loss = 2.23 (35.2 examples/sec; 0.227 sec/batch; 18h:48m:37s remains)
INFO - root - 2017-12-08 07:03:58.188766: step 34760, loss = 2.28, batch loss = 2.23 (35.8 examples/sec; 0.223 sec/batch; 18h:28m:56s remains)
INFO - root - 2017-12-08 07:04:00.441361: step 34770, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 18h:11m:42s remains)
INFO - root - 2017-12-08 07:04:02.706050: step 34780, loss = 2.28, batch loss = 2.23 (34.0 examples/sec; 0.235 sec/batch; 19h:27m:59s remains)
INFO - root - 2017-12-08 07:04:04.964588: step 34790, loss = 2.28, batch loss = 2.23 (35.4 examples/sec; 0.226 sec/batch; 18h:40m:27s remains)
INFO - root - 2017-12-08 07:04:07.201611: step 34800, loss = 2.28, batch loss = 2.23 (35.0 examples/sec; 0.229 sec/batch; 18h:53m:56s remains)
2017-12-08 07:04:07.494313: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4286184 -4.42862 -4.428596 -4.4285564 -4.4285345 -4.4285536 -4.4285913 -4.4285932 -4.4286013 -4.4286509 -4.4287109 -4.4287462 -4.4287462 -4.4287071 -4.42867][-4.4285808 -4.4286003 -4.4285827 -4.4285393 -4.4285159 -4.4285488 -4.428607 -4.4286385 -4.4286628 -4.4287062 -4.4287405 -4.4287539 -4.4287353 -4.42867 -4.4286127][-4.42864 -4.4286551 -4.4286356 -4.4285908 -4.428565 -4.428587 -4.4286375 -4.4286842 -4.4287243 -4.428771 -4.4287963 -4.4287944 -4.4287577 -4.4286885 -4.428638][-4.4287271 -4.4287319 -4.4287052 -4.4286571 -4.4286189 -4.4286103 -4.4286318 -4.42867 -4.4287291 -4.4287953 -4.4288273 -4.4288268 -4.4287915 -4.4287376 -4.4286995][-4.4287639 -4.4287634 -4.4287305 -4.4286728 -4.42861 -4.4285617 -4.4285436 -4.4285831 -4.4286761 -4.4287705 -4.4288263 -4.4288383 -4.4288154 -4.4287758 -4.4287415][-4.4287453 -4.4287386 -4.4286995 -4.4286318 -4.4285407 -4.4284353 -4.4283714 -4.4284339 -4.4285831 -4.4287148 -4.428791 -4.4288154 -4.4288125 -4.4287887 -4.4287543][-4.4286809 -4.4286556 -4.428607 -4.4285321 -4.4284005 -4.4282112 -4.4280982 -4.4282284 -4.4284539 -4.4286218 -4.4287167 -4.428762 -4.4287829 -4.4287796 -4.428751][-4.4286318 -4.4285822 -4.4285321 -4.4284625 -4.4283071 -4.4280686 -4.4279552 -4.4281383 -4.4283881 -4.4285564 -4.4286556 -4.4287162 -4.4287529 -4.4287634 -4.4287438][-4.4286461 -4.4285827 -4.4285474 -4.4285121 -4.4283915 -4.4282074 -4.4281449 -4.4282813 -4.4284582 -4.4285784 -4.4286585 -4.4287195 -4.4287615 -4.42877 -4.42875][-4.4286523 -4.4285831 -4.4285617 -4.42856 -4.4284949 -4.4283886 -4.4283595 -4.4284368 -4.4285398 -4.4286189 -4.4286876 -4.4287453 -4.428781 -4.4287772 -4.4287496][-4.4286385 -4.4285684 -4.4285564 -4.4285684 -4.42854 -4.4284892 -4.428483 -4.4285316 -4.4285951 -4.4286556 -4.428719 -4.4287696 -4.4287944 -4.428782 -4.4287534][-4.4286547 -4.4285903 -4.4285722 -4.4285812 -4.4285741 -4.428555 -4.4285607 -4.428597 -4.4286423 -4.4286895 -4.4287415 -4.4287839 -4.4287996 -4.4287858 -4.4287629][-4.4286985 -4.4286413 -4.4286127 -4.4286108 -4.4286103 -4.4286094 -4.4286237 -4.4286547 -4.4286895 -4.4287238 -4.428761 -4.4287906 -4.4288 -4.4287939 -4.4287834][-4.42876 -4.4287128 -4.4286747 -4.4286604 -4.4286609 -4.4286714 -4.4286938 -4.4287224 -4.4287491 -4.428772 -4.4287944 -4.4288082 -4.428812 -4.4288139 -4.4288154][-4.4288173 -4.4287896 -4.428761 -4.4287453 -4.4287453 -4.4287558 -4.4287739 -4.42879 -4.4288039 -4.4288154 -4.4288268 -4.428834 -4.4288378 -4.428843 -4.4288483]]...]
INFO - root - 2017-12-08 07:04:09.771065: step 34810, loss = 2.28, batch loss = 2.23 (34.0 examples/sec; 0.235 sec/batch; 19h:28m:23s remains)
INFO - root - 2017-12-08 07:04:11.991521: step 34820, loss = 2.28, batch loss = 2.23 (38.0 examples/sec; 0.211 sec/batch; 17h:25m:20s remains)
INFO - root - 2017-12-08 07:04:14.244784: step 34830, loss = 2.28, batch loss = 2.23 (35.3 examples/sec; 0.227 sec/batch; 18h:44m:36s remains)
INFO - root - 2017-12-08 07:04:16.481291: step 34840, loss = 2.28, batch loss = 2.23 (37.7 examples/sec; 0.212 sec/batch; 17h:33m:09s remains)
INFO - root - 2017-12-08 07:04:18.737427: step 34850, loss = 2.28, batch loss = 2.23 (35.6 examples/sec; 0.225 sec/batch; 18h:34m:02s remains)
INFO - root - 2017-12-08 07:04:20.996263: step 34860, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.225 sec/batch; 18h:38m:02s remains)
INFO - root - 2017-12-08 07:04:23.261578: step 34870, loss = 2.28, batch loss = 2.23 (35.2 examples/sec; 0.227 sec/batch; 18h:46m:05s remains)
INFO - root - 2017-12-08 07:04:25.497360: step 34880, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 18h:16m:43s remains)
INFO - root - 2017-12-08 07:04:27.751449: step 34890, loss = 2.28, batch loss = 2.23 (34.6 examples/sec; 0.231 sec/batch; 19h:07m:58s remains)
INFO - root - 2017-12-08 07:04:30.000967: step 34900, loss = 2.28, batch loss = 2.23 (35.3 examples/sec; 0.226 sec/batch; 18h:43m:04s remains)
2017-12-08 07:04:30.297436: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4287453 -4.4286761 -4.4285951 -4.4285665 -4.4286146 -4.4286914 -4.4287305 -4.4287071 -4.4286056 -4.4284863 -4.428443 -4.4285064 -4.428596 -4.42867 -4.4287529][-4.4287977 -4.4287286 -4.428637 -4.428596 -4.4286394 -4.4287095 -4.4287491 -4.4287271 -4.4286261 -4.4285078 -4.4284606 -4.4285164 -4.428607 -4.4286938 -4.4287887][-4.4288116 -4.4287381 -4.4286437 -4.4285927 -4.4286208 -4.4286842 -4.4287348 -4.4287372 -4.4286666 -4.428576 -4.4285364 -4.4285755 -4.428658 -4.4287486 -4.428844][-4.4287853 -4.4287028 -4.42861 -4.4285474 -4.4285564 -4.4286151 -4.428688 -4.4287276 -4.4287047 -4.4286542 -4.4286261 -4.4286432 -4.4287028 -4.4287815 -4.4288697][-4.4287658 -4.4286718 -4.428575 -4.4284987 -4.4284892 -4.4285431 -4.4286318 -4.4287043 -4.4287243 -4.4287081 -4.4286823 -4.4286733 -4.4287 -4.42876 -4.42884][-4.4287596 -4.4286594 -4.4285574 -4.4284706 -4.4284391 -4.428484 -4.4285822 -4.4286723 -4.428719 -4.4287224 -4.4286966 -4.4286671 -4.4286652 -4.4287009 -4.4287724][-4.4287677 -4.4286742 -4.4285793 -4.4284911 -4.4284387 -4.4284611 -4.4285502 -4.4286456 -4.4287024 -4.4287114 -4.4286852 -4.4286418 -4.42862 -4.4286342 -4.4287009][-4.4287839 -4.4287119 -4.4286394 -4.4285641 -4.428504 -4.4284945 -4.4285469 -4.4286242 -4.4286761 -4.4286852 -4.4286609 -4.42861 -4.4285755 -4.4285784 -4.4286466][-4.4287939 -4.4287462 -4.4287019 -4.4286566 -4.4286094 -4.4285774 -4.4285784 -4.428617 -4.4286547 -4.4286585 -4.4286318 -4.4285836 -4.4285488 -4.428555 -4.42863][-4.428771 -4.4287448 -4.4287281 -4.4287219 -4.4287019 -4.4286637 -4.4286265 -4.4286275 -4.4286504 -4.4286494 -4.4286256 -4.4285922 -4.4285731 -4.4285855 -4.4286523][-4.4287124 -4.4287047 -4.4287138 -4.4287434 -4.4287539 -4.4287176 -4.4286656 -4.4286494 -4.4286704 -4.4286776 -4.4286723 -4.4286637 -4.4286618 -4.4286728 -4.4287167][-4.4286437 -4.4286518 -4.4286871 -4.4287391 -4.4287705 -4.4287419 -4.4286895 -4.4286728 -4.4287004 -4.4287267 -4.4287453 -4.4287629 -4.4287739 -4.4287777 -4.42879][-4.4285941 -4.428617 -4.4286723 -4.4287391 -4.4287786 -4.4287529 -4.4287014 -4.428689 -4.4287238 -4.4287658 -4.4287996 -4.4288325 -4.4288526 -4.4288535 -4.4288435][-4.4285879 -4.4286137 -4.4286752 -4.4287434 -4.4287744 -4.4287448 -4.4286962 -4.4286919 -4.428741 -4.4287958 -4.4288349 -4.4288707 -4.4288907 -4.4288859 -4.428863][-4.42862 -4.4286375 -4.4286885 -4.428741 -4.4287529 -4.4287152 -4.4286747 -4.4286885 -4.4287558 -4.4288154 -4.4288511 -4.4288836 -4.4288974 -4.4288845 -4.4288516]]...]
INFO - root - 2017-12-08 07:04:32.507960: step 34910, loss = 2.28, batch loss = 2.23 (35.2 examples/sec; 0.228 sec/batch; 18h:48m:25s remains)
INFO - root - 2017-12-08 07:04:34.773285: step 34920, loss = 2.28, batch loss = 2.23 (35.6 examples/sec; 0.224 sec/batch; 18h:33m:14s remains)
INFO - root - 2017-12-08 07:04:37.029579: step 34930, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.220 sec/batch; 18h:12m:08s remains)
INFO - root - 2017-12-08 07:04:39.272864: step 34940, loss = 2.28, batch loss = 2.23 (35.8 examples/sec; 0.224 sec/batch; 18h:29m:26s remains)
INFO - root - 2017-12-08 07:04:41.522283: step 34950, loss = 2.28, batch loss = 2.23 (35.1 examples/sec; 0.228 sec/batch; 18h:50m:29s remains)
INFO - root - 2017-12-08 07:04:43.749810: step 34960, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.225 sec/batch; 18h:36m:03s remains)
INFO - root - 2017-12-08 07:04:45.968996: step 34970, loss = 2.28, batch loss = 2.23 (34.6 examples/sec; 0.231 sec/batch; 19h:07m:10s remains)
INFO - root - 2017-12-08 07:04:48.189274: step 34980, loss = 2.28, batch loss = 2.23 (35.0 examples/sec; 0.228 sec/batch; 18h:51m:57s remains)
INFO - root - 2017-12-08 07:04:50.412178: step 34990, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 18h:25m:48s remains)
INFO - root - 2017-12-08 07:04:52.653836: step 35000, loss = 2.28, batch loss = 2.23 (34.1 examples/sec; 0.235 sec/batch; 19h:23m:14s remains)
2017-12-08 07:04:52.943180: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4290295 -4.4290366 -4.4290366 -4.4290314 -4.4290242 -4.4290185 -4.429008 -4.4289937 -4.4289832 -4.4289865 -4.4290032 -4.429019 -4.4290237 -4.4290066 -4.4289584][-4.429029 -4.4290333 -4.429029 -4.42902 -4.42901 -4.428998 -4.4289761 -4.4289503 -4.4289303 -4.4289365 -4.4289618 -4.4289885 -4.429 -4.4289832 -4.4289193][-4.4290276 -4.429028 -4.4290214 -4.4290075 -4.4289875 -4.4289584 -4.4289141 -4.4288721 -4.4288449 -4.4288545 -4.4288912 -4.4289341 -4.4289556 -4.4289389 -4.4288626][-4.429028 -4.4290247 -4.4290137 -4.428988 -4.42895 -4.4288931 -4.4288197 -4.42876 -4.4287271 -4.4287467 -4.4287992 -4.4288607 -4.428896 -4.4288898 -4.4288144][-4.4290333 -4.4290237 -4.4290018 -4.4289556 -4.4288926 -4.428802 -4.4286962 -4.4286146 -4.4285936 -4.4286447 -4.4287257 -4.4288077 -4.4288578 -4.42887 -4.4288054][-4.42903 -4.4290152 -4.4289808 -4.4289103 -4.4288135 -4.4286785 -4.4285278 -4.4284263 -4.4284406 -4.428544 -4.4286656 -4.4287715 -4.4288406 -4.4288745 -4.4288268][-4.4290113 -4.4289951 -4.4289494 -4.4288507 -4.4287124 -4.42853 -4.4283323 -4.42823 -4.4283051 -4.4284682 -4.42863 -4.4287591 -4.4288368 -4.4288774 -4.428844][-4.4289746 -4.4289579 -4.4288983 -4.4287739 -4.4286036 -4.4283795 -4.42815 -4.4280863 -4.4282365 -4.4284458 -4.4286284 -4.4287572 -4.4288244 -4.4288516 -4.4288306][-4.42892 -4.4289069 -4.4288411 -4.4287 -4.4285126 -4.4282804 -4.4280739 -4.4280772 -4.4282875 -4.4285154 -4.428678 -4.4287596 -4.4287906 -4.428803 -4.4287996][-4.4288712 -4.428875 -4.4288158 -4.4286737 -4.4285026 -4.4283242 -4.4281926 -4.4282446 -4.4284534 -4.4286485 -4.4287529 -4.4287672 -4.4287553 -4.4287605 -4.4287815][-4.4288483 -4.4288769 -4.4288316 -4.4287105 -4.4285846 -4.4284883 -4.4284306 -4.4284883 -4.4286361 -4.4287539 -4.4287925 -4.4287634 -4.4287343 -4.4287486 -4.4287982][-4.4288473 -4.4288926 -4.4288626 -4.428772 -4.4286957 -4.4286604 -4.428648 -4.4286919 -4.4287691 -4.4288073 -4.4287934 -4.4287567 -4.4287443 -4.428782 -4.42885][-4.4288497 -4.4288926 -4.4288745 -4.4288187 -4.4287887 -4.4287949 -4.4288063 -4.4288254 -4.4288354 -4.4288111 -4.428772 -4.4287534 -4.4287782 -4.4288416 -4.4289145][-4.4288363 -4.4288626 -4.4288549 -4.4288335 -4.4288435 -4.428875 -4.4288845 -4.4288669 -4.4288278 -4.4287767 -4.4287477 -4.4287658 -4.4288259 -4.4289021 -4.4289651][-4.4288111 -4.42882 -4.4288249 -4.4288349 -4.4288678 -4.4288993 -4.4288869 -4.4288349 -4.4287744 -4.4287381 -4.4287477 -4.4288039 -4.4288816 -4.42895 -4.4289932]]...]
INFO - root - 2017-12-08 07:04:55.189411: step 35010, loss = 2.28, batch loss = 2.23 (36.0 examples/sec; 0.222 sec/batch; 18h:21m:33s remains)
INFO - root - 2017-12-08 07:04:57.425790: step 35020, loss = 2.28, batch loss = 2.23 (35.4 examples/sec; 0.226 sec/batch; 18h:39m:43s remains)
INFO - root - 2017-12-08 07:04:59.674347: step 35030, loss = 2.28, batch loss = 2.23 (35.1 examples/sec; 0.228 sec/batch; 18h:49m:32s remains)
INFO - root - 2017-12-08 07:05:01.906998: step 35040, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.221 sec/batch; 18h:13m:13s remains)
INFO - root - 2017-12-08 07:05:04.151369: step 35050, loss = 2.28, batch loss = 2.23 (35.3 examples/sec; 0.227 sec/batch; 18h:43m:14s remains)
INFO - root - 2017-12-08 07:05:06.384047: step 35060, loss = 2.28, batch loss = 2.23 (36.6 examples/sec; 0.219 sec/batch; 18h:04m:21s remains)
INFO - root - 2017-12-08 07:05:08.620371: step 35070, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.221 sec/batch; 18h:13m:44s remains)
INFO - root - 2017-12-08 07:05:10.849645: step 35080, loss = 2.28, batch loss = 2.23 (35.2 examples/sec; 0.227 sec/batch; 18h:47m:02s remains)
INFO - root - 2017-12-08 07:05:13.068836: step 35090, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.221 sec/batch; 18h:16m:59s remains)
INFO - root - 2017-12-08 07:05:15.307742: step 35100, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 18h:24m:23s remains)
2017-12-08 07:05:15.588288: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288197 -4.4287782 -4.4287562 -4.4287491 -4.4287429 -4.4287353 -4.4287443 -4.4287653 -4.4287834 -4.4288073 -4.42884 -4.4288616 -4.4288588 -4.4288521 -4.4288621][-4.4288468 -4.4288054 -4.42878 -4.4287758 -4.4287777 -4.4287786 -4.428791 -4.4288111 -4.4288311 -4.4288578 -4.4288878 -4.428905 -4.4288974 -4.4288878 -4.4288893][-4.4288549 -4.4288163 -4.4287939 -4.4287977 -4.4288106 -4.4288173 -4.4288259 -4.4288368 -4.4288511 -4.42888 -4.4289122 -4.4289274 -4.428915 -4.4289012 -4.428896][-4.428834 -4.4288 -4.4287844 -4.4287958 -4.4288149 -4.4288235 -4.4288292 -4.428833 -4.4288387 -4.42887 -4.42891 -4.4289284 -4.4289165 -4.4289017 -4.428895][-4.4287882 -4.4287548 -4.428741 -4.4287515 -4.4287672 -4.4287791 -4.428793 -4.4287958 -4.4287968 -4.4288321 -4.4288821 -4.42891 -4.4289055 -4.4288969 -4.4288936][-4.4287248 -4.4286842 -4.4286633 -4.4286647 -4.4286737 -4.4286923 -4.4287205 -4.4287271 -4.4287276 -4.4287648 -4.4288268 -4.4288669 -4.428875 -4.4288797 -4.4288859][-4.4286747 -4.428627 -4.4285951 -4.4285817 -4.4285808 -4.4286041 -4.4286404 -4.4286432 -4.4286366 -4.4286737 -4.4287491 -4.4288039 -4.4288249 -4.4288435 -4.4288621][-4.4286628 -4.428618 -4.4285822 -4.4285564 -4.4285383 -4.4285526 -4.428587 -4.4285884 -4.428575 -4.4286051 -4.4286885 -4.4287596 -4.4287949 -4.4288197 -4.4288406][-4.4286971 -4.4286618 -4.4286304 -4.4286027 -4.4285755 -4.4285822 -4.4286132 -4.428616 -4.4286022 -4.4286246 -4.4287047 -4.4287744 -4.4288116 -4.4288316 -4.428844][-4.4287572 -4.4287415 -4.4287257 -4.428709 -4.4286871 -4.428688 -4.4287086 -4.42871 -4.4287 -4.4287176 -4.4287825 -4.4288373 -4.4288621 -4.4288678 -4.428865][-4.428813 -4.4288163 -4.4288177 -4.4288168 -4.4288044 -4.4288025 -4.4288125 -4.4288139 -4.4288096 -4.4288278 -4.4288774 -4.4289122 -4.42892 -4.4289131 -4.4288983][-4.4288573 -4.4288683 -4.4288759 -4.4288816 -4.4288774 -4.4288759 -4.4288793 -4.4288797 -4.4288816 -4.4289002 -4.4289374 -4.428957 -4.4289556 -4.4289455 -4.4289265][-4.4288635 -4.4288769 -4.4288878 -4.4288955 -4.4288964 -4.4288926 -4.4288888 -4.4288878 -4.42889 -4.428906 -4.4289312 -4.4289455 -4.4289494 -4.4289503 -4.4289393][-4.4288416 -4.428854 -4.4288673 -4.428875 -4.4288764 -4.4288688 -4.4288573 -4.4288588 -4.4288664 -4.4288807 -4.4289 -4.4289141 -4.428926 -4.428937 -4.4289336][-4.4288154 -4.4288211 -4.4288273 -4.4288268 -4.428823 -4.4288044 -4.428781 -4.42878 -4.4287953 -4.42882 -4.4288435 -4.4288616 -4.428884 -4.4289055 -4.4289088]]...]
INFO - root - 2017-12-08 07:05:17.846926: step 35110, loss = 2.28, batch loss = 2.23 (34.2 examples/sec; 0.234 sec/batch; 19h:18m:09s remains)
INFO - root - 2017-12-08 07:05:20.104792: step 35120, loss = 2.28, batch loss = 2.23 (36.8 examples/sec; 0.217 sec/batch; 17h:56m:34s remains)
INFO - root - 2017-12-08 07:05:22.368135: step 35130, loss = 2.28, batch loss = 2.23 (34.8 examples/sec; 0.230 sec/batch; 18h:58m:07s remains)
INFO - root - 2017-12-08 07:05:24.651199: step 35140, loss = 2.28, batch loss = 2.23 (35.2 examples/sec; 0.227 sec/batch; 18h:45m:23s remains)
INFO - root - 2017-12-08 07:05:26.883433: step 35150, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.222 sec/batch; 18h:19m:03s remains)
INFO - root - 2017-12-08 07:05:29.106535: step 35160, loss = 2.28, batch loss = 2.23 (36.7 examples/sec; 0.218 sec/batch; 18h:00m:56s remains)
INFO - root - 2017-12-08 07:05:31.342331: step 35170, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 18h:08m:59s remains)
INFO - root - 2017-12-08 07:05:33.573373: step 35180, loss = 2.28, batch loss = 2.23 (34.9 examples/sec; 0.229 sec/batch; 18h:55m:13s remains)
INFO - root - 2017-12-08 07:05:35.800528: step 35190, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.221 sec/batch; 18h:16m:46s remains)
INFO - root - 2017-12-08 07:05:38.095201: step 35200, loss = 2.28, batch loss = 2.23 (35.1 examples/sec; 0.228 sec/batch; 18h:47m:59s remains)
2017-12-08 07:05:38.393750: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289317 -4.4289346 -4.4289322 -4.4289284 -4.4289227 -4.4289222 -4.4289231 -4.4289246 -4.4289284 -4.4289374 -4.4289517 -4.4289594 -4.42896 -4.4289589 -4.4289608][-4.4288955 -4.4288921 -4.4288826 -4.4288735 -4.4288688 -4.4288673 -4.4288654 -4.42887 -4.428885 -4.4289055 -4.4289274 -4.4289384 -4.428937 -4.4289355 -4.4289422][-4.4288187 -4.42881 -4.4287958 -4.4287829 -4.4287748 -4.4287715 -4.4287648 -4.4287753 -4.4288063 -4.4288392 -4.4288712 -4.4288878 -4.4288845 -4.4288807 -4.4288974][-4.4287219 -4.4287176 -4.4287009 -4.4286795 -4.4286585 -4.4286351 -4.4286051 -4.428616 -4.42867 -4.4287252 -4.4287686 -4.4287906 -4.4287877 -4.4287858 -4.428823][-4.4286737 -4.4286766 -4.4286561 -4.428617 -4.4285564 -4.4284797 -4.4284 -4.4284077 -4.428494 -4.4285808 -4.4286351 -4.4286609 -4.4286566 -4.4286613 -4.4287281][-4.428668 -4.42867 -4.428638 -4.4285712 -4.4284558 -4.4283042 -4.4281673 -4.42818 -4.4283166 -4.4284449 -4.4285088 -4.4285316 -4.4285288 -4.4285517 -4.42865][-4.4286714 -4.428668 -4.4286208 -4.4285231 -4.428359 -4.4281363 -4.4279566 -4.4280128 -4.42823 -4.4283996 -4.4284663 -4.42848 -4.428472 -4.4285026 -4.4286103][-4.4286814 -4.4286771 -4.4286232 -4.4285183 -4.4283447 -4.4281163 -4.4279604 -4.4280753 -4.4283247 -4.4284859 -4.42853 -4.4285231 -4.4284873 -4.4285016 -4.4286017][-4.428678 -4.4286852 -4.428658 -4.4285936 -4.4284754 -4.4283242 -4.4282408 -4.4283462 -4.4285192 -4.4286103 -4.4286027 -4.4285569 -4.4284906 -4.4284897 -4.4285889][-4.428678 -4.4287 -4.4286995 -4.4286728 -4.428618 -4.4285412 -4.4285045 -4.428575 -4.42866 -4.4286757 -4.4286194 -4.4285374 -4.4284506 -4.4284458 -4.4285555][-4.4286962 -4.4287205 -4.4287319 -4.4287262 -4.4287071 -4.4286766 -4.4286609 -4.4286957 -4.4287195 -4.4286819 -4.4285913 -4.4284849 -4.4283929 -4.4283972 -4.428524][-4.4287062 -4.4287205 -4.4287295 -4.428731 -4.4287333 -4.4287314 -4.4287267 -4.4287457 -4.42874 -4.428679 -4.4285812 -4.4284773 -4.4283991 -4.428412 -4.4285455][-4.4287324 -4.4287419 -4.4287424 -4.4287381 -4.4287405 -4.4287438 -4.42874 -4.4287481 -4.4287319 -4.4286785 -4.428607 -4.4285326 -4.4284725 -4.4284883 -4.4286127][-4.4287891 -4.4287791 -4.4287586 -4.4287357 -4.4287233 -4.4287219 -4.4287219 -4.4287281 -4.428719 -4.428688 -4.4286404 -4.4285841 -4.4285445 -4.4285722 -4.4286857][-4.4287992 -4.4287615 -4.4287224 -4.4286919 -4.4286842 -4.4286914 -4.4286985 -4.4287095 -4.4287133 -4.4286947 -4.4286575 -4.42862 -4.428607 -4.4286494 -4.4287519]]...]
INFO - root - 2017-12-08 07:05:40.621063: step 35210, loss = 2.28, batch loss = 2.23 (34.9 examples/sec; 0.229 sec/batch; 18h:54m:20s remains)
INFO - root - 2017-12-08 07:05:42.842628: step 35220, loss = 2.28, batch loss = 2.23 (36.0 examples/sec; 0.222 sec/batch; 18h:20m:28s remains)
INFO - root - 2017-12-08 07:05:45.076310: step 35230, loss = 2.28, batch loss = 2.23 (35.6 examples/sec; 0.225 sec/batch; 18h:32m:35s remains)
INFO - root - 2017-12-08 07:05:47.308551: step 35240, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.225 sec/batch; 18h:36m:07s remains)
INFO - root - 2017-12-08 07:05:49.544343: step 35250, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 18h:24m:50s remains)
INFO - root - 2017-12-08 07:05:51.792918: step 35260, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 18h:14m:26s remains)
INFO - root - 2017-12-08 07:05:54.089451: step 35270, loss = 2.28, batch loss = 2.23 (37.0 examples/sec; 0.216 sec/batch; 17h:51m:47s remains)
INFO - root - 2017-12-08 07:05:56.305763: step 35280, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 18h:05m:27s remains)
INFO - root - 2017-12-08 07:05:58.560272: step 35290, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.221 sec/batch; 18h:16m:20s remains)
INFO - root - 2017-12-08 07:06:00.855322: step 35300, loss = 2.28, batch loss = 2.23 (33.7 examples/sec; 0.238 sec/batch; 19h:37m:18s remains)
2017-12-08 07:06:01.134091: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288006 -4.4288135 -4.4288163 -4.4288058 -4.4287963 -4.42879 -4.4287777 -4.4287372 -4.4286895 -4.4286923 -4.4287343 -4.4287887 -4.4288549 -4.4289312 -4.4289889][-4.4288073 -4.4288116 -4.4288025 -4.42879 -4.4287796 -4.4287682 -4.4287429 -4.4286957 -4.4286489 -4.4286637 -4.4287195 -4.4287791 -4.4288459 -4.4289227 -4.4289804][-4.4288077 -4.4288011 -4.4287791 -4.4287605 -4.4287472 -4.4287286 -4.42869 -4.4286308 -4.4285774 -4.4285989 -4.4286742 -4.4287491 -4.4288244 -4.4289055 -4.4289651][-4.4287987 -4.4287872 -4.4287567 -4.4287319 -4.4287181 -4.4286976 -4.428658 -4.4285955 -4.4285293 -4.428546 -4.4286323 -4.428719 -4.4288011 -4.428884 -4.4289441][-4.4287715 -4.4287605 -4.4287271 -4.4287009 -4.4286904 -4.428679 -4.4286509 -4.4285941 -4.4285169 -4.4285212 -4.4286046 -4.4286914 -4.4287753 -4.4288573 -4.4289169][-4.4287271 -4.4287095 -4.4286709 -4.4286375 -4.4286294 -4.428637 -4.4286275 -4.4285789 -4.4284973 -4.428483 -4.4285531 -4.4286389 -4.4287295 -4.4288144 -4.42888][-4.42869 -4.4286656 -4.4286156 -4.4285617 -4.4285407 -4.4285526 -4.4285507 -4.4285069 -4.4284248 -4.4284 -4.4284635 -4.4285579 -4.4286628 -4.4287615 -4.4288383][-4.4286671 -4.4286323 -4.4285631 -4.4284782 -4.4284344 -4.4284406 -4.428443 -4.4284072 -4.4283333 -4.428308 -4.4283743 -4.4284821 -4.4286051 -4.42872 -4.4288063][-4.4286647 -4.4286256 -4.4285469 -4.4284453 -4.4283824 -4.4283805 -4.4283891 -4.42837 -4.4283156 -4.4283004 -4.4283648 -4.4284725 -4.4285955 -4.428709 -4.4287944][-4.428689 -4.4286633 -4.4285994 -4.4285183 -4.4284682 -4.4284682 -4.4284797 -4.4284706 -4.428432 -4.428422 -4.4284739 -4.4285607 -4.4286647 -4.4287596 -4.4288278][-4.4287128 -4.4287062 -4.428668 -4.4286203 -4.4285955 -4.4286013 -4.4286079 -4.4286013 -4.4285746 -4.4285679 -4.4286084 -4.4286795 -4.4287634 -4.4288363 -4.4288816][-4.4287176 -4.4287281 -4.4287138 -4.4286947 -4.428688 -4.4286923 -4.4286909 -4.4286809 -4.4286575 -4.4286489 -4.4286814 -4.4287443 -4.4288158 -4.428875 -4.4289103][-4.4286904 -4.4287119 -4.4287195 -4.4287257 -4.4287376 -4.4287472 -4.4287372 -4.4287167 -4.4286852 -4.4286718 -4.4287024 -4.4287663 -4.4288311 -4.428885 -4.4289231][-4.4286566 -4.4286809 -4.4287071 -4.4287348 -4.428762 -4.4287748 -4.4287581 -4.4287257 -4.4286866 -4.4286723 -4.4287052 -4.4287715 -4.428833 -4.4288859 -4.4289274][-4.4286556 -4.4286776 -4.4287109 -4.4287462 -4.4287758 -4.4287853 -4.428761 -4.4287214 -4.42868 -4.4286666 -4.4287038 -4.4287705 -4.4288273 -4.4288774 -4.4289184]]...]
INFO - root - 2017-12-08 07:06:03.379078: step 35310, loss = 2.28, batch loss = 2.23 (36.9 examples/sec; 0.217 sec/batch; 17h:54m:31s remains)
INFO - root - 2017-12-08 07:06:05.646155: step 35320, loss = 2.28, batch loss = 2.23 (35.3 examples/sec; 0.227 sec/batch; 18h:42m:40s remains)
INFO - root - 2017-12-08 07:06:07.873930: step 35330, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 18h:05m:57s remains)
INFO - root - 2017-12-08 07:06:10.142640: step 35340, loss = 2.28, batch loss = 2.23 (37.0 examples/sec; 0.216 sec/batch; 17h:50m:29s remains)
INFO - root - 2017-12-08 07:06:12.367355: step 35350, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.225 sec/batch; 18h:36m:22s remains)
INFO - root - 2017-12-08 07:06:14.603972: step 35360, loss = 2.28, batch loss = 2.23 (37.1 examples/sec; 0.216 sec/batch; 17h:47m:29s remains)
INFO - root - 2017-12-08 07:06:16.865507: step 35370, loss = 2.28, batch loss = 2.23 (33.1 examples/sec; 0.242 sec/batch; 19h:56m:12s remains)
INFO - root - 2017-12-08 07:06:19.079063: step 35380, loss = 2.28, batch loss = 2.23 (35.4 examples/sec; 0.226 sec/batch; 18h:37m:57s remains)
INFO - root - 2017-12-08 07:06:21.315688: step 35390, loss = 2.28, batch loss = 2.23 (36.8 examples/sec; 0.218 sec/batch; 17h:57m:51s remains)
INFO - root - 2017-12-08 07:06:23.538229: step 35400, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.225 sec/batch; 18h:36m:13s remains)
2017-12-08 07:06:23.825982: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289412 -4.4289608 -4.4289579 -4.4289289 -4.4288864 -4.4288473 -4.4288211 -4.4288 -4.428781 -4.4287577 -4.42875 -4.4287548 -4.4287724 -4.4288125 -4.4288516][-4.4289446 -4.4289618 -4.4289446 -4.4288945 -4.4288359 -4.4287815 -4.4287438 -4.4287276 -4.4287205 -4.4287081 -4.4287052 -4.4287105 -4.4287267 -4.4287691 -4.42881][-4.4289422 -4.4289551 -4.4289255 -4.4288597 -4.4287825 -4.4287062 -4.4286509 -4.428637 -4.4286485 -4.4286585 -4.4286594 -4.4286633 -4.4286823 -4.4287243 -4.4287624][-4.4289412 -4.4289489 -4.4289079 -4.4288306 -4.4287372 -4.4286418 -4.4285665 -4.4285445 -4.4285731 -4.42861 -4.4286232 -4.4286275 -4.4286485 -4.4286876 -4.4287248][-4.4289427 -4.4289484 -4.428905 -4.4288311 -4.4287314 -4.4286165 -4.4285121 -4.42847 -4.4285059 -4.428576 -4.4286208 -4.4286423 -4.4286633 -4.42869 -4.4287143][-4.4289412 -4.4289446 -4.428906 -4.4288383 -4.42873 -4.4285808 -4.4284282 -4.4283576 -4.4284167 -4.4285345 -4.4286294 -4.4286819 -4.4287105 -4.4287214 -4.4287214][-4.4289327 -4.4289312 -4.4288888 -4.4288139 -4.4286866 -4.4284935 -4.4282737 -4.4281716 -4.428288 -4.428473 -4.4286103 -4.4286842 -4.4287252 -4.4287329 -4.4287248][-4.4289217 -4.4289122 -4.4288568 -4.4287634 -4.4286089 -4.4283767 -4.4281168 -4.42804 -4.428247 -4.4284725 -4.4286065 -4.4286714 -4.4287047 -4.4287105 -4.4287105][-4.4289103 -4.428895 -4.428833 -4.4287314 -4.4285812 -4.4283929 -4.4282227 -4.4282174 -4.4283986 -4.4285641 -4.4286437 -4.4286737 -4.4286819 -4.4286814 -4.42869][-4.4289069 -4.4288888 -4.42883 -4.4287453 -4.4286432 -4.4285431 -4.4284697 -4.4284792 -4.4285812 -4.4286652 -4.4286919 -4.4286876 -4.4286737 -4.4286714 -4.4286942][-4.4289112 -4.4288945 -4.4288478 -4.4287887 -4.42873 -4.4286814 -4.4286513 -4.4286561 -4.4286995 -4.4287305 -4.4287271 -4.4287052 -4.4286823 -4.4286885 -4.4287224][-4.4289136 -4.4289017 -4.4288669 -4.4288297 -4.4288 -4.4287744 -4.4287567 -4.4287548 -4.4287729 -4.4287844 -4.4287729 -4.4287462 -4.4287271 -4.4287424 -4.4287753][-4.4289145 -4.4289112 -4.4288859 -4.42886 -4.428843 -4.4288244 -4.4288111 -4.4288096 -4.428822 -4.4288278 -4.4288077 -4.4287853 -4.4287724 -4.4287896 -4.4288168][-4.4289203 -4.4289265 -4.4289055 -4.4288764 -4.4288578 -4.4288411 -4.4288259 -4.4288239 -4.4288306 -4.4288335 -4.4288149 -4.4287982 -4.4287896 -4.4288063 -4.4288278][-4.4289255 -4.4289384 -4.4289179 -4.4288778 -4.4288478 -4.4288249 -4.4288025 -4.4287977 -4.4288039 -4.4288068 -4.4287906 -4.4287786 -4.428781 -4.4288034 -4.4288259]]...]
INFO - root - 2017-12-08 07:06:26.085696: step 35410, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 18h:03m:56s remains)
INFO - root - 2017-12-08 07:06:28.307115: step 35420, loss = 2.28, batch loss = 2.23 (37.2 examples/sec; 0.215 sec/batch; 17h:43m:37s remains)
INFO - root - 2017-12-08 07:06:30.536593: step 35430, loss = 2.28, batch loss = 2.23 (36.6 examples/sec; 0.219 sec/batch; 18h:01m:50s remains)
INFO - root - 2017-12-08 07:06:32.766279: step 35440, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 18h:23m:30s remains)
INFO - root - 2017-12-08 07:06:35.011392: step 35450, loss = 2.28, batch loss = 2.23 (35.6 examples/sec; 0.225 sec/batch; 18h:32m:25s remains)
INFO - root - 2017-12-08 07:06:37.257795: step 35460, loss = 2.28, batch loss = 2.23 (34.9 examples/sec; 0.230 sec/batch; 18h:56m:13s remains)
INFO - root - 2017-12-08 07:06:39.542396: step 35470, loss = 2.28, batch loss = 2.23 (32.2 examples/sec; 0.248 sec/batch; 20h:29m:43s remains)
INFO - root - 2017-12-08 07:06:41.804522: step 35480, loss = 2.28, batch loss = 2.23 (34.9 examples/sec; 0.229 sec/batch; 18h:53m:15s remains)
INFO - root - 2017-12-08 07:06:44.066386: step 35490, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 18h:08m:36s remains)
INFO - root - 2017-12-08 07:06:46.297591: step 35500, loss = 2.28, batch loss = 2.23 (35.4 examples/sec; 0.226 sec/batch; 18h:37m:45s remains)
2017-12-08 07:06:46.613344: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289227 -4.4289265 -4.4289384 -4.4289546 -4.4289651 -4.4289622 -4.4289474 -4.4289351 -4.4289389 -4.4289575 -4.428978 -4.4289889 -4.428988 -4.4289794 -4.4289761][-4.4288759 -4.4288821 -4.4288964 -4.428915 -4.428925 -4.428915 -4.4288907 -4.4288764 -4.4288893 -4.4289231 -4.4289584 -4.42898 -4.4289832 -4.4289746 -4.428968][-4.4287667 -4.428793 -4.4288239 -4.4288483 -4.4288454 -4.4288087 -4.42876 -4.42874 -4.4287739 -4.4288397 -4.4289041 -4.4289455 -4.4289575 -4.4289479 -4.4289384][-4.4286089 -4.4286709 -4.428731 -4.4287629 -4.4287362 -4.4286489 -4.4285493 -4.4285116 -4.4285784 -4.4286976 -4.4288082 -4.4288778 -4.4288988 -4.4288836 -4.4288626][-4.4284506 -4.4285541 -4.4286408 -4.4286742 -4.4286137 -4.4284568 -4.4282846 -4.4282193 -4.4283257 -4.4285183 -4.4286866 -4.4287839 -4.4288073 -4.4287753 -4.4287353][-4.4283824 -4.4285107 -4.4286036 -4.4286208 -4.4285197 -4.4283051 -4.4280815 -4.4280038 -4.428144 -4.4283919 -4.4285984 -4.4287062 -4.4287186 -4.4286585 -4.42859][-4.4284334 -4.4285564 -4.4286337 -4.4286237 -4.4284997 -4.4282789 -4.4280691 -4.4280133 -4.4281616 -4.4284 -4.4285979 -4.4286985 -4.4286971 -4.4286141 -4.428514][-4.4285512 -4.4286442 -4.4286942 -4.4286675 -4.4285531 -4.4283743 -4.428225 -4.428205 -4.4283342 -4.4285192 -4.4286771 -4.4287639 -4.4287615 -4.4286795 -4.4285727][-4.4286695 -4.4287305 -4.42876 -4.4287348 -4.4286518 -4.4285312 -4.428442 -4.4284444 -4.4285398 -4.4286656 -4.4287782 -4.4288492 -4.4288568 -4.4288011 -4.4287195][-4.4287715 -4.4288068 -4.428823 -4.4288073 -4.4287615 -4.4286947 -4.428648 -4.4286532 -4.4287095 -4.4287844 -4.4288588 -4.4289155 -4.4289346 -4.4289107 -4.4288654][-4.4288478 -4.428865 -4.4288759 -4.4288735 -4.4288583 -4.4288321 -4.4288096 -4.4288063 -4.4288235 -4.4288549 -4.4289 -4.4289432 -4.4289713 -4.4289751 -4.4289589][-4.4289017 -4.428906 -4.4289165 -4.4289222 -4.428925 -4.4289217 -4.4289079 -4.4288912 -4.4288836 -4.4288869 -4.4289074 -4.4289374 -4.428967 -4.4289885 -4.4289937][-4.4289165 -4.4289055 -4.428915 -4.4289331 -4.4289522 -4.4289622 -4.4289517 -4.4289269 -4.428906 -4.4288945 -4.4288988 -4.4289179 -4.4289436 -4.4289718 -4.428988][-4.4288816 -4.428854 -4.4288645 -4.4289021 -4.4289441 -4.4289694 -4.4289665 -4.4289412 -4.428916 -4.4288974 -4.42889 -4.4288988 -4.4289179 -4.4289455 -4.428966][-4.42881 -4.4287739 -4.4287877 -4.4288421 -4.4288993 -4.4289346 -4.4289374 -4.4289179 -4.4288988 -4.4288821 -4.4288712 -4.4288731 -4.4288878 -4.428915 -4.4289389]]...]
INFO - root - 2017-12-08 07:06:48.832263: step 35510, loss = 2.28, batch loss = 2.23 (34.6 examples/sec; 0.231 sec/batch; 19h:05m:36s remains)
INFO - root - 2017-12-08 07:06:51.101746: step 35520, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 18h:04m:28s remains)
INFO - root - 2017-12-08 07:06:53.334136: step 35530, loss = 2.28, batch loss = 2.23 (36.6 examples/sec; 0.219 sec/batch; 18h:01m:54s remains)
INFO - root - 2017-12-08 07:06:55.551708: step 35540, loss = 2.28, batch loss = 2.23 (37.3 examples/sec; 0.214 sec/batch; 17h:40m:27s remains)
INFO - root - 2017-12-08 07:06:57.798488: step 35550, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.222 sec/batch; 18h:16m:42s remains)
INFO - root - 2017-12-08 07:07:00.017796: step 35560, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 18h:09m:10s remains)
INFO - root - 2017-12-08 07:07:02.238038: step 35570, loss = 2.28, batch loss = 2.23 (37.0 examples/sec; 0.216 sec/batch; 17h:49m:33s remains)
INFO - root - 2017-12-08 07:07:04.470391: step 35580, loss = 2.28, batch loss = 2.23 (37.2 examples/sec; 0.215 sec/batch; 17h:44m:37s remains)
INFO - root - 2017-12-08 07:07:06.717163: step 35590, loss = 2.28, batch loss = 2.23 (35.2 examples/sec; 0.227 sec/batch; 18h:44m:17s remains)
INFO - root - 2017-12-08 07:07:09.008204: step 35600, loss = 2.28, batch loss = 2.23 (36.6 examples/sec; 0.218 sec/batch; 18h:00m:16s remains)
2017-12-08 07:07:09.279866: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4290061 -4.42899 -4.4289341 -4.4288321 -4.4287109 -4.4286327 -4.4286513 -4.4287271 -4.4288049 -4.4288654 -4.428916 -4.4289627 -4.4289827 -4.4289832 -4.4289789][-4.4290118 -4.4289927 -4.4289236 -4.4288006 -4.4286585 -4.428575 -4.4286146 -4.4287148 -4.4288039 -4.4288611 -4.4289141 -4.4289656 -4.4289885 -4.4289865 -4.4289789][-4.4290104 -4.4289889 -4.4289041 -4.4287591 -4.4285903 -4.4285035 -4.4285789 -4.4287176 -4.4288225 -4.42887 -4.4289207 -4.4289713 -4.4289932 -4.4289885 -4.428977][-4.4290028 -4.4289722 -4.428874 -4.4287105 -4.428514 -4.4284182 -4.4285264 -4.4287071 -4.4288287 -4.4288745 -4.4289255 -4.4289765 -4.4289951 -4.4289875 -4.4289718][-4.4289908 -4.4289484 -4.4288397 -4.4286633 -4.4284458 -4.4283328 -4.428453 -4.4286661 -4.4288087 -4.4288659 -4.4289222 -4.4289761 -4.4289913 -4.4289813 -4.4289603][-4.4289827 -4.4289365 -4.4288239 -4.4286394 -4.4284105 -4.4282761 -4.4283843 -4.4286156 -4.4287796 -4.4288549 -4.4289179 -4.4289722 -4.428988 -4.4289732 -4.4289465][-4.4289794 -4.4289379 -4.4288197 -4.42862 -4.4283776 -4.4282136 -4.4282951 -4.4285426 -4.4287348 -4.428833 -4.428906 -4.4289627 -4.4289813 -4.4289641 -4.4289317][-4.4289708 -4.4289336 -4.4288087 -4.4285913 -4.428319 -4.4281139 -4.4281683 -4.4284391 -4.4286666 -4.4287925 -4.4288774 -4.42894 -4.4289627 -4.4289422 -4.4289026][-4.4289579 -4.4289284 -4.4288068 -4.4285817 -4.4282818 -4.4280343 -4.4280496 -4.4283195 -4.4285803 -4.4287415 -4.4288425 -4.4289088 -4.4289336 -4.4289117 -4.4288664][-4.4289465 -4.4289246 -4.4288216 -4.428618 -4.4283352 -4.4280829 -4.42805 -4.4282789 -4.42854 -4.4287214 -4.4288316 -4.4288955 -4.4289145 -4.42889 -4.4288411][-4.4289384 -4.4289222 -4.428843 -4.4286785 -4.4284496 -4.4282331 -4.4281764 -4.4283438 -4.4285707 -4.428741 -4.4288411 -4.4288979 -4.4289103 -4.4288845 -4.4288349][-4.4289322 -4.4289193 -4.4288607 -4.4287338 -4.42856 -4.4283929 -4.428339 -4.428463 -4.4286485 -4.4287934 -4.4288726 -4.4289141 -4.4289184 -4.4288917 -4.4288445][-4.4289303 -4.4289269 -4.4288921 -4.4288034 -4.4286795 -4.4285636 -4.4285264 -4.4286151 -4.4287505 -4.4288616 -4.428916 -4.428937 -4.4289322 -4.4289036 -4.4288588][-4.4289274 -4.4289341 -4.4289222 -4.4288692 -4.4287891 -4.4287148 -4.4286914 -4.4287505 -4.4288421 -4.4289188 -4.4289522 -4.4289584 -4.4289455 -4.428916 -4.4288764][-4.4289236 -4.4289351 -4.4289346 -4.4289083 -4.4288621 -4.4288144 -4.4288006 -4.4288368 -4.4288964 -4.4289441 -4.4289641 -4.4289641 -4.4289494 -4.4289231 -4.4288912]]...]
INFO - root - 2017-12-08 07:07:11.546069: step 35610, loss = 2.28, batch loss = 2.23 (37.5 examples/sec; 0.213 sec/batch; 17h:34m:49s remains)
INFO - root - 2017-12-08 07:07:13.927023: step 35620, loss = 2.28, batch loss = 2.23 (34.6 examples/sec; 0.232 sec/batch; 19h:05m:32s remains)
INFO - root - 2017-12-08 07:07:16.158764: step 35630, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 18h:06m:14s remains)
INFO - root - 2017-12-08 07:07:18.413105: step 35640, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 18h:12m:48s remains)
INFO - root - 2017-12-08 07:07:20.619215: step 35650, loss = 2.28, batch loss = 2.23 (36.6 examples/sec; 0.219 sec/batch; 18h:01m:20s remains)
INFO - root - 2017-12-08 07:07:22.877310: step 35660, loss = 2.28, batch loss = 2.23 (34.3 examples/sec; 0.233 sec/batch; 19h:13m:44s remains)
INFO - root - 2017-12-08 07:07:25.140814: step 35670, loss = 2.28, batch loss = 2.23 (36.7 examples/sec; 0.218 sec/batch; 17h:58m:56s remains)
INFO - root - 2017-12-08 07:07:27.384550: step 35680, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.220 sec/batch; 18h:09m:40s remains)
INFO - root - 2017-12-08 07:07:29.653088: step 35690, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.225 sec/batch; 18h:33m:18s remains)
INFO - root - 2017-12-08 07:07:31.902146: step 35700, loss = 2.28, batch loss = 2.23 (35.2 examples/sec; 0.227 sec/batch; 18h:43m:06s remains)
2017-12-08 07:07:32.210850: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4286795 -4.4286146 -4.4285913 -4.4286389 -4.4287319 -4.4287848 -4.4287677 -4.4287171 -4.4287033 -4.4287486 -4.4288139 -4.4288683 -4.4288869 -4.4288759 -4.4288459][-4.4286823 -4.428587 -4.4285398 -4.4285789 -4.4286733 -4.4287271 -4.4287157 -4.4286771 -4.4286814 -4.4287453 -4.4288158 -4.4288683 -4.428885 -4.4288764 -4.4288459][-4.4287305 -4.4286418 -4.4285893 -4.4286051 -4.4286585 -4.4286761 -4.4286518 -4.4286165 -4.4286318 -4.4287138 -4.4288006 -4.4288578 -4.4288712 -4.4288669 -4.428844][-4.4287872 -4.4287248 -4.4286904 -4.4286904 -4.4286928 -4.4286547 -4.4285946 -4.4285421 -4.4285507 -4.4286532 -4.428762 -4.4288297 -4.428844 -4.4288406 -4.4288239][-4.4287963 -4.4287758 -4.4287748 -4.4287796 -4.4287467 -4.4286547 -4.4285407 -4.4284506 -4.4284453 -4.4285731 -4.4287081 -4.4287887 -4.4288082 -4.4288058 -4.4287996][-4.4287643 -4.428792 -4.4288335 -4.4288526 -4.4287982 -4.4286647 -4.4285011 -4.4283648 -4.4283519 -4.428503 -4.4286571 -4.4287539 -4.4287853 -4.4287863 -4.4287853][-4.4287014 -4.4287753 -4.4288526 -4.4288826 -4.4288187 -4.4286637 -4.4284697 -4.4283142 -4.4283128 -4.4284773 -4.4286404 -4.4287519 -4.42879 -4.4287925 -4.4287906][-4.4286594 -4.4287581 -4.4288459 -4.4288726 -4.4287996 -4.4286346 -4.428443 -4.4283075 -4.4283357 -4.4285073 -4.42867 -4.428782 -4.4288163 -4.4288149 -4.4288125][-4.4286704 -4.4287663 -4.4288354 -4.42884 -4.4287553 -4.4285917 -4.4284191 -4.4283228 -4.4283819 -4.4285555 -4.4287176 -4.4288206 -4.4288507 -4.4288497 -4.4288526][-4.4287086 -4.4287887 -4.4288325 -4.42881 -4.4287181 -4.4285679 -4.4284196 -4.4283619 -4.4284415 -4.4286017 -4.4287548 -4.4288468 -4.4288759 -4.4288797 -4.4288845][-4.4287686 -4.4288349 -4.4288492 -4.4288039 -4.4287105 -4.428586 -4.4284744 -4.4284515 -4.4285307 -4.4286571 -4.4287772 -4.42885 -4.4288788 -4.4288931 -4.4289031][-4.4288154 -4.4288692 -4.428863 -4.4288044 -4.4287143 -4.4286137 -4.428545 -4.428556 -4.4286308 -4.42872 -4.4287868 -4.4288254 -4.4288521 -4.4288778 -4.4289007][-4.4288611 -4.4288912 -4.4288664 -4.4287987 -4.4287071 -4.4286203 -4.4285932 -4.428637 -4.4287152 -4.4287753 -4.4287925 -4.428793 -4.4288096 -4.4288373 -4.42887][-4.428916 -4.4289174 -4.4288664 -4.4287767 -4.4286733 -4.4286032 -4.4286156 -4.4286847 -4.4287691 -4.4288187 -4.4288158 -4.4287915 -4.4287858 -4.4288049 -4.4288392][-4.4289651 -4.4289365 -4.4288521 -4.4287319 -4.4286203 -4.4285746 -4.4286237 -4.4287138 -4.4288034 -4.428853 -4.4288492 -4.4288206 -4.4288039 -4.4288077 -4.4288297]]...]
INFO - root - 2017-12-08 07:07:34.450339: step 35710, loss = 2.28, batch loss = 2.23 (35.7 examples/sec; 0.224 sec/batch; 18h:28m:45s remains)
INFO - root - 2017-12-08 07:07:36.693854: step 35720, loss = 2.28, batch loss = 2.23 (35.0 examples/sec; 0.228 sec/batch; 18h:49m:12s remains)
INFO - root - 2017-12-08 07:07:39.003092: step 35730, loss = 2.28, batch loss = 2.23 (37.0 examples/sec; 0.216 sec/batch; 17h:50m:15s remains)
INFO - root - 2017-12-08 07:07:41.261969: step 35740, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 18h:23m:01s remains)
INFO - root - 2017-12-08 07:07:43.554965: step 35750, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 18h:03m:50s remains)
INFO - root - 2017-12-08 07:07:45.794960: step 35760, loss = 2.28, batch loss = 2.23 (35.1 examples/sec; 0.228 sec/batch; 18h:47m:41s remains)
INFO - root - 2017-12-08 07:07:48.037009: step 35770, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 18h:13m:35s remains)
INFO - root - 2017-12-08 07:07:50.263936: step 35780, loss = 2.28, batch loss = 2.23 (34.7 examples/sec; 0.230 sec/batch; 18h:59m:24s remains)
INFO - root - 2017-12-08 07:07:52.490372: step 35790, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.225 sec/batch; 18h:33m:16s remains)
INFO - root - 2017-12-08 07:07:54.755542: step 35800, loss = 2.28, batch loss = 2.23 (35.3 examples/sec; 0.227 sec/batch; 18h:40m:12s remains)
2017-12-08 07:07:55.025085: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4283395 -4.4284735 -4.4286661 -4.4288096 -4.4288888 -4.428925 -4.4289026 -4.4287167 -4.4283953 -4.428081 -4.4279714 -4.4281569 -4.4284143 -4.4286327 -4.4287724][-4.4284472 -4.4285555 -4.4287224 -4.4288559 -4.4289269 -4.428966 -4.4289637 -4.4288516 -4.4286561 -4.4284668 -4.4283843 -4.4284754 -4.4285922 -4.4287157 -4.4288073][-4.4286008 -4.4286842 -4.4288054 -4.4288993 -4.4289389 -4.4289613 -4.4289651 -4.4289055 -4.4288077 -4.4287333 -4.4287076 -4.42875 -4.4287872 -4.4288483 -4.4288931][-4.4287577 -4.4288259 -4.4289083 -4.4289575 -4.4289532 -4.4289355 -4.4289179 -4.4288793 -4.4288344 -4.4288354 -4.4288607 -4.4288974 -4.4289136 -4.4289465 -4.4289656][-4.4288507 -4.4289136 -4.4289727 -4.428998 -4.4289656 -4.4288945 -4.4288249 -4.42877 -4.42877 -4.4288297 -4.4288931 -4.428947 -4.4289737 -4.4289932 -4.428998][-4.42892 -4.4289689 -4.4290028 -4.4289927 -4.4289269 -4.4287972 -4.4286485 -4.4285541 -4.4286022 -4.4287362 -4.4288464 -4.4289293 -4.4289761 -4.4289932 -4.4289942][-4.42897 -4.4289932 -4.4289942 -4.4289308 -4.4287887 -4.4285636 -4.4282975 -4.4281411 -4.4282608 -4.4285316 -4.4287295 -4.4288597 -4.4289384 -4.4289703 -4.4289827][-4.4289927 -4.4289894 -4.4289551 -4.4288325 -4.4286 -4.4282689 -4.4278603 -4.4275761 -4.4277592 -4.4282064 -4.4285293 -4.4287343 -4.4288659 -4.428925 -4.4289584][-4.4289932 -4.4289865 -4.4289379 -4.4287982 -4.428556 -4.4282141 -4.4277472 -4.4273138 -4.4274306 -4.4279313 -4.428318 -4.4285803 -4.4287639 -4.428865 -4.4289184][-4.4289942 -4.4289951 -4.4289546 -4.4288483 -4.4286675 -4.4284177 -4.4280696 -4.4276977 -4.4276834 -4.4280057 -4.4283113 -4.4285445 -4.4287233 -4.4288354 -4.4288969][-4.4289951 -4.4290042 -4.428977 -4.4289031 -4.4287896 -4.42864 -4.4284387 -4.4282022 -4.4281383 -4.4282808 -4.4284592 -4.4286189 -4.4287534 -4.4288464 -4.4289007][-4.4289851 -4.4290061 -4.4290037 -4.4289656 -4.4289041 -4.42882 -4.4286985 -4.4285483 -4.4284678 -4.4284911 -4.4285784 -4.4286804 -4.4287777 -4.4288559 -4.4289074][-4.4289641 -4.4289923 -4.429018 -4.4290185 -4.4289966 -4.4289532 -4.4288654 -4.4287472 -4.4286442 -4.428586 -4.428597 -4.4286566 -4.42875 -4.4288354 -4.4288945][-4.4289484 -4.4289789 -4.4290195 -4.4290433 -4.4290447 -4.4290214 -4.4289556 -4.4288526 -4.4287257 -4.4286027 -4.4285369 -4.4285541 -4.4286542 -4.4287663 -4.4288497][-4.4289351 -4.4289637 -4.4290051 -4.4290323 -4.4290433 -4.4290404 -4.4290004 -4.4289145 -4.428781 -4.4286251 -4.4285107 -4.4284816 -4.4285612 -4.4286938 -4.4288139]]...]
INFO - root - 2017-12-08 07:07:57.263897: step 35810, loss = 2.28, batch loss = 2.23 (34.5 examples/sec; 0.232 sec/batch; 19h:05m:29s remains)
INFO - root - 2017-12-08 07:07:59.500110: step 35820, loss = 2.28, batch loss = 2.23 (33.8 examples/sec; 0.237 sec/batch; 19h:30m:40s remains)
INFO - root - 2017-12-08 07:08:01.776103: step 35830, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 18h:22m:13s remains)
INFO - root - 2017-12-08 07:08:04.001319: step 35840, loss = 2.28, batch loss = 2.23 (36.6 examples/sec; 0.219 sec/batch; 18h:00m:52s remains)
INFO - root - 2017-12-08 07:08:06.243523: step 35850, loss = 2.28, batch loss = 2.23 (35.3 examples/sec; 0.227 sec/batch; 18h:40m:44s remains)
INFO - root - 2017-12-08 07:08:08.532890: step 35860, loss = 2.28, batch loss = 2.23 (35.3 examples/sec; 0.227 sec/batch; 18h:40m:10s remains)
INFO - root - 2017-12-08 07:08:10.765273: step 35870, loss = 2.28, batch loss = 2.23 (37.5 examples/sec; 0.214 sec/batch; 17h:35m:46s remains)
INFO - root - 2017-12-08 07:08:12.993112: step 35880, loss = 2.28, batch loss = 2.23 (37.0 examples/sec; 0.216 sec/batch; 17h:49m:50s remains)
INFO - root - 2017-12-08 07:08:15.219062: step 35890, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.221 sec/batch; 18h:10m:49s remains)
INFO - root - 2017-12-08 07:08:17.469494: step 35900, loss = 2.28, batch loss = 2.23 (34.7 examples/sec; 0.231 sec/batch; 18h:59m:40s remains)
2017-12-08 07:08:17.843316: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4286537 -4.4286852 -4.4287024 -4.4286966 -4.4286966 -4.4287086 -4.4287152 -4.4287086 -4.4286962 -4.4287162 -4.4287677 -4.4288154 -4.4288507 -4.4288559 -4.4288197][-4.4286833 -4.4286904 -4.4286957 -4.4286847 -4.4286723 -4.428669 -4.4286675 -4.4286704 -4.4286718 -4.4286952 -4.4287438 -4.4288054 -4.4288626 -4.428884 -4.428854][-4.42874 -4.4287114 -4.4286895 -4.428668 -4.4286485 -4.4286332 -4.4286284 -4.428638 -4.4286504 -4.4286742 -4.4287281 -4.4288034 -4.4288721 -4.428905 -4.4288836][-4.4287939 -4.4287353 -4.4286819 -4.4286466 -4.42862 -4.4285927 -4.4285812 -4.42859 -4.4286113 -4.4286485 -4.4287148 -4.4287963 -4.428864 -4.4289 -4.42889][-4.4288363 -4.4287667 -4.4286957 -4.4286475 -4.4286113 -4.4285679 -4.428524 -4.4285064 -4.4285188 -4.4285831 -4.4286809 -4.4287729 -4.42884 -4.4288774 -4.4288783][-4.428863 -4.4288 -4.4287295 -4.4286761 -4.4286242 -4.42855 -4.4284534 -4.4283724 -4.428349 -4.4284534 -4.4286156 -4.4287343 -4.4288096 -4.4288497 -4.4288597][-4.428874 -4.4288278 -4.4287729 -4.4287181 -4.4286366 -4.4285307 -4.4283891 -4.4282455 -4.4281921 -4.4283476 -4.4285626 -4.4287009 -4.4287848 -4.4288254 -4.4288406][-4.4288478 -4.4288278 -4.4287906 -4.4287376 -4.4286346 -4.4285092 -4.4283705 -4.4282413 -4.4282122 -4.4283628 -4.4285512 -4.4286747 -4.428751 -4.4287872 -4.4287987][-4.4287925 -4.4287949 -4.4287686 -4.4287181 -4.4286189 -4.4285049 -4.4284086 -4.4283395 -4.4283357 -4.4284267 -4.4285369 -4.42863 -4.4287081 -4.4287448 -4.4287591][-4.4287291 -4.4287429 -4.4287286 -4.4286933 -4.428628 -4.4285488 -4.4284878 -4.4284468 -4.4284334 -4.4284573 -4.4285 -4.4285789 -4.4286704 -4.4287171 -4.4287338][-4.4286804 -4.4286919 -4.4286861 -4.4286685 -4.4286304 -4.4285884 -4.4285541 -4.4285264 -4.4285011 -4.4284792 -4.4284887 -4.4285645 -4.4286618 -4.4287062 -4.428709][-4.428668 -4.4286718 -4.4286637 -4.4286475 -4.4286132 -4.4285893 -4.4285789 -4.4285679 -4.4285474 -4.4285278 -4.4285407 -4.4286041 -4.4286656 -4.4286766 -4.4286556][-4.428689 -4.4286828 -4.4286671 -4.4286375 -4.4285965 -4.4285769 -4.4285927 -4.4286051 -4.4286022 -4.4285984 -4.4286165 -4.4286485 -4.4286537 -4.4286246 -4.428596][-4.4287257 -4.4287071 -4.4286861 -4.4286442 -4.4286 -4.4285927 -4.428628 -4.42866 -4.4286785 -4.4286842 -4.4286947 -4.4286919 -4.4286509 -4.4286013 -4.4285817][-4.4287539 -4.4287248 -4.4287024 -4.428658 -4.4286203 -4.428627 -4.4286685 -4.4287086 -4.4287291 -4.4287333 -4.4287367 -4.4287195 -4.4286623 -4.4286084 -4.4285979]]...]
INFO - root - 2017-12-08 07:08:20.108863: step 35910, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.222 sec/batch; 18h:15m:23s remains)
INFO - root - 2017-12-08 07:08:22.356855: step 35920, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 18h:21m:03s remains)
INFO - root - 2017-12-08 07:08:24.597251: step 35930, loss = 2.28, batch loss = 2.23 (36.6 examples/sec; 0.219 sec/batch; 18h:00m:22s remains)
INFO - root - 2017-12-08 07:08:26.839088: step 35940, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 18h:05m:40s remains)
INFO - root - 2017-12-08 07:08:29.085277: step 35950, loss = 2.28, batch loss = 2.23 (35.1 examples/sec; 0.228 sec/batch; 18h:45m:56s remains)
INFO - root - 2017-12-08 07:08:31.337087: step 35960, loss = 2.28, batch loss = 2.23 (37.0 examples/sec; 0.216 sec/batch; 17h:47m:15s remains)
INFO - root - 2017-12-08 07:08:33.552633: step 35970, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.220 sec/batch; 18h:08m:38s remains)
INFO - root - 2017-12-08 07:08:35.784988: step 35980, loss = 2.28, batch loss = 2.23 (37.7 examples/sec; 0.212 sec/batch; 17h:28m:49s remains)
INFO - root - 2017-12-08 07:08:38.055281: step 35990, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.222 sec/batch; 18h:16m:29s remains)
INFO - root - 2017-12-08 07:08:40.294887: step 36000, loss = 2.28, batch loss = 2.23 (34.8 examples/sec; 0.230 sec/batch; 18h:56m:43s remains)
2017-12-08 07:08:40.592594: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4290342 -4.4290414 -4.42904 -4.4290242 -4.4289846 -4.4289403 -4.4289117 -4.4288979 -4.4288859 -4.4288754 -4.4288774 -4.4288759 -4.4288583 -4.4288511 -4.4288535][-4.4290342 -4.4290442 -4.4290447 -4.4290276 -4.4289837 -4.42892 -4.4288716 -4.4288535 -4.4288497 -4.4288473 -4.4288507 -4.42884 -4.4288173 -4.4288111 -4.4288192][-4.429029 -4.4290376 -4.4290376 -4.429019 -4.4289742 -4.4288964 -4.4288325 -4.428802 -4.4288011 -4.4288168 -4.4288287 -4.4288082 -4.4287844 -4.4287872 -4.4288015][-4.4290252 -4.4290285 -4.4290276 -4.429009 -4.4289637 -4.4288797 -4.4288063 -4.4287643 -4.428762 -4.4287953 -4.4288135 -4.4287791 -4.4287543 -4.4287658 -4.4287896][-4.4290252 -4.4290257 -4.4290242 -4.4290061 -4.4289608 -4.4288774 -4.4287963 -4.4287429 -4.4287391 -4.4287844 -4.4288058 -4.4287663 -4.4287448 -4.4287629 -4.4287968][-4.4290266 -4.429028 -4.4290261 -4.42901 -4.4289665 -4.4288864 -4.4288039 -4.42874 -4.4287267 -4.4287653 -4.4287853 -4.4287529 -4.4287438 -4.4287772 -4.4288187][-4.4290276 -4.4290309 -4.4290304 -4.4290137 -4.428966 -4.4288869 -4.4288049 -4.4287357 -4.4287071 -4.4287176 -4.4287252 -4.4287071 -4.4287271 -4.4287834 -4.428833][-4.4290285 -4.4290352 -4.4290371 -4.429019 -4.4289622 -4.4288788 -4.4287987 -4.4287281 -4.4286962 -4.4286857 -4.4286847 -4.4286809 -4.4287219 -4.4287915 -4.4288421][-4.42903 -4.4290409 -4.4290462 -4.42903 -4.4289675 -4.4288793 -4.4288034 -4.428741 -4.4287181 -4.4287086 -4.4287128 -4.4287143 -4.4287543 -4.4288139 -4.4288549][-4.4290295 -4.4290433 -4.429049 -4.4290328 -4.428968 -4.4288764 -4.4288058 -4.4287624 -4.4287577 -4.4287643 -4.4287806 -4.4287863 -4.4288111 -4.4288435 -4.4288645][-4.4290261 -4.4290385 -4.429039 -4.4290195 -4.4289484 -4.4288526 -4.4287844 -4.4287558 -4.4287724 -4.4288025 -4.428833 -4.4288449 -4.4288592 -4.42887 -4.42887][-4.42902 -4.4290266 -4.4290195 -4.4289956 -4.4289246 -4.4288244 -4.428751 -4.4287276 -4.4287596 -4.4288149 -4.4288607 -4.4288764 -4.4288898 -4.4288878 -4.4288669][-4.4290133 -4.4290123 -4.4289947 -4.4289656 -4.4289002 -4.4288 -4.4287238 -4.4286971 -4.4287305 -4.4287968 -4.4288454 -4.4288654 -4.4288826 -4.4288759 -4.4288383][-4.4290094 -4.4290023 -4.4289775 -4.4289422 -4.4288807 -4.428781 -4.4287024 -4.4286695 -4.4287033 -4.4287691 -4.4288096 -4.428834 -4.4288583 -4.4288545 -4.4288125][-4.4290109 -4.4290023 -4.4289765 -4.4289393 -4.4288793 -4.4287834 -4.4287066 -4.4286742 -4.4287057 -4.4287643 -4.4287882 -4.42881 -4.4288421 -4.4288464 -4.4288077]]...]
INFO - root - 2017-12-08 07:08:42.807231: step 36010, loss = 2.28, batch loss = 2.23 (36.8 examples/sec; 0.217 sec/batch; 17h:54m:45s remains)
INFO - root - 2017-12-08 07:08:45.045771: step 36020, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 18h:11m:05s remains)
INFO - root - 2017-12-08 07:08:47.294599: step 36030, loss = 2.28, batch loss = 2.23 (33.8 examples/sec; 0.237 sec/batch; 19h:29m:07s remains)
INFO - root - 2017-12-08 07:08:49.565731: step 36040, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.225 sec/batch; 18h:32m:47s remains)
INFO - root - 2017-12-08 07:08:51.793688: step 36050, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 18h:21m:27s remains)
INFO - root - 2017-12-08 07:08:54.091497: step 36060, loss = 2.28, batch loss = 2.23 (36.0 examples/sec; 0.222 sec/batch; 18h:17m:08s remains)
INFO - root - 2017-12-08 07:08:56.333281: step 36070, loss = 2.28, batch loss = 2.23 (34.9 examples/sec; 0.229 sec/batch; 18h:51m:42s remains)
INFO - root - 2017-12-08 07:08:58.550207: step 36080, loss = 2.28, batch loss = 2.23 (35.1 examples/sec; 0.228 sec/batch; 18h:47m:20s remains)
INFO - root - 2017-12-08 07:09:00.772827: step 36090, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 18h:11m:25s remains)
INFO - root - 2017-12-08 07:09:03.012300: step 36100, loss = 2.28, batch loss = 2.23 (34.1 examples/sec; 0.234 sec/batch; 19h:17m:50s remains)
2017-12-08 07:09:03.328818: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289684 -4.428977 -4.4289856 -4.4289904 -4.4289894 -4.428977 -4.4289451 -4.4289007 -4.4288516 -4.4288216 -4.428812 -4.4288168 -4.428834 -4.4288535 -4.4288816][-4.4289818 -4.4289904 -4.4289966 -4.428998 -4.4289875 -4.4289589 -4.428905 -4.4288416 -4.4287753 -4.4287372 -4.4287362 -4.4287581 -4.4287887 -4.4288154 -4.4288535][-4.4289784 -4.4289894 -4.4289961 -4.4289885 -4.4289637 -4.4289155 -4.4288397 -4.4287539 -4.42868 -4.4286513 -4.4286704 -4.4287171 -4.4287663 -4.4287992 -4.4288406][-4.4289765 -4.4289908 -4.4289913 -4.4289684 -4.4289231 -4.4288564 -4.4287596 -4.4286494 -4.4285746 -4.4285722 -4.4286137 -4.4286766 -4.4287381 -4.428782 -4.4288311][-4.4289627 -4.4289789 -4.4289684 -4.4289308 -4.4288707 -4.4287863 -4.4286647 -4.4285254 -4.4284668 -4.4285088 -4.4285736 -4.4286385 -4.4287066 -4.428762 -4.4288173][-4.4289126 -4.428926 -4.4289045 -4.4288478 -4.4287667 -4.4286532 -4.4284821 -4.428299 -4.428266 -4.4283724 -4.4284778 -4.4285622 -4.4286542 -4.4287319 -4.4288011][-4.4288597 -4.4288583 -4.4288149 -4.4287319 -4.4286222 -4.4284673 -4.4282308 -4.4280186 -4.4280443 -4.4282308 -4.4283876 -4.4285069 -4.4286237 -4.4287214 -4.428802][-4.4288197 -4.4287972 -4.4287381 -4.4286437 -4.4285188 -4.42834 -4.4280868 -4.4279208 -4.4280252 -4.4282513 -4.4284225 -4.4285479 -4.4286628 -4.4287591 -4.4288411][-4.4287915 -4.4287629 -4.4287062 -4.4286304 -4.4285355 -4.4284062 -4.4282436 -4.4281783 -4.4282808 -4.4284506 -4.4285769 -4.4286695 -4.4287586 -4.4288373 -4.4289021][-4.4287748 -4.428751 -4.428719 -4.4286909 -4.4286547 -4.428597 -4.4285316 -4.4285192 -4.4285684 -4.4286604 -4.4287333 -4.4287872 -4.4288487 -4.4289064 -4.4289522][-4.4287896 -4.42878 -4.4287758 -4.4287815 -4.4287858 -4.4287767 -4.428762 -4.4287577 -4.4287653 -4.4288073 -4.4288392 -4.428864 -4.4289017 -4.4289441 -4.42898][-4.4288492 -4.4288421 -4.4288383 -4.4288511 -4.4288735 -4.4288907 -4.4288921 -4.4288821 -4.428865 -4.4288793 -4.4288921 -4.4289055 -4.4289255 -4.4289536 -4.4289832][-4.4289136 -4.42889 -4.428875 -4.4288912 -4.428925 -4.4289584 -4.4289708 -4.428957 -4.4289212 -4.4289155 -4.4289122 -4.4289136 -4.4289227 -4.4289455 -4.4289756][-4.4289427 -4.4289155 -4.4289064 -4.42893 -4.428968 -4.4290056 -4.4290166 -4.4289923 -4.4289379 -4.4289174 -4.4289026 -4.428896 -4.4289045 -4.4289312 -4.4289651][-4.4289403 -4.4289222 -4.4289269 -4.4289541 -4.4289889 -4.4290228 -4.4290261 -4.42899 -4.4289203 -4.428894 -4.4288774 -4.4288716 -4.4288864 -4.4289207 -4.42896]]...]
INFO - root - 2017-12-08 07:09:05.565812: step 36110, loss = 2.28, batch loss = 2.23 (35.8 examples/sec; 0.223 sec/batch; 18h:23m:43s remains)
INFO - root - 2017-12-08 07:09:07.812098: step 36120, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 18h:04m:48s remains)
INFO - root - 2017-12-08 07:09:10.070820: step 36130, loss = 2.28, batch loss = 2.23 (35.6 examples/sec; 0.225 sec/batch; 18h:29m:48s remains)
INFO - root - 2017-12-08 07:09:12.320041: step 36140, loss = 2.28, batch loss = 2.23 (35.1 examples/sec; 0.228 sec/batch; 18h:44m:35s remains)
INFO - root - 2017-12-08 07:09:14.549065: step 36150, loss = 2.28, batch loss = 2.23 (35.0 examples/sec; 0.229 sec/batch; 18h:49m:34s remains)
INFO - root - 2017-12-08 07:09:16.794615: step 36160, loss = 2.28, batch loss = 2.23 (33.5 examples/sec; 0.239 sec/batch; 19h:39m:26s remains)
INFO - root - 2017-12-08 07:09:18.997166: step 36170, loss = 2.28, batch loss = 2.23 (37.3 examples/sec; 0.215 sec/batch; 17h:40m:40s remains)
INFO - root - 2017-12-08 07:09:21.270680: step 36180, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 18h:02m:08s remains)
INFO - root - 2017-12-08 07:09:23.542377: step 36190, loss = 2.28, batch loss = 2.23 (35.3 examples/sec; 0.227 sec/batch; 18h:40m:16s remains)
INFO - root - 2017-12-08 07:09:25.794396: step 36200, loss = 2.28, batch loss = 2.23 (35.4 examples/sec; 0.226 sec/batch; 18h:36m:28s remains)
2017-12-08 07:09:26.077605: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288425 -4.4288716 -4.428874 -4.4288297 -4.428731 -4.4285536 -4.4283285 -4.4281654 -4.4282217 -4.4284058 -4.4286084 -4.4287844 -4.4289093 -4.4289784 -4.4290075][-4.4288454 -4.4288597 -4.4288516 -4.4288058 -4.4287162 -4.4285474 -4.4283328 -4.4281745 -4.4282308 -4.4284096 -4.4286103 -4.4287853 -4.42891 -4.42898 -4.4290109][-4.42884 -4.4288363 -4.4288144 -4.4287724 -4.4287028 -4.4285607 -4.4283834 -4.4282608 -4.4283175 -4.42848 -4.4286609 -4.4288163 -4.4289255 -4.4289865 -4.4290133][-4.4288411 -4.428823 -4.4287887 -4.4287467 -4.4286928 -4.4285741 -4.4284258 -4.42833 -4.42839 -4.4285426 -4.42871 -4.4288483 -4.4289417 -4.4289923 -4.4290142][-4.4288383 -4.4288192 -4.4287863 -4.4287443 -4.4286928 -4.4285831 -4.4284463 -4.4283557 -4.4284182 -4.4285703 -4.4287362 -4.4288678 -4.4289517 -4.4289947 -4.4290137][-4.4288354 -4.428822 -4.4287958 -4.4287567 -4.4286985 -4.4285831 -4.4284406 -4.4283457 -4.4284134 -4.4285722 -4.428741 -4.4288745 -4.4289584 -4.4289975 -4.4290137][-4.4288344 -4.4288268 -4.4288111 -4.4287753 -4.428709 -4.4285841 -4.4284296 -4.4283247 -4.4283967 -4.42856 -4.4287333 -4.4288716 -4.4289594 -4.4289989 -4.4290142][-4.4288363 -4.4288344 -4.4288268 -4.4287958 -4.4287214 -4.4285851 -4.4284158 -4.4282985 -4.4283695 -4.4285355 -4.4287148 -4.42886 -4.4289541 -4.428998 -4.4290147][-4.4288359 -4.4288373 -4.4288354 -4.4288096 -4.4287314 -4.4285913 -4.4284148 -4.4282918 -4.4283605 -4.428525 -4.4287062 -4.4288545 -4.4289508 -4.428997 -4.4290156][-4.4288373 -4.4288363 -4.4288368 -4.4288192 -4.4287443 -4.4286075 -4.4284377 -4.4283204 -4.4283857 -4.42854 -4.4287138 -4.4288597 -4.4289532 -4.4289975 -4.4290161][-4.4288325 -4.4288282 -4.4288306 -4.4288206 -4.4287562 -4.428628 -4.4284697 -4.42836 -4.4284205 -4.42856 -4.4287248 -4.4288673 -4.428957 -4.428998 -4.4290156][-4.4288139 -4.4288077 -4.4288092 -4.4288034 -4.4287553 -4.4286413 -4.4284916 -4.428381 -4.4284344 -4.428565 -4.4287271 -4.4288707 -4.4289603 -4.4289989 -4.4290147][-4.4287786 -4.4287744 -4.4287696 -4.428761 -4.4287224 -4.42862 -4.428472 -4.4283614 -4.4284143 -4.42855 -4.4287195 -4.4288712 -4.4289608 -4.4289989 -4.4290133][-4.4287562 -4.4287562 -4.4287472 -4.4287353 -4.4287028 -4.4286089 -4.4284639 -4.42836 -4.4284167 -4.428556 -4.4287276 -4.4288788 -4.4289632 -4.4289975 -4.4290113][-4.4287519 -4.4287581 -4.4287534 -4.4287429 -4.4287109 -4.4286232 -4.4284859 -4.4283919 -4.4284515 -4.42859 -4.4287539 -4.4288936 -4.428968 -4.428997 -4.429009]]...]
INFO - root - 2017-12-08 07:09:28.303185: step 36210, loss = 2.28, batch loss = 2.23 (35.0 examples/sec; 0.228 sec/batch; 18h:47m:18s remains)
INFO - root - 2017-12-08 07:09:30.549140: step 36220, loss = 2.28, batch loss = 2.23 (34.8 examples/sec; 0.230 sec/batch; 18h:54m:11s remains)
INFO - root - 2017-12-08 07:09:32.763623: step 36230, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 18h:20m:53s remains)
INFO - root - 2017-12-08 07:09:34.998897: step 36240, loss = 2.28, batch loss = 2.23 (36.8 examples/sec; 0.218 sec/batch; 17h:54m:10s remains)
INFO - root - 2017-12-08 07:09:37.228570: step 36250, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 18h:05m:34s remains)
INFO - root - 2017-12-08 07:09:39.492793: step 36260, loss = 2.28, batch loss = 2.23 (35.1 examples/sec; 0.228 sec/batch; 18h:44m:37s remains)
INFO - root - 2017-12-08 07:09:41.743174: step 36270, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.222 sec/batch; 18h:14m:01s remains)
INFO - root - 2017-12-08 07:09:43.966519: step 36280, loss = 2.28, batch loss = 2.23 (35.1 examples/sec; 0.228 sec/batch; 18h:46m:40s remains)
INFO - root - 2017-12-08 07:09:46.203306: step 36290, loss = 2.28, batch loss = 2.23 (35.1 examples/sec; 0.228 sec/batch; 18h:45m:34s remains)
INFO - root - 2017-12-08 07:09:48.419870: step 36300, loss = 2.28, batch loss = 2.23 (34.7 examples/sec; 0.231 sec/batch; 18h:58m:46s remains)
2017-12-08 07:09:48.731960: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288077 -4.428854 -4.4288821 -4.4288845 -4.4288745 -4.4288588 -4.4288135 -4.4287815 -4.4287767 -4.4287825 -4.4287782 -4.4287715 -4.4287829 -4.4288111 -4.4288516][-4.428865 -4.4289002 -4.4289107 -4.4289026 -4.4288917 -4.4288821 -4.4288425 -4.4288211 -4.4288278 -4.4288392 -4.4288373 -4.42883 -4.4288354 -4.4288487 -4.4288712][-4.4288955 -4.4289222 -4.4289222 -4.4289026 -4.428884 -4.4288745 -4.4288416 -4.4288306 -4.4288454 -4.4288645 -4.4288659 -4.4288611 -4.4288621 -4.4288683 -4.4288812][-4.4288712 -4.4288907 -4.4288859 -4.4288597 -4.4288311 -4.4288211 -4.4288011 -4.4287968 -4.4288106 -4.4288278 -4.4288273 -4.4288244 -4.42883 -4.4288468 -4.4288712][-4.4288054 -4.4288054 -4.428781 -4.4287391 -4.4287033 -4.4287019 -4.4287038 -4.4287133 -4.4287257 -4.4287343 -4.4287252 -4.4287176 -4.4287333 -4.4287724 -4.4288239][-4.4286981 -4.4286766 -4.4286237 -4.4285479 -4.4284854 -4.4284835 -4.4285131 -4.4285502 -4.4285779 -4.4285879 -4.428575 -4.4285645 -4.4285936 -4.4286594 -4.4287448][-4.4285803 -4.4285502 -4.4284735 -4.4283581 -4.428247 -4.4282188 -4.4282622 -4.4283319 -4.4283891 -4.4284158 -4.4284172 -4.4284196 -4.428472 -4.4285674 -4.4286833][-4.4285426 -4.4285288 -4.4284534 -4.4283271 -4.4281907 -4.428124 -4.4281425 -4.4282122 -4.4282808 -4.4283228 -4.4283471 -4.4283748 -4.4284525 -4.4285655 -4.4286904][-4.4286175 -4.4286284 -4.4285846 -4.4284959 -4.4283967 -4.4283376 -4.4283223 -4.4283462 -4.4283862 -4.42842 -4.4284463 -4.4284835 -4.4285636 -4.428668 -4.4287786][-4.4287744 -4.4288015 -4.4287882 -4.4287443 -4.4286904 -4.4286537 -4.428628 -4.4286203 -4.4286318 -4.4286475 -4.4286637 -4.4286923 -4.4287515 -4.4288244 -4.4288974][-4.4289188 -4.4289446 -4.4289484 -4.428937 -4.4289165 -4.4288974 -4.4288778 -4.428863 -4.4288597 -4.4288616 -4.428865 -4.4288788 -4.4289103 -4.428946 -4.4289818][-4.4290013 -4.4290166 -4.4290237 -4.4290247 -4.4290195 -4.4290118 -4.4290032 -4.4289937 -4.4289904 -4.4289885 -4.4289865 -4.428988 -4.4289989 -4.4290118 -4.4290257][-4.4290295 -4.4290371 -4.42904 -4.4290419 -4.42904 -4.4290352 -4.4290304 -4.4290266 -4.4290261 -4.4290252 -4.4290247 -4.4290252 -4.429028 -4.4290333 -4.42904][-4.4290342 -4.4290395 -4.4290409 -4.4290414 -4.4290414 -4.4290395 -4.4290366 -4.4290338 -4.4290342 -4.4290347 -4.4290347 -4.4290352 -4.4290371 -4.42904 -4.4290433][-4.4290328 -4.429038 -4.4290395 -4.42904 -4.4290409 -4.4290404 -4.4290395 -4.429039 -4.4290395 -4.4290404 -4.4290414 -4.4290419 -4.4290433 -4.4290452 -4.4290471]]...]
INFO - root - 2017-12-08 07:09:50.964560: step 36310, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.220 sec/batch; 18h:06m:45s remains)
INFO - root - 2017-12-08 07:09:53.215804: step 36320, loss = 2.28, batch loss = 2.23 (34.0 examples/sec; 0.235 sec/batch; 19h:20m:08s remains)
INFO - root - 2017-12-08 07:09:55.478484: step 36330, loss = 2.28, batch loss = 2.23 (35.3 examples/sec; 0.227 sec/batch; 18h:38m:34s remains)
INFO - root - 2017-12-08 07:09:57.751055: step 36340, loss = 2.28, batch loss = 2.23 (35.8 examples/sec; 0.224 sec/batch; 18h:24m:10s remains)
INFO - root - 2017-12-08 07:09:59.997732: step 36350, loss = 2.28, batch loss = 2.23 (34.6 examples/sec; 0.231 sec/batch; 19h:02m:14s remains)
INFO - root - 2017-12-08 07:10:02.218690: step 36360, loss = 2.28, batch loss = 2.23 (35.3 examples/sec; 0.227 sec/batch; 18h:40m:00s remains)
INFO - root - 2017-12-08 07:10:04.462473: step 36370, loss = 2.28, batch loss = 2.23 (36.0 examples/sec; 0.222 sec/batch; 18h:18m:00s remains)
INFO - root - 2017-12-08 07:10:06.682060: step 36380, loss = 2.28, batch loss = 2.23 (36.6 examples/sec; 0.219 sec/batch; 17h:59m:52s remains)
INFO - root - 2017-12-08 07:10:08.912745: step 36390, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 18h:01m:22s remains)
INFO - root - 2017-12-08 07:10:11.188488: step 36400, loss = 2.28, batch loss = 2.23 (37.1 examples/sec; 0.216 sec/batch; 17h:43m:48s remains)
2017-12-08 07:10:11.484334: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288969 -4.4288521 -4.4287939 -4.4287462 -4.4287314 -4.4287438 -4.4287744 -4.4287949 -4.4287715 -4.42872 -4.4286723 -4.4286637 -4.4287019 -4.4287543 -4.4287853][-4.4288979 -4.4288478 -4.4287806 -4.428731 -4.4287262 -4.4287496 -4.4287772 -4.4287891 -4.428762 -4.4287133 -4.4286594 -4.4286304 -4.4286489 -4.4287 -4.4287491][-4.4289041 -4.42885 -4.4287767 -4.4287233 -4.4287205 -4.4287515 -4.4287763 -4.4287839 -4.4287615 -4.4287252 -4.428678 -4.4286408 -4.4286351 -4.4286757 -4.4287338][-4.4289002 -4.4288497 -4.4287844 -4.4287376 -4.4287357 -4.4287648 -4.4287834 -4.4287872 -4.4287744 -4.428761 -4.42874 -4.4287119 -4.4286933 -4.4287143 -4.4287596][-4.4288955 -4.4288549 -4.4288073 -4.428772 -4.4287658 -4.4287753 -4.428772 -4.4287672 -4.4287643 -4.4287791 -4.4287939 -4.4287949 -4.42879 -4.428802 -4.4288268][-4.4288912 -4.42886 -4.4288311 -4.4288096 -4.4287968 -4.4287772 -4.4287424 -4.4287148 -4.4287071 -4.4287386 -4.4287758 -4.4287949 -4.4288044 -4.4288206 -4.4288449][-4.4288855 -4.4288626 -4.4288445 -4.4288325 -4.4288139 -4.4287653 -4.42869 -4.4286203 -4.4285812 -4.4286079 -4.428659 -4.4286923 -4.4287152 -4.4287429 -4.4287786][-4.428874 -4.4288549 -4.4288411 -4.428833 -4.428813 -4.4287491 -4.428647 -4.4285345 -4.4284453 -4.42844 -4.4284797 -4.4285097 -4.4285336 -4.4285665 -4.4286218][-4.4288697 -4.4288511 -4.4288349 -4.4288282 -4.428813 -4.4287572 -4.42866 -4.4285359 -4.4284158 -4.4283676 -4.4283819 -4.4284029 -4.428421 -4.4284453 -4.4284973][-4.428895 -4.4288754 -4.428854 -4.4288445 -4.42884 -4.4288125 -4.4287548 -4.4286747 -4.4285841 -4.4285312 -4.4285235 -4.4285336 -4.4285436 -4.4285526 -4.4285736][-4.428926 -4.4289088 -4.42889 -4.4288807 -4.4288874 -4.4288878 -4.4288688 -4.4288311 -4.4287777 -4.4287372 -4.4287219 -4.4287257 -4.4287319 -4.4287353 -4.4287386][-4.4289355 -4.4289203 -4.428905 -4.428894 -4.4289031 -4.4289155 -4.4289165 -4.4289036 -4.4288712 -4.4288373 -4.4288206 -4.4288239 -4.4288335 -4.4288392 -4.4288344][-4.4289236 -4.4289074 -4.4288912 -4.4288745 -4.4288807 -4.4288988 -4.428905 -4.4288964 -4.4288688 -4.4288325 -4.4288116 -4.42881 -4.428822 -4.4288316 -4.4288297][-4.42889 -4.4288688 -4.4288435 -4.4288135 -4.4288154 -4.4288383 -4.4288478 -4.4288373 -4.4288068 -4.428772 -4.4287539 -4.4287539 -4.4287696 -4.428782 -4.4287786][-4.4288855 -4.42886 -4.4288211 -4.4287696 -4.4287572 -4.4287806 -4.4287996 -4.4287949 -4.4287686 -4.42874 -4.4287233 -4.4287219 -4.4287333 -4.4287434 -4.4287381]]...]
INFO - root - 2017-12-08 07:10:13.705843: step 36410, loss = 2.28, batch loss = 2.23 (34.9 examples/sec; 0.229 sec/batch; 18h:50m:40s remains)
INFO - root - 2017-12-08 07:10:15.943433: step 36420, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.222 sec/batch; 18h:13m:58s remains)
INFO - root - 2017-12-08 07:10:18.207490: step 36430, loss = 2.28, batch loss = 2.23 (34.5 examples/sec; 0.232 sec/batch; 19h:02m:36s remains)
INFO - root - 2017-12-08 07:10:20.430030: step 36440, loss = 2.28, batch loss = 2.23 (35.6 examples/sec; 0.225 sec/batch; 18h:28m:54s remains)
INFO - root - 2017-12-08 07:10:22.689239: step 36450, loss = 2.28, batch loss = 2.23 (35.2 examples/sec; 0.227 sec/batch; 18h:42m:24s remains)
INFO - root - 2017-12-08 07:10:24.929229: step 36460, loss = 2.28, batch loss = 2.23 (35.2 examples/sec; 0.227 sec/batch; 18h:41m:18s remains)
INFO - root - 2017-12-08 07:10:27.152134: step 36470, loss = 2.28, batch loss = 2.23 (35.6 examples/sec; 0.224 sec/batch; 18h:27m:22s remains)
INFO - root - 2017-12-08 07:10:29.383346: step 36480, loss = 2.28, batch loss = 2.23 (35.7 examples/sec; 0.224 sec/batch; 18h:25m:23s remains)
INFO - root - 2017-12-08 07:10:31.580775: step 36490, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.220 sec/batch; 18h:07m:21s remains)
INFO - root - 2017-12-08 07:10:33.801098: step 36500, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 18h:19m:04s remains)
2017-12-08 07:10:34.096912: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288292 -4.4288454 -4.428874 -4.42891 -4.42894 -4.4289517 -4.4289336 -4.42887 -4.4288111 -4.4287987 -4.4288273 -4.4288807 -4.4289336 -4.4289751 -4.4289918][-4.4287906 -4.4288139 -4.4288521 -4.4288945 -4.4289322 -4.4289517 -4.4289389 -4.4288778 -4.42882 -4.4288092 -4.4288368 -4.4288883 -4.428937 -4.4289756 -4.4289918][-4.4287524 -4.4287715 -4.4288111 -4.4288535 -4.4288931 -4.4289184 -4.4289088 -4.4288521 -4.428802 -4.4288 -4.428833 -4.4288864 -4.4289355 -4.4289737 -4.4289894][-4.4286981 -4.4287081 -4.4287448 -4.4287877 -4.4288235 -4.4288497 -4.428843 -4.4287891 -4.4287496 -4.4287624 -4.4288077 -4.4288669 -4.4289184 -4.4289608 -4.4289808][-4.4286456 -4.4286427 -4.4286709 -4.4287086 -4.4287453 -4.428771 -4.4287567 -4.4286938 -4.4286628 -4.4286952 -4.4287572 -4.4288225 -4.4288797 -4.4289322 -4.4289622][-4.4285951 -4.428575 -4.4285851 -4.428616 -4.428658 -4.4286819 -4.4286509 -4.4285665 -4.4285374 -4.4285955 -4.4286833 -4.428762 -4.4288316 -4.4288983 -4.4289379][-4.4285479 -4.4285097 -4.4285007 -4.4285212 -4.4285622 -4.4285765 -4.4285274 -4.42842 -4.4283891 -4.4284768 -4.4286017 -4.4287062 -4.4287944 -4.4288712 -4.428916][-4.4285431 -4.4284863 -4.4284554 -4.4284563 -4.4284873 -4.4284878 -4.4284177 -4.4283023 -4.4282784 -4.4283991 -4.4285569 -4.4286876 -4.428791 -4.4288726 -4.4289174][-4.4286094 -4.4285536 -4.4285183 -4.4285083 -4.4285216 -4.4285069 -4.4284377 -4.4283481 -4.4283419 -4.4284515 -4.4285941 -4.4287171 -4.4288177 -4.4288945 -4.4289351][-4.4286919 -4.4286537 -4.4286361 -4.4286284 -4.4286246 -4.4285979 -4.4285493 -4.4284992 -4.4284978 -4.4285665 -4.4286623 -4.4287562 -4.4288435 -4.4289126 -4.4289484][-4.4287319 -4.4287114 -4.4287066 -4.4286981 -4.4286819 -4.4286575 -4.4286304 -4.4286079 -4.4286 -4.4286361 -4.4286981 -4.4287696 -4.4288464 -4.4289103 -4.428946][-4.4287128 -4.4287019 -4.4286981 -4.4286847 -4.4286666 -4.4286504 -4.4286356 -4.4286208 -4.4286027 -4.4286222 -4.4286742 -4.4287424 -4.4288206 -4.4288878 -4.4289289][-4.4286594 -4.4286489 -4.4286418 -4.4286222 -4.4286003 -4.4285865 -4.4285741 -4.4285626 -4.428545 -4.4285636 -4.428618 -4.4286957 -4.4287829 -4.42886 -4.42891][-4.4285955 -4.4285808 -4.4285736 -4.4285588 -4.42854 -4.4285293 -4.4285188 -4.4285126 -4.4285011 -4.4285235 -4.428586 -4.4286742 -4.4287691 -4.4288526 -4.428906][-4.4285975 -4.428586 -4.4285855 -4.4285851 -4.4285779 -4.4285736 -4.4285684 -4.4285645 -4.4285569 -4.4285808 -4.4286404 -4.4287248 -4.4288125 -4.4288845 -4.4289284]]...]
INFO - root - 2017-12-08 07:10:36.367439: step 36510, loss = 2.28, batch loss = 2.23 (35.4 examples/sec; 0.226 sec/batch; 18h:34m:38s remains)
INFO - root - 2017-12-08 07:10:38.617990: step 36520, loss = 2.28, batch loss = 2.23 (35.4 examples/sec; 0.226 sec/batch; 18h:35m:48s remains)
INFO - root - 2017-12-08 07:10:40.842390: step 36530, loss = 2.28, batch loss = 2.23 (35.4 examples/sec; 0.226 sec/batch; 18h:35m:39s remains)
INFO - root - 2017-12-08 07:10:43.057845: step 36540, loss = 2.28, batch loss = 2.23 (37.8 examples/sec; 0.211 sec/batch; 17h:23m:06s remains)
INFO - root - 2017-12-08 07:10:45.305536: step 36550, loss = 2.28, batch loss = 2.23 (35.0 examples/sec; 0.229 sec/batch; 18h:48m:29s remains)
INFO - root - 2017-12-08 07:10:47.520996: step 36560, loss = 2.28, batch loss = 2.23 (37.0 examples/sec; 0.216 sec/batch; 17h:45m:15s remains)
INFO - root - 2017-12-08 07:10:49.775354: step 36570, loss = 2.28, batch loss = 2.23 (35.0 examples/sec; 0.229 sec/batch; 18h:47m:24s remains)
INFO - root - 2017-12-08 07:10:52.008216: step 36580, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.220 sec/batch; 18h:05m:40s remains)
INFO - root - 2017-12-08 07:10:54.284834: step 36590, loss = 2.28, batch loss = 2.23 (36.7 examples/sec; 0.218 sec/batch; 17h:56m:29s remains)
INFO - root - 2017-12-08 07:10:56.530339: step 36600, loss = 2.28, batch loss = 2.23 (35.8 examples/sec; 0.223 sec/batch; 18h:20m:59s remains)
2017-12-08 07:10:56.833725: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289317 -4.4289069 -4.4288912 -4.4288898 -4.4289 -4.4289126 -4.4289246 -4.4289336 -4.42894 -4.4289436 -4.4289474 -4.4289513 -4.4289527 -4.4289527 -4.4289551][-4.4288864 -4.4288406 -4.4288149 -4.4288173 -4.4288387 -4.4288645 -4.4288855 -4.4289002 -4.4289126 -4.4289255 -4.4289355 -4.4289422 -4.4289441 -4.4289422 -4.4289427][-4.4288273 -4.4287615 -4.4287295 -4.4287367 -4.4287729 -4.428813 -4.4288406 -4.4288554 -4.4288697 -4.4288912 -4.4289131 -4.4289308 -4.4289379 -4.4289336 -4.42893][-4.4287705 -4.4286895 -4.4286542 -4.4286666 -4.4287186 -4.4287739 -4.4287987 -4.428803 -4.4288106 -4.42884 -4.4288769 -4.4289074 -4.4289222 -4.42892 -4.4289126][-4.4287353 -4.428647 -4.4286103 -4.4286242 -4.4286804 -4.428731 -4.4287376 -4.4287176 -4.4287138 -4.4287529 -4.42881 -4.4288588 -4.4288869 -4.42889 -4.42888][-4.4287314 -4.4286456 -4.4286089 -4.4286132 -4.4286504 -4.4286709 -4.4286323 -4.4285665 -4.4285398 -4.4285975 -4.4286885 -4.4287672 -4.4288177 -4.4288354 -4.4288321][-4.4287529 -4.4286776 -4.4286418 -4.4286294 -4.4286304 -4.4285989 -4.4284935 -4.4283648 -4.4283142 -4.4284048 -4.4285398 -4.4286528 -4.4287252 -4.4287634 -4.4287796][-4.4287753 -4.4287119 -4.4286814 -4.4286666 -4.4286437 -4.4285688 -4.4283977 -4.4281921 -4.4281139 -4.4282408 -4.4284153 -4.4285522 -4.4286489 -4.4287105 -4.4287429][-4.4287882 -4.4287333 -4.4287124 -4.4287109 -4.4286933 -4.4286094 -4.4284267 -4.4282141 -4.4281387 -4.4282508 -4.4283967 -4.42851 -4.4286103 -4.4286861 -4.4287291][-4.4288015 -4.4287558 -4.4287434 -4.4287519 -4.4287457 -4.4286842 -4.4285464 -4.4284 -4.4283562 -4.4284248 -4.4285069 -4.4285741 -4.4286475 -4.4287133 -4.4287548][-4.4288344 -4.4287992 -4.428793 -4.4288015 -4.4288011 -4.428762 -4.42867 -4.4285808 -4.4285593 -4.4286 -4.4286475 -4.4286842 -4.4287295 -4.4287734 -4.428802][-4.4288812 -4.4288583 -4.4288559 -4.42886 -4.4288573 -4.4288311 -4.4287696 -4.4287133 -4.4287057 -4.428731 -4.4287586 -4.4287815 -4.4288111 -4.4288383 -4.4288511][-4.428925 -4.4289136 -4.4289136 -4.428915 -4.4289064 -4.4288821 -4.4288359 -4.4287968 -4.428792 -4.4288058 -4.4288211 -4.4288392 -4.428863 -4.4288826 -4.4288874][-4.4289556 -4.4289508 -4.4289513 -4.4289517 -4.4289432 -4.4289246 -4.4288931 -4.4288654 -4.4288573 -4.4288607 -4.4288692 -4.4288855 -4.4289055 -4.4289193 -4.4289227][-4.4289751 -4.4289732 -4.4289742 -4.4289746 -4.4289703 -4.4289589 -4.4289422 -4.4289279 -4.4289203 -4.4289193 -4.4289212 -4.4289279 -4.4289379 -4.4289432 -4.4289446]]...]
INFO - root - 2017-12-08 07:10:59.067076: step 36610, loss = 2.28, batch loss = 2.23 (37.3 examples/sec; 0.214 sec/batch; 17h:36m:42s remains)
INFO - root - 2017-12-08 07:11:01.301054: step 36620, loss = 2.28, batch loss = 2.23 (36.6 examples/sec; 0.218 sec/batch; 17h:56m:53s remains)
INFO - root - 2017-12-08 07:11:03.516700: step 36630, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 18h:00m:03s remains)
INFO - root - 2017-12-08 07:11:05.756746: step 36640, loss = 2.28, batch loss = 2.23 (37.3 examples/sec; 0.214 sec/batch; 17h:36m:20s remains)
INFO - root - 2017-12-08 07:11:08.010995: step 36650, loss = 2.28, batch loss = 2.23 (35.7 examples/sec; 0.224 sec/batch; 18h:23m:32s remains)
INFO - root - 2017-12-08 07:11:10.270398: step 36660, loss = 2.28, batch loss = 2.23 (35.2 examples/sec; 0.227 sec/batch; 18h:40m:56s remains)
INFO - root - 2017-12-08 07:11:12.508861: step 36670, loss = 2.28, batch loss = 2.23 (35.1 examples/sec; 0.228 sec/batch; 18h:43m:52s remains)
INFO - root - 2017-12-08 07:11:14.798875: step 36680, loss = 2.28, batch loss = 2.23 (36.0 examples/sec; 0.222 sec/batch; 18h:16m:31s remains)
INFO - root - 2017-12-08 07:11:17.044798: step 36690, loss = 2.28, batch loss = 2.23 (33.6 examples/sec; 0.238 sec/batch; 19h:33m:35s remains)
INFO - root - 2017-12-08 07:11:19.286644: step 36700, loss = 2.28, batch loss = 2.23 (35.1 examples/sec; 0.228 sec/batch; 18h:44m:08s remains)
2017-12-08 07:11:19.622915: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4287724 -4.4287558 -4.4287634 -4.4287872 -4.4288125 -4.4288254 -4.4288354 -4.4288445 -4.4288487 -4.4288425 -4.4288392 -4.4288425 -4.4288516 -4.428853 -4.428843][-4.4287195 -4.4287024 -4.4287143 -4.4287438 -4.4287667 -4.4287691 -4.42877 -4.4287806 -4.4287882 -4.4287868 -4.4287891 -4.4287987 -4.4288125 -4.4288149 -4.428802][-4.4286637 -4.4286547 -4.4286737 -4.4287024 -4.4287133 -4.42869 -4.4286737 -4.4286842 -4.4286985 -4.4287066 -4.42872 -4.4287424 -4.4287629 -4.4287672 -4.4287539][-4.4286175 -4.428616 -4.4286413 -4.4286642 -4.4286456 -4.4285836 -4.4285507 -4.4285541 -4.4285846 -4.4286132 -4.428647 -4.4286885 -4.42872 -4.4287271 -4.4287186][-4.428606 -4.4286046 -4.4286218 -4.4286108 -4.4285436 -4.4284496 -4.4284091 -4.4284196 -4.4284754 -4.4285283 -4.4285784 -4.428637 -4.4286795 -4.4286919 -4.428689][-4.4286251 -4.4286122 -4.4285951 -4.4285336 -4.4284072 -4.4282942 -4.4282675 -4.4283013 -4.4283915 -4.4284754 -4.4285383 -4.4286103 -4.4286671 -4.4286871 -4.4286909][-4.4286866 -4.4286485 -4.4285774 -4.4284554 -4.4282823 -4.4281754 -4.4281774 -4.4282346 -4.4283466 -4.4284587 -4.4285383 -4.4286156 -4.4286761 -4.4287009 -4.4287124][-4.42878 -4.4287205 -4.4286089 -4.4284568 -4.42828 -4.4281893 -4.428215 -4.428297 -4.4284129 -4.4285178 -4.4285851 -4.4286494 -4.4287019 -4.42873 -4.4287515][-4.4288845 -4.4288135 -4.4286909 -4.4285412 -4.4283781 -4.4283 -4.4283471 -4.4284511 -4.4285555 -4.4286308 -4.4286766 -4.4287276 -4.4287763 -4.4288054 -4.4288278][-4.4289541 -4.4288936 -4.4287796 -4.4286442 -4.4285007 -4.4284372 -4.4284925 -4.4285941 -4.4286857 -4.4287415 -4.4287767 -4.4288192 -4.4288607 -4.4288893 -4.4289041][-4.4289694 -4.4289265 -4.4288311 -4.4287243 -4.42862 -4.428575 -4.4286213 -4.4287062 -4.428782 -4.4288206 -4.4288387 -4.4288683 -4.428906 -4.4289317 -4.4289374][-4.4289436 -4.4289074 -4.428833 -4.4287581 -4.4287004 -4.4286847 -4.4287343 -4.4288054 -4.4288616 -4.428884 -4.4288821 -4.4288855 -4.4289064 -4.4289255 -4.428925][-4.4289083 -4.428874 -4.4288211 -4.4287806 -4.428761 -4.42877 -4.4288192 -4.4288754 -4.4289093 -4.4289184 -4.4289064 -4.428896 -4.4289026 -4.4289117 -4.4289055][-4.4288807 -4.4288578 -4.4288325 -4.4288244 -4.4288263 -4.4288411 -4.42888 -4.428915 -4.4289279 -4.4289279 -4.4289174 -4.4289045 -4.4289041 -4.4289079 -4.4289021][-4.4288859 -4.4288754 -4.4288707 -4.4288793 -4.4288845 -4.4288907 -4.428916 -4.4289374 -4.4289422 -4.4289427 -4.428936 -4.4289241 -4.4289188 -4.4289193 -4.4289169]]...]
INFO - root - 2017-12-08 07:11:21.865256: step 36710, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.225 sec/batch; 18h:30m:09s remains)
INFO - root - 2017-12-08 07:11:24.105620: step 36720, loss = 2.28, batch loss = 2.23 (32.8 examples/sec; 0.244 sec/batch; 20h:00m:52s remains)
INFO - root - 2017-12-08 07:11:26.351664: step 36730, loss = 2.28, batch loss = 2.23 (36.9 examples/sec; 0.217 sec/batch; 17h:49m:17s remains)
INFO - root - 2017-12-08 07:11:28.581582: step 36740, loss = 2.28, batch loss = 2.23 (35.4 examples/sec; 0.226 sec/batch; 18h:35m:06s remains)
INFO - root - 2017-12-08 07:11:30.801556: step 36750, loss = 2.28, batch loss = 2.23 (36.7 examples/sec; 0.218 sec/batch; 17h:55m:40s remains)
INFO - root - 2017-12-08 07:11:33.037043: step 36760, loss = 2.28, batch loss = 2.23 (35.3 examples/sec; 0.226 sec/batch; 18h:36m:03s remains)
INFO - root - 2017-12-08 07:11:35.276698: step 36770, loss = 2.28, batch loss = 2.23 (34.9 examples/sec; 0.229 sec/batch; 18h:49m:05s remains)
INFO - root - 2017-12-08 07:11:37.510428: step 36780, loss = 2.28, batch loss = 2.23 (35.6 examples/sec; 0.225 sec/batch; 18h:27m:16s remains)
INFO - root - 2017-12-08 07:11:39.765168: step 36790, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 18h:10m:28s remains)
INFO - root - 2017-12-08 07:11:41.997007: step 36800, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.219 sec/batch; 18h:01m:44s remains)
2017-12-08 07:11:42.297275: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288955 -4.4288659 -4.4288344 -4.4288263 -4.42885 -4.4288955 -4.4289341 -4.4289432 -4.4289265 -4.4289007 -4.4288673 -4.4288316 -4.4287934 -4.4287553 -4.4287329][-4.4288478 -4.4287982 -4.428751 -4.4287472 -4.4287896 -4.4288535 -4.4289012 -4.4289117 -4.4288926 -4.428865 -4.4288244 -4.42878 -4.4287214 -4.4286571 -4.4286246][-4.428793 -4.4287295 -4.4286752 -4.4286823 -4.4287438 -4.4288177 -4.428865 -4.4288721 -4.4288545 -4.4288325 -4.4287915 -4.42874 -4.4286637 -4.4285717 -4.4285359][-4.428731 -4.4286652 -4.428617 -4.4286413 -4.4287148 -4.42878 -4.4288011 -4.4287882 -4.4287729 -4.4287648 -4.4287338 -4.4286857 -4.4286056 -4.4285049 -4.4284706][-4.4287372 -4.4286795 -4.4286432 -4.428668 -4.4287181 -4.4287405 -4.4287057 -4.4286537 -4.4286413 -4.4286647 -4.4286733 -4.4286571 -4.4285979 -4.42851 -4.4284844][-4.4287786 -4.4287362 -4.4287105 -4.4287105 -4.4287009 -4.4286466 -4.42853 -4.4284186 -4.4284115 -4.4284992 -4.4285879 -4.42864 -4.4286385 -4.4285893 -4.4285731][-4.4288 -4.4287648 -4.4287405 -4.4287066 -4.4286361 -4.4284973 -4.4282732 -4.4280605 -4.4280643 -4.4282627 -4.4284649 -4.4286036 -4.4286709 -4.4286642 -4.42866][-4.4288087 -4.4287763 -4.4287481 -4.4286923 -4.4285879 -4.4283991 -4.4280844 -4.427763 -4.427772 -4.4280825 -4.42838 -4.4285846 -4.4286933 -4.4287138 -4.42872][-4.428793 -4.4287729 -4.4287558 -4.4287076 -4.4286227 -4.4284706 -4.428194 -4.4279 -4.4278979 -4.428174 -4.4284358 -4.4286175 -4.4287119 -4.4287367 -4.4287462][-4.4287715 -4.4287672 -4.4287715 -4.4287481 -4.4287028 -4.4286251 -4.428453 -4.4282532 -4.42824 -4.4284148 -4.4285712 -4.4286771 -4.428719 -4.4287186 -4.4287224][-4.4287162 -4.428721 -4.4287453 -4.4287543 -4.428761 -4.4287467 -4.428658 -4.4285407 -4.4285321 -4.4286284 -4.4286942 -4.4287219 -4.4287033 -4.4286604 -4.4286461][-4.428689 -4.4286962 -4.4287324 -4.4287682 -4.4288096 -4.4288325 -4.4287977 -4.4287357 -4.4287257 -4.4287553 -4.4287558 -4.4287281 -4.4286623 -4.4285893 -4.4285612][-4.4287095 -4.4287229 -4.4287615 -4.4288073 -4.4288568 -4.4288926 -4.4288878 -4.4288588 -4.4288487 -4.428833 -4.4287872 -4.4287224 -4.428637 -4.4285569 -4.428525][-4.4287429 -4.4287558 -4.42879 -4.4288297 -4.4288721 -4.4289112 -4.4289217 -4.4289126 -4.4289036 -4.4288688 -4.4288039 -4.428731 -4.4286547 -4.4285927 -4.42857][-4.4287853 -4.4287925 -4.4288187 -4.4288473 -4.4288774 -4.4289122 -4.4289336 -4.4289327 -4.4289212 -4.4288869 -4.4288297 -4.428771 -4.4287162 -4.4286771 -4.428659]]...]
INFO - root - 2017-12-08 07:11:44.539514: step 36810, loss = 2.28, batch loss = 2.23 (35.7 examples/sec; 0.224 sec/batch; 18h:22m:58s remains)
INFO - root - 2017-12-08 07:11:46.768572: step 36820, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 17h:59m:59s remains)
INFO - root - 2017-12-08 07:11:49.007732: step 36830, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.226 sec/batch; 18h:31m:51s remains)
INFO - root - 2017-12-08 07:11:51.243710: step 36840, loss = 2.28, batch loss = 2.23 (35.7 examples/sec; 0.224 sec/batch; 18h:23m:15s remains)
INFO - root - 2017-12-08 07:11:53.536928: step 36850, loss = 2.28, batch loss = 2.23 (35.7 examples/sec; 0.224 sec/batch; 18h:22m:40s remains)
INFO - root - 2017-12-08 07:11:55.766917: step 36860, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 18h:08m:52s remains)
INFO - root - 2017-12-08 07:11:58.024995: step 36870, loss = 2.28, batch loss = 2.23 (34.7 examples/sec; 0.231 sec/batch; 18h:57m:08s remains)
INFO - root - 2017-12-08 07:12:00.248845: step 36880, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 18h:08m:00s remains)
INFO - root - 2017-12-08 07:12:02.484127: step 36890, loss = 2.28, batch loss = 2.23 (33.3 examples/sec; 0.240 sec/batch; 19h:44m:08s remains)
INFO - root - 2017-12-08 07:12:04.776654: step 36900, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 18h:08m:09s remains)
2017-12-08 07:12:05.057448: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.428926 -4.4289303 -4.428915 -4.4288955 -4.4288912 -4.4288955 -4.4288945 -4.4288855 -4.4288697 -4.4288278 -4.4287639 -4.4287043 -4.4286814 -4.4287114 -4.4287624][-4.4289231 -4.4289365 -4.4289265 -4.4289041 -4.4288964 -4.4288969 -4.4288936 -4.4288793 -4.4288669 -4.4288416 -4.4288077 -4.42878 -4.42878 -4.4288034 -4.428834][-4.42889 -4.4289088 -4.4289074 -4.4288907 -4.4288769 -4.4288692 -4.42886 -4.4288492 -4.4288497 -4.4288464 -4.4288349 -4.4288325 -4.4288454 -4.4288607 -4.4288664][-4.4288483 -4.4288697 -4.4288735 -4.4288607 -4.428843 -4.4288273 -4.428812 -4.4288 -4.4288139 -4.4288344 -4.4288359 -4.4288411 -4.4288521 -4.4288616 -4.4288578][-4.4287939 -4.4288225 -4.4288297 -4.4288139 -4.4287877 -4.4287591 -4.4287333 -4.4287243 -4.4287519 -4.4287858 -4.4287925 -4.4287953 -4.4288015 -4.4288082 -4.4288054][-4.4287024 -4.4287171 -4.4287252 -4.4287181 -4.4286928 -4.4286613 -4.4286237 -4.4286108 -4.4286466 -4.4286876 -4.4287014 -4.4287119 -4.4287257 -4.4287305 -4.4287295][-4.4286523 -4.4286184 -4.4285936 -4.4285669 -4.4285316 -4.4285035 -4.4284654 -4.4284587 -4.4285116 -4.428575 -4.4286208 -4.428658 -4.4286852 -4.428689 -4.4286866][-4.4287057 -4.4286246 -4.4285474 -4.4284778 -4.4284196 -4.4283843 -4.42835 -4.4283538 -4.4284315 -4.428535 -4.4286232 -4.4286857 -4.4287176 -4.4287133 -4.4287009][-4.4288058 -4.4287186 -4.4286218 -4.428535 -4.4284735 -4.4284449 -4.428431 -4.4284468 -4.4285269 -4.428638 -4.4287271 -4.4287758 -4.4287877 -4.4287729 -4.4287481][-4.4288578 -4.4287877 -4.4286923 -4.4286075 -4.4285607 -4.4285536 -4.4285612 -4.4285865 -4.4286528 -4.428741 -4.4288015 -4.4288139 -4.428793 -4.42877 -4.428751][-4.4288516 -4.428793 -4.4287019 -4.4286141 -4.428566 -4.4285626 -4.4285784 -4.4286108 -4.4286704 -4.4287386 -4.428771 -4.4287486 -4.4287009 -4.4286766 -4.4286747][-4.42884 -4.4287949 -4.4287076 -4.4286137 -4.4285522 -4.4285274 -4.4285274 -4.4285531 -4.4286041 -4.4286604 -4.4286757 -4.4286337 -4.4285765 -4.4285545 -4.4285707][-4.4288268 -4.4287944 -4.4287167 -4.4286218 -4.4285502 -4.4285116 -4.4284992 -4.4285097 -4.4285445 -4.4285927 -4.4286017 -4.4285622 -4.4285111 -4.4284949 -4.4285197][-4.428803 -4.428792 -4.4287419 -4.4286714 -4.4286184 -4.4285889 -4.4285784 -4.4285774 -4.4285941 -4.4286332 -4.4286461 -4.4286203 -4.4285879 -4.4285789 -4.4285936][-4.4287748 -4.4287853 -4.4287777 -4.4287505 -4.4287262 -4.4287114 -4.4287019 -4.4286923 -4.4286928 -4.4287205 -4.4287376 -4.4287319 -4.428731 -4.4287381 -4.4287462]]...]
INFO - root - 2017-12-08 07:12:07.285987: step 36910, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.221 sec/batch; 18h:11m:13s remains)
INFO - root - 2017-12-08 07:12:09.539500: step 36920, loss = 2.28, batch loss = 2.23 (37.0 examples/sec; 0.216 sec/batch; 17h:46m:32s remains)
INFO - root - 2017-12-08 07:12:11.765126: step 36930, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 18h:19m:04s remains)
INFO - root - 2017-12-08 07:12:14.020508: step 36940, loss = 2.28, batch loss = 2.23 (35.7 examples/sec; 0.224 sec/batch; 18h:24m:59s remains)
INFO - root - 2017-12-08 07:12:16.246587: step 36950, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 18h:17m:50s remains)
INFO - root - 2017-12-08 07:12:18.486047: step 36960, loss = 2.28, batch loss = 2.23 (35.1 examples/sec; 0.228 sec/batch; 18h:41m:37s remains)
INFO - root - 2017-12-08 07:12:20.730833: step 36970, loss = 2.28, batch loss = 2.23 (35.4 examples/sec; 0.226 sec/batch; 18h:32m:58s remains)
INFO - root - 2017-12-08 07:12:22.993361: step 36980, loss = 2.28, batch loss = 2.23 (37.0 examples/sec; 0.216 sec/batch; 17h:44m:14s remains)
INFO - root - 2017-12-08 07:12:25.260047: step 36990, loss = 2.28, batch loss = 2.23 (36.7 examples/sec; 0.218 sec/batch; 17h:54m:11s remains)
INFO - root - 2017-12-08 07:12:27.480448: step 37000, loss = 2.28, batch loss = 2.23 (37.4 examples/sec; 0.214 sec/batch; 17h:34m:05s remains)
2017-12-08 07:12:27.780229: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288135 -4.4287062 -4.428628 -4.4285889 -4.4285855 -4.4286275 -4.4286757 -4.4286904 -4.4286828 -4.4286866 -4.4287109 -4.4287467 -4.4287963 -4.4288249 -4.4288177][-4.4288173 -4.4287367 -4.4286919 -4.4286804 -4.4286733 -4.4286962 -4.4287238 -4.4287086 -4.4286661 -4.4286408 -4.4286494 -4.42869 -4.4287572 -4.4288068 -4.4288211][-4.4288073 -4.4287496 -4.4287343 -4.4287438 -4.4287391 -4.4287424 -4.4287457 -4.4287105 -4.4286475 -4.4285994 -4.4285908 -4.4286304 -4.4287086 -4.4287667 -4.4287953][-4.4287734 -4.4287233 -4.4287271 -4.4287558 -4.4287624 -4.4287567 -4.428741 -4.4287047 -4.4286351 -4.4285645 -4.4285221 -4.4285364 -4.4286132 -4.4286962 -4.4287519][-4.4287338 -4.4286666 -4.4286685 -4.4287105 -4.4287362 -4.4287353 -4.4287152 -4.4286809 -4.4286003 -4.4285035 -4.4284215 -4.4283962 -4.4284792 -4.4286108 -4.428721][-4.4287133 -4.4286242 -4.4286013 -4.4286308 -4.4286523 -4.4286518 -4.4286342 -4.4286032 -4.42853 -4.4284286 -4.4283195 -4.4282656 -4.4283557 -4.4285326 -4.4286914][-4.4287114 -4.4286032 -4.4285426 -4.4285326 -4.4285226 -4.4285064 -4.4284954 -4.4284925 -4.4284592 -4.4283872 -4.4282808 -4.4282112 -4.4282904 -4.428484 -4.4286637][-4.4287248 -4.4286003 -4.4285069 -4.4284582 -4.4284358 -4.428422 -4.4284368 -4.4284759 -4.4284863 -4.428453 -4.4283743 -4.4283104 -4.42836 -4.4285097 -4.4286604][-4.4287605 -4.4286313 -4.4285364 -4.428493 -4.4284878 -4.4284949 -4.4285364 -4.4285917 -4.4286213 -4.42861 -4.4285583 -4.4285107 -4.4285212 -4.4285989 -4.4286866][-4.428793 -4.4286828 -4.4286118 -4.4285936 -4.4286008 -4.4286218 -4.4286666 -4.4287176 -4.4287505 -4.4287529 -4.4287176 -4.4286733 -4.4286551 -4.4286742 -4.4287133][-4.4288235 -4.4287429 -4.4287 -4.4286985 -4.4287133 -4.4287338 -4.428771 -4.4288116 -4.428844 -4.4288568 -4.4288354 -4.4287934 -4.4287605 -4.4287472 -4.4287562][-4.4288526 -4.428802 -4.4287767 -4.4287815 -4.4287963 -4.4288125 -4.4288383 -4.4288645 -4.42889 -4.4289083 -4.4289007 -4.4288716 -4.4288383 -4.428813 -4.4288068][-4.4288826 -4.428853 -4.4288421 -4.428853 -4.4288669 -4.4288788 -4.4288945 -4.4289093 -4.4289255 -4.4289427 -4.4289441 -4.428926 -4.4289012 -4.4288807 -4.4288688][-4.4289083 -4.428894 -4.4288969 -4.4289074 -4.428916 -4.4289188 -4.4289241 -4.4289317 -4.4289446 -4.4289603 -4.428967 -4.4289589 -4.4289436 -4.4289303 -4.4289193][-4.4289217 -4.4289174 -4.428925 -4.4289331 -4.4289379 -4.428936 -4.4289351 -4.4289384 -4.428946 -4.4289546 -4.4289579 -4.4289517 -4.4289427 -4.4289331 -4.4289227]]...]
INFO - root - 2017-12-08 07:12:30.012447: step 37010, loss = 2.28, batch loss = 2.23 (36.7 examples/sec; 0.218 sec/batch; 17h:53m:50s remains)
INFO - root - 2017-12-08 07:12:32.257599: step 37020, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.220 sec/batch; 18h:05m:02s remains)
INFO - root - 2017-12-08 07:12:34.501546: step 37030, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.225 sec/batch; 18h:28m:49s remains)
INFO - root - 2017-12-08 07:12:36.723841: step 37040, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.222 sec/batch; 18h:12m:35s remains)
INFO - root - 2017-12-08 07:12:38.975809: step 37050, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.225 sec/batch; 18h:28m:23s remains)
INFO - root - 2017-12-08 07:12:41.201549: step 37060, loss = 2.28, batch loss = 2.23 (37.2 examples/sec; 0.215 sec/batch; 17h:38m:05s remains)
INFO - root - 2017-12-08 07:12:43.433723: step 37070, loss = 2.28, batch loss = 2.23 (36.6 examples/sec; 0.219 sec/batch; 17h:56m:03s remains)
INFO - root - 2017-12-08 07:12:45.671300: step 37080, loss = 2.28, batch loss = 2.23 (35.2 examples/sec; 0.227 sec/batch; 18h:38m:04s remains)
INFO - root - 2017-12-08 07:12:47.905183: step 37090, loss = 2.28, batch loss = 2.23 (35.3 examples/sec; 0.227 sec/batch; 18h:35m:40s remains)
INFO - root - 2017-12-08 07:12:50.124059: step 37100, loss = 2.28, batch loss = 2.23 (35.4 examples/sec; 0.226 sec/batch; 18h:32m:19s remains)
2017-12-08 07:12:50.417401: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289522 -4.42892 -4.4288831 -4.4288478 -4.4288158 -4.42879 -4.428761 -4.4286823 -4.4285851 -4.4285407 -4.428597 -4.4286909 -4.4287729 -4.4288239 -4.4288421][-4.42899 -4.4289665 -4.4289432 -4.4289188 -4.4288893 -4.4288535 -4.4288058 -4.4287162 -4.42861 -4.4285626 -4.4286156 -4.4287176 -4.4288125 -4.4288721 -4.42889][-4.4289904 -4.4289761 -4.42897 -4.4289665 -4.4289551 -4.4289269 -4.4288783 -4.4287977 -4.4287 -4.4286427 -4.4286723 -4.4287581 -4.4288449 -4.4289021 -4.4289217][-4.4289546 -4.4289374 -4.4289336 -4.4289451 -4.4289527 -4.4289494 -4.42893 -4.4288864 -4.428822 -4.4287667 -4.4287682 -4.42882 -4.4288764 -4.4289136 -4.42893][-4.4288807 -4.4288335 -4.4287939 -4.4287891 -4.4288082 -4.4288445 -4.4288855 -4.4289041 -4.4288945 -4.4288673 -4.4288616 -4.4288893 -4.4289165 -4.4289322 -4.4289412][-4.4287972 -4.4287043 -4.4286156 -4.4285779 -4.4285927 -4.4286685 -4.4287705 -4.4288483 -4.4288874 -4.428896 -4.4289012 -4.4289246 -4.4289403 -4.4289489 -4.4289565][-4.4287114 -4.4285774 -4.4284477 -4.4283695 -4.4283714 -4.4284806 -4.428627 -4.4287367 -4.428812 -4.428854 -4.4288874 -4.4289222 -4.4289436 -4.4289565 -4.4289651][-4.4286475 -4.4284806 -4.4283171 -4.4282 -4.4281878 -4.428329 -4.4285035 -4.4286351 -4.4287314 -4.4287968 -4.4288478 -4.4288955 -4.4289293 -4.42895 -4.4289608][-4.42866 -4.4284925 -4.42831 -4.4281464 -4.4281044 -4.4282303 -4.4284081 -4.428566 -4.428688 -4.428772 -4.4288239 -4.4288697 -4.4289083 -4.4289322 -4.4289436][-4.4287529 -4.4286251 -4.428462 -4.4282885 -4.42821 -4.428278 -4.4284148 -4.4285583 -4.4286766 -4.4287663 -4.4288225 -4.428864 -4.4288993 -4.4289222 -4.4289289][-4.4288697 -4.428793 -4.4286728 -4.4285421 -4.4284821 -4.4285059 -4.428575 -4.4286551 -4.4287257 -4.428792 -4.4288468 -4.4288859 -4.4289141 -4.4289293 -4.4289279][-4.4289417 -4.4289055 -4.4288387 -4.42877 -4.4287438 -4.4287481 -4.4287682 -4.428791 -4.4288139 -4.4288468 -4.4288831 -4.4289141 -4.4289336 -4.428936 -4.4289231][-4.4289532 -4.4289412 -4.4289155 -4.4288912 -4.4288836 -4.4288826 -4.4288831 -4.4288845 -4.4288845 -4.4288955 -4.4289136 -4.4289308 -4.428936 -4.4289269 -4.4289103][-4.4289384 -4.4289389 -4.4289331 -4.4289341 -4.4289327 -4.4289279 -4.4289241 -4.4289212 -4.4289179 -4.4289246 -4.4289322 -4.4289355 -4.4289308 -4.4289174 -4.4289026][-4.4289336 -4.4289355 -4.4289365 -4.4289408 -4.4289432 -4.4289408 -4.4289384 -4.4289365 -4.4289355 -4.4289393 -4.4289427 -4.4289412 -4.4289351 -4.4289265 -4.4289184]]...]
INFO - root - 2017-12-08 07:12:52.655667: step 37110, loss = 2.28, batch loss = 2.23 (35.1 examples/sec; 0.228 sec/batch; 18h:40m:40s remains)
INFO - root - 2017-12-08 07:12:54.906006: step 37120, loss = 2.28, batch loss = 2.23 (35.6 examples/sec; 0.225 sec/batch; 18h:26m:11s remains)
INFO - root - 2017-12-08 07:12:57.133012: step 37130, loss = 2.28, batch loss = 2.23 (35.1 examples/sec; 0.228 sec/batch; 18h:41m:09s remains)
INFO - root - 2017-12-08 07:12:59.401506: step 37140, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.225 sec/batch; 18h:29m:33s remains)
INFO - root - 2017-12-08 07:13:01.625780: step 37150, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.221 sec/batch; 18h:06m:16s remains)
INFO - root - 2017-12-08 07:13:03.888915: step 37160, loss = 2.28, batch loss = 2.23 (35.3 examples/sec; 0.227 sec/batch; 18h:35m:11s remains)
INFO - root - 2017-12-08 07:13:06.106273: step 37170, loss = 2.28, batch loss = 2.23 (37.2 examples/sec; 0.215 sec/batch; 17h:38m:33s remains)
INFO - root - 2017-12-08 07:13:08.379344: step 37180, loss = 2.28, batch loss = 2.23 (33.4 examples/sec; 0.240 sec/batch; 19h:40m:34s remains)
INFO - root - 2017-12-08 07:13:10.603244: step 37190, loss = 2.28, batch loss = 2.23 (35.2 examples/sec; 0.227 sec/batch; 18h:38m:17s remains)
INFO - root - 2017-12-08 07:13:12.827839: step 37200, loss = 2.28, batch loss = 2.23 (36.9 examples/sec; 0.217 sec/batch; 17h:47m:52s remains)
2017-12-08 07:13:13.104922: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289403 -4.4289379 -4.4289303 -4.4289246 -4.4289289 -4.4289455 -4.4289589 -4.428957 -4.4289446 -4.4289351 -4.4289331 -4.4289389 -4.4289446 -4.428946 -4.4289412][-4.4288859 -4.4288669 -4.428844 -4.42883 -4.4288383 -4.4288707 -4.428896 -4.4288964 -4.4288816 -4.4288697 -4.4288654 -4.4288716 -4.4288793 -4.42888 -4.4288745][-4.42882 -4.4287829 -4.4287453 -4.4287252 -4.4287395 -4.4287906 -4.4288287 -4.4288325 -4.4288177 -4.4288044 -4.4287972 -4.4288025 -4.4288106 -4.4288077 -4.4288006][-4.4287748 -4.4287233 -4.428678 -4.4286585 -4.4286809 -4.4287472 -4.4287934 -4.4287958 -4.4287777 -4.428762 -4.4287529 -4.4287572 -4.4287639 -4.4287558 -4.4287453][-4.4287677 -4.4287086 -4.4286628 -4.4286494 -4.4286785 -4.4287434 -4.4287848 -4.4287791 -4.4287577 -4.4287424 -4.4287367 -4.4287434 -4.4287481 -4.4287372 -4.4287257][-4.4287796 -4.4287257 -4.428688 -4.4286838 -4.4287119 -4.4287586 -4.4287825 -4.4287729 -4.4287548 -4.4287434 -4.428741 -4.428751 -4.4287534 -4.4287405 -4.4287295][-4.4287772 -4.4287319 -4.4287052 -4.4287105 -4.4287391 -4.42877 -4.4287796 -4.4287686 -4.4287572 -4.4287539 -4.4287558 -4.4287653 -4.4287639 -4.4287472 -4.4287367][-4.4287505 -4.4287105 -4.4286857 -4.4286904 -4.4287195 -4.4287505 -4.4287591 -4.4287481 -4.428741 -4.4287491 -4.4287605 -4.4287677 -4.4287629 -4.4287486 -4.4287381][-4.4287081 -4.4286728 -4.4286466 -4.4286456 -4.4286752 -4.4287162 -4.4287314 -4.4287195 -4.4287143 -4.4287319 -4.4287519 -4.4287581 -4.428751 -4.4287391 -4.4287286][-4.4286885 -4.4286613 -4.4286356 -4.4286318 -4.4286585 -4.4287024 -4.4287229 -4.4287138 -4.42871 -4.4287267 -4.4287457 -4.4287491 -4.428741 -4.42873 -4.4287214][-4.4287157 -4.4286976 -4.428678 -4.4286742 -4.4286962 -4.4287343 -4.4287515 -4.4287434 -4.4287386 -4.4287477 -4.4287581 -4.4287605 -4.4287534 -4.4287443 -4.4287338][-4.4287791 -4.4287682 -4.4287567 -4.4287548 -4.4287686 -4.4287939 -4.4288039 -4.4287968 -4.4287868 -4.4287858 -4.4287906 -4.428792 -4.4287877 -4.4287782 -4.4287658][-4.4288549 -4.4288516 -4.4288459 -4.4288435 -4.4288492 -4.4288626 -4.4288659 -4.4288583 -4.428844 -4.4288368 -4.4288383 -4.4288421 -4.4288406 -4.4288297 -4.4288182][-4.42892 -4.4289217 -4.4289193 -4.428915 -4.4289131 -4.428916 -4.4289141 -4.4289088 -4.4288983 -4.4288936 -4.4288979 -4.428905 -4.428905 -4.428896 -4.4288864][-4.4289594 -4.4289627 -4.4289608 -4.4289551 -4.4289494 -4.4289455 -4.4289427 -4.4289408 -4.4289365 -4.428936 -4.4289432 -4.4289508 -4.4289513 -4.4289446 -4.4289379]]...]
INFO - root - 2017-12-08 07:13:15.303902: step 37210, loss = 2.28, batch loss = 2.23 (35.0 examples/sec; 0.229 sec/batch; 18h:45m:41s remains)
INFO - root - 2017-12-08 07:13:17.577952: step 37220, loss = 2.28, batch loss = 2.23 (34.5 examples/sec; 0.232 sec/batch; 18h:59m:50s remains)
INFO - root - 2017-12-08 07:13:19.796616: step 37230, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 18h:08m:10s remains)
INFO - root - 2017-12-08 07:13:22.027289: step 37240, loss = 2.28, batch loss = 2.23 (36.9 examples/sec; 0.217 sec/batch; 17h:45m:39s remains)
INFO - root - 2017-12-08 07:13:24.287562: step 37250, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 18h:01m:14s remains)
INFO - root - 2017-12-08 07:13:26.514739: step 37260, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.220 sec/batch; 18h:04m:27s remains)
INFO - root - 2017-12-08 07:13:28.751016: step 37270, loss = 2.28, batch loss = 2.23 (34.6 examples/sec; 0.231 sec/batch; 18h:59m:04s remains)
INFO - root - 2017-12-08 07:13:30.976701: step 37280, loss = 2.28, batch loss = 2.23 (35.0 examples/sec; 0.228 sec/batch; 18h:43m:57s remains)
INFO - root - 2017-12-08 07:13:33.268107: step 37290, loss = 2.28, batch loss = 2.23 (33.0 examples/sec; 0.243 sec/batch; 19h:53m:49s remains)
INFO - root - 2017-12-08 07:13:35.522099: step 37300, loss = 2.28, batch loss = 2.23 (36.9 examples/sec; 0.217 sec/batch; 17h:47m:29s remains)
2017-12-08 07:13:35.805543: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289846 -4.4289846 -4.4289856 -4.4289875 -4.42899 -4.4289942 -4.428997 -4.4289956 -4.4289908 -4.4289842 -4.4289804 -4.4289794 -4.4289808 -4.4289832 -4.4289846][-4.4289894 -4.4289813 -4.42898 -4.4289813 -4.4289885 -4.4289985 -4.4290037 -4.4289975 -4.4289842 -4.4289718 -4.4289646 -4.4289637 -4.428967 -4.4289742 -4.4289794][-4.4289641 -4.4289417 -4.4289351 -4.4289365 -4.42895 -4.4289641 -4.4289651 -4.4289494 -4.428925 -4.4289074 -4.4289017 -4.4289045 -4.4289179 -4.4289346 -4.428947][-4.4289017 -4.428865 -4.4288578 -4.4288607 -4.4288731 -4.4288812 -4.4288707 -4.4288464 -4.4288168 -4.4287968 -4.4287949 -4.4288139 -4.4288454 -4.4288759 -4.4289007][-4.4288149 -4.4287705 -4.4287667 -4.4287672 -4.4287524 -4.4287281 -4.4286971 -4.4286542 -4.4286165 -4.4286041 -4.4286351 -4.4286981 -4.4287558 -4.4287953 -4.428833][-4.4287291 -4.4286809 -4.4286819 -4.42868 -4.4286265 -4.4285545 -4.4284954 -4.4284353 -4.4283962 -4.4284039 -4.4284868 -4.4285965 -4.428678 -4.4287295 -4.428781][-4.428668 -4.4286289 -4.4286442 -4.4286313 -4.428535 -4.428412 -4.4283223 -4.4282603 -4.4282489 -4.4282975 -4.4284134 -4.4285407 -4.4286284 -4.4286938 -4.4287624][-4.4286489 -4.4286208 -4.4286432 -4.4286165 -4.4284906 -4.4283361 -4.428226 -4.4281735 -4.4282088 -4.4282994 -4.4284267 -4.4285526 -4.4286447 -4.4287148 -4.4287858][-4.4286852 -4.4286633 -4.4286914 -4.4286633 -4.4285421 -4.4284077 -4.428309 -4.4282541 -4.428297 -4.4283991 -4.4285245 -4.4286423 -4.428731 -4.4287949 -4.4288549][-4.4287753 -4.4287562 -4.4287767 -4.4287539 -4.42867 -4.4285831 -4.4284987 -4.4284363 -4.4284592 -4.428545 -4.4286618 -4.42877 -4.4288383 -4.4288797 -4.4289241][-4.4288764 -4.428865 -4.4288692 -4.4288449 -4.4287872 -4.4287353 -4.4286761 -4.4286308 -4.4286427 -4.4287047 -4.4287963 -4.4288836 -4.4289269 -4.428946 -4.4289742][-4.4289594 -4.4289575 -4.4289465 -4.4289079 -4.4288535 -4.42882 -4.428792 -4.4287744 -4.428793 -4.4288392 -4.4289017 -4.4289546 -4.428977 -4.4289851 -4.4290023][-4.429 -4.4290066 -4.428988 -4.4289446 -4.4288931 -4.4288664 -4.4288526 -4.4288535 -4.42889 -4.4289355 -4.4289742 -4.4290018 -4.4290104 -4.4290113 -4.4290161][-4.42902 -4.4290314 -4.4290137 -4.4289694 -4.4289174 -4.4288864 -4.4288735 -4.4288878 -4.4289269 -4.4289694 -4.4290061 -4.4290266 -4.42903 -4.4290252 -4.4290223][-4.4290233 -4.4290323 -4.4290142 -4.4289665 -4.428915 -4.4288807 -4.4288678 -4.4288898 -4.4289312 -4.4289694 -4.4290056 -4.4290266 -4.4290223 -4.4290133 -4.4290118]]...]
INFO - root - 2017-12-08 07:13:38.027911: step 37310, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.226 sec/batch; 18h:29m:28s remains)
INFO - root - 2017-12-08 07:13:40.259291: step 37320, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 17h:58m:32s remains)
INFO - root - 2017-12-08 07:13:42.482213: step 37330, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 18h:05m:46s remains)
INFO - root - 2017-12-08 07:13:44.734884: step 37340, loss = 2.28, batch loss = 2.23 (37.6 examples/sec; 0.213 sec/batch; 17h:25m:31s remains)
INFO - root - 2017-12-08 07:13:46.971964: step 37350, loss = 2.28, batch loss = 2.23 (35.8 examples/sec; 0.223 sec/batch; 18h:19m:00s remains)
INFO - root - 2017-12-08 07:13:49.200600: step 37360, loss = 2.28, batch loss = 2.23 (35.7 examples/sec; 0.224 sec/batch; 18h:22m:17s remains)
INFO - root - 2017-12-08 07:13:51.430341: step 37370, loss = 2.28, batch loss = 2.23 (34.7 examples/sec; 0.230 sec/batch; 18h:53m:12s remains)
INFO - root - 2017-12-08 07:13:53.660154: step 37380, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.221 sec/batch; 18h:08m:44s remains)
INFO - root - 2017-12-08 07:13:55.872735: step 37390, loss = 2.28, batch loss = 2.23 (35.3 examples/sec; 0.226 sec/batch; 18h:33m:14s remains)
INFO - root - 2017-12-08 07:13:58.093188: step 37400, loss = 2.28, batch loss = 2.23 (34.9 examples/sec; 0.229 sec/batch; 18h:46m:27s remains)
2017-12-08 07:13:58.402723: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.42887 -4.4288645 -4.4288783 -4.4289055 -4.4289312 -4.4289479 -4.428957 -4.4289489 -4.4289284 -4.4289083 -4.4288898 -4.42885 -4.4287925 -4.4287477 -4.4287176][-4.4289227 -4.4289255 -4.4289379 -4.4289536 -4.428968 -4.428977 -4.4289784 -4.4289656 -4.4289422 -4.428916 -4.4288883 -4.4288397 -4.4287715 -4.4287238 -4.4286962][-4.4289384 -4.4289417 -4.4289527 -4.4289575 -4.4289579 -4.4289532 -4.4289417 -4.4289246 -4.428906 -4.4288855 -4.4288611 -4.4288135 -4.4287462 -4.4287 -4.4286766][-4.4289222 -4.4289193 -4.4289241 -4.4289169 -4.4289 -4.4288735 -4.4288449 -4.4288249 -4.4288211 -4.42882 -4.4288139 -4.4287758 -4.42872 -4.4286842 -4.4286733][-4.4288645 -4.4288478 -4.4288425 -4.4288225 -4.4287777 -4.4287143 -4.4286594 -4.428647 -4.4286819 -4.4287114 -4.4287281 -4.4287052 -4.428669 -4.4286609 -4.4286742][-4.4287624 -4.4287167 -4.4287047 -4.4286776 -4.4285984 -4.4284744 -4.4283571 -4.4283657 -4.42848 -4.4285626 -4.428597 -4.4285736 -4.4285455 -4.4285769 -4.42863][-4.4286432 -4.428565 -4.4285645 -4.4285469 -4.4284425 -4.4282427 -4.4280233 -4.428061 -4.4282904 -4.4284363 -4.4284687 -4.4284148 -4.4283733 -4.428443 -4.4285417][-4.4285502 -4.42846 -4.4284849 -4.4284873 -4.4283867 -4.42816 -4.4278927 -4.4279442 -4.4282308 -4.4284148 -4.4284453 -4.4283633 -4.4283013 -4.4283748 -4.4284873][-4.4285297 -4.4284782 -4.4285188 -4.4285278 -4.4284587 -4.4283018 -4.4281244 -4.4281559 -4.428371 -4.428534 -4.4285712 -4.4284968 -4.42842 -4.4284387 -4.4285169][-4.4285736 -4.4285717 -4.4286132 -4.4286275 -4.428597 -4.4285245 -4.4284415 -4.4284472 -4.4285707 -4.4286847 -4.4287224 -4.428669 -4.428597 -4.4285769 -4.428616][-4.4286551 -4.4286776 -4.4287109 -4.4287291 -4.4287248 -4.4286985 -4.4286642 -4.4286623 -4.4287248 -4.4287953 -4.4288254 -4.4287996 -4.4287534 -4.4287252 -4.4287438][-4.4287524 -4.428772 -4.4287915 -4.42881 -4.4288239 -4.4288182 -4.4288068 -4.4288125 -4.428844 -4.4288821 -4.4289031 -4.4288955 -4.4288697 -4.42885 -4.4288592][-4.42883 -4.4288421 -4.428853 -4.4288683 -4.4288883 -4.4288945 -4.4288926 -4.4288993 -4.4289155 -4.4289355 -4.4289474 -4.4289432 -4.4289308 -4.4289236 -4.4289312][-4.4288778 -4.4288874 -4.4288945 -4.4289079 -4.4289265 -4.428936 -4.4289365 -4.4289384 -4.4289427 -4.4289522 -4.4289594 -4.428957 -4.42895 -4.428947 -4.4289565][-4.428906 -4.428916 -4.4289265 -4.4289389 -4.4289503 -4.4289536 -4.4289474 -4.4289422 -4.4289412 -4.4289455 -4.4289508 -4.4289513 -4.4289484 -4.42895 -4.42896]]...]
INFO - root - 2017-12-08 07:14:00.678342: step 37410, loss = 2.28, batch loss = 2.23 (34.9 examples/sec; 0.229 sec/batch; 18h:46m:41s remains)
INFO - root - 2017-12-08 07:14:02.908104: step 37420, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 17h:58m:21s remains)
INFO - root - 2017-12-08 07:14:05.135160: step 37430, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 17h:57m:53s remains)
INFO - root - 2017-12-08 07:14:07.412604: step 37440, loss = 2.28, batch loss = 2.23 (34.0 examples/sec; 0.235 sec/batch; 19h:16m:24s remains)
INFO - root - 2017-12-08 07:14:09.676710: step 37450, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 18h:15m:47s remains)
INFO - root - 2017-12-08 07:14:11.907071: step 37460, loss = 2.28, batch loss = 2.23 (35.4 examples/sec; 0.226 sec/batch; 18h:30m:02s remains)
INFO - root - 2017-12-08 07:14:14.131318: step 37470, loss = 2.28, batch loss = 2.23 (36.9 examples/sec; 0.217 sec/batch; 17h:47m:20s remains)
INFO - root - 2017-12-08 07:14:16.357950: step 37480, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 17h:59m:33s remains)
INFO - root - 2017-12-08 07:14:18.669803: step 37490, loss = 2.28, batch loss = 2.23 (34.6 examples/sec; 0.231 sec/batch; 18h:58m:01s remains)
INFO - root - 2017-12-08 07:14:20.892419: step 37500, loss = 2.28, batch loss = 2.23 (36.0 examples/sec; 0.222 sec/batch; 18h:11m:30s remains)
2017-12-08 07:14:21.185711: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4285841 -4.4285669 -4.4285841 -4.4286375 -4.4286819 -4.4287119 -4.4287229 -4.4287238 -4.4287133 -4.4287062 -4.4287205 -4.4287205 -4.4287019 -4.4286838 -4.4286561][-4.4285789 -4.4285665 -4.4285908 -4.4286528 -4.4287081 -4.4287415 -4.4287453 -4.4287362 -4.4287243 -4.4287086 -4.4287176 -4.4287152 -4.4286952 -4.4286842 -4.428659][-4.42861 -4.4285917 -4.4286084 -4.4286637 -4.4287238 -4.4287562 -4.428751 -4.4287281 -4.4287162 -4.4287043 -4.4287157 -4.4287143 -4.4287 -4.4286904 -4.4286609][-4.4286079 -4.4285922 -4.428618 -4.4286766 -4.4287291 -4.4287424 -4.4287171 -4.4286804 -4.428668 -4.4286737 -4.4287124 -4.4287324 -4.4287376 -4.4287329 -4.4286976][-4.4285851 -4.4285903 -4.4286356 -4.4286895 -4.4287119 -4.4286861 -4.4286327 -4.4285865 -4.4285836 -4.4286232 -4.4287071 -4.4287653 -4.4287906 -4.4287896 -4.4287519][-4.4285812 -4.4286027 -4.4286466 -4.428668 -4.4286413 -4.4285607 -4.4284706 -4.428421 -4.42846 -4.4285636 -4.4286971 -4.4287796 -4.4288106 -4.4288044 -4.4287682][-4.4285879 -4.4286156 -4.4286504 -4.4286351 -4.4285588 -4.4284291 -4.4283071 -4.4282646 -4.4283609 -4.4285192 -4.4286709 -4.4287558 -4.4287877 -4.4287815 -4.4287529][-4.4286084 -4.428648 -4.428679 -4.4286494 -4.4285541 -4.428412 -4.4282894 -4.4282603 -4.4283748 -4.4285336 -4.428669 -4.4287434 -4.4287772 -4.4287758 -4.4287534][-4.4286628 -4.4287119 -4.4287438 -4.4287167 -4.4286337 -4.4285131 -4.4284143 -4.4283862 -4.4284716 -4.428597 -4.4287024 -4.4287596 -4.4287863 -4.4287858 -4.428771][-4.4287462 -4.42878 -4.4287949 -4.4287682 -4.428709 -4.4286251 -4.4285545 -4.4285326 -4.4285922 -4.4286833 -4.4287543 -4.4287858 -4.4287968 -4.4287915 -4.428781][-4.4287896 -4.4287972 -4.4287958 -4.428772 -4.4287348 -4.4286895 -4.428658 -4.4286571 -4.4287066 -4.4287667 -4.4287977 -4.4287992 -4.428791 -4.4287782 -4.4287682][-4.42879 -4.428772 -4.4287596 -4.4287448 -4.4287338 -4.4287233 -4.4287252 -4.4287429 -4.4287782 -4.4288039 -4.4287968 -4.4287753 -4.4287639 -4.4287629 -4.4287663][-4.4287806 -4.4287524 -4.42874 -4.4287386 -4.4287524 -4.4287663 -4.4287806 -4.4287939 -4.4288058 -4.4288073 -4.4287858 -4.4287634 -4.4287639 -4.42878 -4.4287958][-4.4287329 -4.428699 -4.4287 -4.4287295 -4.4287734 -4.4288106 -4.4288287 -4.428823 -4.4288106 -4.4287992 -4.4287839 -4.4287767 -4.4287925 -4.4288163 -4.4288321][-4.4286361 -4.4286013 -4.428628 -4.4287014 -4.4287848 -4.4288487 -4.4288731 -4.4288549 -4.4288282 -4.4288096 -4.4287968 -4.4288011 -4.4288254 -4.4288473 -4.4288588]]...]
INFO - root - 2017-12-08 07:14:23.401863: step 37510, loss = 2.28, batch loss = 2.23 (35.4 examples/sec; 0.226 sec/batch; 18h:31m:05s remains)
INFO - root - 2017-12-08 07:14:25.640345: step 37520, loss = 2.28, batch loss = 2.23 (36.0 examples/sec; 0.222 sec/batch; 18h:11m:13s remains)
INFO - root - 2017-12-08 07:14:27.868986: step 37530, loss = 2.28, batch loss = 2.23 (34.9 examples/sec; 0.229 sec/batch; 18h:46m:12s remains)
INFO - root - 2017-12-08 07:14:30.094467: step 37540, loss = 2.28, batch loss = 2.23 (35.2 examples/sec; 0.227 sec/batch; 18h:36m:18s remains)
INFO - root - 2017-12-08 07:14:32.321901: step 37550, loss = 2.28, batch loss = 2.23 (37.5 examples/sec; 0.214 sec/batch; 17h:29m:32s remains)
INFO - root - 2017-12-08 07:14:34.554325: step 37560, loss = 2.28, batch loss = 2.23 (36.6 examples/sec; 0.218 sec/batch; 17h:53m:07s remains)
INFO - root - 2017-12-08 07:14:36.794349: step 37570, loss = 2.28, batch loss = 2.23 (35.8 examples/sec; 0.224 sec/batch; 18h:19m:27s remains)
INFO - root - 2017-12-08 07:14:39.040822: step 37580, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 17h:58m:58s remains)
INFO - root - 2017-12-08 07:14:41.236667: step 37590, loss = 2.28, batch loss = 2.23 (36.6 examples/sec; 0.218 sec/batch; 17h:53m:22s remains)
INFO - root - 2017-12-08 07:14:43.507416: step 37600, loss = 2.28, batch loss = 2.23 (33.3 examples/sec; 0.240 sec/batch; 19h:39m:13s remains)
2017-12-08 07:14:43.794638: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288406 -4.4288383 -4.4288411 -4.4288712 -4.4288955 -4.4288917 -4.4288673 -4.4288378 -4.4288082 -4.4287815 -4.4287705 -4.4287691 -4.4287586 -4.428762 -4.4287963][-4.4287634 -4.4287663 -4.4287758 -4.4288216 -4.4288554 -4.4288535 -4.4288282 -4.4287891 -4.4287496 -4.4287152 -4.4287124 -4.4287281 -4.4287329 -4.428751 -4.4287868][-4.4286747 -4.4286814 -4.4286995 -4.4287639 -4.4288092 -4.42881 -4.42879 -4.4287515 -4.4287133 -4.4286771 -4.428679 -4.4287152 -4.4287395 -4.4287677 -4.4287944][-4.4285779 -4.4285808 -4.4285994 -4.4286718 -4.4287219 -4.4287248 -4.4287171 -4.4286928 -4.4286728 -4.428658 -4.4286776 -4.4287233 -4.4287663 -4.4287992 -4.4288149][-4.428504 -4.4284964 -4.4285064 -4.4285679 -4.4286151 -4.428618 -4.4286165 -4.4286017 -4.4286022 -4.4286261 -4.4286747 -4.4287238 -4.428772 -4.4288135 -4.428834][-4.4284821 -4.428463 -4.4284606 -4.4284992 -4.42854 -4.4285364 -4.4285221 -4.4284954 -4.428504 -4.4285622 -4.4286308 -4.4286838 -4.4287419 -4.4287953 -4.4288263][-4.4284768 -4.428452 -4.4284458 -4.4284611 -4.4284792 -4.4284606 -4.4284158 -4.4283671 -4.4283814 -4.428473 -4.4285622 -4.4286246 -4.4286885 -4.4287524 -4.4287949][-4.4284625 -4.428442 -4.4284496 -4.42845 -4.42843 -4.428369 -4.428278 -4.4282055 -4.428237 -4.4283605 -4.4284744 -4.4285502 -4.4286146 -4.42868 -4.4287291][-4.4284587 -4.4284377 -4.4284663 -4.4284768 -4.4284339 -4.4283257 -4.4281859 -4.4280829 -4.4281316 -4.4282732 -4.4284005 -4.4284878 -4.4285555 -4.4286208 -4.4286728][-4.4285183 -4.4284883 -4.428525 -4.4285531 -4.4285274 -4.4284186 -4.4282713 -4.4281492 -4.4281754 -4.4282885 -4.4284053 -4.4284987 -4.4285784 -4.4286466 -4.4286938][-4.4286427 -4.4286084 -4.4286318 -4.4286618 -4.4286547 -4.4285741 -4.4284682 -4.4283695 -4.4283628 -4.4284167 -4.4284973 -4.4285855 -4.428669 -4.4287305 -4.4287729][-4.4288039 -4.4287739 -4.4287815 -4.4287992 -4.428791 -4.428731 -4.4286671 -4.4286108 -4.4285927 -4.4286003 -4.4286427 -4.4287152 -4.4287915 -4.428844 -4.428875][-4.428915 -4.428905 -4.42891 -4.4289179 -4.42891 -4.428875 -4.4288445 -4.4288197 -4.4288058 -4.4287944 -4.4288096 -4.4288564 -4.4289083 -4.4289465 -4.4289646][-4.4289527 -4.4289579 -4.4289646 -4.4289708 -4.428968 -4.4289565 -4.428946 -4.4289384 -4.4289346 -4.4289265 -4.4289346 -4.4289613 -4.4289885 -4.4290109 -4.42902][-4.4289594 -4.4289651 -4.4289675 -4.4289684 -4.428966 -4.4289646 -4.4289637 -4.4289651 -4.42897 -4.4289718 -4.4289784 -4.4289923 -4.4290066 -4.4290214 -4.4290285]]...]
INFO - root - 2017-12-08 07:14:46.023778: step 37610, loss = 2.28, batch loss = 2.23 (35.3 examples/sec; 0.226 sec/batch; 18h:32m:45s remains)
INFO - root - 2017-12-08 07:14:48.310451: step 37620, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.222 sec/batch; 18h:09m:02s remains)
INFO - root - 2017-12-08 07:14:50.566723: step 37630, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 17h:56m:25s remains)
INFO - root - 2017-12-08 07:14:52.801460: step 37640, loss = 2.28, batch loss = 2.23 (35.6 examples/sec; 0.225 sec/batch; 18h:25m:02s remains)
INFO - root - 2017-12-08 07:14:55.068779: step 37650, loss = 2.28, batch loss = 2.23 (35.6 examples/sec; 0.225 sec/batch; 18h:24m:21s remains)
INFO - root - 2017-12-08 07:14:57.296328: step 37660, loss = 2.28, batch loss = 2.23 (35.1 examples/sec; 0.228 sec/batch; 18h:41m:16s remains)
INFO - root - 2017-12-08 07:14:59.544422: step 37670, loss = 2.28, batch loss = 2.23 (36.6 examples/sec; 0.218 sec/batch; 17h:52m:41s remains)
INFO - root - 2017-12-08 07:15:01.828019: step 37680, loss = 2.28, batch loss = 2.23 (35.1 examples/sec; 0.228 sec/batch; 18h:38m:56s remains)
INFO - root - 2017-12-08 07:15:04.081426: step 37690, loss = 2.28, batch loss = 2.23 (35.8 examples/sec; 0.224 sec/batch; 18h:19m:14s remains)
INFO - root - 2017-12-08 07:15:06.317545: step 37700, loss = 2.28, batch loss = 2.23 (34.5 examples/sec; 0.232 sec/batch; 18h:59m:39s remains)
2017-12-08 07:15:06.642546: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288 -4.4287872 -4.4287882 -4.4287987 -4.4288111 -4.4288239 -4.428833 -4.4288416 -4.4288468 -4.428844 -4.4288387 -4.4288368 -4.4288454 -4.4288511 -4.428844][-4.4288063 -4.4288025 -4.4288068 -4.4288125 -4.428812 -4.4288116 -4.428813 -4.4288177 -4.4288259 -4.4288311 -4.4288282 -4.4288263 -4.4288378 -4.4288445 -4.4288363][-4.4288106 -4.428812 -4.4288149 -4.4288068 -4.4287877 -4.4287667 -4.4287543 -4.4287515 -4.4287558 -4.4287677 -4.4287729 -4.4287786 -4.4287934 -4.4288015 -4.4287949][-4.4287882 -4.4287786 -4.4287715 -4.42875 -4.4287133 -4.4286723 -4.4286489 -4.4286375 -4.4286375 -4.4286485 -4.4286613 -4.4286776 -4.428699 -4.4287105 -4.4287019][-4.4287243 -4.4286871 -4.4286628 -4.4286332 -4.4285946 -4.4285455 -4.4285121 -4.4284921 -4.4284821 -4.4284873 -4.4285069 -4.4285378 -4.4285741 -4.4285955 -4.428586][-4.4286342 -4.4285679 -4.4285293 -4.4284997 -4.4284692 -4.4284205 -4.4283738 -4.4283323 -4.4283056 -4.4283128 -4.4283538 -4.4284115 -4.428473 -4.4285049 -4.4284935][-4.4285474 -4.4284663 -4.4284334 -4.4284196 -4.4284034 -4.42835 -4.4282765 -4.4281969 -4.4281435 -4.4281578 -4.4282236 -4.4283128 -4.4284048 -4.4284463 -4.4284267][-4.4285054 -4.4284306 -4.4284172 -4.4284253 -4.4284186 -4.4283619 -4.4282594 -4.4281406 -4.4280629 -4.4280858 -4.4281673 -4.4282746 -4.4283915 -4.4284372 -4.4284048][-4.4285264 -4.4284716 -4.4284797 -4.4284992 -4.4284978 -4.4284487 -4.4283423 -4.4282079 -4.4281197 -4.4281449 -4.4282255 -4.428328 -4.4284396 -4.4284735 -4.4284215][-4.4285917 -4.428556 -4.4285765 -4.4286056 -4.428616 -4.428586 -4.428493 -4.4283652 -4.4282718 -4.428287 -4.4283581 -4.428442 -4.4285121 -4.42851 -4.4284439][-4.4286642 -4.4286432 -4.4286718 -4.4287105 -4.4287343 -4.4287214 -4.42865 -4.4285321 -4.4284339 -4.4284344 -4.4284949 -4.4285493 -4.428565 -4.4285212 -4.42845][-4.4287138 -4.4287038 -4.4287362 -4.4287806 -4.4288158 -4.4288197 -4.428771 -4.428668 -4.4285645 -4.428534 -4.4285593 -4.4285803 -4.4285665 -4.4285088 -4.4284348][-4.4287505 -4.4287453 -4.4287724 -4.4288173 -4.4288588 -4.4288731 -4.4288397 -4.4287562 -4.4286528 -4.4285831 -4.4285603 -4.4285569 -4.428544 -4.4284949 -4.4284325][-4.4287877 -4.428782 -4.4287939 -4.4288263 -4.428864 -4.4288797 -4.4288583 -4.4288 -4.4287176 -4.428638 -4.4285908 -4.4285779 -4.4285655 -4.428534 -4.4284883][-4.4288092 -4.4288011 -4.4287977 -4.4288082 -4.4288316 -4.4288435 -4.4288335 -4.4288 -4.4287462 -4.4286852 -4.4286489 -4.4286456 -4.4286461 -4.4286318 -4.4286051]]...]
INFO - root - 2017-12-08 07:15:08.900792: step 37710, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.222 sec/batch; 18h:08m:25s remains)
INFO - root - 2017-12-08 07:15:11.126606: step 37720, loss = 2.28, batch loss = 2.23 (34.3 examples/sec; 0.233 sec/batch; 19h:05m:10s remains)
INFO - root - 2017-12-08 07:15:13.374228: step 37730, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 18h:01m:13s remains)
INFO - root - 2017-12-08 07:15:15.641979: step 37740, loss = 2.28, batch loss = 2.23 (35.6 examples/sec; 0.225 sec/batch; 18h:23m:43s remains)
INFO - root - 2017-12-08 07:15:17.867743: step 37750, loss = 2.28, batch loss = 2.23 (34.8 examples/sec; 0.230 sec/batch; 18h:50m:19s remains)
INFO - root - 2017-12-08 07:15:20.119882: step 37760, loss = 2.28, batch loss = 2.23 (35.1 examples/sec; 0.228 sec/batch; 18h:39m:43s remains)
INFO - root - 2017-12-08 07:15:22.384264: step 37770, loss = 2.28, batch loss = 2.23 (34.6 examples/sec; 0.231 sec/batch; 18h:54m:27s remains)
INFO - root - 2017-12-08 07:15:24.616520: step 37780, loss = 2.28, batch loss = 2.23 (36.0 examples/sec; 0.222 sec/batch; 18h:12m:12s remains)
INFO - root - 2017-12-08 07:15:26.868318: step 37790, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.222 sec/batch; 18h:09m:06s remains)
INFO - root - 2017-12-08 07:15:29.089517: step 37800, loss = 2.28, batch loss = 2.23 (35.7 examples/sec; 0.224 sec/batch; 18h:21m:06s remains)
2017-12-08 07:15:29.389212: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4287353 -4.4287252 -4.4287157 -4.4286666 -4.4285889 -4.4285159 -4.4284921 -4.4285321 -4.428618 -4.4286981 -4.4287333 -4.4287319 -4.4287262 -4.4287171 -4.4287181][-4.4287086 -4.4287095 -4.4287162 -4.4286923 -4.4286389 -4.4285793 -4.4285583 -4.4285893 -4.4286556 -4.4287124 -4.4287248 -4.4287124 -4.4287047 -4.4286966 -4.4287024][-4.4286866 -4.4287004 -4.4287291 -4.4287386 -4.4287186 -4.4286771 -4.42865 -4.4286585 -4.4286919 -4.4287181 -4.4287128 -4.4286947 -4.4286895 -4.4286857 -4.4286919][-4.4286761 -4.4287043 -4.4287472 -4.4287763 -4.4287791 -4.4287472 -4.4287028 -4.42868 -4.4286866 -4.4286995 -4.4286885 -4.4286742 -4.4286766 -4.4286761 -4.428679][-4.4286833 -4.4287167 -4.4287548 -4.4287853 -4.428792 -4.4287529 -4.4286857 -4.4286375 -4.42864 -4.4286671 -4.4286728 -4.428668 -4.4286757 -4.42867 -4.4286633][-4.4287233 -4.4287443 -4.428761 -4.428781 -4.4287772 -4.428719 -4.4286194 -4.4285436 -4.4285502 -4.4286113 -4.4286475 -4.428658 -4.4286695 -4.4286604 -4.4286466][-4.4287677 -4.4287643 -4.4287562 -4.428761 -4.4287505 -4.4286785 -4.4285469 -4.4284315 -4.4284315 -4.4285316 -4.4286094 -4.4286418 -4.4286551 -4.42864 -4.4286222][-4.4287829 -4.4287491 -4.4287181 -4.4287105 -4.4286966 -4.4286261 -4.4284825 -4.4283352 -4.4283276 -4.4284678 -4.428587 -4.4286451 -4.4286604 -4.4286361 -4.4286122][-4.4287744 -4.4287124 -4.4286585 -4.4286394 -4.4286318 -4.428587 -4.4284816 -4.4283638 -4.42837 -4.4285121 -4.4286318 -4.42869 -4.4286976 -4.4286642 -4.4286289][-4.4287481 -4.4286776 -4.428617 -4.4285979 -4.4286046 -4.4285994 -4.4285564 -4.428503 -4.4285192 -4.4286175 -4.4286966 -4.4287329 -4.4287252 -4.4286761 -4.428628][-4.4287248 -4.4286776 -4.4286366 -4.4286232 -4.4286351 -4.4286451 -4.4286356 -4.42862 -4.4286318 -4.4286833 -4.4287224 -4.4287415 -4.4287271 -4.4286737 -4.4286222][-4.4287233 -4.4287081 -4.4286895 -4.42869 -4.4287033 -4.4287124 -4.4287038 -4.4286871 -4.42868 -4.4286933 -4.4287109 -4.4287271 -4.42872 -4.42868 -4.4286308][-4.4287386 -4.4287462 -4.4287457 -4.4287558 -4.4287653 -4.4287643 -4.4287424 -4.4287095 -4.4286776 -4.4286661 -4.42868 -4.4287009 -4.4287052 -4.4286833 -4.4286447][-4.4287596 -4.428781 -4.4287877 -4.4287963 -4.4287992 -4.4287915 -4.4287667 -4.4287262 -4.4286795 -4.4286461 -4.4286475 -4.4286675 -4.4286828 -4.4286814 -4.4286594][-4.42878 -4.428813 -4.4288254 -4.4288311 -4.4288282 -4.4288192 -4.4287968 -4.4287562 -4.4287009 -4.428648 -4.4286265 -4.4286318 -4.42865 -4.4286618 -4.4286604]]...]
INFO - root - 2017-12-08 07:15:31.648422: step 37810, loss = 2.28, batch loss = 2.23 (34.8 examples/sec; 0.230 sec/batch; 18h:50m:23s remains)
INFO - root - 2017-12-08 07:15:33.894205: step 37820, loss = 2.28, batch loss = 2.23 (34.2 examples/sec; 0.234 sec/batch; 19h:07m:47s remains)
INFO - root - 2017-12-08 07:15:36.137447: step 37830, loss = 2.28, batch loss = 2.23 (35.8 examples/sec; 0.223 sec/batch; 18h:17m:23s remains)
INFO - root - 2017-12-08 07:15:38.410532: step 37840, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 17h:59m:31s remains)
INFO - root - 2017-12-08 07:15:40.637905: step 37850, loss = 2.28, batch loss = 2.23 (37.1 examples/sec; 0.216 sec/batch; 17h:38m:27s remains)
INFO - root - 2017-12-08 07:15:42.889558: step 37860, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.220 sec/batch; 18h:02m:31s remains)
INFO - root - 2017-12-08 07:15:45.145050: step 37870, loss = 2.28, batch loss = 2.23 (35.6 examples/sec; 0.225 sec/batch; 18h:23m:04s remains)
INFO - root - 2017-12-08 07:15:47.375138: step 37880, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.225 sec/batch; 18h:25m:26s remains)
INFO - root - 2017-12-08 07:15:49.604608: step 37890, loss = 2.28, batch loss = 2.23 (37.3 examples/sec; 0.214 sec/batch; 17h:32m:04s remains)
INFO - root - 2017-12-08 07:15:51.834204: step 37900, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.222 sec/batch; 18h:08m:16s remains)
2017-12-08 07:15:52.153102: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289236 -4.4288974 -4.4288468 -4.4287581 -4.4286566 -4.4285517 -4.4284391 -4.4284024 -4.4284687 -4.4285703 -4.4286227 -4.428596 -4.4285359 -4.42851 -4.4285088][-4.4289193 -4.4288883 -4.4288297 -4.4287281 -4.4286132 -4.4284906 -4.4283671 -4.4283447 -4.4284406 -4.4285612 -4.428627 -4.4286184 -4.4285641 -4.4285274 -4.428515][-4.4289203 -4.4288936 -4.4288392 -4.4287448 -4.4286375 -4.4285159 -4.428391 -4.4283724 -4.428472 -4.4285874 -4.4286466 -4.4286475 -4.4286122 -4.4285774 -4.4285488][-4.4289246 -4.4289069 -4.4288616 -4.4287791 -4.4286795 -4.4285645 -4.428441 -4.4284196 -4.4285164 -4.4286184 -4.428669 -4.4286828 -4.4286752 -4.4286513 -4.4286017][-4.42893 -4.428916 -4.4288745 -4.4287925 -4.4286914 -4.4285765 -4.42845 -4.4284091 -4.4284825 -4.4285765 -4.4286218 -4.4286513 -4.4286804 -4.4286809 -4.4286351][-4.4289331 -4.428915 -4.428865 -4.4287682 -4.4286547 -4.4285254 -4.4283824 -4.4282889 -4.4282932 -4.4283662 -4.4284449 -4.428514 -4.4285841 -4.4286137 -4.4285922][-4.428925 -4.42889 -4.4288239 -4.4287052 -4.4285731 -4.4284205 -4.4282475 -4.42808 -4.4279728 -4.4280267 -4.4282026 -4.4283419 -4.4284472 -4.428504 -4.4285107][-4.4288983 -4.42885 -4.4287763 -4.4286423 -4.4284897 -4.4283257 -4.4281359 -4.4279408 -4.4277921 -4.4278545 -4.4281116 -4.4282765 -4.4283819 -4.4284463 -4.4284778][-4.4288774 -4.4288321 -4.42877 -4.4286485 -4.4285097 -4.4283566 -4.4281898 -4.4280639 -4.4280176 -4.4281044 -4.4282918 -4.428401 -4.4284811 -4.4285412 -4.4285784][-4.4288831 -4.4288568 -4.4288177 -4.4287314 -4.4286246 -4.4285 -4.4283757 -4.4283147 -4.4283342 -4.4284191 -4.428514 -4.4285703 -4.4286385 -4.4287009 -4.4287267][-4.4289088 -4.4289036 -4.4288831 -4.4288206 -4.42874 -4.4286461 -4.428556 -4.4285173 -4.428566 -4.4286466 -4.4287033 -4.4287305 -4.4287777 -4.428822 -4.4288282][-4.4289374 -4.4289436 -4.4289317 -4.4288836 -4.4288168 -4.4287381 -4.4286737 -4.4286542 -4.4287038 -4.4287782 -4.4288287 -4.4288492 -4.428874 -4.4288831 -4.428865][-4.4289584 -4.4289637 -4.4289489 -4.4289064 -4.4288535 -4.4287887 -4.4287391 -4.4287353 -4.4287887 -4.4288559 -4.428894 -4.4289026 -4.4289041 -4.4288912 -4.4288578][-4.4289684 -4.428966 -4.4289384 -4.4288898 -4.4288449 -4.428792 -4.4287496 -4.4287581 -4.4288106 -4.42886 -4.4288869 -4.4288888 -4.4288769 -4.428854 -4.42882][-4.428968 -4.4289556 -4.4289179 -4.4288659 -4.4288177 -4.428762 -4.4287233 -4.4287405 -4.4287992 -4.428843 -4.428863 -4.4288664 -4.4288521 -4.42883 -4.428802]]...]
INFO - root - 2017-12-08 07:15:54.385785: step 37910, loss = 2.28, batch loss = 2.23 (36.0 examples/sec; 0.222 sec/batch; 18h:09m:50s remains)
INFO - root - 2017-12-08 07:15:56.610983: step 37920, loss = 2.28, batch loss = 2.23 (36.6 examples/sec; 0.218 sec/batch; 17h:52m:03s remains)
INFO - root - 2017-12-08 07:15:58.852785: step 37930, loss = 2.28, batch loss = 2.23 (35.7 examples/sec; 0.224 sec/batch; 18h:21m:21s remains)
INFO - root - 2017-12-08 07:16:01.060145: step 37940, loss = 2.28, batch loss = 2.23 (36.9 examples/sec; 0.217 sec/batch; 17h:43m:11s remains)
INFO - root - 2017-12-08 07:16:03.306389: step 37950, loss = 2.28, batch loss = 2.23 (35.2 examples/sec; 0.227 sec/batch; 18h:36m:41s remains)
INFO - root - 2017-12-08 07:16:05.545601: step 37960, loss = 2.28, batch loss = 2.23 (35.0 examples/sec; 0.229 sec/batch; 18h:42m:41s remains)
INFO - root - 2017-12-08 07:16:07.768251: step 37970, loss = 2.28, batch loss = 2.23 (36.0 examples/sec; 0.222 sec/batch; 18h:11m:24s remains)
INFO - root - 2017-12-08 07:16:10.024946: step 37980, loss = 2.28, batch loss = 2.23 (37.4 examples/sec; 0.214 sec/batch; 17h:29m:49s remains)
INFO - root - 2017-12-08 07:16:12.255795: step 37990, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 18h:03m:46s remains)
INFO - root - 2017-12-08 07:16:14.479029: step 38000, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.220 sec/batch; 18h:01m:26s remains)
2017-12-08 07:16:14.776142: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4287148 -4.4286957 -4.428699 -4.42869 -4.428669 -4.4286623 -4.4286871 -4.4286919 -4.4286933 -4.4287238 -4.4287529 -4.4287539 -4.4287248 -4.4286742 -4.4286485][-4.4287405 -4.4287133 -4.4286971 -4.4286661 -4.428628 -4.4286175 -4.4286509 -4.4286637 -4.4286571 -4.4286838 -4.4287205 -4.428731 -4.4287052 -4.4286551 -4.4286265][-4.4287648 -4.4287205 -4.4286866 -4.4286418 -4.4285941 -4.4285736 -4.4286151 -4.4286332 -4.4286165 -4.4286332 -4.4286771 -4.4287057 -4.4286985 -4.4286542 -4.4286141][-4.4287848 -4.4287276 -4.4286795 -4.4286385 -4.42859 -4.4285617 -4.4285984 -4.4286056 -4.4285626 -4.4285693 -4.4286308 -4.4286942 -4.4287119 -4.4286842 -4.4286313][-4.4288135 -4.4287524 -4.42869 -4.4286413 -4.4285855 -4.4285431 -4.42856 -4.4285512 -4.4284921 -4.4285069 -4.4285984 -4.4286971 -4.4287338 -4.4287195 -4.4286585][-4.4288211 -4.4287548 -4.4286776 -4.4286041 -4.4285154 -4.4284363 -4.4284358 -4.4284544 -4.4284325 -4.4284749 -4.4285893 -4.4287024 -4.4287367 -4.4287133 -4.4286447][-4.4287987 -4.428721 -4.4286275 -4.428525 -4.4283895 -4.4282584 -4.4282403 -4.4283247 -4.428391 -4.4284868 -4.4286175 -4.4287162 -4.4287271 -4.4286718 -4.4285941][-4.4287696 -4.4286852 -4.4285746 -4.428452 -4.4282827 -4.428112 -4.4280767 -4.428216 -4.4283676 -4.4285088 -4.4286427 -4.4287205 -4.42871 -4.4286323 -4.4285574][-4.4287939 -4.4287186 -4.42862 -4.4285154 -4.4283767 -4.428226 -4.4281769 -4.42828 -4.4284205 -4.4285474 -4.4286509 -4.4286981 -4.4286761 -4.4286032 -4.4285474][-4.4288278 -4.4287696 -4.4286976 -4.4286256 -4.428546 -4.4284506 -4.4284081 -4.4284506 -4.428535 -4.4286194 -4.4286814 -4.4287043 -4.42868 -4.4286213 -4.4285827][-4.4288726 -4.428833 -4.4287858 -4.4287386 -4.4286962 -4.4286327 -4.4285936 -4.4286027 -4.4286375 -4.42868 -4.4287114 -4.4287171 -4.4286876 -4.4286394 -4.4286218][-4.4289131 -4.4288907 -4.428863 -4.4288354 -4.42881 -4.4287667 -4.4287281 -4.4287205 -4.4287338 -4.4287477 -4.4287591 -4.4287553 -4.42872 -4.4286757 -4.4286613][-4.42893 -4.4289212 -4.42891 -4.4288912 -4.4288726 -4.4288454 -4.4288187 -4.4288096 -4.4288177 -4.428823 -4.4288278 -4.4288263 -4.4287963 -4.4287467 -4.4287152][-4.4289403 -4.4289417 -4.42894 -4.4289236 -4.4289 -4.428885 -4.4288826 -4.4288878 -4.428895 -4.4288988 -4.4288988 -4.4288907 -4.4288616 -4.4288111 -4.4287696][-4.428956 -4.4289565 -4.4289522 -4.4289308 -4.4289007 -4.4288921 -4.4289136 -4.4289427 -4.4289527 -4.4289532 -4.4289489 -4.4289365 -4.4289064 -4.4288607 -4.4288182]]...]
INFO - root - 2017-12-08 07:16:17.000105: step 38010, loss = 2.28, batch loss = 2.23 (37.9 examples/sec; 0.211 sec/batch; 17h:16m:49s remains)
INFO - root - 2017-12-08 07:16:19.279997: step 38020, loss = 2.28, batch loss = 2.23 (35.3 examples/sec; 0.226 sec/batch; 18h:30m:44s remains)
INFO - root - 2017-12-08 07:16:21.551757: step 38030, loss = 2.28, batch loss = 2.23 (34.9 examples/sec; 0.229 sec/batch; 18h:44m:02s remains)
INFO - root - 2017-12-08 07:16:23.803490: step 38040, loss = 2.28, batch loss = 2.23 (36.7 examples/sec; 0.218 sec/batch; 17h:50m:38s remains)
INFO - root - 2017-12-08 07:16:26.045981: step 38050, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.225 sec/batch; 18h:24m:32s remains)
INFO - root - 2017-12-08 07:16:28.278253: step 38060, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.226 sec/batch; 18h:26m:39s remains)
INFO - root - 2017-12-08 07:16:30.501508: step 38070, loss = 2.28, batch loss = 2.23 (35.7 examples/sec; 0.224 sec/batch; 18h:20m:46s remains)
INFO - root - 2017-12-08 07:16:32.738114: step 38080, loss = 2.28, batch loss = 2.23 (34.5 examples/sec; 0.232 sec/batch; 18h:57m:18s remains)
INFO - root - 2017-12-08 07:16:34.982668: step 38090, loss = 2.28, batch loss = 2.23 (34.4 examples/sec; 0.233 sec/batch; 19h:00m:51s remains)
INFO - root - 2017-12-08 07:16:37.228697: step 38100, loss = 2.28, batch loss = 2.23 (35.4 examples/sec; 0.226 sec/batch; 18h:28m:46s remains)
2017-12-08 07:16:37.561403: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4285984 -4.4286175 -4.4286847 -4.4287581 -4.4287848 -4.4287786 -4.4287558 -4.4287143 -4.4286413 -4.4285536 -4.428525 -4.42855 -4.4285812 -4.4286222 -4.4287004][-4.4286022 -4.428628 -4.4286957 -4.4287596 -4.4287677 -4.4287448 -4.42871 -4.4286594 -4.4285779 -4.428473 -4.4284286 -4.4284592 -4.4285064 -4.42856 -4.4286475][-4.4286485 -4.4286733 -4.4287229 -4.4287567 -4.4287376 -4.4286895 -4.4286404 -4.42859 -4.4285173 -4.4284172 -4.4283714 -4.4284129 -4.4284878 -4.4285712 -4.4286723][-4.4287133 -4.4287295 -4.4287477 -4.4287438 -4.4286914 -4.4286127 -4.4285522 -4.4285221 -4.4284887 -4.42842 -4.4283862 -4.4284353 -4.4285183 -4.4286251 -4.4287395][-4.4287972 -4.4288039 -4.4287825 -4.4287305 -4.4286289 -4.4285107 -4.4284463 -4.4284644 -4.4284992 -4.4284816 -4.4284754 -4.4285269 -4.428606 -4.4287109 -4.4288197][-4.4288564 -4.4288363 -4.4287763 -4.4286838 -4.4285359 -4.4283738 -4.428308 -4.4283872 -4.4284921 -4.4285288 -4.4285579 -4.4286132 -4.4286842 -4.428771 -4.4288616][-4.4288664 -4.4288139 -4.4287171 -4.4285917 -4.4284053 -4.4281936 -4.428124 -4.4282708 -4.4284382 -4.4285183 -4.4285741 -4.4286437 -4.428719 -4.4287972 -4.4288735][-4.4288478 -4.4287758 -4.428658 -4.4285226 -4.4283261 -4.4280796 -4.4279823 -4.4281712 -4.4283805 -4.428493 -4.4285717 -4.4286523 -4.4287381 -4.4288144 -4.42888][-4.4288397 -4.4287663 -4.4286523 -4.4285412 -4.4283867 -4.4281783 -4.4280767 -4.4282169 -4.4283929 -4.4285045 -4.428596 -4.4286828 -4.4287682 -4.4288368 -4.4288926][-4.4288459 -4.4287834 -4.4286938 -4.4286218 -4.4285312 -4.4284015 -4.4283285 -4.4284058 -4.428514 -4.4285922 -4.428669 -4.4287386 -4.4288068 -4.4288621 -4.4289093][-4.428874 -4.4288187 -4.4287477 -4.4286957 -4.4286423 -4.4285727 -4.4285364 -4.4285808 -4.4286494 -4.4287086 -4.4287724 -4.4288244 -4.4288659 -4.4289 -4.4289355][-4.4289136 -4.428865 -4.4288096 -4.4287691 -4.4287372 -4.4287047 -4.4286923 -4.4287229 -4.4287705 -4.4288144 -4.4288669 -4.4289041 -4.4289236 -4.4289389 -4.4289613][-4.42895 -4.428916 -4.4288769 -4.4288449 -4.4288225 -4.4288006 -4.4287944 -4.4288149 -4.4288492 -4.4288797 -4.42892 -4.428946 -4.4289546 -4.42896 -4.4289708][-4.4289689 -4.4289503 -4.4289265 -4.428906 -4.4288878 -4.4288678 -4.4288588 -4.4288692 -4.4288921 -4.4289136 -4.4289412 -4.4289584 -4.4289613 -4.4289622 -4.4289675][-4.4289732 -4.4289651 -4.4289517 -4.4289393 -4.4289246 -4.4289093 -4.4289041 -4.4289093 -4.4289222 -4.4289351 -4.4289517 -4.4289608 -4.4289627 -4.4289637 -4.4289665]]...]
INFO - root - 2017-12-08 07:16:39.811653: step 38110, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 18h:03m:21s remains)
INFO - root - 2017-12-08 07:16:42.037613: step 38120, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.220 sec/batch; 18h:00m:38s remains)
INFO - root - 2017-12-08 07:16:44.272387: step 38130, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 17h:54m:49s remains)
INFO - root - 2017-12-08 07:16:46.502746: step 38140, loss = 2.28, batch loss = 2.23 (36.9 examples/sec; 0.217 sec/batch; 17h:44m:38s remains)
INFO - root - 2017-12-08 07:16:48.734029: step 38150, loss = 2.28, batch loss = 2.23 (35.8 examples/sec; 0.224 sec/batch; 18h:17m:21s remains)
INFO - root - 2017-12-08 07:16:50.955312: step 38160, loss = 2.28, batch loss = 2.23 (35.0 examples/sec; 0.228 sec/batch; 18h:40m:45s remains)
INFO - root - 2017-12-08 07:16:53.204027: step 38170, loss = 2.28, batch loss = 2.23 (33.4 examples/sec; 0.239 sec/batch; 19h:33m:48s remains)
INFO - root - 2017-12-08 07:16:55.465683: step 38180, loss = 2.28, batch loss = 2.23 (32.4 examples/sec; 0.247 sec/batch; 20h:12m:23s remains)
INFO - root - 2017-12-08 07:16:57.699194: step 38190, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 17h:56m:24s remains)
INFO - root - 2017-12-08 07:16:59.908947: step 38200, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.220 sec/batch; 18h:01m:26s remains)
2017-12-08 07:17:00.193287: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289236 -4.4289293 -4.4289308 -4.428925 -4.4289155 -4.4289079 -4.4289069 -4.4289041 -4.428894 -4.428884 -4.4288759 -4.4288669 -4.4288578 -4.4288654 -4.4288926][-4.4288964 -4.4289017 -4.4289026 -4.4288945 -4.4288816 -4.4288712 -4.4288673 -4.428865 -4.4288564 -4.4288454 -4.4288363 -4.4288216 -4.4288063 -4.4288135 -4.4288478][-4.4288425 -4.428843 -4.4288411 -4.4288249 -4.4288025 -4.4287844 -4.4287748 -4.4287677 -4.4287677 -4.4287663 -4.4287629 -4.4287519 -4.4287348 -4.4287486 -4.4287939][-4.428762 -4.4287548 -4.4287477 -4.42872 -4.428679 -4.4286404 -4.4286122 -4.4285936 -4.4286137 -4.4286385 -4.4286532 -4.4286532 -4.4286408 -4.4286704 -4.42873][-4.4286675 -4.4286461 -4.4286256 -4.4285865 -4.4285312 -4.4284678 -4.4284062 -4.4283643 -4.4284048 -4.4284711 -4.4285164 -4.428534 -4.4285359 -4.4285865 -4.4286652][-4.4286084 -4.4285717 -4.4285331 -4.4284792 -4.4284053 -4.4283051 -4.4281831 -4.4280844 -4.4281359 -4.428267 -4.4283638 -4.4284067 -4.4284286 -4.4285026 -4.4286056][-4.4286146 -4.4285731 -4.4285254 -4.4284558 -4.4283576 -4.42822 -4.4280348 -4.4278469 -4.4278708 -4.428041 -4.4281831 -4.4282632 -4.42832 -4.4284263 -4.4285579][-4.428678 -4.42865 -4.4286156 -4.4285541 -4.428463 -4.428329 -4.4281459 -4.4279413 -4.4278965 -4.4279995 -4.4281149 -4.4281955 -4.4282618 -4.4283791 -4.4285269][-4.4287453 -4.4287362 -4.4287219 -4.4286842 -4.4286208 -4.4285235 -4.4283953 -4.4282475 -4.4281735 -4.4281921 -4.4282455 -4.4283023 -4.4283533 -4.4284468 -4.4285679][-4.4287958 -4.428802 -4.428803 -4.4287853 -4.4287481 -4.4286871 -4.4286137 -4.4285231 -4.4284582 -4.4284396 -4.4284539 -4.428484 -4.4285145 -4.4285765 -4.4286532][-4.4288521 -4.4288645 -4.4288716 -4.4288678 -4.4288516 -4.42882 -4.428781 -4.4287295 -4.4286842 -4.428658 -4.4286537 -4.4286656 -4.4286833 -4.4287248 -4.4287677][-4.428906 -4.4289165 -4.4289241 -4.4289269 -4.4289241 -4.4289126 -4.4288945 -4.4288678 -4.4288411 -4.4288192 -4.4288073 -4.4288106 -4.428823 -4.4288483 -4.4288688][-4.4289379 -4.4289451 -4.4289527 -4.42896 -4.4289656 -4.4289665 -4.4289618 -4.4289522 -4.4289355 -4.4289155 -4.4289026 -4.4288979 -4.4289002 -4.4289136 -4.428925][-4.4289608 -4.4289641 -4.4289689 -4.4289765 -4.4289842 -4.4289904 -4.4289927 -4.4289923 -4.4289842 -4.4289694 -4.4289551 -4.4289451 -4.4289403 -4.4289446 -4.42895][-4.4289737 -4.4289722 -4.4289718 -4.4289727 -4.4289751 -4.4289794 -4.4289837 -4.428987 -4.428987 -4.4289827 -4.428977 -4.4289708 -4.4289656 -4.4289665 -4.4289684]]...]
INFO - root - 2017-12-08 07:17:02.441281: step 38210, loss = 2.28, batch loss = 2.23 (37.6 examples/sec; 0.213 sec/batch; 17h:23m:31s remains)
INFO - root - 2017-12-08 07:17:04.688654: step 38220, loss = 2.28, batch loss = 2.23 (35.2 examples/sec; 0.227 sec/batch; 18h:35m:28s remains)
INFO - root - 2017-12-08 07:17:06.951084: step 38230, loss = 2.28, batch loss = 2.23 (34.9 examples/sec; 0.229 sec/batch; 18h:43m:05s remains)
INFO - root - 2017-12-08 07:17:09.185895: step 38240, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.225 sec/batch; 18h:24m:38s remains)
INFO - root - 2017-12-08 07:17:11.444031: step 38250, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.222 sec/batch; 18h:08m:16s remains)
INFO - root - 2017-12-08 07:17:13.698695: step 38260, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.222 sec/batch; 18h:07m:00s remains)
INFO - root - 2017-12-08 07:17:15.931030: step 38270, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 17h:56m:59s remains)
INFO - root - 2017-12-08 07:17:18.160001: step 38280, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 18h:12m:24s remains)
INFO - root - 2017-12-08 07:17:20.404469: step 38290, loss = 2.28, batch loss = 2.23 (34.4 examples/sec; 0.232 sec/batch; 18h:59m:08s remains)
INFO - root - 2017-12-08 07:17:22.663241: step 38300, loss = 2.28, batch loss = 2.23 (34.0 examples/sec; 0.235 sec/batch; 19h:12m:20s remains)
2017-12-08 07:17:22.970319: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289594 -4.4289513 -4.4289165 -4.4288616 -4.4288015 -4.4287419 -4.428678 -4.4286518 -4.4286852 -4.4287515 -4.428813 -4.42886 -4.4288974 -4.4289222 -4.4289312][-4.4289675 -4.428956 -4.4289193 -4.4288664 -4.4288197 -4.4287868 -4.4287543 -4.4287457 -4.428782 -4.4288397 -4.428885 -4.4289131 -4.4289289 -4.4289246 -4.4289012][-4.4289694 -4.4289579 -4.4289217 -4.428865 -4.4288163 -4.4287872 -4.4287677 -4.42877 -4.4288087 -4.4288616 -4.4288931 -4.4289041 -4.4289 -4.4288716 -4.4288259][-4.4289613 -4.4289451 -4.4289007 -4.428833 -4.42877 -4.4287276 -4.428709 -4.4287248 -4.428771 -4.4288273 -4.4288583 -4.4288592 -4.4288354 -4.428791 -4.4287362][-4.4289408 -4.4289107 -4.4288454 -4.4287553 -4.4286661 -4.4285984 -4.4285746 -4.4286084 -4.4286804 -4.4287534 -4.4287963 -4.4287987 -4.4287672 -4.4287243 -4.4286861][-4.4289107 -4.4288645 -4.4287748 -4.4286537 -4.4285254 -4.428412 -4.4283667 -4.4284263 -4.4285483 -4.4286652 -4.4287372 -4.4287586 -4.4287419 -4.4287229 -4.4287181][-4.4288907 -4.4288373 -4.4287333 -4.4285893 -4.4284191 -4.4282479 -4.4281669 -4.4282475 -4.4284282 -4.4285932 -4.428699 -4.4287491 -4.4287577 -4.4287658 -4.4287858][-4.4288917 -4.4288416 -4.4287443 -4.42861 -4.4284444 -4.4282737 -4.4281812 -4.4282503 -4.4284282 -4.4285932 -4.4286981 -4.4287586 -4.4287882 -4.4288192 -4.4288559][-4.4289169 -4.4288807 -4.4288068 -4.4287086 -4.4285913 -4.4284682 -4.4283924 -4.4284282 -4.4285412 -4.4286571 -4.4287391 -4.4287949 -4.4288363 -4.4288774 -4.4289141][-4.4289379 -4.4289126 -4.428863 -4.4287972 -4.4287219 -4.4286432 -4.428597 -4.4286175 -4.4286833 -4.4287615 -4.4288225 -4.428864 -4.4288931 -4.4289165 -4.4289327][-4.4289441 -4.4289227 -4.4288845 -4.4288354 -4.4287839 -4.4287376 -4.4287176 -4.4287357 -4.4287786 -4.4288387 -4.4288883 -4.4289145 -4.4289188 -4.4289103 -4.4288917][-4.428946 -4.4289293 -4.4288974 -4.4288492 -4.4288006 -4.428762 -4.4287457 -4.4287624 -4.4288006 -4.4288535 -4.4288912 -4.4289055 -4.42889 -4.4288559 -4.4288092][-4.4289427 -4.4289327 -4.4289069 -4.4288549 -4.4287963 -4.4287405 -4.4287105 -4.4287195 -4.4287653 -4.4288211 -4.4288568 -4.4288626 -4.4288292 -4.4287739 -4.4287076][-4.4289269 -4.428915 -4.428884 -4.4288177 -4.4287405 -4.428659 -4.4286118 -4.4286213 -4.4286876 -4.4287572 -4.4287977 -4.428802 -4.4287634 -4.4287047 -4.4286394][-4.4289088 -4.4288883 -4.4288387 -4.4287448 -4.4286356 -4.4285131 -4.4284339 -4.4284506 -4.4285579 -4.4286642 -4.4287252 -4.4287491 -4.4287276 -4.428688 -4.4286547]]...]
INFO - root - 2017-12-08 07:17:25.230465: step 38310, loss = 2.28, batch loss = 2.23 (34.8 examples/sec; 0.230 sec/batch; 18h:46m:27s remains)
INFO - root - 2017-12-08 07:17:27.460573: step 38320, loss = 2.28, batch loss = 2.23 (36.6 examples/sec; 0.219 sec/batch; 17h:52m:24s remains)
INFO - root - 2017-12-08 07:17:29.702205: step 38330, loss = 2.28, batch loss = 2.23 (35.8 examples/sec; 0.223 sec/batch; 18h:14m:35s remains)
INFO - root - 2017-12-08 07:17:31.978666: step 38340, loss = 2.28, batch loss = 2.23 (36.0 examples/sec; 0.222 sec/batch; 18h:10m:22s remains)
INFO - root - 2017-12-08 07:17:34.233608: step 38350, loss = 2.28, batch loss = 2.23 (35.8 examples/sec; 0.223 sec/batch; 18h:15m:00s remains)
INFO - root - 2017-12-08 07:17:36.476779: step 38360, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 17h:56m:28s remains)
INFO - root - 2017-12-08 07:17:38.746810: step 38370, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.220 sec/batch; 17h:59m:57s remains)
INFO - root - 2017-12-08 07:17:40.994138: step 38380, loss = 2.28, batch loss = 2.23 (35.8 examples/sec; 0.223 sec/batch; 18h:14m:18s remains)
INFO - root - 2017-12-08 07:17:43.227863: step 38390, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 18h:13m:46s remains)
INFO - root - 2017-12-08 07:17:45.485535: step 38400, loss = 2.28, batch loss = 2.23 (37.5 examples/sec; 0.214 sec/batch; 17h:26m:44s remains)
2017-12-08 07:17:45.753199: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4286819 -4.4286642 -4.42868 -4.4287395 -4.4287958 -4.4288292 -4.4288292 -4.4287925 -4.4287348 -4.4286871 -4.42864 -4.4286218 -4.4286375 -4.428699 -4.4287839][-4.428689 -4.4286723 -4.4286804 -4.4287353 -4.4287939 -4.4288292 -4.4288263 -4.4287882 -4.4287348 -4.4286852 -4.428638 -4.4286213 -4.4286432 -4.4287066 -4.4287934][-4.4287143 -4.4287081 -4.4287186 -4.4287581 -4.4287968 -4.4288139 -4.4288011 -4.4287667 -4.4287257 -4.4286819 -4.4286442 -4.4286351 -4.4286613 -4.4287267 -4.4288149][-4.4287572 -4.4287663 -4.4287806 -4.4287987 -4.4287953 -4.4287744 -4.4287391 -4.42871 -4.4286938 -4.4286709 -4.4286475 -4.4286451 -4.4286742 -4.4287391 -4.4288244][-4.42881 -4.4288287 -4.4288383 -4.42883 -4.4287825 -4.428719 -4.4286518 -4.428617 -4.4286246 -4.4286413 -4.4286447 -4.4286542 -4.4286885 -4.42875 -4.4288249][-4.4288626 -4.4288783 -4.4288669 -4.4288211 -4.4287381 -4.4286366 -4.4285283 -4.4284759 -4.4285145 -4.4285922 -4.4286337 -4.4286571 -4.4286962 -4.4287548 -4.4288239][-4.4288993 -4.4289074 -4.4288716 -4.4287963 -4.428688 -4.428555 -4.4283934 -4.4282889 -4.4283566 -4.4285159 -4.4286065 -4.4286466 -4.4286942 -4.4287519 -4.4288216][-4.4288936 -4.4289002 -4.4288645 -4.4287877 -4.4286761 -4.4285293 -4.4283133 -4.4281187 -4.4281907 -4.4284225 -4.428555 -4.4286165 -4.4286838 -4.4287481 -4.4288158][-4.4288611 -4.4288607 -4.4288445 -4.428791 -4.4287024 -4.4285765 -4.4283743 -4.42817 -4.428195 -4.4283891 -4.4285078 -4.428575 -4.4286575 -4.4287362 -4.4288044][-4.4288411 -4.428843 -4.428844 -4.4288197 -4.42876 -4.4286742 -4.4285278 -4.4283776 -4.4283628 -4.4284635 -4.4285235 -4.4285669 -4.4286375 -4.4287143 -4.4287825][-4.4288306 -4.4288468 -4.4288654 -4.4288654 -4.4288363 -4.4287844 -4.4286895 -4.4285893 -4.4285479 -4.4285793 -4.4285884 -4.4286003 -4.4286461 -4.4287047 -4.4287648][-4.4288263 -4.4288535 -4.4288845 -4.4289002 -4.4288969 -4.4288769 -4.4288263 -4.4287663 -4.42871 -4.4286847 -4.4286466 -4.4286346 -4.4286661 -4.4287138 -4.4287634][-4.4288259 -4.4288507 -4.4288855 -4.4289131 -4.4289279 -4.4289303 -4.4289131 -4.4288788 -4.4288173 -4.4287615 -4.4286938 -4.4286566 -4.4286757 -4.4287229 -4.4287677][-4.4288545 -4.4288688 -4.4289002 -4.4289284 -4.428947 -4.4289594 -4.4289579 -4.4289379 -4.428884 -4.4288282 -4.428751 -4.4286938 -4.428699 -4.428741 -4.4287834][-4.4289069 -4.4289103 -4.4289265 -4.4289412 -4.4289546 -4.4289694 -4.4289751 -4.4289665 -4.4289308 -4.4288883 -4.4288206 -4.4287596 -4.4287491 -4.4287744 -4.4288039]]...]
INFO - root - 2017-12-08 07:17:47.990251: step 38410, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 18h:12m:50s remains)
INFO - root - 2017-12-08 07:17:50.228401: step 38420, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 18h:11m:38s remains)
INFO - root - 2017-12-08 07:17:52.470226: step 38430, loss = 2.28, batch loss = 2.23 (32.2 examples/sec; 0.249 sec/batch; 20h:18m:17s remains)
INFO - root - 2017-12-08 07:17:54.710437: step 38440, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 18h:02m:10s remains)
INFO - root - 2017-12-08 07:17:56.935311: step 38450, loss = 2.28, batch loss = 2.23 (35.7 examples/sec; 0.224 sec/batch; 18h:17m:33s remains)
INFO - root - 2017-12-08 07:17:59.157703: step 38460, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 18h:12m:07s remains)
INFO - root - 2017-12-08 07:18:01.406302: step 38470, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 18h:12m:42s remains)
INFO - root - 2017-12-08 07:18:03.683522: step 38480, loss = 2.28, batch loss = 2.23 (34.9 examples/sec; 0.229 sec/batch; 18h:44m:30s remains)
INFO - root - 2017-12-08 07:18:05.931012: step 38490, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.222 sec/batch; 18h:05m:27s remains)
INFO - root - 2017-12-08 07:18:08.194302: step 38500, loss = 2.28, batch loss = 2.23 (35.7 examples/sec; 0.224 sec/batch; 18h:18m:02s remains)
2017-12-08 07:18:08.489615: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288673 -4.4288845 -4.4289012 -4.4289103 -4.4288993 -4.42888 -4.4288692 -4.42888 -4.428895 -4.428906 -4.4289126 -4.4289227 -4.4289393 -4.428957 -4.4289684][-4.4287691 -4.4287839 -4.4288058 -4.4288254 -4.4288154 -4.428802 -4.4288039 -4.4288273 -4.4288445 -4.4288511 -4.4288573 -4.4288731 -4.4289002 -4.4289284 -4.42895][-4.428678 -4.4286895 -4.4287243 -4.4287539 -4.4287391 -4.4287243 -4.4287338 -4.4287677 -4.4287839 -4.4287872 -4.4287944 -4.4288149 -4.4288492 -4.4288859 -4.4289174][-4.4286404 -4.42864 -4.4286742 -4.4287004 -4.4286809 -4.4286556 -4.4286633 -4.4287033 -4.4287195 -4.428719 -4.4287224 -4.42874 -4.4287753 -4.4288216 -4.4288669][-4.4286542 -4.4286327 -4.428647 -4.4286571 -4.4286242 -4.4285727 -4.4285545 -4.4285927 -4.4286189 -4.4286251 -4.4286246 -4.4286327 -4.4286695 -4.4287357 -4.4288087][-4.4286981 -4.428668 -4.428659 -4.4286356 -4.4285617 -4.4284592 -4.428391 -4.4284163 -4.4284692 -4.428504 -4.4285126 -4.4285245 -4.4285722 -4.4286633 -4.4287639][-4.4287477 -4.4287171 -4.42869 -4.4286351 -4.4285173 -4.4283662 -4.4282641 -4.4282737 -4.42834 -4.4284024 -4.428431 -4.4284582 -4.4285216 -4.428628 -4.42874][-4.4287777 -4.4287491 -4.4287171 -4.428648 -4.428514 -4.4283557 -4.428257 -4.4282727 -4.4283462 -4.4284163 -4.428462 -4.4285021 -4.428566 -4.4286575 -4.4287543][-4.4287748 -4.4287443 -4.4287171 -4.4286633 -4.4285545 -4.4284334 -4.4283576 -4.4283719 -4.428441 -4.4285154 -4.4285641 -4.428607 -4.4286671 -4.4287367 -4.4288058][-4.4287753 -4.4287539 -4.4287395 -4.4287038 -4.4286308 -4.4285574 -4.428504 -4.4285097 -4.4285574 -4.4286208 -4.428659 -4.4286947 -4.4287472 -4.4288044 -4.4288545][-4.4287844 -4.4287796 -4.4287782 -4.42875 -4.4286985 -4.4286494 -4.4286127 -4.4286227 -4.4286561 -4.4287019 -4.428719 -4.4287386 -4.4287858 -4.4288387 -4.4288793][-4.4287992 -4.4288025 -4.4288077 -4.4287844 -4.42874 -4.4286976 -4.4286685 -4.428689 -4.4287152 -4.4287429 -4.4287438 -4.4287515 -4.4288 -4.4288535 -4.4288921][-4.4288125 -4.4288106 -4.42881 -4.4287896 -4.428751 -4.4287219 -4.4287062 -4.4287343 -4.4287543 -4.4287653 -4.4287596 -4.4287648 -4.4288111 -4.4288626 -4.4289017][-4.4288244 -4.4288125 -4.4288034 -4.4287882 -4.4287624 -4.4287448 -4.4287381 -4.4287667 -4.4287858 -4.428792 -4.4287868 -4.428792 -4.4288306 -4.428875 -4.4289117][-4.4288387 -4.4288282 -4.42882 -4.42881 -4.428793 -4.4287834 -4.4287786 -4.4287953 -4.4288087 -4.42882 -4.428823 -4.42883 -4.4288583 -4.4288926 -4.4289231]]...]
INFO - root - 2017-12-08 07:18:10.737664: step 38510, loss = 2.28, batch loss = 2.23 (34.6 examples/sec; 0.231 sec/batch; 18h:52m:01s remains)
INFO - root - 2017-12-08 07:18:13.005938: step 38520, loss = 2.28, batch loss = 2.23 (34.6 examples/sec; 0.231 sec/batch; 18h:52m:28s remains)
INFO - root - 2017-12-08 07:18:15.276397: step 38530, loss = 2.28, batch loss = 2.23 (35.4 examples/sec; 0.226 sec/batch; 18h:26m:37s remains)
INFO - root - 2017-12-08 07:18:17.506480: step 38540, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 17h:53m:16s remains)
INFO - root - 2017-12-08 07:18:19.756200: step 38550, loss = 2.28, batch loss = 2.23 (35.2 examples/sec; 0.227 sec/batch; 18h:34m:15s remains)
INFO - root - 2017-12-08 07:18:21.998508: step 38560, loss = 2.28, batch loss = 2.23 (34.9 examples/sec; 0.229 sec/batch; 18h:41m:28s remains)
INFO - root - 2017-12-08 07:18:24.241853: step 38570, loss = 2.28, batch loss = 2.23 (33.9 examples/sec; 0.236 sec/batch; 19h:17m:22s remains)
INFO - root - 2017-12-08 07:18:26.484296: step 38580, loss = 2.28, batch loss = 2.23 (35.8 examples/sec; 0.223 sec/batch; 18h:14m:42s remains)
INFO - root - 2017-12-08 07:18:28.700752: step 38590, loss = 2.28, batch loss = 2.23 (36.8 examples/sec; 0.218 sec/batch; 17h:46m:03s remains)
INFO - root - 2017-12-08 07:18:30.935321: step 38600, loss = 2.28, batch loss = 2.23 (37.0 examples/sec; 0.216 sec/batch; 17h:39m:06s remains)
2017-12-08 07:18:31.210957: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4287176 -4.4286871 -4.4286466 -4.4286175 -4.4286308 -4.4286561 -4.4286923 -4.4287381 -4.4287825 -4.4287925 -4.4287524 -4.4287043 -4.4286757 -4.4286852 -4.4287152][-4.4287405 -4.4287257 -4.4287071 -4.4286857 -4.4286823 -4.4286861 -4.428709 -4.4287462 -4.4287839 -4.4287844 -4.4287434 -4.4286904 -4.42865 -4.42864 -4.42867][-4.4287887 -4.4287939 -4.4288068 -4.4288015 -4.4287848 -4.4287739 -4.4287796 -4.4287977 -4.4288197 -4.4288077 -4.4287724 -4.4287219 -4.4286714 -4.4286418 -4.4286571][-4.428813 -4.4288397 -4.4288845 -4.4288945 -4.4288726 -4.4288526 -4.428843 -4.428843 -4.4288549 -4.4288425 -4.4288182 -4.4287782 -4.4287238 -4.4286809 -4.4286776][-4.4288344 -4.4288659 -4.4289136 -4.4289217 -4.428894 -4.4288659 -4.4288483 -4.4288411 -4.42885 -4.4288387 -4.4288316 -4.428813 -4.4287729 -4.4287329 -4.428719][-4.42884 -4.4288664 -4.4288969 -4.4288807 -4.4288359 -4.4287996 -4.4287686 -4.4287629 -4.4287815 -4.4287825 -4.4288039 -4.4288154 -4.4288039 -4.4287872 -4.428781][-4.4288173 -4.4288354 -4.4288445 -4.4288068 -4.4287477 -4.4287024 -4.4286489 -4.4286346 -4.428659 -4.4286718 -4.4287262 -4.4287715 -4.4287992 -4.4288149 -4.4288235][-4.42879 -4.4287863 -4.4287734 -4.4287267 -4.428668 -4.4286084 -4.4285274 -4.4284911 -4.4284954 -4.4285088 -4.4285927 -4.42867 -4.4287438 -4.4287977 -4.42882][-4.428791 -4.4287786 -4.42876 -4.4287171 -4.4286709 -4.4286151 -4.4285336 -4.4284859 -4.4284544 -4.4284439 -4.4285283 -4.4286141 -4.4287109 -4.428792 -4.42883][-4.4288077 -4.4287958 -4.4287877 -4.4287667 -4.4287438 -4.4287124 -4.4286642 -4.4286222 -4.4285669 -4.4285336 -4.4285922 -4.4286542 -4.42873 -4.4288015 -4.4288483][-4.4288173 -4.4288125 -4.4288197 -4.4288197 -4.4288235 -4.4288197 -4.4288011 -4.4287782 -4.4287286 -4.4286904 -4.4287109 -4.4287267 -4.4287677 -4.4288144 -4.4288535][-4.4288263 -4.4288249 -4.42884 -4.4288511 -4.4288678 -4.4288807 -4.4288845 -4.4288864 -4.4288588 -4.4288282 -4.428813 -4.4287848 -4.4287872 -4.4288063 -4.4288359][-4.4288459 -4.4288373 -4.4288454 -4.4288492 -4.4288678 -4.4288888 -4.4289069 -4.4289227 -4.4289165 -4.4288988 -4.4288645 -4.4288168 -4.4287934 -4.4287949 -4.4288082][-4.4288521 -4.4288354 -4.428834 -4.4288297 -4.4288383 -4.4288568 -4.4288807 -4.4289021 -4.4289083 -4.4289088 -4.4288845 -4.4288449 -4.4288182 -4.4288096 -4.4287949][-4.4288197 -4.4288054 -4.4287968 -4.4287868 -4.4287791 -4.4287848 -4.4288049 -4.4288244 -4.4288397 -4.428863 -4.4288588 -4.4288383 -4.4288225 -4.4288106 -4.4287744]]...]
INFO - root - 2017-12-08 07:18:33.454362: step 38610, loss = 2.28, batch loss = 2.23 (35.1 examples/sec; 0.228 sec/batch; 18h:37m:16s remains)
INFO - root - 2017-12-08 07:18:35.674489: step 38620, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 17h:54m:02s remains)
INFO - root - 2017-12-08 07:18:37.907381: step 38630, loss = 2.28, batch loss = 2.23 (35.4 examples/sec; 0.226 sec/batch; 18h:26m:48s remains)
INFO - root - 2017-12-08 07:18:40.170991: step 38640, loss = 2.28, batch loss = 2.23 (36.0 examples/sec; 0.222 sec/batch; 18h:07m:35s remains)
INFO - root - 2017-12-08 07:18:42.430127: step 38650, loss = 2.28, batch loss = 2.23 (35.2 examples/sec; 0.228 sec/batch; 18h:34m:18s remains)
INFO - root - 2017-12-08 07:18:44.665480: step 38660, loss = 2.28, batch loss = 2.23 (34.8 examples/sec; 0.230 sec/batch; 18h:45m:18s remains)
INFO - root - 2017-12-08 07:18:46.930885: step 38670, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.220 sec/batch; 17h:59m:46s remains)
INFO - root - 2017-12-08 07:18:49.161526: step 38680, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 18h:02m:12s remains)
INFO - root - 2017-12-08 07:18:51.380636: step 38690, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.221 sec/batch; 18h:03m:55s remains)
INFO - root - 2017-12-08 07:18:53.622933: step 38700, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 17h:57m:11s remains)
2017-12-08 07:18:53.950324: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4287391 -4.4287167 -4.4286833 -4.4286695 -4.4286914 -4.42874 -4.4287677 -4.4287529 -4.4287257 -4.428709 -4.4287019 -4.4287009 -4.4287248 -4.4287686 -4.4288015][-4.4287381 -4.4287143 -4.4286847 -4.4286819 -4.4287133 -4.4287581 -4.4287715 -4.4287319 -4.4286895 -4.4286833 -4.4286919 -4.4286885 -4.4287081 -4.4287472 -4.4287815][-4.4287553 -4.4287333 -4.4287119 -4.4287167 -4.4287434 -4.42877 -4.4287648 -4.4287071 -4.4286547 -4.4286647 -4.4286923 -4.4286957 -4.4287152 -4.4287534 -4.4287877][-4.4287815 -4.4287643 -4.4287558 -4.428761 -4.4287739 -4.4287791 -4.4287534 -4.4286847 -4.42863 -4.428648 -4.4286828 -4.428699 -4.4287286 -4.4287748 -4.4288197][-4.4288235 -4.4288139 -4.4288077 -4.4288025 -4.4287972 -4.4287834 -4.42874 -4.4286604 -4.4286065 -4.4286232 -4.4286618 -4.4286981 -4.4287515 -4.4288149 -4.4288688][-4.4288645 -4.4288597 -4.4288487 -4.4288273 -4.428802 -4.4287634 -4.4286947 -4.4286046 -4.4285579 -4.4285831 -4.4286284 -4.4286914 -4.4287672 -4.428844 -4.4289079][-4.428884 -4.4288778 -4.4288568 -4.4288197 -4.4287767 -4.4287229 -4.4286284 -4.4285269 -4.4284911 -4.428534 -4.4286075 -4.4286981 -4.4287868 -4.4288697 -4.428937][-4.4288721 -4.4288635 -4.4288321 -4.4287839 -4.4287353 -4.4286819 -4.4285789 -4.4284806 -4.4284596 -4.4285207 -4.4286203 -4.4287224 -4.4288139 -4.4288969 -4.4289613][-4.4288449 -4.4288311 -4.4287953 -4.4287457 -4.4287033 -4.4286542 -4.4285641 -4.4284835 -4.4284735 -4.4285393 -4.4286427 -4.4287438 -4.4288368 -4.4289184 -4.428978][-4.4288349 -4.4288168 -4.4287853 -4.4287391 -4.4286952 -4.4286423 -4.4285641 -4.4284978 -4.4284978 -4.4285626 -4.4286575 -4.4287562 -4.428854 -4.4289341 -4.4289865][-4.4288454 -4.4288259 -4.4287996 -4.42876 -4.4287138 -4.4286542 -4.4285808 -4.4285245 -4.4285336 -4.4285946 -4.4286785 -4.4287739 -4.4288745 -4.4289513 -4.4289932][-4.4288745 -4.4288611 -4.4288368 -4.4287992 -4.428751 -4.42869 -4.4286251 -4.4285784 -4.4285917 -4.4286466 -4.4287233 -4.4288111 -4.428905 -4.4289703 -4.4289985][-4.4289036 -4.4289021 -4.4288874 -4.4288554 -4.42881 -4.4287539 -4.4287033 -4.428669 -4.428678 -4.4287214 -4.4287858 -4.4288611 -4.42894 -4.4289875 -4.4290009][-4.4289269 -4.428936 -4.4289331 -4.4289131 -4.428874 -4.4288254 -4.4287834 -4.4287553 -4.42876 -4.4287949 -4.4288473 -4.4289083 -4.4289665 -4.4289956 -4.4289989][-4.4289427 -4.4289584 -4.4289651 -4.4289551 -4.4289265 -4.4288864 -4.4288516 -4.42883 -4.4288344 -4.428863 -4.428906 -4.4289517 -4.428988 -4.4290037 -4.429]]...]
INFO - root - 2017-12-08 07:18:56.155819: step 38710, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 17h:55m:35s remains)
INFO - root - 2017-12-08 07:18:58.394775: step 38720, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.226 sec/batch; 18h:24m:15s remains)
INFO - root - 2017-12-08 07:19:00.622788: step 38730, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 18h:03m:12s remains)
INFO - root - 2017-12-08 07:19:02.844168: step 38740, loss = 2.28, batch loss = 2.23 (35.7 examples/sec; 0.224 sec/batch; 18h:16m:26s remains)
INFO - root - 2017-12-08 07:19:05.123461: step 38750, loss = 2.28, batch loss = 2.23 (35.4 examples/sec; 0.226 sec/batch; 18h:25m:55s remains)
INFO - root - 2017-12-08 07:19:07.361705: step 38760, loss = 2.28, batch loss = 2.23 (36.9 examples/sec; 0.217 sec/batch; 17h:40m:24s remains)
INFO - root - 2017-12-08 07:19:09.587315: step 38770, loss = 2.28, batch loss = 2.23 (35.6 examples/sec; 0.225 sec/batch; 18h:19m:49s remains)
INFO - root - 2017-12-08 07:19:11.813742: step 38780, loss = 2.28, batch loss = 2.23 (35.8 examples/sec; 0.223 sec/batch; 18h:13m:56s remains)
INFO - root - 2017-12-08 07:19:14.041358: step 38790, loss = 2.28, batch loss = 2.23 (36.7 examples/sec; 0.218 sec/batch; 17h:48m:10s remains)
INFO - root - 2017-12-08 07:19:16.276558: step 38800, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.222 sec/batch; 18h:04m:31s remains)
2017-12-08 07:19:16.566681: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4285502 -4.4286022 -4.4286594 -4.4286833 -4.4286847 -4.4286537 -4.4286146 -4.4286084 -4.4286366 -4.4286785 -4.4287233 -4.4287696 -4.4287882 -4.4288 -4.4288211][-4.4285893 -4.4286628 -4.4287314 -4.428762 -4.4287586 -4.4287295 -4.4286971 -4.4286952 -4.4287186 -4.428751 -4.4287782 -4.428802 -4.4288073 -4.4288154 -4.4288344][-4.4286885 -4.428761 -4.4288268 -4.4288554 -4.4288416 -4.4288082 -4.428771 -4.4287572 -4.4287639 -4.4287825 -4.4287972 -4.4288063 -4.4288073 -4.4288197 -4.4288454][-4.4287887 -4.4288435 -4.4288945 -4.4289179 -4.4288945 -4.4288464 -4.4287863 -4.4287443 -4.4287372 -4.4287548 -4.4287729 -4.4287872 -4.4288039 -4.4288278 -4.4288597][-4.428854 -4.4288898 -4.4289231 -4.4289279 -4.4288878 -4.4288139 -4.4287143 -4.4286375 -4.4286242 -4.4286628 -4.4287047 -4.4287429 -4.4287877 -4.4288297 -4.4288716][-4.4289031 -4.4289207 -4.4289222 -4.4288983 -4.4288325 -4.4287238 -4.4285789 -4.4284582 -4.42844 -4.4285154 -4.4286027 -4.4286804 -4.4287643 -4.4288278 -4.4288726][-4.4289308 -4.42893 -4.4289012 -4.4288507 -4.4287672 -4.4286432 -4.4284611 -4.428299 -4.4282789 -4.4283819 -4.4285145 -4.4286342 -4.4287477 -4.4288239 -4.4288688][-4.4289365 -4.4289265 -4.4288836 -4.4288225 -4.4287434 -4.428628 -4.4284391 -4.4282608 -4.4282327 -4.4283385 -4.4284968 -4.4286466 -4.42877 -4.4288425 -4.4288764][-4.42894 -4.4289312 -4.4288812 -4.4288154 -4.4287453 -4.4286532 -4.428494 -4.4283342 -4.4283004 -4.42839 -4.4285431 -4.4286971 -4.4288192 -4.4288869 -4.4289112][-4.4289494 -4.428947 -4.4288964 -4.4288249 -4.4287586 -4.42869 -4.4285688 -4.4284387 -4.4284062 -4.4284787 -4.4286041 -4.4287429 -4.4288607 -4.4289312 -4.4289536][-4.4289522 -4.4289479 -4.4289045 -4.4288383 -4.4287653 -4.4287047 -4.4286122 -4.4285045 -4.4284768 -4.4285364 -4.4286323 -4.4287505 -4.4288583 -4.428937 -4.4289665][-4.4289551 -4.428946 -4.4289112 -4.4288535 -4.4287772 -4.4287157 -4.4286337 -4.4285345 -4.4285097 -4.428566 -4.4286442 -4.4287367 -4.4288306 -4.4289045 -4.4289379][-4.4289551 -4.4289465 -4.4289217 -4.4288745 -4.4288 -4.428731 -4.428648 -4.4285522 -4.4285221 -4.4285669 -4.4286242 -4.4286981 -4.4287772 -4.4288397 -4.4288707][-4.4289551 -4.4289494 -4.4289331 -4.4288974 -4.4288363 -4.4287705 -4.4286933 -4.4286022 -4.4285588 -4.4285769 -4.428607 -4.428658 -4.42872 -4.4287648 -4.4287906][-4.4289503 -4.4289474 -4.4289331 -4.4289064 -4.4288654 -4.4288173 -4.4287596 -4.4286857 -4.4286342 -4.4286222 -4.4286237 -4.4286466 -4.4286819 -4.4287062 -4.42873]]...]
INFO - root - 2017-12-08 07:19:18.790234: step 38810, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 18h:09m:19s remains)
INFO - root - 2017-12-08 07:19:21.047918: step 38820, loss = 2.28, batch loss = 2.23 (35.2 examples/sec; 0.227 sec/batch; 18h:33m:03s remains)
INFO - root - 2017-12-08 07:19:23.300424: step 38830, loss = 2.28, batch loss = 2.23 (34.6 examples/sec; 0.231 sec/batch; 18h:52m:22s remains)
INFO - root - 2017-12-08 07:19:25.548771: step 38840, loss = 2.28, batch loss = 2.23 (35.8 examples/sec; 0.224 sec/batch; 18h:14m:55s remains)
INFO - root - 2017-12-08 07:19:27.785567: step 38850, loss = 2.28, batch loss = 2.23 (35.7 examples/sec; 0.224 sec/batch; 18h:16m:29s remains)
INFO - root - 2017-12-08 07:19:30.043718: step 38860, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 18h:02m:49s remains)
INFO - root - 2017-12-08 07:19:32.321991: step 38870, loss = 2.28, batch loss = 2.23 (33.6 examples/sec; 0.238 sec/batch; 19h:23m:46s remains)
INFO - root - 2017-12-08 07:19:34.560769: step 38880, loss = 2.28, batch loss = 2.23 (35.2 examples/sec; 0.227 sec/batch; 18h:31m:39s remains)
INFO - root - 2017-12-08 07:19:36.788716: step 38890, loss = 2.28, batch loss = 2.23 (36.6 examples/sec; 0.219 sec/batch; 17h:50m:48s remains)
INFO - root - 2017-12-08 07:19:39.014136: step 38900, loss = 2.28, batch loss = 2.23 (37.3 examples/sec; 0.214 sec/batch; 17h:29m:31s remains)
2017-12-08 07:19:39.292432: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4285965 -4.4285674 -4.4285479 -4.428556 -4.4285893 -4.428597 -4.4285622 -4.4285216 -4.4285307 -4.4285789 -4.428617 -4.4286208 -4.4285922 -4.4285321 -4.4284806][-4.4286013 -4.4285803 -4.4285622 -4.428566 -4.4286017 -4.4286261 -4.4286108 -4.4285893 -4.4286032 -4.4286418 -4.4286709 -4.428679 -4.42867 -4.4286375 -4.4286046][-4.4286618 -4.4286375 -4.4286075 -4.4286003 -4.4286318 -4.4286709 -4.4286761 -4.4286723 -4.4286876 -4.4287181 -4.4287477 -4.428772 -4.4287848 -4.4287786 -4.428761][-4.4287109 -4.428678 -4.4286323 -4.4286127 -4.4286385 -4.4286866 -4.4287 -4.4286957 -4.4287109 -4.4287472 -4.4287939 -4.4288406 -4.428874 -4.4288855 -4.4288688][-4.4287214 -4.4286866 -4.4286389 -4.4286094 -4.4286227 -4.4286566 -4.4286575 -4.4286366 -4.4286456 -4.4286976 -4.4287658 -4.4288325 -4.4288807 -4.4289012 -4.4288816][-4.4287128 -4.4286785 -4.4286427 -4.4286194 -4.428618 -4.4286156 -4.428575 -4.428515 -4.4285092 -4.4285836 -4.4286819 -4.4287667 -4.4288187 -4.4288349 -4.4288111][-4.4286962 -4.4286633 -4.4286413 -4.4286275 -4.4286041 -4.4285545 -4.428452 -4.4283385 -4.4283304 -4.4284477 -4.428586 -4.4286823 -4.4287248 -4.4287252 -4.4286923][-4.4286747 -4.4286442 -4.4286346 -4.4286261 -4.4285812 -4.42849 -4.4283357 -4.4281807 -4.4281964 -4.428369 -4.4285316 -4.4286184 -4.4286337 -4.428607 -4.4285688][-4.4286647 -4.4286427 -4.4286447 -4.428637 -4.4285889 -4.4284945 -4.4283423 -4.428205 -4.4282436 -4.4284072 -4.4285374 -4.4285822 -4.428555 -4.4285054 -4.4284759][-4.4286819 -4.4286642 -4.42867 -4.4286747 -4.4286485 -4.4285831 -4.4284754 -4.4283857 -4.4284177 -4.4285192 -4.4285812 -4.4285679 -4.428503 -4.4284439 -4.4284325][-4.4286757 -4.4286623 -4.4286737 -4.4286957 -4.4286942 -4.4286585 -4.4285975 -4.428556 -4.4285812 -4.4286289 -4.4286356 -4.4285836 -4.4285011 -4.4284391 -4.428441][-4.4286556 -4.4286542 -4.4286723 -4.4287047 -4.42872 -4.4287043 -4.4286771 -4.4286714 -4.428699 -4.4287252 -4.4287128 -4.4286547 -4.4285769 -4.4285216 -4.4285231][-4.4286523 -4.428669 -4.4286971 -4.4287353 -4.4287615 -4.4287667 -4.4287663 -4.4287767 -4.4288044 -4.4288211 -4.4288077 -4.428762 -4.4287057 -4.4286647 -4.4286566][-4.4286542 -4.4286923 -4.4287405 -4.4287863 -4.4288211 -4.4288392 -4.4288483 -4.4288588 -4.4288788 -4.4288917 -4.4288864 -4.4288635 -4.4288349 -4.4288063 -4.4287882][-4.4286971 -4.4287419 -4.4287944 -4.4288383 -4.42887 -4.4288845 -4.4288912 -4.4288993 -4.4289117 -4.4289231 -4.428926 -4.4289217 -4.4289103 -4.4288907 -4.4288688]]...]
INFO - root - 2017-12-08 07:19:41.529482: step 38910, loss = 2.28, batch loss = 2.23 (35.8 examples/sec; 0.223 sec/batch; 18h:13m:00s remains)
INFO - root - 2017-12-08 07:19:43.759469: step 38920, loss = 2.28, batch loss = 2.23 (34.4 examples/sec; 0.233 sec/batch; 18h:58m:00s remains)
INFO - root - 2017-12-08 07:19:45.990585: step 38930, loss = 2.28, batch loss = 2.23 (35.6 examples/sec; 0.224 sec/batch; 18h:17m:58s remains)
INFO - root - 2017-12-08 07:19:48.201330: step 38940, loss = 2.28, batch loss = 2.23 (37.4 examples/sec; 0.214 sec/batch; 17h:27m:14s remains)
INFO - root - 2017-12-08 07:19:50.426292: step 38950, loss = 2.28, batch loss = 2.23 (37.1 examples/sec; 0.216 sec/batch; 17h:34m:51s remains)
INFO - root - 2017-12-08 07:19:52.656157: step 38960, loss = 2.28, batch loss = 2.23 (35.3 examples/sec; 0.226 sec/batch; 18h:27m:52s remains)
INFO - root - 2017-12-08 07:19:54.882581: step 38970, loss = 2.28, batch loss = 2.23 (36.7 examples/sec; 0.218 sec/batch; 17h:45m:51s remains)
INFO - root - 2017-12-08 07:19:57.143074: step 38980, loss = 2.28, batch loss = 2.23 (36.0 examples/sec; 0.222 sec/batch; 18h:08m:10s remains)
INFO - root - 2017-12-08 07:19:59.378814: step 38990, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.226 sec/batch; 18h:23m:32s remains)
INFO - root - 2017-12-08 07:20:01.618298: step 39000, loss = 2.28, batch loss = 2.23 (34.9 examples/sec; 0.229 sec/batch; 18h:39m:49s remains)
2017-12-08 07:20:01.904137: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289942 -4.4289827 -4.4289627 -4.4289427 -4.4289279 -4.4289217 -4.4289289 -4.4289474 -4.4289684 -4.4289818 -4.428988 -4.4289904 -4.4289894 -4.4289861 -4.4289832][-4.4289894 -4.4289746 -4.4289465 -4.428916 -4.4288874 -4.4288707 -4.4288754 -4.4288993 -4.4289293 -4.4289556 -4.4289761 -4.4289894 -4.4289956 -4.4289961 -4.4289937][-4.428947 -4.4289327 -4.4289002 -4.4288578 -4.4288168 -4.428793 -4.4287996 -4.4288306 -4.4288673 -4.4289055 -4.4289412 -4.42897 -4.4289894 -4.4289994 -4.429][-4.4288878 -4.4288726 -4.4288383 -4.4287868 -4.4287286 -4.42869 -4.4286928 -4.4287276 -4.428771 -4.4288182 -4.4288735 -4.4289227 -4.4289594 -4.4289846 -4.4289951][-4.4288378 -4.4288344 -4.4288068 -4.4287543 -4.4286833 -4.4286203 -4.4285889 -4.4285851 -4.428607 -4.4286647 -4.4287543 -4.4288421 -4.4289083 -4.4289551 -4.4289818][-4.4287586 -4.4287877 -4.4287915 -4.4287691 -4.4287176 -4.4286451 -4.4285665 -4.428493 -4.428452 -4.4284878 -4.4285984 -4.4287214 -4.4288249 -4.4289055 -4.4289556][-4.428647 -4.4286942 -4.4287329 -4.4287548 -4.4287448 -4.4286933 -4.4286027 -4.4284883 -4.4283962 -4.4283957 -4.4284897 -4.4286146 -4.4287324 -4.4288349 -4.4289083][-4.4285917 -4.4286275 -4.4286747 -4.4287143 -4.4287248 -4.4286938 -4.4286313 -4.4285479 -4.4284711 -4.4284525 -4.4285054 -4.4285922 -4.4286895 -4.4287877 -4.4288683][-4.4286284 -4.4286418 -4.4286714 -4.4286942 -4.4286933 -4.4286647 -4.4286327 -4.4285994 -4.4285684 -4.4285622 -4.4285893 -4.4286323 -4.4286923 -4.4287691 -4.4288435][-4.4286704 -4.4286718 -4.4286876 -4.4286976 -4.4286938 -4.4286733 -4.4286647 -4.4286594 -4.4286532 -4.4286551 -4.4286718 -4.428689 -4.428719 -4.4287729 -4.4288349][-4.4286823 -4.4286537 -4.4286461 -4.4286613 -4.4286857 -4.4287019 -4.4287248 -4.4287448 -4.4287515 -4.4287519 -4.4287562 -4.4287562 -4.4287634 -4.428793 -4.4288383][-4.4286952 -4.4286175 -4.4285674 -4.4285793 -4.4286404 -4.4287028 -4.4287615 -4.4288096 -4.428833 -4.4288383 -4.4288368 -4.4288225 -4.4288106 -4.4288216 -4.4288516][-4.428751 -4.4286547 -4.4285707 -4.428556 -4.42861 -4.4286866 -4.4287605 -4.4288239 -4.4288616 -4.4288807 -4.4288878 -4.4288745 -4.4288549 -4.4288535 -4.4288726][-4.4288077 -4.4287424 -4.428668 -4.4286289 -4.4286404 -4.4286809 -4.4287348 -4.4287829 -4.4288158 -4.4288449 -4.4288678 -4.4288759 -4.4288731 -4.4288769 -4.4288907][-4.4288378 -4.4288092 -4.4287663 -4.4287257 -4.4286971 -4.428679 -4.4286761 -4.4286871 -4.4287066 -4.4287481 -4.4287963 -4.4288363 -4.4288654 -4.4288845 -4.4288969]]...]
INFO - root - 2017-12-08 07:20:04.159637: step 39010, loss = 2.28, batch loss = 2.23 (35.2 examples/sec; 0.227 sec/batch; 18h:30m:26s remains)
INFO - root - 2017-12-08 07:20:06.380268: step 39020, loss = 2.28, batch loss = 2.23 (35.6 examples/sec; 0.225 sec/batch; 18h:19m:28s remains)
INFO - root - 2017-12-08 07:20:08.636786: step 39030, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 17h:56m:02s remains)
INFO - root - 2017-12-08 07:20:10.878178: step 39040, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.222 sec/batch; 18h:03m:53s remains)
INFO - root - 2017-12-08 07:20:13.105720: step 39050, loss = 2.28, batch loss = 2.23 (35.6 examples/sec; 0.225 sec/batch; 18h:19m:18s remains)
INFO - root - 2017-12-08 07:20:15.352669: step 39060, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.220 sec/batch; 17h:56m:39s remains)
INFO - root - 2017-12-08 07:20:17.595986: step 39070, loss = 2.28, batch loss = 2.23 (35.1 examples/sec; 0.228 sec/batch; 18h:35m:21s remains)
INFO - root - 2017-12-08 07:20:19.855937: step 39080, loss = 2.28, batch loss = 2.23 (33.6 examples/sec; 0.238 sec/batch; 19h:24m:47s remains)
INFO - root - 2017-12-08 07:20:22.089204: step 39090, loss = 2.28, batch loss = 2.23 (36.0 examples/sec; 0.222 sec/batch; 18h:06m:36s remains)
INFO - root - 2017-12-08 07:20:24.339470: step 39100, loss = 2.28, batch loss = 2.23 (35.0 examples/sec; 0.228 sec/batch; 18h:36m:41s remains)
2017-12-08 07:20:24.644579: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288449 -4.42882 -4.42878 -4.4287653 -4.4287548 -4.4287033 -4.4286423 -4.4286995 -4.428781 -4.4287872 -4.4287739 -4.42874 -4.4286909 -4.4287095 -4.4287181][-4.4288497 -4.4288163 -4.4287744 -4.4287591 -4.428741 -4.4286857 -4.4286313 -4.4286938 -4.4287767 -4.4287839 -4.428772 -4.4287438 -4.4287024 -4.4287214 -4.4287276][-4.4288411 -4.4287953 -4.42875 -4.428731 -4.4287057 -4.4286523 -4.4285989 -4.428658 -4.4287477 -4.42877 -4.4287691 -4.4287481 -4.4287171 -4.4287333 -4.4287448][-4.4288507 -4.4288068 -4.4287663 -4.4287438 -4.4287109 -4.4286551 -4.4285951 -4.4286404 -4.4287395 -4.4287825 -4.42879 -4.4287763 -4.4287581 -4.4287729 -4.4287877][-4.4288621 -4.4288206 -4.4287834 -4.4287605 -4.4287257 -4.4286671 -4.4285936 -4.4286189 -4.428719 -4.4287767 -4.4287915 -4.4287777 -4.4287629 -4.4287763 -4.4287953][-4.4288578 -4.4288125 -4.428772 -4.4287434 -4.4287019 -4.4286404 -4.4285512 -4.4285545 -4.4286523 -4.4287205 -4.4287391 -4.4287257 -4.4287062 -4.4287171 -4.4287415][-4.4288321 -4.4287739 -4.4287205 -4.4286752 -4.4286227 -4.4285436 -4.4284348 -4.4284215 -4.428514 -4.4286 -4.4286385 -4.4286408 -4.4286175 -4.4286294 -4.428668][-4.4288006 -4.4287357 -4.4286666 -4.4286017 -4.4285283 -4.4284215 -4.4282861 -4.4282537 -4.428318 -4.4284148 -4.4285011 -4.428545 -4.4285417 -4.4285607 -4.4286108][-4.4287987 -4.4287467 -4.4286876 -4.4286261 -4.4285417 -4.428412 -4.4282579 -4.4282084 -4.4282327 -4.4283185 -4.4284406 -4.4285293 -4.4285588 -4.4285846 -4.4286208][-4.4288249 -4.4287934 -4.4287586 -4.4287219 -4.4286594 -4.428544 -4.4284163 -4.4283872 -4.4284015 -4.42845 -4.4285407 -4.428618 -4.4286551 -4.4286766 -4.4286885][-4.4288583 -4.42885 -4.4288383 -4.4288192 -4.4287748 -4.4286866 -4.4285994 -4.4286003 -4.42862 -4.4286356 -4.4286785 -4.4287238 -4.4287462 -4.4287572 -4.4287534][-4.4288921 -4.428895 -4.4288874 -4.4288731 -4.4288406 -4.4287739 -4.4287171 -4.428741 -4.4287643 -4.428762 -4.4287691 -4.4287858 -4.428792 -4.428792 -4.4287791][-4.4289103 -4.4289179 -4.4289079 -4.4288917 -4.4288707 -4.4288206 -4.428782 -4.4288096 -4.4288321 -4.4288225 -4.4288116 -4.4288063 -4.4287968 -4.4287848 -4.428771][-4.4289122 -4.4289188 -4.4289055 -4.4288893 -4.4288788 -4.4288406 -4.4288073 -4.4288282 -4.4288454 -4.428833 -4.4288125 -4.428791 -4.4287715 -4.4287539 -4.4287438][-4.4289036 -4.4289074 -4.4288931 -4.4288793 -4.4288807 -4.4288578 -4.428833 -4.4288468 -4.4288621 -4.4288559 -4.4288368 -4.4288087 -4.4287834 -4.4287639 -4.4287539]]...]
INFO - root - 2017-12-08 07:20:26.890817: step 39110, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 17h:54m:39s remains)
INFO - root - 2017-12-08 07:20:29.120766: step 39120, loss = 2.28, batch loss = 2.23 (35.8 examples/sec; 0.224 sec/batch; 18h:13m:13s remains)
INFO - root - 2017-12-08 07:20:31.339948: step 39130, loss = 2.28, batch loss = 2.23 (36.0 examples/sec; 0.222 sec/batch; 18h:05m:11s remains)
INFO - root - 2017-12-08 07:20:33.577412: step 39140, loss = 2.28, batch loss = 2.23 (35.8 examples/sec; 0.223 sec/batch; 18h:12m:14s remains)
INFO - root - 2017-12-08 07:20:35.809158: step 39150, loss = 2.28, batch loss = 2.23 (35.7 examples/sec; 0.224 sec/batch; 18h:16m:44s remains)
INFO - root - 2017-12-08 07:20:38.036338: step 39160, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.222 sec/batch; 18h:03m:07s remains)
INFO - root - 2017-12-08 07:20:40.253783: step 39170, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.226 sec/batch; 18h:23m:12s remains)
INFO - root - 2017-12-08 07:20:42.467291: step 39180, loss = 2.28, batch loss = 2.23 (34.0 examples/sec; 0.236 sec/batch; 19h:11m:38s remains)
INFO - root - 2017-12-08 07:20:44.700354: step 39190, loss = 2.28, batch loss = 2.23 (36.7 examples/sec; 0.218 sec/batch; 17h:46m:09s remains)
INFO - root - 2017-12-08 07:20:46.950185: step 39200, loss = 2.28, batch loss = 2.23 (37.1 examples/sec; 0.215 sec/batch; 17h:33m:13s remains)
2017-12-08 07:20:47.250483: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4287696 -4.4287658 -4.4287868 -4.4288254 -4.4288573 -4.428864 -4.4288597 -4.4288764 -4.4288855 -4.428896 -4.4289179 -4.4289389 -4.4289427 -4.428936 -4.4289083][-4.4287944 -4.4287844 -4.4287853 -4.4287977 -4.4287896 -4.4287739 -4.4287772 -4.4288096 -4.428823 -4.4288416 -4.4288754 -4.4288983 -4.4288974 -4.4288874 -4.4288416][-4.4288197 -4.4288034 -4.428782 -4.4287524 -4.4286928 -4.4286571 -4.4286771 -4.4287314 -4.4287581 -4.4287944 -4.42884 -4.4288592 -4.4288435 -4.4288173 -4.4287486][-4.4288416 -4.4288239 -4.4287853 -4.4287238 -4.4286337 -4.4285836 -4.4286051 -4.4286647 -4.4287038 -4.4287658 -4.4288292 -4.428844 -4.4288135 -4.42877 -4.4286876][-4.4288568 -4.4288378 -4.4287872 -4.4287047 -4.4286046 -4.4285426 -4.4285417 -4.4285789 -4.428628 -4.4287262 -4.4288149 -4.4288335 -4.428793 -4.428741 -4.4286623][-4.428853 -4.4288321 -4.42877 -4.4286752 -4.4285827 -4.4285126 -4.4284573 -4.4284363 -4.4284973 -4.4286451 -4.4287639 -4.4287982 -4.4287658 -4.4287243 -4.4286661][-4.428822 -4.4287963 -4.4287238 -4.42862 -4.4285288 -4.4284286 -4.4282813 -4.4281631 -4.4282293 -4.4284492 -4.42862 -4.4286833 -4.4286752 -4.428659 -4.4286294][-4.4287562 -4.4287262 -4.4286566 -4.4285588 -4.42846 -4.4283123 -4.4280496 -4.4278007 -4.4278526 -4.4281507 -4.4283924 -4.4284968 -4.4285245 -4.4285355 -4.428534][-4.4286661 -4.4286466 -4.4286003 -4.4285388 -4.4284649 -4.4283218 -4.4280334 -4.4277272 -4.4277258 -4.4280038 -4.4282532 -4.4283643 -4.4284182 -4.4284654 -4.428493][-4.4285922 -4.4286032 -4.4285965 -4.4285822 -4.4285583 -4.4284849 -4.428299 -4.4280944 -4.4280639 -4.4282002 -4.4283404 -4.4284034 -4.4284492 -4.4285049 -4.4285436][-4.4285917 -4.4286385 -4.4286666 -4.4286861 -4.4286947 -4.4286819 -4.4286008 -4.4284973 -4.428463 -4.4284978 -4.4285364 -4.4285488 -4.4285774 -4.4286242 -4.4286571][-4.4286714 -4.42873 -4.4287696 -4.4287953 -4.4288116 -4.428822 -4.4287958 -4.4287519 -4.4287252 -4.4287148 -4.428709 -4.4287024 -4.4287186 -4.4287524 -4.4287772][-4.4287577 -4.4288096 -4.4288516 -4.4288793 -4.4288983 -4.4289145 -4.4289122 -4.428896 -4.4288774 -4.4288592 -4.4288425 -4.4288311 -4.4288349 -4.4288507 -4.428864][-4.4288235 -4.4288645 -4.4289031 -4.4289284 -4.4289465 -4.4289627 -4.4289689 -4.4289637 -4.4289522 -4.4289403 -4.4289312 -4.4289227 -4.4289188 -4.4289217 -4.4289269][-4.42888 -4.4289026 -4.4289293 -4.4289484 -4.4289622 -4.4289746 -4.4289837 -4.4289875 -4.4289851 -4.4289789 -4.4289751 -4.4289708 -4.428967 -4.428966 -4.428967]]...]
INFO - root - 2017-12-08 07:20:49.482860: step 39210, loss = 2.28, batch loss = 2.23 (36.8 examples/sec; 0.218 sec/batch; 17h:44m:01s remains)
INFO - root - 2017-12-08 07:20:51.713285: step 39220, loss = 2.28, batch loss = 2.23 (36.6 examples/sec; 0.219 sec/batch; 17h:48m:46s remains)
INFO - root - 2017-12-08 07:20:53.931522: step 39230, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 17h:54m:26s remains)
INFO - root - 2017-12-08 07:20:56.184047: step 39240, loss = 2.28, batch loss = 2.23 (35.1 examples/sec; 0.228 sec/batch; 18h:33m:47s remains)
INFO - root - 2017-12-08 07:20:58.449623: step 39250, loss = 2.28, batch loss = 2.23 (35.1 examples/sec; 0.228 sec/batch; 18h:35m:06s remains)
INFO - root - 2017-12-08 07:21:00.699627: step 39260, loss = 2.28, batch loss = 2.23 (34.7 examples/sec; 0.230 sec/batch; 18h:46m:28s remains)
INFO - root - 2017-12-08 07:21:02.933758: step 39270, loss = 2.28, batch loss = 2.23 (36.7 examples/sec; 0.218 sec/batch; 17h:46m:14s remains)
INFO - root - 2017-12-08 07:21:05.160880: step 39280, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.222 sec/batch; 18h:02m:44s remains)
INFO - root - 2017-12-08 07:21:07.403187: step 39290, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.221 sec/batch; 18h:01m:53s remains)
INFO - root - 2017-12-08 07:21:09.637846: step 39300, loss = 2.28, batch loss = 2.23 (35.3 examples/sec; 0.226 sec/batch; 18h:26m:22s remains)
2017-12-08 07:21:09.929731: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.42879 -4.4287467 -4.4287214 -4.4287148 -4.4287195 -4.4287319 -4.4287491 -4.4287596 -4.4287786 -4.428813 -4.4288392 -4.4288368 -4.428813 -4.4287858 -4.4287663][-4.4287724 -4.4287438 -4.428741 -4.4287515 -4.4287605 -4.4287648 -4.4287729 -4.4287729 -4.4287834 -4.428813 -4.4288363 -4.4288282 -4.4287982 -4.4287686 -4.4287496][-4.4287791 -4.42877 -4.4287882 -4.4288063 -4.4288087 -4.4288068 -4.4288063 -4.4287972 -4.4287953 -4.428812 -4.4288278 -4.4288187 -4.4287887 -4.42876 -4.4287457][-4.4287815 -4.428781 -4.4288082 -4.4288254 -4.4288197 -4.4288116 -4.4288034 -4.4287896 -4.428781 -4.4287891 -4.4288 -4.4287858 -4.4287534 -4.4287324 -4.4287229][-4.4287896 -4.4287806 -4.4287949 -4.428803 -4.4287891 -4.4287744 -4.4287567 -4.4287386 -4.4287329 -4.42874 -4.4287548 -4.4287357 -4.4287014 -4.4286871 -4.4286866][-4.428793 -4.42876 -4.4287486 -4.4287429 -4.4287233 -4.4287057 -4.4286861 -4.4286671 -4.4286642 -4.428678 -4.4286947 -4.4286771 -4.4286418 -4.4286327 -4.428648][-4.4287753 -4.4287167 -4.4286866 -4.4286637 -4.4286313 -4.4286017 -4.4285727 -4.4285474 -4.4285507 -4.4285884 -4.4286194 -4.4286132 -4.4285874 -4.4285946 -4.4286351][-4.42875 -4.428668 -4.42862 -4.4285822 -4.4285316 -4.428473 -4.4284124 -4.428371 -4.4284039 -4.4284863 -4.4285526 -4.4285755 -4.4285722 -4.4286027 -4.4286637][-4.4287076 -4.4286156 -4.4285469 -4.4284883 -4.4284258 -4.4283538 -4.4282784 -4.4282408 -4.4283204 -4.4284511 -4.4285474 -4.4285903 -4.4286113 -4.4286528 -4.4287086][-4.42868 -4.428596 -4.4285264 -4.4284573 -4.4284005 -4.4283619 -4.4283352 -4.4283376 -4.4284096 -4.4285164 -4.4286041 -4.4286604 -4.428699 -4.4287357 -4.4287667][-4.4286804 -4.42862 -4.4285684 -4.4285216 -4.4284945 -4.4284978 -4.428514 -4.428534 -4.4285727 -4.4286342 -4.4287 -4.42876 -4.4287977 -4.4288177 -4.4288216][-4.4287009 -4.4286661 -4.4286356 -4.4286122 -4.4286103 -4.4286318 -4.4286575 -4.4286752 -4.4286942 -4.4287252 -4.4287634 -4.4288034 -4.4288278 -4.4288359 -4.42883][-4.4287066 -4.4286914 -4.4286809 -4.4286757 -4.4286842 -4.4287043 -4.4287186 -4.4287276 -4.4287424 -4.4287629 -4.42878 -4.4287915 -4.428793 -4.4287953 -4.4287996][-4.4287329 -4.4287262 -4.4287248 -4.4287286 -4.4287386 -4.4287567 -4.4287686 -4.4287767 -4.4287887 -4.428802 -4.4288034 -4.4287939 -4.4287839 -4.4287891 -4.4288044][-4.4288106 -4.4288011 -4.4287977 -4.4288039 -4.4288125 -4.428822 -4.4288263 -4.4288297 -4.4288378 -4.4288478 -4.4288421 -4.428822 -4.4288049 -4.4288096 -4.4288278]]...]
INFO - root - 2017-12-08 07:21:12.142856: step 39310, loss = 2.28, batch loss = 2.23 (37.0 examples/sec; 0.216 sec/batch; 17h:35m:38s remains)
INFO - root - 2017-12-08 07:21:14.384199: step 39320, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 18h:00m:14s remains)
INFO - root - 2017-12-08 07:21:16.613060: step 39330, loss = 2.28, batch loss = 2.23 (35.2 examples/sec; 0.227 sec/batch; 18h:30m:50s remains)
INFO - root - 2017-12-08 07:21:18.841250: step 39340, loss = 2.28, batch loss = 2.23 (35.7 examples/sec; 0.224 sec/batch; 18h:14m:54s remains)
INFO - root - 2017-12-08 07:21:21.087178: step 39350, loss = 2.28, batch loss = 2.23 (34.6 examples/sec; 0.231 sec/batch; 18h:48m:38s remains)
INFO - root - 2017-12-08 07:21:23.349160: step 39360, loss = 2.28, batch loss = 2.23 (32.4 examples/sec; 0.247 sec/batch; 20h:05m:26s remains)
INFO - root - 2017-12-08 07:21:25.599964: step 39370, loss = 2.28, batch loss = 2.23 (34.5 examples/sec; 0.232 sec/batch; 18h:52m:12s remains)
INFO - root - 2017-12-08 07:21:27.869095: step 39380, loss = 2.28, batch loss = 2.23 (35.8 examples/sec; 0.224 sec/batch; 18h:12m:54s remains)
INFO - root - 2017-12-08 07:21:30.115484: step 39390, loss = 2.28, batch loss = 2.23 (34.6 examples/sec; 0.231 sec/batch; 18h:50m:10s remains)
INFO - root - 2017-12-08 07:21:32.328302: step 39400, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.221 sec/batch; 18h:01m:28s remains)
2017-12-08 07:21:32.642760: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.428937 -4.4289303 -4.42892 -4.4289112 -4.4289012 -4.4288917 -4.4288845 -4.4288864 -4.4288974 -4.4289055 -4.428906 -4.4289041 -4.4289021 -4.4289041 -4.4289074][-4.4289203 -4.4289126 -4.4289012 -4.4288917 -4.4288821 -4.4288659 -4.4288478 -4.4288464 -4.4288664 -4.428884 -4.4288898 -4.4288907 -4.4288855 -4.428885 -4.4288845][-4.4288912 -4.4288831 -4.4288683 -4.4288549 -4.4288411 -4.4288111 -4.4287753 -4.4287686 -4.428802 -4.4288335 -4.4288464 -4.4288549 -4.428853 -4.4288487 -4.4288445][-4.4288573 -4.4288406 -4.4288168 -4.4287934 -4.4287677 -4.4287128 -4.428648 -4.4286275 -4.4286819 -4.4287429 -4.4287744 -4.4287977 -4.42881 -4.42881 -4.4288034][-4.4288297 -4.4288006 -4.428761 -4.4287219 -4.4286761 -4.42859 -4.4284773 -4.4284263 -4.4285154 -4.4286246 -4.4286842 -4.4287271 -4.4287548 -4.4287629 -4.428761][-4.4288068 -4.4287596 -4.4286971 -4.4286346 -4.4285645 -4.4284496 -4.4282765 -4.4281735 -4.4283028 -4.4284744 -4.4285703 -4.4286342 -4.4286723 -4.428689 -4.428699][-4.4287953 -4.4287257 -4.42864 -4.4285593 -4.4284744 -4.4283509 -4.4281464 -4.4280043 -4.4281492 -4.428371 -4.4285088 -4.4285903 -4.4286313 -4.428648 -4.4286594][-4.4288006 -4.4287229 -4.4286323 -4.4285507 -4.4284778 -4.4283814 -4.4282074 -4.4280844 -4.4281926 -4.4283891 -4.4285345 -4.428616 -4.4286537 -4.4286647 -4.4286704][-4.4288235 -4.4287581 -4.428688 -4.428628 -4.4285836 -4.4285293 -4.4284134 -4.4283218 -4.4283781 -4.4285092 -4.4286261 -4.4286919 -4.4287219 -4.4287257 -4.4287276][-4.428853 -4.4288058 -4.428761 -4.4287291 -4.428709 -4.4286833 -4.4286146 -4.4285502 -4.4285717 -4.4286523 -4.4287338 -4.4287763 -4.4287968 -4.4287972 -4.4287968][-4.4288831 -4.4288516 -4.4288278 -4.4288177 -4.4288149 -4.4288054 -4.4287672 -4.4287267 -4.4287395 -4.4287882 -4.4288378 -4.4288559 -4.4288669 -4.4288654 -4.428863][-4.4289141 -4.4288931 -4.4288816 -4.4288855 -4.4288969 -4.4289 -4.4288793 -4.4288549 -4.4288597 -4.4288826 -4.4289036 -4.4289083 -4.4289155 -4.4289122 -4.4289069][-4.4289389 -4.42893 -4.4289279 -4.42894 -4.4289603 -4.4289727 -4.4289622 -4.4289389 -4.428925 -4.4289193 -4.4289193 -4.4289212 -4.4289303 -4.4289308 -4.4289279][-4.4289527 -4.4289489 -4.42895 -4.4289613 -4.4289813 -4.4289923 -4.4289851 -4.4289651 -4.4289432 -4.4289265 -4.4289193 -4.4289222 -4.4289308 -4.4289331 -4.4289341][-4.4289627 -4.42896 -4.4289589 -4.4289641 -4.4289751 -4.42898 -4.4289722 -4.428957 -4.4289384 -4.4289246 -4.4289179 -4.42892 -4.4289255 -4.4289284 -4.4289322]]...]
INFO - root - 2017-12-08 07:21:34.912690: step 39410, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 17h:54m:26s remains)
INFO - root - 2017-12-08 07:21:37.152544: step 39420, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.222 sec/batch; 18h:02m:50s remains)
INFO - root - 2017-12-08 07:21:39.387106: step 39430, loss = 2.28, batch loss = 2.23 (37.0 examples/sec; 0.216 sec/batch; 17h:36m:21s remains)
INFO - root - 2017-12-08 07:21:41.615214: step 39440, loss = 2.28, batch loss = 2.23 (34.8 examples/sec; 0.230 sec/batch; 18h:42m:23s remains)
INFO - root - 2017-12-08 07:21:43.866658: step 39450, loss = 2.28, batch loss = 2.23 (33.6 examples/sec; 0.238 sec/batch; 19h:23m:22s remains)
INFO - root - 2017-12-08 07:21:46.107415: step 39460, loss = 2.28, batch loss = 2.23 (34.4 examples/sec; 0.232 sec/batch; 18h:55m:03s remains)
INFO - root - 2017-12-08 07:21:48.330342: step 39470, loss = 2.28, batch loss = 2.23 (37.3 examples/sec; 0.215 sec/batch; 17h:28m:00s remains)
INFO - root - 2017-12-08 07:21:50.546415: step 39480, loss = 2.28, batch loss = 2.23 (37.1 examples/sec; 0.215 sec/batch; 17h:31m:50s remains)
INFO - root - 2017-12-08 07:21:52.790683: step 39490, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 17h:49m:35s remains)
INFO - root - 2017-12-08 07:21:55.017716: step 39500, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 17h:52m:00s remains)
2017-12-08 07:21:55.319693: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4287543 -4.4287567 -4.4287586 -4.4287739 -4.4287791 -4.4287748 -4.428781 -4.4287891 -4.4288087 -4.4288321 -4.4288292 -4.4287863 -4.4287438 -4.428699 -4.4286561][-4.4286494 -4.428689 -4.4287224 -4.4287543 -4.4287767 -4.42879 -4.4287977 -4.4287972 -4.4288096 -4.4288321 -4.4288278 -4.4287672 -4.4286876 -4.4286132 -4.4285483][-4.4285145 -4.4286013 -4.4286942 -4.4287577 -4.4287887 -4.428802 -4.4287934 -4.428771 -4.4287653 -4.428772 -4.4287591 -4.4286947 -4.4285984 -4.4285092 -4.428443][-4.4284129 -4.4285431 -4.4286885 -4.4287691 -4.4287944 -4.4287863 -4.4287424 -4.428688 -4.4286633 -4.4286623 -4.4286604 -4.4286175 -4.428546 -4.4284825 -4.428442][-4.4284806 -4.428616 -4.4287491 -4.4287968 -4.4287839 -4.4287286 -4.428637 -4.4285512 -4.4285231 -4.4285421 -4.4285736 -4.4285822 -4.4285693 -4.428555 -4.42854][-4.4286604 -4.42875 -4.4288158 -4.4287977 -4.4287295 -4.4286289 -4.4284911 -4.4283862 -4.4283886 -4.4284573 -4.4285383 -4.4286051 -4.4286418 -4.4286575 -4.4286489][-4.4287863 -4.42882 -4.42881 -4.4287257 -4.4285975 -4.4284582 -4.4282832 -4.4281678 -4.4282451 -4.4283943 -4.4285254 -4.4286318 -4.4287 -4.42872 -4.4286933][-4.428823 -4.4288182 -4.4287543 -4.4286246 -4.4284577 -4.4282994 -4.4281173 -4.4280181 -4.4281712 -4.4283748 -4.4285235 -4.4286408 -4.4287181 -4.42873 -4.4286795][-4.4288287 -4.4287968 -4.4287024 -4.4285703 -4.4284205 -4.4282966 -4.4281836 -4.4281416 -4.4282823 -4.4284592 -4.4285779 -4.4286675 -4.4287324 -4.4287362 -4.4286795][-4.4288015 -4.4287615 -4.4286671 -4.4285593 -4.4284573 -4.4283881 -4.4283519 -4.4283533 -4.4284468 -4.4285665 -4.4286427 -4.4286966 -4.42874 -4.4287405 -4.4286962][-4.4287758 -4.4287324 -4.4286442 -4.4285617 -4.4285016 -4.4284735 -4.4284859 -4.4285159 -4.42858 -4.4286532 -4.4286976 -4.42872 -4.4287415 -4.4287448 -4.4287176][-4.4287992 -4.4287634 -4.4286976 -4.4286489 -4.4286227 -4.4286175 -4.428647 -4.4286814 -4.4287171 -4.4287539 -4.4287767 -4.4287796 -4.4287848 -4.428791 -4.4287782][-4.42888 -4.4288607 -4.4288259 -4.4288068 -4.4287982 -4.4287972 -4.4288192 -4.4288425 -4.4288545 -4.4288683 -4.42888 -4.4288793 -4.4288797 -4.4288869 -4.4288831][-4.4289589 -4.4289527 -4.428937 -4.4289327 -4.4289303 -4.4289312 -4.428946 -4.4289594 -4.4289637 -4.4289703 -4.4289789 -4.42898 -4.4289789 -4.4289808 -4.4289765][-4.4290023 -4.4290004 -4.4289942 -4.4289947 -4.4289947 -4.4289951 -4.4290032 -4.4290104 -4.4290133 -4.4290166 -4.4290223 -4.4290237 -4.4290209 -4.4290204 -4.4290156]]...]
INFO - root - 2017-12-08 07:21:57.520844: step 39510, loss = 2.28, batch loss = 2.23 (37.1 examples/sec; 0.216 sec/batch; 17h:32m:38s remains)
INFO - root - 2017-12-08 07:21:59.807045: step 39520, loss = 2.28, batch loss = 2.23 (34.9 examples/sec; 0.229 sec/batch; 18h:37m:53s remains)
INFO - root - 2017-12-08 07:22:02.056882: step 39530, loss = 2.28, batch loss = 2.23 (35.4 examples/sec; 0.226 sec/batch; 18h:23m:41s remains)
INFO - root - 2017-12-08 07:22:04.278597: step 39540, loss = 2.28, batch loss = 2.23 (35.2 examples/sec; 0.227 sec/batch; 18h:30m:33s remains)
INFO - root - 2017-12-08 07:22:06.510372: step 39550, loss = 2.28, batch loss = 2.23 (35.4 examples/sec; 0.226 sec/batch; 18h:22m:08s remains)
INFO - root - 2017-12-08 07:22:08.789785: step 39560, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 17h:51m:32s remains)
INFO - root - 2017-12-08 07:22:11.009901: step 39570, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 17h:50m:16s remains)
INFO - root - 2017-12-08 07:22:13.248070: step 39580, loss = 2.28, batch loss = 2.23 (36.6 examples/sec; 0.218 sec/batch; 17h:46m:14s remains)
INFO - root - 2017-12-08 07:22:15.504968: step 39590, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 18h:00m:11s remains)
INFO - root - 2017-12-08 07:22:17.729902: step 39600, loss = 2.28, batch loss = 2.23 (35.8 examples/sec; 0.223 sec/batch; 18h:10m:56s remains)
2017-12-08 07:22:18.007765: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4286251 -4.4286585 -4.4287171 -4.4287663 -4.4287744 -4.4287591 -4.4287486 -4.4287653 -4.4288139 -4.4288611 -4.4288788 -4.4288769 -4.4288645 -4.428854 -4.4288573][-4.4285855 -4.4286284 -4.42871 -4.428771 -4.4287786 -4.4287572 -4.4287338 -4.428731 -4.4287643 -4.4288068 -4.4288292 -4.4288344 -4.4288273 -4.4288149 -4.4288077][-4.4285703 -4.4286232 -4.4287162 -4.42878 -4.4287844 -4.4287539 -4.4287152 -4.4286966 -4.4287214 -4.4287639 -4.4287906 -4.4288025 -4.4287963 -4.4287791 -4.4287562][-4.4285851 -4.4286394 -4.4287271 -4.4287839 -4.4287863 -4.4287415 -4.4286761 -4.4286389 -4.4286642 -4.4287162 -4.428751 -4.4287663 -4.4287624 -4.4287362 -4.4286904][-4.4286232 -4.4286761 -4.428751 -4.4287977 -4.4287934 -4.4287238 -4.4286103 -4.4285412 -4.4285731 -4.4286461 -4.4286966 -4.4287167 -4.4287114 -4.4286728 -4.4286084][-4.428659 -4.4287124 -4.4287758 -4.4288149 -4.4287949 -4.4286928 -4.4285159 -4.4283943 -4.4284329 -4.4285526 -4.4286442 -4.4286857 -4.4286838 -4.4286432 -4.4285736][-4.4286776 -4.4287319 -4.4287872 -4.4288139 -4.4287758 -4.4286427 -4.4284086 -4.4282293 -4.4282722 -4.4284511 -4.4285989 -4.4286628 -4.4286685 -4.428637 -4.4285755][-4.4286675 -4.42872 -4.4287705 -4.4287891 -4.4287395 -4.4285965 -4.4283485 -4.4281464 -4.4281859 -4.4283934 -4.4285688 -4.4286413 -4.4286523 -4.4286304 -4.4285855][-4.4286361 -4.4286766 -4.4287267 -4.428751 -4.4287071 -4.4285831 -4.428381 -4.4282165 -4.428247 -4.4284191 -4.4285669 -4.4286275 -4.4286394 -4.4286323 -4.4286151][-4.4286013 -4.428627 -4.4286776 -4.4287124 -4.4286866 -4.4285932 -4.428453 -4.4283481 -4.4283695 -4.4284792 -4.4285779 -4.4286203 -4.428628 -4.4286294 -4.4286294][-4.4285669 -4.4285731 -4.4286175 -4.4286609 -4.4286542 -4.4285874 -4.4284921 -4.4284253 -4.4284439 -4.4285126 -4.428575 -4.4286065 -4.4286084 -4.4286075 -4.4286094][-4.42854 -4.4285316 -4.4285684 -4.428618 -4.4286256 -4.4285812 -4.4285154 -4.4284759 -4.4284968 -4.4285455 -4.4285865 -4.4286056 -4.4286036 -4.4286017 -4.4286032][-4.4285426 -4.4285297 -4.4285583 -4.4286046 -4.4286194 -4.4285922 -4.4285512 -4.4285364 -4.428565 -4.4286032 -4.4286256 -4.4286318 -4.428627 -4.4286242 -4.4286222][-4.4285741 -4.4285722 -4.428597 -4.4286337 -4.4286489 -4.42863 -4.4286089 -4.4286151 -4.4286466 -4.4286718 -4.4286795 -4.4286785 -4.4286704 -4.4286637 -4.4286623][-4.4286218 -4.428628 -4.428648 -4.4286776 -4.4286914 -4.4286814 -4.4286733 -4.4286842 -4.4287076 -4.4287229 -4.4287248 -4.428719 -4.4287124 -4.4287119 -4.4287171]]...]
INFO - root - 2017-12-08 07:22:20.270060: step 39610, loss = 2.28, batch loss = 2.23 (34.4 examples/sec; 0.232 sec/batch; 18h:53m:41s remains)
INFO - root - 2017-12-08 07:22:22.513507: step 39620, loss = 2.28, batch loss = 2.23 (34.8 examples/sec; 0.230 sec/batch; 18h:42m:52s remains)
INFO - root - 2017-12-08 07:22:24.771235: step 39630, loss = 2.28, batch loss = 2.23 (36.0 examples/sec; 0.222 sec/batch; 18h:04m:17s remains)
INFO - root - 2017-12-08 07:22:27.003554: step 39640, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 17h:53m:02s remains)
INFO - root - 2017-12-08 07:22:29.239836: step 39650, loss = 2.28, batch loss = 2.23 (36.0 examples/sec; 0.222 sec/batch; 18h:03m:56s remains)
INFO - root - 2017-12-08 07:22:31.462979: step 39660, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.221 sec/batch; 18h:00m:17s remains)
INFO - root - 2017-12-08 07:22:33.754713: step 39670, loss = 2.28, batch loss = 2.23 (33.9 examples/sec; 0.236 sec/batch; 19h:13m:22s remains)
INFO - root - 2017-12-08 07:22:35.972129: step 39680, loss = 2.28, batch loss = 2.23 (35.3 examples/sec; 0.227 sec/batch; 18h:27m:34s remains)
INFO - root - 2017-12-08 07:22:38.203803: step 39690, loss = 2.28, batch loss = 2.23 (36.7 examples/sec; 0.218 sec/batch; 17h:44m:01s remains)
INFO - root - 2017-12-08 07:22:40.441677: step 39700, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 17h:59m:00s remains)
2017-12-08 07:22:40.731128: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4287648 -4.4288034 -4.4288092 -4.4287977 -4.4287877 -4.4288025 -4.428844 -4.428895 -4.4289351 -4.4289589 -4.4289689 -4.4289722 -4.4289742 -4.4289732 -4.4289632][-4.4288278 -4.4288564 -4.4288492 -4.4288206 -4.4287896 -4.4287858 -4.4288139 -4.4288597 -4.4289021 -4.4289327 -4.4289517 -4.4289632 -4.4289694 -4.4289651 -4.4289436][-4.4288816 -4.4289055 -4.4288964 -4.4288635 -4.428823 -4.4288025 -4.4288116 -4.4288406 -4.428875 -4.4289069 -4.4289351 -4.4289541 -4.4289637 -4.4289551 -4.4289222][-4.4289107 -4.4289322 -4.4289293 -4.4289055 -4.4288707 -4.4288416 -4.4288292 -4.428833 -4.4288507 -4.4288797 -4.428916 -4.4289451 -4.4289603 -4.4289527 -4.4289184][-4.428915 -4.4289355 -4.4289408 -4.4289289 -4.4289017 -4.428864 -4.4288235 -4.4287944 -4.4287934 -4.4288225 -4.4288726 -4.4289184 -4.4289474 -4.4289532 -4.4289312][-4.4288921 -4.4289145 -4.4289269 -4.4289255 -4.4289012 -4.4288468 -4.4287734 -4.4287105 -4.4286947 -4.4287295 -4.4287972 -4.4288626 -4.4289103 -4.4289346 -4.4289341][-4.4288597 -4.4288812 -4.4288936 -4.4288917 -4.4288573 -4.4287772 -4.4286675 -4.4285746 -4.4285512 -4.4285984 -4.4286876 -4.4287744 -4.4288416 -4.428884 -4.4289026][-4.4288354 -4.4288492 -4.428853 -4.4288392 -4.4287872 -4.4286785 -4.4285355 -4.4284153 -4.4283881 -4.4284506 -4.4285603 -4.4286656 -4.42875 -4.42881 -4.4288473][-4.4288259 -4.4288306 -4.4288206 -4.4287896 -4.4287181 -4.4285865 -4.4284186 -4.4282804 -4.4282527 -4.4283285 -4.4284534 -4.4285712 -4.428669 -4.4287405 -4.4287868][-4.4288058 -4.4288039 -4.4287858 -4.4287453 -4.4286666 -4.4285321 -4.4283657 -4.4282322 -4.4282112 -4.4282923 -4.4284196 -4.4285417 -4.4286418 -4.4287095 -4.428751][-4.4287782 -4.4287748 -4.4287596 -4.4287257 -4.4286628 -4.4285555 -4.4284244 -4.4283242 -4.4283147 -4.4283867 -4.4284973 -4.4286046 -4.4286876 -4.4287381 -4.4287643][-4.4287472 -4.4287477 -4.4287448 -4.4287329 -4.4287057 -4.4286489 -4.4285746 -4.4285226 -4.4285269 -4.4285822 -4.428658 -4.4287286 -4.4287791 -4.4288044 -4.428813][-4.4287267 -4.4287319 -4.4287419 -4.4287558 -4.4287682 -4.4287624 -4.4287419 -4.4287257 -4.4287357 -4.4287691 -4.4288092 -4.4288416 -4.42886 -4.428863 -4.4288592][-4.4287314 -4.4287362 -4.42875 -4.4287758 -4.42881 -4.4288344 -4.4288449 -4.4288459 -4.4288511 -4.4288645 -4.4288783 -4.4288859 -4.4288845 -4.4288778 -4.4288721][-4.4287553 -4.4287529 -4.4287591 -4.428781 -4.4288182 -4.4288507 -4.4288692 -4.4288745 -4.428875 -4.4288754 -4.4288731 -4.4288664 -4.4288578 -4.4288516 -4.4288511]]...]
INFO - root - 2017-12-08 07:22:42.961355: step 39710, loss = 2.28, batch loss = 2.23 (34.7 examples/sec; 0.231 sec/batch; 18h:46m:28s remains)
INFO - root - 2017-12-08 07:22:45.189024: step 39720, loss = 2.28, batch loss = 2.23 (34.2 examples/sec; 0.234 sec/batch; 19h:00m:47s remains)
INFO - root - 2017-12-08 07:22:47.424267: step 39730, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.220 sec/batch; 17h:55m:07s remains)
INFO - root - 2017-12-08 07:22:49.662410: step 39740, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.220 sec/batch; 17h:55m:35s remains)
INFO - root - 2017-12-08 07:22:51.908448: step 39750, loss = 2.28, batch loss = 2.23 (35.3 examples/sec; 0.227 sec/batch; 18h:25m:55s remains)
INFO - root - 2017-12-08 07:22:54.134546: step 39760, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.221 sec/batch; 18h:00m:17s remains)
INFO - root - 2017-12-08 07:22:56.366649: step 39770, loss = 2.28, batch loss = 2.23 (35.6 examples/sec; 0.225 sec/batch; 18h:17m:37s remains)
INFO - root - 2017-12-08 07:22:58.593073: step 39780, loss = 2.28, batch loss = 2.23 (34.9 examples/sec; 0.229 sec/batch; 18h:37m:09s remains)
INFO - root - 2017-12-08 07:23:00.846098: step 39790, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 18h:08m:11s remains)
INFO - root - 2017-12-08 07:23:03.083605: step 39800, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 18h:07m:11s remains)
2017-12-08 07:23:03.369981: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289527 -4.4289336 -4.4288945 -4.4288507 -4.4288158 -4.4287786 -4.428741 -4.4287133 -4.42874 -4.4287271 -4.4287043 -4.4287243 -4.4287548 -4.4287848 -4.4288139][-4.428957 -4.4289393 -4.4289 -4.4288535 -4.4288216 -4.4287858 -4.4287457 -4.4287195 -4.4287472 -4.4287333 -4.4287004 -4.4287086 -4.4287224 -4.4287415 -4.4287682][-4.4289584 -4.4289422 -4.4289055 -4.4288645 -4.4288387 -4.4288044 -4.4287581 -4.4287291 -4.4287558 -4.4287391 -4.4287 -4.4286928 -4.4286857 -4.4286966 -4.4287229][-4.4289575 -4.4289379 -4.4288974 -4.428854 -4.4288287 -4.4287925 -4.4287429 -4.428719 -4.428741 -4.4287195 -4.428679 -4.4286642 -4.42865 -4.42866 -4.4286895][-4.4289527 -4.4289246 -4.4288721 -4.4288144 -4.4287729 -4.42873 -4.4286861 -4.4286709 -4.4286833 -4.4286537 -4.4286189 -4.4286113 -4.4286041 -4.4286294 -4.4286613][-4.4289441 -4.4289041 -4.4288321 -4.4287448 -4.4286695 -4.4286103 -4.4285631 -4.4285474 -4.4285517 -4.4285192 -4.4284921 -4.4285026 -4.4285197 -4.4285827 -4.4286275][-4.428937 -4.42889 -4.4287992 -4.42868 -4.4285674 -4.42849 -4.4284377 -4.4284148 -4.4284291 -4.428412 -4.4283948 -4.4284244 -4.4284744 -4.4285727 -4.4286242][-4.4289317 -4.4288821 -4.4287858 -4.4286571 -4.4285355 -4.42845 -4.4283981 -4.4283829 -4.4284348 -4.4284654 -4.4284654 -4.4285 -4.4285593 -4.4286561 -4.4286904][-4.4289265 -4.4288793 -4.4287891 -4.4286728 -4.4285765 -4.42851 -4.4284778 -4.4284797 -4.4285636 -4.4286237 -4.4286294 -4.4286537 -4.428699 -4.4287734 -4.428782][-4.4289255 -4.4288878 -4.4288139 -4.4287233 -4.4286623 -4.4286265 -4.4286122 -4.4286189 -4.428709 -4.4287682 -4.4287691 -4.4287758 -4.4288011 -4.4288464 -4.4288378][-4.4289279 -4.4289012 -4.4288468 -4.428782 -4.4287477 -4.4287314 -4.4287286 -4.4287367 -4.4288068 -4.4288445 -4.4288425 -4.4288363 -4.4288454 -4.4288669 -4.4288578][-4.4289274 -4.4289079 -4.4288735 -4.4288311 -4.4288173 -4.4288144 -4.4288163 -4.4288211 -4.4288635 -4.4288783 -4.4288721 -4.4288578 -4.4288526 -4.428853 -4.4288478][-4.4289284 -4.4289174 -4.4288988 -4.42887 -4.4288673 -4.4288688 -4.428863 -4.4288611 -4.428885 -4.4288874 -4.4288759 -4.4288578 -4.428843 -4.4288287 -4.4288182][-4.4289317 -4.428926 -4.428915 -4.4288936 -4.4288893 -4.428885 -4.4288635 -4.4288507 -4.4288626 -4.4288621 -4.4288507 -4.428834 -4.4288173 -4.4287992 -4.4287839][-4.4289322 -4.4289236 -4.4289122 -4.4288893 -4.4288754 -4.4288611 -4.428833 -4.4288177 -4.4288254 -4.4288273 -4.4288268 -4.4288177 -4.428803 -4.4287839 -4.4287658]]...]
INFO - root - 2017-12-08 07:23:05.605526: step 39810, loss = 2.28, batch loss = 2.23 (34.1 examples/sec; 0.235 sec/batch; 19h:04m:34s remains)
INFO - root - 2017-12-08 07:23:07.908026: step 39820, loss = 2.28, batch loss = 2.23 (26.9 examples/sec; 0.297 sec/batch; 24h:09m:40s remains)
INFO - root - 2017-12-08 07:23:10.174312: step 39830, loss = 2.28, batch loss = 2.23 (34.8 examples/sec; 0.230 sec/batch; 18h:42m:11s remains)
INFO - root - 2017-12-08 07:23:12.412718: step 39840, loss = 2.28, batch loss = 2.23 (32.9 examples/sec; 0.243 sec/batch; 19h:46m:05s remains)
INFO - root - 2017-12-08 07:23:14.666867: step 39850, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 17h:51m:38s remains)
INFO - root - 2017-12-08 07:23:16.962436: step 39860, loss = 2.28, batch loss = 2.23 (34.9 examples/sec; 0.229 sec/batch; 18h:37m:06s remains)
INFO - root - 2017-12-08 07:23:19.201816: step 39870, loss = 2.28, batch loss = 2.23 (36.0 examples/sec; 0.222 sec/batch; 18h:02m:50s remains)
INFO - root - 2017-12-08 07:23:21.436093: step 39880, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 17h:58m:42s remains)
INFO - root - 2017-12-08 07:23:23.699467: step 39890, loss = 2.28, batch loss = 2.23 (37.3 examples/sec; 0.215 sec/batch; 17h:26m:46s remains)
INFO - root - 2017-12-08 07:23:25.977932: step 39900, loss = 2.28, batch loss = 2.23 (35.3 examples/sec; 0.226 sec/batch; 18h:24m:01s remains)
2017-12-08 07:23:26.292988: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4285011 -4.4284568 -4.4284673 -4.4285417 -4.4286356 -4.4287119 -4.4287524 -4.4287333 -4.4286728 -4.4286075 -4.428565 -4.4285483 -4.4285192 -4.4285183 -4.4285727][-4.4285064 -4.4284635 -4.4284716 -4.4285426 -4.4286332 -4.4287119 -4.4287624 -4.4287596 -4.4287114 -4.4286494 -4.4286056 -4.4285846 -4.4285693 -4.4285936 -4.428658][-4.428524 -4.42847 -4.428463 -4.4285226 -4.4286079 -4.428689 -4.4287477 -4.428762 -4.4287291 -4.42867 -4.4286304 -4.428617 -4.4286318 -4.4286914 -4.4287677][-4.4285517 -4.4285107 -4.428494 -4.4285288 -4.4286036 -4.4286876 -4.4287515 -4.4287682 -4.428731 -4.4286675 -4.428628 -4.4286275 -4.4286728 -4.4287543 -4.4288273][-4.4286118 -4.4285793 -4.4285417 -4.4285483 -4.428616 -4.4287033 -4.4287572 -4.4287491 -4.4286942 -4.4286256 -4.4286003 -4.4286213 -4.4286852 -4.4287653 -4.4288197][-4.4286489 -4.4286003 -4.4285517 -4.4285607 -4.4286351 -4.4287167 -4.4287586 -4.4287319 -4.4286494 -4.428566 -4.4285603 -4.4286036 -4.428669 -4.4287224 -4.4287486][-4.428628 -4.4285545 -4.4285035 -4.4285378 -4.4286337 -4.4287171 -4.4287567 -4.4287186 -4.428616 -4.4285316 -4.4285393 -4.4285908 -4.4286418 -4.4286556 -4.4286537][-4.4285436 -4.4284582 -4.4284248 -4.4284911 -4.4286036 -4.4286923 -4.4287276 -4.4286804 -4.4285679 -4.4285026 -4.4285426 -4.4286089 -4.4286423 -4.4286151 -4.4285893][-4.4284897 -4.4283981 -4.4283853 -4.4284682 -4.4285822 -4.4286623 -4.4286776 -4.42861 -4.4284945 -4.4284663 -4.4285469 -4.428627 -4.428648 -4.4286017 -4.4285784][-4.4285364 -4.4284458 -4.4284377 -4.4285059 -4.4285936 -4.4286494 -4.4286451 -4.4285603 -4.4284482 -4.4284568 -4.4285541 -4.4286294 -4.42864 -4.4286017 -4.4285955][-4.4286466 -4.4285693 -4.4285507 -4.428586 -4.4286323 -4.428659 -4.4286485 -4.4285717 -4.4284868 -4.4285164 -4.4286003 -4.4286532 -4.4286571 -4.4286346 -4.4286456][-4.4287305 -4.4286776 -4.428658 -4.4286718 -4.4286938 -4.4287095 -4.4287028 -4.4286504 -4.4285979 -4.428627 -4.4286852 -4.4287181 -4.428721 -4.428709 -4.428719][-4.428771 -4.4287443 -4.4287376 -4.4287477 -4.4287643 -4.4287791 -4.4287772 -4.4287462 -4.4287176 -4.4287338 -4.4287682 -4.4287848 -4.4287806 -4.4287639 -4.428761][-4.4288154 -4.428802 -4.4288006 -4.4288092 -4.4288206 -4.4288344 -4.4288349 -4.4288192 -4.4288082 -4.4288173 -4.4288354 -4.4288411 -4.42883 -4.42881 -4.4287972][-4.4288616 -4.4288511 -4.4288464 -4.42885 -4.4288549 -4.428863 -4.4288659 -4.4288635 -4.428865 -4.4288712 -4.4288793 -4.4288855 -4.4288816 -4.4288669 -4.4288487]]...]
INFO - root - 2017-12-08 07:23:28.520741: step 39910, loss = 2.28, batch loss = 2.23 (36.0 examples/sec; 0.222 sec/batch; 18h:04m:31s remains)
INFO - root - 2017-12-08 07:23:30.770214: step 39920, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 17h:49m:16s remains)
INFO - root - 2017-12-08 07:23:33.011991: step 39930, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 17h:56m:34s remains)
INFO - root - 2017-12-08 07:23:35.233257: step 39940, loss = 2.28, batch loss = 2.23 (37.4 examples/sec; 0.214 sec/batch; 17h:23m:45s remains)
INFO - root - 2017-12-08 07:23:37.481727: step 39950, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 17h:48m:28s remains)
INFO - root - 2017-12-08 07:23:39.732881: step 39960, loss = 2.28, batch loss = 2.23 (34.5 examples/sec; 0.232 sec/batch; 18h:51m:38s remains)
INFO - root - 2017-12-08 07:23:41.961430: step 39970, loss = 2.28, batch loss = 2.23 (35.7 examples/sec; 0.224 sec/batch; 18h:13m:46s remains)
INFO - root - 2017-12-08 07:23:44.221182: step 39980, loss = 2.28, batch loss = 2.23 (35.8 examples/sec; 0.224 sec/batch; 18h:09m:42s remains)
INFO - root - 2017-12-08 07:23:46.447497: step 39990, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.220 sec/batch; 17h:53m:15s remains)
INFO - root - 2017-12-08 07:23:48.699401: step 40000, loss = 2.28, batch loss = 2.23 (36.6 examples/sec; 0.218 sec/batch; 17h:44m:54s remains)
2017-12-08 07:23:49.000930: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289718 -4.428946 -4.4289012 -4.4288621 -4.4288311 -4.4288125 -4.4288244 -4.4288564 -4.4288859 -4.4288945 -4.428894 -4.4288936 -4.4288893 -4.4288907 -4.4288969][-4.4289427 -4.4289155 -4.4288611 -4.4288111 -4.4287686 -4.4287395 -4.4287462 -4.428781 -4.4288192 -4.4288373 -4.4288397 -4.4288368 -4.4288263 -4.4288287 -4.4288383][-4.4289351 -4.42891 -4.4288549 -4.4287982 -4.4287424 -4.4286947 -4.42868 -4.4287 -4.4287453 -4.4287772 -4.4287848 -4.428782 -4.428771 -4.4287753 -4.4287868][-4.4289412 -4.4289236 -4.4288735 -4.4288025 -4.4287219 -4.4286432 -4.42859 -4.4285793 -4.4286256 -4.4286747 -4.4286976 -4.4287133 -4.4287248 -4.4287386 -4.4287562][-4.4289565 -4.4289446 -4.4288931 -4.4288073 -4.4286947 -4.4285636 -4.4284415 -4.4283895 -4.4284515 -4.4285331 -4.4285874 -4.4286356 -4.4286814 -4.4287176 -4.4287477][-4.4289746 -4.4289641 -4.42891 -4.4288173 -4.42867 -4.4284687 -4.4282513 -4.4281616 -4.428266 -4.4283996 -4.4284959 -4.4285765 -4.4286509 -4.4287057 -4.4287457][-4.4289937 -4.4289794 -4.4289188 -4.4288182 -4.4286523 -4.4284 -4.4281039 -4.4279943 -4.4281545 -4.428339 -4.4284678 -4.4285684 -4.4286509 -4.4287066 -4.4287434][-4.4290047 -4.4289842 -4.4289122 -4.4288135 -4.4286618 -4.4284248 -4.4281588 -4.4280915 -4.428247 -4.4284039 -4.428514 -4.4286008 -4.4286757 -4.4287233 -4.4287496][-4.429 -4.4289689 -4.4288907 -4.4288068 -4.4286861 -4.42851 -4.4283452 -4.4283309 -4.4284382 -4.4285283 -4.4285975 -4.4286518 -4.4286985 -4.4287271 -4.4287438][-4.42898 -4.4289412 -4.4288654 -4.428792 -4.4286909 -4.4285617 -4.428472 -4.4285 -4.4285865 -4.4286356 -4.4286771 -4.4287024 -4.4287176 -4.4287267 -4.4287357][-4.4289632 -4.4289212 -4.4288526 -4.4287858 -4.4287047 -4.4286036 -4.4285531 -4.4286017 -4.4286718 -4.4286985 -4.4287195 -4.4287257 -4.4287219 -4.4287257 -4.4287381][-4.42896 -4.4289246 -4.42887 -4.4288139 -4.4287505 -4.4286761 -4.4286518 -4.4286942 -4.4287434 -4.4287462 -4.4287462 -4.4287386 -4.428731 -4.42874 -4.4287586][-4.4289565 -4.4289351 -4.4289031 -4.4288611 -4.428813 -4.4287624 -4.4287529 -4.4287868 -4.4288192 -4.4288111 -4.4287934 -4.4287696 -4.4287553 -4.4287682 -4.4287934][-4.4289584 -4.4289503 -4.4289322 -4.4289017 -4.4288621 -4.428823 -4.4288168 -4.4288406 -4.4288645 -4.4288607 -4.4288416 -4.4288173 -4.4288054 -4.4288192 -4.4288425][-4.4289656 -4.4289637 -4.4289546 -4.4289355 -4.4289055 -4.428874 -4.4288673 -4.4288797 -4.4288955 -4.4289002 -4.4288931 -4.4288821 -4.4288769 -4.428885 -4.4288988]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-momentum-lr0.001-defgroup-1/model.ckpt-40000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-momentum-lr0.001-defgroup-1/model.ckpt-40000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-08 07:23:51.720047: step 40010, loss = 2.28, batch loss = 2.23 (35.4 examples/sec; 0.226 sec/batch; 18h:21m:38s remains)
INFO - root - 2017-12-08 07:23:53.993122: step 40020, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 17h:48m:37s remains)
INFO - root - 2017-12-08 07:23:56.229201: step 40030, loss = 2.28, batch loss = 2.23 (37.1 examples/sec; 0.216 sec/batch; 17h:31m:11s remains)
INFO - root - 2017-12-08 07:23:58.461821: step 40040, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 17h:48m:12s remains)
INFO - root - 2017-12-08 07:24:00.704676: step 40050, loss = 2.28, batch loss = 2.23 (35.2 examples/sec; 0.227 sec/batch; 18h:26m:36s remains)
INFO - root - 2017-12-08 07:24:02.949092: step 40060, loss = 2.28, batch loss = 2.23 (36.8 examples/sec; 0.218 sec/batch; 17h:40m:41s remains)
INFO - root - 2017-12-08 07:24:05.180368: step 40070, loss = 2.28, batch loss = 2.23 (37.5 examples/sec; 0.213 sec/batch; 17h:20m:27s remains)
INFO - root - 2017-12-08 07:24:07.425576: step 40080, loss = 2.28, batch loss = 2.23 (33.7 examples/sec; 0.238 sec/batch; 19h:17m:45s remains)
INFO - root - 2017-12-08 07:24:09.667858: step 40090, loss = 2.28, batch loss = 2.23 (36.9 examples/sec; 0.217 sec/batch; 17h:35m:12s remains)
INFO - root - 2017-12-08 07:24:11.929075: step 40100, loss = 2.28, batch loss = 2.23 (34.5 examples/sec; 0.232 sec/batch; 18h:48m:50s remains)
2017-12-08 07:24:12.232183: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4284625 -4.42857 -4.4286637 -4.4287133 -4.4286733 -4.4285936 -4.4285574 -4.4285994 -4.4286828 -4.4287362 -4.4287829 -4.4288454 -4.4288588 -4.4288125 -4.4287453][-4.4283476 -4.428494 -4.4286203 -4.4286985 -4.428679 -4.4286122 -4.4285903 -4.4286318 -4.4287062 -4.4287415 -4.4287686 -4.4288206 -4.4288306 -4.4287868 -4.4287119][-4.4283247 -4.4284596 -4.428586 -4.4286804 -4.4286947 -4.4286523 -4.4286356 -4.4286714 -4.428721 -4.4287343 -4.4287453 -4.4287858 -4.4287906 -4.4287539 -4.4286723][-4.4284058 -4.4284959 -4.4285893 -4.4286737 -4.4287105 -4.4286928 -4.4286819 -4.4287133 -4.42874 -4.42874 -4.4287405 -4.4287686 -4.4287648 -4.4287229 -4.4286275][-4.4285555 -4.4285927 -4.4286408 -4.4286771 -4.4286919 -4.4286704 -4.4286571 -4.428688 -4.428721 -4.4287333 -4.428741 -4.4287682 -4.4287653 -4.4287205 -4.4286175][-4.4286814 -4.4286942 -4.4287171 -4.4287109 -4.428678 -4.4286284 -4.4285793 -4.4285893 -4.4286485 -4.4286971 -4.4287362 -4.4287772 -4.4287786 -4.4287348 -4.4286323][-4.4287114 -4.4287233 -4.42874 -4.4287071 -4.4286327 -4.4285326 -4.4284129 -4.4283876 -4.4284968 -4.428607 -4.4286942 -4.4287605 -4.42878 -4.4287534 -4.4286685][-4.4286752 -4.428688 -4.4287052 -4.4286604 -4.4285574 -4.4283848 -4.42815 -4.4280567 -4.4282408 -4.4284463 -4.4286003 -4.4287033 -4.4287419 -4.4287333 -4.4286723][-4.4286666 -4.428688 -4.4287081 -4.4286609 -4.4285479 -4.4283361 -4.4279976 -4.4278007 -4.4280205 -4.4283013 -4.4285011 -4.4286251 -4.4286728 -4.4286795 -4.4286504][-4.4286828 -4.4287086 -4.4287467 -4.4287286 -4.4286356 -4.428453 -4.42815 -4.427937 -4.4280839 -4.42833 -4.4285073 -4.428606 -4.4286366 -4.4286466 -4.4286356][-4.4286857 -4.4287 -4.4287457 -4.4287753 -4.4287353 -4.4286165 -4.4284186 -4.428267 -4.4283242 -4.4284716 -4.4285908 -4.42865 -4.4286528 -4.4286456 -4.4286356][-4.4286919 -4.4286838 -4.4287148 -4.4287686 -4.4287767 -4.4287076 -4.4285822 -4.4284854 -4.42851 -4.4285975 -4.4286642 -4.4286866 -4.428658 -4.428618 -4.4285951][-4.4287105 -4.4286919 -4.4286942 -4.4287338 -4.4287591 -4.4287081 -4.4286203 -4.428565 -4.4285913 -4.4286551 -4.4286995 -4.4287024 -4.4286485 -4.4285755 -4.4285431][-4.4287477 -4.4287267 -4.4287028 -4.4287071 -4.4287186 -4.4286623 -4.4285893 -4.4285655 -4.4286127 -4.4286761 -4.4287186 -4.4287157 -4.4286423 -4.42855 -4.4285131][-4.4287605 -4.4287367 -4.428709 -4.4286995 -4.4286995 -4.4286437 -4.4285803 -4.4285665 -4.4286218 -4.4286861 -4.4287324 -4.4287271 -4.4286485 -4.4285507 -4.4285231]]...]
INFO - root - 2017-12-08 07:24:14.505654: step 40110, loss = 2.28, batch loss = 2.23 (33.0 examples/sec; 0.242 sec/batch; 19h:40m:26s remains)
INFO - root - 2017-12-08 07:24:16.755040: step 40120, loss = 2.28, batch loss = 2.23 (34.9 examples/sec; 0.229 sec/batch; 18h:36m:47s remains)
INFO - root - 2017-12-08 07:24:19.005951: step 40130, loss = 2.28, batch loss = 2.23 (36.7 examples/sec; 0.218 sec/batch; 17h:42m:22s remains)
INFO - root - 2017-12-08 07:24:21.261430: step 40140, loss = 2.28, batch loss = 2.23 (35.1 examples/sec; 0.228 sec/batch; 18h:31m:08s remains)
INFO - root - 2017-12-08 07:24:23.490023: step 40150, loss = 2.28, batch loss = 2.23 (34.5 examples/sec; 0.232 sec/batch; 18h:48m:32s remains)
INFO - root - 2017-12-08 07:24:25.752831: step 40160, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.221 sec/batch; 17h:58m:40s remains)
INFO - root - 2017-12-08 07:24:27.998149: step 40170, loss = 2.28, batch loss = 2.23 (36.8 examples/sec; 0.217 sec/batch; 17h:39m:19s remains)
INFO - root - 2017-12-08 07:24:30.260880: step 40180, loss = 2.28, batch loss = 2.23 (35.0 examples/sec; 0.228 sec/batch; 18h:33m:00s remains)
INFO - root - 2017-12-08 07:24:32.517433: step 40190, loss = 2.28, batch loss = 2.23 (35.2 examples/sec; 0.228 sec/batch; 18h:28m:35s remains)
INFO - root - 2017-12-08 07:24:34.763377: step 40200, loss = 2.28, batch loss = 2.23 (35.3 examples/sec; 0.227 sec/batch; 18h:25m:01s remains)
2017-12-08 07:24:35.070427: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289355 -4.428906 -4.4288673 -4.4288344 -4.4288263 -4.4288483 -4.4288626 -4.4288521 -4.4288425 -4.4288487 -4.428863 -4.4288788 -4.4288912 -4.4288964 -4.428905][-4.42894 -4.4289131 -4.4288788 -4.4288449 -4.4288282 -4.4288397 -4.4288468 -4.4288344 -4.4288316 -4.428844 -4.4288607 -4.4288716 -4.4288778 -4.4288778 -4.4288845][-4.4289408 -4.4289212 -4.428895 -4.42886 -4.4288321 -4.4288249 -4.4288192 -4.4288058 -4.428812 -4.4288373 -4.4288611 -4.4288731 -4.4288759 -4.4288697 -4.4288726][-4.4289455 -4.4289374 -4.4289212 -4.4288878 -4.4288487 -4.4288182 -4.4287915 -4.4287691 -4.42878 -4.42882 -4.4288597 -4.4288774 -4.4288726 -4.4288583 -4.4288564][-4.4289536 -4.428957 -4.4289522 -4.4289222 -4.42887 -4.4288087 -4.42875 -4.428709 -4.4287186 -4.4287753 -4.4288368 -4.4288654 -4.4288588 -4.4288363 -4.4288311][-4.4289641 -4.4289732 -4.4289737 -4.4289427 -4.4288754 -4.4287848 -4.428689 -4.4286218 -4.4286313 -4.428709 -4.4287949 -4.4288349 -4.428833 -4.4288044 -4.4287896][-4.4289746 -4.4289846 -4.4289832 -4.4289441 -4.4288654 -4.4287572 -4.4286327 -4.4285431 -4.4285564 -4.4286575 -4.42876 -4.4288068 -4.4288111 -4.4287806 -4.4287572][-4.4289813 -4.4289904 -4.4289813 -4.4289336 -4.4288492 -4.4287343 -4.4285951 -4.4284945 -4.4285135 -4.428628 -4.4287395 -4.4287915 -4.4288011 -4.4287724 -4.4287429][-4.4289832 -4.4289885 -4.4289742 -4.4289255 -4.4288445 -4.4287343 -4.4285936 -4.4284916 -4.4285064 -4.42862 -4.4287357 -4.4287925 -4.4288034 -4.4287729 -4.4287348][-4.4289856 -4.4289865 -4.4289718 -4.42893 -4.4288626 -4.4287663 -4.4286389 -4.4285378 -4.4285355 -4.4286294 -4.4287357 -4.4287949 -4.4288039 -4.4287691 -4.4287176][-4.4289885 -4.4289875 -4.4289746 -4.42894 -4.4288845 -4.4288025 -4.4286952 -4.428607 -4.4285903 -4.4286571 -4.4287386 -4.4287863 -4.42879 -4.4287562 -4.4287038][-4.42899 -4.4289889 -4.4289789 -4.4289522 -4.4289045 -4.4288354 -4.4287558 -4.4286919 -4.4286714 -4.4287095 -4.4287586 -4.4287848 -4.4287796 -4.4287477 -4.4286985][-4.4289885 -4.42899 -4.4289856 -4.4289646 -4.4289179 -4.4288568 -4.4287987 -4.4287591 -4.4287424 -4.4287591 -4.4287796 -4.4287777 -4.4287572 -4.4287262 -4.42869][-4.4289861 -4.4289913 -4.4289951 -4.4289794 -4.4289341 -4.4288783 -4.4288311 -4.4288006 -4.428781 -4.428782 -4.428782 -4.4287567 -4.4287214 -4.4286957 -4.4286828][-4.4289846 -4.4289932 -4.4290047 -4.4289947 -4.4289513 -4.4288988 -4.4288588 -4.428833 -4.4288111 -4.4288015 -4.428782 -4.4287362 -4.428689 -4.4286647 -4.4286613]]...]
INFO - root - 2017-12-08 07:24:37.340802: step 40210, loss = 2.28, batch loss = 2.23 (34.4 examples/sec; 0.232 sec/batch; 18h:51m:33s remains)
INFO - root - 2017-12-08 07:24:39.627421: step 40220, loss = 2.28, batch loss = 2.23 (34.5 examples/sec; 0.232 sec/batch; 18h:49m:01s remains)
INFO - root - 2017-12-08 07:24:41.875294: step 40230, loss = 2.28, batch loss = 2.23 (34.6 examples/sec; 0.231 sec/batch; 18h:47m:22s remains)
INFO - root - 2017-12-08 07:24:44.143253: step 40240, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 18h:06m:57s remains)
INFO - root - 2017-12-08 07:24:46.396197: step 40250, loss = 2.28, batch loss = 2.23 (36.6 examples/sec; 0.219 sec/batch; 17h:44m:46s remains)
INFO - root - 2017-12-08 07:24:48.626645: step 40260, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 17h:47m:32s remains)
INFO - root - 2017-12-08 07:24:50.854003: step 40270, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 18h:05m:09s remains)
INFO - root - 2017-12-08 07:24:53.073402: step 40280, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 17h:55m:31s remains)
INFO - root - 2017-12-08 07:24:55.298973: step 40290, loss = 2.28, batch loss = 2.23 (33.5 examples/sec; 0.239 sec/batch; 19h:22m:03s remains)
INFO - root - 2017-12-08 07:24:57.508352: step 40300, loss = 2.28, batch loss = 2.23 (37.1 examples/sec; 0.216 sec/batch; 17h:30m:55s remains)
2017-12-08 07:24:57.805480: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288754 -4.4288487 -4.4287977 -4.4287381 -4.4287162 -4.4287376 -4.42878 -4.4288311 -4.4288764 -4.4289088 -4.4289193 -4.4289131 -4.42891 -4.4289069 -4.4288979][-4.4289045 -4.4288568 -4.4287682 -4.4286709 -4.428638 -4.4286838 -4.4287567 -4.4288335 -4.4288983 -4.4289417 -4.4289565 -4.4289503 -4.428946 -4.4289408 -4.428926][-4.4289265 -4.428865 -4.4287429 -4.4286036 -4.4285488 -4.4286122 -4.4287086 -4.4288073 -4.4288926 -4.4289522 -4.4289775 -4.428977 -4.4289761 -4.4289756 -4.4289641][-4.4289427 -4.4288788 -4.4287372 -4.4285622 -4.4284763 -4.4285393 -4.4286466 -4.4287553 -4.4288554 -4.4289322 -4.4289703 -4.4289789 -4.4289842 -4.4289947 -4.4289956][-4.4289503 -4.4288926 -4.4287457 -4.4285488 -4.4284277 -4.4284668 -4.4285665 -4.4286695 -4.4287772 -4.4288759 -4.4289336 -4.428957 -4.4289708 -4.4289918 -4.4290066][-4.4289451 -4.4288964 -4.42876 -4.428556 -4.4283919 -4.4283843 -4.4284639 -4.4285579 -4.4286728 -4.4287949 -4.42888 -4.4289231 -4.4289484 -4.428978 -4.4290009][-4.4289441 -4.4289064 -4.4287891 -4.4285855 -4.4283662 -4.42828 -4.428329 -4.4284253 -4.4285569 -4.4287052 -4.4288197 -4.428885 -4.4289236 -4.4289594 -4.4289875][-4.4289513 -4.4289246 -4.4288297 -4.4286394 -4.4283814 -4.4282169 -4.4282231 -4.4283209 -4.428473 -4.4286413 -4.4287748 -4.4288568 -4.4289045 -4.4289432 -4.4289722][-4.4289646 -4.4289513 -4.4288816 -4.4287276 -4.4284859 -4.4282851 -4.4282308 -4.4282994 -4.4284453 -4.4286146 -4.4287515 -4.4288378 -4.4288926 -4.4289317 -4.4289603][-4.4289789 -4.4289813 -4.4289422 -4.4288416 -4.42865 -4.4284434 -4.4283252 -4.4283352 -4.4284506 -4.4286036 -4.4287291 -4.4288111 -4.42887 -4.4289141 -4.4289474][-4.4289894 -4.4290047 -4.4289904 -4.4289374 -4.4288006 -4.42861 -4.4284449 -4.4283915 -4.428462 -4.4285936 -4.4287038 -4.4287815 -4.4288454 -4.4288979 -4.4289393][-4.4289961 -4.4290147 -4.4290147 -4.4289923 -4.4289064 -4.4287496 -4.42857 -4.4284596 -4.4284763 -4.4285803 -4.4286785 -4.4287562 -4.4288263 -4.4288874 -4.4289355][-4.4289975 -4.4290085 -4.4290123 -4.429008 -4.4289589 -4.4288421 -4.4286728 -4.4285183 -4.428472 -4.4285407 -4.4286327 -4.4287167 -4.428803 -4.4288769 -4.4289289][-4.4289904 -4.4289885 -4.428988 -4.4289913 -4.4289675 -4.42889 -4.4287515 -4.4285865 -4.4284921 -4.4285207 -4.4286008 -4.4286919 -4.428793 -4.428874 -4.4289217][-4.4289713 -4.428957 -4.4289489 -4.428957 -4.428956 -4.4289193 -4.4288306 -4.4286985 -4.4285917 -4.4285784 -4.4286323 -4.4287157 -4.428813 -4.428884 -4.4289155]]...]
INFO - root - 2017-12-08 07:25:00.046005: step 40310, loss = 2.28, batch loss = 2.23 (35.2 examples/sec; 0.227 sec/batch; 18h:26m:58s remains)
INFO - root - 2017-12-08 07:25:02.268963: step 40320, loss = 2.28, batch loss = 2.23 (35.4 examples/sec; 0.226 sec/batch; 18h:19m:29s remains)
INFO - root - 2017-12-08 07:25:04.493448: step 40330, loss = 2.28, batch loss = 2.23 (35.7 examples/sec; 0.224 sec/batch; 18h:09m:41s remains)
INFO - root - 2017-12-08 07:25:06.722700: step 40340, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.226 sec/batch; 18h:18m:07s remains)
INFO - root - 2017-12-08 07:25:08.990785: step 40350, loss = 2.28, batch loss = 2.23 (35.3 examples/sec; 0.227 sec/batch; 18h:24m:07s remains)
INFO - root - 2017-12-08 07:25:11.213580: step 40360, loss = 2.28, batch loss = 2.23 (36.8 examples/sec; 0.217 sec/batch; 17h:38m:17s remains)
INFO - root - 2017-12-08 07:25:13.456791: step 40370, loss = 2.28, batch loss = 2.23 (34.8 examples/sec; 0.230 sec/batch; 18h:38m:34s remains)
INFO - root - 2017-12-08 07:25:15.687023: step 40380, loss = 2.28, batch loss = 2.23 (35.8 examples/sec; 0.223 sec/batch; 18h:07m:38s remains)
INFO - root - 2017-12-08 07:25:17.933289: step 40390, loss = 2.28, batch loss = 2.23 (37.2 examples/sec; 0.215 sec/batch; 17h:27m:11s remains)
INFO - root - 2017-12-08 07:25:20.217173: step 40400, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.225 sec/batch; 18h:16m:46s remains)
2017-12-08 07:25:20.529167: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289432 -4.4289665 -4.4289894 -4.4289942 -4.4289894 -4.4289861 -4.4289842 -4.42898 -4.4289765 -4.4289775 -4.4289813 -4.4289885 -4.4289927 -4.4289932 -4.4289842][-4.4289188 -4.4289317 -4.428946 -4.4289479 -4.4289465 -4.4289513 -4.4289508 -4.4289436 -4.4289365 -4.4289379 -4.4289474 -4.4289613 -4.4289684 -4.428966 -4.4289503][-4.4288864 -4.4288645 -4.4288535 -4.428853 -4.4288692 -4.4288964 -4.428906 -4.4289093 -4.42891 -4.4289122 -4.4289231 -4.4289389 -4.4289451 -4.4289365 -4.4289131][-4.428853 -4.4287915 -4.4287415 -4.4287295 -4.4287639 -4.4288144 -4.4288383 -4.428854 -4.4288712 -4.4288845 -4.4289045 -4.4289236 -4.4289284 -4.4289174 -4.4288945][-4.4288282 -4.4287486 -4.428679 -4.4286556 -4.4286857 -4.4287338 -4.4287548 -4.428772 -4.4288096 -4.4288535 -4.4288936 -4.4289165 -4.4289222 -4.4289141 -4.4289017][-4.4288163 -4.4287438 -4.4286838 -4.4286609 -4.4286618 -4.42866 -4.428627 -4.4286132 -4.4286666 -4.4287629 -4.428844 -4.4288845 -4.4289021 -4.4289074 -4.4289107][-4.4287939 -4.4287505 -4.4287238 -4.4287033 -4.42866 -4.4285793 -4.4284554 -4.4283547 -4.4284024 -4.4285746 -4.42872 -4.4287934 -4.4288282 -4.4288487 -4.4288645][-4.4287682 -4.4287686 -4.4287767 -4.4287624 -4.4286876 -4.4285431 -4.4283304 -4.428124 -4.4281492 -4.4283814 -4.4285712 -4.4286594 -4.4286952 -4.4287148 -4.4287333][-4.4287429 -4.428791 -4.4288287 -4.4288297 -4.4287629 -4.4286208 -4.4284172 -4.4282146 -4.4282136 -4.4283929 -4.4285421 -4.4285913 -4.4285879 -4.4285812 -4.4285913][-4.4287319 -4.428792 -4.428844 -4.4288707 -4.4288445 -4.4287543 -4.4286289 -4.4285059 -4.42849 -4.428565 -4.4286237 -4.4286165 -4.4285722 -4.4285316 -4.428524][-4.4287004 -4.4287462 -4.428803 -4.4288578 -4.428874 -4.4288321 -4.4287643 -4.4286995 -4.4286852 -4.4286976 -4.4286957 -4.4286594 -4.4286051 -4.428545 -4.4285126][-4.4286695 -4.4286861 -4.4287291 -4.428792 -4.4288316 -4.4288216 -4.428793 -4.4287696 -4.4287586 -4.4287443 -4.4287238 -4.428699 -4.4286561 -4.4286003 -4.4285603][-4.4286933 -4.4286776 -4.4286895 -4.4287305 -4.4287734 -4.4287863 -4.4287944 -4.4288058 -4.4287963 -4.4287667 -4.4287429 -4.4287271 -4.4287024 -4.4286766 -4.4286571][-4.4287748 -4.4287496 -4.4287481 -4.4287653 -4.4287887 -4.4288006 -4.428823 -4.4288487 -4.4288368 -4.4288034 -4.4287815 -4.4287753 -4.4287667 -4.4287691 -4.4287734][-4.4288645 -4.4288535 -4.4288583 -4.4288654 -4.4288716 -4.4288678 -4.4288793 -4.4288921 -4.4288816 -4.428853 -4.4288368 -4.4288359 -4.4288383 -4.4288516 -4.42886]]...]
INFO - root - 2017-12-08 07:25:22.789350: step 40410, loss = 2.28, batch loss = 2.23 (36.0 examples/sec; 0.222 sec/batch; 18h:01m:50s remains)
INFO - root - 2017-12-08 07:25:25.015530: step 40420, loss = 2.28, batch loss = 2.23 (37.3 examples/sec; 0.214 sec/batch; 17h:23m:27s remains)
INFO - root - 2017-12-08 07:25:27.244006: step 40430, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 17h:49m:40s remains)
INFO - root - 2017-12-08 07:25:29.522969: step 40440, loss = 2.28, batch loss = 2.23 (34.9 examples/sec; 0.229 sec/batch; 18h:35m:42s remains)
INFO - root - 2017-12-08 07:25:31.774989: step 40450, loss = 2.28, batch loss = 2.23 (35.2 examples/sec; 0.227 sec/batch; 18h:27m:18s remains)
INFO - root - 2017-12-08 07:25:34.008175: step 40460, loss = 2.28, batch loss = 2.23 (34.5 examples/sec; 0.232 sec/batch; 18h:49m:24s remains)
INFO - root - 2017-12-08 07:25:36.256037: step 40470, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 17h:45m:31s remains)
INFO - root - 2017-12-08 07:25:38.552005: step 40480, loss = 2.28, batch loss = 2.23 (33.2 examples/sec; 0.241 sec/batch; 19h:33m:39s remains)
INFO - root - 2017-12-08 07:25:40.781755: step 40490, loss = 2.28, batch loss = 2.23 (35.1 examples/sec; 0.228 sec/batch; 18h:28m:55s remains)
INFO - root - 2017-12-08 07:25:43.001105: step 40500, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 17h:49m:09s remains)
2017-12-08 07:25:43.298437: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4282808 -4.4282126 -4.4282355 -4.4283319 -4.4284577 -4.4285836 -4.4286966 -4.4287839 -4.4288483 -4.4288826 -4.428884 -4.4288568 -4.4288249 -4.4288135 -4.4288096][-4.4284282 -4.4283872 -4.4284067 -4.4284759 -4.4285769 -4.4286928 -4.4287858 -4.4288416 -4.42888 -4.4288931 -4.4288864 -4.4288697 -4.4288516 -4.4288526 -4.4288588][-4.4285884 -4.4285645 -4.4285922 -4.428647 -4.428719 -4.4287992 -4.4288521 -4.4288721 -4.4288836 -4.4288807 -4.42887 -4.4288678 -4.4288731 -4.4288917 -4.4289041][-4.4287453 -4.4287252 -4.4287419 -4.428781 -4.4288278 -4.4288673 -4.4288788 -4.42887 -4.4288592 -4.4288445 -4.4288387 -4.4288526 -4.4288793 -4.4289088 -4.4289246][-4.4288797 -4.428863 -4.4288721 -4.4288912 -4.4289 -4.4288864 -4.4288507 -4.4288082 -4.4287677 -4.4287462 -4.4287591 -4.4288025 -4.4288521 -4.4288926 -4.4289141][-4.42895 -4.4289479 -4.4289608 -4.4289603 -4.428925 -4.4288459 -4.4287434 -4.4286504 -4.4285803 -4.4285736 -4.4286218 -4.4287 -4.4287753 -4.428833 -4.4288673][-4.4289422 -4.428946 -4.4289546 -4.4289341 -4.4288588 -4.4287171 -4.4285493 -4.4284048 -4.4283218 -4.42836 -4.4284663 -4.4285808 -4.4286809 -4.4287639 -4.4288187][-4.42894 -4.42893 -4.4289088 -4.4288492 -4.4287252 -4.4285326 -4.428308 -4.4281249 -4.4280586 -4.4281564 -4.4283051 -4.4284387 -4.428565 -4.4286771 -4.4287581][-4.428947 -4.4289212 -4.4288621 -4.4287581 -4.4286022 -4.4283943 -4.4281654 -4.4279947 -4.427979 -4.4281044 -4.4282527 -4.4283853 -4.4285188 -4.428637 -4.4287238][-4.4289341 -4.428905 -4.4288335 -4.4287291 -4.428597 -4.4284387 -4.4282789 -4.4281855 -4.4282041 -4.4282928 -4.4283953 -4.4285021 -4.4286146 -4.4287086 -4.4287748][-4.4289203 -4.4289 -4.4288411 -4.4287734 -4.4287033 -4.4286275 -4.4285574 -4.4285183 -4.4285269 -4.4285655 -4.42862 -4.4286919 -4.4287672 -4.4288182 -4.4288483][-4.4289088 -4.4288974 -4.4288607 -4.4288316 -4.4288154 -4.4288006 -4.4287882 -4.4287739 -4.428772 -4.42878 -4.4288034 -4.428844 -4.4288864 -4.4289069 -4.4289126][-4.428905 -4.4288993 -4.4288807 -4.428875 -4.4288836 -4.428896 -4.4289021 -4.4288945 -4.428884 -4.4288816 -4.4288926 -4.4289165 -4.428937 -4.4289427 -4.4289408][-4.4289112 -4.4289093 -4.4289017 -4.4289045 -4.4289174 -4.4289317 -4.4289384 -4.4289312 -4.4289188 -4.4289136 -4.4289188 -4.4289327 -4.4289436 -4.4289441 -4.4289384][-4.4289217 -4.4289231 -4.4289188 -4.4289207 -4.4289265 -4.4289336 -4.4289384 -4.4289365 -4.4289303 -4.4289255 -4.428925 -4.42893 -4.4289346 -4.4289346 -4.4289312]]...]
INFO - root - 2017-12-08 07:25:45.519185: step 40510, loss = 2.28, batch loss = 2.23 (35.8 examples/sec; 0.224 sec/batch; 18h:08m:27s remains)
INFO - root - 2017-12-08 07:25:47.805581: step 40520, loss = 2.28, batch loss = 2.23 (34.7 examples/sec; 0.230 sec/batch; 18h:40m:54s remains)
INFO - root - 2017-12-08 07:25:50.050310: step 40530, loss = 2.28, batch loss = 2.23 (36.6 examples/sec; 0.219 sec/batch; 17h:43m:46s remains)
INFO - root - 2017-12-08 07:25:52.283913: step 40540, loss = 2.28, batch loss = 2.23 (36.8 examples/sec; 0.217 sec/batch; 17h:37m:48s remains)
INFO - root - 2017-12-08 07:25:54.514810: step 40550, loss = 2.28, batch loss = 2.23 (34.6 examples/sec; 0.231 sec/batch; 18h:43m:45s remains)
INFO - root - 2017-12-08 07:25:56.742057: step 40560, loss = 2.28, batch loss = 2.23 (34.5 examples/sec; 0.232 sec/batch; 18h:48m:51s remains)
INFO - root - 2017-12-08 07:25:59.045046: step 40570, loss = 2.28, batch loss = 2.23 (37.0 examples/sec; 0.216 sec/batch; 17h:31m:53s remains)
INFO - root - 2017-12-08 07:26:01.293178: step 40580, loss = 2.28, batch loss = 2.23 (35.3 examples/sec; 0.226 sec/batch; 18h:21m:26s remains)
INFO - root - 2017-12-08 07:26:03.541954: step 40590, loss = 2.28, batch loss = 2.23 (36.0 examples/sec; 0.222 sec/batch; 18h:01m:21s remains)
INFO - root - 2017-12-08 07:26:05.769895: step 40600, loss = 2.28, batch loss = 2.23 (35.8 examples/sec; 0.224 sec/batch; 18h:07m:30s remains)
2017-12-08 07:26:06.080418: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4287982 -4.4287577 -4.428719 -4.4286838 -4.4286389 -4.4286008 -4.4285541 -4.4285274 -4.4285603 -4.42861 -4.4286528 -4.4287152 -4.4287739 -4.4288 -4.4288187][-4.4288068 -4.42877 -4.4287457 -4.4287329 -4.4287095 -4.428688 -4.42864 -4.4286003 -4.4286094 -4.42863 -4.4286509 -4.4287109 -4.428772 -4.4288044 -4.4288206][-4.4287953 -4.428771 -4.4287648 -4.4287758 -4.4287791 -4.428771 -4.4287062 -4.4286427 -4.4286361 -4.42864 -4.4286532 -4.4287176 -4.4287887 -4.4288244 -4.4288344][-4.4287872 -4.4287744 -4.4287896 -4.4288235 -4.4288492 -4.4288445 -4.4287577 -4.4286561 -4.4286232 -4.4286232 -4.4286413 -4.4287128 -4.4287891 -4.4288282 -4.4288397][-4.4287744 -4.4287786 -4.4288068 -4.428844 -4.4288621 -4.4288392 -4.428721 -4.4285836 -4.4285407 -4.4285688 -4.4286203 -4.4287009 -4.4287772 -4.4288177 -4.4288321][-4.42876 -4.4287744 -4.428802 -4.4288168 -4.4287839 -4.428709 -4.4285388 -4.4283714 -4.4283552 -4.4284558 -4.4285755 -4.4286814 -4.4287682 -4.4288139 -4.4288311][-4.4287577 -4.4287782 -4.4287972 -4.42878 -4.4286876 -4.4285417 -4.4282985 -4.4280882 -4.4280982 -4.4282951 -4.4284954 -4.4286556 -4.4287696 -4.4288263 -4.4288464][-4.428772 -4.4287963 -4.4288049 -4.4287586 -4.428618 -4.4284234 -4.4281545 -4.4279432 -4.4279828 -4.4282293 -4.4284606 -4.4286504 -4.4287772 -4.4288354 -4.4288554][-4.4288235 -4.4288321 -4.4288239 -4.4287529 -4.4286041 -4.4284248 -4.4282246 -4.428102 -4.4281778 -4.42837 -4.4285288 -4.4286804 -4.42879 -4.4288378 -4.428854][-4.4288778 -4.4288735 -4.4288454 -4.4287667 -4.4286337 -4.4285011 -4.4283824 -4.4283328 -4.4284096 -4.4285369 -4.4286294 -4.4287271 -4.428812 -4.4288464 -4.4288554][-4.4289045 -4.4288898 -4.4288526 -4.4287839 -4.4286785 -4.4285879 -4.4285188 -4.4284978 -4.4285583 -4.4286461 -4.4287071 -4.4287739 -4.4288383 -4.4288621 -4.428863][-4.4288416 -4.4288211 -4.4287939 -4.4287562 -4.4286966 -4.4286556 -4.4286346 -4.4286332 -4.4286795 -4.4287372 -4.4287744 -4.4288182 -4.4288616 -4.4288721 -4.4288697][-4.428709 -4.4286904 -4.42868 -4.4286785 -4.4286757 -4.4286957 -4.4287162 -4.4287267 -4.4287634 -4.4288025 -4.4288239 -4.4288497 -4.4288731 -4.4288726 -4.4288697][-4.4286509 -4.42864 -4.42864 -4.4286532 -4.4286795 -4.4287267 -4.428762 -4.4287796 -4.4288154 -4.4288483 -4.4288616 -4.4288783 -4.42889 -4.4288845 -4.4288793][-4.4287262 -4.4287281 -4.4287324 -4.4287438 -4.4287643 -4.4287968 -4.428813 -4.4288139 -4.4288368 -4.4288645 -4.4288821 -4.4289026 -4.4289145 -4.42891 -4.4289012]]...]
INFO - root - 2017-12-08 07:26:08.286451: step 40610, loss = 2.28, batch loss = 2.23 (35.3 examples/sec; 0.227 sec/batch; 18h:23m:13s remains)
INFO - root - 2017-12-08 07:26:10.534132: step 40620, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 18h:03m:01s remains)
INFO - root - 2017-12-08 07:26:12.784437: step 40630, loss = 2.28, batch loss = 2.23 (34.5 examples/sec; 0.232 sec/batch; 18h:47m:31s remains)
INFO - root - 2017-12-08 07:26:14.991421: step 40640, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 17h:53m:48s remains)
INFO - root - 2017-12-08 07:26:17.218366: step 40650, loss = 2.28, batch loss = 2.23 (35.3 examples/sec; 0.227 sec/batch; 18h:21m:59s remains)
INFO - root - 2017-12-08 07:26:19.450054: step 40660, loss = 2.28, batch loss = 2.23 (33.7 examples/sec; 0.237 sec/batch; 19h:13m:43s remains)
INFO - root - 2017-12-08 07:26:21.705450: step 40670, loss = 2.28, batch loss = 2.23 (35.0 examples/sec; 0.228 sec/batch; 18h:30m:55s remains)
INFO - root - 2017-12-08 07:26:23.988215: step 40680, loss = 2.28, batch loss = 2.23 (34.6 examples/sec; 0.231 sec/batch; 18h:43m:07s remains)
INFO - root - 2017-12-08 07:26:26.257666: step 40690, loss = 2.28, batch loss = 2.23 (34.2 examples/sec; 0.234 sec/batch; 18h:58m:44s remains)
INFO - root - 2017-12-08 07:26:28.488293: step 40700, loss = 2.28, batch loss = 2.23 (36.7 examples/sec; 0.218 sec/batch; 17h:39m:48s remains)
2017-12-08 07:26:28.773577: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4286904 -4.4287133 -4.4287653 -4.4288144 -4.4288239 -4.4287953 -4.42873 -4.4286604 -4.4286356 -4.4286771 -4.4287581 -4.4288225 -4.4288669 -4.4288855 -4.4288926][-4.4286985 -4.4287233 -4.4287672 -4.4288034 -4.4288068 -4.428772 -4.4287081 -4.428647 -4.4286342 -4.4286876 -4.428781 -4.4288497 -4.42889 -4.428905 -4.428906][-4.4286923 -4.4287152 -4.42875 -4.4287739 -4.4287658 -4.4287252 -4.4286656 -4.4286122 -4.4286084 -4.4286733 -4.4287729 -4.428843 -4.4288831 -4.4289017 -4.4289079][-4.4286966 -4.4287114 -4.4287295 -4.4287348 -4.42871 -4.4286618 -4.4286036 -4.4285541 -4.4285574 -4.4286304 -4.4287338 -4.4288096 -4.4288554 -4.428884 -4.4289007][-4.4287324 -4.4287286 -4.4287181 -4.4286952 -4.42865 -4.42859 -4.4285254 -4.4284735 -4.42849 -4.42858 -4.4286914 -4.428781 -4.4288392 -4.4288759 -4.4288969][-4.4287739 -4.4287529 -4.4287162 -4.4286685 -4.4286046 -4.4285297 -4.4284544 -4.4284029 -4.428442 -4.4285555 -4.4286785 -4.4287791 -4.4288445 -4.4288836 -4.4289012][-4.4288006 -4.428772 -4.4287162 -4.4286423 -4.4285569 -4.4284668 -4.4283829 -4.4283414 -4.4284129 -4.4285541 -4.4286871 -4.4287915 -4.4288573 -4.4288955 -4.4289074][-4.4287987 -4.4287696 -4.4287128 -4.4286308 -4.4285345 -4.4284296 -4.4283328 -4.4283009 -4.4284015 -4.428566 -4.4287095 -4.4288149 -4.4288788 -4.4289122 -4.4289179][-4.4287777 -4.42875 -4.4287052 -4.4286389 -4.4285536 -4.428453 -4.4283533 -4.4283233 -4.4284253 -4.4285908 -4.4287348 -4.4288392 -4.4288983 -4.4289246 -4.428926][-4.4287424 -4.4287157 -4.4286852 -4.4286427 -4.4285836 -4.4285054 -4.4284153 -4.4283824 -4.4284644 -4.4286103 -4.4287443 -4.4288459 -4.4289031 -4.428926 -4.42893][-4.4287028 -4.428678 -4.428659 -4.4286375 -4.4286022 -4.4285455 -4.4284735 -4.4284434 -4.4285069 -4.4286284 -4.4287453 -4.4288397 -4.4288964 -4.4289207 -4.4289312][-4.4286695 -4.4286442 -4.4286313 -4.4286232 -4.4286051 -4.42857 -4.4285164 -4.4284916 -4.4285417 -4.4286385 -4.4287353 -4.4288211 -4.4288793 -4.4289093 -4.4289289][-4.4286437 -4.4286184 -4.4286084 -4.428606 -4.4286003 -4.4285822 -4.4285421 -4.4285216 -4.4285612 -4.4286408 -4.4287243 -4.4288044 -4.4288635 -4.4288983 -4.4289246][-4.4286284 -4.42861 -4.4286036 -4.4286017 -4.4285984 -4.4285846 -4.4285502 -4.4285316 -4.4285665 -4.4286404 -4.4287205 -4.4287963 -4.4288564 -4.4288964 -4.4289255][-4.4286494 -4.4286361 -4.4286332 -4.4286294 -4.4286218 -4.4286032 -4.4285674 -4.4285483 -4.4285779 -4.4286494 -4.42873 -4.4288039 -4.428865 -4.4289069 -4.4289331]]...]
INFO - root - 2017-12-08 07:26:31.006910: step 40710, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.226 sec/batch; 18h:17m:13s remains)
INFO - root - 2017-12-08 07:26:33.235335: step 40720, loss = 2.28, batch loss = 2.23 (36.8 examples/sec; 0.217 sec/batch; 17h:37m:03s remains)
INFO - root - 2017-12-08 07:26:35.455610: step 40730, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.220 sec/batch; 17h:51m:44s remains)
INFO - root - 2017-12-08 07:26:37.730678: step 40740, loss = 2.28, batch loss = 2.23 (31.8 examples/sec; 0.252 sec/batch; 20h:24m:00s remains)
INFO - root - 2017-12-08 07:26:39.951911: step 40750, loss = 2.28, batch loss = 2.23 (36.6 examples/sec; 0.218 sec/batch; 17h:41m:34s remains)
INFO - root - 2017-12-08 07:26:42.190671: step 40760, loss = 2.28, batch loss = 2.23 (35.8 examples/sec; 0.224 sec/batch; 18h:07m:25s remains)
INFO - root - 2017-12-08 07:26:44.424837: step 40770, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 17h:47m:38s remains)
INFO - root - 2017-12-08 07:26:46.647511: step 40780, loss = 2.28, batch loss = 2.23 (35.1 examples/sec; 0.228 sec/batch; 18h:26m:55s remains)
INFO - root - 2017-12-08 07:26:48.869615: step 40790, loss = 2.28, batch loss = 2.23 (35.4 examples/sec; 0.226 sec/batch; 18h:19m:54s remains)
INFO - root - 2017-12-08 07:26:51.106037: step 40800, loss = 2.28, batch loss = 2.23 (31.3 examples/sec; 0.256 sec/batch; 20h:43m:40s remains)
2017-12-08 07:26:51.391107: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289317 -4.4289141 -4.428895 -4.4288864 -4.4288807 -4.4288788 -4.4288831 -4.4288893 -4.4289002 -4.4289131 -4.4289169 -4.4289079 -4.428896 -4.4288945 -4.4289021][-4.4289451 -4.4289217 -4.4289021 -4.4288955 -4.4288931 -4.4288936 -4.4288969 -4.4289041 -4.4289188 -4.4289408 -4.4289546 -4.4289541 -4.428947 -4.4289427 -4.4289432][-4.4289236 -4.4288874 -4.4288626 -4.4288535 -4.428853 -4.4288616 -4.4288778 -4.428894 -4.4289117 -4.4289389 -4.4289627 -4.4289761 -4.4289804 -4.4289808 -4.4289756][-4.4288826 -4.428833 -4.4287982 -4.4287882 -4.4287858 -4.4288006 -4.42883 -4.4288626 -4.4288907 -4.4289236 -4.4289517 -4.4289708 -4.4289827 -4.42899 -4.4289827][-4.42884 -4.4287734 -4.4287233 -4.4287105 -4.4287109 -4.4287281 -4.42876 -4.4288034 -4.4288473 -4.4288917 -4.4289212 -4.4289408 -4.4289541 -4.4289627 -4.428957][-4.4287872 -4.4286995 -4.4286313 -4.4286127 -4.4286137 -4.4286218 -4.428637 -4.4286761 -4.4287324 -4.428793 -4.4288387 -4.4288731 -4.4288969 -4.4289165 -4.4289193][-4.4287486 -4.4286413 -4.4285522 -4.4285164 -4.4285049 -4.4284878 -4.4284749 -4.4285026 -4.4285631 -4.4286394 -4.428709 -4.4287686 -4.4288182 -4.4288592 -4.4288774][-4.4287472 -4.4286375 -4.4285388 -4.4284873 -4.4284496 -4.4283857 -4.4283252 -4.4283314 -4.4283938 -4.4284849 -4.4285836 -4.4286709 -4.4287457 -4.42881 -4.4288478][-4.4287853 -4.4286871 -4.4286036 -4.4285631 -4.4285207 -4.4284205 -4.428309 -4.4282708 -4.4283028 -4.4283862 -4.428503 -4.4286108 -4.4287047 -4.4287848 -4.4288344][-4.4288459 -4.4287724 -4.428721 -4.4287043 -4.4286752 -4.4285822 -4.4284592 -4.4283857 -4.4283733 -4.428421 -4.4285212 -4.4286275 -4.4287248 -4.4288077 -4.4288573][-4.428905 -4.4288635 -4.4288483 -4.4288483 -4.4288259 -4.4287543 -4.4286575 -4.4285889 -4.4285607 -4.4285769 -4.4286413 -4.42872 -4.4287958 -4.4288659 -4.4289055][-4.4289503 -4.4289346 -4.4289384 -4.4289432 -4.4289222 -4.4288726 -4.4288096 -4.4287682 -4.4287505 -4.4287572 -4.4287925 -4.4288392 -4.4288859 -4.4289346 -4.4289579][-4.4289818 -4.428978 -4.4289832 -4.4289823 -4.4289608 -4.4289241 -4.428885 -4.4288664 -4.4288678 -4.4288821 -4.4289074 -4.4289365 -4.4289622 -4.428988 -4.4289966][-4.4290061 -4.42901 -4.4290104 -4.4290018 -4.4289794 -4.4289503 -4.4289222 -4.4289107 -4.4289207 -4.4289417 -4.428966 -4.42899 -4.4290042 -4.4290128 -4.4290118][-4.4290204 -4.4290266 -4.4290252 -4.4290161 -4.4289961 -4.4289732 -4.4289508 -4.4289393 -4.428946 -4.428968 -4.4289913 -4.4290123 -4.4290223 -4.4290214 -4.4290118]]...]
INFO - root - 2017-12-08 07:26:53.627484: step 40810, loss = 2.28, batch loss = 2.23 (36.6 examples/sec; 0.218 sec/batch; 17h:42m:01s remains)
INFO - root - 2017-12-08 07:26:55.882880: step 40820, loss = 2.28, batch loss = 2.23 (37.4 examples/sec; 0.214 sec/batch; 17h:20m:49s remains)
INFO - root - 2017-12-08 07:26:58.135934: step 40830, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 18h:02m:11s remains)
INFO - root - 2017-12-08 07:27:00.359854: step 40840, loss = 2.28, batch loss = 2.23 (35.1 examples/sec; 0.228 sec/batch; 18h:29m:27s remains)
INFO - root - 2017-12-08 07:27:02.608132: step 40850, loss = 2.28, batch loss = 2.23 (34.7 examples/sec; 0.231 sec/batch; 18h:41m:37s remains)
INFO - root - 2017-12-08 07:27:04.847456: step 40860, loss = 2.28, batch loss = 2.23 (35.3 examples/sec; 0.226 sec/batch; 18h:20m:32s remains)
INFO - root - 2017-12-08 07:27:07.075213: step 40870, loss = 2.28, batch loss = 2.23 (36.8 examples/sec; 0.218 sec/batch; 17h:37m:11s remains)
INFO - root - 2017-12-08 07:27:09.309750: step 40880, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 17h:48m:20s remains)
INFO - root - 2017-12-08 07:27:11.538766: step 40890, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.222 sec/batch; 17h:56m:58s remains)
INFO - root - 2017-12-08 07:27:13.781726: step 40900, loss = 2.28, batch loss = 2.23 (35.2 examples/sec; 0.227 sec/batch; 18h:24m:01s remains)
2017-12-08 07:27:14.070758: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288821 -4.4288788 -4.4288568 -4.4288406 -4.4288378 -4.4288321 -4.4288139 -4.4287944 -4.4287686 -4.42874 -4.4287186 -4.4287105 -4.4287267 -4.4287825 -4.4288435][-4.4289074 -4.4288969 -4.4288626 -4.4288449 -4.4288449 -4.4288416 -4.4288244 -4.4288082 -4.428792 -4.4287715 -4.4287491 -4.4287329 -4.4287324 -4.4287767 -4.4288335][-4.4289041 -4.4288864 -4.4288468 -4.4288297 -4.4288282 -4.4288278 -4.428813 -4.4287958 -4.4287868 -4.4287853 -4.4287796 -4.4287696 -4.4287548 -4.428782 -4.4288292][-4.4288721 -4.4288545 -4.4288082 -4.4287825 -4.4287705 -4.4287634 -4.4287539 -4.4287457 -4.4287529 -4.4287729 -4.4287891 -4.4287949 -4.4287815 -4.4287977 -4.4288363][-4.4288268 -4.4288049 -4.4287562 -4.428709 -4.4286733 -4.4286475 -4.4286389 -4.4286575 -4.4287033 -4.4287453 -4.4287724 -4.428791 -4.4287896 -4.4288049 -4.4288425][-4.4287925 -4.42876 -4.4287057 -4.4286385 -4.4285693 -4.4285035 -4.4284744 -4.4285264 -4.4286351 -4.4287109 -4.4287438 -4.4287648 -4.42877 -4.4287906 -4.4288349][-4.4287815 -4.4287438 -4.4286928 -4.4286094 -4.4284921 -4.4283514 -4.4282541 -4.42832 -4.4285083 -4.4286489 -4.4287081 -4.4287395 -4.4287515 -4.4287753 -4.4288235][-4.4287777 -4.4287548 -4.4287157 -4.4286237 -4.4284806 -4.4282675 -4.428061 -4.4280949 -4.4283452 -4.4285493 -4.4286475 -4.428699 -4.4287281 -4.4287624 -4.4288177][-4.4287667 -4.4287548 -4.4287353 -4.4286757 -4.4285693 -4.4283843 -4.4281778 -4.4281473 -4.4283347 -4.4285245 -4.4286261 -4.4286819 -4.4287181 -4.4287591 -4.4288239][-4.4287252 -4.4287181 -4.4287233 -4.4287052 -4.4286542 -4.42855 -4.42842 -4.4283719 -4.428462 -4.4285755 -4.4286432 -4.4286876 -4.42872 -4.4287658 -4.4288359][-4.4286842 -4.4286761 -4.4286942 -4.4287076 -4.4287105 -4.4286723 -4.4286027 -4.4285603 -4.4285922 -4.4286442 -4.4286728 -4.4287024 -4.4287329 -4.4287782 -4.4288445][-4.4287186 -4.4287038 -4.4287133 -4.4287305 -4.4287529 -4.4287515 -4.4287148 -4.4286785 -4.428678 -4.4286938 -4.4287009 -4.428719 -4.4287467 -4.428792 -4.4288554][-4.4287963 -4.42878 -4.4287825 -4.4287887 -4.4288034 -4.4288049 -4.4287796 -4.428741 -4.4287171 -4.4287143 -4.4287138 -4.4287319 -4.4287615 -4.42881 -4.4288735][-4.4288568 -4.4288411 -4.428843 -4.4288383 -4.4288387 -4.4288368 -4.4288144 -4.4287682 -4.4287329 -4.428719 -4.4287248 -4.4287496 -4.428782 -4.4288316 -4.4288898][-4.4288597 -4.4288459 -4.4288621 -4.4288607 -4.428853 -4.4288449 -4.42882 -4.428771 -4.4287329 -4.4287229 -4.4287343 -4.4287677 -4.4288077 -4.4288573 -4.428906]]...]
INFO - root - 2017-12-08 07:27:16.330395: step 40910, loss = 2.28, batch loss = 2.23 (35.1 examples/sec; 0.228 sec/batch; 18h:26m:38s remains)
INFO - root - 2017-12-08 07:27:18.570873: step 40920, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.221 sec/batch; 17h:52m:25s remains)
INFO - root - 2017-12-08 07:27:20.835849: step 40930, loss = 2.28, batch loss = 2.23 (32.4 examples/sec; 0.247 sec/batch; 20h:01m:23s remains)
INFO - root - 2017-12-08 07:27:23.090022: step 40940, loss = 2.28, batch loss = 2.23 (35.3 examples/sec; 0.226 sec/batch; 18h:20m:03s remains)
INFO - root - 2017-12-08 07:27:25.326640: step 40950, loss = 2.28, batch loss = 2.23 (36.7 examples/sec; 0.218 sec/batch; 17h:38m:56s remains)
INFO - root - 2017-12-08 07:27:27.562527: step 40960, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 18h:02m:34s remains)
INFO - root - 2017-12-08 07:27:29.804187: step 40970, loss = 2.28, batch loss = 2.23 (37.1 examples/sec; 0.216 sec/batch; 17h:28m:10s remains)
INFO - root - 2017-12-08 07:27:32.057521: step 40980, loss = 2.28, batch loss = 2.23 (35.0 examples/sec; 0.229 sec/batch; 18h:31m:43s remains)
INFO - root - 2017-12-08 07:27:34.319562: step 40990, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 17h:46m:44s remains)
INFO - root - 2017-12-08 07:27:36.552727: step 41000, loss = 2.28, batch loss = 2.23 (37.6 examples/sec; 0.213 sec/batch; 17h:13m:06s remains)
2017-12-08 07:27:36.827744: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4286914 -4.428679 -4.4287019 -4.42876 -4.4287853 -4.4287648 -4.428699 -4.4285808 -4.4284787 -4.428494 -4.4286323 -4.4287663 -4.4288421 -4.4288855 -4.4289308][-4.4287076 -4.4286857 -4.4286871 -4.4287219 -4.4287252 -4.4286976 -4.4286408 -4.4285436 -4.4284663 -4.4284997 -4.4286413 -4.4287767 -4.4288521 -4.4288898 -4.4289293][-4.4287415 -4.4287224 -4.4287119 -4.4287148 -4.4286814 -4.4286385 -4.4285817 -4.4284992 -4.4284458 -4.4284897 -4.42863 -4.428771 -4.4288568 -4.4288983 -4.4289346][-4.4288321 -4.4288225 -4.4287977 -4.4287596 -4.428679 -4.4286022 -4.4285126 -4.4284191 -4.4283905 -4.4284697 -4.4286222 -4.4287677 -4.42886 -4.4289079 -4.4289412][-4.4289293 -4.4289346 -4.4289031 -4.4288249 -4.4287033 -4.4285831 -4.4284544 -4.4283438 -4.4283361 -4.4284573 -4.4286189 -4.42876 -4.4288597 -4.4289126 -4.4289446][-4.4289708 -4.4289861 -4.4289517 -4.4288521 -4.428709 -4.4285684 -4.4284177 -4.4282951 -4.4282937 -4.428443 -4.4286094 -4.4287467 -4.4288554 -4.4289155 -4.4289474][-4.4289742 -4.4289861 -4.4289441 -4.4288359 -4.4286885 -4.4285421 -4.4283876 -4.4282622 -4.4282551 -4.4284134 -4.4285908 -4.4287333 -4.4288521 -4.42892 -4.42895][-4.428966 -4.428968 -4.4289203 -4.4288106 -4.4286642 -4.4285159 -4.4283705 -4.4282613 -4.4282603 -4.4284191 -4.4285941 -4.428731 -4.4288449 -4.4289179 -4.42895][-4.4289575 -4.4289503 -4.428905 -4.4288058 -4.428669 -4.4285254 -4.4283891 -4.4283013 -4.4283118 -4.4284568 -4.4286127 -4.4287305 -4.4288278 -4.4289017 -4.4289417][-4.4289517 -4.4289465 -4.4289117 -4.4288321 -4.4287162 -4.4285874 -4.4284654 -4.4283857 -4.4283848 -4.4284954 -4.4286327 -4.4287405 -4.428822 -4.4288888 -4.4289336][-4.4289494 -4.4289541 -4.4289312 -4.4288731 -4.4287786 -4.4286695 -4.4285555 -4.4284682 -4.4284415 -4.4285231 -4.428658 -4.428771 -4.4288406 -4.4288964 -4.4289393][-4.4289336 -4.4289541 -4.4289446 -4.4289012 -4.428822 -4.4287224 -4.4286146 -4.4285259 -4.4284835 -4.4285526 -4.4286933 -4.4288087 -4.4288678 -4.42891 -4.428947][-4.4289021 -4.4289451 -4.4289551 -4.4289255 -4.4288521 -4.4287505 -4.428637 -4.4285426 -4.42849 -4.4285564 -4.4287024 -4.4288211 -4.4288769 -4.428915 -4.4289503][-4.4288669 -4.4289241 -4.4289532 -4.4289432 -4.4288869 -4.428792 -4.4286728 -4.428566 -4.4285 -4.4285641 -4.4287076 -4.4288216 -4.4288769 -4.4289179 -4.4289536][-4.4288316 -4.4288931 -4.42894 -4.4289513 -4.42892 -4.4288497 -4.4287443 -4.4286356 -4.4285574 -4.4286056 -4.4287314 -4.4288321 -4.4288869 -4.4289303 -4.4289656]]...]
INFO - root - 2017-12-08 07:27:39.081436: step 41010, loss = 2.28, batch loss = 2.23 (33.8 examples/sec; 0.236 sec/batch; 19h:08m:25s remains)
INFO - root - 2017-12-08 07:27:41.310213: step 41020, loss = 2.28, batch loss = 2.23 (37.2 examples/sec; 0.215 sec/batch; 17h:24m:28s remains)
INFO - root - 2017-12-08 07:27:43.554770: step 41030, loss = 2.28, batch loss = 2.23 (35.6 examples/sec; 0.225 sec/batch; 18h:12m:48s remains)
INFO - root - 2017-12-08 07:27:45.829421: step 41040, loss = 2.28, batch loss = 2.23 (34.8 examples/sec; 0.230 sec/batch; 18h:37m:24s remains)
INFO - root - 2017-12-08 07:27:48.064585: step 41050, loss = 2.28, batch loss = 2.23 (34.8 examples/sec; 0.230 sec/batch; 18h:36m:14s remains)
INFO - root - 2017-12-08 07:27:50.302882: step 41060, loss = 2.28, batch loss = 2.23 (34.5 examples/sec; 0.232 sec/batch; 18h:45m:28s remains)
INFO - root - 2017-12-08 07:27:52.545929: step 41070, loss = 2.28, batch loss = 2.23 (32.5 examples/sec; 0.246 sec/batch; 19h:53m:56s remains)
INFO - root - 2017-12-08 07:27:54.839752: step 41080, loss = 2.28, batch loss = 2.23 (34.7 examples/sec; 0.231 sec/batch; 18h:41m:00s remains)
INFO - root - 2017-12-08 07:27:57.083568: step 41090, loss = 2.28, batch loss = 2.23 (36.8 examples/sec; 0.217 sec/batch; 17h:36m:09s remains)
INFO - root - 2017-12-08 07:27:59.310953: step 41100, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 18h:02m:01s remains)
2017-12-08 07:27:59.616181: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4287405 -4.4287314 -4.42877 -4.4288225 -4.4288635 -4.42888 -4.4288659 -4.4288335 -4.4287958 -4.4287739 -4.4287686 -4.42876 -4.4287605 -4.4287925 -4.4288387][-4.4287014 -4.4286909 -4.4287319 -4.4287829 -4.4288187 -4.4288278 -4.4288116 -4.4287839 -4.4287577 -4.4287472 -4.4287491 -4.42874 -4.4287333 -4.4287605 -4.4288073][-4.4287214 -4.4287081 -4.4287372 -4.4287758 -4.4287992 -4.42879 -4.4287572 -4.4287181 -4.4286962 -4.4287047 -4.4287271 -4.4287329 -4.4287353 -4.4287629 -4.4288144][-4.4287882 -4.4287758 -4.4287834 -4.4287958 -4.4287887 -4.4287386 -4.4286633 -4.4285994 -4.4285855 -4.4286284 -4.4286871 -4.4287267 -4.4287577 -4.4288 -4.4288588][-4.4288688 -4.4288588 -4.428843 -4.428813 -4.4287481 -4.4286308 -4.4285011 -4.42842 -4.4284372 -4.4285278 -4.4286323 -4.428719 -4.4287882 -4.4288483 -4.4289083][-4.428926 -4.4289227 -4.42889 -4.4288096 -4.4286728 -4.4284744 -4.4282813 -4.4282 -4.4282846 -4.4284453 -4.4286 -4.4287243 -4.4288244 -4.4288964 -4.428947][-4.428957 -4.4289608 -4.428915 -4.4287906 -4.4285922 -4.4283328 -4.4280863 -4.4280205 -4.4281964 -4.4284344 -4.4286261 -4.4287648 -4.4288721 -4.4289403 -4.4289727][-4.4289618 -4.4289684 -4.4289107 -4.4287605 -4.4285431 -4.4282913 -4.428082 -4.4280872 -4.4283 -4.4285369 -4.4287114 -4.4288306 -4.428916 -4.4289618 -4.4289689][-4.4289556 -4.4289613 -4.4288983 -4.42875 -4.4285593 -4.4283748 -4.4282789 -4.4283547 -4.4285369 -4.4287052 -4.4288225 -4.4289002 -4.428947 -4.4289579 -4.4289346][-4.4289346 -4.4289484 -4.4289012 -4.4287848 -4.428647 -4.4285407 -4.4285307 -4.4286237 -4.4287529 -4.4288521 -4.4289103 -4.4289436 -4.4289513 -4.4289336 -4.4288926][-4.4289165 -4.4289489 -4.42893 -4.4288549 -4.4287686 -4.428721 -4.4287462 -4.4288206 -4.4289 -4.4289474 -4.4289618 -4.428957 -4.4289412 -4.428916 -4.428874][-4.428925 -4.4289694 -4.4289689 -4.4289236 -4.4288735 -4.4288597 -4.4288893 -4.4289412 -4.4289827 -4.4289918 -4.4289756 -4.4289503 -4.4289336 -4.4289117 -4.428874][-4.4289513 -4.4289865 -4.4289846 -4.4289527 -4.4289236 -4.4289246 -4.4289517 -4.4289851 -4.4289975 -4.4289823 -4.4289532 -4.4289293 -4.4289227 -4.4289064 -4.4288754][-4.4289761 -4.4289846 -4.4289646 -4.4289341 -4.4289184 -4.4289303 -4.4289513 -4.4289675 -4.4289627 -4.4289403 -4.4289165 -4.4289036 -4.428906 -4.4288936 -4.4288654][-4.4289913 -4.4289722 -4.4289351 -4.4289012 -4.428894 -4.4289117 -4.4289222 -4.4289179 -4.4288974 -4.4288759 -4.4288645 -4.428863 -4.4288759 -4.4288688 -4.4288449]]...]
INFO - root - 2017-12-08 07:28:01.864935: step 41110, loss = 2.28, batch loss = 2.23 (34.2 examples/sec; 0.234 sec/batch; 18h:57m:26s remains)
INFO - root - 2017-12-08 07:28:04.096380: step 41120, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 17h:44m:30s remains)
INFO - root - 2017-12-08 07:28:06.381537: step 41130, loss = 2.28, batch loss = 2.23 (35.3 examples/sec; 0.227 sec/batch; 18h:21m:40s remains)
INFO - root - 2017-12-08 07:28:08.623894: step 41140, loss = 2.28, batch loss = 2.23 (35.2 examples/sec; 0.227 sec/batch; 18h:22m:30s remains)
INFO - root - 2017-12-08 07:28:10.871251: step 41150, loss = 2.28, batch loss = 2.23 (35.4 examples/sec; 0.226 sec/batch; 18h:17m:03s remains)
INFO - root - 2017-12-08 07:28:13.078866: step 41160, loss = 2.28, batch loss = 2.23 (34.7 examples/sec; 0.230 sec/batch; 18h:38m:48s remains)
INFO - root - 2017-12-08 07:28:15.348911: step 41170, loss = 2.28, batch loss = 2.23 (35.7 examples/sec; 0.224 sec/batch; 18h:07m:05s remains)
INFO - root - 2017-12-08 07:28:17.582724: step 41180, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 17h:44m:00s remains)
INFO - root - 2017-12-08 07:28:19.826057: step 41190, loss = 2.28, batch loss = 2.23 (35.3 examples/sec; 0.226 sec/batch; 18h:19m:03s remains)
INFO - root - 2017-12-08 07:28:22.072537: step 41200, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.220 sec/batch; 17h:48m:51s remains)
2017-12-08 07:28:22.391576: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288149 -4.4288015 -4.4287972 -4.4288049 -4.4288206 -4.42884 -4.4288616 -4.4288859 -4.4289112 -4.4289355 -4.4289603 -4.4289808 -4.4289942 -4.4290018 -4.4290061][-4.4288607 -4.4288454 -4.4288368 -4.4288387 -4.4288492 -4.4288635 -4.4288797 -4.4288988 -4.4289184 -4.4289389 -4.4289608 -4.4289804 -4.4289942 -4.4290023 -4.4290075][-4.4289303 -4.4289231 -4.4289155 -4.4289126 -4.4289136 -4.4289179 -4.4289241 -4.4289322 -4.4289422 -4.4289556 -4.4289718 -4.428987 -4.4289975 -4.4290047 -4.4290094][-4.4289861 -4.4289865 -4.4289775 -4.4289641 -4.42895 -4.4289389 -4.4289327 -4.4289336 -4.4289417 -4.428957 -4.4289746 -4.4289904 -4.4290018 -4.4290094 -4.4290142][-4.429008 -4.4290071 -4.4289875 -4.4289546 -4.4289184 -4.4288898 -4.4288731 -4.4288726 -4.4288869 -4.4289126 -4.4289422 -4.4289675 -4.428987 -4.4290013 -4.4290118][-4.4289746 -4.4289665 -4.4289355 -4.4288845 -4.4288273 -4.4287806 -4.4287558 -4.4287543 -4.4287763 -4.4288163 -4.4288621 -4.4289036 -4.4289403 -4.42897 -4.4289927][-4.4288907 -4.42888 -4.4288464 -4.4287882 -4.4287167 -4.4286532 -4.428616 -4.4286056 -4.4286242 -4.4286709 -4.4287324 -4.4287939 -4.4288516 -4.4289041 -4.4289474][-4.4287844 -4.4287863 -4.4287658 -4.4287157 -4.4286408 -4.428565 -4.4285111 -4.42848 -4.4284825 -4.4285231 -4.4285903 -4.4286671 -4.4287438 -4.4288168 -4.4288797][-4.4286861 -4.428709 -4.4287119 -4.4286833 -4.4286218 -4.4285512 -4.4284883 -4.4284377 -4.4284163 -4.4284382 -4.4284935 -4.4285712 -4.4286532 -4.4287357 -4.42881][-4.4286284 -4.4286695 -4.4286957 -4.4286928 -4.4286604 -4.4286156 -4.4285679 -4.4285173 -4.4284835 -4.4284792 -4.428504 -4.428556 -4.42862 -4.4286919 -4.4287605][-4.4286456 -4.4286914 -4.42873 -4.4287496 -4.4287453 -4.4287286 -4.4287043 -4.428669 -4.4286356 -4.4286137 -4.428606 -4.42862 -4.42865 -4.4286942 -4.4287434][-4.4287267 -4.4287653 -4.4288025 -4.4288292 -4.4288411 -4.428843 -4.4288363 -4.4288163 -4.428791 -4.4287639 -4.4287395 -4.4287267 -4.4287252 -4.42874 -4.4287639][-4.42881 -4.4288397 -4.4288707 -4.4288969 -4.4289145 -4.428925 -4.4289269 -4.4289174 -4.4289007 -4.4288778 -4.4288521 -4.4288311 -4.4288173 -4.4288154 -4.4288216][-4.4288635 -4.4288845 -4.4289103 -4.4289327 -4.42895 -4.4289622 -4.428967 -4.4289646 -4.428957 -4.4289432 -4.4289265 -4.4289112 -4.4288993 -4.4288936 -4.4288926][-4.4289 -4.4289131 -4.4289317 -4.42895 -4.4289651 -4.4289761 -4.4289813 -4.4289823 -4.4289808 -4.4289756 -4.4289689 -4.4289627 -4.428957 -4.4289532 -4.4289513]]...]
INFO - root - 2017-12-08 07:28:24.622968: step 41210, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.225 sec/batch; 18h:13m:47s remains)
INFO - root - 2017-12-08 07:28:26.882583: step 41220, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 18h:01m:25s remains)
INFO - root - 2017-12-08 07:28:29.114591: step 41230, loss = 2.28, batch loss = 2.23 (35.0 examples/sec; 0.228 sec/batch; 18h:28m:35s remains)
INFO - root - 2017-12-08 07:28:31.331954: step 41240, loss = 2.28, batch loss = 2.23 (34.3 examples/sec; 0.233 sec/batch; 18h:52m:36s remains)
INFO - root - 2017-12-08 07:28:33.548106: step 41250, loss = 2.28, batch loss = 2.23 (36.8 examples/sec; 0.218 sec/batch; 17h:36m:24s remains)
INFO - root - 2017-12-08 07:28:35.798113: step 41260, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.220 sec/batch; 17h:48m:36s remains)
INFO - root - 2017-12-08 07:28:38.057085: step 41270, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.225 sec/batch; 18h:14m:28s remains)
INFO - root - 2017-12-08 07:28:40.308572: step 41280, loss = 2.28, batch loss = 2.23 (33.9 examples/sec; 0.236 sec/batch; 19h:04m:16s remains)
INFO - root - 2017-12-08 07:28:42.548622: step 41290, loss = 2.28, batch loss = 2.23 (35.0 examples/sec; 0.228 sec/batch; 18h:28m:55s remains)
INFO - root - 2017-12-08 07:28:44.788451: step 41300, loss = 2.28, batch loss = 2.23 (36.9 examples/sec; 0.217 sec/batch; 17h:31m:30s remains)
2017-12-08 07:28:45.081395: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4287696 -4.428792 -4.4288015 -4.4287925 -4.4288068 -4.4288349 -4.428833 -4.4288092 -4.4287834 -4.4287624 -4.4287295 -4.4286971 -4.4286451 -4.42856 -4.428515][-4.42878 -4.4288034 -4.4287987 -4.4287734 -4.4287715 -4.4287848 -4.4287829 -4.4287686 -4.4287705 -4.4287791 -4.428761 -4.4287267 -4.4286852 -4.4286356 -4.4286304][-4.4287753 -4.4288082 -4.4288006 -4.4287677 -4.4287467 -4.4287353 -4.4287167 -4.4287043 -4.4287252 -4.4287615 -4.4287629 -4.428731 -4.428699 -4.4286757 -4.4286938][-4.4287486 -4.4287944 -4.4287996 -4.4287753 -4.4287381 -4.4287014 -4.4286523 -4.4286261 -4.4286571 -4.4287109 -4.4287252 -4.4287024 -4.4286804 -4.4286628 -4.4286814][-4.4287181 -4.4287653 -4.4287763 -4.4287577 -4.4287128 -4.4286556 -4.4285774 -4.4285278 -4.428565 -4.4286437 -4.4286776 -4.4286637 -4.4286404 -4.4286065 -4.4286013][-4.4286938 -4.4287238 -4.4287143 -4.4286866 -4.428638 -4.4285731 -4.4284849 -4.4284286 -4.4284844 -4.4285917 -4.4286437 -4.4286327 -4.4285913 -4.4285216 -4.4284844][-4.4286757 -4.4286485 -4.4285851 -4.4285312 -4.4284816 -4.4284444 -4.4283886 -4.4283476 -4.4284177 -4.4285421 -4.4286 -4.4285831 -4.4285235 -4.4284277 -4.4283695][-4.4286733 -4.4285932 -4.4284773 -4.4283919 -4.42835 -4.4283543 -4.4283528 -4.4283428 -4.4284086 -4.42852 -4.4285603 -4.42853 -4.4284697 -4.4283748 -4.428318][-4.4286785 -4.42858 -4.428452 -4.4283628 -4.4283395 -4.428371 -4.42841 -4.4284306 -4.4284883 -4.4285603 -4.4285688 -4.428525 -4.4284692 -4.4283962 -4.4283671][-4.4287238 -4.4286304 -4.4285188 -4.4284534 -4.4284558 -4.4285 -4.4285488 -4.4285755 -4.4286127 -4.4286427 -4.4286294 -4.4285855 -4.4285431 -4.4284992 -4.4284925][-4.4287834 -4.4287186 -4.4286408 -4.4286027 -4.4286184 -4.4286609 -4.4287009 -4.428721 -4.42873 -4.4287267 -4.4287019 -4.4286718 -4.4286523 -4.4286361 -4.4286437][-4.42881 -4.428781 -4.4287376 -4.4287186 -4.4287419 -4.42878 -4.428812 -4.4288249 -4.428813 -4.4287891 -4.4287577 -4.4287362 -4.4287338 -4.4287314 -4.4287395][-4.4288073 -4.4288125 -4.4287972 -4.4287825 -4.4288 -4.4288316 -4.4288568 -4.4288659 -4.4288511 -4.4288254 -4.4288006 -4.4287853 -4.4287834 -4.4287796 -4.4287772][-4.4287596 -4.4287953 -4.4288077 -4.4287987 -4.4288087 -4.4288335 -4.428853 -4.4288611 -4.428854 -4.428834 -4.4288163 -4.4288087 -4.4288058 -4.428793 -4.428781][-4.4286714 -4.4287262 -4.4287615 -4.4287667 -4.4287763 -4.4287963 -4.428812 -4.4288235 -4.4288297 -4.4288216 -4.4288092 -4.4288 -4.42879 -4.4287691 -4.4287562]]...]
INFO - root - 2017-12-08 07:28:47.316158: step 41310, loss = 2.28, batch loss = 2.23 (35.1 examples/sec; 0.228 sec/batch; 18h:27m:35s remains)
INFO - root - 2017-12-08 07:28:49.529703: step 41320, loss = 2.28, batch loss = 2.23 (34.9 examples/sec; 0.229 sec/batch; 18h:33m:44s remains)
INFO - root - 2017-12-08 07:28:51.775673: step 41330, loss = 2.28, batch loss = 2.23 (31.9 examples/sec; 0.251 sec/batch; 20h:17m:02s remains)
INFO - root - 2017-12-08 07:28:54.051539: step 41340, loss = 2.28, batch loss = 2.23 (34.9 examples/sec; 0.229 sec/batch; 18h:31m:12s remains)
INFO - root - 2017-12-08 07:28:56.337485: step 41350, loss = 2.28, batch loss = 2.23 (34.0 examples/sec; 0.235 sec/batch; 19h:00m:12s remains)
INFO - root - 2017-12-08 07:28:58.582093: step 41360, loss = 2.28, batch loss = 2.23 (36.6 examples/sec; 0.219 sec/batch; 17h:41m:53s remains)
INFO - root - 2017-12-08 07:29:00.829449: step 41370, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.221 sec/batch; 17h:50m:06s remains)
INFO - root - 2017-12-08 07:29:03.060691: step 41380, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 17h:46m:19s remains)
INFO - root - 2017-12-08 07:29:05.289627: step 41390, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.226 sec/batch; 18h:14m:47s remains)
INFO - root - 2017-12-08 07:29:07.542307: step 41400, loss = 2.28, batch loss = 2.23 (34.4 examples/sec; 0.232 sec/batch; 18h:46m:43s remains)
2017-12-08 07:29:07.858419: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289203 -4.4289007 -4.4288545 -4.4287939 -4.428751 -4.4287486 -4.4287758 -4.4288054 -4.4288392 -4.4288607 -4.4288526 -4.42882 -4.428781 -4.4287567 -4.4287653][-4.4289064 -4.4288797 -4.4288144 -4.4287376 -4.4286871 -4.4286852 -4.4287238 -4.4287682 -4.4288168 -4.4288511 -4.4288497 -4.4288173 -4.4287724 -4.4287486 -4.4287691][-4.4288769 -4.428843 -4.4287696 -4.4286857 -4.4286275 -4.4286232 -4.4286685 -4.4287267 -4.4287877 -4.428833 -4.428843 -4.4288192 -4.4287815 -4.4287639 -4.4287949][-4.4288287 -4.4287858 -4.4287109 -4.4286246 -4.428556 -4.4285488 -4.4286041 -4.4286785 -4.4287529 -4.4288068 -4.4288306 -4.4288197 -4.4287968 -4.428792 -4.4288259][-4.4287844 -4.4287324 -4.4286485 -4.4285502 -4.428463 -4.4284477 -4.4285226 -4.4286194 -4.4287095 -4.4287724 -4.4288054 -4.4288058 -4.4287996 -4.4288073 -4.4288387][-4.4287772 -4.4287214 -4.428627 -4.4285107 -4.4283905 -4.4283514 -4.42844 -4.4285612 -4.4286623 -4.4287267 -4.4287663 -4.42877 -4.4287748 -4.4287906 -4.4288192][-4.4287872 -4.4287262 -4.4286184 -4.4284706 -4.4283051 -4.4282351 -4.4283266 -4.4284616 -4.4285731 -4.4286504 -4.4287033 -4.4287143 -4.428721 -4.4287386 -4.4287696][-4.4287953 -4.4287319 -4.4286246 -4.4284611 -4.428267 -4.4281697 -4.4282484 -4.4283772 -4.4284863 -4.42858 -4.4286489 -4.4286675 -4.4286733 -4.4286923 -4.4287271][-4.4288263 -4.4287758 -4.4286904 -4.4285436 -4.4283581 -4.4282575 -4.4283156 -4.4284134 -4.4284992 -4.4285884 -4.4286594 -4.4286809 -4.4286852 -4.4287043 -4.4287429][-4.4288645 -4.4288273 -4.4287615 -4.4286456 -4.4284973 -4.428411 -4.4284506 -4.4285131 -4.4285712 -4.4286408 -4.4287019 -4.4287319 -4.4287457 -4.428771 -4.4288144][-4.4288874 -4.42886 -4.4288087 -4.4287252 -4.42862 -4.4285569 -4.4285851 -4.4286261 -4.4286728 -4.428731 -4.4287796 -4.4288125 -4.4288349 -4.4288616 -4.4288988][-4.4288888 -4.4288669 -4.4288273 -4.4287682 -4.4287019 -4.4286718 -4.4286976 -4.428731 -4.4287715 -4.4288168 -4.4288521 -4.4288774 -4.4288988 -4.4289227 -4.42895][-4.4288716 -4.4288535 -4.4288282 -4.4287915 -4.4287529 -4.4287462 -4.4287734 -4.4288039 -4.4288397 -4.4288726 -4.428894 -4.4289088 -4.4289265 -4.4289474 -4.4289646][-4.4288669 -4.4288459 -4.4288259 -4.4288039 -4.4287834 -4.4287868 -4.4288144 -4.4288473 -4.4288774 -4.4288955 -4.4289055 -4.4289136 -4.4289274 -4.4289427 -4.4289536][-4.4288783 -4.4288549 -4.4288349 -4.4288173 -4.428802 -4.4288015 -4.42882 -4.4288459 -4.4288669 -4.4288774 -4.4288826 -4.4288907 -4.428905 -4.4289179 -4.4289255]]...]
INFO - root - 2017-12-08 07:29:10.111659: step 41410, loss = 2.28, batch loss = 2.23 (35.3 examples/sec; 0.227 sec/batch; 18h:20m:53s remains)
INFO - root - 2017-12-08 07:29:12.361830: step 41420, loss = 2.28, batch loss = 2.23 (34.9 examples/sec; 0.229 sec/batch; 18h:32m:08s remains)
INFO - root - 2017-12-08 07:29:14.602083: step 41430, loss = 2.28, batch loss = 2.23 (37.5 examples/sec; 0.214 sec/batch; 17h:16m:06s remains)
INFO - root - 2017-12-08 07:29:16.815311: step 41440, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 17h:52m:14s remains)
INFO - root - 2017-12-08 07:29:19.054576: step 41450, loss = 2.28, batch loss = 2.23 (35.6 examples/sec; 0.224 sec/batch; 18h:08m:49s remains)
INFO - root - 2017-12-08 07:29:21.336881: step 41460, loss = 2.28, batch loss = 2.23 (33.5 examples/sec; 0.239 sec/batch; 19h:17m:39s remains)
INFO - root - 2017-12-08 07:29:23.610860: step 41470, loss = 2.28, batch loss = 2.23 (36.0 examples/sec; 0.222 sec/batch; 17h:58m:29s remains)
INFO - root - 2017-12-08 07:29:25.882653: step 41480, loss = 2.28, batch loss = 2.23 (35.4 examples/sec; 0.226 sec/batch; 18h:15m:41s remains)
INFO - root - 2017-12-08 07:29:28.137183: step 41490, loss = 2.28, batch loss = 2.23 (37.3 examples/sec; 0.215 sec/batch; 17h:21m:09s remains)
INFO - root - 2017-12-08 07:29:30.367676: step 41500, loss = 2.28, batch loss = 2.23 (35.2 examples/sec; 0.227 sec/batch; 18h:21m:28s remains)
2017-12-08 07:29:30.687381: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4286833 -4.4286971 -4.4287219 -4.4287663 -4.4288335 -4.4289012 -4.4289174 -4.4288659 -4.4287949 -4.4287562 -4.4287467 -4.4287591 -4.4287691 -4.4287591 -4.4287348][-4.4286594 -4.4286928 -4.4287381 -4.4287996 -4.4288731 -4.42893 -4.4289265 -4.4288554 -4.4287734 -4.4287295 -4.4287281 -4.4287624 -4.4287944 -4.4288034 -4.428792][-4.428638 -4.4286976 -4.4287705 -4.4288464 -4.4289122 -4.4289455 -4.4289112 -4.4288149 -4.4287143 -4.4286609 -4.4286785 -4.4287438 -4.4288015 -4.4288363 -4.4288511][-4.4286375 -4.4287248 -4.428812 -4.4288797 -4.4289122 -4.4289026 -4.4288392 -4.4287243 -4.4286141 -4.4285674 -4.428618 -4.4287176 -4.4287987 -4.4288535 -4.4288931][-4.4286852 -4.4287591 -4.4288244 -4.4288645 -4.4288654 -4.4288182 -4.4287205 -4.4285769 -4.4284606 -4.42845 -4.4285531 -4.42869 -4.4287844 -4.4288445 -4.4288931][-4.4287457 -4.4287882 -4.4288154 -4.4288244 -4.4287968 -4.4287171 -4.4285755 -4.4283814 -4.4282584 -4.4283195 -4.4284945 -4.4286642 -4.4287643 -4.4288173 -4.4288592][-4.4287753 -4.4287848 -4.4287777 -4.4287605 -4.4287214 -4.4286175 -4.4284368 -4.4281936 -4.4280629 -4.4282117 -4.4284482 -4.4286346 -4.4287343 -4.4287753 -4.4288087][-4.4287424 -4.4287176 -4.4286942 -4.428688 -4.4286642 -4.4285631 -4.4283795 -4.4281421 -4.4280481 -4.4282374 -4.4284716 -4.4286375 -4.4287162 -4.4287381 -4.4287562][-4.4286842 -4.428637 -4.4286194 -4.4286356 -4.428628 -4.428556 -4.4284215 -4.4282713 -4.4282484 -4.4284024 -4.4285746 -4.4286895 -4.4287338 -4.4287381 -4.4287391][-4.4286313 -4.4285831 -4.4285789 -4.4286065 -4.4286165 -4.4285765 -4.4285035 -4.4284353 -4.4284554 -4.4285669 -4.4286838 -4.4287581 -4.4287815 -4.4287763 -4.4287705][-4.4285927 -4.4285693 -4.4285765 -4.4286065 -4.4286265 -4.4286189 -4.4285946 -4.4285707 -4.4286008 -4.4286804 -4.428762 -4.428812 -4.4288254 -4.4288187 -4.4288158][-4.4285803 -4.4285731 -4.4285851 -4.4286151 -4.4286451 -4.4286647 -4.42867 -4.4286652 -4.4286861 -4.4287434 -4.4288039 -4.4288397 -4.4288483 -4.4288397 -4.4288383][-4.42861 -4.4286151 -4.428627 -4.4286456 -4.4286766 -4.4287076 -4.4287229 -4.4287252 -4.4287434 -4.4287863 -4.4288325 -4.4288549 -4.428853 -4.4288368 -4.4288311][-4.4286695 -4.428688 -4.4286971 -4.4287014 -4.4287214 -4.4287453 -4.42876 -4.4287639 -4.42878 -4.4288187 -4.4288497 -4.4288578 -4.4288406 -4.4288111 -4.428802][-4.4287305 -4.4287591 -4.4287772 -4.4287839 -4.428791 -4.4287925 -4.42879 -4.4287863 -4.4288015 -4.4288344 -4.4288507 -4.4288421 -4.4288106 -4.4287724 -4.4287658]]...]
INFO - root - 2017-12-08 07:29:32.941897: step 41510, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 18h:00m:07s remains)
INFO - root - 2017-12-08 07:29:35.186535: step 41520, loss = 2.28, batch loss = 2.23 (36.0 examples/sec; 0.222 sec/batch; 17h:58m:51s remains)
INFO - root - 2017-12-08 07:29:37.407094: step 41530, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 17h:51m:11s remains)
INFO - root - 2017-12-08 07:29:39.648725: step 41540, loss = 2.28, batch loss = 2.23 (36.6 examples/sec; 0.218 sec/batch; 17h:38m:53s remains)
INFO - root - 2017-12-08 07:29:41.901829: step 41550, loss = 2.28, batch loss = 2.23 (35.4 examples/sec; 0.226 sec/batch; 18h:17m:02s remains)
INFO - root - 2017-12-08 07:29:44.125718: step 41560, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.222 sec/batch; 17h:54m:04s remains)
INFO - root - 2017-12-08 07:29:46.384492: step 41570, loss = 2.28, batch loss = 2.23 (35.7 examples/sec; 0.224 sec/batch; 18h:05m:46s remains)
INFO - root - 2017-12-08 07:29:48.616252: step 41580, loss = 2.28, batch loss = 2.23 (36.9 examples/sec; 0.217 sec/batch; 17h:29m:58s remains)
INFO - root - 2017-12-08 07:29:50.857166: step 41590, loss = 2.28, batch loss = 2.23 (35.6 examples/sec; 0.225 sec/batch; 18h:10m:16s remains)
INFO - root - 2017-12-08 07:29:53.065836: step 41600, loss = 2.28, batch loss = 2.23 (37.0 examples/sec; 0.216 sec/batch; 17h:29m:07s remains)
2017-12-08 07:29:53.388094: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.428957 -4.4289503 -4.428936 -4.4289203 -4.4289026 -4.4288845 -4.4288716 -4.4288697 -4.4288807 -4.4288926 -4.4289031 -4.428916 -4.4289269 -4.4289393 -4.4289551][-4.4289465 -4.4289331 -4.4289093 -4.4288793 -4.4288449 -4.4288139 -4.4287982 -4.428802 -4.4288249 -4.4288564 -4.4288821 -4.4289021 -4.4289141 -4.4289255 -4.4289451][-4.4289122 -4.4288893 -4.428854 -4.4288116 -4.4287586 -4.4287176 -4.4287 -4.42871 -4.4287481 -4.4288034 -4.4288497 -4.4288807 -4.4288945 -4.4289002 -4.4289145][-4.4288678 -4.4288406 -4.4287968 -4.428741 -4.4286695 -4.4286122 -4.4285855 -4.4286017 -4.4286561 -4.4287338 -4.4287949 -4.4288321 -4.4288492 -4.4288554 -4.4288659][-4.428834 -4.4288025 -4.4287505 -4.428678 -4.4285817 -4.4284892 -4.4284306 -4.4284482 -4.4285374 -4.4286442 -4.4287257 -4.4287729 -4.4287977 -4.428812 -4.4288235][-4.4288359 -4.4287987 -4.4287376 -4.4286418 -4.4285097 -4.428359 -4.42824 -4.4282522 -4.428401 -4.4285536 -4.4286594 -4.4287229 -4.4287586 -4.4287868 -4.4288058][-4.4288516 -4.4288225 -4.4287634 -4.428647 -4.4284716 -4.4282441 -4.4280372 -4.4280357 -4.4282618 -4.4284606 -4.4285855 -4.4286613 -4.4287124 -4.4287605 -4.4287987][-4.4288316 -4.4288259 -4.4287944 -4.4287009 -4.4285407 -4.4283032 -4.4280744 -4.4280477 -4.4282627 -4.4284382 -4.428545 -4.4286141 -4.4286718 -4.4287314 -4.428782][-4.4287782 -4.4288034 -4.4288149 -4.4287705 -4.4286718 -4.4285188 -4.4283752 -4.4283438 -4.4284372 -4.4285059 -4.4285555 -4.4286017 -4.4286532 -4.4287076 -4.4287629][-4.428719 -4.4287782 -4.4288316 -4.4288206 -4.4287596 -4.4286823 -4.4286308 -4.4286056 -4.4286046 -4.4285946 -4.4286041 -4.4286327 -4.4286685 -4.4287066 -4.42876][-4.4286938 -4.4287705 -4.4288449 -4.4288549 -4.4288063 -4.4287724 -4.4287777 -4.4287648 -4.4287291 -4.4286919 -4.4286819 -4.4286976 -4.4287119 -4.4287281 -4.4287734][-4.4287028 -4.4287534 -4.428823 -4.4288497 -4.4288249 -4.4288168 -4.4288459 -4.4288492 -4.4288187 -4.4287858 -4.4287663 -4.4287691 -4.4287639 -4.4287658 -4.4288011][-4.4287696 -4.4287639 -4.4287987 -4.4288273 -4.428823 -4.4288297 -4.4288549 -4.4288588 -4.4288516 -4.4288397 -4.42882 -4.4288197 -4.4288182 -4.4288239 -4.4288516][-4.4288406 -4.4287968 -4.42879 -4.4288125 -4.4288263 -4.4288411 -4.4288559 -4.4288559 -4.42886 -4.4288673 -4.4288583 -4.4288688 -4.4288855 -4.4288964 -4.4289169][-4.4289103 -4.428844 -4.4288054 -4.42882 -4.4288497 -4.428865 -4.4288635 -4.4288554 -4.4288578 -4.4288735 -4.428885 -4.4289117 -4.4289393 -4.4289522 -4.4289651]]...]
INFO - root - 2017-12-08 07:29:55.626040: step 41610, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.222 sec/batch; 17h:55m:33s remains)
INFO - root - 2017-12-08 07:29:57.842387: step 41620, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.222 sec/batch; 17h:53m:58s remains)
INFO - root - 2017-12-08 07:30:00.090324: step 41630, loss = 2.28, batch loss = 2.23 (35.6 examples/sec; 0.225 sec/batch; 18h:09m:41s remains)
INFO - root - 2017-12-08 07:30:02.311217: step 41640, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 17h:42m:40s remains)
INFO - root - 2017-12-08 07:30:04.538566: step 41650, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.225 sec/batch; 18h:10m:57s remains)
INFO - root - 2017-12-08 07:30:06.767492: step 41660, loss = 2.28, batch loss = 2.23 (35.8 examples/sec; 0.223 sec/batch; 18h:02m:09s remains)
INFO - root - 2017-12-08 07:30:08.999168: step 41670, loss = 2.28, batch loss = 2.23 (34.1 examples/sec; 0.234 sec/batch; 18h:55m:46s remains)
INFO - root - 2017-12-08 07:30:11.242826: step 41680, loss = 2.28, batch loss = 2.23 (33.6 examples/sec; 0.238 sec/batch; 19h:12m:24s remains)
INFO - root - 2017-12-08 07:30:13.471253: step 41690, loss = 2.28, batch loss = 2.23 (37.4 examples/sec; 0.214 sec/batch; 17h:16m:51s remains)
INFO - root - 2017-12-08 07:30:15.702048: step 41700, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 17h:43m:19s remains)
2017-12-08 07:30:15.979846: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.42868 -4.4286432 -4.428607 -4.428566 -4.4285536 -4.4285855 -4.42865 -4.4287443 -4.4288039 -4.4288006 -4.4287853 -4.428781 -4.4287586 -4.4287305 -4.4286952][-4.4286876 -4.4286656 -4.4286504 -4.4286208 -4.4286113 -4.4286346 -4.428679 -4.428751 -4.4287925 -4.4287829 -4.4287796 -4.4287844 -4.4287772 -4.4287686 -4.428751][-4.4286995 -4.4286866 -4.4286919 -4.4286814 -4.4286747 -4.4286847 -4.428709 -4.4287558 -4.428771 -4.4287534 -4.4287581 -4.4287715 -4.4287791 -4.4287906 -4.4287925][-4.4286718 -4.42867 -4.4286861 -4.4286776 -4.4286718 -4.4286757 -4.4286904 -4.4287176 -4.4287162 -4.4287009 -4.42871 -4.4287376 -4.4287715 -4.4287992 -4.428813][-4.4286122 -4.4286375 -4.4286652 -4.4286556 -4.4286489 -4.4286523 -4.4286666 -4.428678 -4.4286637 -4.4286461 -4.428658 -4.4287 -4.4287539 -4.4287963 -4.4288154][-4.4285636 -4.4286261 -4.4286666 -4.4286504 -4.4286337 -4.4286361 -4.4286456 -4.4286394 -4.4286094 -4.4285955 -4.4286232 -4.4286833 -4.4287481 -4.4287977 -4.4288225][-4.4285665 -4.4286547 -4.4286985 -4.4286737 -4.4286423 -4.4286332 -4.4286203 -4.4285922 -4.4285641 -4.4285645 -4.4286165 -4.42869 -4.428751 -4.4288006 -4.4288263][-4.4286337 -4.4287157 -4.4287491 -4.4287186 -4.428678 -4.4286556 -4.4286189 -4.4285808 -4.4285603 -4.4285746 -4.4286394 -4.4287148 -4.4287663 -4.4288106 -4.4288344][-4.4287348 -4.4287877 -4.428792 -4.4287453 -4.4287052 -4.4286861 -4.4286509 -4.4286146 -4.4285975 -4.428616 -4.428679 -4.428751 -4.4288 -4.4288378 -4.4288559][-4.4288182 -4.428843 -4.4288163 -4.4287586 -4.4287224 -4.4287038 -4.4286718 -4.428638 -4.4286275 -4.4286528 -4.4287081 -4.4287758 -4.4288244 -4.4288554 -4.4288697][-4.4288573 -4.4288545 -4.4288125 -4.4287648 -4.4287381 -4.4287233 -4.4286962 -4.4286757 -4.4286757 -4.4287014 -4.4287357 -4.4287858 -4.4288244 -4.4288492 -4.4288621][-4.4288492 -4.4288259 -4.4287868 -4.4287648 -4.4287639 -4.4287629 -4.4287438 -4.4287343 -4.4287395 -4.4287548 -4.4287624 -4.4287887 -4.4288135 -4.4288325 -4.4288445][-4.4288034 -4.4287658 -4.4287381 -4.4287529 -4.4287877 -4.4288044 -4.428792 -4.4287786 -4.4287734 -4.4287691 -4.4287634 -4.4287786 -4.4288034 -4.4288216 -4.4288278][-4.428731 -4.4286814 -4.4286675 -4.4287176 -4.4287858 -4.428822 -4.4288177 -4.4287963 -4.4287682 -4.4287415 -4.4287291 -4.428751 -4.4287939 -4.42882 -4.4288244][-4.4286537 -4.4285984 -4.4286008 -4.4286714 -4.4287548 -4.4288015 -4.4288058 -4.4287829 -4.4287434 -4.4286995 -4.4286809 -4.4287181 -4.4287825 -4.4288216 -4.4288292]]...]
INFO - root - 2017-12-08 07:30:18.230239: step 41710, loss = 2.28, batch loss = 2.23 (35.4 examples/sec; 0.226 sec/batch; 18h:13m:51s remains)
INFO - root - 2017-12-08 07:30:20.501436: step 41720, loss = 2.28, batch loss = 2.23 (35.1 examples/sec; 0.228 sec/batch; 18h:25m:45s remains)
INFO - root - 2017-12-08 07:30:22.768845: step 41730, loss = 2.28, batch loss = 2.23 (35.4 examples/sec; 0.226 sec/batch; 18h:14m:52s remains)
INFO - root - 2017-12-08 07:30:25.022625: step 41740, loss = 2.28, batch loss = 2.23 (35.0 examples/sec; 0.229 sec/batch; 18h:28m:19s remains)
INFO - root - 2017-12-08 07:30:27.290335: step 41750, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.222 sec/batch; 17h:54m:25s remains)
INFO - root - 2017-12-08 07:30:29.508974: step 41760, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 17h:51m:19s remains)
INFO - root - 2017-12-08 07:30:31.745154: step 41770, loss = 2.28, batch loss = 2.23 (35.6 examples/sec; 0.225 sec/batch; 18h:08m:25s remains)
INFO - root - 2017-12-08 07:30:33.972124: step 41780, loss = 2.28, batch loss = 2.23 (35.2 examples/sec; 0.227 sec/batch; 18h:22m:11s remains)
INFO - root - 2017-12-08 07:30:36.205782: step 41790, loss = 2.28, batch loss = 2.23 (34.9 examples/sec; 0.229 sec/batch; 18h:30m:25s remains)
INFO - root - 2017-12-08 07:30:38.458150: step 41800, loss = 2.28, batch loss = 2.23 (34.1 examples/sec; 0.234 sec/batch; 18h:55m:06s remains)
2017-12-08 07:30:38.741069: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.42881 -4.4288263 -4.4288564 -4.4288812 -4.4288964 -4.4288988 -4.4288945 -4.4288883 -4.4288836 -4.4288745 -4.4288611 -4.428844 -4.428813 -4.4287853 -4.4287691][-4.4287992 -4.428822 -4.4288492 -4.4288683 -4.428875 -4.4288616 -4.4288468 -4.4288306 -4.4288259 -4.4288254 -4.428823 -4.4288268 -4.4288139 -4.4287963 -4.42878][-4.428772 -4.4287934 -4.4288158 -4.428834 -4.4288359 -4.4288058 -4.4287724 -4.4287424 -4.4287362 -4.4287467 -4.4287672 -4.4287925 -4.4287977 -4.428793 -4.4287863][-4.4287477 -4.4287648 -4.4287772 -4.4287825 -4.4287629 -4.4287047 -4.428647 -4.4286084 -4.4286084 -4.428647 -4.4287086 -4.428761 -4.4287844 -4.4287963 -4.428803][-4.4287314 -4.4287314 -4.42872 -4.4286976 -4.4286456 -4.428555 -4.42847 -4.4284286 -4.4284415 -4.4285154 -4.4286189 -4.4287009 -4.428741 -4.428772 -4.4288011][-4.4287229 -4.4287052 -4.4286723 -4.4286151 -4.4285212 -4.4284015 -4.42829 -4.4282408 -4.4282694 -4.4283762 -4.4285154 -4.4286175 -4.4286752 -4.4287276 -4.4287825][-4.4287286 -4.4286971 -4.428648 -4.4285655 -4.4284554 -4.4283328 -4.4282074 -4.4281349 -4.42816 -4.4282746 -4.4284253 -4.4285345 -4.4286075 -4.4286742 -4.4287419][-4.4287214 -4.4286795 -4.4286275 -4.4285588 -4.4284873 -4.4284019 -4.4282918 -4.4282074 -4.4282069 -4.428288 -4.4284034 -4.4284964 -4.4285665 -4.4286256 -4.4286914][-4.4287081 -4.4286628 -4.4286327 -4.42861 -4.4285951 -4.4285536 -4.4284711 -4.4283981 -4.4283905 -4.4284306 -4.4284859 -4.4285321 -4.42857 -4.4285989 -4.4286518][-4.4286776 -4.428647 -4.4286509 -4.4286671 -4.4286871 -4.4286675 -4.4286118 -4.4285645 -4.4285631 -4.428576 -4.4285908 -4.4286 -4.4285994 -4.4285941 -4.4286265][-4.4286203 -4.4286118 -4.428648 -4.4286909 -4.4287238 -4.4287171 -4.4286838 -4.4286604 -4.4286642 -4.42866 -4.4286556 -4.4286442 -4.4286137 -4.4285836 -4.4286027][-4.4285946 -4.428607 -4.4286671 -4.4287224 -4.4287543 -4.42875 -4.4287276 -4.4287157 -4.4287238 -4.42872 -4.4287124 -4.428688 -4.4286447 -4.4286084 -4.428616][-4.4286294 -4.4286609 -4.4287286 -4.4287834 -4.4288087 -4.4288 -4.4287815 -4.4287791 -4.4287891 -4.4287877 -4.4287782 -4.4287505 -4.4287086 -4.4286723 -4.4286637][-4.4286928 -4.4287319 -4.428791 -4.4288316 -4.4288478 -4.4288406 -4.4288287 -4.4288287 -4.4288311 -4.4288254 -4.4288116 -4.4287896 -4.4287577 -4.4287286 -4.4287138][-4.4287643 -4.4287992 -4.4288373 -4.4288626 -4.4288745 -4.4288731 -4.42887 -4.4288712 -4.4288659 -4.428854 -4.4288387 -4.4288225 -4.4288015 -4.4287829 -4.4287682]]...]
INFO - root - 2017-12-08 07:30:40.974605: step 41810, loss = 2.28, batch loss = 2.23 (35.2 examples/sec; 0.227 sec/batch; 18h:20m:17s remains)
INFO - root - 2017-12-08 07:30:43.193402: step 41820, loss = 2.28, batch loss = 2.23 (34.4 examples/sec; 0.232 sec/batch; 18h:45m:12s remains)
INFO - root - 2017-12-08 07:30:45.432491: step 41830, loss = 2.28, batch loss = 2.23 (34.5 examples/sec; 0.232 sec/batch; 18h:43m:23s remains)
INFO - root - 2017-12-08 07:30:47.658095: step 41840, loss = 2.28, batch loss = 2.23 (35.4 examples/sec; 0.226 sec/batch; 18h:13m:17s remains)
INFO - root - 2017-12-08 07:30:49.888642: step 41850, loss = 2.28, batch loss = 2.23 (33.5 examples/sec; 0.239 sec/batch; 19h:15m:43s remains)
INFO - root - 2017-12-08 07:30:52.123805: step 41860, loss = 2.28, batch loss = 2.23 (35.7 examples/sec; 0.224 sec/batch; 18h:06m:28s remains)
INFO - root - 2017-12-08 07:30:54.367294: step 41870, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 17h:42m:58s remains)
INFO - root - 2017-12-08 07:30:56.610851: step 41880, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 17h:45m:01s remains)
INFO - root - 2017-12-08 07:30:58.836821: step 41890, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.222 sec/batch; 17h:54m:30s remains)
INFO - root - 2017-12-08 07:31:01.071627: step 41900, loss = 2.28, batch loss = 2.23 (36.6 examples/sec; 0.219 sec/batch; 17h:39m:48s remains)
2017-12-08 07:31:01.349449: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.428782 -4.4287424 -4.428709 -4.4286962 -4.428669 -4.4286528 -4.4286709 -4.4286647 -4.4286113 -4.4285655 -4.4285793 -4.4286523 -4.4287467 -4.4288111 -4.4288406][-4.4287548 -4.42872 -4.4286828 -4.4286494 -4.428606 -4.4285808 -4.4285889 -4.4285784 -4.4285445 -4.4285269 -4.4285569 -4.4286237 -4.4287033 -4.4287615 -4.4288034][-4.4287443 -4.4287205 -4.4286771 -4.4286289 -4.4285746 -4.4285383 -4.4285331 -4.4285226 -4.4285231 -4.4285388 -4.4285784 -4.4286208 -4.4286675 -4.4287105 -4.4287524][-4.4287515 -4.4287248 -4.4286723 -4.428606 -4.4285378 -4.4285007 -4.428494 -4.4284968 -4.4285259 -4.4285541 -4.4285946 -4.4286103 -4.4286265 -4.4286532 -4.4286933][-4.4287639 -4.4287271 -4.4286585 -4.4285655 -4.4284773 -4.4284415 -4.4284439 -4.4284639 -4.4285216 -4.4285531 -4.4285874 -4.428586 -4.42858 -4.4285941 -4.4286208][-4.42877 -4.4287281 -4.4286447 -4.428525 -4.4284196 -4.4283805 -4.42839 -4.4284291 -4.4285078 -4.4285364 -4.4285612 -4.4285541 -4.4285426 -4.4285536 -4.4285679][-4.4287663 -4.4287281 -4.4286523 -4.428534 -4.4284325 -4.4283948 -4.4283972 -4.4284391 -4.4285049 -4.4285188 -4.4285426 -4.4285469 -4.4285483 -4.428556 -4.428545][-4.4287677 -4.4287367 -4.4286809 -4.42859 -4.4285083 -4.4284816 -4.4284797 -4.428515 -4.4285536 -4.4285388 -4.4285536 -4.4285703 -4.428575 -4.4285665 -4.4285245][-4.4287891 -4.4287567 -4.4287124 -4.4286532 -4.4286046 -4.4285984 -4.4286036 -4.4286356 -4.428648 -4.4286089 -4.4286146 -4.4286275 -4.4286103 -4.4285693 -4.4285035][-4.4288306 -4.4287896 -4.4287457 -4.4287128 -4.4286871 -4.4286933 -4.4287162 -4.4287477 -4.428741 -4.4286995 -4.4286947 -4.4286838 -4.4286485 -4.4285827 -4.4285][-4.4288664 -4.4288311 -4.4288015 -4.4287796 -4.4287553 -4.4287591 -4.4287877 -4.4288082 -4.428792 -4.4287548 -4.4287391 -4.4287186 -4.4286895 -4.4286346 -4.4285517][-4.4288726 -4.42886 -4.4288507 -4.4288373 -4.4288144 -4.4288144 -4.4288378 -4.4288421 -4.42882 -4.4287915 -4.4287748 -4.4287515 -4.4287343 -4.4286923 -4.4286175][-4.4288583 -4.4288669 -4.4288774 -4.4288783 -4.4288673 -4.4288683 -4.4288778 -4.4288688 -4.4288478 -4.42883 -4.4288182 -4.42879 -4.4287663 -4.4287357 -4.42868][-4.4288273 -4.4288478 -4.4288692 -4.42888 -4.4288812 -4.4288917 -4.4289012 -4.4288926 -4.4288778 -4.4288645 -4.4288526 -4.4288268 -4.4287953 -4.4287672 -4.4287295][-4.4288006 -4.4288154 -4.4288359 -4.4288549 -4.4288697 -4.4288874 -4.4289007 -4.4289017 -4.4288964 -4.4288864 -4.4288721 -4.4288497 -4.428823 -4.4287987 -4.4287729]]...]
INFO - root - 2017-12-08 07:31:03.580055: step 41910, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.219 sec/batch; 17h:43m:01s remains)
INFO - root - 2017-12-08 07:31:05.809321: step 41920, loss = 2.28, batch loss = 2.23 (35.2 examples/sec; 0.228 sec/batch; 18h:22m:01s remains)
INFO - root - 2017-12-08 07:31:08.048308: step 41930, loss = 2.28, batch loss = 2.23 (34.9 examples/sec; 0.229 sec/batch; 18h:30m:26s remains)
INFO - root - 2017-12-08 07:31:10.311884: step 41940, loss = 2.28, batch loss = 2.23 (33.7 examples/sec; 0.237 sec/batch; 19h:08m:41s remains)
INFO - root - 2017-12-08 07:31:12.549220: step 41950, loss = 2.28, batch loss = 2.23 (36.6 examples/sec; 0.218 sec/batch; 17h:37m:49s remains)
INFO - root - 2017-12-08 07:31:14.817351: step 41960, loss = 2.28, batch loss = 2.23 (35.2 examples/sec; 0.227 sec/batch; 18h:21m:12s remains)
INFO - root - 2017-12-08 07:31:17.046240: step 41970, loss = 2.28, batch loss = 2.23 (34.8 examples/sec; 0.230 sec/batch; 18h:34m:21s remains)
INFO - root - 2017-12-08 07:31:19.276637: step 41980, loss = 2.28, batch loss = 2.23 (35.8 examples/sec; 0.224 sec/batch; 18h:02m:21s remains)
INFO - root - 2017-12-08 07:31:21.490889: step 41990, loss = 2.28, batch loss = 2.23 (34.5 examples/sec; 0.232 sec/batch; 18h:41m:28s remains)
INFO - root - 2017-12-08 07:31:23.750747: step 42000, loss = 2.28, batch loss = 2.23 (36.0 examples/sec; 0.222 sec/batch; 17h:54m:39s remains)
2017-12-08 07:31:24.023996: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4287715 -4.4287691 -4.4287686 -4.4287634 -4.4287729 -4.4287815 -4.4287605 -4.4287257 -4.428669 -4.4286056 -4.4285908 -4.4286289 -4.4286647 -4.4286861 -4.42872][-4.4287357 -4.4287276 -4.4287395 -4.4287586 -4.42879 -4.4288025 -4.4287748 -4.4287267 -4.4286594 -4.4286 -4.4285951 -4.4286251 -4.4286466 -4.4286542 -4.428688][-4.4287028 -4.42868 -4.4286914 -4.4287291 -4.4287815 -4.4288011 -4.4287729 -4.4287252 -4.4286709 -4.4286294 -4.4286218 -4.4286242 -4.4286156 -4.4286013 -4.4286261][-4.4286652 -4.4286208 -4.4286337 -4.4286776 -4.4287305 -4.4287457 -4.4287152 -4.4286804 -4.4286652 -4.4286575 -4.4286528 -4.4286213 -4.4285665 -4.4285254 -4.4285369][-4.4286103 -4.4285517 -4.4285655 -4.4286003 -4.4286346 -4.4286375 -4.4286032 -4.4285913 -4.4286284 -4.4286542 -4.4286566 -4.4286056 -4.4285154 -4.428453 -4.42845][-4.4285684 -4.4285054 -4.4285154 -4.4285245 -4.4285135 -4.4284744 -4.428422 -4.4284449 -4.428544 -4.42862 -4.4286342 -4.428576 -4.4284792 -4.4284129 -4.4283996][-4.4285545 -4.4284835 -4.4284573 -4.4284115 -4.428328 -4.4281917 -4.4280863 -4.428174 -4.4283829 -4.428535 -4.4285812 -4.4285412 -4.4284687 -4.4284267 -4.4284186][-4.4285412 -4.4284534 -4.4283686 -4.4282436 -4.4280648 -4.427793 -4.4276009 -4.4277892 -4.428154 -4.4284072 -4.4284978 -4.4284897 -4.4284577 -4.4284534 -4.42847][-4.4285297 -4.4284329 -4.4283051 -4.4281344 -4.4279165 -4.4275889 -4.4273553 -4.427608 -4.4280353 -4.4283152 -4.4284124 -4.4284153 -4.4284148 -4.4284515 -4.4285][-4.4285436 -4.4284611 -4.4283466 -4.4282117 -4.428062 -4.4278569 -4.427732 -4.427896 -4.4281826 -4.4283619 -4.4284172 -4.4284158 -4.4284267 -4.4284797 -4.4285355][-4.4285784 -4.4285278 -4.4284606 -4.4283834 -4.4283075 -4.4282155 -4.4281669 -4.4282427 -4.4283891 -4.4284754 -4.4284954 -4.4284892 -4.4284964 -4.4285307 -4.4285707][-4.4286466 -4.4286189 -4.42859 -4.4285588 -4.4285192 -4.4284768 -4.4284577 -4.4284868 -4.42855 -4.4285846 -4.4285922 -4.4285879 -4.4285822 -4.4285903 -4.4286108][-4.4287405 -4.4287195 -4.4287076 -4.4286966 -4.4286718 -4.4286475 -4.4286327 -4.4286413 -4.4286704 -4.428688 -4.428689 -4.4286804 -4.4286666 -4.4286633 -4.42868][-4.4288435 -4.4288306 -4.4288244 -4.4288211 -4.4288087 -4.4287944 -4.428781 -4.4287825 -4.4287958 -4.428803 -4.4287992 -4.4287868 -4.428772 -4.4287658 -4.4287767][-4.4289212 -4.4289193 -4.4289193 -4.4289203 -4.4289145 -4.4289069 -4.4289 -4.4289007 -4.4289088 -4.4289107 -4.4289074 -4.4288983 -4.4288836 -4.4288721 -4.428874]]...]
INFO - root - 2017-12-08 07:31:26.247839: step 42010, loss = 2.28, batch loss = 2.23 (35.7 examples/sec; 0.224 sec/batch; 18h:04m:29s remains)
INFO - root - 2017-12-08 07:31:28.500792: step 42020, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.221 sec/batch; 17h:47m:37s remains)
INFO - root - 2017-12-08 07:31:30.727993: step 42030, loss = 2.28, batch loss = 2.23 (35.8 examples/sec; 0.224 sec/batch; 18h:02m:58s remains)
INFO - root - 2017-12-08 07:31:32.952181: step 42040, loss = 2.28, batch loss = 2.23 (37.5 examples/sec; 0.214 sec/batch; 17h:13m:59s remains)
INFO - root - 2017-12-08 07:31:35.190515: step 42050, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.221 sec/batch; 17h:47m:25s remains)
INFO - root - 2017-12-08 07:31:37.415548: step 42060, loss = 2.28, batch loss = 2.23 (35.3 examples/sec; 0.227 sec/batch; 18h:16m:37s remains)
INFO - root - 2017-12-08 07:31:39.648522: step 42070, loss = 2.28, batch loss = 2.23 (37.0 examples/sec; 0.216 sec/batch; 17h:27m:33s remains)
INFO - root - 2017-12-08 07:31:41.872670: step 42080, loss = 2.28, batch loss = 2.23 (36.7 examples/sec; 0.218 sec/batch; 17h:35m:07s remains)
INFO - root - 2017-12-08 07:31:44.098353: step 42090, loss = 2.28, batch loss = 2.23 (37.4 examples/sec; 0.214 sec/batch; 17h:16m:10s remains)
INFO - root - 2017-12-08 07:31:46.357318: step 42100, loss = 2.28, batch loss = 2.23 (34.6 examples/sec; 0.231 sec/batch; 18h:38m:29s remains)
2017-12-08 07:31:46.682509: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4287748 -4.4288034 -4.4288235 -4.4288278 -4.4287958 -4.4287477 -4.4287071 -4.428658 -4.4286122 -4.4285827 -4.4285951 -4.4286237 -4.4286394 -4.42864 -4.4286346][-4.4287477 -4.4287767 -4.4287939 -4.4287972 -4.4287677 -4.4287257 -4.4286861 -4.4286346 -4.4285903 -4.428566 -4.4285841 -4.4286108 -4.4286222 -4.428618 -4.4285994][-4.4286871 -4.4287143 -4.4287281 -4.428731 -4.4287066 -4.4286752 -4.4286447 -4.4286056 -4.4285793 -4.4285684 -4.4285946 -4.4286213 -4.4286356 -4.4286246 -4.4285893][-4.4286122 -4.4286304 -4.4286394 -4.428637 -4.4286265 -4.4286127 -4.4285955 -4.4285793 -4.428576 -4.4285874 -4.4286246 -4.4286513 -4.4286671 -4.4286556 -4.428616][-4.4285617 -4.4285603 -4.4285445 -4.4285188 -4.428503 -4.4285016 -4.4284954 -4.4284863 -4.428514 -4.4285731 -4.4286332 -4.4286633 -4.4286728 -4.4286618 -4.4286337][-4.4285445 -4.4285412 -4.4285059 -4.428443 -4.4283848 -4.4283557 -4.4283247 -4.4282885 -4.4283586 -4.428494 -4.4285889 -4.4286218 -4.428616 -4.428607 -4.4286046][-4.428556 -4.4285679 -4.4285421 -4.4284635 -4.4283652 -4.4282947 -4.4282155 -4.4281282 -4.42823 -4.4284229 -4.428534 -4.4285593 -4.4285307 -4.4285212 -4.4285469][-4.4285669 -4.4285994 -4.4286008 -4.4285455 -4.4284625 -4.4283967 -4.42833 -4.4282632 -4.4283371 -4.4284759 -4.4285493 -4.4285464 -4.4284954 -4.4284821 -4.4285207][-4.4285746 -4.4286103 -4.4286337 -4.4286184 -4.4285822 -4.42855 -4.4285173 -4.4284883 -4.4285192 -4.4285879 -4.4286218 -4.4286008 -4.42854 -4.4285173 -4.428546][-4.4286146 -4.4286489 -4.4286809 -4.4286871 -4.4286761 -4.4286609 -4.4286432 -4.4286294 -4.4286337 -4.4286666 -4.4286819 -4.42866 -4.4286056 -4.4285769 -4.4285927][-4.4286561 -4.4286942 -4.4287295 -4.4287448 -4.4287424 -4.4287271 -4.4287167 -4.4287124 -4.4287109 -4.4287291 -4.4287338 -4.4287095 -4.4286647 -4.4286361 -4.4286442][-4.4286695 -4.4287128 -4.4287539 -4.4287786 -4.4287806 -4.4287672 -4.4287634 -4.42877 -4.4287739 -4.42878 -4.4287763 -4.4287519 -4.4287143 -4.4286895 -4.4286933][-4.428689 -4.4287329 -4.4287763 -4.4287977 -4.4287944 -4.4287815 -4.4287815 -4.4287958 -4.4288087 -4.4288111 -4.4288073 -4.4287982 -4.4287767 -4.4287562 -4.4287543][-4.4287076 -4.4287505 -4.428793 -4.4288135 -4.4288111 -4.4288025 -4.4288068 -4.428822 -4.4288368 -4.4288373 -4.4288359 -4.4288383 -4.4288306 -4.4288149 -4.4288096][-4.4287443 -4.4287715 -4.4287996 -4.4288173 -4.428823 -4.4288263 -4.4288349 -4.4288507 -4.4288607 -4.428854 -4.42885 -4.4288549 -4.42885 -4.4288392 -4.4288325]]...]
INFO - root - 2017-12-08 07:31:48.923152: step 42110, loss = 2.28, batch loss = 2.23 (37.4 examples/sec; 0.214 sec/batch; 17h:16m:14s remains)
INFO - root - 2017-12-08 07:31:51.162578: step 42120, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.225 sec/batch; 18h:09m:27s remains)
INFO - root - 2017-12-08 07:31:53.394594: step 42130, loss = 2.28, batch loss = 2.23 (36.8 examples/sec; 0.217 sec/batch; 17h:32m:34s remains)
INFO - root - 2017-12-08 07:31:55.611944: step 42140, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 17h:44m:55s remains)
INFO - root - 2017-12-08 07:31:57.839299: step 42150, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 17h:59m:03s remains)
INFO - root - 2017-12-08 07:32:00.070183: step 42160, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.220 sec/batch; 17h:45m:51s remains)
INFO - root - 2017-12-08 07:32:02.307606: step 42170, loss = 2.28, batch loss = 2.23 (34.6 examples/sec; 0.231 sec/batch; 18h:38m:06s remains)
INFO - root - 2017-12-08 07:32:04.553708: step 42180, loss = 2.28, batch loss = 2.23 (35.4 examples/sec; 0.226 sec/batch; 18h:14m:49s remains)
INFO - root - 2017-12-08 07:32:06.833046: step 42190, loss = 2.28, batch loss = 2.23 (35.4 examples/sec; 0.226 sec/batch; 18h:13m:30s remains)
INFO - root - 2017-12-08 07:32:09.115994: step 42200, loss = 2.28, batch loss = 2.23 (34.2 examples/sec; 0.234 sec/batch; 18h:51m:02s remains)
2017-12-08 07:32:09.429297: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289608 -4.4289565 -4.4289532 -4.4289527 -4.4289517 -4.4289522 -4.4289513 -4.4289479 -4.4289484 -4.4289513 -4.4289565 -4.4289632 -4.4289689 -4.4289722 -4.4289761][-4.4289322 -4.4289265 -4.4289212 -4.42892 -4.4289174 -4.4289155 -4.4289136 -4.4289088 -4.4289126 -4.4289207 -4.4289322 -4.4289441 -4.4289513 -4.4289536 -4.4289589][-4.428894 -4.42889 -4.4288893 -4.4288921 -4.4288907 -4.4288845 -4.4288745 -4.4288626 -4.4288731 -4.4288936 -4.428915 -4.4289322 -4.4289403 -4.4289374 -4.4289365][-4.4288564 -4.428854 -4.42886 -4.4288707 -4.4288726 -4.4288607 -4.4288383 -4.4288158 -4.4288321 -4.4288621 -4.4288917 -4.4289103 -4.4289136 -4.4289031 -4.428895][-4.4288154 -4.4288034 -4.4288034 -4.4288163 -4.4288225 -4.4288087 -4.4287744 -4.4287453 -4.428771 -4.4288139 -4.4288516 -4.4288745 -4.428875 -4.4288564 -4.4288435][-4.4287734 -4.4287519 -4.4287467 -4.4287534 -4.4287558 -4.4287362 -4.4286933 -4.4286566 -4.4286957 -4.4287543 -4.4288049 -4.4288373 -4.4288392 -4.4288135 -4.4287972][-4.4287319 -4.4287019 -4.4286823 -4.4286695 -4.4286523 -4.4286184 -4.4285655 -4.42852 -4.4285741 -4.4286566 -4.4287267 -4.428771 -4.4287777 -4.4287472 -4.4287305][-4.4286766 -4.4286308 -4.4285951 -4.4285517 -4.4284987 -4.4284449 -4.4283829 -4.4283376 -4.4284191 -4.428535 -4.4286323 -4.4286847 -4.4286947 -4.4286623 -4.42865][-4.4286609 -4.4286079 -4.4285531 -4.4284673 -4.4283576 -4.42826 -4.4281931 -4.4281654 -4.4282861 -4.4284368 -4.4285569 -4.4286218 -4.4286375 -4.4286127 -4.4286103][-4.4286938 -4.428659 -4.42862 -4.4285417 -4.4284244 -4.4283133 -4.4282355 -4.4281974 -4.4283071 -4.4284387 -4.4285412 -4.4286017 -4.42862 -4.4286036 -4.4286132][-4.42871 -4.4286919 -4.4286776 -4.4286337 -4.4285555 -4.4284768 -4.4284153 -4.4283719 -4.4284434 -4.4285293 -4.4285984 -4.4286451 -4.4286609 -4.428648 -4.4286571][-4.4287038 -4.4286861 -4.428679 -4.4286561 -4.4286075 -4.4285536 -4.4285092 -4.4284768 -4.4285283 -4.4285908 -4.4286432 -4.4286866 -4.428709 -4.4287038 -4.4287128][-4.42872 -4.428699 -4.4286928 -4.4286785 -4.4286447 -4.4286027 -4.4285641 -4.4285378 -4.4285741 -4.4286237 -4.428669 -4.4287105 -4.4287376 -4.4287472 -4.4287658][-4.4287853 -4.4287705 -4.4287705 -4.4287663 -4.4287419 -4.4287038 -4.4286642 -4.4286375 -4.4286594 -4.428699 -4.4287424 -4.4287791 -4.4288034 -4.4288187 -4.4288378][-4.42887 -4.4288697 -4.4288807 -4.428885 -4.4288716 -4.428843 -4.428812 -4.428792 -4.4288025 -4.4288292 -4.428863 -4.4288912 -4.4289103 -4.4289193 -4.428926]]...]
INFO - root - 2017-12-08 07:32:11.765635: step 42210, loss = 2.28, batch loss = 2.23 (27.7 examples/sec; 0.289 sec/batch; 23h:17m:49s remains)
INFO - root - 2017-12-08 07:32:14.014505: step 42220, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 17h:38m:57s remains)
INFO - root - 2017-12-08 07:32:16.253539: step 42230, loss = 2.28, batch loss = 2.23 (35.4 examples/sec; 0.226 sec/batch; 18h:14m:10s remains)
INFO - root - 2017-12-08 07:32:18.475135: step 42240, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.220 sec/batch; 17h:46m:08s remains)
INFO - root - 2017-12-08 07:32:20.715955: step 42250, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.222 sec/batch; 17h:52m:40s remains)
INFO - root - 2017-12-08 07:32:22.955016: step 42260, loss = 2.28, batch loss = 2.23 (35.1 examples/sec; 0.228 sec/batch; 18h:21m:00s remains)
INFO - root - 2017-12-08 07:32:25.222797: step 42270, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.221 sec/batch; 17h:50m:46s remains)
INFO - root - 2017-12-08 07:32:27.445310: step 42280, loss = 2.28, batch loss = 2.23 (36.0 examples/sec; 0.222 sec/batch; 17h:55m:54s remains)
INFO - root - 2017-12-08 07:32:29.664598: step 42290, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.220 sec/batch; 17h:45m:58s remains)
INFO - root - 2017-12-08 07:32:31.935504: step 42300, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.222 sec/batch; 17h:51m:50s remains)
2017-12-08 07:32:32.227029: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288521 -4.4288306 -4.4288249 -4.4288406 -4.4288588 -4.4288692 -4.4288726 -4.428875 -4.4288721 -4.4288716 -4.4288816 -4.4289055 -4.4289217 -4.4289265 -4.4289327][-4.4288163 -4.4287953 -4.4287858 -4.4287987 -4.4288168 -4.428823 -4.4288163 -4.4288125 -4.4288073 -4.4288073 -4.4288192 -4.4288549 -4.4288826 -4.42889 -4.4288969][-4.428802 -4.4287977 -4.428793 -4.4287996 -4.428813 -4.4288077 -4.428782 -4.4287696 -4.4287643 -4.4287705 -4.4287868 -4.4288282 -4.4288626 -4.4288712 -4.4288721][-4.4287839 -4.4287958 -4.4287968 -4.4287972 -4.4288068 -4.4287925 -4.4287491 -4.4287329 -4.4287405 -4.4287682 -4.4287968 -4.4288411 -4.4288721 -4.4288735 -4.4288692][-4.4287367 -4.4287524 -4.4287486 -4.4287453 -4.4287481 -4.4287343 -4.428688 -4.4286742 -4.4286985 -4.4287453 -4.4287925 -4.4288454 -4.4288778 -4.4288788 -4.4288764][-4.4286847 -4.4286976 -4.4286923 -4.4286871 -4.4286861 -4.4286695 -4.4286308 -4.4286256 -4.4286556 -4.4287076 -4.4287658 -4.4288282 -4.4288654 -4.4288745 -4.4288816][-4.4286404 -4.4286466 -4.4286475 -4.4286432 -4.4286337 -4.4286046 -4.4285774 -4.4285879 -4.4286294 -4.4286885 -4.4287529 -4.42882 -4.4288592 -4.4288735 -4.4288831][-4.4286413 -4.42863 -4.428616 -4.428586 -4.4285388 -4.4284749 -4.4284468 -4.4284768 -4.428544 -4.4286332 -4.4287167 -4.4287953 -4.4288464 -4.4288664 -4.4288735][-4.4286785 -4.4286332 -4.4285951 -4.428534 -4.4284449 -4.4283442 -4.4283042 -4.4283376 -4.4284263 -4.4285522 -4.4286604 -4.4287534 -4.4288168 -4.4288449 -4.4288497][-4.4287062 -4.4286518 -4.4286079 -4.4285469 -4.4284654 -4.4283819 -4.4283462 -4.4283571 -4.4284205 -4.4285398 -4.4286427 -4.4287276 -4.4287896 -4.4288168 -4.4288206][-4.4286871 -4.4286542 -4.4286261 -4.4285922 -4.4285574 -4.4285297 -4.4285207 -4.4285159 -4.4285407 -4.4286151 -4.4286842 -4.4287438 -4.4287934 -4.4288125 -4.4288135][-4.4286456 -4.4286385 -4.4286351 -4.4286332 -4.4286408 -4.42866 -4.4286752 -4.4286642 -4.4286618 -4.4286995 -4.4287367 -4.428772 -4.4288092 -4.4288235 -4.4288235][-4.4286451 -4.428658 -4.4286733 -4.4286871 -4.4287119 -4.4287491 -4.4287715 -4.4287605 -4.4287481 -4.4287677 -4.4287872 -4.4288039 -4.4288273 -4.4288354 -4.4288335][-4.4287405 -4.4287477 -4.4287596 -4.4287705 -4.4287891 -4.4288206 -4.4288354 -4.4288244 -4.4288163 -4.4288325 -4.4288478 -4.4288573 -4.4288712 -4.4288731 -4.4288678][-4.42886 -4.428854 -4.4288526 -4.4288478 -4.4288483 -4.4288611 -4.4288635 -4.428854 -4.4288535 -4.428874 -4.4288955 -4.4289074 -4.4289174 -4.4289193 -4.4289131]]...]
INFO - root - 2017-12-08 07:32:34.483303: step 42310, loss = 2.28, batch loss = 2.23 (34.8 examples/sec; 0.230 sec/batch; 18h:33m:10s remains)
INFO - root - 2017-12-08 07:32:36.721005: step 42320, loss = 2.28, batch loss = 2.23 (35.3 examples/sec; 0.227 sec/batch; 18h:16m:24s remains)
INFO - root - 2017-12-08 07:32:39.019207: step 42330, loss = 2.28, batch loss = 2.23 (36.6 examples/sec; 0.219 sec/batch; 17h:37m:57s remains)
INFO - root - 2017-12-08 07:32:41.254469: step 42340, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 17h:57m:33s remains)
INFO - root - 2017-12-08 07:32:43.469228: step 42350, loss = 2.28, batch loss = 2.23 (36.7 examples/sec; 0.218 sec/batch; 17h:33m:25s remains)
INFO - root - 2017-12-08 07:32:45.723504: step 42360, loss = 2.28, batch loss = 2.23 (35.6 examples/sec; 0.225 sec/batch; 18h:07m:16s remains)
INFO - root - 2017-12-08 07:32:47.998273: step 42370, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 17h:48m:54s remains)
INFO - root - 2017-12-08 07:32:50.240923: step 42380, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.220 sec/batch; 17h:44m:47s remains)
INFO - root - 2017-12-08 07:32:52.463004: step 42390, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 17h:48m:28s remains)
INFO - root - 2017-12-08 07:32:54.682825: step 42400, loss = 2.28, batch loss = 2.23 (36.6 examples/sec; 0.219 sec/batch; 17h:36m:52s remains)
2017-12-08 07:32:54.978626: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288034 -4.4288268 -4.4288669 -4.4288878 -4.428885 -4.4288578 -4.4288363 -4.4288077 -4.4288054 -4.4288211 -4.4288096 -4.4287815 -4.4287548 -4.4287219 -4.4287133][-4.4288311 -4.4288526 -4.4288898 -4.4289 -4.4288831 -4.4288368 -4.4288116 -4.4287882 -4.4287977 -4.4288163 -4.428803 -4.4287663 -4.4287395 -4.4287024 -4.4286842][-4.4288263 -4.4288244 -4.4288373 -4.4288297 -4.4287992 -4.4287481 -4.4287243 -4.428709 -4.4287367 -4.4287786 -4.4287782 -4.4287562 -4.4287505 -4.4287229 -4.4287019][-4.4287863 -4.42875 -4.428741 -4.4287314 -4.4287028 -4.4286661 -4.4286423 -4.4286351 -4.4286942 -4.4287677 -4.4287934 -4.4287968 -4.4288 -4.4287729 -4.4287429][-4.428719 -4.4286637 -4.4286485 -4.4286489 -4.4286289 -4.4286032 -4.4285765 -4.4285522 -4.4286222 -4.4287291 -4.4287748 -4.4287896 -4.4287891 -4.4287648 -4.428741][-4.4286318 -4.4285822 -4.4285731 -4.4285789 -4.4285703 -4.4285393 -4.4284759 -4.4283772 -4.4284124 -4.4285522 -4.4286256 -4.4286575 -4.4286609 -4.4286489 -4.42866][-4.428514 -4.4284773 -4.4284763 -4.4284887 -4.4284792 -4.4284267 -4.4282956 -4.4280753 -4.4280477 -4.4282403 -4.4283781 -4.4284568 -4.4284868 -4.4284925 -4.4285383][-4.4283738 -4.4283471 -4.4283438 -4.4283438 -4.4283161 -4.4282455 -4.4280806 -4.4277892 -4.4277086 -4.4279714 -4.4281797 -4.4283037 -4.4283743 -4.4283943 -4.4284487][-4.4283347 -4.4283223 -4.4283152 -4.4283018 -4.4282646 -4.4282079 -4.4280953 -4.4278946 -4.4278417 -4.4280481 -4.4282193 -4.428329 -4.4284039 -4.4284267 -4.4284625][-4.4284105 -4.4284182 -4.4284196 -4.4284081 -4.4283805 -4.4283519 -4.428309 -4.4282241 -4.4282207 -4.4283342 -4.4284172 -4.4284754 -4.4285278 -4.4285426 -4.4285555][-4.4285612 -4.4285812 -4.4285932 -4.42859 -4.4285812 -4.4285731 -4.4285679 -4.4285331 -4.428544 -4.4286089 -4.4286523 -4.428678 -4.4287057 -4.4287043 -4.4286919][-4.42874 -4.4287581 -4.4287729 -4.4287791 -4.4287882 -4.4287939 -4.4287915 -4.428771 -4.4287829 -4.4288244 -4.4288516 -4.4288692 -4.4288826 -4.428865 -4.4288278][-4.4288936 -4.428905 -4.428916 -4.4289222 -4.4289341 -4.4289427 -4.4289312 -4.42891 -4.4289141 -4.4289384 -4.4289532 -4.4289656 -4.4289789 -4.4289608 -4.4289165][-4.4289665 -4.4289722 -4.4289765 -4.4289789 -4.4289832 -4.4289856 -4.4289746 -4.4289565 -4.4289551 -4.428967 -4.4289789 -4.4289913 -4.4290061 -4.4290013 -4.4289689][-4.4289789 -4.4289846 -4.4289837 -4.4289818 -4.42898 -4.4289765 -4.428968 -4.4289565 -4.4289532 -4.4289575 -4.4289618 -4.4289718 -4.428987 -4.4289942 -4.4289827]]...]
INFO - root - 2017-12-08 07:32:57.215789: step 42410, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 17h:48m:13s remains)
INFO - root - 2017-12-08 07:32:59.442549: step 42420, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 17h:43m:58s remains)
INFO - root - 2017-12-08 07:33:01.664547: step 42430, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.222 sec/batch; 17h:50m:57s remains)
INFO - root - 2017-12-08 07:33:03.890360: step 42440, loss = 2.28, batch loss = 2.23 (36.7 examples/sec; 0.218 sec/batch; 17h:32m:24s remains)
INFO - root - 2017-12-08 07:33:06.134621: step 42450, loss = 2.28, batch loss = 2.23 (35.1 examples/sec; 0.228 sec/batch; 18h:20m:20s remains)
INFO - root - 2017-12-08 07:33:08.388840: step 42460, loss = 2.28, batch loss = 2.23 (35.4 examples/sec; 0.226 sec/batch; 18h:13m:47s remains)
INFO - root - 2017-12-08 07:33:10.636046: step 42470, loss = 2.28, batch loss = 2.23 (35.6 examples/sec; 0.225 sec/batch; 18h:07m:26s remains)
INFO - root - 2017-12-08 07:33:12.865308: step 42480, loss = 2.28, batch loss = 2.23 (36.0 examples/sec; 0.222 sec/batch; 17h:54m:47s remains)
INFO - root - 2017-12-08 07:33:15.102087: step 42490, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 17h:57m:54s remains)
INFO - root - 2017-12-08 07:33:17.331049: step 42500, loss = 2.28, batch loss = 2.23 (35.2 examples/sec; 0.227 sec/batch; 18h:19m:19s remains)
2017-12-08 07:33:17.646670: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4286213 -4.428616 -4.42862 -4.4286065 -4.4285779 -4.4285636 -4.4285803 -4.4286051 -4.428618 -4.4286108 -4.4286041 -4.4285951 -4.42859 -4.4285994 -4.4286289][-4.4286332 -4.428628 -4.4286313 -4.4286194 -4.4285879 -4.4285707 -4.4285908 -4.4286232 -4.428648 -4.4286408 -4.4286227 -4.4286003 -4.4285812 -4.4285827 -4.428607][-4.4286733 -4.4286532 -4.4286466 -4.4286394 -4.4286203 -4.4286141 -4.4286394 -4.4286776 -4.4287028 -4.4286909 -4.428669 -4.4286418 -4.4286122 -4.428597 -4.4286017][-4.4286923 -4.4286513 -4.42863 -4.4286261 -4.4286284 -4.4286475 -4.4286876 -4.4287333 -4.4287572 -4.4287367 -4.4287043 -4.4286728 -4.4286413 -4.4286146 -4.4285979][-4.4286761 -4.42863 -4.4286051 -4.4286084 -4.4286246 -4.4286623 -4.4287181 -4.4287663 -4.4287863 -4.4287596 -4.4287114 -4.428669 -4.4286385 -4.4286089 -4.4285822][-4.42864 -4.4285955 -4.4285741 -4.4285827 -4.4286008 -4.4286346 -4.4286857 -4.4287238 -4.4287324 -4.4286971 -4.4286332 -4.428587 -4.4285727 -4.4285674 -4.4285574][-4.4286265 -4.4285779 -4.4285531 -4.4285612 -4.4285769 -4.4285994 -4.4286313 -4.4286504 -4.4286427 -4.4286022 -4.428544 -4.4285135 -4.4285278 -4.4285531 -4.4285645][-4.4286671 -4.4286246 -4.4286036 -4.4286084 -4.428617 -4.4286227 -4.4286265 -4.4286184 -4.4285913 -4.4285531 -4.4285173 -4.4285126 -4.428544 -4.4285755 -4.4285889][-4.4287381 -4.4287095 -4.4287009 -4.4287043 -4.4287033 -4.4286947 -4.4286795 -4.4286537 -4.4286156 -4.4285784 -4.4285588 -4.4285703 -4.4286046 -4.42863 -4.4286356][-4.4288321 -4.4288158 -4.4288116 -4.4288082 -4.4287953 -4.4287748 -4.4287515 -4.4287224 -4.4286938 -4.4286723 -4.4286661 -4.428679 -4.4287014 -4.4287138 -4.4287119][-4.428915 -4.4289036 -4.4288969 -4.4288893 -4.4288726 -4.4288468 -4.4288216 -4.4287996 -4.4287882 -4.4287877 -4.4287925 -4.4287987 -4.4288 -4.428792 -4.4287791][-4.4289479 -4.4289393 -4.4289351 -4.4289317 -4.4289188 -4.4288969 -4.4288769 -4.4288645 -4.428865 -4.428875 -4.4288836 -4.4288816 -4.428864 -4.4288392 -4.4288192][-4.4289322 -4.4289255 -4.4289255 -4.42893 -4.4289265 -4.4289155 -4.4289021 -4.4288936 -4.428895 -4.4289045 -4.4289122 -4.4289107 -4.4288907 -4.4288626 -4.4288421][-4.4288979 -4.4288917 -4.4288945 -4.4289026 -4.4289079 -4.4289055 -4.428896 -4.4288864 -4.4288826 -4.4288869 -4.4288926 -4.4288945 -4.428884 -4.4288654 -4.4288521][-4.4288721 -4.4288683 -4.4288721 -4.4288793 -4.4288855 -4.4288855 -4.4288769 -4.4288669 -4.4288616 -4.428864 -4.42887 -4.4288754 -4.428875 -4.42887 -4.4288659]]...]
INFO - root - 2017-12-08 07:33:19.853641: step 42510, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 17h:43m:29s remains)
INFO - root - 2017-12-08 07:33:22.096187: step 42520, loss = 2.28, batch loss = 2.23 (36.6 examples/sec; 0.218 sec/batch; 17h:35m:09s remains)
INFO - root - 2017-12-08 07:33:24.361395: step 42530, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 17h:48m:20s remains)
INFO - root - 2017-12-08 07:33:26.600160: step 42540, loss = 2.28, batch loss = 2.23 (35.7 examples/sec; 0.224 sec/batch; 18h:04m:05s remains)
INFO - root - 2017-12-08 07:33:28.844749: step 42550, loss = 2.28, batch loss = 2.23 (36.6 examples/sec; 0.219 sec/batch; 17h:37m:21s remains)
INFO - root - 2017-12-08 07:33:31.089985: step 42560, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.220 sec/batch; 17h:45m:30s remains)
INFO - root - 2017-12-08 07:33:33.307216: step 42570, loss = 2.28, batch loss = 2.23 (36.8 examples/sec; 0.218 sec/batch; 17h:31m:15s remains)
INFO - root - 2017-12-08 07:33:35.535209: step 42580, loss = 2.28, batch loss = 2.23 (36.6 examples/sec; 0.218 sec/batch; 17h:34m:46s remains)
INFO - root - 2017-12-08 07:33:37.800918: step 42590, loss = 2.28, batch loss = 2.23 (35.1 examples/sec; 0.228 sec/batch; 18h:20m:52s remains)
INFO - root - 2017-12-08 07:33:40.068298: step 42600, loss = 2.28, batch loss = 2.23 (34.5 examples/sec; 0.232 sec/batch; 18h:39m:31s remains)
2017-12-08 07:33:40.348230: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.428956 -4.4289231 -4.4289012 -4.4288955 -4.4288969 -4.4288931 -4.4288898 -4.4288878 -4.4288878 -4.4288807 -4.4288778 -4.428874 -4.4288726 -4.4288788 -4.4288816][-4.4289255 -4.428884 -4.4288554 -4.4288445 -4.4288397 -4.4288306 -4.428823 -4.4288173 -4.4288111 -4.4287968 -4.4287868 -4.4287791 -4.4287796 -4.4287944 -4.4288077][-4.4288974 -4.4288478 -4.4288077 -4.428791 -4.4287848 -4.4287772 -4.4287672 -4.4287539 -4.4287424 -4.4287224 -4.428709 -4.4287062 -4.4287152 -4.4287405 -4.4287658][-4.4288774 -4.4288182 -4.4287672 -4.4287472 -4.4287481 -4.4287481 -4.4287391 -4.4287233 -4.428709 -4.4286842 -4.428678 -4.4286895 -4.4287033 -4.4287305 -4.4287624][-4.4288669 -4.4287958 -4.4287367 -4.4287171 -4.4287257 -4.428731 -4.4287262 -4.4287138 -4.4287 -4.4286747 -4.4286771 -4.4286962 -4.4287076 -4.4287238 -4.4287548][-4.4288635 -4.4287863 -4.4287252 -4.4287114 -4.428721 -4.4287252 -4.4287195 -4.4287062 -4.4286933 -4.4286752 -4.4286857 -4.4287076 -4.4287214 -4.4287319 -4.4287634][-4.4288697 -4.4287944 -4.4287372 -4.4287219 -4.4287262 -4.4287243 -4.4287171 -4.4287047 -4.4286962 -4.4286938 -4.4287181 -4.4287486 -4.4287677 -4.4287829 -4.4288154][-4.4288821 -4.4288158 -4.4287677 -4.4287491 -4.4287457 -4.4287362 -4.4287319 -4.4287224 -4.4287133 -4.4287109 -4.4287372 -4.4287705 -4.4287972 -4.4288187 -4.4288516][-4.4288983 -4.4288406 -4.4287992 -4.4287772 -4.4287643 -4.428751 -4.4287543 -4.4287543 -4.428741 -4.4287248 -4.4287453 -4.4287796 -4.4288158 -4.4288383 -4.4288659][-4.4289107 -4.4288583 -4.4288158 -4.4287872 -4.4287653 -4.4287562 -4.4287753 -4.4287782 -4.4287648 -4.4287429 -4.4287548 -4.4287848 -4.4288297 -4.4288573 -4.4288778][-4.4289207 -4.4288678 -4.4288177 -4.4287834 -4.4287615 -4.4287624 -4.4287887 -4.4287925 -4.428782 -4.4287605 -4.4287677 -4.4287934 -4.428843 -4.4288726 -4.4288845][-4.4289403 -4.4288912 -4.4288363 -4.4287977 -4.4287782 -4.4287872 -4.4288163 -4.428822 -4.428813 -4.4287939 -4.4287944 -4.428823 -4.4288712 -4.4288964 -4.428894][-4.4289675 -4.4289265 -4.4288721 -4.4288311 -4.4288149 -4.4288268 -4.4288535 -4.428865 -4.4288588 -4.4288349 -4.4288254 -4.4288526 -4.4288936 -4.4289083 -4.4288969][-4.4289885 -4.4289551 -4.4289036 -4.4288621 -4.4288473 -4.4288592 -4.4288836 -4.4288974 -4.4288931 -4.4288673 -4.4288526 -4.4288716 -4.4289017 -4.4289069 -4.4288936][-4.4290047 -4.4289794 -4.4289365 -4.4289002 -4.4288878 -4.4288983 -4.4289155 -4.4289231 -4.4289165 -4.4288969 -4.4288869 -4.4289007 -4.4289212 -4.4289246 -4.428915]]...]
INFO - root - 2017-12-08 07:33:42.544878: step 42610, loss = 2.28, batch loss = 2.23 (37.2 examples/sec; 0.215 sec/batch; 17h:19m:42s remains)
INFO - root - 2017-12-08 07:33:44.815706: step 42620, loss = 2.28, batch loss = 2.23 (35.3 examples/sec; 0.227 sec/batch; 18h:16m:08s remains)
INFO - root - 2017-12-08 07:33:47.035224: step 42630, loss = 2.28, batch loss = 2.23 (34.1 examples/sec; 0.234 sec/batch; 18h:52m:02s remains)
INFO - root - 2017-12-08 07:33:49.257836: step 42640, loss = 2.28, batch loss = 2.23 (37.5 examples/sec; 0.213 sec/batch; 17h:10m:10s remains)
INFO - root - 2017-12-08 07:33:51.542911: step 42650, loss = 2.28, batch loss = 2.23 (35.2 examples/sec; 0.227 sec/batch; 18h:17m:41s remains)
INFO - root - 2017-12-08 07:33:53.798605: step 42660, loss = 2.28, batch loss = 2.23 (35.1 examples/sec; 0.228 sec/batch; 18h:22m:33s remains)
INFO - root - 2017-12-08 07:33:56.031327: step 42670, loss = 2.28, batch loss = 2.23 (35.4 examples/sec; 0.226 sec/batch; 18h:10m:51s remains)
INFO - root - 2017-12-08 07:33:58.264281: step 42680, loss = 2.28, batch loss = 2.23 (35.8 examples/sec; 0.223 sec/batch; 17h:58m:33s remains)
INFO - root - 2017-12-08 07:34:00.495835: step 42690, loss = 2.28, batch loss = 2.23 (34.5 examples/sec; 0.232 sec/batch; 18h:39m:32s remains)
INFO - root - 2017-12-08 07:34:02.714211: step 42700, loss = 2.28, batch loss = 2.23 (36.7 examples/sec; 0.218 sec/batch; 17h:31m:32s remains)
2017-12-08 07:34:02.993185: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288979 -4.4288816 -4.4288583 -4.4288216 -4.4287963 -4.4287853 -4.428802 -4.4288287 -4.42885 -4.4288335 -4.428772 -4.4286976 -4.4286447 -4.4286447 -4.4286976][-4.4288826 -4.4288735 -4.4288487 -4.428803 -4.428782 -4.4287667 -4.4287758 -4.4288034 -4.4288278 -4.4288197 -4.4287615 -4.4286833 -4.4286261 -4.4286227 -4.4286876][-4.4288583 -4.4288549 -4.4288259 -4.4287724 -4.4287453 -4.4287219 -4.4287214 -4.4287434 -4.4287682 -4.4287791 -4.4287481 -4.4286962 -4.4286547 -4.428659 -4.4287295][-4.4288111 -4.4288106 -4.42878 -4.4287257 -4.4286919 -4.4286528 -4.4286423 -4.4286628 -4.4286995 -4.4287405 -4.4287486 -4.4287329 -4.42871 -4.4287128 -4.4287663][-4.4287491 -4.4287605 -4.4287424 -4.4286952 -4.428648 -4.4285893 -4.4285707 -4.4286022 -4.4286561 -4.4287167 -4.428741 -4.4287381 -4.4287219 -4.4287271 -4.42877][-4.4286976 -4.4287195 -4.4287076 -4.4286566 -4.428587 -4.4285064 -4.4284897 -4.4285321 -4.4285917 -4.4286528 -4.4286661 -4.4286556 -4.4286418 -4.4286628 -4.4287257][-4.4286485 -4.4286723 -4.4286547 -4.4285927 -4.4285021 -4.4284105 -4.4283962 -4.4284353 -4.4284792 -4.428524 -4.4285226 -4.4285054 -4.4285049 -4.4285579 -4.4286489][-4.4286137 -4.4286304 -4.4285951 -4.4285083 -4.4283957 -4.4282961 -4.4282827 -4.4283085 -4.42833 -4.4283571 -4.4283576 -4.428349 -4.4283671 -4.4284482 -4.4285603][-4.4285827 -4.428586 -4.4285288 -4.4284225 -4.4283037 -4.4282155 -4.4282131 -4.4282446 -4.4282608 -4.4282761 -4.4282761 -4.4282727 -4.4282947 -4.4283805 -4.4285016][-4.4285717 -4.4285622 -4.4284968 -4.428401 -4.4283094 -4.4282603 -4.4282756 -4.4283109 -4.4283223 -4.4283242 -4.4283185 -4.4283171 -4.4283395 -4.4284158 -4.4285331][-4.4286304 -4.4286232 -4.4285746 -4.4285069 -4.42845 -4.4284329 -4.4284549 -4.42849 -4.4285011 -4.4284983 -4.42849 -4.4284897 -4.4285069 -4.4285626 -4.4286504][-4.4287386 -4.4287429 -4.4287224 -4.4286861 -4.4286523 -4.4286475 -4.4286647 -4.4286914 -4.4287038 -4.4287024 -4.4286957 -4.4286938 -4.4287033 -4.4287362 -4.4287915][-4.4288387 -4.4288464 -4.42884 -4.4288263 -4.4288106 -4.4288135 -4.4288263 -4.4288445 -4.4288545 -4.4288511 -4.4288454 -4.4288449 -4.42885 -4.4288688 -4.4289017][-4.4289064 -4.4289126 -4.42891 -4.4289074 -4.4289036 -4.4289074 -4.4289145 -4.4289231 -4.4289279 -4.4289274 -4.4289269 -4.4289255 -4.4289269 -4.4289374 -4.4289579][-4.4289446 -4.4289522 -4.4289513 -4.42895 -4.428946 -4.428946 -4.4289484 -4.4289532 -4.4289556 -4.428956 -4.428957 -4.4289575 -4.4289584 -4.4289637 -4.4289751]]...]
INFO - root - 2017-12-08 07:34:05.263774: step 42710, loss = 2.28, batch loss = 2.23 (36.7 examples/sec; 0.218 sec/batch; 17h:32m:18s remains)
INFO - root - 2017-12-08 07:34:07.507295: step 42720, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 17h:37m:21s remains)
INFO - root - 2017-12-08 07:34:09.728360: step 42730, loss = 2.28, batch loss = 2.23 (36.6 examples/sec; 0.219 sec/batch; 17h:35m:55s remains)
INFO - root - 2017-12-08 07:34:11.981403: step 42740, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 17h:46m:59s remains)
INFO - root - 2017-12-08 07:34:14.246610: step 42750, loss = 2.28, batch loss = 2.23 (34.6 examples/sec; 0.231 sec/batch; 18h:37m:15s remains)
INFO - root - 2017-12-08 07:34:16.523786: step 42760, loss = 2.28, batch loss = 2.23 (31.9 examples/sec; 0.251 sec/batch; 20h:10m:27s remains)
INFO - root - 2017-12-08 07:34:18.751384: step 42770, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 17h:38m:49s remains)
INFO - root - 2017-12-08 07:34:20.997640: step 42780, loss = 2.28, batch loss = 2.23 (35.0 examples/sec; 0.228 sec/batch; 18h:22m:22s remains)
INFO - root - 2017-12-08 07:34:23.246251: step 42790, loss = 2.28, batch loss = 2.23 (34.6 examples/sec; 0.231 sec/batch; 18h:35m:36s remains)
INFO - root - 2017-12-08 07:34:25.503677: step 42800, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.220 sec/batch; 17h:43m:27s remains)
2017-12-08 07:34:25.803898: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4287643 -4.4287696 -4.4287639 -4.4287434 -4.428721 -4.4286985 -4.4286933 -4.4287186 -4.4287505 -4.4287567 -4.4287491 -4.4287415 -4.4287338 -4.4287348 -4.42875][-4.4287968 -4.42881 -4.4288135 -4.4287963 -4.4287772 -4.4287586 -4.4287386 -4.4287295 -4.4287271 -4.4287033 -4.4286757 -4.4286709 -4.4286757 -4.4286904 -4.4287229][-4.4288263 -4.428843 -4.428854 -4.4288425 -4.4288211 -4.4287968 -4.4287548 -4.4287028 -4.4286575 -4.4285979 -4.42855 -4.4285545 -4.4285808 -4.4286222 -4.4286766][-4.4288287 -4.4288483 -4.4288626 -4.428854 -4.4288177 -4.4287696 -4.4286942 -4.4286013 -4.4285398 -4.4284811 -4.4284425 -4.4284763 -4.4285326 -4.42859 -4.4286432][-4.4288077 -4.4288278 -4.428844 -4.428833 -4.4287777 -4.4287114 -4.4286189 -4.4285131 -4.4284692 -4.4284439 -4.4284372 -4.4285021 -4.4285717 -4.4286189 -4.4286528][-4.4287982 -4.428812 -4.4288182 -4.4287968 -4.4287357 -4.4286757 -4.4285989 -4.4285121 -4.4284964 -4.4285016 -4.4285116 -4.4285779 -4.4286404 -4.4286661 -4.4286761][-4.4288111 -4.4288096 -4.4288006 -4.4287682 -4.4287105 -4.428668 -4.4286261 -4.4285793 -4.4285774 -4.42859 -4.4286046 -4.428658 -4.4287076 -4.428719 -4.4287162][-4.4288287 -4.4288063 -4.4287834 -4.4287519 -4.4287152 -4.4287004 -4.4286919 -4.4286733 -4.4286695 -4.4286761 -4.4286871 -4.4287262 -4.4287663 -4.4287767 -4.4287715][-4.4288406 -4.4288077 -4.4287858 -4.4287672 -4.4287567 -4.4287667 -4.4287758 -4.4287691 -4.4287553 -4.4287529 -4.4287586 -4.4287848 -4.4288168 -4.42883 -4.4288263][-4.42884 -4.4288177 -4.4288135 -4.428812 -4.4288177 -4.428833 -4.4288406 -4.428834 -4.4288163 -4.4288154 -4.4288211 -4.42884 -4.4288645 -4.4288721 -4.428865][-4.4288187 -4.4288216 -4.4288335 -4.428843 -4.4288583 -4.4288769 -4.4288869 -4.4288821 -4.4288692 -4.428874 -4.4288793 -4.4288893 -4.4289 -4.4289007 -4.4288898][-4.4287958 -4.4288249 -4.428854 -4.4288754 -4.4289012 -4.428926 -4.4289403 -4.428937 -4.4289207 -4.4289188 -4.4289174 -4.4289179 -4.4289169 -4.4289122 -4.4288979][-4.428803 -4.4288449 -4.4288821 -4.4289122 -4.4289455 -4.4289703 -4.4289813 -4.4289722 -4.4289484 -4.4289327 -4.4289265 -4.4289231 -4.4289165 -4.4289103 -4.428896][-4.4288206 -4.428864 -4.4289002 -4.4289303 -4.4289589 -4.4289784 -4.4289842 -4.4289713 -4.4289455 -4.4289222 -4.4289126 -4.4289079 -4.4289017 -4.4288969 -4.4288878][-4.4288268 -4.4288611 -4.4288926 -4.4289207 -4.4289417 -4.428957 -4.4289622 -4.4289527 -4.4289331 -4.4289117 -4.4289 -4.4288964 -4.4288912 -4.4288883 -4.4288821]]...]
INFO - root - 2017-12-08 07:34:28.015821: step 42810, loss = 2.28, batch loss = 2.23 (35.2 examples/sec; 0.227 sec/batch; 18h:18m:17s remains)
INFO - root - 2017-12-08 07:34:30.258153: step 42820, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.226 sec/batch; 18h:09m:08s remains)
INFO - root - 2017-12-08 07:34:32.477732: step 42830, loss = 2.28, batch loss = 2.23 (35.2 examples/sec; 0.227 sec/batch; 18h:16m:10s remains)
INFO - root - 2017-12-08 07:34:34.740052: step 42840, loss = 2.28, batch loss = 2.23 (36.6 examples/sec; 0.218 sec/batch; 17h:34m:10s remains)
INFO - root - 2017-12-08 07:34:36.980333: step 42850, loss = 2.28, batch loss = 2.23 (35.2 examples/sec; 0.227 sec/batch; 18h:16m:10s remains)
INFO - root - 2017-12-08 07:34:39.271466: step 42860, loss = 2.28, batch loss = 2.23 (35.6 examples/sec; 0.224 sec/batch; 18h:03m:18s remains)
INFO - root - 2017-12-08 07:34:41.466716: step 42870, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 17h:39m:55s remains)
INFO - root - 2017-12-08 07:34:43.687434: step 42880, loss = 2.28, batch loss = 2.23 (34.5 examples/sec; 0.232 sec/batch; 18h:40m:18s remains)
INFO - root - 2017-12-08 07:34:45.914927: step 42890, loss = 2.28, batch loss = 2.23 (35.4 examples/sec; 0.226 sec/batch; 18h:10m:33s remains)
INFO - root - 2017-12-08 07:34:48.157628: step 42900, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.221 sec/batch; 17h:48m:10s remains)
2017-12-08 07:34:48.438953: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288116 -4.4288244 -4.4288468 -4.428875 -4.4289041 -4.4289136 -4.4289227 -4.4289279 -4.4289336 -4.4289417 -4.4289632 -4.4289751 -4.4289703 -4.4289641 -4.4289541][-4.4287467 -4.4287686 -4.4287972 -4.4288306 -4.428864 -4.4288745 -4.4288831 -4.428896 -4.4289155 -4.4289317 -4.4289541 -4.4289684 -4.428967 -4.4289618 -4.4289513][-4.4286885 -4.4287152 -4.4287477 -4.4287848 -4.428822 -4.4288335 -4.4288397 -4.4288568 -4.4288859 -4.4289088 -4.42893 -4.4289465 -4.4289513 -4.42895 -4.4289408][-4.4286537 -4.4286704 -4.4286957 -4.4287338 -4.4287786 -4.4287949 -4.4287977 -4.4288111 -4.4288411 -4.4288697 -4.4288907 -4.4289103 -4.4289236 -4.4289269 -4.4289203][-4.4286451 -4.4286313 -4.4286237 -4.4286523 -4.4287047 -4.4287305 -4.4287333 -4.4287438 -4.4287739 -4.428812 -4.4288378 -4.4288645 -4.4288869 -4.4288945 -4.4288912][-4.4286532 -4.4286079 -4.4285545 -4.4285493 -4.4285955 -4.4286284 -4.4286385 -4.4286556 -4.4286952 -4.4287467 -4.4287815 -4.4288158 -4.4288487 -4.428863 -4.4288616][-4.4286737 -4.4286194 -4.4285359 -4.4284887 -4.428503 -4.4285254 -4.4285407 -4.4285731 -4.428628 -4.4286904 -4.4287319 -4.428772 -4.4288144 -4.4288383 -4.4288397][-4.4287057 -4.4286704 -4.4285979 -4.4285374 -4.4285169 -4.4285145 -4.4285178 -4.4285483 -4.4286003 -4.4286637 -4.4287133 -4.4287567 -4.428802 -4.4288297 -4.4288316][-4.428751 -4.428741 -4.4286981 -4.42865 -4.4286184 -4.4285975 -4.4285812 -4.4285865 -4.4286137 -4.4286671 -4.4287243 -4.4287744 -4.4288192 -4.4288445 -4.4288406][-4.4287968 -4.4288034 -4.4287858 -4.4287548 -4.4287233 -4.4286952 -4.4286633 -4.4286394 -4.4286356 -4.4286742 -4.4287381 -4.4287987 -4.4288478 -4.42887 -4.42886][-4.4288321 -4.4288473 -4.4288435 -4.4288244 -4.4287963 -4.4287643 -4.42872 -4.4286733 -4.4286489 -4.4286766 -4.4287448 -4.4288135 -4.4288683 -4.4288912 -4.4288788][-4.428863 -4.4288764 -4.4288788 -4.428865 -4.4288411 -4.4288096 -4.4287605 -4.4287066 -4.4286771 -4.4286995 -4.428762 -4.42883 -4.4288859 -4.42891 -4.428896][-4.4288907 -4.4289 -4.4289041 -4.428895 -4.4288769 -4.42885 -4.428803 -4.42875 -4.4287229 -4.4287443 -4.4288011 -4.4288621 -4.4289112 -4.4289308 -4.4289136][-4.4289021 -4.4289074 -4.428916 -4.4289145 -4.4289055 -4.4288859 -4.4288406 -4.4287896 -4.4287653 -4.4287887 -4.4288406 -4.4288936 -4.4289341 -4.4289474 -4.428926][-4.4288955 -4.4289031 -4.4289165 -4.42892 -4.4289155 -4.4289007 -4.4288611 -4.4288173 -4.4287972 -4.4288192 -4.4288654 -4.4289103 -4.4289451 -4.4289541 -4.4289308]]...]
INFO - root - 2017-12-08 07:34:50.687573: step 42910, loss = 2.28, batch loss = 2.23 (35.7 examples/sec; 0.224 sec/batch; 18h:00m:31s remains)
INFO - root - 2017-12-08 07:34:52.922793: step 42920, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 17h:36m:56s remains)
INFO - root - 2017-12-08 07:34:55.179285: step 42930, loss = 2.28, batch loss = 2.23 (36.9 examples/sec; 0.217 sec/batch; 17h:27m:17s remains)
INFO - root - 2017-12-08 07:34:57.453017: step 42940, loss = 2.28, batch loss = 2.23 (35.8 examples/sec; 0.224 sec/batch; 17h:59m:21s remains)
INFO - root - 2017-12-08 07:34:59.692649: step 42950, loss = 2.28, batch loss = 2.23 (34.7 examples/sec; 0.230 sec/batch; 18h:31m:07s remains)
INFO - root - 2017-12-08 07:35:01.910337: step 42960, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.220 sec/batch; 17h:43m:49s remains)
INFO - root - 2017-12-08 07:35:04.134949: step 42970, loss = 2.28, batch loss = 2.23 (37.0 examples/sec; 0.216 sec/batch; 17h:22m:22s remains)
INFO - root - 2017-12-08 07:35:06.358761: step 42980, loss = 2.28, batch loss = 2.23 (37.1 examples/sec; 0.216 sec/batch; 17h:19m:55s remains)
INFO - root - 2017-12-08 07:35:08.626886: step 42990, loss = 2.28, batch loss = 2.23 (34.5 examples/sec; 0.232 sec/batch; 18h:39m:25s remains)
INFO - root - 2017-12-08 07:35:10.853034: step 43000, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 17h:36m:36s remains)
2017-12-08 07:35:11.196151: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4285188 -4.4285231 -4.4285431 -4.4285641 -4.4285836 -4.4286075 -4.4286327 -4.4286404 -4.4286418 -4.42864 -4.4286427 -4.4286623 -4.4286542 -4.4286079 -4.4285531][-4.4285421 -4.4285493 -4.4285855 -4.4286265 -4.428659 -4.4286909 -4.4287043 -4.4287086 -4.4287195 -4.4287171 -4.4287109 -4.4287105 -4.4286871 -4.4286327 -4.42858][-4.4285865 -4.4285917 -4.4286294 -4.428679 -4.4287171 -4.4287443 -4.4287405 -4.4287338 -4.4287472 -4.4287496 -4.4287434 -4.4287305 -4.4286981 -4.4286385 -4.4285893][-4.4286628 -4.4286733 -4.4287014 -4.4287434 -4.4287691 -4.4287744 -4.4287429 -4.4287124 -4.4287252 -4.4287415 -4.4287429 -4.4287233 -4.4286942 -4.4286456 -4.428606][-4.4287395 -4.428751 -4.4287634 -4.428781 -4.4287767 -4.4287429 -4.4286804 -4.4286408 -4.4286785 -4.4287381 -4.428771 -4.4287539 -4.4287229 -4.4286814 -4.4286466][-4.4287853 -4.4287853 -4.4287772 -4.4287667 -4.4287295 -4.4286494 -4.42855 -4.4285021 -4.42858 -4.4287 -4.4287806 -4.4287863 -4.4287653 -4.4287348 -4.4287][-4.4287982 -4.4287848 -4.4287548 -4.4287114 -4.428638 -4.4284983 -4.4283361 -4.4282537 -4.4283743 -4.428556 -4.4286938 -4.4287529 -4.4287639 -4.4287496 -4.4287219][-4.4287682 -4.4287539 -4.4287148 -4.4286513 -4.4285431 -4.428339 -4.4280858 -4.427917 -4.42808 -4.4283528 -4.4285593 -4.4286814 -4.4287324 -4.4287381 -4.4287171][-4.4287257 -4.4287224 -4.4286942 -4.42864 -4.428524 -4.4283009 -4.4280143 -4.4277825 -4.4279337 -4.4282465 -4.4284854 -4.4286366 -4.4287124 -4.42873 -4.4287162][-4.4287219 -4.428731 -4.4287233 -4.4286933 -4.428616 -4.4284458 -4.4282408 -4.4280868 -4.4281816 -4.4284034 -4.4285817 -4.4286942 -4.4287505 -4.4287586 -4.4287434][-4.4287457 -4.4287615 -4.4287624 -4.4287477 -4.42872 -4.4286265 -4.4285116 -4.428441 -4.4285088 -4.4286404 -4.4287429 -4.4288015 -4.4288163 -4.4288015 -4.4287786][-4.4287872 -4.4287992 -4.4287982 -4.4287844 -4.4287825 -4.4287472 -4.4286928 -4.42866 -4.4287076 -4.428792 -4.4288526 -4.4288769 -4.428863 -4.4288354 -4.4288082][-4.4288321 -4.4288354 -4.4288292 -4.4288154 -4.4288211 -4.4288173 -4.4287963 -4.4287825 -4.42881 -4.42886 -4.428895 -4.4289017 -4.428884 -4.4288597 -4.4288344][-4.4288635 -4.4288635 -4.4288568 -4.4288445 -4.4288445 -4.4288454 -4.4288406 -4.4288378 -4.4288573 -4.4288878 -4.4289055 -4.428905 -4.4288936 -4.4288812 -4.428864][-4.4289021 -4.4289007 -4.428895 -4.4288855 -4.4288807 -4.4288788 -4.4288783 -4.4288821 -4.4288945 -4.4289155 -4.4289289 -4.4289303 -4.4289246 -4.4289174 -4.4289079]]...]
INFO - root - 2017-12-08 07:35:13.437952: step 43010, loss = 2.28, batch loss = 2.23 (35.1 examples/sec; 0.228 sec/batch; 18h:20m:54s remains)
INFO - root - 2017-12-08 07:35:15.704523: step 43020, loss = 2.28, batch loss = 2.23 (35.2 examples/sec; 0.228 sec/batch; 18h:17m:59s remains)
INFO - root - 2017-12-08 07:35:17.957983: step 43030, loss = 2.28, batch loss = 2.23 (35.2 examples/sec; 0.227 sec/batch; 18h:17m:08s remains)
INFO - root - 2017-12-08 07:35:20.232276: step 43040, loss = 2.28, batch loss = 2.23 (35.4 examples/sec; 0.226 sec/batch; 18h:10m:11s remains)
INFO - root - 2017-12-08 07:35:22.467480: step 43050, loss = 2.28, batch loss = 2.23 (35.6 examples/sec; 0.225 sec/batch; 18h:03m:40s remains)
INFO - root - 2017-12-08 07:35:24.707787: step 43060, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.225 sec/batch; 18h:06m:25s remains)
INFO - root - 2017-12-08 07:35:26.932465: step 43070, loss = 2.28, batch loss = 2.23 (35.0 examples/sec; 0.229 sec/batch; 18h:23m:53s remains)
INFO - root - 2017-12-08 07:35:29.153243: step 43080, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 17h:41m:28s remains)
INFO - root - 2017-12-08 07:35:31.373616: step 43090, loss = 2.28, batch loss = 2.23 (36.9 examples/sec; 0.217 sec/batch; 17h:26m:19s remains)
INFO - root - 2017-12-08 07:35:33.614948: step 43100, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 17h:44m:44s remains)
2017-12-08 07:35:33.892765: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4286118 -4.4286518 -4.4287043 -4.4287543 -4.4288034 -4.428833 -4.4288316 -4.4288087 -4.428782 -4.42879 -4.4288168 -4.4288359 -4.4288197 -4.4287767 -4.4287448][-4.4286532 -4.4286232 -4.428617 -4.4286594 -4.4287319 -4.4287887 -4.428803 -4.4287891 -4.4287734 -4.4287844 -4.4288073 -4.4288325 -4.4288459 -4.428822 -4.4287872][-4.42872 -4.4286485 -4.4285607 -4.4285393 -4.4286017 -4.4286718 -4.4287019 -4.4287076 -4.4287171 -4.4287434 -4.4287639 -4.4287934 -4.4288216 -4.4288096 -4.4287772][-4.4287758 -4.4287028 -4.4285712 -4.4284663 -4.4284563 -4.4284935 -4.4285369 -4.4285779 -4.4286227 -4.428668 -4.4287038 -4.4287515 -4.4287839 -4.4287648 -4.42873][-4.4288 -4.4287562 -4.428627 -4.4284725 -4.4283676 -4.4283214 -4.4283481 -4.4284353 -4.4285231 -4.4285836 -4.4286346 -4.4287109 -4.4287648 -4.4287505 -4.42871][-4.4287844 -4.4287786 -4.4286828 -4.42853 -4.4283805 -4.4282603 -4.4282303 -4.4283323 -4.4284487 -4.4285231 -4.4285927 -4.4286971 -4.4287739 -4.4287739 -4.4287386][-4.4287515 -4.4287767 -4.4287219 -4.428607 -4.428472 -4.42834 -4.42827 -4.4283338 -4.4284234 -4.4284844 -4.4285684 -4.4286866 -4.4287767 -4.4287982 -4.4287796][-4.4287434 -4.4287829 -4.4287548 -4.4286752 -4.4285836 -4.4284782 -4.4284072 -4.4284191 -4.4284434 -4.4284754 -4.4285593 -4.4286747 -4.4287672 -4.42881 -4.4288087][-4.4287705 -4.4288096 -4.428791 -4.4287295 -4.4286714 -4.4286013 -4.428545 -4.4285173 -4.4284983 -4.4285054 -4.4285774 -4.4286819 -4.428772 -4.4288177 -4.4288225][-4.4288015 -4.4288278 -4.4288135 -4.4287629 -4.4287128 -4.4286718 -4.4286289 -4.4285827 -4.4285355 -4.4285197 -4.4285717 -4.4286666 -4.4287567 -4.4288096 -4.4288163][-4.4288139 -4.4288282 -4.4288173 -4.4287734 -4.42873 -4.4287076 -4.4286737 -4.4286237 -4.4285583 -4.4285059 -4.4285245 -4.4286103 -4.4287076 -4.4287739 -4.4287877][-4.428843 -4.4288535 -4.4288449 -4.4288082 -4.4287729 -4.4287562 -4.4287262 -4.4286718 -4.4285865 -4.4284911 -4.4284625 -4.4285259 -4.4286318 -4.4287205 -4.4287448][-4.4288888 -4.4289002 -4.4288936 -4.4288592 -4.4288297 -4.4288182 -4.4287968 -4.4287372 -4.4286284 -4.428494 -4.4284296 -4.4284649 -4.4285631 -4.4286666 -4.4287043][-4.4289255 -4.4289322 -4.4289227 -4.4288893 -4.428864 -4.4288607 -4.4288549 -4.4288092 -4.4286947 -4.4285502 -4.4284763 -4.42849 -4.4285574 -4.4286389 -4.4286752][-4.4289317 -4.4289293 -4.4289103 -4.4288793 -4.428863 -4.4288712 -4.4288774 -4.42885 -4.4287453 -4.4286194 -4.4285626 -4.4285626 -4.4285975 -4.4286418 -4.4286714]]...]
INFO - root - 2017-12-08 07:35:36.121971: step 43110, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 17h:45m:11s remains)
INFO - root - 2017-12-08 07:35:38.398576: step 43120, loss = 2.28, batch loss = 2.23 (35.7 examples/sec; 0.224 sec/batch; 18h:01m:47s remains)
INFO - root - 2017-12-08 07:35:40.652400: step 43130, loss = 2.28, batch loss = 2.23 (35.4 examples/sec; 0.226 sec/batch; 18h:09m:18s remains)
INFO - root - 2017-12-08 07:35:42.868447: step 43140, loss = 2.28, batch loss = 2.23 (36.7 examples/sec; 0.218 sec/batch; 17h:30m:32s remains)
INFO - root - 2017-12-08 07:35:45.120879: step 43150, loss = 2.28, batch loss = 2.23 (35.2 examples/sec; 0.227 sec/batch; 18h:16m:28s remains)
INFO - root - 2017-12-08 07:35:47.341501: step 43160, loss = 2.28, batch loss = 2.23 (35.1 examples/sec; 0.228 sec/batch; 18h:20m:11s remains)
INFO - root - 2017-12-08 07:35:49.578196: step 43170, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 17h:46m:23s remains)
INFO - root - 2017-12-08 07:35:51.824984: step 43180, loss = 2.28, batch loss = 2.23 (36.7 examples/sec; 0.218 sec/batch; 17h:31m:13s remains)
INFO - root - 2017-12-08 07:35:54.086649: step 43190, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.225 sec/batch; 18h:06m:20s remains)
INFO - root - 2017-12-08 07:35:56.308106: step 43200, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 17h:45m:12s remains)
2017-12-08 07:35:56.625016: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4286256 -4.4286113 -4.4285936 -4.4285636 -4.4285293 -4.4284978 -4.4284868 -4.4285154 -4.4285545 -4.4285769 -4.4285913 -4.4285855 -4.4285736 -4.4285808 -4.4285989][-4.4286275 -4.4286046 -4.4285965 -4.4285879 -4.4285665 -4.428534 -4.42851 -4.4285083 -4.4285207 -4.4285269 -4.4285331 -4.4285259 -4.4285188 -4.428534 -4.4285712][-4.4286141 -4.4285936 -4.4285946 -4.4286032 -4.4285874 -4.4285488 -4.4285073 -4.4284725 -4.4284606 -4.4284616 -4.4284697 -4.4284697 -4.4284654 -4.4284811 -4.4285269][-4.4285922 -4.4285874 -4.4286046 -4.4286222 -4.4285965 -4.428544 -4.4284773 -4.4284139 -4.4283829 -4.4283872 -4.4284096 -4.4284291 -4.4284353 -4.4284449 -4.4284763][-4.4285583 -4.4285831 -4.4286246 -4.428648 -4.4286089 -4.4285307 -4.4284334 -4.4283485 -4.428308 -4.4283228 -4.4283681 -4.428422 -4.4284515 -4.4284611 -4.4284821][-4.4285293 -4.4285917 -4.4286547 -4.4286814 -4.4286327 -4.4285274 -4.4284 -4.4282932 -4.4282479 -4.428278 -4.4283557 -4.4284506 -4.42851 -4.4285316 -4.4285502][-4.4285235 -4.4286194 -4.4287 -4.42873 -4.4286761 -4.4285469 -4.42839 -4.4282603 -4.4282074 -4.4282484 -4.4283528 -4.4284863 -4.4285793 -4.428617 -4.4286346][-4.4285407 -4.4286532 -4.4287457 -4.4287777 -4.4287152 -4.4285669 -4.4283905 -4.4282455 -4.4281893 -4.4282379 -4.4283581 -4.4285097 -4.4286237 -4.4286761 -4.4286981][-4.428576 -4.4286809 -4.428771 -4.4288006 -4.4287362 -4.4285855 -4.4284091 -4.4282703 -4.4282203 -4.4282703 -4.4283881 -4.428534 -4.4286528 -4.4287157 -4.4287424][-4.4286256 -4.4287033 -4.4287729 -4.428793 -4.4287381 -4.4286089 -4.4284549 -4.4283395 -4.4282985 -4.4283366 -4.4284358 -4.4285631 -4.4286728 -4.4287362 -4.428762][-4.4286718 -4.4287176 -4.4287615 -4.428771 -4.4287319 -4.4286342 -4.4285159 -4.4284291 -4.4283977 -4.4284205 -4.4284954 -4.4285984 -4.4286895 -4.4287415 -4.4287596][-4.4287124 -4.4287286 -4.4287496 -4.428751 -4.4287252 -4.4286618 -4.4285831 -4.4285254 -4.4285016 -4.4285111 -4.4285593 -4.4286361 -4.428709 -4.4287462 -4.4287539][-4.4287548 -4.4287453 -4.4287467 -4.428741 -4.4287248 -4.4286895 -4.4286475 -4.428617 -4.4286032 -4.428606 -4.4286323 -4.4286795 -4.42873 -4.4287519 -4.4287486][-4.4287992 -4.4287744 -4.4287605 -4.4287457 -4.4287333 -4.4287171 -4.4287024 -4.4286957 -4.4286909 -4.4286928 -4.4287038 -4.4287262 -4.4287539 -4.4287624 -4.4287534][-4.4288383 -4.4288077 -4.4287782 -4.4287524 -4.4287405 -4.4287372 -4.4287424 -4.4287524 -4.4287548 -4.4287586 -4.4287634 -4.42877 -4.428781 -4.42878 -4.4287729]]...]
INFO - root - 2017-12-08 07:35:58.856424: step 43210, loss = 2.28, batch loss = 2.23 (34.8 examples/sec; 0.230 sec/batch; 18h:28m:00s remains)
INFO - root - 2017-12-08 07:36:01.153766: step 43220, loss = 2.28, batch loss = 2.23 (34.2 examples/sec; 0.234 sec/batch; 18h:49m:21s remains)
INFO - root - 2017-12-08 07:36:03.413738: step 43230, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.225 sec/batch; 18h:06m:51s remains)
INFO - root - 2017-12-08 07:36:05.630870: step 43240, loss = 2.28, batch loss = 2.23 (37.3 examples/sec; 0.214 sec/batch; 17h:13m:09s remains)
INFO - root - 2017-12-08 07:36:07.889666: step 43250, loss = 2.28, batch loss = 2.23 (35.1 examples/sec; 0.228 sec/batch; 18h:17m:54s remains)
INFO - root - 2017-12-08 07:36:10.158460: step 43260, loss = 2.28, batch loss = 2.23 (35.2 examples/sec; 0.227 sec/batch; 18h:14m:58s remains)
INFO - root - 2017-12-08 07:36:12.375584: step 43270, loss = 2.28, batch loss = 2.23 (36.6 examples/sec; 0.218 sec/batch; 17h:32m:14s remains)
INFO - root - 2017-12-08 07:36:14.668820: step 43280, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 17h:54m:25s remains)
INFO - root - 2017-12-08 07:36:16.916228: step 43290, loss = 2.28, batch loss = 2.23 (35.7 examples/sec; 0.224 sec/batch; 17h:59m:49s remains)
INFO - root - 2017-12-08 07:36:19.156660: step 43300, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 17h:38m:06s remains)
2017-12-08 07:36:19.508882: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288564 -4.4288812 -4.4288912 -4.4288797 -4.428864 -4.4288573 -4.4288435 -4.4288306 -4.4288259 -4.42881 -4.4287868 -4.4287791 -4.4287429 -4.4286895 -4.4286861][-4.4288263 -4.4288692 -4.4288931 -4.4288797 -4.4288511 -4.4288344 -4.428812 -4.428792 -4.4287796 -4.4287605 -4.42874 -4.4287324 -4.4286966 -4.428648 -4.4286489][-4.428813 -4.428874 -4.4289122 -4.4288969 -4.428853 -4.4288163 -4.4287777 -4.4287524 -4.4287395 -4.4287176 -4.428699 -4.4286962 -4.4286766 -4.42864 -4.4286366][-4.4288063 -4.4288831 -4.4289217 -4.4288964 -4.4288349 -4.4287767 -4.428719 -4.4286942 -4.4286947 -4.428679 -4.4286613 -4.4286613 -4.4286666 -4.4286466 -4.4286346][-4.4287987 -4.4288917 -4.428925 -4.4288836 -4.428802 -4.4287133 -4.4286246 -4.4285989 -4.4286232 -4.42864 -4.4286375 -4.428637 -4.4286447 -4.428637 -4.428628][-4.4288058 -4.4289 -4.4289165 -4.4288607 -4.4287596 -4.4286351 -4.4285107 -4.4284849 -4.4285412 -4.4286 -4.4286275 -4.4286337 -4.4286294 -4.428606 -4.4285922][-4.4288278 -4.4288983 -4.4288883 -4.4288106 -4.4286795 -4.4284973 -4.4283252 -4.4283152 -4.4284258 -4.4285364 -4.4286065 -4.4286442 -4.4286346 -4.428587 -4.4285593][-4.4288392 -4.4288812 -4.4288468 -4.4287519 -4.4285836 -4.4283252 -4.4280915 -4.4281158 -4.4282928 -4.4284596 -4.4285669 -4.4286361 -4.4286366 -4.4285927 -4.428566][-4.4288497 -4.4288683 -4.4288254 -4.4287238 -4.4285417 -4.4282551 -4.4280019 -4.42805 -4.4282608 -4.4284468 -4.4285631 -4.4286442 -4.4286571 -4.4286294 -4.428617][-4.4288473 -4.4288578 -4.428823 -4.4287333 -4.4285784 -4.4283423 -4.4281483 -4.4281907 -4.4283481 -4.428483 -4.4285769 -4.4286556 -4.4286861 -4.4286714 -4.4286656][-4.4288359 -4.428843 -4.428823 -4.4287586 -4.4286489 -4.4284873 -4.4283619 -4.4283862 -4.4284706 -4.428535 -4.4285908 -4.4286623 -4.4287095 -4.4287052 -4.428699][-4.4288306 -4.4288335 -4.4288259 -4.4287882 -4.4287181 -4.4286075 -4.4285178 -4.4285192 -4.4285607 -4.4285827 -4.428607 -4.4286656 -4.4287248 -4.4287291 -4.4287148][-4.4288158 -4.4288225 -4.4288292 -4.4288149 -4.4287739 -4.4286966 -4.4286165 -4.4285908 -4.4286056 -4.4286108 -4.4286127 -4.428647 -4.4287052 -4.4287276 -4.4287133][-4.4287939 -4.42881 -4.4288244 -4.4288344 -4.428823 -4.4287786 -4.4287057 -4.4286637 -4.4286523 -4.4286342 -4.4286036 -4.4285994 -4.4286418 -4.4286752 -4.4286609][-4.4287915 -4.4288149 -4.4288263 -4.4288445 -4.4288573 -4.4288359 -4.428782 -4.4287415 -4.4287138 -4.428669 -4.4286051 -4.4285631 -4.4285913 -4.428628 -4.428607]]...]
INFO - root - 2017-12-08 07:36:21.759241: step 43310, loss = 2.28, batch loss = 2.23 (36.6 examples/sec; 0.219 sec/batch; 17h:33m:51s remains)
INFO - root - 2017-12-08 07:36:24.000601: step 43320, loss = 2.28, batch loss = 2.23 (35.4 examples/sec; 0.226 sec/batch; 18h:09m:55s remains)
INFO - root - 2017-12-08 07:36:26.245560: step 43330, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 17h:40m:22s remains)
INFO - root - 2017-12-08 07:36:28.471063: step 43340, loss = 2.28, batch loss = 2.23 (34.6 examples/sec; 0.231 sec/batch; 18h:34m:04s remains)
INFO - root - 2017-12-08 07:36:30.716600: step 43350, loss = 2.28, batch loss = 2.23 (36.6 examples/sec; 0.218 sec/batch; 17h:32m:18s remains)
INFO - root - 2017-12-08 07:36:32.935809: step 43360, loss = 2.28, batch loss = 2.23 (36.6 examples/sec; 0.219 sec/batch; 17h:33m:58s remains)
INFO - root - 2017-12-08 07:36:35.175285: step 43370, loss = 2.28, batch loss = 2.23 (34.7 examples/sec; 0.230 sec/batch; 18h:29m:38s remains)
INFO - root - 2017-12-08 07:36:37.396309: step 43380, loss = 2.28, batch loss = 2.23 (37.0 examples/sec; 0.216 sec/batch; 17h:21m:37s remains)
INFO - root - 2017-12-08 07:36:39.622046: step 43390, loss = 2.28, batch loss = 2.23 (36.7 examples/sec; 0.218 sec/batch; 17h:30m:34s remains)
INFO - root - 2017-12-08 07:36:41.851844: step 43400, loss = 2.28, batch loss = 2.23 (36.7 examples/sec; 0.218 sec/batch; 17h:31m:27s remains)
2017-12-08 07:36:42.136670: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4287376 -4.4287276 -4.4287496 -4.4287734 -4.4287872 -4.4288049 -4.428854 -4.428906 -4.4289322 -4.428936 -4.4289284 -4.4289188 -4.4288931 -4.4288468 -4.4287944][-4.4287348 -4.4287124 -4.4287286 -4.4287558 -4.42878 -4.4287996 -4.4288497 -4.4289069 -4.4289355 -4.4289393 -4.4289265 -4.4289145 -4.4288888 -4.4288464 -4.4287949][-4.4287386 -4.4286971 -4.4287014 -4.4287276 -4.4287658 -4.4287963 -4.4288545 -4.4289236 -4.4289546 -4.4289589 -4.4289427 -4.4289284 -4.4289031 -4.4288645 -4.4288144][-4.4287562 -4.4286871 -4.4286604 -4.4286757 -4.4287267 -4.4287858 -4.4288626 -4.4289389 -4.4289708 -4.4289742 -4.4289589 -4.428946 -4.4289231 -4.4288912 -4.4288454][-4.4287786 -4.4286814 -4.4286156 -4.4286036 -4.4286604 -4.4287562 -4.4288635 -4.4289494 -4.4289818 -4.428987 -4.4289713 -4.4289508 -4.4289255 -4.4289002 -4.4288621][-4.42879 -4.42867 -4.4285645 -4.4285126 -4.4285574 -4.4286876 -4.4288349 -4.428937 -4.42897 -4.4289732 -4.4289551 -4.4289255 -4.4288955 -4.4288745 -4.4288468][-4.4287643 -4.4286308 -4.428493 -4.4284015 -4.4284263 -4.42858 -4.4287653 -4.4288883 -4.4289289 -4.4289322 -4.4289136 -4.4288788 -4.4288435 -4.4288225 -4.4288025][-4.4286985 -4.4285636 -4.4284091 -4.4282885 -4.4282861 -4.4284439 -4.428658 -4.4288073 -4.4288664 -4.4288774 -4.4288607 -4.4288244 -4.4287863 -4.4287653 -4.428751][-4.4286165 -4.428493 -4.4283428 -4.4282074 -4.428174 -4.4283137 -4.4285369 -4.428709 -4.428791 -4.4288149 -4.4288034 -4.4287686 -4.4287329 -4.4287152 -4.4287081][-4.4285688 -4.4284678 -4.4283385 -4.4282 -4.4281306 -4.4282269 -4.4284363 -4.4286184 -4.4287186 -4.4287529 -4.4287491 -4.428719 -4.4286895 -4.4286752 -4.428678][-4.4286008 -4.4285221 -4.4284091 -4.428268 -4.4281678 -4.4282107 -4.4283805 -4.4285488 -4.4286528 -4.428699 -4.4287062 -4.4286857 -4.4286647 -4.4286518 -4.4286609][-4.4286947 -4.4286346 -4.4285378 -4.4284067 -4.4282975 -4.4282975 -4.4284115 -4.428546 -4.428638 -4.4286866 -4.428699 -4.4286828 -4.428658 -4.4286346 -4.428637][-4.42881 -4.4287663 -4.4286866 -4.4285789 -4.4284825 -4.4284611 -4.4285235 -4.4286151 -4.4286866 -4.4287286 -4.4287376 -4.4287133 -4.4286704 -4.4286194 -4.4286008][-4.4289026 -4.4288721 -4.4288139 -4.4287329 -4.4286594 -4.4286337 -4.4286618 -4.4287148 -4.4287653 -4.4287977 -4.4288006 -4.4287615 -4.4286919 -4.4286046 -4.428556][-4.4289622 -4.4289446 -4.4289069 -4.4288526 -4.4288015 -4.4287796 -4.4287863 -4.4288077 -4.4288368 -4.4288616 -4.428863 -4.4288197 -4.4287353 -4.428618 -4.4285364]]...]
INFO - root - 2017-12-08 07:36:44.359695: step 43410, loss = 2.28, batch loss = 2.23 (36.0 examples/sec; 0.222 sec/batch; 17h:50m:12s remains)
INFO - root - 2017-12-08 07:36:46.599647: step 43420, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 17h:53m:20s remains)
INFO - root - 2017-12-08 07:36:48.846194: step 43430, loss = 2.28, batch loss = 2.23 (35.1 examples/sec; 0.228 sec/batch; 18h:17m:42s remains)
INFO - root - 2017-12-08 07:36:51.094837: step 43440, loss = 2.28, batch loss = 2.23 (35.3 examples/sec; 0.227 sec/batch; 18h:12m:53s remains)
INFO - root - 2017-12-08 07:36:53.375961: step 43450, loss = 2.28, batch loss = 2.23 (33.9 examples/sec; 0.236 sec/batch; 18h:57m:59s remains)
INFO - root - 2017-12-08 07:36:55.644303: step 43460, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.221 sec/batch; 17h:42m:26s remains)
INFO - root - 2017-12-08 07:36:57.861213: step 43470, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 17h:39m:31s remains)
INFO - root - 2017-12-08 07:37:00.110851: step 43480, loss = 2.28, batch loss = 2.23 (35.4 examples/sec; 0.226 sec/batch; 18h:09m:31s remains)
INFO - root - 2017-12-08 07:37:02.331085: step 43490, loss = 2.28, batch loss = 2.23 (35.8 examples/sec; 0.223 sec/batch; 17h:55m:31s remains)
INFO - root - 2017-12-08 07:37:04.595471: step 43500, loss = 2.28, batch loss = 2.23 (35.1 examples/sec; 0.228 sec/batch; 18h:18m:36s remains)
2017-12-08 07:37:04.896271: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289222 -4.4289002 -4.4288812 -4.428863 -4.4288459 -4.42884 -4.4288378 -4.4288259 -4.428813 -4.4288235 -4.4288549 -4.4288874 -4.4289141 -4.4289403 -4.4289584][-4.428906 -4.4288821 -4.4288659 -4.4288497 -4.4288335 -4.4288287 -4.4288254 -4.4288058 -4.4287868 -4.4288 -4.4288454 -4.428884 -4.4289122 -4.4289365 -4.4289517][-4.4288898 -4.4288616 -4.4288445 -4.4288239 -4.4288006 -4.4287953 -4.428791 -4.4287691 -4.4287543 -4.428781 -4.4288435 -4.4288926 -4.4289227 -4.4289422 -4.4289489][-4.4288688 -4.4288278 -4.4287992 -4.42877 -4.4287405 -4.4287286 -4.4287205 -4.4287004 -4.4286957 -4.4287395 -4.4288168 -4.4288816 -4.4289246 -4.4289432 -4.4289446][-4.4288282 -4.4287658 -4.4287195 -4.4286904 -4.4286695 -4.4286432 -4.4286127 -4.4285746 -4.4285784 -4.4286528 -4.4287481 -4.4288282 -4.4288926 -4.4289222 -4.4289293][-4.4287543 -4.42867 -4.4286094 -4.4285932 -4.4285836 -4.4285283 -4.4284258 -4.4283285 -4.4283538 -4.4284911 -4.42863 -4.4287357 -4.4288259 -4.4288831 -4.4289074][-4.4286652 -4.4285622 -4.4285016 -4.4284983 -4.4284816 -4.4283772 -4.4281497 -4.4279413 -4.4280152 -4.4282737 -4.4284983 -4.4286466 -4.4287624 -4.4288445 -4.4288888][-4.4286036 -4.4285073 -4.428462 -4.4284592 -4.4284158 -4.4282532 -4.4278846 -4.4275522 -4.4277129 -4.428103 -4.4284043 -4.4285874 -4.4287205 -4.4288116 -4.4288712][-4.428617 -4.4285588 -4.4285378 -4.4285235 -4.4284625 -4.4282885 -4.4279237 -4.4276357 -4.4278245 -4.4281883 -4.4284596 -4.4286232 -4.4287314 -4.4288077 -4.4288669][-4.4286895 -4.4286675 -4.4286585 -4.4286366 -4.4285827 -4.4284606 -4.4282317 -4.4280968 -4.4282184 -4.4284348 -4.4286122 -4.4287243 -4.428791 -4.4288392 -4.4288821][-4.4287786 -4.4287772 -4.4287758 -4.4287577 -4.4287205 -4.4286542 -4.4285312 -4.4284744 -4.4285378 -4.4286523 -4.4287558 -4.4288235 -4.4288597 -4.4288836 -4.4289079][-4.42888 -4.4288878 -4.4288826 -4.4288692 -4.4288459 -4.428813 -4.4287438 -4.4287105 -4.4287438 -4.4288139 -4.4288797 -4.4289155 -4.4289293 -4.4289341 -4.4289422][-4.4289575 -4.4289689 -4.4289584 -4.428946 -4.4289365 -4.428926 -4.4288878 -4.4288664 -4.4288883 -4.4289365 -4.4289751 -4.4289927 -4.4289918 -4.4289827 -4.4289794][-4.4289794 -4.4289856 -4.4289761 -4.428967 -4.4289632 -4.4289689 -4.42896 -4.42895 -4.4289637 -4.4289918 -4.4290128 -4.4290166 -4.42901 -4.4290013 -4.4289985][-4.4289641 -4.4289627 -4.4289527 -4.4289441 -4.4289408 -4.428947 -4.4289508 -4.4289503 -4.4289579 -4.4289689 -4.4289794 -4.4289875 -4.4289904 -4.4289923 -4.4289951]]...]
INFO - root - 2017-12-08 07:37:07.105720: step 43510, loss = 2.28, batch loss = 2.23 (35.3 examples/sec; 0.226 sec/batch; 18h:10m:28s remains)
INFO - root - 2017-12-08 07:37:09.362720: step 43520, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.225 sec/batch; 18h:04m:45s remains)
INFO - root - 2017-12-08 07:37:11.593616: step 43530, loss = 2.28, batch loss = 2.23 (36.8 examples/sec; 0.217 sec/batch; 17h:27m:00s remains)
INFO - root - 2017-12-08 07:37:13.859748: step 43540, loss = 2.28, batch loss = 2.23 (36.6 examples/sec; 0.219 sec/batch; 17h:33m:29s remains)
INFO - root - 2017-12-08 07:37:16.075685: step 43550, loss = 2.28, batch loss = 2.23 (38.1 examples/sec; 0.210 sec/batch; 16h:51m:39s remains)
INFO - root - 2017-12-08 07:37:18.330051: step 43560, loss = 2.28, batch loss = 2.23 (35.2 examples/sec; 0.227 sec/batch; 18h:13m:24s remains)
INFO - root - 2017-12-08 07:37:20.561019: step 43570, loss = 2.28, batch loss = 2.23 (35.8 examples/sec; 0.223 sec/batch; 17h:54m:47s remains)
INFO - root - 2017-12-08 07:37:22.796817: step 43580, loss = 2.28, batch loss = 2.23 (35.4 examples/sec; 0.226 sec/batch; 18h:09m:25s remains)
INFO - root - 2017-12-08 07:37:25.054510: step 43590, loss = 2.28, batch loss = 2.23 (34.4 examples/sec; 0.232 sec/batch; 18h:38m:24s remains)
INFO - root - 2017-12-08 07:37:27.309464: step 43600, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.220 sec/batch; 17h:40m:31s remains)
2017-12-08 07:37:27.617813: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4286556 -4.4286785 -4.4286551 -4.428607 -4.428587 -4.4286089 -4.4286628 -4.4287453 -4.4287949 -4.4287872 -4.4287453 -4.4286909 -4.428618 -4.428524 -4.4284167][-4.4286466 -4.4286838 -4.4286747 -4.4286408 -4.4286289 -4.4286494 -4.4286914 -4.4287562 -4.4287992 -4.4288025 -4.4287825 -4.4287481 -4.42869 -4.4286079 -4.428493][-4.4286509 -4.4286952 -4.4286957 -4.42867 -4.4286642 -4.4286828 -4.4287066 -4.4287472 -4.4287848 -4.4288034 -4.4288049 -4.4287877 -4.4287453 -4.4286828 -4.4285812][-4.4286647 -4.4287119 -4.4287148 -4.428689 -4.4286795 -4.4286866 -4.4286885 -4.428709 -4.428741 -4.4287672 -4.42878 -4.428772 -4.4287438 -4.4287043 -4.42863][-4.428689 -4.4287333 -4.4287305 -4.4286914 -4.4286642 -4.4286509 -4.4286323 -4.4286404 -4.4286709 -4.4287004 -4.428721 -4.4287243 -4.4287162 -4.4287066 -4.428668][-4.4287128 -4.4287486 -4.4287291 -4.4286647 -4.4286056 -4.4285617 -4.4285231 -4.4285307 -4.4285769 -4.42862 -4.4286551 -4.428679 -4.4286962 -4.4287195 -4.428721][-4.4287181 -4.4287395 -4.4287 -4.4286084 -4.4285169 -4.4284449 -4.4283867 -4.4283996 -4.4284744 -4.4285426 -4.4286017 -4.4286518 -4.4286976 -4.4287462 -4.4287753][-4.4287109 -4.4287214 -4.4286771 -4.4285736 -4.4284654 -4.4283814 -4.4283094 -4.4283204 -4.4284105 -4.4284973 -4.4285674 -4.4286289 -4.428688 -4.4287481 -4.4287939][-4.4287028 -4.4287143 -4.4286823 -4.42859 -4.4284906 -4.428422 -4.4283586 -4.4283524 -4.4284177 -4.428494 -4.4285512 -4.4285994 -4.4286528 -4.4287138 -4.4287696][-4.4287024 -4.4287248 -4.4287143 -4.4286437 -4.4285603 -4.4285116 -4.4284625 -4.4284396 -4.4284725 -4.4285316 -4.4285707 -4.428597 -4.4286351 -4.4286919 -4.4287515][-4.4286795 -4.42871 -4.4287171 -4.4286666 -4.4285965 -4.4285564 -4.4285131 -4.4284868 -4.4285116 -4.4285688 -4.4286051 -4.4286246 -4.4286513 -4.4286938 -4.4287419][-4.4286127 -4.4286513 -4.4286737 -4.4286437 -4.428597 -4.428565 -4.4285274 -4.4285035 -4.4285269 -4.4285803 -4.4286208 -4.4286437 -4.4286637 -4.4286919 -4.4287219][-4.428525 -4.4285736 -4.4286089 -4.4286017 -4.4285874 -4.4285736 -4.4285488 -4.4285326 -4.4285512 -4.4285903 -4.4286222 -4.4286375 -4.4286494 -4.4286656 -4.4286809][-4.42846 -4.4285178 -4.4285665 -4.4285841 -4.4285984 -4.4286032 -4.4285936 -4.4285846 -4.4285941 -4.4286113 -4.4286237 -4.4286261 -4.4286304 -4.4286385 -4.4286432][-4.4284444 -4.4285083 -4.4285622 -4.4285946 -4.4286213 -4.4286337 -4.4286332 -4.42863 -4.4286337 -4.4286361 -4.4286356 -4.4286313 -4.4286337 -4.42864 -4.42864]]...]
INFO - root - 2017-12-08 07:37:29.825627: step 43610, loss = 2.28, batch loss = 2.23 (37.8 examples/sec; 0.212 sec/batch; 16h:59m:00s remains)
INFO - root - 2017-12-08 07:37:32.057400: step 43620, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 17h:36m:27s remains)
INFO - root - 2017-12-08 07:37:34.301455: step 43630, loss = 2.28, batch loss = 2.23 (35.1 examples/sec; 0.228 sec/batch; 18h:16m:47s remains)
INFO - root - 2017-12-08 07:37:36.555897: step 43640, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.222 sec/batch; 17h:46m:29s remains)
INFO - root - 2017-12-08 07:37:38.812925: step 43650, loss = 2.28, batch loss = 2.23 (33.3 examples/sec; 0.240 sec/batch; 19h:17m:21s remains)
INFO - root - 2017-12-08 07:37:41.079646: step 43660, loss = 2.28, batch loss = 2.23 (36.6 examples/sec; 0.219 sec/batch; 17h:32m:18s remains)
INFO - root - 2017-12-08 07:37:43.307647: step 43670, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.221 sec/batch; 17h:41m:47s remains)
INFO - root - 2017-12-08 07:37:45.557819: step 43680, loss = 2.28, batch loss = 2.23 (37.4 examples/sec; 0.214 sec/batch; 17h:10m:03s remains)
INFO - root - 2017-12-08 07:37:47.847945: step 43690, loss = 2.28, batch loss = 2.23 (32.4 examples/sec; 0.247 sec/batch; 19h:50m:00s remains)
INFO - root - 2017-12-08 07:37:50.137682: step 43700, loss = 2.28, batch loss = 2.23 (35.2 examples/sec; 0.227 sec/batch; 18h:14m:56s remains)
2017-12-08 07:37:50.436496: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.42889 -4.42889 -4.4288898 -4.4288893 -4.4288898 -4.4288874 -4.4288678 -4.4288435 -4.4288306 -4.4288154 -4.4287891 -4.4287519 -4.4287181 -4.4286871 -4.4286504][-4.4288559 -4.4288445 -4.428833 -4.4288311 -4.4288359 -4.428834 -4.4288096 -4.428772 -4.4287505 -4.4287333 -4.4287004 -4.42866 -4.4286323 -4.4286027 -4.4285727][-4.428781 -4.428762 -4.4287457 -4.4287367 -4.4287329 -4.4287281 -4.4287095 -4.4286675 -4.4286313 -4.4286113 -4.4285984 -4.4285855 -4.4285731 -4.428535 -4.4285][-4.4286876 -4.4286757 -4.4286609 -4.428647 -4.4286356 -4.4286203 -4.42859 -4.4285374 -4.4284959 -4.4284916 -4.42852 -4.4285488 -4.4285493 -4.4285059 -4.4284673][-4.4286227 -4.4286208 -4.4286075 -4.4285841 -4.4285531 -4.4285131 -4.4284434 -4.4283619 -4.4283433 -4.4283862 -4.4284582 -4.4285145 -4.4285312 -4.4285111 -4.4284887][-4.4286036 -4.4286036 -4.4285765 -4.428534 -4.4284782 -4.4283895 -4.4282479 -4.428134 -4.4281788 -4.4283037 -4.4284167 -4.4284916 -4.4285297 -4.4285407 -4.4285345][-4.42858 -4.4285803 -4.4285455 -4.4284859 -4.4283924 -4.4282379 -4.4280028 -4.4278531 -4.4280105 -4.4282265 -4.4283648 -4.428442 -4.4284916 -4.4285297 -4.4285326][-4.428535 -4.4285421 -4.4285011 -4.4284286 -4.428308 -4.42812 -4.4278588 -4.4277363 -4.4279666 -4.4281878 -4.4282975 -4.4283581 -4.4284167 -4.4284787 -4.4284973][-4.4285636 -4.4285622 -4.4285116 -4.42844 -4.4283414 -4.4282064 -4.4280491 -4.4280076 -4.4281635 -4.4282894 -4.4283462 -4.4283948 -4.4284577 -4.4285231 -4.4285383][-4.4286442 -4.4286184 -4.4285574 -4.428503 -4.4284549 -4.4283814 -4.4283061 -4.4283061 -4.4283805 -4.4284291 -4.4284487 -4.4284925 -4.42855 -4.428596 -4.42859][-4.4287367 -4.4286933 -4.4286304 -4.4285975 -4.4285741 -4.4285173 -4.4284792 -4.4285 -4.4285369 -4.4285507 -4.42856 -4.4285922 -4.4286294 -4.4286628 -4.4286447][-4.4288497 -4.42881 -4.4287453 -4.4287152 -4.4287066 -4.4286647 -4.4286404 -4.4286652 -4.4286838 -4.4286852 -4.4286914 -4.4287 -4.4287162 -4.428741 -4.428731][-4.4289508 -4.4289203 -4.4288588 -4.4288321 -4.428833 -4.4288049 -4.428793 -4.4288177 -4.428833 -4.4288282 -4.4288197 -4.4288077 -4.4288077 -4.4288268 -4.4288321][-4.4290285 -4.42901 -4.4289637 -4.4289441 -4.4289427 -4.4289174 -4.4289079 -4.4289327 -4.4289517 -4.4289489 -4.4289312 -4.4289107 -4.4289002 -4.4289107 -4.4289179][-4.4290504 -4.4290419 -4.4290118 -4.4289989 -4.4289951 -4.4289708 -4.428966 -4.4289923 -4.4290104 -4.4290075 -4.4289885 -4.428967 -4.4289484 -4.428947 -4.4289532]]...]
INFO - root - 2017-12-08 07:37:52.651857: step 43710, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.220 sec/batch; 17h:39m:36s remains)
INFO - root - 2017-12-08 07:37:54.884243: step 43720, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.221 sec/batch; 17h:45m:49s remains)
INFO - root - 2017-12-08 07:37:57.093487: step 43730, loss = 2.28, batch loss = 2.23 (37.3 examples/sec; 0.214 sec/batch; 17h:11m:17s remains)
INFO - root - 2017-12-08 07:37:59.344473: step 43740, loss = 2.28, batch loss = 2.23 (35.3 examples/sec; 0.227 sec/batch; 18h:11m:25s remains)
INFO - root - 2017-12-08 07:38:01.550880: step 43750, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 17h:44m:39s remains)
INFO - root - 2017-12-08 07:38:03.771414: step 43760, loss = 2.28, batch loss = 2.23 (36.0 examples/sec; 0.222 sec/batch; 17h:48m:36s remains)
INFO - root - 2017-12-08 07:38:06.001935: step 43770, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 17h:35m:02s remains)
INFO - root - 2017-12-08 07:38:08.239843: step 43780, loss = 2.28, batch loss = 2.23 (33.5 examples/sec; 0.239 sec/batch; 19h:07m:50s remains)
INFO - root - 2017-12-08 07:38:10.516554: step 43790, loss = 2.28, batch loss = 2.23 (32.4 examples/sec; 0.247 sec/batch; 19h:48m:16s remains)
INFO - root - 2017-12-08 07:38:12.755764: step 43800, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 17h:52m:27s remains)
2017-12-08 07:38:13.055864: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.428906 -4.4289165 -4.4289255 -4.4289293 -4.4289284 -4.428926 -4.4289255 -4.4289241 -4.4289145 -4.4289055 -4.4289041 -4.428906 -4.428894 -4.4288511 -4.4287934][-4.4289432 -4.4289494 -4.4289536 -4.4289508 -4.4289417 -4.4289312 -4.4289274 -4.4289269 -4.4289131 -4.4288988 -4.4289002 -4.4289122 -4.4289103 -4.4288716 -4.4288063][-4.4289656 -4.4289689 -4.4289675 -4.4289551 -4.4289336 -4.4289136 -4.4289045 -4.4289031 -4.4288816 -4.4288664 -4.4288726 -4.428895 -4.4289165 -4.4289017 -4.4288507][-4.4289703 -4.428967 -4.4289551 -4.4289284 -4.4288945 -4.4288626 -4.4288468 -4.42884 -4.4288106 -4.4287953 -4.4288077 -4.4288421 -4.4288864 -4.4289036 -4.4288759][-4.4289441 -4.4289312 -4.4289117 -4.4288759 -4.4288292 -4.4287815 -4.4287572 -4.4287448 -4.42871 -4.4286995 -4.4287195 -4.4287605 -4.4288192 -4.42886 -4.42885][-4.4289079 -4.4288807 -4.4288445 -4.4287915 -4.4287243 -4.4286456 -4.4285975 -4.4285774 -4.428545 -4.4285479 -4.4285836 -4.428638 -4.428719 -4.4287877 -4.4287963][-4.4288597 -4.4288187 -4.4287591 -4.4286828 -4.4285755 -4.42844 -4.4283576 -4.4283414 -4.4283328 -4.428371 -4.428441 -4.4285212 -4.4286251 -4.4287133 -4.4287319][-4.428812 -4.4287663 -4.4286895 -4.4285865 -4.4284487 -4.4282718 -4.4281745 -4.4281721 -4.4282031 -4.4282861 -4.4283905 -4.4284968 -4.4286132 -4.4287038 -4.4287219][-4.4288139 -4.4287815 -4.4287038 -4.4286089 -4.4284844 -4.4283304 -4.4282618 -4.4282784 -4.4283242 -4.4284043 -4.4284959 -4.4285903 -4.4286876 -4.4287596 -4.4287748][-4.4288449 -4.4288292 -4.428772 -4.4287047 -4.4286184 -4.4285188 -4.4284916 -4.4285154 -4.428555 -4.4286141 -4.4286795 -4.4287415 -4.4288039 -4.4288516 -4.4288611][-4.4288597 -4.4288573 -4.428823 -4.4287829 -4.4287372 -4.4286914 -4.4286919 -4.4287171 -4.4287491 -4.4287891 -4.4288268 -4.4288664 -4.4289017 -4.428926 -4.42893][-4.4288554 -4.4288688 -4.4288616 -4.4288464 -4.4288344 -4.4288282 -4.4288387 -4.4288549 -4.428874 -4.428896 -4.4289207 -4.42894 -4.42895 -4.4289565 -4.42896][-4.4288449 -4.4288712 -4.4288888 -4.4288926 -4.4288993 -4.4289079 -4.4289131 -4.428916 -4.428926 -4.4289422 -4.4289589 -4.4289665 -4.4289651 -4.4289613 -4.42896][-4.42886 -4.4288912 -4.42891 -4.4289126 -4.428915 -4.4289203 -4.4289212 -4.4289193 -4.4289255 -4.4289408 -4.4289579 -4.4289665 -4.4289651 -4.4289589 -4.4289551][-4.4288845 -4.4289103 -4.4289222 -4.42892 -4.4289107 -4.4289041 -4.4288983 -4.428895 -4.4289021 -4.4289184 -4.428937 -4.428946 -4.4289484 -4.4289479 -4.4289489]]...]
INFO - root - 2017-12-08 07:38:15.272802: step 43810, loss = 2.28, batch loss = 2.23 (37.0 examples/sec; 0.216 sec/batch; 17h:21m:04s remains)
INFO - root - 2017-12-08 07:38:17.509588: step 43820, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.219 sec/batch; 17h:36m:04s remains)
INFO - root - 2017-12-08 07:38:19.774465: step 43830, loss = 2.28, batch loss = 2.23 (35.4 examples/sec; 0.226 sec/batch; 18h:07m:44s remains)
INFO - root - 2017-12-08 07:38:21.996040: step 43840, loss = 2.28, batch loss = 2.23 (35.8 examples/sec; 0.223 sec/batch; 17h:54m:44s remains)
INFO - root - 2017-12-08 07:38:24.392272: step 43850, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.226 sec/batch; 18h:05m:19s remains)
INFO - root - 2017-12-08 07:38:26.638901: step 43860, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 17h:52m:30s remains)
INFO - root - 2017-12-08 07:38:28.878339: step 43870, loss = 2.28, batch loss = 2.23 (36.0 examples/sec; 0.222 sec/batch; 17h:48m:22s remains)
INFO - root - 2017-12-08 07:38:31.100698: step 43880, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 17h:52m:44s remains)
INFO - root - 2017-12-08 07:38:33.332899: step 43890, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 17h:36m:44s remains)
INFO - root - 2017-12-08 07:38:35.562089: step 43900, loss = 2.28, batch loss = 2.23 (36.6 examples/sec; 0.219 sec/batch; 17h:31m:27s remains)
2017-12-08 07:38:35.861772: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288893 -4.42889 -4.4288855 -4.4288635 -4.4288411 -4.4288177 -4.4287934 -4.4287686 -4.4287505 -4.4287462 -4.4287391 -4.42873 -4.4287171 -4.4287 -4.4286871][-4.428884 -4.4288907 -4.4288797 -4.4288473 -4.4288063 -4.4287677 -4.4287329 -4.4287 -4.4286852 -4.4287004 -4.4287066 -4.4287014 -4.42869 -4.4286704 -4.4286532][-4.4288692 -4.4288807 -4.4288621 -4.4288139 -4.4287505 -4.4286942 -4.4286523 -4.4286227 -4.4286275 -4.4286718 -4.4286995 -4.428709 -4.4287066 -4.4286857 -4.4286656][-4.4288483 -4.428864 -4.4288397 -4.4287772 -4.4286947 -4.4286127 -4.4285493 -4.4285197 -4.4285517 -4.4286313 -4.4286852 -4.4287167 -4.4287291 -4.4287186 -4.4287043][-4.4288249 -4.428843 -4.4288144 -4.4287438 -4.4286485 -4.4285288 -4.4284215 -4.4283714 -4.4284286 -4.42856 -4.4286609 -4.4287286 -4.4287667 -4.4287786 -4.4287696][-4.4288211 -4.4288425 -4.4288168 -4.4287472 -4.4286389 -4.4284854 -4.4283247 -4.4282336 -4.4283137 -4.4285007 -4.4286532 -4.428761 -4.4288287 -4.4288588 -4.4288526][-4.4288259 -4.4288554 -4.4288383 -4.4287739 -4.4286513 -4.4284678 -4.4282656 -4.4281397 -4.4282308 -4.4284549 -4.4286404 -4.4287682 -4.4288492 -4.4288864 -4.4288898][-4.4288259 -4.4288559 -4.4288435 -4.42878 -4.4286489 -4.4284573 -4.4282341 -4.4280872 -4.428185 -4.4284277 -4.4286256 -4.4287591 -4.4288435 -4.42888 -4.4288945][-4.42883 -4.4288554 -4.4288464 -4.4287882 -4.4286661 -4.4284925 -4.4282761 -4.4281349 -4.428226 -4.42845 -4.4286394 -4.4287667 -4.4288406 -4.4288697 -4.4288883][-4.4288425 -4.428853 -4.42884 -4.4287949 -4.4287004 -4.428575 -4.4284225 -4.4283228 -4.4283838 -4.4285431 -4.4286814 -4.428782 -4.4288387 -4.4288573 -4.4288716][-4.4288468 -4.4288425 -4.4288287 -4.428802 -4.4287486 -4.4286785 -4.4286017 -4.4285464 -4.4285789 -4.428669 -4.4287429 -4.4287968 -4.42883 -4.42884 -4.4288468][-4.4288554 -4.4288392 -4.42883 -4.4288244 -4.4288039 -4.4287739 -4.4287419 -4.4287119 -4.4287219 -4.4287572 -4.4287872 -4.4288073 -4.4288158 -4.4288154 -4.4288154][-4.428875 -4.4288497 -4.4288425 -4.4288526 -4.4288535 -4.4288411 -4.4288278 -4.4288125 -4.42881 -4.4288034 -4.4287987 -4.4287987 -4.4287953 -4.4287968 -4.4287992][-4.428884 -4.4288611 -4.4288568 -4.4288707 -4.4288831 -4.4288831 -4.4288831 -4.4288735 -4.4288616 -4.4288363 -4.4288077 -4.4287887 -4.428782 -4.4287934 -4.4288044][-4.4288764 -4.428863 -4.4288692 -4.4288869 -4.4289088 -4.4289184 -4.428916 -4.4289064 -4.4288898 -4.4288559 -4.4288206 -4.4287953 -4.4287868 -4.4288011 -4.4288177]]...]
INFO - root - 2017-12-08 07:38:38.106525: step 43910, loss = 2.28, batch loss = 2.23 (33.9 examples/sec; 0.236 sec/batch; 18h:55m:51s remains)
INFO - root - 2017-12-08 07:38:40.341209: step 43920, loss = 2.28, batch loss = 2.23 (37.0 examples/sec; 0.216 sec/batch; 17h:19m:13s remains)
INFO - root - 2017-12-08 07:38:42.535435: step 43930, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 17h:35m:54s remains)
INFO - root - 2017-12-08 07:38:44.788377: step 43940, loss = 2.28, batch loss = 2.23 (35.3 examples/sec; 0.227 sec/batch; 18h:10m:54s remains)
INFO - root - 2017-12-08 07:38:47.039326: step 43950, loss = 2.28, batch loss = 2.23 (36.0 examples/sec; 0.222 sec/batch; 17h:49m:20s remains)
INFO - root - 2017-12-08 07:38:49.273919: step 43960, loss = 2.28, batch loss = 2.23 (35.8 examples/sec; 0.224 sec/batch; 17h:55m:05s remains)
INFO - root - 2017-12-08 07:38:51.490633: step 43970, loss = 2.28, batch loss = 2.23 (35.2 examples/sec; 0.228 sec/batch; 18h:14m:14s remains)
INFO - root - 2017-12-08 07:38:53.729025: step 43980, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 17h:38m:11s remains)
INFO - root - 2017-12-08 07:38:55.973432: step 43990, loss = 2.28, batch loss = 2.23 (34.7 examples/sec; 0.230 sec/batch; 18h:27m:42s remains)
INFO - root - 2017-12-08 07:38:58.223383: step 44000, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 17h:37m:37s remains)
2017-12-08 07:38:58.530734: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.428721 -4.4286714 -4.4286623 -4.4286909 -4.4287286 -4.4287391 -4.4287386 -4.4287248 -4.4287062 -4.4287047 -4.4286966 -4.4286871 -4.4286876 -4.4286919 -4.4286923][-4.4286613 -4.4286246 -4.4286408 -4.4286919 -4.4287391 -4.4287438 -4.4287195 -4.4286895 -4.428668 -4.4286709 -4.4286656 -4.4286695 -4.4286895 -4.428689 -4.4286833][-4.4285746 -4.428545 -4.4285893 -4.4286633 -4.4287138 -4.4287186 -4.4286776 -4.4286404 -4.4286313 -4.4286475 -4.42865 -4.4286623 -4.4286847 -4.4286695 -4.428648][-4.4284863 -4.4284654 -4.4285398 -4.428637 -4.4286895 -4.4286771 -4.4286036 -4.4285488 -4.4285583 -4.4286041 -4.4286227 -4.4286451 -4.4286637 -4.4286394 -4.4286051][-4.4283857 -4.4283733 -4.4284687 -4.4285812 -4.4286327 -4.428596 -4.4284744 -4.4283948 -4.4284387 -4.4285479 -4.4286122 -4.4286513 -4.4286661 -4.4286375 -4.4285812][-4.4282675 -4.4282722 -4.4283624 -4.428462 -4.4285035 -4.4284306 -4.4282441 -4.4281373 -4.4282532 -4.4284654 -4.4286022 -4.4286718 -4.428699 -4.4286737 -4.428596][-4.428175 -4.4282012 -4.4282842 -4.4283509 -4.4283566 -4.428237 -4.4279776 -4.4278388 -4.4280491 -4.4283624 -4.428556 -4.4286504 -4.4286928 -4.428679 -4.4286013][-4.428165 -4.4281979 -4.4282618 -4.4282994 -4.4282837 -4.4281569 -4.4279151 -4.4277873 -4.4280133 -4.4283366 -4.428534 -4.4286242 -4.4286718 -4.4286747 -4.4286232][-4.4282374 -4.4282551 -4.4282851 -4.4283147 -4.4283266 -4.4282484 -4.4280963 -4.4280162 -4.4281764 -4.4284172 -4.4285717 -4.4286447 -4.4286819 -4.4286923 -4.4286695][-4.4283524 -4.4283476 -4.4283457 -4.4283657 -4.428401 -4.4283772 -4.4283066 -4.428257 -4.4283514 -4.4285045 -4.4286165 -4.4286742 -4.4287052 -4.4287186 -4.4287176][-4.4284897 -4.4284782 -4.42845 -4.4284544 -4.428493 -4.4285169 -4.4285045 -4.4284725 -4.4285045 -4.4285736 -4.4286346 -4.4286761 -4.4287062 -4.4287276 -4.4287496][-4.4286509 -4.4286404 -4.4285946 -4.4285846 -4.4286141 -4.4286561 -4.428669 -4.4286494 -4.4286337 -4.4286275 -4.4286246 -4.428627 -4.42865 -4.4286876 -4.4287262][-4.4287834 -4.4287815 -4.4287457 -4.4287295 -4.4287405 -4.4287682 -4.4287753 -4.4287572 -4.4287162 -4.428659 -4.4285913 -4.428545 -4.4285479 -4.4285932 -4.4286475][-4.4288588 -4.4288611 -4.4288421 -4.4288244 -4.4288216 -4.4288278 -4.4288306 -4.4288149 -4.4287696 -4.4286976 -4.4286022 -4.428514 -4.4284897 -4.4285278 -4.4285803][-4.4288812 -4.42888 -4.4288707 -4.428854 -4.4288378 -4.42883 -4.428834 -4.42883 -4.4287996 -4.4287362 -4.4286437 -4.4285426 -4.4284935 -4.428514 -4.4285555]]...]
INFO - root - 2017-12-08 07:39:00.773993: step 44010, loss = 2.28, batch loss = 2.23 (35.2 examples/sec; 0.227 sec/batch; 18h:12m:51s remains)
INFO - root - 2017-12-08 07:39:02.992747: step 44020, loss = 2.28, batch loss = 2.23 (34.6 examples/sec; 0.231 sec/batch; 18h:32m:25s remains)
INFO - root - 2017-12-08 07:39:05.268454: step 44030, loss = 2.28, batch loss = 2.23 (37.2 examples/sec; 0.215 sec/batch; 17h:15m:03s remains)
INFO - root - 2017-12-08 07:39:07.565482: step 44040, loss = 2.28, batch loss = 2.23 (34.6 examples/sec; 0.231 sec/batch; 18h:30m:37s remains)
INFO - root - 2017-12-08 07:39:09.802480: step 44050, loss = 2.28, batch loss = 2.23 (36.7 examples/sec; 0.218 sec/batch; 17h:28m:41s remains)
INFO - root - 2017-12-08 07:39:12.039138: step 44060, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 17h:50m:57s remains)
INFO - root - 2017-12-08 07:39:14.274616: step 44070, loss = 2.28, batch loss = 2.23 (36.7 examples/sec; 0.218 sec/batch; 17h:27m:21s remains)
INFO - root - 2017-12-08 07:39:16.502755: step 44080, loss = 2.28, batch loss = 2.23 (35.4 examples/sec; 0.226 sec/batch; 18h:07m:41s remains)
INFO - root - 2017-12-08 07:39:18.754467: step 44090, loss = 2.28, batch loss = 2.23 (34.4 examples/sec; 0.233 sec/batch; 18h:38m:12s remains)
INFO - root - 2017-12-08 07:39:20.994240: step 44100, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 17h:32m:45s remains)
2017-12-08 07:39:21.315493: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288106 -4.4287634 -4.4287 -4.4286857 -4.4287243 -4.4287357 -4.4287133 -4.4286723 -4.4286337 -4.428628 -4.4286666 -4.4287114 -4.4287467 -4.4287763 -4.4287944][-4.428782 -4.4287534 -4.4286962 -4.4286623 -4.428678 -4.4286866 -4.4286795 -4.4286408 -4.4286046 -4.4286251 -4.4286914 -4.4287524 -4.4288015 -4.42883 -4.4288259][-4.4287624 -4.4287491 -4.4287195 -4.4286933 -4.4287028 -4.4287057 -4.428689 -4.4286327 -4.4285913 -4.4286356 -4.4287333 -4.4288144 -4.4288616 -4.428863 -4.4288335][-4.4287381 -4.4287329 -4.4287357 -4.4287415 -4.4287667 -4.4287591 -4.4287195 -4.4286294 -4.4285626 -4.4286289 -4.428751 -4.4288411 -4.4288673 -4.4288244 -4.4287815][-4.42875 -4.4287438 -4.42877 -4.4287844 -4.4288111 -4.4287839 -4.4287024 -4.4285579 -4.4284639 -4.4285541 -4.428689 -4.4287786 -4.4287829 -4.4287186 -4.4286809][-4.4287767 -4.4287786 -4.4288125 -4.4288087 -4.4288077 -4.4287467 -4.4286036 -4.4283972 -4.4282923 -4.4284215 -4.42857 -4.4286628 -4.4286714 -4.4286146 -4.4285889][-4.4287462 -4.4287486 -4.4287772 -4.4287562 -4.4287276 -4.4286308 -4.4284296 -4.428165 -4.4280715 -4.4282603 -4.4284387 -4.4285583 -4.4285994 -4.428576 -4.4285645][-4.4287038 -4.4286823 -4.4286971 -4.4286661 -4.4286189 -4.4285121 -4.4283156 -4.4280591 -4.4280024 -4.4282117 -4.4284 -4.4285345 -4.428597 -4.4286036 -4.4286113][-4.4287181 -4.4286795 -4.42867 -4.428628 -4.4285855 -4.4285097 -4.4283919 -4.4282355 -4.4282179 -4.4283714 -4.4285197 -4.4286361 -4.4286828 -4.4286809 -4.4287][-4.4287868 -4.4287429 -4.4287081 -4.4286513 -4.4286141 -4.4285769 -4.4285269 -4.4284506 -4.4284544 -4.4285526 -4.4286656 -4.4287548 -4.4287834 -4.42877 -4.42879][-4.4288874 -4.4288349 -4.4287863 -4.4287257 -4.4287024 -4.428689 -4.4286695 -4.4286361 -4.4286418 -4.4286976 -4.428771 -4.4288373 -4.4288564 -4.4288397 -4.4288473][-4.4289403 -4.4288955 -4.4288454 -4.4287939 -4.4287815 -4.4287834 -4.4287782 -4.4287686 -4.4287753 -4.4288073 -4.4288511 -4.428896 -4.4289079 -4.42889 -4.4288874][-4.4289446 -4.4289165 -4.4288659 -4.4288263 -4.4288278 -4.4288387 -4.4288416 -4.42884 -4.4288421 -4.42886 -4.4288845 -4.4289031 -4.4289031 -4.4288893 -4.4288859][-4.4289179 -4.4288964 -4.4288464 -4.4288111 -4.4288158 -4.4288316 -4.4288464 -4.4288521 -4.4288521 -4.4288621 -4.428875 -4.4288759 -4.428865 -4.4288521 -4.4288487][-4.4289017 -4.4288855 -4.4288435 -4.4288111 -4.428812 -4.4288249 -4.4288421 -4.4288487 -4.42885 -4.4288588 -4.4288645 -4.4288592 -4.428844 -4.4288282 -4.4288225]]...]
INFO - root - 2017-12-08 07:39:23.577350: step 44110, loss = 2.28, batch loss = 2.23 (34.7 examples/sec; 0.231 sec/batch; 18h:28m:57s remains)
INFO - root - 2017-12-08 07:39:25.864329: step 44120, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.225 sec/batch; 18h:03m:19s remains)
INFO - root - 2017-12-08 07:39:28.090193: step 44130, loss = 2.28, batch loss = 2.23 (37.2 examples/sec; 0.215 sec/batch; 17h:14m:47s remains)
INFO - root - 2017-12-08 07:39:30.312912: step 44140, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 17h:37m:36s remains)
INFO - root - 2017-12-08 07:39:32.549070: step 44150, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 17h:36m:52s remains)
INFO - root - 2017-12-08 07:39:34.794679: step 44160, loss = 2.28, batch loss = 2.23 (35.8 examples/sec; 0.223 sec/batch; 17h:52m:27s remains)
INFO - root - 2017-12-08 07:39:37.042652: step 44170, loss = 2.28, batch loss = 2.23 (36.0 examples/sec; 0.222 sec/batch; 17h:48m:10s remains)
INFO - root - 2017-12-08 07:39:39.279954: step 44180, loss = 2.28, batch loss = 2.23 (34.8 examples/sec; 0.230 sec/batch; 18h:25m:07s remains)
INFO - root - 2017-12-08 07:39:41.509742: step 44190, loss = 2.28, batch loss = 2.23 (35.2 examples/sec; 0.227 sec/batch; 18h:12m:22s remains)
INFO - root - 2017-12-08 07:39:43.744822: step 44200, loss = 2.28, batch loss = 2.23 (33.4 examples/sec; 0.240 sec/batch; 19h:10m:57s remains)
2017-12-08 07:39:44.027816: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289269 -4.4289069 -4.4288993 -4.4288893 -4.428853 -4.4288244 -4.4288034 -4.4287882 -4.4287982 -4.4288082 -4.4287763 -4.4287195 -4.428669 -4.4286537 -4.42868][-4.42891 -4.4288878 -4.428884 -4.4288769 -4.4288526 -4.428834 -4.4288182 -4.4288177 -4.4288335 -4.42884 -4.4288087 -4.4287682 -4.4287443 -4.4287434 -4.4287639][-4.428895 -4.4288735 -4.428875 -4.4288721 -4.4288621 -4.42885 -4.4288392 -4.4288435 -4.4288511 -4.4288445 -4.428813 -4.4287806 -4.4287758 -4.4287891 -4.4288087][-4.4288793 -4.4288621 -4.4288673 -4.4288731 -4.4288778 -4.4288745 -4.4288607 -4.4288487 -4.4288387 -4.42882 -4.4287934 -4.4287734 -4.4287891 -4.4288239 -4.42885][-4.428854 -4.4288335 -4.4288387 -4.4288478 -4.42886 -4.4288678 -4.4288464 -4.428812 -4.4287834 -4.4287629 -4.4287486 -4.4287572 -4.4288073 -4.4288645 -4.4289007][-4.4288325 -4.4287996 -4.4287968 -4.4287958 -4.4287982 -4.4287987 -4.4287682 -4.42872 -4.42869 -4.4286885 -4.4287038 -4.4287519 -4.4288335 -4.4289031 -4.42894][-4.428822 -4.4287791 -4.4287634 -4.4287405 -4.42871 -4.428688 -4.4286528 -4.4286084 -4.4286022 -4.4286413 -4.4287009 -4.428782 -4.4288726 -4.4289374 -4.4289656][-4.4288211 -4.4287753 -4.4287515 -4.4287157 -4.4286542 -4.4286051 -4.4285617 -4.4285274 -4.4285426 -4.4286208 -4.4287133 -4.4288077 -4.4288936 -4.4289479 -4.4289603][-4.4288144 -4.4287715 -4.4287572 -4.4287319 -4.42867 -4.4286127 -4.428576 -4.428556 -4.4285789 -4.4286652 -4.4287572 -4.42884 -4.4289031 -4.4289312 -4.4289207][-4.428813 -4.4287815 -4.4287806 -4.4287815 -4.4287467 -4.4287081 -4.4286895 -4.4286857 -4.428709 -4.4287715 -4.4288344 -4.4288845 -4.4289141 -4.4289131 -4.4288807][-4.4288182 -4.4288 -4.4288158 -4.42884 -4.4288325 -4.4288216 -4.4288235 -4.4288306 -4.4288473 -4.4288783 -4.4289083 -4.4289293 -4.4289293 -4.4289007 -4.4288487][-4.428834 -4.4288263 -4.4288564 -4.4288912 -4.4288993 -4.4289002 -4.4289093 -4.4289212 -4.4289331 -4.4289427 -4.4289503 -4.4289536 -4.4289393 -4.4289031 -4.4288549][-4.4288721 -4.4288759 -4.4289136 -4.4289513 -4.4289637 -4.4289646 -4.42897 -4.4289751 -4.4289808 -4.4289846 -4.4289846 -4.4289784 -4.4289589 -4.4289227 -4.4288883][-4.4289236 -4.4289384 -4.4289727 -4.4290056 -4.4290171 -4.4290195 -4.4290214 -4.4290185 -4.4290204 -4.4290195 -4.4290128 -4.4290004 -4.4289804 -4.4289532 -4.4289331][-4.4289618 -4.428978 -4.4290047 -4.4290276 -4.4290366 -4.4290414 -4.4290433 -4.429038 -4.4290376 -4.4290361 -4.429028 -4.4290142 -4.4289975 -4.4289813 -4.4289722]]...]
INFO - root - 2017-12-08 07:39:46.277355: step 44210, loss = 2.28, batch loss = 2.23 (34.2 examples/sec; 0.234 sec/batch; 18h:45m:26s remains)
INFO - root - 2017-12-08 07:39:48.500323: step 44220, loss = 2.28, batch loss = 2.23 (37.1 examples/sec; 0.215 sec/batch; 17h:14m:41s remains)
INFO - root - 2017-12-08 07:39:50.762928: step 44230, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 17h:32m:58s remains)
INFO - root - 2017-12-08 07:39:53.026411: step 44240, loss = 2.28, batch loss = 2.23 (35.7 examples/sec; 0.224 sec/batch; 17h:56m:50s remains)
INFO - root - 2017-12-08 07:39:55.295437: step 44250, loss = 2.28, batch loss = 2.23 (35.0 examples/sec; 0.229 sec/batch; 18h:17m:56s remains)
INFO - root - 2017-12-08 07:39:57.524800: step 44260, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.222 sec/batch; 17h:44m:42s remains)
INFO - root - 2017-12-08 07:39:59.752157: step 44270, loss = 2.28, batch loss = 2.23 (37.4 examples/sec; 0.214 sec/batch; 17h:08m:02s remains)
INFO - root - 2017-12-08 07:40:02.004423: step 44280, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 17h:32m:48s remains)
INFO - root - 2017-12-08 07:40:04.252959: step 44290, loss = 2.28, batch loss = 2.23 (35.7 examples/sec; 0.224 sec/batch; 17h:56m:25s remains)
INFO - root - 2017-12-08 07:40:06.476920: step 44300, loss = 2.28, batch loss = 2.23 (34.8 examples/sec; 0.230 sec/batch; 18h:24m:02s remains)
2017-12-08 07:40:06.799163: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289665 -4.4289708 -4.4289646 -4.4289508 -4.428937 -4.4289317 -4.4289322 -4.4289355 -4.4289422 -4.4289494 -4.4289565 -4.4289627 -4.4289665 -4.4289656 -4.4289613][-4.4289613 -4.4289608 -4.4289465 -4.4289231 -4.4289017 -4.4288921 -4.42889 -4.4288936 -4.4289055 -4.4289231 -4.4289384 -4.4289484 -4.42895 -4.4289422 -4.4289327][-4.4289231 -4.4289107 -4.4288859 -4.4288526 -4.428823 -4.4288082 -4.4288006 -4.4288049 -4.4288263 -4.4288611 -4.4288845 -4.4288964 -4.4288921 -4.4288745 -4.4288654][-4.4288487 -4.42882 -4.4287796 -4.428731 -4.428688 -4.4286652 -4.4286528 -4.4286604 -4.428699 -4.4287539 -4.4287853 -4.4287963 -4.4287844 -4.4287615 -4.4287634][-4.4287672 -4.4287286 -4.4286704 -4.428607 -4.42855 -4.4285126 -4.4284921 -4.4285 -4.4285531 -4.4286251 -4.428668 -4.4286819 -4.4286666 -4.4286447 -4.4286613][-4.4286876 -4.4286513 -4.4285927 -4.4285326 -4.4284763 -4.4284291 -4.4283991 -4.4283972 -4.4284496 -4.4285269 -4.428575 -4.4285975 -4.4285884 -4.4285746 -4.4286032][-4.4286017 -4.4285736 -4.4285278 -4.4284854 -4.4284525 -4.4284143 -4.4283843 -4.4283724 -4.4284115 -4.4284763 -4.4285226 -4.4285555 -4.4285607 -4.4285564 -4.42859][-4.4285316 -4.42852 -4.4284997 -4.4284883 -4.4284825 -4.4284582 -4.4284315 -4.4284182 -4.428443 -4.4284906 -4.4285312 -4.4285688 -4.4285846 -4.4285851 -4.4286122][-4.4285245 -4.4285274 -4.4285283 -4.4285417 -4.4285545 -4.4285388 -4.4285212 -4.4285164 -4.4285331 -4.4285669 -4.4286003 -4.4286332 -4.4286513 -4.428647 -4.4286609][-4.4285765 -4.428586 -4.4285984 -4.4286227 -4.4286385 -4.4286256 -4.4286127 -4.4286175 -4.4286337 -4.42866 -4.4286876 -4.4287133 -4.4287291 -4.428719 -4.4287205][-4.4286523 -4.42866 -4.4286761 -4.4287 -4.4287124 -4.4287033 -4.4286942 -4.4286971 -4.4287076 -4.4287243 -4.4287429 -4.4287615 -4.4287786 -4.428771 -4.4287658][-4.4287195 -4.4287224 -4.4287386 -4.428761 -4.4287682 -4.42876 -4.4287524 -4.42875 -4.4287505 -4.4287548 -4.4287596 -4.4287691 -4.4287882 -4.4287853 -4.428781][-4.4287729 -4.4287748 -4.428791 -4.4288077 -4.4288068 -4.4287972 -4.4287887 -4.4287806 -4.4287724 -4.4287686 -4.4287634 -4.4287658 -4.4287839 -4.4287834 -4.428781][-4.4288049 -4.4288116 -4.4288273 -4.4288383 -4.4288321 -4.4288173 -4.428802 -4.428791 -4.4287844 -4.4287825 -4.4287729 -4.4287677 -4.4287796 -4.4287758 -4.4287715][-4.42881 -4.4288197 -4.4288344 -4.42884 -4.42883 -4.4288154 -4.4288015 -4.4287963 -4.4287977 -4.4287968 -4.4287806 -4.4287663 -4.4287682 -4.4287581 -4.4287534]]...]
INFO - root - 2017-12-08 07:40:09.022770: step 44310, loss = 2.28, batch loss = 2.23 (36.6 examples/sec; 0.218 sec/batch; 17h:28m:47s remains)
INFO - root - 2017-12-08 07:40:11.315821: step 44320, loss = 2.28, batch loss = 2.23 (34.8 examples/sec; 0.230 sec/batch; 18h:22m:52s remains)
INFO - root - 2017-12-08 07:40:13.546766: step 44330, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 17h:40m:59s remains)
INFO - root - 2017-12-08 07:40:15.770775: step 44340, loss = 2.28, batch loss = 2.23 (35.6 examples/sec; 0.225 sec/batch; 18h:00m:19s remains)
INFO - root - 2017-12-08 07:40:17.988605: step 44350, loss = 2.28, batch loss = 2.23 (35.7 examples/sec; 0.224 sec/batch; 17h:57m:37s remains)
INFO - root - 2017-12-08 07:40:20.217452: step 44360, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 17h:50m:50s remains)
INFO - root - 2017-12-08 07:40:22.448204: step 44370, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.222 sec/batch; 17h:44m:45s remains)
INFO - root - 2017-12-08 07:40:24.695711: step 44380, loss = 2.28, batch loss = 2.23 (34.0 examples/sec; 0.236 sec/batch; 18h:51m:02s remains)
INFO - root - 2017-12-08 07:40:26.971691: step 44390, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.222 sec/batch; 17h:44m:03s remains)
INFO - root - 2017-12-08 07:40:29.230346: step 44400, loss = 2.28, batch loss = 2.23 (36.6 examples/sec; 0.219 sec/batch; 17h:29m:24s remains)
2017-12-08 07:40:29.549101: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.42889 -4.4289074 -4.4289279 -4.42893 -4.4289165 -4.4288826 -4.4288568 -4.4288564 -4.4288678 -4.4288783 -4.4288936 -4.4289007 -4.4288859 -4.4288678 -4.4288583][-4.4288349 -4.4288573 -4.4288816 -4.4288878 -4.4288797 -4.428843 -4.4288073 -4.4287987 -4.4288225 -4.4288521 -4.4288778 -4.4288859 -4.428863 -4.4288354 -4.4288244][-4.428771 -4.4287953 -4.42882 -4.42883 -4.4288211 -4.4287882 -4.4287448 -4.4287195 -4.4287491 -4.4288039 -4.4288459 -4.4288616 -4.4288383 -4.4287992 -4.428772][-4.4287119 -4.4287343 -4.4287548 -4.4287629 -4.4287477 -4.4287124 -4.4286594 -4.4286194 -4.4286556 -4.4287391 -4.4288049 -4.4288259 -4.4288 -4.42875 -4.4287014][-4.4286909 -4.4287081 -4.4287219 -4.4287195 -4.4286885 -4.4286323 -4.4285641 -4.4285231 -4.4285736 -4.4286852 -4.4287705 -4.4287953 -4.4287744 -4.4287252 -4.4286661][-4.4287047 -4.4287062 -4.4287038 -4.4286828 -4.4286232 -4.4285316 -4.4284506 -4.42842 -4.4284921 -4.4286218 -4.4287233 -4.42876 -4.4287605 -4.4287415 -4.4287024][-4.4287219 -4.4286938 -4.4286656 -4.4286227 -4.4285283 -4.4283857 -4.4282618 -4.4282374 -4.4283566 -4.4285207 -4.4286375 -4.4286947 -4.4287195 -4.4287367 -4.4287395][-4.4287319 -4.4286852 -4.4286356 -4.428576 -4.4284563 -4.42827 -4.4280977 -4.4280734 -4.4282336 -4.4284167 -4.42853 -4.4285879 -4.4286265 -4.4286733 -4.4287195][-4.4287376 -4.428688 -4.4286346 -4.4285846 -4.42849 -4.4283442 -4.4282174 -4.4282103 -4.4283395 -4.4284649 -4.4285173 -4.4285336 -4.4285679 -4.4286256 -4.4286871][-4.4287472 -4.4286981 -4.4286566 -4.428627 -4.4285665 -4.4284935 -4.42845 -4.4284654 -4.4285421 -4.4285955 -4.4285874 -4.4285655 -4.4285822 -4.428628 -4.4286714][-4.4287405 -4.4286909 -4.4286604 -4.4286413 -4.4285908 -4.4285541 -4.4285612 -4.4285936 -4.42864 -4.4286566 -4.428627 -4.4285908 -4.4285975 -4.4286308 -4.4286556][-4.4286942 -4.4286537 -4.4286351 -4.4286156 -4.4285607 -4.4285288 -4.4285522 -4.4285908 -4.4286256 -4.4286351 -4.428607 -4.428566 -4.4285645 -4.4285889 -4.4286022][-4.4286404 -4.4286222 -4.42862 -4.428606 -4.4285545 -4.4285164 -4.4285321 -4.4285588 -4.4285765 -4.4285822 -4.4285588 -4.428515 -4.428503 -4.4285164 -4.4285207][-4.428638 -4.4286485 -4.4286666 -4.4286675 -4.4286313 -4.4285917 -4.4285855 -4.4285874 -4.4285836 -4.4285812 -4.4285617 -4.4285178 -4.42849 -4.428484 -4.4284768][-4.4287043 -4.4287343 -4.4287639 -4.4287748 -4.4287505 -4.4287119 -4.4286904 -4.4286752 -4.4286613 -4.4286561 -4.4286456 -4.4286118 -4.4285841 -4.4285703 -4.4285555]]...]
INFO - root - 2017-12-08 07:40:31.801476: step 44410, loss = 2.28, batch loss = 2.23 (35.7 examples/sec; 0.224 sec/batch; 17h:55m:02s remains)
INFO - root - 2017-12-08 07:40:34.032430: step 44420, loss = 2.28, batch loss = 2.23 (35.0 examples/sec; 0.228 sec/batch; 18h:16m:41s remains)
INFO - root - 2017-12-08 07:40:36.244386: step 44430, loss = 2.28, batch loss = 2.23 (35.8 examples/sec; 0.223 sec/batch; 17h:52m:22s remains)
INFO - root - 2017-12-08 07:40:38.499923: step 44440, loss = 2.28, batch loss = 2.23 (37.0 examples/sec; 0.216 sec/batch; 17h:19m:23s remains)
INFO - root - 2017-12-08 07:40:40.729945: step 44450, loss = 2.28, batch loss = 2.23 (34.2 examples/sec; 0.234 sec/batch; 18h:42m:13s remains)
INFO - root - 2017-12-08 07:40:42.957337: step 44460, loss = 2.28, batch loss = 2.23 (34.0 examples/sec; 0.236 sec/batch; 18h:51m:02s remains)
INFO - root - 2017-12-08 07:40:45.226934: step 44470, loss = 2.28, batch loss = 2.23 (34.9 examples/sec; 0.229 sec/batch; 18h:20m:40s remains)
INFO - root - 2017-12-08 07:40:47.488579: step 44480, loss = 2.28, batch loss = 2.23 (33.5 examples/sec; 0.239 sec/batch; 19h:04m:53s remains)
INFO - root - 2017-12-08 07:40:49.761232: step 44490, loss = 2.28, batch loss = 2.23 (34.5 examples/sec; 0.232 sec/batch; 18h:33m:38s remains)
INFO - root - 2017-12-08 07:40:52.001149: step 44500, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.221 sec/batch; 17h:38m:42s remains)
2017-12-08 07:40:52.285777: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289145 -4.4289074 -4.4289 -4.4288883 -4.4288735 -4.4288568 -4.4288478 -4.4288545 -4.4288645 -4.428865 -4.4288392 -4.4287891 -4.4287367 -4.4287114 -4.4287138][-4.4288716 -4.4288592 -4.4288349 -4.4288025 -4.4287729 -4.4287491 -4.4287381 -4.4287534 -4.428781 -4.4287987 -4.4287796 -4.4287271 -4.4286766 -4.4286609 -4.4286742][-4.4288397 -4.4288192 -4.4287729 -4.4287143 -4.4286666 -4.4286351 -4.4286261 -4.4286547 -4.42871 -4.4287591 -4.4287577 -4.4287124 -4.42867 -4.4286618 -4.42868][-4.42883 -4.428802 -4.4287362 -4.4286528 -4.4285831 -4.4285345 -4.4285183 -4.4285588 -4.4286518 -4.4287486 -4.4287815 -4.4287524 -4.4287109 -4.4286981 -4.42871][-4.4288435 -4.4288182 -4.4287419 -4.4286356 -4.4285345 -4.4284482 -4.4283977 -4.4284444 -4.4285827 -4.428741 -4.4288216 -4.42882 -4.4287806 -4.4287534 -4.4287467][-4.4288545 -4.4288425 -4.4287705 -4.4286432 -4.4284897 -4.4283276 -4.4282069 -4.4282537 -4.4284616 -4.4286928 -4.4288325 -4.4288673 -4.4288397 -4.428803 -4.4287767][-4.4288535 -4.4288535 -4.4287906 -4.428648 -4.4284296 -4.4281588 -4.42793 -4.4279795 -4.42828 -4.428597 -4.4287958 -4.4288692 -4.4288669 -4.4288311 -4.4287887][-4.42885 -4.4288578 -4.4288011 -4.4286423 -4.4283657 -4.4279923 -4.427649 -4.4277039 -4.4281082 -4.428504 -4.4287491 -4.4288578 -4.4288797 -4.428844 -4.4287825][-4.42884 -4.4288554 -4.4288111 -4.4286585 -4.4283748 -4.4279737 -4.427578 -4.4276228 -4.4280624 -4.4284778 -4.4287291 -4.4288478 -4.4288826 -4.4288449 -4.4287634][-4.4288297 -4.4288607 -4.4288392 -4.4287205 -4.4284945 -4.4281683 -4.427845 -4.4278703 -4.4282236 -4.4285693 -4.42877 -4.4288611 -4.4288864 -4.4288411 -4.4287419][-4.4288092 -4.4288564 -4.4288621 -4.4287882 -4.4286318 -4.4284067 -4.42819 -4.428206 -4.428443 -4.4286828 -4.42882 -4.4288735 -4.4288788 -4.4288249 -4.4287195][-4.428781 -4.4288349 -4.428863 -4.4288273 -4.4287219 -4.4285684 -4.4284282 -4.4284387 -4.4285922 -4.4287562 -4.4288492 -4.4288764 -4.428863 -4.4288054 -4.4287086][-4.4287782 -4.4288287 -4.4288645 -4.4288554 -4.4287882 -4.4286823 -4.428587 -4.4285917 -4.4286976 -4.4288173 -4.428885 -4.4289002 -4.4288754 -4.4288154 -4.4287362][-4.4288144 -4.4288578 -4.4288907 -4.4288926 -4.4288521 -4.4287777 -4.42871 -4.4287109 -4.4287848 -4.4288707 -4.4289203 -4.4289322 -4.4289074 -4.4288559 -4.4287987][-4.4288774 -4.4289093 -4.4289312 -4.4289374 -4.4289184 -4.4288726 -4.4288254 -4.4288206 -4.4288678 -4.428926 -4.4289589 -4.4289627 -4.4289412 -4.4289064 -4.4288735]]...]
INFO - root - 2017-12-08 07:40:54.517638: step 44510, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.222 sec/batch; 17h:44m:13s remains)
INFO - root - 2017-12-08 07:40:56.742577: step 44520, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 17h:48m:59s remains)
INFO - root - 2017-12-08 07:40:58.971725: step 44530, loss = 2.28, batch loss = 2.23 (37.1 examples/sec; 0.216 sec/batch; 17h:15m:10s remains)
INFO - root - 2017-12-08 07:41:01.213115: step 44540, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 17h:42m:02s remains)
INFO - root - 2017-12-08 07:41:03.491211: step 44550, loss = 2.28, batch loss = 2.23 (35.6 examples/sec; 0.225 sec/batch; 17h:58m:39s remains)
INFO - root - 2017-12-08 07:41:05.754390: step 44560, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.225 sec/batch; 18h:00m:48s remains)
INFO - root - 2017-12-08 07:41:07.991608: step 44570, loss = 2.28, batch loss = 2.23 (34.9 examples/sec; 0.229 sec/batch; 18h:20m:30s remains)
INFO - root - 2017-12-08 07:41:10.212078: step 44580, loss = 2.28, batch loss = 2.23 (37.1 examples/sec; 0.216 sec/batch; 17h:14m:56s remains)
INFO - root - 2017-12-08 07:41:12.443921: step 44590, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 17h:33m:19s remains)
INFO - root - 2017-12-08 07:41:14.675083: step 44600, loss = 2.28, batch loss = 2.23 (37.6 examples/sec; 0.213 sec/batch; 16h:59m:53s remains)
2017-12-08 07:41:14.982105: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289308 -4.4289284 -4.4289241 -4.4289231 -4.4289255 -4.4289317 -4.4289403 -4.4289465 -4.4289479 -4.4289484 -4.4289508 -4.428956 -4.428966 -4.4289775 -4.4289856][-4.4289021 -4.4288945 -4.428885 -4.4288836 -4.4288869 -4.4288955 -4.4289074 -4.428916 -4.4289179 -4.4289179 -4.4289203 -4.4289279 -4.4289427 -4.4289579 -4.4289694][-4.4288659 -4.4288487 -4.428833 -4.4288287 -4.4288321 -4.4288425 -4.4288549 -4.4288654 -4.4288697 -4.4288721 -4.4288759 -4.4288864 -4.4289069 -4.4289265 -4.4289417][-4.42883 -4.4287953 -4.4287667 -4.42876 -4.4287639 -4.4287744 -4.428781 -4.4287906 -4.4288025 -4.4288116 -4.4288192 -4.4288335 -4.4288597 -4.428885 -4.4289088][-4.4287944 -4.4287405 -4.4286985 -4.428688 -4.4286871 -4.4286861 -4.4286771 -4.4286833 -4.4287095 -4.4287338 -4.4287491 -4.4287677 -4.4287996 -4.4288325 -4.4288688][-4.4287481 -4.4286828 -4.4286337 -4.4286151 -4.4285984 -4.4285736 -4.4285383 -4.4285388 -4.4285874 -4.4286389 -4.4286652 -4.4286857 -4.4287224 -4.4287691 -4.4288235][-4.4287047 -4.42864 -4.428586 -4.4285569 -4.4285116 -4.428441 -4.4283595 -4.428359 -4.4284444 -4.42853 -4.4285741 -4.4285941 -4.4286337 -4.4286966 -4.428772][-4.4286714 -4.4286108 -4.4285588 -4.4285245 -4.428452 -4.4283276 -4.4282002 -4.4282169 -4.4283433 -4.4284582 -4.4285054 -4.4285164 -4.4285512 -4.4286213 -4.4287124][-4.4286509 -4.4285893 -4.4285369 -4.4285107 -4.428443 -4.4283094 -4.4281778 -4.4282207 -4.4283557 -4.4284558 -4.4284763 -4.4284639 -4.4284887 -4.428555 -4.4286509][-4.428659 -4.4285913 -4.42853 -4.4285107 -4.4284706 -4.4283805 -4.428298 -4.4283514 -4.4284477 -4.4284992 -4.4284797 -4.4284458 -4.4284558 -4.4285049 -4.4285946][-4.4286838 -4.42861 -4.4285431 -4.4285226 -4.428504 -4.4284635 -4.4284353 -4.4284825 -4.4285326 -4.428545 -4.4285049 -4.4284663 -4.4284654 -4.4284878 -4.4285588][-4.4287462 -4.4286771 -4.428616 -4.4285932 -4.4285851 -4.4285784 -4.4285789 -4.4286089 -4.4286318 -4.4286327 -4.4286003 -4.4285731 -4.4285674 -4.4285655 -4.4286065][-4.4288306 -4.4287791 -4.4287305 -4.4287105 -4.4287081 -4.4287148 -4.4287229 -4.4287429 -4.4287581 -4.42876 -4.4287424 -4.428731 -4.4287262 -4.4287152 -4.4287324][-4.4289045 -4.4288778 -4.4288507 -4.42884 -4.4288411 -4.4288497 -4.4288573 -4.4288664 -4.4288754 -4.4288807 -4.4288778 -4.4288793 -4.4288788 -4.4288678 -4.42887][-4.4289551 -4.428947 -4.4289355 -4.4289327 -4.4289341 -4.42894 -4.4289451 -4.42895 -4.4289532 -4.4289575 -4.42896 -4.4289651 -4.4289665 -4.42896 -4.4289579]]...]
INFO - root - 2017-12-08 07:41:17.215478: step 44610, loss = 2.28, batch loss = 2.23 (35.8 examples/sec; 0.224 sec/batch; 17h:52m:41s remains)
INFO - root - 2017-12-08 07:41:19.456183: step 44620, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.222 sec/batch; 17h:44m:25s remains)
INFO - root - 2017-12-08 07:41:21.713811: step 44630, loss = 2.28, batch loss = 2.23 (33.0 examples/sec; 0.243 sec/batch; 19h:24m:31s remains)
INFO - root - 2017-12-08 07:41:23.965181: step 44640, loss = 2.28, batch loss = 2.23 (37.1 examples/sec; 0.216 sec/batch; 17h:14m:24s remains)
INFO - root - 2017-12-08 07:41:26.223976: step 44650, loss = 2.28, batch loss = 2.23 (36.7 examples/sec; 0.218 sec/batch; 17h:27m:11s remains)
INFO - root - 2017-12-08 07:41:28.470252: step 44660, loss = 2.28, batch loss = 2.23 (35.7 examples/sec; 0.224 sec/batch; 17h:55m:32s remains)
INFO - root - 2017-12-08 07:41:30.765800: step 44670, loss = 2.28, batch loss = 2.23 (34.5 examples/sec; 0.232 sec/batch; 18h:33m:58s remains)
INFO - root - 2017-12-08 07:41:32.999780: step 44680, loss = 2.28, batch loss = 2.23 (34.9 examples/sec; 0.229 sec/batch; 18h:18m:11s remains)
INFO - root - 2017-12-08 07:41:35.284367: step 44690, loss = 2.28, batch loss = 2.23 (36.6 examples/sec; 0.219 sec/batch; 17h:29m:40s remains)
INFO - root - 2017-12-08 07:41:37.508690: step 44700, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 17h:48m:54s remains)
2017-12-08 07:41:37.801836: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289551 -4.4289589 -4.4289532 -4.4289412 -4.4289265 -4.4289122 -4.4289055 -4.4289103 -4.4289236 -4.4289379 -4.4289494 -4.4289575 -4.4289603 -4.4289608 -4.428957][-4.4289479 -4.4289565 -4.4289565 -4.4289503 -4.4289403 -4.4289312 -4.4289274 -4.4289322 -4.4289436 -4.4289556 -4.4289651 -4.4289708 -4.4289694 -4.4289646 -4.4289584][-4.4289451 -4.4289608 -4.4289675 -4.4289646 -4.4289541 -4.4289422 -4.4289336 -4.4289331 -4.4289455 -4.4289632 -4.42898 -4.4289894 -4.4289865 -4.4289761 -4.4289646][-4.4289513 -4.428968 -4.4289756 -4.4289665 -4.4289432 -4.428916 -4.4288912 -4.4288774 -4.42889 -4.4289246 -4.4289646 -4.4289932 -4.429 -4.4289913 -4.4289765][-4.4289608 -4.4289727 -4.4289722 -4.4289494 -4.4289041 -4.4288516 -4.4288 -4.4287658 -4.4287782 -4.4288368 -4.4289117 -4.4289694 -4.4289942 -4.4289947 -4.4289827][-4.428956 -4.4289622 -4.4289536 -4.4289165 -4.42885 -4.4287672 -4.4286795 -4.4286151 -4.4286232 -4.4287086 -4.4288249 -4.4289165 -4.4289632 -4.4289789 -4.4289751][-4.4289217 -4.4289207 -4.428906 -4.4288611 -4.4287786 -4.4286671 -4.4285426 -4.4284434 -4.4284387 -4.4285474 -4.4287052 -4.4288354 -4.42891 -4.428946 -4.4289556][-4.428863 -4.4288607 -4.4288464 -4.4288039 -4.4287162 -4.4285874 -4.4284363 -4.4283085 -4.4282866 -4.4284043 -4.4285879 -4.4287457 -4.4288454 -4.4289017 -4.428926][-4.4288044 -4.4288092 -4.4288039 -4.4287753 -4.4287038 -4.4285879 -4.4284425 -4.4283085 -4.428267 -4.4283662 -4.4285374 -4.4286928 -4.4287996 -4.428863 -4.4288945][-4.428793 -4.4288058 -4.4288135 -4.4288058 -4.4287682 -4.4286966 -4.428596 -4.4284954 -4.4284539 -4.4285078 -4.428618 -4.4287286 -4.4288116 -4.4288626 -4.4288883][-4.4288754 -4.4288883 -4.4288983 -4.4289002 -4.4288859 -4.4288521 -4.4287977 -4.4287376 -4.4287052 -4.4287224 -4.4287748 -4.4288325 -4.4288778 -4.4289055 -4.4289207][-4.4289374 -4.4289412 -4.428946 -4.4289517 -4.4289513 -4.4289403 -4.428916 -4.4288859 -4.428864 -4.428863 -4.42888 -4.4289002 -4.428915 -4.4289231 -4.4289293][-4.4289303 -4.4289231 -4.4289222 -4.4289312 -4.4289436 -4.4289494 -4.4289446 -4.4289341 -4.4289222 -4.4289155 -4.4289131 -4.4289083 -4.4289017 -4.428895 -4.4288945][-4.4288878 -4.4288678 -4.4288583 -4.4288692 -4.4288917 -4.4289107 -4.4289179 -4.4289188 -4.428915 -4.4289074 -4.4288945 -4.4288754 -4.428854 -4.4288368 -4.428833][-4.4288135 -4.4287782 -4.4287591 -4.42877 -4.4287987 -4.4288259 -4.4288392 -4.4288464 -4.42885 -4.4288454 -4.4288321 -4.4288082 -4.4287791 -4.4287558 -4.4287496]]...]
INFO - root - 2017-12-08 07:41:40.053599: step 44710, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 17h:34m:35s remains)
INFO - root - 2017-12-08 07:41:42.283846: step 44720, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.222 sec/batch; 17h:43m:09s remains)
INFO - root - 2017-12-08 07:41:44.551894: step 44730, loss = 2.28, batch loss = 2.23 (34.5 examples/sec; 0.232 sec/batch; 18h:33m:42s remains)
INFO - root - 2017-12-08 07:41:46.803740: step 44740, loss = 2.28, batch loss = 2.23 (33.9 examples/sec; 0.236 sec/batch; 18h:51m:03s remains)
INFO - root - 2017-12-08 07:41:49.054559: step 44750, loss = 2.28, batch loss = 2.23 (34.3 examples/sec; 0.233 sec/batch; 18h:37m:26s remains)
INFO - root - 2017-12-08 07:41:51.260209: step 44760, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 17h:40m:29s remains)
INFO - root - 2017-12-08 07:41:53.516651: step 44770, loss = 2.28, batch loss = 2.23 (35.7 examples/sec; 0.224 sec/batch; 17h:53m:49s remains)
INFO - root - 2017-12-08 07:41:55.748454: step 44780, loss = 2.28, batch loss = 2.23 (35.8 examples/sec; 0.224 sec/batch; 17h:52m:17s remains)
INFO - root - 2017-12-08 07:41:58.000341: step 44790, loss = 2.28, batch loss = 2.23 (33.9 examples/sec; 0.236 sec/batch; 18h:51m:34s remains)
INFO - root - 2017-12-08 07:42:00.239373: step 44800, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.221 sec/batch; 17h:37m:29s remains)
2017-12-08 07:42:00.513155: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289727 -4.4289751 -4.4289765 -4.4289823 -4.4289861 -4.4289846 -4.4289846 -4.4289861 -4.42899 -4.4289937 -4.4289989 -4.4290066 -4.4290109 -4.4290128 -4.429019][-4.4289393 -4.428936 -4.4289365 -4.4289455 -4.4289484 -4.4289427 -4.4289389 -4.4289451 -4.4289546 -4.4289594 -4.4289665 -4.42898 -4.4289927 -4.4289966 -4.4290013][-4.428885 -4.4288707 -4.4288654 -4.4288754 -4.4288778 -4.4288673 -4.428863 -4.4288793 -4.4288936 -4.4288926 -4.4288974 -4.4289188 -4.4289451 -4.4289584 -4.4289665][-4.4288287 -4.4287925 -4.42877 -4.4287667 -4.4287591 -4.4287443 -4.4287462 -4.4287853 -4.428813 -4.4288034 -4.4287963 -4.4288206 -4.4288616 -4.4288912 -4.4289093][-4.4287848 -4.4287281 -4.4286804 -4.4286523 -4.4286256 -4.4285975 -4.4286008 -4.4286747 -4.4287353 -4.4287329 -4.428709 -4.4287233 -4.4287672 -4.4288092 -4.4288378][-4.4287543 -4.4286919 -4.4286304 -4.4285736 -4.4285083 -4.4284372 -4.4284124 -4.4285126 -4.4286175 -4.4286437 -4.4286213 -4.428627 -4.4286742 -4.42873 -4.4287705][-4.4287195 -4.4286642 -4.42861 -4.4285483 -4.4284506 -4.4283171 -4.4282317 -4.428328 -4.4284706 -4.4285417 -4.4285407 -4.42855 -4.4286036 -4.4286704 -4.42872][-4.4286838 -4.4286375 -4.4286103 -4.4285789 -4.42849 -4.4283347 -4.4281993 -4.4282379 -4.4283643 -4.4284577 -4.4284811 -4.428493 -4.42855 -4.4286261 -4.4286804][-4.4286671 -4.4286304 -4.4286284 -4.4286342 -4.4285822 -4.4284616 -4.4283285 -4.4283128 -4.4283748 -4.4284463 -4.4284697 -4.4284706 -4.4285083 -4.4285836 -4.4286485][-4.4286523 -4.42862 -4.4286308 -4.428659 -4.4286356 -4.4285603 -4.428463 -4.4284329 -4.4284444 -4.4284806 -4.4285007 -4.4284968 -4.428514 -4.4285717 -4.4286427][-4.428647 -4.4286079 -4.42862 -4.4286566 -4.4286513 -4.4286084 -4.4285464 -4.4285274 -4.4285235 -4.4285374 -4.4285541 -4.4285507 -4.4285603 -4.4286079 -4.4286766][-4.4286737 -4.4286318 -4.4286337 -4.4286633 -4.4286633 -4.4286427 -4.4286141 -4.4286127 -4.4286084 -4.428607 -4.4286141 -4.4286118 -4.4286218 -4.42867 -4.4287329][-4.4287248 -4.42869 -4.4286861 -4.4287043 -4.4287043 -4.4286971 -4.4286909 -4.4286976 -4.4286938 -4.4286809 -4.4286761 -4.4286728 -4.4286814 -4.4287281 -4.4287834][-4.4287658 -4.4287415 -4.4287429 -4.4287596 -4.4287615 -4.4287586 -4.4287596 -4.4287648 -4.4287634 -4.4287519 -4.4287467 -4.4287438 -4.4287472 -4.4287815 -4.4288278][-4.4288054 -4.4287925 -4.4288025 -4.4288249 -4.4288344 -4.428834 -4.4288321 -4.4288325 -4.4288363 -4.4288344 -4.4288335 -4.4288287 -4.4288254 -4.4288435 -4.4288764]]...]
INFO - root - 2017-12-08 07:42:02.726147: step 44810, loss = 2.28, batch loss = 2.23 (35.4 examples/sec; 0.226 sec/batch; 18h:04m:05s remains)
INFO - root - 2017-12-08 07:42:04.971416: step 44820, loss = 2.28, batch loss = 2.23 (34.5 examples/sec; 0.232 sec/batch; 18h:33m:12s remains)
INFO - root - 2017-12-08 07:42:07.220285: step 44830, loss = 2.28, batch loss = 2.23 (37.6 examples/sec; 0.213 sec/batch; 17h:00m:42s remains)
INFO - root - 2017-12-08 07:42:09.460536: step 44840, loss = 2.28, batch loss = 2.23 (35.3 examples/sec; 0.227 sec/batch; 18h:06m:04s remains)
INFO - root - 2017-12-08 07:42:11.671483: step 44850, loss = 2.28, batch loss = 2.23 (36.8 examples/sec; 0.217 sec/batch; 17h:21m:25s remains)
INFO - root - 2017-12-08 07:42:13.942524: step 44860, loss = 2.28, batch loss = 2.23 (35.6 examples/sec; 0.225 sec/batch; 17h:57m:57s remains)
INFO - root - 2017-12-08 07:42:16.173084: step 44870, loss = 2.28, batch loss = 2.23 (35.4 examples/sec; 0.226 sec/batch; 18h:03m:55s remains)
INFO - root - 2017-12-08 07:42:18.415161: step 44880, loss = 2.28, batch loss = 2.23 (35.2 examples/sec; 0.227 sec/batch; 18h:09m:38s remains)
INFO - root - 2017-12-08 07:42:20.629874: step 44890, loss = 2.28, batch loss = 2.23 (37.1 examples/sec; 0.216 sec/batch; 17h:13m:55s remains)
INFO - root - 2017-12-08 07:42:22.889727: step 44900, loss = 2.28, batch loss = 2.23 (34.9 examples/sec; 0.229 sec/batch; 18h:17m:47s remains)
2017-12-08 07:42:23.197085: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288917 -4.4288936 -4.4288855 -4.4288664 -4.4288559 -4.4288511 -4.4288416 -4.4288211 -4.4288015 -4.4288106 -4.428823 -4.4288387 -4.4288578 -4.4288678 -4.4288707][-4.4288416 -4.4288521 -4.4288487 -4.4288282 -4.4288225 -4.4288254 -4.42881 -4.4287767 -4.4287529 -4.4287677 -4.4287863 -4.4288025 -4.428823 -4.4288363 -4.4288406][-4.428772 -4.428791 -4.4287958 -4.4287724 -4.4287643 -4.4287734 -4.4287543 -4.4287128 -4.4286823 -4.4287038 -4.428731 -4.4287534 -4.4287753 -4.428791 -4.4287944][-4.4287214 -4.4287395 -4.4287562 -4.4287319 -4.4287124 -4.4287167 -4.4286909 -4.42863 -4.4285884 -4.4286218 -4.4286795 -4.4287233 -4.4287577 -4.4287791 -4.4287806][-4.42868 -4.4286947 -4.4287257 -4.4287071 -4.4286776 -4.4286618 -4.4286032 -4.4284883 -4.4284153 -4.4284678 -4.4285755 -4.4286575 -4.4287229 -4.4287643 -4.4287763][-4.4286242 -4.4286394 -4.4286828 -4.4286766 -4.4286451 -4.4286 -4.4284749 -4.4282641 -4.4281368 -4.4282427 -4.42842 -4.4285464 -4.4286551 -4.4287333 -4.4287677][-4.428544 -4.4285784 -4.4286351 -4.4286394 -4.428596 -4.4285135 -4.4283061 -4.4279747 -4.4277997 -4.4280138 -4.4282722 -4.4284382 -4.4285812 -4.428689 -4.4287438][-4.4285 -4.4285645 -4.4286342 -4.4286423 -4.4285765 -4.4284663 -4.4282269 -4.4278574 -4.4276776 -4.4279647 -4.4282589 -4.4284306 -4.4285707 -4.42868 -4.4287362][-4.4285445 -4.4286122 -4.4286842 -4.4286962 -4.4286361 -4.4285455 -4.4283724 -4.4281254 -4.4280257 -4.4282255 -4.4284353 -4.4285588 -4.4286551 -4.4287333 -4.4287696][-4.42865 -4.4286928 -4.4287539 -4.4287682 -4.4287324 -4.4286804 -4.4285917 -4.42847 -4.42843 -4.4285283 -4.4286423 -4.4287114 -4.4287624 -4.4288077 -4.4288173][-4.4287643 -4.4287839 -4.4288235 -4.4288297 -4.4288073 -4.4287815 -4.4287343 -4.4286757 -4.428659 -4.4287105 -4.4287786 -4.4288244 -4.4288583 -4.4288769 -4.4288716][-4.4288507 -4.4288583 -4.4288764 -4.4288635 -4.4288321 -4.4288034 -4.4287667 -4.4287229 -4.4287133 -4.4287658 -4.4288278 -4.4288764 -4.4289074 -4.4289174 -4.4289103][-4.4288874 -4.428885 -4.4288845 -4.428863 -4.4288311 -4.4287963 -4.4287577 -4.4287124 -4.4287076 -4.4287643 -4.4288282 -4.4288788 -4.4289112 -4.4289217 -4.4289227][-4.4288621 -4.4288573 -4.4288473 -4.42883 -4.428812 -4.4287839 -4.4287496 -4.428709 -4.4287 -4.4287534 -4.4288158 -4.428864 -4.4288969 -4.4289126 -4.4289217][-4.4288416 -4.4288363 -4.4288292 -4.4288263 -4.4288287 -4.4288197 -4.4287949 -4.4287515 -4.4287314 -4.4287748 -4.4288306 -4.4288678 -4.4288945 -4.4289107 -4.4289222]]...]
INFO - root - 2017-12-08 07:42:25.458013: step 44910, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.225 sec/batch; 17h:59m:18s remains)
INFO - root - 2017-12-08 07:42:27.714109: step 44920, loss = 2.28, batch loss = 2.23 (34.9 examples/sec; 0.229 sec/batch; 18h:19m:47s remains)
INFO - root - 2017-12-08 07:42:29.939469: step 44930, loss = 2.28, batch loss = 2.23 (35.1 examples/sec; 0.228 sec/batch; 18h:11m:35s remains)
INFO - root - 2017-12-08 07:42:32.192113: step 44940, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.225 sec/batch; 18h:00m:08s remains)
INFO - root - 2017-12-08 07:42:34.425274: step 44950, loss = 2.28, batch loss = 2.23 (36.8 examples/sec; 0.218 sec/batch; 17h:23m:04s remains)
INFO - root - 2017-12-08 07:42:36.665887: step 44960, loss = 2.28, batch loss = 2.23 (35.2 examples/sec; 0.227 sec/batch; 18h:09m:32s remains)
INFO - root - 2017-12-08 07:42:38.911043: step 44970, loss = 2.28, batch loss = 2.23 (35.7 examples/sec; 0.224 sec/batch; 17h:54m:27s remains)
INFO - root - 2017-12-08 07:42:41.163492: step 44980, loss = 2.28, batch loss = 2.23 (35.0 examples/sec; 0.229 sec/batch; 18h:15m:26s remains)
INFO - root - 2017-12-08 07:42:43.386605: step 44990, loss = 2.28, batch loss = 2.23 (35.1 examples/sec; 0.228 sec/batch; 18h:12m:15s remains)
INFO - root - 2017-12-08 07:42:45.678116: step 45000, loss = 2.28, batch loss = 2.23 (37.4 examples/sec; 0.214 sec/batch; 17h:04m:19s remains)
2017-12-08 07:42:45.966789: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4287992 -4.4287672 -4.4287391 -4.4287319 -4.4287505 -4.4287705 -4.4287839 -4.4287891 -4.4288044 -4.428822 -4.4288321 -4.4288349 -4.428843 -4.4288583 -4.4288697][-4.4287739 -4.4287434 -4.428721 -4.4287128 -4.4287295 -4.4287457 -4.4287481 -4.4287443 -4.4287539 -4.4287696 -4.4287839 -4.4287953 -4.4288049 -4.4288216 -4.4288387][-4.4287596 -4.4287238 -4.4287009 -4.4286866 -4.4286957 -4.4287086 -4.428709 -4.4287119 -4.4287291 -4.4287448 -4.4287605 -4.4287829 -4.4287963 -4.4288087 -4.4288149][-4.4287305 -4.4286904 -4.4286656 -4.428638 -4.4286313 -4.4286518 -4.4286642 -4.4286785 -4.4287119 -4.4287348 -4.4287391 -4.42875 -4.4287596 -4.4287758 -4.4287772][-4.4287128 -4.4286728 -4.4286542 -4.4286151 -4.4285922 -4.4286051 -4.4286265 -4.4286504 -4.4286923 -4.4287167 -4.4287043 -4.4286928 -4.4286976 -4.4287252 -4.4287362][-4.4287219 -4.4286809 -4.428658 -4.4286065 -4.4285679 -4.4285574 -4.4285774 -4.428606 -4.4286389 -4.4286518 -4.4286284 -4.4286122 -4.4286304 -4.4286761 -4.4286976][-4.4287262 -4.428678 -4.4286337 -4.4285617 -4.4285007 -4.4284744 -4.428493 -4.428535 -4.42856 -4.428566 -4.4285464 -4.428534 -4.4285717 -4.4286318 -4.4286556][-4.4287038 -4.4286523 -4.4286003 -4.4285274 -4.4284606 -4.4284239 -4.4284472 -4.4285054 -4.4285393 -4.4285507 -4.4285345 -4.42852 -4.4285579 -4.4286175 -4.428628][-4.4286914 -4.4286566 -4.4286256 -4.4285707 -4.4285164 -4.4284792 -4.4284973 -4.428546 -4.4285736 -4.4285979 -4.4285936 -4.4285755 -4.4285932 -4.4286413 -4.4286518][-4.4287353 -4.4287329 -4.4287348 -4.4286962 -4.4286418 -4.4285955 -4.428597 -4.4286189 -4.4286275 -4.4286427 -4.4286423 -4.4286284 -4.4286504 -4.4287038 -4.4287276][-4.4287777 -4.4288015 -4.4288282 -4.4287996 -4.4287419 -4.4286861 -4.4286718 -4.4286838 -4.4286876 -4.4286914 -4.4286828 -4.4286742 -4.4287095 -4.4287696 -4.4287963][-4.4287739 -4.4288068 -4.4288425 -4.4288278 -4.4287844 -4.4287448 -4.4287319 -4.4287434 -4.42875 -4.4287438 -4.4287271 -4.4287224 -4.4287596 -4.4288073 -4.428822][-4.4287634 -4.4287872 -4.4288216 -4.4288177 -4.4287968 -4.4287806 -4.4287806 -4.428792 -4.4287863 -4.4287648 -4.4287434 -4.4287305 -4.4287548 -4.4287891 -4.428802][-4.428781 -4.4287958 -4.428822 -4.428822 -4.428812 -4.4288054 -4.428802 -4.4288006 -4.4287844 -4.428761 -4.4287415 -4.4287252 -4.4287424 -4.4287691 -4.4287844][-4.4288292 -4.4288387 -4.4288449 -4.428834 -4.428822 -4.4288173 -4.428813 -4.4288058 -4.4287868 -4.4287639 -4.4287472 -4.4287386 -4.4287539 -4.4287806 -4.4287992]]...]
INFO - root - 2017-12-08 07:42:48.323415: step 45010, loss = 2.28, batch loss = 2.23 (34.9 examples/sec; 0.229 sec/batch; 18h:17m:25s remains)
INFO - root - 2017-12-08 07:42:50.579984: step 45020, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.220 sec/batch; 17h:36m:27s remains)
INFO - root - 2017-12-08 07:42:52.857400: step 45030, loss = 2.28, batch loss = 2.23 (35.4 examples/sec; 0.226 sec/batch; 18h:03m:37s remains)
INFO - root - 2017-12-08 07:42:55.126904: step 45040, loss = 2.28, batch loss = 2.23 (35.1 examples/sec; 0.228 sec/batch; 18h:12m:49s remains)
INFO - root - 2017-12-08 07:42:57.369746: step 45050, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 17h:31m:20s remains)
INFO - root - 2017-12-08 07:42:59.623609: step 45060, loss = 2.28, batch loss = 2.23 (34.4 examples/sec; 0.233 sec/batch; 18h:35m:03s remains)
INFO - root - 2017-12-08 07:43:01.878223: step 45070, loss = 2.28, batch loss = 2.23 (36.6 examples/sec; 0.219 sec/batch; 17h:27m:46s remains)
INFO - root - 2017-12-08 07:43:04.110890: step 45080, loss = 2.28, batch loss = 2.23 (36.7 examples/sec; 0.218 sec/batch; 17h:23m:27s remains)
INFO - root - 2017-12-08 07:43:06.365887: step 45090, loss = 2.28, batch loss = 2.23 (34.4 examples/sec; 0.233 sec/batch; 18h:35m:27s remains)
INFO - root - 2017-12-08 07:43:08.599987: step 45100, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.220 sec/batch; 17h:35m:17s remains)
2017-12-08 07:43:08.908316: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.42858 -4.4285336 -4.4285531 -4.4285822 -4.428616 -4.4286733 -4.4286919 -4.4286675 -4.4286342 -4.4285927 -4.4285531 -4.4285483 -4.4286032 -4.4286575 -4.4286561][-4.4283648 -4.428308 -4.4283452 -4.4283977 -4.4284716 -4.428596 -4.4286885 -4.4287152 -4.4287028 -4.4286523 -4.4285774 -4.428515 -4.4285221 -4.4285588 -4.4285579][-4.428236 -4.4281797 -4.4282303 -4.4283009 -4.4284015 -4.4285545 -4.428678 -4.4287176 -4.4287028 -4.4286327 -4.4285192 -4.4284077 -4.4283686 -4.4283891 -4.4284058][-4.428318 -4.4282761 -4.4283247 -4.4283919 -4.4284739 -4.4285913 -4.4286895 -4.4287205 -4.4286981 -4.428628 -4.428503 -4.4283633 -4.4282842 -4.4282823 -4.4283147][-4.4285121 -4.4284778 -4.4284992 -4.4285345 -4.4285617 -4.4286184 -4.4286785 -4.4287028 -4.4286876 -4.4286437 -4.4285612 -4.4284639 -4.4283957 -4.4283853 -4.4284153][-4.4286995 -4.4286623 -4.4286423 -4.4286175 -4.4285717 -4.4285727 -4.4286113 -4.4286361 -4.4286318 -4.4286356 -4.4286394 -4.4286203 -4.4285917 -4.4285793 -4.4285936][-4.4288244 -4.4287734 -4.4287071 -4.4286132 -4.4285 -4.4284525 -4.4284711 -4.4284949 -4.4285097 -4.4285808 -4.4286666 -4.4287052 -4.4287109 -4.4287138 -4.4287219][-4.4288926 -4.4288211 -4.4287043 -4.4285436 -4.4283686 -4.4282808 -4.4282804 -4.4283056 -4.4283519 -4.4284835 -4.4286275 -4.4287014 -4.4287305 -4.4287548 -4.4287744][-4.4289117 -4.4288306 -4.4286847 -4.4284863 -4.4282904 -4.4281964 -4.428195 -4.4282312 -4.4283028 -4.4284577 -4.428618 -4.4287052 -4.4287486 -4.4287868 -4.428813][-4.4289155 -4.4288378 -4.4286976 -4.4285212 -4.42837 -4.42831 -4.4283204 -4.4283566 -4.428422 -4.428546 -4.4286709 -4.4287438 -4.4287868 -4.4288263 -4.4288559][-4.4289122 -4.4288554 -4.4287567 -4.4286494 -4.4285684 -4.4285398 -4.4285474 -4.42857 -4.4286089 -4.428688 -4.4287682 -4.4288197 -4.4288521 -4.4288774 -4.428896][-4.42891 -4.4288769 -4.4288239 -4.428782 -4.428751 -4.4287395 -4.4287453 -4.4287519 -4.4287663 -4.4288087 -4.4288545 -4.4288845 -4.4289036 -4.4289179 -4.4289241][-4.4289074 -4.428894 -4.4288735 -4.4288712 -4.4288721 -4.428874 -4.4288769 -4.428874 -4.4288759 -4.4288979 -4.4289222 -4.4289389 -4.4289484 -4.4289513 -4.4289422][-4.4289083 -4.428905 -4.4289021 -4.4289207 -4.42894 -4.42895 -4.4289546 -4.4289532 -4.4289513 -4.4289589 -4.428968 -4.4289727 -4.42897 -4.4289589 -4.428936][-4.4289193 -4.4289184 -4.4289236 -4.4289465 -4.4289684 -4.4289818 -4.4289875 -4.4289846 -4.4289794 -4.4289794 -4.4289784 -4.4289737 -4.4289651 -4.4289484 -4.4289217]]...]
INFO - root - 2017-12-08 07:43:11.121550: step 45110, loss = 2.28, batch loss = 2.23 (35.8 examples/sec; 0.223 sec/batch; 17h:49m:48s remains)
INFO - root - 2017-12-08 07:43:13.378794: step 45120, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.222 sec/batch; 17h:41m:42s remains)
INFO - root - 2017-12-08 07:43:15.608395: step 45130, loss = 2.28, batch loss = 2.23 (36.7 examples/sec; 0.218 sec/batch; 17h:25m:25s remains)
INFO - root - 2017-12-08 07:43:17.852722: step 45140, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 17h:37m:22s remains)
INFO - root - 2017-12-08 07:43:20.107240: step 45150, loss = 2.28, batch loss = 2.23 (35.3 examples/sec; 0.226 sec/batch; 18h:04m:41s remains)
INFO - root - 2017-12-08 07:43:22.363851: step 45160, loss = 2.28, batch loss = 2.23 (35.8 examples/sec; 0.224 sec/batch; 17h:51m:37s remains)
INFO - root - 2017-12-08 07:43:24.616479: step 45170, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 17h:31m:12s remains)
INFO - root - 2017-12-08 07:43:26.855345: step 45180, loss = 2.28, batch loss = 2.23 (36.0 examples/sec; 0.222 sec/batch; 17h:45m:01s remains)
INFO - root - 2017-12-08 07:43:29.083995: step 45190, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 17h:30m:41s remains)
INFO - root - 2017-12-08 07:43:31.328750: step 45200, loss = 2.28, batch loss = 2.23 (35.2 examples/sec; 0.227 sec/batch; 18h:08m:56s remains)
2017-12-08 07:43:31.643447: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4286776 -4.4287243 -4.4288011 -4.4288568 -4.4288688 -4.4288621 -4.4288425 -4.4288049 -4.4287572 -4.42872 -4.4286914 -4.4286604 -4.4286351 -4.4286275 -4.4286151][-4.4287195 -4.4287729 -4.4288435 -4.42889 -4.4288917 -4.4288745 -4.4288487 -4.42881 -4.4287663 -4.4287233 -4.4286871 -4.4286528 -4.4286318 -4.4286385 -4.4286427][-4.4287672 -4.4288135 -4.4288611 -4.4288864 -4.4288764 -4.4288588 -4.4288349 -4.4287992 -4.4287591 -4.4287138 -4.4286747 -4.4286456 -4.4286408 -4.428668 -4.4286947][-4.4288011 -4.4288363 -4.4288588 -4.4288564 -4.4288278 -4.4288058 -4.4287877 -4.4287567 -4.4287195 -4.4286795 -4.4286466 -4.428627 -4.4286427 -4.4286962 -4.4287462][-4.4288268 -4.4288445 -4.4288363 -4.4288054 -4.4287515 -4.4287138 -4.4287004 -4.4286814 -4.4286537 -4.4286289 -4.4286122 -4.4286118 -4.4286504 -4.4287233 -4.42878][-4.4288311 -4.4288197 -4.42878 -4.4287276 -4.4286604 -4.4286118 -4.4285984 -4.4285913 -4.4285789 -4.4285727 -4.4285793 -4.4286065 -4.4286618 -4.4287367 -4.4287887][-4.4287982 -4.4287639 -4.4287109 -4.4286613 -4.4286041 -4.4285588 -4.4285398 -4.428534 -4.4285321 -4.4285359 -4.4285603 -4.4286118 -4.4286828 -4.4287543 -4.4287934][-4.4287577 -4.4287167 -4.4286661 -4.4286275 -4.4285889 -4.4285569 -4.4285407 -4.4285331 -4.4285378 -4.4285479 -4.4285793 -4.4286475 -4.4287286 -4.4287891 -4.4288049][-4.4287329 -4.428689 -4.4286418 -4.428617 -4.4286008 -4.4285865 -4.4285769 -4.428566 -4.4285679 -4.4285836 -4.42862 -4.4286923 -4.4287724 -4.4288163 -4.4288096][-4.4287295 -4.4286852 -4.42864 -4.4286227 -4.4286208 -4.4286194 -4.4286189 -4.428607 -4.4286032 -4.4286184 -4.4286494 -4.4287171 -4.4287882 -4.4288154 -4.4287949][-4.4287243 -4.4286828 -4.4286389 -4.4286265 -4.4286337 -4.4286375 -4.428637 -4.4286232 -4.4286184 -4.4286313 -4.4286556 -4.4287152 -4.4287763 -4.428793 -4.4287653][-4.4287181 -4.4286795 -4.4286356 -4.4286222 -4.4286308 -4.4286342 -4.4286308 -4.4286227 -4.4286232 -4.4286342 -4.4286466 -4.4286876 -4.4287338 -4.4287424 -4.4287138][-4.4287181 -4.428679 -4.4286356 -4.4286175 -4.4286213 -4.4286208 -4.4286194 -4.4286208 -4.4286313 -4.4286447 -4.4286485 -4.4286685 -4.4286919 -4.4286861 -4.428659][-4.428731 -4.4286876 -4.4286494 -4.4286337 -4.4286389 -4.4286447 -4.4286504 -4.4286604 -4.4286742 -4.4286838 -4.428678 -4.42868 -4.4286838 -4.4286685 -4.42865][-4.4287815 -4.4287405 -4.4287128 -4.4287057 -4.4287152 -4.4287271 -4.4287376 -4.42875 -4.4287615 -4.4287581 -4.4287391 -4.4287276 -4.4287238 -4.4287128 -4.4287066]]...]
INFO - root - 2017-12-08 07:43:33.852586: step 45210, loss = 2.28, batch loss = 2.23 (36.6 examples/sec; 0.219 sec/batch; 17h:27m:02s remains)
INFO - root - 2017-12-08 07:43:36.086503: step 45220, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.220 sec/batch; 17h:34m:42s remains)
INFO - root - 2017-12-08 07:43:38.328842: step 45230, loss = 2.28, batch loss = 2.23 (35.0 examples/sec; 0.229 sec/batch; 18h:15m:51s remains)
INFO - root - 2017-12-08 07:43:40.542979: step 45240, loss = 2.28, batch loss = 2.23 (35.0 examples/sec; 0.228 sec/batch; 18h:13m:07s remains)
INFO - root - 2017-12-08 07:43:42.809244: step 45250, loss = 2.28, batch loss = 2.23 (33.7 examples/sec; 0.237 sec/batch; 18h:55m:16s remains)
INFO - root - 2017-12-08 07:43:45.050574: step 45260, loss = 2.28, batch loss = 2.23 (34.8 examples/sec; 0.230 sec/batch; 18h:21m:15s remains)
INFO - root - 2017-12-08 07:43:47.270288: step 45270, loss = 2.28, batch loss = 2.23 (36.9 examples/sec; 0.217 sec/batch; 17h:18m:47s remains)
INFO - root - 2017-12-08 07:43:49.512135: step 45280, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.221 sec/batch; 17h:35m:58s remains)
INFO - root - 2017-12-08 07:43:51.763853: step 45290, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.226 sec/batch; 17h:59m:55s remains)
INFO - root - 2017-12-08 07:43:53.988519: step 45300, loss = 2.28, batch loss = 2.23 (35.4 examples/sec; 0.226 sec/batch; 18h:01m:11s remains)
2017-12-08 07:43:54.298006: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4287925 -4.4287872 -4.4287782 -4.428791 -4.4288039 -4.4287925 -4.4287467 -4.4286981 -4.4286542 -4.4286118 -4.4285741 -4.4285669 -4.4285927 -4.4286275 -4.4286556][-4.4288106 -4.4287872 -4.428761 -4.4287686 -4.428793 -4.4287977 -4.428761 -4.4287152 -4.4286671 -4.4286194 -4.4285827 -4.4285846 -4.4286246 -4.4286709 -4.4286923][-4.4288192 -4.4287839 -4.4287534 -4.4287577 -4.4287887 -4.42881 -4.428781 -4.4287381 -4.4286938 -4.4286466 -4.4286103 -4.42862 -4.4286613 -4.4286976 -4.4287047][-4.4288216 -4.4287934 -4.4287691 -4.4287753 -4.4287982 -4.4288154 -4.42878 -4.4287381 -4.4287019 -4.4286695 -4.4286494 -4.4286623 -4.4286942 -4.4287105 -4.4287047][-4.4288149 -4.4288106 -4.4288006 -4.4287992 -4.4288 -4.4287934 -4.4287434 -4.4286981 -4.4286718 -4.4286652 -4.4286695 -4.4286942 -4.4287224 -4.4287329 -4.4287238][-4.4288039 -4.4288125 -4.4288039 -4.428782 -4.4287529 -4.4287143 -4.4286385 -4.4285774 -4.4285631 -4.4285936 -4.4286404 -4.4286928 -4.4287381 -4.4287624 -4.4287629][-4.4287848 -4.4287925 -4.4287715 -4.4287262 -4.4286656 -4.4285879 -4.4284716 -4.4283752 -4.4283767 -4.4284682 -4.4285722 -4.4286656 -4.4287424 -4.4287877 -4.4288034][-4.4287643 -4.4287696 -4.428741 -4.4286861 -4.4286213 -4.4285259 -4.4283767 -4.4282422 -4.4282565 -4.4283938 -4.4285297 -4.4286432 -4.4287338 -4.428782 -4.4287996][-4.4287763 -4.4287705 -4.4287357 -4.4286895 -4.4286537 -4.4285889 -4.4284735 -4.4283628 -4.4283743 -4.4284792 -4.4285684 -4.4286466 -4.4287128 -4.4287386 -4.4287424][-4.4288006 -4.4287763 -4.428731 -4.4286952 -4.4286947 -4.4286852 -4.4286451 -4.4286008 -4.4286137 -4.4286628 -4.4286838 -4.4286985 -4.42871 -4.4286952 -4.4286714][-4.4288182 -4.4287868 -4.4287367 -4.4287028 -4.4287181 -4.428741 -4.4287491 -4.4287529 -4.4287663 -4.4287782 -4.4287605 -4.4287405 -4.4287128 -4.4286695 -4.428628][-4.4288349 -4.428813 -4.4287734 -4.4287453 -4.4287534 -4.4287739 -4.4287896 -4.4288054 -4.4288116 -4.4288025 -4.428771 -4.4287357 -4.4286923 -4.4286442 -4.4286108][-4.428843 -4.4288383 -4.428822 -4.4288116 -4.4288144 -4.4288168 -4.428812 -4.4288125 -4.4288058 -4.4287906 -4.4287629 -4.4287276 -4.428688 -4.4286494 -4.4286361][-4.4288483 -4.4288445 -4.4288368 -4.4288387 -4.4288464 -4.4288397 -4.4288177 -4.4288034 -4.4287882 -4.4287767 -4.4287653 -4.4287472 -4.4287248 -4.4287043 -4.4287038][-4.4288511 -4.4288335 -4.4288177 -4.4288211 -4.4288368 -4.4288349 -4.428812 -4.4287934 -4.4287791 -4.4287753 -4.4287791 -4.4287825 -4.4287815 -4.4287815 -4.4287868]]...]
INFO - root - 2017-12-08 07:43:56.528968: step 45310, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 17h:27m:58s remains)
INFO - root - 2017-12-08 07:43:58.769310: step 45320, loss = 2.28, batch loss = 2.23 (35.3 examples/sec; 0.226 sec/batch; 18h:03m:43s remains)
INFO - root - 2017-12-08 07:44:01.031365: step 45330, loss = 2.28, batch loss = 2.23 (34.4 examples/sec; 0.233 sec/batch; 18h:33m:21s remains)
INFO - root - 2017-12-08 07:44:03.260083: step 45340, loss = 2.28, batch loss = 2.23 (32.1 examples/sec; 0.249 sec/batch; 19h:52m:13s remains)
INFO - root - 2017-12-08 07:44:05.513459: step 45350, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.220 sec/batch; 17h:34m:43s remains)
INFO - root - 2017-12-08 07:44:07.773895: step 45360, loss = 2.28, batch loss = 2.23 (36.0 examples/sec; 0.222 sec/batch; 17h:42m:03s remains)
INFO - root - 2017-12-08 07:44:10.043484: step 45370, loss = 2.28, batch loss = 2.23 (35.0 examples/sec; 0.229 sec/batch; 18h:14m:15s remains)
INFO - root - 2017-12-08 07:44:12.266859: step 45380, loss = 2.28, batch loss = 2.23 (34.5 examples/sec; 0.232 sec/batch; 18h:28m:19s remains)
INFO - root - 2017-12-08 07:44:14.495849: step 45390, loss = 2.28, batch loss = 2.23 (36.8 examples/sec; 0.217 sec/batch; 17h:19m:21s remains)
INFO - root - 2017-12-08 07:44:16.722869: step 45400, loss = 2.28, batch loss = 2.23 (37.4 examples/sec; 0.214 sec/batch; 17h:04m:14s remains)
2017-12-08 07:44:17.001500: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4287548 -4.4287276 -4.4287128 -4.4287186 -4.4287333 -4.4287496 -4.4287667 -4.4287877 -4.4287896 -4.4287615 -4.4286919 -4.4286056 -4.4285541 -4.4285593 -4.4285855][-4.428772 -4.4287429 -4.4287171 -4.4287128 -4.428731 -4.4287581 -4.4287872 -4.4288139 -4.4288192 -4.4287925 -4.4287314 -4.4286652 -4.4286413 -4.4286547 -4.4286733][-4.4288077 -4.4287767 -4.4287467 -4.4287386 -4.4287529 -4.4287786 -4.4288106 -4.4288387 -4.4288473 -4.4288239 -4.4287691 -4.4287186 -4.4287095 -4.4287238 -4.4287319][-4.4288263 -4.4287915 -4.42876 -4.4287534 -4.4287658 -4.4287815 -4.4288087 -4.4288378 -4.4288511 -4.4288321 -4.4287815 -4.428741 -4.4287372 -4.4287534 -4.4287562][-4.428823 -4.4287834 -4.4287515 -4.4287386 -4.4287324 -4.4287271 -4.4287467 -4.4287806 -4.4288006 -4.4287796 -4.4287271 -4.4287019 -4.4287186 -4.4287477 -4.4287467][-4.4287896 -4.4287376 -4.428689 -4.4286427 -4.4285936 -4.4285574 -4.4285846 -4.4286456 -4.4286814 -4.4286494 -4.428586 -4.4285913 -4.42865 -4.4287024 -4.4287066][-4.4286947 -4.4286351 -4.4285717 -4.428483 -4.4283767 -4.4283128 -4.4283705 -4.428483 -4.4285378 -4.4284835 -4.4283981 -4.4284463 -4.4285688 -4.4286532 -4.4286785][-4.4285831 -4.4285269 -4.4284596 -4.4283414 -4.4281936 -4.428123 -4.428216 -4.42837 -4.428442 -4.4283819 -4.4282918 -4.4283752 -4.4285374 -4.42865 -4.4286962][-4.4285297 -4.4284859 -4.4284277 -4.4283104 -4.4281745 -4.4281273 -4.4282312 -4.4283829 -4.4284668 -4.4284272 -4.4283628 -4.4284391 -4.4285817 -4.428688 -4.4287372][-4.428546 -4.4285207 -4.4284844 -4.4284096 -4.4283347 -4.42832 -4.4283996 -4.4285164 -4.4285908 -4.4285793 -4.4285474 -4.4286 -4.4286942 -4.4287677 -4.4288063][-4.4286022 -4.4285965 -4.4285827 -4.4285512 -4.4285278 -4.4285331 -4.428586 -4.4286571 -4.42871 -4.4287138 -4.4287109 -4.428751 -4.4288125 -4.4288559 -4.4288759][-4.4286585 -4.4286642 -4.4286661 -4.4286642 -4.4286685 -4.4286852 -4.428719 -4.4287605 -4.4287944 -4.4288054 -4.4288144 -4.4288449 -4.4288826 -4.4289069 -4.428915][-4.4287114 -4.4287214 -4.4287314 -4.4287438 -4.42876 -4.4287758 -4.4287958 -4.4288182 -4.4288392 -4.428853 -4.4288659 -4.4288869 -4.4289112 -4.4289269 -4.4289308][-4.4287577 -4.4287667 -4.4287777 -4.428793 -4.4288082 -4.4288206 -4.4288321 -4.4288478 -4.428863 -4.4288745 -4.4288836 -4.4288964 -4.4289145 -4.42893 -4.428937][-4.4287882 -4.4287953 -4.4288068 -4.4288206 -4.4288321 -4.42884 -4.42885 -4.4288645 -4.4288774 -4.4288869 -4.428894 -4.4289074 -4.428926 -4.4289427 -4.4289522]]...]
INFO - root - 2017-12-08 07:44:19.242238: step 45410, loss = 2.28, batch loss = 2.23 (34.8 examples/sec; 0.230 sec/batch; 18h:19m:57s remains)
INFO - root - 2017-12-08 07:44:21.463707: step 45420, loss = 2.28, batch loss = 2.23 (34.6 examples/sec; 0.231 sec/batch; 18h:24m:59s remains)
INFO - root - 2017-12-08 07:44:23.703988: step 45430, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.225 sec/batch; 17h:57m:24s remains)
INFO - root - 2017-12-08 07:44:25.957603: step 45440, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 17h:45m:53s remains)
INFO - root - 2017-12-08 07:44:28.205926: step 45450, loss = 2.28, batch loss = 2.23 (36.6 examples/sec; 0.219 sec/batch; 17h:26m:35s remains)
INFO - root - 2017-12-08 07:44:30.435997: step 45460, loss = 2.28, batch loss = 2.23 (34.8 examples/sec; 0.230 sec/batch; 18h:18m:32s remains)
INFO - root - 2017-12-08 07:44:32.671046: step 45470, loss = 2.28, batch loss = 2.23 (35.8 examples/sec; 0.224 sec/batch; 17h:49m:20s remains)
INFO - root - 2017-12-08 07:44:34.901028: step 45480, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.225 sec/batch; 17h:58m:28s remains)
INFO - root - 2017-12-08 07:44:37.120004: step 45490, loss = 2.28, batch loss = 2.23 (36.8 examples/sec; 0.217 sec/batch; 17h:19m:11s remains)
INFO - root - 2017-12-08 07:44:39.405998: step 45500, loss = 2.28, batch loss = 2.23 (35.3 examples/sec; 0.227 sec/batch; 18h:04m:32s remains)
2017-12-08 07:44:39.700190: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4287791 -4.428772 -4.4287829 -4.4287934 -4.4288 -4.42881 -4.4288144 -4.4288116 -4.4287987 -4.428771 -4.4287381 -4.4287105 -4.4286814 -4.4286637 -4.4286594][-4.4287863 -4.4287896 -4.4288092 -4.4288182 -4.4288111 -4.4288025 -4.4287906 -4.4287848 -4.4287853 -4.4287767 -4.4287577 -4.4287405 -4.4287195 -4.4287019 -4.4286876][-4.4287839 -4.4287891 -4.4288106 -4.4288135 -4.42879 -4.4287596 -4.4287333 -4.4287295 -4.428751 -4.4287724 -4.4287786 -4.4287796 -4.4287658 -4.4287443 -4.4287238][-4.4287858 -4.4287877 -4.4288044 -4.4288006 -4.4287639 -4.4287152 -4.4286733 -4.4286671 -4.4287095 -4.4287615 -4.4287906 -4.428803 -4.428792 -4.4287696 -4.4287496][-4.4287953 -4.4287877 -4.4287915 -4.4287763 -4.428731 -4.4286733 -4.4286151 -4.4285965 -4.4286485 -4.4287219 -4.4287667 -4.4287853 -4.4287758 -4.4287596 -4.42875][-4.4288006 -4.4287748 -4.428761 -4.4287333 -4.42868 -4.4286108 -4.4285288 -4.4284906 -4.4285545 -4.4286556 -4.4287219 -4.4287543 -4.4287467 -4.4287305 -4.4287324][-4.4288087 -4.4287682 -4.4287395 -4.4286947 -4.4286213 -4.4285226 -4.4283962 -4.4283247 -4.4284048 -4.4285517 -4.428658 -4.4287186 -4.4287291 -4.4287195 -4.4287281][-4.428813 -4.42877 -4.4287314 -4.4286761 -4.4285874 -4.4284639 -4.4283 -4.4281898 -4.428278 -4.4284649 -4.4286113 -4.4287057 -4.4287448 -4.4287481 -4.4287591][-4.4288111 -4.4287753 -4.4287386 -4.4286928 -4.4286222 -4.4285197 -4.4283814 -4.4282827 -4.4283457 -4.4285049 -4.42864 -4.4287333 -4.42878 -4.428793 -4.4288039][-4.4287992 -4.4287748 -4.428751 -4.4287291 -4.428699 -4.4286466 -4.4285679 -4.4285021 -4.428525 -4.428616 -4.4287057 -4.428772 -4.4288087 -4.4288244 -4.4288344][-4.4287992 -4.428791 -4.4287825 -4.42878 -4.4287782 -4.42876 -4.4287186 -4.4286695 -4.4286604 -4.428699 -4.4287477 -4.4287868 -4.4288082 -4.4288239 -4.4288363][-4.4288073 -4.4288096 -4.428813 -4.428823 -4.428833 -4.4288268 -4.4288015 -4.4287562 -4.4287271 -4.4287372 -4.4287581 -4.4287729 -4.4287806 -4.4288 -4.4288206][-4.4288 -4.428802 -4.428813 -4.42883 -4.4288459 -4.4288464 -4.4288349 -4.4288015 -4.4287658 -4.4287591 -4.4287643 -4.428762 -4.428762 -4.4287829 -4.4288044][-4.4287777 -4.4287715 -4.4287839 -4.4288063 -4.4288249 -4.4288321 -4.4288354 -4.4288225 -4.428792 -4.4287796 -4.4287767 -4.4287639 -4.4287577 -4.4287748 -4.4287891][-4.4287429 -4.4287229 -4.4287362 -4.4287639 -4.428782 -4.4287877 -4.4287992 -4.428803 -4.4287863 -4.4287791 -4.428782 -4.4287705 -4.4287629 -4.4287734 -4.4287815]]...]
INFO - root - 2017-12-08 07:44:41.976480: step 45510, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 17h:37m:40s remains)
INFO - root - 2017-12-08 07:44:44.179020: step 45520, loss = 2.28, batch loss = 2.23 (35.4 examples/sec; 0.226 sec/batch; 18h:00m:31s remains)
INFO - root - 2017-12-08 07:44:46.425152: step 45530, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 17h:35m:54s remains)
INFO - root - 2017-12-08 07:44:48.675984: step 45540, loss = 2.28, batch loss = 2.23 (34.6 examples/sec; 0.231 sec/batch; 18h:26m:02s remains)
INFO - root - 2017-12-08 07:44:50.894452: step 45550, loss = 2.28, batch loss = 2.23 (35.0 examples/sec; 0.229 sec/batch; 18h:12m:48s remains)
INFO - root - 2017-12-08 07:44:53.128330: step 45560, loss = 2.28, batch loss = 2.23 (35.1 examples/sec; 0.228 sec/batch; 18h:10m:25s remains)
INFO - root - 2017-12-08 07:44:55.359610: step 45570, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 17h:31m:11s remains)
INFO - root - 2017-12-08 07:44:57.591722: step 45580, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 17h:27m:06s remains)
INFO - root - 2017-12-08 07:44:59.805950: step 45590, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 17h:35m:34s remains)
INFO - root - 2017-12-08 07:45:02.034271: step 45600, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 17h:31m:17s remains)
2017-12-08 07:45:02.342399: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289174 -4.428937 -4.4289455 -4.428947 -4.428947 -4.428946 -4.4289441 -4.4289417 -4.4289408 -4.4289427 -4.4289484 -4.4289532 -4.4289551 -4.428956 -4.4289517][-4.4288831 -4.4288979 -4.4289088 -4.4289174 -4.4289265 -4.4289322 -4.4289312 -4.4289284 -4.4289274 -4.42893 -4.42894 -4.4289489 -4.4289517 -4.4289517 -4.4289432][-4.4288206 -4.428812 -4.4288216 -4.4288449 -4.4288774 -4.4289 -4.428906 -4.4289069 -4.4289079 -4.4289103 -4.4289227 -4.4289365 -4.4289403 -4.428937 -4.4289222][-4.42876 -4.4287119 -4.4287009 -4.428731 -4.4287834 -4.4288259 -4.4288473 -4.4288592 -4.42887 -4.4288836 -4.4289036 -4.4289231 -4.4289308 -4.4289279 -4.4289126][-4.4287386 -4.4286571 -4.4286184 -4.4286308 -4.4286761 -4.4287224 -4.4287491 -4.4287734 -4.4288092 -4.4288473 -4.4288807 -4.4289069 -4.4289207 -4.4289231 -4.4289179][-4.4287486 -4.4286628 -4.4286032 -4.4285789 -4.4285741 -4.42857 -4.4285603 -4.4285884 -4.4286685 -4.4287567 -4.4288192 -4.4288607 -4.4288878 -4.4289002 -4.4289122][-4.4287481 -4.4286871 -4.4286318 -4.4285679 -4.4284806 -4.4283767 -4.428268 -4.428257 -4.4283962 -4.4285607 -4.4286733 -4.42874 -4.42878 -4.4288087 -4.42884][-4.4287548 -4.4287434 -4.428719 -4.4286361 -4.42849 -4.4282928 -4.4280496 -4.4279456 -4.4281359 -4.4283724 -4.4285254 -4.4286008 -4.428637 -4.4286671 -4.4287076][-4.4287686 -4.4288111 -4.4288216 -4.428761 -4.4286256 -4.4284277 -4.4281673 -4.4280224 -4.4281611 -4.4283671 -4.4284911 -4.4285312 -4.4285316 -4.428534 -4.4285612][-4.4287467 -4.4288235 -4.4288726 -4.4288583 -4.4287734 -4.4286389 -4.4284663 -4.4283552 -4.4283957 -4.4284968 -4.4285483 -4.4285336 -4.4284954 -4.4284663 -4.428463][-4.4286704 -4.4287648 -4.4288554 -4.4288921 -4.4288578 -4.4287786 -4.4286814 -4.4286051 -4.42859 -4.4286137 -4.4286079 -4.42857 -4.4285192 -4.4284749 -4.4284439][-4.4285789 -4.428668 -4.4287772 -4.4288483 -4.4288497 -4.4288158 -4.4287734 -4.4287219 -4.4286804 -4.4286656 -4.4286485 -4.4286256 -4.42859 -4.4285536 -4.4285088][-4.4285445 -4.4286003 -4.4286828 -4.4287519 -4.4287739 -4.4287786 -4.4287877 -4.4287677 -4.4287219 -4.4286857 -4.4286752 -4.42867 -4.4286571 -4.4286523 -4.4286261][-4.4286 -4.4286222 -4.4286566 -4.4286923 -4.4287124 -4.4287429 -4.42879 -4.4287968 -4.4287505 -4.4287052 -4.4286952 -4.4286976 -4.4287057 -4.42873 -4.4287362][-4.4287195 -4.4287252 -4.4287291 -4.4287367 -4.4287419 -4.4287724 -4.4288211 -4.428834 -4.42879 -4.4287429 -4.4287329 -4.4287429 -4.428762 -4.4287968 -4.4288249]]...]
INFO - root - 2017-12-08 07:45:04.582961: step 45610, loss = 2.28, batch loss = 2.23 (35.4 examples/sec; 0.226 sec/batch; 18h:01m:13s remains)
INFO - root - 2017-12-08 07:45:06.833728: step 45620, loss = 2.28, batch loss = 2.23 (37.5 examples/sec; 0.213 sec/batch; 16h:59m:44s remains)
INFO - root - 2017-12-08 07:45:09.149089: step 45630, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.221 sec/batch; 17h:34m:55s remains)
INFO - root - 2017-12-08 07:45:11.411741: step 45640, loss = 2.28, batch loss = 2.23 (35.6 examples/sec; 0.225 sec/batch; 17h:55m:47s remains)
INFO - root - 2017-12-08 07:45:13.634286: step 45650, loss = 2.28, batch loss = 2.23 (35.8 examples/sec; 0.224 sec/batch; 17h:48m:41s remains)
INFO - root - 2017-12-08 07:45:15.886926: step 45660, loss = 2.28, batch loss = 2.23 (35.7 examples/sec; 0.224 sec/batch; 17h:50m:26s remains)
INFO - root - 2017-12-08 07:45:18.135232: step 45670, loss = 2.28, batch loss = 2.23 (36.8 examples/sec; 0.217 sec/batch; 17h:18m:57s remains)
INFO - root - 2017-12-08 07:45:20.357348: step 45680, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.220 sec/batch; 17h:32m:50s remains)
INFO - root - 2017-12-08 07:45:22.591923: step 45690, loss = 2.28, batch loss = 2.23 (36.9 examples/sec; 0.217 sec/batch; 17h:16m:58s remains)
INFO - root - 2017-12-08 07:45:24.849197: step 45700, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.221 sec/batch; 17h:38m:16s remains)
2017-12-08 07:45:25.187914: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288678 -4.4288621 -4.4288688 -4.4288721 -4.4288769 -4.428864 -4.4288464 -4.428833 -4.428823 -4.4288158 -4.4288125 -4.4288135 -4.4288173 -4.4288187 -4.4288292][-4.4287939 -4.4287825 -4.4287934 -4.4287987 -4.4287972 -4.4287696 -4.4287362 -4.4287133 -4.4286981 -4.4286885 -4.4286876 -4.4286947 -4.4287057 -4.4287105 -4.4287291][-4.4287419 -4.4287276 -4.42874 -4.4287443 -4.4287333 -4.4286909 -4.42864 -4.4286065 -4.4285841 -4.4285731 -4.4285831 -4.4286041 -4.4286304 -4.42864 -4.4286633][-4.4287176 -4.4287043 -4.4287128 -4.4287066 -4.428689 -4.4286437 -4.4285917 -4.4285531 -4.428525 -4.4285121 -4.4285259 -4.4285517 -4.4285865 -4.4286036 -4.4286351][-4.4287333 -4.4287205 -4.428719 -4.4286981 -4.4286695 -4.4286284 -4.4285913 -4.4285669 -4.4285507 -4.428546 -4.4285636 -4.428587 -4.4286208 -4.4286456 -4.4286823][-4.4287686 -4.4287486 -4.4287319 -4.4286942 -4.4286485 -4.4286051 -4.4285769 -4.42857 -4.4285793 -4.4286 -4.4286284 -4.4286494 -4.4286833 -4.4287143 -4.4287543][-4.4287806 -4.4287486 -4.4287152 -4.4286575 -4.4285884 -4.42853 -4.4284959 -4.4284978 -4.4285393 -4.428596 -4.4286408 -4.4286642 -4.4287004 -4.4287415 -4.4287906][-4.4287491 -4.4287071 -4.4286642 -4.4285955 -4.42851 -4.428432 -4.4283767 -4.4283671 -4.4284272 -4.4285169 -4.4285808 -4.4286113 -4.4286513 -4.4287 -4.4287615][-4.4286895 -4.4286561 -4.4286265 -4.4285765 -4.4285073 -4.4284263 -4.4283452 -4.4282994 -4.428339 -4.4284263 -4.4284849 -4.4285145 -4.4285588 -4.4286189 -4.4286919][-4.4286423 -4.4286242 -4.4286251 -4.4286175 -4.4285946 -4.4285493 -4.4284844 -4.4284263 -4.4284248 -4.4284625 -4.428483 -4.4284863 -4.4285212 -4.4285789 -4.428647][-4.4285874 -4.4285827 -4.4286079 -4.4286342 -4.4286475 -4.4286356 -4.4285994 -4.4285541 -4.4285345 -4.4285374 -4.4285316 -4.4285173 -4.4285326 -4.4285727 -4.4286242][-4.428535 -4.4285345 -4.4285684 -4.4286065 -4.4286289 -4.4286275 -4.42861 -4.4285879 -4.428576 -4.4285722 -4.4285622 -4.4285445 -4.4285474 -4.4285736 -4.4286132][-4.4285192 -4.4285235 -4.4285569 -4.4285946 -4.42861 -4.4286036 -4.4285936 -4.4285879 -4.4285855 -4.4285855 -4.4285817 -4.4285679 -4.428566 -4.4285831 -4.4286175][-4.4285913 -4.4285979 -4.4286232 -4.4286485 -4.4286513 -4.4286351 -4.4286218 -4.4286218 -4.428627 -4.4286366 -4.4286466 -4.4286461 -4.4286432 -4.4286518 -4.4286771][-4.4287305 -4.4287405 -4.4287577 -4.42877 -4.4287667 -4.4287491 -4.4287338 -4.42873 -4.4287324 -4.4287395 -4.42875 -4.4287548 -4.4287543 -4.4287562 -4.4287724]]...]
INFO - root - 2017-12-08 07:45:27.433055: step 45710, loss = 2.28, batch loss = 2.23 (35.1 examples/sec; 0.228 sec/batch; 18h:10m:07s remains)
INFO - root - 2017-12-08 07:45:29.661824: step 45720, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.225 sec/batch; 17h:57m:02s remains)
INFO - root - 2017-12-08 07:45:31.926135: step 45730, loss = 2.28, batch loss = 2.23 (35.1 examples/sec; 0.228 sec/batch; 18h:09m:12s remains)
INFO - root - 2017-12-08 07:45:34.145401: step 45740, loss = 2.28, batch loss = 2.23 (36.6 examples/sec; 0.218 sec/batch; 17h:23m:39s remains)
INFO - root - 2017-12-08 07:45:36.389963: step 45750, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 17h:30m:07s remains)
INFO - root - 2017-12-08 07:45:38.623613: step 45760, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.220 sec/batch; 17h:32m:52s remains)
INFO - root - 2017-12-08 07:45:40.861071: step 45770, loss = 2.28, batch loss = 2.23 (35.1 examples/sec; 0.228 sec/batch; 18h:07m:47s remains)
INFO - root - 2017-12-08 07:45:43.086289: step 45780, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.226 sec/batch; 17h:57m:37s remains)
INFO - root - 2017-12-08 07:45:45.329071: step 45790, loss = 2.28, batch loss = 2.23 (35.2 examples/sec; 0.228 sec/batch; 18h:07m:21s remains)
INFO - root - 2017-12-08 07:45:47.576811: step 45800, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 17h:36m:48s remains)
2017-12-08 07:45:47.852258: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4287977 -4.4287791 -4.4287224 -4.4286666 -4.42864 -4.4286351 -4.4286456 -4.42865 -4.4286523 -4.4286847 -4.42872 -4.4287529 -4.4287734 -4.4287806 -4.4287853][-4.4287748 -4.4287572 -4.42871 -4.4286547 -4.4286284 -4.428627 -4.4286451 -4.4286532 -4.4286532 -4.4286871 -4.4287248 -4.4287567 -4.428782 -4.4287968 -4.4288044][-4.4287271 -4.4287176 -4.4286823 -4.4286342 -4.42861 -4.4286118 -4.428638 -4.428647 -4.4286427 -4.4286742 -4.42871 -4.4287424 -4.4287724 -4.4287944 -4.4288077][-4.4287214 -4.4287257 -4.4287071 -4.4286647 -4.4286404 -4.4286361 -4.4286556 -4.4286561 -4.4286461 -4.4286675 -4.4286928 -4.4287252 -4.4287562 -4.428781 -4.4288015][-4.4287281 -4.4287395 -4.4287286 -4.42869 -4.428659 -4.4286346 -4.4286304 -4.4286275 -4.4286242 -4.42865 -4.4286776 -4.4287124 -4.4287424 -4.4287629 -4.4287806][-4.4287257 -4.4287329 -4.428721 -4.4286819 -4.4286394 -4.4285808 -4.4285412 -4.4285331 -4.4285493 -4.4285984 -4.4286504 -4.4286981 -4.4287262 -4.4287319 -4.42874][-4.4287219 -4.4287257 -4.4287119 -4.42866 -4.4285865 -4.4284744 -4.4283915 -4.4283719 -4.42841 -4.4284959 -4.4285908 -4.4286666 -4.4287043 -4.4287066 -4.4287152][-4.4286971 -4.428689 -4.428668 -4.4285903 -4.4284739 -4.4283218 -4.4282179 -4.4282103 -4.4282918 -4.4284215 -4.4285564 -4.4286618 -4.4287162 -4.4287257 -4.4287391][-4.4286532 -4.4286265 -4.4285941 -4.4285 -4.4283662 -4.428216 -4.4281406 -4.4281688 -4.4282904 -4.4284382 -4.4285669 -4.4286652 -4.4287205 -4.4287333 -4.4287486][-4.4286242 -4.4285913 -4.4285617 -4.4284916 -4.4284096 -4.4283414 -4.4283185 -4.4283438 -4.4284315 -4.4285364 -4.4286213 -4.4286971 -4.428741 -4.4287472 -4.4287515][-4.42861 -4.428586 -4.4285746 -4.4285426 -4.428515 -4.4285092 -4.4285154 -4.4285274 -4.4285836 -4.4286518 -4.4286866 -4.4287276 -4.4287581 -4.4287605 -4.4287591][-4.4286246 -4.4286194 -4.4286275 -4.4286156 -4.4286089 -4.428617 -4.42863 -4.4286222 -4.428658 -4.4287133 -4.4287348 -4.4287543 -4.4287624 -4.4287539 -4.4287462][-4.4286456 -4.4286475 -4.4286571 -4.4286513 -4.4286547 -4.4286728 -4.428688 -4.428668 -4.428679 -4.4287119 -4.4287276 -4.4287519 -4.4287682 -4.4287567 -4.4287405][-4.4286575 -4.4286647 -4.4286757 -4.428668 -4.4286661 -4.4286785 -4.4286966 -4.4286742 -4.4286752 -4.4286962 -4.4287095 -4.4287362 -4.42876 -4.4287515 -4.4287367][-4.4286728 -4.4286766 -4.4286809 -4.4286733 -4.4286695 -4.4286714 -4.4286814 -4.4286723 -4.4286809 -4.4287024 -4.4287114 -4.42874 -4.428762 -4.4287496 -4.4287348]]...]
INFO - root - 2017-12-08 07:45:50.077147: step 45810, loss = 2.28, batch loss = 2.23 (36.0 examples/sec; 0.222 sec/batch; 17h:40m:56s remains)
INFO - root - 2017-12-08 07:45:52.333335: step 45820, loss = 2.28, batch loss = 2.23 (34.4 examples/sec; 0.233 sec/batch; 18h:31m:47s remains)
INFO - root - 2017-12-08 07:45:54.579108: step 45830, loss = 2.28, batch loss = 2.23 (36.6 examples/sec; 0.219 sec/batch; 17h:24m:15s remains)
INFO - root - 2017-12-08 07:45:56.852108: step 45840, loss = 2.28, batch loss = 2.23 (34.8 examples/sec; 0.230 sec/batch; 18h:16m:49s remains)
INFO - root - 2017-12-08 07:45:59.097071: step 45850, loss = 2.28, batch loss = 2.23 (36.0 examples/sec; 0.222 sec/batch; 17h:40m:28s remains)
INFO - root - 2017-12-08 07:46:01.337390: step 45860, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 17h:45m:11s remains)
INFO - root - 2017-12-08 07:46:03.568668: step 45870, loss = 2.28, batch loss = 2.23 (36.0 examples/sec; 0.222 sec/batch; 17h:42m:22s remains)
INFO - root - 2017-12-08 07:46:05.793539: step 45880, loss = 2.28, batch loss = 2.23 (36.6 examples/sec; 0.218 sec/batch; 17h:23m:34s remains)
INFO - root - 2017-12-08 07:46:08.058689: step 45890, loss = 2.28, batch loss = 2.23 (34.5 examples/sec; 0.232 sec/batch; 18h:28m:57s remains)
INFO - root - 2017-12-08 07:46:10.311868: step 45900, loss = 2.28, batch loss = 2.23 (35.6 examples/sec; 0.225 sec/batch; 17h:52m:39s remains)
2017-12-08 07:46:10.604929: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288607 -4.42886 -4.4288707 -4.4288821 -4.4288893 -4.4288793 -4.4288588 -4.4288497 -4.4288478 -4.4288521 -4.428875 -4.4289083 -4.4289427 -4.4289708 -4.4289837][-4.4287572 -4.4287605 -4.4287825 -4.4288025 -4.4288235 -4.4288206 -4.4288025 -4.4287963 -4.4287953 -4.428803 -4.4288325 -4.4288774 -4.42892 -4.4289551 -4.4289727][-4.4286575 -4.4286695 -4.4286981 -4.4287238 -4.428761 -4.4287643 -4.4287477 -4.4287353 -4.4287233 -4.4287376 -4.4287777 -4.428834 -4.4288859 -4.4289317 -4.4289546][-4.4285803 -4.4285979 -4.428616 -4.4286394 -4.4286828 -4.4286728 -4.4286509 -4.4286294 -4.4286036 -4.4286447 -4.4287076 -4.428782 -4.4288416 -4.4288979 -4.4289284][-4.4285126 -4.4285297 -4.4285321 -4.4285493 -4.4285884 -4.428544 -4.4285135 -4.4284739 -4.4284358 -4.4285026 -4.4285922 -4.4286847 -4.42876 -4.4288349 -4.4288855][-4.4284534 -4.4284549 -4.4284263 -4.4284444 -4.4284735 -4.4283757 -4.4283323 -4.4282651 -4.428215 -4.4283209 -4.4284549 -4.4285731 -4.4286723 -4.4287682 -4.4288373][-4.4284272 -4.4283943 -4.4283175 -4.4283218 -4.428328 -4.428175 -4.4281149 -4.4280038 -4.4279389 -4.4280944 -4.4282861 -4.4284439 -4.4285784 -4.4286981 -4.4287872][-4.428411 -4.4283524 -4.4282436 -4.4282322 -4.4282084 -4.4280081 -4.42793 -4.4277959 -4.427742 -4.4279518 -4.4281807 -4.4283595 -4.4285178 -4.4286532 -4.4287562][-4.4284506 -4.4283834 -4.4282722 -4.4282508 -4.4282064 -4.4279928 -4.4279156 -4.4277873 -4.4277649 -4.4279966 -4.4282246 -4.4283948 -4.4285455 -4.4286723 -4.4287658][-4.4285893 -4.4285474 -4.42848 -4.4284711 -4.428422 -4.4282365 -4.428154 -4.4280362 -4.4280329 -4.4282141 -4.4283886 -4.4285212 -4.4286351 -4.428731 -4.4288044][-4.4286947 -4.4286737 -4.4286437 -4.4286489 -4.4286208 -4.428494 -4.4284306 -4.4283381 -4.4283433 -4.4284658 -4.4285827 -4.4286823 -4.4287634 -4.428822 -4.4288659][-4.4287605 -4.4287491 -4.4287372 -4.4287496 -4.4287395 -4.4286613 -4.4286213 -4.4285612 -4.4285741 -4.4286571 -4.4287319 -4.4288039 -4.4288588 -4.4288921 -4.4289145][-4.4288235 -4.4288244 -4.42883 -4.4288454 -4.4288507 -4.4288139 -4.4287906 -4.42876 -4.4287739 -4.4288182 -4.4288592 -4.4288988 -4.4289246 -4.4289393 -4.4289503][-4.4288564 -4.4288611 -4.4288731 -4.4288888 -4.4289041 -4.4288969 -4.4288859 -4.4288754 -4.4288869 -4.4289074 -4.4289269 -4.4289451 -4.4289589 -4.428968 -4.4289761][-4.428896 -4.428894 -4.4288993 -4.4289079 -4.4289184 -4.4289184 -4.4289136 -4.4289122 -4.42892 -4.4289293 -4.4289408 -4.4289556 -4.4289722 -4.4289846 -4.4289932]]...]
INFO - root - 2017-12-08 07:46:12.830705: step 45910, loss = 2.28, batch loss = 2.23 (35.6 examples/sec; 0.225 sec/batch; 17h:52m:24s remains)
INFO - root - 2017-12-08 07:46:15.101751: step 45920, loss = 2.28, batch loss = 2.23 (34.4 examples/sec; 0.232 sec/batch; 18h:30m:10s remains)
INFO - root - 2017-12-08 07:46:17.359406: step 45930, loss = 2.28, batch loss = 2.23 (35.8 examples/sec; 0.223 sec/batch; 17h:47m:10s remains)
INFO - root - 2017-12-08 07:46:19.595570: step 45940, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 17h:43m:25s remains)
INFO - root - 2017-12-08 07:46:21.852074: step 45950, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 17h:34m:37s remains)
INFO - root - 2017-12-08 07:46:24.090549: step 45960, loss = 2.28, batch loss = 2.23 (36.7 examples/sec; 0.218 sec/batch; 17h:21m:02s remains)
INFO - root - 2017-12-08 07:46:26.346144: step 45970, loss = 2.28, batch loss = 2.23 (34.7 examples/sec; 0.230 sec/batch; 18h:20m:11s remains)
INFO - root - 2017-12-08 07:46:28.582283: step 45980, loss = 2.28, batch loss = 2.23 (35.2 examples/sec; 0.227 sec/batch; 18h:04m:28s remains)
INFO - root - 2017-12-08 07:46:30.807305: step 45990, loss = 2.28, batch loss = 2.23 (37.0 examples/sec; 0.216 sec/batch; 17h:12m:44s remains)
INFO - root - 2017-12-08 07:46:33.063096: step 46000, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 17h:26m:21s remains)
2017-12-08 07:46:33.358366: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4285851 -4.4285593 -4.4285479 -4.4285707 -4.4285908 -4.4286027 -4.4286556 -4.4286819 -4.42864 -4.4285922 -4.4285722 -4.428565 -4.4285445 -4.4285173 -4.4284921][-4.4286237 -4.4286 -4.4285855 -4.4285922 -4.4285922 -4.4285936 -4.4286356 -4.4286728 -4.4286604 -4.4286313 -4.4286194 -4.428616 -4.4286051 -4.4285951 -4.42859][-4.4285927 -4.4285731 -4.4285607 -4.4285588 -4.4285531 -4.4285536 -4.4285965 -4.4286561 -4.4286842 -4.4286852 -4.4286876 -4.4286914 -4.4286923 -4.4287019 -4.4287205][-4.4285412 -4.42852 -4.4285059 -4.4285 -4.4284959 -4.4285045 -4.4285569 -4.4286356 -4.4286909 -4.4287176 -4.4287391 -4.428762 -4.4287763 -4.4287968 -4.4288306][-4.4285197 -4.4285 -4.4284773 -4.4284544 -4.4284487 -4.4284654 -4.4285169 -4.4285913 -4.4286609 -4.4287138 -4.428761 -4.42881 -4.428843 -4.4288688 -4.4289064][-4.4285483 -4.4285321 -4.4285016 -4.428453 -4.4284234 -4.4284306 -4.4284673 -4.4285245 -4.4285951 -4.4286666 -4.4287405 -4.4288125 -4.4288654 -4.4288988 -4.4289331][-4.4286146 -4.4285851 -4.4285345 -4.4284549 -4.4283876 -4.4283671 -4.4283919 -4.4284492 -4.4285264 -4.4286122 -4.4287047 -4.4287896 -4.4288545 -4.428895 -4.4289241][-4.4286819 -4.4286304 -4.4285536 -4.4284434 -4.4283352 -4.4282856 -4.428309 -4.4283862 -4.428483 -4.4285803 -4.4286771 -4.4287629 -4.4288297 -4.4288726 -4.4288993][-4.4287329 -4.4286604 -4.4285631 -4.4284377 -4.4283094 -4.428246 -4.4282651 -4.4283533 -4.4284611 -4.4285622 -4.4286561 -4.4287415 -4.4288106 -4.428854 -4.4288783][-4.4287624 -4.4286685 -4.428565 -4.4284477 -4.4283333 -4.4282794 -4.4282928 -4.4283681 -4.4284658 -4.428565 -4.4286561 -4.4287448 -4.4288182 -4.4288573 -4.4288745][-4.42879 -4.4286838 -4.4285765 -4.4284711 -4.4283805 -4.4283414 -4.4283528 -4.4284158 -4.4285069 -4.4286075 -4.4287024 -4.4287915 -4.4288578 -4.428884 -4.4288888][-4.4288368 -4.4287343 -4.4286246 -4.4285221 -4.4284406 -4.4284067 -4.4284186 -4.428483 -4.4285765 -4.4286804 -4.4287744 -4.4288549 -4.4289069 -4.4289169 -4.4289031][-4.42889 -4.42881 -4.4287171 -4.4286304 -4.4285603 -4.42853 -4.4285388 -4.4285927 -4.4286728 -4.4287639 -4.428844 -4.4289064 -4.4289432 -4.4289389 -4.4289055][-4.4289312 -4.4288826 -4.42882 -4.428762 -4.4287162 -4.4286981 -4.4287033 -4.4287357 -4.4287882 -4.4288521 -4.4289079 -4.42895 -4.4289651 -4.4289422 -4.4288878][-4.4289546 -4.428926 -4.4288869 -4.4288497 -4.428823 -4.4288158 -4.4288235 -4.4288454 -4.4288797 -4.4289222 -4.4289541 -4.4289703 -4.42896 -4.4289141 -4.4288421]]...]
INFO - root - 2017-12-08 07:46:35.592093: step 46010, loss = 2.28, batch loss = 2.23 (34.2 examples/sec; 0.234 sec/batch; 18h:38m:29s remains)
INFO - root - 2017-12-08 07:46:37.856080: step 46020, loss = 2.28, batch loss = 2.23 (36.7 examples/sec; 0.218 sec/batch; 17h:20m:49s remains)
INFO - root - 2017-12-08 07:46:40.104642: step 46030, loss = 2.28, batch loss = 2.23 (34.9 examples/sec; 0.229 sec/batch; 18h:13m:51s remains)
INFO - root - 2017-12-08 07:46:42.345426: step 46040, loss = 2.28, batch loss = 2.23 (34.8 examples/sec; 0.230 sec/batch; 18h:17m:27s remains)
INFO - root - 2017-12-08 07:46:44.592982: step 46050, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.221 sec/batch; 17h:36m:48s remains)
INFO - root - 2017-12-08 07:46:46.816888: step 46060, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 17h:26m:54s remains)
INFO - root - 2017-12-08 07:46:49.056150: step 46070, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 17h:25m:25s remains)
INFO - root - 2017-12-08 07:46:51.282661: step 46080, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 17h:28m:02s remains)
INFO - root - 2017-12-08 07:46:53.527674: step 46090, loss = 2.28, batch loss = 2.23 (35.7 examples/sec; 0.224 sec/batch; 17h:49m:11s remains)
INFO - root - 2017-12-08 07:46:55.767877: step 46100, loss = 2.28, batch loss = 2.23 (36.6 examples/sec; 0.218 sec/batch; 17h:22m:23s remains)
2017-12-08 07:46:56.080293: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288349 -4.4288111 -4.4287968 -4.428791 -4.4287848 -4.4287915 -4.4287996 -4.4288049 -4.4288192 -4.428823 -4.4288216 -4.4288211 -4.4288216 -4.4288216 -4.42882][-4.4288163 -4.4287829 -4.4287581 -4.4287419 -4.428731 -4.4287405 -4.4287572 -4.4287729 -4.4287963 -4.4288006 -4.4287968 -4.4287934 -4.4287925 -4.428792 -4.4287906][-4.4288254 -4.42879 -4.4287605 -4.4287376 -4.4287214 -4.428731 -4.4287515 -4.4287772 -4.42881 -4.4288216 -4.4288216 -4.4288192 -4.4288168 -4.4288144 -4.4288116][-4.4288445 -4.4288073 -4.4287729 -4.4287448 -4.4287257 -4.42873 -4.4287491 -4.4287806 -4.4288225 -4.4288487 -4.428865 -4.4288759 -4.428884 -4.4288864 -4.4288845][-4.4288611 -4.428823 -4.428781 -4.4287415 -4.4287095 -4.4286971 -4.4287033 -4.4287329 -4.4287834 -4.4288278 -4.4288626 -4.4288917 -4.4289179 -4.4289341 -4.4289408][-4.4288578 -4.4288144 -4.4287581 -4.4286957 -4.4286423 -4.4286113 -4.4286065 -4.4286289 -4.4286785 -4.4287286 -4.4287705 -4.4288168 -4.4288688 -4.4289083 -4.4289312][-4.4288292 -4.428781 -4.428719 -4.4286437 -4.4285707 -4.4285212 -4.4284997 -4.4285035 -4.4285436 -4.4285893 -4.4286304 -4.42869 -4.4287677 -4.4288316 -4.4288721][-4.4288077 -4.4287648 -4.4287138 -4.4286451 -4.4285688 -4.4285135 -4.4284763 -4.4284544 -4.4284744 -4.4285073 -4.4285378 -4.428597 -4.4286804 -4.4287496 -4.4287944][-4.4288077 -4.4287844 -4.4287586 -4.4287152 -4.4286609 -4.4286261 -4.4285927 -4.4285617 -4.4285631 -4.4285774 -4.4285913 -4.4286323 -4.4286904 -4.4287338 -4.4287577][-4.4288154 -4.428812 -4.4288144 -4.4288044 -4.4287825 -4.4287705 -4.42875 -4.4287271 -4.4287195 -4.4287219 -4.4287243 -4.4287438 -4.4287682 -4.4287758 -4.4287667][-4.4288158 -4.4288206 -4.42884 -4.4288497 -4.4288483 -4.4288487 -4.4288363 -4.4288235 -4.4288177 -4.4288177 -4.4288177 -4.4288254 -4.4288263 -4.4288073 -4.4287705][-4.4288011 -4.4288044 -4.4288259 -4.4288416 -4.4288483 -4.4288511 -4.4288454 -4.4288387 -4.4288378 -4.4288507 -4.428864 -4.4288688 -4.4288507 -4.42881 -4.4287529][-4.4287682 -4.4287653 -4.4287834 -4.4287982 -4.4288063 -4.4288125 -4.4288116 -4.4288039 -4.428803 -4.4288187 -4.42884 -4.4288483 -4.4288282 -4.4287925 -4.4287415][-4.4287329 -4.4287233 -4.4287372 -4.428751 -4.4287591 -4.4287667 -4.4287682 -4.42876 -4.4287548 -4.4287648 -4.4287891 -4.4288044 -4.4287987 -4.4287858 -4.4287648][-4.4287162 -4.4286981 -4.4287033 -4.4287076 -4.4287114 -4.4287238 -4.4287319 -4.428731 -4.42873 -4.42874 -4.4287653 -4.4287887 -4.4287996 -4.428812 -4.4288187]]...]
INFO - root - 2017-12-08 07:46:58.332448: step 46110, loss = 2.28, batch loss = 2.23 (34.4 examples/sec; 0.232 sec/batch; 18h:29m:35s remains)
INFO - root - 2017-12-08 07:47:00.571965: step 46120, loss = 2.28, batch loss = 2.23 (35.7 examples/sec; 0.224 sec/batch; 17h:48m:45s remains)
INFO - root - 2017-12-08 07:47:02.784169: step 46130, loss = 2.28, batch loss = 2.23 (35.2 examples/sec; 0.227 sec/batch; 18h:03m:46s remains)
INFO - root - 2017-12-08 07:47:05.062631: step 46140, loss = 2.28, batch loss = 2.23 (35.3 examples/sec; 0.227 sec/batch; 18h:01m:27s remains)
INFO - root - 2017-12-08 07:47:07.278392: step 46150, loss = 2.28, batch loss = 2.23 (36.7 examples/sec; 0.218 sec/batch; 17h:19m:50s remains)
INFO - root - 2017-12-08 07:47:09.513594: step 46160, loss = 2.28, batch loss = 2.23 (35.0 examples/sec; 0.228 sec/batch; 18h:09m:20s remains)
INFO - root - 2017-12-08 07:47:11.757541: step 46170, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 17h:44m:01s remains)
INFO - root - 2017-12-08 07:47:14.024817: step 46180, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 17h:44m:07s remains)
INFO - root - 2017-12-08 07:47:16.283081: step 46190, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.222 sec/batch; 17h:37m:06s remains)
INFO - root - 2017-12-08 07:47:18.514847: step 46200, loss = 2.28, batch loss = 2.23 (36.6 examples/sec; 0.219 sec/batch; 17h:23m:47s remains)
2017-12-08 07:47:18.805018: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288545 -4.4288507 -4.4288321 -4.4288197 -4.4288173 -4.4288087 -4.4288144 -4.4288535 -4.4288898 -4.4288898 -4.4288669 -4.4288254 -4.4287691 -4.4287367 -4.4287505][-4.4288278 -4.4288177 -4.42879 -4.4287806 -4.4287868 -4.4287772 -4.4287772 -4.428813 -4.4288459 -4.4288363 -4.4288054 -4.4287519 -4.4286885 -4.4286661 -4.4287033][-4.4288111 -4.4288006 -4.4287753 -4.4287682 -4.4287729 -4.4287539 -4.4287405 -4.4287643 -4.4287891 -4.4287739 -4.4287329 -4.4286671 -4.428597 -4.4285836 -4.4286337][-4.4288168 -4.4288268 -4.4288116 -4.4288034 -4.4288015 -4.4287705 -4.4287381 -4.4287405 -4.4287453 -4.4287219 -4.4286757 -4.4286056 -4.4285421 -4.4285393 -4.4285913][-4.4288177 -4.4288564 -4.42885 -4.4288354 -4.428823 -4.4287729 -4.4287162 -4.42869 -4.4286695 -4.4286351 -4.4285955 -4.428534 -4.4284863 -4.4285016 -4.4285603][-4.42882 -4.4288745 -4.4288712 -4.4288378 -4.4288015 -4.4287267 -4.4286327 -4.4285669 -4.42852 -4.428493 -4.4284921 -4.4284673 -4.4284463 -4.4284759 -4.42855][-4.4288664 -4.4289165 -4.4289031 -4.4288454 -4.4287748 -4.4286489 -4.4284797 -4.4283495 -4.4282804 -4.4282908 -4.4283714 -4.4284225 -4.4284434 -4.4284835 -4.4285603][-4.4289441 -4.4289813 -4.4289556 -4.4288731 -4.4287596 -4.4285665 -4.4283009 -4.4280915 -4.4279981 -4.4280515 -4.4282374 -4.42839 -4.4284768 -4.4285359 -4.4286046][-4.4289951 -4.4290147 -4.4289775 -4.4288793 -4.42874 -4.4285283 -4.4282451 -4.4280233 -4.4279261 -4.4279876 -4.42821 -4.428412 -4.4285464 -4.428628 -4.4286861][-4.4290028 -4.4290009 -4.4289613 -4.428865 -4.4287372 -4.42856 -4.42834 -4.4281912 -4.4281368 -4.4281964 -4.42837 -4.4285378 -4.4286633 -4.4287381 -4.4287815][-4.4289727 -4.4289556 -4.4289165 -4.4288373 -4.4287415 -4.4286175 -4.4284811 -4.4284163 -4.4284067 -4.4284616 -4.4285793 -4.4286985 -4.4287877 -4.4288354 -4.4288564][-4.4289408 -4.4289179 -4.4288893 -4.428843 -4.4287863 -4.4287152 -4.4286394 -4.4286151 -4.4286218 -4.428658 -4.4287381 -4.428823 -4.4288745 -4.4288907 -4.4288878][-4.4289222 -4.4289069 -4.428896 -4.4288816 -4.42886 -4.4288225 -4.4287739 -4.4287553 -4.4287615 -4.428781 -4.4288363 -4.4288917 -4.4289117 -4.4288993 -4.4288726][-4.4288988 -4.4289036 -4.4289174 -4.4289336 -4.428946 -4.4289269 -4.4288874 -4.4288597 -4.4288559 -4.42886 -4.4288926 -4.4289227 -4.4289165 -4.4288797 -4.4288349][-4.4288611 -4.4288816 -4.428915 -4.4289513 -4.4289775 -4.4289675 -4.428937 -4.4289103 -4.4289026 -4.4289017 -4.4289188 -4.4289303 -4.4289045 -4.4288483 -4.4287868]]...]
INFO - root - 2017-12-08 07:47:21.022496: step 46210, loss = 2.28, batch loss = 2.23 (36.6 examples/sec; 0.219 sec/batch; 17h:23m:19s remains)
INFO - root - 2017-12-08 07:47:23.264906: step 46220, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.222 sec/batch; 17h:37m:41s remains)
INFO - root - 2017-12-08 07:47:25.522741: step 46230, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.225 sec/batch; 17h:55m:26s remains)
INFO - root - 2017-12-08 07:47:27.735637: step 46240, loss = 2.28, batch loss = 2.23 (36.8 examples/sec; 0.217 sec/batch; 17h:16m:40s remains)
INFO - root - 2017-12-08 07:47:29.976762: step 46250, loss = 2.28, batch loss = 2.23 (36.6 examples/sec; 0.219 sec/batch; 17h:23m:22s remains)
INFO - root - 2017-12-08 07:47:32.211408: step 46260, loss = 2.28, batch loss = 2.23 (35.0 examples/sec; 0.229 sec/batch; 18h:10m:12s remains)
INFO - root - 2017-12-08 07:47:34.469078: step 46270, loss = 2.28, batch loss = 2.23 (35.8 examples/sec; 0.224 sec/batch; 17h:47m:02s remains)
INFO - root - 2017-12-08 07:47:36.692510: step 46280, loss = 2.28, batch loss = 2.23 (35.0 examples/sec; 0.229 sec/batch; 18h:10m:12s remains)
INFO - root - 2017-12-08 07:47:38.947572: step 46290, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 17h:28m:02s remains)
INFO - root - 2017-12-08 07:47:41.215398: step 46300, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.221 sec/batch; 17h:32m:40s remains)
2017-12-08 07:47:41.498047: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4287653 -4.4287667 -4.4287691 -4.4288039 -4.4288373 -4.4288416 -4.428802 -4.4287577 -4.4287572 -4.4287915 -4.4288282 -4.4288516 -4.4288588 -4.4288492 -4.4288516][-4.4287696 -4.4287686 -4.4287729 -4.428803 -4.4288239 -4.4288192 -4.4287791 -4.4287429 -4.4287519 -4.4287996 -4.4288511 -4.4288831 -4.4288964 -4.4288826 -4.4288764][-4.4287453 -4.4287395 -4.4287438 -4.4287648 -4.4287777 -4.4287682 -4.4287376 -4.4287157 -4.4287333 -4.4287915 -4.4288545 -4.4289017 -4.428925 -4.4289126 -4.4289036][-4.4286981 -4.428689 -4.4286942 -4.4287109 -4.4287214 -4.428719 -4.4287062 -4.4287004 -4.4287176 -4.4287753 -4.428843 -4.4288964 -4.4289222 -4.4289174 -4.4289126][-4.428668 -4.4286556 -4.428659 -4.4286737 -4.4286804 -4.4286823 -4.4286842 -4.42869 -4.4287047 -4.4287591 -4.4288244 -4.4288735 -4.4288979 -4.4289012 -4.4289045][-4.42868 -4.4286647 -4.4286566 -4.4286642 -4.4286733 -4.4286795 -4.4286814 -4.428688 -4.4287047 -4.4287562 -4.4288135 -4.4288564 -4.4288778 -4.428885 -4.42889][-4.4286571 -4.4286413 -4.4286222 -4.428618 -4.428628 -4.4286366 -4.4286289 -4.4286346 -4.4286623 -4.4287171 -4.4287748 -4.42882 -4.4288445 -4.4288597 -4.4288721][-4.4286122 -4.4285836 -4.4285488 -4.4285312 -4.4285412 -4.4285469 -4.4285259 -4.4285269 -4.4285636 -4.4286242 -4.4286861 -4.4287424 -4.4287806 -4.4288154 -4.4288421][-4.4286275 -4.4285755 -4.4285245 -4.4284945 -4.4285 -4.4285011 -4.4284668 -4.4284511 -4.4284735 -4.4285216 -4.4285779 -4.4286413 -4.4286981 -4.4287558 -4.4288034][-4.4286938 -4.4286342 -4.4285841 -4.4285541 -4.4285569 -4.4285612 -4.4285288 -4.4284954 -4.42849 -4.4285154 -4.428556 -4.42861 -4.4286695 -4.4287329 -4.4287896][-4.4287424 -4.4286933 -4.4286513 -4.4286237 -4.4286251 -4.4286289 -4.4285979 -4.4285622 -4.4285502 -4.4285674 -4.4286017 -4.4286494 -4.4287043 -4.428761 -4.428812][-4.4287672 -4.4287372 -4.4287095 -4.4286866 -4.4286885 -4.4286871 -4.4286561 -4.4286289 -4.4286251 -4.4286509 -4.4286895 -4.4287391 -4.4287882 -4.4288278 -4.4288592][-4.428782 -4.4287653 -4.428751 -4.4287343 -4.4287314 -4.4287195 -4.4286871 -4.4286675 -4.42868 -4.428721 -4.4287696 -4.4288263 -4.4288712 -4.4288945 -4.4289088][-4.4288154 -4.4288049 -4.4287987 -4.428791 -4.4287863 -4.4287667 -4.4287357 -4.4287224 -4.4287443 -4.4287915 -4.4288416 -4.4288983 -4.4289365 -4.4289465 -4.428947][-4.4288125 -4.4288049 -4.4288092 -4.4288206 -4.4288311 -4.4288225 -4.428802 -4.428793 -4.4288144 -4.4288549 -4.4288955 -4.4289403 -4.4289689 -4.4289708 -4.428966]]...]
INFO - root - 2017-12-08 07:47:43.760121: step 46310, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.222 sec/batch; 17h:36m:39s remains)
INFO - root - 2017-12-08 07:47:45.990636: step 46320, loss = 2.28, batch loss = 2.23 (35.2 examples/sec; 0.227 sec/batch; 18h:04m:54s remains)
INFO - root - 2017-12-08 07:47:48.227214: step 46330, loss = 2.28, batch loss = 2.23 (35.3 examples/sec; 0.227 sec/batch; 18h:01m:23s remains)
INFO - root - 2017-12-08 07:47:50.461080: step 46340, loss = 2.28, batch loss = 2.23 (34.9 examples/sec; 0.229 sec/batch; 18h:11m:54s remains)
INFO - root - 2017-12-08 07:47:52.701980: step 46350, loss = 2.28, batch loss = 2.23 (33.1 examples/sec; 0.241 sec/batch; 19h:11m:42s remains)
INFO - root - 2017-12-08 07:47:54.953010: step 46360, loss = 2.28, batch loss = 2.23 (35.8 examples/sec; 0.223 sec/batch; 17h:44m:41s remains)
INFO - root - 2017-12-08 07:47:57.171371: step 46370, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 17h:25m:35s remains)
INFO - root - 2017-12-08 07:47:59.396987: step 46380, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 17h:28m:21s remains)
INFO - root - 2017-12-08 07:48:01.635132: step 46390, loss = 2.28, batch loss = 2.23 (34.2 examples/sec; 0.234 sec/batch; 18h:36m:50s remains)
INFO - root - 2017-12-08 07:48:03.882275: step 46400, loss = 2.28, batch loss = 2.23 (35.6 examples/sec; 0.225 sec/batch; 17h:51m:08s remains)
2017-12-08 07:48:04.214176: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289432 -4.4289427 -4.4289455 -4.4289522 -4.4289584 -4.4289589 -4.4289556 -4.4289541 -4.4289527 -4.42895 -4.4289474 -4.428946 -4.428947 -4.4289503 -4.4289575][-4.4289093 -4.4289064 -4.42891 -4.4289203 -4.4289293 -4.4289308 -4.4289265 -4.4289284 -4.4289274 -4.4289188 -4.42891 -4.4289036 -4.4289045 -4.4289141 -4.4289303][-4.428843 -4.4288354 -4.4288368 -4.4288507 -4.4288616 -4.4288588 -4.4288516 -4.4288545 -4.4288516 -4.4288321 -4.4288092 -4.4287977 -4.4288063 -4.4288278 -4.4288588][-4.4287472 -4.4287324 -4.4287271 -4.4287372 -4.4287405 -4.4287267 -4.4287138 -4.4287181 -4.4287119 -4.42868 -4.4286451 -4.4286313 -4.4286532 -4.4286947 -4.4287457][-4.4286408 -4.42862 -4.428606 -4.4286108 -4.4286013 -4.4285684 -4.4285398 -4.4285321 -4.4285216 -4.4284983 -4.4284787 -4.4284768 -4.4285092 -4.4285631 -4.4286294][-4.4285517 -4.4285269 -4.4285121 -4.4285188 -4.428503 -4.4284534 -4.4283972 -4.4283509 -4.42833 -4.4283409 -4.4283638 -4.4283915 -4.4284329 -4.4284892 -4.4285545][-4.4284744 -4.428443 -4.4284348 -4.4284511 -4.428442 -4.4283915 -4.428309 -4.4282084 -4.4281745 -4.4282322 -4.4283071 -4.4283695 -4.42842 -4.4284649 -4.4285169][-4.4284306 -4.4283948 -4.4284015 -4.4284315 -4.4284391 -4.4284 -4.4283195 -4.4282055 -4.4281716 -4.428247 -4.4283414 -4.4284134 -4.4284563 -4.4284787 -4.4285111][-4.428463 -4.4284296 -4.4284477 -4.4284897 -4.4285121 -4.4284854 -4.4284215 -4.4283371 -4.4283195 -4.4283772 -4.4284511 -4.4285135 -4.428544 -4.4285417 -4.4285445][-4.4285507 -4.4285278 -4.4285502 -4.4285946 -4.428617 -4.4285889 -4.4285355 -4.4284859 -4.4284816 -4.4285216 -4.4285793 -4.4286389 -4.4286623 -4.428637 -4.428606][-4.4286437 -4.4286337 -4.4286537 -4.4286866 -4.4286947 -4.4286623 -4.4286189 -4.4286 -4.4286036 -4.4286327 -4.4286814 -4.4287424 -4.4287634 -4.4287276 -4.4286709][-4.4287024 -4.4287009 -4.4287157 -4.4287324 -4.4287257 -4.4286985 -4.4286733 -4.4286723 -4.428678 -4.4287033 -4.42875 -4.4288154 -4.4288378 -4.4287949 -4.4287205][-4.4287162 -4.42872 -4.428731 -4.4287405 -4.4287348 -4.4287248 -4.42872 -4.4287291 -4.4287338 -4.4287558 -4.4288011 -4.428863 -4.4288754 -4.4288254 -4.4287434][-4.4286995 -4.4287138 -4.4287314 -4.4287491 -4.4287558 -4.4287648 -4.4287753 -4.4287887 -4.428791 -4.4288082 -4.4288483 -4.4289002 -4.4289 -4.4288397 -4.4287491][-4.4286714 -4.4287057 -4.4287372 -4.42877 -4.4287953 -4.4288168 -4.428834 -4.4288483 -4.4288468 -4.4288564 -4.4288878 -4.428926 -4.4289126 -4.4288425 -4.4287424]]...]
INFO - root - 2017-12-08 07:48:06.491691: step 46410, loss = 2.28, batch loss = 2.23 (33.9 examples/sec; 0.236 sec/batch; 18h:46m:39s remains)
INFO - root - 2017-12-08 07:48:08.727434: step 46420, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.221 sec/batch; 17h:35m:41s remains)
INFO - root - 2017-12-08 07:48:10.946975: step 46430, loss = 2.28, batch loss = 2.23 (36.8 examples/sec; 0.217 sec/batch; 17h:16m:09s remains)
INFO - root - 2017-12-08 07:48:13.180976: step 46440, loss = 2.28, batch loss = 2.23 (36.0 examples/sec; 0.222 sec/batch; 17h:39m:11s remains)
INFO - root - 2017-12-08 07:48:15.437682: step 46450, loss = 2.28, batch loss = 2.23 (34.6 examples/sec; 0.231 sec/batch; 18h:22m:21s remains)
INFO - root - 2017-12-08 07:48:17.661867: step 46460, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 17h:43m:09s remains)
INFO - root - 2017-12-08 07:48:19.952614: step 46470, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.221 sec/batch; 17h:35m:06s remains)
INFO - root - 2017-12-08 07:48:22.211469: step 46480, loss = 2.28, batch loss = 2.23 (34.5 examples/sec; 0.232 sec/batch; 18h:26m:39s remains)
INFO - root - 2017-12-08 07:48:24.466754: step 46490, loss = 2.28, batch loss = 2.23 (36.0 examples/sec; 0.222 sec/batch; 17h:38m:52s remains)
INFO - root - 2017-12-08 07:48:26.708694: step 46500, loss = 2.28, batch loss = 2.23 (35.1 examples/sec; 0.228 sec/batch; 18h:07m:56s remains)
2017-12-08 07:48:27.012603: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.428906 -4.428896 -4.4288936 -4.428884 -4.4288616 -4.428854 -4.42886 -4.4288583 -4.4288597 -4.4288607 -4.428863 -4.4288731 -4.4288769 -4.4288778 -4.428885][-4.4288912 -4.4288807 -4.4288807 -4.4288721 -4.4288459 -4.4288387 -4.4288421 -4.4288368 -4.4288368 -4.4288378 -4.4288435 -4.4288554 -4.4288583 -4.4288588 -4.4288669][-4.4288611 -4.4288511 -4.4288607 -4.4288607 -4.4288387 -4.4288287 -4.4288211 -4.4288034 -4.428791 -4.4287949 -4.4288225 -4.4288473 -4.4288549 -4.4288564 -4.4288621][-4.4288225 -4.428813 -4.4288344 -4.4288459 -4.4288344 -4.4288211 -4.4287868 -4.4287333 -4.4286919 -4.4287081 -4.428781 -4.4288354 -4.4288616 -4.4288645 -4.4288645][-4.4287806 -4.4287739 -4.4288054 -4.4288216 -4.4288216 -4.4288054 -4.4287376 -4.4286304 -4.42855 -4.4285917 -4.42872 -4.42881 -4.4288559 -4.4288588 -4.428854][-4.4287405 -4.4287357 -4.4287763 -4.4287996 -4.4288092 -4.4287872 -4.4286871 -4.4285188 -4.4283996 -4.4284911 -4.4286737 -4.4287934 -4.4288483 -4.4288416 -4.4288216][-4.4287095 -4.4287038 -4.4287481 -4.4287858 -4.4288073 -4.428771 -4.4286275 -4.4283895 -4.4282436 -4.4284 -4.428638 -4.4287839 -4.4288349 -4.4288154 -4.4287767][-4.4286938 -4.4286933 -4.4287405 -4.4287915 -4.4288135 -4.4287486 -4.4285617 -4.4282742 -4.4281225 -4.4283457 -4.4286308 -4.4287887 -4.4288235 -4.4287786 -4.42872][-4.4286985 -4.4287114 -4.4287634 -4.42882 -4.4288363 -4.4287448 -4.4285331 -4.428246 -4.4281235 -4.4283748 -4.4286628 -4.4288025 -4.428802 -4.4287248 -4.4286575][-4.4287081 -4.4287395 -4.4287977 -4.4288549 -4.4288673 -4.4287686 -4.428565 -4.4283261 -4.4282546 -4.4284768 -4.4287105 -4.4288058 -4.4287653 -4.4286637 -4.4286137][-4.4287267 -4.4287715 -4.4288363 -4.4288893 -4.4288974 -4.4288063 -4.4286323 -4.4284592 -4.4284353 -4.428606 -4.4287639 -4.4288058 -4.4287319 -4.4286323 -4.4286151][-4.4287591 -4.4288082 -4.4288726 -4.4289174 -4.42892 -4.4288449 -4.4287114 -4.4285979 -4.4286013 -4.4287186 -4.4288063 -4.4288034 -4.4287186 -4.4286475 -4.4286661][-4.4288044 -4.4288468 -4.4289017 -4.428936 -4.4289322 -4.4288716 -4.4287753 -4.428709 -4.4287248 -4.4287939 -4.4288311 -4.4288073 -4.4287376 -4.4287038 -4.4287472][-4.4288454 -4.4288726 -4.4289107 -4.4289346 -4.4289279 -4.4288864 -4.4288259 -4.4287896 -4.4288054 -4.4288373 -4.4288425 -4.4288177 -4.4287806 -4.4287777 -4.428822][-4.4288888 -4.4289026 -4.4289217 -4.4289341 -4.4289284 -4.4289036 -4.4288712 -4.4288564 -4.4288664 -4.4288754 -4.4288683 -4.428853 -4.4288425 -4.4288535 -4.4288826]]...]
INFO - root - 2017-12-08 07:48:29.217809: step 46510, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.225 sec/batch; 17h:53m:23s remains)
INFO - root - 2017-12-08 07:48:31.440322: step 46520, loss = 2.28, batch loss = 2.23 (34.9 examples/sec; 0.229 sec/batch; 18h:12m:50s remains)
INFO - root - 2017-12-08 07:48:33.668929: step 46530, loss = 2.28, batch loss = 2.23 (35.8 examples/sec; 0.224 sec/batch; 17h:45m:57s remains)
INFO - root - 2017-12-08 07:48:35.912080: step 46540, loss = 2.28, batch loss = 2.23 (36.0 examples/sec; 0.222 sec/batch; 17h:38m:01s remains)
INFO - root - 2017-12-08 07:48:38.134085: step 46550, loss = 2.28, batch loss = 2.23 (35.8 examples/sec; 0.224 sec/batch; 17h:45m:42s remains)
INFO - root - 2017-12-08 07:48:40.373428: step 46560, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.220 sec/batch; 17h:29m:51s remains)
INFO - root - 2017-12-08 07:48:42.603175: step 46570, loss = 2.28, batch loss = 2.23 (35.8 examples/sec; 0.223 sec/batch; 17h:43m:27s remains)
INFO - root - 2017-12-08 07:48:44.844395: step 46580, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.225 sec/batch; 17h:52m:49s remains)
INFO - root - 2017-12-08 07:48:47.112528: step 46590, loss = 2.28, batch loss = 2.23 (35.2 examples/sec; 0.227 sec/batch; 18h:02m:35s remains)
INFO - root - 2017-12-08 07:48:49.323542: step 46600, loss = 2.28, batch loss = 2.23 (35.8 examples/sec; 0.223 sec/batch; 17h:43m:58s remains)
2017-12-08 07:48:49.620214: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289441 -4.4289379 -4.4289231 -4.4288783 -4.4288397 -4.4288163 -4.4287887 -4.4287677 -4.4287877 -4.4288383 -4.4288683 -4.4288821 -4.4288831 -4.4288697 -4.4288511][-4.4289536 -4.4289494 -4.4289446 -4.4289069 -4.4288673 -4.4288435 -4.4288158 -4.4287877 -4.4287915 -4.4288297 -4.4288521 -4.4288583 -4.4288526 -4.4288306 -4.4288116][-4.4289389 -4.4289351 -4.4289412 -4.428916 -4.4288745 -4.4288425 -4.4288082 -4.4287748 -4.4287715 -4.4288044 -4.4288187 -4.4288187 -4.4288054 -4.4287715 -4.4287572][-4.4288926 -4.4288716 -4.42888 -4.4288673 -4.4288335 -4.4287915 -4.4287372 -4.4286876 -4.4286819 -4.4287205 -4.4287472 -4.4287653 -4.4287434 -4.4286828 -4.4286647][-4.428812 -4.4287586 -4.4287477 -4.4287391 -4.4287186 -4.4286661 -4.4285736 -4.4284806 -4.4284716 -4.4285464 -4.4286237 -4.428679 -4.4286504 -4.4285474 -4.4285154][-4.4287181 -4.4286356 -4.4286022 -4.4285865 -4.4285703 -4.428503 -4.4283485 -4.4281688 -4.4281416 -4.4283004 -4.42847 -4.4285779 -4.4285479 -4.4283996 -4.4283428][-4.428658 -4.4285679 -4.428525 -4.4284992 -4.4284697 -4.4283934 -4.4281836 -4.427906 -4.4278288 -4.4280734 -4.4283433 -4.4285164 -4.4285078 -4.428339 -4.428256][-4.4286675 -4.4285941 -4.4285607 -4.4285226 -4.4284759 -4.4284148 -4.4282317 -4.4279666 -4.4278469 -4.4280486 -4.4283295 -4.4285307 -4.4285593 -4.42842 -4.4283438][-4.4287243 -4.4286785 -4.4286656 -4.428628 -4.4285836 -4.4285445 -4.4284239 -4.4282451 -4.4281306 -4.4282379 -4.4284406 -4.4286113 -4.4286656 -4.4285789 -4.4285264][-4.4287891 -4.4287629 -4.4287653 -4.4287472 -4.4287243 -4.4287086 -4.4286232 -4.4285107 -4.4284267 -4.42848 -4.42861 -4.4287295 -4.4287853 -4.4287467 -4.4287186][-4.428853 -4.4288449 -4.428853 -4.4288507 -4.4288459 -4.4288454 -4.4287853 -4.4287081 -4.4286437 -4.4286714 -4.4287567 -4.4288449 -4.4288917 -4.4288826 -4.4288707][-4.4289045 -4.4289083 -4.4289184 -4.4289212 -4.4289231 -4.4289308 -4.4288931 -4.4288421 -4.428793 -4.4288049 -4.4288578 -4.4289246 -4.4289579 -4.4289541 -4.428947][-4.4289341 -4.4289336 -4.4289374 -4.4289336 -4.4289279 -4.4289403 -4.4289222 -4.4288983 -4.4288692 -4.4288778 -4.428915 -4.4289603 -4.4289751 -4.4289656 -4.4289556][-4.4289331 -4.4289222 -4.428916 -4.4289002 -4.428885 -4.4288945 -4.4288864 -4.4288821 -4.428874 -4.4288878 -4.428916 -4.4289427 -4.4289451 -4.4289336 -4.4289255][-4.4289231 -4.4289064 -4.4288926 -4.4288726 -4.4288568 -4.4288626 -4.4288564 -4.4288545 -4.4288535 -4.4288659 -4.428885 -4.4289007 -4.4289031 -4.4288983 -4.4288936]]...]
INFO - root - 2017-12-08 07:48:51.853858: step 46610, loss = 2.28, batch loss = 2.23 (35.2 examples/sec; 0.228 sec/batch; 18h:04m:22s remains)
INFO - root - 2017-12-08 07:48:54.104998: step 46620, loss = 2.28, batch loss = 2.23 (35.1 examples/sec; 0.228 sec/batch; 18h:04m:33s remains)
INFO - root - 2017-12-08 07:48:56.360700: step 46630, loss = 2.28, batch loss = 2.23 (35.3 examples/sec; 0.227 sec/batch; 18h:00m:37s remains)
INFO - root - 2017-12-08 07:48:58.608695: step 46640, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.221 sec/batch; 17h:30m:55s remains)
INFO - root - 2017-12-08 07:49:00.835511: step 46650, loss = 2.28, batch loss = 2.23 (36.7 examples/sec; 0.218 sec/batch; 17h:18m:14s remains)
INFO - root - 2017-12-08 07:49:03.074352: step 46660, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 17h:23m:27s remains)
INFO - root - 2017-12-08 07:49:05.298320: step 46670, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 17h:40m:11s remains)
INFO - root - 2017-12-08 07:49:07.541375: step 46680, loss = 2.28, batch loss = 2.23 (35.1 examples/sec; 0.228 sec/batch; 18h:06m:21s remains)
INFO - root - 2017-12-08 07:49:09.790006: step 46690, loss = 2.28, batch loss = 2.23 (34.9 examples/sec; 0.229 sec/batch; 18h:11m:41s remains)
INFO - root - 2017-12-08 07:49:12.011290: step 46700, loss = 2.28, batch loss = 2.23 (36.8 examples/sec; 0.217 sec/batch; 17h:14m:39s remains)
2017-12-08 07:49:12.288997: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288216 -4.428771 -4.4286747 -4.4285674 -4.4284997 -4.428515 -4.4285784 -4.428638 -4.4286671 -4.4286718 -4.4286976 -4.4287271 -4.4287195 -4.42869 -4.4286704][-4.4288421 -4.4288011 -4.4287028 -4.4285865 -4.4284968 -4.4284782 -4.4285259 -4.428597 -4.4286618 -4.4287062 -4.4287491 -4.4287782 -4.42878 -4.4287643 -4.4287505][-4.4288383 -4.4288192 -4.4287348 -4.4286213 -4.428514 -4.4284506 -4.4284625 -4.4285421 -4.4286556 -4.42875 -4.4288182 -4.428854 -4.4288669 -4.4288588 -4.4288435][-4.428843 -4.4288473 -4.4287882 -4.4286828 -4.4285541 -4.4284382 -4.4283891 -4.428452 -4.4286013 -4.4287443 -4.42884 -4.4288898 -4.428915 -4.4289107 -4.42889][-4.428863 -4.4288774 -4.4288359 -4.4287457 -4.4286156 -4.4284658 -4.4283566 -4.4283729 -4.4285254 -4.4286952 -4.4288116 -4.4288745 -4.42891 -4.4289079 -4.4288816][-4.4288912 -4.4289026 -4.428864 -4.4287815 -4.4286551 -4.4284959 -4.4283442 -4.4283061 -4.4284453 -4.4286308 -4.4287748 -4.4288583 -4.4289012 -4.428895 -4.428865][-4.4289122 -4.4289179 -4.4288707 -4.4287853 -4.42866 -4.4285107 -4.4283605 -4.4283109 -4.4284353 -4.4286232 -4.4287806 -4.4288688 -4.4289088 -4.4288931 -4.4288545][-4.4289126 -4.4289083 -4.4288497 -4.4287548 -4.4286337 -4.4285178 -4.4284215 -4.428411 -4.4285269 -4.4286923 -4.4288292 -4.4288983 -4.4289145 -4.4288807 -4.4288316][-4.42889 -4.4288712 -4.4288025 -4.4286995 -4.4285817 -4.4285078 -4.4284787 -4.4285221 -4.428638 -4.4287729 -4.4288764 -4.4289179 -4.428906 -4.428853 -4.4287906][-4.4288778 -4.4288473 -4.4287729 -4.4286594 -4.4285336 -4.4284787 -4.4284964 -4.4285874 -4.4287148 -4.4288316 -4.4289064 -4.4289203 -4.4288816 -4.4288049 -4.4287276][-4.428865 -4.4288383 -4.4287739 -4.4286637 -4.4285336 -4.4284768 -4.4285088 -4.4286265 -4.428762 -4.428864 -4.4289255 -4.4289312 -4.4288788 -4.4287868 -4.4286971][-4.4288554 -4.42884 -4.4287925 -4.4286976 -4.4285827 -4.4285283 -4.4285641 -4.4286909 -4.4288187 -4.428905 -4.4289551 -4.4289556 -4.4289007 -4.4288073 -4.428721][-4.4288354 -4.42884 -4.4288149 -4.4287405 -4.4286513 -4.4286113 -4.4286551 -4.4287729 -4.4288793 -4.428946 -4.4289784 -4.4289727 -4.428926 -4.4288521 -4.4287839][-4.4287949 -4.428823 -4.428823 -4.4287772 -4.4287152 -4.4286904 -4.4287381 -4.4288392 -4.4289207 -4.4289641 -4.4289794 -4.4289618 -4.4289217 -4.4288659 -4.4288197][-4.4287343 -4.4287777 -4.4287863 -4.4287481 -4.4287028 -4.428688 -4.4287386 -4.4288182 -4.4288788 -4.4289136 -4.4289255 -4.4288993 -4.4288635 -4.4288268 -4.4288054]]...]
INFO - root - 2017-12-08 07:49:14.520365: step 46710, loss = 2.28, batch loss = 2.23 (34.5 examples/sec; 0.232 sec/batch; 18h:24m:37s remains)
INFO - root - 2017-12-08 07:49:16.790725: step 46720, loss = 2.28, batch loss = 2.23 (37.7 examples/sec; 0.212 sec/batch; 16h:50m:23s remains)
INFO - root - 2017-12-08 07:49:19.052847: step 46730, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 17h:31m:23s remains)
INFO - root - 2017-12-08 07:49:21.286518: step 46740, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.225 sec/batch; 17h:53m:41s remains)
INFO - root - 2017-12-08 07:49:23.513554: step 46750, loss = 2.28, batch loss = 2.23 (36.7 examples/sec; 0.218 sec/batch; 17h:18m:45s remains)
INFO - root - 2017-12-08 07:49:25.751796: step 46760, loss = 2.28, batch loss = 2.23 (35.6 examples/sec; 0.225 sec/batch; 17h:49m:22s remains)
INFO - root - 2017-12-08 07:49:28.002103: step 46770, loss = 2.28, batch loss = 2.23 (35.3 examples/sec; 0.227 sec/batch; 17h:59m:03s remains)
INFO - root - 2017-12-08 07:49:30.236331: step 46780, loss = 2.28, batch loss = 2.23 (35.4 examples/sec; 0.226 sec/batch; 17h:56m:50s remains)
INFO - root - 2017-12-08 07:49:32.457541: step 46790, loss = 2.28, batch loss = 2.23 (36.8 examples/sec; 0.217 sec/batch; 17h:15m:33s remains)
INFO - root - 2017-12-08 07:49:34.698079: step 46800, loss = 2.28, batch loss = 2.23 (35.2 examples/sec; 0.227 sec/batch; 18h:00m:57s remains)
2017-12-08 07:49:34.986500: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288712 -4.4288836 -4.4288912 -4.4288878 -4.4288816 -4.4288774 -4.428854 -4.4288268 -4.4288177 -4.4288192 -4.4288211 -4.4288206 -4.4288139 -4.4288054 -4.4287958][-4.4288588 -4.428874 -4.4288788 -4.4288673 -4.4288492 -4.428833 -4.4288049 -4.4287763 -4.4287629 -4.4287624 -4.428762 -4.4287648 -4.4287572 -4.4287467 -4.4287419][-4.4288526 -4.4288654 -4.4288597 -4.4288321 -4.4287939 -4.4287677 -4.4287519 -4.4287329 -4.428719 -4.4287162 -4.4287133 -4.42872 -4.4287195 -4.4287176 -4.428719][-4.4288464 -4.4288526 -4.428834 -4.4287863 -4.4287252 -4.4286914 -4.4286871 -4.4286723 -4.4286623 -4.4286671 -4.4286685 -4.4286771 -4.4286747 -4.4286737 -4.4286737][-4.42883 -4.428823 -4.4287872 -4.4287205 -4.4286427 -4.428597 -4.4285874 -4.4285717 -4.4285793 -4.4286118 -4.4286294 -4.4286208 -4.4286008 -4.4286032 -4.4286075][-4.4288135 -4.428791 -4.4287415 -4.4286704 -4.4285822 -4.428514 -4.4284754 -4.428441 -4.4284825 -4.4285626 -4.4286089 -4.4285879 -4.4285388 -4.42854 -4.4285531][-4.428781 -4.4287415 -4.428679 -4.4286113 -4.4285183 -4.4284205 -4.4283247 -4.4282484 -4.4283385 -4.4284749 -4.4285517 -4.428534 -4.4284763 -4.4284644 -4.4284797][-4.4287314 -4.4286747 -4.4285946 -4.4285297 -4.4284482 -4.4283266 -4.4281597 -4.4280214 -4.4281459 -4.4283252 -4.4284277 -4.4284358 -4.4284043 -4.4284 -4.42842][-4.4287052 -4.4286518 -4.4285779 -4.4285355 -4.4284821 -4.4283824 -4.4282341 -4.4281125 -4.4281912 -4.4283228 -4.4284019 -4.4284177 -4.4284177 -4.42843 -4.4284606][-4.4287276 -4.4286942 -4.4286394 -4.4286103 -4.4285793 -4.4285245 -4.4284453 -4.4283853 -4.42842 -4.4284697 -4.4284997 -4.428503 -4.428515 -4.4285474 -4.4285817][-4.4287758 -4.4287648 -4.4287252 -4.4286976 -4.4286685 -4.428637 -4.4285975 -4.4285717 -4.4285812 -4.42859 -4.4285879 -4.4285812 -4.4285936 -4.4286427 -4.4286828][-4.4288287 -4.4288387 -4.4288235 -4.4288063 -4.4287863 -4.4287643 -4.4287353 -4.4287176 -4.428721 -4.428719 -4.4287143 -4.4287095 -4.4287171 -4.4287615 -4.428791][-4.4288678 -4.4288888 -4.4288845 -4.4288831 -4.4288754 -4.4288592 -4.4288306 -4.4288177 -4.4288206 -4.4288216 -4.4288216 -4.4288239 -4.428833 -4.4288626 -4.4288778][-4.4288864 -4.4289126 -4.4289165 -4.4289322 -4.4289403 -4.4289303 -4.4289083 -4.4288969 -4.4288945 -4.4288926 -4.4288926 -4.4289 -4.4289126 -4.42893 -4.4289274][-4.42889 -4.4289155 -4.4289246 -4.4289436 -4.4289527 -4.4289465 -4.4289355 -4.428926 -4.4289179 -4.4289112 -4.42891 -4.4289179 -4.4289274 -4.4289379 -4.4289265]]...]
INFO - root - 2017-12-08 07:49:37.204976: step 46810, loss = 2.28, batch loss = 2.23 (32.9 examples/sec; 0.243 sec/batch; 19h:19m:10s remains)
INFO - root - 2017-12-08 07:49:39.481651: step 46820, loss = 2.28, batch loss = 2.23 (35.7 examples/sec; 0.224 sec/batch; 17h:45m:58s remains)
INFO - root - 2017-12-08 07:49:41.734783: step 46830, loss = 2.28, batch loss = 2.23 (34.9 examples/sec; 0.229 sec/batch; 18h:10m:40s remains)
INFO - root - 2017-12-08 07:49:43.959838: step 46840, loss = 2.28, batch loss = 2.23 (35.8 examples/sec; 0.223 sec/batch; 17h:42m:39s remains)
INFO - root - 2017-12-08 07:49:46.172194: step 46850, loss = 2.28, batch loss = 2.23 (36.0 examples/sec; 0.222 sec/batch; 17h:39m:01s remains)
INFO - root - 2017-12-08 07:49:48.434575: step 46860, loss = 2.28, batch loss = 2.23 (34.1 examples/sec; 0.234 sec/batch; 18h:35m:19s remains)
INFO - root - 2017-12-08 07:49:50.657127: step 46870, loss = 2.28, batch loss = 2.23 (38.0 examples/sec; 0.211 sec/batch; 16h:42m:41s remains)
INFO - root - 2017-12-08 07:49:52.889347: step 46880, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 17h:41m:02s remains)
INFO - root - 2017-12-08 07:49:55.126882: step 46890, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.225 sec/batch; 17h:53m:06s remains)
INFO - root - 2017-12-08 07:49:57.337238: step 46900, loss = 2.28, batch loss = 2.23 (36.9 examples/sec; 0.217 sec/batch; 17h:11m:44s remains)
2017-12-08 07:49:57.642520: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289079 -4.4288769 -4.4288383 -4.4288259 -4.4288235 -4.4288297 -4.4288592 -4.4288926 -4.4289002 -4.428894 -4.4288855 -4.4288573 -4.4288235 -4.4288073 -4.4288034][-4.4289031 -4.4288626 -4.4288192 -4.4288034 -4.4288006 -4.4288 -4.4288278 -4.4288678 -4.428874 -4.4288654 -4.4288578 -4.4288297 -4.4287996 -4.4287906 -4.428793][-4.4288611 -4.4288168 -4.4287744 -4.4287615 -4.4287653 -4.4287691 -4.4288015 -4.4288468 -4.4288487 -4.4288363 -4.428833 -4.4288139 -4.4287853 -4.4287729 -4.4287696][-4.4288025 -4.4287524 -4.4287109 -4.4287076 -4.42872 -4.428731 -4.4287677 -4.4288111 -4.4288092 -4.4287977 -4.4288025 -4.42879 -4.4287591 -4.4287486 -4.4287467][-4.4287457 -4.4286966 -4.4286609 -4.428659 -4.4286671 -4.4286757 -4.428709 -4.4287486 -4.4287496 -4.4287443 -4.4287591 -4.4287543 -4.4287281 -4.4287238 -4.4287271][-4.4286771 -4.4286485 -4.4286246 -4.42862 -4.4286146 -4.4286175 -4.4286437 -4.4286814 -4.4286923 -4.4286942 -4.4287086 -4.4287014 -4.4286752 -4.4286742 -4.4286871][-4.4286017 -4.4285994 -4.4285836 -4.42857 -4.428555 -4.4285493 -4.4285727 -4.4286137 -4.4286375 -4.4286423 -4.4286466 -4.428627 -4.4285965 -4.4285936 -4.4286132][-4.4285703 -4.4285827 -4.4285755 -4.42856 -4.4285359 -4.4285216 -4.428534 -4.4285645 -4.4285903 -4.4286017 -4.4286013 -4.4285746 -4.4285412 -4.4285383 -4.4285512][-4.4285469 -4.4285769 -4.4285927 -4.4285841 -4.428555 -4.4285264 -4.4285231 -4.42854 -4.4285665 -4.4285851 -4.428587 -4.4285607 -4.4285278 -4.4285183 -4.4285107][-4.4285307 -4.428565 -4.4286008 -4.4286084 -4.4285865 -4.4285588 -4.4285555 -4.428566 -4.4285851 -4.4285984 -4.4285965 -4.42857 -4.4285369 -4.4285164 -4.4284945][-4.4285665 -4.4285841 -4.4286213 -4.4286423 -4.4286351 -4.4286227 -4.4286218 -4.4286256 -4.4286332 -4.4286423 -4.4286418 -4.4286218 -4.4285951 -4.4285636 -4.4285226][-4.4286408 -4.4286313 -4.4286532 -4.4286757 -4.4286737 -4.4286704 -4.4286728 -4.428668 -4.4286656 -4.4286747 -4.4286823 -4.4286761 -4.428669 -4.4286466 -4.4285979][-4.42872 -4.4287004 -4.4286962 -4.4287019 -4.4286962 -4.4286909 -4.42868 -4.4286604 -4.428647 -4.4286547 -4.42867 -4.42868 -4.4286904 -4.4286866 -4.4286494][-4.4287825 -4.4287791 -4.4287691 -4.4287605 -4.4287467 -4.4287252 -4.4286876 -4.4286485 -4.4286256 -4.428628 -4.4286485 -4.4286709 -4.4286942 -4.4287062 -4.4286819][-4.4287939 -4.42882 -4.4288235 -4.428813 -4.4287953 -4.4287596 -4.4287 -4.4286518 -4.4286313 -4.428637 -4.428668 -4.4286995 -4.4287267 -4.4287362 -4.4287062]]...]
INFO - root - 2017-12-08 07:49:59.861397: step 46910, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.226 sec/batch; 17h:53m:40s remains)
INFO - root - 2017-12-08 07:50:02.106800: step 46920, loss = 2.28, batch loss = 2.23 (35.4 examples/sec; 0.226 sec/batch; 17h:56m:43s remains)
INFO - root - 2017-12-08 07:50:04.354556: step 46930, loss = 2.28, batch loss = 2.23 (35.0 examples/sec; 0.229 sec/batch; 18h:08m:28s remains)
INFO - root - 2017-12-08 07:50:06.616752: step 46940, loss = 2.28, batch loss = 2.23 (35.2 examples/sec; 0.227 sec/batch; 18h:01m:16s remains)
INFO - root - 2017-12-08 07:50:08.848298: step 46950, loss = 2.28, batch loss = 2.23 (35.3 examples/sec; 0.226 sec/batch; 17h:57m:29s remains)
INFO - root - 2017-12-08 07:50:11.096839: step 46960, loss = 2.28, batch loss = 2.23 (35.4 examples/sec; 0.226 sec/batch; 17h:56m:55s remains)
INFO - root - 2017-12-08 07:50:13.344269: step 46970, loss = 2.28, batch loss = 2.23 (35.3 examples/sec; 0.226 sec/batch; 17h:57m:12s remains)
INFO - root - 2017-12-08 07:50:15.629727: step 46980, loss = 2.28, batch loss = 2.23 (33.0 examples/sec; 0.242 sec/batch; 19h:13m:10s remains)
INFO - root - 2017-12-08 07:50:17.855019: step 46990, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 17h:25m:47s remains)
INFO - root - 2017-12-08 07:50:20.070149: step 47000, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 17h:23m:55s remains)
2017-12-08 07:50:20.353004: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4287171 -4.4287391 -4.4287486 -4.4287481 -4.4287438 -4.4287367 -4.4287405 -4.42876 -4.4287848 -4.4288044 -4.428823 -4.4288411 -4.4288349 -4.4287939 -4.4287705][-4.4287324 -4.4287477 -4.4287515 -4.42875 -4.4287457 -4.4287424 -4.4287539 -4.4287806 -4.4288144 -4.4288354 -4.4288435 -4.4288492 -4.4288425 -4.4288087 -4.4287858][-4.4287696 -4.4287729 -4.4287677 -4.4287586 -4.4287519 -4.4287524 -4.4287643 -4.4287944 -4.4288292 -4.4288445 -4.4288421 -4.4288383 -4.4288306 -4.4288068 -4.42879][-4.4288058 -4.4287968 -4.4287825 -4.4287667 -4.4287562 -4.4287567 -4.4287605 -4.4287825 -4.4288092 -4.4288197 -4.4288063 -4.4287944 -4.4287858 -4.4287682 -4.4287543][-4.4288287 -4.42881 -4.428781 -4.4287529 -4.4287305 -4.428719 -4.4287047 -4.4287062 -4.4287252 -4.4287376 -4.4287267 -4.4287176 -4.4287219 -4.4287157 -4.4287109][-4.4288087 -4.4287829 -4.42874 -4.4287009 -4.4286675 -4.4286327 -4.4285836 -4.4285502 -4.4285617 -4.4285846 -4.4285893 -4.4286075 -4.4286475 -4.4286675 -4.428678][-4.4287381 -4.4287009 -4.4286523 -4.4286132 -4.4285769 -4.4285235 -4.4284291 -4.4283485 -4.4283576 -4.4284167 -4.4284658 -4.4285173 -4.4285879 -4.4286361 -4.4286628][-4.4286404 -4.428586 -4.4285407 -4.4285169 -4.4284887 -4.4284348 -4.42832 -4.4282112 -4.4282365 -4.4283566 -4.4284558 -4.4285245 -4.4285965 -4.4286528 -4.4286842][-4.4285326 -4.4284792 -4.4284549 -4.4284558 -4.4284496 -4.4284234 -4.4283376 -4.4282589 -4.428308 -4.4284477 -4.4285479 -4.4285994 -4.4286485 -4.4286909 -4.4287114][-4.4284749 -4.4284515 -4.4284563 -4.4284759 -4.4284897 -4.4284887 -4.4284492 -4.4284248 -4.4284816 -4.4285979 -4.4286675 -4.4286914 -4.4287176 -4.428741 -4.4287434][-4.4284811 -4.4284797 -4.4285083 -4.4285378 -4.4285607 -4.4285769 -4.4285703 -4.4285741 -4.4286232 -4.4287004 -4.4287429 -4.4287505 -4.4287577 -4.4287653 -4.4287591][-4.4285145 -4.4285131 -4.4285517 -4.4285913 -4.4286208 -4.4286451 -4.428658 -4.4286675 -4.428699 -4.428741 -4.4287648 -4.4287667 -4.428762 -4.4287596 -4.4287553][-4.4285092 -4.4285111 -4.4285655 -4.4286189 -4.4286556 -4.4286838 -4.4287014 -4.4287071 -4.4287186 -4.4287357 -4.4287548 -4.4287677 -4.4287667 -4.4287553 -4.4287457][-4.4284883 -4.4285083 -4.4285789 -4.4286404 -4.4286823 -4.4287148 -4.4287338 -4.4287333 -4.4287362 -4.4287424 -4.4287634 -4.4287839 -4.4287872 -4.4287686 -4.4287548][-4.4285154 -4.4285517 -4.4286203 -4.4286742 -4.428709 -4.4287348 -4.4287519 -4.4287529 -4.4287586 -4.4287696 -4.4287877 -4.4287944 -4.4287877 -4.4287724 -4.4287663]]...]
INFO - root - 2017-12-08 07:50:22.612392: step 47010, loss = 2.28, batch loss = 2.23 (34.3 examples/sec; 0.233 sec/batch; 18h:28m:17s remains)
INFO - root - 2017-12-08 07:50:24.854833: step 47020, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.222 sec/batch; 17h:34m:47s remains)
INFO - root - 2017-12-08 07:50:27.087385: step 47030, loss = 2.28, batch loss = 2.23 (36.6 examples/sec; 0.218 sec/batch; 17h:19m:21s remains)
INFO - root - 2017-12-08 07:50:29.332006: step 47040, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.222 sec/batch; 17h:34m:36s remains)
INFO - root - 2017-12-08 07:50:31.579520: step 47050, loss = 2.28, batch loss = 2.23 (34.8 examples/sec; 0.230 sec/batch; 18h:14m:02s remains)
INFO - root - 2017-12-08 07:50:33.808524: step 47060, loss = 2.28, batch loss = 2.23 (35.6 examples/sec; 0.225 sec/batch; 17h:48m:53s remains)
INFO - root - 2017-12-08 07:50:36.034423: step 47070, loss = 2.28, batch loss = 2.23 (35.3 examples/sec; 0.226 sec/batch; 17h:57m:21s remains)
INFO - root - 2017-12-08 07:50:38.276480: step 47080, loss = 2.28, batch loss = 2.23 (34.2 examples/sec; 0.234 sec/batch; 18h:31m:48s remains)
INFO - root - 2017-12-08 07:50:40.494829: step 47090, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 17h:24m:20s remains)
INFO - root - 2017-12-08 07:50:42.739482: step 47100, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 17h:26m:33s remains)
2017-12-08 07:50:43.024439: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288568 -4.4288568 -4.4288592 -4.4288621 -4.428865 -4.4288669 -4.4288673 -4.4288654 -4.4288616 -4.4288578 -4.428854 -4.4288511 -4.4288468 -4.4288335 -4.4288158][-4.4288836 -4.4288826 -4.4288864 -4.4288921 -4.4288955 -4.4288964 -4.428895 -4.4288898 -4.428884 -4.42888 -4.4288774 -4.428875 -4.4288712 -4.4288592 -4.4288473][-4.4289093 -4.4289093 -4.4289141 -4.4289231 -4.4289289 -4.42893 -4.4289241 -4.4289126 -4.4289007 -4.4288931 -4.4288907 -4.4288912 -4.42889 -4.4288812 -4.428874][-4.4288926 -4.4288936 -4.4289036 -4.4289207 -4.4289331 -4.4289379 -4.42893 -4.4289055 -4.4288797 -4.4288607 -4.4288507 -4.4288483 -4.4288507 -4.4288478 -4.4288497][-4.4288068 -4.428813 -4.428834 -4.4288621 -4.4288831 -4.4288878 -4.4288731 -4.4288354 -4.4287925 -4.4287562 -4.4287348 -4.42873 -4.4287357 -4.4287395 -4.4287562][-4.4286666 -4.42868 -4.4287195 -4.4287682 -4.4288039 -4.4288087 -4.4287782 -4.4287167 -4.4286432 -4.4285774 -4.4285421 -4.4285483 -4.4285769 -4.4286032 -4.4286404][-4.4284883 -4.4285135 -4.4285755 -4.4286432 -4.4286866 -4.4286842 -4.4286294 -4.4285331 -4.4284229 -4.4283347 -4.4283061 -4.4283438 -4.4284115 -4.4284673 -4.428525][-4.4283957 -4.4284368 -4.428514 -4.4285793 -4.4286165 -4.4286075 -4.428534 -4.4284081 -4.4282708 -4.4281874 -4.4281936 -4.42827 -4.42836 -4.4284234 -4.4284792][-4.4284916 -4.42853 -4.4285965 -4.4286447 -4.4286652 -4.4286466 -4.4285727 -4.42846 -4.4283552 -4.4283104 -4.4283295 -4.4283929 -4.4284606 -4.4285059 -4.4285474][-4.4286714 -4.4286947 -4.4287386 -4.4287667 -4.4287705 -4.4287505 -4.4287038 -4.4286461 -4.4285984 -4.4285779 -4.4285808 -4.4286032 -4.4286308 -4.4286504 -4.4286757][-4.428822 -4.4288321 -4.4288549 -4.4288683 -4.4288621 -4.4288468 -4.4288268 -4.4288068 -4.42879 -4.4287815 -4.4287782 -4.4287815 -4.4287877 -4.4287853 -4.4287853][-4.428905 -4.4289079 -4.4289165 -4.4289184 -4.4289069 -4.4288955 -4.4288864 -4.4288774 -4.42887 -4.4288669 -4.4288683 -4.4288707 -4.4288726 -4.42886 -4.4288397][-4.4289141 -4.4289174 -4.4289222 -4.4289212 -4.4289145 -4.4289083 -4.4289041 -4.4288974 -4.4288917 -4.4288917 -4.428895 -4.4288988 -4.4288979 -4.4288778 -4.4288416][-4.4288526 -4.4288597 -4.4288616 -4.4288597 -4.4288578 -4.4288549 -4.4288511 -4.4288464 -4.428843 -4.4288468 -4.4288535 -4.4288568 -4.4288521 -4.4288192 -4.4287682][-4.4287324 -4.4287438 -4.4287453 -4.4287462 -4.4287491 -4.4287505 -4.4287472 -4.4287405 -4.4287353 -4.4287381 -4.4287448 -4.4287496 -4.4287448 -4.4287047 -4.4286404]]...]
INFO - root - 2017-12-08 07:50:45.276920: step 47110, loss = 2.28, batch loss = 2.23 (33.9 examples/sec; 0.236 sec/batch; 18h:41m:42s remains)
INFO - root - 2017-12-08 07:50:47.531288: step 47120, loss = 2.28, batch loss = 2.23 (35.6 examples/sec; 0.224 sec/batch; 17h:47m:44s remains)
INFO - root - 2017-12-08 07:50:49.764879: step 47130, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.222 sec/batch; 17h:33m:42s remains)
INFO - root - 2017-12-08 07:50:52.001517: step 47140, loss = 2.28, batch loss = 2.23 (35.2 examples/sec; 0.227 sec/batch; 18h:01m:53s remains)
INFO - root - 2017-12-08 07:50:54.235338: step 47150, loss = 2.28, batch loss = 2.23 (34.9 examples/sec; 0.229 sec/batch; 18h:10m:29s remains)
INFO - root - 2017-12-08 07:50:56.483799: step 47160, loss = 2.28, batch loss = 2.23 (35.7 examples/sec; 0.224 sec/batch; 17h:44m:43s remains)
INFO - root - 2017-12-08 07:50:58.753847: step 47170, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.222 sec/batch; 17h:34m:02s remains)
INFO - root - 2017-12-08 07:51:00.996463: step 47180, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 17h:32m:03s remains)
INFO - root - 2017-12-08 07:51:03.233663: step 47190, loss = 2.28, batch loss = 2.23 (35.4 examples/sec; 0.226 sec/batch; 17h:55m:33s remains)
INFO - root - 2017-12-08 07:51:05.478873: step 47200, loss = 2.28, batch loss = 2.23 (35.4 examples/sec; 0.226 sec/batch; 17h:54m:51s remains)
2017-12-08 07:51:05.762980: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289718 -4.428967 -4.4289579 -4.4289479 -4.4289374 -4.4289346 -4.4289312 -4.428925 -4.4289193 -4.4289174 -4.4289165 -4.4289179 -4.4289246 -4.428937 -4.4289503][-4.4289389 -4.4289336 -4.4289212 -4.4289031 -4.4288864 -4.4288845 -4.4288826 -4.4288726 -4.4288611 -4.4288616 -4.4288564 -4.4288478 -4.4288483 -4.4288645 -4.4288883][-4.428916 -4.4289079 -4.4288898 -4.4288607 -4.4288349 -4.4288316 -4.4288249 -4.4287996 -4.428772 -4.4287705 -4.4287639 -4.4287491 -4.4287438 -4.4287643 -4.4288015][-4.4288912 -4.4288769 -4.4288573 -4.4288211 -4.4287848 -4.4287715 -4.4287529 -4.4287052 -4.4286575 -4.4286561 -4.4286594 -4.4286475 -4.4286375 -4.4286547 -4.4287014][-4.4288697 -4.428854 -4.4288392 -4.4288039 -4.4287615 -4.42873 -4.4286833 -4.4285941 -4.4285235 -4.4285383 -4.4285684 -4.4285645 -4.4285531 -4.428565 -4.42862][-4.4288325 -4.4288163 -4.4288044 -4.4287691 -4.4287128 -4.4286556 -4.4285631 -4.4284086 -4.428319 -4.4283772 -4.4284573 -4.4284787 -4.4284825 -4.4285054 -4.4285746][-4.4287834 -4.428771 -4.4287539 -4.4287076 -4.4286261 -4.4285345 -4.4283848 -4.4281425 -4.4280272 -4.4281549 -4.4283094 -4.4283791 -4.4284172 -4.4284668 -4.4285579][-4.4287381 -4.4287362 -4.4287152 -4.428658 -4.4285574 -4.4284358 -4.4282074 -4.4278464 -4.4276867 -4.4279141 -4.4281654 -4.428299 -4.4283824 -4.4284635 -4.4285746][-4.4287071 -4.4287162 -4.4286957 -4.4286227 -4.4285164 -4.4283981 -4.4281526 -4.4277697 -4.4276285 -4.4279022 -4.4281783 -4.4283261 -4.4284244 -4.4285192 -4.4286304][-4.4287181 -4.4287329 -4.4287095 -4.4286389 -4.4285574 -4.4284849 -4.4283028 -4.4280295 -4.4279461 -4.4281397 -4.4283414 -4.4284568 -4.4285364 -4.4286213 -4.4287143][-4.4287605 -4.4287763 -4.4287539 -4.4286938 -4.4286356 -4.4285932 -4.428472 -4.4283009 -4.4282589 -4.4283786 -4.4285154 -4.4286046 -4.4286666 -4.4287338 -4.428802][-4.4288263 -4.428844 -4.4288282 -4.428781 -4.428741 -4.4287109 -4.4286275 -4.428525 -4.4285135 -4.4285917 -4.4286828 -4.4287496 -4.4287939 -4.4288368 -4.42888][-4.428885 -4.4288988 -4.4288845 -4.4288487 -4.4288216 -4.4288082 -4.4287667 -4.4287176 -4.4287267 -4.4287791 -4.42883 -4.4288678 -4.4288907 -4.42891 -4.4289327][-4.4288988 -4.4289026 -4.4288907 -4.4288692 -4.4288588 -4.4288673 -4.4288568 -4.4288411 -4.42886 -4.4288945 -4.4289184 -4.4289365 -4.4289465 -4.4289527 -4.4289627][-4.4289126 -4.4289117 -4.4289026 -4.4288931 -4.428895 -4.4289079 -4.4289083 -4.4289088 -4.4289289 -4.42895 -4.4289613 -4.42897 -4.4289732 -4.4289751 -4.42898]]...]
INFO - root - 2017-12-08 07:51:07.990914: step 47210, loss = 2.28, batch loss = 2.23 (34.5 examples/sec; 0.232 sec/batch; 18h:21m:12s remains)
INFO - root - 2017-12-08 07:51:10.233898: step 47220, loss = 2.28, batch loss = 2.23 (35.1 examples/sec; 0.228 sec/batch; 18h:03m:55s remains)
INFO - root - 2017-12-08 07:51:12.463833: step 47230, loss = 2.28, batch loss = 2.23 (35.8 examples/sec; 0.224 sec/batch; 17h:43m:03s remains)
INFO - root - 2017-12-08 07:51:14.726136: step 47240, loss = 2.28, batch loss = 2.23 (33.9 examples/sec; 0.236 sec/batch; 18h:41m:52s remains)
INFO - root - 2017-12-08 07:51:16.989239: step 47250, loss = 2.28, batch loss = 2.23 (35.7 examples/sec; 0.224 sec/batch; 17h:45m:11s remains)
INFO - root - 2017-12-08 07:51:19.238912: step 47260, loss = 2.28, batch loss = 2.23 (35.4 examples/sec; 0.226 sec/batch; 17h:54m:32s remains)
INFO - root - 2017-12-08 07:51:21.468990: step 47270, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 17h:23m:13s remains)
INFO - root - 2017-12-08 07:51:23.705271: step 47280, loss = 2.28, batch loss = 2.23 (35.4 examples/sec; 0.226 sec/batch; 17h:55m:26s remains)
INFO - root - 2017-12-08 07:51:25.933368: step 47290, loss = 2.28, batch loss = 2.23 (35.6 examples/sec; 0.225 sec/batch; 17h:49m:25s remains)
INFO - root - 2017-12-08 07:51:28.155325: step 47300, loss = 2.28, batch loss = 2.23 (36.6 examples/sec; 0.218 sec/batch; 17h:18m:12s remains)
2017-12-08 07:51:28.453201: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289632 -4.4289522 -4.42894 -4.4289269 -4.4289231 -4.4289279 -4.42894 -4.4289556 -4.4289656 -4.4289656 -4.42895 -4.428937 -4.4289379 -4.4289513 -4.4289722][-4.4289389 -4.428926 -4.4289103 -4.4288931 -4.4288845 -4.4288812 -4.42889 -4.4289112 -4.4289327 -4.4289427 -4.4289293 -4.4289141 -4.4289122 -4.4289265 -4.4289513][-4.428894 -4.4288845 -4.4288707 -4.4288526 -4.4288387 -4.4288249 -4.4288235 -4.4288406 -4.4288721 -4.4289021 -4.4289079 -4.4289055 -4.4289074 -4.42892 -4.4289408][-4.428843 -4.4288416 -4.428833 -4.4288163 -4.4287996 -4.4287653 -4.4287415 -4.4287453 -4.4287777 -4.4288249 -4.4288626 -4.4288888 -4.4289088 -4.4289241 -4.4289417][-4.4288068 -4.428813 -4.4288082 -4.42879 -4.4287624 -4.4287028 -4.4286423 -4.4286165 -4.4286408 -4.4287038 -4.4287734 -4.4288316 -4.4288821 -4.4289174 -4.4289422][-4.4287763 -4.4287839 -4.4287753 -4.4287443 -4.4286928 -4.4286051 -4.4285064 -4.4284363 -4.428443 -4.4285274 -4.4286356 -4.4287248 -4.4288044 -4.4288712 -4.4289188][-4.4287395 -4.4287372 -4.4287171 -4.4286723 -4.4286032 -4.4284949 -4.42836 -4.4282284 -4.4281845 -4.4282842 -4.4284368 -4.4285631 -4.4286695 -4.4287691 -4.428853][-4.42871 -4.4286885 -4.4286542 -4.4286027 -4.4285283 -4.4284172 -4.4282675 -4.428091 -4.4279795 -4.4280477 -4.428206 -4.4283509 -4.4284816 -4.4286146 -4.4287391][-4.4286952 -4.428658 -4.4286141 -4.428565 -4.428503 -4.4284067 -4.428278 -4.4281225 -4.4279966 -4.4279852 -4.4280558 -4.4281597 -4.42829 -4.4284449 -4.4286027][-4.4287357 -4.4286985 -4.4286604 -4.4286242 -4.4285884 -4.4285283 -4.4284387 -4.4283314 -4.4282279 -4.4281554 -4.4281206 -4.4281449 -4.4282374 -4.4283757 -4.4285269][-4.4288287 -4.42881 -4.4287891 -4.4287696 -4.4287558 -4.4287348 -4.42869 -4.4286256 -4.42855 -4.4284639 -4.4283843 -4.4283524 -4.4283919 -4.4284797 -4.428587][-4.4289203 -4.42892 -4.4289126 -4.4289012 -4.4288988 -4.4289007 -4.4288893 -4.4288592 -4.4288163 -4.4287548 -4.4286866 -4.428638 -4.4286346 -4.42867 -4.4287262][-4.42898 -4.4289856 -4.4289865 -4.4289823 -4.4289823 -4.4289889 -4.428988 -4.428977 -4.428957 -4.4289255 -4.4288869 -4.4288492 -4.4288292 -4.4288335 -4.428853][-4.4290047 -4.42901 -4.4290137 -4.4290128 -4.4290133 -4.4290152 -4.4290118 -4.4290042 -4.4289951 -4.4289832 -4.428967 -4.4289479 -4.4289322 -4.4289255 -4.4289284][-4.4290118 -4.4290147 -4.4290156 -4.4290152 -4.4290152 -4.4290147 -4.4290109 -4.4290066 -4.4290037 -4.4290004 -4.4289947 -4.4289861 -4.428977 -4.4289713 -4.4289703]]...]
INFO - root - 2017-12-08 07:51:30.653707: step 47310, loss = 2.28, batch loss = 2.23 (37.6 examples/sec; 0.213 sec/batch; 16h:51m:47s remains)
INFO - root - 2017-12-08 07:51:32.932415: step 47320, loss = 2.28, batch loss = 2.23 (35.6 examples/sec; 0.225 sec/batch; 17h:48m:15s remains)
INFO - root - 2017-12-08 07:51:35.165459: step 47330, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.220 sec/batch; 17h:27m:22s remains)
INFO - root - 2017-12-08 07:51:37.393468: step 47340, loss = 2.28, batch loss = 2.23 (36.6 examples/sec; 0.218 sec/batch; 17h:18m:12s remains)
INFO - root - 2017-12-08 07:51:39.632878: step 47350, loss = 2.28, batch loss = 2.23 (35.3 examples/sec; 0.226 sec/batch; 17h:55m:59s remains)
INFO - root - 2017-12-08 07:51:41.898577: step 47360, loss = 2.28, batch loss = 2.23 (34.9 examples/sec; 0.229 sec/batch; 18h:09m:31s remains)
INFO - root - 2017-12-08 07:51:44.147959: step 47370, loss = 2.28, batch loss = 2.23 (34.9 examples/sec; 0.229 sec/batch; 18h:10m:00s remains)
INFO - root - 2017-12-08 07:51:46.441532: step 47380, loss = 2.28, batch loss = 2.23 (35.1 examples/sec; 0.228 sec/batch; 18h:03m:19s remains)
INFO - root - 2017-12-08 07:51:48.676584: step 47390, loss = 2.28, batch loss = 2.23 (35.3 examples/sec; 0.227 sec/batch; 17h:56m:53s remains)
INFO - root - 2017-12-08 07:51:50.941769: step 47400, loss = 2.28, batch loss = 2.23 (33.7 examples/sec; 0.237 sec/batch; 18h:47m:51s remains)
2017-12-08 07:51:51.250280: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4287629 -4.4287643 -4.4287786 -4.4287953 -4.4287834 -4.4287581 -4.4287548 -4.4287643 -4.4287672 -4.4287586 -4.4287629 -4.4287753 -4.4287663 -4.4287443 -4.4287133][-4.4287233 -4.4287076 -4.4287167 -4.4287395 -4.4287319 -4.4287148 -4.4287214 -4.4287267 -4.4287105 -4.4286728 -4.428668 -4.4286962 -4.4287004 -4.428678 -4.4286313][-4.4286613 -4.4286342 -4.4286361 -4.4286537 -4.4286413 -4.4286251 -4.428638 -4.428659 -4.4286466 -4.4286051 -4.4286151 -4.4286585 -4.4286594 -4.4286289 -4.4285712][-4.4285979 -4.428587 -4.4286 -4.4286146 -4.4285865 -4.4285607 -4.4285684 -4.4286036 -4.4285932 -4.4285617 -4.428587 -4.4286351 -4.4286394 -4.4286237 -4.4285841][-4.428596 -4.4286013 -4.4286261 -4.4286423 -4.4286151 -4.4285755 -4.4285645 -4.4285803 -4.428556 -4.4285231 -4.4285564 -4.4286084 -4.428637 -4.428658 -4.4286494][-4.4286675 -4.4286771 -4.4287014 -4.4287062 -4.4286556 -4.42858 -4.4285288 -4.4285045 -4.4284754 -4.4284678 -4.4285178 -4.428575 -4.4286275 -4.4286838 -4.4287081][-4.428793 -4.42878 -4.42877 -4.4287395 -4.4286571 -4.4285436 -4.4284554 -4.4283986 -4.4283886 -4.42843 -4.4285083 -4.4285707 -4.428637 -4.4287114 -4.4287596][-4.4288764 -4.4288383 -4.4287968 -4.4287443 -4.4286442 -4.4285226 -4.4284186 -4.4283528 -4.4283834 -4.4284773 -4.4285674 -4.4286313 -4.4286942 -4.42876 -4.4288049][-4.4289093 -4.4288578 -4.428812 -4.4287562 -4.4286671 -4.4285755 -4.4285007 -4.4284587 -4.4285164 -4.4286289 -4.428709 -4.4287577 -4.4287958 -4.4288244 -4.4288435][-4.4289012 -4.4288592 -4.4288297 -4.4287934 -4.4287424 -4.4286976 -4.4286609 -4.4286437 -4.4287 -4.4287882 -4.4288397 -4.4288669 -4.4288793 -4.4288793 -4.4288764][-4.4288745 -4.4288545 -4.4288459 -4.428834 -4.42882 -4.4288116 -4.4287939 -4.4287868 -4.4288187 -4.4288678 -4.428896 -4.4289126 -4.4289188 -4.4289112 -4.4289021][-4.4288568 -4.4288545 -4.4288568 -4.428863 -4.4288716 -4.4288754 -4.428864 -4.4288478 -4.4288459 -4.4288635 -4.4288859 -4.4289107 -4.4289231 -4.42892 -4.4289112][-4.4288521 -4.4288573 -4.4288611 -4.4288707 -4.4288764 -4.42887 -4.4288521 -4.4288206 -4.4287958 -4.428803 -4.4288349 -4.4288759 -4.4289007 -4.4289055 -4.4289036][-4.428865 -4.4288673 -4.42886 -4.4288507 -4.428833 -4.4288111 -4.4287872 -4.4287529 -4.4287295 -4.4287505 -4.4287958 -4.4288483 -4.4288807 -4.4288898 -4.42889][-4.4288669 -4.4288654 -4.4288492 -4.4288254 -4.4287868 -4.4287567 -4.4287457 -4.4287353 -4.428741 -4.4287829 -4.4288316 -4.4288774 -4.4289012 -4.4289012 -4.42889]]...]
INFO - root - 2017-12-08 07:51:53.475260: step 47410, loss = 2.28, batch loss = 2.23 (34.4 examples/sec; 0.233 sec/batch; 18h:26m:31s remains)
INFO - root - 2017-12-08 07:51:55.716224: step 47420, loss = 2.28, batch loss = 2.23 (35.0 examples/sec; 0.228 sec/batch; 18h:05m:35s remains)
INFO - root - 2017-12-08 07:51:57.941237: step 47430, loss = 2.28, batch loss = 2.23 (35.4 examples/sec; 0.226 sec/batch; 17h:53m:42s remains)
INFO - root - 2017-12-08 07:52:00.198113: step 47440, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 17h:24m:25s remains)
INFO - root - 2017-12-08 07:52:02.442684: step 47450, loss = 2.28, batch loss = 2.23 (35.1 examples/sec; 0.228 sec/batch; 18h:02m:16s remains)
INFO - root - 2017-12-08 07:52:04.708993: step 47460, loss = 2.28, batch loss = 2.23 (34.4 examples/sec; 0.232 sec/batch; 18h:23m:27s remains)
INFO - root - 2017-12-08 07:52:06.956820: step 47470, loss = 2.28, batch loss = 2.23 (35.4 examples/sec; 0.226 sec/batch; 17h:52m:35s remains)
INFO - root - 2017-12-08 07:52:09.247514: step 47480, loss = 2.28, batch loss = 2.23 (35.0 examples/sec; 0.229 sec/batch; 18h:07m:08s remains)
INFO - root - 2017-12-08 07:52:11.473226: step 47490, loss = 2.28, batch loss = 2.23 (35.2 examples/sec; 0.227 sec/batch; 17h:58m:14s remains)
INFO - root - 2017-12-08 07:52:13.707343: step 47500, loss = 2.28, batch loss = 2.23 (35.3 examples/sec; 0.226 sec/batch; 17h:55m:37s remains)
2017-12-08 07:52:14.001138: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4287257 -4.4287314 -4.4287114 -4.428688 -4.4286809 -4.4286942 -4.4287114 -4.4287214 -4.4287248 -4.4287233 -4.42872 -4.4287148 -4.4287014 -4.4286885 -4.42869][-4.4287133 -4.4287024 -4.4286623 -4.428627 -4.428616 -4.4286323 -4.4286618 -4.4286866 -4.4287076 -4.4287257 -4.428731 -4.4287205 -4.4286962 -4.4286804 -4.42869][-4.428689 -4.4286485 -4.4285955 -4.428565 -4.4285612 -4.4285827 -4.4286265 -4.4286661 -4.4286971 -4.4287257 -4.4287252 -4.4287019 -4.4286656 -4.4286494 -4.4286737][-4.4286423 -4.4285755 -4.4285259 -4.4285173 -4.428535 -4.4285617 -4.428606 -4.4286566 -4.4286971 -4.42873 -4.4287205 -4.4286842 -4.4286337 -4.4286141 -4.4286432][-4.4285779 -4.4284964 -4.4284558 -4.4284711 -4.4285111 -4.4285374 -4.4285679 -4.4286232 -4.4286761 -4.4287176 -4.4287105 -4.4286637 -4.428606 -4.42859 -4.4286227][-4.4285283 -4.4284458 -4.4284163 -4.428443 -4.4284835 -4.4284863 -4.4284883 -4.4285336 -4.4285965 -4.4286566 -4.4286675 -4.4286284 -4.428586 -4.428586 -4.4286284][-4.4285479 -4.42848 -4.4284616 -4.4284735 -4.4284763 -4.4284244 -4.4283781 -4.4284043 -4.428472 -4.428555 -4.4285951 -4.4285812 -4.4285707 -4.4285951 -4.4286461][-4.4286261 -4.4285822 -4.4285684 -4.4285607 -4.4285293 -4.4284334 -4.4283485 -4.4283557 -4.4284205 -4.4285064 -4.4285526 -4.428555 -4.4285722 -4.4286156 -4.428668][-4.4287171 -4.4286904 -4.4286766 -4.4286594 -4.4286156 -4.4285221 -4.42844 -4.428442 -4.4284954 -4.428556 -4.4285822 -4.4285831 -4.428616 -4.4286575 -4.4286962][-4.428782 -4.4287639 -4.4287496 -4.4287305 -4.4286885 -4.428618 -4.4285622 -4.428565 -4.4286027 -4.428638 -4.4286451 -4.4286408 -4.4286704 -4.4286995 -4.4287233][-4.4288278 -4.4288135 -4.4287996 -4.428781 -4.4287477 -4.4287052 -4.4286761 -4.42868 -4.428699 -4.42871 -4.4287043 -4.4286985 -4.4287181 -4.4287338 -4.4287486][-4.4288592 -4.4288468 -4.4288344 -4.4288182 -4.4287949 -4.4287734 -4.4287634 -4.4287663 -4.42877 -4.4287629 -4.4287496 -4.4287448 -4.4287457 -4.4287434 -4.4287505][-4.428884 -4.42887 -4.4288549 -4.4288373 -4.4288192 -4.4288096 -4.428812 -4.4288182 -4.4288158 -4.428802 -4.4287834 -4.42877 -4.4287457 -4.4287214 -4.428719][-4.428916 -4.4289021 -4.4288845 -4.428863 -4.4288411 -4.4288278 -4.4288273 -4.4288311 -4.4288249 -4.4288106 -4.428793 -4.4287677 -4.4287229 -4.4286804 -4.4286642][-4.4289451 -4.4289346 -4.4289184 -4.4288969 -4.4288716 -4.4288492 -4.4288344 -4.428823 -4.4288049 -4.4287915 -4.42878 -4.42875 -4.4286995 -4.4286461 -4.428617]]...]
INFO - root - 2017-12-08 07:52:16.232391: step 47510, loss = 2.28, batch loss = 2.23 (35.0 examples/sec; 0.229 sec/batch; 18h:05m:33s remains)
INFO - root - 2017-12-08 07:52:18.447300: step 47520, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 17h:21m:43s remains)
INFO - root - 2017-12-08 07:52:20.708771: step 47530, loss = 2.28, batch loss = 2.23 (34.4 examples/sec; 0.233 sec/batch; 18h:24m:37s remains)
INFO - root - 2017-12-08 07:52:22.971408: step 47540, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.226 sec/batch; 17h:51m:14s remains)
INFO - root - 2017-12-08 07:52:25.205362: step 47550, loss = 2.28, batch loss = 2.23 (37.3 examples/sec; 0.214 sec/batch; 16h:58m:35s remains)
INFO - root - 2017-12-08 07:52:27.429841: step 47560, loss = 2.28, batch loss = 2.23 (37.0 examples/sec; 0.216 sec/batch; 17h:05m:58s remains)
INFO - root - 2017-12-08 07:52:29.683880: step 47570, loss = 2.28, batch loss = 2.23 (34.9 examples/sec; 0.229 sec/batch; 18h:09m:34s remains)
INFO - root - 2017-12-08 07:52:31.921854: step 47580, loss = 2.28, batch loss = 2.23 (36.6 examples/sec; 0.219 sec/batch; 17h:19m:07s remains)
INFO - root - 2017-12-08 07:52:34.170972: step 47590, loss = 2.28, batch loss = 2.23 (35.6 examples/sec; 0.225 sec/batch; 17h:47m:59s remains)
INFO - root - 2017-12-08 07:52:36.402849: step 47600, loss = 2.28, batch loss = 2.23 (35.0 examples/sec; 0.228 sec/batch; 18h:04m:45s remains)
2017-12-08 07:52:36.674124: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4287052 -4.4287143 -4.4287295 -4.4287539 -4.4287972 -4.428812 -4.4288025 -4.4288034 -4.4288387 -4.4288611 -4.4288492 -4.4288354 -4.4288254 -4.4288239 -4.4288487][-4.4286566 -4.428679 -4.4287076 -4.42874 -4.4287853 -4.4287982 -4.42878 -4.4287672 -4.4287977 -4.4288197 -4.4287968 -4.4287696 -4.4287505 -4.4287467 -4.4287868][-4.4286361 -4.4286904 -4.4287314 -4.4287658 -4.4288011 -4.4288044 -4.4287629 -4.4287224 -4.4287415 -4.4287634 -4.4287448 -4.428719 -4.4287057 -4.4287062 -4.4287553][-4.4286809 -4.4287667 -4.4288073 -4.4288263 -4.4288292 -4.4288011 -4.4287181 -4.4286413 -4.4286594 -4.4287004 -4.42871 -4.4287109 -4.4287124 -4.4287157 -4.4287615][-4.42876 -4.428843 -4.4288621 -4.4288521 -4.4288168 -4.4287453 -4.4286056 -4.4284868 -4.4285254 -4.4286113 -4.4286604 -4.4286962 -4.4287167 -4.4287267 -4.4287648][-4.4288092 -4.4288597 -4.4288397 -4.4287996 -4.4287357 -4.4286218 -4.4284191 -4.428247 -4.4283133 -4.4284606 -4.4285536 -4.4286275 -4.4286728 -4.4286938 -4.4287233][-4.4287996 -4.4288135 -4.4287581 -4.4286981 -4.428617 -4.428472 -4.4282279 -4.4280243 -4.4281116 -4.428309 -4.4284372 -4.4285336 -4.4285927 -4.4286084 -4.428616][-4.4287643 -4.4287548 -4.4286885 -4.4286323 -4.42856 -4.428422 -4.4282165 -4.428062 -4.428133 -4.4282956 -4.42841 -4.4284964 -4.4285455 -4.4285407 -4.428515][-4.4287534 -4.4287343 -4.4286718 -4.4286308 -4.4285865 -4.4284859 -4.428359 -4.42829 -4.4283361 -4.4284225 -4.4284863 -4.4285331 -4.428555 -4.4285293 -4.4284811][-4.4287848 -4.4287658 -4.4287138 -4.4286828 -4.4286618 -4.4286003 -4.428534 -4.4285226 -4.4285541 -4.4285913 -4.428617 -4.428628 -4.42863 -4.4285994 -4.4285502][-4.428844 -4.4288287 -4.4287834 -4.42875 -4.4287353 -4.428699 -4.4286656 -4.4286747 -4.4287028 -4.4287267 -4.4287434 -4.4287457 -4.4287477 -4.4287248 -4.4286833][-4.4288993 -4.4288883 -4.4288487 -4.4288096 -4.4287896 -4.4287648 -4.4287472 -4.428762 -4.4287844 -4.4288077 -4.4288282 -4.4288383 -4.4288468 -4.4288368 -4.4288077][-4.4289174 -4.4289079 -4.4288754 -4.4288349 -4.4288077 -4.4287863 -4.4287724 -4.4287825 -4.4287977 -4.4288235 -4.4288507 -4.4288716 -4.4288898 -4.4288964 -4.428885][-4.4289017 -4.428895 -4.4288721 -4.4288383 -4.4288077 -4.4287858 -4.4287696 -4.4287696 -4.42878 -4.4288082 -4.4288383 -4.4288621 -4.4288793 -4.4288974 -4.4289002][-4.4288626 -4.4288683 -4.4288616 -4.4288416 -4.4288144 -4.4287896 -4.4287696 -4.4287596 -4.4287653 -4.4287877 -4.4288111 -4.4288268 -4.428833 -4.4288511 -4.4288626]]...]
INFO - root - 2017-12-08 07:52:38.892089: step 47610, loss = 2.28, batch loss = 2.23 (35.3 examples/sec; 0.226 sec/batch; 17h:54m:43s remains)
INFO - root - 2017-12-08 07:52:41.112057: step 47620, loss = 2.28, batch loss = 2.23 (35.8 examples/sec; 0.224 sec/batch; 17h:42m:17s remains)
INFO - root - 2017-12-08 07:52:43.384450: step 47630, loss = 2.28, batch loss = 2.23 (35.4 examples/sec; 0.226 sec/batch; 17h:53m:38s remains)
INFO - root - 2017-12-08 07:52:45.621084: step 47640, loss = 2.28, batch loss = 2.23 (36.7 examples/sec; 0.218 sec/batch; 17h:14m:22s remains)
INFO - root - 2017-12-08 07:52:47.848981: step 47650, loss = 2.28, batch loss = 2.23 (36.0 examples/sec; 0.222 sec/batch; 17h:36m:15s remains)
INFO - root - 2017-12-08 07:52:50.073317: step 47660, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 17h:23m:39s remains)
INFO - root - 2017-12-08 07:52:52.307073: step 47670, loss = 2.28, batch loss = 2.23 (35.6 examples/sec; 0.225 sec/batch; 17h:47m:19s remains)
INFO - root - 2017-12-08 07:52:54.565882: step 47680, loss = 2.28, batch loss = 2.23 (36.0 examples/sec; 0.222 sec/batch; 17h:34m:07s remains)
INFO - root - 2017-12-08 07:52:56.788490: step 47690, loss = 2.28, batch loss = 2.23 (35.3 examples/sec; 0.227 sec/batch; 17h:56m:08s remains)
INFO - root - 2017-12-08 07:52:59.031244: step 47700, loss = 2.28, batch loss = 2.23 (34.9 examples/sec; 0.229 sec/batch; 18h:08m:24s remains)
2017-12-08 07:52:59.341914: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4286084 -4.4285769 -4.4285512 -4.4285345 -4.4285417 -4.4285836 -4.4286323 -4.4286485 -4.4286346 -4.4286032 -4.4285574 -4.4285312 -4.42853 -4.428555 -4.4286017][-4.4286084 -4.4285831 -4.4285636 -4.4285645 -4.4285889 -4.428638 -4.4286895 -4.4287219 -4.4287276 -4.4287133 -4.42868 -4.4286604 -4.4286523 -4.4286542 -4.4286752][-4.4286036 -4.428596 -4.4285889 -4.4285979 -4.4286222 -4.4286628 -4.4287057 -4.4287415 -4.4287615 -4.428772 -4.4287605 -4.4287505 -4.4287395 -4.4287333 -4.4287395][-4.4286 -4.42862 -4.4286265 -4.428628 -4.4286203 -4.4286213 -4.4286366 -4.4286666 -4.4286985 -4.4287314 -4.4287419 -4.4287438 -4.4287424 -4.4287524 -4.4287705][-4.4285941 -4.4286408 -4.4286585 -4.4286418 -4.4285855 -4.4285221 -4.4284854 -4.4285007 -4.4285512 -4.4286141 -4.428648 -4.4286685 -4.4286876 -4.4287195 -4.4287577][-4.4286017 -4.4286556 -4.4286733 -4.4286432 -4.428555 -4.428443 -4.4283509 -4.4283385 -4.4284086 -4.4285026 -4.428555 -4.4285879 -4.4286232 -4.42867 -4.428719][-4.4286213 -4.4286652 -4.428678 -4.4286466 -4.4285593 -4.4284496 -4.4283376 -4.4282961 -4.4283566 -4.4284534 -4.4285049 -4.4285364 -4.428576 -4.4286275 -4.4286776][-4.4286404 -4.4286714 -4.4286876 -4.4286656 -4.4285994 -4.4285216 -4.4284272 -4.4283738 -4.4284072 -4.4284782 -4.4285135 -4.428535 -4.4285617 -4.4286041 -4.4286494][-4.428688 -4.4287066 -4.4287252 -4.4287181 -4.4286795 -4.4286342 -4.4285674 -4.4285226 -4.4285355 -4.428576 -4.428596 -4.4286036 -4.42861 -4.4286313 -4.4286618][-4.4287691 -4.4287748 -4.4287882 -4.4287915 -4.4287777 -4.4287572 -4.42872 -4.4286928 -4.4286928 -4.4287052 -4.4287081 -4.4287047 -4.4286957 -4.4287004 -4.4287138][-4.4288478 -4.4288392 -4.4288421 -4.4288535 -4.4288588 -4.4288578 -4.4288459 -4.4288354 -4.4288297 -4.4288187 -4.4288077 -4.4287982 -4.4287829 -4.4287806 -4.4287863][-4.4289036 -4.4288883 -4.4288855 -4.4288974 -4.4289117 -4.4289212 -4.4289255 -4.428925 -4.4289174 -4.428896 -4.4288774 -4.4288659 -4.428853 -4.4288507 -4.4288578][-4.4289303 -4.4289155 -4.4289107 -4.4289203 -4.428937 -4.4289513 -4.4289641 -4.4289708 -4.4289684 -4.4289503 -4.42893 -4.428916 -4.4289069 -4.4289083 -4.4289184][-4.4289508 -4.4289422 -4.4289379 -4.4289412 -4.4289517 -4.4289651 -4.42898 -4.4289904 -4.4289942 -4.4289851 -4.4289722 -4.4289618 -4.4289565 -4.4289584 -4.4289665][-4.428978 -4.4289765 -4.4289742 -4.4289732 -4.4289756 -4.4289818 -4.4289894 -4.428997 -4.4290023 -4.4290009 -4.4289956 -4.4289908 -4.4289894 -4.4289913 -4.4289956]]...]
INFO - root - 2017-12-08 07:53:01.575692: step 47710, loss = 2.28, batch loss = 2.23 (34.5 examples/sec; 0.232 sec/batch; 18h:19m:29s remains)
INFO - root - 2017-12-08 07:53:03.801163: step 47720, loss = 2.28, batch loss = 2.23 (35.3 examples/sec; 0.227 sec/batch; 17h:55m:26s remains)
INFO - root - 2017-12-08 07:53:06.034687: step 47730, loss = 2.28, batch loss = 2.23 (34.1 examples/sec; 0.235 sec/batch; 18h:34m:24s remains)
INFO - root - 2017-12-08 07:53:08.272945: step 47740, loss = 2.28, batch loss = 2.23 (34.9 examples/sec; 0.229 sec/batch; 18h:06m:59s remains)
INFO - root - 2017-12-08 07:53:10.514107: step 47750, loss = 2.28, batch loss = 2.23 (35.1 examples/sec; 0.228 sec/batch; 18h:02m:12s remains)
INFO - root - 2017-12-08 07:53:12.734060: step 47760, loss = 2.28, batch loss = 2.23 (36.7 examples/sec; 0.218 sec/batch; 17h:14m:56s remains)
INFO - root - 2017-12-08 07:53:14.982792: step 47770, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.222 sec/batch; 17h:31m:14s remains)
INFO - root - 2017-12-08 07:53:17.218714: step 47780, loss = 2.28, batch loss = 2.23 (35.0 examples/sec; 0.229 sec/batch; 18h:05m:24s remains)
INFO - root - 2017-12-08 07:53:19.485507: step 47790, loss = 2.28, batch loss = 2.23 (33.6 examples/sec; 0.238 sec/batch; 18h:49m:55s remains)
INFO - root - 2017-12-08 07:53:21.703180: step 47800, loss = 2.28, batch loss = 2.23 (35.4 examples/sec; 0.226 sec/batch; 17h:52m:52s remains)
2017-12-08 07:53:21.986200: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4287844 -4.4287386 -4.4286332 -4.4285369 -4.4286051 -4.4287133 -4.4287825 -4.4288058 -4.4287972 -4.428792 -4.4287872 -4.4287887 -4.4287963 -4.4287987 -4.4287963][-4.4287772 -4.4287252 -4.4286246 -4.4285445 -4.4286003 -4.4286823 -4.4287395 -4.4287591 -4.4287591 -4.4287615 -4.4287543 -4.4287496 -4.4287534 -4.4287596 -4.4287658][-4.4287987 -4.4287453 -4.4286604 -4.4286056 -4.4286375 -4.428679 -4.4287114 -4.4287186 -4.4287314 -4.4287438 -4.4287381 -4.4287291 -4.4287281 -4.4287362 -4.4287524][-4.4288321 -4.4287686 -4.4286933 -4.4286509 -4.4286551 -4.4286518 -4.4286432 -4.428638 -4.4286842 -4.4287343 -4.4287438 -4.4287395 -4.4287219 -4.4287133 -4.4287257][-4.4288445 -4.42876 -4.4286761 -4.4286289 -4.4286022 -4.4285431 -4.4284635 -4.4284353 -4.4285421 -4.4286523 -4.4286895 -4.4286971 -4.4286752 -4.4286575 -4.4286714][-4.4288268 -4.4287219 -4.4286165 -4.4285283 -4.4284396 -4.4282975 -4.428102 -4.4280477 -4.4282727 -4.4284854 -4.4285793 -4.4286084 -4.428586 -4.4285579 -4.4285874][-4.4287972 -4.4286714 -4.4285331 -4.4283843 -4.4282246 -4.4279876 -4.4276519 -4.427578 -4.4279747 -4.4283175 -4.4284687 -4.428503 -4.4284687 -4.4284463 -4.4284973][-4.4287968 -4.4286704 -4.4285183 -4.4283485 -4.4281783 -4.4279404 -4.4276009 -4.4275584 -4.4279804 -4.4283233 -4.4284587 -4.4284554 -4.4284105 -4.4284182 -4.4284892][-4.428843 -4.4287386 -4.42861 -4.428484 -4.4283781 -4.4282217 -4.4280033 -4.4279947 -4.4282551 -4.4284706 -4.42854 -4.4285011 -4.428463 -4.4284992 -4.4285755][-4.4288831 -4.4288068 -4.4287109 -4.4286351 -4.4285994 -4.4285264 -4.4284148 -4.4284067 -4.4285221 -4.42862 -4.4286456 -4.4286013 -4.4285755 -4.4286079 -4.4286585][-4.4288921 -4.4288354 -4.4287605 -4.4287262 -4.4287424 -4.42873 -4.4286847 -4.4286733 -4.4287066 -4.4287453 -4.4287558 -4.4287219 -4.428709 -4.428719 -4.42873][-4.428884 -4.4288378 -4.4287705 -4.4287424 -4.4287777 -4.4287992 -4.4287963 -4.4287877 -4.4287834 -4.4288 -4.4288106 -4.4288011 -4.4288015 -4.4287992 -4.4287853][-4.4288692 -4.428833 -4.4287672 -4.4287262 -4.4287553 -4.428793 -4.4288177 -4.4288158 -4.428802 -4.4288168 -4.4288344 -4.4288435 -4.42885 -4.428843 -4.4288268][-4.4288788 -4.4288564 -4.4287915 -4.4287376 -4.4287577 -4.4288011 -4.4288454 -4.4288511 -4.4288387 -4.4288545 -4.4288764 -4.42889 -4.4288974 -4.4288945 -4.42888][-4.4288859 -4.42887 -4.4288163 -4.4287629 -4.428772 -4.4288116 -4.42886 -4.42887 -4.4288621 -4.4288797 -4.428906 -4.4289269 -4.42894 -4.4289403 -4.4289274]]...]
INFO - root - 2017-12-08 07:53:24.228931: step 47810, loss = 2.28, batch loss = 2.23 (34.8 examples/sec; 0.230 sec/batch; 18h:11m:30s remains)
INFO - root - 2017-12-08 07:53:26.464475: step 47820, loss = 2.28, batch loss = 2.23 (35.6 examples/sec; 0.225 sec/batch; 17h:46m:32s remains)
INFO - root - 2017-12-08 07:53:28.688163: step 47830, loss = 2.28, batch loss = 2.23 (37.4 examples/sec; 0.214 sec/batch; 16h:55m:10s remains)
INFO - root - 2017-12-08 07:53:30.947545: step 47840, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 17h:38m:42s remains)
INFO - root - 2017-12-08 07:53:33.213157: step 47850, loss = 2.28, batch loss = 2.23 (34.5 examples/sec; 0.232 sec/batch; 18h:18m:55s remains)
INFO - root - 2017-12-08 07:53:35.454385: step 47860, loss = 2.28, batch loss = 2.23 (36.0 examples/sec; 0.222 sec/batch; 17h:34m:05s remains)
INFO - root - 2017-12-08 07:53:37.691039: step 47870, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 17h:36m:29s remains)
INFO - root - 2017-12-08 07:53:39.969381: step 47880, loss = 2.28, batch loss = 2.23 (34.5 examples/sec; 0.232 sec/batch; 18h:20m:33s remains)
INFO - root - 2017-12-08 07:53:42.209020: step 47890, loss = 2.28, batch loss = 2.23 (34.9 examples/sec; 0.229 sec/batch; 18h:06m:15s remains)
INFO - root - 2017-12-08 07:53:44.442942: step 47900, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 17h:37m:13s remains)
2017-12-08 07:53:44.728158: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.428936 -4.4289289 -4.4289331 -4.4289365 -4.4289346 -4.4289289 -4.42892 -4.4289093 -4.4289031 -4.428894 -4.4288826 -4.4288664 -4.4288478 -4.4288397 -4.4288192][-4.4288979 -4.4288859 -4.4288898 -4.42889 -4.4288869 -4.4288793 -4.4288669 -4.4288549 -4.4288516 -4.4288435 -4.4288273 -4.4288044 -4.4287748 -4.4287567 -4.4287157][-4.4288678 -4.4288526 -4.4288535 -4.4288449 -4.4288311 -4.4288173 -4.4287996 -4.4287863 -4.4287863 -4.4287796 -4.428761 -4.4287343 -4.4286985 -4.4286709 -4.4286132][-4.4288392 -4.4288216 -4.4288235 -4.4288044 -4.4287753 -4.4287477 -4.4287162 -4.4287004 -4.4287043 -4.4287004 -4.4286785 -4.4286561 -4.428628 -4.4286041 -4.4285464][-4.4288139 -4.4287968 -4.428802 -4.4287739 -4.4287267 -4.4286737 -4.4286141 -4.4285927 -4.4286022 -4.42861 -4.4285979 -4.4285975 -4.4285989 -4.4285927 -4.4285507][-4.4288087 -4.4287953 -4.4287992 -4.42876 -4.4286838 -4.4285936 -4.4285007 -4.4284678 -4.4284883 -4.4285269 -4.4285536 -4.4285884 -4.4286289 -4.4286494 -4.4286337][-4.4288244 -4.4288144 -4.4288116 -4.4287581 -4.4286523 -4.42853 -4.4284072 -4.4283624 -4.4283977 -4.42847 -4.4285421 -4.4286165 -4.4286852 -4.4287262 -4.42874][-4.4288473 -4.4288392 -4.4288278 -4.4287653 -4.4286456 -4.4285092 -4.4283881 -4.4283547 -4.4284048 -4.42849 -4.4285827 -4.4286761 -4.4287524 -4.4288044 -4.428834][-4.4288621 -4.4288545 -4.428843 -4.4287791 -4.42867 -4.4285564 -4.4284649 -4.4284415 -4.4284859 -4.428555 -4.4286394 -4.4287295 -4.4288025 -4.4288545 -4.4288893][-4.4288535 -4.4288492 -4.4288487 -4.4288011 -4.4287167 -4.4286356 -4.42858 -4.4285693 -4.4286022 -4.4286451 -4.4287038 -4.4287715 -4.4288292 -4.4288707 -4.4289002][-4.42882 -4.4288225 -4.4288459 -4.42883 -4.4287758 -4.4287214 -4.428689 -4.4286847 -4.428709 -4.4287391 -4.4287796 -4.4288263 -4.4288654 -4.428894 -4.4289165][-4.4287643 -4.4287705 -4.4288225 -4.4288378 -4.4288125 -4.4287791 -4.4287558 -4.4287553 -4.4287744 -4.428803 -4.4288373 -4.4288764 -4.4289055 -4.4289241 -4.428936][-4.4287095 -4.4287028 -4.4287682 -4.4288087 -4.4288135 -4.4287996 -4.4287887 -4.4287953 -4.4288139 -4.4288445 -4.42888 -4.4289165 -4.4289403 -4.4289527 -4.4289575][-4.4286919 -4.4286685 -4.4287291 -4.4287748 -4.4287963 -4.4287949 -4.4287977 -4.4288177 -4.4288445 -4.4288788 -4.4289136 -4.4289422 -4.4289579 -4.428968 -4.4289727][-4.4287276 -4.4286876 -4.4287267 -4.4287605 -4.4287796 -4.4287806 -4.4287939 -4.4288297 -4.4288692 -4.42891 -4.4289389 -4.428957 -4.4289627 -4.4289689 -4.4289746]]...]
INFO - root - 2017-12-08 07:53:46.971583: step 47910, loss = 2.28, batch loss = 2.23 (36.6 examples/sec; 0.218 sec/batch; 17h:16m:22s remains)
INFO - root - 2017-12-08 07:53:49.200686: step 47920, loss = 2.28, batch loss = 2.23 (34.2 examples/sec; 0.234 sec/batch; 18h:28m:26s remains)
INFO - root - 2017-12-08 07:53:51.459214: step 47930, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 17h:35m:32s remains)
INFO - root - 2017-12-08 07:53:53.684198: step 47940, loss = 2.28, batch loss = 2.23 (36.6 examples/sec; 0.219 sec/batch; 17h:17m:05s remains)
INFO - root - 2017-12-08 07:53:55.916189: step 47950, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 17h:37m:28s remains)
INFO - root - 2017-12-08 07:53:58.143627: step 47960, loss = 2.28, batch loss = 2.23 (37.4 examples/sec; 0.214 sec/batch; 16h:55m:22s remains)
INFO - root - 2017-12-08 07:54:00.375832: step 47970, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.221 sec/batch; 17h:26m:11s remains)
INFO - root - 2017-12-08 07:54:02.620113: step 47980, loss = 2.28, batch loss = 2.23 (35.4 examples/sec; 0.226 sec/batch; 17h:50m:44s remains)
INFO - root - 2017-12-08 07:54:04.871526: step 47990, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 17h:36m:13s remains)
INFO - root - 2017-12-08 07:54:07.109385: step 48000, loss = 2.28, batch loss = 2.23 (35.0 examples/sec; 0.229 sec/batch; 18h:04m:13s remains)
2017-12-08 07:54:07.439558: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4287353 -4.4287233 -4.4287229 -4.4287295 -4.4287333 -4.4287062 -4.4286776 -4.4286685 -4.4286661 -4.4286566 -4.4286351 -4.4286246 -4.4286189 -4.4286103 -4.4286051][-4.4287267 -4.4287181 -4.4287176 -4.4287133 -4.4287162 -4.4286971 -4.4286718 -4.4286528 -4.4286389 -4.428628 -4.4286122 -4.4285989 -4.4285927 -4.4285855 -4.4285922][-4.4287262 -4.4287195 -4.4287171 -4.428709 -4.4287138 -4.4286985 -4.4286819 -4.4286666 -4.4286528 -4.4286413 -4.4286208 -4.428596 -4.4285936 -4.428596 -4.4286065][-4.4287114 -4.4287047 -4.4287057 -4.4287 -4.4287066 -4.4286914 -4.4286919 -4.4286985 -4.4286947 -4.4286809 -4.428647 -4.4286203 -4.4286208 -4.4286261 -4.4286261][-4.4286761 -4.428668 -4.4286709 -4.4286666 -4.4286594 -4.4286251 -4.4286203 -4.4286485 -4.4286671 -4.4286647 -4.4286261 -4.4285994 -4.4286 -4.4286103 -4.4286046][-4.4286385 -4.4286366 -4.4286361 -4.4286141 -4.4285793 -4.4285173 -4.4284887 -4.4285164 -4.4285522 -4.4285612 -4.4285283 -4.4285078 -4.4285192 -4.4285364 -4.4285269][-4.4286141 -4.4286251 -4.428616 -4.4285741 -4.4285173 -4.4284363 -4.4283967 -4.4284105 -4.4284453 -4.4284649 -4.4284539 -4.42845 -4.428473 -4.4284806 -4.4284554][-4.4286141 -4.4286356 -4.4286232 -4.4285755 -4.4285216 -4.4284544 -4.4284244 -4.4284291 -4.4284444 -4.4284616 -4.4284678 -4.4284806 -4.428504 -4.4284925 -4.4284458][-4.428627 -4.4286523 -4.4286461 -4.4286146 -4.4285917 -4.4285564 -4.4285417 -4.428545 -4.4285455 -4.4285474 -4.4285526 -4.4285669 -4.4285831 -4.4285688 -4.4285274][-4.4286251 -4.4286427 -4.428648 -4.4286518 -4.4286618 -4.428647 -4.4286451 -4.4286556 -4.4286566 -4.4286542 -4.4286547 -4.4286685 -4.4286914 -4.42869 -4.4286685][-4.4286103 -4.428618 -4.4286366 -4.4286661 -4.4286985 -4.4287014 -4.4287066 -4.42872 -4.4287267 -4.4287286 -4.428731 -4.4287472 -4.4287744 -4.4287839 -4.4287772][-4.4286327 -4.42863 -4.4286504 -4.4286852 -4.4287086 -4.4287119 -4.428719 -4.4287276 -4.4287367 -4.4287453 -4.4287491 -4.4287577 -4.4287853 -4.4288044 -4.4288058][-4.4286771 -4.4286628 -4.4286723 -4.4286971 -4.4287014 -4.4286942 -4.4286952 -4.4286928 -4.4286933 -4.4287066 -4.4287081 -4.4287057 -4.4287295 -4.4287581 -4.4287686][-4.4287071 -4.4286957 -4.4287028 -4.4287114 -4.4286938 -4.4286709 -4.4286523 -4.4286346 -4.4286275 -4.4286437 -4.4286427 -4.4286337 -4.4286575 -4.4286885 -4.4286971][-4.4287248 -4.4287133 -4.4287171 -4.4287162 -4.4286852 -4.4286509 -4.4286094 -4.4285803 -4.42858 -4.4286075 -4.4286065 -4.4285913 -4.4286065 -4.4286218 -4.4286113]]...]
INFO - root - 2017-12-08 07:54:09.704657: step 48010, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.220 sec/batch; 17h:25m:01s remains)
INFO - root - 2017-12-08 07:54:11.936648: step 48020, loss = 2.28, batch loss = 2.23 (36.7 examples/sec; 0.218 sec/batch; 17h:12m:17s remains)
INFO - root - 2017-12-08 07:54:14.183860: step 48030, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.225 sec/batch; 17h:47m:46s remains)
INFO - root - 2017-12-08 07:54:16.442007: step 48040, loss = 2.28, batch loss = 2.23 (35.8 examples/sec; 0.224 sec/batch; 17h:40m:09s remains)
INFO - root - 2017-12-08 07:54:18.730789: step 48050, loss = 2.28, batch loss = 2.23 (36.7 examples/sec; 0.218 sec/batch; 17h:13m:44s remains)
INFO - root - 2017-12-08 07:54:20.962998: step 48060, loss = 2.28, batch loss = 2.23 (36.6 examples/sec; 0.219 sec/batch; 17h:17m:34s remains)
INFO - root - 2017-12-08 07:54:23.187836: step 48070, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.221 sec/batch; 17h:29m:58s remains)
INFO - root - 2017-12-08 07:54:25.432840: step 48080, loss = 2.28, batch loss = 2.23 (35.0 examples/sec; 0.229 sec/batch; 18h:03m:23s remains)
INFO - root - 2017-12-08 07:54:27.663123: step 48090, loss = 2.28, batch loss = 2.23 (36.6 examples/sec; 0.219 sec/batch; 17h:17m:11s remains)
INFO - root - 2017-12-08 07:54:29.891321: step 48100, loss = 2.28, batch loss = 2.23 (35.0 examples/sec; 0.228 sec/batch; 18h:02m:21s remains)
2017-12-08 07:54:30.195499: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4287558 -4.428751 -4.42876 -4.4287739 -4.4287858 -4.4287763 -4.4287686 -4.4287839 -4.4288244 -4.4288616 -4.4288826 -4.4288936 -4.4288936 -4.428894 -4.4289093][-4.4287047 -4.4286842 -4.4286718 -4.4286828 -4.4286981 -4.4286933 -4.4287066 -4.4287467 -4.4287958 -4.4288287 -4.428843 -4.4288526 -4.4288554 -4.4288645 -4.4288907][-4.4286361 -4.4286256 -4.4286036 -4.4286056 -4.4286189 -4.4286237 -4.4286628 -4.4287233 -4.4287672 -4.4287872 -4.4287872 -4.4287863 -4.4287872 -4.4287996 -4.4288316][-4.4286065 -4.4286275 -4.4286089 -4.4285841 -4.4285741 -4.4285636 -4.428606 -4.428668 -4.4287081 -4.4287353 -4.4287391 -4.4287319 -4.4287338 -4.4287453 -4.428772][-4.4285822 -4.4286466 -4.4286466 -4.4285941 -4.4285517 -4.4285192 -4.4285297 -4.4285612 -4.4285889 -4.4286323 -4.4286609 -4.4286652 -4.4286785 -4.428699 -4.4287281][-4.4285731 -4.4286652 -4.4286733 -4.4285941 -4.4285183 -4.4284625 -4.4284391 -4.4284425 -4.4284678 -4.4285274 -4.4285908 -4.4286213 -4.4286523 -4.4286938 -4.4287357][-4.428597 -4.4286704 -4.4286613 -4.4285579 -4.4284673 -4.4284196 -4.4283895 -4.4283834 -4.428411 -4.4284744 -4.4285607 -4.4286122 -4.42866 -4.4287162 -4.428771][-4.4286389 -4.4286594 -4.428627 -4.4285321 -4.4284706 -4.4284563 -4.4284406 -4.4284263 -4.4284434 -4.4284954 -4.4285893 -4.4286537 -4.4287066 -4.4287624 -4.4288163][-4.4287066 -4.42868 -4.4286304 -4.428556 -4.4285245 -4.4285288 -4.42853 -4.4285092 -4.4285183 -4.4285555 -4.4286418 -4.4287114 -4.4287596 -4.4288087 -4.4288592][-4.4287462 -4.4287033 -4.4286528 -4.4285989 -4.4285851 -4.4286 -4.4286132 -4.428597 -4.4285965 -4.4286127 -4.4286833 -4.4287496 -4.4287944 -4.4288368 -4.4288788][-4.428761 -4.4287162 -4.4286661 -4.4286304 -4.4286275 -4.4286494 -4.4286733 -4.4286685 -4.428669 -4.42867 -4.4287233 -4.4287772 -4.4288158 -4.4288526 -4.4288864][-4.4287467 -4.4287004 -4.4286551 -4.4286394 -4.4286447 -4.428669 -4.4287047 -4.42871 -4.428709 -4.4287009 -4.4287372 -4.4287748 -4.4288011 -4.4288321 -4.428864][-4.4287286 -4.428678 -4.428628 -4.4286132 -4.4286256 -4.4286504 -4.4286904 -4.4287076 -4.4287081 -4.4287014 -4.4287252 -4.4287529 -4.4287663 -4.4287863 -4.4288173][-4.4287338 -4.4286737 -4.4286118 -4.4285793 -4.4285784 -4.4285989 -4.4286442 -4.4286761 -4.4286866 -4.4286819 -4.4287076 -4.4287348 -4.428731 -4.4287319 -4.4287586][-4.428751 -4.4286962 -4.4286251 -4.4285736 -4.428556 -4.4285731 -4.4286256 -4.4286704 -4.4286919 -4.42869 -4.4287124 -4.4287324 -4.4287114 -4.4286852 -4.4286985]]...]
INFO - root - 2017-12-08 07:54:32.423570: step 48110, loss = 2.28, batch loss = 2.23 (37.0 examples/sec; 0.216 sec/batch; 17h:05m:33s remains)
INFO - root - 2017-12-08 07:54:34.676126: step 48120, loss = 2.28, batch loss = 2.23 (34.8 examples/sec; 0.230 sec/batch; 18h:08m:32s remains)
INFO - root - 2017-12-08 07:54:36.899028: step 48130, loss = 2.28, batch loss = 2.23 (35.2 examples/sec; 0.227 sec/batch; 17h:57m:57s remains)
INFO - root - 2017-12-08 07:54:39.133520: step 48140, loss = 2.28, batch loss = 2.23 (36.8 examples/sec; 0.217 sec/batch; 17h:10m:06s remains)
INFO - root - 2017-12-08 07:54:41.358936: step 48150, loss = 2.28, batch loss = 2.23 (35.0 examples/sec; 0.229 sec/batch; 18h:04m:34s remains)
INFO - root - 2017-12-08 07:54:43.578060: step 48160, loss = 2.28, batch loss = 2.23 (38.1 examples/sec; 0.210 sec/batch; 16h:35m:13s remains)
INFO - root - 2017-12-08 07:54:45.778426: step 48170, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.225 sec/batch; 17h:47m:36s remains)
INFO - root - 2017-12-08 07:54:48.049570: step 48180, loss = 2.28, batch loss = 2.23 (36.0 examples/sec; 0.222 sec/batch; 17h:33m:12s remains)
INFO - root - 2017-12-08 07:54:50.274354: step 48190, loss = 2.28, batch loss = 2.23 (37.4 examples/sec; 0.214 sec/batch; 16h:54m:40s remains)
INFO - root - 2017-12-08 07:54:52.514169: step 48200, loss = 2.28, batch loss = 2.23 (35.3 examples/sec; 0.227 sec/batch; 17h:55m:14s remains)
2017-12-08 07:54:52.809281: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289589 -4.4289761 -4.4289746 -4.4289541 -4.42893 -4.4289088 -4.428896 -4.4288778 -4.4288507 -4.428834 -4.4288306 -4.4288497 -4.4288735 -4.4288936 -4.4289126][-4.4289656 -4.4289665 -4.428947 -4.4289174 -4.4288826 -4.4288445 -4.4288206 -4.428792 -4.4287553 -4.4287353 -4.4287324 -4.4287629 -4.42881 -4.428853 -4.4288821][-4.4289474 -4.4289279 -4.42889 -4.4288535 -4.4288139 -4.4287734 -4.4287462 -4.4287086 -4.4286628 -4.428647 -4.4286556 -4.4287043 -4.4287767 -4.4288383 -4.4288721][-4.4289026 -4.4288669 -4.4288116 -4.4287686 -4.4287329 -4.4287019 -4.4286776 -4.4286294 -4.4285784 -4.4285684 -4.4285851 -4.428648 -4.4287477 -4.4288292 -4.4288673][-4.4288616 -4.4288111 -4.4287477 -4.4287081 -4.4286866 -4.4286714 -4.4286523 -4.4285975 -4.4285431 -4.428545 -4.4285755 -4.4286423 -4.4287524 -4.428843 -4.4288836][-4.4288111 -4.4287496 -4.4286795 -4.428637 -4.4286156 -4.4286017 -4.4285846 -4.4285359 -4.428494 -4.4285169 -4.4285679 -4.4286451 -4.428762 -4.4288578 -4.428905][-4.4287496 -4.4286885 -4.4286265 -4.4285731 -4.4285283 -4.4284868 -4.4284449 -4.4283853 -4.4283471 -4.4284034 -4.4285045 -4.4286222 -4.4287562 -4.4288549 -4.4289064][-4.4287372 -4.4286842 -4.4286313 -4.4285674 -4.4284987 -4.4284282 -4.4283452 -4.4282408 -4.4281735 -4.4282393 -4.4283834 -4.4285464 -4.4287019 -4.428812 -4.4288754][-4.4287534 -4.4287143 -4.4286757 -4.428618 -4.4285684 -4.4285307 -4.4284835 -4.4283991 -4.4283037 -4.428297 -4.4283772 -4.4285069 -4.4286494 -4.42877 -4.4288454][-4.4287581 -4.4287252 -4.4287014 -4.428669 -4.4286542 -4.4286528 -4.4286432 -4.4285846 -4.4284997 -4.4284658 -4.4284878 -4.4285593 -4.4286647 -4.4287734 -4.4288464][-4.4287524 -4.4287148 -4.4286971 -4.4286776 -4.4286776 -4.4286962 -4.4287128 -4.4286852 -4.4286294 -4.428606 -4.4286156 -4.4286532 -4.4287229 -4.4288058 -4.4288607][-4.4287577 -4.4287238 -4.428709 -4.4286895 -4.4286852 -4.4287038 -4.4287367 -4.4287381 -4.4287176 -4.4287143 -4.4287243 -4.4287467 -4.4287958 -4.4288511 -4.4288836][-4.4287748 -4.4287524 -4.4287438 -4.4287252 -4.4287133 -4.4287214 -4.4287505 -4.4287629 -4.4287553 -4.428761 -4.4287772 -4.4288039 -4.4288435 -4.4288812 -4.4288993][-4.428782 -4.4287686 -4.4287667 -4.4287596 -4.42875 -4.4287486 -4.4287629 -4.4287691 -4.4287629 -4.4287677 -4.4287848 -4.4288144 -4.4288516 -4.4288812 -4.4288974][-4.4287744 -4.4287648 -4.4287648 -4.4287663 -4.428762 -4.4287577 -4.4287643 -4.4287682 -4.4287639 -4.4287634 -4.4287734 -4.4287996 -4.428834 -4.4288621 -4.4288812]]...]
INFO - root - 2017-12-08 07:54:55.060441: step 48210, loss = 2.28, batch loss = 2.23 (35.0 examples/sec; 0.229 sec/batch; 18h:02m:48s remains)
INFO - root - 2017-12-08 07:54:57.324066: step 48220, loss = 2.28, batch loss = 2.23 (31.5 examples/sec; 0.254 sec/batch; 20h:03m:09s remains)
INFO - root - 2017-12-08 07:54:59.554447: step 48230, loss = 2.28, batch loss = 2.23 (36.0 examples/sec; 0.222 sec/batch; 17h:33m:07s remains)
INFO - root - 2017-12-08 07:55:01.789171: step 48240, loss = 2.28, batch loss = 2.23 (35.3 examples/sec; 0.227 sec/batch; 17h:53m:33s remains)
INFO - root - 2017-12-08 07:55:04.051347: step 48250, loss = 2.28, batch loss = 2.23 (35.3 examples/sec; 0.226 sec/batch; 17h:52m:59s remains)
INFO - root - 2017-12-08 07:55:06.312025: step 48260, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 17h:26m:28s remains)
INFO - root - 2017-12-08 07:55:08.545415: step 48270, loss = 2.28, batch loss = 2.23 (34.9 examples/sec; 0.230 sec/batch; 18h:07m:15s remains)
INFO - root - 2017-12-08 07:55:10.802397: step 48280, loss = 2.28, batch loss = 2.23 (33.9 examples/sec; 0.236 sec/batch; 18h:38m:25s remains)
INFO - root - 2017-12-08 07:55:13.072971: step 48290, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.220 sec/batch; 17h:23m:28s remains)
INFO - root - 2017-12-08 07:55:15.335537: step 48300, loss = 2.28, batch loss = 2.23 (35.0 examples/sec; 0.229 sec/batch; 18h:03m:40s remains)
2017-12-08 07:55:15.637996: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289289 -4.4289203 -4.4289227 -4.4289279 -4.4289269 -4.4289308 -4.4289322 -4.4289379 -4.4289403 -4.4289303 -4.4289184 -4.4289136 -4.4289155 -4.4289155 -4.4289179][-4.4289122 -4.4289079 -4.4289112 -4.4289246 -4.4289308 -4.4289327 -4.4289255 -4.4289222 -4.4289203 -4.4289141 -4.4289041 -4.428896 -4.4288926 -4.4288883 -4.4288898][-4.4288745 -4.4288864 -4.4288969 -4.4289126 -4.4289141 -4.4289012 -4.4288712 -4.4288487 -4.4288464 -4.4288526 -4.4288578 -4.42886 -4.4288592 -4.4288583 -4.4288635][-4.42884 -4.4288731 -4.4288931 -4.4289026 -4.4288812 -4.4288297 -4.4287577 -4.42871 -4.4287171 -4.4287558 -4.428793 -4.4288163 -4.428823 -4.4288254 -4.4288311][-4.4287887 -4.4288492 -4.4288855 -4.4288874 -4.4288316 -4.428721 -4.4285855 -4.4285059 -4.428534 -4.4286256 -4.4287114 -4.4287643 -4.4287891 -4.4288015 -4.4288096][-4.4287138 -4.4288054 -4.4288568 -4.4288497 -4.4287567 -4.4285827 -4.4283595 -4.4282265 -4.428287 -4.4284549 -4.4286065 -4.428699 -4.4287519 -4.4287782 -4.4287915][-4.4286594 -4.4287639 -4.4288125 -4.4287949 -4.428678 -4.4284611 -4.4281621 -4.42798 -4.4280934 -4.4283452 -4.4285493 -4.4286604 -4.4287219 -4.4287534 -4.4287734][-4.428648 -4.4287319 -4.4287615 -4.4287415 -4.4286408 -4.4284492 -4.428174 -4.4280143 -4.4281473 -4.4283929 -4.428576 -4.4286714 -4.4287262 -4.4287572 -4.4287815][-4.4286704 -4.4287124 -4.428721 -4.4287205 -4.4286776 -4.4285712 -4.4284139 -4.4283247 -4.4284096 -4.4285607 -4.4286728 -4.4287329 -4.428772 -4.4287992 -4.4288259][-4.4286952 -4.4287095 -4.4287114 -4.4287329 -4.4287434 -4.4287176 -4.4286518 -4.4286079 -4.4286456 -4.4287143 -4.4287663 -4.4287949 -4.428822 -4.4288497 -4.4288774][-4.4287114 -4.4287105 -4.4287133 -4.4287438 -4.4287782 -4.428792 -4.4287753 -4.4287586 -4.4287753 -4.4288039 -4.4288211 -4.4288282 -4.4288464 -4.4288759 -4.428906][-4.4287233 -4.4287124 -4.4287086 -4.42873 -4.4287658 -4.4287992 -4.4288154 -4.4288173 -4.4288263 -4.42883 -4.4288278 -4.4288254 -4.4288397 -4.4288692 -4.428896][-4.42875 -4.4287338 -4.4287262 -4.4287329 -4.4287567 -4.428791 -4.4288182 -4.4288268 -4.4288235 -4.4288034 -4.4287858 -4.4287829 -4.4287992 -4.42883 -4.42885][-4.428792 -4.4287834 -4.4287753 -4.4287682 -4.4287696 -4.4287825 -4.428802 -4.4288139 -4.4288092 -4.4287815 -4.4287596 -4.4287586 -4.4287758 -4.4287987 -4.4288058][-4.4288373 -4.42884 -4.4288335 -4.4288139 -4.4287896 -4.4287724 -4.4287753 -4.4287939 -4.4288068 -4.4287953 -4.42878 -4.4287777 -4.4287829 -4.4287887 -4.428782]]...]
INFO - root - 2017-12-08 07:55:17.880581: step 48310, loss = 2.28, batch loss = 2.23 (35.3 examples/sec; 0.227 sec/batch; 17h:53m:28s remains)
INFO - root - 2017-12-08 07:55:20.132270: step 48320, loss = 2.28, batch loss = 2.23 (34.2 examples/sec; 0.234 sec/batch; 18h:26m:47s remains)
INFO - root - 2017-12-08 07:55:22.373646: step 48330, loss = 2.28, batch loss = 2.23 (36.6 examples/sec; 0.218 sec/batch; 17h:14m:46s remains)
INFO - root - 2017-12-08 07:55:24.609082: step 48340, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 17h:27m:31s remains)
INFO - root - 2017-12-08 07:55:26.825326: step 48350, loss = 2.28, batch loss = 2.23 (37.1 examples/sec; 0.216 sec/batch; 17h:00m:58s remains)
INFO - root - 2017-12-08 07:55:29.058949: step 48360, loss = 2.28, batch loss = 2.23 (36.6 examples/sec; 0.219 sec/batch; 17h:14m:55s remains)
INFO - root - 2017-12-08 07:55:31.284146: step 48370, loss = 2.28, batch loss = 2.23 (36.8 examples/sec; 0.217 sec/batch; 17h:09m:56s remains)
INFO - root - 2017-12-08 07:55:33.550994: step 48380, loss = 2.28, batch loss = 2.23 (33.9 examples/sec; 0.236 sec/batch; 18h:36m:19s remains)
INFO - root - 2017-12-08 07:55:35.790530: step 48390, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.225 sec/batch; 17h:47m:37s remains)
INFO - root - 2017-12-08 07:55:38.045005: step 48400, loss = 2.28, batch loss = 2.23 (35.7 examples/sec; 0.224 sec/batch; 17h:40m:00s remains)
2017-12-08 07:55:38.334760: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4290175 -4.4290042 -4.4289761 -4.4289322 -4.4288521 -4.4287052 -4.4285545 -4.4284348 -4.42846 -4.4286318 -4.4287419 -4.4287591 -4.4287214 -4.4286947 -4.4286833][-4.4290214 -4.42901 -4.4289851 -4.4289432 -4.4288626 -4.4287114 -4.4285479 -4.4284134 -4.428441 -4.42862 -4.4287391 -4.42876 -4.4287243 -4.4287028 -4.4286938][-4.4290228 -4.4290128 -4.4289908 -4.42895 -4.4288678 -4.4287124 -4.4285326 -4.4283829 -4.428411 -4.4285989 -4.4287276 -4.4287572 -4.4287324 -4.4287233 -4.4287214][-4.4290242 -4.4290156 -4.4289961 -4.4289579 -4.4288764 -4.4287171 -4.4285245 -4.4283581 -4.4283843 -4.4285789 -4.4287119 -4.4287515 -4.4287434 -4.4287491 -4.4287486][-4.4290252 -4.4290175 -4.4289994 -4.4289622 -4.4288831 -4.4287238 -4.4285235 -4.42834 -4.4283662 -4.4285674 -4.4287009 -4.4287486 -4.4287515 -4.4287615 -4.4287591][-4.4290261 -4.429019 -4.4289994 -4.4289618 -4.4288831 -4.4287257 -4.4285207 -4.4283204 -4.4283462 -4.4285531 -4.4286852 -4.4287362 -4.4287424 -4.4287519 -4.4287505][-4.4290261 -4.4290195 -4.4289985 -4.4289608 -4.4288812 -4.4287248 -4.4285131 -4.4282966 -4.4283175 -4.4285283 -4.4286575 -4.4287095 -4.4287214 -4.4287372 -4.4287448][-4.4290261 -4.429019 -4.428998 -4.4289594 -4.4288769 -4.4287181 -4.4285035 -4.4282789 -4.4282951 -4.4285073 -4.4286356 -4.4286914 -4.4287162 -4.4287434 -4.4287596][-4.4290257 -4.4290185 -4.4289966 -4.4289556 -4.4288716 -4.4287148 -4.4285083 -4.4282942 -4.4283123 -4.4285188 -4.4286451 -4.4287033 -4.4287324 -4.428762 -4.4287806][-4.4290252 -4.4290166 -4.4289937 -4.4289517 -4.4288688 -4.4287167 -4.4285254 -4.4283338 -4.4283566 -4.4285507 -4.4286718 -4.4287276 -4.4287562 -4.4287834 -4.4288011][-4.4290237 -4.4290133 -4.4289904 -4.4289489 -4.4288692 -4.4287248 -4.4285493 -4.4283819 -4.4284072 -4.4285922 -4.428709 -4.4287605 -4.4287858 -4.42881 -4.4288278][-4.4290209 -4.4290075 -4.4289837 -4.4289432 -4.4288683 -4.4287338 -4.4285779 -4.4284353 -4.4284658 -4.4286408 -4.4287515 -4.4287949 -4.4288173 -4.4288416 -4.4288573][-4.4290175 -4.4290009 -4.4289761 -4.4289346 -4.4288626 -4.4287405 -4.4286046 -4.4284878 -4.4285235 -4.4286857 -4.4287858 -4.4288235 -4.428844 -4.4288659 -4.4288793][-4.4290142 -4.4289947 -4.428968 -4.428926 -4.4288564 -4.4287472 -4.4286332 -4.4285388 -4.4285769 -4.428719 -4.4288049 -4.4288383 -4.4288568 -4.4288754 -4.4288874][-4.4290123 -4.4289894 -4.4289603 -4.4289188 -4.4288549 -4.428762 -4.4286714 -4.428597 -4.4286313 -4.4287491 -4.4288211 -4.4288545 -4.4288731 -4.4288883 -4.4288974]]...]
INFO - root - 2017-12-08 07:55:40.559845: step 48410, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 17h:21m:17s remains)
INFO - root - 2017-12-08 07:55:42.814441: step 48420, loss = 2.28, batch loss = 2.23 (34.3 examples/sec; 0.233 sec/batch; 18h:24m:51s remains)
INFO - root - 2017-12-08 07:55:45.045052: step 48430, loss = 2.28, batch loss = 2.23 (36.8 examples/sec; 0.217 sec/batch; 17h:09m:30s remains)
INFO - root - 2017-12-08 07:55:47.285992: step 48440, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.225 sec/batch; 17h:46m:15s remains)
INFO - root - 2017-12-08 07:55:49.525572: step 48450, loss = 2.28, batch loss = 2.23 (34.9 examples/sec; 0.229 sec/batch; 18h:05m:08s remains)
INFO - root - 2017-12-08 07:55:51.834032: step 48460, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 17h:27m:30s remains)
INFO - root - 2017-12-08 07:55:54.074105: step 48470, loss = 2.28, batch loss = 2.23 (35.0 examples/sec; 0.229 sec/batch; 18h:02m:33s remains)
INFO - root - 2017-12-08 07:55:56.307173: step 48480, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 17h:18m:22s remains)
INFO - root - 2017-12-08 07:55:58.538001: step 48490, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 17h:16m:17s remains)
INFO - root - 2017-12-08 07:56:00.769723: step 48500, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.220 sec/batch; 17h:22m:47s remains)
2017-12-08 07:56:01.104715: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.428617 -4.4286704 -4.42873 -4.4287424 -4.428731 -4.4286981 -4.4286575 -4.428638 -4.4286542 -4.4287233 -4.4287996 -4.4288473 -4.4288464 -4.4287982 -4.4287362][-4.4286389 -4.4286714 -4.4287086 -4.4287114 -4.4287066 -4.4286895 -4.42866 -4.4286413 -4.4286566 -4.4287252 -4.4288034 -4.4288416 -4.4288249 -4.4287543 -4.4286795][-4.428669 -4.4286819 -4.4286957 -4.4286976 -4.4287 -4.4286976 -4.4286742 -4.428637 -4.4286385 -4.4287019 -4.4287815 -4.4288135 -4.4287791 -4.4286904 -4.4286084][-4.42872 -4.4287162 -4.4287071 -4.4287109 -4.4287119 -4.4287019 -4.428668 -4.4286208 -4.428606 -4.4286575 -4.4287319 -4.4287696 -4.4287362 -4.4286418 -4.4285617][-4.4287949 -4.4287858 -4.4287596 -4.4287353 -4.4287028 -4.4286613 -4.4286079 -4.4285612 -4.4285564 -4.428606 -4.4286709 -4.4287181 -4.4287047 -4.4286242 -4.4285555][-4.4288788 -4.4288816 -4.4288387 -4.4287624 -4.4286685 -4.4285622 -4.428463 -4.428412 -4.4284472 -4.4285393 -4.42863 -4.4286966 -4.428709 -4.4286571 -4.4285979][-4.4289293 -4.4289365 -4.4288535 -4.428709 -4.428534 -4.4283547 -4.4282165 -4.4281874 -4.428298 -4.428472 -4.4286137 -4.4287086 -4.4287486 -4.4287214 -4.4286695][-4.4289408 -4.4289403 -4.4288235 -4.4286251 -4.4283867 -4.428154 -4.4280143 -4.42804 -4.428226 -4.4284592 -4.4286356 -4.4287472 -4.4288015 -4.4287934 -4.4287472][-4.4289322 -4.4289246 -4.4288187 -4.4286237 -4.4283843 -4.4281473 -4.4280219 -4.4280963 -4.42832 -4.4285369 -4.4286976 -4.4288096 -4.4288583 -4.4288535 -4.4288106][-4.4289126 -4.4289069 -4.4288373 -4.4286962 -4.42851 -4.42831 -4.4281893 -4.4282727 -4.42847 -4.4286361 -4.4287605 -4.428864 -4.4289088 -4.4288969 -4.4288549][-4.4288874 -4.428896 -4.4288726 -4.4287939 -4.42867 -4.4285212 -4.4284267 -4.428483 -4.4286089 -4.4287024 -4.4287791 -4.4288659 -4.4289083 -4.4288969 -4.4288669][-4.4288688 -4.4288926 -4.4288917 -4.4288435 -4.4287634 -4.4286642 -4.4285941 -4.42863 -4.4286952 -4.4287314 -4.4287786 -4.428843 -4.4288826 -4.428875 -4.4288549][-4.4288454 -4.4288797 -4.4288878 -4.4288578 -4.4287992 -4.4287329 -4.4286919 -4.428709 -4.428731 -4.4287477 -4.4287853 -4.4288259 -4.4288507 -4.4288497 -4.4288497][-4.4288383 -4.4288759 -4.4288845 -4.428854 -4.4288 -4.4287615 -4.4287481 -4.428741 -4.4287362 -4.4287558 -4.4287949 -4.4288163 -4.4288144 -4.4288082 -4.4288139][-4.4288187 -4.4288507 -4.4288592 -4.428833 -4.4287906 -4.4287763 -4.4287782 -4.4287682 -4.4287591 -4.4287767 -4.4288 -4.4288006 -4.4287829 -4.4287663 -4.42877]]...]
INFO - root - 2017-12-08 07:56:03.323937: step 48510, loss = 2.28, batch loss = 2.23 (35.6 examples/sec; 0.225 sec/batch; 17h:44m:00s remains)
INFO - root - 2017-12-08 07:56:05.561342: step 48520, loss = 2.28, batch loss = 2.23 (35.1 examples/sec; 0.228 sec/batch; 17h:58m:19s remains)
INFO - root - 2017-12-08 07:56:07.797268: step 48530, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 17h:16m:04s remains)
INFO - root - 2017-12-08 07:56:10.044303: step 48540, loss = 2.28, batch loss = 2.23 (36.0 examples/sec; 0.223 sec/batch; 17h:33m:03s remains)
INFO - root - 2017-12-08 07:56:12.272581: step 48550, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.221 sec/batch; 17h:23m:47s remains)
INFO - root - 2017-12-08 07:56:14.495009: step 48560, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 17h:16m:52s remains)
INFO - root - 2017-12-08 07:56:16.729927: step 48570, loss = 2.28, batch loss = 2.23 (37.2 examples/sec; 0.215 sec/batch; 16h:58m:42s remains)
INFO - root - 2017-12-08 07:56:18.962544: step 48580, loss = 2.28, batch loss = 2.23 (34.3 examples/sec; 0.233 sec/batch; 18h:23m:48s remains)
INFO - root - 2017-12-08 07:56:21.200036: step 48590, loss = 2.28, batch loss = 2.23 (34.2 examples/sec; 0.234 sec/batch; 18h:25m:34s remains)
INFO - root - 2017-12-08 07:56:23.409534: step 48600, loss = 2.28, batch loss = 2.23 (35.1 examples/sec; 0.228 sec/batch; 17h:58m:45s remains)
2017-12-08 07:56:23.713762: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289994 -4.4289994 -4.4289923 -4.4289842 -4.4289765 -4.4289684 -4.4289579 -4.42895 -4.428956 -4.428957 -4.4289603 -4.4289579 -4.4289441 -4.4289422 -4.4289517][-4.4289923 -4.4289937 -4.428987 -4.4289804 -4.4289742 -4.4289675 -4.4289603 -4.4289551 -4.4289632 -4.428966 -4.4289665 -4.4289594 -4.4289408 -4.4289327 -4.4289412][-4.4289775 -4.4289813 -4.428978 -4.4289746 -4.4289708 -4.4289641 -4.4289565 -4.428947 -4.4289489 -4.42895 -4.428946 -4.428937 -4.4289184 -4.4289079 -4.4289184][-4.428967 -4.4289718 -4.4289689 -4.4289651 -4.4289565 -4.4289393 -4.4289188 -4.4288893 -4.4288735 -4.4288692 -4.428863 -4.4288588 -4.4288549 -4.4288549 -4.4288778][-4.4289522 -4.4289503 -4.4289379 -4.4289217 -4.4288921 -4.42885 -4.4288077 -4.4287524 -4.4287181 -4.4287167 -4.428721 -4.4287376 -4.4287624 -4.4287853 -4.4288259][-4.4289055 -4.4288812 -4.42884 -4.428792 -4.4287238 -4.4286485 -4.4285922 -4.4285221 -4.4284825 -4.4285045 -4.4285436 -4.4286013 -4.4286675 -4.4287181 -4.4287744][-4.4287944 -4.4287305 -4.4286408 -4.4285517 -4.428452 -4.4283619 -4.4283257 -4.428277 -4.4282632 -4.4283328 -4.4284234 -4.4285293 -4.42863 -4.4286971 -4.4287529][-4.4286408 -4.428544 -4.4284263 -4.4283333 -4.4282455 -4.4281836 -4.4281993 -4.4281969 -4.428226 -4.4283309 -4.4284492 -4.4285736 -4.4286766 -4.4287343 -4.4287739][-4.4285564 -4.42847 -4.4283805 -4.428339 -4.4283094 -4.4282913 -4.42833 -4.42835 -4.4283953 -4.428493 -4.4285936 -4.4286981 -4.4287729 -4.4288058 -4.4288239][-4.4285994 -4.4285564 -4.42852 -4.4285312 -4.4285464 -4.42855 -4.4285765 -4.4285903 -4.42863 -4.4287033 -4.4287715 -4.4288397 -4.4288836 -4.4288931 -4.4288912][-4.4287233 -4.42872 -4.4287205 -4.4287481 -4.4287691 -4.4287734 -4.4287858 -4.4287925 -4.4288244 -4.4288759 -4.4289188 -4.4289565 -4.4289756 -4.428968 -4.4289503][-4.4288173 -4.4288282 -4.4288392 -4.4288621 -4.4288778 -4.4288783 -4.428884 -4.4288931 -4.4289227 -4.4289618 -4.4289918 -4.4290137 -4.4290209 -4.4290056 -4.4289818][-4.4288387 -4.4288468 -4.4288487 -4.4288597 -4.4288759 -4.4288769 -4.4288845 -4.4289036 -4.428937 -4.4289737 -4.4290023 -4.4290209 -4.4290257 -4.4290128 -4.4289894][-4.4287767 -4.42878 -4.4287748 -4.428782 -4.4288092 -4.4288197 -4.4288354 -4.4288683 -4.4289131 -4.428957 -4.4289918 -4.4290118 -4.4290156 -4.4290051 -4.4289842][-4.428648 -4.4286489 -4.4286451 -4.4286628 -4.4287176 -4.4287481 -4.4287763 -4.428823 -4.4288778 -4.4289312 -4.4289732 -4.428997 -4.429 -4.4289894 -4.4289718]]...]
INFO - root - 2017-12-08 07:56:25.937418: step 48610, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 17h:26m:56s remains)
INFO - root - 2017-12-08 07:56:28.168770: step 48620, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 17h:26m:15s remains)
INFO - root - 2017-12-08 07:56:30.398489: step 48630, loss = 2.28, batch loss = 2.23 (35.7 examples/sec; 0.224 sec/batch; 17h:39m:26s remains)
INFO - root - 2017-12-08 07:56:32.705952: step 48640, loss = 2.28, batch loss = 2.23 (34.1 examples/sec; 0.235 sec/batch; 18h:30m:45s remains)
INFO - root - 2017-12-08 07:56:34.953061: step 48650, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 17h:26m:35s remains)
INFO - root - 2017-12-08 07:56:37.161878: step 48660, loss = 2.28, batch loss = 2.23 (37.2 examples/sec; 0.215 sec/batch; 16h:58m:28s remains)
INFO - root - 2017-12-08 07:56:39.456086: step 48670, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.220 sec/batch; 17h:22m:04s remains)
INFO - root - 2017-12-08 07:56:41.675283: step 48680, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 17h:34m:06s remains)
INFO - root - 2017-12-08 07:56:43.900124: step 48690, loss = 2.28, batch loss = 2.23 (37.0 examples/sec; 0.216 sec/batch; 17h:03m:58s remains)
INFO - root - 2017-12-08 07:56:46.143439: step 48700, loss = 2.28, batch loss = 2.23 (35.7 examples/sec; 0.224 sec/batch; 17h:41m:19s remains)
2017-12-08 07:56:46.440499: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.42873 -4.4287548 -4.4287796 -4.4287786 -4.4287634 -4.4287586 -4.4287577 -4.4287591 -4.4287534 -4.4287548 -4.42874 -4.4287 -4.4286551 -4.4286103 -4.4286008][-4.4286895 -4.4287162 -4.4287395 -4.4287391 -4.4287233 -4.4287143 -4.4287167 -4.4287276 -4.4287314 -4.428751 -4.4287539 -4.4287152 -4.4286566 -4.4285851 -4.4285574][-4.4286485 -4.428678 -4.4286995 -4.4286909 -4.428669 -4.428648 -4.4286561 -4.42868 -4.4287028 -4.4287448 -4.4287639 -4.4287281 -4.428658 -4.4285693 -4.4285312][-4.4286046 -4.4286356 -4.4286542 -4.4286337 -4.4286 -4.4285684 -4.4285774 -4.4286118 -4.4286594 -4.4287276 -4.4287629 -4.4287343 -4.4286613 -4.4285679 -4.4285321][-4.4285693 -4.4285932 -4.4285994 -4.4285655 -4.428515 -4.4284735 -4.42848 -4.4285131 -4.4285769 -4.4286733 -4.4287348 -4.4287276 -4.4286695 -4.428586 -4.4285574][-4.4285455 -4.4285541 -4.4285417 -4.4284978 -4.428431 -4.4283586 -4.4283252 -4.4283209 -4.4283843 -4.4285207 -4.4286361 -4.4286723 -4.42865 -4.4285941 -4.4285765][-4.4285407 -4.4285369 -4.4285154 -4.4284687 -4.428391 -4.4282756 -4.4281759 -4.4281 -4.4281263 -4.4282956 -4.4284744 -4.4285631 -4.4285803 -4.4285603 -4.4285617][-4.4285493 -4.4285512 -4.4285417 -4.4285049 -4.4284348 -4.4283218 -4.4282012 -4.4280796 -4.4280367 -4.4281654 -4.4283562 -4.4284673 -4.4285045 -4.4285049 -4.4285178][-4.4285393 -4.4285593 -4.4285827 -4.4285855 -4.4285541 -4.4284925 -4.4284148 -4.428308 -4.4282136 -4.4282432 -4.42836 -4.4284406 -4.4284682 -4.4284678 -4.4284787][-4.4285355 -4.428576 -4.4286289 -4.4286695 -4.4286785 -4.4286642 -4.4286275 -4.4285569 -4.4284611 -4.42842 -4.4284549 -4.42849 -4.4284883 -4.4284706 -4.4284658][-4.4285746 -4.4286175 -4.4286714 -4.4287229 -4.4287496 -4.4287548 -4.4287372 -4.4286957 -4.428628 -4.4285731 -4.4285593 -4.42856 -4.4285307 -4.4284997 -4.4284816][-4.4286132 -4.4286346 -4.4286604 -4.4286976 -4.4287343 -4.4287572 -4.4287672 -4.4287586 -4.4287281 -4.428688 -4.4286571 -4.4286318 -4.428586 -4.428544 -4.4285159][-4.4286332 -4.428616 -4.4286003 -4.428627 -4.4286861 -4.4287343 -4.4287658 -4.4287834 -4.4287825 -4.4287605 -4.4287329 -4.4286933 -4.4286375 -4.428586 -4.42855][-4.4286394 -4.4285855 -4.4285421 -4.428565 -4.4286466 -4.4287195 -4.42876 -4.42879 -4.4288063 -4.4288011 -4.4287853 -4.428741 -4.4286771 -4.4286146 -4.42857][-4.4286571 -4.4285827 -4.4285188 -4.4285283 -4.428607 -4.4286895 -4.4287376 -4.4287758 -4.4288092 -4.4288249 -4.4288149 -4.4287682 -4.4286985 -4.4286275 -4.4285784]]...]
INFO - root - 2017-12-08 07:56:48.659806: step 48710, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 17h:19m:04s remains)
INFO - root - 2017-12-08 07:56:50.912086: step 48720, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.220 sec/batch; 17h:21m:03s remains)
INFO - root - 2017-12-08 07:56:53.158580: step 48730, loss = 2.28, batch loss = 2.23 (33.8 examples/sec; 0.237 sec/batch; 18h:39m:48s remains)
INFO - root - 2017-12-08 07:56:55.400955: step 48740, loss = 2.28, batch loss = 2.23 (35.6 examples/sec; 0.225 sec/batch; 17h:41m:58s remains)
INFO - root - 2017-12-08 07:56:57.679040: step 48750, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 17h:34m:14s remains)
INFO - root - 2017-12-08 07:56:59.916169: step 48760, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.220 sec/batch; 17h:22m:34s remains)
INFO - root - 2017-12-08 07:57:02.130860: step 48770, loss = 2.28, batch loss = 2.23 (36.8 examples/sec; 0.217 sec/batch; 17h:08m:22s remains)
INFO - root - 2017-12-08 07:57:04.363416: step 48780, loss = 2.28, batch loss = 2.23 (36.8 examples/sec; 0.217 sec/batch; 17h:06m:53s remains)
INFO - root - 2017-12-08 07:57:06.611190: step 48790, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 17h:16m:43s remains)
INFO - root - 2017-12-08 07:57:08.871935: step 48800, loss = 2.28, batch loss = 2.23 (35.7 examples/sec; 0.224 sec/batch; 17h:41m:00s remains)
2017-12-08 07:57:09.174507: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289613 -4.4289694 -4.4289794 -4.4289851 -4.4289908 -4.4289751 -4.4289279 -4.428844 -4.4286971 -4.4284811 -4.42829 -4.4282641 -4.4283829 -4.4285431 -4.4287105][-4.4289417 -4.4289613 -4.4289775 -4.4289813 -4.4289789 -4.4289656 -4.4289389 -4.428894 -4.4288077 -4.4286804 -4.4285669 -4.4285374 -4.4285975 -4.4286933 -4.4288092][-4.4289179 -4.42895 -4.42897 -4.4289684 -4.4289556 -4.4289327 -4.4289136 -4.4288936 -4.4288535 -4.4287939 -4.4287467 -4.4287362 -4.4287724 -4.4288287 -4.4289026][-4.4289093 -4.4289474 -4.4289641 -4.4289503 -4.4289165 -4.428874 -4.4288483 -4.4288354 -4.4288249 -4.4288139 -4.4288139 -4.4288325 -4.4288688 -4.4289107 -4.4289603][-4.4289269 -4.4289632 -4.4289708 -4.4289374 -4.4288712 -4.428791 -4.4287291 -4.4286952 -4.4286971 -4.4287333 -4.4287844 -4.4288359 -4.4288926 -4.4289441 -4.4289861][-4.4289584 -4.4289856 -4.42898 -4.4289222 -4.4288173 -4.4286866 -4.4285583 -4.4284687 -4.4284692 -4.4285607 -4.4286742 -4.428771 -4.4288659 -4.4289384 -4.4289846][-4.4289827 -4.4289994 -4.4289794 -4.428896 -4.4287581 -4.4285765 -4.4283628 -4.428185 -4.4281754 -4.4283309 -4.4285231 -4.4286819 -4.4288187 -4.4289126 -4.4289656][-4.4289842 -4.4289875 -4.42896 -4.4288635 -4.4287109 -4.4285016 -4.4282308 -4.42798 -4.4279532 -4.42815 -4.4284029 -4.4286165 -4.4287868 -4.4288964 -4.4289532][-4.4289646 -4.4289613 -4.4289365 -4.4288511 -4.4287105 -4.4285121 -4.4282575 -4.4280329 -4.4280105 -4.4281783 -4.4284115 -4.4286289 -4.428802 -4.4289088 -4.4289589][-4.4289365 -4.4289393 -4.428926 -4.4288688 -4.428762 -4.4286013 -4.4283991 -4.4282393 -4.4282246 -4.4283371 -4.4285173 -4.4287 -4.4288487 -4.428936 -4.4289727][-4.428916 -4.4289327 -4.4289341 -4.428905 -4.4288373 -4.4287214 -4.4285731 -4.428463 -4.4284453 -4.4285111 -4.428637 -4.4287777 -4.428895 -4.4289603 -4.4289851][-4.4289126 -4.4289389 -4.4289541 -4.4289465 -4.4289112 -4.4288425 -4.4287491 -4.4286761 -4.4286556 -4.4286866 -4.4287648 -4.4288621 -4.4289412 -4.4289794 -4.428988][-4.4289303 -4.4289541 -4.4289689 -4.4289703 -4.4289579 -4.4289284 -4.42888 -4.4288387 -4.42882 -4.4288316 -4.4288759 -4.4289331 -4.4289737 -4.4289851 -4.4289789][-4.4289603 -4.4289732 -4.4289789 -4.428978 -4.4289742 -4.4289651 -4.4289451 -4.428926 -4.4289145 -4.4289193 -4.4289436 -4.4289751 -4.4289889 -4.4289808 -4.4289656][-4.4289827 -4.4289851 -4.4289813 -4.4289765 -4.4289751 -4.4289761 -4.4289713 -4.4289637 -4.4289575 -4.4289603 -4.4289727 -4.428988 -4.428988 -4.4289732 -4.4289579]]...]
INFO - root - 2017-12-08 07:57:11.422443: step 48810, loss = 2.28, batch loss = 2.23 (36.0 examples/sec; 0.222 sec/batch; 17h:30m:02s remains)
INFO - root - 2017-12-08 07:57:13.785082: step 48820, loss = 2.28, batch loss = 2.23 (35.1 examples/sec; 0.228 sec/batch; 17h:58m:48s remains)
INFO - root - 2017-12-08 07:57:16.031412: step 48830, loss = 2.28, batch loss = 2.23 (34.6 examples/sec; 0.231 sec/batch; 18h:13m:47s remains)
INFO - root - 2017-12-08 07:57:18.249060: step 48840, loss = 2.28, batch loss = 2.23 (35.6 examples/sec; 0.225 sec/batch; 17h:43m:33s remains)
INFO - root - 2017-12-08 07:57:20.476557: step 48850, loss = 2.28, batch loss = 2.23 (38.0 examples/sec; 0.210 sec/batch; 16h:34m:41s remains)
INFO - root - 2017-12-08 07:57:22.689567: step 48860, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.220 sec/batch; 17h:20m:40s remains)
INFO - root - 2017-12-08 07:57:24.910248: step 48870, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.220 sec/batch; 17h:20m:58s remains)
INFO - root - 2017-12-08 07:57:27.157597: step 48880, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 17h:33m:05s remains)
INFO - root - 2017-12-08 07:57:29.439672: step 48890, loss = 2.28, batch loss = 2.23 (35.4 examples/sec; 0.226 sec/batch; 17h:49m:24s remains)
INFO - root - 2017-12-08 07:57:31.685508: step 48900, loss = 2.28, batch loss = 2.23 (37.0 examples/sec; 0.216 sec/batch; 17h:01m:43s remains)
2017-12-08 07:57:31.982672: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4286704 -4.4286623 -4.4286995 -4.428761 -4.42881 -4.4288378 -4.4288549 -4.428854 -4.4288354 -4.4288087 -4.4287868 -4.428772 -4.4287748 -4.4287829 -4.428771][-4.4286594 -4.428659 -4.4286971 -4.4287496 -4.4287887 -4.4288087 -4.4288297 -4.4288435 -4.4288445 -4.4288378 -4.4288297 -4.4288206 -4.4288149 -4.4287963 -4.4287519][-4.4286504 -4.4286585 -4.4286928 -4.4287295 -4.4287519 -4.42876 -4.4287724 -4.42879 -4.4288077 -4.4288268 -4.4288411 -4.4288454 -4.4288344 -4.4287953 -4.4287286][-4.428679 -4.4286823 -4.4286938 -4.4287038 -4.4287033 -4.4286914 -4.4286842 -4.4286923 -4.428721 -4.4287682 -4.428812 -4.4288383 -4.4288373 -4.428802 -4.4287386][-4.428719 -4.4287124 -4.4286981 -4.4286757 -4.4286408 -4.4285941 -4.4285517 -4.4285388 -4.4285789 -4.4286613 -4.428741 -4.4287968 -4.4288173 -4.4288 -4.4287529][-4.4287505 -4.42874 -4.4287052 -4.4286485 -4.4285731 -4.428484 -4.4283929 -4.4283452 -4.428391 -4.4285116 -4.4286289 -4.4287114 -4.4287548 -4.4287596 -4.4287353][-4.42878 -4.4287744 -4.4287305 -4.4286509 -4.4285479 -4.4284272 -4.4282932 -4.4282093 -4.4282565 -4.4284039 -4.4285464 -4.4286442 -4.4287004 -4.428719 -4.4287181][-4.4288125 -4.4288154 -4.4287777 -4.4287019 -4.4285994 -4.4284792 -4.4283404 -4.428246 -4.4282851 -4.4284191 -4.4285512 -4.4286404 -4.4286904 -4.42871 -4.4287233][-4.4288378 -4.4288392 -4.4288111 -4.4287505 -4.4286695 -4.4285769 -4.4284716 -4.4284019 -4.428432 -4.4285259 -4.4286189 -4.4286819 -4.4287148 -4.42873 -4.4287515][-4.4288445 -4.428833 -4.4288034 -4.4287553 -4.428699 -4.4286413 -4.4285774 -4.4285412 -4.4285665 -4.4286246 -4.42868 -4.4287181 -4.4287391 -4.4287553 -4.4287834][-4.4288421 -4.4288144 -4.4287777 -4.4287395 -4.428709 -4.428688 -4.4286633 -4.4286585 -4.4286804 -4.42871 -4.4287319 -4.4287434 -4.4287519 -4.4287658 -4.4287934][-4.4288278 -4.4287877 -4.4287453 -4.4287176 -4.4287128 -4.4287238 -4.4287319 -4.4287515 -4.4287705 -4.4287739 -4.4287624 -4.4287453 -4.4287362 -4.4287395 -4.4287629][-4.4288034 -4.4287705 -4.4287353 -4.4287195 -4.4287329 -4.4287629 -4.4287858 -4.42881 -4.42882 -4.428803 -4.4287658 -4.4287262 -4.4287014 -4.4286971 -4.4287162][-4.4287868 -4.4287748 -4.4287577 -4.4287519 -4.4287705 -4.4288015 -4.4288206 -4.428834 -4.4288316 -4.4288015 -4.4287505 -4.4287043 -4.4286766 -4.4286714 -4.4286866][-4.4287839 -4.4287925 -4.428793 -4.4287963 -4.4288154 -4.4288392 -4.42885 -4.4288483 -4.428833 -4.4287958 -4.4287457 -4.4287071 -4.428688 -4.4286876 -4.4287033]]...]
INFO - root - 2017-12-08 07:57:34.211897: step 48910, loss = 2.28, batch loss = 2.23 (35.7 examples/sec; 0.224 sec/batch; 17h:39m:48s remains)
INFO - root - 2017-12-08 07:57:36.456394: step 48920, loss = 2.28, batch loss = 2.23 (34.9 examples/sec; 0.229 sec/batch; 18h:03m:36s remains)
INFO - root - 2017-12-08 07:57:38.723936: step 48930, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 17h:16m:09s remains)
INFO - root - 2017-12-08 07:57:41.007619: step 48940, loss = 2.28, batch loss = 2.23 (35.2 examples/sec; 0.228 sec/batch; 17h:55m:24s remains)
INFO - root - 2017-12-08 07:57:43.281265: step 48950, loss = 2.28, batch loss = 2.23 (34.7 examples/sec; 0.231 sec/batch; 18h:09m:53s remains)
INFO - root - 2017-12-08 07:57:45.559658: step 48960, loss = 2.28, batch loss = 2.23 (34.7 examples/sec; 0.231 sec/batch; 18h:09m:21s remains)
INFO - root - 2017-12-08 07:57:47.859098: step 48970, loss = 2.28, batch loss = 2.23 (34.2 examples/sec; 0.234 sec/batch; 18h:24m:20s remains)
INFO - root - 2017-12-08 07:57:50.089085: step 48980, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 17h:24m:33s remains)
INFO - root - 2017-12-08 07:57:52.330845: step 48990, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 17h:23m:13s remains)
INFO - root - 2017-12-08 07:57:54.552082: step 49000, loss = 2.28, batch loss = 2.23 (36.7 examples/sec; 0.218 sec/batch; 17h:08m:43s remains)
2017-12-08 07:57:54.860451: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4290118 -4.4290037 -4.4289942 -4.4289823 -4.4289732 -4.4289708 -4.4289742 -4.4289818 -4.4289875 -4.4289913 -4.4289908 -4.4289865 -4.4289861 -4.4289908 -4.4290018][-4.4290032 -4.4289894 -4.4289756 -4.42896 -4.4289484 -4.4289441 -4.4289479 -4.428957 -4.4289618 -4.4289665 -4.4289675 -4.4289618 -4.4289579 -4.4289608 -4.4289765][-4.4289908 -4.4289708 -4.428946 -4.4289188 -4.4288974 -4.4288878 -4.4288888 -4.4288945 -4.428896 -4.4288993 -4.4289079 -4.4289083 -4.4289112 -4.42892 -4.4289427][-4.4289789 -4.4289451 -4.4289002 -4.4288521 -4.4288111 -4.4287887 -4.4287777 -4.4287744 -4.4287705 -4.4287777 -4.428803 -4.4288173 -4.4288325 -4.4288573 -4.428896][-4.4289675 -4.4289231 -4.4288616 -4.4287896 -4.428719 -4.4286652 -4.4286261 -4.4285922 -4.4285755 -4.4285984 -4.4286594 -4.4287 -4.4287324 -4.4287729 -4.4288278][-4.4289532 -4.428906 -4.4288363 -4.4287457 -4.42865 -4.4285517 -4.4284539 -4.4283485 -4.428288 -4.4283404 -4.4284739 -4.4285693 -4.4286304 -4.42869 -4.4287605][-4.4289436 -4.4289041 -4.4288368 -4.4287462 -4.4286418 -4.4285145 -4.4283605 -4.4281611 -4.4280124 -4.4280729 -4.4282746 -4.4284353 -4.4285359 -4.4286237 -4.4287128][-4.4289403 -4.4289169 -4.4288635 -4.4287896 -4.4287019 -4.4285927 -4.4284406 -4.4282212 -4.4280276 -4.428041 -4.4282207 -4.4283848 -4.4285 -4.4286056 -4.4287052][-4.4289246 -4.4289131 -4.4288769 -4.4288187 -4.4287477 -4.4286761 -4.4285727 -4.4284177 -4.4282694 -4.4282579 -4.4283614 -4.428473 -4.4285641 -4.4286571 -4.4287448][-4.4289126 -4.4289103 -4.4288864 -4.4288464 -4.42879 -4.4287491 -4.4286938 -4.4286041 -4.4285197 -4.4285135 -4.4285622 -4.4286218 -4.4286828 -4.4287529 -4.4288125][-4.4289088 -4.4289136 -4.4289026 -4.4288769 -4.4288368 -4.4288206 -4.4288 -4.4287567 -4.4287152 -4.4287143 -4.4287305 -4.4287515 -4.4287839 -4.4288297 -4.4288645][-4.4289088 -4.428916 -4.4289141 -4.4289002 -4.4288759 -4.428874 -4.4288707 -4.4288497 -4.4288268 -4.4288249 -4.4288268 -4.4288268 -4.42884 -4.4288678 -4.4288888][-4.4289136 -4.4289145 -4.4289174 -4.4289169 -4.4289155 -4.4289303 -4.4289351 -4.4289231 -4.4289041 -4.428895 -4.4288864 -4.4288774 -4.4288816 -4.4288974 -4.4289107][-4.4289289 -4.4289203 -4.42892 -4.428926 -4.4289408 -4.4289641 -4.4289737 -4.4289694 -4.4289579 -4.4289451 -4.4289308 -4.4289169 -4.428916 -4.4289274 -4.42894][-4.428957 -4.4289441 -4.4289379 -4.4289412 -4.4289565 -4.4289775 -4.42899 -4.4289932 -4.4289894 -4.4289804 -4.428967 -4.4289546 -4.4289527 -4.4289613 -4.4289713]]...]
INFO - root - 2017-12-08 07:57:57.071686: step 49010, loss = 2.28, batch loss = 2.23 (36.8 examples/sec; 0.217 sec/batch; 17h:06m:04s remains)
INFO - root - 2017-12-08 07:57:59.312186: step 49020, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 17h:31m:33s remains)
INFO - root - 2017-12-08 07:58:01.549971: step 49030, loss = 2.28, batch loss = 2.23 (35.4 examples/sec; 0.226 sec/batch; 17h:48m:50s remains)
INFO - root - 2017-12-08 07:58:03.807064: step 49040, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.221 sec/batch; 17h:22m:03s remains)
INFO - root - 2017-12-08 07:58:06.080225: step 49050, loss = 2.28, batch loss = 2.23 (36.6 examples/sec; 0.219 sec/batch; 17h:12m:48s remains)
INFO - root - 2017-12-08 07:58:08.328535: step 49060, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 17h:16m:36s remains)
INFO - root - 2017-12-08 07:58:10.547943: step 49070, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 17h:24m:09s remains)
INFO - root - 2017-12-08 07:58:12.782369: step 49080, loss = 2.28, batch loss = 2.23 (35.3 examples/sec; 0.226 sec/batch; 17h:49m:08s remains)
INFO - root - 2017-12-08 07:58:15.073282: step 49090, loss = 2.28, batch loss = 2.23 (34.1 examples/sec; 0.235 sec/batch; 18h:29m:07s remains)
INFO - root - 2017-12-08 07:58:17.305021: step 49100, loss = 2.28, batch loss = 2.23 (35.4 examples/sec; 0.226 sec/batch; 17h:46m:56s remains)
2017-12-08 07:58:17.592640: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289575 -4.4289508 -4.4289122 -4.428863 -4.4288259 -4.4287715 -4.4287529 -4.428782 -4.4287906 -4.4287934 -4.4288077 -4.4288392 -4.4288754 -4.4289031 -4.4289279][-4.4289694 -4.4289603 -4.428906 -4.4288383 -4.4287834 -4.4287062 -4.428668 -4.4286871 -4.4286861 -4.4286871 -4.42871 -4.4287572 -4.4288034 -4.4288363 -4.42887][-4.4289765 -4.4289632 -4.4288926 -4.428803 -4.4287205 -4.4286141 -4.428565 -4.4285884 -4.4285922 -4.4286013 -4.4286394 -4.4286966 -4.4287448 -4.4287744 -4.4288077][-4.4289746 -4.4289503 -4.4288659 -4.428762 -4.428658 -4.4285278 -4.4284711 -4.4285088 -4.4285326 -4.4285502 -4.4285936 -4.4286394 -4.4286761 -4.4286952 -4.4287348][-4.4289718 -4.428936 -4.4288397 -4.4287243 -4.4286051 -4.4284711 -4.4284229 -4.4284773 -4.4285188 -4.4285359 -4.4285626 -4.4285741 -4.4285736 -4.4285746 -4.4286375][-4.4289637 -4.4289165 -4.4288116 -4.4286823 -4.4285474 -4.428431 -4.4284143 -4.428494 -4.4285436 -4.4285522 -4.4285555 -4.4285164 -4.428462 -4.4284511 -4.4285579][-4.4289584 -4.4289069 -4.4287958 -4.4286509 -4.4284968 -4.4283895 -4.4283962 -4.4284983 -4.428556 -4.4285607 -4.4285517 -4.4284816 -4.4283891 -4.4283805 -4.4285288][-4.4289508 -4.4289017 -4.42879 -4.4286337 -4.4284558 -4.4283342 -4.4283395 -4.4284568 -4.4285278 -4.42854 -4.4285436 -4.4284844 -4.4283962 -4.4283967 -4.4285493][-4.4289455 -4.4289017 -4.428803 -4.4286494 -4.42846 -4.4283209 -4.428308 -4.4284196 -4.42849 -4.4285078 -4.42853 -4.428503 -4.4284425 -4.4284496 -4.4285746][-4.4289484 -4.4289165 -4.42884 -4.4287152 -4.4285526 -4.4284244 -4.4283891 -4.4284592 -4.42851 -4.4285269 -4.4285679 -4.4285736 -4.4285426 -4.4285569 -4.4286423][-4.42896 -4.4289389 -4.4288774 -4.4287782 -4.428659 -4.4285593 -4.4285212 -4.4285684 -4.4286132 -4.4286356 -4.428679 -4.4286947 -4.428679 -4.4286871 -4.4287357][-4.4289737 -4.4289622 -4.4289103 -4.4288254 -4.4287353 -4.4286609 -4.4286261 -4.428669 -4.42872 -4.4287419 -4.4287686 -4.42878 -4.4287739 -4.4287777 -4.4288011][-4.4289746 -4.428966 -4.4289179 -4.428843 -4.4287691 -4.4287066 -4.4286785 -4.4287314 -4.428793 -4.4288187 -4.428833 -4.4288406 -4.4288392 -4.4288387 -4.4288473][-4.428957 -4.4289417 -4.428895 -4.4288278 -4.4287643 -4.4287086 -4.4286833 -4.4287496 -4.4288216 -4.4288545 -4.4288654 -4.4288712 -4.4288669 -4.428863 -4.4288664][-4.4289412 -4.4289212 -4.4288697 -4.4287972 -4.4287353 -4.4286819 -4.4286566 -4.4287391 -4.4288239 -4.4288611 -4.428874 -4.4288797 -4.4288764 -4.4288731 -4.428875]]...]
INFO - root - 2017-12-08 07:58:19.847373: step 49110, loss = 2.28, batch loss = 2.23 (33.0 examples/sec; 0.242 sec/batch; 19h:03m:20s remains)
INFO - root - 2017-12-08 07:58:22.079834: step 49120, loss = 2.28, batch loss = 2.23 (36.0 examples/sec; 0.222 sec/batch; 17h:28m:16s remains)
INFO - root - 2017-12-08 07:58:24.314388: step 49130, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 17h:16m:04s remains)
INFO - root - 2017-12-08 07:58:26.546791: step 49140, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.220 sec/batch; 17h:20m:56s remains)
INFO - root - 2017-12-08 07:58:28.781286: step 49150, loss = 2.28, batch loss = 2.23 (36.0 examples/sec; 0.222 sec/batch; 17h:30m:38s remains)
INFO - root - 2017-12-08 07:58:31.031183: step 49160, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.221 sec/batch; 17h:22m:04s remains)
INFO - root - 2017-12-08 07:58:33.251702: step 49170, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 17h:23m:40s remains)
INFO - root - 2017-12-08 07:58:35.506681: step 49180, loss = 2.28, batch loss = 2.23 (35.4 examples/sec; 0.226 sec/batch; 17h:48m:18s remains)
INFO - root - 2017-12-08 07:58:37.735396: step 49190, loss = 2.28, batch loss = 2.23 (34.9 examples/sec; 0.229 sec/batch; 18h:03m:12s remains)
INFO - root - 2017-12-08 07:58:40.010890: step 49200, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 17h:23m:40s remains)
2017-12-08 07:58:40.311682: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289212 -4.4289284 -4.4289169 -4.4288945 -4.4288664 -4.4288449 -4.4288464 -4.4288669 -4.4288864 -4.4288974 -4.4289012 -4.4289021 -4.4289041 -4.428905 -4.4289036][-4.4289474 -4.4289465 -4.42892 -4.4288788 -4.4288325 -4.4287977 -4.4288 -4.4288378 -4.4288774 -4.4289026 -4.4289141 -4.4289207 -4.4289279 -4.4289346 -4.4289365][-4.42895 -4.4289341 -4.4288859 -4.4288173 -4.4287434 -4.4286861 -4.4286809 -4.4287338 -4.4288006 -4.4288468 -4.4288735 -4.4288955 -4.4289184 -4.4289374 -4.4289474][-4.4289351 -4.4289031 -4.4288306 -4.4287314 -4.428627 -4.4285421 -4.4285183 -4.4285755 -4.4286656 -4.4287348 -4.4287786 -4.4288249 -4.4288764 -4.4289174 -4.4289412][-4.428926 -4.428884 -4.4287953 -4.4286714 -4.4285364 -4.4284172 -4.4283557 -4.4283967 -4.4285049 -4.4285941 -4.4286528 -4.4287257 -4.4288182 -4.4288912 -4.4289355][-4.4289169 -4.4288735 -4.4287829 -4.4286466 -4.42849 -4.4283314 -4.4282088 -4.4282069 -4.4283276 -4.4284353 -4.4285121 -4.4286175 -4.4287548 -4.4288635 -4.4289308][-4.4289064 -4.4288654 -4.428782 -4.4286513 -4.4284906 -4.4282994 -4.4280996 -4.4280238 -4.4281478 -4.4282928 -4.4284081 -4.42855 -4.4287214 -4.4288511 -4.4289279][-4.4288936 -4.4288583 -4.4287848 -4.4286642 -4.4284992 -4.4282904 -4.4280434 -4.4279065 -4.4280305 -4.4282207 -4.4283776 -4.4285436 -4.4287291 -4.4288607 -4.4289331][-4.4288893 -4.4288611 -4.4287992 -4.4286938 -4.4285393 -4.4283495 -4.42813 -4.428 -4.4280939 -4.4282842 -4.4284492 -4.4286084 -4.4287786 -4.4288907 -4.4289451][-4.4289031 -4.4288788 -4.4288254 -4.4287367 -4.4286046 -4.428453 -4.4282947 -4.4281974 -4.4282594 -4.4284286 -4.4285831 -4.4287181 -4.4288468 -4.4289246 -4.4289575][-4.4289303 -4.4289112 -4.428863 -4.4287844 -4.4286695 -4.42855 -4.4284468 -4.4283881 -4.4284377 -4.4285855 -4.4287214 -4.4288249 -4.4289079 -4.4289494 -4.4289641][-4.4289474 -4.4289303 -4.4288912 -4.4288273 -4.4287343 -4.4286461 -4.4285893 -4.4285655 -4.4286046 -4.4287224 -4.4288349 -4.4289079 -4.4289503 -4.4289627 -4.4289646][-4.4289412 -4.42892 -4.4288874 -4.4288421 -4.4287834 -4.4287391 -4.4287195 -4.4287109 -4.4287348 -4.4288187 -4.4289069 -4.4289565 -4.4289718 -4.428966 -4.4289618][-4.4289131 -4.4288769 -4.4288421 -4.4288077 -4.4287767 -4.4287763 -4.4287925 -4.4287968 -4.4288135 -4.4288712 -4.4289351 -4.4289703 -4.4289756 -4.4289651 -4.4289589][-4.4288678 -4.4288058 -4.4287462 -4.4287043 -4.4286833 -4.4287152 -4.4287715 -4.4288034 -4.4288254 -4.4288721 -4.4289274 -4.4289622 -4.4289718 -4.4289637 -4.4289575]]...]
INFO - root - 2017-12-08 07:58:42.571601: step 49210, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 17h:22m:18s remains)
INFO - root - 2017-12-08 07:58:44.800256: step 49220, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.225 sec/batch; 17h:42m:33s remains)
INFO - root - 2017-12-08 07:58:47.033032: step 49230, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 17h:17m:15s remains)
INFO - root - 2017-12-08 07:58:49.308996: step 49240, loss = 2.28, batch loss = 2.23 (35.0 examples/sec; 0.229 sec/batch; 17h:59m:39s remains)
INFO - root - 2017-12-08 07:58:51.539268: step 49250, loss = 2.28, batch loss = 2.23 (35.3 examples/sec; 0.226 sec/batch; 17h:48m:46s remains)
INFO - root - 2017-12-08 07:58:53.769639: step 49260, loss = 2.28, batch loss = 2.23 (36.6 examples/sec; 0.219 sec/batch; 17h:11m:57s remains)
INFO - root - 2017-12-08 07:58:56.018420: step 49270, loss = 2.28, batch loss = 2.23 (35.2 examples/sec; 0.227 sec/batch; 17h:53m:16s remains)
INFO - root - 2017-12-08 07:58:58.282844: step 49280, loss = 2.28, batch loss = 2.23 (34.9 examples/sec; 0.229 sec/batch; 18h:00m:46s remains)
INFO - root - 2017-12-08 07:59:00.535000: step 49290, loss = 2.28, batch loss = 2.23 (34.8 examples/sec; 0.230 sec/batch; 18h:04m:40s remains)
INFO - root - 2017-12-08 07:59:02.820450: step 49300, loss = 2.28, batch loss = 2.23 (35.7 examples/sec; 0.224 sec/batch; 17h:39m:05s remains)
2017-12-08 07:59:03.127933: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4286866 -4.4286656 -4.4287138 -4.4288049 -4.4288626 -4.4288731 -4.428863 -4.428853 -4.4288616 -4.4288764 -4.4288635 -4.428813 -4.4287424 -4.4286532 -4.4285388][-4.4286976 -4.428699 -4.4287624 -4.4288387 -4.4288673 -4.428854 -4.4288235 -4.4287953 -4.4288039 -4.4288435 -4.4288511 -4.4288087 -4.4287372 -4.4286261 -4.4284744][-4.4287 -4.4287224 -4.428791 -4.4288435 -4.4288383 -4.4287958 -4.4287381 -4.4286842 -4.4286952 -4.4287763 -4.4288278 -4.4288158 -4.4287591 -4.4286518 -4.4284983][-4.4287181 -4.4287462 -4.4288015 -4.4288182 -4.4287772 -4.4287047 -4.4286151 -4.4285412 -4.4285655 -4.4286942 -4.4288011 -4.4288282 -4.4287968 -4.4287219 -4.4286079][-4.4287663 -4.4287848 -4.4288058 -4.4287739 -4.4286938 -4.4285855 -4.4284587 -4.4283729 -4.4284272 -4.4286122 -4.4287734 -4.4288421 -4.4288425 -4.4288054 -4.4287395][-4.4288282 -4.4288268 -4.4288087 -4.4287357 -4.4286251 -4.4284792 -4.428308 -4.4281926 -4.4282627 -4.4284954 -4.4287033 -4.4288163 -4.42886 -4.4288626 -4.4288392][-4.4288874 -4.4288793 -4.4288492 -4.4287677 -4.4286542 -4.4284925 -4.4282923 -4.4281335 -4.4281664 -4.4283819 -4.4285927 -4.4287324 -4.4288197 -4.4288664 -4.428885][-4.428937 -4.4289403 -4.4289212 -4.4288592 -4.4287643 -4.428617 -4.4284286 -4.4282527 -4.4282184 -4.4283419 -4.4285021 -4.4286427 -4.4287548 -4.4288344 -4.4288831][-4.4289608 -4.4289804 -4.4289823 -4.4289465 -4.4288778 -4.428761 -4.4285994 -4.4284315 -4.4283466 -4.4283667 -4.4284525 -4.4285789 -4.4287062 -4.4288039 -4.428863][-4.42894 -4.4289684 -4.428987 -4.428977 -4.4289322 -4.4288363 -4.4286933 -4.4285369 -4.4284372 -4.4284105 -4.428453 -4.428565 -4.4286923 -4.4287882 -4.4288406][-4.4288926 -4.4289203 -4.4289422 -4.4289422 -4.4289064 -4.4288225 -4.4286976 -4.4285679 -4.428493 -4.4284716 -4.4285097 -4.4286165 -4.4287295 -4.4288063 -4.4288373][-4.4288478 -4.4288645 -4.4288797 -4.4288812 -4.4288483 -4.4287796 -4.4286838 -4.4285989 -4.428566 -4.4285674 -4.4286103 -4.4287043 -4.4287949 -4.4288507 -4.4288611][-4.4288268 -4.4288325 -4.4288363 -4.4288325 -4.4288049 -4.4287596 -4.4287009 -4.42866 -4.428659 -4.4286737 -4.4287162 -4.4287896 -4.428853 -4.4288888 -4.42889][-4.4288445 -4.4288392 -4.4288321 -4.4288206 -4.4287977 -4.4287744 -4.4287472 -4.4287324 -4.42874 -4.4287534 -4.42879 -4.4288516 -4.4289007 -4.4289222 -4.4289207][-4.4288812 -4.4288707 -4.4288611 -4.4288507 -4.4288354 -4.4288244 -4.4288125 -4.428803 -4.4288044 -4.4288106 -4.4288383 -4.4288888 -4.4289327 -4.4289479 -4.4289446]]...]
INFO - root - 2017-12-08 07:59:05.357895: step 49310, loss = 2.28, batch loss = 2.23 (35.6 examples/sec; 0.225 sec/batch; 17h:40m:47s remains)
INFO - root - 2017-12-08 07:59:07.621428: step 49320, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.222 sec/batch; 17h:26m:03s remains)
INFO - root - 2017-12-08 07:59:09.848094: step 49330, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 17h:30m:32s remains)
INFO - root - 2017-12-08 07:59:12.084306: step 49340, loss = 2.28, batch loss = 2.23 (34.1 examples/sec; 0.235 sec/batch; 18h:27m:14s remains)
INFO - root - 2017-12-08 07:59:14.337600: step 49350, loss = 2.28, batch loss = 2.23 (37.1 examples/sec; 0.216 sec/batch; 16h:58m:08s remains)
INFO - root - 2017-12-08 07:59:16.559388: step 49360, loss = 2.28, batch loss = 2.23 (37.4 examples/sec; 0.214 sec/batch; 16h:50m:16s remains)
INFO - root - 2017-12-08 07:59:18.788653: step 49370, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.220 sec/batch; 17h:18m:32s remains)
INFO - root - 2017-12-08 07:59:21.100699: step 49380, loss = 2.28, batch loss = 2.23 (35.3 examples/sec; 0.227 sec/batch; 17h:49m:59s remains)
INFO - root - 2017-12-08 07:59:23.353421: step 49390, loss = 2.28, batch loss = 2.23 (35.8 examples/sec; 0.223 sec/batch; 17h:32m:57s remains)
INFO - root - 2017-12-08 07:59:25.593694: step 49400, loss = 2.28, batch loss = 2.23 (36.7 examples/sec; 0.218 sec/batch; 17h:08m:27s remains)
2017-12-08 07:59:25.870321: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288716 -4.4288721 -4.4288654 -4.4288611 -4.4288635 -4.4288692 -4.4288607 -4.4288349 -4.4287863 -4.4287395 -4.4287281 -4.42873 -4.42874 -4.428771 -4.4288158][-4.4288692 -4.4288678 -4.428863 -4.428864 -4.4288654 -4.4288611 -4.4288383 -4.4288068 -4.4287577 -4.4287157 -4.4287124 -4.4287186 -4.4287386 -4.428781 -4.4288321][-4.4288344 -4.4288282 -4.4288263 -4.4288316 -4.4288259 -4.4288068 -4.42877 -4.428741 -4.4287052 -4.4286733 -4.4286823 -4.4287043 -4.4287481 -4.4288011 -4.4288473][-4.4287591 -4.4287457 -4.4287486 -4.4287562 -4.4287386 -4.4286928 -4.4286318 -4.4286046 -4.4286056 -4.4286103 -4.4286408 -4.4286809 -4.4287405 -4.4288034 -4.4288473][-4.4286561 -4.428627 -4.4286327 -4.4286485 -4.4286227 -4.4285407 -4.4284306 -4.4283986 -4.428463 -4.4285307 -4.4285846 -4.4286304 -4.4286995 -4.42877 -4.4288206][-4.4285507 -4.4285026 -4.428515 -4.4285412 -4.4285088 -4.4283905 -4.4282074 -4.428154 -4.4283066 -4.4284577 -4.4285278 -4.4285603 -4.4286237 -4.4287043 -4.4287696][-4.4284267 -4.4283681 -4.4283705 -4.42839 -4.4283419 -4.4281697 -4.4278851 -4.4277868 -4.42804 -4.42829 -4.4283905 -4.4284267 -4.4285026 -4.428616 -4.4287171][-4.4283428 -4.4282928 -4.4282951 -4.4283228 -4.4282894 -4.4281154 -4.4278035 -4.4276805 -4.4279523 -4.4282303 -4.4283462 -4.428381 -4.4284554 -4.428586 -4.4287162][-4.4284172 -4.4283824 -4.4283967 -4.4284406 -4.4284382 -4.428328 -4.4281106 -4.4280133 -4.42819 -4.4283967 -4.4284873 -4.428503 -4.4285564 -4.4286747 -4.4287872][-4.4285269 -4.4284954 -4.4285107 -4.4285526 -4.4285636 -4.4285078 -4.4283805 -4.4283209 -4.4284258 -4.428566 -4.428637 -4.4286551 -4.4287028 -4.4287949 -4.42887][-4.4286156 -4.4285736 -4.428586 -4.4286232 -4.428648 -4.4286327 -4.428565 -4.428524 -4.4285793 -4.4286747 -4.4287314 -4.4287486 -4.428791 -4.4288626 -4.4289126][-4.4286771 -4.4286332 -4.4286366 -4.4286647 -4.4286966 -4.4286971 -4.4286485 -4.4285984 -4.428616 -4.4286833 -4.4287319 -4.4287443 -4.4287829 -4.4288507 -4.4289][-4.4287057 -4.4286742 -4.4286761 -4.4287 -4.4287333 -4.4287376 -4.4286866 -4.4286208 -4.42861 -4.4286528 -4.4286904 -4.4287014 -4.42873 -4.4287925 -4.42885][-4.4287252 -4.4287038 -4.4287057 -4.4287243 -4.4287553 -4.4287581 -4.4287019 -4.4286251 -4.4285946 -4.4286151 -4.428647 -4.4286618 -4.4286895 -4.4287524 -4.4288187][-4.4287553 -4.4287448 -4.4287496 -4.4287605 -4.4287877 -4.4287925 -4.4287376 -4.428658 -4.4286137 -4.4286213 -4.4286542 -4.428679 -4.4287128 -4.4287777 -4.4288435]]...]
INFO - root - 2017-12-08 07:59:28.117284: step 49410, loss = 2.28, batch loss = 2.23 (33.9 examples/sec; 0.236 sec/batch; 18h:33m:06s remains)
INFO - root - 2017-12-08 07:59:30.327146: step 49420, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 17h:22m:59s remains)
INFO - root - 2017-12-08 07:59:32.572578: step 49430, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 17h:14m:15s remains)
INFO - root - 2017-12-08 07:59:34.806526: step 49440, loss = 2.28, batch loss = 2.23 (36.0 examples/sec; 0.222 sec/batch; 17h:27m:45s remains)
INFO - root - 2017-12-08 07:59:37.025033: step 49450, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 17h:13m:51s remains)
INFO - root - 2017-12-08 07:59:39.271112: step 49460, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 17h:29m:53s remains)
INFO - root - 2017-12-08 07:59:41.553691: step 49470, loss = 2.28, batch loss = 2.23 (35.2 examples/sec; 0.227 sec/batch; 17h:52m:16s remains)
INFO - root - 2017-12-08 07:59:43.806348: step 49480, loss = 2.28, batch loss = 2.23 (35.2 examples/sec; 0.227 sec/batch; 17h:51m:43s remains)
INFO - root - 2017-12-08 07:59:46.027138: step 49490, loss = 2.28, batch loss = 2.23 (36.7 examples/sec; 0.218 sec/batch; 17h:07m:42s remains)
INFO - root - 2017-12-08 07:59:48.265735: step 49500, loss = 2.28, batch loss = 2.23 (36.8 examples/sec; 0.217 sec/batch; 17h:05m:33s remains)
2017-12-08 07:59:48.575076: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4287176 -4.4286952 -4.4286637 -4.428616 -4.4285731 -4.4285626 -4.4285707 -4.4285774 -4.4285936 -4.428616 -4.4286537 -4.4286995 -4.4287553 -4.4288259 -4.428894][-4.4286885 -4.4286647 -4.4286466 -4.428617 -4.4285765 -4.4285655 -4.4285722 -4.4285707 -4.428576 -4.4286008 -4.4286466 -4.4286933 -4.4287519 -4.4288254 -4.4288878][-4.4286242 -4.42861 -4.428617 -4.4286203 -4.4285979 -4.428597 -4.4286137 -4.428617 -4.4286132 -4.4286366 -4.4286785 -4.4287124 -4.428762 -4.4288287 -4.4288821][-4.4285789 -4.4285827 -4.4286122 -4.4286475 -4.4286518 -4.4286623 -4.428688 -4.4287019 -4.428699 -4.4287133 -4.428741 -4.4287581 -4.428791 -4.4288445 -4.4288836][-4.4285851 -4.4286022 -4.4286356 -4.4286766 -4.4286876 -4.4286966 -4.4287267 -4.4287548 -4.4287691 -4.4287882 -4.4288092 -4.4288177 -4.4288397 -4.4288793 -4.4289007][-4.4286208 -4.4286261 -4.4286375 -4.4286551 -4.4286518 -4.4286475 -4.4286618 -4.4286871 -4.4287243 -4.4287667 -4.4288025 -4.4288206 -4.4288497 -4.4288945 -4.4289112][-4.4286551 -4.4286261 -4.4286003 -4.4285784 -4.4285431 -4.428504 -4.4284816 -4.4284935 -4.4285564 -4.4286427 -4.4287119 -4.42876 -4.4288077 -4.4288683 -4.4288931][-4.4286895 -4.42863 -4.4285736 -4.4285212 -4.428453 -4.428369 -4.4282961 -4.4282727 -4.4283376 -4.4284649 -4.4285789 -4.4286656 -4.4287405 -4.4288206 -4.42886][-4.4287233 -4.4286566 -4.4285965 -4.4285407 -4.4284668 -4.4283657 -4.4282632 -4.4282026 -4.4282365 -4.4283566 -4.4284868 -4.428597 -4.4286952 -4.4287896 -4.42884][-4.4287663 -4.4287143 -4.4286718 -4.4286332 -4.4285817 -4.4285064 -4.4284258 -4.4283786 -4.4284 -4.428483 -4.4285736 -4.4286556 -4.4287329 -4.4288077 -4.4288473][-4.4287872 -4.4287581 -4.4287419 -4.4287286 -4.4287043 -4.4286609 -4.4286175 -4.428597 -4.4286151 -4.4286666 -4.4287186 -4.4287624 -4.4288044 -4.4288487 -4.4288712][-4.4288106 -4.4288039 -4.4288087 -4.428812 -4.4288011 -4.428771 -4.4287419 -4.4287252 -4.4287286 -4.4287553 -4.4287848 -4.428813 -4.4288416 -4.4288712 -4.428885][-4.4288359 -4.4288363 -4.4288459 -4.4288549 -4.4288535 -4.4288306 -4.4287996 -4.4287667 -4.4287543 -4.42877 -4.4287839 -4.428803 -4.4288311 -4.4288635 -4.4288816][-4.42882 -4.4288163 -4.4288197 -4.4288235 -4.4288235 -4.4288015 -4.4287639 -4.4287133 -4.4286952 -4.4287148 -4.42873 -4.4287543 -4.4287858 -4.4288273 -4.4288573][-4.4287615 -4.4287529 -4.4287524 -4.4287524 -4.4287491 -4.4287281 -4.4286895 -4.4286361 -4.4286213 -4.4286518 -4.428679 -4.4287109 -4.4287457 -4.4287968 -4.4288335]]...]
INFO - root - 2017-12-08 07:59:50.792017: step 49510, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 17h:14m:58s remains)
INFO - root - 2017-12-08 07:59:53.014758: step 49520, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.225 sec/batch; 17h:43m:31s remains)
INFO - root - 2017-12-08 07:59:55.249853: step 49530, loss = 2.28, batch loss = 2.23 (35.1 examples/sec; 0.228 sec/batch; 17h:55m:48s remains)
INFO - root - 2017-12-08 07:59:57.500105: step 49540, loss = 2.28, batch loss = 2.23 (34.8 examples/sec; 0.230 sec/batch; 18h:05m:14s remains)
INFO - root - 2017-12-08 07:59:59.747688: step 49550, loss = 2.28, batch loss = 2.23 (35.4 examples/sec; 0.226 sec/batch; 17h:46m:50s remains)
INFO - root - 2017-12-08 08:00:01.973302: step 49560, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.225 sec/batch; 17h:43m:19s remains)
INFO - root - 2017-12-08 08:00:04.201501: step 49570, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 17h:15m:10s remains)
INFO - root - 2017-12-08 08:00:06.467136: step 49580, loss = 2.28, batch loss = 2.23 (36.7 examples/sec; 0.218 sec/batch; 17h:06m:48s remains)
INFO - root - 2017-12-08 08:00:08.750938: step 49590, loss = 2.28, batch loss = 2.23 (35.2 examples/sec; 0.227 sec/batch; 17h:52m:38s remains)
INFO - root - 2017-12-08 08:00:10.994824: step 49600, loss = 2.28, batch loss = 2.23 (36.6 examples/sec; 0.219 sec/batch; 17h:11m:01s remains)
2017-12-08 08:00:11.275804: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4287062 -4.4287162 -4.4286847 -4.4285994 -4.4285393 -4.4285502 -4.4285822 -4.4286156 -4.4286623 -4.4287019 -4.4287429 -4.4287839 -4.4288034 -4.4288273 -4.4288578][-4.4286962 -4.4286919 -4.4286718 -4.4286137 -4.4285655 -4.4285741 -4.42861 -4.4286613 -4.4287271 -4.4287682 -4.4288034 -4.4288311 -4.4288325 -4.4288411 -4.4288607][-4.4286962 -4.428679 -4.4286637 -4.4286227 -4.4285851 -4.4286008 -4.4286456 -4.4287081 -4.4287796 -4.4288197 -4.4288425 -4.4288549 -4.4288459 -4.4288454 -4.4288621][-4.4286809 -4.4286618 -4.4286528 -4.4286146 -4.4285707 -4.4285879 -4.4286456 -4.4287109 -4.4287786 -4.4288144 -4.428833 -4.42884 -4.4288311 -4.4288363 -4.4288573][-4.4286532 -4.4286294 -4.428627 -4.428587 -4.428534 -4.4285412 -4.4285865 -4.4286346 -4.4286942 -4.4287248 -4.4287491 -4.4287691 -4.42878 -4.4288073 -4.42884][-4.4286284 -4.4286084 -4.4286051 -4.4285574 -4.4284949 -4.428484 -4.4284973 -4.4285192 -4.4285731 -4.4286103 -4.4286466 -4.428688 -4.4287295 -4.4287815 -4.4288249][-4.4286489 -4.4286222 -4.4286118 -4.4285545 -4.4284883 -4.4284573 -4.4284253 -4.42841 -4.4284577 -4.428514 -4.4285707 -4.4286308 -4.4287014 -4.4287753 -4.4288235][-4.4287167 -4.428688 -4.4286647 -4.4286032 -4.4285345 -4.4284835 -4.4284134 -4.4283638 -4.4284015 -4.4284792 -4.4285445 -4.4286089 -4.4286962 -4.428781 -4.4288311][-4.4288058 -4.4287777 -4.4287381 -4.4286785 -4.4286203 -4.4285693 -4.4284921 -4.428431 -4.428453 -4.4285221 -4.428575 -4.4286342 -4.4287224 -4.4287996 -4.4288416][-4.428875 -4.4288511 -4.4288034 -4.4287477 -4.4287014 -4.4286652 -4.4286027 -4.4285617 -4.4285827 -4.4286289 -4.4286556 -4.4287038 -4.4287782 -4.4288282 -4.428854][-4.4289107 -4.4289 -4.4288616 -4.4288125 -4.4287734 -4.4287457 -4.4287066 -4.4286904 -4.4287109 -4.4287367 -4.4287434 -4.4287767 -4.4288316 -4.4288559 -4.4288692][-4.4289107 -4.4289093 -4.4288931 -4.4288592 -4.4288297 -4.4288116 -4.428792 -4.4287953 -4.42881 -4.4288158 -4.4288106 -4.4288306 -4.428863 -4.4288726 -4.428884][-4.42891 -4.428915 -4.4289093 -4.4288921 -4.4288769 -4.4288697 -4.4288635 -4.4288764 -4.4288874 -4.4288778 -4.4288664 -4.4288754 -4.4288869 -4.4288883 -4.4289][-4.4289103 -4.4289165 -4.42891 -4.4289002 -4.4288974 -4.4288993 -4.4289031 -4.4289193 -4.42893 -4.4289203 -4.4289088 -4.4289079 -4.428906 -4.4289045 -4.428916][-4.4289074 -4.4289131 -4.4289074 -4.4289 -4.4288993 -4.4289021 -4.4289069 -4.4289203 -4.4289255 -4.4289227 -4.4289179 -4.4289141 -4.4289103 -4.4289131 -4.4289279]]...]
INFO - root - 2017-12-08 08:00:13.485848: step 49610, loss = 2.28, batch loss = 2.23 (34.8 examples/sec; 0.230 sec/batch; 18h:02m:29s remains)
INFO - root - 2017-12-08 08:00:15.737124: step 49620, loss = 2.28, batch loss = 2.23 (35.0 examples/sec; 0.228 sec/batch; 17h:56m:52s remains)
INFO - root - 2017-12-08 08:00:17.991696: step 49630, loss = 2.28, batch loss = 2.23 (35.7 examples/sec; 0.224 sec/batch; 17h:35m:55s remains)
INFO - root - 2017-12-08 08:00:20.235010: step 49640, loss = 2.28, batch loss = 2.23 (36.6 examples/sec; 0.219 sec/batch; 17h:10m:22s remains)
INFO - root - 2017-12-08 08:00:22.458201: step 49650, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 17h:29m:48s remains)
INFO - root - 2017-12-08 08:00:24.677561: step 49660, loss = 2.28, batch loss = 2.23 (35.8 examples/sec; 0.223 sec/batch; 17h:33m:25s remains)
INFO - root - 2017-12-08 08:00:26.909616: step 49670, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 17h:22m:33s remains)
INFO - root - 2017-12-08 08:00:29.148934: step 49680, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.225 sec/batch; 17h:41m:54s remains)
INFO - root - 2017-12-08 08:00:31.374924: step 49690, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 17h:14m:10s remains)
INFO - root - 2017-12-08 08:00:33.613039: step 49700, loss = 2.28, batch loss = 2.23 (35.1 examples/sec; 0.228 sec/batch; 17h:55m:29s remains)
2017-12-08 08:00:33.900812: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4286747 -4.4286132 -4.4285989 -4.4286165 -4.4286261 -4.4286075 -4.4285641 -4.4285631 -4.4286017 -4.4286618 -4.428731 -4.4288058 -4.4288754 -4.4289246 -4.42896][-4.4286389 -4.4285564 -4.4285431 -4.42858 -4.4286089 -4.4285984 -4.4285607 -4.4285769 -4.4286294 -4.428688 -4.4287505 -4.4288158 -4.4288778 -4.4289212 -4.4289546][-4.4286318 -4.4285359 -4.4285264 -4.4285731 -4.428606 -4.4285917 -4.4285579 -4.428587 -4.4286532 -4.4287205 -4.4287872 -4.4288483 -4.4288983 -4.42893 -4.4289546][-4.4286485 -4.4285626 -4.4285617 -4.428607 -4.4286318 -4.428606 -4.42857 -4.4285941 -4.4286652 -4.4287448 -4.42882 -4.4288788 -4.4289212 -4.4289432 -4.4289608][-4.42869 -4.4286227 -4.4286261 -4.4286613 -4.4286718 -4.4286394 -4.4285936 -4.4286003 -4.4286656 -4.4287562 -4.4288406 -4.4289021 -4.4289412 -4.4289584 -4.4289713][-4.42873 -4.4286819 -4.4286842 -4.4287076 -4.4287043 -4.4286675 -4.4286122 -4.428596 -4.4286547 -4.4287515 -4.4288487 -4.4289179 -4.428956 -4.4289718 -4.4289804][-4.428772 -4.4287314 -4.4287257 -4.4287338 -4.428719 -4.4286747 -4.4286094 -4.4285803 -4.4286394 -4.4287386 -4.4288383 -4.428915 -4.4289579 -4.4289775 -4.4289856][-4.4288073 -4.4287686 -4.4287562 -4.4287524 -4.42872 -4.4286656 -4.4285946 -4.4285674 -4.4286337 -4.4287314 -4.4288192 -4.4288955 -4.4289451 -4.4289727 -4.4289851][-4.428833 -4.4287996 -4.428781 -4.4287653 -4.4287124 -4.4286475 -4.4285817 -4.428566 -4.428638 -4.4287324 -4.4288044 -4.4288726 -4.4289274 -4.4289627 -4.4289808][-4.4288487 -4.4288182 -4.4287972 -4.4287777 -4.4287205 -4.4286551 -4.4286013 -4.4285984 -4.4286652 -4.4287467 -4.4288044 -4.4288621 -4.428916 -4.4289541 -4.4289765][-4.4288607 -4.4288311 -4.4288139 -4.4288034 -4.4287543 -4.4286914 -4.4286489 -4.4286561 -4.4287152 -4.4287791 -4.4288244 -4.4288735 -4.42892 -4.4289522 -4.4289751][-4.4288669 -4.4288425 -4.4288259 -4.4288173 -4.4287782 -4.4287291 -4.4287043 -4.4287252 -4.4287825 -4.4288349 -4.4288659 -4.4289007 -4.4289346 -4.4289589 -4.4289789][-4.428864 -4.4288445 -4.4288297 -4.4288144 -4.4287844 -4.4287548 -4.428751 -4.4287863 -4.4288435 -4.42889 -4.4289088 -4.4289317 -4.4289541 -4.4289703 -4.4289846][-4.4288788 -4.4288688 -4.4288568 -4.4288363 -4.4288068 -4.42879 -4.4287987 -4.4288321 -4.42888 -4.4289174 -4.428936 -4.4289575 -4.4289756 -4.4289851 -4.4289927][-4.4289236 -4.4289207 -4.4289112 -4.428885 -4.4288483 -4.4288287 -4.4288268 -4.4288483 -4.4288883 -4.4289255 -4.4289513 -4.428977 -4.4289942 -4.4289989 -4.4290013]]...]
INFO - root - 2017-12-08 08:00:36.142306: step 49710, loss = 2.28, batch loss = 2.23 (34.7 examples/sec; 0.231 sec/batch; 18h:07m:34s remains)
INFO - root - 2017-12-08 08:00:38.425994: step 49720, loss = 2.28, batch loss = 2.23 (33.4 examples/sec; 0.240 sec/batch; 18h:49m:42s remains)
INFO - root - 2017-12-08 08:00:40.642956: step 49730, loss = 2.28, batch loss = 2.23 (37.2 examples/sec; 0.215 sec/batch; 16h:52m:33s remains)
INFO - root - 2017-12-08 08:00:42.895890: step 49740, loss = 2.28, batch loss = 2.23 (36.0 examples/sec; 0.222 sec/batch; 17h:28m:05s remains)
INFO - root - 2017-12-08 08:00:45.177539: step 49750, loss = 2.28, batch loss = 2.23 (35.4 examples/sec; 0.226 sec/batch; 17h:44m:25s remains)
INFO - root - 2017-12-08 08:00:47.424496: step 49760, loss = 2.28, batch loss = 2.23 (33.1 examples/sec; 0.242 sec/batch; 18h:59m:07s remains)
INFO - root - 2017-12-08 08:00:49.645235: step 49770, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 17h:16m:49s remains)
INFO - root - 2017-12-08 08:00:51.892303: step 49780, loss = 2.28, batch loss = 2.23 (36.7 examples/sec; 0.218 sec/batch; 17h:05m:49s remains)
INFO - root - 2017-12-08 08:00:54.144572: step 49790, loss = 2.28, batch loss = 2.23 (37.2 examples/sec; 0.215 sec/batch; 16h:53m:44s remains)
INFO - root - 2017-12-08 08:00:56.380189: step 49800, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.226 sec/batch; 17h:42m:51s remains)
2017-12-08 08:00:56.679651: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288831 -4.428895 -4.4289136 -4.4289317 -4.4289436 -4.4289465 -4.4289365 -4.4289126 -4.4288921 -4.4288831 -4.4288898 -4.4289064 -4.4289193 -4.4289284 -4.4289279][-4.4289227 -4.428937 -4.4289536 -4.428966 -4.428967 -4.4289546 -4.4289289 -4.4288955 -4.4288716 -4.4288597 -4.4288712 -4.4288983 -4.428926 -4.428947 -4.4289551][-4.4289403 -4.4289565 -4.4289665 -4.428967 -4.4289494 -4.4289103 -4.4288573 -4.4288044 -4.428771 -4.4287572 -4.4287806 -4.4288311 -4.428885 -4.4289293 -4.4289546][-4.4289088 -4.428925 -4.4289312 -4.4289203 -4.4288836 -4.4288187 -4.4287381 -4.4286609 -4.4286089 -4.4285889 -4.4286251 -4.4287009 -4.4287815 -4.4288483 -4.42889][-4.4288454 -4.4288588 -4.4288621 -4.428844 -4.4287949 -4.4287124 -4.4286122 -4.4285192 -4.428453 -4.4284253 -4.42847 -4.42856 -4.4286509 -4.4287248 -4.4287739][-4.4287767 -4.4287877 -4.4287915 -4.428772 -4.4287186 -4.4286313 -4.4285293 -4.4284372 -4.4283733 -4.4283543 -4.4284029 -4.4284835 -4.4285569 -4.4286146 -4.4286542][-4.4287262 -4.4287367 -4.4287453 -4.4287319 -4.428689 -4.4286156 -4.4285312 -4.4284577 -4.4284105 -4.4284105 -4.4284525 -4.4285045 -4.4285431 -4.4285665 -4.4285774][-4.4287291 -4.4287395 -4.4287505 -4.4287443 -4.4287171 -4.4286685 -4.4286137 -4.4285669 -4.4285407 -4.4285526 -4.4285822 -4.428607 -4.4286165 -4.4286079 -4.42859][-4.428772 -4.4287772 -4.4287834 -4.42878 -4.4287677 -4.4287448 -4.4287186 -4.4286952 -4.4286814 -4.4286909 -4.4287047 -4.42871 -4.4287024 -4.4286785 -4.4286489][-4.4288187 -4.4288154 -4.4288144 -4.42881 -4.4288054 -4.4288011 -4.4287925 -4.4287815 -4.4287748 -4.4287834 -4.42879 -4.4287868 -4.428772 -4.4287438 -4.4287148][-4.4288564 -4.4288473 -4.42884 -4.4288354 -4.428833 -4.428834 -4.4288297 -4.428822 -4.42882 -4.4288306 -4.4288406 -4.4288387 -4.4288239 -4.4287992 -4.4287753][-4.4288907 -4.4288793 -4.42887 -4.4288635 -4.4288597 -4.4288607 -4.4288564 -4.4288483 -4.4288464 -4.4288588 -4.4288731 -4.4288769 -4.4288683 -4.4288511 -4.4288349][-4.4289155 -4.4289064 -4.4289 -4.428895 -4.4288917 -4.4288917 -4.4288864 -4.4288774 -4.428874 -4.4288831 -4.4288974 -4.4289041 -4.4289007 -4.42889 -4.4288816][-4.4289241 -4.4289165 -4.4289122 -4.42891 -4.42891 -4.4289093 -4.4289012 -4.4288888 -4.4288816 -4.4288859 -4.428895 -4.4288993 -4.4288979 -4.4288936 -4.428894][-4.42893 -4.4289236 -4.42892 -4.42892 -4.4289231 -4.4289222 -4.4289126 -4.4288964 -4.4288831 -4.4288797 -4.4288807 -4.4288812 -4.42888 -4.4288816 -4.4288912]]...]
INFO - root - 2017-12-08 08:00:58.926274: step 49810, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 17h:13m:58s remains)
INFO - root - 2017-12-08 08:01:01.175038: step 49820, loss = 2.28, batch loss = 2.23 (35.4 examples/sec; 0.226 sec/batch; 17h:44m:01s remains)
INFO - root - 2017-12-08 08:01:03.393331: step 49830, loss = 2.28, batch loss = 2.23 (34.4 examples/sec; 0.232 sec/batch; 18h:14m:05s remains)
INFO - root - 2017-12-08 08:01:05.632851: step 49840, loss = 2.28, batch loss = 2.23 (35.0 examples/sec; 0.228 sec/batch; 17h:55m:17s remains)
INFO - root - 2017-12-08 08:01:07.854110: step 49850, loss = 2.28, batch loss = 2.23 (34.4 examples/sec; 0.232 sec/batch; 18h:15m:09s remains)
INFO - root - 2017-12-08 08:01:10.085785: step 49860, loss = 2.28, batch loss = 2.23 (36.7 examples/sec; 0.218 sec/batch; 17h:07m:28s remains)
INFO - root - 2017-12-08 08:01:12.317079: step 49870, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 17h:13m:47s remains)
INFO - root - 2017-12-08 08:01:14.574295: step 49880, loss = 2.28, batch loss = 2.23 (35.6 examples/sec; 0.225 sec/batch; 17h:39m:58s remains)
INFO - root - 2017-12-08 08:01:16.813937: step 49890, loss = 2.28, batch loss = 2.23 (35.0 examples/sec; 0.228 sec/batch; 17h:55m:11s remains)
INFO - root - 2017-12-08 08:01:19.037251: step 49900, loss = 2.28, batch loss = 2.23 (36.0 examples/sec; 0.222 sec/batch; 17h:27m:13s remains)
2017-12-08 08:01:19.333239: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4286337 -4.4286366 -4.4286342 -4.4286542 -4.4286866 -4.4287362 -4.4287949 -4.42883 -4.428834 -4.4287806 -4.4286766 -4.4285617 -4.4284854 -4.4284458 -4.4284406][-4.4286504 -4.4286604 -4.4286976 -4.428761 -4.4288168 -4.4288492 -4.428865 -4.4288626 -4.428834 -4.4287486 -4.4286308 -4.4285312 -4.4284658 -4.4284458 -4.4284692][-4.4286532 -4.4286718 -4.4287438 -4.4288392 -4.4289045 -4.4289184 -4.4288979 -4.4288535 -4.4287891 -4.4286823 -4.4285707 -4.4284954 -4.4284515 -4.4284477 -4.4284968][-4.42867 -4.4287076 -4.4287996 -4.4289036 -4.4289608 -4.4289484 -4.4288859 -4.4287972 -4.4287009 -4.4286041 -4.4285336 -4.4284925 -4.4284668 -4.4284744 -4.4285393][-4.428709 -4.4287815 -4.4288826 -4.4289684 -4.428988 -4.4289284 -4.42881 -4.4286757 -4.4285727 -4.4285336 -4.428525 -4.4285235 -4.4285135 -4.4285278 -4.4285932][-4.4287744 -4.4288754 -4.4289656 -4.4290047 -4.4289575 -4.4288235 -4.4286261 -4.4284463 -4.42838 -4.428442 -4.4285231 -4.42858 -4.4286022 -4.4286246 -4.4286675][-4.4288468 -4.4289517 -4.4290123 -4.4289956 -4.4288735 -4.4286456 -4.4283433 -4.4281149 -4.4281311 -4.4283309 -4.4285197 -4.42864 -4.4286985 -4.42872 -4.4287291][-4.428905 -4.4289985 -4.4290247 -4.4289541 -4.428762 -4.4284482 -4.4280553 -4.4278054 -4.427948 -4.4282794 -4.4285531 -4.4287124 -4.4287796 -4.4287863 -4.4287696][-4.4289274 -4.429 -4.4289961 -4.4288917 -4.4286671 -4.4283323 -4.4279604 -4.4278007 -4.4280281 -4.4283776 -4.4286461 -4.4287939 -4.4288478 -4.4288392 -4.4288049][-4.4289131 -4.4289651 -4.4289503 -4.4288464 -4.4286437 -4.4283795 -4.4281425 -4.4281082 -4.42831 -4.4285674 -4.4287658 -4.4288769 -4.4289112 -4.4288926 -4.4288526][-4.4289031 -4.4289427 -4.4289331 -4.4288516 -4.4287047 -4.4285431 -4.428431 -4.4284592 -4.4286008 -4.4287558 -4.4288769 -4.4289427 -4.4289541 -4.42893 -4.4288931][-4.4289041 -4.4289331 -4.4289322 -4.4288826 -4.4287963 -4.4287124 -4.4286752 -4.4287224 -4.4288116 -4.4288926 -4.4289503 -4.4289742 -4.4289651 -4.4289393 -4.4289074][-4.4289079 -4.4289269 -4.4289322 -4.4289093 -4.4288688 -4.4288373 -4.4288416 -4.4288859 -4.4289322 -4.4289618 -4.4289737 -4.4289646 -4.4289432 -4.4289207 -4.4288979][-4.4289031 -4.4289141 -4.4289165 -4.42891 -4.4289002 -4.4289041 -4.4289274 -4.4289589 -4.4289761 -4.4289703 -4.4289436 -4.4289103 -4.4288921 -4.4288845 -4.4288774][-4.4288611 -4.4288616 -4.428853 -4.4288478 -4.4288568 -4.4288874 -4.4289289 -4.4289536 -4.4289503 -4.4289112 -4.4288516 -4.4288068 -4.4288011 -4.4288139 -4.4288225]]...]
INFO - root - 2017-12-08 08:01:21.571542: step 49910, loss = 2.28, batch loss = 2.23 (35.1 examples/sec; 0.228 sec/batch; 17h:54m:03s remains)
INFO - root - 2017-12-08 08:01:23.830420: step 49920, loss = 2.28, batch loss = 2.23 (36.0 examples/sec; 0.222 sec/batch; 17h:25m:30s remains)
INFO - root - 2017-12-08 08:01:26.078789: step 49930, loss = 2.28, batch loss = 2.23 (36.8 examples/sec; 0.218 sec/batch; 17h:04m:58s remains)
INFO - root - 2017-12-08 08:01:28.308186: step 49940, loss = 2.28, batch loss = 2.23 (36.6 examples/sec; 0.218 sec/batch; 17h:08m:29s remains)
INFO - root - 2017-12-08 08:01:30.540438: step 49950, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 17h:11m:59s remains)
INFO - root - 2017-12-08 08:01:32.764213: step 49960, loss = 2.28, batch loss = 2.23 (34.8 examples/sec; 0.230 sec/batch; 18h:03m:55s remains)
INFO - root - 2017-12-08 08:01:34.980589: step 49970, loss = 2.28, batch loss = 2.23 (35.4 examples/sec; 0.226 sec/batch; 17h:44m:39s remains)
INFO - root - 2017-12-08 08:01:37.266983: step 49980, loss = 2.28, batch loss = 2.23 (35.3 examples/sec; 0.227 sec/batch; 17h:47m:42s remains)
INFO - root - 2017-12-08 08:01:39.534306: step 49990, loss = 2.28, batch loss = 2.23 (34.8 examples/sec; 0.230 sec/batch; 18h:01m:39s remains)
INFO - root - 2017-12-08 08:01:41.769209: step 50000, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 17h:20m:05s remains)
2017-12-08 08:01:42.057212: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289966 -4.4290056 -4.4290094 -4.4290071 -4.4289923 -4.4289646 -4.4289312 -4.4289045 -4.428896 -4.4289136 -4.4289479 -4.4289761 -4.4289923 -4.4290032 -4.4290113][-4.4289589 -4.428967 -4.4289737 -4.4289727 -4.4289508 -4.4289055 -4.4288511 -4.4288096 -4.4288025 -4.4288349 -4.4288888 -4.4289308 -4.4289532 -4.4289665 -4.4289761][-4.4288907 -4.4289002 -4.4289141 -4.4289184 -4.4288921 -4.4288335 -4.4287605 -4.42871 -4.4287138 -4.4287691 -4.4288392 -4.4288859 -4.4289074 -4.4289174 -4.4289255][-4.4288011 -4.4288192 -4.4288459 -4.4288597 -4.4288344 -4.4287677 -4.4286785 -4.4286194 -4.42864 -4.4287238 -4.4288096 -4.4288578 -4.4288726 -4.4288731 -4.4288764][-4.4287038 -4.4287372 -4.4287815 -4.4288116 -4.428793 -4.4287162 -4.4286027 -4.4285212 -4.4285502 -4.4286675 -4.4287748 -4.4288287 -4.4288397 -4.4288335 -4.4288306][-4.4286108 -4.4286561 -4.4287186 -4.4287648 -4.4287505 -4.4286556 -4.4285026 -4.4283838 -4.4284158 -4.428566 -4.4287019 -4.4287705 -4.4287891 -4.428792 -4.428791][-4.428535 -4.4285836 -4.4286566 -4.4287143 -4.4286938 -4.4285731 -4.4283781 -4.4282246 -4.4282703 -4.4284563 -4.4286213 -4.4287062 -4.4287386 -4.4287567 -4.4287634][-4.428493 -4.42853 -4.4285989 -4.4286504 -4.4286151 -4.4284768 -4.4282646 -4.4281092 -4.4281774 -4.4283838 -4.4285583 -4.4286551 -4.4286981 -4.4287257 -4.4287376][-4.4285078 -4.4285278 -4.4285812 -4.4286132 -4.4285655 -4.4284329 -4.4282532 -4.4281387 -4.4282126 -4.4283972 -4.4285583 -4.428658 -4.4287062 -4.4287295 -4.4287367][-4.4285741 -4.42858 -4.4286175 -4.4286323 -4.4285817 -4.4284768 -4.4283533 -4.4282875 -4.4283533 -4.4284964 -4.428627 -4.428721 -4.4287696 -4.4287829 -4.4287782][-4.4286737 -4.4286709 -4.4287 -4.4287114 -4.4286752 -4.4286041 -4.4285254 -4.4284892 -4.4285407 -4.4286418 -4.428741 -4.4288192 -4.4288592 -4.42886 -4.4288435][-4.4287791 -4.428771 -4.428791 -4.4288034 -4.4287825 -4.4287353 -4.42868 -4.4286561 -4.4286947 -4.4287672 -4.4288406 -4.4289 -4.428926 -4.4289169 -4.4288931][-4.4288716 -4.42886 -4.4288692 -4.4288774 -4.428864 -4.4288306 -4.4287887 -4.4287705 -4.4287977 -4.4288459 -4.428894 -4.4289331 -4.4289494 -4.4289384 -4.4289184][-4.4289389 -4.4289255 -4.4289231 -4.428925 -4.4289174 -4.4288969 -4.42887 -4.4288549 -4.4288664 -4.4288917 -4.4289174 -4.4289389 -4.42895 -4.4289441 -4.4289355][-4.4289827 -4.4289732 -4.428966 -4.4289637 -4.42896 -4.4289517 -4.4289417 -4.4289341 -4.428937 -4.4289455 -4.4289551 -4.4289637 -4.4289684 -4.4289675 -4.428967]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-momentum-lr0.001-defgroup-1/model.ckpt-50000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-momentum-lr0.001-defgroup-1/model.ckpt-50000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-08 08:01:44.786982: step 50010, loss = 2.28, batch loss = 2.23 (35.2 examples/sec; 0.227 sec/batch; 17h:50m:45s remains)
INFO - root - 2017-12-08 08:01:47.011634: step 50020, loss = 2.28, batch loss = 2.23 (35.4 examples/sec; 0.226 sec/batch; 17h:44m:23s remains)
INFO - root - 2017-12-08 08:01:49.266905: step 50030, loss = 2.28, batch loss = 2.23 (35.1 examples/sec; 0.228 sec/batch; 17h:52m:16s remains)
INFO - root - 2017-12-08 08:01:51.497887: step 50040, loss = 2.28, batch loss = 2.23 (35.7 examples/sec; 0.224 sec/batch; 17h:33m:54s remains)
INFO - root - 2017-12-08 08:01:53.764839: step 50050, loss = 2.28, batch loss = 2.23 (34.8 examples/sec; 0.230 sec/batch; 18h:01m:11s remains)
INFO - root - 2017-12-08 08:01:56.024806: step 50060, loss = 2.28, batch loss = 2.23 (36.0 examples/sec; 0.222 sec/batch; 17h:26m:11s remains)
INFO - root - 2017-12-08 08:01:58.260126: step 50070, loss = 2.28, batch loss = 2.23 (34.2 examples/sec; 0.234 sec/batch; 18h:20m:45s remains)
INFO - root - 2017-12-08 08:02:00.486450: step 50080, loss = 2.28, batch loss = 2.23 (37.1 examples/sec; 0.216 sec/batch; 16h:55m:59s remains)
INFO - root - 2017-12-08 08:02:02.715036: step 50090, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.220 sec/batch; 17h:16m:11s remains)
INFO - root - 2017-12-08 08:02:04.951753: step 50100, loss = 2.28, batch loss = 2.23 (37.2 examples/sec; 0.215 sec/batch; 16h:53m:15s remains)
2017-12-08 08:02:05.250769: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4287119 -4.428721 -4.428771 -4.4288235 -4.4288287 -4.4287539 -4.4286337 -4.428546 -4.4285603 -4.4286413 -4.4287004 -4.4287238 -4.4287338 -4.4287362 -4.4287453][-4.4287171 -4.4287453 -4.4287982 -4.4288249 -4.4288077 -4.4287257 -4.4286103 -4.4285474 -4.4285736 -4.4286466 -4.4286895 -4.4287024 -4.4287162 -4.42873 -4.4287438][-4.4287186 -4.4287577 -4.428812 -4.4288263 -4.4287939 -4.4286995 -4.4285841 -4.4285326 -4.4285612 -4.428627 -4.4286671 -4.4286904 -4.4287181 -4.4287424 -4.4287586][-4.4287086 -4.4287596 -4.4288173 -4.428823 -4.4287829 -4.4286923 -4.4285903 -4.4285517 -4.4285846 -4.4286366 -4.4286647 -4.4286923 -4.42873 -4.4287567 -4.428772][-4.4286704 -4.4287324 -4.4287977 -4.4288077 -4.4287672 -4.4286928 -4.4286151 -4.4285984 -4.4286404 -4.4286847 -4.4287052 -4.4287281 -4.4287663 -4.428792 -4.4288068][-4.4286113 -4.4286871 -4.4287724 -4.4287925 -4.4287567 -4.4286876 -4.4286137 -4.4286017 -4.428658 -4.4287076 -4.4287395 -4.4287744 -4.4288125 -4.42883 -4.428834][-4.428524 -4.4286132 -4.4287305 -4.4287715 -4.4287362 -4.4286566 -4.4285717 -4.4285669 -4.428647 -4.4287138 -4.4287558 -4.4287925 -4.4288287 -4.42884 -4.4288368][-4.4284768 -4.4285741 -4.4287071 -4.4287624 -4.4287291 -4.4286366 -4.428544 -4.4285541 -4.428658 -4.4287333 -4.4287786 -4.4288054 -4.4288335 -4.4288478 -4.4288383][-4.4285522 -4.4286308 -4.4287367 -4.4287829 -4.42875 -4.4286571 -4.4285679 -4.4285836 -4.4286923 -4.4287677 -4.4288092 -4.4288249 -4.4288449 -4.4288597 -4.428853][-4.4286547 -4.4287057 -4.4287753 -4.4288125 -4.4287925 -4.42872 -4.4286447 -4.4286609 -4.4287472 -4.4288077 -4.428844 -4.428865 -4.4288788 -4.4288831 -4.4288831][-4.4287157 -4.4287477 -4.4287915 -4.4288282 -4.4288273 -4.4287724 -4.4287176 -4.428731 -4.4287944 -4.4288349 -4.4288554 -4.42888 -4.4288988 -4.4289074 -4.4289136][-4.4287663 -4.4287934 -4.4288087 -4.4288273 -4.4288359 -4.4287925 -4.4287467 -4.4287639 -4.4288106 -4.4288363 -4.4288387 -4.428864 -4.4288931 -4.4289188 -4.4289365][-4.4287887 -4.4288092 -4.4287958 -4.4287949 -4.4288116 -4.4287915 -4.4287567 -4.428771 -4.428803 -4.4288216 -4.4288249 -4.428853 -4.4288874 -4.4289222 -4.42895][-4.4287791 -4.4287906 -4.4287663 -4.4287524 -4.4287853 -4.4287934 -4.4287791 -4.4287968 -4.4288125 -4.4288216 -4.4288278 -4.4288526 -4.42888 -4.4289126 -4.4289422][-4.4287825 -4.4287891 -4.4287629 -4.428751 -4.4287882 -4.4288073 -4.4288139 -4.4288368 -4.4288454 -4.4288464 -4.4288516 -4.4288659 -4.428884 -4.4289045 -4.4289269]]...]
INFO - root - 2017-12-08 08:02:07.494325: step 50110, loss = 2.28, batch loss = 2.23 (36.8 examples/sec; 0.218 sec/batch; 17h:03m:52s remains)
INFO - root - 2017-12-08 08:02:09.730684: step 50120, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 17h:13m:03s remains)
INFO - root - 2017-12-08 08:02:11.980919: step 50130, loss = 2.28, batch loss = 2.23 (35.6 examples/sec; 0.225 sec/batch; 17h:37m:05s remains)
INFO - root - 2017-12-08 08:02:14.221494: step 50140, loss = 2.28, batch loss = 2.23 (35.1 examples/sec; 0.228 sec/batch; 17h:52m:32s remains)
INFO - root - 2017-12-08 08:02:16.458171: step 50150, loss = 2.28, batch loss = 2.23 (34.9 examples/sec; 0.229 sec/batch; 17h:58m:15s remains)
INFO - root - 2017-12-08 08:02:18.686180: step 50160, loss = 2.28, batch loss = 2.23 (37.2 examples/sec; 0.215 sec/batch; 16h:53m:01s remains)
INFO - root - 2017-12-08 08:02:20.922803: step 50170, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 17h:15m:04s remains)
INFO - root - 2017-12-08 08:02:23.163262: step 50180, loss = 2.28, batch loss = 2.23 (34.6 examples/sec; 0.231 sec/batch; 18h:07m:08s remains)
INFO - root - 2017-12-08 08:02:25.414064: step 50190, loss = 2.28, batch loss = 2.23 (34.7 examples/sec; 0.230 sec/batch; 18h:03m:27s remains)
INFO - root - 2017-12-08 08:02:27.647666: step 50200, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.225 sec/batch; 17h:39m:36s remains)
2017-12-08 08:02:27.969865: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.42899 -4.4289789 -4.4289837 -4.4289985 -4.4290056 -4.4290013 -4.428988 -4.4289737 -4.4289622 -4.4289536 -4.4289422 -4.428926 -4.4289112 -4.4289012 -4.4289026][-4.4289937 -4.4289751 -4.4289827 -4.4290094 -4.4290209 -4.4290104 -4.4289823 -4.4289513 -4.4289279 -4.4289122 -4.4288874 -4.4288564 -4.4288363 -4.4288306 -4.428843][-4.4289789 -4.4289427 -4.4289412 -4.4289708 -4.4289908 -4.428987 -4.4289513 -4.4289083 -4.4288754 -4.4288611 -4.4288349 -4.4287987 -4.4287772 -4.4287724 -4.4287944][-4.42894 -4.4288859 -4.4288692 -4.4288921 -4.4289145 -4.428916 -4.4288759 -4.4288187 -4.4287829 -4.4287834 -4.42877 -4.4287419 -4.4287329 -4.4287338 -4.4287605][-4.4289036 -4.4288282 -4.4287868 -4.4287944 -4.4288092 -4.4288044 -4.4287534 -4.42868 -4.4286571 -4.4286885 -4.4286952 -4.428689 -4.4287062 -4.4287229 -4.4287543][-4.4288654 -4.4287663 -4.4286976 -4.4286804 -4.4286757 -4.4286509 -4.4285636 -4.4284682 -4.4284859 -4.42857 -4.4286013 -4.4286175 -4.4286642 -4.4287119 -4.4287629][-4.4287977 -4.4286761 -4.4285893 -4.4285517 -4.4285221 -4.4284549 -4.4283094 -4.428196 -4.4282818 -4.4284339 -4.4285016 -4.4285417 -4.4286089 -4.4286823 -4.4287562][-4.4287381 -4.428616 -4.4285331 -4.4285 -4.4284682 -4.4283853 -4.4282279 -4.4281349 -4.4282622 -4.42843 -4.4285049 -4.4285545 -4.4286227 -4.4286885 -4.42876][-4.4287462 -4.4286585 -4.4285975 -4.4285803 -4.4285693 -4.4285159 -4.428421 -4.428391 -4.4284821 -4.4285774 -4.4286056 -4.4286342 -4.4286847 -4.4287372 -4.428791][-4.4287777 -4.4287143 -4.4286637 -4.4286551 -4.4286575 -4.4286351 -4.4286 -4.4286079 -4.4286704 -4.4287138 -4.4287109 -4.4287157 -4.4287457 -4.4287868 -4.4288249][-4.428823 -4.4287791 -4.4287457 -4.428751 -4.4287658 -4.428761 -4.4287481 -4.428762 -4.4288011 -4.4288125 -4.4287872 -4.4287715 -4.4287863 -4.4288211 -4.4288516][-4.428874 -4.4288378 -4.4288177 -4.4288368 -4.4288564 -4.4288521 -4.4288464 -4.4288578 -4.4288793 -4.4288731 -4.4288359 -4.428822 -4.4288406 -4.4288731 -4.4288945][-4.4289174 -4.4288878 -4.4288754 -4.4288983 -4.428915 -4.4289074 -4.4289002 -4.4289083 -4.42892 -4.4289045 -4.4288673 -4.4288654 -4.4288931 -4.4289212 -4.4289355][-4.4289279 -4.4289055 -4.4288969 -4.4289131 -4.4289231 -4.428916 -4.4289083 -4.428915 -4.4289231 -4.428905 -4.4288754 -4.4288831 -4.4289141 -4.42894 -4.4289527][-4.428915 -4.4288979 -4.4288898 -4.4288979 -4.4289026 -4.4288979 -4.4288917 -4.4288983 -4.4289045 -4.4288888 -4.4288697 -4.4288788 -4.4289055 -4.4289289 -4.4289446]]...]
INFO - root - 2017-12-08 08:02:30.197666: step 50210, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 17h:27m:01s remains)
INFO - root - 2017-12-08 08:02:32.427459: step 50220, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.220 sec/batch; 17h:16m:25s remains)
INFO - root - 2017-12-08 08:02:34.665075: step 50230, loss = 2.28, batch loss = 2.23 (34.7 examples/sec; 0.231 sec/batch; 18h:05m:48s remains)
INFO - root - 2017-12-08 08:02:36.889877: step 50240, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 17h:13m:31s remains)
INFO - root - 2017-12-08 08:02:39.137792: step 50250, loss = 2.28, batch loss = 2.23 (37.1 examples/sec; 0.216 sec/batch; 16h:55m:37s remains)
INFO - root - 2017-12-08 08:02:41.363693: step 50260, loss = 2.28, batch loss = 2.23 (35.3 examples/sec; 0.227 sec/batch; 17h:47m:01s remains)
INFO - root - 2017-12-08 08:02:43.595172: step 50270, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 17h:10m:42s remains)
INFO - root - 2017-12-08 08:02:45.826236: step 50280, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.220 sec/batch; 17h:15m:37s remains)
INFO - root - 2017-12-08 08:02:48.054200: step 50290, loss = 2.28, batch loss = 2.23 (35.1 examples/sec; 0.228 sec/batch; 17h:52m:10s remains)
INFO - root - 2017-12-08 08:02:50.274289: step 50300, loss = 2.28, batch loss = 2.23 (34.6 examples/sec; 0.231 sec/batch; 18h:07m:32s remains)
2017-12-08 08:02:50.580904: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4287982 -4.4288197 -4.4288225 -4.4288106 -4.428792 -4.428782 -4.4287848 -4.4287963 -4.4288096 -4.4288149 -4.42881 -4.42881 -4.4288149 -4.4288173 -4.4287949][-4.4287477 -4.4287696 -4.4287639 -4.4287381 -4.4287133 -4.4287019 -4.4287009 -4.42871 -4.4287314 -4.4287539 -4.4287724 -4.4287872 -4.4287887 -4.428781 -4.4287481][-4.4286942 -4.4287138 -4.4287047 -4.4286747 -4.4286447 -4.4286256 -4.4286127 -4.4286151 -4.4286413 -4.4286828 -4.4287281 -4.4287586 -4.4287548 -4.4287391 -4.4287062][-4.4286685 -4.4286842 -4.4286742 -4.4286489 -4.428617 -4.4285793 -4.4285359 -4.4285126 -4.4285464 -4.428606 -4.4286709 -4.4287114 -4.4287109 -4.428699 -4.4286757][-4.4286551 -4.4286585 -4.4286442 -4.4286261 -4.4285831 -4.4285078 -4.4284081 -4.4283566 -4.4284158 -4.428514 -4.4286051 -4.4286571 -4.4286652 -4.4286661 -4.4286594][-4.4286156 -4.4286 -4.4285874 -4.4285717 -4.4285088 -4.4283729 -4.4281926 -4.4281139 -4.4282355 -4.42841 -4.4285436 -4.4286084 -4.4286222 -4.4286385 -4.4286442][-4.4285369 -4.428504 -4.42849 -4.4284606 -4.4283748 -4.428165 -4.4278841 -4.4277806 -4.4280229 -4.4283109 -4.4284949 -4.4285712 -4.4285975 -4.4286184 -4.4286222][-4.4285 -4.4284678 -4.4284449 -4.4283977 -4.4282808 -4.4280157 -4.4276795 -4.4275818 -4.4279289 -4.4282875 -4.4284906 -4.4285679 -4.4285884 -4.4286017 -4.4285979][-4.4285593 -4.4285421 -4.4285126 -4.4284563 -4.4283419 -4.4281073 -4.4278488 -4.4278097 -4.4280977 -4.42839 -4.4285464 -4.4286022 -4.4286079 -4.4286084 -4.4285951][-4.4286375 -4.4286427 -4.4286113 -4.4285588 -4.4284682 -4.428309 -4.4281673 -4.4281693 -4.4283495 -4.4285326 -4.4286332 -4.4286633 -4.4286575 -4.4286342 -4.4286017][-4.4286795 -4.4287 -4.4286776 -4.4286332 -4.4285774 -4.4284911 -4.4284229 -4.4284334 -4.4285355 -4.4286404 -4.4287062 -4.428719 -4.4287028 -4.4286633 -4.4286137][-4.4286876 -4.4287081 -4.4286947 -4.4286637 -4.4286356 -4.42859 -4.4285588 -4.4285655 -4.428618 -4.4286842 -4.4287372 -4.4287434 -4.4287219 -4.4286861 -4.4286447][-4.4287324 -4.4287353 -4.4287143 -4.4286923 -4.4286733 -4.4286451 -4.428628 -4.4286318 -4.4286604 -4.428709 -4.428751 -4.4287534 -4.4287348 -4.4287119 -4.4286947][-4.428802 -4.4287853 -4.4287567 -4.4287395 -4.4287271 -4.4287071 -4.4286952 -4.4286962 -4.4287143 -4.4287505 -4.4287777 -4.4287853 -4.428782 -4.4287744 -4.4287682][-4.4288645 -4.42884 -4.4288168 -4.4288063 -4.4287992 -4.4287848 -4.4287753 -4.4287767 -4.428792 -4.4288173 -4.4288392 -4.4288583 -4.4288678 -4.428863 -4.4288511]]...]
INFO - root - 2017-12-08 08:02:52.824409: step 50310, loss = 2.28, batch loss = 2.23 (35.8 examples/sec; 0.223 sec/batch; 17h:29m:32s remains)
INFO - root - 2017-12-08 08:02:55.068091: step 50320, loss = 2.28, batch loss = 2.23 (34.9 examples/sec; 0.229 sec/batch; 17h:57m:00s remains)
INFO - root - 2017-12-08 08:02:57.318397: step 50330, loss = 2.28, batch loss = 2.23 (35.4 examples/sec; 0.226 sec/batch; 17h:43m:33s remains)
INFO - root - 2017-12-08 08:02:59.550220: step 50340, loss = 2.28, batch loss = 2.23 (36.9 examples/sec; 0.217 sec/batch; 17h:00m:02s remains)
INFO - root - 2017-12-08 08:03:01.784364: step 50350, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.222 sec/batch; 17h:22m:14s remains)
INFO - root - 2017-12-08 08:03:04.000522: step 50360, loss = 2.28, batch loss = 2.23 (36.8 examples/sec; 0.217 sec/batch; 17h:02m:36s remains)
INFO - root - 2017-12-08 08:03:06.262207: step 50370, loss = 2.28, batch loss = 2.23 (35.0 examples/sec; 0.229 sec/batch; 17h:55m:07s remains)
INFO - root - 2017-12-08 08:03:08.534978: step 50380, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.220 sec/batch; 17h:15m:28s remains)
INFO - root - 2017-12-08 08:03:10.762442: step 50390, loss = 2.28, batch loss = 2.23 (37.2 examples/sec; 0.215 sec/batch; 16h:50m:31s remains)
INFO - root - 2017-12-08 08:03:12.988329: step 50400, loss = 2.28, batch loss = 2.23 (36.6 examples/sec; 0.218 sec/batch; 17h:06m:45s remains)
2017-12-08 08:03:13.263717: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4286184 -4.4285889 -4.4285555 -4.4285779 -4.4286184 -4.4286685 -4.4287214 -4.4287639 -4.4287934 -4.4287996 -4.4287848 -4.4287605 -4.4287286 -4.428689 -4.4286671][-4.4286175 -4.4286184 -4.4286013 -4.4286337 -4.4286675 -4.4287081 -4.4287472 -4.42877 -4.4287786 -4.4287543 -4.4287066 -4.4286528 -4.4285965 -4.4285398 -4.4285111][-4.4286728 -4.428689 -4.428679 -4.4287081 -4.4287343 -4.4287653 -4.4287839 -4.4287777 -4.4287558 -4.4286957 -4.4286094 -4.428524 -4.4284625 -4.4284229 -4.4284048][-4.4287214 -4.4287419 -4.4287448 -4.42878 -4.4288034 -4.4288154 -4.428802 -4.4287672 -4.4287138 -4.4286261 -4.4285159 -4.4284263 -4.4283862 -4.4283834 -4.4283905][-4.4287477 -4.4287734 -4.4287882 -4.4288216 -4.4288363 -4.4288254 -4.428782 -4.42872 -4.4286489 -4.4285555 -4.4284515 -4.4283834 -4.4283714 -4.4283967 -4.4284291][-4.4287763 -4.4287877 -4.4287882 -4.4287949 -4.4287934 -4.4287648 -4.4286985 -4.4286175 -4.4285474 -4.4284682 -4.4284005 -4.4283681 -4.4283791 -4.4284263 -4.428483][-4.428803 -4.4287834 -4.4287486 -4.4287157 -4.4286952 -4.4286532 -4.4285769 -4.428493 -4.428442 -4.4283972 -4.4283886 -4.4283953 -4.4284163 -4.4284782 -4.4285555][-4.4288239 -4.4287734 -4.4287052 -4.4286408 -4.4286089 -4.4285784 -4.4285254 -4.4284635 -4.4284329 -4.428422 -4.4284525 -4.428473 -4.428494 -4.4285603 -4.4286451][-4.4288034 -4.4287438 -4.4286723 -4.4286041 -4.4285836 -4.428586 -4.4285779 -4.4285493 -4.4285297 -4.4285293 -4.4285593 -4.4285707 -4.4285893 -4.4286528 -4.42873][-4.4287558 -4.4287152 -4.4286723 -4.4286294 -4.4286342 -4.428668 -4.42869 -4.4286695 -4.4286389 -4.4286275 -4.42864 -4.4286475 -4.4286737 -4.4287343 -4.4287958][-4.4287267 -4.4287162 -4.4287019 -4.4286819 -4.4286957 -4.4287338 -4.4287581 -4.4287367 -4.4286838 -4.42865 -4.4286532 -4.428668 -4.4287105 -4.4287715 -4.4288187][-4.4287448 -4.4287486 -4.4287481 -4.428741 -4.4287524 -4.4287782 -4.4287877 -4.4287539 -4.4286752 -4.4286151 -4.4286141 -4.4286408 -4.4286857 -4.4287438 -4.4287896][-4.42881 -4.4288139 -4.4288149 -4.428813 -4.4288211 -4.4288311 -4.4288211 -4.4287677 -4.4286709 -4.4286065 -4.4286127 -4.4286456 -4.4286809 -4.4287324 -4.4287825][-4.428865 -4.4288664 -4.4288669 -4.4288626 -4.428863 -4.4288635 -4.4288492 -4.4288015 -4.4287195 -4.4286647 -4.4286714 -4.4286938 -4.4287076 -4.4287462 -4.4287915][-4.4288807 -4.4288797 -4.42888 -4.4288778 -4.4288797 -4.4288826 -4.4288735 -4.4288392 -4.4287786 -4.4287376 -4.42874 -4.4287453 -4.4287372 -4.42876 -4.4287982]]...]
INFO - root - 2017-12-08 08:03:15.509461: step 50410, loss = 2.28, batch loss = 2.23 (34.3 examples/sec; 0.233 sec/batch; 18h:15m:06s remains)
INFO - root - 2017-12-08 08:03:17.733326: step 50420, loss = 2.28, batch loss = 2.23 (35.4 examples/sec; 0.226 sec/batch; 17h:42m:39s remains)
INFO - root - 2017-12-08 08:03:20.007223: step 50430, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 17h:26m:11s remains)
INFO - root - 2017-12-08 08:03:22.266499: step 50440, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 17h:11m:40s remains)
INFO - root - 2017-12-08 08:03:24.505005: step 50450, loss = 2.28, batch loss = 2.23 (35.4 examples/sec; 0.226 sec/batch; 17h:42m:58s remains)
INFO - root - 2017-12-08 08:03:26.765072: step 50460, loss = 2.28, batch loss = 2.23 (32.1 examples/sec; 0.249 sec/batch; 19h:31m:15s remains)
INFO - root - 2017-12-08 08:03:28.993926: step 50470, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 17h:17m:39s remains)
INFO - root - 2017-12-08 08:03:31.212985: step 50480, loss = 2.28, batch loss = 2.23 (37.0 examples/sec; 0.216 sec/batch; 16h:56m:13s remains)
INFO - root - 2017-12-08 08:03:33.442860: step 50490, loss = 2.28, batch loss = 2.23 (36.8 examples/sec; 0.217 sec/batch; 17h:02m:02s remains)
INFO - root - 2017-12-08 08:03:35.706339: step 50500, loss = 2.28, batch loss = 2.23 (35.6 examples/sec; 0.225 sec/batch; 17h:36m:56s remains)
2017-12-08 08:03:36.003068: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289603 -4.4289556 -4.428946 -4.4289408 -4.4289403 -4.4289432 -4.4289484 -4.4289513 -4.4289508 -4.4289527 -4.4289556 -4.4289579 -4.4289608 -4.4289646 -4.4289689][-4.4289489 -4.428937 -4.4289184 -4.428905 -4.4288993 -4.4289012 -4.4289088 -4.4289141 -4.4289136 -4.4289174 -4.4289188 -4.4289207 -4.4289269 -4.4289346 -4.4289412][-4.4289346 -4.4289165 -4.4288921 -4.4288721 -4.4288592 -4.42886 -4.4288731 -4.4288855 -4.4288898 -4.4288945 -4.4288926 -4.4288893 -4.4288955 -4.428905 -4.4289126][-4.4289131 -4.428885 -4.4288535 -4.4288235 -4.4287982 -4.4287977 -4.428823 -4.428853 -4.42887 -4.4288759 -4.4288683 -4.428864 -4.4288721 -4.428885 -4.4288936][-4.4288669 -4.4288135 -4.4287663 -4.4287162 -4.4286737 -4.4286814 -4.428731 -4.4287887 -4.4288182 -4.4288268 -4.4288235 -4.4288316 -4.42885 -4.4288716 -4.4288845][-4.4288039 -4.42872 -4.4286413 -4.4285512 -4.428483 -4.4285069 -4.4285989 -4.4286909 -4.42873 -4.4287405 -4.4287472 -4.4287772 -4.4288182 -4.4288549 -4.4288788][-4.4287624 -4.4286537 -4.4285464 -4.4284182 -4.4283175 -4.4283524 -4.4284811 -4.4285946 -4.4286304 -4.4286332 -4.4286451 -4.4286885 -4.4287477 -4.428802 -4.4288387][-4.428772 -4.4286542 -4.428545 -4.4284258 -4.4283228 -4.4283466 -4.4284673 -4.4285645 -4.4285674 -4.42855 -4.4285645 -4.4286127 -4.4286819 -4.4287353 -4.4287705][-4.4288287 -4.4287229 -4.4286246 -4.4285355 -4.4284558 -4.4284658 -4.4285483 -4.4285941 -4.4285541 -4.42851 -4.4285216 -4.4285822 -4.4286609 -4.4287033 -4.4287219][-4.4288926 -4.4288182 -4.4287376 -4.4286695 -4.4286113 -4.4286127 -4.4286504 -4.4286509 -4.4285841 -4.4285178 -4.428524 -4.428597 -4.4286876 -4.4287248 -4.4287243][-4.4289484 -4.4289045 -4.4288416 -4.4287806 -4.4287391 -4.4287314 -4.4287314 -4.4287062 -4.4286327 -4.4285588 -4.42856 -4.4286385 -4.4287367 -4.4287705 -4.4287634][-4.4289665 -4.4289436 -4.4289007 -4.4288492 -4.4288177 -4.4288025 -4.4287777 -4.4287381 -4.4286704 -4.428597 -4.428587 -4.42866 -4.4287558 -4.4287968 -4.4288054][-4.428956 -4.4289393 -4.4289083 -4.42887 -4.428844 -4.4288192 -4.4287677 -4.4287014 -4.4286251 -4.4285617 -4.4285536 -4.4286294 -4.4287362 -4.4287953 -4.4288216][-4.42894 -4.4289236 -4.4288974 -4.4288664 -4.4288449 -4.428812 -4.4287419 -4.4286513 -4.4285617 -4.4285021 -4.4285016 -4.4285893 -4.4287138 -4.428792 -4.4288278][-4.428937 -4.4289184 -4.4288893 -4.4288545 -4.42883 -4.4287996 -4.4287343 -4.4286442 -4.4285569 -4.4285 -4.4284973 -4.4285822 -4.4287114 -4.4287992 -4.4288354]]...]
INFO - root - 2017-12-08 08:03:38.211228: step 50510, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 17h:19m:30s remains)
INFO - root - 2017-12-08 08:03:40.449855: step 50520, loss = 2.28, batch loss = 2.23 (37.0 examples/sec; 0.216 sec/batch; 16h:55m:38s remains)
INFO - root - 2017-12-08 08:03:42.685452: step 50530, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.225 sec/batch; 17h:38m:52s remains)
INFO - root - 2017-12-08 08:03:44.911813: step 50540, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 17h:26m:11s remains)
INFO - root - 2017-12-08 08:03:47.164330: step 50550, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 17h:19m:12s remains)
INFO - root - 2017-12-08 08:03:49.400100: step 50560, loss = 2.28, batch loss = 2.23 (37.9 examples/sec; 0.211 sec/batch; 16h:32m:27s remains)
INFO - root - 2017-12-08 08:03:51.655432: step 50570, loss = 2.28, batch loss = 2.23 (36.6 examples/sec; 0.219 sec/batch; 17h:07m:32s remains)
INFO - root - 2017-12-08 08:03:53.921489: step 50580, loss = 2.28, batch loss = 2.23 (35.1 examples/sec; 0.228 sec/batch; 17h:51m:53s remains)
INFO - root - 2017-12-08 08:03:56.194110: step 50590, loss = 2.28, batch loss = 2.23 (37.2 examples/sec; 0.215 sec/batch; 16h:49m:09s remains)
INFO - root - 2017-12-08 08:03:58.409714: step 50600, loss = 2.28, batch loss = 2.23 (36.9 examples/sec; 0.217 sec/batch; 16h:59m:17s remains)
2017-12-08 08:03:58.701124: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4287596 -4.4287753 -4.4287672 -4.4287496 -4.4287348 -4.4287343 -4.428751 -4.4287782 -4.4287953 -4.4288 -4.4287958 -4.4287906 -4.42879 -4.4287891 -4.4287853][-4.4287086 -4.4287143 -4.4287066 -4.4287019 -4.4287033 -4.4287124 -4.4287281 -4.4287496 -4.4287605 -4.4287577 -4.42875 -4.4287543 -4.4287729 -4.4287877 -4.428791][-4.4287052 -4.4287066 -4.4287047 -4.4287138 -4.4287276 -4.4287405 -4.4287467 -4.4287519 -4.4287486 -4.4287353 -4.4287252 -4.4287381 -4.4287691 -4.4287939 -4.4287987][-4.4287271 -4.4287286 -4.4287238 -4.4287319 -4.4287477 -4.4287577 -4.4287529 -4.4287419 -4.4287243 -4.4287047 -4.4287009 -4.4287243 -4.4287596 -4.4287825 -4.4287796][-4.4287148 -4.4287105 -4.4286914 -4.4286909 -4.4287 -4.4287028 -4.4286909 -4.4286685 -4.4286361 -4.4286132 -4.4286208 -4.4286561 -4.4286909 -4.4287047 -4.4286904][-4.4286308 -4.4286175 -4.4285812 -4.4285655 -4.4285603 -4.428556 -4.4285464 -4.4285245 -4.4284787 -4.4284558 -4.4284773 -4.4285245 -4.4285607 -4.42857 -4.4285517][-4.4285336 -4.4285135 -4.4284625 -4.428421 -4.42839 -4.4283776 -4.4283776 -4.4283633 -4.4283128 -4.4282923 -4.4283304 -4.4283938 -4.4284415 -4.4284577 -4.4284496][-4.428525 -4.4284983 -4.4284377 -4.4283686 -4.42831 -4.428297 -4.4283123 -4.4283133 -4.4282708 -4.4282508 -4.4282846 -4.4283485 -4.4284048 -4.42843 -4.4284329][-4.4286232 -4.4285979 -4.4285359 -4.4284568 -4.4283915 -4.4283867 -4.4284167 -4.4284306 -4.4284043 -4.428381 -4.4283924 -4.4284325 -4.4284759 -4.4285059 -4.4285231][-4.4287348 -4.4287171 -4.428668 -4.4286056 -4.4285603 -4.4285669 -4.4286027 -4.4286213 -4.4286056 -4.4285827 -4.4285789 -4.4285984 -4.4286256 -4.4286551 -4.4286809][-4.428793 -4.4287834 -4.4287467 -4.428709 -4.4286928 -4.4287148 -4.4287558 -4.428781 -4.4287753 -4.4287586 -4.4287486 -4.4287567 -4.4287734 -4.4287992 -4.428822][-4.4288044 -4.4288011 -4.4287705 -4.4287496 -4.4287558 -4.4287891 -4.4288383 -4.4288707 -4.4288764 -4.4288669 -4.4288564 -4.428854 -4.4288611 -4.4288812 -4.4289][-4.4287853 -4.4287853 -4.428761 -4.4287496 -4.4287682 -4.4288087 -4.428864 -4.4289069 -4.428925 -4.4289217 -4.4289074 -4.4288888 -4.42888 -4.4288874 -4.4288964][-4.4287782 -4.4287744 -4.4287486 -4.4287391 -4.4287591 -4.4287992 -4.428853 -4.4289007 -4.4289279 -4.42893 -4.42891 -4.428875 -4.4288487 -4.4288435 -4.4288459][-4.4287977 -4.4287896 -4.4287605 -4.4287481 -4.4287624 -4.4287944 -4.4288392 -4.4288855 -4.4289169 -4.4289231 -4.4289 -4.4288554 -4.4288192 -4.4288077 -4.4288096]]...]
INFO - root - 2017-12-08 08:04:00.942167: step 50610, loss = 2.28, batch loss = 2.23 (35.3 examples/sec; 0.227 sec/batch; 17h:44m:09s remains)
INFO - root - 2017-12-08 08:04:03.161377: step 50620, loss = 2.28, batch loss = 2.23 (34.5 examples/sec; 0.232 sec/batch; 18h:10m:12s remains)
INFO - root - 2017-12-08 08:04:05.399567: step 50630, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.220 sec/batch; 17h:14m:36s remains)
INFO - root - 2017-12-08 08:04:07.668057: step 50640, loss = 2.28, batch loss = 2.23 (34.6 examples/sec; 0.231 sec/batch; 18h:05m:23s remains)
INFO - root - 2017-12-08 08:04:09.931601: step 50650, loss = 2.28, batch loss = 2.23 (35.1 examples/sec; 0.228 sec/batch; 17h:49m:50s remains)
INFO - root - 2017-12-08 08:04:12.205267: step 50660, loss = 2.28, batch loss = 2.23 (35.7 examples/sec; 0.224 sec/batch; 17h:31m:50s remains)
INFO - root - 2017-12-08 08:04:14.445567: step 50670, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 17h:10m:11s remains)
INFO - root - 2017-12-08 08:04:16.668437: step 50680, loss = 2.28, batch loss = 2.23 (36.6 examples/sec; 0.219 sec/batch; 17h:06m:49s remains)
INFO - root - 2017-12-08 08:04:18.892905: step 50690, loss = 2.28, batch loss = 2.23 (36.8 examples/sec; 0.217 sec/batch; 16h:59m:56s remains)
INFO - root - 2017-12-08 08:04:21.141112: step 50700, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.222 sec/batch; 17h:21m:59s remains)
2017-12-08 08:04:21.441499: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4287739 -4.4288058 -4.4288111 -4.4287939 -4.4287677 -4.4287515 -4.4287434 -4.4287424 -4.4287605 -4.4287882 -4.4287853 -4.4287243 -4.4286304 -4.4285636 -4.4285669][-4.4287658 -4.4287934 -4.4287877 -4.4287591 -4.4287267 -4.4287186 -4.4287214 -4.4287281 -4.4287496 -4.4287758 -4.4287715 -4.4287119 -4.4286151 -4.4285436 -4.4285455][-4.4287663 -4.428792 -4.428771 -4.4287243 -4.4286866 -4.4286947 -4.4287157 -4.4287324 -4.4287558 -4.4287786 -4.4287734 -4.4287186 -4.4286318 -4.4285622 -4.4285541][-4.4287796 -4.4287987 -4.4287577 -4.42869 -4.4286475 -4.4286709 -4.4287114 -4.4287405 -4.4287667 -4.4287877 -4.4287834 -4.4287395 -4.4286718 -4.42861 -4.4285913][-4.4288054 -4.4288135 -4.4287496 -4.4286561 -4.4285984 -4.4286208 -4.4286757 -4.4287252 -4.4287663 -4.4287944 -4.4287953 -4.42876 -4.4287071 -4.4286509 -4.4286242][-4.4288163 -4.4288158 -4.4287391 -4.4286289 -4.4285483 -4.4285474 -4.4286032 -4.428679 -4.4287453 -4.4287868 -4.4287939 -4.4287591 -4.4287043 -4.4286461 -4.4286218][-4.4288034 -4.4288039 -4.4287333 -4.42862 -4.4285169 -4.4284825 -4.4285293 -4.4286242 -4.4287171 -4.4287734 -4.4287858 -4.4287457 -4.4286828 -4.4286232 -4.4286103][-4.4287472 -4.4287577 -4.428709 -4.4286118 -4.4285069 -4.428452 -4.4284854 -4.4285808 -4.4286861 -4.428751 -4.4287648 -4.4287214 -4.4286551 -4.4285975 -4.4285965][-4.4286594 -4.42868 -4.4286575 -4.4285855 -4.4284954 -4.428442 -4.4284706 -4.4285574 -4.4286594 -4.4287267 -4.4287391 -4.4286947 -4.4286313 -4.4285841 -4.4285917][-4.428587 -4.4285989 -4.428587 -4.42853 -4.4284577 -4.4284196 -4.4284592 -4.4285431 -4.4286366 -4.4287014 -4.4287148 -4.4286757 -4.4286203 -4.428587 -4.4286003][-4.4285927 -4.4285727 -4.4285436 -4.4284863 -4.4284344 -4.4284191 -4.4284687 -4.428545 -4.4286184 -4.4286695 -4.4286809 -4.4286494 -4.4286041 -4.4285851 -4.4286046][-4.4286737 -4.4286294 -4.4285789 -4.42852 -4.4284849 -4.4284835 -4.4285192 -4.4285622 -4.428597 -4.4286227 -4.4286294 -4.4286094 -4.4285822 -4.42858 -4.4286084][-4.428761 -4.4287252 -4.4286804 -4.4286318 -4.4286022 -4.428596 -4.4286041 -4.4286075 -4.4286041 -4.4285975 -4.4285927 -4.4285793 -4.4285674 -4.4285736 -4.4286032][-4.4287944 -4.4287915 -4.4287772 -4.4287567 -4.4287386 -4.4287243 -4.4287066 -4.4286814 -4.4286542 -4.4286261 -4.4286075 -4.4285855 -4.4285684 -4.4285645 -4.4285789][-4.4287705 -4.4287987 -4.4288177 -4.4288259 -4.4288244 -4.4288139 -4.4287915 -4.4287567 -4.4287233 -4.4286933 -4.4286714 -4.4286366 -4.4285979 -4.4285674 -4.4285541]]...]
INFO - root - 2017-12-08 08:04:23.704392: step 50710, loss = 2.28, batch loss = 2.23 (35.0 examples/sec; 0.228 sec/batch; 17h:52m:10s remains)
INFO - root - 2017-12-08 08:04:25.940749: step 50720, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.225 sec/batch; 17h:38m:13s remains)
INFO - root - 2017-12-08 08:04:28.168567: step 50730, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 17h:25m:46s remains)
INFO - root - 2017-12-08 08:04:30.404148: step 50740, loss = 2.28, batch loss = 2.23 (37.0 examples/sec; 0.216 sec/batch; 16h:54m:50s remains)
INFO - root - 2017-12-08 08:04:32.652171: step 50750, loss = 2.28, batch loss = 2.23 (36.7 examples/sec; 0.218 sec/batch; 17h:04m:56s remains)
INFO - root - 2017-12-08 08:04:34.888098: step 50760, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 17h:08m:29s remains)
INFO - root - 2017-12-08 08:04:37.111188: step 50770, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.221 sec/batch; 17h:19m:44s remains)
INFO - root - 2017-12-08 08:04:39.370371: step 50780, loss = 2.28, batch loss = 2.23 (35.2 examples/sec; 0.227 sec/batch; 17h:47m:05s remains)
INFO - root - 2017-12-08 08:04:41.618815: step 50790, loss = 2.28, batch loss = 2.23 (35.7 examples/sec; 0.224 sec/batch; 17h:30m:57s remains)
INFO - root - 2017-12-08 08:04:43.856842: step 50800, loss = 2.28, batch loss = 2.23 (32.9 examples/sec; 0.243 sec/batch; 19h:01m:07s remains)
2017-12-08 08:04:44.141938: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4285369 -4.4285512 -4.4285731 -4.4285617 -4.4285359 -4.4285264 -4.42854 -4.428555 -4.4285884 -4.4286308 -4.4286556 -4.4286976 -4.4287462 -4.42875 -4.4287577][-4.4284611 -4.4284654 -4.4284854 -4.4284821 -4.4284596 -4.4284554 -4.4284797 -4.4284987 -4.4285359 -4.4285889 -4.4286237 -4.4286718 -4.4287286 -4.4287319 -4.4287395][-4.4284439 -4.4284506 -4.4284768 -4.4284868 -4.42847 -4.428463 -4.4284792 -4.4284878 -4.4285121 -4.4285631 -4.4286132 -4.4286685 -4.4287248 -4.428731 -4.4287405][-4.4284916 -4.428514 -4.4285493 -4.4285669 -4.42856 -4.4285464 -4.4285345 -4.428514 -4.4285164 -4.4285588 -4.4286127 -4.4286695 -4.4287229 -4.4287357 -4.4287505][-4.4285192 -4.4285588 -4.4286013 -4.4286175 -4.4286141 -4.4285932 -4.4285502 -4.4285049 -4.4284897 -4.4285221 -4.42858 -4.4286475 -4.4287128 -4.4287381 -4.4287605][-4.4284997 -4.4285412 -4.4285831 -4.4285941 -4.4285841 -4.4285417 -4.4284635 -4.4283943 -4.4283662 -4.4283991 -4.4284792 -4.4285765 -4.4286661 -4.4287181 -4.4287539][-4.4284415 -4.4284673 -4.428503 -4.4285035 -4.4284763 -4.4284043 -4.4283004 -4.4282312 -4.4282212 -4.4282818 -4.4283962 -4.4285245 -4.42864 -4.4287133 -4.4287596][-4.4283967 -4.4283991 -4.428422 -4.4284086 -4.4283619 -4.4282675 -4.4281645 -4.4281263 -4.4281678 -4.4282718 -4.4284077 -4.428544 -4.4286609 -4.4287343 -4.4287806][-4.428401 -4.4283938 -4.4284134 -4.4283962 -4.42835 -4.4282684 -4.4282 -4.4282031 -4.428277 -4.4283862 -4.4285007 -4.4286051 -4.4286971 -4.4287586 -4.4287977][-4.4284339 -4.4284239 -4.4284415 -4.4284258 -4.4283857 -4.4283323 -4.4283028 -4.4283342 -4.4284139 -4.4284997 -4.4285793 -4.4286551 -4.4287252 -4.4287786 -4.4288144][-4.4285131 -4.4285064 -4.42852 -4.4285088 -4.4284863 -4.42846 -4.4284463 -4.4284682 -4.4285283 -4.4285879 -4.4286423 -4.4286976 -4.4287519 -4.4288015 -4.4288363][-4.4286256 -4.4286222 -4.4286332 -4.42863 -4.4286237 -4.4286184 -4.4286132 -4.4286213 -4.4286556 -4.4286928 -4.4287281 -4.42877 -4.428812 -4.4288483 -4.4288721][-4.4287362 -4.4287291 -4.428731 -4.4287267 -4.4287248 -4.42873 -4.4287357 -4.4287424 -4.4287615 -4.4287834 -4.4288087 -4.4288406 -4.4288716 -4.428895 -4.4289083][-4.4288211 -4.428813 -4.4288116 -4.4288077 -4.4288068 -4.42881 -4.4288158 -4.42882 -4.42883 -4.428843 -4.4288621 -4.4288888 -4.4289117 -4.4289269 -4.4289336][-4.428864 -4.4288573 -4.4288559 -4.4288545 -4.4288549 -4.4288578 -4.4288621 -4.4288654 -4.4288712 -4.4288788 -4.4288936 -4.4289136 -4.4289322 -4.4289451 -4.4289508]]...]
INFO - root - 2017-12-08 08:04:46.390431: step 50810, loss = 2.28, batch loss = 2.23 (34.9 examples/sec; 0.229 sec/batch; 17h:55m:01s remains)
INFO - root - 2017-12-08 08:04:48.645600: step 50820, loss = 2.28, batch loss = 2.23 (35.2 examples/sec; 0.227 sec/batch; 17h:47m:09s remains)
INFO - root - 2017-12-08 08:04:50.887627: step 50830, loss = 2.28, batch loss = 2.23 (35.6 examples/sec; 0.225 sec/batch; 17h:34m:57s remains)
INFO - root - 2017-12-08 08:04:53.114839: step 50840, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 17h:25m:55s remains)
INFO - root - 2017-12-08 08:04:55.368696: step 50850, loss = 2.28, batch loss = 2.23 (34.6 examples/sec; 0.232 sec/batch; 18h:06m:52s remains)
INFO - root - 2017-12-08 08:04:57.608712: step 50860, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.225 sec/batch; 17h:36m:58s remains)
INFO - root - 2017-12-08 08:04:59.831089: step 50870, loss = 2.28, batch loss = 2.23 (35.0 examples/sec; 0.229 sec/batch; 17h:53m:47s remains)
INFO - root - 2017-12-08 08:05:02.089350: step 50880, loss = 2.28, batch loss = 2.23 (36.7 examples/sec; 0.218 sec/batch; 17h:03m:26s remains)
INFO - root - 2017-12-08 08:05:04.313260: step 50890, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 17h:08m:09s remains)
INFO - root - 2017-12-08 08:05:06.537404: step 50900, loss = 2.28, batch loss = 2.23 (36.9 examples/sec; 0.217 sec/batch; 16h:58m:06s remains)
2017-12-08 08:05:06.826889: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4287872 -4.4287343 -4.4287109 -4.4287024 -4.4286923 -4.4287038 -4.4287281 -4.4287581 -4.4287596 -4.4287381 -4.4287043 -4.428669 -4.4286637 -4.4286742 -4.4286785][-4.4287715 -4.4287071 -4.4286785 -4.4286685 -4.4286647 -4.4286895 -4.4287271 -4.428762 -4.42876 -4.4287443 -4.4287148 -4.428678 -4.428668 -4.4286809 -4.4286871][-4.4287453 -4.4286814 -4.4286561 -4.4286528 -4.4286571 -4.4286819 -4.4287186 -4.4287505 -4.4287539 -4.4287486 -4.4287233 -4.4286842 -4.4286675 -4.4286823 -4.4286966][-4.428731 -4.4286795 -4.4286675 -4.4286685 -4.4286628 -4.4286661 -4.4286823 -4.4287066 -4.4287214 -4.4287305 -4.4287133 -4.428678 -4.4286633 -4.4286819 -4.4287066][-4.4287491 -4.4287143 -4.4287105 -4.4287066 -4.4286785 -4.4286437 -4.4286251 -4.4286385 -4.42867 -4.4286966 -4.4286904 -4.4286675 -4.4286571 -4.4286776 -4.428709][-4.4287667 -4.4287357 -4.4287276 -4.4287133 -4.4286547 -4.4285789 -4.4285245 -4.4285269 -4.4285836 -4.4286351 -4.428648 -4.4286432 -4.4286456 -4.4286633 -4.4287043][-4.4287248 -4.4286776 -4.4286523 -4.4286275 -4.4285488 -4.4284406 -4.4283504 -4.4283442 -4.4284315 -4.4285159 -4.4285545 -4.4285765 -4.4286094 -4.4286437 -4.4286952][-4.4286561 -4.4285865 -4.4285393 -4.4285054 -4.4284239 -4.4282942 -4.4281769 -4.4281735 -4.4282918 -4.4284115 -4.4284825 -4.4285321 -4.4285936 -4.428648 -4.4286971][-4.4286265 -4.4285421 -4.4284854 -4.4284639 -4.4284215 -4.4283195 -4.4282207 -4.4282303 -4.428339 -4.4284496 -4.4285207 -4.4285688 -4.4286327 -4.4286852 -4.4287119][-4.4286366 -4.4285512 -4.4285116 -4.4285274 -4.428546 -4.4285 -4.4284387 -4.4284449 -4.42851 -4.42858 -4.4286313 -4.4286666 -4.4287119 -4.4287395 -4.4287353][-4.4286518 -4.4285722 -4.428555 -4.4286046 -4.4286656 -4.4286561 -4.42862 -4.4286165 -4.4286432 -4.4286828 -4.4287267 -4.428762 -4.4287915 -4.428791 -4.4287624][-4.4286456 -4.4285622 -4.4285569 -4.4286304 -4.4287148 -4.4287243 -4.4287052 -4.4287019 -4.4287119 -4.4287424 -4.4287848 -4.42882 -4.42883 -4.4288063 -4.4287667][-4.4286337 -4.428544 -4.4285407 -4.4286222 -4.4287162 -4.4287424 -4.4287429 -4.4287496 -4.4287639 -4.4287915 -4.4288235 -4.4288425 -4.42883 -4.4287915 -4.4287543][-4.4286494 -4.42856 -4.42855 -4.428618 -4.4287062 -4.4287443 -4.4287682 -4.428793 -4.4288149 -4.428833 -4.4288454 -4.4288421 -4.4288158 -4.4287767 -4.4287486][-4.4287047 -4.428627 -4.4286046 -4.4286494 -4.4287238 -4.4287663 -4.4288025 -4.4288344 -4.4288492 -4.4288549 -4.4288473 -4.42883 -4.428802 -4.428771 -4.428751]]...]
INFO - root - 2017-12-08 08:05:09.070946: step 50910, loss = 2.28, batch loss = 2.23 (36.7 examples/sec; 0.218 sec/batch; 17h:03m:35s remains)
INFO - root - 2017-12-08 08:05:11.299477: step 50920, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.220 sec/batch; 17h:14m:19s remains)
INFO - root - 2017-12-08 08:05:13.557800: step 50930, loss = 2.28, batch loss = 2.23 (35.4 examples/sec; 0.226 sec/batch; 17h:41m:01s remains)
INFO - root - 2017-12-08 08:05:15.790930: step 50940, loss = 2.28, batch loss = 2.23 (35.6 examples/sec; 0.225 sec/batch; 17h:35m:19s remains)
INFO - root - 2017-12-08 08:05:18.015645: step 50950, loss = 2.28, batch loss = 2.23 (35.8 examples/sec; 0.223 sec/batch; 17h:27m:40s remains)
INFO - root - 2017-12-08 08:05:20.254154: step 50960, loss = 2.28, batch loss = 2.23 (35.2 examples/sec; 0.227 sec/batch; 17h:45m:09s remains)
INFO - root - 2017-12-08 08:05:22.522898: step 50970, loss = 2.28, batch loss = 2.23 (34.5 examples/sec; 0.232 sec/batch; 18h:08m:56s remains)
INFO - root - 2017-12-08 08:05:24.752756: step 50980, loss = 2.28, batch loss = 2.23 (36.6 examples/sec; 0.219 sec/batch; 17h:06m:39s remains)
INFO - root - 2017-12-08 08:05:27.020535: step 50990, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 17h:17m:34s remains)
INFO - root - 2017-12-08 08:05:29.250126: step 51000, loss = 2.28, batch loss = 2.23 (35.8 examples/sec; 0.223 sec/batch; 17h:27m:50s remains)
2017-12-08 08:05:29.558227: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289641 -4.4289684 -4.4289713 -4.4289751 -4.4289756 -4.4289641 -4.4289479 -4.4289346 -4.4289336 -4.4289389 -4.4289494 -4.4289627 -4.428978 -4.4289913 -4.4290032][-4.4289351 -4.4289346 -4.428936 -4.4289417 -4.4289422 -4.4289212 -4.428896 -4.4288797 -4.4288793 -4.428885 -4.4288979 -4.4289179 -4.42894 -4.4289608 -4.4289808][-4.4289126 -4.428906 -4.4289031 -4.4289031 -4.4288917 -4.4288554 -4.4288144 -4.4287944 -4.4287953 -4.4288077 -4.428834 -4.4288697 -4.4289036 -4.4289303 -4.4289556][-4.4288864 -4.4288769 -4.428874 -4.4288669 -4.4288411 -4.4287872 -4.4287248 -4.4287033 -4.4287109 -4.4287314 -4.4287715 -4.4288239 -4.4288716 -4.4289079 -4.428937][-4.4288673 -4.4288583 -4.4288559 -4.4288387 -4.42879 -4.4287152 -4.42863 -4.4286137 -4.4286366 -4.4286714 -4.4287205 -4.4287786 -4.4288387 -4.4288883 -4.4289227][-4.428833 -4.4288154 -4.4288034 -4.4287772 -4.4287167 -4.4286222 -4.42851 -4.4285007 -4.4285541 -4.4286132 -4.4286761 -4.4287424 -4.4288125 -4.4288769 -4.4289169][-4.4287438 -4.4287114 -4.4286876 -4.4286561 -4.4285927 -4.4284668 -4.4283056 -4.4282947 -4.4283981 -4.4285007 -4.4285917 -4.4286809 -4.4287715 -4.4288578 -4.4289112][-4.4286456 -4.42861 -4.4285822 -4.4285488 -4.4284787 -4.4283109 -4.428081 -4.4280796 -4.4282579 -4.4284148 -4.4285374 -4.428648 -4.428751 -4.4288487 -4.42891][-4.4286003 -4.4285617 -4.4285374 -4.4285188 -4.4284706 -4.4283285 -4.4281039 -4.4280977 -4.4282923 -4.4284635 -4.4285884 -4.4286928 -4.4287796 -4.428863 -4.4289169][-4.4285822 -4.4285436 -4.4285297 -4.4285331 -4.4285288 -4.4284573 -4.4283247 -4.428308 -4.4284382 -4.4285722 -4.42867 -4.4287477 -4.428812 -4.4288788 -4.4289265][-4.428596 -4.4285588 -4.4285488 -4.4285569 -4.4285712 -4.4285431 -4.4284811 -4.4284782 -4.4285665 -4.4286637 -4.4287381 -4.428793 -4.4288435 -4.4289002 -4.4289455][-4.4286766 -4.4286304 -4.4286094 -4.4286122 -4.42863 -4.4286156 -4.4285793 -4.4285846 -4.4286456 -4.4287171 -4.4287791 -4.4288273 -4.4288783 -4.4289289 -4.42897][-4.4287953 -4.4287472 -4.4287128 -4.4287076 -4.4287238 -4.4287233 -4.42871 -4.4287143 -4.4287467 -4.4287848 -4.4288263 -4.42887 -4.4289217 -4.4289646 -4.4289947][-4.4288926 -4.4288583 -4.4288249 -4.4288125 -4.4288235 -4.4288335 -4.4288349 -4.4288397 -4.4288568 -4.4288759 -4.4288979 -4.4289322 -4.4289713 -4.4289989 -4.4290147][-4.4289451 -4.4289231 -4.4289 -4.4288859 -4.4288893 -4.4289002 -4.4289088 -4.4289193 -4.428936 -4.4289503 -4.4289651 -4.428988 -4.4290118 -4.4290266 -4.4290309]]...]
INFO - root - 2017-12-08 08:05:31.772090: step 51010, loss = 2.28, batch loss = 2.23 (36.0 examples/sec; 0.222 sec/batch; 17h:23m:41s remains)
INFO - root - 2017-12-08 08:05:33.999790: step 51020, loss = 2.28, batch loss = 2.23 (34.9 examples/sec; 0.229 sec/batch; 17h:54m:22s remains)
INFO - root - 2017-12-08 08:05:36.226570: step 51030, loss = 2.28, batch loss = 2.23 (35.2 examples/sec; 0.228 sec/batch; 17h:47m:14s remains)
INFO - root - 2017-12-08 08:05:38.466906: step 51040, loss = 2.28, batch loss = 2.23 (36.0 examples/sec; 0.222 sec/batch; 17h:22m:21s remains)
INFO - root - 2017-12-08 08:05:40.706590: step 51050, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.225 sec/batch; 17h:37m:11s remains)
INFO - root - 2017-12-08 08:05:42.928376: step 51060, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.222 sec/batch; 17h:20m:23s remains)
INFO - root - 2017-12-08 08:05:45.161305: step 51070, loss = 2.28, batch loss = 2.23 (36.7 examples/sec; 0.218 sec/batch; 17h:03m:29s remains)
INFO - root - 2017-12-08 08:05:47.387886: step 51080, loss = 2.28, batch loss = 2.23 (35.8 examples/sec; 0.224 sec/batch; 17h:28m:40s remains)
INFO - root - 2017-12-08 08:05:49.659322: step 51090, loss = 2.28, batch loss = 2.23 (35.8 examples/sec; 0.224 sec/batch; 17h:28m:48s remains)
INFO - root - 2017-12-08 08:05:51.885289: step 51100, loss = 2.28, batch loss = 2.23 (35.3 examples/sec; 0.227 sec/batch; 17h:42m:55s remains)
2017-12-08 08:05:52.201381: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288678 -4.4289031 -4.4289117 -4.42887 -4.4287844 -4.4286833 -4.4285903 -4.4285364 -4.4285307 -4.428546 -4.4285803 -4.428637 -4.4287186 -4.4287906 -4.4288383][-4.4288893 -4.4289007 -4.4288969 -4.4288549 -4.4287796 -4.4286909 -4.4286194 -4.4286065 -4.4286466 -4.4286928 -4.4287376 -4.4287844 -4.42884 -4.4288845 -4.4289074][-4.4288912 -4.4288793 -4.428865 -4.4288282 -4.4287677 -4.4286942 -4.4286346 -4.4286366 -4.4286861 -4.42873 -4.4287729 -4.4288235 -4.42888 -4.428926 -4.4289503][-4.4288735 -4.4288416 -4.4288135 -4.4287739 -4.4287148 -4.4286432 -4.4285789 -4.4285736 -4.428616 -4.4286609 -4.4287143 -4.428781 -4.428854 -4.42892 -4.4289584][-4.4288425 -4.4287977 -4.4287553 -4.4287043 -4.4286318 -4.4285407 -4.4284472 -4.4284205 -4.4284706 -4.428544 -4.4286304 -4.428721 -4.4288116 -4.4288936 -4.4289446][-4.4288077 -4.4287596 -4.4287028 -4.42863 -4.4285307 -4.4284043 -4.4282727 -4.4282317 -4.4283175 -4.428452 -4.428587 -4.4287033 -4.4288006 -4.4288812 -4.4289279][-4.4287763 -4.4287291 -4.4286575 -4.4285588 -4.4284329 -4.4282913 -4.4281588 -4.428134 -4.4282627 -4.4284453 -4.4286094 -4.4287348 -4.4288268 -4.4288931 -4.4289222][-4.4287457 -4.4286966 -4.428618 -4.4285107 -4.4283929 -4.4282904 -4.4282169 -4.428237 -4.4283719 -4.4285483 -4.4286962 -4.4287987 -4.4288673 -4.4289122 -4.428925][-4.4287419 -4.428688 -4.4286079 -4.4285207 -4.4284549 -4.428432 -4.4284363 -4.4284825 -4.4285817 -4.4287014 -4.4287953 -4.4288559 -4.4288964 -4.4289246 -4.4289279][-4.42877 -4.42872 -4.4286528 -4.4286008 -4.4285903 -4.428627 -4.4286671 -4.4287095 -4.4287663 -4.428822 -4.4288607 -4.4288821 -4.4288974 -4.4289131 -4.4289141][-4.4288135 -4.428771 -4.4287248 -4.4287052 -4.4287233 -4.4287744 -4.4288158 -4.4288468 -4.4288759 -4.4288917 -4.4288888 -4.4288735 -4.4288611 -4.42886 -4.4288664][-4.4288421 -4.4288087 -4.4287815 -4.428781 -4.4288082 -4.428854 -4.4288859 -4.428905 -4.4289155 -4.4289055 -4.4288683 -4.4288139 -4.4287724 -4.4287639 -4.428791][-4.4288292 -4.4288087 -4.4288082 -4.4288268 -4.4288568 -4.42889 -4.42891 -4.4289193 -4.4289088 -4.428864 -4.4287791 -4.4286838 -4.4286237 -4.4286275 -4.4286904][-4.4288006 -4.4287953 -4.4288163 -4.4288454 -4.4288707 -4.4288859 -4.428885 -4.4288745 -4.4288363 -4.4287562 -4.4286332 -4.4285197 -4.4284668 -4.428494 -4.4285884][-4.4287906 -4.4287953 -4.4288192 -4.4288445 -4.4288559 -4.4288487 -4.4288235 -4.4287896 -4.4287314 -4.428637 -4.4285083 -4.4284058 -4.4283676 -4.4284029 -4.428504]]...]
INFO - root - 2017-12-08 08:05:54.419371: step 51110, loss = 2.28, batch loss = 2.23 (35.2 examples/sec; 0.227 sec/batch; 17h:44m:57s remains)
INFO - root - 2017-12-08 08:05:56.641529: step 51120, loss = 2.28, batch loss = 2.23 (35.7 examples/sec; 0.224 sec/batch; 17h:32m:01s remains)
INFO - root - 2017-12-08 08:05:58.912669: step 51130, loss = 2.28, batch loss = 2.23 (34.0 examples/sec; 0.235 sec/batch; 18h:23m:38s remains)
INFO - root - 2017-12-08 08:06:01.128113: step 51140, loss = 2.28, batch loss = 2.23 (36.6 examples/sec; 0.219 sec/batch; 17h:04m:54s remains)
INFO - root - 2017-12-08 08:06:03.370138: step 51150, loss = 2.28, batch loss = 2.23 (36.7 examples/sec; 0.218 sec/batch; 17h:03m:22s remains)
INFO - root - 2017-12-08 08:06:05.624193: step 51160, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 17h:10m:15s remains)
INFO - root - 2017-12-08 08:06:07.936136: step 51170, loss = 2.28, batch loss = 2.23 (34.2 examples/sec; 0.234 sec/batch; 18h:17m:30s remains)
INFO - root - 2017-12-08 08:06:10.189877: step 51180, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.221 sec/batch; 17h:14m:25s remains)
INFO - root - 2017-12-08 08:06:12.418648: step 51190, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 17h:09m:07s remains)
INFO - root - 2017-12-08 08:06:14.650074: step 51200, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 17h:07m:20s remains)
2017-12-08 08:06:14.947759: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4287386 -4.4288149 -4.4288793 -4.4289155 -4.4289088 -4.4288659 -4.4288287 -4.4287977 -4.4287791 -4.4287868 -4.4288111 -4.4288316 -4.4288092 -4.4287734 -4.4287434][-4.4287562 -4.4288535 -4.4289217 -4.4289408 -4.42891 -4.4288583 -4.4288187 -4.4287868 -4.4287696 -4.4287882 -4.4288154 -4.4288373 -4.4288135 -4.4287696 -4.4287333][-4.4288173 -4.4289031 -4.428946 -4.4289355 -4.4288878 -4.4288278 -4.428781 -4.42875 -4.4287405 -4.4287772 -4.4288192 -4.4288535 -4.4288368 -4.42879 -4.4287519][-4.4288888 -4.428946 -4.4289522 -4.4289103 -4.4288497 -4.4287853 -4.4287372 -4.4287243 -4.428731 -4.4287829 -4.4288354 -4.4288764 -4.4288754 -4.4288354 -4.4287953][-4.4289355 -4.4289656 -4.4289441 -4.4288745 -4.4288068 -4.4287543 -4.428719 -4.4287257 -4.4287524 -4.4288149 -4.4288654 -4.4288993 -4.4289103 -4.428884 -4.4288478][-4.4289474 -4.428947 -4.4289141 -4.4288359 -4.42877 -4.4287348 -4.4287152 -4.4287238 -4.4287505 -4.4288063 -4.4288487 -4.4288821 -4.4289079 -4.4289055 -4.42889][-4.4289074 -4.4288831 -4.4288507 -4.4287724 -4.4287143 -4.4286966 -4.4286919 -4.4286957 -4.4286976 -4.4287257 -4.4287658 -4.4288087 -4.4288578 -4.4288883 -4.4289007][-4.4288206 -4.4287705 -4.4287271 -4.4286575 -4.4286132 -4.4286122 -4.4286275 -4.4286242 -4.4285817 -4.428566 -4.4286036 -4.4286642 -4.4287515 -4.42882 -4.4288654][-4.4286852 -4.4285917 -4.4285116 -4.4284382 -4.42842 -4.4284663 -4.4285073 -4.4284887 -4.4284077 -4.42835 -4.4283814 -4.4284787 -4.4286189 -4.4287333 -4.428813][-4.4285378 -4.4284034 -4.428288 -4.4282241 -4.4282436 -4.4283357 -4.4283853 -4.4283552 -4.428268 -4.4281969 -4.4282432 -4.428369 -4.4285398 -4.428688 -4.4287968][-4.4284425 -4.4283071 -4.4282103 -4.4281874 -4.4282365 -4.4283319 -4.4283581 -4.4283166 -4.4282551 -4.42821 -4.4282722 -4.428391 -4.4285545 -4.4287105 -4.4288282][-4.4284673 -4.4283752 -4.4283295 -4.4283361 -4.4283857 -4.4284592 -4.4284692 -4.428432 -4.4283991 -4.4283786 -4.4284334 -4.4285178 -4.4286504 -4.4287877 -4.4288907][-4.428597 -4.4285455 -4.4285297 -4.4285522 -4.4285984 -4.4286528 -4.428658 -4.4286284 -4.4286103 -4.4286013 -4.4286413 -4.4286976 -4.428792 -4.4288931 -4.4289622][-4.4287534 -4.4287276 -4.4287271 -4.4287534 -4.4287915 -4.4288278 -4.4288344 -4.4288139 -4.4288025 -4.4287972 -4.4288268 -4.4288712 -4.4289308 -4.4289875 -4.4290237][-4.4288816 -4.4288745 -4.428884 -4.4289002 -4.4289203 -4.4289417 -4.4289536 -4.4289422 -4.4289327 -4.4289312 -4.4289494 -4.4289784 -4.4290104 -4.4290419 -4.4290576]]...]
INFO - root - 2017-12-08 08:06:17.161211: step 51210, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 17h:09m:50s remains)
INFO - root - 2017-12-08 08:06:19.387160: step 51220, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 17h:08m:18s remains)
INFO - root - 2017-12-08 08:06:21.673195: step 51230, loss = 2.28, batch loss = 2.23 (34.9 examples/sec; 0.229 sec/batch; 17h:54m:03s remains)
INFO - root - 2017-12-08 08:06:23.916348: step 51240, loss = 2.28, batch loss = 2.23 (36.6 examples/sec; 0.219 sec/batch; 17h:05m:43s remains)
INFO - root - 2017-12-08 08:06:26.137562: step 51250, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 17h:10m:42s remains)
INFO - root - 2017-12-08 08:06:28.390915: step 51260, loss = 2.28, batch loss = 2.23 (36.0 examples/sec; 0.222 sec/batch; 17h:20m:16s remains)
INFO - root - 2017-12-08 08:06:30.618068: step 51270, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.220 sec/batch; 17h:12m:38s remains)
INFO - root - 2017-12-08 08:06:32.849812: step 51280, loss = 2.28, batch loss = 2.23 (35.3 examples/sec; 0.227 sec/batch; 17h:42m:03s remains)
INFO - root - 2017-12-08 08:06:35.075091: step 51290, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.225 sec/batch; 17h:36m:29s remains)
INFO - root - 2017-12-08 08:06:37.301043: step 51300, loss = 2.28, batch loss = 2.23 (34.5 examples/sec; 0.232 sec/batch; 18h:06m:44s remains)
2017-12-08 08:06:37.595140: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288521 -4.4288249 -4.428802 -4.4287705 -4.4287291 -4.4286785 -4.42864 -4.428627 -4.4286265 -4.428617 -4.4285865 -4.4285474 -4.4285212 -4.4285116 -4.4285483][-4.4288077 -4.42877 -4.4287453 -4.4287219 -4.4286919 -4.4286628 -4.4286485 -4.4286551 -4.428669 -4.4286709 -4.4286451 -4.4286108 -4.4285688 -4.4285307 -4.4285421][-4.4287562 -4.4287214 -4.4287081 -4.4286885 -4.4286633 -4.4286509 -4.4286485 -4.4286489 -4.4286652 -4.42869 -4.4286857 -4.4286695 -4.4286208 -4.4285684 -4.4285564][-4.4287276 -4.4287047 -4.4287095 -4.4286976 -4.4286718 -4.4286523 -4.428627 -4.4285955 -4.4286118 -4.428669 -4.4287033 -4.4287043 -4.428658 -4.4285975 -4.4285583][-4.4287343 -4.4287262 -4.4287348 -4.4287291 -4.428699 -4.4286504 -4.4285741 -4.42849 -4.4284954 -4.4285941 -4.4286752 -4.4287024 -4.4286757 -4.4286208 -4.4285669][-4.4287453 -4.42876 -4.4287677 -4.4287496 -4.4286985 -4.428616 -4.42848 -4.4283233 -4.4283047 -4.4284592 -4.4286017 -4.4286575 -4.428659 -4.4286427 -4.4286084][-4.42876 -4.4287877 -4.4287844 -4.42874 -4.4286618 -4.4285522 -4.4283719 -4.4281511 -4.4280987 -4.4283071 -4.4285059 -4.4285975 -4.4286351 -4.4286604 -4.4286551][-4.4287786 -4.4287939 -4.4287696 -4.4287043 -4.4286146 -4.4285073 -4.428339 -4.4281287 -4.4280653 -4.4282627 -4.4284577 -4.4285655 -4.4286242 -4.4286714 -4.4286747][-4.4287877 -4.4287777 -4.4287281 -4.4286585 -4.4285889 -4.4285259 -4.4284267 -4.4282908 -4.4282413 -4.4283614 -4.4285045 -4.4285865 -4.4286165 -4.428637 -4.4286327][-4.4287615 -4.42872 -4.4286571 -4.4286094 -4.4285893 -4.4285812 -4.42855 -4.428484 -4.42845 -4.4285064 -4.4285865 -4.428618 -4.4286 -4.4285803 -4.4285617][-4.4287171 -4.4286509 -4.4285889 -4.428575 -4.4286003 -4.4286375 -4.4286523 -4.42863 -4.4286132 -4.4286366 -4.4286604 -4.4286394 -4.4285846 -4.4285369 -4.4284992][-4.4287066 -4.4286447 -4.4286008 -4.428606 -4.4286489 -4.4286976 -4.4287257 -4.4287171 -4.4287071 -4.428709 -4.4286995 -4.428658 -4.428597 -4.4285364 -4.4284878][-4.4287434 -4.4287038 -4.4286742 -4.4286795 -4.4287128 -4.4287477 -4.4287663 -4.4287558 -4.4287419 -4.4287271 -4.4287019 -4.4286628 -4.4286218 -4.4285665 -4.4285197][-4.4287882 -4.4287663 -4.4287515 -4.4287524 -4.4287596 -4.4287658 -4.42876 -4.4287405 -4.4287219 -4.4286928 -4.4286642 -4.4286461 -4.4286375 -4.4286065 -4.4285674][-4.4288216 -4.4288058 -4.4287953 -4.4287872 -4.4287705 -4.4287481 -4.4287233 -4.4287009 -4.4286866 -4.4286575 -4.4286308 -4.4286275 -4.4286518 -4.4286556 -4.4286342]]...]
INFO - root - 2017-12-08 08:06:39.854431: step 51310, loss = 2.28, batch loss = 2.23 (34.5 examples/sec; 0.232 sec/batch; 18h:06m:37s remains)
INFO - root - 2017-12-08 08:06:42.074467: step 51320, loss = 2.28, batch loss = 2.23 (35.2 examples/sec; 0.227 sec/batch; 17h:43m:41s remains)
INFO - root - 2017-12-08 08:06:44.319699: step 51330, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 17h:17m:01s remains)
INFO - root - 2017-12-08 08:06:46.557276: step 51340, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 17h:10m:04s remains)
INFO - root - 2017-12-08 08:06:48.800095: step 51350, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 17h:07m:01s remains)
INFO - root - 2017-12-08 08:06:51.045380: step 51360, loss = 2.28, batch loss = 2.23 (35.3 examples/sec; 0.227 sec/batch; 17h:42m:05s remains)
INFO - root - 2017-12-08 08:06:53.281294: step 51370, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 17h:25m:19s remains)
INFO - root - 2017-12-08 08:06:55.525620: step 51380, loss = 2.28, batch loss = 2.23 (35.7 examples/sec; 0.224 sec/batch; 17h:28m:29s remains)
INFO - root - 2017-12-08 08:06:57.766945: step 51390, loss = 2.28, batch loss = 2.23 (34.1 examples/sec; 0.235 sec/batch; 18h:18m:47s remains)
INFO - root - 2017-12-08 08:07:00.020357: step 51400, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.220 sec/batch; 17h:11m:10s remains)
2017-12-08 08:07:00.301382: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289312 -4.4289212 -4.4289136 -4.4289069 -4.4289007 -4.428895 -4.428894 -4.428896 -4.4289036 -4.4289169 -4.4289317 -4.4289446 -4.4289513 -4.4289494 -4.4289474][-4.4289069 -4.4288917 -4.428884 -4.42888 -4.428875 -4.428865 -4.4288573 -4.4288507 -4.4288564 -4.4288769 -4.4289064 -4.4289322 -4.4289446 -4.4289436 -4.4289427][-4.4288573 -4.4288359 -4.4288273 -4.4288254 -4.4288197 -4.428802 -4.4287791 -4.4287572 -4.4287586 -4.4287925 -4.4288435 -4.4288878 -4.4289093 -4.4289136 -4.4289174][-4.428792 -4.428762 -4.4287529 -4.4287534 -4.4287434 -4.4287167 -4.42868 -4.4286423 -4.4286389 -4.4286842 -4.4287572 -4.4288192 -4.4288516 -4.428863 -4.4288788][-4.4287291 -4.4286914 -4.4286833 -4.4286838 -4.4286695 -4.4286394 -4.4285922 -4.4285412 -4.4285312 -4.4285879 -4.4286795 -4.4287562 -4.4287949 -4.4288106 -4.4288354][-4.4286952 -4.4286551 -4.4286475 -4.4286466 -4.4286284 -4.4285913 -4.4285326 -4.4284682 -4.4284563 -4.4285283 -4.4286356 -4.428721 -4.4287639 -4.4287796 -4.4288049][-4.4286962 -4.4286575 -4.4286532 -4.4286547 -4.428638 -4.4285979 -4.428534 -4.4284678 -4.4284635 -4.428544 -4.4286547 -4.4287372 -4.4287758 -4.4287844 -4.4287963][-4.4287109 -4.4286795 -4.4286871 -4.4287014 -4.4286981 -4.4286695 -4.4286065 -4.4285388 -4.4285355 -4.4286127 -4.4287138 -4.4287834 -4.4288106 -4.4288082 -4.4288073][-4.4287152 -4.4286914 -4.4287095 -4.4287348 -4.4287462 -4.4287319 -4.4286761 -4.4286127 -4.4286122 -4.4286847 -4.4287696 -4.4288235 -4.4288425 -4.4288373 -4.4288321][-4.4287214 -4.4287071 -4.42873 -4.4287558 -4.4287639 -4.4287529 -4.4287076 -4.4286623 -4.4286742 -4.428741 -4.4288135 -4.4288564 -4.4288688 -4.4288645 -4.4288592][-4.4287472 -4.4287438 -4.4287653 -4.4287844 -4.4287872 -4.4287782 -4.4287481 -4.4287243 -4.4287448 -4.4288025 -4.428863 -4.4288964 -4.4289064 -4.4289012 -4.4288926][-4.4288011 -4.4288034 -4.4288211 -4.4288359 -4.4288383 -4.428833 -4.4288173 -4.4288087 -4.4288316 -4.4288774 -4.4289222 -4.4289455 -4.4289517 -4.4289441 -4.4289312][-4.428875 -4.4288797 -4.4288912 -4.4289012 -4.4289045 -4.4289012 -4.4288917 -4.4288883 -4.4289064 -4.4289379 -4.428967 -4.4289804 -4.4289808 -4.4289708 -4.428957][-4.428926 -4.4289289 -4.4289341 -4.4289393 -4.4289422 -4.4289417 -4.428937 -4.4289351 -4.4289446 -4.4289608 -4.4289756 -4.4289823 -4.4289808 -4.4289751 -4.4289675][-4.4289527 -4.4289536 -4.4289546 -4.4289575 -4.4289603 -4.4289622 -4.4289608 -4.4289594 -4.4289618 -4.4289665 -4.4289722 -4.4289746 -4.4289742 -4.4289727 -4.42897]]...]
INFO - root - 2017-12-08 08:07:02.565507: step 51410, loss = 2.28, batch loss = 2.23 (35.0 examples/sec; 0.228 sec/batch; 17h:49m:53s remains)
INFO - root - 2017-12-08 08:07:04.808945: step 51420, loss = 2.28, batch loss = 2.23 (36.6 examples/sec; 0.219 sec/batch; 17h:04m:17s remains)
INFO - root - 2017-12-08 08:07:07.035406: step 51430, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 17h:10m:14s remains)
INFO - root - 2017-12-08 08:07:09.293500: step 51440, loss = 2.28, batch loss = 2.23 (34.8 examples/sec; 0.230 sec/batch; 17h:58m:18s remains)
INFO - root - 2017-12-08 08:07:11.536029: step 51450, loss = 2.28, batch loss = 2.23 (35.6 examples/sec; 0.224 sec/batch; 17h:31m:27s remains)
INFO - root - 2017-12-08 08:07:13.797726: step 51460, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.220 sec/batch; 17h:11m:52s remains)
INFO - root - 2017-12-08 08:07:16.057444: step 51470, loss = 2.28, batch loss = 2.23 (35.0 examples/sec; 0.229 sec/batch; 17h:51m:14s remains)
INFO - root - 2017-12-08 08:07:18.297613: step 51480, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 17h:22m:35s remains)
INFO - root - 2017-12-08 08:07:20.532280: step 51490, loss = 2.28, batch loss = 2.23 (35.8 examples/sec; 0.223 sec/batch; 17h:25m:30s remains)
INFO - root - 2017-12-08 08:07:22.772172: step 51500, loss = 2.28, batch loss = 2.23 (35.1 examples/sec; 0.228 sec/batch; 17h:47m:12s remains)
2017-12-08 08:07:23.078766: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.428586 -4.4286165 -4.42866 -4.4286814 -4.4286718 -4.4286871 -4.428731 -4.42877 -4.4287915 -4.4288044 -4.4288206 -4.4288406 -4.4288564 -4.4288478 -4.4288073][-4.4285483 -4.4285727 -4.4286141 -4.4286404 -4.4286408 -4.4286737 -4.4287314 -4.428772 -4.4287968 -4.4288187 -4.4288459 -4.4288769 -4.4288945 -4.42889 -4.428865][-4.4285793 -4.4285712 -4.4285688 -4.4285655 -4.428565 -4.428618 -4.4287062 -4.4287753 -4.4288173 -4.4288454 -4.4288754 -4.4289083 -4.4289179 -4.4289165 -4.4289083][-4.4286523 -4.4285994 -4.4285226 -4.4284668 -4.4284558 -4.4285188 -4.4286318 -4.4287305 -4.42879 -4.4288292 -4.4288688 -4.4289093 -4.4289184 -4.4289169 -4.4289136][-4.4287291 -4.4286323 -4.4284911 -4.4283867 -4.4283638 -4.4284325 -4.4285541 -4.4286642 -4.4287257 -4.4287705 -4.428823 -4.42887 -4.4288845 -4.4288816 -4.4288797][-4.4287763 -4.4286575 -4.428493 -4.4283681 -4.4283357 -4.4284 -4.428515 -4.4286113 -4.4286604 -4.4287033 -4.4287629 -4.4288168 -4.4288344 -4.4288316 -4.4288239][-4.4287987 -4.4286866 -4.4285512 -4.4284577 -4.4284315 -4.4284754 -4.4285469 -4.4285884 -4.4285941 -4.42862 -4.4286771 -4.4287424 -4.4287763 -4.4287744 -4.4287581][-4.4288316 -4.4287567 -4.4286752 -4.42862 -4.4286003 -4.4286065 -4.428617 -4.4285965 -4.4285488 -4.4285421 -4.4285922 -4.4286685 -4.4287229 -4.4287305 -4.4287105][-4.4288626 -4.4288282 -4.428792 -4.4287629 -4.4287438 -4.4287248 -4.4286938 -4.4286404 -4.4285707 -4.4285502 -4.428587 -4.4286551 -4.4287071 -4.4287148 -4.4287][-4.4288559 -4.4288492 -4.4288487 -4.4288492 -4.4288483 -4.4288287 -4.42879 -4.4287376 -4.4286733 -4.4286509 -4.4286685 -4.4287009 -4.4287252 -4.4287262 -4.428719][-4.4288311 -4.4288449 -4.4288697 -4.4288912 -4.4289002 -4.4288845 -4.4288573 -4.4288263 -4.428781 -4.42876 -4.4287696 -4.428782 -4.4287863 -4.42878 -4.4287729][-4.4288063 -4.4288278 -4.428854 -4.4288697 -4.4288735 -4.428864 -4.4288497 -4.4288397 -4.4288244 -4.4288197 -4.4288287 -4.42884 -4.4288468 -4.4288473 -4.4288449][-4.4287939 -4.4288039 -4.4288163 -4.4288187 -4.4288158 -4.4288063 -4.4287953 -4.4287987 -4.4288092 -4.4288239 -4.4288363 -4.4288511 -4.4288712 -4.4288869 -4.4288983][-4.428781 -4.4287772 -4.4287734 -4.4287615 -4.4287443 -4.4287333 -4.4287329 -4.428751 -4.4287696 -4.4287877 -4.4288073 -4.4288373 -4.4288726 -4.4289021 -4.428925][-4.428772 -4.428761 -4.428741 -4.4287105 -4.4286823 -4.4286804 -4.4287028 -4.4287353 -4.4287534 -4.4287677 -4.428793 -4.4288321 -4.4288759 -4.4289079 -4.4289284]]...]
INFO - root - 2017-12-08 08:07:25.327291: step 51510, loss = 2.28, batch loss = 2.23 (35.3 examples/sec; 0.227 sec/batch; 17h:40m:45s remains)
INFO - root - 2017-12-08 08:07:27.571853: step 51520, loss = 2.28, batch loss = 2.23 (33.1 examples/sec; 0.241 sec/batch; 18h:50m:31s remains)
INFO - root - 2017-12-08 08:07:29.815023: step 51530, loss = 2.28, batch loss = 2.23 (35.8 examples/sec; 0.223 sec/batch; 17h:25m:56s remains)
INFO - root - 2017-12-08 08:07:32.040881: step 51540, loss = 2.28, batch loss = 2.23 (36.7 examples/sec; 0.218 sec/batch; 17h:02m:00s remains)
INFO - root - 2017-12-08 08:07:34.299644: step 51550, loss = 2.28, batch loss = 2.23 (35.8 examples/sec; 0.224 sec/batch; 17h:27m:48s remains)
INFO - root - 2017-12-08 08:07:36.566828: step 51560, loss = 2.28, batch loss = 2.23 (35.1 examples/sec; 0.228 sec/batch; 17h:46m:23s remains)
INFO - root - 2017-12-08 08:07:38.818122: step 51570, loss = 2.28, batch loss = 2.23 (35.1 examples/sec; 0.228 sec/batch; 17h:47m:59s remains)
INFO - root - 2017-12-08 08:07:41.089869: step 51580, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 17h:05m:49s remains)
INFO - root - 2017-12-08 08:07:43.358284: step 51590, loss = 2.28, batch loss = 2.23 (34.8 examples/sec; 0.230 sec/batch; 17h:56m:09s remains)
INFO - root - 2017-12-08 08:07:45.585768: step 51600, loss = 2.28, batch loss = 2.23 (36.0 examples/sec; 0.222 sec/batch; 17h:20m:30s remains)
2017-12-08 08:07:45.861545: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288969 -4.42893 -4.4289303 -4.4289122 -4.4288883 -4.428863 -4.428834 -4.428813 -4.4288158 -4.4288216 -4.4288187 -4.4288025 -4.428781 -4.4287663 -4.4287572][-4.4289 -4.4289389 -4.4289446 -4.4289308 -4.4289083 -4.4288764 -4.428843 -4.4288282 -4.4288487 -4.4288707 -4.4288683 -4.4288363 -4.4288025 -4.4287786 -4.4287591][-4.4288836 -4.42892 -4.4289279 -4.4289227 -4.4289045 -4.4288573 -4.4288025 -4.4287829 -4.4288211 -4.4288673 -4.4288721 -4.4288335 -4.4287944 -4.4287667 -4.4287362][-4.4288697 -4.4288926 -4.4288983 -4.4288993 -4.4288778 -4.4288077 -4.4287195 -4.4286857 -4.42874 -4.4288216 -4.4288564 -4.4288373 -4.4288058 -4.4287705 -4.4287214][-4.4288683 -4.4288774 -4.42888 -4.428874 -4.4288363 -4.4287348 -4.4286036 -4.428544 -4.428616 -4.4287486 -4.4288344 -4.4288535 -4.4288359 -4.4287934 -4.4287281][-4.4288673 -4.4288697 -4.428863 -4.4288373 -4.4287691 -4.428628 -4.4284368 -4.4283295 -4.4284282 -4.428628 -4.4287682 -4.4288259 -4.4288287 -4.4287934 -4.4287314][-4.4288697 -4.4288697 -4.428854 -4.4288077 -4.4287086 -4.4285226 -4.4282556 -4.428062 -4.428185 -4.428472 -4.4286733 -4.428762 -4.4287863 -4.4287715 -4.4287305][-4.4288878 -4.4288883 -4.4288707 -4.42881 -4.4286866 -4.4284735 -4.4281616 -4.4278946 -4.4280133 -4.4283566 -4.4285994 -4.4287076 -4.4287486 -4.4287581 -4.4287434][-4.428906 -4.4289131 -4.428906 -4.4288507 -4.4287267 -4.428525 -4.4282565 -4.4280238 -4.4280782 -4.4283543 -4.4285822 -4.4286966 -4.4287457 -4.4287686 -4.4287772][-4.42891 -4.4289179 -4.428926 -4.4288974 -4.4288044 -4.4286513 -4.4284658 -4.4283023 -4.428288 -4.4284363 -4.4286113 -4.428719 -4.428771 -4.4287944 -4.4288039][-4.428885 -4.4288921 -4.4289112 -4.4289141 -4.4288635 -4.4287672 -4.4286494 -4.4285364 -4.4284849 -4.4285359 -4.42865 -4.4287448 -4.4287977 -4.4288197 -4.4288173][-4.4288273 -4.4288344 -4.4288683 -4.4288988 -4.4288888 -4.4288406 -4.4287715 -4.42869 -4.4286318 -4.4286413 -4.4287076 -4.4287772 -4.4288187 -4.4288335 -4.4288225][-4.4287314 -4.4287415 -4.4287958 -4.4288597 -4.4288878 -4.4288783 -4.4288478 -4.4287977 -4.428751 -4.4287486 -4.4287844 -4.428823 -4.428844 -4.4288511 -4.4288368][-4.4286218 -4.4286442 -4.4287248 -4.428822 -4.4288826 -4.4289017 -4.4288969 -4.4288707 -4.4288435 -4.42884 -4.4288592 -4.428874 -4.4288764 -4.4288721 -4.4288578][-4.4285207 -4.4285622 -4.428669 -4.4287891 -4.4288697 -4.4289074 -4.4289141 -4.4289002 -4.4288898 -4.4288931 -4.4289093 -4.4289165 -4.4289117 -4.4289007 -4.4288878]]...]
INFO - root - 2017-12-08 08:07:48.066621: step 51610, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.221 sec/batch; 17h:13m:02s remains)
INFO - root - 2017-12-08 08:07:50.297349: step 51620, loss = 2.28, batch loss = 2.23 (36.7 examples/sec; 0.218 sec/batch; 16h:59m:59s remains)
INFO - root - 2017-12-08 08:07:52.523821: step 51630, loss = 2.28, batch loss = 2.23 (34.2 examples/sec; 0.234 sec/batch; 18h:16m:06s remains)
INFO - root - 2017-12-08 08:07:54.793726: step 51640, loss = 2.28, batch loss = 2.23 (34.2 examples/sec; 0.234 sec/batch; 18h:13m:55s remains)
INFO - root - 2017-12-08 08:07:57.012646: step 51650, loss = 2.28, batch loss = 2.23 (37.5 examples/sec; 0.214 sec/batch; 16h:39m:52s remains)
INFO - root - 2017-12-08 08:07:59.266828: step 51660, loss = 2.28, batch loss = 2.23 (36.8 examples/sec; 0.217 sec/batch; 16h:56m:25s remains)
INFO - root - 2017-12-08 08:08:01.510743: step 51670, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.222 sec/batch; 17h:17m:50s remains)
INFO - root - 2017-12-08 08:08:03.724693: step 51680, loss = 2.28, batch loss = 2.23 (37.5 examples/sec; 0.213 sec/batch; 16h:38m:28s remains)
INFO - root - 2017-12-08 08:08:05.963271: step 51690, loss = 2.28, batch loss = 2.23 (34.3 examples/sec; 0.233 sec/batch; 18h:11m:10s remains)
INFO - root - 2017-12-08 08:08:08.209861: step 51700, loss = 2.28, batch loss = 2.23 (34.4 examples/sec; 0.232 sec/batch; 18h:07m:32s remains)
2017-12-08 08:08:08.550128: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289594 -4.42896 -4.4289618 -4.4289608 -4.4289584 -4.4289565 -4.4289517 -4.4289489 -4.428946 -4.4289403 -4.4289322 -4.428926 -4.4289227 -4.4289217 -4.4289236][-4.4289541 -4.4289589 -4.4289689 -4.42897 -4.4289632 -4.42896 -4.4289536 -4.4289479 -4.4289432 -4.4289351 -4.4289212 -4.4289069 -4.4288979 -4.4288955 -4.4288979][-4.4289379 -4.4289389 -4.4289513 -4.4289503 -4.4289355 -4.4289284 -4.4289217 -4.4289165 -4.4289126 -4.4289021 -4.4288845 -4.4288683 -4.4288573 -4.4288564 -4.4288611][-4.4289031 -4.428894 -4.4289012 -4.428894 -4.4288664 -4.4288573 -4.4288526 -4.428854 -4.4288559 -4.4288487 -4.4288287 -4.4288092 -4.4287977 -4.4287996 -4.4288073][-4.4288445 -4.4288239 -4.4288192 -4.4288039 -4.4287663 -4.4287429 -4.4287267 -4.428731 -4.4287567 -4.4287696 -4.4287567 -4.4287329 -4.4287148 -4.4287195 -4.4287376][-4.4287839 -4.4287524 -4.4287391 -4.4287167 -4.4286675 -4.4286113 -4.42856 -4.4285483 -4.4286041 -4.4286518 -4.4286571 -4.4286394 -4.4286137 -4.4286189 -4.4286537][-4.4287448 -4.4287009 -4.42868 -4.4286494 -4.4285812 -4.4284916 -4.4283776 -4.4282923 -4.4283619 -4.4284811 -4.4285431 -4.4285603 -4.4285407 -4.428545 -4.4285903][-4.4287214 -4.4286628 -4.4286332 -4.4285946 -4.428504 -4.4283829 -4.4282 -4.4280005 -4.4280548 -4.4282727 -4.428431 -4.4285135 -4.4285188 -4.4285212 -4.428566][-4.4287419 -4.4286847 -4.42866 -4.4286294 -4.4285398 -4.4284186 -4.4282446 -4.4280286 -4.4280238 -4.4282322 -4.4284067 -4.42851 -4.4285331 -4.4285407 -4.4285822][-4.4287915 -4.4287534 -4.4287395 -4.4287252 -4.4286523 -4.4285579 -4.4284554 -4.4283438 -4.4283395 -4.42844 -4.4285159 -4.4285603 -4.4285617 -4.4285727 -4.428616][-4.42883 -4.4288092 -4.4288054 -4.4287977 -4.4287372 -4.4286594 -4.4286065 -4.4285731 -4.4285879 -4.4286389 -4.4286518 -4.428638 -4.4286127 -4.4286165 -4.4286566][-4.4288592 -4.428853 -4.4288554 -4.4288521 -4.4288034 -4.4287386 -4.4287057 -4.4287 -4.4287243 -4.4287615 -4.4287672 -4.4287453 -4.428719 -4.4287138 -4.4287395][-4.4288826 -4.428884 -4.4288874 -4.42889 -4.4288564 -4.4288054 -4.4287887 -4.4287915 -4.4288096 -4.4288363 -4.4288445 -4.4288363 -4.4288254 -4.4288239 -4.4288359][-4.4289041 -4.428906 -4.4289083 -4.4289169 -4.4288964 -4.4288607 -4.4288554 -4.4288621 -4.4288754 -4.4288931 -4.4288993 -4.4288969 -4.4288926 -4.428894 -4.4288993][-4.4289131 -4.4289141 -4.4289155 -4.428926 -4.4289179 -4.4288988 -4.4288974 -4.4289021 -4.4289122 -4.4289236 -4.4289284 -4.4289312 -4.4289308 -4.4289293 -4.4289279]]...]
INFO - root - 2017-12-08 08:08:10.766573: step 51710, loss = 2.28, batch loss = 2.23 (36.0 examples/sec; 0.222 sec/batch; 17h:18m:45s remains)
INFO - root - 2017-12-08 08:08:12.999719: step 51720, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 17h:05m:12s remains)
INFO - root - 2017-12-08 08:08:15.268153: step 51730, loss = 2.28, batch loss = 2.23 (35.4 examples/sec; 0.226 sec/batch; 17h:38m:06s remains)
INFO - root - 2017-12-08 08:08:17.544684: step 51740, loss = 2.28, batch loss = 2.23 (34.0 examples/sec; 0.235 sec/batch; 18h:21m:01s remains)
INFO - root - 2017-12-08 08:08:19.782382: step 51750, loss = 2.28, batch loss = 2.23 (34.9 examples/sec; 0.229 sec/batch; 17h:52m:13s remains)
INFO - root - 2017-12-08 08:08:22.029922: step 51760, loss = 2.28, batch loss = 2.23 (35.4 examples/sec; 0.226 sec/batch; 17h:35m:55s remains)
INFO - root - 2017-12-08 08:08:24.273098: step 51770, loss = 2.28, batch loss = 2.23 (36.6 examples/sec; 0.219 sec/batch; 17h:03m:13s remains)
INFO - root - 2017-12-08 08:08:26.503711: step 51780, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 17h:06m:31s remains)
INFO - root - 2017-12-08 08:08:28.743510: step 51790, loss = 2.28, batch loss = 2.23 (35.8 examples/sec; 0.224 sec/batch; 17h:25m:38s remains)
INFO - root - 2017-12-08 08:08:30.961704: step 51800, loss = 2.28, batch loss = 2.23 (35.1 examples/sec; 0.228 sec/batch; 17h:47m:16s remains)
2017-12-08 08:08:31.259903: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289565 -4.4289522 -4.4289427 -4.42892 -4.4288831 -4.4288421 -4.4288049 -4.4287925 -4.4288116 -4.4288464 -4.4288611 -4.4288468 -4.4288163 -4.4288168 -4.4288368][-4.4289322 -4.4289289 -4.4289217 -4.4288979 -4.4288487 -4.4287853 -4.42872 -4.428689 -4.4287095 -4.4287467 -4.428762 -4.4287605 -4.4287486 -4.4287748 -4.4288235][-4.4288764 -4.4288673 -4.4288659 -4.4288526 -4.4288087 -4.4287376 -4.4286528 -4.4286046 -4.4286208 -4.4286556 -4.4286652 -4.4286718 -4.4286723 -4.4287128 -4.4287829][-4.4287982 -4.4287667 -4.42876 -4.4287624 -4.4287448 -4.4286923 -4.4286213 -4.4285746 -4.4285841 -4.4286051 -4.4285913 -4.428586 -4.42858 -4.4286189 -4.4286985][-4.4287567 -4.4287081 -4.4286809 -4.4286776 -4.4286776 -4.4286585 -4.4286251 -4.4285932 -4.4285951 -4.4286075 -4.4285722 -4.4285355 -4.4285007 -4.428524 -4.4286032][-4.4287524 -4.4287152 -4.4286757 -4.4286513 -4.4286432 -4.4286413 -4.4286432 -4.4286361 -4.4286375 -4.4286437 -4.4286017 -4.4285507 -4.4284997 -4.4285097 -4.4285784][-4.4287639 -4.4287577 -4.4287333 -4.4286976 -4.428668 -4.4286532 -4.4286647 -4.4286766 -4.4286814 -4.4286871 -4.4286532 -4.4286032 -4.4285522 -4.428566 -4.428628][-4.4287596 -4.428792 -4.428802 -4.4287839 -4.4287462 -4.4287119 -4.4287109 -4.4287271 -4.4287367 -4.4287434 -4.4287157 -4.4286633 -4.4286051 -4.42862 -4.4286795][-4.4287186 -4.4287734 -4.4288092 -4.4288211 -4.4288063 -4.4287796 -4.4287772 -4.4287868 -4.4287968 -4.4288106 -4.428791 -4.4287333 -4.4286652 -4.4286695 -4.4287162][-4.4286795 -4.4287267 -4.4287658 -4.4287891 -4.4287963 -4.42879 -4.4287934 -4.428803 -4.4288182 -4.428844 -4.428844 -4.4287963 -4.4287295 -4.4287219 -4.4287472][-4.4286819 -4.4287081 -4.4287324 -4.4287519 -4.4287672 -4.4287729 -4.4287786 -4.4287839 -4.4288015 -4.4288387 -4.4288554 -4.4288306 -4.4287853 -4.428781 -4.4287925][-4.4286966 -4.4287119 -4.4287248 -4.4287357 -4.4287472 -4.4287558 -4.4287577 -4.4287581 -4.4287691 -4.4288044 -4.428834 -4.42883 -4.4288068 -4.4288173 -4.428833][-4.4287162 -4.4287243 -4.4287295 -4.428731 -4.4287267 -4.4287214 -4.4287171 -4.4287252 -4.4287457 -4.4287772 -4.4288087 -4.428812 -4.4287992 -4.4288163 -4.4288387][-4.4287543 -4.4287591 -4.4287624 -4.4287577 -4.4287362 -4.4287057 -4.4286847 -4.4286942 -4.4287205 -4.4287472 -4.4287696 -4.4287705 -4.4287562 -4.4287786 -4.4288135][-4.428793 -4.4287982 -4.4288011 -4.4287915 -4.428762 -4.4287171 -4.4286819 -4.4286861 -4.4287086 -4.4287238 -4.4287286 -4.4287033 -4.4286613 -4.4286833 -4.4287405]]...]
INFO - root - 2017-12-08 08:08:33.463123: step 51810, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 17h:07m:55s remains)
INFO - root - 2017-12-08 08:08:35.691698: step 51820, loss = 2.28, batch loss = 2.23 (37.0 examples/sec; 0.216 sec/batch; 16h:50m:49s remains)
INFO - root - 2017-12-08 08:08:37.966889: step 51830, loss = 2.28, batch loss = 2.23 (34.0 examples/sec; 0.235 sec/batch; 18h:20m:52s remains)
INFO - root - 2017-12-08 08:08:40.228032: step 51840, loss = 2.28, batch loss = 2.23 (36.9 examples/sec; 0.217 sec/batch; 16h:54m:58s remains)
INFO - root - 2017-12-08 08:08:42.492113: step 51850, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 17h:12m:30s remains)
INFO - root - 2017-12-08 08:08:44.712776: step 51860, loss = 2.28, batch loss = 2.23 (35.8 examples/sec; 0.223 sec/batch; 17h:24m:20s remains)
INFO - root - 2017-12-08 08:08:46.990220: step 51870, loss = 2.28, batch loss = 2.23 (35.4 examples/sec; 0.226 sec/batch; 17h:35m:40s remains)
INFO - root - 2017-12-08 08:08:49.211207: step 51880, loss = 2.28, batch loss = 2.23 (36.0 examples/sec; 0.222 sec/batch; 17h:19m:51s remains)
INFO - root - 2017-12-08 08:08:51.438126: step 51890, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 17h:23m:12s remains)
INFO - root - 2017-12-08 08:08:53.675707: step 51900, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 17h:07m:24s remains)
2017-12-08 08:08:53.953259: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4286723 -4.4286461 -4.4286613 -4.4286838 -4.4286952 -4.4287119 -4.42874 -4.4287653 -4.4287825 -4.4288096 -4.4288168 -4.428781 -4.4287152 -4.428659 -4.4286504][-4.4286752 -4.4286456 -4.4286566 -4.4286723 -4.4286776 -4.42869 -4.4287262 -4.4287648 -4.428781 -4.4288025 -4.4288149 -4.4287906 -4.4287276 -4.4286704 -4.4286537][-4.4287128 -4.4286833 -4.428679 -4.4286776 -4.4286642 -4.4286575 -4.428689 -4.4287391 -4.4287672 -4.4287906 -4.4288011 -4.4287848 -4.4287281 -4.4286714 -4.4286509][-4.4287591 -4.4287286 -4.4287071 -4.4286866 -4.4286556 -4.428628 -4.4286437 -4.4286971 -4.4287472 -4.4287825 -4.4287882 -4.4287658 -4.4287171 -4.4286633 -4.4286394][-4.4287891 -4.428762 -4.4287343 -4.4287057 -4.4286695 -4.4286289 -4.4286208 -4.4286671 -4.4287262 -4.4287648 -4.4287663 -4.4287386 -4.428699 -4.4286528 -4.4286261][-4.4287968 -4.428781 -4.4287581 -4.4287324 -4.4286952 -4.4286485 -4.4286151 -4.4286366 -4.4286828 -4.4287124 -4.4287076 -4.428688 -4.4286728 -4.4286461 -4.4286284][-4.4287724 -4.42877 -4.4287572 -4.4287381 -4.4287019 -4.4286423 -4.4285855 -4.4285789 -4.428607 -4.4286308 -4.4286308 -4.4286361 -4.4286547 -4.4286671 -4.4286685][-4.4287219 -4.4287281 -4.4287281 -4.428719 -4.4286861 -4.4286184 -4.4285374 -4.4284992 -4.4285192 -4.4285564 -4.4285812 -4.4286146 -4.4286551 -4.4286952 -4.4287114][-4.4286551 -4.4286623 -4.4286757 -4.4286833 -4.4286623 -4.4285855 -4.4284816 -4.4284225 -4.4284492 -4.4285216 -4.4285812 -4.4286332 -4.4286742 -4.4287162 -4.4287343][-4.4286075 -4.4286079 -4.4286218 -4.428637 -4.4286275 -4.4285479 -4.4284382 -4.4283853 -4.4284368 -4.4285326 -4.4286127 -4.4286661 -4.428699 -4.42873 -4.4287438][-4.4286141 -4.4286075 -4.4286127 -4.4286208 -4.428618 -4.4285517 -4.4284635 -4.4284329 -4.42849 -4.4285784 -4.4286513 -4.4286976 -4.4287214 -4.4287424 -4.4287539][-4.42867 -4.4286623 -4.4286594 -4.4286661 -4.4286737 -4.4286313 -4.4285712 -4.4285531 -4.4285936 -4.4286513 -4.4286995 -4.4287314 -4.4287496 -4.4287682 -4.4287806][-4.4287353 -4.4287348 -4.4287362 -4.4287477 -4.4287586 -4.4287295 -4.4286861 -4.4286652 -4.4286876 -4.4287224 -4.4287481 -4.4287677 -4.4287858 -4.4288054 -4.4288144][-4.4287839 -4.4287872 -4.42879 -4.4288006 -4.4288087 -4.4287848 -4.4287558 -4.4287386 -4.4287529 -4.4287767 -4.4287891 -4.4287996 -4.4288154 -4.4288321 -4.4288354][-4.428803 -4.4288034 -4.4288135 -4.4288273 -4.4288349 -4.4288192 -4.4288011 -4.428791 -4.4288011 -4.4288125 -4.4288144 -4.4288149 -4.4288249 -4.4288363 -4.4288359]]...]
INFO - root - 2017-12-08 08:08:56.194426: step 51910, loss = 2.28, batch loss = 2.23 (35.6 examples/sec; 0.225 sec/batch; 17h:31m:01s remains)
INFO - root - 2017-12-08 08:08:58.442819: step 51920, loss = 2.28, batch loss = 2.23 (34.7 examples/sec; 0.231 sec/batch; 17h:58m:22s remains)
INFO - root - 2017-12-08 08:09:00.699855: step 51930, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.220 sec/batch; 17h:09m:48s remains)
INFO - root - 2017-12-08 08:09:02.942113: step 51940, loss = 2.28, batch loss = 2.23 (36.7 examples/sec; 0.218 sec/batch; 17h:00m:25s remains)
INFO - root - 2017-12-08 08:09:05.165570: step 51950, loss = 2.28, batch loss = 2.23 (36.0 examples/sec; 0.222 sec/batch; 17h:19m:04s remains)
INFO - root - 2017-12-08 08:09:07.418658: step 51960, loss = 2.28, batch loss = 2.23 (33.7 examples/sec; 0.237 sec/batch; 18h:30m:26s remains)
INFO - root - 2017-12-08 08:09:09.674056: step 51970, loss = 2.28, batch loss = 2.23 (37.2 examples/sec; 0.215 sec/batch; 16h:44m:54s remains)
INFO - root - 2017-12-08 08:09:11.908884: step 51980, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.222 sec/batch; 17h:15m:37s remains)
INFO - root - 2017-12-08 08:09:14.169674: step 51990, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.220 sec/batch; 17h:09m:34s remains)
INFO - root - 2017-12-08 08:09:16.392591: step 52000, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 17h:22m:01s remains)
2017-12-08 08:09:16.701880: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4287186 -4.42873 -4.4287367 -4.4287338 -4.4287295 -4.4287305 -4.4287357 -4.4287415 -4.4287448 -4.4287462 -4.4287481 -4.4287553 -4.4287667 -4.4287748 -4.4287796][-4.42868 -4.4286919 -4.4286971 -4.4286952 -4.4286962 -4.4287057 -4.42872 -4.4287333 -4.4287438 -4.42875 -4.4287558 -4.4287663 -4.4287791 -4.4287834 -4.428781][-4.4286809 -4.4286904 -4.4286938 -4.4286957 -4.4287043 -4.4287214 -4.42874 -4.4287553 -4.4287648 -4.4287686 -4.4287705 -4.4287772 -4.4287825 -4.4287744 -4.4287572][-4.4287105 -4.428721 -4.4287248 -4.4287267 -4.4287319 -4.4287395 -4.4287472 -4.42875 -4.4287491 -4.4287434 -4.4287362 -4.4287362 -4.4287357 -4.4287224 -4.4287009][-4.4287248 -4.4287381 -4.4287419 -4.4287395 -4.4287343 -4.4287233 -4.4287124 -4.4286976 -4.4286828 -4.42867 -4.4286594 -4.4286613 -4.4286671 -4.4286623 -4.4286528][-4.4287319 -4.42874 -4.4287386 -4.4287291 -4.4287114 -4.4286852 -4.4286609 -4.4286323 -4.4286089 -4.4285946 -4.4285893 -4.4285994 -4.428617 -4.428627 -4.4286351][-4.4287486 -4.4287472 -4.4287367 -4.4287186 -4.4286919 -4.4286585 -4.4286313 -4.4286022 -4.4285831 -4.42858 -4.4285865 -4.4286051 -4.4286289 -4.4286461 -4.4286637][-4.428762 -4.4287553 -4.4287405 -4.4287238 -4.4287014 -4.4286795 -4.4286661 -4.4286532 -4.4286494 -4.4286561 -4.4286671 -4.4286833 -4.4286995 -4.42871 -4.4287214][-4.4287834 -4.428771 -4.4287543 -4.4287424 -4.4287343 -4.4287324 -4.4287353 -4.4287353 -4.4287376 -4.4287424 -4.4287481 -4.4287534 -4.4287562 -4.4287562 -4.4287581][-4.4288034 -4.4287806 -4.4287558 -4.4287438 -4.4287486 -4.4287658 -4.4287834 -4.428791 -4.428792 -4.42879 -4.4287891 -4.4287872 -4.4287834 -4.4287786 -4.4287786][-4.4288206 -4.428792 -4.4287643 -4.4287534 -4.4287648 -4.4287896 -4.428812 -4.42882 -4.4288168 -4.4288096 -4.4288049 -4.4287996 -4.4287968 -4.4287949 -4.4287934][-4.4288378 -4.4288139 -4.4287906 -4.4287815 -4.42879 -4.4288106 -4.42883 -4.428834 -4.4288254 -4.4288154 -4.4288092 -4.4288034 -4.428803 -4.4288054 -4.4288006][-4.4288578 -4.4288373 -4.4288197 -4.428813 -4.4288173 -4.4288287 -4.4288416 -4.4288416 -4.42883 -4.4288216 -4.4288158 -4.4288116 -4.4288158 -4.4288177 -4.4288068][-4.4288731 -4.4288564 -4.4288483 -4.4288468 -4.4288487 -4.428854 -4.4288583 -4.4288516 -4.4288378 -4.4288321 -4.4288297 -4.4288273 -4.4288335 -4.4288316 -4.4288111][-4.4288869 -4.428875 -4.4288726 -4.428874 -4.4288735 -4.428874 -4.4288712 -4.4288611 -4.4288507 -4.4288535 -4.4288592 -4.42886 -4.4288616 -4.4288507 -4.4288187]]...]
INFO - root - 2017-12-08 08:09:18.898197: step 52010, loss = 2.28, batch loss = 2.23 (35.2 examples/sec; 0.228 sec/batch; 17h:43m:36s remains)
INFO - root - 2017-12-08 08:09:21.142961: step 52020, loss = 2.28, batch loss = 2.23 (35.6 examples/sec; 0.225 sec/batch; 17h:30m:14s remains)
INFO - root - 2017-12-08 08:09:23.375670: step 52030, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.225 sec/batch; 17h:31m:58s remains)
INFO - root - 2017-12-08 08:09:25.635625: step 52040, loss = 2.28, batch loss = 2.23 (33.8 examples/sec; 0.237 sec/batch; 18h:26m:48s remains)
INFO - root - 2017-12-08 08:09:27.864580: step 52050, loss = 2.28, batch loss = 2.23 (35.0 examples/sec; 0.229 sec/batch; 17h:49m:17s remains)
INFO - root - 2017-12-08 08:09:30.095439: step 52060, loss = 2.28, batch loss = 2.23 (34.8 examples/sec; 0.230 sec/batch; 17h:53m:53s remains)
INFO - root - 2017-12-08 08:09:32.347090: step 52070, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.221 sec/batch; 17h:14m:23s remains)
INFO - root - 2017-12-08 08:09:34.611173: step 52080, loss = 2.28, batch loss = 2.23 (35.0 examples/sec; 0.229 sec/batch; 17h:49m:16s remains)
INFO - root - 2017-12-08 08:09:36.885995: step 52090, loss = 2.28, batch loss = 2.23 (34.2 examples/sec; 0.234 sec/batch; 18h:12m:23s remains)
INFO - root - 2017-12-08 08:09:39.136553: step 52100, loss = 2.28, batch loss = 2.23 (35.6 examples/sec; 0.225 sec/batch; 17h:30m:33s remains)
2017-12-08 08:09:39.429387: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288025 -4.428844 -4.4288797 -4.4289002 -4.4288979 -4.4288673 -4.4288421 -4.4288344 -4.4288568 -4.4288945 -4.4289203 -4.428926 -4.4289131 -4.428894 -4.4288969][-4.4287705 -4.4288344 -4.4288874 -4.4289217 -4.4289165 -4.4288726 -4.4288273 -4.4288054 -4.4288335 -4.4288836 -4.4289141 -4.4289165 -4.4288936 -4.4288688 -4.4288783][-4.4287357 -4.4288235 -4.4288893 -4.4289293 -4.4289174 -4.4288559 -4.4287791 -4.4287486 -4.428793 -4.4288635 -4.4289007 -4.4288945 -4.4288497 -4.4288139 -4.4288349][-4.4287195 -4.4288192 -4.428884 -4.4289169 -4.4288945 -4.4288087 -4.4286947 -4.4286532 -4.4287248 -4.4288263 -4.4288783 -4.4288626 -4.4287872 -4.4287324 -4.4287634][-4.4287095 -4.428812 -4.4288754 -4.428895 -4.4288516 -4.428731 -4.4285769 -4.4285226 -4.428627 -4.428772 -4.4288454 -4.4288187 -4.4287171 -4.4286485 -4.4286914][-4.4287262 -4.4288197 -4.428875 -4.4288769 -4.4287987 -4.4286351 -4.4284334 -4.4283509 -4.4284911 -4.4287014 -4.4288068 -4.4287748 -4.4286585 -4.4285917 -4.4286489][-4.4287934 -4.4288664 -4.4289045 -4.4288869 -4.4287763 -4.4285645 -4.4282932 -4.4281459 -4.4283118 -4.4285955 -4.4287529 -4.4287381 -4.428628 -4.4285684 -4.4286332][-4.4288626 -4.4289165 -4.428947 -4.4289255 -4.4288068 -4.4285622 -4.4282112 -4.427949 -4.4281063 -4.4284582 -4.4286814 -4.4287143 -4.4286394 -4.4285932 -4.4286466][-4.4288907 -4.42894 -4.428977 -4.4289751 -4.4288759 -4.428637 -4.42827 -4.4279475 -4.4280391 -4.4283853 -4.4286423 -4.4287224 -4.4286957 -4.4286747 -4.428709][-4.4288759 -4.4289331 -4.4289813 -4.4290018 -4.4289417 -4.4287553 -4.4284444 -4.4281535 -4.4281678 -4.4284334 -4.4286714 -4.4287758 -4.4287858 -4.4287829 -4.428802][-4.428834 -4.4288917 -4.4289446 -4.4289818 -4.4289637 -4.4288397 -4.428606 -4.4283762 -4.4283533 -4.4285455 -4.4287472 -4.4288526 -4.42888 -4.4288869 -4.4288988][-4.4288116 -4.4288573 -4.4289041 -4.4289532 -4.4289689 -4.4289031 -4.4287524 -4.4285951 -4.4285645 -4.4286909 -4.4288397 -4.4289236 -4.4289508 -4.4289594 -4.428968][-4.4288383 -4.4288707 -4.4289055 -4.4289532 -4.4289846 -4.4289613 -4.4288769 -4.4287777 -4.4287467 -4.4288177 -4.4289179 -4.4289813 -4.4290018 -4.4290071 -4.429009][-4.428896 -4.4289231 -4.4289441 -4.4289789 -4.4290066 -4.4290037 -4.4289622 -4.4289069 -4.4288793 -4.4289103 -4.4289675 -4.4290104 -4.4290228 -4.4290261 -4.4290261][-4.4289374 -4.4289618 -4.4289742 -4.4289932 -4.4290109 -4.4290118 -4.4289937 -4.4289675 -4.4289517 -4.42896 -4.4289823 -4.4290013 -4.429008 -4.4290123 -4.4290156]]...]
INFO - root - 2017-12-08 08:09:41.647454: step 52110, loss = 2.28, batch loss = 2.23 (37.0 examples/sec; 0.216 sec/batch; 16h:49m:14s remains)
INFO - root - 2017-12-08 08:09:43.853729: step 52120, loss = 2.28, batch loss = 2.23 (35.0 examples/sec; 0.229 sec/batch; 17h:48m:16s remains)
INFO - root - 2017-12-08 08:09:46.077867: step 52130, loss = 2.28, batch loss = 2.23 (37.1 examples/sec; 0.216 sec/batch; 16h:47m:53s remains)
INFO - root - 2017-12-08 08:09:48.324719: step 52140, loss = 2.28, batch loss = 2.23 (35.7 examples/sec; 0.224 sec/batch; 17h:26m:29s remains)
INFO - root - 2017-12-08 08:09:50.545150: step 52150, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 17h:03m:14s remains)
INFO - root - 2017-12-08 08:09:52.784646: step 52160, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 17h:04m:55s remains)
INFO - root - 2017-12-08 08:09:55.046256: step 52170, loss = 2.28, batch loss = 2.23 (33.8 examples/sec; 0.237 sec/batch; 18h:25m:01s remains)
INFO - root - 2017-12-08 08:09:57.302122: step 52180, loss = 2.28, batch loss = 2.23 (36.8 examples/sec; 0.218 sec/batch; 16h:56m:17s remains)
INFO - root - 2017-12-08 08:09:59.565362: step 52190, loss = 2.28, batch loss = 2.23 (36.6 examples/sec; 0.218 sec/batch; 17h:00m:37s remains)
INFO - root - 2017-12-08 08:10:01.806957: step 52200, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 17h:03m:06s remains)
2017-12-08 08:10:02.138684: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289069 -4.4289107 -4.4289141 -4.42892 -4.4289026 -4.4288783 -4.4288688 -4.4288807 -4.428896 -4.4288974 -4.4288983 -4.4289041 -4.4289055 -4.4289041 -4.4289055][-4.4288745 -4.4288731 -4.428874 -4.4288816 -4.42887 -4.4288445 -4.4288282 -4.428834 -4.4288564 -4.4288778 -4.428894 -4.42891 -4.42891 -4.4289045 -4.4289026][-4.4288354 -4.4288282 -4.4288225 -4.4288325 -4.4288287 -4.4287982 -4.428762 -4.4287596 -4.4287863 -4.4288249 -4.4288588 -4.4288874 -4.4288931 -4.4288912 -4.4288912][-4.4288015 -4.4287953 -4.4287853 -4.4287953 -4.4288011 -4.4287658 -4.4287062 -4.4286885 -4.428719 -4.4287753 -4.4288254 -4.428865 -4.4288778 -4.428874 -4.4288726][-4.4288 -4.4287949 -4.428782 -4.42879 -4.4287944 -4.4287472 -4.4286604 -4.4286203 -4.4286685 -4.4287472 -4.4288092 -4.4288573 -4.4288836 -4.4288797 -4.4288664][-4.4288192 -4.4288182 -4.4288054 -4.4288092 -4.4287977 -4.4287195 -4.4285917 -4.4285145 -4.4285932 -4.4287138 -4.4287972 -4.4288549 -4.4288869 -4.4288883 -4.4288726][-4.4288239 -4.4288445 -4.428843 -4.4288349 -4.4287968 -4.4286771 -4.4284735 -4.4283328 -4.4284692 -4.4286518 -4.4287648 -4.4288344 -4.4288731 -4.4288845 -4.4288764][-4.4288211 -4.4288588 -4.4288692 -4.4288473 -4.428781 -4.4286294 -4.4283571 -4.4281521 -4.4283457 -4.428586 -4.4287176 -4.4287848 -4.4288268 -4.4288497 -4.4288478][-4.4288368 -4.4288754 -4.4288888 -4.4288568 -4.428782 -4.4286313 -4.4283743 -4.4281855 -4.4283571 -4.4285955 -4.4287176 -4.4287572 -4.4287829 -4.4288 -4.4288015][-4.4288721 -4.4288931 -4.4288898 -4.4288516 -4.4287848 -4.4286675 -4.428494 -4.4283814 -4.4284863 -4.428658 -4.4287472 -4.428762 -4.4287682 -4.4287782 -4.4287806][-4.4289055 -4.4289002 -4.4288764 -4.428844 -4.4287949 -4.42872 -4.4286251 -4.4285703 -4.4286261 -4.4287286 -4.4287882 -4.4287968 -4.4287934 -4.4287982 -4.4288006][-4.4289217 -4.4288983 -4.4288688 -4.4288478 -4.428823 -4.4287858 -4.4287395 -4.4287162 -4.4287472 -4.4287982 -4.4288311 -4.4288335 -4.4288292 -4.4288335 -4.4288335][-4.4289269 -4.4288964 -4.4288735 -4.4288673 -4.428864 -4.4288507 -4.42883 -4.4288168 -4.4288292 -4.42885 -4.428865 -4.42886 -4.4288545 -4.4288588 -4.4288592][-4.4289355 -4.42891 -4.428895 -4.4288979 -4.428905 -4.4289031 -4.428895 -4.4288845 -4.4288826 -4.4288888 -4.4288964 -4.42889 -4.4288845 -4.4288869 -4.4288883][-4.4289508 -4.4289355 -4.4289289 -4.4289365 -4.428947 -4.4289494 -4.4289479 -4.4289432 -4.4289351 -4.4289317 -4.4289327 -4.4289269 -4.428925 -4.4289284 -4.4289317]]...]
INFO - root - 2017-12-08 08:10:04.359366: step 52210, loss = 2.28, batch loss = 2.23 (35.7 examples/sec; 0.224 sec/batch; 17h:25m:43s remains)
INFO - root - 2017-12-08 08:10:06.587917: step 52220, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 17h:02m:28s remains)
INFO - root - 2017-12-08 08:10:08.852823: step 52230, loss = 2.28, batch loss = 2.23 (34.7 examples/sec; 0.231 sec/batch; 17h:58m:07s remains)
INFO - root - 2017-12-08 08:10:11.084298: step 52240, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 17h:12m:34s remains)
INFO - root - 2017-12-08 08:10:13.325495: step 52250, loss = 2.28, batch loss = 2.23 (32.5 examples/sec; 0.246 sec/batch; 19h:11m:20s remains)
INFO - root - 2017-12-08 08:10:15.578665: step 52260, loss = 2.28, batch loss = 2.23 (35.2 examples/sec; 0.227 sec/batch; 17h:41m:15s remains)
INFO - root - 2017-12-08 08:10:17.822537: step 52270, loss = 2.28, batch loss = 2.23 (36.6 examples/sec; 0.219 sec/batch; 17h:01m:40s remains)
INFO - root - 2017-12-08 08:10:20.053052: step 52280, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 17h:21m:13s remains)
INFO - root - 2017-12-08 08:10:22.292244: step 52290, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.220 sec/batch; 17h:08m:18s remains)
INFO - root - 2017-12-08 08:10:24.526526: step 52300, loss = 2.28, batch loss = 2.23 (37.2 examples/sec; 0.215 sec/batch; 16h:45m:30s remains)
2017-12-08 08:10:24.814626: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288754 -4.428885 -4.428885 -4.4288697 -4.4288416 -4.4288163 -4.4287763 -4.4287295 -4.4286847 -4.4286757 -4.4287057 -4.4287271 -4.42874 -4.4287367 -4.4287167][-4.4288239 -4.4288378 -4.4288397 -4.4288192 -4.4287782 -4.4287405 -4.4286857 -4.428627 -4.4285812 -4.4285727 -4.4286113 -4.4286366 -4.4286537 -4.4286551 -4.4286308][-4.42879 -4.4288015 -4.4288 -4.4287691 -4.4287119 -4.4286566 -4.428586 -4.428525 -4.4284854 -4.4284811 -4.42852 -4.4285488 -4.4285731 -4.4285789 -4.4285555][-4.4287629 -4.428771 -4.4287591 -4.428709 -4.4286337 -4.4285583 -4.4284773 -4.428422 -4.4283924 -4.4283953 -4.428431 -4.42846 -4.4284863 -4.4284959 -4.4284725][-4.4287114 -4.4287243 -4.4286976 -4.4286218 -4.4285283 -4.4284396 -4.4283609 -4.4283218 -4.4283166 -4.4283385 -4.4283767 -4.4284096 -4.4284444 -4.4284487 -4.4284239][-4.428638 -4.4286709 -4.4286389 -4.4285345 -4.4284248 -4.4283333 -4.4282556 -4.428216 -4.428236 -4.428297 -4.4283581 -4.42841 -4.4284596 -4.4284592 -4.4284306][-4.4286027 -4.4286561 -4.4286208 -4.4284968 -4.4283772 -4.4282765 -4.4281735 -4.4280915 -4.428122 -4.4282379 -4.428339 -4.4284182 -4.4284816 -4.4284797 -4.4284444][-4.4286041 -4.4286528 -4.4286175 -4.4284983 -4.4283972 -4.42829 -4.4281659 -4.4280405 -4.4280667 -4.4282165 -4.4283452 -4.4284382 -4.42851 -4.4285188 -4.4284925][-4.4286389 -4.4286723 -4.4286346 -4.4285431 -4.4284692 -4.4283805 -4.4282842 -4.4281988 -4.4282327 -4.4283504 -4.4284492 -4.4285107 -4.4285631 -4.4285679 -4.4285483][-4.428719 -4.4287386 -4.4287047 -4.42864 -4.4285793 -4.4284954 -4.4284182 -4.4283719 -4.4284024 -4.4284868 -4.4285507 -4.4285855 -4.428618 -4.4286132 -4.4285975][-4.4288106 -4.428823 -4.4287987 -4.4287596 -4.4287152 -4.4286437 -4.4285765 -4.4285364 -4.428555 -4.4286208 -4.428668 -4.4286914 -4.4287128 -4.4286957 -4.4286757][-4.4288855 -4.42889 -4.42887 -4.4288454 -4.4288177 -4.4287715 -4.4287267 -4.4286923 -4.4286981 -4.4287434 -4.4287772 -4.428792 -4.4288039 -4.42879 -4.428771][-4.4289122 -4.428915 -4.4289026 -4.4288907 -4.42888 -4.4288573 -4.428834 -4.4288163 -4.428813 -4.4288349 -4.4288526 -4.4288607 -4.428874 -4.4288712 -4.4288545][-4.4289026 -4.4289055 -4.428895 -4.4288874 -4.4288859 -4.4288793 -4.4288754 -4.4288745 -4.4288764 -4.4288893 -4.4288979 -4.4288988 -4.4289083 -4.4289069 -4.4288931][-4.4288921 -4.4288945 -4.4288855 -4.4288807 -4.4288812 -4.4288821 -4.4288874 -4.428894 -4.4288993 -4.4289079 -4.4289126 -4.4289122 -4.4289136 -4.4289107 -4.4289021]]...]
INFO - root - 2017-12-08 08:10:27.025504: step 52310, loss = 2.28, batch loss = 2.23 (34.9 examples/sec; 0.229 sec/batch; 17h:50m:56s remains)
INFO - root - 2017-12-08 08:10:29.287390: step 52320, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.221 sec/batch; 17h:10m:22s remains)
INFO - root - 2017-12-08 08:10:31.554442: step 52330, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.220 sec/batch; 17h:08m:28s remains)
INFO - root - 2017-12-08 08:10:33.774350: step 52340, loss = 2.28, batch loss = 2.23 (36.7 examples/sec; 0.218 sec/batch; 16h:58m:40s remains)
INFO - root - 2017-12-08 08:10:36.002140: step 52350, loss = 2.28, batch loss = 2.23 (36.7 examples/sec; 0.218 sec/batch; 16h:57m:20s remains)
INFO - root - 2017-12-08 08:10:38.226515: step 52360, loss = 2.28, batch loss = 2.23 (36.9 examples/sec; 0.217 sec/batch; 16h:51m:19s remains)
INFO - root - 2017-12-08 08:10:40.469312: step 52370, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 17h:19m:52s remains)
INFO - root - 2017-12-08 08:10:42.718061: step 52380, loss = 2.28, batch loss = 2.23 (34.5 examples/sec; 0.232 sec/batch; 18h:02m:30s remains)
INFO - root - 2017-12-08 08:10:44.935426: step 52390, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.222 sec/batch; 17h:15m:09s remains)
INFO - root - 2017-12-08 08:10:47.161850: step 52400, loss = 2.28, batch loss = 2.23 (35.2 examples/sec; 0.227 sec/batch; 17h:40m:33s remains)
2017-12-08 08:10:47.455910: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4287434 -4.428771 -4.4288015 -4.4288044 -4.4288011 -4.4288116 -4.42881 -4.4287891 -4.4287677 -4.4287567 -4.4287438 -4.4287214 -4.4287004 -4.4286876 -4.4286852][-4.4287329 -4.4287696 -4.4288054 -4.4288073 -4.4287982 -4.4287972 -4.4287868 -4.4287696 -4.428761 -4.4287639 -4.4287567 -4.4287353 -4.4287128 -4.4286938 -4.4286838][-4.4287319 -4.4287748 -4.4288106 -4.4288125 -4.4288015 -4.4287844 -4.4287596 -4.4287443 -4.4287496 -4.4287691 -4.4287786 -4.428761 -4.42873 -4.4287004 -4.42868][-4.4287381 -4.4287786 -4.4288049 -4.4288054 -4.42879 -4.4287543 -4.428709 -4.4286933 -4.4287133 -4.4287491 -4.4287691 -4.4287496 -4.4287105 -4.4286776 -4.428659][-4.4287443 -4.4287605 -4.428762 -4.4287524 -4.4287229 -4.4286585 -4.4285846 -4.4285679 -4.4286156 -4.4286828 -4.4287219 -4.4287033 -4.4286594 -4.4286323 -4.4286208][-4.4287553 -4.4287314 -4.4287004 -4.4286785 -4.4286323 -4.4285321 -4.4284205 -4.4284072 -4.4284983 -4.4286122 -4.4286757 -4.4286628 -4.428617 -4.4285965 -4.4285884][-4.4287677 -4.4287124 -4.4286637 -4.4286404 -4.4285874 -4.4284768 -4.4283581 -4.4283528 -4.4284663 -4.4286017 -4.4286752 -4.4286685 -4.4286304 -4.4286132 -4.428607][-4.4287734 -4.4287133 -4.4286766 -4.4286733 -4.4286442 -4.4285641 -4.4284782 -4.42848 -4.4285688 -4.4286723 -4.4287252 -4.4287152 -4.428678 -4.4286551 -4.4286542][-4.4287724 -4.4287252 -4.4287119 -4.428731 -4.4287338 -4.4286962 -4.4286494 -4.4286528 -4.4287062 -4.4287663 -4.4287906 -4.428771 -4.4287243 -4.4286866 -4.4286814][-4.4287596 -4.4287291 -4.4287357 -4.4287691 -4.4287958 -4.4287896 -4.4287672 -4.4287634 -4.4287891 -4.4288206 -4.428822 -4.4287925 -4.4287386 -4.4286928 -4.4286852][-4.4287524 -4.4287381 -4.4287524 -4.4287872 -4.42882 -4.4288259 -4.4288163 -4.4288092 -4.4288168 -4.428834 -4.4288263 -4.4288011 -4.4287577 -4.428719 -4.4287148][-4.4287758 -4.4287777 -4.428792 -4.4288177 -4.4288454 -4.428853 -4.4288478 -4.4288397 -4.4288383 -4.4288492 -4.4288421 -4.4288244 -4.4287906 -4.4287605 -4.428761][-4.4288206 -4.4288306 -4.428843 -4.42886 -4.4288821 -4.4288907 -4.4288907 -4.4288869 -4.4288826 -4.4288859 -4.428875 -4.4288545 -4.4288192 -4.4287891 -4.4287858][-4.4288445 -4.4288487 -4.4288521 -4.4288592 -4.4288716 -4.4288788 -4.428885 -4.42889 -4.428895 -4.4288964 -4.4288793 -4.4288497 -4.4288106 -4.4287825 -4.42878][-4.4288449 -4.4288359 -4.4288263 -4.428822 -4.4288249 -4.4288292 -4.4288383 -4.4288483 -4.4288568 -4.42886 -4.4288511 -4.4288311 -4.4288054 -4.4287891 -4.4287915]]...]
INFO - root - 2017-12-08 08:10:49.669774: step 52410, loss = 2.28, batch loss = 2.23 (35.8 examples/sec; 0.223 sec/batch; 17h:23m:02s remains)
INFO - root - 2017-12-08 08:10:51.916311: step 52420, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.225 sec/batch; 17h:31m:18s remains)
INFO - root - 2017-12-08 08:10:54.165697: step 52430, loss = 2.28, batch loss = 2.23 (34.7 examples/sec; 0.231 sec/batch; 17h:56m:37s remains)
INFO - root - 2017-12-08 08:10:56.392944: step 52440, loss = 2.28, batch loss = 2.23 (36.8 examples/sec; 0.217 sec/batch; 16h:55m:08s remains)
INFO - root - 2017-12-08 08:10:58.694236: step 52450, loss = 2.28, batch loss = 2.23 (32.5 examples/sec; 0.246 sec/batch; 19h:09m:46s remains)
INFO - root - 2017-12-08 08:11:00.916927: step 52460, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.221 sec/batch; 17h:09m:39s remains)
INFO - root - 2017-12-08 08:11:03.179040: step 52470, loss = 2.28, batch loss = 2.23 (34.1 examples/sec; 0.235 sec/batch; 18h:16m:26s remains)
INFO - root - 2017-12-08 08:11:05.414168: step 52480, loss = 2.28, batch loss = 2.23 (37.1 examples/sec; 0.216 sec/batch; 16h:46m:32s remains)
INFO - root - 2017-12-08 08:11:07.720607: step 52490, loss = 2.28, batch loss = 2.23 (33.9 examples/sec; 0.236 sec/batch; 18h:22m:27s remains)
INFO - root - 2017-12-08 08:11:09.979857: step 52500, loss = 2.28, batch loss = 2.23 (34.6 examples/sec; 0.232 sec/batch; 18h:00m:33s remains)
2017-12-08 08:11:10.321401: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289513 -4.4289427 -4.4289427 -4.4289489 -4.4289584 -4.4289589 -4.4289408 -4.4289169 -4.4288878 -4.4288607 -4.428844 -4.4288378 -4.4288411 -4.4288621 -4.4288812][-4.4289575 -4.42895 -4.4289465 -4.428947 -4.4289517 -4.4289503 -4.4289274 -4.4288945 -4.4288564 -4.42882 -4.4287977 -4.428792 -4.4287987 -4.428822 -4.4288406][-4.4289694 -4.428968 -4.4289627 -4.4289556 -4.4289484 -4.4289355 -4.4289026 -4.428863 -4.4288249 -4.4287906 -4.4287715 -4.4287691 -4.4287796 -4.4288015 -4.4288187][-4.4289713 -4.4289756 -4.4289708 -4.428957 -4.4289379 -4.4289107 -4.4288688 -4.4288278 -4.428791 -4.42876 -4.4287457 -4.4287524 -4.4287734 -4.4287934 -4.4288106][-4.4289613 -4.4289732 -4.4289694 -4.42895 -4.4289217 -4.4288826 -4.4288325 -4.4287896 -4.4287534 -4.4287262 -4.42872 -4.4287429 -4.4287748 -4.4287896 -4.4288058][-4.4289384 -4.428956 -4.4289532 -4.42893 -4.4288936 -4.428844 -4.4287872 -4.4287467 -4.4287167 -4.4286981 -4.4287033 -4.4287472 -4.42879 -4.428803 -4.4288197][-4.4289026 -4.4289203 -4.4289193 -4.4288936 -4.4288511 -4.4287953 -4.42874 -4.4287095 -4.4286928 -4.428688 -4.4287109 -4.42877 -4.4288158 -4.4288244 -4.4288363][-4.4288621 -4.4288745 -4.4288688 -4.4288383 -4.428793 -4.4287362 -4.4286876 -4.4286675 -4.4286571 -4.4286585 -4.4286923 -4.4287581 -4.4287996 -4.4288034 -4.4288154][-4.4288359 -4.42883 -4.4288111 -4.428772 -4.4287214 -4.4286633 -4.428618 -4.4285989 -4.4285822 -4.4285755 -4.4286108 -4.4286809 -4.4287176 -4.4287195 -4.4287405][-4.4288383 -4.4288158 -4.4287834 -4.4287329 -4.4286723 -4.4286118 -4.4285684 -4.4285436 -4.4285135 -4.428493 -4.4285216 -4.42859 -4.4286232 -4.4286265 -4.4286571][-4.4288678 -4.4288406 -4.4288054 -4.4287519 -4.4286914 -4.4286346 -4.4285951 -4.4285674 -4.4285336 -4.4285107 -4.4285374 -4.4285936 -4.42862 -4.4286261 -4.4286551][-4.4288979 -4.4288774 -4.4288521 -4.4288068 -4.4287558 -4.4287047 -4.4286685 -4.4286461 -4.4286222 -4.428607 -4.4286323 -4.4286723 -4.4286909 -4.4287014 -4.4287271][-4.4289088 -4.4288974 -4.4288831 -4.4288516 -4.4288144 -4.4287758 -4.4287515 -4.4287467 -4.4287453 -4.4287467 -4.4287691 -4.4287915 -4.4287953 -4.4287963 -4.4288106][-4.4288907 -4.4288826 -4.4288764 -4.4288616 -4.4288459 -4.4288282 -4.4288239 -4.4288387 -4.4288588 -4.4288721 -4.4288888 -4.4288969 -4.4288893 -4.4288759 -4.4288712][-4.428844 -4.4288335 -4.4288387 -4.428844 -4.4288464 -4.4288468 -4.4288611 -4.4288869 -4.4289126 -4.4289279 -4.4289384 -4.4289384 -4.4289246 -4.4289012 -4.4288821]]...]
INFO - root - 2017-12-08 08:11:12.559151: step 52510, loss = 2.28, batch loss = 2.23 (35.2 examples/sec; 0.227 sec/batch; 17h:41m:35s remains)
INFO - root - 2017-12-08 08:11:14.821764: step 52520, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.226 sec/batch; 17h:33m:01s remains)
INFO - root - 2017-12-08 08:11:17.074526: step 52530, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.221 sec/batch; 17h:13m:09s remains)
INFO - root - 2017-12-08 08:11:19.307672: step 52540, loss = 2.28, batch loss = 2.23 (34.6 examples/sec; 0.231 sec/batch; 17h:57m:38s remains)
INFO - root - 2017-12-08 08:11:21.545857: step 52550, loss = 2.28, batch loss = 2.23 (35.2 examples/sec; 0.227 sec/batch; 17h:39m:05s remains)
INFO - root - 2017-12-08 08:11:23.823000: step 52560, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.225 sec/batch; 17h:31m:05s remains)
INFO - root - 2017-12-08 08:11:26.082232: step 52570, loss = 2.28, batch loss = 2.23 (37.4 examples/sec; 0.214 sec/batch; 16h:37m:13s remains)
INFO - root - 2017-12-08 08:11:28.330625: step 52580, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 17h:01m:26s remains)
INFO - root - 2017-12-08 08:11:30.564514: step 52590, loss = 2.28, batch loss = 2.23 (35.6 examples/sec; 0.224 sec/batch; 17h:27m:03s remains)
INFO - root - 2017-12-08 08:11:32.787253: step 52600, loss = 2.28, batch loss = 2.23 (35.4 examples/sec; 0.226 sec/batch; 17h:33m:36s remains)
2017-12-08 08:11:33.106367: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289236 -4.4288878 -4.42886 -4.4288559 -4.4288712 -4.4289036 -4.4289351 -4.4289522 -4.4289613 -4.4289594 -4.428947 -4.4289269 -4.4289007 -4.4288764 -4.4288621][-4.4288588 -4.4287958 -4.4287457 -4.4287319 -4.4287515 -4.4288058 -4.4288626 -4.4288945 -4.4289122 -4.4289112 -4.4288898 -4.4288526 -4.4288039 -4.4287572 -4.4287267][-4.4287815 -4.428688 -4.4286065 -4.4285731 -4.4285884 -4.4286575 -4.4287333 -4.4287815 -4.4288125 -4.4288187 -4.4287949 -4.4287424 -4.4286685 -4.4285975 -4.4285474][-4.4287314 -4.4286175 -4.4285045 -4.4284368 -4.4284282 -4.4284968 -4.4285793 -4.4286389 -4.428689 -4.4287157 -4.4287081 -4.4286585 -4.4285755 -4.4284897 -4.4284253][-4.4287395 -4.428617 -4.4284773 -4.4283586 -4.4283032 -4.4283419 -4.4284067 -4.4284635 -4.4285407 -4.4286118 -4.4286489 -4.4286332 -4.4285712 -4.4285011 -4.4284444][-4.4287648 -4.428628 -4.4284534 -4.4282789 -4.4281645 -4.4281616 -4.4282012 -4.4282665 -4.4283762 -4.4284935 -4.42858 -4.4286127 -4.4285979 -4.4285722 -4.4285493][-4.4287915 -4.428638 -4.428432 -4.4282117 -4.4280486 -4.4280071 -4.428031 -4.4281144 -4.4282613 -4.4284182 -4.4285455 -4.4286256 -4.42866 -4.4286742 -4.428679][-4.4288125 -4.4286647 -4.4284663 -4.4282484 -4.4280682 -4.4279962 -4.4280062 -4.4281106 -4.4282846 -4.4284606 -4.4286051 -4.4287114 -4.4287705 -4.4288006 -4.4288144][-4.4288468 -4.4287167 -4.428545 -4.4283552 -4.42819 -4.4281192 -4.4281292 -4.4282346 -4.4284005 -4.4285626 -4.4286942 -4.4287949 -4.4288545 -4.4288831 -4.4289002][-4.4288754 -4.4287605 -4.428616 -4.4284687 -4.4283571 -4.4283233 -4.4283481 -4.4284291 -4.42855 -4.4286613 -4.4287491 -4.428822 -4.4288754 -4.4289088 -4.4289351][-4.4289021 -4.4288139 -4.4287038 -4.4286051 -4.428544 -4.42854 -4.42857 -4.4286222 -4.4286909 -4.4287443 -4.4287834 -4.4288282 -4.4288707 -4.4289041 -4.4289327][-4.4289389 -4.4288783 -4.4288058 -4.4287457 -4.4287114 -4.4287148 -4.4287381 -4.428762 -4.4287906 -4.42881 -4.4288244 -4.4288473 -4.4288745 -4.4289041 -4.4289279][-4.4289675 -4.4289303 -4.4288874 -4.4288497 -4.4288278 -4.4288278 -4.4288368 -4.4288445 -4.4288554 -4.428863 -4.4288664 -4.4288769 -4.4288912 -4.4289088 -4.4289207][-4.4289818 -4.4289584 -4.4289303 -4.4289064 -4.4288921 -4.4288888 -4.4288883 -4.4288869 -4.4288893 -4.4288955 -4.4289031 -4.4289117 -4.4289145 -4.4289174 -4.4289169][-4.4289875 -4.4289727 -4.4289532 -4.4289379 -4.4289293 -4.4289274 -4.4289227 -4.4289193 -4.4289217 -4.4289293 -4.4289393 -4.4289479 -4.4289465 -4.4289412 -4.4289336]]...]
INFO - root - 2017-12-08 08:11:35.335298: step 52610, loss = 2.28, batch loss = 2.23 (36.9 examples/sec; 0.217 sec/batch; 16h:52m:25s remains)
INFO - root - 2017-12-08 08:11:37.618370: step 52620, loss = 2.28, batch loss = 2.23 (34.9 examples/sec; 0.229 sec/batch; 17h:49m:08s remains)
INFO - root - 2017-12-08 08:11:39.884919: step 52630, loss = 2.28, batch loss = 2.23 (36.0 examples/sec; 0.222 sec/batch; 17h:16m:13s remains)
INFO - root - 2017-12-08 08:11:42.129306: step 52640, loss = 2.28, batch loss = 2.23 (34.6 examples/sec; 0.231 sec/batch; 17h:58m:39s remains)
INFO - root - 2017-12-08 08:11:44.365963: step 52650, loss = 2.28, batch loss = 2.23 (35.8 examples/sec; 0.224 sec/batch; 17h:23m:38s remains)
INFO - root - 2017-12-08 08:11:46.611118: step 52660, loss = 2.28, batch loss = 2.23 (36.6 examples/sec; 0.219 sec/batch; 16h:59m:50s remains)
INFO - root - 2017-12-08 08:11:48.854104: step 52670, loss = 2.28, batch loss = 2.23 (37.7 examples/sec; 0.212 sec/batch; 16h:29m:12s remains)
INFO - root - 2017-12-08 08:11:51.101201: step 52680, loss = 2.28, batch loss = 2.23 (34.7 examples/sec; 0.230 sec/batch; 17h:54m:53s remains)
INFO - root - 2017-12-08 08:11:53.356838: step 52690, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 17h:18m:42s remains)
INFO - root - 2017-12-08 08:11:55.587894: step 52700, loss = 2.28, batch loss = 2.23 (34.1 examples/sec; 0.235 sec/batch; 18h:14m:49s remains)
2017-12-08 08:11:55.900234: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288259 -4.428822 -4.4288154 -4.4287939 -4.4287667 -4.4287434 -4.4287286 -4.4287343 -4.4287639 -4.4288063 -4.4288435 -4.4288549 -4.428854 -4.4288568 -4.428854][-4.4288836 -4.4288797 -4.4288745 -4.4288521 -4.4288168 -4.4287748 -4.4287343 -4.4287219 -4.4287496 -4.4288077 -4.4288635 -4.4288912 -4.4289031 -4.4289083 -4.4288993][-4.4289131 -4.4289107 -4.4289021 -4.4288745 -4.4288235 -4.4287553 -4.428689 -4.4286623 -4.4286919 -4.4287629 -4.4288373 -4.4288845 -4.4289174 -4.4289374 -4.4289346][-4.428916 -4.4289069 -4.4288845 -4.4288454 -4.4287786 -4.4286952 -4.428627 -4.4286036 -4.4286318 -4.4286981 -4.4287758 -4.4288373 -4.4288917 -4.4289303 -4.4289403][-4.4288888 -4.4288688 -4.4288292 -4.4287839 -4.4287252 -4.4286623 -4.4286203 -4.4286132 -4.4286337 -4.4286737 -4.4287233 -4.4287734 -4.4288325 -4.428885 -4.4289107][-4.4288421 -4.4288077 -4.4287605 -4.4287252 -4.4286919 -4.4286637 -4.4286513 -4.4286566 -4.4286714 -4.4286842 -4.4286938 -4.4287081 -4.4287505 -4.4288054 -4.4288454][-4.428791 -4.4287438 -4.428699 -4.428678 -4.4286637 -4.4286556 -4.42866 -4.4286757 -4.42869 -4.4286885 -4.4286709 -4.4286532 -4.4286714 -4.4287186 -4.4287653][-4.4287391 -4.4286909 -4.428658 -4.4286451 -4.4286327 -4.428628 -4.4286442 -4.4286666 -4.4286809 -4.4286757 -4.4286451 -4.4286103 -4.4286132 -4.4286542 -4.4287057][-4.4286904 -4.4286523 -4.4286346 -4.4286246 -4.4286089 -4.4286032 -4.4286232 -4.4286461 -4.42866 -4.4286528 -4.4286151 -4.4285707 -4.428566 -4.4286089 -4.4286671][-4.4286704 -4.4286547 -4.4286585 -4.428658 -4.4286394 -4.4286227 -4.4286308 -4.4286432 -4.4286518 -4.4286427 -4.4286022 -4.4285488 -4.4285359 -4.4285803 -4.4286466][-4.4287024 -4.4287052 -4.4287262 -4.4287353 -4.4287167 -4.4286876 -4.4286757 -4.4286718 -4.4286718 -4.4286609 -4.4286213 -4.4285707 -4.4285517 -4.4285855 -4.4286475][-4.4287672 -4.4287739 -4.4288 -4.4288154 -4.4288034 -4.4287705 -4.4287457 -4.4287333 -4.4287271 -4.4287148 -4.4286785 -4.4286361 -4.4286146 -4.4286294 -4.4286709][-4.4288354 -4.428843 -4.4288707 -4.428895 -4.428895 -4.4288726 -4.4288468 -4.4288268 -4.4288139 -4.4287958 -4.4287596 -4.428721 -4.4286966 -4.4286933 -4.42871][-4.4288807 -4.428894 -4.4289255 -4.4289579 -4.4289694 -4.4289565 -4.4289336 -4.4289079 -4.4288878 -4.4288673 -4.4288378 -4.4288068 -4.42878 -4.4287667 -4.4287705][-4.4289012 -4.4289174 -4.428947 -4.428978 -4.4289913 -4.4289837 -4.4289675 -4.4289465 -4.4289289 -4.4289136 -4.4288936 -4.4288712 -4.4288483 -4.428833 -4.4288344]]...]
INFO - root - 2017-12-08 08:11:58.156903: step 52710, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.220 sec/batch; 17h:07m:19s remains)
INFO - root - 2017-12-08 08:12:00.442284: step 52720, loss = 2.28, batch loss = 2.23 (35.0 examples/sec; 0.229 sec/batch; 17h:46m:04s remains)
INFO - root - 2017-12-08 08:12:02.660397: step 52730, loss = 2.28, batch loss = 2.23 (35.6 examples/sec; 0.225 sec/batch; 17h:27m:12s remains)
INFO - root - 2017-12-08 08:12:04.895924: step 52740, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 17h:04m:52s remains)
INFO - root - 2017-12-08 08:12:07.173297: step 52750, loss = 2.28, batch loss = 2.23 (37.1 examples/sec; 0.216 sec/batch; 16h:46m:36s remains)
INFO - root - 2017-12-08 08:12:09.409096: step 52760, loss = 2.28, batch loss = 2.23 (36.9 examples/sec; 0.217 sec/batch; 16h:50m:25s remains)
INFO - root - 2017-12-08 08:12:11.652951: step 52770, loss = 2.28, batch loss = 2.23 (35.8 examples/sec; 0.224 sec/batch; 17h:22m:07s remains)
INFO - root - 2017-12-08 08:12:13.895859: step 52780, loss = 2.28, batch loss = 2.23 (37.5 examples/sec; 0.213 sec/batch; 16h:34m:56s remains)
INFO - root - 2017-12-08 08:12:16.129616: step 52790, loss = 2.28, batch loss = 2.23 (35.1 examples/sec; 0.228 sec/batch; 17h:42m:25s remains)
INFO - root - 2017-12-08 08:12:18.420316: step 52800, loss = 2.28, batch loss = 2.23 (35.8 examples/sec; 0.223 sec/batch; 17h:20m:40s remains)
2017-12-08 08:12:18.704235: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289932 -4.4289737 -4.4289513 -4.4289365 -4.4289188 -4.4288907 -4.4288745 -4.4288969 -4.4289312 -4.42895 -4.4289556 -4.4289603 -4.4289527 -4.4289446 -4.4289408][-4.4289923 -4.428966 -4.4289379 -4.428925 -4.4289083 -4.4288735 -4.4288473 -4.4288721 -4.4289241 -4.4289618 -4.4289708 -4.4289718 -4.4289541 -4.4289351 -4.4289222][-4.4289794 -4.4289455 -4.4289141 -4.4288931 -4.4288726 -4.4288378 -4.4288082 -4.428833 -4.4288979 -4.4289532 -4.4289632 -4.428956 -4.4289265 -4.428895 -4.4288735][-4.428966 -4.4289255 -4.4288864 -4.4288568 -4.4288254 -4.4287858 -4.4287496 -4.4287677 -4.4288421 -4.42891 -4.4289217 -4.4289088 -4.4288712 -4.4288306 -4.4288039][-4.4289603 -4.4289155 -4.4288673 -4.4288282 -4.4287834 -4.4287248 -4.4286618 -4.4286647 -4.4287472 -4.4288335 -4.4288578 -4.4288492 -4.4288106 -4.4287705 -4.428751][-4.42896 -4.4289117 -4.428854 -4.4287977 -4.428731 -4.4286418 -4.4285431 -4.428514 -4.4286118 -4.4287372 -4.4287915 -4.4287915 -4.42876 -4.42873 -4.4287248][-4.4289603 -4.4289122 -4.4288492 -4.4287767 -4.428679 -4.4285407 -4.4283757 -4.4282813 -4.4284015 -4.4285955 -4.428699 -4.4287176 -4.4286962 -4.4286723 -4.4286714][-4.4289536 -4.4289079 -4.4288464 -4.4287705 -4.4286594 -4.4284883 -4.4282579 -4.4280739 -4.4281893 -4.4284329 -4.4285836 -4.4286332 -4.4286284 -4.428607 -4.4286013][-4.4289374 -4.4288859 -4.4288239 -4.4287567 -4.4286656 -4.4285235 -4.4283166 -4.4281349 -4.4282026 -4.4283972 -4.42853 -4.428575 -4.4285779 -4.4285603 -4.4285388][-4.4289293 -4.4288754 -4.4288106 -4.4287491 -4.4286761 -4.428576 -4.4284334 -4.4283109 -4.4283466 -4.4284773 -4.4285789 -4.4286146 -4.4286156 -4.4285903 -4.4285436][-4.4289346 -4.42889 -4.4288368 -4.4287858 -4.4287271 -4.4286561 -4.4285574 -4.4284697 -4.4284773 -4.4285564 -4.4286361 -4.4286795 -4.428688 -4.4286618 -4.4285979][-4.4289446 -4.428915 -4.42888 -4.4288454 -4.4288025 -4.428751 -4.4286823 -4.4286118 -4.42859 -4.4286265 -4.42869 -4.4287343 -4.4287462 -4.4287167 -4.4286489][-4.4289575 -4.4289412 -4.4289227 -4.4289031 -4.4288764 -4.4288416 -4.4287934 -4.4287391 -4.4287076 -4.4287248 -4.4287763 -4.4288144 -4.4288187 -4.4287844 -4.4287214][-4.4289737 -4.4289684 -4.4289608 -4.4289513 -4.42894 -4.4289207 -4.4288878 -4.4288464 -4.4288206 -4.4288325 -4.4288692 -4.4288974 -4.4288969 -4.4288688 -4.428822][-4.4289794 -4.4289775 -4.4289732 -4.4289684 -4.4289641 -4.4289536 -4.4289341 -4.4289083 -4.4288888 -4.4288931 -4.428915 -4.4289327 -4.4289331 -4.4289193 -4.4288926]]...]
INFO - root - 2017-12-08 08:12:20.946547: step 52810, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.221 sec/batch; 17h:08m:19s remains)
INFO - root - 2017-12-08 08:12:23.188863: step 52820, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 17h:01m:36s remains)
INFO - root - 2017-12-08 08:12:25.444054: step 52830, loss = 2.28, batch loss = 2.23 (35.8 examples/sec; 0.224 sec/batch; 17h:22m:55s remains)
INFO - root - 2017-12-08 08:12:27.661039: step 52840, loss = 2.28, batch loss = 2.23 (36.9 examples/sec; 0.217 sec/batch; 16h:49m:33s remains)
INFO - root - 2017-12-08 08:12:29.898967: step 52850, loss = 2.28, batch loss = 2.23 (36.0 examples/sec; 0.222 sec/batch; 17h:14m:28s remains)
INFO - root - 2017-12-08 08:12:32.140597: step 52860, loss = 2.28, batch loss = 2.23 (36.0 examples/sec; 0.222 sec/batch; 17h:16m:33s remains)
INFO - root - 2017-12-08 08:12:34.368756: step 52870, loss = 2.28, batch loss = 2.23 (35.6 examples/sec; 0.225 sec/batch; 17h:26m:28s remains)
INFO - root - 2017-12-08 08:12:36.601356: step 52880, loss = 2.28, batch loss = 2.23 (34.8 examples/sec; 0.230 sec/batch; 17h:50m:21s remains)
INFO - root - 2017-12-08 08:12:38.848266: step 52890, loss = 2.28, batch loss = 2.23 (36.7 examples/sec; 0.218 sec/batch; 16h:56m:42s remains)
INFO - root - 2017-12-08 08:12:41.082775: step 52900, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.221 sec/batch; 17h:08m:00s remains)
2017-12-08 08:12:41.373320: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289541 -4.4289112 -4.42884 -4.4287438 -4.4286456 -4.4285583 -4.4284911 -4.4285197 -4.4285655 -4.4286051 -4.4286752 -4.4287486 -4.428782 -4.4287691 -4.4287257][-4.4289508 -4.4289007 -4.4288163 -4.4287095 -4.4285984 -4.4284911 -4.4284029 -4.4284472 -4.4285254 -4.4285712 -4.4286275 -4.4286804 -4.4286938 -4.4286752 -4.4286294][-4.4289441 -4.4288917 -4.4288044 -4.4287047 -4.4286017 -4.4284863 -4.4283776 -4.4284158 -4.4285 -4.4285469 -4.4285841 -4.4286127 -4.4286137 -4.4286079 -4.428586][-4.4289389 -4.4288898 -4.4288096 -4.4287205 -4.42863 -4.4285173 -4.428391 -4.4284005 -4.4284616 -4.4285035 -4.4285417 -4.4285626 -4.4285655 -4.4285831 -4.4285946][-4.4289417 -4.4288969 -4.428822 -4.4287405 -4.4286561 -4.4285364 -4.4283938 -4.4283614 -4.4283948 -4.4284549 -4.428514 -4.4285393 -4.428545 -4.4285717 -4.4286][-4.4289451 -4.4289036 -4.4288349 -4.4287572 -4.428678 -4.4285603 -4.428422 -4.4283657 -4.4283834 -4.4284611 -4.4285359 -4.4285593 -4.428555 -4.428565 -4.4285841][-4.4289527 -4.4289112 -4.428844 -4.428771 -4.4286952 -4.4285817 -4.4284625 -4.4284129 -4.4284391 -4.4285254 -4.4286036 -4.428616 -4.4285889 -4.428566 -4.4285684][-4.4289594 -4.4289179 -4.4288464 -4.4287739 -4.4286985 -4.4286 -4.4285131 -4.428494 -4.4285398 -4.42862 -4.4286728 -4.42866 -4.4286127 -4.4285665 -4.428556][-4.4289637 -4.4289193 -4.4288449 -4.4287715 -4.4287019 -4.4286246 -4.4285731 -4.42859 -4.4286418 -4.4286966 -4.4287076 -4.4286671 -4.4286094 -4.42855 -4.4285345][-4.4289641 -4.4289179 -4.4288416 -4.4287686 -4.4287081 -4.4286523 -4.4286256 -4.4286585 -4.4286976 -4.4287119 -4.4286842 -4.4286318 -4.4285688 -4.4285045 -4.4284906][-4.4289618 -4.4289184 -4.4288468 -4.4287753 -4.4287219 -4.4286804 -4.4286718 -4.428699 -4.4287043 -4.4286771 -4.4286237 -4.4285784 -4.4285231 -4.4284592 -4.4284472][-4.4289579 -4.4289212 -4.4288573 -4.4287848 -4.4287305 -4.4286966 -4.428699 -4.4287171 -4.4286923 -4.4286337 -4.4285722 -4.428555 -4.4285283 -4.4284768 -4.4284668][-4.4289479 -4.4289145 -4.4288516 -4.4287767 -4.42872 -4.4286904 -4.4287066 -4.4287329 -4.42871 -4.4286537 -4.4286013 -4.4286017 -4.4285965 -4.42856 -4.42855][-4.4289417 -4.4289131 -4.428853 -4.4287782 -4.4287205 -4.4286895 -4.4287128 -4.4287605 -4.4287586 -4.4287186 -4.428678 -4.4286866 -4.4286909 -4.4286652 -4.4286532][-4.428946 -4.4289241 -4.4288669 -4.42879 -4.4287271 -4.42869 -4.4287181 -4.4287868 -4.4288054 -4.4287815 -4.4287524 -4.4287615 -4.4287562 -4.4287305 -4.4287081]]...]
INFO - root - 2017-12-08 08:12:43.594531: step 52910, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 17h:01m:38s remains)
INFO - root - 2017-12-08 08:12:45.836372: step 52920, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.220 sec/batch; 17h:05m:54s remains)
INFO - root - 2017-12-08 08:12:48.092162: step 52930, loss = 2.28, batch loss = 2.23 (35.3 examples/sec; 0.227 sec/batch; 17h:35m:25s remains)
INFO - root - 2017-12-08 08:12:50.337898: step 52940, loss = 2.28, batch loss = 2.23 (35.7 examples/sec; 0.224 sec/batch; 17h:24m:39s remains)
INFO - root - 2017-12-08 08:12:52.553521: step 52950, loss = 2.28, batch loss = 2.23 (34.4 examples/sec; 0.233 sec/batch; 18h:03m:28s remains)
INFO - root - 2017-12-08 08:12:54.817068: step 52960, loss = 2.28, batch loss = 2.23 (35.1 examples/sec; 0.228 sec/batch; 17h:42m:31s remains)
INFO - root - 2017-12-08 08:12:57.069372: step 52970, loss = 2.28, batch loss = 2.23 (35.0 examples/sec; 0.229 sec/batch; 17h:44m:33s remains)
INFO - root - 2017-12-08 08:12:59.303059: step 52980, loss = 2.28, batch loss = 2.23 (36.6 examples/sec; 0.219 sec/batch; 16h:58m:08s remains)
INFO - root - 2017-12-08 08:13:01.526143: step 52990, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 17h:02m:48s remains)
INFO - root - 2017-12-08 08:13:03.768974: step 53000, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.220 sec/batch; 17h:06m:44s remains)
2017-12-08 08:13:04.100675: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4287224 -4.4287 -4.4286847 -4.4286814 -4.4286551 -4.428616 -4.4286122 -4.42864 -4.4287014 -4.42874 -4.428731 -4.4287066 -4.428647 -4.42857 -4.428534][-4.4287047 -4.4286904 -4.4286857 -4.4286938 -4.4286742 -4.4286351 -4.4286194 -4.4286356 -4.4286842 -4.4287167 -4.4287028 -4.4286871 -4.4286461 -4.4285731 -4.42854][-4.4286895 -4.4286895 -4.4286957 -4.4287014 -4.428678 -4.428638 -4.4286184 -4.4286356 -4.4286866 -4.428719 -4.4287162 -4.4287176 -4.4286876 -4.4286208 -4.4285731][-4.4286928 -4.4286985 -4.4287114 -4.4287052 -4.4286718 -4.4286265 -4.4286165 -4.4286523 -4.4287176 -4.4287596 -4.4287663 -4.4287763 -4.4287529 -4.428689 -4.4286342][-4.4287062 -4.4287081 -4.4287157 -4.4287019 -4.428658 -4.4286079 -4.4286065 -4.4286571 -4.4287343 -4.4287848 -4.4287958 -4.4288044 -4.4287796 -4.4287152 -4.428658][-4.4287167 -4.4287081 -4.4287004 -4.4286757 -4.4286227 -4.4285712 -4.4285707 -4.4286261 -4.4287143 -4.4287724 -4.4287882 -4.4287853 -4.4287362 -4.4286475 -4.4285731][-4.428721 -4.4287024 -4.428678 -4.4286366 -4.428575 -4.428515 -4.4285035 -4.4285583 -4.4286647 -4.42874 -4.4287667 -4.4287543 -4.4286804 -4.4285588 -4.4284563][-4.4287372 -4.4287128 -4.4286809 -4.4286265 -4.428545 -4.4284663 -4.4284453 -4.4285035 -4.4286246 -4.4287148 -4.4287596 -4.4287558 -4.428679 -4.42855 -4.4284382][-4.4287724 -4.4287434 -4.4287071 -4.4286413 -4.4285421 -4.4284506 -4.4284329 -4.4285 -4.4286222 -4.4287114 -4.4287643 -4.428772 -4.4287076 -4.4285936 -4.4284868][-4.4288068 -4.4287796 -4.428741 -4.4286737 -4.4285769 -4.4284921 -4.4284849 -4.4285536 -4.4286618 -4.4287338 -4.4287772 -4.42879 -4.4287367 -4.428638 -4.4285445][-4.4288297 -4.4288116 -4.4287848 -4.4287333 -4.4286571 -4.4285879 -4.4285827 -4.4286366 -4.428721 -4.4287696 -4.4288006 -4.4288144 -4.4287748 -4.4286942 -4.4286194][-4.4288464 -4.4288387 -4.4288249 -4.4287939 -4.4287415 -4.4286885 -4.4286861 -4.4287267 -4.4287844 -4.42881 -4.42883 -4.4288435 -4.4288192 -4.4287639 -4.4287114][-4.428875 -4.4288712 -4.4288635 -4.4288449 -4.4288125 -4.4287825 -4.4287887 -4.4288177 -4.4288492 -4.4288568 -4.4288692 -4.4288788 -4.4288664 -4.4288335 -4.428803][-4.4289107 -4.4289045 -4.4288988 -4.4288878 -4.4288697 -4.4288554 -4.4288635 -4.4288831 -4.428896 -4.4288907 -4.4288926 -4.4288969 -4.4288931 -4.4288764 -4.4288616][-4.4289293 -4.4289212 -4.428916 -4.4289117 -4.4289017 -4.4288917 -4.428896 -4.4289117 -4.4289193 -4.4289107 -4.4289064 -4.4289069 -4.4289055 -4.428895 -4.4288912]]...]
INFO - root - 2017-12-08 08:13:06.333617: step 53010, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.222 sec/batch; 17h:12m:05s remains)
INFO - root - 2017-12-08 08:13:08.575486: step 53020, loss = 2.28, batch loss = 2.23 (35.4 examples/sec; 0.226 sec/batch; 17h:32m:29s remains)
INFO - root - 2017-12-08 08:13:10.826117: step 53030, loss = 2.28, batch loss = 2.23 (36.0 examples/sec; 0.222 sec/batch; 17h:15m:10s remains)
INFO - root - 2017-12-08 08:13:13.069447: step 53040, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 17h:10m:13s remains)
INFO - root - 2017-12-08 08:13:15.325318: step 53050, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 17h:01m:32s remains)
INFO - root - 2017-12-08 08:13:17.554048: step 53060, loss = 2.28, batch loss = 2.23 (35.3 examples/sec; 0.227 sec/batch; 17h:36m:21s remains)
INFO - root - 2017-12-08 08:13:19.787145: step 53070, loss = 2.28, batch loss = 2.23 (35.6 examples/sec; 0.225 sec/batch; 17h:27m:24s remains)
INFO - root - 2017-12-08 08:13:22.014351: step 53080, loss = 2.28, batch loss = 2.23 (35.1 examples/sec; 0.228 sec/batch; 17h:41m:17s remains)
INFO - root - 2017-12-08 08:13:24.267036: step 53090, loss = 2.28, batch loss = 2.23 (36.6 examples/sec; 0.219 sec/batch; 16h:58m:31s remains)
INFO - root - 2017-12-08 08:13:26.494709: step 53100, loss = 2.28, batch loss = 2.23 (38.0 examples/sec; 0.211 sec/batch; 16h:20m:32s remains)
2017-12-08 08:13:26.796553: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4286871 -4.4286523 -4.4286313 -4.428668 -4.428721 -4.4287362 -4.4287186 -4.428689 -4.4286771 -4.4286876 -4.42871 -4.4287314 -4.4287338 -4.4287138 -4.4286704][-4.4286876 -4.42866 -4.42865 -4.4286842 -4.428741 -4.4287658 -4.4287577 -4.4287338 -4.4287233 -4.4287391 -4.4287639 -4.4287848 -4.4287887 -4.4287739 -4.4287415][-4.4286742 -4.4286652 -4.4286652 -4.4286842 -4.4287295 -4.4287577 -4.4287634 -4.4287558 -4.42875 -4.4287653 -4.4287877 -4.4288049 -4.4288077 -4.4287972 -4.4287796][-4.4286513 -4.428658 -4.4286566 -4.4286575 -4.4286885 -4.4287181 -4.4287362 -4.4287438 -4.4287443 -4.4287534 -4.4287682 -4.4287829 -4.4287868 -4.428782 -4.4287763][-4.4286413 -4.4286475 -4.4286256 -4.4286027 -4.4286203 -4.4286475 -4.4286718 -4.4286833 -4.4286804 -4.4286823 -4.4286928 -4.42871 -4.4287176 -4.4287157 -4.4287143][-4.4286394 -4.4286308 -4.4285865 -4.4285464 -4.4285588 -4.4285822 -4.4286027 -4.4286127 -4.4286027 -4.4285932 -4.4285984 -4.4286232 -4.4286389 -4.428637 -4.4286366][-4.4286318 -4.4286146 -4.4285688 -4.4285297 -4.4285374 -4.4285569 -4.4285684 -4.4285645 -4.4285355 -4.4285083 -4.428514 -4.4285526 -4.4285851 -4.4285908 -4.4285932][-4.428606 -4.4285917 -4.4285669 -4.4285421 -4.4285455 -4.4285555 -4.4285541 -4.4285293 -4.4284835 -4.4284592 -4.4284773 -4.4285297 -4.4285812 -4.4286022 -4.42861][-4.4285669 -4.4285593 -4.4285579 -4.4285483 -4.4285483 -4.4285488 -4.4285412 -4.4285088 -4.4284682 -4.4284639 -4.4284964 -4.428556 -4.4286122 -4.4286394 -4.4286475][-4.4285359 -4.4285307 -4.4285359 -4.428535 -4.4285383 -4.4285336 -4.4285274 -4.4285064 -4.4284835 -4.4284983 -4.4285436 -4.4286013 -4.4286418 -4.42866 -4.4286633][-4.4285326 -4.4285259 -4.4285178 -4.4285092 -4.4285192 -4.428514 -4.428514 -4.4285073 -4.4285007 -4.42853 -4.42858 -4.4286251 -4.4286385 -4.4286451 -4.4286523][-4.4285817 -4.4285688 -4.4285421 -4.4285278 -4.4285541 -4.4285588 -4.42856 -4.4285474 -4.4285383 -4.4285679 -4.428607 -4.4286256 -4.428606 -4.4286027 -4.4286246][-4.4286337 -4.4286127 -4.4285784 -4.428575 -4.42862 -4.4286356 -4.428627 -4.4285994 -4.4285831 -4.4286008 -4.4286146 -4.4286008 -4.4285579 -4.4285541 -4.4285927][-4.4286704 -4.4286389 -4.4285946 -4.4285903 -4.4286332 -4.4286466 -4.4286327 -4.4286 -4.4285846 -4.4285913 -4.4285855 -4.4285512 -4.4285111 -4.4285283 -4.4285846][-4.4286938 -4.4286556 -4.4285984 -4.428575 -4.4286041 -4.4286189 -4.4286079 -4.4285727 -4.4285507 -4.4285445 -4.4285278 -4.4284949 -4.4284825 -4.4285293 -4.4285932]]...]
INFO - root - 2017-12-08 08:13:29.027232: step 53110, loss = 2.28, batch loss = 2.23 (37.5 examples/sec; 0.213 sec/batch; 16h:33m:34s remains)
INFO - root - 2017-12-08 08:13:31.251687: step 53120, loss = 2.28, batch loss = 2.23 (36.0 examples/sec; 0.222 sec/batch; 17h:13m:56s remains)
INFO - root - 2017-12-08 08:13:33.478452: step 53130, loss = 2.28, batch loss = 2.23 (37.3 examples/sec; 0.214 sec/batch; 16h:38m:18s remains)
INFO - root - 2017-12-08 08:13:35.709343: step 53140, loss = 2.28, batch loss = 2.23 (36.7 examples/sec; 0.218 sec/batch; 16h:55m:41s remains)
INFO - root - 2017-12-08 08:13:37.938161: step 53150, loss = 2.28, batch loss = 2.23 (35.6 examples/sec; 0.225 sec/batch; 17h:25m:26s remains)
INFO - root - 2017-12-08 08:13:40.204777: step 53160, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.220 sec/batch; 17h:06m:09s remains)
INFO - root - 2017-12-08 08:13:42.443560: step 53170, loss = 2.28, batch loss = 2.23 (35.8 examples/sec; 0.224 sec/batch; 17h:20m:50s remains)
INFO - root - 2017-12-08 08:13:44.665172: step 53180, loss = 2.28, batch loss = 2.23 (34.6 examples/sec; 0.231 sec/batch; 17h:57m:16s remains)
INFO - root - 2017-12-08 08:13:46.896572: step 53190, loss = 2.28, batch loss = 2.23 (35.1 examples/sec; 0.228 sec/batch; 17h:40m:13s remains)
INFO - root - 2017-12-08 08:13:49.123868: step 53200, loss = 2.28, batch loss = 2.23 (34.3 examples/sec; 0.233 sec/batch; 18h:06m:02s remains)
2017-12-08 08:13:49.420422: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289074 -4.4289036 -4.428915 -4.4289207 -4.4289317 -4.4289374 -4.4289203 -4.4289031 -4.4288964 -4.4288797 -4.4288731 -4.4288831 -4.4289 -4.4289165 -4.4289312][-4.4288645 -4.4288516 -4.4288564 -4.4288616 -4.4288783 -4.4288898 -4.42887 -4.4288478 -4.4288425 -4.42883 -4.4288292 -4.4288425 -4.4288592 -4.4288788 -4.4288974][-4.4288268 -4.4288077 -4.428803 -4.4287987 -4.42881 -4.4288216 -4.4287944 -4.4287558 -4.4287519 -4.4287572 -4.428771 -4.4287939 -4.4288158 -4.4288416 -4.428865][-4.4288044 -4.4287829 -4.4287696 -4.4287481 -4.4287381 -4.4287362 -4.4286885 -4.4286265 -4.4286304 -4.4286647 -4.4287033 -4.4287443 -4.42878 -4.4288144 -4.4288478][-4.4287758 -4.4287529 -4.4287338 -4.4286957 -4.4286518 -4.4286218 -4.4285331 -4.428422 -4.4284325 -4.4285116 -4.4285827 -4.4286489 -4.4287143 -4.4287686 -4.4288211][-4.4287539 -4.4287176 -4.42869 -4.4286275 -4.4285364 -4.4284654 -4.4283147 -4.4281225 -4.4281492 -4.4283037 -4.4284239 -4.4285259 -4.4286318 -4.4287143 -4.4287887][-4.4287505 -4.4286938 -4.4286523 -4.4285564 -4.4284282 -4.42833 -4.42813 -4.4278655 -4.4279213 -4.42815 -4.4283037 -4.4284329 -4.4285731 -4.4286761 -4.428771][-4.428781 -4.4287109 -4.42866 -4.4285607 -4.4284592 -4.4283929 -4.4282365 -4.4280171 -4.4280648 -4.4282579 -4.4283714 -4.4284811 -4.4286046 -4.4286923 -4.428782][-4.428833 -4.42876 -4.4287066 -4.4286289 -4.4285707 -4.4285502 -4.4284644 -4.428328 -4.4283519 -4.42846 -4.4285264 -4.4286065 -4.4286942 -4.4287539 -4.4288235][-4.4288726 -4.4288116 -4.4287653 -4.4287152 -4.4286852 -4.4286866 -4.4286494 -4.4285736 -4.4285817 -4.4286418 -4.4286809 -4.4287353 -4.4287891 -4.4288249 -4.428875][-4.4289012 -4.4288588 -4.4288211 -4.4287863 -4.4287667 -4.4287772 -4.42877 -4.4287286 -4.4287271 -4.4287639 -4.4287887 -4.4288268 -4.4288559 -4.4288793 -4.4289141][-4.4289136 -4.4288845 -4.4288554 -4.4288297 -4.428812 -4.4288182 -4.4288244 -4.4288063 -4.4288058 -4.4288306 -4.4288511 -4.4288731 -4.4288931 -4.4289126 -4.4289365][-4.4289212 -4.4289026 -4.4288778 -4.4288578 -4.4288459 -4.4288497 -4.4288621 -4.4288597 -4.4288635 -4.4288826 -4.4288993 -4.4289117 -4.4289246 -4.4289393 -4.4289503][-4.4289389 -4.4289293 -4.4289107 -4.428896 -4.4288888 -4.4288869 -4.4288945 -4.4288974 -4.4289026 -4.4289203 -4.4289341 -4.4289417 -4.42895 -4.4289556 -4.4289584][-4.4289613 -4.4289594 -4.4289508 -4.4289417 -4.4289365 -4.4289327 -4.4289331 -4.4289346 -4.4289389 -4.4289527 -4.4289622 -4.428968 -4.4289722 -4.4289732 -4.4289718]]...]
INFO - root - 2017-12-08 08:13:51.661527: step 53210, loss = 2.28, batch loss = 2.23 (36.8 examples/sec; 0.217 sec/batch; 16h:51m:48s remains)
INFO - root - 2017-12-08 08:13:53.950388: step 53220, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 17h:09m:23s remains)
INFO - root - 2017-12-08 08:13:56.192431: step 53230, loss = 2.28, batch loss = 2.23 (34.7 examples/sec; 0.231 sec/batch; 17h:54m:13s remains)
INFO - root - 2017-12-08 08:13:58.453121: step 53240, loss = 2.28, batch loss = 2.23 (35.0 examples/sec; 0.228 sec/batch; 17h:42m:47s remains)
INFO - root - 2017-12-08 08:14:00.702274: step 53250, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 17h:17m:14s remains)
INFO - root - 2017-12-08 08:14:02.895481: step 53260, loss = 2.28, batch loss = 2.23 (36.9 examples/sec; 0.217 sec/batch; 16h:49m:42s remains)
INFO - root - 2017-12-08 08:14:05.122979: step 53270, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 17h:16m:49s remains)
INFO - root - 2017-12-08 08:14:07.367449: step 53280, loss = 2.28, batch loss = 2.23 (34.5 examples/sec; 0.232 sec/batch; 17h:58m:28s remains)
INFO - root - 2017-12-08 08:14:09.587231: step 53290, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.220 sec/batch; 17h:04m:37s remains)
INFO - root - 2017-12-08 08:14:11.831888: step 53300, loss = 2.28, batch loss = 2.23 (35.8 examples/sec; 0.223 sec/batch; 17h:19m:06s remains)
2017-12-08 08:14:12.140597: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.428966 -4.4289722 -4.4289742 -4.4289765 -4.428978 -4.428978 -4.4289765 -4.4289746 -4.4289732 -4.4289727 -4.4289727 -4.4289722 -4.4289713 -4.4289703 -4.4289684][-4.42894 -4.4289508 -4.4289584 -4.4289651 -4.4289684 -4.428968 -4.4289646 -4.4289603 -4.4289575 -4.4289556 -4.4289541 -4.4289508 -4.4289479 -4.4289451 -4.4289389][-4.428905 -4.4289231 -4.4289374 -4.428947 -4.4289508 -4.4289489 -4.4289432 -4.4289374 -4.4289341 -4.4289303 -4.4289246 -4.4289179 -4.4289131 -4.4289103 -4.4288993][-4.4288712 -4.4288907 -4.4289064 -4.4289145 -4.4289145 -4.4289083 -4.4289021 -4.4288988 -4.4288979 -4.4288926 -4.4288816 -4.42887 -4.4288645 -4.4288597 -4.4288459][-4.4288359 -4.428853 -4.428865 -4.4288654 -4.4288592 -4.4288497 -4.428843 -4.4288421 -4.428843 -4.4288363 -4.4288197 -4.4288049 -4.4288015 -4.4287987 -4.4287858][-4.4288206 -4.4288325 -4.4288383 -4.4288311 -4.4288216 -4.42881 -4.4288039 -4.4288049 -4.4288063 -4.4287953 -4.428772 -4.4287529 -4.4287534 -4.4287572 -4.4287548][-4.4288116 -4.4288206 -4.4288244 -4.4288168 -4.4288044 -4.4287887 -4.4287806 -4.4287825 -4.4287858 -4.4287724 -4.42874 -4.4287086 -4.4287052 -4.4287219 -4.4287391][-4.4287806 -4.428792 -4.4288058 -4.4288049 -4.4287906 -4.4287696 -4.4287581 -4.42876 -4.4287658 -4.4287524 -4.4287138 -4.4286666 -4.4286504 -4.428669 -4.4287009][-4.4287233 -4.4287519 -4.4287844 -4.4287944 -4.4287782 -4.4287515 -4.4287348 -4.4287329 -4.4287367 -4.4287233 -4.4286842 -4.4286327 -4.4286118 -4.4286313 -4.4286733][-4.4286628 -4.4287081 -4.4287548 -4.4287686 -4.4287496 -4.4287195 -4.4287 -4.4286952 -4.4286971 -4.4286833 -4.4286494 -4.4286094 -4.4286032 -4.4286375 -4.4286904][-4.4286332 -4.4286895 -4.4287415 -4.4287553 -4.4287333 -4.4286976 -4.4286733 -4.428668 -4.4286728 -4.4286685 -4.4286528 -4.4286385 -4.4286513 -4.4286923 -4.4287405][-4.4286127 -4.428668 -4.4287195 -4.4287391 -4.4287238 -4.4286914 -4.42867 -4.4286714 -4.4286866 -4.4286962 -4.4287028 -4.4287138 -4.4287343 -4.4287639 -4.4287877][-4.428597 -4.4286456 -4.4286985 -4.4287276 -4.4287257 -4.4287028 -4.4286904 -4.4287024 -4.4287243 -4.4287453 -4.4287643 -4.4287848 -4.4288025 -4.428813 -4.428813][-4.4285932 -4.4286389 -4.4286957 -4.4287319 -4.4287386 -4.4287243 -4.42872 -4.4287372 -4.428762 -4.4287853 -4.4288054 -4.4288273 -4.4288416 -4.4288349 -4.4288139][-4.4286118 -4.4286604 -4.4287224 -4.428761 -4.428772 -4.4287643 -4.4287648 -4.4287825 -4.4288006 -4.4288139 -4.4288225 -4.4288344 -4.4288421 -4.4288239 -4.4287953]]...]
INFO - root - 2017-12-08 08:14:14.376924: step 53310, loss = 2.28, batch loss = 2.23 (37.0 examples/sec; 0.216 sec/batch; 16h:46m:04s remains)
INFO - root - 2017-12-08 08:14:16.635765: step 53320, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.222 sec/batch; 17h:12m:03s remains)
INFO - root - 2017-12-08 08:14:18.908358: step 53330, loss = 2.28, batch loss = 2.23 (36.6 examples/sec; 0.218 sec/batch; 16h:56m:35s remains)
INFO - root - 2017-12-08 08:14:21.162345: step 53340, loss = 2.28, batch loss = 2.23 (36.7 examples/sec; 0.218 sec/batch; 16h:53m:21s remains)
INFO - root - 2017-12-08 08:14:23.399020: step 53350, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 17h:07m:34s remains)
INFO - root - 2017-12-08 08:14:25.637734: step 53360, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.225 sec/batch; 17h:27m:44s remains)
INFO - root - 2017-12-08 08:14:27.858826: step 53370, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.221 sec/batch; 17h:10m:06s remains)
INFO - root - 2017-12-08 08:14:30.087209: step 53380, loss = 2.28, batch loss = 2.23 (35.7 examples/sec; 0.224 sec/batch; 17h:22m:20s remains)
INFO - root - 2017-12-08 08:14:32.323500: step 53390, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 17h:01m:39s remains)
INFO - root - 2017-12-08 08:14:34.549965: step 53400, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 17h:17m:46s remains)
2017-12-08 08:14:34.843907: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4285908 -4.4286008 -4.428647 -4.4287095 -4.4287648 -4.4287848 -4.4287658 -4.4287167 -4.428658 -4.4286141 -4.4286103 -4.4286418 -4.4286919 -4.4287133 -4.4287171][-4.4286075 -4.4286003 -4.42863 -4.428689 -4.4287477 -4.4287667 -4.4287415 -4.428688 -4.4286218 -4.4285688 -4.4285631 -4.4286022 -4.4286609 -4.4286752 -4.4286661][-4.4286122 -4.4285822 -4.4285855 -4.428638 -4.4286995 -4.4287224 -4.4286985 -4.42864 -4.4285755 -4.4285345 -4.4285369 -4.4285746 -4.4286265 -4.4286313 -4.4286022][-4.4285669 -4.4285221 -4.4285135 -4.4285607 -4.4286208 -4.4286394 -4.4286079 -4.4285483 -4.4285083 -4.4285083 -4.4285312 -4.4285665 -4.4286022 -4.4285941 -4.4285522][-4.4284997 -4.4284577 -4.428453 -4.4284906 -4.428534 -4.4285388 -4.4285 -4.4284511 -4.4284487 -4.4284883 -4.428525 -4.428555 -4.4285851 -4.4285812 -4.4285574][-4.4284844 -4.4284663 -4.4284725 -4.428494 -4.4285073 -4.4284945 -4.4284525 -4.4284124 -4.42843 -4.428484 -4.4285192 -4.4285393 -4.4285703 -4.4285917 -4.4285941][-4.4285688 -4.428575 -4.4285893 -4.4286041 -4.4285951 -4.4285631 -4.4285021 -4.4284444 -4.4284415 -4.4284744 -4.4284983 -4.4285135 -4.4285569 -4.4285994 -4.4286122][-4.4286866 -4.4286995 -4.42871 -4.428721 -4.4286995 -4.4286537 -4.4285812 -4.4285083 -4.4284773 -4.4284887 -4.4285064 -4.4285269 -4.428575 -4.42861 -4.4286118][-4.428771 -4.4287848 -4.4287858 -4.4287887 -4.4287624 -4.4287095 -4.4286351 -4.4285641 -4.4285245 -4.4285288 -4.4285617 -4.4286017 -4.428638 -4.4286466 -4.4286261][-4.428781 -4.4287963 -4.4287977 -4.4288006 -4.4287839 -4.4287333 -4.4286537 -4.4285874 -4.4285536 -4.4285712 -4.4286361 -4.4286985 -4.4287271 -4.4287138 -4.4286776][-4.4287167 -4.4287233 -4.428731 -4.4287491 -4.4287553 -4.4287114 -4.428627 -4.428565 -4.4285469 -4.4285741 -4.428658 -4.4287415 -4.4287844 -4.4287858 -4.4287591][-4.4286213 -4.428617 -4.4286356 -4.4286847 -4.4287143 -4.4286766 -4.4285822 -4.4285192 -4.4285083 -4.4285355 -4.4286194 -4.4287162 -4.4287882 -4.4288316 -4.4288321][-4.4285631 -4.4285679 -4.4286094 -4.4286757 -4.4287109 -4.4286761 -4.4285803 -4.4285078 -4.4284863 -4.4284983 -4.4285603 -4.4286532 -4.4287448 -4.4288225 -4.4288483][-4.4285803 -4.4285927 -4.4286442 -4.4287114 -4.4287453 -4.4287195 -4.4286332 -4.4285522 -4.4285083 -4.4284973 -4.4285283 -4.4285994 -4.4286838 -4.4287639 -4.4288][-4.42866 -4.42867 -4.4287081 -4.4287572 -4.4287834 -4.4287686 -4.4287019 -4.4286232 -4.428566 -4.4285374 -4.4285431 -4.4285822 -4.4286408 -4.4287109 -4.4287515]]...]
INFO - root - 2017-12-08 08:14:37.045160: step 53410, loss = 2.28, batch loss = 2.23 (37.3 examples/sec; 0.214 sec/batch; 16h:36m:20s remains)
INFO - root - 2017-12-08 08:14:39.331175: step 53420, loss = 2.28, batch loss = 2.23 (35.8 examples/sec; 0.223 sec/batch; 17h:19m:01s remains)
INFO - root - 2017-12-08 08:14:41.563697: step 53430, loss = 2.28, batch loss = 2.23 (36.7 examples/sec; 0.218 sec/batch; 16h:52m:47s remains)
INFO - root - 2017-12-08 08:14:43.809524: step 53440, loss = 2.28, batch loss = 2.23 (35.3 examples/sec; 0.226 sec/batch; 17h:33m:12s remains)
INFO - root - 2017-12-08 08:14:46.066201: step 53450, loss = 2.28, batch loss = 2.23 (34.0 examples/sec; 0.235 sec/batch; 18h:14m:14s remains)
INFO - root - 2017-12-08 08:14:48.277765: step 53460, loss = 2.28, batch loss = 2.23 (36.7 examples/sec; 0.218 sec/batch; 16h:54m:10s remains)
INFO - root - 2017-12-08 08:14:50.498815: step 53470, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 17h:16m:14s remains)
INFO - root - 2017-12-08 08:14:52.750457: step 53480, loss = 2.28, batch loss = 2.23 (34.4 examples/sec; 0.233 sec/batch; 18h:02m:10s remains)
INFO - root - 2017-12-08 08:14:55.012510: step 53490, loss = 2.28, batch loss = 2.23 (36.6 examples/sec; 0.219 sec/batch; 16h:57m:37s remains)
INFO - root - 2017-12-08 08:14:57.235621: step 53500, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.222 sec/batch; 17h:10m:04s remains)
2017-12-08 08:14:57.513538: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4287472 -4.4287133 -4.4287024 -4.4287262 -4.4287705 -4.4288058 -4.4288197 -4.4287972 -4.4287524 -4.4287205 -4.4287124 -4.4287124 -4.4287066 -4.4287143 -4.428731][-4.42873 -4.4287 -4.4286942 -4.4287181 -4.428762 -4.4287949 -4.4288082 -4.4287877 -4.4287467 -4.4287238 -4.4287305 -4.4287419 -4.428741 -4.4287419 -4.4287429][-4.4287186 -4.4286919 -4.4286861 -4.4287081 -4.4287491 -4.4287796 -4.4287968 -4.4287839 -4.4287567 -4.4287467 -4.4287672 -4.4287896 -4.4287915 -4.4287863 -4.4287734][-4.428688 -4.4286585 -4.4286513 -4.4286709 -4.4287119 -4.4287453 -4.4287653 -4.4287615 -4.4287496 -4.4287529 -4.4287848 -4.4288182 -4.4288263 -4.4288197 -4.4287963][-4.4286585 -4.4286208 -4.4286084 -4.4286265 -4.42867 -4.4287081 -4.4287248 -4.4287157 -4.4287014 -4.4287086 -4.4287515 -4.4287996 -4.4288235 -4.4288244 -4.4287958][-4.4286594 -4.4286213 -4.428607 -4.4286237 -4.4286661 -4.4286957 -4.42869 -4.428658 -4.42863 -4.4286361 -4.4286914 -4.4287562 -4.4287963 -4.4288073 -4.4287767][-4.428658 -4.4286256 -4.4286108 -4.4286218 -4.428648 -4.4286485 -4.4286013 -4.4285245 -4.4284682 -4.4284835 -4.4285722 -4.4286709 -4.4287415 -4.4287724 -4.4287491][-4.4286294 -4.4286056 -4.428587 -4.4285803 -4.428575 -4.4285331 -4.4284391 -4.4283147 -4.428236 -4.4282851 -4.4284368 -4.428587 -4.4286938 -4.4287457 -4.4287338][-4.4286046 -4.42858 -4.4285522 -4.4285274 -4.4284959 -4.4284291 -4.4283175 -4.4281797 -4.4280972 -4.4281788 -4.42837 -4.4285388 -4.4286523 -4.4287062 -4.4286971][-4.4286175 -4.428586 -4.4285522 -4.4285288 -4.4285045 -4.4284563 -4.4283795 -4.4282889 -4.4282403 -4.4283133 -4.4284663 -4.4285846 -4.4286489 -4.4286714 -4.4286585][-4.4286661 -4.4286356 -4.4286032 -4.4285903 -4.4285803 -4.4285536 -4.4285126 -4.428463 -4.428432 -4.4284763 -4.4285746 -4.428637 -4.4286494 -4.4286394 -4.4286251][-4.4287238 -4.4287052 -4.4286866 -4.4286852 -4.4286809 -4.4286618 -4.4286356 -4.4286013 -4.4285688 -4.4285769 -4.4286242 -4.4286442 -4.4286213 -4.4285908 -4.4285822][-4.4287367 -4.4287271 -4.428721 -4.4287271 -4.4287243 -4.4287095 -4.4286933 -4.4286723 -4.4286447 -4.4286361 -4.4286537 -4.4286528 -4.4286175 -4.4285827 -4.4285812][-4.4287176 -4.4287057 -4.4287014 -4.4287066 -4.4287028 -4.4286928 -4.4286842 -4.4286752 -4.4286618 -4.4286547 -4.4286613 -4.4286566 -4.4286294 -4.4286046 -4.4286079][-4.4286695 -4.4286556 -4.4286585 -4.4286718 -4.4286771 -4.4286752 -4.4286718 -4.4286709 -4.428668 -4.4286633 -4.4286633 -4.4286575 -4.4286394 -4.4286237 -4.4286261]]...]
INFO - root - 2017-12-08 08:14:59.752114: step 53510, loss = 2.28, batch loss = 2.23 (35.0 examples/sec; 0.229 sec/batch; 17h:43m:24s remains)
INFO - root - 2017-12-08 08:15:01.969623: step 53520, loss = 2.28, batch loss = 2.23 (36.8 examples/sec; 0.217 sec/batch; 16h:49m:28s remains)
INFO - root - 2017-12-08 08:15:04.220507: step 53530, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 16h:58m:26s remains)
INFO - root - 2017-12-08 08:15:06.441799: step 53540, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 16h:59m:51s remains)
INFO - root - 2017-12-08 08:15:08.707820: step 53550, loss = 2.28, batch loss = 2.23 (34.4 examples/sec; 0.232 sec/batch; 18h:00m:12s remains)
INFO - root - 2017-12-08 08:15:10.927518: step 53560, loss = 2.28, batch loss = 2.23 (37.4 examples/sec; 0.214 sec/batch; 16h:35m:21s remains)
INFO - root - 2017-12-08 08:15:13.183614: step 53570, loss = 2.28, batch loss = 2.23 (35.8 examples/sec; 0.224 sec/batch; 17h:20m:01s remains)
INFO - root - 2017-12-08 08:15:15.412974: step 53580, loss = 2.28, batch loss = 2.23 (33.8 examples/sec; 0.237 sec/batch; 18h:20m:41s remains)
INFO - root - 2017-12-08 08:15:17.644406: step 53590, loss = 2.28, batch loss = 2.23 (34.1 examples/sec; 0.235 sec/batch; 18h:10m:49s remains)
INFO - root - 2017-12-08 08:15:19.898785: step 53600, loss = 2.28, batch loss = 2.23 (36.6 examples/sec; 0.219 sec/batch; 16h:56m:29s remains)
2017-12-08 08:15:20.208869: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4287715 -4.4287763 -4.428793 -4.4288111 -4.4288321 -4.4288716 -4.4288955 -4.4288936 -4.4288936 -4.428894 -4.4288878 -4.4288712 -4.4288378 -4.4288068 -4.4287987][-4.4287453 -4.4287472 -4.4287586 -4.4287753 -4.4288025 -4.4288459 -4.4288754 -4.428885 -4.4289064 -4.4289279 -4.4289355 -4.428926 -4.428894 -4.4288568 -4.4288344][-4.4287281 -4.4287224 -4.428731 -4.4287486 -4.4287767 -4.4288073 -4.4288211 -4.4288297 -4.4288688 -4.428916 -4.4289489 -4.428957 -4.4289322 -4.4288979 -4.4288759][-4.4287171 -4.4287128 -4.4287214 -4.4287329 -4.4287448 -4.428741 -4.4287143 -4.4287119 -4.4287739 -4.4288559 -4.4289227 -4.4289522 -4.4289374 -4.4289088 -4.428896][-4.4287276 -4.4287181 -4.4287195 -4.4287133 -4.42869 -4.4286304 -4.4285393 -4.4285207 -4.4286127 -4.4287419 -4.4288516 -4.4289083 -4.4289036 -4.4288712 -4.4288626][-4.4287572 -4.4287467 -4.4287395 -4.4287095 -4.4286332 -4.4285021 -4.4283242 -4.42828 -4.4284196 -4.4285975 -4.4287467 -4.4288344 -4.4288397 -4.4287996 -4.4287858][-4.4287744 -4.4287677 -4.4287586 -4.4287038 -4.4285803 -4.4283881 -4.4281211 -4.4280448 -4.4282465 -4.4284692 -4.4286427 -4.4287553 -4.4287744 -4.4287295 -4.4287043][-4.4287839 -4.4287791 -4.4287691 -4.4287081 -4.4285831 -4.4283929 -4.4281158 -4.42803 -4.4282351 -4.4284482 -4.4286113 -4.4287252 -4.42875 -4.4287057 -4.4286675][-4.4288092 -4.4288015 -4.4287868 -4.4287386 -4.4286594 -4.4285336 -4.4283395 -4.4282746 -4.4284072 -4.4285526 -4.4286718 -4.4287634 -4.4287887 -4.42875 -4.42871][-4.4288445 -4.4288316 -4.428813 -4.4287863 -4.4287515 -4.4286857 -4.4285727 -4.4285359 -4.428607 -4.4286857 -4.4287543 -4.4288177 -4.4288449 -4.4288278 -4.4288025][-4.4288721 -4.4288616 -4.42885 -4.4288387 -4.4288278 -4.428793 -4.4287348 -4.4287214 -4.4287648 -4.4287977 -4.4288187 -4.4288526 -4.42888 -4.4288859 -4.42888][-4.4288917 -4.4288855 -4.4288845 -4.4288807 -4.4288774 -4.42886 -4.4288316 -4.4288278 -4.4288559 -4.4288645 -4.4288592 -4.4288731 -4.4288955 -4.4289079 -4.4289131][-4.4289002 -4.4288993 -4.4289031 -4.4289026 -4.428905 -4.4289007 -4.4288888 -4.428884 -4.4288969 -4.4288917 -4.4288754 -4.4288821 -4.4288993 -4.4289107 -4.4289188][-4.4288974 -4.4289002 -4.4289069 -4.4289064 -4.4289079 -4.4289083 -4.4289041 -4.4288945 -4.4288921 -4.4288812 -4.4288678 -4.428874 -4.4288883 -4.4288945 -4.4289002][-4.428884 -4.4288912 -4.4289012 -4.4289021 -4.4289036 -4.4289055 -4.4289007 -4.4288855 -4.4288716 -4.4288626 -4.4288573 -4.428865 -4.4288745 -4.4288721 -4.4288716]]...]
INFO - root - 2017-12-08 08:15:22.457673: step 53610, loss = 2.28, batch loss = 2.23 (34.2 examples/sec; 0.234 sec/batch; 18h:06m:24s remains)
INFO - root - 2017-12-08 08:15:24.703065: step 53620, loss = 2.28, batch loss = 2.23 (35.2 examples/sec; 0.227 sec/batch; 17h:34m:54s remains)
INFO - root - 2017-12-08 08:15:26.926476: step 53630, loss = 2.28, batch loss = 2.23 (35.6 examples/sec; 0.225 sec/batch; 17h:24m:52s remains)
INFO - root - 2017-12-08 08:15:29.171377: step 53640, loss = 2.28, batch loss = 2.23 (35.0 examples/sec; 0.228 sec/batch; 17h:41m:14s remains)
INFO - root - 2017-12-08 08:15:31.412133: step 53650, loss = 2.28, batch loss = 2.23 (35.2 examples/sec; 0.227 sec/batch; 17h:37m:17s remains)
INFO - root - 2017-12-08 08:15:33.632142: step 53660, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 17h:00m:14s remains)
INFO - root - 2017-12-08 08:15:35.870750: step 53670, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 17h:06m:25s remains)
INFO - root - 2017-12-08 08:15:38.134856: step 53680, loss = 2.28, batch loss = 2.23 (35.4 examples/sec; 0.226 sec/batch; 17h:30m:38s remains)
INFO - root - 2017-12-08 08:15:40.405629: step 53690, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.222 sec/batch; 17h:11m:00s remains)
INFO - root - 2017-12-08 08:15:42.653984: step 53700, loss = 2.28, batch loss = 2.23 (36.7 examples/sec; 0.218 sec/batch; 16h:51m:40s remains)
2017-12-08 08:15:42.929535: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288745 -4.4288845 -4.4288859 -4.4288826 -4.42887 -4.4288511 -4.4288316 -4.4288058 -4.4287691 -4.428751 -4.4287658 -4.4287772 -4.4287958 -4.42883 -4.4288683][-4.4289517 -4.4289575 -4.4289522 -4.428946 -4.4289312 -4.4289083 -4.4288845 -4.4288688 -4.4288397 -4.4288049 -4.4287887 -4.4287782 -4.4287896 -4.4288287 -4.4288745][-4.4289522 -4.4289517 -4.4289422 -4.4289331 -4.4289131 -4.4288836 -4.428865 -4.4288659 -4.4288578 -4.4288206 -4.4287834 -4.4287615 -4.428782 -4.4288287 -4.4288731][-4.4289217 -4.4289212 -4.4289174 -4.4289093 -4.4288793 -4.4288397 -4.4288292 -4.4288392 -4.428834 -4.4287925 -4.4287519 -4.4287333 -4.4287653 -4.42882 -4.4288616][-4.428906 -4.4289074 -4.42891 -4.4289017 -4.4288521 -4.428792 -4.4287634 -4.4287648 -4.42876 -4.4287267 -4.4287052 -4.42871 -4.4287572 -4.4288135 -4.4288483][-4.4289155 -4.4289179 -4.4289136 -4.4288936 -4.4288378 -4.4287562 -4.4286876 -4.4286494 -4.4286261 -4.4286079 -4.4286203 -4.4286594 -4.428731 -4.4287987 -4.4288268][-4.4289031 -4.4288917 -4.4288583 -4.4288225 -4.4287677 -4.4286737 -4.42855 -4.4284449 -4.4283919 -4.4284163 -4.4284992 -4.4285917 -4.4287019 -4.4287815 -4.4288111][-4.4288368 -4.428792 -4.4287181 -4.4286656 -4.4286203 -4.4285226 -4.4283605 -4.4282002 -4.4281168 -4.428205 -4.4283738 -4.4285235 -4.4286652 -4.4287553 -4.4287963][-4.4287543 -4.4286609 -4.4285388 -4.4284611 -4.4284267 -4.4283381 -4.4281769 -4.4280272 -4.4279766 -4.4281173 -4.4283261 -4.4284968 -4.4286461 -4.4287424 -4.4287934][-4.42871 -4.4285917 -4.4284525 -4.4283619 -4.4283285 -4.4282522 -4.4281249 -4.4280453 -4.4280663 -4.4282136 -4.4283719 -4.4285007 -4.4286289 -4.4287295 -4.428793][-4.4287162 -4.4286103 -4.4284959 -4.4284191 -4.4283824 -4.42831 -4.4282193 -4.42819 -4.4282455 -4.4283576 -4.4284534 -4.4285393 -4.4286394 -4.4287319 -4.428802][-4.4287424 -4.4286604 -4.428576 -4.42852 -4.4284945 -4.4284377 -4.4283681 -4.4283471 -4.4283924 -4.4284673 -4.4285283 -4.4286008 -4.4286809 -4.42876 -4.4288273][-4.4287477 -4.4287009 -4.4286504 -4.42862 -4.428618 -4.4285913 -4.4285417 -4.4285145 -4.4285312 -4.4285707 -4.4286156 -4.4286761 -4.4287376 -4.428802 -4.4288597][-4.4287486 -4.4287381 -4.4287205 -4.4287095 -4.4287157 -4.4287071 -4.4286766 -4.4286528 -4.4286547 -4.4286819 -4.428721 -4.4287658 -4.4288068 -4.428853 -4.4289][-4.4287467 -4.4287715 -4.4287825 -4.4287839 -4.428793 -4.4287915 -4.4287715 -4.4287543 -4.4287539 -4.4287767 -4.4288044 -4.4288282 -4.4288535 -4.4288869 -4.4289241]]...]
INFO - root - 2017-12-08 08:15:45.167496: step 53710, loss = 2.28, batch loss = 2.23 (34.4 examples/sec; 0.233 sec/batch; 18h:01m:50s remains)
INFO - root - 2017-12-08 08:15:47.408249: step 53720, loss = 2.28, batch loss = 2.23 (37.0 examples/sec; 0.216 sec/batch; 16h:45m:47s remains)
INFO - root - 2017-12-08 08:15:49.646181: step 53730, loss = 2.28, batch loss = 2.23 (36.9 examples/sec; 0.217 sec/batch; 16h:46m:46s remains)
INFO - root - 2017-12-08 08:15:51.886209: step 53740, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.225 sec/batch; 17h:26m:12s remains)
INFO - root - 2017-12-08 08:15:54.145857: step 53750, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 17h:07m:08s remains)
INFO - root - 2017-12-08 08:15:56.380807: step 53760, loss = 2.28, batch loss = 2.23 (35.4 examples/sec; 0.226 sec/batch; 17h:30m:46s remains)
INFO - root - 2017-12-08 08:15:58.647478: step 53770, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 17h:00m:50s remains)
INFO - root - 2017-12-08 08:16:00.883167: step 53780, loss = 2.28, batch loss = 2.23 (37.5 examples/sec; 0.214 sec/batch; 16h:32m:01s remains)
INFO - root - 2017-12-08 08:16:03.117277: step 53790, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 17h:01m:40s remains)
INFO - root - 2017-12-08 08:16:05.356054: step 53800, loss = 2.28, batch loss = 2.23 (35.8 examples/sec; 0.224 sec/batch; 17h:18m:11s remains)
2017-12-08 08:16:05.667388: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4287419 -4.4287267 -4.4286466 -4.4285069 -4.4284229 -4.4284315 -4.428494 -4.4285965 -4.4287205 -4.4287672 -4.4287763 -4.4287972 -4.4287791 -4.4287033 -4.4286394][-4.4287043 -4.4287119 -4.4286475 -4.428515 -4.4284353 -4.4284525 -4.4285078 -4.4286003 -4.4287143 -4.4287453 -4.4287367 -4.4287515 -4.4287405 -4.4286733 -4.4286208][-4.4286723 -4.4286971 -4.4286561 -4.4285474 -4.4284873 -4.4285164 -4.4285612 -4.4286218 -4.428699 -4.4287186 -4.4287043 -4.4287024 -4.4286909 -4.4286523 -4.4286146][-4.428647 -4.428679 -4.4286628 -4.4285812 -4.4285359 -4.4285712 -4.4286065 -4.4286203 -4.4286466 -4.4286528 -4.428647 -4.428647 -4.4286566 -4.4286466 -4.4286175][-4.4286213 -4.4286442 -4.428647 -4.428587 -4.428544 -4.4285688 -4.4285765 -4.4285483 -4.4285421 -4.4285522 -4.428575 -4.4286127 -4.42865 -4.4286618 -4.4286418][-4.428597 -4.428606 -4.4286251 -4.4285812 -4.4285216 -4.4285016 -4.4284525 -4.4283915 -4.4284024 -4.4284678 -4.4285579 -4.4286318 -4.4286838 -4.4287138 -4.4287057][-4.4285831 -4.4285884 -4.4286156 -4.4285789 -4.4284968 -4.4284124 -4.4282722 -4.4281759 -4.4282718 -4.4284487 -4.428607 -4.4286957 -4.4287534 -4.4287848 -4.4287767][-4.4286003 -4.4286017 -4.4286165 -4.4285727 -4.4284692 -4.4283266 -4.4281087 -4.4280148 -4.4282427 -4.4285135 -4.4286938 -4.4287758 -4.4288254 -4.4288449 -4.4288239][-4.4286442 -4.4286389 -4.4286251 -4.4285574 -4.4284534 -4.4283032 -4.4280996 -4.4280849 -4.4283667 -4.4286313 -4.4287786 -4.4288425 -4.4288712 -4.4288735 -4.4288454][-4.4286861 -4.4286728 -4.4286289 -4.4285469 -4.4284587 -4.4283509 -4.4282432 -4.4283047 -4.4285355 -4.4287314 -4.428834 -4.4288731 -4.4288836 -4.4288697 -4.4288359][-4.4287043 -4.4286995 -4.4286389 -4.4285569 -4.4284935 -4.4284363 -4.4283977 -4.4284792 -4.4286466 -4.4287767 -4.4288464 -4.4288626 -4.428854 -4.4288316 -4.4287949][-4.4287157 -4.4287362 -4.4286804 -4.428606 -4.4285579 -4.4285274 -4.4285116 -4.4285784 -4.4286976 -4.4287825 -4.4288268 -4.4288321 -4.4288177 -4.4287953 -4.428762][-4.4287286 -4.4287696 -4.4287329 -4.4286628 -4.4286137 -4.4285808 -4.4285688 -4.4286256 -4.42872 -4.4287739 -4.4287963 -4.4287953 -4.4287767 -4.4287596 -4.4287438][-4.4287477 -4.428791 -4.4287772 -4.428719 -4.4286661 -4.4286284 -4.4286194 -4.4286675 -4.4287353 -4.4287581 -4.4287653 -4.4287624 -4.4287467 -4.4287405 -4.4287438][-4.4287982 -4.4288335 -4.4288292 -4.4287791 -4.42873 -4.4286947 -4.4286971 -4.4287333 -4.4287663 -4.4287586 -4.4287558 -4.4287558 -4.4287453 -4.4287448 -4.4287615]]...]
INFO - root - 2017-12-08 08:16:07.884235: step 53810, loss = 2.28, batch loss = 2.23 (35.8 examples/sec; 0.224 sec/batch; 17h:19m:08s remains)
INFO - root - 2017-12-08 08:16:10.143704: step 53820, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 16h:58m:21s remains)
INFO - root - 2017-12-08 08:16:12.401446: step 53830, loss = 2.28, batch loss = 2.23 (33.8 examples/sec; 0.237 sec/batch; 18h:19m:29s remains)
INFO - root - 2017-12-08 08:16:14.647671: step 53840, loss = 2.28, batch loss = 2.23 (33.6 examples/sec; 0.238 sec/batch; 18h:25m:14s remains)
INFO - root - 2017-12-08 08:16:16.903740: step 53850, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 17h:15m:52s remains)
INFO - root - 2017-12-08 08:16:19.121917: step 53860, loss = 2.28, batch loss = 2.23 (36.0 examples/sec; 0.222 sec/batch; 17h:11m:54s remains)
INFO - root - 2017-12-08 08:16:21.366547: step 53870, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.225 sec/batch; 17h:26m:30s remains)
INFO - root - 2017-12-08 08:16:23.595987: step 53880, loss = 2.28, batch loss = 2.23 (35.6 examples/sec; 0.225 sec/batch; 17h:23m:41s remains)
INFO - root - 2017-12-08 08:16:25.831806: step 53890, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.221 sec/batch; 17h:08m:22s remains)
INFO - root - 2017-12-08 08:16:28.092992: step 53900, loss = 2.28, batch loss = 2.23 (35.2 examples/sec; 0.227 sec/batch; 17h:36m:07s remains)
2017-12-08 08:16:28.402924: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4286976 -4.4287438 -4.4287958 -4.4288321 -4.4288473 -4.4288645 -4.4288759 -4.428884 -4.4289 -4.4288969 -4.4288287 -4.4287143 -4.428607 -4.4285827 -4.4286075][-4.4286895 -4.4287462 -4.4287968 -4.4288306 -4.4288507 -4.4288745 -4.4288869 -4.4289045 -4.4289274 -4.428937 -4.428896 -4.4288106 -4.428719 -4.4286809 -4.4286656][-4.4287076 -4.4287539 -4.4287963 -4.4288244 -4.4288445 -4.4288664 -4.4288754 -4.4288898 -4.4289093 -4.4289145 -4.4288945 -4.4288464 -4.4287815 -4.4287376 -4.4287004][-4.4287596 -4.42877 -4.428791 -4.4288106 -4.4288225 -4.4288349 -4.428834 -4.4288464 -4.4288778 -4.4288945 -4.4288816 -4.4288454 -4.4287939 -4.4287567 -4.4287157][-4.4288111 -4.4287887 -4.4287825 -4.4287753 -4.4287658 -4.4287553 -4.4287424 -4.4287677 -4.4288335 -4.4288726 -4.4288688 -4.4288392 -4.428803 -4.4287734 -4.4287353][-4.428853 -4.428813 -4.4287724 -4.4287176 -4.4286637 -4.4286151 -4.4285893 -4.4286389 -4.4287457 -4.4288182 -4.4288378 -4.4288282 -4.4287987 -4.4287724 -4.4287376][-4.4288645 -4.4288163 -4.428751 -4.4286518 -4.4285426 -4.4284306 -4.4283676 -4.4284477 -4.4286127 -4.4287281 -4.4287758 -4.4287977 -4.4287896 -4.4287663 -4.4287305][-4.428853 -4.4288025 -4.4287305 -4.4286165 -4.4284787 -4.4283128 -4.4281888 -4.4282689 -4.4284787 -4.4286346 -4.4287171 -4.4287691 -4.42878 -4.4287615 -4.4287329][-4.4288383 -4.4288 -4.4287486 -4.4286661 -4.4285626 -4.4284191 -4.4282913 -4.428339 -4.4285135 -4.4286385 -4.4287152 -4.4287696 -4.4287834 -4.428772 -4.4287634][-4.4288473 -4.4288287 -4.4288049 -4.4287515 -4.4286966 -4.4286184 -4.4285474 -4.428575 -4.4286637 -4.4287181 -4.4287529 -4.4287834 -4.42879 -4.428782 -4.4287777][-4.4288645 -4.4288564 -4.4288487 -4.4288192 -4.4287896 -4.42875 -4.4287281 -4.4287505 -4.4287858 -4.4287868 -4.4287839 -4.4287858 -4.4287806 -4.4287758 -4.428772][-4.4288712 -4.4288616 -4.4288583 -4.42884 -4.4288139 -4.4287963 -4.428812 -4.4288383 -4.428853 -4.4288397 -4.4288206 -4.4287968 -4.4287748 -4.4287677 -4.4287567][-4.4288545 -4.428843 -4.4288368 -4.4288192 -4.4287982 -4.4288034 -4.4288383 -4.4288654 -4.42888 -4.4288754 -4.4288535 -4.4288158 -4.4287772 -4.4287539 -4.4287405][-4.4288368 -4.4288244 -4.4288139 -4.4287882 -4.4287639 -4.428782 -4.4288268 -4.4288554 -4.4288716 -4.4288831 -4.428874 -4.4288416 -4.4287992 -4.4287629 -4.4287424][-4.4288373 -4.4288239 -4.4288049 -4.4287586 -4.4287148 -4.4287281 -4.4287815 -4.4288239 -4.4288521 -4.4288774 -4.4288893 -4.4288745 -4.42884 -4.4287992 -4.428771]]...]
INFO - root - 2017-12-08 08:16:30.642442: step 53910, loss = 2.28, batch loss = 2.23 (37.3 examples/sec; 0.214 sec/batch; 16h:35m:04s remains)
INFO - root - 2017-12-08 08:16:32.862055: step 53920, loss = 2.28, batch loss = 2.23 (38.0 examples/sec; 0.211 sec/batch; 16h:18m:07s remains)
INFO - root - 2017-12-08 08:16:35.099861: step 53930, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.221 sec/batch; 17h:04m:27s remains)
INFO - root - 2017-12-08 08:16:37.326366: step 53940, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.222 sec/batch; 17h:09m:58s remains)
INFO - root - 2017-12-08 08:16:39.594292: step 53950, loss = 2.28, batch loss = 2.23 (36.8 examples/sec; 0.218 sec/batch; 16h:50m:10s remains)
INFO - root - 2017-12-08 08:16:41.851403: step 53960, loss = 2.28, batch loss = 2.23 (35.0 examples/sec; 0.228 sec/batch; 17h:40m:29s remains)
INFO - root - 2017-12-08 08:16:44.071223: step 53970, loss = 2.28, batch loss = 2.23 (35.4 examples/sec; 0.226 sec/batch; 17h:28m:45s remains)
INFO - root - 2017-12-08 08:16:46.313079: step 53980, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 16h:58m:20s remains)
INFO - root - 2017-12-08 08:16:48.522914: step 53990, loss = 2.28, batch loss = 2.23 (38.1 examples/sec; 0.210 sec/batch; 16h:15m:15s remains)
INFO - root - 2017-12-08 08:16:50.751446: step 54000, loss = 2.28, batch loss = 2.23 (35.8 examples/sec; 0.224 sec/batch; 17h:18m:10s remains)
2017-12-08 08:16:51.047044: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288816 -4.4288869 -4.4288898 -4.4288917 -4.4288917 -4.4288898 -4.4288855 -4.4288816 -4.42888 -4.4288821 -4.428885 -4.4288888 -4.4288931 -4.4288979 -4.4289031][-4.4288826 -4.4288888 -4.4288921 -4.4288955 -4.4288955 -4.4288931 -4.4288869 -4.4288797 -4.4288764 -4.4288778 -4.4288812 -4.4288859 -4.4288907 -4.4288974 -4.4289036][-4.42888 -4.428884 -4.4288864 -4.4288869 -4.428884 -4.4288774 -4.4288673 -4.4288592 -4.4288564 -4.4288592 -4.428865 -4.4288731 -4.4288807 -4.42889 -4.4288983][-4.4288735 -4.4288731 -4.4288712 -4.4288683 -4.4288626 -4.4288521 -4.4288392 -4.42883 -4.4288282 -4.4288325 -4.42884 -4.4288507 -4.428863 -4.4288774 -4.42889][-4.4288573 -4.4288535 -4.4288483 -4.4288425 -4.4288335 -4.4288187 -4.428803 -4.4287934 -4.4287887 -4.4287887 -4.4287949 -4.4288092 -4.4288287 -4.4288516 -4.4288735][-4.4288249 -4.4288211 -4.4288163 -4.4288106 -4.4288006 -4.428781 -4.4287624 -4.42875 -4.4287391 -4.42873 -4.4287295 -4.4287434 -4.4287691 -4.428802 -4.4288344][-4.4287839 -4.4287853 -4.4287872 -4.4287887 -4.428781 -4.4287581 -4.4287348 -4.4287162 -4.4286962 -4.4286757 -4.4286642 -4.4286728 -4.4286962 -4.428731 -4.4287691][-4.4287529 -4.4287586 -4.4287672 -4.4287739 -4.4287677 -4.4287443 -4.428719 -4.4286952 -4.4286675 -4.4286389 -4.4286194 -4.428618 -4.4286318 -4.4286585 -4.4286933][-4.4287539 -4.4287577 -4.4287663 -4.4287715 -4.4287615 -4.4287391 -4.4287167 -4.4286933 -4.4286671 -4.4286394 -4.428618 -4.42861 -4.4286141 -4.4286275 -4.42865][-4.4287677 -4.4287672 -4.4287739 -4.428781 -4.4287767 -4.4287648 -4.4287543 -4.4287405 -4.4287252 -4.4287057 -4.4286861 -4.4286723 -4.4286666 -4.4286647 -4.4286704][-4.4287663 -4.428762 -4.428771 -4.4287868 -4.4287944 -4.4287968 -4.4287977 -4.4287972 -4.4287968 -4.4287887 -4.4287729 -4.4287577 -4.4287481 -4.4287381 -4.4287338][-4.428771 -4.4287653 -4.4287772 -4.4287996 -4.4288177 -4.42883 -4.4288378 -4.428844 -4.4288516 -4.4288521 -4.4288421 -4.4288306 -4.4288225 -4.4288139 -4.4288082][-4.4287786 -4.4287729 -4.4287863 -4.4288106 -4.428833 -4.4288487 -4.4288564 -4.4288607 -4.4288659 -4.4288683 -4.428865 -4.4288616 -4.4288611 -4.42886 -4.4288607][-4.4287863 -4.4287806 -4.428792 -4.4288111 -4.4288282 -4.4288387 -4.4288411 -4.4288397 -4.4288392 -4.4288411 -4.428843 -4.42885 -4.4288588 -4.4288659 -4.4288726][-4.4288149 -4.42881 -4.4288144 -4.428823 -4.4288306 -4.4288325 -4.4288278 -4.4288192 -4.428813 -4.4288111 -4.4288144 -4.4288259 -4.4288378 -4.4288468 -4.4288559]]...]
INFO - root - 2017-12-08 08:16:53.296103: step 54010, loss = 2.28, batch loss = 2.23 (36.0 examples/sec; 0.222 sec/batch; 17h:10m:36s remains)
INFO - root - 2017-12-08 08:16:55.509601: step 54020, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 16h:55m:58s remains)
INFO - root - 2017-12-08 08:16:57.747973: step 54030, loss = 2.28, batch loss = 2.23 (34.8 examples/sec; 0.230 sec/batch; 17h:48m:06s remains)
INFO - root - 2017-12-08 08:16:59.966329: step 54040, loss = 2.28, batch loss = 2.23 (35.4 examples/sec; 0.226 sec/batch; 17h:30m:00s remains)
INFO - root - 2017-12-08 08:17:02.215237: step 54050, loss = 2.28, batch loss = 2.23 (34.4 examples/sec; 0.232 sec/batch; 17h:58m:43s remains)
INFO - root - 2017-12-08 08:17:04.433341: step 54060, loss = 2.28, batch loss = 2.23 (36.0 examples/sec; 0.222 sec/batch; 17h:10m:01s remains)
INFO - root - 2017-12-08 08:17:06.670019: step 54070, loss = 2.28, batch loss = 2.23 (34.6 examples/sec; 0.231 sec/batch; 17h:52m:49s remains)
INFO - root - 2017-12-08 08:17:08.985090: step 54080, loss = 2.28, batch loss = 2.23 (35.7 examples/sec; 0.224 sec/batch; 17h:18m:43s remains)
INFO - root - 2017-12-08 08:17:11.225622: step 54090, loss = 2.28, batch loss = 2.23 (35.4 examples/sec; 0.226 sec/batch; 17h:29m:00s remains)
INFO - root - 2017-12-08 08:17:13.453962: step 54100, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.222 sec/batch; 17h:08m:16s remains)
2017-12-08 08:17:13.764334: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4286809 -4.4286623 -4.4286675 -4.4286795 -4.4286885 -4.4287 -4.4287171 -4.4287453 -4.4287896 -4.4288192 -4.4288096 -4.4287729 -4.4287391 -4.4287238 -4.4287205][-4.4287305 -4.4287043 -4.4287133 -4.4287386 -4.4287624 -4.42879 -4.4288173 -4.4288454 -4.4288774 -4.4288898 -4.4288678 -4.4288259 -4.42879 -4.4287782 -4.4287772][-4.4287672 -4.4287405 -4.4287596 -4.4288 -4.4288368 -4.4288721 -4.4288993 -4.4289203 -4.4289384 -4.4289346 -4.4289031 -4.4288607 -4.4288273 -4.4288173 -4.4288206][-4.4287705 -4.4287486 -4.4287791 -4.428822 -4.4288559 -4.4288845 -4.4289031 -4.428915 -4.4289212 -4.4289088 -4.4288778 -4.4288483 -4.4288268 -4.428822 -4.4288287][-4.4287524 -4.4287367 -4.4287677 -4.4287987 -4.4288168 -4.4288335 -4.4288449 -4.4288516 -4.4288492 -4.4288311 -4.4288092 -4.4287987 -4.4287906 -4.4287882 -4.4287949][-4.4287534 -4.4287381 -4.4287543 -4.4287615 -4.4287543 -4.4287534 -4.42876 -4.4287634 -4.428751 -4.4287295 -4.428721 -4.42873 -4.4287338 -4.4287324 -4.4287419][-4.4287744 -4.4287462 -4.4287357 -4.4287119 -4.4286714 -4.4286547 -4.4286647 -4.4286685 -4.4286523 -4.4286332 -4.42864 -4.4286642 -4.4286757 -4.4286742 -4.4286847][-4.4287882 -4.4287405 -4.4286995 -4.4286442 -4.4285812 -4.4285645 -4.4285941 -4.4286132 -4.4286046 -4.4285932 -4.4286084 -4.4286342 -4.42864 -4.428627 -4.4286337][-4.428771 -4.4287109 -4.4286513 -4.4285827 -4.4285288 -4.4285388 -4.4285936 -4.428628 -4.4286275 -4.4286165 -4.4286242 -4.4286385 -4.4286294 -4.4286036 -4.4286022][-4.4287372 -4.42868 -4.4286256 -4.4285784 -4.4285626 -4.4285941 -4.42865 -4.4286819 -4.42868 -4.4286661 -4.4286637 -4.428668 -4.428647 -4.4286118 -4.4286051][-4.4287066 -4.4286585 -4.4286213 -4.4286103 -4.4286294 -4.428668 -4.428709 -4.4287314 -4.4287291 -4.428719 -4.4287148 -4.4287148 -4.4286876 -4.4286513 -4.4286485][-4.4286885 -4.4286489 -4.4286289 -4.4286432 -4.4286809 -4.4287171 -4.4287443 -4.4287605 -4.4287653 -4.4287624 -4.4287591 -4.4287596 -4.4287372 -4.42871 -4.4287124][-4.4286752 -4.4286475 -4.4286394 -4.4286642 -4.4287057 -4.4287324 -4.4287472 -4.4287596 -4.428771 -4.4287734 -4.4287691 -4.4287705 -4.4287572 -4.4287434 -4.4287467][-4.4286771 -4.4286618 -4.42866 -4.4286795 -4.4287086 -4.4287171 -4.4287152 -4.4287214 -4.4287376 -4.4287457 -4.4287438 -4.4287457 -4.4287429 -4.42874 -4.42874][-4.4287095 -4.4287009 -4.4287014 -4.42871 -4.428719 -4.42871 -4.4286985 -4.4287004 -4.4287167 -4.4287291 -4.4287319 -4.4287357 -4.42874 -4.4287376 -4.4287281]]...]
INFO - root - 2017-12-08 08:17:16.003727: step 54110, loss = 2.28, batch loss = 2.23 (35.8 examples/sec; 0.223 sec/batch; 17h:16m:26s remains)
INFO - root - 2017-12-08 08:17:18.233022: step 54120, loss = 2.28, batch loss = 2.23 (35.6 examples/sec; 0.225 sec/batch; 17h:23m:09s remains)
INFO - root - 2017-12-08 08:17:20.471871: step 54130, loss = 2.28, batch loss = 2.23 (34.8 examples/sec; 0.230 sec/batch; 17h:45m:49s remains)
INFO - root - 2017-12-08 08:17:22.722123: step 54140, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 17h:12m:44s remains)
INFO - root - 2017-12-08 08:17:24.950837: step 54150, loss = 2.28, batch loss = 2.23 (36.9 examples/sec; 0.217 sec/batch; 16h:46m:29s remains)
INFO - root - 2017-12-08 08:17:27.173049: step 54160, loss = 2.28, batch loss = 2.23 (36.9 examples/sec; 0.217 sec/batch; 16h:47m:01s remains)
INFO - root - 2017-12-08 08:17:29.423365: step 54170, loss = 2.28, batch loss = 2.23 (35.2 examples/sec; 0.227 sec/batch; 17h:33m:13s remains)
INFO - root - 2017-12-08 08:17:31.689316: step 54180, loss = 2.28, batch loss = 2.23 (36.0 examples/sec; 0.222 sec/batch; 17h:10m:56s remains)
INFO - root - 2017-12-08 08:17:33.917734: step 54190, loss = 2.28, batch loss = 2.23 (34.9 examples/sec; 0.229 sec/batch; 17h:43m:07s remains)
INFO - root - 2017-12-08 08:17:36.151281: step 54200, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.221 sec/batch; 17h:07m:10s remains)
2017-12-08 08:17:36.433941: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288011 -4.428782 -4.4287539 -4.4287071 -4.4286551 -4.4286261 -4.4286432 -4.428679 -4.4286981 -4.4287233 -4.4287467 -4.42874 -4.4287214 -4.4287128 -4.4287281][-4.4288383 -4.42883 -4.4288206 -4.4287944 -4.4287519 -4.4287124 -4.4287028 -4.4287066 -4.4287019 -4.428709 -4.4287405 -4.4287581 -4.4287686 -4.4287872 -4.4288149][-4.4288626 -4.4288545 -4.42885 -4.4288282 -4.428791 -4.42875 -4.4287343 -4.4287171 -4.4286904 -4.4286757 -4.4287066 -4.4287429 -4.4287753 -4.4288125 -4.428853][-4.4288645 -4.4288473 -4.4288363 -4.42881 -4.4287825 -4.4287534 -4.4287438 -4.4287205 -4.4286766 -4.4286566 -4.4286957 -4.4287367 -4.4287667 -4.4288044 -4.4288511][-4.4288373 -4.4288 -4.4287763 -4.4287496 -4.4287281 -4.4287047 -4.4286985 -4.4286742 -4.4286284 -4.4286213 -4.4286747 -4.4287238 -4.4287515 -4.4287882 -4.4288411][-4.428803 -4.4287443 -4.4286966 -4.428659 -4.4286327 -4.4285979 -4.4285879 -4.4285612 -4.428514 -4.428524 -4.42861 -4.4286876 -4.4287286 -4.4287715 -4.4288278][-4.4287729 -4.4286938 -4.4286261 -4.4285727 -4.4285235 -4.4284573 -4.4284329 -4.4284039 -4.428359 -4.4283891 -4.4285145 -4.4286242 -4.4286885 -4.4287491 -4.4288106][-4.4287295 -4.4286404 -4.4285684 -4.4285107 -4.4284377 -4.4283442 -4.428308 -4.4282904 -4.4282632 -4.4283209 -4.4284687 -4.4285922 -4.428678 -4.428762 -4.4288278][-4.4287057 -4.4286284 -4.4285779 -4.4285336 -4.4284525 -4.4283495 -4.4283195 -4.42833 -4.428339 -4.4284096 -4.428535 -4.4286408 -4.4287291 -4.4288135 -4.4288692][-4.4287271 -4.4286604 -4.4286256 -4.4285927 -4.4285207 -4.4284306 -4.4284105 -4.428442 -4.4284883 -4.4285679 -4.4286675 -4.4287429 -4.42881 -4.428875 -4.4289165][-4.4287777 -4.4287171 -4.4286847 -4.4286594 -4.4286027 -4.428535 -4.4285169 -4.42855 -4.4286103 -4.4286971 -4.4287872 -4.428843 -4.4288869 -4.4289241 -4.4289474][-4.4288421 -4.4287992 -4.4287734 -4.4287553 -4.4287157 -4.4286628 -4.4286385 -4.4286623 -4.4287186 -4.4287977 -4.4288754 -4.4289184 -4.4289422 -4.4289584 -4.42897][-4.4289055 -4.4288812 -4.428865 -4.4288516 -4.4288273 -4.4287887 -4.4287615 -4.4287739 -4.4288173 -4.428875 -4.4289317 -4.4289584 -4.4289703 -4.428977 -4.4289837][-4.4289465 -4.428937 -4.4289284 -4.4289217 -4.4289079 -4.4288869 -4.4288645 -4.4288669 -4.428896 -4.4289327 -4.4289632 -4.4289751 -4.428978 -4.4289832 -4.4289894][-4.4289713 -4.428968 -4.4289627 -4.4289584 -4.42895 -4.4289389 -4.4289255 -4.4289246 -4.4289393 -4.4289589 -4.4289727 -4.4289789 -4.4289827 -4.4289894 -4.4289961]]...]
INFO - root - 2017-12-08 08:17:38.726694: step 54210, loss = 2.28, batch loss = 2.23 (34.4 examples/sec; 0.233 sec/batch; 17h:58m:30s remains)
INFO - root - 2017-12-08 08:17:40.987443: step 54220, loss = 2.28, batch loss = 2.23 (35.2 examples/sec; 0.227 sec/batch; 17h:32m:55s remains)
INFO - root - 2017-12-08 08:17:43.242047: step 54230, loss = 2.28, batch loss = 2.23 (35.0 examples/sec; 0.229 sec/batch; 17h:41m:35s remains)
INFO - root - 2017-12-08 08:17:45.478284: step 54240, loss = 2.28, batch loss = 2.23 (35.1 examples/sec; 0.228 sec/batch; 17h:38m:13s remains)
INFO - root - 2017-12-08 08:17:47.743270: step 54250, loss = 2.28, batch loss = 2.23 (33.9 examples/sec; 0.236 sec/batch; 18h:13m:23s remains)
INFO - root - 2017-12-08 08:17:49.988607: step 54260, loss = 2.28, batch loss = 2.23 (36.8 examples/sec; 0.218 sec/batch; 16h:49m:08s remains)
INFO - root - 2017-12-08 08:17:52.227109: step 54270, loss = 2.28, batch loss = 2.23 (35.7 examples/sec; 0.224 sec/batch; 17h:19m:39s remains)
INFO - root - 2017-12-08 08:17:54.488332: step 54280, loss = 2.28, batch loss = 2.23 (36.0 examples/sec; 0.222 sec/batch; 17h:09m:38s remains)
INFO - root - 2017-12-08 08:17:56.714644: step 54290, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 17h:05m:52s remains)
INFO - root - 2017-12-08 08:17:58.946289: step 54300, loss = 2.28, batch loss = 2.23 (35.1 examples/sec; 0.228 sec/batch; 17h:37m:04s remains)
2017-12-08 08:17:59.252563: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.42899 -4.429019 -4.4289918 -4.4288878 -4.4287477 -4.4286242 -4.4285393 -4.4285374 -4.428637 -4.4287448 -4.4287848 -4.4287767 -4.4287438 -4.4287467 -4.4287763][-4.4289947 -4.4290271 -4.4290023 -4.4289 -4.4287596 -4.4286194 -4.4285207 -4.4285283 -4.4286475 -4.4287767 -4.4288392 -4.4288645 -4.4288535 -4.4288282 -4.4288116][-4.429 -4.4290309 -4.4290056 -4.4289036 -4.4287639 -4.4286065 -4.4284749 -4.4284739 -4.42861 -4.428761 -4.4288421 -4.428884 -4.4288907 -4.428863 -4.4288177][-4.4290037 -4.4290338 -4.4290047 -4.4288926 -4.4287395 -4.4285636 -4.4283953 -4.4283714 -4.4285173 -4.4286876 -4.428792 -4.4288597 -4.4288921 -4.4288731 -4.4288087][-4.4290051 -4.4290333 -4.4289956 -4.4288673 -4.4286942 -4.4284897 -4.4282751 -4.428216 -4.4283772 -4.4285779 -4.4287195 -4.4288259 -4.4288793 -4.428865 -4.4287853][-4.4290028 -4.4290266 -4.4289804 -4.4288373 -4.4286423 -4.4284043 -4.4281497 -4.4280529 -4.4282393 -4.4284878 -4.4286757 -4.4288135 -4.4288759 -4.4288578 -4.4287643][-4.4289951 -4.4290166 -4.4289656 -4.4288163 -4.4286122 -4.4283543 -4.4280825 -4.4279823 -4.4281912 -4.4284697 -4.4286819 -4.4288311 -4.4288921 -4.4288578 -4.4287391][-4.4289885 -4.4290104 -4.4289627 -4.4288187 -4.4286275 -4.4283857 -4.4281478 -4.4280815 -4.4282846 -4.4285431 -4.4287305 -4.4288692 -4.4289188 -4.4288611 -4.4287081][-4.4289823 -4.4290071 -4.428968 -4.428843 -4.4286871 -4.4284987 -4.4283214 -4.4282956 -4.4284663 -4.4286637 -4.4288039 -4.42891 -4.4289417 -4.4288673 -4.4286962][-4.4289751 -4.4290028 -4.4289761 -4.4288759 -4.4287572 -4.4286304 -4.4285192 -4.4285259 -4.4286547 -4.4287896 -4.4288783 -4.4289384 -4.4289575 -4.42888 -4.4287243][-4.4289665 -4.4289942 -4.4289804 -4.4289041 -4.42882 -4.4287486 -4.4286962 -4.428721 -4.4288106 -4.4288983 -4.4289465 -4.4289713 -4.4289775 -4.4289031 -4.4287758][-4.4289584 -4.4289846 -4.4289794 -4.428926 -4.4288745 -4.4288421 -4.4288254 -4.42885 -4.4289103 -4.4289689 -4.4289947 -4.4290056 -4.4290018 -4.4289336 -4.428833][-4.4289508 -4.4289727 -4.4289746 -4.4289427 -4.428916 -4.4289036 -4.4289002 -4.42892 -4.4289565 -4.4289985 -4.429019 -4.4290271 -4.4290152 -4.4289579 -4.4288845][-4.42894 -4.428956 -4.4289641 -4.4289489 -4.4289346 -4.4289317 -4.428937 -4.4289513 -4.4289727 -4.4290023 -4.429018 -4.4290242 -4.4290133 -4.4289737 -4.4289303][-4.4289265 -4.4289346 -4.4289422 -4.4289346 -4.4289274 -4.4289293 -4.4289389 -4.4289513 -4.4289632 -4.4289775 -4.4289851 -4.4289927 -4.4289889 -4.428966 -4.4289441]]...]
INFO - root - 2017-12-08 08:18:01.525472: step 54310, loss = 2.28, batch loss = 2.23 (36.9 examples/sec; 0.217 sec/batch; 16h:44m:14s remains)
INFO - root - 2017-12-08 08:18:03.789471: step 54320, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.225 sec/batch; 17h:24m:01s remains)
INFO - root - 2017-12-08 08:18:06.024770: step 54330, loss = 2.28, batch loss = 2.23 (36.0 examples/sec; 0.222 sec/batch; 17h:10m:18s remains)
INFO - root - 2017-12-08 08:18:08.263650: step 54340, loss = 2.28, batch loss = 2.23 (35.3 examples/sec; 0.227 sec/batch; 17h:30m:04s remains)
INFO - root - 2017-12-08 08:18:10.481475: step 54350, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.220 sec/batch; 17h:00m:18s remains)
INFO - root - 2017-12-08 08:18:12.706959: step 54360, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.225 sec/batch; 17h:24m:41s remains)
INFO - root - 2017-12-08 08:18:14.987136: step 54370, loss = 2.28, batch loss = 2.23 (34.7 examples/sec; 0.231 sec/batch; 17h:48m:46s remains)
INFO - root - 2017-12-08 08:18:17.215761: step 54380, loss = 2.28, batch loss = 2.23 (36.6 examples/sec; 0.219 sec/batch; 16h:53m:17s remains)
INFO - root - 2017-12-08 08:18:19.443406: step 54390, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.225 sec/batch; 17h:25m:04s remains)
INFO - root - 2017-12-08 08:18:21.716356: step 54400, loss = 2.28, batch loss = 2.23 (35.0 examples/sec; 0.228 sec/batch; 17h:38m:22s remains)
2017-12-08 08:18:22.005830: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289703 -4.4289556 -4.4289212 -4.4288712 -4.4288058 -4.4287314 -4.4286742 -4.4286804 -4.4287353 -4.4287977 -4.4288454 -4.4288731 -4.4288645 -4.4288096 -4.4287333][-4.4289713 -4.428957 -4.4289227 -4.4288659 -4.4287848 -4.4286962 -4.4286346 -4.4286408 -4.4286976 -4.4287643 -4.4288125 -4.4288387 -4.4288383 -4.4287982 -4.4287319][-4.4289694 -4.4289579 -4.4289241 -4.4288564 -4.4287639 -4.4286661 -4.428606 -4.4286227 -4.4286861 -4.4287462 -4.4287782 -4.4287934 -4.4287996 -4.4287729 -4.4287162][-4.42897 -4.4289637 -4.4289317 -4.4288616 -4.4287705 -4.4286728 -4.428617 -4.4286366 -4.4286919 -4.4287305 -4.4287419 -4.428751 -4.4287696 -4.4287539 -4.4287057][-4.4289708 -4.4289637 -4.4289293 -4.4288535 -4.428762 -4.4286647 -4.4286118 -4.42864 -4.4286933 -4.4287176 -4.4287148 -4.4287133 -4.4287386 -4.4287348 -4.4287024][-4.4289684 -4.4289527 -4.4289074 -4.4288135 -4.4286952 -4.4285817 -4.4285254 -4.4285755 -4.4286642 -4.4287009 -4.4286981 -4.4286942 -4.4287148 -4.4287171 -4.4287009][-4.4289641 -4.4289384 -4.4288778 -4.4287586 -4.4285975 -4.4284396 -4.428349 -4.4284153 -4.4285631 -4.4286456 -4.4286652 -4.4286752 -4.4286971 -4.4287024 -4.4286962][-4.4289608 -4.4289303 -4.4288588 -4.4287167 -4.4285126 -4.4283004 -4.4281521 -4.4282093 -4.4284077 -4.4285464 -4.4286108 -4.42865 -4.4286847 -4.4286952 -4.4286962][-4.4289575 -4.4289274 -4.4288583 -4.4287095 -4.4284949 -4.4282756 -4.4281154 -4.42816 -4.4283648 -4.4285207 -4.4286008 -4.4286518 -4.4286895 -4.4287009 -4.4287066][-4.42895 -4.4289207 -4.4288573 -4.4287257 -4.428544 -4.428381 -4.4282713 -4.42831 -4.4284658 -4.4285855 -4.4286451 -4.4286842 -4.4287043 -4.4287071 -4.4287171][-4.4289403 -4.4289074 -4.4288478 -4.42874 -4.4286032 -4.4285035 -4.428453 -4.4284949 -4.4286056 -4.4286828 -4.4287128 -4.4287314 -4.4287367 -4.428731 -4.4287386][-4.4289341 -4.4288974 -4.428843 -4.42876 -4.4286623 -4.42861 -4.4286056 -4.4286575 -4.4287391 -4.4287844 -4.4287915 -4.4287949 -4.428791 -4.4287772 -4.4287748][-4.4289346 -4.4289002 -4.4288549 -4.4287953 -4.4287324 -4.428709 -4.4287262 -4.4287758 -4.428833 -4.4288611 -4.4288611 -4.4288568 -4.4288507 -4.4288359 -4.4288235][-4.4289422 -4.4289126 -4.4288721 -4.4288254 -4.4287834 -4.4287744 -4.4287977 -4.428843 -4.4288869 -4.4289055 -4.4289074 -4.428905 -4.4289021 -4.4288921 -4.4288745][-4.4289551 -4.42893 -4.4288888 -4.4288449 -4.4288096 -4.428803 -4.4288268 -4.4288712 -4.4289107 -4.4289289 -4.4289327 -4.4289331 -4.4289336 -4.4289284 -4.4289112]]...]
INFO - root - 2017-12-08 08:18:24.261551: step 54410, loss = 2.28, batch loss = 2.23 (35.6 examples/sec; 0.225 sec/batch; 17h:20m:53s remains)
INFO - root - 2017-12-08 08:18:26.513919: step 54420, loss = 2.28, batch loss = 2.23 (35.1 examples/sec; 0.228 sec/batch; 17h:36m:08s remains)
INFO - root - 2017-12-08 08:18:28.729546: step 54430, loss = 2.28, batch loss = 2.23 (36.9 examples/sec; 0.217 sec/batch; 16h:43m:42s remains)
INFO - root - 2017-12-08 08:18:30.965956: step 54440, loss = 2.28, batch loss = 2.23 (36.7 examples/sec; 0.218 sec/batch; 16h:48m:59s remains)
INFO - root - 2017-12-08 08:18:33.195419: step 54450, loss = 2.28, batch loss = 2.23 (37.7 examples/sec; 0.212 sec/batch; 16h:22m:31s remains)
INFO - root - 2017-12-08 08:18:35.429360: step 54460, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 16h:59m:22s remains)
INFO - root - 2017-12-08 08:18:37.654472: step 54470, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 16h:58m:46s remains)
INFO - root - 2017-12-08 08:18:39.938813: step 54480, loss = 2.28, batch loss = 2.23 (34.0 examples/sec; 0.235 sec/batch; 18h:10m:04s remains)
INFO - root - 2017-12-08 08:18:42.172823: step 54490, loss = 2.28, batch loss = 2.23 (35.1 examples/sec; 0.228 sec/batch; 17h:35m:58s remains)
INFO - root - 2017-12-08 08:18:44.396658: step 54500, loss = 2.28, batch loss = 2.23 (35.2 examples/sec; 0.227 sec/batch; 17h:32m:57s remains)
2017-12-08 08:18:44.694189: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4285436 -4.4285588 -4.4285793 -4.4285917 -4.4286041 -4.4286208 -4.4286528 -4.4286828 -4.4287124 -4.4287343 -4.4287505 -4.4287405 -4.4287038 -4.4286518 -4.4286113][-4.4285579 -4.4285569 -4.428575 -4.4285927 -4.4286084 -4.4286289 -4.4286671 -4.4286923 -4.4287066 -4.4287205 -4.4287376 -4.4287333 -4.4286966 -4.428647 -4.4286056][-4.4286027 -4.4285908 -4.4286051 -4.4286189 -4.4286304 -4.4286447 -4.4286695 -4.4286709 -4.428668 -4.4286804 -4.4287028 -4.4287124 -4.4286895 -4.4286509 -4.4286122][-4.4286585 -4.4286427 -4.4286456 -4.428638 -4.4286327 -4.4286304 -4.4286232 -4.4285965 -4.428587 -4.4286137 -4.4286551 -4.42868 -4.4286757 -4.4286556 -4.4286318][-4.4287033 -4.42868 -4.4286623 -4.4286327 -4.4286041 -4.4285717 -4.4285173 -4.4284496 -4.4284415 -4.4285069 -4.4285865 -4.4286408 -4.4286633 -4.4286723 -4.42867][-4.4287438 -4.4287114 -4.4286795 -4.4286332 -4.42858 -4.4285092 -4.4283881 -4.4282537 -4.4282603 -4.4283905 -4.4285264 -4.4286265 -4.4286809 -4.4287176 -4.4287357][-4.4287844 -4.4287577 -4.4287143 -4.4286623 -4.4285913 -4.4284935 -4.4283352 -4.4281659 -4.4281907 -4.4283624 -4.4285259 -4.4286513 -4.4287276 -4.428782 -4.4288154][-4.4288158 -4.4287939 -4.4287477 -4.4286976 -4.4286256 -4.4285307 -4.4284034 -4.428278 -4.4282961 -4.4284325 -4.4285727 -4.4286852 -4.428762 -4.428822 -4.4288683][-4.4288192 -4.428802 -4.42876 -4.4287138 -4.4286504 -4.4285722 -4.4284968 -4.4284344 -4.4284534 -4.4285469 -4.4286432 -4.4287229 -4.4287853 -4.4288387 -4.4288855][-4.4287953 -4.4287877 -4.4287586 -4.4287148 -4.4286518 -4.4285784 -4.4285374 -4.4285293 -4.4285693 -4.4286418 -4.428709 -4.4287653 -4.4288082 -4.4288449 -4.4288816][-4.4287591 -4.428761 -4.4287457 -4.4287066 -4.4286404 -4.4285736 -4.4285536 -4.4285817 -4.4286394 -4.4287024 -4.4287553 -4.4287958 -4.428823 -4.4288383 -4.428853][-4.4287271 -4.4287305 -4.4287238 -4.4286985 -4.4286494 -4.4286056 -4.4286003 -4.4286323 -4.428689 -4.4287438 -4.4287853 -4.4288125 -4.4288244 -4.4288177 -4.4288068][-4.4286995 -4.4287114 -4.4287119 -4.4287009 -4.4286809 -4.4286661 -4.4286656 -4.4286866 -4.4287319 -4.4287724 -4.428803 -4.4288158 -4.428812 -4.4287944 -4.4287696][-4.4286971 -4.4287076 -4.4287109 -4.4287119 -4.4287086 -4.4287052 -4.4287004 -4.4287133 -4.4287496 -4.4287791 -4.4287953 -4.4288006 -4.4287925 -4.4287758 -4.4287515][-4.4287033 -4.4287047 -4.4287095 -4.4287176 -4.4287243 -4.4287219 -4.4287143 -4.4287248 -4.4287567 -4.4287786 -4.4287853 -4.4287891 -4.4287858 -4.4287758 -4.4287581]]...]
INFO - root - 2017-12-08 08:18:46.905704: step 54510, loss = 2.28, batch loss = 2.23 (34.7 examples/sec; 0.230 sec/batch; 17h:47m:52s remains)
INFO - root - 2017-12-08 08:18:49.152551: step 54520, loss = 2.28, batch loss = 2.23 (33.6 examples/sec; 0.238 sec/batch; 18h:23m:34s remains)
INFO - root - 2017-12-08 08:18:51.380134: step 54530, loss = 2.28, batch loss = 2.23 (34.4 examples/sec; 0.233 sec/batch; 17h:57m:39s remains)
INFO - root - 2017-12-08 08:18:53.653572: step 54540, loss = 2.28, batch loss = 2.23 (36.6 examples/sec; 0.219 sec/batch; 16h:52m:36s remains)
INFO - root - 2017-12-08 08:18:55.901613: step 54550, loss = 2.28, batch loss = 2.23 (35.6 examples/sec; 0.225 sec/batch; 17h:20m:16s remains)
INFO - root - 2017-12-08 08:18:58.152628: step 54560, loss = 2.28, batch loss = 2.23 (33.4 examples/sec; 0.239 sec/batch; 18h:28m:46s remains)
INFO - root - 2017-12-08 08:19:00.424081: step 54570, loss = 2.28, batch loss = 2.23 (34.3 examples/sec; 0.233 sec/batch; 18h:01m:30s remains)
INFO - root - 2017-12-08 08:19:02.702278: step 54580, loss = 2.28, batch loss = 2.23 (35.0 examples/sec; 0.228 sec/batch; 17h:37m:44s remains)
INFO - root - 2017-12-08 08:19:04.942510: step 54590, loss = 2.28, batch loss = 2.23 (36.7 examples/sec; 0.218 sec/batch; 16h:49m:40s remains)
INFO - root - 2017-12-08 08:19:07.211691: step 54600, loss = 2.28, batch loss = 2.23 (34.7 examples/sec; 0.231 sec/batch; 17h:48m:49s remains)
2017-12-08 08:19:07.510572: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289265 -4.4289103 -4.4288864 -4.4288549 -4.42882 -4.4287853 -4.4287686 -4.4287663 -4.4287853 -4.4288268 -4.4288821 -4.4289346 -4.4289684 -4.4289856 -4.428988][-4.4288836 -4.4288588 -4.428822 -4.428772 -4.4287243 -4.42868 -4.4286613 -4.4286685 -4.4287105 -4.4287772 -4.4288511 -4.4289122 -4.4289541 -4.4289742 -4.4289732][-4.4288397 -4.428803 -4.428751 -4.4286947 -4.4286504 -4.4286113 -4.4286027 -4.4286261 -4.4286904 -4.4287772 -4.4288592 -4.428916 -4.4289522 -4.428967 -4.4289637][-4.4287982 -4.428751 -4.4286976 -4.4286466 -4.4286127 -4.4285831 -4.4285812 -4.4286146 -4.4286933 -4.4287891 -4.4288707 -4.4289236 -4.4289536 -4.4289646 -4.42896][-4.4287705 -4.4287186 -4.4286652 -4.4286132 -4.4285736 -4.4285469 -4.4285569 -4.4286084 -4.428699 -4.4287906 -4.4288673 -4.4289203 -4.4289489 -4.4289613 -4.4289556][-4.4287391 -4.4286809 -4.4286094 -4.428535 -4.4284825 -4.4284611 -4.4284964 -4.4285779 -4.4286842 -4.4287758 -4.42885 -4.4289041 -4.4289393 -4.4289575 -4.4289503][-4.4287 -4.4286313 -4.42853 -4.4284277 -4.4283648 -4.4283357 -4.4283752 -4.428484 -4.4286194 -4.4287305 -4.4288154 -4.4288783 -4.4289207 -4.4289446 -4.42894][-4.428668 -4.4286032 -4.4284906 -4.4283724 -4.4282837 -4.4282236 -4.4282475 -4.4283738 -4.428544 -4.4286823 -4.4287848 -4.4288564 -4.4289036 -4.4289322 -4.4289327][-4.4286213 -4.4285603 -4.4284525 -4.4283328 -4.4282336 -4.4281611 -4.4281831 -4.4283161 -4.4285035 -4.4286551 -4.4287653 -4.4288411 -4.428895 -4.4289274 -4.4289308][-4.4286127 -4.4285431 -4.4284434 -4.42834 -4.4282503 -4.4281874 -4.4282184 -4.4283433 -4.4285192 -4.4286613 -4.4287639 -4.4288373 -4.4288926 -4.4289246 -4.4289269][-4.4286761 -4.4286008 -4.4285126 -4.4284286 -4.4283538 -4.4283009 -4.4283381 -4.4284439 -4.4285903 -4.4287143 -4.4288044 -4.4288721 -4.4289174 -4.428937 -4.428937][-4.4287624 -4.4286904 -4.4286203 -4.4285555 -4.428494 -4.4284534 -4.4284844 -4.4285731 -4.4286952 -4.4287982 -4.4288697 -4.428926 -4.4289603 -4.4289651 -4.4289603][-4.4288487 -4.4287844 -4.4287343 -4.42869 -4.4286456 -4.4286203 -4.4286475 -4.4287195 -4.4288139 -4.4288921 -4.4289451 -4.4289889 -4.4290094 -4.4289989 -4.4289856][-4.4289117 -4.4288611 -4.4288273 -4.4287982 -4.4287682 -4.4287558 -4.428782 -4.4288368 -4.428905 -4.4289641 -4.4290028 -4.4290309 -4.42904 -4.4290214 -4.4290037][-4.4289427 -4.4289107 -4.42889 -4.4288726 -4.4288549 -4.4288483 -4.4288664 -4.4289017 -4.4289494 -4.4289927 -4.4290209 -4.42904 -4.4290419 -4.4290247 -4.42901]]...]
INFO - root - 2017-12-08 08:19:09.767604: step 54610, loss = 2.28, batch loss = 2.23 (34.4 examples/sec; 0.232 sec/batch; 17h:56m:36s remains)
INFO - root - 2017-12-08 08:19:11.996452: step 54620, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 17h:12m:40s remains)
INFO - root - 2017-12-08 08:19:14.228886: step 54630, loss = 2.28, batch loss = 2.23 (36.8 examples/sec; 0.217 sec/batch; 16h:46m:00s remains)
INFO - root - 2017-12-08 08:19:16.458677: step 54640, loss = 2.28, batch loss = 2.23 (36.6 examples/sec; 0.218 sec/batch; 16h:51m:31s remains)
INFO - root - 2017-12-08 08:19:18.732496: step 54650, loss = 2.28, batch loss = 2.23 (35.8 examples/sec; 0.224 sec/batch; 17h:16m:04s remains)
INFO - root - 2017-12-08 08:19:21.018022: step 54660, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.222 sec/batch; 17h:06m:25s remains)
INFO - root - 2017-12-08 08:19:23.275994: step 54670, loss = 2.28, batch loss = 2.23 (35.1 examples/sec; 0.228 sec/batch; 17h:36m:00s remains)
INFO - root - 2017-12-08 08:19:25.557783: step 54680, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.225 sec/batch; 17h:22m:31s remains)
INFO - root - 2017-12-08 08:19:27.818480: step 54690, loss = 2.28, batch loss = 2.23 (35.1 examples/sec; 0.228 sec/batch; 17h:35m:58s remains)
INFO - root - 2017-12-08 08:19:30.072869: step 54700, loss = 2.28, batch loss = 2.23 (35.1 examples/sec; 0.228 sec/batch; 17h:36m:44s remains)
2017-12-08 08:19:30.373191: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4287243 -4.4287567 -4.428803 -4.4288239 -4.42882 -4.4287958 -4.4287724 -4.4287572 -4.4287519 -4.4287248 -4.4286966 -4.4286933 -4.4287 -4.42869 -4.4286733][-4.4287124 -4.4287457 -4.4287982 -4.4288292 -4.4288392 -4.4288335 -4.4288173 -4.4287944 -4.4287815 -4.4287543 -4.4287376 -4.4287515 -4.4287672 -4.4287577 -4.4287391][-4.4287095 -4.42874 -4.4287953 -4.4288287 -4.4288492 -4.4288521 -4.4288411 -4.4288135 -4.4287958 -4.4287748 -4.4287648 -4.4287863 -4.4288058 -4.4287992 -4.4287806][-4.4287229 -4.428751 -4.4288044 -4.4288392 -4.4288626 -4.4288645 -4.4288516 -4.42882 -4.428802 -4.4287868 -4.4287767 -4.4287939 -4.4288154 -4.4288144 -4.428802][-4.4287481 -4.42877 -4.4288125 -4.428843 -4.4288588 -4.4288435 -4.4288154 -4.4287691 -4.428751 -4.4287586 -4.4287686 -4.4287825 -4.4288096 -4.4288182 -4.4288173][-4.4287658 -4.428793 -4.4288263 -4.4288383 -4.4288259 -4.4287739 -4.4286933 -4.4285941 -4.4285789 -4.428637 -4.4286919 -4.4287267 -4.4287772 -4.4288077 -4.4288206][-4.42876 -4.4288082 -4.4288397 -4.4288235 -4.42877 -4.4286604 -4.4284797 -4.4282875 -4.4282904 -4.4284325 -4.4285603 -4.4286451 -4.4287291 -4.4287839 -4.4288116][-4.4287577 -4.4288154 -4.42884 -4.4288034 -4.4287219 -4.4285603 -4.428308 -4.4280639 -4.4281149 -4.4283218 -4.4284911 -4.4286089 -4.4287014 -4.428762 -4.4287944][-4.4287844 -4.4288406 -4.4288583 -4.4288254 -4.4287586 -4.4286256 -4.4284368 -4.428278 -4.4283242 -4.4284577 -4.4285679 -4.4286427 -4.4286909 -4.4287281 -4.4287572][-4.4288363 -4.42887 -4.4288874 -4.4288735 -4.4288373 -4.42875 -4.4286432 -4.4285564 -4.42857 -4.4286146 -4.4286575 -4.4286847 -4.4286976 -4.4287076 -4.4287214][-4.4288869 -4.428894 -4.4289012 -4.4288969 -4.4288697 -4.4288054 -4.4287467 -4.4286942 -4.4286761 -4.4286604 -4.4286594 -4.4286604 -4.4286613 -4.4286685 -4.428689][-4.4288936 -4.4288764 -4.4288745 -4.4288683 -4.4288397 -4.4287877 -4.4287591 -4.4287286 -4.4286985 -4.4286523 -4.4286265 -4.4286065 -4.428606 -4.4286337 -4.42868][-4.4288635 -4.4288373 -4.4288325 -4.4288273 -4.4287968 -4.4287591 -4.4287519 -4.4287419 -4.4287171 -4.4286675 -4.4286346 -4.428606 -4.4286127 -4.428659 -4.4287195][-4.4288287 -4.4288039 -4.4288039 -4.4288044 -4.4287872 -4.4287748 -4.4287906 -4.4287982 -4.4287863 -4.4287462 -4.4287119 -4.4286809 -4.4286847 -4.4287271 -4.4287782][-4.4288087 -4.4287858 -4.42879 -4.4288039 -4.4288139 -4.428823 -4.4288454 -4.4288597 -4.4288645 -4.4288387 -4.4288082 -4.4287758 -4.428771 -4.4287963 -4.4288268]]...]
INFO - root - 2017-12-08 08:19:32.598889: step 54710, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.220 sec/batch; 16h:59m:30s remains)
INFO - root - 2017-12-08 08:19:34.832069: step 54720, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 16h:58m:14s remains)
INFO - root - 2017-12-08 08:19:37.063469: step 54730, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.221 sec/batch; 17h:04m:40s remains)
INFO - root - 2017-12-08 08:19:39.289978: step 54740, loss = 2.28, batch loss = 2.23 (35.7 examples/sec; 0.224 sec/batch; 17h:18m:27s remains)
INFO - root - 2017-12-08 08:19:41.509344: step 54750, loss = 2.28, batch loss = 2.23 (35.8 examples/sec; 0.223 sec/batch; 17h:14m:26s remains)
INFO - root - 2017-12-08 08:19:43.752383: step 54760, loss = 2.28, batch loss = 2.23 (34.3 examples/sec; 0.233 sec/batch; 17h:58m:20s remains)
INFO - root - 2017-12-08 08:19:46.009312: step 54770, loss = 2.28, batch loss = 2.23 (35.0 examples/sec; 0.228 sec/batch; 17h:36m:40s remains)
INFO - root - 2017-12-08 08:19:48.232131: step 54780, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 16h:54m:20s remains)
INFO - root - 2017-12-08 08:19:50.462253: step 54790, loss = 2.28, batch loss = 2.23 (36.9 examples/sec; 0.217 sec/batch; 16h:43m:50s remains)
INFO - root - 2017-12-08 08:19:52.692610: step 54800, loss = 2.28, batch loss = 2.23 (36.0 examples/sec; 0.222 sec/batch; 17h:07m:52s remains)
2017-12-08 08:19:52.984032: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4286485 -4.428689 -4.4287105 -4.4287133 -4.4287066 -4.4286814 -4.4286308 -4.428576 -4.4285531 -4.4285622 -4.4285874 -4.4286332 -4.4286833 -4.4287014 -4.4286904][-4.4286475 -4.4286776 -4.4286919 -4.4286895 -4.4286819 -4.4286485 -4.428587 -4.428524 -4.428503 -4.4285288 -4.428576 -4.4286318 -4.4286685 -4.4286551 -4.4286275][-4.4286833 -4.4286928 -4.4286885 -4.428668 -4.4286489 -4.4286032 -4.4285326 -4.4284625 -4.428443 -4.4284854 -4.4285455 -4.4285955 -4.4286065 -4.428566 -4.4285331][-4.4287148 -4.4286995 -4.4286723 -4.4286218 -4.4285793 -4.4285197 -4.428443 -4.4283762 -4.4283686 -4.4284368 -4.4285011 -4.4285336 -4.4285159 -4.4284534 -4.4284253][-4.4287076 -4.4286609 -4.428606 -4.4285274 -4.42846 -4.4283853 -4.4283051 -4.4282451 -4.4282589 -4.4283614 -4.4284353 -4.4284649 -4.428442 -4.4283752 -4.4283476][-4.42868 -4.4286137 -4.4285431 -4.4284573 -4.428381 -4.4283042 -4.4282179 -4.4281349 -4.4281473 -4.42829 -4.4283862 -4.4284348 -4.4284234 -4.4283628 -4.4283352][-4.4286804 -4.4286079 -4.4285369 -4.4284668 -4.4284024 -4.4283195 -4.4282055 -4.4280562 -4.4280248 -4.4282093 -4.428349 -4.4284277 -4.4284348 -4.4283843 -4.4283652][-4.4287086 -4.4286385 -4.4285779 -4.4285245 -4.4284759 -4.4283924 -4.4282427 -4.4280195 -4.4279304 -4.4281392 -4.4283223 -4.4284225 -4.4284544 -4.4284267 -4.4284186][-4.428761 -4.4287004 -4.4286456 -4.4285941 -4.4285541 -4.4284868 -4.4283371 -4.4281163 -4.4280167 -4.4281917 -4.4283533 -4.4284444 -4.4284987 -4.428514 -4.4285293][-4.4288163 -4.4287591 -4.4287047 -4.4286528 -4.4286165 -4.4285688 -4.4284635 -4.4283094 -4.4282451 -4.4283466 -4.4284477 -4.4285097 -4.4285669 -4.4286122 -4.4286513][-4.4288359 -4.4287858 -4.428741 -4.4287062 -4.4286752 -4.4286437 -4.4285803 -4.4284883 -4.4284496 -4.4284997 -4.4285555 -4.4285922 -4.42864 -4.4286947 -4.4287457][-4.42883 -4.4287949 -4.4287691 -4.4287539 -4.4287319 -4.4287057 -4.4286661 -4.4286056 -4.4285722 -4.4285874 -4.4286122 -4.4286313 -4.4286633 -4.428709 -4.42876][-4.428823 -4.4288006 -4.4287915 -4.4287796 -4.4287524 -4.4287205 -4.4286914 -4.428647 -4.4286146 -4.4286146 -4.4286308 -4.4286408 -4.4286537 -4.4286752 -4.4287038][-4.4288092 -4.428793 -4.4287877 -4.4287777 -4.4287553 -4.42873 -4.4287071 -4.4286704 -4.4286408 -4.4286385 -4.4286532 -4.4286523 -4.4286408 -4.4286337 -4.4286413][-4.428791 -4.4287753 -4.4287734 -4.4287744 -4.4287724 -4.4287677 -4.4287529 -4.4287133 -4.4286795 -4.42867 -4.4286771 -4.4286747 -4.4286518 -4.4286337 -4.4286323]]...]
INFO - root - 2017-12-08 08:19:55.207088: step 54810, loss = 2.28, batch loss = 2.23 (35.1 examples/sec; 0.228 sec/batch; 17h:35m:22s remains)
INFO - root - 2017-12-08 08:19:57.427716: step 54820, loss = 2.28, batch loss = 2.23 (36.7 examples/sec; 0.218 sec/batch; 16h:49m:03s remains)
INFO - root - 2017-12-08 08:19:59.666735: step 54830, loss = 2.28, batch loss = 2.23 (36.7 examples/sec; 0.218 sec/batch; 16h:48m:20s remains)
INFO - root - 2017-12-08 08:20:01.925638: step 54840, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.220 sec/batch; 16h:59m:12s remains)
INFO - root - 2017-12-08 08:20:04.152403: step 54850, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 17h:03m:07s remains)
INFO - root - 2017-12-08 08:20:06.392421: step 54860, loss = 2.28, batch loss = 2.23 (36.0 examples/sec; 0.222 sec/batch; 17h:08m:49s remains)
INFO - root - 2017-12-08 08:20:08.683096: step 54870, loss = 2.28, batch loss = 2.23 (32.3 examples/sec; 0.248 sec/batch; 19h:07m:03s remains)
INFO - root - 2017-12-08 08:20:10.913538: step 54880, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 17h:03m:54s remains)
INFO - root - 2017-12-08 08:20:13.139673: step 54890, loss = 2.28, batch loss = 2.23 (36.9 examples/sec; 0.217 sec/batch; 16h:43m:59s remains)
INFO - root - 2017-12-08 08:20:15.373785: step 54900, loss = 2.28, batch loss = 2.23 (35.4 examples/sec; 0.226 sec/batch; 17h:24m:21s remains)
2017-12-08 08:20:15.671149: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.428884 -4.42892 -4.4289365 -4.4289274 -4.4288945 -4.4288573 -4.4288306 -4.428823 -4.4288325 -4.4288507 -4.4288726 -4.4288893 -4.428894 -4.4288969 -4.428896][-4.4288912 -4.4289241 -4.4289384 -4.4289274 -4.4288974 -4.4288654 -4.4288387 -4.4288244 -4.4288287 -4.4288459 -4.4288721 -4.4288945 -4.4289012 -4.428896 -4.4288807][-4.4289088 -4.4289403 -4.4289551 -4.4289508 -4.4289327 -4.4289155 -4.4288955 -4.4288774 -4.4288683 -4.4288721 -4.4288926 -4.4289103 -4.4289093 -4.4288874 -4.4288592][-4.4289269 -4.4289556 -4.4289708 -4.4289708 -4.4289603 -4.4289522 -4.4289374 -4.4289188 -4.4289021 -4.4288945 -4.4289012 -4.4289064 -4.4288917 -4.4288478 -4.428803][-4.4289374 -4.428968 -4.4289832 -4.4289789 -4.4289589 -4.4289408 -4.4289188 -4.4288893 -4.4288621 -4.4288554 -4.428864 -4.42887 -4.4288507 -4.4287853 -4.4287229][-4.4289284 -4.4289584 -4.4289684 -4.4289489 -4.428906 -4.4288592 -4.4288049 -4.4287415 -4.4286981 -4.4287038 -4.4287481 -4.4287891 -4.4287853 -4.428709 -4.4286256][-4.4288917 -4.4289107 -4.4289136 -4.4288836 -4.4288216 -4.4287472 -4.4286504 -4.4285374 -4.4284611 -4.4284854 -4.428586 -4.4286852 -4.4287152 -4.428647 -4.4285569][-4.4288511 -4.4288578 -4.4288554 -4.4288287 -4.4287686 -4.428689 -4.428576 -4.4284387 -4.4283376 -4.4283624 -4.4284978 -4.4286332 -4.4286923 -4.4286485 -4.4285722][-4.4288239 -4.42882 -4.4288144 -4.4287968 -4.4287586 -4.4287086 -4.4286318 -4.4285345 -4.4284534 -4.4284635 -4.4285622 -4.42867 -4.4287271 -4.4287038 -4.4286427][-4.4288054 -4.42879 -4.4287748 -4.428761 -4.4287453 -4.4287314 -4.4287081 -4.4286733 -4.4286366 -4.4286447 -4.4287014 -4.4287653 -4.428803 -4.4287877 -4.4287395][-4.4288058 -4.428791 -4.42877 -4.4287472 -4.4287357 -4.4287438 -4.4287581 -4.4287639 -4.4287596 -4.4287796 -4.428823 -4.4288645 -4.4288821 -4.4288564 -4.4288106][-4.4288263 -4.4288254 -4.4288025 -4.4287658 -4.4287376 -4.4287386 -4.4287615 -4.428782 -4.4287977 -4.4288344 -4.428884 -4.4289274 -4.4289341 -4.42889 -4.4288325][-4.4288473 -4.4288645 -4.4288445 -4.4287939 -4.4287405 -4.4287114 -4.4287128 -4.4287233 -4.4287386 -4.4287868 -4.4288473 -4.4288979 -4.428905 -4.4288564 -4.4287968][-4.4288611 -4.4288893 -4.4288716 -4.4288177 -4.4287524 -4.4287 -4.4286771 -4.428669 -4.428678 -4.4287233 -4.4287796 -4.4288244 -4.42883 -4.42879 -4.4287415][-4.4288788 -4.4289117 -4.4288993 -4.4288526 -4.4287925 -4.4287386 -4.4287062 -4.4286904 -4.4286985 -4.4287333 -4.4287677 -4.4287896 -4.42879 -4.4287643 -4.4287367]]...]
INFO - root - 2017-12-08 08:20:17.883698: step 54910, loss = 2.28, batch loss = 2.23 (36.8 examples/sec; 0.217 sec/batch; 16h:45m:59s remains)
INFO - root - 2017-12-08 08:20:20.128985: step 54920, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 17h:01m:16s remains)
INFO - root - 2017-12-08 08:20:22.387448: step 54930, loss = 2.28, batch loss = 2.23 (35.4 examples/sec; 0.226 sec/batch; 17h:25m:11s remains)
INFO - root - 2017-12-08 08:20:24.605765: step 54940, loss = 2.28, batch loss = 2.23 (34.8 examples/sec; 0.230 sec/batch; 17h:42m:07s remains)
INFO - root - 2017-12-08 08:20:26.871286: step 54950, loss = 2.28, batch loss = 2.23 (35.1 examples/sec; 0.228 sec/batch; 17h:34m:18s remains)
INFO - root - 2017-12-08 08:20:29.099052: step 54960, loss = 2.28, batch loss = 2.23 (35.7 examples/sec; 0.224 sec/batch; 17h:16m:10s remains)
INFO - root - 2017-12-08 08:20:31.329649: step 54970, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 16h:55m:44s remains)
INFO - root - 2017-12-08 08:20:33.547358: step 54980, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 16h:53m:18s remains)
INFO - root - 2017-12-08 08:20:35.780346: step 54990, loss = 2.28, batch loss = 2.23 (37.2 examples/sec; 0.215 sec/batch; 16h:34m:05s remains)
INFO - root - 2017-12-08 08:20:38.032050: step 55000, loss = 2.28, batch loss = 2.23 (37.0 examples/sec; 0.216 sec/batch; 16h:40m:06s remains)
2017-12-08 08:20:38.349998: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4287934 -4.428762 -4.4287462 -4.4287443 -4.4287581 -4.4287772 -4.42879 -4.4287896 -4.4287868 -4.4287863 -4.4287944 -4.4288087 -4.4288139 -4.4287972 -4.4287553][-4.4287615 -4.4287424 -4.4287453 -4.4287648 -4.4287915 -4.4288087 -4.428812 -4.4288044 -4.4287915 -4.4287829 -4.4287858 -4.4287949 -4.4288011 -4.4287844 -4.4287372][-4.4287472 -4.4287505 -4.4287763 -4.4288073 -4.4288282 -4.4288306 -4.4288144 -4.4287858 -4.4287534 -4.4287333 -4.4287367 -4.4287548 -4.4287739 -4.42877 -4.4287319][-4.4287615 -4.428792 -4.4288316 -4.4288521 -4.4288526 -4.4288321 -4.4287953 -4.4287405 -4.4286833 -4.4286456 -4.4286532 -4.4286928 -4.4287362 -4.4287567 -4.4287477][-4.4288044 -4.4288468 -4.4288774 -4.42888 -4.4288568 -4.428813 -4.4287543 -4.4286723 -4.4285831 -4.4285264 -4.4285336 -4.4285946 -4.4286695 -4.4287338 -4.4287729][-4.4288392 -4.4288793 -4.4288917 -4.4288683 -4.42882 -4.4287543 -4.4286685 -4.4285536 -4.428432 -4.4283576 -4.4283743 -4.4284663 -4.4285884 -4.4287076 -4.428791][-4.4288311 -4.4288659 -4.4288607 -4.4288163 -4.4287438 -4.4286537 -4.4285412 -4.4283962 -4.4282589 -4.4281955 -4.4282427 -4.4283738 -4.4285407 -4.4286962 -4.4288054][-4.428782 -4.4288192 -4.4288077 -4.4287491 -4.4286556 -4.4285445 -4.4284244 -4.4282851 -4.4281812 -4.4281716 -4.4282613 -4.4284115 -4.4285822 -4.4287333 -4.4288383][-4.4286852 -4.4287481 -4.4287496 -4.4286928 -4.4286013 -4.4285016 -4.4284139 -4.4283314 -4.428299 -4.4283485 -4.4284549 -4.4285841 -4.4287138 -4.4288259 -4.4289007][-4.4285536 -4.4286623 -4.4287038 -4.4286833 -4.4286304 -4.4285779 -4.428545 -4.4285259 -4.4285464 -4.4286156 -4.4287019 -4.428782 -4.4288583 -4.4289231 -4.4289584][-4.4284635 -4.428618 -4.4287095 -4.4287429 -4.4287415 -4.4287362 -4.4287424 -4.4287577 -4.4287896 -4.4288368 -4.4288836 -4.4289174 -4.4289513 -4.428978 -4.4289846][-4.4285049 -4.4286671 -4.4287786 -4.4288406 -4.4288669 -4.4288821 -4.4289007 -4.4289231 -4.428946 -4.4289651 -4.4289746 -4.4289761 -4.4289827 -4.4289875 -4.4289804][-4.4286666 -4.428793 -4.4288836 -4.4289317 -4.4289484 -4.4289565 -4.4289703 -4.4289861 -4.4289985 -4.4290032 -4.428997 -4.4289865 -4.4289818 -4.4289775 -4.4289703][-4.428834 -4.4289122 -4.428966 -4.428987 -4.4289818 -4.428977 -4.4289789 -4.4289865 -4.4289927 -4.4289918 -4.4289856 -4.4289784 -4.4289756 -4.4289732 -4.4289684][-4.4289408 -4.4289727 -4.4289918 -4.4289904 -4.428977 -4.428967 -4.4289646 -4.4289684 -4.4289732 -4.4289751 -4.4289765 -4.4289775 -4.428978 -4.4289751 -4.42897]]...]
INFO - root - 2017-12-08 08:20:40.592896: step 55010, loss = 2.28, batch loss = 2.23 (35.8 examples/sec; 0.224 sec/batch; 17h:14m:42s remains)
INFO - root - 2017-12-08 08:20:42.858141: step 55020, loss = 2.28, batch loss = 2.23 (36.0 examples/sec; 0.222 sec/batch; 17h:06m:35s remains)
INFO - root - 2017-12-08 08:20:45.089352: step 55030, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.220 sec/batch; 16h:59m:24s remains)
INFO - root - 2017-12-08 08:20:47.315307: step 55040, loss = 2.28, batch loss = 2.23 (37.6 examples/sec; 0.213 sec/batch; 16h:23m:11s remains)
INFO - root - 2017-12-08 08:20:49.551532: step 55050, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 16h:53m:10s remains)
INFO - root - 2017-12-08 08:20:51.777520: step 55060, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 16h:53m:28s remains)
INFO - root - 2017-12-08 08:20:54.025049: step 55070, loss = 2.28, batch loss = 2.23 (35.8 examples/sec; 0.223 sec/batch; 17h:13m:19s remains)
INFO - root - 2017-12-08 08:20:56.246087: step 55080, loss = 2.28, batch loss = 2.23 (37.7 examples/sec; 0.212 sec/batch; 16h:21m:53s remains)
INFO - root - 2017-12-08 08:20:58.482876: step 55090, loss = 2.28, batch loss = 2.23 (35.6 examples/sec; 0.225 sec/batch; 17h:19m:39s remains)
INFO - root - 2017-12-08 08:21:00.708598: step 55100, loss = 2.28, batch loss = 2.23 (35.2 examples/sec; 0.227 sec/batch; 17h:29m:42s remains)
2017-12-08 08:21:01.058818: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4290137 -4.4290133 -4.4290104 -4.4290113 -4.4290133 -4.4290147 -4.4290142 -4.4290113 -4.429009 -4.4290094 -4.4290118 -4.4290109 -4.4290066 -4.428997 -4.4289784][-4.4290156 -4.4290175 -4.4290166 -4.4290171 -4.4290171 -4.4290152 -4.42901 -4.4290023 -4.428998 -4.4290009 -4.4290094 -4.4290171 -4.4290204 -4.4290166 -4.4290032][-4.4290133 -4.4290161 -4.4290137 -4.4290109 -4.4290051 -4.4289942 -4.4289784 -4.4289618 -4.4289551 -4.4289637 -4.4289842 -4.4290071 -4.4290237 -4.429029 -4.4290214][-4.4290113 -4.4290128 -4.4290071 -4.4289942 -4.4289727 -4.4289389 -4.4288988 -4.4288626 -4.4288497 -4.42887 -4.4289136 -4.4289651 -4.4290075 -4.4290285 -4.42903][-4.4290061 -4.4290037 -4.4289889 -4.428956 -4.4289002 -4.4288206 -4.4287324 -4.4286633 -4.4286485 -4.4286976 -4.4287829 -4.42888 -4.4289603 -4.4290061 -4.4290228][-4.4289966 -4.4289851 -4.4289522 -4.4288816 -4.4287667 -4.4286122 -4.4284592 -4.4283576 -4.428359 -4.4284596 -4.428607 -4.4287624 -4.4288864 -4.4289618 -4.429][-4.4289832 -4.4289575 -4.428894 -4.42877 -4.4285822 -4.4283514 -4.4281464 -4.4280353 -4.428081 -4.4282537 -4.4284673 -4.4286728 -4.4288268 -4.4289222 -4.4289756][-4.4289675 -4.4289222 -4.428823 -4.4286485 -4.4284086 -4.4281449 -4.4279447 -4.427876 -4.42799 -4.4282212 -4.4284658 -4.428679 -4.4288282 -4.4289174 -4.4289665][-4.4289527 -4.4288926 -4.4287748 -4.42859 -4.4283614 -4.4281435 -4.4280181 -4.4280229 -4.4281678 -4.4283824 -4.4285927 -4.4287677 -4.4288826 -4.4289446 -4.4289751][-4.4289479 -4.42889 -4.4287863 -4.4286385 -4.4284768 -4.4283476 -4.4283009 -4.4283423 -4.4284635 -4.4286141 -4.4287586 -4.428875 -4.4289479 -4.4289813 -4.4289918][-4.4289584 -4.4289203 -4.4288521 -4.4287643 -4.428678 -4.4286213 -4.4286184 -4.4286618 -4.4287386 -4.4288244 -4.4289074 -4.4289713 -4.4290075 -4.4290171 -4.4290133][-4.428977 -4.4289608 -4.4289279 -4.4288898 -4.4288559 -4.4288397 -4.42885 -4.4288821 -4.4289241 -4.428967 -4.429009 -4.4290361 -4.4290466 -4.4290419 -4.4290304][-4.4289942 -4.4289923 -4.4289808 -4.42897 -4.4289622 -4.4289637 -4.4289775 -4.429 -4.4290218 -4.4290414 -4.4290566 -4.4290628 -4.429059 -4.4290476 -4.4290347][-4.4290075 -4.4290123 -4.4290109 -4.4290118 -4.4290156 -4.4290228 -4.4290357 -4.4290495 -4.4290586 -4.4290633 -4.4290633 -4.4290586 -4.429049 -4.4290385 -4.4290295][-4.4290147 -4.4290214 -4.4290223 -4.4290252 -4.4290295 -4.4290352 -4.4290419 -4.4290481 -4.4290504 -4.429049 -4.4290447 -4.429038 -4.4290309 -4.4290242 -4.42902]]...]
INFO - root - 2017-12-08 08:21:03.292775: step 55110, loss = 2.28, batch loss = 2.23 (35.7 examples/sec; 0.224 sec/batch; 17h:16m:01s remains)
INFO - root - 2017-12-08 08:21:05.522467: step 55120, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.220 sec/batch; 16h:57m:26s remains)
INFO - root - 2017-12-08 08:21:07.750693: step 55130, loss = 2.28, batch loss = 2.23 (35.2 examples/sec; 0.227 sec/batch; 17h:30m:53s remains)
INFO - root - 2017-12-08 08:21:10.041614: step 55140, loss = 2.28, batch loss = 2.23 (34.5 examples/sec; 0.232 sec/batch; 17h:51m:32s remains)
INFO - root - 2017-12-08 08:21:12.277271: step 55150, loss = 2.28, batch loss = 2.23 (35.3 examples/sec; 0.227 sec/batch; 17h:27m:50s remains)
INFO - root - 2017-12-08 08:21:14.511728: step 55160, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 17h:00m:24s remains)
INFO - root - 2017-12-08 08:21:16.735785: step 55170, loss = 2.28, batch loss = 2.23 (34.3 examples/sec; 0.233 sec/batch; 17h:59m:01s remains)
INFO - root - 2017-12-08 08:21:18.977906: step 55180, loss = 2.28, batch loss = 2.23 (34.6 examples/sec; 0.231 sec/batch; 17h:48m:46s remains)
INFO - root - 2017-12-08 08:21:21.181310: step 55190, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 16h:55m:05s remains)
INFO - root - 2017-12-08 08:21:23.432779: step 55200, loss = 2.28, batch loss = 2.23 (35.6 examples/sec; 0.225 sec/batch; 17h:17m:56s remains)
2017-12-08 08:21:23.733259: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4287791 -4.42878 -4.4287815 -4.4287782 -4.4287729 -4.4287696 -4.4287643 -4.4287634 -4.4287663 -4.4287724 -4.4287829 -4.4288 -4.4288259 -4.4288616 -4.4288988][-4.4287286 -4.42874 -4.4287539 -4.4287639 -4.428772 -4.4287786 -4.4287791 -4.4287806 -4.4287834 -4.4287896 -4.4287958 -4.4288034 -4.4288154 -4.4288373 -4.428865][-4.4286718 -4.4286938 -4.4287205 -4.4287457 -4.4287686 -4.4287863 -4.4287939 -4.4287977 -4.4288034 -4.4288125 -4.4288182 -4.4288192 -4.4288158 -4.4288177 -4.4288268][-4.4286566 -4.4286819 -4.4287095 -4.4287353 -4.4287591 -4.4287772 -4.4287872 -4.4287958 -4.4288125 -4.4288344 -4.4288487 -4.4288516 -4.4288397 -4.4288211 -4.4288049][-4.4286909 -4.4287024 -4.4287157 -4.4287267 -4.4287353 -4.4287381 -4.4287367 -4.4287434 -4.4287739 -4.4288154 -4.4288478 -4.4288607 -4.4288507 -4.428823 -4.4287896][-4.428699 -4.4286876 -4.4286766 -4.4286613 -4.4286418 -4.4286137 -4.4285841 -4.4285822 -4.4286337 -4.42871 -4.4287744 -4.42881 -4.428813 -4.4287844 -4.4287419][-4.4286675 -4.428638 -4.4286032 -4.4285622 -4.4285116 -4.4284391 -4.4283648 -4.4283442 -4.4284143 -4.4285297 -4.4286342 -4.4287019 -4.4287252 -4.4287038 -4.4286566][-4.4286442 -4.4286146 -4.4285812 -4.4285445 -4.4284945 -4.4284115 -4.4283152 -4.428268 -4.4283171 -4.4284277 -4.4285407 -4.4286246 -4.4286666 -4.4286594 -4.4286208][-4.4286671 -4.4286461 -4.4286304 -4.4286189 -4.4285965 -4.4285455 -4.4284797 -4.4284453 -4.4284697 -4.4285345 -4.428607 -4.4286666 -4.4287019 -4.4287038 -4.4286809][-4.4287148 -4.4286952 -4.4286871 -4.4286909 -4.4286919 -4.4286757 -4.4286485 -4.428638 -4.428658 -4.4286966 -4.4287362 -4.4287658 -4.4287786 -4.4287734 -4.4287543][-4.4287629 -4.4287448 -4.4287372 -4.4287429 -4.42875 -4.4287486 -4.4287405 -4.4287419 -4.4287581 -4.428782 -4.4288034 -4.4288158 -4.4288139 -4.4288011 -4.42878][-4.4287858 -4.4287734 -4.4287663 -4.42877 -4.4287777 -4.4287825 -4.4287868 -4.4287939 -4.4288058 -4.4288187 -4.4288259 -4.4288225 -4.4288077 -4.4287887 -4.4287682][-4.42877 -4.4287648 -4.4287591 -4.428761 -4.4287682 -4.4287772 -4.4287896 -4.428802 -4.4288125 -4.4288177 -4.4288125 -4.4287958 -4.4287715 -4.4287481 -4.4287343][-4.4287634 -4.4287586 -4.4287519 -4.42875 -4.4287524 -4.4287591 -4.42877 -4.428781 -4.4287896 -4.4287896 -4.4287782 -4.4287562 -4.4287276 -4.4287033 -4.4286976][-4.4287786 -4.4287705 -4.4287615 -4.4287553 -4.4287529 -4.4287553 -4.4287629 -4.4287715 -4.4287782 -4.4287763 -4.4287634 -4.4287419 -4.4287138 -4.4286885 -4.4286833]]...]
INFO - root - 2017-12-08 08:21:25.984249: step 55210, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.221 sec/batch; 17h:03m:30s remains)
INFO - root - 2017-12-08 08:21:28.221468: step 55220, loss = 2.28, batch loss = 2.23 (34.7 examples/sec; 0.230 sec/batch; 17h:44m:04s remains)
INFO - root - 2017-12-08 08:21:30.462491: step 55230, loss = 2.28, batch loss = 2.23 (37.3 examples/sec; 0.214 sec/batch; 16h:31m:04s remains)
INFO - root - 2017-12-08 08:21:32.739005: step 55240, loss = 2.28, batch loss = 2.23 (37.1 examples/sec; 0.216 sec/batch; 16h:36m:04s remains)
INFO - root - 2017-12-08 08:21:34.971839: step 55250, loss = 2.28, batch loss = 2.23 (37.4 examples/sec; 0.214 sec/batch; 16h:28m:01s remains)
INFO - root - 2017-12-08 08:21:37.204249: step 55260, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 17h:00m:05s remains)
INFO - root - 2017-12-08 08:21:39.493098: step 55270, loss = 2.28, batch loss = 2.23 (34.7 examples/sec; 0.230 sec/batch; 17h:44m:52s remains)
INFO - root - 2017-12-08 08:21:41.743485: step 55280, loss = 2.28, batch loss = 2.23 (34.4 examples/sec; 0.233 sec/batch; 17h:55m:47s remains)
INFO - root - 2017-12-08 08:21:43.974370: step 55290, loss = 2.28, batch loss = 2.23 (33.7 examples/sec; 0.237 sec/batch; 18h:15m:38s remains)
INFO - root - 2017-12-08 08:21:46.169766: step 55300, loss = 2.28, batch loss = 2.23 (36.0 examples/sec; 0.222 sec/batch; 17h:07m:44s remains)
2017-12-08 08:21:46.481294: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4286342 -4.42861 -4.4285846 -4.4285545 -4.4285541 -4.4285674 -4.4285917 -4.4286237 -4.428658 -4.4286785 -4.4286895 -4.4286904 -4.4287038 -4.4287138 -4.4287148][-4.4286809 -4.4286761 -4.4286609 -4.4286346 -4.4286456 -4.4286652 -4.4286876 -4.4287038 -4.4287124 -4.4287105 -4.4287066 -4.4286971 -4.4287038 -4.428709 -4.42871][-4.4286861 -4.4286976 -4.4286976 -4.4286828 -4.4287043 -4.4287252 -4.4287415 -4.4287415 -4.4287333 -4.4287181 -4.4287009 -4.4286776 -4.4286737 -4.4286704 -4.428668][-4.42863 -4.4286447 -4.4286551 -4.4286528 -4.4286866 -4.4287128 -4.4287276 -4.4287181 -4.4287009 -4.4286809 -4.4286609 -4.42864 -4.4286346 -4.4286284 -4.4286208][-4.4285641 -4.4285607 -4.4285588 -4.4285574 -4.4285989 -4.4286308 -4.428648 -4.4286304 -4.4285984 -4.4285645 -4.4285436 -4.4285331 -4.4285407 -4.428546 -4.4285417][-4.4284573 -4.4284167 -4.4283834 -4.4283795 -4.4284286 -4.4284692 -4.4284925 -4.4284635 -4.4284034 -4.428359 -4.4283509 -4.4283638 -4.4284129 -4.4284606 -4.4284778][-4.428329 -4.4282694 -4.4282174 -4.4282107 -4.4282594 -4.4282937 -4.4282994 -4.4282346 -4.4281344 -4.4280896 -4.4281125 -4.4281678 -4.42828 -4.42839 -4.4284449][-4.428299 -4.4282541 -4.4282107 -4.4282088 -4.42824 -4.4282475 -4.4282274 -4.4281387 -4.4280148 -4.4279647 -4.4279966 -4.4280705 -4.4282174 -4.4283614 -4.4284387][-4.4283967 -4.4283624 -4.4283333 -4.4283395 -4.4283628 -4.42836 -4.4283371 -4.4282713 -4.4281859 -4.4281487 -4.428165 -4.4282184 -4.4283261 -4.4284234 -4.4284587][-4.4285636 -4.4285388 -4.4285226 -4.4285331 -4.42855 -4.4285431 -4.4285216 -4.4284782 -4.42843 -4.4284039 -4.4284015 -4.4284244 -4.4284787 -4.4285073 -4.4284782][-4.4287119 -4.4286938 -4.4286809 -4.4286842 -4.4286952 -4.4286933 -4.428679 -4.4286509 -4.4286213 -4.428597 -4.4285827 -4.4285831 -4.4285965 -4.42858 -4.4285116][-4.4288044 -4.4287896 -4.4287796 -4.4287791 -4.4287882 -4.428792 -4.4287844 -4.4287639 -4.4287395 -4.4287143 -4.4286957 -4.42869 -4.428678 -4.4286361 -4.428566][-4.4288664 -4.428854 -4.4288445 -4.4288449 -4.4288526 -4.4288578 -4.4288554 -4.4288406 -4.4288144 -4.4287844 -4.42876 -4.4287486 -4.4287243 -4.4286747 -4.42862][-4.4289017 -4.4288931 -4.4288883 -4.4288893 -4.428894 -4.4288974 -4.428896 -4.4288845 -4.4288578 -4.4288225 -4.42879 -4.4287691 -4.4287353 -4.4286914 -4.4286575][-4.4289231 -4.4289193 -4.4289188 -4.4289227 -4.4289269 -4.428926 -4.4289212 -4.42891 -4.4288869 -4.4288545 -4.4288197 -4.4287939 -4.4287558 -4.4287162 -4.4286947]]...]
INFO - root - 2017-12-08 08:21:48.718420: step 55310, loss = 2.28, batch loss = 2.23 (34.6 examples/sec; 0.231 sec/batch; 17h:47m:53s remains)
INFO - root - 2017-12-08 08:21:50.943084: step 55320, loss = 2.28, batch loss = 2.23 (36.0 examples/sec; 0.222 sec/batch; 17h:05m:52s remains)
INFO - root - 2017-12-08 08:21:53.168613: step 55330, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 17h:02m:08s remains)
INFO - root - 2017-12-08 08:21:55.387879: step 55340, loss = 2.28, batch loss = 2.23 (36.9 examples/sec; 0.217 sec/batch; 16h:42m:31s remains)
INFO - root - 2017-12-08 08:21:57.619690: step 55350, loss = 2.28, batch loss = 2.23 (37.6 examples/sec; 0.213 sec/batch; 16h:22m:00s remains)
INFO - root - 2017-12-08 08:21:59.851161: step 55360, loss = 2.28, batch loss = 2.23 (36.0 examples/sec; 0.222 sec/batch; 17h:07m:08s remains)
INFO - root - 2017-12-08 08:22:02.077250: step 55370, loss = 2.28, batch loss = 2.23 (36.7 examples/sec; 0.218 sec/batch; 16h:46m:59s remains)
INFO - root - 2017-12-08 08:22:04.384707: step 55380, loss = 2.28, batch loss = 2.23 (34.0 examples/sec; 0.236 sec/batch; 18h:07m:54s remains)
INFO - root - 2017-12-08 08:22:06.626700: step 55390, loss = 2.28, batch loss = 2.23 (34.2 examples/sec; 0.234 sec/batch; 17h:58m:50s remains)
INFO - root - 2017-12-08 08:22:08.902968: step 55400, loss = 2.28, batch loss = 2.23 (33.6 examples/sec; 0.238 sec/batch; 18h:21m:13s remains)
2017-12-08 08:22:09.201532: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.42868 -4.4287124 -4.4287591 -4.4288168 -4.4288745 -4.4289107 -4.4289036 -4.4288564 -4.4287829 -4.4287014 -4.4286332 -4.428575 -4.4286036 -4.4287047 -4.4287782][-4.42872 -4.4287624 -4.4288158 -4.4288712 -4.428916 -4.4289408 -4.4289303 -4.42889 -4.4288306 -4.4287682 -4.4287167 -4.4286728 -4.42869 -4.428761 -4.42882][-4.4287748 -4.42882 -4.4288688 -4.4289079 -4.4289284 -4.4289303 -4.4289126 -4.4288797 -4.4288416 -4.4288054 -4.428782 -4.4287663 -4.42878 -4.4288187 -4.4288597][-4.4288044 -4.4288526 -4.4288926 -4.4289055 -4.4288898 -4.4288578 -4.428823 -4.4288006 -4.4288025 -4.4288135 -4.4288173 -4.428822 -4.4288244 -4.4288321 -4.4288568][-4.428802 -4.4288511 -4.428874 -4.428854 -4.428791 -4.4287076 -4.4286361 -4.4286332 -4.4287076 -4.4287877 -4.4288292 -4.4288483 -4.4288368 -4.4288163 -4.4288244][-4.4287791 -4.4288282 -4.4288373 -4.428781 -4.4286604 -4.4285011 -4.4283538 -4.4283648 -4.4285412 -4.4287076 -4.428802 -4.4288445 -4.428823 -4.4287767 -4.4287672][-4.4287739 -4.4288192 -4.4288192 -4.4287362 -4.4285707 -4.4283376 -4.4280896 -4.4281092 -4.4283886 -4.4286256 -4.4287591 -4.4288259 -4.42881 -4.4287496 -4.4287219][-4.4288149 -4.4288497 -4.4288445 -4.42876 -4.4286 -4.4283714 -4.4281292 -4.4281549 -4.4284067 -4.4286213 -4.4287405 -4.4288077 -4.4287949 -4.4287248 -4.4286823][-4.4288774 -4.4289 -4.428894 -4.4288268 -4.4287086 -4.4285526 -4.4284029 -4.4284172 -4.4285588 -4.4286804 -4.4287448 -4.4287848 -4.4287519 -4.4286594 -4.4286][-4.4289389 -4.4289508 -4.4289474 -4.4289041 -4.4288325 -4.4287462 -4.428669 -4.4286766 -4.4287324 -4.4287648 -4.4287629 -4.4287581 -4.4286861 -4.4285579 -4.42848][-4.428968 -4.4289732 -4.4289708 -4.4289489 -4.42892 -4.4288855 -4.4288492 -4.4288521 -4.428863 -4.4288487 -4.4288106 -4.4287634 -4.4286575 -4.4284916 -4.42841][-4.4289646 -4.428966 -4.4289646 -4.4289632 -4.4289637 -4.4289589 -4.4289474 -4.4289513 -4.4289474 -4.4289141 -4.4288611 -4.4287939 -4.4286847 -4.4285297 -4.4284639][-4.4289422 -4.4289412 -4.4289432 -4.428956 -4.4289708 -4.4289818 -4.428987 -4.4290004 -4.4289947 -4.4289603 -4.4289107 -4.4288511 -4.4287715 -4.4286637 -4.4286213][-4.4289289 -4.4289265 -4.42893 -4.4289436 -4.4289594 -4.4289742 -4.4289865 -4.4290066 -4.4290032 -4.4289808 -4.4289503 -4.4289122 -4.4288692 -4.4288158 -4.4287972][-4.4289207 -4.4289207 -4.4289227 -4.4289312 -4.4289432 -4.428957 -4.428968 -4.4289837 -4.4289808 -4.428968 -4.4289594 -4.4289455 -4.4289289 -4.4289045 -4.4288974]]...]
INFO - root - 2017-12-08 08:22:11.441930: step 55410, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.225 sec/batch; 17h:20m:32s remains)
INFO - root - 2017-12-08 08:22:13.717816: step 55420, loss = 2.28, batch loss = 2.23 (35.1 examples/sec; 0.228 sec/batch; 17h:33m:18s remains)
INFO - root - 2017-12-08 08:22:16.007804: step 55430, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 16h:54m:09s remains)
INFO - root - 2017-12-08 08:22:18.266691: step 55440, loss = 2.28, batch loss = 2.23 (36.8 examples/sec; 0.218 sec/batch; 16h:44m:59s remains)
INFO - root - 2017-12-08 08:22:20.510104: step 55450, loss = 2.28, batch loss = 2.23 (34.9 examples/sec; 0.229 sec/batch; 17h:37m:48s remains)
INFO - root - 2017-12-08 08:22:22.733297: step 55460, loss = 2.28, batch loss = 2.23 (36.0 examples/sec; 0.222 sec/batch; 17h:06m:48s remains)
INFO - root - 2017-12-08 08:22:24.962864: step 55470, loss = 2.28, batch loss = 2.23 (34.8 examples/sec; 0.230 sec/batch; 17h:41m:26s remains)
INFO - root - 2017-12-08 08:22:27.216691: step 55480, loss = 2.28, batch loss = 2.23 (37.5 examples/sec; 0.213 sec/batch; 16h:25m:33s remains)
INFO - root - 2017-12-08 08:22:29.481509: step 55490, loss = 2.28, batch loss = 2.23 (35.2 examples/sec; 0.227 sec/batch; 17h:29m:58s remains)
INFO - root - 2017-12-08 08:22:31.739742: step 55500, loss = 2.28, batch loss = 2.23 (35.0 examples/sec; 0.228 sec/batch; 17h:34m:32s remains)
2017-12-08 08:22:32.038227: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4286065 -4.4286242 -4.4286504 -4.4287033 -4.4287419 -4.428719 -4.4286418 -4.4286132 -4.4286332 -4.42868 -4.4287562 -4.4288325 -4.4288983 -4.4289441 -4.4289527][-4.428679 -4.4287047 -4.4287314 -4.4287648 -4.4287786 -4.4287467 -4.4286761 -4.428638 -4.4286151 -4.4286304 -4.4287047 -4.4287882 -4.4288707 -4.4289341 -4.4289513][-4.4287624 -4.4287906 -4.4288054 -4.428812 -4.4287987 -4.4287581 -4.4286957 -4.4286509 -4.4285951 -4.4285927 -4.4286685 -4.4287505 -4.4288449 -4.4289165 -4.42894][-4.4288206 -4.4288416 -4.4288464 -4.4288378 -4.4288139 -4.42878 -4.428719 -4.4286389 -4.4285483 -4.4285588 -4.4286528 -4.4287486 -4.4288468 -4.4289141 -4.4289365][-4.428863 -4.428874 -4.4288712 -4.4288611 -4.4288406 -4.4288111 -4.4287224 -4.4285684 -4.4284358 -4.4284964 -4.4286437 -4.4287705 -4.4288697 -4.4289241 -4.4289422][-4.4288688 -4.4288726 -4.4288659 -4.428853 -4.4288321 -4.4287868 -4.4286461 -4.4284048 -4.4282551 -4.4284039 -4.4286275 -4.4287882 -4.4288878 -4.4289274 -4.4289403][-4.4288359 -4.4288263 -4.4288144 -4.4288025 -4.4287786 -4.428709 -4.4285259 -4.4282503 -4.4281406 -4.4283652 -4.4286189 -4.4287872 -4.4288745 -4.4289036 -4.4289179][-4.4288216 -4.4288092 -4.4287925 -4.4287777 -4.4287539 -4.4286785 -4.4285 -4.428278 -4.4282479 -4.42846 -4.4286704 -4.4288073 -4.4288688 -4.4288893 -4.4289][-4.4288397 -4.4288321 -4.4288077 -4.4287863 -4.4287624 -4.4286981 -4.428565 -4.4284196 -4.428441 -4.4286089 -4.42876 -4.4288568 -4.428896 -4.4289041 -4.4289069][-4.4288697 -4.4288659 -4.4288416 -4.4288173 -4.4287982 -4.4287457 -4.4286518 -4.4285622 -4.4286056 -4.4287338 -4.4288445 -4.4289093 -4.4289284 -4.4289265 -4.4289231][-4.4289012 -4.4289012 -4.4288859 -4.4288673 -4.4288492 -4.4287982 -4.42873 -4.428679 -4.4287353 -4.4288392 -4.4289184 -4.4289513 -4.4289532 -4.4289465 -4.4289403][-4.4289064 -4.4289165 -4.428916 -4.4289021 -4.4288831 -4.4288392 -4.4287939 -4.4287715 -4.4288268 -4.4289112 -4.4289627 -4.42897 -4.4289613 -4.4289522 -4.4289432][-4.4289145 -4.4289274 -4.4289274 -4.4289155 -4.4289012 -4.4288731 -4.4288507 -4.4288449 -4.4288859 -4.4289412 -4.4289632 -4.428956 -4.4289479 -4.4289403 -4.4289279][-4.4289236 -4.4289331 -4.4289279 -4.4289203 -4.4289136 -4.4288969 -4.4288826 -4.4288783 -4.4289026 -4.4289317 -4.428937 -4.4289284 -4.4289255 -4.4289212 -4.4289093][-4.428915 -4.4289255 -4.4289246 -4.4289227 -4.42892 -4.4289069 -4.4288936 -4.4288883 -4.4289 -4.4289126 -4.4289145 -4.4289117 -4.4289126 -4.4289107 -4.4289026]]...]
INFO - root - 2017-12-08 08:22:34.286440: step 55510, loss = 2.28, batch loss = 2.23 (35.2 examples/sec; 0.227 sec/batch; 17h:28m:24s remains)
INFO - root - 2017-12-08 08:22:36.517790: step 55520, loss = 2.28, batch loss = 2.23 (35.1 examples/sec; 0.228 sec/batch; 17h:31m:34s remains)
INFO - root - 2017-12-08 08:22:38.755285: step 55530, loss = 2.28, batch loss = 2.23 (35.7 examples/sec; 0.224 sec/batch; 17h:13m:24s remains)
INFO - root - 2017-12-08 08:22:40.985342: step 55540, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.225 sec/batch; 17h:20m:50s remains)
INFO - root - 2017-12-08 08:22:43.216865: step 55550, loss = 2.28, batch loss = 2.23 (36.6 examples/sec; 0.218 sec/batch; 16h:47m:35s remains)
INFO - root - 2017-12-08 08:22:45.503463: step 55560, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.221 sec/batch; 17h:01m:42s remains)
INFO - root - 2017-12-08 08:22:47.754428: step 55570, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.225 sec/batch; 17h:20m:45s remains)
INFO - root - 2017-12-08 08:22:50.021060: step 55580, loss = 2.28, batch loss = 2.23 (35.1 examples/sec; 0.228 sec/batch; 17h:31m:08s remains)
INFO - root - 2017-12-08 08:22:52.263367: step 55590, loss = 2.28, batch loss = 2.23 (36.6 examples/sec; 0.218 sec/batch; 16h:47m:26s remains)
INFO - root - 2017-12-08 08:22:54.520149: step 55600, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.225 sec/batch; 17h:19m:37s remains)
2017-12-08 08:22:54.818858: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4287972 -4.4288321 -4.4288597 -4.4288678 -4.4288564 -4.4288292 -4.4288006 -4.4288015 -4.4288025 -4.4287968 -4.4287972 -4.4287887 -4.428771 -4.4287686 -4.4287872][-4.4288187 -4.4288516 -4.428875 -4.4288774 -4.428854 -4.4288268 -4.4287972 -4.4287968 -4.4288015 -4.4288068 -4.4288254 -4.4288387 -4.4288316 -4.4288235 -4.4288225][-4.4288063 -4.4288354 -4.4288635 -4.428864 -4.4288311 -4.4288049 -4.4287825 -4.4287853 -4.4287996 -4.4288216 -4.4288487 -4.4288678 -4.4288592 -4.42884 -4.42882][-4.4287672 -4.4287887 -4.4288354 -4.4288545 -4.4288297 -4.4288092 -4.4287891 -4.4287944 -4.4288144 -4.4288335 -4.4288464 -4.428853 -4.4288335 -4.4288 -4.4287667][-4.4287357 -4.4287462 -4.4288015 -4.4288363 -4.4288249 -4.4288011 -4.4287782 -4.4287786 -4.4287953 -4.4288006 -4.4287953 -4.4287891 -4.4287615 -4.428721 -4.4286828][-4.4287105 -4.4287171 -4.4287696 -4.428812 -4.428802 -4.428762 -4.4287176 -4.4287004 -4.4287148 -4.428719 -4.4287124 -4.4287057 -4.4286771 -4.4286284 -4.4285879][-4.4286637 -4.4286757 -4.4287338 -4.4287829 -4.4287748 -4.4287157 -4.4286308 -4.4285741 -4.4285803 -4.4286013 -4.4286156 -4.4286275 -4.4286122 -4.4285727 -4.4285421][-4.4285717 -4.4286008 -4.4286809 -4.4287405 -4.42874 -4.4286642 -4.4285369 -4.4284482 -4.4284463 -4.4284821 -4.4285192 -4.4285483 -4.4285617 -4.4285512 -4.4285398][-4.4285007 -4.4285531 -4.4286513 -4.4287195 -4.4287205 -4.4286375 -4.4284949 -4.4283881 -4.428371 -4.4284067 -4.4284539 -4.4284959 -4.4285369 -4.4285603 -4.4285645][-4.4285088 -4.4285703 -4.4286656 -4.4287271 -4.4287229 -4.4286413 -4.4285035 -4.4283972 -4.4283662 -4.4283986 -4.42846 -4.4285131 -4.4285693 -4.4286132 -4.4286304][-4.4285855 -4.4286432 -4.4287167 -4.4287519 -4.4287353 -4.4286575 -4.4285331 -4.4284449 -4.4284186 -4.4284558 -4.4285245 -4.42858 -4.4286351 -4.4286814 -4.4287024][-4.4286451 -4.4286976 -4.4287496 -4.4287572 -4.4287348 -4.4286733 -4.4285727 -4.4285059 -4.4284968 -4.4285417 -4.4286046 -4.4286556 -4.4287057 -4.42875 -4.4287739][-4.4286594 -4.4287 -4.4287348 -4.4287181 -4.42869 -4.4286532 -4.4285884 -4.4285526 -4.4285636 -4.4286141 -4.4286718 -4.4287219 -4.428772 -4.4288144 -4.4288373][-4.4286346 -4.4286532 -4.4286742 -4.4286509 -4.4286222 -4.4286041 -4.4285736 -4.4285688 -4.4285975 -4.4286537 -4.4287081 -4.4287605 -4.4288125 -4.4288588 -4.4288864][-4.428628 -4.4286275 -4.4286346 -4.4286132 -4.4285893 -4.4285879 -4.4285922 -4.428618 -4.42866 -4.4287148 -4.4287648 -4.4288168 -4.428864 -4.4289036 -4.4289308]]...]
INFO - root - 2017-12-08 08:22:57.034383: step 55610, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.220 sec/batch; 16h:56m:08s remains)
INFO - root - 2017-12-08 08:22:59.301443: step 55620, loss = 2.28, batch loss = 2.23 (34.9 examples/sec; 0.229 sec/batch; 17h:37m:25s remains)
INFO - root - 2017-12-08 08:23:01.535131: step 55630, loss = 2.28, batch loss = 2.23 (36.0 examples/sec; 0.222 sec/batch; 17h:05m:10s remains)
INFO - root - 2017-12-08 08:23:03.816489: step 55640, loss = 2.28, batch loss = 2.23 (33.6 examples/sec; 0.238 sec/batch; 18h:20m:04s remains)
INFO - root - 2017-12-08 08:23:06.034393: step 55650, loss = 2.28, batch loss = 2.23 (38.0 examples/sec; 0.211 sec/batch; 16h:12m:02s remains)
INFO - root - 2017-12-08 08:23:08.329655: step 55660, loss = 2.28, batch loss = 2.23 (32.7 examples/sec; 0.244 sec/batch; 18h:47m:54s remains)
INFO - root - 2017-12-08 08:23:10.617050: step 55670, loss = 2.28, batch loss = 2.23 (34.3 examples/sec; 0.233 sec/batch; 17h:55m:13s remains)
INFO - root - 2017-12-08 08:23:12.865004: step 55680, loss = 2.28, batch loss = 2.23 (35.8 examples/sec; 0.224 sec/batch; 17h:11m:18s remains)
INFO - root - 2017-12-08 08:23:15.117983: step 55690, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 17h:00m:45s remains)
INFO - root - 2017-12-08 08:23:17.385366: step 55700, loss = 2.28, batch loss = 2.23 (34.6 examples/sec; 0.231 sec/batch; 17h:45m:34s remains)
2017-12-08 08:23:17.706175: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289608 -4.42897 -4.428978 -4.4289837 -4.4289851 -4.4289842 -4.4289861 -4.4289885 -4.4289865 -4.4289784 -4.4289651 -4.4289541 -4.4289513 -4.4289546 -4.4289594][-4.4289536 -4.4289618 -4.4289651 -4.4289632 -4.4289579 -4.4289527 -4.4289513 -4.4289513 -4.4289436 -4.4289284 -4.42891 -4.4288955 -4.4288931 -4.4289031 -4.428916][-4.4289145 -4.4289155 -4.42891 -4.4288988 -4.4288855 -4.4288745 -4.4288673 -4.4288645 -4.4288573 -4.428843 -4.4288254 -4.4288125 -4.4288106 -4.4288263 -4.4288468][-4.4288473 -4.4288363 -4.4288182 -4.4287953 -4.4287696 -4.4287496 -4.4287357 -4.4287329 -4.4287395 -4.42874 -4.428731 -4.428721 -4.4287219 -4.4287429 -4.428771][-4.4287863 -4.4287615 -4.4287281 -4.4286861 -4.4286332 -4.4285917 -4.428566 -4.4285688 -4.4286022 -4.4286356 -4.4286509 -4.4286547 -4.4286656 -4.4286909 -4.4287243][-4.4287438 -4.4287086 -4.4286656 -4.4286027 -4.4285154 -4.4284434 -4.4283981 -4.4284067 -4.4284749 -4.4285536 -4.428607 -4.4286323 -4.4286518 -4.4286766 -4.4287071][-4.4287186 -4.4286809 -4.4286337 -4.4285541 -4.428431 -4.428319 -4.428247 -4.4282618 -4.4283671 -4.428494 -4.4285913 -4.4286427 -4.4286671 -4.4286838 -4.4287033][-4.4286947 -4.4286656 -4.4286261 -4.4285417 -4.4284062 -4.4282775 -4.4281926 -4.42821 -4.4283328 -4.4284773 -4.4285922 -4.4286556 -4.4286771 -4.4286838 -4.4286919][-4.4286809 -4.4286652 -4.4286437 -4.4285803 -4.4284759 -4.4283786 -4.4283218 -4.4283433 -4.4284425 -4.4285483 -4.4286313 -4.4286747 -4.4286838 -4.4286828 -4.4286923][-4.4286823 -4.4286795 -4.4286776 -4.4286432 -4.4285803 -4.4285197 -4.4284887 -4.4285116 -4.4285808 -4.4286427 -4.4286819 -4.4286938 -4.4286923 -4.4286895 -4.4287038][-4.4286947 -4.4287047 -4.4287214 -4.4287114 -4.428678 -4.4286366 -4.4286137 -4.4286351 -4.4286842 -4.4287171 -4.4287248 -4.42872 -4.4287205 -4.4287233 -4.4287367][-4.428741 -4.4287615 -4.4287848 -4.4287839 -4.4287605 -4.4287243 -4.4287038 -4.4287257 -4.4287682 -4.4287906 -4.4287877 -4.4287767 -4.4287748 -4.4287758 -4.4287791][-4.4287972 -4.428813 -4.4288254 -4.4288144 -4.4287906 -4.4287572 -4.42874 -4.4287634 -4.4288049 -4.42883 -4.4288311 -4.4288225 -4.4288187 -4.4288149 -4.4288039][-4.4288383 -4.4288392 -4.4288378 -4.4288187 -4.4287925 -4.4287648 -4.4287539 -4.4287734 -4.4288068 -4.4288306 -4.4288373 -4.4288335 -4.4288325 -4.4288321 -4.4288197][-4.4288316 -4.4288235 -4.42882 -4.4288096 -4.4287953 -4.4287786 -4.4287729 -4.4287858 -4.4288073 -4.4288211 -4.4288244 -4.428823 -4.4288268 -4.4288363 -4.4288368]]...]
INFO - root - 2017-12-08 08:23:19.908209: step 55710, loss = 2.28, batch loss = 2.23 (36.7 examples/sec; 0.218 sec/batch; 16h:46m:47s remains)
INFO - root - 2017-12-08 08:23:22.135397: step 55720, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.220 sec/batch; 16h:57m:08s remains)
INFO - root - 2017-12-08 08:23:24.375098: step 55730, loss = 2.28, batch loss = 2.23 (35.2 examples/sec; 0.228 sec/batch; 17h:29m:47s remains)
INFO - root - 2017-12-08 08:23:26.605486: step 55740, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 16h:50m:52s remains)
INFO - root - 2017-12-08 08:23:28.880697: step 55750, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 16h:59m:49s remains)
INFO - root - 2017-12-08 08:23:31.151087: step 55760, loss = 2.28, batch loss = 2.23 (34.2 examples/sec; 0.234 sec/batch; 17h:57m:48s remains)
INFO - root - 2017-12-08 08:23:33.389817: step 55770, loss = 2.28, batch loss = 2.23 (35.1 examples/sec; 0.228 sec/batch; 17h:31m:22s remains)
INFO - root - 2017-12-08 08:23:35.618342: step 55780, loss = 2.28, batch loss = 2.23 (35.0 examples/sec; 0.228 sec/batch; 17h:32m:55s remains)
INFO - root - 2017-12-08 08:23:37.840174: step 55790, loss = 2.28, batch loss = 2.23 (35.6 examples/sec; 0.225 sec/batch; 17h:16m:13s remains)
INFO - root - 2017-12-08 08:23:40.112884: step 55800, loss = 2.28, batch loss = 2.23 (35.6 examples/sec; 0.225 sec/batch; 17h:16m:08s remains)
2017-12-08 08:23:40.432528: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4286122 -4.4285984 -4.428586 -4.4285755 -4.4285841 -4.4286132 -4.4286652 -4.4287157 -4.4287248 -4.4286942 -4.4286647 -4.4286485 -4.428637 -4.4286203 -4.4286227][-4.428627 -4.4286094 -4.4285975 -4.4285755 -4.4285674 -4.4285951 -4.4286561 -4.4287219 -4.4287434 -4.4287243 -4.4287024 -4.4287014 -4.4287081 -4.4287047 -4.4287057][-4.4286475 -4.4286065 -4.4285703 -4.4285378 -4.4285183 -4.4285479 -4.4286194 -4.4286919 -4.4287295 -4.428719 -4.4287038 -4.4287167 -4.4287434 -4.4287529 -4.4287591][-4.4286642 -4.4285975 -4.4285436 -4.428514 -4.4285035 -4.4285254 -4.4285851 -4.4286447 -4.4286809 -4.42869 -4.4286966 -4.428731 -4.4287639 -4.4287782 -4.4287906][-4.4286838 -4.4286017 -4.4285407 -4.4285126 -4.4284997 -4.4284935 -4.4285369 -4.4285941 -4.4286427 -4.42868 -4.4287171 -4.4287672 -4.4288011 -4.4288154 -4.42883][-4.4286985 -4.4286194 -4.4285531 -4.428514 -4.4284725 -4.4284348 -4.4284596 -4.4285226 -4.4286056 -4.4286804 -4.4287405 -4.4288034 -4.4288425 -4.4288592 -4.4288721][-4.4286513 -4.4285936 -4.4285431 -4.4285016 -4.428443 -4.4283862 -4.4283977 -4.4284654 -4.4285617 -4.428648 -4.4287233 -4.4287987 -4.4288445 -4.4288654 -4.4288774][-4.4285522 -4.4285278 -4.4285064 -4.4284849 -4.4284477 -4.4284139 -4.4284396 -4.4285026 -4.4285731 -4.42864 -4.4287095 -4.4287758 -4.4288216 -4.4288507 -4.4288659][-4.4284782 -4.4284811 -4.4284897 -4.4284964 -4.4284968 -4.4285069 -4.4285588 -4.4286075 -4.4286342 -4.4286637 -4.4287057 -4.4287515 -4.4287963 -4.428833 -4.4288516][-4.4284616 -4.428473 -4.4285045 -4.4285264 -4.428546 -4.4285812 -4.4286447 -4.4286923 -4.4286962 -4.4286966 -4.4287105 -4.4287405 -4.4287796 -4.4288168 -4.42884][-4.4285297 -4.4285316 -4.42856 -4.4285793 -4.4285941 -4.4286256 -4.4286833 -4.4287324 -4.4287353 -4.4287238 -4.4287152 -4.4287348 -4.428771 -4.4288116 -4.4288421][-4.4285989 -4.4285917 -4.4286 -4.4286094 -4.4286342 -4.4286637 -4.4287052 -4.4287486 -4.4287543 -4.4287324 -4.428709 -4.4287176 -4.4287496 -4.4287925 -4.4288306][-4.4286261 -4.4286151 -4.4286227 -4.4286318 -4.4286661 -4.4286909 -4.4287086 -4.4287286 -4.4287267 -4.4287014 -4.4286723 -4.4286795 -4.4287105 -4.4287567 -4.4288025][-4.4286542 -4.4286342 -4.4286342 -4.4286485 -4.4286914 -4.4287081 -4.428699 -4.4286909 -4.4286757 -4.4286489 -4.4286203 -4.4286361 -4.4286771 -4.4287314 -4.4287906][-4.4286733 -4.4286256 -4.4286 -4.4286184 -4.4286695 -4.4286942 -4.4286809 -4.4286561 -4.4286289 -4.4286008 -4.4285774 -4.4286051 -4.4286594 -4.4287238 -4.428791]]...]
INFO - root - 2017-12-08 08:23:42.654268: step 55810, loss = 2.28, batch loss = 2.23 (35.1 examples/sec; 0.228 sec/batch; 17h:31m:14s remains)
INFO - root - 2017-12-08 08:23:44.891375: step 55820, loss = 2.28, batch loss = 2.23 (35.6 examples/sec; 0.225 sec/batch; 17h:16m:00s remains)
INFO - root - 2017-12-08 08:23:47.110989: step 55830, loss = 2.28, batch loss = 2.23 (36.7 examples/sec; 0.218 sec/batch; 16h:44m:31s remains)
INFO - root - 2017-12-08 08:23:49.340469: step 55840, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 16h:58m:23s remains)
INFO - root - 2017-12-08 08:23:51.551929: step 55850, loss = 2.28, batch loss = 2.23 (37.2 examples/sec; 0.215 sec/batch; 16h:32m:46s remains)
INFO - root - 2017-12-08 08:23:53.846597: step 55860, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.222 sec/batch; 17h:01m:35s remains)
INFO - root - 2017-12-08 08:23:56.086067: step 55870, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 16h:53m:16s remains)
INFO - root - 2017-12-08 08:23:58.328167: step 55880, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 17h:07m:42s remains)
INFO - root - 2017-12-08 08:24:00.555632: step 55890, loss = 2.28, batch loss = 2.23 (35.8 examples/sec; 0.224 sec/batch; 17h:11m:29s remains)
INFO - root - 2017-12-08 08:24:02.785838: step 55900, loss = 2.28, batch loss = 2.23 (36.6 examples/sec; 0.219 sec/batch; 16h:47m:17s remains)
2017-12-08 08:24:03.094157: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4281979 -4.4283366 -4.4285407 -4.4287291 -4.4288635 -4.4289336 -4.4289622 -4.4289708 -4.4289565 -4.4289374 -4.4289179 -4.4288945 -4.4288688 -4.4288392 -4.4288054][-4.4284821 -4.4285579 -4.4286876 -4.4288163 -4.4289079 -4.4289541 -4.4289742 -4.4289837 -4.4289765 -4.4289656 -4.4289551 -4.4289389 -4.4289169 -4.4288893 -4.4288588][-4.4287429 -4.4287772 -4.4288492 -4.428916 -4.4289556 -4.428966 -4.4289656 -4.4289651 -4.4289641 -4.428966 -4.4289689 -4.4289641 -4.4289513 -4.4289317 -4.4289079][-4.4289169 -4.4289303 -4.4289589 -4.428977 -4.4289732 -4.4289503 -4.4289246 -4.4289131 -4.4289212 -4.4289422 -4.4289646 -4.4289737 -4.428968 -4.4289536 -4.4289355][-4.4289956 -4.4290004 -4.4290042 -4.4289913 -4.4289546 -4.4288988 -4.4288487 -4.4288335 -4.4288568 -4.4289007 -4.4289455 -4.4289708 -4.4289727 -4.4289579 -4.4289408][-4.4290123 -4.42901 -4.4289989 -4.4289646 -4.4288988 -4.4288087 -4.4287338 -4.4287133 -4.4287539 -4.4288273 -4.4289031 -4.4289532 -4.4289684 -4.428957 -4.4289417][-4.4290018 -4.4289923 -4.4289656 -4.4289045 -4.4288096 -4.428688 -4.4285784 -4.4285321 -4.4285836 -4.4286995 -4.4288163 -4.4289 -4.4289384 -4.4289451 -4.42894][-4.4289927 -4.4289813 -4.4289432 -4.4288588 -4.4287333 -4.4285741 -4.4284077 -4.4282956 -4.4283466 -4.428525 -4.4287009 -4.4288239 -4.4288859 -4.4289112 -4.42892][-4.4289961 -4.4289875 -4.4289517 -4.4288659 -4.4287267 -4.4285407 -4.4283175 -4.4281187 -4.4281354 -4.4283576 -4.4285793 -4.4287357 -4.4288206 -4.4288568 -4.4288716][-4.4290023 -4.429 -4.42898 -4.4289184 -4.4287982 -4.4286242 -4.4284086 -4.4281988 -4.4281626 -4.4283328 -4.4285307 -4.4286828 -4.4287686 -4.4288049 -4.4288163][-4.4290071 -4.4290123 -4.4290094 -4.4289818 -4.4289069 -4.4287772 -4.4286118 -4.4284568 -4.4284139 -4.4285035 -4.4286294 -4.4287329 -4.4287868 -4.4287977 -4.4287872][-4.4290085 -4.4290171 -4.4290242 -4.4290209 -4.4289908 -4.4289174 -4.4288135 -4.4287181 -4.4286876 -4.4287314 -4.428802 -4.4288549 -4.4288692 -4.4288483 -4.4288092][-4.4290071 -4.4290175 -4.4290285 -4.429039 -4.4290385 -4.4290104 -4.4289579 -4.428905 -4.4288859 -4.4289064 -4.42894 -4.4289551 -4.42894 -4.4289017 -4.4288521][-4.4290028 -4.4290123 -4.4290223 -4.4290342 -4.4290433 -4.4290395 -4.4290228 -4.4290013 -4.4289937 -4.4290023 -4.4290137 -4.4290028 -4.428966 -4.4289126 -4.4288621][-4.4289985 -4.4290061 -4.4290137 -4.4290214 -4.429029 -4.4290314 -4.4290295 -4.4290257 -4.4290276 -4.4290328 -4.4290333 -4.4290042 -4.4289484 -4.4288783 -4.4288287]]...]
INFO - root - 2017-12-08 08:24:05.296898: step 55910, loss = 2.28, batch loss = 2.23 (36.0 examples/sec; 0.222 sec/batch; 17h:03m:26s remains)
INFO - root - 2017-12-08 08:24:07.555444: step 55920, loss = 2.28, batch loss = 2.23 (36.6 examples/sec; 0.219 sec/batch; 16h:48m:44s remains)
INFO - root - 2017-12-08 08:24:09.823920: step 55930, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.226 sec/batch; 17h:20m:13s remains)
INFO - root - 2017-12-08 08:24:12.104362: step 55940, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 16h:59m:17s remains)
INFO - root - 2017-12-08 08:24:14.337398: step 55950, loss = 2.28, batch loss = 2.23 (35.6 examples/sec; 0.225 sec/batch; 17h:16m:30s remains)
INFO - root - 2017-12-08 08:24:16.588920: step 55960, loss = 2.28, batch loss = 2.23 (35.6 examples/sec; 0.224 sec/batch; 17h:14m:17s remains)
INFO - root - 2017-12-08 08:24:18.853391: step 55970, loss = 2.28, batch loss = 2.23 (33.2 examples/sec; 0.241 sec/batch; 18h:30m:15s remains)
INFO - root - 2017-12-08 08:24:21.119536: step 55980, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 16h:58m:46s remains)
INFO - root - 2017-12-08 08:24:23.381570: step 55990, loss = 2.28, batch loss = 2.23 (32.2 examples/sec; 0.248 sec/batch; 19h:03m:57s remains)
INFO - root - 2017-12-08 08:24:25.618737: step 56000, loss = 2.28, batch loss = 2.23 (34.4 examples/sec; 0.232 sec/batch; 17h:50m:37s remains)
2017-12-08 08:24:25.924913: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289446 -4.4289293 -4.4289103 -4.4288921 -4.428884 -4.4288826 -4.4288874 -4.4289002 -4.4289021 -4.428885 -4.4288678 -4.428863 -4.4288535 -4.4288354 -4.4288054][-4.4289274 -4.4289122 -4.4288964 -4.4288797 -4.4288683 -4.4288659 -4.4288726 -4.4288931 -4.4289045 -4.4288893 -4.4288635 -4.4288597 -4.4288635 -4.4288611 -4.4288363][-4.42891 -4.4288983 -4.4288898 -4.4288745 -4.4288568 -4.4288468 -4.4288468 -4.4288621 -4.4288712 -4.4288521 -4.4288235 -4.4288306 -4.4288549 -4.4288707 -4.4288526][-4.4289074 -4.4289 -4.4288936 -4.428875 -4.4288473 -4.4288244 -4.4288111 -4.4288144 -4.4288135 -4.428792 -4.4287672 -4.4287891 -4.4288335 -4.42886 -4.4288473][-4.42892 -4.4289126 -4.4289 -4.428874 -4.4288368 -4.428802 -4.4287739 -4.4287629 -4.4287524 -4.4287353 -4.4287276 -4.4287629 -4.4288144 -4.428844 -4.4288354][-4.4289393 -4.4289355 -4.428916 -4.4288816 -4.4288363 -4.4287853 -4.4287395 -4.4287124 -4.4286957 -4.4286957 -4.4287114 -4.4287553 -4.4288044 -4.4288344 -4.4288363][-4.4289589 -4.4289632 -4.4289408 -4.4288912 -4.4288287 -4.428761 -4.4287081 -4.4286795 -4.428678 -4.4287014 -4.4287295 -4.428772 -4.4288116 -4.4288344 -4.4288411][-4.4289742 -4.4289842 -4.428956 -4.4288869 -4.428802 -4.4287214 -4.428668 -4.4286551 -4.4286814 -4.4287305 -4.4287639 -4.4287987 -4.4288268 -4.4288373 -4.4288378][-4.428977 -4.4289856 -4.4289474 -4.42886 -4.4287586 -4.4286656 -4.4286118 -4.4286242 -4.4286847 -4.4287567 -4.4287915 -4.4288144 -4.4288282 -4.4288287 -4.4288216][-4.4289727 -4.4289751 -4.4289274 -4.4288282 -4.4287186 -4.4286208 -4.4285736 -4.4286027 -4.42868 -4.4287567 -4.4287944 -4.4288082 -4.4288087 -4.4288049 -4.4287944][-4.4289703 -4.4289675 -4.4289174 -4.4288187 -4.4287114 -4.4286227 -4.4285855 -4.4286113 -4.4286809 -4.4287534 -4.4287887 -4.4287963 -4.428792 -4.4287915 -4.428781][-4.4289742 -4.4289689 -4.4289255 -4.4288406 -4.4287486 -4.4286737 -4.4286413 -4.4286542 -4.4287052 -4.42877 -4.4288039 -4.4288116 -4.42881 -4.428812 -4.4288015][-4.4289837 -4.4289784 -4.428947 -4.4288869 -4.4288192 -4.4287648 -4.4287329 -4.4287333 -4.428771 -4.4288254 -4.4288578 -4.4288654 -4.4288664 -4.4288692 -4.4288583][-4.4289951 -4.4289932 -4.4289761 -4.4289432 -4.4289036 -4.4288712 -4.4288464 -4.4288406 -4.4288654 -4.4289031 -4.428925 -4.4289322 -4.4289384 -4.4289432 -4.4289336][-4.429008 -4.429009 -4.4290023 -4.4289885 -4.4289713 -4.428956 -4.4289403 -4.4289312 -4.4289455 -4.428966 -4.428977 -4.4289813 -4.4289904 -4.4289989 -4.4289947]]...]
INFO - root - 2017-12-08 08:24:28.138442: step 56010, loss = 2.28, batch loss = 2.23 (35.7 examples/sec; 0.224 sec/batch; 17h:13m:10s remains)
INFO - root - 2017-12-08 08:24:30.387448: step 56020, loss = 2.28, batch loss = 2.23 (35.1 examples/sec; 0.228 sec/batch; 17h:31m:31s remains)
INFO - root - 2017-12-08 08:24:32.641274: step 56030, loss = 2.28, batch loss = 2.23 (34.6 examples/sec; 0.231 sec/batch; 17h:45m:30s remains)
INFO - root - 2017-12-08 08:24:34.898165: step 56040, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 16h:52m:58s remains)
INFO - root - 2017-12-08 08:24:37.124374: step 56050, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.222 sec/batch; 17h:02m:10s remains)
INFO - root - 2017-12-08 08:24:39.390041: step 56060, loss = 2.28, batch loss = 2.23 (36.0 examples/sec; 0.222 sec/batch; 17h:03m:18s remains)
INFO - root - 2017-12-08 08:24:41.621848: step 56070, loss = 2.28, batch loss = 2.23 (36.0 examples/sec; 0.222 sec/batch; 17h:02m:45s remains)
INFO - root - 2017-12-08 08:24:43.885741: step 56080, loss = 2.28, batch loss = 2.23 (35.2 examples/sec; 0.227 sec/batch; 17h:26m:56s remains)
INFO - root - 2017-12-08 08:24:46.104478: step 56090, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 16h:49m:51s remains)
INFO - root - 2017-12-08 08:24:48.357700: step 56100, loss = 2.28, batch loss = 2.23 (36.8 examples/sec; 0.217 sec/batch; 16h:41m:06s remains)
2017-12-08 08:24:48.674016: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4287152 -4.4287066 -4.4287181 -4.42874 -4.4287581 -4.4287472 -4.4287157 -4.4286995 -4.4287081 -4.4287028 -4.4286942 -4.4286838 -4.4286904 -4.4287024 -4.4287062][-4.4287658 -4.4287915 -4.4288158 -4.4288406 -4.42885 -4.4288268 -4.4287839 -4.4287629 -4.4287796 -4.4287863 -4.42879 -4.4287891 -4.4287992 -4.4288139 -4.428822][-4.4288244 -4.4288573 -4.4288769 -4.4288898 -4.4288816 -4.4288468 -4.4287992 -4.4287753 -4.4288025 -4.4288311 -4.4288554 -4.4288726 -4.4288883 -4.4289021 -4.4289041][-4.4288955 -4.4289145 -4.4289174 -4.42891 -4.42888 -4.4288306 -4.4287705 -4.4287405 -4.428772 -4.4288154 -4.4288597 -4.4288955 -4.4289255 -4.4289441 -4.4289412][-4.4289446 -4.42894 -4.4289169 -4.4288845 -4.4288344 -4.4287643 -4.4286785 -4.4286256 -4.4286575 -4.4287162 -4.4287753 -4.4288282 -4.4288812 -4.428915 -4.4289093][-4.4289389 -4.4289141 -4.4288688 -4.428813 -4.4287357 -4.42864 -4.4285221 -4.428432 -4.4284577 -4.4285417 -4.4286242 -4.4286976 -4.4287748 -4.4288263 -4.4288263][-4.4288669 -4.4288268 -4.4287672 -4.4286914 -4.4285831 -4.4284515 -4.4282937 -4.4281526 -4.4281607 -4.4282722 -4.4283881 -4.4284973 -4.4286184 -4.4287043 -4.4287353][-4.4287562 -4.4287176 -4.4286642 -4.4285851 -4.4284616 -4.4283042 -4.4281116 -4.4279208 -4.4279056 -4.4280348 -4.4281797 -4.4283319 -4.4284964 -4.4286165 -4.4286914][-4.4287171 -4.4286971 -4.4286695 -4.42861 -4.4285026 -4.428359 -4.4281964 -4.4280148 -4.4279642 -4.4280658 -4.4282045 -4.4283576 -4.4285159 -4.4286313 -4.428719][-4.4287963 -4.4287953 -4.4287868 -4.428751 -4.4286776 -4.4285722 -4.4284596 -4.4283304 -4.4282641 -4.4283175 -4.4284139 -4.428524 -4.4286346 -4.4287186 -4.4287877][-4.4289079 -4.4289107 -4.4289088 -4.4288859 -4.4288368 -4.4287577 -4.4286785 -4.4285955 -4.4285359 -4.4285545 -4.4286151 -4.4286871 -4.4287567 -4.4288182 -4.4288664][-4.4289622 -4.4289556 -4.42895 -4.4289351 -4.4289064 -4.4288521 -4.4287972 -4.4287424 -4.4286928 -4.4286928 -4.4287362 -4.4287939 -4.4288468 -4.4288936 -4.4289222][-4.4289565 -4.4289403 -4.4289312 -4.4289207 -4.4289017 -4.4288678 -4.4288325 -4.4287968 -4.4287548 -4.4287472 -4.4287868 -4.428843 -4.4288936 -4.4289322 -4.4289465][-4.428925 -4.4289031 -4.4288878 -4.4288754 -4.4288621 -4.428844 -4.4288268 -4.4288077 -4.4287782 -4.4287772 -4.4288158 -4.4288688 -4.428915 -4.4289479 -4.428957][-4.4288969 -4.4288726 -4.4288521 -4.4288383 -4.4288325 -4.4288278 -4.428822 -4.428812 -4.4287953 -4.4287996 -4.4288321 -4.428874 -4.4289136 -4.4289403 -4.4289479]]...]
INFO - root - 2017-12-08 08:24:50.906950: step 56110, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 17h:06m:22s remains)
INFO - root - 2017-12-08 08:24:53.146041: step 56120, loss = 2.28, batch loss = 2.23 (35.2 examples/sec; 0.227 sec/batch; 17h:25m:59s remains)
INFO - root - 2017-12-08 08:24:55.382053: step 56130, loss = 2.28, batch loss = 2.23 (35.8 examples/sec; 0.223 sec/batch; 17h:08m:32s remains)
INFO - root - 2017-12-08 08:24:57.609405: step 56140, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.225 sec/batch; 17h:16m:45s remains)
INFO - root - 2017-12-08 08:24:59.842902: step 56150, loss = 2.28, batch loss = 2.23 (35.7 examples/sec; 0.224 sec/batch; 17h:10m:59s remains)
INFO - root - 2017-12-08 08:25:02.091167: step 56160, loss = 2.28, batch loss = 2.23 (35.3 examples/sec; 0.227 sec/batch; 17h:24m:41s remains)
INFO - root - 2017-12-08 08:25:04.340839: step 56170, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.222 sec/batch; 17h:01m:55s remains)
INFO - root - 2017-12-08 08:25:06.565726: step 56180, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 16h:49m:17s remains)
INFO - root - 2017-12-08 08:25:08.797730: step 56190, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.221 sec/batch; 17h:00m:00s remains)
INFO - root - 2017-12-08 08:25:11.045437: step 56200, loss = 2.28, batch loss = 2.23 (36.0 examples/sec; 0.222 sec/batch; 17h:03m:15s remains)
2017-12-08 08:25:11.358146: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289875 -4.4289927 -4.4289904 -4.4289889 -4.4289885 -4.428987 -4.4289808 -4.4289694 -4.4289565 -4.4289455 -4.4289389 -4.4289346 -4.4289346 -4.4289408 -4.4289489][-4.428978 -4.428978 -4.4289675 -4.4289603 -4.42896 -4.4289622 -4.4289608 -4.4289546 -4.4289455 -4.42894 -4.428936 -4.4289284 -4.42892 -4.4289203 -4.4289303][-4.4289727 -4.4289689 -4.42895 -4.4289322 -4.4289289 -4.4289351 -4.4289451 -4.4289532 -4.4289565 -4.42896 -4.4289608 -4.4289546 -4.4289393 -4.4289289 -4.4289289][-4.4289584 -4.4289565 -4.4289351 -4.4289107 -4.4289017 -4.428905 -4.4289155 -4.4289274 -4.4289379 -4.4289432 -4.4289432 -4.4289417 -4.4289222 -4.4289041 -4.4288988][-4.4289083 -4.4289083 -4.4288869 -4.42886 -4.4288454 -4.42884 -4.4288459 -4.4288568 -4.4288692 -4.4288712 -4.4288659 -4.428865 -4.4288454 -4.4288273 -4.4288249][-4.4288273 -4.4288273 -4.4288049 -4.42877 -4.4287477 -4.4287362 -4.4287419 -4.4287605 -4.4287853 -4.4287968 -4.4287934 -4.4287915 -4.428771 -4.4287543 -4.4287567][-4.4287553 -4.4287572 -4.4287343 -4.4286819 -4.428637 -4.4286113 -4.4286127 -4.4286494 -4.4287004 -4.4287314 -4.4287391 -4.4287415 -4.4287205 -4.4287052 -4.4287162][-4.42873 -4.428731 -4.4287019 -4.4286275 -4.4285502 -4.428494 -4.4284749 -4.4285059 -4.4285655 -4.4286203 -4.4286494 -4.4286623 -4.4286518 -4.4286489 -4.4286776][-4.4287486 -4.4287496 -4.4287186 -4.4286394 -4.4285421 -4.428462 -4.4284186 -4.4284291 -4.4284792 -4.4285378 -4.4285746 -4.4285975 -4.4286137 -4.4286361 -4.4286833][-4.4287734 -4.4287839 -4.4287643 -4.4287052 -4.428627 -4.4285712 -4.4285512 -4.4285655 -4.4286089 -4.4286494 -4.4286647 -4.4286742 -4.4286928 -4.4287257 -4.4287772][-4.4287672 -4.4287987 -4.428803 -4.428772 -4.4287314 -4.4287248 -4.4287419 -4.4287672 -4.4287996 -4.4288125 -4.4288011 -4.428792 -4.4287987 -4.4288235 -4.4288683][-4.4287457 -4.4287953 -4.4288187 -4.4288082 -4.4287896 -4.4288073 -4.4288406 -4.4288626 -4.4288769 -4.4288654 -4.4288487 -4.4288373 -4.4288349 -4.4288578 -4.4289002][-4.4287019 -4.4287686 -4.428803 -4.4288092 -4.4288068 -4.4288392 -4.4288774 -4.4288883 -4.4288878 -4.428865 -4.42885 -4.4288397 -4.4288311 -4.4288526 -4.4288931][-4.42865 -4.428731 -4.4287767 -4.4287987 -4.4288125 -4.4288478 -4.4288821 -4.428885 -4.4288726 -4.4288411 -4.4288254 -4.4288163 -4.4288015 -4.4288173 -4.4288559][-4.4286027 -4.4286852 -4.4287395 -4.4287753 -4.4287987 -4.4288254 -4.4288487 -4.4288445 -4.428823 -4.4287882 -4.4287734 -4.4287658 -4.4287553 -4.4287667 -4.4288015]]...]
INFO - root - 2017-12-08 08:25:13.575976: step 56210, loss = 2.28, batch loss = 2.23 (36.0 examples/sec; 0.222 sec/batch; 17h:03m:00s remains)
INFO - root - 2017-12-08 08:25:15.833695: step 56220, loss = 2.28, batch loss = 2.23 (34.3 examples/sec; 0.233 sec/batch; 17h:54m:02s remains)
INFO - root - 2017-12-08 08:25:18.079275: step 56230, loss = 2.28, batch loss = 2.23 (34.9 examples/sec; 0.229 sec/batch; 17h:34m:52s remains)
INFO - root - 2017-12-08 08:25:20.306020: step 56240, loss = 2.28, batch loss = 2.23 (36.6 examples/sec; 0.219 sec/batch; 16h:46m:29s remains)
INFO - root - 2017-12-08 08:25:22.537522: step 56250, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 16h:50m:05s remains)
INFO - root - 2017-12-08 08:25:24.772473: step 56260, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.222 sec/batch; 17h:00m:41s remains)
INFO - root - 2017-12-08 08:25:27.002281: step 56270, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 17h:07m:15s remains)
INFO - root - 2017-12-08 08:25:29.243535: step 56280, loss = 2.28, batch loss = 2.23 (35.0 examples/sec; 0.228 sec/batch; 17h:31m:05s remains)
INFO - root - 2017-12-08 08:25:31.469218: step 56290, loss = 2.28, batch loss = 2.23 (35.4 examples/sec; 0.226 sec/batch; 17h:19m:10s remains)
INFO - root - 2017-12-08 08:25:33.693192: step 56300, loss = 2.28, batch loss = 2.23 (35.8 examples/sec; 0.223 sec/batch; 17h:08m:16s remains)
2017-12-08 08:25:34.000421: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288268 -4.4288826 -4.4289227 -4.4289355 -4.4289327 -4.4289327 -4.4289031 -4.4288559 -4.4288239 -4.4288254 -4.4288445 -4.4288716 -4.4289031 -4.42891 -4.4289036][-4.4288254 -4.4288898 -4.4289403 -4.4289579 -4.4289556 -4.4289379 -4.4288793 -4.4288073 -4.4287767 -4.4288006 -4.4288416 -4.4288807 -4.4289155 -4.4289184 -4.428905][-4.4288278 -4.4288974 -4.4289451 -4.4289565 -4.4289455 -4.4288979 -4.4288116 -4.4287362 -4.4287386 -4.4288 -4.4288645 -4.428906 -4.4289393 -4.4289446 -4.428926][-4.4288454 -4.4289093 -4.428947 -4.4289474 -4.4289131 -4.428822 -4.4287047 -4.4286423 -4.4286995 -4.4288011 -4.4288797 -4.4289241 -4.4289546 -4.4289613 -4.4289465][-4.4288549 -4.4289093 -4.4289351 -4.4289203 -4.4288487 -4.4287066 -4.4285607 -4.4285364 -4.4286618 -4.4287944 -4.4288754 -4.4289231 -4.4289532 -4.4289584 -4.42895][-4.4288645 -4.4289122 -4.4289227 -4.4288807 -4.4287577 -4.4285622 -4.4284019 -4.4284472 -4.428638 -4.4287848 -4.4288607 -4.4289112 -4.4289408 -4.4289551 -4.4289603][-4.4288907 -4.428925 -4.4289074 -4.4288206 -4.4286442 -4.4284043 -4.4282589 -4.4283915 -4.4286361 -4.4287868 -4.428853 -4.4288955 -4.4289241 -4.4289441 -4.4289594][-4.4289141 -4.4289312 -4.428884 -4.4287624 -4.4285512 -4.4282823 -4.4281597 -4.428371 -4.4286518 -4.4287968 -4.4288545 -4.4288878 -4.4289212 -4.4289412 -4.428947][-4.4289227 -4.4289289 -4.4288735 -4.4287424 -4.4285355 -4.4282861 -4.4282 -4.4284267 -4.4286876 -4.4288106 -4.4288588 -4.428895 -4.428937 -4.4289536 -4.4289455][-4.4289227 -4.4289145 -4.4288568 -4.4287338 -4.4285631 -4.4283772 -4.4283442 -4.4285374 -4.4287429 -4.4288359 -4.42887 -4.4289079 -4.4289541 -4.428966 -4.4289479][-4.4289131 -4.4288874 -4.4288344 -4.4287281 -4.4286027 -4.4284868 -4.4285 -4.4286571 -4.4288058 -4.4288621 -4.4288759 -4.4289155 -4.4289665 -4.4289789 -4.4289546][-4.4289031 -4.4288664 -4.4288168 -4.4287348 -4.4286566 -4.4286065 -4.4286494 -4.4287691 -4.428863 -4.4288878 -4.428895 -4.4289317 -4.4289765 -4.4289856 -4.4289565][-4.4289002 -4.4288712 -4.4288344 -4.42878 -4.4287381 -4.4287286 -4.4287729 -4.42885 -4.428896 -4.42891 -4.4289188 -4.4289455 -4.4289756 -4.4289756 -4.428946][-4.4289036 -4.4288898 -4.4288712 -4.4288421 -4.4288239 -4.4288306 -4.4288592 -4.4289002 -4.4289155 -4.428926 -4.4289346 -4.4289532 -4.4289708 -4.4289613 -4.4289327][-4.428894 -4.4288945 -4.4288869 -4.4288764 -4.4288731 -4.4288812 -4.4288974 -4.4289155 -4.4289174 -4.4289279 -4.4289365 -4.4289536 -4.4289684 -4.428956 -4.4289269]]...]
INFO - root - 2017-12-08 08:25:36.243845: step 56310, loss = 2.28, batch loss = 2.23 (35.8 examples/sec; 0.224 sec/batch; 17h:09m:47s remains)
INFO - root - 2017-12-08 08:25:38.465394: step 56320, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.221 sec/batch; 16h:59m:23s remains)
INFO - root - 2017-12-08 08:25:40.719946: step 56330, loss = 2.28, batch loss = 2.23 (33.7 examples/sec; 0.237 sec/batch; 18h:12m:20s remains)
INFO - root - 2017-12-08 08:25:42.966286: step 56340, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.221 sec/batch; 16h:59m:03s remains)
INFO - root - 2017-12-08 08:25:45.228277: step 56350, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.226 sec/batch; 17h:18m:38s remains)
INFO - root - 2017-12-08 08:25:47.445570: step 56360, loss = 2.28, batch loss = 2.23 (37.5 examples/sec; 0.213 sec/batch; 16h:22m:28s remains)
INFO - root - 2017-12-08 08:25:49.679562: step 56370, loss = 2.28, batch loss = 2.23 (34.8 examples/sec; 0.230 sec/batch; 17h:38m:48s remains)
INFO - root - 2017-12-08 08:25:51.904844: step 56380, loss = 2.28, batch loss = 2.23 (35.6 examples/sec; 0.224 sec/batch; 17h:12m:57s remains)
INFO - root - 2017-12-08 08:25:54.161015: step 56390, loss = 2.28, batch loss = 2.23 (36.9 examples/sec; 0.217 sec/batch; 16h:38m:13s remains)
INFO - root - 2017-12-08 08:25:56.399411: step 56400, loss = 2.28, batch loss = 2.23 (37.0 examples/sec; 0.216 sec/batch; 16h:35m:42s remains)
2017-12-08 08:25:56.702547: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288707 -4.4288635 -4.4288669 -4.4288754 -4.428875 -4.4288678 -4.4288635 -4.4288654 -4.4288788 -4.4288945 -4.4288964 -4.4288855 -4.42888 -4.42888 -4.4288845][-4.4288526 -4.4288383 -4.4288383 -4.4288468 -4.4288449 -4.4288354 -4.4288311 -4.4288368 -4.4288607 -4.4288898 -4.4289007 -4.4288926 -4.4288869 -4.4288821 -4.42888][-4.4288387 -4.4288225 -4.4288206 -4.4288216 -4.428803 -4.428782 -4.4287753 -4.4287839 -4.4288187 -4.4288626 -4.4288878 -4.4288969 -4.4288974 -4.4288888 -4.4288774][-4.4288268 -4.4288154 -4.4288144 -4.4288049 -4.4287653 -4.4287314 -4.4287214 -4.4287276 -4.4287729 -4.42883 -4.4288659 -4.4288926 -4.4289088 -4.4288993 -4.4288783][-4.4288149 -4.4288149 -4.4288192 -4.4287972 -4.4287333 -4.428679 -4.428647 -4.428638 -4.4286966 -4.4287791 -4.4288321 -4.428875 -4.4289012 -4.428895 -4.4288692][-4.4288187 -4.4288321 -4.4288421 -4.4288087 -4.4287233 -4.4286337 -4.4285455 -4.4284973 -4.4285669 -4.4286914 -4.428782 -4.4288468 -4.4288807 -4.4288778 -4.428854][-4.4288287 -4.4288454 -4.4288497 -4.4288049 -4.4287057 -4.428586 -4.4284382 -4.4283276 -4.4283829 -4.4285436 -4.4286838 -4.4287777 -4.4288268 -4.4288392 -4.4288273][-4.4288368 -4.4288554 -4.4288545 -4.4288077 -4.4287109 -4.4285812 -4.4284005 -4.4282355 -4.428256 -4.4284239 -4.4285903 -4.4287066 -4.4287686 -4.4287944 -4.428802][-4.4288287 -4.428853 -4.4288626 -4.4288383 -4.428771 -4.4286652 -4.4285107 -4.4283552 -4.4283485 -4.4284644 -4.42859 -4.4286861 -4.4287438 -4.4287796 -4.4288039][-4.4287949 -4.4288182 -4.4288435 -4.4288526 -4.4288287 -4.4287629 -4.4286604 -4.4285574 -4.4285464 -4.4286051 -4.4286709 -4.4287243 -4.4287643 -4.4287963 -4.428822][-4.4287596 -4.4287763 -4.428813 -4.42884 -4.4288392 -4.4288054 -4.4287491 -4.4287 -4.4286985 -4.4287262 -4.42876 -4.4287844 -4.4288015 -4.4288163 -4.4288282][-4.4287395 -4.4287448 -4.4287767 -4.4288015 -4.4288025 -4.4287887 -4.4287753 -4.4287643 -4.4287677 -4.4287906 -4.4288192 -4.4288297 -4.4288321 -4.428834 -4.4288235][-4.42873 -4.4287162 -4.4287314 -4.428741 -4.42873 -4.428721 -4.4287267 -4.4287286 -4.428731 -4.4287581 -4.4287953 -4.4288111 -4.4288187 -4.4288244 -4.4288058][-4.4287438 -4.4287224 -4.42872 -4.4287086 -4.4286747 -4.4286504 -4.428648 -4.4286337 -4.4286251 -4.4286661 -4.4287271 -4.4287605 -4.4287786 -4.4287934 -4.4287806][-4.4287629 -4.4287505 -4.4287372 -4.4287071 -4.428647 -4.4285994 -4.4285665 -4.428514 -4.4284892 -4.4285502 -4.4286413 -4.4287024 -4.4287438 -4.4287758 -4.428782]]...]
INFO - root - 2017-12-08 08:25:58.947775: step 56410, loss = 2.28, batch loss = 2.23 (37.6 examples/sec; 0.213 sec/batch; 16h:19m:14s remains)
INFO - root - 2017-12-08 08:26:01.184465: step 56420, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.221 sec/batch; 16h:58m:27s remains)
INFO - root - 2017-12-08 08:26:03.440890: step 56430, loss = 2.28, batch loss = 2.23 (35.2 examples/sec; 0.227 sec/batch; 17h:25m:02s remains)
INFO - root - 2017-12-08 08:26:05.692411: step 56440, loss = 2.28, batch loss = 2.23 (34.7 examples/sec; 0.230 sec/batch; 17h:39m:16s remains)
INFO - root - 2017-12-08 08:26:07.939329: step 56450, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 17h:04m:27s remains)
INFO - root - 2017-12-08 08:26:10.174861: step 56460, loss = 2.28, batch loss = 2.23 (36.6 examples/sec; 0.219 sec/batch; 16h:45m:15s remains)
INFO - root - 2017-12-08 08:26:12.429534: step 56470, loss = 2.28, batch loss = 2.23 (36.6 examples/sec; 0.218 sec/batch; 16h:44m:58s remains)
INFO - root - 2017-12-08 08:26:14.657127: step 56480, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.221 sec/batch; 16h:58m:46s remains)
INFO - root - 2017-12-08 08:26:16.930740: step 56490, loss = 2.28, batch loss = 2.23 (35.7 examples/sec; 0.224 sec/batch; 17h:09m:49s remains)
INFO - root - 2017-12-08 08:26:19.182886: step 56500, loss = 2.28, batch loss = 2.23 (34.5 examples/sec; 0.232 sec/batch; 17h:45m:56s remains)
2017-12-08 08:26:19.495217: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4286923 -4.4286675 -4.4286475 -4.4286413 -4.4286766 -4.4287362 -4.4287858 -4.4288116 -4.4288011 -4.4287548 -4.4286933 -4.4286551 -4.428648 -4.4286475 -4.4286609][-4.4286985 -4.42868 -4.4286652 -4.4286566 -4.4286704 -4.4287119 -4.4287639 -4.4288139 -4.4288359 -4.4288273 -4.4287968 -4.4287643 -4.4287434 -4.4287171 -4.4286995][-4.4287362 -4.4287224 -4.428721 -4.4287176 -4.428709 -4.4287162 -4.4287548 -4.4288149 -4.42886 -4.4288774 -4.4288626 -4.428833 -4.4288087 -4.428772 -4.4287324][-4.4287825 -4.4287696 -4.42878 -4.42878 -4.42875 -4.4287081 -4.4287128 -4.4287739 -4.4288425 -4.4288874 -4.4288898 -4.4288626 -4.4288387 -4.4288049 -4.4287534][-4.4287896 -4.4287772 -4.4287987 -4.4288073 -4.4287677 -4.4286876 -4.4286518 -4.4286995 -4.4287887 -4.4288535 -4.4288669 -4.4288425 -4.4288249 -4.4288068 -4.4287567][-4.4287362 -4.4287281 -4.4287682 -4.4288006 -4.4287739 -4.4286723 -4.428596 -4.428616 -4.4287057 -4.4287834 -4.4288063 -4.428791 -4.4287848 -4.4287844 -4.4287462][-4.4286313 -4.4286323 -4.4287 -4.4287715 -4.4287734 -4.4286728 -4.428565 -4.42855 -4.4286227 -4.4287009 -4.42874 -4.4287362 -4.4287419 -4.4287591 -4.4287353][-4.428534 -4.4285421 -4.4286289 -4.4287286 -4.4287586 -4.4286766 -4.4285603 -4.4285126 -4.4285522 -4.4286175 -4.4286642 -4.4286742 -4.4286923 -4.4287295 -4.4287291][-4.4285188 -4.4285293 -4.4286065 -4.4287009 -4.4287372 -4.428679 -4.4285831 -4.4285245 -4.4285364 -4.4285779 -4.4286146 -4.428627 -4.4286523 -4.4286985 -4.4287171][-4.4285827 -4.4285917 -4.4286389 -4.4287004 -4.4287214 -4.4286752 -4.4286079 -4.4285669 -4.4285746 -4.428596 -4.4286113 -4.4286118 -4.4286323 -4.428678 -4.4287114][-4.4286914 -4.4286952 -4.4287095 -4.4287343 -4.4287314 -4.4286857 -4.4286385 -4.4286213 -4.4286404 -4.4286542 -4.42865 -4.42863 -4.4286308 -4.4286695 -4.4287224][-4.428802 -4.4288039 -4.4287992 -4.4288015 -4.4287868 -4.4287372 -4.4286957 -4.4286985 -4.4287329 -4.4287462 -4.4287281 -4.4286861 -4.4286604 -4.4286814 -4.4287481][-4.4288697 -4.428874 -4.4288645 -4.4288621 -4.42885 -4.4288058 -4.4287715 -4.428782 -4.4288206 -4.4288325 -4.4288082 -4.4287496 -4.4286957 -4.4286957 -4.42876][-4.4288988 -4.4289036 -4.4288912 -4.4288878 -4.4288797 -4.4288435 -4.4288144 -4.4288239 -4.4288607 -4.4288759 -4.4288564 -4.4287934 -4.4287181 -4.4286976 -4.4287534][-4.4289 -4.428906 -4.4288936 -4.4288926 -4.4288898 -4.4288664 -4.428844 -4.4288516 -4.4288821 -4.4288988 -4.4288836 -4.428822 -4.4287338 -4.4286957 -4.4287424]]...]
INFO - root - 2017-12-08 08:26:21.756574: step 56510, loss = 2.28, batch loss = 2.23 (34.8 examples/sec; 0.230 sec/batch; 17h:38m:19s remains)
INFO - root - 2017-12-08 08:26:24.014261: step 56520, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 16h:47m:39s remains)
INFO - root - 2017-12-08 08:26:26.233095: step 56530, loss = 2.28, batch loss = 2.23 (37.1 examples/sec; 0.215 sec/batch; 16h:30m:55s remains)
INFO - root - 2017-12-08 08:26:28.501210: step 56540, loss = 2.28, batch loss = 2.23 (34.9 examples/sec; 0.229 sec/batch; 17h:32m:55s remains)
INFO - root - 2017-12-08 08:26:30.733115: step 56550, loss = 2.28, batch loss = 2.23 (35.4 examples/sec; 0.226 sec/batch; 17h:19m:36s remains)
INFO - root - 2017-12-08 08:26:32.985813: step 56560, loss = 2.28, batch loss = 2.23 (36.7 examples/sec; 0.218 sec/batch; 16h:43m:40s remains)
INFO - root - 2017-12-08 08:26:35.265993: step 56570, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.221 sec/batch; 16h:54m:27s remains)
INFO - root - 2017-12-08 08:26:37.505460: step 56580, loss = 2.28, batch loss = 2.23 (37.5 examples/sec; 0.213 sec/batch; 16h:21m:08s remains)
INFO - root - 2017-12-08 08:26:39.750098: step 56590, loss = 2.28, batch loss = 2.23 (34.5 examples/sec; 0.232 sec/batch; 17h:46m:17s remains)
INFO - root - 2017-12-08 08:26:42.005948: step 56600, loss = 2.28, batch loss = 2.23 (36.7 examples/sec; 0.218 sec/batch; 16h:41m:05s remains)
2017-12-08 08:26:42.283572: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289112 -4.4289093 -4.42891 -4.4289074 -4.4288983 -4.4288878 -4.4288831 -4.4288821 -4.4288774 -4.4288654 -4.4288535 -4.4288468 -4.4288416 -4.4288454 -4.4288564][-4.4288859 -4.428885 -4.4288917 -4.4288969 -4.42889 -4.4288788 -4.4288826 -4.4288926 -4.428896 -4.4288888 -4.428874 -4.4288635 -4.4288492 -4.4288411 -4.4288435][-4.428863 -4.4288559 -4.4288592 -4.4288669 -4.4288554 -4.4288378 -4.4288445 -4.4288669 -4.4288855 -4.4288883 -4.4288745 -4.4288697 -4.42886 -4.4288492 -4.4288454][-4.4288588 -4.428844 -4.4288392 -4.428843 -4.4288239 -4.4287972 -4.4287972 -4.4288235 -4.4288516 -4.4288621 -4.4288535 -4.4288588 -4.4288616 -4.4288597 -4.4288554][-4.4288726 -4.4288554 -4.4288464 -4.4288435 -4.4288206 -4.428782 -4.4287682 -4.428791 -4.428823 -4.4288368 -4.4288282 -4.4288378 -4.4288521 -4.4288588 -4.4288545][-4.4288688 -4.4288435 -4.4288163 -4.4287925 -4.4287548 -4.4286985 -4.4286647 -4.4286795 -4.4287252 -4.4287519 -4.4287462 -4.428761 -4.4287972 -4.4288268 -4.4288359][-4.428844 -4.4288015 -4.4287462 -4.4286909 -4.4286284 -4.428545 -4.4284754 -4.4284773 -4.4285502 -4.4285994 -4.4286 -4.4286246 -4.4286895 -4.4287543 -4.4287858][-4.4288049 -4.4287353 -4.4286432 -4.4285593 -4.4284649 -4.4283319 -4.4282117 -4.4282 -4.4283104 -4.42839 -4.4284 -4.4284325 -4.4285221 -4.4286265 -4.4286871][-4.4287739 -4.42869 -4.4285765 -4.4284768 -4.4283648 -4.4282055 -4.4280548 -4.4280262 -4.4281492 -4.4282427 -4.4282522 -4.4282775 -4.4283667 -4.4284921 -4.4285793][-4.4287853 -4.4287229 -4.4286308 -4.42855 -4.4284611 -4.428339 -4.4282312 -4.428206 -4.42829 -4.4283648 -4.4283714 -4.42838 -4.4284358 -4.4285312 -4.4286003][-4.4288244 -4.428791 -4.4287372 -4.4286828 -4.4286222 -4.4285388 -4.4284754 -4.4284625 -4.4285116 -4.4285641 -4.4285669 -4.4285617 -4.4285803 -4.4286346 -4.4286785][-4.4288611 -4.4288459 -4.4288177 -4.4287858 -4.4287453 -4.4286933 -4.4286642 -4.4286675 -4.4286981 -4.42873 -4.4287271 -4.4287157 -4.4287181 -4.4287453 -4.4287739][-4.4288831 -4.4288726 -4.4288568 -4.42884 -4.4288177 -4.4287891 -4.4287796 -4.42879 -4.4288049 -4.4288225 -4.4288211 -4.4288149 -4.4288158 -4.4288297 -4.4288473][-4.4288878 -4.4288764 -4.4288635 -4.4288588 -4.4288492 -4.428833 -4.4288311 -4.428844 -4.4288511 -4.4288659 -4.4288688 -4.4288726 -4.4288788 -4.4288869 -4.4288979][-4.4288845 -4.4288673 -4.4288507 -4.4288435 -4.4288349 -4.4288278 -4.4288306 -4.4288425 -4.4288459 -4.4288573 -4.428865 -4.428875 -4.4288869 -4.428896 -4.4289064]]...]
INFO - root - 2017-12-08 08:26:44.505546: step 56610, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 17h:05m:56s remains)
INFO - root - 2017-12-08 08:26:46.742375: step 56620, loss = 2.28, batch loss = 2.23 (35.2 examples/sec; 0.227 sec/batch; 17h:24m:16s remains)
INFO - root - 2017-12-08 08:26:48.997542: step 56630, loss = 2.28, batch loss = 2.23 (34.4 examples/sec; 0.233 sec/batch; 17h:50m:30s remains)
INFO - root - 2017-12-08 08:26:51.220664: step 56640, loss = 2.28, batch loss = 2.23 (37.6 examples/sec; 0.213 sec/batch; 16h:18m:32s remains)
INFO - root - 2017-12-08 08:26:53.507920: step 56650, loss = 2.28, batch loss = 2.23 (35.7 examples/sec; 0.224 sec/batch; 17h:10m:48s remains)
INFO - root - 2017-12-08 08:26:55.740023: step 56660, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.225 sec/batch; 17h:14m:48s remains)
INFO - root - 2017-12-08 08:26:58.011001: step 56670, loss = 2.28, batch loss = 2.23 (35.0 examples/sec; 0.229 sec/batch; 17h:31m:12s remains)
INFO - root - 2017-12-08 08:27:00.246546: step 56680, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.221 sec/batch; 16h:57m:42s remains)
INFO - root - 2017-12-08 08:27:02.468949: step 56690, loss = 2.28, batch loss = 2.23 (36.7 examples/sec; 0.218 sec/batch; 16h:43m:01s remains)
INFO - root - 2017-12-08 08:27:04.697688: step 56700, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 16h:55m:10s remains)
2017-12-08 08:27:04.995902: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4286675 -4.428668 -4.4286785 -4.4287052 -4.4287167 -4.4287024 -4.4286861 -4.4286766 -4.428679 -4.4286976 -4.4287171 -4.4287243 -4.4287333 -4.4287429 -4.4287605][-4.4286227 -4.4286141 -4.4286265 -4.4286551 -4.4286695 -4.428658 -4.4286447 -4.42863 -4.4286318 -4.4286485 -4.4286652 -4.4286733 -4.4286828 -4.4286971 -4.4287276][-4.4286542 -4.428628 -4.4286237 -4.4286385 -4.4286423 -4.4286242 -4.4286032 -4.4285836 -4.4285893 -4.428606 -4.4286089 -4.4286118 -4.4286151 -4.4286256 -4.42866][-4.42871 -4.4286804 -4.4286604 -4.4286571 -4.428648 -4.4286242 -4.4286013 -4.4285879 -4.428607 -4.428627 -4.4286218 -4.4286203 -4.4286079 -4.4285984 -4.4286208][-4.4287047 -4.4286847 -4.4286594 -4.4286404 -4.4286118 -4.4285707 -4.42855 -4.4285593 -4.4286008 -4.4286356 -4.4286375 -4.42864 -4.4286242 -4.4285994 -4.4286046][-4.4286036 -4.4285879 -4.428556 -4.4285183 -4.4284573 -4.4283772 -4.4283476 -4.4283977 -4.4284849 -4.4285517 -4.4285793 -4.4286051 -4.4286056 -4.4285865 -4.4285955][-4.4284458 -4.4284325 -4.4283996 -4.4283581 -4.4282675 -4.428133 -4.4280729 -4.4281564 -4.42829 -4.4283957 -4.4284616 -4.4285126 -4.42853 -4.4285345 -4.42857][-4.4283638 -4.4283638 -4.4283514 -4.4283271 -4.4282451 -4.4281039 -4.4280171 -4.4280753 -4.4281869 -4.4282818 -4.4283614 -4.4284234 -4.4284506 -4.42847 -4.4285297][-4.4284668 -4.4284735 -4.4284806 -4.428483 -4.428443 -4.4283528 -4.4282784 -4.4282818 -4.4283223 -4.4283643 -4.4284205 -4.4284654 -4.4284787 -4.4284949 -4.4285636][-4.4286547 -4.4286551 -4.4286647 -4.4286795 -4.4286695 -4.4286313 -4.4285822 -4.4285583 -4.4285593 -4.4285674 -4.428596 -4.4286203 -4.428618 -4.428617 -4.4286661][-4.4288154 -4.4288106 -4.4288149 -4.4288249 -4.428822 -4.4288039 -4.4287739 -4.4287496 -4.42874 -4.4287362 -4.4287505 -4.428761 -4.4287486 -4.4287314 -4.4287539][-4.4289074 -4.4289007 -4.4289021 -4.4289041 -4.4289012 -4.4288917 -4.4288716 -4.4288497 -4.428834 -4.428822 -4.4288239 -4.4288192 -4.4287939 -4.4287705 -4.4287791][-4.4289365 -4.4289365 -4.42894 -4.4289389 -4.4289341 -4.4289274 -4.4289112 -4.4288845 -4.4288559 -4.4288311 -4.4288211 -4.4288 -4.4287624 -4.428741 -4.4287486][-4.4289403 -4.4289474 -4.4289541 -4.4289541 -4.4289503 -4.4289427 -4.4289207 -4.4288821 -4.428834 -4.4287934 -4.4287734 -4.4287448 -4.4287076 -4.4287004 -4.4287157][-4.4289255 -4.4289379 -4.4289484 -4.4289536 -4.4289556 -4.42895 -4.4289265 -4.428884 -4.4288325 -4.4287877 -4.4287615 -4.4287329 -4.4287043 -4.4287047 -4.4287152]]...]
INFO - root - 2017-12-08 08:27:07.226322: step 56710, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 16h:47m:16s remains)
INFO - root - 2017-12-08 08:27:09.482375: step 56720, loss = 2.28, batch loss = 2.23 (36.0 examples/sec; 0.223 sec/batch; 17h:02m:47s remains)
INFO - root - 2017-12-08 08:27:11.705760: step 56730, loss = 2.28, batch loss = 2.23 (37.3 examples/sec; 0.215 sec/batch; 16h:26m:18s remains)
INFO - root - 2017-12-08 08:27:13.942648: step 56740, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 17h:03m:16s remains)
INFO - root - 2017-12-08 08:27:16.179377: step 56750, loss = 2.28, batch loss = 2.23 (34.7 examples/sec; 0.231 sec/batch; 17h:39m:33s remains)
INFO - root - 2017-12-08 08:27:18.410624: step 56760, loss = 2.28, batch loss = 2.23 (35.7 examples/sec; 0.224 sec/batch; 17h:10m:39s remains)
INFO - root - 2017-12-08 08:27:20.649584: step 56770, loss = 2.28, batch loss = 2.23 (36.9 examples/sec; 0.217 sec/batch; 16h:35m:51s remains)
INFO - root - 2017-12-08 08:27:22.896351: step 56780, loss = 2.28, batch loss = 2.23 (36.0 examples/sec; 0.222 sec/batch; 17h:00m:48s remains)
INFO - root - 2017-12-08 08:27:25.120726: step 56790, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.220 sec/batch; 16h:52m:23s remains)
INFO - root - 2017-12-08 08:27:27.356087: step 56800, loss = 2.28, batch loss = 2.23 (36.9 examples/sec; 0.217 sec/batch; 16h:35m:53s remains)
2017-12-08 08:27:27.664017: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.42874 -4.42876 -4.4287677 -4.428771 -4.4287734 -4.4287758 -4.4287944 -4.4288263 -4.4288549 -4.4288683 -4.428884 -4.4288898 -4.4288735 -4.4288507 -4.428843][-4.4287786 -4.428782 -4.4287753 -4.4287753 -4.4287815 -4.4287906 -4.4288158 -4.4288497 -4.4288793 -4.42889 -4.4289002 -4.4288936 -4.42887 -4.4288445 -4.42883][-4.4288297 -4.4288154 -4.4287844 -4.428771 -4.4287715 -4.4287791 -4.4288044 -4.4288445 -4.4288788 -4.428894 -4.4289021 -4.4288774 -4.4288297 -4.4287896 -4.4287658][-4.428853 -4.4288173 -4.428761 -4.4287267 -4.4287114 -4.4287105 -4.4287324 -4.4287786 -4.4288263 -4.42886 -4.4288816 -4.4288507 -4.4287767 -4.42871 -4.428669][-4.428863 -4.4288034 -4.4287171 -4.4286551 -4.428617 -4.4286036 -4.4286222 -4.4286704 -4.42873 -4.4287925 -4.4288363 -4.4288158 -4.4287286 -4.4286337 -4.4285607][-4.4288621 -4.428791 -4.4286795 -4.4285812 -4.4285092 -4.4284739 -4.4284744 -4.4285107 -4.428586 -4.4286904 -4.4287786 -4.4287887 -4.4287076 -4.4285917 -4.4284797][-4.4288621 -4.4287872 -4.4286494 -4.4285135 -4.4284024 -4.42832 -4.4282703 -4.4282703 -4.4283652 -4.4285331 -4.4286847 -4.4287605 -4.4287357 -4.42864 -4.428524][-4.4288812 -4.428823 -4.428689 -4.4285421 -4.4283948 -4.4282408 -4.4281049 -4.4280262 -4.4281168 -4.4283452 -4.4285626 -4.4287019 -4.4287438 -4.4287076 -4.4286423][-4.4289036 -4.4288769 -4.4287806 -4.4286666 -4.4285374 -4.4283676 -4.4281912 -4.4280477 -4.4280677 -4.4282618 -4.4284787 -4.4286466 -4.428731 -4.4287505 -4.4287415][-4.4289093 -4.4289069 -4.4288545 -4.4287891 -4.4287019 -4.42857 -4.4284344 -4.428319 -4.4283066 -4.4284072 -4.4285412 -4.4286656 -4.4287486 -4.4287925 -4.4288168][-4.4289012 -4.4289093 -4.4288931 -4.4288726 -4.4288335 -4.4287529 -4.4286723 -4.4286056 -4.4285927 -4.4286284 -4.4286895 -4.4287615 -4.4288192 -4.4288559 -4.4288859][-4.4288888 -4.4288974 -4.4289012 -4.4289131 -4.4289126 -4.4288788 -4.4288473 -4.4288168 -4.4288116 -4.4288211 -4.4288378 -4.4288659 -4.4288969 -4.4289169 -4.4289384][-4.4288836 -4.4288898 -4.4288983 -4.4289217 -4.4289422 -4.428936 -4.428936 -4.4289308 -4.4289327 -4.4289365 -4.4289374 -4.4289393 -4.4289436 -4.42895 -4.4289622][-4.4288778 -4.4288812 -4.4288898 -4.428915 -4.4289432 -4.4289484 -4.4289618 -4.4289722 -4.4289823 -4.4289851 -4.4289794 -4.4289708 -4.4289608 -4.4289517 -4.4289522][-4.4288707 -4.42887 -4.4288716 -4.428895 -4.4289231 -4.4289327 -4.4289479 -4.4289641 -4.4289784 -4.4289837 -4.4289756 -4.4289603 -4.4289422 -4.4289284 -4.4289231]]...]
INFO - root - 2017-12-08 08:27:29.910326: step 56810, loss = 2.28, batch loss = 2.23 (37.4 examples/sec; 0.214 sec/batch; 16h:23m:08s remains)
INFO - root - 2017-12-08 08:27:32.158010: step 56820, loss = 2.28, batch loss = 2.23 (36.0 examples/sec; 0.222 sec/batch; 17h:00m:50s remains)
INFO - root - 2017-12-08 08:27:34.425585: step 56830, loss = 2.28, batch loss = 2.23 (34.7 examples/sec; 0.231 sec/batch; 17h:39m:31s remains)
INFO - root - 2017-12-08 08:27:36.649625: step 56840, loss = 2.28, batch loss = 2.23 (34.8 examples/sec; 0.230 sec/batch; 17h:36m:26s remains)
INFO - root - 2017-12-08 08:27:38.912517: step 56850, loss = 2.28, batch loss = 2.23 (36.7 examples/sec; 0.218 sec/batch; 16h:41m:03s remains)
INFO - root - 2017-12-08 08:27:41.172468: step 56860, loss = 2.28, batch loss = 2.23 (35.1 examples/sec; 0.228 sec/batch; 17h:28m:21s remains)
INFO - root - 2017-12-08 08:27:43.428656: step 56870, loss = 2.28, batch loss = 2.23 (34.2 examples/sec; 0.234 sec/batch; 17h:54m:53s remains)
INFO - root - 2017-12-08 08:27:45.644419: step 56880, loss = 2.28, batch loss = 2.23 (34.5 examples/sec; 0.232 sec/batch; 17h:45m:06s remains)
INFO - root - 2017-12-08 08:27:47.907706: step 56890, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.220 sec/batch; 16h:51m:26s remains)
INFO - root - 2017-12-08 08:27:50.157358: step 56900, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 16h:45m:26s remains)
2017-12-08 08:27:50.459432: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289775 -4.4289393 -4.4288993 -4.4288964 -4.4289222 -4.4289436 -4.4289479 -4.428925 -4.4288955 -4.4288888 -4.4289055 -4.4289331 -4.4289665 -4.428988 -4.4289975][-4.4289746 -4.4289412 -4.4289079 -4.4289064 -4.4289274 -4.4289317 -4.4289122 -4.4288721 -4.4288354 -4.428833 -4.4288607 -4.4289031 -4.4289494 -4.4289775 -4.4289927][-4.4289746 -4.4289455 -4.4289169 -4.4289141 -4.4289227 -4.4289026 -4.4288564 -4.4287953 -4.4287481 -4.428751 -4.4287939 -4.4288511 -4.4289117 -4.4289527 -4.4289761][-4.4289708 -4.42895 -4.4289274 -4.4289174 -4.4289045 -4.4288549 -4.4287796 -4.4286971 -4.4286442 -4.42866 -4.4287224 -4.4287949 -4.4288673 -4.4289207 -4.4289536][-4.4289379 -4.4289346 -4.4289193 -4.428906 -4.4288774 -4.428803 -4.4287019 -4.428597 -4.42854 -4.4285712 -4.4286561 -4.4287448 -4.4288311 -4.4288955 -4.4289365][-4.4288645 -4.4288788 -4.4288745 -4.428864 -4.4288282 -4.4287419 -4.4286237 -4.4284973 -4.4284325 -4.4284816 -4.4285879 -4.4286952 -4.4287992 -4.4288769 -4.4289231][-4.4287643 -4.4287958 -4.4288049 -4.4287992 -4.4287581 -4.42866 -4.4285235 -4.4283843 -4.428319 -4.4283948 -4.4285274 -4.4286609 -4.4287825 -4.4288692 -4.428915][-4.4286737 -4.4287214 -4.4287472 -4.4287443 -4.4286895 -4.4285731 -4.4284091 -4.42825 -4.4282002 -4.4283233 -4.4284973 -4.42866 -4.4287944 -4.4288821 -4.4289227][-4.4286222 -4.4286904 -4.428731 -4.4287281 -4.4286613 -4.4285216 -4.4283304 -4.4281673 -4.4281478 -4.4283109 -4.4285131 -4.4286933 -4.428833 -4.4289155 -4.4289455][-4.4286485 -4.4287171 -4.4287543 -4.4287462 -4.4286819 -4.4285507 -4.4283757 -4.4282408 -4.4282389 -4.4283876 -4.42857 -4.4287386 -4.4288697 -4.4289427 -4.4289632][-4.4287281 -4.4287744 -4.4287939 -4.4287782 -4.4287276 -4.4286289 -4.428503 -4.4284039 -4.4283915 -4.4284925 -4.428627 -4.4287672 -4.4288816 -4.428947 -4.428968][-4.4287829 -4.4287996 -4.4287953 -4.4287729 -4.4287391 -4.4286742 -4.4285884 -4.428503 -4.4284687 -4.428525 -4.4286218 -4.4287386 -4.4288483 -4.4289207 -4.4289541][-4.4287772 -4.4287696 -4.4287529 -4.4287319 -4.4287152 -4.4286814 -4.4286251 -4.4285469 -4.4284992 -4.4285307 -4.42861 -4.4287124 -4.4288149 -4.4288893 -4.4289336][-4.4287462 -4.4287252 -4.4287052 -4.4286885 -4.4286857 -4.4286776 -4.4286427 -4.4285722 -4.4285183 -4.4285426 -4.4286165 -4.4287109 -4.4288063 -4.42888 -4.4289293][-4.4287558 -4.4287333 -4.4287138 -4.4287 -4.4287009 -4.4287052 -4.4286847 -4.4286256 -4.4285793 -4.428606 -4.4286771 -4.428762 -4.4288473 -4.4289122 -4.4289522]]...]
INFO - root - 2017-12-08 08:27:52.678863: step 56910, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 16h:48m:28s remains)
INFO - root - 2017-12-08 08:27:54.916813: step 56920, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 16h:54m:31s remains)
INFO - root - 2017-12-08 08:27:57.139742: step 56930, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 16h:46m:22s remains)
INFO - root - 2017-12-08 08:27:59.375889: step 56940, loss = 2.28, batch loss = 2.23 (35.3 examples/sec; 0.227 sec/batch; 17h:22m:18s remains)
INFO - root - 2017-12-08 08:28:01.637693: step 56950, loss = 2.28, batch loss = 2.23 (33.5 examples/sec; 0.239 sec/batch; 18h:16m:50s remains)
INFO - root - 2017-12-08 08:28:03.895909: step 56960, loss = 2.28, batch loss = 2.23 (35.3 examples/sec; 0.226 sec/batch; 17h:19m:59s remains)
INFO - root - 2017-12-08 08:28:06.123341: step 56970, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.221 sec/batch; 16h:56m:22s remains)
INFO - root - 2017-12-08 08:28:08.389139: step 56980, loss = 2.28, batch loss = 2.23 (36.6 examples/sec; 0.219 sec/batch; 16h:44m:27s remains)
INFO - root - 2017-12-08 08:28:10.628001: step 56990, loss = 2.28, batch loss = 2.23 (35.8 examples/sec; 0.224 sec/batch; 17h:06m:47s remains)
INFO - root - 2017-12-08 08:28:12.907882: step 57000, loss = 2.28, batch loss = 2.23 (34.2 examples/sec; 0.234 sec/batch; 17h:52m:53s remains)
2017-12-08 08:28:13.266244: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289112 -4.4289064 -4.4288769 -4.4288578 -4.4288659 -4.4288783 -4.4288898 -4.4289064 -4.4289241 -4.4289379 -4.4289446 -4.42894 -4.4289379 -4.4289622 -4.4290085][-4.4288831 -4.4289136 -4.42891 -4.4288912 -4.4288659 -4.4288282 -4.4288082 -4.428822 -4.4288564 -4.4288945 -4.4289241 -4.4289422 -4.428957 -4.4289865 -4.4290323][-4.4288611 -4.4289145 -4.4289203 -4.428894 -4.42883 -4.4287395 -4.4286914 -4.4287157 -4.4287806 -4.4288492 -4.4289012 -4.4289341 -4.4289579 -4.4289889 -4.4290352][-4.4288492 -4.4289136 -4.4289179 -4.428875 -4.4287624 -4.4286175 -4.4285483 -4.428597 -4.4287014 -4.4288044 -4.4288783 -4.4289236 -4.4289532 -4.4289813 -4.4290266][-4.4288554 -4.428925 -4.4289246 -4.4288554 -4.428689 -4.4284892 -4.4284043 -4.4284878 -4.4286361 -4.4287686 -4.4288592 -4.4289117 -4.4289432 -4.428968 -4.4290109][-4.4288683 -4.4289417 -4.428937 -4.4288449 -4.4286327 -4.4283776 -4.4282742 -4.4283972 -4.4285903 -4.4287462 -4.4288459 -4.4288983 -4.42893 -4.4289551 -4.4289966][-4.4288607 -4.4289374 -4.4289365 -4.4288297 -4.4285874 -4.4282861 -4.4281516 -4.4283028 -4.4285374 -4.4287171 -4.4288282 -4.42888 -4.4289079 -4.428936 -4.428978][-4.4288445 -4.428916 -4.4289145 -4.4288 -4.428545 -4.4282155 -4.4280462 -4.4282188 -4.42849 -4.4286919 -4.4288096 -4.4288635 -4.4288869 -4.4289169 -4.4289594][-4.4288244 -4.4288917 -4.4288869 -4.4287772 -4.4285336 -4.428206 -4.4280267 -4.4282122 -4.4284964 -4.4286938 -4.4288025 -4.4288549 -4.4288754 -4.4289069 -4.42895][-4.4288135 -4.4288688 -4.428864 -4.4287705 -4.4285665 -4.4282889 -4.4281473 -4.4283166 -4.4285665 -4.4287238 -4.4288068 -4.4288545 -4.4288764 -4.4289074 -4.4289494][-4.42881 -4.4288449 -4.4288454 -4.428781 -4.4286318 -4.4284272 -4.428339 -4.4284787 -4.42867 -4.4287763 -4.4288292 -4.428864 -4.4288812 -4.4289069 -4.4289484][-4.4287982 -4.4288116 -4.4288182 -4.4287944 -4.4287119 -4.4285812 -4.4285355 -4.4286327 -4.4287643 -4.4288273 -4.42885 -4.4288616 -4.4288688 -4.4288931 -4.4289379][-4.4287834 -4.4287686 -4.4287724 -4.4287863 -4.4287686 -4.4287081 -4.4286904 -4.4287505 -4.4288297 -4.4288588 -4.4288578 -4.4288497 -4.428854 -4.428884 -4.4289355][-4.4287763 -4.4287257 -4.4287176 -4.4287558 -4.4287853 -4.4287782 -4.4287806 -4.4288177 -4.4288597 -4.4288654 -4.4288516 -4.4288354 -4.428844 -4.428885 -4.4289436][-4.4287925 -4.4287133 -4.4286847 -4.4287291 -4.4287767 -4.4287963 -4.4288054 -4.4288282 -4.4288483 -4.4288421 -4.428823 -4.4288111 -4.4288306 -4.4288855 -4.4289503]]...]
INFO - root - 2017-12-08 08:28:15.511885: step 57010, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.222 sec/batch; 16h:58m:02s remains)
INFO - root - 2017-12-08 08:28:17.742798: step 57020, loss = 2.28, batch loss = 2.23 (35.6 examples/sec; 0.225 sec/batch; 17h:12m:56s remains)
INFO - root - 2017-12-08 08:28:19.994128: step 57030, loss = 2.28, batch loss = 2.23 (35.1 examples/sec; 0.228 sec/batch; 17h:25m:12s remains)
INFO - root - 2017-12-08 08:28:22.261740: step 57040, loss = 2.28, batch loss = 2.23 (34.2 examples/sec; 0.234 sec/batch; 17h:54m:17s remains)
INFO - root - 2017-12-08 08:28:24.504964: step 57050, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 16h:47m:24s remains)
INFO - root - 2017-12-08 08:28:26.747210: step 57060, loss = 2.28, batch loss = 2.23 (37.5 examples/sec; 0.213 sec/batch; 16h:18m:56s remains)
INFO - root - 2017-12-08 08:28:28.980310: step 57070, loss = 2.28, batch loss = 2.23 (35.3 examples/sec; 0.227 sec/batch; 17h:21m:48s remains)
INFO - root - 2017-12-08 08:28:31.169137: step 57080, loss = 2.28, batch loss = 2.23 (36.6 examples/sec; 0.218 sec/batch; 16h:42m:07s remains)
INFO - root - 2017-12-08 08:28:33.417137: step 57090, loss = 2.28, batch loss = 2.23 (35.2 examples/sec; 0.227 sec/batch; 17h:21m:52s remains)
INFO - root - 2017-12-08 08:28:35.661719: step 57100, loss = 2.28, batch loss = 2.23 (35.1 examples/sec; 0.228 sec/batch; 17h:26m:09s remains)
2017-12-08 08:28:35.958606: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4285784 -4.4285827 -4.4286079 -4.4286089 -4.4286065 -4.4285908 -4.4285512 -4.4284782 -4.4284673 -4.4285383 -4.428606 -4.4286489 -4.4286942 -4.4287343 -4.4287796][-4.4285769 -4.428587 -4.428618 -4.4286194 -4.4286022 -4.4285727 -4.4285235 -4.4284339 -4.4284067 -4.4284892 -4.4285803 -4.4286385 -4.4286957 -4.4287372 -4.4287763][-4.4285846 -4.4286027 -4.428648 -4.4286489 -4.4286127 -4.4285688 -4.4284992 -4.4283829 -4.4283442 -4.4284492 -4.4285722 -4.4286427 -4.4287033 -4.4287434 -4.4287791][-4.4286203 -4.428638 -4.4286876 -4.428689 -4.4286318 -4.4285622 -4.4284692 -4.4283438 -4.4283004 -4.4284234 -4.4285617 -4.4286346 -4.4286923 -4.4287348 -4.4287744][-4.4286695 -4.428689 -4.4287324 -4.4287262 -4.4286504 -4.4285626 -4.4284582 -4.42832 -4.4282804 -4.4284153 -4.4285583 -4.428637 -4.4286838 -4.4287243 -4.4287672][-4.4287057 -4.4287219 -4.4287443 -4.4287133 -4.4286132 -4.4284978 -4.4283624 -4.4281955 -4.428196 -4.4283843 -4.4285464 -4.4286351 -4.4286871 -4.4287262 -4.4287667][-4.42869 -4.4287 -4.428709 -4.4286556 -4.4285274 -4.4283743 -4.4281683 -4.4279122 -4.427958 -4.428257 -4.4284873 -4.4286141 -4.4286814 -4.4287257 -4.4287705][-4.42864 -4.4286542 -4.4286685 -4.4286194 -4.4285073 -4.4283681 -4.4281349 -4.4278359 -4.427896 -4.4282365 -4.4284844 -4.4286194 -4.4286838 -4.4287286 -4.4287724][-4.4286265 -4.4286475 -4.4286585 -4.4286156 -4.4285612 -4.4284997 -4.4283533 -4.4281712 -4.4282022 -4.4284172 -4.4285903 -4.4286809 -4.4287248 -4.4287519 -4.428781][-4.4285784 -4.4286156 -4.4286366 -4.428607 -4.4286079 -4.428606 -4.4285245 -4.4284215 -4.4284363 -4.428566 -4.4286752 -4.4287271 -4.4287567 -4.4287767 -4.4287925][-4.428504 -4.4285583 -4.4286022 -4.4285979 -4.4286246 -4.4286394 -4.4285712 -4.4284978 -4.4285164 -4.428617 -4.4286966 -4.4287276 -4.4287515 -4.4287782 -4.4287968][-4.4284635 -4.4285326 -4.428597 -4.428606 -4.4286313 -4.4286432 -4.4285874 -4.4285307 -4.4285464 -4.4286308 -4.4286971 -4.4287238 -4.4287453 -4.42877 -4.4287953][-4.4285164 -4.4285827 -4.4286427 -4.428647 -4.4286561 -4.4286613 -4.428616 -4.4285688 -4.4285736 -4.4286323 -4.4286909 -4.4287205 -4.4287381 -4.4287605 -4.4287896][-4.4285769 -4.42863 -4.4286757 -4.428679 -4.4286733 -4.4286709 -4.4286404 -4.4285941 -4.4285784 -4.4286146 -4.4286666 -4.4287028 -4.4287305 -4.4287591 -4.4287896][-4.4285841 -4.4286146 -4.4286423 -4.4286504 -4.4286375 -4.42863 -4.4286122 -4.4285669 -4.4285469 -4.4285755 -4.428627 -4.4286757 -4.4287224 -4.42876 -4.4287944]]...]
INFO - root - 2017-12-08 08:28:38.183666: step 57110, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 16h:46m:11s remains)
INFO - root - 2017-12-08 08:28:40.451861: step 57120, loss = 2.28, batch loss = 2.23 (35.4 examples/sec; 0.226 sec/batch; 17h:18m:30s remains)
INFO - root - 2017-12-08 08:28:42.689736: step 57130, loss = 2.28, batch loss = 2.23 (34.0 examples/sec; 0.235 sec/batch; 17h:59m:57s remains)
INFO - root - 2017-12-08 08:28:44.934654: step 57140, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 16h:45m:37s remains)
INFO - root - 2017-12-08 08:28:47.168285: step 57150, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.220 sec/batch; 16h:50m:33s remains)
INFO - root - 2017-12-08 08:28:49.408924: step 57160, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 16h:48m:17s remains)
INFO - root - 2017-12-08 08:28:51.599575: step 57170, loss = 2.28, batch loss = 2.23 (37.3 examples/sec; 0.215 sec/batch; 16h:25m:03s remains)
INFO - root - 2017-12-08 08:28:53.846724: step 57180, loss = 2.28, batch loss = 2.23 (36.7 examples/sec; 0.218 sec/batch; 16h:40m:12s remains)
INFO - root - 2017-12-08 08:28:56.071663: step 57190, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.220 sec/batch; 16h:50m:53s remains)
INFO - root - 2017-12-08 08:28:58.300785: step 57200, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 16h:49m:21s remains)
2017-12-08 08:28:58.592142: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288964 -4.4288917 -4.4288983 -4.4288988 -4.4289036 -4.4288979 -4.428885 -4.4288764 -4.4288754 -4.4288812 -4.4288778 -4.4288783 -4.428885 -4.428896 -4.4289093][-4.4288354 -4.4288306 -4.4288416 -4.4288526 -4.4288621 -4.4288559 -4.4288325 -4.4288139 -4.4288096 -4.4288173 -4.4288149 -4.4288154 -4.4288278 -4.4288468 -4.4288688][-4.4287758 -4.4287667 -4.4287763 -4.4287972 -4.4288173 -4.4288111 -4.4287763 -4.4287457 -4.4287357 -4.4287367 -4.4287367 -4.42874 -4.4287577 -4.4287844 -4.4288135][-4.428709 -4.4286914 -4.4286957 -4.4287229 -4.4287524 -4.4287505 -4.4287133 -4.4286761 -4.42866 -4.4286551 -4.4286537 -4.4286528 -4.4286666 -4.4286952 -4.4287276][-4.428658 -4.4286385 -4.4286385 -4.4286642 -4.4287019 -4.4287052 -4.428669 -4.4286275 -4.4286065 -4.4285994 -4.4285908 -4.4285784 -4.4285851 -4.4286127 -4.4286447][-4.4286356 -4.4286208 -4.4286227 -4.428647 -4.4286819 -4.4286828 -4.4286442 -4.4285994 -4.4285741 -4.4285707 -4.4285655 -4.4285488 -4.4285464 -4.4285717 -4.428607][-4.42863 -4.4286218 -4.4286265 -4.4286447 -4.4286656 -4.4286518 -4.4286022 -4.4285522 -4.4285269 -4.4285345 -4.4285431 -4.428534 -4.42853 -4.4285607 -4.4286041][-4.428628 -4.4286261 -4.4286332 -4.4286475 -4.4286532 -4.4286246 -4.4285679 -4.4285131 -4.4284859 -4.428504 -4.4285254 -4.4285207 -4.4285192 -4.428555 -4.4286036][-4.4286251 -4.4286203 -4.4286275 -4.4286375 -4.4286375 -4.4286084 -4.4285603 -4.4285169 -4.4285016 -4.4285259 -4.4285464 -4.4285369 -4.4285312 -4.4285588 -4.4285989][-4.4286442 -4.4286275 -4.4286227 -4.4286256 -4.4286304 -4.4286194 -4.4285984 -4.4285789 -4.42858 -4.4286051 -4.4286175 -4.4286013 -4.4285865 -4.4285936 -4.4286141][-4.4286966 -4.4286695 -4.4286547 -4.4286494 -4.4286561 -4.42866 -4.4286594 -4.4286594 -4.4286766 -4.4287043 -4.4287124 -4.4286962 -4.4286771 -4.4286685 -4.428669][-4.4287667 -4.4287434 -4.4287295 -4.4287238 -4.42873 -4.4287381 -4.4287448 -4.4287515 -4.4287691 -4.4287915 -4.4288 -4.4287906 -4.4287734 -4.4287577 -4.4287453][-4.4288435 -4.42883 -4.4288235 -4.4288225 -4.4288282 -4.4288359 -4.4288435 -4.4288497 -4.42886 -4.4288712 -4.4288764 -4.4288721 -4.4288635 -4.4288497 -4.4288344][-4.4289074 -4.428905 -4.428905 -4.4289064 -4.4289107 -4.428916 -4.4289217 -4.4289269 -4.4289327 -4.428936 -4.4289365 -4.4289341 -4.4289312 -4.4289227 -4.4289126][-4.428926 -4.4289336 -4.428937 -4.4289384 -4.4289403 -4.4289427 -4.428947 -4.4289513 -4.4289527 -4.4289513 -4.4289484 -4.4289465 -4.428947 -4.4289446 -4.4289412]]...]
INFO - root - 2017-12-08 08:29:00.814294: step 57210, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 16h:52m:39s remains)
INFO - root - 2017-12-08 08:29:03.086098: step 57220, loss = 2.28, batch loss = 2.23 (35.0 examples/sec; 0.229 sec/batch; 17h:29m:01s remains)
INFO - root - 2017-12-08 08:29:05.306739: step 57230, loss = 2.28, batch loss = 2.23 (35.2 examples/sec; 0.227 sec/batch; 17h:21m:23s remains)
INFO - root - 2017-12-08 08:29:07.538244: step 57240, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.220 sec/batch; 16h:50m:32s remains)
INFO - root - 2017-12-08 08:29:09.774956: step 57250, loss = 2.28, batch loss = 2.23 (34.7 examples/sec; 0.231 sec/batch; 17h:37m:32s remains)
INFO - root - 2017-12-08 08:29:12.004059: step 57260, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 16h:45m:43s remains)
INFO - root - 2017-12-08 08:29:14.237125: step 57270, loss = 2.28, batch loss = 2.23 (36.6 examples/sec; 0.219 sec/batch; 16h:43m:43s remains)
INFO - root - 2017-12-08 08:29:16.482123: step 57280, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 16h:44m:19s remains)
INFO - root - 2017-12-08 08:29:18.696796: step 57290, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.225 sec/batch; 17h:13m:17s remains)
INFO - root - 2017-12-08 08:29:20.946099: step 57300, loss = 2.28, batch loss = 2.23 (34.9 examples/sec; 0.229 sec/batch; 17h:31m:24s remains)
2017-12-08 08:29:21.271624: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4286332 -4.428637 -4.4286389 -4.4286451 -4.4286757 -4.4286957 -4.4286947 -4.428659 -4.4286323 -4.4286375 -4.428679 -4.4287348 -4.428802 -4.4288468 -4.4288683][-4.4286838 -4.4286942 -4.4286923 -4.4286919 -4.4287105 -4.4287057 -4.4286876 -4.4286356 -4.4285884 -4.4285827 -4.4286265 -4.4286985 -4.4287863 -4.4288383 -4.4288645][-4.4287019 -4.4287057 -4.4287028 -4.428709 -4.4287233 -4.4287114 -4.428678 -4.428607 -4.4285345 -4.4285197 -4.4285693 -4.4286637 -4.4287686 -4.4288292 -4.42886][-4.428731 -4.4287157 -4.42871 -4.428721 -4.4287262 -4.4287004 -4.4286432 -4.428545 -4.42847 -4.4284649 -4.4285307 -4.4286423 -4.4287572 -4.4288216 -4.4288545][-4.4287639 -4.4287391 -4.4287333 -4.4287367 -4.4287233 -4.4286771 -4.4285831 -4.4284549 -4.4283876 -4.4284158 -4.4285097 -4.4286337 -4.4287443 -4.4288111 -4.42885][-4.4288111 -4.4287887 -4.42877 -4.4287496 -4.4287024 -4.4286265 -4.4284911 -4.4283357 -4.4282727 -4.4283433 -4.4284792 -4.428617 -4.4287286 -4.4287963 -4.4288406][-4.4288554 -4.4288468 -4.4288244 -4.4287782 -4.4286871 -4.4285579 -4.4283619 -4.4281554 -4.4280868 -4.4281993 -4.4283938 -4.4285617 -4.4286928 -4.4287744 -4.4288268][-4.42891 -4.42891 -4.4288836 -4.4288182 -4.4287024 -4.4285436 -4.42831 -4.4280629 -4.4279566 -4.4280572 -4.4282842 -4.4284811 -4.4286313 -4.4287329 -4.428803][-4.4289412 -4.4289494 -4.4289289 -4.4288626 -4.4287462 -4.428597 -4.4284015 -4.428194 -4.4280825 -4.4281306 -4.4283142 -4.4284844 -4.4286132 -4.42871 -4.4287877][-4.4289379 -4.4289527 -4.4289513 -4.4289088 -4.4288225 -4.4287148 -4.4285808 -4.428422 -4.4283085 -4.4283066 -4.4284306 -4.4285607 -4.428658 -4.4287362 -4.4288054][-4.4289117 -4.428925 -4.4289408 -4.4289317 -4.4288883 -4.4288249 -4.4287395 -4.4286227 -4.4285159 -4.42849 -4.42857 -4.4286647 -4.4287333 -4.428791 -4.4288468][-4.42889 -4.428896 -4.4289155 -4.4289241 -4.4289093 -4.4288764 -4.4288287 -4.4287491 -4.4286647 -4.4286447 -4.4286976 -4.4287605 -4.4288073 -4.42885 -4.4288917][-4.4288864 -4.4288864 -4.4289026 -4.4289165 -4.4289131 -4.4288964 -4.4288716 -4.4288225 -4.4287634 -4.4287529 -4.4287891 -4.4288316 -4.4288692 -4.4289021 -4.4289293][-4.4288812 -4.4288816 -4.4288921 -4.4289021 -4.4289045 -4.4288945 -4.42888 -4.42885 -4.42881 -4.4288058 -4.428833 -4.428865 -4.4288988 -4.4289308 -4.4289546][-4.428885 -4.4288878 -4.4288917 -4.4288955 -4.4288988 -4.428895 -4.428884 -4.4288616 -4.4288349 -4.4288335 -4.4288526 -4.4288769 -4.4289074 -4.4289408 -4.428966]]...]
INFO - root - 2017-12-08 08:29:23.483183: step 57310, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 16h:52m:44s remains)
INFO - root - 2017-12-08 08:29:25.719957: step 57320, loss = 2.28, batch loss = 2.23 (34.5 examples/sec; 0.232 sec/batch; 17h:44m:40s remains)
INFO - root - 2017-12-08 08:29:27.952183: step 57330, loss = 2.28, batch loss = 2.23 (35.3 examples/sec; 0.226 sec/batch; 17h:18m:09s remains)
INFO - root - 2017-12-08 08:29:30.189124: step 57340, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 16h:54m:27s remains)
INFO - root - 2017-12-08 08:29:32.405595: step 57350, loss = 2.28, batch loss = 2.23 (37.7 examples/sec; 0.212 sec/batch; 16h:13m:42s remains)
INFO - root - 2017-12-08 08:29:34.657526: step 57360, loss = 2.28, batch loss = 2.23 (35.7 examples/sec; 0.224 sec/batch; 17h:08m:47s remains)
INFO - root - 2017-12-08 08:29:36.906751: step 57370, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 16h:53m:01s remains)
INFO - root - 2017-12-08 08:29:39.140626: step 57380, loss = 2.28, batch loss = 2.23 (35.8 examples/sec; 0.223 sec/batch; 17h:04m:05s remains)
INFO - root - 2017-12-08 08:29:41.342957: step 57390, loss = 2.28, batch loss = 2.23 (37.2 examples/sec; 0.215 sec/batch; 16h:25m:15s remains)
INFO - root - 2017-12-08 08:29:43.598957: step 57400, loss = 2.28, batch loss = 2.23 (35.1 examples/sec; 0.228 sec/batch; 17h:25m:39s remains)
2017-12-08 08:29:43.883485: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4287915 -4.4287992 -4.4288287 -4.4288449 -4.428823 -4.4287844 -4.4287581 -4.4287453 -4.4287205 -4.4286757 -4.4286485 -4.4286413 -4.4286346 -4.4286366 -4.4286675][-4.4287443 -4.4287705 -4.4288225 -4.4288616 -4.4288445 -4.428782 -4.4287286 -4.4287162 -4.4287028 -4.428668 -4.4286532 -4.4286561 -4.4286494 -4.4286461 -4.4286718][-4.4286928 -4.4287314 -4.4287982 -4.4288487 -4.42884 -4.428762 -4.4286852 -4.4286776 -4.4286981 -4.4286985 -4.4287071 -4.4287205 -4.4287124 -4.4286995 -4.4287071][-4.4286408 -4.4286771 -4.4287443 -4.4287972 -4.4287934 -4.4287052 -4.4286137 -4.4286113 -4.4286728 -4.428719 -4.4287472 -4.4287634 -4.4287491 -4.4287262 -4.4287119][-4.4286036 -4.4286308 -4.4286876 -4.4287281 -4.42872 -4.4286151 -4.4284949 -4.4284916 -4.4285984 -4.428689 -4.4287372 -4.4287629 -4.4287553 -4.428731 -4.4287033][-4.4285755 -4.4285955 -4.4286361 -4.4286585 -4.4286294 -4.428494 -4.4283223 -4.4282956 -4.4284525 -4.4286017 -4.4286833 -4.4287291 -4.4287353 -4.4287081 -4.4286704][-4.428566 -4.4285583 -4.4285688 -4.4285645 -4.4285078 -4.4283423 -4.4281144 -4.4280515 -4.4282665 -4.4284849 -4.4286103 -4.428678 -4.4287014 -4.4286733 -4.4286289][-4.4285803 -4.4285426 -4.4285269 -4.4285011 -4.4284372 -4.4282775 -4.4280534 -4.4279766 -4.4281931 -4.4284267 -4.4285736 -4.4286613 -4.4286995 -4.4286733 -4.4286184][-4.4286242 -4.4285822 -4.4285641 -4.4285393 -4.4284959 -4.4283924 -4.4282427 -4.4281845 -4.4283271 -4.4284964 -4.4286132 -4.4286923 -4.4287267 -4.4287014 -4.4286389][-4.4286876 -4.428658 -4.4286504 -4.4286375 -4.4286208 -4.4285707 -4.428483 -4.4284358 -4.4285054 -4.4286051 -4.4286819 -4.4287362 -4.4287581 -4.4287319 -4.4286766][-4.4287453 -4.4287314 -4.4287353 -4.4287338 -4.428731 -4.4287071 -4.4286532 -4.4286141 -4.4286432 -4.4287009 -4.4287472 -4.4287786 -4.4287853 -4.4287515 -4.4287057][-4.4287858 -4.4287796 -4.4287872 -4.4287925 -4.4287958 -4.4287806 -4.4287419 -4.42871 -4.4287214 -4.4287577 -4.4287872 -4.4288015 -4.4287972 -4.4287615 -4.4287295][-4.4287987 -4.4287972 -4.4287996 -4.4288049 -4.4288125 -4.4288092 -4.4287882 -4.4287677 -4.4287715 -4.4287949 -4.4288144 -4.42882 -4.4288106 -4.4287863 -4.4287724][-4.4288282 -4.4288278 -4.428823 -4.4288187 -4.4288211 -4.4288235 -4.4288173 -4.4288058 -4.4288044 -4.4288158 -4.4288287 -4.4288311 -4.4288211 -4.4288063 -4.428803][-4.4288526 -4.4288497 -4.4288425 -4.4288325 -4.4288287 -4.4288282 -4.4288254 -4.4288177 -4.4288092 -4.4288082 -4.4288116 -4.4288092 -4.4287972 -4.4287887 -4.428792]]...]
INFO - root - 2017-12-08 08:29:46.112058: step 57410, loss = 2.28, batch loss = 2.23 (35.8 examples/sec; 0.224 sec/batch; 17h:04m:58s remains)
INFO - root - 2017-12-08 08:29:48.340543: step 57420, loss = 2.28, batch loss = 2.23 (35.4 examples/sec; 0.226 sec/batch; 17h:16m:50s remains)
INFO - root - 2017-12-08 08:29:50.592119: step 57430, loss = 2.28, batch loss = 2.23 (35.8 examples/sec; 0.223 sec/batch; 17h:03m:48s remains)
INFO - root - 2017-12-08 08:29:52.821428: step 57440, loss = 2.28, batch loss = 2.23 (36.0 examples/sec; 0.223 sec/batch; 17h:00m:02s remains)
INFO - root - 2017-12-08 08:29:55.056617: step 57450, loss = 2.28, batch loss = 2.23 (36.7 examples/sec; 0.218 sec/batch; 16h:40m:24s remains)
INFO - root - 2017-12-08 08:29:57.292926: step 57460, loss = 2.28, batch loss = 2.23 (35.3 examples/sec; 0.227 sec/batch; 17h:18m:21s remains)
INFO - root - 2017-12-08 08:29:59.538122: step 57470, loss = 2.28, batch loss = 2.23 (35.1 examples/sec; 0.228 sec/batch; 17h:24m:57s remains)
INFO - root - 2017-12-08 08:30:01.761338: step 57480, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 16h:53m:18s remains)
INFO - root - 2017-12-08 08:30:04.029674: step 57490, loss = 2.28, batch loss = 2.23 (35.8 examples/sec; 0.224 sec/batch; 17h:04m:27s remains)
INFO - root - 2017-12-08 08:30:06.259701: step 57500, loss = 2.28, batch loss = 2.23 (36.0 examples/sec; 0.222 sec/batch; 16h:57m:43s remains)
2017-12-08 08:30:06.561907: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.429029 -4.4290538 -4.4290419 -4.4289913 -4.4289074 -4.4288135 -4.4287205 -4.4286532 -4.4286385 -4.4286766 -4.4287477 -4.4288249 -4.4288926 -4.4289374 -4.4289556][-4.429059 -4.4290862 -4.4290671 -4.4290051 -4.4289045 -4.4287858 -4.4286761 -4.428607 -4.4285989 -4.4286604 -4.4287529 -4.4288373 -4.4289002 -4.4289441 -4.4289684][-4.4290614 -4.4290915 -4.4290695 -4.4290075 -4.4288988 -4.4287562 -4.4286418 -4.4285855 -4.4285893 -4.4286528 -4.4287448 -4.4288282 -4.4288907 -4.4289374 -4.4289722][-4.4290743 -4.4290967 -4.4290633 -4.4289923 -4.4288807 -4.4287281 -4.4286213 -4.4285917 -4.4286122 -4.4286637 -4.4287372 -4.4288154 -4.4288783 -4.4289303 -4.4289742][-4.4290881 -4.4290977 -4.4290423 -4.4289536 -4.4288344 -4.4286776 -4.4285622 -4.4285398 -4.4285741 -4.4286261 -4.4286957 -4.4287767 -4.4288497 -4.428915 -4.4289727][-4.4290819 -4.4290738 -4.4289918 -4.4288793 -4.4287477 -4.4285808 -4.4284434 -4.4284086 -4.4284525 -4.4285188 -4.4286013 -4.4287052 -4.4288082 -4.4288874 -4.4289494][-4.4290419 -4.42901 -4.4289017 -4.4287677 -4.428616 -4.4284468 -4.4282994 -4.428247 -4.4282975 -4.4283872 -4.4284978 -4.4286375 -4.4287648 -4.4288487 -4.4289069][-4.4289832 -4.4289303 -4.4288082 -4.4286671 -4.428515 -4.4283686 -4.4282365 -4.4281764 -4.4282203 -4.4283185 -4.4284506 -4.4286022 -4.4287276 -4.4287992 -4.4288406][-4.4289451 -4.4288921 -4.4287839 -4.4286604 -4.42853 -4.42842 -4.4283285 -4.4282794 -4.4283018 -4.4283895 -4.4285264 -4.4286518 -4.4287291 -4.4287558 -4.4287629][-4.4289551 -4.428926 -4.4288597 -4.4287782 -4.4286919 -4.4286242 -4.4285755 -4.4285336 -4.4285221 -4.4285765 -4.4286847 -4.4287467 -4.4287529 -4.4287205 -4.4286842][-4.428978 -4.4289837 -4.428967 -4.4289274 -4.4288726 -4.4288282 -4.428793 -4.4287572 -4.4287333 -4.4287605 -4.428822 -4.4288244 -4.4287772 -4.428688 -4.4286108][-4.4289646 -4.4290023 -4.4290161 -4.4289942 -4.4289536 -4.4289174 -4.4288769 -4.4288526 -4.4288487 -4.4288788 -4.4289136 -4.4288845 -4.4288082 -4.4286866 -4.4285851][-4.4289041 -4.4289594 -4.4289861 -4.4289637 -4.4289317 -4.4289069 -4.4288764 -4.4288783 -4.4289031 -4.4289479 -4.4289742 -4.4289455 -4.4288778 -4.428762 -4.4286513][-4.4288268 -4.4288754 -4.4288974 -4.4288664 -4.4288282 -4.4287996 -4.4287705 -4.4287906 -4.4288521 -4.4289269 -4.4289832 -4.4289937 -4.4289565 -4.4288692 -4.4287658][-4.4287529 -4.4287848 -4.4287977 -4.4287596 -4.4287086 -4.4286647 -4.4286294 -4.428659 -4.4287467 -4.4288578 -4.4289479 -4.4290028 -4.4289989 -4.4289451 -4.4288578]]...]
INFO - root - 2017-12-08 08:30:08.812430: step 57510, loss = 2.28, batch loss = 2.23 (34.9 examples/sec; 0.229 sec/batch; 17h:30m:31s remains)
INFO - root - 2017-12-08 08:30:11.086218: step 57520, loss = 2.28, batch loss = 2.23 (35.6 examples/sec; 0.225 sec/batch; 17h:10m:02s remains)
INFO - root - 2017-12-08 08:30:13.306064: step 57530, loss = 2.28, batch loss = 2.23 (35.3 examples/sec; 0.227 sec/batch; 17h:18m:19s remains)
INFO - root - 2017-12-08 08:30:15.542449: step 57540, loss = 2.28, batch loss = 2.23 (36.0 examples/sec; 0.222 sec/batch; 16h:57m:16s remains)
INFO - root - 2017-12-08 08:30:17.770338: step 57550, loss = 2.28, batch loss = 2.23 (35.6 examples/sec; 0.225 sec/batch; 17h:09m:20s remains)
INFO - root - 2017-12-08 08:30:20.015734: step 57560, loss = 2.28, batch loss = 2.23 (33.9 examples/sec; 0.236 sec/batch; 18h:01m:11s remains)
INFO - root - 2017-12-08 08:30:22.264785: step 57570, loss = 2.28, batch loss = 2.23 (37.2 examples/sec; 0.215 sec/batch; 16h:26m:03s remains)
INFO - root - 2017-12-08 08:30:24.555652: step 57580, loss = 2.28, batch loss = 2.23 (35.3 examples/sec; 0.227 sec/batch; 17h:18m:21s remains)
INFO - root - 2017-12-08 08:30:26.768023: step 57590, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 16h:47m:13s remains)
INFO - root - 2017-12-08 08:30:28.986951: step 57600, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 16h:59m:54s remains)
2017-12-08 08:30:29.280781: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4287877 -4.4287906 -4.4287658 -4.4287124 -4.428669 -4.4286585 -4.428638 -4.4286084 -4.4286265 -4.428688 -4.4287591 -4.4288068 -4.4288135 -4.4287825 -4.4287286][-4.4288392 -4.428843 -4.4288087 -4.4287434 -4.4286885 -4.4286566 -4.4286246 -4.4285951 -4.4286237 -4.4286976 -4.4287696 -4.4288116 -4.4288154 -4.4287772 -4.4287109][-4.4288826 -4.428875 -4.4288292 -4.4287558 -4.4286866 -4.4286323 -4.4285841 -4.4285541 -4.4285917 -4.4286795 -4.4287548 -4.4287996 -4.4288068 -4.4287634 -4.4286847][-4.4288926 -4.4288788 -4.4288273 -4.4287448 -4.4286613 -4.4285889 -4.4285288 -4.4285 -4.4285502 -4.4286509 -4.4287267 -4.4287734 -4.4287839 -4.4287405 -4.4286566][-4.4288907 -4.4288731 -4.4288149 -4.4287267 -4.4286385 -4.4285531 -4.4284835 -4.4284563 -4.4285216 -4.4286394 -4.42872 -4.4287639 -4.4287753 -4.428731 -4.428648][-4.4288745 -4.4288516 -4.428792 -4.4287095 -4.428628 -4.428535 -4.4284534 -4.428422 -4.4285049 -4.4286408 -4.4287281 -4.4287724 -4.4287839 -4.4287395 -4.4286656][-4.4288507 -4.428822 -4.4287629 -4.4286885 -4.428617 -4.4285178 -4.4284267 -4.4283957 -4.4284992 -4.4286523 -4.4287443 -4.4287925 -4.4288063 -4.4287658 -4.4287014][-4.4288335 -4.4288006 -4.4287434 -4.428679 -4.4286194 -4.4285183 -4.4284139 -4.4283762 -4.4284925 -4.4286537 -4.4287472 -4.428803 -4.4288259 -4.4287968 -4.4287434][-4.4288125 -4.4287796 -4.4287248 -4.4286709 -4.4286251 -4.4285288 -4.4284096 -4.4283609 -4.428484 -4.4286447 -4.4287367 -4.4288 -4.4288354 -4.4288168 -4.4287672][-4.4287982 -4.4287624 -4.4287066 -4.4286594 -4.4286256 -4.4285398 -4.4284191 -4.4283767 -4.4284983 -4.4286418 -4.4287224 -4.4287872 -4.4288306 -4.4288154 -4.4287648][-4.428792 -4.4287562 -4.4287047 -4.4286642 -4.428637 -4.4285617 -4.4284573 -4.4284358 -4.4285426 -4.4286509 -4.4287086 -4.4287677 -4.4288135 -4.428802 -4.4287515][-4.428792 -4.4287677 -4.4287295 -4.4286919 -4.4286604 -4.4285941 -4.4285135 -4.42851 -4.4285932 -4.4286628 -4.4286942 -4.4287462 -4.428792 -4.4287872 -4.4287443][-4.4287949 -4.4287848 -4.428761 -4.4287271 -4.4286895 -4.4286313 -4.4285736 -4.428576 -4.4286227 -4.4286537 -4.4286623 -4.4287143 -4.428771 -4.4287834 -4.4287524][-4.4287767 -4.428781 -4.4287715 -4.4287448 -4.4287052 -4.428659 -4.4286246 -4.4286227 -4.4286337 -4.4286318 -4.4286261 -4.4286804 -4.4287486 -4.4287753 -4.4287539][-4.4287448 -4.4287553 -4.4287553 -4.4287348 -4.4287033 -4.4286823 -4.4286747 -4.4286671 -4.428647 -4.428617 -4.4286 -4.4286509 -4.4287229 -4.4287581 -4.4287472]]...]
INFO - root - 2017-12-08 08:30:31.513629: step 57610, loss = 2.28, batch loss = 2.23 (35.7 examples/sec; 0.224 sec/batch; 17h:05m:37s remains)
INFO - root - 2017-12-08 08:30:33.775501: step 57620, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 16h:51m:55s remains)
INFO - root - 2017-12-08 08:30:36.006789: step 57630, loss = 2.28, batch loss = 2.23 (34.8 examples/sec; 0.230 sec/batch; 17h:32m:54s remains)
INFO - root - 2017-12-08 08:30:38.253765: step 57640, loss = 2.28, batch loss = 2.23 (35.6 examples/sec; 0.224 sec/batch; 17h:08m:21s remains)
INFO - root - 2017-12-08 08:30:40.498755: step 57650, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.221 sec/batch; 16h:50m:09s remains)
INFO - root - 2017-12-08 08:30:42.755015: step 57660, loss = 2.28, batch loss = 2.23 (34.1 examples/sec; 0.235 sec/batch; 17h:55m:04s remains)
INFO - root - 2017-12-08 08:30:44.999221: step 57670, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 17h:01m:16s remains)
INFO - root - 2017-12-08 08:30:47.241780: step 57680, loss = 2.28, batch loss = 2.23 (35.4 examples/sec; 0.226 sec/batch; 17h:14m:17s remains)
INFO - root - 2017-12-08 08:30:49.466610: step 57690, loss = 2.28, batch loss = 2.23 (34.9 examples/sec; 0.229 sec/batch; 17h:30m:59s remains)
INFO - root - 2017-12-08 08:30:51.704215: step 57700, loss = 2.28, batch loss = 2.23 (35.4 examples/sec; 0.226 sec/batch; 17h:16m:00s remains)
2017-12-08 08:30:52.018180: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288869 -4.4288945 -4.4288964 -4.4288955 -4.4288983 -4.4288907 -4.428863 -4.4288135 -4.4287524 -4.42871 -4.4287033 -4.4287281 -4.4287686 -4.4287925 -4.428781][-4.4289365 -4.4289389 -4.4289327 -4.428925 -4.428915 -4.4288855 -4.428834 -4.4287572 -4.4286714 -4.428618 -4.4286213 -4.4286604 -4.4287186 -4.4287548 -4.4287386][-4.4289436 -4.428937 -4.4289241 -4.4289107 -4.428884 -4.4288249 -4.428741 -4.4286332 -4.428525 -4.4284716 -4.4284935 -4.4285622 -4.4286513 -4.4286966 -4.4286637][-4.4289336 -4.4289145 -4.4288983 -4.4288826 -4.4288416 -4.4287624 -4.4286556 -4.4285297 -4.4284215 -4.4283905 -4.4284444 -4.4285407 -4.4286389 -4.4286695 -4.4286113][-4.4289136 -4.4288783 -4.4288621 -4.4288535 -4.4288168 -4.4287343 -4.428618 -4.4284921 -4.4284143 -4.4284253 -4.4285011 -4.4285955 -4.4286723 -4.4286714 -4.4285927][-4.4288712 -4.4288273 -4.4288177 -4.4288254 -4.4288 -4.4287238 -4.4286094 -4.4285021 -4.4284639 -4.4285035 -4.428575 -4.4286485 -4.4286957 -4.4286666 -4.4285769][-4.4288344 -4.4287853 -4.4287806 -4.4288044 -4.4287925 -4.4287181 -4.4286075 -4.4285154 -4.4284983 -4.4285493 -4.4286265 -4.4286852 -4.4287071 -4.4286561 -4.4285588][-4.428803 -4.42876 -4.4287663 -4.428803 -4.4287968 -4.4287128 -4.4285955 -4.4285 -4.4284887 -4.4285541 -4.4286451 -4.4287004 -4.4287028 -4.4286346 -4.4285407][-4.428761 -4.4287419 -4.4287724 -4.4288249 -4.428823 -4.4287262 -4.4285908 -4.4284825 -4.4284768 -4.4285603 -4.4286585 -4.4287124 -4.4287009 -4.4286308 -4.4285531][-4.428731 -4.4287443 -4.4288 -4.428863 -4.4288645 -4.4287586 -4.4286094 -4.4285035 -4.4285135 -4.4286036 -4.4286909 -4.4287395 -4.4287176 -4.4286594 -4.4285955][-4.4287281 -4.42876 -4.4288273 -4.4288926 -4.4288979 -4.4288015 -4.4286652 -4.4285812 -4.4285979 -4.4286718 -4.4287434 -4.428782 -4.4287529 -4.4287019 -4.4286437][-4.4287663 -4.4287949 -4.4288516 -4.4289093 -4.4289207 -4.4288545 -4.4287548 -4.4286914 -4.4286962 -4.4287472 -4.4288054 -4.4288344 -4.4288049 -4.428762 -4.4287095][-4.4288325 -4.4288473 -4.4288778 -4.4289231 -4.4289412 -4.4289069 -4.428844 -4.4287934 -4.4287848 -4.4288182 -4.4288664 -4.428895 -4.4288759 -4.4288435 -4.4287992][-4.4289026 -4.4289055 -4.4289131 -4.4289432 -4.428966 -4.4289551 -4.4289165 -4.428874 -4.4288597 -4.4288812 -4.4289203 -4.42895 -4.4289455 -4.4289246 -4.4288878][-4.4289422 -4.4289446 -4.428946 -4.428966 -4.4289885 -4.42899 -4.4289651 -4.4289284 -4.4289093 -4.42892 -4.4289484 -4.4289746 -4.4289818 -4.4289732 -4.4289517]]...]
INFO - root - 2017-12-08 08:30:54.264986: step 57710, loss = 2.28, batch loss = 2.23 (37.4 examples/sec; 0.214 sec/batch; 16h:18m:23s remains)
INFO - root - 2017-12-08 08:30:56.501721: step 57720, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.222 sec/batch; 16h:56m:01s remains)
INFO - root - 2017-12-08 08:30:58.739749: step 57730, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 17h:01m:54s remains)
INFO - root - 2017-12-08 08:31:00.997781: step 57740, loss = 2.28, batch loss = 2.23 (35.2 examples/sec; 0.227 sec/batch; 17h:20m:46s remains)
INFO - root - 2017-12-08 08:31:03.253970: step 57750, loss = 2.28, batch loss = 2.23 (35.3 examples/sec; 0.226 sec/batch; 17h:17m:09s remains)
INFO - root - 2017-12-08 08:31:05.508002: step 57760, loss = 2.28, batch loss = 2.23 (37.0 examples/sec; 0.216 sec/batch; 16h:31m:13s remains)
INFO - root - 2017-12-08 08:31:07.756315: step 57770, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.221 sec/batch; 16h:54m:09s remains)
INFO - root - 2017-12-08 08:31:10.041707: step 57780, loss = 2.28, batch loss = 2.23 (33.7 examples/sec; 0.237 sec/batch; 18h:06m:51s remains)
INFO - root - 2017-12-08 08:31:12.272245: step 57790, loss = 2.28, batch loss = 2.23 (35.8 examples/sec; 0.224 sec/batch; 17h:04m:17s remains)
INFO - root - 2017-12-08 08:31:14.495071: step 57800, loss = 2.28, batch loss = 2.23 (36.6 examples/sec; 0.218 sec/batch; 16h:39m:51s remains)
2017-12-08 08:31:14.779192: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4287071 -4.4287214 -4.428762 -4.4287953 -4.4288168 -4.4288273 -4.4288349 -4.428844 -4.4288416 -4.4288254 -4.4287953 -4.4287653 -4.4287448 -4.4287415 -4.4287419][-4.4286942 -4.4287176 -4.4287705 -4.4288049 -4.4288187 -4.428822 -4.428823 -4.4288368 -4.4288559 -4.4288692 -4.4288635 -4.4288416 -4.4288154 -4.4287987 -4.4287896][-4.4287014 -4.4287238 -4.428782 -4.4288168 -4.4288158 -4.4287992 -4.4287848 -4.4287977 -4.4288368 -4.4288812 -4.4289002 -4.4288859 -4.4288545 -4.4288259 -4.4288068][-4.4287167 -4.4287348 -4.4287858 -4.4288125 -4.4287963 -4.4287486 -4.4287062 -4.4287114 -4.42877 -4.4288435 -4.4288816 -4.4288783 -4.4288464 -4.4288096 -4.4287868][-4.4287329 -4.4287434 -4.4287753 -4.4287767 -4.4287395 -4.4286556 -4.4285736 -4.4285727 -4.428659 -4.4287696 -4.4288325 -4.4288421 -4.4288168 -4.4287791 -4.4287567][-4.4287486 -4.4287519 -4.4287629 -4.4287367 -4.4286747 -4.4285483 -4.42841 -4.4283838 -4.4284997 -4.4286623 -4.4287667 -4.4288044 -4.4287896 -4.4287558 -4.4287376][-4.4287581 -4.4287567 -4.42875 -4.4286962 -4.428606 -4.4284339 -4.4282269 -4.428153 -4.4282966 -4.4285274 -4.4286923 -4.4287691 -4.4287729 -4.4287477 -4.4287376][-4.4287391 -4.4287329 -4.4287195 -4.4286547 -4.4285464 -4.4283452 -4.4280877 -4.4279623 -4.4281082 -4.4283915 -4.4286184 -4.4287362 -4.4287653 -4.4287519 -4.4287529][-4.4287243 -4.4287162 -4.4287095 -4.4286594 -4.4285684 -4.4283962 -4.4281745 -4.4280462 -4.4281397 -4.4283886 -4.4286213 -4.4287491 -4.4287853 -4.4287734 -4.428762][-4.4287481 -4.4287443 -4.4287434 -4.42871 -4.4286461 -4.4285283 -4.4283848 -4.4282904 -4.42833 -4.4284997 -4.4286885 -4.4287972 -4.4288244 -4.4288025 -4.4287729][-4.4288034 -4.4288092 -4.4288135 -4.4287934 -4.4287486 -4.4286737 -4.4285927 -4.4285345 -4.4285421 -4.4286442 -4.4287763 -4.4288516 -4.4288669 -4.4288425 -4.4288063][-4.4288445 -4.4288573 -4.4288721 -4.428864 -4.4288359 -4.4287939 -4.4287605 -4.4287333 -4.4287305 -4.4287848 -4.4288645 -4.4289079 -4.4289155 -4.4288955 -4.4288626][-4.4288921 -4.4289045 -4.4289193 -4.4289179 -4.4289036 -4.4288836 -4.4288764 -4.4288716 -4.4288731 -4.4288979 -4.4289341 -4.4289551 -4.4289551 -4.428936 -4.4289055][-4.4289422 -4.4289479 -4.4289551 -4.4289536 -4.4289451 -4.4289341 -4.4289336 -4.4289384 -4.4289455 -4.428956 -4.4289684 -4.4289737 -4.4289675 -4.4289517 -4.4289274][-4.4289575 -4.4289608 -4.4289656 -4.4289646 -4.4289579 -4.4289489 -4.4289474 -4.4289527 -4.4289613 -4.4289675 -4.4289727 -4.4289732 -4.42897 -4.4289608 -4.4289455]]...]
INFO - root - 2017-12-08 08:31:17.008501: step 57810, loss = 2.28, batch loss = 2.23 (34.1 examples/sec; 0.235 sec/batch; 17h:54m:19s remains)
INFO - root - 2017-12-08 08:31:19.274030: step 57820, loss = 2.28, batch loss = 2.23 (34.3 examples/sec; 0.233 sec/batch; 17h:47m:39s remains)
INFO - root - 2017-12-08 08:31:21.534978: step 57830, loss = 2.28, batch loss = 2.23 (36.7 examples/sec; 0.218 sec/batch; 16h:38m:02s remains)
INFO - root - 2017-12-08 08:31:23.811051: step 57840, loss = 2.28, batch loss = 2.23 (34.0 examples/sec; 0.235 sec/batch; 17h:56m:10s remains)
INFO - root - 2017-12-08 08:31:26.053772: step 57850, loss = 2.28, batch loss = 2.23 (34.7 examples/sec; 0.231 sec/batch; 17h:35m:57s remains)
INFO - root - 2017-12-08 08:31:28.304475: step 57860, loss = 2.28, batch loss = 2.23 (37.3 examples/sec; 0.214 sec/batch; 16h:20m:32s remains)
INFO - root - 2017-12-08 08:31:30.506681: step 57870, loss = 2.28, batch loss = 2.23 (36.9 examples/sec; 0.217 sec/batch; 16h:31m:39s remains)
INFO - root - 2017-12-08 08:31:32.735066: step 57880, loss = 2.28, batch loss = 2.23 (36.7 examples/sec; 0.218 sec/batch; 16h:37m:12s remains)
INFO - root - 2017-12-08 08:31:34.958426: step 57890, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.225 sec/batch; 17h:10m:50s remains)
INFO - root - 2017-12-08 08:31:37.202008: step 57900, loss = 2.28, batch loss = 2.23 (35.4 examples/sec; 0.226 sec/batch; 17h:13m:44s remains)
2017-12-08 08:31:37.529533: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4287739 -4.4287806 -4.4288015 -4.4288197 -4.4288321 -4.4288306 -4.4288273 -4.4288087 -4.4287863 -4.4287772 -4.428781 -4.4287834 -4.4287925 -4.4288154 -4.4288259][-4.4287472 -4.4287577 -4.4287829 -4.4287939 -4.4287896 -4.428762 -4.4287333 -4.428689 -4.4286637 -4.4286771 -4.4287081 -4.4287305 -4.4287567 -4.4287915 -4.4287987][-4.4287443 -4.4287529 -4.4287691 -4.4287643 -4.42874 -4.4286737 -4.4285836 -4.4284897 -4.4284773 -4.4285517 -4.4286313 -4.4286785 -4.4287229 -4.4287696 -4.428772][-4.4287686 -4.4287748 -4.4287705 -4.4287457 -4.4286947 -4.42858 -4.4284129 -4.4282708 -4.4282885 -4.4284406 -4.4285803 -4.42866 -4.4287157 -4.4287577 -4.4287486][-4.4288039 -4.4288139 -4.428803 -4.42877 -4.4286981 -4.4285426 -4.4283285 -4.4281793 -4.428225 -4.4284129 -4.4285784 -4.4286647 -4.4287143 -4.4287324 -4.4287028][-4.42884 -4.4288573 -4.4288483 -4.4288163 -4.4287367 -4.4285746 -4.4283628 -4.4282422 -4.4283047 -4.428483 -4.4286342 -4.4287028 -4.4287257 -4.4287128 -4.4286566][-4.4288487 -4.4288721 -4.4288735 -4.428854 -4.4287815 -4.4286385 -4.42846 -4.4283748 -4.4284372 -4.4285817 -4.4287062 -4.4287553 -4.4287539 -4.4287186 -4.4286442][-4.4288135 -4.428834 -4.4288464 -4.4288435 -4.428793 -4.4286962 -4.4285808 -4.4285383 -4.428587 -4.4286766 -4.428761 -4.4287848 -4.4287672 -4.4287219 -4.428648][-4.4287238 -4.4287434 -4.4287663 -4.4287782 -4.4287505 -4.4287009 -4.4286523 -4.4286485 -4.4286866 -4.4287333 -4.4287724 -4.4287667 -4.4287357 -4.4286952 -4.4286447][-4.4286547 -4.4286757 -4.4287033 -4.4287257 -4.4287033 -4.4286795 -4.4286761 -4.4287004 -4.4287333 -4.4287519 -4.4287586 -4.4287348 -4.4287004 -4.4286747 -4.4286489][-4.4286613 -4.4286771 -4.4286995 -4.4287152 -4.4286971 -4.4286852 -4.4287014 -4.42873 -4.4287562 -4.4287629 -4.4287534 -4.4287238 -4.4286966 -4.4286861 -4.4286771][-4.428731 -4.4287357 -4.42874 -4.428741 -4.4287267 -4.4287281 -4.4287529 -4.4287724 -4.4287848 -4.4287806 -4.4287677 -4.4287438 -4.428731 -4.4287329 -4.4287324][-4.4288182 -4.4288106 -4.4287972 -4.4287872 -4.4287744 -4.4287863 -4.4288163 -4.4288316 -4.4288373 -4.4288321 -4.428813 -4.4287853 -4.4287734 -4.4287786 -4.4287844][-4.4289079 -4.428895 -4.4288731 -4.4288592 -4.4288521 -4.4288692 -4.4288979 -4.4289064 -4.4289026 -4.4288926 -4.4288735 -4.4288459 -4.42883 -4.4288316 -4.4288321][-4.42896 -4.4289513 -4.4289336 -4.4289246 -4.4289255 -4.4289427 -4.4289637 -4.428968 -4.4289627 -4.4289489 -4.4289322 -4.4289136 -4.4289031 -4.4289007 -4.4288921]]...]
INFO - root - 2017-12-08 08:31:39.749451: step 57910, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 16h:52m:26s remains)
INFO - root - 2017-12-08 08:31:41.963169: step 57920, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.225 sec/batch; 17h:10m:01s remains)
INFO - root - 2017-12-08 08:31:44.226667: step 57930, loss = 2.28, batch loss = 2.23 (35.2 examples/sec; 0.227 sec/batch; 17h:19m:56s remains)
INFO - root - 2017-12-08 08:31:46.455989: step 57940, loss = 2.28, batch loss = 2.23 (35.6 examples/sec; 0.225 sec/batch; 17h:09m:21s remains)
INFO - root - 2017-12-08 08:31:48.704115: step 57950, loss = 2.28, batch loss = 2.23 (34.4 examples/sec; 0.232 sec/batch; 17h:42m:51s remains)
INFO - root - 2017-12-08 08:31:50.949861: step 57960, loss = 2.28, batch loss = 2.23 (37.3 examples/sec; 0.215 sec/batch; 16h:21m:55s remains)
INFO - root - 2017-12-08 08:31:53.177687: step 57970, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 16h:58m:42s remains)
INFO - root - 2017-12-08 08:31:55.411489: step 57980, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.222 sec/batch; 16h:55m:10s remains)
INFO - root - 2017-12-08 08:31:57.654061: step 57990, loss = 2.28, batch loss = 2.23 (36.0 examples/sec; 0.222 sec/batch; 16h:57m:23s remains)
INFO - root - 2017-12-08 08:31:59.876988: step 58000, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 16h:50m:25s remains)
2017-12-08 08:32:00.174906: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289474 -4.4289355 -4.4289308 -4.428936 -4.4289451 -4.4289451 -4.4289403 -4.4289508 -4.4289665 -4.4289742 -4.42898 -4.428988 -4.4290028 -4.4290161 -4.4290276][-4.4289 -4.4288754 -4.4288664 -4.4288816 -4.4289064 -4.4289141 -4.4289117 -4.4289274 -4.4289432 -4.4289484 -4.42895 -4.4289575 -4.4289784 -4.4289985 -4.4290161][-4.4288249 -4.4287906 -4.4287786 -4.4287987 -4.4288321 -4.4288535 -4.428864 -4.4288893 -4.4289069 -4.4289074 -4.4289041 -4.4289112 -4.4289408 -4.4289675 -4.4289932][-4.4287348 -4.4287043 -4.4286942 -4.428709 -4.4287338 -4.4287577 -4.4287853 -4.4288325 -4.4288607 -4.4288611 -4.4288511 -4.4288507 -4.4288845 -4.4289193 -4.428956][-4.4286256 -4.4286065 -4.4286084 -4.4286141 -4.4286208 -4.42864 -4.4286795 -4.428751 -4.428802 -4.4288082 -4.4287853 -4.4287705 -4.4288039 -4.4288468 -4.4288979][-4.4285107 -4.4284921 -4.4284911 -4.4284773 -4.4284663 -4.4284816 -4.4285364 -4.4286389 -4.4287171 -4.4287362 -4.42871 -4.4286919 -4.4287267 -4.4287744 -4.4288392][-4.4284534 -4.4284234 -4.4284062 -4.4283791 -4.4283571 -4.4283724 -4.4284334 -4.4285483 -4.4286385 -4.4286704 -4.4286518 -4.4286442 -4.4286828 -4.4287314 -4.4288044][-4.42846 -4.4284415 -4.4284282 -4.4283986 -4.4283686 -4.4283705 -4.4284148 -4.4285192 -4.4286032 -4.42864 -4.428637 -4.4286427 -4.428688 -4.4287343 -4.4288025][-4.4284973 -4.4285169 -4.4285197 -4.4284897 -4.42844 -4.4284205 -4.428441 -4.4285226 -4.4285941 -4.4286284 -4.4286418 -4.4286647 -4.4287152 -4.4287586 -4.4288211][-4.428504 -4.4285536 -4.4285851 -4.4285722 -4.428525 -4.428493 -4.4284897 -4.4285483 -4.4286003 -4.4286356 -4.4286609 -4.4286933 -4.4287419 -4.4287848 -4.4288435][-4.4284258 -4.4284739 -4.4285316 -4.4285483 -4.4285226 -4.4284916 -4.4284816 -4.42853 -4.4285655 -4.4286065 -4.428647 -4.42869 -4.4287424 -4.4287915 -4.4288535][-4.4283428 -4.4283643 -4.4284263 -4.4284635 -4.4284625 -4.4284468 -4.4284511 -4.4285178 -4.4285583 -4.4285941 -4.4286318 -4.4286757 -4.428731 -4.4287882 -4.4288578][-4.4283371 -4.4283257 -4.42837 -4.4284077 -4.4284244 -4.4284253 -4.4284472 -4.4285393 -4.4286022 -4.4286346 -4.4286537 -4.42868 -4.4287257 -4.4287853 -4.428863][-4.4284148 -4.4283929 -4.4284091 -4.4284277 -4.4284439 -4.4284511 -4.4284873 -4.4285903 -4.4286718 -4.4287119 -4.4287167 -4.4287295 -4.4287581 -4.4288054 -4.4288783][-4.4285188 -4.4285007 -4.4284954 -4.4284849 -4.4284878 -4.4284983 -4.4285421 -4.4286442 -4.4287248 -4.4287624 -4.4287663 -4.4287825 -4.4288073 -4.4288421 -4.4289026]]...]
INFO - root - 2017-12-08 08:32:02.392984: step 58010, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 16h:50m:17s remains)
INFO - root - 2017-12-08 08:32:04.637024: step 58020, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.225 sec/batch; 17h:10m:52s remains)
INFO - root - 2017-12-08 08:32:06.891669: step 58030, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.225 sec/batch; 17h:11m:26s remains)
INFO - root - 2017-12-08 08:32:09.136486: step 58040, loss = 2.28, batch loss = 2.23 (35.8 examples/sec; 0.223 sec/batch; 17h:01m:51s remains)
INFO - root - 2017-12-08 08:32:11.371297: step 58050, loss = 2.28, batch loss = 2.23 (36.9 examples/sec; 0.217 sec/batch; 16h:30m:43s remains)
INFO - root - 2017-12-08 08:32:13.599765: step 58060, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 16h:52m:08s remains)
INFO - root - 2017-12-08 08:32:15.862168: step 58070, loss = 2.28, batch loss = 2.23 (35.3 examples/sec; 0.226 sec/batch; 17h:15m:09s remains)
INFO - root - 2017-12-08 08:32:18.095143: step 58080, loss = 2.28, batch loss = 2.23 (35.2 examples/sec; 0.227 sec/batch; 17h:18m:33s remains)
INFO - root - 2017-12-08 08:32:20.439388: step 58090, loss = 2.28, batch loss = 2.23 (35.1 examples/sec; 0.228 sec/batch; 17h:21m:12s remains)
INFO - root - 2017-12-08 08:32:22.677013: step 58100, loss = 2.28, batch loss = 2.23 (35.1 examples/sec; 0.228 sec/batch; 17h:23m:35s remains)
2017-12-08 08:32:22.963352: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288745 -4.4289379 -4.4289684 -4.4289303 -4.4288306 -4.428721 -4.42866 -4.428659 -4.42871 -4.4287648 -4.4287887 -4.4287491 -4.4286518 -4.4285393 -4.4284654][-4.4288378 -4.4289289 -4.4289646 -4.4289231 -4.4288211 -4.4287148 -4.4286633 -4.4286594 -4.4286957 -4.4287477 -4.4287848 -4.4287572 -4.428659 -4.4285269 -4.4284329][-4.4287915 -4.4288778 -4.4289 -4.428853 -4.4287691 -4.4286914 -4.4286642 -4.4286594 -4.4286766 -4.4287186 -4.4287663 -4.4287519 -4.4286566 -4.4285059 -4.4284019][-4.4287648 -4.4288287 -4.4288282 -4.4287705 -4.4286995 -4.42865 -4.4286404 -4.4286356 -4.4286456 -4.4286847 -4.428741 -4.4287367 -4.4286413 -4.4284883 -4.4283853][-4.4287357 -4.4287872 -4.428782 -4.4287133 -4.4286423 -4.4286137 -4.4286213 -4.4286313 -4.4286528 -4.4286995 -4.4287581 -4.4287505 -4.4286394 -4.4284916 -4.4284034][-4.428669 -4.4287233 -4.4287357 -4.4286714 -4.4286051 -4.42859 -4.4286 -4.4286103 -4.4286404 -4.42869 -4.4287472 -4.4287362 -4.4286156 -4.428484 -4.428422][-4.4285994 -4.4286537 -4.4286885 -4.428638 -4.4285774 -4.4285641 -4.4285617 -4.4285636 -4.4285994 -4.4286547 -4.4287081 -4.4287004 -4.4285865 -4.428473 -4.4284363][-4.428545 -4.4285946 -4.4286351 -4.428596 -4.4285383 -4.4285164 -4.4284921 -4.4284883 -4.4285431 -4.4286184 -4.4286714 -4.42866 -4.4285512 -4.4284477 -4.4284277][-4.4284973 -4.42855 -4.4285822 -4.4285469 -4.4284978 -4.4284697 -4.42843 -4.4284172 -4.4284854 -4.4285722 -4.4286208 -4.4286017 -4.4284873 -4.4283838 -4.4283791][-4.4285321 -4.4285736 -4.4285927 -4.4285607 -4.4285192 -4.4284935 -4.4284472 -4.42843 -4.4284954 -4.4285812 -4.4286194 -4.4285884 -4.4284821 -4.4283934 -4.4283981][-4.4286308 -4.4286675 -4.4286866 -4.4286613 -4.4286294 -4.4286089 -4.4285607 -4.4285369 -4.4285846 -4.4286528 -4.4286771 -4.428648 -4.4285707 -4.4285069 -4.4285131][-4.4287372 -4.4287686 -4.4287887 -4.4287753 -4.4287572 -4.4287486 -4.4287128 -4.4286962 -4.4287271 -4.4287672 -4.4287772 -4.4287539 -4.4287004 -4.4286566 -4.4286609][-4.428844 -4.4288654 -4.4288793 -4.4288707 -4.4288597 -4.4288545 -4.4288378 -4.4288321 -4.4288511 -4.42887 -4.4288778 -4.4288616 -4.4288225 -4.4287977 -4.4287972][-4.4289236 -4.4289393 -4.4289465 -4.4289374 -4.4289279 -4.4289227 -4.4289179 -4.42892 -4.4289327 -4.4289427 -4.428947 -4.4289341 -4.428906 -4.4288917 -4.4288878][-4.4289589 -4.4289703 -4.4289737 -4.4289689 -4.4289646 -4.4289613 -4.428957 -4.4289594 -4.428966 -4.4289703 -4.4289722 -4.4289646 -4.4289455 -4.4289317 -4.4289269]]...]
INFO - root - 2017-12-08 08:32:25.221847: step 58110, loss = 2.28, batch loss = 2.23 (35.6 examples/sec; 0.225 sec/batch; 17h:07m:18s remains)
INFO - root - 2017-12-08 08:32:27.472589: step 58120, loss = 2.28, batch loss = 2.23 (35.6 examples/sec; 0.225 sec/batch; 17h:08m:37s remains)
INFO - root - 2017-12-08 08:32:29.732884: step 58130, loss = 2.28, batch loss = 2.23 (35.7 examples/sec; 0.224 sec/batch; 17h:04m:25s remains)
INFO - root - 2017-12-08 08:32:31.967076: step 58140, loss = 2.28, batch loss = 2.23 (36.6 examples/sec; 0.219 sec/batch; 16h:39m:41s remains)
INFO - root - 2017-12-08 08:32:34.209605: step 58150, loss = 2.28, batch loss = 2.23 (35.6 examples/sec; 0.225 sec/batch; 17h:06m:47s remains)
INFO - root - 2017-12-08 08:32:36.468236: step 58160, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.221 sec/batch; 16h:51m:52s remains)
INFO - root - 2017-12-08 08:32:38.722015: step 58170, loss = 2.28, batch loss = 2.23 (34.4 examples/sec; 0.233 sec/batch; 17h:43m:19s remains)
INFO - root - 2017-12-08 08:32:40.945189: step 58180, loss = 2.28, batch loss = 2.23 (35.0 examples/sec; 0.229 sec/batch; 17h:25m:01s remains)
INFO - root - 2017-12-08 08:32:43.164572: step 58190, loss = 2.28, batch loss = 2.23 (33.3 examples/sec; 0.241 sec/batch; 18h:19m:46s remains)
INFO - root - 2017-12-08 08:32:45.407811: step 58200, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.220 sec/batch; 16h:47m:41s remains)
2017-12-08 08:32:45.706188: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288054 -4.4287944 -4.428762 -4.4287229 -4.4287066 -4.4287148 -4.4287419 -4.4287729 -4.4287825 -4.4287791 -4.4287915 -4.4288268 -4.4288516 -4.4288454 -4.4288297][-4.4288788 -4.42886 -4.428822 -4.4287825 -4.4287629 -4.4287686 -4.42879 -4.4288096 -4.4288125 -4.428812 -4.428823 -4.4288397 -4.4288449 -4.4288392 -4.4288416][-4.4289217 -4.4288979 -4.4288568 -4.4288168 -4.4287958 -4.4288025 -4.4288254 -4.4288406 -4.4288454 -4.4288464 -4.4288449 -4.428834 -4.4288135 -4.428803 -4.4288158][-4.4289308 -4.4288979 -4.4288464 -4.4288006 -4.4287772 -4.4287825 -4.4288087 -4.428833 -4.4288478 -4.4288511 -4.4288368 -4.4287972 -4.42875 -4.4287248 -4.4287376][-4.4289107 -4.4288578 -4.4287891 -4.428731 -4.4287043 -4.4287052 -4.4287262 -4.4287534 -4.4287767 -4.4287891 -4.4287748 -4.4287214 -4.4286594 -4.4286275 -4.4286408][-4.4288664 -4.4287877 -4.4286976 -4.4286246 -4.4285846 -4.4285669 -4.4285693 -4.4285927 -4.4286294 -4.4286661 -4.4286704 -4.4286222 -4.4285641 -4.4285393 -4.4285612][-4.4287949 -4.4286971 -4.428596 -4.4285126 -4.428453 -4.4284039 -4.4283752 -4.428391 -4.4284472 -4.4285245 -4.4285679 -4.4285493 -4.428514 -4.4285064 -4.4285316][-4.4287219 -4.4286232 -4.4285326 -4.4284563 -4.4283848 -4.4283066 -4.4282494 -4.4282584 -4.4283276 -4.4284315 -4.4285097 -4.4285297 -4.4285245 -4.4285316 -4.4285536][-4.4286938 -4.4286194 -4.4285588 -4.4285045 -4.4284434 -4.4283657 -4.4283 -4.428297 -4.4283581 -4.4284539 -4.4285316 -4.4285684 -4.4285846 -4.4285994 -4.428618][-4.4287095 -4.4286675 -4.4286356 -4.4286017 -4.4285612 -4.4285045 -4.428453 -4.4284444 -4.4284854 -4.4285522 -4.4286089 -4.4286413 -4.4286594 -4.4286757 -4.4286909][-4.4287114 -4.4286971 -4.4286833 -4.4286637 -4.4286375 -4.4286022 -4.4285712 -4.4285674 -4.4285941 -4.428638 -4.4286761 -4.4286985 -4.4287095 -4.4287133 -4.4287171][-4.4286928 -4.4286966 -4.42869 -4.4286704 -4.4286451 -4.428617 -4.4285927 -4.4285946 -4.4286203 -4.4286542 -4.4286838 -4.4286995 -4.4286981 -4.4286857 -4.4286757][-4.4286695 -4.4286795 -4.4286709 -4.4286423 -4.4286056 -4.4285693 -4.4285426 -4.4285464 -4.4285746 -4.428607 -4.4286308 -4.4286427 -4.428637 -4.4286237 -4.428616][-4.4286828 -4.4286923 -4.4286804 -4.4286451 -4.4286036 -4.4285679 -4.428546 -4.4285493 -4.4285741 -4.4286008 -4.4286194 -4.4286284 -4.4286251 -4.4286227 -4.4286251][-4.4287639 -4.428772 -4.4287605 -4.4287319 -4.4287004 -4.4286771 -4.4286633 -4.4286652 -4.4286814 -4.4286971 -4.4287076 -4.4287133 -4.4287157 -4.4287229 -4.4287314]]...]
INFO - root - 2017-12-08 08:32:47.955477: step 58210, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.226 sec/batch; 17h:11m:33s remains)
INFO - root - 2017-12-08 08:32:50.161011: step 58220, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 16h:43m:13s remains)
INFO - root - 2017-12-08 08:32:52.411467: step 58230, loss = 2.28, batch loss = 2.23 (36.7 examples/sec; 0.218 sec/batch; 16h:36m:26s remains)
INFO - root - 2017-12-08 08:32:54.645618: step 58240, loss = 2.28, batch loss = 2.23 (36.9 examples/sec; 0.217 sec/batch; 16h:30m:20s remains)
INFO - root - 2017-12-08 08:32:56.871824: step 58250, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.220 sec/batch; 16h:46m:37s remains)
INFO - root - 2017-12-08 08:32:59.130395: step 58260, loss = 2.28, batch loss = 2.23 (33.7 examples/sec; 0.238 sec/batch; 18h:06m:00s remains)
INFO - root - 2017-12-08 08:33:01.366832: step 58270, loss = 2.28, batch loss = 2.23 (35.8 examples/sec; 0.224 sec/batch; 17h:02m:43s remains)
INFO - root - 2017-12-08 08:33:03.600452: step 58280, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.222 sec/batch; 16h:52m:30s remains)
INFO - root - 2017-12-08 08:33:05.849265: step 58290, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.222 sec/batch; 16h:52m:50s remains)
INFO - root - 2017-12-08 08:33:08.087143: step 58300, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.225 sec/batch; 17h:09m:46s remains)
2017-12-08 08:33:08.409765: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4287844 -4.4287858 -4.4287786 -4.4287758 -4.4287663 -4.4287539 -4.4287534 -4.428793 -4.4288235 -4.428812 -4.428793 -4.4287782 -4.4287763 -4.4287958 -4.42882][-4.4287729 -4.4287753 -4.4287734 -4.4287777 -4.4287696 -4.4287515 -4.4287438 -4.4287825 -4.4288149 -4.4288044 -4.4287896 -4.428781 -4.4287815 -4.4288 -4.4288173][-4.4287591 -4.428762 -4.4287648 -4.4287724 -4.4287577 -4.4287314 -4.4287162 -4.4287477 -4.4287777 -4.4287729 -4.4287663 -4.4287696 -4.4287729 -4.4287887 -4.428803][-4.4287515 -4.42876 -4.428762 -4.4287596 -4.4287343 -4.4287024 -4.4286814 -4.4287047 -4.4287324 -4.4287305 -4.4287305 -4.4287443 -4.4287519 -4.4287658 -4.4287834][-4.4287438 -4.4287605 -4.4287634 -4.4287453 -4.4287004 -4.4286518 -4.428616 -4.4286208 -4.4286408 -4.4286437 -4.4286547 -4.4286876 -4.4287157 -4.42874 -4.4287634][-4.4287395 -4.428761 -4.42876 -4.4287195 -4.4286427 -4.4285631 -4.4284983 -4.4284754 -4.4284811 -4.4284921 -4.4285307 -4.4286008 -4.428658 -4.4287033 -4.4287376][-4.4287333 -4.4287424 -4.4287262 -4.4286656 -4.4285579 -4.4284434 -4.428359 -4.4283276 -4.4283276 -4.4283485 -4.428421 -4.4285254 -4.4286056 -4.4286675 -4.4287109][-4.4287319 -4.4287171 -4.4286766 -4.4286 -4.4284844 -4.4283628 -4.4282851 -4.4282761 -4.4282889 -4.4283237 -4.4284053 -4.428514 -4.4285946 -4.428658 -4.4287047][-4.4287534 -4.4287248 -4.4286704 -4.4285946 -4.4284997 -4.4284077 -4.4283586 -4.4283748 -4.4283962 -4.4284229 -4.4284854 -4.428575 -4.4286442 -4.4286976 -4.4287372][-4.4287825 -4.4287586 -4.4287143 -4.4286623 -4.4286 -4.4285369 -4.4285059 -4.4285226 -4.4285321 -4.4285369 -4.4285774 -4.4286489 -4.4287109 -4.4287581 -4.428793][-4.4288125 -4.4288039 -4.4287853 -4.4287591 -4.4287205 -4.4286771 -4.4286537 -4.4286585 -4.428647 -4.4286327 -4.4286575 -4.4287167 -4.4287724 -4.4288163 -4.4288478][-4.4288363 -4.4288383 -4.4288411 -4.4288368 -4.4288163 -4.4287872 -4.4287696 -4.4287634 -4.4287376 -4.4287148 -4.4287353 -4.4287834 -4.4288287 -4.4288611 -4.4288831][-4.4288654 -4.4288664 -4.428874 -4.4288783 -4.4288721 -4.4288626 -4.42886 -4.4288535 -4.42883 -4.4288063 -4.4288158 -4.4288483 -4.4288788 -4.4289002 -4.4289126][-4.4289055 -4.4289031 -4.4289074 -4.4289107 -4.4289131 -4.4289155 -4.4289231 -4.4289236 -4.4289117 -4.4288969 -4.4288974 -4.4289112 -4.4289265 -4.428936 -4.4289417][-4.428946 -4.4289412 -4.4289408 -4.4289427 -4.428946 -4.4289503 -4.4289579 -4.4289622 -4.4289575 -4.4289503 -4.4289494 -4.4289532 -4.4289584 -4.4289618 -4.4289646]]...]
INFO - root - 2017-12-08 08:33:10.693223: step 58310, loss = 2.28, batch loss = 2.23 (35.7 examples/sec; 0.224 sec/batch; 17h:05m:12s remains)
INFO - root - 2017-12-08 08:33:12.913217: step 58320, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 16h:56m:56s remains)
INFO - root - 2017-12-08 08:33:15.169075: step 58330, loss = 2.28, batch loss = 2.23 (35.7 examples/sec; 0.224 sec/batch; 17h:04m:44s remains)
INFO - root - 2017-12-08 08:33:17.442093: step 58340, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 16h:45m:02s remains)
INFO - root - 2017-12-08 08:33:19.704348: step 58350, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 16h:45m:06s remains)
INFO - root - 2017-12-08 08:33:21.964899: step 58360, loss = 2.28, batch loss = 2.23 (34.9 examples/sec; 0.229 sec/batch; 17h:28m:01s remains)
INFO - root - 2017-12-08 08:33:24.246686: step 58370, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.225 sec/batch; 17h:08m:55s remains)
INFO - root - 2017-12-08 08:33:26.494445: step 58380, loss = 2.28, batch loss = 2.23 (35.1 examples/sec; 0.228 sec/batch; 17h:20m:40s remains)
INFO - root - 2017-12-08 08:33:28.720111: step 58390, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 16h:58m:33s remains)
INFO - root - 2017-12-08 08:33:30.948624: step 58400, loss = 2.28, batch loss = 2.23 (35.1 examples/sec; 0.228 sec/batch; 17h:22m:11s remains)
2017-12-08 08:33:31.229951: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4286919 -4.4286947 -4.4287214 -4.4287395 -4.4287148 -4.4286742 -4.4286947 -4.4287496 -4.4287758 -4.428761 -4.4287429 -4.4287319 -4.4287009 -4.428678 -4.4286814][-4.4286432 -4.4286652 -4.4286923 -4.4287009 -4.4286675 -4.428616 -4.4286289 -4.428679 -4.4286971 -4.4286785 -4.42867 -4.42867 -4.4286442 -4.4286194 -4.428628][-4.4286118 -4.4286413 -4.4286675 -4.428668 -4.4286308 -4.4285746 -4.4285779 -4.4286218 -4.4286375 -4.4286246 -4.42863 -4.4286366 -4.428617 -4.428597 -4.428606][-4.4285889 -4.4286141 -4.4286389 -4.4286313 -4.4285946 -4.428545 -4.4285426 -4.4285822 -4.4286051 -4.4286013 -4.4286122 -4.42862 -4.4286156 -4.4286151 -4.4286256][-4.4285827 -4.4285975 -4.4286151 -4.428597 -4.4285512 -4.4285054 -4.4285116 -4.428556 -4.4285841 -4.4285827 -4.428587 -4.428597 -4.4286089 -4.42863 -4.4286528][-4.428544 -4.4285631 -4.4285884 -4.428566 -4.4285021 -4.4284554 -4.4284806 -4.4285297 -4.4285522 -4.4285412 -4.4285336 -4.4285445 -4.4285688 -4.4286 -4.4286332][-4.4285145 -4.4285474 -4.4285851 -4.42856 -4.4284844 -4.4284353 -4.4284663 -4.4285083 -4.4285207 -4.4284978 -4.4284725 -4.4284782 -4.4285126 -4.4285507 -4.428587][-4.4285188 -4.4285679 -4.4286113 -4.4285975 -4.428534 -4.4284878 -4.4285016 -4.4285183 -4.428514 -4.4284821 -4.428442 -4.4284387 -4.4284744 -4.4285173 -4.4285531][-4.4285259 -4.4285793 -4.4286184 -4.4286127 -4.4285693 -4.4285336 -4.4285359 -4.4285359 -4.4285264 -4.4284916 -4.4284453 -4.4284296 -4.4284554 -4.4285026 -4.4285374][-4.4285512 -4.4285922 -4.4286122 -4.4286046 -4.4285803 -4.4285617 -4.4285603 -4.4285564 -4.42855 -4.4285216 -4.4284792 -4.4284544 -4.4284639 -4.4285026 -4.4285374][-4.4286094 -4.4286332 -4.4286389 -4.4286327 -4.4286265 -4.4286256 -4.4286261 -4.4286218 -4.4286151 -4.4285927 -4.4285583 -4.4285336 -4.4285331 -4.428556 -4.4285822][-4.4286928 -4.4287043 -4.4287043 -4.428699 -4.4287 -4.4287057 -4.4287081 -4.4287043 -4.4287009 -4.42869 -4.4286718 -4.4286551 -4.4286518 -4.4286609 -4.4286709][-4.4287815 -4.4287863 -4.4287844 -4.4287806 -4.4287786 -4.4287815 -4.4287825 -4.4287796 -4.4287782 -4.4287782 -4.4287791 -4.4287763 -4.4287748 -4.4287763 -4.4287748][-4.4288564 -4.42886 -4.4288607 -4.4288592 -4.4288573 -4.4288564 -4.4288545 -4.4288507 -4.4288468 -4.4288473 -4.428854 -4.42886 -4.428863 -4.4288621 -4.4288549][-4.4289083 -4.4289107 -4.4289107 -4.4289088 -4.428905 -4.4289021 -4.4288993 -4.4288964 -4.4288945 -4.4288974 -4.4289064 -4.4289141 -4.4289165 -4.4289107 -4.4288988]]...]
INFO - root - 2017-12-08 08:33:33.451553: step 58410, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 16h:44m:40s remains)
INFO - root - 2017-12-08 08:33:35.691302: step 58420, loss = 2.28, batch loss = 2.23 (35.8 examples/sec; 0.224 sec/batch; 17h:00m:57s remains)
INFO - root - 2017-12-08 08:33:37.931332: step 58430, loss = 2.28, batch loss = 2.23 (34.0 examples/sec; 0.235 sec/batch; 17h:55m:04s remains)
INFO - root - 2017-12-08 08:33:40.170406: step 58440, loss = 2.28, batch loss = 2.23 (35.7 examples/sec; 0.224 sec/batch; 17h:02m:12s remains)
INFO - root - 2017-12-08 08:33:42.395846: step 58450, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 16h:56m:27s remains)
INFO - root - 2017-12-08 08:33:44.640664: step 58460, loss = 2.28, batch loss = 2.23 (34.7 examples/sec; 0.231 sec/batch; 17h:33m:13s remains)
INFO - root - 2017-12-08 08:33:46.859352: step 58470, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 16h:43m:29s remains)
INFO - root - 2017-12-08 08:33:49.078747: step 58480, loss = 2.28, batch loss = 2.23 (36.9 examples/sec; 0.217 sec/batch; 16h:31m:17s remains)
INFO - root - 2017-12-08 08:33:51.305888: step 58490, loss = 2.28, batch loss = 2.23 (35.7 examples/sec; 0.224 sec/batch; 17h:02m:14s remains)
INFO - root - 2017-12-08 08:33:53.578379: step 58500, loss = 2.28, batch loss = 2.23 (35.6 examples/sec; 0.225 sec/batch; 17h:06m:02s remains)
2017-12-08 08:33:53.872670: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4287643 -4.4288359 -4.4288874 -4.4288745 -4.4288144 -4.4287353 -4.4286766 -4.4286246 -4.4286032 -4.4286566 -4.4287515 -4.4288449 -4.4289165 -4.4289651 -4.4289708][-4.4287562 -4.4288321 -4.4288931 -4.4288878 -4.4288187 -4.4287243 -4.4286575 -4.4286184 -4.4286094 -4.4286642 -4.4287596 -4.428854 -4.428936 -4.4289908 -4.4289947][-4.4287586 -4.4288335 -4.4288921 -4.4288836 -4.4288 -4.4286885 -4.4286113 -4.4285808 -4.4285865 -4.4286523 -4.4287548 -4.4288492 -4.4289427 -4.4290037 -4.4290085][-4.4287677 -4.4288368 -4.4288898 -4.4288664 -4.4287653 -4.4286413 -4.42856 -4.428546 -4.4285712 -4.42865 -4.4287567 -4.4288487 -4.4289446 -4.4290104 -4.429018][-4.4287972 -4.4288507 -4.4288874 -4.4288411 -4.4287167 -4.4285731 -4.4284873 -4.4284987 -4.428555 -4.4286494 -4.4287648 -4.4288545 -4.4289494 -4.4290128 -4.4290223][-4.4288516 -4.4288807 -4.428896 -4.4288216 -4.4286709 -4.428503 -4.4284015 -4.4284258 -4.4285192 -4.4286351 -4.4287629 -4.4288592 -4.428956 -4.4290128 -4.4290209][-4.4289179 -4.4289289 -4.4289207 -4.4288244 -4.4286542 -4.4284658 -4.4283381 -4.4283514 -4.4284754 -4.4286165 -4.4287515 -4.4288573 -4.428956 -4.429008 -4.4290156][-4.4289584 -4.4289594 -4.428936 -4.42883 -4.428658 -4.4284668 -4.4283161 -4.42831 -4.4284444 -4.4285984 -4.4287415 -4.4288592 -4.4289551 -4.4290013 -4.4290094][-4.4289737 -4.4289684 -4.4289379 -4.4288273 -4.4286575 -4.4284687 -4.428319 -4.4283075 -4.4284348 -4.4285913 -4.4287424 -4.4288692 -4.4289589 -4.4290023 -4.429009][-4.4289727 -4.4289622 -4.4289293 -4.4288249 -4.4286685 -4.4284959 -4.4283724 -4.4283619 -4.4284506 -4.4285874 -4.4287386 -4.4288692 -4.4289551 -4.4290013 -4.4290104][-4.428968 -4.4289594 -4.4289336 -4.4288425 -4.4287004 -4.4285536 -4.4284539 -4.4284272 -4.4284658 -4.4285703 -4.4287133 -4.4288473 -4.4289346 -4.4289894 -4.4290066][-4.4289694 -4.4289618 -4.4289451 -4.4288707 -4.4287472 -4.4286246 -4.4285288 -4.4284692 -4.4284654 -4.428546 -4.4286819 -4.4288168 -4.4289107 -4.4289756 -4.4290018][-4.4289737 -4.4289656 -4.4289541 -4.4288974 -4.4287949 -4.4286895 -4.4285808 -4.4284883 -4.4284563 -4.4285207 -4.4286532 -4.42879 -4.4288945 -4.428968 -4.429][-4.4289737 -4.4289646 -4.4289546 -4.4289141 -4.428833 -4.4287434 -4.4286265 -4.4285069 -4.4284549 -4.4285069 -4.4286323 -4.4287729 -4.428885 -4.428967 -4.4290004][-4.4289694 -4.4289603 -4.4289508 -4.4289212 -4.42886 -4.4287882 -4.4286718 -4.4285316 -4.4284568 -4.4284906 -4.428606 -4.4287524 -4.4288774 -4.4289675 -4.429]]...]
INFO - root - 2017-12-08 08:33:56.106285: step 58510, loss = 2.28, batch loss = 2.23 (34.9 examples/sec; 0.229 sec/batch; 17h:25m:40s remains)
INFO - root - 2017-12-08 08:33:58.323116: step 58520, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 16h:49m:21s remains)
INFO - root - 2017-12-08 08:34:00.572724: step 58530, loss = 2.28, batch loss = 2.23 (35.0 examples/sec; 0.229 sec/batch; 17h:24m:16s remains)
INFO - root - 2017-12-08 08:34:02.828266: step 58540, loss = 2.28, batch loss = 2.23 (36.8 examples/sec; 0.218 sec/batch; 16h:33m:24s remains)
INFO - root - 2017-12-08 08:34:05.074819: step 58550, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 16h:40m:25s remains)
INFO - root - 2017-12-08 08:34:07.304047: step 58560, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 16h:49m:00s remains)
INFO - root - 2017-12-08 08:34:09.585867: step 58570, loss = 2.28, batch loss = 2.23 (34.7 examples/sec; 0.230 sec/batch; 17h:31m:47s remains)
INFO - root - 2017-12-08 08:34:11.827114: step 58580, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.221 sec/batch; 16h:51m:07s remains)
INFO - root - 2017-12-08 08:34:14.064856: step 58590, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 16h:56m:15s remains)
INFO - root - 2017-12-08 08:34:16.316309: step 58600, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.225 sec/batch; 17h:08m:27s remains)
2017-12-08 08:34:16.609060: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4287791 -4.4288096 -4.4288239 -4.4287982 -4.4287915 -4.4287934 -4.428772 -4.428741 -4.4287128 -4.4287286 -4.4287386 -4.4287267 -4.4287186 -4.4287429 -4.4287939][-4.4287796 -4.4288168 -4.4288306 -4.428803 -4.4288073 -4.4288163 -4.4287934 -4.4287562 -4.4287205 -4.4287348 -4.4287391 -4.4287157 -4.4286971 -4.4287133 -4.4287691][-4.4287677 -4.4288063 -4.4288158 -4.4287882 -4.4287949 -4.4288011 -4.4287663 -4.4287233 -4.4286919 -4.4287171 -4.4287167 -4.4286904 -4.428668 -4.42868 -4.428741][-4.4287553 -4.4287844 -4.4287896 -4.4287615 -4.4287529 -4.4287357 -4.4286675 -4.4285989 -4.4285731 -4.4286327 -4.4286509 -4.4286375 -4.4286237 -4.4286466 -4.4287162][-4.4287586 -4.428772 -4.4287677 -4.4287252 -4.428688 -4.4286323 -4.4285107 -4.4283915 -4.4283795 -4.4284973 -4.4285645 -4.4285884 -4.4286013 -4.4286418 -4.4287176][-4.4287949 -4.4287834 -4.4287591 -4.4287009 -4.4286375 -4.4285316 -4.42835 -4.42817 -4.4281898 -4.4283814 -4.4285049 -4.4285717 -4.4286175 -4.4286776 -4.4287562][-4.4288363 -4.4288006 -4.4287572 -4.4286823 -4.4286022 -4.4284649 -4.4282494 -4.4280558 -4.4281349 -4.4283648 -4.4285083 -4.4285965 -4.428659 -4.4287243 -4.4287963][-4.428844 -4.4288034 -4.4287553 -4.4286766 -4.4285951 -4.4284639 -4.4282866 -4.42815 -4.4282656 -4.4284582 -4.4285655 -4.4286361 -4.42869 -4.4287481 -4.4288135][-4.42882 -4.4287858 -4.4287462 -4.4286819 -4.4286108 -4.4285064 -4.4283919 -4.4283109 -4.4284053 -4.4285374 -4.4286027 -4.4286489 -4.4286904 -4.4287477 -4.4288111][-4.4288244 -4.4287877 -4.4287529 -4.4287 -4.4286423 -4.4285655 -4.4284983 -4.4284468 -4.4285097 -4.4285917 -4.4286308 -4.4286637 -4.4287014 -4.4287567 -4.4288139][-4.4288635 -4.4288263 -4.428782 -4.428721 -4.4286566 -4.4285831 -4.4285297 -4.4284959 -4.428544 -4.428607 -4.4286423 -4.4286814 -4.4287229 -4.4287772 -4.4288292][-4.4288969 -4.4288554 -4.4287949 -4.4287148 -4.4286408 -4.4285655 -4.4285159 -4.4285 -4.428556 -4.4286218 -4.4286685 -4.4287171 -4.428762 -4.4288073 -4.4288497][-4.428926 -4.428884 -4.4288216 -4.4287314 -4.428648 -4.42858 -4.4285502 -4.4285569 -4.4286232 -4.4286923 -4.4287391 -4.4287829 -4.42882 -4.4288511 -4.4288793][-4.4289508 -4.428915 -4.4288583 -4.428781 -4.4287148 -4.4286609 -4.4286451 -4.4286671 -4.4287248 -4.4287796 -4.4288177 -4.4288492 -4.4288764 -4.4288979 -4.4289122][-4.428957 -4.4289312 -4.4288869 -4.4288344 -4.4287963 -4.4287629 -4.4287605 -4.4287872 -4.4288278 -4.4288616 -4.4288874 -4.4289041 -4.4289179 -4.4289265 -4.4289308]]...]
INFO - root - 2017-12-08 08:34:18.876536: step 58610, loss = 2.28, batch loss = 2.23 (35.2 examples/sec; 0.227 sec/batch; 17h:16m:48s remains)
INFO - root - 2017-12-08 08:34:21.108498: step 58620, loss = 2.28, batch loss = 2.23 (35.6 examples/sec; 0.225 sec/batch; 17h:05m:06s remains)
INFO - root - 2017-12-08 08:34:23.351707: step 58630, loss = 2.28, batch loss = 2.23 (36.8 examples/sec; 0.217 sec/batch; 16h:32m:27s remains)
INFO - root - 2017-12-08 08:34:25.634764: step 58640, loss = 2.28, batch loss = 2.23 (35.6 examples/sec; 0.225 sec/batch; 17h:04m:59s remains)
INFO - root - 2017-12-08 08:34:27.867611: step 58650, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 16h:49m:15s remains)
INFO - root - 2017-12-08 08:34:30.107982: step 58660, loss = 2.28, batch loss = 2.23 (34.4 examples/sec; 0.233 sec/batch; 17h:42m:50s remains)
INFO - root - 2017-12-08 08:34:32.318008: step 58670, loss = 2.28, batch loss = 2.23 (37.0 examples/sec; 0.216 sec/batch; 16h:27m:17s remains)
INFO - root - 2017-12-08 08:34:34.579752: step 58680, loss = 2.28, batch loss = 2.23 (34.7 examples/sec; 0.231 sec/batch; 17h:33m:29s remains)
INFO - root - 2017-12-08 08:34:36.824226: step 58690, loss = 2.28, batch loss = 2.23 (35.0 examples/sec; 0.228 sec/batch; 17h:22m:43s remains)
INFO - root - 2017-12-08 08:34:39.071165: step 58700, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 16h:57m:06s remains)
2017-12-08 08:34:39.348703: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289002 -4.4289055 -4.4289112 -4.4289093 -4.4288974 -4.4288712 -4.428843 -4.42882 -4.4288087 -4.4288235 -4.4288607 -4.4288969 -4.4289227 -4.4289474 -4.4289703][-4.4288573 -4.4288611 -4.4288659 -4.4288592 -4.4288387 -4.4288015 -4.4287658 -4.4287415 -4.4287338 -4.4287558 -4.4288087 -4.428864 -4.4289002 -4.4289284 -4.4289551][-4.4288125 -4.4288192 -4.42883 -4.4288216 -4.4287887 -4.428731 -4.4286718 -4.4286413 -4.4286437 -4.4286747 -4.4287372 -4.4288073 -4.42886 -4.4289012 -4.4289336][-4.4287705 -4.4287858 -4.42881 -4.428803 -4.4287581 -4.4286728 -4.4285755 -4.4285288 -4.4285488 -4.428596 -4.4286637 -4.4287367 -4.4288054 -4.4288654 -4.4289112][-4.4287357 -4.4287581 -4.4287872 -4.4287763 -4.428721 -4.42861 -4.4284616 -4.4283829 -4.4284368 -4.4285231 -4.4285965 -4.4286637 -4.4287434 -4.4288273 -4.42889][-4.4287066 -4.4287276 -4.4287367 -4.4287062 -4.4286404 -4.42851 -4.4283075 -4.4281783 -4.4282742 -4.4284329 -4.4285278 -4.428597 -4.4286876 -4.428791 -4.4288712][-4.4287 -4.4287119 -4.4286895 -4.4286318 -4.4285536 -4.4284148 -4.428174 -4.4279785 -4.4280877 -4.428318 -4.4284568 -4.4285464 -4.4286513 -4.4287658 -4.428854][-4.4287338 -4.4287257 -4.4286747 -4.428606 -4.4285393 -4.4284248 -4.428215 -4.42803 -4.4281116 -4.42833 -4.4284763 -4.4285693 -4.4286752 -4.428781 -4.4288592][-4.4287839 -4.4287639 -4.4286985 -4.4286366 -4.4285975 -4.4285321 -4.4284034 -4.4282947 -4.4283352 -4.4284759 -4.4285851 -4.42866 -4.4287491 -4.4288292 -4.4288883][-4.428823 -4.4287939 -4.42873 -4.4286871 -4.4286761 -4.4286523 -4.4285893 -4.4285417 -4.4285612 -4.4286356 -4.4287047 -4.4287624 -4.4288321 -4.4288883 -4.4289265][-4.42884 -4.4288092 -4.4287663 -4.4287581 -4.4287739 -4.4287748 -4.4287443 -4.4287305 -4.4287472 -4.4287825 -4.4288163 -4.4288507 -4.428894 -4.4289293 -4.4289541][-4.4288425 -4.4288197 -4.4288073 -4.4288282 -4.4288607 -4.4288754 -4.428865 -4.4288621 -4.4288764 -4.4288917 -4.4289031 -4.4289155 -4.4289336 -4.4289546 -4.4289722][-4.4288254 -4.428823 -4.42884 -4.4288797 -4.4289188 -4.4289432 -4.4289474 -4.4289451 -4.4289474 -4.4289484 -4.4289484 -4.4289513 -4.42896 -4.4289746 -4.4289875][-4.42879 -4.4288163 -4.4288578 -4.4289036 -4.4289389 -4.4289608 -4.4289684 -4.4289651 -4.4289589 -4.4289536 -4.4289522 -4.428956 -4.428966 -4.4289813 -4.4289927][-4.4287553 -4.4287996 -4.4288507 -4.4288936 -4.4289207 -4.4289389 -4.4289503 -4.4289494 -4.4289446 -4.4289412 -4.4289441 -4.4289536 -4.4289675 -4.4289837 -4.4289956]]...]
INFO - root - 2017-12-08 08:34:41.575538: step 58710, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.222 sec/batch; 16h:52m:17s remains)
INFO - root - 2017-12-08 08:34:43.787426: step 58720, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 16h:57m:31s remains)
INFO - root - 2017-12-08 08:34:46.028344: step 58730, loss = 2.28, batch loss = 2.23 (35.7 examples/sec; 0.224 sec/batch; 17h:01m:24s remains)
INFO - root - 2017-12-08 08:34:48.256965: step 58740, loss = 2.28, batch loss = 2.23 (35.6 examples/sec; 0.225 sec/batch; 17h:05m:42s remains)
INFO - root - 2017-12-08 08:34:50.486312: step 58750, loss = 2.28, batch loss = 2.23 (37.1 examples/sec; 0.216 sec/batch; 16h:23m:18s remains)
INFO - root - 2017-12-08 08:34:52.719218: step 58760, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 16h:57m:21s remains)
INFO - root - 2017-12-08 08:34:54.944034: step 58770, loss = 2.28, batch loss = 2.23 (36.6 examples/sec; 0.219 sec/batch; 16h:37m:44s remains)
INFO - root - 2017-12-08 08:34:57.170299: step 58780, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 16h:43m:52s remains)
INFO - root - 2017-12-08 08:34:59.435235: step 58790, loss = 2.28, batch loss = 2.23 (35.0 examples/sec; 0.228 sec/batch; 17h:21m:17s remains)
INFO - root - 2017-12-08 08:35:01.700364: step 58800, loss = 2.28, batch loss = 2.23 (36.8 examples/sec; 0.217 sec/batch; 16h:31m:02s remains)
2017-12-08 08:35:01.993415: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288135 -4.4288144 -4.428823 -4.4288268 -4.4288292 -4.428834 -4.4288454 -4.4288526 -4.4288335 -4.4287996 -4.428762 -4.4287515 -4.4287457 -4.42874 -4.4287443][-4.4288154 -4.4288139 -4.4288144 -4.4288087 -4.4288034 -4.428802 -4.428812 -4.4288187 -4.4288039 -4.4287715 -4.4287314 -4.4287329 -4.4287429 -4.42875 -4.4287624][-4.4288144 -4.4288092 -4.4287996 -4.4287848 -4.428772 -4.4287596 -4.4287586 -4.4287677 -4.4287634 -4.4287391 -4.4287038 -4.4287181 -4.4287448 -4.4287658 -4.4287887][-4.4287591 -4.4287481 -4.4287324 -4.4287167 -4.4287033 -4.428679 -4.4286542 -4.4286652 -4.4286847 -4.4286847 -4.4286656 -4.4286823 -4.4287152 -4.4287491 -4.4287868][-4.4286747 -4.4286494 -4.4286246 -4.42861 -4.428586 -4.4285254 -4.4284549 -4.4284563 -4.4285188 -4.4285722 -4.4285879 -4.4286051 -4.4286394 -4.428688 -4.4287477][-4.428637 -4.4286127 -4.4285841 -4.4285622 -4.4285154 -4.4284034 -4.4282608 -4.4282475 -4.4283848 -4.4285212 -4.4285741 -4.4285812 -4.4285984 -4.4286485 -4.4287181][-4.4285436 -4.4285326 -4.428504 -4.4284706 -4.4283915 -4.4282222 -4.42799 -4.4279461 -4.4281759 -4.4284039 -4.428504 -4.4285216 -4.428544 -4.428607 -4.4286914][-4.4284577 -4.4284587 -4.428442 -4.4284139 -4.42833 -4.4281425 -4.4278708 -4.4277968 -4.4280505 -4.4283047 -4.428421 -4.4284472 -4.4284792 -4.4285574 -4.428658][-4.4284763 -4.4284897 -4.4284897 -4.4284792 -4.4284282 -4.428308 -4.42812 -4.4280486 -4.4282103 -4.4283943 -4.4284911 -4.4285131 -4.4285336 -4.428597 -4.4286771][-4.428546 -4.4285631 -4.4285684 -4.4285707 -4.4285483 -4.4284959 -4.4283948 -4.428339 -4.4284143 -4.4285264 -4.4286094 -4.4286375 -4.4286432 -4.4286733 -4.4287157][-4.4286265 -4.4286423 -4.4286528 -4.4286656 -4.4286647 -4.4286551 -4.4286046 -4.4285555 -4.4285693 -4.4286208 -4.4286847 -4.4287238 -4.4287262 -4.4287267 -4.4287362][-4.4286814 -4.4286933 -4.4287128 -4.4287381 -4.4287519 -4.4287624 -4.4287367 -4.428688 -4.4286633 -4.4286695 -4.4287081 -4.4287505 -4.4287539 -4.4287419 -4.42874][-4.428731 -4.4287353 -4.4287624 -4.4287939 -4.428812 -4.4288268 -4.4288149 -4.4287734 -4.4287314 -4.4287086 -4.428721 -4.4287572 -4.4287667 -4.42876 -4.4287624][-4.4288034 -4.4288 -4.4288197 -4.4288449 -4.4288592 -4.4288716 -4.4288692 -4.4288445 -4.4288054 -4.428771 -4.4287629 -4.4287834 -4.4287987 -4.4288096 -4.4288278][-4.4288874 -4.4288745 -4.4288759 -4.4288855 -4.4288926 -4.4288988 -4.4289017 -4.4288893 -4.4288626 -4.4288311 -4.428812 -4.4288244 -4.4288468 -4.4288726 -4.4289007]]...]
INFO - root - 2017-12-08 08:35:04.218496: step 58810, loss = 2.28, batch loss = 2.23 (35.3 examples/sec; 0.226 sec/batch; 17h:13m:01s remains)
INFO - root - 2017-12-08 08:35:06.454604: step 58820, loss = 2.28, batch loss = 2.23 (35.1 examples/sec; 0.228 sec/batch; 17h:20m:19s remains)
INFO - root - 2017-12-08 08:35:08.687926: step 58830, loss = 2.28, batch loss = 2.23 (35.2 examples/sec; 0.227 sec/batch; 17h:15m:43s remains)
INFO - root - 2017-12-08 08:35:10.915649: step 58840, loss = 2.28, batch loss = 2.23 (35.7 examples/sec; 0.224 sec/batch; 17h:01m:54s remains)
INFO - root - 2017-12-08 08:35:13.161361: step 58850, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 16h:57m:17s remains)
INFO - root - 2017-12-08 08:35:15.423777: step 58860, loss = 2.28, batch loss = 2.23 (36.0 examples/sec; 0.222 sec/batch; 16h:54m:38s remains)
INFO - root - 2017-12-08 08:35:17.668546: step 58870, loss = 2.28, batch loss = 2.23 (35.3 examples/sec; 0.226 sec/batch; 17h:12m:21s remains)
INFO - root - 2017-12-08 08:35:19.897175: step 58880, loss = 2.28, batch loss = 2.23 (34.8 examples/sec; 0.230 sec/batch; 17h:28m:09s remains)
INFO - root - 2017-12-08 08:35:22.156919: step 58890, loss = 2.28, batch loss = 2.23 (35.2 examples/sec; 0.227 sec/batch; 17h:16m:25s remains)
INFO - root - 2017-12-08 08:35:24.381521: step 58900, loss = 2.28, batch loss = 2.23 (36.8 examples/sec; 0.217 sec/batch; 16h:30m:40s remains)
2017-12-08 08:35:24.675052: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289784 -4.428966 -4.428957 -4.428956 -4.4289532 -4.4289417 -4.4289174 -4.4288821 -4.4288497 -4.4288306 -4.4288306 -4.4288516 -4.4288845 -4.4289169 -4.4289432][-4.4289784 -4.428967 -4.4289608 -4.4289556 -4.428936 -4.4289017 -4.4288511 -4.428792 -4.4287405 -4.4287071 -4.4287028 -4.4287305 -4.428781 -4.4288321 -4.4288745][-4.4289742 -4.4289575 -4.4289494 -4.4289331 -4.4288888 -4.428822 -4.4287391 -4.4286556 -4.4285874 -4.4285479 -4.4285445 -4.4285765 -4.4286437 -4.4287105 -4.42877][-4.42897 -4.4289474 -4.4289322 -4.4288993 -4.4288244 -4.4287238 -4.42861 -4.4284992 -4.428421 -4.4283891 -4.4284053 -4.4284568 -4.4285312 -4.4285965 -4.4286604][-4.4289641 -4.4289327 -4.4289036 -4.4288473 -4.4287386 -4.4286027 -4.4284587 -4.4283366 -4.4282718 -4.4282713 -4.4283252 -4.4284 -4.4284673 -4.4285169 -4.4285755][-4.4289532 -4.4289083 -4.4288645 -4.4287815 -4.4286432 -4.4284782 -4.4283128 -4.4281983 -4.4281716 -4.428226 -4.4283261 -4.4284096 -4.4284544 -4.42848 -4.42853][-4.4289522 -4.4289007 -4.4288425 -4.4287348 -4.4285779 -4.4283948 -4.4282246 -4.4281321 -4.4281511 -4.4282551 -4.4283814 -4.4284573 -4.4284778 -4.428484 -4.42853][-4.4289565 -4.4289026 -4.4288306 -4.4287028 -4.4285488 -4.4283738 -4.428216 -4.4281459 -4.4281969 -4.4283309 -4.4284668 -4.4285336 -4.4285383 -4.4285293 -4.4285703][-4.4289627 -4.4289093 -4.4288335 -4.4287043 -4.428566 -4.4284043 -4.4282556 -4.4281931 -4.4282575 -4.4284086 -4.4285455 -4.4286036 -4.428596 -4.4285789 -4.4286122][-4.4289665 -4.4289174 -4.4288492 -4.4287248 -4.4285951 -4.4284358 -4.4282794 -4.4282146 -4.4282932 -4.4284587 -4.4285955 -4.4286447 -4.4286294 -4.42861 -4.4286375][-4.428956 -4.4289188 -4.4288669 -4.4287543 -4.4286318 -4.4284735 -4.4283094 -4.4282441 -4.4283323 -4.4284959 -4.4286222 -4.4286647 -4.4286451 -4.4286222 -4.4286375][-4.4289436 -4.42892 -4.4288778 -4.4287806 -4.42867 -4.428524 -4.4283667 -4.4283013 -4.4283848 -4.4285369 -4.4286547 -4.4286966 -4.4286709 -4.4286361 -4.4286284][-4.4289341 -4.4289179 -4.4288816 -4.4287982 -4.4286971 -4.428575 -4.4284463 -4.4284005 -4.4284797 -4.42861 -4.4287128 -4.4287472 -4.4287143 -4.4286609 -4.4286332][-4.4289341 -4.4289217 -4.428885 -4.4288116 -4.4287233 -4.4286289 -4.4285474 -4.4285374 -4.4286118 -4.4287195 -4.4287977 -4.428812 -4.4287658 -4.4287019 -4.4286704][-4.4289436 -4.4289312 -4.4288955 -4.4288316 -4.4287615 -4.4287024 -4.4286714 -4.4286909 -4.4287529 -4.4288306 -4.428874 -4.4288664 -4.4288163 -4.4287605 -4.4287386]]...]
INFO - root - 2017-12-08 08:35:26.905675: step 58910, loss = 2.28, batch loss = 2.23 (36.7 examples/sec; 0.218 sec/batch; 16h:33m:07s remains)
INFO - root - 2017-12-08 08:35:29.136759: step 58920, loss = 2.28, batch loss = 2.23 (36.7 examples/sec; 0.218 sec/batch; 16h:34m:40s remains)
INFO - root - 2017-12-08 08:35:31.367788: step 58930, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.221 sec/batch; 16h:46m:04s remains)
INFO - root - 2017-12-08 08:35:33.609115: step 58940, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 16h:38m:56s remains)
INFO - root - 2017-12-08 08:35:35.835264: step 58950, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.225 sec/batch; 17h:06m:35s remains)
INFO - root - 2017-12-08 08:35:38.057468: step 58960, loss = 2.28, batch loss = 2.23 (34.9 examples/sec; 0.229 sec/batch; 17h:25m:50s remains)
INFO - root - 2017-12-08 08:35:40.284292: step 58970, loss = 2.28, batch loss = 2.23 (35.4 examples/sec; 0.226 sec/batch; 17h:10m:11s remains)
INFO - root - 2017-12-08 08:35:42.531400: step 58980, loss = 2.28, batch loss = 2.23 (35.4 examples/sec; 0.226 sec/batch; 17h:11m:28s remains)
INFO - root - 2017-12-08 08:35:44.774032: step 58990, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 16h:38m:50s remains)
INFO - root - 2017-12-08 08:35:47.024769: step 59000, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 16h:55m:31s remains)
2017-12-08 08:35:47.322143: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4286461 -4.4286971 -4.428719 -4.4286923 -4.4286618 -4.4286404 -4.428638 -4.4286327 -4.4286427 -4.4286814 -4.4287114 -4.4287181 -4.4286895 -4.42866 -4.4286623][-4.4287157 -4.4287372 -4.4287434 -4.4287205 -4.4286919 -4.4286671 -4.4286537 -4.4286346 -4.4286375 -4.4286671 -4.4286971 -4.4287291 -4.4287243 -4.4287009 -4.4286938][-4.428762 -4.42877 -4.4287667 -4.4287491 -4.4287267 -4.4287128 -4.4287052 -4.4286871 -4.4286747 -4.4286709 -4.4286685 -4.4286857 -4.4286971 -4.4286909 -4.4286938][-4.42878 -4.4287772 -4.4287677 -4.4287562 -4.4287467 -4.4287415 -4.4287491 -4.4287467 -4.4287271 -4.4286847 -4.4286366 -4.4286261 -4.4286494 -4.4286628 -4.4286695][-4.4287553 -4.4287386 -4.4287171 -4.4287071 -4.4287014 -4.4286914 -4.4287133 -4.42874 -4.4287386 -4.4286909 -4.4286318 -4.4286132 -4.4286361 -4.4286518 -4.4286551][-4.4286985 -4.4286871 -4.428659 -4.4286356 -4.4285955 -4.42854 -4.42857 -4.4286575 -4.4287062 -4.4286928 -4.4286547 -4.4286323 -4.4286337 -4.4286456 -4.4286561][-4.4286509 -4.4286666 -4.4286361 -4.4285827 -4.4284706 -4.4283276 -4.42836 -4.4285312 -4.4286447 -4.4286785 -4.4286647 -4.4286337 -4.4286175 -4.4286427 -4.4286761][-4.4286528 -4.4286876 -4.4286628 -4.4285975 -4.4284534 -4.4282646 -4.4282904 -4.4284782 -4.4285889 -4.4286351 -4.4286394 -4.4286118 -4.4285932 -4.4286218 -4.4286642][-4.4287004 -4.4287481 -4.4287386 -4.4286971 -4.4285917 -4.4284534 -4.4284444 -4.4285293 -4.4285641 -4.4285836 -4.4286036 -4.4286022 -4.4285917 -4.4285984 -4.4286118][-4.4287634 -4.4287939 -4.4287829 -4.4287634 -4.42871 -4.4286318 -4.4286036 -4.4286008 -4.4285736 -4.4285755 -4.4286194 -4.4286447 -4.4286265 -4.4285917 -4.428565][-4.4287982 -4.4287906 -4.4287672 -4.428771 -4.4287696 -4.4287357 -4.4287057 -4.4286747 -4.428628 -4.42863 -4.4286814 -4.4286985 -4.4286532 -4.4285917 -4.428555][-4.4288063 -4.4287686 -4.4287314 -4.4287496 -4.4287753 -4.4287729 -4.4287667 -4.4287386 -4.428689 -4.4286785 -4.4287043 -4.4287019 -4.4286537 -4.4286132 -4.4286003][-4.4287996 -4.4287519 -4.4287086 -4.428721 -4.4287496 -4.4287643 -4.4287758 -4.4287548 -4.4286995 -4.4286704 -4.4286728 -4.4286666 -4.4286413 -4.4286308 -4.4286265][-4.428791 -4.4287486 -4.4287066 -4.4287071 -4.42873 -4.4287515 -4.428762 -4.4287343 -4.4286742 -4.4286447 -4.428647 -4.4286509 -4.4286461 -4.42864 -4.4286222][-4.4287972 -4.4287653 -4.428731 -4.4287291 -4.4287443 -4.4287577 -4.4287548 -4.4287195 -4.4286661 -4.4286485 -4.4286633 -4.4286733 -4.428659 -4.4286289 -4.4285975]]...]
INFO - root - 2017-12-08 08:35:49.548988: step 59010, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.221 sec/batch; 16h:45m:43s remains)
INFO - root - 2017-12-08 08:35:51.814807: step 59020, loss = 2.28, batch loss = 2.23 (35.4 examples/sec; 0.226 sec/batch; 17h:09m:49s remains)
INFO - root - 2017-12-08 08:35:54.101195: step 59030, loss = 2.28, batch loss = 2.23 (34.9 examples/sec; 0.229 sec/batch; 17h:23m:21s remains)
INFO - root - 2017-12-08 08:35:56.361549: step 59040, loss = 2.28, batch loss = 2.23 (32.2 examples/sec; 0.248 sec/batch; 18h:51m:41s remains)
INFO - root - 2017-12-08 08:35:58.593886: step 59050, loss = 2.28, batch loss = 2.23 (35.8 examples/sec; 0.224 sec/batch; 16h:58m:39s remains)
INFO - root - 2017-12-08 08:36:00.825100: step 59060, loss = 2.28, batch loss = 2.23 (35.1 examples/sec; 0.228 sec/batch; 17h:17m:55s remains)
INFO - root - 2017-12-08 08:36:03.094182: step 59070, loss = 2.28, batch loss = 2.23 (35.3 examples/sec; 0.227 sec/batch; 17h:13m:38s remains)
INFO - root - 2017-12-08 08:36:05.357434: step 59080, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.220 sec/batch; 16h:44m:44s remains)
INFO - root - 2017-12-08 08:36:07.601753: step 59090, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 16h:41m:11s remains)
INFO - root - 2017-12-08 08:36:09.842054: step 59100, loss = 2.28, batch loss = 2.23 (35.3 examples/sec; 0.227 sec/batch; 17h:13m:11s remains)
2017-12-08 08:36:10.131065: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4282641 -4.4282622 -4.4283562 -4.4284849 -4.4286432 -4.4287853 -4.4288383 -4.4287772 -4.4286227 -4.4284492 -4.4283204 -4.4282966 -4.4283962 -4.4285164 -4.4286304][-4.4282856 -4.4282932 -4.428391 -4.4285121 -4.4286489 -4.4287786 -4.4288473 -4.4288316 -4.4287434 -4.4286242 -4.4285226 -4.428494 -4.4285722 -4.4286666 -4.4287381][-4.4284315 -4.4284425 -4.4285297 -4.4286222 -4.428709 -4.4287848 -4.4288349 -4.4288478 -4.4288263 -4.4287858 -4.42874 -4.428721 -4.4287596 -4.4288054 -4.4288206][-4.4286151 -4.4286132 -4.4286675 -4.4287243 -4.4287586 -4.4287758 -4.4287934 -4.4288154 -4.4288378 -4.4288592 -4.4288726 -4.42888 -4.428895 -4.4288907 -4.4288521][-4.4287839 -4.4287634 -4.4287758 -4.4287815 -4.4287639 -4.4287295 -4.4287162 -4.4287429 -4.4287882 -4.4288454 -4.428905 -4.4289427 -4.42895 -4.4289103 -4.4288225][-4.4289021 -4.4288664 -4.4288287 -4.4287705 -4.4286914 -4.4286127 -4.4285789 -4.428617 -4.4286842 -4.4287615 -4.4288588 -4.428926 -4.4289336 -4.42887 -4.4287353][-4.4289565 -4.4289093 -4.4288297 -4.4287095 -4.4285722 -4.42846 -4.4284062 -4.4284434 -4.4285288 -4.4286342 -4.4287705 -4.4288707 -4.4288855 -4.4288039 -4.4286404][-4.4289742 -4.4289236 -4.4288254 -4.4286652 -4.4284873 -4.4283419 -4.42826 -4.4282732 -4.4283705 -4.4285078 -4.4286833 -4.4288197 -4.4288511 -4.42877 -4.4286][-4.4289756 -4.4289351 -4.4288468 -4.4286952 -4.4285169 -4.4283509 -4.4282355 -4.4282193 -4.4283104 -4.42846 -4.4286518 -4.4288049 -4.4288554 -4.4287968 -4.4286518][-4.4289651 -4.4289341 -4.428874 -4.4287744 -4.4286432 -4.428494 -4.428369 -4.4283381 -4.4284163 -4.4285479 -4.4287038 -4.4288216 -4.4288683 -4.4288397 -4.428751][-4.4289417 -4.4289088 -4.4288726 -4.4288287 -4.4287667 -4.4286718 -4.4285784 -4.428556 -4.4286175 -4.4287071 -4.4287972 -4.42885 -4.4288673 -4.4288549 -4.428822][-4.42891 -4.4288683 -4.428844 -4.4288406 -4.428834 -4.4288054 -4.4287734 -4.4287753 -4.4288177 -4.4288554 -4.4288688 -4.4288568 -4.4288387 -4.4288259 -4.4288282][-4.428865 -4.428813 -4.4287868 -4.4287987 -4.4288306 -4.428853 -4.4288735 -4.4289069 -4.4289403 -4.4289393 -4.4288893 -4.428823 -4.4287663 -4.4287515 -4.4287758][-4.4288096 -4.4287357 -4.428699 -4.4287162 -4.4287686 -4.4288192 -4.4288712 -4.4289327 -4.4289684 -4.4289465 -4.4288559 -4.4287357 -4.4286437 -4.4286256 -4.4286709][-4.4287395 -4.4286351 -4.428586 -4.4286056 -4.4286695 -4.428731 -4.4288011 -4.42889 -4.4289336 -4.428905 -4.4287887 -4.4286184 -4.4284935 -4.4284635 -4.4285164]]...]
INFO - root - 2017-12-08 08:36:12.377676: step 59110, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.220 sec/batch; 16h:42m:56s remains)
INFO - root - 2017-12-08 08:36:14.641770: step 59120, loss = 2.28, batch loss = 2.23 (35.7 examples/sec; 0.224 sec/batch; 17h:02m:03s remains)
INFO - root - 2017-12-08 08:36:16.871979: step 59130, loss = 2.28, batch loss = 2.23 (35.8 examples/sec; 0.223 sec/batch; 16h:56m:59s remains)
INFO - root - 2017-12-08 08:36:19.111037: step 59140, loss = 2.28, batch loss = 2.23 (34.8 examples/sec; 0.230 sec/batch; 17h:28m:40s remains)
INFO - root - 2017-12-08 08:36:21.359182: step 59150, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.222 sec/batch; 16h:49m:15s remains)
INFO - root - 2017-12-08 08:36:23.643722: step 59160, loss = 2.28, batch loss = 2.23 (34.9 examples/sec; 0.229 sec/batch; 17h:23m:30s remains)
INFO - root - 2017-12-08 08:36:25.901633: step 59170, loss = 2.28, batch loss = 2.23 (34.9 examples/sec; 0.230 sec/batch; 17h:25m:37s remains)
INFO - root - 2017-12-08 08:36:28.121202: step 59180, loss = 2.28, batch loss = 2.23 (35.4 examples/sec; 0.226 sec/batch; 17h:08m:16s remains)
INFO - root - 2017-12-08 08:36:30.367623: step 59190, loss = 2.28, batch loss = 2.23 (36.7 examples/sec; 0.218 sec/batch; 16h:32m:35s remains)
INFO - root - 2017-12-08 08:36:32.591858: step 59200, loss = 2.28, batch loss = 2.23 (37.0 examples/sec; 0.216 sec/batch; 16h:24m:43s remains)
2017-12-08 08:36:32.878704: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288707 -4.4288521 -4.4288211 -4.4287925 -4.4287567 -4.4287271 -4.4287319 -4.4287672 -4.4287996 -4.4288287 -4.4288487 -4.428854 -4.428834 -4.4288135 -4.428791][-4.4288578 -4.4288325 -4.4288 -4.4287739 -4.4287357 -4.4287014 -4.4287062 -4.4287415 -4.428772 -4.4288011 -4.4288211 -4.4288235 -4.4288006 -4.4287767 -4.4287457][-4.4288416 -4.4288225 -4.4288034 -4.4287891 -4.4287543 -4.4287124 -4.4287062 -4.4287262 -4.4287462 -4.4287786 -4.4288054 -4.4288111 -4.4287887 -4.428761 -4.4287162][-4.4288239 -4.4288249 -4.4288287 -4.4288278 -4.4287949 -4.4287405 -4.4287128 -4.4287043 -4.4287114 -4.4287543 -4.4287944 -4.4288082 -4.428793 -4.4287653 -4.4287124][-4.4288039 -4.4288321 -4.4288607 -4.4288678 -4.4288316 -4.4287615 -4.4287057 -4.4286642 -4.4286585 -4.4287157 -4.4287744 -4.428802 -4.4288011 -4.428781 -4.4287333][-4.4287767 -4.4288244 -4.4288692 -4.428875 -4.4288292 -4.4287395 -4.42865 -4.4285769 -4.4285679 -4.4286489 -4.4287324 -4.42878 -4.4288006 -4.4287949 -4.4287605][-4.4287395 -4.4287963 -4.4288449 -4.4288425 -4.4287815 -4.428669 -4.4285469 -4.4284482 -4.4284487 -4.4285665 -4.4286814 -4.4287534 -4.4287949 -4.4288068 -4.4287877][-4.4287152 -4.4287691 -4.42881 -4.428793 -4.4287128 -4.4285803 -4.428441 -4.4283338 -4.4283605 -4.4285178 -4.4286633 -4.4287558 -4.4288063 -4.4288273 -4.4288149][-4.4287391 -4.4287839 -4.42881 -4.4287772 -4.428688 -4.4285631 -4.4284458 -4.42837 -4.4284215 -4.4285722 -4.4287033 -4.4287858 -4.4288282 -4.4288445 -4.4288316][-4.428782 -4.4288182 -4.4288325 -4.4287977 -4.4287186 -4.4286227 -4.4285507 -4.4285178 -4.4285674 -4.4286695 -4.4287553 -4.4288106 -4.4288368 -4.4288445 -4.4288344][-4.4288192 -4.4288545 -4.4288669 -4.4288397 -4.4287753 -4.4287019 -4.4286604 -4.42865 -4.4286809 -4.4287338 -4.4287758 -4.428803 -4.4288154 -4.4288177 -4.4288077][-4.4288511 -4.4288883 -4.4289031 -4.4288845 -4.4288354 -4.4287791 -4.4287481 -4.4287343 -4.4287372 -4.4287486 -4.4287567 -4.4287634 -4.4287663 -4.4287672 -4.4287567][-4.4288864 -4.4289155 -4.4289246 -4.4289079 -4.4288692 -4.42882 -4.4287825 -4.428751 -4.4287271 -4.4287038 -4.4286857 -4.4286833 -4.4286871 -4.4286971 -4.4286966][-4.4289112 -4.4289241 -4.4289184 -4.4288921 -4.4288521 -4.4288034 -4.4287572 -4.4287105 -4.4286718 -4.4286308 -4.4286027 -4.4286056 -4.4286213 -4.4286442 -4.4286618][-4.4289112 -4.4289083 -4.4288917 -4.4288588 -4.4288139 -4.428761 -4.4287071 -4.4286556 -4.428616 -4.4285793 -4.4285607 -4.428576 -4.4286056 -4.42864 -4.4286685]]...]
INFO - root - 2017-12-08 08:36:35.151907: step 59210, loss = 2.28, batch loss = 2.23 (37.8 examples/sec; 0.212 sec/batch; 16h:03m:46s remains)
INFO - root - 2017-12-08 08:36:37.388813: step 59220, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.220 sec/batch; 16h:43m:06s remains)
INFO - root - 2017-12-08 08:36:39.614733: step 59230, loss = 2.28, batch loss = 2.23 (36.6 examples/sec; 0.218 sec/batch; 16h:34m:23s remains)
INFO - root - 2017-12-08 08:36:41.849012: step 59240, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.222 sec/batch; 16h:49m:35s remains)
INFO - root - 2017-12-08 08:36:44.101145: step 59250, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.225 sec/batch; 17h:05m:42s remains)
INFO - root - 2017-12-08 08:36:46.339831: step 59260, loss = 2.28, batch loss = 2.23 (37.0 examples/sec; 0.216 sec/batch; 16h:25m:17s remains)
INFO - root - 2017-12-08 08:36:48.576381: step 59270, loss = 2.28, batch loss = 2.23 (34.9 examples/sec; 0.229 sec/batch; 17h:24m:58s remains)
INFO - root - 2017-12-08 08:36:50.829859: step 59280, loss = 2.28, batch loss = 2.23 (32.5 examples/sec; 0.246 sec/batch; 18h:40m:27s remains)
INFO - root - 2017-12-08 08:36:53.096641: step 59290, loss = 2.28, batch loss = 2.23 (34.4 examples/sec; 0.233 sec/batch; 17h:39m:55s remains)
INFO - root - 2017-12-08 08:36:55.343371: step 59300, loss = 2.28, batch loss = 2.23 (34.9 examples/sec; 0.229 sec/batch; 17h:24m:29s remains)
2017-12-08 08:36:55.652991: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4287758 -4.4287353 -4.4286957 -4.4286647 -4.4286771 -4.4287267 -4.4288011 -4.42886 -4.4288983 -4.4289041 -4.4288845 -4.42887 -4.4288707 -4.4288697 -4.42887][-4.4287529 -4.4287124 -4.4286819 -4.428668 -4.4286861 -4.4287319 -4.428791 -4.4288316 -4.4288487 -4.42883 -4.4287906 -4.4287677 -4.4287643 -4.4287658 -4.42878][-4.4287543 -4.4287286 -4.4287138 -4.4287086 -4.428721 -4.4287424 -4.4287643 -4.4287686 -4.4287438 -4.4286919 -4.4286385 -4.4286132 -4.4286175 -4.4286375 -4.4286833][-4.4287777 -4.4287682 -4.4287605 -4.428751 -4.4287434 -4.4287248 -4.4286919 -4.42864 -4.4285641 -4.4284868 -4.4284391 -4.4284358 -4.4284739 -4.42854 -4.428627][-4.4288 -4.4287992 -4.428792 -4.4287686 -4.428731 -4.4286637 -4.4285541 -4.4284282 -4.4283066 -4.4282446 -4.4282489 -4.4282985 -4.4283867 -4.4285054 -4.4286284][-4.4288116 -4.4288054 -4.4287887 -4.4287443 -4.4286685 -4.4285388 -4.4283428 -4.4281397 -4.4280066 -4.4280319 -4.4281435 -4.4282722 -4.4284072 -4.42855 -4.4286761][-4.4287968 -4.4287677 -4.4287267 -4.4286551 -4.4285412 -4.4283652 -4.4281082 -4.4278493 -4.4277582 -4.427938 -4.4281807 -4.4283767 -4.4285274 -4.4286532 -4.4287434][-4.4287438 -4.4286871 -4.428616 -4.4285235 -4.4284039 -4.4282422 -4.4280014 -4.427773 -4.4277706 -4.4280424 -4.4283252 -4.4285274 -4.4286714 -4.4287672 -4.4288073][-4.4286561 -4.4285808 -4.428494 -4.4284077 -4.4283366 -4.428257 -4.4281144 -4.4279928 -4.4280362 -4.4282618 -4.4284883 -4.4286528 -4.4287758 -4.428844 -4.4288464][-4.4285631 -4.4284925 -4.4284196 -4.4283719 -4.4283738 -4.4283795 -4.4283347 -4.428297 -4.4283428 -4.4284859 -4.4286356 -4.4287543 -4.4288478 -4.4288883 -4.428865][-4.4285364 -4.4284873 -4.4284406 -4.4284344 -4.4284887 -4.4285522 -4.4285736 -4.42858 -4.4286151 -4.4286971 -4.4287839 -4.4288588 -4.4289165 -4.428925 -4.4288836][-4.4286017 -4.4285688 -4.4285431 -4.4285612 -4.4286351 -4.4287176 -4.4287667 -4.4287891 -4.4288154 -4.42886 -4.4289064 -4.4289474 -4.4289665 -4.4289441 -4.4288874][-4.4287176 -4.4286976 -4.4286847 -4.4287014 -4.4287643 -4.4288397 -4.4288945 -4.4289231 -4.4289412 -4.4289603 -4.428978 -4.4289856 -4.4289641 -4.4289141 -4.4288492][-4.428822 -4.42881 -4.4288006 -4.4288058 -4.4288449 -4.4288993 -4.428947 -4.4289746 -4.4289846 -4.428987 -4.4289789 -4.4289541 -4.4288979 -4.4288259 -4.4287543][-4.4288797 -4.428875 -4.4288673 -4.428864 -4.4288826 -4.4289145 -4.4289441 -4.4289603 -4.4289589 -4.4289479 -4.4289222 -4.4288764 -4.4288034 -4.4287238 -4.428658]]...]
INFO - root - 2017-12-08 08:36:57.851470: step 59310, loss = 2.28, batch loss = 2.23 (36.9 examples/sec; 0.217 sec/batch; 16h:27m:48s remains)
INFO - root - 2017-12-08 08:37:00.119701: step 59320, loss = 2.28, batch loss = 2.23 (35.3 examples/sec; 0.227 sec/batch; 17h:12m:14s remains)
INFO - root - 2017-12-08 08:37:02.352343: step 59330, loss = 2.28, batch loss = 2.23 (35.6 examples/sec; 0.224 sec/batch; 17h:02m:04s remains)
INFO - root - 2017-12-08 08:37:04.585864: step 59340, loss = 2.28, batch loss = 2.23 (35.0 examples/sec; 0.228 sec/batch; 17h:19m:34s remains)
INFO - root - 2017-12-08 08:37:06.821389: step 59350, loss = 2.28, batch loss = 2.23 (34.9 examples/sec; 0.229 sec/batch; 17h:23m:35s remains)
INFO - root - 2017-12-08 08:37:09.073506: step 59360, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 16h:37m:24s remains)
INFO - root - 2017-12-08 08:37:11.298181: step 59370, loss = 2.28, batch loss = 2.23 (36.7 examples/sec; 0.218 sec/batch; 16h:32m:01s remains)
INFO - root - 2017-12-08 08:37:13.558115: step 59380, loss = 2.28, batch loss = 2.23 (35.2 examples/sec; 0.227 sec/batch; 17h:15m:01s remains)
INFO - root - 2017-12-08 08:37:15.838331: step 59390, loss = 2.28, batch loss = 2.23 (36.8 examples/sec; 0.217 sec/batch; 16h:28m:37s remains)
INFO - root - 2017-12-08 08:37:18.099351: step 59400, loss = 2.28, batch loss = 2.23 (35.4 examples/sec; 0.226 sec/batch; 17h:08m:34s remains)
2017-12-08 08:37:18.454512: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4287925 -4.428792 -4.4287896 -4.4287863 -4.4287844 -4.428782 -4.4287796 -4.4287777 -4.4287763 -4.4287729 -4.4287672 -4.428762 -4.4287591 -4.4287581 -4.4287586][-4.4287629 -4.4287643 -4.4287634 -4.428761 -4.4287581 -4.4287539 -4.428751 -4.4287505 -4.428751 -4.4287477 -4.4287419 -4.4287362 -4.4287324 -4.4287314 -4.4287314][-4.428721 -4.4287252 -4.4287257 -4.4287248 -4.4287219 -4.4287162 -4.4287138 -4.4287176 -4.4287219 -4.4287224 -4.4287186 -4.4287148 -4.4287105 -4.4287081 -4.4287062][-4.4286885 -4.4286981 -4.4287014 -4.4287038 -4.4287028 -4.4286981 -4.4286981 -4.4287052 -4.4287148 -4.4287176 -4.4287148 -4.4287105 -4.4287038 -4.4286995 -4.4286942][-4.42866 -4.4286752 -4.4286814 -4.4286847 -4.4286842 -4.42868 -4.4286795 -4.4286876 -4.4287014 -4.4287066 -4.4287024 -4.4286928 -4.4286819 -4.4286757 -4.4286709][-4.42863 -4.4286504 -4.4286585 -4.4286604 -4.4286575 -4.4286523 -4.4286475 -4.4286532 -4.4286704 -4.4286776 -4.4286709 -4.4286551 -4.4286404 -4.4286346 -4.4286313][-4.4286451 -4.4286652 -4.4286714 -4.4286671 -4.428658 -4.4286489 -4.428638 -4.4286389 -4.4286551 -4.4286613 -4.4286489 -4.4286232 -4.4286008 -4.4285922 -4.4285884][-4.4287062 -4.4287214 -4.4287262 -4.428721 -4.4287114 -4.4287033 -4.4286933 -4.4286942 -4.4287057 -4.4287057 -4.4286857 -4.4286509 -4.42862 -4.4286 -4.4285865][-4.4287634 -4.4287724 -4.4287739 -4.4287686 -4.428761 -4.4287577 -4.4287548 -4.4287591 -4.4287682 -4.4287663 -4.4287472 -4.4287148 -4.4286819 -4.4286551 -4.4286332][-4.42881 -4.4288149 -4.4288144 -4.4288106 -4.4288068 -4.4288096 -4.4288139 -4.428822 -4.4288287 -4.4288268 -4.4288111 -4.4287839 -4.4287534 -4.4287262 -4.4287028][-4.428843 -4.4288473 -4.4288468 -4.4288459 -4.4288468 -4.4288526 -4.42886 -4.4288678 -4.428874 -4.4288735 -4.4288626 -4.4288425 -4.4288192 -4.4287953 -4.4287739][-4.4288554 -4.4288592 -4.4288597 -4.4288611 -4.4288654 -4.4288716 -4.4288788 -4.4288855 -4.4288912 -4.4288917 -4.4288845 -4.4288716 -4.4288554 -4.4288378 -4.428822][-4.4288521 -4.4288554 -4.4288568 -4.4288597 -4.4288635 -4.4288669 -4.4288697 -4.4288731 -4.4288754 -4.4288754 -4.4288721 -4.4288664 -4.4288583 -4.4288487 -4.4288411][-4.4288511 -4.4288549 -4.4288559 -4.4288578 -4.4288597 -4.4288588 -4.4288564 -4.4288549 -4.428854 -4.4288521 -4.4288507 -4.4288483 -4.4288454 -4.4288411 -4.4288392][-4.4288545 -4.4288588 -4.4288583 -4.4288588 -4.42886 -4.4288588 -4.4288549 -4.4288521 -4.4288511 -4.4288492 -4.4288478 -4.4288459 -4.4288425 -4.4288387 -4.4288378]]...]
INFO - root - 2017-12-08 08:37:20.725368: step 59410, loss = 2.28, batch loss = 2.23 (35.3 examples/sec; 0.227 sec/batch; 17h:11m:54s remains)
INFO - root - 2017-12-08 08:37:22.993300: step 59420, loss = 2.28, batch loss = 2.23 (35.8 examples/sec; 0.223 sec/batch; 16h:56m:24s remains)
INFO - root - 2017-12-08 08:37:25.242989: step 59430, loss = 2.28, batch loss = 2.23 (36.9 examples/sec; 0.217 sec/batch; 16h:25m:32s remains)
INFO - root - 2017-12-08 08:37:27.475086: step 59440, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 16h:38m:59s remains)
INFO - root - 2017-12-08 08:37:29.730951: step 59450, loss = 2.28, batch loss = 2.23 (34.4 examples/sec; 0.232 sec/batch; 17h:37m:51s remains)
INFO - root - 2017-12-08 08:37:31.975000: step 59460, loss = 2.28, batch loss = 2.23 (35.4 examples/sec; 0.226 sec/batch; 17h:08m:58s remains)
INFO - root - 2017-12-08 08:37:34.229744: step 59470, loss = 2.28, batch loss = 2.23 (35.0 examples/sec; 0.228 sec/batch; 17h:19m:27s remains)
INFO - root - 2017-12-08 08:37:36.479191: step 59480, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 16h:39m:54s remains)
INFO - root - 2017-12-08 08:37:38.749155: step 59490, loss = 2.28, batch loss = 2.23 (35.2 examples/sec; 0.227 sec/batch; 17h:13m:26s remains)
INFO - root - 2017-12-08 08:37:41.016236: step 59500, loss = 2.28, batch loss = 2.23 (37.0 examples/sec; 0.216 sec/batch; 16h:22m:32s remains)
2017-12-08 08:37:41.299221: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289813 -4.4289579 -4.4289141 -4.4288831 -4.4288726 -4.4288721 -4.4288635 -4.428853 -4.4288583 -4.4288793 -4.428894 -4.4288888 -4.4288597 -4.4288235 -4.4288239][-4.4289713 -4.4289365 -4.4288745 -4.4288273 -4.4288154 -4.4288244 -4.4288263 -4.4288268 -4.4288421 -4.4288745 -4.428895 -4.4288907 -4.4288669 -4.4288287 -4.42881][-4.428956 -4.4289131 -4.4288344 -4.4287658 -4.4287381 -4.4287424 -4.428751 -4.4287672 -4.4288015 -4.4288511 -4.4288816 -4.4288788 -4.4288645 -4.4288378 -4.4288096][-4.4289541 -4.4289093 -4.4288206 -4.4287333 -4.4286776 -4.4286642 -4.428678 -4.4287162 -4.4287705 -4.4288321 -4.4288731 -4.4288793 -4.4288735 -4.4288616 -4.4288383][-4.4289608 -4.428916 -4.4288335 -4.4287467 -4.4286733 -4.4286408 -4.4286556 -4.4286995 -4.4287524 -4.4288068 -4.4288507 -4.4288692 -4.4288721 -4.4288726 -4.428864][-4.4289675 -4.428926 -4.4288578 -4.4287887 -4.4287195 -4.4286752 -4.428678 -4.4287114 -4.42875 -4.4287839 -4.4288173 -4.428834 -4.4288349 -4.4288363 -4.42884][-4.428968 -4.4289274 -4.4288712 -4.4288197 -4.4287658 -4.42872 -4.4287095 -4.4287291 -4.4287519 -4.4287724 -4.4287963 -4.4288116 -4.4288049 -4.4287977 -4.4288087][-4.4289589 -4.4289131 -4.4288554 -4.4288058 -4.4287639 -4.4287262 -4.4287052 -4.4287105 -4.4287229 -4.428741 -4.4287677 -4.4287825 -4.4287763 -4.4287744 -4.4287906][-4.4289441 -4.4288931 -4.42883 -4.4287782 -4.4287405 -4.4287066 -4.4286771 -4.4286709 -4.4286871 -4.4287162 -4.4287491 -4.4287663 -4.42877 -4.4287724 -4.4287868][-4.4289308 -4.4288769 -4.4288111 -4.4287548 -4.4287181 -4.4286847 -4.4286513 -4.4286432 -4.4286685 -4.4287086 -4.4287467 -4.4287705 -4.4287863 -4.4288006 -4.4288087][-4.428915 -4.4288611 -4.4287939 -4.4287357 -4.4286995 -4.42867 -4.4286423 -4.42864 -4.4286752 -4.4287229 -4.4287624 -4.42878 -4.4287944 -4.4288077 -4.4288168][-4.4288993 -4.4288492 -4.4287858 -4.4287343 -4.4287076 -4.4286919 -4.4286766 -4.4286761 -4.4287057 -4.4287443 -4.4287772 -4.4287863 -4.4287968 -4.4288096 -4.4288192][-4.4288964 -4.4288549 -4.4288034 -4.4287667 -4.4287553 -4.4287505 -4.4287438 -4.42874 -4.4287534 -4.4287744 -4.4287987 -4.428802 -4.4288058 -4.4288149 -4.4288235][-4.4289103 -4.4288774 -4.4288392 -4.4288135 -4.4288063 -4.4288058 -4.4288015 -4.4288015 -4.4288106 -4.4288206 -4.4288292 -4.4288254 -4.428822 -4.4288249 -4.4288292][-4.4289355 -4.4289069 -4.4288783 -4.4288597 -4.4288526 -4.4288511 -4.4288511 -4.4288588 -4.4288678 -4.4288683 -4.4288645 -4.4288559 -4.4288473 -4.428844 -4.4288454]]...]
INFO - root - 2017-12-08 08:37:43.509469: step 59510, loss = 2.28, batch loss = 2.23 (34.6 examples/sec; 0.231 sec/batch; 17h:30m:32s remains)
INFO - root - 2017-12-08 08:37:45.775029: step 59520, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 16h:39m:43s remains)
INFO - root - 2017-12-08 08:37:48.005773: step 59530, loss = 2.28, batch loss = 2.23 (33.4 examples/sec; 0.239 sec/batch; 18h:09m:32s remains)
INFO - root - 2017-12-08 08:37:50.244632: step 59540, loss = 2.28, batch loss = 2.23 (35.6 examples/sec; 0.225 sec/batch; 17h:01m:37s remains)
INFO - root - 2017-12-08 08:37:52.506657: step 59550, loss = 2.28, batch loss = 2.23 (34.8 examples/sec; 0.230 sec/batch; 17h:26m:32s remains)
INFO - root - 2017-12-08 08:37:54.774841: step 59560, loss = 2.28, batch loss = 2.23 (34.2 examples/sec; 0.234 sec/batch; 17h:43m:05s remains)
INFO - root - 2017-12-08 08:37:57.025381: step 59570, loss = 2.28, batch loss = 2.23 (33.3 examples/sec; 0.240 sec/batch; 18h:11m:42s remains)
INFO - root - 2017-12-08 08:37:59.287576: step 59580, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 16h:39m:31s remains)
INFO - root - 2017-12-08 08:38:01.509174: step 59590, loss = 2.28, batch loss = 2.23 (36.9 examples/sec; 0.217 sec/batch; 16h:25m:17s remains)
INFO - root - 2017-12-08 08:38:03.751316: step 59600, loss = 2.28, batch loss = 2.23 (35.1 examples/sec; 0.228 sec/batch; 17h:15m:14s remains)
2017-12-08 08:38:04.068704: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289412 -4.4289365 -4.4289289 -4.4289188 -4.4289145 -4.4289222 -4.4289279 -4.428926 -4.4289265 -4.4289274 -4.4289179 -4.428906 -4.4288969 -4.4289007 -4.428915][-4.4289126 -4.4288983 -4.4288859 -4.4288716 -4.4288683 -4.4288821 -4.4288955 -4.4288955 -4.4289012 -4.4289045 -4.4288878 -4.4288683 -4.4288535 -4.4288568 -4.4288754][-4.4288692 -4.4288421 -4.4288225 -4.4288082 -4.4288106 -4.4288349 -4.4288554 -4.4288554 -4.4288611 -4.4288611 -4.4288363 -4.4288154 -4.4288082 -4.4288216 -4.428843][-4.4288063 -4.4287653 -4.4287481 -4.428751 -4.4287705 -4.4288006 -4.4288082 -4.4287944 -4.428802 -4.4288077 -4.4287753 -4.4287539 -4.4287634 -4.4287992 -4.4288268][-4.428731 -4.4286785 -4.428659 -4.4286814 -4.4287205 -4.4287419 -4.4287238 -4.428688 -4.428699 -4.4287229 -4.4287 -4.4286857 -4.4287157 -4.4287734 -4.4288144][-4.4286661 -4.4285989 -4.428575 -4.4286046 -4.4286427 -4.428637 -4.4285641 -4.4284873 -4.4285207 -4.428597 -4.4286189 -4.4286284 -4.428679 -4.42875 -4.4288025][-4.4286642 -4.4285946 -4.4285593 -4.4285703 -4.4285755 -4.4285169 -4.428359 -4.4282 -4.4282656 -4.4284239 -4.4285178 -4.4285669 -4.4286408 -4.4287286 -4.428792][-4.42875 -4.4286995 -4.428647 -4.4286184 -4.428586 -4.4284968 -4.4283032 -4.428102 -4.4281883 -4.4283862 -4.4285083 -4.4285746 -4.4286518 -4.4287324 -4.428792][-4.4288445 -4.4288278 -4.428782 -4.4287457 -4.4287052 -4.4286294 -4.4284873 -4.4283519 -4.4284163 -4.4285579 -4.4286394 -4.428679 -4.4287238 -4.4287734 -4.4288111][-4.4288859 -4.4288926 -4.42886 -4.4288287 -4.428793 -4.4287362 -4.4286513 -4.4285836 -4.4286313 -4.4287138 -4.4287558 -4.4287715 -4.4287925 -4.4288177 -4.4288354][-4.4289107 -4.4289188 -4.4288926 -4.4288621 -4.4288321 -4.4287953 -4.4287453 -4.4287128 -4.4287486 -4.428792 -4.4288096 -4.4288149 -4.4288278 -4.4288478 -4.4288578][-4.4289489 -4.4289489 -4.428925 -4.4289017 -4.4288783 -4.4288573 -4.4288244 -4.4288054 -4.4288306 -4.428853 -4.4288607 -4.4288588 -4.4288645 -4.4288807 -4.4288883][-4.4289727 -4.4289732 -4.4289503 -4.4289303 -4.428915 -4.4289026 -4.4288783 -4.4288621 -4.4288745 -4.4288893 -4.4289 -4.4289021 -4.4289064 -4.4289169 -4.4289203][-4.4289742 -4.4289722 -4.4289522 -4.4289331 -4.4289207 -4.4289141 -4.4288988 -4.4288878 -4.428895 -4.4289069 -4.4289217 -4.4289312 -4.4289389 -4.4289455 -4.428947][-4.4289546 -4.4289503 -4.4289365 -4.428925 -4.4289193 -4.4289169 -4.4289103 -4.428905 -4.42891 -4.42892 -4.4289327 -4.4289451 -4.4289556 -4.4289608 -4.4289613]]...]
INFO - root - 2017-12-08 08:38:06.307894: step 59610, loss = 2.28, batch loss = 2.23 (34.4 examples/sec; 0.233 sec/batch; 17h:37m:28s remains)
INFO - root - 2017-12-08 08:38:08.557972: step 59620, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.221 sec/batch; 16h:47m:12s remains)
INFO - root - 2017-12-08 08:38:10.810533: step 59630, loss = 2.28, batch loss = 2.23 (35.3 examples/sec; 0.227 sec/batch; 17h:11m:22s remains)
INFO - root - 2017-12-08 08:38:13.075957: step 59640, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 16h:37m:30s remains)
INFO - root - 2017-12-08 08:38:15.297692: step 59650, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 16h:52m:42s remains)
INFO - root - 2017-12-08 08:38:17.555430: step 59660, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 16h:52m:24s remains)
INFO - root - 2017-12-08 08:38:19.825083: step 59670, loss = 2.28, batch loss = 2.23 (35.7 examples/sec; 0.224 sec/batch; 16h:58m:21s remains)
INFO - root - 2017-12-08 08:38:22.061741: step 59680, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 16h:39m:44s remains)
INFO - root - 2017-12-08 08:38:24.299020: step 59690, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.221 sec/batch; 16h:46m:30s remains)
INFO - root - 2017-12-08 08:38:26.516248: step 59700, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.222 sec/batch; 16h:47m:43s remains)
2017-12-08 08:38:26.802634: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.428844 -4.4288087 -4.4287639 -4.4287186 -4.4286966 -4.4287071 -4.4287343 -4.4287605 -4.4287624 -4.4287195 -4.42867 -4.4286494 -4.4286642 -4.4286876 -4.4286923][-4.4287949 -4.4287615 -4.4287314 -4.4286885 -4.428658 -4.4286528 -4.4286771 -4.4287267 -4.4287634 -4.4287338 -4.428679 -4.428647 -4.42864 -4.4286442 -4.4286413][-4.4287462 -4.4287186 -4.4287038 -4.42866 -4.4286065 -4.4285707 -4.42859 -4.4286675 -4.4287434 -4.42874 -4.42869 -4.4286494 -4.4286203 -4.4286032 -4.4285989][-4.4287171 -4.4286995 -4.4286952 -4.4286427 -4.4285541 -4.4284887 -4.4285107 -4.4286118 -4.4287176 -4.4287472 -4.4287138 -4.4286637 -4.4286084 -4.4285674 -4.4285636][-4.4287171 -4.4287119 -4.4287062 -4.4286375 -4.4285121 -4.4284215 -4.4284558 -4.428575 -4.4286938 -4.4287505 -4.4287391 -4.4286838 -4.4286041 -4.4285426 -4.42855][-4.4287348 -4.4287276 -4.4287105 -4.4286203 -4.4284611 -4.4283457 -4.4283915 -4.4285312 -4.42866 -4.4287324 -4.4287391 -4.42869 -4.428596 -4.4285169 -4.4285421][-4.428761 -4.4287386 -4.4287038 -4.4285955 -4.4284134 -4.4282808 -4.4283276 -4.4284768 -4.4286156 -4.4286981 -4.4287229 -4.4286847 -4.4285817 -4.428493 -4.4285388][-4.428812 -4.4287753 -4.4287252 -4.4286089 -4.4284205 -4.4282784 -4.428298 -4.4284291 -4.4285703 -4.4286628 -4.4286928 -4.4286566 -4.4285607 -4.4284992 -4.4285736][-4.4288697 -4.4288325 -4.428782 -4.4286747 -4.4284983 -4.4283543 -4.4283304 -4.4284244 -4.4285531 -4.4286385 -4.4286518 -4.428617 -4.428555 -4.428546 -4.4286442][-4.428906 -4.42888 -4.4288464 -4.4287663 -4.4286194 -4.4284782 -4.4284282 -4.4284887 -4.4285865 -4.428638 -4.4286218 -4.4285932 -4.4285803 -4.4286213 -4.4287229][-4.4288964 -4.4288864 -4.4288769 -4.4288321 -4.4287224 -4.4285975 -4.428544 -4.4285831 -4.4286432 -4.4286575 -4.4286218 -4.4286032 -4.4286218 -4.4286857 -4.4287796][-4.4288688 -4.4288659 -4.4288774 -4.4288678 -4.4287968 -4.4287024 -4.4286571 -4.42868 -4.4287124 -4.4286981 -4.4286542 -4.4286442 -4.4286695 -4.428731 -4.4288163][-4.4288363 -4.4288344 -4.4288568 -4.4288783 -4.428843 -4.4287772 -4.4287424 -4.4287553 -4.428772 -4.4287457 -4.4286981 -4.42869 -4.428709 -4.4287543 -4.4288235][-4.4288015 -4.4287992 -4.428823 -4.4288592 -4.4288535 -4.4288144 -4.42879 -4.4287963 -4.4288034 -4.42878 -4.428741 -4.4287333 -4.4287467 -4.4287763 -4.4288235][-4.4287796 -4.4287772 -4.4287953 -4.4288273 -4.4288383 -4.4288211 -4.4288058 -4.4288092 -4.4288163 -4.428802 -4.4287758 -4.4287658 -4.4287729 -4.428791 -4.4288192]]...]
INFO - root - 2017-12-08 08:38:29.059736: step 59710, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.220 sec/batch; 16h:41m:56s remains)
INFO - root - 2017-12-08 08:38:31.296219: step 59720, loss = 2.28, batch loss = 2.23 (35.8 examples/sec; 0.223 sec/batch; 16h:55m:34s remains)
INFO - root - 2017-12-08 08:38:33.530261: step 59730, loss = 2.28, batch loss = 2.23 (35.3 examples/sec; 0.227 sec/batch; 17h:09m:44s remains)
INFO - root - 2017-12-08 08:38:35.748935: step 59740, loss = 2.28, batch loss = 2.23 (34.5 examples/sec; 0.232 sec/batch; 17h:33m:43s remains)
INFO - root - 2017-12-08 08:38:37.963243: step 59750, loss = 2.28, batch loss = 2.23 (37.7 examples/sec; 0.212 sec/batch; 16h:05m:43s remains)
INFO - root - 2017-12-08 08:38:40.219499: step 59760, loss = 2.28, batch loss = 2.23 (35.7 examples/sec; 0.224 sec/batch; 16h:58m:39s remains)
INFO - root - 2017-12-08 08:38:42.447088: step 59770, loss = 2.28, batch loss = 2.23 (35.2 examples/sec; 0.227 sec/batch; 17h:12m:13s remains)
INFO - root - 2017-12-08 08:38:44.676413: step 59780, loss = 2.28, batch loss = 2.23 (35.4 examples/sec; 0.226 sec/batch; 17h:08m:01s remains)
INFO - root - 2017-12-08 08:38:46.891374: step 59790, loss = 2.28, batch loss = 2.23 (36.0 examples/sec; 0.222 sec/batch; 16h:49m:50s remains)
INFO - root - 2017-12-08 08:38:49.124346: step 59800, loss = 2.28, batch loss = 2.23 (37.5 examples/sec; 0.213 sec/batch; 16h:10m:10s remains)
2017-12-08 08:38:49.402068: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4287639 -4.4287705 -4.4287624 -4.4287224 -4.4286952 -4.4287143 -4.4287314 -4.4287381 -4.4287562 -4.428772 -4.4287844 -4.4287939 -4.4287825 -4.4287643 -4.4287767][-4.4287863 -4.4287739 -4.4287477 -4.4286942 -4.4286594 -4.4286828 -4.4287143 -4.4287333 -4.4287596 -4.4287777 -4.428793 -4.4288006 -4.4287844 -4.4287567 -4.4287648][-4.4287772 -4.4287457 -4.4287047 -4.42865 -4.4286118 -4.4286246 -4.4286666 -4.4286985 -4.4287357 -4.4287624 -4.4287772 -4.4287767 -4.4287567 -4.4287233 -4.4287362][-4.4287438 -4.4287062 -4.4286652 -4.428618 -4.4285784 -4.4285779 -4.428606 -4.4286346 -4.4286761 -4.42871 -4.4287291 -4.4287195 -4.4286938 -4.428659 -4.428679][-4.4287157 -4.428679 -4.4286418 -4.4286141 -4.428575 -4.4285502 -4.4285293 -4.428524 -4.4285679 -4.4286165 -4.4286528 -4.428659 -4.4286432 -4.428627 -4.4286551][-4.4287081 -4.4286714 -4.4286404 -4.4286156 -4.42856 -4.4284892 -4.4283867 -4.4283276 -4.4283977 -4.4284945 -4.4285727 -4.4286079 -4.4286208 -4.4286342 -4.4286647][-4.4286957 -4.4286504 -4.4286218 -4.4285917 -4.4285121 -4.4283752 -4.4281454 -4.4280105 -4.4281545 -4.4283471 -4.4284868 -4.4285612 -4.4286084 -4.4286404 -4.4286642][-4.4286718 -4.4286242 -4.4286084 -4.4285836 -4.4284911 -4.4283357 -4.4280548 -4.4278574 -4.4280429 -4.428299 -4.4284744 -4.4285583 -4.4286127 -4.4286442 -4.4286628][-4.4286728 -4.4286432 -4.4286461 -4.4286332 -4.4285717 -4.4284763 -4.4282889 -4.4281445 -4.428247 -4.4284329 -4.4285617 -4.4286089 -4.4286423 -4.4286675 -4.4286919][-4.428731 -4.4287214 -4.4287295 -4.4287143 -4.4286723 -4.4286385 -4.4285531 -4.4284778 -4.4285111 -4.4286056 -4.4286704 -4.428688 -4.4286995 -4.4287195 -4.4287534][-4.4288015 -4.4288015 -4.4288073 -4.4287887 -4.4287529 -4.4287391 -4.4287047 -4.4286728 -4.4286828 -4.4287238 -4.4287462 -4.4287548 -4.4287596 -4.4287753 -4.4288163][-4.4288645 -4.4288616 -4.4288535 -4.4288282 -4.4287915 -4.42879 -4.4287848 -4.42877 -4.428782 -4.4288054 -4.4288063 -4.4288068 -4.4288087 -4.4288144 -4.4288597][-4.4289074 -4.428905 -4.4288979 -4.4288678 -4.4288268 -4.4288349 -4.4288464 -4.4288373 -4.4288473 -4.4288664 -4.4288659 -4.4288597 -4.4288535 -4.428853 -4.4288926][-4.4289331 -4.4289417 -4.4289355 -4.428906 -4.4288783 -4.428896 -4.4289117 -4.4289088 -4.4289141 -4.4289269 -4.4289289 -4.42892 -4.4289079 -4.428905 -4.4289312][-4.4289412 -4.4289656 -4.4289665 -4.4289484 -4.4289351 -4.428947 -4.4289541 -4.4289532 -4.4289522 -4.4289513 -4.4289494 -4.4289389 -4.42893 -4.4289265 -4.4289441]]...]
INFO - root - 2017-12-08 08:38:51.656944: step 59810, loss = 2.28, batch loss = 2.23 (33.5 examples/sec; 0.239 sec/batch; 18h:04m:46s remains)
INFO - root - 2017-12-08 08:38:53.877751: step 59820, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 16h:38m:30s remains)
INFO - root - 2017-12-08 08:38:56.115667: step 59830, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 16h:43m:28s remains)
INFO - root - 2017-12-08 08:38:58.340863: step 59840, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 16h:38m:40s remains)
INFO - root - 2017-12-08 08:39:00.561979: step 59850, loss = 2.28, batch loss = 2.23 (36.7 examples/sec; 0.218 sec/batch; 16h:30m:02s remains)
INFO - root - 2017-12-08 08:39:02.808152: step 59860, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 16h:39m:41s remains)
INFO - root - 2017-12-08 08:39:05.036207: step 59870, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 16h:43m:44s remains)
INFO - root - 2017-12-08 08:39:07.259520: step 59880, loss = 2.28, batch loss = 2.23 (35.0 examples/sec; 0.229 sec/batch; 17h:18m:28s remains)
INFO - root - 2017-12-08 08:39:09.536482: step 59890, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.225 sec/batch; 17h:03m:51s remains)
INFO - root - 2017-12-08 08:39:11.791520: step 59900, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 16h:35m:56s remains)
2017-12-08 08:39:12.092633: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.428905 -4.428875 -4.4288487 -4.428844 -4.4288583 -4.4288726 -4.4288859 -4.4289064 -4.4289274 -4.4289336 -4.4289255 -4.4289041 -4.4288807 -4.4288635 -4.4288492][-4.4289064 -4.4288697 -4.4288349 -4.4288211 -4.4288273 -4.4288311 -4.4288349 -4.4288464 -4.4288611 -4.4288688 -4.4288673 -4.4288535 -4.428834 -4.4288239 -4.42882][-4.428946 -4.4289117 -4.4288673 -4.4288349 -4.4288163 -4.4287982 -4.4287834 -4.4287806 -4.4287906 -4.4288058 -4.4288154 -4.42881 -4.428793 -4.4287877 -4.4287939][-4.4289989 -4.4289756 -4.4289227 -4.4288664 -4.4288168 -4.4287677 -4.4287238 -4.4287004 -4.4287109 -4.4287424 -4.4287691 -4.4287772 -4.4287663 -4.4287634 -4.4287767][-4.4290347 -4.4290247 -4.4289708 -4.4288974 -4.4288187 -4.4287319 -4.42865 -4.428597 -4.4286056 -4.428659 -4.4287086 -4.4287372 -4.428741 -4.4287453 -4.4287658][-4.4290452 -4.4290438 -4.4289894 -4.4289031 -4.4287972 -4.4286704 -4.4285479 -4.4284687 -4.4284744 -4.4285502 -4.4286289 -4.4286871 -4.4287157 -4.4287386 -4.4287696][-4.4290347 -4.4290304 -4.4289722 -4.4288783 -4.4287553 -4.428596 -4.4284325 -4.4283218 -4.4283223 -4.4284215 -4.428534 -4.4286284 -4.4286947 -4.4287467 -4.4287925][-4.4290209 -4.4290104 -4.4289479 -4.4288507 -4.42872 -4.4285388 -4.4283352 -4.4281878 -4.42818 -4.4283 -4.4284453 -4.4285746 -4.4286742 -4.4287462 -4.428802][-4.4290137 -4.4289989 -4.428937 -4.4288449 -4.4287257 -4.4285507 -4.4283323 -4.4281554 -4.4281273 -4.4282427 -4.428401 -4.4285464 -4.4286633 -4.4287462 -4.4288073][-4.4290004 -4.4289808 -4.4289308 -4.4288597 -4.4287734 -4.4286413 -4.4284587 -4.4282947 -4.4282427 -4.4283171 -4.4284463 -4.428576 -4.4286842 -4.4287615 -4.4288177][-4.4289784 -4.4289551 -4.4289184 -4.4288778 -4.4288349 -4.4287581 -4.4286318 -4.4285121 -4.4284587 -4.4284897 -4.42857 -4.4286633 -4.4287434 -4.4287996 -4.4288397][-4.42896 -4.4289312 -4.4288979 -4.42888 -4.4288735 -4.4288464 -4.4287739 -4.4287019 -4.4286656 -4.4286728 -4.4287109 -4.4287639 -4.428812 -4.4288406 -4.4288588][-4.4289613 -4.4289241 -4.4288836 -4.4288697 -4.4288821 -4.4288864 -4.4288549 -4.42882 -4.4288039 -4.428803 -4.4288163 -4.42884 -4.42886 -4.4288664 -4.4288673][-4.4289813 -4.4289422 -4.428895 -4.4288735 -4.4288888 -4.4289079 -4.4289012 -4.4288893 -4.4288826 -4.4288759 -4.4288735 -4.4288778 -4.4288769 -4.4288669 -4.4288568][-4.4289927 -4.42896 -4.4289141 -4.4288864 -4.4288983 -4.42892 -4.42892 -4.428916 -4.4289126 -4.4289045 -4.4288926 -4.4288812 -4.4288645 -4.42884 -4.4288211]]...]
INFO - root - 2017-12-08 08:39:14.296477: step 59910, loss = 2.28, batch loss = 2.23 (37.2 examples/sec; 0.215 sec/batch; 16h:16m:12s remains)
INFO - root - 2017-12-08 08:39:16.546013: step 59920, loss = 2.28, batch loss = 2.23 (36.6 examples/sec; 0.219 sec/batch; 16h:33m:55s remains)
INFO - root - 2017-12-08 08:39:18.801954: step 59930, loss = 2.28, batch loss = 2.23 (35.3 examples/sec; 0.227 sec/batch; 17h:10m:02s remains)
INFO - root - 2017-12-08 08:39:21.036224: step 59940, loss = 2.28, batch loss = 2.23 (36.8 examples/sec; 0.217 sec/batch; 16h:26m:17s remains)
INFO - root - 2017-12-08 08:39:23.320442: step 59950, loss = 2.28, batch loss = 2.23 (31.9 examples/sec; 0.251 sec/batch; 18h:59m:35s remains)
INFO - root - 2017-12-08 08:39:25.558651: step 59960, loss = 2.28, batch loss = 2.23 (36.8 examples/sec; 0.217 sec/batch; 16h:27m:43s remains)
INFO - root - 2017-12-08 08:39:27.786053: step 59970, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 16h:37m:34s remains)
INFO - root - 2017-12-08 08:39:30.022575: step 59980, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.220 sec/batch; 16h:40m:39s remains)
INFO - root - 2017-12-08 08:39:32.242156: step 59990, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.220 sec/batch; 16h:41m:17s remains)
INFO - root - 2017-12-08 08:39:34.454303: step 60000, loss = 2.28, batch loss = 2.23 (37.1 examples/sec; 0.216 sec/batch; 16h:19m:13s remains)
2017-12-08 08:39:34.767528: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289241 -4.4289284 -4.4289289 -4.4289284 -4.4289274 -4.428925 -4.4289203 -4.4289179 -4.4289155 -4.428915 -4.4289093 -4.4289045 -4.428905 -4.4289055 -4.4289069][-4.4289107 -4.428916 -4.4289155 -4.4289155 -4.4289126 -4.4289031 -4.4288893 -4.4288845 -4.4288845 -4.428885 -4.4288764 -4.4288726 -4.42888 -4.4288855 -4.4288859][-4.4289055 -4.4289083 -4.4289041 -4.4289055 -4.4288988 -4.4288793 -4.4288568 -4.4288521 -4.4288554 -4.4288611 -4.4288535 -4.4288511 -4.4288626 -4.4288721 -4.4288754][-4.4288993 -4.4288955 -4.4288874 -4.42889 -4.42888 -4.4288511 -4.4288249 -4.4288211 -4.4288273 -4.42884 -4.4288373 -4.4288354 -4.4288473 -4.4288626 -4.428875][-4.4288836 -4.4288726 -4.4288626 -4.4288669 -4.4288454 -4.4288063 -4.4287868 -4.4287887 -4.4287963 -4.4288173 -4.4288216 -4.4288211 -4.4288349 -4.4288511 -4.4288707][-4.4288597 -4.4288478 -4.428834 -4.4288239 -4.4287786 -4.4287343 -4.4287324 -4.4287372 -4.4287419 -4.4287739 -4.4287853 -4.4287858 -4.4288044 -4.4288235 -4.4288473][-4.4288306 -4.4288073 -4.4287834 -4.428751 -4.4286795 -4.4286408 -4.4286628 -4.4286656 -4.4286633 -4.4287081 -4.4287343 -4.4287391 -4.42876 -4.4287868 -4.4288177][-4.4288063 -4.4287658 -4.4287267 -4.4286723 -4.4285913 -4.428565 -4.4286013 -4.42859 -4.4285679 -4.4286113 -4.4286513 -4.4286685 -4.4287009 -4.4287429 -4.428782][-4.4287934 -4.4287367 -4.4286866 -4.4286246 -4.428546 -4.4285288 -4.4285555 -4.4285212 -4.4284778 -4.4285164 -4.4285712 -4.42861 -4.428659 -4.4287157 -4.4287572][-4.4287925 -4.4287353 -4.4286895 -4.4286394 -4.4285765 -4.42856 -4.4285574 -4.4285049 -4.4284558 -4.4284916 -4.4285631 -4.428618 -4.4286723 -4.4287286 -4.4287643][-4.4288039 -4.4287643 -4.4287348 -4.4287033 -4.4286604 -4.4286394 -4.4286151 -4.4285583 -4.4285069 -4.428534 -4.4286094 -4.4286718 -4.428721 -4.4287667 -4.4287906][-4.4288111 -4.4287858 -4.4287634 -4.428741 -4.4287076 -4.4286833 -4.4286537 -4.4285975 -4.4285436 -4.4285626 -4.4286366 -4.4287038 -4.4287467 -4.4287834 -4.4287996][-4.4288197 -4.4287982 -4.4287763 -4.4287543 -4.4287267 -4.4287062 -4.4286847 -4.4286375 -4.4285879 -4.4286008 -4.42867 -4.4287333 -4.4287705 -4.4287972 -4.4288039][-4.4288325 -4.4288087 -4.4287882 -4.4287658 -4.4287457 -4.4287376 -4.4287329 -4.4286962 -4.4286518 -4.4286628 -4.4287229 -4.4287782 -4.4288092 -4.4288239 -4.4288216][-4.4288406 -4.4288211 -4.4288054 -4.4287872 -4.4287724 -4.4287729 -4.42878 -4.4287553 -4.42872 -4.4287319 -4.42878 -4.4288239 -4.4288473 -4.4288521 -4.4288397]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-momentum-lr0.001-defgroup-1/model.ckpt-60000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-momentum-lr0.001-defgroup-1/model.ckpt-60000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-08 08:39:37.348042: step 60010, loss = 2.28, batch loss = 2.23 (35.6 examples/sec; 0.225 sec/batch; 17h:01m:25s remains)
INFO - root - 2017-12-08 08:39:39.601457: step 60020, loss = 2.28, batch loss = 2.23 (36.0 examples/sec; 0.222 sec/batch; 16h:50m:14s remains)
INFO - root - 2017-12-08 08:39:41.847234: step 60030, loss = 2.28, batch loss = 2.23 (36.0 examples/sec; 0.222 sec/batch; 16h:50m:10s remains)
INFO - root - 2017-12-08 08:39:44.080021: step 60040, loss = 2.28, batch loss = 2.23 (34.6 examples/sec; 0.231 sec/batch; 17h:30m:08s remains)
INFO - root - 2017-12-08 08:39:46.327824: step 60050, loss = 2.28, batch loss = 2.23 (37.2 examples/sec; 0.215 sec/batch; 16h:17m:28s remains)
INFO - root - 2017-12-08 08:39:48.577669: step 60060, loss = 2.28, batch loss = 2.23 (36.9 examples/sec; 0.217 sec/batch; 16h:24m:45s remains)
INFO - root - 2017-12-08 08:39:50.848912: step 60070, loss = 2.28, batch loss = 2.23 (34.9 examples/sec; 0.229 sec/batch; 17h:20m:54s remains)
INFO - root - 2017-12-08 08:39:53.156650: step 60080, loss = 2.28, batch loss = 2.23 (35.8 examples/sec; 0.223 sec/batch; 16h:54m:30s remains)
INFO - root - 2017-12-08 08:39:55.420899: step 60090, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.225 sec/batch; 17h:03m:37s remains)
INFO - root - 2017-12-08 08:39:57.665343: step 60100, loss = 2.28, batch loss = 2.23 (35.7 examples/sec; 0.224 sec/batch; 16h:57m:10s remains)
2017-12-08 08:39:58.004463: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4286942 -4.4287052 -4.4286852 -4.4286213 -4.4285712 -4.428576 -4.4286494 -4.4287252 -4.42878 -4.4288096 -4.4288177 -4.4288144 -4.4288077 -4.4288034 -4.428812][-4.4287095 -4.4287124 -4.428688 -4.4286227 -4.4285693 -4.4285731 -4.4286642 -4.4287491 -4.4287944 -4.4288068 -4.4287996 -4.4287963 -4.4287944 -4.4287891 -4.4288][-4.4287138 -4.4287009 -4.4286671 -4.4286013 -4.4285407 -4.4285388 -4.42865 -4.4287429 -4.4287777 -4.4287767 -4.4287624 -4.4287643 -4.4287744 -4.4287782 -4.4287977][-4.42871 -4.4286947 -4.4286528 -4.4285789 -4.4285007 -4.42849 -4.4286165 -4.4287162 -4.4287372 -4.4287229 -4.4287014 -4.4287095 -4.4287333 -4.4287496 -4.4287796][-4.4287095 -4.4287057 -4.4286566 -4.4285655 -4.428453 -4.4284134 -4.4285378 -4.4286447 -4.4286718 -4.4286518 -4.4286304 -4.4286542 -4.4286966 -4.4287248 -4.4287615][-4.4287071 -4.4287081 -4.4286513 -4.42853 -4.4283562 -4.4282618 -4.4283876 -4.4285212 -4.4285812 -4.4285755 -4.4285588 -4.4285979 -4.428658 -4.4287028 -4.4287491][-4.4286995 -4.4286962 -4.4286256 -4.42848 -4.4282484 -4.428102 -4.4282374 -4.4284096 -4.4285121 -4.4285364 -4.4285278 -4.4285736 -4.4286418 -4.4286947 -4.4287448][-4.4286819 -4.4286814 -4.428618 -4.428483 -4.4282656 -4.4281368 -4.4282761 -4.4284444 -4.428545 -4.428566 -4.4285541 -4.4285917 -4.4286485 -4.4286947 -4.4287415][-4.4287071 -4.4287181 -4.4286842 -4.4285908 -4.4284282 -4.4283338 -4.4284348 -4.4285517 -4.4286165 -4.4286127 -4.4285955 -4.4286222 -4.4286647 -4.4287047 -4.428751][-4.4287529 -4.4287615 -4.4287481 -4.4286914 -4.4285674 -4.4284878 -4.4285493 -4.4286275 -4.4286728 -4.4286623 -4.4286451 -4.42866 -4.4286928 -4.4287295 -4.4287777][-4.4287992 -4.428802 -4.4287982 -4.428762 -4.4286642 -4.4285979 -4.428638 -4.4286995 -4.4287434 -4.42874 -4.4287257 -4.4287324 -4.4287553 -4.4287877 -4.428833][-4.4288449 -4.4288464 -4.4288464 -4.4288239 -4.4287586 -4.4287157 -4.4287457 -4.4287953 -4.428833 -4.4288335 -4.4288168 -4.4288139 -4.4288273 -4.4288549 -4.428896][-4.4288836 -4.4288893 -4.4288964 -4.4288874 -4.4288507 -4.4288273 -4.42885 -4.4288878 -4.4289155 -4.4289145 -4.4289002 -4.428896 -4.4289031 -4.4289222 -4.4289503][-4.4289279 -4.4289322 -4.4289389 -4.4289336 -4.4289145 -4.4289031 -4.4289193 -4.4289446 -4.4289637 -4.4289637 -4.428957 -4.4289551 -4.428957 -4.428966 -4.4289808][-4.4289794 -4.4289761 -4.4289742 -4.428968 -4.4289589 -4.4289551 -4.4289656 -4.4289818 -4.4289942 -4.4289975 -4.4289985 -4.4289985 -4.4289975 -4.4289994 -4.4290056]]...]
INFO - root - 2017-12-08 08:40:00.268575: step 60110, loss = 2.28, batch loss = 2.23 (33.6 examples/sec; 0.238 sec/batch; 18h:02m:13s remains)
INFO - root - 2017-12-08 08:40:02.516120: step 60120, loss = 2.28, batch loss = 2.23 (36.6 examples/sec; 0.219 sec/batch; 16h:33m:02s remains)
INFO - root - 2017-12-08 08:40:04.763546: step 60130, loss = 2.28, batch loss = 2.23 (35.1 examples/sec; 0.228 sec/batch; 17h:13m:33s remains)
INFO - root - 2017-12-08 08:40:07.020002: step 60140, loss = 2.28, batch loss = 2.23 (35.8 examples/sec; 0.223 sec/batch; 16h:54m:06s remains)
INFO - root - 2017-12-08 08:40:09.272339: step 60150, loss = 2.28, batch loss = 2.23 (33.4 examples/sec; 0.239 sec/batch; 18h:06m:10s remains)
INFO - root - 2017-12-08 08:40:11.527294: step 60160, loss = 2.28, batch loss = 2.23 (33.2 examples/sec; 0.241 sec/batch; 18h:13m:36s remains)
INFO - root - 2017-12-08 08:40:13.774361: step 60170, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.220 sec/batch; 16h:38m:57s remains)
INFO - root - 2017-12-08 08:40:16.009808: step 60180, loss = 2.28, batch loss = 2.23 (35.1 examples/sec; 0.228 sec/batch; 17h:14m:13s remains)
INFO - root - 2017-12-08 08:40:18.233756: step 60190, loss = 2.28, batch loss = 2.23 (35.1 examples/sec; 0.228 sec/batch; 17h:14m:19s remains)
INFO - root - 2017-12-08 08:40:20.473085: step 60200, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 16h:35m:23s remains)
2017-12-08 08:40:20.823986: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288859 -4.428895 -4.428905 -4.4289036 -4.4288864 -4.4288564 -4.4288468 -4.4288683 -4.4289026 -4.4289141 -4.4289041 -4.4288712 -4.428822 -4.4287825 -4.4287605][-4.4288287 -4.42886 -4.4288893 -4.4288883 -4.4288554 -4.4288082 -4.4287992 -4.4288478 -4.4289093 -4.42893 -4.4289122 -4.428865 -4.4288 -4.4287515 -4.4287205][-4.4287357 -4.428792 -4.4288392 -4.428834 -4.4287691 -4.4286866 -4.4286761 -4.4287629 -4.4288611 -4.4289 -4.4288821 -4.4288259 -4.42875 -4.4286914 -4.4286575][-4.4286718 -4.4287462 -4.4288 -4.4287777 -4.4286594 -4.4285221 -4.428515 -4.428647 -4.4287853 -4.4288468 -4.4288325 -4.4287682 -4.4286828 -4.428617 -4.4285908][-4.4286366 -4.4287171 -4.4287615 -4.4287105 -4.4285307 -4.42833 -4.4283233 -4.428503 -4.4286823 -4.4287791 -4.4287767 -4.4287076 -4.42861 -4.4285383 -4.42852][-4.4286346 -4.4287057 -4.4287229 -4.4286418 -4.4284067 -4.4281497 -4.42813 -4.4283466 -4.4285679 -4.4286938 -4.4287043 -4.4286304 -4.4285245 -4.4284515 -4.4284506][-4.42865 -4.4287014 -4.4286919 -4.4285889 -4.42834 -4.4280739 -4.4280438 -4.4282508 -4.4284873 -4.428627 -4.42864 -4.42856 -4.4284477 -4.4283795 -4.4284086][-4.4286613 -4.4286981 -4.4286823 -4.4285793 -4.4283628 -4.4281464 -4.4281206 -4.4282775 -4.4284825 -4.4286075 -4.4286046 -4.4285173 -4.4283938 -4.428328 -4.4283748][-4.4286661 -4.4286985 -4.4286957 -4.428616 -4.4284496 -4.428297 -4.4282722 -4.4283662 -4.4285126 -4.4286027 -4.4285879 -4.4285078 -4.4283805 -4.4283032 -4.4283495][-4.4286642 -4.4286981 -4.4287057 -4.4286489 -4.4285388 -4.4284472 -4.4284272 -4.4284678 -4.428556 -4.4286084 -4.4285965 -4.4285336 -4.4284167 -4.4283361 -4.4283705][-4.42869 -4.4287195 -4.4287286 -4.4286871 -4.4286189 -4.4285712 -4.4285522 -4.4285579 -4.4286013 -4.4286327 -4.4286356 -4.428596 -4.4284992 -4.4284225 -4.4284387][-4.4287443 -4.4287663 -4.4287691 -4.4287372 -4.4286962 -4.42867 -4.4286523 -4.4286437 -4.4286633 -4.4286823 -4.4286957 -4.4286823 -4.4286108 -4.428544 -4.428545][-4.4288125 -4.4288263 -4.4288225 -4.428803 -4.4287806 -4.4287643 -4.4287515 -4.4287405 -4.4287453 -4.4287553 -4.4287658 -4.428762 -4.4287181 -4.4286733 -4.4286737][-4.4288874 -4.4288955 -4.4288883 -4.4288769 -4.4288659 -4.4288578 -4.4288507 -4.4288435 -4.4288411 -4.4288435 -4.4288464 -4.4288445 -4.4288259 -4.4288077 -4.4288158][-4.4289541 -4.4289575 -4.42895 -4.4289403 -4.4289346 -4.42893 -4.4289241 -4.42892 -4.428916 -4.4289131 -4.4289093 -4.428906 -4.4289031 -4.4289083 -4.4289274]]...]
INFO - root - 2017-12-08 08:40:23.145676: step 60210, loss = 2.28, batch loss = 2.23 (35.0 examples/sec; 0.228 sec/batch; 17h:16m:00s remains)
INFO - root - 2017-12-08 08:40:25.392271: step 60220, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.222 sec/batch; 16h:45m:56s remains)
INFO - root - 2017-12-08 08:40:27.621746: step 60230, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.226 sec/batch; 17h:04m:01s remains)
INFO - root - 2017-12-08 08:40:29.900644: step 60240, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.221 sec/batch; 16h:44m:34s remains)
INFO - root - 2017-12-08 08:40:32.146896: step 60250, loss = 2.28, batch loss = 2.23 (35.8 examples/sec; 0.223 sec/batch; 16h:53m:05s remains)
INFO - root - 2017-12-08 08:40:34.406254: step 60260, loss = 2.28, batch loss = 2.23 (35.4 examples/sec; 0.226 sec/batch; 17h:06m:18s remains)
INFO - root - 2017-12-08 08:40:36.676595: step 60270, loss = 2.28, batch loss = 2.23 (35.7 examples/sec; 0.224 sec/batch; 16h:56m:50s remains)
INFO - root - 2017-12-08 08:40:38.930128: step 60280, loss = 2.28, batch loss = 2.23 (34.2 examples/sec; 0.234 sec/batch; 17h:41m:18s remains)
INFO - root - 2017-12-08 08:40:41.183896: step 60290, loss = 2.28, batch loss = 2.23 (35.7 examples/sec; 0.224 sec/batch; 16h:55m:19s remains)
INFO - root - 2017-12-08 08:40:43.424731: step 60300, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 16h:35m:40s remains)
2017-12-08 08:40:43.730844: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.42877 -4.4287996 -4.4288106 -4.4287939 -4.4288034 -4.4288373 -4.4288626 -4.4288659 -4.4288621 -4.4288549 -4.4288416 -4.4288263 -4.4288354 -4.42886 -4.4288826][-4.4287982 -4.4288225 -4.4288144 -4.4287682 -4.4287577 -4.4287844 -4.4288092 -4.428813 -4.4288149 -4.4288135 -4.4288006 -4.42878 -4.4287863 -4.4288154 -4.4288478][-4.4288421 -4.4288564 -4.4288239 -4.4287477 -4.428719 -4.4287357 -4.428751 -4.4287491 -4.4287605 -4.4287667 -4.4287658 -4.4287519 -4.4287581 -4.4287891 -4.4288244][-4.4288664 -4.428874 -4.4288177 -4.4287162 -4.4286737 -4.4286857 -4.4286852 -4.4286671 -4.4286866 -4.4287195 -4.4287429 -4.4287429 -4.4287529 -4.4287839 -4.4288206][-4.4288588 -4.4288626 -4.4287877 -4.4286709 -4.4286261 -4.4286332 -4.4286032 -4.4285407 -4.4285574 -4.4286332 -4.4286866 -4.4287024 -4.4287291 -4.428772 -4.4288177][-4.4288096 -4.4288039 -4.4287243 -4.4286146 -4.4285712 -4.4285574 -4.4284725 -4.4283333 -4.4283533 -4.4284883 -4.4285731 -4.42861 -4.4286642 -4.4287338 -4.4287968][-4.4287696 -4.4287572 -4.4286933 -4.4286065 -4.4285522 -4.4284959 -4.4283271 -4.428093 -4.4281335 -4.4283242 -4.42843 -4.4284868 -4.4285626 -4.4286618 -4.4287472][-4.4287577 -4.4287534 -4.4287071 -4.4286413 -4.4285746 -4.4284849 -4.4282851 -4.4280424 -4.4280906 -4.4282651 -4.42836 -4.4284148 -4.4284987 -4.428616 -4.42872][-4.4287987 -4.4288015 -4.4287696 -4.4287219 -4.4286594 -4.4285731 -4.4284186 -4.4282651 -4.4282866 -4.4283729 -4.42842 -4.4284463 -4.4285131 -4.4286275 -4.4287367][-4.4288712 -4.4288874 -4.4288745 -4.428844 -4.4288015 -4.4287353 -4.4286337 -4.428544 -4.4285364 -4.428555 -4.4285665 -4.4285674 -4.4286075 -4.4286976 -4.4287891][-4.4289303 -4.4289575 -4.4289632 -4.4289536 -4.4289351 -4.4288955 -4.4288354 -4.4287753 -4.4287415 -4.4287271 -4.4287257 -4.4287152 -4.4287281 -4.4287834 -4.4288473][-4.4289508 -4.4289813 -4.428998 -4.4290047 -4.4290023 -4.4289823 -4.4289517 -4.428916 -4.4288797 -4.4288578 -4.4288568 -4.4288445 -4.4288378 -4.4288607 -4.428896][-4.4289532 -4.42898 -4.429 -4.4290109 -4.4290085 -4.4289947 -4.428977 -4.42896 -4.4289384 -4.4289241 -4.4289284 -4.4289212 -4.4289064 -4.4289079 -4.4289255][-4.42897 -4.4289889 -4.4290051 -4.4290137 -4.429009 -4.4289937 -4.42898 -4.4289732 -4.4289665 -4.4289603 -4.4289622 -4.4289575 -4.4289441 -4.4289389 -4.42895][-4.4289823 -4.4289942 -4.4290047 -4.4290123 -4.4290085 -4.4289989 -4.4289927 -4.4289908 -4.4289894 -4.4289889 -4.4289856 -4.428977 -4.4289665 -4.4289637 -4.4289713]]...]
INFO - root - 2017-12-08 08:40:45.934727: step 60310, loss = 2.28, batch loss = 2.23 (36.7 examples/sec; 0.218 sec/batch; 16h:27m:38s remains)
INFO - root - 2017-12-08 08:40:48.161037: step 60320, loss = 2.28, batch loss = 2.23 (35.8 examples/sec; 0.224 sec/batch; 16h:54m:56s remains)
INFO - root - 2017-12-08 08:40:50.422462: step 60330, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.226 sec/batch; 17h:03m:22s remains)
INFO - root - 2017-12-08 08:40:52.656916: step 60340, loss = 2.28, batch loss = 2.23 (34.9 examples/sec; 0.229 sec/batch; 17h:18m:22s remains)
INFO - root - 2017-12-08 08:40:54.913810: step 60350, loss = 2.28, batch loss = 2.23 (35.3 examples/sec; 0.226 sec/batch; 17h:07m:20s remains)
INFO - root - 2017-12-08 08:40:57.143765: step 60360, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.225 sec/batch; 17h:01m:06s remains)
INFO - root - 2017-12-08 08:40:59.376736: step 60370, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 16h:37m:08s remains)
INFO - root - 2017-12-08 08:41:01.667882: step 60380, loss = 2.28, batch loss = 2.23 (34.2 examples/sec; 0.234 sec/batch; 17h:40m:40s remains)
INFO - root - 2017-12-08 08:41:03.927322: step 60390, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.222 sec/batch; 16h:45m:57s remains)
INFO - root - 2017-12-08 08:41:06.167099: step 60400, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.221 sec/batch; 16h:40m:01s remains)
2017-12-08 08:41:06.448120: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4286575 -4.4286375 -4.4286561 -4.4287133 -4.4287539 -4.4287477 -4.4287434 -4.4287343 -4.4286909 -4.428648 -4.4286327 -4.4286637 -4.4286895 -4.4286895 -4.4287071][-4.4287448 -4.4287214 -4.4287367 -4.4287791 -4.4288073 -4.4287963 -4.42877 -4.4287348 -4.4286919 -4.4286685 -4.4286571 -4.4286723 -4.4286847 -4.4286857 -4.4287181][-4.4288168 -4.42879 -4.4288015 -4.4288282 -4.4288425 -4.4288278 -4.4287887 -4.42874 -4.4287052 -4.4287138 -4.4287219 -4.4287324 -4.4287229 -4.4287119 -4.4287443][-4.4288349 -4.4288077 -4.4288144 -4.4288325 -4.4288383 -4.4288182 -4.4287729 -4.4287243 -4.428699 -4.4287338 -4.4287724 -4.4287882 -4.4287653 -4.4287381 -4.4287658][-4.4287915 -4.42877 -4.4287748 -4.4287868 -4.42878 -4.4287472 -4.4286985 -4.4286647 -4.42866 -4.4287138 -4.428772 -4.4287963 -4.4287767 -4.4287434 -4.4287639][-4.4287381 -4.4287119 -4.4287071 -4.4287009 -4.4286709 -4.4286208 -4.4285665 -4.4285522 -4.4285941 -4.4286771 -4.4287467 -4.4287863 -4.4287786 -4.4287477 -4.4287553][-4.4287071 -4.4286695 -4.4286442 -4.4286065 -4.42854 -4.4284663 -4.4283934 -4.4283881 -4.4285 -4.4286304 -4.4287176 -4.42877 -4.42877 -4.4287434 -4.4287415][-4.4286885 -4.4286413 -4.4285979 -4.4285374 -4.4284472 -4.4283447 -4.4282188 -4.4281869 -4.4283819 -4.4285831 -4.428699 -4.4287658 -4.4287691 -4.4287438 -4.4287338][-4.4286413 -4.4285922 -4.4285507 -4.4284935 -4.428411 -4.4283023 -4.4281487 -4.4280853 -4.4283166 -4.4285512 -4.4286842 -4.4287581 -4.4287667 -4.4287362 -4.4287176][-4.4285712 -4.4285197 -4.4285035 -4.4284787 -4.4284348 -4.4283667 -4.4282756 -4.4282513 -4.4284172 -4.428596 -4.4287019 -4.4287615 -4.4287639 -4.4287267 -4.4286962][-4.428525 -4.428483 -4.4284973 -4.4285178 -4.4285097 -4.4284749 -4.4284477 -4.4284558 -4.4285522 -4.4286566 -4.4287238 -4.428761 -4.428762 -4.4287257 -4.4286928][-4.42853 -4.4285169 -4.4285536 -4.4285955 -4.4286041 -4.4285893 -4.4286008 -4.4286222 -4.4286618 -4.4287047 -4.4287405 -4.4287653 -4.4287686 -4.4287434 -4.4287148][-4.4285588 -4.42858 -4.4286346 -4.4286776 -4.428689 -4.4286857 -4.4287148 -4.4287424 -4.4287486 -4.4287515 -4.4287643 -4.4287758 -4.4287744 -4.4287548 -4.4287281][-4.4285865 -4.4286256 -4.4286871 -4.4287281 -4.4287438 -4.4287543 -4.4287925 -4.4288211 -4.4288135 -4.428793 -4.4287806 -4.42877 -4.428761 -4.4287477 -4.4287291][-4.4286289 -4.4286561 -4.4287052 -4.4287386 -4.4287572 -4.4287767 -4.4288173 -4.428844 -4.42883 -4.4287963 -4.428762 -4.4287291 -4.4287181 -4.4287205 -4.4287243]]...]
INFO - root - 2017-12-08 08:41:08.713178: step 60410, loss = 2.28, batch loss = 2.23 (36.7 examples/sec; 0.218 sec/batch; 16h:27m:54s remains)
INFO - root - 2017-12-08 08:41:10.967457: step 60420, loss = 2.28, batch loss = 2.23 (36.7 examples/sec; 0.218 sec/batch; 16h:28m:17s remains)
INFO - root - 2017-12-08 08:41:13.216741: step 60430, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 16h:41m:27s remains)
INFO - root - 2017-12-08 08:41:15.471468: step 60440, loss = 2.28, batch loss = 2.23 (34.8 examples/sec; 0.230 sec/batch; 17h:23m:27s remains)
INFO - root - 2017-12-08 08:41:17.698505: step 60450, loss = 2.28, batch loss = 2.23 (34.7 examples/sec; 0.230 sec/batch; 17h:25m:01s remains)
INFO - root - 2017-12-08 08:41:19.936970: step 60460, loss = 2.28, batch loss = 2.23 (36.8 examples/sec; 0.217 sec/batch; 16h:24m:43s remains)
INFO - root - 2017-12-08 08:41:22.150870: step 60470, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.220 sec/batch; 16h:39m:06s remains)
INFO - root - 2017-12-08 08:41:24.390769: step 60480, loss = 2.28, batch loss = 2.23 (35.8 examples/sec; 0.224 sec/batch; 16h:54m:17s remains)
INFO - root - 2017-12-08 08:41:26.612845: step 60490, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.222 sec/batch; 16h:44m:42s remains)
INFO - root - 2017-12-08 08:41:28.861159: step 60500, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.221 sec/batch; 16h:43m:35s remains)
2017-12-08 08:41:29.161047: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289079 -4.4288673 -4.4288073 -4.4287415 -4.4286766 -4.4286141 -4.4285965 -4.4286194 -4.4286804 -4.4287519 -4.4287982 -4.4288249 -4.4288473 -4.428874 -4.4288979][-4.4289327 -4.42888 -4.4287968 -4.4287124 -4.4286304 -4.4285641 -4.4285622 -4.4285946 -4.4286571 -4.4287362 -4.4287853 -4.4288144 -4.4288406 -4.4288754 -4.4289074][-4.428937 -4.4288769 -4.4287758 -4.42867 -4.4285712 -4.4284992 -4.4285021 -4.4285512 -4.428628 -4.428721 -4.4287767 -4.4288054 -4.4288278 -4.4288597 -4.428894][-4.4289317 -4.4288688 -4.4287553 -4.4286304 -4.4285116 -4.4284306 -4.4284353 -4.4285045 -4.4286022 -4.4287043 -4.4287677 -4.4287987 -4.4288187 -4.4288459 -4.428875][-4.4289141 -4.4288573 -4.42875 -4.4286113 -4.4284682 -4.4283733 -4.4283862 -4.4284768 -4.4285893 -4.4286957 -4.4287667 -4.4288011 -4.4288192 -4.4288378 -4.4288588][-4.42891 -4.4288607 -4.4287572 -4.4286046 -4.4284225 -4.4282947 -4.4283185 -4.428441 -4.428575 -4.4286919 -4.42877 -4.4288087 -4.4288244 -4.4288325 -4.42884][-4.4289079 -4.4288673 -4.428772 -4.4286137 -4.4283848 -4.4282007 -4.4282126 -4.4283724 -4.4285507 -4.4286914 -4.4287729 -4.4288068 -4.4288149 -4.4288135 -4.4288082][-4.4288883 -4.428865 -4.4287987 -4.4286528 -4.4284058 -4.4281793 -4.428165 -4.4283504 -4.4285645 -4.4287119 -4.42878 -4.4287925 -4.42878 -4.4287667 -4.4287539][-4.4288435 -4.4288478 -4.4288249 -4.4287171 -4.4285 -4.4282713 -4.4282293 -4.4283972 -4.4286056 -4.4287362 -4.4287839 -4.4287786 -4.4287529 -4.4287314 -4.4287176][-4.4287858 -4.4288087 -4.4288177 -4.4287553 -4.4285917 -4.4283953 -4.4283352 -4.4284639 -4.4286394 -4.428741 -4.4287643 -4.4287491 -4.4287257 -4.4287181 -4.4287233][-4.4287443 -4.4287691 -4.4287963 -4.42877 -4.4286537 -4.4284968 -4.4284239 -4.4285016 -4.4286308 -4.4287052 -4.4287114 -4.4286938 -4.4286885 -4.4287109 -4.4287453][-4.4287291 -4.4287395 -4.4287806 -4.4287896 -4.4287176 -4.4285965 -4.4285107 -4.4285288 -4.4286046 -4.4286523 -4.4286528 -4.4286432 -4.4286675 -4.4287205 -4.4287767][-4.4287291 -4.4287281 -4.4287829 -4.4288206 -4.4287887 -4.4287009 -4.4286118 -4.4285846 -4.4286189 -4.4286518 -4.428658 -4.4286575 -4.4286957 -4.4287534 -4.4288135][-4.4287357 -4.4287314 -4.4287882 -4.4288421 -4.4288445 -4.4287934 -4.4287171 -4.42867 -4.428678 -4.4287028 -4.4287076 -4.4287071 -4.4287367 -4.4287834 -4.4288363][-4.4287457 -4.4287395 -4.428791 -4.428853 -4.4288769 -4.4288559 -4.4288006 -4.4287529 -4.4287457 -4.4287596 -4.4287572 -4.4287496 -4.4287653 -4.4288 -4.428843]]...]
INFO - root - 2017-12-08 08:41:31.397172: step 60510, loss = 2.28, batch loss = 2.23 (36.9 examples/sec; 0.217 sec/batch; 16h:23m:16s remains)
INFO - root - 2017-12-08 08:41:33.631440: step 60520, loss = 2.28, batch loss = 2.23 (34.6 examples/sec; 0.231 sec/batch; 17h:28m:15s remains)
INFO - root - 2017-12-08 08:41:35.860125: step 60530, loss = 2.28, batch loss = 2.23 (36.0 examples/sec; 0.222 sec/batch; 16h:47m:54s remains)
INFO - root - 2017-12-08 08:41:38.108867: step 60540, loss = 2.28, batch loss = 2.23 (33.5 examples/sec; 0.239 sec/batch; 18h:02m:52s remains)
INFO - root - 2017-12-08 08:41:40.356621: step 60550, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.220 sec/batch; 16h:38m:47s remains)
INFO - root - 2017-12-08 08:41:42.560918: step 60560, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.221 sec/batch; 16h:43m:42s remains)
INFO - root - 2017-12-08 08:41:44.829063: step 60570, loss = 2.28, batch loss = 2.23 (35.1 examples/sec; 0.228 sec/batch; 17h:13m:20s remains)
INFO - root - 2017-12-08 08:41:47.106171: step 60580, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.222 sec/batch; 16h:43m:52s remains)
INFO - root - 2017-12-08 08:41:49.339939: step 60590, loss = 2.28, batch loss = 2.23 (35.2 examples/sec; 0.227 sec/batch; 17h:09m:42s remains)
INFO - root - 2017-12-08 08:41:51.577819: step 60600, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 16h:32m:04s remains)
2017-12-08 08:41:51.862248: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288683 -4.4288697 -4.4288673 -4.4288497 -4.4288244 -4.4288087 -4.4288077 -4.4288149 -4.4288254 -4.4288378 -4.428834 -4.4288225 -4.4288111 -4.4288011 -4.4288092][-4.4288864 -4.428884 -4.4288774 -4.428853 -4.4288144 -4.4287825 -4.4287705 -4.428772 -4.42878 -4.4288044 -4.4288273 -4.4288397 -4.4288392 -4.4288325 -4.4288354][-4.4288869 -4.4288869 -4.4288778 -4.42885 -4.4288049 -4.42876 -4.4287295 -4.4287143 -4.4287162 -4.428751 -4.4287963 -4.4288325 -4.4288473 -4.4288521 -4.4288588][-4.4288611 -4.4288607 -4.4288497 -4.4288268 -4.4287863 -4.4287343 -4.4286861 -4.4286461 -4.4286304 -4.4286647 -4.4287257 -4.4287825 -4.4288116 -4.4288287 -4.4288449][-4.42884 -4.4288282 -4.4288087 -4.4287906 -4.4287558 -4.4286909 -4.4286113 -4.4285307 -4.4284816 -4.4285111 -4.4285936 -4.4286852 -4.4287467 -4.4287848 -4.4288111][-4.4288468 -4.428822 -4.4287896 -4.4287663 -4.4287291 -4.4286466 -4.4285216 -4.4283838 -4.4282722 -4.4282675 -4.4283748 -4.4285212 -4.4286351 -4.4287047 -4.4287467][-4.4288745 -4.428843 -4.428802 -4.4287696 -4.4287252 -4.4286294 -4.4284697 -4.4282689 -4.4280562 -4.4279609 -4.4280725 -4.4282861 -4.4284678 -4.4285789 -4.4286475][-4.4289055 -4.4288793 -4.4288373 -4.4287987 -4.4287567 -4.4286718 -4.4285264 -4.4283261 -4.4280815 -4.42793 -4.4280019 -4.4282074 -4.4283957 -4.4285111 -4.42859][-4.4289289 -4.4289126 -4.4288816 -4.4288473 -4.4288192 -4.4287739 -4.4286909 -4.428565 -4.4283967 -4.4282808 -4.4282961 -4.4283943 -4.428503 -4.4285688 -4.4286237][-4.4289508 -4.42894 -4.4289222 -4.4289 -4.4288883 -4.4288759 -4.428844 -4.4287834 -4.4286904 -4.4286213 -4.4286108 -4.4286294 -4.4286671 -4.428679 -4.4286942][-4.4289684 -4.4289622 -4.4289541 -4.4289365 -4.4289269 -4.4289227 -4.4289093 -4.4288831 -4.4288311 -4.4287887 -4.4287734 -4.4287672 -4.4287806 -4.4287715 -4.4287672][-4.42898 -4.4289765 -4.4289737 -4.4289608 -4.4289455 -4.4289341 -4.4289174 -4.4288936 -4.4288492 -4.428813 -4.4288025 -4.4288058 -4.4288268 -4.4288268 -4.4288306][-4.4289856 -4.4289827 -4.4289823 -4.4289742 -4.4289565 -4.4289374 -4.4289117 -4.4288793 -4.4288325 -4.428791 -4.4287777 -4.4287882 -4.4288249 -4.4288449 -4.4288635][-4.428978 -4.4289813 -4.4289827 -4.4289775 -4.4289637 -4.4289403 -4.4289093 -4.4288683 -4.4288154 -4.4287658 -4.4287467 -4.4287639 -4.42881 -4.4288492 -4.4288788][-4.4289455 -4.4289632 -4.4289756 -4.428978 -4.4289689 -4.4289465 -4.42891 -4.4288597 -4.428802 -4.4287472 -4.4287257 -4.4287472 -4.4287972 -4.4288435 -4.4288769]]...]
INFO - root - 2017-12-08 08:41:54.117641: step 60610, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 16h:49m:10s remains)
INFO - root - 2017-12-08 08:41:56.349261: step 60620, loss = 2.28, batch loss = 2.23 (36.8 examples/sec; 0.217 sec/batch; 16h:25m:33s remains)
INFO - root - 2017-12-08 08:41:58.622282: step 60630, loss = 2.28, batch loss = 2.23 (35.0 examples/sec; 0.229 sec/batch; 17h:15m:47s remains)
INFO - root - 2017-12-08 08:42:00.889021: step 60640, loss = 2.28, batch loss = 2.23 (34.4 examples/sec; 0.233 sec/batch; 17h:34m:46s remains)
INFO - root - 2017-12-08 08:42:03.154973: step 60650, loss = 2.28, batch loss = 2.23 (34.8 examples/sec; 0.230 sec/batch; 17h:22m:26s remains)
INFO - root - 2017-12-08 08:42:05.377011: step 60660, loss = 2.28, batch loss = 2.23 (37.5 examples/sec; 0.213 sec/batch; 16h:05m:40s remains)
INFO - root - 2017-12-08 08:42:07.629550: step 60670, loss = 2.28, batch loss = 2.23 (36.6 examples/sec; 0.218 sec/batch; 16h:29m:23s remains)
INFO - root - 2017-12-08 08:42:09.874728: step 60680, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.219 sec/batch; 16h:34m:23s remains)
INFO - root - 2017-12-08 08:42:12.134166: step 60690, loss = 2.28, batch loss = 2.23 (33.4 examples/sec; 0.240 sec/batch; 18h:05m:03s remains)
INFO - root - 2017-12-08 08:42:14.367702: step 60700, loss = 2.28, batch loss = 2.23 (35.8 examples/sec; 0.224 sec/batch; 16h:53m:38s remains)
2017-12-08 08:42:14.659389: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289064 -4.4289103 -4.4289136 -4.428915 -4.4289169 -4.4289179 -4.4289174 -4.4289174 -4.4289188 -4.4289222 -4.4289241 -4.4289231 -4.42892 -4.4289165 -4.4289141][-4.4288521 -4.4288573 -4.428865 -4.4288697 -4.4288731 -4.4288745 -4.4288731 -4.4288716 -4.4288745 -4.4288793 -4.4288826 -4.4288807 -4.4288759 -4.4288726 -4.4288707][-4.4287596 -4.4287663 -4.4287782 -4.4287868 -4.428791 -4.428792 -4.4287882 -4.4287853 -4.4287882 -4.4287944 -4.4287982 -4.4287944 -4.4287887 -4.4287863 -4.4287872][-4.4286532 -4.4286633 -4.42868 -4.4286895 -4.4286923 -4.4286923 -4.4286876 -4.4286828 -4.4286852 -4.4286914 -4.4286938 -4.428688 -4.4286814 -4.4286819 -4.4286876][-4.4285831 -4.4285989 -4.4286151 -4.4286222 -4.4286227 -4.4286227 -4.4286184 -4.4286137 -4.4286151 -4.4286175 -4.428616 -4.4286089 -4.4286056 -4.4286137 -4.4286284][-4.4285684 -4.4285908 -4.4286027 -4.4286027 -4.4285979 -4.4285984 -4.4285984 -4.4285965 -4.428597 -4.4285951 -4.42859 -4.4285879 -4.4285979 -4.42862 -4.4286447][-4.4285851 -4.4286103 -4.4286127 -4.4286046 -4.4285941 -4.4285989 -4.4286103 -4.4286194 -4.4286265 -4.4286265 -4.4286237 -4.4286323 -4.4286561 -4.4286861 -4.4287143][-4.4286284 -4.4286509 -4.4286423 -4.4286256 -4.4286141 -4.4286275 -4.4286532 -4.4286766 -4.4286966 -4.4287043 -4.4287076 -4.4287219 -4.4287496 -4.4287786 -4.428802][-4.4286828 -4.4287062 -4.4286952 -4.4286723 -4.4286571 -4.4286728 -4.4287047 -4.4287395 -4.42877 -4.4287839 -4.4287896 -4.4288049 -4.42883 -4.428853 -4.4288683][-4.4287386 -4.4287658 -4.4287529 -4.4287219 -4.4286971 -4.4287066 -4.4287386 -4.4287796 -4.4288173 -4.4288349 -4.4288411 -4.4288535 -4.4288745 -4.4288926 -4.428905][-4.4287834 -4.428812 -4.4287963 -4.4287581 -4.4287276 -4.4287333 -4.4287648 -4.4288087 -4.4288478 -4.4288654 -4.4288712 -4.4288807 -4.4288979 -4.4289141 -4.428926][-4.4288044 -4.4288297 -4.42881 -4.428771 -4.4287457 -4.4287572 -4.428791 -4.4288335 -4.4288678 -4.4288831 -4.4288869 -4.4288936 -4.428906 -4.4289193 -4.4289317][-4.4288149 -4.4288282 -4.4288044 -4.4287724 -4.428762 -4.4287829 -4.42882 -4.4288573 -4.428884 -4.4288945 -4.428896 -4.4289 -4.4289088 -4.4289212 -4.4289331][-4.4288163 -4.4288168 -4.4287958 -4.4287791 -4.4287834 -4.4288116 -4.42885 -4.4288807 -4.4289 -4.428905 -4.4289041 -4.428906 -4.4289126 -4.4289217 -4.4289317][-4.4288034 -4.428803 -4.4287987 -4.428802 -4.4288163 -4.428844 -4.4288769 -4.4288988 -4.4289103 -4.4289131 -4.4289112 -4.4289126 -4.4289169 -4.4289231 -4.4289293]]...]
INFO - root - 2017-12-08 08:42:16.911455: step 60710, loss = 2.28, batch loss = 2.23 (35.2 examples/sec; 0.227 sec/batch; 17h:10m:28s remains)
INFO - root - 2017-12-08 08:42:19.153384: step 60720, loss = 2.28, batch loss = 2.23 (34.0 examples/sec; 0.235 sec/batch; 17h:46m:39s remains)
INFO - root - 2017-12-08 08:42:21.398262: step 60730, loss = 2.28, batch loss = 2.23 (36.9 examples/sec; 0.217 sec/batch; 16h:21m:43s remains)
INFO - root - 2017-12-08 08:42:23.700367: step 60740, loss = 2.28, batch loss = 2.23 (34.6 examples/sec; 0.231 sec/batch; 17h:26m:20s remains)
INFO - root - 2017-12-08 08:42:25.912555: step 60750, loss = 2.28, batch loss = 2.23 (35.0 examples/sec; 0.229 sec/batch; 17h:16m:37s remains)
INFO - root - 2017-12-08 08:42:28.152147: step 60760, loss = 2.28, batch loss = 2.23 (35.4 examples/sec; 0.226 sec/batch; 17h:03m:14s remains)
INFO - root - 2017-12-08 08:42:30.389357: step 60770, loss = 2.28, batch loss = 2.23 (35.0 examples/sec; 0.229 sec/batch; 17h:15m:16s remains)
INFO - root - 2017-12-08 08:42:32.614188: step 60780, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 16h:34m:34s remains)
INFO - root - 2017-12-08 08:42:34.852063: step 60790, loss = 2.28, batch loss = 2.23 (36.6 examples/sec; 0.219 sec/batch; 16h:29m:47s remains)
INFO - root - 2017-12-08 08:42:37.102847: step 60800, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.221 sec/batch; 16h:42m:42s remains)
2017-12-08 08:42:37.413333: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289613 -4.4289665 -4.4289622 -4.4289613 -4.4289651 -4.4289718 -4.4289751 -4.4289765 -4.4289746 -4.4289703 -4.4289684 -4.4289708 -4.4289751 -4.4289789 -4.4289794][-4.4289522 -4.4289494 -4.4289384 -4.4289317 -4.4289341 -4.4289441 -4.4289546 -4.4289651 -4.4289684 -4.4289632 -4.4289579 -4.4289575 -4.4289613 -4.428967 -4.42897][-4.4289269 -4.4289093 -4.428884 -4.4288645 -4.428863 -4.4288774 -4.4288993 -4.4289265 -4.4289446 -4.4289451 -4.4289355 -4.4289279 -4.4289289 -4.428936 -4.4289422][-4.4288826 -4.4288387 -4.4287915 -4.428761 -4.4287586 -4.4287796 -4.4288154 -4.4288592 -4.4288898 -4.4289026 -4.428896 -4.4288836 -4.4288831 -4.4288917 -4.428905][-4.4288111 -4.4287357 -4.4286685 -4.4286342 -4.4286361 -4.4286637 -4.4287128 -4.4287677 -4.4288087 -4.4288378 -4.428843 -4.4288359 -4.4288335 -4.4288454 -4.4288664][-4.4287262 -4.4286203 -4.428535 -4.4284973 -4.4285007 -4.4285312 -4.4285893 -4.4286518 -4.4287109 -4.4287591 -4.4287815 -4.4287868 -4.4287834 -4.4287996 -4.4288297][-4.4286542 -4.4285259 -4.4284177 -4.4283614 -4.4283495 -4.4283733 -4.4284143 -4.4284663 -4.4285622 -4.4286504 -4.4287057 -4.42873 -4.4287286 -4.4287505 -4.4287877][-4.4286337 -4.4285035 -4.4283767 -4.4282885 -4.4282484 -4.42823 -4.4281974 -4.4282036 -4.4283414 -4.4284863 -4.4285903 -4.4286509 -4.4286718 -4.4287119 -4.4287629][-4.4286814 -4.4285784 -4.4284596 -4.4283581 -4.4282889 -4.4281974 -4.4280462 -4.4279733 -4.4281392 -4.4283366 -4.4284859 -4.4285865 -4.4286432 -4.4287109 -4.4287767][-4.4287472 -4.428679 -4.4285889 -4.4285097 -4.4284444 -4.4283171 -4.4281096 -4.4280052 -4.4281516 -4.4283357 -4.4284897 -4.428606 -4.4286842 -4.4287615 -4.428823][-4.42878 -4.42873 -4.428668 -4.4286308 -4.4286017 -4.4284997 -4.4283309 -4.4282603 -4.4283533 -4.42847 -4.4285841 -4.4286752 -4.4287467 -4.428813 -4.4288635][-4.4287925 -4.4287457 -4.4287014 -4.4286995 -4.4287128 -4.4286618 -4.4285617 -4.4285221 -4.428566 -4.4286265 -4.4286995 -4.4287567 -4.4288049 -4.4288487 -4.4288855][-4.4288096 -4.4287648 -4.42873 -4.4287443 -4.4287753 -4.4287605 -4.4287119 -4.4286876 -4.4287019 -4.4287333 -4.4287782 -4.4288087 -4.4288344 -4.4288659 -4.4288983][-4.4288406 -4.428802 -4.428781 -4.4287987 -4.4288263 -4.4288292 -4.4288116 -4.4287968 -4.4288006 -4.4288216 -4.4288497 -4.428865 -4.4288793 -4.4289012 -4.4289241][-4.4288731 -4.4288487 -4.4288411 -4.4288588 -4.4288759 -4.4288821 -4.4288812 -4.4288754 -4.4288764 -4.4288907 -4.4289117 -4.4289236 -4.4289317 -4.4289441 -4.4289546]]...]
INFO - root - 2017-12-08 08:42:39.711420: step 60810, loss = 2.28, batch loss = 2.23 (34.4 examples/sec; 0.233 sec/batch; 17h:33m:18s remains)
INFO - root - 2017-12-08 08:42:41.974196: step 60820, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 16h:34m:50s remains)
INFO - root - 2017-12-08 08:42:44.208403: step 60830, loss = 2.28, batch loss = 2.23 (35.7 examples/sec; 0.224 sec/batch; 16h:54m:43s remains)
INFO - root - 2017-12-08 08:42:46.423268: step 60840, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 16h:35m:32s remains)
INFO - root - 2017-12-08 08:42:48.647118: step 60850, loss = 2.28, batch loss = 2.23 (36.7 examples/sec; 0.218 sec/batch; 16h:27m:18s remains)
INFO - root - 2017-12-08 08:42:50.895381: step 60860, loss = 2.28, batch loss = 2.23 (34.7 examples/sec; 0.231 sec/batch; 17h:24m:32s remains)
INFO - root - 2017-12-08 08:42:53.126489: step 60870, loss = 2.28, batch loss = 2.23 (35.7 examples/sec; 0.224 sec/batch; 16h:55m:06s remains)
INFO - root - 2017-12-08 08:42:55.377936: step 60880, loss = 2.28, batch loss = 2.23 (36.9 examples/sec; 0.217 sec/batch; 16h:20m:31s remains)
INFO - root - 2017-12-08 08:42:57.610845: step 60890, loss = 2.28, batch loss = 2.23 (36.6 examples/sec; 0.219 sec/batch; 16h:29m:09s remains)
INFO - root - 2017-12-08 08:42:59.878591: step 60900, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 16h:48m:45s remains)
2017-12-08 08:43:00.217660: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4287524 -4.4287887 -4.4288392 -4.4288878 -4.4289365 -4.4289646 -4.4289689 -4.4289594 -4.4289446 -4.4289327 -4.4289126 -4.4288807 -4.4288435 -4.4288158 -4.4288034][-4.4287553 -4.4288 -4.4288564 -4.4289007 -4.4289408 -4.4289618 -4.4289575 -4.428937 -4.4289217 -4.4289136 -4.4289 -4.4288721 -4.4288387 -4.4288168 -4.4288158][-4.4287591 -4.4288082 -4.4288626 -4.4288917 -4.4289074 -4.4289079 -4.4288797 -4.4288497 -4.4288449 -4.4288559 -4.4288611 -4.4288492 -4.4288173 -4.4287958 -4.428803][-4.428793 -4.4288383 -4.4288754 -4.428874 -4.4288468 -4.4288054 -4.4287372 -4.4286942 -4.4287181 -4.4287696 -4.4288044 -4.4288092 -4.42877 -4.4287343 -4.4287391][-4.4288406 -4.42888 -4.4288898 -4.4288464 -4.4287691 -4.4286733 -4.4285474 -4.4284806 -4.4285445 -4.4286571 -4.4287434 -4.4287791 -4.4287381 -4.4286776 -4.4286633][-4.4288859 -4.42892 -4.4289 -4.428813 -4.4286833 -4.4285212 -4.428318 -4.4282169 -4.4283395 -4.4285297 -4.4286804 -4.4287596 -4.4287391 -4.4286723 -4.4286442][-4.4289064 -4.4289331 -4.4288974 -4.42878 -4.4286046 -4.4283648 -4.42805 -4.42791 -4.4281273 -4.4284225 -4.4286394 -4.4287658 -4.4287744 -4.4287143 -4.4286847][-4.4289002 -4.4289165 -4.428874 -4.4287457 -4.4285297 -4.42821 -4.4277916 -4.4276185 -4.4279327 -4.4283252 -4.4285975 -4.42875 -4.4287777 -4.4287276 -4.4287009][-4.4288955 -4.4288964 -4.4288473 -4.4287252 -4.4285154 -4.4282007 -4.4277949 -4.4276361 -4.427959 -4.4283476 -4.4286075 -4.4287467 -4.428771 -4.4287195 -4.4286909][-4.4288883 -4.4288893 -4.4288468 -4.4287419 -4.4285793 -4.4283552 -4.4280787 -4.4279842 -4.4282184 -4.4285016 -4.4286885 -4.4287829 -4.4287844 -4.4287229 -4.4286876][-4.4288745 -4.428885 -4.4288626 -4.4287863 -4.4286776 -4.4285464 -4.4283981 -4.4283576 -4.4284987 -4.4286652 -4.4287677 -4.4288139 -4.4287939 -4.4287281 -4.4286938][-4.4288459 -4.428863 -4.4288583 -4.4288125 -4.4287519 -4.4286914 -4.4286232 -4.4286056 -4.428678 -4.42876 -4.428803 -4.4288125 -4.4287834 -4.4287224 -4.4286947][-4.4288073 -4.4288177 -4.4288149 -4.4287891 -4.4287667 -4.4287558 -4.4287324 -4.4287229 -4.4287558 -4.42879 -4.4288058 -4.4287944 -4.4287586 -4.4287076 -4.428688][-4.4287987 -4.4288063 -4.428803 -4.4287891 -4.4287863 -4.4287958 -4.4287944 -4.4287906 -4.4288011 -4.4288082 -4.4288073 -4.42879 -4.428761 -4.4287243 -4.428709][-4.4288368 -4.4288425 -4.4288397 -4.4288335 -4.4288316 -4.4288397 -4.4288435 -4.428843 -4.4288459 -4.428844 -4.4288383 -4.4288263 -4.4288077 -4.4287829 -4.4287677]]...]
INFO - root - 2017-12-08 08:43:02.434706: step 60910, loss = 2.28, batch loss = 2.23 (35.6 examples/sec; 0.224 sec/batch; 16h:56m:01s remains)
INFO - root - 2017-12-08 08:43:04.688669: step 60920, loss = 2.28, batch loss = 2.23 (36.7 examples/sec; 0.218 sec/batch; 16h:27m:45s remains)
INFO - root - 2017-12-08 08:43:06.914911: step 60930, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 16h:47m:26s remains)
INFO - root - 2017-12-08 08:43:09.129271: step 60940, loss = 2.28, batch loss = 2.23 (35.0 examples/sec; 0.228 sec/batch; 17h:14m:10s remains)
INFO - root - 2017-12-08 08:43:11.411286: step 60950, loss = 2.28, batch loss = 2.23 (31.9 examples/sec; 0.251 sec/batch; 18h:53m:47s remains)
INFO - root - 2017-12-08 08:43:13.675093: step 60960, loss = 2.28, batch loss = 2.23 (35.2 examples/sec; 0.227 sec/batch; 17h:08m:40s remains)
INFO - root - 2017-12-08 08:43:15.942259: step 60970, loss = 2.28, batch loss = 2.23 (35.4 examples/sec; 0.226 sec/batch; 17h:01m:30s remains)
INFO - root - 2017-12-08 08:43:18.200152: step 60980, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.225 sec/batch; 16h:58m:50s remains)
INFO - root - 2017-12-08 08:43:20.469031: step 60990, loss = 2.28, batch loss = 2.23 (35.4 examples/sec; 0.226 sec/batch; 17h:01m:24s remains)
INFO - root - 2017-12-08 08:43:22.744687: step 61000, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.222 sec/batch; 16h:42m:19s remains)
2017-12-08 08:43:23.069295: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289637 -4.4289632 -4.42894 -4.4289055 -4.4288697 -4.428844 -4.4288445 -4.4288545 -4.4288645 -4.4288793 -4.4288936 -4.4289031 -4.428906 -4.4289055 -4.4289079][-4.4289622 -4.4289689 -4.4289527 -4.4289222 -4.428885 -4.4288554 -4.4288507 -4.4288578 -4.4288678 -4.4288836 -4.4288964 -4.4289055 -4.4289093 -4.4289079 -4.4289093][-4.4289474 -4.428957 -4.4289441 -4.4289141 -4.4288754 -4.4288411 -4.4288292 -4.4288354 -4.4288521 -4.4288735 -4.4288883 -4.4289007 -4.4289079 -4.4289088 -4.4289103][-4.4289279 -4.4289336 -4.4289188 -4.4288869 -4.428844 -4.4287987 -4.4287744 -4.42878 -4.4288082 -4.4288411 -4.4288635 -4.4288831 -4.4288979 -4.4289045 -4.4289083][-4.4289117 -4.4289107 -4.4288917 -4.4288573 -4.4288106 -4.4287577 -4.4287205 -4.4287229 -4.4287615 -4.4288049 -4.4288344 -4.42886 -4.4288812 -4.4288936 -4.4289][-4.4289 -4.4288945 -4.4288764 -4.4288454 -4.4288034 -4.4287539 -4.4287138 -4.4287081 -4.4287424 -4.4287858 -4.4288158 -4.4288416 -4.428865 -4.42888 -4.4288878][-4.428894 -4.4288874 -4.4288726 -4.4288497 -4.4288163 -4.4287767 -4.42874 -4.4287262 -4.4287462 -4.4287777 -4.4288025 -4.4288244 -4.4288478 -4.428865 -4.428875][-4.4288936 -4.4288859 -4.4288745 -4.4288583 -4.428833 -4.4288025 -4.428771 -4.4287534 -4.4287577 -4.4287753 -4.4287934 -4.4288111 -4.4288335 -4.4288516 -4.4288635][-4.4288969 -4.4288893 -4.4288797 -4.4288678 -4.4288492 -4.4288273 -4.4288044 -4.4287877 -4.428782 -4.4287882 -4.4287982 -4.42881 -4.4288282 -4.4288459 -4.4288597][-4.4289017 -4.4288964 -4.4288883 -4.4288807 -4.4288678 -4.428854 -4.4288406 -4.4288282 -4.4288192 -4.4288173 -4.4288211 -4.4288273 -4.4288392 -4.428853 -4.4288659][-4.428906 -4.4289036 -4.4288983 -4.4288936 -4.4288855 -4.4288778 -4.4288716 -4.4288635 -4.4288559 -4.4288516 -4.4288521 -4.4288549 -4.4288616 -4.4288712 -4.42888][-4.4289103 -4.42891 -4.4289074 -4.4289045 -4.4288993 -4.4288945 -4.4288921 -4.4288878 -4.4288836 -4.4288816 -4.4288816 -4.4288826 -4.4288864 -4.4288917 -4.4288964][-4.4289122 -4.4289136 -4.4289122 -4.4289107 -4.4289079 -4.428905 -4.4289045 -4.4289026 -4.4289012 -4.4289017 -4.4289021 -4.4289031 -4.428905 -4.4289079 -4.4289093][-4.4289141 -4.428916 -4.4289155 -4.4289141 -4.4289122 -4.4289107 -4.42891 -4.4289088 -4.4289088 -4.4289103 -4.4289107 -4.4289117 -4.4289126 -4.4289136 -4.4289136][-4.4289155 -4.4289184 -4.4289174 -4.4289169 -4.4289155 -4.4289145 -4.4289136 -4.4289131 -4.4289136 -4.4289141 -4.4289145 -4.4289141 -4.4289145 -4.4289145 -4.4289145]]...]
INFO - root - 2017-12-08 08:43:25.276356: step 61010, loss = 2.28, batch loss = 2.23 (35.3 examples/sec; 0.227 sec/batch; 17h:06m:05s remains)
INFO - root - 2017-12-08 08:43:27.484899: step 61020, loss = 2.28, batch loss = 2.23 (37.2 examples/sec; 0.215 sec/batch; 16h:12m:27s remains)
INFO - root - 2017-12-08 08:43:29.736948: step 61030, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 16h:35m:23s remains)
INFO - root - 2017-12-08 08:43:31.984359: step 61040, loss = 2.28, batch loss = 2.23 (33.6 examples/sec; 0.238 sec/batch; 17h:57m:19s remains)
INFO - root - 2017-12-08 08:43:34.210916: step 61050, loss = 2.28, batch loss = 2.23 (36.8 examples/sec; 0.217 sec/batch; 16h:22m:50s remains)
INFO - root - 2017-12-08 08:43:36.462942: step 61060, loss = 2.28, batch loss = 2.23 (35.4 examples/sec; 0.226 sec/batch; 17h:01m:19s remains)
INFO - root - 2017-12-08 08:43:38.813798: step 61070, loss = 2.28, batch loss = 2.23 (31.6 examples/sec; 0.253 sec/batch; 19h:05m:12s remains)
INFO - root - 2017-12-08 08:43:41.061569: step 61080, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 16h:33m:22s remains)
INFO - root - 2017-12-08 08:43:43.301317: step 61090, loss = 2.28, batch loss = 2.23 (36.0 examples/sec; 0.222 sec/batch; 16h:43m:55s remains)
INFO - root - 2017-12-08 08:43:45.524353: step 61100, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.222 sec/batch; 16h:42m:55s remains)
2017-12-08 08:43:45.819785: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289584 -4.4289532 -4.4289436 -4.4289327 -4.428925 -4.4289222 -4.4289241 -4.4289279 -4.4289303 -4.4289303 -4.4289284 -4.4289246 -4.4289165 -4.4289045 -4.4288845][-4.4289594 -4.4289517 -4.4289379 -4.4289217 -4.4289088 -4.4289045 -4.4289117 -4.4289231 -4.4289322 -4.428937 -4.428937 -4.4289341 -4.4289274 -4.4289179 -4.4289002][-4.4289446 -4.4289303 -4.4289064 -4.4288797 -4.4288573 -4.4288507 -4.4288654 -4.4288917 -4.4289174 -4.4289351 -4.4289422 -4.4289412 -4.4289365 -4.42893 -4.4289169][-4.4289217 -4.428895 -4.4288511 -4.4288015 -4.4287624 -4.4287515 -4.4287758 -4.4288211 -4.4288712 -4.4289122 -4.4289327 -4.4289389 -4.4289403 -4.4289389 -4.4289312][-4.4288707 -4.4288263 -4.4287562 -4.4286828 -4.4286289 -4.4286141 -4.4286456 -4.4287076 -4.428781 -4.4288468 -4.4288855 -4.4289055 -4.4289188 -4.4289255 -4.4289236][-4.4287872 -4.4287343 -4.4286456 -4.4285526 -4.4284883 -4.4284725 -4.4285049 -4.4285703 -4.4286528 -4.4287343 -4.42879 -4.4288263 -4.4288554 -4.42887 -4.4288688][-4.4286952 -4.4286475 -4.4285555 -4.4284525 -4.4283776 -4.4283514 -4.4283695 -4.4284277 -4.42851 -4.4285975 -4.4286652 -4.4287162 -4.4287553 -4.4287682 -4.4287543][-4.4286318 -4.4286032 -4.4285278 -4.4284358 -4.4283586 -4.4283118 -4.4282985 -4.4283357 -4.4284062 -4.4284883 -4.4285622 -4.4286175 -4.4286485 -4.428637 -4.428596][-4.4286246 -4.4286246 -4.4285812 -4.4285135 -4.4284425 -4.4283819 -4.4283466 -4.4283652 -4.4284172 -4.4284821 -4.4285493 -4.428597 -4.4286075 -4.4285631 -4.4284959][-4.4286995 -4.4287157 -4.4287009 -4.4286604 -4.428607 -4.4285564 -4.4285278 -4.4285426 -4.4285774 -4.428618 -4.4286637 -4.4286923 -4.4286804 -4.4286151 -4.4285398][-4.4288125 -4.4288287 -4.4288259 -4.4288054 -4.4287753 -4.4287472 -4.4287319 -4.4287429 -4.4287629 -4.4287829 -4.4288058 -4.4288177 -4.4287977 -4.4287391 -4.428679][-4.4288964 -4.4289069 -4.4289088 -4.4288983 -4.4288816 -4.428865 -4.4288526 -4.4288516 -4.4288573 -4.4288664 -4.428884 -4.428896 -4.4288874 -4.428853 -4.4288192][-4.428947 -4.4289541 -4.428956 -4.4289441 -4.4289227 -4.4289 -4.428874 -4.4288497 -4.4288387 -4.4288497 -4.4288816 -4.4289126 -4.4289265 -4.4289179 -4.4289026][-4.4289703 -4.4289727 -4.4289684 -4.4289417 -4.4288974 -4.4288454 -4.4287906 -4.42874 -4.4287148 -4.4287376 -4.428802 -4.4288707 -4.4289122 -4.4289236 -4.4289165][-4.4289603 -4.4289551 -4.428937 -4.4288831 -4.4287987 -4.4287014 -4.4286103 -4.4285383 -4.4285069 -4.4285483 -4.4286532 -4.4287639 -4.4288383 -4.4288721 -4.4288678]]...]
INFO - root - 2017-12-08 08:43:48.030354: step 61110, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 16h:35m:08s remains)
INFO - root - 2017-12-08 08:43:50.276864: step 61120, loss = 2.28, batch loss = 2.23 (35.2 examples/sec; 0.228 sec/batch; 17h:09m:09s remains)
INFO - root - 2017-12-08 08:43:52.505662: step 61130, loss = 2.28, batch loss = 2.23 (35.6 examples/sec; 0.225 sec/batch; 16h:55m:36s remains)
INFO - root - 2017-12-08 08:43:54.744136: step 61140, loss = 2.28, batch loss = 2.23 (35.4 examples/sec; 0.226 sec/batch; 17h:01m:33s remains)
INFO - root - 2017-12-08 08:43:56.967335: step 61150, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.225 sec/batch; 16h:57m:54s remains)
INFO - root - 2017-12-08 08:43:59.189676: step 61160, loss = 2.28, batch loss = 2.23 (35.1 examples/sec; 0.228 sec/batch; 17h:11m:58s remains)
INFO - root - 2017-12-08 08:44:01.423866: step 61170, loss = 2.28, batch loss = 2.23 (35.1 examples/sec; 0.228 sec/batch; 17h:11m:10s remains)
INFO - root - 2017-12-08 08:44:03.661769: step 61180, loss = 2.28, batch loss = 2.23 (35.6 examples/sec; 0.225 sec/batch; 16h:56m:48s remains)
INFO - root - 2017-12-08 08:44:05.932142: step 61190, loss = 2.28, batch loss = 2.23 (34.9 examples/sec; 0.229 sec/batch; 17h:15m:14s remains)
INFO - root - 2017-12-08 08:44:08.155615: step 61200, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 16h:31m:45s remains)
2017-12-08 08:44:08.464611: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288807 -4.4288483 -4.4288321 -4.4288259 -4.4288011 -4.4287367 -4.4286523 -4.4285688 -4.4285469 -4.4286132 -4.4287281 -4.4288263 -4.4289093 -4.4289589 -4.4289908][-4.4288344 -4.4287848 -4.42876 -4.4287572 -4.4287481 -4.4287009 -4.4286261 -4.42854 -4.4285164 -4.4285922 -4.4287186 -4.4288249 -4.4289122 -4.4289622 -4.4289932][-4.4287839 -4.4287229 -4.4286919 -4.4286914 -4.4286909 -4.4286609 -4.4285979 -4.42851 -4.4284878 -4.428575 -4.4287109 -4.4288249 -4.4289145 -4.4289632 -4.4289937][-4.4287386 -4.4286728 -4.4286356 -4.428628 -4.4286237 -4.4286046 -4.428555 -4.4284725 -4.4284596 -4.4285569 -4.428699 -4.4288206 -4.4289141 -4.4289637 -4.4289932][-4.4287152 -4.4286513 -4.4286075 -4.4285855 -4.4285655 -4.4285383 -4.4284911 -4.4284086 -4.4284124 -4.4285274 -4.4286795 -4.4288111 -4.4289117 -4.4289641 -4.4289927][-4.4287262 -4.4286637 -4.4286103 -4.4285731 -4.4285307 -4.4284768 -4.428412 -4.4283214 -4.428339 -4.4284787 -4.4286489 -4.4287953 -4.4289079 -4.4289646 -4.4289932][-4.4287753 -4.4287214 -4.4286695 -4.4286218 -4.4285522 -4.4284554 -4.4283471 -4.4282269 -4.4282508 -4.4284205 -4.4286141 -4.4287782 -4.4289031 -4.4289641 -4.4289942][-4.4288435 -4.4288087 -4.428771 -4.4287205 -4.4286289 -4.4284883 -4.4283247 -4.4281597 -4.4281778 -4.4283724 -4.4285889 -4.4287648 -4.4288979 -4.4289637 -4.4289961][-4.428906 -4.4288931 -4.4288759 -4.4288363 -4.428741 -4.4285765 -4.4283786 -4.4281774 -4.4281631 -4.4283495 -4.4285736 -4.4287539 -4.4288921 -4.4289641 -4.4289985][-4.4289436 -4.4289432 -4.4289412 -4.4289184 -4.4288373 -4.4286828 -4.4284792 -4.4282594 -4.4281888 -4.42834 -4.4285607 -4.4287453 -4.4288874 -4.4289637 -4.429][-4.4289637 -4.4289708 -4.4289746 -4.4289637 -4.4289055 -4.4287767 -4.4285874 -4.4283614 -4.4282413 -4.4283509 -4.4285626 -4.4287486 -4.42889 -4.4289646 -4.4290013][-4.4289737 -4.4289846 -4.4289913 -4.42899 -4.4289536 -4.428853 -4.4286866 -4.4284668 -4.4283171 -4.4283876 -4.4285827 -4.4287605 -4.4288969 -4.428968 -4.4290023][-4.4289789 -4.428988 -4.4289918 -4.4289956 -4.4289784 -4.4289055 -4.4287639 -4.4285626 -4.4284034 -4.4284391 -4.4286084 -4.4287724 -4.4289007 -4.4289703 -4.4290037][-4.428977 -4.4289775 -4.4289756 -4.4289818 -4.428978 -4.42893 -4.4288163 -4.4286404 -4.4284868 -4.428493 -4.4286313 -4.4287825 -4.4289031 -4.4289727 -4.4290051][-4.4289713 -4.4289646 -4.4289536 -4.4289584 -4.4289627 -4.4289317 -4.4288425 -4.4286962 -4.4285588 -4.4285445 -4.4286528 -4.4287915 -4.428906 -4.4289751 -4.4290066]]...]
INFO - root - 2017-12-08 08:44:10.694195: step 61210, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 16h:34m:31s remains)
INFO - root - 2017-12-08 08:44:12.964477: step 61220, loss = 2.28, batch loss = 2.23 (35.1 examples/sec; 0.228 sec/batch; 17h:10m:45s remains)
INFO - root - 2017-12-08 08:44:15.200817: step 61230, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.222 sec/batch; 16h:41m:50s remains)
INFO - root - 2017-12-08 08:44:17.423688: step 61240, loss = 2.28, batch loss = 2.23 (35.2 examples/sec; 0.227 sec/batch; 17h:07m:35s remains)
INFO - root - 2017-12-08 08:44:19.654772: step 61250, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 16h:31m:02s remains)
INFO - root - 2017-12-08 08:44:21.959859: step 61260, loss = 2.28, batch loss = 2.23 (34.4 examples/sec; 0.233 sec/batch; 17h:31m:40s remains)
INFO - root - 2017-12-08 08:44:24.206640: step 61270, loss = 2.28, batch loss = 2.23 (35.1 examples/sec; 0.228 sec/batch; 17h:11m:19s remains)
INFO - root - 2017-12-08 08:44:26.457603: step 61280, loss = 2.28, batch loss = 2.23 (34.5 examples/sec; 0.232 sec/batch; 17h:27m:47s remains)
INFO - root - 2017-12-08 08:44:28.681939: step 61290, loss = 2.28, batch loss = 2.23 (37.6 examples/sec; 0.213 sec/batch; 16h:01m:00s remains)
INFO - root - 2017-12-08 08:44:30.912973: step 61300, loss = 2.28, batch loss = 2.23 (36.7 examples/sec; 0.218 sec/batch; 16h:26m:30s remains)
2017-12-08 08:44:31.193344: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4286976 -4.42871 -4.4287243 -4.4287295 -4.4287329 -4.4287329 -4.4287214 -4.4287157 -4.42872 -4.4287291 -4.4287333 -4.4287319 -4.4287391 -4.4287543 -4.4287925][-4.4287686 -4.42878 -4.4287906 -4.4287939 -4.4287906 -4.4287782 -4.4287529 -4.42874 -4.4287415 -4.4287453 -4.4287462 -4.4287386 -4.4287395 -4.428751 -4.4287872][-4.4288173 -4.4288225 -4.4288187 -4.4288054 -4.4287815 -4.4287496 -4.4287109 -4.4286947 -4.4287043 -4.4287205 -4.4287267 -4.4287224 -4.4287238 -4.4287333 -4.42877][-4.4287963 -4.4287982 -4.4287815 -4.4287477 -4.4287009 -4.4286532 -4.4286156 -4.4286051 -4.4286256 -4.4286656 -4.4286957 -4.4287062 -4.428709 -4.4287162 -4.4287539][-4.4287868 -4.4287882 -4.4287577 -4.4286995 -4.4286256 -4.4285583 -4.4285154 -4.42851 -4.4285464 -4.4286208 -4.4286857 -4.4287167 -4.42872 -4.4287157 -4.4287448][-4.4287872 -4.4287872 -4.4287434 -4.428659 -4.4285579 -4.4284682 -4.4284105 -4.4283981 -4.4284396 -4.4285364 -4.4286318 -4.4286895 -4.4287004 -4.4286871 -4.4287071][-4.4287214 -4.4287219 -4.4286733 -4.42858 -4.4284706 -4.4283767 -4.4283142 -4.4282918 -4.428329 -4.428431 -4.4285455 -4.4286156 -4.4286289 -4.4286113 -4.4286327][-4.4286342 -4.4286461 -4.428618 -4.4285493 -4.4284692 -4.428401 -4.4283471 -4.4283171 -4.4283395 -4.4284291 -4.4285378 -4.4286032 -4.4286103 -4.4285874 -4.4286075][-4.428647 -4.4286761 -4.4286795 -4.4286518 -4.4286118 -4.428575 -4.4285264 -4.4284844 -4.4284868 -4.428546 -4.4286251 -4.428668 -4.42866 -4.4286327 -4.4286447][-4.4287291 -4.4287591 -4.428771 -4.4287586 -4.4287362 -4.4287114 -4.4286652 -4.4286208 -4.4286146 -4.42866 -4.4287176 -4.4287395 -4.4287233 -4.4286971 -4.4287038][-4.4287992 -4.428822 -4.428833 -4.428823 -4.4288025 -4.4287796 -4.4287424 -4.4287076 -4.4287071 -4.4287462 -4.4287906 -4.428803 -4.4287887 -4.42877 -4.4287748][-4.4288554 -4.4288745 -4.4288855 -4.4288769 -4.4288573 -4.4288368 -4.4288073 -4.4287796 -4.4287834 -4.4288163 -4.4288549 -4.4288688 -4.4288597 -4.4288468 -4.4288492][-4.4289045 -4.4289227 -4.4289346 -4.4289293 -4.4289126 -4.428894 -4.4288673 -4.428843 -4.4288468 -4.4288778 -4.428916 -4.4289331 -4.4289279 -4.4289174 -4.4289165][-4.4289379 -4.4289522 -4.4289627 -4.4289622 -4.4289527 -4.4289432 -4.4289293 -4.428915 -4.42892 -4.4289465 -4.4289784 -4.4289956 -4.4289927 -4.4289837 -4.4289765][-4.4289584 -4.4289675 -4.4289746 -4.4289742 -4.4289689 -4.4289646 -4.42896 -4.4289556 -4.4289618 -4.4289832 -4.4290094 -4.4290233 -4.4290237 -4.4290185 -4.4290094]]...]
INFO - root - 2017-12-08 08:44:33.442207: step 61310, loss = 2.28, batch loss = 2.23 (34.6 examples/sec; 0.231 sec/batch; 17h:23m:57s remains)
INFO - root - 2017-12-08 08:44:35.670208: step 61320, loss = 2.28, batch loss = 2.23 (37.1 examples/sec; 0.215 sec/batch; 16h:13m:53s remains)
INFO - root - 2017-12-08 08:44:37.902590: step 61330, loss = 2.28, batch loss = 2.23 (36.0 examples/sec; 0.222 sec/batch; 16h:43m:48s remains)
INFO - root - 2017-12-08 08:44:40.134835: step 61340, loss = 2.28, batch loss = 2.23 (36.0 examples/sec; 0.222 sec/batch; 16h:43m:51s remains)
INFO - root - 2017-12-08 08:44:42.366088: step 61350, loss = 2.28, batch loss = 2.23 (37.2 examples/sec; 0.215 sec/batch; 16h:12m:17s remains)
INFO - root - 2017-12-08 08:44:44.611285: step 61360, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 16h:48m:24s remains)
INFO - root - 2017-12-08 08:44:46.826135: step 61370, loss = 2.28, batch loss = 2.23 (34.8 examples/sec; 0.230 sec/batch; 17h:19m:48s remains)
INFO - root - 2017-12-08 08:44:49.069099: step 61380, loss = 2.28, batch loss = 2.23 (34.7 examples/sec; 0.231 sec/batch; 17h:22m:21s remains)
INFO - root - 2017-12-08 08:44:51.299689: step 61390, loss = 2.28, batch loss = 2.23 (35.3 examples/sec; 0.226 sec/batch; 17h:03m:20s remains)
INFO - root - 2017-12-08 08:44:53.578593: step 61400, loss = 2.28, batch loss = 2.23 (34.6 examples/sec; 0.231 sec/batch; 17h:23m:18s remains)
2017-12-08 08:44:53.879360: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288096 -4.4287953 -4.42879 -4.4287968 -4.4288073 -4.4288125 -4.4288278 -4.4288454 -4.4288535 -4.4288549 -4.4288564 -4.428853 -4.4288478 -4.4288497 -4.4288573][-4.4287534 -4.4287376 -4.4287324 -4.4287391 -4.42875 -4.4287624 -4.42879 -4.4288177 -4.428833 -4.4288392 -4.428844 -4.4288445 -4.4288421 -4.428843 -4.4288487][-4.4286733 -4.42866 -4.4286561 -4.4286628 -4.4286737 -4.4286923 -4.4287353 -4.4287753 -4.4287977 -4.4288049 -4.42881 -4.4288139 -4.4288182 -4.4288235 -4.42883][-4.4285941 -4.428586 -4.4285874 -4.4285903 -4.4285927 -4.4286084 -4.4286594 -4.4287081 -4.4287319 -4.4287357 -4.4287324 -4.4287338 -4.4287448 -4.4287581 -4.4287729][-4.4285541 -4.428546 -4.428546 -4.4285374 -4.4285159 -4.4285121 -4.4285569 -4.4286056 -4.4286313 -4.4286342 -4.4286232 -4.428616 -4.4286265 -4.4286466 -4.4286728][-4.4285693 -4.4285522 -4.4285407 -4.4285116 -4.4284611 -4.4284253 -4.42844 -4.4284787 -4.428515 -4.4285312 -4.4285226 -4.4285083 -4.4285126 -4.4285312 -4.4285569][-4.4286351 -4.4286065 -4.4285822 -4.4285378 -4.428472 -4.4284091 -4.4283757 -4.4283843 -4.4284291 -4.428463 -4.4284658 -4.428452 -4.4284492 -4.4284568 -4.4284611][-4.4287128 -4.4286861 -4.42866 -4.4286208 -4.428566 -4.4285026 -4.4284425 -4.4284167 -4.4284387 -4.4284616 -4.4284654 -4.4284568 -4.428453 -4.4284458 -4.4284172][-4.428762 -4.4287453 -4.4287286 -4.4287066 -4.4286747 -4.4286308 -4.428575 -4.4285369 -4.4285293 -4.4285269 -4.4285212 -4.4285159 -4.4285154 -4.4284992 -4.4284563][-4.4287696 -4.4287648 -4.4287586 -4.4287524 -4.4287419 -4.4287195 -4.4286814 -4.4286432 -4.4286213 -4.4286089 -4.4286032 -4.4286003 -4.4286 -4.4285822 -4.4285417][-4.4287534 -4.4287558 -4.4287562 -4.42876 -4.4287624 -4.4287558 -4.4287319 -4.4286995 -4.428678 -4.4286718 -4.4286723 -4.4286723 -4.4286742 -4.4286623 -4.4286332][-4.4287448 -4.4287467 -4.42875 -4.4287572 -4.4287653 -4.4287686 -4.4287567 -4.42873 -4.4287095 -4.4287076 -4.4287133 -4.4287167 -4.4287214 -4.4287171 -4.4287057][-4.4287572 -4.4287558 -4.4287586 -4.4287658 -4.4287734 -4.4287791 -4.4287782 -4.428762 -4.4287434 -4.4287424 -4.4287477 -4.4287491 -4.4287534 -4.4287553 -4.4287581][-4.428791 -4.4287882 -4.4287906 -4.4287977 -4.428803 -4.4288077 -4.4288139 -4.4288068 -4.4287939 -4.428791 -4.428793 -4.4287896 -4.4287877 -4.4287887 -4.4287939][-4.4288473 -4.4288454 -4.4288483 -4.4288549 -4.4288597 -4.428864 -4.4288721 -4.42887 -4.4288611 -4.4288559 -4.4288535 -4.4288464 -4.4288387 -4.4288354 -4.4288368]]...]
INFO - root - 2017-12-08 08:44:56.096528: step 61410, loss = 2.28, batch loss = 2.23 (36.8 examples/sec; 0.217 sec/batch; 16h:22m:14s remains)
INFO - root - 2017-12-08 08:44:58.331189: step 61420, loss = 2.28, batch loss = 2.23 (36.8 examples/sec; 0.217 sec/batch; 16h:21m:09s remains)
INFO - root - 2017-12-08 08:45:00.565640: step 61430, loss = 2.28, batch loss = 2.23 (35.4 examples/sec; 0.226 sec/batch; 17h:00m:02s remains)
INFO - root - 2017-12-08 08:45:02.827124: step 61440, loss = 2.28, batch loss = 2.23 (35.8 examples/sec; 0.223 sec/batch; 16h:49m:21s remains)
INFO - root - 2017-12-08 08:45:05.078264: step 61450, loss = 2.28, batch loss = 2.23 (36.9 examples/sec; 0.217 sec/batch; 16h:19m:52s remains)
INFO - root - 2017-12-08 08:45:07.332880: step 61460, loss = 2.28, batch loss = 2.23 (33.6 examples/sec; 0.238 sec/batch; 17h:56m:03s remains)
INFO - root - 2017-12-08 08:45:09.576914: step 61470, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 16h:30m:13s remains)
INFO - root - 2017-12-08 08:45:11.792697: step 61480, loss = 2.28, batch loss = 2.23 (36.8 examples/sec; 0.218 sec/batch; 16h:22m:42s remains)
INFO - root - 2017-12-08 08:45:14.030182: step 61490, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 16h:32m:11s remains)
INFO - root - 2017-12-08 08:45:16.263069: step 61500, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.222 sec/batch; 16h:42m:02s remains)
2017-12-08 08:45:16.580821: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289508 -4.4289346 -4.42892 -4.428915 -4.4289169 -4.4289241 -4.4289327 -4.428936 -4.42894 -4.4289465 -4.4289522 -4.42895 -4.4289427 -4.4289351 -4.4289322][-4.4289327 -4.4289107 -4.4288945 -4.4288907 -4.4288917 -4.4288993 -4.4289088 -4.4289131 -4.4289227 -4.4289374 -4.4289489 -4.4289451 -4.4289336 -4.4289207 -4.428915][-4.4289036 -4.4288745 -4.4288564 -4.428854 -4.428854 -4.4288578 -4.4288664 -4.42887 -4.4288874 -4.428916 -4.4289355 -4.4289303 -4.4289112 -4.4288974 -4.4288898][-4.4288726 -4.4288335 -4.4288 -4.428781 -4.4287696 -4.4287624 -4.4287634 -4.4287696 -4.4288044 -4.4288549 -4.428884 -4.4288754 -4.4288511 -4.4288387 -4.4288306][-4.4288492 -4.4287953 -4.4287376 -4.4286914 -4.428668 -4.4286504 -4.4286327 -4.4286256 -4.4286647 -4.4287314 -4.4287724 -4.4287634 -4.4287386 -4.4287324 -4.4287295][-4.4288235 -4.4287577 -4.4286766 -4.4286017 -4.4285645 -4.4285393 -4.4284945 -4.4284487 -4.4284644 -4.428546 -4.4286056 -4.4286056 -4.4285908 -4.4286 -4.4286046][-4.4287958 -4.4287181 -4.428617 -4.42852 -4.4284663 -4.4284234 -4.4283276 -4.4282117 -4.4281836 -4.42829 -4.4284029 -4.4284453 -4.4284534 -4.4284778 -4.4284968][-4.4287939 -4.4287167 -4.4286113 -4.4285021 -4.4284263 -4.4283471 -4.4281993 -4.4279914 -4.4278903 -4.4280553 -4.428257 -4.4283576 -4.4283919 -4.4284148 -4.4284377][-4.4288135 -4.42875 -4.4286656 -4.4285817 -4.4285245 -4.4284453 -4.4282885 -4.4280562 -4.4279318 -4.428102 -4.428308 -4.4284205 -4.4284544 -4.4284511 -4.4284582][-4.4288363 -4.4287887 -4.4287286 -4.4286718 -4.4286356 -4.4285779 -4.4284678 -4.4283185 -4.4282613 -4.4283628 -4.4284873 -4.4285655 -4.42859 -4.4285603 -4.42854][-4.4288607 -4.4288263 -4.4287958 -4.4287639 -4.4287395 -4.4286957 -4.4286313 -4.4285617 -4.428544 -4.4285984 -4.4286585 -4.4287043 -4.4287181 -4.4286819 -4.4286489][-4.4288821 -4.4288516 -4.4288282 -4.4288068 -4.4287953 -4.4287748 -4.4287529 -4.4287291 -4.4287238 -4.4287491 -4.4287744 -4.4287934 -4.4288015 -4.4287796 -4.4287572][-4.4288988 -4.4288707 -4.4288478 -4.4288335 -4.4288359 -4.4288387 -4.4288435 -4.4288383 -4.428834 -4.4288397 -4.428844 -4.4288464 -4.4288478 -4.4288397 -4.4288297][-4.4289141 -4.4288921 -4.428875 -4.4288659 -4.4288726 -4.4288836 -4.4288945 -4.4288979 -4.4288917 -4.428885 -4.4288788 -4.428875 -4.4288735 -4.4288688 -4.4288635][-4.4289322 -4.4289184 -4.4289074 -4.4289041 -4.4289083 -4.4289122 -4.4289169 -4.4289179 -4.42891 -4.4289017 -4.4288955 -4.428894 -4.4288926 -4.4288864 -4.428884]]...]
INFO - root - 2017-12-08 08:45:18.808579: step 61510, loss = 2.28, batch loss = 2.23 (34.5 examples/sec; 0.232 sec/batch; 17h:26m:53s remains)
INFO - root - 2017-12-08 08:45:21.055910: step 61520, loss = 2.28, batch loss = 2.23 (34.7 examples/sec; 0.231 sec/batch; 17h:21m:33s remains)
INFO - root - 2017-12-08 08:45:23.318494: step 61530, loss = 2.28, batch loss = 2.23 (34.4 examples/sec; 0.232 sec/batch; 17h:29m:26s remains)
INFO - root - 2017-12-08 08:45:25.563227: step 61540, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 16h:45m:14s remains)
INFO - root - 2017-12-08 08:45:27.794833: step 61550, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 16h:32m:48s remains)
INFO - root - 2017-12-08 08:45:30.036274: step 61560, loss = 2.28, batch loss = 2.23 (35.1 examples/sec; 0.228 sec/batch; 17h:08m:40s remains)
INFO - root - 2017-12-08 08:45:32.275092: step 61570, loss = 2.28, batch loss = 2.23 (35.6 examples/sec; 0.225 sec/batch; 16h:55m:06s remains)
INFO - root - 2017-12-08 08:45:34.479260: step 61580, loss = 2.28, batch loss = 2.23 (34.8 examples/sec; 0.230 sec/batch; 17h:19m:25s remains)
INFO - root - 2017-12-08 08:45:36.712788: step 61590, loss = 2.28, batch loss = 2.23 (35.3 examples/sec; 0.227 sec/batch; 17h:03m:12s remains)
INFO - root - 2017-12-08 08:45:38.978764: step 61600, loss = 2.28, batch loss = 2.23 (36.7 examples/sec; 0.218 sec/batch; 16h:23m:20s remains)
2017-12-08 08:45:39.297439: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289842 -4.4289985 -4.4290261 -4.4290657 -4.4290948 -4.4291053 -4.42907 -4.4289589 -4.4287739 -4.4285426 -4.4283695 -4.428371 -4.428524 -4.4287076 -4.4288564][-4.4290571 -4.4290934 -4.429132 -4.4291553 -4.4291453 -4.42911 -4.4290385 -4.4289079 -4.4287324 -4.4285455 -4.4284344 -4.4284773 -4.428627 -4.4287882 -4.4289174][-4.4290934 -4.4291387 -4.4291716 -4.4291763 -4.4291372 -4.42907 -4.4289727 -4.4288368 -4.4286857 -4.4285641 -4.42852 -4.4285922 -4.4287324 -4.42887 -4.4289732][-4.4291172 -4.42916 -4.4291754 -4.4291539 -4.4290814 -4.4289818 -4.4288568 -4.428719 -4.4286075 -4.4285665 -4.4285893 -4.4286742 -4.4287968 -4.4289122 -4.4289927][-4.4291229 -4.4291563 -4.4291492 -4.42909 -4.4289689 -4.4288239 -4.4286594 -4.4285016 -4.4284248 -4.4284735 -4.4285736 -4.4286842 -4.4288039 -4.42891 -4.4289856][-4.4291039 -4.4291282 -4.4290967 -4.4289861 -4.4288054 -4.42861 -4.4283776 -4.4281573 -4.4281111 -4.4282718 -4.4284616 -4.4286275 -4.4287734 -4.4288907 -4.4289742][-4.429059 -4.4290786 -4.4290266 -4.4288745 -4.4286413 -4.4283748 -4.42805 -4.42777 -4.4277873 -4.4280553 -4.4283223 -4.428546 -4.4287291 -4.4288688 -4.4289618][-4.4290152 -4.4290342 -4.428977 -4.4288082 -4.4285474 -4.4282331 -4.4278603 -4.4275894 -4.4276853 -4.4279943 -4.4282856 -4.4285331 -4.4287281 -4.4288712 -4.4289646][-4.4289627 -4.4289989 -4.4289637 -4.428823 -4.4285884 -4.4283028 -4.4279943 -4.4278183 -4.4279218 -4.4281707 -4.4284148 -4.4286342 -4.4288025 -4.4289165 -4.4289918][-4.4289131 -4.4289756 -4.4289765 -4.428884 -4.4287076 -4.4284983 -4.4282961 -4.4282 -4.4282727 -4.4284391 -4.4286165 -4.4287825 -4.4289026 -4.4289789 -4.4290309][-4.4288735 -4.4289622 -4.42899 -4.4289351 -4.4288177 -4.4286852 -4.4285765 -4.428535 -4.4285827 -4.428688 -4.4288068 -4.4289117 -4.4289865 -4.4290323 -4.42906][-4.4288659 -4.4289646 -4.4290013 -4.428968 -4.4288931 -4.4288211 -4.4287834 -4.4287839 -4.42882 -4.4288797 -4.4289403 -4.4289913 -4.4290338 -4.4290605 -4.4290729][-4.4289041 -4.4289832 -4.4290032 -4.428968 -4.4289117 -4.4288759 -4.4288859 -4.4289184 -4.4289541 -4.4289846 -4.4290047 -4.4290209 -4.4290433 -4.4290609 -4.4290709][-4.4289632 -4.4290071 -4.4289932 -4.4289322 -4.4288626 -4.4288406 -4.4288836 -4.428946 -4.4289904 -4.4290113 -4.4290133 -4.4290133 -4.4290266 -4.4290457 -4.4290619][-4.4290152 -4.4290285 -4.4289837 -4.428894 -4.4288034 -4.4287829 -4.4288464 -4.4289308 -4.4289832 -4.4290018 -4.429 -4.4289961 -4.4290075 -4.4290318 -4.4290547]]...]
INFO - root - 2017-12-08 08:45:41.526901: step 61610, loss = 2.28, batch loss = 2.23 (37.5 examples/sec; 0.213 sec/batch; 16h:02m:23s remains)
INFO - root - 2017-12-08 08:45:43.757000: step 61620, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.220 sec/batch; 16h:34m:56s remains)
INFO - root - 2017-12-08 08:45:45.987061: step 61630, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 16h:28m:11s remains)
INFO - root - 2017-12-08 08:45:48.208444: step 61640, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.220 sec/batch; 16h:34m:45s remains)
INFO - root - 2017-12-08 08:45:50.452466: step 61650, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 16h:45m:54s remains)
INFO - root - 2017-12-08 08:45:52.693968: step 61660, loss = 2.28, batch loss = 2.23 (34.9 examples/sec; 0.229 sec/batch; 17h:13m:46s remains)
INFO - root - 2017-12-08 08:45:54.932902: step 61670, loss = 2.28, batch loss = 2.23 (35.7 examples/sec; 0.224 sec/batch; 16h:50m:30s remains)
INFO - root - 2017-12-08 08:45:57.159628: step 61680, loss = 2.28, batch loss = 2.23 (35.7 examples/sec; 0.224 sec/batch; 16h:50m:56s remains)
INFO - root - 2017-12-08 08:45:59.399916: step 61690, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.222 sec/batch; 16h:40m:16s remains)
INFO - root - 2017-12-08 08:46:01.654591: step 61700, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 16h:27m:56s remains)
2017-12-08 08:46:01.952444: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4286637 -4.4285522 -4.4284692 -4.4284468 -4.4284286 -4.4283977 -4.4283943 -4.4284396 -4.4284773 -4.4284983 -4.4285226 -4.4285593 -4.4285994 -4.4286132 -4.4285994][-4.4286456 -4.4285364 -4.4284635 -4.428453 -4.4284449 -4.4284015 -4.4283724 -4.4283934 -4.4284077 -4.4284253 -4.4284687 -4.428535 -4.4285955 -4.4286108 -4.4285774][-4.4286213 -4.4285541 -4.4285188 -4.4285126 -4.428493 -4.4284244 -4.4283781 -4.4283872 -4.4283957 -4.4284215 -4.428483 -4.4285593 -4.4286189 -4.4286146 -4.4285426][-4.4286127 -4.4285822 -4.4285665 -4.42855 -4.4285069 -4.42842 -4.4283729 -4.4283915 -4.42842 -4.4284663 -4.4285326 -4.4286017 -4.4286475 -4.4286313 -4.4285455][-4.4286237 -4.4286017 -4.4285774 -4.428544 -4.4284968 -4.428422 -4.4283915 -4.4284272 -4.4284863 -4.4285488 -4.4286141 -4.4286709 -4.4287014 -4.4286919 -4.4286251][-4.4286232 -4.4285975 -4.4285665 -4.4285336 -4.4284935 -4.4284391 -4.4284115 -4.4284482 -4.4285221 -4.4285989 -4.4286728 -4.4287338 -4.4287634 -4.4287691 -4.4287338][-4.4286318 -4.4286084 -4.428587 -4.4285731 -4.4285407 -4.4284863 -4.4284277 -4.428431 -4.4284997 -4.4285893 -4.4286819 -4.4287519 -4.4287887 -4.4288073 -4.4287992][-4.4286523 -4.4286427 -4.4286427 -4.4286556 -4.428638 -4.4285851 -4.4285007 -4.4284663 -4.428514 -4.4285936 -4.4286823 -4.4287481 -4.4287953 -4.4288311 -4.4288421][-4.4286814 -4.4286871 -4.4286985 -4.4287305 -4.4287367 -4.4287014 -4.4286203 -4.4285636 -4.4285784 -4.4286284 -4.4286981 -4.4287486 -4.4287972 -4.4288425 -4.4288659][-4.4287128 -4.4287286 -4.42874 -4.4287682 -4.4287872 -4.4287767 -4.428719 -4.4286518 -4.4286294 -4.4286485 -4.4286976 -4.428741 -4.4287863 -4.4288297 -4.428853][-4.4287348 -4.4287581 -4.4287672 -4.4287839 -4.4288034 -4.4288063 -4.4287643 -4.4286909 -4.4286485 -4.428658 -4.4287024 -4.4287457 -4.4287839 -4.4288144 -4.4288273][-4.4287357 -4.4287572 -4.4287577 -4.4287596 -4.428771 -4.4287653 -4.4287276 -4.4286594 -4.4286184 -4.4286375 -4.4286895 -4.4287419 -4.4287796 -4.4287968 -4.4287972][-4.4287119 -4.4287262 -4.4287148 -4.4287047 -4.4287057 -4.4286847 -4.4286423 -4.42859 -4.42857 -4.42861 -4.4286747 -4.4287372 -4.4287744 -4.4287806 -4.4287715][-4.4287205 -4.4287214 -4.4287024 -4.42869 -4.4286847 -4.42865 -4.4285955 -4.42855 -4.4285488 -4.4286036 -4.4286838 -4.4287496 -4.4287796 -4.4287744 -4.4287572][-4.4287643 -4.42875 -4.4287238 -4.4287105 -4.4287024 -4.4286647 -4.42861 -4.4285741 -4.428586 -4.4286404 -4.4287133 -4.4287705 -4.4287915 -4.4287791 -4.428761]]...]
INFO - root - 2017-12-08 08:46:04.195244: step 61710, loss = 2.28, batch loss = 2.23 (35.7 examples/sec; 0.224 sec/batch; 16h:51m:11s remains)
INFO - root - 2017-12-08 08:46:06.459087: step 61720, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 16h:36m:18s remains)
INFO - root - 2017-12-08 08:46:08.722535: step 61730, loss = 2.28, batch loss = 2.23 (34.8 examples/sec; 0.230 sec/batch; 17h:18m:26s remains)
INFO - root - 2017-12-08 08:46:10.952076: step 61740, loss = 2.28, batch loss = 2.23 (35.8 examples/sec; 0.223 sec/batch; 16h:47m:40s remains)
INFO - root - 2017-12-08 08:46:13.167165: step 61750, loss = 2.28, batch loss = 2.23 (36.9 examples/sec; 0.217 sec/batch; 16h:18m:46s remains)
INFO - root - 2017-12-08 08:46:15.408382: step 61760, loss = 2.28, batch loss = 2.23 (35.3 examples/sec; 0.227 sec/batch; 17h:03m:50s remains)
INFO - root - 2017-12-08 08:46:17.689642: step 61770, loss = 2.28, batch loss = 2.23 (33.6 examples/sec; 0.238 sec/batch; 17h:55m:00s remains)
INFO - root - 2017-12-08 08:46:19.962873: step 61780, loss = 2.28, batch loss = 2.23 (34.0 examples/sec; 0.235 sec/batch; 17h:41m:20s remains)
INFO - root - 2017-12-08 08:46:22.216108: step 61790, loss = 2.28, batch loss = 2.23 (36.0 examples/sec; 0.222 sec/batch; 16h:42m:29s remains)
INFO - root - 2017-12-08 08:46:24.452478: step 61800, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.220 sec/batch; 16h:33m:28s remains)
2017-12-08 08:46:24.777447: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288793 -4.4288907 -4.428905 -4.4289145 -4.4289179 -4.4289188 -4.4289193 -4.4289193 -4.4289203 -4.4289217 -4.4289227 -4.42892 -4.428915 -4.4289103 -4.4289069][-4.4289103 -4.4289222 -4.428937 -4.428947 -4.4289508 -4.4289513 -4.42895 -4.4289484 -4.4289489 -4.4289513 -4.4289541 -4.4289546 -4.4289522 -4.4289474 -4.4289422][-4.4289446 -4.4289546 -4.4289632 -4.428968 -4.4289656 -4.4289579 -4.4289494 -4.4289455 -4.4289494 -4.4289584 -4.4289694 -4.428978 -4.4289808 -4.4289775 -4.4289718][-4.4289494 -4.4289556 -4.428957 -4.4289532 -4.4289412 -4.4289222 -4.4289041 -4.4288974 -4.4289055 -4.4289246 -4.4289489 -4.4289703 -4.4289827 -4.4289837 -4.4289808][-4.4289179 -4.4289184 -4.4289117 -4.4288974 -4.4288721 -4.4288373 -4.4288058 -4.4287906 -4.4288 -4.4288292 -4.4288712 -4.4289145 -4.4289451 -4.4289594 -4.4289675][-4.4288697 -4.4288659 -4.4288526 -4.4288292 -4.428793 -4.4287443 -4.4286952 -4.4286623 -4.4286637 -4.4286976 -4.4287553 -4.4288192 -4.4288707 -4.4289045 -4.4289312][-4.4288235 -4.4288173 -4.4288011 -4.4287758 -4.4287381 -4.4286909 -4.428637 -4.4285932 -4.42858 -4.4286036 -4.42866 -4.4287286 -4.4287891 -4.4288349 -4.4288783][-4.4287963 -4.428792 -4.4287767 -4.4287529 -4.4287238 -4.4286919 -4.4286518 -4.4286127 -4.42859 -4.4285946 -4.4286289 -4.4286795 -4.4287324 -4.42878 -4.4288311][-4.4288187 -4.4288192 -4.4288044 -4.4287786 -4.4287515 -4.4287286 -4.4287047 -4.4286809 -4.4286656 -4.4286671 -4.4286857 -4.4287152 -4.4287467 -4.4287791 -4.4288206][-4.4288669 -4.4288678 -4.4288487 -4.4288168 -4.4287858 -4.4287658 -4.4287539 -4.4287472 -4.4287505 -4.4287634 -4.42878 -4.4287939 -4.4288058 -4.4288197 -4.4288449][-4.4288964 -4.428895 -4.4288764 -4.4288454 -4.4288187 -4.4288054 -4.428803 -4.4288077 -4.4288206 -4.4288397 -4.428854 -4.4288592 -4.4288592 -4.4288621 -4.428874][-4.4289112 -4.4289103 -4.428895 -4.4288721 -4.4288554 -4.42885 -4.4288526 -4.4288588 -4.42887 -4.4288855 -4.4288969 -4.4288964 -4.4288912 -4.4288864 -4.4288883][-4.428926 -4.4289241 -4.4289117 -4.4288955 -4.428885 -4.4288826 -4.4288831 -4.428884 -4.4288898 -4.4288993 -4.428906 -4.4289026 -4.428895 -4.428885 -4.4288783][-4.4289432 -4.4289412 -4.4289308 -4.4289188 -4.4289131 -4.4289126 -4.4289107 -4.428905 -4.4289007 -4.4288974 -4.4288926 -4.4288816 -4.4288683 -4.4288526 -4.4288397][-4.4289594 -4.4289565 -4.4289474 -4.42894 -4.4289374 -4.4289384 -4.4289341 -4.4289227 -4.4289093 -4.428895 -4.4288793 -4.4288626 -4.4288468 -4.4288282 -4.4288106]]...]
INFO - root - 2017-12-08 08:46:27.024735: step 61810, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 16h:36m:42s remains)
INFO - root - 2017-12-08 08:46:29.291125: step 61820, loss = 2.28, batch loss = 2.23 (34.9 examples/sec; 0.229 sec/batch; 17h:13m:18s remains)
INFO - root - 2017-12-08 08:46:31.533090: step 61830, loss = 2.28, batch loss = 2.23 (34.3 examples/sec; 0.233 sec/batch; 17h:30m:54s remains)
INFO - root - 2017-12-08 08:46:33.769920: step 61840, loss = 2.28, batch loss = 2.23 (35.8 examples/sec; 0.224 sec/batch; 16h:49m:25s remains)
INFO - root - 2017-12-08 08:46:36.016957: step 61850, loss = 2.28, batch loss = 2.23 (36.9 examples/sec; 0.217 sec/batch; 16h:18m:05s remains)
INFO - root - 2017-12-08 08:46:38.237647: step 61860, loss = 2.28, batch loss = 2.23 (36.8 examples/sec; 0.218 sec/batch; 16h:21m:14s remains)
INFO - root - 2017-12-08 08:46:40.506437: step 61870, loss = 2.28, batch loss = 2.23 (34.9 examples/sec; 0.229 sec/batch; 17h:13m:25s remains)
INFO - root - 2017-12-08 08:46:42.744321: step 61880, loss = 2.28, batch loss = 2.23 (36.0 examples/sec; 0.222 sec/batch; 16h:42m:34s remains)
INFO - root - 2017-12-08 08:46:45.003298: step 61890, loss = 2.28, batch loss = 2.23 (35.4 examples/sec; 0.226 sec/batch; 16h:58m:59s remains)
INFO - root - 2017-12-08 08:46:47.256254: step 61900, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 16h:28m:33s remains)
2017-12-08 08:46:47.582933: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288821 -4.4289012 -4.4288974 -4.4288545 -4.4287858 -4.4286885 -4.4286 -4.4285793 -4.4286213 -4.4286952 -4.4287782 -4.4288597 -4.4289141 -4.4289355 -4.4289494][-4.4288864 -4.4289074 -4.4289041 -4.4288507 -4.4287629 -4.4286466 -4.4285469 -4.4285431 -4.4286084 -4.428699 -4.4287877 -4.4288712 -4.428925 -4.428947 -4.4289594][-4.42888 -4.4289 -4.4288993 -4.4288459 -4.4287505 -4.4286222 -4.4285092 -4.4285064 -4.4285803 -4.4286809 -4.4287786 -4.4288683 -4.4289265 -4.4289513 -4.4289637][-4.428874 -4.4288883 -4.4288869 -4.4288378 -4.4287477 -4.4286165 -4.4284911 -4.4284725 -4.4285369 -4.4286404 -4.4287524 -4.4288535 -4.42892 -4.4289527 -4.4289656][-4.428864 -4.4288712 -4.4288678 -4.4288254 -4.4287429 -4.4286146 -4.4284711 -4.4284229 -4.4284697 -4.42858 -4.4287114 -4.4288273 -4.4289055 -4.4289479 -4.4289641][-4.4288473 -4.4288468 -4.4288406 -4.4288044 -4.4287252 -4.4286 -4.4284482 -4.4283757 -4.4284091 -4.4285345 -4.4286876 -4.4288139 -4.4288969 -4.4289436 -4.4289622][-4.4288259 -4.4288211 -4.4288135 -4.4287825 -4.4287071 -4.4285851 -4.4284391 -4.4283619 -4.4283991 -4.4285412 -4.4287014 -4.4288259 -4.4289026 -4.428947 -4.4289637][-4.4287868 -4.4287791 -4.4287744 -4.4287539 -4.4286876 -4.428576 -4.4284472 -4.4283843 -4.4284358 -4.4285808 -4.42873 -4.4288454 -4.428916 -4.4289556 -4.4289689][-4.4287167 -4.4287066 -4.4287143 -4.4287152 -4.4286723 -4.4285841 -4.4284811 -4.4284425 -4.4284997 -4.42863 -4.4287596 -4.4288611 -4.4289274 -4.4289641 -4.4289737][-4.4286146 -4.4286017 -4.4286289 -4.4286561 -4.4286442 -4.4285903 -4.4285212 -4.4285026 -4.4285569 -4.4286656 -4.4287744 -4.4288721 -4.4289393 -4.4289722 -4.428978][-4.4285626 -4.4285431 -4.4285774 -4.4286175 -4.428627 -4.4285955 -4.4285445 -4.4285369 -4.4285879 -4.4286795 -4.4287796 -4.4288788 -4.428947 -4.4289756 -4.4289794][-4.4286084 -4.4285827 -4.4286075 -4.4286432 -4.4286523 -4.4286151 -4.4285522 -4.4285374 -4.4285822 -4.4286704 -4.4287767 -4.4288793 -4.4289484 -4.4289761 -4.4289794][-4.4286823 -4.428648 -4.4286566 -4.4286842 -4.4286838 -4.4286318 -4.4285469 -4.428514 -4.4285526 -4.428647 -4.4287658 -4.4288721 -4.4289412 -4.4289718 -4.4289775][-4.4287467 -4.4287109 -4.428709 -4.4287324 -4.4287329 -4.4286776 -4.4285789 -4.4285259 -4.4285569 -4.4286537 -4.4287715 -4.428874 -4.4289384 -4.428968 -4.4289761][-4.4287958 -4.4287643 -4.4287562 -4.428772 -4.4287734 -4.4287224 -4.4286208 -4.4285555 -4.4285846 -4.4286757 -4.4287844 -4.4288783 -4.428937 -4.4289665 -4.428977]]...]
INFO - root - 2017-12-08 08:46:49.806045: step 61910, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 16h:37m:08s remains)
INFO - root - 2017-12-08 08:46:52.036942: step 61920, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 16h:36m:20s remains)
INFO - root - 2017-12-08 08:46:54.276198: step 61930, loss = 2.28, batch loss = 2.23 (35.7 examples/sec; 0.224 sec/batch; 16h:50m:34s remains)
INFO - root - 2017-12-08 08:46:56.521459: step 61940, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.222 sec/batch; 16h:39m:53s remains)
INFO - root - 2017-12-08 08:46:58.761701: step 61950, loss = 2.28, batch loss = 2.23 (34.9 examples/sec; 0.229 sec/batch; 17h:12m:47s remains)
INFO - root - 2017-12-08 08:47:00.999318: step 61960, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.225 sec/batch; 16h:55m:11s remains)
INFO - root - 2017-12-08 08:47:03.261994: step 61970, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 16h:44m:53s remains)
INFO - root - 2017-12-08 08:47:05.507104: step 61980, loss = 2.28, batch loss = 2.23 (37.1 examples/sec; 0.215 sec/batch; 16h:10m:56s remains)
INFO - root - 2017-12-08 08:47:07.731444: step 61990, loss = 2.28, batch loss = 2.23 (36.9 examples/sec; 0.217 sec/batch; 16h:17m:21s remains)
INFO - root - 2017-12-08 08:47:09.964201: step 62000, loss = 2.28, batch loss = 2.23 (34.8 examples/sec; 0.230 sec/batch; 17h:17m:43s remains)
2017-12-08 08:47:10.267694: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.42854 -4.4285488 -4.428576 -4.4286318 -4.4286718 -4.4286823 -4.4286413 -4.4285388 -4.4284062 -4.4283595 -4.4284573 -4.4286041 -4.4287515 -4.4288697 -4.4289494][-4.428546 -4.4285607 -4.42859 -4.4286389 -4.42866 -4.428659 -4.428606 -4.4284887 -4.4283414 -4.4282913 -4.4284077 -4.4285769 -4.4287391 -4.4288588 -4.4289403][-4.4285321 -4.4285545 -4.428587 -4.4286337 -4.4286518 -4.42865 -4.4285831 -4.4284344 -4.4282632 -4.4282069 -4.4283495 -4.4285474 -4.4287276 -4.4288535 -4.4289351][-4.4284978 -4.4285245 -4.4285593 -4.428607 -4.4286323 -4.4286346 -4.4285512 -4.4283652 -4.4281731 -4.4281225 -4.428297 -4.428525 -4.4287186 -4.4288492 -4.4289312][-4.4284558 -4.4284811 -4.4285111 -4.4285569 -4.4285889 -4.4285855 -4.4284887 -4.4282856 -4.4281011 -4.4280705 -4.4282737 -4.4285212 -4.4287167 -4.4288492 -4.4289322][-4.4284639 -4.4284821 -4.4284911 -4.4285121 -4.4285264 -4.4284959 -4.4283795 -4.4281807 -4.4280276 -4.42803 -4.4282541 -4.4285121 -4.4287105 -4.4288497 -4.4289384][-4.4285021 -4.4285126 -4.4285135 -4.4285083 -4.4284811 -4.4284019 -4.4282546 -4.4280686 -4.4279523 -4.4279842 -4.4282217 -4.4284897 -4.4286947 -4.4288468 -4.4289436][-4.4285326 -4.4285445 -4.4285607 -4.4285536 -4.4285092 -4.4283996 -4.4282336 -4.4280577 -4.4279518 -4.4279847 -4.4282088 -4.4284739 -4.4286842 -4.4288464 -4.4289508][-4.4285564 -4.4285703 -4.4286003 -4.4286122 -4.4285903 -4.42849 -4.4283352 -4.4281673 -4.428041 -4.4280405 -4.4282274 -4.4284759 -4.4286866 -4.4288521 -4.4289579][-4.4285817 -4.4285822 -4.428616 -4.4286551 -4.4286547 -4.4285727 -4.4284348 -4.4282823 -4.4281454 -4.4281173 -4.4282746 -4.4284959 -4.4286938 -4.4288549 -4.42896][-4.4286203 -4.4286032 -4.42863 -4.4286804 -4.428679 -4.4286065 -4.4284792 -4.428349 -4.4282188 -4.4281778 -4.4283171 -4.4285178 -4.4286995 -4.428854 -4.4289594][-4.428647 -4.4286222 -4.4286375 -4.4286976 -4.4286861 -4.428618 -4.4284992 -4.4283857 -4.4282703 -4.4282217 -4.4283481 -4.4285378 -4.4287071 -4.4288545 -4.4289594][-4.4286485 -4.4286242 -4.4286437 -4.4287143 -4.4286919 -4.4286242 -4.4285092 -4.4283919 -4.4282866 -4.4282389 -4.4283624 -4.4285526 -4.4287148 -4.4288573 -4.4289608][-4.4286575 -4.428637 -4.4286604 -4.4287295 -4.4287019 -4.4286437 -4.42854 -4.4284096 -4.4283075 -4.4282603 -4.4283776 -4.4285631 -4.4287238 -4.4288635 -4.4289641][-4.428658 -4.428647 -4.4286766 -4.428731 -4.4287004 -4.4286671 -4.4285784 -4.4284477 -4.4283566 -4.428308 -4.4284091 -4.4285769 -4.4287314 -4.42887 -4.428968]]...]
INFO - root - 2017-12-08 08:47:12.526946: step 62010, loss = 2.28, batch loss = 2.23 (36.0 examples/sec; 0.222 sec/batch; 16h:41m:53s remains)
INFO - root - 2017-12-08 08:47:14.841324: step 62020, loss = 2.28, batch loss = 2.23 (35.6 examples/sec; 0.225 sec/batch; 16h:53m:36s remains)
INFO - root - 2017-12-08 08:47:17.120528: step 62030, loss = 2.28, batch loss = 2.23 (34.9 examples/sec; 0.230 sec/batch; 17h:14m:39s remains)
INFO - root - 2017-12-08 08:47:19.361378: step 62040, loss = 2.28, batch loss = 2.23 (34.5 examples/sec; 0.232 sec/batch; 17h:26m:29s remains)
INFO - root - 2017-12-08 08:47:21.611080: step 62050, loss = 2.28, batch loss = 2.23 (35.7 examples/sec; 0.224 sec/batch; 16h:51m:05s remains)
INFO - root - 2017-12-08 08:47:23.881132: step 62060, loss = 2.28, batch loss = 2.23 (36.9 examples/sec; 0.217 sec/batch; 16h:18m:28s remains)
INFO - root - 2017-12-08 08:47:26.100938: step 62070, loss = 2.28, batch loss = 2.23 (36.6 examples/sec; 0.219 sec/batch; 16h:26m:28s remains)
INFO - root - 2017-12-08 08:47:28.361889: step 62080, loss = 2.28, batch loss = 2.23 (36.6 examples/sec; 0.219 sec/batch; 16h:24m:55s remains)
INFO - root - 2017-12-08 08:47:30.609527: step 62090, loss = 2.28, batch loss = 2.23 (34.9 examples/sec; 0.229 sec/batch; 17h:11m:37s remains)
INFO - root - 2017-12-08 08:47:32.830112: step 62100, loss = 2.28, batch loss = 2.23 (35.7 examples/sec; 0.224 sec/batch; 16h:48m:40s remains)
2017-12-08 08:47:33.130834: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289913 -4.4289923 -4.4289908 -4.4289913 -4.4289951 -4.4289989 -4.4290004 -4.429 -4.4290009 -4.428997 -4.4289875 -4.4289804 -4.4289794 -4.4289808 -4.4289856][-4.4289608 -4.4289584 -4.4289522 -4.4289503 -4.4289575 -4.428966 -4.4289675 -4.4289651 -4.428966 -4.4289618 -4.4289474 -4.428937 -4.4289384 -4.4289422 -4.428947][-4.42891 -4.4288993 -4.4288845 -4.4288778 -4.4288893 -4.428905 -4.4289074 -4.4289036 -4.4289036 -4.4289017 -4.4288855 -4.4288716 -4.4288712 -4.428875 -4.4288797][-4.4288459 -4.428823 -4.4287968 -4.4287844 -4.4287949 -4.428812 -4.4288182 -4.4288163 -4.4288149 -4.4288116 -4.4287972 -4.4287829 -4.428781 -4.4287834 -4.4287863][-4.4287963 -4.4287591 -4.4287195 -4.4286976 -4.4286971 -4.4287066 -4.4287157 -4.4287219 -4.4287167 -4.4287119 -4.4287081 -4.4287028 -4.4287009 -4.4286976 -4.4286928][-4.4287696 -4.4287214 -4.4286652 -4.4286313 -4.4286184 -4.4286189 -4.4286356 -4.4286547 -4.4286504 -4.4286423 -4.4286528 -4.428659 -4.4286594 -4.4286528 -4.4286413][-4.4287462 -4.4286904 -4.4286189 -4.4285746 -4.4285579 -4.4285588 -4.4285865 -4.4286165 -4.428617 -4.4286032 -4.4286203 -4.4286404 -4.4286504 -4.4286518 -4.4286432][-4.4287305 -4.4286704 -4.4285893 -4.428544 -4.4285369 -4.4285445 -4.4285808 -4.4286165 -4.4286175 -4.4286022 -4.4286227 -4.4286547 -4.4286766 -4.4286909 -4.4286909][-4.4287362 -4.4286761 -4.4286003 -4.4285665 -4.4285746 -4.42859 -4.4286261 -4.428658 -4.4286585 -4.4286423 -4.4286594 -4.428699 -4.4287319 -4.4287615 -4.428771][-4.4287748 -4.4287214 -4.4286547 -4.4286304 -4.4286461 -4.4286609 -4.4286861 -4.4287124 -4.4287128 -4.4286919 -4.4286957 -4.4287324 -4.42877 -4.4288087 -4.4288316][-4.4288316 -4.4287796 -4.4287243 -4.4287105 -4.4287281 -4.4287333 -4.4287419 -4.4287581 -4.4287586 -4.4287376 -4.4287295 -4.428751 -4.4287806 -4.4288177 -4.4288497][-4.4288859 -4.4288354 -4.4287882 -4.4287806 -4.4287977 -4.4287953 -4.4287896 -4.4287906 -4.4287868 -4.4287739 -4.4287653 -4.42877 -4.4287806 -4.4288058 -4.4288325][-4.4288964 -4.4288554 -4.4288197 -4.4288182 -4.4288273 -4.4288168 -4.428802 -4.4287982 -4.4288006 -4.4288087 -4.4288154 -4.4288139 -4.4288096 -4.4288163 -4.4288259][-4.4288688 -4.4288406 -4.4288177 -4.4288177 -4.4288149 -4.4287963 -4.4287767 -4.4287686 -4.4287853 -4.4288197 -4.4288511 -4.42886 -4.428853 -4.4288478 -4.4288387][-4.4288354 -4.4288197 -4.4288068 -4.4288063 -4.4287949 -4.4287705 -4.4287434 -4.4287286 -4.4287448 -4.428792 -4.4288435 -4.4288673 -4.4288683 -4.42886 -4.4288416]]...]
INFO - root - 2017-12-08 08:47:35.388572: step 62110, loss = 2.28, batch loss = 2.23 (34.9 examples/sec; 0.229 sec/batch; 17h:13m:53s remains)
INFO - root - 2017-12-08 08:47:37.619250: step 62120, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.225 sec/batch; 16h:55m:10s remains)
INFO - root - 2017-12-08 08:47:39.842369: step 62130, loss = 2.28, batch loss = 2.23 (37.5 examples/sec; 0.213 sec/batch; 16h:00m:12s remains)
INFO - root - 2017-12-08 08:47:42.070117: step 62140, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 16h:26m:54s remains)
INFO - root - 2017-12-08 08:47:44.296498: step 62150, loss = 2.28, batch loss = 2.23 (36.6 examples/sec; 0.219 sec/batch; 16h:25m:21s remains)
INFO - root - 2017-12-08 08:47:46.534927: step 62160, loss = 2.28, batch loss = 2.23 (37.0 examples/sec; 0.216 sec/batch; 16h:13m:10s remains)
INFO - root - 2017-12-08 08:47:48.795055: step 62170, loss = 2.28, batch loss = 2.23 (33.4 examples/sec; 0.239 sec/batch; 17h:58m:47s remains)
INFO - root - 2017-12-08 08:47:51.030960: step 62180, loss = 2.28, batch loss = 2.23 (35.0 examples/sec; 0.228 sec/batch; 17h:08m:51s remains)
INFO - root - 2017-12-08 08:47:53.283853: step 62190, loss = 2.28, batch loss = 2.23 (35.6 examples/sec; 0.225 sec/batch; 16h:52m:37s remains)
INFO - root - 2017-12-08 08:47:55.537302: step 62200, loss = 2.28, batch loss = 2.23 (35.7 examples/sec; 0.224 sec/batch; 16h:48m:45s remains)
2017-12-08 08:47:55.837186: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4287939 -4.4288335 -4.4288607 -4.4288664 -4.4288368 -4.4287963 -4.4287677 -4.4287448 -4.4287271 -4.4287324 -4.4287691 -4.4288063 -4.4288082 -4.4287882 -4.4287639][-4.4287772 -4.4288011 -4.4288268 -4.4288435 -4.4288316 -4.4288054 -4.4287825 -4.4287591 -4.4287329 -4.4287252 -4.4287553 -4.4287977 -4.4288082 -4.4287977 -4.4287796][-4.4287648 -4.4287658 -4.4287834 -4.4288082 -4.4288149 -4.4288077 -4.4287987 -4.4287848 -4.428762 -4.4287457 -4.4287615 -4.4287915 -4.4287963 -4.4287858 -4.428772][-4.4287667 -4.4287462 -4.4287472 -4.4287677 -4.4287844 -4.4287891 -4.4287939 -4.4287934 -4.4287853 -4.4287724 -4.4287782 -4.4287968 -4.4287925 -4.4287796 -4.4287653][-4.4287639 -4.4287357 -4.4287276 -4.4287367 -4.4287462 -4.42875 -4.4287605 -4.4287744 -4.4287829 -4.4287806 -4.4287839 -4.4287944 -4.4287848 -4.4287682 -4.4287505][-4.4287472 -4.4287243 -4.4287171 -4.42872 -4.4287138 -4.4286995 -4.4286995 -4.4287162 -4.4287391 -4.4287572 -4.4287715 -4.4287796 -4.42877 -4.4287519 -4.4287338][-4.4287291 -4.4287133 -4.4287128 -4.4287138 -4.4286942 -4.4286575 -4.4286337 -4.4286342 -4.428659 -4.4287028 -4.42874 -4.4287562 -4.428751 -4.4287319 -4.4287119][-4.4287362 -4.4287295 -4.4287362 -4.4287395 -4.4287119 -4.4286537 -4.4285884 -4.4285412 -4.4285493 -4.4286194 -4.428688 -4.4287238 -4.4287248 -4.4286966 -4.4286714][-4.4287643 -4.4287658 -4.4287748 -4.4287763 -4.4287434 -4.4286747 -4.4285817 -4.4284892 -4.42847 -4.428545 -4.4286304 -4.4286795 -4.4286857 -4.4286566 -4.4286284][-4.4287739 -4.4287829 -4.4287925 -4.428793 -4.4287686 -4.4287167 -4.4286356 -4.428544 -4.4285049 -4.4285474 -4.4286137 -4.4286571 -4.4286661 -4.4286375 -4.4286041][-4.4287572 -4.4287724 -4.4287829 -4.4287839 -4.4287744 -4.4287567 -4.4287157 -4.4286571 -4.4286141 -4.4286175 -4.4286494 -4.4286757 -4.42868 -4.42865 -4.4286113][-4.4287343 -4.4287467 -4.4287486 -4.4287434 -4.4287467 -4.4287624 -4.4287648 -4.4287453 -4.4287162 -4.4286995 -4.4287028 -4.4287124 -4.4287119 -4.4286866 -4.4286561][-4.4287195 -4.4287224 -4.4287109 -4.4286952 -4.4287043 -4.4287434 -4.428782 -4.4287996 -4.428791 -4.4287691 -4.428751 -4.4287472 -4.4287424 -4.4287238 -4.4287109][-4.4287286 -4.4287238 -4.4287028 -4.4286747 -4.4286752 -4.4287181 -4.4287753 -4.4288163 -4.4288249 -4.4288058 -4.4287748 -4.4287577 -4.4287481 -4.4287405 -4.4287505][-4.4287715 -4.4287643 -4.4287415 -4.4287076 -4.4286952 -4.4287214 -4.4287667 -4.4288082 -4.4288263 -4.4288135 -4.428781 -4.4287548 -4.4287376 -4.4287367 -4.4287634]]...]
INFO - root - 2017-12-08 08:47:58.062004: step 62210, loss = 2.28, batch loss = 2.23 (37.2 examples/sec; 0.215 sec/batch; 16h:09m:58s remains)
INFO - root - 2017-12-08 08:48:00.280326: step 62220, loss = 2.28, batch loss = 2.23 (37.6 examples/sec; 0.213 sec/batch; 15h:59m:12s remains)
INFO - root - 2017-12-08 08:48:02.516656: step 62230, loss = 2.28, batch loss = 2.23 (36.0 examples/sec; 0.222 sec/batch; 16h:40m:41s remains)
INFO - root - 2017-12-08 08:48:04.752059: step 62240, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 16h:36m:25s remains)
INFO - root - 2017-12-08 08:48:06.993473: step 62250, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 16h:30m:02s remains)
INFO - root - 2017-12-08 08:48:09.281288: step 62260, loss = 2.28, batch loss = 2.23 (35.4 examples/sec; 0.226 sec/batch; 16h:57m:22s remains)
INFO - root - 2017-12-08 08:48:11.515618: step 62270, loss = 2.28, batch loss = 2.23 (37.0 examples/sec; 0.216 sec/batch; 16h:13m:39s remains)
INFO - root - 2017-12-08 08:48:13.749182: step 62280, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.221 sec/batch; 16h:37m:17s remains)
INFO - root - 2017-12-08 08:48:16.029407: step 62290, loss = 2.28, batch loss = 2.23 (35.2 examples/sec; 0.227 sec/batch; 17h:03m:12s remains)
INFO - root - 2017-12-08 08:48:18.275739: step 62300, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.226 sec/batch; 16h:55m:50s remains)
2017-12-08 08:48:18.598947: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288168 -4.4288359 -4.4288597 -4.428864 -4.4288445 -4.4288135 -4.4287786 -4.4287577 -4.4287581 -4.4287691 -4.4287848 -4.4288 -4.428812 -4.4288278 -4.4288611][-4.4288669 -4.4288769 -4.4288859 -4.4288664 -4.4288154 -4.4287586 -4.4287105 -4.4287 -4.4287233 -4.4287486 -4.4287734 -4.4287968 -4.4288077 -4.4288163 -4.4288416][-4.428906 -4.4289069 -4.4289012 -4.4288573 -4.4287696 -4.428678 -4.4286127 -4.4286103 -4.4286585 -4.4287052 -4.4287505 -4.4287925 -4.4288149 -4.4288244 -4.4288435][-4.42892 -4.42891 -4.428895 -4.4288397 -4.4287267 -4.4286022 -4.4285169 -4.4285154 -4.4285836 -4.4286594 -4.4287295 -4.428791 -4.4288325 -4.4288511 -4.4288707][-4.4289126 -4.4288979 -4.42888 -4.4288254 -4.4287052 -4.4285622 -4.4284577 -4.4284453 -4.4285259 -4.42863 -4.4287238 -4.4287987 -4.428854 -4.4288831 -4.4289079][-4.4288974 -4.4288898 -4.4288731 -4.4288111 -4.4286823 -4.428524 -4.4283957 -4.4283576 -4.428441 -4.4285803 -4.4287109 -4.4288082 -4.4288769 -4.4289131 -4.42894][-4.4289002 -4.4289 -4.428884 -4.4288158 -4.4286795 -4.4285092 -4.4283571 -4.428288 -4.4283595 -4.4285207 -4.4286852 -4.4288077 -4.4288917 -4.4289393 -4.428968][-4.4289188 -4.428926 -4.4289117 -4.42884 -4.4286985 -4.428525 -4.4283695 -4.4282904 -4.4283409 -4.4284916 -4.4286628 -4.4287977 -4.4288936 -4.4289522 -4.4289846][-4.4289284 -4.428947 -4.4289365 -4.4288611 -4.4287119 -4.4285316 -4.4283738 -4.4283032 -4.4283514 -4.4284887 -4.4286551 -4.4287944 -4.428896 -4.4289575 -4.4289894][-4.4289227 -4.4289432 -4.4289312 -4.4288487 -4.4286962 -4.4285145 -4.4283638 -4.4283147 -4.4283752 -4.4285131 -4.428679 -4.4288173 -4.428915 -4.4289665 -4.428988][-4.4289007 -4.4289184 -4.4289031 -4.4288177 -4.4286709 -4.4285021 -4.4283772 -4.428359 -4.4284348 -4.4285755 -4.4287357 -4.4288626 -4.4289441 -4.428978 -4.4289832][-4.4288664 -4.428884 -4.4288726 -4.4287987 -4.4286747 -4.4285331 -4.4284449 -4.4284577 -4.4285412 -4.4286671 -4.4288025 -4.428906 -4.4289665 -4.4289842 -4.4289765][-4.4288511 -4.4288754 -4.4288774 -4.428823 -4.4287257 -4.4286156 -4.4285583 -4.4285874 -4.4286637 -4.4287639 -4.428863 -4.4289374 -4.428978 -4.4289827 -4.4289646][-4.4288683 -4.4288993 -4.4289117 -4.4288745 -4.4288039 -4.4287252 -4.42869 -4.4287171 -4.4287758 -4.4288464 -4.4289093 -4.4289575 -4.4289818 -4.4289775 -4.4289508][-4.4289093 -4.4289374 -4.4289474 -4.42892 -4.4288716 -4.428823 -4.428803 -4.42882 -4.4288564 -4.4288969 -4.4289303 -4.4289594 -4.4289751 -4.428967 -4.428936]]...]
INFO - root - 2017-12-08 08:48:20.837982: step 62310, loss = 2.28, batch loss = 2.23 (34.6 examples/sec; 0.231 sec/batch; 17h:21m:31s remains)
INFO - root - 2017-12-08 08:48:23.074339: step 62320, loss = 2.28, batch loss = 2.23 (35.2 examples/sec; 0.227 sec/batch; 17h:01m:58s remains)
INFO - root - 2017-12-08 08:48:25.289937: step 62330, loss = 2.28, batch loss = 2.23 (38.3 examples/sec; 0.209 sec/batch; 15h:40m:31s remains)
INFO - root - 2017-12-08 08:48:27.522125: step 62340, loss = 2.28, batch loss = 2.23 (36.6 examples/sec; 0.218 sec/batch; 16h:23m:00s remains)
INFO - root - 2017-12-08 08:48:29.796633: step 62350, loss = 2.28, batch loss = 2.23 (34.0 examples/sec; 0.235 sec/batch; 17h:38m:10s remains)
INFO - root - 2017-12-08 08:48:32.050427: step 62360, loss = 2.28, batch loss = 2.23 (35.2 examples/sec; 0.228 sec/batch; 17h:04m:21s remains)
INFO - root - 2017-12-08 08:48:34.295687: step 62370, loss = 2.28, batch loss = 2.23 (34.5 examples/sec; 0.232 sec/batch; 17h:25m:07s remains)
INFO - root - 2017-12-08 08:48:36.524418: step 62380, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 16h:33m:38s remains)
INFO - root - 2017-12-08 08:48:38.747893: step 62390, loss = 2.28, batch loss = 2.23 (36.8 examples/sec; 0.218 sec/batch; 16h:19m:43s remains)
INFO - root - 2017-12-08 08:48:40.980840: step 62400, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.221 sec/batch; 16h:33m:22s remains)
2017-12-08 08:48:41.271567: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.428679 -4.4287057 -4.4287238 -4.4287219 -4.428719 -4.4287324 -4.42874 -4.4287281 -4.4287128 -4.4287062 -4.42871 -4.4287205 -4.4287219 -4.4287205 -4.4287329][-4.4286823 -4.4287062 -4.428719 -4.4287071 -4.4286885 -4.4286842 -4.4286919 -4.4286957 -4.4286928 -4.4286885 -4.4286923 -4.4287033 -4.4287066 -4.4287062 -4.4287205][-4.428721 -4.4287319 -4.4287305 -4.4287009 -4.4286528 -4.4286132 -4.428607 -4.4286246 -4.42864 -4.4286456 -4.4286628 -4.4286919 -4.4287105 -4.4287195 -4.4287305][-4.4287629 -4.4287577 -4.4287353 -4.4286823 -4.4286008 -4.42852 -4.4284911 -4.4285169 -4.4285469 -4.42857 -4.4286079 -4.4286628 -4.4287019 -4.4287233 -4.4287338][-4.4287848 -4.4287663 -4.4287195 -4.4286327 -4.4285173 -4.4284043 -4.4283538 -4.4283795 -4.4284258 -4.4284768 -4.4285502 -4.4286389 -4.4287071 -4.42875 -4.4287643][-4.4287548 -4.4287353 -4.4286761 -4.4285679 -4.428431 -4.4283009 -4.4282236 -4.4282346 -4.4283037 -4.428391 -4.4284992 -4.4286151 -4.4287057 -4.4287667 -4.4287882][-4.4286857 -4.4286728 -4.4286203 -4.428515 -4.4283767 -4.4282379 -4.4281416 -4.4281325 -4.4282131 -4.4283257 -4.4284472 -4.4285722 -4.4286742 -4.428751 -4.4287958][-4.4286351 -4.4286294 -4.4285913 -4.4285078 -4.42839 -4.4282565 -4.4281521 -4.428133 -4.42822 -4.4283485 -4.4284644 -4.428565 -4.4286513 -4.4287324 -4.4287972][-4.4286275 -4.4286289 -4.4286113 -4.428556 -4.4284711 -4.4283633 -4.4282689 -4.4282413 -4.428309 -4.428432 -4.4285264 -4.4285836 -4.4286389 -4.4287119 -4.4287806][-4.4286332 -4.4286346 -4.42863 -4.4286041 -4.4285583 -4.4284887 -4.4284153 -4.4283772 -4.4284196 -4.42852 -4.4285913 -4.4286165 -4.4286408 -4.428688 -4.4287372][-4.4286246 -4.4286385 -4.428647 -4.4286432 -4.4286232 -4.4285822 -4.4285293 -4.4284887 -4.4284997 -4.4285631 -4.4286156 -4.4286304 -4.4286413 -4.4286695 -4.428699][-4.4286151 -4.4286427 -4.4286671 -4.4286804 -4.4286771 -4.4286523 -4.428617 -4.4285765 -4.4285607 -4.4285865 -4.4286218 -4.4286404 -4.4286518 -4.4286685 -4.4286876][-4.4286456 -4.4286737 -4.4287043 -4.4287257 -4.4287324 -4.4287148 -4.4286866 -4.428648 -4.4286122 -4.4286017 -4.42862 -4.4286423 -4.4286532 -4.4286652 -4.4286852][-4.4287419 -4.4287548 -4.4287739 -4.4287925 -4.4287968 -4.4287829 -4.428761 -4.4287229 -4.4286709 -4.4286361 -4.4286437 -4.4286695 -4.4286795 -4.4286871 -4.4287133][-4.4288468 -4.4288487 -4.4288573 -4.4288692 -4.4288712 -4.428865 -4.4288511 -4.4288158 -4.4287639 -4.4287238 -4.4287295 -4.4287543 -4.42876 -4.4287629 -4.4287915]]...]
INFO - root - 2017-12-08 08:48:43.502194: step 62410, loss = 2.28, batch loss = 2.23 (34.8 examples/sec; 0.230 sec/batch; 17h:14m:53s remains)
INFO - root - 2017-12-08 08:48:45.744795: step 62420, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 16h:25m:46s remains)
INFO - root - 2017-12-08 08:48:48.019017: step 62430, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 16h:43m:02s remains)
INFO - root - 2017-12-08 08:48:50.246999: step 62440, loss = 2.28, batch loss = 2.23 (36.6 examples/sec; 0.218 sec/batch; 16h:23m:07s remains)
INFO - root - 2017-12-08 08:48:52.505524: step 62450, loss = 2.28, batch loss = 2.23 (36.7 examples/sec; 0.218 sec/batch; 16h:22m:14s remains)
INFO - root - 2017-12-08 08:48:54.770952: step 62460, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 16h:25m:24s remains)
INFO - root - 2017-12-08 08:48:57.003925: step 62470, loss = 2.28, batch loss = 2.23 (35.4 examples/sec; 0.226 sec/batch; 16h:55m:51s remains)
INFO - root - 2017-12-08 08:48:59.226174: step 62480, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.221 sec/batch; 16h:36m:10s remains)
INFO - root - 2017-12-08 08:49:01.492053: step 62490, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.225 sec/batch; 16h:54m:29s remains)
INFO - root - 2017-12-08 08:49:03.726775: step 62500, loss = 2.28, batch loss = 2.23 (34.3 examples/sec; 0.233 sec/batch; 17h:30m:13s remains)
2017-12-08 08:49:04.068953: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4286737 -4.4287114 -4.4287729 -4.4288015 -4.4288144 -4.4288125 -4.4287691 -4.4287238 -4.4286966 -4.4287171 -4.4287381 -4.4287105 -4.4286976 -4.4287081 -4.4287438][-4.4285569 -4.4285932 -4.4286819 -4.4287386 -4.428761 -4.4287667 -4.42872 -4.4286656 -4.4286275 -4.4286571 -4.4286914 -4.4286623 -4.4286418 -4.4286485 -4.4286804][-4.4284372 -4.4284644 -4.4285626 -4.4286366 -4.4286642 -4.4286718 -4.428627 -4.4285688 -4.4285326 -4.4285846 -4.4286308 -4.4286103 -4.4285879 -4.4285865 -4.4286194][-4.4283957 -4.4284096 -4.428504 -4.4285851 -4.4286146 -4.4286156 -4.4285703 -4.4285097 -4.4284778 -4.4285431 -4.4285936 -4.4285936 -4.4285903 -4.4285946 -4.4286294][-4.4283991 -4.4284205 -4.428514 -4.4285975 -4.4286318 -4.42863 -4.4285836 -4.4285121 -4.428473 -4.42853 -4.4285855 -4.4286046 -4.4286146 -4.42863 -4.4286609][-4.4283857 -4.4284124 -4.4285145 -4.428607 -4.4286542 -4.4286447 -4.4285769 -4.4284773 -4.4284258 -4.4284973 -4.4285812 -4.42863 -4.4286542 -4.428678 -4.4286976][-4.4283395 -4.4283652 -4.4284749 -4.4285793 -4.4286375 -4.4285955 -4.4284592 -4.4282827 -4.4282136 -4.4283304 -4.4284778 -4.4285779 -4.4286323 -4.4286833 -4.4287024][-4.4282684 -4.4283 -4.4284148 -4.4285278 -4.4285836 -4.4284997 -4.4282732 -4.4280005 -4.4279132 -4.4280825 -4.4282956 -4.4284372 -4.428515 -4.4285955 -4.4286404][-4.4282227 -4.428267 -4.4283838 -4.4285126 -4.4285779 -4.4284892 -4.4282618 -4.4280028 -4.4279246 -4.4280643 -4.4282551 -4.42838 -4.4284453 -4.4285226 -4.4285779][-4.4282618 -4.4283071 -4.4284091 -4.42853 -4.4286 -4.4285493 -4.4283972 -4.4282279 -4.4281721 -4.4282455 -4.4283614 -4.4284291 -4.4284487 -4.4284925 -4.4285421][-4.4283853 -4.4284234 -4.428503 -4.4286008 -4.428668 -4.4286518 -4.4285712 -4.4284749 -4.4284439 -4.4284735 -4.4285312 -4.4285436 -4.4285207 -4.42853 -4.4285669][-4.4285655 -4.4285975 -4.4286556 -4.4287262 -4.4287777 -4.4287777 -4.4287367 -4.4286809 -4.4286628 -4.4286766 -4.4287052 -4.4286985 -4.4286594 -4.4286489 -4.4286685][-4.4287505 -4.4287739 -4.428812 -4.4288611 -4.4288945 -4.4288931 -4.428864 -4.4288249 -4.4288135 -4.4288263 -4.4288416 -4.4288359 -4.4288044 -4.4287915 -4.4287992][-4.4288816 -4.4288979 -4.4289236 -4.4289575 -4.4289784 -4.4289718 -4.4289465 -4.4289165 -4.4289079 -4.4289174 -4.428925 -4.42892 -4.4289021 -4.428895 -4.4289][-4.4289613 -4.4289703 -4.4289856 -4.4290047 -4.4290123 -4.4290051 -4.4289889 -4.4289722 -4.4289646 -4.428966 -4.4289703 -4.4289689 -4.4289613 -4.42896 -4.4289651]]...]
INFO - root - 2017-12-08 08:49:06.280732: step 62510, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.221 sec/batch; 16h:32m:44s remains)
INFO - root - 2017-12-08 08:49:08.564270: step 62520, loss = 2.28, batch loss = 2.23 (35.7 examples/sec; 0.224 sec/batch; 16h:49m:32s remains)
INFO - root - 2017-12-08 08:49:10.789254: step 62530, loss = 2.28, batch loss = 2.23 (37.3 examples/sec; 0.214 sec/batch; 16h:04m:29s remains)
INFO - root - 2017-12-08 08:49:13.019415: step 62540, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.222 sec/batch; 16h:36m:38s remains)
INFO - root - 2017-12-08 08:49:15.251882: step 62550, loss = 2.28, batch loss = 2.23 (35.4 examples/sec; 0.226 sec/batch; 16h:56m:19s remains)
INFO - root - 2017-12-08 08:49:17.490509: step 62560, loss = 2.28, batch loss = 2.23 (35.6 examples/sec; 0.225 sec/batch; 16h:50m:17s remains)
INFO - root - 2017-12-08 08:49:19.718878: step 62570, loss = 2.28, batch loss = 2.23 (35.2 examples/sec; 0.227 sec/batch; 17h:03m:05s remains)
INFO - root - 2017-12-08 08:49:21.980518: step 62580, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 16h:34m:15s remains)
INFO - root - 2017-12-08 08:49:24.221401: step 62590, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 16h:33m:08s remains)
INFO - root - 2017-12-08 08:49:26.456995: step 62600, loss = 2.28, batch loss = 2.23 (36.0 examples/sec; 0.222 sec/batch; 16h:38m:25s remains)
2017-12-08 08:49:26.762917: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4290161 -4.4290085 -4.4289823 -4.4289508 -4.4289265 -4.4289155 -4.4289207 -4.4289327 -4.428947 -4.4289575 -4.428967 -4.4289756 -4.4289842 -4.4289908 -4.4289961][-4.4290271 -4.4290061 -4.4289613 -4.4289107 -4.4288716 -4.4288516 -4.4288583 -4.4288783 -4.428905 -4.42893 -4.4289508 -4.428966 -4.4289761 -4.4289851 -4.4289932][-4.4290257 -4.42899 -4.4289231 -4.4288497 -4.4287906 -4.4287581 -4.428761 -4.4287887 -4.4288325 -4.4288754 -4.4289155 -4.4289503 -4.4289689 -4.42898 -4.4289875][-4.4290142 -4.42897 -4.428885 -4.4287887 -4.4287009 -4.4286528 -4.4286528 -4.4286933 -4.4287515 -4.4288125 -4.4288726 -4.4289212 -4.4289513 -4.4289708 -4.4289775][-4.4289885 -4.4289351 -4.4288392 -4.4287181 -4.4286 -4.4285388 -4.4285407 -4.4286032 -4.42868 -4.4287472 -4.4288192 -4.428875 -4.4289174 -4.4289503 -4.4289651][-4.4289474 -4.4288912 -4.4287734 -4.4286103 -4.4284387 -4.4283347 -4.4283266 -4.4284348 -4.4285607 -4.428658 -4.4287424 -4.4288077 -4.4288654 -4.4289112 -4.4289379][-4.4288979 -4.428843 -4.4287014 -4.4284925 -4.428257 -4.4280629 -4.4280143 -4.4281912 -4.4283895 -4.428535 -4.4286451 -4.4287238 -4.4287934 -4.4288449 -4.4288774][-4.4288216 -4.4287758 -4.4286385 -4.4284143 -4.4281211 -4.4278035 -4.4276576 -4.4278879 -4.42818 -4.4283872 -4.4285312 -4.428627 -4.4287047 -4.4287572 -4.4287925][-4.4287744 -4.4287395 -4.4286356 -4.4284439 -4.4281569 -4.4277883 -4.4275537 -4.4277396 -4.4280353 -4.428268 -4.4284368 -4.4285545 -4.428628 -4.4286647 -4.4286919][-4.4287968 -4.42877 -4.4287095 -4.428587 -4.4283881 -4.4281268 -4.4279594 -4.4280276 -4.4281754 -4.4283342 -4.428473 -4.428576 -4.4286232 -4.4286294 -4.4286385][-4.4288411 -4.4288158 -4.4287791 -4.4287105 -4.4286036 -4.4284768 -4.4284124 -4.4284258 -4.4284673 -4.4285474 -4.4286375 -4.4287019 -4.4287167 -4.4287 -4.4286923][-4.4288993 -4.4288788 -4.4288588 -4.428823 -4.4287686 -4.4287176 -4.4287071 -4.4287052 -4.4287128 -4.4287524 -4.4287949 -4.4288197 -4.4288192 -4.4288025 -4.428791][-4.4289451 -4.4289355 -4.4289317 -4.4289217 -4.4289002 -4.4288797 -4.4288778 -4.4288774 -4.4288826 -4.4289012 -4.4289117 -4.4289055 -4.428895 -4.4288845 -4.4288754][-4.4289675 -4.4289703 -4.4289751 -4.4289765 -4.4289703 -4.4289608 -4.4289608 -4.4289613 -4.4289675 -4.4289789 -4.428977 -4.4289608 -4.4289455 -4.428937 -4.4289312][-4.4289865 -4.4289918 -4.4289942 -4.4289942 -4.4289918 -4.4289856 -4.4289808 -4.428978 -4.4289842 -4.4289932 -4.4289932 -4.4289842 -4.4289746 -4.4289694 -4.4289656]]...]
INFO - root - 2017-12-08 08:49:29.033063: step 62610, loss = 2.28, batch loss = 2.23 (35.0 examples/sec; 0.228 sec/batch; 17h:07m:31s remains)
INFO - root - 2017-12-08 08:49:31.282240: step 62620, loss = 2.28, batch loss = 2.23 (35.3 examples/sec; 0.226 sec/batch; 16h:57m:57s remains)
INFO - root - 2017-12-08 08:49:33.508796: step 62630, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.220 sec/batch; 16h:30m:45s remains)
INFO - root - 2017-12-08 08:49:35.743315: step 62640, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 16h:26m:38s remains)
INFO - root - 2017-12-08 08:49:37.989828: step 62650, loss = 2.28, batch loss = 2.23 (35.4 examples/sec; 0.226 sec/batch; 16h:57m:34s remains)
INFO - root - 2017-12-08 08:49:40.270210: step 62660, loss = 2.28, batch loss = 2.23 (35.6 examples/sec; 0.225 sec/batch; 16h:51m:13s remains)
INFO - root - 2017-12-08 08:49:42.492234: step 62670, loss = 2.28, batch loss = 2.23 (35.2 examples/sec; 0.227 sec/batch; 17h:01m:05s remains)
INFO - root - 2017-12-08 08:49:44.734304: step 62680, loss = 2.28, batch loss = 2.23 (34.8 examples/sec; 0.230 sec/batch; 17h:13m:50s remains)
INFO - root - 2017-12-08 08:49:46.952378: step 62690, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 16h:42m:13s remains)
INFO - root - 2017-12-08 08:49:49.193064: step 62700, loss = 2.28, batch loss = 2.23 (34.5 examples/sec; 0.232 sec/batch; 17h:21m:53s remains)
2017-12-08 08:49:49.482434: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4287114 -4.4286537 -4.4286 -4.4285855 -4.4286218 -4.4286742 -4.4286923 -4.4286833 -4.4286618 -4.4286356 -4.4286156 -4.4286103 -4.4285889 -4.428556 -4.4285479][-4.4286752 -4.428616 -4.428556 -4.4285445 -4.4285889 -4.4286451 -4.4286642 -4.428647 -4.428617 -4.4285941 -4.428587 -4.4285879 -4.4285617 -4.42852 -4.4285092][-4.4286556 -4.4286137 -4.428565 -4.42855 -4.4285889 -4.4286475 -4.428678 -4.4286709 -4.4286523 -4.4286389 -4.4286418 -4.4286284 -4.4285746 -4.4285111 -4.4284859][-4.428658 -4.428627 -4.4285927 -4.4285879 -4.4286232 -4.4286718 -4.4287009 -4.4287047 -4.4286909 -4.42868 -4.4286814 -4.428647 -4.42857 -4.4284892 -4.4284482][-4.4286847 -4.4286637 -4.428638 -4.4286332 -4.4286518 -4.4286704 -4.4286718 -4.4286671 -4.4286575 -4.4286509 -4.428658 -4.428618 -4.4285378 -4.4284592 -4.4284048][-4.4286995 -4.4286809 -4.4286571 -4.4286323 -4.4286032 -4.4285522 -4.4285054 -4.4285007 -4.4285226 -4.4285526 -4.428587 -4.4285746 -4.4285169 -4.4284563 -4.4284053][-4.4286776 -4.428647 -4.428606 -4.4285412 -4.4284396 -4.4283037 -4.4282055 -4.4282231 -4.4283028 -4.4283881 -4.4284692 -4.4285078 -4.428504 -4.4284854 -4.4284687][-4.4286304 -4.4285946 -4.4285364 -4.4284363 -4.4282727 -4.4280834 -4.4279733 -4.4280224 -4.42814 -4.4282594 -4.4283767 -4.4284687 -4.4285197 -4.428546 -4.4285765][-4.4285893 -4.4285607 -4.4285088 -4.4284172 -4.4282708 -4.42811 -4.4280295 -4.4280868 -4.4281869 -4.4282975 -4.4284167 -4.4285259 -4.4286036 -4.4286556 -4.4287043][-4.4285817 -4.4285665 -4.4285316 -4.4284782 -4.4283986 -4.4283104 -4.4282675 -4.4283066 -4.4283695 -4.4284544 -4.4285522 -4.4286423 -4.4287148 -4.4287663 -4.4288082][-4.4286227 -4.4286184 -4.4286013 -4.4285789 -4.4285493 -4.4285121 -4.4284959 -4.4285269 -4.4285707 -4.4286313 -4.4286966 -4.4287577 -4.4288125 -4.4288487 -4.4288678][-4.4286494 -4.4286523 -4.4286532 -4.4286571 -4.4286532 -4.42864 -4.4286418 -4.4286718 -4.4287052 -4.4287448 -4.4287806 -4.4288087 -4.4288278 -4.4288344 -4.4288297][-4.4286513 -4.4286637 -4.4286823 -4.4286981 -4.4287019 -4.4286952 -4.4287004 -4.4287286 -4.4287567 -4.4287767 -4.4287906 -4.428793 -4.4287863 -4.4287763 -4.4287658][-4.4286556 -4.4286723 -4.4286938 -4.42871 -4.4287171 -4.4287167 -4.4287224 -4.4287391 -4.4287524 -4.4287586 -4.4287605 -4.4287562 -4.4287491 -4.4287438 -4.42874][-4.4286666 -4.42867 -4.4286776 -4.4286819 -4.4286842 -4.4286861 -4.4286909 -4.4286985 -4.4287033 -4.4287057 -4.4287086 -4.4287105 -4.4287119 -4.4287167 -4.428721]]...]
INFO - root - 2017-12-08 08:49:51.744812: step 62710, loss = 2.28, batch loss = 2.23 (33.6 examples/sec; 0.238 sec/batch; 17h:49m:07s remains)
INFO - root - 2017-12-08 08:49:54.002377: step 62720, loss = 2.28, batch loss = 2.23 (36.9 examples/sec; 0.217 sec/batch; 16h:14m:52s remains)
INFO - root - 2017-12-08 08:49:56.255651: step 62730, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.222 sec/batch; 16h:37m:05s remains)
INFO - root - 2017-12-08 08:49:58.482667: step 62740, loss = 2.28, batch loss = 2.23 (35.8 examples/sec; 0.224 sec/batch; 16h:45m:35s remains)
INFO - root - 2017-12-08 08:50:00.720371: step 62750, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.226 sec/batch; 16h:54m:27s remains)
INFO - root - 2017-12-08 08:50:02.949019: step 62760, loss = 2.28, batch loss = 2.23 (35.3 examples/sec; 0.226 sec/batch; 16h:58m:06s remains)
INFO - root - 2017-12-08 08:50:05.181686: step 62770, loss = 2.28, batch loss = 2.23 (36.0 examples/sec; 0.222 sec/batch; 16h:39m:55s remains)
INFO - root - 2017-12-08 08:50:07.440956: step 62780, loss = 2.28, batch loss = 2.23 (32.5 examples/sec; 0.246 sec/batch; 18h:26m:25s remains)
INFO - root - 2017-12-08 08:50:09.661714: step 62790, loss = 2.28, batch loss = 2.23 (35.8 examples/sec; 0.224 sec/batch; 16h:45m:23s remains)
INFO - root - 2017-12-08 08:50:11.862342: step 62800, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 16h:28m:52s remains)
2017-12-08 08:50:12.169504: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4287333 -4.4287462 -4.4287882 -4.4288034 -4.4287963 -4.4287834 -4.4287677 -4.4287524 -4.4287529 -4.4287839 -4.42883 -4.4288473 -4.4288039 -4.4287176 -4.4286723][-4.4286771 -4.428679 -4.4287162 -4.4287257 -4.4287114 -4.4286933 -4.42868 -4.4286647 -4.4286814 -4.4287271 -4.4287739 -4.4287934 -4.4287572 -4.4286666 -4.4286127][-4.4285946 -4.428586 -4.4286222 -4.4286351 -4.4286232 -4.4286032 -4.4285831 -4.4285541 -4.428576 -4.428628 -4.42868 -4.4287157 -4.4286947 -4.4286089 -4.4285421][-4.4285369 -4.428503 -4.4285212 -4.428534 -4.428524 -4.428504 -4.4284706 -4.4284382 -4.4284735 -4.4285283 -4.4285946 -4.4286509 -4.4286437 -4.4285665 -4.4284921][-4.4285617 -4.4285026 -4.4284849 -4.4284711 -4.4284506 -4.428422 -4.428369 -4.4283252 -4.4283791 -4.42846 -4.4285474 -4.4286337 -4.4286418 -4.4285789 -4.4285016][-4.4286528 -4.4285874 -4.4285412 -4.4284945 -4.4284568 -4.4284172 -4.4283366 -4.42825 -4.4282861 -4.4283834 -4.4285064 -4.428617 -4.4286537 -4.428616 -4.42855][-4.4287148 -4.4286661 -4.428606 -4.428535 -4.4284816 -4.428442 -4.4283628 -4.4282556 -4.4282517 -4.4283404 -4.4284654 -4.4285841 -4.4286447 -4.4286337 -4.4285879][-4.4287167 -4.4286871 -4.4286385 -4.4285512 -4.4284825 -4.4284382 -4.4283795 -4.428297 -4.42829 -4.428371 -4.4284759 -4.4285741 -4.4286242 -4.4286194 -4.4285913][-4.4286995 -4.4286871 -4.4286647 -4.4285851 -4.4285035 -4.4284606 -4.4284163 -4.4283686 -4.4283791 -4.4284554 -4.4285359 -4.4286008 -4.4286194 -4.4285946 -4.4285693][-4.4286885 -4.4286861 -4.428688 -4.4286308 -4.4285526 -4.42851 -4.4284716 -4.4284468 -4.4284687 -4.4285355 -4.4285979 -4.4286308 -4.4286122 -4.4285622 -4.4285345][-4.4286737 -4.4286823 -4.4286985 -4.4286561 -4.4285812 -4.4285321 -4.4284949 -4.428484 -4.428514 -4.4285784 -4.4286265 -4.4286351 -4.42859 -4.4285164 -4.4284749][-4.4286604 -4.428678 -4.4287014 -4.4286633 -4.4285917 -4.4285421 -4.4285173 -4.4285145 -4.4285431 -4.4286036 -4.4286404 -4.428628 -4.4285645 -4.4284811 -4.4284325][-4.4287038 -4.4287162 -4.4287362 -4.4287062 -4.4286427 -4.4286022 -4.4285994 -4.4286079 -4.4286323 -4.4286776 -4.4286966 -4.4286633 -4.4285927 -4.42852 -4.4284873][-4.4287744 -4.4287839 -4.428792 -4.4287705 -4.4287219 -4.4286871 -4.4286995 -4.4287238 -4.4287448 -4.4287739 -4.428781 -4.4287405 -4.4286747 -4.4286151 -4.4285965][-4.4288454 -4.428853 -4.428854 -4.4288363 -4.4287963 -4.4287586 -4.4287677 -4.4287949 -4.4288135 -4.42883 -4.4288349 -4.4288077 -4.42876 -4.4287109 -4.4286957]]...]
INFO - root - 2017-12-08 08:50:14.440376: step 62810, loss = 2.28, batch loss = 2.23 (35.3 examples/sec; 0.227 sec/batch; 16h:59m:13s remains)
INFO - root - 2017-12-08 08:50:16.686898: step 62820, loss = 2.28, batch loss = 2.23 (36.6 examples/sec; 0.219 sec/batch; 16h:22m:07s remains)
INFO - root - 2017-12-08 08:50:18.910211: step 62830, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 16h:42m:13s remains)
INFO - root - 2017-12-08 08:50:21.174539: step 62840, loss = 2.28, batch loss = 2.23 (35.4 examples/sec; 0.226 sec/batch; 16h:54m:34s remains)
INFO - root - 2017-12-08 08:50:23.439816: step 62850, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 16h:42m:32s remains)
INFO - root - 2017-12-08 08:50:25.682474: step 62860, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 16h:32m:25s remains)
INFO - root - 2017-12-08 08:50:27.911673: step 62870, loss = 2.28, batch loss = 2.23 (36.0 examples/sec; 0.222 sec/batch; 16h:37m:45s remains)
INFO - root - 2017-12-08 08:50:30.131890: step 62880, loss = 2.28, batch loss = 2.23 (34.9 examples/sec; 0.229 sec/batch; 17h:09m:04s remains)
INFO - root - 2017-12-08 08:50:32.364045: step 62890, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.225 sec/batch; 16h:51m:29s remains)
INFO - root - 2017-12-08 08:50:34.600998: step 62900, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 16h:24m:40s remains)
2017-12-08 08:50:34.902800: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.42854 -4.428503 -4.4285488 -4.4286408 -4.4287319 -4.4288034 -4.4288335 -4.4288182 -4.4287877 -4.4287667 -4.42875 -4.4287233 -4.428689 -4.4286671 -4.42867][-4.4285531 -4.428535 -4.4286027 -4.4287052 -4.4287887 -4.4288354 -4.4288411 -4.4288073 -4.42878 -4.4287806 -4.4287806 -4.4287543 -4.4287076 -4.4286656 -4.4286571][-4.4286108 -4.4286151 -4.4286852 -4.428772 -4.4288259 -4.4288363 -4.4288063 -4.4287415 -4.42872 -4.4287553 -4.42879 -4.428791 -4.428762 -4.4287233 -4.4287043][-4.42868 -4.4286933 -4.4287534 -4.428812 -4.4288354 -4.4288111 -4.4287362 -4.4286327 -4.4286127 -4.4286871 -4.4287663 -4.42881 -4.4288182 -4.4288015 -4.4287786][-4.4287605 -4.4287758 -4.4288173 -4.4288425 -4.4288325 -4.4287634 -4.4286304 -4.4284849 -4.4284596 -4.4285736 -4.4287019 -4.4287944 -4.428843 -4.4288507 -4.4288354][-4.428833 -4.4288464 -4.4288721 -4.4288645 -4.4288144 -4.4286971 -4.4285073 -4.4283261 -4.4282894 -4.4284391 -4.42862 -4.4287591 -4.42884 -4.4288683 -4.4288621][-4.4288716 -4.4288878 -4.4289026 -4.4288688 -4.4287858 -4.4286327 -4.4284019 -4.4281964 -4.4281449 -4.4283123 -4.4285359 -4.4287157 -4.42882 -4.4288583 -4.4288626][-4.42889 -4.4289041 -4.4289064 -4.4288592 -4.4287596 -4.428596 -4.4283566 -4.428144 -4.4280772 -4.4282346 -4.4284739 -4.42868 -4.4287944 -4.4288349 -4.4288468][-4.428896 -4.4289017 -4.42889 -4.4288449 -4.4287529 -4.42861 -4.4284062 -4.4282207 -4.4281569 -4.428278 -4.4284911 -4.4286819 -4.4287925 -4.4288263 -4.4288359][-4.4288898 -4.4288878 -4.42887 -4.4288344 -4.4287629 -4.4286489 -4.4284987 -4.4283638 -4.4283209 -4.4284072 -4.4285655 -4.4287114 -4.4287982 -4.4288163 -4.4288154][-4.4288826 -4.4288731 -4.4288516 -4.4288287 -4.4287806 -4.4286966 -4.4285989 -4.4285226 -4.4285111 -4.4285779 -4.4286842 -4.4287786 -4.4288273 -4.4288287 -4.4288197][-4.4288845 -4.4288797 -4.428864 -4.4288549 -4.4288268 -4.4287744 -4.4287205 -4.42869 -4.4287004 -4.4287496 -4.4288135 -4.42886 -4.42887 -4.4288549 -4.4288373][-4.4288983 -4.4289 -4.4288912 -4.4288878 -4.4288774 -4.4288526 -4.4288306 -4.4288249 -4.4288392 -4.4288692 -4.4289031 -4.4289184 -4.4289026 -4.4288764 -4.428854][-4.4289107 -4.4289079 -4.4289002 -4.4288955 -4.4288912 -4.4288836 -4.4288821 -4.4288945 -4.4289136 -4.4289327 -4.4289455 -4.4289365 -4.4289079 -4.4288778 -4.4288559][-4.4289031 -4.4288931 -4.42888 -4.4288678 -4.4288583 -4.4288526 -4.4288669 -4.4288974 -4.4289284 -4.4289465 -4.4289441 -4.4289193 -4.4288816 -4.4288468 -4.4288282]]...]
INFO - root - 2017-12-08 08:50:37.154967: step 62910, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 16h:42m:31s remains)
INFO - root - 2017-12-08 08:50:39.382875: step 62920, loss = 2.28, batch loss = 2.23 (34.5 examples/sec; 0.232 sec/batch; 17h:23m:05s remains)
INFO - root - 2017-12-08 08:50:41.614404: step 62930, loss = 2.28, batch loss = 2.23 (34.2 examples/sec; 0.234 sec/batch; 17h:29m:50s remains)
INFO - root - 2017-12-08 08:50:43.827671: step 62940, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.221 sec/batch; 16h:34m:40s remains)
INFO - root - 2017-12-08 08:50:46.065858: step 62950, loss = 2.28, batch loss = 2.23 (36.9 examples/sec; 0.217 sec/batch; 16h:14m:15s remains)
INFO - root - 2017-12-08 08:50:48.310292: step 62960, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.225 sec/batch; 16h:51m:25s remains)
INFO - root - 2017-12-08 08:50:50.562163: step 62970, loss = 2.28, batch loss = 2.23 (34.8 examples/sec; 0.230 sec/batch; 17h:11m:56s remains)
INFO - root - 2017-12-08 08:50:52.805262: step 62980, loss = 2.28, batch loss = 2.23 (35.2 examples/sec; 0.227 sec/batch; 17h:01m:52s remains)
INFO - root - 2017-12-08 08:50:55.056331: step 62990, loss = 2.28, batch loss = 2.23 (37.6 examples/sec; 0.213 sec/batch; 15h:54m:45s remains)
INFO - root - 2017-12-08 08:50:57.274647: step 63000, loss = 2.28, batch loss = 2.23 (36.9 examples/sec; 0.217 sec/batch; 16h:14m:33s remains)
2017-12-08 08:50:57.551793: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4286413 -4.4286804 -4.4287195 -4.4287033 -4.4286723 -4.4286652 -4.4286838 -4.4286809 -4.4286661 -4.4286814 -4.4287014 -4.4286919 -4.4286838 -4.4286971 -4.4287052][-4.4286613 -4.4287043 -4.4287305 -4.4287086 -4.4286723 -4.4286566 -4.4286795 -4.4286823 -4.4286704 -4.42868 -4.4287009 -4.4286895 -4.4286785 -4.4286933 -4.4287014][-4.4286609 -4.4286885 -4.4287052 -4.4286938 -4.4286528 -4.4286194 -4.4286318 -4.4286494 -4.4286675 -4.4286919 -4.4287133 -4.4286985 -4.4286747 -4.4286728 -4.4286685][-4.42864 -4.4286489 -4.4286628 -4.4286609 -4.4286165 -4.4285579 -4.4285469 -4.4285755 -4.4286313 -4.4286766 -4.4286952 -4.4286709 -4.4286246 -4.4286013 -4.4285874][-4.4285941 -4.4285989 -4.4286237 -4.4286327 -4.4285817 -4.428493 -4.4284458 -4.4284863 -4.4285884 -4.4286532 -4.428659 -4.4286046 -4.4285331 -4.4285045 -4.4284921][-4.4285655 -4.4285755 -4.4286041 -4.4286032 -4.4285283 -4.4283891 -4.4282742 -4.4283214 -4.428473 -4.4285603 -4.4285617 -4.4284925 -4.4284282 -4.4284396 -4.4284558][-4.4285879 -4.4285851 -4.4285955 -4.4285655 -4.4284487 -4.4282308 -4.4280372 -4.4280977 -4.4283051 -4.4284196 -4.4284267 -4.4283791 -4.4283724 -4.4284348 -4.4284782][-4.4286537 -4.4286332 -4.4286265 -4.4285793 -4.4284492 -4.4282284 -4.42804 -4.4281006 -4.4282928 -4.4284005 -4.4284058 -4.4283857 -4.4284272 -4.4285164 -4.4285712][-4.4287262 -4.4287038 -4.4286952 -4.4286575 -4.4285555 -4.4284062 -4.4282908 -4.4283218 -4.4284325 -4.4284835 -4.4284687 -4.4284616 -4.4285235 -4.4286232 -4.4286866][-4.4287581 -4.4287467 -4.4287453 -4.4287195 -4.4286451 -4.4285512 -4.4284868 -4.4284964 -4.4285383 -4.4285455 -4.4285283 -4.4285426 -4.4286151 -4.4287062 -4.4287586][-4.4287171 -4.4287238 -4.4287395 -4.4287262 -4.4286718 -4.4286151 -4.4285927 -4.4286003 -4.4286127 -4.4286041 -4.4285927 -4.4286108 -4.4286685 -4.4287229 -4.4287515][-4.4286704 -4.4286904 -4.4287219 -4.4287157 -4.4286804 -4.428658 -4.4286642 -4.4286785 -4.4286785 -4.4286623 -4.4286523 -4.4286628 -4.428699 -4.4287219 -4.4287291][-4.4286904 -4.428719 -4.4287553 -4.428751 -4.4287319 -4.4287281 -4.4287434 -4.4287524 -4.4287481 -4.4287329 -4.4287271 -4.4287333 -4.4287515 -4.4287577 -4.4287534][-4.4287453 -4.4287705 -4.428803 -4.428802 -4.4287939 -4.4287949 -4.4288034 -4.42881 -4.4288087 -4.4288011 -4.4287992 -4.4288015 -4.4288116 -4.4288135 -4.4288087][-4.428813 -4.4288259 -4.4288435 -4.4288449 -4.4288449 -4.4288487 -4.4288545 -4.4288573 -4.4288583 -4.428854 -4.4288526 -4.428853 -4.4288573 -4.4288573 -4.4288573]]...]
INFO - root - 2017-12-08 08:50:59.746338: step 63010, loss = 2.28, batch loss = 2.23 (37.0 examples/sec; 0.216 sec/batch; 16h:09m:52s remains)
INFO - root - 2017-12-08 08:51:02.002723: step 63020, loss = 2.28, batch loss = 2.23 (35.3 examples/sec; 0.226 sec/batch; 16h:56m:59s remains)
INFO - root - 2017-12-08 08:51:04.246355: step 63030, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.225 sec/batch; 16h:51m:23s remains)
INFO - root - 2017-12-08 08:51:06.507236: step 63040, loss = 2.28, batch loss = 2.23 (35.3 examples/sec; 0.227 sec/batch; 16h:57m:14s remains)
INFO - root - 2017-12-08 08:51:08.786019: step 63050, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.220 sec/batch; 16h:29m:05s remains)
INFO - root - 2017-12-08 08:51:11.005384: step 63060, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.220 sec/batch; 16h:30m:00s remains)
INFO - root - 2017-12-08 08:51:13.267076: step 63070, loss = 2.28, batch loss = 2.23 (35.4 examples/sec; 0.226 sec/batch; 16h:55m:32s remains)
INFO - root - 2017-12-08 08:51:15.521075: step 63080, loss = 2.28, batch loss = 2.23 (36.0 examples/sec; 0.222 sec/batch; 16h:36m:53s remains)
INFO - root - 2017-12-08 08:51:17.741968: step 63090, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.220 sec/batch; 16h:29m:32s remains)
INFO - root - 2017-12-08 08:51:19.970674: step 63100, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 16h:32m:50s remains)
2017-12-08 08:51:20.288162: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4287639 -4.4287519 -4.4287457 -4.4287262 -4.428741 -4.4287786 -4.4287686 -4.4287486 -4.4287105 -4.4286795 -4.4286828 -4.4286923 -4.4287462 -4.4288154 -4.4288692][-4.4286919 -4.428679 -4.4286842 -4.4286757 -4.4286909 -4.4287252 -4.4287186 -4.4287038 -4.4286661 -4.4286394 -4.4286456 -4.4286637 -4.4287248 -4.4287944 -4.4288521][-4.4286165 -4.4286175 -4.4286404 -4.4286571 -4.4286742 -4.4287009 -4.4286947 -4.428679 -4.4286394 -4.4286165 -4.4286194 -4.428637 -4.4286995 -4.4287682 -4.4288344][-4.42861 -4.4286284 -4.4286633 -4.4286976 -4.4287148 -4.4287291 -4.4287128 -4.4286947 -4.4286523 -4.4286332 -4.4286375 -4.4286594 -4.4287114 -4.4287643 -4.4288268][-4.4287004 -4.4287314 -4.4287648 -4.4287891 -4.4287891 -4.4287786 -4.4287424 -4.4287128 -4.4286761 -4.4286656 -4.4286747 -4.4287033 -4.4287429 -4.4287825 -4.4288368][-4.4287958 -4.4288316 -4.4288511 -4.4288435 -4.4288015 -4.4287467 -4.4286785 -4.4286394 -4.42863 -4.4286566 -4.4286914 -4.4287343 -4.4287696 -4.42881 -4.4288578][-4.4288416 -4.4288621 -4.4288483 -4.4288044 -4.4287133 -4.4286008 -4.4284849 -4.4284348 -4.4284768 -4.4285803 -4.4286704 -4.428741 -4.4287863 -4.4288344 -4.4288759][-4.42885 -4.4288459 -4.4287977 -4.4287047 -4.4285583 -4.428381 -4.4281917 -4.4281158 -4.428225 -4.4284363 -4.4286075 -4.4287133 -4.4287772 -4.4288306 -4.4288735][-4.4288054 -4.4287872 -4.4287286 -4.4286203 -4.4284697 -4.428277 -4.4280591 -4.4279613 -4.4280963 -4.4283566 -4.4285712 -4.4286957 -4.4287691 -4.4288244 -4.4288654][-4.4287453 -4.4287243 -4.4286852 -4.42861 -4.4285188 -4.4283967 -4.4282656 -4.4282 -4.4282827 -4.4284697 -4.4286451 -4.42875 -4.4288092 -4.428853 -4.4288864][-4.4287605 -4.4287462 -4.4287329 -4.4286942 -4.4286547 -4.4286008 -4.4285455 -4.4285054 -4.4285398 -4.4286423 -4.4287524 -4.4288192 -4.428853 -4.4288783 -4.4289021][-4.4288011 -4.428792 -4.4287972 -4.4287877 -4.42878 -4.4287658 -4.4287477 -4.4287238 -4.4287286 -4.4287696 -4.4288244 -4.42886 -4.4288812 -4.4288926 -4.4289103][-4.428802 -4.4288025 -4.4288182 -4.4288249 -4.4288383 -4.4288549 -4.4288659 -4.4288573 -4.4288521 -4.4288588 -4.4288735 -4.4288869 -4.4288983 -4.4289041 -4.4289203][-4.4287739 -4.428782 -4.4288092 -4.4288297 -4.4288478 -4.428875 -4.4288988 -4.428906 -4.4289036 -4.4289017 -4.4289012 -4.4289055 -4.4289112 -4.4289131 -4.4289255][-4.4287548 -4.4287591 -4.4287844 -4.4288111 -4.4288306 -4.4288521 -4.4288721 -4.428885 -4.4288898 -4.4288917 -4.4288936 -4.4289017 -4.4289103 -4.4289122 -4.4289236]]...]
INFO - root - 2017-12-08 08:51:22.535670: step 63110, loss = 2.28, batch loss = 2.23 (35.4 examples/sec; 0.226 sec/batch; 16h:55m:01s remains)
INFO - root - 2017-12-08 08:51:24.780580: step 63120, loss = 2.28, batch loss = 2.23 (35.4 examples/sec; 0.226 sec/batch; 16h:54m:47s remains)
INFO - root - 2017-12-08 08:51:27.000129: step 63130, loss = 2.28, batch loss = 2.23 (35.6 examples/sec; 0.225 sec/batch; 16h:48m:18s remains)
INFO - root - 2017-12-08 08:51:29.234058: step 63140, loss = 2.28, batch loss = 2.23 (34.8 examples/sec; 0.230 sec/batch; 17h:13m:20s remains)
INFO - root - 2017-12-08 08:51:31.427569: step 63150, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 16h:39m:11s remains)
INFO - root - 2017-12-08 08:51:33.625810: step 63160, loss = 2.28, batch loss = 2.23 (35.4 examples/sec; 0.226 sec/batch; 16h:55m:16s remains)
INFO - root - 2017-12-08 08:51:35.860281: step 63170, loss = 2.28, batch loss = 2.23 (35.8 examples/sec; 0.224 sec/batch; 16h:44m:02s remains)
INFO - root - 2017-12-08 08:51:38.126834: step 63180, loss = 2.28, batch loss = 2.23 (31.8 examples/sec; 0.252 sec/batch; 18h:49m:49s remains)
INFO - root - 2017-12-08 08:51:40.357739: step 63190, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.222 sec/batch; 16h:34m:36s remains)
INFO - root - 2017-12-08 08:51:42.603545: step 63200, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 16h:23m:15s remains)
2017-12-08 08:51:42.903444: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289384 -4.4289346 -4.428925 -4.4289169 -4.4289093 -4.428896 -4.4288778 -4.4288597 -4.4288321 -4.4287934 -4.4287739 -4.4287677 -4.4287705 -4.4287858 -4.4288039][-4.4289365 -4.428926 -4.4289103 -4.4289021 -4.4289002 -4.4288912 -4.4288769 -4.4288592 -4.4288359 -4.4288096 -4.4287968 -4.4287848 -4.4287786 -4.4287853 -4.428802][-4.4289479 -4.4289346 -4.4289165 -4.4289083 -4.4289069 -4.4288969 -4.4288807 -4.4288573 -4.4288335 -4.4288135 -4.4288011 -4.4287863 -4.4287777 -4.42878 -4.4288006][-4.4289479 -4.4289351 -4.4289169 -4.4289055 -4.4288993 -4.4288826 -4.4288568 -4.4288273 -4.4288068 -4.4287925 -4.4287825 -4.4287767 -4.4287734 -4.428782 -4.4288063][-4.428906 -4.428895 -4.4288774 -4.4288578 -4.4288411 -4.4288163 -4.4287872 -4.4287634 -4.4287496 -4.428741 -4.4287362 -4.4287372 -4.42874 -4.4287596 -4.4287906][-4.4288278 -4.42882 -4.4288006 -4.4287715 -4.4287453 -4.4287171 -4.4286904 -4.4286771 -4.4286742 -4.4286728 -4.4286718 -4.4286733 -4.4286819 -4.4287105 -4.4287395][-4.4287467 -4.42874 -4.4287148 -4.4286761 -4.4286389 -4.4286075 -4.4285822 -4.428576 -4.4285879 -4.4286017 -4.4286056 -4.4286056 -4.4286156 -4.4286394 -4.428658][-4.4286942 -4.4286842 -4.4286475 -4.4285951 -4.4285507 -4.4285226 -4.4285011 -4.4285069 -4.42854 -4.428566 -4.4285617 -4.4285474 -4.4285507 -4.4285679 -4.42858][-4.4286776 -4.4286594 -4.4286108 -4.4285469 -4.4285092 -4.4284992 -4.4284906 -4.4285059 -4.4285522 -4.42858 -4.4285593 -4.4285231 -4.428514 -4.4285274 -4.4285426][-4.428688 -4.4286652 -4.4286156 -4.4285574 -4.4285388 -4.4285517 -4.4285555 -4.4285679 -4.4286027 -4.428617 -4.4285817 -4.4285378 -4.4285288 -4.4285536 -4.4285846][-4.4287195 -4.4287 -4.4286551 -4.4286108 -4.428607 -4.4286304 -4.428638 -4.4286437 -4.4286623 -4.42867 -4.4286442 -4.428618 -4.4286251 -4.4286604 -4.4286904][-4.4287572 -4.4287319 -4.4286838 -4.4286418 -4.4286423 -4.4286728 -4.4286966 -4.42871 -4.4287267 -4.4287438 -4.4287391 -4.4287333 -4.4287467 -4.428772 -4.4287815][-4.4287829 -4.428751 -4.4286914 -4.4286423 -4.428637 -4.4286704 -4.4287086 -4.4287376 -4.4287639 -4.4287939 -4.4288096 -4.4288163 -4.4288249 -4.4288306 -4.4288192][-4.4287925 -4.4287553 -4.4286852 -4.4286294 -4.428616 -4.428648 -4.4286962 -4.4287353 -4.4287686 -4.4288044 -4.4288292 -4.428843 -4.4288397 -4.4288263 -4.4288][-4.4287939 -4.4287553 -4.4286804 -4.4286237 -4.4286065 -4.4286342 -4.4286866 -4.4287319 -4.4287667 -4.4287972 -4.4288163 -4.428823 -4.4288044 -4.4287748 -4.4287372]]...]
INFO - root - 2017-12-08 08:51:45.135485: step 63210, loss = 2.28, batch loss = 2.23 (35.3 examples/sec; 0.227 sec/batch; 16h:57m:31s remains)
INFO - root - 2017-12-08 08:51:47.351955: step 63220, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.225 sec/batch; 16h:51m:58s remains)
INFO - root - 2017-12-08 08:51:49.581413: step 63230, loss = 2.28, batch loss = 2.23 (36.7 examples/sec; 0.218 sec/batch; 16h:19m:33s remains)
INFO - root - 2017-12-08 08:51:51.823533: step 63240, loss = 2.28, batch loss = 2.23 (36.0 examples/sec; 0.222 sec/batch; 16h:38m:26s remains)
INFO - root - 2017-12-08 08:51:54.074765: step 63250, loss = 2.28, batch loss = 2.23 (34.9 examples/sec; 0.229 sec/batch; 17h:07m:54s remains)
INFO - root - 2017-12-08 08:51:56.317051: step 63260, loss = 2.28, batch loss = 2.23 (34.7 examples/sec; 0.231 sec/batch; 17h:15m:13s remains)
INFO - root - 2017-12-08 08:51:58.533801: step 63270, loss = 2.28, batch loss = 2.23 (36.7 examples/sec; 0.218 sec/batch; 16h:16m:58s remains)
INFO - root - 2017-12-08 08:52:00.767525: step 63280, loss = 2.28, batch loss = 2.23 (36.7 examples/sec; 0.218 sec/batch; 16h:17m:38s remains)
INFO - root - 2017-12-08 08:52:02.991951: step 63290, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 16h:23m:34s remains)
INFO - root - 2017-12-08 08:52:05.236615: step 63300, loss = 2.28, batch loss = 2.23 (37.3 examples/sec; 0.214 sec/batch; 16h:01m:36s remains)
2017-12-08 08:52:05.543011: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.42884 -4.42886 -4.4288712 -4.4288678 -4.4288468 -4.4288111 -4.4287786 -4.428762 -4.428771 -4.4287963 -4.4288239 -4.4288487 -4.4288578 -4.4288478 -4.4288297][-4.4288611 -4.4288726 -4.4288793 -4.4288754 -4.428854 -4.4288144 -4.4287763 -4.4287548 -4.4287605 -4.4287839 -4.4288135 -4.4288435 -4.4288535 -4.4288378 -4.4288154][-4.4288344 -4.4288397 -4.4288487 -4.4288478 -4.4288244 -4.4287786 -4.4287376 -4.428721 -4.4287324 -4.4287663 -4.428803 -4.4288349 -4.4288383 -4.4288092 -4.4287772][-4.4287968 -4.4287996 -4.4288125 -4.4288149 -4.428792 -4.4287419 -4.4286985 -4.4286842 -4.4287071 -4.4287581 -4.4288039 -4.428833 -4.428822 -4.4287686 -4.428719][-4.4287729 -4.4287763 -4.4287891 -4.4287853 -4.4287539 -4.4286933 -4.4286361 -4.4286184 -4.4286547 -4.4287295 -4.4287891 -4.4288154 -4.4287915 -4.4287157 -4.4286423][-4.4287372 -4.4287395 -4.4287438 -4.4287238 -4.4286718 -4.428586 -4.4285045 -4.4284892 -4.4285555 -4.428669 -4.4287529 -4.4287806 -4.4287453 -4.4286432 -4.4285383][-4.4286695 -4.4286666 -4.428659 -4.4286222 -4.42855 -4.42844 -4.4283447 -4.4283481 -4.4284573 -4.4286118 -4.4287152 -4.4287376 -4.4286814 -4.4285455 -4.42842][-4.4285932 -4.4285817 -4.4285688 -4.4285293 -4.4284549 -4.4283528 -4.4282818 -4.4283175 -4.4284477 -4.4286022 -4.4286928 -4.4286995 -4.4286175 -4.42846 -4.4283409][-4.428565 -4.4285488 -4.42854 -4.4285169 -4.4284716 -4.4284129 -4.4283862 -4.4284334 -4.4285355 -4.4286418 -4.4286919 -4.4286723 -4.4285817 -4.4284458 -4.4283652][-4.428586 -4.4285769 -4.428575 -4.4285665 -4.4285469 -4.4285226 -4.4285192 -4.4285545 -4.4286113 -4.4286671 -4.4286871 -4.4286633 -4.4286003 -4.4285131 -4.4284635][-4.4286289 -4.4286251 -4.4286227 -4.4286146 -4.428596 -4.428576 -4.428566 -4.4285793 -4.4286036 -4.4286313 -4.4286466 -4.4286423 -4.42862 -4.4285784 -4.4285464][-4.4286804 -4.4286728 -4.4286613 -4.4286451 -4.4286194 -4.4285946 -4.4285693 -4.428556 -4.4285564 -4.42857 -4.42859 -4.4286032 -4.4286094 -4.4285951 -4.4285736][-4.4287362 -4.4287219 -4.4287 -4.4286742 -4.4286427 -4.428616 -4.4285779 -4.4285412 -4.4285221 -4.4285212 -4.428535 -4.4285536 -4.4285741 -4.4285769 -4.4285684][-4.4287944 -4.428781 -4.4287553 -4.4287224 -4.4286847 -4.4286523 -4.4286036 -4.4285522 -4.4285183 -4.4285054 -4.4285116 -4.428525 -4.4285417 -4.428546 -4.4285436][-4.4288487 -4.4288378 -4.4288135 -4.4287772 -4.4287319 -4.4286866 -4.428627 -4.4285679 -4.4285312 -4.4285192 -4.4285274 -4.4285369 -4.4285445 -4.4285421 -4.4285398]]...]
INFO - root - 2017-12-08 08:52:07.751219: step 63310, loss = 2.28, batch loss = 2.23 (36.7 examples/sec; 0.218 sec/batch; 16h:17m:01s remains)
INFO - root - 2017-12-08 08:52:10.010368: step 63320, loss = 2.28, batch loss = 2.23 (35.6 examples/sec; 0.225 sec/batch; 16h:48m:08s remains)
INFO - root - 2017-12-08 08:52:12.259779: step 63330, loss = 2.28, batch loss = 2.23 (35.7 examples/sec; 0.224 sec/batch; 16h:44m:27s remains)
INFO - root - 2017-12-08 08:52:14.493140: step 63340, loss = 2.28, batch loss = 2.23 (30.7 examples/sec; 0.260 sec/batch; 19h:27m:28s remains)
INFO - root - 2017-12-08 08:52:16.739327: step 63350, loss = 2.28, batch loss = 2.23 (36.0 examples/sec; 0.222 sec/batch; 16h:35m:41s remains)
INFO - root - 2017-12-08 08:52:18.966297: step 63360, loss = 2.28, batch loss = 2.23 (37.3 examples/sec; 0.215 sec/batch; 16h:03m:20s remains)
INFO - root - 2017-12-08 08:52:21.240042: step 63370, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.225 sec/batch; 16h:50m:27s remains)
INFO - root - 2017-12-08 08:52:23.477711: step 63380, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 16h:38m:09s remains)
INFO - root - 2017-12-08 08:52:25.705460: step 63390, loss = 2.28, batch loss = 2.23 (33.4 examples/sec; 0.240 sec/batch; 17h:54m:12s remains)
INFO - root - 2017-12-08 08:52:27.945388: step 63400, loss = 2.28, batch loss = 2.23 (35.4 examples/sec; 0.226 sec/batch; 16h:53m:53s remains)
2017-12-08 08:52:28.264256: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.42873 -4.4287434 -4.4287858 -4.4288335 -4.4288554 -4.4288726 -4.4288964 -4.4289322 -4.4289808 -4.42902 -4.4290223 -4.4289813 -4.4289193 -4.4288645 -4.42885][-4.4286919 -4.42873 -4.4287782 -4.4288206 -4.4288306 -4.4288359 -4.42885 -4.4288821 -4.4289327 -4.4289765 -4.4289818 -4.428946 -4.4288874 -4.4288406 -4.4288325][-4.4286547 -4.4287162 -4.4287705 -4.4288082 -4.4288077 -4.4287949 -4.4287915 -4.4288139 -4.4288588 -4.4289069 -4.4289141 -4.42888 -4.4288287 -4.4287958 -4.4288034][-4.4286389 -4.4287167 -4.4287715 -4.4288006 -4.4287872 -4.4287472 -4.4287138 -4.4287224 -4.4287686 -4.4288244 -4.4288449 -4.4288235 -4.4287949 -4.42879 -4.428822][-4.4285927 -4.4286857 -4.4287472 -4.4287786 -4.42877 -4.4287043 -4.4286237 -4.4286036 -4.4286656 -4.4287486 -4.428803 -4.42882 -4.4288325 -4.4288592 -4.4289021][-4.4284835 -4.4285955 -4.4286962 -4.4287586 -4.4287624 -4.4286666 -4.4285135 -4.4284143 -4.4284825 -4.4286251 -4.4287424 -4.4288268 -4.4288874 -4.428937 -4.4289804][-4.4283819 -4.4285069 -4.428659 -4.4287672 -4.4287715 -4.4286346 -4.4283824 -4.428153 -4.428184 -4.4283996 -4.4286132 -4.4287744 -4.4288845 -4.4289589 -4.4290147][-4.4283214 -4.4284453 -4.4286366 -4.428782 -4.4287834 -4.428618 -4.4282837 -4.4279146 -4.4278541 -4.42812 -4.4284291 -4.428659 -4.4288154 -4.4289246 -4.4290032][-4.4283485 -4.4284387 -4.4286227 -4.4287763 -4.4287853 -4.4286466 -4.4283428 -4.4279394 -4.4277725 -4.4279947 -4.4283285 -4.4285774 -4.4287562 -4.4288869 -4.4289804][-4.4284678 -4.4285164 -4.4286637 -4.428803 -4.4288373 -4.4287472 -4.4285355 -4.4281893 -4.4279656 -4.4280682 -4.4283423 -4.4285789 -4.4287596 -4.42889 -4.4289784][-4.4286041 -4.42865 -4.4287639 -4.4288816 -4.4289336 -4.4288783 -4.4287286 -4.4284568 -4.4282346 -4.4282441 -4.4284248 -4.4286227 -4.42879 -4.42892 -4.4289994][-4.4287205 -4.4287834 -4.4288716 -4.4289584 -4.429008 -4.4289761 -4.4288726 -4.4286776 -4.4284949 -4.4284582 -4.4285603 -4.4286966 -4.4288321 -4.4289489 -4.4290142][-4.4288006 -4.4288731 -4.4289446 -4.428998 -4.4290318 -4.4290233 -4.4289641 -4.4288383 -4.4287066 -4.4286566 -4.4286995 -4.4287772 -4.4288807 -4.4289794 -4.4290333][-4.4288368 -4.42892 -4.428978 -4.4290056 -4.4290242 -4.4290271 -4.4290028 -4.4289351 -4.42886 -4.4288187 -4.4288225 -4.4288545 -4.4289274 -4.4290133 -4.4290619][-4.4288669 -4.4289494 -4.4289961 -4.4290028 -4.4290051 -4.4290032 -4.4289908 -4.4289608 -4.4289246 -4.4289055 -4.4288974 -4.428906 -4.4289575 -4.4290352 -4.4290848]]...]
INFO - root - 2017-12-08 08:52:30.489885: step 63410, loss = 2.28, batch loss = 2.23 (34.7 examples/sec; 0.230 sec/batch; 17h:13m:02s remains)
INFO - root - 2017-12-08 08:52:32.731360: step 63420, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 16h:38m:09s remains)
INFO - root - 2017-12-08 08:52:35.024476: step 63430, loss = 2.28, batch loss = 2.23 (35.4 examples/sec; 0.226 sec/batch; 16h:54m:25s remains)
INFO - root - 2017-12-08 08:52:37.316018: step 63440, loss = 2.28, batch loss = 2.23 (35.3 examples/sec; 0.227 sec/batch; 16h:56m:04s remains)
INFO - root - 2017-12-08 08:52:39.545550: step 63450, loss = 2.28, batch loss = 2.23 (34.4 examples/sec; 0.233 sec/batch; 17h:23m:57s remains)
INFO - root - 2017-12-08 08:52:41.769692: step 63460, loss = 2.28, batch loss = 2.23 (37.5 examples/sec; 0.214 sec/batch; 15h:57m:38s remains)
INFO - root - 2017-12-08 08:52:44.019744: step 63470, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 16h:25m:33s remains)
INFO - root - 2017-12-08 08:52:46.293779: step 63480, loss = 2.28, batch loss = 2.23 (34.5 examples/sec; 0.232 sec/batch; 17h:19m:58s remains)
INFO - root - 2017-12-08 08:52:48.530429: step 63490, loss = 2.28, batch loss = 2.23 (35.0 examples/sec; 0.229 sec/batch; 17h:04m:57s remains)
INFO - root - 2017-12-08 08:52:50.786088: step 63500, loss = 2.28, batch loss = 2.23 (35.4 examples/sec; 0.226 sec/batch; 16h:52m:05s remains)
2017-12-08 08:52:51.069708: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289103 -4.4288883 -4.4288621 -4.428844 -4.428834 -4.42884 -4.4288549 -4.4288678 -4.4288778 -4.4288664 -4.4288354 -4.428792 -4.4287543 -4.4287467 -4.4287572][-4.4289026 -4.4288731 -4.4288468 -4.428822 -4.4288135 -4.4288216 -4.4288335 -4.4288425 -4.428843 -4.4288216 -4.4287896 -4.4287314 -4.4286776 -4.4286604 -4.4286823][-4.4288898 -4.4288483 -4.4288158 -4.4287848 -4.4287729 -4.4287848 -4.428793 -4.4287963 -4.4287896 -4.4287763 -4.4287572 -4.4287052 -4.4286447 -4.428617 -4.4286447][-4.4288683 -4.4288173 -4.4287767 -4.4287405 -4.4287229 -4.4287324 -4.4287357 -4.4287386 -4.4287372 -4.4287438 -4.4287477 -4.4287186 -4.4286728 -4.4286504 -4.4286814][-4.4288573 -4.4288096 -4.4287591 -4.4287138 -4.4286852 -4.4286795 -4.4286714 -4.4286838 -4.4287119 -4.4287529 -4.4287806 -4.4287767 -4.4287472 -4.4287338 -4.4287667][-4.4288435 -4.4288235 -4.42877 -4.4287162 -4.4286723 -4.42865 -4.4286342 -4.4286642 -4.4287229 -4.4287863 -4.4288263 -4.4288354 -4.4288192 -4.428813 -4.4288483][-4.4288149 -4.4288192 -4.4287763 -4.4287238 -4.4286709 -4.4286404 -4.4286242 -4.4286666 -4.4287381 -4.4288073 -4.4288473 -4.4288583 -4.4288487 -4.4288592 -4.4289026][-4.4287753 -4.4287896 -4.4287558 -4.4287119 -4.4286695 -4.4286585 -4.4286566 -4.4286995 -4.4287629 -4.4288187 -4.4288526 -4.4288621 -4.428863 -4.4288855 -4.4289317][-4.4287348 -4.4287558 -4.4287267 -4.4286838 -4.4286547 -4.4286642 -4.4286857 -4.4287329 -4.428792 -4.4288335 -4.4288545 -4.4288588 -4.4288692 -4.4288936 -4.4289379][-4.4286828 -4.4287081 -4.4286871 -4.4286494 -4.4286294 -4.4286337 -4.42866 -4.42873 -4.4288015 -4.4288368 -4.4288535 -4.4288521 -4.4288621 -4.4288836 -4.4289265][-4.428637 -4.4286609 -4.428669 -4.4286594 -4.4286561 -4.4286485 -4.4286547 -4.4287281 -4.428812 -4.4288459 -4.4288607 -4.4288535 -4.4288583 -4.4288774 -4.4289165][-4.4286323 -4.4286447 -4.4286766 -4.4286966 -4.4287009 -4.4286819 -4.4286747 -4.4287357 -4.4288211 -4.42886 -4.4288683 -4.4288554 -4.4288549 -4.4288769 -4.428915][-4.42869 -4.428678 -4.4286952 -4.4287152 -4.4287119 -4.428689 -4.42869 -4.4287462 -4.42883 -4.4288769 -4.4288816 -4.4288554 -4.4288445 -4.4288673 -4.4289117][-4.4287596 -4.4287291 -4.4287186 -4.4287243 -4.4287152 -4.4287076 -4.4287224 -4.4287772 -4.4288535 -4.4288955 -4.4288955 -4.4288597 -4.428833 -4.4288492 -4.428895][-4.42879 -4.4287505 -4.4287257 -4.4287252 -4.4287276 -4.4287333 -4.4287634 -4.4288168 -4.4288788 -4.428906 -4.4289002 -4.428864 -4.4288287 -4.4288335 -4.4288707]]...]
INFO - root - 2017-12-08 08:52:53.307425: step 63510, loss = 2.28, batch loss = 2.23 (35.6 examples/sec; 0.225 sec/batch; 16h:48m:31s remains)
INFO - root - 2017-12-08 08:52:55.542882: step 63520, loss = 2.28, batch loss = 2.23 (33.8 examples/sec; 0.237 sec/batch; 17h:42m:26s remains)
INFO - root - 2017-12-08 08:52:57.748632: step 63530, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 16h:21m:22s remains)
INFO - root - 2017-12-08 08:52:59.990645: step 63540, loss = 2.28, batch loss = 2.23 (36.7 examples/sec; 0.218 sec/batch; 16h:16m:29s remains)
INFO - root - 2017-12-08 08:53:02.246000: step 63550, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 16h:25m:12s remains)
INFO - root - 2017-12-08 08:53:04.513941: step 63560, loss = 2.28, batch loss = 2.23 (33.4 examples/sec; 0.239 sec/batch; 17h:52m:48s remains)
INFO - root - 2017-12-08 08:53:06.762474: step 63570, loss = 2.28, batch loss = 2.23 (34.9 examples/sec; 0.229 sec/batch; 17h:06m:57s remains)
INFO - root - 2017-12-08 08:53:08.977273: step 63580, loss = 2.28, batch loss = 2.23 (36.7 examples/sec; 0.218 sec/batch; 16h:17m:15s remains)
INFO - root - 2017-12-08 08:53:11.258827: step 63590, loss = 2.28, batch loss = 2.23 (36.7 examples/sec; 0.218 sec/batch; 16h:16m:25s remains)
INFO - root - 2017-12-08 08:53:13.505840: step 63600, loss = 2.28, batch loss = 2.23 (34.8 examples/sec; 0.230 sec/batch; 17h:09m:46s remains)
2017-12-08 08:53:13.822209: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4285164 -4.4285779 -4.4286475 -4.4286857 -4.4287004 -4.4287024 -4.4287066 -4.4287081 -4.4287128 -4.4287276 -4.4287481 -4.4287615 -4.4287486 -4.4287248 -4.4287205][-4.428596 -4.4286623 -4.4287205 -4.4287353 -4.4287252 -4.4287086 -4.4287057 -4.4287143 -4.4287281 -4.4287505 -4.4287767 -4.4287848 -4.4287672 -4.4287486 -4.4287496][-4.4286757 -4.4287243 -4.428762 -4.4287605 -4.4287314 -4.4287009 -4.4286871 -4.4287 -4.4287205 -4.42875 -4.4287844 -4.4287868 -4.4287682 -4.4287572 -4.4287572][-4.4287515 -4.4287729 -4.4287906 -4.428781 -4.4287462 -4.4287043 -4.4286761 -4.4286885 -4.4287176 -4.4287534 -4.4287939 -4.4287953 -4.4287682 -4.4287505 -4.4287419][-4.4287181 -4.4287219 -4.4287286 -4.4287195 -4.4286914 -4.4286442 -4.4285979 -4.4286041 -4.4286509 -4.4287086 -4.4287548 -4.428751 -4.42871 -4.4286709 -4.4286361][-4.4285874 -4.4285903 -4.4285994 -4.4286013 -4.4285884 -4.4285326 -4.4284558 -4.4284348 -4.4284797 -4.428556 -4.4286084 -4.4286041 -4.4285564 -4.4285145 -4.4284811][-4.4285269 -4.4285173 -4.4285059 -4.4285021 -4.4284868 -4.4284143 -4.4283018 -4.4282188 -4.4282231 -4.4283161 -4.4284062 -4.4284329 -4.428412 -4.4284015 -4.428411][-4.4285846 -4.4285522 -4.4285049 -4.4284687 -4.4284306 -4.4283276 -4.4281659 -4.4280028 -4.4279323 -4.4280424 -4.4282 -4.4282942 -4.4283333 -4.4283834 -4.4284506][-4.428721 -4.42868 -4.4286141 -4.4285541 -4.4284973 -4.4283886 -4.4282265 -4.4280519 -4.4279447 -4.4280334 -4.4281955 -4.4283161 -4.4284062 -4.4284983 -4.4285917][-4.4288335 -4.4288087 -4.42875 -4.4286814 -4.4286227 -4.4285378 -4.4284196 -4.4282966 -4.4282169 -4.428268 -4.4283767 -4.4284644 -4.4285541 -4.4286571 -4.42875][-4.428853 -4.4288521 -4.4288187 -4.4287667 -4.4287229 -4.4286704 -4.4286 -4.4285355 -4.4284992 -4.4285245 -4.4285769 -4.4286194 -4.4286785 -4.4287596 -4.4288292][-4.4288106 -4.4288297 -4.4288287 -4.4288092 -4.4287934 -4.4287772 -4.4287462 -4.4287195 -4.4286985 -4.4287009 -4.4287233 -4.4287419 -4.4287691 -4.4288135 -4.4288545][-4.4287658 -4.4287934 -4.428822 -4.4288354 -4.4288459 -4.4288526 -4.4288445 -4.428833 -4.4288158 -4.4288106 -4.4288211 -4.4288282 -4.4288349 -4.4288554 -4.42888][-4.428689 -4.4287095 -4.4287534 -4.4287882 -4.4288077 -4.4288192 -4.4288192 -4.4288216 -4.4288192 -4.4288197 -4.4288273 -4.4288306 -4.4288373 -4.4288535 -4.4288707][-4.4286213 -4.4286432 -4.4286981 -4.4287481 -4.428771 -4.4287834 -4.4287891 -4.4287977 -4.4288034 -4.4288077 -4.4288111 -4.4288135 -4.428822 -4.4288335 -4.4288416]]...]
INFO - root - 2017-12-08 08:53:16.058526: step 63610, loss = 2.28, batch loss = 2.23 (35.0 examples/sec; 0.229 sec/batch; 17h:05m:00s remains)
INFO - root - 2017-12-08 08:53:18.282270: step 63620, loss = 2.28, batch loss = 2.23 (34.4 examples/sec; 0.232 sec/batch; 17h:21m:34s remains)
INFO - root - 2017-12-08 08:53:20.555050: step 63630, loss = 2.28, batch loss = 2.23 (32.7 examples/sec; 0.245 sec/batch; 18h:15m:54s remains)
INFO - root - 2017-12-08 08:53:22.799786: step 63640, loss = 2.28, batch loss = 2.23 (35.6 examples/sec; 0.225 sec/batch; 16h:46m:59s remains)
INFO - root - 2017-12-08 08:53:25.057236: step 63650, loss = 2.28, batch loss = 2.23 (35.4 examples/sec; 0.226 sec/batch; 16h:52m:08s remains)
INFO - root - 2017-12-08 08:53:27.280724: step 63660, loss = 2.28, batch loss = 2.23 (35.4 examples/sec; 0.226 sec/batch; 16h:53m:16s remains)
INFO - root - 2017-12-08 08:53:29.532622: step 63670, loss = 2.28, batch loss = 2.23 (34.6 examples/sec; 0.231 sec/batch; 17h:15m:15s remains)
INFO - root - 2017-12-08 08:53:31.776840: step 63680, loss = 2.28, batch loss = 2.23 (35.0 examples/sec; 0.228 sec/batch; 17h:03m:14s remains)
INFO - root - 2017-12-08 08:53:34.015665: step 63690, loss = 2.28, batch loss = 2.23 (34.1 examples/sec; 0.235 sec/batch; 17h:30m:48s remains)
INFO - root - 2017-12-08 08:53:36.274768: step 63700, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.225 sec/batch; 16h:48m:53s remains)
2017-12-08 08:53:36.582623: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288621 -4.4288721 -4.4288821 -4.428865 -4.4288373 -4.4288254 -4.4288487 -4.428885 -4.4288974 -4.4288788 -4.4288335 -4.4287825 -4.4287405 -4.4287095 -4.4286919][-4.4288816 -4.4288883 -4.42889 -4.4288783 -4.4288621 -4.4288588 -4.428874 -4.4288912 -4.4288931 -4.4288712 -4.4288139 -4.4287453 -4.4286938 -4.4286613 -4.4286385][-4.4288678 -4.4288793 -4.4288797 -4.4288716 -4.4288607 -4.4288516 -4.4288535 -4.4288507 -4.4288383 -4.4288092 -4.4287505 -4.4286895 -4.4286537 -4.4286466 -4.4286413][-4.4288516 -4.4288774 -4.4288826 -4.42887 -4.4288449 -4.428822 -4.4288177 -4.4288106 -4.4287949 -4.4287658 -4.4287162 -4.4286704 -4.4286685 -4.4286981 -4.4287157][-4.4288359 -4.4288697 -4.4288774 -4.4288545 -4.4288044 -4.4287624 -4.4287605 -4.4287763 -4.4287806 -4.4287615 -4.4287181 -4.4286871 -4.4287133 -4.4287705 -4.428802][-4.4288149 -4.4288559 -4.428865 -4.4288263 -4.4287362 -4.428659 -4.4286532 -4.4287038 -4.4287467 -4.4287515 -4.4287353 -4.4287372 -4.4287791 -4.4288363 -4.4288688][-4.4288025 -4.4288425 -4.4288416 -4.428771 -4.4286346 -4.4285083 -4.4284911 -4.4285808 -4.4286938 -4.4287562 -4.4287891 -4.4288216 -4.4288516 -4.4288831 -4.4288931][-4.4287968 -4.4288163 -4.428793 -4.4286966 -4.4285321 -4.4283752 -4.4283519 -4.4284854 -4.4286685 -4.4287896 -4.42885 -4.428874 -4.42887 -4.4288688 -4.42886][-4.4287996 -4.428802 -4.4287753 -4.42869 -4.4285455 -4.4284096 -4.4284 -4.4285407 -4.4287205 -4.428843 -4.42889 -4.4288874 -4.4288597 -4.42884 -4.4288211][-4.4287872 -4.4287906 -4.4287858 -4.4287438 -4.4286714 -4.4285994 -4.4286027 -4.4286947 -4.4288125 -4.4288878 -4.4289031 -4.4288797 -4.4288483 -4.4288278 -4.4288111][-4.4287724 -4.4287953 -4.42883 -4.4288363 -4.4288139 -4.4287777 -4.4287729 -4.428813 -4.4288764 -4.42891 -4.4289026 -4.4288731 -4.4288421 -4.4288211 -4.4288025][-4.4287834 -4.4288259 -4.4288783 -4.4288993 -4.4288888 -4.4288616 -4.4288521 -4.4288573 -4.4288864 -4.4289093 -4.4289093 -4.4288893 -4.4288621 -4.4288387 -4.4288092][-4.4288149 -4.4288616 -4.428906 -4.4289203 -4.4289064 -4.42888 -4.4288592 -4.42884 -4.4288507 -4.4288788 -4.4288974 -4.428894 -4.4288783 -4.4288526 -4.4288158][-4.4288406 -4.4288769 -4.4289055 -4.4289117 -4.4289007 -4.4288735 -4.4288359 -4.4287996 -4.4287958 -4.4288282 -4.428863 -4.4288788 -4.4288793 -4.42886 -4.4288259][-4.4288449 -4.4288721 -4.4288917 -4.4288979 -4.4288912 -4.428863 -4.4288163 -4.4287715 -4.42876 -4.4287906 -4.4288316 -4.4288592 -4.4288688 -4.4288583 -4.4288359]]...]
INFO - root - 2017-12-08 08:53:38.851011: step 63710, loss = 2.28, batch loss = 2.23 (36.0 examples/sec; 0.222 sec/batch; 16h:36m:37s remains)
INFO - root - 2017-12-08 08:53:41.082008: step 63720, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 16h:22m:22s remains)
INFO - root - 2017-12-08 08:53:43.304586: step 63730, loss = 2.28, batch loss = 2.23 (35.8 examples/sec; 0.223 sec/batch; 16h:41m:04s remains)
INFO - root - 2017-12-08 08:53:45.514515: step 63740, loss = 2.28, batch loss = 2.23 (37.1 examples/sec; 0.216 sec/batch; 16h:06m:05s remains)
INFO - root - 2017-12-08 08:53:47.783756: step 63750, loss = 2.28, batch loss = 2.23 (33.3 examples/sec; 0.241 sec/batch; 17h:57m:34s remains)
INFO - root - 2017-12-08 08:53:50.021749: step 63760, loss = 2.28, batch loss = 2.23 (37.6 examples/sec; 0.213 sec/batch; 15h:54m:00s remains)
INFO - root - 2017-12-08 08:53:52.253893: step 63770, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.222 sec/batch; 16h:33m:42s remains)
INFO - root - 2017-12-08 08:53:54.494975: step 63780, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.225 sec/batch; 16h:48m:09s remains)
INFO - root - 2017-12-08 08:53:56.736290: step 63790, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.221 sec/batch; 16h:31m:09s remains)
INFO - root - 2017-12-08 08:53:58.981189: step 63800, loss = 2.28, batch loss = 2.23 (35.2 examples/sec; 0.227 sec/batch; 16h:57m:03s remains)
2017-12-08 08:53:59.292988: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.428946 -4.4289026 -4.4287896 -4.428648 -4.428534 -4.4284883 -4.42853 -4.428627 -4.4287257 -4.4287968 -4.4288435 -4.4288507 -4.4288025 -4.4287271 -4.428658][-4.4289422 -4.4288993 -4.4287887 -4.4286566 -4.428555 -4.4285121 -4.4285522 -4.428638 -4.4287114 -4.4287663 -4.4288173 -4.4288359 -4.4288 -4.4287262 -4.4286432][-4.4289489 -4.4289131 -4.4288149 -4.428709 -4.42863 -4.428576 -4.4285874 -4.4286327 -4.4286733 -4.4287252 -4.4287896 -4.4288168 -4.428791 -4.4287205 -4.4286308][-4.4289594 -4.4289355 -4.4288578 -4.42878 -4.4287119 -4.428647 -4.4286122 -4.4285946 -4.4286008 -4.428669 -4.428762 -4.4288058 -4.4287872 -4.4287128 -4.428617][-4.4289675 -4.4289508 -4.4288926 -4.4288287 -4.4287581 -4.4286752 -4.4285836 -4.4284916 -4.4284639 -4.4285731 -4.4287152 -4.428793 -4.4287863 -4.4287095 -4.4285984][-4.4289722 -4.4289603 -4.4289155 -4.4288492 -4.42876 -4.4286408 -4.4284816 -4.4283104 -4.428246 -4.4284167 -4.4286246 -4.4287481 -4.4287739 -4.4287114 -4.4285994][-4.4289746 -4.4289641 -4.4289165 -4.428834 -4.42871 -4.428545 -4.4283338 -4.4281082 -4.4280233 -4.4282546 -4.4285207 -4.4286838 -4.428741 -4.4287157 -4.4286332][-4.4289732 -4.4289575 -4.4288983 -4.4287953 -4.4286542 -4.4284778 -4.4282846 -4.4280944 -4.4280391 -4.4282584 -4.4285088 -4.428668 -4.4287343 -4.4287338 -4.4286861][-4.42897 -4.4289489 -4.428885 -4.4287763 -4.4286408 -4.4284911 -4.4283581 -4.4282503 -4.428236 -4.4283962 -4.4285803 -4.4287062 -4.4287677 -4.42878 -4.4287519][-4.4289713 -4.428956 -4.4288983 -4.4287996 -4.4286795 -4.4285622 -4.42848 -4.4284358 -4.4284449 -4.4285526 -4.4286752 -4.4287686 -4.4288211 -4.4288492 -4.4288397][-4.4289722 -4.4289694 -4.4289351 -4.4288673 -4.4287767 -4.4286857 -4.4286332 -4.4286184 -4.4286351 -4.4287014 -4.4287767 -4.4288373 -4.4288769 -4.4289122 -4.4289193][-4.4289694 -4.4289751 -4.4289618 -4.4289255 -4.4288688 -4.4288058 -4.4287691 -4.4287648 -4.4287806 -4.4288192 -4.4288626 -4.4288988 -4.4289284 -4.4289618 -4.4289761][-4.4289651 -4.4289727 -4.428968 -4.4289508 -4.4289212 -4.4288855 -4.4288621 -4.4288611 -4.4288754 -4.4289026 -4.4289274 -4.4289479 -4.4289627 -4.4289846 -4.4289966][-4.4289603 -4.4289632 -4.4289589 -4.4289513 -4.4289379 -4.4289217 -4.4289093 -4.4289107 -4.4289217 -4.4289417 -4.4289579 -4.4289689 -4.4289713 -4.4289813 -4.428988][-4.4289589 -4.4289579 -4.4289517 -4.428947 -4.4289422 -4.4289384 -4.4289355 -4.4289389 -4.4289484 -4.42896 -4.4289689 -4.4289746 -4.4289737 -4.4289756 -4.4289765]]...]
INFO - root - 2017-12-08 08:54:01.517112: step 63810, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.222 sec/batch; 16h:32m:34s remains)
INFO - root - 2017-12-08 08:54:03.774097: step 63820, loss = 2.28, batch loss = 2.23 (35.4 examples/sec; 0.226 sec/batch; 16h:53m:09s remains)
INFO - root - 2017-12-08 08:54:06.010461: step 63830, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 16h:39m:04s remains)
INFO - root - 2017-12-08 08:54:08.268569: step 63840, loss = 2.28, batch loss = 2.23 (35.8 examples/sec; 0.223 sec/batch; 16h:39m:28s remains)
INFO - root - 2017-12-08 08:54:10.537745: step 63850, loss = 2.28, batch loss = 2.23 (35.1 examples/sec; 0.228 sec/batch; 17h:00m:59s remains)
INFO - root - 2017-12-08 08:54:12.800190: step 63860, loss = 2.28, batch loss = 2.23 (36.8 examples/sec; 0.217 sec/batch; 16h:12m:51s remains)
INFO - root - 2017-12-08 08:54:15.035950: step 63870, loss = 2.28, batch loss = 2.23 (37.1 examples/sec; 0.215 sec/batch; 16h:04m:44s remains)
INFO - root - 2017-12-08 08:54:17.269083: step 63880, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 16h:23m:57s remains)
INFO - root - 2017-12-08 08:54:19.491997: step 63890, loss = 2.28, batch loss = 2.23 (36.8 examples/sec; 0.217 sec/batch; 16h:12m:22s remains)
INFO - root - 2017-12-08 08:54:21.736084: step 63900, loss = 2.28, batch loss = 2.23 (36.0 examples/sec; 0.222 sec/batch; 16h:34m:03s remains)
2017-12-08 08:54:22.036388: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289312 -4.4289289 -4.4289131 -4.4288883 -4.4288526 -4.4287972 -4.4287395 -4.4286962 -4.428731 -4.4287763 -4.4287958 -4.4288063 -4.4288044 -4.428792 -4.4287825][-4.4289327 -4.4289303 -4.4289141 -4.4288864 -4.4288468 -4.4287887 -4.428731 -4.4286957 -4.4287386 -4.4287853 -4.4288015 -4.4288058 -4.4287944 -4.4287739 -4.42876][-4.4289322 -4.4289303 -4.4289136 -4.428884 -4.4288421 -4.4287844 -4.4287305 -4.4287062 -4.4287581 -4.4287992 -4.4288025 -4.4287939 -4.4287724 -4.4287519 -4.4287381][-4.4289317 -4.4289303 -4.4289155 -4.4288859 -4.428843 -4.4287877 -4.4287333 -4.4287105 -4.4287596 -4.4287915 -4.4287815 -4.4287634 -4.4287434 -4.4287286 -4.4287143][-4.4289327 -4.4289336 -4.4289222 -4.4288936 -4.42885 -4.4287915 -4.4287252 -4.4286852 -4.4287243 -4.4287553 -4.4287438 -4.4287276 -4.4287167 -4.4287033 -4.428689][-4.4289346 -4.4289365 -4.428925 -4.4288945 -4.4288397 -4.4287586 -4.4286566 -4.4285827 -4.4286222 -4.4286795 -4.4286923 -4.4286871 -4.4286857 -4.4286785 -4.4286757][-4.4289355 -4.4289351 -4.4289174 -4.428874 -4.4287972 -4.4286795 -4.4285259 -4.4284081 -4.4284616 -4.4285741 -4.4286389 -4.4286613 -4.4286737 -4.428678 -4.4286866][-4.4289331 -4.4289284 -4.4289026 -4.4288454 -4.4287553 -4.4286213 -4.428442 -4.4282994 -4.4283614 -4.4285159 -4.4286232 -4.4286718 -4.4286971 -4.4287038 -4.42871][-4.4289269 -4.4289203 -4.4288931 -4.4288363 -4.4287577 -4.428648 -4.428493 -4.4283652 -4.4284124 -4.4285455 -4.4286475 -4.4287009 -4.4287281 -4.4287267 -4.42872][-4.4289212 -4.4289136 -4.4288874 -4.4288392 -4.4287767 -4.4286985 -4.4285836 -4.42849 -4.42853 -4.4286203 -4.4286871 -4.4287238 -4.4287434 -4.4287348 -4.428721][-4.4289155 -4.4289041 -4.4288745 -4.42883 -4.4287758 -4.4287143 -4.4286284 -4.4285736 -4.4286203 -4.4286885 -4.4287281 -4.4287491 -4.4287596 -4.4287529 -4.4287415][-4.4289145 -4.4288988 -4.4288683 -4.4288282 -4.428781 -4.4287305 -4.428668 -4.4286442 -4.4286952 -4.4287496 -4.4287715 -4.4287791 -4.4287829 -4.4287825 -4.4287758][-4.4289155 -4.4289 -4.4288716 -4.4288373 -4.4287972 -4.4287596 -4.4287214 -4.4287181 -4.4287663 -4.4288096 -4.4288216 -4.4288216 -4.4288216 -4.4288192 -4.4288082][-4.42891 -4.4288907 -4.428865 -4.4288378 -4.4288068 -4.428782 -4.4287648 -4.4287763 -4.4288139 -4.4288406 -4.4288464 -4.4288435 -4.4288411 -4.4288368 -4.428822][-4.4288979 -4.4288673 -4.4288373 -4.4288135 -4.4287887 -4.4287729 -4.4287734 -4.4287944 -4.4288254 -4.4288445 -4.4288478 -4.4288445 -4.4288411 -4.4288359 -4.4288192]]...]
INFO - root - 2017-12-08 08:54:24.266401: step 63910, loss = 2.28, batch loss = 2.23 (36.8 examples/sec; 0.217 sec/batch; 16h:13m:27s remains)
INFO - root - 2017-12-08 08:54:26.508403: step 63920, loss = 2.28, batch loss = 2.23 (35.8 examples/sec; 0.224 sec/batch; 16h:41m:31s remains)
INFO - root - 2017-12-08 08:54:28.742404: step 63930, loss = 2.28, batch loss = 2.23 (36.0 examples/sec; 0.222 sec/batch; 16h:33m:49s remains)
INFO - root - 2017-12-08 08:54:30.966457: step 63940, loss = 2.28, batch loss = 2.23 (35.2 examples/sec; 0.227 sec/batch; 16h:57m:56s remains)
INFO - root - 2017-12-08 08:54:33.200997: step 63950, loss = 2.28, batch loss = 2.23 (35.8 examples/sec; 0.223 sec/batch; 16h:39m:16s remains)
INFO - root - 2017-12-08 08:54:35.423276: step 63960, loss = 2.28, batch loss = 2.23 (35.3 examples/sec; 0.227 sec/batch; 16h:54m:58s remains)
INFO - root - 2017-12-08 08:54:37.668071: step 63970, loss = 2.28, batch loss = 2.23 (35.8 examples/sec; 0.223 sec/batch; 16h:40m:15s remains)
INFO - root - 2017-12-08 08:54:39.911332: step 63980, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 16h:22m:38s remains)
INFO - root - 2017-12-08 08:54:42.140278: step 63990, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 16h:24m:19s remains)
INFO - root - 2017-12-08 08:54:44.378966: step 64000, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.226 sec/batch; 16h:49m:45s remains)
2017-12-08 08:54:44.682546: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288783 -4.42888 -4.4288793 -4.4288783 -4.4288774 -4.4288797 -4.4288769 -4.428864 -4.4288368 -4.4287868 -4.4287491 -4.4287372 -4.4287534 -4.4287748 -4.4287691][-4.4288955 -4.4288983 -4.428896 -4.4288874 -4.4288745 -4.428865 -4.4288483 -4.4288282 -4.4287992 -4.4287419 -4.4286928 -4.4286771 -4.4286866 -4.4287152 -4.4287262][-4.4288931 -4.4288907 -4.4288807 -4.4288616 -4.4288383 -4.4288187 -4.4287953 -4.4287744 -4.4287519 -4.4287014 -4.4286547 -4.4286318 -4.4286323 -4.428658 -4.4286809][-4.4288611 -4.4288478 -4.4288306 -4.4288063 -4.4287777 -4.4287515 -4.4287243 -4.4287105 -4.428709 -4.4286804 -4.4286375 -4.4286041 -4.4285951 -4.4286184 -4.4286642][-4.4288368 -4.42882 -4.4288015 -4.4287705 -4.4287252 -4.4286776 -4.4286356 -4.4286242 -4.4286385 -4.4286318 -4.4286036 -4.4285684 -4.4285502 -4.4285822 -4.4286633][-4.4288216 -4.42881 -4.4287887 -4.4287448 -4.4286747 -4.4285941 -4.4285264 -4.4285111 -4.4285474 -4.4285727 -4.4285707 -4.4285474 -4.4285254 -4.4285626 -4.4286613][-4.4288073 -4.4287934 -4.4287658 -4.4287076 -4.4286108 -4.4284897 -4.428371 -4.4283309 -4.428401 -4.4284768 -4.4285235 -4.4285436 -4.4285493 -4.428586 -4.4286642][-4.4287906 -4.4287667 -4.4287291 -4.4286561 -4.4285274 -4.4283581 -4.4281607 -4.4280677 -4.4282131 -4.4283962 -4.4285216 -4.428596 -4.4286275 -4.428647 -4.428688][-4.4287777 -4.4287395 -4.4286904 -4.4286094 -4.4284735 -4.4282904 -4.4280548 -4.4279313 -4.4281344 -4.4283829 -4.4285421 -4.4286323 -4.4286671 -4.4286761 -4.4286971][-4.4287748 -4.4287348 -4.4286876 -4.4286256 -4.4285216 -4.4283848 -4.4282184 -4.4281268 -4.4282575 -4.4284444 -4.4285641 -4.4286327 -4.428668 -4.4286804 -4.4286985][-4.428782 -4.4287462 -4.4287009 -4.4286594 -4.4285951 -4.4285111 -4.4284067 -4.4283338 -4.4283938 -4.4285188 -4.4285908 -4.4286323 -4.4286656 -4.4286871 -4.4287109][-4.4287825 -4.4287438 -4.4287014 -4.428668 -4.4286289 -4.4285831 -4.4285188 -4.4284625 -4.4285011 -4.4285951 -4.4286351 -4.42866 -4.4286838 -4.4287004 -4.4287205][-4.4287553 -4.4287229 -4.4286776 -4.4286447 -4.4286256 -4.4286113 -4.42858 -4.428545 -4.428576 -4.4286528 -4.4286847 -4.4286966 -4.4287086 -4.4287157 -4.4287276][-4.4287057 -4.4286819 -4.4286337 -4.4286084 -4.4286165 -4.428628 -4.4286246 -4.4286108 -4.4286284 -4.428688 -4.4287248 -4.428741 -4.4287539 -4.4287581 -4.4287648][-4.4286866 -4.42866 -4.4286141 -4.4285965 -4.4286175 -4.4286418 -4.4286547 -4.4286551 -4.4286661 -4.4287157 -4.4287472 -4.4287581 -4.4287653 -4.4287648 -4.4287682]]...]
INFO - root - 2017-12-08 08:54:46.898362: step 64010, loss = 2.28, batch loss = 2.23 (35.6 examples/sec; 0.225 sec/batch; 16h:45m:42s remains)
INFO - root - 2017-12-08 08:54:49.169585: step 64020, loss = 2.28, batch loss = 2.23 (34.8 examples/sec; 0.230 sec/batch; 17h:09m:16s remains)
INFO - root - 2017-12-08 08:54:51.417599: step 64030, loss = 2.28, batch loss = 2.23 (35.0 examples/sec; 0.229 sec/batch; 17h:04m:08s remains)
INFO - root - 2017-12-08 08:54:53.640783: step 64040, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.220 sec/batch; 16h:25m:32s remains)
INFO - root - 2017-12-08 08:54:55.860627: step 64050, loss = 2.28, batch loss = 2.23 (36.6 examples/sec; 0.218 sec/batch; 16h:17m:20s remains)
INFO - root - 2017-12-08 08:54:58.114672: step 64060, loss = 2.28, batch loss = 2.23 (35.6 examples/sec; 0.225 sec/batch; 16h:45m:42s remains)
INFO - root - 2017-12-08 08:55:00.391890: step 64070, loss = 2.28, batch loss = 2.23 (35.1 examples/sec; 0.228 sec/batch; 17h:00m:40s remains)
INFO - root - 2017-12-08 08:55:02.663740: step 64080, loss = 2.28, batch loss = 2.23 (34.2 examples/sec; 0.234 sec/batch; 17h:26m:16s remains)
INFO - root - 2017-12-08 08:55:04.886944: step 64090, loss = 2.28, batch loss = 2.23 (36.9 examples/sec; 0.217 sec/batch; 16h:08m:38s remains)
INFO - root - 2017-12-08 08:55:07.118779: step 64100, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.220 sec/batch; 16h:24m:59s remains)
2017-12-08 08:55:07.459427: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289975 -4.4289918 -4.4289842 -4.4289794 -4.4289765 -4.428977 -4.4289784 -4.42898 -4.4289804 -4.4289808 -4.428978 -4.4289689 -4.4289608 -4.4289584 -4.4289613][-4.4289551 -4.4289479 -4.4289412 -4.4289389 -4.42894 -4.4289446 -4.4289503 -4.4289536 -4.4289536 -4.4289541 -4.4289484 -4.4289331 -4.4289207 -4.4289203 -4.4289217][-4.4289141 -4.4289088 -4.4289045 -4.4289074 -4.4289136 -4.4289241 -4.4289312 -4.4289308 -4.4289269 -4.4289269 -4.4289203 -4.428906 -4.4288936 -4.4288936 -4.4288907][-4.428894 -4.4288912 -4.4288874 -4.4288869 -4.4288869 -4.4288874 -4.4288836 -4.4288759 -4.4288707 -4.4288764 -4.4288783 -4.4288764 -4.4288735 -4.428874 -4.4288659][-4.4288669 -4.4288559 -4.4288406 -4.4288268 -4.4288087 -4.4287896 -4.428771 -4.4287581 -4.4287615 -4.428782 -4.428802 -4.4288187 -4.4288316 -4.4288445 -4.4288445][-4.4287858 -4.4287543 -4.4287214 -4.4286957 -4.4286661 -4.4286265 -4.428586 -4.4285645 -4.4285865 -4.4286366 -4.4286785 -4.4287152 -4.4287443 -4.428782 -4.4288111][-4.428688 -4.4286342 -4.4285827 -4.428544 -4.4285026 -4.4284348 -4.4283476 -4.4283 -4.4283552 -4.4284592 -4.4285383 -4.4285946 -4.4286323 -4.4286885 -4.4287519][-4.4286351 -4.4285746 -4.4285073 -4.4284506 -4.4283981 -4.4283137 -4.428185 -4.4281049 -4.4282 -4.4283586 -4.428473 -4.4285359 -4.428566 -4.4286203 -4.4286971][-4.4286461 -4.4286051 -4.428544 -4.428483 -4.4284377 -4.4283829 -4.428298 -4.4282513 -4.4283304 -4.4284568 -4.4285488 -4.4285879 -4.4285908 -4.4286237 -4.4286776][-4.4287052 -4.4286909 -4.4286523 -4.4286089 -4.4285817 -4.42856 -4.4285297 -4.4285164 -4.4285555 -4.4286156 -4.4286494 -4.4286427 -4.4286165 -4.4286227 -4.4286432][-4.428772 -4.4287729 -4.4287529 -4.4287248 -4.4287076 -4.4287004 -4.4286947 -4.4286857 -4.4286838 -4.4286857 -4.4286723 -4.428638 -4.4286003 -4.4285946 -4.4286046][-4.4288068 -4.4288154 -4.4288073 -4.4287934 -4.4287839 -4.428781 -4.4287758 -4.4287562 -4.4287276 -4.4286966 -4.4286652 -4.4286304 -4.4285979 -4.4285932 -4.4286122][-4.4288116 -4.4288244 -4.42882 -4.4288139 -4.4288092 -4.4288068 -4.4287953 -4.4287691 -4.4287324 -4.4287 -4.428679 -4.4286637 -4.4286423 -4.4286389 -4.4286556][-4.4288111 -4.4288263 -4.4288225 -4.4288177 -4.4288139 -4.4288073 -4.4287877 -4.4287548 -4.4287214 -4.4287038 -4.4287033 -4.4287038 -4.428689 -4.4286852 -4.4287014][-4.4288421 -4.4288616 -4.4288592 -4.4288526 -4.4288459 -4.4288321 -4.4288058 -4.4287744 -4.4287543 -4.4287534 -4.4287648 -4.4287724 -4.4287653 -4.4287639 -4.4287777]]...]
INFO - root - 2017-12-08 08:55:09.753058: step 64110, loss = 2.28, batch loss = 2.23 (34.7 examples/sec; 0.231 sec/batch; 17h:11m:54s remains)
INFO - root - 2017-12-08 08:55:12.030169: step 64120, loss = 2.28, batch loss = 2.23 (34.1 examples/sec; 0.235 sec/batch; 17h:30m:08s remains)
INFO - root - 2017-12-08 08:55:14.258816: step 64130, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 16h:22m:17s remains)
INFO - root - 2017-12-08 08:55:16.490936: step 64140, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.225 sec/batch; 16h:46m:35s remains)
INFO - root - 2017-12-08 08:55:18.713443: step 64150, loss = 2.28, batch loss = 2.23 (36.7 examples/sec; 0.218 sec/batch; 16h:16m:13s remains)
INFO - root - 2017-12-08 08:55:21.000735: step 64160, loss = 2.28, batch loss = 2.23 (34.8 examples/sec; 0.230 sec/batch; 17h:07m:20s remains)
INFO - root - 2017-12-08 08:55:23.282634: step 64170, loss = 2.28, batch loss = 2.23 (32.0 examples/sec; 0.250 sec/batch; 18h:36m:49s remains)
INFO - root - 2017-12-08 08:55:25.538650: step 64180, loss = 2.28, batch loss = 2.23 (35.4 examples/sec; 0.226 sec/batch; 16h:51m:17s remains)
INFO - root - 2017-12-08 08:55:27.779465: step 64190, loss = 2.28, batch loss = 2.23 (37.2 examples/sec; 0.215 sec/batch; 16h:01m:24s remains)
INFO - root - 2017-12-08 08:55:30.020465: step 64200, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.225 sec/batch; 16h:46m:44s remains)
2017-12-08 08:55:30.330467: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289021 -4.4288454 -4.4287529 -4.428658 -4.4286132 -4.428586 -4.4286284 -4.4287496 -4.4288607 -4.4289103 -4.4288936 -4.4288511 -4.4288063 -4.4287534 -4.4287][-4.4288855 -4.4288321 -4.4287491 -4.4286537 -4.4286027 -4.4285712 -4.4286132 -4.4287357 -4.4288316 -4.428864 -4.4288325 -4.4287786 -4.4287262 -4.4286566 -4.4285836][-4.4288774 -4.4288383 -4.4287724 -4.428679 -4.4286165 -4.4285789 -4.4286113 -4.4287133 -4.4287777 -4.428792 -4.4287515 -4.4286919 -4.428638 -4.4285665 -4.4284878][-4.4288726 -4.4288411 -4.4287872 -4.4287033 -4.4286318 -4.4285736 -4.4285703 -4.428627 -4.4286556 -4.4286656 -4.4286437 -4.4286046 -4.4285717 -4.4285135 -4.4284372][-4.4288549 -4.4288206 -4.4287753 -4.4287114 -4.4286346 -4.4285407 -4.428473 -4.4284554 -4.4284482 -4.4284678 -4.4284968 -4.4285159 -4.4285321 -4.4285164 -4.4284682][-4.4288373 -4.428793 -4.42874 -4.4286704 -4.4285693 -4.4284291 -4.4283051 -4.4282284 -4.4281969 -4.4282436 -4.4283471 -4.4284449 -4.4285231 -4.4285526 -4.4285336][-4.4288383 -4.4287825 -4.4287105 -4.4286189 -4.4284964 -4.4283319 -4.4281912 -4.4281015 -4.4280815 -4.428153 -4.4282908 -4.4284267 -4.4285378 -4.4285955 -4.428596][-4.4288445 -4.4287724 -4.4286728 -4.4285693 -4.4284534 -4.4283071 -4.4281969 -4.4281406 -4.4281454 -4.4282074 -4.4283266 -4.42845 -4.4285522 -4.4286137 -4.428617][-4.428834 -4.4287496 -4.42864 -4.4285455 -4.4284668 -4.4283695 -4.4283032 -4.4282823 -4.4282966 -4.428339 -4.4284248 -4.4285064 -4.4285645 -4.4285979 -4.42859][-4.4288282 -4.4287515 -4.428659 -4.4285893 -4.4285474 -4.4284887 -4.428443 -4.4284329 -4.4284353 -4.4284511 -4.4285016 -4.428544 -4.4285674 -4.4285803 -4.4285626][-4.4288363 -4.4287796 -4.4287133 -4.4286652 -4.42864 -4.4285975 -4.4285622 -4.428555 -4.4285502 -4.4285474 -4.4285688 -4.4285808 -4.4285831 -4.4285889 -4.4285712][-4.42886 -4.4288197 -4.4287734 -4.4287362 -4.428719 -4.4286885 -4.4286609 -4.4286633 -4.4286709 -4.4286652 -4.4286666 -4.4286585 -4.4286475 -4.4286475 -4.42863][-4.42889 -4.42886 -4.4288278 -4.4288044 -4.4287972 -4.4287834 -4.4287682 -4.4287786 -4.4287958 -4.428792 -4.4287829 -4.4287682 -4.4287505 -4.4287472 -4.4287348][-4.4289231 -4.4289012 -4.4288812 -4.4288688 -4.42887 -4.4288673 -4.4288597 -4.4288664 -4.4288793 -4.4288754 -4.4288664 -4.4288559 -4.4288468 -4.4288516 -4.4288511][-4.428946 -4.4289269 -4.4289117 -4.4289031 -4.4289036 -4.4289012 -4.4288955 -4.4288955 -4.4288988 -4.4288936 -4.4288893 -4.4288864 -4.4288874 -4.4289002 -4.4289093]]...]
INFO - root - 2017-12-08 08:55:32.564050: step 64210, loss = 2.28, batch loss = 2.23 (35.6 examples/sec; 0.225 sec/batch; 16h:44m:40s remains)
INFO - root - 2017-12-08 08:55:34.825726: step 64220, loss = 2.28, batch loss = 2.23 (37.0 examples/sec; 0.216 sec/batch; 16h:06m:50s remains)
INFO - root - 2017-12-08 08:55:37.062297: step 64230, loss = 2.28, batch loss = 2.23 (37.0 examples/sec; 0.216 sec/batch; 16h:06m:54s remains)
INFO - root - 2017-12-08 08:55:39.319651: step 64240, loss = 2.28, batch loss = 2.23 (35.7 examples/sec; 0.224 sec/batch; 16h:40m:53s remains)
INFO - root - 2017-12-08 08:55:41.556546: step 64250, loss = 2.28, batch loss = 2.23 (35.3 examples/sec; 0.226 sec/batch; 16h:52m:05s remains)
INFO - root - 2017-12-08 08:55:43.798004: step 64260, loss = 2.28, batch loss = 2.23 (35.3 examples/sec; 0.227 sec/batch; 16h:53m:29s remains)
INFO - root - 2017-12-08 08:55:46.047285: step 64270, loss = 2.28, batch loss = 2.23 (35.2 examples/sec; 0.227 sec/batch; 16h:56m:18s remains)
INFO - root - 2017-12-08 08:55:48.264861: step 64280, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 16h:20m:10s remains)
INFO - root - 2017-12-08 08:55:50.496939: step 64290, loss = 2.28, batch loss = 2.23 (36.7 examples/sec; 0.218 sec/batch; 16h:13m:27s remains)
INFO - root - 2017-12-08 08:55:52.733776: step 64300, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 16h:26m:48s remains)
2017-12-08 08:55:53.059654: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4287291 -4.4287395 -4.4287462 -4.4287624 -4.4287868 -4.4288073 -4.42882 -4.4288163 -4.4288111 -4.4288135 -4.4288054 -4.4287887 -4.4287767 -4.4287853 -4.4288139][-4.4286737 -4.4286728 -4.4286718 -4.4286866 -4.4287176 -4.4287477 -4.4287696 -4.4287715 -4.4287691 -4.42877 -4.42876 -4.4287453 -4.4287386 -4.4287534 -4.4287887][-4.4286318 -4.4286165 -4.4286089 -4.4286242 -4.428659 -4.4286952 -4.4287243 -4.4287291 -4.4287295 -4.4287319 -4.4287162 -4.4286995 -4.4286985 -4.428719 -4.4287558][-4.4286308 -4.428607 -4.4285922 -4.4286046 -4.42863 -4.4286613 -4.4286904 -4.4286962 -4.4287014 -4.4287057 -4.4286904 -4.4286733 -4.4286771 -4.4286995 -4.4287333][-4.4285994 -4.428586 -4.4285784 -4.4285913 -4.4286013 -4.4286113 -4.4286213 -4.4286251 -4.4286385 -4.4286475 -4.4286413 -4.4286284 -4.4286327 -4.4286542 -4.4286928][-4.4285316 -4.428535 -4.4285378 -4.4285526 -4.4285541 -4.4285355 -4.4285131 -4.4285188 -4.4285612 -4.4285836 -4.4285827 -4.42857 -4.4285741 -4.4285979 -4.4286423][-4.4284735 -4.4284792 -4.4284768 -4.4284811 -4.4284682 -4.4284067 -4.4283295 -4.42835 -4.4284458 -4.4285021 -4.428515 -4.428514 -4.4285345 -4.4285665 -4.4286141][-4.4284506 -4.4284425 -4.4284277 -4.4284124 -4.4283633 -4.4282331 -4.4280777 -4.4281178 -4.4282837 -4.4283829 -4.4284072 -4.42842 -4.4284759 -4.4285398 -4.4286][-4.428484 -4.4284692 -4.4284558 -4.4284348 -4.4283676 -4.4281988 -4.4280005 -4.4280367 -4.4282146 -4.428318 -4.4283385 -4.428359 -4.4284358 -4.42852 -4.4285889][-4.4285512 -4.4285398 -4.42854 -4.42853 -4.4284763 -4.4283504 -4.4282126 -4.4282317 -4.4283433 -4.428401 -4.4283996 -4.4284058 -4.4284635 -4.4285369 -4.4286032][-4.4285893 -4.4285851 -4.4285932 -4.4285917 -4.4285579 -4.4284844 -4.4284163 -4.428431 -4.4284897 -4.428514 -4.4284978 -4.428483 -4.4285111 -4.4285641 -4.4286261][-4.4286304 -4.4286284 -4.4286408 -4.4286489 -4.4286318 -4.4285936 -4.4285636 -4.4285755 -4.4286036 -4.4286094 -4.4285846 -4.4285645 -4.4285755 -4.4286103 -4.4286609][-4.4286981 -4.4286981 -4.4287095 -4.428719 -4.4287114 -4.4286942 -4.4286842 -4.4286923 -4.4287038 -4.4286957 -4.4286704 -4.4286594 -4.4286695 -4.4286904 -4.4287233][-4.4287844 -4.4287868 -4.4287949 -4.4288015 -4.4287992 -4.4287949 -4.428792 -4.4287949 -4.4287977 -4.4287896 -4.4287763 -4.4287763 -4.4287877 -4.4288006 -4.4288187][-4.4288726 -4.428875 -4.4288793 -4.4288826 -4.4288821 -4.4288797 -4.4288754 -4.4288726 -4.428874 -4.4288735 -4.4288712 -4.4288759 -4.4288864 -4.428896 -4.4289064]]...]
INFO - root - 2017-12-08 08:55:55.281432: step 64310, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 16h:28m:28s remains)
INFO - root - 2017-12-08 08:55:57.522164: step 64320, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 16h:19m:56s remains)
INFO - root - 2017-12-08 08:55:59.745844: step 64330, loss = 2.28, batch loss = 2.23 (36.9 examples/sec; 0.217 sec/batch; 16h:08m:40s remains)
INFO - root - 2017-12-08 08:56:01.989458: step 64340, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 16h:36m:43s remains)
INFO - root - 2017-12-08 08:56:04.229491: step 64350, loss = 2.28, batch loss = 2.23 (35.2 examples/sec; 0.227 sec/batch; 16h:55m:54s remains)
INFO - root - 2017-12-08 08:56:06.454808: step 64360, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.220 sec/batch; 16h:24m:26s remains)
INFO - root - 2017-12-08 08:56:08.677205: step 64370, loss = 2.28, batch loss = 2.23 (35.3 examples/sec; 0.227 sec/batch; 16h:52m:17s remains)
INFO - root - 2017-12-08 08:56:10.898152: step 64380, loss = 2.28, batch loss = 2.23 (36.7 examples/sec; 0.218 sec/batch; 16h:14m:06s remains)
INFO - root - 2017-12-08 08:56:13.139353: step 64390, loss = 2.28, batch loss = 2.23 (35.6 examples/sec; 0.225 sec/batch; 16h:44m:33s remains)
INFO - root - 2017-12-08 08:56:15.372015: step 64400, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.221 sec/batch; 16h:29m:35s remains)
2017-12-08 08:56:15.662396: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289041 -4.4289017 -4.4288993 -4.4288945 -4.4288883 -4.4288816 -4.4288774 -4.4288764 -4.4288769 -4.42888 -4.4288816 -4.4288778 -4.4288707 -4.4288578 -4.4288459][-4.4288807 -4.42888 -4.4288864 -4.4288912 -4.4288931 -4.4288921 -4.4288926 -4.4288955 -4.4289002 -4.4289055 -4.4289045 -4.428896 -4.4288826 -4.4288583 -4.4288359][-4.4288526 -4.4288535 -4.4288669 -4.4288807 -4.4288893 -4.4288917 -4.4288907 -4.428894 -4.4289007 -4.4289074 -4.4289088 -4.4289041 -4.4288936 -4.4288664 -4.4288378][-4.4288282 -4.4288254 -4.4288363 -4.4288492 -4.4288507 -4.4288425 -4.4288297 -4.4288273 -4.4288363 -4.4288473 -4.428854 -4.4288554 -4.428853 -4.4288316 -4.4288068][-4.4288177 -4.4288044 -4.428802 -4.4288044 -4.4287891 -4.4287615 -4.4287324 -4.4287229 -4.4287357 -4.4287529 -4.4287639 -4.428772 -4.4287767 -4.4287615 -4.4287453][-4.428792 -4.4287596 -4.4287376 -4.4287157 -4.4286766 -4.4286284 -4.4285851 -4.4285727 -4.4285946 -4.4286203 -4.4286404 -4.4286623 -4.4286785 -4.4286766 -4.4286766][-4.4287119 -4.4286547 -4.4286051 -4.4285507 -4.4284821 -4.4284091 -4.4283466 -4.4283347 -4.4283738 -4.4284215 -4.4284649 -4.4285111 -4.428546 -4.4285626 -4.4285846][-4.4285607 -4.4284754 -4.4283991 -4.4283147 -4.4282107 -4.4281054 -4.4280152 -4.4280005 -4.4280667 -4.4281473 -4.4282289 -4.4283118 -4.4283743 -4.4284124 -4.4284506][-4.4284258 -4.4283328 -4.4282556 -4.42817 -4.4280562 -4.4279394 -4.4278369 -4.42782 -4.4279051 -4.4280105 -4.4281178 -4.4282217 -4.4282947 -4.4283347 -4.4283705][-4.4283862 -4.4283314 -4.4283066 -4.4282813 -4.42822 -4.4281507 -4.4280896 -4.4280796 -4.4281406 -4.4282203 -4.4282975 -4.4283619 -4.4284 -4.4284058 -4.4284081][-4.4283977 -4.4283934 -4.428421 -4.4284468 -4.4284339 -4.428412 -4.4283905 -4.4283924 -4.4284306 -4.4284754 -4.4285083 -4.4285274 -4.42853 -4.428504 -4.4284754][-4.4284568 -4.4284816 -4.4285331 -4.4285746 -4.4285779 -4.4285693 -4.4285622 -4.4285684 -4.4285908 -4.428616 -4.4286289 -4.4286313 -4.4286208 -4.4285855 -4.4285488][-4.4286132 -4.4286442 -4.4286923 -4.4287295 -4.4287333 -4.4287262 -4.4287219 -4.4287267 -4.4287391 -4.4287529 -4.42876 -4.4287605 -4.4287519 -4.4287238 -4.4286962][-4.4288116 -4.4288387 -4.4288712 -4.4288945 -4.4288955 -4.4288878 -4.4288831 -4.4288855 -4.4288907 -4.4288969 -4.4289002 -4.4289036 -4.4289045 -4.4288907 -4.4288769][-4.4289446 -4.428967 -4.4289856 -4.428997 -4.4289961 -4.4289913 -4.428988 -4.4289885 -4.4289894 -4.42899 -4.42899 -4.4289923 -4.4289951 -4.4289908 -4.4289861]]...]
INFO - root - 2017-12-08 08:56:17.877684: step 64410, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 16h:21m:44s remains)
INFO - root - 2017-12-08 08:56:20.144451: step 64420, loss = 2.28, batch loss = 2.23 (35.6 examples/sec; 0.225 sec/batch; 16h:44m:57s remains)
INFO - root - 2017-12-08 08:56:22.427182: step 64430, loss = 2.28, batch loss = 2.23 (35.7 examples/sec; 0.224 sec/batch; 16h:41m:51s remains)
INFO - root - 2017-12-08 08:56:24.708861: step 64440, loss = 2.28, batch loss = 2.23 (34.3 examples/sec; 0.233 sec/batch; 17h:22m:39s remains)
INFO - root - 2017-12-08 08:56:26.939852: step 64450, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 16h:27m:17s remains)
INFO - root - 2017-12-08 08:56:29.157777: step 64460, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 16h:28m:05s remains)
INFO - root - 2017-12-08 08:56:31.402731: step 64470, loss = 2.28, batch loss = 2.23 (36.6 examples/sec; 0.219 sec/batch; 16h:17m:18s remains)
INFO - root - 2017-12-08 08:56:33.614974: step 64480, loss = 2.28, batch loss = 2.23 (36.6 examples/sec; 0.219 sec/batch; 16h:16m:14s remains)
INFO - root - 2017-12-08 08:56:35.861663: step 64490, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.220 sec/batch; 16h:24m:08s remains)
INFO - root - 2017-12-08 08:56:38.093838: step 64500, loss = 2.28, batch loss = 2.23 (35.0 examples/sec; 0.229 sec/batch; 17h:01m:33s remains)
2017-12-08 08:56:38.417992: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288025 -4.4287925 -4.4287848 -4.428781 -4.4287639 -4.4287267 -4.428699 -4.4287128 -4.4287524 -4.4287844 -4.4287925 -4.4287744 -4.4287338 -4.4286928 -4.4286747][-4.4288225 -4.4288092 -4.4288011 -4.4287958 -4.4287763 -4.42874 -4.4287138 -4.4287214 -4.4287529 -4.4287734 -4.4287744 -4.428751 -4.4287047 -4.4286647 -4.4286485][-4.4288387 -4.4288268 -4.4288149 -4.4288068 -4.4287868 -4.4287543 -4.42873 -4.42873 -4.4287472 -4.4287505 -4.4287443 -4.4287186 -4.4286728 -4.4286418 -4.4286351][-4.4288559 -4.428843 -4.4288235 -4.4288063 -4.4287806 -4.4287486 -4.4287276 -4.4287219 -4.4287286 -4.4287271 -4.4287229 -4.4287033 -4.4286637 -4.42864 -4.4286332][-4.428863 -4.4288468 -4.4288182 -4.4287906 -4.4287562 -4.42872 -4.4286971 -4.4286847 -4.4286861 -4.4286904 -4.4286971 -4.4286914 -4.4286613 -4.4286356 -4.4286165][-4.4288521 -4.4288321 -4.4287972 -4.4287596 -4.4287128 -4.4286652 -4.4286294 -4.4285994 -4.4285946 -4.4286132 -4.4286408 -4.4286547 -4.4286366 -4.4286113 -4.4285841][-4.4288244 -4.4288011 -4.4287672 -4.4287224 -4.4286618 -4.4285984 -4.4285383 -4.4284768 -4.4284563 -4.4284835 -4.4285355 -4.4285784 -4.428587 -4.4285803 -4.4285669][-4.428792 -4.4287677 -4.4287343 -4.4286804 -4.4286089 -4.4285331 -4.4284458 -4.4283476 -4.4283037 -4.42834 -4.4284244 -4.428514 -4.4285679 -4.4285893 -4.4285979][-4.4287753 -4.4287448 -4.4287086 -4.4286532 -4.4285855 -4.4285078 -4.4284067 -4.4282913 -4.4282312 -4.4282722 -4.4283814 -4.4285092 -4.4286065 -4.4286585 -4.4286866][-4.4287806 -4.4287443 -4.4287071 -4.4286575 -4.4286036 -4.428544 -4.4284668 -4.4283819 -4.4283376 -4.42837 -4.42846 -4.4285765 -4.4286804 -4.4287434 -4.42878][-4.428803 -4.4287663 -4.4287319 -4.4286914 -4.4286547 -4.4286218 -4.4285851 -4.428545 -4.4285278 -4.4285531 -4.4286065 -4.4286814 -4.4287553 -4.4288068 -4.4288406][-4.428833 -4.4288073 -4.4287853 -4.4287605 -4.4287395 -4.4287267 -4.4287181 -4.4287081 -4.4287076 -4.4287229 -4.4287453 -4.4287772 -4.4288149 -4.4288445 -4.4288673][-4.4288573 -4.4288483 -4.4288421 -4.4288325 -4.4288235 -4.428822 -4.4288263 -4.4288263 -4.4288268 -4.4288325 -4.4288344 -4.4288378 -4.4288468 -4.4288559 -4.4288654][-4.42888 -4.4288831 -4.4288859 -4.4288831 -4.4288793 -4.4288812 -4.4288874 -4.4288878 -4.4288869 -4.4288878 -4.4288831 -4.4288745 -4.4288697 -4.42887 -4.4288754][-4.4288964 -4.428905 -4.4289107 -4.42891 -4.4289069 -4.4289083 -4.4289126 -4.4289136 -4.4289122 -4.4289126 -4.4289079 -4.4289 -4.428894 -4.4288955 -4.4289031]]...]
INFO - root - 2017-12-08 08:56:40.635930: step 64510, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.225 sec/batch; 16h:45m:40s remains)
INFO - root - 2017-12-08 08:56:42.864023: step 64520, loss = 2.28, batch loss = 2.23 (34.7 examples/sec; 0.231 sec/batch; 17h:09m:35s remains)
INFO - root - 2017-12-08 08:56:45.126159: step 64530, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.222 sec/batch; 16h:30m:18s remains)
INFO - root - 2017-12-08 08:56:47.367863: step 64540, loss = 2.28, batch loss = 2.23 (35.7 examples/sec; 0.224 sec/batch; 16h:40m:26s remains)
INFO - root - 2017-12-08 08:56:49.589513: step 64550, loss = 2.28, batch loss = 2.23 (36.0 examples/sec; 0.222 sec/batch; 16h:33m:35s remains)
INFO - root - 2017-12-08 08:56:51.818868: step 64560, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 16h:20m:47s remains)
INFO - root - 2017-12-08 08:56:54.063262: step 64570, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.225 sec/batch; 16h:46m:33s remains)
INFO - root - 2017-12-08 08:56:56.271688: step 64580, loss = 2.28, batch loss = 2.23 (36.9 examples/sec; 0.217 sec/batch; 16h:09m:01s remains)
INFO - root - 2017-12-08 08:56:58.517662: step 64590, loss = 2.28, batch loss = 2.23 (35.1 examples/sec; 0.228 sec/batch; 16h:58m:01s remains)
INFO - root - 2017-12-08 08:57:00.760922: step 64600, loss = 2.28, batch loss = 2.23 (35.2 examples/sec; 0.227 sec/batch; 16h:54m:26s remains)
2017-12-08 08:57:01.059326: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.428659 -4.428678 -4.4287081 -4.4287357 -4.4287663 -4.4287944 -4.4288049 -4.4288025 -4.4287934 -4.4287963 -4.4287877 -4.4287744 -4.4287772 -4.428771 -4.4287686][-4.4286475 -4.4286814 -4.4287262 -4.42876 -4.4287915 -4.4288182 -4.4288354 -4.42884 -4.4288368 -4.4288468 -4.4288387 -4.428822 -4.4288082 -4.4287915 -4.4287825][-4.4286833 -4.4287186 -4.4287524 -4.4287748 -4.4287958 -4.4288111 -4.4288211 -4.4288211 -4.4288158 -4.4288225 -4.4288144 -4.4287949 -4.428772 -4.4287491 -4.4287448][-4.4287567 -4.4287844 -4.4287944 -4.4287839 -4.4287739 -4.4287577 -4.4287457 -4.4287343 -4.4287257 -4.4287291 -4.4287338 -4.4287381 -4.4287376 -4.4287229 -4.4287219][-4.42882 -4.4288344 -4.4288144 -4.428772 -4.4287291 -4.4286618 -4.4285936 -4.4285655 -4.4285707 -4.4285865 -4.4286203 -4.42866 -4.4287038 -4.42873 -4.4287467][-4.428844 -4.4288268 -4.4287658 -4.4286823 -4.4286 -4.428473 -4.4283304 -4.4282842 -4.428329 -4.4283948 -4.4284739 -4.4285555 -4.4286518 -4.4287281 -4.4287686][-4.4288344 -4.42877 -4.4286489 -4.4285026 -4.4283662 -4.4281821 -4.4279761 -4.4279346 -4.4280367 -4.4281693 -4.4283056 -4.4284358 -4.4285831 -4.4286957 -4.4287548][-4.4288154 -4.4287148 -4.4285564 -4.42837 -4.4282036 -4.4280105 -4.4277978 -4.4277797 -4.42791 -4.428071 -4.4282351 -4.4283876 -4.4285555 -4.4286819 -4.4287467][-4.4288139 -4.4287195 -4.4285827 -4.4284244 -4.4282947 -4.4281764 -4.4280539 -4.4280529 -4.4281306 -4.4282384 -4.4283643 -4.4284792 -4.428607 -4.4287076 -4.4287558][-4.4287982 -4.4287424 -4.4286709 -4.4285755 -4.4284983 -4.42845 -4.4284129 -4.4284158 -4.4284344 -4.428472 -4.4285345 -4.4285936 -4.4286675 -4.42873 -4.4287605][-4.428802 -4.4287848 -4.4287658 -4.4287271 -4.4286966 -4.4286885 -4.4286952 -4.4287014 -4.4286938 -4.4286866 -4.4287019 -4.4287086 -4.4287357 -4.4287696 -4.4287834][-4.4288378 -4.4288411 -4.4288454 -4.4288383 -4.4288282 -4.4288311 -4.4288468 -4.4288568 -4.4288497 -4.4288368 -4.4288287 -4.4288082 -4.428803 -4.42881 -4.4288116][-4.4288821 -4.4288945 -4.4289036 -4.4289036 -4.4288983 -4.4289 -4.4289064 -4.4289145 -4.4289165 -4.4289103 -4.4289007 -4.4288774 -4.4288611 -4.4288588 -4.4288597][-4.4289069 -4.4289222 -4.4289322 -4.4289336 -4.4289303 -4.4289312 -4.4289336 -4.4289412 -4.4289494 -4.4289503 -4.428946 -4.4289303 -4.4289207 -4.4289212 -4.4289241][-4.428926 -4.4289384 -4.4289441 -4.4289451 -4.4289427 -4.4289427 -4.4289436 -4.42895 -4.4289589 -4.4289627 -4.4289608 -4.4289532 -4.4289527 -4.4289589 -4.4289632]]...]
INFO - root - 2017-12-08 08:57:03.265615: step 64610, loss = 2.28, batch loss = 2.23 (34.8 examples/sec; 0.230 sec/batch; 17h:06m:03s remains)
INFO - root - 2017-12-08 08:57:05.530797: step 64620, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 16h:35m:54s remains)
INFO - root - 2017-12-08 08:57:07.789203: step 64630, loss = 2.28, batch loss = 2.23 (35.2 examples/sec; 0.227 sec/batch; 16h:53m:29s remains)
INFO - root - 2017-12-08 08:57:10.032108: step 64640, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.225 sec/batch; 16h:45m:14s remains)
INFO - root - 2017-12-08 08:57:12.257753: step 64650, loss = 2.28, batch loss = 2.23 (35.8 examples/sec; 0.223 sec/batch; 16h:36m:17s remains)
INFO - root - 2017-12-08 08:57:14.486353: step 64660, loss = 2.28, batch loss = 2.23 (35.0 examples/sec; 0.228 sec/batch; 16h:59m:52s remains)
INFO - root - 2017-12-08 08:57:16.709594: step 64670, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 16h:33m:40s remains)
INFO - root - 2017-12-08 08:57:18.944149: step 64680, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.220 sec/batch; 16h:23m:45s remains)
INFO - root - 2017-12-08 08:57:21.175219: step 64690, loss = 2.28, batch loss = 2.23 (35.4 examples/sec; 0.226 sec/batch; 16h:49m:03s remains)
INFO - root - 2017-12-08 08:57:23.415324: step 64700, loss = 2.28, batch loss = 2.23 (36.6 examples/sec; 0.219 sec/batch; 16h:16m:05s remains)
2017-12-08 08:57:23.700904: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4287033 -4.4286661 -4.4286652 -4.4287066 -4.4287624 -4.4287972 -4.4288125 -4.4287729 -4.4287214 -4.4287319 -4.4287777 -4.428803 -4.4287839 -4.4287348 -4.4286933][-4.4286771 -4.4286308 -4.4286075 -4.42864 -4.4287167 -4.4287848 -4.4288354 -4.4288239 -4.4287829 -4.4287844 -4.428813 -4.4288278 -4.4288111 -4.4287648 -4.4287167][-4.4286938 -4.4286408 -4.4285903 -4.4285846 -4.4286466 -4.4287295 -4.4288158 -4.4288507 -4.4288406 -4.4288349 -4.4288354 -4.4288373 -4.4288216 -4.4287786 -4.4287181][-4.4287243 -4.4286742 -4.428616 -4.4285789 -4.4286022 -4.4286704 -4.4287739 -4.4288473 -4.4288635 -4.4288516 -4.4288292 -4.4288158 -4.4288006 -4.4287696 -4.4287105][-4.4287515 -4.4287109 -4.4286609 -4.4285994 -4.4285707 -4.4285922 -4.4286757 -4.428772 -4.4288182 -4.428822 -4.4287934 -4.4287691 -4.4287581 -4.4287462 -4.4287157][-4.4287457 -4.428719 -4.4286723 -4.4285893 -4.4284945 -4.428431 -4.4284577 -4.4285746 -4.4286861 -4.4287348 -4.428719 -4.4286857 -4.4286804 -4.4287019 -4.4287181][-4.4286437 -4.4286265 -4.4285836 -4.4284835 -4.4283252 -4.42815 -4.4280882 -4.428236 -4.428443 -4.428565 -4.4285817 -4.4285507 -4.4285526 -4.428606 -4.4286618][-4.4285297 -4.4285188 -4.4284763 -4.4283772 -4.4282026 -4.4279637 -4.42783 -4.4279909 -4.4282584 -4.428431 -4.4284768 -4.4284649 -4.4284749 -4.4285464 -4.4286308][-4.4285727 -4.4285674 -4.4285383 -4.4284658 -4.4283404 -4.4281549 -4.4280467 -4.4281449 -4.42835 -4.4285016 -4.4285545 -4.4285579 -4.4285717 -4.4286327 -4.4287119][-4.4286714 -4.4286661 -4.4286542 -4.4286156 -4.4285579 -4.4284539 -4.4283829 -4.4284191 -4.4285326 -4.4286346 -4.4286847 -4.4286966 -4.4286981 -4.4287267 -4.428772][-4.4287195 -4.4287176 -4.4287233 -4.4287143 -4.4287047 -4.4286633 -4.4286213 -4.4286218 -4.42867 -4.4287276 -4.4287653 -4.4287682 -4.4287519 -4.4287543 -4.4287691][-4.4287443 -4.4287434 -4.4287572 -4.4287667 -4.4287815 -4.4287786 -4.42876 -4.4287524 -4.4287705 -4.4287968 -4.4288077 -4.4287858 -4.4287539 -4.4287453 -4.4287539][-4.4287729 -4.4287677 -4.4287744 -4.4287863 -4.4288 -4.4288054 -4.428802 -4.4288063 -4.4288216 -4.4288278 -4.4288149 -4.4287715 -4.4287338 -4.4287276 -4.4287457][-4.4287987 -4.4287915 -4.4287906 -4.4287934 -4.4287915 -4.4287872 -4.428792 -4.4288096 -4.4288244 -4.4288192 -4.4287863 -4.4287305 -4.4286942 -4.4286966 -4.4287257][-4.4288058 -4.4287982 -4.4287896 -4.428782 -4.4287634 -4.4287477 -4.4287539 -4.4287753 -4.4287858 -4.4287682 -4.4287214 -4.42866 -4.4286284 -4.4286466 -4.4286871]]...]
INFO - root - 2017-12-08 08:57:25.931053: step 64710, loss = 2.28, batch loss = 2.23 (34.6 examples/sec; 0.231 sec/batch; 17h:10m:46s remains)
INFO - root - 2017-12-08 08:57:28.162260: step 64720, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.220 sec/batch; 16h:23m:53s remains)
INFO - root - 2017-12-08 08:57:30.391893: step 64730, loss = 2.28, batch loss = 2.23 (35.8 examples/sec; 0.223 sec/batch; 16h:36m:56s remains)
INFO - root - 2017-12-08 08:57:32.706534: step 64740, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 16h:25m:37s remains)
INFO - root - 2017-12-08 08:57:34.933805: step 64750, loss = 2.28, batch loss = 2.23 (36.7 examples/sec; 0.218 sec/batch; 16h:13m:09s remains)
INFO - root - 2017-12-08 08:57:37.181553: step 64760, loss = 2.28, batch loss = 2.23 (34.8 examples/sec; 0.230 sec/batch; 17h:06m:46s remains)
INFO - root - 2017-12-08 08:57:39.438928: step 64770, loss = 2.28, batch loss = 2.23 (36.6 examples/sec; 0.219 sec/batch; 16h:15m:04s remains)
INFO - root - 2017-12-08 08:57:41.659770: step 64780, loss = 2.28, batch loss = 2.23 (36.6 examples/sec; 0.218 sec/batch; 16h:14m:09s remains)
INFO - root - 2017-12-08 08:57:43.896803: step 64790, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 16h:20m:08s remains)
INFO - root - 2017-12-08 08:57:46.144221: step 64800, loss = 2.28, batch loss = 2.23 (37.2 examples/sec; 0.215 sec/batch; 16h:00m:24s remains)
2017-12-08 08:57:46.444581: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289317 -4.4289141 -4.428915 -4.4289303 -4.4289422 -4.4289403 -4.4289365 -4.4289393 -4.4289284 -4.4288807 -4.4288092 -4.428731 -4.4286671 -4.428678 -4.4287419][-4.4289241 -4.4288926 -4.4288845 -4.4288988 -4.4289103 -4.4289103 -4.4289165 -4.4289284 -4.428936 -4.4289 -4.4288268 -4.4287457 -4.4286766 -4.4286766 -4.4287357][-4.4289012 -4.428865 -4.4288454 -4.4288478 -4.4288464 -4.4288435 -4.4288588 -4.4288874 -4.4289179 -4.428905 -4.4288359 -4.4287515 -4.42868 -4.4286804 -4.4287386][-4.4288831 -4.4288507 -4.428823 -4.4288111 -4.4287949 -4.4287829 -4.4287939 -4.4288273 -4.428875 -4.4288831 -4.4288197 -4.4287343 -4.4286623 -4.4286695 -4.4287343][-4.428853 -4.4288306 -4.4287977 -4.428771 -4.4287386 -4.4287176 -4.4287138 -4.4287405 -4.4287972 -4.4288282 -4.42878 -4.4286995 -4.4286346 -4.4286513 -4.4287214][-4.4287953 -4.4287925 -4.4287658 -4.428731 -4.4286833 -4.428647 -4.4286141 -4.4286208 -4.4286823 -4.4287362 -4.4287148 -4.4286523 -4.428606 -4.428638 -4.4287162][-4.4287348 -4.4287505 -4.4287353 -4.4287 -4.4286437 -4.4285927 -4.4285331 -4.4285135 -4.4285693 -4.4286342 -4.4286284 -4.4285803 -4.4285512 -4.4286027 -4.4286962][-4.4287052 -4.4287367 -4.4287333 -4.4287047 -4.4286532 -4.4285932 -4.4285259 -4.4284863 -4.4285221 -4.4285769 -4.4285779 -4.4285331 -4.4285026 -4.4285655 -4.4286728][-4.4287257 -4.4287682 -4.428781 -4.4287643 -4.4287238 -4.428669 -4.4286118 -4.4285688 -4.4285822 -4.4286141 -4.4286103 -4.4285617 -4.4285192 -4.428575 -4.4286795][-4.4287844 -4.4288321 -4.4288568 -4.4288487 -4.4288192 -4.4287791 -4.4287367 -4.4287014 -4.4286985 -4.4287124 -4.4287038 -4.4286571 -4.4286127 -4.4286518 -4.4287353][-4.4288535 -4.4288979 -4.4289255 -4.4289193 -4.4288983 -4.4288707 -4.4288507 -4.4288306 -4.4288211 -4.4288244 -4.42881 -4.4287643 -4.4287262 -4.4287524 -4.4288125][-4.4289141 -4.4289479 -4.4289718 -4.428967 -4.4289484 -4.42893 -4.4289265 -4.4289155 -4.428905 -4.4289069 -4.4288926 -4.4288578 -4.4288311 -4.4288492 -4.4288859][-4.428947 -4.4289689 -4.428987 -4.4289856 -4.4289718 -4.4289603 -4.4289627 -4.428957 -4.4289522 -4.4289527 -4.4289408 -4.4289169 -4.4288988 -4.4289107 -4.4289317][-4.4289494 -4.4289622 -4.4289761 -4.4289784 -4.4289732 -4.4289703 -4.4289732 -4.4289722 -4.4289732 -4.4289756 -4.4289708 -4.4289589 -4.4289474 -4.4289489 -4.4289546][-4.4289293 -4.4289403 -4.42895 -4.4289513 -4.4289503 -4.4289541 -4.428957 -4.4289579 -4.4289603 -4.4289656 -4.428968 -4.4289665 -4.4289608 -4.4289551 -4.4289536]]...]
INFO - root - 2017-12-08 08:57:48.689003: step 64810, loss = 2.28, batch loss = 2.23 (34.9 examples/sec; 0.229 sec/batch; 17h:02m:17s remains)
INFO - root - 2017-12-08 08:57:50.917139: step 64820, loss = 2.28, batch loss = 2.23 (35.8 examples/sec; 0.223 sec/batch; 16h:36m:54s remains)
INFO - root - 2017-12-08 08:57:53.174748: step 64830, loss = 2.28, batch loss = 2.23 (35.1 examples/sec; 0.228 sec/batch; 16h:57m:35s remains)
INFO - root - 2017-12-08 08:57:55.411636: step 64840, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 16h:20m:49s remains)
INFO - root - 2017-12-08 08:57:57.659677: step 64850, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 16h:26m:55s remains)
INFO - root - 2017-12-08 08:57:59.891208: step 64860, loss = 2.28, batch loss = 2.23 (36.6 examples/sec; 0.219 sec/batch; 16h:15m:17s remains)
INFO - root - 2017-12-08 08:58:02.147102: step 64870, loss = 2.28, batch loss = 2.23 (35.1 examples/sec; 0.228 sec/batch; 16h:55m:56s remains)
INFO - root - 2017-12-08 08:58:04.360701: step 64880, loss = 2.28, batch loss = 2.23 (35.7 examples/sec; 0.224 sec/batch; 16h:40m:51s remains)
INFO - root - 2017-12-08 08:58:06.618078: step 64890, loss = 2.28, batch loss = 2.23 (34.4 examples/sec; 0.233 sec/batch; 17h:17m:06s remains)
INFO - root - 2017-12-08 08:58:08.842706: step 64900, loss = 2.28, batch loss = 2.23 (35.2 examples/sec; 0.227 sec/batch; 16h:54m:04s remains)
2017-12-08 08:58:09.136232: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288568 -4.4288263 -4.4288259 -4.428843 -4.4288592 -4.4288845 -4.42891 -4.4289422 -4.428977 -4.4289656 -4.4288874 -4.4288006 -4.4287605 -4.4287744 -4.4288149][-4.4288483 -4.428822 -4.4288216 -4.4288287 -4.4288359 -4.4288583 -4.4288888 -4.4289322 -4.4289732 -4.428967 -4.4288955 -4.428813 -4.4287672 -4.4287686 -4.4287944][-4.4288454 -4.4288249 -4.4288158 -4.4288106 -4.4288092 -4.4288273 -4.42886 -4.4289012 -4.4289408 -4.4289403 -4.4288864 -4.4288187 -4.428781 -4.428782 -4.4287977][-4.4288278 -4.4288087 -4.428791 -4.4287686 -4.4287562 -4.4287729 -4.4288068 -4.4288383 -4.42887 -4.4288793 -4.4288549 -4.4288182 -4.4288011 -4.4288135 -4.4288297][-4.428803 -4.4287696 -4.428731 -4.4286814 -4.428659 -4.4286819 -4.428721 -4.4287524 -4.4287868 -4.4288087 -4.4288158 -4.4288154 -4.4288225 -4.4288507 -4.4288735][-4.428762 -4.4286933 -4.428618 -4.4285278 -4.4284863 -4.4285245 -4.4285903 -4.4286423 -4.4286957 -4.4287386 -4.42878 -4.4288034 -4.4288239 -4.4288664 -4.4289041][-4.4287024 -4.4286017 -4.4284868 -4.4283414 -4.4282565 -4.4283085 -4.4284191 -4.4285131 -4.42859 -4.4286652 -4.4287419 -4.4287829 -4.4288077 -4.4288611 -4.4289136][-4.4286556 -4.4285469 -4.428412 -4.4282217 -4.4280849 -4.4281373 -4.4282827 -4.4284124 -4.4285011 -4.4285951 -4.4287004 -4.4287605 -4.4287992 -4.428863 -4.4289207][-4.4286647 -4.4285669 -4.4284468 -4.4282832 -4.4281487 -4.428174 -4.4283013 -4.4284286 -4.4285111 -4.4285975 -4.4287024 -4.4287753 -4.4288287 -4.4288912 -4.4289331][-4.428731 -4.4286528 -4.428566 -4.4284692 -4.4283867 -4.4283862 -4.428462 -4.4285531 -4.4286151 -4.4286804 -4.4287639 -4.4288239 -4.4288697 -4.4289112 -4.428926][-4.4288073 -4.4287505 -4.4286981 -4.4286609 -4.4286294 -4.4286256 -4.4286647 -4.4287195 -4.4287672 -4.4288073 -4.4288559 -4.4288859 -4.4289036 -4.4289093 -4.428884][-4.428874 -4.4288387 -4.4288182 -4.4288149 -4.4288082 -4.4288068 -4.4288278 -4.4288583 -4.4288878 -4.4289069 -4.4289246 -4.4289269 -4.4289126 -4.4288783 -4.4288111][-4.4289255 -4.4289055 -4.4288988 -4.4289064 -4.4289036 -4.4288974 -4.4289064 -4.4289246 -4.4289384 -4.4289365 -4.4289341 -4.4289231 -4.4288845 -4.4288225 -4.4287314][-4.4289556 -4.4289451 -4.4289408 -4.4289441 -4.4289379 -4.4289279 -4.4289269 -4.428936 -4.4289384 -4.4289184 -4.4289026 -4.4288836 -4.4288373 -4.4287724 -4.4286819][-4.4289579 -4.4289594 -4.4289589 -4.4289584 -4.4289508 -4.4289441 -4.4289389 -4.4289393 -4.4289284 -4.42889 -4.4288564 -4.428834 -4.4287953 -4.4287438 -4.4286704]]...]
INFO - root - 2017-12-08 08:58:11.357567: step 64910, loss = 2.28, batch loss = 2.23 (35.2 examples/sec; 0.227 sec/batch; 16h:52m:19s remains)
INFO - root - 2017-12-08 08:58:13.624674: step 64920, loss = 2.28, batch loss = 2.23 (34.2 examples/sec; 0.234 sec/batch; 17h:23m:31s remains)
INFO - root - 2017-12-08 08:58:15.885754: step 64930, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 16h:16m:43s remains)
INFO - root - 2017-12-08 08:58:18.146118: step 64940, loss = 2.28, batch loss = 2.23 (36.0 examples/sec; 0.222 sec/batch; 16h:29m:49s remains)
INFO - root - 2017-12-08 08:58:20.377280: step 64950, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.221 sec/batch; 16h:27m:16s remains)
INFO - root - 2017-12-08 08:58:22.632172: step 64960, loss = 2.28, batch loss = 2.23 (35.1 examples/sec; 0.228 sec/batch; 16h:57m:44s remains)
INFO - root - 2017-12-08 08:58:24.866005: step 64970, loss = 2.28, batch loss = 2.23 (35.7 examples/sec; 0.224 sec/batch; 16h:38m:16s remains)
INFO - root - 2017-12-08 08:58:27.121016: step 64980, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 16h:19m:11s remains)
INFO - root - 2017-12-08 08:58:29.356061: step 64990, loss = 2.28, batch loss = 2.23 (36.9 examples/sec; 0.217 sec/batch; 16h:07m:51s remains)
INFO - root - 2017-12-08 08:58:31.575710: step 65000, loss = 2.28, batch loss = 2.23 (36.0 examples/sec; 0.222 sec/batch; 16h:30m:38s remains)
2017-12-08 08:58:31.879855: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4286032 -4.4285836 -4.428565 -4.4285831 -4.4286208 -4.4286304 -4.4286332 -4.428637 -4.4286094 -4.4285965 -4.428597 -4.4285746 -4.4285445 -4.4285474 -4.4285603][-4.4286633 -4.4286323 -4.4286156 -4.4286423 -4.4286776 -4.4286828 -4.4286895 -4.4286942 -4.428668 -4.4286618 -4.4286623 -4.428638 -4.42861 -4.4286194 -4.42864][-4.428771 -4.4287496 -4.4287448 -4.42877 -4.4287982 -4.4287982 -4.4288025 -4.4288 -4.4287734 -4.4287634 -4.4287515 -4.4287152 -4.4286838 -4.4286938 -4.4287186][-4.4288478 -4.4288464 -4.4288521 -4.4288635 -4.4288688 -4.4288597 -4.42886 -4.4288616 -4.4288454 -4.4288321 -4.42881 -4.4287653 -4.4287219 -4.42872 -4.4287434][-4.4288707 -4.4288764 -4.4288778 -4.428865 -4.42884 -4.4288197 -4.4288235 -4.4288363 -4.4288354 -4.4288282 -4.4288225 -4.4288 -4.4287581 -4.4287438 -4.4287529][-4.4288425 -4.4288392 -4.42883 -4.4287958 -4.4287438 -4.4287004 -4.4286981 -4.4287238 -4.4287453 -4.428761 -4.4287868 -4.4288025 -4.4287829 -4.42877 -4.428762][-4.4287829 -4.4287624 -4.4287295 -4.4286637 -4.4285779 -4.4285011 -4.4284739 -4.4285107 -4.4285669 -4.4286294 -4.4287047 -4.4287677 -4.4287863 -4.4287882 -4.4287791][-4.4286852 -4.4286518 -4.4286013 -4.4285197 -4.4284234 -4.4283166 -4.4282575 -4.4282813 -4.4283533 -4.4284668 -4.4285989 -4.4287081 -4.4287581 -4.4287686 -4.4287524][-4.4286103 -4.4285831 -4.428544 -4.4284878 -4.4284186 -4.4283304 -4.42827 -4.4282503 -4.4282856 -4.4283977 -4.4285493 -4.4286685 -4.4287214 -4.4287243 -4.4286966][-4.4285946 -4.4285884 -4.4285822 -4.428566 -4.4285383 -4.4284954 -4.4284625 -4.428422 -4.4284015 -4.4284558 -4.4285717 -4.4286666 -4.4287047 -4.4286928 -4.4286528][-4.4286494 -4.4286575 -4.4286742 -4.4286795 -4.4286718 -4.4286561 -4.4286537 -4.4286246 -4.4285769 -4.4285746 -4.4286385 -4.4286962 -4.4287095 -4.4286852 -4.4286442][-4.4287443 -4.4287496 -4.428762 -4.4287672 -4.4287691 -4.4287686 -4.42878 -4.4287653 -4.4287219 -4.428699 -4.4287238 -4.4287524 -4.42875 -4.4287252 -4.4286904][-4.4288239 -4.4288249 -4.4288282 -4.4288325 -4.428843 -4.4288478 -4.4288526 -4.4288373 -4.4288092 -4.4287906 -4.4287939 -4.4288049 -4.4288049 -4.4287982 -4.4287786][-4.4288654 -4.4288659 -4.428865 -4.4288673 -4.4288807 -4.4288883 -4.4288907 -4.4288826 -4.4288688 -4.4288554 -4.4288464 -4.4288487 -4.4288521 -4.4288554 -4.4288487][-4.4288721 -4.4288707 -4.428863 -4.4288635 -4.4288783 -4.42889 -4.4288993 -4.4289036 -4.4289036 -4.4288983 -4.4288859 -4.4288807 -4.4288821 -4.428885 -4.4288836]]...]
INFO - root - 2017-12-08 08:58:34.135350: step 65010, loss = 2.28, batch loss = 2.23 (35.1 examples/sec; 0.228 sec/batch; 16h:55m:06s remains)
INFO - root - 2017-12-08 08:58:36.367879: step 65020, loss = 2.28, batch loss = 2.23 (35.8 examples/sec; 0.224 sec/batch; 16h:37m:16s remains)
INFO - root - 2017-12-08 08:58:38.609786: step 65030, loss = 2.28, batch loss = 2.23 (33.7 examples/sec; 0.237 sec/batch; 17h:36m:55s remains)
INFO - root - 2017-12-08 08:58:40.878014: step 65040, loss = 2.28, batch loss = 2.23 (34.6 examples/sec; 0.231 sec/batch; 17h:10m:13s remains)
INFO - root - 2017-12-08 08:58:43.110414: step 65050, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 16h:18m:14s remains)
INFO - root - 2017-12-08 08:58:45.343414: step 65060, loss = 2.28, batch loss = 2.23 (36.0 examples/sec; 0.222 sec/batch; 16h:30m:12s remains)
INFO - root - 2017-12-08 08:58:47.591389: step 65070, loss = 2.28, batch loss = 2.23 (35.6 examples/sec; 0.225 sec/batch; 16h:42m:20s remains)
INFO - root - 2017-12-08 08:58:49.832616: step 65080, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 16h:17m:53s remains)
INFO - root - 2017-12-08 08:58:52.110299: step 65090, loss = 2.28, batch loss = 2.23 (31.9 examples/sec; 0.251 sec/batch; 18h:36m:52s remains)
INFO - root - 2017-12-08 08:58:54.359634: step 65100, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 16h:15m:53s remains)
2017-12-08 08:58:54.655874: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.428956 -4.4289503 -4.4289484 -4.4289589 -4.4289627 -4.4289532 -4.4289293 -4.4288974 -4.4288721 -4.4288635 -4.42886 -4.4288707 -4.4288659 -4.4288516 -4.4288497][-4.4289513 -4.4289441 -4.42894 -4.428947 -4.4289393 -4.4289179 -4.4288836 -4.4288321 -4.4287972 -4.428793 -4.428793 -4.4288087 -4.4288096 -4.4287996 -4.4287915][-4.4289517 -4.4289417 -4.4289284 -4.428925 -4.4289021 -4.4288678 -4.42882 -4.4287395 -4.4286871 -4.4286852 -4.4286842 -4.4287043 -4.4287281 -4.428731 -4.4287133][-4.4289522 -4.4289393 -4.4289165 -4.4289002 -4.428863 -4.4288139 -4.4287424 -4.4286284 -4.4285583 -4.4285574 -4.4285545 -4.4285841 -4.4286366 -4.42866 -4.4286447][-4.4289455 -4.4289322 -4.4289083 -4.4288807 -4.4288282 -4.4287615 -4.4286709 -4.4285274 -4.42843 -4.4284267 -4.4284191 -4.4284544 -4.4285426 -4.4286 -4.4285893][-4.4289322 -4.4289231 -4.4289074 -4.4288774 -4.4288144 -4.4287362 -4.4286265 -4.4284544 -4.4283218 -4.4283171 -4.4283071 -4.4283495 -4.428484 -4.428576 -4.4285688][-4.4289165 -4.4289165 -4.4289093 -4.4288759 -4.4288058 -4.4287066 -4.4285712 -4.4283748 -4.4282146 -4.4282093 -4.4282002 -4.4282722 -4.4284458 -4.428555 -4.4285502][-4.4289002 -4.4289031 -4.4288926 -4.4288654 -4.4287992 -4.4286795 -4.4285116 -4.4282913 -4.4281187 -4.4281278 -4.4281406 -4.4282475 -4.4284444 -4.4285603 -4.428566][-4.428885 -4.4288826 -4.4288611 -4.428843 -4.428793 -4.4286757 -4.4284978 -4.4282765 -4.4281178 -4.4281573 -4.428216 -4.4283466 -4.428524 -4.4286323 -4.42866][-4.4288855 -4.4288797 -4.42886 -4.42885 -4.4288125 -4.4287133 -4.4285636 -4.4283919 -4.4282775 -4.4283271 -4.42841 -4.42853 -4.428647 -4.4287214 -4.4287615][-4.4288969 -4.428895 -4.42889 -4.4288831 -4.4288487 -4.4287758 -4.4286771 -4.4285679 -4.4284887 -4.4285288 -4.4286036 -4.42868 -4.4287443 -4.4287906 -4.4288192][-4.4289136 -4.4289107 -4.4289112 -4.4289055 -4.4288745 -4.4288263 -4.4287696 -4.4287052 -4.4286537 -4.4286814 -4.428741 -4.4287891 -4.4288292 -4.4288578 -4.4288683][-4.4289322 -4.4289289 -4.4289322 -4.4289317 -4.4289117 -4.4288831 -4.4288478 -4.4288096 -4.4287753 -4.4287925 -4.4288292 -4.4288592 -4.4288845 -4.428905 -4.4289064][-4.4289465 -4.4289446 -4.4289522 -4.428957 -4.428946 -4.4289284 -4.4289103 -4.4288898 -4.4288692 -4.4288759 -4.4288888 -4.428905 -4.4289236 -4.428936 -4.4289389][-4.4289579 -4.4289541 -4.4289613 -4.4289665 -4.4289613 -4.4289522 -4.4289465 -4.428937 -4.428925 -4.4289241 -4.4289184 -4.4289217 -4.4289346 -4.4289455 -4.4289532]]...]
INFO - root - 2017-12-08 08:58:56.894717: step 65110, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.221 sec/batch; 16h:22m:43s remains)
INFO - root - 2017-12-08 08:58:59.150275: step 65120, loss = 2.28, batch loss = 2.23 (35.3 examples/sec; 0.227 sec/batch; 16h:50m:39s remains)
INFO - root - 2017-12-08 08:59:01.408396: step 65130, loss = 2.28, batch loss = 2.23 (36.0 examples/sec; 0.222 sec/batch; 16h:31m:10s remains)
INFO - root - 2017-12-08 08:59:03.644698: step 65140, loss = 2.28, batch loss = 2.23 (34.1 examples/sec; 0.235 sec/batch; 17h:26m:44s remains)
INFO - root - 2017-12-08 08:59:05.882921: step 65150, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 16h:24m:21s remains)
INFO - root - 2017-12-08 08:59:08.148466: step 65160, loss = 2.28, batch loss = 2.23 (35.3 examples/sec; 0.226 sec/batch; 16h:48m:30s remains)
INFO - root - 2017-12-08 08:59:10.365851: step 65170, loss = 2.28, batch loss = 2.23 (36.6 examples/sec; 0.218 sec/batch; 16h:13m:09s remains)
INFO - root - 2017-12-08 08:59:12.602753: step 65180, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 16h:33m:26s remains)
INFO - root - 2017-12-08 08:59:14.862743: step 65190, loss = 2.28, batch loss = 2.23 (33.9 examples/sec; 0.236 sec/batch; 17h:31m:05s remains)
INFO - root - 2017-12-08 08:59:17.103062: step 65200, loss = 2.28, batch loss = 2.23 (35.0 examples/sec; 0.229 sec/batch; 16h:59m:18s remains)
2017-12-08 08:59:17.423631: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4287677 -4.42877 -4.4287596 -4.4287648 -4.4287691 -4.4287643 -4.4287581 -4.4287491 -4.4287424 -4.4287462 -4.4287562 -4.4287658 -4.42877 -4.4287872 -4.4288116][-4.4287748 -4.4287767 -4.4287663 -4.428771 -4.4287715 -4.4287653 -4.4287572 -4.4287434 -4.4287319 -4.428731 -4.4287376 -4.4287443 -4.4287462 -4.4287629 -4.4287891][-4.4287786 -4.428771 -4.4287548 -4.4287539 -4.4287586 -4.4287586 -4.4287596 -4.4287534 -4.4287419 -4.4287353 -4.4287333 -4.4287276 -4.428719 -4.4287214 -4.42874][-4.4287739 -4.4287558 -4.4287305 -4.42872 -4.4287224 -4.4287267 -4.4287386 -4.42875 -4.4287529 -4.4287491 -4.4287395 -4.4287205 -4.4286962 -4.4286785 -4.4286804][-4.4287519 -4.4287205 -4.4286828 -4.4286585 -4.428647 -4.4286423 -4.4286556 -4.4286885 -4.4287181 -4.4287333 -4.4287257 -4.4287 -4.428668 -4.4286394 -4.4286294][-4.42871 -4.4286637 -4.4286151 -4.4285693 -4.4285231 -4.4284854 -4.4284906 -4.4285536 -4.4286251 -4.4286771 -4.4286928 -4.4286785 -4.4286532 -4.4286251 -4.4286089][-4.428658 -4.4285946 -4.4285336 -4.4284673 -4.4283872 -4.4282985 -4.4282713 -4.4283581 -4.4284763 -4.4285731 -4.4286208 -4.4286327 -4.4286318 -4.42862 -4.4286036][-4.4286356 -4.4285612 -4.4284925 -4.4284186 -4.4283314 -4.4282174 -4.4281588 -4.4282374 -4.4283638 -4.4284768 -4.4285483 -4.4285874 -4.42861 -4.428616 -4.42861][-4.4286423 -4.4285693 -4.4285116 -4.428462 -4.4284096 -4.4283366 -4.4282975 -4.4283452 -4.4284277 -4.4285145 -4.4285803 -4.4286218 -4.428647 -4.4286551 -4.4286523][-4.4286857 -4.4286227 -4.428586 -4.4285622 -4.4285398 -4.4285078 -4.4284959 -4.4285259 -4.4285727 -4.4286313 -4.4286785 -4.4287095 -4.4287238 -4.4287257 -4.4287195][-4.4287457 -4.4286981 -4.4286809 -4.428679 -4.4286757 -4.4286671 -4.42867 -4.4286871 -4.4287105 -4.4287448 -4.4287739 -4.4287925 -4.4287996 -4.4287992 -4.428793][-4.4287724 -4.4287443 -4.4287419 -4.4287586 -4.4287744 -4.4287848 -4.4287958 -4.4288063 -4.4288154 -4.42883 -4.4288421 -4.4288464 -4.4288478 -4.4288473 -4.4288435][-4.4287786 -4.4287672 -4.4287772 -4.4288077 -4.4288368 -4.4288583 -4.4288692 -4.4288726 -4.4288707 -4.4288716 -4.4288716 -4.4288664 -4.4288616 -4.4288592 -4.4288559][-4.4287968 -4.4287958 -4.428812 -4.4288454 -4.4288769 -4.4289036 -4.4289141 -4.4289122 -4.4289041 -4.4288969 -4.4288888 -4.4288793 -4.4288721 -4.42887 -4.4288683][-4.42884 -4.4288454 -4.4288616 -4.4288917 -4.4289155 -4.4289351 -4.4289427 -4.4289393 -4.4289327 -4.4289274 -4.4289217 -4.4289145 -4.4289103 -4.4289093 -4.4289064]]...]
INFO - root - 2017-12-08 08:59:19.671139: step 65210, loss = 2.28, batch loss = 2.23 (34.3 examples/sec; 0.233 sec/batch; 17h:19m:03s remains)
INFO - root - 2017-12-08 08:59:21.888776: step 65220, loss = 2.28, batch loss = 2.23 (36.0 examples/sec; 0.222 sec/batch; 16h:28m:41s remains)
INFO - root - 2017-12-08 08:59:24.152055: step 65230, loss = 2.28, batch loss = 2.23 (36.0 examples/sec; 0.222 sec/batch; 16h:28m:53s remains)
INFO - root - 2017-12-08 08:59:26.368121: step 65240, loss = 2.28, batch loss = 2.23 (36.0 examples/sec; 0.222 sec/batch; 16h:31m:03s remains)
INFO - root - 2017-12-08 08:59:28.624151: step 65250, loss = 2.28, batch loss = 2.23 (37.3 examples/sec; 0.215 sec/batch; 15h:56m:17s remains)
INFO - root - 2017-12-08 08:59:30.866938: step 65260, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.225 sec/batch; 16h:43m:16s remains)
INFO - root - 2017-12-08 08:59:33.096888: step 65270, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.225 sec/batch; 16h:42m:53s remains)
INFO - root - 2017-12-08 08:59:35.319409: step 65280, loss = 2.28, batch loss = 2.23 (35.2 examples/sec; 0.227 sec/batch; 16h:52m:06s remains)
INFO - root - 2017-12-08 08:59:37.554588: step 65290, loss = 2.28, batch loss = 2.23 (34.2 examples/sec; 0.234 sec/batch; 17h:21m:36s remains)
INFO - root - 2017-12-08 08:59:39.799505: step 65300, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 16h:18m:51s remains)
2017-12-08 08:59:40.114306: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289556 -4.4289732 -4.428977 -4.4289875 -4.4289989 -4.4290094 -4.4290061 -4.4289856 -4.4289527 -4.4289012 -4.4288387 -4.4288054 -4.4287944 -4.4288011 -4.4288068][-4.4289784 -4.4289684 -4.4289455 -4.4289336 -4.4289336 -4.42895 -4.4289627 -4.4289646 -4.4289522 -4.4289141 -4.4288592 -4.4288263 -4.4288144 -4.4288263 -4.4288478][-4.4289761 -4.4289436 -4.4288945 -4.428853 -4.42883 -4.4288454 -4.42888 -4.4289093 -4.4289212 -4.4289088 -4.428874 -4.428843 -4.4288282 -4.4288416 -4.4288721][-4.428957 -4.4289136 -4.4288449 -4.428772 -4.4287066 -4.4286966 -4.4287505 -4.4288192 -4.4288645 -4.428884 -4.4288783 -4.428863 -4.4288545 -4.42887 -4.4288988][-4.4289284 -4.4288793 -4.4288 -4.4286971 -4.4285727 -4.4284978 -4.4285502 -4.4286771 -4.4287815 -4.428843 -4.4288716 -4.42888 -4.4288855 -4.4289041 -4.428926][-4.4289174 -4.4288664 -4.4287853 -4.4286613 -4.4284749 -4.4282956 -4.4282966 -4.428484 -4.4286728 -4.4287858 -4.428843 -4.4288735 -4.4288955 -4.4289203 -4.4289365][-4.428915 -4.4288688 -4.4287987 -4.4286709 -4.4284549 -4.42818 -4.4280682 -4.4282789 -4.4285417 -4.42871 -4.4287925 -4.4288359 -4.4288707 -4.4289036 -4.4289255][-4.4289351 -4.428895 -4.4288373 -4.4287348 -4.428555 -4.428278 -4.428062 -4.4281907 -4.4284587 -4.4286513 -4.42875 -4.4288006 -4.428844 -4.4288878 -4.42892][-4.4289575 -4.4289336 -4.4288964 -4.4288282 -4.4287171 -4.4285173 -4.4283051 -4.4283061 -4.4284768 -4.4286418 -4.4287434 -4.4287977 -4.4288421 -4.4288898 -4.428926][-4.4289351 -4.4289279 -4.4289126 -4.4288778 -4.428823 -4.42871 -4.4285536 -4.4284897 -4.4285483 -4.42865 -4.4287338 -4.4287872 -4.4288311 -4.4288712 -4.4289041][-4.4288921 -4.4288912 -4.4288888 -4.4288783 -4.4288607 -4.4288082 -4.4287128 -4.4286423 -4.42864 -4.4286861 -4.4287415 -4.4287853 -4.4288216 -4.42885 -4.4288731][-4.4288564 -4.4288568 -4.4288592 -4.4288611 -4.4288611 -4.4288411 -4.428793 -4.4287457 -4.4287314 -4.4287481 -4.428772 -4.4287992 -4.428822 -4.4288411 -4.428853][-4.4288573 -4.428853 -4.4288507 -4.4288507 -4.4288511 -4.428843 -4.4288263 -4.4288111 -4.4288106 -4.4288187 -4.4288244 -4.4288335 -4.4288459 -4.4288578 -4.428863][-4.428864 -4.4288583 -4.4288549 -4.4288554 -4.4288549 -4.4288492 -4.4288435 -4.4288464 -4.4288545 -4.4288597 -4.42886 -4.4288597 -4.428865 -4.42887 -4.4288716][-4.4288654 -4.4288621 -4.42886 -4.4288645 -4.42887 -4.4288707 -4.4288678 -4.4288707 -4.4288774 -4.4288807 -4.4288797 -4.4288764 -4.4288774 -4.42888 -4.4288793]]...]
INFO - root - 2017-12-08 08:59:42.321328: step 65310, loss = 2.28, batch loss = 2.23 (35.6 examples/sec; 0.225 sec/batch; 16h:40m:33s remains)
INFO - root - 2017-12-08 08:59:44.538807: step 65320, loss = 2.28, batch loss = 2.23 (37.7 examples/sec; 0.212 sec/batch; 15h:45m:45s remains)
INFO - root - 2017-12-08 08:59:46.785767: step 65330, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.221 sec/batch; 16h:21m:55s remains)
INFO - root - 2017-12-08 08:59:49.010978: step 65340, loss = 2.28, batch loss = 2.23 (35.7 examples/sec; 0.224 sec/batch; 16h:38m:01s remains)
INFO - root - 2017-12-08 08:59:51.223336: step 65350, loss = 2.28, batch loss = 2.23 (37.0 examples/sec; 0.216 sec/batch; 16h:02m:49s remains)
INFO - root - 2017-12-08 08:59:53.492415: step 65360, loss = 2.28, batch loss = 2.23 (33.7 examples/sec; 0.237 sec/batch; 17h:36m:36s remains)
INFO - root - 2017-12-08 08:59:55.732107: step 65370, loss = 2.28, batch loss = 2.23 (35.2 examples/sec; 0.227 sec/batch; 16h:50m:34s remains)
INFO - root - 2017-12-08 08:59:57.955167: step 65380, loss = 2.28, batch loss = 2.23 (35.3 examples/sec; 0.226 sec/batch; 16h:47m:54s remains)
INFO - root - 2017-12-08 09:00:00.185972: step 65390, loss = 2.28, batch loss = 2.23 (37.1 examples/sec; 0.215 sec/batch; 15h:58m:57s remains)
INFO - root - 2017-12-08 09:00:02.413263: step 65400, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 16h:15m:59s remains)
2017-12-08 09:00:02.707878: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.42876 -4.4287686 -4.4287877 -4.4288015 -4.428803 -4.4287858 -4.4287653 -4.4287529 -4.4287529 -4.4287562 -4.4287715 -4.4287715 -4.4287548 -4.4287434 -4.4287548][-4.4287734 -4.4287887 -4.4288087 -4.4288268 -4.4288287 -4.4288 -4.4287558 -4.4287176 -4.4287119 -4.428731 -4.4287581 -4.4287581 -4.4287362 -4.42872 -4.42873][-4.4288244 -4.4288421 -4.4288559 -4.42886 -4.42884 -4.428792 -4.4287362 -4.4286942 -4.4286952 -4.4287295 -4.4287667 -4.4287667 -4.4287395 -4.4287195 -4.4287171][-4.4288559 -4.4288707 -4.4288707 -4.4288487 -4.4287953 -4.4287267 -4.4286633 -4.4286313 -4.4286594 -4.42872 -4.42877 -4.4287786 -4.4287562 -4.4287381 -4.4287195][-4.4288378 -4.4288435 -4.4288254 -4.4287729 -4.4286885 -4.4285984 -4.4285274 -4.4285111 -4.4285736 -4.428669 -4.4287415 -4.428771 -4.4287739 -4.4287677 -4.4287448][-4.4287915 -4.428782 -4.4287496 -4.4286752 -4.4285693 -4.4284649 -4.4283881 -4.4283824 -4.4284744 -4.4286032 -4.4286952 -4.4287496 -4.42878 -4.4287939 -4.4287829][-4.428762 -4.428741 -4.4287047 -4.4286304 -4.4285235 -4.428421 -4.428349 -4.4283419 -4.4284329 -4.4285607 -4.4286509 -4.4287105 -4.4287615 -4.4288073 -4.4288187][-4.42876 -4.4287353 -4.4287081 -4.4286523 -4.4285712 -4.4284854 -4.4284253 -4.4284153 -4.4284735 -4.4285684 -4.4286375 -4.4286852 -4.42873 -4.4287815 -4.4288068][-4.4287748 -4.4287572 -4.428751 -4.4287219 -4.4286804 -4.4286213 -4.4285703 -4.4285483 -4.4285626 -4.4286156 -4.4286623 -4.4286966 -4.4287305 -4.4287715 -4.4287925][-4.4287972 -4.4287939 -4.4288073 -4.4288011 -4.4287806 -4.4287395 -4.4287 -4.4286737 -4.4286561 -4.42868 -4.4287128 -4.4287386 -4.4287648 -4.428791 -4.4287992][-4.428803 -4.4288034 -4.428823 -4.4288254 -4.4288139 -4.4287863 -4.4287682 -4.4287605 -4.4287472 -4.4287639 -4.428793 -4.4288168 -4.4288354 -4.428843 -4.4288316][-4.4287767 -4.4287696 -4.42879 -4.4287996 -4.42879 -4.4287705 -4.4287705 -4.4287925 -4.4288049 -4.4288268 -4.4288549 -4.4288764 -4.4288869 -4.4288826 -4.4288659][-4.4287305 -4.4287238 -4.42875 -4.4287686 -4.4287539 -4.4287324 -4.4287329 -4.42877 -4.4288 -4.4288225 -4.4288497 -4.4288683 -4.4288692 -4.4288568 -4.4288392][-4.42871 -4.4287176 -4.4287553 -4.42878 -4.4287615 -4.4287333 -4.4287186 -4.4287448 -4.4287734 -4.4287844 -4.4287982 -4.4288082 -4.42881 -4.4288039 -4.4287958][-4.4287333 -4.4287515 -4.428792 -4.4288192 -4.428802 -4.4287691 -4.4287372 -4.4287343 -4.4287467 -4.4287481 -4.4287543 -4.4287624 -4.4287705 -4.4287767 -4.4287806]]...]
INFO - root - 2017-12-08 09:00:04.937616: step 65410, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 16h:17m:12s remains)
INFO - root - 2017-12-08 09:00:07.147429: step 65420, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 16h:22m:28s remains)
INFO - root - 2017-12-08 09:00:09.394799: step 65430, loss = 2.28, batch loss = 2.23 (35.2 examples/sec; 0.227 sec/batch; 16h:50m:41s remains)
INFO - root - 2017-12-08 09:00:11.631845: step 65440, loss = 2.28, batch loss = 2.23 (35.3 examples/sec; 0.226 sec/batch; 16h:47m:24s remains)
INFO - root - 2017-12-08 09:00:13.854741: step 65450, loss = 2.28, batch loss = 2.23 (37.2 examples/sec; 0.215 sec/batch; 15h:56m:12s remains)
INFO - root - 2017-12-08 09:00:16.113774: step 65460, loss = 2.28, batch loss = 2.23 (35.3 examples/sec; 0.226 sec/batch; 16h:47m:34s remains)
INFO - root - 2017-12-08 09:00:18.344727: step 65470, loss = 2.28, batch loss = 2.23 (36.9 examples/sec; 0.217 sec/batch; 16h:05m:41s remains)
INFO - root - 2017-12-08 09:00:20.613763: step 65480, loss = 2.28, batch loss = 2.23 (33.0 examples/sec; 0.243 sec/batch; 18h:00m:11s remains)
INFO - root - 2017-12-08 09:00:22.873501: step 65490, loss = 2.28, batch loss = 2.23 (35.0 examples/sec; 0.228 sec/batch; 16h:56m:11s remains)
INFO - root - 2017-12-08 09:00:25.163996: step 65500, loss = 2.28, batch loss = 2.23 (34.7 examples/sec; 0.230 sec/batch; 17h:05m:35s remains)
2017-12-08 09:00:25.494634: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4287977 -4.4288087 -4.4288092 -4.4287925 -4.4287734 -4.428751 -4.4287367 -4.4287381 -4.4287519 -4.42877 -4.428772 -4.4287772 -4.4287848 -4.4287848 -4.4287887][-4.4288287 -4.4288487 -4.4288588 -4.4288459 -4.4288263 -4.428803 -4.4287782 -4.4287715 -4.428782 -4.4287863 -4.4287643 -4.4287462 -4.4287415 -4.4287438 -4.4287639][-4.4288507 -4.4288626 -4.42887 -4.4288511 -4.4288287 -4.428813 -4.4287987 -4.4287992 -4.4288158 -4.4288111 -4.4287691 -4.42872 -4.4286885 -4.4286842 -4.4287105][-4.4288526 -4.4288545 -4.4288487 -4.4288197 -4.4287958 -4.4287872 -4.4287925 -4.4288125 -4.4288397 -4.4288406 -4.4287953 -4.4287267 -4.4286723 -4.4286513 -4.4286642][-4.4288378 -4.4288325 -4.4288077 -4.4287691 -4.428741 -4.4287367 -4.42876 -4.4287949 -4.4288278 -4.4288392 -4.4288054 -4.4287419 -4.4286876 -4.4286623 -4.4286594][-4.4288092 -4.4288034 -4.42877 -4.4287219 -4.4286876 -4.4286823 -4.428709 -4.4287505 -4.4287891 -4.4288082 -4.428793 -4.4287539 -4.428719 -4.4286985 -4.428688][-4.4287763 -4.4287753 -4.4287519 -4.4287076 -4.4286785 -4.4286652 -4.4286718 -4.4287009 -4.4287353 -4.4287553 -4.428762 -4.4287543 -4.4287429 -4.4287276 -4.4287109][-4.4287367 -4.4287524 -4.4287486 -4.4287262 -4.4287181 -4.4286928 -4.4286547 -4.4286451 -4.4286518 -4.4286623 -4.4286942 -4.428721 -4.42873 -4.4287205 -4.4286981][-4.4287014 -4.4287276 -4.4287434 -4.4287496 -4.4287639 -4.4287357 -4.4286542 -4.4285913 -4.4285564 -4.4285522 -4.4286146 -4.4286747 -4.4286957 -4.4286842 -4.4286523][-4.4286804 -4.4286962 -4.4287176 -4.4287524 -4.4287829 -4.4287658 -4.4286804 -4.4285889 -4.4285226 -4.428503 -4.4285784 -4.428647 -4.4286671 -4.4286551 -4.428617][-4.428679 -4.4286714 -4.4286857 -4.4287338 -4.4287772 -4.4287748 -4.4287071 -4.4286265 -4.4285607 -4.4285326 -4.4285955 -4.4286423 -4.4286556 -4.4286518 -4.4286218][-4.4286704 -4.4286504 -4.4286566 -4.4287195 -4.4287858 -4.4287963 -4.4287462 -4.4286909 -4.42864 -4.4286094 -4.4286513 -4.4286718 -4.4286742 -4.4286823 -4.4286752][-4.4286928 -4.4286685 -4.42867 -4.4287429 -4.4288158 -4.4288263 -4.4287858 -4.4287434 -4.4286971 -4.428668 -4.4286957 -4.4286995 -4.4286957 -4.4287157 -4.4287348][-4.428721 -4.4286985 -4.4286933 -4.4287629 -4.4288282 -4.42884 -4.4288058 -4.4287653 -4.4287224 -4.4287043 -4.4287257 -4.428719 -4.4287071 -4.4287314 -4.4287639][-4.4287291 -4.4287076 -4.4286985 -4.4287567 -4.4288116 -4.4288282 -4.4287992 -4.4287596 -4.4287224 -4.4287181 -4.4287424 -4.4287286 -4.4287052 -4.4287248 -4.4287634]]...]
INFO - root - 2017-12-08 09:00:27.743297: step 65510, loss = 2.28, batch loss = 2.23 (34.0 examples/sec; 0.235 sec/batch; 17h:26m:21s remains)
INFO - root - 2017-12-08 09:00:29.964993: step 65520, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 16h:17m:42s remains)
INFO - root - 2017-12-08 09:00:32.217263: step 65530, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.222 sec/batch; 16h:27m:01s remains)
INFO - root - 2017-12-08 09:00:34.457216: step 65540, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.222 sec/batch; 16h:25m:47s remains)
INFO - root - 2017-12-08 09:00:36.693739: step 65550, loss = 2.28, batch loss = 2.23 (34.6 examples/sec; 0.231 sec/batch; 17h:07m:46s remains)
INFO - root - 2017-12-08 09:00:38.962761: step 65560, loss = 2.28, batch loss = 2.23 (34.6 examples/sec; 0.231 sec/batch; 17h:08m:05s remains)
INFO - root - 2017-12-08 09:00:41.179628: step 65570, loss = 2.28, batch loss = 2.23 (33.8 examples/sec; 0.237 sec/batch; 17h:33m:10s remains)
INFO - root - 2017-12-08 09:00:43.412450: step 65580, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 16h:31m:37s remains)
INFO - root - 2017-12-08 09:00:45.664365: step 65590, loss = 2.28, batch loss = 2.23 (36.7 examples/sec; 0.218 sec/batch; 16h:08m:37s remains)
INFO - root - 2017-12-08 09:00:47.911129: step 65600, loss = 2.28, batch loss = 2.23 (35.7 examples/sec; 0.224 sec/batch; 16h:37m:33s remains)
2017-12-08 09:00:48.232005: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4287372 -4.4286938 -4.4286652 -4.4286494 -4.4286413 -4.4286261 -4.42862 -4.4286213 -4.4286342 -4.4286885 -4.4287395 -4.4287295 -4.4286842 -4.4286413 -4.4286041][-4.4287152 -4.4286757 -4.4286561 -4.4286461 -4.4286423 -4.4286327 -4.4286294 -4.4286366 -4.4286561 -4.4287157 -4.4287653 -4.4287496 -4.4286933 -4.4286261 -4.4285679][-4.4286847 -4.4286513 -4.428638 -4.4286294 -4.4286284 -4.428618 -4.4286103 -4.4286156 -4.4286366 -4.4287004 -4.428751 -4.4287395 -4.4286885 -4.4286232 -4.4285631][-4.4286456 -4.4286175 -4.4286032 -4.4285927 -4.4285908 -4.42857 -4.4285469 -4.4285421 -4.4285688 -4.4286475 -4.4287128 -4.4287152 -4.4286747 -4.4286208 -4.4285669][-4.4286079 -4.4285841 -4.4285669 -4.4285522 -4.4285345 -4.4284906 -4.4284387 -4.42842 -4.4284596 -4.428565 -4.428658 -4.4286814 -4.4286528 -4.4286084 -4.4285593][-4.428575 -4.4285665 -4.4285545 -4.4285331 -4.4284892 -4.4284086 -4.4283133 -4.42828 -4.4283438 -4.4284821 -4.4286046 -4.4286489 -4.4286294 -4.4285874 -4.4285326][-4.4285464 -4.4285622 -4.4285583 -4.4285259 -4.4284549 -4.4283361 -4.428206 -4.4281712 -4.4282651 -4.4284296 -4.4285712 -4.4286318 -4.4286218 -4.4285789 -4.4285107][-4.4285312 -4.4285731 -4.428587 -4.428546 -4.4284573 -4.4283171 -4.4281855 -4.4281673 -4.4282732 -4.4284306 -4.428565 -4.4286256 -4.4286218 -4.4285765 -4.4284983][-4.4285445 -4.4286017 -4.4286284 -4.4285889 -4.4285078 -4.4283853 -4.4282885 -4.4282846 -4.428371 -4.4284868 -4.4285827 -4.4286227 -4.4286175 -4.4285727 -4.4284997][-4.4285712 -4.4286275 -4.4286613 -4.428637 -4.4285822 -4.4284964 -4.4284377 -4.428441 -4.4284935 -4.4285531 -4.4286032 -4.4286251 -4.42862 -4.4285812 -4.4285131][-4.4286003 -4.4286404 -4.42867 -4.4286604 -4.4286323 -4.4285808 -4.4285512 -4.4285583 -4.4285822 -4.4285946 -4.4286084 -4.4286218 -4.4286227 -4.4285893 -4.42853][-4.428618 -4.4286351 -4.4286547 -4.42866 -4.4286532 -4.428627 -4.4286156 -4.4286265 -4.4286337 -4.4286208 -4.4286089 -4.4286165 -4.4286184 -4.4285941 -4.4285488][-4.428618 -4.4286218 -4.4286389 -4.428659 -4.4286652 -4.4286518 -4.4286423 -4.428648 -4.4286547 -4.4286432 -4.4286261 -4.428628 -4.4286237 -4.4286022 -4.4285717][-4.4286208 -4.4286194 -4.4286332 -4.4286547 -4.4286642 -4.4286585 -4.4286475 -4.428648 -4.42866 -4.4286618 -4.428659 -4.4286585 -4.4286432 -4.4286175 -4.4285965][-4.4286318 -4.4286318 -4.42864 -4.42865 -4.4286532 -4.4286528 -4.4286456 -4.4286437 -4.4286561 -4.4286718 -4.4286833 -4.4286842 -4.4286623 -4.4286323 -4.4286184]]...]
INFO - root - 2017-12-08 09:00:50.474973: step 65610, loss = 2.28, batch loss = 2.23 (34.8 examples/sec; 0.230 sec/batch; 17h:01m:47s remains)
INFO - root - 2017-12-08 09:00:52.718272: step 65620, loss = 2.28, batch loss = 2.23 (34.1 examples/sec; 0.234 sec/batch; 17h:22m:49s remains)
INFO - root - 2017-12-08 09:00:54.966455: step 65630, loss = 2.28, batch loss = 2.23 (36.6 examples/sec; 0.219 sec/batch; 16h:12m:48s remains)
INFO - root - 2017-12-08 09:00:57.212478: step 65640, loss = 2.28, batch loss = 2.23 (34.1 examples/sec; 0.235 sec/batch; 17h:24m:13s remains)
INFO - root - 2017-12-08 09:00:59.442200: step 65650, loss = 2.28, batch loss = 2.23 (35.3 examples/sec; 0.227 sec/batch; 16h:48m:16s remains)
INFO - root - 2017-12-08 09:01:01.673975: step 65660, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 16h:21m:51s remains)
INFO - root - 2017-12-08 09:01:03.941101: step 65670, loss = 2.28, batch loss = 2.23 (33.4 examples/sec; 0.240 sec/batch; 17h:45m:46s remains)
INFO - root - 2017-12-08 09:01:06.176212: step 65680, loss = 2.28, batch loss = 2.23 (35.6 examples/sec; 0.225 sec/batch; 16h:39m:45s remains)
INFO - root - 2017-12-08 09:01:08.409700: step 65690, loss = 2.28, batch loss = 2.23 (33.6 examples/sec; 0.238 sec/batch; 17h:38m:36s remains)
INFO - root - 2017-12-08 09:01:10.657315: step 65700, loss = 2.28, batch loss = 2.23 (36.7 examples/sec; 0.218 sec/batch; 16h:08m:48s remains)
2017-12-08 09:01:10.939146: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4285316 -4.4285216 -4.428535 -4.4285641 -4.4285884 -4.4286041 -4.428618 -4.4286332 -4.42865 -4.4286542 -4.428637 -4.4286308 -4.4286389 -4.4286613 -4.4287062][-4.4285169 -4.4284873 -4.4284968 -4.4285464 -4.4285808 -4.428597 -4.4286113 -4.428618 -4.4286208 -4.42862 -4.4286008 -4.4285984 -4.428617 -4.4286532 -4.4287081][-4.4284959 -4.428452 -4.4284587 -4.4285216 -4.4285612 -4.4285774 -4.4285865 -4.4285746 -4.428565 -4.4285688 -4.4285603 -4.4285626 -4.428587 -4.428637 -4.428699][-4.4285321 -4.4284782 -4.4284739 -4.4285331 -4.4285612 -4.4285612 -4.4285622 -4.4285388 -4.4285293 -4.4285426 -4.4285421 -4.4285541 -4.4285932 -4.4286547 -4.42871][-4.4285979 -4.4285488 -4.4285283 -4.4285645 -4.4285784 -4.4285684 -4.4285636 -4.4285522 -4.4285512 -4.4285569 -4.4285541 -4.4285803 -4.42864 -4.4287066 -4.4287486][-4.4286218 -4.428587 -4.4285684 -4.4285917 -4.4285994 -4.4285903 -4.4285917 -4.4286046 -4.4286122 -4.4286041 -4.4285936 -4.4286222 -4.4286952 -4.428762 -4.4287968][-4.4286022 -4.4285855 -4.428587 -4.4286118 -4.4286261 -4.4286304 -4.42864 -4.4286656 -4.428679 -4.4286628 -4.4286427 -4.4286594 -4.428721 -4.428781 -4.428812][-4.42858 -4.428575 -4.4285903 -4.4286184 -4.4286437 -4.4286661 -4.4286852 -4.4287105 -4.42872 -4.4287024 -4.4286795 -4.42868 -4.4287105 -4.42875 -4.4287786][-4.4285893 -4.4285817 -4.4285946 -4.4286218 -4.4286604 -4.4286947 -4.4287143 -4.4287291 -4.4287224 -4.4286952 -4.4286704 -4.4286685 -4.428678 -4.4286966 -4.4287171][-4.4286056 -4.428597 -4.4285989 -4.4286213 -4.4286561 -4.4286847 -4.4287052 -4.4287095 -4.4286885 -4.4286556 -4.4286327 -4.4286332 -4.4286356 -4.4286394 -4.4286442][-4.4286094 -4.4286036 -4.4286056 -4.4286218 -4.42864 -4.428658 -4.42868 -4.4286852 -4.4286542 -4.4286222 -4.4286075 -4.4286122 -4.4286146 -4.4286079 -4.4286032][-4.4286022 -4.428586 -4.4285865 -4.4286103 -4.4286327 -4.4286513 -4.4286718 -4.4286757 -4.4286485 -4.4286284 -4.4286189 -4.4286256 -4.4286342 -4.4286318 -4.4286308][-4.4285913 -4.4285393 -4.4285212 -4.4285655 -4.42862 -4.4286556 -4.4286761 -4.42867 -4.4286494 -4.42864 -4.4286304 -4.4286423 -4.4286647 -4.4286785 -4.42869][-4.4285817 -4.4284968 -4.4284577 -4.42851 -4.4285893 -4.4286437 -4.428668 -4.4286623 -4.4286523 -4.4286404 -4.4286232 -4.4286442 -4.4286838 -4.4287138 -4.4287391][-4.4285841 -4.4284925 -4.4284477 -4.4284921 -4.4285622 -4.428618 -4.4286461 -4.4286423 -4.4286389 -4.4286256 -4.4286113 -4.4286413 -4.4286966 -4.4287415 -4.4287744]]...]
INFO - root - 2017-12-08 09:01:13.210311: step 65710, loss = 2.28, batch loss = 2.23 (35.2 examples/sec; 0.227 sec/batch; 16h:49m:30s remains)
INFO - root - 2017-12-08 09:01:15.461424: step 65720, loss = 2.28, batch loss = 2.23 (36.8 examples/sec; 0.217 sec/batch; 16h:05m:56s remains)
INFO - root - 2017-12-08 09:01:17.709052: step 65730, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.222 sec/batch; 16h:26m:27s remains)
INFO - root - 2017-12-08 09:01:19.941358: step 65740, loss = 2.28, batch loss = 2.23 (34.9 examples/sec; 0.229 sec/batch; 17h:00m:16s remains)
INFO - root - 2017-12-08 09:01:22.201486: step 65750, loss = 2.28, batch loss = 2.23 (34.9 examples/sec; 0.229 sec/batch; 16h:57m:51s remains)
INFO - root - 2017-12-08 09:01:24.428688: step 65760, loss = 2.28, batch loss = 2.23 (37.0 examples/sec; 0.216 sec/batch; 16h:02m:24s remains)
INFO - root - 2017-12-08 09:01:26.653782: step 65770, loss = 2.28, batch loss = 2.23 (36.7 examples/sec; 0.218 sec/batch; 16h:08m:09s remains)
INFO - root - 2017-12-08 09:01:28.893598: step 65780, loss = 2.28, batch loss = 2.23 (36.8 examples/sec; 0.217 sec/batch; 16h:06m:00s remains)
INFO - root - 2017-12-08 09:01:31.161102: step 65790, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.222 sec/batch; 16h:25m:25s remains)
INFO - root - 2017-12-08 09:01:33.402302: step 65800, loss = 2.28, batch loss = 2.23 (34.7 examples/sec; 0.230 sec/batch; 17h:03m:29s remains)
2017-12-08 09:01:33.717470: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288082 -4.4288087 -4.4288054 -4.428803 -4.4288015 -4.4288 -4.4287968 -4.4287915 -4.42878 -4.4287586 -4.4287324 -4.4287081 -4.4286928 -4.4286976 -4.4287243][-4.4288125 -4.428812 -4.4288073 -4.4288034 -4.4287996 -4.4287949 -4.4287868 -4.4287739 -4.4287491 -4.4287109 -4.4286695 -4.4286361 -4.4286208 -4.4286313 -4.4286714][-4.4288125 -4.4288077 -4.4287996 -4.4287925 -4.4287829 -4.4287734 -4.428762 -4.4287453 -4.4287119 -4.4286594 -4.4286051 -4.4285655 -4.4285502 -4.4285622 -4.428606][-4.4287906 -4.428781 -4.4287682 -4.4287539 -4.4287367 -4.4287233 -4.4287138 -4.4286985 -4.4286671 -4.4286094 -4.4285436 -4.428494 -4.428472 -4.4284811 -4.4285226][-4.428741 -4.4287219 -4.4286981 -4.428668 -4.428637 -4.4286232 -4.4286242 -4.4286256 -4.4286141 -4.4285707 -4.4285088 -4.428453 -4.4284163 -4.4284062 -4.4284391][-4.4287052 -4.4286752 -4.42864 -4.4285922 -4.4285431 -4.42852 -4.4285307 -4.4285583 -4.4285722 -4.428546 -4.4284906 -4.4284291 -4.4283776 -4.4283514 -4.42838][-4.4287133 -4.4286838 -4.428648 -4.4285951 -4.4285388 -4.428504 -4.4285116 -4.4285502 -4.4285793 -4.4285688 -4.4285197 -4.4284554 -4.4283962 -4.4283562 -4.4283795][-4.4287438 -4.4287229 -4.4286957 -4.4286523 -4.4286 -4.4285636 -4.4285674 -4.4286056 -4.4286394 -4.4286442 -4.4286094 -4.4285526 -4.4284992 -4.4284506 -4.4284606][-4.4287753 -4.4287624 -4.4287448 -4.4287167 -4.428678 -4.4286485 -4.4286513 -4.4286819 -4.4287148 -4.4287319 -4.4287171 -4.4286819 -4.4286423 -4.4285893 -4.4285793][-4.4287915 -4.428781 -4.4287672 -4.4287481 -4.4287214 -4.4287009 -4.4287014 -4.4287276 -4.428761 -4.4287829 -4.4287839 -4.4287682 -4.4287419 -4.4286981 -4.428679][-4.4288073 -4.428803 -4.4287992 -4.4287882 -4.4287691 -4.4287529 -4.4287505 -4.4287729 -4.428803 -4.4288187 -4.42882 -4.4288054 -4.4287806 -4.4287472 -4.4287324][-4.4288225 -4.4288287 -4.4288363 -4.4288344 -4.4288225 -4.4288116 -4.4288106 -4.4288225 -4.4288359 -4.4288344 -4.4288278 -4.42881 -4.4287844 -4.4287586 -4.4287491][-4.4288144 -4.4288297 -4.428844 -4.4288464 -4.4288383 -4.4288325 -4.4288335 -4.4288392 -4.4288387 -4.4288235 -4.4288096 -4.4287934 -4.4287739 -4.4287586 -4.4287567][-4.4287882 -4.4288068 -4.4288254 -4.4288316 -4.4288297 -4.4288287 -4.4288306 -4.4288321 -4.4288259 -4.4288049 -4.4287853 -4.4287705 -4.4287586 -4.4287543 -4.4287577][-4.4287663 -4.4287782 -4.4287949 -4.428803 -4.4288049 -4.4288049 -4.428803 -4.4287996 -4.428792 -4.4287705 -4.4287486 -4.4287362 -4.4287333 -4.4287372 -4.4287462]]...]
INFO - root - 2017-12-08 09:01:35.943086: step 65810, loss = 2.28, batch loss = 2.23 (34.9 examples/sec; 0.229 sec/batch; 16h:57m:56s remains)
INFO - root - 2017-12-08 09:01:38.237702: step 65820, loss = 2.28, batch loss = 2.23 (31.8 examples/sec; 0.251 sec/batch; 18h:37m:04s remains)
INFO - root - 2017-12-08 09:01:40.479076: step 65830, loss = 2.28, batch loss = 2.23 (35.6 examples/sec; 0.225 sec/batch; 16h:38m:56s remains)
INFO - root - 2017-12-08 09:01:42.747065: step 65840, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.225 sec/batch; 16h:41m:55s remains)
INFO - root - 2017-12-08 09:01:44.953379: step 65850, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 16h:23m:21s remains)
INFO - root - 2017-12-08 09:01:47.198110: step 65860, loss = 2.28, batch loss = 2.23 (35.8 examples/sec; 0.223 sec/batch; 16h:32m:50s remains)
INFO - root - 2017-12-08 09:01:49.423804: step 65870, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 16h:16m:40s remains)
INFO - root - 2017-12-08 09:01:51.656592: step 65880, loss = 2.28, batch loss = 2.23 (34.7 examples/sec; 0.231 sec/batch; 17h:05m:54s remains)
INFO - root - 2017-12-08 09:01:53.877759: step 65890, loss = 2.28, batch loss = 2.23 (36.7 examples/sec; 0.218 sec/batch; 16h:07m:33s remains)
INFO - root - 2017-12-08 09:01:56.105426: step 65900, loss = 2.28, batch loss = 2.23 (36.7 examples/sec; 0.218 sec/batch; 16h:09m:15s remains)
2017-12-08 09:01:56.400434: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.428894 -4.4288898 -4.4288783 -4.428875 -4.4288845 -4.4288983 -4.4289017 -4.4289031 -4.4289074 -4.4289069 -4.4288907 -4.4288568 -4.42882 -4.4288058 -4.4288197][-4.4289289 -4.428916 -4.428896 -4.4288883 -4.4288917 -4.4288926 -4.4288807 -4.428874 -4.428875 -4.4288716 -4.4288478 -4.428803 -4.4287581 -4.4287467 -4.4287734][-4.4289479 -4.4289246 -4.4289012 -4.4288926 -4.4288883 -4.4288731 -4.4288478 -4.4288359 -4.4288363 -4.4288316 -4.4288054 -4.4287539 -4.4286962 -4.4286718 -4.4286962][-4.4289432 -4.428915 -4.4288926 -4.4288855 -4.4288735 -4.4288392 -4.4287968 -4.4287767 -4.4287777 -4.4287772 -4.4287624 -4.4287271 -4.4286695 -4.4286351 -4.4286523][-4.4289212 -4.4288888 -4.4288669 -4.4288592 -4.4288368 -4.4287834 -4.4287195 -4.4286833 -4.4286842 -4.4286952 -4.4287105 -4.4287133 -4.4286885 -4.4286652 -4.4286752][-4.4288783 -4.4288435 -4.4288206 -4.4288073 -4.4287715 -4.4287009 -4.4286175 -4.4285545 -4.428545 -4.4285784 -4.4286451 -4.4287062 -4.4287314 -4.4287314 -4.4287348][-4.428812 -4.4287834 -4.4287686 -4.4287519 -4.4287019 -4.4286132 -4.4285007 -4.4283953 -4.4283595 -4.4284186 -4.4285526 -4.428678 -4.4287567 -4.4287853 -4.4287915][-4.4287329 -4.4287114 -4.4287076 -4.4286933 -4.4286284 -4.4285264 -4.4283915 -4.4282436 -4.428184 -4.42828 -4.4284725 -4.4286532 -4.4287739 -4.4288259 -4.4288397][-4.4286542 -4.4286361 -4.4286394 -4.4286318 -4.4285755 -4.4285011 -4.4283977 -4.4282742 -4.4282422 -4.4283528 -4.4285388 -4.4287133 -4.4288263 -4.4288692 -4.4288754][-4.4286132 -4.42861 -4.4286251 -4.428637 -4.4286127 -4.4285812 -4.4285235 -4.4284496 -4.4284444 -4.4285355 -4.42867 -4.4287906 -4.4288683 -4.4288921 -4.428885][-4.428659 -4.4286675 -4.4286847 -4.4287028 -4.4287019 -4.4286962 -4.4286718 -4.4286342 -4.4286418 -4.4287033 -4.4287782 -4.4288459 -4.4288878 -4.4288964 -4.428885][-4.4287472 -4.4287496 -4.4287605 -4.4287786 -4.4287896 -4.4287915 -4.4287796 -4.42876 -4.4287677 -4.4288 -4.4288387 -4.4288721 -4.4288917 -4.4288917 -4.4288869][-4.4288168 -4.4288096 -4.4288106 -4.4288206 -4.4288292 -4.4288354 -4.428834 -4.4288249 -4.4288239 -4.4288363 -4.4288526 -4.4288669 -4.4288797 -4.4288864 -4.4288926][-4.4288731 -4.4288616 -4.4288535 -4.4288535 -4.4288549 -4.4288583 -4.4288549 -4.4288445 -4.4288325 -4.4288282 -4.4288363 -4.4288497 -4.4288669 -4.4288831 -4.4289002][-4.4289169 -4.4289074 -4.4288955 -4.4288859 -4.4288774 -4.4288683 -4.4288535 -4.4288359 -4.4288187 -4.4288187 -4.4288359 -4.4288578 -4.4288812 -4.4289017 -4.4289236]]...]
INFO - root - 2017-12-08 09:01:58.619881: step 65910, loss = 2.28, batch loss = 2.23 (36.9 examples/sec; 0.217 sec/batch; 16h:04m:19s remains)
INFO - root - 2017-12-08 09:02:00.844793: step 65920, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 16h:12m:57s remains)
INFO - root - 2017-12-08 09:02:03.086838: step 65930, loss = 2.28, batch loss = 2.23 (36.0 examples/sec; 0.222 sec/batch; 16h:26m:41s remains)
INFO - root - 2017-12-08 09:02:05.305536: step 65940, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 16h:15m:14s remains)
INFO - root - 2017-12-08 09:02:07.548505: step 65950, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.222 sec/batch; 16h:24m:34s remains)
INFO - root - 2017-12-08 09:02:09.764174: step 65960, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 16h:15m:57s remains)
INFO - root - 2017-12-08 09:02:12.006852: step 65970, loss = 2.28, batch loss = 2.23 (34.5 examples/sec; 0.232 sec/batch; 17h:08m:37s remains)
INFO - root - 2017-12-08 09:02:14.267403: step 65980, loss = 2.28, batch loss = 2.23 (35.7 examples/sec; 0.224 sec/batch; 16h:36m:45s remains)
INFO - root - 2017-12-08 09:02:16.490231: step 65990, loss = 2.28, batch loss = 2.23 (36.6 examples/sec; 0.219 sec/batch; 16h:11m:41s remains)
INFO - root - 2017-12-08 09:02:18.713984: step 66000, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.222 sec/batch; 16h:25m:17s remains)
2017-12-08 09:02:18.996068: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288154 -4.42884 -4.4288254 -4.4287658 -4.4286981 -4.4286757 -4.428688 -4.4287157 -4.4287567 -4.4287944 -4.4288268 -4.428822 -4.4288087 -4.4288073 -4.4288197][-4.4287968 -4.4288259 -4.4288249 -4.4287877 -4.4287496 -4.4287462 -4.4287648 -4.4287672 -4.4287682 -4.4287786 -4.4287972 -4.4287839 -4.4287624 -4.4287562 -4.4287691][-4.4287419 -4.4287682 -4.4287648 -4.4287457 -4.4287276 -4.4287424 -4.4287691 -4.4287577 -4.4287343 -4.4287319 -4.4287462 -4.4287362 -4.4287229 -4.4287333 -4.4287558][-4.4287019 -4.4287043 -4.4286838 -4.4286733 -4.4286709 -4.4286938 -4.4287167 -4.4286957 -4.4286566 -4.4286661 -4.4286885 -4.4286857 -4.4286909 -4.4287109 -4.4287376][-4.4286623 -4.4286318 -4.4285979 -4.4285975 -4.4286132 -4.4286494 -4.4286642 -4.4286189 -4.4285526 -4.4285765 -4.4286213 -4.4286432 -4.4286628 -4.4286828 -4.4287004][-4.4286022 -4.42854 -4.4284978 -4.4285049 -4.4285326 -4.42857 -4.4285679 -4.4284716 -4.4283681 -4.4284267 -4.4285421 -4.4286189 -4.4286575 -4.4286742 -4.4286823][-4.4285083 -4.4284163 -4.4283652 -4.4283857 -4.4284229 -4.4284477 -4.4283996 -4.4282184 -4.4280558 -4.4281836 -4.4284186 -4.42858 -4.4286561 -4.4286766 -4.42869][-4.42841 -4.4282985 -4.4282608 -4.4282951 -4.42833 -4.4283352 -4.4282346 -4.4279647 -4.4277492 -4.4279661 -4.4283085 -4.4285297 -4.4286351 -4.4286671 -4.4286871][-4.4283991 -4.4282851 -4.4282718 -4.4283042 -4.4283204 -4.4283233 -4.4282312 -4.4279871 -4.4278297 -4.4280553 -4.4283638 -4.4285469 -4.4286246 -4.4286571 -4.4286737][-4.42846 -4.4283648 -4.4283614 -4.4283757 -4.4283738 -4.4283857 -4.4283419 -4.4282012 -4.4281421 -4.4283185 -4.4285192 -4.4286227 -4.4286518 -4.42867 -4.4286776][-4.428545 -4.4284754 -4.4284739 -4.4284773 -4.4284663 -4.4284782 -4.428472 -4.4284134 -4.4284115 -4.42853 -4.42864 -4.4286785 -4.4286737 -4.4286742 -4.428679][-4.4286528 -4.4286137 -4.4286141 -4.4286165 -4.428606 -4.428618 -4.42863 -4.4286132 -4.42862 -4.4286776 -4.4287148 -4.4287004 -4.4286733 -4.4286695 -4.428679][-4.4287629 -4.4287486 -4.4287581 -4.428762 -4.4287486 -4.4287524 -4.4287648 -4.4287539 -4.4287424 -4.4287491 -4.4287353 -4.4286776 -4.4286227 -4.4286156 -4.4286308][-4.428854 -4.428853 -4.4288607 -4.428865 -4.4288564 -4.4288568 -4.4288564 -4.4288335 -4.4288034 -4.4287815 -4.4287429 -4.428659 -4.4285736 -4.428544 -4.4285507][-4.4289126 -4.428916 -4.4289174 -4.4289207 -4.4289165 -4.4289145 -4.4289045 -4.4288745 -4.4288421 -4.428823 -4.4287882 -4.42871 -4.4286051 -4.4285331 -4.428503]]...]
INFO - root - 2017-12-08 09:02:21.269992: step 66010, loss = 2.28, batch loss = 2.23 (34.3 examples/sec; 0.233 sec/batch; 17h:16m:11s remains)
INFO - root - 2017-12-08 09:02:23.505092: step 66020, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.221 sec/batch; 16h:22m:53s remains)
INFO - root - 2017-12-08 09:02:25.722250: step 66030, loss = 2.28, batch loss = 2.23 (36.7 examples/sec; 0.218 sec/batch; 16h:08m:07s remains)
INFO - root - 2017-12-08 09:02:27.949126: step 66040, loss = 2.28, batch loss = 2.23 (36.9 examples/sec; 0.217 sec/batch; 16h:03m:31s remains)
INFO - root - 2017-12-08 09:02:30.176822: step 66050, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 16h:13m:58s remains)
INFO - root - 2017-12-08 09:02:32.418005: step 66060, loss = 2.28, batch loss = 2.23 (35.6 examples/sec; 0.225 sec/batch; 16h:37m:18s remains)
INFO - root - 2017-12-08 09:02:34.648329: step 66070, loss = 2.28, batch loss = 2.23 (35.7 examples/sec; 0.224 sec/batch; 16h:36m:27s remains)
INFO - root - 2017-12-08 09:02:36.864456: step 66080, loss = 2.28, batch loss = 2.23 (35.6 examples/sec; 0.224 sec/batch; 16h:36m:44s remains)
INFO - root - 2017-12-08 09:02:39.185536: step 66090, loss = 2.28, batch loss = 2.23 (36.0 examples/sec; 0.222 sec/batch; 16h:26m:33s remains)
INFO - root - 2017-12-08 09:02:41.437763: step 66100, loss = 2.28, batch loss = 2.23 (35.0 examples/sec; 0.229 sec/batch; 16h:55m:40s remains)
2017-12-08 09:02:41.743891: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289451 -4.4289265 -4.4289041 -4.42889 -4.4288893 -4.4288955 -4.4288955 -4.4288831 -4.4288683 -4.4288549 -4.4288516 -4.4288654 -4.428895 -4.4289322 -4.4289608][-4.4289293 -4.4289064 -4.4288807 -4.428865 -4.4288697 -4.4288769 -4.4288678 -4.4288368 -4.4288144 -4.4287982 -4.4287949 -4.428813 -4.4288535 -4.4289002 -4.4289365][-4.4289103 -4.4288788 -4.4288521 -4.428833 -4.4288359 -4.4288464 -4.428833 -4.4287839 -4.4287519 -4.42874 -4.4287448 -4.4287739 -4.428823 -4.4288764 -4.4289174][-4.428915 -4.4288754 -4.4288421 -4.4288192 -4.428813 -4.42881 -4.4287891 -4.4287224 -4.428688 -4.4286904 -4.4287105 -4.4287486 -4.4287992 -4.4288564 -4.4289031][-4.4288759 -4.4288197 -4.4287744 -4.4287438 -4.4287324 -4.4287152 -4.4286871 -4.4286103 -4.4285884 -4.4286194 -4.4286637 -4.428719 -4.4287744 -4.4288349 -4.4288898][-4.42877 -4.4286933 -4.4286437 -4.4286079 -4.4285941 -4.4285588 -4.4285097 -4.4284067 -4.4283891 -4.4284663 -4.4285588 -4.4286551 -4.4287357 -4.4288111 -4.4288745][-4.4286275 -4.42853 -4.4284806 -4.4284482 -4.4284196 -4.4283528 -4.4282484 -4.4280887 -4.4280658 -4.4282045 -4.4283695 -4.4285345 -4.4286652 -4.4287715 -4.428854][-4.4284978 -4.4283819 -4.4283271 -4.4282956 -4.4282632 -4.4281712 -4.4280005 -4.42777 -4.4277277 -4.4279313 -4.4281778 -4.4284124 -4.4285984 -4.42874 -4.4288445][-4.4285574 -4.4284463 -4.4283981 -4.4283724 -4.4283381 -4.4282494 -4.4280791 -4.4278617 -4.427803 -4.4279737 -4.4281983 -4.4284186 -4.4286041 -4.42875 -4.4288568][-4.4287052 -4.4286208 -4.4285908 -4.4285717 -4.4285388 -4.428473 -4.4283543 -4.4282084 -4.4281573 -4.4282532 -4.4284015 -4.428556 -4.4286957 -4.42881 -4.428894][-4.4288278 -4.4287663 -4.4287481 -4.4287329 -4.4287057 -4.4286723 -4.428606 -4.42852 -4.4284878 -4.4285355 -4.4286237 -4.4287186 -4.4288054 -4.4288812 -4.4289379][-4.4288931 -4.4288483 -4.4288387 -4.4288335 -4.4288158 -4.4288049 -4.4287739 -4.4287248 -4.4287114 -4.4287424 -4.4287968 -4.42885 -4.4289 -4.4289451 -4.428978][-4.4289255 -4.4289031 -4.4289017 -4.4288988 -4.4288864 -4.428884 -4.4288721 -4.4288483 -4.4288449 -4.4288683 -4.4289021 -4.4289317 -4.428957 -4.4289837 -4.4290004][-4.4289374 -4.4289336 -4.428936 -4.4289322 -4.4289236 -4.42892 -4.4289117 -4.4288988 -4.4289036 -4.4289236 -4.4289474 -4.4289675 -4.4289856 -4.4290032 -4.4290109][-4.4289417 -4.4289484 -4.4289522 -4.4289489 -4.4289408 -4.4289341 -4.4289274 -4.4289222 -4.4289284 -4.4289441 -4.4289622 -4.4289761 -4.4289904 -4.4290047 -4.4290085]]...]
INFO - root - 2017-12-08 09:02:43.983233: step 66110, loss = 2.28, batch loss = 2.23 (35.2 examples/sec; 0.227 sec/batch; 16h:48m:45s remains)
INFO - root - 2017-12-08 09:02:46.228605: step 66120, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 16h:28m:18s remains)
INFO - root - 2017-12-08 09:02:48.456058: step 66130, loss = 2.28, batch loss = 2.23 (35.8 examples/sec; 0.223 sec/batch; 16h:30m:47s remains)
INFO - root - 2017-12-08 09:02:50.727370: step 66140, loss = 2.28, batch loss = 2.23 (33.7 examples/sec; 0.237 sec/batch; 17h:33m:36s remains)
INFO - root - 2017-12-08 09:02:52.971119: step 66150, loss = 2.28, batch loss = 2.23 (34.0 examples/sec; 0.235 sec/batch; 17h:23m:44s remains)
INFO - root - 2017-12-08 09:02:55.213253: step 66160, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.226 sec/batch; 16h:41m:44s remains)
INFO - root - 2017-12-08 09:02:57.455061: step 66170, loss = 2.28, batch loss = 2.23 (35.2 examples/sec; 0.227 sec/batch; 16h:49m:05s remains)
INFO - root - 2017-12-08 09:02:59.702783: step 66180, loss = 2.28, batch loss = 2.23 (35.3 examples/sec; 0.226 sec/batch; 16h:44m:43s remains)
INFO - root - 2017-12-08 09:03:01.928559: step 66190, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.225 sec/batch; 16h:39m:16s remains)
INFO - root - 2017-12-08 09:03:04.159673: step 66200, loss = 2.28, batch loss = 2.23 (35.6 examples/sec; 0.225 sec/batch; 16h:37m:31s remains)
2017-12-08 09:03:04.473625: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4290314 -4.429028 -4.42902 -4.4290018 -4.4289727 -4.4289389 -4.4289122 -4.4289031 -4.4289112 -4.4289293 -4.4289427 -4.4289656 -4.4289889 -4.4289947 -4.4289804][-4.4290342 -4.4290323 -4.4290237 -4.4290042 -4.428978 -4.4289532 -4.428936 -4.4289336 -4.4289403 -4.4289422 -4.42893 -4.4289417 -4.4289727 -4.4289918 -4.428987][-4.4290147 -4.4290118 -4.4289956 -4.4289632 -4.4289246 -4.428895 -4.4288816 -4.4288898 -4.4289069 -4.4289122 -4.4288979 -4.4289083 -4.4289427 -4.42897 -4.4289727][-4.4289756 -4.4289651 -4.4289355 -4.4288778 -4.4288068 -4.4287543 -4.4287348 -4.4287553 -4.4287939 -4.4288216 -4.4288263 -4.4288511 -4.4288926 -4.4289269 -4.4289403][-4.4289312 -4.4289131 -4.4288673 -4.42878 -4.4286709 -4.4285822 -4.4285436 -4.4285665 -4.4286289 -4.4286909 -4.4287324 -4.4287834 -4.4288325 -4.4288697 -4.4288926][-4.4288678 -4.4288511 -4.4288058 -4.4287076 -4.4285736 -4.4284496 -4.4283743 -4.428381 -4.4284644 -4.428565 -4.4286442 -4.4287138 -4.4287624 -4.428792 -4.4288154][-4.4287362 -4.4287343 -4.4287171 -4.4286437 -4.4285131 -4.4283528 -4.4282117 -4.4281678 -4.4282594 -4.4284048 -4.42853 -4.4286256 -4.4286757 -4.4286933 -4.4287114][-4.4285455 -4.4285769 -4.428617 -4.4286056 -4.428515 -4.4283514 -4.4281645 -4.4280696 -4.4281592 -4.4283323 -4.4284821 -4.4285908 -4.4286375 -4.4286408 -4.428648][-4.4284959 -4.4285564 -4.4286427 -4.4286895 -4.4286633 -4.4285612 -4.4284205 -4.4283276 -4.428359 -4.4284625 -4.4285617 -4.4286389 -4.4286757 -4.4286752 -4.4286742][-4.4285612 -4.4286079 -4.4286895 -4.4287567 -4.4287748 -4.4287333 -4.4286523 -4.4285831 -4.4285765 -4.4286156 -4.4286556 -4.4286942 -4.42871 -4.4287019 -4.4286938][-4.4286332 -4.4286652 -4.4287276 -4.4287906 -4.4288259 -4.4288182 -4.4287839 -4.4287467 -4.4287362 -4.42875 -4.4287558 -4.4287596 -4.4287472 -4.4287224 -4.4286985][-4.4286871 -4.4287248 -4.4287748 -4.4288182 -4.42884 -4.4288373 -4.4288259 -4.4288144 -4.4288197 -4.4288335 -4.4288335 -4.428822 -4.4287925 -4.4287529 -4.4287152][-4.4287343 -4.4287806 -4.4288235 -4.42885 -4.4288516 -4.4288359 -4.4288287 -4.4288282 -4.428843 -4.4288573 -4.42886 -4.4288507 -4.4288216 -4.4287791 -4.4287372][-4.4287152 -4.4287724 -4.4288239 -4.4288511 -4.4288354 -4.4288049 -4.428793 -4.4287953 -4.4288144 -4.4288311 -4.4288416 -4.4288344 -4.4288 -4.4287558 -4.428721][-4.4286814 -4.4287429 -4.4288025 -4.4288311 -4.4288054 -4.4287672 -4.4287539 -4.4287558 -4.428771 -4.4287853 -4.4287934 -4.4287806 -4.4287448 -4.4287119 -4.4286962]]...]
INFO - root - 2017-12-08 09:03:06.708457: step 66210, loss = 2.28, batch loss = 2.23 (35.4 examples/sec; 0.226 sec/batch; 16h:43m:35s remains)
INFO - root - 2017-12-08 09:03:08.939795: step 66220, loss = 2.28, batch loss = 2.23 (36.7 examples/sec; 0.218 sec/batch; 16h:07m:17s remains)
INFO - root - 2017-12-08 09:03:11.177497: step 66230, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 16h:15m:34s remains)
INFO - root - 2017-12-08 09:03:13.408823: step 66240, loss = 2.28, batch loss = 2.23 (35.4 examples/sec; 0.226 sec/batch; 16h:43m:28s remains)
INFO - root - 2017-12-08 09:03:15.662546: step 66250, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 16h:28m:45s remains)
INFO - root - 2017-12-08 09:03:17.907153: step 66260, loss = 2.28, batch loss = 2.23 (33.5 examples/sec; 0.239 sec/batch; 17h:40m:34s remains)
INFO - root - 2017-12-08 09:03:20.153235: step 66270, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 16h:14m:01s remains)
INFO - root - 2017-12-08 09:03:22.378077: step 66280, loss = 2.28, batch loss = 2.23 (35.8 examples/sec; 0.224 sec/batch; 16h:32m:13s remains)
INFO - root - 2017-12-08 09:03:24.672468: step 66290, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.222 sec/batch; 16h:22m:58s remains)
INFO - root - 2017-12-08 09:03:26.905053: step 66300, loss = 2.28, batch loss = 2.23 (35.1 examples/sec; 0.228 sec/batch; 16h:50m:52s remains)
2017-12-08 09:03:27.207768: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4290032 -4.4290018 -4.429 -4.4289923 -4.4289846 -4.4289751 -4.4289665 -4.4289622 -4.4289522 -4.42893 -4.428894 -4.4288225 -4.428751 -4.4287167 -4.4287415][-4.4290085 -4.4290066 -4.4290032 -4.4289947 -4.4289842 -4.4289746 -4.4289713 -4.4289756 -4.428977 -4.4289656 -4.4289455 -4.4289041 -4.4288626 -4.4288397 -4.4288445][-4.4290118 -4.4290109 -4.4290042 -4.42899 -4.4289732 -4.4289575 -4.4289489 -4.4289517 -4.4289565 -4.4289556 -4.4289508 -4.428936 -4.4289231 -4.4289203 -4.4289212][-4.4290085 -4.4290042 -4.4289927 -4.4289718 -4.4289494 -4.4289265 -4.4289083 -4.4289045 -4.4289088 -4.4289103 -4.4289155 -4.4289165 -4.4289241 -4.42894 -4.42895][-4.428998 -4.428988 -4.4289722 -4.42895 -4.4289274 -4.4288988 -4.4288678 -4.4288497 -4.4288354 -4.4288249 -4.4288268 -4.4288383 -4.4288635 -4.4288993 -4.4289231][-4.4289832 -4.4289727 -4.4289589 -4.4289427 -4.4289203 -4.42888 -4.428833 -4.4287906 -4.4287405 -4.4287038 -4.4287019 -4.4287338 -4.4287839 -4.4288383 -4.4288774][-4.42896 -4.4289503 -4.4289432 -4.4289374 -4.4289236 -4.4288797 -4.4288154 -4.4287443 -4.4286489 -4.42858 -4.4285841 -4.4286494 -4.42873 -4.4288054 -4.4288559][-4.4289222 -4.428916 -4.4289155 -4.4289212 -4.4289184 -4.428884 -4.4288187 -4.4287338 -4.428616 -4.4285307 -4.4285393 -4.4286218 -4.4287171 -4.4288044 -4.4288592][-4.428875 -4.4288726 -4.4288807 -4.4288964 -4.4289017 -4.42888 -4.428834 -4.4287663 -4.428668 -4.4285979 -4.4286036 -4.4286723 -4.428751 -4.4288259 -4.428875][-4.4288316 -4.428822 -4.428822 -4.428843 -4.4288654 -4.4288664 -4.4288464 -4.42881 -4.4287581 -4.4287252 -4.4287319 -4.4287686 -4.428812 -4.4288597 -4.4288993][-4.4287982 -4.4287567 -4.4287271 -4.428751 -4.4288039 -4.4288373 -4.4288449 -4.4288363 -4.4288244 -4.4288187 -4.4288259 -4.4288344 -4.4288473 -4.4288778 -4.4289126][-4.42877 -4.4286842 -4.4286218 -4.4286575 -4.4287491 -4.42882 -4.42885 -4.428853 -4.4288535 -4.4288516 -4.4288507 -4.4288421 -4.4288416 -4.4288688 -4.428906][-4.4287653 -4.4286571 -4.4285803 -4.4286156 -4.4287243 -4.4288177 -4.4288621 -4.4288697 -4.4288592 -4.4288368 -4.4288154 -4.4287953 -4.4287944 -4.4288244 -4.4288707][-4.4288044 -4.4287148 -4.4286475 -4.4286637 -4.4287505 -4.4288306 -4.4288712 -4.4288754 -4.4288454 -4.4287987 -4.4287529 -4.4287066 -4.4286938 -4.4287252 -4.4287887][-4.4288692 -4.4288135 -4.428772 -4.42877 -4.4288116 -4.4288535 -4.4288754 -4.4288716 -4.4288268 -4.428762 -4.4286938 -4.4286141 -4.4285789 -4.4286151 -4.4286966]]...]
INFO - root - 2017-12-08 09:03:29.427571: step 66310, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.222 sec/batch; 16h:24m:16s remains)
INFO - root - 2017-12-08 09:03:31.647649: step 66320, loss = 2.28, batch loss = 2.23 (36.0 examples/sec; 0.222 sec/batch; 16h:24m:36s remains)
INFO - root - 2017-12-08 09:03:33.877020: step 66330, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 16h:19m:19s remains)
INFO - root - 2017-12-08 09:03:36.116461: step 66340, loss = 2.28, batch loss = 2.23 (34.7 examples/sec; 0.230 sec/batch; 17h:01m:15s remains)
INFO - root - 2017-12-08 09:03:38.365445: step 66350, loss = 2.28, batch loss = 2.23 (32.7 examples/sec; 0.245 sec/batch; 18h:05m:49s remains)
INFO - root - 2017-12-08 09:03:40.575295: step 66360, loss = 2.28, batch loss = 2.23 (37.4 examples/sec; 0.214 sec/batch; 15h:47m:34s remains)
INFO - root - 2017-12-08 09:03:42.807818: step 66370, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 16h:13m:47s remains)
INFO - root - 2017-12-08 09:03:45.022002: step 66380, loss = 2.28, batch loss = 2.23 (37.1 examples/sec; 0.216 sec/batch; 15h:57m:19s remains)
INFO - root - 2017-12-08 09:03:47.261847: step 66390, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.221 sec/batch; 16h:18m:17s remains)
INFO - root - 2017-12-08 09:03:49.493748: step 66400, loss = 2.28, batch loss = 2.23 (36.0 examples/sec; 0.222 sec/batch; 16h:26m:41s remains)
2017-12-08 09:03:49.787881: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4286642 -4.4286919 -4.4287281 -4.4287415 -4.4287367 -4.4287233 -4.4287243 -4.4287357 -4.4287539 -4.4287682 -4.4287729 -4.4287715 -4.4287653 -4.4287591 -4.4287443][-4.4286137 -4.428648 -4.4286866 -4.4287071 -4.4287105 -4.4287024 -4.4287066 -4.4287238 -4.4287496 -4.4287672 -4.4287748 -4.4287796 -4.4287825 -4.4287848 -4.4287729][-4.4285626 -4.4285994 -4.4286432 -4.4286747 -4.4286871 -4.4286847 -4.42869 -4.42871 -4.4287424 -4.4287639 -4.4287715 -4.428781 -4.4287915 -4.4288025 -4.4288015][-4.4284897 -4.4285359 -4.4285984 -4.4286509 -4.4286814 -4.4286828 -4.4286847 -4.4286962 -4.4287205 -4.4287395 -4.4287505 -4.4287672 -4.4287853 -4.4288058 -4.4288173][-4.4284263 -4.428483 -4.4285641 -4.4286361 -4.4286795 -4.4286847 -4.4286804 -4.4286704 -4.428669 -4.428679 -4.4286981 -4.4287252 -4.4287534 -4.4287834 -4.4288049][-4.4284353 -4.4284754 -4.4285612 -4.4286366 -4.4286752 -4.4286752 -4.4286585 -4.4286265 -4.4285994 -4.4285965 -4.4286208 -4.42866 -4.4287014 -4.4287434 -4.4287791][-4.4284992 -4.4285159 -4.4285941 -4.4286633 -4.4286885 -4.4286823 -4.4286628 -4.4286175 -4.4285684 -4.4285464 -4.4285641 -4.4286022 -4.4286509 -4.4287071 -4.4287624][-4.4285769 -4.4285746 -4.4286375 -4.4286952 -4.4287095 -4.4286981 -4.4286852 -4.4286447 -4.4285903 -4.4285574 -4.4285693 -4.4286 -4.428638 -4.4286876 -4.42875][-4.4286242 -4.4286084 -4.4286528 -4.4286981 -4.4287167 -4.428719 -4.4287176 -4.4286947 -4.428658 -4.4286351 -4.4286427 -4.4286613 -4.4286747 -4.4286962 -4.4287376][-4.4286671 -4.4286289 -4.4286547 -4.4286976 -4.4287353 -4.428762 -4.4287829 -4.4287844 -4.4287634 -4.4287424 -4.42874 -4.4287467 -4.4287434 -4.42874 -4.4287543][-4.4287095 -4.4286542 -4.4286623 -4.4287 -4.4287496 -4.4287972 -4.4288335 -4.428853 -4.428844 -4.4288268 -4.4288125 -4.42881 -4.4288015 -4.4287887 -4.4287858][-4.428731 -4.428668 -4.4286618 -4.4286852 -4.428731 -4.4287839 -4.4288316 -4.428865 -4.4288735 -4.4288688 -4.4288549 -4.4288487 -4.4288363 -4.4288206 -4.4288111][-4.4287353 -4.4286757 -4.4286633 -4.4286723 -4.4287038 -4.4287419 -4.4287848 -4.4288239 -4.4288468 -4.4288549 -4.42885 -4.42885 -4.42884 -4.4288254 -4.4288187][-4.4287462 -4.4286957 -4.4286857 -4.4286833 -4.4286914 -4.4287047 -4.4287248 -4.428751 -4.4287753 -4.4287958 -4.4288063 -4.4288163 -4.4288163 -4.4288135 -4.4288158][-4.4287639 -4.4287205 -4.4287195 -4.4287162 -4.4287047 -4.4286904 -4.4286809 -4.4286895 -4.4287028 -4.4287243 -4.4287496 -4.428772 -4.428782 -4.4287925 -4.4288082]]...]
INFO - root - 2017-12-08 09:03:52.007093: step 66410, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 16h:15m:30s remains)
INFO - root - 2017-12-08 09:03:54.272519: step 66420, loss = 2.28, batch loss = 2.23 (35.3 examples/sec; 0.226 sec/batch; 16h:44m:07s remains)
INFO - root - 2017-12-08 09:03:56.498139: step 66430, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.222 sec/batch; 16h:22m:44s remains)
INFO - root - 2017-12-08 09:03:58.733599: step 66440, loss = 2.28, batch loss = 2.23 (36.0 examples/sec; 0.222 sec/batch; 16h:25m:30s remains)
INFO - root - 2017-12-08 09:04:00.966532: step 66450, loss = 2.28, batch loss = 2.23 (34.9 examples/sec; 0.229 sec/batch; 16h:56m:15s remains)
INFO - root - 2017-12-08 09:04:03.193372: step 66460, loss = 2.28, batch loss = 2.23 (35.7 examples/sec; 0.224 sec/batch; 16h:34m:32s remains)
INFO - root - 2017-12-08 09:04:05.418653: step 66470, loss = 2.28, batch loss = 2.23 (35.7 examples/sec; 0.224 sec/batch; 16h:34m:25s remains)
INFO - root - 2017-12-08 09:04:07.671758: step 66480, loss = 2.28, batch loss = 2.23 (35.1 examples/sec; 0.228 sec/batch; 16h:49m:12s remains)
INFO - root - 2017-12-08 09:04:09.941615: step 66490, loss = 2.28, batch loss = 2.23 (35.0 examples/sec; 0.229 sec/batch; 16h:53m:36s remains)
INFO - root - 2017-12-08 09:04:12.190820: step 66500, loss = 2.28, batch loss = 2.23 (33.8 examples/sec; 0.237 sec/batch; 17h:30m:48s remains)
2017-12-08 09:04:12.518575: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289565 -4.4289608 -4.4289584 -4.4289427 -4.4289036 -4.4288378 -4.4287562 -4.4286656 -4.4285593 -4.4284277 -4.428318 -4.4282904 -4.4283409 -4.4284267 -4.4285245][-4.4289722 -4.4289784 -4.4289775 -4.4289656 -4.4289351 -4.4288869 -4.4288306 -4.4287772 -4.4287124 -4.4286232 -4.428545 -4.4285069 -4.4285092 -4.4285464 -4.428607][-4.4289808 -4.428988 -4.4289885 -4.4289794 -4.4289565 -4.4289217 -4.4288874 -4.4288654 -4.4288454 -4.4288015 -4.4287524 -4.4287119 -4.4286895 -4.4286938 -4.42872][-4.4289837 -4.4289885 -4.4289851 -4.4289703 -4.4289403 -4.4289031 -4.4288735 -4.4288692 -4.4288831 -4.4288859 -4.428874 -4.4288507 -4.4288278 -4.4288173 -4.428823][-4.4289813 -4.4289784 -4.4289618 -4.4289231 -4.4288635 -4.4287996 -4.4287572 -4.4287581 -4.4288011 -4.4288554 -4.4288974 -4.4289117 -4.4289079 -4.4289007 -4.428896][-4.42898 -4.4289694 -4.4289327 -4.4288573 -4.4287457 -4.4286246 -4.4285398 -4.4285307 -4.4286036 -4.428719 -4.4288287 -4.4288974 -4.42893 -4.4289384 -4.4289327][-4.4289837 -4.4289703 -4.42892 -4.42882 -4.4286628 -4.4284773 -4.4283195 -4.4282665 -4.4283576 -4.4285269 -4.4287 -4.4288287 -4.4289069 -4.4289422 -4.4289489][-4.4289885 -4.4289737 -4.4289184 -4.4288116 -4.4286356 -4.4284148 -4.4282064 -4.4281116 -4.42821 -4.4284048 -4.4286094 -4.4287734 -4.4288831 -4.4289427 -4.4289641][-4.4289947 -4.4289832 -4.4289341 -4.4288445 -4.4286909 -4.4284945 -4.4283166 -4.4282389 -4.4283104 -4.42846 -4.42863 -4.4287806 -4.428885 -4.4289455 -4.42897][-4.4290018 -4.4289951 -4.4289641 -4.4289045 -4.4287925 -4.4286489 -4.4285245 -4.4284668 -4.4284859 -4.4285574 -4.4286666 -4.4287748 -4.4288526 -4.4289017 -4.4289217][-4.4289832 -4.4289756 -4.4289589 -4.4289284 -4.4288607 -4.4287682 -4.4286819 -4.4286304 -4.4286127 -4.42863 -4.4286833 -4.4287415 -4.4287877 -4.4288182 -4.4288273][-4.4289331 -4.4289155 -4.4289055 -4.428895 -4.4288645 -4.428813 -4.42875 -4.4286962 -4.4286604 -4.4286494 -4.4286637 -4.4286857 -4.4287081 -4.4287171 -4.4287066][-4.4288511 -4.4288168 -4.4288025 -4.4288034 -4.4288025 -4.4287844 -4.4287462 -4.4287014 -4.4286613 -4.4286389 -4.4286332 -4.4286394 -4.4286537 -4.4286461 -4.4286079][-4.4287453 -4.4286942 -4.4286747 -4.4286833 -4.4287033 -4.4287124 -4.4287014 -4.4286804 -4.4286542 -4.4286332 -4.4286246 -4.4286337 -4.4286523 -4.4286375 -4.4285822][-4.4286747 -4.4286237 -4.4286113 -4.4286304 -4.4286618 -4.4286833 -4.4286942 -4.4286995 -4.4286933 -4.4286752 -4.4286604 -4.4286671 -4.4286871 -4.4286747 -4.4286227]]...]
INFO - root - 2017-12-08 09:04:14.752108: step 66510, loss = 2.28, batch loss = 2.23 (34.4 examples/sec; 0.232 sec/batch; 17h:09m:34s remains)
INFO - root - 2017-12-08 09:04:16.974256: step 66520, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 16h:26m:50s remains)
INFO - root - 2017-12-08 09:04:19.206309: step 66530, loss = 2.28, batch loss = 2.23 (34.5 examples/sec; 0.232 sec/batch; 17h:06m:49s remains)
INFO - root - 2017-12-08 09:04:21.432496: step 66540, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 16h:11m:26s remains)
INFO - root - 2017-12-08 09:04:23.658154: step 66550, loss = 2.28, batch loss = 2.23 (36.8 examples/sec; 0.217 sec/batch; 16h:03m:56s remains)
INFO - root - 2017-12-08 09:04:25.900058: step 66560, loss = 2.28, batch loss = 2.23 (35.4 examples/sec; 0.226 sec/batch; 16h:42m:48s remains)
INFO - root - 2017-12-08 09:04:28.136172: step 66570, loss = 2.28, batch loss = 2.23 (35.7 examples/sec; 0.224 sec/batch; 16h:32m:30s remains)
INFO - root - 2017-12-08 09:04:30.369298: step 66580, loss = 2.28, batch loss = 2.23 (36.0 examples/sec; 0.222 sec/batch; 16h:24m:18s remains)
INFO - root - 2017-12-08 09:04:32.611950: step 66590, loss = 2.28, batch loss = 2.23 (35.3 examples/sec; 0.227 sec/batch; 16h:45m:11s remains)
INFO - root - 2017-12-08 09:04:34.834531: step 66600, loss = 2.28, batch loss = 2.23 (35.1 examples/sec; 0.228 sec/batch; 16h:51m:28s remains)
2017-12-08 09:04:35.159267: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4286046 -4.4286432 -4.4287572 -4.4288468 -4.428875 -4.4288564 -4.4288239 -4.4288106 -4.4288239 -4.4288445 -4.4288421 -4.4287825 -4.4286981 -4.4286594 -4.4286942][-4.4285936 -4.4286427 -4.4287729 -4.4288664 -4.428884 -4.4288359 -4.4287677 -4.4287329 -4.4287558 -4.4288082 -4.4288349 -4.4287949 -4.4287167 -4.42867 -4.4286947][-4.4286103 -4.4286671 -4.428802 -4.4288859 -4.4288797 -4.4287863 -4.4286675 -4.4286127 -4.428658 -4.4287615 -4.428833 -4.4288239 -4.4287539 -4.4286928 -4.4286962][-4.4286437 -4.428699 -4.4288278 -4.4288993 -4.428864 -4.4287214 -4.4285545 -4.4284859 -4.4285717 -4.4287257 -4.4288344 -4.4288516 -4.4287906 -4.4287157 -4.4286914][-4.4286609 -4.4287076 -4.4288254 -4.4288874 -4.4288325 -4.4286556 -4.4284568 -4.4283891 -4.4285197 -4.4287043 -4.4288239 -4.4288511 -4.4287953 -4.4287081 -4.4286628][-4.4286613 -4.4286933 -4.4287996 -4.42886 -4.4288015 -4.4286075 -4.4283895 -4.4283237 -4.4284816 -4.4286771 -4.4287896 -4.4288158 -4.4287663 -4.4286737 -4.4286137][-4.4286613 -4.4286685 -4.4287629 -4.4288273 -4.4287753 -4.4285836 -4.4283571 -4.4282918 -4.4284525 -4.4286385 -4.4287353 -4.4287577 -4.4287138 -4.4286251 -4.4285665][-4.4286613 -4.4286456 -4.428731 -4.4288063 -4.428772 -4.4286056 -4.4283953 -4.428328 -4.4284563 -4.4286079 -4.4286861 -4.4287024 -4.4286661 -4.4285951 -4.4285569][-4.4286585 -4.4286323 -4.4287143 -4.4288011 -4.4287848 -4.4286447 -4.4284558 -4.4283743 -4.4284549 -4.4285836 -4.4286609 -4.4286785 -4.4286556 -4.428616 -4.4286089][-4.428658 -4.4286313 -4.4287105 -4.4288034 -4.4287968 -4.4286776 -4.428514 -4.428422 -4.4284716 -4.428597 -4.4286838 -4.4287043 -4.4286933 -4.428679 -4.4286876][-4.4286609 -4.4286366 -4.4287124 -4.4288082 -4.4288087 -4.4287076 -4.4285717 -4.4284854 -4.4285245 -4.4286466 -4.4287319 -4.4287519 -4.4287462 -4.4287438 -4.4287577][-4.4286628 -4.42864 -4.4287133 -4.4288096 -4.4288163 -4.4287229 -4.4286036 -4.4285316 -4.4285741 -4.4286942 -4.4287782 -4.4288063 -4.4288111 -4.4288139 -4.4288244][-4.4286661 -4.4286389 -4.4287066 -4.4287996 -4.42881 -4.4287243 -4.4286165 -4.4285579 -4.4286065 -4.4287243 -4.4288125 -4.4288511 -4.428864 -4.428865 -4.4288673][-4.4286723 -4.4286346 -4.4286895 -4.4287786 -4.4287987 -4.42873 -4.4286375 -4.4285903 -4.4286366 -4.4287472 -4.4288354 -4.4288716 -4.4288816 -4.4288812 -4.4288807][-4.4286833 -4.4286332 -4.42867 -4.4287567 -4.4287944 -4.4287496 -4.4286752 -4.4286318 -4.4286628 -4.4287539 -4.4288373 -4.4288688 -4.4288745 -4.428874 -4.4288764]]...]
INFO - root - 2017-12-08 09:04:37.390738: step 66610, loss = 2.28, batch loss = 2.23 (34.0 examples/sec; 0.235 sec/batch; 17h:22m:42s remains)
INFO - root - 2017-12-08 09:04:39.630663: step 66620, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.220 sec/batch; 16h:16m:39s remains)
INFO - root - 2017-12-08 09:04:41.891293: step 66630, loss = 2.28, batch loss = 2.23 (35.7 examples/sec; 0.224 sec/batch; 16h:32m:52s remains)
INFO - root - 2017-12-08 09:04:44.160303: step 66640, loss = 2.28, batch loss = 2.23 (35.3 examples/sec; 0.226 sec/batch; 16h:42m:56s remains)
INFO - root - 2017-12-08 09:04:46.436991: step 66650, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.225 sec/batch; 16h:38m:03s remains)
INFO - root - 2017-12-08 09:04:48.661775: step 66660, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 16h:15m:02s remains)
INFO - root - 2017-12-08 09:04:50.888292: step 66670, loss = 2.28, batch loss = 2.23 (36.9 examples/sec; 0.217 sec/batch; 16h:01m:22s remains)
INFO - root - 2017-12-08 09:04:53.148629: step 66680, loss = 2.28, batch loss = 2.23 (33.9 examples/sec; 0.236 sec/batch; 17h:26m:02s remains)
INFO - root - 2017-12-08 09:04:55.389255: step 66690, loss = 2.28, batch loss = 2.23 (35.3 examples/sec; 0.227 sec/batch; 16h:44m:52s remains)
INFO - root - 2017-12-08 09:04:57.611286: step 66700, loss = 2.28, batch loss = 2.23 (34.7 examples/sec; 0.230 sec/batch; 17h:00m:55s remains)
2017-12-08 09:04:57.913772: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4287443 -4.4287653 -4.428793 -4.4288173 -4.428823 -4.428792 -4.4287248 -4.4286437 -4.4285908 -4.4285884 -4.4286208 -4.4286661 -4.4286861 -4.4286857 -4.4286833][-4.4288273 -4.4288397 -4.4288592 -4.4288692 -4.4288578 -4.4288182 -4.4287553 -4.4286876 -4.4286451 -4.42865 -4.4286847 -4.4287138 -4.4287152 -4.4287052 -4.4287033][-4.4288573 -4.4288716 -4.4288874 -4.428884 -4.4288564 -4.4288111 -4.4287529 -4.4287028 -4.4286833 -4.4286981 -4.4287324 -4.4287472 -4.4287262 -4.4287028 -4.42872][-4.428822 -4.4288359 -4.4288511 -4.4288421 -4.4288106 -4.4287677 -4.4287257 -4.4286957 -4.4286928 -4.4287086 -4.4287367 -4.4287362 -4.4286966 -4.4286666 -4.4286985][-4.4287577 -4.4287639 -4.4287872 -4.428791 -4.4287643 -4.4287243 -4.4286866 -4.4286652 -4.4286761 -4.4287004 -4.4287257 -4.4287186 -4.4286714 -4.4286413 -4.4286685][-4.4286952 -4.4286928 -4.4287195 -4.4287372 -4.42872 -4.4286852 -4.4286494 -4.4286275 -4.4286366 -4.4286652 -4.4286976 -4.4287024 -4.4286661 -4.428638 -4.4286523][-4.4286642 -4.4286618 -4.4286876 -4.4287062 -4.4286957 -4.4286718 -4.4286537 -4.4286408 -4.4286475 -4.4286675 -4.4286933 -4.4287 -4.4286737 -4.4286451 -4.4286461][-4.4287009 -4.4286904 -4.4287033 -4.4287128 -4.4287052 -4.4286928 -4.4286933 -4.4286981 -4.4287043 -4.4287076 -4.4287152 -4.42872 -4.4286985 -4.4286642 -4.4286456][-4.4287891 -4.4287639 -4.4287567 -4.4287534 -4.4287438 -4.4287419 -4.4287624 -4.4287796 -4.428781 -4.4287705 -4.4287591 -4.4287581 -4.4287391 -4.4287004 -4.4286594][-4.4288735 -4.4288421 -4.4288173 -4.4288073 -4.4288006 -4.4288044 -4.428833 -4.4288549 -4.42886 -4.4288487 -4.4288297 -4.4288135 -4.4287839 -4.4287357 -4.4286828][-4.4288926 -4.4288626 -4.4288349 -4.4288321 -4.4288344 -4.4288397 -4.42887 -4.4288979 -4.4289055 -4.4288969 -4.4288745 -4.4288464 -4.4288034 -4.4287548 -4.4287143][-4.4288416 -4.4288144 -4.4287958 -4.4288087 -4.4288244 -4.4288387 -4.4288745 -4.4289031 -4.4289093 -4.428896 -4.4288707 -4.4288397 -4.4287977 -4.4287605 -4.4287405][-4.4287491 -4.4287162 -4.4287033 -4.4287257 -4.4287643 -4.4288044 -4.4288516 -4.4288778 -4.4288783 -4.4288607 -4.428833 -4.4288063 -4.4287786 -4.428761 -4.4287586][-4.428659 -4.4286132 -4.4285893 -4.4286103 -4.4286594 -4.428721 -4.4287786 -4.4288058 -4.4288058 -4.4288011 -4.4287868 -4.4287734 -4.4287596 -4.4287596 -4.4287677][-4.4286065 -4.4285479 -4.4285069 -4.4285111 -4.428556 -4.4286261 -4.4286895 -4.4287229 -4.4287343 -4.4287462 -4.428751 -4.4287505 -4.4287472 -4.4287572 -4.4287696]]...]
INFO - root - 2017-12-08 09:05:00.131072: step 66710, loss = 2.28, batch loss = 2.23 (36.7 examples/sec; 0.218 sec/batch; 16h:06m:48s remains)
INFO - root - 2017-12-08 09:05:02.364104: step 66720, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.222 sec/batch; 16h:21m:23s remains)
INFO - root - 2017-12-08 09:05:04.590011: step 66730, loss = 2.28, batch loss = 2.23 (35.6 examples/sec; 0.225 sec/batch; 16h:36m:45s remains)
INFO - root - 2017-12-08 09:05:06.831693: step 66740, loss = 2.28, batch loss = 2.23 (35.4 examples/sec; 0.226 sec/batch; 16h:42m:19s remains)
INFO - root - 2017-12-08 09:05:09.063612: step 66750, loss = 2.28, batch loss = 2.23 (35.6 examples/sec; 0.225 sec/batch; 16h:35m:57s remains)
INFO - root - 2017-12-08 09:05:11.283840: step 66760, loss = 2.28, batch loss = 2.23 (36.9 examples/sec; 0.217 sec/batch; 15h:59m:37s remains)
INFO - root - 2017-12-08 09:05:13.519227: step 66770, loss = 2.28, batch loss = 2.23 (36.0 examples/sec; 0.222 sec/batch; 16h:25m:13s remains)
INFO - root - 2017-12-08 09:05:15.745688: step 66780, loss = 2.28, batch loss = 2.23 (36.7 examples/sec; 0.218 sec/batch; 16h:05m:57s remains)
INFO - root - 2017-12-08 09:05:17.962898: step 66790, loss = 2.28, batch loss = 2.23 (37.4 examples/sec; 0.214 sec/batch; 15h:46m:44s remains)
INFO - root - 2017-12-08 09:05:20.234340: step 66800, loss = 2.28, batch loss = 2.23 (36.6 examples/sec; 0.219 sec/batch; 16h:08m:05s remains)
2017-12-08 09:05:20.597914: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4287043 -4.4287176 -4.4287481 -4.4287744 -4.4287872 -4.4287496 -4.4286861 -4.4286489 -4.4286685 -4.4287105 -4.4287419 -4.4287782 -4.4288 -4.4287758 -4.4287362][-4.4287167 -4.4287324 -4.4287634 -4.4288049 -4.4288287 -4.428802 -4.428741 -4.4286942 -4.4287057 -4.4287362 -4.4287543 -4.4287806 -4.4287963 -4.4287648 -4.4287105][-4.4287148 -4.4287338 -4.4287643 -4.4288154 -4.4288468 -4.42883 -4.4287643 -4.4287038 -4.4287066 -4.4287257 -4.4287324 -4.4287477 -4.4287615 -4.4287329 -4.4286833][-4.428709 -4.4287324 -4.4287586 -4.4287996 -4.4288249 -4.428803 -4.4287252 -4.4286451 -4.4286437 -4.4286728 -4.4286828 -4.4286923 -4.4287143 -4.4287019 -4.428668][-4.4287119 -4.4287429 -4.428762 -4.4287677 -4.4287677 -4.4287257 -4.4286151 -4.4285045 -4.4285069 -4.4285774 -4.4286156 -4.428637 -4.4286733 -4.4286842 -4.42867][-4.4287238 -4.4287639 -4.4287705 -4.4287286 -4.4286819 -4.4286022 -4.4284358 -4.4282708 -4.4282956 -4.428452 -4.4285469 -4.4285955 -4.4286451 -4.4286776 -4.4286771][-4.4287467 -4.428791 -4.4287844 -4.4287081 -4.4286141 -4.4284835 -4.4282517 -4.4280195 -4.4280782 -4.4283447 -4.4285007 -4.4285693 -4.4286261 -4.4286642 -4.4286571][-4.4287877 -4.4288225 -4.4288163 -4.4287381 -4.4286251 -4.4284773 -4.428226 -4.4279761 -4.4280448 -4.4283433 -4.4285097 -4.4285727 -4.4286165 -4.4286385 -4.4286089][-4.4288387 -4.4288597 -4.42886 -4.4287977 -4.4286962 -4.4285779 -4.4283891 -4.4282093 -4.4282522 -4.4284697 -4.4285831 -4.4286094 -4.4286213 -4.4286127 -4.4285588][-4.4288754 -4.42889 -4.4288955 -4.4288487 -4.4287729 -4.4286957 -4.4285827 -4.4284797 -4.4285049 -4.4286342 -4.4286942 -4.4286866 -4.4286628 -4.4286227 -4.4285541][-4.4288745 -4.4288859 -4.4289 -4.428875 -4.4288273 -4.4287753 -4.428709 -4.4286542 -4.4286752 -4.4287553 -4.4287896 -4.4287782 -4.4287457 -4.4286957 -4.4286213][-4.42885 -4.4288497 -4.428863 -4.4288545 -4.4288211 -4.4287834 -4.4287453 -4.4287271 -4.4287572 -4.4288177 -4.4288473 -4.4288468 -4.4288211 -4.4287796 -4.428721][-4.4288211 -4.4288106 -4.4288211 -4.4288135 -4.4287882 -4.4287591 -4.4287443 -4.4287558 -4.4287925 -4.4288487 -4.428885 -4.4289 -4.4288831 -4.4288559 -4.4288244][-4.4288282 -4.4288135 -4.4288211 -4.4288139 -4.4287972 -4.4287844 -4.4287915 -4.4288173 -4.4288454 -4.4288921 -4.4289293 -4.4289508 -4.4289465 -4.4289331 -4.42892][-4.4288683 -4.428854 -4.428853 -4.4288468 -4.4288359 -4.4288397 -4.4288654 -4.4288969 -4.4289088 -4.4289327 -4.4289579 -4.4289765 -4.4289827 -4.4289804 -4.4289775]]...]
INFO - root - 2017-12-08 09:05:22.847347: step 66810, loss = 2.28, batch loss = 2.23 (35.4 examples/sec; 0.226 sec/batch; 16h:41m:00s remains)
INFO - root - 2017-12-08 09:05:25.101111: step 66820, loss = 2.28, batch loss = 2.23 (36.9 examples/sec; 0.217 sec/batch; 15h:58m:47s remains)
INFO - root - 2017-12-08 09:05:27.330430: step 66830, loss = 2.28, batch loss = 2.23 (35.8 examples/sec; 0.223 sec/batch; 16h:29m:31s remains)
INFO - root - 2017-12-08 09:05:29.561666: step 66840, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 16h:18m:15s remains)
INFO - root - 2017-12-08 09:05:31.801927: step 66850, loss = 2.28, batch loss = 2.23 (36.7 examples/sec; 0.218 sec/batch; 16h:04m:17s remains)
INFO - root - 2017-12-08 09:05:34.020987: step 66860, loss = 2.28, batch loss = 2.23 (37.2 examples/sec; 0.215 sec/batch; 15h:52m:36s remains)
INFO - root - 2017-12-08 09:05:36.281928: step 66870, loss = 2.28, batch loss = 2.23 (35.8 examples/sec; 0.224 sec/batch; 16h:29m:42s remains)
INFO - root - 2017-12-08 09:05:38.515285: step 66880, loss = 2.28, batch loss = 2.23 (33.9 examples/sec; 0.236 sec/batch; 17h:24m:19s remains)
INFO - root - 2017-12-08 09:05:40.762187: step 66890, loss = 2.28, batch loss = 2.23 (35.4 examples/sec; 0.226 sec/batch; 16h:39m:49s remains)
INFO - root - 2017-12-08 09:05:42.987272: step 66900, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 16h:11m:07s remains)
2017-12-08 09:05:43.273790: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289703 -4.4289651 -4.4289513 -4.4289293 -4.4289007 -4.4288616 -4.4288387 -4.4288378 -4.42885 -4.4288549 -4.42885 -4.4288416 -4.4288249 -4.4288225 -4.428833][-4.4289703 -4.4289632 -4.4289403 -4.4289045 -4.4288559 -4.4287987 -4.4287667 -4.4287767 -4.4288144 -4.4288335 -4.42883 -4.4288192 -4.4287992 -4.4287958 -4.4288044][-4.4289556 -4.4289494 -4.4289212 -4.42887 -4.4287958 -4.4287257 -4.428688 -4.4287009 -4.4287639 -4.4288044 -4.4288063 -4.4287949 -4.4287748 -4.4287734 -4.4287829][-4.4289384 -4.4289389 -4.428916 -4.4288583 -4.4287682 -4.4286795 -4.42862 -4.4286261 -4.428709 -4.42878 -4.4287968 -4.4287868 -4.4287605 -4.428761 -4.4287767][-4.4289007 -4.4289088 -4.4288988 -4.4288383 -4.428731 -4.428606 -4.4285054 -4.4284983 -4.4286089 -4.4287176 -4.4287572 -4.4287472 -4.4287138 -4.4287205 -4.4287543][-4.4288445 -4.4288568 -4.4288516 -4.4287853 -4.4286542 -4.4284816 -4.4283214 -4.4282937 -4.4284372 -4.4285951 -4.4286685 -4.4286695 -4.4286413 -4.4286556 -4.4287109][-4.4287281 -4.4287448 -4.4287448 -4.4286814 -4.4285436 -4.4283452 -4.4281397 -4.4280825 -4.42825 -4.4284477 -4.4285483 -4.4285636 -4.428546 -4.4285712 -4.4286451][-4.4286075 -4.4286294 -4.4286356 -4.4285822 -4.4284639 -4.4282932 -4.4281082 -4.4280367 -4.4281754 -4.4283552 -4.4284515 -4.4284616 -4.4284434 -4.4284782 -4.4285669][-4.428597 -4.4286208 -4.4286323 -4.4285922 -4.4285111 -4.4284081 -4.4282994 -4.4282393 -4.428297 -4.4283962 -4.4284511 -4.4284387 -4.4284148 -4.4284487 -4.428535][-4.4286556 -4.4286737 -4.4286852 -4.4286585 -4.4286103 -4.4285626 -4.4285145 -4.4284706 -4.4284706 -4.428503 -4.4285212 -4.4284897 -4.4284596 -4.4284887 -4.4285674][-4.4287324 -4.428741 -4.42875 -4.4287343 -4.4287066 -4.4286866 -4.4286675 -4.4286375 -4.428616 -4.4286132 -4.4286108 -4.428576 -4.4285531 -4.4285793 -4.428647][-4.4287891 -4.4287934 -4.4287992 -4.4287896 -4.4287763 -4.4287686 -4.428761 -4.4287434 -4.4287224 -4.4287066 -4.4286942 -4.428669 -4.4286537 -4.4286742 -4.4287262][-4.4288292 -4.4288359 -4.4288411 -4.4288359 -4.4288292 -4.4288249 -4.4288197 -4.4288087 -4.4287925 -4.4287786 -4.4287667 -4.4287491 -4.42874 -4.4287515 -4.4287891][-4.4289036 -4.4289155 -4.4289203 -4.4289169 -4.4289112 -4.4289064 -4.4289026 -4.4288955 -4.4288855 -4.4288759 -4.4288673 -4.4288545 -4.4288468 -4.4288492 -4.4288712][-4.4289536 -4.4289651 -4.4289689 -4.4289675 -4.4289627 -4.4289584 -4.4289551 -4.4289508 -4.4289446 -4.4289393 -4.4289346 -4.4289274 -4.4289217 -4.4289227 -4.4289351]]...]
INFO - root - 2017-12-08 09:05:45.499311: step 66910, loss = 2.28, batch loss = 2.23 (34.1 examples/sec; 0.234 sec/batch; 17h:17m:56s remains)
INFO - root - 2017-12-08 09:05:47.699392: step 66920, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.221 sec/batch; 16h:16m:45s remains)
INFO - root - 2017-12-08 09:05:49.933959: step 66930, loss = 2.28, batch loss = 2.23 (33.5 examples/sec; 0.239 sec/batch; 17h:35m:45s remains)
INFO - root - 2017-12-08 09:05:52.162432: step 66940, loss = 2.28, batch loss = 2.23 (36.8 examples/sec; 0.217 sec/batch; 16h:02m:24s remains)
INFO - root - 2017-12-08 09:05:54.418369: step 66950, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.220 sec/batch; 16h:15m:25s remains)
INFO - root - 2017-12-08 09:05:56.666489: step 66960, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.221 sec/batch; 16h:16m:28s remains)
INFO - root - 2017-12-08 09:05:58.888314: step 66970, loss = 2.28, batch loss = 2.23 (34.9 examples/sec; 0.230 sec/batch; 16h:55m:40s remains)
INFO - root - 2017-12-08 09:06:01.109155: step 66980, loss = 2.28, batch loss = 2.23 (35.8 examples/sec; 0.223 sec/batch; 16h:27m:53s remains)
INFO - root - 2017-12-08 09:06:03.363772: step 66990, loss = 2.28, batch loss = 2.23 (34.1 examples/sec; 0.235 sec/batch; 17h:18m:46s remains)
INFO - root - 2017-12-08 09:06:05.605739: step 67000, loss = 2.28, batch loss = 2.23 (35.8 examples/sec; 0.223 sec/batch; 16h:28m:17s remains)
2017-12-08 09:06:05.883349: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289656 -4.4289541 -4.4289412 -4.4289379 -4.4289451 -4.4289441 -4.4289336 -4.4289193 -4.4288979 -4.4288769 -4.4288673 -4.4288645 -4.4288664 -4.4288745 -4.4288831][-4.428987 -4.42898 -4.4289651 -4.4289522 -4.4289451 -4.4289355 -4.4289217 -4.4289103 -4.4288912 -4.4288707 -4.4288616 -4.4288592 -4.4288659 -4.4288769 -4.4288878][-4.4289894 -4.42898 -4.4289622 -4.4289408 -4.4289236 -4.4289069 -4.4288931 -4.4288869 -4.4288783 -4.4288688 -4.4288669 -4.4288673 -4.4288735 -4.4288797 -4.4288807][-4.4289732 -4.4289517 -4.4289236 -4.4288917 -4.4288621 -4.4288406 -4.4288306 -4.4288311 -4.4288349 -4.4288397 -4.42885 -4.4288573 -4.4288597 -4.4288559 -4.4288435][-4.4289527 -4.4289188 -4.428875 -4.4288282 -4.4287877 -4.4287643 -4.42876 -4.4287691 -4.4287825 -4.4287939 -4.4288135 -4.4288292 -4.4288321 -4.4288225 -4.4288025][-4.4289384 -4.4288926 -4.4288316 -4.42877 -4.428721 -4.4286985 -4.4287024 -4.4287219 -4.4287419 -4.4287558 -4.428781 -4.42881 -4.428823 -4.4288197 -4.4288034][-4.4289269 -4.4288664 -4.4287848 -4.4287047 -4.4286418 -4.4286165 -4.42863 -4.4286652 -4.4287 -4.4287233 -4.4287591 -4.4288006 -4.4288282 -4.4288335 -4.428823][-4.4289188 -4.4288425 -4.4287381 -4.4286346 -4.4285545 -4.4285197 -4.42854 -4.4285932 -4.428647 -4.4286857 -4.42873 -4.4287782 -4.4288135 -4.4288278 -4.4288287][-4.4289103 -4.428822 -4.4287028 -4.428587 -4.4284949 -4.4284506 -4.4284725 -4.4285369 -4.4286036 -4.4286556 -4.4287019 -4.4287443 -4.4287748 -4.428792 -4.4288][-4.4289026 -4.4288125 -4.4286942 -4.4285851 -4.4284983 -4.4284534 -4.4284711 -4.4285283 -4.4285903 -4.4286366 -4.4286733 -4.4287057 -4.4287314 -4.4287448 -4.4287539][-4.4289007 -4.4288187 -4.4287171 -4.4286361 -4.428576 -4.4285412 -4.4285512 -4.4285841 -4.4286165 -4.4286356 -4.4286556 -4.4286804 -4.4287 -4.4287109 -4.4287219][-4.4289007 -4.4288282 -4.4287481 -4.4286962 -4.4286656 -4.4286447 -4.4286504 -4.4286613 -4.4286594 -4.4286489 -4.4286494 -4.4286618 -4.4286752 -4.4286876 -4.4287033][-4.4289093 -4.4288478 -4.4287858 -4.4287548 -4.4287453 -4.4287395 -4.4287467 -4.4287486 -4.4287262 -4.4286947 -4.4286737 -4.4286695 -4.4286742 -4.4286852 -4.4287062][-4.4289274 -4.42888 -4.4288354 -4.4288192 -4.428823 -4.4288311 -4.4288421 -4.4288411 -4.4288139 -4.4287791 -4.4287519 -4.42874 -4.4287372 -4.4287405 -4.4287572][-4.428946 -4.4289117 -4.4288836 -4.4288769 -4.4288864 -4.4289002 -4.4289141 -4.4289184 -4.4289002 -4.4288783 -4.4288592 -4.4288464 -4.4288373 -4.428833 -4.42884]]...]
INFO - root - 2017-12-08 09:06:08.133969: step 67010, loss = 2.28, batch loss = 2.23 (35.4 examples/sec; 0.226 sec/batch; 16h:40m:17s remains)
INFO - root - 2017-12-08 09:06:10.388924: step 67020, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 16h:11m:55s remains)
INFO - root - 2017-12-08 09:06:12.654011: step 67030, loss = 2.28, batch loss = 2.23 (35.1 examples/sec; 0.228 sec/batch; 16h:48m:06s remains)
INFO - root - 2017-12-08 09:06:14.878562: step 67040, loss = 2.28, batch loss = 2.23 (35.7 examples/sec; 0.224 sec/batch; 16h:31m:11s remains)
INFO - root - 2017-12-08 09:06:17.108248: step 67050, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.220 sec/batch; 16h:15m:28s remains)
INFO - root - 2017-12-08 09:06:19.330485: step 67060, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 16h:25m:19s remains)
INFO - root - 2017-12-08 09:06:21.580523: step 67070, loss = 2.28, batch loss = 2.23 (35.1 examples/sec; 0.228 sec/batch; 16h:46m:52s remains)
INFO - root - 2017-12-08 09:06:23.814381: step 67080, loss = 2.28, batch loss = 2.23 (35.2 examples/sec; 0.228 sec/batch; 16h:46m:29s remains)
INFO - root - 2017-12-08 09:06:26.045181: step 67090, loss = 2.28, batch loss = 2.23 (35.6 examples/sec; 0.224 sec/batch; 16h:32m:56s remains)
INFO - root - 2017-12-08 09:06:28.276992: step 67100, loss = 2.28, batch loss = 2.23 (36.0 examples/sec; 0.222 sec/batch; 16h:23m:05s remains)
2017-12-08 09:06:28.557740: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.428771 -4.4287758 -4.428762 -4.4287577 -4.4287615 -4.42875 -4.4287338 -4.4287391 -4.4287562 -4.428771 -4.428762 -4.4287314 -4.4286938 -4.4286661 -4.4286451][-4.4287944 -4.4287987 -4.4287853 -4.428771 -4.4287558 -4.4287229 -4.4286923 -4.4286923 -4.4287176 -4.4287419 -4.4287443 -4.4287229 -4.42869 -4.42866 -4.4286284][-4.428792 -4.4287939 -4.4287815 -4.4287605 -4.4287286 -4.4286852 -4.4286504 -4.4286509 -4.4286838 -4.4287167 -4.4287271 -4.4287119 -4.4286814 -4.4286456 -4.4286][-4.4287724 -4.4287663 -4.4287539 -4.4287314 -4.4286966 -4.4286561 -4.4286275 -4.428628 -4.428659 -4.4286933 -4.4287057 -4.4286904 -4.4286594 -4.4286165 -4.4285541][-4.4287434 -4.4287305 -4.4287195 -4.4287 -4.4286656 -4.4286261 -4.4285936 -4.4285889 -4.4286203 -4.4286633 -4.4286804 -4.4286695 -4.4286427 -4.4286 -4.4285326][-4.4287133 -4.4286971 -4.4286909 -4.4286757 -4.4286375 -4.4285812 -4.4285231 -4.4285097 -4.4285607 -4.4286308 -4.4286633 -4.4286532 -4.428628 -4.4285965 -4.4285536][-4.4286914 -4.4286718 -4.4286704 -4.4286571 -4.4286113 -4.4285336 -4.4284506 -4.4284382 -4.4285231 -4.4286251 -4.4286661 -4.428648 -4.4286165 -4.4286013 -4.4285908][-4.4286742 -4.42865 -4.428648 -4.4286408 -4.4286 -4.4285245 -4.4284492 -4.4284554 -4.4285507 -4.4286513 -4.4286852 -4.4286556 -4.4286156 -4.428606 -4.42861][-4.4286547 -4.4286261 -4.4286265 -4.4286356 -4.428618 -4.4285769 -4.428545 -4.428565 -4.4286227 -4.4286828 -4.4287043 -4.4286733 -4.4286289 -4.42861 -4.4286084][-4.4286489 -4.42862 -4.4286265 -4.428648 -4.4286561 -4.4286504 -4.4286466 -4.4286523 -4.4286613 -4.428679 -4.4286962 -4.4286771 -4.4286423 -4.4286218 -4.4286189][-4.4286795 -4.4286633 -4.4286709 -4.428688 -4.4287047 -4.4287081 -4.4287019 -4.4286833 -4.4286566 -4.42865 -4.428669 -4.4286695 -4.4286551 -4.4286451 -4.4286447][-4.4287252 -4.4287214 -4.4287271 -4.4287372 -4.4287496 -4.4287438 -4.4287224 -4.4286852 -4.4286404 -4.4286251 -4.4286442 -4.4286571 -4.4286566 -4.4286585 -4.428658][-4.4287467 -4.4287496 -4.4287539 -4.4287672 -4.4287815 -4.4287696 -4.4287357 -4.4286942 -4.4286489 -4.4286237 -4.4286289 -4.4286389 -4.4286361 -4.42864 -4.4286461][-4.4287233 -4.4287286 -4.4287367 -4.4287572 -4.4287782 -4.4287691 -4.4287391 -4.4287143 -4.4286814 -4.4286489 -4.4286308 -4.4286304 -4.4286189 -4.4286203 -4.4286323][-4.4287043 -4.428709 -4.4287119 -4.4287248 -4.4287453 -4.4287457 -4.4287338 -4.4287281 -4.4287105 -4.4286776 -4.4286432 -4.4286385 -4.4286346 -4.4286447 -4.4286613]]...]
INFO - root - 2017-12-08 09:06:30.790403: step 67110, loss = 2.28, batch loss = 2.23 (35.7 examples/sec; 0.224 sec/batch; 16h:30m:27s remains)
INFO - root - 2017-12-08 09:06:33.018331: step 67120, loss = 2.28, batch loss = 2.23 (36.7 examples/sec; 0.218 sec/batch; 16h:03m:28s remains)
INFO - root - 2017-12-08 09:06:35.266029: step 67130, loss = 2.28, batch loss = 2.23 (35.3 examples/sec; 0.227 sec/batch; 16h:42m:50s remains)
INFO - root - 2017-12-08 09:06:37.506587: step 67140, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.221 sec/batch; 16h:19m:28s remains)
INFO - root - 2017-12-08 09:06:39.753555: step 67150, loss = 2.28, batch loss = 2.23 (35.7 examples/sec; 0.224 sec/batch; 16h:30m:45s remains)
INFO - root - 2017-12-08 09:06:42.038888: step 67160, loss = 2.28, batch loss = 2.23 (34.0 examples/sec; 0.235 sec/batch; 17h:19m:33s remains)
INFO - root - 2017-12-08 09:06:44.260804: step 67170, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.221 sec/batch; 16h:15m:19s remains)
INFO - root - 2017-12-08 09:06:46.513729: step 67180, loss = 2.28, batch loss = 2.23 (36.6 examples/sec; 0.219 sec/batch; 16h:06m:44s remains)
INFO - root - 2017-12-08 09:06:48.752887: step 67190, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.221 sec/batch; 16h:15m:43s remains)
INFO - root - 2017-12-08 09:06:50.984104: step 67200, loss = 2.28, batch loss = 2.23 (35.6 examples/sec; 0.225 sec/batch; 16h:33m:03s remains)
2017-12-08 09:06:51.284107: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288158 -4.4288149 -4.4288154 -4.4288182 -4.4288249 -4.4288249 -4.4288092 -4.4287825 -4.4287572 -4.4287438 -4.4287496 -4.4287729 -4.428803 -4.4288292 -4.4288454][-4.428823 -4.4288206 -4.4288187 -4.4288173 -4.4288144 -4.4288015 -4.4287696 -4.4287271 -4.4286919 -4.4286785 -4.4286971 -4.4287453 -4.4287996 -4.4288406 -4.428863][-4.4288306 -4.4288244 -4.42882 -4.4288125 -4.4287958 -4.42876 -4.4287009 -4.4286323 -4.4285803 -4.4285679 -4.4286118 -4.4286966 -4.4287815 -4.42884 -4.4288716][-4.4288425 -4.4288316 -4.4288216 -4.4288025 -4.428761 -4.4286919 -4.4285941 -4.4284883 -4.4284134 -4.4284086 -4.4284945 -4.428627 -4.4287434 -4.4288182 -4.4288592][-4.4288487 -4.4288359 -4.4288168 -4.4287748 -4.4286995 -4.4285932 -4.428452 -4.4283009 -4.4281988 -4.4282136 -4.4283557 -4.4285412 -4.4286871 -4.4287748 -4.4288244][-4.4288497 -4.4288373 -4.4288058 -4.4287415 -4.42864 -4.4285073 -4.4283333 -4.4281516 -4.4280372 -4.4280748 -4.4282646 -4.4284859 -4.4286447 -4.4287353 -4.4287887][-4.4288282 -4.428812 -4.4287663 -4.4286885 -4.4285841 -4.4284515 -4.4282804 -4.428112 -4.428019 -4.4280734 -4.4282651 -4.4284739 -4.4286156 -4.4286923 -4.4287453][-4.4287791 -4.4287572 -4.4287038 -4.4286289 -4.4285455 -4.428442 -4.4283075 -4.4281888 -4.4281306 -4.4281807 -4.4283357 -4.4285 -4.4286032 -4.428659 -4.428709][-4.4287081 -4.4286852 -4.428638 -4.4285889 -4.4285469 -4.428493 -4.4284139 -4.4283452 -4.4283075 -4.4283428 -4.4284568 -4.4285741 -4.4286447 -4.4286828 -4.4287243][-4.4286675 -4.4286532 -4.4286323 -4.4286232 -4.4286218 -4.4286079 -4.4285707 -4.42853 -4.428503 -4.4285235 -4.4285994 -4.4286795 -4.4287295 -4.4287581 -4.4287868][-4.4287066 -4.4287019 -4.4287043 -4.4287229 -4.4287381 -4.4287395 -4.4287252 -4.4287052 -4.4286876 -4.4286909 -4.4287271 -4.4287724 -4.4288068 -4.4288311 -4.42885][-4.4287558 -4.4287562 -4.4287744 -4.4288049 -4.4288249 -4.4288335 -4.4288378 -4.4288335 -4.4288211 -4.428812 -4.4288173 -4.4288368 -4.4288607 -4.4288831 -4.428896][-4.4287586 -4.4287615 -4.4287863 -4.4288244 -4.4288573 -4.4288812 -4.4289055 -4.4289126 -4.4288988 -4.4288769 -4.4288607 -4.4288607 -4.428875 -4.428894 -4.4289026][-4.4287262 -4.4287252 -4.4287524 -4.4287972 -4.428844 -4.4288821 -4.4289165 -4.4289241 -4.4289045 -4.4288764 -4.4288521 -4.4288411 -4.4288478 -4.4288573 -4.428853][-4.4287062 -4.4286981 -4.4287248 -4.4287744 -4.4288259 -4.4288626 -4.4288831 -4.4288745 -4.4288449 -4.4288173 -4.4287992 -4.428792 -4.428792 -4.4287825 -4.4287553]]...]
INFO - root - 2017-12-08 09:06:53.518041: step 67210, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 16h:26m:04s remains)
INFO - root - 2017-12-08 09:06:55.764991: step 67220, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.225 sec/batch; 16h:35m:21s remains)
INFO - root - 2017-12-08 09:06:57.996714: step 67230, loss = 2.28, batch loss = 2.23 (34.5 examples/sec; 0.232 sec/batch; 17h:05m:56s remains)
INFO - root - 2017-12-08 09:07:00.252520: step 67240, loss = 2.28, batch loss = 2.23 (34.7 examples/sec; 0.230 sec/batch; 16h:57m:49s remains)
INFO - root - 2017-12-08 09:07:02.479021: step 67250, loss = 2.28, batch loss = 2.23 (36.7 examples/sec; 0.218 sec/batch; 16h:04m:46s remains)
INFO - root - 2017-12-08 09:07:04.712867: step 67260, loss = 2.28, batch loss = 2.23 (36.6 examples/sec; 0.218 sec/batch; 16h:05m:13s remains)
INFO - root - 2017-12-08 09:07:06.939085: step 67270, loss = 2.28, batch loss = 2.23 (36.8 examples/sec; 0.217 sec/batch; 16h:00m:16s remains)
INFO - root - 2017-12-08 09:07:09.162898: step 67280, loss = 2.28, batch loss = 2.23 (35.6 examples/sec; 0.224 sec/batch; 16h:31m:59s remains)
INFO - root - 2017-12-08 09:07:11.416298: step 67290, loss = 2.28, batch loss = 2.23 (34.6 examples/sec; 0.231 sec/batch; 17h:00m:56s remains)
INFO - root - 2017-12-08 09:07:13.675171: step 67300, loss = 2.28, batch loss = 2.23 (35.8 examples/sec; 0.223 sec/batch; 16h:26m:29s remains)
2017-12-08 09:07:13.979508: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.42883 -4.4288197 -4.42883 -4.4288607 -4.4288893 -4.4288936 -4.4288826 -4.4288645 -4.4288592 -4.4288807 -4.4289036 -4.4289126 -4.42891 -4.4289031 -4.4289231][-4.4288526 -4.4288373 -4.4288464 -4.4288807 -4.4289064 -4.4288883 -4.42885 -4.4288144 -4.4288044 -4.4288297 -4.4288654 -4.4288931 -4.4289 -4.4288993 -4.4289246][-4.42888 -4.4288669 -4.4288726 -4.4289012 -4.4289131 -4.4288678 -4.4287977 -4.4287491 -4.4287438 -4.4287815 -4.4288359 -4.4288831 -4.4288983 -4.4289012 -4.4289279][-4.4289031 -4.428895 -4.4289007 -4.4289193 -4.4289083 -4.4288325 -4.4287319 -4.4286737 -4.4286785 -4.4287305 -4.4288054 -4.4288659 -4.42889 -4.4289012 -4.4289303][-4.4289179 -4.4289126 -4.4289184 -4.428925 -4.4288921 -4.4287834 -4.4286485 -4.428575 -4.4285841 -4.4286509 -4.4287515 -4.4288282 -4.4288669 -4.4288893 -4.428925][-4.4289103 -4.4289026 -4.4289041 -4.4288993 -4.4288483 -4.4287114 -4.4285412 -4.42845 -4.4284644 -4.4285507 -4.42868 -4.4287796 -4.4288363 -4.4288726 -4.4289174][-4.4288774 -4.4288707 -4.4288692 -4.42885 -4.4287806 -4.4286242 -4.4284334 -4.4283361 -4.4283609 -4.4284668 -4.4286208 -4.4287453 -4.4288187 -4.4288669 -4.4289174][-4.4288492 -4.4288487 -4.4288478 -4.4288182 -4.42873 -4.4285655 -4.4283714 -4.4282794 -4.4283156 -4.4284339 -4.4285975 -4.4287357 -4.4288177 -4.4288707 -4.4289207][-4.4288216 -4.4288282 -4.4288387 -4.4288106 -4.4287138 -4.4285545 -4.4283609 -4.428268 -4.428299 -4.4284167 -4.4285865 -4.4287333 -4.4288211 -4.4288759 -4.4289246][-4.4287949 -4.4288073 -4.4288287 -4.4288139 -4.4287305 -4.4285889 -4.428412 -4.4283137 -4.4283223 -4.4284334 -4.428607 -4.4287534 -4.428843 -4.428895 -4.4289351][-4.4287734 -4.4287868 -4.4288125 -4.42881 -4.4287605 -4.4286618 -4.4285264 -4.428441 -4.428443 -4.4285369 -4.4286804 -4.4288034 -4.428884 -4.4289241 -4.4289527][-4.42874 -4.4287415 -4.4287639 -4.4287767 -4.4287667 -4.4287214 -4.4286447 -4.4285975 -4.4286089 -4.4286804 -4.4287777 -4.4288673 -4.4289269 -4.4289503 -4.4289689][-4.4286728 -4.428647 -4.4286566 -4.4286785 -4.4286966 -4.4287062 -4.4287024 -4.4287076 -4.428741 -4.4288039 -4.4288688 -4.4289289 -4.4289622 -4.42897 -4.4289804][-4.4285774 -4.4285192 -4.4285131 -4.4285502 -4.4286 -4.4286585 -4.4287138 -4.4287663 -4.4288239 -4.4288878 -4.42894 -4.4289742 -4.4289823 -4.4289789 -4.4289851][-4.4285111 -4.42844 -4.4284368 -4.4284911 -4.4285617 -4.4286447 -4.4287229 -4.4287972 -4.4288683 -4.4289331 -4.4289756 -4.4289913 -4.4289832 -4.4289718 -4.42898]]...]
INFO - root - 2017-12-08 09:07:16.231714: step 67310, loss = 2.28, batch loss = 2.23 (34.9 examples/sec; 0.229 sec/batch; 16h:53m:00s remains)
INFO - root - 2017-12-08 09:07:18.457536: step 67320, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.225 sec/batch; 16h:35m:01s remains)
INFO - root - 2017-12-08 09:07:20.690101: step 67330, loss = 2.28, batch loss = 2.23 (36.7 examples/sec; 0.218 sec/batch; 16h:03m:13s remains)
INFO - root - 2017-12-08 09:07:22.916243: step 67340, loss = 2.28, batch loss = 2.23 (36.6 examples/sec; 0.219 sec/batch; 16h:06m:26s remains)
INFO - root - 2017-12-08 09:07:25.153732: step 67350, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.220 sec/batch; 16h:13m:34s remains)
INFO - root - 2017-12-08 09:07:27.451319: step 67360, loss = 2.28, batch loss = 2.23 (35.1 examples/sec; 0.228 sec/batch; 16h:48m:12s remains)
INFO - root - 2017-12-08 09:07:29.680372: step 67370, loss = 2.28, batch loss = 2.23 (34.0 examples/sec; 0.236 sec/batch; 17h:20m:59s remains)
INFO - root - 2017-12-08 09:07:31.921123: step 67380, loss = 2.28, batch loss = 2.23 (37.5 examples/sec; 0.213 sec/batch; 15h:42m:48s remains)
INFO - root - 2017-12-08 09:07:34.150799: step 67390, loss = 2.28, batch loss = 2.23 (35.8 examples/sec; 0.223 sec/batch; 16h:26m:38s remains)
INFO - root - 2017-12-08 09:07:36.427377: step 67400, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.225 sec/batch; 16h:34m:36s remains)
2017-12-08 09:07:36.732811: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4286418 -4.4286413 -4.4286489 -4.4286661 -4.4286895 -4.4287014 -4.4286942 -4.4286737 -4.4286594 -4.4286475 -4.428637 -4.4286313 -4.4286342 -4.4286385 -4.4286404][-4.4286327 -4.4286208 -4.4286208 -4.428638 -4.4286661 -4.4286823 -4.428678 -4.4286585 -4.428647 -4.4286337 -4.4286237 -4.42862 -4.4286222 -4.4286218 -4.4286242][-4.4286518 -4.4286418 -4.4286437 -4.4286594 -4.4286785 -4.428688 -4.4286795 -4.4286623 -4.4286523 -4.4286375 -4.4286237 -4.4286165 -4.4286118 -4.4286046 -4.4286051][-4.4286747 -4.428679 -4.4286942 -4.4287143 -4.428721 -4.4287105 -4.4286933 -4.4286804 -4.428668 -4.4286485 -4.4286289 -4.4286151 -4.4285989 -4.4285827 -4.4285841][-4.4286652 -4.4286819 -4.42871 -4.4287343 -4.4287295 -4.428699 -4.4286723 -4.4286623 -4.4286509 -4.4286327 -4.4286122 -4.4285913 -4.4285674 -4.4285522 -4.4285607][-4.4286237 -4.428647 -4.4286809 -4.4287043 -4.4286919 -4.4286466 -4.4286113 -4.4286075 -4.4286008 -4.4285841 -4.4285564 -4.4285264 -4.4285059 -4.4285021 -4.4285221][-4.4285502 -4.428576 -4.4286041 -4.428607 -4.4285746 -4.4285135 -4.4284768 -4.4284835 -4.4284916 -4.4284768 -4.4284348 -4.4283943 -4.4283895 -4.4284048 -4.4284444][-4.4284763 -4.4284992 -4.4285045 -4.428462 -4.4283834 -4.428297 -4.4282622 -4.4282937 -4.4283328 -4.4283361 -4.4282951 -4.4282565 -4.428268 -4.4282894 -4.4283352][-4.4284015 -4.428421 -4.428411 -4.4283309 -4.4282017 -4.428082 -4.4280558 -4.4281311 -4.428225 -4.4282613 -4.4282274 -4.428196 -4.4282122 -4.4282269 -4.4282603][-4.4283738 -4.4283977 -4.4283915 -4.4283142 -4.4281878 -4.4280853 -4.4280877 -4.4281807 -4.4282904 -4.4283376 -4.428308 -4.4282751 -4.4282722 -4.4282694 -4.4282813][-4.4283843 -4.4284172 -4.4284415 -4.4284115 -4.4283462 -4.4282961 -4.42831 -4.4283738 -4.4284539 -4.4284916 -4.4284706 -4.4284368 -4.4284196 -4.428411 -4.4284148][-4.4284616 -4.4284911 -4.428524 -4.4285269 -4.4285126 -4.4285011 -4.4285192 -4.4285579 -4.4286122 -4.4286413 -4.4286294 -4.4286008 -4.42858 -4.4285693 -4.4285684][-4.4285297 -4.4285398 -4.4285641 -4.428575 -4.4285803 -4.4285903 -4.4286141 -4.4286432 -4.428678 -4.4286981 -4.4286966 -4.4286819 -4.42867 -4.4286594 -4.4286528][-4.4285874 -4.4285817 -4.4285936 -4.4285989 -4.4286051 -4.4286189 -4.4286418 -4.4286628 -4.4286866 -4.4287052 -4.4287095 -4.4287038 -4.4286947 -4.4286814 -4.428669][-4.428637 -4.4286251 -4.42863 -4.4286289 -4.4286304 -4.4286442 -4.4286647 -4.4286804 -4.4286947 -4.4287071 -4.4287105 -4.4287071 -4.4286995 -4.4286861 -4.4286761]]...]
INFO - root - 2017-12-08 09:07:38.956344: step 67410, loss = 2.28, batch loss = 2.23 (36.7 examples/sec; 0.218 sec/batch; 16h:03m:37s remains)
INFO - root - 2017-12-08 09:07:41.195387: step 67420, loss = 2.28, batch loss = 2.23 (35.6 examples/sec; 0.224 sec/batch; 16h:31m:26s remains)
INFO - root - 2017-12-08 09:07:43.427548: step 67430, loss = 2.28, batch loss = 2.23 (35.1 examples/sec; 0.228 sec/batch; 16h:46m:09s remains)
INFO - root - 2017-12-08 09:07:45.687339: step 67440, loss = 2.28, batch loss = 2.23 (35.8 examples/sec; 0.224 sec/batch; 16h:28m:09s remains)
INFO - root - 2017-12-08 09:07:47.952736: step 67450, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 16h:07m:08s remains)
INFO - root - 2017-12-08 09:07:50.177876: step 67460, loss = 2.28, batch loss = 2.23 (35.8 examples/sec; 0.223 sec/batch; 16h:27m:11s remains)
INFO - root - 2017-12-08 09:07:52.401511: step 67470, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.221 sec/batch; 16h:17m:59s remains)
INFO - root - 2017-12-08 09:07:54.637185: step 67480, loss = 2.28, batch loss = 2.23 (34.8 examples/sec; 0.230 sec/batch; 16h:55m:03s remains)
INFO - root - 2017-12-08 09:07:56.880620: step 67490, loss = 2.28, batch loss = 2.23 (34.8 examples/sec; 0.230 sec/batch; 16h:56m:39s remains)
INFO - root - 2017-12-08 09:07:59.104469: step 67500, loss = 2.28, batch loss = 2.23 (35.8 examples/sec; 0.224 sec/batch; 16h:27m:35s remains)
2017-12-08 09:07:59.419282: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289327 -4.4288898 -4.4288659 -4.4288545 -4.4288197 -4.4287539 -4.4286895 -4.4286556 -4.4287 -4.4287982 -4.4288893 -4.4289532 -4.428987 -4.4289889 -4.4289775][-4.4289346 -4.4288931 -4.428874 -4.4288683 -4.4288387 -4.4287624 -4.4286656 -4.4285936 -4.428618 -4.4287267 -4.4288425 -4.4289312 -4.4289846 -4.4289956 -4.4289842][-4.4289341 -4.4288936 -4.42888 -4.4288807 -4.4288559 -4.4287744 -4.4286523 -4.4285426 -4.4285426 -4.4286547 -4.4287872 -4.4289012 -4.4289765 -4.428998 -4.4289875][-4.4289365 -4.4288964 -4.4288855 -4.42889 -4.4288697 -4.4287934 -4.4286623 -4.4285283 -4.4285064 -4.428606 -4.4287376 -4.4288669 -4.4289613 -4.4289956 -4.4289908][-4.4289432 -4.4289021 -4.4288869 -4.4288917 -4.4288783 -4.4288182 -4.4286957 -4.4285574 -4.4285169 -4.4285903 -4.4287081 -4.4288373 -4.4289432 -4.4289918 -4.4289951][-4.4289565 -4.4289136 -4.4288898 -4.4288921 -4.4288855 -4.428843 -4.4287405 -4.4286118 -4.4285593 -4.428606 -4.4287028 -4.4288192 -4.4289269 -4.4289865 -4.4289989][-4.4289718 -4.4289284 -4.4288993 -4.4288964 -4.4288945 -4.4288692 -4.4287896 -4.4286733 -4.4286094 -4.4286337 -4.428709 -4.4288077 -4.4289093 -4.4289751 -4.4289956][-4.4289846 -4.4289432 -4.4289103 -4.4289021 -4.4289055 -4.4288936 -4.4288316 -4.4287252 -4.4286466 -4.4286518 -4.4287128 -4.4287987 -4.4288926 -4.4289613 -4.4289856][-4.4289904 -4.4289522 -4.4289179 -4.4289103 -4.4289207 -4.42892 -4.4288673 -4.4287572 -4.4286566 -4.4286413 -4.4286904 -4.4287696 -4.4288573 -4.4289227 -4.4289494][-4.4289827 -4.42895 -4.428916 -4.42891 -4.428925 -4.4289289 -4.4288836 -4.4287724 -4.4286542 -4.428616 -4.4286494 -4.4287214 -4.4287996 -4.4288573 -4.428884][-4.4289556 -4.4289246 -4.4288912 -4.428884 -4.4288988 -4.4289064 -4.4288707 -4.4287658 -4.4286432 -4.4285846 -4.4285984 -4.4286561 -4.4287186 -4.4287648 -4.4287858][-4.4289227 -4.4288898 -4.4288635 -4.428863 -4.4288759 -4.42888 -4.4288521 -4.4287667 -4.4286628 -4.4285965 -4.4285965 -4.428638 -4.4286757 -4.4287 -4.4287052][-4.4288855 -4.4288445 -4.4288225 -4.4288306 -4.4288416 -4.4288368 -4.4288163 -4.4287629 -4.4286923 -4.4286394 -4.4286389 -4.4286737 -4.4286904 -4.42869 -4.4286766][-4.4288545 -4.4288054 -4.428791 -4.4288087 -4.4288225 -4.4288139 -4.4288025 -4.4287748 -4.4287353 -4.4287014 -4.4287081 -4.4287419 -4.428751 -4.4287386 -4.4287171][-4.4288378 -4.4287863 -4.428781 -4.4288139 -4.4288416 -4.4288454 -4.4288454 -4.4288335 -4.4288068 -4.4287829 -4.428791 -4.4288177 -4.4288225 -4.4288063 -4.4287844]]...]
INFO - root - 2017-12-08 09:08:01.641097: step 67510, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.225 sec/batch; 16h:34m:32s remains)
INFO - root - 2017-12-08 09:08:03.906397: step 67520, loss = 2.28, batch loss = 2.23 (35.3 examples/sec; 0.227 sec/batch; 16h:41m:17s remains)
INFO - root - 2017-12-08 09:08:06.167445: step 67530, loss = 2.28, batch loss = 2.23 (35.6 examples/sec; 0.225 sec/batch; 16h:32m:53s remains)
INFO - root - 2017-12-08 09:08:08.427995: step 67540, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.222 sec/batch; 16h:19m:29s remains)
INFO - root - 2017-12-08 09:08:10.670460: step 67550, loss = 2.28, batch loss = 2.23 (33.7 examples/sec; 0.237 sec/batch; 17h:28m:10s remains)
INFO - root - 2017-12-08 09:08:12.916059: step 67560, loss = 2.28, batch loss = 2.23 (36.0 examples/sec; 0.222 sec/batch; 16h:22m:14s remains)
INFO - root - 2017-12-08 09:08:15.138210: step 67570, loss = 2.28, batch loss = 2.23 (37.5 examples/sec; 0.213 sec/batch; 15h:40m:47s remains)
INFO - root - 2017-12-08 09:08:17.365855: step 67580, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.222 sec/batch; 16h:19m:11s remains)
INFO - root - 2017-12-08 09:08:19.604700: step 67590, loss = 2.28, batch loss = 2.23 (36.6 examples/sec; 0.219 sec/batch; 16h:06m:13s remains)
INFO - root - 2017-12-08 09:08:21.834782: step 67600, loss = 2.28, batch loss = 2.23 (36.7 examples/sec; 0.218 sec/batch; 16h:01m:35s remains)
2017-12-08 09:08:22.132705: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288721 -4.4288349 -4.4287806 -4.4287047 -4.4286437 -4.4286318 -4.4286585 -4.4286981 -4.4287357 -4.4287548 -4.4287634 -4.4287663 -4.4287786 -4.4288116 -4.4288335][-4.4288826 -4.428844 -4.4287906 -4.428709 -4.4286432 -4.4286313 -4.4286618 -4.4286942 -4.4287081 -4.4287176 -4.4287362 -4.4287553 -4.4287829 -4.4288225 -4.4288483][-4.4288974 -4.4288635 -4.4288096 -4.4287214 -4.4286437 -4.4286151 -4.4286318 -4.4286427 -4.4286337 -4.4286489 -4.428689 -4.4287386 -4.4287839 -4.4288244 -4.4288521][-4.4288926 -4.4288635 -4.4288092 -4.4287176 -4.4286323 -4.4285903 -4.4285831 -4.42856 -4.428524 -4.4285426 -4.4286189 -4.428709 -4.4287758 -4.4288216 -4.4288478][-4.4288931 -4.4288716 -4.4288249 -4.4287477 -4.4286761 -4.4286361 -4.4286051 -4.4285374 -4.4284558 -4.4284639 -4.4285712 -4.4286938 -4.4287734 -4.4288139 -4.4288325][-4.428905 -4.428896 -4.4288645 -4.4288015 -4.4287443 -4.4287038 -4.428647 -4.4285359 -4.4283967 -4.4283905 -4.4285264 -4.4286737 -4.4287562 -4.4287925 -4.4288092][-4.4289002 -4.428896 -4.4288721 -4.4288187 -4.4287667 -4.4287095 -4.4286165 -4.42847 -4.42829 -4.428287 -4.4284539 -4.428617 -4.4287133 -4.4287586 -4.4287834][-4.4288888 -4.4288821 -4.4288564 -4.4288116 -4.4287615 -4.4286876 -4.4285688 -4.4284105 -4.4282231 -4.428226 -4.4284062 -4.4285722 -4.4286752 -4.4287291 -4.4287624][-4.4288859 -4.4288793 -4.4288578 -4.4288282 -4.4287963 -4.4287329 -4.4286232 -4.4284782 -4.4283152 -4.4283147 -4.4284611 -4.4286051 -4.4286871 -4.4287276 -4.4287539][-4.4288964 -4.4288931 -4.42888 -4.4288611 -4.4288445 -4.4288087 -4.4287281 -4.4286013 -4.4284563 -4.4284458 -4.42855 -4.4286628 -4.4287286 -4.4287529 -4.4287667][-4.4289012 -4.4289031 -4.4288983 -4.4288826 -4.4288683 -4.4288516 -4.4287963 -4.4286885 -4.4285655 -4.4285493 -4.4286256 -4.4287176 -4.4287753 -4.428792 -4.4287949][-4.4288945 -4.4289026 -4.4289031 -4.4288917 -4.4288754 -4.428863 -4.4288192 -4.4287233 -4.4286194 -4.428607 -4.4286685 -4.4287491 -4.428803 -4.4288154 -4.4288096][-4.428875 -4.4288883 -4.4288917 -4.4288816 -4.4288678 -4.4288607 -4.4288268 -4.4287438 -4.428659 -4.4286475 -4.4286914 -4.4287534 -4.4288025 -4.428822 -4.4288163][-4.4288611 -4.4288735 -4.4288764 -4.428865 -4.4288559 -4.4288635 -4.4288497 -4.428792 -4.4287252 -4.4287028 -4.4287186 -4.4287515 -4.428791 -4.4288192 -4.4288235][-4.4288659 -4.4288721 -4.42887 -4.4288588 -4.4288492 -4.4288616 -4.4288669 -4.4288387 -4.4287906 -4.4287567 -4.4287477 -4.4287534 -4.4287744 -4.4287992 -4.4288139]]...]
INFO - root - 2017-12-08 09:08:24.363821: step 67610, loss = 2.28, batch loss = 2.23 (37.0 examples/sec; 0.216 sec/batch; 15h:53m:43s remains)
INFO - root - 2017-12-08 09:08:26.657983: step 67620, loss = 2.28, batch loss = 2.23 (35.6 examples/sec; 0.225 sec/batch; 16h:31m:40s remains)
INFO - root - 2017-12-08 09:08:28.916862: step 67630, loss = 2.28, batch loss = 2.23 (34.2 examples/sec; 0.234 sec/batch; 17h:11m:58s remains)
INFO - root - 2017-12-08 09:08:31.176471: step 67640, loss = 2.28, batch loss = 2.23 (36.6 examples/sec; 0.219 sec/batch; 16h:05m:53s remains)
INFO - root - 2017-12-08 09:08:33.394255: step 67650, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 16h:10m:12s remains)
INFO - root - 2017-12-08 09:08:35.655684: step 67660, loss = 2.28, batch loss = 2.23 (34.9 examples/sec; 0.230 sec/batch; 16h:53m:13s remains)
INFO - root - 2017-12-08 09:08:37.896213: step 67670, loss = 2.28, batch loss = 2.23 (35.3 examples/sec; 0.227 sec/batch; 16h:40m:20s remains)
INFO - root - 2017-12-08 09:08:40.155677: step 67680, loss = 2.28, batch loss = 2.23 (32.4 examples/sec; 0.247 sec/batch; 18h:11m:15s remains)
INFO - root - 2017-12-08 09:08:42.382458: step 67690, loss = 2.28, batch loss = 2.23 (36.6 examples/sec; 0.218 sec/batch; 16h:04m:10s remains)
INFO - root - 2017-12-08 09:08:44.609393: step 67700, loss = 2.28, batch loss = 2.23 (37.2 examples/sec; 0.215 sec/batch; 15h:47m:50s remains)
2017-12-08 09:08:44.895620: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.42885 -4.4288635 -4.4288478 -4.4288321 -4.4288287 -4.4288282 -4.4288077 -4.4287705 -4.4287448 -4.4287462 -4.4287648 -4.4287863 -4.4287977 -4.4287939 -4.4287663][-4.428875 -4.4288859 -4.4288716 -4.4288583 -4.4288588 -4.4288621 -4.4288459 -4.4288116 -4.4287939 -4.4287968 -4.4288082 -4.4288187 -4.4288163 -4.4288 -4.4287634][-4.4288759 -4.4288878 -4.4288697 -4.4288559 -4.4288616 -4.42888 -4.4288816 -4.428865 -4.4288635 -4.4288697 -4.4288697 -4.428863 -4.4288378 -4.428802 -4.4287653][-4.4288549 -4.4288654 -4.4288392 -4.4288239 -4.428833 -4.4288554 -4.4288659 -4.4288673 -4.4288836 -4.4288964 -4.4288955 -4.4288797 -4.4288368 -4.428791 -4.4287634][-4.4288278 -4.4288378 -4.4288087 -4.4287887 -4.4287868 -4.4287925 -4.4287963 -4.4287968 -4.4288177 -4.4288421 -4.4288564 -4.428853 -4.4288182 -4.4287844 -4.4287667][-4.4288068 -4.428813 -4.4287872 -4.4287629 -4.4287391 -4.4287195 -4.4287028 -4.4286761 -4.4286757 -4.4287138 -4.4287615 -4.4287934 -4.4287939 -4.4287872 -4.4287796][-4.4288149 -4.428813 -4.4287853 -4.4287395 -4.428678 -4.4286222 -4.4285688 -4.4284945 -4.4284568 -4.4285178 -4.4286213 -4.4286995 -4.4287443 -4.4287744 -4.4287868][-4.4288464 -4.4288368 -4.4287977 -4.4287262 -4.4286413 -4.4285603 -4.4284763 -4.4283586 -4.4282765 -4.42835 -4.4284973 -4.4286051 -4.4286842 -4.4287429 -4.4287782][-4.4288716 -4.4288573 -4.4288034 -4.4287224 -4.4286447 -4.4285755 -4.4285054 -4.4284086 -4.4283366 -4.4283948 -4.4285083 -4.4285946 -4.4286704 -4.4287286 -4.4287667][-4.4288731 -4.4288573 -4.4287944 -4.4287176 -4.4286637 -4.42862 -4.4285846 -4.4285407 -4.4285049 -4.4285445 -4.4286103 -4.4286642 -4.428719 -4.4287596 -4.4287868][-4.4288521 -4.4288397 -4.4287882 -4.4287329 -4.4286981 -4.4286671 -4.4286547 -4.4286485 -4.4286427 -4.4286737 -4.4287181 -4.4287562 -4.4287915 -4.428813 -4.4288273][-4.42882 -4.4288149 -4.4287939 -4.4287686 -4.4287534 -4.4287353 -4.4287424 -4.4287586 -4.428771 -4.4287949 -4.428822 -4.4288435 -4.4288616 -4.4288683 -4.4288735][-4.4288011 -4.4288068 -4.4288177 -4.4288192 -4.4288211 -4.4288168 -4.4288363 -4.4288573 -4.42887 -4.4288816 -4.4288955 -4.4289021 -4.428906 -4.4289041 -4.4289041][-4.428792 -4.4288054 -4.4288335 -4.4288497 -4.4288554 -4.4288592 -4.4288878 -4.4289112 -4.4289217 -4.428925 -4.4289289 -4.4289284 -4.428926 -4.4289217 -4.4289188][-4.4287748 -4.4287896 -4.4288254 -4.4288483 -4.4288564 -4.4288716 -4.4289107 -4.4289412 -4.4289536 -4.4289517 -4.428947 -4.4289432 -4.42894 -4.4289336 -4.4289284]]...]
INFO - root - 2017-12-08 09:08:47.108363: step 67710, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.221 sec/batch; 16h:13m:40s remains)
INFO - root - 2017-12-08 09:08:49.326853: step 67720, loss = 2.28, batch loss = 2.23 (36.6 examples/sec; 0.219 sec/batch; 16h:05m:41s remains)
INFO - root - 2017-12-08 09:08:51.555807: step 67730, loss = 2.28, batch loss = 2.23 (36.9 examples/sec; 0.217 sec/batch; 15h:57m:59s remains)
INFO - root - 2017-12-08 09:08:53.804729: step 67740, loss = 2.28, batch loss = 2.23 (37.1 examples/sec; 0.216 sec/batch; 15h:52m:36s remains)
INFO - root - 2017-12-08 09:08:56.056517: step 67750, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.220 sec/batch; 16h:12m:21s remains)
INFO - root - 2017-12-08 09:08:58.280991: step 67760, loss = 2.28, batch loss = 2.23 (34.5 examples/sec; 0.232 sec/batch; 17h:03m:55s remains)
INFO - root - 2017-12-08 09:09:00.547165: step 67770, loss = 2.28, batch loss = 2.23 (34.7 examples/sec; 0.230 sec/batch; 16h:56m:02s remains)
INFO - root - 2017-12-08 09:09:02.768537: step 67780, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 16h:23m:06s remains)
INFO - root - 2017-12-08 09:09:05.000686: step 67790, loss = 2.28, batch loss = 2.23 (37.4 examples/sec; 0.214 sec/batch; 15h:43m:05s remains)
INFO - root - 2017-12-08 09:09:07.235731: step 67800, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 16h:08m:48s remains)
2017-12-08 09:09:07.572854: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288535 -4.4288769 -4.4289002 -4.4289079 -4.4288788 -4.4288096 -4.4287138 -4.428617 -4.4285655 -4.4285789 -4.4286737 -4.4287791 -4.4288445 -4.4288797 -4.4289][-4.4288759 -4.4289064 -4.4289308 -4.4289227 -4.4288635 -4.4287634 -4.4286265 -4.4284816 -4.4283957 -4.4284072 -4.4285517 -4.4287119 -4.428813 -4.42887 -4.428905][-4.4288878 -4.4289246 -4.4289465 -4.4289169 -4.4288273 -4.4286962 -4.428515 -4.4283137 -4.428174 -4.4281778 -4.4283862 -4.428618 -4.4287658 -4.42885 -4.4289012][-4.4289327 -4.4289718 -4.4289804 -4.4289227 -4.4288049 -4.4286518 -4.4284434 -4.4282036 -4.4280162 -4.4280071 -4.4282579 -4.4285488 -4.4287367 -4.428843 -4.4289069][-4.4289808 -4.4290137 -4.4290018 -4.4289207 -4.4287848 -4.4286237 -4.4284077 -4.428154 -4.4279456 -4.4279222 -4.4281764 -4.4284916 -4.4287057 -4.4288254 -4.428894][-4.4290142 -4.4290361 -4.4289961 -4.4288893 -4.4287405 -4.4285793 -4.4283772 -4.4281478 -4.4279618 -4.4279342 -4.4281611 -4.4284654 -4.4286852 -4.42881 -4.4288793][-4.4290361 -4.4290476 -4.4289865 -4.4288659 -4.42872 -4.4285655 -4.42838 -4.4281826 -4.4280291 -4.4279971 -4.4281778 -4.4284534 -4.4286761 -4.4288073 -4.4288754][-4.4290462 -4.4290438 -4.4289722 -4.428854 -4.4287305 -4.4285994 -4.428443 -4.4282804 -4.4281483 -4.428112 -4.4282427 -4.4284759 -4.4286942 -4.4288282 -4.4288921][-4.429038 -4.4290261 -4.4289584 -4.4288607 -4.4287724 -4.4286761 -4.4285507 -4.4284124 -4.428299 -4.4282579 -4.4283471 -4.4285378 -4.4287386 -4.428863 -4.4289155][-4.4290147 -4.4290061 -4.4289565 -4.4288912 -4.4288435 -4.4287839 -4.4286823 -4.4285603 -4.4284616 -4.428422 -4.428484 -4.4286346 -4.4287963 -4.4288893 -4.4289212][-4.4289861 -4.4289904 -4.428968 -4.4289365 -4.4289203 -4.4288926 -4.4288154 -4.4287038 -4.4286079 -4.4285679 -4.4286151 -4.4287319 -4.4288478 -4.4289036 -4.4289122][-4.4289532 -4.42897 -4.42897 -4.4289613 -4.4289618 -4.4289551 -4.4288988 -4.4287996 -4.4287047 -4.4286594 -4.4286995 -4.4287949 -4.4288745 -4.4289017 -4.428896][-4.4289269 -4.4289474 -4.4289546 -4.4289556 -4.4289618 -4.4289665 -4.4289308 -4.4288511 -4.4287648 -4.4287224 -4.4287562 -4.428833 -4.4288869 -4.4288988 -4.4288921][-4.4289145 -4.4289303 -4.4289346 -4.4289374 -4.428947 -4.4289565 -4.4289374 -4.4288816 -4.4288144 -4.4287791 -4.428803 -4.4288573 -4.428894 -4.4289002 -4.4288974][-4.4289074 -4.4289155 -4.4289136 -4.4289155 -4.4289241 -4.4289384 -4.4289365 -4.4289045 -4.4288564 -4.4288282 -4.4288411 -4.4288726 -4.428895 -4.4289002 -4.4289021]]...]
INFO - root - 2017-12-08 09:09:09.822605: step 67810, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 16h:15m:07s remains)
INFO - root - 2017-12-08 09:09:12.062350: step 67820, loss = 2.28, batch loss = 2.23 (37.5 examples/sec; 0.214 sec/batch; 15h:42m:19s remains)
INFO - root - 2017-12-08 09:09:14.306776: step 67830, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.222 sec/batch; 16h:17m:48s remains)
INFO - root - 2017-12-08 09:09:16.549272: step 67840, loss = 2.28, batch loss = 2.23 (35.2 examples/sec; 0.227 sec/batch; 16h:42m:31s remains)
INFO - root - 2017-12-08 09:09:18.785471: step 67850, loss = 2.28, batch loss = 2.23 (37.2 examples/sec; 0.215 sec/batch; 15h:47m:41s remains)
INFO - root - 2017-12-08 09:09:21.038059: step 67860, loss = 2.28, batch loss = 2.23 (35.4 examples/sec; 0.226 sec/batch; 16h:35m:43s remains)
INFO - root - 2017-12-08 09:09:23.259387: step 67870, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.222 sec/batch; 16h:16m:56s remains)
INFO - root - 2017-12-08 09:09:25.525663: step 67880, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 16h:22m:23s remains)
INFO - root - 2017-12-08 09:09:27.771635: step 67890, loss = 2.28, batch loss = 2.23 (34.8 examples/sec; 0.230 sec/batch; 16h:53m:39s remains)
INFO - root - 2017-12-08 09:09:29.995047: step 67900, loss = 2.28, batch loss = 2.23 (35.3 examples/sec; 0.227 sec/batch; 16h:40m:00s remains)
2017-12-08 09:09:30.284589: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4286256 -4.4286284 -4.4286261 -4.4286222 -4.4286 -4.4285903 -4.4286175 -4.4286647 -4.4286828 -4.4286828 -4.4286838 -4.4287038 -4.4287248 -4.42874 -4.4287462][-4.4286261 -4.4286137 -4.428596 -4.428576 -4.4285412 -4.4285307 -4.4285822 -4.428669 -4.4287243 -4.4287424 -4.42873 -4.4287281 -4.4287429 -4.4287543 -4.4287624][-4.4286451 -4.4286342 -4.4286103 -4.428576 -4.428534 -4.4285169 -4.4285688 -4.4286671 -4.4287381 -4.428761 -4.4287367 -4.4287252 -4.4287362 -4.4287519 -4.4287777][-4.4286652 -4.4286537 -4.4286351 -4.4286046 -4.4285665 -4.4285383 -4.4285636 -4.4286437 -4.4287124 -4.4287391 -4.4287276 -4.4287181 -4.428731 -4.4287558 -4.4287958][-4.4286857 -4.4286718 -4.4286523 -4.4286289 -4.4285936 -4.4285417 -4.4285069 -4.4285383 -4.4286141 -4.4286823 -4.4287157 -4.4287195 -4.4287329 -4.4287572 -4.4287939][-4.4287024 -4.428688 -4.4286604 -4.4286385 -4.4286 -4.4285111 -4.4283729 -4.4283261 -4.4284506 -4.4285984 -4.4286728 -4.4287014 -4.4287233 -4.4287434 -4.4287677][-4.4286952 -4.4286876 -4.4286537 -4.428627 -4.4285879 -4.4284678 -4.428225 -4.4280977 -4.4282832 -4.4285054 -4.4286213 -4.4286728 -4.4287047 -4.42872 -4.4287443][-4.42867 -4.4286609 -4.4286175 -4.4285851 -4.4285583 -4.4284463 -4.4281797 -4.4280195 -4.4282112 -4.4284678 -4.4286137 -4.4286876 -4.4287181 -4.4287229 -4.4287419][-4.4286532 -4.4286337 -4.428587 -4.4285641 -4.4285631 -4.4284925 -4.4282875 -4.4281516 -4.42828 -4.4284983 -4.4286532 -4.4287415 -4.42877 -4.428762 -4.4287605][-4.4286752 -4.4286504 -4.4286075 -4.4285874 -4.4285908 -4.428555 -4.4284382 -4.4283485 -4.4284039 -4.4285574 -4.4286933 -4.4287815 -4.428812 -4.4288034 -4.4287839][-4.4287019 -4.4286714 -4.4286289 -4.4285965 -4.4285922 -4.4285879 -4.4285359 -4.4284782 -4.4284987 -4.4286051 -4.4287124 -4.4287887 -4.4288216 -4.4288268 -4.4288168][-4.4286966 -4.4286537 -4.4286103 -4.4285722 -4.4285669 -4.42859 -4.4285812 -4.4285512 -4.4285569 -4.4286246 -4.4287033 -4.4287696 -4.4288077 -4.428823 -4.428823][-4.4286761 -4.4286261 -4.4285841 -4.4285464 -4.4285388 -4.4285674 -4.4285841 -4.4285736 -4.4285793 -4.42862 -4.4286737 -4.428731 -4.4287629 -4.4287825 -4.4287944][-4.4286704 -4.4286242 -4.4285874 -4.4285507 -4.4285355 -4.4285545 -4.4285769 -4.4285822 -4.4285927 -4.42862 -4.4286518 -4.4286895 -4.4287071 -4.4287353 -4.4287663][-4.4286923 -4.428648 -4.42862 -4.4285917 -4.428575 -4.4285855 -4.4286065 -4.4286189 -4.4286323 -4.4286547 -4.4286776 -4.4286976 -4.4287038 -4.4287319 -4.4287715]]...]
INFO - root - 2017-12-08 09:09:32.493549: step 67910, loss = 2.28, batch loss = 2.23 (36.6 examples/sec; 0.219 sec/batch; 16h:04m:54s remains)
INFO - root - 2017-12-08 09:09:34.772369: step 67920, loss = 2.28, batch loss = 2.23 (34.3 examples/sec; 0.234 sec/batch; 17h:09m:48s remains)
INFO - root - 2017-12-08 09:09:36.991137: step 67930, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 16h:06m:11s remains)
INFO - root - 2017-12-08 09:09:39.238036: step 67940, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 16h:08m:21s remains)
INFO - root - 2017-12-08 09:09:41.472060: step 67950, loss = 2.28, batch loss = 2.23 (37.1 examples/sec; 0.216 sec/batch; 15h:50m:21s remains)
INFO - root - 2017-12-08 09:09:43.690970: step 67960, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.220 sec/batch; 16h:11m:56s remains)
INFO - root - 2017-12-08 09:09:45.927926: step 67970, loss = 2.28, batch loss = 2.23 (35.3 examples/sec; 0.227 sec/batch; 16h:40m:01s remains)
INFO - root - 2017-12-08 09:09:48.166872: step 67980, loss = 2.28, batch loss = 2.23 (35.4 examples/sec; 0.226 sec/batch; 16h:35m:50s remains)
INFO - root - 2017-12-08 09:09:50.443291: step 67990, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 16h:06m:36s remains)
INFO - root - 2017-12-08 09:09:52.676123: step 68000, loss = 2.28, batch loss = 2.23 (36.6 examples/sec; 0.218 sec/batch; 16h:02m:36s remains)
2017-12-08 09:09:52.985680: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4286747 -4.4286337 -4.4286618 -4.4287195 -4.4287691 -4.428802 -4.4288125 -4.4287834 -4.4287519 -4.4287419 -4.428709 -4.4286389 -4.428575 -4.4285588 -4.4286218][-4.4287248 -4.4287081 -4.4287233 -4.4287462 -4.4287744 -4.4287996 -4.4288197 -4.4288034 -4.4287691 -4.42874 -4.4286833 -4.4285975 -4.4285278 -4.4285245 -4.4286051][-4.4287715 -4.4287672 -4.4287691 -4.4287596 -4.4287596 -4.4287763 -4.4288058 -4.4287996 -4.4287634 -4.4287271 -4.428678 -4.4286194 -4.4285741 -4.4285908 -4.4286737][-4.428822 -4.4288182 -4.4288015 -4.4287672 -4.4287405 -4.4287295 -4.4287329 -4.4287124 -4.42869 -4.4286928 -4.4286914 -4.428678 -4.4286633 -4.4286914 -4.4287534][-4.4288435 -4.4288325 -4.428802 -4.4287567 -4.42871 -4.4286518 -4.4285827 -4.4285073 -4.4285088 -4.4285889 -4.428669 -4.4287066 -4.428721 -4.4287548 -4.42879][-4.4288406 -4.4288235 -4.4287863 -4.4287424 -4.4286842 -4.4285831 -4.4284339 -4.4282923 -4.4283137 -4.4284816 -4.4286342 -4.4287114 -4.4287415 -4.4287682 -4.4287815][-4.4288311 -4.428812 -4.4287844 -4.4287472 -4.4286933 -4.428596 -4.4284444 -4.428298 -4.4283252 -4.4284925 -4.4286375 -4.4287086 -4.4287295 -4.42874 -4.4287443][-4.428793 -4.4287739 -4.4287639 -4.4287415 -4.4287057 -4.4286423 -4.4285517 -4.4284711 -4.4284987 -4.4285922 -4.4286628 -4.4286838 -4.42868 -4.428689 -4.4287009][-4.4287267 -4.4287004 -4.4287019 -4.4286938 -4.4286785 -4.4286585 -4.4286275 -4.4285975 -4.428607 -4.4286389 -4.4286485 -4.4286242 -4.4285941 -4.428596 -4.4286251][-4.4286513 -4.428606 -4.4286017 -4.428607 -4.4286094 -4.4286165 -4.4286261 -4.4286175 -4.4286041 -4.4285903 -4.4285622 -4.428515 -4.4284782 -4.4284716 -4.4285035][-4.4286242 -4.4285803 -4.4285669 -4.428565 -4.4285564 -4.4285593 -4.4285817 -4.4285889 -4.428575 -4.4285512 -4.4285221 -4.4284873 -4.4284678 -4.4284568 -4.4284711][-4.4286919 -4.4286675 -4.428647 -4.4286213 -4.428587 -4.4285693 -4.4285812 -4.4285946 -4.42859 -4.4285741 -4.4285607 -4.4285502 -4.4285455 -4.4285412 -4.4285488][-4.4287939 -4.4287815 -4.42875 -4.4286995 -4.4286385 -4.4286122 -4.4286213 -4.4286437 -4.4286494 -4.4286342 -4.4286261 -4.428627 -4.428628 -4.4286318 -4.4286423][-4.4288568 -4.4288492 -4.4288206 -4.4287629 -4.4286947 -4.4286671 -4.4286785 -4.4287052 -4.4287229 -4.4287081 -4.4286909 -4.4286847 -4.4286847 -4.4286952 -4.428709][-4.42889 -4.4288831 -4.4288573 -4.428812 -4.4287553 -4.4287267 -4.4287324 -4.4287581 -4.4287834 -4.4287748 -4.4287548 -4.4287395 -4.4287319 -4.4287376 -4.4287481]]...]
INFO - root - 2017-12-08 09:09:55.260380: step 68010, loss = 2.28, batch loss = 2.23 (35.7 examples/sec; 0.224 sec/batch; 16h:26m:35s remains)
INFO - root - 2017-12-08 09:09:57.477215: step 68020, loss = 2.28, batch loss = 2.23 (36.6 examples/sec; 0.219 sec/batch; 16h:03m:30s remains)
INFO - root - 2017-12-08 09:09:59.708142: step 68030, loss = 2.28, batch loss = 2.23 (37.4 examples/sec; 0.214 sec/batch; 15h:41m:44s remains)
INFO - root - 2017-12-08 09:10:01.974486: step 68040, loss = 2.28, batch loss = 2.23 (35.7 examples/sec; 0.224 sec/batch; 16h:27m:31s remains)
INFO - root - 2017-12-08 09:10:04.212788: step 68050, loss = 2.28, batch loss = 2.23 (36.0 examples/sec; 0.222 sec/batch; 16h:20m:09s remains)
INFO - root - 2017-12-08 09:10:06.433291: step 68060, loss = 2.28, batch loss = 2.23 (35.3 examples/sec; 0.227 sec/batch; 16h:38m:19s remains)
INFO - root - 2017-12-08 09:10:08.655447: step 68070, loss = 2.28, batch loss = 2.23 (36.6 examples/sec; 0.219 sec/batch; 16h:03m:46s remains)
INFO - root - 2017-12-08 09:10:10.895274: step 68080, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 16h:06m:41s remains)
INFO - root - 2017-12-08 09:10:13.119136: step 68090, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.220 sec/batch; 16h:11m:33s remains)
INFO - root - 2017-12-08 09:10:15.365695: step 68100, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 16h:07m:05s remains)
2017-12-08 09:10:15.668230: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4290147 -4.4289885 -4.4289303 -4.428853 -4.4287472 -4.428618 -4.4285278 -4.4284854 -4.4285259 -4.4286017 -4.4286633 -4.42872 -4.4287753 -4.4287982 -4.4288163][-4.4290204 -4.428997 -4.4289432 -4.4288683 -4.42876 -4.4286284 -4.4285455 -4.4285221 -4.4285774 -4.4286513 -4.4287057 -4.4287572 -4.4288116 -4.4288306 -4.4288373][-4.4290214 -4.4290028 -4.4289536 -4.42888 -4.428772 -4.4286361 -4.4285574 -4.4285493 -4.4286122 -4.4286885 -4.4287381 -4.428782 -4.4288325 -4.42885 -4.4288483][-4.4290237 -4.4290094 -4.4289651 -4.4288907 -4.4287777 -4.4286327 -4.42854 -4.4285307 -4.4285989 -4.4286938 -4.4287586 -4.4288054 -4.4288516 -4.4288673 -4.4288559][-4.4290247 -4.4290128 -4.42897 -4.4288926 -4.42877 -4.4286003 -4.4284697 -4.4284348 -4.428503 -4.428627 -4.4287252 -4.428792 -4.4288421 -4.4288568 -4.42884][-4.4290271 -4.4290147 -4.4289689 -4.4288888 -4.4287596 -4.4285679 -4.4283891 -4.428308 -4.4283643 -4.4285088 -4.428638 -4.42873 -4.4287939 -4.4288168 -4.4288063][-4.4290309 -4.4290204 -4.4289761 -4.4288945 -4.428761 -4.4285579 -4.4283357 -4.4282017 -4.4282484 -4.4284143 -4.4285679 -4.4286776 -4.4287548 -4.4287891 -4.42878][-4.4290338 -4.4290285 -4.4289913 -4.4289165 -4.4287925 -4.4286036 -4.4283872 -4.4282451 -4.4282851 -4.4284496 -4.4286032 -4.4287024 -4.4287691 -4.4287982 -4.4287777][-4.4290333 -4.4290318 -4.4290037 -4.4289427 -4.4288406 -4.4286804 -4.4284983 -4.4283791 -4.4284186 -4.428565 -4.4287019 -4.428781 -4.4288206 -4.4288292 -4.4287906][-4.42903 -4.429028 -4.4290032 -4.4289532 -4.428865 -4.4287229 -4.4285703 -4.4284787 -4.4285207 -4.4286456 -4.4287658 -4.4288325 -4.4288526 -4.4288454 -4.4287977][-4.4290276 -4.4290228 -4.4289956 -4.4289494 -4.4288707 -4.4287448 -4.4286184 -4.428556 -4.4286032 -4.4287009 -4.4287982 -4.4288554 -4.4288635 -4.4288435 -4.428791][-4.4290285 -4.4290237 -4.4289961 -4.42895 -4.4288793 -4.4287724 -4.4286785 -4.4286427 -4.4286866 -4.4287596 -4.428833 -4.42888 -4.4288812 -4.4288535 -4.4288006][-4.4290318 -4.429029 -4.4290013 -4.4289503 -4.4288807 -4.4287896 -4.4287195 -4.4286981 -4.4287333 -4.4287891 -4.4288492 -4.428895 -4.4288917 -4.4288592 -4.4288068][-4.4290318 -4.4290257 -4.42899 -4.4289269 -4.4288473 -4.4287639 -4.4287128 -4.4287071 -4.4287443 -4.4287968 -4.4288521 -4.4288945 -4.4288864 -4.4288507 -4.4288015][-4.42903 -4.4290161 -4.4289684 -4.4288888 -4.4287891 -4.4286957 -4.4286494 -4.4286628 -4.4287138 -4.4287744 -4.4288306 -4.4288688 -4.4288611 -4.4288344 -4.4287968]]...]
INFO - root - 2017-12-08 09:10:17.870619: step 68110, loss = 2.28, batch loss = 2.23 (36.9 examples/sec; 0.217 sec/batch; 15h:55m:37s remains)
INFO - root - 2017-12-08 09:10:20.112726: step 68120, loss = 2.28, batch loss = 2.23 (35.3 examples/sec; 0.227 sec/batch; 16h:38m:56s remains)
INFO - root - 2017-12-08 09:10:22.380516: step 68130, loss = 2.28, batch loss = 2.23 (35.6 examples/sec; 0.225 sec/batch; 16h:29m:57s remains)
INFO - root - 2017-12-08 09:10:24.638613: step 68140, loss = 2.28, batch loss = 2.23 (36.0 examples/sec; 0.222 sec/batch; 16h:18m:24s remains)
INFO - root - 2017-12-08 09:10:26.869666: step 68150, loss = 2.28, batch loss = 2.23 (34.2 examples/sec; 0.234 sec/batch; 17h:09m:55s remains)
INFO - root - 2017-12-08 09:10:29.101691: step 68160, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.220 sec/batch; 16h:10m:33s remains)
INFO - root - 2017-12-08 09:10:31.358022: step 68170, loss = 2.28, batch loss = 2.23 (35.7 examples/sec; 0.224 sec/batch; 16h:26m:03s remains)
INFO - root - 2017-12-08 09:10:33.603963: step 68180, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.222 sec/batch; 16h:16m:05s remains)
INFO - root - 2017-12-08 09:10:35.858206: step 68190, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.220 sec/batch; 16h:10m:06s remains)
INFO - root - 2017-12-08 09:10:38.089486: step 68200, loss = 2.28, batch loss = 2.23 (36.8 examples/sec; 0.218 sec/batch; 15h:58m:36s remains)
2017-12-08 09:10:38.410147: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.42902 -4.4290061 -4.4289923 -4.42898 -4.4289737 -4.4289722 -4.4289722 -4.4289656 -4.4289532 -4.4289441 -4.4289436 -4.4289494 -4.4289484 -4.4289293 -4.4288845][-4.4290013 -4.4289913 -4.4289765 -4.4289622 -4.4289503 -4.4289336 -4.4289064 -4.4288683 -4.4288363 -4.4288311 -4.4288507 -4.4288774 -4.4288912 -4.4288745 -4.4288254][-4.428947 -4.4289541 -4.4289441 -4.4289236 -4.428896 -4.4288511 -4.428782 -4.4287009 -4.4286532 -4.4286661 -4.4287205 -4.4287744 -4.4288058 -4.4288 -4.4287663][-4.4288654 -4.4288898 -4.4288831 -4.4288511 -4.4288011 -4.4287229 -4.4286141 -4.4285026 -4.4284563 -4.4285021 -4.4285984 -4.4286852 -4.4287314 -4.4287362 -4.4287276][-4.4287577 -4.428793 -4.4287887 -4.4287534 -4.4286928 -4.428597 -4.4284697 -4.4283509 -4.4283195 -4.428401 -4.428535 -4.4286427 -4.4286914 -4.4287019 -4.4287138][-4.4286528 -4.428689 -4.4286871 -4.4286633 -4.4286113 -4.42853 -4.4284282 -4.4283485 -4.4283538 -4.4284391 -4.4285631 -4.4286575 -4.4286938 -4.4287043 -4.4287248][-4.4285975 -4.4286265 -4.4286275 -4.4286151 -4.4285913 -4.4285545 -4.4285131 -4.4284825 -4.4285007 -4.4285536 -4.42863 -4.4286895 -4.4287095 -4.4287195 -4.4287467][-4.4286056 -4.4286203 -4.4286203 -4.42862 -4.4286242 -4.42863 -4.4286337 -4.4286289 -4.4286366 -4.4286532 -4.428688 -4.4287195 -4.4287353 -4.4287529 -4.4287887][-4.42864 -4.4286509 -4.4286633 -4.4286795 -4.4286966 -4.4287157 -4.4287262 -4.4287214 -4.4287133 -4.4287128 -4.4287348 -4.4287596 -4.4287791 -4.428803 -4.4288392][-4.4286785 -4.4287062 -4.4287343 -4.4287558 -4.4287691 -4.4287825 -4.4287815 -4.4287691 -4.4287591 -4.4287639 -4.4287834 -4.4288068 -4.4288282 -4.42885 -4.428874][-4.4287233 -4.4287658 -4.428803 -4.4288206 -4.4288206 -4.4288192 -4.4288054 -4.4287891 -4.4287877 -4.4287987 -4.4288139 -4.42883 -4.4288507 -4.4288673 -4.4288816][-4.42877 -4.4288177 -4.4288549 -4.4288645 -4.4288487 -4.4288306 -4.4288092 -4.4287996 -4.4288049 -4.4288106 -4.428813 -4.4288206 -4.4288378 -4.4288549 -4.4288707][-4.4288125 -4.4288549 -4.4288826 -4.4288778 -4.4288497 -4.4288225 -4.4288073 -4.4288063 -4.4288096 -4.428802 -4.4287877 -4.428781 -4.4287934 -4.4288168 -4.4288435][-4.428833 -4.4288573 -4.42887 -4.42886 -4.4288363 -4.4288187 -4.4288149 -4.4288168 -4.4288073 -4.4287786 -4.4287429 -4.4287252 -4.4287386 -4.4287705 -4.4288077][-4.4288344 -4.428833 -4.4288344 -4.4288335 -4.4288297 -4.4288297 -4.428833 -4.4288273 -4.4287972 -4.4287477 -4.4287014 -4.4286842 -4.4287 -4.4287353 -4.4287763]]...]
INFO - root - 2017-12-08 09:10:40.650100: step 68210, loss = 2.28, batch loss = 2.23 (34.5 examples/sec; 0.232 sec/batch; 17h:01m:29s remains)
INFO - root - 2017-12-08 09:10:42.874624: step 68220, loss = 2.28, batch loss = 2.23 (36.8 examples/sec; 0.218 sec/batch; 15h:58m:44s remains)
INFO - root - 2017-12-08 09:10:45.103165: step 68230, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 16h:08m:43s remains)
INFO - root - 2017-12-08 09:10:47.335422: step 68240, loss = 2.28, batch loss = 2.23 (36.7 examples/sec; 0.218 sec/batch; 15h:59m:40s remains)
INFO - root - 2017-12-08 09:10:49.596952: step 68250, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.222 sec/batch; 16h:16m:28s remains)
INFO - root - 2017-12-08 09:10:51.828569: step 68260, loss = 2.28, batch loss = 2.23 (35.6 examples/sec; 0.225 sec/batch; 16h:29m:40s remains)
INFO - root - 2017-12-08 09:10:54.100041: step 68270, loss = 2.28, batch loss = 2.23 (34.0 examples/sec; 0.235 sec/batch; 17h:15m:16s remains)
INFO - root - 2017-12-08 09:10:56.359489: step 68280, loss = 2.28, batch loss = 2.23 (36.7 examples/sec; 0.218 sec/batch; 16h:00m:01s remains)
INFO - root - 2017-12-08 09:10:58.612553: step 68290, loss = 2.28, batch loss = 2.23 (35.6 examples/sec; 0.225 sec/batch; 16h:28m:47s remains)
INFO - root - 2017-12-08 09:11:00.841965: step 68300, loss = 2.28, batch loss = 2.23 (35.4 examples/sec; 0.226 sec/batch; 16h:33m:59s remains)
2017-12-08 09:11:01.155753: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288268 -4.4286985 -4.4285755 -4.4285278 -4.4285836 -4.4286222 -4.4286156 -4.42866 -4.4287419 -4.4288039 -4.4288325 -4.4288235 -4.4287767 -4.4287572 -4.4287963][-4.4288568 -4.4287257 -4.4285927 -4.4285216 -4.4285555 -4.4285908 -4.4286017 -4.4286704 -4.4287648 -4.428813 -4.4288306 -4.4288192 -4.428793 -4.428813 -4.4288745][-4.4288955 -4.4287643 -4.4286232 -4.4285231 -4.4285226 -4.428546 -4.4285693 -4.428647 -4.4287391 -4.4287753 -4.4287891 -4.4287887 -4.4287963 -4.4288554 -4.4289317][-4.4289207 -4.4287972 -4.428658 -4.4285407 -4.4284978 -4.4284897 -4.4285107 -4.4285975 -4.4286995 -4.4287372 -4.4287529 -4.428772 -4.4288154 -4.4288983 -4.4289722][-4.428916 -4.4288077 -4.4286795 -4.4285545 -4.4284749 -4.428431 -4.428443 -4.4285469 -4.4286613 -4.428699 -4.42872 -4.4287643 -4.428843 -4.4289351 -4.4289966][-4.4289055 -4.4288187 -4.4287057 -4.4285812 -4.428484 -4.4284205 -4.4284215 -4.4285259 -4.42863 -4.4286623 -4.4286947 -4.428771 -4.4288778 -4.4289656 -4.429008][-4.4289107 -4.4288478 -4.4287519 -4.4286394 -4.4285359 -4.4284649 -4.4284592 -4.4285493 -4.4286242 -4.4286432 -4.428689 -4.4287925 -4.4289083 -4.4289842 -4.4290071][-4.42893 -4.4288888 -4.4288158 -4.42872 -4.4286175 -4.4285393 -4.4285254 -4.42859 -4.4286361 -4.4286513 -4.4287105 -4.4288259 -4.4289336 -4.4289932 -4.4290013][-4.4289536 -4.4289312 -4.4288783 -4.4287953 -4.4286914 -4.4285955 -4.4285583 -4.4285922 -4.4286356 -4.4286728 -4.4287529 -4.4288645 -4.4289556 -4.4289985 -4.4289956][-4.428968 -4.4289546 -4.4289093 -4.4288316 -4.4287248 -4.4286094 -4.4285388 -4.4285502 -4.428607 -4.4286814 -4.4287815 -4.428885 -4.42896 -4.4289923 -4.4289832][-4.4289641 -4.42895 -4.4289069 -4.4288349 -4.4287295 -4.4286017 -4.4285045 -4.4285026 -4.4285803 -4.4286914 -4.428802 -4.428895 -4.428957 -4.4289827 -4.4289689][-4.4289522 -4.4289341 -4.4288926 -4.4288292 -4.4287372 -4.4286156 -4.4285111 -4.4285007 -4.4285879 -4.4287157 -4.4288263 -4.4289069 -4.4289584 -4.4289804 -4.4289656][-4.4289465 -4.4289284 -4.4288921 -4.4288378 -4.4287639 -4.4286642 -4.4285712 -4.4285555 -4.4286294 -4.4287453 -4.4288459 -4.428916 -4.4289632 -4.4289861 -4.4289737][-4.4289527 -4.4289384 -4.4289093 -4.4288659 -4.4288106 -4.42874 -4.4286737 -4.42866 -4.4287138 -4.428803 -4.4288831 -4.4289389 -4.428978 -4.428997 -4.4289875][-4.4289632 -4.4289551 -4.428936 -4.4289069 -4.4288688 -4.428823 -4.4287786 -4.4287686 -4.428802 -4.428864 -4.4289212 -4.4289641 -4.4289956 -4.4290085 -4.4290004]]...]
INFO - root - 2017-12-08 09:11:03.398198: step 68310, loss = 2.28, batch loss = 2.23 (34.8 examples/sec; 0.230 sec/batch; 16h:51m:01s remains)
INFO - root - 2017-12-08 09:11:05.630271: step 68320, loss = 2.28, batch loss = 2.23 (36.8 examples/sec; 0.218 sec/batch; 15h:57m:47s remains)
INFO - root - 2017-12-08 09:11:07.890873: step 68330, loss = 2.28, batch loss = 2.23 (36.7 examples/sec; 0.218 sec/batch; 15h:58m:42s remains)
INFO - root - 2017-12-08 09:11:10.145409: step 68340, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.222 sec/batch; 16h:15m:15s remains)
INFO - root - 2017-12-08 09:11:12.377369: step 68350, loss = 2.28, batch loss = 2.23 (35.6 examples/sec; 0.225 sec/batch; 16h:30m:38s remains)
INFO - root - 2017-12-08 09:11:14.606506: step 68360, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.220 sec/batch; 16h:10m:26s remains)
INFO - root - 2017-12-08 09:11:16.810820: step 68370, loss = 2.28, batch loss = 2.23 (37.2 examples/sec; 0.215 sec/batch; 15h:47m:40s remains)
INFO - root - 2017-12-08 09:11:19.063990: step 68380, loss = 2.28, batch loss = 2.23 (35.1 examples/sec; 0.228 sec/batch; 16h:44m:37s remains)
INFO - root - 2017-12-08 09:11:21.293491: step 68390, loss = 2.28, batch loss = 2.23 (35.6 examples/sec; 0.225 sec/batch; 16h:29m:18s remains)
INFO - root - 2017-12-08 09:11:23.519983: step 68400, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 16h:07m:01s remains)
2017-12-08 09:11:23.800877: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289889 -4.4289947 -4.4289951 -4.4289961 -4.4289961 -4.4289961 -4.4289956 -4.4289937 -4.42899 -4.4289875 -4.4289875 -4.4289875 -4.428987 -4.4289861 -4.4289827][-4.4289589 -4.4289618 -4.4289622 -4.4289618 -4.42896 -4.428957 -4.4289522 -4.4289451 -4.428936 -4.4289331 -4.428936 -4.4289389 -4.4289412 -4.4289427 -4.4289417][-4.4288936 -4.4288926 -4.4288926 -4.4288893 -4.4288826 -4.4288759 -4.4288659 -4.4288521 -4.4288368 -4.428833 -4.4288411 -4.4288483 -4.4288554 -4.4288611 -4.428865][-4.4288082 -4.428803 -4.4288011 -4.428793 -4.4287796 -4.4287672 -4.4287505 -4.4287305 -4.4287081 -4.4287033 -4.4287176 -4.4287319 -4.42875 -4.4287653 -4.4287767][-4.428721 -4.4287086 -4.4287019 -4.4286866 -4.4286647 -4.428647 -4.4286242 -4.4286017 -4.428576 -4.4285712 -4.4285932 -4.4286175 -4.4286518 -4.4286804 -4.4286985][-4.4286666 -4.4286466 -4.4286289 -4.4286017 -4.4285688 -4.4285421 -4.4285111 -4.4284849 -4.4284563 -4.4284539 -4.4284863 -4.428525 -4.4285803 -4.4286242 -4.4286456][-4.4286413 -4.4286137 -4.4285846 -4.4285431 -4.4284954 -4.4284558 -4.4284124 -4.4283786 -4.4283504 -4.4283543 -4.4283962 -4.4284468 -4.4285159 -4.42857 -4.4285922][-4.4286532 -4.4286208 -4.4285831 -4.4285312 -4.4284716 -4.4284172 -4.4283614 -4.4283228 -4.4283018 -4.4283156 -4.4283619 -4.4284139 -4.4284821 -4.42853 -4.4285445][-4.4286909 -4.4286556 -4.4286156 -4.4285631 -4.4285054 -4.4284492 -4.4283919 -4.4283581 -4.42835 -4.428371 -4.4284105 -4.4284496 -4.4284978 -4.4285221 -4.428515][-4.4287286 -4.4286966 -4.4286623 -4.4286208 -4.4285765 -4.428534 -4.4284916 -4.42847 -4.4284735 -4.4284921 -4.428514 -4.4285269 -4.4285378 -4.4285278 -4.4284916][-4.4287591 -4.4287329 -4.428709 -4.4286833 -4.428658 -4.4286337 -4.4286094 -4.4285979 -4.4286027 -4.4286127 -4.428617 -4.428606 -4.4285822 -4.4285407 -4.4284792][-4.4287782 -4.4287577 -4.4287424 -4.4287286 -4.4287162 -4.4287043 -4.4286909 -4.4286833 -4.4286833 -4.4286852 -4.428679 -4.4286556 -4.4286132 -4.4285526 -4.4284754][-4.4287906 -4.4287724 -4.4287615 -4.4287539 -4.4287481 -4.4287424 -4.4287343 -4.4287281 -4.4287257 -4.4287229 -4.4287138 -4.42869 -4.4286451 -4.4285784 -4.4284964][-4.4288015 -4.4287839 -4.4287744 -4.4287686 -4.4287653 -4.428762 -4.4287558 -4.428751 -4.4287491 -4.4287477 -4.4287415 -4.4287229 -4.4286842 -4.4286237 -4.4285493][-4.4288096 -4.4287934 -4.4287844 -4.4287796 -4.4287763 -4.428772 -4.4287653 -4.4287605 -4.42876 -4.428762 -4.4287615 -4.428751 -4.4287224 -4.4286747 -4.4286165]]...]
INFO - root - 2017-12-08 09:11:26.005699: step 68410, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 16h:11m:43s remains)
INFO - root - 2017-12-08 09:11:28.256436: step 68420, loss = 2.28, batch loss = 2.23 (36.0 examples/sec; 0.222 sec/batch; 16h:16m:47s remains)
INFO - root - 2017-12-08 09:11:30.493818: step 68430, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.219 sec/batch; 16h:06m:03s remains)
INFO - root - 2017-12-08 09:11:32.734053: step 68440, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 16h:04m:50s remains)
INFO - root - 2017-12-08 09:11:34.981126: step 68450, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 16h:20m:16s remains)
INFO - root - 2017-12-08 09:11:37.263698: step 68460, loss = 2.28, batch loss = 2.23 (34.6 examples/sec; 0.231 sec/batch; 16h:56m:43s remains)
INFO - root - 2017-12-08 09:11:39.526455: step 68470, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 16h:08m:07s remains)
INFO - root - 2017-12-08 09:11:41.764402: step 68480, loss = 2.28, batch loss = 2.23 (36.6 examples/sec; 0.218 sec/batch; 16h:00m:45s remains)
INFO - root - 2017-12-08 09:11:43.997075: step 68490, loss = 2.28, batch loss = 2.23 (36.6 examples/sec; 0.219 sec/batch; 16h:01m:43s remains)
INFO - root - 2017-12-08 09:11:46.221338: step 68500, loss = 2.28, batch loss = 2.23 (37.2 examples/sec; 0.215 sec/batch; 15h:46m:21s remains)
2017-12-08 09:11:46.541472: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288464 -4.4288483 -4.4288368 -4.4288235 -4.4288077 -4.4287734 -4.4287238 -4.4286962 -4.4287105 -4.4287505 -4.4287953 -4.4288325 -4.4288626 -4.428896 -4.4289303][-4.4288549 -4.4288592 -4.4288511 -4.4288368 -4.4288135 -4.4287677 -4.4286938 -4.4286451 -4.4286618 -4.4287219 -4.4287834 -4.4288263 -4.4288673 -4.4289117 -4.4289508][-4.4288592 -4.4288654 -4.4288559 -4.4288378 -4.4288125 -4.4287677 -4.4286828 -4.4286108 -4.4286237 -4.4287 -4.4287848 -4.4288392 -4.4288807 -4.428925 -4.4289603][-4.4288154 -4.4288187 -4.4288106 -4.4287953 -4.4287763 -4.4287419 -4.4286547 -4.4285731 -4.42858 -4.428668 -4.4287691 -4.4288297 -4.4288659 -4.4289007 -4.428926][-4.4287534 -4.42875 -4.42874 -4.4287353 -4.4287224 -4.4286938 -4.4285984 -4.428503 -4.428515 -4.4286275 -4.4287429 -4.4288068 -4.4288383 -4.4288588 -4.4288735][-4.4287109 -4.4286962 -4.4286685 -4.4286604 -4.4286528 -4.4286284 -4.4285221 -4.4283934 -4.4284024 -4.4285502 -4.4286861 -4.4287653 -4.4288106 -4.42883 -4.4288373][-4.428689 -4.4286551 -4.4286003 -4.4285784 -4.4285579 -4.4285259 -4.4284019 -4.4282188 -4.4282031 -4.4283967 -4.4285755 -4.4286871 -4.4287596 -4.4287958 -4.4287992][-4.4286695 -4.4286394 -4.4285817 -4.4285474 -4.4285045 -4.4284592 -4.4283347 -4.428123 -4.4280715 -4.4282966 -4.4285145 -4.4286466 -4.4287333 -4.4287806 -4.4287834][-4.4286489 -4.4286375 -4.4285984 -4.4285607 -4.4285216 -4.4284954 -4.4284191 -4.428246 -4.4281526 -4.4283237 -4.4285331 -4.4286623 -4.4287457 -4.4287934 -4.428791][-4.428618 -4.4286213 -4.4286089 -4.4285779 -4.4285579 -4.4285684 -4.4285398 -4.4284124 -4.4283013 -4.4284043 -4.4285769 -4.4286966 -4.4287748 -4.4288197 -4.4288073][-4.4286056 -4.4286237 -4.4286342 -4.4286065 -4.4285989 -4.4286318 -4.42862 -4.4285212 -4.4284182 -4.4284825 -4.4286265 -4.4287348 -4.4287968 -4.4288192 -4.4287796][-4.4286141 -4.4286547 -4.4286809 -4.42865 -4.4286256 -4.4286513 -4.4286404 -4.4285612 -4.4284697 -4.4285159 -4.428647 -4.4287415 -4.4287858 -4.4287829 -4.4287105][-4.4285827 -4.4286461 -4.4286804 -4.42865 -4.42862 -4.4286366 -4.428627 -4.4285727 -4.4284992 -4.42853 -4.4286509 -4.428731 -4.42876 -4.4287386 -4.4286466][-4.4285283 -4.4285879 -4.428627 -4.4286075 -4.4285913 -4.42861 -4.428617 -4.4286003 -4.4285507 -4.4285603 -4.4286532 -4.4287124 -4.4287238 -4.4286942 -4.4286036][-4.4285464 -4.4285803 -4.4286079 -4.428587 -4.4285645 -4.4285722 -4.4286075 -4.4286351 -4.4286237 -4.4286246 -4.4286838 -4.4287105 -4.4287019 -4.42867 -4.4285879]]...]
INFO - root - 2017-12-08 09:11:48.763321: step 68510, loss = 2.28, batch loss = 2.23 (36.7 examples/sec; 0.218 sec/batch; 15h:59m:03s remains)
INFO - root - 2017-12-08 09:11:51.011671: step 68520, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.222 sec/batch; 16h:14m:50s remains)
INFO - root - 2017-12-08 09:11:53.226228: step 68530, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 16h:06m:38s remains)
INFO - root - 2017-12-08 09:11:55.445134: step 68540, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 16h:07m:28s remains)
INFO - root - 2017-12-08 09:11:57.658457: step 68550, loss = 2.28, batch loss = 2.23 (35.2 examples/sec; 0.227 sec/batch; 16h:39m:41s remains)
INFO - root - 2017-12-08 09:11:59.897271: step 68560, loss = 2.28, batch loss = 2.23 (35.8 examples/sec; 0.224 sec/batch; 16h:23m:28s remains)
INFO - root - 2017-12-08 09:12:02.117629: step 68570, loss = 2.28, batch loss = 2.23 (34.5 examples/sec; 0.232 sec/batch; 17h:01m:21s remains)
INFO - root - 2017-12-08 09:12:04.355908: step 68580, loss = 2.28, batch loss = 2.23 (35.0 examples/sec; 0.228 sec/batch; 16h:44m:10s remains)
INFO - root - 2017-12-08 09:12:06.590529: step 68590, loss = 2.28, batch loss = 2.23 (34.0 examples/sec; 0.235 sec/batch; 17h:13m:47s remains)
INFO - root - 2017-12-08 09:12:08.839180: step 68600, loss = 2.28, batch loss = 2.23 (35.4 examples/sec; 0.226 sec/batch; 16h:32m:54s remains)
2017-12-08 09:12:09.145627: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4289613 -4.4289594 -4.4289508 -4.4289422 -4.4289374 -4.4289389 -4.4289422 -4.4289408 -4.4289384 -4.4289365 -4.4289341 -4.4289346 -4.4289422 -4.4289546 -4.4289637][-4.4289637 -4.4289546 -4.4289374 -4.4289227 -4.4289165 -4.4289193 -4.428925 -4.4289265 -4.4289236 -4.4289169 -4.428905 -4.4288983 -4.4289055 -4.4289255 -4.4289432][-4.4289608 -4.4289412 -4.4289141 -4.4288893 -4.4288745 -4.4288754 -4.4288821 -4.4288883 -4.4288893 -4.4288826 -4.4288621 -4.42885 -4.4288568 -4.4288826 -4.4289103][-4.42895 -4.4289184 -4.4288816 -4.428843 -4.4288163 -4.4288111 -4.4288139 -4.4288239 -4.428834 -4.4288354 -4.4288163 -4.4288044 -4.4288096 -4.4288383 -4.4288731][-4.4289351 -4.4288983 -4.4288545 -4.4287987 -4.4287524 -4.428731 -4.428721 -4.4287305 -4.4287567 -4.4287705 -4.4287572 -4.4287529 -4.4287553 -4.4287763 -4.4288144][-4.4289145 -4.4288716 -4.4288197 -4.4287486 -4.4286771 -4.428627 -4.4285932 -4.4285946 -4.4286489 -4.4286938 -4.4286952 -4.428699 -4.4286952 -4.4286976 -4.4287333][-4.4288812 -4.4288239 -4.4287615 -4.4286847 -4.4285812 -4.4284782 -4.4283919 -4.4283657 -4.4284587 -4.4285693 -4.4286227 -4.4286513 -4.42865 -4.428638 -4.4286685][-4.4288363 -4.4287615 -4.4286842 -4.4286075 -4.4284878 -4.4283328 -4.4281683 -4.4280615 -4.4281688 -4.4283633 -4.4284978 -4.4285741 -4.4285936 -4.4285865 -4.4286194][-4.4288158 -4.4287348 -4.4286528 -4.4285822 -4.4284763 -4.4283161 -4.4281154 -4.4279428 -4.4280052 -4.4282203 -4.4284048 -4.4285173 -4.4285641 -4.4285746 -4.4286084][-4.4288244 -4.4287591 -4.4286985 -4.42866 -4.4285922 -4.4284744 -4.4283247 -4.428194 -4.4282041 -4.4283276 -4.4284573 -4.4285455 -4.4285936 -4.4286122 -4.4286442][-4.4288492 -4.4288068 -4.4287682 -4.4287519 -4.42871 -4.4286308 -4.428544 -4.4284706 -4.42847 -4.4285283 -4.4285932 -4.4286432 -4.4286809 -4.4286942 -4.4287171][-4.4288883 -4.4288712 -4.4288507 -4.4288416 -4.4288192 -4.4287729 -4.4287257 -4.42869 -4.4286871 -4.4287105 -4.4287386 -4.4287677 -4.4287939 -4.4288025 -4.428813][-4.42892 -4.4289188 -4.4289126 -4.4289126 -4.4289064 -4.4288878 -4.4288735 -4.4288592 -4.4288507 -4.4288511 -4.4288568 -4.4288707 -4.428885 -4.4288864 -4.42889][-4.4289374 -4.4289379 -4.428936 -4.4289374 -4.4289336 -4.4289289 -4.4289308 -4.4289322 -4.42893 -4.4289274 -4.4289293 -4.4289355 -4.4289389 -4.4289331 -4.4289308][-4.4289517 -4.4289536 -4.4289513 -4.4289494 -4.4289455 -4.4289432 -4.4289474 -4.4289541 -4.4289584 -4.4289589 -4.4289603 -4.4289627 -4.4289613 -4.4289551 -4.4289522]]...]
INFO - root - 2017-12-08 09:12:11.391558: step 68610, loss = 2.28, batch loss = 2.23 (36.7 examples/sec; 0.218 sec/batch; 15h:58m:22s remains)
INFO - root - 2017-12-08 09:12:13.618347: step 68620, loss = 2.28, batch loss = 2.23 (35.6 examples/sec; 0.225 sec/batch; 16h:28m:04s remains)
INFO - root - 2017-12-08 09:12:15.874505: step 68630, loss = 2.28, batch loss = 2.23 (35.4 examples/sec; 0.226 sec/batch; 16h:34m:50s remains)
INFO - root - 2017-12-08 09:12:18.137604: step 68640, loss = 2.28, batch loss = 2.23 (35.0 examples/sec; 0.228 sec/batch; 16h:44m:03s remains)
INFO - root - 2017-12-08 09:12:20.369497: step 68650, loss = 2.28, batch loss = 2.23 (34.1 examples/sec; 0.234 sec/batch; 17h:10m:21s remains)
INFO - root - 2017-12-08 09:12:22.611864: step 68660, loss = 2.28, batch loss = 2.23 (35.0 examples/sec; 0.229 sec/batch; 16h:45m:03s remains)
INFO - root - 2017-12-08 09:12:24.844186: step 68670, loss = 2.28, batch loss = 2.23 (36.6 examples/sec; 0.219 sec/batch; 16h:01m:24s remains)
INFO - root - 2017-12-08 09:12:27.084719: step 68680, loss = 2.28, batch loss = 2.23 (35.2 examples/sec; 0.227 sec/batch; 16h:39m:55s remains)
INFO - root - 2017-12-08 09:12:29.310912: step 68690, loss = 2.28, batch loss = 2.23 (34.9 examples/sec; 0.229 sec/batch; 16h:47m:45s remains)
INFO - root - 2017-12-08 09:12:31.575815: step 68700, loss = 2.28, batch loss = 2.23 (33.6 examples/sec; 0.238 sec/batch; 17h:26m:24s remains)
2017-12-08 09:12:31.903811: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4287858 -4.42877 -4.428782 -4.4288216 -4.4288549 -4.4288545 -4.4288383 -4.4288144 -4.4288015 -4.4287963 -4.4287992 -4.4288249 -4.4288559 -4.4288912 -4.4289351][-4.4287457 -4.4287548 -4.4287972 -4.4288583 -4.4288955 -4.4288859 -4.4288554 -4.4288173 -4.4287963 -4.4287848 -4.4287796 -4.4288049 -4.4288378 -4.4288726 -4.4289207][-4.4287376 -4.4287786 -4.4288449 -4.4289131 -4.4289441 -4.4289203 -4.4288692 -4.4288054 -4.4287615 -4.4287333 -4.4287205 -4.4287519 -4.4287963 -4.4288425 -4.4289002][-4.4287529 -4.4288158 -4.4288974 -4.4289641 -4.4289842 -4.4289379 -4.4288511 -4.4287415 -4.4286594 -4.4286089 -4.428596 -4.4286532 -4.4287281 -4.4288025 -4.4288793][-4.4287887 -4.4288597 -4.4289384 -4.4289889 -4.4289846 -4.4289055 -4.4287682 -4.4285984 -4.4284739 -4.4284177 -4.4284382 -4.4285517 -4.4286704 -4.4287753 -4.4288683][-4.4288464 -4.4289103 -4.4289665 -4.4289851 -4.4289451 -4.4288249 -4.4286385 -4.4284191 -4.4282789 -4.4282589 -4.4283524 -4.4285283 -4.4286766 -4.4287891 -4.4288793][-4.4288969 -4.4289417 -4.4289713 -4.4289589 -4.4288893 -4.4287419 -4.4285321 -4.4283071 -4.4282 -4.428247 -4.4284034 -4.428596 -4.4287348 -4.4288282 -4.4288974][-4.42893 -4.4289622 -4.4289689 -4.4289341 -4.42885 -4.4286976 -4.4285021 -4.4283195 -4.4282689 -4.4283595 -4.4285188 -4.4286761 -4.4287772 -4.4288464 -4.4288988][-4.4289331 -4.4289556 -4.428947 -4.4289041 -4.428822 -4.4286857 -4.4285235 -4.428401 -4.4283929 -4.4284849 -4.4285984 -4.4287028 -4.4287753 -4.4288292 -4.4288788][-4.4289255 -4.4289351 -4.4289141 -4.4288692 -4.4287972 -4.4286885 -4.4285703 -4.428494 -4.4285 -4.4285655 -4.4286284 -4.4286938 -4.42875 -4.4287953 -4.4288507][-4.4289222 -4.4289145 -4.4288797 -4.428834 -4.4287777 -4.4287019 -4.4286251 -4.4285769 -4.4285822 -4.428616 -4.4286413 -4.4286861 -4.4287405 -4.4287882 -4.4288497][-4.4289303 -4.4289107 -4.4288707 -4.4288354 -4.4287992 -4.4287519 -4.4287047 -4.428668 -4.4286661 -4.4286742 -4.4286785 -4.4287157 -4.4287777 -4.4288316 -4.4288874][-4.4289441 -4.4289227 -4.4288898 -4.4288707 -4.4288516 -4.4288273 -4.4287977 -4.4287686 -4.4287658 -4.4287639 -4.4287648 -4.428802 -4.42886 -4.4289064 -4.4289436][-4.4289451 -4.4289265 -4.4289026 -4.4288936 -4.4288898 -4.4288816 -4.428864 -4.428844 -4.4288459 -4.4288473 -4.428853 -4.4288874 -4.4289327 -4.428966 -4.428987][-4.4289532 -4.4289389 -4.4289236 -4.4289184 -4.4289203 -4.4289184 -4.4289074 -4.4288945 -4.4288983 -4.4289026 -4.4289131 -4.4289427 -4.4289732 -4.4289951 -4.429008]]...]
INFO - root - 2017-12-08 09:12:34.145433: step 68710, loss = 2.28, batch loss = 2.23 (36.8 examples/sec; 0.217 sec/batch; 15h:56m:12s remains)
INFO - root - 2017-12-08 09:12:36.376744: step 68720, loss = 2.28, batch loss = 2.23 (35.8 examples/sec; 0.223 sec/batch; 16h:22m:34s remains)
INFO - root - 2017-12-08 09:12:38.639987: step 68730, loss = 2.28, batch loss = 2.23 (35.7 examples/sec; 0.224 sec/batch; 16h:25m:27s remains)
INFO - root - 2017-12-08 09:12:40.873374: step 68740, loss = 2.28, batch loss = 2.23 (35.0 examples/sec; 0.228 sec/batch; 16h:43m:25s remains)
INFO - root - 2017-12-08 09:12:43.102861: step 68750, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 16h:18m:46s remains)
INFO - root - 2017-12-08 09:12:45.324459: step 68760, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 16h:10m:22s remains)
INFO - root - 2017-12-08 09:12:47.561444: step 68770, loss = 2.28, batch loss = 2.23 (35.0 examples/sec; 0.228 sec/batch; 16h:43m:55s remains)
INFO - root - 2017-12-08 09:12:49.802951: step 68780, loss = 2.28, batch loss = 2.23 (35.8 examples/sec; 0.223 sec/batch; 16h:21m:31s remains)
INFO - root - 2017-12-08 09:12:52.052792: step 68790, loss = 2.28, batch loss = 2.23 (36.5 examples/sec; 0.219 sec/batch; 16h:03m:52s remains)
INFO - root - 2017-12-08 09:12:54.289954: step 68800, loss = 2.28, batch loss = 2.23 (37.1 examples/sec; 0.215 sec/batch; 15h:46m:48s remains)
2017-12-08 09:12:54.608772: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4287677 -4.4287429 -4.4287486 -4.4287844 -4.4287992 -4.4287758 -4.4287162 -4.428658 -4.4286551 -4.4287009 -4.4287362 -4.4287286 -4.4286976 -4.4286757 -4.4286809][-4.4287543 -4.4287305 -4.4287248 -4.4287591 -4.4287877 -4.428781 -4.4287419 -4.4286995 -4.4286995 -4.428741 -4.4287729 -4.4287596 -4.4287262 -4.4286995 -4.4286895][-4.4286771 -4.4286628 -4.4286675 -4.4287114 -4.4287529 -4.4287596 -4.42874 -4.4287195 -4.4287305 -4.4287772 -4.4288139 -4.4288054 -4.4287786 -4.4287505 -4.4287267][-4.4285851 -4.4285865 -4.4286079 -4.4286551 -4.4287 -4.4287162 -4.4287124 -4.4287119 -4.4287333 -4.4287834 -4.428822 -4.4288182 -4.4287987 -4.428771 -4.4287372][-4.428576 -4.4285917 -4.4286184 -4.4286542 -4.4286776 -4.4286737 -4.4286604 -4.4286652 -4.4287024 -4.4287653 -4.4288068 -4.4288049 -4.428781 -4.4287429 -4.4287009][-4.4286184 -4.4286427 -4.4286671 -4.4286828 -4.4286728 -4.4286289 -4.42858 -4.4285679 -4.4286141 -4.428699 -4.4287586 -4.4287624 -4.4287286 -4.4286752 -4.4286265][-4.4286289 -4.4286613 -4.428688 -4.4286866 -4.4286427 -4.4285531 -4.4284506 -4.4283957 -4.4284434 -4.4285731 -4.4286766 -4.4287109 -4.4286876 -4.4286385 -4.4285951][-4.4286141 -4.4286413 -4.4286594 -4.4286375 -4.4285593 -4.4284215 -4.4282603 -4.4281526 -4.4282069 -4.4283938 -4.4285517 -4.4286337 -4.4286475 -4.4286318 -4.4286127][-4.4286075 -4.428616 -4.4286189 -4.4285836 -4.4284997 -4.4283648 -4.4282045 -4.42809 -4.4281406 -4.4283328 -4.4284997 -4.4285994 -4.4286404 -4.4286547 -4.4286575][-4.4286356 -4.4286275 -4.4286203 -4.4285927 -4.4285445 -4.42846 -4.4283595 -4.4282842 -4.4283161 -4.4284506 -4.428575 -4.4286571 -4.4287028 -4.4287252 -4.4287281][-4.4287 -4.428678 -4.4286666 -4.4286542 -4.4286404 -4.4286013 -4.4285517 -4.4285111 -4.4285216 -4.4285955 -4.4286757 -4.4287338 -4.4287767 -4.4287987 -4.4287944][-4.4287658 -4.4287481 -4.4287429 -4.4287434 -4.4287486 -4.4287362 -4.4287128 -4.4286876 -4.428678 -4.4287081 -4.4287524 -4.428791 -4.4288249 -4.4288445 -4.4288421][-4.4288363 -4.4288268 -4.4288239 -4.4288292 -4.4288406 -4.4288406 -4.4288292 -4.4288092 -4.4287925 -4.4288015 -4.4288206 -4.4288392 -4.4288578 -4.4288692 -4.4288707][-4.42889 -4.4288864 -4.4288826 -4.4288869 -4.4288969 -4.4288993 -4.4288936 -4.42888 -4.4288678 -4.4288688 -4.4288769 -4.428885 -4.4288936 -4.4288993 -4.4289007][-4.4289203 -4.4289184 -4.428916 -4.42892 -4.4289255 -4.4289274 -4.428926 -4.4289217 -4.428916 -4.4289136 -4.4289131 -4.4289145 -4.4289179 -4.4289207 -4.4289222]]...]
INFO - root - 2017-12-08 09:12:56.855662: step 68810, loss = 2.28, batch loss = 2.23 (35.8 examples/sec; 0.224 sec/batch; 16h:22m:31s remains)
INFO - root - 2017-12-08 09:12:59.087847: step 68820, loss = 2.28, batch loss = 2.23 (35.0 examples/sec; 0.228 sec/batch; 16h:43m:06s remains)
INFO - root - 2017-12-08 09:13:01.316915: step 68830, loss = 2.28, batch loss = 2.23 (36.0 examples/sec; 0.222 sec/batch; 16h:17m:08s remains)
INFO - root - 2017-12-08 09:13:03.550419: step 68840, loss = 2.28, batch loss = 2.23 (35.4 examples/sec; 0.226 sec/batch; 16h:32m:11s remains)
INFO - root - 2017-12-08 09:13:05.790586: step 68850, loss = 2.28, batch loss = 2.23 (34.6 examples/sec; 0.231 sec/batch; 16h:54m:34s remains)
INFO - root - 2017-12-08 09:13:08.010676: step 68860, loss = 2.28, batch loss = 2.23 (35.6 examples/sec; 0.225 sec/batch; 16h:28m:37s remains)
INFO - root - 2017-12-08 09:13:10.247166: step 68870, loss = 2.28, batch loss = 2.23 (35.4 examples/sec; 0.226 sec/batch; 16h:32m:39s remains)
INFO - root - 2017-12-08 09:13:12.474677: step 68880, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.221 sec/batch; 16h:09m:29s remains)
INFO - root - 2017-12-08 09:13:14.704074: step 68890, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.220 sec/batch; 16h:08m:20s remains)
INFO - root - 2017-12-08 09:13:16.920253: step 68900, loss = 2.28, batch loss = 2.23 (35.7 examples/sec; 0.224 sec/batch; 16h:23m:07s remains)
2017-12-08 09:13:17.222061: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4288478 -4.428864 -4.4288816 -4.4288821 -4.4288468 -4.4287724 -4.4286909 -4.4286385 -4.4286637 -4.4287381 -4.4288158 -4.4288497 -4.4288425 -4.4288239 -4.4288158][-4.4288673 -4.4288754 -4.4288793 -4.4288654 -4.4288149 -4.4287252 -4.4286346 -4.4285932 -4.4286356 -4.4287257 -4.4288139 -4.4288592 -4.4288568 -4.4288268 -4.4288][-4.428884 -4.428884 -4.4288726 -4.4288383 -4.4287686 -4.4286714 -4.428586 -4.4285626 -4.4286232 -4.4287252 -4.4288182 -4.4288745 -4.4288731 -4.4288325 -4.42879][-4.4289012 -4.428896 -4.4288731 -4.4288235 -4.4287381 -4.42864 -4.42856 -4.4285531 -4.4286284 -4.4287362 -4.4288316 -4.428894 -4.4288969 -4.4288583 -4.4288063][-4.4289203 -4.4289136 -4.4288831 -4.4288244 -4.4287262 -4.4286165 -4.4285264 -4.428515 -4.428586 -4.4286995 -4.4288096 -4.4288921 -4.4289131 -4.4288888 -4.4288325][-4.4289289 -4.4289227 -4.4288893 -4.4288263 -4.4287171 -4.4285884 -4.4284725 -4.4284334 -4.4284887 -4.42861 -4.4287419 -4.4288507 -4.4289017 -4.4289002 -4.4288507][-4.4289308 -4.4289246 -4.4288921 -4.42883 -4.4287167 -4.4285693 -4.4284296 -4.4283605 -4.4284015 -4.4285293 -4.4286742 -4.4288025 -4.4288788 -4.4289 -4.4288659][-4.428936 -4.4289327 -4.428906 -4.42885 -4.4287434 -4.4285927 -4.4284382 -4.4283419 -4.4283624 -4.428484 -4.42863 -4.4287663 -4.4288626 -4.428906 -4.4288955][-4.4289494 -4.42895 -4.4289312 -4.4288888 -4.4288 -4.4286609 -4.4285045 -4.4283819 -4.4283614 -4.4284558 -4.4285975 -4.4287405 -4.4288535 -4.4289145 -4.4289274][-4.4289503 -4.4289613 -4.4289551 -4.4289279 -4.4288621 -4.4287462 -4.4286003 -4.4284697 -4.4284205 -4.4284835 -4.4286079 -4.4287395 -4.4288478 -4.4289131 -4.4289422][-4.4289169 -4.4289436 -4.4289536 -4.4289427 -4.4288979 -4.4288082 -4.428688 -4.4285774 -4.4285283 -4.4285722 -4.4286637 -4.4287567 -4.4288368 -4.4288898 -4.4289236][-4.4288616 -4.4289017 -4.4289236 -4.4289207 -4.4288888 -4.4288163 -4.4287119 -4.4286208 -4.4285846 -4.4286227 -4.4286923 -4.4287562 -4.4288111 -4.4288521 -4.4288898][-4.4288278 -4.4288707 -4.4288921 -4.4288855 -4.4288535 -4.4287853 -4.4286866 -4.4286013 -4.4285722 -4.4286108 -4.4286752 -4.4287305 -4.428772 -4.42881 -4.4288549][-4.4288335 -4.4288688 -4.4288793 -4.428863 -4.4288254 -4.428761 -4.4286709 -4.428597 -4.4285746 -4.4286146 -4.4286737 -4.4287157 -4.4287367 -4.4287643 -4.4288168][-4.4288425 -4.4288745 -4.4288759 -4.4288526 -4.4288154 -4.4287624 -4.4286904 -4.4286332 -4.4286208 -4.4286542 -4.4286995 -4.42872 -4.4287162 -4.4287238 -4.4287753]]...]
INFO - root - 2017-12-08 09:13:19.438711: step 68910, loss = 2.28, batch loss = 2.23 (37.5 examples/sec; 0.213 sec/batch; 15h:37m:23s remains)
INFO - root - 2017-12-08 09:13:21.689083: step 68920, loss = 2.28, batch loss = 2.23 (35.6 examples/sec; 0.224 sec/batch; 16h:25m:55s remains)
INFO - root - 2017-12-08 09:13:23.978335: step 68930, loss = 2.28, batch loss = 2.23 (35.3 examples/sec; 0.226 sec/batch; 16h:34m:44s remains)
INFO - root - 2017-12-08 09:13:26.197845: step 68940, loss = 2.28, batch loss = 2.23 (36.0 examples/sec; 0.222 sec/batch; 16h:16m:43s remains)
INFO - root - 2017-12-08 09:13:28.472257: step 68950, loss = 2.28, batch loss = 2.23 (36.2 examples/sec; 0.221 sec/batch; 16h:10m:21s remains)
INFO - root - 2017-12-08 09:13:30.686479: step 68960, loss = 2.28, batch loss = 2.23 (36.6 examples/sec; 0.218 sec/batch; 15h:58m:51s remains)
INFO - root - 2017-12-08 09:13:32.949602: step 68970, loss = 2.28, batch loss = 2.23 (35.0 examples/sec; 0.228 sec/batch; 16h:43m:32s remains)
INFO - root - 2017-12-08 09:13:35.178907: step 68980, loss = 2.28, batch loss = 2.23 (34.8 examples/sec; 0.230 sec/batch; 16h:50m:21s remains)
INFO - root - 2017-12-08 09:13:37.431303: step 68990, loss = 2.28, batch loss = 2.23 (34.4 examples/sec; 0.233 sec/batch; 17h:02m:26s remains)
INFO - root - 2017-12-08 09:13:39.685734: step 69000, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.221 sec/batch; 16h:12m:03s remains)
2017-12-08 09:13:39.961939: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4287934 -4.4287457 -4.42869 -4.428638 -4.428586 -4.4285479 -4.428524 -4.4285159 -4.4284945 -4.4285054 -4.4285707 -4.4286532 -4.4287295 -4.428802 -4.42884][-4.4287806 -4.4287343 -4.4286742 -4.4286218 -4.4285741 -4.4285421 -4.4285288 -4.4285364 -4.4285345 -4.4285464 -4.4285989 -4.4286737 -4.4287505 -4.4288254 -4.4288697][-4.4288144 -4.4287748 -4.4287128 -4.4286575 -4.4286041 -4.4285645 -4.4285393 -4.428545 -4.4285569 -4.4285755 -4.4286242 -4.4286942 -4.4287729 -4.4288464 -4.4288907][-4.4288163 -4.4287791 -4.4287152 -4.4286475 -4.4285765 -4.4285226 -4.4284883 -4.4284959 -4.42853 -4.4285684 -4.4286256 -4.4286981 -4.4287806 -4.428853 -4.4288864][-4.4287834 -4.4287372 -4.4286661 -4.4285851 -4.4284964 -4.42842 -4.4283729 -4.4283857 -4.4284449 -4.4285183 -4.4285946 -4.4286776 -4.4287634 -4.4288321 -4.4288511][-4.4287624 -4.42871 -4.4286389 -4.42855 -4.4284458 -4.4283428 -4.428268 -4.4282637 -4.4283342 -4.428442 -4.42855 -4.428648 -4.4287324 -4.4287844 -4.4287829][-4.4287834 -4.428731 -4.4286695 -4.4285884 -4.4284835 -4.4283643 -4.4282579 -4.4282146 -4.4282708 -4.4283915 -4.4285197 -4.4286308 -4.4287109 -4.4287462 -4.4287295][-4.4288311 -4.4287853 -4.4287376 -4.4286685 -4.42857 -4.428452 -4.4283338 -4.4282675 -4.4283004 -4.4284115 -4.4285369 -4.4286489 -4.4287243 -4.4287477 -4.42872][-4.4288826 -4.42885 -4.4288182 -4.4287615 -4.4286652 -4.4285517 -4.4284406 -4.4283752 -4.4283938 -4.4284854 -4.4285932 -4.4286885 -4.4287496 -4.4287643 -4.4287324][-4.42891 -4.4288921 -4.4288788 -4.428843 -4.428761 -4.4286575 -4.4285536 -4.4284897 -4.4284897 -4.4285612 -4.4286513 -4.4287286 -4.428771 -4.4287777 -4.4287477][-4.4289131 -4.4289002 -4.4289 -4.4288898 -4.4288363 -4.4287572 -4.4286685 -4.428606 -4.4285903 -4.4286389 -4.4287114 -4.428771 -4.4287953 -4.4287872 -4.428751][-4.4289079 -4.428896 -4.4288983 -4.4289041 -4.4288821 -4.4288335 -4.4287715 -4.4287229 -4.4287066 -4.4287419 -4.4287982 -4.4288397 -4.4288473 -4.4288192 -4.4287682][-4.4289136 -4.4289055 -4.4289045 -4.4289122 -4.4289064 -4.4288778 -4.4288378 -4.4288034 -4.4287963 -4.4288292 -4.4288759 -4.4289093 -4.4289131 -4.4288816 -4.4288282][-4.4289341 -4.4289317 -4.4289284 -4.4289293 -4.4289246 -4.4289012 -4.4288673 -4.4288383 -4.4288368 -4.4288678 -4.4289107 -4.4289446 -4.4289546 -4.4289384 -4.4288969][-4.428956 -4.4289584 -4.4289541 -4.4289484 -4.42894 -4.4289174 -4.4288859 -4.4288564 -4.42885 -4.4288688 -4.4289026 -4.4289336 -4.4289522 -4.4289536 -4.42893]]...]
INFO - root - 2017-12-08 09:13:42.190152: step 69010, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.222 sec/batch; 16h:13m:33s remains)
INFO - root - 2017-12-08 09:13:44.413125: step 69020, loss = 2.28, batch loss = 2.23 (37.0 examples/sec; 0.216 sec/batch; 15h:48m:48s remains)
INFO - root - 2017-12-08 09:13:46.646688: step 69030, loss = 2.28, batch loss = 2.23 (36.1 examples/sec; 0.222 sec/batch; 16h:12m:49s remains)
INFO - root - 2017-12-08 09:13:48.904207: step 69040, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.220 sec/batch; 16h:08m:08s remains)
INFO - root - 2017-12-08 09:13:51.137990: step 69050, loss = 2.28, batch loss = 2.23 (35.9 examples/sec; 0.223 sec/batch; 16h:17m:25s remains)
INFO - root - 2017-12-08 09:13:53.446755: step 69060, loss = 2.28, batch loss = 2.23 (35.7 examples/sec; 0.224 sec/batch; 16h:22m:54s remains)
INFO - root - 2017-12-08 09:13:55.697917: step 69070, loss = 2.28, batch loss = 2.23 (36.3 examples/sec; 0.220 sec/batch; 16h:06m:30s remains)
INFO - root - 2017-12-08 09:13:57.929303: step 69080, loss = 2.28, batch loss = 2.23 (35.7 examples/sec; 0.224 sec/batch; 16h:24m:09s remains)
INFO - root - 2017-12-08 09:14:00.160155: step 69090, loss = 2.28, batch loss = 2.23 (35.5 examples/sec; 0.226 sec/batch; 16h:30m:25s remains)
INFO - root - 2017-12-08 09:14:02.387010: step 69100, loss = 2.28, batch loss = 2.23 (36.4 examples/sec; 0.220 sec/batch; 16h:05m:20s remains)
2017-12-08 09:14:02.706342: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4287362 -4.4287248 -4.4287472 -4.4287882 -4.4288416 -4.4288845 -4.4288955 -4.4288673 -4.4288149 -4.4287596 -4.4287038 -4.4286647 -4.428678 -4.4287543 -4.4288445][-4.4287114 -4.4286976 -4.428709 -4.4287381 -4.4287863 -4.4288259 -4.42884 -4.428823 -4.4287834 -4.4287353 -4.4286737 -4.4286094 -4.4285979 -4.4286723 -4.4287748][-4.4286928 -4.4286785 -4.4286685 -4.4286714 -4.4287004 -4.4287219 -4.4287291 -4.4287276 -4.4287205 -4.4287057 -4.4286647 -4.4285984 -4.428566 -4.4286132 -4.4287014][-4.4287014 -4.4286928 -4.4286714 -4.4286456 -4.42864 -4.4286332 -4.4286222 -4.4286218 -4.4286447 -4.428678 -4.4286838 -4.4286489 -4.4286203 -4.4286375 -4.4286852][-4.4287295 -4.42874 -4.4287224 -4.4286776 -4.428637 -4.4285941 -4.4285374 -4.4284978 -4.428525 -4.4286151 -4.4286919 -4.4287071 -4.4286985 -4.4287052 -4.4287214][-4.4287105 -4.4287381 -4.4287343 -4.4286885 -4.4286356 -4.4285636 -4.4284558 -4.4283495 -4.4283543 -4.4285088 -4.4286695 -4.4287372 -4.4287519 -4.4287643 -4.4287763][-4.4285989 -4.4286504 -4.4286795 -4.4286585 -4.4286146 -4.4285326 -4.4283957 -4.4282384 -4.4282026 -4.4283972 -4.42863 -4.4287367 -4.4287591 -4.428772 -4.4287896][-4.4284821 -4.4285579 -4.4286265 -4.4286451 -4.4286275 -4.4285579 -4.4284244 -4.4282665 -4.4282022 -4.4283667 -4.4285941 -4.4287038 -4.4287109 -4.4286957 -4.428709][-4.4284987 -4.4285703 -4.4286494 -4.4286909 -4.4286928 -4.4286451 -4.4285393 -4.4284058 -4.4283237 -4.4283991 -4.4285636 -4.4286585 -4.4286308 -4.4285593 -4.4285426][-4.4286065 -4.4286556 -4.4287133 -4.4287448 -4.4287472 -4.42872 -4.4286509 -4.4285545 -4.4284711 -4.4284697 -4.4285564 -4.4286151 -4.4285574 -4.4284282 -4.4283605][-4.4286938 -4.4287257 -4.428761 -4.4287796 -4.4287848 -4.4287782 -4.4287438 -4.4286876 -4.428616 -4.4285746 -4.428597 -4.4286113 -4.4285331 -4.4283876 -4.4282837][-4.4287586 -4.4287767 -4.4287944 -4.428802 -4.4288058 -4.42881 -4.4288044 -4.4287815 -4.4287338 -4.4286914 -4.4286857 -4.4286723 -4.4285946 -4.4284744 -4.4283729][-4.4288454 -4.4288483 -4.4288492 -4.4288478 -4.4288406 -4.4288359 -4.4288363 -4.4288292 -4.4288 -4.4287758 -4.4287672 -4.4287448 -4.4286852 -4.4285989 -4.428514][-4.42889 -4.4288831 -4.4288769 -4.4288688 -4.428853 -4.4288387 -4.428834 -4.428833 -4.428813 -4.4287977 -4.4287972 -4.4287796 -4.4287376 -4.4286809 -4.4286132][-4.4289026 -4.4288945 -4.4288793 -4.4288621 -4.4288459 -4.42883 -4.4288206 -4.4288163 -4.4287906 -4.4287672 -4.42877 -4.4287758 -4.4287591 -4.4287257 -4.4286742]]...]
INFO - root - 2017-12-08 09:14:04.932006: step 69110, loss = 2.28, batch loss = 2.23 (35.8 examples/sec; 0.223 sec/batch; 16h:21m:01s remains)
INFO - root - 2017-12-08 09:14:07.182652: step 69120, loss = 2.28, batch loss = 2.23 (35.8 examples/sec; 0.223 sec/batch; 16h:20m:46s remains)
INFO - root - 2017-12-08 09:14:09.463963: step 69130, loss = 2.28, batch loss = 2.23 (34.3 examples/sec; 0.233 sec/batch; 17h:04m:27s remains)
INFO - root - 2017-12-08 09:14:11.727696: step 69140, loss = 2.28, batch loss = 2.23 (35.3 examples/sec; 0.227 sec/batch; 16h:35m:07s remains)
