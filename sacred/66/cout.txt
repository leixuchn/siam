INFO - Siam-FC - Running command 'main'
INFO - Siam-FC - Started run with ID "66"
INFO - root - Creating training directory: /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2
INFO - root - preproces -- siamese_fc_color
WARNING - root - root is not explicitly specified, using default value: None
INFO - root - For training, we use instance size 255 - 2 * 8 ...
WARNING - root - init_method is not explicitly specified, using default value: None
sdiufhasudf Tensor("siamese_fc/conv5/split:0", shape=(8, 8, 8, 192), dtype=float32)
Tensor("siamese_fc/conv5/def/transpose:0", shape=(8, 192, 8, 8), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/b1/weights:0' shape=(128, 192, 3, 3) dtype=float32_ref> Tensor("siamese_fc/conv5/def/transpose_1:0", shape=(8, 72, 8, 8), dtype=float32)
ddd Tensor("siamese_fc/conv5/def/b1/transpose:0", shape=(8, 6, 6, 128), dtype=float32)
Tensor("siamese_fc/conv5/def/transpose_2:0", shape=(8, 192, 8, 8), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/b2/weights:0' shape=(128, 192, 3, 3) dtype=float32_ref> Tensor("siamese_fc/conv5/def/transpose_3:0", shape=(8, 72, 8, 8), dtype=float32)
sdiufhasudf Tensor("siamese_fc_1/conv5/split:0", shape=(8, 22, 22, 192), dtype=float32)
Tensor("siamese_fc_1/conv5/def/transpose:0", shape=(8, 192, 22, 22), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/b1/weights:0' shape=(128, 192, 3, 3) dtype=float32_ref> Tensor("siamese_fc_1/conv5/def/transpose_1:0", shape=(8, 72, 22, 22), dtype=float32)
ddd Tensor("siamese_fc_1/conv5/def/b1/transpose:0", shape=(8, 20, 20, 128), dtype=float32)
Tensor("siamese_fc_1/conv5/def/transpose_2:0", shape=(8, 192, 22, 22), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/b2/weights:0' shape=(128, 192, 3, 3) dtype=float32_ref> Tensor("siamese_fc_1/conv5/def/transpose_3:0", shape=(8, 72, 22, 22), dtype=float32)
Tensor("detection/add:0", shape=(8, 15, 15), dtype=float32)
INFO - root - Model moving average is disabled since decay factor is 0
2017-12-05 08:37:17.197641: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-05 08:37:17.197682: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-05 08:37:17.197688: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2017-12-05 08:37:17.197692: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-05 08:37:17.197696: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
2017-12-05 08:37:17.755080: I tensorflow/core/common_runtime/gpu/gpu_device.cc:887] Found device 0 with properties: 
name: Tesla K80
major: 3 minor: 7 memoryClockRate (GHz) 0.8235
pciBusID 5791:00:00.0
Total memory: 11.17GiB
Free memory: 11.11GiB
2017-12-05 08:37:17.755116: I tensorflow/core/common_runtime/gpu/gpu_device.cc:908] DMA: 0 
2017-12-05 08:37:17.755123: I tensorflow/core/common_runtime/gpu/gpu_device.cc:918] 0:   Y 
2017-12-05 08:37:17.755135: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla K80, pci bus id: 5791:00:00.0)
INFO:tensorflow:Restoring parameters from /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-only/model.ckpt-150000
INFO - tensorflow - Restoring parameters from /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-only/model.ckpt-150000
INFO - root - Starting threads 0 ...

INFO - root - training for 332500 steps
INFO - root - 2017-12-05 08:37:21.066135: step 0, loss = 2.03, batch loss = 1.97 (3.2 examples/sec; 2.469 sec/batch; 228h:00m:41s remains)
2017-12-05 08:37:21.484104: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3269739 -4.3253407 -4.3271694 -4.3306451 -4.3299193 -4.3197207 -4.3007479 -4.2755709 -4.2506542 -4.2389431 -4.2416515 -4.25337 -4.2736063 -4.298665 -4.3179803][-4.3242307 -4.322753 -4.3260841 -4.3293982 -4.323112 -4.3005581 -4.2659812 -4.2264171 -4.1952052 -4.1888151 -4.202168 -4.2227464 -4.2499938 -4.2811832 -4.3058896][-4.3170238 -4.3160148 -4.3210821 -4.3213587 -4.3014 -4.2590246 -4.202086 -4.1421695 -4.10318 -4.1046634 -4.1345367 -4.1725221 -4.2149234 -4.2575865 -4.2913709][-4.302906 -4.3042545 -4.3121204 -4.3052707 -4.2668591 -4.2020421 -4.1194596 -4.0329514 -3.9802279 -3.9942856 -4.0479183 -4.1102929 -4.1763511 -4.2352829 -4.2793651][-4.2848363 -4.2888184 -4.2977686 -4.2824578 -4.226573 -4.1409693 -4.0306206 -3.9098845 -3.8440993 -3.8842368 -3.9685931 -4.0573807 -4.1456528 -4.2190561 -4.2725372][-4.266212 -4.2733221 -4.281137 -4.2557731 -4.1820631 -4.0714297 -3.9268332 -3.7667258 -3.6947289 -3.7796164 -3.9023163 -4.0170045 -4.1243825 -4.2080131 -4.2691164][-4.2513003 -4.2570162 -4.2589993 -4.2220554 -4.1346936 -4.00233 -3.8285677 -3.635778 -3.5817585 -3.7281718 -3.8868456 -4.01726 -4.1279659 -4.2107854 -4.2715774][-4.2429795 -4.2414494 -4.2324948 -4.1890521 -4.1005411 -3.9692104 -3.8000631 -3.6239467 -3.6229157 -3.7987366 -3.953877 -4.0669928 -4.1580935 -4.2287312 -4.2817674][-4.2357903 -4.2249813 -4.2092366 -4.1694822 -4.1000719 -4.0046539 -3.8864422 -3.7782564 -3.812597 -3.9515269 -4.0600576 -4.1315632 -4.1954451 -4.2508535 -4.2946482][-4.2296476 -4.2124538 -4.1946111 -4.1641626 -4.1213851 -4.0666504 -3.9999676 -3.9466078 -3.9834747 -4.0731611 -4.1382422 -4.1742749 -4.2181063 -4.2649083 -4.3031459][-4.2224436 -4.2037129 -4.1866364 -4.1662855 -4.1428037 -4.1169529 -4.0834966 -4.0604792 -4.087678 -4.1425343 -4.1793613 -4.1957579 -4.22999 -4.273778 -4.3089647][-4.2197485 -4.2050004 -4.192131 -4.18201 -4.1711164 -4.1629395 -4.1491003 -4.1408629 -4.1568975 -4.1845665 -4.2026043 -4.2111821 -4.2427621 -4.2839494 -4.3147526][-4.2281375 -4.2216578 -4.217473 -4.2159781 -4.2136264 -4.2152061 -4.2086959 -4.2011056 -4.2021136 -4.2069178 -4.2115765 -4.2162004 -4.2476687 -4.2886748 -4.3172936][-4.2392707 -4.240694 -4.2452083 -4.2491913 -4.2506943 -4.2546144 -4.246398 -4.2325559 -4.2195573 -4.2104864 -4.208322 -4.2127166 -4.2451382 -4.2873282 -4.3161159][-4.2362738 -4.2422247 -4.2529025 -4.2617345 -4.266387 -4.269773 -4.2556138 -4.2317467 -4.2080388 -4.194468 -4.1932688 -4.2015991 -4.236506 -4.280303 -4.3106146]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2/model.ckpt-0 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2/model.ckpt-0 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-05 08:37:24.291793: step 10, loss = 2.05, batch loss = 2.00 (29.5 examples/sec; 0.271 sec/batch; 25h:02m:24s remains)
INFO - root - 2017-12-05 08:37:26.414279: step 20, loss = 2.05, batch loss = 1.99 (40.5 examples/sec; 0.198 sec/batch; 18h:14m:37s remains)
INFO - root - 2017-12-05 08:37:28.509770: step 30, loss = 2.05, batch loss = 1.99 (39.8 examples/sec; 0.201 sec/batch; 18h:35m:07s remains)
INFO - root - 2017-12-05 08:37:30.671585: step 40, loss = 2.07, batch loss = 2.01 (34.3 examples/sec; 0.233 sec/batch; 21h:32m:45s remains)
INFO - root - 2017-12-05 08:37:32.703157: step 50, loss = 2.07, batch loss = 2.01 (40.2 examples/sec; 0.199 sec/batch; 18h:23m:54s remains)
INFO - root - 2017-12-05 08:37:34.761450: step 60, loss = 2.09, batch loss = 2.03 (38.6 examples/sec; 0.207 sec/batch; 19h:08m:25s remains)
INFO - root - 2017-12-05 08:37:36.821319: step 70, loss = 2.05, batch loss = 1.99 (38.5 examples/sec; 0.208 sec/batch; 19h:12m:25s remains)
INFO - root - 2017-12-05 08:37:38.925173: step 80, loss = 2.05, batch loss = 1.99 (38.9 examples/sec; 0.206 sec/batch; 18h:58m:53s remains)
INFO - root - 2017-12-05 08:37:40.955804: step 90, loss = 2.08, batch loss = 2.02 (39.0 examples/sec; 0.205 sec/batch; 18h:55m:50s remains)
INFO - root - 2017-12-05 08:37:42.993343: step 100, loss = 2.05, batch loss = 1.99 (38.9 examples/sec; 0.206 sec/batch; 18h:58m:46s remains)
2017-12-05 08:37:43.267823: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2291861 -4.216763 -4.2172961 -4.220583 -4.2091169 -4.18973 -4.1835828 -4.1802344 -4.1729512 -4.1714625 -4.2014031 -4.24357 -4.2608318 -4.2494698 -4.2277417][-4.252111 -4.239881 -4.2345381 -4.2287135 -4.2033043 -4.1769638 -4.1712179 -4.1745996 -4.1730952 -4.1731958 -4.1978064 -4.2394896 -4.2599931 -4.2490563 -4.2308664][-4.27745 -4.2589073 -4.2410069 -4.2211571 -4.1839356 -4.1585836 -4.1615067 -4.1756296 -4.1820583 -4.1811733 -4.1969256 -4.2321658 -4.2534962 -4.247613 -4.2370687][-4.2788253 -4.252624 -4.2203026 -4.1858196 -4.1388259 -4.1157012 -4.1349468 -4.1657305 -4.18245 -4.1865525 -4.19959 -4.2310929 -4.2524471 -4.2517347 -4.2478409][-4.2487936 -4.2140279 -4.1678534 -4.1185956 -4.06307 -4.0445514 -4.0836258 -4.1388249 -4.16928 -4.1824231 -4.2013407 -4.2326493 -4.2513704 -4.251792 -4.2510953][-4.2110639 -4.1711807 -4.1143289 -4.0576057 -4.0007358 -3.9812939 -4.0331535 -4.1107521 -4.154531 -4.1762247 -4.1999292 -4.22484 -4.2375379 -4.2369552 -4.2393861][-4.192071 -4.153213 -4.0945821 -4.0394387 -3.9918418 -3.9704347 -4.0185118 -4.0999308 -4.1479139 -4.1743231 -4.1959262 -4.2127128 -4.2179766 -4.2164669 -4.22108][-4.2025285 -4.1746407 -4.1270661 -4.0831008 -4.04938 -4.0279779 -4.0630851 -4.1276264 -4.165719 -4.1858654 -4.2009869 -4.2119417 -4.2118297 -4.2069888 -4.2108431][-4.2399182 -4.2251563 -4.1952529 -4.1662893 -4.1455836 -4.1271577 -4.1455808 -4.1858845 -4.205132 -4.2130928 -4.2214322 -4.2275548 -4.2215142 -4.2099471 -4.2091475][-4.2784123 -4.2727895 -4.2565928 -4.2405214 -4.22665 -4.2060556 -4.2082472 -4.22629 -4.2318478 -4.2325211 -4.2391191 -4.2446737 -4.2361946 -4.2193394 -4.2110386][-4.2971497 -4.2954707 -4.2867808 -4.279449 -4.2708168 -4.2496495 -4.2431288 -4.2484975 -4.2470188 -4.2452917 -4.2536192 -4.2602377 -4.2508626 -4.2288785 -4.2096868][-4.2907352 -4.2907329 -4.2863884 -4.2854114 -4.28277 -4.2655258 -4.256845 -4.2557106 -4.2504158 -4.24849 -4.2561889 -4.2630019 -4.2528782 -4.2267447 -4.2010388][-4.2813883 -4.2812819 -4.2795038 -4.2829595 -4.2865505 -4.27543 -4.2656131 -4.2595258 -4.2531786 -4.2514057 -4.2584066 -4.263032 -4.2489181 -4.2233043 -4.1956429][-4.2906928 -4.2900252 -4.2907739 -4.2952781 -4.2998791 -4.2936559 -4.2848 -4.2773023 -4.269455 -4.2666512 -4.2699819 -4.2670918 -4.2473745 -4.224596 -4.1962738][-4.3020649 -4.3007703 -4.302433 -4.3061237 -4.3098679 -4.3069243 -4.3008175 -4.2938104 -4.2865086 -4.2836328 -4.2834959 -4.2748451 -4.2528338 -4.2339292 -4.206778]]...]
INFO - root - 2017-12-05 08:37:45.287697: step 110, loss = 2.07, batch loss = 2.01 (39.7 examples/sec; 0.202 sec/batch; 18h:36m:17s remains)
INFO - root - 2017-12-05 08:37:47.331152: step 120, loss = 2.04, batch loss = 1.98 (39.5 examples/sec; 0.202 sec/batch; 18h:40m:33s remains)
INFO - root - 2017-12-05 08:37:49.424107: step 130, loss = 2.05, batch loss = 1.99 (39.0 examples/sec; 0.205 sec/batch; 18h:56m:18s remains)
INFO - root - 2017-12-05 08:37:51.481949: step 140, loss = 2.05, batch loss = 1.99 (39.1 examples/sec; 0.205 sec/batch; 18h:53m:06s remains)
INFO - root - 2017-12-05 08:37:53.522183: step 150, loss = 2.09, batch loss = 2.03 (38.5 examples/sec; 0.208 sec/batch; 19h:09m:35s remains)
INFO - root - 2017-12-05 08:37:55.559330: step 160, loss = 2.04, batch loss = 1.98 (38.1 examples/sec; 0.210 sec/batch; 19h:21m:41s remains)
INFO - root - 2017-12-05 08:37:57.599851: step 170, loss = 2.07, batch loss = 2.01 (37.7 examples/sec; 0.212 sec/batch; 19h:34m:35s remains)
INFO - root - 2017-12-05 08:37:59.629887: step 180, loss = 2.04, batch loss = 1.99 (38.8 examples/sec; 0.206 sec/batch; 19h:01m:49s remains)
INFO - root - 2017-12-05 08:38:01.694265: step 190, loss = 2.07, batch loss = 2.01 (37.9 examples/sec; 0.211 sec/batch; 19h:28m:17s remains)
INFO - root - 2017-12-05 08:38:03.728555: step 200, loss = 2.05, batch loss = 1.99 (38.3 examples/sec; 0.209 sec/batch; 19h:15m:42s remains)
2017-12-05 08:38:04.047642: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3199196 -4.3133712 -4.3095531 -4.3097186 -4.3138084 -4.318954 -4.3229389 -4.3240032 -4.32432 -4.3257322 -4.3303585 -4.3365507 -4.3425603 -4.3463116 -4.3458014][-4.3056293 -4.2955523 -4.2895856 -4.2921567 -4.2995391 -4.3037004 -4.3024254 -4.29695 -4.2928138 -4.2930646 -4.3019748 -4.31579 -4.32899 -4.3365512 -4.3368788][-4.2895317 -4.2768307 -4.2724514 -4.27826 -4.2850876 -4.2838292 -4.2742672 -4.2576818 -4.2470632 -4.2497196 -4.2686567 -4.2932291 -4.3157997 -4.3272982 -4.3289251][-4.2785435 -4.2652931 -4.2639651 -4.2690792 -4.266386 -4.2537537 -4.2323709 -4.2001729 -4.1812029 -4.1910925 -4.2244377 -4.2608342 -4.292695 -4.3111415 -4.3175178][-4.2732964 -4.2601433 -4.2576432 -4.2555895 -4.2391229 -4.2119164 -4.1741276 -4.1238427 -4.1020207 -4.1266532 -4.1771393 -4.2230706 -4.2625256 -4.2897449 -4.3033772][-4.2497211 -4.2313523 -4.2205248 -4.2090774 -4.1795244 -4.1361251 -4.07763 -4.0149789 -4.0072064 -4.0593247 -4.1277161 -4.1800847 -4.2279348 -4.266602 -4.2897019][-4.2013788 -4.1705284 -4.1456718 -4.1206145 -4.076745 -4.0139112 -3.9358079 -3.8738756 -3.9014983 -3.991586 -4.0754619 -4.1366453 -4.1958389 -4.2476358 -4.279954][-4.1540914 -4.1088891 -4.0678473 -4.0263977 -3.9695985 -3.8922062 -3.8054252 -3.7596366 -3.8295021 -3.951309 -4.0459824 -4.11615 -4.1856103 -4.24409 -4.2785616][-4.106432 -4.0562487 -4.013638 -3.97539 -3.9272547 -3.8670099 -3.8052905 -3.7892773 -3.8630519 -3.9750457 -4.0635152 -4.1343174 -4.2042637 -4.2587686 -4.2875967][-4.0802879 -4.0384417 -4.0117326 -3.9985111 -3.979697 -3.950954 -3.9203072 -3.9197655 -3.97259 -4.0490317 -4.1153431 -4.1759911 -4.2351265 -4.2788429 -4.300107][-4.0899687 -4.065464 -4.0620675 -4.0731359 -4.081347 -4.0709605 -4.0548806 -4.057168 -4.089355 -4.1331286 -4.1756105 -4.220664 -4.2651749 -4.2985511 -4.3140464][-4.1355433 -4.1290708 -4.1417184 -4.163341 -4.1802697 -4.17701 -4.1670718 -4.1696258 -4.1868834 -4.2089148 -4.2323427 -4.2629523 -4.2960734 -4.3203588 -4.3305621][-4.20223 -4.2042332 -4.2183328 -4.2368388 -4.248816 -4.2442045 -4.235219 -4.2362876 -4.2454333 -4.2580533 -4.2727017 -4.2952023 -4.3208456 -4.3381925 -4.3445182][-4.2602963 -4.2650638 -4.2733922 -4.2813935 -4.2837443 -4.2761769 -4.2675562 -4.2676415 -4.2735338 -4.2825437 -4.2934656 -4.311008 -4.3319345 -4.3451438 -4.3491979][-4.300684 -4.3034711 -4.3066821 -4.3074026 -4.3049579 -4.2991209 -4.2947373 -4.2956729 -4.298831 -4.3029394 -4.30964 -4.322032 -4.3368177 -4.3456087 -4.3475533]]...]
INFO - root - 2017-12-05 08:38:06.120531: step 210, loss = 2.04, batch loss = 1.98 (38.6 examples/sec; 0.207 sec/batch; 19h:06m:21s remains)
INFO - root - 2017-12-05 08:38:08.180093: step 220, loss = 2.07, batch loss = 2.01 (39.6 examples/sec; 0.202 sec/batch; 18h:37m:59s remains)
INFO - root - 2017-12-05 08:38:10.234093: step 230, loss = 2.05, batch loss = 1.99 (39.1 examples/sec; 0.205 sec/batch; 18h:53m:14s remains)
INFO - root - 2017-12-05 08:38:12.283443: step 240, loss = 2.03, batch loss = 1.97 (38.1 examples/sec; 0.210 sec/batch; 19h:22m:41s remains)
INFO - root - 2017-12-05 08:38:14.307466: step 250, loss = 2.05, batch loss = 1.99 (39.9 examples/sec; 0.201 sec/batch; 18h:30m:50s remains)
INFO - root - 2017-12-05 08:38:16.378529: step 260, loss = 2.04, batch loss = 1.98 (35.1 examples/sec; 0.228 sec/batch; 21h:01m:38s remains)
INFO - root - 2017-12-05 08:38:18.407603: step 270, loss = 2.07, batch loss = 2.01 (38.7 examples/sec; 0.207 sec/batch; 19h:05m:12s remains)
INFO - root - 2017-12-05 08:38:20.449403: step 280, loss = 2.03, batch loss = 1.98 (37.7 examples/sec; 0.212 sec/batch; 19h:35m:34s remains)
INFO - root - 2017-12-05 08:38:22.513078: step 290, loss = 2.06, batch loss = 2.00 (38.2 examples/sec; 0.209 sec/batch; 19h:19m:43s remains)
INFO - root - 2017-12-05 08:38:24.572860: step 300, loss = 2.05, batch loss = 1.99 (39.7 examples/sec; 0.201 sec/batch; 18h:34m:56s remains)
2017-12-05 08:38:24.889123: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1796727 -4.1632924 -4.1423745 -4.1198549 -4.1119232 -4.1223946 -4.1333165 -4.1390233 -4.1444945 -4.146697 -4.1624713 -4.1798921 -4.1845918 -4.166069 -4.1366296][-4.1639938 -4.1526451 -4.1471658 -4.1373363 -4.1269283 -4.1274385 -4.1275554 -4.1226549 -4.1295671 -4.1419516 -4.1607413 -4.1759005 -4.1766067 -4.1590552 -4.1362758][-4.146081 -4.1422687 -4.1534352 -4.1595483 -4.1534152 -4.1471295 -4.134737 -4.1181092 -4.1220622 -4.1379266 -4.153029 -4.16179 -4.1586657 -4.1441 -4.1335998][-4.1239147 -4.1233273 -4.1474419 -4.1681309 -4.1683946 -4.159833 -4.1409082 -4.1152158 -4.1077542 -4.1123924 -4.1161709 -4.11908 -4.1229258 -4.1233091 -4.1316252][-4.1143336 -4.1126571 -4.1396313 -4.170785 -4.1779528 -4.1662107 -4.1398773 -4.106297 -4.0813761 -4.064867 -4.0534949 -4.0584326 -4.079555 -4.1028671 -4.1278419][-4.1306677 -4.1262403 -4.1485333 -4.1770978 -4.1849623 -4.1687956 -4.133976 -4.088057 -4.0435171 -4.0075526 -3.9810128 -3.9853516 -4.0213556 -4.0688243 -4.1118212][-4.1550455 -4.1497669 -4.165946 -4.1884537 -4.1942396 -4.1761317 -4.13378 -4.0765915 -4.0185575 -3.9712405 -3.9307609 -3.9266086 -3.9634385 -4.0257616 -4.09071][-4.1755924 -4.1690693 -4.1802897 -4.198915 -4.2102261 -4.1993036 -4.1614184 -4.1029577 -4.0426092 -3.9910235 -3.942709 -3.9225581 -3.9418967 -4.0020032 -4.0770121][-4.2096834 -4.2000561 -4.2037897 -4.2152638 -4.2255917 -4.2225389 -4.1974888 -4.152009 -4.1023512 -4.0542812 -4.0059142 -3.9753706 -3.9731359 -4.0145273 -4.0786562][-4.2527862 -4.2441864 -4.2429852 -4.2455411 -4.2509966 -4.2526097 -4.2400317 -4.2127852 -4.1795917 -4.1379786 -4.0896974 -4.0528154 -4.0346408 -4.0498762 -4.0898623][-4.2845659 -4.283699 -4.284461 -4.2844768 -4.2859459 -4.2894006 -4.2839689 -4.266634 -4.2448411 -4.2118134 -4.1676908 -4.1269369 -4.09615 -4.086585 -4.0988808][-4.2889504 -4.2960186 -4.3025937 -4.3043714 -4.3065782 -4.3109832 -4.3101249 -4.2981505 -4.2829537 -4.2570672 -4.2209163 -4.1825581 -4.147984 -4.1252627 -4.1172047][-4.2707586 -4.2806463 -4.2925229 -4.300127 -4.3075337 -4.3178549 -4.3231993 -4.3192887 -4.3101168 -4.2895269 -4.2600012 -4.2272663 -4.1966615 -4.1703267 -4.1464515][-4.2618966 -4.2720757 -4.2846117 -4.2927737 -4.3008037 -4.3128047 -4.322361 -4.3268075 -4.3251619 -4.3108754 -4.2895231 -4.265317 -4.23945 -4.2113361 -4.1790819][-4.2661161 -4.2754679 -4.286799 -4.2910724 -4.2943797 -4.3017168 -4.3106818 -4.3179483 -4.3190856 -4.3101358 -4.2974157 -4.2843676 -4.2667694 -4.2420511 -4.2061744]]...]
INFO - root - 2017-12-05 08:38:26.919903: step 310, loss = 2.05, batch loss = 1.99 (40.2 examples/sec; 0.199 sec/batch; 18h:22m:13s remains)
INFO - root - 2017-12-05 08:38:28.957057: step 320, loss = 2.02, batch loss = 1.96 (39.7 examples/sec; 0.201 sec/batch; 18h:34m:30s remains)
INFO - root - 2017-12-05 08:38:30.986572: step 330, loss = 2.06, batch loss = 2.00 (38.9 examples/sec; 0.206 sec/batch; 18h:59m:00s remains)
INFO - root - 2017-12-05 08:38:33.026478: step 340, loss = 2.03, batch loss = 1.98 (39.7 examples/sec; 0.201 sec/batch; 18h:34m:38s remains)
INFO - root - 2017-12-05 08:38:35.081116: step 350, loss = 2.05, batch loss = 1.99 (39.3 examples/sec; 0.204 sec/batch; 18h:47m:09s remains)
INFO - root - 2017-12-05 08:38:37.118331: step 360, loss = 2.07, batch loss = 2.01 (39.6 examples/sec; 0.202 sec/batch; 18h:39m:25s remains)
INFO - root - 2017-12-05 08:38:39.189883: step 370, loss = 2.05, batch loss = 1.99 (38.9 examples/sec; 0.206 sec/batch; 18h:58m:10s remains)
INFO - root - 2017-12-05 08:38:41.264316: step 380, loss = 2.04, batch loss = 1.98 (38.6 examples/sec; 0.207 sec/batch; 19h:07m:54s remains)
INFO - root - 2017-12-05 08:38:43.335188: step 390, loss = 2.02, batch loss = 1.96 (39.1 examples/sec; 0.205 sec/batch; 18h:53m:06s remains)
INFO - root - 2017-12-05 08:38:45.411582: step 400, loss = 2.03, batch loss = 1.97 (39.1 examples/sec; 0.205 sec/batch; 18h:53m:41s remains)
2017-12-05 08:38:45.672614: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2845874 -4.2759748 -4.26721 -4.2557535 -4.2428203 -4.2388134 -4.2457833 -4.2584991 -4.2674551 -4.2674975 -4.2647228 -4.2589192 -4.2453413 -4.2275815 -4.2114782][-4.2855282 -4.281177 -4.2715659 -4.2567382 -4.2414317 -4.2366166 -4.2469597 -4.267055 -4.2845087 -4.2926183 -4.2967582 -4.294301 -4.2779522 -4.249227 -4.21625][-4.271204 -4.2672176 -4.2531462 -4.2320995 -4.2131276 -4.2053909 -4.2121058 -4.2349558 -4.2622933 -4.285449 -4.3030868 -4.3101072 -4.2992892 -4.2688236 -4.222816][-4.2546039 -4.2429814 -4.2173018 -4.1862364 -4.1628766 -4.1493578 -4.146472 -4.1641245 -4.1979327 -4.2365808 -4.2710252 -4.2924604 -4.2949967 -4.2729278 -4.2261763][-4.2467604 -4.2249904 -4.1862988 -4.1434402 -4.1109238 -4.0869427 -4.0711813 -4.0778003 -4.1108513 -4.16414 -4.2161808 -4.2531981 -4.272789 -4.264534 -4.225976][-4.2405796 -4.2141137 -4.1668649 -4.1124496 -4.0672431 -4.0279641 -3.9917235 -3.9772379 -4.0067229 -4.0760465 -4.1490889 -4.2028437 -4.2399678 -4.2510481 -4.2310257][-4.2399774 -4.2157874 -4.1695232 -4.1059909 -4.0435386 -3.979944 -3.9083042 -3.856688 -3.8778329 -3.9697504 -4.0670509 -4.1410217 -4.1977878 -4.2307057 -4.2369404][-4.2473707 -4.2278781 -4.1898136 -4.13017 -4.0636058 -3.9869258 -3.8869851 -3.7932863 -3.7898638 -3.8855724 -3.9916606 -4.0787907 -4.1515589 -4.2038555 -4.2349944][-4.2592311 -4.2464066 -4.2217288 -4.1801853 -4.1294155 -4.0667067 -3.9793558 -3.8872917 -3.8620505 -3.9131234 -3.9807844 -4.0479865 -4.1197095 -4.1835113 -4.23333][-4.2768 -4.26949 -4.257822 -4.2370872 -4.208375 -4.1703711 -4.1108012 -4.0411086 -4.0081673 -4.0162749 -4.0413442 -4.0766525 -4.1296358 -4.1873832 -4.2394657][-4.2947364 -4.2906351 -4.2871189 -4.2821388 -4.2724276 -4.2573938 -4.2288218 -4.1853795 -4.1564474 -4.1431108 -4.14125 -4.152554 -4.183805 -4.2239323 -4.2640591][-4.3085608 -4.307724 -4.3050904 -4.3050785 -4.3062735 -4.3070683 -4.3010468 -4.282197 -4.2647257 -4.2499728 -4.2377243 -4.2346 -4.2465596 -4.2667952 -4.2906723][-4.3171062 -4.3194971 -4.3167777 -4.3165507 -4.3214741 -4.3286572 -4.3335619 -4.3277721 -4.3187737 -4.3080034 -4.2967372 -4.2900143 -4.2910213 -4.2974811 -4.3084335][-4.3214312 -4.323329 -4.3203521 -4.31856 -4.3226047 -4.3302975 -4.3376818 -4.3392167 -4.3359084 -4.3302517 -4.3232193 -4.3168306 -4.3132515 -4.3132439 -4.3177438][-4.3241868 -4.3243613 -4.3204288 -4.3165064 -4.3171506 -4.3216033 -4.327723 -4.3317075 -4.3327723 -4.3317633 -4.3292942 -4.3256574 -4.322823 -4.3222036 -4.3234272]]...]
INFO - root - 2017-12-05 08:38:47.731467: step 410, loss = 2.06, batch loss = 2.00 (39.1 examples/sec; 0.204 sec/batch; 18h:51m:40s remains)
INFO - root - 2017-12-05 08:38:49.799553: step 420, loss = 2.03, batch loss = 1.97 (39.3 examples/sec; 0.203 sec/batch; 18h:45m:14s remains)
INFO - root - 2017-12-05 08:38:51.860320: step 430, loss = 2.06, batch loss = 2.01 (39.8 examples/sec; 0.201 sec/batch; 18h:31m:27s remains)
INFO - root - 2017-12-05 08:38:53.940569: step 440, loss = 2.04, batch loss = 1.98 (39.3 examples/sec; 0.204 sec/batch; 18h:47m:04s remains)
INFO - root - 2017-12-05 08:38:55.998357: step 450, loss = 2.05, batch loss = 1.99 (39.2 examples/sec; 0.204 sec/batch; 18h:49m:30s remains)
INFO - root - 2017-12-05 08:38:58.076122: step 460, loss = 2.07, batch loss = 2.01 (38.5 examples/sec; 0.208 sec/batch; 19h:08m:55s remains)
INFO - root - 2017-12-05 08:39:00.110337: step 470, loss = 2.03, batch loss = 1.97 (38.1 examples/sec; 0.210 sec/batch; 19h:22m:20s remains)
INFO - root - 2017-12-05 08:39:02.168632: step 480, loss = 2.03, batch loss = 1.97 (38.5 examples/sec; 0.208 sec/batch; 19h:08m:59s remains)
INFO - root - 2017-12-05 08:39:04.245523: step 490, loss = 2.02, batch loss = 1.97 (39.5 examples/sec; 0.202 sec/batch; 18h:40m:23s remains)
INFO - root - 2017-12-05 08:39:06.301882: step 500, loss = 2.05, batch loss = 1.99 (38.4 examples/sec; 0.208 sec/batch; 19h:11m:55s remains)
2017-12-05 08:39:06.601066: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.110662 -4.1472454 -4.1971941 -4.2435727 -4.2738018 -4.2653265 -4.2272325 -4.1912909 -4.169322 -4.1698513 -4.1913157 -4.2124033 -4.2269006 -4.2357588 -4.2247925][-4.0936012 -4.1449203 -4.2027097 -4.2488637 -4.2754474 -4.2611718 -4.2155437 -4.1720562 -4.1445217 -4.1451058 -4.17211 -4.2005782 -4.2230997 -4.2368312 -4.2335262][-4.105258 -4.1659288 -4.222518 -4.2597656 -4.277174 -4.256629 -4.2049437 -4.1514282 -4.1151443 -4.1153407 -4.1541557 -4.1945977 -4.2277193 -4.2506227 -4.2599015][-4.1421351 -4.1980438 -4.2394528 -4.2604 -4.2633438 -4.2339768 -4.1763735 -4.1132226 -4.0711179 -4.0797596 -4.1364522 -4.19003 -4.229991 -4.2582994 -4.278379][-4.1812472 -4.2180095 -4.2410445 -4.2454863 -4.2313285 -4.1893229 -4.1192842 -4.0429635 -4.005651 -4.0359488 -4.1133947 -4.179882 -4.2237439 -4.2512708 -4.2745762][-4.2077036 -4.21904 -4.226243 -4.2148104 -4.1844554 -4.1271744 -4.0319023 -3.9391723 -3.9205925 -3.9886231 -4.0924449 -4.1698122 -4.2127519 -4.236793 -4.2564592][-4.2138634 -4.2045741 -4.2024288 -4.1842437 -4.1401348 -4.0626564 -3.9379482 -3.8275259 -3.8413162 -3.9516051 -4.0731053 -4.1526814 -4.1944375 -4.2152267 -4.2301831][-4.1871457 -4.1646519 -4.1649957 -4.1519666 -4.1029263 -4.018311 -3.88621 -3.7852468 -3.8378277 -3.9638135 -4.0808673 -4.1507864 -4.1848435 -4.1976786 -4.2048445][-4.1467166 -4.1223731 -4.1335783 -4.128109 -4.0857005 -4.0122213 -3.9064569 -3.8516645 -3.9202461 -4.024086 -4.1155591 -4.1674933 -4.1881495 -4.1919904 -4.1920109][-4.1239853 -4.1030469 -4.1204205 -4.1220746 -4.0890441 -4.0345836 -3.9660172 -3.9502771 -4.0159707 -4.0906367 -4.154335 -4.1888018 -4.2003651 -4.1998339 -4.1973257][-4.11557 -4.09893 -4.1182122 -4.1290913 -4.1085525 -4.0743814 -4.0326104 -4.0334644 -4.0821805 -4.1314931 -4.1729078 -4.1984038 -4.2079496 -4.2091422 -4.2096248][-4.1150279 -4.102869 -4.1243205 -4.1401877 -4.1344934 -4.118587 -4.0915775 -4.0903525 -4.1189737 -4.1498127 -4.1761394 -4.1976385 -4.2103839 -4.2170997 -4.2213693][-4.1295538 -4.116962 -4.1359034 -4.1540318 -4.160079 -4.1562309 -4.1367612 -4.1308212 -4.1441045 -4.1637473 -4.1834598 -4.204792 -4.2195086 -4.2264876 -4.2304378][-4.1737804 -4.1598616 -4.1730185 -4.1905575 -4.1991305 -4.1987886 -4.1843882 -4.176755 -4.183084 -4.19679 -4.2123952 -4.230422 -4.2434587 -4.2482977 -4.2513075][-4.23451 -4.2206421 -4.2283621 -4.2391796 -4.2448082 -4.2437611 -4.232944 -4.2263002 -4.2318163 -4.2411318 -4.252387 -4.2647185 -4.2728715 -4.2738366 -4.2755136]]...]
INFO - root - 2017-12-05 08:39:08.699950: step 510, loss = 2.07, batch loss = 2.01 (37.7 examples/sec; 0.212 sec/batch; 19h:32m:35s remains)
INFO - root - 2017-12-05 08:39:10.787952: step 520, loss = 2.05, batch loss = 1.99 (38.9 examples/sec; 0.206 sec/batch; 18h:58m:39s remains)
INFO - root - 2017-12-05 08:39:12.881126: step 530, loss = 2.03, batch loss = 1.97 (38.3 examples/sec; 0.209 sec/batch; 19h:17m:07s remains)
INFO - root - 2017-12-05 08:39:14.919389: step 540, loss = 2.03, batch loss = 1.97 (39.3 examples/sec; 0.204 sec/batch; 18h:46m:28s remains)
INFO - root - 2017-12-05 08:39:16.960321: step 550, loss = 2.04, batch loss = 1.98 (39.3 examples/sec; 0.203 sec/batch; 18h:45m:04s remains)
INFO - root - 2017-12-05 08:39:18.995575: step 560, loss = 2.03, batch loss = 1.97 (39.7 examples/sec; 0.201 sec/batch; 18h:33m:47s remains)
INFO - root - 2017-12-05 08:39:21.071698: step 570, loss = 2.03, batch loss = 1.98 (37.1 examples/sec; 0.216 sec/batch; 19h:53m:12s remains)
INFO - root - 2017-12-05 08:39:23.133685: step 580, loss = 2.04, batch loss = 1.98 (39.4 examples/sec; 0.203 sec/batch; 18h:42m:06s remains)
INFO - root - 2017-12-05 08:39:25.183409: step 590, loss = 2.05, batch loss = 1.99 (39.6 examples/sec; 0.202 sec/batch; 18h:38m:23s remains)
INFO - root - 2017-12-05 08:39:27.232849: step 600, loss = 2.01, batch loss = 1.95 (39.4 examples/sec; 0.203 sec/batch; 18h:43m:46s remains)
2017-12-05 08:39:27.513800: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1670637 -4.1485691 -4.1443319 -4.1405945 -4.1403909 -4.1398778 -4.1481624 -4.1748419 -4.2112107 -4.2360978 -4.2514453 -4.2520351 -4.2266808 -4.1920924 -4.1787553][-4.1609716 -4.1501431 -4.1511745 -4.1505423 -4.1496215 -4.1477284 -4.1554852 -4.1802921 -4.2153044 -4.2366905 -4.2481246 -4.2482 -4.2290812 -4.2052784 -4.201272][-4.14635 -4.1344223 -4.1384439 -4.1471257 -4.1553078 -4.156599 -4.1644974 -4.1880774 -4.216784 -4.2292776 -4.2322383 -4.2275009 -4.2115984 -4.1970334 -4.2038188][-4.1280107 -4.1134315 -4.1156573 -4.1247416 -4.1338129 -4.13083 -4.1332011 -4.1504922 -4.172121 -4.1801691 -4.1828656 -4.1835523 -4.1748433 -4.1668963 -4.1804223][-4.1052475 -4.0936875 -4.0962992 -4.1059022 -4.1101227 -4.0953689 -4.0846815 -4.0940976 -4.1122088 -4.1239552 -4.1343536 -4.1435046 -4.1404943 -4.1313386 -4.1396995][-4.0730057 -4.0654941 -4.0694828 -4.0731525 -4.0634441 -4.0314097 -4.0092831 -4.0174975 -4.0435309 -4.0745506 -4.09877 -4.1104221 -4.102283 -4.084208 -4.0804625][-4.0595884 -4.0541191 -4.0504632 -4.0326929 -3.9955218 -3.9393835 -3.9030631 -3.91647 -3.9653654 -4.0191317 -4.0509868 -4.0573545 -4.0360336 -4.0049291 -3.9948401][-4.0627046 -4.0569739 -4.0474544 -4.0124493 -3.9531257 -3.8770876 -3.8315353 -3.8498204 -3.9104757 -3.9669845 -3.990088 -3.9856944 -3.9548635 -3.9186077 -3.9180539][-4.0826511 -4.0736637 -4.0609541 -4.0261149 -3.964922 -3.8888168 -3.8457637 -3.8587728 -3.9039021 -3.9375703 -3.9415243 -3.923728 -3.8903735 -3.8678732 -3.8938408][-4.1179185 -4.1120067 -4.1059947 -4.0841794 -4.0368552 -3.9770119 -3.9421654 -3.9441381 -3.9647532 -3.9759872 -3.9659157 -3.9399521 -3.9068701 -3.8946409 -3.9344816][-4.1532083 -4.1525822 -4.1543064 -4.1467009 -4.1202455 -4.0840063 -4.0630541 -4.0614171 -4.0683002 -4.0672398 -4.0504084 -4.0244279 -3.9955528 -3.9879448 -4.0204949][-4.1790195 -4.1781492 -4.1790686 -4.1795535 -4.1717558 -4.1595249 -4.1572795 -4.1613903 -4.1644497 -4.1604886 -4.1459394 -4.1247072 -4.1007614 -4.0934095 -4.1116977][-4.1849942 -4.1823797 -4.1802692 -4.1842394 -4.18947 -4.1975188 -4.2104588 -4.2218761 -4.2273679 -4.2252979 -4.2155166 -4.2008138 -4.1822748 -4.1754994 -4.1837368][-4.1808023 -4.1823187 -4.1782851 -4.1801934 -4.191474 -4.2115889 -4.2333164 -4.2472448 -4.2535033 -4.2529483 -4.2460132 -4.235568 -4.2217927 -4.2165313 -4.2202158][-4.1890917 -4.1933589 -4.186841 -4.1857581 -4.1979403 -4.2192726 -4.2396526 -4.2513986 -4.2550683 -4.2519374 -4.2442575 -4.2351341 -4.2236137 -4.2205987 -4.2247934]]...]
INFO - root - 2017-12-05 08:39:29.585126: step 610, loss = 2.07, batch loss = 2.01 (38.8 examples/sec; 0.206 sec/batch; 19h:01m:03s remains)
INFO - root - 2017-12-05 08:39:31.668306: step 620, loss = 2.01, batch loss = 1.96 (39.5 examples/sec; 0.202 sec/batch; 18h:39m:58s remains)
INFO - root - 2017-12-05 08:39:33.738531: step 630, loss = 2.04, batch loss = 1.98 (37.8 examples/sec; 0.211 sec/batch; 19h:29m:26s remains)
INFO - root - 2017-12-05 08:39:35.843828: step 640, loss = 2.05, batch loss = 1.99 (37.4 examples/sec; 0.214 sec/batch; 19h:44m:15s remains)
INFO - root - 2017-12-05 08:39:37.901121: step 650, loss = 2.04, batch loss = 1.98 (39.5 examples/sec; 0.203 sec/batch; 18h:40m:47s remains)
INFO - root - 2017-12-05 08:39:39.973189: step 660, loss = 2.02, batch loss = 1.96 (39.5 examples/sec; 0.203 sec/batch; 18h:41m:29s remains)
INFO - root - 2017-12-05 08:39:42.056696: step 670, loss = 2.05, batch loss = 1.99 (39.1 examples/sec; 0.204 sec/batch; 18h:50m:12s remains)
INFO - root - 2017-12-05 08:39:44.116253: step 680, loss = 2.05, batch loss = 1.99 (39.0 examples/sec; 0.205 sec/batch; 18h:53m:54s remains)
INFO - root - 2017-12-05 08:39:46.189704: step 690, loss = 2.03, batch loss = 1.97 (38.6 examples/sec; 0.208 sec/batch; 19h:07m:32s remains)
INFO - root - 2017-12-05 08:39:48.262668: step 700, loss = 2.03, batch loss = 1.97 (39.5 examples/sec; 0.203 sec/batch; 18h:39m:54s remains)
2017-12-05 08:39:48.541956: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2014031 -4.19378 -4.1908059 -4.1878352 -4.1855164 -4.1846833 -4.1842294 -4.1823988 -4.1807551 -4.1802707 -4.1806884 -4.1790543 -4.1698751 -4.1549907 -4.1396937][-4.2050285 -4.1955338 -4.19058 -4.18423 -4.1774125 -4.1719923 -4.1667185 -4.1594043 -4.152957 -4.1493654 -4.1479583 -4.1449471 -4.134635 -4.1201057 -4.1071978][-4.2257314 -4.21628 -4.2107935 -4.2039347 -4.1965652 -4.190033 -4.181829 -4.1702824 -4.160017 -4.1531806 -4.1495304 -4.1444879 -4.1343727 -4.122191 -4.1121945][-4.2312975 -4.222187 -4.2164421 -4.2102456 -4.2055688 -4.20261 -4.1971178 -4.1864548 -4.1770859 -4.1718554 -4.1696873 -4.1656404 -4.1580381 -4.1476512 -4.1374254][-4.2087617 -4.1961656 -4.1870508 -4.1795807 -4.1778927 -4.1798458 -4.1799297 -4.1743712 -4.1708007 -4.1732397 -4.1793809 -4.1828575 -4.1815634 -4.1739731 -4.1619582][-4.1744723 -4.1536288 -4.1372852 -4.1263452 -4.1271977 -4.1330276 -4.13509 -4.1311216 -4.1324086 -4.1459169 -4.1666045 -4.1833396 -4.1927776 -4.1920085 -4.1821074][-4.1461778 -4.1183581 -4.0959411 -4.0823188 -4.0840406 -4.0896 -4.086525 -4.0774145 -4.0787611 -4.1008754 -4.1350131 -4.165854 -4.1874924 -4.1952791 -4.1905813][-4.1340275 -4.1076417 -4.0877819 -4.0770884 -4.07896 -4.0803304 -4.0681787 -4.048512 -4.0431857 -4.0649991 -4.103426 -4.1417756 -4.1711822 -4.1854134 -4.186182][-4.1328511 -4.1155849 -4.105751 -4.1023555 -4.1065631 -4.1059904 -4.09198 -4.0696654 -4.0586996 -4.072732 -4.1030807 -4.1364126 -4.1630106 -4.17636 -4.1778078][-4.1437869 -4.1343861 -4.1313744 -4.1311297 -4.1353369 -4.1345787 -4.1241517 -4.1082706 -4.1014714 -4.1120672 -4.1352482 -4.1603432 -4.1775303 -4.1821647 -4.1770535][-4.1686649 -4.1611047 -4.1566939 -4.15112 -4.1481204 -4.1415224 -4.1304259 -4.1202488 -4.1199446 -4.1337018 -4.1566429 -4.1794996 -4.1920147 -4.1899705 -4.1783915][-4.1982555 -4.189095 -4.1809325 -4.1699595 -4.1610074 -4.1490779 -4.135571 -4.1267037 -4.128592 -4.1429267 -4.1648307 -4.1854649 -4.1960297 -4.191925 -4.1782761][-4.2160306 -4.2060647 -4.1968551 -4.18619 -4.178359 -4.168489 -4.1567 -4.1487837 -4.1485982 -4.157598 -4.1729465 -4.1872382 -4.1932168 -4.1880445 -4.1768308][-4.2178988 -4.2076292 -4.19969 -4.1928892 -4.1898656 -4.1861129 -4.1809759 -4.1767197 -4.1754522 -4.1787438 -4.1864519 -4.1931763 -4.1931982 -4.1857719 -4.1761551][-4.21544 -4.2049913 -4.1972933 -4.1915541 -4.1900649 -4.1899147 -4.1902509 -4.1909518 -4.1913433 -4.192678 -4.195775 -4.1976924 -4.1949043 -4.1874208 -4.1792779]]...]
INFO - root - 2017-12-05 08:39:50.603217: step 710, loss = 2.03, batch loss = 1.97 (39.3 examples/sec; 0.204 sec/batch; 18h:45m:25s remains)
INFO - root - 2017-12-05 08:39:52.681815: step 720, loss = 2.01, batch loss = 1.96 (37.8 examples/sec; 0.212 sec/batch; 19h:31m:18s remains)
INFO - root - 2017-12-05 08:39:54.744550: step 730, loss = 2.04, batch loss = 1.98 (39.1 examples/sec; 0.205 sec/batch; 18h:52m:01s remains)
INFO - root - 2017-12-05 08:39:56.820663: step 740, loss = 2.05, batch loss = 1.99 (38.1 examples/sec; 0.210 sec/batch; 19h:21m:47s remains)
INFO - root - 2017-12-05 08:39:58.935765: step 750, loss = 2.02, batch loss = 1.96 (39.1 examples/sec; 0.205 sec/batch; 18h:50m:55s remains)
INFO - root - 2017-12-05 08:40:00.984875: step 760, loss = 2.05, batch loss = 1.99 (37.1 examples/sec; 0.215 sec/batch; 19h:51m:16s remains)
INFO - root - 2017-12-05 08:40:03.048391: step 770, loss = 2.02, batch loss = 1.97 (40.3 examples/sec; 0.198 sec/batch; 18h:17m:13s remains)
INFO - root - 2017-12-05 08:40:05.124782: step 780, loss = 2.04, batch loss = 1.98 (38.6 examples/sec; 0.207 sec/batch; 19h:06m:05s remains)
INFO - root - 2017-12-05 08:40:07.202999: step 790, loss = 2.02, batch loss = 1.96 (39.0 examples/sec; 0.205 sec/batch; 18h:55m:19s remains)
INFO - root - 2017-12-05 08:40:09.267445: step 800, loss = 2.04, batch loss = 1.99 (37.7 examples/sec; 0.212 sec/batch; 19h:32m:35s remains)
2017-12-05 08:40:09.611297: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2764521 -4.2633977 -4.2572994 -4.2489996 -4.2234516 -4.1845775 -4.1486297 -4.1271906 -4.12462 -4.1349216 -4.1475644 -4.1526332 -4.1493664 -4.1329532 -4.11458][-4.2753215 -4.2626572 -4.2552643 -4.2459521 -4.2199039 -4.1802597 -4.1409979 -4.114038 -4.103868 -4.10824 -4.1168842 -4.12129 -4.1183329 -4.1031 -4.0874462][-4.255609 -4.2410936 -4.2318373 -4.2211452 -4.1958013 -4.1571765 -4.1171894 -4.0867853 -4.0728064 -4.07587 -4.0849118 -4.0944986 -4.0969591 -4.0893493 -4.0819168][-4.2143588 -4.1945624 -4.1814241 -4.1691895 -4.1477842 -4.1149321 -4.0805569 -4.0559955 -4.0457907 -4.0517883 -4.0640688 -4.0758224 -4.08098 -4.078558 -4.0788126][-4.1641965 -4.1387472 -4.1211257 -4.1078992 -4.0896411 -4.0617285 -4.0323873 -4.0136461 -4.0107365 -4.0251865 -4.0459456 -4.0613527 -4.0654449 -4.0651951 -4.0720329][-4.1202765 -4.0890203 -4.0631952 -4.0420957 -4.0213757 -3.9933274 -3.9623747 -3.9442377 -3.9515584 -3.9830358 -4.022326 -4.0483346 -4.0562744 -4.0578585 -4.0659351][-4.09534 -4.0644989 -4.0326867 -4.0029869 -3.9758849 -3.9398015 -3.8940949 -3.8598676 -3.8688893 -3.9183083 -3.9785244 -4.0169253 -4.0304875 -4.0296617 -4.0302973][-4.095645 -4.0733972 -4.0419884 -4.00684 -3.9738367 -3.9336877 -3.880044 -3.8348205 -3.8391647 -3.8896933 -3.9525959 -3.991401 -4.00361 -3.9988554 -3.9909115][-4.1192179 -4.1039281 -4.0730996 -4.0370121 -4.0077024 -3.9780495 -3.9383419 -3.9018745 -3.9001248 -3.9354742 -3.983542 -4.0118885 -4.0160751 -4.0067234 -3.9955664][-4.161705 -4.1492414 -4.1210809 -4.0858464 -4.0599289 -4.0378666 -4.0085006 -3.9800978 -3.9748938 -3.9971948 -4.0308075 -4.0507035 -4.0488672 -4.0333486 -4.0197105][-4.2113767 -4.2014537 -4.1799841 -4.151659 -4.1295853 -4.1081481 -4.0807972 -4.0545511 -4.0481038 -4.0626397 -4.08494 -4.0982318 -4.09322 -4.0724068 -4.0540986][-4.25894 -4.2504058 -4.2357073 -4.2157111 -4.1973906 -4.1773291 -4.1526628 -4.1300611 -4.1252952 -4.1354918 -4.1500263 -4.1596723 -4.1550894 -4.1359043 -4.1158133][-4.2879171 -4.2810969 -4.27292 -4.2604356 -4.2468452 -4.2311058 -4.2141 -4.200994 -4.2016912 -4.2104788 -4.2177877 -4.2220235 -4.2178583 -4.2032294 -4.1865363][-4.3005767 -4.2967367 -4.2925553 -4.2846594 -4.2747159 -4.2640228 -4.2550449 -4.2501063 -4.253211 -4.2593656 -4.2617407 -4.2613297 -4.2570243 -4.246851 -4.2331629][-4.3092785 -4.3068275 -4.3047137 -4.2998824 -4.29323 -4.2872243 -4.2838597 -4.2834144 -4.2861533 -4.2893133 -4.2883077 -4.2846737 -4.2788148 -4.2701211 -4.2576537]]...]
INFO - root - 2017-12-05 08:40:11.666276: step 810, loss = 2.00, batch loss = 1.95 (38.6 examples/sec; 0.207 sec/batch; 19h:06m:54s remains)
INFO - root - 2017-12-05 08:40:13.712924: step 820, loss = 2.00, batch loss = 1.94 (37.7 examples/sec; 0.212 sec/batch; 19h:33m:14s remains)
INFO - root - 2017-12-05 08:40:15.770945: step 830, loss = 2.04, batch loss = 1.99 (37.8 examples/sec; 0.212 sec/batch; 19h:29m:21s remains)
INFO - root - 2017-12-05 08:40:17.839377: step 840, loss = 2.00, batch loss = 1.95 (38.8 examples/sec; 0.206 sec/batch; 19h:00m:32s remains)
INFO - root - 2017-12-05 08:40:19.893663: step 850, loss = 2.04, batch loss = 1.98 (38.6 examples/sec; 0.207 sec/batch; 19h:05m:29s remains)
INFO - root - 2017-12-05 08:40:21.971846: step 860, loss = 2.05, batch loss = 1.99 (39.1 examples/sec; 0.205 sec/batch; 18h:50m:50s remains)
INFO - root - 2017-12-05 08:40:24.062087: step 870, loss = 2.01, batch loss = 1.95 (38.9 examples/sec; 0.206 sec/batch; 18h:56m:31s remains)
INFO - root - 2017-12-05 08:40:26.174894: step 880, loss = 2.04, batch loss = 1.98 (31.4 examples/sec; 0.255 sec/batch; 23h:27m:35s remains)
INFO - root - 2017-12-05 08:40:28.252625: step 890, loss = 2.04, batch loss = 1.98 (37.7 examples/sec; 0.212 sec/batch; 19h:32m:50s remains)
INFO - root - 2017-12-05 08:40:30.332312: step 900, loss = 1.99, batch loss = 1.93 (39.2 examples/sec; 0.204 sec/batch; 18h:46m:32s remains)
2017-12-05 08:40:30.639023: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.0364432 -4.0070009 -4.0009651 -3.9954505 -3.9797511 -3.989542 -4.0209332 -4.0372949 -4.0320592 -4.0211205 -4.0179024 -4.0170655 -3.9910672 -3.9711404 -3.9839773][-3.9826336 -3.976321 -3.9907 -3.989738 -3.9664757 -3.9639108 -3.994272 -4.0220518 -4.02818 -4.0225239 -4.0121512 -4.0079017 -3.9810643 -3.964972 -3.9813688][-3.9186029 -3.949528 -4.0027332 -4.0146475 -3.9858859 -3.9649565 -3.9835811 -4.0114059 -4.0250707 -4.0274324 -4.018 -4.0094228 -3.9791236 -3.9586711 -3.9785154][-3.8438005 -3.911339 -4.0036774 -4.0329776 -4.0062385 -3.9696131 -3.9695725 -3.9891388 -4.0039167 -4.016933 -4.0152078 -4.0120482 -3.9887667 -3.9670143 -3.9831216][-3.8604634 -3.924525 -4.0190644 -4.0549507 -4.0210266 -3.964397 -3.935972 -3.9371407 -3.9560652 -3.982655 -3.9950175 -4.0068817 -3.9995327 -3.9810131 -3.9922662][-3.9669209 -3.9980345 -4.0580206 -4.0742297 -4.0165677 -3.9390564 -3.8784392 -3.8490324 -3.8781247 -3.9318175 -3.9662044 -3.9949551 -3.9997475 -3.9842935 -3.9898324][-4.0584292 -4.0593586 -4.0747571 -4.0610275 -3.9784474 -3.874589 -3.7840347 -3.7338552 -3.7901778 -3.8802207 -3.9365954 -3.9710178 -3.9728835 -3.9539723 -3.9622295][-4.1124382 -4.0896192 -4.0707593 -4.0250268 -3.9169657 -3.7838597 -3.6733224 -3.6219714 -3.7231314 -3.8567371 -3.933053 -3.9677067 -3.9613249 -3.9253407 -3.9263647][-4.1416373 -4.1081753 -4.0702138 -4.0044422 -3.8908081 -3.7683523 -3.6868687 -3.6644692 -3.7796965 -3.9149005 -3.9842942 -4.0100126 -3.9908183 -3.93334 -3.9212108][-4.1548791 -4.1321464 -4.09888 -4.0394425 -3.9524612 -3.8709557 -3.8349931 -3.8354225 -3.919266 -4.0161314 -4.0523777 -4.066587 -4.04891 -3.9856369 -3.9654961][-4.156992 -4.1529989 -4.1392946 -4.1018314 -4.0473323 -4.0011463 -3.9866793 -3.9927235 -4.040966 -4.0966792 -4.107336 -4.1125841 -4.1021094 -4.0464087 -4.0150504][-4.1488633 -4.1541657 -4.1552911 -4.1397219 -4.11192 -4.0898218 -4.0859876 -4.0892181 -4.1116219 -4.1374612 -4.1327615 -4.1308618 -4.1220493 -4.0706644 -4.031208][-4.1465721 -4.1475348 -4.1544471 -4.1561284 -4.1454463 -4.1341796 -4.1352568 -4.1367936 -4.1478553 -4.1619463 -4.1548696 -4.1484017 -4.133925 -4.0852609 -4.0409117][-4.1573591 -4.1516495 -4.1538105 -4.157681 -4.1565313 -4.1529527 -4.1563759 -4.1583433 -4.1658363 -4.1718249 -4.167665 -4.1681533 -4.15752 -4.1135392 -4.0702462][-4.1696839 -4.1622024 -4.1614108 -4.1653233 -4.1682405 -4.170464 -4.1770873 -4.1806903 -4.18759 -4.1874466 -4.1822553 -4.1837292 -4.174777 -4.1371374 -4.1041651]]...]
INFO - root - 2017-12-05 08:40:32.741461: step 910, loss = 2.04, batch loss = 1.98 (38.9 examples/sec; 0.206 sec/batch; 18h:56m:31s remains)
INFO - root - 2017-12-05 08:40:34.818008: step 920, loss = 2.05, batch loss = 2.00 (39.3 examples/sec; 0.204 sec/batch; 18h:45m:20s remains)
INFO - root - 2017-12-05 08:40:36.890109: step 930, loss = 2.03, batch loss = 1.97 (38.0 examples/sec; 0.211 sec/batch; 19h:23m:46s remains)
INFO - root - 2017-12-05 08:40:38.944613: step 940, loss = 2.01, batch loss = 1.95 (38.8 examples/sec; 0.206 sec/batch; 19h:00m:16s remains)
INFO - root - 2017-12-05 08:40:41.001474: step 950, loss = 2.05, batch loss = 1.99 (38.8 examples/sec; 0.206 sec/batch; 18h:57m:54s remains)
INFO - root - 2017-12-05 08:40:43.069520: step 960, loss = 1.99, batch loss = 1.93 (35.9 examples/sec; 0.223 sec/batch; 20h:31m:31s remains)
INFO - root - 2017-12-05 08:40:45.126278: step 970, loss = 1.99, batch loss = 1.93 (38.2 examples/sec; 0.209 sec/batch; 19h:16m:13s remains)
INFO - root - 2017-12-05 08:40:47.212925: step 980, loss = 2.02, batch loss = 1.96 (37.6 examples/sec; 0.213 sec/batch; 19h:36m:28s remains)
INFO - root - 2017-12-05 08:40:49.263540: step 990, loss = 1.99, batch loss = 1.93 (39.1 examples/sec; 0.205 sec/batch; 18h:51m:08s remains)
INFO - root - 2017-12-05 08:40:51.334987: step 1000, loss = 1.97, batch loss = 1.92 (38.1 examples/sec; 0.210 sec/batch; 19h:20m:55s remains)
2017-12-05 08:40:51.593713: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.0648875 -4.067121 -4.09075 -4.119657 -4.1323457 -4.1333728 -4.1179218 -4.0997672 -4.0960875 -4.103663 -4.123755 -4.1513424 -4.1871123 -4.224431 -4.25913][-4.0495257 -4.052536 -4.0747266 -4.102747 -4.1169672 -4.118906 -4.1012058 -4.086215 -4.0924764 -4.1070118 -4.1318693 -4.1618934 -4.1956453 -4.2303128 -4.2625709][-4.0437155 -4.0493159 -4.0694375 -4.0915966 -4.1000319 -4.0971584 -4.0764575 -4.0651689 -4.0809216 -4.1040554 -4.1352267 -4.167315 -4.1986194 -4.2319593 -4.2637739][-4.0424495 -4.0489678 -4.065928 -4.0817327 -4.0859513 -4.0770278 -4.0518737 -4.043221 -4.0669622 -4.0975332 -4.1324825 -4.165019 -4.195044 -4.2280374 -4.2613368][-4.0527234 -4.0527272 -4.0587411 -4.0662136 -4.0616789 -4.0386686 -4.0041714 -3.9984908 -4.0353594 -4.0796156 -4.1198854 -4.156518 -4.191349 -4.2268486 -4.2626705][-4.0756526 -4.0671887 -4.061532 -4.0558829 -4.0355706 -3.9927108 -3.9388068 -3.9300277 -3.9864717 -4.0537825 -4.1072106 -4.15236 -4.19374 -4.2317152 -4.2683167][-4.1066294 -4.0870857 -4.0714321 -4.0533381 -4.0172396 -3.954987 -3.877203 -3.8614111 -3.937933 -4.030036 -4.0996256 -4.1532283 -4.1977286 -4.2365494 -4.2727485][-4.1255803 -4.0979929 -4.0752745 -4.0506015 -4.0108633 -3.9444592 -3.8598862 -3.8424714 -3.92634 -4.0270691 -4.1032143 -4.1609192 -4.204175 -4.2429624 -4.2781987][-4.1371007 -4.106111 -4.0839987 -4.06446 -4.0379057 -3.991837 -3.9309762 -3.9208078 -3.9848309 -4.0631585 -4.1241198 -4.1741862 -4.212131 -4.2492428 -4.2841344][-4.1409659 -4.1096845 -4.0914979 -4.0828905 -4.0757546 -4.0572357 -4.02401 -4.0168738 -4.0548124 -4.103961 -4.1450624 -4.1833572 -4.2162757 -4.251955 -4.2876968][-4.1411304 -4.1118083 -4.0975404 -4.0975423 -4.1047893 -4.105752 -4.09046 -4.083468 -4.1032624 -4.133081 -4.1597433 -4.1901851 -4.2220335 -4.2571864 -4.2919955][-4.145165 -4.1190948 -4.1078897 -4.1133041 -4.1287513 -4.1415172 -4.1356416 -4.1278129 -4.1390963 -4.1586866 -4.1775293 -4.2054253 -4.2379026 -4.2713928 -4.3015609][-4.1638446 -4.143034 -4.1352544 -4.1425939 -4.1588058 -4.1733174 -4.1679831 -4.158659 -4.1670403 -4.1825714 -4.1999254 -4.2285395 -4.2607718 -4.2904811 -4.3143063][-4.197505 -4.1838493 -4.1791677 -4.1846042 -4.1953835 -4.2044244 -4.1973724 -4.1885948 -4.1971631 -4.2136393 -4.2324381 -4.2594185 -4.2871222 -4.310534 -4.3278503][-4.2333436 -4.2246208 -4.222024 -4.2250867 -4.2312312 -4.2356992 -4.2287354 -4.2222152 -4.2302175 -4.24537 -4.2632022 -4.2851377 -4.306448 -4.3236594 -4.3362913]]...]
INFO - root - 2017-12-05 08:40:53.650374: step 1010, loss = 2.00, batch loss = 1.94 (37.3 examples/sec; 0.214 sec/batch; 19h:43m:39s remains)
INFO - root - 2017-12-05 08:40:55.751372: step 1020, loss = 1.99, batch loss = 1.93 (39.1 examples/sec; 0.205 sec/batch; 18h:49m:58s remains)
INFO - root - 2017-12-05 08:40:57.810295: step 1030, loss = 1.99, batch loss = 1.93 (39.2 examples/sec; 0.204 sec/batch; 18h:48m:17s remains)
INFO - root - 2017-12-05 08:40:59.901213: step 1040, loss = 2.01, batch loss = 1.95 (38.9 examples/sec; 0.206 sec/batch; 18h:55m:48s remains)
INFO - root - 2017-12-05 08:41:01.974257: step 1050, loss = 2.03, batch loss = 1.97 (36.7 examples/sec; 0.218 sec/batch; 20h:04m:26s remains)
INFO - root - 2017-12-05 08:41:04.015283: step 1060, loss = 2.00, batch loss = 1.94 (39.3 examples/sec; 0.203 sec/batch; 18h:43m:03s remains)
INFO - root - 2017-12-05 08:41:06.126802: step 1070, loss = 2.00, batch loss = 1.94 (38.4 examples/sec; 0.209 sec/batch; 19h:11m:47s remains)
INFO - root - 2017-12-05 08:41:08.189041: step 1080, loss = 2.00, batch loss = 1.94 (39.4 examples/sec; 0.203 sec/batch; 18h:40m:55s remains)
INFO - root - 2017-12-05 08:41:10.274860: step 1090, loss = 1.98, batch loss = 1.92 (37.9 examples/sec; 0.211 sec/batch; 19h:26m:50s remains)
INFO - root - 2017-12-05 08:41:12.349813: step 1100, loss = 2.00, batch loss = 1.94 (39.2 examples/sec; 0.204 sec/batch; 18h:45m:49s remains)
2017-12-05 08:41:12.619368: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1353245 -4.0929642 -4.0341811 -3.9960358 -3.9918542 -4.0016537 -4.0436893 -4.0911212 -4.1268768 -4.15179 -4.1702123 -4.1802783 -4.20398 -4.22819 -4.2484555][-4.1366138 -4.0944886 -4.0322924 -3.9645023 -3.9171202 -3.8991747 -3.954529 -4.0258255 -4.0773211 -4.1152964 -4.1442122 -4.1620464 -4.1898141 -4.2210255 -4.2490973][-4.1491947 -4.1097097 -4.0486703 -3.9712591 -3.8942142 -3.8499985 -3.8990536 -3.9806795 -4.0440931 -4.0888486 -4.1217861 -4.14744 -4.180057 -4.216176 -4.2478828][-4.162323 -4.1329794 -4.082 -4.014811 -3.9397824 -3.881669 -3.8988791 -3.9609501 -4.0221438 -4.0685463 -4.1033287 -4.1359468 -4.1743531 -4.2138419 -4.2465725][-4.149826 -4.1287818 -4.0870986 -4.0345759 -3.973649 -3.91778 -3.9064484 -3.9352021 -3.9862156 -4.0368228 -4.0808191 -4.1210971 -4.1640358 -4.2075043 -4.2434096][-4.1208544 -4.1047878 -4.0699177 -4.0239382 -3.9739623 -3.9197149 -3.8818297 -3.8734319 -3.9135034 -3.9773762 -4.0386796 -4.0908442 -4.1409197 -4.1918278 -4.2345858][-4.1061487 -4.0913749 -4.0628867 -4.0252233 -3.9842513 -3.9285581 -3.8721457 -3.8316994 -3.8534837 -3.924829 -4.0005426 -4.0632057 -4.1189847 -4.1744847 -4.2244349][-4.1232553 -4.1082563 -4.0810809 -4.048841 -4.0141721 -3.9609137 -3.9047873 -3.8570576 -3.8607256 -3.9211576 -3.9949141 -4.0567627 -4.1144352 -4.1724305 -4.2244649][-4.1511383 -4.1390104 -4.1156363 -4.0857739 -4.0517354 -4.0047264 -3.9610603 -3.9209 -3.912354 -3.9507062 -4.0111165 -4.0667048 -4.1226025 -4.1807508 -4.2337737][-4.15839 -4.1535978 -4.1424184 -4.1204529 -4.0886846 -4.0438328 -4.0038142 -3.9686518 -3.9506822 -3.9657166 -4.0115175 -4.0679274 -4.1234279 -4.183322 -4.2377028][-4.1524668 -4.154284 -4.1560583 -4.1473961 -4.1246285 -4.0834155 -4.0416303 -4.0052609 -3.9799435 -3.97569 -4.0081329 -4.0636516 -4.1220512 -4.1833911 -4.2368836][-4.1591949 -4.1630912 -4.1712508 -4.1720333 -4.1602139 -4.1292338 -4.0912108 -4.055037 -4.023572 -4.0041027 -4.0221848 -4.0698967 -4.1250558 -4.1828461 -4.233367][-4.1818562 -4.1836905 -4.1884561 -4.1888151 -4.1841254 -4.1665454 -4.1407423 -4.11056 -4.0766878 -4.048768 -4.0552931 -4.0883589 -4.1339173 -4.1850266 -4.2311645][-4.1953907 -4.1930203 -4.1920729 -4.1891532 -4.1874065 -4.1802378 -4.168232 -4.1469469 -4.1145177 -4.0836205 -4.0809369 -4.1018481 -4.1382232 -4.1833539 -4.2249427][-4.1910486 -4.1851525 -4.182528 -4.1797943 -4.1796041 -4.179687 -4.1752977 -4.1568289 -4.1247072 -4.0957346 -4.0910068 -4.1065569 -4.1399121 -4.1798272 -4.2173576]]...]
INFO - root - 2017-12-05 08:41:14.700975: step 1110, loss = 1.97, batch loss = 1.91 (38.1 examples/sec; 0.210 sec/batch; 19h:20m:49s remains)
INFO - root - 2017-12-05 08:41:16.810079: step 1120, loss = 2.02, batch loss = 1.96 (37.5 examples/sec; 0.214 sec/batch; 19h:39m:19s remains)
INFO - root - 2017-12-05 08:41:18.879731: step 1130, loss = 1.99, batch loss = 1.93 (39.1 examples/sec; 0.205 sec/batch; 18h:49m:28s remains)
INFO - root - 2017-12-05 08:41:20.950797: step 1140, loss = 2.00, batch loss = 1.95 (38.1 examples/sec; 0.210 sec/batch; 19h:20m:46s remains)
INFO - root - 2017-12-05 08:41:23.054405: step 1150, loss = 1.98, batch loss = 1.92 (37.1 examples/sec; 0.215 sec/batch; 19h:49m:50s remains)
INFO - root - 2017-12-05 08:41:25.126456: step 1160, loss = 1.98, batch loss = 1.92 (38.0 examples/sec; 0.210 sec/batch; 19h:21m:57s remains)
INFO - root - 2017-12-05 08:41:27.202860: step 1170, loss = 2.01, batch loss = 1.95 (39.3 examples/sec; 0.203 sec/batch; 18h:43m:38s remains)
INFO - root - 2017-12-05 08:41:29.268366: step 1180, loss = 1.97, batch loss = 1.91 (38.8 examples/sec; 0.206 sec/batch; 18h:57m:07s remains)
INFO - root - 2017-12-05 08:41:31.359616: step 1190, loss = 1.97, batch loss = 1.91 (39.7 examples/sec; 0.202 sec/batch; 18h:34m:02s remains)
INFO - root - 2017-12-05 08:41:33.406395: step 1200, loss = 1.98, batch loss = 1.92 (38.6 examples/sec; 0.207 sec/batch; 19h:03m:34s remains)
2017-12-05 08:41:33.738064: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2143703 -4.1975775 -4.1919928 -4.1877933 -4.1918764 -4.2056918 -4.2173624 -4.2255068 -4.2174296 -4.1943264 -4.1755404 -4.17883 -4.2050095 -4.2338362 -4.2517686][-4.1992965 -4.1789351 -4.1627207 -4.1453943 -4.1426888 -4.1549015 -4.1709356 -4.1885219 -4.1876245 -4.1694183 -4.1557708 -4.1593862 -4.1835928 -4.2088618 -4.2258115][-4.1701956 -4.1461182 -4.1147323 -4.079833 -4.0690827 -4.0826221 -4.1087151 -4.1427565 -4.1585293 -4.1582561 -4.15715 -4.1622825 -4.1795335 -4.1950035 -4.2033091][-4.1313353 -4.1040874 -4.0582409 -4.0016875 -3.9787841 -3.99604 -4.0316916 -4.0706849 -4.10292 -4.1341443 -4.1564121 -4.1713886 -4.1869736 -4.1949463 -4.1944232][-4.0894594 -4.0612774 -4.00442 -3.9245949 -3.8875706 -3.903254 -3.9276171 -3.9436421 -3.9856653 -4.059453 -4.1192074 -4.1573153 -4.1851606 -4.1965027 -4.1942239][-4.0575776 -4.0363822 -3.9757886 -3.8804412 -3.8303721 -3.8273213 -3.8054924 -3.7675619 -3.8057411 -3.9326329 -4.0432234 -4.1147242 -4.1629443 -4.187067 -4.1932969][-4.0496821 -4.0439725 -3.9968755 -3.9046736 -3.8382287 -3.7900834 -3.6898649 -3.5662947 -3.5894334 -3.7775235 -3.941124 -4.0443149 -4.1138625 -4.1561608 -4.1791563][-4.0462675 -4.0699372 -4.0553026 -3.9907346 -3.9203632 -3.832777 -3.6632838 -3.4538598 -3.4634194 -3.687623 -3.8714185 -3.981194 -4.0604014 -4.1150985 -4.1530285][-4.036438 -4.0874715 -4.1103106 -4.0867043 -4.0398483 -3.9601185 -3.7941723 -3.5907845 -3.5848489 -3.7487214 -3.8838868 -3.9638939 -4.0284305 -4.0841975 -4.1317697][-4.0278125 -4.0909081 -4.1356826 -4.1469679 -4.1327319 -4.0906839 -3.9748406 -3.8295879 -3.8135624 -3.8961263 -3.9605737 -3.9967828 -4.0379429 -4.0892644 -4.1368251][-4.0392761 -4.0954552 -4.138783 -4.1648054 -4.1720891 -4.1564312 -4.0874672 -3.9975441 -3.9855292 -4.0237107 -4.0485783 -4.061976 -4.0847239 -4.120851 -4.1538739][-4.069356 -4.1053572 -4.1335421 -4.1595 -4.1757574 -4.1767249 -4.1394267 -4.0814509 -4.0726833 -4.0914788 -4.1056581 -4.1130447 -4.1234112 -4.139555 -4.1536722][-4.0818882 -4.0969176 -4.1126113 -4.1360369 -4.1559639 -4.1644926 -4.1412454 -4.0976739 -4.0909076 -4.1006756 -4.1138134 -4.1190443 -4.1168914 -4.1156068 -4.1190205][-4.0863123 -4.0825267 -4.0878892 -4.1054506 -4.1248674 -4.1372352 -4.1200757 -4.0840082 -4.07312 -4.0700955 -4.0760055 -4.0755959 -4.0588684 -4.0460634 -4.0507274][-4.1031837 -4.0865054 -4.0788221 -4.0830545 -4.0970478 -4.11107 -4.1010265 -4.07254 -4.0512 -4.02558 -4.0077829 -3.9929481 -3.9678631 -3.9584231 -3.97864]]...]
INFO - root - 2017-12-05 08:41:35.789166: step 1210, loss = 1.99, batch loss = 1.94 (38.4 examples/sec; 0.208 sec/batch; 19h:10m:03s remains)
INFO - root - 2017-12-05 08:41:37.892738: step 1220, loss = 1.98, batch loss = 1.93 (38.4 examples/sec; 0.209 sec/batch; 19h:11m:28s remains)
INFO - root - 2017-12-05 08:41:39.961650: step 1230, loss = 1.98, batch loss = 1.92 (39.6 examples/sec; 0.202 sec/batch; 18h:36m:04s remains)
INFO - root - 2017-12-05 08:41:42.044520: step 1240, loss = 2.00, batch loss = 1.94 (38.5 examples/sec; 0.208 sec/batch; 19h:06m:49s remains)
INFO - root - 2017-12-05 08:41:44.149703: step 1250, loss = 2.03, batch loss = 1.97 (39.6 examples/sec; 0.202 sec/batch; 18h:35m:47s remains)
INFO - root - 2017-12-05 08:41:46.233249: step 1260, loss = 1.96, batch loss = 1.90 (38.8 examples/sec; 0.206 sec/batch; 18h:57m:17s remains)
INFO - root - 2017-12-05 08:41:48.302676: step 1270, loss = 2.02, batch loss = 1.97 (38.4 examples/sec; 0.208 sec/batch; 19h:10m:48s remains)
INFO - root - 2017-12-05 08:41:50.373024: step 1280, loss = 1.99, batch loss = 1.93 (38.6 examples/sec; 0.207 sec/batch; 19h:04m:15s remains)
INFO - root - 2017-12-05 08:41:52.478626: step 1290, loss = 1.98, batch loss = 1.93 (39.3 examples/sec; 0.204 sec/batch; 18h:43m:52s remains)
INFO - root - 2017-12-05 08:41:54.598115: step 1300, loss = 2.00, batch loss = 1.94 (37.8 examples/sec; 0.212 sec/batch; 19h:27m:32s remains)
2017-12-05 08:41:54.905164: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.177403 -4.1496363 -4.1130304 -4.0814137 -4.0493808 -4.0111303 -3.9975209 -3.9988363 -4.0078778 -4.039784 -4.0591741 -4.0738268 -4.1094179 -4.1171503 -4.0967717][-4.1616926 -4.1393285 -4.1064081 -4.0750132 -4.0432086 -3.9949605 -3.9712658 -3.978682 -3.9935274 -4.0252028 -4.0472636 -4.0628557 -4.0973721 -4.1115265 -4.0960445][-4.140636 -4.1234612 -4.0950308 -4.0677214 -4.0416918 -3.9937081 -3.9645369 -3.9767942 -3.9970713 -4.0228415 -4.046824 -4.0666876 -4.0944591 -4.1125488 -4.1063328][-4.12373 -4.1088524 -4.0816636 -4.0531573 -4.03106 -3.9838829 -3.94718 -3.961586 -3.989347 -4.0140843 -4.0447993 -4.0717678 -4.0938268 -4.1092892 -4.1068845][-4.109261 -4.0885882 -4.0577359 -4.0225739 -4.0007129 -3.9582531 -3.9210787 -3.9312918 -3.9627607 -3.9911506 -4.0241342 -4.057795 -4.0751467 -4.0848103 -4.083035][-4.0619082 -4.0228281 -3.9808154 -3.938983 -3.9130008 -3.87749 -3.8405068 -3.8397331 -3.8795025 -3.9176359 -3.9556324 -4.0012622 -4.023737 -4.0285406 -4.0274963][-3.993001 -3.9342554 -3.8857 -3.8439584 -3.8153198 -3.7808669 -3.7365341 -3.7193019 -3.7665794 -3.8197083 -3.8626556 -3.9145148 -3.9452448 -3.9494021 -3.9483514][-3.9415357 -3.8705966 -3.8200302 -3.778966 -3.744657 -3.7026758 -3.6488037 -3.618937 -3.6709466 -3.7421193 -3.7895615 -3.8353751 -3.8642249 -3.8645906 -3.8643045][-3.9370649 -3.8659964 -3.8154645 -3.7724938 -3.7315717 -3.6875377 -3.6411066 -3.6162515 -3.6558027 -3.7209907 -3.7631319 -3.7954109 -3.8160641 -3.8114858 -3.8106918][-3.96094 -3.898397 -3.8537161 -3.8120377 -3.7740669 -3.7421286 -3.7166204 -3.7036195 -3.7257891 -3.7678995 -3.7951736 -3.8080099 -3.8137622 -3.8044076 -3.7958972][-4.00089 -3.9502594 -3.9158173 -3.8836355 -3.8560693 -3.8373177 -3.827424 -3.8217 -3.8305464 -3.8509007 -3.8625314 -3.8575118 -3.8493717 -3.837477 -3.8210738][-4.0478849 -4.0100679 -3.9900606 -3.9728451 -3.9588904 -3.9549952 -3.9568136 -3.9514718 -3.9464993 -3.9457116 -3.9428198 -3.9242346 -3.9053404 -3.8916039 -3.8698058][-4.0969625 -4.0667467 -4.0543365 -4.0482054 -4.0458212 -4.0521889 -4.0587664 -4.0535626 -4.0395164 -4.0271235 -4.0198226 -4.0013208 -3.9804647 -3.9672346 -3.9466538][-4.1403747 -4.1108308 -4.0959888 -4.0923638 -4.096077 -4.1062255 -4.1139526 -4.1106653 -4.0976229 -4.0856709 -4.0804191 -4.0682616 -4.0502329 -4.0394688 -4.0247488][-4.1818938 -4.1516361 -4.1303058 -4.1207247 -4.1226892 -4.1321764 -4.138926 -4.137135 -4.128335 -4.1200328 -4.11815 -4.1117806 -4.1004477 -4.0961165 -4.0907669]]...]
INFO - root - 2017-12-05 08:41:56.962431: step 1310, loss = 2.00, batch loss = 1.94 (39.3 examples/sec; 0.203 sec/batch; 18h:43m:01s remains)
INFO - root - 2017-12-05 08:41:59.000499: step 1320, loss = 1.96, batch loss = 1.91 (39.6 examples/sec; 0.202 sec/batch; 18h:34m:31s remains)
INFO - root - 2017-12-05 08:42:01.084225: step 1330, loss = 1.98, batch loss = 1.92 (38.7 examples/sec; 0.207 sec/batch; 19h:01m:46s remains)
INFO - root - 2017-12-05 08:42:03.170371: step 1340, loss = 1.99, batch loss = 1.93 (37.2 examples/sec; 0.215 sec/batch; 19h:45m:39s remains)
INFO - root - 2017-12-05 08:42:05.242860: step 1350, loss = 1.98, batch loss = 1.92 (38.3 examples/sec; 0.209 sec/batch; 19h:14m:16s remains)
INFO - root - 2017-12-05 08:42:07.355953: step 1360, loss = 1.98, batch loss = 1.92 (39.0 examples/sec; 0.205 sec/batch; 18h:51m:30s remains)
INFO - root - 2017-12-05 08:42:09.421482: step 1370, loss = 1.96, batch loss = 1.90 (38.0 examples/sec; 0.210 sec/batch; 19h:21m:17s remains)
INFO - root - 2017-12-05 08:42:11.495078: step 1380, loss = 1.95, batch loss = 1.89 (38.8 examples/sec; 0.206 sec/batch; 18h:56m:48s remains)
INFO - root - 2017-12-05 08:42:13.572735: step 1390, loss = 1.95, batch loss = 1.89 (38.3 examples/sec; 0.209 sec/batch; 19h:13m:12s remains)
INFO - root - 2017-12-05 08:42:15.688338: step 1400, loss = 1.94, batch loss = 1.88 (38.7 examples/sec; 0.207 sec/batch; 19h:00m:16s remains)
2017-12-05 08:42:15.984985: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1354876 -4.139658 -4.1419568 -4.1321659 -4.117312 -4.10617 -4.10154 -4.0934954 -4.0806532 -4.0696464 -4.065886 -4.0739369 -4.0833583 -4.089705 -4.103303][-4.1134729 -4.1212363 -4.1289072 -4.1220689 -4.1074371 -4.09351 -4.0865755 -4.0765038 -4.0642228 -4.0554271 -4.0545635 -4.0638595 -4.0742931 -4.0803366 -4.0905318][-4.1114039 -4.1159768 -4.1224489 -4.1157856 -4.1004605 -4.085268 -4.077189 -4.0647936 -4.0511231 -4.0446777 -4.0465808 -4.0577888 -4.0711355 -4.0793443 -4.0884104][-4.1119142 -4.1065073 -4.1084981 -4.1027508 -4.0887585 -4.0723886 -4.0644097 -4.0529761 -4.0398593 -4.0373855 -4.0456719 -4.0623984 -4.0778732 -4.0863585 -4.0946937][-4.097753 -4.079308 -4.0753942 -4.07142 -4.0600853 -4.0414023 -4.0309672 -4.0193377 -4.0101123 -4.0129137 -4.0267754 -4.0498586 -4.0694566 -4.080308 -4.08946][-4.0824723 -4.0469847 -4.0291643 -4.0179858 -4.0031447 -3.9772902 -3.959801 -3.9508808 -3.954057 -3.9719903 -3.9975326 -4.0310307 -4.057169 -4.0716734 -4.0805955][-4.0957 -4.040473 -3.9987895 -3.9685435 -3.9371591 -3.8921452 -3.862813 -3.8585374 -3.8819933 -3.9260347 -3.9745717 -4.0240879 -4.0606575 -4.0824442 -4.0938148][-4.1271715 -4.0593529 -3.9975288 -3.9460759 -3.8949766 -3.8266521 -3.7830157 -3.78263 -3.8222039 -3.8938158 -3.9701257 -4.0421591 -4.0976381 -4.13067 -4.1464219][-4.1618667 -4.0933838 -4.0266414 -3.9678247 -3.9099228 -3.834146 -3.7858574 -3.7863884 -3.8283296 -3.9079609 -3.9927957 -4.0713735 -4.1344624 -4.1773634 -4.1989937][-4.1894827 -4.1296482 -4.0736213 -4.023581 -3.974813 -3.9151282 -3.8789651 -3.8837643 -3.9179986 -3.9810033 -4.0452466 -4.1025062 -4.1503291 -4.1868076 -4.2093329][-4.2155151 -4.1650286 -4.1186318 -4.0773549 -4.0414743 -4.0015907 -3.977684 -3.9823875 -4.0061564 -4.0464373 -4.0843625 -4.1146154 -4.1416416 -4.1625185 -4.1816521][-4.2409635 -4.203342 -4.1661081 -4.12957 -4.099298 -4.0664058 -4.0423117 -4.0361013 -4.042336 -4.0627995 -4.0824471 -4.0964942 -4.1125731 -4.1282806 -4.1496778][-4.2502408 -4.2307291 -4.2068129 -4.1777411 -4.1506834 -4.1175141 -4.0841689 -4.0608587 -4.0472894 -4.050386 -4.0583897 -4.071198 -4.0944843 -4.119916 -4.1499624][-4.2349324 -4.2362504 -4.230104 -4.2144251 -4.1946454 -4.1648178 -4.1278381 -4.0933228 -4.0689273 -4.0605416 -4.0599003 -4.0750809 -4.1073675 -4.1439018 -4.1819124][-4.1961536 -4.2136822 -4.225132 -4.22709 -4.2200222 -4.2014966 -4.1719146 -4.13867 -4.1116853 -4.0958033 -4.0881581 -4.1019945 -4.1361337 -4.1749692 -4.2139206]]...]
INFO - root - 2017-12-05 08:42:18.045028: step 1410, loss = 1.99, batch loss = 1.93 (37.8 examples/sec; 0.212 sec/batch; 19h:28m:06s remains)
INFO - root - 2017-12-05 08:42:20.141243: step 1420, loss = 1.97, batch loss = 1.91 (35.8 examples/sec; 0.224 sec/batch; 20h:34m:07s remains)
INFO - root - 2017-12-05 08:42:22.249183: step 1430, loss = 2.01, batch loss = 1.95 (38.4 examples/sec; 0.208 sec/batch; 19h:09m:38s remains)
INFO - root - 2017-12-05 08:42:24.328742: step 1440, loss = 1.99, batch loss = 1.93 (39.1 examples/sec; 0.205 sec/batch; 18h:48m:31s remains)
INFO - root - 2017-12-05 08:42:26.412828: step 1450, loss = 1.99, batch loss = 1.93 (37.6 examples/sec; 0.213 sec/batch; 19h:32m:53s remains)
INFO - root - 2017-12-05 08:42:28.495085: step 1460, loss = 2.01, batch loss = 1.95 (38.3 examples/sec; 0.209 sec/batch; 19h:12m:55s remains)
INFO - root - 2017-12-05 08:42:30.554219: step 1470, loss = 2.00, batch loss = 1.94 (39.3 examples/sec; 0.204 sec/batch; 18h:43m:27s remains)
INFO - root - 2017-12-05 08:42:32.636416: step 1480, loss = 2.00, batch loss = 1.94 (37.8 examples/sec; 0.212 sec/batch; 19h:27m:33s remains)
INFO - root - 2017-12-05 08:42:34.715196: step 1490, loss = 1.99, batch loss = 1.93 (39.2 examples/sec; 0.204 sec/batch; 18h:45m:19s remains)
INFO - root - 2017-12-05 08:42:36.803620: step 1500, loss = 2.00, batch loss = 1.95 (38.3 examples/sec; 0.209 sec/batch; 19h:12m:21s remains)
2017-12-05 08:42:37.087317: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.0870948 -4.0845079 -4.1139297 -4.1362815 -4.128861 -4.1161146 -4.1134953 -4.1152582 -4.1116996 -4.100605 -4.0976782 -4.1311703 -4.1722369 -4.1910462 -4.1966739][-3.977684 -3.9807105 -4.0341973 -4.0777845 -4.0705042 -4.0491962 -4.0438461 -4.0486655 -4.0447526 -4.0343714 -4.0376449 -4.0795746 -4.1320763 -4.1579971 -4.1634831][-3.9101324 -3.9143806 -3.9799759 -4.0333509 -4.02115 -3.9846265 -3.9681721 -3.9733372 -3.9756021 -3.9795492 -3.9957154 -4.0450516 -4.1033921 -4.1344643 -4.1424775][-3.901794 -3.9028664 -3.9667578 -4.01534 -3.9929814 -3.9382291 -3.9080503 -3.9097025 -3.9213169 -3.9436443 -3.9738963 -4.0329103 -4.094367 -4.1295619 -4.1424174][-3.9348166 -3.934025 -3.9914606 -4.0265155 -3.9912548 -3.92385 -3.8878882 -3.8900423 -3.9093585 -3.9381588 -3.9715858 -4.0346184 -4.097569 -4.13526 -4.1509748][-3.9856381 -3.9846077 -4.0290074 -4.0431237 -3.9924462 -3.91465 -3.8749332 -3.8804553 -3.9037309 -3.9309807 -3.9606168 -4.0240488 -4.0910039 -4.1326375 -4.1551991][-4.0236144 -4.0222459 -4.0482807 -4.0414953 -3.9788551 -3.8940244 -3.8469934 -3.850368 -3.8760107 -3.8996527 -3.923933 -3.9843698 -4.0570035 -4.1072416 -4.139895][-4.0244379 -4.0194387 -4.0294418 -4.0105815 -3.9464948 -3.8609729 -3.8052368 -3.8017306 -3.8264163 -3.8425252 -3.8563423 -3.9085467 -3.989579 -4.0574613 -4.1030521][-4.0042119 -3.996052 -3.9990032 -3.9812112 -3.9287653 -3.8480549 -3.7824626 -3.768028 -3.7888641 -3.7975197 -3.7983027 -3.8371782 -3.917748 -3.9983666 -4.0566339][-3.9886909 -3.9793284 -3.9806938 -3.9719782 -3.9395494 -3.8746834 -3.8100953 -3.786413 -3.7982283 -3.8018508 -3.7945333 -3.8211517 -3.8918314 -3.9702985 -4.0290236][-3.9844677 -3.9804122 -3.9853568 -3.9859734 -3.9759841 -3.93644 -3.8847282 -3.8541589 -3.85498 -3.8530707 -3.8405805 -3.8557744 -3.9137075 -3.9815936 -4.0312681][-4.01474 -4.0176911 -4.0261364 -4.03181 -4.0352259 -4.0178423 -3.9820778 -3.9501774 -3.9413135 -3.9398649 -3.9314921 -3.9407501 -3.9821694 -4.0377507 -4.07789][-4.0650492 -4.072876 -4.0850682 -4.0932045 -4.1009617 -4.0970931 -4.0787058 -4.0540633 -4.0405774 -4.0391655 -4.0343642 -4.0387964 -4.0639639 -4.107089 -4.1430793][-4.1290312 -4.1377525 -4.1514487 -4.1621242 -4.1724019 -4.1759048 -4.1699071 -4.1563873 -4.1453257 -4.1432319 -4.1404 -4.1420717 -4.1554041 -4.1830139 -4.2107739][-4.1997271 -4.2066817 -4.2183561 -4.2279119 -4.2366629 -4.2408962 -4.2395711 -4.2340355 -4.2284479 -4.227479 -4.2257252 -4.2275448 -4.2370558 -4.2545037 -4.2712946]]...]
INFO - root - 2017-12-05 08:42:39.194595: step 1510, loss = 1.97, batch loss = 1.91 (36.0 examples/sec; 0.222 sec/batch; 20h:27m:05s remains)
INFO - root - 2017-12-05 08:42:41.277796: step 1520, loss = 1.97, batch loss = 1.91 (38.9 examples/sec; 0.206 sec/batch; 18h:55m:35s remains)
INFO - root - 2017-12-05 08:42:43.350394: step 1530, loss = 1.96, batch loss = 1.90 (38.9 examples/sec; 0.206 sec/batch; 18h:55m:48s remains)
INFO - root - 2017-12-05 08:42:45.408275: step 1540, loss = 1.97, batch loss = 1.91 (38.2 examples/sec; 0.209 sec/batch; 19h:15m:09s remains)
INFO - root - 2017-12-05 08:42:47.516227: step 1550, loss = 1.94, batch loss = 1.88 (38.4 examples/sec; 0.209 sec/batch; 19h:10m:25s remains)
INFO - root - 2017-12-05 08:42:49.607023: step 1560, loss = 1.96, batch loss = 1.90 (39.3 examples/sec; 0.204 sec/batch; 18h:42m:46s remains)
INFO - root - 2017-12-05 08:42:51.658751: step 1570, loss = 1.97, batch loss = 1.91 (38.3 examples/sec; 0.209 sec/batch; 19h:13m:12s remains)
INFO - root - 2017-12-05 08:42:53.754649: step 1580, loss = 1.97, batch loss = 1.91 (36.9 examples/sec; 0.217 sec/batch; 19h:55m:13s remains)
INFO - root - 2017-12-05 08:42:55.853351: step 1590, loss = 1.98, batch loss = 1.92 (38.0 examples/sec; 0.210 sec/batch; 19h:20m:29s remains)
INFO - root - 2017-12-05 08:42:57.915090: step 1600, loss = 1.94, batch loss = 1.88 (36.9 examples/sec; 0.217 sec/batch; 19h:56m:01s remains)
2017-12-05 08:42:58.205599: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.0271497 -4.0192575 -4.0262775 -4.0443497 -4.0640464 -4.0852637 -4.1139073 -4.1326952 -4.130178 -4.1067457 -4.0726624 -4.0466 -4.0384517 -4.0258522 -4.0064716][-4.020051 -4.028862 -4.0470157 -4.0664039 -4.0804853 -4.0926027 -4.113524 -4.1281924 -4.1179328 -4.0806193 -4.027988 -3.9875481 -3.9762235 -3.9664173 -3.9517875][-4.0302877 -4.04662 -4.0644093 -4.0788565 -4.0836992 -4.0841789 -4.0920954 -4.0988989 -4.0835214 -4.0413518 -3.9837739 -3.9401629 -3.929384 -3.9272525 -3.9247046][-4.0459619 -4.0604224 -4.0706072 -4.0745673 -4.0658174 -4.0480266 -4.0347347 -4.0281477 -4.0118914 -3.9790509 -3.933588 -3.8979459 -3.890636 -3.895071 -3.9020953][-4.0531464 -4.0598779 -4.0593076 -4.050127 -4.0241327 -3.9822769 -3.9413495 -3.917083 -3.9097462 -3.9021192 -3.8809779 -3.8609695 -3.8600683 -3.8665612 -3.874373][-4.0366082 -4.0318928 -4.0225329 -4.0040808 -3.9643779 -3.9013319 -3.8326461 -3.790926 -3.8044314 -3.8400011 -3.8543479 -3.8521018 -3.8532009 -3.8538642 -3.8524797][-4.0035005 -3.98633 -3.9700694 -3.9464948 -3.9020059 -3.8308327 -3.7390451 -3.6731834 -3.7067144 -3.7911015 -3.8444972 -3.8603313 -3.8603904 -3.851975 -3.8424428][-3.9773126 -3.9460917 -3.9232469 -3.8995953 -3.8617816 -3.8017263 -3.708864 -3.6244004 -3.66003 -3.7680993 -3.8469894 -3.8820124 -3.8859019 -3.8709035 -3.8550959][-3.9813943 -3.936198 -3.9074025 -3.8892608 -3.8690228 -3.8336365 -3.7741797 -3.7133965 -3.7305694 -3.8073018 -3.8719504 -3.9093521 -3.9206307 -3.9096093 -3.8872695][-4.0166807 -3.9624088 -3.926578 -3.9105783 -3.9027491 -3.8870075 -3.8568611 -3.82309 -3.8255062 -3.8601549 -3.8938725 -3.9251685 -3.94518 -3.9448078 -3.9247832][-4.0578346 -4.0022526 -3.9658055 -3.9512155 -3.9500217 -3.943872 -3.9287944 -3.9118595 -3.9073057 -3.91723 -3.9275024 -3.9460864 -3.9668918 -3.9744155 -3.9638517][-4.0915632 -4.0443382 -4.0157475 -4.0097446 -4.01668 -4.0187263 -4.0104671 -3.9999769 -3.9903574 -3.9846904 -3.9779131 -3.9844692 -4.0024624 -4.0159097 -4.0147252][-4.1123991 -4.0827427 -4.0669785 -4.0679126 -4.0825324 -4.093389 -4.0922723 -4.0844517 -4.0712361 -4.0549779 -4.0382371 -4.0379381 -4.0532837 -4.0688305 -4.0686841][-4.1151605 -4.1006017 -4.095376 -4.1010251 -4.1202703 -4.1389108 -4.1453791 -4.141439 -4.1289992 -4.1094165 -4.0898938 -4.0847626 -4.093926 -4.10349 -4.097693][-4.101244 -4.0948524 -4.09644 -4.1063762 -4.1309791 -4.1549888 -4.1662126 -4.1656804 -4.1587634 -4.1431766 -4.1245346 -4.1138573 -4.1140757 -4.1159563 -4.1050005]]...]
INFO - root - 2017-12-05 08:43:00.283152: step 1610, loss = 1.99, batch loss = 1.93 (39.4 examples/sec; 0.203 sec/batch; 18h:38m:34s remains)
INFO - root - 2017-12-05 08:43:02.379304: step 1620, loss = 1.93, batch loss = 1.87 (39.2 examples/sec; 0.204 sec/batch; 18h:46m:39s remains)
INFO - root - 2017-12-05 08:43:04.492987: step 1630, loss = 1.96, batch loss = 1.90 (36.8 examples/sec; 0.218 sec/batch; 19h:59m:46s remains)
INFO - root - 2017-12-05 08:43:06.568408: step 1640, loss = 1.95, batch loss = 1.89 (39.4 examples/sec; 0.203 sec/batch; 18h:38m:36s remains)
INFO - root - 2017-12-05 08:43:08.652855: step 1650, loss = 1.92, batch loss = 1.86 (37.4 examples/sec; 0.214 sec/batch; 19h:40m:21s remains)
INFO - root - 2017-12-05 08:43:10.748470: step 1660, loss = 1.95, batch loss = 1.89 (38.9 examples/sec; 0.206 sec/batch; 18h:55m:12s remains)
INFO - root - 2017-12-05 08:43:12.858941: step 1670, loss = 1.96, batch loss = 1.90 (38.9 examples/sec; 0.206 sec/batch; 18h:54m:37s remains)
INFO - root - 2017-12-05 08:43:14.946873: step 1680, loss = 1.96, batch loss = 1.90 (38.5 examples/sec; 0.208 sec/batch; 19h:05m:38s remains)
INFO - root - 2017-12-05 08:43:17.023922: step 1690, loss = 1.94, batch loss = 1.89 (37.9 examples/sec; 0.211 sec/batch; 19h:23m:34s remains)
INFO - root - 2017-12-05 08:43:19.138938: step 1700, loss = 1.94, batch loss = 1.89 (39.4 examples/sec; 0.203 sec/batch; 18h:38m:43s remains)
2017-12-05 08:43:19.413878: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.0978208 -4.0649667 -4.0426135 -4.0221229 -4.0061822 -3.9983635 -4.0006385 -4.01103 -4.0243797 -4.0352311 -4.0322 -4.0169024 -3.9969196 -3.9797068 -3.9685664][-4.0760016 -4.0476227 -4.0265126 -4.0007858 -3.9839916 -3.9854999 -3.997751 -4.0134683 -4.0284982 -4.0394835 -4.0320492 -4.0097923 -3.9871583 -3.9716835 -3.9704769][-4.0893922 -4.0650187 -4.0405903 -4.0074267 -3.9846959 -3.9864092 -4.0051432 -4.0279469 -4.0463681 -4.0561891 -4.0503979 -4.0282078 -4.0067639 -3.9935653 -4.0012465][-4.1036153 -4.0828977 -4.0541792 -4.0179753 -3.991555 -3.9837146 -3.9964783 -4.0258193 -4.0539265 -4.0684757 -4.0695572 -4.0544 -4.0333872 -4.0166483 -4.0217705][-4.1017728 -4.0824885 -4.0507736 -4.0159273 -3.9911191 -3.9717617 -3.971467 -3.9971116 -4.0326328 -4.0576382 -4.0695291 -4.0623035 -4.0398073 -4.0168734 -4.0167603][-4.0950394 -4.0712013 -4.0365057 -4.0001559 -3.9716694 -3.9358184 -3.9125543 -3.9282889 -3.9760926 -4.0184603 -4.0458145 -4.0514131 -4.0366082 -4.0159874 -4.0112138][-4.0939484 -4.0659833 -4.0241361 -3.9778621 -3.9294987 -3.8578973 -3.7917709 -3.7902567 -3.86682 -3.9469142 -4.0038986 -4.02976 -4.0331278 -4.0274091 -4.0263934][-4.0944462 -4.0619144 -4.0132 -3.9596066 -3.8872321 -3.7675467 -3.6362653 -3.6052969 -3.7191846 -3.8483844 -3.9401724 -3.9897285 -4.0122867 -4.0223331 -4.0311456][-4.0790043 -4.046288 -4.003551 -3.9515014 -3.8741739 -3.7453706 -3.5911505 -3.538518 -3.6509938 -3.7899747 -3.8956223 -3.9616835 -3.9968007 -4.0149384 -4.0263543][-4.0491486 -4.02087 -3.99469 -3.9602332 -3.9058099 -3.8191543 -3.719589 -3.6798859 -3.7399819 -3.8327005 -3.9133193 -3.9677932 -3.9961936 -4.0105982 -4.0207348][-4.026875 -3.9992228 -3.987195 -3.969182 -3.9395633 -3.8959813 -3.8531656 -3.8403707 -3.8698306 -3.9173462 -3.9624026 -3.9937165 -4.0095353 -4.0163088 -4.0211921][-4.028615 -3.9957821 -3.9837961 -3.9732227 -3.9565225 -3.9347577 -3.9231775 -3.9295871 -3.9499831 -3.9746742 -4.0017438 -4.0192671 -4.0270815 -4.0249209 -4.0209732][-4.0548768 -4.0211644 -4.0006318 -3.9858716 -3.9735508 -3.9599462 -3.9559555 -3.9640141 -3.9784775 -3.9966104 -4.0191207 -4.0342107 -4.0387592 -4.02882 -4.0109763][-4.0824656 -4.0558319 -4.0363874 -4.0196123 -4.0077381 -3.99626 -3.9940088 -4.0001464 -4.01053 -4.02348 -4.0409822 -4.0530558 -4.05294 -4.0388994 -4.0139542][-4.0986094 -4.085135 -4.076107 -4.0652843 -4.0571032 -4.0491281 -4.0487022 -4.0525517 -4.0565209 -4.0611105 -4.0668058 -4.0720525 -4.0752954 -4.0687022 -4.0455847]]...]
INFO - root - 2017-12-05 08:43:21.507539: step 1710, loss = 2.00, batch loss = 1.95 (37.9 examples/sec; 0.211 sec/batch; 19h:25m:02s remains)
INFO - root - 2017-12-05 08:43:23.603228: step 1720, loss = 1.94, batch loss = 1.88 (36.5 examples/sec; 0.219 sec/batch; 20h:06m:43s remains)
INFO - root - 2017-12-05 08:43:25.700240: step 1730, loss = 1.95, batch loss = 1.89 (39.0 examples/sec; 0.205 sec/batch; 18h:51m:33s remains)
INFO - root - 2017-12-05 08:43:27.782236: step 1740, loss = 1.95, batch loss = 1.89 (38.6 examples/sec; 0.207 sec/batch; 19h:02m:39s remains)
INFO - root - 2017-12-05 08:43:29.878588: step 1750, loss = 1.96, batch loss = 1.90 (38.5 examples/sec; 0.208 sec/batch; 19h:04m:47s remains)
INFO - root - 2017-12-05 08:43:31.974406: step 1760, loss = 1.97, batch loss = 1.91 (38.4 examples/sec; 0.208 sec/batch; 19h:07m:01s remains)
INFO - root - 2017-12-05 08:43:34.095088: step 1770, loss = 1.95, batch loss = 1.89 (37.8 examples/sec; 0.212 sec/batch; 19h:26m:25s remains)
INFO - root - 2017-12-05 08:43:36.143350: step 1780, loss = 1.99, batch loss = 1.93 (38.7 examples/sec; 0.207 sec/batch; 18h:59m:21s remains)
INFO - root - 2017-12-05 08:43:38.262300: step 1790, loss = 1.91, batch loss = 1.85 (37.6 examples/sec; 0.213 sec/batch; 19h:32m:34s remains)
INFO - root - 2017-12-05 08:43:40.381134: step 1800, loss = 1.94, batch loss = 1.88 (38.9 examples/sec; 0.206 sec/batch; 18h:52m:49s remains)
2017-12-05 08:43:40.653525: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.0031714 -3.9703767 -3.9546664 -3.942502 -3.9326015 -3.9433827 -3.976537 -4.0140724 -4.0459595 -4.072794 -4.0938144 -4.1041651 -4.1059403 -4.1015129 -4.0872641][-3.9571867 -3.9099078 -3.8852258 -3.8718827 -3.8655663 -3.8849802 -3.9275217 -3.97138 -4.0118704 -4.0457458 -4.0663853 -4.07327 -4.069941 -4.0621533 -4.0461187][-3.9094734 -3.8442483 -3.8073971 -3.7883096 -3.7840562 -3.8102612 -3.864995 -3.9208827 -3.9737854 -4.0123243 -4.030467 -4.0356083 -4.0332685 -4.0273232 -4.0128775][-3.8877063 -3.8206465 -3.7762694 -3.7471247 -3.7322369 -3.7533085 -3.8134747 -3.8804679 -3.9458709 -3.9868419 -3.996393 -3.9977477 -3.9964819 -3.9966168 -3.9897788][-3.9101915 -3.8565049 -3.8164318 -3.7753448 -3.7385156 -3.7349486 -3.7772136 -3.8478467 -3.9242373 -3.9708989 -3.9735565 -3.964767 -3.9605215 -3.9605353 -3.9565868][-3.9321365 -3.8985374 -3.8721015 -3.8301988 -3.7722554 -3.7288458 -3.7331181 -3.7911859 -3.8779166 -3.9407368 -3.9493375 -3.9332321 -3.9227839 -3.9087739 -3.8900328][-3.9235907 -3.9181066 -3.9159434 -3.883215 -3.8058221 -3.7166095 -3.6702163 -3.7011709 -3.794013 -3.8856969 -3.9155259 -3.8948822 -3.8786824 -3.8436613 -3.796066][-3.892477 -3.9114592 -3.9289796 -3.9134116 -3.8369389 -3.7119994 -3.6060243 -3.5969543 -3.6907816 -3.8134453 -3.870997 -3.8555179 -3.8293166 -3.7725332 -3.6901202][-3.8516798 -3.8882 -3.917263 -3.9145594 -3.8488371 -3.7073734 -3.5448363 -3.4856849 -3.577641 -3.7282927 -3.8186002 -3.8220928 -3.7943039 -3.7307432 -3.624917][-3.8186607 -3.8594329 -3.8942044 -3.8981125 -3.8443413 -3.7173815 -3.5398121 -3.4457257 -3.5236802 -3.6864622 -3.7964888 -3.8199217 -3.8020811 -3.7524841 -3.66197][-3.8090246 -3.8413792 -3.878134 -3.8904514 -3.8556175 -3.7732761 -3.6468115 -3.5571051 -3.59635 -3.7242029 -3.8215091 -3.8510623 -3.8522389 -3.8339324 -3.7808223][-3.8543274 -3.8655834 -3.8980927 -3.9172468 -3.90079 -3.865243 -3.8139174 -3.7658343 -3.7726486 -3.8425164 -3.9038122 -3.9255381 -3.9371514 -3.942889 -3.9152656][-3.9528661 -3.9397218 -3.9553685 -3.9723716 -3.9645846 -3.9549901 -3.951427 -3.9418085 -3.9449995 -3.975071 -4.0072603 -4.0196285 -4.0263596 -4.0342917 -4.019486][-4.0524893 -4.0283113 -4.0228271 -4.0282679 -4.023931 -4.0249138 -4.037425 -4.0446873 -4.0531864 -4.0709243 -4.0877857 -4.0957923 -4.0970883 -4.1019993 -4.0942841][-4.1230555 -4.1014528 -4.0896912 -4.0878973 -4.0831709 -4.0863066 -4.0998831 -4.1112342 -4.1230793 -4.1358123 -4.1466932 -4.1549253 -4.1548553 -4.1568155 -4.1514587]]...]
INFO - root - 2017-12-05 08:43:42.754500: step 1810, loss = 1.91, batch loss = 1.86 (36.8 examples/sec; 0.217 sec/batch; 19h:57m:06s remains)
INFO - root - 2017-12-05 08:43:44.849618: step 1820, loss = 1.96, batch loss = 1.90 (37.5 examples/sec; 0.213 sec/batch; 19h:36m:18s remains)
INFO - root - 2017-12-05 08:43:46.931077: step 1830, loss = 1.90, batch loss = 1.84 (39.0 examples/sec; 0.205 sec/batch; 18h:51m:11s remains)
INFO - root - 2017-12-05 08:43:49.028552: step 1840, loss = 1.93, batch loss = 1.87 (37.5 examples/sec; 0.213 sec/batch; 19h:35m:03s remains)
INFO - root - 2017-12-05 08:43:51.122543: step 1850, loss = 1.95, batch loss = 1.89 (38.8 examples/sec; 0.206 sec/batch; 18h:56m:29s remains)
INFO - root - 2017-12-05 08:43:53.193221: step 1860, loss = 1.88, batch loss = 1.82 (39.3 examples/sec; 0.204 sec/batch; 18h:42m:20s remains)
INFO - root - 2017-12-05 08:43:55.247553: step 1870, loss = 1.95, batch loss = 1.89 (39.5 examples/sec; 0.202 sec/batch; 18h:35m:40s remains)
INFO - root - 2017-12-05 08:43:57.310943: step 1880, loss = 1.94, batch loss = 1.88 (39.2 examples/sec; 0.204 sec/batch; 18h:43m:36s remains)
INFO - root - 2017-12-05 08:43:59.414138: step 1890, loss = 1.93, batch loss = 1.87 (36.2 examples/sec; 0.221 sec/batch; 20h:18m:50s remains)
INFO - root - 2017-12-05 08:44:01.512663: step 1900, loss = 1.92, batch loss = 1.86 (38.7 examples/sec; 0.207 sec/batch; 18h:57m:54s remains)
2017-12-05 08:44:01.816202: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.9088712 -3.8691044 -3.8808875 -3.9137049 -3.9267309 -3.9477835 -3.9752069 -4.0009875 -4.0240593 -4.0304461 -4.0105195 -3.9865389 -3.9565847 -3.9536989 -3.9770079][-3.8055923 -3.7333148 -3.7211483 -3.7577724 -3.7952785 -3.8348637 -3.8639646 -3.895319 -3.9303737 -3.9458833 -3.9310434 -3.9149137 -3.8938859 -3.8896766 -3.9055192][-3.7434142 -3.6477122 -3.6018562 -3.6246331 -3.6799381 -3.7392106 -3.7716503 -3.8048732 -3.8430138 -3.8631968 -3.8565192 -3.8606465 -3.8576262 -3.8526835 -3.8598592][-3.7085671 -3.5990248 -3.523519 -3.5246668 -3.586904 -3.6604054 -3.7004259 -3.7370565 -3.7825096 -3.8053341 -3.8072102 -3.8301809 -3.8426619 -3.8382473 -3.8371949][-3.7087257 -3.5951946 -3.5033939 -3.4817872 -3.5301414 -3.5967503 -3.6360521 -3.6823406 -3.7447045 -3.7789602 -3.7878911 -3.8151019 -3.8291497 -3.8264351 -3.8242836][-3.7261288 -3.624063 -3.5354509 -3.4921212 -3.5059853 -3.5417371 -3.5712061 -3.6244493 -3.6959794 -3.7435012 -3.7558861 -3.779635 -3.7922208 -3.7946022 -3.7994089][-3.735209 -3.6525233 -3.5776157 -3.5224717 -3.5052066 -3.5104313 -3.5267546 -3.5740855 -3.6461892 -3.7027173 -3.7115355 -3.7283912 -3.7424219 -3.758373 -3.7773712][-3.7221088 -3.6591232 -3.603076 -3.556478 -3.5268488 -3.5101981 -3.5072739 -3.5363307 -3.6007597 -3.6574342 -3.6678405 -3.68671 -3.7125106 -3.745332 -3.7769639][-3.7147136 -3.6599364 -3.6149564 -3.5781684 -3.5510197 -3.5252914 -3.5091414 -3.5167279 -3.5585189 -3.6061928 -3.6229439 -3.6519601 -3.6922553 -3.7387538 -3.7798622][-3.7390611 -3.6898954 -3.6484241 -3.6134639 -3.5856011 -3.5562086 -3.5311894 -3.5219946 -3.5407281 -3.5803072 -3.6085806 -3.6545997 -3.7082272 -3.7636926 -3.8091674][-3.7909236 -3.7505488 -3.7144508 -3.6794682 -3.6502953 -3.6220732 -3.5955462 -3.5775011 -3.5783529 -3.6020203 -3.6266022 -3.6773176 -3.7374721 -3.7969065 -3.8436532][-3.88529 -3.8504112 -3.8190379 -3.7885346 -3.76447 -3.7441564 -3.7246218 -3.7072446 -3.6978045 -3.7013812 -3.706212 -3.7403069 -3.7908854 -3.8441105 -3.8873763][-4.0162621 -3.990597 -3.9652603 -3.9400091 -3.9215546 -3.9075353 -3.8945811 -3.8821077 -3.8707418 -3.862401 -3.8475268 -3.8573625 -3.885303 -3.9215052 -3.9552498][-4.1396379 -4.1243095 -4.10736 -4.0890694 -4.0752683 -4.0658159 -4.0573382 -4.048842 -4.0398288 -4.0299745 -4.0097594 -4.0045791 -4.0117078 -4.029213 -4.0494647][-4.2390203 -4.2326016 -4.2242293 -4.2139769 -4.2054963 -4.1992564 -4.1927705 -4.1856513 -4.1783667 -4.1698108 -4.1527395 -4.1424494 -4.1392803 -4.1434188 -4.1529841]]...]
INFO - root - 2017-12-05 08:44:03.891651: step 1910, loss = 1.90, batch loss = 1.84 (38.3 examples/sec; 0.209 sec/batch; 19h:10m:27s remains)
INFO - root - 2017-12-05 08:44:05.999714: step 1920, loss = 1.95, batch loss = 1.89 (38.3 examples/sec; 0.209 sec/batch; 19h:11m:15s remains)
INFO - root - 2017-12-05 08:44:08.078064: step 1930, loss = 1.92, batch loss = 1.86 (38.7 examples/sec; 0.207 sec/batch; 18h:59m:28s remains)
INFO - root - 2017-12-05 08:44:10.155956: step 1940, loss = 1.95, batch loss = 1.89 (39.4 examples/sec; 0.203 sec/batch; 18h:38m:39s remains)
INFO - root - 2017-12-05 08:44:12.260612: step 1950, loss = 1.95, batch loss = 1.89 (39.5 examples/sec; 0.203 sec/batch; 18h:36m:21s remains)
INFO - root - 2017-12-05 08:44:14.359835: step 1960, loss = 1.93, batch loss = 1.87 (35.7 examples/sec; 0.224 sec/batch; 20h:36m:07s remains)
INFO - root - 2017-12-05 08:44:16.446035: step 1970, loss = 1.98, batch loss = 1.92 (37.9 examples/sec; 0.211 sec/batch; 19h:23m:54s remains)
INFO - root - 2017-12-05 08:44:18.515771: step 1980, loss = 1.90, batch loss = 1.84 (39.6 examples/sec; 0.202 sec/batch; 18h:32m:23s remains)
INFO - root - 2017-12-05 08:44:20.587866: step 1990, loss = 1.96, batch loss = 1.90 (39.2 examples/sec; 0.204 sec/batch; 18h:43m:50s remains)
INFO - root - 2017-12-05 08:44:22.632135: step 2000, loss = 1.93, batch loss = 1.87 (39.2 examples/sec; 0.204 sec/batch; 18h:43m:19s remains)
2017-12-05 08:44:22.930034: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3659 -4.3619289 -4.3601408 -4.3597126 -4.3583164 -4.3565722 -4.3549423 -4.3514686 -4.3451328 -4.3380952 -4.3348694 -4.3358126 -4.3393526 -4.3457527 -4.3527808][-4.3502994 -4.3391891 -4.3317966 -4.3273845 -4.3233776 -4.320343 -4.3156376 -4.3033056 -4.2855625 -4.2679725 -4.2588797 -4.2623653 -4.2760916 -4.2980728 -4.3202796][-4.3236861 -4.3033628 -4.288096 -4.2767367 -4.2675118 -4.2614617 -4.2493486 -4.2217121 -4.1874552 -4.15655 -4.1418095 -4.1505461 -4.1809769 -4.2276435 -4.2722631][-4.2879796 -4.2551212 -4.2286067 -4.2072558 -4.1892695 -4.1768236 -4.1540875 -4.1085362 -4.0530596 -4.0017858 -3.9795089 -3.999068 -4.055223 -4.1336355 -4.208025][-4.2392263 -4.1922188 -4.1518273 -4.1187358 -4.0917163 -4.0725107 -4.0386744 -3.9759693 -3.9005144 -3.8295932 -3.799721 -3.8322594 -3.916213 -4.0289383 -4.1378331][-4.1675024 -4.1030226 -4.0487785 -4.0073309 -3.9788296 -3.9595022 -3.9184158 -3.84129 -3.7578611 -3.6872468 -3.669126 -3.7219338 -3.8292556 -3.9654264 -4.0951033][-4.0786338 -3.9940293 -3.9247015 -3.8748932 -3.84485 -3.8335428 -3.7972412 -3.719095 -3.6520085 -3.6162071 -3.6335845 -3.7116311 -3.8332608 -3.9732425 -4.0998945][-3.9883156 -3.8768806 -3.7782741 -3.7151678 -3.689796 -3.6996696 -3.6872129 -3.6301842 -3.6029925 -3.6230149 -3.6774607 -3.7697768 -3.8936698 -4.0239434 -4.1358919][-3.9105351 -3.7708473 -3.6323767 -3.5578976 -3.5583751 -3.6121886 -3.6401951 -3.630414 -3.649374 -3.7055929 -3.7766213 -3.8669224 -3.9792545 -4.0913186 -4.1828022][-3.8823769 -3.7306213 -3.5773265 -3.5128057 -3.5501976 -3.6425579 -3.7101896 -3.7399261 -3.7815952 -3.8426726 -3.9107981 -3.9886353 -4.0773344 -4.1619172 -4.2291708][-3.9368362 -3.8177004 -3.7088256 -3.6736093 -3.7173769 -3.8036108 -3.8769538 -3.9165835 -3.9561384 -4.0032988 -4.05376 -4.1102209 -4.1721306 -4.2282476 -4.2717776][-4.049593 -3.9731767 -3.915797 -3.9061427 -3.941324 -4.0061388 -4.0632715 -4.0943995 -4.12017 -4.1465645 -4.175849 -4.2110586 -4.24999 -4.2828255 -4.3077044][-4.1652207 -4.1221514 -4.0989571 -4.1049781 -4.1323395 -4.1755276 -4.2120538 -4.2302928 -4.2416983 -4.2521796 -4.2659926 -4.283998 -4.3054166 -4.3217845 -4.3342209][-4.2553596 -4.2330809 -4.2265444 -4.2359138 -4.253274 -4.2797604 -4.3008513 -4.3104334 -4.315485 -4.3195519 -4.3239732 -4.3305507 -4.3396735 -4.3462944 -4.3518171][-4.3173881 -4.3079715 -4.3070278 -4.3126879 -4.32265 -4.3362164 -4.3461161 -4.35079 -4.3525615 -4.3541584 -4.3556771 -4.357254 -4.3598094 -4.3611937 -4.3626151]]...]
INFO - root - 2017-12-05 08:44:25.009166: step 2010, loss = 1.94, batch loss = 1.89 (38.5 examples/sec; 0.208 sec/batch; 19h:03m:42s remains)
INFO - root - 2017-12-05 08:44:27.111537: step 2020, loss = 1.95, batch loss = 1.89 (37.2 examples/sec; 0.215 sec/batch; 19h:43m:37s remains)
INFO - root - 2017-12-05 08:44:29.187052: step 2030, loss = 1.89, batch loss = 1.83 (39.4 examples/sec; 0.203 sec/batch; 18h:38m:17s remains)
INFO - root - 2017-12-05 08:44:31.262992: step 2040, loss = 1.90, batch loss = 1.84 (39.0 examples/sec; 0.205 sec/batch; 18h:49m:29s remains)
INFO - root - 2017-12-05 08:44:33.330861: step 2050, loss = 1.94, batch loss = 1.88 (39.7 examples/sec; 0.201 sec/batch; 18h:28m:44s remains)
INFO - root - 2017-12-05 08:44:35.369439: step 2060, loss = 1.89, batch loss = 1.83 (39.3 examples/sec; 0.203 sec/batch; 18h:39m:41s remains)
INFO - root - 2017-12-05 08:44:37.412493: step 2070, loss = 1.92, batch loss = 1.87 (38.8 examples/sec; 0.206 sec/batch; 18h:55m:29s remains)
INFO - root - 2017-12-05 08:44:39.491908: step 2080, loss = 1.97, batch loss = 1.91 (38.7 examples/sec; 0.207 sec/batch; 18h:58m:55s remains)
INFO - root - 2017-12-05 08:44:41.574754: step 2090, loss = 1.91, batch loss = 1.85 (40.0 examples/sec; 0.200 sec/batch; 18h:22m:34s remains)
INFO - root - 2017-12-05 08:44:43.660435: step 2100, loss = 1.93, batch loss = 1.87 (39.0 examples/sec; 0.205 sec/batch; 18h:50m:33s remains)
2017-12-05 08:44:43.972896: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.9196737 -3.8992345 -3.9073722 -3.9305849 -3.9457967 -3.9521377 -3.9607611 -3.9684467 -3.9830828 -4.0157633 -4.0535593 -4.092134 -4.1216373 -4.1426263 -4.1441045][-3.922502 -3.9119139 -3.9278514 -3.9513474 -3.9654245 -3.9734812 -3.9813781 -3.9918034 -4.0156059 -4.0529528 -4.0866327 -4.1157365 -4.1399875 -4.155477 -4.1506367][-3.9396272 -3.9278591 -3.9449916 -3.9680014 -3.979274 -3.9830761 -3.9882712 -4.0021415 -4.0322862 -4.0728455 -4.1077919 -4.1371422 -4.1644373 -4.1772504 -4.167716][-3.9599328 -3.945087 -3.9630442 -3.9843438 -3.9859712 -3.9749389 -3.9705555 -3.9808605 -4.0178051 -4.0663314 -4.1043 -4.138206 -4.1690979 -4.1810021 -4.1722212][-3.977072 -3.9655681 -3.9824018 -3.9973588 -3.9778719 -3.9385345 -3.9124217 -3.9165807 -3.9644194 -4.0295439 -4.0799208 -4.1257133 -4.1651807 -4.1807241 -4.1760283][-3.997478 -3.9907742 -4.0032506 -4.0067029 -3.965333 -3.893244 -3.8329978 -3.8252091 -3.8856304 -3.9698107 -4.0404105 -4.1026764 -4.1540432 -4.1791759 -4.1824322][-4.0189476 -4.0202847 -4.028502 -4.0213151 -3.9707971 -3.8805637 -3.7894483 -3.7596231 -3.8218622 -3.9178872 -4.0039983 -4.08068 -4.1424375 -4.1745014 -4.1827664][-4.0420775 -4.0478845 -4.0503688 -4.0347352 -3.9840715 -3.8970931 -3.7981341 -3.7543836 -3.8043084 -3.8957055 -3.9799213 -4.0616064 -4.1253967 -4.1566162 -4.1626015][-4.0448971 -4.0525308 -4.049387 -4.0287318 -3.9834998 -3.9070387 -3.8147175 -3.7709689 -3.8057671 -3.8880672 -3.9684241 -4.0481219 -4.1090789 -4.1349235 -4.1329885][-4.0343223 -4.0414028 -4.0358658 -4.0128345 -3.9728954 -3.9053352 -3.8242848 -3.78789 -3.816283 -3.8892055 -3.9612026 -4.0326443 -4.089581 -4.1112156 -4.104836][-4.03787 -4.0381522 -4.0290365 -4.0058317 -3.9701397 -3.9105976 -3.8406966 -3.8129733 -3.8434708 -3.9100168 -3.9711056 -4.0261316 -4.0706096 -4.0875893 -4.0819745][-4.0601668 -4.0508013 -4.0347614 -4.0102215 -3.9792428 -3.9298322 -3.8756008 -3.8559139 -3.8843987 -3.9384613 -3.983618 -4.0196962 -4.0446739 -4.0519743 -4.052197][-4.0875664 -4.0758328 -4.0553255 -4.0280972 -3.9986095 -3.9591622 -3.9223602 -3.9119713 -3.9369285 -3.9783809 -4.0057182 -4.0129671 -4.005578 -3.9944959 -3.9955857][-4.0992441 -4.09451 -4.0806532 -4.0598741 -4.0352683 -4.0058627 -3.9804006 -3.9768248 -3.9975033 -4.0254836 -4.0362926 -4.0181661 -3.9774566 -3.9382794 -3.9331243][-4.0798821 -4.0842915 -4.0857959 -4.0814672 -4.069818 -4.05129 -4.036881 -4.0389824 -4.0562372 -4.0717239 -4.0688233 -4.0338831 -3.9740751 -3.9134631 -3.8957393]]...]
INFO - root - 2017-12-05 08:44:46.036616: step 2110, loss = 1.96, batch loss = 1.90 (39.0 examples/sec; 0.205 sec/batch; 18h:50m:08s remains)
INFO - root - 2017-12-05 08:44:48.075036: step 2120, loss = 1.91, batch loss = 1.85 (39.5 examples/sec; 0.203 sec/batch; 18h:35m:49s remains)
INFO - root - 2017-12-05 08:44:50.109958: step 2130, loss = 1.89, batch loss = 1.83 (38.7 examples/sec; 0.207 sec/batch; 18h:57m:47s remains)
INFO - root - 2017-12-05 08:44:52.195608: step 2140, loss = 1.93, batch loss = 1.87 (38.5 examples/sec; 0.208 sec/batch; 19h:04m:51s remains)
INFO - root - 2017-12-05 08:44:54.324240: step 2150, loss = 1.93, batch loss = 1.87 (34.8 examples/sec; 0.230 sec/batch; 21h:05m:14s remains)
INFO - root - 2017-12-05 08:44:56.428636: step 2160, loss = 1.91, batch loss = 1.85 (37.1 examples/sec; 0.216 sec/batch; 19h:48m:39s remains)
INFO - root - 2017-12-05 08:44:58.543268: step 2170, loss = 1.93, batch loss = 1.87 (38.5 examples/sec; 0.208 sec/batch; 19h:04m:22s remains)
INFO - root - 2017-12-05 08:45:00.619284: step 2180, loss = 1.92, batch loss = 1.87 (37.6 examples/sec; 0.213 sec/batch; 19h:30m:30s remains)
INFO - root - 2017-12-05 08:45:02.703638: step 2190, loss = 1.86, batch loss = 1.81 (39.1 examples/sec; 0.205 sec/batch; 18h:47m:36s remains)
INFO - root - 2017-12-05 08:45:04.785796: step 2200, loss = 1.90, batch loss = 1.84 (37.8 examples/sec; 0.211 sec/batch; 19h:24m:12s remains)
2017-12-05 08:45:05.064368: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.9469934 -3.92526 -3.9874597 -4.0574183 -4.1238446 -4.1786551 -4.2135572 -4.2265644 -4.2434783 -4.269145 -4.2917285 -4.3087215 -4.3170185 -4.3193769 -4.3181033][-3.8281093 -3.8054101 -3.8910058 -3.9819162 -4.0589409 -4.115478 -4.1455884 -4.1598511 -4.1868548 -4.2250791 -4.2619295 -4.2920618 -4.30849 -4.3121705 -4.3102937][-3.7269671 -3.7066872 -3.8206177 -3.9290445 -4.0066175 -4.0543385 -4.0731692 -4.086791 -4.1238508 -4.1747632 -4.2290707 -4.2745247 -4.3010736 -4.3068123 -4.3043127][-3.7161627 -3.7037225 -3.8241982 -3.9266295 -3.980607 -3.9981117 -3.9922671 -3.99918 -4.0449004 -4.1108589 -4.1853161 -4.2489014 -4.28751 -4.2991476 -4.2991343][-3.8004532 -3.7965999 -3.8885121 -3.955497 -3.9686093 -3.939178 -3.8934989 -3.8889427 -3.9522054 -4.0402513 -4.1327691 -4.2139168 -4.2648835 -4.2848167 -4.2907796][-3.882977 -3.8800528 -3.936332 -3.9663181 -3.9438512 -3.8669991 -3.7734826 -3.7612379 -3.8543534 -3.9722586 -4.0846453 -4.1782188 -4.2376332 -4.26533 -4.2800307][-3.9294319 -3.9285774 -3.9631314 -3.9663661 -3.9185176 -3.7983291 -3.6567585 -3.6409485 -3.7712016 -3.9168718 -4.0443044 -4.1440053 -4.2092104 -4.2444768 -4.2682123][-3.9414549 -3.9453123 -3.9723785 -3.9642248 -3.9091744 -3.7774565 -3.6234982 -3.6111393 -3.75538 -3.9037366 -4.0243964 -4.12003 -4.1856985 -4.2251835 -4.2549896][-3.9132102 -3.92636 -3.962971 -3.9635527 -3.9201038 -3.8195128 -3.7077355 -3.7022457 -3.8174806 -3.9346371 -4.0311928 -4.1140733 -4.1733441 -4.2111731 -4.24374][-3.8411505 -3.8662353 -3.9271338 -3.9509559 -3.9335818 -3.8755748 -3.8092232 -3.8092222 -3.8904729 -3.9759166 -4.0530825 -4.124661 -4.1741362 -4.2064509 -4.2383232][-3.7480152 -3.7872515 -3.8806803 -3.9317036 -3.9396336 -3.9109914 -3.8734636 -3.8818212 -3.9435017 -4.0104656 -4.0799704 -4.1430244 -4.1832185 -4.2090559 -4.2390347][-3.7043405 -3.7559245 -3.872086 -3.9375205 -3.9513719 -3.9276941 -3.8982344 -3.9130507 -3.9670885 -4.0262032 -4.095418 -4.1555147 -4.1931129 -4.2167449 -4.2446237][-3.7649565 -3.8087411 -3.9100649 -3.9624543 -3.9601126 -3.9164162 -3.8761978 -3.8884616 -3.9495323 -4.01696 -4.0914392 -4.1538057 -4.1944752 -4.2188764 -4.2445478][-3.8501959 -3.8753998 -3.9434948 -3.9710174 -3.947293 -3.8727605 -3.7978678 -3.8029368 -3.8869786 -3.981699 -4.0726995 -4.13778 -4.1764212 -4.1986833 -4.2252789][-3.9139018 -3.9233804 -3.9643161 -3.9693048 -3.9239028 -3.8091547 -3.6866481 -3.6862621 -3.8106527 -3.9401073 -4.0430055 -4.1056986 -4.137197 -4.1561418 -4.1858959]]...]
INFO - root - 2017-12-05 08:45:07.176741: step 2210, loss = 1.92, batch loss = 1.86 (38.0 examples/sec; 0.210 sec/batch; 19h:18m:06s remains)
INFO - root - 2017-12-05 08:45:09.315899: step 2220, loss = 1.90, batch loss = 1.85 (38.3 examples/sec; 0.209 sec/batch; 19h:09m:23s remains)
INFO - root - 2017-12-05 08:45:11.357197: step 2230, loss = 1.92, batch loss = 1.86 (38.6 examples/sec; 0.207 sec/batch; 19h:00m:46s remains)
INFO - root - 2017-12-05 08:45:13.417043: step 2240, loss = 1.92, batch loss = 1.86 (38.8 examples/sec; 0.206 sec/batch; 18h:55m:32s remains)
INFO - root - 2017-12-05 08:45:15.523371: step 2250, loss = 1.89, batch loss = 1.83 (37.9 examples/sec; 0.211 sec/batch; 19h:22m:49s remains)
INFO - root - 2017-12-05 08:45:17.615111: step 2260, loss = 1.86, batch loss = 1.80 (39.1 examples/sec; 0.205 sec/batch; 18h:46m:49s remains)
INFO - root - 2017-12-05 08:45:19.709410: step 2270, loss = 1.91, batch loss = 1.85 (39.6 examples/sec; 0.202 sec/batch; 18h:33m:04s remains)
INFO - root - 2017-12-05 08:45:21.796913: step 2280, loss = 1.93, batch loss = 1.87 (38.1 examples/sec; 0.210 sec/batch; 19h:15m:07s remains)
INFO - root - 2017-12-05 08:45:23.879852: step 2290, loss = 1.93, batch loss = 1.87 (38.4 examples/sec; 0.208 sec/batch; 19h:06m:41s remains)
INFO - root - 2017-12-05 08:45:25.939306: step 2300, loss = 1.88, batch loss = 1.82 (38.0 examples/sec; 0.211 sec/batch; 19h:19m:29s remains)
2017-12-05 08:45:26.217651: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.9486005 -3.931078 -3.9131455 -3.8695066 -3.8072796 -3.7488377 -3.7013912 -3.6452157 -3.602788 -3.568742 -3.5292554 -3.4911046 -3.4752614 -3.5065341 -3.5976157][-3.9151044 -3.8913631 -3.8638697 -3.8163145 -3.7520721 -3.6861651 -3.6235564 -3.5541868 -3.4942365 -3.4348061 -3.3975 -3.4035234 -3.4472156 -3.5154743 -3.6239815][-3.9105909 -3.8728561 -3.8252518 -3.7625511 -3.6925468 -3.623276 -3.5547547 -3.4782286 -3.402117 -3.3288932 -3.3164649 -3.3798087 -3.4834666 -3.5828743 -3.7041607][-3.8932381 -3.8437219 -3.7865343 -3.7200198 -3.6522434 -3.5887651 -3.5228531 -3.4457583 -3.3648505 -3.3083887 -3.3280191 -3.4155962 -3.5351419 -3.6480575 -3.7761347][-3.8633561 -3.8124628 -3.7617154 -3.7002897 -3.6378286 -3.5772429 -3.5032625 -3.4187379 -3.3513505 -3.3452554 -3.3962536 -3.4885464 -3.6050255 -3.7220421 -3.8474197][-3.8102629 -3.7661312 -3.733356 -3.6851919 -3.6189435 -3.5387745 -3.4324274 -3.3236182 -3.2774296 -3.3474617 -3.4512949 -3.5593431 -3.6807847 -3.804131 -3.9210413][-3.7245855 -3.6888745 -3.6802897 -3.6459389 -3.568476 -3.4570224 -3.3107634 -3.1749706 -3.1650023 -3.3181691 -3.4773707 -3.6075525 -3.740212 -3.8680155 -3.9756711][-3.6286945 -3.598701 -3.6041315 -3.5834672 -3.507122 -3.3920112 -3.2480903 -3.1331975 -3.1629035 -3.349165 -3.5276151 -3.6668031 -3.7992554 -3.92082 -4.016849][-3.5082741 -3.4823329 -3.5078423 -3.5200644 -3.4866323 -3.425173 -3.3509016 -3.3065 -3.3555336 -3.4987521 -3.6394646 -3.76 -3.8717496 -3.9716139 -4.0519466][-3.4114459 -3.4138789 -3.4765503 -3.5227046 -3.5322866 -3.522341 -3.5053148 -3.5052769 -3.5575082 -3.6604633 -3.7679935 -3.8652804 -3.9525549 -4.0272961 -4.0924168][-3.4078891 -3.4403756 -3.5192957 -3.5736427 -3.5963964 -3.6051538 -3.6115351 -3.6352596 -3.6919279 -3.7786787 -3.87131 -3.9587355 -4.0327082 -4.0900464 -4.1414857][-3.4866295 -3.5205486 -3.58686 -3.6288939 -3.6480274 -3.6584663 -3.6745303 -3.7169242 -3.785284 -3.8634226 -3.9445939 -4.0260954 -4.0926957 -4.1425343 -4.185492][-3.5810976 -3.6037405 -3.6469176 -3.67493 -3.693119 -3.708925 -3.7342114 -3.7850227 -3.856631 -3.9305222 -4.0026355 -4.0786514 -4.1399512 -4.1847558 -4.2191343][-3.6395395 -3.6538904 -3.687546 -3.7118933 -3.7370949 -3.7708931 -3.8158224 -3.8712752 -3.9376075 -4.0064349 -4.0708685 -4.136168 -4.1862884 -4.2198739 -4.2420449][-3.687067 -3.7056875 -3.7434363 -3.7699246 -3.8002892 -3.8461537 -3.9056664 -3.9653409 -4.0279522 -4.0870481 -4.1381292 -4.1861711 -4.2188358 -4.2389975 -4.2504749]]...]
INFO - root - 2017-12-05 08:45:28.295956: step 2310, loss = 1.90, batch loss = 1.85 (39.2 examples/sec; 0.204 sec/batch; 18h:42m:44s remains)
INFO - root - 2017-12-05 08:45:30.376731: step 2320, loss = 1.89, batch loss = 1.83 (38.6 examples/sec; 0.207 sec/batch; 18h:59m:46s remains)
INFO - root - 2017-12-05 08:45:32.453917: step 2330, loss = 1.89, batch loss = 1.83 (39.3 examples/sec; 0.204 sec/batch; 18h:40m:58s remains)
INFO - root - 2017-12-05 08:45:34.569985: step 2340, loss = 1.87, batch loss = 1.82 (38.4 examples/sec; 0.208 sec/batch; 19h:07m:12s remains)
INFO - root - 2017-12-05 08:45:36.641657: step 2350, loss = 1.90, batch loss = 1.84 (37.8 examples/sec; 0.212 sec/batch; 19h:25m:26s remains)
INFO - root - 2017-12-05 08:45:38.731778: step 2360, loss = 1.92, batch loss = 1.86 (38.7 examples/sec; 0.207 sec/batch; 18h:56m:28s remains)
INFO - root - 2017-12-05 08:45:40.796605: step 2370, loss = 1.89, batch loss = 1.83 (37.5 examples/sec; 0.213 sec/batch; 19h:32m:36s remains)
INFO - root - 2017-12-05 08:45:42.875788: step 2380, loss = 1.85, batch loss = 1.79 (37.9 examples/sec; 0.211 sec/batch; 19h:20m:25s remains)
INFO - root - 2017-12-05 08:45:44.981087: step 2390, loss = 1.91, batch loss = 1.85 (37.7 examples/sec; 0.212 sec/batch; 19h:26m:42s remains)
INFO - root - 2017-12-05 08:45:47.085413: step 2400, loss = 1.86, batch loss = 1.80 (39.0 examples/sec; 0.205 sec/batch; 18h:49m:57s remains)
2017-12-05 08:45:47.372224: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.8718877 -3.888063 -3.9317658 -3.9630833 -3.9873424 -4.0028534 -3.9843469 -3.9422567 -3.9147136 -3.9274647 -3.9616306 -4.0162644 -4.07801 -4.1404324 -4.1984305][-3.829227 -3.8570213 -3.9126716 -3.9507291 -3.971303 -3.9743257 -3.945596 -3.8985996 -3.8711321 -3.8870461 -3.9275296 -3.9923406 -4.0610681 -4.1255455 -4.1860771][-3.8141141 -3.8436205 -3.9031591 -3.9411929 -3.9523087 -3.9389286 -3.9036472 -3.8590617 -3.836077 -3.858433 -3.9070039 -3.9771895 -4.0484929 -4.1150818 -4.1790423][-3.8209763 -3.8437355 -3.8894114 -3.9074225 -3.8983762 -3.8690383 -3.8300073 -3.7938087 -3.7849898 -3.8248544 -3.888839 -3.9658937 -4.0374823 -4.1066513 -4.1744013][-3.8301649 -3.839159 -3.859724 -3.8489277 -3.8154774 -3.7674084 -3.7196679 -3.6901722 -3.7031634 -3.7713439 -3.8564394 -3.9432707 -4.0206847 -4.0971556 -4.1703711][-3.824523 -3.8153617 -3.8098328 -3.7744145 -3.7196629 -3.6554275 -3.591907 -3.5523381 -3.5809817 -3.6784217 -3.7870281 -3.8918624 -3.9866338 -4.0760651 -4.1578112][-3.7963796 -3.7731862 -3.7550263 -3.7109742 -3.6525416 -3.5817893 -3.5029058 -3.442533 -3.4684603 -3.5773742 -3.703362 -3.8283105 -3.9438338 -4.0494018 -4.1404772][-3.779417 -3.7565804 -3.7395778 -3.7072873 -3.6692529 -3.6193337 -3.5457449 -3.4794471 -3.4861643 -3.5699658 -3.6837721 -3.8090422 -3.9291139 -4.0397453 -4.1336074][-3.8117266 -3.7971563 -3.7923725 -3.7823796 -3.7683394 -3.7445769 -3.6951411 -3.641078 -3.6324525 -3.6799545 -3.7608347 -3.8598614 -3.9611769 -4.057899 -4.1422286][-3.9025109 -3.8927181 -3.8992372 -3.9046273 -3.9031472 -3.8927109 -3.8654692 -3.8281417 -3.8125017 -3.8344815 -3.8849702 -3.9503174 -4.0218797 -4.0953135 -4.164444][-4.0273046 -4.0204067 -4.0283623 -4.0320539 -4.0303922 -4.0239024 -4.0107851 -3.9884212 -3.9747486 -3.98424 -4.012856 -4.0501595 -4.0940104 -4.1450138 -4.1964417][-4.1477137 -4.143085 -4.1465874 -4.1455584 -4.14296 -4.138804 -4.1353612 -4.1256785 -4.118257 -4.120811 -4.1334348 -4.1507559 -4.17369 -4.2033505 -4.2367158][-4.2391114 -4.2344289 -4.23485 -4.2332911 -4.2334824 -4.2340579 -4.2353258 -4.2322216 -4.2275653 -4.22627 -4.2305946 -4.2382479 -4.2481904 -4.2628865 -4.280859][-4.2972221 -4.2927208 -4.2924824 -4.2919493 -4.2939258 -4.2961721 -4.2975807 -4.2961488 -4.29327 -4.291543 -4.2931757 -4.2975006 -4.3024917 -4.3094635 -4.3180075][-4.3288317 -4.3244877 -4.3242192 -4.3245077 -4.3257074 -4.3267007 -4.3271327 -4.3267574 -4.3263192 -4.3266339 -4.3285561 -4.3312035 -4.3338175 -4.337121 -4.3409548]]...]
INFO - root - 2017-12-05 08:45:49.463726: step 2410, loss = 1.90, batch loss = 1.84 (38.5 examples/sec; 0.208 sec/batch; 19h:03m:54s remains)
INFO - root - 2017-12-05 08:45:51.541115: step 2420, loss = 1.91, batch loss = 1.85 (38.4 examples/sec; 0.208 sec/batch; 19h:06m:40s remains)
INFO - root - 2017-12-05 08:45:53.612124: step 2430, loss = 1.84, batch loss = 1.79 (38.5 examples/sec; 0.208 sec/batch; 19h:03m:36s remains)
INFO - root - 2017-12-05 08:45:55.674321: step 2440, loss = 1.84, batch loss = 1.78 (38.9 examples/sec; 0.205 sec/batch; 18h:49m:54s remains)
INFO - root - 2017-12-05 08:45:57.750724: step 2450, loss = 1.83, batch loss = 1.78 (39.4 examples/sec; 0.203 sec/batch; 18h:36m:02s remains)
INFO - root - 2017-12-05 08:45:59.834847: step 2460, loss = 1.85, batch loss = 1.79 (37.7 examples/sec; 0.212 sec/batch; 19h:25m:56s remains)
INFO - root - 2017-12-05 08:46:01.931433: step 2470, loss = 1.86, batch loss = 1.80 (36.7 examples/sec; 0.218 sec/batch; 19h:57m:39s remains)
INFO - root - 2017-12-05 08:46:04.003779: step 2480, loss = 1.89, batch loss = 1.83 (39.1 examples/sec; 0.205 sec/batch; 18h:45m:03s remains)
INFO - root - 2017-12-05 08:46:06.103018: step 2490, loss = 1.89, batch loss = 1.83 (38.7 examples/sec; 0.207 sec/batch; 18h:56m:15s remains)
INFO - root - 2017-12-05 08:46:08.191823: step 2500, loss = 1.87, batch loss = 1.81 (38.1 examples/sec; 0.210 sec/batch; 19h:13m:52s remains)
2017-12-05 08:46:08.487456: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3536091 -4.3464541 -4.3425336 -4.3435392 -4.3458109 -4.34736 -4.3486185 -4.350306 -4.3523917 -4.3542948 -4.3566623 -4.359982 -4.3628931 -4.36564 -4.3683538][-4.3307438 -4.3154368 -4.3060966 -4.30752 -4.3135118 -4.3190618 -4.3252373 -4.3311491 -4.3356276 -4.3386993 -4.3426394 -4.3484664 -4.3537245 -4.3584471 -4.3633251][-4.2945175 -4.2673569 -4.2502193 -4.2516308 -4.2606173 -4.2682252 -4.2787633 -4.2901869 -4.2972221 -4.3013616 -4.3080091 -4.32022 -4.3332663 -4.3444247 -4.3539219][-4.2491765 -4.209981 -4.183989 -4.1815209 -4.1892366 -4.1930108 -4.2021284 -4.21833 -4.2300191 -4.2358479 -4.2455039 -4.2684541 -4.2959723 -4.3198419 -4.3376155][-4.1974378 -4.1458583 -4.10836 -4.0955582 -4.0967422 -4.0940189 -4.1004233 -4.1216478 -4.1402693 -4.1488581 -4.1614661 -4.1958914 -4.2424417 -4.2846694 -4.3140788][-4.1372256 -4.0715556 -4.0176778 -3.9880574 -3.9764597 -3.9644594 -3.9658456 -3.9933145 -4.0209346 -4.0369415 -4.0573778 -4.1058416 -4.1728773 -4.2378483 -4.2832031][-4.0722985 -3.9917133 -3.9208653 -3.8753967 -3.8508863 -3.82471 -3.8163831 -3.850127 -3.8910887 -3.9197559 -3.9547038 -4.0199137 -4.1053729 -4.1886835 -4.2502031][-4.0179 -3.9256277 -3.8446193 -3.7940948 -3.7646341 -3.7314939 -3.7178705 -3.7568941 -3.8087988 -3.8474183 -3.8929667 -3.9658818 -4.0573125 -4.1485868 -4.2202053][-3.980973 -3.8838141 -3.8014791 -3.7616892 -3.7556126 -3.7469292 -3.7421327 -3.7760949 -3.8234007 -3.8570018 -3.8952837 -3.9567525 -4.0402603 -4.1293373 -4.20401][-3.9694974 -3.8786175 -3.8094339 -3.7936323 -3.8260236 -3.8574798 -3.8677483 -3.8855872 -3.9112697 -3.9313595 -3.953707 -3.9942765 -4.0606313 -4.1406083 -4.2112455][-3.9905536 -3.914021 -3.8627763 -3.8674121 -3.9235713 -3.9796078 -4.0015583 -4.0087633 -4.0181913 -4.0279741 -4.04003 -4.0664754 -4.1162238 -4.1802216 -4.2389517][-4.0337319 -3.9703913 -3.9331884 -3.9517128 -4.0126367 -4.0721097 -4.0981827 -4.1055818 -4.1121168 -4.1195526 -4.1295571 -4.1505718 -4.1863847 -4.23272 -4.2749429][-4.0920811 -4.0427351 -4.0178561 -4.0394969 -4.0931106 -4.1429415 -4.1666136 -4.1749487 -4.1833024 -4.192534 -4.2026567 -4.2200623 -4.2453556 -4.2769856 -4.3056498][-4.1585674 -4.12559 -4.1116371 -4.1287723 -4.1678238 -4.2031546 -4.2200117 -4.2252512 -4.2327151 -4.2407002 -4.2483053 -4.2619696 -4.2813482 -4.3042936 -4.3255877][-4.2201576 -4.2009249 -4.1927218 -4.2023439 -4.2261071 -4.2491617 -4.2620206 -4.266531 -4.2720766 -4.2772803 -4.2820945 -4.2918463 -4.3069525 -4.3242989 -4.340363]]...]
INFO - root - 2017-12-05 08:46:10.592285: step 2510, loss = 1.83, batch loss = 1.77 (37.3 examples/sec; 0.215 sec/batch; 19h:40m:43s remains)
INFO - root - 2017-12-05 08:46:12.711022: step 2520, loss = 1.90, batch loss = 1.84 (39.3 examples/sec; 0.204 sec/batch; 18h:40m:30s remains)
INFO - root - 2017-12-05 08:46:14.781315: step 2530, loss = 1.91, batch loss = 1.85 (39.4 examples/sec; 0.203 sec/batch; 18h:36m:25s remains)
INFO - root - 2017-12-05 08:46:16.891133: step 2540, loss = 1.81, batch loss = 1.75 (36.2 examples/sec; 0.221 sec/batch; 20h:14m:00s remains)
INFO - root - 2017-12-05 08:46:18.981699: step 2550, loss = 1.83, batch loss = 1.77 (39.2 examples/sec; 0.204 sec/batch; 18h:41m:56s remains)
INFO - root - 2017-12-05 08:46:21.062512: step 2560, loss = 1.88, batch loss = 1.82 (38.3 examples/sec; 0.209 sec/batch; 19h:08m:13s remains)
INFO - root - 2017-12-05 08:46:23.138435: step 2570, loss = 1.85, batch loss = 1.79 (37.3 examples/sec; 0.214 sec/batch; 19h:38m:02s remains)
INFO - root - 2017-12-05 08:46:25.227203: step 2580, loss = 1.87, batch loss = 1.81 (38.9 examples/sec; 0.206 sec/batch; 18h:50m:33s remains)
INFO - root - 2017-12-05 08:46:27.309025: step 2590, loss = 1.88, batch loss = 1.82 (38.3 examples/sec; 0.209 sec/batch; 19h:07m:37s remains)
INFO - root - 2017-12-05 08:46:29.369659: step 2600, loss = 1.87, batch loss = 1.81 (39.3 examples/sec; 0.204 sec/batch; 18h:40m:31s remains)
2017-12-05 08:46:29.659427: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1013141 -4.0765243 -4.0635295 -4.0529985 -4.0494766 -4.0430336 -4.0319138 -4.0195112 -4.0042276 -3.9834788 -3.967463 -3.9538546 -3.9384534 -3.9423695 -3.9523926][-4.0845628 -4.0578575 -4.0455618 -4.0408688 -4.041677 -4.0365672 -4.0295911 -4.0189414 -4.00422 -3.9770968 -3.9521275 -3.9337075 -3.9201365 -3.9273932 -3.9358883][-4.0743313 -4.050139 -4.0401893 -4.0391049 -4.0391574 -4.0304365 -4.0204568 -4.0060067 -3.9898977 -3.9627004 -3.9378507 -3.9230757 -3.9137692 -3.917088 -3.9175336][-4.0657926 -4.0444317 -4.0339346 -4.0295563 -4.0224009 -4.0065823 -3.9904733 -3.9709942 -3.9553745 -3.9388852 -3.9254127 -3.9181442 -3.9070382 -3.899493 -3.8900919][-4.0509486 -4.0322924 -4.0207 -4.0112095 -3.9956169 -3.9712267 -3.9467335 -3.9219279 -3.9054103 -3.9028 -3.9085886 -3.9146035 -3.905498 -3.8930497 -3.8782854][-4.0317912 -4.011076 -3.9982233 -3.9861956 -3.9667788 -3.9359543 -3.9019036 -3.8686016 -3.8514264 -3.8610501 -3.8843482 -3.9040685 -3.9018669 -3.8911848 -3.8796875][-4.0051661 -3.9853535 -3.9741213 -3.9625559 -3.9435859 -3.9096661 -3.870173 -3.8309052 -3.813798 -3.8318875 -3.8670456 -3.89717 -3.9045775 -3.8987958 -3.8927932][-3.9841084 -3.973577 -3.969295 -3.9613371 -3.9440937 -3.911541 -3.8732023 -3.8378806 -3.8237665 -3.8458281 -3.8814483 -3.9124107 -3.9252288 -3.9203281 -3.9129333][-3.9750762 -3.977169 -3.9819624 -3.9805226 -3.9701908 -3.9465215 -3.9159031 -3.8899109 -3.8810217 -3.9015527 -3.928632 -3.9534354 -3.9672441 -3.9632025 -3.952436][-3.9843905 -3.9970021 -4.0108051 -4.0152221 -4.0129175 -3.9995973 -3.9794607 -3.9625361 -3.957988 -3.9792261 -4.0017557 -4.0185008 -4.0264044 -4.0186505 -4.0060587][-4.0162492 -4.0325069 -4.0526209 -4.0627112 -4.0653238 -4.0591569 -4.0474467 -4.0368948 -4.0391293 -4.0638723 -4.0843229 -4.092361 -4.0898123 -4.074543 -4.0602407][-4.0646234 -4.0805311 -4.1038013 -4.1191897 -4.1281509 -4.128726 -4.1191649 -4.1079736 -4.1118684 -4.134748 -4.1504917 -4.1542287 -4.14559 -4.1277804 -4.1148558][-4.1178694 -4.129756 -4.1535444 -4.1733027 -4.1874733 -4.1920261 -4.1837039 -4.1723504 -4.1745739 -4.1911893 -4.2027435 -4.204421 -4.1943855 -4.1785893 -4.1675024][-4.1772189 -4.1806931 -4.1984439 -4.2167168 -4.2329316 -4.2406921 -4.2362351 -4.2273331 -4.2279758 -4.2383103 -4.2448039 -4.2444668 -4.23739 -4.2275858 -4.2210345][-4.2360196 -4.234767 -4.2452359 -4.2571263 -4.2702956 -4.2777 -4.2762251 -4.2710996 -4.2706404 -4.2767224 -4.2808347 -4.2808437 -4.2769966 -4.2720184 -4.2686467]]...]
INFO - root - 2017-12-05 08:46:31.719830: step 2610, loss = 1.87, batch loss = 1.81 (39.8 examples/sec; 0.201 sec/batch; 18h:24m:46s remains)
INFO - root - 2017-12-05 08:46:33.794288: step 2620, loss = 1.82, batch loss = 1.77 (39.6 examples/sec; 0.202 sec/batch; 18h:29m:48s remains)
INFO - root - 2017-12-05 08:46:35.858232: step 2630, loss = 1.86, batch loss = 1.80 (38.9 examples/sec; 0.206 sec/batch; 18h:51m:34s remains)
INFO - root - 2017-12-05 08:46:37.943064: step 2640, loss = 1.88, batch loss = 1.82 (37.8 examples/sec; 0.211 sec/batch; 19h:22m:15s remains)
INFO - root - 2017-12-05 08:46:40.037712: step 2650, loss = 1.87, batch loss = 1.81 (38.0 examples/sec; 0.210 sec/batch; 19h:16m:45s remains)
INFO - root - 2017-12-05 08:46:42.114289: step 2660, loss = 1.88, batch loss = 1.83 (38.7 examples/sec; 0.207 sec/batch; 18h:57m:06s remains)
INFO - root - 2017-12-05 08:46:44.238882: step 2670, loss = 1.91, batch loss = 1.85 (37.1 examples/sec; 0.216 sec/batch; 19h:46m:00s remains)
INFO - root - 2017-12-05 08:46:46.317252: step 2680, loss = 1.87, batch loss = 1.81 (38.2 examples/sec; 0.209 sec/batch; 19h:09m:48s remains)
INFO - root - 2017-12-05 08:46:48.394610: step 2690, loss = 1.87, batch loss = 1.81 (38.6 examples/sec; 0.207 sec/batch; 18h:59m:12s remains)
INFO - root - 2017-12-05 08:46:50.447464: step 2700, loss = 1.83, batch loss = 1.77 (39.4 examples/sec; 0.203 sec/batch; 18h:35m:41s remains)
2017-12-05 08:46:50.721072: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.0214639 -4.0532041 -4.1032867 -4.1484513 -4.1751976 -4.1873784 -4.1924329 -4.1957636 -4.1968641 -4.1963873 -4.1959939 -4.1980233 -4.2018805 -4.2052517 -4.206913][-4.0248213 -4.0523891 -4.0927234 -4.1290603 -4.1505189 -4.1585226 -4.1620336 -4.1644087 -4.1662469 -4.1685252 -4.1738992 -4.183743 -4.1945229 -4.2025766 -4.2073574][-4.0455904 -4.0658269 -4.094183 -4.1164107 -4.1262364 -4.1248174 -4.1236019 -4.1268287 -4.1332922 -4.1422796 -4.1574097 -4.1778846 -4.1977148 -4.2104826 -4.2163944][-4.0337954 -4.04736 -4.064496 -4.07388 -4.071106 -4.0588613 -4.0505943 -4.0539641 -4.0664444 -4.0835929 -4.1081786 -4.1387882 -4.1677651 -4.1833529 -4.1871638][-3.9747608 -3.9786413 -3.9855046 -3.9831054 -3.9704664 -3.9499798 -3.9386423 -3.94687 -3.9684486 -3.9944446 -4.027132 -4.0664539 -4.1021423 -4.1181674 -4.116817][-3.8944495 -3.8887146 -3.8864732 -3.8764307 -3.856822 -3.829875 -3.8143935 -3.825212 -3.8581762 -3.898598 -3.9405792 -3.9843774 -4.0192456 -4.0328736 -4.0271239][-3.8083405 -3.7899928 -3.7784097 -3.7624717 -3.737401 -3.7044675 -3.6818023 -3.6898003 -3.7310755 -3.7855721 -3.834162 -3.8771822 -3.908365 -3.918819 -3.9135385][-3.7589674 -3.7281666 -3.7100694 -3.6922805 -3.6642675 -3.6247008 -3.5934799 -3.5937681 -3.6364224 -3.6983032 -3.7508733 -3.7905941 -3.8149424 -3.8201535 -3.8108459][-3.7865415 -3.7596836 -3.747786 -3.7399209 -3.721411 -3.6927409 -3.6685367 -3.6668191 -3.6990266 -3.7472486 -3.7871306 -3.8100882 -3.8161933 -3.8068933 -3.783601][-3.8696582 -3.8504555 -3.8428898 -3.8393373 -3.8299398 -3.8140481 -3.8007677 -3.8002498 -3.8207119 -3.8497293 -3.871577 -3.8775728 -3.8725712 -3.8584709 -3.8321376][-3.9505854 -3.933008 -3.922586 -3.9129217 -3.9029076 -3.892647 -3.8854985 -3.8859015 -3.8971596 -3.9119949 -3.9189937 -3.9135756 -3.9049563 -3.8926485 -3.8689179][-3.9777112 -3.9569376 -3.9371979 -3.9170969 -3.9041231 -3.8972075 -3.892997 -3.8915825 -3.8977849 -3.9051671 -3.9045599 -3.8978224 -3.8944154 -3.8923378 -3.882107][-3.9755979 -3.9481525 -3.9198179 -3.89147 -3.8748355 -3.8709197 -3.8693564 -3.8651607 -3.8692009 -3.8769367 -3.8800006 -3.8830314 -3.8922131 -3.9033608 -3.9073963][-3.9792194 -3.9463725 -3.9148815 -3.88822 -3.8753009 -3.875746 -3.8772879 -3.8733742 -3.8782907 -3.8877714 -3.8953378 -3.9062486 -3.9231076 -3.9409256 -3.9505713][-3.9846516 -3.9531851 -3.9274883 -3.910033 -3.9066396 -3.9140587 -3.919317 -3.9167261 -3.9198174 -3.9290981 -3.9395218 -3.953948 -3.9722466 -3.9894218 -3.9966459]]...]
INFO - root - 2017-12-05 08:46:52.791313: step 2710, loss = 1.93, batch loss = 1.87 (39.2 examples/sec; 0.204 sec/batch; 18h:40m:47s remains)
INFO - root - 2017-12-05 08:46:54.910935: step 2720, loss = 1.82, batch loss = 1.76 (37.6 examples/sec; 0.213 sec/batch; 19h:28m:54s remains)
INFO - root - 2017-12-05 08:46:56.995561: step 2730, loss = 1.82, batch loss = 1.76 (38.9 examples/sec; 0.206 sec/batch; 18h:50m:31s remains)
INFO - root - 2017-12-05 08:46:59.058709: step 2740, loss = 1.87, batch loss = 1.81 (38.5 examples/sec; 0.208 sec/batch; 19h:03m:17s remains)
INFO - root - 2017-12-05 08:47:01.127388: step 2750, loss = 1.87, batch loss = 1.82 (38.8 examples/sec; 0.206 sec/batch; 18h:53m:06s remains)
INFO - root - 2017-12-05 08:47:03.243812: step 2760, loss = 1.82, batch loss = 1.76 (37.8 examples/sec; 0.212 sec/batch; 19h:23m:37s remains)
INFO - root - 2017-12-05 08:47:05.320051: step 2770, loss = 1.81, batch loss = 1.75 (37.6 examples/sec; 0.212 sec/batch; 19h:27m:42s remains)
INFO - root - 2017-12-05 08:47:07.443247: step 2780, loss = 1.80, batch loss = 1.75 (38.8 examples/sec; 0.206 sec/batch; 18h:52m:55s remains)
INFO - root - 2017-12-05 08:47:09.579375: step 2790, loss = 1.83, batch loss = 1.77 (38.0 examples/sec; 0.211 sec/batch; 19h:18m:06s remains)
INFO - root - 2017-12-05 08:47:11.649224: step 2800, loss = 1.84, batch loss = 1.78 (37.5 examples/sec; 0.213 sec/batch; 19h:31m:02s remains)
2017-12-05 08:47:11.909966: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.7526178 -3.7091656 -3.6776991 -3.6564319 -3.651792 -3.6472998 -3.6169257 -3.5667655 -3.5361359 -3.5747972 -3.6865077 -3.8298566 -3.9696319 -4.0904336 -4.1839628][-3.7098053 -3.6431491 -3.5917385 -3.5578284 -3.5504265 -3.5515327 -3.5377073 -3.498313 -3.4722054 -3.5163648 -3.6367865 -3.7876065 -3.9312873 -4.0559411 -4.1565194][-3.71516 -3.6371298 -3.573818 -3.5231657 -3.4994411 -3.4952793 -3.4952388 -3.4753304 -3.4614506 -3.5144114 -3.6412308 -3.79262 -3.9326715 -4.0536981 -4.1525855][-3.7286603 -3.6522603 -3.5880752 -3.525708 -3.4825661 -3.4581096 -3.4562237 -3.4497273 -3.451817 -3.5201797 -3.6606083 -3.8120513 -3.9441366 -4.0599961 -4.1568623][-3.7325444 -3.6643329 -3.6033778 -3.5277963 -3.4593754 -3.4089994 -3.3954158 -3.3920565 -3.4066429 -3.4926093 -3.6477876 -3.8073945 -3.939929 -4.0552273 -4.1527257][-3.712132 -3.6562712 -3.6035662 -3.5179024 -3.4197698 -3.3426747 -3.3257556 -3.3255196 -3.3454552 -3.443758 -3.6130195 -3.7838545 -3.9232538 -4.0418262 -4.1418252][-3.6799333 -3.630774 -3.5808024 -3.4933426 -3.3743834 -3.2800825 -3.2631168 -3.2644217 -3.285538 -3.3890791 -3.5698366 -3.7552822 -3.9086175 -4.03353 -4.1366324][-3.6450105 -3.5949626 -3.5415921 -3.4607909 -3.3534181 -3.2726645 -3.2672482 -3.2668827 -3.2819839 -3.3767185 -3.552402 -3.7396348 -3.8997412 -4.032115 -4.13729][-3.5987663 -3.5373116 -3.4823642 -3.4249747 -3.3730965 -3.3443055 -3.3528528 -3.3460078 -3.3469996 -3.4188342 -3.5695586 -3.7421656 -3.8942661 -4.0252733 -4.132339][-3.5473793 -3.4737504 -3.4168878 -3.3819747 -3.3789494 -3.401037 -3.4282053 -3.4320903 -3.4330473 -3.4890141 -3.6136141 -3.7640183 -3.9016516 -4.023807 -4.1298456][-3.5090854 -3.4258864 -3.3633382 -3.3347359 -3.3517032 -3.3961434 -3.4383769 -3.4648876 -3.4880967 -3.5503829 -3.6612182 -3.7924137 -3.9164648 -4.0322337 -4.1367836][-3.4925075 -3.4110539 -3.3469684 -3.3207483 -3.3430355 -3.3872216 -3.4298465 -3.4687772 -3.5171456 -3.5952208 -3.7018559 -3.8187394 -3.9317324 -4.043014 -4.146975][-3.5198352 -3.4470572 -3.3871059 -3.3659773 -3.3911386 -3.4293551 -3.4614658 -3.4994607 -3.5578454 -3.6420846 -3.7425647 -3.8458493 -3.9461362 -4.0510492 -4.15515][-3.5771964 -3.5171168 -3.4664845 -3.4482152 -3.4665868 -3.5003924 -3.5266321 -3.5593269 -3.618181 -3.7007608 -3.791961 -3.8813734 -3.967664 -4.0637617 -4.1639848][-3.6273289 -3.5833828 -3.5497165 -3.5367298 -3.5539358 -3.593122 -3.6249576 -3.6500592 -3.6969516 -3.767966 -3.8478091 -3.924639 -4.0023293 -4.08738 -4.1789141]]...]
INFO - root - 2017-12-05 08:47:13.997838: step 2810, loss = 1.84, batch loss = 1.78 (36.9 examples/sec; 0.217 sec/batch; 19h:51m:25s remains)
INFO - root - 2017-12-05 08:47:16.105021: step 2820, loss = 1.83, batch loss = 1.77 (39.4 examples/sec; 0.203 sec/batch; 18h:34m:41s remains)
INFO - root - 2017-12-05 08:47:18.186993: step 2830, loss = 1.85, batch loss = 1.79 (38.8 examples/sec; 0.206 sec/batch; 18h:54m:14s remains)
INFO - root - 2017-12-05 08:47:20.294159: step 2840, loss = 1.81, batch loss = 1.76 (33.5 examples/sec; 0.238 sec/batch; 21h:50m:20s remains)
INFO - root - 2017-12-05 08:47:22.379198: step 2850, loss = 1.79, batch loss = 1.73 (39.4 examples/sec; 0.203 sec/batch; 18h:36m:40s remains)
INFO - root - 2017-12-05 08:47:24.456485: step 2860, loss = 1.83, batch loss = 1.77 (38.1 examples/sec; 0.210 sec/batch; 19h:13m:22s remains)
INFO - root - 2017-12-05 08:47:26.519388: step 2870, loss = 1.86, batch loss = 1.80 (38.5 examples/sec; 0.208 sec/batch; 19h:02m:52s remains)
INFO - root - 2017-12-05 08:47:28.590337: step 2880, loss = 1.81, batch loss = 1.75 (39.0 examples/sec; 0.205 sec/batch; 18h:45m:51s remains)
INFO - root - 2017-12-05 08:47:30.653232: step 2890, loss = 1.83, batch loss = 1.77 (39.2 examples/sec; 0.204 sec/batch; 18h:42m:25s remains)
INFO - root - 2017-12-05 08:47:32.769989: step 2900, loss = 1.83, batch loss = 1.77 (39.2 examples/sec; 0.204 sec/batch; 18h:41m:25s remains)
2017-12-05 08:47:33.060693: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2857251 -4.2940779 -4.2964511 -4.2891335 -4.2794175 -4.2667274 -4.2441559 -4.2204995 -4.2052975 -4.2019377 -4.2152686 -4.2449746 -4.2728796 -4.2850914 -4.2788978][-4.2863483 -4.2886882 -4.279984 -4.265317 -4.257544 -4.2550688 -4.2440081 -4.226891 -4.2136478 -4.2106295 -4.2260084 -4.2566185 -4.2846351 -4.2968745 -4.2919812][-4.2903175 -4.2869034 -4.2701507 -4.2515879 -4.2496042 -4.2604809 -4.2629752 -4.2535377 -4.2388506 -4.2300534 -4.2387428 -4.2629957 -4.28521 -4.29354 -4.2883487][-4.2965198 -4.2901492 -4.271657 -4.2549291 -4.2592716 -4.2781477 -4.2884707 -4.2830892 -4.2673092 -4.2521324 -4.2508292 -4.2643876 -4.2766547 -4.2787814 -4.2713594][-4.3034296 -4.2986045 -4.2846851 -4.2730036 -4.2786651 -4.29659 -4.3068485 -4.3028789 -4.2893624 -4.2716312 -4.260715 -4.2620459 -4.2630463 -4.2573752 -4.2465162][-4.3103356 -4.3109007 -4.3036113 -4.296658 -4.3003421 -4.3115396 -4.3177834 -4.3153191 -4.3047786 -4.2876544 -4.2718391 -4.2647824 -4.2572103 -4.2449565 -4.2299705][-4.3132868 -4.3198709 -4.3191433 -4.315361 -4.3158655 -4.3188267 -4.3203259 -4.31903 -4.3119593 -4.298841 -4.2860165 -4.2786684 -4.2693048 -4.2556767 -4.238956][-4.3075366 -4.3209052 -4.3267603 -4.3247948 -4.3196473 -4.313344 -4.3091488 -4.3077664 -4.3040848 -4.2970538 -4.2915998 -4.2901211 -4.2860341 -4.27744 -4.265275][-4.2915797 -4.3138871 -4.3274956 -4.3268023 -4.3146563 -4.2993135 -4.2901869 -4.288218 -4.2850394 -4.2797856 -4.27835 -4.2809334 -4.2816887 -4.2802482 -4.2782493][-4.2704635 -4.3024645 -4.3246746 -4.3254633 -4.3071575 -4.2831874 -4.2693968 -4.2665257 -4.2619576 -4.2541595 -4.2500725 -4.2512269 -4.2537861 -4.2585654 -4.2664661][-4.2571025 -4.296011 -4.3234429 -4.325069 -4.3032002 -4.2728858 -4.2551651 -4.2498293 -4.2418919 -4.228476 -4.2170281 -4.2114296 -4.2113504 -4.2176895 -4.2324429][-4.2641521 -4.3017364 -4.3250575 -4.3234954 -4.2997718 -4.2679377 -4.249465 -4.2423825 -4.2315893 -4.2133493 -4.1941767 -4.1789913 -4.1699686 -4.1719546 -4.1892772][-4.2847114 -4.3098736 -4.3215222 -4.313343 -4.2903008 -4.2633371 -4.2487192 -4.2423148 -4.2326016 -4.2154307 -4.1944242 -4.1710768 -4.1514258 -4.1475711 -4.1621156][-4.2985005 -4.3068 -4.3069434 -4.295289 -4.2779016 -4.2596216 -4.2501321 -4.2450743 -4.237977 -4.2262473 -4.208425 -4.1812959 -4.1555667 -4.1473784 -4.1559987][-4.2900105 -4.2860036 -4.281292 -4.2722807 -4.2642064 -4.2556257 -4.2514582 -4.249382 -4.2467628 -4.2403007 -4.2265582 -4.1996055 -4.1724329 -4.1618981 -4.1645994]]...]
INFO - root - 2017-12-05 08:47:35.157535: step 2910, loss = 1.88, batch loss = 1.82 (38.2 examples/sec; 0.210 sec/batch; 19h:11m:25s remains)
INFO - root - 2017-12-05 08:47:37.234152: step 2920, loss = 1.82, batch loss = 1.76 (37.6 examples/sec; 0.213 sec/batch; 19h:27m:37s remains)
INFO - root - 2017-12-05 08:47:39.369814: step 2930, loss = 1.92, batch loss = 1.86 (36.9 examples/sec; 0.217 sec/batch; 19h:52m:02s remains)
INFO - root - 2017-12-05 08:47:41.444562: step 2940, loss = 1.82, batch loss = 1.76 (38.8 examples/sec; 0.206 sec/batch; 18h:51m:28s remains)
INFO - root - 2017-12-05 08:47:43.547732: step 2950, loss = 1.87, batch loss = 1.82 (38.5 examples/sec; 0.208 sec/batch; 19h:00m:25s remains)
INFO - root - 2017-12-05 08:47:45.598522: step 2960, loss = 1.90, batch loss = 1.84 (39.2 examples/sec; 0.204 sec/batch; 18h:41m:33s remains)
INFO - root - 2017-12-05 08:47:47.696063: step 2970, loss = 1.83, batch loss = 1.77 (39.1 examples/sec; 0.205 sec/batch; 18h:44m:16s remains)
INFO - root - 2017-12-05 08:47:49.761501: step 2980, loss = 1.83, batch loss = 1.77 (39.1 examples/sec; 0.205 sec/batch; 18h:43m:36s remains)
INFO - root - 2017-12-05 08:47:51.834150: step 2990, loss = 1.77, batch loss = 1.72 (38.9 examples/sec; 0.206 sec/batch; 18h:50m:14s remains)
INFO - root - 2017-12-05 08:47:53.895575: step 3000, loss = 1.84, batch loss = 1.78 (37.2 examples/sec; 0.215 sec/batch; 19h:39m:56s remains)
2017-12-05 08:47:54.191274: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.0872331 -4.1297641 -4.1731658 -4.1783304 -4.1397357 -4.05567 -3.9516871 -3.8777428 -3.8503532 -3.8593798 -3.8751245 -3.9112427 -3.9515905 -3.9733434 -3.9804955][-4.039649 -4.0813189 -4.1256557 -4.1378407 -4.1013412 -4.0145245 -3.9044878 -3.8259618 -3.8000603 -3.8148785 -3.8324628 -3.8712234 -3.9159083 -3.9416072 -3.9516957][-3.9836683 -4.0182257 -4.0634737 -4.0865335 -4.0614762 -3.9873154 -3.893641 -3.8267522 -3.8075109 -3.8237591 -3.8387244 -3.8725667 -3.9109259 -3.9324951 -3.9408412][-3.929529 -3.952395 -3.9921827 -4.0220757 -4.0115819 -3.9610097 -3.8991618 -3.8590403 -3.8577533 -3.8785696 -3.8945856 -3.9198952 -3.9437141 -3.9539323 -3.9570203][-3.8819213 -3.8809228 -3.8961043 -3.9145689 -3.9099896 -3.8777792 -3.8401656 -3.8219461 -3.8441286 -3.8863039 -3.9159796 -3.9415889 -3.9598768 -3.9706163 -3.9767096][-3.8355048 -3.8020983 -3.7825313 -3.7818575 -3.7794197 -3.7521396 -3.7143712 -3.6975012 -3.7397392 -3.8071179 -3.8571155 -3.8914068 -3.9159298 -3.93864 -3.956804][-3.8016644 -3.7275693 -3.658725 -3.6282597 -3.6152945 -3.5763214 -3.5083132 -3.4702764 -3.5371966 -3.6456957 -3.7251127 -3.7702417 -3.8029428 -3.8401136 -3.8751068][-3.8248169 -3.7111113 -3.5928409 -3.5227094 -3.4873357 -3.4223223 -3.309588 -3.2435479 -3.3434191 -3.4937577 -3.59489 -3.6405535 -3.668741 -3.7094076 -3.7531505][-3.9174869 -3.8026371 -3.6782074 -3.59364 -3.54799 -3.4799261 -3.3737292 -3.3183413 -3.405488 -3.534914 -3.6110113 -3.6241083 -3.618207 -3.6315172 -3.6607866][-4.0396686 -3.9464862 -3.8451023 -3.7688117 -3.7237573 -3.6718006 -3.5996704 -3.5644703 -3.6124823 -3.6847284 -3.7159474 -3.6926384 -3.6515083 -3.6352463 -3.6440251][-4.1494913 -4.0809216 -4.0072308 -3.9466114 -3.9063475 -3.8671145 -3.8181145 -3.7850513 -3.7930877 -3.8097193 -3.7944503 -3.7402334 -3.6802654 -3.6472659 -3.6356936][-4.2298751 -4.1836767 -4.1362009 -4.0953078 -4.0653276 -4.0371256 -3.997309 -3.9564569 -3.9282446 -3.8966508 -3.8343668 -3.7461762 -3.6644943 -3.6103191 -3.5711308][-4.2856112 -4.2561369 -4.2266583 -4.2001944 -4.17965 -4.156302 -4.1179395 -4.0705323 -4.0216441 -3.9608846 -3.865222 -3.7462921 -3.6414094 -3.5702097 -3.5062571][-4.3224874 -4.3045464 -4.28708 -4.2716422 -4.2592564 -4.2416463 -4.2073984 -4.160255 -4.1051707 -4.0331955 -3.9250789 -3.7896609 -3.6687949 -3.5850198 -3.5086341][-4.3433962 -4.3319583 -4.3217463 -4.3135486 -4.3069448 -4.2959785 -4.2708535 -4.2317967 -4.1808071 -4.11164 -4.0147729 -3.8948555 -3.7802429 -3.692853 -3.616533]]...]
INFO - root - 2017-12-05 08:47:56.286743: step 3010, loss = 1.78, batch loss = 1.72 (38.4 examples/sec; 0.208 sec/batch; 19h:04m:26s remains)
INFO - root - 2017-12-05 08:47:58.433662: step 3020, loss = 1.82, batch loss = 1.76 (38.9 examples/sec; 0.206 sec/batch; 18h:48m:38s remains)
INFO - root - 2017-12-05 08:48:00.518729: step 3030, loss = 1.80, batch loss = 1.74 (37.6 examples/sec; 0.213 sec/batch; 19h:27m:53s remains)
INFO - root - 2017-12-05 08:48:02.634446: step 3040, loss = 1.73, batch loss = 1.67 (37.7 examples/sec; 0.212 sec/batch; 19h:24m:26s remains)
INFO - root - 2017-12-05 08:48:04.728777: step 3050, loss = 1.77, batch loss = 1.71 (39.2 examples/sec; 0.204 sec/batch; 18h:40m:08s remains)
INFO - root - 2017-12-05 08:48:06.789876: step 3060, loss = 1.82, batch loss = 1.77 (38.0 examples/sec; 0.211 sec/batch; 19h:16m:01s remains)
INFO - root - 2017-12-05 08:48:08.894555: step 3070, loss = 1.77, batch loss = 1.71 (38.0 examples/sec; 0.210 sec/batch; 19h:15m:11s remains)
INFO - root - 2017-12-05 08:48:10.979370: step 3080, loss = 1.84, batch loss = 1.78 (39.3 examples/sec; 0.203 sec/batch; 18h:36m:38s remains)
INFO - root - 2017-12-05 08:48:13.080649: step 3090, loss = 1.86, batch loss = 1.80 (38.9 examples/sec; 0.206 sec/batch; 18h:48m:51s remains)
INFO - root - 2017-12-05 08:48:15.136466: step 3100, loss = 1.80, batch loss = 1.74 (38.2 examples/sec; 0.210 sec/batch; 19h:10m:25s remains)
2017-12-05 08:48:15.397081: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2235656 -4.2179909 -4.2170954 -4.2104049 -4.2025208 -4.1962261 -4.1885052 -4.1840172 -4.1811442 -4.1742077 -4.1658158 -4.1572714 -4.1507406 -4.1521821 -4.1570435][-4.2042532 -4.1986651 -4.19399 -4.17993 -4.16399 -4.1496029 -4.1341395 -4.1232047 -4.1137295 -4.1009092 -4.0874414 -4.073494 -4.0606241 -4.0565519 -4.0561838][-4.1700559 -4.1642332 -4.1523895 -4.1263371 -4.096971 -4.0692644 -4.0432367 -4.0235658 -4.0070915 -3.9922347 -3.9786952 -3.9614847 -3.9430459 -3.9336355 -3.9267588][-4.1251454 -4.1178632 -4.0962582 -4.0553274 -4.0092196 -3.9672887 -3.9289949 -3.8985891 -3.8770373 -3.8676085 -3.86147 -3.8398337 -3.8123796 -3.796994 -3.7835796][-4.0699334 -4.0574207 -4.02247 -3.9629576 -3.89884 -3.8415282 -3.7873678 -3.7425621 -3.7192998 -3.7267697 -3.733449 -3.7108109 -3.6764755 -3.657943 -3.6435442][-4.0255246 -4.0046105 -3.9556096 -3.8772631 -3.7948232 -3.7192066 -3.6429219 -3.5741675 -3.5435739 -3.5691559 -3.5938339 -3.5801859 -3.552485 -3.5450745 -3.5469897][-4.0104871 -3.9756207 -3.9071038 -3.8054519 -3.7014167 -3.604152 -3.4995127 -3.394474 -3.3523173 -3.4074445 -3.4676447 -3.4817371 -3.484194 -3.5098953 -3.5470283][-4.03026 -3.9805169 -3.8915803 -3.7670388 -3.6428313 -3.5301583 -3.4018097 -3.2588229 -3.2082667 -3.3058405 -3.4123354 -3.4702816 -3.5216403 -3.5937328 -3.6672773][-4.0767365 -4.0195189 -3.9236796 -3.792912 -3.6631663 -3.5535564 -3.4312124 -3.28763 -3.2388859 -3.3474846 -3.4732997 -3.5606072 -3.6465161 -3.7484291 -3.8385789][-4.1445146 -4.090291 -4.0031013 -3.8864803 -3.7692027 -3.672452 -3.5742009 -3.465107 -3.4277315 -3.5089068 -3.6189022 -3.7105415 -3.8041577 -3.9061117 -3.9927962][-4.2104764 -4.1671715 -4.0992904 -4.0086985 -3.9159451 -3.8392034 -3.7671888 -3.6970465 -3.6715167 -3.7213771 -3.8012445 -3.8754272 -3.9527371 -4.0341792 -4.1023979][-4.2602348 -4.2309203 -4.1841764 -4.1213984 -4.0567288 -4.0025773 -3.9545708 -3.9139082 -3.8985059 -3.9263048 -3.9761724 -4.0239496 -4.0737171 -4.1274023 -4.1727238][-4.2889867 -4.2731342 -4.24517 -4.2064672 -4.1667805 -4.1330581 -4.1043568 -4.0828891 -4.0753293 -4.0894661 -4.1149378 -4.1387911 -4.1630812 -4.1907077 -4.2160106][-4.297492 -4.290936 -4.2765217 -4.2562432 -4.2346473 -4.2160935 -4.2017288 -4.1918883 -4.1896696 -4.1971984 -4.2082062 -4.2167878 -4.2253532 -4.2364283 -4.2472448][-4.2916074 -4.2903519 -4.2860904 -4.2786841 -4.2696009 -4.2615628 -4.256556 -4.2539253 -4.2548413 -4.2590089 -4.2631845 -4.2649913 -4.2657671 -4.2682295 -4.2694721]]...]
INFO - root - 2017-12-05 08:48:17.463792: step 3110, loss = 1.76, batch loss = 1.70 (39.1 examples/sec; 0.205 sec/batch; 18h:44m:29s remains)
INFO - root - 2017-12-05 08:48:19.569540: step 3120, loss = 1.75, batch loss = 1.69 (38.5 examples/sec; 0.208 sec/batch; 18h:59m:57s remains)
INFO - root - 2017-12-05 08:48:21.657245: step 3130, loss = 1.81, batch loss = 1.75 (37.2 examples/sec; 0.215 sec/batch; 19h:39m:01s remains)
INFO - root - 2017-12-05 08:48:23.749782: step 3140, loss = 1.84, batch loss = 1.79 (37.3 examples/sec; 0.215 sec/batch; 19h:37m:58s remains)
INFO - root - 2017-12-05 08:48:25.812074: step 3150, loss = 1.75, batch loss = 1.69 (38.6 examples/sec; 0.207 sec/batch; 18h:57m:37s remains)
INFO - root - 2017-12-05 08:48:27.888280: step 3160, loss = 1.75, batch loss = 1.69 (39.3 examples/sec; 0.204 sec/batch; 18h:38m:40s remains)
INFO - root - 2017-12-05 08:48:29.948778: step 3170, loss = 1.77, batch loss = 1.71 (38.5 examples/sec; 0.208 sec/batch; 18h:59m:44s remains)
INFO - root - 2017-12-05 08:48:32.040650: step 3180, loss = 1.84, batch loss = 1.78 (39.5 examples/sec; 0.203 sec/batch; 18h:32m:52s remains)
INFO - root - 2017-12-05 08:48:34.116958: step 3190, loss = 1.83, batch loss = 1.77 (39.6 examples/sec; 0.202 sec/batch; 18h:29m:23s remains)
INFO - root - 2017-12-05 08:48:36.210633: step 3200, loss = 1.78, batch loss = 1.72 (37.5 examples/sec; 0.214 sec/batch; 19h:31m:49s remains)
2017-12-05 08:48:36.507339: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3037224 -4.2872744 -4.2755666 -4.2692604 -4.2686186 -4.265666 -4.2585211 -4.2499685 -4.2407637 -4.2353964 -4.2317247 -4.2372613 -4.2557592 -4.2817335 -4.3086314][-4.2343254 -4.2014537 -4.1782589 -4.1675749 -4.1694889 -4.1667542 -4.1583662 -4.1482282 -4.1371593 -4.1329556 -4.1325922 -4.1471033 -4.1807404 -4.2229042 -4.2634859][-4.1415763 -4.0879407 -4.0491595 -4.0302744 -4.0336814 -4.0277019 -4.0148897 -4.0042048 -3.9970057 -4.000392 -4.01083 -4.0443707 -4.0988717 -4.1595149 -4.2154565][-4.0279269 -3.9508543 -3.8964882 -3.866848 -3.86476 -3.8474207 -3.8243456 -3.8137116 -3.813549 -3.8288674 -3.858572 -3.9189141 -4.0034428 -4.0887542 -4.1648159][-3.9003968 -3.7988634 -3.727849 -3.68314 -3.6664271 -3.6348023 -3.6050811 -3.5967512 -3.6013942 -3.6272168 -3.6786928 -3.7765012 -3.902328 -4.0194759 -4.1183805][-3.7759628 -3.6481428 -3.5561802 -3.491884 -3.4570937 -3.4179635 -3.3901687 -3.3835635 -3.3854132 -3.4137909 -3.4849796 -3.6242549 -3.7991338 -3.9556806 -4.0818996][-3.6681619 -3.5162697 -3.4043529 -3.3282673 -3.2824991 -3.2372305 -3.2068071 -3.1940713 -3.185396 -3.2092185 -3.2974095 -3.4805961 -3.7067974 -3.9037652 -4.0561814][-3.5749507 -3.4067416 -3.28376 -3.2062826 -3.1579099 -3.1129088 -3.0788527 -3.0596745 -3.0456786 -3.0670254 -3.168992 -3.386025 -3.6476195 -3.871444 -4.0422716][-3.5378268 -3.3670897 -3.2480922 -3.1809201 -3.1402144 -3.1005056 -3.0689311 -3.0534835 -3.0445294 -3.0714009 -3.1802616 -3.3989141 -3.6579866 -3.8828163 -4.0561595][-3.5991368 -3.44508 -3.3452926 -3.2942097 -3.2648458 -3.2349346 -3.2134223 -3.2087586 -3.2100196 -3.2461538 -3.3531275 -3.542738 -3.7585239 -3.95015 -4.1016469][-3.7298398 -3.6063974 -3.5328231 -3.4989729 -3.4813673 -3.4649539 -3.4558759 -3.4624398 -3.4722869 -3.5100868 -3.6028254 -3.748399 -3.9092183 -4.0527697 -4.1683273][-3.8881264 -3.8054259 -3.7624302 -3.7454908 -3.7383373 -3.73364 -3.7348623 -3.7482665 -3.7648854 -3.7973995 -3.8648038 -3.9618037 -4.0658112 -4.1605659 -4.2378263][-4.0350676 -3.9888406 -3.9702554 -3.9660134 -3.9649165 -3.9664793 -3.973011 -3.9880564 -4.0061984 -4.0317736 -4.0745845 -4.1311622 -4.1902671 -4.2458038 -4.2926397][-4.1585436 -4.1360297 -4.1310463 -4.1342721 -4.1365352 -4.1400824 -4.146389 -4.1579881 -4.1718106 -4.1889215 -4.2138457 -4.2440214 -4.2743168 -4.303412 -4.3292723][-4.2603688 -4.2506576 -4.2504964 -4.25479 -4.2580481 -4.2623549 -4.2673254 -4.2742314 -4.2818117 -4.2904663 -4.3016858 -4.3153071 -4.3289418 -4.3422365 -4.3545489]]...]
INFO - root - 2017-12-05 08:48:38.582237: step 3210, loss = 1.72, batch loss = 1.66 (38.5 examples/sec; 0.208 sec/batch; 19h:01m:20s remains)
INFO - root - 2017-12-05 08:48:40.672558: step 3220, loss = 1.79, batch loss = 1.73 (39.5 examples/sec; 0.203 sec/batch; 18h:32m:05s remains)
INFO - root - 2017-12-05 08:48:42.733700: step 3230, loss = 1.73, batch loss = 1.67 (38.9 examples/sec; 0.205 sec/batch; 18h:47m:25s remains)
INFO - root - 2017-12-05 08:48:44.814962: step 3240, loss = 1.80, batch loss = 1.74 (38.8 examples/sec; 0.206 sec/batch; 18h:51m:49s remains)
INFO - root - 2017-12-05 08:48:46.889116: step 3250, loss = 1.82, batch loss = 1.76 (39.7 examples/sec; 0.202 sec/batch; 18h:26m:35s remains)
INFO - root - 2017-12-05 08:48:48.961179: step 3260, loss = 1.80, batch loss = 1.74 (39.2 examples/sec; 0.204 sec/batch; 18h:40m:30s remains)
INFO - root - 2017-12-05 08:48:51.040235: step 3270, loss = 1.81, batch loss = 1.75 (38.7 examples/sec; 0.207 sec/batch; 18h:53m:53s remains)
INFO - root - 2017-12-05 08:48:53.101465: step 3280, loss = 1.68, batch loss = 1.62 (39.0 examples/sec; 0.205 sec/batch; 18h:45m:09s remains)
INFO - root - 2017-12-05 08:48:55.190098: step 3290, loss = 1.68, batch loss = 1.62 (38.7 examples/sec; 0.207 sec/batch; 18h:54m:25s remains)
INFO - root - 2017-12-05 08:48:57.294117: step 3300, loss = 1.72, batch loss = 1.66 (35.6 examples/sec; 0.225 sec/batch; 20h:34m:13s remains)
2017-12-05 08:48:57.580562: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2313356 -4.2376633 -4.2630234 -4.2907119 -4.3115082 -4.3252449 -4.3332348 -4.3338156 -4.3271136 -4.315815 -4.3017797 -4.2860656 -4.2754345 -4.269803 -4.2628675][-4.2491112 -4.2600732 -4.28495 -4.3088479 -4.3263893 -4.3384585 -4.3468723 -4.3503542 -4.349072 -4.3443933 -4.3378086 -4.3299251 -4.322166 -4.3111429 -4.2941923][-4.2748837 -4.285717 -4.3038206 -4.3193755 -4.3310065 -4.3402119 -4.3475466 -4.3512936 -4.3532677 -4.3551288 -4.3566723 -4.3571634 -4.3553648 -4.3459864 -4.3268037][-4.306262 -4.3119664 -4.3192086 -4.3224039 -4.3228436 -4.3226323 -4.3222833 -4.3207135 -4.3227649 -4.3308625 -4.3418283 -4.3531218 -4.3618684 -4.3615656 -4.3493462][-4.3300047 -4.3298454 -4.326035 -4.3159637 -4.3008914 -4.2832537 -4.2655764 -4.2517262 -4.2528076 -4.2697272 -4.2944465 -4.3219662 -4.3459754 -4.3578472 -4.3549242][-4.3385806 -4.3370795 -4.3261218 -4.3035364 -4.2701912 -4.2295032 -4.1873713 -4.1565032 -4.1572475 -4.187695 -4.2319164 -4.2793403 -4.3189516 -4.341619 -4.3459167][-4.339046 -4.3413215 -4.328877 -4.2984357 -4.2501979 -4.1893573 -4.1236358 -4.0751772 -4.07552 -4.11956 -4.18186 -4.2454257 -4.2956929 -4.3255186 -4.33415][-4.3424997 -4.3497906 -4.3399873 -4.3092046 -4.257093 -4.188869 -4.1139946 -4.0595031 -4.0605164 -4.1091366 -4.1755857 -4.241375 -4.2920818 -4.3225522 -4.3334684][-4.3539743 -4.3617053 -4.3553934 -4.3297062 -4.2849574 -4.2262073 -4.1627827 -4.1187282 -4.1217227 -4.1627073 -4.21656 -4.2687144 -4.3086667 -4.3339753 -4.3452044][-4.3702583 -4.3743839 -4.3687043 -4.3485332 -4.3159642 -4.2753005 -4.2340884 -4.2090969 -4.2142081 -4.2409244 -4.2751541 -4.3079324 -4.3335557 -4.3515544 -4.3616815][-4.3808589 -4.3806496 -4.3739762 -4.3593888 -4.33918 -4.3166165 -4.2973619 -4.2896967 -4.2952347 -4.3076849 -4.3234119 -4.3387442 -4.3517556 -4.3637896 -4.373261][-4.3813043 -4.3786564 -4.371376 -4.3619151 -4.3516989 -4.3435073 -4.3402238 -4.343585 -4.3489213 -4.3515477 -4.3535886 -4.3548932 -4.3573055 -4.3646379 -4.3737521][-4.3723497 -4.3705125 -4.364892 -4.3599143 -4.3575735 -4.3593044 -4.365087 -4.3730845 -4.3776932 -4.3754907 -4.3692474 -4.3603258 -4.3546023 -4.3575792 -4.365962][-4.3598437 -4.3608088 -4.3593802 -4.3589239 -4.3612113 -4.3670483 -4.3749895 -4.3827214 -4.3863316 -4.383635 -4.3753986 -4.3628397 -4.3536525 -4.3535571 -4.3598404][-4.3532138 -4.3566465 -4.3591309 -4.3624048 -4.36705 -4.3728032 -4.3785973 -4.3831325 -4.3849907 -4.383121 -4.3766 -4.3659444 -4.3583207 -4.3575253 -4.3615308]]...]
INFO - root - 2017-12-05 08:48:59.710450: step 3310, loss = 1.77, batch loss = 1.71 (37.8 examples/sec; 0.212 sec/batch; 19h:20m:24s remains)
INFO - root - 2017-12-05 08:49:01.813549: step 3320, loss = 1.80, batch loss = 1.74 (38.9 examples/sec; 0.205 sec/batch; 18h:47m:10s remains)
INFO - root - 2017-12-05 08:49:03.901920: step 3330, loss = 1.76, batch loss = 1.71 (37.6 examples/sec; 0.213 sec/batch; 19h:27m:51s remains)
INFO - root - 2017-12-05 08:49:05.989850: step 3340, loss = 1.74, batch loss = 1.68 (39.1 examples/sec; 0.205 sec/batch; 18h:42m:13s remains)
INFO - root - 2017-12-05 08:49:08.081489: step 3350, loss = 1.81, batch loss = 1.76 (38.8 examples/sec; 0.206 sec/batch; 18h:50m:34s remains)
INFO - root - 2017-12-05 08:49:10.216640: step 3360, loss = 1.71, batch loss = 1.65 (38.2 examples/sec; 0.210 sec/batch; 19h:10m:09s remains)
INFO - root - 2017-12-05 08:49:12.304881: step 3370, loss = 1.73, batch loss = 1.67 (38.9 examples/sec; 0.206 sec/batch; 18h:47m:35s remains)
INFO - root - 2017-12-05 08:49:14.398008: step 3380, loss = 1.67, batch loss = 1.62 (37.0 examples/sec; 0.216 sec/batch; 19h:47m:10s remains)
INFO - root - 2017-12-05 08:49:16.449281: step 3390, loss = 1.79, batch loss = 1.73 (38.7 examples/sec; 0.206 sec/batch; 18h:52m:27s remains)
INFO - root - 2017-12-05 08:49:18.523967: step 3400, loss = 1.67, batch loss = 1.61 (37.7 examples/sec; 0.212 sec/batch; 19h:23m:50s remains)
2017-12-05 08:49:18.796488: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.2945721 -3.3290706 -3.3957438 -3.4356461 -3.4525421 -3.463856 -3.4800217 -3.5211344 -3.5803413 -3.6407809 -3.6872656 -3.71095 -3.7262983 -3.7343431 -3.7362273][-3.1664863 -3.2306828 -3.3243673 -3.3818524 -3.4120533 -3.4254749 -3.4292688 -3.4562764 -3.5083604 -3.5642319 -3.6034925 -3.6171675 -3.6212218 -3.617883 -3.6135652][-3.1279874 -3.2151203 -3.3259633 -3.3891091 -3.4201591 -3.4293008 -3.42308 -3.4328218 -3.4692016 -3.5137439 -3.5421469 -3.5426137 -3.5290375 -3.5090628 -3.4992192][-3.1639438 -3.2479494 -3.3481722 -3.3967972 -3.4122448 -3.409018 -3.3906817 -3.3879189 -3.4160676 -3.458205 -3.4831471 -3.4787776 -3.4519989 -3.4159434 -3.401916][-3.2133541 -3.2729492 -3.348125 -3.3677514 -3.3595991 -3.3322694 -3.2929628 -3.2765224 -3.3088422 -3.3668063 -3.4068959 -3.4137406 -3.3909798 -3.3564262 -3.3422141][-3.2291849 -3.2551556 -3.2980587 -3.2885532 -3.2534165 -3.1922708 -3.1175046 -3.0808539 -3.134037 -3.231811 -3.3064728 -3.3422439 -3.3444276 -3.3305683 -3.3241768][-3.215291 -3.2064266 -3.2122214 -3.1752439 -3.1127605 -3.0173492 -2.8993845 -2.8430018 -2.9331956 -3.0904727 -3.2115123 -3.2829542 -3.313755 -3.3201432 -3.3185315][-3.1984611 -3.1572862 -3.1305788 -3.0686388 -2.9836347 -2.8691854 -2.7262549 -2.6687522 -2.8048811 -3.0152228 -3.1713479 -3.2639866 -3.3081756 -3.3217888 -3.3156395][-3.2167006 -3.15075 -3.1091304 -3.0433831 -2.9591124 -2.8604479 -2.749146 -2.723237 -2.8645906 -3.0673685 -3.2109361 -3.2830205 -3.3130338 -3.3179836 -3.2999701][-3.2452087 -3.1691914 -3.1343398 -3.0871017 -3.0282962 -2.9733415 -2.9185071 -2.9153934 -3.0220776 -3.1668406 -3.2564921 -3.2849002 -3.2869945 -3.275002 -3.24994][-3.2739377 -3.1975338 -3.1714058 -3.146769 -3.1160507 -3.0956836 -3.075314 -3.0749588 -3.137579 -3.2211084 -3.2564564 -3.2433987 -3.2144279 -3.1873996 -3.1688046][-3.3038793 -3.22539 -3.2046018 -3.1932628 -3.1758153 -3.1649346 -3.1599064 -3.164186 -3.1986146 -3.2341866 -3.2244048 -3.1714239 -3.1129379 -3.0757804 -3.0696707][-3.3185065 -3.2438619 -3.2260091 -3.2178209 -3.2027144 -3.1887994 -3.1824145 -3.1891875 -3.2056279 -3.2133603 -3.1755919 -3.0938315 -3.0156803 -2.9782076 -2.9886832][-3.333446 -3.2690954 -3.2583075 -3.2524552 -3.2322292 -3.2030363 -3.1841869 -3.1846297 -3.1876962 -3.1765037 -3.127286 -3.0446587 -2.9719675 -2.9428148 -2.9618354][-3.3703513 -3.316365 -3.31496 -3.3091483 -3.2782097 -3.2345612 -3.2054472 -3.1992483 -3.1957326 -3.1797042 -3.1340377 -3.0693634 -3.0132279 -2.9929545 -3.01092]]...]
INFO - root - 2017-12-05 08:49:20.867645: step 3410, loss = 1.60, batch loss = 1.54 (38.0 examples/sec; 0.210 sec/batch; 19h:14m:27s remains)
INFO - root - 2017-12-05 08:49:22.987836: step 3420, loss = 1.71, batch loss = 1.65 (38.2 examples/sec; 0.209 sec/batch; 19h:07m:47s remains)
INFO - root - 2017-12-05 08:49:25.091863: step 3430, loss = 1.71, batch loss = 1.65 (37.7 examples/sec; 0.212 sec/batch; 19h:22m:54s remains)
INFO - root - 2017-12-05 08:49:27.202725: step 3440, loss = 1.79, batch loss = 1.73 (36.9 examples/sec; 0.217 sec/batch; 19h:48m:00s remains)
INFO - root - 2017-12-05 08:49:29.318328: step 3450, loss = 1.79, batch loss = 1.73 (39.2 examples/sec; 0.204 sec/batch; 18h:40m:29s remains)
INFO - root - 2017-12-05 08:49:31.432532: step 3460, loss = 1.68, batch loss = 1.62 (37.1 examples/sec; 0.216 sec/batch; 19h:43m:22s remains)
INFO - root - 2017-12-05 08:49:33.485994: step 3470, loss = 1.73, batch loss = 1.68 (38.9 examples/sec; 0.206 sec/batch; 18h:48m:13s remains)
INFO - root - 2017-12-05 08:49:35.568168: step 3480, loss = 1.78, batch loss = 1.72 (39.1 examples/sec; 0.205 sec/batch; 18h:42m:48s remains)
INFO - root - 2017-12-05 08:49:37.671810: step 3490, loss = 1.69, batch loss = 1.63 (37.6 examples/sec; 0.213 sec/batch; 19h:28m:14s remains)
INFO - root - 2017-12-05 08:49:39.791455: step 3500, loss = 1.65, batch loss = 1.60 (38.3 examples/sec; 0.209 sec/batch; 19h:03m:56s remains)
2017-12-05 08:49:40.071898: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.3718567 -3.3215196 -3.3047929 -3.2979336 -3.303154 -3.3063366 -3.2930686 -3.2852111 -3.280091 -3.2758527 -3.2746391 -3.2560053 -3.2238755 -3.1933975 -3.1717546][-3.2775853 -3.2078524 -3.1808515 -3.1706378 -3.176754 -3.1783113 -3.1625159 -3.1561892 -3.154881 -3.151073 -3.1487281 -3.1268857 -3.0894125 -3.0520864 -3.0246243][-3.2149887 -3.1338449 -3.1025453 -3.0919552 -3.1001775 -3.1033392 -3.0872145 -3.0845213 -3.0909963 -3.0880156 -3.0818863 -3.0510321 -3.0003347 -2.9491141 -2.9112229][-3.1906304 -3.1016059 -3.0698397 -3.0609667 -3.0747805 -3.0841308 -3.0721934 -3.0754933 -3.0912347 -3.0902309 -3.072192 -3.0279253 -2.9616175 -2.8906593 -2.8383512][-3.1963534 -3.0959785 -3.0596857 -3.0521989 -3.0772963 -3.101846 -3.1041117 -3.1180868 -3.1453924 -3.1504354 -3.1250763 -3.0723851 -2.9979606 -2.9160855 -2.855408][-3.199332 -3.0913639 -3.0483375 -3.0461817 -3.0844336 -3.1224527 -3.1394243 -3.1692729 -3.2141275 -3.2318625 -3.2091203 -3.1587341 -3.0888805 -3.0021675 -2.9241893][-3.1782959 -3.076314 -3.0404592 -3.0490324 -3.097136 -3.1456721 -3.1727772 -3.2099526 -3.2612114 -3.2825832 -3.2604909 -3.2129984 -3.1513166 -3.0655351 -2.9817448][-3.1559 -3.0688229 -3.0551691 -3.0829248 -3.1458788 -3.2064 -3.2408276 -3.2762833 -3.3173194 -3.3227236 -3.2870536 -3.2362704 -3.1823072 -3.1098351 -3.0394888][-3.1653686 -3.0984225 -3.1112144 -3.1681452 -3.2517319 -3.3263152 -3.3668652 -3.3950257 -3.417429 -3.4042816 -3.357465 -3.303757 -3.2583947 -3.2053211 -3.1615705][-3.2241659 -3.1779194 -3.2160997 -3.2975082 -3.3941951 -3.4740822 -3.5174575 -3.5394688 -3.5451066 -3.5176594 -3.4658229 -3.4140882 -3.3773293 -3.3452182 -3.3278832][-3.3117242 -3.2794619 -3.3309298 -3.4173276 -3.5119452 -3.5909126 -3.6353955 -3.6562662 -3.6548011 -3.6246881 -3.580111 -3.5380776 -3.5103364 -3.4944098 -3.4963148][-3.3921854 -3.3738196 -3.4318752 -3.5123312 -3.5982432 -3.671562 -3.7130823 -3.7349157 -3.7356255 -3.7168891 -3.689784 -3.6627424 -3.64356 -3.63688 -3.6491466][-3.4632807 -3.4532232 -3.5098772 -3.5808265 -3.6551294 -3.7173538 -3.7519426 -3.7699122 -3.7715373 -3.7642498 -3.7537518 -3.7430615 -3.7355762 -3.7380428 -3.755028][-3.5072653 -3.4941936 -3.5416558 -3.5979271 -3.6559248 -3.7045367 -3.7334692 -3.7468615 -3.7469089 -3.7446265 -3.7452054 -3.7474089 -3.7514215 -3.7630866 -3.7846282][-3.5055242 -3.4852262 -3.5231676 -3.5715752 -3.6190944 -3.6592066 -3.6838338 -3.6935742 -3.6922822 -3.6939325 -3.7023039 -3.7134018 -3.7259181 -3.7424657 -3.7621281]]...]
INFO - root - 2017-12-05 08:49:42.158228: step 3510, loss = 1.75, batch loss = 1.70 (35.2 examples/sec; 0.228 sec/batch; 20h:47m:28s remains)
INFO - root - 2017-12-05 08:49:44.254858: step 3520, loss = 1.74, batch loss = 1.68 (38.2 examples/sec; 0.209 sec/batch; 19h:08m:40s remains)
INFO - root - 2017-12-05 08:49:46.324465: step 3530, loss = 1.67, batch loss = 1.61 (38.6 examples/sec; 0.207 sec/batch; 18h:55m:56s remains)
INFO - root - 2017-12-05 08:49:48.421958: step 3540, loss = 1.77, batch loss = 1.72 (36.8 examples/sec; 0.218 sec/batch; 19h:53m:08s remains)
INFO - root - 2017-12-05 08:49:50.495888: step 3550, loss = 1.70, batch loss = 1.64 (36.6 examples/sec; 0.219 sec/batch; 19h:59m:38s remains)
INFO - root - 2017-12-05 08:49:52.577547: step 3560, loss = 1.67, batch loss = 1.61 (37.2 examples/sec; 0.215 sec/batch; 19h:37m:29s remains)
INFO - root - 2017-12-05 08:49:54.683730: step 3570, loss = 1.70, batch loss = 1.64 (37.8 examples/sec; 0.211 sec/batch; 19h:19m:01s remains)
INFO - root - 2017-12-05 08:49:56.765114: step 3580, loss = 1.67, batch loss = 1.61 (38.0 examples/sec; 0.211 sec/batch; 19h:14m:40s remains)
INFO - root - 2017-12-05 08:49:58.869919: step 3590, loss = 1.64, batch loss = 1.58 (37.7 examples/sec; 0.212 sec/batch; 19h:24m:31s remains)
INFO - root - 2017-12-05 08:50:00.964117: step 3600, loss = 1.64, batch loss = 1.58 (38.7 examples/sec; 0.207 sec/batch; 18h:53m:31s remains)
2017-12-05 08:50:01.261220: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.9706993 -3.9840009 -3.9895399 -3.9629819 -3.8982995 -3.7982545 -3.6855369 -3.5809448 -3.5076156 -3.4774189 -3.5016785 -3.5917208 -3.7222631 -3.8534269 -3.9726622][-3.9878035 -4.0070443 -4.0116243 -3.9747274 -3.9001312 -3.7917295 -3.6746538 -3.5702295 -3.4920726 -3.457756 -3.481385 -3.5667062 -3.6917415 -3.8169379 -3.9292057][-3.9295621 -3.957561 -3.9671097 -3.9336851 -3.8652782 -3.7625427 -3.6527019 -3.5591688 -3.4917772 -3.4645 -3.487685 -3.5588925 -3.6565104 -3.7551935 -3.8539021][-3.8103337 -3.8465657 -3.8599281 -3.8286185 -3.7659168 -3.6720068 -3.5706868 -3.4880931 -3.4373376 -3.4265895 -3.4506009 -3.5021262 -3.569685 -3.6422758 -3.7335811][-3.6933887 -3.7297938 -3.7323067 -3.6880679 -3.6180537 -3.5248256 -3.4254556 -3.3532348 -3.3169174 -3.319747 -3.342577 -3.3782568 -3.4215906 -3.4782414 -3.5717764][-3.6264782 -3.6418569 -3.6190495 -3.5487852 -3.4568925 -3.3517032 -3.2464037 -3.1803832 -3.1541271 -3.1684942 -3.2007124 -3.2332306 -3.2569654 -3.2990217 -3.3956316][-3.6182175 -3.5980227 -3.5410979 -3.4429064 -3.3301616 -3.2076972 -3.0866513 -3.0152838 -2.9881458 -3.0090923 -3.0577755 -3.0946012 -3.1062689 -3.1423621 -3.2473314][-3.6745107 -3.6190844 -3.5345876 -3.4207458 -3.2994165 -3.1687365 -3.0361905 -2.9587398 -2.9262807 -2.9436612 -2.9933641 -3.0205059 -3.0153797 -3.0457349 -3.1568775][-3.78306 -3.7066343 -3.612361 -3.4980676 -3.3867192 -3.2710035 -3.1529911 -3.0806832 -3.044023 -3.0420804 -3.0652127 -3.0668118 -3.037982 -3.0573015 -3.1647918][-3.9205146 -3.8349955 -3.7419028 -3.6365943 -3.5425811 -3.45315 -3.3580918 -3.2939959 -3.2551539 -3.2323627 -3.2236252 -3.2026539 -3.1625535 -3.1810267 -3.2839665][-4.0535951 -3.9702518 -3.8842869 -3.7957993 -3.7210679 -3.6539001 -3.5832224 -3.5314603 -3.4897056 -3.4518821 -3.4214931 -3.3856053 -3.3498828 -3.3739076 -3.4661419][-4.1623759 -4.0865722 -4.0108643 -3.9402483 -3.8835814 -3.8350041 -3.7858531 -3.7448039 -3.7033782 -3.6574583 -3.6159992 -3.579638 -3.5563464 -3.5827634 -3.6592436][-4.237009 -4.169518 -4.1026187 -4.0452232 -4.0014858 -3.9688888 -3.9366753 -3.9057992 -3.8718183 -3.8335202 -3.7966466 -3.7674453 -3.7546511 -3.776181 -3.8309906][-4.277492 -4.2195358 -4.1592345 -4.1076202 -4.070117 -4.0458679 -4.0268965 -4.0100083 -3.9937637 -3.9754448 -3.9562125 -3.9391775 -3.9304652 -3.9400613 -3.9710858][-4.2993426 -4.248208 -4.1895885 -4.1375875 -4.0990787 -4.0747762 -4.0634322 -4.0620031 -4.0676589 -4.0745029 -4.0766468 -4.0735126 -4.0657439 -4.0630412 -4.075]]...]
INFO - root - 2017-12-05 08:50:03.345230: step 3610, loss = 1.64, batch loss = 1.58 (36.5 examples/sec; 0.219 sec/batch; 20h:00m:50s remains)
INFO - root - 2017-12-05 08:50:05.436805: step 3620, loss = 1.61, batch loss = 1.56 (39.2 examples/sec; 0.204 sec/batch; 18h:37m:35s remains)
INFO - root - 2017-12-05 08:50:07.507826: step 3630, loss = 1.62, batch loss = 1.56 (39.1 examples/sec; 0.205 sec/batch; 18h:41m:51s remains)
INFO - root - 2017-12-05 08:50:09.619856: step 3640, loss = 1.63, batch loss = 1.57 (37.4 examples/sec; 0.214 sec/batch; 19h:32m:12s remains)
INFO - root - 2017-12-05 08:50:11.722601: step 3650, loss = 1.71, batch loss = 1.65 (38.5 examples/sec; 0.208 sec/batch; 18h:58m:54s remains)
INFO - root - 2017-12-05 08:50:13.814298: step 3660, loss = 1.80, batch loss = 1.74 (39.5 examples/sec; 0.203 sec/batch; 18h:31m:05s remains)
INFO - root - 2017-12-05 08:50:15.898047: step 3670, loss = 1.67, batch loss = 1.61 (38.0 examples/sec; 0.210 sec/batch; 19h:12m:19s remains)
INFO - root - 2017-12-05 08:50:17.976299: step 3680, loss = 1.67, batch loss = 1.61 (39.1 examples/sec; 0.205 sec/batch; 18h:41m:13s remains)
INFO - root - 2017-12-05 08:50:20.049576: step 3690, loss = 1.69, batch loss = 1.63 (39.2 examples/sec; 0.204 sec/batch; 18h:38m:37s remains)
INFO - root - 2017-12-05 08:50:22.173746: step 3700, loss = 1.60, batch loss = 1.54 (38.9 examples/sec; 0.206 sec/batch; 18h:48m:00s remains)
2017-12-05 08:50:22.456172: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.8637657 -3.8592374 -3.8754191 -3.9014256 -3.922375 -3.9425497 -3.9539886 -3.9488411 -3.93429 -3.9298596 -3.9584098 -4.0284071 -4.1219692 -4.2088079 -4.2725434][-3.5961871 -3.6116436 -3.6539135 -3.7052615 -3.7389526 -3.760699 -3.7624846 -3.7392626 -3.7058668 -3.6926274 -3.7343087 -3.8398671 -3.9825532 -4.1192961 -4.2232394][-3.3988791 -3.4343905 -3.5016274 -3.5691195 -3.6063256 -3.6123333 -3.5811806 -3.520236 -3.4640203 -3.4505968 -3.5113058 -3.6506598 -3.8376236 -4.0222831 -4.1656971][-3.2616127 -3.3172412 -3.4061165 -3.4835637 -3.5150547 -3.4904511 -3.4077449 -3.2983649 -3.2309217 -3.2424698 -3.3409402 -3.5176718 -3.7420058 -3.9586775 -4.1246338][-3.1420507 -3.208792 -3.3169031 -3.3996615 -3.4164519 -3.3563743 -3.2162967 -3.0603075 -3.0037549 -3.0739717 -3.2361395 -3.4570832 -3.7043149 -3.9301584 -4.1030989][-3.0611863 -3.1200817 -3.224031 -3.291235 -3.2815549 -3.1737733 -2.9631815 -2.7626188 -2.7511632 -2.9191151 -3.1671059 -3.4340124 -3.6983023 -3.9239798 -4.0951409][-3.0506318 -3.0826406 -3.153266 -3.1718502 -3.1017523 -2.9090171 -2.5946822 -2.3264065 -2.3860636 -2.6930518 -3.0576882 -3.3947449 -3.6892052 -3.9223905 -4.0947628][-3.0989172 -3.1067715 -3.1349914 -3.0947189 -2.9539828 -2.6666665 -2.2403 -1.9045868 -2.0365918 -2.4667177 -2.9350021 -3.336431 -3.6605237 -3.9086039 -4.0895514][-3.171679 -3.1550999 -3.1462436 -3.0674634 -2.8918128 -2.5914807 -2.1799579 -1.9011991 -2.0485542 -2.4658313 -2.9228354 -3.3213756 -3.6403134 -3.8896008 -4.0786743][-3.257947 -3.2162862 -3.1725745 -3.0663555 -2.8899598 -2.632627 -2.3167253 -2.1527143 -2.3064611 -2.6499944 -3.0312133 -3.3752229 -3.6584711 -3.8892927 -4.0744429][-3.3677678 -3.3081419 -3.2348447 -3.1080494 -2.9419646 -2.7392116 -2.5206287 -2.4535689 -2.6113102 -2.8885579 -3.1933634 -3.4741793 -3.7139578 -3.9171271 -4.0858216][-3.4925289 -3.433311 -3.3543882 -3.2299521 -3.087517 -2.9338281 -2.788137 -2.7726812 -2.9100497 -3.1241021 -3.3594403 -3.5852964 -3.7842574 -3.9604609 -4.1100168][-3.6058042 -3.5607145 -3.4959671 -3.3969479 -3.2916417 -3.183341 -3.0845404 -3.0824256 -3.1906798 -3.349895 -3.5267596 -3.7046034 -3.8634779 -4.0128393 -4.1420083][-3.7115948 -3.6808059 -3.6366093 -3.5712795 -3.5058656 -3.4385381 -3.3711119 -3.3687935 -3.4461379 -3.5617375 -3.69418 -3.8338063 -3.9597297 -4.0786281 -4.1816425][-3.7899878 -3.7726254 -3.7500215 -3.7122066 -3.6712286 -3.62653 -3.580565 -3.5813775 -3.6397877 -3.7261548 -3.8275816 -3.9393904 -4.0420966 -4.1359649 -4.2166085]]...]
INFO - root - 2017-12-05 08:50:24.530861: step 3710, loss = 1.57, batch loss = 1.51 (39.6 examples/sec; 0.202 sec/batch; 18h:26m:48s remains)
INFO - root - 2017-12-05 08:50:26.588113: step 3720, loss = 1.72, batch loss = 1.67 (39.1 examples/sec; 0.204 sec/batch; 18h:39m:52s remains)
INFO - root - 2017-12-05 08:50:28.640139: step 3730, loss = 1.58, batch loss = 1.53 (39.4 examples/sec; 0.203 sec/batch; 18h:31m:39s remains)
INFO - root - 2017-12-05 08:50:30.725134: step 3740, loss = 1.67, batch loss = 1.62 (38.8 examples/sec; 0.206 sec/batch; 18h:50m:22s remains)
INFO - root - 2017-12-05 08:50:32.826659: step 3750, loss = 1.66, batch loss = 1.60 (38.9 examples/sec; 0.206 sec/batch; 18h:46m:46s remains)
INFO - root - 2017-12-05 08:50:34.940533: step 3760, loss = 1.66, batch loss = 1.61 (37.1 examples/sec; 0.216 sec/batch; 19h:41m:53s remains)
INFO - root - 2017-12-05 08:50:37.014308: step 3770, loss = 1.65, batch loss = 1.59 (37.9 examples/sec; 0.211 sec/batch; 19h:17m:17s remains)
INFO - root - 2017-12-05 08:50:39.081365: step 3780, loss = 1.60, batch loss = 1.54 (39.6 examples/sec; 0.202 sec/batch; 18h:26m:47s remains)
INFO - root - 2017-12-05 08:50:41.162171: step 3790, loss = 1.57, batch loss = 1.51 (38.3 examples/sec; 0.209 sec/batch; 19h:03m:13s remains)
INFO - root - 2017-12-05 08:50:43.271298: step 3800, loss = 1.62, batch loss = 1.56 (37.8 examples/sec; 0.212 sec/batch; 19h:20m:53s remains)
2017-12-05 08:50:43.563558: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2874732 -4.2582989 -4.2291994 -4.2051382 -4.1895432 -4.1832948 -4.184926 -4.1845479 -4.1817551 -4.1796265 -4.1790276 -4.1827636 -4.1904731 -4.2076902 -4.2312264][-4.2291961 -4.1747022 -4.1196318 -4.0764265 -4.0496588 -4.0394897 -4.0415249 -4.040802 -4.0408592 -4.043715 -4.050777 -4.0620575 -4.07883 -4.1094379 -4.1469245][-4.1450033 -4.0501652 -3.9546022 -3.880532 -3.8369246 -3.8243091 -3.8307753 -3.83504 -3.8463721 -3.8635514 -3.883729 -3.906373 -3.9381645 -3.9862986 -4.0412941][-4.0393648 -3.892314 -3.7441626 -3.6299195 -3.5633473 -3.5440331 -3.5523863 -3.5670018 -3.5984666 -3.640594 -3.6841118 -3.7243004 -3.7747989 -3.8429375 -3.9177003][-3.9249442 -3.722198 -3.5150287 -3.3482628 -3.2426836 -3.2007427 -3.2004616 -3.2244492 -3.287549 -3.3699343 -3.4503427 -3.5188992 -3.59576 -3.6898282 -3.7886086][-3.8303621 -3.5791111 -3.3176279 -3.0941446 -2.9315989 -2.8444343 -2.8136246 -2.8325548 -2.9331894 -3.0723591 -3.2030263 -3.3113124 -3.4239843 -3.5512426 -3.6786556][-3.7741284 -3.4968421 -3.2037134 -2.9387 -2.7208161 -2.5714221 -2.4706025 -2.4445319 -2.5754054 -2.777184 -2.95943 -3.113534 -3.2714663 -3.4379778 -3.6008832][-3.7590475 -3.4840198 -3.1898642 -2.9177537 -2.6807585 -2.489536 -2.3156004 -2.2153664 -2.3402429 -2.5735726 -2.786839 -2.9771287 -3.1742539 -3.377583 -3.5731406][-3.781805 -3.5347495 -3.2771559 -3.0439796 -2.8395667 -2.6629364 -2.493536 -2.3805113 -2.4527307 -2.6311848 -2.813067 -2.9955211 -3.1962786 -3.406795 -3.6114912][-3.8284812 -3.6210732 -3.413975 -3.2376463 -3.0895972 -2.9565732 -2.8357692 -2.7615466 -2.7993233 -2.9055898 -3.0272326 -3.1645622 -3.3293362 -3.5129359 -3.7013867][-3.8892663 -3.7203434 -3.564271 -3.4444966 -3.352787 -3.2701426 -3.2031965 -3.1776347 -3.2092259 -3.2707529 -3.3409965 -3.4275546 -3.5422444 -3.6815591 -3.8363264][-3.9651005 -3.8333216 -3.7233057 -3.6488183 -3.6016762 -3.5650766 -3.5442386 -3.5553243 -3.5879059 -3.6269753 -3.6651559 -3.7130749 -3.7815542 -3.8753493 -3.9870512][-4.0561852 -3.9591122 -3.8857081 -3.843508 -3.8263092 -3.8216705 -3.8299534 -3.8574672 -3.8874667 -3.9158998 -3.9393833 -3.9642253 -4.0007272 -4.0566096 -4.1264949][-4.1559196 -4.0919313 -4.0447783 -4.0190339 -4.0132842 -4.0210953 -4.0395393 -4.0676751 -4.0913768 -4.1120968 -4.1279192 -4.1421924 -4.1609879 -4.1912351 -4.2301531][-4.2520337 -4.2152681 -4.1847625 -4.1664972 -4.1621237 -4.1674719 -4.1802258 -4.1992159 -4.2157693 -4.2304711 -4.2414107 -4.2513242 -4.2624979 -4.2780418 -4.295785]]...]
INFO - root - 2017-12-05 08:50:45.643528: step 3810, loss = 1.65, batch loss = 1.59 (39.2 examples/sec; 0.204 sec/batch; 18h:37m:22s remains)
INFO - root - 2017-12-05 08:50:47.733811: step 3820, loss = 1.56, batch loss = 1.50 (37.0 examples/sec; 0.216 sec/batch; 19h:44m:10s remains)
INFO - root - 2017-12-05 08:50:49.830494: step 3830, loss = 1.54, batch loss = 1.48 (37.2 examples/sec; 0.215 sec/batch; 19h:36m:40s remains)
INFO - root - 2017-12-05 08:50:51.936031: step 3840, loss = 1.54, batch loss = 1.48 (38.3 examples/sec; 0.209 sec/batch; 19h:03m:59s remains)
INFO - root - 2017-12-05 08:50:54.062237: step 3850, loss = 1.64, batch loss = 1.59 (38.1 examples/sec; 0.210 sec/batch; 19h:09m:50s remains)
INFO - root - 2017-12-05 08:50:56.153606: step 3860, loss = 1.65, batch loss = 1.59 (36.7 examples/sec; 0.218 sec/batch; 19h:54m:04s remains)
INFO - root - 2017-12-05 08:50:58.229779: step 3870, loss = 1.68, batch loss = 1.62 (38.4 examples/sec; 0.208 sec/batch; 19h:01m:30s remains)
INFO - root - 2017-12-05 08:51:00.283091: step 3880, loss = 1.71, batch loss = 1.65 (38.7 examples/sec; 0.207 sec/batch; 18h:51m:55s remains)
INFO - root - 2017-12-05 08:51:02.337776: step 3890, loss = 1.62, batch loss = 1.56 (37.3 examples/sec; 0.214 sec/batch; 19h:34m:16s remains)
INFO - root - 2017-12-05 08:51:04.446734: step 3900, loss = 1.61, batch loss = 1.55 (38.2 examples/sec; 0.209 sec/batch; 19h:07m:02s remains)
2017-12-05 08:51:04.722311: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.9342744 -3.7136827 -3.4897454 -3.3208923 -3.2215927 -3.1934838 -3.1966665 -3.2017803 -3.1918137 -3.177145 -3.1720557 -3.1675463 -3.1729486 -3.1605296 -3.1347091][-3.9119306 -3.67389 -3.4229155 -3.2237656 -3.1134973 -3.0980039 -3.1185555 -3.1350198 -3.1299877 -3.1185944 -3.1086547 -3.1039891 -3.1133487 -3.1085954 -3.0916882][-3.9063644 -3.6608691 -3.3969543 -3.1777754 -3.0626793 -3.062995 -3.1019907 -3.1276193 -3.1237745 -3.1110921 -3.0960579 -3.0868826 -3.0929546 -3.0879822 -3.0760982][-3.9150155 -3.6660469 -3.3939583 -3.1578352 -3.0357261 -3.0460696 -3.0986171 -3.1321769 -3.1304369 -3.1150312 -3.0943108 -3.0788536 -3.0760765 -3.0673919 -3.063112][-3.9378479 -3.6960928 -3.4275968 -3.1819043 -3.0459685 -3.053551 -3.1078916 -3.1421723 -3.1395 -3.1188922 -3.0915275 -3.067462 -3.0543158 -3.0438786 -3.051281][-3.9699054 -3.7458858 -3.498476 -3.2619789 -3.115572 -3.1077652 -3.148221 -3.1671748 -3.1481252 -3.1132019 -3.0769916 -3.0457759 -3.0291581 -3.0206199 -3.0326054][-4.0017838 -3.8022437 -3.5864503 -3.3774827 -3.2356005 -3.21529 -3.2387393 -3.2376561 -3.1963792 -3.1368053 -3.0804086 -3.0356457 -3.0120466 -3.0059495 -3.0220203][-4.0329967 -3.8591146 -3.6762064 -3.5028782 -3.3836632 -3.36325 -3.3738847 -3.3584583 -3.3007264 -3.2165132 -3.1359487 -3.0725436 -3.0338416 -3.0204439 -3.0353436][-4.0616574 -3.9124467 -3.7617497 -3.6267412 -3.5377538 -3.5269413 -3.531842 -3.5069077 -3.4370284 -3.3365779 -3.2391415 -3.1609221 -3.1094794 -3.0900514 -3.0971789][-4.083447 -3.9540365 -3.8316643 -3.7303348 -3.6719284 -3.673856 -3.67981 -3.6562047 -3.5858896 -3.4817078 -3.3786342 -3.291635 -3.2319963 -3.2078617 -3.2078929][-4.1035161 -3.9894857 -3.8897247 -3.8160734 -3.7831104 -3.796361 -3.8075545 -3.792398 -3.7310636 -3.6325636 -3.5271306 -3.4364386 -3.3738115 -3.346405 -3.3464642][-4.13127 -4.0319238 -3.9521618 -3.8998933 -3.8846397 -3.9049029 -3.9202843 -3.912499 -3.8595734 -3.7704911 -3.6690784 -3.5806777 -3.5192316 -3.490634 -3.4947419][-4.166985 -4.0842557 -4.0228062 -3.9866755 -3.9817026 -4.0022368 -4.0182743 -4.0131617 -3.9674704 -3.8903847 -3.8011527 -3.7222726 -3.6648192 -3.6365793 -3.6450951][-4.2086625 -4.1447105 -4.0996308 -4.07565 -4.0769367 -4.0956087 -4.1099567 -4.1057782 -4.067081 -4.0020776 -3.9262617 -3.8609469 -3.8143139 -3.7898543 -3.7982552][-4.2539563 -4.2076 -4.1763034 -4.1612749 -4.1655135 -4.1819091 -4.1940846 -4.190239 -4.157979 -4.1033258 -4.0395 -3.9865174 -3.952143 -3.9357994 -3.9460735]]...]
INFO - root - 2017-12-05 08:51:06.820127: step 3910, loss = 1.74, batch loss = 1.68 (37.5 examples/sec; 0.214 sec/batch; 19h:29m:20s remains)
INFO - root - 2017-12-05 08:51:08.898924: step 3920, loss = 1.52, batch loss = 1.46 (38.7 examples/sec; 0.207 sec/batch; 18h:51m:29s remains)
INFO - root - 2017-12-05 08:51:11.015437: step 3930, loss = 1.63, batch loss = 1.57 (37.3 examples/sec; 0.215 sec/batch; 19h:34m:43s remains)
INFO - root - 2017-12-05 08:51:13.076843: step 3940, loss = 1.56, batch loss = 1.50 (38.7 examples/sec; 0.207 sec/batch; 18h:52m:47s remains)
INFO - root - 2017-12-05 08:51:15.165906: step 3950, loss = 1.63, batch loss = 1.57 (39.1 examples/sec; 0.205 sec/batch; 18h:41m:10s remains)
INFO - root - 2017-12-05 08:51:17.247757: step 3960, loss = 1.51, batch loss = 1.45 (39.3 examples/sec; 0.204 sec/batch; 18h:35m:48s remains)
INFO - root - 2017-12-05 08:51:19.329986: step 3970, loss = 1.59, batch loss = 1.53 (38.2 examples/sec; 0.210 sec/batch; 19h:07m:19s remains)
INFO - root - 2017-12-05 08:51:21.458324: step 3980, loss = 1.62, batch loss = 1.56 (39.3 examples/sec; 0.204 sec/batch; 18h:35m:24s remains)
INFO - root - 2017-12-05 08:51:23.574266: step 3990, loss = 1.51, batch loss = 1.45 (37.2 examples/sec; 0.215 sec/batch; 19h:37m:32s remains)
INFO - root - 2017-12-05 08:51:25.660425: step 4000, loss = 1.57, batch loss = 1.51 (38.8 examples/sec; 0.206 sec/batch; 18h:48m:37s remains)
2017-12-05 08:51:25.922600: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.8130081 -3.8079882 -3.8398752 -3.862937 -3.8738184 -3.8830965 -3.8961148 -3.9199536 -3.955018 -3.9947572 -4.031918 -4.0545077 -4.0665846 -4.0688758 -4.0715294][-3.798053 -3.7855086 -3.8066373 -3.8206515 -3.823633 -3.8271127 -3.8372824 -3.8606219 -3.897135 -3.9366615 -3.9708748 -3.9916437 -4.0016155 -3.997741 -3.9910874][-3.8054965 -3.7792451 -3.7856219 -3.7872396 -3.7790878 -3.7740932 -3.7815456 -3.8105707 -3.8571639 -3.9030912 -3.9348814 -3.950743 -3.9552426 -3.9428921 -3.9251585][-3.7989848 -3.7583544 -3.7535894 -3.7482452 -3.7357385 -3.7278135 -3.7362442 -3.7736864 -3.8310418 -3.8817565 -3.9108548 -3.9195783 -3.9153969 -3.8965135 -3.8720844][-3.7854476 -3.7347808 -3.7228057 -3.7162426 -3.7065091 -3.699594 -3.7071855 -3.7486854 -3.8098278 -3.8604429 -3.8858654 -3.8900626 -3.8816156 -3.8642983 -3.8406816][-3.7687716 -3.710036 -3.6930554 -3.6859488 -3.6766384 -3.6665659 -3.6683798 -3.7062666 -3.7679257 -3.8231182 -3.8534729 -3.8616638 -3.8582253 -3.8493915 -3.8339033][-3.7490458 -3.6871085 -3.6677969 -3.6567855 -3.6390169 -3.6155157 -3.6004648 -3.6274316 -3.6952586 -3.7679546 -3.8166423 -3.8411686 -3.851084 -3.8538687 -3.8488388][-3.7329273 -3.6772809 -3.6597929 -3.6449368 -3.617116 -3.5756097 -3.5347743 -3.5424588 -3.6173272 -3.712503 -3.7852249 -3.8306105 -3.8567402 -3.870085 -3.8730073][-3.7319596 -3.684618 -3.6680508 -3.652411 -3.623764 -3.5774088 -3.5220158 -3.5099707 -3.5770051 -3.67975 -3.7671506 -3.8269756 -3.863925 -3.8840811 -3.8921866][-3.7549067 -3.7142143 -3.6986411 -3.6845543 -3.6587734 -3.6185722 -3.5648477 -3.5396674 -3.5837591 -3.67252 -3.7608767 -3.8260703 -3.8672142 -3.8902071 -3.9038224][-3.8043864 -3.7687678 -3.7545915 -3.7409432 -3.7155387 -3.6786416 -3.6281176 -3.5946851 -3.6159275 -3.6838577 -3.7652681 -3.8316512 -3.875385 -3.9008546 -3.9176221][-3.8836911 -3.8533173 -3.8373053 -3.8179204 -3.7864072 -3.7466195 -3.696651 -3.6590996 -3.6648173 -3.7171445 -3.7905207 -3.8546219 -3.8986394 -3.9228024 -3.9356067][-3.9801564 -3.953711 -3.9311941 -3.8985209 -3.855871 -3.8107419 -3.7610192 -3.7241774 -3.722837 -3.7659469 -3.8316557 -3.8915629 -3.932178 -3.9510381 -3.9563479][-4.06177 -4.038238 -4.0091944 -3.962513 -3.9068377 -3.8560457 -3.8079219 -3.7742183 -3.770916 -3.8101459 -3.8727562 -3.9304695 -3.9690342 -3.9816468 -3.9797382][-4.1027074 -4.0814023 -4.0491138 -3.9923735 -3.927022 -3.8711019 -3.826046 -3.79637 -3.7937737 -3.8341503 -3.8993542 -3.9590378 -4.0003939 -4.0118709 -4.0054436]]...]
INFO - root - 2017-12-05 08:51:28.011990: step 4010, loss = 1.66, batch loss = 1.60 (37.3 examples/sec; 0.214 sec/batch; 19h:33m:09s remains)
INFO - root - 2017-12-05 08:51:30.108294: step 4020, loss = 1.47, batch loss = 1.42 (37.9 examples/sec; 0.211 sec/batch; 19h:15m:25s remains)
INFO - root - 2017-12-05 08:51:32.192175: step 4030, loss = 1.45, batch loss = 1.39 (38.3 examples/sec; 0.209 sec/batch; 19h:02m:03s remains)
INFO - root - 2017-12-05 08:51:34.268447: step 4040, loss = 1.48, batch loss = 1.42 (39.0 examples/sec; 0.205 sec/batch; 18h:42m:04s remains)
INFO - root - 2017-12-05 08:51:36.350586: step 4050, loss = 1.48, batch loss = 1.43 (38.4 examples/sec; 0.208 sec/batch; 19h:00m:25s remains)
INFO - root - 2017-12-05 08:51:38.454452: step 4060, loss = 1.45, batch loss = 1.39 (37.3 examples/sec; 0.215 sec/batch; 19h:35m:13s remains)
INFO - root - 2017-12-05 08:51:40.528507: step 4070, loss = 1.48, batch loss = 1.43 (39.0 examples/sec; 0.205 sec/batch; 18h:41m:41s remains)
INFO - root - 2017-12-05 08:51:42.636654: step 4080, loss = 1.51, batch loss = 1.45 (39.7 examples/sec; 0.201 sec/batch; 18h:22m:22s remains)
INFO - root - 2017-12-05 08:51:44.678224: step 4090, loss = 1.47, batch loss = 1.41 (39.6 examples/sec; 0.202 sec/batch; 18h:25m:25s remains)
INFO - root - 2017-12-05 08:51:46.779995: step 4100, loss = 1.56, batch loss = 1.50 (38.2 examples/sec; 0.209 sec/batch; 19h:05m:36s remains)
2017-12-05 08:51:47.068768: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2863569 -4.2804809 -4.2794924 -4.2789922 -4.27804 -4.2763114 -4.2725649 -4.2673163 -4.2644367 -4.26387 -4.2624211 -4.2606936 -4.2612085 -4.2663364 -4.2768283][-4.1915331 -4.1770849 -4.1731815 -4.1732059 -4.1726251 -4.1714945 -4.166543 -4.1609912 -4.1599307 -4.1578393 -4.1517935 -4.1459513 -4.1464081 -4.1564789 -4.1791534][-4.049603 -4.025691 -4.0190377 -4.0199442 -4.0190749 -4.0180931 -4.0135808 -4.00945 -4.0085144 -4.0001831 -3.9853988 -3.9761291 -3.9799571 -4.0013018 -4.0452347][-3.8598685 -3.8297863 -3.8251097 -3.8293 -3.8279266 -3.8230977 -3.8143125 -3.809 -3.8056211 -3.7909694 -3.7721636 -3.7659698 -3.7800162 -3.8191285 -3.8868937][-3.6476054 -3.6151676 -3.6122892 -3.6186957 -3.6141381 -3.6017561 -3.582931 -3.5699241 -3.5607641 -3.5417254 -3.5290756 -3.5380232 -3.5695119 -3.6255665 -3.7106016][-3.4582236 -3.4240465 -3.419867 -3.4246173 -3.4137349 -3.3877654 -3.3504055 -3.3212481 -3.3097289 -3.3037958 -3.3146486 -3.3471174 -3.3942754 -3.4577994 -3.5473504][-3.3214455 -3.2845912 -3.27551 -3.2730141 -3.2502925 -3.2033639 -3.140326 -3.0945277 -3.0985513 -3.1322715 -3.1798644 -3.2313838 -3.2805614 -3.3369393 -3.4203596][-3.2364597 -3.2000759 -3.1869531 -3.1766133 -3.1412785 -3.0738127 -2.9849708 -2.928792 -2.9656115 -3.0478849 -3.1247878 -3.1793642 -3.2108431 -3.2491269 -3.3202977][-3.172431 -3.1426473 -3.1349335 -3.1254129 -3.0903 -3.0235023 -2.9395332 -2.9029486 -2.9652781 -3.0646462 -3.1378989 -3.1683021 -3.1634464 -3.1770296 -3.235795][-3.1250939 -3.1097364 -3.1176834 -3.1182475 -3.095444 -3.0517337 -3.0056658 -3.0041182 -3.0632465 -3.1354694 -3.1723442 -3.1608596 -3.1187294 -3.1171799 -3.1787775][-3.0904717 -3.091877 -3.1178176 -3.1301088 -3.1208477 -3.1004291 -3.0858526 -3.102663 -3.1456852 -3.1776409 -3.1712911 -3.1254692 -3.0692916 -3.0744944 -3.1586504][-3.0780911 -3.0821013 -3.1126022 -3.1280837 -3.1264174 -3.1178699 -3.1181364 -3.1360641 -3.1601067 -3.1620154 -3.1288922 -3.0705242 -3.0267179 -3.0597081 -3.1717019][-3.0924149 -3.0894403 -3.1097622 -3.11662 -3.1119835 -3.1028874 -3.1052232 -3.1223607 -3.1363194 -3.1276698 -3.0897927 -3.035253 -3.0139282 -3.0722103 -3.1986089][-3.1374006 -3.1183386 -3.119843 -3.1106362 -3.0967212 -3.0830302 -3.0840549 -3.103502 -3.1182475 -3.1152906 -3.0871472 -3.0461841 -3.0428052 -3.1112895 -3.240098][-3.2335875 -3.1953104 -3.1732411 -3.1461787 -3.1214032 -3.1044798 -3.1090808 -3.1318944 -3.1548443 -3.1655583 -3.1542263 -3.1270611 -3.1291273 -3.1932468 -3.3131266]]...]
INFO - root - 2017-12-05 08:51:49.158057: step 4110, loss = 1.48, batch loss = 1.42 (39.1 examples/sec; 0.204 sec/batch; 18h:39m:10s remains)
INFO - root - 2017-12-05 08:51:51.232965: step 4120, loss = 1.61, batch loss = 1.55 (38.9 examples/sec; 0.206 sec/batch; 18h:45m:29s remains)
INFO - root - 2017-12-05 08:51:53.300165: step 4130, loss = 1.52, batch loss = 1.47 (37.4 examples/sec; 0.214 sec/batch; 19h:30m:01s remains)
INFO - root - 2017-12-05 08:51:55.445828: step 4140, loss = 1.50, batch loss = 1.45 (37.5 examples/sec; 0.213 sec/batch; 19h:26m:15s remains)
INFO - root - 2017-12-05 08:51:57.543762: step 4150, loss = 1.50, batch loss = 1.44 (38.3 examples/sec; 0.209 sec/batch; 19h:02m:18s remains)
INFO - root - 2017-12-05 08:51:59.684771: step 4160, loss = 1.43, batch loss = 1.37 (37.4 examples/sec; 0.214 sec/batch; 19h:31m:18s remains)
INFO - root - 2017-12-05 08:52:01.768351: step 4170, loss = 1.60, batch loss = 1.54 (37.9 examples/sec; 0.211 sec/batch; 19h:13m:53s remains)
INFO - root - 2017-12-05 08:52:03.843150: step 4180, loss = 1.45, batch loss = 1.39 (39.2 examples/sec; 0.204 sec/batch; 18h:37m:16s remains)
INFO - root - 2017-12-05 08:52:05.914600: step 4190, loss = 1.46, batch loss = 1.40 (37.8 examples/sec; 0.212 sec/batch; 19h:17m:45s remains)
INFO - root - 2017-12-05 08:52:07.993425: step 4200, loss = 1.47, batch loss = 1.42 (39.0 examples/sec; 0.205 sec/batch; 18h:42m:44s remains)
2017-12-05 08:52:08.268161: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.1646934 -3.1217713 -3.1060762 -3.0498061 -2.9399328 -2.7990603 -2.6645699 -2.5683575 -2.5486543 -2.6379418 -2.83925 -3.093257 -3.3339734 -3.5482216 -3.7381685][-3.1373258 -3.096756 -3.0791574 -3.0040154 -2.8602686 -2.6789441 -2.5027623 -2.3648763 -2.309001 -2.3876643 -2.6112995 -2.9090958 -3.195024 -3.4450703 -3.6632795][-3.188653 -3.1517608 -3.1187923 -3.0191321 -2.8456044 -2.6333361 -2.4235947 -2.2457078 -2.1518116 -2.2119575 -2.4451914 -2.7757573 -3.1041698 -3.3922572 -3.6367912][-3.2505667 -3.2083974 -3.1526427 -3.0234623 -2.8273287 -2.5941038 -2.3608112 -2.1594698 -2.0434275 -2.088073 -2.3217566 -2.6737175 -3.0431392 -3.3702667 -3.6414132][-3.302175 -3.24309 -3.1607533 -3.0062385 -2.7947631 -2.5540559 -2.314971 -2.1145022 -1.996779 -2.0367136 -2.2647526 -2.6212544 -3.0131879 -3.3689039 -3.6644874][-3.3438935 -3.2607012 -3.1522856 -2.9816804 -2.7667818 -2.537571 -2.3192005 -2.1427467 -2.0444586 -2.0904071 -2.3095739 -2.6460834 -3.0287528 -3.3896031 -3.6982563][-3.3785462 -3.2711267 -3.1420708 -2.9697423 -2.7691126 -2.5705173 -2.3901112 -2.2517567 -2.186887 -2.251513 -2.4564214 -2.7504206 -3.0940044 -3.4372637 -3.7415874][-3.4383671 -3.3110194 -3.1689687 -3.0057747 -2.8388798 -2.6843829 -2.5525806 -2.4559603 -2.4199157 -2.4906025 -2.6688814 -2.9088578 -3.1990209 -3.507508 -3.791887][-3.5308557 -3.3955791 -3.256613 -3.1151936 -2.9899564 -2.8856468 -2.8045349 -2.7461276 -2.7278161 -2.78884 -2.9286859 -3.11374 -3.3473325 -3.6086829 -3.8575711][-3.6564932 -3.5302696 -3.4109986 -3.3024087 -3.2190423 -3.1566916 -3.1138594 -3.0853992 -3.0789819 -3.1239004 -3.2227435 -3.3573623 -3.5345466 -3.7394106 -3.939393][-3.8089359 -3.7060227 -3.6184108 -3.5452421 -3.4931817 -3.4583907 -3.4417813 -3.4360824 -3.4378943 -3.4665627 -3.5302415 -3.6210234 -3.7429814 -3.8869932 -4.0357389][-3.9641409 -3.8913152 -3.8339739 -3.7872348 -3.7564025 -3.7415633 -3.7419095 -3.7500467 -3.75672 -3.7751617 -3.8133824 -3.8687372 -3.9434333 -4.0358949 -4.1368732][-4.1089525 -4.062438 -4.0286689 -4.0006561 -3.9817474 -3.9765506 -3.9823031 -3.9921908 -3.9990849 -4.0103769 -4.0322952 -4.0640454 -4.1073442 -4.1624479 -4.2240644][-4.2126427 -4.1851549 -4.1661544 -4.1517181 -4.1428876 -4.1424475 -4.14737 -4.1533766 -4.1592536 -4.1672921 -4.1797628 -4.1974649 -4.2220597 -4.2536688 -4.2870941][-4.2771649 -4.2618251 -4.2529578 -4.2475147 -4.2451639 -4.2459097 -4.2487149 -4.2526097 -4.2575855 -4.2639642 -4.2720909 -4.2820311 -4.2950463 -4.3110166 -4.3268366]]...]
INFO - root - 2017-12-05 08:52:10.416964: step 4210, loss = 1.45, batch loss = 1.39 (38.0 examples/sec; 0.211 sec/batch; 19h:12m:49s remains)
INFO - root - 2017-12-05 08:52:12.531500: step 4220, loss = 1.59, batch loss = 1.53 (37.7 examples/sec; 0.212 sec/batch; 19h:20m:54s remains)
INFO - root - 2017-12-05 08:52:14.639983: step 4230, loss = 1.41, batch loss = 1.35 (37.9 examples/sec; 0.211 sec/batch; 19h:16m:12s remains)
INFO - root - 2017-12-05 08:52:16.717072: step 4240, loss = 1.56, batch loss = 1.50 (39.0 examples/sec; 0.205 sec/batch; 18h:42m:37s remains)
INFO - root - 2017-12-05 08:52:18.793654: step 4250, loss = 1.38, batch loss = 1.32 (39.2 examples/sec; 0.204 sec/batch; 18h:36m:05s remains)
INFO - root - 2017-12-05 08:52:20.863685: step 4260, loss = 1.44, batch loss = 1.38 (38.7 examples/sec; 0.207 sec/batch; 18h:50m:43s remains)
INFO - root - 2017-12-05 08:52:22.950388: step 4270, loss = 1.47, batch loss = 1.41 (37.6 examples/sec; 0.213 sec/batch; 19h:25m:27s remains)
INFO - root - 2017-12-05 08:52:25.044614: step 4280, loss = 1.44, batch loss = 1.39 (39.2 examples/sec; 0.204 sec/batch; 18h:37m:13s remains)
INFO - root - 2017-12-05 08:52:27.136522: step 4290, loss = 1.59, batch loss = 1.53 (38.3 examples/sec; 0.209 sec/batch; 19h:02m:42s remains)
INFO - root - 2017-12-05 08:52:29.217270: step 4300, loss = 1.34, batch loss = 1.28 (39.9 examples/sec; 0.201 sec/batch; 18h:17m:17s remains)
2017-12-05 08:52:29.511311: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2055893 -4.2061152 -4.203815 -4.193119 -4.1741214 -4.15032 -4.12997 -4.1224566 -4.131042 -4.1516891 -4.1754456 -4.19563 -4.2083254 -4.2124009 -4.2111721][-4.1748161 -4.181375 -4.1805825 -4.1647654 -4.1337819 -4.0936785 -4.0587926 -4.046176 -4.0591531 -4.0902781 -4.1279869 -4.1641011 -4.1895032 -4.19972 -4.200562][-4.14235 -4.1532211 -4.1521077 -4.1284881 -4.0828776 -4.0247111 -3.9759433 -3.9608924 -3.9806948 -4.023375 -4.0756779 -4.128067 -4.1662436 -4.182313 -4.1838183][-4.11525 -4.1249208 -4.1186371 -4.0835528 -4.0210071 -3.9449143 -3.8854504 -3.8701236 -3.8988786 -3.9558649 -4.0233154 -4.0902815 -4.1375928 -4.1564794 -4.1551247][-4.1009569 -4.1019883 -4.0846725 -4.0354629 -3.9562216 -3.863806 -3.7955346 -3.7806697 -3.8213775 -3.8962736 -3.9800274 -4.0583787 -4.1094289 -4.1255927 -4.1150303][-4.1004324 -4.0869355 -4.0543346 -3.9895267 -3.8953424 -3.7890747 -3.7130337 -3.7000318 -3.7553215 -3.8487909 -3.9472308 -4.0335445 -4.0842271 -4.0937042 -4.0699797][-4.1078339 -4.0787454 -4.031002 -3.952333 -3.8455076 -3.726881 -3.6428134 -3.6319995 -3.7014725 -3.8106806 -3.9202249 -4.0120964 -4.0635653 -4.0677557 -4.033361][-4.1158428 -4.0734906 -4.014606 -3.9277294 -3.8135078 -3.6858151 -3.5937448 -3.5841382 -3.6633685 -3.7824805 -3.8993497 -3.9950886 -4.0500889 -4.05472 -4.0171628][-4.1183414 -4.0654278 -4.0022221 -3.9177477 -3.8092542 -3.6847651 -3.5912039 -3.5788786 -3.6557095 -3.7725708 -3.8892946 -3.9871221 -4.0472064 -4.0577831 -4.0261307][-4.1138134 -4.0524249 -3.9906163 -3.918119 -3.8286402 -3.7229657 -3.6399612 -3.6265304 -3.6911161 -3.7932322 -3.8991518 -3.9915962 -4.0521946 -4.0693417 -4.0473084][-4.1025906 -4.0382557 -3.9835956 -3.9298289 -3.8662806 -3.7866905 -3.7200418 -3.7069843 -3.75591 -3.8381419 -3.9277661 -4.0094032 -4.0652108 -4.085289 -4.0689654][-4.0883055 -4.0301943 -3.990571 -3.9597826 -3.9240112 -3.8717186 -3.8218784 -3.8091342 -3.8424881 -3.9039345 -3.9751587 -4.0423822 -4.0887794 -4.1058121 -4.0880871][-4.0722337 -4.0287943 -4.0084705 -3.9985173 -3.9837267 -3.9532623 -3.9183853 -3.9071243 -3.9295812 -3.9751322 -4.0313225 -4.0852222 -4.1216464 -4.1318092 -4.1082735][-4.0526052 -4.0279241 -4.0238104 -4.0267997 -4.0216665 -4.0012507 -3.9759064 -3.9663088 -3.9826372 -4.0209951 -4.0714164 -4.1193037 -4.15148 -4.1584511 -4.1327157][-4.0218515 -4.0168457 -4.0247579 -4.0324469 -4.0265641 -4.0052991 -3.9820719 -3.9710979 -3.9831166 -4.0220675 -4.077517 -4.12952 -4.165216 -4.1755085 -4.1544886]]...]
INFO - root - 2017-12-05 08:52:31.597478: step 4310, loss = 1.52, batch loss = 1.46 (38.4 examples/sec; 0.208 sec/batch; 18h:58m:04s remains)
INFO - root - 2017-12-05 08:52:33.694612: step 4320, loss = 1.48, batch loss = 1.42 (38.3 examples/sec; 0.209 sec/batch; 19h:02m:43s remains)
INFO - root - 2017-12-05 08:52:35.764542: step 4330, loss = 1.52, batch loss = 1.46 (38.8 examples/sec; 0.206 sec/batch; 18h:48m:16s remains)
INFO - root - 2017-12-05 08:52:37.874687: step 4340, loss = 1.43, batch loss = 1.37 (37.4 examples/sec; 0.214 sec/batch; 19h:28m:34s remains)
INFO - root - 2017-12-05 08:52:39.966573: step 4350, loss = 1.52, batch loss = 1.46 (39.4 examples/sec; 0.203 sec/batch; 18h:30m:16s remains)
INFO - root - 2017-12-05 08:52:42.055371: step 4360, loss = 1.51, batch loss = 1.45 (38.0 examples/sec; 0.210 sec/batch; 19h:09m:55s remains)
INFO - root - 2017-12-05 08:52:44.161292: step 4370, loss = 1.52, batch loss = 1.46 (39.9 examples/sec; 0.201 sec/batch; 18h:17m:45s remains)
INFO - root - 2017-12-05 08:52:46.221612: step 4380, loss = 1.36, batch loss = 1.30 (39.1 examples/sec; 0.205 sec/batch; 18h:39m:43s remains)
INFO - root - 2017-12-05 08:52:48.296459: step 4390, loss = 1.48, batch loss = 1.42 (39.6 examples/sec; 0.202 sec/batch; 18h:25m:02s remains)
INFO - root - 2017-12-05 08:52:50.381145: step 4400, loss = 1.49, batch loss = 1.43 (37.7 examples/sec; 0.212 sec/batch; 19h:20m:03s remains)
2017-12-05 08:52:50.682957: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.0299952 -2.9702792 -2.9553919 -2.9519391 -2.9563549 -2.9666443 -2.9775057 -2.9847074 -2.9896674 -3.0050812 -3.0394578 -3.104373 -3.194263 -3.3221903 -3.4906456][-2.8685775 -2.8068759 -2.7963867 -2.7962537 -2.8022151 -2.8111897 -2.8224673 -2.8344164 -2.8483806 -2.8709383 -2.9088964 -2.9817252 -3.0835159 -3.2254157 -3.4075966][-2.7931104 -2.7389777 -2.7381997 -2.7471919 -2.7566209 -2.7631559 -2.7713878 -2.7851429 -2.806848 -2.8340979 -2.8726797 -2.9489987 -3.0569263 -3.2037687 -3.3889289][-2.7757487 -2.7314155 -2.7401183 -2.7610869 -2.7779374 -2.7843261 -2.7862616 -2.7952862 -2.8192806 -2.8490078 -2.8855474 -2.9587417 -3.0671344 -3.216929 -3.4025602][-2.7892966 -2.7545791 -2.7723346 -2.8062611 -2.8358541 -2.8466167 -2.8421917 -2.8413763 -2.8605571 -2.8892252 -2.9225419 -2.9891894 -3.0959601 -3.2493958 -3.435184][-2.8164353 -2.7910781 -2.8161263 -2.8606308 -2.9046 -2.9213996 -2.9103832 -2.8965819 -2.9076309 -2.9344354 -2.9682984 -3.0288296 -3.1344638 -3.2905393 -3.4744782][-2.8473465 -2.8235664 -2.8483877 -2.8980312 -2.9531474 -2.9766941 -2.9632955 -2.9406891 -2.94402 -2.9715428 -3.0112531 -3.0698662 -3.1728673 -3.32827 -3.5090294][-2.8702002 -2.8457747 -2.866797 -2.9203646 -2.9851193 -3.0142946 -2.9978144 -2.968209 -2.9655857 -2.9922757 -3.035583 -3.0947819 -3.1966615 -3.350543 -3.5300422][-2.8971763 -2.8681426 -2.8880486 -2.9449911 -3.014046 -3.0460572 -3.026474 -2.9839644 -2.9683628 -2.9883804 -3.0313191 -3.0949192 -3.2006271 -3.3563442 -3.5381131][-2.9371762 -2.9000926 -2.9166181 -2.9722657 -3.0402966 -3.0701084 -3.0446823 -2.9901934 -2.9611483 -2.9736748 -3.0133953 -3.0805712 -3.1915874 -3.3507957 -3.5364289][-2.9833326 -2.9438338 -2.9545593 -2.9976654 -3.0514562 -3.0713592 -3.0361195 -2.9738965 -2.9408691 -2.9534943 -2.9925621 -3.0619082 -3.1748598 -3.3371339 -3.5276306][-3.0191393 -2.9815636 -2.9904733 -3.0223789 -3.0617588 -3.0710106 -3.026248 -2.9576054 -2.9208896 -2.9358706 -2.974328 -3.0414104 -3.1539361 -3.3178668 -3.5153172][-3.0158162 -2.9837885 -3.0019126 -3.0387549 -3.0765493 -3.0831344 -3.0352647 -2.9658346 -2.9242079 -2.9360585 -2.9685435 -3.0293493 -3.1421525 -3.3096519 -3.5114233][-2.9861503 -2.9577634 -2.9902828 -3.0417042 -3.0877743 -3.1020865 -3.0656879 -3.0084884 -2.9682238 -2.9719055 -2.9944246 -3.0463264 -3.1596451 -3.3282328 -3.5265222][-2.9680498 -2.9392257 -2.9751821 -3.0352392 -3.093276 -3.1225741 -3.1014905 -3.0579371 -3.0247853 -3.0258379 -3.0438113 -3.0898685 -3.2020559 -3.3655496 -3.5549409]]...]
INFO - root - 2017-12-05 08:52:52.795214: step 4410, loss = 1.55, batch loss = 1.49 (37.7 examples/sec; 0.212 sec/batch; 19h:19m:03s remains)
INFO - root - 2017-12-05 08:52:54.866469: step 4420, loss = 1.56, batch loss = 1.51 (39.3 examples/sec; 0.204 sec/batch; 18h:32m:59s remains)
INFO - root - 2017-12-05 08:52:56.981369: step 4430, loss = 1.59, batch loss = 1.53 (37.8 examples/sec; 0.211 sec/batch; 19h:16m:03s remains)
INFO - root - 2017-12-05 08:52:59.040027: step 4440, loss = 1.45, batch loss = 1.40 (38.9 examples/sec; 0.205 sec/batch; 18h:43m:34s remains)
INFO - root - 2017-12-05 08:53:01.141034: step 4450, loss = 1.37, batch loss = 1.31 (37.1 examples/sec; 0.216 sec/batch; 19h:39m:10s remains)
INFO - root - 2017-12-05 08:53:03.220994: step 4460, loss = 1.47, batch loss = 1.41 (38.3 examples/sec; 0.209 sec/batch; 19h:02m:18s remains)
INFO - root - 2017-12-05 08:53:05.332126: step 4470, loss = 1.46, batch loss = 1.40 (38.2 examples/sec; 0.210 sec/batch; 19h:05m:27s remains)
INFO - root - 2017-12-05 08:53:07.481930: step 4480, loss = 1.39, batch loss = 1.34 (34.0 examples/sec; 0.235 sec/batch; 21h:25m:48s remains)
INFO - root - 2017-12-05 08:53:09.583379: step 4490, loss = 1.47, batch loss = 1.41 (38.0 examples/sec; 0.210 sec/batch; 19h:09m:50s remains)
INFO - root - 2017-12-05 08:53:11.642267: step 4500, loss = 1.34, batch loss = 1.28 (38.6 examples/sec; 0.207 sec/batch; 18h:51m:31s remains)
2017-12-05 08:53:11.902642: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1049633 -4.0772681 -4.0436258 -3.9974265 -3.9474175 -3.904778 -3.8680081 -3.8371093 -3.8312011 -3.8601513 -3.9107084 -3.98149 -4.0597057 -4.1361222 -4.1952391][-4.0422015 -3.9988627 -3.9437554 -3.8690722 -3.7844543 -3.7082448 -3.63864 -3.5855896 -3.5810032 -3.6334081 -3.7167022 -3.8302939 -3.9556098 -4.0742183 -4.1668987][-3.981056 -3.9159966 -3.8309333 -3.7200556 -3.5998769 -3.4886765 -3.3812089 -3.3005121 -3.3007891 -3.3881226 -3.5142274 -3.6762953 -3.8542321 -4.0193191 -4.1478434][-3.9092803 -3.8186684 -3.6999834 -3.5548816 -3.4051585 -3.2632666 -3.1146543 -3.0070262 -3.0205746 -3.1489291 -3.3175035 -3.5261793 -3.7547913 -3.9664254 -4.1293488][-3.8244696 -3.7088454 -3.5585442 -3.3828788 -3.2094197 -3.0431356 -2.8600121 -2.7324946 -2.7678668 -2.9390364 -3.1489139 -3.3987508 -3.6711078 -3.9228811 -4.1126909][-3.7377894 -3.6033235 -3.4279983 -3.2254913 -3.0315385 -2.8448982 -2.6362522 -2.5046732 -2.5759158 -2.78996 -3.0346518 -3.3145118 -3.61801 -3.8962049 -4.103024][-3.6604123 -3.5230398 -3.341259 -3.1274104 -2.920836 -2.7125638 -2.4780517 -2.3493104 -2.4650755 -2.7185063 -2.989542 -3.2879703 -3.604707 -3.8898585 -4.1006656][-3.6015625 -3.4806528 -3.3131809 -3.1091106 -2.9060588 -2.6843722 -2.4261785 -2.2953651 -2.4539289 -2.7394204 -3.0220065 -3.3192499 -3.6284056 -3.8996389 -4.1016431][-3.5649543 -3.4687605 -3.328968 -3.154959 -2.9753137 -2.7710562 -2.5277929 -2.4045699 -2.5615144 -2.8330965 -3.0996432 -3.3773162 -3.6606746 -3.9049289 -4.0898113][-3.5460987 -3.4756756 -3.365967 -3.2252197 -3.0789304 -2.9129953 -2.721386 -2.625463 -2.7382984 -2.9507253 -3.1766973 -3.4191747 -3.6669464 -3.8830371 -4.05157][-3.5508509 -3.5021319 -3.4187403 -3.30587 -3.1871004 -3.0478754 -2.8974662 -2.8286247 -2.9004321 -3.0482292 -3.2229381 -3.4203808 -3.628629 -3.8180795 -3.9721589][-3.5712612 -3.5511553 -3.4991155 -3.4113874 -3.3080053 -3.1833234 -3.0562949 -2.9971049 -3.033783 -3.1252556 -3.2465911 -3.3909411 -3.5512967 -3.7076895 -3.8440168][-3.6018591 -3.6230292 -3.614347 -3.5547421 -3.4646103 -3.3488445 -3.2311387 -3.1643934 -3.1611114 -3.1980302 -3.2647126 -3.3545122 -3.4614656 -3.5764909 -3.687495][-3.6413465 -3.7090278 -3.7448258 -3.7155933 -3.6409121 -3.5343602 -3.4227095 -3.3439391 -3.3030195 -3.2898185 -3.3044226 -3.3400497 -3.3911862 -3.457696 -3.5346556][-3.6901994 -3.7916651 -3.8655236 -3.8663523 -3.8114338 -3.7175698 -3.611923 -3.5218785 -3.4524779 -3.4019725 -3.3728678 -3.361871 -3.3611217 -3.3769631 -3.4162192]]...]
INFO - root - 2017-12-05 08:53:14.015460: step 4510, loss = 1.29, batch loss = 1.23 (39.0 examples/sec; 0.205 sec/batch; 18h:40m:24s remains)
INFO - root - 2017-12-05 08:53:16.089285: step 4520, loss = 1.44, batch loss = 1.38 (36.9 examples/sec; 0.217 sec/batch; 19h:46m:38s remains)
INFO - root - 2017-12-05 08:53:18.162324: step 4530, loss = 1.40, batch loss = 1.34 (38.7 examples/sec; 0.207 sec/batch; 18h:49m:07s remains)
INFO - root - 2017-12-05 08:53:20.254791: step 4540, loss = 1.38, batch loss = 1.32 (38.4 examples/sec; 0.208 sec/batch; 18h:58m:47s remains)
INFO - root - 2017-12-05 08:53:22.306822: step 4550, loss = 1.40, batch loss = 1.35 (39.5 examples/sec; 0.203 sec/batch; 18h:26m:50s remains)
INFO - root - 2017-12-05 08:53:24.375721: step 4560, loss = 1.36, batch loss = 1.30 (38.1 examples/sec; 0.210 sec/batch; 19h:07m:43s remains)
INFO - root - 2017-12-05 08:53:26.460449: step 4570, loss = 1.36, batch loss = 1.30 (37.7 examples/sec; 0.212 sec/batch; 19h:19m:01s remains)
INFO - root - 2017-12-05 08:53:28.563793: step 4580, loss = 1.49, batch loss = 1.43 (38.0 examples/sec; 0.211 sec/batch; 19h:10m:59s remains)
INFO - root - 2017-12-05 08:53:30.646213: step 4590, loss = 1.38, batch loss = 1.33 (38.0 examples/sec; 0.210 sec/batch; 19h:10m:00s remains)
INFO - root - 2017-12-05 08:53:32.734974: step 4600, loss = 1.57, batch loss = 1.51 (37.4 examples/sec; 0.214 sec/batch; 19h:28m:58s remains)
2017-12-05 08:53:33.000714: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.0866637 -4.0727644 -4.072372 -4.0733151 -4.0734477 -4.0732265 -4.073452 -4.0740852 -4.0684566 -4.0563593 -4.042984 -4.0357485 -4.0372791 -4.049365 -4.0764031][-3.9946458 -3.9723697 -3.9682965 -3.9673226 -3.9672017 -3.9683883 -3.9708633 -3.9747863 -3.9714174 -3.9609852 -3.9471879 -3.9397326 -3.9429822 -3.9615393 -3.9987659][-3.900876 -3.8688388 -3.8588881 -3.8545427 -3.8542821 -3.858757 -3.8670425 -3.8783448 -3.8847246 -3.8862302 -3.8837698 -3.8855178 -3.8974671 -3.925684 -3.9713278][-3.78618 -3.7408803 -3.7219253 -3.7131648 -3.7129865 -3.7217598 -3.7370198 -3.7591636 -3.7824421 -3.8053331 -3.8263242 -3.8492324 -3.879786 -3.9236593 -3.9794419][-3.6353512 -3.5772684 -3.5500832 -3.5388882 -3.5396194 -3.5502024 -3.5687447 -3.5981028 -3.6369123 -3.68328 -3.7346196 -3.790792 -3.8529015 -3.9212649 -3.9947224][-3.4640968 -3.3962097 -3.3645558 -3.3526042 -3.3519542 -3.3578424 -3.373507 -3.4036717 -3.4514408 -3.5154357 -3.5935612 -3.6826444 -3.7795789 -3.8790834 -3.9776404][-3.3559535 -3.2887058 -3.2594113 -3.24673 -3.2397566 -3.2340598 -3.2366977 -3.2563992 -3.2989857 -3.364434 -3.4536538 -3.561738 -3.6835184 -3.808552 -3.9298871][-3.3617098 -3.3101218 -3.2946908 -3.2893095 -3.2838402 -3.2739472 -3.2663655 -3.2698591 -3.2912467 -3.3352447 -3.4093981 -3.5111187 -3.633709 -3.7651396 -3.8955956][-3.4508421 -3.4175153 -3.4185162 -3.4251685 -3.429291 -3.4255447 -3.4187245 -3.413857 -3.4176247 -3.4387805 -3.4871128 -3.5628276 -3.6621413 -3.7766414 -3.8966198][-3.5785737 -3.5597935 -3.5702581 -3.5847359 -3.5969148 -3.5994122 -3.596 -3.589283 -3.584615 -3.5917029 -3.619837 -3.6702809 -3.7420306 -3.8304381 -3.9278593][-3.7065043 -3.6965377 -3.7128785 -3.7317154 -3.7491915 -3.7589056 -3.7610221 -3.7556934 -3.7463155 -3.7417154 -3.750082 -3.7751546 -3.8196168 -3.88147 -3.9560099][-3.8081489 -3.805717 -3.8254471 -3.846483 -3.8661304 -3.8797958 -3.885499 -3.8823037 -3.8681412 -3.8495641 -3.8369317 -3.8375187 -3.8585286 -3.8985372 -3.9554887][-3.8728995 -3.8756137 -3.8962884 -3.9178414 -3.938592 -3.9545953 -3.9617152 -3.958874 -3.9389925 -3.9056258 -3.8718307 -3.8503888 -3.8528056 -3.8787529 -3.9259865][-3.9251702 -3.9262567 -3.9422009 -3.9582918 -3.9750609 -3.9907315 -3.9986377 -3.9963267 -3.9734886 -3.9312754 -3.8834295 -3.8465619 -3.8358691 -3.8539381 -3.8967896][-3.9970336 -3.9917035 -3.9987283 -4.0064154 -4.0160117 -4.0279188 -4.0342646 -4.0314665 -4.0104575 -3.969928 -3.91883 -3.8750367 -3.8566387 -3.8686094 -3.9062304]]...]
INFO - root - 2017-12-05 08:53:35.079928: step 4610, loss = 1.21, batch loss = 1.15 (37.0 examples/sec; 0.216 sec/batch; 19h:39m:59s remains)
INFO - root - 2017-12-05 08:53:37.160528: step 4620, loss = 1.36, batch loss = 1.30 (39.3 examples/sec; 0.204 sec/batch; 18h:33m:42s remains)
INFO - root - 2017-12-05 08:53:39.238698: step 4630, loss = 1.27, batch loss = 1.22 (38.9 examples/sec; 0.206 sec/batch; 18h:43m:04s remains)
INFO - root - 2017-12-05 08:53:41.322568: step 4640, loss = 1.38, batch loss = 1.32 (38.7 examples/sec; 0.207 sec/batch; 18h:49m:08s remains)
INFO - root - 2017-12-05 08:53:43.416240: step 4650, loss = 1.23, batch loss = 1.18 (38.8 examples/sec; 0.206 sec/batch; 18h:47m:19s remains)
INFO - root - 2017-12-05 08:53:45.489663: step 4660, loss = 1.46, batch loss = 1.40 (36.5 examples/sec; 0.219 sec/batch; 19h:58m:57s remains)
INFO - root - 2017-12-05 08:53:47.596590: step 4670, loss = 1.34, batch loss = 1.28 (38.9 examples/sec; 0.206 sec/batch; 18h:43m:13s remains)
INFO - root - 2017-12-05 08:53:49.687849: step 4680, loss = 1.25, batch loss = 1.19 (37.9 examples/sec; 0.211 sec/batch; 19h:12m:08s remains)
INFO - root - 2017-12-05 08:53:51.772207: step 4690, loss = 1.32, batch loss = 1.26 (35.3 examples/sec; 0.227 sec/batch; 20h:37m:52s remains)
INFO - root - 2017-12-05 08:53:53.871016: step 4700, loss = 1.31, batch loss = 1.26 (38.9 examples/sec; 0.206 sec/batch; 18h:43m:16s remains)
2017-12-05 08:53:54.156455: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.363842 -4.3664646 -4.36775 -4.3674836 -4.3668456 -4.3668652 -4.3675508 -4.3693266 -4.3719425 -4.3733387 -4.3708291 -4.3622403 -4.34478 -4.3146782 -4.2736092][-4.3782396 -4.3809671 -4.3823929 -4.3818064 -4.3806081 -4.3784156 -4.3748741 -4.3706503 -4.3661256 -4.3602977 -4.3515468 -4.3362174 -4.3108382 -4.2735591 -4.23005][-4.379528 -4.38264 -4.3838506 -4.3816795 -4.3769841 -4.3692751 -4.3587718 -4.3464341 -4.33283 -4.318418 -4.3023853 -4.281559 -4.2518005 -4.2136726 -4.1756864][-4.3712149 -4.3722935 -4.37034 -4.3630686 -4.3500886 -4.33087 -4.3074718 -4.2825356 -4.2566128 -4.2326636 -4.2121964 -4.1915784 -4.1658359 -4.1358004 -4.1103349][-4.3502426 -4.3458247 -4.336102 -4.3170891 -4.2869964 -4.24807 -4.2081165 -4.1705885 -4.1349449 -4.1058211 -4.0869222 -4.0744715 -4.0625777 -4.0502868 -4.0434012][-4.309979 -4.2956252 -4.2725549 -4.23446 -4.1800528 -4.1181331 -4.0663109 -4.0270481 -3.994025 -3.9695282 -3.957202 -3.9579334 -3.9669394 -3.9806416 -3.9965856][-4.2539616 -4.2265387 -4.1882119 -4.1316237 -4.0545993 -3.9744563 -3.922617 -3.8972957 -3.8792403 -3.8639913 -3.8582282 -3.8693955 -3.8973808 -3.9385259 -3.9800675][-4.2143254 -4.1763735 -4.1278648 -4.0618391 -3.973866 -3.8874149 -3.8428574 -3.8338633 -3.8304715 -3.8235836 -3.8196673 -3.831059 -3.8675911 -3.9262533 -3.9857705][-4.2005916 -4.1639428 -4.1198182 -4.0633307 -3.9890163 -3.9161141 -3.8796012 -3.8747315 -3.874692 -3.8708234 -3.8641896 -3.864182 -3.8859828 -3.9356022 -3.9928703][-4.2099376 -4.1837468 -4.1534081 -4.1160111 -4.0669904 -4.0186076 -3.9923856 -3.9834912 -3.9758158 -3.9648333 -3.9501476 -3.9325533 -3.926599 -3.9455104 -3.9811456][-4.2348895 -4.2207785 -4.20449 -4.1843038 -4.1576223 -4.130168 -4.1130047 -4.1014986 -4.0862751 -4.0663214 -4.0402713 -4.0041509 -3.9688752 -3.9508069 -3.9555111][-4.2669435 -4.2622843 -4.2572765 -4.249321 -4.2353573 -4.2165542 -4.201591 -4.1881676 -4.1706405 -4.1471691 -4.11281 -4.0650334 -4.0103183 -3.9648278 -3.9407573][-4.3059535 -4.3051276 -4.3053689 -4.3013864 -4.2890124 -4.2689323 -4.2501512 -4.234549 -4.2186661 -4.1973081 -4.1619329 -4.1125751 -4.0555096 -4.0023589 -3.9626286][-4.3442769 -4.342165 -4.3399458 -4.33181 -4.3133907 -4.2861786 -4.260437 -4.2423468 -4.2296548 -4.2163792 -4.1901979 -4.1518788 -4.1074181 -4.0630178 -4.0232573][-4.364171 -4.359489 -4.3516097 -4.3357677 -4.3087974 -4.2742939 -4.2428951 -4.22293 -4.2133465 -4.21129 -4.2009058 -4.1808391 -4.1548853 -4.126111 -4.0966592]]...]
INFO - root - 2017-12-05 08:53:56.226311: step 4710, loss = 1.37, batch loss = 1.31 (39.0 examples/sec; 0.205 sec/batch; 18h:41m:51s remains)
INFO - root - 2017-12-05 08:53:58.305401: step 4720, loss = 1.27, batch loss = 1.22 (38.6 examples/sec; 0.207 sec/batch; 18h:50m:48s remains)
INFO - root - 2017-12-05 08:54:00.399510: step 4730, loss = 1.37, batch loss = 1.31 (35.7 examples/sec; 0.224 sec/batch; 20h:23m:53s remains)
INFO - root - 2017-12-05 08:54:02.473228: step 4740, loss = 1.33, batch loss = 1.27 (38.5 examples/sec; 0.208 sec/batch; 18h:54m:58s remains)
INFO - root - 2017-12-05 08:54:04.570786: step 4750, loss = 1.28, batch loss = 1.23 (37.8 examples/sec; 0.212 sec/batch; 19h:15m:54s remains)
INFO - root - 2017-12-05 08:54:06.689625: step 4760, loss = 1.51, batch loss = 1.45 (37.8 examples/sec; 0.212 sec/batch; 19h:16m:04s remains)
INFO - root - 2017-12-05 08:54:08.778650: step 4770, loss = 1.35, batch loss = 1.29 (36.1 examples/sec; 0.221 sec/batch; 20h:08m:49s remains)
INFO - root - 2017-12-05 08:54:10.874162: step 4780, loss = 1.44, batch loss = 1.39 (38.6 examples/sec; 0.207 sec/batch; 18h:52m:43s remains)
INFO - root - 2017-12-05 08:54:12.943112: step 4790, loss = 1.39, batch loss = 1.33 (38.2 examples/sec; 0.209 sec/batch; 19h:02m:34s remains)
INFO - root - 2017-12-05 08:54:15.023490: step 4800, loss = 1.22, batch loss = 1.16 (36.4 examples/sec; 0.220 sec/batch; 19h:58m:52s remains)
2017-12-05 08:54:15.315701: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1407585 -4.1678667 -4.2004023 -4.2181025 -4.2258191 -4.2296572 -4.2348509 -4.2469234 -4.2675333 -4.2939181 -4.3203487 -4.3403816 -4.3516846 -4.3568711 -4.3608251][-3.9482031 -3.9840925 -4.0285254 -4.0522814 -4.061964 -4.0664182 -4.07615 -4.1026645 -4.1479468 -4.2059674 -4.2642865 -4.3113747 -4.3415155 -4.3570905 -4.3655596][-3.7367263 -3.7741833 -3.8216646 -3.8428569 -3.8465586 -3.8437994 -3.8525968 -3.8915961 -3.9652045 -4.0635905 -4.1651926 -4.2493205 -4.3071895 -4.3405104 -4.3585644][-3.559757 -3.5908794 -3.6292858 -3.6348543 -3.6189296 -3.5949707 -3.5902958 -3.6326165 -3.7299614 -3.8686829 -4.01703 -4.1443152 -4.2372026 -4.2959166 -4.3298111][-3.4719377 -3.4870267 -3.5007095 -3.4724782 -3.4163013 -3.35223 -3.3166385 -3.3488975 -3.4607203 -3.6338263 -3.8232293 -3.9904876 -4.1191993 -4.2078314 -4.2636876][-3.4954226 -3.4876771 -3.4625969 -3.3854516 -3.2754631 -3.1588411 -3.0766528 -3.0775137 -3.1872058 -3.3834348 -3.6031408 -3.79928 -3.956177 -4.0714273 -4.1502237][-3.5957663 -3.5720031 -3.5108924 -3.3852096 -3.2219696 -3.0540175 -2.9229352 -2.8788939 -2.9661381 -3.1674514 -3.4014902 -3.6098783 -3.7787015 -3.9068937 -4.000021][-3.713351 -3.6914868 -3.6160514 -3.4619474 -3.263104 -3.060168 -2.8935876 -2.8125846 -2.8693376 -3.0506043 -3.2724581 -3.4740131 -3.6375668 -3.7622724 -3.8557529][-3.8048744 -3.7988377 -3.7327752 -3.5812461 -3.3779335 -3.1689014 -2.995717 -2.9017372 -2.9320951 -3.074286 -3.2597487 -3.4362187 -3.5796056 -3.6869993 -3.7656779][-3.8555055 -3.8657668 -3.825942 -3.7054453 -3.5315726 -3.3473487 -3.1961982 -3.1142545 -3.1290557 -3.2322888 -3.3758278 -3.5188665 -3.6330237 -3.7108405 -3.7597933][-3.8630314 -3.886529 -3.8801131 -3.8092947 -3.6874244 -3.5487883 -3.4366527 -3.3800862 -3.3945043 -3.4727857 -3.583 -3.694313 -3.7792647 -3.825808 -3.8375947][-3.8388586 -3.8687556 -3.8953958 -3.8772335 -3.8146996 -3.7318456 -3.6667945 -3.6429942 -3.6672995 -3.7323523 -3.8167946 -3.899225 -3.9597421 -3.9829936 -3.9660401][-3.7858832 -3.8143225 -3.8640449 -3.8884449 -3.8798573 -3.8535726 -3.837101 -3.8489377 -3.8904095 -3.9517069 -4.0184569 -4.0782614 -4.1201105 -4.1302986 -4.100975][-3.7159317 -3.7342525 -3.7935662 -3.8451886 -3.8807781 -3.905117 -3.934216 -3.9793177 -4.0372648 -4.0992064 -4.1561995 -4.2024789 -4.2329774 -4.2380881 -4.208765][-3.6434164 -3.6462169 -3.705255 -3.77259 -3.8378236 -3.9021776 -3.9694159 -4.0413971 -4.1126323 -4.1753664 -4.2270055 -4.2664356 -4.2927194 -4.2991195 -4.2781096]]...]
INFO - root - 2017-12-05 08:54:17.379269: step 4810, loss = 1.23, batch loss = 1.17 (38.7 examples/sec; 0.207 sec/batch; 18h:50m:23s remains)
INFO - root - 2017-12-05 08:54:19.458131: step 4820, loss = 1.26, batch loss = 1.20 (36.7 examples/sec; 0.218 sec/batch; 19h:51m:02s remains)
INFO - root - 2017-12-05 08:54:21.560166: step 4830, loss = 1.31, batch loss = 1.25 (38.1 examples/sec; 0.210 sec/batch; 19h:05m:49s remains)
INFO - root - 2017-12-05 08:54:23.658958: step 4840, loss = 1.32, batch loss = 1.26 (37.3 examples/sec; 0.214 sec/batch; 19h:30m:33s remains)
INFO - root - 2017-12-05 08:54:25.760942: step 4850, loss = 1.29, batch loss = 1.23 (38.3 examples/sec; 0.209 sec/batch; 18h:59m:49s remains)
INFO - root - 2017-12-05 08:54:27.881791: step 4860, loss = 1.38, batch loss = 1.33 (38.4 examples/sec; 0.208 sec/batch; 18h:57m:16s remains)
INFO - root - 2017-12-05 08:54:29.974579: step 4870, loss = 1.33, batch loss = 1.27 (39.5 examples/sec; 0.202 sec/batch; 18h:24m:40s remains)
INFO - root - 2017-12-05 08:54:32.044571: step 4880, loss = 1.28, batch loss = 1.22 (38.3 examples/sec; 0.209 sec/batch; 19h:01m:03s remains)
INFO - root - 2017-12-05 08:54:34.112246: step 4890, loss = 1.42, batch loss = 1.36 (38.4 examples/sec; 0.208 sec/batch; 18h:56m:47s remains)
INFO - root - 2017-12-05 08:54:36.192693: step 4900, loss = 1.37, batch loss = 1.31 (39.1 examples/sec; 0.204 sec/batch; 18h:36m:16s remains)
2017-12-05 08:54:36.494388: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2799363 -4.3049765 -4.3244543 -4.3285222 -4.3139219 -4.2913394 -4.2724533 -4.2632089 -4.2641845 -4.2726188 -4.2842674 -4.2963161 -4.3055596 -4.309094 -4.306694][-4.2692661 -4.2957897 -4.31602 -4.3205595 -4.3044839 -4.277667 -4.2556434 -4.248312 -4.2543659 -4.2664795 -4.2785325 -4.2893872 -4.2973781 -4.3009992 -4.2975249][-4.2576289 -4.2858315 -4.3071961 -4.3123922 -4.2949872 -4.2649236 -4.2399769 -4.2333751 -4.2452526 -4.2633505 -4.2784581 -4.2893634 -4.2963572 -4.2999597 -4.297574][-4.2425532 -4.2726984 -4.296772 -4.3026476 -4.2848029 -4.251925 -4.2232418 -4.2150826 -4.2312055 -4.2550955 -4.2747254 -4.2892475 -4.29734 -4.300889 -4.2986507][-4.233047 -4.2629333 -4.2891912 -4.2953286 -4.2764459 -4.2395945 -4.203434 -4.1902585 -4.2098436 -4.2394047 -4.2652907 -4.2853985 -4.2959328 -4.2980375 -4.2922683][-4.2372 -4.2645926 -4.2884974 -4.29314 -4.2720652 -4.2298484 -4.1846876 -4.1640244 -4.1859384 -4.2226362 -4.2556553 -4.2810817 -4.2941227 -4.2943673 -4.2840524][-4.2533283 -4.2742424 -4.2908015 -4.2910905 -4.2676005 -4.2229934 -4.1750789 -4.152277 -4.1750236 -4.2155328 -4.2532997 -4.2829981 -4.2984862 -4.298512 -4.2857323][-4.27151 -4.2823586 -4.2880058 -4.2802362 -4.2541151 -4.2128444 -4.1727386 -4.1566286 -4.1785164 -4.2186975 -4.2589841 -4.2915578 -4.3093095 -4.3120608 -4.3001766][-4.283113 -4.28341 -4.2784433 -4.2620792 -4.2343926 -4.1994414 -4.1713467 -4.1645212 -4.1876426 -4.2270689 -4.2682691 -4.3022308 -4.3215156 -4.3276691 -4.3185253][-4.2888927 -4.2824097 -4.2704325 -4.2488642 -4.22086 -4.1934562 -4.1770897 -4.1782 -4.2026248 -4.2391281 -4.27772 -4.3110771 -4.3302026 -4.337615 -4.332325][-4.2965465 -4.2877951 -4.2732863 -4.2514887 -4.2257147 -4.2048306 -4.1950779 -4.201221 -4.225553 -4.2584162 -4.291626 -4.3205428 -4.3367262 -4.3441796 -4.3432794][-4.3103004 -4.3010683 -4.2871079 -4.2679691 -4.2467208 -4.2316589 -4.2261214 -4.2344956 -4.2572336 -4.2845578 -4.3103685 -4.33195 -4.3444433 -4.3509974 -4.3523269][-4.3270168 -4.317389 -4.3051825 -4.2898889 -4.2737718 -4.2642407 -4.2624464 -4.2712507 -4.2898455 -4.3109612 -4.329977 -4.3446016 -4.3535414 -4.3588643 -4.3609543][-4.3409796 -4.3321719 -4.3224664 -4.3113251 -4.3001256 -4.2952151 -4.2960114 -4.3019929 -4.3149137 -4.3299146 -4.3428235 -4.352582 -4.3596077 -4.3647151 -4.3670597][-4.3481779 -4.3419409 -4.3356524 -4.328917 -4.3220468 -4.3191543 -4.3190951 -4.3214006 -4.3289785 -4.3381896 -4.3457408 -4.3522668 -4.3577824 -4.3617196 -4.3644228]]...]
INFO - root - 2017-12-05 08:54:38.564644: step 4910, loss = 1.25, batch loss = 1.19 (39.0 examples/sec; 0.205 sec/batch; 18h:39m:45s remains)
INFO - root - 2017-12-05 08:54:40.651681: step 4920, loss = 1.16, batch loss = 1.10 (38.8 examples/sec; 0.206 sec/batch; 18h:46m:12s remains)
INFO - root - 2017-12-05 08:54:42.755847: step 4930, loss = 1.27, batch loss = 1.21 (35.4 examples/sec; 0.226 sec/batch; 20h:33m:33s remains)
INFO - root - 2017-12-05 08:54:44.863069: step 4940, loss = 1.50, batch loss = 1.44 (38.5 examples/sec; 0.208 sec/batch; 18h:53m:48s remains)
INFO - root - 2017-12-05 08:54:46.964288: step 4950, loss = 1.12, batch loss = 1.06 (37.9 examples/sec; 0.211 sec/batch; 19h:13m:12s remains)
INFO - root - 2017-12-05 08:54:49.037785: step 4960, loss = 1.31, batch loss = 1.25 (39.1 examples/sec; 0.205 sec/batch; 18h:38m:16s remains)
INFO - root - 2017-12-05 08:54:51.116038: step 4970, loss = 1.26, batch loss = 1.20 (38.8 examples/sec; 0.206 sec/batch; 18h:46m:13s remains)
INFO - root - 2017-12-05 08:54:53.192424: step 4980, loss = 1.38, batch loss = 1.32 (39.3 examples/sec; 0.203 sec/batch; 18h:30m:32s remains)
INFO - root - 2017-12-05 08:54:55.246962: step 4990, loss = 1.33, batch loss = 1.27 (39.5 examples/sec; 0.203 sec/batch; 18h:25m:50s remains)
INFO - root - 2017-12-05 08:54:57.324169: step 5000, loss = 1.32, batch loss = 1.27 (38.0 examples/sec; 0.211 sec/batch; 19h:09m:10s remains)
2017-12-05 08:54:57.613589: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.162672 -4.1741734 -4.200387 -4.2352581 -4.267704 -4.291369 -4.2995634 -4.288826 -4.2666826 -4.241323 -4.2196722 -4.205617 -4.1963263 -4.191916 -4.2014928][-4.1652341 -4.1896124 -4.2207832 -4.2523055 -4.2756672 -4.2861829 -4.2782578 -4.2501411 -4.2149024 -4.1826186 -4.1600404 -4.1486244 -4.1444197 -4.1448565 -4.1587734][-4.2157979 -4.2433443 -4.2697124 -4.2903771 -4.300014 -4.2957067 -4.272943 -4.2319622 -4.1878986 -4.1531024 -4.1345038 -4.129396 -4.1323013 -4.1364546 -4.1483445][-4.2779932 -4.300148 -4.3179646 -4.3276916 -4.3252678 -4.3079758 -4.2746773 -4.2277322 -4.1816087 -4.1492648 -4.135613 -4.1347213 -4.1399693 -4.14384 -4.1504674][-4.3285961 -4.3420906 -4.3499322 -4.347887 -4.3329792 -4.3030987 -4.2606111 -4.2122006 -4.1703935 -4.14468 -4.1374741 -4.1405306 -4.1479468 -4.1539488 -4.1601396][-4.3586774 -4.3621135 -4.3579226 -4.3411508 -4.310297 -4.2659674 -4.2159677 -4.1730747 -4.1425557 -4.1265745 -4.1253314 -4.13346 -4.1475091 -4.1617212 -4.17441][-4.3692522 -4.3617024 -4.3426476 -4.3071718 -4.2565708 -4.19655 -4.1434326 -4.1125197 -4.0991483 -4.0986056 -4.1088533 -4.1273232 -4.1519761 -4.1777382 -4.1974688][-4.3632212 -4.3457327 -4.3112249 -4.2579589 -4.1925869 -4.1268139 -4.0816889 -4.0717454 -4.082972 -4.102427 -4.1254969 -4.1507463 -4.1776347 -4.2032776 -4.2211862][-4.3484144 -4.3259945 -4.2866673 -4.2318711 -4.170929 -4.1168547 -4.0872269 -4.0935826 -4.1182542 -4.1444435 -4.1676145 -4.1876445 -4.2048392 -4.2208867 -4.2304759][-4.3381939 -4.3201962 -4.2912588 -4.2511859 -4.2061005 -4.1674204 -4.1472464 -4.1537004 -4.1728425 -4.190505 -4.2041664 -4.21218 -4.21681 -4.2215624 -4.2231259][-4.3295074 -4.3216057 -4.3074636 -4.2841024 -4.25424 -4.2267327 -4.20946 -4.2084479 -4.2146635 -4.2199974 -4.2224884 -4.2188168 -4.21301 -4.2102675 -4.2077241][-4.3133779 -4.3169961 -4.3157225 -4.3044868 -4.2849545 -4.2637057 -4.2464986 -4.2362037 -4.2293692 -4.2245011 -4.2190452 -4.2082949 -4.197854 -4.1945605 -4.1944275][-4.293004 -4.306438 -4.314115 -4.3098178 -4.294384 -4.2735577 -4.254354 -4.2374773 -4.222703 -4.2131166 -4.205327 -4.1947813 -4.1869426 -4.1880984 -4.1931505][-4.2798719 -4.29827 -4.3090839 -4.305727 -4.2896733 -4.2678089 -4.2481694 -4.2310691 -4.2162089 -4.2083254 -4.2031302 -4.1958046 -4.1920133 -4.1966219 -4.2042084][-4.2916055 -4.3063273 -4.31377 -4.3081446 -4.2919731 -4.2722569 -4.256218 -4.2433476 -4.2326689 -4.2290096 -4.2274375 -4.2237 -4.2221494 -4.2250638 -4.2285442]]...]
INFO - root - 2017-12-05 08:54:59.669516: step 5010, loss = 1.26, batch loss = 1.20 (38.1 examples/sec; 0.210 sec/batch; 19h:06m:18s remains)
INFO - root - 2017-12-05 08:55:01.725366: step 5020, loss = 1.26, batch loss = 1.20 (38.9 examples/sec; 0.206 sec/batch; 18h:42m:39s remains)
INFO - root - 2017-12-05 08:55:03.815053: step 5030, loss = 1.30, batch loss = 1.24 (36.7 examples/sec; 0.218 sec/batch; 19h:48m:14s remains)
INFO - root - 2017-12-05 08:55:05.889630: step 5040, loss = 1.28, batch loss = 1.22 (38.5 examples/sec; 0.208 sec/batch; 18h:52m:54s remains)
INFO - root - 2017-12-05 08:55:07.982879: step 5050, loss = 1.42, batch loss = 1.36 (39.2 examples/sec; 0.204 sec/batch; 18h:33m:53s remains)
INFO - root - 2017-12-05 08:55:10.111263: step 5060, loss = 1.29, batch loss = 1.23 (38.2 examples/sec; 0.209 sec/batch; 19h:02m:58s remains)
INFO - root - 2017-12-05 08:55:12.154815: step 5070, loss = 1.42, batch loss = 1.36 (39.6 examples/sec; 0.202 sec/batch; 18h:22m:54s remains)
INFO - root - 2017-12-05 08:55:14.273341: step 5080, loss = 1.18, batch loss = 1.12 (37.7 examples/sec; 0.212 sec/batch; 19h:17m:54s remains)
INFO - root - 2017-12-05 08:55:16.336845: step 5090, loss = 1.30, batch loss = 1.24 (38.2 examples/sec; 0.210 sec/batch; 19h:03m:15s remains)
INFO - root - 2017-12-05 08:55:18.426922: step 5100, loss = 1.36, batch loss = 1.30 (36.9 examples/sec; 0.217 sec/batch; 19h:43m:51s remains)
2017-12-05 08:55:18.710592: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.4406066 -2.3635814 -2.3748722 -2.3834016 -2.3863974 -2.4048998 -2.4477975 -2.5058799 -2.5795958 -2.6761746 -2.7893116 -2.9085133 -3.0090976 -3.0813007 -3.142571][-1.8254335 -1.7318468 -1.7539933 -1.7745607 -1.7788453 -1.7961116 -1.8427212 -1.909946 -1.9938676 -2.1040814 -2.23252 -2.3709407 -2.4908376 -2.5796471 -2.6550694][-1.3828394 -1.2824683 -1.3099148 -1.3405006 -1.341573 -1.3457336 -1.3763814 -1.4338181 -1.5123146 -1.6227808 -1.7608256 -1.9151974 -2.0536757 -2.1615977 -2.2560978][-1.2640157 -1.1550732 -1.1688061 -1.1851163 -1.1654739 -1.1377366 -1.1252604 -1.1439352 -1.1976616 -1.2972085 -1.4407036 -1.6061602 -1.7613873 -1.895123 -2.0225992][-1.3992 -1.2802582 -1.2608643 -1.2402887 -1.1875904 -1.1229215 -1.0637481 -1.034178 -1.0537698 -1.1342564 -1.2696593 -1.4333398 -1.6018424 -1.7663686 -1.9393065][-1.6105421 -1.4738865 -1.4127092 -1.3437581 -1.2519834 -1.1513746 -1.0528181 -0.98314786 -0.97573662 -1.041934 -1.1696086 -1.3341196 -1.5208328 -1.7214146 -1.9450316][-1.7700493 -1.6118598 -1.5043395 -1.378293 -1.236444 -1.0934834 -0.95451665 -0.85168648 -0.83336473 -0.91155028 -1.0581372 -1.247113 -1.4692202 -1.7154987 -1.9946766][-1.8450825 -1.6632059 -1.5135195 -1.3367968 -1.1491954 -0.96692419 -0.78787565 -0.65320945 -0.634398 -0.74759626 -0.93961835 -1.1725943 -1.4396343 -1.7328627 -2.0579557][-1.893549 -1.6855795 -1.5040441 -1.3009942 -1.0971532 -0.90464926 -0.71973205 -0.58672 -0.57979989 -0.7187686 -0.93638182 -1.1889701 -1.4699781 -1.7719922 -2.1006794][-1.9905472 -1.7819052 -1.5956774 -1.3943679 -1.2077422 -1.0436106 -0.89773512 -0.80227304 -0.81127071 -0.93785739 -1.1284647 -1.3509996 -1.5968554 -1.857439 -2.1428282][-2.2036734 -2.0195267 -1.8552473 -1.6791811 -1.5247715 -1.3990831 -1.2990777 -1.2396939 -1.2510622 -1.3426006 -1.4818184 -1.6466966 -1.8245633 -2.0108209 -2.2255967][-2.5460892 -2.4070802 -2.2819202 -2.1433468 -2.0242038 -1.9333456 -1.8712423 -1.840399 -1.8491333 -1.9022593 -1.9834924 -2.0810118 -2.180321 -2.2854085 -2.4202316][-2.9760208 -2.8923223 -2.8171942 -2.7266326 -2.646596 -2.5873246 -2.5504234 -2.5317411 -2.5311286 -2.5465322 -2.5745769 -2.6091559 -2.6390944 -2.6732607 -2.7341723][-3.4341331 -3.399991 -3.3692522 -3.3217382 -3.2750518 -3.2391663 -3.2144325 -3.1955988 -3.1797376 -3.1645732 -3.1524389 -3.1438649 -3.1302378 -3.1193728 -3.1263461][-3.82553 -3.8256269 -3.823009 -3.8044014 -3.7805643 -3.7599015 -3.7419851 -3.7222772 -3.6993427 -3.6711891 -3.6410184 -3.6137903 -3.5861197 -3.5601504 -3.543725]]...]
INFO - root - 2017-12-05 08:55:20.789443: step 5110, loss = 1.28, batch loss = 1.22 (37.6 examples/sec; 0.213 sec/batch; 19h:20m:18s remains)
INFO - root - 2017-12-05 08:55:22.881442: step 5120, loss = 1.22, batch loss = 1.17 (39.1 examples/sec; 0.204 sec/batch; 18h:34m:59s remains)
INFO - root - 2017-12-05 08:55:24.961381: step 5130, loss = 1.32, batch loss = 1.26 (39.2 examples/sec; 0.204 sec/batch; 18h:33m:00s remains)
INFO - root - 2017-12-05 08:55:27.018918: step 5140, loss = 1.40, batch loss = 1.34 (37.3 examples/sec; 0.215 sec/batch; 19h:30m:30s remains)
INFO - root - 2017-12-05 08:55:29.113896: step 5150, loss = 1.18, batch loss = 1.12 (37.1 examples/sec; 0.216 sec/batch; 19h:37m:16s remains)
INFO - root - 2017-12-05 08:55:31.174852: step 5160, loss = 1.29, batch loss = 1.24 (39.3 examples/sec; 0.203 sec/batch; 18h:29m:56s remains)
INFO - root - 2017-12-05 08:55:33.271730: step 5170, loss = 1.16, batch loss = 1.10 (39.6 examples/sec; 0.202 sec/batch; 18h:22m:56s remains)
INFO - root - 2017-12-05 08:55:35.348132: step 5180, loss = 1.23, batch loss = 1.17 (39.0 examples/sec; 0.205 sec/batch; 18h:38m:34s remains)
INFO - root - 2017-12-05 08:55:37.441578: step 5190, loss = 1.18, batch loss = 1.12 (37.1 examples/sec; 0.215 sec/batch; 19h:35m:16s remains)
INFO - root - 2017-12-05 08:55:39.530682: step 5200, loss = 1.27, batch loss = 1.21 (36.9 examples/sec; 0.217 sec/batch; 19h:42m:25s remains)
2017-12-05 08:55:39.791848: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3578186 -4.3591118 -4.3601623 -4.3603921 -4.3603492 -4.3606763 -4.3612609 -4.3620462 -4.3626757 -4.3631415 -4.3632712 -4.3634014 -4.3637381 -4.3641205 -4.3642836][-4.3274164 -4.3365111 -4.3447838 -4.3477817 -4.3471007 -4.3460245 -4.3470845 -4.3504515 -4.3544497 -4.3577118 -4.3595867 -4.3603482 -4.3607574 -4.3610449 -4.3612614][-4.2514453 -4.2765718 -4.3004503 -4.3108544 -4.3092275 -4.3036919 -4.3025088 -4.31004 -4.3243566 -4.33965 -4.3510418 -4.3571296 -4.3596277 -4.3603849 -4.3605332][-4.1191072 -4.1674538 -4.2146311 -4.2357016 -4.2297854 -4.2125654 -4.2036338 -4.21497 -4.2456617 -4.2859306 -4.3215 -4.3442607 -4.355278 -4.3593783 -4.3603191][-3.9504251 -4.0247335 -4.09759 -4.12946 -4.1144547 -4.074553 -4.04631 -4.0573421 -4.1080241 -4.1826248 -4.2574105 -4.3117542 -4.3416147 -4.3546786 -4.3589454][-3.8103437 -3.8997667 -3.9918385 -4.0320807 -4.00603 -3.9358792 -3.871856 -3.8654597 -3.9302595 -4.040246 -4.1595993 -4.255929 -4.3151507 -4.3436174 -4.3547444][-3.7893782 -3.8633738 -3.9513316 -3.9901857 -3.9572134 -3.8669229 -3.7655449 -3.7214353 -3.7754645 -3.9037127 -4.0572414 -4.1904378 -4.2805681 -4.3281245 -4.3480697][-3.8937752 -3.9377465 -3.9960639 -4.0174518 -3.9798002 -3.889837 -3.7810483 -3.7116783 -3.7319922 -3.8414557 -3.9967961 -4.1439848 -4.251596 -4.3136768 -4.3416686][-4.0470734 -4.0672474 -4.0911479 -4.0867581 -4.0414128 -3.9622519 -3.8722806 -3.8094797 -3.8115168 -3.8861115 -4.01037 -4.1405334 -4.2439413 -4.3078604 -4.3387346][-4.183579 -4.1874828 -4.1845627 -4.1583734 -4.1050053 -4.03471 -3.9660082 -3.9201331 -3.9195726 -3.9696074 -4.0585461 -4.1596808 -4.2477856 -4.30714 -4.3379216][-4.2760916 -4.2696118 -4.2499366 -4.2097216 -4.1524644 -4.0881124 -4.0288429 -3.9866204 -3.9749806 -4.0010767 -4.0616941 -4.1425176 -4.2257237 -4.2903624 -4.3291278][-4.3235016 -4.3104129 -4.2830014 -4.2395473 -4.1848855 -4.1244612 -4.0640821 -4.007987 -3.9667978 -3.9552927 -3.9857795 -4.0554075 -4.148005 -4.2362609 -4.29912][-4.3384771 -4.322722 -4.2957797 -4.2577248 -4.211184 -4.1579819 -4.0942488 -4.0153608 -3.931061 -3.8672776 -3.8567603 -3.913784 -4.0206876 -4.1404161 -4.2407055][-4.3384933 -4.3230944 -4.3021164 -4.27535 -4.2423763 -4.2007804 -4.1405926 -4.0474734 -3.9278495 -3.8131928 -3.7519975 -3.7806845 -3.8899088 -4.0336056 -4.167696][-4.3387251 -4.3253336 -4.3111658 -4.2955189 -4.2773542 -4.2497759 -4.1993341 -4.1096878 -3.9847164 -3.8514664 -3.7553461 -3.7428477 -3.8253329 -3.9654191 -4.1124287]]...]
INFO - root - 2017-12-05 08:55:41.865762: step 5210, loss = 1.37, batch loss = 1.32 (37.1 examples/sec; 0.215 sec/batch; 19h:35m:15s remains)
INFO - root - 2017-12-05 08:55:43.989867: step 5220, loss = 1.26, batch loss = 1.20 (37.3 examples/sec; 0.214 sec/batch; 19h:28m:40s remains)
INFO - root - 2017-12-05 08:55:46.090896: step 5230, loss = 1.20, batch loss = 1.15 (37.9 examples/sec; 0.211 sec/batch; 19h:12m:05s remains)
INFO - root - 2017-12-05 08:55:48.202249: step 5240, loss = 1.46, batch loss = 1.40 (39.2 examples/sec; 0.204 sec/batch; 18h:32m:36s remains)
INFO - root - 2017-12-05 08:55:50.283123: step 5250, loss = 1.27, batch loss = 1.22 (39.4 examples/sec; 0.203 sec/batch; 18h:28m:47s remains)
INFO - root - 2017-12-05 08:55:52.368955: step 5260, loss = 1.37, batch loss = 1.31 (38.6 examples/sec; 0.207 sec/batch; 18h:49m:01s remains)
INFO - root - 2017-12-05 08:55:54.452260: step 5270, loss = 1.23, batch loss = 1.17 (37.3 examples/sec; 0.215 sec/batch; 19h:31m:10s remains)
INFO - root - 2017-12-05 08:55:56.533452: step 5280, loss = 1.09, batch loss = 1.04 (39.6 examples/sec; 0.202 sec/batch; 18h:21m:12s remains)
INFO - root - 2017-12-05 08:55:58.621385: step 5290, loss = 1.22, batch loss = 1.16 (39.1 examples/sec; 0.205 sec/batch; 18h:37m:09s remains)
INFO - root - 2017-12-05 08:56:00.691349: step 5300, loss = 1.30, batch loss = 1.24 (39.2 examples/sec; 0.204 sec/batch; 18h:32m:35s remains)
2017-12-05 08:56:00.969773: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.8555965 -3.8290443 -3.8187492 -3.8017316 -3.7725663 -3.733851 -3.7017853 -3.6808774 -3.6806388 -3.696434 -3.7352772 -3.7962499 -3.8744812 -3.9657624 -4.0587373][-3.4421468 -3.3914509 -3.3682837 -3.3384066 -3.2920547 -3.2330184 -3.1862717 -3.1585112 -3.1635964 -3.1970892 -3.2688069 -3.3771052 -3.5136454 -3.6714685 -3.8324969][-2.8847415 -2.8023694 -2.7616482 -2.7159805 -2.6503313 -2.5698903 -2.5091662 -2.4774132 -2.4925952 -2.5523913 -2.6676807 -2.8361187 -3.0469508 -3.2918143 -3.5430183][-2.2396297 -2.1217368 -2.0616219 -1.9980371 -1.9161766 -1.8200357 -1.749666 -1.7150729 -1.7417645 -1.8350005 -2.0012074 -2.2338223 -2.5244184 -2.8648844 -3.2188053][-1.6053736 -1.4548793 -1.3733823 -1.29333 -1.203265 -1.1018777 -1.0277719 -0.9956286 -1.0399883 -1.169142 -1.3823831 -1.6728177 -2.037564 -2.4658165 -2.9149616][-1.1136079 -0.93234038 -0.82410836 -0.72528267 -0.62942433 -0.52735615 -0.45362449 -0.42829776 -0.49297094 -0.65208793 -0.90115166 -1.2380805 -1.6651022 -2.1665215 -2.6875365][-0.8590076 -0.6546855 -0.52386856 -0.4151659 -0.32052994 -0.22207689 -0.14326954 -0.11306572 -0.18074703 -0.34795618 -0.61416268 -0.97994542 -1.4476166 -1.994169 -2.557173][-0.83436275 -0.62486482 -0.49705267 -0.40402889 -0.33459091 -0.25056171 -0.16262007 -0.10458946 -0.14017677 -0.27917576 -0.53639507 -0.90550303 -1.3854306 -1.9477975 -2.5265446][-0.98713326 -0.79239154 -0.69401145 -0.64139056 -0.6178782 -0.563298 -0.47720289 -0.39749813 -0.39112043 -0.4767592 -0.69252205 -1.0292177 -1.4851906 -2.0278275 -2.5922861][-1.2752187 -1.1128728 -1.0537591 -1.044708 -1.059648 -1.0353012 -0.96963263 -0.892328 -0.86005735 -0.89889526 -1.0656483 -1.3509009 -1.7545364 -2.245223 -2.7613292][-1.6463931 -1.5274765 -1.5093408 -1.5346682 -1.5712526 -1.5685263 -1.5273988 -1.4684618 -1.4321685 -1.4477665 -1.5769799 -1.8090968 -2.1456764 -2.5648031 -3.0099859][-2.0648847 -1.9871016 -2.0008311 -2.046308 -2.0910769 -2.1007006 -2.0828006 -2.0482638 -2.0219862 -2.037375 -2.1462376 -2.3356562 -2.6038625 -2.9398422 -3.2999902][-2.5165854 -2.4653146 -2.4929616 -2.5432105 -2.5854189 -2.6022804 -2.6010947 -2.5902624 -2.5817313 -2.605123 -2.6987052 -2.8490057 -3.0546005 -3.3102326 -3.5852089][-2.9681549 -2.9354539 -2.9639745 -3.008096 -3.0435858 -3.0620351 -3.0716836 -3.0782661 -3.0859356 -3.1162748 -3.1928654 -3.3078609 -3.4590778 -3.6421733 -3.8374276][-3.4092689 -3.3916621 -3.4173241 -3.4530339 -3.481277 -3.4984288 -3.511471 -3.5247841 -3.5401604 -3.5697069 -3.6269069 -3.7075899 -3.8096032 -3.9279513 -4.0494285]]...]
INFO - root - 2017-12-05 08:56:03.055457: step 5310, loss = 1.39, batch loss = 1.33 (38.9 examples/sec; 0.206 sec/batch; 18h:40m:41s remains)
INFO - root - 2017-12-05 08:56:05.128424: step 5320, loss = 1.28, batch loss = 1.22 (37.8 examples/sec; 0.212 sec/batch; 19h:14m:42s remains)
INFO - root - 2017-12-05 08:56:07.253668: step 5330, loss = 1.33, batch loss = 1.27 (38.4 examples/sec; 0.208 sec/batch; 18h:54m:54s remains)
INFO - root - 2017-12-05 08:56:09.305959: step 5340, loss = 1.34, batch loss = 1.28 (38.6 examples/sec; 0.207 sec/batch; 18h:49m:13s remains)
INFO - root - 2017-12-05 08:56:11.432394: step 5350, loss = 1.22, batch loss = 1.16 (38.7 examples/sec; 0.207 sec/batch; 18h:47m:13s remains)
INFO - root - 2017-12-05 08:56:13.528417: step 5360, loss = 1.37, batch loss = 1.31 (39.1 examples/sec; 0.205 sec/batch; 18h:35m:51s remains)
INFO - root - 2017-12-05 08:56:15.593339: step 5370, loss = 1.16, batch loss = 1.10 (36.2 examples/sec; 0.221 sec/batch; 20h:03m:31s remains)
INFO - root - 2017-12-05 08:56:17.683763: step 5380, loss = 1.29, batch loss = 1.23 (39.0 examples/sec; 0.205 sec/batch; 18h:39m:38s remains)
INFO - root - 2017-12-05 08:56:19.768212: step 5390, loss = 1.15, batch loss = 1.09 (37.8 examples/sec; 0.212 sec/batch; 19h:14m:05s remains)
INFO - root - 2017-12-05 08:56:21.875650: step 5400, loss = 1.25, batch loss = 1.19 (38.0 examples/sec; 0.211 sec/batch; 19h:09m:01s remains)
2017-12-05 08:56:22.177951: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.8357859 -3.5531437 -3.2547293 -2.9645903 -2.699826 -2.488224 -2.3487637 -2.2403407 -2.1355398 -2.0313385 -1.9485698 -1.8926694 -1.8247867 -1.6848438 -1.4944427][-3.7013741 -3.3403883 -2.9546094 -2.5814018 -2.2469962 -1.9847348 -1.8222852 -1.7063835 -1.5896327 -1.4624448 -1.3607941 -1.2922275 -1.2094414 -1.044209 -0.83296847][-3.5733578 -3.1373456 -2.6662312 -2.2074671 -1.7972724 -1.482064 -1.2998219 -1.1868961 -1.0724332 -0.94616055 -0.84642196 -0.77757025 -0.69085217 -0.51872849 -0.30711222][-3.4693642 -2.9720273 -2.4305534 -1.89575 -1.4116457 -1.0358565 -0.82159376 -0.70559764 -0.60247207 -0.4958837 -0.42134333 -0.37057209 -0.29458046 -0.13842583 0.04995203][-3.393857 -2.8493681 -2.2505696 -1.647671 -1.0924296 -0.64782476 -0.38222647 -0.24391985 -0.13889265 -0.054920197 -0.027625561 -0.022527218 0.016815662 0.13389444 0.28398895][-3.325038 -2.7282429 -2.0629561 -1.3835835 -0.751503 -0.23368168 0.088485241 0.25889063 0.36476851 0.41015625 0.36277914 0.28299904 0.24951935 0.30402327 0.40395689][-3.2399359 -2.5779037 -1.8342433 -1.0694473 -0.35953665 0.2121377 0.56673431 0.73980284 0.81617785 0.78809452 0.6383009 0.45551062 0.33030558 0.30913115 0.35227156][-3.1516628 -2.4254885 -1.6071265 -0.77499843 -0.025482655 0.548007 0.88110924 0.99751472 0.98537016 0.84750319 0.59606361 0.3284936 0.13088846 0.051386833 0.058739185][-3.0919349 -2.3379352 -1.4936104 -0.66397309 0.045305729 0.55755711 0.81849194 0.84221125 0.72332525 0.49177694 0.17985249 -0.12889147 -0.3572216 -0.45797825 -0.44741988][-3.0956397 -2.3794491 -1.5916495 -0.84418082 -0.22964621 0.19254112 0.37523365 0.33015156 0.15013027 -0.12255287 -0.44801116 -0.75760961 -0.98036003 -1.0645783 -1.0161142][-3.1723542 -2.540853 -1.8589644 -1.2264595 -0.72415948 -0.39698792 -0.28026962 -0.35581112 -0.550056 -0.82341647 -1.1319315 -1.4105241 -1.5939791 -1.6264739 -1.5135908][-3.3051221 -2.7755313 -2.2112916 -1.6939404 -1.2934167 -1.0460918 -0.9752996 -1.0575333 -1.2398584 -1.4914916 -1.7637472 -1.988359 -2.1037843 -2.0538545 -1.8589821][-3.4627495 -3.0219212 -2.5563586 -2.1331272 -1.8096485 -1.6155667 -1.5708489 -1.654465 -1.8202491 -2.0346978 -2.2432749 -2.3796277 -2.3939557 -2.2372012 -1.9493818][-3.6103225 -3.2306936 -2.8297596 -2.4672918 -2.1935489 -2.0349581 -2.0130889 -2.1004386 -2.2457244 -2.4051046 -2.5191607 -2.5304933 -2.4098818 -2.1312017 -1.7567418][-3.7158055 -3.366493 -3.0070419 -2.6870153 -2.4529891 -2.3332324 -2.3398139 -2.4346504 -2.5557299 -2.6427994 -2.6340351 -2.4906602 -2.2202241 -1.8333111 -1.4053214]]...]
INFO - root - 2017-12-05 08:56:24.252884: step 5410, loss = 1.20, batch loss = 1.14 (38.2 examples/sec; 0.210 sec/batch; 19h:02m:59s remains)
INFO - root - 2017-12-05 08:56:26.343298: step 5420, loss = 1.26, batch loss = 1.20 (37.5 examples/sec; 0.213 sec/batch; 19h:21m:40s remains)
INFO - root - 2017-12-05 08:56:28.426139: step 5430, loss = 1.17, batch loss = 1.12 (38.9 examples/sec; 0.206 sec/batch; 18h:40m:32s remains)
INFO - root - 2017-12-05 08:56:30.529057: step 5440, loss = 1.37, batch loss = 1.31 (37.9 examples/sec; 0.211 sec/batch; 19h:10m:05s remains)
INFO - root - 2017-12-05 08:56:32.621874: step 5450, loss = 1.09, batch loss = 1.03 (39.7 examples/sec; 0.202 sec/batch; 18h:19m:41s remains)
INFO - root - 2017-12-05 08:56:34.683930: step 5460, loss = 1.20, batch loss = 1.14 (38.9 examples/sec; 0.206 sec/batch; 18h:40m:19s remains)
INFO - root - 2017-12-05 08:56:36.788031: step 5470, loss = 1.17, batch loss = 1.11 (38.7 examples/sec; 0.207 sec/batch; 18h:46m:08s remains)
INFO - root - 2017-12-05 08:56:38.887245: step 5480, loss = 1.45, batch loss = 1.39 (37.1 examples/sec; 0.216 sec/batch; 19h:36m:13s remains)
INFO - root - 2017-12-05 08:56:40.980586: step 5490, loss = 1.41, batch loss = 1.35 (38.0 examples/sec; 0.211 sec/batch; 19h:08m:43s remains)
INFO - root - 2017-12-05 08:56:43.077045: step 5500, loss = 1.12, batch loss = 1.06 (37.0 examples/sec; 0.216 sec/batch; 19h:37m:09s remains)
2017-12-05 08:56:43.403793: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.40173864 -0.10210133 -0.12638664 -0.38771152 -0.77374053 -1.1597581 -1.432935 -1.5446579 -1.5164888 -1.3622386 -1.0809369 -0.70377064 -0.29599905 0.029694557 0.12372923][-0.2692337 0.10954857 0.13162661 -0.11052275 -0.49306417 -0.87623858 -1.1428928 -1.2534239 -1.2362385 -1.1167238 -0.90616751 -0.62830639 -0.33166981 -0.10837173 -0.098938465][-0.26522684 0.20228052 0.29754496 0.1058321 -0.24867249 -0.6151824 -0.87614465 -0.99489784 -1.0058839 -0.94104815 -0.82128119 -0.65890813 -0.48765039 -0.38528538 -0.47460866][-0.26236439 0.29339361 0.47937012 0.36053371 0.054594517 -0.28059626 -0.53237224 -0.67226982 -0.7355988 -0.75393748 -0.74507356 -0.71115208 -0.674371 -0.69709921 -0.87927318][-0.28479815 0.34823704 0.6332078 0.6125536 0.38518 0.11194038 -0.10652351 -0.26190424 -0.38639212 -0.50804043 -0.63198352 -0.74973059 -0.86400127 -1.0142663 -1.2770782][-0.34020996 0.3444972 0.72454023 0.81762886 0.69551039 0.51698732 0.36172676 0.20982647 0.030000687 -0.20195055 -0.47328544 -0.75562668 -1.0269759 -1.301867 -1.6331146][-0.42292523 0.28335667 0.73300362 0.93640614 0.9295001 0.85573292 0.77317476 0.638546 0.41827059 0.090874672 -0.31513977 -0.74653006 -1.1533308 -1.5279403 -1.9052556][-0.54121757 0.15624905 0.64125776 0.92865705 1.0263934 1.0459638 1.0241876 0.90496063 0.6563592 0.26514244 -0.22822952 -0.75485277 -1.2444427 -1.6720183 -2.060286][-0.76867747 -0.1101408 0.37406445 0.71164322 0.89699125 0.99710131 1.0253062 0.92513609 0.67334747 0.2668829 -0.24855852 -0.7983191 -1.2995293 -1.7193894 -2.0779476][-1.1462524 -0.55284286 -0.0980072 0.25020027 0.49318361 0.65319538 0.71806431 0.64349747 0.42083168 0.046256542 -0.4338243 -0.93883562 -1.3827918 -1.7364962 -2.0298114][-1.6627052 -1.1494427 -0.74319959 -0.41883802 -0.16407251 0.016749859 0.099980354 0.054791927 -0.11539602 -0.41783118 -0.814682 -1.2302682 -1.5789886 -1.8312695 -2.0344138][-2.2696452 -1.8530152 -1.5096569 -1.2271848 -0.99482632 -0.82448459 -0.74467397 -0.77127123 -0.88933754 -1.109127 -1.400615 -1.699389 -1.932302 -2.0753973 -2.183145][-2.9016986 -2.5898962 -2.321105 -2.0916324 -1.8977702 -1.7532148 -1.6865745 -1.7030816 -1.7848902 -1.9334908 -2.124028 -2.3060338 -2.4246793 -2.4682698 -2.4893584][-3.4565866 -3.2501974 -3.0621102 -2.8905177 -2.7406874 -2.6313808 -2.581821 -2.5917099 -2.6468308 -2.7390633 -2.8486004 -2.9369302 -2.9703779 -2.9496679 -2.9174795][-3.8367388 -3.719121 -3.6084445 -3.5015423 -3.4028962 -3.3281121 -3.2903092 -3.2923822 -3.3268185 -3.381444 -3.4405091 -3.4759362 -3.4706237 -3.4311326 -3.3861032]]...]
INFO - root - 2017-12-05 08:56:45.521163: step 5510, loss = 1.31, batch loss = 1.25 (38.9 examples/sec; 0.206 sec/batch; 18h:41m:54s remains)
INFO - root - 2017-12-05 08:56:47.593556: step 5520, loss = 1.22, batch loss = 1.17 (39.0 examples/sec; 0.205 sec/batch; 18h:37m:14s remains)
INFO - root - 2017-12-05 08:56:49.713252: step 5530, loss = 1.19, batch loss = 1.13 (38.6 examples/sec; 0.207 sec/batch; 18h:50m:11s remains)
INFO - root - 2017-12-05 08:56:51.804995: step 5540, loss = 1.09, batch loss = 1.03 (39.1 examples/sec; 0.205 sec/batch; 18h:34m:42s remains)
INFO - root - 2017-12-05 08:56:53.890960: step 5550, loss = 1.13, batch loss = 1.07 (38.7 examples/sec; 0.206 sec/batch; 18h:45m:11s remains)
INFO - root - 2017-12-05 08:56:55.994117: step 5560, loss = 1.31, batch loss = 1.25 (39.7 examples/sec; 0.201 sec/batch; 18h:17m:40s remains)
INFO - root - 2017-12-05 08:56:58.070151: step 5570, loss = 1.40, batch loss = 1.34 (38.3 examples/sec; 0.209 sec/batch; 18h:57m:16s remains)
INFO - root - 2017-12-05 08:57:00.168733: step 5580, loss = 1.22, batch loss = 1.16 (38.3 examples/sec; 0.209 sec/batch; 18h:56m:39s remains)
INFO - root - 2017-12-05 08:57:02.259842: step 5590, loss = 1.22, batch loss = 1.16 (37.6 examples/sec; 0.213 sec/batch; 19h:19m:32s remains)
INFO - root - 2017-12-05 08:57:04.408149: step 5600, loss = 1.30, batch loss = 1.24 (37.3 examples/sec; 0.215 sec/batch; 19h:29m:21s remains)
2017-12-05 08:57:04.690908: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3055196 -4.2671919 -4.2183537 -4.1603851 -4.1044683 -4.0690675 -4.0647216 -4.0902367 -4.1374297 -4.1939974 -4.2496605 -4.2924285 -4.322413 -4.3416734 -4.352705][-4.2443256 -4.1668663 -4.0658336 -3.9432607 -3.8199944 -3.7336195 -3.7072911 -3.7471011 -3.8408022 -3.962862 -4.0872159 -4.1879191 -4.2601933 -4.3068371 -4.3337107][-4.1738157 -4.0502987 -3.8841176 -3.674334 -3.4538369 -3.2836781 -3.20891 -3.2512603 -3.3974361 -3.6078172 -3.8315625 -4.0206294 -4.1612048 -4.252223 -4.3058591][-4.1025157 -3.9332833 -3.6994164 -3.3927476 -3.0528703 -2.7663445 -2.6102662 -2.63313 -2.8295631 -3.143147 -3.4916072 -3.7964714 -4.0278378 -4.1800637 -4.2701726][-4.0253978 -3.8172579 -3.5245583 -3.1265898 -2.6655197 -2.2447927 -1.9787011 -1.9562042 -2.1876578 -2.6032848 -3.083993 -3.5209296 -3.8595459 -4.0858736 -4.221302][-3.9266253 -3.6900117 -3.356266 -2.8907046 -2.3302248 -1.7871895 -1.4061186 -1.3151619 -1.5517282 -2.0482619 -2.6480451 -3.2120633 -3.6614854 -3.9677076 -4.1561027][-3.7886975 -3.5330753 -3.1821272 -2.6892953 -2.0767226 -1.454217 -0.98309994 -0.81496835 -1.018496 -1.5541968 -2.237633 -2.8969636 -3.4373424 -3.8194575 -4.06476][-3.6213288 -3.3476906 -2.9998589 -2.5256567 -1.9256117 -1.2872226 -0.7647891 -0.51854587 -0.65889859 -1.183316 -1.8936698 -2.5987258 -3.1993866 -3.6440794 -3.9460022][-3.4782226 -3.18347 -2.8472166 -2.4177334 -1.8753545 -1.2723098 -0.73652339 -0.43216419 -0.51020288 -0.98947954 -1.6708877 -2.3687468 -2.9912291 -3.4754744 -3.8226743][-3.424859 -3.1165261 -2.7937765 -2.417202 -1.9559562 -1.4323931 -0.94248986 -0.63327289 -0.66274953 -1.06053 -1.6566441 -2.2918758 -2.8857012 -3.3721886 -3.7386796][-3.4892259 -3.1902251 -2.8913763 -2.5693984 -2.194205 -1.7711165 -1.3661861 -1.0887992 -1.0751944 -1.3669677 -1.8463814 -2.3855033 -2.9137967 -3.3669598 -3.7215083][-3.6472754 -3.3823857 -3.1195908 -2.8507676 -2.5565982 -2.2373273 -1.9290423 -1.7001753 -1.6565213 -1.8449032 -2.2000933 -2.6305633 -3.0724168 -3.4658971 -3.7809298][-3.8498693 -3.6439059 -3.4338822 -3.2226896 -3.0024085 -2.7740479 -2.55266 -2.3764915 -2.3199193 -2.4257054 -2.6668029 -2.9863105 -3.3294632 -3.6443019 -3.8985329][-4.0432086 -3.9043813 -3.7553487 -3.6031222 -3.4463944 -3.2877884 -3.1333442 -3.0036561 -2.949307 -3.0037737 -3.1569076 -3.378541 -3.6259093 -3.8557117 -4.0397563][-4.1939731 -4.113462 -4.0208392 -3.9218187 -3.8173997 -3.7117825 -3.61137 -3.5269248 -3.4871621 -3.5141342 -3.6047583 -3.7451465 -3.9064527 -4.0555167 -4.1703148]]...]
INFO - root - 2017-12-05 08:57:06.778238: step 5610, loss = 1.26, batch loss = 1.20 (38.9 examples/sec; 0.206 sec/batch; 18h:41m:49s remains)
INFO - root - 2017-12-05 08:57:08.881601: step 5620, loss = 1.17, batch loss = 1.12 (38.9 examples/sec; 0.205 sec/batch; 18h:39m:07s remains)
INFO - root - 2017-12-05 08:57:10.979352: step 5630, loss = 1.31, batch loss = 1.25 (38.9 examples/sec; 0.205 sec/batch; 18h:39m:31s remains)
INFO - root - 2017-12-05 08:57:13.053656: step 5640, loss = 1.11, batch loss = 1.05 (38.1 examples/sec; 0.210 sec/batch; 19h:02m:52s remains)
INFO - root - 2017-12-05 08:57:15.215136: step 5650, loss = 1.10, batch loss = 1.04 (38.9 examples/sec; 0.206 sec/batch; 18h:39m:56s remains)
INFO - root - 2017-12-05 08:57:17.315863: step 5660, loss = 1.11, batch loss = 1.06 (37.8 examples/sec; 0.212 sec/batch; 19h:13m:53s remains)
INFO - root - 2017-12-05 08:57:19.425145: step 5670, loss = 1.23, batch loss = 1.17 (38.6 examples/sec; 0.207 sec/batch; 18h:49m:22s remains)
INFO - root - 2017-12-05 08:57:21.502295: step 5680, loss = 1.14, batch loss = 1.08 (38.0 examples/sec; 0.211 sec/batch; 19h:06m:58s remains)
INFO - root - 2017-12-05 08:57:23.589778: step 5690, loss = 1.28, batch loss = 1.22 (38.3 examples/sec; 0.209 sec/batch; 18h:56m:19s remains)
INFO - root - 2017-12-05 08:57:25.700371: step 5700, loss = 1.27, batch loss = 1.22 (38.0 examples/sec; 0.210 sec/batch; 19h:05m:52s remains)
2017-12-05 08:57:26.019597: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2637391 -4.2389121 -4.204834 -4.1652522 -4.1294293 -4.0944328 -4.0572162 -4.026938 -4.0088429 -4.0034857 -4.0048218 -4.0111184 -4.0144291 -4.0156159 -4.0207777][-4.2565241 -4.2309556 -4.1927371 -4.1463337 -4.1033406 -4.0640383 -4.0248137 -3.9955153 -3.980768 -3.9843295 -3.9988041 -4.0146875 -4.0227637 -4.0263948 -4.027966][-4.2655311 -4.2472272 -4.2150307 -4.1717396 -4.1301551 -4.0934768 -4.0581656 -4.0333357 -4.0231633 -4.0318489 -4.0527229 -4.0712738 -4.080699 -4.0850883 -4.0840521][-4.2736464 -4.2691917 -4.2502594 -4.2183843 -4.1854048 -4.1561418 -4.1289172 -4.11194 -4.1072612 -4.1170125 -4.1352439 -4.1496067 -4.1558013 -4.1577883 -4.1552997][-4.2809172 -4.2921739 -4.2912498 -4.2762384 -4.2569046 -4.2385187 -4.2215633 -4.2131596 -4.2127542 -4.2196236 -4.2304206 -4.237958 -4.2397146 -4.2392626 -4.237124][-4.2878952 -4.3111534 -4.3244352 -4.32363 -4.3167987 -4.3085566 -4.300952 -4.2990165 -4.3011627 -4.3046222 -4.3093014 -4.3123069 -4.3120327 -4.3104038 -4.309453][-4.2936096 -4.3224115 -4.3423939 -4.348577 -4.347837 -4.3448353 -4.3426094 -4.3447809 -4.3487668 -4.35259 -4.3559375 -4.3577604 -4.3575392 -4.3563395 -4.3564253][-4.2956915 -4.3238192 -4.3433456 -4.3490453 -4.348525 -4.3468051 -4.3474469 -4.3528042 -4.3601937 -4.3668838 -4.3721972 -4.3755393 -4.3769622 -4.37694 -4.3773589][-4.2929673 -4.3164492 -4.3307447 -4.3315096 -4.3283296 -4.3269792 -4.3295093 -4.3384714 -4.3506403 -4.3618979 -4.371161 -4.3778205 -4.3815989 -4.3827677 -4.3834438][-4.2858682 -4.3036041 -4.3117132 -4.3073373 -4.3020654 -4.3015065 -4.3058825 -4.3186541 -4.3359952 -4.3523655 -4.3658133 -4.37554 -4.3810382 -4.3830929 -4.3839655][-4.2781906 -4.292325 -4.2970829 -4.2905874 -4.2857904 -4.2871027 -4.2936878 -4.309165 -4.3296103 -4.3487687 -4.3644505 -4.3752265 -4.3809237 -4.3830919 -4.383965][-4.2737408 -4.2885289 -4.2945347 -4.2905116 -4.2889123 -4.2932391 -4.3019691 -4.3172345 -4.3364925 -4.3538432 -4.3677568 -4.3769941 -4.3815956 -4.3832784 -4.383842][-4.2756643 -4.2938361 -4.3047938 -4.3065619 -4.3096523 -4.316411 -4.3251638 -4.3374043 -4.351512 -4.3637409 -4.3732085 -4.3794694 -4.3824024 -4.3834605 -4.38368][-4.2816381 -4.304635 -4.3217425 -4.3298216 -4.3362889 -4.3435087 -4.3504691 -4.3579454 -4.3660588 -4.3729286 -4.3780465 -4.3814631 -4.3831263 -4.3837385 -4.3837619][-4.2844315 -4.3139071 -4.337101 -4.35067 -4.3593678 -4.3655376 -4.3698163 -4.3730373 -4.3764052 -4.3792596 -4.3814125 -4.3829556 -4.383821 -4.384109 -4.3840232]]...]
INFO - root - 2017-12-05 08:57:28.088532: step 5710, loss = 0.98, batch loss = 0.92 (39.5 examples/sec; 0.203 sec/batch; 18h:24m:27s remains)
INFO - root - 2017-12-05 08:57:30.141171: step 5720, loss = 1.22, batch loss = 1.16 (39.9 examples/sec; 0.200 sec/batch; 18h:11m:55s remains)
INFO - root - 2017-12-05 08:57:32.180211: step 5730, loss = 1.13, batch loss = 1.07 (39.7 examples/sec; 0.201 sec/batch; 18h:16m:10s remains)
INFO - root - 2017-12-05 08:57:34.258401: step 5740, loss = 1.38, batch loss = 1.32 (38.9 examples/sec; 0.206 sec/batch; 18h:39m:15s remains)
INFO - root - 2017-12-05 08:57:36.344190: step 5750, loss = 1.14, batch loss = 1.08 (38.3 examples/sec; 0.209 sec/batch; 18h:58m:21s remains)
INFO - root - 2017-12-05 08:57:38.418881: step 5760, loss = 1.35, batch loss = 1.29 (38.3 examples/sec; 0.209 sec/batch; 18h:57m:23s remains)
INFO - root - 2017-12-05 08:57:40.493716: step 5770, loss = 1.34, batch loss = 1.28 (38.6 examples/sec; 0.207 sec/batch; 18h:47m:59s remains)
INFO - root - 2017-12-05 08:57:42.554030: step 5780, loss = 1.23, batch loss = 1.17 (38.9 examples/sec; 0.206 sec/batch; 18h:39m:01s remains)
INFO - root - 2017-12-05 08:57:44.643303: step 5790, loss = 1.20, batch loss = 1.14 (37.6 examples/sec; 0.213 sec/batch; 19h:18m:17s remains)
INFO - root - 2017-12-05 08:57:46.751433: step 5800, loss = 1.04, batch loss = 0.98 (38.0 examples/sec; 0.211 sec/batch; 19h:06m:32s remains)
2017-12-05 08:57:47.034828: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.9773068 -3.0093584 -3.0424426 -3.0452485 -3.0061159 -2.9124427 -2.7590702 -2.5651584 -2.3387737 -2.1053429 -1.9017062 -1.7513664 -1.6807761 -1.6687667 -1.6565993][-2.5937076 -2.65504 -2.7073336 -2.7043338 -2.6411135 -2.5221138 -2.3586941 -2.1865103 -2.0211794 -1.882025 -1.7926252 -1.754482 -1.7825017 -1.8562737 -1.9222808][-2.2045519 -2.2686484 -2.3161955 -2.2857032 -2.1787758 -2.0277069 -1.8675973 -1.7417829 -1.6617737 -1.6350865 -1.671128 -1.7536085 -1.8798306 -2.0358229 -2.1781504][-1.7913287 -1.8207293 -1.8231726 -1.7285807 -1.5516124 -1.362112 -1.2206523 -1.164602 -1.1872799 -1.2756312 -1.4269888 -1.6187341 -1.8312869 -2.0587697 -2.269685][-1.409183 -1.3640647 -1.283293 -1.0964677 -0.82825041 -0.58280134 -0.45337462 -0.47141767 -0.59943509 -0.7976172 -1.0505884 -1.3378375 -1.6323581 -1.9250085 -2.1990738][-1.2100534 -1.0530934 -0.84862757 -0.53705573 -0.15883112 0.17294455 0.33395004 0.2762394 0.059112549 -0.23813581 -0.58555746 -0.9713788 -1.3639333 -1.7374907 -2.0800481][-1.3208685 -1.0542088 -0.7130003 -0.26252842 0.229918 0.66763163 0.92346478 0.90412807 0.64809847 0.27226353 -0.15682316 -0.64405465 -1.1478348 -1.6154594 -2.027312][-1.7470894 -1.4074614 -0.9745357 -0.42519975 0.15175724 0.6688323 1.022449 1.1057949 0.90310478 0.51868868 0.050935268 -0.50019932 -1.0912013 -1.6448662 -2.1209509][-2.3627892 -2.0131166 -1.5676627 -1.0058267 -0.41520619 0.11670351 0.50620556 0.67901468 0.58469725 0.27216578 -0.16565323 -0.71069479 -1.3102384 -1.8897974 -2.3876939][-3.0047204 -2.6937795 -2.2971923 -1.8028612 -1.275924 -0.79604053 -0.43210864 -0.2296176 -0.2408371 -0.45409727 -0.81250644 -1.2850904 -1.8174465 -2.3468575 -2.801183][-3.5553513 -3.3114505 -2.9989796 -2.6161714 -2.203649 -1.8227992 -1.5244131 -1.3405778 -1.3105543 -1.439209 -1.7006853 -2.063817 -2.4828348 -2.9068708 -3.2684588][-3.9465878 -3.7840669 -3.5726247 -3.313241 -3.0301104 -2.7639961 -2.5466521 -2.4024413 -2.3594391 -2.4270151 -2.5961223 -2.842895 -3.1323195 -3.4294887 -3.6818075][-4.1760883 -4.0862455 -3.9657822 -3.815454 -3.647387 -3.485786 -3.3476596 -3.2501438 -3.2115254 -3.2387402 -3.3345704 -3.4816911 -3.6564717 -3.8371639 -3.9903252][-4.2893629 -4.2477322 -4.1909747 -4.1183281 -4.034575 -3.9528894 -3.8804462 -3.82725 -3.8028359 -3.8112609 -3.8564482 -3.9286821 -4.0178308 -4.1103058 -4.1880078][-4.339643 -4.3234611 -4.30124 -4.2716994 -4.2367773 -4.2020612 -4.1706862 -4.1472893 -4.1363568 -4.1388888 -4.1565089 -4.1856461 -4.2228975 -4.2614441 -4.2934966]]...]
INFO - root - 2017-12-05 08:57:49.153811: step 5810, loss = 1.16, batch loss = 1.10 (38.1 examples/sec; 0.210 sec/batch; 19h:02m:13s remains)
INFO - root - 2017-12-05 08:57:51.250299: step 5820, loss = 1.18, batch loss = 1.12 (38.4 examples/sec; 0.208 sec/batch; 18h:55m:00s remains)
INFO - root - 2017-12-05 08:57:53.306361: step 5830, loss = 1.17, batch loss = 1.11 (38.9 examples/sec; 0.206 sec/batch; 18h:41m:00s remains)
INFO - root - 2017-12-05 08:57:55.379725: step 5840, loss = 1.18, batch loss = 1.13 (39.3 examples/sec; 0.204 sec/batch; 18h:28m:26s remains)
INFO - root - 2017-12-05 08:57:57.476903: step 5850, loss = 1.14, batch loss = 1.08 (37.3 examples/sec; 0.214 sec/batch; 19h:26m:21s remains)
INFO - root - 2017-12-05 08:57:59.566782: step 5860, loss = 1.26, batch loss = 1.20 (38.6 examples/sec; 0.207 sec/batch; 18h:49m:14s remains)
INFO - root - 2017-12-05 08:58:01.649817: step 5870, loss = 1.14, batch loss = 1.08 (39.4 examples/sec; 0.203 sec/batch; 18h:24m:09s remains)
INFO - root - 2017-12-05 08:58:03.721137: step 5880, loss = 1.30, batch loss = 1.24 (38.9 examples/sec; 0.206 sec/batch; 18h:39m:04s remains)
INFO - root - 2017-12-05 08:58:05.816615: step 5890, loss = 1.11, batch loss = 1.05 (36.6 examples/sec; 0.218 sec/batch; 19h:49m:10s remains)
INFO - root - 2017-12-05 08:58:07.910778: step 5900, loss = 1.46, batch loss = 1.40 (37.8 examples/sec; 0.211 sec/batch; 19h:10m:48s remains)
2017-12-05 08:58:08.180518: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.8987379 -3.8993087 -3.9460614 -4.015358 -4.0781622 -4.1095376 -4.1020231 -4.0718851 -4.0362577 -4.011507 -3.9920769 -3.9704278 -3.9342546 -3.8860319 -3.8530142][-3.47902 -3.465867 -3.5387328 -3.6531165 -3.7537155 -3.7985134 -3.7753854 -3.7144103 -3.6512008 -3.6122854 -3.5901928 -3.5689909 -3.5250688 -3.455225 -3.4022698][-2.9567437 -2.90693 -2.9898443 -3.140368 -3.2693112 -3.3157964 -3.2602406 -3.1557808 -3.0613184 -3.0148413 -3.0066705 -3.0101943 -2.9869916 -2.9154441 -2.8487954][-2.4275758 -2.3086274 -2.3664587 -2.521893 -2.6525037 -2.6776798 -2.5741239 -2.4178863 -2.2984061 -2.2627473 -2.2952807 -2.3553166 -2.3881483 -2.3496528 -2.2902753][-1.9994504 -1.7886019 -1.7799804 -1.8905194 -1.9797275 -1.9532411 -1.7826595 -1.5687716 -1.4370944 -1.4399388 -1.5469739 -1.6978483 -1.8231039 -1.8535023 -1.830739][-1.7511859 -1.4455023 -1.3433797 -1.358902 -1.3581986 -1.2483642 -0.99977326 -0.72804976 -0.59758067 -0.66503 -0.87696147 -1.1452887 -1.3862264 -1.5107615 -1.5481536][-1.6770086 -1.2995558 -1.1074531 -1.0090575 -0.88969851 -0.67431068 -0.34376478 -0.023551464 0.096301079 -0.049152851 -0.37866449 -0.77562928 -1.1296494 -1.3437114 -1.4422493][-1.7117128 -1.2994385 -1.045891 -0.86025715 -0.64102364 -0.33920717 0.054522038 0.39765263 0.49650192 0.28805256 -0.13217115 -0.62259912 -1.0551498 -1.3223691 -1.4559066][-1.7956405 -1.3867321 -1.1198907 -0.90842867 -0.65812826 -0.33817482 0.058773041 0.39102316 0.47724628 0.25755882 -0.17852306 -0.68895936 -1.1371417 -1.4078333 -1.5383747][-1.892777 -1.5169237 -1.283818 -1.11328 -0.91544557 -0.65449882 -0.32301903 -0.036686897 0.043703556 -0.14063644 -0.516665 -0.96489882 -1.3587153 -1.5816913 -1.6709113][-2.0082552 -1.6818559 -1.5100203 -1.4213161 -1.3252563 -1.1688578 -0.93915486 -0.72228527 -0.6544373 -0.78858638 -1.0652978 -1.398057 -1.685991 -1.8257473 -1.8513398][-2.1861312 -1.9128382 -1.8075004 -1.8041499 -1.8142786 -1.7696552 -1.6453869 -1.5049071 -1.4539027 -1.5411677 -1.7173631 -1.928241 -2.0986781 -2.1490827 -2.1096361][-2.4608183 -2.2431943 -2.1936834 -2.2536101 -2.3380747 -2.3760808 -2.334708 -2.2597861 -2.2294745 -2.28671 -2.3899698 -2.5062881 -2.5795827 -2.5565486 -2.4680166][-2.8306117 -2.66955 -2.6582792 -2.7477474 -2.8609154 -2.9348137 -2.9433589 -2.9170449 -2.9084466 -2.9512448 -3.0138457 -3.0713859 -3.0782523 -3.0081487 -2.8944478][-3.2407451 -3.1312926 -3.1403341 -3.2298217 -3.337584 -3.4153616 -3.4470196 -3.4523675 -3.460716 -3.4938231 -3.5336442 -3.5574455 -3.5290167 -3.4414852 -3.32763]]...]
INFO - root - 2017-12-05 08:58:10.252738: step 5910, loss = 1.33, batch loss = 1.27 (38.9 examples/sec; 0.206 sec/batch; 18h:39m:05s remains)
INFO - root - 2017-12-05 08:58:12.333562: step 5920, loss = 1.42, batch loss = 1.36 (39.1 examples/sec; 0.205 sec/batch; 18h:33m:57s remains)
INFO - root - 2017-12-05 08:58:14.403755: step 5930, loss = 1.29, batch loss = 1.24 (39.4 examples/sec; 0.203 sec/batch; 18h:26m:04s remains)
INFO - root - 2017-12-05 08:58:16.459987: step 5940, loss = 1.35, batch loss = 1.29 (38.9 examples/sec; 0.206 sec/batch; 18h:38m:55s remains)
INFO - root - 2017-12-05 08:58:18.532045: step 5950, loss = 1.12, batch loss = 1.06 (37.4 examples/sec; 0.214 sec/batch; 19h:23m:14s remains)
INFO - root - 2017-12-05 08:58:20.586566: step 5960, loss = 1.20, batch loss = 1.14 (38.4 examples/sec; 0.208 sec/batch; 18h:53m:11s remains)
INFO - root - 2017-12-05 08:58:22.657537: step 5970, loss = 1.14, batch loss = 1.08 (39.0 examples/sec; 0.205 sec/batch; 18h:36m:15s remains)
INFO - root - 2017-12-05 08:58:24.737050: step 5980, loss = 1.37, batch loss = 1.31 (38.9 examples/sec; 0.206 sec/batch; 18h:39m:38s remains)
INFO - root - 2017-12-05 08:58:26.828880: step 5990, loss = 1.00, batch loss = 0.94 (38.7 examples/sec; 0.207 sec/batch; 18h:45m:35s remains)
INFO - root - 2017-12-05 08:58:28.908597: step 6000, loss = 1.15, batch loss = 1.09 (38.8 examples/sec; 0.206 sec/batch; 18h:42m:01s remains)
2017-12-05 08:58:29.200818: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.5263629 -1.4336724 -1.457839 -1.5062125 -1.5512133 -1.5748286 -1.5848372 -1.5914111 -1.5971029 -1.5911665 -1.5498443 -1.4502633 -1.2975531 -1.1210592 -0.96679306][-0.97411776 -0.86619782 -0.91367245 -1.0051253 -1.0988698 -1.1606784 -1.1930213 -1.2187037 -1.2556486 -1.2974191 -1.3153472 -1.2794313 -1.1890407 -1.083005 -1.0202672][-0.48663664 -0.34740591 -0.3827548 -0.47960353 -0.59025788 -0.67907548 -0.74425554 -0.816102 -0.91380954 -1.0253441 -1.1192482 -1.1664391 -1.1663477 -1.1601431 -1.2146597][-0.084540844 0.10703564 0.11923599 0.060834408 -0.033687592 -0.13281918 -0.23165607 -0.3601861 -0.53221941 -0.7277503 -0.91284323 -1.0620146 -1.1718705 -1.2831602 -1.4593487][0.15409946 0.40470886 0.48600769 0.50062275 0.46606827 0.39799356 0.29167461 0.11488199 -0.12924385 -0.41090822 -0.69952393 -0.972188 -1.2152357 -1.4531503 -1.7395694][0.21830893 0.50975132 0.64687967 0.73594284 0.78326464 0.78231621 0.70386314 0.51674891 0.24491978 -0.082691669 -0.450104 -0.843981 -1.2307749 -1.6039062 -1.9945731][0.15963984 0.46284819 0.61662006 0.73872757 0.841135 0.906631 0.88606453 0.742867 0.50745821 0.19745684 -0.19338322 -0.67325521 -1.1909323 -1.6994786 -2.1961379][0.076383114 0.34946012 0.46233225 0.55144548 0.64804935 0.744998 0.79048252 0.73946524 0.60581732 0.38183117 0.03093195 -0.48410106 -1.10166 -1.7333643 -2.3357081][0.026431561 0.21293879 0.21691227 0.20285606 0.23479414 0.32714462 0.43531656 0.50270414 0.51676273 0.43122959 0.16763973 -0.33548832 -1.0119047 -1.7382812 -2.431963][0.014818668 0.062456131 -0.099025249 -0.26588011 -0.33592033 -0.27412653 -0.11749125 0.069640636 0.2382884 0.30595398 0.15584087 -0.29174757 -0.97719574 -1.7559361 -2.510756][0.071660042 -0.055907726 -0.40843916 -0.7467432 -0.93716764 -0.92640185 -0.74482894 -0.46961427 -0.18389988 0.0019698143 -0.045529842 -0.41786098 -1.0639069 -1.8358309 -2.6012688][0.20299292 -0.088219166 -0.61341405 -1.1003697 -1.4035666 -1.4566514 -1.2864327 -0.98414135 -0.65638089 -0.42115378 -0.41141677 -0.71344876 -1.2848551 -1.9946573 -2.7140858][0.37550163 -0.036571503 -0.66410661 -1.2387037 -1.6187561 -1.7402697 -1.6278701 -1.3741939 -1.091346 -0.88619971 -0.87541676 -1.1282241 -1.6114786 -2.2233255 -2.8556604][0.52454185 0.047570229 -0.610548 -1.2083511 -1.6223273 -1.8018525 -1.7728243 -1.6187413 -1.4407904 -1.3259215 -1.3622897 -1.5956218 -1.9997063 -2.5027204 -3.0284765][0.55941725 0.075253963 -0.55538583 -1.1298137 -1.5454054 -1.7667499 -1.8206933 -1.7821324 -1.7338755 -1.7361553 -1.8440812 -2.0807931 -2.4244294 -2.8293123 -3.2461507]]...]
INFO - root - 2017-12-05 08:58:31.281040: step 6010, loss = 1.31, batch loss = 1.25 (37.5 examples/sec; 0.213 sec/batch; 19h:20m:45s remains)
INFO - root - 2017-12-05 08:58:33.393931: step 6020, loss = 1.32, batch loss = 1.26 (36.2 examples/sec; 0.221 sec/batch; 20h:01m:44s remains)
INFO - root - 2017-12-05 08:58:35.480114: step 6030, loss = 1.05, batch loss = 0.99 (39.4 examples/sec; 0.203 sec/batch; 18h:24m:25s remains)
INFO - root - 2017-12-05 08:58:37.541404: step 6040, loss = 1.04, batch loss = 0.98 (39.3 examples/sec; 0.204 sec/batch; 18h:28m:50s remains)
INFO - root - 2017-12-05 08:58:39.635644: step 6050, loss = 1.15, batch loss = 1.09 (39.2 examples/sec; 0.204 sec/batch; 18h:31m:23s remains)
INFO - root - 2017-12-05 08:58:41.718754: step 6060, loss = 1.24, batch loss = 1.18 (38.2 examples/sec; 0.209 sec/batch; 18h:58m:45s remains)
INFO - root - 2017-12-05 08:58:43.822413: step 6070, loss = 1.26, batch loss = 1.20 (38.0 examples/sec; 0.210 sec/batch; 19h:03m:59s remains)
INFO - root - 2017-12-05 08:58:45.897478: step 6080, loss = 1.37, batch loss = 1.31 (37.6 examples/sec; 0.213 sec/batch; 19h:16m:53s remains)
INFO - root - 2017-12-05 08:58:47.984576: step 6090, loss = 1.21, batch loss = 1.15 (40.1 examples/sec; 0.199 sec/batch; 18h:05m:10s remains)
INFO - root - 2017-12-05 08:58:50.049102: step 6100, loss = 1.05, batch loss = 0.99 (39.5 examples/sec; 0.203 sec/batch; 18h:22m:13s remains)
2017-12-05 08:58:50.315956: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3523006 -4.3103766 -4.2567544 -4.193604 -4.1235056 -4.0577116 -4.0108314 -3.9847238 -3.97493 -3.9741762 -3.9791133 -3.982218 -3.9813306 -3.9807131 -3.9850245][-4.3301973 -4.2636704 -4.1786466 -4.0766306 -3.9619491 -3.8525331 -3.7714136 -3.7254443 -3.7099195 -3.7113523 -3.7229638 -3.7329559 -3.7360871 -3.7371497 -3.745877][-4.3083158 -4.2162457 -4.0977464 -3.9549828 -3.7939153 -3.6360483 -3.51316 -3.4401388 -3.4132807 -3.4130135 -3.4313109 -3.4528835 -3.466027 -3.4733644 -3.4898252][-4.2871752 -4.17387 -4.0259676 -3.8468046 -3.6447906 -3.4446061 -3.2830224 -3.1821141 -3.1405795 -3.1339946 -3.1567252 -3.1907923 -3.2177267 -3.2338657 -3.2564898][-4.2636318 -4.1327572 -3.9618795 -3.7558763 -3.5255311 -3.2979469 -3.1090097 -2.9872255 -2.935245 -2.9225616 -2.9421237 -2.9786906 -3.0129371 -3.0335996 -3.0585561][-4.2401257 -4.0939808 -3.9058366 -3.6818881 -3.4375844 -3.199544 -3.0013738 -2.8729644 -2.8190117 -2.8048184 -2.8176935 -2.844785 -2.8736887 -2.8919377 -2.91323][-4.2166891 -4.0566826 -3.8555365 -3.6208587 -3.371999 -3.1362915 -2.9462533 -2.8274107 -2.7831495 -2.7768021 -2.7882922 -2.8050547 -2.8231945 -2.8345561 -2.8497102][-4.1976967 -4.0257645 -3.8127661 -3.5698462 -3.3195274 -3.0924015 -2.9200048 -2.8189042 -2.789247 -2.7974756 -2.8206155 -2.8418911 -2.8593564 -2.8743491 -2.8931215][-4.1883354 -4.01083 -3.7926719 -3.5470133 -3.3006091 -3.0865784 -2.934958 -2.8560548 -2.8432884 -2.8688641 -2.9129324 -2.9551489 -2.9895468 -3.0210481 -3.0523639][-4.1912785 -4.0180149 -3.8052387 -3.5663228 -3.33151 -3.1338074 -3.0024552 -2.9471726 -2.9563487 -3.0030155 -3.0679455 -3.1348457 -3.1928225 -3.2443824 -3.2896438][-4.2071066 -4.0492449 -3.8545585 -3.6352568 -3.423106 -3.2487588 -3.1388061 -3.1057923 -3.1368146 -3.2033339 -3.2841759 -3.3681011 -3.4420114 -3.5051489 -3.5569828][-4.2333403 -4.1004658 -3.9357448 -3.7493649 -3.5713944 -3.4308615 -3.3492229 -3.3369484 -3.3814917 -3.4574735 -3.5419989 -3.6263216 -3.7006254 -3.7641602 -3.8154106][-4.2677951 -4.1661043 -4.0406756 -3.8982432 -3.7638683 -3.6649518 -3.614486 -3.6176891 -3.6635942 -3.7332854 -3.8052013 -3.8743818 -3.9363279 -3.9897225 -4.0327721][-4.3050838 -4.2356124 -4.1514921 -4.0561619 -3.967721 -3.9070272 -3.881551 -3.8927402 -3.930316 -3.9811642 -4.0300722 -4.07566 -4.1173105 -4.1535859 -4.18282][-4.3382835 -4.2965641 -4.2468495 -4.1904192 -4.1385746 -4.1056314 -4.0951133 -4.1070948 -4.1321115 -4.1624255 -4.18999 -4.2142372 -4.23644 -4.2561607 -4.272429]]...]
INFO - root - 2017-12-05 08:58:52.386233: step 6110, loss = 1.21, batch loss = 1.16 (39.4 examples/sec; 0.203 sec/batch; 18h:24m:23s remains)
INFO - root - 2017-12-05 08:58:54.476688: step 6120, loss = 1.20, batch loss = 1.14 (37.4 examples/sec; 0.214 sec/batch; 19h:23m:16s remains)
INFO - root - 2017-12-05 08:58:56.613694: step 6130, loss = 1.22, batch loss = 1.17 (37.1 examples/sec; 0.216 sec/batch; 19h:33m:13s remains)
INFO - root - 2017-12-05 08:58:58.696089: step 6140, loss = 1.22, batch loss = 1.17 (39.9 examples/sec; 0.201 sec/batch; 18h:10m:48s remains)
INFO - root - 2017-12-05 08:59:00.780275: step 6150, loss = 1.09, batch loss = 1.03 (38.7 examples/sec; 0.207 sec/batch; 18h:45m:16s remains)
INFO - root - 2017-12-05 08:59:02.894334: step 6160, loss = 1.40, batch loss = 1.34 (36.4 examples/sec; 0.220 sec/batch; 19h:53m:58s remains)
INFO - root - 2017-12-05 08:59:04.995260: step 6170, loss = 1.37, batch loss = 1.31 (38.5 examples/sec; 0.208 sec/batch; 18h:49m:35s remains)
INFO - root - 2017-12-05 08:59:07.075688: step 6180, loss = 1.03, batch loss = 0.97 (38.0 examples/sec; 0.210 sec/batch; 19h:04m:43s remains)
INFO - root - 2017-12-05 08:59:09.161282: step 6190, loss = 1.39, batch loss = 1.33 (38.4 examples/sec; 0.208 sec/batch; 18h:53m:12s remains)
INFO - root - 2017-12-05 08:59:11.267090: step 6200, loss = 1.32, batch loss = 1.26 (38.7 examples/sec; 0.207 sec/batch; 18h:43m:57s remains)
2017-12-05 08:59:11.527700: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.0435011 -2.0094478 -2.0403137 -2.0581825 -2.0164473 -1.8583827 -1.5741839 -1.2136416 -0.87622952 -0.70378089 -0.78555012 -1.0790024 -1.4781947 -1.9050076 -2.3184853][-1.9335437 -1.9233556 -1.9798069 -2.015204 -1.9617143 -1.7573502 -1.4149654 -1.0107648 -0.676054 -0.54098439 -0.67937708 -1.0459671 -1.5316722 -2.0479026 -2.5243063][-1.9908884 -2.008369 -2.0853014 -2.128484 -2.0561323 -1.8145192 -1.4330573 -1.013576 -0.70085096 -0.60356641 -0.78304267 -1.2005544 -1.7424619 -2.3051331 -2.7988091][-2.1578815 -2.2028172 -2.2921622 -2.3292766 -2.2257705 -1.9484534 -1.5406497 -1.1207995 -0.83598256 -0.77614474 -0.99008131 -1.4415193 -2.0068295 -2.5676463 -3.0333085][-2.4120536 -2.4709074 -2.5500054 -2.5537426 -2.3998687 -2.0762639 -1.6399832 -1.2164562 -0.96136594 -0.95180225 -1.2145047 -1.69312 -2.2557013 -2.7790303 -3.1811404][-2.7221479 -2.7693977 -2.8050931 -2.7429693 -2.5167685 -2.1319554 -1.6582212 -1.2354898 -1.0303118 -1.0970345 -1.4194345 -1.9140437 -2.4482369 -2.9052429 -3.21904][-3.0341496 -3.0444269 -3.0093091 -2.8554485 -2.5367098 -2.0786932 -1.5683806 -1.1663687 -1.0435104 -1.206569 -1.5874343 -2.0822058 -2.5649381 -2.9308596 -3.1392839][-3.2665658 -3.218199 -3.09696 -2.8434796 -2.4335735 -1.9121432 -1.3933444 -1.0559719 -1.0356984 -1.2895973 -1.7136793 -2.1923068 -2.6050344 -2.8684893 -2.9728742][-3.3513522 -3.239747 -3.0412531 -2.7035294 -2.22473 -1.6785779 -1.2053409 -0.972131 -1.0520003 -1.3757584 -1.8246789 -2.2752016 -2.6096497 -2.7727118 -2.7839975][-3.2940526 -3.1313314 -2.8726594 -2.4787576 -1.973417 -1.4561028 -1.0754397 -0.95757866 -1.1245353 -1.4960451 -1.954025 -2.3655183 -2.6206913 -2.6906686 -2.6199989][-3.1228216 -2.92455 -2.6322818 -2.2248137 -1.7473934 -1.310816 -1.0464439 -1.0355334 -1.2675114 -1.6622496 -2.1077216 -2.4704356 -2.6496749 -2.6387 -2.4968576][-2.8945403 -2.6811819 -2.3881662 -2.0106382 -1.6097224 -1.2845793 -1.1371551 -1.2142217 -1.4833391 -1.8710334 -2.2760553 -2.5758419 -2.6818504 -2.6007152 -2.4022908][-2.6709101 -2.4718642 -2.2115786 -1.9019046 -1.6076558 -1.4006624 -1.348613 -1.4799101 -1.7595899 -2.1149008 -2.4529898 -2.6746898 -2.7097225 -2.5719075 -2.3317037][-2.5157592 -2.3540213 -2.155472 -1.9384382 -1.7565534 -1.6513116 -1.6648357 -1.8143098 -2.0713878 -2.3657255 -2.6202126 -2.7610145 -2.73281 -2.5497279 -2.2771583][-2.4774957 -2.3663146 -2.2396417 -2.1142828 -2.0261862 -1.9911957 -2.0326037 -2.1624036 -2.3605852 -2.5718684 -2.7419109 -2.8130214 -2.7365992 -2.5219295 -2.2306437]]...]
INFO - root - 2017-12-05 08:59:13.638211: step 6210, loss = 1.40, batch loss = 1.34 (39.4 examples/sec; 0.203 sec/batch; 18h:24m:47s remains)
INFO - root - 2017-12-05 08:59:15.740292: step 6220, loss = 1.40, batch loss = 1.34 (38.1 examples/sec; 0.210 sec/batch; 19h:00m:24s remains)
INFO - root - 2017-12-05 08:59:17.841096: step 6230, loss = 1.03, batch loss = 0.98 (38.0 examples/sec; 0.211 sec/batch; 19h:04m:42s remains)
INFO - root - 2017-12-05 08:59:19.919251: step 6240, loss = 1.16, batch loss = 1.10 (38.5 examples/sec; 0.208 sec/batch; 18h:51m:14s remains)
INFO - root - 2017-12-05 08:59:22.007675: step 6250, loss = 1.33, batch loss = 1.27 (38.7 examples/sec; 0.207 sec/batch; 18h:45m:13s remains)
INFO - root - 2017-12-05 08:59:24.132185: step 6260, loss = 1.09, batch loss = 1.04 (35.7 examples/sec; 0.224 sec/batch; 20h:19m:08s remains)
INFO - root - 2017-12-05 08:59:26.235369: step 6270, loss = 1.07, batch loss = 1.01 (39.1 examples/sec; 0.205 sec/batch; 18h:33m:40s remains)
INFO - root - 2017-12-05 08:59:28.323338: step 6280, loss = 1.40, batch loss = 1.34 (37.2 examples/sec; 0.215 sec/batch; 19h:30m:18s remains)
INFO - root - 2017-12-05 08:59:30.445620: step 6290, loss = 1.48, batch loss = 1.42 (38.6 examples/sec; 0.207 sec/batch; 18h:46m:25s remains)
INFO - root - 2017-12-05 08:59:32.554160: step 6300, loss = 1.42, batch loss = 1.36 (39.2 examples/sec; 0.204 sec/batch; 18h:30m:08s remains)
2017-12-05 08:59:32.826904: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3766003 -4.325542 -4.2072959 -4.0015645 -3.7116189 -3.4000115 -3.1949987 -3.1857405 -3.2980976 -3.428915 -3.5306344 -3.5836327 -3.6112819 -3.6418757 -3.6871338][-4.3760996 -4.3225336 -4.1950221 -3.9697824 -3.6515687 -3.3053584 -3.0717397 -3.0658603 -3.2153678 -3.3882914 -3.5206742 -3.5922594 -3.6292791 -3.6671584 -3.7195482][-4.3775868 -4.3226786 -4.1886315 -3.9474425 -3.6053927 -3.2291632 -2.9653969 -2.9577785 -3.1455548 -3.3694181 -3.5432341 -3.643513 -3.6976519 -3.7472348 -3.8081336][-4.3789048 -4.3241239 -4.1877508 -3.9359398 -3.5738716 -3.1702394 -2.8721688 -2.8481286 -3.0654342 -3.3416724 -3.5650358 -3.7032125 -3.7834916 -3.8520823 -3.9250782][-4.3796163 -4.3264103 -4.1927991 -3.9386342 -3.5660737 -3.1426096 -2.8131621 -2.7582059 -2.98314 -3.2966418 -3.5631163 -3.7397408 -3.8506927 -3.9428134 -4.0303588][-4.3802962 -4.3295326 -4.2025824 -3.9556012 -3.5838943 -3.1513481 -2.7966847 -2.7000947 -2.9040985 -3.2290735 -3.5227571 -3.7307739 -3.8691611 -3.9826624 -4.0835276][-4.3807974 -4.3323865 -4.212316 -3.9772112 -3.612422 -3.17603 -2.799356 -2.6561167 -2.8151059 -3.1258893 -3.4315944 -3.665556 -3.8308134 -3.9648359 -4.0759053][-4.3800488 -4.3323092 -4.2168384 -3.9924545 -3.6387608 -3.2057409 -2.8147764 -2.6299386 -2.7328186 -3.0091624 -3.3146453 -3.5696931 -3.7592452 -3.9091263 -4.0245495][-4.3784084 -4.3299885 -4.2167988 -4.0008469 -3.6623168 -3.2433252 -2.852298 -2.6377263 -2.6864786 -2.9157157 -3.207876 -3.4759059 -3.6869428 -3.8483453 -3.9626825][-4.37722 -4.3274369 -4.2156339 -4.0085292 -3.6898222 -3.297709 -2.9255762 -2.7000749 -2.7061996 -2.8845179 -3.1472821 -3.4143336 -3.6409519 -3.8118157 -3.923841][-4.3764753 -4.3265028 -4.21819 -4.022788 -3.7280498 -3.3718421 -3.034338 -2.8163466 -2.7939105 -2.9266086 -3.1519668 -3.4067111 -3.6388607 -3.8149121 -3.9245439][-4.3779297 -4.3307185 -4.2302461 -4.0525026 -3.7898393 -3.4802451 -3.1925869 -2.9987361 -2.9584503 -3.0506885 -3.2346411 -3.4652307 -3.6895509 -3.8617797 -3.965071][-4.3817735 -4.342731 -4.2577763 -4.106194 -3.8852613 -3.6305711 -3.4013529 -3.2447515 -3.1990161 -3.2571115 -3.3966842 -3.5876796 -3.785568 -3.9409807 -4.0336571][-4.3855867 -4.35747 -4.2939024 -4.1761756 -4.002121 -3.8035398 -3.6300764 -3.5133727 -3.4725454 -3.5051389 -3.6031229 -3.7464981 -3.9056115 -4.035099 -4.1142292][-4.3880692 -4.3697343 -4.3267384 -4.2443128 -4.1190338 -3.9759033 -3.8537347 -3.7762022 -3.7468088 -3.7639351 -3.82586 -3.9216297 -4.0341697 -4.1305265 -4.193109]]...]
INFO - root - 2017-12-05 08:59:34.915218: step 6310, loss = 1.10, batch loss = 1.05 (39.8 examples/sec; 0.201 sec/batch; 18h:13m:53s remains)
INFO - root - 2017-12-05 08:59:37.001496: step 6320, loss = 1.06, batch loss = 1.00 (38.7 examples/sec; 0.207 sec/batch; 18h:43m:01s remains)
INFO - root - 2017-12-05 08:59:39.113701: step 6330, loss = 1.09, batch loss = 1.03 (38.5 examples/sec; 0.208 sec/batch; 18h:48m:56s remains)
INFO - root - 2017-12-05 08:59:41.206629: step 6340, loss = 1.20, batch loss = 1.14 (36.8 examples/sec; 0.217 sec/batch; 19h:41m:13s remains)
INFO - root - 2017-12-05 08:59:43.277635: step 6350, loss = 1.14, batch loss = 1.09 (38.8 examples/sec; 0.206 sec/batch; 18h:42m:01s remains)
INFO - root - 2017-12-05 08:59:45.369459: step 6360, loss = 1.35, batch loss = 1.29 (38.9 examples/sec; 0.206 sec/batch; 18h:37m:32s remains)
INFO - root - 2017-12-05 08:59:47.448352: step 6370, loss = 1.54, batch loss = 1.48 (38.0 examples/sec; 0.211 sec/batch; 19h:04m:36s remains)
INFO - root - 2017-12-05 08:59:49.542616: step 6380, loss = 1.23, batch loss = 1.17 (39.4 examples/sec; 0.203 sec/batch; 18h:23m:27s remains)
INFO - root - 2017-12-05 08:59:51.638864: step 6390, loss = 1.17, batch loss = 1.11 (37.5 examples/sec; 0.213 sec/batch; 19h:20m:02s remains)
INFO - root - 2017-12-05 08:59:53.734924: step 6400, loss = 1.00, batch loss = 0.94 (37.8 examples/sec; 0.212 sec/batch; 19h:10m:17s remains)
2017-12-05 08:59:54.011027: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2619019 -4.2576056 -4.2438369 -4.2239914 -4.2061939 -4.1904135 -4.1764226 -4.1578674 -4.1341171 -4.11184 -4.099544 -4.1010389 -4.11168 -4.1166368 -4.1058435][-4.3035941 -4.2868814 -4.2575274 -4.2256184 -4.2021394 -4.1873264 -4.1805015 -4.1706743 -4.1522703 -4.133235 -4.1242533 -4.1314888 -4.1499944 -4.165514 -4.1676855][-4.3049521 -4.2770624 -4.2342181 -4.1917896 -4.1615424 -4.1417966 -4.1334515 -4.1255112 -4.1112046 -4.100275 -4.1038389 -4.1281571 -4.1660175 -4.200099 -4.2201281][-4.2385988 -4.1991944 -4.1446419 -4.0928125 -4.0552011 -4.0268106 -4.0105014 -3.9994018 -3.9909966 -3.9968483 -4.0255527 -4.0814123 -4.1503444 -4.2102017 -4.2505703][-4.0971074 -4.0483241 -3.985435 -3.9267762 -3.8803403 -3.839412 -3.8119979 -3.8005927 -3.806211 -3.838366 -3.9026458 -3.9993727 -4.1065674 -4.1955047 -4.2559419][-3.9103234 -3.8558044 -3.7893348 -3.7250273 -3.6667793 -3.6109746 -3.574445 -3.5686178 -3.5973144 -3.6639333 -3.7678347 -3.9045579 -4.0475349 -4.1633415 -4.2411361][-3.7449863 -3.6909475 -3.6251485 -3.5524263 -3.4772637 -3.4048233 -3.3618612 -3.3655596 -3.4206791 -3.5221367 -3.660424 -3.8253083 -3.9908206 -4.1228962 -4.2099113][-3.6731787 -3.6244228 -3.5626247 -3.4831898 -3.3927083 -3.3079228 -3.26157 -3.2718916 -3.3446009 -3.4678342 -3.6208291 -3.791666 -3.9584336 -4.0888085 -4.1711078][-3.7155414 -3.6743722 -3.6202877 -3.5428669 -3.4503708 -3.3664804 -3.3216166 -3.3318415 -3.4047546 -3.5256605 -3.6689472 -3.8231945 -3.9704554 -4.0817389 -4.1465878][-3.8462491 -3.812258 -3.7667675 -3.6997857 -3.6219225 -3.5540738 -3.5181274 -3.5252211 -3.5821152 -3.6800466 -3.7960126 -3.9175434 -4.0309048 -4.1125731 -4.1545377][-4.015677 -3.9889076 -3.9524081 -3.9001102 -3.8430059 -3.795661 -3.7708457 -3.7742453 -3.8116689 -3.8793163 -3.9605055 -4.0443082 -4.1198773 -4.1710453 -4.1925416][-4.1749969 -4.1562705 -4.1304832 -4.095376 -4.0584812 -4.0283232 -4.0116768 -4.0109148 -4.0305037 -4.069685 -4.1183176 -4.1679077 -4.2106066 -4.2368708 -4.2439065][-4.2892604 -4.2777324 -4.2621446 -4.2424045 -4.2219543 -4.2044992 -4.1925812 -4.1885095 -4.19503 -4.2134767 -4.2385449 -4.2634869 -4.2834191 -4.2937922 -4.2928166][-4.349937 -4.3439221 -4.3362894 -4.3273964 -4.3185186 -4.3103008 -4.3030391 -4.2982216 -4.2976332 -4.3033972 -4.3134217 -4.3239369 -4.3316512 -4.3346591 -4.3317842][-4.3732677 -4.3713837 -4.3686929 -4.3655753 -4.3626275 -4.3598003 -4.3565612 -4.3532705 -4.3506603 -4.3507452 -4.3531365 -4.3562646 -4.35864 -4.3593249 -4.357481]]...]
INFO - root - 2017-12-05 08:59:56.105345: step 6410, loss = 1.11, batch loss = 1.05 (35.1 examples/sec; 0.228 sec/batch; 20h:37m:22s remains)
INFO - root - 2017-12-05 08:59:58.206965: step 6420, loss = 1.36, batch loss = 1.30 (38.8 examples/sec; 0.206 sec/batch; 18h:40m:18s remains)
INFO - root - 2017-12-05 09:00:00.254895: step 6430, loss = 1.32, batch loss = 1.26 (39.4 examples/sec; 0.203 sec/batch; 18h:22m:09s remains)
INFO - root - 2017-12-05 09:00:02.339308: step 6440, loss = 1.27, batch loss = 1.21 (39.0 examples/sec; 0.205 sec/batch; 18h:35m:04s remains)
INFO - root - 2017-12-05 09:00:04.427033: step 6450, loss = 1.19, batch loss = 1.13 (37.4 examples/sec; 0.214 sec/batch; 19h:22m:34s remains)
INFO - root - 2017-12-05 09:00:06.474455: step 6460, loss = 1.15, batch loss = 1.09 (39.0 examples/sec; 0.205 sec/batch; 18h:34m:21s remains)
INFO - root - 2017-12-05 09:00:08.554242: step 6470, loss = 1.34, batch loss = 1.28 (37.8 examples/sec; 0.212 sec/batch; 19h:10m:51s remains)
INFO - root - 2017-12-05 09:00:10.689334: step 6480, loss = 1.54, batch loss = 1.48 (36.9 examples/sec; 0.217 sec/batch; 19h:38m:26s remains)
INFO - root - 2017-12-05 09:00:12.757038: step 6490, loss = 1.33, batch loss = 1.27 (39.3 examples/sec; 0.204 sec/batch; 18h:26m:22s remains)
INFO - root - 2017-12-05 09:00:14.845326: step 6500, loss = 1.04, batch loss = 0.98 (38.3 examples/sec; 0.209 sec/batch; 18h:54m:18s remains)
2017-12-05 09:00:15.121619: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.8097022 -1.0340252 -0.33259296 0.21819448 0.592401 0.8599925 1.1940856 1.7458339 2.5208058 3.3503156 4.0722523 4.5737295 4.7233572 4.4344654 3.7333298][-1.4211795 -0.5713613 0.16042185 0.69344234 1.0086617 1.2005858 1.4496956 1.9084888 2.5967464 3.3661175 4.0612812 4.5521984 4.7149858 4.45566 3.796113][-1.0281239 -0.09402132 0.67058134 1.1787019 1.4227824 1.5143237 1.642477 1.9532847 2.4818382 3.1135435 3.7053585 4.132802 4.2820292 4.0686445 3.4918652][-0.678746 0.34782839 1.1611109 1.6719728 1.8793683 1.9037523 1.9345427 2.1111507 2.4741526 2.9332056 3.3703618 3.6789312 3.7720757 3.5773807 3.0813284][-0.45786858 0.64487696 1.5253763 2.0906663 2.3324924 2.3643179 2.3630295 2.4616537 2.6917453 2.9787064 3.2291899 3.3658948 3.3427563 3.1123557 2.6628237][-0.3947649 0.76408434 1.7258129 2.3976655 2.74921 2.8700867 2.908586 2.9836187 3.1182384 3.24341 3.2950444 3.2380824 3.0619087 2.7485209 2.3065786][-0.49172211 0.69915485 1.7388167 2.5355606 3.0308967 3.2904563 3.4246235 3.5140533 3.5705314 3.5420666 3.408752 3.1709652 2.8479595 2.4530697 2.0142288][-0.71613955 0.469944 1.5526557 2.4423518 3.0598831 3.4414144 3.6661243 3.7713652 3.7558575 3.6017008 3.3283391 2.9639211 2.5474396 2.1140051 1.6986761][-1.0757563 0.03470993 1.0864301 1.990253 2.6565289 3.0976977 3.3665953 3.4663849 3.3948202 3.1723566 2.8415108 2.442574 2.0235977 1.6243687 1.272963][-1.5650499 -0.60514832 0.32541943 1.1476917 1.7719717 2.1949954 2.4487572 2.5264888 2.4323206 2.2031827 1.8894405 1.5342007 1.1868353 0.87668276 0.62004042][-2.1654465 -1.4096503 -0.66908932 -0.0069861412 0.50834179 0.856102 1.0585775 1.1080294 1.0181737 0.82840633 0.58320904 0.32217693 0.081947327 -0.12112665 -0.28164244][-2.8042428 -2.2712073 -1.7455153 -1.2751436 -0.90542483 -0.65828109 -0.51987624 -0.49811792 -0.57154369 -0.7036767 -0.86260104 -1.0181043 -1.1508491 -1.2543969 -1.3320439][-3.38772 -3.0596604 -2.7346885 -2.4468436 -2.2206278 -2.0726845 -1.9931169 -1.9919133 -2.0450187 -2.1260107 -2.214479 -2.2911732 -2.3494532 -2.3848913 -2.4072297][-3.8559365 -3.6864512 -3.5173495 -3.37016 -3.2573385 -3.1853042 -3.1478057 -3.155519 -3.1879163 -3.23038 -3.2733033 -3.3056326 -3.3255012 -3.3293345 -3.3259177][-4.1585894 -4.087111 -4.0154934 -3.9532 -3.9075661 -3.8804386 -3.8658433 -3.8731127 -3.8897572 -3.9098611 -3.9305089 -3.9439545 -3.950808 -3.949286 -3.9429114]]...]
INFO - root - 2017-12-05 09:00:17.225652: step 6510, loss = 1.16, batch loss = 1.11 (38.7 examples/sec; 0.206 sec/batch; 18h:41m:56s remains)
INFO - root - 2017-12-05 09:00:19.284231: step 6520, loss = 1.25, batch loss = 1.19 (39.1 examples/sec; 0.205 sec/batch; 18h:31m:21s remains)
INFO - root - 2017-12-05 09:00:21.363662: step 6530, loss = 1.39, batch loss = 1.33 (38.1 examples/sec; 0.210 sec/batch; 19h:02m:13s remains)
INFO - root - 2017-12-05 09:00:23.454631: step 6540, loss = 1.33, batch loss = 1.27 (37.6 examples/sec; 0.213 sec/batch; 19h:14m:55s remains)
INFO - root - 2017-12-05 09:00:25.546643: step 6550, loss = 1.41, batch loss = 1.35 (38.6 examples/sec; 0.207 sec/batch; 18h:46m:29s remains)
INFO - root - 2017-12-05 09:00:27.638862: step 6560, loss = 1.35, batch loss = 1.29 (38.9 examples/sec; 0.206 sec/batch; 18h:37m:11s remains)
INFO - root - 2017-12-05 09:00:29.736189: step 6570, loss = 0.91, batch loss = 0.86 (36.8 examples/sec; 0.217 sec/batch; 19h:39m:40s remains)
INFO - root - 2017-12-05 09:00:31.793132: step 6580, loss = 1.26, batch loss = 1.20 (39.3 examples/sec; 0.203 sec/batch; 18h:24m:46s remains)
INFO - root - 2017-12-05 09:00:33.852360: step 6590, loss = 1.30, batch loss = 1.24 (38.0 examples/sec; 0.211 sec/batch; 19h:03m:30s remains)
INFO - root - 2017-12-05 09:00:35.913641: step 6600, loss = 1.10, batch loss = 1.05 (38.9 examples/sec; 0.206 sec/batch; 18h:37m:59s remains)
2017-12-05 09:00:36.185034: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.2096074 -2.2701666 -1.0842264 0.23897266 1.5153937 2.6027288 3.4552121 4.0494647 4.4341512 4.6727996 4.7851005 4.7617879 4.6038976 4.3935437 4.1945634][-3.0317621 -1.9464991 -0.57592773 0.94552183 2.3930454 3.5866022 4.4817634 5.0818877 5.45423 5.6550155 5.7161012 5.6475911 5.4702554 5.2783179 5.12503][-2.9465742 -1.7913947 -0.33619976 1.2686973 2.7819877 4.0049253 4.87979 5.4277425 5.7319331 5.85514 5.8440938 5.7270541 5.5447106 5.3909755 5.3068719][-2.937974 -1.7749186 -0.31421041 1.2790723 2.7655244 3.9504609 4.7584033 5.2081084 5.3989606 5.4192491 5.335568 5.186131 5.0158172 4.9019313 4.8806806][-2.9720221 -1.8271973 -0.39540863 1.1434045 2.5560255 3.6611114 4.3702664 4.6952214 4.7505183 4.6496062 4.4810796 4.3012023 4.1396747 4.0579944 4.0813918][-3.0114627 -1.8865733 -0.48956418 0.99379873 2.3312802 3.3551989 3.9717269 4.1780429 4.0994344 3.8753524 3.6209474 3.3999805 3.2372561 3.1714511 3.2191572][-3.0251095 -1.9126327 -0.53772306 0.90932131 2.2015181 3.1817493 3.7457156 3.8684688 3.6777921 3.3406253 2.9961071 2.7258291 2.552217 2.4976025 2.5679016][-3.0117664 -1.9078217 -0.54679632 0.88006258 2.1580706 3.1397853 3.7057309 3.8058486 3.5580626 3.1454659 2.7264571 2.4018798 2.2063928 2.154551 2.2461114][-2.9995241 -1.9164255 -0.58355713 0.81542683 2.0805402 3.0732775 3.6700721 3.8070207 3.5785751 3.155818 2.7104998 2.3621025 2.1591096 2.1155386 2.2292619][-2.9961352 -1.9577882 -0.68436909 0.66024351 1.8875198 2.8668299 3.4869885 3.6830678 3.5230947 3.1492524 2.7350011 2.41821 2.2580056 2.2623634 2.413919][-3.0162454 -2.0443971 -0.85785532 0.39801884 1.5492072 2.4723692 3.0809398 3.3220472 3.2408838 2.9540005 2.6322508 2.4137912 2.3475761 2.4236536 2.6131902][-3.0982783 -2.2215915 -1.153609 -0.02757597 1.0015168 1.819612 2.3713126 2.6241689 2.6191378 2.4533134 2.2734003 2.1973681 2.2511506 2.4036903 2.614779][-3.2556293 -2.5056462 -1.5911376 -0.63087606 0.23869371 0.91570711 1.3703618 1.5971279 1.6462779 1.602345 1.5729551 1.6372771 1.8057618 2.0300608 2.2538433][-3.4642835 -2.8610411 -2.1242876 -1.3520842 -0.6641922 -0.14468861 0.19484186 0.36495352 0.43138933 0.46886826 0.5539484 0.72024536 0.96459723 1.2330523 1.4579768][-3.6842186 -3.232903 -2.6827655 -2.1111393 -1.6122756 -1.2500572 -1.0216494 -0.90958786 -0.85119295 -0.77861 -0.6413722 -0.437881 -0.17889929 0.08374691 0.28509235]]...]
INFO - root - 2017-12-05 09:00:38.255386: step 6610, loss = 1.06, batch loss = 1.01 (38.9 examples/sec; 0.206 sec/batch; 18h:36m:55s remains)
INFO - root - 2017-12-05 09:00:40.359108: step 6620, loss = 0.99, batch loss = 0.93 (38.8 examples/sec; 0.206 sec/batch; 18h:39m:02s remains)
INFO - root - 2017-12-05 09:00:42.460405: step 6630, loss = 1.17, batch loss = 1.11 (38.3 examples/sec; 0.209 sec/batch; 18h:55m:02s remains)
INFO - root - 2017-12-05 09:00:44.542570: step 6640, loss = 1.32, batch loss = 1.26 (37.8 examples/sec; 0.212 sec/batch; 19h:08m:54s remains)
INFO - root - 2017-12-05 09:00:46.683253: step 6650, loss = 0.95, batch loss = 0.89 (38.3 examples/sec; 0.209 sec/batch; 18h:53m:55s remains)
INFO - root - 2017-12-05 09:00:48.767568: step 6660, loss = 1.59, batch loss = 1.53 (38.6 examples/sec; 0.207 sec/batch; 18h:45m:29s remains)
INFO - root - 2017-12-05 09:00:50.864764: step 6670, loss = 1.04, batch loss = 0.98 (40.0 examples/sec; 0.200 sec/batch; 18h:05m:42s remains)
INFO - root - 2017-12-05 09:00:52.946991: step 6680, loss = 1.13, batch loss = 1.07 (38.5 examples/sec; 0.208 sec/batch; 18h:46m:56s remains)
INFO - root - 2017-12-05 09:00:55.097861: step 6690, loss = 1.04, batch loss = 0.98 (37.8 examples/sec; 0.212 sec/batch; 19h:08m:51s remains)
INFO - root - 2017-12-05 09:00:57.168134: step 6700, loss = 1.07, batch loss = 1.01 (38.0 examples/sec; 0.211 sec/batch; 19h:03m:43s remains)
2017-12-05 09:00:57.435669: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.7262425 -2.4674358 -2.2664015 -2.096158 -1.9427791 -1.8255544 -1.7819202 -1.8204341 -1.9186468 -2.0061038 -2.0478778 -2.0481448 -2.061317 -2.0938826 -2.16307][-2.5769296 -2.3239496 -2.1399069 -1.994071 -1.8639262 -1.7669196 -1.7374349 -1.7825749 -1.8724587 -1.9424715 -1.9703014 -1.9786687 -2.0139453 -2.092967 -2.2169559][-2.5469635 -2.3168523 -2.1601167 -2.0399694 -1.9343064 -1.8532639 -1.8232677 -1.8595052 -1.930712 -1.9834907 -2.0025303 -2.0182955 -2.0756924 -2.1949508 -2.36145][-2.5556741 -2.3472745 -2.2169197 -2.1132545 -2.0188718 -1.9356215 -1.8892186 -1.9069719 -1.9680457 -2.0254288 -2.0622604 -2.1044588 -2.1900916 -2.3353448 -2.5191205][-2.5634313 -2.3692803 -2.2591479 -2.1668599 -2.0704565 -1.9718049 -1.899358 -1.8945465 -1.9554222 -2.0416975 -2.1236148 -2.2087002 -2.3252 -2.4842715 -2.6673019][-2.5426092 -2.3510993 -2.2520163 -2.1652238 -2.0632844 -1.9466476 -1.8539915 -1.8373873 -1.9116931 -2.0417364 -2.1776872 -2.3084192 -2.4506707 -2.6164007 -2.7920656][-2.4982867 -2.3020132 -2.2042444 -2.117718 -2.0132174 -1.8931983 -1.8034661 -1.798619 -1.8966131 -2.0621188 -2.2378125 -2.3994799 -2.556659 -2.7204132 -2.8870952][-2.4550505 -2.255954 -2.1558723 -2.0698698 -1.9762487 -1.8774695 -1.8151364 -1.8397303 -1.9595218 -2.1407633 -2.3311555 -2.50157 -2.65919 -2.8146443 -2.9721179][-2.4622264 -2.2783573 -2.1864831 -2.1087267 -2.0377669 -1.9748402 -1.9500983 -2.0076013 -2.1426749 -2.3221529 -2.5052443 -2.665709 -2.8109393 -2.951304 -3.0960608][-2.5852931 -2.4332829 -2.3566513 -2.2917805 -2.2428339 -2.2133245 -2.223511 -2.3011534 -2.4362836 -2.6021483 -2.7675118 -2.9101763 -3.038497 -3.1601305 -3.2839732][-2.8460641 -2.7321177 -2.6685667 -2.6152134 -2.579844 -2.5731575 -2.6058183 -2.6886048 -2.8107095 -2.9521894 -3.0944161 -3.2210379 -3.332443 -3.4329839 -3.5270569][-3.211869 -3.1285825 -3.0767148 -3.0369978 -3.0133886 -3.0183368 -3.0559196 -3.1276097 -3.2248812 -3.3345141 -3.4485109 -3.5541859 -3.6465604 -3.7241359 -3.7868371][-3.6140606 -3.5550904 -3.5170083 -3.4920502 -3.4822044 -3.4889669 -3.5156446 -3.5629904 -3.6281095 -3.7034607 -3.7856388 -3.8644485 -3.933162 -3.9862404 -4.0227652][-3.9598188 -3.9228492 -3.9001997 -3.8884645 -3.8870468 -3.8922334 -3.9054377 -3.9298253 -3.9666195 -4.0107365 -4.0599809 -4.1084628 -4.1515117 -4.1829896 -4.2023044][-4.1958489 -4.1770668 -4.1664529 -4.1628976 -4.1642118 -4.16685 -4.1717777 -4.1813 -4.1976619 -4.2180357 -4.241632 -4.2649727 -4.2861438 -4.3021803 -4.3122611]]...]
INFO - root - 2017-12-05 09:00:59.503170: step 6710, loss = 1.45, batch loss = 1.39 (38.4 examples/sec; 0.208 sec/batch; 18h:50m:24s remains)
INFO - root - 2017-12-05 09:01:01.607184: step 6720, loss = 1.26, batch loss = 1.20 (38.7 examples/sec; 0.207 sec/batch; 18h:43m:33s remains)
INFO - root - 2017-12-05 09:01:03.675712: step 6730, loss = 1.27, batch loss = 1.21 (39.0 examples/sec; 0.205 sec/batch; 18h:34m:24s remains)
INFO - root - 2017-12-05 09:01:05.793140: step 6740, loss = 1.24, batch loss = 1.18 (37.5 examples/sec; 0.213 sec/batch; 19h:18m:08s remains)
INFO - root - 2017-12-05 09:01:07.873324: step 6750, loss = 1.19, batch loss = 1.14 (38.7 examples/sec; 0.207 sec/batch; 18h:42m:43s remains)
INFO - root - 2017-12-05 09:01:09.986033: step 6760, loss = 1.17, batch loss = 1.11 (37.6 examples/sec; 0.213 sec/batch; 19h:13m:53s remains)
INFO - root - 2017-12-05 09:01:12.081256: step 6770, loss = 1.19, batch loss = 1.13 (39.3 examples/sec; 0.204 sec/batch; 18h:25m:42s remains)
INFO - root - 2017-12-05 09:01:14.179252: step 6780, loss = 1.10, batch loss = 1.04 (38.5 examples/sec; 0.208 sec/batch; 18h:47m:05s remains)
INFO - root - 2017-12-05 09:01:16.222802: step 6790, loss = 1.35, batch loss = 1.29 (38.8 examples/sec; 0.206 sec/batch; 18h:39m:49s remains)
INFO - root - 2017-12-05 09:01:18.304500: step 6800, loss = 1.21, batch loss = 1.15 (38.6 examples/sec; 0.207 sec/batch; 18h:45m:26s remains)
2017-12-05 09:01:18.591282: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.7887323 -3.2243724 -2.56145 -1.9459791 -1.5060456 -1.2721722 -1.2040501 -1.2207093 -1.2604027 -1.253505 -1.1989374 -1.1271725 -1.0842819 -1.0815327 -1.1009629][-3.7036405 -3.0684216 -2.3141074 -1.5998652 -1.0670447 -0.7503736 -0.61802053 -0.58654451 -0.58989024 -0.55789852 -0.49017882 -0.41804361 -0.38034868 -0.38317871 -0.39865351][-3.6291096 -2.9567406 -2.1595397 -1.4040041 -0.82693267 -0.45924807 -0.27050495 -0.18054914 -0.1313591 -0.066173553 0.016539097 0.091979027 0.12707472 0.12161827 0.11264181][-3.5156531 -2.7978725 -1.9702427 -1.2025216 -0.62748146 -0.26553535 -0.069830894 0.040807724 0.1238246 0.21319199 0.30744076 0.38417387 0.41157866 0.39397526 0.37410212][-3.3541718 -2.5760107 -1.7167726 -0.95734596 -0.42357707 -0.12090111 0.023334503 0.10121298 0.16982746 0.24382353 0.3207798 0.37503147 0.37134695 0.32211065 0.27181292][-3.1746311 -2.3286057 -1.4334142 -0.69474125 -0.22192144 -0.00098896027 0.058740616 0.05809927 0.056642056 0.062024117 0.067295551 0.047153473 -0.02358532 -0.12866735 -0.22891855][-3.0254469 -2.1263688 -1.2050638 -0.49125004 -0.079061031 0.068163395 0.044111729 -0.050870895 -0.1542573 -0.25840855 -0.37226486 -0.50831318 -0.67562389 -0.85845423 -1.0223179][-2.9457362 -2.0197558 -1.096504 -0.41505122 -0.057565212 0.032692909 -0.046904087 -0.21892738 -0.41691375 -0.62971354 -0.86807728 -1.1285684 -1.4037716 -1.6727686 -1.9046292][-2.9653213 -2.0556345 -1.1656818 -0.52876711 -0.21418953 -0.15339708 -0.25333786 -0.45531178 -0.70219135 -0.98144674 -1.3035836 -1.6549089 -2.0149899 -2.3557489 -2.6473718][-3.0891876 -2.2561998 -1.4501164 -0.88157821 -0.60354853 -0.55119514 -0.64501238 -0.83488107 -1.0742333 -1.3535645 -1.6859353 -2.0586565 -2.4499893 -2.8264666 -3.1553361][-3.3124671 -2.6183884 -1.9492369 -1.4787648 -1.2426128 -1.1907198 -1.2651482 -1.4182413 -1.6078818 -1.8298986 -2.1033762 -2.4232779 -2.7759838 -3.1294718 -3.4518795][-3.5959041 -3.0809417 -2.5844145 -2.2339811 -2.0494733 -2.0031252 -2.0535896 -2.1604204 -2.2843175 -2.4265025 -2.6094577 -2.836992 -3.1048994 -3.3889422 -3.6624088][-3.8822169 -3.5508657 -3.2302551 -3.0048711 -2.8809524 -2.847564 -2.8797715 -2.9463093 -3.0165896 -3.0932989 -3.1962194 -3.3304834 -3.49816 -3.6863971 -3.8761356][-4.1111321 -3.9286847 -3.7529438 -3.6310399 -3.563972 -3.5493662 -3.5737815 -3.6180916 -3.6590862 -3.6985888 -3.7491736 -3.8141878 -3.8988607 -3.9964046 -4.0974941][-4.2644081 -4.181766 -4.1012659 -4.0446696 -4.014555 -4.0121107 -4.0284185 -4.0541921 -4.0785608 -4.1014671 -4.1257453 -4.1532559 -4.1870589 -4.224483 -4.2642956]]...]
INFO - root - 2017-12-05 09:01:20.702378: step 6810, loss = 1.07, batch loss = 1.01 (37.4 examples/sec; 0.214 sec/batch; 19h:19m:53s remains)
INFO - root - 2017-12-05 09:01:22.765464: step 6820, loss = 1.08, batch loss = 1.02 (37.3 examples/sec; 0.214 sec/batch; 19h:23m:33s remains)
INFO - root - 2017-12-05 09:01:24.839224: step 6830, loss = 1.04, batch loss = 0.98 (38.8 examples/sec; 0.206 sec/batch; 18h:38m:21s remains)
INFO - root - 2017-12-05 09:01:26.917518: step 6840, loss = 1.03, batch loss = 0.97 (38.8 examples/sec; 0.206 sec/batch; 18h:38m:29s remains)
INFO - root - 2017-12-05 09:01:29.001692: step 6850, loss = 1.36, batch loss = 1.30 (38.0 examples/sec; 0.211 sec/batch; 19h:03m:28s remains)
INFO - root - 2017-12-05 09:01:31.081831: step 6860, loss = 1.43, batch loss = 1.37 (38.6 examples/sec; 0.207 sec/batch; 18h:44m:57s remains)
INFO - root - 2017-12-05 09:01:33.158141: step 6870, loss = 1.10, batch loss = 1.04 (37.7 examples/sec; 0.212 sec/batch; 19h:13m:06s remains)
INFO - root - 2017-12-05 09:01:35.256229: step 6880, loss = 1.71, batch loss = 1.65 (38.0 examples/sec; 0.210 sec/batch; 19h:01m:55s remains)
INFO - root - 2017-12-05 09:01:37.317662: step 6890, loss = 1.29, batch loss = 1.23 (38.6 examples/sec; 0.207 sec/batch; 18h:45m:09s remains)
INFO - root - 2017-12-05 09:01:39.412949: step 6900, loss = 1.48, batch loss = 1.42 (38.2 examples/sec; 0.209 sec/batch; 18h:56m:25s remains)
2017-12-05 09:01:39.699962: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.6048939 -3.242084 -2.8714867 -2.5407181 -2.3081987 -2.2073815 -2.2402167 -2.3838298 -2.5861039 -2.7723184 -2.8894086 -2.9094062 -2.8439078 -2.71924 -2.6086824][-3.4041288 -2.9186304 -2.4131973 -1.9536409 -1.6141098 -1.4517167 -1.4772022 -1.6660366 -1.9316487 -2.1737442 -2.3273022 -2.3605883 -2.2957783 -2.165894 -2.056318][-3.3030589 -2.7278624 -2.1152427 -1.5477505 -1.112541 -0.88306618 -0.87896132 -1.080792 -1.3856006 -1.672493 -1.871732 -1.9430277 -1.9052432 -1.7924356 -1.6958814][-3.3073437 -2.6891556 -2.0098932 -1.3619151 -0.83928514 -0.52190757 -0.44688559 -0.61430049 -0.93300438 -1.2626312 -1.522985 -1.6604741 -1.6827602 -1.6154203 -1.5429404][-3.3845191 -2.7625971 -2.0512569 -1.3484616 -0.74503207 -0.31995058 -0.13764143 -0.24086809 -0.55872059 -0.93594 -1.2753866 -1.5046146 -1.6177456 -1.6246903 -1.5977089][-3.4971552 -2.8906221 -2.1674376 -1.4168329 -0.72664452 -0.17773438 0.12231445 0.076254845 -0.24498892 -0.67580175 -1.1114202 -1.4612927 -1.6974099 -1.8100226 -1.8538213][-3.5869942 -2.9913354 -2.251024 -1.4445212 -0.65621829 0.0128479 0.40438414 0.38225985 0.041258812 -0.45329332 -0.99416709 -1.4766359 -1.8451266 -2.0723193 -2.1967943][-3.6259098 -3.0236518 -2.2566876 -1.3954475 -0.52694654 0.22082663 0.6577239 0.64503 0.28854704 -0.26097441 -0.89842367 -1.5018139 -1.9883051 -2.3158896 -2.508585][-3.6260219 -3.0046115 -2.2090263 -1.3112481 -0.41031241 0.34721756 0.78144407 0.78259277 0.4243331 -0.16214132 -0.8623383 -1.5418403 -2.0997753 -2.4836545 -2.714045][-3.619741 -2.9860775 -2.1740878 -1.2622583 -0.35978746 0.37364864 0.78372574 0.77739811 0.41238117 -0.19256735 -0.91801453 -1.6247871 -2.2000213 -2.5944719 -2.8347642][-3.6494994 -3.039959 -2.255542 -1.3727696 -0.50725293 0.18115854 0.55911255 0.53647947 0.17176008 -0.42277765 -1.1273015 -1.8021092 -2.34263 -2.7133336 -2.9469151][-3.7238181 -3.1809349 -2.4780407 -1.6824355 -0.9066608 -0.29217052 0.040707111 0.010800838 -0.32876015 -0.87246871 -1.5057926 -2.1016731 -2.5741439 -2.8993359 -3.1076839][-3.8446095 -3.4008565 -2.8211145 -2.1561072 -1.5049696 -0.98448157 -0.7056098 -0.7379818 -1.0320053 -1.4926376 -2.0221167 -2.5118537 -2.8974516 -3.1606424 -3.3272657][-3.9847116 -3.6552262 -3.2189317 -2.7135048 -2.2074692 -1.7908072 -1.5648241 -1.5896952 -1.8213999 -2.1821647 -2.5921698 -2.9661152 -3.2581689 -3.452179 -3.5705209][-4.1178432 -3.8991542 -3.6072776 -3.2659845 -2.9157457 -2.6152754 -2.4434404 -2.4520078 -2.6071591 -2.8580842 -3.1458647 -3.4076576 -3.6108079 -3.7410965 -3.8175557]]...]
INFO - root - 2017-12-05 09:01:41.785880: step 6910, loss = 1.22, batch loss = 1.16 (37.9 examples/sec; 0.211 sec/batch; 19h:04m:44s remains)
INFO - root - 2017-12-05 09:01:43.869247: step 6920, loss = 1.06, batch loss = 1.01 (37.7 examples/sec; 0.212 sec/batch; 19h:11m:41s remains)
INFO - root - 2017-12-05 09:01:45.948189: step 6930, loss = 1.34, batch loss = 1.28 (39.0 examples/sec; 0.205 sec/batch; 18h:34m:10s remains)
INFO - root - 2017-12-05 09:01:48.013813: step 6940, loss = 1.42, batch loss = 1.36 (38.9 examples/sec; 0.205 sec/batch; 18h:35m:00s remains)
INFO - root - 2017-12-05 09:01:50.164922: step 6950, loss = 1.30, batch loss = 1.24 (34.3 examples/sec; 0.233 sec/batch; 21h:05m:47s remains)
INFO - root - 2017-12-05 09:01:52.266275: step 6960, loss = 1.28, batch loss = 1.22 (37.9 examples/sec; 0.211 sec/batch; 19h:06m:37s remains)
INFO - root - 2017-12-05 09:01:54.342844: step 6970, loss = 1.33, batch loss = 1.27 (38.3 examples/sec; 0.209 sec/batch; 18h:53m:02s remains)
INFO - root - 2017-12-05 09:01:56.410644: step 6980, loss = 1.15, batch loss = 1.09 (37.1 examples/sec; 0.215 sec/batch; 19h:28m:35s remains)
INFO - root - 2017-12-05 09:01:58.483595: step 6990, loss = 1.21, batch loss = 1.15 (38.8 examples/sec; 0.206 sec/batch; 18h:39m:21s remains)
INFO - root - 2017-12-05 09:02:00.615453: step 7000, loss = 1.22, batch loss = 1.16 (38.9 examples/sec; 0.206 sec/batch; 18h:36m:19s remains)
2017-12-05 09:02:00.903383: I tensorflow/core/kernels/logging_ops.cc:79] [[[1.9413533 2.386281 2.6724491 2.9739351 3.3218055 3.6818013 4.0369906 4.3677793 4.6351323 4.7936635 4.8055797 4.6874232 4.4474721 4.092545 3.6348605][2.289557 2.8358774 3.2094393 3.5648556 3.9300914 4.2819314 4.6170449 4.9305215 5.1834321 5.334228 5.3473811 5.245007 5.0335641 4.691751 4.2321324][2.4503016 3.0581708 3.4944625 3.88419 4.2578588 4.5885348 4.87857 5.1273608 5.3067451 5.3997445 5.3892817 5.3098941 5.1451545 4.8623185 4.4682231][2.5993371 3.223649 3.6914477 4.1038938 4.4829431 4.7943997 5.0373349 5.205255 5.2720313 5.2602515 5.1874633 5.1044297 4.9691358 4.7402654 4.43899][2.8211617 3.4345155 3.9013572 4.3023849 4.6611667 4.9387689 5.1390061 5.2319236 5.1765018 5.0320134 4.8622966 4.7232108 4.562676 4.35381 4.136373][3.148283 3.7147002 4.1349311 4.4818826 4.7978616 5.0345092 5.202127 5.2398 5.0774369 4.8106151 4.5408969 4.3201871 4.1006503 3.870667 3.6900496][3.5635538 4.0714459 4.411984 4.6753421 4.9225783 5.1111369 5.2445121 5.2352576 4.9876 4.6283116 4.2823844 3.991364 3.7095046 3.4325719 3.2287188][3.9736686 4.4308438 4.6830583 4.8515763 5.0044141 5.114531 5.184978 5.1289964 4.8445268 4.46431 4.1055627 3.7947602 3.4818435 3.1615033 2.8984752][4.2061362 4.6544871 4.8484139 4.9284987 4.9726377 4.9816661 4.9664235 4.8627849 4.6000524 4.2704682 3.9652228 3.6915784 3.3989091 3.0738006 2.7730184][4.1914697 4.6440721 4.8048654 4.8137507 4.7609782 4.6779008 4.5863433 4.4497333 4.2301583 3.9834523 3.7724586 3.5850115 3.3693113 3.1059198 2.8320942][3.944675 4.3916287 4.5185323 4.4749284 4.3496327 4.2029395 4.0629506 3.9099822 3.7402711 3.5877037 3.4854207 3.4129639 3.3254457 3.1962438 3.0235848][3.5475798 3.9758487 4.0692048 3.9898477 3.819387 3.638042 3.4743366 3.3153563 3.1824412 3.1076493 3.1046824 3.1458168 3.1961279 3.2218118 3.1859097][3.0495729 3.4571609 3.5360284 3.44003 3.2460055 3.0512705 2.8835521 2.7323828 2.6259918 2.6032691 2.6673512 2.791997 2.9502048 3.0904627 3.1656446][2.5074711 2.9039426 2.9878716 2.8897057 2.6864581 2.4937692 2.3397713 2.2145724 2.1354613 2.1451206 2.2382312 2.392189 2.5884962 2.7710915 2.896781][1.9480605 2.3395066 2.43899 2.3549833 2.1596088 1.9835019 1.8582129 1.7721615 1.7227712 1.7446451 1.8265181 1.9498982 2.1138368 2.2659636 2.3745494]]...]
INFO - root - 2017-12-05 09:02:02.989518: step 7010, loss = 1.44, batch loss = 1.38 (37.2 examples/sec; 0.215 sec/batch; 19h:26m:13s remains)
INFO - root - 2017-12-05 09:02:05.049765: step 7020, loss = 1.23, batch loss = 1.18 (39.4 examples/sec; 0.203 sec/batch; 18h:21m:54s remains)
INFO - root - 2017-12-05 09:02:07.135810: step 7030, loss = 1.05, batch loss = 0.99 (38.8 examples/sec; 0.206 sec/batch; 18h:39m:49s remains)
INFO - root - 2017-12-05 09:02:09.229169: step 7040, loss = 1.07, batch loss = 1.02 (39.2 examples/sec; 0.204 sec/batch; 18h:25m:41s remains)
INFO - root - 2017-12-05 09:02:11.310246: step 7050, loss = 1.17, batch loss = 1.11 (38.6 examples/sec; 0.207 sec/batch; 18h:42m:48s remains)
INFO - root - 2017-12-05 09:02:13.391982: step 7060, loss = 1.22, batch loss = 1.16 (37.2 examples/sec; 0.215 sec/batch; 19h:27m:40s remains)
INFO - root - 2017-12-05 09:02:15.485722: step 7070, loss = 1.40, batch loss = 1.34 (39.5 examples/sec; 0.202 sec/batch; 18h:17m:41s remains)
INFO - root - 2017-12-05 09:02:17.557773: step 7080, loss = 1.39, batch loss = 1.33 (38.9 examples/sec; 0.206 sec/batch; 18h:35m:31s remains)
INFO - root - 2017-12-05 09:02:19.635202: step 7090, loss = 0.95, batch loss = 0.89 (39.3 examples/sec; 0.204 sec/batch; 18h:23m:51s remains)
INFO - root - 2017-12-05 09:02:21.734725: step 7100, loss = 1.33, batch loss = 1.28 (39.3 examples/sec; 0.204 sec/batch; 18h:24m:10s remains)
2017-12-05 09:02:22.029863: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.6019382 -3.6414642 -3.7452602 -3.8641603 -3.9736111 -4.0600934 -4.1290259 -4.1894188 -4.237401 -4.2650995 -4.2659879 -4.2446074 -4.20837 -4.1615119 -4.1127462][-3.5808616 -3.6288416 -3.7562973 -3.9071765 -4.0454931 -4.148725 -4.2197504 -4.2692714 -4.3032179 -4.3227754 -4.3274927 -4.3206215 -4.3049145 -4.2813363 -4.2540956][-3.5591602 -3.615695 -3.7550635 -3.9230864 -4.0781841 -4.1930537 -4.2656255 -4.3084078 -4.3328805 -4.3457479 -4.3516088 -4.3532853 -4.3507156 -4.344008 -4.3348193][-3.5554781 -3.6104848 -3.7375319 -3.8931134 -4.0396605 -4.1516519 -4.2231674 -4.2658787 -4.2916374 -4.3076143 -4.318295 -4.32568 -4.3312054 -4.3361254 -4.341373][-3.5490394 -3.5815353 -3.6676626 -3.7801359 -3.8959367 -3.9955707 -4.0692558 -4.123641 -4.1649561 -4.1964169 -4.2188854 -4.2340708 -4.2466736 -4.2613654 -4.281251][-3.5043011 -3.4963205 -3.5200148 -3.5682437 -3.6378462 -3.7193666 -3.8013465 -3.8802567 -3.9515967 -4.009573 -4.0490522 -4.0740805 -4.0942583 -4.1205015 -4.1584811][-3.419302 -3.3573122 -3.3149488 -3.299099 -3.3203011 -3.3775473 -3.4643524 -3.5715361 -3.6803303 -3.7699561 -3.8276188 -3.8598452 -3.8841834 -3.9206145 -3.9789343][-3.3191905 -3.2099614 -3.1209517 -3.0632682 -3.0553575 -3.0978904 -3.186708 -3.3134069 -3.4495473 -3.5618074 -3.630908 -3.6652603 -3.6868248 -3.7246189 -3.7951951][-3.2431855 -3.1098223 -3.0042529 -2.9316571 -2.9174695 -2.9636469 -3.0618072 -3.1984968 -3.3451695 -3.467515 -3.5438461 -3.5804927 -3.5982802 -3.6287982 -3.6940012][-3.2192483 -3.0805969 -2.9767292 -2.9030008 -2.8871269 -2.9333339 -3.0354984 -3.1787615 -3.3356681 -3.4745896 -3.5716724 -3.6268234 -3.6555886 -3.6828892 -3.7325411][-3.25141 -3.1258395 -3.0336225 -2.9609933 -2.9335756 -2.9635909 -3.0513253 -3.1899815 -3.3561921 -3.5202873 -3.6535704 -3.7479074 -3.8079348 -3.8480642 -3.8897982][-3.3429842 -3.2419252 -3.1621161 -3.0840731 -3.0317907 -3.027009 -3.0800197 -3.1962886 -3.3597355 -3.5438256 -3.7165813 -3.8576014 -3.9615846 -4.0287862 -4.0757861][-3.4754839 -3.4114983 -3.3436522 -3.2531643 -3.165062 -3.1115117 -3.115315 -3.1894057 -3.3300982 -3.5175219 -3.72021 -3.9048233 -4.05331 -4.154902 -4.2162189][-3.6064529 -3.5857348 -3.5399592 -3.443794 -3.3247886 -3.2220457 -3.1705174 -3.1917086 -3.292583 -3.4627502 -3.6763315 -3.8915515 -4.0766315 -4.2107091 -4.2903829][-3.6930308 -3.7200358 -3.7112496 -3.6285677 -3.4958947 -3.3569379 -3.2552195 -3.2223122 -3.2747779 -3.4127679 -3.6177089 -3.8472447 -4.056468 -4.2162971 -4.3143511]]...]
INFO - root - 2017-12-05 09:02:24.131483: step 7110, loss = 1.25, batch loss = 1.20 (36.4 examples/sec; 0.220 sec/batch; 19h:52m:51s remains)
INFO - root - 2017-12-05 09:02:26.191029: step 7120, loss = 1.25, batch loss = 1.19 (39.9 examples/sec; 0.200 sec/batch; 18h:06m:53s remains)
INFO - root - 2017-12-05 09:02:28.278265: step 7130, loss = 1.37, batch loss = 1.31 (39.2 examples/sec; 0.204 sec/batch; 18h:27m:20s remains)
INFO - root - 2017-12-05 09:02:30.378719: step 7140, loss = 1.31, batch loss = 1.25 (37.9 examples/sec; 0.211 sec/batch; 19h:04m:38s remains)
INFO - root - 2017-12-05 09:02:32.445567: step 7150, loss = 1.42, batch loss = 1.36 (36.8 examples/sec; 0.218 sec/batch; 19h:39m:40s remains)
INFO - root - 2017-12-05 09:02:34.498106: step 7160, loss = 1.30, batch loss = 1.24 (38.9 examples/sec; 0.206 sec/batch; 18h:34m:56s remains)
INFO - root - 2017-12-05 09:02:36.582648: step 7170, loss = 1.12, batch loss = 1.06 (39.5 examples/sec; 0.202 sec/batch; 18h:17m:52s remains)
INFO - root - 2017-12-05 09:02:38.684955: step 7180, loss = 1.77, batch loss = 1.72 (38.7 examples/sec; 0.207 sec/batch; 18h:39m:49s remains)
INFO - root - 2017-12-05 09:02:40.764613: step 7190, loss = 1.43, batch loss = 1.38 (38.2 examples/sec; 0.209 sec/batch; 18h:55m:41s remains)
INFO - root - 2017-12-05 09:02:42.835220: step 7200, loss = 1.01, batch loss = 0.95 (39.5 examples/sec; 0.203 sec/batch; 18h:19m:10s remains)
2017-12-05 09:02:43.090747: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3975582 -4.3973188 -4.3971338 -4.3968582 -4.3966174 -4.3965645 -4.3966045 -4.3966789 -4.3968387 -4.3971643 -4.3974419 -4.3973718 -4.3971267 -4.3967867 -4.39654][-4.3960042 -4.3941789 -4.3924103 -4.3905969 -4.3890648 -4.3880682 -4.3873777 -4.38714 -4.3874421 -4.3883815 -4.3897867 -4.391326 -4.3927469 -4.3937988 -4.3946056][-4.3933315 -4.3879914 -4.3818331 -4.375495 -4.37018 -4.3663459 -4.3641338 -4.3637629 -4.3650656 -4.3680182 -4.3726869 -4.3781996 -4.3837457 -4.3881631 -4.3914809][-4.3861179 -4.3719397 -4.354547 -4.3365445 -4.3222866 -4.3142786 -4.3126011 -4.3170209 -4.3253326 -4.3354788 -4.3474793 -4.3601356 -4.3719687 -4.3813853 -4.38813][-4.3675857 -4.3344593 -4.2931929 -4.251832 -4.2205095 -4.2063675 -4.2105651 -4.229733 -4.2563019 -4.2840285 -4.3109818 -4.3356371 -4.3565607 -4.3729558 -4.384366][-4.3322959 -4.265523 -4.1834931 -4.1038547 -4.0461626 -4.0247583 -4.0400429 -4.0832553 -4.1391559 -4.1946287 -4.2456679 -4.2897682 -4.326077 -4.3547635 -4.3750978][-4.2835236 -4.1732569 -4.0385551 -3.9083204 -3.8148355 -3.7822225 -3.8094759 -3.8785558 -3.9655681 -4.0523443 -4.1336946 -4.2066493 -4.2689095 -4.3195119 -4.3562613][-4.2367506 -4.0883093 -3.9065251 -3.72867 -3.5983446 -3.5510912 -3.5853786 -3.6704879 -3.7767262 -3.8862212 -3.9952352 -4.1000891 -4.1950636 -4.2743 -4.3320551][-4.2189317 -4.0595331 -3.8651845 -3.6751745 -3.534091 -3.4773393 -3.5027785 -3.5776377 -3.6751122 -3.7830484 -3.9005702 -4.0227284 -4.1392269 -4.2394757 -4.3132725][-4.2439709 -4.1095414 -3.9472992 -3.7908728 -3.6764178 -3.6259286 -3.6328995 -3.676965 -3.74171 -3.822572 -3.9208632 -4.0303478 -4.1399431 -4.2374082 -4.3109665][-4.2954373 -4.20601 -4.098208 -3.9944794 -3.9179668 -3.8806512 -3.8770168 -3.8961248 -3.9296958 -3.9778864 -4.0418606 -4.116776 -4.1957655 -4.2687464 -4.3260884][-4.3429737 -4.2959466 -4.2386761 -4.1828394 -4.1397223 -4.1159186 -4.1095524 -4.1162076 -4.1310086 -4.1548266 -4.188283 -4.2281051 -4.2720485 -4.3145828 -4.3496084][-4.3735032 -4.3544693 -4.3308315 -4.3064733 -4.2859116 -4.2722936 -4.2664142 -4.2675424 -4.2728477 -4.2828469 -4.297647 -4.314743 -4.3337374 -4.3528762 -4.3697453][-4.3889918 -4.3835173 -4.3768735 -4.3691688 -4.3616037 -4.3556 -4.3519268 -4.3507957 -4.3514724 -4.3545861 -4.3599443 -4.3656936 -4.37163 -4.3776388 -4.3837218][-4.3952603 -4.3946595 -4.3937597 -4.3922315 -4.3902469 -4.3883204 -4.3869586 -4.3862038 -4.3860011 -4.3864851 -4.3875141 -4.3886094 -4.389492 -4.3902807 -4.391592]]...]
INFO - root - 2017-12-05 09:02:45.186444: step 7210, loss = 1.29, batch loss = 1.23 (37.3 examples/sec; 0.214 sec/batch; 19h:22m:26s remains)
INFO - root - 2017-12-05 09:02:47.247391: step 7220, loss = 1.45, batch loss = 1.39 (40.2 examples/sec; 0.199 sec/batch; 17h:59m:47s remains)
INFO - root - 2017-12-05 09:02:49.320404: step 7230, loss = 1.23, batch loss = 1.17 (37.9 examples/sec; 0.211 sec/batch; 19h:03m:09s remains)
INFO - root - 2017-12-05 09:02:51.403733: step 7240, loss = 1.64, batch loss = 1.58 (38.3 examples/sec; 0.209 sec/batch; 18h:53m:19s remains)
INFO - root - 2017-12-05 09:02:53.471078: step 7250, loss = 1.71, batch loss = 1.65 (39.0 examples/sec; 0.205 sec/batch; 18h:33m:22s remains)
INFO - root - 2017-12-05 09:02:55.578696: step 7260, loss = 1.01, batch loss = 0.95 (39.2 examples/sec; 0.204 sec/batch; 18h:26m:50s remains)
INFO - root - 2017-12-05 09:02:57.674845: step 7270, loss = 0.98, batch loss = 0.93 (39.3 examples/sec; 0.203 sec/batch; 18h:22m:08s remains)
INFO - root - 2017-12-05 09:02:59.729180: step 7280, loss = 1.18, batch loss = 1.12 (39.1 examples/sec; 0.205 sec/batch; 18h:28m:29s remains)
INFO - root - 2017-12-05 09:03:01.817693: step 7290, loss = 1.13, batch loss = 1.08 (38.1 examples/sec; 0.210 sec/batch; 18h:58m:24s remains)
INFO - root - 2017-12-05 09:03:03.910020: step 7300, loss = 1.13, batch loss = 1.07 (39.3 examples/sec; 0.204 sec/batch; 18h:23m:55s remains)
2017-12-05 09:03:04.170853: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.9603524 -2.382849 -1.864886 -1.4293609 -1.1210408 -0.94442582 -0.82730842 -0.76658392 -0.74789262 -0.78619623 -0.94291234 -1.2348928 -1.6057279 -2.0068085 -2.3761969][-2.8773699 -2.2367861 -1.6419585 -1.1118636 -0.70446277 -0.42757559 -0.22368336 -0.094317436 -0.023501873 -0.025581837 -0.1526289 -0.41568685 -0.75370646 -1.1198838 -1.4584284][-2.8637269 -2.1855786 -1.5358999 -0.93538785 -0.45061469 -0.10752344 0.15278339 0.33509302 0.45763922 0.51111746 0.45694923 0.2863493 0.047362328 -0.22395992 -0.4864614][-2.8851137 -2.1854763 -1.5021541 -0.86249137 -0.33612347 0.043704033 0.33681536 0.56096506 0.73149729 0.84442711 0.88261318 0.8331933 0.72059059 0.560472 0.38412142][-2.9397912 -2.2303126 -1.5280082 -0.86728764 -0.3090148 0.10251665 0.42925453 0.700582 0.91441107 1.0734973 1.1879134 1.2473674 1.2448506 1.1748095 1.0594072][-3.0157809 -2.3030412 -1.5812871 -0.89263606 -0.29196739 0.16636419 0.53583431 0.850204 1.0863724 1.2523174 1.3880119 1.494504 1.5446115 1.5154462 1.4181099][-3.1104946 -2.4031396 -1.6609371 -0.93868613 -0.28586626 0.23384953 0.65634012 1.0079784 1.2422757 1.3744931 1.4777503 1.5656171 1.6059804 1.5663452 1.449636][-3.2220559 -2.5314145 -1.7763953 -1.0193725 -0.31251144 0.27534485 0.75711346 1.1418147 1.3618097 1.4421577 1.4864554 1.5210176 1.5163908 1.4368148 1.2838101][-3.3474486 -2.6900983 -1.9439449 -1.1717892 -0.4295311 0.20205164 0.71886539 1.112464 1.3152156 1.3536716 1.3475657 1.3333135 1.2866173 1.1804914 1.0127969][-3.4934158 -2.9017949 -2.208854 -1.4712753 -0.74796081 -0.12455225 0.37729597 0.74554586 0.9347353 0.9637394 0.94184589 0.91322088 0.86456394 0.77665329 0.64471817][-3.6553514 -3.160769 -2.5636313 -1.9126143 -1.2641244 -0.69297695 -0.23441124 0.091937542 0.26613188 0.30295134 0.28826761 0.26522589 0.23931646 0.20368671 0.15103626][-3.8287678 -3.4528122 -2.9827831 -2.4582117 -1.9272671 -1.4456599 -1.0545707 -0.78190875 -0.63012457 -0.58766842 -0.590847 -0.60277438 -0.59780073 -0.56882119 -0.52036572][-4.00002 -3.7479362 -3.4207158 -3.0447259 -2.65736 -2.2971749 -1.9982045 -1.7891817 -1.6649859 -1.6227338 -1.6175358 -1.6138482 -1.5766604 -1.4837749 -1.3337886][-4.1358867 -3.9893456 -3.7927003 -3.5585027 -3.3130188 -3.0803111 -2.8827825 -2.7430658 -2.6555758 -2.6234136 -2.6182265 -2.6055503 -2.545857 -2.4086461 -2.1846995][-4.2180557 -4.144031 -4.0458889 -3.9281359 -3.8034225 -3.6841393 -3.5810313 -3.5079226 -3.4592021 -3.4417562 -3.439122 -3.4278588 -3.3668613 -3.2196078 -2.9707084]]...]
INFO - root - 2017-12-05 09:03:06.224604: step 7310, loss = 1.33, batch loss = 1.27 (38.9 examples/sec; 0.206 sec/batch; 18h:34m:53s remains)
INFO - root - 2017-12-05 09:03:08.304463: step 7320, loss = 1.29, batch loss = 1.23 (39.1 examples/sec; 0.205 sec/batch; 18h:28m:55s remains)
INFO - root - 2017-12-05 09:03:10.403386: step 7330, loss = 1.47, batch loss = 1.41 (38.2 examples/sec; 0.210 sec/batch; 18h:55m:50s remains)
INFO - root - 2017-12-05 09:03:12.464711: step 7340, loss = 1.26, batch loss = 1.20 (38.5 examples/sec; 0.208 sec/batch; 18h:45m:23s remains)
INFO - root - 2017-12-05 09:03:14.586744: step 7350, loss = 1.37, batch loss = 1.31 (37.4 examples/sec; 0.214 sec/batch; 19h:19m:42s remains)
INFO - root - 2017-12-05 09:03:16.663306: step 7360, loss = 0.92, batch loss = 0.87 (38.5 examples/sec; 0.208 sec/batch; 18h:46m:22s remains)
INFO - root - 2017-12-05 09:03:18.737426: step 7370, loss = 1.48, batch loss = 1.43 (38.8 examples/sec; 0.206 sec/batch; 18h:37m:35s remains)
INFO - root - 2017-12-05 09:03:20.838135: step 7380, loss = 1.41, batch loss = 1.36 (38.6 examples/sec; 0.207 sec/batch; 18h:43m:05s remains)
INFO - root - 2017-12-05 09:03:22.923909: step 7390, loss = 1.06, batch loss = 1.00 (38.6 examples/sec; 0.207 sec/batch; 18h:43m:24s remains)
INFO - root - 2017-12-05 09:03:25.022841: step 7400, loss = 1.13, batch loss = 1.07 (37.3 examples/sec; 0.215 sec/batch; 19h:22m:27s remains)
2017-12-05 09:03:25.301310: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.396666 -4.3842359 -4.3548088 -4.3138852 -4.2646656 -4.2094555 -4.162313 -4.1284385 -4.102602 -4.0603 -3.9822321 -3.8637459 -3.7214293 -3.5811114 -3.4659934][-4.3919206 -4.3673921 -4.3139644 -4.2370024 -4.1423197 -4.0381274 -3.9475596 -3.8852143 -3.8471718 -3.7996268 -3.7157807 -3.5808263 -3.4035923 -3.2133703 -3.0420866][-4.3852472 -4.3432436 -4.2552886 -4.1256094 -3.9647787 -3.7891564 -3.6343973 -3.5294306 -3.4771953 -3.4350963 -3.3663692 -3.2382956 -3.0486231 -2.8250098 -2.6056457][-4.3766241 -4.3135858 -4.183455 -3.9874787 -3.7427764 -3.4770093 -3.241996 -3.0817218 -3.0091157 -2.9793653 -2.9454308 -2.8528967 -2.6859682 -2.4626336 -2.2224352][-4.3673568 -4.2831283 -4.1086116 -3.8417904 -3.5046446 -3.14031 -2.8152947 -2.5888417 -2.4846852 -2.4619412 -2.4696317 -2.4357805 -2.3294833 -2.1497087 -1.9270363][-4.3591204 -4.2579956 -4.0466628 -3.716979 -3.2961657 -2.8409183 -2.4267664 -2.1255212 -1.9738104 -1.9419227 -1.9811337 -2.0097711 -1.9823937 -1.8796852 -1.7134631][-4.35294 -4.2416716 -4.0069323 -3.6345451 -3.1556759 -2.6358075 -2.1540937 -1.7877796 -1.5822663 -1.5208375 -1.5672915 -1.6377873 -1.6803722 -1.6597703 -1.5707843][-4.3478007 -4.2325153 -3.9899132 -3.6029825 -3.1039629 -2.5613685 -2.0555983 -1.6609156 -1.4188673 -1.325357 -1.3544714 -1.4317284 -1.5072596 -1.5375748 -1.5101376][-4.3427634 -4.22871 -3.9941161 -3.6207211 -3.1410928 -2.6229229 -2.1418443 -1.7658494 -1.528255 -1.4247398 -1.4276328 -1.4815576 -1.5438507 -1.5776427 -1.5740643][-4.3380809 -4.2297854 -4.0161185 -3.6816497 -3.2566357 -2.80415 -2.3916242 -2.0746691 -1.8770595 -1.785074 -1.7660334 -1.7837148 -1.8060553 -1.8069465 -1.7874844][-4.3360119 -4.2377934 -4.0547228 -3.7787173 -3.4333704 -3.0720329 -2.7516387 -2.513653 -2.3717589 -2.3049138 -2.2772436 -2.2634881 -2.2372444 -2.1824117 -2.1129634][-4.3393526 -4.25469 -4.1069384 -3.8944652 -3.6345859 -3.3701186 -3.1445446 -2.9880288 -2.9046888 -2.8713093 -2.8489926 -2.8150029 -2.7463193 -2.6319518 -2.494009][-4.3475885 -4.278409 -4.1646309 -4.0103478 -3.8285317 -3.6513221 -3.5087771 -3.4221711 -3.3908 -3.3900502 -3.3816185 -3.3412118 -3.2469177 -3.0864995 -2.8816795][-4.3586349 -4.3048377 -4.2195563 -4.11057 -3.9902303 -3.8794918 -3.7988565 -3.763938 -3.7711573 -3.7954159 -3.8027527 -3.7704325 -3.6727629 -3.4906402 -3.2409847][-4.3689876 -4.3273926 -4.2620654 -4.1832123 -4.104084 -4.0368977 -3.9953406 -3.9935825 -4.0249963 -4.0639925 -4.0843887 -4.0666256 -3.9830174 -3.8079143 -3.5465472]]...]
INFO - root - 2017-12-05 09:03:27.410176: step 7410, loss = 1.61, batch loss = 1.55 (38.7 examples/sec; 0.207 sec/batch; 18h:39m:43s remains)
INFO - root - 2017-12-05 09:03:29.519550: step 7420, loss = 1.35, batch loss = 1.29 (38.0 examples/sec; 0.211 sec/batch; 19h:00m:45s remains)
INFO - root - 2017-12-05 09:03:31.603892: step 7430, loss = 1.62, batch loss = 1.57 (38.8 examples/sec; 0.206 sec/batch; 18h:35m:57s remains)
INFO - root - 2017-12-05 09:03:33.687303: step 7440, loss = 1.54, batch loss = 1.48 (38.6 examples/sec; 0.207 sec/batch; 18h:44m:03s remains)
INFO - root - 2017-12-05 09:03:35.782205: step 7450, loss = 1.27, batch loss = 1.21 (38.7 examples/sec; 0.207 sec/batch; 18h:40m:44s remains)
INFO - root - 2017-12-05 09:03:37.875549: step 7460, loss = 1.41, batch loss = 1.35 (39.1 examples/sec; 0.205 sec/batch; 18h:29m:34s remains)
INFO - root - 2017-12-05 09:03:39.949776: step 7470, loss = 1.16, batch loss = 1.10 (39.1 examples/sec; 0.205 sec/batch; 18h:29m:39s remains)
INFO - root - 2017-12-05 09:03:42.030892: step 7480, loss = 1.20, batch loss = 1.14 (38.9 examples/sec; 0.206 sec/batch; 18h:34m:24s remains)
INFO - root - 2017-12-05 09:03:44.115876: step 7490, loss = 1.38, batch loss = 1.33 (37.8 examples/sec; 0.211 sec/batch; 19h:05m:07s remains)
INFO - root - 2017-12-05 09:03:46.195903: step 7500, loss = 1.36, batch loss = 1.30 (38.8 examples/sec; 0.206 sec/batch; 18h:37m:39s remains)
2017-12-05 09:03:46.466066: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.275341 -4.1745262 -3.9894023 -3.704617 -3.3441949 -2.9564562 -2.6272478 -2.4389658 -2.4315715 -2.5919745 -2.8646419 -3.1613536 -3.3813214 -3.4687042 -3.427712][-4.2192774 -4.039959 -3.7111554 -3.2082922 -2.5753665 -1.8995783 -1.3280797 -1.0053356 -1.0023346 -1.2876542 -1.7505765 -2.2380018 -2.5920758 -2.7272129 -2.6664491][-4.1585779 -3.8834884 -3.3848858 -2.6319923 -1.689429 -0.687552 0.15313339 0.62296915 0.62150049 0.20617056 -0.44451642 -1.1066532 -1.5709357 -1.7378576 -1.6586525][-4.0757751 -3.6890128 -3.0100641 -2.007185 -0.76420474 0.54708529 1.6361885 2.2400689 2.2455912 1.7404246 0.96677685 0.19904518 -0.32332134 -0.50561929 -0.42566323][-3.9249003 -3.4034128 -2.5387979 -1.3093851 0.18468285 1.7391605 3.0196953 3.7333479 3.7736015 3.2636242 2.4736381 1.6927228 1.1630192 0.96651077 1.0010467][-3.6277704 -2.9467506 -1.8989456 -0.48739338 1.1737747 2.8639417 4.2474375 5.0326014 5.1263628 4.6867418 3.981019 3.2709603 2.7750297 2.5521431 2.4837675][-3.1167526 -2.2651398 -1.0631719 0.46035528 2.18123 3.8814306 5.2584639 6.0520678 6.1915913 5.8549957 5.2915144 4.70634 4.2732105 4.0172815 3.8074102][-2.39564 -1.38659 -0.081660748 1.466126 3.1281891 4.70372 5.955554 6.6649036 6.794529 6.5420036 6.1302757 5.7042065 5.3682985 5.1058044 4.7742205][-1.6020284 -0.48624969 0.85277367 2.3379726 3.8342667 5.167695 6.1708889 6.6910715 6.7453184 6.5343447 6.2506151 5.9894218 5.7925625 5.5864162 5.2086153][-0.98098922 0.16097927 1.4548712 2.8008776 4.0597296 5.087781 5.7724571 6.03349 5.957552 5.7384958 5.5400782 5.4194231 5.3762045 5.2897453 4.9609938][-0.75655818 0.31896019 1.4857068 2.6307855 3.6279716 4.3584042 4.7393684 4.7499819 4.5285492 4.2608862 4.0951834 4.0692654 4.1610718 4.2191181 4.0092735][-1.0027049 -0.074410915 0.90019512 1.8108773 2.5495396 3.0175891 3.1597977 2.9904165 2.6643467 2.3535361 2.195507 2.2257476 2.4116373 2.5867267 2.509326][-1.6139424 -0.89037991 -0.14329624 0.5310874 1.0389624 1.3024764 1.2889895 1.0266728 0.66025925 0.34141731 0.19418383 0.25159264 0.47728157 0.71432734 0.74503326][-2.4015298 -1.8962791 -1.3732219 -0.90718126 -0.57887864 -0.44737625 -0.53488874 -0.80543828 -1.1403229 -1.4201007 -1.5419548 -1.4820862 -1.2701912 -1.0316663 -0.94065571][-3.1677091 -2.8503017 -2.5151093 -2.2153475 -2.0158081 -1.9590521 -2.0559828 -2.2751794 -2.5289774 -2.7360897 -2.8249245 -2.7780433 -2.6161213 -2.4239287 -2.3220003]]...]
INFO - root - 2017-12-05 09:03:48.548979: step 7510, loss = 1.49, batch loss = 1.44 (37.4 examples/sec; 0.214 sec/batch; 19h:17m:20s remains)
INFO - root - 2017-12-05 09:03:50.621241: step 7520, loss = 1.19, batch loss = 1.14 (37.7 examples/sec; 0.212 sec/batch; 19h:08m:33s remains)
INFO - root - 2017-12-05 09:03:52.685443: step 7530, loss = 1.71, batch loss = 1.65 (39.1 examples/sec; 0.205 sec/batch; 18h:28m:32s remains)
INFO - root - 2017-12-05 09:03:54.782081: step 7540, loss = 1.93, batch loss = 1.87 (37.9 examples/sec; 0.211 sec/batch; 19h:04m:27s remains)
INFO - root - 2017-12-05 09:03:56.870314: step 7550, loss = 1.29, batch loss = 1.23 (36.9 examples/sec; 0.217 sec/batch; 19h:34m:56s remains)
INFO - root - 2017-12-05 09:03:58.965150: step 7560, loss = 1.55, batch loss = 1.49 (37.8 examples/sec; 0.212 sec/batch; 19h:06m:30s remains)
INFO - root - 2017-12-05 09:04:01.054647: step 7570, loss = 1.38, batch loss = 1.33 (38.5 examples/sec; 0.208 sec/batch; 18h:45m:55s remains)
INFO - root - 2017-12-05 09:04:03.126645: step 7580, loss = 1.40, batch loss = 1.34 (37.7 examples/sec; 0.212 sec/batch; 19h:10m:23s remains)
INFO - root - 2017-12-05 09:04:05.215031: step 7590, loss = 1.30, batch loss = 1.24 (38.3 examples/sec; 0.209 sec/batch; 18h:51m:07s remains)
INFO - root - 2017-12-05 09:04:07.307795: step 7600, loss = 1.56, batch loss = 1.50 (36.9 examples/sec; 0.217 sec/batch; 19h:33m:40s remains)
2017-12-05 09:04:07.578853: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3921933 -4.3829527 -4.3733315 -4.3672285 -4.3625789 -4.3527107 -4.3358073 -4.3164082 -4.3074026 -4.3097205 -4.3173628 -4.3288383 -4.3447528 -4.3612561 -4.3737111][-4.3830671 -4.3619123 -4.3400731 -4.3252063 -4.3126941 -4.2895088 -4.2530894 -4.2133632 -4.1912994 -4.1895409 -4.2007632 -4.223526 -4.2590132 -4.299284 -4.33305][-4.3706241 -4.3326297 -4.2925849 -4.2646551 -4.2410808 -4.2014036 -4.1409478 -4.0757904 -4.0323424 -4.0156155 -4.0215425 -4.0533061 -4.1148648 -4.1906424 -4.2594323][-4.3570108 -4.29964 -4.2366333 -4.1914015 -4.1542878 -4.0981975 -4.0158305 -3.9276989 -3.8602362 -3.8193011 -3.8097055 -3.845335 -3.9316995 -4.0464106 -4.1578813][-4.3447042 -4.2687707 -4.1814914 -4.1150956 -4.0618157 -3.9903297 -3.8920476 -3.791415 -3.7072299 -3.6434317 -3.6154418 -3.6493256 -3.7531381 -3.8988128 -4.0482821][-4.3359804 -4.2450724 -4.1358113 -4.047401 -3.9767582 -3.8894365 -3.7807045 -3.6814256 -3.5980978 -3.5272496 -3.4903865 -3.522788 -3.6337132 -3.7934494 -3.9633346][-4.33201 -4.2323012 -4.1094584 -4.0035806 -3.9186962 -3.8205838 -3.7083673 -3.6203208 -3.5526063 -3.4925015 -3.4605441 -3.4962406 -3.6056447 -3.7598183 -3.9259963][-4.3324561 -4.23225 -4.1074934 -3.9942958 -3.9038138 -3.8076048 -3.7076802 -3.6363771 -3.5864649 -3.5439801 -3.5265548 -3.5682731 -3.6690226 -3.8007712 -3.9401107][-4.3354487 -4.2404943 -4.1217294 -4.0098 -3.9225478 -3.8393672 -3.7650564 -3.7172084 -3.6864641 -3.6631465 -3.6648445 -3.7134566 -3.799799 -3.8982773 -3.9943023][-4.3385916 -4.2495174 -4.1385655 -4.03354 -3.9566002 -3.8925996 -3.8464103 -3.8249757 -3.81777 -3.8153548 -3.8340929 -3.8859813 -3.9540153 -4.0169067 -4.0694151][-4.3395562 -4.2521458 -4.146091 -4.0505681 -3.988693 -3.9467096 -3.9284608 -3.9316266 -3.945519 -3.9601994 -3.988025 -4.0348392 -4.0815063 -4.113802 -4.1346312][-4.3370142 -4.2460284 -4.1393909 -4.0509591 -4.0022368 -3.9813914 -3.987606 -4.0106678 -4.0383868 -4.0627742 -4.092186 -4.1281466 -4.1553459 -4.1679869 -4.17137][-4.3322906 -4.2358613 -4.12567 -4.0398993 -3.99646 -3.9869328 -4.0061145 -4.0396204 -4.0750451 -4.1040812 -4.1302104 -4.1547723 -4.17027 -4.1753769 -4.1726665][-4.3282852 -4.229332 -4.1189518 -4.0354619 -3.9921668 -3.9824882 -4.0005918 -4.032589 -4.0693707 -4.0999246 -4.1223979 -4.1395473 -4.1507807 -4.1569581 -4.1552315][-4.3290744 -4.2360291 -4.1347828 -4.0582314 -4.0156741 -4.0013623 -4.0106177 -4.0340748 -4.0663886 -4.0951104 -4.1157036 -4.1307931 -4.1432896 -4.1532702 -4.1533566]]...]
INFO - root - 2017-12-05 09:04:09.688421: step 7610, loss = 1.22, batch loss = 1.16 (37.4 examples/sec; 0.214 sec/batch; 19h:17m:43s remains)
INFO - root - 2017-12-05 09:04:11.773216: step 7620, loss = 1.14, batch loss = 1.08 (38.1 examples/sec; 0.210 sec/batch; 18h:58m:04s remains)
INFO - root - 2017-12-05 09:04:13.850372: step 7630, loss = 1.42, batch loss = 1.36 (37.6 examples/sec; 0.213 sec/batch; 19h:11m:23s remains)
INFO - root - 2017-12-05 09:04:15.900648: step 7640, loss = 1.02, batch loss = 0.96 (38.5 examples/sec; 0.208 sec/batch; 18h:46m:06s remains)
INFO - root - 2017-12-05 09:04:18.006440: step 7650, loss = 1.30, batch loss = 1.24 (38.3 examples/sec; 0.209 sec/batch; 18h:50m:39s remains)
INFO - root - 2017-12-05 09:04:20.070050: step 7660, loss = 1.18, batch loss = 1.12 (38.9 examples/sec; 0.205 sec/batch; 18h:32m:20s remains)
INFO - root - 2017-12-05 09:04:22.155153: step 7670, loss = 1.06, batch loss = 1.01 (38.6 examples/sec; 0.207 sec/batch; 18h:43m:19s remains)
INFO - root - 2017-12-05 09:04:24.276349: step 7680, loss = 1.25, batch loss = 1.20 (38.9 examples/sec; 0.206 sec/batch; 18h:33m:39s remains)
INFO - root - 2017-12-05 09:04:26.340284: step 7690, loss = 1.72, batch loss = 1.66 (38.8 examples/sec; 0.206 sec/batch; 18h:36m:22s remains)
INFO - root - 2017-12-05 09:04:28.438352: step 7700, loss = 1.29, batch loss = 1.24 (38.4 examples/sec; 0.208 sec/batch; 18h:46m:30s remains)
2017-12-05 09:04:28.744554: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.2016 -3.1727929 -3.0984683 -2.9489594 -2.799253 -2.7490182 -2.85713 -3.114717 -3.4602468 -3.7999153 -4.0572758 -4.2072878 -4.2721152 -4.2893395 -4.2913094][-2.5767539 -2.488692 -2.31715 -2.0525432 -1.8191495 -1.7520387 -1.9237092 -2.3136156 -2.8356619 -3.3496232 -3.7472 -3.9936554 -4.1204953 -4.1764517 -4.202395][-2.0655279 -1.870441 -1.5669131 -1.177876 -0.86267447 -0.76765966 -0.96535611 -1.4338801 -2.0720975 -2.720202 -3.2483912 -3.6121674 -3.8414559 -3.9820549 -4.0727205][-1.7979228 -1.4666302 -1.0242622 -0.52301121 -0.12315178 0.043440819 -0.097149372 -0.53336453 -1.1746905 -1.8692172 -2.4847527 -2.9732618 -3.3477793 -3.6290267 -3.842737][-1.7736475 -1.3033133 -0.72721744 -0.11263561 0.39861536 0.69494534 0.69506884 0.3907814 -0.1528964 -0.80605078 -1.4496155 -2.0400035 -2.5716357 -3.034121 -3.432179][-1.8950706 -1.3115742 -0.61826706 0.11531258 0.75943184 1.2111082 1.3842893 1.253653 0.85968685 0.31304979 -0.29336834 -0.933481 -1.5959961 -2.2478054 -2.8653016][-2.1292477 -1.4854891 -0.71944642 0.10459757 0.86154938 1.4434114 1.7587395 1.7733817 1.5250711 1.1159024 0.60677814 -0.0093569756 -0.7304585 -1.5175645 -2.3205042][-2.4748967 -1.8370056 -1.062928 -0.21709204 0.58094072 1.2126336 1.585535 1.6669536 1.5128846 1.2245231 0.83871984 0.31814432 -0.36131811 -1.175333 -2.0541344][-2.8947315 -2.3266792 -1.6203983 -0.84307528 -0.10848045 0.46658611 0.79738808 0.86774683 0.7496376 0.53567839 0.25142431 -0.15175676 -0.7152071 -1.4297774 -2.2303798][-3.3171008 -2.859479 -2.2796726 -1.6483426 -1.0677247 -0.63647366 -0.41718769 -0.4052248 -0.52679229 -0.70545793 -0.91831517 -1.2101135 -1.6171751 -2.1459782 -2.7501616][-3.6567519 -3.3263257 -2.9075809 -2.4638691 -2.0753582 -1.8134499 -1.7134211 -1.7600162 -1.8900714 -2.0431685 -2.1986558 -2.3914609 -2.6485291 -2.9809408 -3.3604956][-3.8553224 -3.650809 -3.4043341 -3.1536713 -2.9463253 -2.8249278 -2.8078156 -2.8797462 -2.9976404 -3.118752 -3.2279658 -3.3456888 -3.487519 -3.6634119 -3.8606908][-3.8834581 -3.7892852 -3.7033124 -3.6266468 -3.5673871 -3.5424094 -3.5662978 -3.6315224 -3.7188597 -3.8026056 -3.8734515 -3.940428 -4.008986 -4.0855913 -4.1687803][-3.7668438 -3.7602484 -3.8087714 -3.8747988 -3.9287479 -3.96621 -4.0045695 -4.0510936 -4.1036663 -4.1520538 -4.1917572 -4.2264524 -4.2558746 -4.2846956 -4.3143406][-3.5911789 -3.6457503 -3.7924125 -3.9613485 -4.0942459 -4.1722288 -4.2173839 -4.2493005 -4.2766514 -4.3002291 -4.3201008 -4.3367891 -4.3494415 -4.36028 -4.3704929]]...]
INFO - root - 2017-12-05 09:04:30.820256: step 7710, loss = 1.43, batch loss = 1.37 (38.9 examples/sec; 0.206 sec/batch; 18h:33m:11s remains)
INFO - root - 2017-12-05 09:04:32.904758: step 7720, loss = 1.13, batch loss = 1.07 (37.1 examples/sec; 0.216 sec/batch; 19h:27m:06s remains)
INFO - root - 2017-12-05 09:04:34.988066: step 7730, loss = 1.59, batch loss = 1.53 (37.8 examples/sec; 0.212 sec/batch; 19h:06m:32s remains)
INFO - root - 2017-12-05 09:04:37.028335: step 7740, loss = 1.45, batch loss = 1.39 (39.7 examples/sec; 0.201 sec/batch; 18h:09m:21s remains)
INFO - root - 2017-12-05 09:04:39.107070: step 7750, loss = 0.97, batch loss = 0.91 (39.2 examples/sec; 0.204 sec/batch; 18h:24m:37s remains)
INFO - root - 2017-12-05 09:04:41.202463: step 7760, loss = 1.04, batch loss = 0.98 (38.0 examples/sec; 0.211 sec/batch; 18h:59m:59s remains)
INFO - root - 2017-12-05 09:04:43.301353: step 7770, loss = 1.45, batch loss = 1.39 (39.9 examples/sec; 0.200 sec/batch; 18h:05m:05s remains)
INFO - root - 2017-12-05 09:04:45.365134: step 7780, loss = 1.17, batch loss = 1.11 (39.0 examples/sec; 0.205 sec/batch; 18h:31m:18s remains)
INFO - root - 2017-12-05 09:04:47.437320: step 7790, loss = 1.37, batch loss = 1.31 (38.5 examples/sec; 0.208 sec/batch; 18h:44m:50s remains)
INFO - root - 2017-12-05 09:04:49.495097: step 7800, loss = 1.28, batch loss = 1.22 (39.3 examples/sec; 0.203 sec/batch; 18h:20m:57s remains)
2017-12-05 09:04:49.783359: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.9131663 -2.7127934 -2.6803179 -2.7496386 -2.8526013 -2.9509625 -3.0291581 -3.0968812 -3.1656716 -3.2499707 -3.3671637 -3.5177917 -3.6740742 -3.7892442 -3.8132606][-2.3236439 -2.0334914 -1.9606137 -2.0223005 -2.133713 -2.2496793 -2.3490074 -2.4448118 -2.5534205 -2.6937814 -2.8871243 -3.1261959 -3.3677359 -3.53881 -3.5700524][-1.8438032 -1.4850945 -1.3651824 -1.3913033 -1.4726176 -1.5686498 -1.6630235 -1.7735183 -1.9192059 -2.1207175 -2.3984663 -2.7358093 -3.06959 -3.3017664 -3.3463569][-1.6033783 -1.2089596 -1.0362415 -0.9900589 -0.99092364 -1.0153489 -1.065232 -1.1684022 -1.3412693 -1.599853 -1.9590347 -2.392632 -2.8153286 -3.1093047 -3.1782737][-1.6228232 -1.2183211 -0.98894382 -0.83824539 -0.71244287 -0.61712313 -0.58549929 -0.66341043 -0.85811162 -1.1709373 -1.6043901 -2.1244166 -2.6271698 -2.9833207 -3.091466][-1.8240063 -1.4180453 -1.1242151 -0.85016394 -0.5702126 -0.33013868 -0.19995165 -0.24635983 -0.46848059 -0.84394717 -1.3531892 -1.9512227 -2.5265136 -2.9441795 -3.1035979][-2.0684259 -1.6481869 -1.2816575 -0.88899064 -0.47107029 -0.10997772 0.095238686 0.062986851 -0.20683908 -0.66652179 -1.2602987 -1.9265585 -2.5568862 -3.0209093 -3.2241921][-2.2570474 -1.8145781 -1.3878896 -0.91206074 -0.40663242 0.020843506 0.25614214 0.20912933 -0.13107967 -0.69643044 -1.3789887 -2.0960233 -2.7485542 -3.2215362 -3.4364378][-2.3662081 -1.9101622 -1.4580302 -0.95565128 -0.43686247 -0.017786026 0.18959713 0.095647812 -0.32135057 -0.97247458 -1.7108808 -2.4361758 -3.0633969 -3.501049 -3.6937141][-2.4220476 -1.974241 -1.5449464 -1.0887315 -0.64130473 -0.30792904 -0.18409967 -0.34636879 -0.80859685 -1.4714515 -2.1895192 -2.8613248 -3.4119513 -3.7760596 -3.924937][-2.4682622 -2.0532987 -1.6922376 -1.3436522 -1.0297997 -0.83004785 -0.80821252 -1.0205438 -1.4702151 -2.0676122 -2.6941819 -3.2592895 -3.7015452 -3.9770465 -4.0829558][-2.5083561 -2.1414604 -1.8778687 -1.6685116 -1.5153377 -1.4606493 -1.5323212 -1.7624173 -2.1501722 -2.6309056 -3.1214914 -3.5519948 -3.8765113 -4.0703359 -4.1438627][-2.5456531 -2.2291625 -2.0627975 -1.9918091 -1.9909725 -2.0609741 -2.198916 -2.419064 -2.7212787 -3.0682013 -3.4119997 -3.7076278 -3.9262645 -4.0578761 -4.1140347][-2.5862093 -2.3152757 -2.2298677 -2.267308 -2.3797088 -2.5321636 -2.7017655 -2.8937869 -3.1113539 -3.3392048 -3.555717 -3.7394948 -3.8784797 -3.9705408 -4.0218043][-2.677937 -2.451077 -2.4201622 -2.5181184 -2.6844673 -2.8645291 -3.0284452 -3.1802721 -3.3263059 -3.464736 -3.5908365 -3.6994166 -3.7891908 -3.8604434 -3.9130809]]...]
INFO - root - 2017-12-05 09:04:51.877130: step 7810, loss = 1.14, batch loss = 1.08 (39.4 examples/sec; 0.203 sec/batch; 18h:17m:43s remains)
INFO - root - 2017-12-05 09:04:53.938997: step 7820, loss = 1.58, batch loss = 1.53 (39.0 examples/sec; 0.205 sec/batch; 18h:31m:21s remains)
INFO - root - 2017-12-05 09:04:56.009818: step 7830, loss = 1.43, batch loss = 1.37 (38.1 examples/sec; 0.210 sec/batch; 18h:57m:32s remains)
INFO - root - 2017-12-05 09:04:58.081091: step 7840, loss = 1.28, batch loss = 1.22 (40.0 examples/sec; 0.200 sec/batch; 18h:02m:23s remains)
INFO - root - 2017-12-05 09:05:00.165301: step 7850, loss = 0.84, batch loss = 0.78 (38.1 examples/sec; 0.210 sec/batch; 18h:56m:16s remains)
INFO - root - 2017-12-05 09:05:02.235438: step 7860, loss = 1.62, batch loss = 1.56 (39.5 examples/sec; 0.202 sec/batch; 18h:15m:20s remains)
INFO - root - 2017-12-05 09:05:04.326915: step 7870, loss = 1.49, batch loss = 1.43 (37.6 examples/sec; 0.213 sec/batch; 19h:10m:12s remains)
INFO - root - 2017-12-05 09:05:06.398820: step 7880, loss = 1.27, batch loss = 1.21 (39.4 examples/sec; 0.203 sec/batch; 18h:17m:55s remains)
INFO - root - 2017-12-05 09:05:08.508153: step 7890, loss = 1.66, batch loss = 1.61 (37.8 examples/sec; 0.212 sec/batch; 19h:05m:21s remains)
INFO - root - 2017-12-05 09:05:10.601366: step 7900, loss = 1.26, batch loss = 1.20 (37.9 examples/sec; 0.211 sec/batch; 19h:02m:20s remains)
2017-12-05 09:05:10.875298: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4035969 -4.4027872 -4.4018049 -4.4013767 -4.4015265 -4.4020963 -4.4030075 -4.4037952 -4.40401 -4.4036856 -4.403121 -4.4027467 -4.4026151 -4.4024515 -4.4023318][-4.4047604 -4.4038653 -4.4029741 -4.4028587 -4.4032426 -4.4040604 -4.4050817 -4.4055648 -4.405252 -4.4043989 -4.4034929 -4.4029016 -4.4025335 -4.4023056 -4.4023986][-4.4052525 -4.4041643 -4.4034204 -4.4038429 -4.404614 -4.4055123 -4.4063206 -4.4062748 -4.4051857 -4.4038334 -4.4028139 -4.4023304 -4.4020858 -4.4020281 -4.4024096][-4.4046412 -4.4033871 -4.4028463 -4.4037437 -4.4048228 -4.4056435 -4.4059896 -4.4054832 -4.4039145 -4.4020867 -4.4009657 -4.400835 -4.4011049 -4.4015193 -4.4023533][-4.4036403 -4.4023886 -4.4022903 -4.4036188 -4.4049149 -4.4055457 -4.4054108 -4.4043236 -4.4021039 -4.3998027 -4.3985767 -4.3987913 -4.3996706 -4.4006977 -4.4018092][-4.4033384 -4.4023032 -4.402617 -4.4042692 -4.4054651 -4.4057703 -4.4052858 -4.4037442 -4.4012609 -4.3988419 -4.3977175 -4.3981147 -4.3990936 -4.4001727 -4.4012303][-4.4027133 -4.4018359 -4.402494 -4.4043255 -4.4053922 -4.4054828 -4.4048772 -4.4033213 -4.4011793 -4.3993325 -4.3986878 -4.39921 -4.399931 -4.4006839 -4.4016027][-4.4015985 -4.4008431 -4.4015503 -4.4031878 -4.4040394 -4.4042082 -4.4038568 -4.4026675 -4.4013047 -4.4003339 -4.4002337 -4.4005218 -4.40069 -4.4009285 -4.4014587][-4.40076 -4.4001169 -4.4006972 -4.40215 -4.4030018 -4.4036894 -4.4039621 -4.4033828 -4.4029059 -4.4023337 -4.4022484 -4.4019752 -4.4013915 -4.4007974 -4.4005966][-4.4008322 -4.400106 -4.4004989 -4.4018517 -4.4031425 -4.4046979 -4.4060063 -4.4061575 -4.4061151 -4.4052467 -4.4044013 -4.4032488 -4.4016671 -4.4002256 -4.3993258][-4.4009748 -4.4001627 -4.4003572 -4.4016347 -4.4032536 -4.4056263 -4.4079642 -4.4088149 -4.4088497 -4.4073582 -4.4056292 -4.4035883 -4.4012556 -4.3991966 -4.3981528][-4.4005885 -4.399581 -4.39965 -4.4010158 -4.4029508 -4.4060555 -4.4092817 -4.4107027 -4.4106183 -4.4085774 -4.4060054 -4.4030256 -4.4001818 -4.3979206 -4.3968353][-4.3997278 -4.3985481 -4.3985596 -4.3998971 -4.401855 -4.4051771 -4.4088035 -4.4105191 -4.4101996 -4.4079676 -4.4051657 -4.4018354 -4.3989258 -4.3966331 -4.3955288][-4.3987541 -4.3974929 -4.3974042 -4.3986125 -4.4006906 -4.4039311 -4.4075308 -4.4094667 -4.4090586 -4.40659 -4.4034929 -4.4002142 -4.397469 -4.3952417 -4.3943596][-4.3978043 -4.3966613 -4.3965964 -4.3976679 -4.3995934 -4.4022589 -4.40527 -4.4070163 -4.406496 -4.4042506 -4.4013443 -4.398416 -4.3959746 -4.3940458 -4.3934088]]...]
INFO - root - 2017-12-05 09:05:12.941667: step 7910, loss = 1.07, batch loss = 1.02 (39.2 examples/sec; 0.204 sec/batch; 18h:23m:49s remains)
INFO - root - 2017-12-05 09:05:15.027077: step 7920, loss = 1.26, batch loss = 1.21 (37.8 examples/sec; 0.212 sec/batch; 19h:05m:33s remains)
INFO - root - 2017-12-05 09:05:17.102911: step 7930, loss = 1.12, batch loss = 1.06 (38.6 examples/sec; 0.207 sec/batch; 18h:42m:23s remains)
INFO - root - 2017-12-05 09:05:19.170625: step 7940, loss = 1.22, batch loss = 1.17 (39.6 examples/sec; 0.202 sec/batch; 18h:12m:11s remains)
INFO - root - 2017-12-05 09:05:21.242994: step 7950, loss = 1.57, batch loss = 1.51 (39.1 examples/sec; 0.205 sec/batch; 18h:27m:08s remains)
INFO - root - 2017-12-05 09:05:23.309507: step 7960, loss = 1.37, batch loss = 1.31 (38.6 examples/sec; 0.207 sec/batch; 18h:39m:38s remains)
INFO - root - 2017-12-05 09:05:25.377801: step 7970, loss = 1.21, batch loss = 1.15 (38.2 examples/sec; 0.209 sec/batch; 18h:51m:49s remains)
INFO - root - 2017-12-05 09:05:27.471979: step 7980, loss = 1.62, batch loss = 1.56 (38.8 examples/sec; 0.206 sec/batch; 18h:35m:46s remains)
INFO - root - 2017-12-05 09:05:29.554686: step 7990, loss = 1.15, batch loss = 1.10 (38.9 examples/sec; 0.206 sec/batch; 18h:32m:15s remains)
INFO - root - 2017-12-05 09:05:31.636170: step 8000, loss = 1.42, batch loss = 1.36 (38.7 examples/sec; 0.206 sec/batch; 18h:36m:42s remains)
2017-12-05 09:05:31.924920: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2581673 -4.263165 -4.2588568 -4.2376971 -4.202889 -4.1682754 -4.1495047 -4.14493 -4.1449647 -4.1475382 -4.1511545 -4.1527271 -4.1503787 -4.1482782 -4.1443262][-4.3032603 -4.294723 -4.271452 -4.2284031 -4.1733365 -4.1233511 -4.096046 -4.0903654 -4.0939031 -4.0987768 -4.1016 -4.0989361 -4.08991 -4.0803657 -4.0715923][-4.337213 -4.3175864 -4.2789187 -4.2212105 -4.1547089 -4.0947342 -4.0577135 -4.046155 -4.049654 -4.0568862 -4.06032 -4.0553684 -4.0415812 -4.0254374 -4.0117016][-4.3513436 -4.3227539 -4.2749972 -4.2120423 -4.1437926 -4.080729 -4.0365419 -4.0162654 -4.0159869 -4.022963 -4.0263562 -4.0190783 -4.0017304 -3.9809823 -3.9633145][-4.3525925 -4.31527 -4.261313 -4.1983271 -4.13258 -4.0699477 -4.0213637 -3.9950459 -3.9910905 -3.9951282 -3.9945223 -3.9832764 -3.9633341 -3.9406765 -3.92125][-4.3454447 -4.3006797 -4.2420335 -4.1789641 -4.1158767 -4.0541782 -4.0022292 -3.971149 -3.9635353 -3.9622083 -3.954365 -3.9365771 -3.913579 -3.8899059 -3.8716726][-4.3335919 -4.281846 -4.2175069 -4.1509094 -4.0852795 -4.0202551 -3.9633865 -3.925349 -3.9113014 -3.9038157 -3.8901527 -3.8670187 -3.840308 -3.8162227 -3.8015199][-4.32148 -4.2626424 -4.1902843 -4.1135406 -4.0382314 -3.9653268 -3.8994422 -3.8504744 -3.8260951 -3.8115263 -3.793201 -3.7652948 -3.7368453 -3.7156038 -3.7068558][-4.309906 -4.2422919 -4.1575413 -4.0669074 -3.9794486 -3.8976321 -3.8236585 -3.761857 -3.7239256 -3.6992512 -3.6733768 -3.6387398 -3.6080768 -3.5914791 -3.5914149][-4.2972741 -4.2187581 -4.1214843 -4.0182881 -3.9202459 -3.8317375 -3.75133 -3.6781998 -3.6252935 -3.5859194 -3.54881 -3.5069857 -3.4740002 -3.4611802 -3.4696031][-4.2845993 -4.1959481 -4.0880008 -3.9741402 -3.8674285 -3.7735169 -3.6884925 -3.6071696 -3.5406733 -3.4877093 -3.4404359 -3.3941226 -3.3612933 -3.3519294 -3.3657055][-4.2735291 -4.1766787 -4.0603242 -3.9385738 -3.8262236 -3.7299266 -3.6408463 -3.5531971 -3.4763272 -3.4134336 -3.3589158 -3.311116 -3.2807794 -3.2738509 -3.2900209][-4.2588987 -4.1602244 -4.0428262 -3.9215834 -3.8101306 -3.7146659 -3.6205826 -3.5256219 -3.4390097 -3.3655787 -3.3047261 -3.2565305 -3.2299285 -3.2261128 -3.2422137][-4.2319984 -4.1387615 -4.0297394 -3.9173012 -3.81323 -3.7212021 -3.6231575 -3.5213919 -3.4268064 -3.3467979 -3.2846689 -3.2422128 -3.2243009 -3.2259846 -3.242439][-4.1921463 -4.111598 -4.0194044 -3.9245508 -3.836292 -3.7523973 -3.6562476 -3.5539498 -3.4602818 -3.3855658 -3.3319876 -3.3031132 -3.3001513 -3.312186 -3.3299456]]...]
INFO - root - 2017-12-05 09:05:33.991117: step 8010, loss = 1.19, batch loss = 1.13 (38.9 examples/sec; 0.206 sec/batch; 18h:32m:12s remains)
INFO - root - 2017-12-05 09:05:36.054222: step 8020, loss = 1.31, batch loss = 1.25 (39.2 examples/sec; 0.204 sec/batch; 18h:25m:00s remains)
INFO - root - 2017-12-05 09:05:38.184843: step 8030, loss = 1.09, batch loss = 1.03 (39.7 examples/sec; 0.201 sec/batch; 18h:09m:27s remains)
INFO - root - 2017-12-05 09:05:40.259813: step 8040, loss = 1.63, batch loss = 1.57 (38.6 examples/sec; 0.207 sec/batch; 18h:39m:22s remains)
INFO - root - 2017-12-05 09:05:42.341447: step 8050, loss = 1.19, batch loss = 1.13 (39.2 examples/sec; 0.204 sec/batch; 18h:23m:09s remains)
INFO - root - 2017-12-05 09:05:44.453978: step 8060, loss = 1.22, batch loss = 1.16 (37.5 examples/sec; 0.214 sec/batch; 19h:14m:49s remains)
INFO - root - 2017-12-05 09:05:46.505965: step 8070, loss = 1.43, batch loss = 1.37 (39.8 examples/sec; 0.201 sec/batch; 18h:07m:17s remains)
INFO - root - 2017-12-05 09:05:48.582228: step 8080, loss = 1.46, batch loss = 1.41 (38.2 examples/sec; 0.210 sec/batch; 18h:53m:01s remains)
INFO - root - 2017-12-05 09:05:50.666195: step 8090, loss = 1.11, batch loss = 1.05 (38.7 examples/sec; 0.207 sec/batch; 18h:36m:56s remains)
INFO - root - 2017-12-05 09:05:52.792178: step 8100, loss = 1.38, batch loss = 1.32 (38.9 examples/sec; 0.205 sec/batch; 18h:30m:32s remains)
2017-12-05 09:05:53.067253: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4010782 -4.40077 -4.4001169 -4.3999972 -4.4005094 -4.4008303 -4.4008379 -4.4009948 -4.4011693 -4.4010029 -4.4003425 -4.3995924 -4.3990545 -4.3985538 -4.3980684][-4.401433 -4.4007964 -4.3997812 -4.3993163 -4.3999586 -4.400619 -4.4007516 -4.4010067 -4.4014263 -4.4016037 -4.4010859 -4.4002609 -4.3996215 -4.3989887 -4.398375][-4.4011412 -4.4004765 -4.3994389 -4.3988657 -4.3997316 -4.40064 -4.4007411 -4.4008493 -4.4013715 -4.4019518 -4.401648 -4.4008369 -4.4001055 -4.3994665 -4.398869][-4.3990397 -4.3989677 -4.39837 -4.3981309 -4.3992896 -4.4003606 -4.4003243 -4.4001479 -4.4007835 -4.4017787 -4.4017243 -4.4010129 -4.400353 -4.3998241 -4.3993816][-4.3942924 -4.3951397 -4.39561 -4.396266 -4.3978748 -4.3990192 -4.39901 -4.3988094 -4.399601 -4.4009638 -4.4012489 -4.4007721 -4.4003205 -4.400075 -4.3998308][-4.3889809 -4.39057 -4.39196 -4.393682 -4.395875 -4.3969231 -4.3967204 -4.3962431 -4.3968954 -4.3984404 -4.39909 -4.399189 -4.3993645 -4.3996587 -4.3997579][-4.386055 -4.3878436 -4.389184 -4.3913107 -4.393908 -4.3948174 -4.3938737 -4.3924565 -4.39228 -4.3935165 -4.3947525 -4.3959627 -4.3971868 -4.3982477 -4.3989635][-4.3859525 -4.3872962 -4.38801 -4.3899117 -4.3925495 -4.3932981 -4.3915339 -4.3889723 -4.387795 -4.3885212 -4.3903365 -4.392777 -4.3949847 -4.396699 -4.398025][-4.3878226 -4.3882084 -4.3881092 -4.3893628 -4.3916006 -4.3922148 -4.3901796 -4.3872204 -4.385725 -4.3864841 -4.3888836 -4.3917556 -4.3943167 -4.3960876 -4.3975744][-4.3916345 -4.3909287 -4.3900237 -4.3907022 -4.3922782 -4.3926563 -4.3907108 -4.3880696 -4.3869724 -4.3881273 -4.3908 -4.3933582 -4.3953581 -4.39668 -4.3978415][-4.3968053 -4.3956971 -4.3944035 -4.3944068 -4.3950219 -4.3949003 -4.3932252 -4.3913727 -4.3911281 -4.3926277 -4.3949285 -4.39637 -4.397275 -4.3980422 -4.3987269][-4.4012928 -4.4004946 -4.3994026 -4.3989019 -4.3988733 -4.3984332 -4.3971257 -4.3963971 -4.3971663 -4.3987479 -4.4000707 -4.4001541 -4.3999329 -4.4001389 -4.4003367][-4.4034882 -4.403409 -4.403049 -4.4026628 -4.4023018 -4.4019313 -4.4012403 -4.4013848 -4.4026489 -4.4039865 -4.4044447 -4.4037528 -4.40293 -4.4027071 -4.40242][-4.4049644 -4.4051461 -4.4052258 -4.4050984 -4.404954 -4.4049363 -4.4047217 -4.4051094 -4.4062581 -4.4071827 -4.407125 -4.406116 -4.40519 -4.4048238 -4.4042749][-4.4040303 -4.4046474 -4.4053912 -4.4059787 -4.4063392 -4.4067802 -4.4068623 -4.4071121 -4.4077992 -4.4082003 -4.4077086 -4.4066796 -4.4058909 -4.4055023 -4.404901]]...]
INFO - root - 2017-12-05 09:05:55.149950: step 8110, loss = 1.39, batch loss = 1.33 (37.5 examples/sec; 0.213 sec/batch; 19h:12m:59s remains)
INFO - root - 2017-12-05 09:05:57.234172: step 8120, loss = 1.07, batch loss = 1.01 (38.2 examples/sec; 0.209 sec/batch; 18h:51m:26s remains)
INFO - root - 2017-12-05 09:05:59.342073: step 8130, loss = 1.51, batch loss = 1.46 (39.1 examples/sec; 0.205 sec/batch; 18h:26m:14s remains)
INFO - root - 2017-12-05 09:06:01.445953: step 8140, loss = 1.44, batch loss = 1.38 (37.7 examples/sec; 0.212 sec/batch; 19h:07m:29s remains)
INFO - root - 2017-12-05 09:06:03.520648: step 8150, loss = 1.00, batch loss = 0.94 (39.1 examples/sec; 0.205 sec/batch; 18h:26m:26s remains)
INFO - root - 2017-12-05 09:06:05.578634: step 8160, loss = 1.42, batch loss = 1.36 (38.1 examples/sec; 0.210 sec/batch; 18h:55m:25s remains)
INFO - root - 2017-12-05 09:06:07.674540: step 8170, loss = 1.19, batch loss = 1.13 (37.6 examples/sec; 0.213 sec/batch; 19h:11m:09s remains)
INFO - root - 2017-12-05 09:06:09.766115: step 8180, loss = 1.32, batch loss = 1.26 (37.3 examples/sec; 0.214 sec/batch; 19h:19m:10s remains)
INFO - root - 2017-12-05 09:06:11.862033: step 8190, loss = 1.27, batch loss = 1.21 (36.9 examples/sec; 0.217 sec/batch; 19h:30m:44s remains)
INFO - root - 2017-12-05 09:06:13.926709: step 8200, loss = 1.25, batch loss = 1.19 (38.9 examples/sec; 0.206 sec/batch; 18h:32m:42s remains)
2017-12-05 09:06:14.215208: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3908348 -4.3644819 -4.3142643 -4.2480893 -4.1786952 -4.1233196 -4.0942612 -4.0991716 -4.1217546 -4.1353 -4.13389 -4.1298118 -4.132555 -4.1452231 -4.1675911][-4.3908587 -4.3627052 -4.3090382 -4.238698 -4.1678648 -4.1180482 -4.103035 -4.1295004 -4.1758056 -4.2105079 -4.2247629 -4.2289314 -4.232338 -4.2395158 -4.2523422][-4.3885446 -4.3540025 -4.2891231 -4.2051215 -4.1227379 -4.0693502 -4.064033 -4.1127992 -4.1872087 -4.2491736 -4.2853179 -4.3027329 -4.3103971 -4.3151293 -4.321362][-4.3849144 -4.344017 -4.2676277 -4.1669388 -4.0657749 -3.9977362 -3.9875982 -4.0449953 -4.1394811 -4.2264915 -4.2885847 -4.3262949 -4.3456831 -4.3551383 -4.3610997][-4.3811579 -4.3349752 -4.2488141 -4.1326714 -4.0107369 -3.9190316 -3.8866639 -3.9323411 -4.0299911 -4.1334119 -4.2221317 -4.288063 -4.3303585 -4.3560553 -4.37171][-4.3764186 -4.3238573 -4.2271891 -4.0949421 -3.9496746 -3.8287261 -3.7633879 -3.7807684 -3.8650784 -3.9746525 -4.086463 -4.1846447 -4.2608147 -4.315804 -4.3534937][-4.3697214 -4.3091936 -4.1997786 -4.0494032 -3.8791718 -3.7277358 -3.6285772 -3.6150656 -3.6795332 -3.7846427 -3.9098918 -4.0370522 -4.1508651 -4.2429414 -4.3116617][-4.3628573 -4.2951012 -4.1747861 -4.0080771 -3.8173339 -3.6429989 -3.5200105 -3.4840007 -3.5308752 -3.623719 -3.7474737 -3.8902225 -4.0332985 -4.1592727 -4.2598996][-4.3576741 -4.2860003 -4.1618223 -3.9913771 -3.7955809 -3.6152105 -3.4849012 -3.4395137 -3.4745388 -3.5529711 -3.6644652 -3.8042734 -3.9563437 -4.0986094 -4.2190304][-4.3563962 -4.286026 -4.1680942 -4.0098104 -3.8307378 -3.6653857 -3.5448725 -3.5005004 -3.5272479 -3.5922856 -3.6872239 -3.810699 -3.9503613 -4.08624 -4.206605][-4.3590541 -4.2944937 -4.1900277 -4.0545473 -3.9066563 -3.7730775 -3.6764641 -3.6420176 -3.6653535 -3.7195697 -3.7958417 -3.8954513 -4.0094786 -4.1224542 -4.2251449][-4.3654089 -4.3094406 -4.2220049 -4.11427 -4.003489 -3.910017 -3.846925 -3.8302541 -3.8544376 -3.8974774 -3.9525871 -4.0231466 -4.1043425 -4.1863384 -4.2622409][-4.372776 -4.3257685 -4.2545285 -4.1714196 -4.092526 -4.0343108 -4.0031662 -4.0063286 -4.0354228 -4.0707679 -4.10765 -4.1517029 -4.2016168 -4.25316 -4.30222][-4.3781214 -4.3371477 -4.2758346 -4.2076006 -4.1478853 -4.1126709 -4.1057272 -4.1275945 -4.1648464 -4.198061 -4.2239466 -4.24964 -4.2770691 -4.3059335 -4.3344727][-4.379508 -4.3392444 -4.2788968 -4.2125068 -4.1561251 -4.1271491 -4.1298985 -4.1640081 -4.21326 -4.2545652 -4.2829919 -4.3047032 -4.3227205 -4.3394709 -4.3552489]]...]
INFO - root - 2017-12-05 09:06:16.297663: step 8210, loss = 1.48, batch loss = 1.42 (38.1 examples/sec; 0.210 sec/batch; 18h:54m:51s remains)
INFO - root - 2017-12-05 09:06:18.388884: step 8220, loss = 1.30, batch loss = 1.24 (38.2 examples/sec; 0.209 sec/batch; 18h:50m:45s remains)
INFO - root - 2017-12-05 09:06:20.503528: step 8230, loss = 1.35, batch loss = 1.30 (38.9 examples/sec; 0.206 sec/batch; 18h:31m:54s remains)
INFO - root - 2017-12-05 09:06:22.602647: step 8240, loss = 1.73, batch loss = 1.67 (37.4 examples/sec; 0.214 sec/batch; 19h:14m:42s remains)
INFO - root - 2017-12-05 09:06:24.680627: step 8250, loss = 1.56, batch loss = 1.50 (38.2 examples/sec; 0.209 sec/batch; 18h:52m:03s remains)
INFO - root - 2017-12-05 09:06:26.748513: step 8260, loss = 1.53, batch loss = 1.48 (38.5 examples/sec; 0.208 sec/batch; 18h:42m:38s remains)
INFO - root - 2017-12-05 09:06:28.835217: step 8270, loss = 1.30, batch loss = 1.24 (38.8 examples/sec; 0.206 sec/batch; 18h:33m:28s remains)
INFO - root - 2017-12-05 09:06:30.930448: step 8280, loss = 1.31, batch loss = 1.25 (38.9 examples/sec; 0.205 sec/batch; 18h:30m:21s remains)
INFO - root - 2017-12-05 09:06:32.982380: step 8290, loss = 1.23, batch loss = 1.18 (38.0 examples/sec; 0.211 sec/batch; 18h:57m:28s remains)
INFO - root - 2017-12-05 09:06:35.073439: step 8300, loss = 1.35, batch loss = 1.29 (38.6 examples/sec; 0.207 sec/batch; 18h:39m:11s remains)
2017-12-05 09:06:35.340010: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2165613 -4.2253261 -4.2522454 -4.2889428 -4.323658 -4.3457756 -4.353766 -4.3500743 -4.3360639 -4.3039961 -4.2290211 -4.070231 -3.7957797 -3.3882203 -2.865943][-4.0147877 -4.0267482 -4.0696535 -4.1261835 -4.1784563 -4.2114577 -4.2202959 -4.2094107 -4.1812687 -4.1264677 -4.0200748 -3.8232102 -3.5074949 -3.0680485 -2.5302234][-3.6609011 -3.6764274 -3.7360334 -3.8069377 -3.863127 -3.8866405 -3.8715932 -3.8300214 -3.7692723 -3.6832805 -3.5504034 -3.3355572 -3.0166183 -2.6004462 -2.1187546][-3.2300317 -3.2566369 -3.3340614 -3.4038897 -3.4318838 -3.3975067 -3.3014596 -3.1736264 -3.0367875 -2.9024167 -2.7620189 -2.5874364 -2.3570814 -2.0725663 -1.75759][-2.9044118 -2.9519522 -3.0457623 -3.0978758 -3.060298 -2.9104552 -2.6621222 -2.3725183 -2.0995228 -1.8899274 -1.7624593 -1.6990998 -1.666322 -1.6339886 -1.592639][-2.8181195 -2.8863256 -2.986444 -3.0053785 -2.8778439 -2.5767565 -2.1284056 -1.6216457 -1.1633081 -0.85318804 -0.74875331 -0.848819 -1.0930085 -1.3917322 -1.6802971][-2.9775171 -3.0603266 -3.1582217 -3.1408722 -2.9267607 -2.4806144 -1.8322165 -1.1020584 -0.44540787 -0.020684242 0.06801796 -0.1997242 -0.73130441 -1.3637638 -1.9654815][-3.2622328 -3.3542416 -3.4458961 -3.4040837 -3.1335413 -2.5904737 -1.8033476 -0.90792179 -0.091541767 0.43529034 0.52406788 0.13481569 -0.61468959 -1.5012665 -2.331141][-3.5378618 -3.6333821 -3.7187083 -3.6697001 -3.3864236 -2.8212183 -1.9994304 -1.0504103 -0.16573191 0.41354322 0.51206875 0.077777386 -0.76465416 -1.7663622 -2.6949432][-3.737431 -3.8314638 -3.9145854 -3.8799152 -3.6352985 -3.1348007 -2.399194 -1.5339458 -0.70359683 -0.1425066 -0.027915 -0.41528678 -1.1992977 -2.1505961 -3.034694][-3.8806705 -3.9654953 -4.044692 -4.0318031 -3.8550761 -3.4785881 -2.9133053 -2.2349362 -1.5641882 -1.0916853 -0.96895504 -1.2469363 -1.8559945 -2.6220696 -3.3466167][-4.0039725 -4.0754619 -4.1467838 -4.1528082 -4.0470433 -3.8067024 -3.4354956 -2.9794331 -2.5146375 -2.1697865 -2.0553946 -2.2132683 -2.6050224 -3.1237426 -3.6306374][-4.1278296 -4.1813364 -4.2375488 -4.2528563 -4.2019744 -4.0735207 -3.8680034 -3.6082022 -3.33496 -3.119895 -3.0287647 -3.0915668 -3.2944527 -3.582972 -3.8789258][-4.2438469 -4.278707 -4.3157263 -4.3307867 -4.312037 -4.2559748 -4.1622076 -4.0401678 -3.9071388 -3.7958751 -3.7365913 -3.7450981 -3.8209167 -3.9418721 -4.075985][-4.3197637 -4.3395844 -4.3603244 -4.3718047 -4.3688927 -4.349751 -4.3132491 -4.262888 -4.2069879 -4.1584716 -4.1279473 -4.118433 -4.1303558 -4.1565084 -4.1906037]]...]
INFO - root - 2017-12-05 09:06:37.450556: step 8310, loss = 1.68, batch loss = 1.62 (38.7 examples/sec; 0.207 sec/batch; 18h:37m:44s remains)
INFO - root - 2017-12-05 09:06:39.602904: step 8320, loss = 1.63, batch loss = 1.57 (38.2 examples/sec; 0.210 sec/batch; 18h:52m:23s remains)
INFO - root - 2017-12-05 09:06:41.689358: step 8330, loss = 1.00, batch loss = 0.94 (38.2 examples/sec; 0.209 sec/batch; 18h:50m:48s remains)
INFO - root - 2017-12-05 09:06:43.793952: step 8340, loss = 1.56, batch loss = 1.51 (38.4 examples/sec; 0.208 sec/batch; 18h:44m:27s remains)
INFO - root - 2017-12-05 09:06:45.870178: step 8350, loss = 1.56, batch loss = 1.50 (39.1 examples/sec; 0.205 sec/batch; 18h:26m:29s remains)
INFO - root - 2017-12-05 09:06:47.935997: step 8360, loss = 1.52, batch loss = 1.46 (39.0 examples/sec; 0.205 sec/batch; 18h:28m:54s remains)
INFO - root - 2017-12-05 09:06:50.020717: step 8370, loss = 1.79, batch loss = 1.73 (38.8 examples/sec; 0.206 sec/batch; 18h:33m:48s remains)
INFO - root - 2017-12-05 09:06:52.107385: step 8380, loss = 1.28, batch loss = 1.22 (39.1 examples/sec; 0.205 sec/batch; 18h:25m:23s remains)
INFO - root - 2017-12-05 09:06:54.211873: step 8390, loss = 1.21, batch loss = 1.15 (38.3 examples/sec; 0.209 sec/batch; 18h:48m:29s remains)
INFO - root - 2017-12-05 09:06:56.277089: step 8400, loss = 1.19, batch loss = 1.13 (39.1 examples/sec; 0.205 sec/batch; 18h:25m:42s remains)
2017-12-05 09:06:56.574135: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4008808 -4.3972025 -4.3781276 -4.324769 -4.2215385 -4.0723085 -3.9141998 -3.8151865 -3.8328435 -3.9566998 -4.1213083 -4.2600131 -4.3434539 -4.3813219 -4.3946738][-4.4008732 -4.3973961 -4.3797183 -4.328692 -4.2261872 -4.0715747 -3.8998873 -3.7780147 -3.7715368 -3.8840013 -4.0545115 -4.2111773 -4.3148527 -4.3679004 -4.3895388][-4.4011769 -4.398181 -4.3828077 -4.335959 -4.2381153 -4.08407 -3.9030507 -3.7571959 -3.7180367 -3.8078167 -3.9758115 -4.1482496 -4.2752838 -4.3485632 -4.3825703][-4.4014359 -4.3986936 -4.3841233 -4.3369346 -4.2359929 -4.0729966 -3.8727212 -3.6970797 -3.6239781 -3.6924334 -3.8626704 -4.0595531 -4.2196774 -4.321331 -4.372591][-4.4015894 -4.3986912 -4.3835597 -4.3335547 -4.2259545 -4.0496588 -3.8270345 -3.6188772 -3.5085914 -3.5520158 -3.7256064 -3.9510658 -4.14859 -4.2837477 -4.3571033][-4.4016728 -4.3984394 -4.3824615 -4.3300304 -4.2178435 -4.0320678 -3.7910743 -3.5523202 -3.4029355 -3.4166028 -3.5895348 -3.8392677 -4.0705676 -4.2386632 -4.3359547][-4.4017167 -4.398138 -4.38138 -4.3280149 -4.2148104 -4.0260267 -3.7756152 -3.5168147 -3.3373294 -3.3251457 -3.4932337 -3.7538416 -4.0040083 -4.1945748 -4.3118811][-4.4017506 -4.3978105 -4.3803182 -4.3278117 -4.2182226 -4.0366158 -3.794384 -3.5389161 -3.3540406 -3.3315077 -3.487602 -3.7326126 -3.9728208 -4.1639404 -4.2890663][-4.401598 -4.3971891 -4.3794775 -4.3302236 -4.2303271 -4.0687017 -3.8566697 -3.6358418 -3.4793391 -3.4636569 -3.5925121 -3.7895937 -3.9878204 -4.1551566 -4.2725039][-4.4013042 -4.396544 -4.3800411 -4.3369827 -4.2524076 -4.1205363 -3.9541345 -3.7884672 -3.679805 -3.6760924 -3.7663062 -3.8987703 -4.0376749 -4.1654716 -4.263576][-4.4011269 -4.3964539 -4.3826137 -4.3478146 -4.2813325 -4.1817536 -4.063477 -3.9556091 -3.8942688 -3.8991039 -3.9526138 -4.0260673 -4.1072497 -4.1906919 -4.2607803][-4.4012322 -4.3972387 -4.38678 -4.3611808 -4.3132224 -4.2445021 -4.1686525 -4.10775 -4.080914 -4.0895824 -4.1165237 -4.1482248 -4.1845894 -4.226459 -4.262145][-4.4016175 -4.3987265 -4.3916831 -4.3751717 -4.3448262 -4.3032761 -4.2612343 -4.2322025 -4.2238832 -4.2310719 -4.2412834 -4.2481003 -4.2544513 -4.2617254 -4.2626605][-4.4021173 -4.4003992 -4.3965297 -4.3876977 -4.3718505 -4.3508062 -4.3311429 -4.319459 -4.3173213 -4.3196111 -4.318666 -4.3109879 -4.2975764 -4.2787309 -4.2505207][-4.4025507 -4.4017797 -4.4002938 -4.3968916 -4.3906198 -4.3820825 -4.3739967 -4.3688245 -4.36593 -4.3622923 -4.3534284 -4.335393 -4.3072739 -4.2685475 -4.2188044]]...]
INFO - root - 2017-12-05 09:06:58.710077: step 8410, loss = 1.12, batch loss = 1.06 (37.7 examples/sec; 0.212 sec/batch; 19h:06m:56s remains)
INFO - root - 2017-12-05 09:07:00.764014: step 8420, loss = 1.12, batch loss = 1.06 (39.1 examples/sec; 0.204 sec/batch; 18h:24m:00s remains)
INFO - root - 2017-12-05 09:07:02.895086: step 8430, loss = 0.92, batch loss = 0.86 (37.9 examples/sec; 0.211 sec/batch; 18h:58m:51s remains)
INFO - root - 2017-12-05 09:07:04.969462: step 8440, loss = 1.32, batch loss = 1.26 (37.6 examples/sec; 0.213 sec/batch; 19h:08m:54s remains)
INFO - root - 2017-12-05 09:07:07.063387: step 8450, loss = 1.01, batch loss = 0.95 (37.1 examples/sec; 0.216 sec/batch; 19h:24m:09s remains)
INFO - root - 2017-12-05 09:07:09.182314: step 8460, loss = 1.14, batch loss = 1.09 (38.3 examples/sec; 0.209 sec/batch; 18h:46m:58s remains)
INFO - root - 2017-12-05 09:07:11.305529: step 8470, loss = 1.20, batch loss = 1.14 (38.9 examples/sec; 0.206 sec/batch; 18h:31m:12s remains)
INFO - root - 2017-12-05 09:07:13.388568: step 8480, loss = 1.14, batch loss = 1.08 (38.4 examples/sec; 0.209 sec/batch; 18h:46m:04s remains)
INFO - root - 2017-12-05 09:07:15.479700: step 8490, loss = 1.40, batch loss = 1.34 (38.1 examples/sec; 0.210 sec/batch; 18h:53m:20s remains)
INFO - root - 2017-12-05 09:07:17.531721: step 8500, loss = 1.47, batch loss = 1.41 (37.8 examples/sec; 0.212 sec/batch; 19h:02m:59s remains)
2017-12-05 09:07:17.804885: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.9101233 -3.8776212 -3.837198 -3.7838087 -3.7266207 -3.6871758 -3.6773348 -3.6907315 -3.7177167 -3.7486305 -3.7746551 -3.7876093 -3.784369 -3.7710693 -3.7553439][-3.9369459 -3.9049685 -3.8612888 -3.8026469 -3.7411714 -3.6980033 -3.684603 -3.6961482 -3.7210214 -3.7485404 -3.7672398 -3.7697487 -3.7543168 -3.72688 -3.6963036][-4.0147319 -3.9829288 -3.9365308 -3.8725166 -3.8032098 -3.7499514 -3.7259965 -3.72976 -3.7507625 -3.7748501 -3.7871158 -3.7802238 -3.7529435 -3.7125678 -3.6688874][-4.1025004 -4.0706005 -4.0219059 -3.9499106 -3.8656833 -3.7923059 -3.7498672 -3.7425277 -3.7610116 -3.7878282 -3.8029711 -3.7948887 -3.7639184 -3.7190793 -3.6706619][-4.1745267 -4.1427517 -4.087924 -4.0018878 -3.8956447 -3.7951002 -3.7306111 -3.7152076 -3.7398133 -3.78063 -3.8114457 -3.8183224 -3.8010325 -3.7685394 -3.726593][-4.2275209 -4.1937509 -4.1279244 -4.0218096 -3.8885865 -3.760819 -3.6777997 -3.6575344 -3.6923246 -3.7546489 -3.8138947 -3.8531604 -3.8665876 -3.8588312 -3.8315165][-4.2727809 -4.234983 -4.1576982 -4.0328093 -3.8773592 -3.7307749 -3.6360807 -3.6131272 -3.6546159 -3.7327363 -3.8173912 -3.8872678 -3.9311609 -3.948642 -3.9377081][-4.3180141 -4.2768188 -4.1942554 -4.0636873 -3.901376 -3.7493279 -3.6508865 -3.6235728 -3.659373 -3.7338827 -3.821583 -3.9004059 -3.9590046 -3.9934778 -3.9986589][-4.357265 -4.3185387 -4.2419605 -4.1229115 -3.9747932 -3.8355291 -3.7390318 -3.6994493 -3.7102225 -3.7545786 -3.8150644 -3.8742914 -3.9270241 -3.9699776 -3.9943888][-4.3792605 -4.3483257 -4.2865891 -4.1899724 -4.0699944 -3.954977 -3.8671637 -3.816041 -3.794466 -3.793191 -3.8039742 -3.8220816 -3.8521848 -3.8951228 -3.9404366][-4.3888574 -4.3665175 -4.3206673 -4.2480378 -4.1574059 -4.0686436 -3.9923286 -3.9329011 -3.8832641 -3.8402324 -3.8023229 -3.7784474 -3.7825322 -3.8188648 -3.8781862][-4.3930454 -4.37793 -4.3457541 -4.2927942 -4.2252669 -4.1558356 -4.0889883 -4.0255914 -3.9582558 -3.8869879 -3.8163435 -3.7618046 -3.7420106 -3.7670088 -3.8307116][-4.3951721 -4.3850231 -4.3624253 -4.3242612 -4.2746992 -4.2207355 -4.1631684 -4.1003628 -4.0260863 -3.9386337 -3.8465316 -3.7697773 -3.7290692 -3.7396462 -3.7974448][-4.396421 -4.3898716 -4.374743 -4.3487153 -4.3145375 -4.2751594 -4.228466 -4.17215 -4.0999742 -4.005765 -3.8980846 -3.8018918 -3.7410727 -3.733674 -3.7789669][-4.3972249 -4.3933368 -4.3838177 -4.3672624 -4.3454785 -4.3193727 -4.28518 -4.2388344 -4.174602 -4.0849581 -3.9754691 -3.8704357 -3.7959282 -3.7711177 -3.8001444]]...]
INFO - root - 2017-12-05 09:07:19.869540: step 8510, loss = 1.77, batch loss = 1.71 (39.0 examples/sec; 0.205 sec/batch; 18h:27m:23s remains)
INFO - root - 2017-12-05 09:07:21.973544: step 8520, loss = 1.16, batch loss = 1.11 (36.4 examples/sec; 0.220 sec/batch; 19h:46m:16s remains)
INFO - root - 2017-12-05 09:07:24.036911: step 8530, loss = 1.38, batch loss = 1.32 (39.8 examples/sec; 0.201 sec/batch; 18h:05m:30s remains)
INFO - root - 2017-12-05 09:07:26.117633: step 8540, loss = 1.18, batch loss = 1.12 (39.2 examples/sec; 0.204 sec/batch; 18h:21m:40s remains)
INFO - root - 2017-12-05 09:07:28.210823: step 8550, loss = 1.29, batch loss = 1.23 (38.3 examples/sec; 0.209 sec/batch; 18h:48m:01s remains)
INFO - root - 2017-12-05 09:07:30.312234: step 8560, loss = 1.67, batch loss = 1.61 (38.3 examples/sec; 0.209 sec/batch; 18h:48m:53s remains)
INFO - root - 2017-12-05 09:07:32.409931: step 8570, loss = 1.55, batch loss = 1.49 (39.4 examples/sec; 0.203 sec/batch; 18h:15m:56s remains)
INFO - root - 2017-12-05 09:07:34.470097: step 8580, loss = 1.15, batch loss = 1.09 (39.0 examples/sec; 0.205 sec/batch; 18h:26m:59s remains)
INFO - root - 2017-12-05 09:07:36.571202: step 8590, loss = 2.05, batch loss = 1.99 (37.5 examples/sec; 0.214 sec/batch; 19h:12m:39s remains)
INFO - root - 2017-12-05 09:07:38.642958: step 8600, loss = 1.06, batch loss = 1.00 (38.2 examples/sec; 0.209 sec/batch; 18h:50m:49s remains)
2017-12-05 09:07:38.903803: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2822967 -4.2806692 -4.2827215 -4.2843871 -4.2854991 -4.2859426 -4.2855906 -4.2840676 -4.2816324 -4.2783895 -4.27465 -4.270627 -4.2663293 -4.2584071 -4.2425537][-4.3245392 -4.3212028 -4.3192925 -4.3172307 -4.315774 -4.3155165 -4.3158774 -4.3162632 -4.3173604 -4.3183451 -4.3184838 -4.3178391 -4.3166237 -4.311687 -4.2999244][-4.3299575 -4.3187728 -4.3055615 -4.292347 -4.2832131 -4.2805791 -4.2830019 -4.2894492 -4.3013911 -4.3149056 -4.3264894 -4.3360672 -4.3440561 -4.348609 -4.3471727][-4.2330046 -4.1972647 -4.15452 -4.1101971 -4.0750813 -4.058423 -4.0606656 -4.0814795 -4.1216908 -4.169251 -4.21354 -4.2533784 -4.2897248 -4.3221622 -4.3465333][-3.9902678 -3.9142065 -3.8236623 -3.7256844 -3.6389422 -3.5845423 -3.5689549 -3.5997562 -3.6799645 -3.7873757 -3.897732 -4.0041223 -4.1058025 -4.2007465 -4.2814126][-3.6201603 -3.5051308 -3.3664331 -3.2102232 -3.05985 -2.9447398 -2.8813596 -2.8922977 -2.9962103 -3.1678104 -3.365653 -3.5725207 -3.7801156 -3.978651 -4.151165][-3.2024941 -3.0670815 -2.9039381 -2.7100627 -2.5074964 -2.3280721 -2.2006702 -2.1633937 -2.2559767 -2.4661658 -2.7425704 -3.0539384 -3.3798959 -3.6962967 -3.9737673][-2.813997 -2.6777532 -2.5190208 -2.3210711 -2.1003575 -1.8904154 -1.7282367 -1.659539 -1.7339988 -1.9553201 -2.2786233 -2.6633515 -3.0757008 -3.4747398 -3.8232107][-2.4784706 -2.3484039 -2.2161844 -2.0484123 -1.8552399 -1.669512 -1.5348108 -1.4955544 -1.5924177 -1.8319395 -2.1805477 -2.5944123 -3.0280824 -3.4359429 -3.782515][-2.2589114 -2.1219606 -2.0145261 -1.886606 -1.749608 -1.6300492 -1.5750353 -1.622138 -1.7916536 -2.0768542 -2.4430282 -2.8464715 -3.2401705 -3.5865488 -3.8645372][-2.2025917 -2.06052 -1.9712014 -1.8799441 -1.8018687 -1.7621148 -1.8023539 -1.951381 -2.2063537 -2.5402422 -2.9115496 -3.2736287 -3.5891724 -3.8393855 -4.0199709][-2.3761981 -2.2387414 -2.1648297 -2.1001277 -2.0643022 -2.0857873 -2.1953812 -2.4095459 -2.7082429 -3.0511732 -3.3926268 -3.6890054 -3.9152403 -4.0713835 -4.1684518][-2.7552059 -2.6391912 -2.581862 -2.5377021 -2.5265326 -2.5756631 -2.7065072 -2.9241581 -3.2014 -3.4987445 -3.7743225 -3.9919312 -4.1384468 -4.2254639 -4.2717752][-3.2244642 -3.140729 -3.1026952 -3.0775561 -3.0788915 -3.1294672 -3.241663 -3.4135766 -3.6218548 -3.83709 -4.0274434 -4.1687942 -4.2569823 -4.3049111 -4.3285856][-3.6269736 -3.5754979 -3.5569191 -3.5469236 -3.5534792 -3.5910311 -3.6659153 -3.7762084 -3.9066789 -4.0391474 -4.155251 -4.2417674 -4.2967558 -4.3288426 -4.3471732]]...]
INFO - root - 2017-12-05 09:07:41.014197: step 8610, loss = 1.46, batch loss = 1.40 (38.2 examples/sec; 0.209 sec/batch; 18h:50m:35s remains)
INFO - root - 2017-12-05 09:07:43.086572: step 8620, loss = 1.42, batch loss = 1.36 (39.1 examples/sec; 0.205 sec/batch; 18h:24m:11s remains)
INFO - root - 2017-12-05 09:07:45.119906: step 8630, loss = 1.44, batch loss = 1.39 (39.2 examples/sec; 0.204 sec/batch; 18h:20m:23s remains)
INFO - root - 2017-12-05 09:07:47.205205: step 8640, loss = 1.16, batch loss = 1.10 (37.7 examples/sec; 0.212 sec/batch; 19h:06m:27s remains)
INFO - root - 2017-12-05 09:07:49.313829: step 8650, loss = 1.29, batch loss = 1.23 (36.3 examples/sec; 0.220 sec/batch; 19h:49m:25s remains)
INFO - root - 2017-12-05 09:07:51.412098: step 8660, loss = 1.44, batch loss = 1.38 (39.0 examples/sec; 0.205 sec/batch; 18h:26m:55s remains)
INFO - root - 2017-12-05 09:07:53.476531: step 8670, loss = 1.39, batch loss = 1.33 (38.3 examples/sec; 0.209 sec/batch; 18h:46m:57s remains)
INFO - root - 2017-12-05 09:07:55.548829: step 8680, loss = 1.17, batch loss = 1.11 (39.2 examples/sec; 0.204 sec/batch; 18h:21m:39s remains)
INFO - root - 2017-12-05 09:07:57.635321: step 8690, loss = 1.18, batch loss = 1.12 (38.5 examples/sec; 0.208 sec/batch; 18h:40m:11s remains)
INFO - root - 2017-12-05 09:07:59.750105: step 8700, loss = 1.54, batch loss = 1.48 (38.5 examples/sec; 0.208 sec/batch; 18h:40m:30s remains)
2017-12-05 09:08:00.028349: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.8165865 -3.5578866 -3.2874496 -3.0424724 -2.8608341 -2.7566457 -2.7197118 -2.7109566 -2.7080798 -2.7208092 -2.7767065 -2.8921041 -3.0682292 -3.3051357 -3.5807333][-3.1781006 -2.666636 -2.1466644 -1.6964173 -1.3876314 -1.2346606 -1.202445 -1.213479 -1.2238986 -1.248827 -1.3397017 -1.532299 -1.8397648 -2.272985 -2.7892847][-2.2471356 -1.3872967 -0.54577804 0.139575 0.56293631 0.72513962 0.70937157 0.63894653 0.59784651 0.55966234 0.43715906 0.16665411 -0.29201174 -0.96265483 -1.7797737][-1.1051445 0.15309334 1.3386617 2.2409415 2.7302117 2.84944 2.746243 2.5986919 2.5244017 2.4780664 2.3309183 1.988862 1.3745055 0.450418 -0.68906331][0.0832839 1.7321286 3.2429652 4.330348 4.8557043 4.9132624 4.7147865 4.484931 4.3590746 4.2806473 4.0895824 3.6675892 2.9039364 1.7525668 0.32805061][1.1222272 3.0955596 4.8796043 6.1327486 6.7131824 6.7516184 6.5057907 6.2144833 6.0138378 5.8522024 5.5668097 5.0294676 4.1082792 2.7607765 1.1182098][1.8504133 4.040719 6.0244722 7.4322977 8.1201963 8.2320814 8.0284891 7.7172213 7.4195385 7.1148939 6.6784735 5.9822769 4.8897142 3.3773842 1.5874557][2.1918454 4.4776726 6.5679226 8.1003075 8.9357862 9.2074251 9.131649 8.85056 8.4622774 7.9937611 7.3807182 6.5052824 5.2414951 3.5941267 1.7227383][2.1694918 4.4337854 6.5346651 8.1372023 9.1080265 9.54605 9.6103992 9.3865423 8.9369669 8.3374071 7.5739918 6.5437422 5.148653 3.4199433 1.5376716][1.8267899 3.958147 5.9662371 7.5569305 8.5941753 9.1347227 9.2887449 9.1084948 8.6358843 7.9578238 7.0934582 5.9655437 4.5177221 2.8052955 1.0129576][1.1566029 3.0438476 4.8426371 6.3117371 7.31448 7.8675175 8.0485754 7.8941231 7.4341335 6.7425652 5.8562202 4.724844 3.3363657 1.7658143 0.18046284][0.20379496 1.7561913 3.2473283 4.4883919 5.3573017 5.8435926 6.0007076 5.8573742 5.4379969 4.7994814 3.9813766 2.9598165 1.7572937 0.45051289 -0.8311944][-0.89605 0.27939224 1.4153123 2.3732529 3.0483098 3.4205656 3.5281787 3.3957634 3.0395074 2.5011349 1.8211012 0.99620914 0.062284946 -0.91993737 -1.8623691][-1.9796484 -1.1686556 -0.37850428 0.29249 0.76332426 1.0147386 1.0754724 0.96262312 0.68543816 0.2738905 -0.23807669 -0.83660841 -1.4861038 -2.148905 -2.7725854][-2.8979993 -2.3930774 -1.895812 -1.4740379 -1.1817524 -1.0308874 -1.0024974 -1.0901926 -1.2868404 -1.5726955 -1.9235277 -2.3171961 -2.724817 -3.1228971 -3.4872723]]...]
INFO - root - 2017-12-05 09:08:02.087401: step 8710, loss = 1.18, batch loss = 1.12 (39.1 examples/sec; 0.204 sec/batch; 18h:23m:29s remains)
INFO - root - 2017-12-05 09:08:04.161327: step 8720, loss = 1.15, batch loss = 1.10 (39.2 examples/sec; 0.204 sec/batch; 18h:22m:32s remains)
INFO - root - 2017-12-05 09:08:06.239270: step 8730, loss = 0.98, batch loss = 0.93 (39.0 examples/sec; 0.205 sec/batch; 18h:26m:09s remains)
INFO - root - 2017-12-05 09:08:08.307986: step 8740, loss = 1.43, batch loss = 1.37 (38.1 examples/sec; 0.210 sec/batch; 18h:52m:46s remains)
INFO - root - 2017-12-05 09:08:10.424988: step 8750, loss = 1.49, batch loss = 1.43 (38.7 examples/sec; 0.207 sec/batch; 18h:35m:25s remains)
INFO - root - 2017-12-05 09:08:12.528005: step 8760, loss = 1.15, batch loss = 1.09 (39.4 examples/sec; 0.203 sec/batch; 18h:15m:08s remains)
INFO - root - 2017-12-05 09:08:14.614859: step 8770, loss = 1.63, batch loss = 1.57 (38.0 examples/sec; 0.211 sec/batch; 18h:56m:09s remains)
INFO - root - 2017-12-05 09:08:16.702559: step 8780, loss = 1.31, batch loss = 1.25 (39.0 examples/sec; 0.205 sec/batch; 18h:26m:32s remains)
INFO - root - 2017-12-05 09:08:18.780749: step 8790, loss = 1.19, batch loss = 1.13 (38.4 examples/sec; 0.208 sec/batch; 18h:43m:45s remains)
INFO - root - 2017-12-05 09:08:20.907478: step 8800, loss = 1.29, batch loss = 1.23 (36.9 examples/sec; 0.217 sec/batch; 19h:30m:40s remains)
2017-12-05 09:08:21.198981: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.8681142 -3.8217201 -3.7744017 -3.7218287 -3.6733904 -3.631175 -3.5927949 -3.5581312 -3.5265372 -3.4841466 -3.4180584 -3.3258224 -3.2199998 -3.1225419 -3.0474815][-3.7644072 -3.7441368 -3.726716 -3.6998458 -3.6696665 -3.638299 -3.6036153 -3.5676613 -3.5328932 -3.4898949 -3.4275849 -3.3407307 -3.239814 -3.146152 -3.0751662][-3.6795411 -3.6914313 -3.7072606 -3.70221 -3.6835272 -3.6569488 -3.6227076 -3.5836949 -3.5440943 -3.5007274 -3.4461091 -3.3739061 -3.292068 -3.2190747 -3.1676922][-3.6224148 -3.6611 -3.7004015 -3.7034965 -3.6811471 -3.6467059 -3.6042013 -3.5596147 -3.5180457 -3.479871 -3.4390965 -3.39008 -3.338037 -3.2958353 -3.2700822][-3.5977855 -3.6479561 -3.6936166 -3.6894176 -3.6488595 -3.5940075 -3.5349231 -3.4831398 -3.4464991 -3.4237959 -3.405575 -3.3868532 -3.3698063 -3.3589442 -3.3568876][-3.6003819 -3.6427126 -3.6739931 -3.6486275 -3.5824633 -3.5021429 -3.4236608 -3.3648458 -3.3390164 -3.3436768 -3.3586679 -3.3727365 -3.3849716 -3.3970604 -3.4116566][-3.610013 -3.6283178 -3.6314089 -3.5791631 -3.4884109 -3.3869262 -3.292675 -3.2286429 -3.2193344 -3.2600427 -3.3141236 -3.3582413 -3.3889098 -3.4114964 -3.4326506][-3.6105785 -3.5885234 -3.5585227 -3.4842772 -3.3822064 -3.2736349 -3.174788 -3.1133542 -3.1245236 -3.1997719 -3.2854824 -3.3508782 -3.3895142 -3.412436 -3.4337645][-3.5937145 -3.5293496 -3.4680042 -3.3823342 -3.2846909 -3.1864827 -3.1003911 -3.0553608 -3.08739 -3.1822476 -3.2810225 -3.3542771 -3.3944697 -3.4160533 -3.4344485][-3.567507 -3.470871 -3.3875542 -3.30118 -3.2203994 -3.1442668 -3.0822487 -3.0639062 -3.1128192 -3.2104592 -3.3067269 -3.377347 -3.4162595 -3.4357674 -3.4483051][-3.5547757 -3.442508 -3.3491435 -3.2680237 -3.204668 -3.1520677 -3.1168323 -3.1234188 -3.1814675 -3.2725921 -3.3607721 -3.4257026 -3.4615984 -3.4763126 -3.4804358][-3.5676525 -3.454565 -3.3625944 -3.2891483 -3.2376781 -3.2020724 -3.1874547 -3.2091386 -3.2683148 -3.350563 -3.4322429 -3.4928064 -3.5242386 -3.5318859 -3.5243664][-3.6160021 -3.5108361 -3.4258695 -3.3581305 -3.3115158 -3.2836318 -3.2781551 -3.3029456 -3.3570671 -3.4299147 -3.504108 -3.5598404 -3.585258 -3.5843623 -3.5627961][-3.7017863 -3.6064394 -3.5287161 -3.4631143 -3.4154072 -3.3870029 -3.379673 -3.3956237 -3.4359894 -3.4940467 -3.5549352 -3.6003706 -3.6164799 -3.6050434 -3.5708263][-3.8116026 -3.7228994 -3.6505578 -3.5868273 -3.5363779 -3.5003014 -3.4804733 -3.4769263 -3.4921398 -3.5246351 -3.5642846 -3.5944455 -3.5987496 -3.5763559 -3.5337238]]...]
INFO - root - 2017-12-05 09:08:23.284067: step 8810, loss = 1.17, batch loss = 1.11 (38.0 examples/sec; 0.211 sec/batch; 18h:55m:53s remains)
INFO - root - 2017-12-05 09:08:25.386239: step 8820, loss = 1.22, batch loss = 1.17 (38.6 examples/sec; 0.207 sec/batch; 18h:38m:11s remains)
INFO - root - 2017-12-05 09:08:27.433905: step 8830, loss = 1.23, batch loss = 1.17 (39.3 examples/sec; 0.204 sec/batch; 18h:17m:55s remains)
INFO - root - 2017-12-05 09:08:29.470901: step 8840, loss = 1.42, batch loss = 1.36 (39.0 examples/sec; 0.205 sec/batch; 18h:25m:33s remains)
INFO - root - 2017-12-05 09:08:31.567101: step 8850, loss = 1.14, batch loss = 1.08 (37.9 examples/sec; 0.211 sec/batch; 18h:58m:51s remains)
INFO - root - 2017-12-05 09:08:33.677544: step 8860, loss = 1.39, batch loss = 1.34 (38.1 examples/sec; 0.210 sec/batch; 18h:52m:53s remains)
INFO - root - 2017-12-05 09:08:35.767149: step 8870, loss = 1.12, batch loss = 1.06 (37.5 examples/sec; 0.213 sec/batch; 19h:09m:12s remains)
INFO - root - 2017-12-05 09:08:37.857381: step 8880, loss = 1.91, batch loss = 1.85 (37.6 examples/sec; 0.213 sec/batch; 19h:08m:25s remains)
INFO - root - 2017-12-05 09:08:39.950163: step 8890, loss = 1.10, batch loss = 1.05 (36.6 examples/sec; 0.219 sec/batch; 19h:39m:47s remains)
INFO - root - 2017-12-05 09:08:42.011254: step 8900, loss = 1.47, batch loss = 1.42 (39.0 examples/sec; 0.205 sec/batch; 18h:27m:42s remains)
2017-12-05 09:08:42.299748: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.33860159 0.15378952 0.47947836 0.66803503 0.74591208 0.74918604 0.72158575 0.69416523 0.68525743 0.69767046 0.71710014 0.71986532 0.66900396 0.57041645 0.43674469][0.53821993 1.0323076 1.3357711 1.4906869 1.5313625 1.4946594 1.4297709 1.3752232 1.3516855 1.3593121 1.3805041 1.3834558 1.3186059 1.1914892 1.0248795][1.206121 1.6597815 1.9225912 2.0538082 2.0861497 2.0468302 1.9767885 1.9122796 1.8739042 1.8589363 1.8501244 1.8163772 1.7076321 1.5316501 1.3165178][1.6354842 2.0502887 2.2932534 2.4417691 2.5157242 2.5215364 2.4814405 2.4167585 2.3480911 2.276535 2.1933122 2.0755963 1.8854966 1.637506 1.3631215][1.7917638 2.2073908 2.4903884 2.7200065 2.8954425 2.9944921 3.0104532 2.9400539 2.8118081 2.6433187 2.4430261 2.2059298 1.913939 1.5888882 1.2628059][1.700254 2.1817269 2.5772009 2.9557338 3.284018 3.5071774 3.588892 3.4948416 3.2672286 2.9598389 2.6115117 2.2420182 1.8540473 1.4767799 1.1338167][1.5061769 2.0924244 2.6414776 3.2025032 3.7020912 4.0514822 4.1838255 4.039279 3.681159 3.210681 2.705512 2.2147632 1.7599421 1.3716674 1.0568724][1.2942691 1.9843726 2.6721144 3.395741 4.0357647 4.4784212 4.6340895 4.4247103 3.9392929 3.3240347 2.6901851 2.1132979 1.6312957 1.2710266 1.0168648][1.0283942 1.7728996 2.5339646 3.3397913 4.0401945 4.5081339 4.6461649 4.3849783 3.8245983 3.1331325 2.4405737 1.8378129 1.3754039 1.0698304 0.88723135][0.62400341 1.3457332 2.0803552 2.8579025 3.5163565 3.9316359 4.0177965 3.7343636 3.1740618 2.4973521 1.8313975 1.2695637 0.86464357 0.62299061 0.50175953][-0.0092720985 0.61660004 1.2371321 1.8931184 2.4309125 2.7442584 2.7669115 2.4898219 1.993948 1.4086404 0.84045124 0.37235165 0.047756195 -0.13416529 -0.21351004][-0.86977005 -0.38334084 0.075783253 0.55917597 0.94081974 1.1381731 1.1072836 0.86284018 0.47204733 0.024319649 -0.40512514 -0.75101709 -0.98568034 -1.1148231 -1.1677434][-1.8463612 -1.5105987 -1.2157438 -0.90904856 -0.67759395 -0.57776141 -0.63559294 -0.82616019 -1.0991302 -1.40201 -1.6894305 -1.916008 -2.0677595 -2.1512275 -2.1847913][-2.7767735 -2.5736728 -2.4092364 -2.2447131 -2.1271853 -2.0933366 -2.1549244 -2.2876258 -2.457814 -2.6389203 -2.8073833 -2.9358504 -3.0201883 -3.0672088 -3.086071][-3.5289834 -3.4187646 -3.3389754 -3.2658925 -3.2183111 -3.2169042 -3.2656569 -3.3467345 -3.4397433 -3.5339985 -3.6191382 -3.6808388 -3.7207711 -3.7427485 -3.7514062]]...]
INFO - root - 2017-12-05 09:08:44.401170: step 8910, loss = 1.38, batch loss = 1.32 (36.0 examples/sec; 0.222 sec/batch; 19h:57m:09s remains)
INFO - root - 2017-12-05 09:08:46.488866: step 8920, loss = 1.03, batch loss = 0.98 (39.2 examples/sec; 0.204 sec/batch; 18h:20m:05s remains)
INFO - root - 2017-12-05 09:08:48.555725: step 8930, loss = 1.26, batch loss = 1.20 (38.8 examples/sec; 0.206 sec/batch; 18h:30m:48s remains)
INFO - root - 2017-12-05 09:08:50.618357: step 8940, loss = 1.04, batch loss = 0.98 (38.1 examples/sec; 0.210 sec/batch; 18h:52m:30s remains)
INFO - root - 2017-12-05 09:08:52.713542: step 8950, loss = 1.09, batch loss = 1.04 (36.3 examples/sec; 0.221 sec/batch; 19h:49m:48s remains)
INFO - root - 2017-12-05 09:08:54.784466: step 8960, loss = 1.31, batch loss = 1.25 (40.1 examples/sec; 0.199 sec/batch; 17h:55m:25s remains)
INFO - root - 2017-12-05 09:08:56.886387: step 8970, loss = 1.20, batch loss = 1.14 (39.2 examples/sec; 0.204 sec/batch; 18h:19m:04s remains)
INFO - root - 2017-12-05 09:08:58.976774: step 8980, loss = 2.22, batch loss = 2.16 (38.4 examples/sec; 0.209 sec/batch; 18h:44m:42s remains)
INFO - root - 2017-12-05 09:09:01.074700: step 8990, loss = 1.80, batch loss = 1.74 (38.9 examples/sec; 0.206 sec/batch; 18h:29m:45s remains)
INFO - root - 2017-12-05 09:09:03.171593: step 9000, loss = 1.58, batch loss = 1.52 (38.5 examples/sec; 0.208 sec/batch; 18h:41m:18s remains)
2017-12-05 09:09:03.476913: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3790846 -4.3759551 -4.3749776 -4.3763633 -4.3799629 -4.3845963 -4.389647 -4.394093 -4.3973904 -4.3993006 -4.3989024 -4.3945184 -4.3849196 -4.3714046 -4.3564119][-4.3613992 -4.3573833 -4.3561826 -4.3577843 -4.3616734 -4.3662715 -4.3714323 -4.3767986 -4.3819594 -4.3862643 -4.3878336 -4.3846803 -4.3754969 -4.3611937 -4.3444943][-4.3408647 -4.3351059 -4.3320541 -4.3324103 -4.3356791 -4.3394461 -4.3440967 -4.3502464 -4.3574591 -4.3648152 -4.3692546 -4.3678732 -4.3592896 -4.3443971 -4.326066][-4.3240666 -4.3148742 -4.3082404 -4.3053164 -4.3062143 -4.3086295 -4.3127823 -4.3194156 -4.3282328 -4.338356 -4.345902 -4.3468027 -4.339488 -4.3252268 -4.3065543][-4.3098578 -4.2967095 -4.2865639 -4.2798243 -4.2778516 -4.278656 -4.2818565 -4.2882972 -4.2981014 -4.3107338 -4.32182 -4.3267236 -4.3235226 -4.3130546 -4.29781][-4.2925673 -4.2759061 -4.2626061 -4.2528272 -4.2488861 -4.2490773 -4.2518835 -4.2585783 -4.2695508 -4.2848263 -4.2998695 -4.3102617 -4.3141179 -4.3114891 -4.3042011][-4.2735052 -4.2545981 -4.2396278 -4.2282748 -4.2230744 -4.2228465 -4.2256002 -4.2318845 -4.243083 -4.2596726 -4.2772546 -4.2917652 -4.3024106 -4.3092446 -4.3123889][-4.2574224 -4.2389846 -4.2246962 -4.2131429 -4.2073703 -4.2064362 -4.2087936 -4.2144766 -4.2245455 -4.2394009 -4.25542 -4.2696929 -4.2830992 -4.2957959 -4.3062434][-4.2601924 -4.2434483 -4.2305136 -4.2195907 -4.2132721 -4.2104173 -4.2104096 -4.2137747 -4.22035 -4.2301598 -4.2407303 -4.2510285 -4.2625084 -4.2754874 -4.2879682][-4.2913141 -4.277246 -4.2663331 -4.2565269 -4.249589 -4.2445292 -4.2407465 -4.2392321 -4.2396784 -4.2417545 -4.244318 -4.2473664 -4.252039 -4.2590227 -4.266973][-4.336894 -4.3270473 -4.3187428 -4.3109493 -4.3045354 -4.2986279 -4.292861 -4.2878728 -4.2833524 -4.2792592 -4.2751083 -4.2712159 -4.268672 -4.2681646 -4.2690468][-4.3742442 -4.3686471 -4.3634629 -4.3583384 -4.3537011 -4.3488216 -4.343698 -4.3385983 -4.33326 -4.3276138 -4.3215785 -4.3155079 -4.3097515 -4.3047233 -4.3004656][-4.3946757 -4.3922606 -4.3900471 -4.387816 -4.3855572 -4.3828821 -4.3798485 -4.3764257 -4.3725505 -4.3682184 -4.3634391 -4.3581777 -4.3527641 -4.347487 -4.3424926][-4.4002438 -4.4000983 -4.40033 -4.4002643 -4.3998585 -4.3990927 -4.398087 -4.3967161 -4.3950076 -4.3929415 -4.3904867 -4.387548 -4.3841906 -4.380589 -4.3769355][-4.3978491 -4.3993554 -4.4015322 -4.402946 -4.4035635 -4.4036427 -4.4034896 -4.4030752 -4.4024568 -4.4016876 -4.4008675 -4.3999195 -4.3987327 -4.3973174 -4.3958321]]...]
INFO - root - 2017-12-05 09:09:05.550556: step 9010, loss = 1.80, batch loss = 1.74 (38.7 examples/sec; 0.206 sec/batch; 18h:33m:13s remains)
INFO - root - 2017-12-05 09:09:07.668744: step 9020, loss = 1.77, batch loss = 1.71 (38.6 examples/sec; 0.207 sec/batch; 18h:36m:58s remains)
INFO - root - 2017-12-05 09:09:09.753028: step 9030, loss = 1.50, batch loss = 1.44 (38.9 examples/sec; 0.206 sec/batch; 18h:29m:16s remains)
INFO - root - 2017-12-05 09:09:11.855053: step 9040, loss = 1.40, batch loss = 1.34 (37.7 examples/sec; 0.212 sec/batch; 19h:03m:00s remains)
INFO - root - 2017-12-05 09:09:13.967210: step 9050, loss = 1.23, batch loss = 1.17 (39.8 examples/sec; 0.201 sec/batch; 18h:04m:33s remains)
INFO - root - 2017-12-05 09:09:16.049854: step 9060, loss = 1.40, batch loss = 1.34 (37.1 examples/sec; 0.216 sec/batch; 19h:22m:05s remains)
INFO - root - 2017-12-05 09:09:18.146561: step 9070, loss = 1.31, batch loss = 1.25 (39.3 examples/sec; 0.203 sec/batch; 18h:16m:49s remains)
INFO - root - 2017-12-05 09:09:20.264500: step 9080, loss = 1.45, batch loss = 1.39 (37.9 examples/sec; 0.211 sec/batch; 18h:57m:41s remains)
INFO - root - 2017-12-05 09:09:22.363213: step 9090, loss = 1.09, batch loss = 1.03 (39.0 examples/sec; 0.205 sec/batch; 18h:26m:05s remains)
INFO - root - 2017-12-05 09:09:24.458485: step 9100, loss = 1.37, batch loss = 1.31 (37.7 examples/sec; 0.212 sec/batch; 19h:04m:24s remains)
2017-12-05 09:09:24.742138: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3867707 -4.3868146 -4.3867311 -4.386641 -4.3865237 -4.3863711 -4.3863049 -4.3864288 -4.3867784 -4.3872185 -4.3876781 -4.3879428 -4.38764 -4.3868952 -4.3860078][-4.34095 -4.3404355 -4.3403625 -4.3401656 -4.3398533 -4.3394861 -4.339241 -4.3394623 -4.3402152 -4.341239 -4.3422632 -4.34288 -4.3425164 -4.3411655 -4.3388][-4.2354746 -4.2310376 -4.2291155 -4.2275186 -4.2263122 -4.2256932 -4.2254682 -4.2263322 -4.2287078 -4.2320757 -4.2357173 -4.238626 -4.2394152 -4.2376237 -4.2327857][-4.0428634 -4.0248547 -4.0142918 -4.0066853 -4.0025964 -4.0019312 -4.0034208 -4.0080104 -4.0165863 -4.0283594 -4.0414333 -4.0530066 -4.0596409 -4.0604496 -4.0545273][-3.7864282 -3.736634 -3.7030225 -3.6796045 -3.6674316 -3.6652839 -3.6706767 -3.6845489 -3.7075064 -3.7385492 -3.7744212 -3.8087142 -3.8338549 -3.847631 -3.8491478][-3.5766969 -3.470844 -3.3884025 -3.3291571 -3.2965078 -3.2875447 -3.2974682 -3.3265676 -3.3751693 -3.441478 -3.5193415 -3.5974128 -3.6618896 -3.7072864 -3.7321858][-3.5199561 -3.3419023 -3.1855416 -3.0665126 -2.9937773 -2.9650216 -2.9742627 -3.0192082 -3.1005092 -3.2145619 -3.3503554 -3.49062 -3.615273 -3.7111592 -3.774786][-3.6306281 -3.3963108 -3.170012 -2.9855082 -2.861371 -2.8017197 -2.8006151 -2.8525705 -2.959137 -3.1160302 -3.3059444 -3.5062466 -3.6917918 -3.8409948 -3.9447412][-3.8399692 -3.5966594 -3.3437383 -3.1226282 -2.9626384 -2.8771853 -2.8633578 -2.9122515 -3.0242856 -3.194694 -3.4044459 -3.6279638 -3.8400156 -4.0153828 -4.1392436][-4.0401015 -3.8409925 -3.62314 -3.422642 -3.269897 -3.1836014 -3.1654539 -3.2060061 -3.2984715 -3.4387381 -3.6144328 -3.8051319 -3.9906201 -4.1485443 -4.2621851][-4.1984696 -4.0720572 -3.9292483 -3.7933679 -3.6869781 -3.6253793 -3.611378 -3.6379027 -3.6959178 -3.781939 -3.8926916 -4.0172749 -4.1416616 -4.2501984 -4.3293605][-4.3088369 -4.2483916 -4.1792006 -4.111187 -4.0562639 -4.0239792 -4.0164008 -4.0285158 -4.0541205 -4.092742 -4.1454573 -4.2072797 -4.2701116 -4.3261156 -4.3674316][-4.3683047 -4.3468876 -4.322278 -4.29797 -4.277483 -4.2648182 -4.2610583 -4.2642746 -4.27209 -4.2852807 -4.30476 -4.3276706 -4.3513689 -4.3726687 -4.3884835][-4.3932109 -4.3880835 -4.3818612 -4.37564 -4.369844 -4.3657875 -4.3635588 -4.3630915 -4.3645592 -4.3684916 -4.3745894 -4.3812904 -4.3881168 -4.3940859 -4.3981643][-4.3987412 -4.3972068 -4.3952241 -4.393414 -4.3916087 -4.3900118 -4.3889451 -4.3886456 -4.3891191 -4.3908582 -4.3935122 -4.3962021 -4.398572 -4.4003286 -4.4011121]]...]
INFO - root - 2017-12-05 09:09:26.807941: step 9110, loss = 1.17, batch loss = 1.11 (39.0 examples/sec; 0.205 sec/batch; 18h:25m:57s remains)
INFO - root - 2017-12-05 09:09:28.885395: step 9120, loss = 1.40, batch loss = 1.34 (39.1 examples/sec; 0.205 sec/batch; 18h:22m:44s remains)
INFO - root - 2017-12-05 09:09:30.972493: step 9130, loss = 1.49, batch loss = 1.43 (39.7 examples/sec; 0.201 sec/batch; 18h:05m:47s remains)
INFO - root - 2017-12-05 09:09:33.061220: step 9140, loss = 1.64, batch loss = 1.59 (39.1 examples/sec; 0.205 sec/batch; 18h:23m:24s remains)
INFO - root - 2017-12-05 09:09:35.139857: step 9150, loss = 1.30, batch loss = 1.24 (38.3 examples/sec; 0.209 sec/batch; 18h:45m:20s remains)
INFO - root - 2017-12-05 09:09:37.230035: step 9160, loss = 1.50, batch loss = 1.45 (37.1 examples/sec; 0.215 sec/batch; 19h:20m:58s remains)
INFO - root - 2017-12-05 09:09:39.348307: step 9170, loss = 1.39, batch loss = 1.34 (36.6 examples/sec; 0.219 sec/batch; 19h:39m:02s remains)
INFO - root - 2017-12-05 09:09:41.438274: step 9180, loss = 2.12, batch loss = 2.06 (38.6 examples/sec; 0.207 sec/batch; 18h:36m:19s remains)
INFO - root - 2017-12-05 09:09:43.525843: step 9190, loss = 1.46, batch loss = 1.40 (38.3 examples/sec; 0.209 sec/batch; 18h:44m:31s remains)
INFO - root - 2017-12-05 09:09:45.567463: step 9200, loss = 1.27, batch loss = 1.21 (38.8 examples/sec; 0.206 sec/batch; 18h:31m:31s remains)
2017-12-05 09:09:45.836766: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1669226 -3.9040499 -3.5130849 -2.99896 -2.4130647 -1.8509672 -1.4217877 -1.1948218 -1.1697598 -1.3225315 -1.6092367 -1.9764745 -2.3732519 -2.7621527 -3.1175146][-3.9590251 -3.4633138 -2.7244146 -1.7617521 -0.67974496 0.34607697 1.1184354 1.5033903 1.4985385 1.1665449 0.60595179 -0.081799507 -0.80186868 -1.4976065 -2.1286588][-3.6865211 -2.8825779 -1.686852 -0.14900875 1.5547757 3.147994 4.3278551 4.8818932 4.8098 4.2376528 3.3378773 2.2675481 1.1651111 0.10035419 -0.8751924][-3.3799243 -2.2318041 -0.53008652 1.6361604 4.0098591 6.1997547 7.7905226 8.4901009 8.3105221 7.4660931 6.2213068 4.787097 3.3274312 1.9061923 0.5751071][-3.0966151 -1.6338663 0.53061008 3.27105 6.2506533 8.9648762 10.894166 11.686597 11.377735 10.277349 8.7481031 7.0422182 5.32172 3.6269474 1.992209][-2.885448 -1.1879947 1.3228912 4.4914503 7.9069757 10.972352 13.095027 13.896184 13.435969 12.124873 10.422066 8.5990734 6.7763252 4.9491625 3.1211762][-2.7636051 -0.93091369 1.7736163 5.1689072 8.7888222 11.97519 14.104665 14.817046 14.210364 12.781672 11.054836 9.2904329 7.5394115 5.7365856 3.8440742][-2.729208 -0.86072659 1.8821788 5.2938433 8.8817768 11.959468 13.911962 14.443257 13.726628 12.304225 10.703691 9.1470194 7.602807 5.9473181 4.11082][-2.7798514 -0.97493315 1.657093 4.8939896 8.2404432 11.026794 12.683034 12.994538 12.212766 10.88223 9.49188 8.216979 6.955945 5.533556 3.8595362][-2.9161024 -1.2673361 1.1188645 4.0171537 6.9598074 9.3348083 10.643065 10.745381 9.9419155 8.74954 7.5927191 6.5997009 5.6246872 4.4684973 3.0371923][-3.1234617 -1.7128775 0.31339598 2.7423196 5.1628819 7.0535192 8.0087757 7.9484377 7.1803923 6.1595831 5.2351971 4.4957256 3.7821169 2.8971429 1.7580562][-3.3865576 -2.2773066 -0.69733524 1.1726065 3.0014024 4.3826313 5.0195532 4.8735075 4.2161131 3.4090872 2.7176104 2.2031546 1.7207904 1.101377 0.2738409][-3.6755471 -2.8913021 -1.782311 -0.48602128 0.75921822 1.6688294 2.0485435 1.8853822 1.3864779 0.811883 0.34402132 0.023599625 -0.26399612 -0.6457181 -1.1793389][-3.9489198 -3.4654791 -2.7846074 -1.9960396 -1.2525041 -0.72915363 -0.53835297 -0.67524052 -1.0057802 -1.3662739 -1.6422977 -1.8107255 -1.9498799 -2.144773 -2.4396961][-4.1688008 -3.9212089 -3.5706694 -3.1658325 -2.7883153 -2.5306971 -2.4493294 -2.5360012 -2.7195892 -2.9151287 -3.0579271 -3.1331413 -3.1841702 -3.2623024 -3.3967831]]...]
INFO - root - 2017-12-05 09:09:47.911432: step 9210, loss = 1.73, batch loss = 1.67 (39.1 examples/sec; 0.204 sec/batch; 18h:21m:18s remains)
INFO - root - 2017-12-05 09:09:49.988217: step 9220, loss = 1.45, batch loss = 1.40 (39.3 examples/sec; 0.204 sec/batch; 18h:17m:01s remains)
INFO - root - 2017-12-05 09:09:52.063500: step 9230, loss = 1.39, batch loss = 1.33 (39.2 examples/sec; 0.204 sec/batch; 18h:19m:56s remains)
INFO - root - 2017-12-05 09:09:54.127083: step 9240, loss = 1.48, batch loss = 1.42 (39.0 examples/sec; 0.205 sec/batch; 18h:24m:05s remains)
INFO - root - 2017-12-05 09:09:56.210548: step 9250, loss = 1.31, batch loss = 1.25 (39.2 examples/sec; 0.204 sec/batch; 18h:20m:10s remains)
INFO - root - 2017-12-05 09:09:58.313086: step 9260, loss = 1.46, batch loss = 1.40 (39.3 examples/sec; 0.204 sec/batch; 18h:16m:42s remains)
INFO - root - 2017-12-05 09:10:00.377478: step 9270, loss = 1.13, batch loss = 1.07 (40.1 examples/sec; 0.199 sec/batch; 17h:54m:07s remains)
INFO - root - 2017-12-05 09:10:02.444474: step 9280, loss = 1.31, batch loss = 1.25 (38.7 examples/sec; 0.207 sec/batch; 18h:34m:12s remains)
INFO - root - 2017-12-05 09:10:04.518284: step 9290, loss = 1.63, batch loss = 1.57 (39.0 examples/sec; 0.205 sec/batch; 18h:24m:26s remains)
INFO - root - 2017-12-05 09:10:06.607228: step 9300, loss = 1.44, batch loss = 1.38 (37.1 examples/sec; 0.216 sec/batch; 19h:21m:26s remains)
2017-12-05 09:10:07.041971: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3127346 -4.2918835 -4.2799139 -4.277812 -4.2848115 -4.2946897 -4.3015289 -4.303143 -4.2977247 -4.2867508 -4.2766428 -4.27677 -4.2920666 -4.3213673 -4.3558588][-4.2494955 -4.2177424 -4.2016706 -4.2016983 -4.2165413 -4.2356172 -4.2482061 -4.2503295 -4.2415237 -4.2258911 -4.2150483 -4.2223239 -4.25183 -4.2974396 -4.345129][-4.1811886 -4.1411371 -4.1236677 -4.1273847 -4.1506534 -4.1800036 -4.1989784 -4.201057 -4.1893134 -4.1716876 -4.1637006 -4.1805539 -4.2245636 -4.2844362 -4.3414197][-4.1306405 -4.089385 -4.0748835 -4.0829077 -4.1112638 -4.1453166 -4.1666121 -4.1678786 -4.1557379 -4.1407456 -4.1393352 -4.1656127 -4.2193031 -4.2855773 -4.3442082][-4.1042919 -4.0690494 -4.0615649 -4.0726461 -4.100338 -4.1316552 -4.1500597 -4.1504869 -4.140717 -4.1314216 -4.1380863 -4.1724691 -4.2302618 -4.2953839 -4.3499637][-4.0856986 -4.0613813 -4.0616837 -4.0737829 -4.0963845 -4.1209149 -4.1356778 -4.1379309 -4.1348395 -4.1346188 -4.149303 -4.1881304 -4.2454448 -4.3059278 -4.3550773][-4.0488482 -4.0365248 -4.0452347 -4.0578423 -4.0761757 -4.0969129 -4.1132097 -4.1232052 -4.1311398 -4.1415844 -4.1630445 -4.2036633 -4.2584586 -4.3142323 -4.3587484][-3.9867673 -3.9827833 -3.9986415 -4.0132356 -4.0321016 -4.0563688 -4.0816507 -4.1044254 -4.1250858 -4.14562 -4.1726856 -4.2144065 -4.2678967 -4.32111 -4.3625345][-3.9173822 -3.9124932 -3.9324739 -3.9525201 -3.9794767 -4.014874 -4.0537062 -4.0900393 -4.1214075 -4.1495538 -4.1815839 -4.2253962 -4.278307 -4.3295703 -4.367548][-3.8688245 -3.8572688 -3.8798494 -3.9083161 -3.9487696 -3.9995415 -4.0520158 -4.0991387 -4.1372514 -4.1694994 -4.2042046 -4.2478933 -4.2971811 -4.3427811 -4.374825][-3.8591065 -3.8411484 -3.8650312 -3.900924 -3.9539416 -4.0188751 -4.0821147 -4.135736 -4.1766996 -4.2098975 -4.243803 -4.2824821 -4.3228111 -4.3586617 -4.3827887][-3.8896451 -3.8695786 -3.8944128 -3.9353805 -3.9969273 -4.071475 -4.1413131 -4.197032 -4.236681 -4.2660923 -4.2937756 -4.322422 -4.3501949 -4.3741155 -4.3898263][-3.9556463 -3.9389472 -3.9637494 -4.0057521 -4.0687485 -4.1444407 -4.2136436 -4.2657909 -4.2996058 -4.32119 -4.3389564 -4.3558483 -4.3713465 -4.3848119 -4.3940315][-4.0505776 -4.0385065 -4.0588679 -4.0950689 -4.1509309 -4.21727 -4.276474 -4.319057 -4.34422 -4.3573709 -4.3662329 -4.373899 -4.3808885 -4.3877578 -4.3938336][-4.1597037 -4.1511383 -4.1628866 -4.1877928 -4.2293673 -4.2779961 -4.3197193 -4.3481455 -4.3633175 -4.3696995 -4.372808 -4.3753757 -4.378396 -4.383091 -4.3894782]]...]
INFO - root - 2017-12-05 09:10:09.129926: step 9310, loss = 1.55, batch loss = 1.50 (38.9 examples/sec; 0.206 sec/batch; 18h:28m:50s remains)
INFO - root - 2017-12-05 09:10:11.196968: step 9320, loss = 1.19, batch loss = 1.13 (39.4 examples/sec; 0.203 sec/batch; 18h:14m:53s remains)
INFO - root - 2017-12-05 09:10:13.290445: step 9330, loss = 1.21, batch loss = 1.15 (38.9 examples/sec; 0.206 sec/batch; 18h:27m:06s remains)
INFO - root - 2017-12-05 09:10:15.360861: step 9340, loss = 2.14, batch loss = 2.09 (38.7 examples/sec; 0.207 sec/batch; 18h:33m:06s remains)
INFO - root - 2017-12-05 09:10:17.433958: step 9350, loss = 1.03, batch loss = 0.97 (39.7 examples/sec; 0.202 sec/batch; 18h:05m:16s remains)
INFO - root - 2017-12-05 09:10:19.527009: step 9360, loss = 1.77, batch loss = 1.71 (37.2 examples/sec; 0.215 sec/batch; 19h:19m:06s remains)
INFO - root - 2017-12-05 09:10:21.606856: step 9370, loss = 1.21, batch loss = 1.15 (38.5 examples/sec; 0.208 sec/batch; 18h:38m:21s remains)
INFO - root - 2017-12-05 09:10:23.708657: step 9380, loss = 1.53, batch loss = 1.47 (36.6 examples/sec; 0.218 sec/batch; 19h:35m:53s remains)
INFO - root - 2017-12-05 09:10:25.802377: step 9390, loss = 1.51, batch loss = 1.45 (38.1 examples/sec; 0.210 sec/batch; 18h:50m:37s remains)
INFO - root - 2017-12-05 09:10:27.898396: step 9400, loss = 1.31, batch loss = 1.25 (37.7 examples/sec; 0.212 sec/batch; 19h:03m:24s remains)
2017-12-05 09:10:28.185487: I tensorflow/core/kernels/logging_ops.cc:79] [[[8.4543056 7.5600076 6.3642049 5.2638535 4.41184 3.8398614 3.5213842 3.351975 3.2987914 3.3379965 3.3682966 3.2140646 2.7467046 1.9554811 0.87054396][7.6527686 6.8019195 5.7237878 4.8115106 4.1758494 3.7909703 3.6034589 3.5008769 3.466011 3.5191126 3.5667849 3.4491892 3.0084691 2.2328219 1.1430459][6.940084 6.1939845 5.3053374 4.6327138 4.2405257 4.0591331 3.9989972 3.944222 3.897656 3.9033551 3.9067082 3.7552114 3.28377 2.4850521 1.3635054][6.9871678 6.4434819 5.8224664 5.4353709 5.314508 5.3332944 5.3705039 5.3175626 5.2007651 5.0861 4.9423957 4.6436458 4.0254741 3.0800366 1.8142934][7.7535548 7.5589013 7.2976117 7.2453437 7.4067492 7.6083369 7.7306466 7.66274 7.441957 7.1411815 6.764174 6.211802 5.3247809 4.1044526 2.5687304][8.81213 9.08453 9.2693214 9.6066818 10.051716 10.431538 10.634703 10.559834 10.232603 9.7257156 9.0558739 8.163147 6.90867 5.3111405 3.4192963][9.6767769 10.366682 10.951679 11.653551 12.37402 12.92675 13.229681 13.192553 12.800598 12.111019 11.139343 9.875555 8.2216434 6.2293477 3.9946923][9.7807827 10.721102 11.522314 12.436335 13.330593 14.026867 14.446798 14.501972 14.123339 13.34255 12.161545 10.615938 8.6618176 6.3979316 3.9663396][8.81517 9.7759743 10.574924 11.498512 12.426746 13.185375 13.682316 13.837481 13.528164 12.752743 11.501801 9.8539009 7.8172889 5.5223365 3.1483502][6.88067 7.6642947 8.2768955 9.0440235 9.8686771 10.593565 11.107159 11.331928 11.1147 10.430838 9.2574615 7.7048717 5.8243537 3.7542672 1.6854744][4.3995585 4.9041305 5.2439609 5.7468524 6.3588881 6.962873 7.4304414 7.6811576 7.5570741 7.024529 6.0556483 4.7589908 3.2125611 1.5481825 -0.060547352][1.82792 2.062695 2.141067 2.3591185 2.7229962 3.1499519 3.5125046 3.7448764 3.7033553 3.3539195 2.6689544 1.7348909 0.62981892 -0.54450035 -1.648972][-0.43006659 -0.38123369 -0.47258449 -0.46345806 -0.32534647 -0.086942673 0.13720989 0.30577421 0.31179142 0.1396265 -0.24565411 -0.79539418 -1.4503849 -2.1526976 -2.806711][-2.0917325 -2.1288548 -2.2694709 -2.356195 -2.35542 -2.2618327 -2.1583428 -2.0623114 -2.0379503 -2.0871742 -2.2437055 -2.487957 -2.7971318 -3.1497912 -3.4874909][-3.0831506 -3.1295547 -3.2450318 -3.346092 -3.40129 -3.3911438 -3.3664088 -3.3247643 -3.2976336 -3.2732673 -3.2860396 -3.348208 -3.4609752 -3.6184998 -3.7891693]]...]
INFO - root - 2017-12-05 09:10:30.248992: step 9410, loss = 1.28, batch loss = 1.22 (38.2 examples/sec; 0.210 sec/batch; 18h:49m:06s remains)
INFO - root - 2017-12-05 09:10:32.325540: step 9420, loss = 1.61, batch loss = 1.55 (38.7 examples/sec; 0.207 sec/batch; 18h:32m:35s remains)
INFO - root - 2017-12-05 09:10:34.407783: step 9430, loss = 1.64, batch loss = 1.59 (39.2 examples/sec; 0.204 sec/batch; 18h:19m:51s remains)
INFO - root - 2017-12-05 09:10:36.466487: step 9440, loss = 1.53, batch loss = 1.47 (39.0 examples/sec; 0.205 sec/batch; 18h:24m:02s remains)
INFO - root - 2017-12-05 09:10:38.537068: step 9450, loss = 1.90, batch loss = 1.84 (37.6 examples/sec; 0.213 sec/batch; 19h:05m:21s remains)
INFO - root - 2017-12-05 09:10:40.600747: step 9460, loss = 1.31, batch loss = 1.25 (37.9 examples/sec; 0.211 sec/batch; 18h:56m:17s remains)
INFO - root - 2017-12-05 09:10:42.658507: step 9470, loss = 1.45, batch loss = 1.39 (36.5 examples/sec; 0.219 sec/batch; 19h:39m:02s remains)
INFO - root - 2017-12-05 09:10:44.722367: step 9480, loss = 1.88, batch loss = 1.82 (38.8 examples/sec; 0.206 sec/batch; 18h:28m:38s remains)
INFO - root - 2017-12-05 09:10:46.845775: step 9490, loss = 1.89, batch loss = 1.83 (37.8 examples/sec; 0.211 sec/batch; 18h:57m:51s remains)
INFO - root - 2017-12-05 09:10:48.959645: step 9500, loss = 1.60, batch loss = 1.54 (36.0 examples/sec; 0.222 sec/batch; 19h:55m:17s remains)
2017-12-05 09:10:49.244884: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.970999 -3.5182467 -2.5208249 -0.84340262 1.4017253 3.840981 5.9077945 7.0711837 7.0597844 5.9693303 4.1946797 2.1994615 0.38148117 -0.99194813 -1.8143852][-3.6330185 -3.0200152 -1.7780902 0.2357583 2.8944092 5.76753 8.2171459 9.6408768 9.7169666 8.5334721 6.5090833 4.15041 1.9172635 0.13683987 -1.0366313][-3.0126266 -2.1820834 -0.69028354 1.5932269 4.5214353 7.6361976 10.281466 11.849274 12.002588 10.811304 8.6650581 6.0637975 3.4954271 1.3266463 -0.23048639][-2.0685244 -0.95379305 0.80666161 3.3117852 6.3803267 9.5474262 12.194386 13.765579 13.944935 12.776299 10.592062 7.8513513 5.0318456 2.5227776 0.58977652][-0.86749053 0.59172058 2.6478896 5.3481841 8.4632607 11.534225 14.012974 15.445522 15.575197 14.430328 12.269842 9.4817848 6.499455 3.7184777 1.4529371][0.44038963 2.2601805 4.6066937 7.4646058 10.556679 13.439072 15.651478 16.858955 16.888821 15.764601 13.682529 10.935917 7.8923936 4.9314666 2.3993192][1.5783372 3.7002721 6.2678885 9.2034416 12.207947 14.870131 16.803076 17.777981 17.711464 16.639301 14.692108 12.06913 9.0621891 6.0265231 3.3257928][2.1666527 4.4313712 7.0670934 9.9540024 12.79953 15.233694 16.921041 17.711767 17.601976 16.638037 14.896301 12.489943 9.6347561 6.6596484 3.9364295][1.9769902 4.1661086 6.6621938 9.3283939 11.905743 14.06662 15.518784 16.160509 16.050064 15.249584 13.791273 11.713928 9.170023 6.4533906 3.9103951][1.0954819 3.0067897 5.1731715 7.4605355 9.6607647 11.488838 12.697548 13.214363 13.127287 12.512011 11.378372 9.7129021 7.6127067 5.3258433 3.1500983][-0.17846203 1.321156 3.0247293 4.8192439 6.5502334 7.9899526 8.9506531 9.37599 9.3435516 8.9207077 8.1026382 6.86006 5.2585835 3.4910583 1.7927823][-1.5560818 -0.51157355 0.68602514 1.9557929 3.1902504 4.2234864 4.9280629 5.2654319 5.287746 5.0355964 4.4975915 3.6534734 2.5522017 1.3250008 0.1372962][-2.7898688 -2.1594067 -1.4207854 -0.62438607 0.15631628 0.80483389 1.2513757 1.4743528 1.5104513 1.379262 1.065589 0.56603909 -0.087545395 -0.81789136 -1.5313241][-3.6730509 -3.3507826 -2.9602489 -2.5280638 -2.1002202 -1.7568624 -1.5302622 -1.4286745 -1.4240658 -1.5061429 -1.6802096 -1.9338896 -2.2511864 -2.5976293 -2.9365556][-4.1390219 -3.9990442 -3.8212547 -3.6176095 -3.4126949 -3.2548981 -3.1632495 -3.1391444 -3.1657007 -3.2370689 -3.3456023 -3.4723368 -3.6074042 -3.7382913 -3.8560545]]...]
INFO - root - 2017-12-05 09:10:51.316691: step 9510, loss = 1.16, batch loss = 1.10 (38.8 examples/sec; 0.206 sec/batch; 18h:30m:57s remains)
INFO - root - 2017-12-05 09:10:53.378552: step 9520, loss = 1.88, batch loss = 1.82 (39.0 examples/sec; 0.205 sec/batch; 18h:22m:55s remains)
INFO - root - 2017-12-05 09:10:55.438255: step 9530, loss = 1.25, batch loss = 1.19 (38.5 examples/sec; 0.208 sec/batch; 18h:37m:43s remains)
INFO - root - 2017-12-05 09:10:57.503936: step 9540, loss = 1.54, batch loss = 1.48 (38.4 examples/sec; 0.208 sec/batch; 18h:41m:00s remains)
INFO - root - 2017-12-05 09:10:59.584607: step 9550, loss = 1.76, batch loss = 1.70 (39.4 examples/sec; 0.203 sec/batch; 18h:12m:57s remains)
INFO - root - 2017-12-05 09:11:01.671640: step 9560, loss = 1.65, batch loss = 1.59 (36.4 examples/sec; 0.220 sec/batch; 19h:42m:01s remains)
INFO - root - 2017-12-05 09:11:03.789328: step 9570, loss = 1.62, batch loss = 1.56 (37.8 examples/sec; 0.212 sec/batch; 19h:00m:24s remains)
INFO - root - 2017-12-05 09:11:05.877960: step 9580, loss = 2.10, batch loss = 2.04 (38.6 examples/sec; 0.207 sec/batch; 18h:34m:24s remains)
INFO - root - 2017-12-05 09:11:07.961188: step 9590, loss = 1.69, batch loss = 1.63 (38.3 examples/sec; 0.209 sec/batch; 18h:43m:41s remains)
INFO - root - 2017-12-05 09:11:10.034402: step 9600, loss = 1.16, batch loss = 1.10 (36.4 examples/sec; 0.220 sec/batch; 19h:44m:19s remains)
2017-12-05 09:11:10.344794: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3840451 -4.3896585 -4.3878031 -4.3512917 -4.23703 -4.0018077 -3.6664891 -3.2708578 -2.8661063 -2.4937177 -2.185816 -1.9679034 -1.8319008 -1.7799733 -1.822238][-4.2890396 -4.3204713 -4.3375082 -4.2943778 -4.1320314 -3.8047 -3.3567619 -2.8655825 -2.4115009 -2.0389626 -1.7657552 -1.5942228 -1.4946091 -1.4525406 -1.4791887][-4.0301037 -4.128499 -4.200561 -4.1775217 -3.9922457 -3.6019015 -3.0685453 -2.5094304 -2.0303094 -1.6786089 -1.4649751 -1.3777869 -1.3683312 -1.3946187 -1.4542274][-3.5584974 -3.7572143 -3.9204326 -3.9575093 -3.7979672 -3.4000111 -2.831305 -2.240582 -1.7560318 -1.4353888 -1.2953796 -1.3198051 -1.4426413 -1.587141 -1.7245362][-2.9236803 -3.2158413 -3.4771121 -3.5956111 -3.4922428 -3.1269751 -2.5640333 -1.9753203 -1.504703 -1.2313681 -1.1872125 -1.350297 -1.6342475 -1.9257939 -2.1675723][-2.2730322 -2.6051667 -2.9208355 -3.09484 -3.0327063 -2.6959248 -2.1506505 -1.5876827 -1.158375 -0.963192 -1.0473652 -1.3766124 -1.8387711 -2.2854838 -2.6389244][-1.7528603 -2.0516405 -2.3435628 -2.495954 -2.4186165 -2.0762329 -1.5436594 -1.0257592 -0.67259765 -0.59418154 -0.83669853 -1.3434174 -1.9756427 -2.5608203 -3.0113375][-1.439656 -1.6376877 -1.8298755 -1.8844907 -1.7332811 -1.3565438 -0.84272075 -0.3932395 -0.15090942 -0.21527863 -0.61855364 -1.2796767 -2.0434034 -2.727561 -3.2419639][-1.3401055 -1.4064837 -1.4608467 -1.3889346 -1.1486547 -0.74623442 -0.27604771 0.084703445 0.20737267 0.011214256 -0.51395941 -1.2702227 -2.1013584 -2.8279479 -3.3619556][-1.435859 -1.3827436 -1.3117592 -1.1395211 -0.85031986 -0.46937346 -0.079802513 0.18083048 0.20776939 -0.064181805 -0.63376212 -1.3979316 -2.2143686 -2.917969 -3.423986][-1.7221799 -1.5914848 -1.4437301 -1.2304044 -0.95109034 -0.63547945 -0.34528446 -0.17591524 -0.2026844 -0.47961283 -1.0075018 -1.6931796 -2.4148006 -3.0295181 -3.4605656][-2.1538219 -1.998467 -1.8333697 -1.6319003 -1.3988056 -1.1588185 -0.95529151 -0.84784317 -0.88712716 -1.1169097 -1.5440948 -2.0931437 -2.664983 -3.1441267 -3.4654975][-2.6186032 -2.4851155 -2.3453932 -2.1796176 -1.993881 -1.807878 -1.6538341 -1.569278 -1.5898082 -1.7489882 -2.0523281 -2.4440489 -2.8467755 -3.1723242 -3.37085][-2.926136 -2.834981 -2.74124 -2.6165817 -2.4659963 -2.3064737 -2.1650693 -2.0697412 -2.0506573 -2.1333258 -2.3199677 -2.5684288 -2.8182092 -3.0046847 -3.093678][-2.9026084 -2.8474064 -2.8028305 -2.7202508 -2.5987782 -2.4486728 -2.2948761 -2.1674674 -2.0985286 -2.1109409 -2.2048807 -2.3472416 -2.4881921 -2.5770292 -2.5932636]]...]
INFO - root - 2017-12-05 09:11:12.429984: step 9610, loss = 1.18, batch loss = 1.12 (39.1 examples/sec; 0.204 sec/batch; 18h:20m:17s remains)
INFO - root - 2017-12-05 09:11:14.490063: step 9620, loss = 1.32, batch loss = 1.26 (38.5 examples/sec; 0.208 sec/batch; 18h:37m:33s remains)
INFO - root - 2017-12-05 09:11:16.565063: step 9630, loss = 1.36, batch loss = 1.30 (39.1 examples/sec; 0.205 sec/batch; 18h:21m:27s remains)
INFO - root - 2017-12-05 09:11:18.654581: step 9640, loss = 1.57, batch loss = 1.51 (38.2 examples/sec; 0.209 sec/batch; 18h:46m:26s remains)
INFO - root - 2017-12-05 09:11:20.758304: step 9650, loss = 1.21, batch loss = 1.16 (38.0 examples/sec; 0.211 sec/batch; 18h:52m:59s remains)
INFO - root - 2017-12-05 09:11:22.835550: step 9660, loss = 1.29, batch loss = 1.24 (39.0 examples/sec; 0.205 sec/batch; 18h:24m:08s remains)
INFO - root - 2017-12-05 09:11:24.984110: step 9670, loss = 1.17, batch loss = 1.11 (38.1 examples/sec; 0.210 sec/batch; 18h:50m:50s remains)
INFO - root - 2017-12-05 09:11:27.050758: step 9680, loss = 1.33, batch loss = 1.27 (39.9 examples/sec; 0.201 sec/batch; 17h:59m:49s remains)
INFO - root - 2017-12-05 09:11:29.096152: step 9690, loss = 1.75, batch loss = 1.69 (38.6 examples/sec; 0.207 sec/batch; 18h:33m:49s remains)
INFO - root - 2017-12-05 09:11:31.189764: step 9700, loss = 1.08, batch loss = 1.02 (39.1 examples/sec; 0.205 sec/batch; 18h:21m:54s remains)
2017-12-05 09:11:31.475207: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.1552329 -4.1490469 -4.1486707 -4.1512861 -4.1560082 -4.1618829 -4.1682744 -4.1733065 -4.1740522 -4.1690016 -4.1579304 -4.1427808 -4.12552 -4.1078262 -4.0914426][-4.1625113 -4.14185 -4.1286049 -4.1208625 -4.1166668 -4.113883 -4.1119308 -4.1092434 -4.1014347 -4.0866404 -4.0642285 -4.0378647 -4.0121422 -3.9926145 -3.9831491][-4.2138596 -4.1809521 -4.1546736 -4.1348066 -4.1187444 -4.1044431 -4.0922132 -4.0805578 -4.0642362 -4.0416317 -4.0126271 -3.9818661 -3.955745 -3.9421179 -3.9456825][-4.2612934 -4.21941 -4.1817904 -4.1497378 -4.1209736 -4.0949736 -4.0729952 -4.0533919 -4.0319715 -4.0089679 -3.985276 -3.9648829 -3.9542158 -3.9589014 -3.981725][-4.2801275 -4.2276254 -4.176187 -4.1293373 -4.0872812 -4.0511475 -4.0222511 -3.9982407 -3.9780602 -3.9653091 -3.9606445 -3.9655409 -3.9837971 -4.0145516 -4.0570827][-4.2722073 -4.2065816 -4.1391673 -4.0762305 -4.0209837 -3.9759576 -3.9437208 -3.9220138 -3.9121819 -3.91939 -3.9428236 -3.9789951 -4.0268736 -4.0790873 -4.1340995][-4.2568488 -4.1806459 -4.1003222 -4.0251012 -3.9608531 -3.9107935 -3.8800879 -3.8672996 -3.8736482 -3.9026194 -3.9524109 -4.0151377 -4.0835648 -4.148129 -4.2067575][-4.2576857 -4.1806731 -4.0989237 -4.0223026 -3.9577224 -3.9095161 -3.8850553 -3.8829265 -3.9031305 -3.9469831 -4.0120778 -4.0864358 -4.1591372 -4.2224703 -4.2747416][-4.2866139 -4.2231417 -4.1555862 -4.0916862 -4.0379777 -3.9993496 -3.9828584 -3.9881659 -4.013617 -4.0587206 -4.1199875 -4.1846375 -4.24309 -4.291719 -4.3295484][-4.3297849 -4.2888041 -4.2455258 -4.2044382 -4.1699786 -4.1458826 -4.1368361 -4.1434288 -4.1640787 -4.1972165 -4.2388296 -4.2798262 -4.3153753 -4.3447356 -4.366477][-4.36556 -4.3444524 -4.3225803 -4.3025179 -4.2856431 -4.2739139 -4.2701392 -4.2747846 -4.2867322 -4.3041873 -4.3252072 -4.3456092 -4.3631525 -4.3774257 -4.387711][-4.386652 -4.377418 -4.3681984 -4.3603139 -4.3538275 -4.3492851 -4.3480921 -4.350246 -4.3553576 -4.36257 -4.3714032 -4.3800344 -4.3875456 -4.3935909 -4.3978505][-4.3968258 -4.39311 -4.3894868 -4.3865719 -4.3844953 -4.3832674 -4.3829851 -4.3839025 -4.3860655 -4.3889465 -4.3923335 -4.3955584 -4.3983817 -4.400619 -4.4021773][-4.4016781 -4.4002285 -4.3987789 -4.3974872 -4.3967805 -4.39655 -4.3964691 -4.3969159 -4.3979149 -4.3990359 -4.4002476 -4.4014859 -4.402607 -4.4034023 -4.4038935][-4.4039645 -4.4034615 -4.40289 -4.402348 -4.4020391 -4.4019527 -4.4019523 -4.4021134 -4.4023867 -4.4026823 -4.4031 -4.4035735 -4.4039893 -4.4043012 -4.4044981]]...]
INFO - root - 2017-12-05 09:11:33.543202: step 9710, loss = 1.46, batch loss = 1.40 (38.2 examples/sec; 0.209 sec/batch; 18h:46m:49s remains)
INFO - root - 2017-12-05 09:11:35.610191: step 9720, loss = 1.37, batch loss = 1.31 (39.5 examples/sec; 0.203 sec/batch; 18h:09m:49s remains)
INFO - root - 2017-12-05 09:11:37.693732: step 9730, loss = 1.04, batch loss = 0.99 (38.2 examples/sec; 0.210 sec/batch; 18h:47m:39s remains)
INFO - root - 2017-12-05 09:11:39.779307: step 9740, loss = 1.45, batch loss = 1.39 (40.0 examples/sec; 0.200 sec/batch; 17h:56m:40s remains)
INFO - root - 2017-12-05 09:11:41.861059: step 9750, loss = 1.44, batch loss = 1.38 (37.3 examples/sec; 0.214 sec/batch; 19h:13m:31s remains)
INFO - root - 2017-12-05 09:11:43.978130: step 9760, loss = 2.00, batch loss = 1.94 (37.8 examples/sec; 0.212 sec/batch; 18h:58m:29s remains)
INFO - root - 2017-12-05 09:11:46.030360: step 9770, loss = 1.73, batch loss = 1.68 (37.4 examples/sec; 0.214 sec/batch; 19h:11m:15s remains)
INFO - root - 2017-12-05 09:11:48.120842: step 9780, loss = 0.95, batch loss = 0.89 (38.4 examples/sec; 0.209 sec/batch; 18h:41m:46s remains)
INFO - root - 2017-12-05 09:11:50.211729: step 9790, loss = 1.36, batch loss = 1.30 (38.7 examples/sec; 0.207 sec/batch; 18h:32m:08s remains)
INFO - root - 2017-12-05 09:11:52.335794: step 9800, loss = 1.49, batch loss = 1.43 (38.7 examples/sec; 0.207 sec/batch; 18h:31m:13s remains)
2017-12-05 09:11:52.661959: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4000888 -4.3999367 -4.3997889 -4.399641 -4.3995261 -4.399467 -4.3994026 -4.3993354 -4.3992867 -4.3992782 -4.3992076 -4.3990855 -4.3988423 -4.3985572 -4.3981609][-4.3972921 -4.3968587 -4.3965149 -4.3963218 -4.3962145 -4.3962474 -4.3963146 -4.3963208 -4.3962841 -4.3962045 -4.3960266 -4.3957481 -4.3953252 -4.3948836 -4.3943005][-4.3891292 -4.3884029 -4.3879557 -4.3875966 -4.3873625 -4.3874083 -4.3875465 -4.3876524 -4.3877349 -4.3876448 -4.38729 -4.3866777 -4.3858895 -4.3851185 -4.3843417][-4.3744907 -4.373342 -4.3728185 -4.3722873 -4.3719029 -4.3718195 -4.3718557 -4.3719382 -4.3719759 -4.3717217 -4.37121 -4.3704128 -4.3693657 -4.3679619 -4.366497][-4.3581104 -4.3562183 -4.3552718 -4.3544664 -4.3539276 -4.3537455 -4.3538241 -4.3538885 -4.3538604 -4.3533974 -4.3527379 -4.3516793 -4.3502221 -4.3480091 -4.3454413][-4.3488297 -4.346067 -4.3445024 -4.3430448 -4.34207 -4.3418374 -4.3422451 -4.3428407 -4.3434386 -4.3434544 -4.3433533 -4.3424768 -4.3407307 -4.3377123 -4.3340874][-4.3565135 -4.3533678 -4.3509464 -4.3483644 -4.3462157 -4.3452721 -4.3454857 -4.3463912 -4.3474841 -4.3481889 -4.34875 -4.3484306 -4.3467197 -4.3433547 -4.3387718][-4.3748975 -4.3719845 -4.3693762 -4.3665485 -4.3638878 -4.3623719 -4.3620753 -4.3627467 -4.3639607 -4.364985 -4.36561 -4.365263 -4.3635688 -4.3598142 -4.3543196][-4.3917589 -4.3892379 -4.3867435 -4.3841767 -4.3816514 -4.3803172 -4.3802409 -4.3810658 -4.3822193 -4.3831081 -4.3831878 -4.3820977 -4.3798251 -4.3755779 -4.3692465][-4.3977122 -4.3953824 -4.3931227 -4.3909221 -4.3889155 -4.3878636 -4.3880358 -4.3888927 -4.3897624 -4.3902383 -4.39006 -4.3889189 -4.3864932 -4.382122 -4.3758345][-4.3976889 -4.394906 -4.3920808 -4.3894629 -4.3875093 -4.3867531 -4.3871388 -4.3880596 -4.3888421 -4.3892813 -4.3893046 -4.3886161 -4.3865476 -4.3824177 -4.3763275][-4.3961778 -4.3926859 -4.3887644 -4.384984 -4.38218 -4.3808017 -4.380774 -4.3817296 -4.382988 -4.3840318 -4.3845558 -4.3842468 -4.3823752 -4.3785176 -4.3726621][-4.3943295 -4.3902135 -4.3854136 -4.3805103 -4.3764582 -4.3735642 -4.3721023 -4.3722272 -4.3734512 -4.3749447 -4.3761239 -4.3763247 -4.3748584 -4.3713269 -4.366045][-4.3918328 -4.3874331 -4.3824234 -4.3773007 -4.3725786 -4.3685241 -4.3654675 -4.3639073 -4.3638968 -4.3646579 -4.3657684 -4.3662949 -4.3653007 -4.3624353 -4.3581104][-4.3885078 -4.3839874 -4.379137 -4.3744 -4.3700037 -4.3659859 -4.3626432 -4.3601303 -4.3589864 -4.3587437 -4.3592587 -4.3595328 -4.3586926 -4.3561859 -4.3525648]]...]
INFO - root - 2017-12-05 09:11:54.823130: step 9810, loss = 1.39, batch loss = 1.33 (38.7 examples/sec; 0.207 sec/batch; 18h:32m:33s remains)
INFO - root - 2017-12-05 09:11:56.928502: step 9820, loss = 1.97, batch loss = 1.91 (39.1 examples/sec; 0.204 sec/batch; 18h:19m:41s remains)
INFO - root - 2017-12-05 09:11:59.016932: step 9830, loss = 1.30, batch loss = 1.24 (39.2 examples/sec; 0.204 sec/batch; 18h:18m:07s remains)
INFO - root - 2017-12-05 09:12:01.084494: step 9840, loss = 1.15, batch loss = 1.09 (39.2 examples/sec; 0.204 sec/batch; 18h:17m:42s remains)
INFO - root - 2017-12-05 09:12:03.170381: step 9850, loss = 1.34, batch loss = 1.28 (38.0 examples/sec; 0.210 sec/batch; 18h:50m:52s remains)
INFO - root - 2017-12-05 09:12:05.242551: step 9860, loss = 1.25, batch loss = 1.19 (38.7 examples/sec; 0.207 sec/batch; 18h:31m:21s remains)
INFO - root - 2017-12-05 09:12:07.336187: step 9870, loss = 1.76, batch loss = 1.70 (37.5 examples/sec; 0.213 sec/batch; 19h:07m:22s remains)
INFO - root - 2017-12-05 09:12:09.460591: step 9880, loss = 1.35, batch loss = 1.29 (37.9 examples/sec; 0.211 sec/batch; 18h:53m:33s remains)
INFO - root - 2017-12-05 09:12:11.563133: step 9890, loss = 1.26, batch loss = 1.20 (39.3 examples/sec; 0.204 sec/batch; 18h:15m:44s remains)
INFO - root - 2017-12-05 09:12:13.685063: step 9900, loss = 1.27, batch loss = 1.21 (37.5 examples/sec; 0.213 sec/batch; 19h:07m:45s remains)
2017-12-05 09:12:13.988757: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.8755744 -3.8749003 -3.8845677 -3.9046998 -3.9351795 -3.9713373 -4.0101619 -4.0492091 -4.0864434 -4.1172061 -4.1363916 -4.14067 -4.1278534 -4.0961022 -4.0480108][-3.4604578 -3.4530571 -3.4625306 -3.489903 -3.5339551 -3.5867064 -3.644155 -3.7032602 -3.760221 -3.8075461 -3.8362131 -3.8412178 -3.8176277 -3.7608397 -3.6801839][-2.9038281 -2.8745065 -2.8705616 -2.8938363 -2.9406838 -3.0001416 -3.0695388 -3.1467719 -3.2263322 -3.2967105 -3.3441386 -3.3598409 -3.3299351 -3.2460656 -3.130847][-2.3021326 -2.2308228 -2.1906981 -2.1820688 -2.2001317 -2.2370765 -2.2943709 -2.3742616 -2.4714675 -2.5708342 -2.6524189 -2.6989522 -2.6823707 -2.5916185 -2.4619625][-1.7828724 -1.6585441 -1.5646565 -1.4951727 -1.4455919 -1.4190876 -1.4266152 -1.4811189 -1.5817411 -1.7094114 -1.8367929 -1.93558 -1.9618104 -1.9012578 -1.7946255][-1.4123323 -1.2402983 -1.0914211 -0.94961405 -0.8126049 -0.69802213 -0.6314733 -0.64021969 -0.72882795 -0.87572837 -1.048641 -1.2071517 -1.2964756 -1.2961791 -1.2487996][-1.1581504 -0.95751405 -0.764169 -0.55445766 -0.33415031 -0.13918638 -0.011082649 0.015210152 -0.061988831 -0.21740961 -0.41993594 -0.62833428 -0.78288889 -0.85773087 -0.89501071][-0.95720243 -0.741158 -0.51352763 -0.24938726 0.029249191 0.27134848 0.42362928 0.45265532 0.37091684 0.20786953 -0.0092954636 -0.24917126 -0.45884848 -0.61137009 -0.74407506][-0.78973174 -0.55349422 -0.29059458 0.012451172 0.31566525 0.55790758 0.6864171 0.68312025 0.57604742 0.39923763 0.17666817 -0.075381279 -0.32456017 -0.54717994 -0.7720685][-0.65501046 -0.37460804 -0.0716095 0.24703026 0.530972 0.72528648 0.791553 0.73353624 0.59207535 0.39826155 0.16798115 -0.096324921 -0.37837887 -0.65882492 -0.95671892][-0.55100822 -0.22168255 0.1037569 0.40228462 0.6274085 0.73869085 0.72327375 0.60463524 0.4302702 0.2232461 -0.017325878 -0.3010788 -0.6148963 -0.93717027 -1.2760634][-0.49347687 -0.14025593 0.17039967 0.41396904 0.55497646 0.57067013 0.47154522 0.29609632 0.092661858 -0.12352037 -0.37273932 -0.67229748 -1.005532 -1.342391 -1.6798294][-0.53810072 -0.20782614 0.047539711 0.21327353 0.26710081 0.19683409 0.026637077 -0.19684267 -0.42552257 -0.64995813 -0.89986014 -1.194768 -1.5139632 -1.8219466 -2.1143875][-0.79593635 -0.51618934 -0.3326683 -0.2478466 -0.27182913 -0.40608263 -0.62238455 -0.8756671 -1.1217206 -1.3507476 -1.5893207 -1.8516774 -2.117527 -2.3562186 -2.5735762][-1.3573761 -1.1375449 -1.0194924 -0.99555326 -1.0696576 -1.2331855 -1.459708 -1.7119613 -1.9530239 -2.1710188 -2.3778019 -2.5816014 -2.7703536 -2.9257588 -3.0631094]]...]
INFO - root - 2017-12-05 09:12:16.089202: step 9910, loss = 1.57, batch loss = 1.51 (38.1 examples/sec; 0.210 sec/batch; 18h:48m:09s remains)
INFO - root - 2017-12-05 09:12:18.169401: step 9920, loss = 1.42, batch loss = 1.36 (38.5 examples/sec; 0.208 sec/batch; 18h:35m:55s remains)
INFO - root - 2017-12-05 09:12:20.239651: step 9930, loss = 1.54, batch loss = 1.48 (39.5 examples/sec; 0.203 sec/batch; 18h:09m:56s remains)
INFO - root - 2017-12-05 09:12:22.295742: step 9940, loss = 1.25, batch loss = 1.19 (39.4 examples/sec; 0.203 sec/batch; 18h:11m:09s remains)
INFO - root - 2017-12-05 09:12:24.356008: step 9950, loss = 1.21, batch loss = 1.15 (39.2 examples/sec; 0.204 sec/batch; 18h:16m:09s remains)
INFO - root - 2017-12-05 09:12:26.448093: step 9960, loss = 1.42, batch loss = 1.37 (37.9 examples/sec; 0.211 sec/batch; 18h:54m:16s remains)
INFO - root - 2017-12-05 09:12:28.574601: step 9970, loss = 1.41, batch loss = 1.36 (37.5 examples/sec; 0.214 sec/batch; 19h:07m:43s remains)
INFO - root - 2017-12-05 09:12:30.651709: step 9980, loss = 1.35, batch loss = 1.29 (39.4 examples/sec; 0.203 sec/batch; 18h:10m:55s remains)
INFO - root - 2017-12-05 09:12:32.746204: step 9990, loss = 1.31, batch loss = 1.25 (37.7 examples/sec; 0.212 sec/batch; 18h:59m:52s remains)
INFO - root - 2017-12-05 09:12:34.869718: step 10000, loss = 2.14, batch loss = 2.08 (37.6 examples/sec; 0.213 sec/batch; 19h:02m:32s remains)
2017-12-05 09:12:35.141460: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3789892 -4.3594532 -4.3372631 -4.3145509 -4.294323 -4.2776661 -4.265718 -4.2607951 -4.2612162 -4.2683282 -4.2835717 -4.3075066 -4.335372 -4.3606076 -4.3799367][-4.325666 -4.2654176 -4.1938829 -4.1176963 -4.0447917 -3.9806724 -3.9317102 -3.9053817 -3.8996267 -3.9197595 -3.9700849 -4.0492425 -4.1429324 -4.232614 -4.305737][-4.2236447 -4.08257 -3.9102604 -3.7228165 -3.5368879 -3.3713541 -3.2443907 -3.1698341 -3.1472116 -3.1899834 -3.3073473 -3.4944544 -3.7231784 -3.9508486 -4.1423645][-4.0754452 -3.8104627 -3.4810286 -3.1192348 -2.7532902 -2.4257307 -2.1754856 -2.0239561 -1.9730349 -2.0458403 -2.2621875 -2.613018 -3.0498514 -3.4928675 -3.873508][-3.8964028 -3.475467 -2.9460309 -2.3604429 -1.7655003 -1.2322161 -0.82564521 -0.58024693 -0.504385 -0.62575674 -0.9715035 -1.5260186 -2.2157576 -2.9187937 -3.5299845][-3.7107544 -3.1199665 -2.3725841 -1.5442154 -0.705709 0.039139271 0.59886169 0.92379951 0.99435997 0.78744316 0.28420305 -0.48568892 -1.4228652 -2.3722949 -3.2003579][-3.5481415 -2.7999072 -1.8512278 -0.80198932 0.24830723 1.1624799 1.829298 2.18965 2.2106404 1.8848853 1.2199974 0.26709938 -0.85750318 -1.9850841 -2.9690022][-3.4242659 -2.5527015 -1.4464028 -0.23220634 0.95658016 1.9557757 2.6470857 2.9762902 2.9026022 2.4419274 1.6393442 0.5702424 -0.64631057 -1.8487597 -2.8942616][-3.3466449 -2.4054258 -1.2159657 0.069231033 1.2814813 2.2439618 2.8469152 3.0608592 2.8655252 2.2986836 1.4263062 0.3371315 -0.85378337 -2.0079827 -3.0016479][-3.3274279 -2.389744 -1.2200854 0.011926174 1.116776 1.9245133 2.3523483 2.406405 2.10396 1.4945326 0.64349127 -0.36026764 -1.4140801 -2.4106455 -3.2546339][-3.3958144 -2.5492332 -1.515317 -0.4585669 0.4364171 1.0266147 1.2637849 1.1847467 0.83362913 0.25920486 -0.48394084 -1.3193445 -2.1625116 -2.9364707 -3.5746522][-3.5586312 -2.882627 -2.0790148 -1.286083 -0.65371919 -0.28343678 -0.19436836 -0.33834219 -0.66637397 -1.1376023 -1.7104805 -2.3283527 -2.9284682 -3.460371 -3.8835697][-3.7837768 -3.3161697 -2.7774801 -2.265275 -1.8827066 -1.6891167 -1.6844995 -1.8262897 -2.0773251 -2.4109511 -2.795598 -3.193347 -3.5636017 -3.8795466 -4.1219869][-4.0149751 -3.7383916 -3.4314473 -3.151546 -2.9563844 -2.8745031 -2.899554 -3.0036511 -3.1639833 -3.3659692 -3.5867445 -3.8043647 -3.9979568 -4.1571059 -4.2749715][-4.2062168 -4.0715022 -3.9270048 -3.800859 -3.7185674 -3.6917541 -3.7151885 -3.7746716 -3.8589897 -3.9604633 -4.0645909 -4.1615109 -4.2441769 -4.3098211 -4.3566442]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2/model.ckpt-10000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2/model.ckpt-10000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-05 09:12:37.509387: step 10010, loss = 0.99, batch loss = 0.93 (37.5 examples/sec; 0.213 sec/batch; 19h:06m:25s remains)
INFO - root - 2017-12-05 09:12:39.576625: step 10020, loss = 1.47, batch loss = 1.41 (39.2 examples/sec; 0.204 sec/batch; 18h:17m:14s remains)
INFO - root - 2017-12-05 09:12:41.642397: step 10030, loss = 1.68, batch loss = 1.62 (38.2 examples/sec; 0.210 sec/batch; 18h:46m:19s remains)
INFO - root - 2017-12-05 09:12:43.728748: step 10040, loss = 1.49, batch loss = 1.43 (39.1 examples/sec; 0.205 sec/batch; 18h:19m:31s remains)
INFO - root - 2017-12-05 09:12:45.810123: step 10050, loss = 1.33, batch loss = 1.27 (38.3 examples/sec; 0.209 sec/batch; 18h:43m:25s remains)
INFO - root - 2017-12-05 09:12:47.869579: step 10060, loss = 1.43, batch loss = 1.37 (38.0 examples/sec; 0.211 sec/batch; 18h:51m:34s remains)
INFO - root - 2017-12-05 09:12:49.935658: step 10070, loss = 1.24, batch loss = 1.18 (39.4 examples/sec; 0.203 sec/batch; 18h:10m:52s remains)
INFO - root - 2017-12-05 09:12:52.012004: step 10080, loss = 1.26, batch loss = 1.21 (39.0 examples/sec; 0.205 sec/batch; 18h:21m:12s remains)
INFO - root - 2017-12-05 09:12:54.088104: step 10090, loss = 1.56, batch loss = 1.50 (39.9 examples/sec; 0.200 sec/batch; 17h:56m:53s remains)
INFO - root - 2017-12-05 09:12:56.148465: step 10100, loss = 1.21, batch loss = 1.15 (39.2 examples/sec; 0.204 sec/batch; 18h:16m:45s remains)
2017-12-05 09:12:56.421204: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.3618574 -2.0902832 -1.8716009 -1.7412837 -1.768327 -1.9949877 -2.4077728 -2.927557 -3.4502685 -3.8836727 -4.1646533 -4.3041978 -4.3592954 -4.3736343 -4.3693371][-2.2367311 -1.9505122 -1.7392704 -1.6416669 -1.7242723 -2.0096676 -2.4659405 -3.0017509 -3.5115685 -3.9193149 -4.1784587 -4.30853 -4.3588748 -4.3721657 -4.365993][-2.2488499 -1.9500971 -1.7385268 -1.656539 -1.7685726 -2.0835819 -2.5562115 -3.0905774 -3.5789249 -3.9596996 -4.197392 -4.3174 -4.3635969 -4.3752666 -4.3695679][-2.3260064 -2.0315182 -1.8259034 -1.7534115 -1.8762596 -2.194454 -2.6565843 -3.170625 -3.63184 -3.9838891 -4.1994805 -4.3093467 -4.3524861 -4.362802 -4.35834][-2.3374145 -2.0597103 -1.8766186 -1.8191543 -1.9459422 -2.2541156 -2.6929965 -3.1800072 -3.6171632 -3.9473665 -4.1461854 -4.2441344 -4.2804542 -4.285408 -4.2774634][-2.2072721 -1.9457626 -1.7953815 -1.7610717 -1.8915117 -2.1858704 -2.5993381 -3.0574062 -3.4723225 -3.7833843 -3.9618394 -4.0333233 -4.04154 -4.0199151 -3.9904704][-1.9423575 -1.6895316 -1.5703554 -1.557375 -1.68435 -1.9560447 -2.3367257 -2.7627149 -3.1537828 -3.4441123 -3.5944443 -3.6217422 -3.5726142 -3.4906149 -3.412261][-1.5851903 -1.3347943 -1.247051 -1.2444651 -1.355227 -1.5879791 -1.9212894 -2.304424 -2.6659908 -2.9351144 -3.0572014 -3.0325861 -2.9072952 -2.7385581 -2.5839043][-1.2224941 -0.96806669 -0.90370059 -0.90949678 -0.99614096 -1.1777313 -1.4466429 -1.7706058 -2.0892355 -2.332741 -2.435606 -2.3790629 -2.1967089 -1.9489543 -1.7133691][-0.95748377 -0.69837832 -0.65196562 -0.65806651 -0.71983886 -0.84858012 -1.0454271 -1.291894 -1.5460784 -1.7494624 -1.8343759 -1.7658029 -1.5630429 -1.2806294 -1.0000951][-0.94989109 -0.69592667 -0.65382552 -0.64720392 -0.67848706 -0.75052357 -0.86794806 -1.0233095 -1.1914268 -1.3327582 -1.3867426 -1.3144667 -1.122184 -0.85164 -0.57469034][-1.3120234 -1.0743127 -1.0300105 -1.0037117 -0.99968576 -1.014034 -1.0555744 -1.1198971 -1.1969514 -1.2625735 -1.2699006 -1.1882985 -1.0187976 -0.79127216 -0.55590606][-2.024821 -1.8195012 -1.7682369 -1.7308018 -1.7044764 -1.6832228 -1.6766415 -1.6804862 -1.6927366 -1.6976385 -1.6652374 -1.5762327 -1.4310887 -1.2509248 -1.0666351][-2.8436389 -2.6816654 -2.6332598 -2.5981078 -2.5669303 -2.5356717 -2.512156 -2.4939489 -2.4783351 -2.455822 -2.4096689 -2.3292167 -2.2163422 -2.0846179 -1.9504566][-3.5302789 -3.4238927 -3.3862896 -3.3613629 -3.339231 -3.3145487 -3.2934391 -3.2764814 -3.2599757 -3.238622 -3.2019598 -3.1447725 -3.068248 -2.9814391 -2.8928969]]...]
INFO - root - 2017-12-05 09:12:58.502800: step 10110, loss = 1.50, batch loss = 1.44 (39.1 examples/sec; 0.205 sec/batch; 18h:20m:13s remains)
INFO - root - 2017-12-05 09:13:00.571904: step 10120, loss = 1.31, batch loss = 1.26 (38.0 examples/sec; 0.211 sec/batch; 18h:51m:01s remains)
INFO - root - 2017-12-05 09:13:02.664055: step 10130, loss = 1.53, batch loss = 1.47 (37.2 examples/sec; 0.215 sec/batch; 19h:15m:30s remains)
INFO - root - 2017-12-05 09:13:04.733136: step 10140, loss = 1.75, batch loss = 1.70 (39.3 examples/sec; 0.203 sec/batch; 18h:12m:44s remains)
INFO - root - 2017-12-05 09:13:06.821214: step 10150, loss = 1.16, batch loss = 1.10 (39.5 examples/sec; 0.202 sec/batch; 18h:07m:04s remains)
INFO - root - 2017-12-05 09:13:08.902391: step 10160, loss = 1.89, batch loss = 1.83 (38.7 examples/sec; 0.207 sec/batch; 18h:31m:05s remains)
INFO - root - 2017-12-05 09:13:10.967791: step 10170, loss = 1.84, batch loss = 1.78 (37.6 examples/sec; 0.213 sec/batch; 19h:03m:02s remains)
INFO - root - 2017-12-05 09:13:13.039642: step 10180, loss = 1.68, batch loss = 1.63 (38.2 examples/sec; 0.209 sec/batch; 18h:44m:01s remains)
INFO - root - 2017-12-05 09:13:15.149637: step 10190, loss = 1.44, batch loss = 1.38 (37.1 examples/sec; 0.216 sec/batch; 19h:18m:48s remains)
INFO - root - 2017-12-05 09:13:17.245855: step 10200, loss = 1.28, batch loss = 1.22 (37.8 examples/sec; 0.212 sec/batch; 18h:57m:33s remains)
2017-12-05 09:13:17.538377: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.971909 -3.8694804 -3.7887487 -3.7372994 -3.7154317 -3.725121 -3.7696218 -3.8428094 -3.9230385 -3.9855468 -4.0167704 -4.0163627 -3.9954171 -3.96945 -3.9529595][-3.5648086 -3.3752565 -3.2324204 -3.1437497 -3.1066811 -3.1194634 -3.1860926 -3.2986715 -3.4241323 -3.5239782 -3.575212 -3.5771265 -3.547945 -3.5145404 -3.5016551][-3.1101639 -2.8339489 -2.6323307 -2.5049248 -2.4451256 -2.4487224 -2.5187368 -2.6463172 -2.7933998 -2.9148078 -2.9828095 -2.9978638 -2.9797492 -2.9615235 -2.9738607][-2.7547112 -2.4202623 -2.1773024 -2.0168765 -1.9295969 -1.9118018 -1.9619486 -2.0709374 -2.206759 -2.3276958 -2.4074965 -2.4475722 -2.4665284 -2.4929829 -2.5514028][-2.5428534 -2.1763003 -1.9082255 -1.723146 -1.6104801 -1.5690074 -1.59232 -1.6722639 -1.7875721 -1.905293 -2.0020547 -2.0789654 -2.1507215 -2.2331951 -2.3408308][-2.4181018 -2.0324812 -1.7497246 -1.5511215 -1.4261489 -1.3738267 -1.3862398 -1.4573178 -1.5724735 -1.7025757 -1.8257291 -1.9419961 -2.0606019 -2.1865122 -2.325628][-2.3262789 -1.9253759 -1.6383555 -1.4453955 -1.3312984 -1.2926371 -1.3212211 -1.4125376 -1.5554097 -1.7182519 -1.8744977 -2.022275 -2.1660445 -2.30378 -2.442544][-2.2616007 -1.8553841 -1.5804927 -1.4148352 -1.3361561 -1.3338401 -1.3961167 -1.5228086 -1.707083 -1.9105043 -2.0988045 -2.2649994 -2.4087417 -2.5285859 -2.6395118][-2.247499 -1.8597441 -1.6245518 -1.5129437 -1.4883461 -1.5306089 -1.6259701 -1.7797959 -1.9916983 -2.2192016 -2.4246945 -2.5928667 -2.7175584 -2.8046553 -2.8754411][-2.3409398 -2.0051186 -1.8362892 -1.7933528 -1.826524 -1.904067 -2.0151017 -2.1706412 -2.3741539 -2.5903258 -2.7818546 -2.9291074 -3.0235865 -3.0771942 -3.1122096][-2.5959673 -2.3357942 -2.2355375 -2.2454255 -2.3094528 -2.3897667 -2.4819024 -2.6043687 -2.7663631 -2.9425182 -3.100704 -3.2200966 -3.2928777 -3.329844 -3.3497193][-3.0234427 -2.8425677 -2.7834439 -2.8020158 -2.8502617 -2.8977625 -2.9471912 -3.0201235 -3.1309223 -3.2603922 -3.3825488 -3.4784164 -3.5411093 -3.5767448 -3.5966282][-3.5300343 -3.4156342 -3.370326 -3.3652878 -3.3737726 -3.3788338 -3.3878613 -3.420933 -3.4891946 -3.5777197 -3.6663043 -3.7392266 -3.7908266 -3.8232751 -3.8428221][-3.9624236 -3.8968506 -3.8603175 -3.8399508 -3.8259838 -3.8115265 -3.8023806 -3.8129375 -3.8490496 -3.9004884 -3.955404 -4.0015807 -4.0345025 -4.0542617 -4.0646439][-4.2288146 -4.196866 -4.1747389 -4.1583228 -4.1450477 -4.1324539 -4.1238394 -4.1266713 -4.14237 -4.1655974 -4.1910191 -4.2115688 -4.2250476 -4.23113 -4.2325063]]...]
INFO - root - 2017-12-05 09:13:19.614407: step 10210, loss = 2.06, batch loss = 2.00 (38.4 examples/sec; 0.208 sec/batch; 18h:38m:22s remains)
INFO - root - 2017-12-05 09:13:21.692385: step 10220, loss = 1.29, batch loss = 1.23 (38.3 examples/sec; 0.209 sec/batch; 18h:42m:20s remains)
INFO - root - 2017-12-05 09:13:23.807680: step 10230, loss = 1.51, batch loss = 1.45 (35.2 examples/sec; 0.227 sec/batch; 20h:20m:19s remains)
INFO - root - 2017-12-05 09:13:25.922665: step 10240, loss = 1.85, batch loss = 1.79 (38.2 examples/sec; 0.209 sec/batch; 18h:44m:23s remains)
INFO - root - 2017-12-05 09:13:27.990878: step 10250, loss = 1.05, batch loss = 0.99 (38.6 examples/sec; 0.207 sec/batch; 18h:32m:12s remains)
INFO - root - 2017-12-05 09:13:30.108353: step 10260, loss = 1.58, batch loss = 1.53 (36.9 examples/sec; 0.217 sec/batch; 19h:22m:59s remains)
INFO - root - 2017-12-05 09:13:32.163494: step 10270, loss = 1.31, batch loss = 1.25 (38.6 examples/sec; 0.207 sec/batch; 18h:32m:15s remains)
INFO - root - 2017-12-05 09:13:34.252405: step 10280, loss = 1.72, batch loss = 1.67 (37.5 examples/sec; 0.213 sec/batch; 19h:04m:19s remains)
INFO - root - 2017-12-05 09:13:36.308145: step 10290, loss = 1.24, batch loss = 1.19 (39.1 examples/sec; 0.205 sec/batch; 18h:19m:17s remains)
INFO - root - 2017-12-05 09:13:38.407819: step 10300, loss = 1.48, batch loss = 1.42 (39.4 examples/sec; 0.203 sec/batch; 18h:09m:59s remains)
2017-12-05 09:13:38.697043: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2043157 -4.2051854 -4.2086105 -4.2121897 -4.2155638 -4.2168865 -4.2129207 -4.1951637 -4.1558862 -4.0993485 -4.0438437 -4.0177555 -4.03495 -4.0847154 -4.1519122][-4.1682916 -4.1715593 -4.1776752 -4.1838279 -4.1889362 -4.1920161 -4.1897879 -4.1734757 -4.1348257 -4.0774384 -4.0183163 -3.9868665 -3.9992709 -4.0485339 -4.1187172][-4.100318 -4.1058016 -4.11729 -4.1305456 -4.1414995 -4.1492052 -4.150413 -4.1396976 -4.110517 -4.0660119 -4.0192208 -3.9953938 -4.009593 -4.0576873 -4.1239734][-3.9132028 -3.9170098 -3.9364622 -3.962539 -3.9861565 -4.0021448 -4.0091372 -4.0072932 -3.9934535 -3.9703777 -3.9482505 -3.9472694 -3.9784298 -4.0374589 -4.1105752][-3.547358 -3.5408902 -3.5710425 -3.6181645 -3.662077 -3.6900473 -3.7035379 -3.7108035 -3.7152777 -3.7212954 -3.737247 -3.7781496 -3.8490269 -3.9427443 -4.0468688][-3.0613008 -3.0413632 -3.0846167 -3.1568525 -3.2244039 -3.2658997 -3.2855175 -3.3008747 -3.3240378 -3.3639417 -3.4271352 -3.5225163 -3.6504254 -3.7988963 -3.9541731][-2.617403 -2.591814 -2.6544094 -2.7536266 -2.8464384 -2.9035342 -2.9293928 -2.9504046 -2.9898143 -3.0605655 -3.1665943 -3.3114216 -3.4923828 -3.6931312 -3.8959115][-2.3800797 -2.3682497 -2.4608381 -2.5907121 -2.7092543 -2.7837024 -2.817728 -2.8411696 -2.8866704 -2.9718652 -3.1004643 -3.271481 -3.4797373 -3.7048552 -3.9254289][-2.3922613 -2.4103954 -2.5393581 -2.701551 -2.8471744 -2.9442124 -2.9952469 -3.0223026 -3.0585752 -3.1325948 -3.2535954 -3.4193714 -3.6204894 -3.8342779 -4.0354872][-2.5910084 -2.6418476 -2.8008397 -2.9888802 -3.1582365 -3.2789688 -3.3513064 -3.3859229 -3.4100156 -3.4570098 -3.5445421 -3.6742346 -3.8348544 -4.0066586 -4.1625104][-2.8478422 -2.9317136 -3.1128893 -3.3190608 -3.5055013 -3.6443892 -3.7334287 -3.7740498 -3.787142 -3.8043742 -3.8491564 -3.9290214 -4.0352211 -4.1529984 -4.2578092][-3.0471525 -3.1487851 -3.3460214 -3.562701 -3.7602959 -3.9133394 -4.0154214 -4.06285 -4.0712447 -4.0680404 -4.0783935 -4.1148205 -4.1734815 -4.2437739 -4.3086848][-3.162838 -3.2654738 -3.4646783 -3.683382 -3.8846 -4.0466552 -4.1584978 -4.2137189 -4.2254019 -4.2161088 -4.2099524 -4.21925 -4.2466254 -4.2857642 -4.3256474][-3.2051678 -3.2973504 -3.4899132 -3.7033565 -3.9031227 -4.0687146 -4.1876459 -4.2512903 -4.2701612 -4.26376 -4.2538886 -4.2534561 -4.2676091 -4.292532 -4.3220854][-3.2144451 -3.2901142 -3.4665604 -3.6667309 -3.8580942 -4.0214534 -4.1423283 -4.2106142 -4.2357345 -4.2334661 -4.2246284 -4.22462 -4.2404938 -4.2673268 -4.2993979]]...]
INFO - root - 2017-12-05 09:13:40.761770: step 10310, loss = 1.40, batch loss = 1.35 (38.8 examples/sec; 0.206 sec/batch; 18h:26m:43s remains)
INFO - root - 2017-12-05 09:13:42.817919: step 10320, loss = 1.45, batch loss = 1.39 (39.3 examples/sec; 0.203 sec/batch; 18h:12m:30s remains)
INFO - root - 2017-12-05 09:13:44.896076: step 10330, loss = 1.35, batch loss = 1.29 (38.6 examples/sec; 0.207 sec/batch; 18h:32m:11s remains)
INFO - root - 2017-12-05 09:13:46.972853: step 10340, loss = 1.35, batch loss = 1.29 (38.9 examples/sec; 0.206 sec/batch; 18h:23m:51s remains)
