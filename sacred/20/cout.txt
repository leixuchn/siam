INFO - Obj-Siam-FC - Running command 'main'
INFO - Obj-Siam-FC - Started run with ID "20"
INFO - root - preproces -- siamese_fc_color
WARNING - root - root is not explicitly specified, using default value: None
INFO - root - For training, we use instance size 255 - 2 * 8 ...
WARNING - root - init_method is not explicitly specified, using default value: None
sdiufhasudf Tensor("siamese_fc/conv5/split:0", shape=(8, 8, 8, 192), dtype=float32)
sdfah Tensor("siamese_fc/conv5/offset2/BiasAdd:0", shape=(8, 8, 8, 72), dtype=float32)
Tensor("siamese_fc/conv5/transpose:0", shape=(8, 192, 8, 8), dtype=float32) <tf.Variable 'siamese_fc/conv5/b1/weights:0' shape=(128, 192, 3, 3) dtype=float32_ref> Tensor("siamese_fc/conv5/transpose_1:0", shape=(8, 72, 8, 8), dtype=float32)
ddd Tensor("siamese_fc/conv5/b1/transpose:0", shape=(8, 6, 6, 128), dtype=float32)
Tensor("siamese_fc/conv5/transpose_2:0", shape=(8, 192, 8, 8), dtype=float32) <tf.Variable 'siamese_fc/conv5/b2/weights:0' shape=(128, 192, 3, 3) dtype=float32_ref> Tensor("siamese_fc/conv5/transpose_3:0", shape=(8, 72, 8, 8), dtype=float32)
sdiufhasudf Tensor("siamese_fc_1/conv5/split:0", shape=(8, 22, 22, 192), dtype=float32)
sdfah Tensor("siamese_fc_1/conv5/offset2/BiasAdd:0", shape=(8, 22, 22, 72), dtype=float32)
Tensor("siamese_fc_1/conv5/transpose:0", shape=(8, 192, 22, 22), dtype=float32) <tf.Variable 'siamese_fc/conv5/b1/weights:0' shape=(128, 192, 3, 3) dtype=float32_ref> Tensor("siamese_fc_1/conv5/transpose_1:0", shape=(8, 72, 22, 22), dtype=float32)
ddd Tensor("siamese_fc_1/conv5/b1/transpose:0", shape=(8, 20, 20, 128), dtype=float32)
Tensor("siamese_fc_1/conv5/transpose_2:0", shape=(8, 192, 22, 22), dtype=float32) <tf.Variable 'siamese_fc/conv5/b2/weights:0' shape=(128, 192, 3, 3) dtype=float32_ref> Tensor("siamese_fc_1/conv5/transpose_3:0", shape=(8, 72, 22, 22), dtype=float32)
INFO - root - Model moving average is disabled since decay factor is 0
2017-12-02 11:47:09.536876: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-02 11:47:09.536921: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-02 11:47:09.536927: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2017-12-02 11:47:09.536931: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-02 11:47:09.536935: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
2017-12-02 11:47:10.124946: I tensorflow/core/common_runtime/gpu/gpu_device.cc:887] Found device 0 with properties: 
name: Tesla K80
major: 3 minor: 7 memoryClockRate (GHz) 0.8235
pciBusID 5791:00:00.0
Total memory: 11.17GiB
Free memory: 11.11GiB
2017-12-02 11:47:10.124983: I tensorflow/core/common_runtime/gpu/gpu_device.cc:908] DMA: 0 
2017-12-02 11:47:10.124990: I tensorflow/core/common_runtime/gpu/gpu_device.cc:918] 0:   Y 
2017-12-02 11:47:10.124997: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla K80, pci bus id: 5791:00:00.0)
INFO - root - Restore from last checkpoint: /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC/model.ckpt-0
INFO:tensorflow:Restoring parameters from /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC/model.ckpt-0
INFO - tensorflow - Restoring parameters from /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC/model.ckpt-0
INFO - root - Starting threads 0 ...

INFO - root - training for 332500 steps
INFO - root - 2017-12-02 11:47:14.463477: step 10, loss = 0.91, batch loss = 0.69 (38.6 examples/sec; 0.207 sec/batch; 19h:08m:54s remains)
INFO - root - 2017-12-02 11:47:16.475052: step 20, loss = nan, batch loss = 0.69 (39.6 examples/sec; 0.202 sec/batch; 18h:38m:08s remains)
INFO - root - 2017-12-02 11:47:18.480458: step 30, loss = nan, batch loss = 0.69 (40.0 examples/sec; 0.200 sec/batch; 18h:28m:42s remains)
INFO - root - 2017-12-02 11:47:20.480635: step 40, loss = nan, batch loss = 0.69 (40.3 examples/sec; 0.199 sec/batch; 18h:21m:15s remains)
INFO - root - 2017-12-02 11:47:22.480178: step 50, loss = nan, batch loss = 0.69 (40.2 examples/sec; 0.199 sec/batch; 18h:23m:53s remains)
INFO - root - 2017-12-02 11:47:24.466497: step 60, loss = nan, batch loss = 0.69 (40.0 examples/sec; 0.200 sec/batch; 18h:29m:30s remains)
INFO - root - 2017-12-02 11:47:26.455156: step 70, loss = nan, batch loss = 0.69 (40.2 examples/sec; 0.199 sec/batch; 18h:22m:52s remains)
INFO - root - 2017-12-02 11:47:28.431511: step 80, loss = nan, batch loss = 0.69 (40.6 examples/sec; 0.197 sec/batch; 18h:11m:56s remains)
INFO - root - 2017-12-02 11:47:30.391403: step 90, loss = nan, batch loss = 0.69 (40.7 examples/sec; 0.197 sec/batch; 18h:09m:43s remains)
INFO - root - 2017-12-02 11:47:32.378809: step 100, loss = nan, batch loss = 0.69 (40.3 examples/sec; 0.199 sec/batch; 18h:20m:27s remains)
INFO - root - 2017-12-02 11:47:34.501240: step 110, loss = nan, batch loss = 0.69 (40.3 examples/sec; 0.199 sec/batch; 18h:20m:27s remains)
INFO - root - 2017-12-02 11:47:36.484559: step 120, loss = nan, batch loss = 0.69 (40.3 examples/sec; 0.199 sec/batch; 18h:20m:23s remains)
INFO - root - 2017-12-02 11:47:38.461894: step 130, loss = nan, batch loss = 0.69 (40.6 examples/sec; 0.197 sec/batch; 18h:10m:47s remains)
INFO - root - 2017-12-02 11:47:40.448521: step 140, loss = nan, batch loss = 0.69 (40.3 examples/sec; 0.199 sec/batch; 18h:19m:48s remains)
INFO - root - 2017-12-02 11:47:42.431398: step 150, loss = nan, batch loss = 0.69 (40.2 examples/sec; 0.199 sec/batch; 18h:23m:11s remains)
INFO - root - 2017-12-02 11:47:44.406909: step 160, loss = nan, batch loss = 0.69 (40.5 examples/sec; 0.197 sec/batch; 18h:13m:12s remains)
INFO - root - 2017-12-02 11:47:46.397604: step 170, loss = nan, batch loss = 0.69 (39.9 examples/sec; 0.200 sec/batch; 18h:29m:41s remains)
INFO - root - 2017-12-02 11:47:48.385060: step 180, loss = nan, batch loss = 0.69 (40.2 examples/sec; 0.199 sec/batch; 18h:20m:56s remains)
