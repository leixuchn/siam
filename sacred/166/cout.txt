INFO - Siam-FC - Running command 'main'
INFO - Siam-FC - Started run with ID "166"
INFO - root - preproces -- siamese_fc_color
WARNING - root - root is not explicitly specified, using default value: None
INFO - root - For training, we use instance size 255 - 2 * 8 ...
WARNING - root - init_method is not explicitly specified, using default value: None
sdiufhasudf Tensor("siamese_fc/conv5/split:0", shape=(8, 8, 8, 192), dtype=float32)
Tensor("siamese_fc/conv5/def/transpose:0", shape=(8, 192, 8, 8), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/b1/weights:0' shape=(128, 192, 3, 3) dtype=float32_ref> Tensor("siamese_fc/conv5/def/transpose_1:0", shape=(8, 72, 8, 8), dtype=float32)
ddd Tensor("siamese_fc/conv5/def/b1/Relu:0", shape=(8, 6, 6, 128), dtype=float32)
Tensor("siamese_fc/conv5/def/transpose_2:0", shape=(8, 192, 8, 8), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/b2/weights:0' shape=(128, 192, 3, 3) dtype=float32_ref> Tensor("siamese_fc/conv5/def/transpose_3:0", shape=(8, 72, 8, 8), dtype=float32)
sdiufhasudf Tensor("siamese_fc_1/conv5/split:0", shape=(8, 22, 22, 192), dtype=float32)
Tensor("siamese_fc_1/conv5/def/transpose:0", shape=(8, 192, 22, 22), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/b1/weights:0' shape=(128, 192, 3, 3) dtype=float32_ref> Tensor("siamese_fc_1/conv5/def/transpose_1:0", shape=(8, 72, 22, 22), dtype=float32)
ddd Tensor("siamese_fc_1/conv5/def/b1/Relu:0", shape=(8, 20, 20, 128), dtype=float32)
Tensor("siamese_fc_1/conv5/def/transpose_2:0", shape=(8, 192, 22, 22), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/b2/weights:0' shape=(128, 192, 3, 3) dtype=float32_ref> Tensor("siamese_fc_1/conv5/def/transpose_3:0", shape=(8, 72, 22, 22), dtype=float32)
Tensor("detection/add:0", shape=(8, 15, 15), dtype=float32)
INFO - root - Model moving average is disabled since decay factor is 0
2017-12-09 05:37:51.165227: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-09 05:37:51.165265: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-09 05:37:51.165271: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2017-12-09 05:37:51.165276: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-09 05:37:51.165280: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
2017-12-09 05:37:52.028351: I tensorflow/core/common_runtime/gpu/gpu_device.cc:887] Found device 0 with properties: 
name: Tesla K80
major: 3 minor: 7 memoryClockRate (GHz) 0.8235
pciBusID 6bee:00:00.0
Total memory: 11.17GiB
Free memory: 11.10GiB
2017-12-09 05:37:52.028389: I tensorflow/core/common_runtime/gpu/gpu_device.cc:908] DMA: 0 
2017-12-09 05:37:52.028396: I tensorflow/core/common_runtime/gpu/gpu_device.cc:918] 0:   Y 
2017-12-09 05:37:52.028407: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla K80, pci bus id: 6bee:00:00.0)
INFO - root - Starting threads 0 ...

INFO - root - training for 332500 steps
INFO - root - 2017-12-09 05:37:55.681352: step 0, loss = 0.90, batch loss = 0.69 (3.0 examples/sec; 2.696 sec/batch; 248h:57m:35s remains)
2017-12-09 05:37:56.181381: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.00012014301 0.00012633594 0.00013108259 0.0001368004 0.00014003713 0.00014422947 0.00014767886 0.00014750785 0.00014659643 0.00014218086 0.00013907523 0.00013783314 0.00013494531 0.00013202569 0.00012711805][0.00012285472 0.00013266692 0.00014355591 0.00015586009 0.00016287011 0.00016856272 0.00017268134 0.00017094545 0.00016890797 0.00016504381 0.00015893808 0.00015305844 0.00014711905 0.00014137727 0.00013288758][0.00012612304 0.00013894867 0.00015469706 0.00017166337 0.00018265225 0.00019025775 0.00019635177 0.00019900179 0.00019538192 0.00019263777 0.00018493157 0.00017558846 0.00016200956 0.00015248705 0.00013976678][0.00013052115 0.00014471142 0.00016474746 0.00018466005 0.00019864399 0.00021124202 0.00022303096 0.00023450606 0.00023160188 0.00022817579 0.00022001311 0.0002009172 0.00017994862 0.00016380945 0.00014641738][0.00013554891 0.00015104121 0.00017282424 0.00019346295 0.0002120032 0.00023230349 0.00025654552 0.00028145316 0.000283912 0.00028134725 0.00026548636 0.00023650656 0.00020385184 0.00017794935 0.00015465044][0.00014095081 0.00015600756 0.00017756889 0.00020146478 0.00022927318 0.0002642927 0.00030195166 0.00033137863 0.00033645658 0.00032565271 0.00030003206 0.0002651299 0.00022518286 0.0001917584 0.00016125654][0.0001453872 0.00015864271 0.00018097615 0.00020746548 0.00024408782 0.00029052692 0.00034080917 0.00038036273 0.00038553373 0.00036164737 0.00031995188 0.00027209221 0.00022994832 0.00019358909 0.00016092214][0.00014405017 0.00015727468 0.00017633564 0.00020467183 0.00024742328 0.00030137776 0.00036880892 0.00042931372 0.00041700382 0.00036175436 0.00030944834 0.00025782749 0.00021469298 0.00018432543 0.00015756467][0.00014110759 0.00015360571 0.00017276606 0.00020107691 0.00024188671 0.00029024441 0.00035221531 0.00040885125 0.00037978526 0.00032020727 0.00027550821 0.00023192204 0.00019704789 0.00017197288 0.000152444][0.00013819647 0.00014705851 0.00016754134 0.0001926166 0.00022364677 0.00025870765 0.00029483167 0.00031194548 0.0002893995 0.00025406349 0.0002256478 0.00020144138 0.00017736176 0.00015998111 0.00014674939][0.00013360345 0.00013947226 0.00015731787 0.00017757114 0.0002003257 0.00022334722 0.00023692708 0.00023714155 0.00022166318 0.00020372412 0.00018843212 0.00017652125 0.00016126859 0.00015286737 0.00014385245][0.00012992653 0.00013478308 0.00014577431 0.0001604492 0.00017492678 0.00018832018 0.00019356859 0.00019197361 0.00018208867 0.00017045841 0.00016274303 0.00015734097 0.00014871101 0.0001445849 0.00013943792][0.00012958434 0.00013135445 0.00013886654 0.00014845957 0.00015523976 0.00016347445 0.00016829914 0.00017089216 0.00016495335 0.00015286505 0.00014786735 0.00014599148 0.00014178298 0.00013947269 0.00013620497][0.00013142139 0.00013147364 0.00013581668 0.00014177483 0.00014654317 0.00015281794 0.00015606391 0.00016031141 0.00015636739 0.0001453071 0.00014270004 0.00014243189 0.00013944774 0.00013653397 0.00013396829][0.00013485091 0.00013477715 0.00013701936 0.00014071516 0.00014547864 0.00015026871 0.00015198531 0.00015625761 0.00015274058 0.00014451031 0.00014238473 0.00014258808 0.00013847362 0.00013542485 0.00013258278]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-momentum-lr0.01-clip-zeroinit-relu-bias-from-scratch/model.ckpt-0 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-momentum-lr0.01-clip-zeroinit-relu-bias-from-scratch/model.ckpt-0 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-09 05:37:59.005362: step 10, loss = 0.90, batch loss = 0.69 (35.5 examples/sec; 0.226 sec/batch; 20h:50m:26s remains)
INFO - root - 2017-12-09 05:38:01.220299: step 20, loss = 0.90, batch loss = 0.69 (35.7 examples/sec; 0.224 sec/batch; 20h:43m:01s remains)
INFO - root - 2017-12-09 05:38:03.348644: step 30, loss = 0.90, batch loss = 0.69 (37.3 examples/sec; 0.214 sec/batch; 19h:47m:56s remains)
INFO - root - 2017-12-09 05:38:05.507844: step 40, loss = 0.91, batch loss = 0.69 (38.2 examples/sec; 0.209 sec/batch; 19h:19m:35s remains)
INFO - root - 2017-12-09 05:38:07.701805: step 50, loss = 0.91, batch loss = 0.69 (36.2 examples/sec; 0.221 sec/batch; 20h:25m:02s remains)
INFO - root - 2017-12-09 05:38:09.861614: step 60, loss = 0.91, batch loss = 0.69 (38.1 examples/sec; 0.210 sec/batch; 19h:22m:02s remains)
INFO - root - 2017-12-09 05:38:12.064923: step 70, loss = 0.91, batch loss = 0.69 (36.6 examples/sec; 0.218 sec/batch; 20h:10m:21s remains)
INFO - root - 2017-12-09 05:38:14.230173: step 80, loss = 0.92, batch loss = 0.69 (37.4 examples/sec; 0.214 sec/batch; 19h:44m:48s remains)
INFO - root - 2017-12-09 05:38:16.387970: step 90, loss = 0.92, batch loss = 0.69 (37.7 examples/sec; 0.212 sec/batch; 19h:36m:10s remains)
INFO - root - 2017-12-09 05:38:18.529086: step 100, loss = 0.92, batch loss = 0.69 (37.8 examples/sec; 0.212 sec/batch; 19h:33m:12s remains)
2017-12-09 05:38:18.909334: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.0005067889 0.00048383567 0.00049895421 0.00063502073 0.00099537836 0.0016551423 0.0025601266 0.0035137774 0.0042770649 0.0046301857 0.0044641308 0.0038636187 0.0030361186 0.0022033439 0.0015062181][0.00021211508 0.00020643111 0.00022135457 0.00030430118 0.00051746081 0.00091251149 0.0014660909 0.0020519842 0.0025174215 0.0027291949 0.0026167487 0.0022370687 0.001715692 0.0012000422 0.00077792478][9.5894437e-05 9.9700519e-05 0.00011946253 0.00017760049 0.00029650665 0.00049516093 0.00076742005 0.0010520873 0.0012730185 0.0013681838 0.0013026585 0.001105428 0.0008367523 0.00057646871 0.00036674648][7.2583309e-05 8.8671768e-05 0.000135028 0.00021786164 0.00031581137 0.00040593679 0.00048592055 0.00054816343 0.00058474339 0.0005872972 0.0005444673 0.00046506696 0.00035811384 0.00025576013 0.00017198615][8.920308e-05 0.00012782661 0.00023068096 0.00038559141 0.00052352168 0.00057800993 0.00053894636 0.00044204839 0.00033876224 0.00025848884 0.00020543423 0.00017304701 0.00014190718 0.0001136701 8.6293585e-05][0.00010860596 0.00017031195 0.00033402306 0.00057457696 0.00078026688 0.00084669341 0.00075904041 0.00055961666 0.00034894413 0.00018984328 0.00010242662 7.0082388e-05 6.4894892e-05 6.2325933e-05 5.4108044e-05][0.00011747974 0.00019030763 0.0003814926 0.0006619991 0.00090568577 0.00099546823 0.00091031351 0.0006755669 0.00042148377 0.00023229445 0.00011675437 6.6988534e-05 6.8671921e-05 6.7182147e-05 5.9242258e-05][0.00011047182 0.00017253032 0.00033754882 0.0005835651 0.00080561242 0.00090181438 0.00084884395 0.00065008883 0.0004272901 0.00026991224 0.00016988516 0.00011368187 0.00011386115 0.00010175526 8.3065519e-05][9.2045077e-05 0.00013291463 0.00023929217 0.000402594 0.00055789232 0.00063454459 0.00061452959 0.00048815334 0.00034497035 0.00025852217 0.00020230921 0.00015540405 0.00015489767 0.00013911206 0.00011596767][6.6607674e-05 8.7784967e-05 0.00014018269 0.00022424747 0.00031021691 0.00035879167 0.00035846652 0.00029889197 0.00023192151 0.000207895 0.0001962301 0.00016887965 0.00016854877 0.00015271807 0.00013088508][4.9535389e-05 5.6992641e-05 7.47011e-05 0.00010675846 0.00014426814 0.00016948539 0.00017450281 0.0001552817 0.00013368791 0.00013997202 0.00015484625 0.00014408358 0.00014291798 0.00013411774 0.00012211193][4.1974359e-05 4.396135e-05 4.7715963e-05 5.7457524e-05 7.1183436e-05 8.2015642e-05 8.3859661e-05 7.84706e-05 7.1899791e-05 8.0201091e-05 9.913119e-05 9.8796409e-05 9.7494027e-05 9.5182309e-05 9.42112e-05][3.974641e-05 4.02086e-05 3.9278595e-05 4.0548111e-05 4.3906635e-05 4.7184636e-05 4.8311049e-05 4.7556263e-05 4.5135581e-05 4.598924e-05 5.5500561e-05 5.8265075e-05 5.788503e-05 5.8724661e-05 6.5964465e-05][3.8989252e-05 3.9941362e-05 3.8973827e-05 3.8398648e-05 3.8595623e-05 3.9066814e-05 3.9344086e-05 3.9279279e-05 3.8932325e-05 3.8106402e-05 3.898389e-05 4.028398e-05 4.055148e-05 4.1709056e-05 5.1747375e-05][4.5851964e-05 4.46704e-05 4.1844854e-05 3.9935272e-05 3.9548533e-05 3.9216226e-05 3.9076309e-05 3.9001039e-05 3.8933475e-05 3.8528655e-05 3.8945072e-05 3.9774422e-05 4.0523468e-05 3.9601451e-05 4.7338108e-05]]...]
INFO - root - 2017-12-09 05:38:21.067331: step 110, loss = 0.93, batch loss = 0.69 (35.3 examples/sec; 0.226 sec/batch; 20h:53m:55s remains)
INFO - root - 2017-12-09 05:38:23.274834: step 120, loss = 0.93, batch loss = 0.69 (36.4 examples/sec; 0.220 sec/batch; 20h:17m:34s remains)
INFO - root - 2017-12-09 05:38:25.407593: step 130, loss = 0.93, batch loss = 0.69 (38.5 examples/sec; 0.208 sec/batch; 19h:11m:28s remains)
INFO - root - 2017-12-09 05:38:27.601177: step 140, loss = 0.93, batch loss = 0.69 (36.9 examples/sec; 0.217 sec/batch; 20h:00m:59s remains)
INFO - root - 2017-12-09 05:38:29.771848: step 150, loss = 0.93, batch loss = 0.69 (36.3 examples/sec; 0.220 sec/batch; 20h:20m:22s remains)
INFO - root - 2017-12-09 05:38:31.936743: step 160, loss = 0.93, batch loss = 0.69 (36.5 examples/sec; 0.219 sec/batch; 20h:14m:46s remains)
INFO - root - 2017-12-09 05:38:34.080667: step 170, loss = 0.94, batch loss = 0.69 (36.0 examples/sec; 0.222 sec/batch; 20h:30m:08s remains)
INFO - root - 2017-12-09 05:38:36.270156: step 180, loss = 0.94, batch loss = 0.69 (36.7 examples/sec; 0.218 sec/batch; 20h:06m:47s remains)
INFO - root - 2017-12-09 05:38:38.449031: step 190, loss = 0.94, batch loss = 0.69 (36.9 examples/sec; 0.217 sec/batch; 20h:00m:28s remains)
INFO - root - 2017-12-09 05:38:40.603254: step 200, loss = 0.94, batch loss = 0.70 (37.2 examples/sec; 0.215 sec/batch; 19h:52m:35s remains)
2017-12-09 05:38:40.918948: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.0032946607 0.0039415713 0.0046178293 0.0049646515 0.0050920509 0.004872656 0.0044372 0.0038288883 0.0031444666 0.0024788051 0.001868629 0.0013490061 0.00096159289 0.000663981 0.00040849828][0.0051556206 0.0063734306 0.007475425 0.0080882115 0.0083 0.0079733673 0.0072858841 0.006318809 0.0052195028 0.0041346545 0.0031362423 0.002299021 0.001646965 0.0011458753 0.00072988367][0.0070826421 0.0089228544 0.010526199 0.011503418 0.011851061 0.011435215 0.010474499 0.0091246022 0.0076040262 0.006110149 0.0047481549 0.0035935892 0.0026559348 0.0019037902 0.0012590348][0.0088680228 0.011297018 0.013378689 0.014729341 0.01528328 0.014880741 0.013741103 0.012077886 0.010200031 0.0083488114 0.0066582826 0.0052022813 0.0039715837 0.0029346654 0.0020095932][0.010248455 0.013200695 0.015723677 0.01743976 0.018192938 0.017858969 0.016645761 0.014803196 0.012715138 0.010640524 0.0087239491 0.0070312317 0.0055314372 0.0041856444 0.0029266088][0.010977102 0.014304891 0.017173136 0.019225035 0.020214217 0.019998759 0.01878871 0.016909773 0.014768627 0.012631594 0.010628446 0.0087957876 0.0070878491 0.00547426 0.0038888806][0.01085799 0.014375323 0.017481696 0.019779738 0.02099058 0.020957628 0.019865977 0.018096553 0.016055958 0.013994016 0.01202908 0.010172981 0.0083460547 0.0065263682 0.0046692495][0.0099443542 0.013415495 0.016595641 0.019065818 0.020500315 0.020671064 0.01978419 0.018243963 0.016416507 0.01452776 0.012664089 0.010859555 0.0090050278 0.0070857271 0.0050698821][0.0084830169 0.011674022 0.014728899 0.0172296 0.018820379 0.019219706 0.018601853 0.017350525 0.015777521 0.014100169 0.012393454 0.010710451 0.008924026 0.0070234826 0.0049941749][0.006722582 0.0094383778 0.012169094 0.014522518 0.01613901 0.01670674 0.016363313 0.015425374 0.014148344 0.012724699 0.011215652 0.00971615 0.008091215 0.006336912 0.0044505638][0.0048918165 0.0070074168 0.0092440564 0.011267828 0.012753727 0.013389167 0.013262825 0.012612156 0.011636603 0.010511348 0.0092824195 0.0080422936 0.0066623762 0.0051663248 0.0035630357][0.0032265345 0.0047269054 0.0063890023 0.0079523176 0.0091601256 0.0097562177 0.0097757075 0.0093624 0.0086689834 0.0078372937 0.0069162077 0.0059840232 0.0049228058 0.0037588931 0.0025295594][0.0019169229 0.0028645829 0.0039675427 0.0050494513 0.005919775 0.0063921218 0.0064716516 0.0062301457 0.0057767695 0.0052236035 0.0045992984 0.0039602215 0.0032278069 0.0024198859 0.0015805908][0.0010069053 0.0015353069 0.0021780324 0.0028308039 0.0033781612 0.0036968144 0.0037729971 0.0036398543 0.0033750662 0.0030524323 0.0026893504 0.0023091363 0.0018555754 0.0013577089 0.00085660105][0.00043811084 0.000700212 0.0010230701 0.0013559324 0.0016414507 0.0018132017 0.0018583026 0.0017927809 0.0016635893 0.0015122747 0.0013420836 0.0011514928 0.00091003987 0.00064427149 0.00038737233]]...]
INFO - root - 2017-12-09 05:38:43.087097: step 210, loss = 0.94, batch loss = 0.69 (37.4 examples/sec; 0.214 sec/batch; 19h:46m:01s remains)
INFO - root - 2017-12-09 05:38:45.245676: step 220, loss = 0.95, batch loss = 0.70 (37.6 examples/sec; 0.213 sec/batch; 19h:37m:23s remains)
INFO - root - 2017-12-09 05:38:47.420123: step 230, loss = 0.95, batch loss = 0.69 (36.6 examples/sec; 0.219 sec/batch; 20h:10m:44s remains)
INFO - root - 2017-12-09 05:38:49.611099: step 240, loss = 0.96, batch loss = 0.70 (35.2 examples/sec; 0.227 sec/batch; 20h:57m:09s remains)
INFO - root - 2017-12-09 05:38:51.773040: step 250, loss = 0.95, batch loss = 0.69 (37.8 examples/sec; 0.212 sec/batch; 19h:31m:51s remains)
INFO - root - 2017-12-09 05:38:53.930447: step 260, loss = 1.15, batch loss = 0.90 (36.4 examples/sec; 0.220 sec/batch; 20h:17m:15s remains)
INFO - root - 2017-12-09 05:38:56.083782: step 270, loss = 17.75, batch loss = 17.49 (36.3 examples/sec; 0.221 sec/batch; 20h:21m:52s remains)
INFO - root - 2017-12-09 05:38:58.257886: step 280, loss = 22.64, batch loss = 22.38 (35.3 examples/sec; 0.227 sec/batch; 20h:54m:12s remains)
INFO - root - 2017-12-09 05:39:00.408347: step 290, loss = 0.95, batch loss = 0.69 (36.6 examples/sec; 0.219 sec/batch; 20h:11m:09s remains)
INFO - root - 2017-12-09 05:39:02.619867: step 300, loss = 0.96, batch loss = 0.69 (36.2 examples/sec; 0.221 sec/batch; 20h:23m:02s remains)
2017-12-09 05:39:02.976303: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0014701053 -0.0014535397 -0.0014486588 -0.0014508867 -0.0014621526 -0.0014684548 -0.0014792073 -0.0014719128 -0.0014565861 -0.0014529489 -0.0014437408 -0.0014369304 -0.001432701 -0.0014278248 -0.0014361439][-0.0014701432 -0.0014640865 -0.0014579397 -0.0014530122 -0.0014577517 -0.0014658602 -0.0014647237 -0.0014651123 -0.0014599706 -0.001448743 -0.0014409722 -0.0014364766 -0.0014309412 -0.0014365641 -0.0014327102][-0.0014735869 -0.0014685036 -0.0014650689 -0.0014648511 -0.0014646654 -0.0014560409 -0.0014605063 -0.001455723 -0.0014546411 -0.0014523617 -0.001447667 -0.0014415985 -0.0014442274 -0.0014373822 -0.0014377752][-0.0014835144 -0.0014673822 -0.0014568062 -0.0014603123 -0.0014575388 -0.0014559916 -0.0014525723 -0.001446788 -0.0014478827 -0.0014538107 -0.001453235 -0.0014511171 -0.0014420492 -0.0014362315 -0.0014372166][-0.0014807582 -0.0014689039 -0.0014560737 -0.0014302139 -0.0014181191 -0.0014094338 -0.0014187299 -0.001433864 -0.0014421711 -0.0014535204 -0.0014648238 -0.0014653454 -0.0014570834 -0.0014479893 -0.0014354575][-0.0014837201 -0.0014699117 -0.0014431418 -0.001411963 -0.0013768424 -0.0013639543 -0.0013844534 -0.0014064983 -0.0014261071 -0.0014460019 -0.0014592732 -0.0014661729 -0.0014644809 -0.0014524761 -0.0014480266][-0.0014978787 -0.0014723421 -0.0014330994 -0.0013893722 -0.0013688582 -0.0013572215 -0.0013558518 -0.0013833866 -0.0014105262 -0.0014446554 -0.0014714845 -0.0014715375 -0.0014642217 -0.0014615591 -0.0014527376][-0.0015125487 -0.0014949705 -0.0014521143 -0.0014122399 -0.0013758269 -0.0013631204 -0.0013671315 -0.0013792366 -0.0014066005 -0.0014434739 -0.001470463 -0.0014720692 -0.0014638985 -0.0014555533 -0.0014511247][-0.0015134198 -0.0015002545 -0.0014791979 -0.0014445337 -0.0014113591 -0.0013920226 -0.0013887996 -0.00140621 -0.001419958 -0.0014452615 -0.0014648191 -0.001471567 -0.0014616543 -0.0014546479 -0.0014479482][-0.0015111883 -0.00149691 -0.001480757 -0.0014638876 -0.001450591 -0.0014388271 -0.0014358782 -0.0014389661 -0.0014489052 -0.0014595238 -0.0014667574 -0.0014605636 -0.0014622252 -0.0014593909 -0.0014517474][-0.0014931764 -0.0014864702 -0.0014890132 -0.0014824045 -0.0014745392 -0.0014623515 -0.0014552231 -0.0014568781 -0.0014572321 -0.0014621242 -0.0014561694 -0.0014473428 -0.0014417198 -0.0014365352 -0.0014338815][-0.0014834037 -0.001474465 -0.0014680047 -0.0014680619 -0.0014742773 -0.0014778044 -0.0014678432 -0.0014615715 -0.001451292 -0.0014344073 -0.0014281381 -0.0014378425 -0.0014352858 -0.0014295032 -0.0014282183][-0.0014733329 -0.0014634135 -0.0014661041 -0.0014643749 -0.001456658 -0.0014505587 -0.001446394 -0.0014437551 -0.0014312416 -0.0014291955 -0.0014251304 -0.0014252621 -0.0014216484 -0.0014246192 -0.0014239375][-0.0014572219 -0.0014513341 -0.0014515886 -0.0014592122 -0.0014577152 -0.0014493222 -0.0014409041 -0.0014322057 -0.0014181122 -0.0014155065 -0.0014266901 -0.0014261081 -0.001415516 -0.0014170188 -0.0014173924][-0.0014639583 -0.0014517147 -0.0014413478 -0.0014313539 -0.001426655 -0.0014225718 -0.0014177898 -0.0014099166 -0.0014003199 -0.0014019578 -0.0014054768 -0.0014126855 -0.0014228397 -0.0014218049 -0.0014183508]]...]
INFO - root - 2017-12-09 05:39:05.144613: step 310, loss = 0.96, batch loss = 0.69 (36.4 examples/sec; 0.219 sec/batch; 20h:15m:09s remains)
INFO - root - 2017-12-09 05:39:07.290266: step 320, loss = 0.96, batch loss = 0.69 (37.9 examples/sec; 0.211 sec/batch; 19h:28m:04s remains)
INFO - root - 2017-12-09 05:39:09.472520: step 330, loss = 0.97, batch loss = 0.69 (37.4 examples/sec; 0.214 sec/batch; 19h:44m:30s remains)
INFO - root - 2017-12-09 05:39:11.640619: step 340, loss = 0.97, batch loss = 0.69 (34.4 examples/sec; 0.233 sec/batch; 21h:27m:39s remains)
INFO - root - 2017-12-09 05:39:13.832961: step 350, loss = 0.97, batch loss = 0.69 (38.3 examples/sec; 0.209 sec/batch; 19h:17m:27s remains)
INFO - root - 2017-12-09 05:39:16.003297: step 360, loss = 0.97, batch loss = 0.69 (36.8 examples/sec; 0.217 sec/batch; 20h:02m:09s remains)
INFO - root - 2017-12-09 05:39:18.153975: step 370, loss = 0.98, batch loss = 0.69 (36.8 examples/sec; 0.218 sec/batch; 20h:04m:46s remains)
INFO - root - 2017-12-09 05:39:20.357696: step 380, loss = 0.98, batch loss = 0.69 (37.3 examples/sec; 0.215 sec/batch; 19h:48m:22s remains)
INFO - root - 2017-12-09 05:39:22.564546: step 390, loss = 0.98, batch loss = 0.69 (37.3 examples/sec; 0.215 sec/batch; 19h:48m:40s remains)
INFO - root - 2017-12-09 05:39:24.695162: step 400, loss = 0.98, batch loss = 0.69 (37.4 examples/sec; 0.214 sec/batch; 19h:44m:14s remains)
2017-12-09 05:39:25.030153: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.0076394863 0.00801793 0.0081230961 0.0082108648 0.0082630534 0.00829686 0.0083266972 0.0083295535 0.0083168307 0.0082783261 0.0082551111 0.0082702292 0.0083018681 0.0083554732 0.00839092][0.0073742429 0.0077586481 0.0078185732 0.0078692622 0.0079266643 0.007983556 0.0080160638 0.0080618467 0.0081181359 0.0081646722 0.0082073836 0.0082326047 0.0082593877 0.0082892682 0.0082921358][0.0070018228 0.0073229438 0.0072922716 0.0072543006 0.007220488 0.0072246064 0.0072940327 0.0074274493 0.0075944997 0.00776155 0.00790902 0.0080246078 0.0081091477 0.0081582768 0.0081619443][0.0066761496 0.0068534147 0.0066460418 0.0064041214 0.0062213624 0.0061460733 0.0062167142 0.0064341882 0.006756735 0.0071181105 0.0074376827 0.0076903468 0.0078688385 0.0079724081 0.0080049476][0.0063923113 0.0063649565 0.0059093954 0.0054187435 0.005004066 0.004779716 0.0048167869 0.0051175952 0.0056261276 0.0062308377 0.0068029296 0.0072622169 0.0075736959 0.0077578165 0.0078332927][0.0061553679 0.0059470972 0.0052609509 0.0045067836 0.003839206 0.0034262263 0.0033777948 0.0037658261 0.004481277 0.0053129848 0.006124083 0.0068073468 0.0072843311 0.00755821 0.0076809432][0.0059904503 0.00565168 0.0048108008 0.0038834726 0.0030386224 0.0024651233 0.0023528761 0.0027692844 0.0035847253 0.0046009147 0.0056256591 0.0064692097 0.0070594912 0.0074074157 0.0075755036][0.0059180073 0.005470281 0.0045638829 0.0035540927 0.0026186365 0.0020029396 0.001880979 0.0023254235 0.0032003415 0.0042898967 0.0053686444 0.0062673744 0.0069193337 0.0073071467 0.0075130174][0.0057815053 0.0053101066 0.0043831179 0.0033832463 0.0024774144 0.0018936325 0.0018154742 0.0023032343 0.0032033571 0.0042724921 0.0053346148 0.0062424727 0.0068660416 0.0072408346 0.0074686445][0.0055418811 0.0050090412 0.0040754024 0.0030995957 0.0022455743 0.0017532931 0.0017846551 0.0023425869 0.0032618763 0.0043280204 0.0053549567 0.0062039 0.0068060867 0.007183345 0.0074197883][0.0050039617 0.0044724466 0.0035447283 0.002598953 0.0018150379 0.0013970915 0.0015092603 0.0021594311 0.0031610616 0.0042630285 0.00529737 0.0061484007 0.0067382809 0.0071076695 0.0073527675][0.00396208 0.0035047717 0.0026749414 0.0018090428 0.0011085804 0.00079988141 0.0010078301 0.0017160863 0.0027692141 0.0039496804 0.0050584916 0.0059365584 0.0065691294 0.0069922935 0.0072605293][0.0024349848 0.0021178862 0.0014833444 0.0008213151 0.00028575596 7.6101162e-05 0.00034583535 0.0011069385 0.0022028163 0.0034283134 0.0046107457 0.0056133331 0.0063481247 0.0068361918 0.0071479203][0.00078876934 0.0006193592 0.0002347728 -0.00019175163 -0.00050887966 -0.00055308407 -0.00021093618 0.00053087284 0.0015916039 0.0028323317 0.0040828506 0.0051891892 0.006057363 0.0066600405 0.0070309127][-0.00046943594 -0.0005429372 -0.00073120091 -0.00094333844 -0.0010682116 -0.0009831693 -0.00059635483 0.0001245212 0.0011477588 0.0023776521 0.0036567384 0.0048309937 0.0057926695 0.0064872624 0.0069202138]]...]
INFO - root - 2017-12-09 05:39:27.222208: step 410, loss = 0.98, batch loss = 0.69 (35.1 examples/sec; 0.228 sec/batch; 20h:59m:50s remains)
INFO - root - 2017-12-09 05:39:29.379269: step 420, loss = 0.99, batch loss = 0.69 (38.3 examples/sec; 0.209 sec/batch; 19h:14m:36s remains)
INFO - root - 2017-12-09 05:39:31.513486: step 430, loss = 0.99, batch loss = 0.69 (36.8 examples/sec; 0.217 sec/batch; 20h:02m:14s remains)
INFO - root - 2017-12-09 05:39:33.715779: step 440, loss = 0.99, batch loss = 0.69 (36.7 examples/sec; 0.218 sec/batch; 20h:06m:08s remains)
INFO - root - 2017-12-09 05:39:35.911517: step 450, loss = 0.99, batch loss = 0.69 (35.9 examples/sec; 0.223 sec/batch; 20h:33m:26s remains)
INFO - root - 2017-12-09 05:39:38.083301: step 460, loss = 0.99, batch loss = 0.69 (37.6 examples/sec; 0.213 sec/batch; 19h:36m:28s remains)
INFO - root - 2017-12-09 05:39:40.232223: step 470, loss = 0.99, batch loss = 0.69 (37.7 examples/sec; 0.212 sec/batch; 19h:32m:47s remains)
INFO - root - 2017-12-09 05:39:42.422548: step 480, loss = 0.99, batch loss = 0.69 (36.8 examples/sec; 0.218 sec/batch; 20h:04m:09s remains)
INFO - root - 2017-12-09 05:39:44.592726: step 490, loss = 1.00, batch loss = 0.69 (37.0 examples/sec; 0.216 sec/batch; 19h:57m:24s remains)
INFO - root - 2017-12-09 05:39:46.805753: step 500, loss = 1.00, batch loss = 0.69 (34.9 examples/sec; 0.229 sec/batch; 21h:09m:10s remains)
2017-12-09 05:39:47.222763: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.022677438 0.023363179 0.023358718 0.023362847 0.023375519 0.023349049 0.023231866 0.022978112 0.022530656 0.0218386 0.020858727 0.019569987 0.01797731 0.01609784 0.01396814][0.02702857 0.027740953 0.027675135 0.027667467 0.027676458 0.027626151 0.027441584 0.02701846 0.026320079 0.025327183 0.024028989 0.022424789 0.020490615 0.018155422 0.015440265][0.026823945 0.027405817 0.02721747 0.027104788 0.027034067 0.026905507 0.026617099 0.026093362 0.025297033 0.02425234 0.022980306 0.021471 0.019632112 0.017254841 0.014310917][0.026173092 0.026527572 0.026092237 0.025765507 0.025529234 0.02529615 0.024953781 0.024450354 0.023734756 0.022866026 0.021861371 0.020649651 0.019071989 0.016828073 0.0138286][0.02499081 0.025080852 0.0242838 0.023657944 0.023218013 0.022886941 0.022576718 0.022201942 0.021713659 0.021111211 0.020429898 0.019569058 0.018355751 0.016455466 0.013701335][0.023011189 0.022791786 0.021655852 0.020755719 0.020118207 0.019710641 0.0194338 0.019175118 0.018886765 0.018558942 0.018196931 0.017698629 0.016916543 0.015507936 0.013234581][0.020075608 0.019465672 0.018057436 0.016987385 0.01627993 0.0158829 0.015669852 0.015516367 0.015365044 0.015204713 0.015059859 0.014846062 0.014440216 0.013522381 0.011789912][0.016167363 0.015247151 0.013614455 0.012371411 0.011564801 0.011150576 0.01099905 0.010946942 0.01090714 0.010877577 0.010896231 0.010908168 0.010805377 0.010282811 0.0090518016][0.011609882 0.010467923 0.0086556831 0.007243868 0.006316998 0.0058492189 0.0057122773 0.0057252636 0.0057700779 0.0058285045 0.0059430646 0.0061063049 0.0062133465 0.0060081482 0.005270062][0.0069576958 0.0056932196 0.0039066 0.0025377858 0.0016635538 0.0012462526 0.0011470928 0.001185975 0.0012566692 0.0013408931 0.0014689805 0.0016394008 0.0017854364 0.0017603782 0.0014402943][0.002761374 0.0016562731 0.00033047609 -0.00059123372 -0.001122424 -0.001351069 -0.0013945737 -0.0013759125 -0.001343976 -0.0013037908 -0.0012406039 -0.001155153 -0.0010780217 -0.0010606098 -0.001137979][-0.00020786817 -0.00088855671 -0.0015149869 -0.0018639336 -0.002014369 -0.002059903 -0.0020626939 -0.0020628255 -0.0020620783 -0.002059785 -0.0020531374 -0.0020428381 -0.002034453 -0.0020306038 -0.0020314327][-0.0017067931 -0.0018898483 -0.0020246927 -0.0020656376 -0.0020678395 -0.0020680991 -0.0020682288 -0.0020683135 -0.0020683461 -0.0020683706 -0.0020683792 -0.0020615184 -0.002048349 -0.002031791 -0.0020146158][-0.0020589996 -0.0020657962 -0.0020670844 -0.0020673845 -0.002067551 -0.0020676693 -0.002067755 -0.0020678083 -0.0020678279 -0.0020678295 -0.0020678095 -0.0020501984 -0.0020132624 -0.0019576189 -0.0018915316][-0.0020682823 -0.0020659387 -0.0020662302 -0.0020664253 -0.0020665429 -0.0020666092 -0.002066653 -0.0020666812 -0.0020666723 -0.0020666451 -0.0020666106 -0.0020436496 -0.0019863637 -0.0018849365 -0.0017559815]]...]
INFO - root - 2017-12-09 05:39:49.444214: step 510, loss = 1.00, batch loss = 0.69 (36.4 examples/sec; 0.220 sec/batch; 20h:16m:39s remains)
INFO - root - 2017-12-09 05:39:51.641701: step 520, loss = 1.00, batch loss = 0.69 (36.1 examples/sec; 0.222 sec/batch; 20h:27m:09s remains)
INFO - root - 2017-12-09 05:39:53.820592: step 530, loss = 1.00, batch loss = 0.69 (37.7 examples/sec; 0.212 sec/batch; 19h:33m:57s remains)
INFO - root - 2017-12-09 05:39:55.949611: step 540, loss = 1.00, batch loss = 0.68 (38.0 examples/sec; 0.211 sec/batch; 19h:24m:41s remains)
INFO - root - 2017-12-09 05:39:58.133405: step 550, loss = 1.00, batch loss = 0.69 (35.4 examples/sec; 0.226 sec/batch; 20h:49m:02s remains)
INFO - root - 2017-12-09 05:40:00.316529: step 560, loss = 1.01, batch loss = 0.69 (36.9 examples/sec; 0.217 sec/batch; 19h:58m:29s remains)
INFO - root - 2017-12-09 05:40:02.481204: step 570, loss = 1.01, batch loss = 0.69 (36.9 examples/sec; 0.217 sec/batch; 19h:58m:24s remains)
INFO - root - 2017-12-09 05:40:04.678939: step 580, loss = 1.01, batch loss = 0.69 (36.2 examples/sec; 0.221 sec/batch; 20h:21m:46s remains)
INFO - root - 2017-12-09 05:40:06.894999: step 590, loss = 1.01, batch loss = 0.69 (35.7 examples/sec; 0.224 sec/batch; 20h:41m:08s remains)
INFO - root - 2017-12-09 05:40:09.104736: step 600, loss = 1.02, batch loss = 0.69 (35.4 examples/sec; 0.226 sec/batch; 20h:51m:17s remains)
2017-12-09 05:40:09.435021: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.072973035 0.07841967 0.082125835 0.086598217 0.090592526 0.093750089 0.097389385 0.10509557 0.12059555 0.14594243 0.17997032 0.21924074 0.25849804 0.29257733 0.31855306][0.080714241 0.091609545 0.10140993 0.11171332 0.12081573 0.12789097 0.13381933 0.14203832 0.15651263 0.17967443 0.2108586 0.24684921 0.28314161 0.31514773 0.33979708][0.11598846 0.13114782 0.1439518 0.15657362 0.16740558 0.17551118 0.18132889 0.18768455 0.19817357 0.21522655 0.23870476 0.26629144 0.29462233 0.32007667 0.34010151][0.17789409 0.19628148 0.20937534 0.22147582 0.23128375 0.2379974 0.24174975 0.24459627 0.2494113 0.25827086 0.27143335 0.28757375 0.30474293 0.32073933 0.333927][0.2498986 0.27148911 0.28400019 0.2943874 0.30166835 0.3053247 0.30545062 0.303464 0.30157214 0.30141175 0.30350462 0.30744547 0.31269094 0.31857261 0.32447314][0.31580675 0.33965829 0.35053065 0.35801411 0.36153695 0.36099786 0.35668895 0.34967315 0.3416799 0.33399689 0.32722321 0.32163432 0.3177568 0.31605414 0.31655553][0.36350948 0.3868205 0.39438251 0.39786935 0.39719141 0.39270309 0.38484493 0.37452254 0.36314511 0.35176578 0.34087035 0.33099836 0.32313162 0.31814021 0.31617108][0.38110206 0.40268648 0.4078173 0.40947461 0.40754971 0.4023785 0.39433277 0.38414583 0.37298697 0.36182061 0.35110933 0.3414239 0.33366627 0.32856652 0.32613257][0.37251246 0.39264223 0.39804807 0.40175691 0.40304476 0.40153196 0.39702767 0.38994372 0.38134125 0.37235841 0.36368471 0.35594374 0.34980971 0.34570882 0.34347382][0.35452732 0.37320721 0.37974954 0.3868635 0.3930898 0.3970522 0.39772922 0.39503029 0.38986775 0.38360009 0.37724397 0.37154627 0.36701569 0.36384973 0.36178356][0.33936915 0.35583553 0.36229566 0.37149239 0.38148767 0.39006427 0.39539528 0.39680481 0.39492261 0.39117339 0.38680848 0.38270783 0.37932441 0.37676758 0.37477955][0.33101973 0.34505513 0.35016164 0.35958466 0.37121847 0.38233358 0.39050964 0.39455923 0.39479098 0.39258581 0.38934389 0.3860743 0.38325697 0.38101229 0.37915215][0.32902038 0.34131834 0.34466359 0.3529636 0.36418834 0.37547764 0.38416243 0.3888016 0.38953412 0.38770741 0.38479665 0.38186347 0.37939095 0.37750202 0.37604931][0.333766 0.34526637 0.34710163 0.35349318 0.36265436 0.37196213 0.37891808 0.38219494 0.38200417 0.37970072 0.37671098 0.37398517 0.37188846 0.37046182 0.36952594][0.34456196 0.35654327 0.357499 0.36184326 0.36808571 0.37408209 0.37783626 0.3784402 0.37636361 0.3729921 0.36960864 0.36693433 0.36511588 0.36402628 0.36338517]]...]
INFO - root - 2017-12-09 05:40:11.615635: step 610, loss = 1.02, batch loss = 0.70 (38.1 examples/sec; 0.210 sec/batch; 19h:20m:12s remains)
INFO - root - 2017-12-09 05:40:13.816443: step 620, loss = 1.02, batch loss = 0.69 (35.7 examples/sec; 0.224 sec/batch; 20h:41m:05s remains)
INFO - root - 2017-12-09 05:40:15.973046: step 630, loss = 1.02, batch loss = 0.69 (36.5 examples/sec; 0.219 sec/batch; 20h:12m:06s remains)
INFO - root - 2017-12-09 05:40:18.154839: step 640, loss = 1.02, batch loss = 0.69 (36.0 examples/sec; 0.222 sec/batch; 20h:29m:54s remains)
INFO - root - 2017-12-09 05:40:20.313147: step 650, loss = 1.03, batch loss = 0.69 (36.5 examples/sec; 0.219 sec/batch; 20h:12m:16s remains)
INFO - root - 2017-12-09 05:40:22.503543: step 660, loss = 1.04, batch loss = 0.69 (37.1 examples/sec; 0.216 sec/batch; 19h:53m:10s remains)
INFO - root - 2017-12-09 05:40:24.666466: step 670, loss = 1.05, batch loss = 0.70 (37.4 examples/sec; 0.214 sec/batch; 19h:43m:04s remains)
INFO - root - 2017-12-09 05:40:26.833199: step 680, loss = 1.05, batch loss = 0.70 (36.6 examples/sec; 0.219 sec/batch; 20h:10m:12s remains)
INFO - root - 2017-12-09 05:40:28.983046: step 690, loss = 1.05, batch loss = 0.69 (38.4 examples/sec; 0.208 sec/batch; 19h:12m:03s remains)
INFO - root - 2017-12-09 05:40:31.159482: step 700, loss = 1.05, batch loss = 0.69 (36.3 examples/sec; 0.220 sec/batch; 20h:18m:44s remains)
2017-12-09 05:40:31.492179: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.12503842 0.13012037 0.13252005 0.13344958 0.13367774 0.13321725 0.13237891 0.13099286 0.12914273 0.12648822 0.12264916 0.11704339 0.10896568 0.097599119 0.082509123][0.12345445 0.12894529 0.13136938 0.13225739 0.13217673 0.13153958 0.13057783 0.1293868 0.1279086 0.12582424 0.12260865 0.11762527 0.11015882 0.09937723 0.084815629][0.11992948 0.12548548 0.12741637 0.1274675 0.12651028 0.1252268 0.12405565 0.12319678 0.12253171 0.12158814 0.1195697 0.1157712 0.1094026 0.099662505 0.086155526][0.11449824 0.11918581 0.1196162 0.11771271 0.11480301 0.11198782 0.11014698 0.1097938 0.1107235 0.11204983 0.11244708 0.1110068 0.10681358 0.099098794 0.087408446][0.10622768 0.10914896 0.10724629 0.1026604 0.097108826 0.09216322 0.089284725 0.089417763 0.092320628 0.096643306 0.100508 0.10267097 0.1020321 0.097598411 0.088750117][0.095039628 0.095572092 0.091359004 0.084399104 0.076636411 0.069972195 0.066176087 0.066704564 0.07122241 0.078253843 0.085619822 0.0918143 0.095573172 0.095414907 0.090281568][0.081404224 0.079937026 0.073756143 0.064842552 0.055359676 0.047566622 0.04341897 0.044242281 0.049946789 0.059355587 0.07008522 0.080300517 0.088532157 0.093070664 0.091959514][0.067254871 0.064112484 0.056332923 0.045886386 0.035123941 0.026597485 0.02233018 0.02332985 0.029961292 0.041369785 0.055172473 0.069178954 0.081686951 0.090762444 0.093502149][0.054094888 0.049700275 0.040977523 0.029789362 0.018485341 0.0097300066 0.0055109253 0.0065425858 0.013665718 0.026459858 0.042583134 0.059582822 0.0756502 0.088609442 0.094589263][0.042725731 0.037900217 0.029261056 0.018515732 0.0078029865 -0.00032941625 -0.0041763932 -0.0032366849 0.0037547452 0.016975645 0.034250956 0.052983772 0.071360044 0.087102361 0.095474333][0.033133425 0.028763369 0.021279998 0.012066853 0.0029695015 -0.0037412427 -0.0068487544 -0.0061207432 0.00032036193 0.013275501 0.030722264 0.049935833 0.069209442 0.086373448 0.096101142][0.02438689 0.021156635 0.015230858 0.0077565396 0.00044904975 -0.00476793 -0.0070448108 -0.0064946609 -0.0003954526 0.012465143 0.030013435 0.049355526 0.06867329 0.086219713 0.096419208][0.016395286 0.014213943 0.0098263575 0.0041788435 -0.0013578446 -0.005302133 -0.00704977 -0.0065231631 -0.00049182633 0.012291695 0.02979102 0.049087267 0.06832061 0.0857649 0.095977381][0.0098652886 0.0084612677 0.0053556943 0.0013124775 -0.0026759398 -0.0056208605 -0.0070641893 -0.0065555512 -0.00067278976 0.011792243 0.028904337 0.047870271 0.06681738 0.084007412 0.094172][0.0039653881 0.0031887544 0.0012127673 -0.0013844389 -0.0039713825 -0.005961908 -0.007059467 -0.0066590942 -0.0012974464 0.010331311 0.026597261 0.044882488 0.0633088 0.080192082 0.090501495]]...]
INFO - root - 2017-12-09 05:40:33.694246: step 710, loss = 1.06, batch loss = 0.69 (36.1 examples/sec; 0.222 sec/batch; 20h:26m:59s remains)
INFO - root - 2017-12-09 05:40:35.880263: step 720, loss = 1.06, batch loss = 0.69 (37.0 examples/sec; 0.216 sec/batch; 19h:54m:00s remains)
INFO - root - 2017-12-09 05:40:38.088179: step 730, loss = 1.06, batch loss = 0.69 (33.0 examples/sec; 0.242 sec/batch; 22h:20m:13s remains)
INFO - root - 2017-12-09 05:40:40.259203: step 740, loss = 1.07, batch loss = 0.69 (36.0 examples/sec; 0.222 sec/batch; 20h:29m:46s remains)
INFO - root - 2017-12-09 05:40:42.434236: step 750, loss = 1.08, batch loss = 0.71 (36.2 examples/sec; 0.221 sec/batch; 20h:21m:53s remains)
INFO - root - 2017-12-09 05:40:44.578363: step 760, loss = 1.07, batch loss = 0.69 (35.4 examples/sec; 0.226 sec/batch; 20h:49m:56s remains)
INFO - root - 2017-12-09 05:40:46.769954: step 770, loss = 1.09, batch loss = 0.71 (37.3 examples/sec; 0.214 sec/batch; 19h:45m:29s remains)
INFO - root - 2017-12-09 05:40:48.962091: step 780, loss = 1.10, batch loss = 0.72 (37.7 examples/sec; 0.212 sec/batch; 19h:33m:44s remains)
INFO - root - 2017-12-09 05:40:51.094754: step 790, loss = 1.10, batch loss = 0.71 (38.1 examples/sec; 0.210 sec/batch; 19h:19m:44s remains)
INFO - root - 2017-12-09 05:40:53.340911: step 800, loss = 1.08, batch loss = 0.70 (35.1 examples/sec; 0.228 sec/batch; 20h:59m:03s remains)
2017-12-09 05:40:53.773305: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.033217933 0.041270208 0.039570566 0.038767558 0.037919864 0.036294818 0.03099183 0.020519134 0.0071931938 -0.0032957727 -0.0076118251 -0.0082268734 -0.0081684757 -0.0078180265 -0.007159736][0.03062705 0.036324296 0.032176618 0.029752653 0.028686982 0.026862103 0.021512993 0.011876588 0.0013519572 -0.0061021787 -0.0087792454 -0.0086272163 -0.008201967 -0.007725833 -0.0070172343][0.026236199 0.028162971 0.021477453 0.017795838 0.016792353 0.015003172 0.010794607 0.0040135682 -0.0032830774 -0.0079548433 -0.0093884747 -0.0091033932 -0.0087150633 -0.0081207938 -0.0071594324][0.017618194 0.017268967 0.0099518215 0.0065100817 0.0062724212 0.0054513887 0.0029220823 -0.0013839537 -0.0061368621 -0.0086870957 -0.0096420031 -0.0095755048 -0.0092410315 -0.0086690281 -0.0077455309][0.0075775245 0.0063915057 0.00078205112 -0.0012644194 -0.00055951159 -0.00063958578 -0.0017720815 -0.0045322659 -0.0074047046 -0.0086819632 -0.0095326016 -0.0098540038 -0.0095708389 -0.0089631341 -0.0081152981][-0.00042648986 -0.0015335865 -0.004982648 -0.0060636722 -0.0047397166 -0.0039001661 -0.0046506836 -0.0061614914 -0.0078052869 -0.0084960191 -0.0093860924 -0.0099516315 -0.0098370444 -0.0092889825 -0.00872877][-0.0057929517 -0.0063159387 -0.0078892838 -0.0080508795 -0.0064843534 -0.0053026653 -0.0060611507 -0.0069655115 -0.0079639563 -0.0083056157 -0.0092027532 -0.0098655317 -0.0099083344 -0.0095823407 -0.0093076769][-0.0086671244 -0.0088907611 -0.0092572849 -0.0086394139 -0.0067200251 -0.0059113931 -0.0068762195 -0.0077746594 -0.0085782912 -0.0087127881 -0.0092505571 -0.0097706523 -0.0098391194 -0.0097904075 -0.0095697809][-0.0097460207 -0.0097999442 -0.0096840523 -0.0088502085 -0.0072924462 -0.0068209535 -0.0079199579 -0.0086750733 -0.0091642393 -0.0091983555 -0.0094907153 -0.0097983954 -0.009857282 -0.0099212034 -0.009786481][-0.0099565089 -0.0099556847 -0.0098600481 -0.0091089848 -0.0081414878 -0.0079853954 -0.0088257026 -0.0093460819 -0.0096050249 -0.0095762182 -0.0096897138 -0.00985071 -0.0098857172 -0.0099761281 -0.009948086][-0.009975465 -0.0099704163 -0.0099168783 -0.0095384736 -0.0088955909 -0.0088847708 -0.0094100125 -0.00971328 -0.0098793283 -0.0098741241 -0.0098602679 -0.009910359 -0.0099078994 -0.009985311 -0.009983168][-0.0099822218 -0.0099788606 -0.0099716932 -0.00981693 -0.0094496952 -0.0094877817 -0.0097538745 -0.0098995408 -0.0099634938 -0.0099674119 -0.0099526551 -0.0099633019 -0.0099514872 -0.00997945 -0.0099798618][-0.0099755088 -0.0099801878 -0.0099598244 -0.0099283988 -0.0097861746 -0.009830662 -0.0099236788 -0.0099716913 -0.0099707851 -0.0099714259 -0.009976441 -0.0099718254 -0.0099752331 -0.009977798 -0.0099618724][-0.0099100238 -0.0098649058 -0.0099081239 -0.0098658428 -0.0099356277 -0.0099005364 -0.0097409077 -0.00965082 -0.0099653192 -0.0099659907 -0.0099745616 -0.009979194 -0.0099796792 -0.0099654179 -0.0099338712][-0.0098541863 -0.0097964043 -0.0099003157 -0.0098997811 -0.0099743335 -0.0099217668 -0.0095982915 -0.009477227 -0.0098804953 -0.0099648507 -0.0099741649 -0.0099749155 -0.0099670384 -0.009951666 -0.0099131716]]...]
INFO - root - 2017-12-09 05:40:55.931584: step 810, loss = 1.08, batch loss = 0.69 (37.3 examples/sec; 0.214 sec/batch; 19h:44m:33s remains)
INFO - root - 2017-12-09 05:40:58.099460: step 820, loss = 1.10, batch loss = 0.71 (36.9 examples/sec; 0.217 sec/batch; 19h:58m:29s remains)
INFO - root - 2017-12-09 05:41:00.252301: step 830, loss = 1.08, batch loss = 0.69 (38.3 examples/sec; 0.209 sec/batch; 19h:15m:01s remains)
INFO - root - 2017-12-09 05:41:02.407294: step 840, loss = 1.09, batch loss = 0.69 (37.5 examples/sec; 0.213 sec/batch; 19h:38m:29s remains)
INFO - root - 2017-12-09 05:41:04.600412: step 850, loss = 1.09, batch loss = 0.69 (37.5 examples/sec; 0.213 sec/batch; 19h:38m:59s remains)
INFO - root - 2017-12-09 05:41:06.797139: step 860, loss = 1.09, batch loss = 0.69 (34.1 examples/sec; 0.235 sec/batch; 21h:37m:34s remains)
INFO - root - 2017-12-09 05:41:08.999460: step 870, loss = 1.09, batch loss = 0.69 (36.6 examples/sec; 0.219 sec/batch; 20h:09m:12s remains)
INFO - root - 2017-12-09 05:41:11.188503: step 880, loss = 1.10, batch loss = 0.69 (35.9 examples/sec; 0.223 sec/batch; 20h:31m:36s remains)
INFO - root - 2017-12-09 05:41:13.374079: step 890, loss = 1.10, batch loss = 0.69 (37.0 examples/sec; 0.216 sec/batch; 19h:54m:56s remains)
INFO - root - 2017-12-09 05:41:15.616980: step 900, loss = 1.10, batch loss = 0.69 (34.9 examples/sec; 0.229 sec/batch; 21h:05m:35s remains)
2017-12-09 05:41:16.047561: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.011811019 -0.011787346 -0.011778103 -0.011771124 -0.011768418 -0.011771748 -0.011783062 -0.011804589 -0.0118372 -0.011877649 -0.01191745 -0.011948083 -0.011963112 -0.011958555 -0.011936484][-0.011805668 -0.011784199 -0.011776669 -0.011771289 -0.011769724 -0.01177347 -0.011784367 -0.011804384 -0.011834561 -0.011872257 -0.011909869 -0.011939519 -0.011954825 -0.011951812 -0.01193178][-0.011802555 -0.011783545 -0.011778121 -0.011774316 -0.011773334 -0.011776457 -0.01178529 -0.011801186 -0.011825306 -0.01185591 -0.011887077 -0.011912703 -0.011927222 -0.011927029 -0.011912561][-0.011802745 -0.011785769 -0.011783158 -0.011781342 -0.011781059 -0.01178327 -0.011789112 -0.01179961 -0.011815631 -0.01183633 -0.011857893 -0.011876206 -0.011887633 -0.011889647 -0.011882119][-0.011802595 -0.011788193 -0.011788584 -0.011789026 -0.011789808 -0.011791407 -0.011794521 -0.011799938 -0.01180821 -0.01181918 -0.011830948 -0.011841313 -0.011848517 -0.011851229 -0.011849075][-0.011801346 -0.011789713 -0.011792324 -0.011794419 -0.011795976 -0.011797043 -0.011798145 -0.011799957 -0.011802942 -0.011807358 -0.011812543 -0.011817587 -0.01182183 -0.011824754 -0.011825687][-0.011799417 -0.011790692 -0.011794271 -0.011796979 -0.011798637 -0.011799087 -0.011798865 -0.011798685 -0.011799037 -0.01180042 -0.011802743 -0.011805908 -0.011809592 -0.01181319 -0.011816049][-0.011798395 -0.011791695 -0.01179538 -0.011797992 -0.011799324 -0.011799204 -0.011798188 -0.011797064 -0.011796496 -0.011797065 -0.011799012 -0.011802335 -0.011806651 -0.011811178 -0.011815066][-0.011800651 -0.01179251 -0.011795679 -0.011797731 -0.011798537 -0.011797976 -0.011796655 -0.011795332 -0.011794731 -0.011795435 -0.011797639 -0.011801334 -0.011806152 -0.011811267 -0.011815732][-0.011800146 -0.011791852 -0.011794091 -0.011795273 -0.011795406 -0.011794433 -0.011792997 -0.011791776 -0.011791402 -0.011792415 -0.011794941 -0.011798942 -0.011804073 -0.011809565 -0.011814471][-0.01179656 -0.0117875 -0.011788707 -0.011788918 -0.011788282 -0.011786862 -0.011785332 -0.011784253 -0.011784162 -0.011785503 -0.011788292 -0.011792437 -0.011797645 -0.011803268 -0.011808383][-0.011788117 -0.01177819 -0.011778498 -0.01177788 -0.011776621 -0.011774975 -0.011773473 -0.011772611 -0.011772819 -0.011774387 -0.011777227 -0.011781235 -0.011786187 -0.011791578 -0.011796593][-0.011777725 -0.011764792 -0.011764637 -0.011763628 -0.011762145 -0.011760505 -0.011759172 -0.011758531 -0.011758928 -0.011760526 -0.011763136 -0.011766592 -0.011770766 -0.011775346 -0.011779697][-0.011763097 -0.011749341 -0.011749112 -0.011748148 -0.011746835 -0.011745447 -0.011744381 -0.0117439 -0.011744265 -0.0117456 -0.011747676 -0.011750334 -0.011753457 -0.011756904 -0.011760268][-0.011747665 -0.011733032 -0.011732899 -0.011732213 -0.011731276 -0.011730295 -0.011729551 -0.011729218 -0.011729461 -0.011730405 -0.011731839 -0.011733613 -0.011735634 -0.011737869 -0.011740121]]...]
INFO - root - 2017-12-09 05:41:18.188882: step 910, loss = 1.10, batch loss = 0.69 (34.8 examples/sec; 0.230 sec/batch; 21h:11m:23s remains)
INFO - root - 2017-12-09 05:41:20.366476: step 920, loss = 1.10, batch loss = 0.69 (35.4 examples/sec; 0.226 sec/batch; 20h:47m:56s remains)
INFO - root - 2017-12-09 05:41:22.550137: step 930, loss = 1.10, batch loss = 0.69 (37.2 examples/sec; 0.215 sec/batch; 19h:48m:49s remains)
INFO - root - 2017-12-09 05:41:24.725416: step 940, loss = 1.10, batch loss = 0.69 (34.5 examples/sec; 0.232 sec/batch; 21h:21m:13s remains)
INFO - root - 2017-12-09 05:41:26.880236: step 950, loss = 1.11, batch loss = 0.69 (36.3 examples/sec; 0.221 sec/batch; 20h:18m:34s remains)
INFO - root - 2017-12-09 05:41:29.009327: step 960, loss = 1.11, batch loss = 0.69 (37.6 examples/sec; 0.213 sec/batch; 19h:36m:31s remains)
INFO - root - 2017-12-09 05:41:31.136875: step 970, loss = 1.11, batch loss = 0.69 (37.2 examples/sec; 0.215 sec/batch; 19h:48m:49s remains)
INFO - root - 2017-12-09 05:41:33.302115: step 980, loss = 1.11, batch loss = 0.69 (37.1 examples/sec; 0.215 sec/batch; 19h:50m:41s remains)
INFO - root - 2017-12-09 05:41:35.461050: step 990, loss = 1.11, batch loss = 0.69 (37.0 examples/sec; 0.216 sec/batch; 19h:54m:53s remains)
INFO - root - 2017-12-09 05:41:37.627962: step 1000, loss = 1.11, batch loss = 0.69 (37.4 examples/sec; 0.214 sec/batch; 19h:42m:48s remains)
2017-12-09 05:41:37.980501: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.15279174 0.15803768 0.15781952 0.15560132 0.15248811 0.14987545 0.14897381 0.15032594 0.15351188 0.15746443 0.16097715 0.16319957 0.16370782 0.16265497 0.16068305][0.17014536 0.17438674 0.17300962 0.16992369 0.16607779 0.16268335 0.16079617 0.16089535 0.16261616 0.16500868 0.16699794 0.16785237 0.16726553 0.16545115 0.16301516][0.18546946 0.18867528 0.18676737 0.18367836 0.18004915 0.17671996 0.17441338 0.17343847 0.17349514 0.17384613 0.17367332 0.17247869 0.17017965 0.16712609 0.16390403][0.1961491 0.19884807 0.19725615 0.19509169 0.19262964 0.19026002 0.18829088 0.18679948 0.18552177 0.18395302 0.18157686 0.17817681 0.17394021 0.16940087 0.16516861][0.20129415 0.20411846 0.20324807 0.20230937 0.20126863 0.20013225 0.19885665 0.19730555 0.19522737 0.19230464 0.18826918 0.18314303 0.17733626 0.17154513 0.1664169][0.20335585 0.20621406 0.20602202 0.20610283 0.20621352 0.20609625 0.20546743 0.20404033 0.2015613 0.19783632 0.19277115 0.18655476 0.17973605 0.17311734 0.16737536][0.20427503 0.20717408 0.20750083 0.20829847 0.20920169 0.20978801 0.20961933 0.20830777 0.20559376 0.20136812 0.19565767 0.18876502 0.18132035 0.17418794 0.16805793][0.20490693 0.20811804 0.20881489 0.21004699 0.21140096 0.21237123 0.21243343 0.21114153 0.20823674 0.20366921 0.19754238 0.19022804 0.18239729 0.17494622 0.1685674][0.20576729 0.20947644 0.21031283 0.21162789 0.21303213 0.21401954 0.21405266 0.21267378 0.20962858 0.2048925 0.19858764 0.19110259 0.18310744 0.17550372 0.16898073][0.20716527 0.21118741 0.21189547 0.21295524 0.21404986 0.21473879 0.21453381 0.21300523 0.20990203 0.20518661 0.19894259 0.19152294 0.18355757 0.17593397 0.16934024][0.20882685 0.21301827 0.21341664 0.21402518 0.21461415 0.214841 0.21430261 0.21262152 0.20954576 0.20499459 0.19897173 0.19175631 0.18391903 0.17632702 0.16967882][0.21042989 0.21469073 0.2147014 0.21478932 0.21480437 0.2145111 0.21361156 0.21179719 0.20881419 0.20451607 0.19880074 0.19185331 0.18418205 0.17663592 0.16993742][0.21130376 0.21556447 0.21517238 0.21474452 0.21419744 0.21339983 0.21216497 0.21026094 0.20743041 0.20344749 0.19809672 0.19147015 0.1840241 0.17658968 0.16991548][0.21022098 0.21420413 0.21347682 0.2126347 0.2116425 0.21045668 0.20899433 0.20709659 0.20450327 0.20091589 0.19602935 0.18986621 0.18283869 0.17574085 0.16932574][0.20573044 0.20945495 0.20854636 0.2074863 0.20626815 0.20491093 0.20340972 0.20166042 0.1994077 0.19631243 0.19203289 0.18655601 0.18025051 0.17384094 0.16803721]]...]
INFO - root - 2017-12-09 05:41:40.163789: step 1010, loss = 1.11, batch loss = 0.69 (36.8 examples/sec; 0.217 sec/batch; 20h:01m:01s remains)
INFO - root - 2017-12-09 05:41:42.372896: step 1020, loss = 1.11, batch loss = 0.69 (34.4 examples/sec; 0.232 sec/batch; 21h:23m:56s remains)
INFO - root - 2017-12-09 05:41:44.541112: step 1030, loss = 1.11, batch loss = 0.69 (35.7 examples/sec; 0.224 sec/batch; 20h:37m:15s remains)
INFO - root - 2017-12-09 05:41:46.730240: step 1040, loss = 1.12, batch loss = 0.69 (36.7 examples/sec; 0.218 sec/batch; 20h:04m:39s remains)
INFO - root - 2017-12-09 05:41:48.864313: step 1050, loss = 1.12, batch loss = 0.69 (37.2 examples/sec; 0.215 sec/batch; 19h:49m:16s remains)
INFO - root - 2017-12-09 05:41:51.050363: step 1060, loss = 1.12, batch loss = 0.69 (36.7 examples/sec; 0.218 sec/batch; 20h:03m:21s remains)
INFO - root - 2017-12-09 05:41:53.243356: step 1070, loss = 1.12, batch loss = 0.69 (38.3 examples/sec; 0.209 sec/batch; 19h:13m:11s remains)
INFO - root - 2017-12-09 05:41:55.412280: step 1080, loss = 1.12, batch loss = 0.69 (38.6 examples/sec; 0.207 sec/batch; 19h:03m:59s remains)
INFO - root - 2017-12-09 05:41:57.589134: step 1090, loss = 1.13, batch loss = 0.69 (36.8 examples/sec; 0.217 sec/batch; 20h:00m:12s remains)
INFO - root - 2017-12-09 05:41:59.738833: step 1100, loss = 1.13, batch loss = 0.69 (38.0 examples/sec; 0.211 sec/batch; 19h:23m:28s remains)
2017-12-09 05:42:00.080342: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.016302617 -0.01630057 -0.016309723 -0.016327377 -0.016143216 -0.016297119 -0.016316157 -0.016361943 -0.016424321 -0.016430127 -0.016341623 -0.016349947 -0.016441202 -0.016428992 -0.016415989][-0.016349208 -0.016331917 -0.016313875 -0.016323002 -0.01631177 -0.016328769 -0.016198128 -0.01629746 -0.016324662 -0.016367547 -0.016440772 -0.016445853 -0.016441664 -0.016430639 -0.016409082][-0.016397782 -0.016383974 -0.016365165 -0.016361762 -0.016349673 -0.016376698 -0.016359923 -0.016355356 -0.016231351 -0.016335066 -0.016346518 -0.0163784 -0.016438644 -0.016429583 -0.016417293][-0.016232777 -0.016302593 -0.016391069 -0.016383749 -0.016350092 -0.016379073 -0.016356183 -0.016359029 -0.01637727 -0.016391164 -0.016349483 -0.016335431 -0.016364228 -0.016417779 -0.016413836][-0.016433593 -0.01637801 -0.016062748 -0.016131742 -0.016307637 -0.016384855 -0.016365679 -0.016374582 -0.016356317 -0.01637125 -0.016394526 -0.016401203 -0.01640754 -0.016414752 -0.016402088][-0.016436467 -0.016427845 -0.016416406 -0.016386881 -0.016253695 -0.016243203 -0.01638047 -0.01637865 -0.016359681 -0.016376063 -0.01636968 -0.01637667 -0.016390905 -0.016387684 -0.016388537][-0.016400214 -0.016402174 -0.01641931 -0.01640659 -0.01639642 -0.016385287 -0.016340392 -0.016355682 -0.01638227 -0.016382959 -0.016367687 -0.016366111 -0.01636738 -0.0163922 -0.016388711][-0.016443718 -0.016434763 -0.016358439 -0.016351826 -0.01636217 -0.016378351 -0.016383948 -0.016384328 -0.01636528 -0.016214548 -0.01613383 -0.016382575 -0.016378356 -0.01638565 -0.016385384][-0.016446684 -0.01643719 -0.016422121 -0.016404737 -0.016363468 -0.01634682 -0.016355796 -0.016358748 -0.016370608 -0.016371254 -0.016313061 -0.016213115 -0.016140623 -0.016378447 -0.016376697][-0.016444838 -0.016438713 -0.016425133 -0.016407819 -0.016390212 -0.016376726 -0.016359989 -0.0163603 -0.016361447 -0.016362842 -0.016375924 -0.016378967 -0.016369538 -0.01630876 -0.01615148][-0.01644627 -0.016441245 -0.016430264 -0.016415814 -0.016400084 -0.016385876 -0.016376387 -0.016372107 -0.016371652 -0.016339978 -0.016322318 -0.016378647 -0.01638397 -0.016380096 -0.016338829][-0.01644003 -0.016444931 -0.016437531 -0.0164267 -0.016413951 -0.016401106 -0.01639099 -0.0163852 -0.016383076 -0.016383125 -0.016374027 -0.016357662 -0.016336383 -0.016378386 -0.016373962][-0.016280958 -0.016447412 -0.016439473 -0.016434385 -0.016423874 -0.016412377 -0.016402395 -0.016396163 -0.01639322 -0.016392084 -0.016391257 -0.01639007 -0.016387252 -0.016377978 -0.016338309][-0.016373739 -0.016384054 -0.016306968 -0.016433567 -0.016423972 -0.016413575 -0.016404355 -0.016398439 -0.016395587 -0.016394217 -0.016393311 -0.016392078 -0.016390808 -0.016390376 -0.016379409][-0.016300725 -0.016332317 -0.016384978 -0.016423514 -0.016416017 -0.0164085 -0.016402459 -0.016398996 -0.016397888 -0.016397681 -0.016397456 -0.016392767 -0.016371625 -0.016392289 -0.016391978]]...]
INFO - root - 2017-12-09 05:42:02.247824: step 1110, loss = 1.13, batch loss = 0.69 (36.5 examples/sec; 0.219 sec/batch; 20h:11m:44s remains)
INFO - root - 2017-12-09 05:42:04.406207: step 1120, loss = 1.13, batch loss = 0.69 (36.2 examples/sec; 0.221 sec/batch; 20h:21m:46s remains)
INFO - root - 2017-12-09 05:42:06.576968: step 1130, loss = 1.13, batch loss = 0.69 (34.4 examples/sec; 0.232 sec/batch; 21h:24m:03s remains)
INFO - root - 2017-12-09 05:42:08.734776: step 1140, loss = 1.14, batch loss = 0.69 (36.8 examples/sec; 0.218 sec/batch; 20h:01m:49s remains)
INFO - root - 2017-12-09 05:42:10.874692: step 1150, loss = 1.14, batch loss = 0.69 (37.9 examples/sec; 0.211 sec/batch; 19h:24m:28s remains)
INFO - root - 2017-12-09 05:42:13.058673: step 1160, loss = 1.13, batch loss = 0.69 (34.4 examples/sec; 0.233 sec/batch; 21h:24m:19s remains)
INFO - root - 2017-12-09 05:42:15.240159: step 1170, loss = 1.14, batch loss = 0.69 (36.6 examples/sec; 0.219 sec/batch; 20h:07m:08s remains)
INFO - root - 2017-12-09 05:42:17.391248: step 1180, loss = 1.14, batch loss = 0.69 (37.8 examples/sec; 0.212 sec/batch; 19h:28m:50s remains)
INFO - root - 2017-12-09 05:42:19.535062: step 1190, loss = 1.14, batch loss = 0.68 (36.5 examples/sec; 0.219 sec/batch; 20h:11m:42s remains)
INFO - root - 2017-12-09 05:42:21.707354: step 1200, loss = 1.14, batch loss = 0.69 (37.2 examples/sec; 0.215 sec/batch; 19h:47m:54s remains)
2017-12-09 05:42:22.076966: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.78678077 0.79094356 0.79034483 0.79012108 0.79011053 0.79023373 0.79022968 0.78953534 0.78751475 0.7839278 0.77891564 0.77285957 0.76638484 0.76039106 0.75553137][0.80084139 0.806 0.80540389 0.80511689 0.80523419 0.80591404 0.80701643 0.80793816 0.80785179 0.80622751 0.80284381 0.79779232 0.79157585 0.7851721 0.77945662][0.7934978 0.79986215 0.79915667 0.79840696 0.79801655 0.79850763 0.80007839 0.80234843 0.80447191 0.80560261 0.80498356 0.80218422 0.79735786 0.79137838 0.7852484][0.78640532 0.79497862 0.79417694 0.79278016 0.79142487 0.79098016 0.79207855 0.79477024 0.79843283 0.8020401 0.80429435 0.804176 0.80139339 0.79656065 0.790632][0.78057623 0.79144818 0.79070759 0.78881574 0.78648216 0.78477746 0.78472847 0.78691012 0.79108983 0.79623216 0.80067772 0.80295342 0.80233604 0.79908615 0.79394537][0.77568549 0.7888633 0.78825045 0.78601581 0.78281528 0.77977312 0.77820188 0.779195 0.782982 0.78867817 0.79448456 0.79866219 0.80015349 0.79880363 0.79499704][0.7719295 0.78717482 0.78658527 0.7840324 0.78004092 0.77567804 0.77242017 0.77177459 0.77444053 0.77983987 0.78627908 0.79195553 0.79555506 0.79647171 0.79460984][0.76897454 0.78613389 0.78573167 0.78324467 0.7789399 0.77366626 0.76888496 0.76635504 0.76720273 0.77132678 0.77744371 0.78391755 0.789288 0.79252231 0.79299152][0.76476693 0.78418916 0.78466779 0.78319895 0.779671 0.77454513 0.76903051 0.76489019 0.76359129 0.76562047 0.77031332 0.7764172 0.78251278 0.78731132 0.7897442][0.75849962 0.77990216 0.78200352 0.78227413 0.78029853 0.77618533 0.77081817 0.76581222 0.76278245 0.762691 0.76548314 0.7703492 0.77605969 0.78129709 0.78479308][0.74636376 0.77084422 0.77553231 0.77804804 0.77792865 0.77519953 0.77059996 0.7656064 0.76183397 0.76047617 0.76182765 0.76539338 0.77018183 0.7750169 0.77865255][0.72356343 0.75286144 0.76189816 0.76783043 0.77021676 0.76918805 0.76559579 0.76098305 0.75701219 0.75499994 0.75541747 0.75792849 0.76168597 0.765702 0.76885128][0.68563825 0.72094762 0.73611003 0.74698359 0.75283223 0.75378609 0.75095695 0.7461983 0.74147576 0.73841649 0.7377184 0.73910528 0.74172103 0.74467516 0.74694943][0.6328674 0.67230648 0.69331592 0.7090289 0.71817136 0.72069281 0.71791685 0.71205944 0.7056213 0.70078927 0.69857383 0.69869161 0.700114 0.70188767 0.70305622][0.56482708 0.6054619 0.62983614 0.64841604 0.6594575 0.66273314 0.65967137 0.65266442 0.64461046 0.6381349 0.634549 0.63358814 0.6339854 0.63469285 0.6347785]]...]
INFO - root - 2017-12-09 05:42:24.240206: step 1210, loss = 1.16, batch loss = 0.70 (36.4 examples/sec; 0.220 sec/batch; 20h:13m:32s remains)
INFO - root - 2017-12-09 05:42:26.408259: step 1220, loss = 1.14, batch loss = 0.68 (37.2 examples/sec; 0.215 sec/batch; 19h:48m:36s remains)
INFO - root - 2017-12-09 05:42:28.589712: step 1230, loss = 1.15, batch loss = 0.69 (37.3 examples/sec; 0.214 sec/batch; 19h:42m:43s remains)
INFO - root - 2017-12-09 05:42:30.738565: step 1240, loss = 1.15, batch loss = 0.69 (38.3 examples/sec; 0.209 sec/batch; 19h:13m:56s remains)
INFO - root - 2017-12-09 05:42:32.891426: step 1250, loss = 1.15, batch loss = 0.69 (37.3 examples/sec; 0.214 sec/batch; 19h:43m:15s remains)
INFO - root - 2017-12-09 05:42:35.065261: step 1260, loss = 1.16, batch loss = 0.69 (37.4 examples/sec; 0.214 sec/batch; 19h:40m:15s remains)
INFO - root - 2017-12-09 05:42:37.202393: step 1270, loss = 1.16, batch loss = 0.69 (37.8 examples/sec; 0.211 sec/batch; 19h:27m:07s remains)
INFO - root - 2017-12-09 05:42:39.373363: step 1280, loss = 1.16, batch loss = 0.69 (35.5 examples/sec; 0.225 sec/batch; 20h:43m:22s remains)
INFO - root - 2017-12-09 05:42:41.550392: step 1290, loss = 1.16, batch loss = 0.69 (36.8 examples/sec; 0.217 sec/batch; 19h:58m:27s remains)
INFO - root - 2017-12-09 05:42:43.718316: step 1300, loss = 1.16, batch loss = 0.69 (38.0 examples/sec; 0.211 sec/batch; 19h:22m:33s remains)
2017-12-09 05:42:44.044863: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.012458431 -0.010621662 -0.0080168005 -0.0064903824 -0.006010131 -0.0053445762 -0.0046991855 -0.0053922888 -0.0070406329 -0.00790961 -0.0092493612 -0.010934088 -0.012728726 -0.01434554 -0.015637921][-0.010638093 -0.0089987265 -0.0077995993 -0.0066890214 -0.0053305943 -0.0044521689 -0.004299229 -0.0047100764 -0.0052131033 -0.0065006567 -0.0089478279 -0.010634631 -0.012444431 -0.014119375 -0.015479363][-0.00888691 -0.007806845 -0.0074361544 -0.0063542491 -0.00551523 -0.0052532479 -0.004769478 -0.0043249205 -0.0047214963 -0.0063292664 -0.0080035618 -0.0096880058 -0.012299529 -0.013996871 -0.015391485][-0.00758001 -0.0076229963 -0.006016707 -0.0054280153 -0.0062789638 -0.0055750674 -0.0048479736 -0.0052749496 -0.0057266634 -0.0060758889 -0.0073627839 -0.0096748052 -0.012141754 -0.013851658 -0.015322696][-0.0073134238 -0.0069440491 -0.0066594994 -0.0060875751 -0.0049632806 -0.0048337691 -0.0059890877 -0.0055613676 -0.0053248974 -0.00666965 -0.0083529055 -0.00996726 -0.012074848 -0.01380764 -0.015256841][-0.0070526414 -0.0067076823 -0.0063232603 -0.0060851127 -0.0059933234 -0.0058890861 -0.0050666966 -0.0049772831 -0.0064392705 -0.006672957 -0.0078341244 -0.010165429 -0.01196845 -0.013711006 -0.015202854][-0.006136803 -0.0062257322 -0.0062639723 -0.0060503511 -0.005881547 -0.0058522308 -0.0059585785 -0.0058916975 -0.0055760872 -0.0064257821 -0.0084705837 -0.010106266 -0.011916366 -0.013671385 -0.015157361][-0.0063106585 -0.0059904857 -0.0055671511 -0.0057267863 -0.0059644021 -0.0059326524 -0.0059022335 -0.0059910631 -0.0063928245 -0.0071629472 -0.0081655746 -0.00974529 -0.011863993 -0.013624007 -0.015120327][-0.0061505139 -0.0056017125 -0.0058543328 -0.0057206294 -0.00543591 -0.0057471478 -0.0059317918 -0.0060267383 -0.006286202 -0.0062905615 -0.0069650477 -0.0093798479 -0.011836646 -0.013602274 -0.01510556][-0.0065177707 -0.006198274 -0.0060512852 -0.0060666148 -0.0059961155 -0.0059421761 -0.0059336666 -0.0060385996 -0.0063876882 -0.00703979 -0.0082716057 -0.0097322268 -0.011434503 -0.013554255 -0.015107208][-0.0066536432 -0.0064384528 -0.0063126534 -0.0061892159 -0.0061093392 -0.0060499497 -0.0060373703 -0.0061342726 -0.0064569786 -0.00664132 -0.0071335156 -0.0093218544 -0.011937509 -0.013679773 -0.015154855][-0.0070919143 -0.0068735471 -0.0067358715 -0.0065949224 -0.0064871414 -0.0063978219 -0.0063629709 -0.0064548403 -0.0068181138 -0.0072455667 -0.0084084524 -0.0099465763 -0.010869254 -0.012973016 -0.01526068][-0.007991706 -0.0077698445 -0.0076074135 -0.007436295 -0.0072895046 -0.0071612597 -0.007098238 -0.0071767513 -0.00753472 -0.0077723265 -0.0081871981 -0.010185272 -0.012621101 -0.013720914 -0.014366335][-0.0093257343 -0.0091019068 -0.0089185722 -0.0087247333 -0.008546792 -0.008390706 -0.0083050681 -0.0083693033 -0.0087053077 -0.0093924338 -0.010469845 -0.011423076 -0.01213263 -0.013840904 -0.015707035][-0.010768875 -0.010735282 -0.010543145 -0.010337677 -0.010144517 -0.0099753337 -0.0098784426 -0.0099265492 -0.010219997 -0.010517333 -0.011043414 -0.012467736 -0.01403785 -0.014923643 -0.015456191]]...]
INFO - root - 2017-12-09 05:42:46.236787: step 1310, loss = 1.16, batch loss = 0.69 (36.6 examples/sec; 0.219 sec/batch; 20h:06m:36s remains)
INFO - root - 2017-12-09 05:42:48.394786: step 1320, loss = 1.16, batch loss = 0.69 (36.4 examples/sec; 0.220 sec/batch; 20h:11m:59s remains)
INFO - root - 2017-12-09 05:42:50.593026: step 1330, loss = 1.17, batch loss = 0.70 (34.8 examples/sec; 0.230 sec/batch; 21h:08m:42s remains)
INFO - root - 2017-12-09 05:42:52.772211: step 1340, loss = 1.20, batch loss = 0.73 (37.4 examples/sec; 0.214 sec/batch; 19h:40m:48s remains)
INFO - root - 2017-12-09 05:42:54.922664: step 1350, loss = 1.27, batch loss = 0.80 (36.0 examples/sec; 0.222 sec/batch; 20h:25m:04s remains)
INFO - root - 2017-12-09 05:42:57.096501: step 1360, loss = 1.23, batch loss = 0.76 (37.3 examples/sec; 0.214 sec/batch; 19h:42m:26s remains)
INFO - root - 2017-12-09 05:42:59.277789: step 1370, loss = 1.24, batch loss = 0.77 (37.9 examples/sec; 0.211 sec/batch; 19h:25m:13s remains)
INFO - root - 2017-12-09 05:43:01.469606: step 1380, loss = 1.17, batch loss = 0.69 (35.4 examples/sec; 0.226 sec/batch; 20h:47m:41s remains)
INFO - root - 2017-12-09 05:43:03.638969: step 1390, loss = 1.17, batch loss = 0.69 (36.9 examples/sec; 0.217 sec/batch; 19h:55m:49s remains)
INFO - root - 2017-12-09 05:43:05.822914: step 1400, loss = 1.17, batch loss = 0.69 (35.9 examples/sec; 0.223 sec/batch; 20h:30m:49s remains)
2017-12-09 05:43:06.261664: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.022357069 -0.022327336 -0.022335388 -0.022282127 -0.022271957 -0.02219945 -0.022156708 -0.022178892 -0.022204213 -0.02225207 -0.022312185 -0.0223893 -0.022424888 -0.022443034 -0.0224512][-0.022224383 -0.022174148 -0.022179201 -0.022109007 -0.022087911 -0.022031121 -0.021994427 -0.022001043 -0.022017119 -0.022087026 -0.022189133 -0.022284826 -0.022336885 -0.022392139 -0.022418829][-0.022054747 -0.022014 -0.021998318 -0.02189506 -0.021866053 -0.021792581 -0.021747591 -0.021734577 -0.021780763 -0.021883203 -0.022014016 -0.022144556 -0.022223229 -0.022290602 -0.0223042][-0.021876421 -0.021816026 -0.021781698 -0.021680102 -0.021664392 -0.021612497 -0.021596048 -0.021597482 -0.021681134 -0.021809064 -0.021980075 -0.02213853 -0.022234239 -0.022282295 -0.02227442][-0.02184833 -0.02175254 -0.021693055 -0.021590134 -0.021577092 -0.021554545 -0.021571793 -0.021605946 -0.021721186 -0.021860033 -0.022044171 -0.022220759 -0.022328168 -0.022367958 -0.022334453][-0.021947555 -0.021825645 -0.02171379 -0.021599239 -0.021574443 -0.021563184 -0.021589562 -0.021656949 -0.021806965 -0.021970445 -0.022160221 -0.022341337 -0.022441553 -0.022464832 -0.0224206][-0.022170782 -0.022035319 -0.02190068 -0.02176884 -0.021728475 -0.021728301 -0.021779824 -0.021864722 -0.02203214 -0.022212975 -0.022386156 -0.022538247 -0.022609686 -0.022599295 -0.022533555][-0.022441236 -0.02232516 -0.022198331 -0.022077978 -0.022037305 -0.022012966 -0.022059578 -0.022166407 -0.022356449 -0.022523036 -0.022661578 -0.022788839 -0.022838369 -0.02279463 -0.022708394][-0.022699093 -0.022596108 -0.022498002 -0.022410788 -0.022375269 -0.022354448 -0.022399677 -0.022512911 -0.022666445 -0.022794705 -0.022902552 -0.022995513 -0.023018213 -0.022965625 -0.022891585][-0.022939427 -0.02287874 -0.022821672 -0.022767033 -0.022727679 -0.022697877 -0.022722574 -0.022793239 -0.022878543 -0.022949284 -0.023008483 -0.023066677 -0.023078399 -0.023043606 -0.022988718][-0.023038076 -0.023006102 -0.022978241 -0.02295325 -0.022928286 -0.022907548 -0.02292398 -0.022963246 -0.023004463 -0.023037978 -0.02306563 -0.023096498 -0.023094432 -0.023066126 -0.023034358][-0.023091607 -0.023075232 -0.023063313 -0.023054501 -0.023050653 -0.023046838 -0.023059022 -0.023075242 -0.023093618 -0.023102911 -0.023106152 -0.023115657 -0.023109546 -0.023092562 -0.023075867][-0.023122421 -0.023116346 -0.023110349 -0.023103571 -0.023099378 -0.02309157 -0.023094775 -0.023100587 -0.023108393 -0.023113638 -0.023117904 -0.023124492 -0.023118095 -0.023106851 -0.023098012][-0.023127096 -0.02312574 -0.023125274 -0.023122981 -0.023121165 -0.023117986 -0.023117935 -0.0231191 -0.023121521 -0.023123557 -0.02312416 -0.023126137 -0.02312443 -0.023118835 -0.023111662][-0.023129724 -0.023129432 -0.0231288 -0.02312878 -0.02312878 -0.023128754 -0.023128744 -0.023128744 -0.023129368 -0.023129376 -0.023129372 -0.02312935 -0.023128631 -0.023127997 -0.023126669]]...]
INFO - root - 2017-12-09 05:43:08.414383: step 1410, loss = 1.18, batch loss = 0.69 (38.0 examples/sec; 0.211 sec/batch; 19h:22m:48s remains)
INFO - root - 2017-12-09 05:43:10.554750: step 1420, loss = 1.18, batch loss = 0.69 (37.8 examples/sec; 0.212 sec/batch; 19h:28m:47s remains)
INFO - root - 2017-12-09 05:43:12.715654: step 1430, loss = 1.18, batch loss = 0.69 (37.8 examples/sec; 0.212 sec/batch; 19h:28m:03s remains)
INFO - root - 2017-12-09 05:43:14.868209: step 1440, loss = 1.18, batch loss = 0.69 (37.2 examples/sec; 0.215 sec/batch; 19h:46m:10s remains)
INFO - root - 2017-12-09 05:43:17.011452: step 1450, loss = 1.18, batch loss = 0.69 (36.9 examples/sec; 0.217 sec/batch; 19h:57m:47s remains)
INFO - root - 2017-12-09 05:43:19.154799: step 1460, loss = 1.18, batch loss = 0.69 (36.5 examples/sec; 0.219 sec/batch; 20h:08m:27s remains)
INFO - root - 2017-12-09 05:43:21.318775: step 1470, loss = 1.20, batch loss = 0.71 (34.0 examples/sec; 0.236 sec/batch; 21h:39m:34s remains)
INFO - root - 2017-12-09 05:43:23.449560: step 1480, loss = 1.19, batch loss = 0.69 (37.5 examples/sec; 0.214 sec/batch; 19h:38m:05s remains)
INFO - root - 2017-12-09 05:43:25.617482: step 1490, loss = 1.19, batch loss = 0.69 (36.6 examples/sec; 0.219 sec/batch; 20h:06m:16s remains)
INFO - root - 2017-12-09 05:43:27.787523: step 1500, loss = 1.19, batch loss = 0.69 (36.7 examples/sec; 0.218 sec/batch; 20h:02m:20s remains)
2017-12-09 05:43:28.170737: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.032100085 -0.032149345 -0.032198798 -0.03219182 -0.0318093 0.0061981082 0.052549113 0.059542079 0.03911842 -0.028653275 -0.031766243 -0.031808894 -0.031941876 -0.028821161 -0.031108085][-0.02782337 -0.029259531 -0.02873949 -0.026776362 -0.028738001 -0.027012546 -0.027107561 -0.022646844 -0.0079491287 0.057197649 0.055524852 0.0283041 -0.00056249276 -0.030673411 -0.03180949][-0.00077527389 -0.007740194 -0.0167841 -0.017633788 -0.017834064 -0.019579072 -0.010709142 0.0019277893 0.0080231354 0.011176281 0.0078128055 0.0023231767 0.00515189 0.019214038 -0.0044182204][0.0031736158 -0.0099245757 -0.0029711965 -0.013423612 -0.011779986 -0.013224343 -0.014092064 -0.015148301 -0.014193431 -0.016289914 -0.015933145 -0.013044965 0.000948932 -0.013117099 -0.0043312386][-0.0069269054 -0.009254124 0.0055781119 -0.016405292 -0.014682863 -0.013793195 -0.011783199 -0.0093764346 0.0097089671 -0.012550352 -0.012272106 -0.015667437 -0.016710306 -0.014180124 -0.013587384][-0.014217721 -0.031754784 -0.016989773 -0.032093838 -0.018099252 -0.032295272 -0.032392167 -0.032381855 -0.032041986 -0.020814439 -0.011512075 -0.032378577 -0.026223363 -0.03253163 -0.032559451][-0.014795432 -0.016911482 -0.0089509487 -0.032026462 -0.032168373 -0.032260165 -0.032341626 -0.032341283 -0.032336503 -0.03198823 -0.013573827 -0.013826685 -0.012467904 -0.026416088 -0.032571062][-0.013781447 -0.031348519 -0.011413166 -0.032002922 -0.032133788 -0.032224681 -0.032292604 -0.032298144 -0.0323034 -0.032396492 -0.032460582 -0.032389905 -0.018873267 -0.032576907 -0.032609142][0.022277806 0.016448393 0.0089426 -0.031991217 -0.03211366 -0.032210656 -0.032248028 -0.03225356 -0.03226012 -0.032365069 -0.032454293 -0.032494806 -0.032584816 -0.032583352 -0.032621343][0.02921851 0.0065289252 -0.0016562175 -0.032045148 0.00066243112 0.011532273 -0.0015124828 -0.032239377 -0.032246154 -0.032351207 -0.032441989 -0.032483492 -0.0325821 -0.032582056 -0.032620206][0.0026984327 -0.018909387 -0.0040204935 -0.032203391 -0.031255856 -0.0027077384 -0.0054229181 -0.00064464286 0.011166241 -0.0323599 -0.03243611 -0.032478657 -0.032579567 -0.032580744 -0.032619365][0.010784276 -0.011129601 -0.0053749736 -0.032295473 -0.024453185 -0.021595962 -0.022374978 -0.026383042 -0.0028245822 -0.03244739 -0.0028489344 -0.032516018 -0.03258444 -0.032584179 -0.032621872][-0.0146717 -0.013888452 -0.01640331 -0.02657295 -0.032432739 -0.018188443 -0.018345948 -0.026524518 -0.023060743 -0.032519788 -0.025973914 -0.032572445 -0.030873474 -0.032611698 -0.032622367][-0.011651803 -0.025726732 -0.031824954 -0.032478467 -0.027030444 -0.020098381 -0.018689 -0.032524787 -0.018015504 -0.032555334 -0.028612368 -0.03258957 -0.032620065 -0.03261961 -0.032624152][-0.012141729 -0.010861268 -0.013827454 -0.021097165 -0.032537229 -0.032558236 -0.029984079 -0.02978155 -0.023298284 -0.032586992 -0.032603629 -0.032608174 -0.032623995 -0.032623615 -0.032624424]]...]
INFO - root - 2017-12-09 05:43:30.304295: step 1510, loss = 1.19, batch loss = 0.69 (37.1 examples/sec; 0.216 sec/batch; 19h:49m:51s remains)
INFO - root - 2017-12-09 05:43:32.497670: step 1520, loss = 1.19, batch loss = 0.69 (37.9 examples/sec; 0.211 sec/batch; 19h:24m:54s remains)
INFO - root - 2017-12-09 05:43:34.643380: step 1530, loss = 1.19, batch loss = 0.69 (36.7 examples/sec; 0.218 sec/batch; 20h:03m:25s remains)
INFO - root - 2017-12-09 05:43:36.799234: step 1540, loss = 1.20, batch loss = 0.69 (38.0 examples/sec; 0.211 sec/batch; 19h:21m:58s remains)
INFO - root - 2017-12-09 05:43:38.935335: step 1550, loss = 1.20, batch loss = 0.69 (37.3 examples/sec; 0.215 sec/batch; 19h:43m:30s remains)
INFO - root - 2017-12-09 05:43:41.086197: step 1560, loss = 1.19, batch loss = 0.68 (36.6 examples/sec; 0.218 sec/batch; 20h:04m:45s remains)
INFO - root - 2017-12-09 05:43:43.315705: step 1570, loss = 1.20, batch loss = 0.68 (37.0 examples/sec; 0.216 sec/batch; 19h:53m:00s remains)
INFO - root - 2017-12-09 05:43:45.511547: step 1580, loss = 1.21, batch loss = 0.69 (36.0 examples/sec; 0.222 sec/batch; 20h:25m:29s remains)
INFO - root - 2017-12-09 05:43:47.656461: step 1590, loss = 1.22, batch loss = 0.70 (36.1 examples/sec; 0.222 sec/batch; 20h:22m:02s remains)
INFO - root - 2017-12-09 05:43:49.794824: step 1600, loss = 1.20, batch loss = 0.68 (37.7 examples/sec; 0.212 sec/batch; 19h:31m:14s remains)
2017-12-09 05:43:50.175219: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0359007 -0.035900686 -0.03590063 -0.035900548 -0.035900462 -0.03590038 -0.035900313 -0.035900284 -0.035900265 -0.035900243 -0.035900243 -0.035900325 -0.035900414 -0.035900515 -0.035900619][-0.035900831 -0.035900816 -0.035900764 -0.035900682 -0.035900597 -0.035900503 -0.035900429 -0.035900392 -0.035900366 -0.035900343 -0.035900339 -0.035900418 -0.035900503 -0.035900608 -0.035900719][-0.035900805 -0.035900827 -0.03590082 -0.035900805 -0.035900787 -0.03590076 -0.035900738 -0.035900727 -0.035900723 -0.035900719 -0.035900719 -0.035900727 -0.035900742 -0.035900768 -0.0359008][-0.035900716 -0.035900831 -0.035900831 -0.035900831 -0.035900831 -0.035900831 -0.035900831 -0.035900831 -0.035900831 -0.035900831 -0.035900831 -0.035900831 -0.035900831 -0.035900824 -0.035900798][-0.035899807 -0.035899907 -0.035899825 -0.035899702 -0.035899684 -0.03589981 -0.035900023 -0.035900239 -0.03590047 -0.035900604 -0.035900593 -0.035900433 -0.035900269 -0.035900183 -0.035900194][-0.035897441 -0.03589613 -0.035894822 -0.035893805 -0.03589325 -0.035893179 -0.035893552 -0.035894219 -0.035895128 -0.035895817 -0.03589613 -0.035896156 -0.035896361 -0.035896841 -0.035897613][-0.035894245 -0.035891037 -0.035887349 -0.035884116 -0.035881829 -0.035880819 -0.03588105 -0.035882119 -0.035883714 -0.035884902 -0.035885666 -0.035886463 -0.035887923 -0.035890084 -0.035892844][-0.035893429 -0.0358896 -0.035884146 -0.035878912 -0.03587465 -0.035871904 -0.035870966 -0.035871372 -0.035872452 -0.035873126 -0.035873782 -0.035875261 -0.035878338 -0.035882838 -0.035888188][-0.035895646 -0.035893664 -0.035889339 -0.035884637 -0.035880245 -0.035876848 -0.035874806 -0.035873547 -0.035872385 -0.03587104 -0.03587053 -0.035871778 -0.035875458 -0.035881 -0.035887539][-0.035898007 -0.035898462 -0.035896637 -0.035894416 -0.03589198 -0.035889562 -0.035887338 -0.035884678 -0.035881355 -0.035877988 -0.0358761 -0.035876684 -0.03587985 -0.035884902 -0.035890821][-0.035898555 -0.035900082 -0.035899784 -0.0358994 -0.035898779 -0.035897989 -0.035896827 -0.035894375 -0.035890933 -0.035887446 -0.035885338 -0.035885483 -0.035887714 -0.03589135 -0.035895485][-0.035898138 -0.035899535 -0.035899445 -0.035899535 -0.035899617 -0.035899915 -0.035900004 -0.035898887 -0.03589702 -0.035895124 -0.035893947 -0.035893943 -0.035895031 -0.035896838 -0.03589879][-0.0358965 -0.035897639 -0.035897363 -0.035897616 -0.035898194 -0.035899282 -0.035900284 -0.035900362 -0.035899881 -0.035899386 -0.035899047 -0.035898868 -0.035899032 -0.035899378 -0.035899766][-0.035894237 -0.035894748 -0.035893954 -0.035894252 -0.03589537 -0.035897139 -0.035898644 -0.035899352 -0.035899475 -0.035899431 -0.035899356 -0.035899229 -0.035899144 -0.035899039 -0.035899043][-0.035892904 -0.035893008 -0.035891693 -0.035891712 -0.035892859 -0.035894711 -0.035896264 -0.035897169 -0.035897512 -0.035897627 -0.035897665 -0.035897683 -0.035897762 -0.035897944 -0.03589832]]...]
INFO - root - 2017-12-09 05:43:52.313076: step 1610, loss = 1.21, batch loss = 0.69 (37.8 examples/sec; 0.212 sec/batch; 19h:28m:37s remains)
INFO - root - 2017-12-09 05:43:54.496085: step 1620, loss = 1.21, batch loss = 0.69 (38.5 examples/sec; 0.208 sec/batch; 19h:04m:35s remains)
INFO - root - 2017-12-09 05:43:56.654792: step 1630, loss = 1.18, batch loss = 0.66 (36.6 examples/sec; 0.219 sec/batch; 20h:05m:08s remains)
INFO - root - 2017-12-09 05:43:58.801848: step 1640, loss = 1.21, batch loss = 0.68 (37.7 examples/sec; 0.212 sec/batch; 19h:29m:05s remains)
INFO - root - 2017-12-09 05:44:00.986503: step 1650, loss = 1.22, batch loss = 0.69 (34.7 examples/sec; 0.230 sec/batch; 21h:10m:07s remains)
INFO - root - 2017-12-09 05:44:03.157329: step 1660, loss = 1.22, batch loss = 0.69 (35.9 examples/sec; 0.223 sec/batch; 20h:28m:46s remains)
INFO - root - 2017-12-09 05:44:05.314059: step 1670, loss = 1.21, batch loss = 0.68 (37.3 examples/sec; 0.215 sec/batch; 19h:43m:40s remains)
INFO - root - 2017-12-09 05:44:07.460588: step 1680, loss = 1.23, batch loss = 0.70 (36.8 examples/sec; 0.218 sec/batch; 19h:59m:46s remains)
INFO - root - 2017-12-09 05:44:09.632332: step 1690, loss = 1.22, batch loss = 0.69 (37.6 examples/sec; 0.213 sec/batch; 19h:33m:13s remains)
INFO - root - 2017-12-09 05:44:11.794190: step 1700, loss = 1.21, batch loss = 0.67 (36.3 examples/sec; 0.220 sec/batch; 20h:15m:39s remains)
2017-12-09 05:44:12.106067: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.079078183 0.080666825 0.081402868 0.082354724 0.084664494 0.08951284 0.097849593 0.10998134 0.12459515 0.14011291 0.15537624 0.16889171 0.17920473 0.18515444 0.18708529][0.11531217 0.11672701 0.11668767 0.11671177 0.11794181 0.12156849 0.12833077 0.13822725 0.15001297 0.16243604 0.1742979 0.18429042 0.19095476 0.19382957 0.19384503][0.14408538 0.14592904 0.14554536 0.14491494 0.1451156 0.14757626 0.15297322 0.16082691 0.16966049 0.17849287 0.18621004 0.1914548 0.19330895 0.19231029 0.19009677][0.16674367 0.16956341 0.16914082 0.16804005 0.16718505 0.16835518 0.17226569 0.17806296 0.18413709 0.18987043 0.19431368 0.1960521 0.19453245 0.19097982 0.18742855][0.18016417 0.18427236 0.18398519 0.18279095 0.18140279 0.18181267 0.18491659 0.18938191 0.19355711 0.19721483 0.1998679 0.19992116 0.19665734 0.19161655 0.18716466][0.18323581 0.18889238 0.18881518 0.18787745 0.1867539 0.1875028 0.19090611 0.19517539 0.19859816 0.20138164 0.20347387 0.20312862 0.19930317 0.19352645 0.18846439][0.18230654 0.18990634 0.19010824 0.18954997 0.18894659 0.19023629 0.1939228 0.19798037 0.20076714 0.20296921 0.20484038 0.20457137 0.20081134 0.19493069 0.18971331][0.18142101 0.19111285 0.19157842 0.19132952 0.19102503 0.19238885 0.19572109 0.19911458 0.20116399 0.20277113 0.20435973 0.20419647 0.20081832 0.19535299 0.19044816][0.18103229 0.19247755 0.19311306 0.19309698 0.19293272 0.1939875 0.1965017 0.19901465 0.20043869 0.20155065 0.20268439 0.20241052 0.19951592 0.19490151 0.19078282][0.18085472 0.19350106 0.19411817 0.19423278 0.19416396 0.19480875 0.19637454 0.19796553 0.19884573 0.19945239 0.19990753 0.19929999 0.19695053 0.19357134 0.19067281][0.18070064 0.19403069 0.19438992 0.19441335 0.19426472 0.19442293 0.19503121 0.19572297 0.19608796 0.19622296 0.19610506 0.19533527 0.19375081 0.19179153 0.19021621][0.17995204 0.19360825 0.19363323 0.19342901 0.19309981 0.19284838 0.19275808 0.19277821 0.19274023 0.19262561 0.19235107 0.19181958 0.19109194 0.19033703 0.18979505][0.17869146 0.19233434 0.19212747 0.19177765 0.19135796 0.19094786 0.19059131 0.19034347 0.19018237 0.19005907 0.18992394 0.18977813 0.18969262 0.1896722 0.18968667][0.17741276 0.19091259 0.19066118 0.19032876 0.18997027 0.18961367 0.18928106 0.18904205 0.18892609 0.18891245 0.1889638 0.18908867 0.1893215 0.18960455 0.18983647][0.1764324 0.18992537 0.1897741 0.1895714 0.18935083 0.18912853 0.18892057 0.18878278 0.18875743 0.18883729 0.18898571 0.18920256 0.18950509 0.18984075 0.19010562]]...]
INFO - root - 2017-12-09 05:44:14.293017: step 1710, loss = 1.24, batch loss = 0.70 (37.5 examples/sec; 0.213 sec/batch; 19h:35m:46s remains)
INFO - root - 2017-12-09 05:44:16.467314: step 1720, loss = 1.23, batch loss = 0.69 (36.1 examples/sec; 0.222 sec/batch; 20h:22m:19s remains)
INFO - root - 2017-12-09 05:44:18.644685: step 1730, loss = 1.23, batch loss = 0.69 (37.9 examples/sec; 0.211 sec/batch; 19h:25m:03s remains)
INFO - root - 2017-12-09 05:44:20.794558: step 1740, loss = 1.24, batch loss = 0.70 (38.3 examples/sec; 0.209 sec/batch; 19h:10m:24s remains)
INFO - root - 2017-12-09 05:44:23.008652: step 1750, loss = 1.24, batch loss = 0.70 (36.6 examples/sec; 0.219 sec/batch; 20h:05m:52s remains)
INFO - root - 2017-12-09 05:44:25.196957: step 1760, loss = 1.23, batch loss = 0.68 (35.7 examples/sec; 0.224 sec/batch; 20h:35m:18s remains)
INFO - root - 2017-12-09 05:44:27.371530: step 1770, loss = 1.24, batch loss = 0.69 (37.9 examples/sec; 0.211 sec/batch; 19h:23m:44s remains)
INFO - root - 2017-12-09 05:44:29.538685: step 1780, loss = 1.24, batch loss = 0.69 (37.0 examples/sec; 0.216 sec/batch; 19h:51m:49s remains)
INFO - root - 2017-12-09 05:44:31.704825: step 1790, loss = 1.24, batch loss = 0.69 (36.9 examples/sec; 0.217 sec/batch; 19h:55m:33s remains)
INFO - root - 2017-12-09 05:44:33.838280: step 1800, loss = 1.24, batch loss = 0.69 (37.4 examples/sec; 0.214 sec/batch; 19h:39m:04s remains)
2017-12-09 05:44:34.256338: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.036966041 -0.037061121 -0.037181936 -0.037231252 -0.037256751 -0.037230525 -0.037182052 -0.037135586 -0.037137128 -0.037101734 -0.037196271 -0.037127197 -0.037103839 -0.037071403 -0.037000969][-0.036826521 -0.036970142 -0.037137225 -0.037232351 -0.0372579 -0.037231755 -0.037183359 -0.037136957 -0.037138529 -0.037103087 -0.037197534 -0.037105475 -0.037077438 -0.037044987 -0.036974587][-0.036826078 -0.036970202 -0.037137289 -0.037232943 -0.037258506 -0.037232384 -0.037184015 -0.037137639 -0.037139233 -0.037148431 -0.037242856 -0.037195921 -0.037213009 -0.037180532 -0.037110109][-0.037036929 -0.037049178 -0.037080314 -0.03710819 -0.03716604 -0.037294228 -0.037269082 -0.037246954 -0.037260372 -0.037284013 -0.037346154 -0.037380856 -0.037484586 -0.037481714 -0.037470147][-0.037131164 -0.0370414 -0.036922246 -0.036904402 -0.036902614 -0.037121024 -0.03725319 -0.037333276 -0.037441641 -0.037448447 -0.037509836 -0.037492771 -0.037536222 -0.037542097 -0.037592232][-0.03731567 -0.037286106 -0.037223782 -0.037269577 -0.037269566 -0.037488524 -0.037614893 -0.037694342 -0.037802991 -0.037803017 -0.037803046 -0.037780177 -0.03777552 -0.037775505 -0.037775494][-0.037280563 -0.037240222 -0.037161898 -0.037207697 -0.037207697 -0.037442982 -0.037586175 -0.03767835 -0.037802991 -0.037803013 -0.037803046 -0.037780177 -0.03777552 -0.037775498 -0.037775483][-0.037408467 -0.037324429 -0.037199713 -0.037205122 -0.037205108 -0.037442531 -0.0375859 -0.037678272 -0.037802979 -0.037802994 -0.037803032 -0.037803032 -0.03780302 -0.037802994 -0.037802976][-0.037375439 -0.037293866 -0.037158772 -0.037168056 -0.037167694 -0.037417013 -0.037574265 -0.03766695 -0.037802104 -0.037802462 -0.037802778 -0.037802652 -0.037802253 -0.037802 -0.03780194][-0.037468072 -0.037398685 -0.03728351 -0.037303243 -0.037307348 -0.03744667 -0.037582941 -0.037676565 -0.037800923 -0.037801776 -0.037802331 -0.037802149 -0.0378015 -0.037800953 -0.037800811][-0.03765719 -0.037664924 -0.037644994 -0.037664991 -0.037669398 -0.037708133 -0.037754189 -0.03777124 -0.037800811 -0.037801679 -0.037802305 -0.037802294 -0.037801962 -0.037801571 -0.037801381][-0.037659988 -0.037670296 -0.037652873 -0.037673261 -0.037677854 -0.037714735 -0.037760455 -0.037774589 -0.037801865 -0.037801705 -0.037801266 -0.037801608 -0.037801638 -0.037801456 -0.037801381][-0.037696302 -0.037717067 -0.037715897 -0.037736531 -0.037741017 -0.037761107 -0.037789665 -0.037790727 -0.037801679 -0.0378016 -0.037801012 -0.037801947 -0.037802089 -0.037802018 -0.037802055][-0.037710313 -0.037726048 -0.037724752 -0.037739903 -0.037744127 -0.037761841 -0.037789896 -0.037790548 -0.037801325 -0.037801139 -0.03780061 -0.037801735 -0.037801735 -0.037801594 -0.037801716][-0.037743036 -0.037756253 -0.037765037 -0.037775833 -0.037779879 -0.03778534 -0.03779937 -0.037799597 -0.037800077 -0.037799791 -0.037799533 -0.037800748 -0.037800651 -0.037800413 -0.0378006]]...]
INFO - root - 2017-12-09 05:44:36.409612: step 1810, loss = 1.24, batch loss = 0.69 (36.3 examples/sec; 0.220 sec/batch; 20h:14m:38s remains)
INFO - root - 2017-12-09 05:44:38.571846: step 1820, loss = 1.24, batch loss = 0.69 (38.6 examples/sec; 0.207 sec/batch; 19h:02m:32s remains)
INFO - root - 2017-12-09 05:44:40.733928: step 1830, loss = 1.24, batch loss = 0.69 (37.1 examples/sec; 0.215 sec/batch; 19h:46m:54s remains)
INFO - root - 2017-12-09 05:44:42.947423: step 1840, loss = 1.25, batch loss = 0.69 (34.7 examples/sec; 0.231 sec/batch; 21h:11m:03s remains)
INFO - root - 2017-12-09 05:44:45.130407: step 1850, loss = 1.25, batch loss = 0.69 (38.0 examples/sec; 0.210 sec/batch; 19h:19m:17s remains)
INFO - root - 2017-12-09 05:44:47.330876: step 1860, loss = 1.25, batch loss = 0.69 (35.8 examples/sec; 0.223 sec/batch; 20h:30m:35s remains)
INFO - root - 2017-12-09 05:44:49.477436: step 1870, loss = 1.25, batch loss = 0.69 (37.6 examples/sec; 0.213 sec/batch; 19h:31m:24s remains)
INFO - root - 2017-12-09 05:44:51.610296: step 1880, loss = 1.25, batch loss = 0.68 (37.4 examples/sec; 0.214 sec/batch; 19h:38m:49s remains)
INFO - root - 2017-12-09 05:44:53.773889: step 1890, loss = 1.26, batch loss = 0.69 (37.6 examples/sec; 0.213 sec/batch; 19h:31m:20s remains)
INFO - root - 2017-12-09 05:44:55.921512: step 1900, loss = 1.26, batch loss = 0.69 (36.4 examples/sec; 0.220 sec/batch; 20h:11m:58s remains)
2017-12-09 05:44:56.330960: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.033360876 -0.033357322 -0.033356916 -0.033357311 -0.033358306 -0.033359494 -0.033360314 -0.03336044 -0.033360116 -0.033359893 -0.03336022 -0.033360947 -0.033361796 -0.03336243 -0.033362765][-0.033365853 -0.033358127 -0.033359386 -0.033361256 -0.033363245 -0.03336481 -0.033365425 -0.033364996 -0.0333641 -0.033363603 -0.0333641 -0.03336538 -0.033366855 -0.033367917 -0.0333683][-0.033402324 -0.03338442 -0.033388022 -0.03339174 -0.033394825 -0.033396684 -0.033397097 -0.033396415 -0.033395566 -0.03339551 -0.03339678 -0.033398967 -0.033401128 -0.033402406 -0.033402443][-0.033438768 -0.033411939 -0.033418112 -0.033423647 -0.033427548 -0.033429459 -0.0334298 -0.033429462 -0.033429354 -0.033430263 -0.033432607 -0.033435665 -0.033437993 -0.033438675 -0.033437498][-0.033477794 -0.033441722 -0.033450525 -0.033457834 -0.033462487 -0.033464607 -0.033465128 -0.033465225 -0.033465654 -0.033467069 -0.033469759 -0.033472672 -0.033474144 -0.033473182 -0.033469718][-0.033523969 -0.033471968 -0.033483226 -0.033492584 -0.033498526 -0.033501204 -0.033501919 -0.033501823 -0.03350132 -0.033501022 -0.033501524 -0.03350208 -0.033500984 -0.033497334 -0.033491433][-0.03356285 -0.03349676 -0.033509534 -0.033520766 -0.033528719 -0.033533193 -0.033534985 -0.033534706 -0.033532351 -0.033528309 -0.033523798 -0.033519033 -0.033513397 -0.033507008 -0.033500567][-0.033596624 -0.03351282 -0.033526473 -0.033539649 -0.033550445 -0.033557862 -0.033561505 -0.033560988 -0.033555623 -0.033545874 -0.033533771 -0.033521086 -0.033509385 -0.033500321 -0.033494975][-0.03361227 -0.033517789 -0.033532042 -0.033547405 -0.033561636 -0.033572651 -0.033578318 -0.03357676 -0.03356678 -0.033549424 -0.033527993 -0.033506211 -0.033487733 -0.033475731 -0.033471011][-0.033619076 -0.033513334 -0.033528429 -0.033546295 -0.033564247 -0.033578746 -0.033585656 -0.033581764 -0.03356583 -0.033540126 -0.033509549 -0.033479571 -0.033455428 -0.033440895 -0.0334365][-0.0336188 -0.033501059 -0.033516258 -0.033535797 -0.033556528 -0.033573348 -0.03358046 -0.033573572 -0.033551767 -0.033518661 -0.033480711 -0.033444926 -0.033417314 -0.03340197 -0.03339918][-0.033604272 -0.033480488 -0.033494126 -0.03351314 -0.03353446 -0.033552181 -0.033559196 -0.033550598 -0.033525806 -0.033489317 -0.033448353 -0.033410557 -0.03338249 -0.033368383 -0.033368163][-0.033588991 -0.033456065 -0.033466406 -0.033482756 -0.033502195 -0.033518642 -0.033524852 -0.033515774 -0.03349093 -0.033455282 -0.033416141 -0.033380993 -0.033355765 -0.033343893 -0.033345178][-0.033572074 -0.033435039 -0.033441503 -0.033453688 -0.033469111 -0.033482328 -0.033486731 -0.033477649 -0.033454925 -0.033423509 -0.03339028 -0.033361625 -0.033341605 -0.033332378 -0.033333685][-0.033561479 -0.033421 -0.033423472 -0.033431008 -0.033441633 -0.03345082 -0.033453025 -0.033444319 -0.033424735 -0.033398908 -0.033372961 -0.033351783 -0.033337831 -0.033332091 -0.03333468]]...]
INFO - root - 2017-12-09 05:44:58.494554: step 1910, loss = 1.24, batch loss = 0.68 (35.7 examples/sec; 0.224 sec/batch; 20h:35m:47s remains)
INFO - root - 2017-12-09 05:45:00.683442: step 1920, loss = 1.26, batch loss = 0.69 (38.0 examples/sec; 0.211 sec/batch; 19h:21m:12s remains)
INFO - root - 2017-12-09 05:45:02.871209: step 1930, loss = 1.26, batch loss = 0.69 (36.0 examples/sec; 0.222 sec/batch; 20h:24m:50s remains)
INFO - root - 2017-12-09 05:45:05.028436: step 1940, loss = 1.27, batch loss = 0.70 (37.9 examples/sec; 0.211 sec/batch; 19h:23m:33s remains)
INFO - root - 2017-12-09 05:45:07.189177: step 1950, loss = 1.24, batch loss = 0.67 (37.1 examples/sec; 0.216 sec/batch; 19h:48m:52s remains)
INFO - root - 2017-12-09 05:45:09.345698: step 1960, loss = 1.26, batch loss = 0.69 (35.2 examples/sec; 0.227 sec/batch; 20h:50m:50s remains)
INFO - root - 2017-12-09 05:45:11.535629: step 1970, loss = 1.26, batch loss = 0.69 (38.2 examples/sec; 0.210 sec/batch; 19h:14m:21s remains)
INFO - root - 2017-12-09 05:45:13.702916: step 1980, loss = 1.26, batch loss = 0.69 (36.3 examples/sec; 0.220 sec/batch; 20h:13m:48s remains)
INFO - root - 2017-12-09 05:45:15.884418: step 1990, loss = 1.27, batch loss = 0.69 (38.2 examples/sec; 0.209 sec/batch; 19h:12m:58s remains)
INFO - root - 2017-12-09 05:45:18.014813: step 2000, loss = 1.27, batch loss = 0.69 (36.7 examples/sec; 0.218 sec/batch; 19h:59m:36s remains)
2017-12-09 05:45:18.406095: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.41685939 0.41976029 0.42126173 0.42230982 0.42283326 0.42292327 0.42277884 0.42257667 0.42235267 0.42203361 0.42151493 0.42076266 0.41990471 0.41900891 0.41816568][0.43340552 0.43685609 0.43842703 0.43955857 0.440095 0.44012916 0.43987614 0.43954402 0.43918997 0.43874973 0.4381308 0.43734032 0.43656719 0.43591744 0.4353857][0.43209791 0.43665165 0.43813854 0.43921649 0.43971246 0.43971068 0.43940681 0.43900162 0.43856221 0.43804467 0.43738675 0.43662995 0.43598986 0.43559551 0.43534929][0.43022436 0.43637365 0.43758738 0.43846291 0.43886685 0.43886107 0.43858325 0.43818676 0.43773335 0.43720961 0.43658942 0.43593639 0.43545741 0.43527478 0.43523586][0.42791474 0.43605757 0.43682677 0.43736786 0.43762511 0.43764335 0.4374761 0.43720019 0.43685025 0.43641651 0.4358899 0.43534946 0.43500775 0.4349981 0.43514752][0.42508858 0.43568933 0.43595254 0.43612355 0.43623066 0.43630767 0.43633819 0.43631017 0.43619007 0.43592966 0.43551391 0.43504578 0.43476576 0.43483835 0.4351272][0.42214972 0.435256 0.43511337 0.4349854 0.43497705 0.43512458 0.43536586 0.43561471 0.43575782 0.43569034 0.43539602 0.43501496 0.43481457 0.43498003 0.435418][0.41905379 0.43487126 0.4345234 0.43423706 0.43415821 0.43432933 0.43468863 0.43511182 0.43542928 0.43550187 0.4353379 0.43511391 0.4350931 0.43544012 0.43606597][0.41653776 0.43475628 0.43440473 0.43408519 0.43395591 0.43407595 0.43441164 0.43484479 0.43519437 0.43532741 0.43528849 0.43527067 0.43549764 0.43607098 0.43686908][0.41463244 0.4349578 0.43474084 0.43449306 0.43435317 0.4343946 0.434627 0.43495977 0.43522877 0.43533784 0.43537986 0.43554097 0.43597376 0.43670642 0.4375757][0.41345751 0.43550313 0.43550861 0.43541497 0.43531507 0.43528831 0.43538332 0.43554151 0.43563622 0.43563879 0.43569309 0.43595749 0.43650007 0.43727928 0.4381122][0.41309571 0.43633163 0.43658298 0.43665528 0.4365924 0.43647802 0.43639594 0.43632281 0.43618941 0.43604624 0.43608028 0.43640143 0.43698305 0.437725 0.43845075][0.41318339 0.43710983 0.4375453 0.43771148 0.4376204 0.43736207 0.43704289 0.43667823 0.43627977 0.435982 0.43598527 0.43633389 0.43691444 0.4375847 0.43818128][0.41303784 0.43695682 0.43736964 0.43741781 0.43709373 0.43647391 0.43570697 0.43488747 0.43414414 0.43369019 0.43371052 0.43417907 0.43488544 0.43562675 0.4362222][0.41076928 0.43427795 0.43433559 0.4339723 0.43312258 0.43182141 0.43030649 0.42879778 0.4275952 0.42698485 0.42713755 0.4279533 0.42909521 0.43024087 0.43110883]]...]
INFO - root - 2017-12-09 05:45:20.555397: step 2010, loss = 1.27, batch loss = 0.69 (36.9 examples/sec; 0.217 sec/batch; 19h:54m:40s remains)
INFO - root - 2017-12-09 05:45:22.728025: step 2020, loss = 1.28, batch loss = 0.69 (35.7 examples/sec; 0.224 sec/batch; 20h:33m:20s remains)
INFO - root - 2017-12-09 05:45:24.924808: step 2030, loss = 1.27, batch loss = 0.68 (38.0 examples/sec; 0.210 sec/batch; 19h:18m:29s remains)
INFO - root - 2017-12-09 05:45:27.112952: step 2040, loss = 1.28, batch loss = 0.69 (36.2 examples/sec; 0.221 sec/batch; 20h:18m:26s remains)
INFO - root - 2017-12-09 05:45:29.310560: step 2050, loss = 1.28, batch loss = 0.69 (33.8 examples/sec; 0.237 sec/batch; 21h:45m:21s remains)
INFO - root - 2017-12-09 05:45:31.513366: step 2060, loss = 1.28, batch loss = 0.69 (36.0 examples/sec; 0.222 sec/batch; 20h:22m:55s remains)
INFO - root - 2017-12-09 05:45:33.671211: step 2070, loss = 1.28, batch loss = 0.69 (37.0 examples/sec; 0.216 sec/batch; 19h:51m:18s remains)
INFO - root - 2017-12-09 05:45:35.835934: step 2080, loss = 1.28, batch loss = 0.69 (37.2 examples/sec; 0.215 sec/batch; 19h:43m:43s remains)
INFO - root - 2017-12-09 05:45:38.036194: step 2090, loss = 1.27, batch loss = 0.68 (34.2 examples/sec; 0.234 sec/batch; 21h:28m:04s remains)
INFO - root - 2017-12-09 05:45:40.209236: step 2100, loss = 1.28, batch loss = 0.69 (36.7 examples/sec; 0.218 sec/batch; 20h:01m:30s remains)
2017-12-09 05:45:40.606408: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0371673 -0.037167531 -0.037167657 -0.03716778 -0.037167862 -0.03716791 -0.037167933 -0.037167944 -0.037167944 -0.037167925 -0.037167877 -0.037167795 -0.037167683 -0.037167568 -0.037167467][-0.037173226 -0.03717386 -0.037173897 -0.037173953 -0.037174016 -0.037174072 -0.037174113 -0.037174147 -0.037174162 -0.037174162 -0.037174154 -0.037174147 -0.037174135 -0.037174121 -0.037174109][-0.037173197 -0.037174053 -0.037174072 -0.037174091 -0.037174106 -0.037174124 -0.037174139 -0.037174117 -0.0371741 -0.037174109 -0.037174143 -0.037174139 -0.037174121 -0.037174117 -0.037174121][-0.037173267 -0.037174076 -0.0371741 -0.037174124 -0.037174143 -0.03717415 -0.037174128 -0.03717405 -0.037173994 -0.037174005 -0.037174083 -0.0371741 -0.037174091 -0.037174102 -0.037174128][-0.037173219 -0.037174083 -0.037174109 -0.037174139 -0.037174165 -0.037174173 -0.037174132 -0.037174065 -0.037174009 -0.037173986 -0.037174035 -0.037174068 -0.037174083 -0.037174102 -0.037174117][-0.037173465 -0.037174091 -0.037174109 -0.037174135 -0.037174158 -0.037174169 -0.037174158 -0.037174139 -0.037174135 -0.037174117 -0.037174121 -0.037174124 -0.037174117 -0.037174113 -0.037174109][-0.037172917 -0.037174091 -0.037174106 -0.037174128 -0.037174147 -0.03717415 -0.037174154 -0.037174169 -0.0371742 -0.037174221 -0.037174214 -0.037174176 -0.037174143 -0.037174121 -0.037174109][-0.03717329 -0.037174087 -0.037174057 -0.037173998 -0.037173931 -0.0371739 -0.037173923 -0.037173945 -0.037174005 -0.037174106 -0.03717418 -0.037174165 -0.037174124 -0.037174102 -0.037174109][-0.037172697 -0.037174065 -0.037173998 -0.037173878 -0.037173741 -0.037173644 -0.037173573 -0.037173573 -0.037173651 -0.037173808 -0.037173975 -0.037174061 -0.037174087 -0.037174102 -0.037174113][-0.037173092 -0.037174094 -0.037174065 -0.037173998 -0.037173897 -0.037173811 -0.037173759 -0.037173752 -0.0371738 -0.037173882 -0.037174005 -0.037174091 -0.037174117 -0.037174124 -0.037174121][-0.037173163 -0.037174106 -0.037174102 -0.0371741 -0.037174091 -0.037174083 -0.037174072 -0.037174065 -0.037174061 -0.037174068 -0.0371741 -0.037174121 -0.037174124 -0.037174128 -0.037174128][-0.037173383 -0.037174113 -0.037174109 -0.037174106 -0.037174102 -0.037174091 -0.037174091 -0.0371741 -0.037174094 -0.0371741 -0.037174113 -0.037174128 -0.037174135 -0.037174135 -0.037174132][-0.037173241 -0.037174132 -0.037174124 -0.037174117 -0.037174113 -0.037174109 -0.037174106 -0.037174109 -0.037174109 -0.037174113 -0.037174124 -0.037174135 -0.037174139 -0.037174132 -0.037174128][-0.037173193 -0.037174132 -0.037174128 -0.037174117 -0.037174113 -0.037174113 -0.037174117 -0.037174124 -0.037174135 -0.037174143 -0.037174154 -0.037174158 -0.037174162 -0.037174162 -0.037174158][-0.037173174 -0.037174132 -0.037174128 -0.037174113 -0.037174117 -0.037174121 -0.037174128 -0.037174135 -0.037174143 -0.037174147 -0.03717415 -0.037174158 -0.037174162 -0.037174165 -0.037174173]]...]
INFO - root - 2017-12-09 05:45:42.812097: step 2110, loss = 1.28, batch loss = 0.69 (37.6 examples/sec; 0.213 sec/batch; 19h:31m:08s remains)
INFO - root - 2017-12-09 05:45:44.949326: step 2120, loss = 1.28, batch loss = 0.69 (38.2 examples/sec; 0.210 sec/batch; 19h:14m:10s remains)
INFO - root - 2017-12-09 05:45:47.101356: step 2130, loss = 1.29, batch loss = 0.69 (37.8 examples/sec; 0.212 sec/batch; 19h:24m:37s remains)
INFO - root - 2017-12-09 05:45:49.278864: step 2140, loss = 1.28, batch loss = 0.68 (37.1 examples/sec; 0.215 sec/batch; 19h:45m:48s remains)
INFO - root - 2017-12-09 05:45:51.470790: step 2150, loss = 1.30, batch loss = 0.70 (37.0 examples/sec; 0.216 sec/batch; 19h:51m:17s remains)
INFO - root - 2017-12-09 05:45:53.609403: step 2160, loss = 1.30, batch loss = 0.69 (37.0 examples/sec; 0.216 sec/batch; 19h:51m:01s remains)
INFO - root - 2017-12-09 05:45:55.764041: step 2170, loss = 1.31, batch loss = 0.70 (37.8 examples/sec; 0.212 sec/batch; 19h:24m:28s remains)
INFO - root - 2017-12-09 05:45:57.910262: step 2180, loss = 1.30, batch loss = 0.69 (36.9 examples/sec; 0.217 sec/batch; 19h:52m:09s remains)
INFO - root - 2017-12-09 05:46:00.048358: step 2190, loss = 1.30, batch loss = 0.69 (36.7 examples/sec; 0.218 sec/batch; 19h:58m:51s remains)
INFO - root - 2017-12-09 05:46:02.241310: step 2200, loss = 1.31, batch loss = 0.70 (35.5 examples/sec; 0.225 sec/batch; 20h:39m:23s remains)
2017-12-09 05:46:02.615163: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0373346 -0.037328955 -0.037326656 -0.03732739 -0.037329931 -0.037332837 -0.037335344 -0.037337106 -0.037337895 -0.037337746 -0.0373367 -0.037335075 -0.0373335 -0.037332572 -0.037332997][-0.037305612 -0.037303522 -0.037302569 -0.037303962 -0.037306994 -0.037310392 -0.037313353 -0.037315432 -0.037316334 -0.037316021 -0.03731465 -0.037312698 -0.037310828 -0.037309688 -0.037310049][-0.037297554 -0.037300169 -0.037301075 -0.037303429 -0.037306998 -0.037310705 -0.037313823 -0.037315935 -0.037316773 -0.037316203 -0.037314467 -0.037312251 -0.037310209 -0.037308916 -0.037309159][-0.037291292 -0.037297368 -0.037299417 -0.037302539 -0.037306551 -0.03731047 -0.037313536 -0.037315387 -0.037315939 -0.037315089 -0.037313063 -0.037310671 -0.037308581 -0.037307326 -0.037307523][-0.037287056 -0.037295043 -0.037297476 -0.037300952 -0.037305195 -0.037309319 -0.037312284 -0.037313785 -0.037313927 -0.0373127 -0.037310474 -0.037308067 -0.037306063 -0.037304968 -0.037305254][-0.037290812 -0.0372974 -0.037298433 -0.037300829 -0.037304297 -0.037307978 -0.037310645 -0.037311897 -0.037311658 -0.037309986 -0.037307497 -0.037305053 -0.037303139 -0.037302166 -0.037302546][-0.0373059 -0.0373078 -0.037305053 -0.037304152 -0.037305146 -0.037307419 -0.037309408 -0.037310235 -0.037309647 -0.037307579 -0.037304725 -0.037302084 -0.037300117 -0.037299156 -0.037299555][-0.037337806 -0.037329856 -0.037319593 -0.037312124 -0.037308 -0.037307147 -0.037307877 -0.03730835 -0.037307672 -0.037305452 -0.03730236 -0.037299439 -0.037297282 -0.037296236 -0.037296593][-0.037382923 -0.037362222 -0.037341774 -0.037324812 -0.037313014 -0.037307363 -0.037306022 -0.037305947 -0.037305374 -0.037303381 -0.037300408 -0.03729739 -0.037295066 -0.03729387 -0.037294097][-0.037433159 -0.037399437 -0.037368171 -0.037340716 -0.037319988 -0.037308212 -0.0373039 -0.037303016 -0.037302569 -0.03730106 -0.037298575 -0.037295822 -0.03729355 -0.037292268 -0.037292317][-0.037476014 -0.037432496 -0.037392396 -0.037355982 -0.037327256 -0.037309483 -0.03730179 -0.037299741 -0.037299342 -0.037298378 -0.037296496 -0.037294194 -0.03729216 -0.037290931 -0.037290871][-0.03750281 -0.037454672 -0.037409525 -0.037367474 -0.037333179 -0.037310675 -0.03729992 -0.037296556 -0.037296012 -0.037295468 -0.0372942 -0.037292406 -0.037290726 -0.037289642 -0.037289534][-0.037508439 -0.037461672 -0.037416633 -0.037373837 -0.037338026 -0.037313208 -0.037300244 -0.03729555 -0.037294563 -0.037294157 -0.037293319 -0.03729194 -0.037290581 -0.037289526 -0.037289217][-0.037497006 -0.037455428 -0.037414938 -0.037375916 -0.037342537 -0.037318591 -0.037305254 -0.037299998 -0.037298802 -0.037298527 -0.0372981 -0.037297163 -0.037296135 -0.037295032 -0.037294429][-0.03748443 -0.037449848 -0.037415605 -0.037382223 -0.03735311 -0.03733182 -0.03731906 -0.037313681 -0.037312321 -0.037312262 -0.037312336 -0.037312236 -0.03731202 -0.037311442 -0.037311178]]...]
INFO - root - 2017-12-09 05:46:04.780735: step 2210, loss = 1.31, batch loss = 0.69 (36.8 examples/sec; 0.218 sec/batch; 19h:57m:45s remains)
INFO - root - 2017-12-09 05:46:06.936455: step 2220, loss = 1.31, batch loss = 0.69 (36.6 examples/sec; 0.219 sec/batch; 20h:03m:40s remains)
INFO - root - 2017-12-09 05:46:09.157636: step 2230, loss = 1.30, batch loss = 0.69 (36.1 examples/sec; 0.222 sec/batch; 20h:19m:40s remains)
INFO - root - 2017-12-09 05:46:11.323905: step 2240, loss = 1.30, batch loss = 0.69 (37.2 examples/sec; 0.215 sec/batch; 19h:44m:01s remains)
INFO - root - 2017-12-09 05:46:13.460320: step 2250, loss = 1.31, batch loss = 0.69 (38.0 examples/sec; 0.210 sec/batch; 19h:17m:35s remains)
INFO - root - 2017-12-09 05:46:15.633337: step 2260, loss = 1.31, batch loss = 0.69 (37.0 examples/sec; 0.216 sec/batch; 19h:50m:08s remains)
INFO - root - 2017-12-09 05:46:17.803939: step 2270, loss = 1.31, batch loss = 0.69 (36.8 examples/sec; 0.217 sec/batch; 19h:56m:49s remains)
INFO - root - 2017-12-09 05:46:19.938647: step 2280, loss = 1.32, batch loss = 0.69 (37.3 examples/sec; 0.214 sec/batch; 19h:39m:48s remains)
INFO - root - 2017-12-09 05:46:22.107088: step 2290, loss = 1.31, batch loss = 0.69 (36.6 examples/sec; 0.219 sec/batch; 20h:03m:29s remains)
INFO - root - 2017-12-09 05:46:24.271379: step 2300, loss = 1.37, batch loss = 0.75 (35.0 examples/sec; 0.228 sec/batch; 20h:56m:20s remains)
2017-12-09 05:46:24.642403: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.98503393 0.9990626 1.0112977 1.0204105 1.0258832 1.0286088 1.0294976 1.0291479 1.027751 1.0254135 1.0221989 1.0184431 1.0149093 1.0123258 1.0110259][1.0011497 1.02739 1.050194 1.0675211 1.0781283 1.0832047 1.0843666 1.082989 1.0799346 1.0759778 1.071605 1.0672381 1.0635581 1.0611105 1.0600567][0.96804088 1.0064592 1.0398188 1.065825 1.0819619 1.0893745 1.0904372 1.0875609 1.0824699 1.0766857 1.0711458 1.0663464 1.0628521 1.0609441 1.0604947][0.94117731 0.98949188 1.030838 1.0639319 1.084621 1.0936399 1.0941598 1.0896702 1.0826838 1.0753348 1.0689046 1.0639629 1.0609453 1.0598497 1.0602098][0.92542326 0.97962981 1.0243132 1.0609088 1.0839095 1.0935163 1.0934393 1.0879302 1.0799553 1.0719326 1.0653143 1.0607207 1.0584612 1.0582786 1.0594096][0.92742717 0.98264784 1.0244327 1.0592542 1.0811267 1.0897825 1.0890286 1.0832357 1.0753181 1.0675919 1.0615127 1.0577154 1.0563588 1.0569699 1.0586064][0.95155889 1.0024407 1.0349858 1.0624528 1.0793412 1.0852487 1.0835862 1.0779891 1.0708089 1.0639765 1.0588207 1.0559342 1.0553219 1.056371 1.0580988][0.9876613 1.0320437 1.0520296 1.0688226 1.0785754 1.08109 1.0786345 1.0735736 1.067446 1.0617137 1.0575345 1.0554183 1.0552399 1.0563343 1.0578127][1.0166652 1.0563276 1.0657375 1.0735579 1.0774524 1.077318 1.0743775 1.0698802 1.0647657 1.0601056 1.05685 1.0553713 1.0554383 1.0564048 1.0575404][1.0311673 1.0694356 1.0725131 1.0747104 1.0749748 1.073319 1.0701995 1.066242 1.0620952 1.0585482 1.0562627 1.055398 1.0556493 1.0564559 1.0572793][1.0333545 1.0728836 1.0733781 1.0731697 1.071799 1.0693946 1.0662692 1.0628843 1.0597594 1.0573781 1.0560408 1.0556898 1.0560076 1.0565891 1.0571091][1.0310507 1.0720172 1.071408 1.070071 1.067966 1.0653787 1.0626333 1.0601139 1.0581658 1.0569253 1.0563581 1.0562887 1.0564866 1.0567591 1.0569685][1.0269647 1.068279 1.0670862 1.0653629 1.063328 1.0613008 1.0594993 1.0581477 1.0573597 1.0570066 1.0568831 1.0568383 1.056807 1.0567741 1.0567322][1.0220721 1.0629034 1.0617411 1.0604131 1.0591803 1.0582061 1.0575228 1.0571725 1.0571179 1.0571709 1.0571418 1.0570002 1.0567896 1.0565848 1.0564218][1.0145304 1.05617 1.0565035 1.0552185 1.0540526 1.0529416 1.0521944 1.0517144 1.0514464 1.0514812 1.0513549 1.0511119 1.0508332 1.0505948 1.0504189]]...]
INFO - root - 2017-12-09 05:46:26.813788: step 2310, loss = 1.44, batch loss = 0.81 (38.4 examples/sec; 0.208 sec/batch; 19h:07m:02s remains)
INFO - root - 2017-12-09 05:46:28.996315: step 2320, loss = 1.48, batch loss = 0.85 (37.1 examples/sec; 0.216 sec/batch; 19h:47m:26s remains)
INFO - root - 2017-12-09 05:46:31.122311: step 2330, loss = 1.32, batch loss = 0.69 (35.8 examples/sec; 0.224 sec/batch; 20h:30m:55s remains)
INFO - root - 2017-12-09 05:46:33.290469: step 2340, loss = 1.32, batch loss = 0.69 (36.2 examples/sec; 0.221 sec/batch; 20h:17m:35s remains)
INFO - root - 2017-12-09 05:46:35.437949: step 2350, loss = 1.33, batch loss = 0.69 (37.5 examples/sec; 0.213 sec/batch; 19h:34m:21s remains)
INFO - root - 2017-12-09 05:46:37.624998: step 2360, loss = 1.33, batch loss = 0.69 (34.7 examples/sec; 0.230 sec/batch; 21h:07m:29s remains)
INFO - root - 2017-12-09 05:46:39.839140: step 2370, loss = 1.33, batch loss = 0.69 (38.4 examples/sec; 0.209 sec/batch; 19h:07m:37s remains)
INFO - root - 2017-12-09 05:46:42.056863: step 2380, loss = 1.33, batch loss = 0.69 (35.6 examples/sec; 0.225 sec/batch; 20h:36m:51s remains)
INFO - root - 2017-12-09 05:46:44.229767: step 2390, loss = 1.33, batch loss = 0.69 (36.7 examples/sec; 0.218 sec/batch; 19h:58m:01s remains)
INFO - root - 2017-12-09 05:46:46.364408: step 2400, loss = 1.37, batch loss = 0.73 (37.4 examples/sec; 0.214 sec/batch; 19h:35m:36s remains)
2017-12-09 05:46:46.791880: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.028338833 -0.024369577 -0.010353236 0.015705578 0.050445877 0.086133383 0.11107018 0.11499683 0.097520195 0.067478165 0.033688977 0.004528109 -0.015721945 -0.026563562 -0.030716127][-0.022159006 -0.017801905 -0.0013553165 0.030287474 0.073297143 0.11712693 0.15084389 0.16100365 0.14552322 0.11186922 0.069612533 0.030211061 -0.00055075437 -0.019668613 -0.028585102][-0.0070533007 -0.0065277778 0.0094352663 0.046478309 0.099453144 0.15541428 0.20106885 0.22157183 0.21080118 0.1742194 0.12178037 0.068192758 0.023229919 -0.0079396144 -0.024288055][0.024907604 0.020166717 0.033890978 0.073851958 0.13619792 0.2051484 0.26380298 0.29504943 0.28902838 0.2487759 0.18530747 0.11621348 0.05519408 0.0097535625 -0.016809482][0.083275422 0.071499847 0.081397682 0.1227399 0.19145915 0.26931426 0.33684874 0.37566471 0.37275374 0.32873872 0.25476238 0.17092592 0.093576528 0.032898553 -0.00582695][0.16943085 0.1545516 0.16050905 0.1991469 0.26665589 0.34377351 0.41143483 0.450943 0.44804436 0.40135333 0.32004878 0.2248542 0.13385883 0.059266128 0.0081875958][0.272631 0.25987229 0.26429549 0.2984668 0.35862193 0.42584473 0.48380426 0.51646215 0.51065612 0.46178928 0.3752566 0.27175677 0.16994557 0.083978869 0.021977805][0.37393007 0.36630329 0.37017927 0.39793405 0.44525915 0.4972358 0.5397079 0.56248426 0.55305517 0.50378549 0.41579261 0.30649525 0.19648662 0.10163782 0.031535812][0.46567783 0.46595684 0.46929494 0.48945865 0.52268493 0.55716 0.58328503 0.59443283 0.580088 0.52973878 0.44038665 0.32648566 0.21005356 0.10899595 0.034087725][0.54111987 0.54856604 0.54976135 0.56331736 0.58310145 0.60126245 0.6128456 0.61411774 0.59525555 0.54312056 0.45141077 0.33346319 0.21245679 0.10759545 0.031106554][0.59055543 0.60466027 0.60429627 0.61149842 0.61989623 0.62644613 0.62859976 0.6229465 0.6003502 0.54501808 0.45019203 0.32880422 0.20573172 0.10032488 0.025245026][0.61261672 0.6283263 0.62490785 0.62617517 0.62740505 0.627917 0.62665856 0.62010825 0.59699655 0.53971487 0.44205245 0.31851098 0.19515526 0.091283493 0.019123107][0.61485231 0.62512 0.61353624 0.60589397 0.60006773 0.5980702 0.598562 0.59669983 0.57765532 0.52234238 0.42519239 0.30255663 0.18180546 0.081479929 0.013368513][0.59982026 0.59706771 0.57198471 0.55183542 0.53888863 0.53690493 0.5435102 0.55095619 0.53972059 0.48972377 0.39621985 0.27835536 0.16334531 0.069220349 0.0070494227][0.56267452 0.54190797 0.49941823 0.46531036 0.44657633 0.4473069 0.46348974 0.48263833 0.48159912 0.43991309 0.35386842 0.24527466 0.1394985 0.054497026 0.00010497868]]...]
INFO - root - 2017-12-09 05:46:48.929902: step 2410, loss = 1.33, batch loss = 0.69 (37.3 examples/sec; 0.215 sec/batch; 19h:40m:16s remains)
INFO - root - 2017-12-09 05:46:51.080135: step 2420, loss = 1.34, batch loss = 0.69 (35.9 examples/sec; 0.223 sec/batch; 20h:27m:37s remains)
INFO - root - 2017-12-09 05:46:53.232676: step 2430, loss = 1.33, batch loss = 0.69 (36.8 examples/sec; 0.218 sec/batch; 19h:57m:12s remains)
INFO - root - 2017-12-09 05:46:55.382481: step 2440, loss = 1.34, batch loss = 0.69 (37.1 examples/sec; 0.216 sec/batch; 19h:47m:35s remains)
INFO - root - 2017-12-09 05:46:57.566743: step 2450, loss = 1.34, batch loss = 0.69 (35.0 examples/sec; 0.229 sec/batch; 20h:57m:34s remains)
INFO - root - 2017-12-09 05:46:59.728235: step 2460, loss = 1.34, batch loss = 0.69 (37.2 examples/sec; 0.215 sec/batch; 19h:43m:55s remains)
INFO - root - 2017-12-09 05:47:01.903393: step 2470, loss = 1.34, batch loss = 0.69 (38.1 examples/sec; 0.210 sec/batch; 19h:14m:17s remains)
INFO - root - 2017-12-09 05:47:04.075411: step 2480, loss = 1.34, batch loss = 0.69 (35.5 examples/sec; 0.225 sec/batch; 20h:38m:24s remains)
INFO - root - 2017-12-09 05:47:06.252593: step 2490, loss = 1.35, batch loss = 0.69 (37.9 examples/sec; 0.211 sec/batch; 19h:19m:53s remains)
INFO - root - 2017-12-09 05:47:08.439091: step 2500, loss = 1.35, batch loss = 0.69 (34.0 examples/sec; 0.236 sec/batch; 21h:36m:00s remains)
2017-12-09 05:47:08.853416: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.045670453 -0.045670453 -0.045670453 -0.045670453 -0.045670453 -0.045670453 -0.045670453 -0.045670453 -0.045670453 -0.045670453 -0.045670453 -0.045670453 -0.045670453 -0.045670453 -0.045670453][-0.045671772 -0.045671772 -0.045671772 -0.045671772 -0.045671772 -0.045671772 -0.045671772 -0.045671772 -0.045671772 -0.045671772 -0.045671772 -0.045671772 -0.045671772 -0.045671772 -0.045671772][-0.045671772 -0.045671772 -0.045671772 -0.045671772 -0.045671772 -0.045671772 -0.045671772 -0.045671772 -0.045671772 -0.045671772 -0.045671772 -0.045671772 -0.045671772 -0.045671772 -0.045671772][-0.045671772 -0.045671772 -0.045671772 -0.045671772 -0.045671772 -0.045671772 -0.045671772 -0.045671772 -0.045671772 -0.045671772 -0.045671772 -0.045671772 -0.045671772 -0.045671772 -0.045671772][-0.045671772 -0.045671772 -0.045671772 -0.045671772 -0.045671772 -0.045671772 -0.045671772 -0.045671772 -0.045671772 -0.045671772 -0.045671772 -0.045671772 -0.045671772 -0.045671772 -0.045671772][-0.045671772 -0.045671772 -0.045671772 -0.045671772 -0.045671772 -0.045671772 -0.045671772 -0.045671772 -0.045671772 -0.045671772 -0.045671772 -0.045671772 -0.045671772 -0.045671772 -0.045671772][-0.045671772 -0.045671772 -0.045671772 -0.045671772 -0.045671772 -0.045671772 -0.045671772 -0.045671772 -0.045671772 -0.045671772 -0.045671772 -0.045671772 -0.045671772 -0.045671772 -0.045671772][-0.045671757 -0.045671772 -0.045671772 -0.045671772 -0.045671772 -0.045671772 -0.045671772 -0.045671772 -0.045671772 -0.045671772 -0.045671772 -0.045671772 -0.045671772 -0.045671772 -0.045671772][-0.045671713 -0.045671772 -0.045671772 -0.045671772 -0.045671772 -0.045671772 -0.045671772 -0.045671772 -0.045671772 -0.045671772 -0.045671772 -0.045671772 -0.045671772 -0.045671772 -0.045671772][-0.045671657 -0.045671772 -0.045671772 -0.045671772 -0.045671772 -0.045671772 -0.045671772 -0.045671772 -0.045671772 -0.045671772 -0.045671772 -0.045671772 -0.045671772 -0.045671772 -0.045671772][-0.045671638 -0.045671772 -0.045671772 -0.045671772 -0.045671772 -0.045671772 -0.045671772 -0.045671772 -0.045671772 -0.045671772 -0.045671772 -0.045671772 -0.045671772 -0.045671772 -0.045671772][-0.045671634 -0.045671772 -0.045671772 -0.045671772 -0.045671772 -0.045671772 -0.045671772 -0.045671772 -0.045671772 -0.045671772 -0.045671772 -0.045671772 -0.045671772 -0.045671772 -0.045671772][-0.045671552 -0.045671772 -0.045671772 -0.045671772 -0.045671772 -0.045671772 -0.045671772 -0.045671772 -0.045671772 -0.045671772 -0.045671772 -0.045671772 -0.045671772 -0.045671772 -0.045671772][-0.045671556 -0.045671772 -0.045671772 -0.045671772 -0.045671772 -0.045671772 -0.045671772 -0.045671772 -0.045671772 -0.045671772 -0.045671772 -0.045671772 -0.045671772 -0.045671772 -0.045671772][-0.045671564 -0.045671772 -0.045671772 -0.045671772 -0.045671772 -0.045671772 -0.045671772 -0.045671772 -0.045671772 -0.045671772 -0.045671772 -0.045671772 -0.045671772 -0.045671772 -0.045671772]]...]
INFO - root - 2017-12-09 05:47:11.021013: step 2510, loss = 1.35, batch loss = 0.69 (36.0 examples/sec; 0.222 sec/batch; 20h:21m:13s remains)
INFO - root - 2017-12-09 05:47:13.357981: step 2520, loss = 1.34, batch loss = 0.68 (37.8 examples/sec; 0.212 sec/batch; 19h:24m:52s remains)
INFO - root - 2017-12-09 05:47:15.490184: step 2530, loss = 1.35, batch loss = 0.69 (36.6 examples/sec; 0.219 sec/batch; 20h:02m:46s remains)
INFO - root - 2017-12-09 05:47:17.899496: step 2540, loss = 1.36, batch loss = 0.69 (20.4 examples/sec; 0.393 sec/batch; 36h:01m:35s remains)
INFO - root - 2017-12-09 05:47:21.343459: step 2550, loss = 1.36, batch loss = 0.69 (38.5 examples/sec; 0.208 sec/batch; 19h:01m:39s remains)
INFO - root - 2017-12-09 05:47:25.630639: step 2560, loss = 1.36, batch loss = 0.69 (19.3 examples/sec; 0.415 sec/batch; 38h:03m:43s remains)
INFO - root - 2017-12-09 05:47:29.928687: step 2570, loss = 1.36, batch loss = 0.69 (19.0 examples/sec; 0.420 sec/batch; 38h:31m:01s remains)
INFO - root - 2017-12-09 05:47:34.196216: step 2580, loss = 1.36, batch loss = 0.69 (18.6 examples/sec; 0.431 sec/batch; 39h:28m:58s remains)
INFO - root - 2017-12-09 05:47:38.546848: step 2590, loss = 1.36, batch loss = 0.69 (18.4 examples/sec; 0.435 sec/batch; 39h:50m:57s remains)
INFO - root - 2017-12-09 05:47:42.828545: step 2600, loss = 1.37, batch loss = 0.69 (17.9 examples/sec; 0.446 sec/batch; 40h:53m:53s remains)
2017-12-09 05:47:43.419860: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.045364853 -0.045364857 -0.045364857 -0.04536486 -0.045364887 -0.045364928 -0.045364957 -0.045364954 -0.045364928 -0.045364913 -0.045364913 -0.045364931 -0.045364931 -0.04536489 -0.045364842][-0.0453665 -0.0453665 -0.0453665 -0.0453665 -0.0453665 -0.0453665 -0.0453665 -0.0453665 -0.0453665 -0.0453665 -0.0453665 -0.0453665 -0.0453665 -0.0453665 -0.0453665][-0.0453665 -0.0453665 -0.0453665 -0.0453665 -0.0453665 -0.0453665 -0.0453665 -0.0453665 -0.0453665 -0.0453665 -0.0453665 -0.0453665 -0.0453665 -0.0453665 -0.0453665][-0.0453665 -0.0453665 -0.0453665 -0.0453665 -0.0453665 -0.0453665 -0.0453665 -0.0453665 -0.0453665 -0.0453665 -0.0453665 -0.0453665 -0.0453665 -0.0453665 -0.0453665][-0.0453665 -0.0453665 -0.0453665 -0.0453665 -0.0453665 -0.0453665 -0.0453665 -0.0453665 -0.0453665 -0.0453665 -0.0453665 -0.0453665 -0.0453665 -0.0453665 -0.0453665][-0.0453665 -0.0453665 -0.0453665 -0.0453665 -0.0453665 -0.0453665 -0.0453665 -0.0453665 -0.0453665 -0.0453665 -0.0453665 -0.0453665 -0.0453665 -0.0453665 -0.0453665][-0.0453665 -0.0453665 -0.0453665 -0.0453665 -0.0453665 -0.0453665 -0.0453665 -0.0453665 -0.0453665 -0.0453665 -0.0453665 -0.0453665 -0.0453665 -0.0453665 -0.0453665][-0.045366455 -0.0453665 -0.0453665 -0.0453665 -0.0453665 -0.0453665 -0.0453665 -0.0453665 -0.0453665 -0.0453665 -0.0453665 -0.0453665 -0.0453665 -0.0453665 -0.0453665][-0.045366339 -0.0453665 -0.0453665 -0.0453665 -0.0453665 -0.0453665 -0.0453665 -0.0453665 -0.0453665 -0.0453665 -0.0453665 -0.0453665 -0.0453665 -0.0453665 -0.0453665][-0.045366209 -0.0453665 -0.0453665 -0.0453665 -0.0453665 -0.0453665 -0.0453665 -0.0453665 -0.0453665 -0.0453665 -0.0453665 -0.0453665 -0.0453665 -0.0453665 -0.0453665][-0.045366149 -0.0453665 -0.0453665 -0.0453665 -0.0453665 -0.0453665 -0.0453665 -0.0453665 -0.0453665 -0.0453665 -0.0453665 -0.0453665 -0.0453665 -0.0453665 -0.0453665][-0.045366149 -0.0453665 -0.0453665 -0.0453665 -0.0453665 -0.0453665 -0.0453665 -0.0453665 -0.0453665 -0.0453665 -0.0453665 -0.0453665 -0.0453665 -0.0453665 -0.0453665][-0.045366127 -0.0453665 -0.0453665 -0.0453665 -0.0453665 -0.0453665 -0.0453665 -0.0453665 -0.0453665 -0.0453665 -0.0453665 -0.0453665 -0.0453665 -0.0453665 -0.0453665][-0.045366138 -0.0453665 -0.0453665 -0.0453665 -0.0453665 -0.0453665 -0.0453665 -0.0453665 -0.0453665 -0.0453665 -0.0453665 -0.0453665 -0.0453665 -0.0453665 -0.0453665][-0.045366149 -0.0453665 -0.0453665 -0.0453665 -0.0453665 -0.0453665 -0.0453665 -0.0453665 -0.0453665 -0.0453665 -0.0453665 -0.0453665 -0.0453665 -0.0453665 -0.0453665]]...]
INFO - root - 2017-12-09 05:47:47.789316: step 2610, loss = 1.37, batch loss = 0.69 (18.4 examples/sec; 0.435 sec/batch; 39h:49m:01s remains)
INFO - root - 2017-12-09 05:47:52.149981: step 2620, loss = 1.37, batch loss = 0.69 (18.1 examples/sec; 0.442 sec/batch; 40h:28m:35s remains)
INFO - root - 2017-12-09 05:47:56.464351: step 2630, loss = 1.37, batch loss = 0.69 (18.9 examples/sec; 0.423 sec/batch; 38h:46m:21s remains)
INFO - root - 2017-12-09 05:48:00.847721: step 2640, loss = 1.37, batch loss = 0.69 (18.3 examples/sec; 0.437 sec/batch; 40h:02m:22s remains)
INFO - root - 2017-12-09 05:48:05.168490: step 2650, loss = 1.38, batch loss = 0.69 (21.5 examples/sec; 0.371 sec/batch; 34h:01m:24s remains)
INFO - root - 2017-12-09 05:48:09.335230: step 2660, loss = 1.37, batch loss = 0.69 (18.4 examples/sec; 0.434 sec/batch; 39h:47m:58s remains)
INFO - root - 2017-12-09 05:48:13.707916: step 2670, loss = 1.38, batch loss = 0.69 (18.6 examples/sec; 0.431 sec/batch; 39h:28m:34s remains)
INFO - root - 2017-12-09 05:48:18.027504: step 2680, loss = 1.37, batch loss = 0.69 (18.2 examples/sec; 0.439 sec/batch; 40h:14m:31s remains)
INFO - root - 2017-12-09 05:48:22.454664: step 2690, loss = 1.38, batch loss = 0.69 (18.1 examples/sec; 0.441 sec/batch; 40h:26m:06s remains)
INFO - root - 2017-12-09 05:48:26.876135: step 2700, loss = 1.38, batch loss = 0.69 (18.1 examples/sec; 0.441 sec/batch; 40h:24m:44s remains)
2017-12-09 05:48:27.389050: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.042875271 -0.04287564 -0.042875767 -0.042875804 -0.042875696 -0.042875618 -0.042875633 -0.042875636 -0.042875689 -0.042875707 -0.042875782 -0.042875808 -0.042875838 -0.042875838 -0.042875774][-0.042875659 -0.042875875 -0.042875879 -0.042875886 -0.042875789 -0.042875711 -0.042875625 -0.042875618 -0.04287567 -0.042875711 -0.042875789 -0.042875823 -0.042875867 -0.042875879 -0.04287586][-0.042875297 -0.042875882 -0.042875882 -0.042875886 -0.042875845 -0.042875778 -0.042875715 -0.042875707 -0.0428757 -0.042875722 -0.042875741 -0.042875763 -0.042875819 -0.042875838 -0.042875845][-0.042875294 -0.04287589 -0.04287589 -0.042875879 -0.042875879 -0.042875864 -0.042875871 -0.04287589 -0.04287586 -0.042875867 -0.042875845 -0.042875856 -0.042875871 -0.042875893 -0.042875893][-0.042875495 -0.042875748 -0.0428757 -0.042875595 -0.042875458 -0.042875297 -0.042875152 -0.042875089 -0.042875126 -0.042875294 -0.042875521 -0.042875733 -0.042875882 -0.042875893 -0.04287589][-0.042874265 -0.042873614 -0.042872678 -0.042871587 -0.042870462 -0.042870045 -0.042870086 -0.042870343 -0.042871129 -0.042871937 -0.042872507 -0.042873126 -0.042873733 -0.0428743 -0.042874809][-0.042873189 -0.04287282 -0.042872291 -0.042871952 -0.0428719 -0.04287212 -0.042872503 -0.042872246 -0.042871512 -0.042870883 -0.042870518 -0.042870183 -0.042869888 -0.04287019 -0.04287098][-0.042874496 -0.042874724 -0.042874455 -0.042874344 -0.042874318 -0.042874262 -0.042874023 -0.042873606 -0.04287317 -0.042873073 -0.042873204 -0.042873424 -0.042873655 -0.042873681 -0.042873666][-0.042875107 -0.042875554 -0.042875491 -0.042875443 -0.0428754 -0.042875353 -0.042875305 -0.042875294 -0.042875309 -0.042875361 -0.042875428 -0.0428755 -0.042875569 -0.042875648 -0.042875718][-0.042875607 -0.042875852 -0.042875815 -0.042875756 -0.042875707 -0.042875685 -0.042875674 -0.042875681 -0.042875715 -0.042875785 -0.042875849 -0.042875882 -0.042875897 -0.042875886 -0.042875852][-0.042875681 -0.04287583 -0.042875782 -0.04287577 -0.042875767 -0.042875793 -0.042875826 -0.04287586 -0.042875893 -0.042875897 -0.042875897 -0.042875897 -0.042875897 -0.04287589 -0.042875875][-0.042875644 -0.042875808 -0.042875804 -0.042875804 -0.042875763 -0.042875793 -0.042875811 -0.042875841 -0.042875856 -0.042875867 -0.042875867 -0.04287586 -0.042875849 -0.042875834 -0.042875819][-0.042875577 -0.042875752 -0.042875808 -0.042875819 -0.042875856 -0.042875875 -0.042875849 -0.04287586 -0.042875849 -0.042875849 -0.042875834 -0.042875815 -0.042875793 -0.042875763 -0.042875737][-0.042875588 -0.042875808 -0.042875793 -0.0428758 -0.042875864 -0.042875867 -0.04287589 -0.042875879 -0.042875838 -0.042875826 -0.042875797 -0.042875782 -0.042875763 -0.042875741 -0.042875692][-0.042875476 -0.042875841 -0.04287583 -0.042875841 -0.042875826 -0.042875823 -0.042875867 -0.042875845 -0.042875871 -0.04287586 -0.042875826 -0.042875823 -0.042875782 -0.042875759 -0.042875703]]...]
INFO - root - 2017-12-09 05:48:31.700845: step 2710, loss = 1.39, batch loss = 0.69 (18.3 examples/sec; 0.437 sec/batch; 40h:01m:42s remains)
INFO - root - 2017-12-09 05:48:36.047638: step 2720, loss = 1.38, batch loss = 0.69 (17.6 examples/sec; 0.454 sec/batch; 41h:37m:46s remains)
INFO - root - 2017-12-09 05:48:40.513733: step 2730, loss = 1.39, batch loss = 0.69 (17.6 examples/sec; 0.455 sec/batch; 41h:38m:54s remains)
INFO - root - 2017-12-09 05:48:45.046122: step 2740, loss = 1.39, batch loss = 0.69 (18.4 examples/sec; 0.435 sec/batch; 39h:52m:55s remains)
INFO - root - 2017-12-09 05:48:49.414540: step 2750, loss = 1.39, batch loss = 0.69 (18.9 examples/sec; 0.423 sec/batch; 38h:46m:47s remains)
INFO - root - 2017-12-09 05:48:53.537777: step 2760, loss = 1.39, batch loss = 0.69 (17.9 examples/sec; 0.447 sec/batch; 40h:55m:01s remains)
INFO - root - 2017-12-09 05:48:57.936099: step 2770, loss = 1.38, batch loss = 0.68 (18.2 examples/sec; 0.439 sec/batch; 40h:11m:32s remains)
INFO - root - 2017-12-09 05:49:02.384061: step 2780, loss = 1.39, batch loss = 0.69 (17.7 examples/sec; 0.451 sec/batch; 41h:20m:47s remains)
INFO - root - 2017-12-09 05:49:06.866348: step 2790, loss = 1.39, batch loss = 0.68 (17.9 examples/sec; 0.447 sec/batch; 40h:57m:37s remains)
INFO - root - 2017-12-09 05:49:11.350263: step 2800, loss = 1.40, batch loss = 0.69 (17.8 examples/sec; 0.450 sec/batch; 41h:14m:38s remains)
2017-12-09 05:49:11.867295: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.04202015 -0.042020041 -0.042019874 -0.042019855 -0.042019788 -0.042019643 -0.042019662 -0.042019755 -0.042020015 -0.042020556 -0.042021267 -0.042022567 -0.0420232 -0.042022977 -0.04202361][-0.042020805 -0.042020775 -0.042020675 -0.042020563 -0.042020496 -0.042020451 -0.042020585 -0.042020991 -0.042021465 -0.042022176 -0.042023219 -0.04202427 -0.042024598 -0.042024471 -0.042024653][-0.04202066 -0.042020582 -0.042020436 -0.042020325 -0.042020328 -0.042020421 -0.0420207 -0.042021047 -0.042021509 -0.042022318 -0.042023387 -0.04202437 -0.042024687 -0.042024538 -0.042024329][-0.042020705 -0.042020597 -0.042020436 -0.042020377 -0.04202047 -0.042020664 -0.042021 -0.042021271 -0.0420217 -0.042022325 -0.042023249 -0.04202437 -0.042024739 -0.04202459 -0.04202434][-0.042020727 -0.042020731 -0.04202066 -0.042020723 -0.042020869 -0.042021126 -0.042021427 -0.042021677 -0.0420221 -0.042022638 -0.04202348 -0.042024285 -0.042024609 -0.042024549 -0.042024348][-0.04202069 -0.042020872 -0.042020947 -0.042021126 -0.042021267 -0.04202152 -0.042021763 -0.042021979 -0.042022422 -0.042023018 -0.0420239 -0.042024434 -0.042024653 -0.042024538 -0.042024352][-0.04202053 -0.042020794 -0.042021 -0.042021222 -0.042021323 -0.042021491 -0.042021673 -0.042021886 -0.042022355 -0.042022981 -0.042023871 -0.042024381 -0.042024549 -0.04202446 -0.042024314][-0.04202025 -0.042020533 -0.042020828 -0.04202101 -0.04202107 -0.0420212 -0.042021371 -0.0420216 -0.042022038 -0.042022604 -0.04202354 -0.042024072 -0.0420243 -0.042024318 -0.042024273][-0.042020079 -0.04202025 -0.042020496 -0.042020597 -0.042020623 -0.042020772 -0.042020973 -0.04202133 -0.04202171 -0.042022143 -0.042022988 -0.042023472 -0.042023759 -0.042024 -0.042024162][-0.042019892 -0.042019974 -0.042020109 -0.042020105 -0.042020123 -0.042020235 -0.042020462 -0.04202098 -0.042021323 -0.042021733 -0.04202234 -0.04202278 -0.042023208 -0.042023614 -0.042024013][-0.042019848 -0.042019818 -0.042019878 -0.042019844 -0.042019922 -0.042019978 -0.042020183 -0.042020682 -0.042020988 -0.042021461 -0.042021912 -0.042022537 -0.042023104 -0.04202354 -0.042023983][-0.042020049 -0.042019919 -0.042019978 -0.042019986 -0.042020191 -0.04202025 -0.042020429 -0.042020679 -0.042020828 -0.042021375 -0.042021934 -0.042022888 -0.042023458 -0.042023771 -0.04202405][-0.042020448 -0.04202047 -0.042020623 -0.042020727 -0.042020965 -0.042021092 -0.042021271 -0.042021248 -0.042021181 -0.0420214 -0.042021889 -0.042023048 -0.04202367 -0.042024031 -0.042024154][-0.042020798 -0.042021058 -0.042021334 -0.042021543 -0.04202177 -0.042021945 -0.042022157 -0.042022169 -0.042022038 -0.04202199 -0.042022306 -0.0420231 -0.042023644 -0.042024095 -0.042024195][-0.042021178 -0.042021606 -0.042021949 -0.042022448 -0.042022731 -0.042022932 -0.042023081 -0.042023059 -0.04202278 -0.042022694 -0.042022988 -0.042023376 -0.042023722 -0.04202402 -0.042024158]]...]
INFO - root - 2017-12-09 05:49:16.247069: step 2810, loss = 1.40, batch loss = 0.69 (18.6 examples/sec; 0.430 sec/batch; 39h:21m:54s remains)
INFO - root - 2017-12-09 05:49:20.613742: step 2820, loss = 1.39, batch loss = 0.68 (18.9 examples/sec; 0.423 sec/batch; 38h:46m:55s remains)
INFO - root - 2017-12-09 05:49:25.047014: step 2830, loss = 1.40, batch loss = 0.69 (18.7 examples/sec; 0.427 sec/batch; 39h:08m:53s remains)
INFO - root - 2017-12-09 05:49:29.477598: step 2840, loss = 1.40, batch loss = 0.69 (17.8 examples/sec; 0.450 sec/batch; 41h:11m:53s remains)
INFO - root - 2017-12-09 05:49:33.894206: step 2850, loss = 1.40, batch loss = 0.69 (17.6 examples/sec; 0.456 sec/batch; 41h:43m:53s remains)
INFO - root - 2017-12-09 05:49:38.135218: step 2860, loss = 1.39, batch loss = 0.68 (17.7 examples/sec; 0.453 sec/batch; 41h:29m:40s remains)
INFO - root - 2017-12-09 05:49:42.656603: step 2870, loss = 1.40, batch loss = 0.68 (17.4 examples/sec; 0.460 sec/batch; 42h:09m:49s remains)
INFO - root - 2017-12-09 05:49:47.041591: step 2880, loss = 1.39, batch loss = 0.67 (18.5 examples/sec; 0.434 sec/batch; 39h:41m:55s remains)
INFO - root - 2017-12-09 05:49:51.493020: step 2890, loss = 1.94, batch loss = 1.22 (18.5 examples/sec; 0.434 sec/batch; 39h:41m:38s remains)
INFO - root - 2017-12-09 05:49:55.811064: step 2900, loss = 2.33, batch loss = 1.61 (18.2 examples/sec; 0.441 sec/batch; 40h:20m:17s remains)
2017-12-09 05:49:56.396538: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.04178486 -0.041784991 -0.041784894 -0.041784558 -0.041784585 -0.04178457 -0.041784577 -0.041784581 -0.041784588 -0.041784633 -0.041784875 -0.041784965 -0.041784976 -0.0417849 -0.041784968][-0.041784603 -0.041784793 -0.041784797 -0.041784808 -0.04159075 -0.040949196 -0.039996877 -0.039346274 -0.03936132 -0.039908066 -0.040665321 -0.041306753 -0.041657094 -0.041769084 -0.041784856][-0.041784324 -0.041784782 -0.041784782 -0.041410305 -0.039117228 -0.033862252 -0.026944984 -0.022104893 -0.021829374 -0.025723711 -0.03144769 -0.036657874 -0.03997447 -0.041392226 -0.041747954][-0.041784246 -0.041784786 -0.041784786 -0.039541144 -0.029852742 -0.01020423 0.014243525 0.031461492 0.032934256 0.019440155 -0.00114429 -0.020399947 -0.033418298 -0.039625641 -0.04151633][-0.041784171 -0.041784789 -0.041784782 -0.035162035 -0.010477792 0.036540896 0.093055293 0.13292477 0.13673985 0.10583246 0.057719335 0.012227025 -0.019468311 -0.03548288 -0.040869661][-0.041784253 -0.041784789 -0.041784793 -0.028914038 0.01521394 0.096224815 0.19131511 0.25812998 0.26429969 0.21199131 0.1301779 0.052890129 -0.0015775412 -0.029883377 -0.039888382][-0.041784778 -0.041784804 -0.041784808 -0.023609135 0.035899825 0.14280508 0.26619762 0.35215646 0.358898 0.28971556 0.18229923 0.081784233 0.011121612 -0.025849313 -0.039136343][-0.041785344 -0.041784819 -0.041784823 -0.022225311 0.040161 0.15074791 0.27672961 0.36346355 0.36851457 0.295978 0.18488537 0.082290992 0.010960151 -0.026011517 -0.039169226][-0.041785542 -0.041784827 -0.041784871 -0.025758425 0.024888888 0.1142375 0.21511596 0.28355467 0.2857759 0.22591043 0.13584866 0.053984381 -0.0018953495 -0.030178685 -0.039943337][-0.039029352 -0.037238836 -0.03626189 -0.027033631 0.002222728 0.054900534 0.11462155 0.15453573 0.15428397 0.11676526 0.061793938 0.012797657 -0.019797513 -0.035690181 -0.040906239][-0.023315649 -0.012736237 -0.0070961043 -0.005276531 -0.00073009729 0.012277972 0.029845387 0.041586608 0.03882125 0.0219343 -0.0011513792 -0.021121869 -0.033892218 -0.039771542 -0.041539185][0.020327725 0.053230584 0.070432775 0.065026894 0.042900838 0.017693792 -0.0019118562 -0.014174098 -0.02176776 -0.027880132 -0.033457104 -0.037845269 -0.040429626 -0.041488387 -0.041755687][0.095451534 0.1640514 0.19906513 0.18539459 0.13291681 0.06862811 0.014304597 -0.020154217 -0.036054265 -0.040916089 -0.041683879 -0.041757919 -0.041782048 -0.041784942 -0.041784875][0.17999221 0.28645509 0.33932117 0.316009 0.23288515 0.13173926 0.046608977 -0.0075210556 -0.032758057 -0.040593952 -0.041785046 -0.041784938 -0.041784886 -0.041784842 -0.041784797][0.23458871 0.3634564 0.42535087 0.3938688 0.29055426 0.16678061 0.063943215 -0.00085343421 -0.03094022 -0.040308811 -0.041784979 -0.041784871 -0.041784812 -0.041784778 -0.04178476]]...]
INFO - root - 2017-12-09 05:50:00.807989: step 2910, loss = 2.59, batch loss = 1.87 (18.3 examples/sec; 0.436 sec/batch; 39h:56m:01s remains)
INFO - root - 2017-12-09 05:50:05.247720: step 2920, loss = 2.81, batch loss = 2.08 (17.8 examples/sec; 0.450 sec/batch; 41h:13m:14s remains)
INFO - root - 2017-12-09 05:50:09.652855: step 2930, loss = 2.59, batch loss = 1.87 (18.1 examples/sec; 0.443 sec/batch; 40h:31m:23s remains)
INFO - root - 2017-12-09 05:50:14.138585: step 2940, loss = 2.75, batch loss = 2.03 (17.7 examples/sec; 0.451 sec/batch; 41h:15m:36s remains)
INFO - root - 2017-12-09 05:50:18.548841: step 2950, loss = 1.42, batch loss = 0.69 (18.5 examples/sec; 0.433 sec/batch; 39h:37m:38s remains)
INFO - root - 2017-12-09 05:50:22.601453: step 2960, loss = 1.42, batch loss = 0.69 (18.0 examples/sec; 0.444 sec/batch; 40h:39m:30s remains)
INFO - root - 2017-12-09 05:50:26.898695: step 2970, loss = 1.42, batch loss = 0.69 (18.8 examples/sec; 0.426 sec/batch; 39h:00m:33s remains)
INFO - root - 2017-12-09 05:50:31.205488: step 2980, loss = 1.43, batch loss = 0.69 (19.3 examples/sec; 0.415 sec/batch; 38h:01m:21s remains)
INFO - root - 2017-12-09 05:50:35.532398: step 2990, loss = 1.43, batch loss = 0.69 (17.3 examples/sec; 0.461 sec/batch; 42h:13m:03s remains)
INFO - root - 2017-12-09 05:50:39.951070: step 3000, loss = 1.43, batch loss = 0.69 (18.3 examples/sec; 0.438 sec/batch; 40h:03m:46s remains)
2017-12-09 05:50:40.473619: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.042580526 -0.042803142 -0.042108692 -0.042112995 -0.042097114 -0.041966543 -0.04140839 -0.041456725 -0.042177461 -0.042195477 -0.041474935 -0.041310336 -0.040979248 -0.04043626 -0.040415559][-0.042430855 -0.042449303 -0.042448934 -0.042449541 -0.042557165 -0.042572379 -0.042083666 -0.042161465 -0.042164288 -0.04213427 -0.041117437 -0.040951256 -0.04066316 -0.040132977 -0.040117394][-0.041756466 -0.04116061 -0.040438235 -0.040545262 -0.040649273 -0.040870737 -0.0416316 -0.042323146 -0.043050081 -0.043068282 -0.042048693 -0.041877296 -0.040635459 -0.04010576 -0.040094566][-0.040878929 -0.040126897 -0.039248768 -0.039485082 -0.039829254 -0.040297106 -0.040925041 -0.041710529 -0.042597957 -0.042777151 -0.041802887 -0.041487291 -0.040557671 -0.040089682 -0.040077358][-0.040091731 -0.039334316 -0.038294718 -0.038531862 -0.03890276 -0.039677482 -0.04051144 -0.041300956 -0.042616487 -0.042789735 -0.041783828 -0.041463543 -0.040824424 -0.040355366 -0.040346857][-0.039613698 -0.038547706 -0.037381902 -0.037515931 -0.037926119 -0.038899086 -0.039979938 -0.041082937 -0.042529278 -0.042809684 -0.042575389 -0.042585991 -0.042896178 -0.04295044 -0.042936716][-0.039798807 -0.038627066 -0.037382115 -0.037577733 -0.038027432 -0.039043877 -0.0397928 -0.041002665 -0.042529158 -0.04280344 -0.042562433 -0.042563319 -0.042866711 -0.042919748 -0.042904295][-0.040218249 -0.039028533 -0.037771735 -0.037964076 -0.038291126 -0.039162874 -0.039846279 -0.040983643 -0.041606091 -0.04193126 -0.041719005 -0.041713804 -0.042020481 -0.042070381 -0.042970616][-0.04088803 -0.040070608 -0.039541837 -0.039629448 -0.039553009 -0.039392192 -0.038929276 -0.039685939 -0.039583165 -0.039853208 -0.03925414 -0.040081508 -0.041237675 -0.041289374 -0.042189665][-0.04054739 -0.039503153 -0.038554691 -0.038022995 -0.037076604 -0.036452681 -0.03608783 -0.036703229 -0.03643389 -0.036714938 -0.035550762 -0.03626658 -0.037409265 -0.037968513 -0.039465468][-0.040552814 -0.039603528 -0.03863081 -0.038030323 -0.037118472 -0.036132943 -0.035527773 -0.03594147 -0.034925286 -0.034635693 -0.033127096 -0.033084322 -0.03342744 -0.034284428 -0.036071919][-0.039029479 -0.038207613 -0.037472412 -0.037169665 -0.036419615 -0.035259552 -0.034514979 -0.034696743 -0.033657782 -0.033268936 -0.031941421 -0.032076713 -0.032500185 -0.033573695 -0.035532072][-0.037378363 -0.035917647 -0.034953151 -0.034099147 -0.032684695 -0.031026389 -0.03008933 -0.03055441 -0.030056868 -0.030461188 -0.030056927 -0.030922811 -0.031986639 -0.03340707 -0.035505667][-0.036643215 -0.035069004 -0.03388894 -0.032996271 -0.030917794 -0.027920935 -0.025976883 -0.025561858 -0.025179185 -0.025440779 -0.025878537 -0.028093316 -0.030598383 -0.033016048 -0.035279423][-0.036670338 -0.035334151 -0.0341481 -0.033250466 -0.031576008 -0.029403798 -0.028283261 -0.027605088 -0.0270339 -0.026700586 -0.026643241 -0.027828284 -0.029332627 -0.031779438 -0.034232583]]...]
INFO - root - 2017-12-09 05:50:44.891972: step 3010, loss = 1.43, batch loss = 0.69 (19.4 examples/sec; 0.412 sec/batch; 37h:40m:40s remains)
INFO - root - 2017-12-09 05:50:49.223922: step 3020, loss = 1.43, batch loss = 0.69 (18.4 examples/sec; 0.434 sec/batch; 39h:42m:50s remains)
INFO - root - 2017-12-09 05:50:53.596869: step 3030, loss = 1.42, batch loss = 0.68 (18.9 examples/sec; 0.424 sec/batch; 38h:49m:32s remains)
INFO - root - 2017-12-09 05:50:58.035785: step 3040, loss = 1.42, batch loss = 0.68 (18.3 examples/sec; 0.437 sec/batch; 39h:58m:13s remains)
INFO - root - 2017-12-09 05:51:02.400595: step 3050, loss = 1.43, batch loss = 0.68 (17.8 examples/sec; 0.450 sec/batch; 41h:13m:02s remains)
INFO - root - 2017-12-09 05:51:06.520192: step 3060, loss = 1.43, batch loss = 0.69 (18.7 examples/sec; 0.428 sec/batch; 39h:09m:21s remains)
INFO - root - 2017-12-09 05:51:10.932824: step 3070, loss = 1.44, batch loss = 0.69 (18.2 examples/sec; 0.438 sec/batch; 40h:06m:54s remains)
INFO - root - 2017-12-09 05:51:15.332988: step 3080, loss = 1.44, batch loss = 0.69 (18.2 examples/sec; 0.440 sec/batch; 40h:14m:51s remains)
INFO - root - 2017-12-09 05:51:19.726364: step 3090, loss = 1.44, batch loss = 0.69 (17.8 examples/sec; 0.449 sec/batch; 41h:05m:47s remains)
INFO - root - 2017-12-09 05:51:24.119995: step 3100, loss = 1.45, batch loss = 0.69 (18.2 examples/sec; 0.441 sec/batch; 40h:19m:36s remains)
2017-12-09 05:51:24.707172: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.043251321 -0.043251473 -0.043251622 -0.043251678 -0.0432517 -0.043251563 -0.043251466 -0.043251414 -0.043251392 -0.043251377 -0.043251321 -0.043251321 -0.043251321 -0.043250553 -0.043249421][-0.043251768 -0.043251768 -0.043251768 -0.043251768 -0.043251768 -0.043251768 -0.043251768 -0.043251768 -0.043251768 -0.043251768 -0.043251768 -0.043251768 -0.043251768 -0.043251768 -0.043251768][-0.043251768 -0.043251768 -0.043251768 -0.043251768 -0.043251768 -0.043251768 -0.043251768 -0.043251768 -0.043251768 -0.043251768 -0.043251768 -0.043251768 -0.043251768 -0.043251768 -0.043251768][-0.043251768 -0.043251768 -0.043251768 -0.043251768 -0.043251768 -0.043251768 -0.043251768 -0.043251768 -0.043251768 -0.043251768 -0.043251768 -0.043251768 -0.043251768 -0.043251768 -0.043251768][-0.043251768 -0.043251768 -0.043251768 -0.043251768 -0.043251768 -0.043251768 -0.043251768 -0.043251768 -0.043251768 -0.043251768 -0.043251768 -0.043251768 -0.043251768 -0.043251768 -0.043251768][-0.043251768 -0.043251768 -0.043251768 -0.043251768 -0.043251768 -0.043251768 -0.043251768 -0.043251768 -0.043251768 -0.043251768 -0.043251768 -0.043251768 -0.043251768 -0.043251768 -0.043251768][-0.043251768 -0.043251768 -0.043251768 -0.043251768 -0.043251768 -0.043251768 -0.043251768 -0.043251768 -0.043251768 -0.043251768 -0.043251768 -0.043251768 -0.043251768 -0.043251768 -0.043251768][-0.043251548 -0.043251768 -0.043251768 -0.043251768 -0.043251768 -0.043251768 -0.043251768 -0.043251768 -0.043251768 -0.043251768 -0.043251768 -0.043251768 -0.043251768 -0.043251768 -0.043251768][-0.04325119 -0.043251768 -0.043251768 -0.043251768 -0.043251768 -0.043251768 -0.043251768 -0.043251768 -0.043251768 -0.043251768 -0.043251768 -0.043251768 -0.043251768 -0.043251768 -0.043251768][-0.043251146 -0.043251634 -0.043251768 -0.043251768 -0.043251768 -0.043251768 -0.043251768 -0.043251768 -0.043251768 -0.043251768 -0.043251768 -0.043251768 -0.043251768 -0.043251768 -0.043251768][-0.043251205 -0.043251552 -0.043251768 -0.043251768 -0.043251768 -0.043251768 -0.043251768 -0.043251768 -0.043251768 -0.043251768 -0.043251768 -0.043251768 -0.043251768 -0.043251768 -0.043251768][-0.0432512 -0.043251637 -0.043251768 -0.043251768 -0.043251768 -0.043251768 -0.043251768 -0.043251768 -0.043251768 -0.043251768 -0.043251768 -0.043251768 -0.043251768 -0.043251768 -0.043251768][-0.043251149 -0.043251768 -0.043251768 -0.043251768 -0.043251768 -0.043251768 -0.043251768 -0.043251768 -0.043251768 -0.043251768 -0.043251768 -0.043251768 -0.043251768 -0.043251768 -0.043251768][-0.043251198 -0.043251768 -0.043251768 -0.043251768 -0.043251768 -0.043251768 -0.043251768 -0.043251768 -0.043251768 -0.043251768 -0.043251768 -0.043251768 -0.043251768 -0.043251768 -0.043251768][-0.043251321 -0.043251768 -0.043251768 -0.043251768 -0.043251768 -0.043251768 -0.043251768 -0.043251768 -0.043251768 -0.043251768 -0.043251768 -0.043251768 -0.043251768 -0.043251768 -0.043251768]]...]
INFO - root - 2017-12-09 05:51:29.111231: step 3110, loss = 1.45, batch loss = 0.69 (19.4 examples/sec; 0.413 sec/batch; 37h:45m:00s remains)
INFO - root - 2017-12-09 05:51:33.462467: step 3120, loss = 1.45, batch loss = 0.69 (19.5 examples/sec; 0.410 sec/batch; 37h:31m:43s remains)
INFO - root - 2017-12-09 05:51:37.791990: step 3130, loss = 1.45, batch loss = 0.69 (19.1 examples/sec; 0.420 sec/batch; 38h:25m:06s remains)
INFO - root - 2017-12-09 05:51:42.276050: step 3140, loss = 1.45, batch loss = 0.69 (17.6 examples/sec; 0.455 sec/batch; 41h:40m:11s remains)
INFO - root - 2017-12-09 05:51:46.717459: step 3150, loss = 1.46, batch loss = 0.69 (18.0 examples/sec; 0.444 sec/batch; 40h:39m:26s remains)
INFO - root - 2017-12-09 05:51:50.851556: step 3160, loss = 1.46, batch loss = 0.69 (19.2 examples/sec; 0.417 sec/batch; 38h:06m:38s remains)
INFO - root - 2017-12-09 05:51:55.317439: step 3170, loss = 1.46, batch loss = 0.69 (18.8 examples/sec; 0.425 sec/batch; 38h:54m:03s remains)
INFO - root - 2017-12-09 05:51:59.671021: step 3180, loss = 1.46, batch loss = 0.69 (18.3 examples/sec; 0.438 sec/batch; 40h:05m:22s remains)
INFO - root - 2017-12-09 05:52:04.050391: step 3190, loss = 1.46, batch loss = 0.69 (17.7 examples/sec; 0.452 sec/batch; 41h:18m:25s remains)
INFO - root - 2017-12-09 05:52:08.514713: step 3200, loss = 1.47, batch loss = 0.69 (17.8 examples/sec; 0.450 sec/batch; 41h:09m:38s remains)
2017-12-09 05:52:09.063382: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.031727325 -0.030524464 -0.02995842 -0.030466061 -0.031903237 -0.033810824 -0.035297822 -0.037070941 -0.039016906 -0.03993988 -0.039029602 -0.0371495 -0.0360723 -0.03568235 -0.035910908][-0.028998766 -0.027123824 -0.025615171 -0.025360452 -0.026710572 -0.029445406 -0.031618524 -0.034143023 -0.037106011 -0.039251748 -0.038590457 -0.037326381 -0.036326014 -0.035905041 -0.036110003][-0.0282068 -0.024287809 -0.022076847 -0.021158246 -0.022110755 -0.024700787 -0.027293447 -0.031205237 -0.034904819 -0.038036168 -0.038171936 -0.037629988 -0.036761671 -0.036991425 -0.037181079][-0.029318351 -0.025129586 -0.021672728 -0.018664872 -0.019192073 -0.021154907 -0.023515835 -0.026817823 -0.031378016 -0.035713829 -0.037627824 -0.039107777 -0.039430983 -0.0407206 -0.040724162][-0.031322733 -0.026881434 -0.021607362 -0.016718114 -0.015472699 -0.014343258 -0.015162162 -0.016250005 -0.020980438 -0.025635108 -0.028416213 -0.031852491 -0.033674471 -0.037031904 -0.037772935][-0.033794254 -0.030005671 -0.024929941 -0.018695015 -0.015427295 -0.011987852 -0.010050166 -0.0082708448 -0.010212556 -0.012934824 -0.01544931 -0.020007318 -0.023985306 -0.02971979 -0.032599077][-0.037009627 -0.034399927 -0.030194093 -0.024253145 -0.020253846 -0.016345711 -0.01378106 -0.011044525 -0.011088956 -0.01122921 -0.011418609 -0.014491446 -0.018611291 -0.024292203 -0.027804004][-0.039009411 -0.036929063 -0.033665873 -0.028496791 -0.024666583 -0.02020902 -0.017298946 -0.013990652 -0.013027133 -0.012196053 -0.01208978 -0.013471333 -0.016643232 -0.022350652 -0.02588157][-0.039760184 -0.03967502 -0.03712333 -0.032480016 -0.029053275 -0.024772225 -0.02150267 -0.016933214 -0.015240928 -0.013818905 -0.013187941 -0.014024859 -0.017062606 -0.0211819 -0.02465949][-0.039919447 -0.04009096 -0.038029302 -0.03522424 -0.032227095 -0.028427806 -0.025308991 -0.021332132 -0.019154646 -0.016680604 -0.015598532 -0.015913162 -0.01870691 -0.021707706 -0.025176149][-0.039423756 -0.040349912 -0.039975729 -0.038980976 -0.037763413 -0.036207948 -0.03458222 -0.032671481 -0.030218642 -0.02730765 -0.02536086 -0.023724122 -0.025024181 -0.026069451 -0.028417788][-0.039662596 -0.040182665 -0.040099554 -0.040130287 -0.04028615 -0.040616896 -0.041019756 -0.041345045 -0.040743712 -0.03933787 -0.037897225 -0.035345219 -0.034625657 -0.033487651 -0.034070581][-0.039814029 -0.039967675 -0.039875276 -0.0398987 -0.039884008 -0.040922754 -0.041099273 -0.0412901 -0.041253157 -0.041407716 -0.041609984 -0.040555846 -0.039766483 -0.038834307 -0.038907494][-0.040646587 -0.040942986 -0.040855229 -0.04086924 -0.040654924 -0.041420944 -0.041261092 -0.041271802 -0.041225772 -0.041386366 -0.041626602 -0.041648675 -0.041730747 -0.040807508 -0.0408898][-0.040715974 -0.041061722 -0.040969349 -0.041121583 -0.040890947 -0.041628927 -0.041434955 -0.041395858 -0.041353032 -0.041368794 -0.041609198 -0.041826393 -0.042042919 -0.041985914 -0.042125702]]...]
INFO - root - 2017-12-09 05:52:13.433695: step 3210, loss = 1.46, batch loss = 0.69 (19.1 examples/sec; 0.418 sec/batch; 38h:15m:13s remains)
INFO - root - 2017-12-09 05:52:17.789587: step 3220, loss = 1.47, batch loss = 0.69 (17.9 examples/sec; 0.447 sec/batch; 40h:54m:20s remains)
INFO - root - 2017-12-09 05:52:22.046046: step 3230, loss = 1.46, batch loss = 0.68 (18.6 examples/sec; 0.429 sec/batch; 39h:16m:55s remains)
INFO - root - 2017-12-09 05:52:26.404886: step 3240, loss = 1.47, batch loss = 0.69 (17.9 examples/sec; 0.446 sec/batch; 40h:46m:15s remains)
INFO - root - 2017-12-09 05:52:30.839859: step 3250, loss = 1.47, batch loss = 0.69 (17.0 examples/sec; 0.469 sec/batch; 42h:55m:05s remains)
INFO - root - 2017-12-09 05:52:34.978208: step 3260, loss = 1.47, batch loss = 0.69 (18.3 examples/sec; 0.438 sec/batch; 40h:03m:40s remains)
INFO - root - 2017-12-09 05:52:39.409218: step 3270, loss = 1.47, batch loss = 0.69 (18.6 examples/sec; 0.429 sec/batch; 39h:16m:19s remains)
INFO - root - 2017-12-09 05:52:43.827674: step 3280, loss = 1.48, batch loss = 0.69 (17.7 examples/sec; 0.451 sec/batch; 41h:14m:50s remains)
INFO - root - 2017-12-09 05:52:48.217053: step 3290, loss = 1.48, batch loss = 0.69 (18.3 examples/sec; 0.438 sec/batch; 40h:04m:04s remains)
INFO - root - 2017-12-09 05:52:52.587345: step 3300, loss = 1.48, batch loss = 0.69 (18.9 examples/sec; 0.423 sec/batch; 38h:38m:35s remains)
2017-12-09 05:52:53.143048: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.041261345 -0.041261371 -0.041261423 -0.041261423 -0.041261423 -0.041261423 -0.041261423 -0.041261364 -0.041261312 -0.04126095 -0.041260459 -0.041260544 -0.041260641 -0.041260343 -0.041259527][-0.041261423 -0.041261423 -0.041261423 -0.041261423 -0.041261423 -0.041261386 -0.041261394 -0.041261405 -0.041261416 -0.041261423 -0.041261423 -0.041261423 -0.041261423 -0.041261423 -0.041261423][-0.041261423 -0.041261423 -0.041261423 -0.041261423 -0.041261423 -0.041261423 -0.041261423 -0.041261423 -0.041261423 -0.041261423 -0.041261423 -0.041261423 -0.041261423 -0.041261423 -0.041261423][-0.041261423 -0.041261423 -0.041261423 -0.041261423 -0.041261423 -0.041261423 -0.041261423 -0.041261423 -0.041261423 -0.041261423 -0.041261423 -0.041261423 -0.041261423 -0.041261423 -0.041261423][-0.041261423 -0.041261423 -0.041261423 -0.041261423 -0.041261423 -0.041261423 -0.041261423 -0.041261423 -0.041261423 -0.041261423 -0.041261423 -0.041261423 -0.041261423 -0.041261423 -0.041261423][-0.041261423 -0.041261423 -0.041261423 -0.041261423 -0.041261423 -0.041261423 -0.041261423 -0.041261423 -0.041261423 -0.041261423 -0.041261423 -0.041261423 -0.041261423 -0.041261423 -0.041261423][-0.041261423 -0.041261423 -0.041261423 -0.041261423 -0.041261423 -0.041261423 -0.041261423 -0.041261423 -0.041261423 -0.041261423 -0.041261423 -0.041261423 -0.041261423 -0.041261423 -0.041261423][-0.041261423 -0.041261423 -0.041261423 -0.041261423 -0.041261423 -0.041261423 -0.041261423 -0.041261423 -0.041261423 -0.041261423 -0.041261423 -0.041261423 -0.041261423 -0.041261423 -0.041261423][-0.041261423 -0.041261423 -0.041261423 -0.041261423 -0.041261423 -0.041261423 -0.041261423 -0.041261423 -0.041261423 -0.041261423 -0.041261423 -0.041261423 -0.041261423 -0.041261423 -0.041261423][-0.041261386 -0.041261423 -0.041261423 -0.041261423 -0.041261423 -0.041261423 -0.041261423 -0.041261423 -0.041261423 -0.041261423 -0.041261423 -0.041261423 -0.041261423 -0.041261423 -0.041261423][-0.04126066 -0.041261423 -0.041261423 -0.041261423 -0.041261423 -0.041261423 -0.041261423 -0.041261423 -0.041261423 -0.041261423 -0.041261423 -0.041261423 -0.041261423 -0.041261423 -0.041261423][-0.041260034 -0.041261092 -0.041261423 -0.041261423 -0.041261423 -0.041261423 -0.041261423 -0.041261423 -0.041261423 -0.041261423 -0.041261423 -0.041261423 -0.041261423 -0.041261423 -0.041261423][-0.041260716 -0.041260507 -0.041261423 -0.041261423 -0.041261423 -0.041261423 -0.041261423 -0.041261423 -0.041261423 -0.041261423 -0.041261423 -0.041261423 -0.041261423 -0.041261423 -0.041261423][-0.041261297 -0.041260246 -0.041261423 -0.041261423 -0.041261423 -0.041261423 -0.041261423 -0.041261423 -0.041261423 -0.041261423 -0.041261423 -0.041261423 -0.041261423 -0.041261423 -0.041261423][-0.041261256 -0.041260485 -0.041261423 -0.041261423 -0.041261423 -0.041261423 -0.041261423 -0.041261423 -0.041261423 -0.041261423 -0.041261423 -0.041261423 -0.041261423 -0.041261423 -0.041261423]]...]
INFO - root - 2017-12-09 05:52:57.525174: step 3310, loss = 1.48, batch loss = 0.69 (17.7 examples/sec; 0.452 sec/batch; 41h:18m:53s remains)
INFO - root - 2017-12-09 05:53:01.974532: step 3320, loss = 1.48, batch loss = 0.69 (17.6 examples/sec; 0.454 sec/batch; 41h:33m:28s remains)
INFO - root - 2017-12-09 05:53:06.359196: step 3330, loss = 1.48, batch loss = 0.69 (18.8 examples/sec; 0.426 sec/batch; 38h:54m:46s remains)
INFO - root - 2017-12-09 05:53:10.780880: step 3340, loss = 1.48, batch loss = 0.69 (18.5 examples/sec; 0.433 sec/batch; 39h:36m:44s remains)
INFO - root - 2017-12-09 05:53:15.116484: step 3350, loss = 1.49, batch loss = 0.69 (18.0 examples/sec; 0.443 sec/batch; 40h:32m:50s remains)
INFO - root - 2017-12-09 05:53:19.231909: step 3360, loss = 1.48, batch loss = 0.68 (19.2 examples/sec; 0.417 sec/batch; 38h:08m:00s remains)
INFO - root - 2017-12-09 05:53:23.573757: step 3370, loss = 1.49, batch loss = 0.69 (18.4 examples/sec; 0.436 sec/batch; 39h:51m:01s remains)
INFO - root - 2017-12-09 05:53:27.967475: step 3380, loss = 1.49, batch loss = 0.69 (17.3 examples/sec; 0.461 sec/batch; 42h:11m:13s remains)
INFO - root - 2017-12-09 05:53:32.350440: step 3390, loss = 1.49, batch loss = 0.69 (18.5 examples/sec; 0.432 sec/batch; 39h:30m:37s remains)
INFO - root - 2017-12-09 05:53:36.667682: step 3400, loss = 1.49, batch loss = 0.69 (18.3 examples/sec; 0.438 sec/batch; 40h:01m:30s remains)
2017-12-09 05:53:37.257952: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.041260548 -0.041260581 -0.041260615 -0.041260511 -0.041260362 -0.041260246 -0.041260291 -0.041260328 -0.041260254 -0.041260295 -0.041260414 -0.041260626 -0.041260608 -0.041259978 -0.041258633][-0.041260626 -0.041260626 -0.041260626 -0.041260626 -0.041260626 -0.041260626 -0.041260626 -0.041260626 -0.041260626 -0.041260626 -0.041260626 -0.041260626 -0.041260626 -0.041260626 -0.041260626][-0.041260626 -0.041260626 -0.041260626 -0.041260626 -0.041260626 -0.041260626 -0.041260626 -0.041260626 -0.041260626 -0.041260626 -0.041260626 -0.041260626 -0.041260626 -0.041260626 -0.041260626][-0.041260626 -0.041260626 -0.041260626 -0.041260626 -0.041260626 -0.041260626 -0.041260626 -0.041260626 -0.041260626 -0.041260626 -0.041260626 -0.041260626 -0.041260626 -0.041260626 -0.041260626][-0.041260626 -0.041260626 -0.041260626 -0.041260626 -0.041260626 -0.041260626 -0.041260626 -0.041260626 -0.041260626 -0.041260626 -0.041260626 -0.041260626 -0.041260626 -0.041260626 -0.041260626][-0.041260626 -0.041260626 -0.041260626 -0.041260626 -0.041260626 -0.041260626 -0.041260626 -0.041260626 -0.041260626 -0.041260626 -0.041260626 -0.041260626 -0.041260626 -0.041260626 -0.041260626][-0.041260626 -0.041260626 -0.041260626 -0.041260626 -0.041260626 -0.041260626 -0.041260626 -0.041260626 -0.041260626 -0.041260626 -0.041260626 -0.041260626 -0.041260626 -0.041260626 -0.041260626][-0.041260626 -0.041260626 -0.041260626 -0.041260626 -0.041260626 -0.041260626 -0.041260626 -0.041260626 -0.041260626 -0.041260626 -0.041260626 -0.041260626 -0.041260626 -0.041260626 -0.041260626][-0.041260626 -0.041260626 -0.041260626 -0.041260626 -0.041260626 -0.041260626 -0.041260626 -0.041260626 -0.041260626 -0.041260626 -0.041260626 -0.041260626 -0.041260626 -0.041260626 -0.041260626][-0.041260626 -0.041260626 -0.041260626 -0.041260626 -0.041260626 -0.041260626 -0.041260626 -0.041260626 -0.041260626 -0.041260626 -0.041260626 -0.041260626 -0.041260626 -0.041260626 -0.041260626][-0.041259911 -0.041260626 -0.041260626 -0.041260626 -0.041260626 -0.041260626 -0.041260626 -0.041260626 -0.041260626 -0.041260626 -0.041260626 -0.041260626 -0.041260626 -0.041260626 -0.041260626][-0.041259736 -0.041260239 -0.041260626 -0.041260626 -0.041260626 -0.041260626 -0.041260626 -0.041260626 -0.041260626 -0.041260626 -0.041260626 -0.041260626 -0.041260626 -0.041260626 -0.041260626][-0.041259978 -0.041259605 -0.041260626 -0.041260626 -0.041260626 -0.041260626 -0.041260626 -0.041260626 -0.041260626 -0.041260626 -0.041260626 -0.041260626 -0.041260626 -0.041260626 -0.041260626][-0.041259684 -0.041259132 -0.041260626 -0.041260626 -0.041260626 -0.041260626 -0.041260626 -0.041260626 -0.041260626 -0.041260626 -0.041260626 -0.041260626 -0.041260626 -0.041260626 -0.041260626][-0.041260138 -0.041259203 -0.041260626 -0.041260626 -0.041260626 -0.041260626 -0.041260626 -0.041260626 -0.041260626 -0.041260626 -0.041260626 -0.041260626 -0.041260626 -0.041260626 -0.041260626]]...]
INFO - root - 2017-12-09 05:53:41.552569: step 3410, loss = 1.49, batch loss = 0.69 (19.1 examples/sec; 0.419 sec/batch; 38h:19m:43s remains)
INFO - root - 2017-12-09 05:53:45.994092: step 3420, loss = 1.49, batch loss = 0.68 (17.8 examples/sec; 0.450 sec/batch; 41h:07m:45s remains)
INFO - root - 2017-12-09 05:53:50.426138: step 3430, loss = 1.50, batch loss = 0.69 (17.1 examples/sec; 0.468 sec/batch; 42h:47m:44s remains)
INFO - root - 2017-12-09 05:53:54.976568: step 3440, loss = 1.50, batch loss = 0.69 (18.9 examples/sec; 0.423 sec/batch; 38h:40m:58s remains)
INFO - root - 2017-12-09 05:53:59.409284: step 3450, loss = 1.50, batch loss = 0.69 (17.7 examples/sec; 0.452 sec/batch; 41h:17m:51s remains)
INFO - root - 2017-12-09 05:54:03.549455: step 3460, loss = 1.51, batch loss = 0.69 (29.1 examples/sec; 0.275 sec/batch; 25h:07m:06s remains)
INFO - root - 2017-12-09 05:54:07.983065: step 3470, loss = 1.51, batch loss = 0.69 (18.1 examples/sec; 0.442 sec/batch; 40h:22m:03s remains)
INFO - root - 2017-12-09 05:54:12.448654: step 3480, loss = 1.51, batch loss = 0.69 (17.3 examples/sec; 0.463 sec/batch; 42h:18m:29s remains)
INFO - root - 2017-12-09 05:54:16.896407: step 3490, loss = 1.51, batch loss = 0.69 (18.8 examples/sec; 0.424 sec/batch; 38h:47m:16s remains)
INFO - root - 2017-12-09 05:54:21.357281: step 3500, loss = 1.51, batch loss = 0.69 (18.3 examples/sec; 0.437 sec/batch; 39h:58m:45s remains)
2017-12-09 05:54:21.867638: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.040237542 -0.04023711 -0.040238038 -0.041260105 -0.041261014 -0.041260865 -0.041260619 -0.041260473 -0.041260343 -0.041260328 -0.04126031 -0.041017987 -0.041018147 -0.041018233 -0.0410181][-0.038094494 -0.03809455 -0.03809455 -0.039824754 -0.039824754 -0.041261014 -0.041261014 -0.041261014 -0.041261014 -0.041261014 -0.041261014 -0.041018702 -0.041018702 -0.041018702 -0.041018702][-0.037342966 -0.037343185 -0.037343185 -0.039073389 -0.039073389 -0.041261014 -0.041261014 -0.041261014 -0.041261014 -0.041261014 -0.041261014 -0.041018702 -0.041018702 -0.041018702 -0.041018702][-0.037356507 -0.03735666 -0.037351854 -0.039077461 -0.039077461 -0.041261014 -0.041261014 -0.041261014 -0.041261014 -0.041261014 -0.041261014 -0.041261014 -0.041261014 -0.041261014 -0.041261014][-0.037360784 -0.037360962 -0.03735631 -0.039081972 -0.039081987 -0.041261014 -0.041261014 -0.041261014 -0.041261014 -0.041261014 -0.041261014 -0.041261014 -0.041261014 -0.041261014 -0.041261014][-0.037368476 -0.037364367 -0.037360229 -0.039086193 -0.0390864 -0.041260958 -0.041261014 -0.041261014 -0.041261014 -0.041261014 -0.041261014 -0.041261014 -0.041261014 -0.041261014 -0.041261014][-0.038393494 -0.038389429 -0.038385361 -0.039089654 -0.039090209 -0.041260507 -0.041260958 -0.041261014 -0.041261014 -0.041261014 -0.041261014 -0.041261014 -0.041261014 -0.041261014 -0.041261014][-0.040547993 -0.0405482 -0.040543411 -0.040538978 -0.040539451 -0.041268889 -0.041265175 -0.041261014 -0.041261014 -0.041261014 -0.041261014 -0.041261014 -0.041261014 -0.041261014 -0.041261014][-0.04100332 -0.04100332 -0.040998138 -0.040994484 -0.040994458 -0.041269567 -0.041265726 -0.041261997 -0.041262005 -0.041261014 -0.041261014 -0.041261014 -0.041261014 -0.041261014 -0.041261014][-0.040990613 -0.04099058 -0.040990636 -0.040991616 -0.04099134 -0.041270122 -0.041265547 -0.041261759 -0.041261863 -0.041260995 -0.041261014 -0.041261014 -0.041261014 -0.041261014 -0.041261014][-0.040986173 -0.040986065 -0.040986516 -0.040987525 -0.040987406 -0.041270845 -0.041265842 -0.041264229 -0.041264374 -0.041263517 -0.041263524 -0.041263524 -0.041263524 -0.041261014 -0.041261014][-0.040977124 -0.040981758 -0.040981922 -0.040982913 -0.040982917 -0.041275851 -0.041275684 -0.041273836 -0.041273888 -0.041272931 -0.041272931 -0.041268222 -0.041263524 -0.041261014 -0.041261002][-0.039662022 -0.03966669 -0.039666839 -0.039667837 -0.039667889 -0.041275926 -0.041275911 -0.041244123 -0.041244123 -0.041243128 -0.041243128 -0.041238423 -0.041233722 -0.041261 -0.041261][-0.039652582 -0.0387652 -0.038765281 -0.038766276 -0.038766336 -0.040379051 -0.04038379 -0.041244123 -0.041244123 -0.041243128 -0.041243128 -0.041238423 -0.041233722 -0.041261014 -0.041261006][-0.039950185 -0.039062969 -0.039063007 -0.038051587 -0.036759086 -0.038074192 -0.038078878 -0.038944032 -0.038944032 -0.039955445 -0.041247938 -0.041243229 -0.041238531 -0.041261014 -0.041261014]]...]
INFO - root - 2017-12-09 05:54:26.288000: step 3510, loss = 1.52, batch loss = 0.69 (18.7 examples/sec; 0.428 sec/batch; 39h:07m:16s remains)
INFO - root - 2017-12-09 05:54:30.652106: step 3520, loss = 1.52, batch loss = 0.69 (18.2 examples/sec; 0.440 sec/batch; 40h:11m:33s remains)
INFO - root - 2017-12-09 05:54:35.018862: step 3530, loss = 1.51, batch loss = 0.69 (18.4 examples/sec; 0.436 sec/batch; 39h:49m:42s remains)
INFO - root - 2017-12-09 05:54:39.507226: step 3540, loss = 1.52, batch loss = 0.69 (18.3 examples/sec; 0.437 sec/batch; 39h:54m:23s remains)
INFO - root - 2017-12-09 05:54:43.901520: step 3550, loss = 1.52, batch loss = 0.69 (18.0 examples/sec; 0.444 sec/batch; 40h:36m:16s remains)
INFO - root - 2017-12-09 05:54:48.077976: step 3560, loss = 1.52, batch loss = 0.69 (37.3 examples/sec; 0.215 sec/batch; 19h:37m:20s remains)
INFO - root - 2017-12-09 05:54:52.399548: step 3570, loss = 1.53, batch loss = 0.70 (19.1 examples/sec; 0.419 sec/batch; 38h:19m:21s remains)
INFO - root - 2017-12-09 05:54:56.755881: step 3580, loss = 1.53, batch loss = 0.69 (18.4 examples/sec; 0.435 sec/batch; 39h:44m:29s remains)
INFO - root - 2017-12-09 05:55:01.155354: step 3590, loss = 1.53, batch loss = 0.69 (18.0 examples/sec; 0.444 sec/batch; 40h:34m:22s remains)
INFO - root - 2017-12-09 05:55:05.618827: step 3600, loss = 1.53, batch loss = 0.69 (18.4 examples/sec; 0.435 sec/batch; 39h:43m:12s remains)
2017-12-09 05:55:06.094520: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.041259997 -0.041260254 -0.041260518 -0.041260518 -0.041260518 -0.041260522 -0.041260533 -0.041260537 -0.041260533 -0.041260522 -0.041260254 -0.041259985 -0.041259985 -0.041259985 -0.041259989][-0.041260622 -0.041260622 -0.041260622 -0.041260622 -0.041260622 -0.041260622 -0.041260622 -0.041260622 -0.041260622 -0.041260622 -0.041260622 -0.041260622 -0.041260622 -0.041260622 -0.041260622][-0.041260622 -0.041260622 -0.041260622 -0.041260622 -0.041260622 -0.041260622 -0.041260622 -0.041260622 -0.041260622 -0.041260622 -0.041260622 -0.041260622 -0.041260622 -0.041260622 -0.041260622][-0.041260622 -0.041260622 -0.041260622 -0.041260622 -0.041260622 -0.041260622 -0.041260622 -0.041260622 -0.041260622 -0.041260622 -0.041260622 -0.041260622 -0.041260622 -0.041260622 -0.041260622][-0.041260622 -0.041260622 -0.041260622 -0.041260622 -0.041260622 -0.041260622 -0.041260622 -0.041260622 -0.041260622 -0.041260622 -0.041260622 -0.041260622 -0.041260622 -0.041260622 -0.041260622][-0.041260622 -0.041260622 -0.041260622 -0.041260622 -0.041260622 -0.041260622 -0.041260622 -0.041260622 -0.041260622 -0.041260622 -0.041260622 -0.041260622 -0.041260622 -0.041260622 -0.041260622][-0.041260321 -0.041260622 -0.041260622 -0.041260622 -0.041260622 -0.041260622 -0.041260622 -0.041260622 -0.041260622 -0.041260622 -0.041260622 -0.041260622 -0.041260622 -0.041260622 -0.041260622][-0.041260388 -0.041260622 -0.041260622 -0.041260622 -0.041260622 -0.041260622 -0.041260622 -0.041260622 -0.041260622 -0.041260622 -0.041260622 -0.041260622 -0.041260622 -0.041260622 -0.041260622][-0.0412592 -0.0412604 -0.041260622 -0.041260622 -0.041260622 -0.041260622 -0.041260622 -0.041260622 -0.041260622 -0.041260622 -0.041260622 -0.041260622 -0.041260622 -0.041260622 -0.041260622][-0.041259252 -0.0412605 -0.041260622 -0.041260622 -0.041260622 -0.041260622 -0.041260622 -0.041260622 -0.041260622 -0.041260622 -0.041260622 -0.041260622 -0.041260622 -0.041260622 -0.041260622][-0.041259926 -0.041260384 -0.041260377 -0.041260567 -0.04126054 -0.041260507 -0.041260514 -0.041260447 -0.041260466 -0.041260492 -0.041260511 -0.041260533 -0.041260537 -0.041260533 -0.041260507][-0.041260328 -0.041260496 -0.041260481 -0.041260354 -0.041260324 -0.041260466 -0.041260451 -0.041260429 -0.041260421 -0.041260339 -0.041260362 -0.041260403 -0.041260418 -0.041260403 -0.041260384][-0.041260451 -0.041260622 -0.041260619 -0.041260552 -0.041260526 -0.04126044 -0.041260414 -0.041260485 -0.04126047 -0.041260462 -0.041260485 -0.041260481 -0.0412605 -0.041260507 -0.041260496][-0.041260615 -0.041260622 -0.041260622 -0.041260622 -0.041260622 -0.041260622 -0.041260622 -0.041260615 -0.041260608 -0.041260622 -0.041260622 -0.041260622 -0.041260622 -0.041260622 -0.041260622][-0.041260578 -0.041260622 -0.041260622 -0.041260622 -0.041260622 -0.041260622 -0.041260622 -0.041260622 -0.041260622 -0.041260622 -0.041260622 -0.041260622 -0.041260622 -0.041260622 -0.041260622]]...]
INFO - root - 2017-12-09 05:55:10.420700: step 3610, loss = 1.53, batch loss = 0.69 (18.1 examples/sec; 0.442 sec/batch; 40h:23m:16s remains)
INFO - root - 2017-12-09 05:55:14.811196: step 3620, loss = 1.53, batch loss = 0.69 (18.3 examples/sec; 0.437 sec/batch; 39h:52m:57s remains)
INFO - root - 2017-12-09 05:55:19.231409: step 3630, loss = 1.54, batch loss = 0.69 (17.4 examples/sec; 0.461 sec/batch; 42h:06m:20s remains)
INFO - root - 2017-12-09 05:55:23.540761: step 3640, loss = 1.54, batch loss = 0.69 (18.2 examples/sec; 0.440 sec/batch; 40h:11m:24s remains)
INFO - root - 2017-12-09 05:55:27.889573: step 3650, loss = 1.54, batch loss = 0.69 (18.0 examples/sec; 0.445 sec/batch; 40h:41m:15s remains)
INFO - root - 2017-12-09 05:55:32.361962: step 3660, loss = 1.54, batch loss = 0.69 (18.0 examples/sec; 0.445 sec/batch; 40h:36m:43s remains)
INFO - root - 2017-12-09 05:55:36.546765: step 3670, loss = 1.54, batch loss = 0.69 (18.6 examples/sec; 0.430 sec/batch; 39h:15m:42s remains)
INFO - root - 2017-12-09 05:55:40.977319: step 3680, loss = 1.54, batch loss = 0.69 (17.4 examples/sec; 0.461 sec/batch; 42h:03m:46s remains)
INFO - root - 2017-12-09 05:55:45.452656: step 3690, loss = 1.55, batch loss = 0.69 (18.4 examples/sec; 0.434 sec/batch; 39h:36m:28s remains)
INFO - root - 2017-12-09 05:55:49.832313: step 3700, loss = 1.55, batch loss = 0.69 (18.3 examples/sec; 0.438 sec/batch; 40h:01m:31s remains)
2017-12-09 05:55:50.402790: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.040823784 -0.040824324 -0.040825836 -0.040827822 -0.040827826 -0.040826995 -0.040826641 -0.040826786 -0.040826883 -0.040827561 -0.040827733 -0.040825125 -0.040824074 -0.040822081 -0.040807426][-0.040827848 -0.040827848 -0.040827848 -0.040827848 -0.040827848 -0.040827446 -0.040827043 -0.040827006 -0.04082698 -0.040827014 -0.04082692 -0.04082606 -0.040825304 -0.040825181 -0.040823564][-0.040827848 -0.040827848 -0.040827848 -0.040827848 -0.040827848 -0.040827848 -0.040827848 -0.040827848 -0.040827848 -0.040827848 -0.040827848 -0.040827848 -0.040827848 -0.040827848 -0.040827017][-0.040827848 -0.040827848 -0.040827848 -0.040827848 -0.040827848 -0.040827848 -0.040827848 -0.040827848 -0.040827848 -0.040827848 -0.040827848 -0.040827848 -0.040827848 -0.040827848 -0.040827595][-0.040827848 -0.040827848 -0.040827848 -0.040827848 -0.040827848 -0.040827848 -0.040827848 -0.040827848 -0.040827848 -0.040827848 -0.040827848 -0.040827848 -0.040827848 -0.040827848 -0.040827848][-0.040827844 -0.040827848 -0.040827848 -0.040827848 -0.040827848 -0.040827848 -0.040827848 -0.040827848 -0.040827848 -0.040827848 -0.040827848 -0.040827848 -0.040827848 -0.040827848 -0.040827848][-0.040827725 -0.040827848 -0.040827848 -0.040827848 -0.040827848 -0.040827848 -0.040827848 -0.040827848 -0.040827848 -0.040827848 -0.040827848 -0.040827848 -0.040827848 -0.040827848 -0.040827848][-0.040826365 -0.040827848 -0.040827848 -0.040827848 -0.040827848 -0.040827848 -0.040827848 -0.040827848 -0.040827848 -0.040827848 -0.040827848 -0.040827848 -0.040827848 -0.040827848 -0.040827848][-0.04082476 -0.040827848 -0.040827848 -0.040827848 -0.040827848 -0.040827848 -0.040827848 -0.040827848 -0.040827848 -0.040827848 -0.040827848 -0.040827848 -0.040827848 -0.040827848 -0.040827848][-0.040823475 -0.040827837 -0.040827848 -0.040827848 -0.040827848 -0.040827848 -0.040827848 -0.040827848 -0.040827848 -0.040827848 -0.040827848 -0.040827848 -0.040827848 -0.040827848 -0.040827848][-0.040822145 -0.040827483 -0.040827848 -0.040827848 -0.040827848 -0.040827848 -0.040827848 -0.040827848 -0.040827848 -0.040827848 -0.040827848 -0.040827848 -0.040827848 -0.040827848 -0.040827848][-0.040822249 -0.04082714 -0.040827848 -0.040827848 -0.040827848 -0.040827848 -0.040827848 -0.040827848 -0.040827848 -0.040827848 -0.040827848 -0.040827848 -0.040827848 -0.040827848 -0.040827848][-0.04082096 -0.040826496 -0.040827848 -0.040827848 -0.040827848 -0.040827848 -0.040827848 -0.040827848 -0.040827848 -0.040827848 -0.040827848 -0.040827848 -0.040827848 -0.040827848 -0.040827848][-0.040822629 -0.040826622 -0.040827848 -0.040827848 -0.040827848 -0.040827848 -0.040827848 -0.040827848 -0.040827848 -0.040827848 -0.040827848 -0.040827848 -0.040827848 -0.040827848 -0.040827848][-0.040822063 -0.040823918 -0.040827848 -0.040827848 -0.040827848 -0.040827792 -0.040827792 -0.040827721 -0.040827721 -0.040827766 -0.040827658 -0.040827733 -0.040827766 -0.040827766 -0.04082777]]...]
INFO - root - 2017-12-09 05:55:54.740494: step 3710, loss = 1.55, batch loss = 0.69 (18.4 examples/sec; 0.436 sec/batch; 39h:48m:47s remains)
INFO - root - 2017-12-09 05:55:59.097346: step 3720, loss = 1.55, batch loss = 0.69 (18.2 examples/sec; 0.438 sec/batch; 40h:02m:22s remains)
INFO - root - 2017-12-09 05:56:03.381517: step 3730, loss = 1.55, batch loss = 0.69 (18.7 examples/sec; 0.427 sec/batch; 39h:01m:59s remains)
INFO - root - 2017-12-09 05:56:07.692792: step 3740, loss = 1.55, batch loss = 0.69 (17.8 examples/sec; 0.449 sec/batch; 41h:01m:54s remains)
INFO - root - 2017-12-09 05:56:12.047289: step 3750, loss = 1.56, batch loss = 0.69 (18.4 examples/sec; 0.435 sec/batch; 39h:44m:35s remains)
INFO - root - 2017-12-09 05:56:16.423904: step 3760, loss = 1.56, batch loss = 0.69 (17.4 examples/sec; 0.461 sec/batch; 42h:04m:10s remains)
INFO - root - 2017-12-09 05:56:20.560340: step 3770, loss = 1.56, batch loss = 0.69 (17.7 examples/sec; 0.452 sec/batch; 41h:16m:42s remains)
INFO - root - 2017-12-09 05:56:24.979854: step 3780, loss = 1.56, batch loss = 0.69 (20.9 examples/sec; 0.383 sec/batch; 34h:58m:19s remains)
INFO - root - 2017-12-09 05:56:29.343081: step 3790, loss = 1.55, batch loss = 0.68 (17.9 examples/sec; 0.447 sec/batch; 40h:48m:39s remains)
INFO - root - 2017-12-09 05:56:33.733150: step 3800, loss = 1.56, batch loss = 0.69 (17.6 examples/sec; 0.455 sec/batch; 41h:34m:25s remains)
2017-12-09 05:56:34.286438: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.040093329 -0.04009359 -0.040093277 -0.040092561 -0.040092062 -0.040091865 -0.040091734 -0.040091544 -0.040091116 -0.040090483 -0.040090062 -0.040089656 -0.040089149 -0.040088587 -0.040087815][-0.040093787 -0.040093742 -0.040093493 -0.040092967 -0.040092621 -0.040092468 -0.040092368 -0.040092144 -0.040091723 -0.04009131 -0.040091068 -0.040090859 -0.040090524 -0.040090069 -0.040089268][-0.040093765 -0.040093724 -0.040093616 -0.040093452 -0.040093344 -0.040093277 -0.040093236 -0.04009315 -0.040092938 -0.040092662 -0.0400925 -0.04009233 -0.0400918 -0.040090907 -0.040089838][-0.040093821 -0.040093821 -0.040093821 -0.040093821 -0.040093821 -0.040093817 -0.040093783 -0.04009375 -0.04009369 -0.040093575 -0.040093496 -0.040093433 -0.040093053 -0.040092111 -0.040090557][-0.040093821 -0.040093821 -0.040093821 -0.040093821 -0.040093821 -0.040093821 -0.040093821 -0.040093821 -0.040093821 -0.040093817 -0.040093768 -0.040093686 -0.040093474 -0.040092729 -0.040091116][-0.040093821 -0.040093821 -0.040093821 -0.040093821 -0.040093821 -0.040093821 -0.040093821 -0.040093821 -0.040093821 -0.040093821 -0.040093821 -0.040093765 -0.040093664 -0.040093135 -0.040091772][-0.040093821 -0.040093821 -0.040093821 -0.040093813 -0.040093802 -0.040093794 -0.040093821 -0.040093821 -0.040093821 -0.040093821 -0.040093821 -0.040093787 -0.040093664 -0.040093392 -0.040092442][-0.040093821 -0.040093821 -0.040093821 -0.040093821 -0.040093791 -0.040093753 -0.04009375 -0.040093791 -0.040093821 -0.040093821 -0.040093821 -0.040093821 -0.040093724 -0.040093571 -0.040093176][-0.040093821 -0.040093821 -0.040093817 -0.040093821 -0.040093813 -0.04009378 -0.040093753 -0.040093791 -0.040093821 -0.040093821 -0.040093821 -0.040093821 -0.040093791 -0.040093642 -0.040093541][-0.040093821 -0.040093821 -0.040093821 -0.040093809 -0.040093813 -0.040093806 -0.040093794 -0.040093809 -0.040093821 -0.040093821 -0.040093821 -0.040093821 -0.040093821 -0.040093753 -0.040093742][-0.040093821 -0.040093821 -0.040093821 -0.040093821 -0.040093821 -0.040093821 -0.040093821 -0.040093821 -0.040093821 -0.040093821 -0.040093821 -0.040093821 -0.040093821 -0.040093821 -0.040093821][-0.040093821 -0.040093821 -0.040093821 -0.040093821 -0.040093821 -0.040093821 -0.040093821 -0.040093821 -0.040093821 -0.040093821 -0.040093821 -0.040093821 -0.040093821 -0.040093821 -0.040093821][-0.040093821 -0.040093821 -0.040093821 -0.040093821 -0.040093821 -0.040093821 -0.040093821 -0.040093821 -0.040093821 -0.040093821 -0.040093821 -0.040093821 -0.040093821 -0.040093821 -0.040093821][-0.040093821 -0.040093821 -0.040093821 -0.040093821 -0.040093821 -0.040093821 -0.040093821 -0.040093821 -0.040093821 -0.040093821 -0.040093821 -0.040093821 -0.040093821 -0.040093821 -0.040093821][-0.040093821 -0.040093821 -0.040093821 -0.040093821 -0.040093821 -0.040093821 -0.040093821 -0.040093821 -0.040093821 -0.040093821 -0.040093821 -0.040093821 -0.040093821 -0.040093821 -0.040093821]]...]
INFO - root - 2017-12-09 05:56:38.652109: step 3810, loss = 1.56, batch loss = 0.69 (18.0 examples/sec; 0.443 sec/batch; 40h:28m:30s remains)
INFO - root - 2017-12-09 05:56:42.996318: step 3820, loss = 1.57, batch loss = 0.69 (19.1 examples/sec; 0.420 sec/batch; 38h:19m:47s remains)
INFO - root - 2017-12-09 05:56:47.329506: step 3830, loss = 1.56, batch loss = 0.68 (17.3 examples/sec; 0.461 sec/batch; 42h:06m:29s remains)
INFO - root - 2017-12-09 05:56:51.812655: step 3840, loss = 1.56, batch loss = 0.68 (17.0 examples/sec; 0.469 sec/batch; 42h:51m:17s remains)
INFO - root - 2017-12-09 05:56:56.228869: step 3850, loss = 1.57, batch loss = 0.69 (18.9 examples/sec; 0.423 sec/batch; 38h:38m:35s remains)
INFO - root - 2017-12-09 05:57:00.814163: step 3860, loss = 1.57, batch loss = 0.69 (17.4 examples/sec; 0.460 sec/batch; 41h:59m:47s remains)
INFO - root - 2017-12-09 05:57:05.039548: step 3870, loss = 1.57, batch loss = 0.68 (17.2 examples/sec; 0.466 sec/batch; 42h:33m:56s remains)
INFO - root - 2017-12-09 05:57:09.511187: step 3880, loss = 1.58, batch loss = 0.69 (18.1 examples/sec; 0.442 sec/batch; 40h:18m:31s remains)
INFO - root - 2017-12-09 05:57:13.880319: step 3890, loss = 1.58, batch loss = 0.69 (18.6 examples/sec; 0.431 sec/batch; 39h:19m:31s remains)
INFO - root - 2017-12-09 05:57:18.295258: step 3900, loss = 1.58, batch loss = 0.69 (18.2 examples/sec; 0.439 sec/batch; 40h:05m:59s remains)
2017-12-09 05:57:18.839103: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0286493 -0.034049831 -0.034934513 -0.039017778 -0.039142106 -0.039637778 -0.039636772 -0.040114179 -0.040111672 -0.038792055 -0.037236914 -0.030522596 -0.026811291 -0.023317982 -0.023074385][-0.027719438 -0.033273324 -0.034367796 -0.038511906 -0.038635969 -0.039311882 -0.039310817 -0.040112689 -0.040108494 -0.039074659 -0.03781423 -0.032072686 -0.02919393 -0.025991902 -0.025646478][-0.024784543 -0.029274039 -0.030367825 -0.035480097 -0.035605706 -0.037123166 -0.039013114 -0.040117573 -0.040112812 -0.0390782 -0.037892114 -0.034420136 -0.031853717 -0.02929695 -0.029196477][-0.019391878 -0.02136397 -0.019870913 -0.022755444 -0.023970485 -0.027743187 -0.032211266 -0.03543191 -0.038013592 -0.039907768 -0.039617363 -0.037068076 -0.035202451 -0.034434982 -0.034334838][-0.019369869 -0.019030111 -0.015189832 -0.016923195 -0.018138029 -0.020406056 -0.025890365 -0.030665519 -0.035035305 -0.037809953 -0.037520371 -0.038497835 -0.037697267 -0.036929157 -0.036829069][-0.024295103 -0.02051826 -0.016351579 -0.017502593 -0.0185928 -0.01999848 -0.025478143 -0.028615203 -0.032981858 -0.033812333 -0.033809062 -0.036185693 -0.035761073 -0.036593579 -0.036493305][-0.027372289 -0.023583522 -0.019422408 -0.017858755 -0.018948909 -0.020130211 -0.025609892 -0.028414775 -0.032777425 -0.032276776 -0.032273006 -0.034828417 -0.034707 -0.036067486 -0.035967037][-0.028313769 -0.02438351 -0.020019228 -0.018399736 -0.019489776 -0.020491332 -0.025966194 -0.028445933 -0.032804061 -0.031999633 -0.031989876 -0.034628298 -0.034621704 -0.036092162 -0.036094457][-0.031760331 -0.02811107 -0.023770029 -0.021456119 -0.022544628 -0.022704838 -0.026289012 -0.028466612 -0.032820351 -0.032021727 -0.03201241 -0.034639832 -0.0346275 -0.036097735 -0.036100175][-0.037661742 -0.036134608 -0.034386378 -0.033812642 -0.033941098 -0.032110419 -0.03311082 -0.033172507 -0.0349354 -0.03203772 -0.032029517 -0.034657683 -0.034645468 -0.03611033 -0.036112215][-0.039316926 -0.03918102 -0.038981333 -0.039288871 -0.039417289 -0.038933091 -0.039037749 -0.037708767 -0.037924293 -0.0341524 -0.034145251 -0.034668993 -0.034657512 -0.036122464 -0.036123864][-0.03928424 -0.039147962 -0.038944535 -0.039271321 -0.039399866 -0.039719269 -0.039828721 -0.039768491 -0.039993729 -0.038379181 -0.038372885 -0.038098872 -0.038087755 -0.03828492 -0.038280323][-0.039267749 -0.039126143 -0.038918339 -0.039245352 -0.039373897 -0.039695144 -0.039804738 -0.039968066 -0.040197838 -0.039920311 -0.03992008 -0.039903395 -0.039897658 -0.039876398 -0.039871126][-0.039252017 -0.039103813 -0.038890831 -0.039212581 -0.039341029 -0.039662268 -0.039777249 -0.039942108 -0.040177122 -0.040198125 -0.040203329 -0.040192526 -0.040187076 -0.040170804 -0.040165175][-0.039525066 -0.039371729 -0.03913733 -0.039191164 -0.039320022 -0.039641269 -0.039756048 -0.039920703 -0.040160958 -0.04017695 -0.04018217 -0.04017666 -0.04017666 -0.040165816 -0.040160183]]...]
INFO - root - 2017-12-09 05:57:23.257025: step 3910, loss = 1.59, batch loss = 0.69 (18.9 examples/sec; 0.424 sec/batch; 38h:41m:28s remains)
INFO - root - 2017-12-09 05:57:27.588950: step 3920, loss = 1.59, batch loss = 0.69 (19.0 examples/sec; 0.421 sec/batch; 38h:24m:21s remains)
INFO - root - 2017-12-09 05:57:31.912370: step 3930, loss = 1.59, batch loss = 0.69 (18.3 examples/sec; 0.438 sec/batch; 39h:59m:35s remains)
INFO - root - 2017-12-09 05:57:36.346231: step 3940, loss = 1.59, batch loss = 0.69 (18.5 examples/sec; 0.433 sec/batch; 39h:30m:29s remains)
INFO - root - 2017-12-09 05:57:40.821971: step 3950, loss = 1.59, batch loss = 0.69 (17.4 examples/sec; 0.460 sec/batch; 41h:57m:18s remains)
INFO - root - 2017-12-09 05:57:45.358633: step 3960, loss = 1.60, batch loss = 0.69 (17.8 examples/sec; 0.450 sec/batch; 41h:05m:17s remains)
INFO - root - 2017-12-09 05:57:49.441039: step 3970, loss = 1.60, batch loss = 0.69 (18.3 examples/sec; 0.436 sec/batch; 39h:47m:10s remains)
INFO - root - 2017-12-09 05:57:53.894137: step 3980, loss = 1.60, batch loss = 0.69 (18.1 examples/sec; 0.443 sec/batch; 40h:24m:13s remains)
INFO - root - 2017-12-09 05:57:58.397068: step 3990, loss = 1.60, batch loss = 0.69 (17.9 examples/sec; 0.447 sec/batch; 40h:48m:26s remains)
INFO - root - 2017-12-09 05:58:02.765295: step 4000, loss = 1.60, batch loss = 0.69 (18.8 examples/sec; 0.425 sec/batch; 38h:44m:29s remains)
2017-12-09 05:58:03.381849: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.040132672 -0.040132675 -0.040132675 -0.040132675 -0.04013266 -0.04013266 -0.040132675 -0.040132675 -0.040132675 -0.040132675 -0.040132675 -0.040132675 -0.040132675 -0.040132675 -0.040132675][-0.040132675 -0.040132675 -0.040132675 -0.040132675 -0.040132675 -0.040132675 -0.040132675 -0.040132675 -0.040132675 -0.040132675 -0.040132675 -0.040132675 -0.040132675 -0.040132675 -0.040132675][-0.040132675 -0.040132675 -0.040132675 -0.040132675 -0.040132675 -0.040132675 -0.040132675 -0.040132675 -0.040132675 -0.040132675 -0.040132675 -0.040132675 -0.040132675 -0.040132675 -0.040132675][-0.040132675 -0.040132675 -0.040132675 -0.040132675 -0.040132675 -0.040132675 -0.040132675 -0.040132675 -0.040132675 -0.040132675 -0.040132675 -0.040132675 -0.040132675 -0.040132675 -0.040132675][-0.040132675 -0.040132675 -0.040132675 -0.040132675 -0.040132675 -0.040132675 -0.040132675 -0.040132675 -0.040132675 -0.040132675 -0.040132675 -0.040132675 -0.040132675 -0.040132675 -0.040132675][-0.040132675 -0.040132675 -0.040132675 -0.040132675 -0.040132675 -0.040132675 -0.040132675 -0.040132675 -0.040132675 -0.040132675 -0.040132675 -0.040132675 -0.040132675 -0.040132675 -0.040132675][-0.040132675 -0.040132675 -0.040132675 -0.040132675 -0.040132675 -0.040132675 -0.040132675 -0.040132675 -0.040132675 -0.040132675 -0.040132675 -0.040132675 -0.040132675 -0.040132675 -0.040132675][-0.040132675 -0.040132675 -0.040132675 -0.040132675 -0.040132675 -0.040132675 -0.040132675 -0.040132675 -0.040132675 -0.040132675 -0.040132675 -0.040132675 -0.040132675 -0.040132675 -0.040132672][-0.040132675 -0.040132675 -0.040132675 -0.040132675 -0.040132675 -0.040132675 -0.040132675 -0.040132675 -0.040132675 -0.040132675 -0.040132675 -0.040132675 -0.040132675 -0.040132672 -0.040132672][-0.040132675 -0.040132675 -0.040132675 -0.040132675 -0.040132675 -0.040132675 -0.040132675 -0.040132675 -0.040132675 -0.040132675 -0.040132675 -0.040132672 -0.040132672 -0.040132672 -0.040132672][-0.040132675 -0.040132675 -0.040132675 -0.040132675 -0.040132675 -0.040132672 -0.040132675 -0.040132672 -0.040132675 -0.040132672 -0.040132675 -0.040132672 -0.040132675 -0.040132672 -0.040132675][-0.040132675 -0.040132675 -0.040132675 -0.040132675 -0.040132675 -0.040132675 -0.040132675 -0.040132675 -0.040132675 -0.040132672 -0.040132675 -0.040132672 -0.040132672 -0.040132672 -0.040132675][-0.040132675 -0.040132675 -0.040132675 -0.040132675 -0.040132675 -0.040132675 -0.040132675 -0.040132675 -0.040132675 -0.040132675 -0.040132675 -0.040132672 -0.040132672 -0.040132672 -0.040132675][-0.040132664 -0.040132675 -0.040132672 -0.040132675 -0.040132672 -0.040132672 -0.040132672 -0.040132672 -0.040132672 -0.040132675 -0.040132675 -0.040132672 -0.040132675 -0.040132672 -0.040132672][-0.040132664 -0.040132675 -0.040132675 -0.040132675 -0.040132675 -0.040132675 -0.040132672 -0.040132675 -0.040132675 -0.040132675 -0.040132675 -0.040132672 -0.040132672 -0.040132672 -0.040132672]]...]
INFO - root - 2017-12-09 05:58:07.711993: step 4010, loss = 1.61, batch loss = 0.69 (18.7 examples/sec; 0.427 sec/batch; 38h:58m:12s remains)
INFO - root - 2017-12-09 05:58:12.067989: step 4020, loss = 1.61, batch loss = 0.69 (18.4 examples/sec; 0.434 sec/batch; 39h:35m:47s remains)
INFO - root - 2017-12-09 05:58:16.490263: step 4030, loss = 1.61, batch loss = 0.69 (18.0 examples/sec; 0.443 sec/batch; 40h:27m:42s remains)
INFO - root - 2017-12-09 05:58:20.942749: step 4040, loss = 1.61, batch loss = 0.69 (17.0 examples/sec; 0.472 sec/batch; 43h:03m:19s remains)
INFO - root - 2017-12-09 05:58:25.444303: step 4050, loss = 1.61, batch loss = 0.69 (17.9 examples/sec; 0.447 sec/batch; 40h:49m:03s remains)
INFO - root - 2017-12-09 05:58:29.787345: step 4060, loss = 1.61, batch loss = 0.69 (18.3 examples/sec; 0.437 sec/batch; 39h:52m:37s remains)
INFO - root - 2017-12-09 05:58:33.695142: step 4070, loss = 1.61, batch loss = 0.69 (18.5 examples/sec; 0.432 sec/batch; 39h:24m:30s remains)
INFO - root - 2017-12-09 05:58:38.156836: step 4080, loss = 1.62, batch loss = 0.69 (17.0 examples/sec; 0.470 sec/batch; 42h:53m:07s remains)
INFO - root - 2017-12-09 05:58:42.594460: step 4090, loss = 1.60, batch loss = 0.68 (19.1 examples/sec; 0.419 sec/batch; 38h:15m:18s remains)
INFO - root - 2017-12-09 05:58:46.995781: step 4100, loss = 1.62, batch loss = 0.69 (18.0 examples/sec; 0.444 sec/batch; 40h:32m:35s remains)
2017-12-09 05:58:47.497201: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.040047605 -0.04004766 -0.040047411 -0.040047694 -0.040046755 -0.040046338 -0.040047623 -0.040046919 -0.040046494 -0.0400459 -0.040045887 -0.040046249 -0.040046874 -0.040046729 -0.040016327][-0.040045422 -0.040046126 -0.040046915 -0.040047579 -0.04004813 -0.040048223 -0.040047985 -0.040047448 -0.040046841 -0.040046457 -0.040046588 -0.040046874 -0.040047716 -0.040048845 -0.040029287][-0.040041111 -0.040042438 -0.040044006 -0.0400454 -0.040046617 -0.040047076 -0.040047169 -0.040046811 -0.040046308 -0.040046137 -0.040046345 -0.04004683 -0.040047433 -0.040047914 -0.040036596][-0.040041909 -0.040040761 -0.040042434 -0.04004408 -0.040045589 -0.040046781 -0.040047728 -0.04004826 -0.040048234 -0.040048115 -0.040048048 -0.040047918 -0.040047564 -0.0400473 -0.040042695][-0.040045004 -0.040041149 -0.040042229 -0.040044188 -0.040046584 -0.040048569 -0.0400503 -0.040051449 -0.040051647 -0.040051505 -0.040051039 -0.040050149 -0.04004889 -0.040047709 -0.040046088][-0.040048789 -0.040042885 -0.04004363 -0.040045321 -0.040047787 -0.0400503 -0.040052351 -0.040053848 -0.040054303 -0.040054072 -0.040053278 -0.040051959 -0.040050063 -0.040047847 -0.040047321][-0.040057354 -0.040046852 -0.040046811 -0.040047172 -0.040048461 -0.04005054 -0.040052895 -0.040054958 -0.040056191 -0.040056232 -0.040055346 -0.040053926 -0.040051866 -0.040049326 -0.040047623][-0.0400618 -0.040049948 -0.040049855 -0.040049586 -0.040049687 -0.040050741 -0.040052596 -0.040054951 -0.040057015 -0.040057987 -0.040057555 -0.040056 -0.0400539 -0.040051192 -0.040048607][-0.04006552 -0.040051397 -0.040051054 -0.040050231 -0.040049508 -0.040049855 -0.040051721 -0.040054254 -0.040056895 -0.040058382 -0.040058337 -0.040056944 -0.040054653 -0.040051952 -0.040049229][-0.040067151 -0.040051751 -0.040050883 -0.040049773 -0.040048633 -0.040048786 -0.040050458 -0.040053468 -0.040056337 -0.040057946 -0.040058021 -0.040056679 -0.040054508 -0.040051848 -0.040049318][-0.04006578 -0.040051632 -0.040050764 -0.040049788 -0.040048752 -0.0400487 -0.040050022 -0.040052678 -0.040055495 -0.040057026 -0.040056869 -0.040055443 -0.04005345 -0.040051419 -0.040049654][-0.040066175 -0.040051818 -0.040051483 -0.040050849 -0.040050056 -0.040049713 -0.04005022 -0.040052187 -0.040054362 -0.04005564 -0.040055759 -0.040054511 -0.040053092 -0.040051874 -0.040051028][-0.040067095 -0.040053442 -0.040053304 -0.040052857 -0.040052284 -0.040051814 -0.040051881 -0.040052902 -0.040054157 -0.040055063 -0.040055323 -0.040054705 -0.040053852 -0.040053517 -0.040053356][-0.040065639 -0.040055409 -0.040056203 -0.040060576 -0.04006476 -0.04006391 -0.040063422 -0.040062774 -0.040062465 -0.040062971 -0.040063493 -0.040063627 -0.040064141 -0.04006432 -0.040065106][-0.040074714 -0.040071785 -0.040078606 -0.040077534 -0.040076539 -0.040075965 -0.040075213 -0.040075306 -0.040075954 -0.04007557 -0.04007547 -0.04007607 -0.040077906 -0.040079057 -0.040079653]]...]
INFO - root - 2017-12-09 05:58:51.884124: step 4110, loss = 1.62, batch loss = 0.69 (18.8 examples/sec; 0.425 sec/batch; 38h:45m:50s remains)
INFO - root - 2017-12-09 05:58:56.283197: step 4120, loss = 1.62, batch loss = 0.69 (17.9 examples/sec; 0.448 sec/batch; 40h:50m:50s remains)
INFO - root - 2017-12-09 05:59:00.635664: step 4130, loss = 1.62, batch loss = 0.68 (18.8 examples/sec; 0.426 sec/batch; 38h:52m:05s remains)
INFO - root - 2017-12-09 05:59:05.087164: step 4140, loss = 1.63, batch loss = 0.69 (18.0 examples/sec; 0.445 sec/batch; 40h:36m:19s remains)
INFO - root - 2017-12-09 05:59:09.550747: step 4150, loss = 1.63, batch loss = 0.69 (17.0 examples/sec; 0.471 sec/batch; 42h:57m:27s remains)
INFO - root - 2017-12-09 05:59:13.922216: step 4160, loss = 1.63, batch loss = 0.69 (18.4 examples/sec; 0.435 sec/batch; 39h:37m:46s remains)
INFO - root - 2017-12-09 05:59:18.079609: step 4170, loss = 1.64, batch loss = 0.69 (17.1 examples/sec; 0.467 sec/batch; 42h:32m:51s remains)
INFO - root - 2017-12-09 05:59:22.544132: step 4180, loss = 1.64, batch loss = 0.69 (18.0 examples/sec; 0.443 sec/batch; 40h:26m:40s remains)
INFO - root - 2017-12-09 05:59:26.966807: step 4190, loss = 1.64, batch loss = 0.69 (19.1 examples/sec; 0.419 sec/batch; 38h:12m:48s remains)
INFO - root - 2017-12-09 05:59:31.401238: step 4200, loss = 1.64, batch loss = 0.69 (18.8 examples/sec; 0.426 sec/batch; 38h:48m:12s remains)
2017-12-09 05:59:31.908251: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.038983051 -0.038974226 -0.038970184 -0.038962871 -0.038956616 -0.038958587 -0.038975265 -0.039006852 -0.0390482 -0.039084658 -0.039102785 -0.039095085 -0.039071463 -0.039033659 -0.039006852][-0.038915429 -0.038914923 -0.038918894 -0.03890954 -0.038893323 -0.038881611 -0.038884949 -0.038906727 -0.038942441 -0.03897636 -0.038992114 -0.038981214 -0.038948666 -0.038911123 -0.038886596][-0.038920324 -0.038922563 -0.038931228 -0.038919967 -0.038894434 -0.038870398 -0.038861848 -0.038874745 -0.038903877 -0.038934249 -0.038947761 -0.038936738 -0.038907152 -0.03887536 -0.0388573][-0.03895551 -0.038954481 -0.038968027 -0.038958844 -0.038931012 -0.038903393 -0.038891029 -0.038899865 -0.038922977 -0.038945496 -0.038952496 -0.038939461 -0.038913295 -0.038888872 -0.038879693][-0.039010126 -0.038995713 -0.039012484 -0.039008282 -0.038984638 -0.038960535 -0.038950294 -0.038957674 -0.038973823 -0.038985256 -0.038982365 -0.038965784 -0.038944453 -0.038931351 -0.038935054][-0.039076786 -0.039040823 -0.03905835 -0.039061029 -0.039045908 -0.039029621 -0.039023373 -0.039027728 -0.039033134 -0.039029647 -0.039014123 -0.038992573 -0.038976826 -0.038976964 -0.03899591][-0.039161149 -0.039102998 -0.039115764 -0.03912491 -0.039118711 -0.039110411 -0.039106738 -0.039105237 -0.039096709 -0.039074931 -0.039042953 -0.03901279 -0.038998332 -0.039007679 -0.039039344][-0.0392587 -0.039182287 -0.039191555 -0.039207108 -0.039209325 -0.039207678 -0.039204478 -0.039195329 -0.039172105 -0.039131667 -0.039081752 -0.039038863 -0.039018854 -0.0390289 -0.039064907][-0.0393593 -0.039274096 -0.039286014 -0.039308976 -0.039319333 -0.03932222 -0.039316814 -0.039297894 -0.039259508 -0.039201766 -0.039134964 -0.039077349 -0.039045796 -0.039047413 -0.03907647][-0.039460823 -0.039373979 -0.039391916 -0.039423682 -0.039440993 -0.039445847 -0.039435539 -0.039405361 -0.03935229 -0.03927923 -0.039198276 -0.039127529 -0.0390826 -0.039069016 -0.039080366][-0.039553948 -0.039470069 -0.039493214 -0.039532226 -0.039553382 -0.039557058 -0.039539509 -0.039497945 -0.039432567 -0.039349105 -0.039259903 -0.039180592 -0.039123245 -0.039089814 -0.039072074][-0.039622005 -0.0395432 -0.039571002 -0.039613862 -0.039634656 -0.039633341 -0.039607339 -0.039556511 -0.0394839 -0.039396666 -0.039305881 -0.039223544 -0.039156 -0.039099623 -0.039045248][-0.039650075 -0.039576367 -0.039608907 -0.039649911 -0.039665587 -0.039656647 -0.039622895 -0.039566707 -0.03949289 -0.039409183 -0.039324895 -0.03924641 -0.039172936 -0.039095271 -0.039003354][-0.039647143 -0.039570488 -0.039597806 -0.039629158 -0.039635498 -0.039618772 -0.039580837 -0.039525613 -0.039458603 -0.03938707 -0.039317437 -0.03925065 -0.039177261 -0.03908157 -0.03895453][-0.039639402 -0.039541923 -0.039544918 -0.039564069 -0.039565571 -0.039553486 -0.0395167 -0.039469179 -0.03941327 -0.039353512 -0.039297149 -0.039241992 -0.03918248 -0.039084144 -0.038939826]]...]
INFO - root - 2017-12-09 05:59:36.298788: step 4210, loss = 1.64, batch loss = 0.69 (18.9 examples/sec; 0.423 sec/batch; 38h:35m:52s remains)
INFO - root - 2017-12-09 05:59:40.630654: step 4220, loss = 1.65, batch loss = 0.70 (19.0 examples/sec; 0.422 sec/batch; 38h:28m:44s remains)
INFO - root - 2017-12-09 05:59:45.028175: step 4230, loss = 1.65, batch loss = 0.69 (18.6 examples/sec; 0.429 sec/batch; 39h:07m:52s remains)
INFO - root - 2017-12-09 05:59:49.428519: step 4240, loss = 1.65, batch loss = 0.69 (18.2 examples/sec; 0.439 sec/batch; 40h:01m:41s remains)
INFO - root - 2017-12-09 05:59:53.781919: step 4250, loss = 1.66, batch loss = 0.70 (18.5 examples/sec; 0.432 sec/batch; 39h:25m:11s remains)
INFO - root - 2017-12-09 05:59:58.097681: step 4260, loss = 1.65, batch loss = 0.69 (19.0 examples/sec; 0.420 sec/batch; 38h:18m:50s remains)
INFO - root - 2017-12-09 06:00:02.123459: step 4270, loss = 1.66, batch loss = 0.69 (18.9 examples/sec; 0.424 sec/batch; 38h:38m:31s remains)
INFO - root - 2017-12-09 06:00:06.465576: step 4280, loss = 1.66, batch loss = 0.69 (17.6 examples/sec; 0.454 sec/batch; 41h:22m:25s remains)
INFO - root - 2017-12-09 06:00:10.846461: step 4290, loss = 1.66, batch loss = 0.69 (18.9 examples/sec; 0.424 sec/batch; 38h:37m:04s remains)
INFO - root - 2017-12-09 06:00:15.268442: step 4300, loss = 1.67, batch loss = 0.69 (18.0 examples/sec; 0.444 sec/batch; 40h:29m:50s remains)
2017-12-09 06:00:15.759905: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.012592223 0.037950896 0.064616539 0.087978281 0.10405443 0.11774629 0.12485736 0.12442546 0.12254187 0.12236089 0.11793462 0.11860744 0.11906634 0.11946914 0.11853676][0.020957839 0.048520967 0.075548619 0.1006855 0.11905929 0.12836206 0.1338 0.13194647 0.13004386 0.12696898 0.12175142 0.12263232 0.12374546 0.12562338 0.12602633][0.018447809 0.05131124 0.080930263 0.10729646 0.12754956 0.13798991 0.14448574 0.13766685 0.13305789 0.12905857 0.12263796 0.12224951 0.12241033 0.1253396 0.12674472][0.013296861 0.047389865 0.075766273 0.10773357 0.13011733 0.14208931 0.15026966 0.14209163 0.13844803 0.12905696 0.12290097 0.12295306 0.12364004 0.12533009 0.12547752][0.0065096617 0.038698964 0.067333661 0.10173284 0.12657738 0.14548999 0.15820816 0.15228409 0.14926055 0.13799572 0.13147667 0.12598014 0.1236036 0.1250056 0.12531459][-0.0022839978 0.029463492 0.058716096 0.092181273 0.11817905 0.14054489 0.15652129 0.15729636 0.1578446 0.15034074 0.1447883 0.13757089 0.13365111 0.12934563 0.12628344][-0.013339106 0.015790146 0.04413987 0.081876226 0.11147916 0.13422394 0.15042391 0.15387547 0.15593472 0.15425533 0.15173915 0.14744812 0.14629391 0.14075834 0.13640994][-0.020644309 0.0034618191 0.031736165 0.068031408 0.096043251 0.1238246 0.14197525 0.14668375 0.1480076 0.14747852 0.14727527 0.14641562 0.146918 0.14158836 0.13750079][-0.020653162 -0.0023802929 0.022243433 0.056617476 0.082100153 0.10820917 0.12477999 0.13400429 0.13784897 0.13767454 0.13906634 0.13823372 0.13881847 0.13553041 0.13159218][-0.016394479 -0.0025788359 0.020732284 0.047897629 0.070880957 0.094710954 0.10926195 0.11990026 0.12301559 0.12839651 0.13045326 0.12983274 0.13060436 0.12752771 0.12386227][-0.011740405 0.00013539195 0.020424515 0.042942204 0.062391706 0.078514859 0.088461347 0.098086216 0.10142028 0.10973985 0.11329665 0.11840772 0.12232394 0.11974102 0.11675722][-0.010725945 0.00043465197 0.018764764 0.038886145 0.054158464 0.066196404 0.073093787 0.077117488 0.078251332 0.084466584 0.088894792 0.09665633 0.10307405 0.10635138 0.10728423][-0.0093233995 0.0007570982 0.016464256 0.034388162 0.047611162 0.056818604 0.062011726 0.063913241 0.064955175 0.066057146 0.068791263 0.0743908 0.079094365 0.084599487 0.088226341][-0.014844252 -0.0041900948 0.0099794716 0.027629271 0.042088144 0.053410754 0.061894275 0.063603774 0.064421475 0.063337043 0.063367255 0.064613819 0.06613303 0.073195986 0.078697808][-0.018755052 -0.010809738 0.00066570565 0.017204817 0.032091178 0.042962842 0.051937014 0.054499269 0.056263566 0.055209227 0.054655127 0.056986913 0.059808068 0.066543005 0.073872626]]...]
INFO - root - 2017-12-09 06:00:20.063641: step 4310, loss = 1.65, batch loss = 0.68 (18.9 examples/sec; 0.423 sec/batch; 38h:32m:43s remains)
INFO - root - 2017-12-09 06:00:24.546124: step 4320, loss = 1.67, batch loss = 0.69 (18.2 examples/sec; 0.440 sec/batch; 40h:05m:38s remains)
INFO - root - 2017-12-09 06:00:29.068507: step 4330, loss = 1.67, batch loss = 0.69 (18.3 examples/sec; 0.437 sec/batch; 39h:50m:27s remains)
INFO - root - 2017-12-09 06:00:33.543826: step 4340, loss = 1.68, batch loss = 0.69 (17.9 examples/sec; 0.448 sec/batch; 40h:51m:04s remains)
INFO - root - 2017-12-09 06:00:37.957371: step 4350, loss = 1.68, batch loss = 0.69 (17.6 examples/sec; 0.455 sec/batch; 41h:29m:55s remains)
INFO - root - 2017-12-09 06:00:42.392491: step 4360, loss = 1.67, batch loss = 0.69 (19.0 examples/sec; 0.421 sec/batch; 38h:24m:26s remains)
INFO - root - 2017-12-09 06:00:46.547081: step 4370, loss = 1.68, batch loss = 0.70 (27.0 examples/sec; 0.296 sec/batch; 26h:57m:58s remains)
INFO - root - 2017-12-09 06:00:50.971981: step 4380, loss = 1.68, batch loss = 0.69 (17.4 examples/sec; 0.460 sec/batch; 41h:52m:53s remains)
INFO - root - 2017-12-09 06:00:55.344829: step 4390, loss = 1.68, batch loss = 0.69 (18.5 examples/sec; 0.432 sec/batch; 39h:22m:04s remains)
INFO - root - 2017-12-09 06:00:59.648861: step 4400, loss = 1.69, batch loss = 0.69 (19.0 examples/sec; 0.421 sec/batch; 38h:21m:44s remains)
2017-12-09 06:01:00.192124: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.046874031 0.040107384 0.032241821 0.026341297 0.015845075 0.017993558 0.0086912736 0.010127578 0.010339998 0.016463228 0.022071742 0.027954966 0.042521536 0.050757214 0.063655324][0.086489946 0.074757315 0.057289511 0.047489278 0.042230196 0.042823531 0.04228089 0.040509708 0.038677573 0.034152918 0.042886831 0.047239766 0.046775736 0.054724172 0.064639919][0.11332656 0.10786389 0.097482711 0.098032966 0.091538936 0.093935296 0.066812247 0.058159 0.057985872 0.057185665 0.075506195 0.073719956 0.073722117 0.079600058 0.078234844][0.0857285 0.095672935 0.12137561 0.12397765 0.11696483 0.12255646 0.10721338 0.10344695 0.087522134 0.086087137 0.070798442 0.080187164 0.0768946 0.097399682 0.093063042][0.065720983 0.0821101 0.10005771 0.10057095 0.10520253 0.10832794 0.11476366 0.11122338 0.10338697 0.10764103 0.0975682 0.10429284 0.10049792 0.12093364 0.086990476][0.030385956 0.043080039 0.074564457 0.078967929 0.081678234 0.086282223 0.076902464 0.073349029 0.072966941 0.072623551 0.083701745 0.091181114 0.0932153 0.10423099 0.11511369][0.0079149827 0.01826692 0.025221251 0.030626222 0.044559747 0.051257692 0.052316904 0.051433481 0.044476554 0.046521962 0.05041822 0.053362772 0.05519557 0.061880551 0.082301795][-0.015215501 -0.0089544579 -0.0020588338 0.0039718896 0.011383165 0.016374603 0.020739667 0.021926984 0.023317754 0.027425565 0.03272105 0.036704168 0.039655752 0.046640359 0.054513551][-0.024604661 -0.026718803 -0.022172458 -0.017150577 -0.011275591 -0.0059908405 -0.0029409938 -0.0010117069 0.00072316453 0.0039524436 0.0083943382 0.01488781 0.02146785 0.027817972 0.034116641][-0.036585663 -0.034920126 -0.028794646 -0.027509585 -0.016252967 -0.017695507 -0.014080621 -0.011881271 -0.00975173 -0.0075084567 -0.0045201741 0.00063112751 0.00595358 0.011603612 0.016787052][-0.038352951 -0.036777809 -0.033745807 -0.030607581 -0.026575908 -0.0225232 -0.016039779 -0.01703698 -0.014571456 -0.011761175 -0.0083815195 -0.0042968728 -0.0002669692 0.0035968833 0.007148616][-0.038782954 -0.037678842 -0.035500657 -0.030884817 -0.030332509 -0.026985694 -0.024102062 -0.021935856 -0.019143997 -0.015906196 -0.012339236 -0.0086216368 -0.0051659979 -0.0022270456 0.00034463406][-0.035954073 -0.038584232 -0.037790112 -0.036623016 -0.034304067 -0.029897295 -0.029730417 -0.027657105 -0.024839936 -0.021655679 -0.018446261 -0.014986785 -0.011779685 -0.00887502 -0.0064511374][-0.039993145 -0.040493764 -0.039931484 -0.038421873 -0.0381586 -0.036926921 -0.035590161 -0.033288468 -0.03163708 -0.029201005 -0.026558975 -0.023724638 -0.020972518 -0.018195273 -0.015393501][-0.040090438 -0.040856019 -0.040780142 -0.040458277 -0.040199257 -0.039277483 -0.039161935 -0.038300056 -0.037254792 -0.035858214 -0.034249872 -0.03245008 -0.030484224 -0.028153602 -0.025578026]]...]
INFO - root - 2017-12-09 06:01:04.640794: step 4410, loss = 1.68, batch loss = 0.69 (17.6 examples/sec; 0.454 sec/batch; 41h:23m:16s remains)
INFO - root - 2017-12-09 06:01:09.084699: step 4420, loss = 1.68, batch loss = 0.69 (17.7 examples/sec; 0.453 sec/batch; 41h:16m:35s remains)
INFO - root - 2017-12-09 06:01:13.430333: step 4430, loss = 1.69, batch loss = 0.69 (18.3 examples/sec; 0.438 sec/batch; 39h:55m:44s remains)
INFO - root - 2017-12-09 06:01:17.795890: step 4440, loss = 1.69, batch loss = 0.69 (17.6 examples/sec; 0.455 sec/batch; 41h:28m:56s remains)
INFO - root - 2017-12-09 06:01:22.198660: step 4450, loss = 1.69, batch loss = 0.69 (18.5 examples/sec; 0.432 sec/batch; 39h:20m:49s remains)
INFO - root - 2017-12-09 06:01:26.531570: step 4460, loss = 1.69, batch loss = 0.69 (18.4 examples/sec; 0.435 sec/batch; 39h:39m:56s remains)
INFO - root - 2017-12-09 06:01:30.872763: step 4470, loss = 1.69, batch loss = 0.68 (18.8 examples/sec; 0.427 sec/batch; 38h:52m:25s remains)
INFO - root - 2017-12-09 06:01:34.893329: step 4480, loss = 1.69, batch loss = 0.69 (18.7 examples/sec; 0.428 sec/batch; 38h:59m:57s remains)
INFO - root - 2017-12-09 06:01:39.196698: step 4490, loss = 1.70, batch loss = 0.69 (18.1 examples/sec; 0.441 sec/batch; 40h:11m:46s remains)
INFO - root - 2017-12-09 06:01:43.614289: step 4500, loss = 1.70, batch loss = 0.69 (18.4 examples/sec; 0.434 sec/batch; 39h:34m:37s remains)
2017-12-09 06:01:44.165317: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.039761223 0.038159966 0.036636829 0.0354617 0.034838535 0.034367554 0.033823386 0.033183597 0.032691553 0.032222211 0.031625427 0.03092698 0.030266397 0.029747732 0.028992534][0.050781779 0.049292661 0.047444969 0.04604435 0.0451461 0.044491313 0.043969877 0.043581739 0.04341799 0.0431311 0.04271467 0.042193346 0.04150708 0.040629856 0.039525636][0.057295896 0.056535013 0.054912269 0.053380251 0.052124254 0.051064812 0.05033201 0.049959846 0.049844213 0.049831644 0.049778819 0.049580663 0.049073167 0.048150539 0.046845414][0.060954474 0.06119886 0.059779227 0.057890862 0.055950381 0.054134212 0.052866682 0.052298225 0.052328624 0.052749492 0.053354874 0.05391413 0.054043114 0.053471975 0.05216486][0.062891193 0.064191759 0.062779263 0.060287744 0.057386741 0.054629691 0.052726313 0.051955119 0.052214846 0.053262152 0.054856166 0.05653657 0.057664134 0.057733819 0.056646809][0.063238978 0.065707877 0.064238571 0.06095048 0.057013758 0.053380862 0.050920755 0.049939014 0.050401121 0.052136667 0.05484736 0.057796 0.060075626 0.060981043 0.060340054][0.06250146 0.065862067 0.064476326 0.0604555 0.055656105 0.05129087 0.048385553 0.047237232 0.047857478 0.050177693 0.05392383 0.0580993 0.061477639 0.063199885 0.063086957][0.060929604 0.064850688 0.063460611 0.058859386 0.053531826 0.048764549 0.045597993 0.044398218 0.045312949 0.048266225 0.052918881 0.058052026 0.062289007 0.0646741 0.065027758][0.058573745 0.06257493 0.06086643 0.055781744 0.050245412 0.045541696 0.042611592 0.041845366 0.043434508 0.047197051 0.052619651 0.058289893 0.062891915 0.0655557 0.066152968][0.055210218 0.0588736 0.0567049 0.05117511 0.045766637 0.041687377 0.039707907 0.040108249 0.0429255 0.047699414 0.053635828 0.059295192 0.063623711 0.06605608 0.066581249][0.051262408 0.054335684 0.051630139 0.0457545 0.04077787 0.037804656 0.037366994 0.03942129 0.043702118 0.049441889 0.055544153 0.060723245 0.064320244 0.066187494 0.066467576][0.04718896 0.04935462 0.045998119 0.039919764 0.035520762 0.033757873 0.035021819 0.038917512 0.0446809 0.051165506 0.05720111 0.061765432 0.064579159 0.065854751 0.0658788][0.04324118 0.04452695 0.040437125 0.034071021 0.030110084 0.02933611 0.031986944 0.037410684 0.044447057 0.05157584 0.057553366 0.061637454 0.063905865 0.064799555 0.064689569][0.039516412 0.039733894 0.035144895 0.028530359 0.024777628 0.02454754 0.028109491 0.03465011 0.0425326 0.050094582 0.056073986 0.059944317 0.0619609 0.062717371 0.062659279][0.033883795 0.033476532 0.028686233 0.022102833 0.018691313 0.01897851 0.023068793 0.030061498 0.038258322 0.046002492 0.051900856 0.055691406 0.057540774 0.058314592 0.058464006]]...]
INFO - root - 2017-12-09 06:01:48.466817: step 4510, loss = 1.70, batch loss = 0.69 (18.4 examples/sec; 0.434 sec/batch; 39h:30m:49s remains)
INFO - root - 2017-12-09 06:01:52.754271: step 4520, loss = 1.70, batch loss = 0.69 (18.5 examples/sec; 0.433 sec/batch; 39h:28m:16s remains)
INFO - root - 2017-12-09 06:01:57.051607: step 4530, loss = 1.70, batch loss = 0.69 (18.5 examples/sec; 0.432 sec/batch; 39h:21m:09s remains)
INFO - root - 2017-12-09 06:02:01.377555: step 4540, loss = 1.70, batch loss = 0.68 (18.9 examples/sec; 0.423 sec/batch; 38h:31m:04s remains)
INFO - root - 2017-12-09 06:02:05.772511: step 4550, loss = 1.70, batch loss = 0.68 (18.3 examples/sec; 0.436 sec/batch; 39h:43m:32s remains)
INFO - root - 2017-12-09 06:02:10.171611: step 4560, loss = 1.71, batch loss = 0.69 (21.1 examples/sec; 0.378 sec/batch; 34h:28m:36s remains)
INFO - root - 2017-12-09 06:02:14.516511: step 4570, loss = 1.71, batch loss = 0.69 (18.3 examples/sec; 0.436 sec/batch; 39h:44m:46s remains)
INFO - root - 2017-12-09 06:02:18.677051: step 4580, loss = 1.71, batch loss = 0.68 (17.1 examples/sec; 0.468 sec/batch; 42h:39m:12s remains)
INFO - root - 2017-12-09 06:02:23.098383: step 4590, loss = 1.71, batch loss = 0.68 (19.5 examples/sec; 0.410 sec/batch; 37h:23m:00s remains)
INFO - root - 2017-12-09 06:02:27.539839: step 4600, loss = 1.72, batch loss = 0.69 (17.5 examples/sec; 0.458 sec/batch; 41h:44m:39s remains)
2017-12-09 06:02:28.061636: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.011519585 0.011647046 0.01176805 0.011911508 0.012014698 0.011918128 0.011648741 0.011175483 0.010600731 0.0099356957 0.0091647021 0.0080762841 0.0061999634 0.0031015649 -0.0013309903][0.017017614 0.01733223 0.017330628 0.017366376 0.017295241 0.016954891 0.016324364 0.015410431 0.014339685 0.01316186 0.011842329 0.010155126 0.0076771565 0.0040344931 -0.00080091134][0.020910695 0.021677531 0.021741435 0.021609582 0.021258548 0.020585652 0.019567069 0.018242247 0.016726755 0.015089009 0.013270192 0.011038553 0.008023452 0.003924448 -0.0012067445][0.023353778 0.024668291 0.0248691 0.024662219 0.024111912 0.023182392 0.021879643 0.020310119 0.018576086 0.016726125 0.014662948 0.012132436 0.0087998509 0.0044300891 -0.00088573992][0.024697348 0.02671089 0.027087852 0.02687525 0.02624175 0.025218688 0.023834892 0.022267208 0.020582803 0.018817391 0.016805742 0.014248218 0.010795373 0.0062408969 0.00069319084][0.025712393 0.02857925 0.029114939 0.028937474 0.028323427 0.027344339 0.026059091 0.024663135 0.023203209 0.021693356 0.019884635 0.01744239 0.013963196 0.0092415288 0.0033723935][0.026526086 0.030207649 0.031229563 0.031078495 0.030536167 0.02970089 0.02864942 0.027555309 0.026451431 0.025293373 0.0237725 0.021504387 0.018019326 0.013115779 0.0068474896][0.027284689 0.03163676 0.033066452 0.03297174 0.0325543 0.031923145 0.031154573 0.030395091 0.029661 0.028865516 0.027670093 0.025623515 0.022193536 0.017155807 0.010511436][0.027548224 0.032265529 0.033959724 0.033965446 0.033726007 0.033336557 0.032862879 0.032414131 0.03199628 0.0315117 0.030622393 0.02884075 0.025582425 0.020577841 0.013762534][0.026898392 0.031674296 0.033320017 0.033413708 0.033349827 0.033190347 0.032969721 0.032758534 0.03254623 0.032250375 0.031599969 0.030119404 0.027218558 0.022552297 0.015967458][0.024665467 0.029107623 0.030415319 0.030528128 0.030586936 0.030615531 0.030552268 0.030413181 0.030186146 0.029858597 0.029277779 0.028079621 0.025750376 0.021849819 0.01609312][0.019779101 0.023634382 0.024748862 0.024774343 0.024840876 0.024955489 0.024905525 0.024674706 0.024238206 0.023677453 0.023013055 0.02207163 0.020440247 0.017610028 0.013154171][0.012298685 0.015432511 0.016384047 0.016381472 0.016427565 0.016496114 0.016329605 0.015939768 0.015253052 0.014447439 0.013685964 0.012970928 0.012005668 0.010274522 0.0072344653][0.0029370487 0.0053408965 0.0061991252 0.0062994622 0.0064235926 0.0064720027 0.0062093325 0.005673863 0.0048230961 0.00393714 0.0032264851 0.0027908832 0.0024132691 0.0015824102 -0.00026921183][-0.0066607 -0.0049311705 -0.0042255297 -0.0041014515 -0.0039911866 -0.004009936 -0.0043220557 -0.0048880093 -0.005753424 -0.006608028 -0.0071940459 -0.0073312223 -0.0072442591 -0.0073883533 -0.0083101727]]...]
INFO - root - 2017-12-09 06:02:32.327135: step 4610, loss = 1.72, batch loss = 0.69 (19.1 examples/sec; 0.419 sec/batch; 38h:08m:20s remains)
INFO - root - 2017-12-09 06:02:36.648470: step 4620, loss = 1.72, batch loss = 0.69 (19.1 examples/sec; 0.419 sec/batch; 38h:10m:35s remains)
INFO - root - 2017-12-09 06:02:41.046312: step 4630, loss = 1.71, batch loss = 0.68 (18.8 examples/sec; 0.425 sec/batch; 38h:44m:28s remains)
INFO - root - 2017-12-09 06:02:45.525262: step 4640, loss = 1.72, batch loss = 0.69 (17.8 examples/sec; 0.451 sec/batch; 41h:02m:41s remains)
INFO - root - 2017-12-09 06:02:49.907342: step 4650, loss = 1.72, batch loss = 0.69 (19.6 examples/sec; 0.409 sec/batch; 37h:12m:12s remains)
INFO - root - 2017-12-09 06:02:54.329664: step 4660, loss = 1.73, batch loss = 0.69 (18.3 examples/sec; 0.436 sec/batch; 39h:44m:57s remains)
INFO - root - 2017-12-09 06:02:58.703993: step 4670, loss = 1.72, batch loss = 0.68 (19.2 examples/sec; 0.417 sec/batch; 37h:59m:16s remains)
INFO - root - 2017-12-09 06:03:02.796847: step 4680, loss = 1.72, batch loss = 0.69 (18.7 examples/sec; 0.427 sec/batch; 38h:54m:36s remains)
INFO - root - 2017-12-09 06:03:07.083130: step 4690, loss = 1.72, batch loss = 0.69 (18.2 examples/sec; 0.440 sec/batch; 40h:05m:50s remains)
INFO - root - 2017-12-09 06:03:11.441706: step 4700, loss = 1.73, batch loss = 0.69 (18.1 examples/sec; 0.442 sec/batch; 40h:16m:41s remains)
2017-12-09 06:03:12.007588: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.0083364621 0.012893792 0.018556055 0.02470924 0.030709669 0.036178678 0.040662125 0.043874554 0.045808405 0.046743438 0.047261551 0.047735482 0.048222132 0.048748411 0.048796952][-0.0089810006 -0.0037350208 0.0035321452 0.012165397 0.021172859 0.029753387 0.037098087 0.042465553 0.045696177 0.047283381 0.048017956 0.048505351 0.048979439 0.049513891 0.049897015][-0.020795116 -0.015861249 -0.007942155 0.0021526888 0.013328526 0.024431176 0.03411936 0.041345686 0.045634113 0.047746398 0.048575751 0.04893963 0.049272977 0.04970964 0.050200693][-0.021637624 -0.01669029 -0.0088017024 0.0015367009 0.013273112 0.025020257 0.035301805 0.042893603 0.047317505 0.049422435 0.050099589 0.050228857 0.050330691 0.050558716 0.050928362][-0.011871023 -0.0065542273 0.00092675909 0.010270007 0.020867579 0.031379938 0.040453427 0.046996109 0.05068247 0.052303173 0.05264917 0.052494451 0.05231604 0.052255936 0.052328989][0.0047112666 0.010383137 0.01670213 0.023928471 0.032142073 0.04025206 0.047161229 0.052053988 0.054702953 0.055732489 0.055759929 0.055359468 0.054912485 0.054536663 0.054259121][0.022571914 0.02840621 0.033002123 0.037769191 0.043236122 0.048634015 0.053225704 0.056477554 0.058221065 0.058839135 0.058726102 0.058231913 0.057644852 0.057049178 0.056475833][0.036855087 0.042795114 0.045739047 0.048367143 0.0514732 0.054621808 0.057371169 0.059371658 0.060501188 0.060941719 0.060891658 0.060517095 0.059989944 0.059358172 0.058659635][0.0460954 0.052102387 0.053905956 0.054957882 0.056334212 0.057867751 0.059328258 0.060523644 0.061320096 0.061760925 0.061893739 0.061767146 0.061440043 0.060932316 0.06028159][0.051437795 0.057252377 0.058353983 0.058423646 0.05869592 0.059179179 0.059813835 0.060501926 0.061115354 0.061623484 0.061953731 0.062055998 0.061914854 0.061555088 0.061014734][0.054151922 0.059959471 0.060569972 0.060100235 0.059730895 0.059579678 0.059703216 0.060070463 0.060568906 0.061116345 0.06156604 0.061795726 0.06173151 0.061408259 0.060900167][0.055397719 0.061248332 0.061600447 0.06087938 0.060191326 0.059712872 0.059567027 0.059763551 0.060196705 0.060747571 0.06122002 0.061442867 0.061310686 0.060871646 0.060259424][0.055288315 0.0614895 0.061920367 0.061149918 0.060394824 0.059843108 0.05963359 0.059782729 0.060190096 0.060713947 0.06113711 0.0612567 0.060948163 0.060257196 0.059392929][0.054611132 0.061041437 0.061870359 0.061171971 0.060520872 0.060084656 0.059964672 0.060155518 0.060561873 0.061032869 0.061353244 0.06131357 0.060771309 0.059733048 0.058508478][0.050467223 0.056805797 0.05809202 0.057820074 0.057643443 0.057680368 0.057978988 0.058207624 0.058494888 0.058586456 0.058474436 0.057973213 0.057009168 0.055705808 0.054226086]]...]
INFO - root - 2017-12-09 06:03:16.380178: step 4710, loss = 1.73, batch loss = 0.69 (18.2 examples/sec; 0.440 sec/batch; 40h:01m:12s remains)
INFO - root - 2017-12-09 06:03:20.675906: step 4720, loss = 1.73, batch loss = 0.69 (18.5 examples/sec; 0.433 sec/batch; 39h:27m:57s remains)
INFO - root - 2017-12-09 06:03:25.038171: step 4730, loss = 1.73, batch loss = 0.68 (18.2 examples/sec; 0.440 sec/batch; 40h:05m:54s remains)
INFO - root - 2017-12-09 06:03:29.354686: step 4740, loss = 1.74, batch loss = 0.69 (18.9 examples/sec; 0.423 sec/batch; 38h:33m:00s remains)
INFO - root - 2017-12-09 06:03:33.759647: step 4750, loss = 1.74, batch loss = 0.69 (18.2 examples/sec; 0.438 sec/batch; 39h:55m:02s remains)
INFO - root - 2017-12-09 06:03:38.171129: step 4760, loss = 1.73, batch loss = 0.67 (17.9 examples/sec; 0.447 sec/batch; 40h:43m:50s remains)
INFO - root - 2017-12-09 06:03:42.536606: step 4770, loss = 1.73, batch loss = 0.68 (19.5 examples/sec; 0.411 sec/batch; 37h:24m:02s remains)
INFO - root - 2017-12-09 06:03:46.777780: step 4780, loss = 1.75, batch loss = 0.70 (18.0 examples/sec; 0.445 sec/batch; 40h:28m:44s remains)
INFO - root - 2017-12-09 06:03:51.210439: step 4790, loss = 1.76, batch loss = 0.69 (18.0 examples/sec; 0.444 sec/batch; 40h:24m:41s remains)
INFO - root - 2017-12-09 06:03:55.552738: step 4800, loss = 1.75, batch loss = 0.69 (18.8 examples/sec; 0.427 sec/batch; 38h:49m:35s remains)
2017-12-09 06:03:56.107005: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.057312816 0.057783604 0.05833666 0.058579996 0.058810048 0.058941409 0.059211843 0.059322089 0.059365623 0.059860177 0.0601354 0.060344055 0.060328022 0.060293697 0.058993533][0.063154019 0.063832387 0.064390264 0.06463483 0.064877681 0.065000594 0.065212466 0.0654212 0.065545782 0.065757528 0.065872453 0.065936357 0.065810747 0.065672882 0.064552464][0.06451121 0.065625407 0.066349506 0.066469446 0.066607751 0.066740893 0.066811241 0.066827007 0.0668101 0.066788733 0.066781171 0.06676814 0.066749826 0.066722624 0.066168964][0.063754685 0.065635152 0.066633709 0.066637687 0.066633984 0.066630915 0.066574372 0.06648355 0.066374548 0.066254549 0.0661486 0.066051826 0.065987036 0.065962076 0.065841988][0.0628124 0.065768607 0.067284472 0.0671968 0.067041419 0.06686224 0.066616058 0.066332392 0.066025063 0.065673284 0.06531205 0.06496612 0.064704768 0.064578645 0.064595208][0.061759546 0.066045985 0.068134412 0.068021357 0.067759179 0.06741593 0.066954933 0.066397771 0.065758422 0.064976588 0.064124629 0.063247934 0.062534004 0.062121548 0.062063202][0.060747191 0.066356435 0.068958476 0.068891369 0.068586051 0.068123952 0.06745743 0.066594295 0.06551706 0.064132743 0.0625238 0.060773768 0.059258662 0.058244683 0.057951704][0.059618123 0.066512972 0.069625668 0.069636688 0.069324493 0.068758689 0.067866735 0.066628106 0.06493862 0.062696107 0.05998005 0.057015412 0.054412805 0.05262398 0.052120626][0.058430791 0.066502996 0.070037745 0.070134558 0.069856465 0.069228783 0.068137087 0.066471718 0.064049244 0.060721055 0.056635536 0.052222542 0.048335224 0.045739435 0.045083314][0.057212412 0.066207424 0.0702279 0.070353173 0.070081785 0.069395989 0.068123512 0.066071115 0.062953837 0.058596037 0.053217039 0.047411539 0.042281032 0.038853712 0.037990652][0.056289107 0.066047952 0.070147723 0.07021641 0.069899723 0.0691464 0.067739405 0.065416925 0.061815344 0.056737103 0.05043038 0.043581158 0.037461162 0.033304617 0.032127842][0.055367626 0.065631919 0.069744609 0.0696924 0.0692883 0.06846974 0.066993766 0.064560823 0.060760841 0.055391759 0.048687249 0.041372493 0.034702577 0.029975988 0.028289892][0.054274604 0.064716972 0.068945564 0.06876488 0.068277717 0.0674188 0.065946512 0.063554727 0.059847295 0.054632559 0.048125915 0.040972859 0.034275852 0.029257454 0.026943043][0.0529667 0.063403137 0.067747019 0.067481808 0.0667279 0.06563583 0.06417527 0.061895482 0.058648311 0.054105647 0.048518926 0.042280711 0.03605894 0.031014547 0.028017469][0.049853265 0.059815854 0.063852742 0.0633885 0.062435836 0.061114356 0.059586704 0.0571723 0.053952597 0.049869217 0.045033269 0.039840937 0.034560993 0.030001149 0.026772939]]...]
INFO - root - 2017-12-09 06:04:00.411387: step 4810, loss = 1.75, batch loss = 0.69 (18.8 examples/sec; 0.425 sec/batch; 38h:41m:47s remains)
INFO - root - 2017-12-09 06:04:04.756952: step 4820, loss = 1.75, batch loss = 0.68 (18.6 examples/sec; 0.430 sec/batch; 39h:08m:45s remains)
INFO - root - 2017-12-09 06:04:09.217517: step 4830, loss = 1.75, batch loss = 0.69 (17.9 examples/sec; 0.448 sec/batch; 40h:44m:17s remains)
INFO - root - 2017-12-09 06:04:13.622776: step 4840, loss = 1.76, batch loss = 0.69 (18.3 examples/sec; 0.438 sec/batch; 39h:52m:06s remains)
INFO - root - 2017-12-09 06:04:17.970047: step 4850, loss = 1.76, batch loss = 0.69 (18.7 examples/sec; 0.427 sec/batch; 38h:51m:56s remains)
INFO - root - 2017-12-09 06:04:22.308682: step 4860, loss = 1.77, batch loss = 0.69 (18.7 examples/sec; 0.427 sec/batch; 38h:51m:35s remains)
INFO - root - 2017-12-09 06:04:26.666708: step 4870, loss = 1.77, batch loss = 0.69 (19.0 examples/sec; 0.422 sec/batch; 38h:23m:37s remains)
INFO - root - 2017-12-09 06:04:30.830809: step 4880, loss = 1.77, batch loss = 0.69 (17.9 examples/sec; 0.447 sec/batch; 40h:38m:04s remains)
INFO - root - 2017-12-09 06:04:35.240535: step 4890, loss = 1.77, batch loss = 0.69 (18.5 examples/sec; 0.433 sec/batch; 39h:25m:29s remains)
INFO - root - 2017-12-09 06:04:39.624029: step 4900, loss = 1.77, batch loss = 0.69 (18.2 examples/sec; 0.440 sec/batch; 40h:02m:39s remains)
2017-12-09 06:04:40.161884: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.040009379 -0.038489696 -0.037560098 -0.034940794 -0.032765239 -0.0305309 -0.027993087 -0.028888673 -0.02879023 -0.032183915 -0.034587733 -0.036765188 -0.039601348 -0.040329561 -0.04146773][-0.039919771 -0.039804988 -0.03980545 -0.039353266 -0.038758792 -0.037901275 -0.036865119 -0.036526345 -0.035438824 -0.036611691 -0.037949305 -0.038809512 -0.040388834 -0.040939864 -0.042057082][-0.03952425 -0.038246624 -0.038444597 -0.036511932 -0.036406711 -0.035804026 -0.035632543 -0.036983423 -0.036478613 -0.038568955 -0.039096765 -0.039903589 -0.040550176 -0.041090827 -0.0421558][-0.033535637 -0.0315053 -0.030774595 -0.028529834 -0.027323658 -0.027966376 -0.030063229 -0.032651223 -0.033723895 -0.037377227 -0.039654948 -0.04038056 -0.04057968 -0.0408722 -0.041937169][-0.024338279 -0.019017538 -0.015553594 -0.012438977 -0.0104036 -0.013972759 -0.018297598 -0.024222013 -0.029640459 -0.035559114 -0.039663319 -0.04024934 -0.040903956 -0.041505851 -0.041549925][-0.015611777 -0.0051036812 0.0010156967 0.0061695278 0.010504697 0.0063089803 0.0012704916 -0.00973985 -0.018443935 -0.026836388 -0.0337268 -0.036321551 -0.038891077 -0.040148906 -0.040683918][-0.014568523 -0.0016619302 0.006431587 0.014774419 0.022289172 0.021756664 0.019118734 0.0070267022 -0.0028106868 -0.014383739 -0.024310792 -0.030802906 -0.036114011 -0.038746055 -0.040307324][-0.014076471 -0.00095699728 0.0076180995 0.017216284 0.026084676 0.028890908 0.028841957 0.018011622 0.0083543062 -0.0042150244 -0.015402783 -0.025261231 -0.033003539 -0.037214179 -0.039603982][-0.014436642 -0.0036202744 0.0051021017 0.013353679 0.021854311 0.025729626 0.026959494 0.018480953 0.0098654516 -0.0013091937 -0.012003234 -0.022733064 -0.031464152 -0.036347471 -0.039218828][-0.014680399 -0.0049575344 0.0037836656 0.011916824 0.01911829 0.02543769 0.028494768 0.020363875 0.012374721 0.0013748519 -0.0082050189 -0.020799415 -0.030568149 -0.035662286 -0.039383568][-0.018120764 -0.013083 -0.0072673783 0.0006875284 0.0079643279 0.017476715 0.023226753 0.017362732 0.010473665 0.00083477795 -0.0080358721 -0.021261962 -0.031388715 -0.036236472 -0.039887577][-0.01467968 -0.01689033 -0.015358193 -0.012366174 -0.01044916 -0.00039681792 0.00643079 0.00466007 0.0011016019 -0.0054548271 -0.011901552 -0.023545763 -0.032571394 -0.036719508 -0.040362425][-0.0045479089 -0.01050267 -0.01257501 -0.014219156 -0.016811 -0.011694238 -0.00810143 -0.0088622794 -0.011383487 -0.015078202 -0.018877264 -0.027549908 -0.034585372 -0.037712708 -0.040587626][0.0079601407 -0.0010018311 -0.0059172325 -0.010917939 -0.016816426 -0.01547103 -0.014601933 -0.015407907 -0.018376557 -0.021066496 -0.023644432 -0.029521324 -0.035186384 -0.037842032 -0.040775269][0.020801067 0.015955023 0.01263313 0.0067694448 -0.0010699891 -0.0040896237 -0.0073020533 -0.011278188 -0.015725011 -0.020300241 -0.024682157 -0.029575109 -0.034490723 -0.037055373 -0.040302202]]...]
INFO - root - 2017-12-09 06:04:44.441556: step 4910, loss = 1.78, batch loss = 0.69 (18.5 examples/sec; 0.431 sec/batch; 39h:15m:08s remains)
INFO - root - 2017-12-09 06:04:48.785885: step 4920, loss = 1.77, batch loss = 0.68 (19.1 examples/sec; 0.418 sec/batch; 38h:03m:37s remains)
INFO - root - 2017-12-09 06:04:53.239872: step 4930, loss = 1.77, batch loss = 0.68 (17.7 examples/sec; 0.451 sec/batch; 41h:02m:27s remains)
INFO - root - 2017-12-09 06:04:57.640769: step 4940, loss = 1.78, batch loss = 0.69 (18.3 examples/sec; 0.436 sec/batch; 39h:41m:27s remains)
INFO - root - 2017-12-09 06:05:02.081977: step 4950, loss = 1.79, batch loss = 0.69 (18.0 examples/sec; 0.446 sec/batch; 40h:32m:42s remains)
INFO - root - 2017-12-09 06:05:06.458605: step 4960, loss = 1.79, batch loss = 0.69 (17.0 examples/sec; 0.470 sec/batch; 42h:44m:26s remains)
INFO - root - 2017-12-09 06:05:10.790488: step 4970, loss = 1.79, batch loss = 0.69 (18.7 examples/sec; 0.429 sec/batch; 39h:00m:22s remains)
INFO - root - 2017-12-09 06:05:14.880960: step 4980, loss = 1.78, batch loss = 0.68 (17.8 examples/sec; 0.449 sec/batch; 40h:53m:00s remains)
INFO - root - 2017-12-09 06:05:19.311666: step 4990, loss = 1.79, batch loss = 0.69 (17.2 examples/sec; 0.465 sec/batch; 42h:15m:36s remains)
INFO - root - 2017-12-09 06:05:23.687643: step 5000, loss = 1.80, batch loss = 0.70 (18.4 examples/sec; 0.436 sec/batch; 39h:39m:07s remains)
2017-12-09 06:05:24.158933: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.029241137 -0.028573168 -0.027758554 -0.027016038 -0.02642745 -0.026033431 -0.025931876 -0.026081534 -0.026320117 -0.02651556 -0.026659738 -0.026730694 -0.026747514 -0.026732495 -0.026865352][-0.028296296 -0.027527407 -0.026673453 -0.02590272 -0.025302161 -0.024947779 -0.02485141 -0.024934229 -0.025113609 -0.025267649 -0.025378583 -0.025432711 -0.025442543 -0.025459241 -0.02560509][-0.027888644 -0.027088087 -0.026304485 -0.025624158 -0.025070168 -0.024719745 -0.02457645 -0.02459315 -0.024701359 -0.02482786 -0.024930529 -0.024990505 -0.025028821 -0.025086667 -0.025216002][-0.027673855 -0.026869748 -0.026244186 -0.02574002 -0.025298184 -0.024965815 -0.024752367 -0.024649395 -0.024638439 -0.024686417 -0.024765471 -0.024853759 -0.024949057 -0.025068872 -0.025218818][-0.027370013 -0.026539033 -0.026031349 -0.025716517 -0.025397148 -0.025090229 -0.024810046 -0.024588119 -0.024469206 -0.024463007 -0.024559058 -0.024719866 -0.02491276 -0.025122995 -0.025331773][-0.027111124 -0.0262016 -0.025779517 -0.025630122 -0.025420725 -0.025150839 -0.024836445 -0.024534728 -0.024339655 -0.024301937 -0.024422623 -0.024656046 -0.024941539 -0.025235722 -0.025498716][-0.027005639 -0.02595661 -0.025565149 -0.025542129 -0.025442576 -0.025243985 -0.02495876 -0.024645835 -0.024420263 -0.024350723 -0.024464156 -0.024721613 -0.025050582 -0.025395321 -0.025691522][-0.027082864 -0.025816623 -0.025389751 -0.025443049 -0.025441157 -0.025342908 -0.025144078 -0.024893805 -0.024696602 -0.02462016 -0.024709363 -0.024944149 -0.02525951 -0.02559587 -0.025883241][-0.027248474 -0.025747973 -0.025226997 -0.025301913 -0.025367407 -0.025379188 -0.025309324 -0.025178039 -0.025060268 -0.025010627 -0.025081562 -0.025272299 -0.02553718 -0.025823269 -0.026067851][-0.027427806 -0.02572979 -0.025081916 -0.025164386 -0.025271671 -0.025363497 -0.02540723 -0.02540149 -0.025379894 -0.025374118 -0.025433861 -0.025581054 -0.0257906 -0.026017122 -0.026212616][-0.027441181 -0.025615696 -0.024934698 -0.025017779 -0.025145533 -0.025287632 -0.025410671 -0.025498755 -0.025554487 -0.025588932 -0.025642531 -0.025750529 -0.025905088 -0.026078155 -0.026232066][-0.027437691 -0.025531348 -0.024812466 -0.02488411 -0.025006432 -0.025156798 -0.025308335 -0.025442149 -0.025542174 -0.02560411 -0.025654553 -0.025731415 -0.025841551 -0.025968822 -0.02609002][-0.027507327 -0.025517257 -0.024751006 -0.024794715 -0.024883229 -0.025002848 -0.025134461 -0.025261624 -0.025366656 -0.02543959 -0.025491001 -0.025553297 -0.025636153 -0.02573359 -0.025831932][-0.027539678 -0.025541963 -0.024759816 -0.024772113 -0.024813626 -0.024880558 -0.02496448 -0.02505354 -0.025133753 -0.025196096 -0.025242785 -0.025294624 -0.025359815 -0.025439806 -0.025523543][-0.027933769 -0.02597334 -0.02519743 -0.025189264 -0.025194583 -0.025215702 -0.025250595 -0.025353925 -0.025521327 -0.025695171 -0.025809791 -0.025888057 -0.025918651 -0.025870491 -0.025839642]]...]
INFO - root - 2017-12-09 06:05:28.459858: step 5010, loss = 1.79, batch loss = 0.69 (18.7 examples/sec; 0.428 sec/batch; 38h:55m:50s remains)
INFO - root - 2017-12-09 06:05:32.789300: step 5020, loss = 1.82, batch loss = 0.71 (18.3 examples/sec; 0.436 sec/batch; 39h:40m:50s remains)
INFO - root - 2017-12-09 06:05:37.139764: step 5030, loss = 1.80, batch loss = 0.69 (18.6 examples/sec; 0.429 sec/batch; 39h:03m:11s remains)
INFO - root - 2017-12-09 06:05:41.549500: step 5040, loss = 1.80, batch loss = 0.69 (18.3 examples/sec; 0.436 sec/batch; 39h:40m:17s remains)
INFO - root - 2017-12-09 06:05:46.000112: step 5050, loss = 1.81, batch loss = 0.70 (18.4 examples/sec; 0.435 sec/batch; 39h:35m:47s remains)
INFO - root - 2017-12-09 06:05:50.413897: step 5060, loss = 1.80, batch loss = 0.69 (18.5 examples/sec; 0.433 sec/batch; 39h:24m:53s remains)
INFO - root - 2017-12-09 06:05:54.836440: step 5070, loss = 1.81, batch loss = 0.69 (18.2 examples/sec; 0.439 sec/batch; 39h:54m:06s remains)
INFO - root - 2017-12-09 06:05:58.857869: step 5080, loss = 1.81, batch loss = 0.69 (17.3 examples/sec; 0.463 sec/batch; 42h:09m:01s remains)
INFO - root - 2017-12-09 06:06:03.218036: step 5090, loss = 1.81, batch loss = 0.69 (18.4 examples/sec; 0.435 sec/batch; 39h:35m:39s remains)
INFO - root - 2017-12-09 06:06:07.536775: step 5100, loss = 1.80, batch loss = 0.69 (18.3 examples/sec; 0.438 sec/batch; 39h:49m:56s remains)
2017-12-09 06:06:08.117930: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.034683812 -0.034480497 -0.034213059 -0.033889975 -0.033460923 -0.032846592 -0.031994812 -0.0308722 -0.029607669 -0.028352572 -0.027236287 -0.026324634 -0.025728222 -0.025415294 -0.025497744][-0.0313199 -0.031097949 -0.030870536 -0.030636467 -0.030289959 -0.029786205 -0.029105827 -0.028260659 -0.027346123 -0.026458424 -0.025680616 -0.025060982 -0.024657436 -0.024460929 -0.024584705][-0.028483016 -0.028145224 -0.02793264 -0.027826563 -0.027624499 -0.027307389 -0.02687552 -0.026359526 -0.025823073 -0.025308127 -0.024861833 -0.024512324 -0.024275174 -0.024141362 -0.024260869][-0.026454067 -0.025964122 -0.025785686 -0.025791341 -0.025733491 -0.02560506 -0.025400726 -0.025152976 -0.024899948 -0.024664344 -0.02446384 -0.024298951 -0.024173439 -0.024081293 -0.024210192][-0.025494402 -0.024747325 -0.02453508 -0.024614008 -0.024656598 -0.024651488 -0.0245882 -0.024495574 -0.0243967 -0.024309766 -0.024237122 -0.024171809 -0.024111312 -0.024048015 -0.024218533][-0.025273783 -0.024275668 -0.024013096 -0.024123423 -0.024209438 -0.024251575 -0.024239747 -0.02419886 -0.024153229 -0.02412633 -0.024109004 -0.024089837 -0.024066085 -0.024031062 -0.024198987][-0.025561577 -0.024295378 -0.023957824 -0.024077075 -0.024171878 -0.024220752 -0.024212644 -0.024165263 -0.024110295 -0.024079015 -0.024064738 -0.024057457 -0.024051208 -0.024040429 -0.024182696][-0.026008647 -0.024515707 -0.02410732 -0.0242284 -0.024321452 -0.024359409 -0.024341002 -0.024277618 -0.024197515 -0.024137495 -0.024103118 -0.024087638 -0.024083607 -0.02408839 -0.024192546][-0.026565913 -0.024826789 -0.02433892 -0.024419978 -0.024513377 -0.024551079 -0.024533397 -0.024465395 -0.024368355 -0.024278143 -0.024216346 -0.024181552 -0.024169151 -0.024178741 -0.024248054][-0.026931457 -0.025047056 -0.024499817 -0.024566885 -0.0246566 -0.024697455 -0.024689162 -0.024629237 -0.024532832 -0.024429118 -0.024351859 -0.024307463 -0.024289567 -0.024299853 -0.024343899][-0.027148029 -0.025209771 -0.024595389 -0.024642903 -0.024725975 -0.024771415 -0.024776893 -0.024735175 -0.024653407 -0.024553834 -0.024473453 -0.024425406 -0.024406984 -0.024418719 -0.024453174][-0.027251385 -0.025253549 -0.024619061 -0.02465572 -0.024728619 -0.024776775 -0.024793994 -0.024771921 -0.024712488 -0.024629802 -0.024556328 -0.02450986 -0.024493365 -0.024507223 -0.024544548][-0.027459372 -0.025372123 -0.024655802 -0.024637947 -0.024697524 -0.024743011 -0.024766451 -0.024759552 -0.024721779 -0.024661014 -0.024601653 -0.024561502 -0.024547042 -0.024559936 -0.024595695][-0.027509445 -0.02549433 -0.024730952 -0.024721203 -0.024768807 -0.024766196 -0.024770766 -0.024731662 -0.024710007 -0.024669068 -0.024624495 -0.024590403 -0.024575457 -0.024583533 -0.024612932][-0.02898271 -0.027120627 -0.026320972 -0.0261696 -0.026052233 -0.025963493 -0.025923537 -0.025999168 -0.02614159 -0.026272973 -0.026376162 -0.026400946 -0.026420619 -0.02642275 -0.02644315]]...]
INFO - root - 2017-12-09 06:06:12.508345: step 5110, loss = 1.81, batch loss = 0.69 (19.0 examples/sec; 0.422 sec/batch; 38h:20m:27s remains)
INFO - root - 2017-12-09 06:06:16.810273: step 5120, loss = 1.81, batch loss = 0.69 (18.7 examples/sec; 0.427 sec/batch; 38h:48m:08s remains)
INFO - root - 2017-12-09 06:06:21.152232: step 5130, loss = 1.81, batch loss = 0.69 (18.7 examples/sec; 0.428 sec/batch; 38h:57m:34s remains)
INFO - root - 2017-12-09 06:06:25.512774: step 5140, loss = 1.81, batch loss = 0.69 (18.7 examples/sec; 0.428 sec/batch; 38h:52m:37s remains)
INFO - root - 2017-12-09 06:06:29.969285: step 5150, loss = 1.82, batch loss = 0.69 (17.8 examples/sec; 0.450 sec/batch; 40h:54m:22s remains)
INFO - root - 2017-12-09 06:06:34.365924: step 5160, loss = 1.82, batch loss = 0.69 (18.1 examples/sec; 0.443 sec/batch; 40h:15m:14s remains)
INFO - root - 2017-12-09 06:06:38.857593: step 5170, loss = 1.82, batch loss = 0.69 (17.5 examples/sec; 0.458 sec/batch; 41h:38m:15s remains)
INFO - root - 2017-12-09 06:06:42.979957: step 5180, loss = 1.83, batch loss = 0.69 (17.6 examples/sec; 0.455 sec/batch; 41h:23m:11s remains)
INFO - root - 2017-12-09 06:06:47.429009: step 5190, loss = 1.83, batch loss = 0.69 (17.7 examples/sec; 0.453 sec/batch; 41h:09m:08s remains)
INFO - root - 2017-12-09 06:06:51.817011: step 5200, loss = 1.87, batch loss = 0.73 (19.0 examples/sec; 0.420 sec/batch; 38h:13m:03s remains)
2017-12-09 06:06:52.321245: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.021011567 -0.021012057 -0.020911952 -0.020868631 -0.020824224 -0.020731792 -0.020653091 -0.020602774 -0.020593455 -0.02059604 -0.020610476 -0.0206402 -0.020661432 -0.020606693 -0.020932198][-0.019255733 -0.01914279 -0.018981462 -0.018937964 -0.01889322 -0.018833827 -0.018777477 -0.01873013 -0.018715505 -0.018690515 -0.018676842 -0.018700687 -0.018718641 -0.018715683 -0.019140827][-0.018863402 -0.018543657 -0.01832355 -0.018257663 -0.018233564 -0.018204441 -0.018149436 -0.018100267 -0.018082533 -0.018060667 -0.01803869 -0.01802017 -0.018006392 -0.017996876 -0.018399462][-0.019114263 -0.018511957 -0.018232154 -0.018159548 -0.018182775 -0.018200483 -0.018206446 -0.018196562 -0.018173482 -0.018141374 -0.018106554 -0.018074965 -0.018049618 -0.018031545 -0.018361654][-0.019606052 -0.018725488 -0.018278943 -0.018199565 -0.018235769 -0.018267168 -0.018282855 -0.01827563 -0.018248513 -0.018207075 -0.018160142 -0.018116305 -0.01808071 -0.018056041 -0.018379584][-0.020110358 -0.018893648 -0.018272366 -0.018195486 -0.018240068 -0.01828129 -0.018306023 -0.018307146 -0.018285336 -0.018243769 -0.018191844 -0.018140187 -0.018097742 -0.018069033 -0.018360006][-0.020609263 -0.01898594 -0.018253678 -0.018136216 -0.018180041 -0.018219402 -0.018245524 -0.018255634 -0.01824978 -0.018225212 -0.018185636 -0.018140135 -0.018099979 -0.018071912 -0.018300671][-0.021084227 -0.019113444 -0.01822518 -0.018044287 -0.018077983 -0.018106105 -0.018127874 -0.018147005 -0.018163469 -0.018166987 -0.018153859 -0.018127585 -0.018098036 -0.018074485 -0.018227125][-0.021475662 -0.019185362 -0.018166631 -0.017946459 -0.017957542 -0.017962502 -0.017972639 -0.018002376 -0.018048637 -0.018091673 -0.018117644 -0.018120866 -0.018108973 -0.018091014 -0.018170966][-0.021930333 -0.019290764 -0.018115733 -0.017858012 -0.017839793 -0.017816521 -0.017814726 -0.017856833 -0.017937168 -0.018025972 -0.018097023 -0.018134784 -0.018141566 -0.018127089 -0.01813687][-0.022321928 -0.0194337 -0.018135803 -0.017794229 -0.017744949 -0.017693002 -0.01768014 -0.017735818 -0.017850047 -0.017986089 -0.018104497 -0.018177759 -0.018200781 -0.018185003 -0.018150367][-0.022616463 -0.019604161 -0.018188536 -0.017755054 -0.017679378 -0.017606165 -0.017587597 -0.017657666 -0.017801978 -0.017975908 -0.018133169 -0.018236035 -0.018272515 -0.018253107 -0.018204615][-0.022832545 -0.019694529 -0.018188048 -0.017738668 -0.017649338 -0.017567785 -0.017548857 -0.017628409 -0.017789587 -0.01798516 -0.01816521 -0.018286277 -0.018331328 -0.018308302 -0.018248476][-0.022854989 -0.01980933 -0.018309023 -0.018029131 -0.018048609 -0.018020619 -0.018054277 -0.018133616 -0.018293152 -0.018487858 -0.018668894 -0.018794507 -0.018843656 -0.01882118 -0.018757518][-0.024183484 -0.021400802 -0.019990599 -0.019654818 -0.019589961 -0.019549904 -0.019573079 -0.019813713 -0.020108068 -0.020434355 -0.020738546 -0.020892702 -0.020980315 -0.020961646 -0.020902503]]...]
INFO - root - 2017-12-09 06:06:56.659352: step 5210, loss = 1.83, batch loss = 0.70 (19.1 examples/sec; 0.418 sec/batch; 38h:00m:28s remains)
INFO - root - 2017-12-09 06:07:01.002673: step 5220, loss = 1.83, batch loss = 0.69 (18.4 examples/sec; 0.436 sec/batch; 39h:37m:51s remains)
INFO - root - 2017-12-09 06:07:05.481800: step 5230, loss = 1.83, batch loss = 0.69 (18.3 examples/sec; 0.437 sec/batch; 39h:43m:42s remains)
INFO - root - 2017-12-09 06:07:09.926706: step 5240, loss = 1.84, batch loss = 0.69 (18.5 examples/sec; 0.432 sec/batch; 39h:17m:43s remains)
INFO - root - 2017-12-09 06:07:14.404469: step 5250, loss = 1.83, batch loss = 0.69 (18.2 examples/sec; 0.440 sec/batch; 40h:01m:45s remains)
INFO - root - 2017-12-09 06:07:18.800416: step 5260, loss = 1.84, batch loss = 0.69 (18.8 examples/sec; 0.425 sec/batch; 38h:36m:45s remains)
INFO - root - 2017-12-09 06:07:23.143757: step 5270, loss = 1.84, batch loss = 0.69 (18.1 examples/sec; 0.441 sec/batch; 40h:07m:41s remains)
INFO - root - 2017-12-09 06:07:27.205306: step 5280, loss = 1.84, batch loss = 0.69 (37.6 examples/sec; 0.213 sec/batch; 19h:21m:29s remains)
INFO - root - 2017-12-09 06:07:31.562207: step 5290, loss = 1.86, batch loss = 0.70 (18.4 examples/sec; 0.435 sec/batch; 39h:32m:07s remains)
INFO - root - 2017-12-09 06:07:35.963150: step 5300, loss = 1.84, batch loss = 0.69 (18.4 examples/sec; 0.434 sec/batch; 39h:29m:08s remains)
2017-12-09 06:07:36.483543: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.042189866 -0.042190522 -0.042192463 -0.042194739 -0.042197175 -0.042199522 -0.042178832 -0.042179924 -0.038955152 -0.038952868 -0.038950328 -0.038947731 -0.038966831 -0.0389638 -0.042186737][-0.042192072 -0.042193338 -0.042195778 -0.042198479 -0.042201348 -0.042204283 -0.042183477 -0.042184286 -0.038959004 -0.038956169 -0.038952984 -0.038950045 -0.038968574 -0.03896508 -0.042187855][-0.042192776 -0.042194255 -0.042197108 -0.042200223 -0.042203628 -0.042207431 -0.042186067 -0.042020358 -0.038794272 -0.03879584 -0.038792025 -0.038788918 -0.038807254 -0.038969304 -0.042191889][-0.042191226 -0.042194318 -0.042197492 -0.042200826 -0.04220444 -0.042208444 -0.042186748 -0.042019919 -0.038792852 -0.038793642 -0.038789507 -0.038786206 -0.038801208 -0.038969468 -0.042189594][-0.042188466 -0.042193655 -0.042196929 -0.042200297 -0.042203777 -0.042207647 -0.042213719 -0.042045731 -0.03881792 -0.038818251 -0.038813982 -0.038810603 -0.038794972 -0.03896948 -0.042195316][-0.042186759 -0.042191662 -0.042194333 -0.042196989 -0.042199653 -0.042203028 -0.042197369 -0.042029191 -0.038801689 -0.038802631 -0.038799025 -0.038796276 -0.038792893 -0.03897341 -0.042204969][-0.042184677 -0.042189237 -0.042191174 -0.042193014 -0.042194761 -0.042196956 -0.042190805 -0.042023 -0.042020179 -0.042022422 -0.042019725 -0.042017587 -0.042018015 -0.042097267 -0.042111013][-0.042187952 -0.04219225 -0.042193495 -0.042194657 -0.042189565 -0.042190149 -0.042184394 -0.04201765 -0.042016421 -0.042019669 -0.042017825 -0.042016171 -0.0420226 -0.042099338 -0.042118918][-0.042186432 -0.042190056 -0.042190686 -0.042191148 -0.042185187 -0.042184711 -0.042179335 -0.04217989 -0.042179585 -0.042178284 -0.042177048 -0.042176094 -0.042188704 -0.041997489 -0.041950479][-0.04219152 -0.042194303 -0.042188831 -0.042188838 -0.042182937 -0.042182874 -0.04217745 -0.042177968 -0.042177785 -0.042133246 -0.042134568 -0.042136632 -0.040655952 -0.040317267 -0.040269222][-0.042202432 -0.04220473 -0.042193506 -0.042187802 -0.042182237 -0.042182371 -0.042170875 -0.042171352 -0.042172477 -0.042131025 -0.042115565 -0.042122871 -0.040657546 -0.040318914 -0.040216129][-0.042205963 -0.042208266 -0.0421969 -0.042187095 -0.042181458 -0.042181551 -0.04218211 -0.042186078 -0.042192709 -0.042155441 -0.042144313 -0.0421172 -0.040612016 -0.040174056 -0.03962582][-0.042209603 -0.042211812 -0.042194754 -0.042184889 -0.042180806 -0.042181138 -0.042184882 -0.042194504 -0.04220045 -0.042158682 -0.041835409 -0.039253343 -0.033720572 -0.031168845 -0.027450558][-0.042209256 -0.042211261 -0.042194132 -0.042178679 -0.042180281 -0.042184018 -0.042190906 -0.042194374 -0.042048838 -0.0409302 -0.037196122 -0.030112306 -0.018083993 -0.010479268 -0.0010984465][-0.042213734 -0.042215571 -0.042204075 -0.04218873 -0.042187795 -0.042193025 -0.04210858 -0.042114131 -0.041769814 -0.037654996 -0.028329521 -0.015005296 0.0064091533 0.022719149 0.039360728]]...]
INFO - root - 2017-12-09 06:07:40.907457: step 5310, loss = 1.85, batch loss = 0.69 (17.7 examples/sec; 0.452 sec/batch; 41h:04m:17s remains)
INFO - root - 2017-12-09 06:07:45.376537: step 5320, loss = 1.84, batch loss = 0.68 (18.0 examples/sec; 0.445 sec/batch; 40h:27m:19s remains)
INFO - root - 2017-12-09 06:07:49.767628: step 5330, loss = 1.85, batch loss = 0.69 (20.0 examples/sec; 0.401 sec/batch; 36h:24m:14s remains)
INFO - root - 2017-12-09 06:07:54.225466: step 5340, loss = 1.85, batch loss = 0.69 (18.4 examples/sec; 0.434 sec/batch; 39h:28m:59s remains)
INFO - root - 2017-12-09 06:07:58.664420: step 5350, loss = 1.85, batch loss = 0.69 (18.3 examples/sec; 0.437 sec/batch; 39h:44m:27s remains)
INFO - root - 2017-12-09 06:08:03.080353: step 5360, loss = 1.85, batch loss = 0.69 (18.2 examples/sec; 0.440 sec/batch; 39h:56m:43s remains)
INFO - root - 2017-12-09 06:08:07.534652: step 5370, loss = 1.85, batch loss = 0.69 (17.9 examples/sec; 0.446 sec/batch; 40h:33m:41s remains)
INFO - root - 2017-12-09 06:08:11.782731: step 5380, loss = 1.84, batch loss = 0.67 (27.1 examples/sec; 0.296 sec/batch; 26h:51m:29s remains)
INFO - root - 2017-12-09 06:08:16.074146: step 5390, loss = 1.86, batch loss = 0.69 (18.6 examples/sec; 0.431 sec/batch; 39h:11m:02s remains)
INFO - root - 2017-12-09 06:08:20.491867: step 5400, loss = 1.86, batch loss = 0.69 (17.9 examples/sec; 0.447 sec/batch; 40h:38m:54s remains)
2017-12-09 06:08:21.002143: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.02558247 -0.020049021 -0.014490986 -0.012399727 -0.011099093 -0.010628697 -0.013123905 -0.015758764 -0.018785531 -0.020833481 -0.024343593 -0.026409319 -0.028319135 -0.031213913 -0.03475751][-0.011951078 -0.0055571571 0.0031875335 0.0074063763 0.0092349052 0.0084774271 0.0042217411 -0.00064743683 -0.0059467852 -0.010538626 -0.015835149 -0.019209271 -0.022685552 -0.026277978 -0.030732423][-0.0040299222 0.0047011003 0.016365033 0.022513326 0.026148383 0.028052341 0.022929583 0.015848462 0.0083788633 0.0018667094 -0.0060760304 -0.011180885 -0.017029265 -0.022068491 -0.027473751][0.0091226771 0.023755085 0.040791068 0.049873035 0.051842246 0.051662851 0.042251702 0.028823081 0.014693912 0.0056286044 -0.0051397756 -0.0095069557 -0.015722459 -0.019725373 -0.023725523][0.0194228 0.038764376 0.060084436 0.074408606 0.0812051 0.08209978 0.072055295 0.057484049 0.043120537 0.031698871 0.017641533 0.010983579 0.0011802018 -0.005905088 -0.013218183][0.030396488 0.055129293 0.082036957 0.10128689 0.1148365 0.11835726 0.10972224 0.09512265 0.08031778 0.067346767 0.047714408 0.035970535 0.020602401 0.009707693 -0.0020895526][0.026817482 0.051750172 0.076805249 0.09734726 0.11270732 0.11909953 0.1159502 0.10596369 0.097676218 0.087814823 0.069830105 0.055298086 0.034818839 0.019796968 0.0045331009][0.014366858 0.038971197 0.061355498 0.080150411 0.095267773 0.10301109 0.10220619 0.094872683 0.089483038 0.085187122 0.07190226 0.060122896 0.041797448 0.027021337 0.011902045][0.001366701 0.021845389 0.041698139 0.059670124 0.074355543 0.081539482 0.083556592 0.07959865 0.076387316 0.074543223 0.0644151 0.055209022 0.040614937 0.028540153 0.014192495][-0.011994652 0.0029278286 0.017467432 0.032221343 0.047959063 0.056024674 0.061223913 0.062095318 0.063215256 0.063387379 0.056617055 0.048041146 0.035665978 0.02454095 0.011214331][-0.022626016 -0.014524603 -0.0048535168 0.0044239759 0.014753401 0.020585265 0.026536975 0.029126082 0.0313962 0.033821624 0.030835655 0.024603564 0.01601385 0.008231923 -0.0014835261][-0.034314074 -0.032907426 -0.029970119 -0.027490348 -0.024420653 -0.021176467 -0.016554536 -0.013363903 -0.0099494532 -0.0054224655 -0.0028078258 -0.0041775256 -0.0069946907 -0.010530353 -0.015267892][-0.037580945 -0.038693171 -0.037348624 -0.037426457 -0.037109666 -0.03719582 -0.036838267 -0.036469404 -0.036475826 -0.033421103 -0.030101545 -0.027796052 -0.025060007 -0.024041595 -0.024741963][-0.03873422 -0.040266488 -0.03931668 -0.039697781 -0.039552014 -0.03961347 -0.039675228 -0.039668735 -0.040255081 -0.04020622 -0.039825417 -0.039026257 -0.036975075 -0.035643388 -0.035731465][-0.039945327 -0.041011728 -0.040928915 -0.041454151 -0.041356564 -0.041420113 -0.041488368 -0.041519608 -0.041525062 -0.0416283 -0.041675046 -0.041661784 -0.04095732 -0.040551465 -0.040529184]]...]
INFO - root - 2017-12-09 06:08:25.485338: step 5410, loss = 1.86, batch loss = 0.69 (18.3 examples/sec; 0.437 sec/batch; 39h:43m:16s remains)
INFO - root - 2017-12-09 06:08:29.884244: step 5420, loss = 1.87, batch loss = 0.70 (18.1 examples/sec; 0.443 sec/batch; 40h:12m:28s remains)
INFO - root - 2017-12-09 06:08:34.276594: step 5430, loss = 1.86, batch loss = 0.69 (18.3 examples/sec; 0.437 sec/batch; 39h:40m:16s remains)
INFO - root - 2017-12-09 06:08:38.606736: step 5440, loss = 1.87, batch loss = 0.69 (18.1 examples/sec; 0.442 sec/batch; 40h:09m:34s remains)
INFO - root - 2017-12-09 06:08:42.974854: step 5450, loss = 1.87, batch loss = 0.69 (18.7 examples/sec; 0.427 sec/batch; 38h:50m:11s remains)
INFO - root - 2017-12-09 06:08:47.255078: step 5460, loss = 1.87, batch loss = 0.69 (18.5 examples/sec; 0.433 sec/batch; 39h:22m:43s remains)
INFO - root - 2017-12-09 06:08:51.674964: step 5470, loss = 1.87, batch loss = 0.69 (18.4 examples/sec; 0.434 sec/batch; 39h:26m:40s remains)
INFO - root - 2017-12-09 06:08:56.123701: step 5480, loss = 1.96, batch loss = 0.77 (18.1 examples/sec; 0.441 sec/batch; 40h:04m:37s remains)
INFO - root - 2017-12-09 06:09:00.384123: step 5490, loss = 2.14, batch loss = 0.95 (16.8 examples/sec; 0.475 sec/batch; 43h:09m:35s remains)
INFO - root - 2017-12-09 06:09:04.793786: step 5500, loss = 2.24, batch loss = 1.05 (18.6 examples/sec; 0.429 sec/batch; 39h:00m:26s remains)
2017-12-09 06:09:05.391426: I tensorflow/core/kernels/logging_ops.cc:79] [[[1.958222 2.0058911 2.0210314 1.9977726 1.9340121 1.8269888 1.6742867 1.4836842 1.2795216 1.0953445 0.95987737 0.88792312 0.87407327 0.89588493 0.91377032][2.274035 2.3393774 2.3643529 2.3502321 2.2927775 2.18857 2.0346153 1.8373249 1.6224236 1.4239326 1.2712017 1.1790116 1.1446407 1.1474597 1.1467164][2.4908359 2.5737867 2.6077473 2.6053329 2.559696 2.4675453 2.3274333 2.1456575 1.944388 1.7544515 1.602671 1.5016029 1.4508965 1.4320731 1.4065946][2.61853 2.7197194 2.7619588 2.7679026 2.7336416 2.6589885 2.5428169 2.3910768 2.2227035 2.0625939 1.9308783 1.8351178 1.7758209 1.7375029 1.6864659][2.7020504 2.8210163 2.8653123 2.872782 2.8450236 2.7848036 2.6936011 2.5777042 2.4513464 2.3328354 2.2345507 2.1570091 2.0980246 2.0447741 1.9693743][2.7546141 2.8888762 2.9317136 2.9367034 2.9097683 2.858948 2.7886171 2.7052381 2.6195598 2.5430248 2.4798968 2.424803 2.37224 2.3111517 2.2181351][2.7856507 2.9273241 2.9627957 2.9606535 2.9302275 2.885555 2.8325236 2.7761996 2.7245023 2.6828842 2.6495214 2.6146009 2.5697138 2.5067973 2.4060783][2.7774222 2.9222591 2.9490981 2.9362953 2.8993006 2.8575668 2.8180137 2.7841125 2.7598326 2.7456107 2.7360327 2.7190592 2.6846123 2.626802 2.5298417][2.72638 2.8654532 2.8793495 2.8525317 2.8060305 2.7646186 2.7358415 2.7210555 2.720696 2.7301352 2.7418711 2.7436249 2.7250001 2.6819007 2.6010334][2.627327 2.7495613 2.739089 2.6933997 2.6338451 2.5892489 2.5685124 2.5713172 2.5943756 2.6289427 2.6659691 2.6928368 2.6996517 2.6831667 2.6312003][2.4598494 2.553283 2.5176892 2.4489861 2.3738072 2.3248808 2.3108082 2.3294358 2.3750012 2.43658 2.504185 2.5651126 2.60901 2.6315234 2.6199136][2.2337036 2.2886174 2.2215214 2.1262534 2.035357 1.9828824 1.9750241 2.0082517 2.0749376 2.1630709 2.2635205 2.3636079 2.451669 2.5198693 2.5512757][1.9435381 1.957781 1.8657357 1.7515609 1.6491898 1.5942259 1.5905963 1.6350596 1.7186017 1.829722 1.9603513 2.0976136 2.228631 2.3403668 2.4095418][1.6168506 1.5933781 1.4853863 1.3618484 1.256376 1.20118 1.1987718 1.2475656 1.3395171 1.4651228 1.6168776 1.7820309 1.9459294 2.088907 2.1816282][1.2686795 1.2187943 1.1092666 0.98973346 0.89106643 0.84084117 0.83952922 0.88606119 0.97564089 1.1023654 1.259915 1.4356743 1.6130947 1.7690016 1.869187]]...]
INFO - root - 2017-12-09 06:09:09.851644: step 5510, loss = 2.29, batch loss = 1.11 (17.7 examples/sec; 0.452 sec/batch; 41h:05m:46s remains)
INFO - root - 2017-12-09 06:09:14.228014: step 5520, loss = 2.29, batch loss = 1.10 (17.5 examples/sec; 0.456 sec/batch; 41h:25m:12s remains)
INFO - root - 2017-12-09 06:09:18.548726: step 5530, loss = 2.37, batch loss = 1.17 (17.9 examples/sec; 0.447 sec/batch; 40h:34m:44s remains)
INFO - root - 2017-12-09 06:09:22.826998: step 5540, loss = 2.30, batch loss = 1.11 (18.5 examples/sec; 0.432 sec/batch; 39h:15m:18s remains)
INFO - root - 2017-12-09 06:09:27.260600: step 5550, loss = 2.34, batch loss = 1.14 (18.4 examples/sec; 0.434 sec/batch; 39h:26m:25s remains)
INFO - root - 2017-12-09 06:09:31.596031: step 5560, loss = 2.39, batch loss = 1.20 (18.2 examples/sec; 0.439 sec/batch; 39h:53m:43s remains)
INFO - root - 2017-12-09 06:09:36.021069: step 5570, loss = 2.35, batch loss = 1.15 (17.9 examples/sec; 0.446 sec/batch; 40h:31m:13s remains)
INFO - root - 2017-12-09 06:09:40.577183: step 5580, loss = 2.33, batch loss = 1.13 (18.2 examples/sec; 0.439 sec/batch; 39h:53m:40s remains)
INFO - root - 2017-12-09 06:09:44.697278: step 5590, loss = 2.33, batch loss = 1.13 (17.3 examples/sec; 0.462 sec/batch; 41h:57m:19s remains)
INFO - root - 2017-12-09 06:09:49.163172: step 5600, loss = 2.29, batch loss = 1.09 (17.9 examples/sec; 0.447 sec/batch; 40h:33m:06s remains)
2017-12-09 06:09:49.707675: I tensorflow/core/kernels/logging_ops.cc:79] [[[1.0502989 1.1210799 1.1672547 1.1895298 1.1899681 1.1716542 1.1384447 1.0945973 1.043493 0.98953116 0.93678415 0.88873887 0.84533322 0.80634558 0.76830757][1.2654916 1.3702108 1.4387748 1.4763827 1.4868776 1.474546 1.4424862 1.3921217 1.3248141 1.2457314 1.1614 1.0788788 1.0027275 0.93609732 0.87734008][1.4466722 1.584883 1.675153 1.7295202 1.7536646 1.7523812 1.7273409 1.6769079 1.5997365 1.4998564 1.3857765 1.2686346 1.1583374 1.0622427 0.98181641][1.5919549 1.7600656 1.8676581 1.9362204 1.9736093 1.9854645 1.9720076 1.9287162 1.8505045 1.7391503 1.603964 1.4594662 1.3200223 1.197629 1.096288][1.6964357 1.8888302 2.0095038 2.0880036 2.1363611 2.1609118 2.1609633 2.1298854 2.0593772 1.9487224 1.8061764 1.6471881 1.4885488 1.3456656 1.2254313][1.7634112 1.9744743 2.1024058 2.1860569 2.241617 2.2761879 2.2886531 2.2715573 2.2148619 2.1153135 1.9789389 1.8197975 1.6542008 1.4989574 1.3631469][1.7939576 2.0182939 2.1506474 2.2373366 2.2977762 2.3396723 2.3619313 2.3575072 2.3162961 2.233151 2.1116502 1.9629813 1.8012308 1.6419064 1.4945599][1.7818658 2.0149012 2.1527314 2.2431693 2.3087656 2.3568823 2.3866148 2.3920624 2.3645656 2.2985795 2.1956804 2.0633941 1.9126272 1.7555486 1.6004999][1.7199446 1.9552405 2.0983632 2.1928792 2.2643104 2.31899 2.3556664 2.3692088 2.3525152 2.3006203 2.2141125 2.0979748 1.9601604 1.809466 1.6523639][1.6059197 1.8338596 1.9749423 2.071034 2.1470804 2.2074871 2.2498777 2.2694519 2.2597656 2.2170329 2.1419735 2.0387819 1.9137996 1.7732121 1.621963][1.4436129 1.6526122 1.7822748 1.8744785 1.950682 2.0131047 2.0577512 2.079654 2.0727434 2.0350845 1.9681228 1.8765819 1.7661012 1.6413131 1.5059369][1.2438493 1.4249657 1.537926 1.6204789 1.691591 1.7514868 1.7947624 1.8159596 1.8093891 1.774578 1.7146227 1.6356386 1.5427624 1.439834 1.3291789][1.0374862 1.1861566 1.2792401 1.3464772 1.4068973 1.4593 1.4978788 1.5172453 1.5119579 1.482175 1.431774 1.3670719 1.2932906 1.2141271 1.1311896][0.85857195 0.975124 1.0456285 1.0944655 1.1393729 1.1792226 1.2085856 1.2232299 1.218603 1.1947879 1.1561472 1.1071944 1.0527431 0.99724138 0.94182289][0.71182287 0.80235803 0.85393465 0.88629395 0.91469455 0.93827492 0.95603794 0.96310472 0.95737976 0.93797475 0.90936738 0.87350559 0.83565772 0.80083632 0.7687397]]...]
INFO - root - 2017-12-09 06:09:54.035804: step 5610, loss = 2.28, batch loss = 1.08 (18.2 examples/sec; 0.439 sec/batch; 39h:53m:23s remains)
INFO - root - 2017-12-09 06:09:58.309043: step 5620, loss = 2.28, batch loss = 1.07 (18.9 examples/sec; 0.423 sec/batch; 38h:26m:08s remains)
INFO - root - 2017-12-09 06:10:02.650793: step 5630, loss = 2.29, batch loss = 1.08 (17.4 examples/sec; 0.460 sec/batch; 41h:45m:33s remains)
INFO - root - 2017-12-09 06:10:07.225429: step 5640, loss = 2.28, batch loss = 1.07 (16.8 examples/sec; 0.477 sec/batch; 43h:16m:27s remains)
INFO - root - 2017-12-09 06:10:11.639046: step 5650, loss = 2.32, batch loss = 1.10 (18.0 examples/sec; 0.443 sec/batch; 40h:15m:50s remains)
INFO - root - 2017-12-09 06:10:15.997797: step 5660, loss = 2.25, batch loss = 1.04 (18.1 examples/sec; 0.443 sec/batch; 40h:12m:26s remains)
INFO - root - 2017-12-09 06:10:20.330901: step 5670, loss = 2.31, batch loss = 1.10 (18.9 examples/sec; 0.424 sec/batch; 38h:27m:36s remains)
INFO - root - 2017-12-09 06:10:24.694702: step 5680, loss = 2.25, batch loss = 1.03 (18.1 examples/sec; 0.441 sec/batch; 40h:02m:50s remains)
INFO - root - 2017-12-09 06:10:28.807540: step 5690, loss = 2.30, batch loss = 1.08 (18.2 examples/sec; 0.439 sec/batch; 39h:52m:00s remains)
INFO - root - 2017-12-09 06:10:33.188900: step 5700, loss = 2.24, batch loss = 1.02 (18.5 examples/sec; 0.431 sec/batch; 39h:09m:50s remains)
2017-12-09 06:10:33.679836: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.27343953 0.27562919 0.30470097 0.34755215 0.39622051 0.43697318 0.4648 0.4784587 0.48418552 0.48426181 0.48313695 0.48307908 0.48495698 0.48077512 0.47405624][0.32188809 0.31446689 0.33885971 0.38614237 0.44422385 0.49732882 0.53574288 0.55357432 0.5567894 0.55118471 0.54402357 0.53818786 0.5353182 0.52920049 0.52089286][0.32412383 0.30351055 0.31866172 0.35966328 0.41427761 0.4671303 0.50456244 0.5221191 0.52366555 0.517309 0.50831568 0.50098068 0.49581391 0.49513203 0.49152541][0.3136434 0.28077775 0.28238556 0.31177157 0.3590759 0.40801811 0.44378227 0.45987421 0.45919305 0.44917363 0.43515816 0.42237079 0.41294956 0.41058069 0.40994129][0.30374479 0.26019111 0.24942076 0.26739916 0.30639526 0.35013047 0.38373679 0.39870426 0.39595726 0.38193229 0.3621785 0.34310684 0.32798526 0.32115018 0.32060933][0.32094151 0.25145128 0.23304302 0.24318951 0.27595985 0.3158378 0.34729642 0.36023861 0.35392949 0.33410841 0.30658844 0.27888268 0.25561923 0.24207595 0.23692504][0.30620566 0.25785714 0.23691714 0.24474874 0.2754294 0.31381088 0.34369037 0.35368872 0.34220365 0.31460249 0.27741784 0.23914087 0.20613885 0.1843283 0.17312366][0.3188473 0.27530506 0.25631478 0.26430577 0.29506624 0.33391789 0.36351353 0.37139344 0.3554101 0.32002988 0.2729139 0.22415879 0.18172385 0.15165441 0.13440543][0.33813816 0.301184 0.28343728 0.28797755 0.31655788 0.35320872 0.38013145 0.38479215 0.36507866 0.3244738 0.27124077 0.21606877 0.16812493 0.13357174 0.11317693][0.35604656 0.32314044 0.30475444 0.30357787 0.3272095 0.35852438 0.38080576 0.38118556 0.35789952 0.31428993 0.25875369 0.20251374 0.15435375 0.12021303 0.10060699][0.36738846 0.33420482 0.30960843 0.30198136 0.31723326 0.34050322 0.35704473 0.35454294 0.33040082 0.28752646 0.23446617 0.18170711 0.13763967 0.10757768 0.091479182][0.37043437 0.33547255 0.30279416 0.28733385 0.29253307 0.30665103 0.31649876 0.31077793 0.2866503 0.24733329 0.19999176 0.154177 0.11745119 0.093826756 0.083080471][0.3695623 0.33097959 0.28994811 0.26386222 0.25841936 0.26376438 0.26751384 0.25968367 0.2370187 0.20246537 0.16251218 0.12549827 0.0974997 0.081233576 0.076286122][0.3731696 0.33017948 0.28120762 0.24321768 0.22714147 0.22392929 0.22186023 0.2126562 0.19257896 0.16348405 0.13095626 0.10215786 0.081903636 0.072003573 0.071682453][0.37865898 0.33014044 0.27308935 0.22562695 0.20169169 0.19222881 0.1866385 0.17680839 0.15929909 0.13536675 0.10929213 0.087168425 0.072985232 0.067698 0.070345253]]...]
INFO - root - 2017-12-09 06:10:38.032850: step 5710, loss = 2.30, batch loss = 1.08 (19.3 examples/sec; 0.415 sec/batch; 37h:40m:38s remains)
INFO - root - 2017-12-09 06:10:42.330929: step 5720, loss = 2.25, batch loss = 1.02 (17.8 examples/sec; 0.449 sec/batch; 40h:44m:47s remains)
INFO - root - 2017-12-09 06:10:46.642620: step 5730, loss = 2.18, batch loss = 0.95 (18.1 examples/sec; 0.442 sec/batch; 40h:05m:13s remains)
INFO - root - 2017-12-09 06:10:51.001991: step 5740, loss = 2.17, batch loss = 0.94 (18.4 examples/sec; 0.435 sec/batch; 39h:26m:31s remains)
INFO - root - 2017-12-09 06:10:55.366048: step 5750, loss = 2.16, batch loss = 0.93 (18.9 examples/sec; 0.423 sec/batch; 38h:25m:17s remains)
INFO - root - 2017-12-09 06:10:59.750162: step 5760, loss = 2.18, batch loss = 0.94 (18.5 examples/sec; 0.432 sec/batch; 39h:14m:18s remains)
INFO - root - 2017-12-09 06:11:04.019359: step 5770, loss = 2.22, batch loss = 0.97 (20.2 examples/sec; 0.396 sec/batch; 35h:54m:46s remains)
INFO - root - 2017-12-09 06:11:08.439570: step 5780, loss = 2.17, batch loss = 0.92 (18.2 examples/sec; 0.440 sec/batch; 39h:56m:35s remains)
INFO - root - 2017-12-09 06:11:12.503854: step 5790, loss = 2.11, batch loss = 0.86 (18.3 examples/sec; 0.437 sec/batch; 39h:40m:24s remains)
INFO - root - 2017-12-09 06:11:16.899739: step 5800, loss = 2.14, batch loss = 0.89 (18.3 examples/sec; 0.437 sec/batch; 39h:41m:50s remains)
2017-12-09 06:11:17.409455: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.71618682 0.66084659 0.62582088 0.62803066 0.68247688 0.79111326 0.9433915 1.1150217 1.2794658 1.4142854 1.4993701 1.5337802 1.5251342 1.4966985 1.4639125][0.934771 0.89678591 0.88564944 0.91201985 0.98344475 1.0936269 1.229864 1.371405 1.4954722 1.5835334 1.6185145 1.602531 1.5446721 1.4685143 1.394271][1.1594814 1.1533781 1.1756661 1.2246858 1.307112 1.4119395 1.5229672 1.6250026 1.7005522 1.7361468 1.7182224 1.6511223 1.5440031 1.419618 1.3025031][1.3727922 1.4110851 1.472312 1.5430088 1.6325651 1.7239627 1.8060122 1.869017 1.9027783 1.8985972 1.8417104 1.7365758 1.5906948 1.4251721 1.2653615][1.5147048 1.5992275 1.6972129 1.7872438 1.8818969 1.9615568 2.0230548 2.0603881 2.0701916 2.0461216 1.972814 1.8538996 1.691641 1.5035098 1.3155806][1.5473331 1.6663069 1.7909267 1.8942705 1.9928075 2.0695992 2.1260352 2.1566329 2.1624486 2.1378543 2.0695002 1.957274 1.7986159 1.607785 1.4104768][1.4674412 1.6059924 1.7465211 1.8589579 1.9621223 2.041605 2.102423 2.1389263 2.1545386 2.142832 2.0928371 1.9991807 1.8565246 1.6770442 1.4862417][1.2888354 1.4377402 1.586498 1.7037177 1.8102527 1.8945241 1.962396 2.0072858 2.0357935 2.0410254 2.0129929 1.9413835 1.8202519 1.6605927 1.4876404][1.051168 1.1974304 1.3435287 1.4630632 1.5703704 1.6563345 1.7280097 1.778783 1.8169736 1.8351277 1.8244154 1.7711035 1.6702538 1.5324878 1.383342][0.79894549 0.92788017 1.0570885 1.1682086 1.2676404 1.3511149 1.4232053 1.47856 1.5236002 1.5491552 1.549428 1.5078833 1.4233625 1.306547 1.1823407][0.58830464 0.69219559 0.795108 0.88493013 0.96481556 1.0358959 1.0991659 1.1512142 1.1958708 1.2232133 1.2285208 1.1937027 1.1224557 1.0259265 0.92596924][0.43620828 0.516566 0.59304219 0.65758175 0.71384931 0.76437658 0.80987734 0.84905469 0.88371128 0.9056505 0.90919715 0.87897104 0.82024473 0.74292713 0.66500312][0.3397322 0.40180045 0.45781529 0.50212848 0.53977346 0.57236338 0.59993982 0.62201929 0.63950378 0.64751863 0.64104849 0.6102491 0.56045872 0.49964744 0.44075412][0.29432914 0.34460503 0.38834873 0.42114913 0.44771853 0.46812451 0.48183471 0.48839951 0.48814648 0.47923994 0.4591924 0.42372647 0.3779943 0.32841137 0.2826409][0.28641573 0.33427578 0.37435329 0.40312105 0.42506951 0.43830782 0.44133091 0.43416509 0.41783881 0.3934927 0.36100763 0.3200745 0.2759037 0.23242015 0.19417864]]...]
INFO - root - 2017-12-09 06:11:21.764522: step 5810, loss = 2.20, batch loss = 0.95 (18.7 examples/sec; 0.429 sec/batch; 38h:54m:55s remains)
INFO - root - 2017-12-09 06:11:26.128723: step 5820, loss = 2.22, batch loss = 0.97 (19.0 examples/sec; 0.421 sec/batch; 38h:13m:47s remains)
INFO - root - 2017-12-09 06:11:30.508654: step 5830, loss = 2.15, batch loss = 0.90 (17.8 examples/sec; 0.449 sec/batch; 40h:43m:27s remains)
INFO - root - 2017-12-09 06:11:35.006396: step 5840, loss = 2.16, batch loss = 0.90 (18.1 examples/sec; 0.443 sec/batch; 40h:09m:54s remains)
INFO - root - 2017-12-09 06:11:39.421623: step 5850, loss = 2.16, batch loss = 0.90 (17.8 examples/sec; 0.449 sec/batch; 40h:46m:35s remains)
INFO - root - 2017-12-09 06:11:43.867048: step 5860, loss = 2.18, batch loss = 0.92 (17.9 examples/sec; 0.447 sec/batch; 40h:33m:08s remains)
INFO - root - 2017-12-09 06:11:48.472140: step 5870, loss = 2.13, batch loss = 0.87 (17.6 examples/sec; 0.455 sec/batch; 41h:17m:52s remains)
INFO - root - 2017-12-09 06:11:52.920612: step 5880, loss = 2.19, batch loss = 0.93 (18.5 examples/sec; 0.433 sec/batch; 39h:18m:35s remains)
INFO - root - 2017-12-09 06:11:57.045262: step 5890, loss = 2.15, batch loss = 0.89 (18.6 examples/sec; 0.430 sec/batch; 39h:02m:57s remains)
INFO - root - 2017-12-09 06:12:01.449137: step 5900, loss = 2.14, batch loss = 0.88 (17.6 examples/sec; 0.456 sec/batch; 41h:20m:24s remains)
2017-12-09 06:12:01.974718: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.064782523 0.072940022 0.075448789 0.077866413 0.079703763 0.081645451 0.083422668 0.085351042 0.085926287 0.089490794 0.0917596 0.097881012 0.10096002 0.10283119 0.10688073][0.083727725 0.089656 0.089600347 0.088420473 0.089319535 0.090323679 0.091421984 0.093941428 0.095060579 0.099036969 0.10181653 0.107636 0.11052734 0.11166392 0.11521056][0.099386372 0.10747597 0.10584045 0.098963551 0.095326908 0.09189292 0.089677833 0.090060271 0.092162631 0.098140441 0.10128727 0.10683125 0.10939566 0.10955221 0.11235397][0.1068187 0.11914643 0.11959108 0.10802438 0.10154935 0.093514033 0.087946512 0.085628934 0.086406104 0.093152143 0.095039792 0.098651193 0.10022608 0.099960633 0.10285907][0.099862508 0.11300667 0.11424207 0.10116834 0.093934439 0.085350372 0.080707118 0.079331532 0.080864251 0.088485636 0.089473061 0.0907565 0.089221857 0.086880766 0.087782316][0.088273756 0.10099823 0.10154178 0.08797463 0.081192672 0.074488923 0.0722005 0.073022038 0.076383382 0.083115928 0.082412258 0.080792233 0.077024907 0.072876133 0.072057426][0.076245457 0.087301068 0.087445267 0.074376807 0.069367051 0.065668352 0.066853747 0.070284031 0.075193726 0.081563182 0.0792004 0.074359894 0.067147017 0.060366586 0.057195954][0.054522879 0.066951953 0.069970876 0.061238393 0.058867328 0.058539033 0.062809445 0.067941926 0.073175371 0.078960918 0.075046837 0.068276674 0.059208937 0.051139683 0.047071278][0.026617445 0.035980962 0.041006804 0.038148515 0.041155234 0.046099313 0.054670982 0.063009039 0.067760006 0.071615286 0.066936463 0.059565619 0.050150745 0.041907437 0.037733071][0.00099251792 0.0053425878 0.008393921 0.010487292 0.016890157 0.027082421 0.039594747 0.050412096 0.05613289 0.057532109 0.052661777 0.044829547 0.035409339 0.02716098 0.0224398][-0.012779213 -0.0104421 -0.0084669814 -0.0052143112 0.0017943718 0.012349058 0.02392114 0.033254325 0.038296387 0.0384797 0.033966176 0.026414461 0.017807212 0.010262843 0.0053959116][-0.022877542 -0.021521077 -0.019701267 -0.016405119 -0.010124963 -0.0015601665 0.0071422122 0.013618149 0.016669068 0.016226355 0.012454621 0.006395705 -0.00025754049 -0.0060377046 -0.0098766424][-0.030889563 -0.03029242 -0.028843123 -0.026131922 -0.021422749 -0.015482258 -0.009852577 -0.0060673282 -0.0047062933 -0.00553295 -0.0084007718 -0.012571428 -0.016977457 -0.020715728 -0.023179723][-0.036502421 -0.036399577 -0.0353855 -0.033544611 -0.030590987 -0.027122252 -0.024057936 -0.022317527 -0.022095911 -0.023003893 -0.024963614 -0.02750881 -0.030051962 -0.032121785 -0.033451103][-0.040154688 -0.0398929 -0.039233036 -0.038201515 -0.036692854 -0.035045773 -0.033690643 -0.033158667 -0.033458561 -0.034183774 -0.0353436 -0.03666538 -0.037874758 -0.038799528 -0.039370693]]...]
INFO - root - 2017-12-09 06:12:06.340995: step 5910, loss = 2.17, batch loss = 0.90 (18.4 examples/sec; 0.435 sec/batch; 39h:27m:23s remains)
INFO - root - 2017-12-09 06:12:10.711383: step 5920, loss = 2.15, batch loss = 0.88 (18.3 examples/sec; 0.438 sec/batch; 39h:45m:33s remains)
INFO - root - 2017-12-09 06:12:15.079125: step 5930, loss = 2.18, batch loss = 0.91 (18.5 examples/sec; 0.432 sec/batch; 39h:12m:59s remains)
INFO - root - 2017-12-09 06:12:19.453806: step 5940, loss = 1.96, batch loss = 0.68 (18.1 examples/sec; 0.442 sec/batch; 40h:06m:42s remains)
INFO - root - 2017-12-09 06:12:23.947124: step 5950, loss = 1.97, batch loss = 0.70 (18.2 examples/sec; 0.439 sec/batch; 39h:50m:28s remains)
INFO - root - 2017-12-09 06:12:28.417275: step 5960, loss = 1.98, batch loss = 0.69 (17.4 examples/sec; 0.460 sec/batch; 41h:42m:47s remains)
INFO - root - 2017-12-09 06:12:32.789460: step 5970, loss = 1.97, batch loss = 0.68 (17.9 examples/sec; 0.446 sec/batch; 40h:26m:32s remains)
INFO - root - 2017-12-09 06:12:37.120941: step 5980, loss = 1.98, batch loss = 0.69 (18.7 examples/sec; 0.427 sec/batch; 38h:42m:02s remains)
INFO - root - 2017-12-09 06:12:41.171158: step 5990, loss = 1.99, batch loss = 0.69 (19.2 examples/sec; 0.417 sec/batch; 37h:46m:49s remains)
INFO - root - 2017-12-09 06:12:45.588651: step 6000, loss = 1.98, batch loss = 0.69 (18.3 examples/sec; 0.436 sec/batch; 39h:34m:11s remains)
2017-12-09 06:12:46.064280: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.042466469 -0.042462878 -0.042459838 -0.042458065 -0.042456251 -0.042463526 -0.042469297 -0.04247174 -0.042473868 -0.042482339 -0.042493135 -0.042483769 -0.042475276 -0.042467594 -0.042459823][-0.042503886 -0.042503886 -0.042503886 -0.042503886 -0.042503886 -0.042503886 -0.042503886 -0.042503577 -0.042502046 -0.042500988 -0.042499039 -0.042496596 -0.042496189 -0.042494655 -0.042493973][-0.042503886 -0.042503886 -0.042503886 -0.042503886 -0.042503886 -0.042503886 -0.042503886 -0.042503886 -0.042503886 -0.042503886 -0.042503886 -0.042503886 -0.042503886 -0.042503886 -0.042503886][-0.042503886 -0.042503886 -0.042503886 -0.042503886 -0.042503886 -0.042503886 -0.042503886 -0.042503886 -0.042503886 -0.042503886 -0.042503886 -0.042503886 -0.042503886 -0.042503886 -0.042503886][-0.04250383 -0.042503886 -0.042503886 -0.042503886 -0.042503886 -0.042503886 -0.042503886 -0.042503886 -0.042503886 -0.042503886 -0.042503886 -0.0425038 -0.042503886 -0.042503886 -0.042503834][-0.0425037 -0.042503886 -0.042503886 -0.042503886 -0.042503886 -0.042503886 -0.042503886 -0.042503886 -0.042503886 -0.042503886 -0.042503886 -0.042503886 -0.042503562 -0.042503707 -0.042503115][-0.042503286 -0.042503886 -0.042503886 -0.042503886 -0.042503886 -0.042503886 -0.042503886 -0.042503886 -0.042503886 -0.042503886 -0.042503886 -0.042503886 -0.042503875 -0.042503703 -0.042502794][-0.042503569 -0.042503886 -0.042503886 -0.042503886 -0.042503886 -0.042503886 -0.042503886 -0.042503886 -0.042503886 -0.042503886 -0.042503886 -0.042503886 -0.042503886 -0.0425037 -0.042503886][-0.042503282 -0.042503886 -0.042503886 -0.042503886 -0.042503886 -0.042503886 -0.042503886 -0.042503886 -0.042503886 -0.042503886 -0.042503886 -0.042503886 -0.042503886 -0.042503886 -0.042503886][-0.042503476 -0.042502422 -0.042503886 -0.042503886 -0.042503886 -0.042503886 -0.042503886 -0.042503886 -0.042503886 -0.042503886 -0.042503886 -0.042503886 -0.042503886 -0.042503886 -0.042503886][-0.042501494 -0.042502817 -0.042503886 -0.042503886 -0.042503886 -0.042503886 -0.042503886 -0.042503886 -0.042503886 -0.042503886 -0.042503886 -0.042503886 -0.042503886 -0.042503886 -0.042503886][-0.042501543 -0.042502429 -0.042503886 -0.042503886 -0.042503886 -0.042503886 -0.042503886 -0.042503886 -0.042503886 -0.042503886 -0.042503886 -0.042503886 -0.042503886 -0.042503886 -0.042503886][-0.042501412 -0.042502884 -0.042503886 -0.042503886 -0.042503886 -0.042503886 -0.042503886 -0.042503886 -0.042503886 -0.042503886 -0.042503886 -0.042503886 -0.042503886 -0.042503886 -0.042503886][-0.042500019 -0.042503517 -0.042503886 -0.042503886 -0.042503886 -0.042503886 -0.042503886 -0.042503886 -0.042503886 -0.042503886 -0.042503886 -0.042503886 -0.042503886 -0.042503886 -0.042503886][-0.042499319 -0.042503208 -0.042503886 -0.042503629 -0.042503666 -0.042503886 -0.042503886 -0.042503886 -0.042503886 -0.042502239 -0.042502236 -0.042502955 -0.042502981 -0.042503316 -0.042503398]]...]
INFO - root - 2017-12-09 06:12:50.328884: step 6010, loss = 1.98, batch loss = 0.69 (18.6 examples/sec; 0.431 sec/batch; 39h:05m:03s remains)
INFO - root - 2017-12-09 06:12:54.757128: step 6020, loss = 1.99, batch loss = 0.69 (17.4 examples/sec; 0.459 sec/batch; 41h:35m:53s remains)
INFO - root - 2017-12-09 06:12:59.228786: step 6030, loss = 1.99, batch loss = 0.69 (18.1 examples/sec; 0.442 sec/batch; 40h:03m:11s remains)
INFO - root - 2017-12-09 06:13:03.631690: step 6040, loss = 1.99, batch loss = 0.69 (18.7 examples/sec; 0.428 sec/batch; 38h:46m:48s remains)
INFO - root - 2017-12-09 06:13:08.071891: step 6050, loss = 1.99, batch loss = 0.69 (17.5 examples/sec; 0.458 sec/batch; 41h:33m:33s remains)
INFO - root - 2017-12-09 06:13:12.402481: step 6060, loss = 1.99, batch loss = 0.69 (18.3 examples/sec; 0.436 sec/batch; 39h:32m:09s remains)
INFO - root - 2017-12-09 06:13:16.814456: step 6070, loss = 1.98, batch loss = 0.67 (18.5 examples/sec; 0.433 sec/batch; 39h:15m:15s remains)
INFO - root - 2017-12-09 06:13:21.055261: step 6080, loss = 1.99, batch loss = 0.69 (18.8 examples/sec; 0.427 sec/batch; 38h:40m:58s remains)
INFO - root - 2017-12-09 06:13:25.193385: step 6090, loss = 2.00, batch loss = 0.69 (30.3 examples/sec; 0.264 sec/batch; 23h:56m:05s remains)
INFO - root - 2017-12-09 06:13:29.545203: step 6100, loss = 2.00, batch loss = 0.69 (18.3 examples/sec; 0.437 sec/batch; 39h:37m:42s remains)
2017-12-09 06:13:30.066923: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.040988844 -0.040988777 -0.040988985 -0.040988971 -0.040988814 -0.040988382 -0.040988002 -0.040987503 -0.040987235 -0.040986933 -0.040986136 -0.040986821 -0.040987108 -0.040988076 -0.0409885][-0.04099007 -0.040990192 -0.040990274 -0.040990464 -0.040990524 -0.040990114 -0.040989928 -0.040990233 -0.040990341 -0.04099039 -0.040990397 -0.040990517 -0.040990535 -0.040990546 -0.04099055][-0.040989961 -0.04098998 -0.04099011 -0.040990219 -0.040990282 -0.040990312 -0.040990315 -0.040990233 -0.040990166 -0.040990129 -0.040990122 -0.040990427 -0.040990531 -0.040990565 -0.04099055][-0.040990375 -0.040990409 -0.040990461 -0.040990505 -0.040990546 -0.040990565 -0.040990565 -0.040990565 -0.040990565 -0.040990565 -0.040990565 -0.040990565 -0.040990565 -0.040990565 -0.040990464][-0.040990263 -0.040990405 -0.040990468 -0.0409905 -0.040990535 -0.040990561 -0.040990565 -0.040990565 -0.040990565 -0.040990565 -0.040990565 -0.040990565 -0.040990565 -0.040990565 -0.040990371][-0.040989831 -0.040990312 -0.040990479 -0.040990528 -0.04099055 -0.040990561 -0.040990565 -0.040990565 -0.040990565 -0.040990565 -0.040990565 -0.040990565 -0.040990565 -0.040990565 -0.040990341][-0.040989973 -0.040990375 -0.0409905 -0.040990558 -0.040990565 -0.040990565 -0.040990565 -0.040990565 -0.040990565 -0.040990565 -0.040990565 -0.040990565 -0.040990565 -0.040990565 -0.040990159][-0.040990047 -0.04099014 -0.040990531 -0.040990561 -0.040990565 -0.040990565 -0.040990565 -0.040990565 -0.040990565 -0.040990565 -0.040990565 -0.040990565 -0.040990565 -0.040990565 -0.040990222][-0.040989585 -0.04098979 -0.04099055 -0.040990565 -0.040990565 -0.040990565 -0.040990565 -0.040990565 -0.040990565 -0.040990565 -0.040990565 -0.040990565 -0.040990565 -0.040990565 -0.040990308][-0.040988185 -0.040989432 -0.040990539 -0.040990561 -0.040990565 -0.040990565 -0.040990565 -0.040990565 -0.040990565 -0.040990565 -0.040990565 -0.040990565 -0.040990565 -0.040990565 -0.040990397][-0.040986657 -0.0409899 -0.04099055 -0.040990565 -0.040990565 -0.040990565 -0.040990565 -0.040990565 -0.040990565 -0.040990565 -0.040990565 -0.040990565 -0.040990565 -0.040990565 -0.040990565][-0.040986083 -0.040990431 -0.040990565 -0.040990565 -0.040990565 -0.040990565 -0.040990565 -0.040990565 -0.040990565 -0.040990565 -0.040990565 -0.040990565 -0.040990565 -0.040990565 -0.040990565][-0.040987108 -0.040990215 -0.040990565 -0.040990565 -0.040990565 -0.040990565 -0.040990565 -0.040990565 -0.040990565 -0.040990565 -0.040990565 -0.040990565 -0.040990565 -0.040990565 -0.040990565][-0.040988743 -0.040989466 -0.040990565 -0.040990565 -0.040990565 -0.040990565 -0.040990565 -0.040990565 -0.040990565 -0.040990565 -0.040990565 -0.040990565 -0.040990565 -0.040990565 -0.040990565][-0.040989514 -0.040988348 -0.040990565 -0.040990565 -0.040990565 -0.040990565 -0.040990565 -0.040990565 -0.040990565 -0.040990565 -0.040990565 -0.040990565 -0.040990565 -0.040990565 -0.040990565]]...]
INFO - root - 2017-12-09 06:13:34.361046: step 6110, loss = 2.00, batch loss = 0.69 (18.7 examples/sec; 0.428 sec/batch; 38h:46m:07s remains)
INFO - root - 2017-12-09 06:13:38.754065: step 6120, loss = 2.01, batch loss = 0.69 (18.6 examples/sec; 0.430 sec/batch; 38h:59m:14s remains)
INFO - root - 2017-12-09 06:13:43.103517: step 6130, loss = 2.01, batch loss = 0.69 (18.3 examples/sec; 0.437 sec/batch; 39h:38m:52s remains)
INFO - root - 2017-12-09 06:13:47.413232: step 6140, loss = 2.01, batch loss = 0.69 (18.1 examples/sec; 0.441 sec/batch; 39h:59m:52s remains)
INFO - root - 2017-12-09 06:13:51.725085: step 6150, loss = 2.01, batch loss = 0.69 (18.7 examples/sec; 0.427 sec/batch; 38h:41m:11s remains)
INFO - root - 2017-12-09 06:13:56.156984: step 6160, loss = 2.01, batch loss = 0.69 (18.2 examples/sec; 0.439 sec/batch; 39h:48m:49s remains)
INFO - root - 2017-12-09 06:14:00.625538: step 6170, loss = 2.01, batch loss = 0.69 (17.5 examples/sec; 0.457 sec/batch; 41h:26m:40s remains)
INFO - root - 2017-12-09 06:14:04.957016: step 6180, loss = 2.01, batch loss = 0.69 (18.3 examples/sec; 0.438 sec/batch; 39h:43m:35s remains)
INFO - root - 2017-12-09 06:14:09.401517: step 6190, loss = 2.02, batch loss = 0.69 (18.3 examples/sec; 0.436 sec/batch; 39h:31m:21s remains)
INFO - root - 2017-12-09 06:14:13.532881: step 6200, loss = 2.02, batch loss = 0.69 (18.1 examples/sec; 0.441 sec/batch; 40h:00m:23s remains)
2017-12-09 06:14:14.062084: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.041553825 -0.0415543 -0.04155479 -0.04155492 -0.041554384 -0.041553188 -0.041553229 -0.041553009 -0.041553512 -0.04155492 -0.04155492 -0.04155492 -0.04155492 -0.041553959 -0.04154839][-0.04155492 -0.04155492 -0.04155492 -0.04155492 -0.04155492 -0.04155492 -0.04155492 -0.04155492 -0.04155492 -0.04155492 -0.04155492 -0.04155492 -0.04155492 -0.041554339 -0.041554194][-0.04155492 -0.04155492 -0.04155492 -0.04155492 -0.04155492 -0.04155492 -0.04155492 -0.04155492 -0.04155492 -0.04155492 -0.04155492 -0.04155492 -0.04155492 -0.04155492 -0.04155492][-0.04155492 -0.04155492 -0.04155492 -0.04155492 -0.04155492 -0.04155492 -0.04155492 -0.04155492 -0.04155492 -0.04155492 -0.04155492 -0.04155492 -0.04155492 -0.04155492 -0.04155492][-0.04155492 -0.04155492 -0.04155492 -0.04155492 -0.04155492 -0.04155492 -0.04155492 -0.04155492 -0.04155492 -0.04155492 -0.04155492 -0.04155492 -0.04155492 -0.04155492 -0.04155492][-0.04155492 -0.04155492 -0.04155492 -0.04155492 -0.04155492 -0.04155492 -0.04155492 -0.04155492 -0.04155492 -0.04155492 -0.04155492 -0.04155492 -0.04155492 -0.04155492 -0.04155492][-0.041554794 -0.04155492 -0.04155492 -0.04155492 -0.04155492 -0.04155492 -0.04155492 -0.04155492 -0.04155492 -0.04155492 -0.04155492 -0.04155492 -0.04155492 -0.04155492 -0.04155492][-0.041554548 -0.04155492 -0.04155492 -0.04155492 -0.04155492 -0.04155492 -0.04155492 -0.04155492 -0.04155492 -0.04155492 -0.04155492 -0.04155492 -0.04155492 -0.04155492 -0.04155492][-0.04155454 -0.04155492 -0.04155492 -0.04155492 -0.04155492 -0.04155492 -0.04155492 -0.04155492 -0.04155492 -0.04155492 -0.04155492 -0.04155492 -0.04155492 -0.04155492 -0.04155492][-0.041554548 -0.04155492 -0.04155492 -0.04155492 -0.04155492 -0.04155492 -0.04155492 -0.04155492 -0.04155492 -0.04155492 -0.04155492 -0.04155492 -0.04155492 -0.04155492 -0.04155492][-0.041554589 -0.04155492 -0.04155492 -0.04155492 -0.04155492 -0.04155492 -0.04155492 -0.04155492 -0.04155492 -0.04155492 -0.04155492 -0.04155492 -0.04155492 -0.04155492 -0.04155492][-0.041554779 -0.041554779 -0.04155492 -0.04155492 -0.04155492 -0.04155492 -0.04155492 -0.04155492 -0.04155492 -0.04155492 -0.04155492 -0.04155492 -0.04155492 -0.04155492 -0.04155492][-0.041554723 -0.041554864 -0.04155492 -0.04155492 -0.04155492 -0.04155492 -0.04155492 -0.04155492 -0.04155492 -0.04155492 -0.04155492 -0.04155492 -0.04155492 -0.04155492 -0.04155492][-0.04155492 -0.041554797 -0.04155492 -0.04155492 -0.04155492 -0.04155492 -0.04155492 -0.04155492 -0.04155492 -0.04155492 -0.04155492 -0.04155492 -0.04155492 -0.04155492 -0.04155492][-0.04155492 -0.041554898 -0.04155492 -0.04155492 -0.04155492 -0.04155492 -0.04155492 -0.04155492 -0.04155492 -0.04155492 -0.04155492 -0.04155492 -0.04155492 -0.04155492 -0.04155492]]...]
INFO - root - 2017-12-09 06:14:18.426248: step 6210, loss = 2.04, batch loss = 0.71 (18.8 examples/sec; 0.425 sec/batch; 38h:31m:17s remains)
INFO - root - 2017-12-09 06:14:22.800667: step 6220, loss = 2.02, batch loss = 0.69 (18.1 examples/sec; 0.442 sec/batch; 40h:04m:31s remains)
INFO - root - 2017-12-09 06:14:27.186846: step 6230, loss = 2.02, batch loss = 0.69 (19.2 examples/sec; 0.417 sec/batch; 37h:48m:46s remains)
INFO - root - 2017-12-09 06:14:31.488399: step 6240, loss = 2.02, batch loss = 0.69 (19.0 examples/sec; 0.421 sec/batch; 38h:07m:08s remains)
INFO - root - 2017-12-09 06:14:35.801155: step 6250, loss = 2.02, batch loss = 0.69 (18.1 examples/sec; 0.442 sec/batch; 40h:05m:13s remains)
INFO - root - 2017-12-09 06:14:40.322093: step 6260, loss = 2.03, batch loss = 0.69 (18.3 examples/sec; 0.438 sec/batch; 39h:42m:47s remains)
INFO - root - 2017-12-09 06:14:44.788541: step 6270, loss = 2.02, batch loss = 0.69 (17.0 examples/sec; 0.470 sec/batch; 42h:34m:30s remains)
INFO - root - 2017-12-09 06:14:49.312633: step 6280, loss = 2.02, batch loss = 0.68 (16.4 examples/sec; 0.488 sec/batch; 44h:14m:00s remains)
INFO - root - 2017-12-09 06:14:53.766696: step 6290, loss = 2.03, batch loss = 0.69 (18.2 examples/sec; 0.440 sec/batch; 39h:52m:53s remains)
INFO - root - 2017-12-09 06:14:57.809248: step 6300, loss = 2.04, batch loss = 0.69 (18.9 examples/sec; 0.423 sec/batch; 38h:21m:58s remains)
2017-12-09 06:14:58.315278: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.040180944 -0.040171925 -0.040163327 -0.040164523 -0.040165033 -0.040166158 -0.040166609 -0.04016735 -0.040168244 -0.040166631 -0.040166982 -0.04016633 -0.0401661 -0.040166825 -0.040168345][-0.04018518 -0.040178053 -0.040171254 -0.040171955 -0.040172298 -0.04017216 -0.040172171 -0.040172368 -0.040172663 -0.040172696 -0.040172596 -0.040172528 -0.040172521 -0.0401727 -0.040172782][-0.040172279 -0.040172458 -0.040172677 -0.040172786 -0.0401728 -0.0401728 -0.0401728 -0.0401728 -0.0401728 -0.0401728 -0.0401728 -0.0401728 -0.0401728 -0.0401728 -0.0401728][-0.040172789 -0.040172663 -0.040172651 -0.040172748 -0.040172797 -0.040172797 -0.0401728 -0.0401728 -0.0401728 -0.0401728 -0.0401728 -0.0401728 -0.0401728 -0.0401728 -0.0401728][-0.040172778 -0.0401728 -0.040172771 -0.040172774 -0.040172797 -0.0401728 -0.0401728 -0.0401728 -0.0401728 -0.0401728 -0.0401728 -0.0401728 -0.0401728 -0.0401728 -0.0401728][-0.040172752 -0.040172663 -0.040172596 -0.0401728 -0.0401728 -0.0401728 -0.0401728 -0.0401728 -0.0401728 -0.0401728 -0.0401728 -0.0401728 -0.0401728 -0.0401728 -0.0401728][-0.04017242 -0.040172745 -0.040172417 -0.040172771 -0.040172782 -0.0401728 -0.0401728 -0.0401728 -0.0401728 -0.0401728 -0.0401728 -0.0401728 -0.0401728 -0.0401728 -0.0401728][-0.040171009 -0.040171634 -0.040171973 -0.040172517 -0.040172733 -0.040172778 -0.0401728 -0.0401728 -0.0401728 -0.0401728 -0.0401728 -0.0401728 -0.0401728 -0.0401728 -0.0401728][-0.040170979 -0.040171243 -0.040171828 -0.040172409 -0.040172745 -0.040172763 -0.0401728 -0.0401728 -0.0401728 -0.0401728 -0.0401728 -0.0401728 -0.0401728 -0.0401728 -0.0401728][-0.040172268 -0.040172353 -0.040172644 -0.040172786 -0.040172786 -0.0401728 -0.0401728 -0.0401728 -0.0401728 -0.0401728 -0.0401728 -0.0401728 -0.0401728 -0.0401728 -0.0401728][-0.040170662 -0.040171526 -0.0401728 -0.0401728 -0.0401728 -0.0401728 -0.0401728 -0.0401728 -0.0401728 -0.0401728 -0.0401728 -0.0401728 -0.0401728 -0.0401728 -0.0401728][-0.04017074 -0.040171884 -0.0401728 -0.0401728 -0.0401728 -0.0401728 -0.0401728 -0.0401728 -0.0401728 -0.0401728 -0.0401728 -0.0401728 -0.0401728 -0.0401728 -0.0401728][-0.040170565 -0.040172458 -0.0401728 -0.0401728 -0.0401728 -0.0401728 -0.0401728 -0.0401728 -0.0401728 -0.0401728 -0.0401728 -0.0401728 -0.0401728 -0.0401728 -0.0401728][-0.040170792 -0.040172752 -0.0401728 -0.0401728 -0.0401728 -0.0401728 -0.0401728 -0.0401728 -0.0401728 -0.0401728 -0.0401728 -0.0401728 -0.0401728 -0.0401728 -0.0401728][-0.040169138 -0.040172715 -0.0401728 -0.0401728 -0.0401728 -0.0401728 -0.0401728 -0.0401728 -0.0401728 -0.0401728 -0.0401728 -0.0401728 -0.0401728 -0.0401728 -0.040172767]]...]
