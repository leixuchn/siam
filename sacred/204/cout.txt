INFO - Siam-FC - Running command 'main'
INFO - Siam-FC - Started run with ID "204"
INFO - root - Creating training directory: /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-SGD-lr0.001-clip50-initfromfixqian30k-fixqian
INFO - root - preproces -- siamese_fc_color
WARNING - root - root is not explicitly specified, using default value: None
INFO - root - For training, we use instance size 255 - 2 * 8 ...
WARNING - root - init_method is not explicitly specified, using default value: None
sdiufhasudf Tensor("siamese_fc/conv5/split:0", shape=(8, 8, 8, 192), dtype=float32)
Tensor("siamese_fc/conv5/def/transpose:0", shape=(8, 192, 8, 8), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/b1/weights:0' shape=(128, 192, 3, 3) dtype=float32_ref> Tensor("siamese_fc/conv5/def/transpose_1:0", shape=(8, 72, 8, 8), dtype=float32)
ddd Tensor("siamese_fc/conv5/def/b1/transpose:0", shape=(8, 6, 6, 128), dtype=float32)
Tensor("siamese_fc/conv5/def/transpose_2:0", shape=(8, 192, 8, 8), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/b2/weights:0' shape=(128, 192, 3, 3) dtype=float32_ref> Tensor("siamese_fc/conv5/def/transpose_3:0", shape=(8, 72, 8, 8), dtype=float32)
sdiufhasudf Tensor("siamese_fc_1/conv5/split:0", shape=(8, 22, 22, 192), dtype=float32)
Tensor("siamese_fc_1/conv5/def/transpose:0", shape=(8, 192, 22, 22), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/b1/weights:0' shape=(128, 192, 3, 3) dtype=float32_ref> Tensor("siamese_fc_1/conv5/def/transpose_1:0", shape=(8, 72, 22, 22), dtype=float32)
ddd Tensor("siamese_fc_1/conv5/def/b1/transpose:0", shape=(8, 20, 20, 128), dtype=float32)
Tensor("siamese_fc_1/conv5/def/transpose_2:0", shape=(8, 192, 22, 22), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/b2/weights:0' shape=(128, 192, 3, 3) dtype=float32_ref> Tensor("siamese_fc_1/conv5/def/transpose_3:0", shape=(8, 72, 22, 22), dtype=float32)
Tensor("detection/add:0", shape=(8, 15, 15), dtype=float32)
INFO - root - Model moving average is disabled since decay factor is 0
2017-12-11 02:56:17.667370: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-11 02:56:17.667411: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-11 02:56:17.667417: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2017-12-11 02:56:17.667421: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-11 02:56:17.667425: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
2017-12-11 02:56:18.467766: I tensorflow/core/common_runtime/gpu/gpu_device.cc:887] Found device 0 with properties: 
name: Tesla K80
major: 3 minor: 7 memoryClockRate (GHz) 0.8235
pciBusID 6bee:00:00.0
Total memory: 11.17GiB
Free memory: 11.10GiB
2017-12-11 02:56:18.467802: I tensorflow/core/common_runtime/gpu/gpu_device.cc:908] DMA: 0 
2017-12-11 02:56:18.467809: I tensorflow/core/common_runtime/gpu/gpu_device.cc:918] 0:   Y 
2017-12-11 02:56:18.467817: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla K80, pci bus id: 6bee:00:00.0)
INFO:tensorflow:Restoring parameters from /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-momentum-lr0.001-clip50-initconv1-4-fixqian/model.ckpt-30000
INFO - tensorflow - Restoring parameters from /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-momentum-lr0.001-clip50-initconv1-4-fixqian/model.ckpt-30000
INFO - root - Starting threads 0 ...

INFO - root - training for 332500 steps
aiaiaiaiaiaiaiiaaiia [<tf.Variable 'siamese_fc/conv5/def/offset1/weights:0' shape=(3, 3, 192, 72) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv5/def/offset2/weights:0' shape=(3, 3, 192, 72) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv5/def/b1/weights:0' shape=(128, 192, 3, 3) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv5/def/b2/weights:0' shape=(128, 192, 3, 3) dtype=float32_ref>]
INFO - root - 2017-12-11 02:56:23.189463: step 0, loss = 0.70, batch loss = 0.64 (2.5 examples/sec; 3.150 sec/batch; 290h:55m:32s remains)
2017-12-11 02:56:23.761325: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.066052809 -0.068458922 -0.069783218 -0.068330795 -0.060523245 -0.04624271 -0.02701821 -0.0065465388 0.0094763208 0.013414578 0.0002439573 -0.026533147 -0.058774062 -0.08602462 -0.10159416][-0.066820934 -0.067854054 -0.065921195 -0.057451662 -0.038109254 -0.009400622 0.023840116 0.05470857 0.075141445 0.075230852 0.04862459 0.0016214501 -0.050081935 -0.089599751 -0.10969837][-0.058864761 -0.056941733 -0.0490468 -0.028672356 0.0088961981 0.060511753 0.11692907 0.16579656 0.19442157 0.18887684 0.14192636 0.063739546 -0.018908875 -0.079372019 -0.10921494][-0.038963255 -0.032241832 -0.015627868 0.021016046 0.08279819 0.16574301 0.25505587 0.3299478 0.369549 0.35467726 0.27878976 0.1583166 0.034034554 -0.055490036 -0.10034977][-0.0069945836 0.0063711093 0.033435296 0.088338 0.17681307 0.29480278 0.42132553 0.52465874 0.57233948 0.54003811 0.42581692 0.25693995 0.089357659 -0.028922018 -0.088829465][0.035801485 0.056838345 0.095373929 0.1685224 0.28229609 0.43235078 0.59162718 0.7159434 0.76051009 0.69933438 0.54121256 0.32829243 0.12757245 -0.010616509 -0.080471314][0.084635869 0.11480299 0.16580647 0.25581634 0.38927412 0.5603376 0.73571754 0.860516 0.88334793 0.7842381 0.58782792 0.34918961 0.13681166 -0.0055807652 -0.077272639][0.13488854 0.17445271 0.23753849 0.34024659 0.48265183 0.65505475 0.81862658 0.91509855 0.89773822 0.76092595 0.5447849 0.30849788 0.11018789 -0.019432802 -0.084129222][0.17953327 0.22569755 0.29653826 0.40275374 0.53799516 0.6869151 0.8090139 0.85361993 0.7895481 0.62947857 0.42146516 0.21742065 0.055758234 -0.048061237 -0.099328116][0.20972145 0.25847489 0.32966113 0.42697787 0.53807932 0.64387244 0.70875579 0.69822341 0.60082263 0.44317055 0.27010566 0.11690346 8.3084109e-05 -0.076425388 -0.11510281][0.21829392 0.26422796 0.32766372 0.4059695 0.48373458 0.54283732 0.55744249 0.51098526 0.40750027 0.27781904 0.15378025 0.050820787 -0.030698175 -0.090153188 -0.123432][0.20122582 0.23764202 0.28556404 0.33913887 0.38460866 0.4088124 0.39606398 0.34299335 0.26038209 0.17333363 0.095904835 0.027907884 -0.036186997 -0.091374516 -0.126004][0.16592781 0.18756272 0.21523631 0.24329843 0.26358882 0.26919124 0.25282559 0.21732259 0.17167309 0.12861481 0.085502267 0.034524012 -0.027241658 -0.086803764 -0.12642163][0.12888588 0.13652249 0.14546348 0.15219539 0.15566038 0.15491812 0.1488975 0.14131449 0.13470536 0.12817129 0.10645001 0.060214341 -0.0073586581 -0.075487368 -0.12191281][0.10578369 0.10551955 0.10181919 0.094278663 0.089096338 0.09052521 0.10069174 0.12083015 0.1459929 0.16415535 0.1517531 0.10093503 0.021548593 -0.058656137 -0.11366854]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-SGD-lr0.001-clip50-initfromfixqian30k-fixqian/model.ckpt-0 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-SGD-lr0.001-clip50-initfromfixqian30k-fixqian/model.ckpt-0 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-11 02:56:27.036721: step 10, loss = 0.68, batch loss = 0.62 (30.0 examples/sec; 0.267 sec/batch; 24h:36m:55s remains)
INFO - root - 2017-12-11 02:56:29.604363: step 20, loss = 0.69, batch loss = 0.64 (32.0 examples/sec; 0.250 sec/batch; 23h:05m:26s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-SGD-lr0.001-clip50-initfromfixqian30k-fixqian
INFO - root - 2017-12-11 02:56:32.185552: step 30, loss = 0.70, batch loss = 0.64 (31.4 examples/sec; 0.255 sec/batch; 23h:32m:00s remains)
INFO - root - 2017-12-11 02:56:34.787266: step 40, loss = 0.71, batch loss = 0.65 (31.3 examples/sec; 0.255 sec/batch; 23h:35m:31s remains)
INFO - root - 2017-12-11 02:56:37.383507: step 50, loss = 0.72, batch loss = 0.66 (30.2 examples/sec; 0.265 sec/batch; 24h:26m:18s remains)
INFO - root - 2017-12-11 02:56:39.949790: step 60, loss = 0.72, batch loss = 0.66 (30.6 examples/sec; 0.261 sec/batch; 24h:06m:48s remains)
INFO - root - 2017-12-11 02:56:42.515374: step 70, loss = 0.68, batch loss = 0.62 (29.7 examples/sec; 0.269 sec/batch; 24h:50m:51s remains)
INFO - root - 2017-12-11 02:56:45.132844: step 80, loss = 0.70, batch loss = 0.64 (30.9 examples/sec; 0.259 sec/batch; 23h:54m:33s remains)
INFO - root - 2017-12-11 02:56:47.752123: step 90, loss = 0.70, batch loss = 0.64 (29.2 examples/sec; 0.274 sec/batch; 25h:17m:25s remains)
INFO - root - 2017-12-11 02:56:50.348292: step 100, loss = 0.69, batch loss = 0.64 (31.2 examples/sec; 0.256 sec/batch; 23h:38m:23s remains)
2017-12-11 02:56:50.773957: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.028712519 0.089740284 0.16676886 0.24239811 0.30188808 0.33479238 0.33265248 0.29545107 0.23940392 0.18904567 0.16058975 0.15523659 0.16503148 0.1844954 0.20497508][0.034874029 0.099765435 0.18081249 0.25916144 0.31973255 0.35255113 0.35002169 0.3129932 0.25755098 0.20742913 0.17634791 0.16461621 0.16618162 0.17812751 0.19422315][0.038616292 0.10169574 0.17942052 0.25420523 0.31193125 0.34446371 0.3458066 0.31700349 0.27046126 0.22624595 0.19609149 0.17999861 0.17382374 0.17759958 0.18813607][0.038064994 0.095134504 0.16507497 0.23251024 0.28530172 0.31775647 0.32597122 0.31023896 0.27736318 0.24290976 0.21656702 0.19860804 0.18757509 0.18661137 0.19512507][0.034090061 0.08424291 0.14619975 0.20668232 0.25562081 0.28885043 0.30409145 0.30029157 0.27954552 0.25425789 0.23261365 0.21540754 0.20336705 0.20227741 0.21330965][0.028890351 0.074884281 0.13379671 0.19372992 0.24475569 0.28214833 0.30360639 0.30657008 0.29047626 0.26778182 0.24805108 0.23256443 0.22264643 0.22521731 0.24146812][0.022930047 0.06757395 0.12816083 0.19306208 0.25120792 0.29611275 0.32376277 0.32899362 0.31075481 0.28427792 0.26247457 0.24676235 0.23855212 0.24576212 0.26843929][0.017960023 0.062722966 0.12624602 0.19675478 0.26206881 0.31414905 0.34633294 0.35099268 0.32793072 0.29535142 0.2694864 0.25200245 0.24497099 0.25666904 0.28569379][0.016664231 0.063565038 0.13104331 0.20649958 0.27672502 0.33236489 0.36453864 0.36402798 0.33317593 0.29356733 0.26374471 0.24550886 0.24111897 0.25788575 0.29184926][0.01715664 0.065759696 0.13522154 0.21191344 0.28219062 0.33599189 0.36314869 0.35422328 0.31478363 0.26943988 0.23765297 0.22116943 0.22143516 0.24292058 0.27851209][0.015074014 0.062239375 0.12896866 0.20147389 0.26645941 0.31409055 0.33363527 0.31674856 0.27175802 0.22453324 0.19394104 0.18136425 0.1870594 0.21124034 0.24386509][0.00640963 0.046512317 0.10331805 0.16422741 0.21753182 0.25496909 0.26666963 0.24608155 0.2027608 0.16069333 0.1358382 0.1287585 0.13861045 0.16204844 0.18818118][-0.0067546121 0.022773163 0.065260775 0.11038423 0.1487277 0.17415011 0.17878851 0.15809703 0.12147677 0.088807896 0.071858354 0.070419684 0.082886174 0.10388892 0.12325148][-0.020701952 -0.0024046251 0.025337718 0.05457776 0.078331351 0.092225313 0.090962365 0.071673535 0.042488024 0.018656354 0.008484656 0.011545674 0.025215322 0.043340351 0.057545356][-0.033538722 -0.026027614 -0.012451705 0.0017492896 0.012279643 0.016506992 0.011445376 -0.0044032061 -0.02525541 -0.040707666 -0.045614958 -0.040501203 -0.028053 -0.013609398 -0.0034637023]]...]
INFO - root - 2017-12-11 02:56:53.356107: step 110, loss = 0.71, batch loss = 0.65 (30.8 examples/sec; 0.260 sec/batch; 23h:59m:39s remains)
INFO - root - 2017-12-11 02:56:55.901778: step 120, loss = 0.69, batch loss = 0.63 (31.7 examples/sec; 0.253 sec/batch; 23h:18m:54s remains)
INFO - root - 2017-12-11 02:56:58.507037: step 130, loss = 0.69, batch loss = 0.63 (30.4 examples/sec; 0.263 sec/batch; 24h:17m:32s remains)
INFO - root - 2017-12-11 02:57:01.129316: step 140, loss = 0.70, batch loss = 0.64 (32.4 examples/sec; 0.247 sec/batch; 22h:46m:28s remains)
INFO - root - 2017-12-11 02:57:03.732626: step 150, loss = 0.69, batch loss = 0.64 (31.3 examples/sec; 0.256 sec/batch; 23h:37m:42s remains)
INFO - root - 2017-12-11 02:57:06.332628: step 160, loss = 0.70, batch loss = 0.64 (29.3 examples/sec; 0.273 sec/batch; 25h:14m:31s remains)
INFO - root - 2017-12-11 02:57:09.021956: step 170, loss = 0.73, batch loss = 0.67 (29.1 examples/sec; 0.275 sec/batch; 25h:23m:16s remains)
INFO - root - 2017-12-11 02:57:11.693119: step 180, loss = 0.69, batch loss = 0.64 (28.1 examples/sec; 0.285 sec/batch; 26h:17m:43s remains)
INFO - root - 2017-12-11 02:57:14.367462: step 190, loss = 0.70, batch loss = 0.65 (30.0 examples/sec; 0.267 sec/batch; 24h:37m:28s remains)
INFO - root - 2017-12-11 02:57:17.045307: step 200, loss = 0.69, batch loss = 0.63 (30.0 examples/sec; 0.266 sec/batch; 24h:35m:15s remains)
2017-12-11 02:57:17.503263: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.12698631 0.14020215 0.13344193 0.11502551 0.091183059 0.07264705 0.063845381 0.065425538 0.072173446 0.0727662 0.0711134 0.076428108 0.094359405 0.1136661 0.11599997][0.2031869 0.22930837 0.22897761 0.2122301 0.18879472 0.17342398 0.16973659 0.17540897 0.18190029 0.17661588 0.16760291 0.16876599 0.18908668 0.2139194 0.2160812][0.27938 0.32150048 0.33073556 0.31766641 0.29735431 0.28643215 0.28598517 0.29037637 0.28979921 0.27285835 0.25128254 0.24431722 0.26453421 0.29537016 0.30135751][0.31739986 0.37225181 0.39275095 0.38869441 0.37844369 0.37591791 0.37766922 0.37506637 0.36054897 0.32764217 0.29092965 0.27394062 0.29194069 0.32852229 0.34210059][0.30953541 0.37191409 0.40372127 0.414002 0.42021963 0.43120098 0.4377352 0.4288708 0.4005942 0.35272372 0.30205145 0.27435109 0.28623512 0.32275581 0.34048328][0.28349388 0.34749237 0.3880527 0.41410032 0.43995374 0.46909958 0.48556525 0.47545967 0.43795857 0.37853512 0.31656203 0.27847204 0.28166109 0.31440088 0.33524457][0.25719675 0.32088405 0.36876997 0.4092932 0.45229873 0.49665493 0.52076173 0.51035058 0.46846756 0.4041737 0.33803859 0.29463172 0.2907629 0.31870735 0.34081909][0.24395244 0.30837759 0.36305013 0.41382718 0.46519566 0.513603 0.53706473 0.52431393 0.48160824 0.41792169 0.35350436 0.30972308 0.30131474 0.32515126 0.34763157][0.23336454 0.30020142 0.36102223 0.41711229 0.46677604 0.508366 0.52584314 0.51364434 0.47747454 0.42148525 0.36511332 0.32590902 0.31683734 0.33860254 0.3613871][0.22861195 0.30184016 0.36814681 0.42320728 0.46134046 0.487382 0.49627066 0.48941222 0.46780708 0.42725053 0.38510802 0.35482046 0.34780836 0.36683598 0.38667914][0.23469171 0.31273916 0.37912807 0.4254249 0.44613528 0.45322952 0.45203903 0.44987184 0.44259891 0.41858315 0.39157596 0.3702791 0.36548811 0.38134229 0.3978475][0.22367716 0.30139807 0.36223257 0.3958436 0.39801291 0.38543972 0.37127972 0.36814031 0.36980453 0.3597596 0.34628972 0.33283809 0.32893848 0.340487 0.35256207][0.16997425 0.23928142 0.29063791 0.31325233 0.30343276 0.27946249 0.25765106 0.25216469 0.25671226 0.25371346 0.248212 0.23980302 0.23543131 0.24225849 0.24984312][0.08642856 0.13966885 0.17843451 0.19193605 0.17647292 0.14953621 0.12698676 0.12121788 0.12638898 0.12703168 0.12690011 0.12371171 0.12096714 0.12537381 0.13082311][0.02209425 0.056790743 0.081175409 0.085467637 0.06713172 0.040989831 0.020355046 0.014230072 0.017410973 0.018674223 0.020821953 0.021401305 0.020818245 0.023623455 0.027851054]]...]
INFO - root - 2017-12-11 02:57:20.121536: step 210, loss = 0.69, batch loss = 0.63 (30.4 examples/sec; 0.263 sec/batch; 24h:19m:07s remains)
INFO - root - 2017-12-11 02:57:22.756274: step 220, loss = 0.71, batch loss = 0.65 (32.1 examples/sec; 0.249 sec/batch; 23h:00m:34s remains)
INFO - root - 2017-12-11 02:57:25.388364: step 230, loss = 0.69, batch loss = 0.64 (31.9 examples/sec; 0.251 sec/batch; 23h:07m:51s remains)
INFO - root - 2017-12-11 02:57:28.021475: step 240, loss = 0.70, batch loss = 0.64 (30.4 examples/sec; 0.263 sec/batch; 24h:17m:45s remains)
INFO - root - 2017-12-11 02:57:30.674158: step 250, loss = 0.71, batch loss = 0.65 (29.8 examples/sec; 0.268 sec/batch; 24h:46m:17s remains)
INFO - root - 2017-12-11 02:57:33.379661: step 260, loss = 0.68, batch loss = 0.62 (30.0 examples/sec; 0.267 sec/batch; 24h:39m:00s remains)
INFO - root - 2017-12-11 02:57:35.977745: step 270, loss = 0.73, batch loss = 0.67 (30.7 examples/sec; 0.260 sec/batch; 24h:01m:55s remains)
INFO - root - 2017-12-11 02:57:38.587296: step 280, loss = 0.69, batch loss = 0.63 (30.7 examples/sec; 0.260 sec/batch; 24h:01m:46s remains)
INFO - root - 2017-12-11 02:57:41.189688: step 290, loss = 0.70, batch loss = 0.64 (31.5 examples/sec; 0.254 sec/batch; 23h:27m:15s remains)
INFO - root - 2017-12-11 02:57:43.827948: step 300, loss = 0.70, batch loss = 0.64 (29.5 examples/sec; 0.271 sec/batch; 25h:00m:27s remains)
2017-12-11 02:57:44.299218: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.15861659 0.13637564 0.12837915 0.13092893 0.13836536 0.15568556 0.187487 0.22279072 0.23300467 0.21104188 0.17651032 0.14714436 0.13208371 0.11982509 0.10419999][0.14483638 0.13169897 0.12192148 0.11276387 0.10388613 0.10458574 0.12309281 0.15128238 0.16393605 0.15183777 0.12933987 0.11086106 0.10426395 0.10055201 0.093161263][0.12962808 0.13052593 0.12388334 0.10677302 0.082799561 0.064746231 0.065854482 0.084020771 0.099398836 0.09995009 0.092662282 0.086804539 0.087190509 0.0871604 0.0837825][0.13196261 0.15089357 0.1536724 0.13614444 0.10308012 0.068021342 0.051037103 0.058738783 0.077226251 0.09051732 0.097972587 0.10364579 0.10596641 0.099287972 0.088287883][0.16229151 0.20251447 0.22046646 0.21009994 0.17545912 0.12820594 0.095468983 0.093077168 0.11321873 0.1356312 0.15310718 0.16597427 0.16374123 0.13972536 0.10840803][0.21241075 0.27511522 0.31109962 0.31239441 0.28183222 0.228148 0.18380128 0.17202277 0.19106388 0.21750654 0.23890494 0.25326803 0.24117476 0.19429116 0.13604493][0.26321656 0.34554586 0.39807594 0.41113681 0.38651705 0.33092058 0.27971655 0.25978929 0.27508393 0.30169278 0.3232291 0.33532152 0.31192011 0.24422798 0.1622891][0.2939792 0.38772139 0.44975668 0.4701041 0.4497005 0.39525229 0.34210742 0.31647339 0.32600838 0.349255 0.36804169 0.37528628 0.34291694 0.26401305 0.17141032][0.28830114 0.38022146 0.441839 0.46424928 0.44774264 0.39920577 0.35040802 0.32287812 0.32521412 0.34123629 0.35434511 0.35591775 0.32025936 0.24350004 0.15673149][0.24796742 0.32642591 0.38025934 0.40215382 0.39165 0.35522965 0.31801483 0.29395241 0.28934968 0.29439881 0.29757318 0.2912958 0.25583223 0.19065472 0.12175813][0.18705347 0.244292 0.28544691 0.30571789 0.3043597 0.28594145 0.26704413 0.25259537 0.24412754 0.23717906 0.22697103 0.21001776 0.17450047 0.12310837 0.075708844][0.12588124 0.16038801 0.18755107 0.20512767 0.21281081 0.21272404 0.21324791 0.21087435 0.20208463 0.185734 0.16300294 0.13560152 0.099458538 0.05981357 0.030812932][0.0786288 0.095324375 0.11096087 0.12468725 0.13804297 0.15130021 0.16675612 0.17478652 0.16831174 0.14760251 0.11678102 0.0817698 0.044914506 0.013459782 -0.0031976129][0.045495797 0.051710133 0.06005102 0.06881801 0.081846647 0.099524647 0.12156324 0.13601063 0.13438444 0.11646674 0.085472465 0.049047548 0.013818818 -0.011509228 -0.021223694][0.023532456 0.026109163 0.031365667 0.035087634 0.042271916 0.055192903 0.074403018 0.089465186 0.093053885 0.0837515 0.061186627 0.031250976 0.0020560913 -0.017227678 -0.023334607]]...]
INFO - root - 2017-12-11 02:57:46.903246: step 310, loss = 0.70, batch loss = 0.64 (30.4 examples/sec; 0.263 sec/batch; 24h:18m:25s remains)
INFO - root - 2017-12-11 02:57:49.536641: step 320, loss = 0.69, batch loss = 0.63 (29.8 examples/sec; 0.269 sec/batch; 24h:48m:35s remains)
INFO - root - 2017-12-11 02:57:52.219952: step 330, loss = 0.70, batch loss = 0.64 (29.4 examples/sec; 0.272 sec/batch; 25h:07m:03s remains)
INFO - root - 2017-12-11 02:57:54.871709: step 340, loss = 0.69, batch loss = 0.63 (29.8 examples/sec; 0.268 sec/batch; 24h:45m:05s remains)
INFO - root - 2017-12-11 02:57:57.565896: step 350, loss = 0.70, batch loss = 0.64 (29.8 examples/sec; 0.269 sec/batch; 24h:48m:14s remains)
INFO - root - 2017-12-11 02:58:00.223121: step 360, loss = 0.72, batch loss = 0.66 (29.8 examples/sec; 0.269 sec/batch; 24h:47m:05s remains)
INFO - root - 2017-12-11 02:58:02.905188: step 370, loss = 0.70, batch loss = 0.65 (29.5 examples/sec; 0.271 sec/batch; 25h:02m:25s remains)
INFO - root - 2017-12-11 02:58:05.554930: step 380, loss = 0.71, batch loss = 0.65 (30.7 examples/sec; 0.261 sec/batch; 24h:03m:54s remains)
INFO - root - 2017-12-11 02:58:08.176850: step 390, loss = 0.69, batch loss = 0.63 (31.2 examples/sec; 0.257 sec/batch; 23h:40m:35s remains)
INFO - root - 2017-12-11 02:58:10.818106: step 400, loss = 0.68, batch loss = 0.62 (30.7 examples/sec; 0.261 sec/batch; 24h:03m:00s remains)
2017-12-11 02:58:11.301340: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.11065952 0.12373791 0.13810423 0.14993745 0.15074451 0.13537207 0.10602921 0.070378728 0.043166611 0.031442381 0.033713553 0.046270479 0.065614492 0.086816944 0.099547952][0.084718138 0.0987505 0.11770511 0.13453974 0.13888764 0.12341997 0.08889351 0.043625865 0.004368932 -0.016582521 -0.01542262 0.0072791828 0.047815673 0.097477742 0.14046143][0.061875857 0.083072364 0.11362717 0.14276825 0.15727942 0.14725675 0.11141209 0.057815433 0.0039274408 -0.033613022 -0.044033848 -0.022017552 0.033127189 0.11215457 0.19460559][0.049920991 0.087487727 0.14051591 0.19452888 0.23196681 0.23831138 0.20755976 0.14719926 0.074760072 0.011032821 -0.025514651 -0.021634378 0.03169186 0.12859951 0.24656956][0.041943971 0.10177916 0.18545391 0.27523279 0.34897169 0.38590628 0.37189326 0.31099465 0.21955387 0.12306193 0.048821971 0.018694207 0.05118281 0.14523461 0.27897286][0.033386718 0.11294242 0.22421683 0.3492049 0.46436059 0.54272985 0.55907619 0.50882423 0.40458217 0.27509457 0.15749696 0.083206 0.079433247 0.14936781 0.27328143][0.023210656 0.11039727 0.23394328 0.37877756 0.52516752 0.64328521 0.69731116 0.67062056 0.56839085 0.41854921 0.26433057 0.1454598 0.097221732 0.12755492 0.21978404][0.0070672915 0.0846449 0.19797419 0.33744338 0.49171811 0.6334365 0.72115391 0.72787535 0.64887971 0.50489515 0.33577639 0.184865 0.094493389 0.0792585 0.12797][-0.012351594 0.039811183 0.12192421 0.23111929 0.36529917 0.50328106 0.60531968 0.64156866 0.59978575 0.48905051 0.3357482 0.17929719 0.065366864 0.01518583 0.024155037][-0.029563989 -0.0096416781 0.030541612 0.094340429 0.18711087 0.29523847 0.38648382 0.4351337 0.42698762 0.36068147 0.2463306 0.11422733 0.0068940585 -0.052563086 -0.0646741][-0.039492749 -0.045157757 -0.041740604 -0.02252746 0.022135831 0.08634419 0.14827329 0.18992729 0.1995275 0.17035551 0.10078035 0.010849737 -0.065862708 -0.11059202 -0.12334805][-0.043928273 -0.061911803 -0.079497941 -0.090414517 -0.084740251 -0.06255459 -0.03539215 -0.012748022 -0.0018597069 -0.00962643 -0.042560395 -0.089340776 -0.12817712 -0.1479567 -0.14993152][-0.047381777 -0.066737428 -0.089536443 -0.11219685 -0.12728907 -0.13230839 -0.13140933 -0.12771037 -0.12422974 -0.12531421 -0.13668969 -0.15250836 -0.16128215 -0.15919797 -0.14909595][-0.050923757 -0.065256685 -0.083173037 -0.10418424 -0.12371345 -0.13905792 -0.15054861 -0.15820813 -0.16259378 -0.16541761 -0.16933008 -0.17073882 -0.16437696 -0.15067206 -0.1333999][-0.053963084 -0.061196346 -0.0703163 -0.083463967 -0.098296836 -0.11292728 -0.12627937 -0.1372731 -0.14552224 -0.15124519 -0.1547901 -0.15367115 -0.1454806 -0.13156626 -0.11501249]]...]
INFO - root - 2017-12-11 02:58:13.987797: step 410, loss = 0.70, batch loss = 0.64 (30.2 examples/sec; 0.265 sec/batch; 24h:27m:47s remains)
INFO - root - 2017-12-11 02:58:16.639085: step 420, loss = 0.69, batch loss = 0.63 (30.9 examples/sec; 0.259 sec/batch; 23h:52m:52s remains)
INFO - root - 2017-12-11 02:58:19.329183: step 430, loss = 0.71, batch loss = 0.65 (27.9 examples/sec; 0.286 sec/batch; 26h:24m:35s remains)
INFO - root - 2017-12-11 02:58:21.977772: step 440, loss = 0.69, batch loss = 0.63 (30.0 examples/sec; 0.267 sec/batch; 24h:35m:03s remains)
INFO - root - 2017-12-11 02:58:24.629181: step 450, loss = 0.70, batch loss = 0.64 (31.0 examples/sec; 0.258 sec/batch; 23h:48m:10s remains)
INFO - root - 2017-12-11 02:58:27.255484: step 460, loss = 0.70, batch loss = 0.64 (28.9 examples/sec; 0.277 sec/batch; 25h:31m:33s remains)
INFO - root - 2017-12-11 02:58:29.919628: step 470, loss = 0.68, batch loss = 0.62 (29.9 examples/sec; 0.267 sec/batch; 24h:38m:45s remains)
INFO - root - 2017-12-11 02:58:32.562494: step 480, loss = 0.70, batch loss = 0.64 (30.5 examples/sec; 0.262 sec/batch; 24h:10m:03s remains)
INFO - root - 2017-12-11 02:58:35.257461: step 490, loss = 0.71, batch loss = 0.65 (30.1 examples/sec; 0.266 sec/batch; 24h:30m:19s remains)
INFO - root - 2017-12-11 02:58:37.899780: step 500, loss = 0.72, batch loss = 0.66 (30.1 examples/sec; 0.265 sec/batch; 24h:28m:19s remains)
2017-12-11 02:58:38.353483: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.069800943 0.071289048 0.061202448 0.048259687 0.041831583 0.050250676 0.073938817 0.11201893 0.15735996 0.20456147 0.2456888 0.27652517 0.29708114 0.305886 0.30562392][0.12618658 0.13345914 0.12378573 0.10854168 0.099312782 0.10707527 0.13373597 0.17614152 0.22418576 0.27054641 0.3063772 0.32672071 0.33321583 0.32819918 0.31644851][0.1822978 0.19599752 0.18882309 0.17389889 0.1629817 0.1684874 0.19460107 0.23636879 0.28156438 0.32224697 0.34950593 0.35661575 0.34601784 0.32372275 0.29821354][0.22979526 0.24886072 0.24481246 0.23103268 0.21896547 0.22135843 0.24453449 0.28311044 0.3248499 0.3615365 0.38354939 0.38212177 0.35949877 0.32392237 0.28641129][0.2766619 0.29859161 0.29489589 0.27957103 0.26347992 0.25900275 0.27460012 0.30698955 0.34591144 0.38208437 0.40360048 0.39934224 0.3711746 0.32794827 0.28138039][0.32016456 0.34122151 0.33446681 0.31504852 0.29295179 0.27926216 0.28390852 0.30773965 0.34388515 0.38119009 0.40487209 0.40242061 0.3750836 0.33012542 0.27847049][0.34312245 0.36069161 0.35079336 0.32962841 0.30513233 0.28548238 0.28009051 0.29519495 0.32799387 0.36652657 0.39366591 0.39596379 0.37329134 0.32995552 0.27531448][0.33842808 0.35051116 0.33728451 0.31717035 0.29554042 0.27568787 0.26436391 0.2727547 0.30200705 0.34043053 0.37006849 0.3778998 0.36221108 0.32320285 0.26780009][0.32412726 0.33065823 0.31387323 0.29490632 0.277214 0.25921836 0.24388745 0.24677628 0.27166474 0.30649036 0.33380163 0.34326437 0.33273163 0.29867727 0.24559766][0.30488348 0.30745998 0.28819764 0.26954919 0.25378415 0.23620848 0.21797796 0.21687908 0.23652372 0.26414534 0.28374067 0.28861019 0.27804983 0.24760035 0.2010843][0.27594134 0.27582228 0.25667682 0.23962079 0.2260752 0.21030891 0.19339131 0.19182588 0.20679234 0.225188 0.23406661 0.23091854 0.21713611 0.19022484 0.15486997][0.23554106 0.23494475 0.21945952 0.20720173 0.19834779 0.18754667 0.17640321 0.17767408 0.18923193 0.19898674 0.19823802 0.18773971 0.17026728 0.14747782 0.12503399][0.19874559 0.19950692 0.18879917 0.18190524 0.17841056 0.17426927 0.17116939 0.17652056 0.18535908 0.18762448 0.17931171 0.16369809 0.14440277 0.12669052 0.11691199][0.18061648 0.18281184 0.17506255 0.16988306 0.16792125 0.16742416 0.1702456 0.17831787 0.18540086 0.18415681 0.17408362 0.15902533 0.14241946 0.13160951 0.13299562][0.16589145 0.16930494 0.16328782 0.15711188 0.15298052 0.15201406 0.15674509 0.16522019 0.17172243 0.1717056 0.16626707 0.15774734 0.1483907 0.14579266 0.15544657]]...]
INFO - root - 2017-12-11 02:58:41.016133: step 510, loss = 0.71, batch loss = 0.65 (29.0 examples/sec; 0.276 sec/batch; 25h:28m:37s remains)
INFO - root - 2017-12-11 02:58:43.673526: step 520, loss = 0.70, batch loss = 0.64 (30.0 examples/sec; 0.266 sec/batch; 24h:33m:46s remains)
INFO - root - 2017-12-11 02:58:46.327098: step 530, loss = 0.71, batch loss = 0.65 (30.6 examples/sec; 0.262 sec/batch; 24h:08m:49s remains)
INFO - root - 2017-12-11 02:58:49.002052: step 540, loss = 0.69, batch loss = 0.63 (31.2 examples/sec; 0.256 sec/batch; 23h:37m:10s remains)
INFO - root - 2017-12-11 02:58:51.642562: step 550, loss = 0.69, batch loss = 0.64 (29.4 examples/sec; 0.272 sec/batch; 25h:07m:20s remains)
INFO - root - 2017-12-11 02:58:54.298827: step 560, loss = 0.70, batch loss = 0.64 (30.5 examples/sec; 0.262 sec/batch; 24h:10m:36s remains)
INFO - root - 2017-12-11 02:58:56.963657: step 570, loss = 0.70, batch loss = 0.64 (29.1 examples/sec; 0.275 sec/batch; 25h:20m:46s remains)
INFO - root - 2017-12-11 02:58:59.609076: step 580, loss = 0.71, batch loss = 0.65 (30.2 examples/sec; 0.265 sec/batch; 24h:23m:39s remains)
INFO - root - 2017-12-11 02:59:02.246729: step 590, loss = 0.69, batch loss = 0.64 (31.1 examples/sec; 0.257 sec/batch; 23h:43m:51s remains)
INFO - root - 2017-12-11 02:59:04.897358: step 600, loss = 0.69, batch loss = 0.63 (30.2 examples/sec; 0.265 sec/batch; 24h:23m:22s remains)
2017-12-11 02:59:05.365753: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.099907406 -0.096985392 -0.089041233 -0.084588431 -0.086557195 -0.092355 -0.10117535 -0.11268516 -0.12329883 -0.12953922 -0.13115403 -0.13127619 -0.13015096 -0.12774119 -0.12595139][-0.072170436 -0.056059912 -0.034827922 -0.02178032 -0.021024542 -0.027258478 -0.040118825 -0.060148057 -0.08155027 -0.096999809 -0.10638941 -0.11504225 -0.12175276 -0.12429308 -0.12550162][-0.02361339 0.01759189 0.067314528 0.10505162 0.1231247 0.12880385 0.11903786 0.089438558 0.049194034 0.012476549 -0.018382879 -0.050165467 -0.07834781 -0.096105583 -0.10667521][0.034330405 0.11099648 0.20501213 0.28648612 0.34086764 0.37471712 0.37880912 0.34188884 0.27620676 0.20524763 0.1374339 0.066366628 0.00083930208 -0.045379397 -0.074259013][0.086535551 0.20131432 0.34622115 0.48197922 0.58492559 0.65890485 0.68726951 0.64970124 0.55973864 0.45020685 0.33781815 0.21816066 0.10394677 0.018288666 -0.036372043][0.12334778 0.27150798 0.46266866 0.65200812 0.80589443 0.92369622 0.98262763 0.95247543 0.84566569 0.70050138 0.54423416 0.37514687 0.2086335 0.078790344 -0.0044940952][0.139225 0.3091498 0.53171331 0.76035172 0.95430195 1.1077913 1.1939248 1.1741464 1.0600306 0.89141095 0.70385426 0.49803957 0.28989467 0.12365552 0.016667267][0.12596656 0.29892194 0.52813512 0.76988137 0.98063141 1.150438 1.2526149 1.245178 1.1382456 0.96809047 0.77378029 0.55615109 0.32914618 0.14382066 0.023675051][0.081721604 0.23420037 0.43894503 0.65977395 0.85599858 1.0154121 1.116096 1.1199892 1.0349894 0.88778043 0.71445262 0.5150966 0.30034375 0.12206911 0.0065897219][0.018942079 0.13201478 0.28648332 0.45666844 0.610236 0.7361564 0.82017297 0.83275944 0.7790727 0.67289275 0.54148549 0.38368827 0.20762138 0.060782932 -0.032629915][-0.042146929 0.025963785 0.12154832 0.22882965 0.32683471 0.40829042 0.46633402 0.48161954 0.45724046 0.39548013 0.31279886 0.20694883 0.084061973 -0.01730323 -0.078487277][-0.091638424 -0.063386664 -0.019742819 0.030154262 0.076143317 0.11532859 0.14589669 0.15752421 0.15225606 0.1263525 0.086868 0.030813776 -0.037812524 -0.091894053 -0.11971153][-0.12712932 -0.12790525 -0.12086453 -0.11116567 -0.10156266 -0.0921165 -0.082329154 -0.076031871 -0.0720826 -0.075818665 -0.085991912 -0.10519066 -0.13122314 -0.14831248 -0.15061212][-0.14316633 -0.16000105 -0.17268589 -0.18471344 -0.19498231 -0.20179899 -0.20406145 -0.20214942 -0.19593629 -0.18993457 -0.18538044 -0.18382601 -0.18413404 -0.17917512 -0.16698976][-0.1382706 -0.15783405 -0.17442963 -0.19189167 -0.20815684 -0.22068796 -0.22829401 -0.2301771 -0.22662134 -0.22006537 -0.21199772 -0.20327652 -0.19366063 -0.18083732 -0.1646924]]...]
INFO - root - 2017-12-11 02:59:08.061681: step 610, loss = 0.71, batch loss = 0.65 (29.1 examples/sec; 0.275 sec/batch; 25h:23m:12s remains)
INFO - root - 2017-12-11 02:59:10.787717: step 620, loss = 0.69, batch loss = 0.64 (29.7 examples/sec; 0.269 sec/batch; 24h:49m:24s remains)
INFO - root - 2017-12-11 02:59:13.432029: step 630, loss = 0.68, batch loss = 0.62 (30.1 examples/sec; 0.266 sec/batch; 24h:32m:17s remains)
INFO - root - 2017-12-11 02:59:16.124968: step 640, loss = 0.70, batch loss = 0.64 (30.0 examples/sec; 0.266 sec/batch; 24h:33m:47s remains)
INFO - root - 2017-12-11 02:59:18.764481: step 650, loss = 0.70, batch loss = 0.64 (30.7 examples/sec; 0.260 sec/batch; 23h:59m:55s remains)
INFO - root - 2017-12-11 02:59:21.466936: step 660, loss = 0.70, batch loss = 0.65 (30.3 examples/sec; 0.264 sec/batch; 24h:18m:13s remains)
INFO - root - 2017-12-11 02:59:24.096374: step 670, loss = 0.70, batch loss = 0.64 (30.9 examples/sec; 0.259 sec/batch; 23h:54m:02s remains)
INFO - root - 2017-12-11 02:59:26.736921: step 680, loss = 0.69, batch loss = 0.63 (29.7 examples/sec; 0.269 sec/batch; 24h:48m:03s remains)
INFO - root - 2017-12-11 02:59:29.403525: step 690, loss = 0.68, batch loss = 0.63 (30.3 examples/sec; 0.264 sec/batch; 24h:18m:00s remains)
INFO - root - 2017-12-11 02:59:32.023893: step 700, loss = 0.70, batch loss = 0.64 (31.4 examples/sec; 0.255 sec/batch; 23h:30m:28s remains)
2017-12-11 02:59:32.486895: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.021754419 -0.015422587 -0.011319449 -0.013466941 -0.02294695 -0.036776673 -0.049445879 -0.05435577 -0.048116814 -0.033937495 -0.019299565 -0.0098793814 -0.0065002511 -0.0056993305 -0.0032857859][-0.016927434 -0.00072494417 0.014616062 0.021062585 0.013869916 -0.0059272558 -0.031446725 -0.052233096 -0.059996504 -0.053610232 -0.039642081 -0.025387596 -0.01520105 -0.0090346681 -0.0042392276][-0.00044124413 0.034582544 0.072966635 0.099169329 0.10072426 0.074643232 0.030273527 -0.016057877 -0.049118403 -0.061811518 -0.057780646 -0.04467174 -0.029891443 -0.017008454 -0.00732181][0.025918955 0.090539813 0.16586003 0.22436593 0.24199294 0.20999314 0.14133072 0.061084896 -0.0066988347 -0.047929987 -0.063104138 -0.059562195 -0.046126869 -0.029460283 -0.015171329][0.0535328 0.15150937 0.27028912 0.3700453 0.4149721 0.38848934 0.30357632 0.19168603 0.085310854 0.007198723 -0.037683234 -0.055087946 -0.053071674 -0.039481476 -0.023773119][0.0707641 0.19377923 0.34908292 0.49032837 0.57499129 0.57635581 0.49927676 0.3730031 0.23440342 0.1161147 0.032168258 -0.018167717 -0.03967534 -0.0389822 -0.027918093][0.069622815 0.19933 0.3714292 0.542749 0.67272794 0.72427154 0.68493563 0.57106483 0.41801527 0.26666322 0.14197461 0.052374393 -0.00092675787 -0.021467835 -0.021872468][0.051422976 0.16763096 0.33180696 0.51193821 0.67751527 0.78502244 0.80255312 0.7273916 0.58599371 0.42072603 0.26510936 0.13925761 0.053440157 0.0087362519 -0.0068732304][0.028818024 0.11778783 0.25304139 0.41670325 0.59281605 0.74004555 0.81453425 0.79536223 0.69298321 0.54031777 0.37229928 0.22088328 0.10827856 0.042174883 0.012195969][0.010874069 0.067614645 0.16225018 0.28969049 0.44773147 0.60510933 0.718354 0.75552011 0.70855665 0.59193707 0.43347353 0.27339655 0.14586039 0.066393219 0.027036302][0.0012714539 0.02967212 0.083969072 0.16791998 0.2887499 0.42781284 0.550928 0.62487459 0.62863177 0.55775076 0.42754939 0.27928504 0.15386607 0.072643816 0.030954836][-0.00083048252 0.0076796114 0.030152308 0.074752346 0.15286735 0.25603414 0.36196259 0.44399732 0.47800869 0.44796613 0.35662717 0.23817217 0.13180138 0.06052674 0.023408663][0.0023371202 0.00057215884 0.0033787347 0.019454328 0.060170837 0.12356592 0.1978596 0.26586211 0.30787632 0.30553707 0.25322816 0.17301545 0.095032759 0.039976526 0.010839753][0.0088552991 0.0050319331 -0.00037717822 -0.000714799 0.013040955 0.042436082 0.08375 0.12887855 0.16520734 0.17770572 0.15704148 0.11282932 0.063249253 0.024351269 0.0023789587][0.014902321 0.012895112 0.0060610776 -0.0018924456 -0.004827139 0.00050040928 0.015947483 0.040622134 0.06831561 0.088353291 0.090271443 0.0741489 0.047423709 0.021047473 0.0031187688]]...]
INFO - root - 2017-12-11 02:59:35.101863: step 710, loss = 0.71, batch loss = 0.65 (30.5 examples/sec; 0.263 sec/batch; 24h:12m:22s remains)
INFO - root - 2017-12-11 02:59:37.784317: step 720, loss = 0.69, batch loss = 0.64 (29.2 examples/sec; 0.274 sec/batch; 25h:12m:26s remains)
INFO - root - 2017-12-11 02:59:40.414007: step 730, loss = 0.70, batch loss = 0.64 (30.7 examples/sec; 0.260 sec/batch; 23h:58m:36s remains)
INFO - root - 2017-12-11 02:59:43.068517: step 740, loss = 0.70, batch loss = 0.64 (29.2 examples/sec; 0.274 sec/batch; 25h:12m:51s remains)
INFO - root - 2017-12-11 02:59:45.715367: step 750, loss = 0.70, batch loss = 0.64 (31.4 examples/sec; 0.255 sec/batch; 23h:27m:27s remains)
INFO - root - 2017-12-11 02:59:48.377252: step 760, loss = 0.69, batch loss = 0.63 (30.5 examples/sec; 0.262 sec/batch; 24h:08m:17s remains)
INFO - root - 2017-12-11 02:59:51.000817: step 770, loss = 0.69, batch loss = 0.63 (31.0 examples/sec; 0.258 sec/batch; 23h:46m:03s remains)
INFO - root - 2017-12-11 02:59:53.675797: step 780, loss = 0.70, batch loss = 0.65 (31.2 examples/sec; 0.257 sec/batch; 23h:38m:19s remains)
INFO - root - 2017-12-11 02:59:56.358478: step 790, loss = 0.71, batch loss = 0.65 (29.1 examples/sec; 0.275 sec/batch; 25h:20m:06s remains)
INFO - root - 2017-12-11 02:59:59.004285: step 800, loss = 0.70, batch loss = 0.64 (30.1 examples/sec; 0.266 sec/batch; 24h:28m:13s remains)
2017-12-11 02:59:59.430932: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.16620879 0.15529221 0.15276158 0.16575307 0.1927381 0.23276912 0.28351465 0.33321139 0.37237576 0.39383644 0.40249518 0.41940981 0.43822879 0.44576558 0.44029427][0.23229323 0.21603943 0.19826557 0.19280897 0.20565465 0.23797536 0.28791645 0.34392545 0.39210966 0.42369127 0.44019228 0.45880765 0.47623092 0.4833909 0.47702798][0.3301208 0.30905923 0.2762585 0.25107044 0.24542806 0.26385573 0.30650505 0.36285996 0.41512805 0.45290968 0.47231907 0.48484167 0.49366292 0.49650323 0.4896127][0.4398075 0.41211802 0.36502394 0.32074139 0.29555035 0.29946932 0.33502904 0.38980663 0.44075271 0.47618055 0.48968086 0.48855135 0.48465669 0.4832547 0.48079532][0.51770777 0.48162034 0.425338 0.37361419 0.34344649 0.34886774 0.39073536 0.44935733 0.49482718 0.51661646 0.51285619 0.49041471 0.47028244 0.46411368 0.46966925][0.54580492 0.50174421 0.44634765 0.40489992 0.39133283 0.41875225 0.47991383 0.54512119 0.57851017 0.57495463 0.54307222 0.49305069 0.45425719 0.44266367 0.45746785][0.52784878 0.48107907 0.43572736 0.41434121 0.42619705 0.47974351 0.55784929 0.62414634 0.64054924 0.60927618 0.54885072 0.4739399 0.41919449 0.40257457 0.42451575][0.46225053 0.4247345 0.39642361 0.39446607 0.42426464 0.48990533 0.56970811 0.62697256 0.6261301 0.57497519 0.49753895 0.41078493 0.34883806 0.32975134 0.35561931][0.36870155 0.34765479 0.3359991 0.34396747 0.37490255 0.43170044 0.49533847 0.53458422 0.51991224 0.46149704 0.38315475 0.30037713 0.24209511 0.22558749 0.25479242][0.28855354 0.28213426 0.27693796 0.27860054 0.29172084 0.32143104 0.35613236 0.37309572 0.34986988 0.29531723 0.22754031 0.15789448 0.10886217 0.09745793 0.13029757][0.26314375 0.26276392 0.25013909 0.23042221 0.2111297 0.20117635 0.19869298 0.19118926 0.16293672 0.11817768 0.065756693 0.012244648 -0.024856962 -0.028757203 0.0092595676][0.30663958 0.30426508 0.27509177 0.22794314 0.1720185 0.11977986 0.080121629 0.050501652 0.020413551 -0.013049931 -0.050462086 -0.089088723 -0.11506959 -0.11302942 -0.0736201][0.39326575 0.38635525 0.34002253 0.26982328 0.18504399 0.10062618 0.033813693 -0.010134728 -0.038479716 -0.061681561 -0.086877152 -0.11364386 -0.13146666 -0.12835631 -0.095824912][0.47760043 0.46616659 0.40947202 0.32835162 0.23188023 0.13534677 0.058626205 0.011135041 -0.012643479 -0.028221315 -0.045602251 -0.065209083 -0.079513617 -0.081038781 -0.062698305][0.49931771 0.48632163 0.43136227 0.35703346 0.27112415 0.18619531 0.1190234 0.079321794 0.0627419 0.052599534 0.039585564 0.023341687 0.0085763549 -0.0019123231 -0.0014737397]]...]
INFO - root - 2017-12-11 03:00:02.053818: step 810, loss = 0.69, batch loss = 0.63 (30.8 examples/sec; 0.260 sec/batch; 23h:55m:31s remains)
INFO - root - 2017-12-11 03:00:04.733177: step 820, loss = 0.69, batch loss = 0.63 (30.9 examples/sec; 0.259 sec/batch; 23h:51m:22s remains)
INFO - root - 2017-12-11 03:00:07.387245: step 830, loss = 0.71, batch loss = 0.65 (31.5 examples/sec; 0.254 sec/batch; 23h:24m:25s remains)
INFO - root - 2017-12-11 03:00:10.048039: step 840, loss = 0.69, batch loss = 0.63 (30.4 examples/sec; 0.263 sec/batch; 24h:15m:45s remains)
INFO - root - 2017-12-11 03:00:12.668812: step 850, loss = 0.70, batch loss = 0.64 (30.3 examples/sec; 0.264 sec/batch; 24h:20m:12s remains)
INFO - root - 2017-12-11 03:00:15.331020: step 860, loss = 0.72, batch loss = 0.66 (30.6 examples/sec; 0.261 sec/batch; 24h:03m:00s remains)
INFO - root - 2017-12-11 03:00:17.984761: step 870, loss = 0.69, batch loss = 0.63 (27.7 examples/sec; 0.289 sec/batch; 26h:38m:31s remains)
INFO - root - 2017-12-11 03:00:20.703771: step 880, loss = 0.69, batch loss = 0.64 (30.6 examples/sec; 0.261 sec/batch; 24h:03m:00s remains)
INFO - root - 2017-12-11 03:00:23.364921: step 890, loss = 0.70, batch loss = 0.64 (29.8 examples/sec; 0.268 sec/batch; 24h:43m:10s remains)
INFO - root - 2017-12-11 03:00:26.020342: step 900, loss = 0.69, batch loss = 0.63 (30.8 examples/sec; 0.259 sec/batch; 23h:54m:08s remains)
2017-12-11 03:00:26.553220: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.038984906 -0.035526119 -0.015638158 0.012797925 0.04708093 0.077581771 0.099557474 0.10810595 0.094245352 0.059137996 0.010905324 -0.040028084 -0.088373624 -0.12723115 -0.14747004][-0.03761911 -0.039581005 -0.026207412 -0.0044276775 0.021721693 0.0415824 0.048821174 0.041645296 0.016726974 -0.021871444 -0.066183977 -0.10694621 -0.14023435 -0.16163003 -0.16508646][-0.0068031619 -0.00076347165 0.019596081 0.04664927 0.074322104 0.091276377 0.090857588 0.074847929 0.042703912 -0.00081268029 -0.047949366 -0.089226529 -0.12057381 -0.13935372 -0.14116317][0.042657912 0.069283895 0.11008845 0.15416904 0.19415888 0.21948978 0.22282356 0.2071425 0.17109871 0.12047815 0.0623976 0.006684666 -0.040126614 -0.073739521 -0.089264348][0.10126872 0.15597124 0.22402143 0.28983122 0.34689373 0.38781703 0.40362629 0.39513949 0.35761768 0.29923439 0.22603819 0.14731398 0.073516294 0.014265496 -0.023193613][0.15716101 0.24025078 0.33450329 0.41990682 0.49271938 0.55058616 0.58269268 0.58582157 0.54994011 0.48569125 0.39819384 0.29546797 0.19171746 0.10384672 0.043467302][0.19796437 0.30362037 0.41873941 0.52061939 0.60734648 0.68039405 0.72715378 0.74013346 0.70408779 0.63253647 0.53224993 0.41052577 0.28405288 0.17487013 0.098484591][0.22087115 0.33826947 0.46531913 0.57763505 0.67172658 0.74967217 0.798257 0.80958104 0.76716691 0.68793344 0.58176845 0.4551363 0.32378846 0.20974773 0.1304507][0.23486134 0.34886897 0.47388381 0.58505434 0.67373919 0.7394138 0.7714746 0.76825041 0.71705139 0.63580453 0.53636879 0.42360845 0.3081983 0.20664631 0.13671145][0.2428629 0.33845267 0.447264 0.5453822 0.61706889 0.65748221 0.66083831 0.63625932 0.57813978 0.502737 0.42266557 0.34061036 0.2583653 0.18211822 0.12894019][0.2432536 0.31134069 0.39403638 0.47014484 0.51771128 0.52832437 0.50096983 0.45573255 0.39535922 0.33426017 0.2841678 0.24357423 0.20363452 0.15895443 0.12561323][0.23315968 0.27134198 0.32362407 0.37309244 0.3952404 0.38052315 0.33166257 0.27559906 0.22119404 0.18151081 0.16656779 0.16814686 0.1676788 0.15122114 0.13480581][0.19914494 0.2100953 0.23348601 0.25740945 0.25974053 0.23160817 0.17765 0.1255499 0.085649155 0.069483586 0.08563938 0.11882839 0.14427689 0.14492553 0.1383528][0.13424644 0.12449469 0.12475435 0.12899879 0.12120498 0.093515158 0.049970705 0.013621912 -0.0080975164 -0.0058681779 0.028170275 0.074910752 0.10859005 0.11391275 0.1098056][0.044130083 0.023761008 0.011307789 0.0048621814 -0.0043738079 -0.022858083 -0.048616014 -0.065728307 -0.071269825 -0.058897112 -0.021238076 0.023458889 0.052805427 0.055567011 0.04979834]]...]
INFO - root - 2017-12-11 03:00:29.223502: step 910, loss = 0.70, batch loss = 0.65 (31.2 examples/sec; 0.257 sec/batch; 23h:38m:27s remains)
INFO - root - 2017-12-11 03:00:31.917361: step 920, loss = 0.71, batch loss = 0.65 (28.2 examples/sec; 0.283 sec/batch; 26h:05m:32s remains)
INFO - root - 2017-12-11 03:00:34.569260: step 930, loss = 0.71, batch loss = 0.65 (29.8 examples/sec; 0.269 sec/batch; 24h:43m:53s remains)
INFO - root - 2017-12-11 03:00:37.232981: step 940, loss = 0.69, batch loss = 0.63 (30.4 examples/sec; 0.263 sec/batch; 24h:15m:55s remains)
INFO - root - 2017-12-11 03:00:39.916015: step 950, loss = 0.71, batch loss = 0.65 (28.5 examples/sec; 0.281 sec/batch; 25h:50m:06s remains)
INFO - root - 2017-12-11 03:00:42.549205: step 960, loss = 0.67, batch loss = 0.61 (30.0 examples/sec; 0.267 sec/batch; 24h:33m:05s remains)
INFO - root - 2017-12-11 03:00:45.224848: step 970, loss = 0.67, batch loss = 0.61 (30.3 examples/sec; 0.264 sec/batch; 24h:20m:01s remains)
INFO - root - 2017-12-11 03:00:47.914287: step 980, loss = 0.72, batch loss = 0.66 (28.3 examples/sec; 0.283 sec/batch; 26h:03m:17s remains)
INFO - root - 2017-12-11 03:00:50.562545: step 990, loss = 0.70, batch loss = 0.64 (30.7 examples/sec; 0.261 sec/batch; 24h:00m:00s remains)
INFO - root - 2017-12-11 03:00:53.216191: step 1000, loss = 0.70, batch loss = 0.64 (30.2 examples/sec; 0.265 sec/batch; 24h:21m:53s remains)
2017-12-11 03:00:53.665801: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.28916842 0.27912039 0.25738943 0.23421605 0.21296509 0.20748518 0.21112983 0.21464433 0.20530449 0.18197578 0.1512938 0.1099782 0.063907437 0.021797547 -0.0071725622][0.31523639 0.3006697 0.27653089 0.25585043 0.23961 0.24123439 0.24899232 0.24945092 0.22973573 0.19168158 0.1465887 0.093652241 0.041141063 -0.0021223375 -0.028443437][0.29484579 0.27600524 0.25360778 0.24138933 0.23731673 0.25281721 0.27130294 0.27477711 0.25023225 0.2024831 0.146167 0.082576506 0.023175053 -0.021806344 -0.046357349][0.25671786 0.2380974 0.22327259 0.22404969 0.23571645 0.26753646 0.29971635 0.30978248 0.28444937 0.23076151 0.1649555 0.090554267 0.021778805 -0.028465822 -0.054991838][0.20070387 0.1894723 0.1893654 0.206694 0.23450521 0.28039375 0.32404938 0.34027711 0.31563869 0.25841779 0.18617009 0.10409933 0.027698971 -0.027974797 -0.057822306][0.13073391 0.13281372 0.15144698 0.18645287 0.22936366 0.28587708 0.33766586 0.35939205 0.33659357 0.27726778 0.20045741 0.11382379 0.033194818 -0.025826959 -0.058319408][0.078738414 0.095486522 0.13061816 0.17922978 0.23210676 0.29264981 0.34614906 0.36984059 0.34807566 0.28704411 0.20660508 0.11779586 0.036268216 -0.023624979 -0.057643104][0.0709901 0.09864188 0.1425698 0.19550599 0.24875951 0.3042123 0.35113052 0.37128758 0.34830159 0.28639486 0.20448716 0.11626154 0.036626276 -0.022163216 -0.056528091][0.093918718 0.1254271 0.16888483 0.2171011 0.26259857 0.30658698 0.34227473 0.35656071 0.33266214 0.27284715 0.19365132 0.10964869 0.03440645 -0.0218715 -0.055465292][0.12843992 0.15664256 0.19201738 0.22928798 0.26261228 0.29362783 0.31863397 0.32790032 0.30506733 0.25072339 0.17812213 0.10051285 0.030370504 -0.022714119 -0.054669164][0.16195495 0.18241175 0.20567045 0.22965989 0.25022185 0.27019629 0.28748524 0.29367733 0.27309227 0.22541353 0.16049159 0.089448519 0.02424909 -0.025051286 -0.054488592][0.17271267 0.18398944 0.19528715 0.20680363 0.21600382 0.22737801 0.23942912 0.24403179 0.22640349 0.18623221 0.13061573 0.068358555 0.010758274 -0.032222092 -0.057171747][0.1485997 0.15071705 0.15231849 0.15420477 0.15534662 0.16117482 0.17021181 0.17417589 0.15989523 0.12779441 0.083316289 0.033172514 -0.012814729 -0.04592922 -0.063728862][0.089623734 0.085189231 0.08116772 0.077728905 0.074924387 0.078131519 0.085855104 0.089782961 0.079277478 0.055692047 0.023662113 -0.01190243 -0.04337348 -0.064088993 -0.072884537][0.018120639 0.011003765 0.0057332041 0.00098916539 -0.0027618371 -0.00082311255 0.0054400503 0.0088291047 0.0018041111 -0.013746247 -0.033939667 -0.055301987 -0.072435886 -0.080997624 -0.081171341]]...]
INFO - root - 2017-12-11 03:00:56.316073: step 1010, loss = 0.70, batch loss = 0.64 (31.7 examples/sec; 0.253 sec/batch; 23h:15m:10s remains)
INFO - root - 2017-12-11 03:00:59.024467: step 1020, loss = 0.68, batch loss = 0.62 (30.1 examples/sec; 0.266 sec/batch; 24h:27m:11s remains)
INFO - root - 2017-12-11 03:01:01.701969: step 1030, loss = 0.69, batch loss = 0.63 (27.8 examples/sec; 0.288 sec/batch; 26h:30m:30s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-SGD-lr0.001-clip50-initfromfixqian30k-fixqian
INFO - root - 2017-12-11 03:01:04.375661: step 1040, loss = 0.70, batch loss = 0.65 (29.4 examples/sec; 0.272 sec/batch; 25h:03m:31s remains)
INFO - root - 2017-12-11 03:01:07.032557: step 1050, loss = 0.71, batch loss = 0.65 (29.3 examples/sec; 0.273 sec/batch; 25h:07m:20s remains)
INFO - root - 2017-12-11 03:01:09.709702: step 1060, loss = 0.69, batch loss = 0.63 (30.2 examples/sec; 0.265 sec/batch; 24h:21m:18s remains)
INFO - root - 2017-12-11 03:01:12.359378: step 1070, loss = 0.69, batch loss = 0.63 (30.0 examples/sec; 0.267 sec/batch; 24h:34m:40s remains)
INFO - root - 2017-12-11 03:01:15.004209: step 1080, loss = 0.70, batch loss = 0.64 (30.8 examples/sec; 0.260 sec/batch; 23h:54m:38s remains)
INFO - root - 2017-12-11 03:01:17.648496: step 1090, loss = 0.69, batch loss = 0.63 (30.0 examples/sec; 0.267 sec/batch; 24h:32m:26s remains)
INFO - root - 2017-12-11 03:01:20.330640: step 1100, loss = 0.70, batch loss = 0.65 (30.2 examples/sec; 0.265 sec/batch; 24h:23m:31s remains)
2017-12-11 03:01:20.740317: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.35903031 0.36002266 0.35494187 0.36473647 0.37164697 0.35399818 0.31910849 0.2926282 0.27810174 0.25755727 0.22605126 0.19653282 0.1816995 0.17920022 0.18379837][0.34066519 0.34655616 0.34608045 0.36120254 0.37249032 0.36038819 0.33356354 0.31407046 0.30192482 0.279071 0.24295139 0.20577207 0.1841832 0.18024415 0.18900241][0.30663723 0.3142761 0.31780705 0.33990583 0.36263749 0.36825019 0.36159804 0.35575718 0.34464109 0.31198615 0.26240334 0.2127813 0.18442248 0.1815546 0.19674461][0.28509244 0.2890088 0.29022166 0.31693318 0.35503474 0.38600448 0.4060255 0.41562557 0.40217531 0.3535485 0.28521302 0.22286448 0.19221263 0.1958617 0.22055888][0.2797077 0.27585584 0.26997992 0.29760826 0.34946302 0.4055239 0.45121372 0.47439265 0.45684111 0.39212441 0.30757773 0.2381853 0.2128702 0.2289177 0.26563561][0.32116875 0.31409678 0.30130884 0.32424727 0.37983486 0.44893855 0.50853574 0.53739125 0.51469159 0.43962717 0.34649268 0.27622268 0.25956041 0.28852811 0.33651212][0.41987494 0.42101952 0.4041048 0.41541797 0.45986512 0.52348047 0.5796591 0.60329217 0.57476026 0.49700406 0.40436837 0.33769381 0.32821402 0.36541826 0.42029995][0.5409348 0.55450726 0.5347622 0.53000009 0.55336207 0.59803027 0.63801324 0.6493665 0.61618364 0.54311764 0.45908511 0.399269 0.394037 0.43323934 0.48932326][0.62725312 0.64865792 0.624295 0.60325313 0.60433394 0.62616885 0.64564764 0.64361686 0.60901123 0.54665112 0.47779304 0.42878041 0.42678624 0.46380994 0.51632482][0.6325624 0.653543 0.62558526 0.59376514 0.57785624 0.57853681 0.57796025 0.56472039 0.53269452 0.48574376 0.4368799 0.40270051 0.40471068 0.43764681 0.48323348][0.5513745 0.56522822 0.53527611 0.49962911 0.47536549 0.46264362 0.44929338 0.43068913 0.40436494 0.37229344 0.34081095 0.31869674 0.32191628 0.34846956 0.3853595][0.39877963 0.4036009 0.37583613 0.344146 0.3205002 0.3034178 0.28553578 0.26659784 0.24605632 0.22461887 0.20457329 0.18968573 0.19103935 0.20905848 0.2355012][0.21254517 0.20753217 0.18438822 0.16132869 0.14443913 0.13034308 0.11486296 0.099845953 0.085429981 0.071835317 0.0596006 0.050016925 0.049937256 0.060492009 0.077070825][0.05092911 0.038649604 0.02086403 0.0064843446 -0.0030848582 -0.012064015 -0.022487337 -0.031824414 -0.039961834 -0.047002252 -0.052728143 -0.05683605 -0.056254316 -0.050441552 -0.041569415][-0.054086708 -0.0684907 -0.080546744 -0.088158712 -0.092370018 -0.0969103 -0.10250617 -0.10701316 -0.11044727 -0.11308977 -0.11465825 -0.1152209 -0.11413899 -0.11134052 -0.10738471]]...]
INFO - root - 2017-12-11 03:01:23.381916: step 1110, loss = 0.69, batch loss = 0.63 (29.3 examples/sec; 0.274 sec/batch; 25h:10m:36s remains)
INFO - root - 2017-12-11 03:01:26.058934: step 1120, loss = 0.70, batch loss = 0.64 (31.3 examples/sec; 0.256 sec/batch; 23h:33m:51s remains)
INFO - root - 2017-12-11 03:01:28.738376: step 1130, loss = 0.70, batch loss = 0.64 (29.1 examples/sec; 0.275 sec/batch; 25h:16m:44s remains)
INFO - root - 2017-12-11 03:01:31.387909: step 1140, loss = 0.71, batch loss = 0.65 (30.0 examples/sec; 0.266 sec/batch; 24h:31m:19s remains)
INFO - root - 2017-12-11 03:01:34.053505: step 1150, loss = 0.70, batch loss = 0.64 (30.2 examples/sec; 0.265 sec/batch; 24h:22m:38s remains)
INFO - root - 2017-12-11 03:01:36.662580: step 1160, loss = 0.69, batch loss = 0.63 (31.7 examples/sec; 0.253 sec/batch; 23h:15m:22s remains)
INFO - root - 2017-12-11 03:01:39.321188: step 1170, loss = 0.71, batch loss = 0.65 (29.0 examples/sec; 0.276 sec/batch; 25h:21m:39s remains)
INFO - root - 2017-12-11 03:01:41.938366: step 1180, loss = 0.68, batch loss = 0.62 (29.3 examples/sec; 0.273 sec/batch; 25h:05m:11s remains)
INFO - root - 2017-12-11 03:01:44.613567: step 1190, loss = 0.69, batch loss = 0.63 (31.0 examples/sec; 0.258 sec/batch; 23h:44m:07s remains)
INFO - root - 2017-12-11 03:01:47.294231: step 1200, loss = 0.68, batch loss = 0.62 (29.2 examples/sec; 0.274 sec/batch; 25h:13m:05s remains)
2017-12-11 03:01:47.724849: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.086280182 0.091945834 0.099475406 0.10460107 0.09937235 0.084881276 0.069467545 0.0634008 0.06903185 0.080960631 0.093519747 0.10082187 0.099303015 0.09041737 0.078746676][0.11244757 0.11989593 0.13231006 0.14400308 0.14233622 0.12558343 0.10296471 0.087901339 0.085091464 0.089973636 0.098704807 0.10451181 0.10264753 0.092463367 0.078225084][0.13276631 0.1464975 0.17080551 0.19716643 0.20644847 0.19217896 0.16315195 0.13556223 0.1171834 0.1060148 0.10201776 0.099806294 0.0938435 0.080821596 0.063786887][0.14411573 0.16882516 0.21196894 0.26021433 0.28782046 0.28392413 0.25553945 0.21873604 0.183016 0.14886847 0.12248878 0.10300947 0.085158363 0.064168975 0.041745696][0.15876195 0.19373922 0.25565952 0.32595918 0.37534729 0.39184263 0.37864271 0.3455787 0.29812282 0.23844834 0.18183392 0.13559215 0.096771047 0.061329715 0.029889703][0.18477169 0.22214888 0.29322574 0.37704623 0.44667056 0.49380031 0.51518875 0.50594693 0.45961964 0.37854615 0.28924459 0.21019785 0.14244349 0.084911585 0.039433841][0.22760354 0.25419623 0.31717813 0.39953682 0.4824616 0.56430483 0.63308513 0.66386873 0.63238406 0.54092908 0.42603222 0.31594911 0.21619569 0.13194567 0.069792][0.28744721 0.29341543 0.331803 0.39573246 0.47751984 0.58163065 0.688532 0.75670403 0.746914 0.66208684 0.54157215 0.41505891 0.29031771 0.18086192 0.10332274][0.36627692 0.34835213 0.35072529 0.37945142 0.43968314 0.54044276 0.65972579 0.74975896 0.76421392 0.7050944 0.60369718 0.48085469 0.34365728 0.21564223 0.12689757][0.44887921 0.41309908 0.38012454 0.36702627 0.39071831 0.46485606 0.57215321 0.6682263 0.70600331 0.68307263 0.61477625 0.50820237 0.36997783 0.23330872 0.14000255][0.50026709 0.46052822 0.40733379 0.36238283 0.3497875 0.38829574 0.47011796 0.55836368 0.61002785 0.61709142 0.57976907 0.49461034 0.36996686 0.2432733 0.15762098][0.50463021 0.47566262 0.42212215 0.36391219 0.32653776 0.3332476 0.38560009 0.45427647 0.50410485 0.52463895 0.50787991 0.44720209 0.35333332 0.25783944 0.19201696][0.46391591 0.45299008 0.41312015 0.35816708 0.31129393 0.29781947 0.32555491 0.3726919 0.41400266 0.43947023 0.43873593 0.40666202 0.35375768 0.29813528 0.25324655][0.39025968 0.39489374 0.37129995 0.32772109 0.28414443 0.26447153 0.28005567 0.31798768 0.36100084 0.39866346 0.418856 0.4172214 0.40087014 0.37516755 0.34045732][0.30990231 0.3250792 0.315424 0.28587848 0.25258783 0.23532905 0.24788778 0.28748184 0.34079981 0.39504078 0.43594226 0.45881674 0.46659359 0.45727113 0.42601264]]...]
INFO - root - 2017-12-11 03:01:50.351744: step 1210, loss = 0.70, batch loss = 0.64 (28.1 examples/sec; 0.284 sec/batch; 26h:10m:20s remains)
INFO - root - 2017-12-11 03:01:53.022793: step 1220, loss = 0.71, batch loss = 0.65 (31.0 examples/sec; 0.258 sec/batch; 23h:46m:09s remains)
INFO - root - 2017-12-11 03:01:55.699203: step 1230, loss = 0.69, batch loss = 0.63 (30.2 examples/sec; 0.265 sec/batch; 24h:24m:09s remains)
INFO - root - 2017-12-11 03:01:58.336727: step 1240, loss = 0.68, batch loss = 0.63 (29.7 examples/sec; 0.269 sec/batch; 24h:47m:41s remains)
INFO - root - 2017-12-11 03:02:00.988731: step 1250, loss = 0.72, batch loss = 0.66 (30.8 examples/sec; 0.260 sec/batch; 23h:54m:53s remains)
INFO - root - 2017-12-11 03:02:03.677001: step 1260, loss = 0.69, batch loss = 0.63 (29.7 examples/sec; 0.269 sec/batch; 24h:46m:11s remains)
INFO - root - 2017-12-11 03:02:06.349655: step 1270, loss = 0.70, batch loss = 0.65 (30.9 examples/sec; 0.259 sec/batch; 23h:49m:55s remains)
INFO - root - 2017-12-11 03:02:09.060054: step 1280, loss = 0.70, batch loss = 0.64 (31.1 examples/sec; 0.257 sec/batch; 23h:40m:47s remains)
INFO - root - 2017-12-11 03:02:11.729334: step 1290, loss = 0.70, batch loss = 0.64 (30.5 examples/sec; 0.262 sec/batch; 24h:07m:14s remains)
INFO - root - 2017-12-11 03:02:14.409278: step 1300, loss = 0.70, batch loss = 0.65 (30.4 examples/sec; 0.263 sec/batch; 24h:11m:12s remains)
2017-12-11 03:02:14.868217: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.017170755 -0.02304263 -0.024446953 -0.021854989 -0.012716937 0.0048960573 0.03035116 0.056654047 0.072145306 0.070353471 0.049984034 0.018432105 -0.01240263 -0.035098825 -0.048315294][-0.0050233519 -0.012695325 -0.015476313 -0.012842405 -0.0012917276 0.021036591 0.051400475 0.080404207 0.095959343 0.092115939 0.067099348 0.028798198 -0.0080643371 -0.034474593 -0.049360812][0.011185162 0.0029792606 -0.00037975312 0.0026430222 0.016462069 0.04303968 0.077032954 0.1069392 0.12093256 0.11444907 0.085127994 0.040775325 -0.0014619399 -0.03135626 -0.048013821][0.028130475 0.019890722 0.016691305 0.020500677 0.03599824 0.0655772 0.10174469 0.13116884 0.14191376 0.13138309 0.097916454 0.049106486 0.0029528877 -0.029410303 -0.04695135][0.054502483 0.044446681 0.039098457 0.041748866 0.05623677 0.0858535 0.12177646 0.14912176 0.15556827 0.14058103 0.10393713 0.052914433 0.0049896147 -0.028680736 -0.04680723][0.095670469 0.083708264 0.075764023 0.075512178 0.085507579 0.11060644 0.14221007 0.16508466 0.16619065 0.14632699 0.10698203 0.055012736 0.0066857971 -0.027607383 -0.046573136][0.13706611 0.12761161 0.12099304 0.1202324 0.12645137 0.14540425 0.16998579 0.18638732 0.18114367 0.15560827 0.11231738 0.058532555 0.0098513151 -0.024635335 -0.044493791][0.16834462 0.16394611 0.16235074 0.16422516 0.16905977 0.18243037 0.19944026 0.20942798 0.19887605 0.1681238 0.12017445 0.063270837 0.01320443 -0.022143502 -0.043039866][0.17930621 0.18114698 0.18528239 0.19146128 0.19779906 0.20807011 0.21911256 0.22418398 0.21099436 0.17797163 0.12712435 0.06775514 0.016434105 -0.019654596 -0.041404456][0.16875096 0.17371349 0.18209285 0.19290294 0.20373774 0.21539897 0.22432159 0.22630671 0.21191813 0.17888719 0.12736298 0.067007385 0.015472977 -0.020195534 -0.041304659][0.14042459 0.14831388 0.16040897 0.17543115 0.1916475 0.20725639 0.21652937 0.21612522 0.2000763 0.16756591 0.11729985 0.058144465 0.008244154 -0.025139119 -0.043623228][0.0966616 0.10741536 0.12279839 0.1408889 0.16119753 0.18049461 0.19103794 0.18943153 0.17221907 0.14096759 0.094329134 0.039860014 -0.0052956333 -0.034086425 -0.048283048][0.045829233 0.057395756 0.073951282 0.09262456 0.11413532 0.1348307 0.14598052 0.14383405 0.12650059 0.097729161 0.05725887 0.011227853 -0.02563017 -0.047187477 -0.055523906][-0.0087982239 0.0014369574 0.017259194 0.034732644 0.055289231 0.075393982 0.086379088 0.084160455 0.06771341 0.042680874 0.010507187 -0.023996679 -0.049609486 -0.061891977 -0.06340456][-0.055495489 -0.048239637 -0.034536034 -0.019178255 -0.00067850685 0.017816296 0.028284078 0.026533244 0.01190686 -0.0090112416 -0.033072092 -0.056228459 -0.070694342 -0.074117616 -0.069547452]]...]
INFO - root - 2017-12-11 03:02:17.564735: step 1310, loss = 0.70, batch loss = 0.65 (28.5 examples/sec; 0.281 sec/batch; 25h:51m:05s remains)
INFO - root - 2017-12-11 03:02:20.214001: step 1320, loss = 0.68, batch loss = 0.62 (29.1 examples/sec; 0.275 sec/batch; 25h:18m:28s remains)
INFO - root - 2017-12-11 03:02:22.842192: step 1330, loss = 0.69, batch loss = 0.63 (31.8 examples/sec; 0.252 sec/batch; 23h:09m:17s remains)
INFO - root - 2017-12-11 03:02:25.508438: step 1340, loss = 0.70, batch loss = 0.64 (29.7 examples/sec; 0.269 sec/batch; 24h:46m:10s remains)
INFO - root - 2017-12-11 03:02:28.133032: step 1350, loss = 0.69, batch loss = 0.63 (30.0 examples/sec; 0.267 sec/batch; 24h:33m:22s remains)
INFO - root - 2017-12-11 03:02:30.776006: step 1360, loss = 0.68, batch loss = 0.62 (31.0 examples/sec; 0.258 sec/batch; 23h:44m:37s remains)
INFO - root - 2017-12-11 03:02:33.420637: step 1370, loss = 0.69, batch loss = 0.63 (31.1 examples/sec; 0.257 sec/batch; 23h:39m:01s remains)
INFO - root - 2017-12-11 03:02:36.082435: step 1380, loss = 0.69, batch loss = 0.63 (30.6 examples/sec; 0.262 sec/batch; 24h:04m:39s remains)
INFO - root - 2017-12-11 03:02:38.709593: step 1390, loss = 0.69, batch loss = 0.63 (30.7 examples/sec; 0.261 sec/batch; 23h:58m:31s remains)
INFO - root - 2017-12-11 03:02:41.360282: step 1400, loss = 0.69, batch loss = 0.63 (30.5 examples/sec; 0.263 sec/batch; 24h:09m:32s remains)
2017-12-11 03:02:41.835387: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.35988465 0.36807463 0.35598814 0.33461848 0.32092813 0.32496729 0.34007266 0.34948304 0.34447971 0.31852692 0.2766118 0.23299317 0.20529622 0.20438954 0.22064839][0.39593667 0.40680391 0.39265591 0.37061694 0.35834309 0.36559182 0.38473865 0.39707822 0.38990805 0.35486728 0.2975066 0.23497914 0.19025664 0.18071733 0.20279592][0.41069442 0.41946247 0.40855807 0.39517808 0.39311367 0.40820095 0.42971322 0.43660951 0.41567972 0.36071402 0.28046662 0.19662297 0.13728805 0.1248521 0.15929285][0.40709206 0.41682616 0.4162586 0.42053962 0.43610808 0.46128651 0.48138002 0.47451717 0.43051749 0.34972656 0.24663645 0.14789861 0.082998857 0.075674772 0.12690395][0.3881267 0.40125337 0.41547588 0.44060892 0.473206 0.50375736 0.51598442 0.49003094 0.42158541 0.31906894 0.20249799 0.10180835 0.043673586 0.050151661 0.12138516][0.35030282 0.36629006 0.39472273 0.43919307 0.48555374 0.51773781 0.51923853 0.47494516 0.38747051 0.27272707 0.15408614 0.061417941 0.01722925 0.040045168 0.13058719][0.29957464 0.31674153 0.35800433 0.42088804 0.48147115 0.51792473 0.51359516 0.45900264 0.36225441 0.24420346 0.12916806 0.045524012 0.013518555 0.04888067 0.15455195][0.24757053 0.26463461 0.31699318 0.39727986 0.4736852 0.51939607 0.51775795 0.46309638 0.36603332 0.24946031 0.13895124 0.062726177 0.039652288 0.082974993 0.19705434][0.20231049 0.21999717 0.27941778 0.37089753 0.45955208 0.515269 0.52168703 0.47402763 0.38236219 0.26957139 0.16352531 0.093619257 0.0778743 0.12666807 0.24370012][0.15865278 0.17207593 0.23001412 0.32222623 0.41504848 0.47813612 0.49464023 0.45893025 0.37825555 0.27353397 0.17458029 0.11214729 0.10344684 0.15545005 0.27018458][0.10557578 0.11281668 0.16228981 0.24607161 0.33472192 0.40032241 0.4248516 0.401696 0.3350102 0.24318574 0.15594761 0.10302649 0.10007586 0.15063292 0.25518209][0.050882708 0.052858219 0.090615451 0.15856187 0.23343232 0.29164112 0.31617284 0.30111849 0.2490077 0.17483278 0.10517222 0.065404542 0.068113774 0.11337955 0.19937664][0.0028658258 0.0014999848 0.026602907 0.074165121 0.12741008 0.16823146 0.1836817 0.17072175 0.13186976 0.078103229 0.030551538 0.0078513762 0.017909037 0.057092626 0.12044522][-0.034036953 -0.038218308 -0.025125986 0.002006674 0.032401673 0.053458814 0.057360996 0.044464197 0.017592685 -0.016302168 -0.042423069 -0.049032293 -0.033186968 -0.0011267996 0.040272724][-0.061556485 -0.067925356 -0.064713374 -0.054123092 -0.041859407 -0.035237543 -0.038158983 -0.048358053 -0.063490435 -0.080409139 -0.090403371 -0.08701349 -0.070426255 -0.047496524 -0.024559952]]...]
INFO - root - 2017-12-11 03:02:44.505484: step 1410, loss = 0.71, batch loss = 0.65 (30.5 examples/sec; 0.262 sec/batch; 24h:05m:32s remains)
INFO - root - 2017-12-11 03:02:47.137073: step 1420, loss = 0.69, batch loss = 0.64 (31.3 examples/sec; 0.255 sec/batch; 23h:29m:08s remains)
INFO - root - 2017-12-11 03:02:49.780068: step 1430, loss = 0.70, batch loss = 0.64 (30.5 examples/sec; 0.263 sec/batch; 24h:08m:36s remains)
INFO - root - 2017-12-11 03:02:52.514719: step 1440, loss = 0.71, batch loss = 0.65 (30.4 examples/sec; 0.263 sec/batch; 24h:11m:35s remains)
INFO - root - 2017-12-11 03:02:55.172209: step 1450, loss = 0.70, batch loss = 0.65 (30.5 examples/sec; 0.263 sec/batch; 24h:08m:40s remains)
INFO - root - 2017-12-11 03:02:57.802035: step 1460, loss = 0.71, batch loss = 0.65 (31.2 examples/sec; 0.256 sec/batch; 23h:33m:37s remains)
INFO - root - 2017-12-11 03:03:00.490394: step 1470, loss = 0.70, batch loss = 0.65 (28.5 examples/sec; 0.281 sec/batch; 25h:49m:37s remains)
INFO - root - 2017-12-11 03:03:03.123760: step 1480, loss = 0.69, batch loss = 0.63 (30.0 examples/sec; 0.267 sec/batch; 24h:30m:23s remains)
INFO - root - 2017-12-11 03:03:05.771440: step 1490, loss = 0.70, batch loss = 0.64 (29.3 examples/sec; 0.273 sec/batch; 25h:05m:33s remains)
INFO - root - 2017-12-11 03:03:08.460844: step 1500, loss = 0.71, batch loss = 0.65 (28.6 examples/sec; 0.279 sec/batch; 25h:41m:49s remains)
2017-12-11 03:03:08.945326: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.15994348 0.17145646 0.17389095 0.1731175 0.17271207 0.17280023 0.18137033 0.2006451 0.21744658 0.21982963 0.19907448 0.16304962 0.12162706 0.075411275 0.026454942][0.22705409 0.24516208 0.24955262 0.24740815 0.24298693 0.2369274 0.24133784 0.26066139 0.27938694 0.28171024 0.25598481 0.21153057 0.16058964 0.10449729 0.044985518][0.28056407 0.30503321 0.31105706 0.30682227 0.29782307 0.28632113 0.28750458 0.30696058 0.32763809 0.3316521 0.30450076 0.25415516 0.19405055 0.12907219 0.061582886][0.31524089 0.34371853 0.34959328 0.34281474 0.33070204 0.31747079 0.31890127 0.33960396 0.36187813 0.367261 0.33987758 0.28449446 0.21504283 0.1412601 0.067647651][0.32961661 0.35911021 0.36382473 0.35607421 0.34498531 0.33633137 0.34362471 0.36801285 0.39112666 0.39602977 0.36832282 0.30896074 0.2312448 0.1493092 0.070724338][0.32910705 0.35818458 0.36268285 0.35757193 0.35314968 0.35522467 0.37262166 0.40218356 0.42497656 0.42715377 0.39816669 0.33532539 0.25141 0.1630325 0.0797422][0.31139019 0.33824337 0.34340858 0.34416211 0.35093847 0.36797303 0.39753315 0.43268019 0.45506197 0.45426175 0.42367396 0.35868305 0.27254376 0.18141429 0.095227882][0.27867043 0.30202273 0.30847609 0.31613067 0.33380556 0.36368668 0.40179291 0.4392648 0.45970482 0.45575818 0.42374858 0.35936043 0.27678114 0.18914627 0.10483155][0.23471828 0.25234258 0.25895393 0.27140835 0.29532656 0.33071086 0.36945263 0.40434244 0.42190287 0.4162007 0.38499132 0.32580915 0.25425419 0.17741038 0.10079385][0.176281 0.18731411 0.19327471 0.20792165 0.23309697 0.26683822 0.3001343 0.32934937 0.34405464 0.33889484 0.3119615 0.26275471 0.20721079 0.14655678 0.082847022][0.10923092 0.11497412 0.12004699 0.13404819 0.1558753 0.1825408 0.20636699 0.22742461 0.23847833 0.23435932 0.21326317 0.17632838 0.13840687 0.0966749 0.04992035][0.042700976 0.043414272 0.046120744 0.056254763 0.07146614 0.088687524 0.10223845 0.11498065 0.12244312 0.11998294 0.10603214 0.082729742 0.062171448 0.039085094 0.010622544][-0.010868792 -0.014517724 -0.015253583 -0.010790137 -0.0035955466 0.0041900389 0.0090069165 0.015054336 0.019906076 0.019927198 0.013799975 0.003456688 -0.0029559741 -0.011291686 -0.024280759][-0.04908504 -0.055397425 -0.058434114 -0.058013111 -0.056364253 -0.054907329 -0.055041231 -0.052799094 -0.049359016 -0.04735031 -0.047621768 -0.049067516 -0.047254663 -0.046861622 -0.049555942][-0.071222395 -0.078084491 -0.081326254 -0.082184881 -0.082266606 -0.082603157 -0.083225965 -0.081709214 -0.07877472 -0.0761271 -0.074051544 -0.072094709 -0.06823235 -0.064968765 -0.062822223]]...]
INFO - root - 2017-12-11 03:03:11.578937: step 1510, loss = 0.69, batch loss = 0.63 (30.4 examples/sec; 0.263 sec/batch; 24h:09m:32s remains)
INFO - root - 2017-12-11 03:03:14.218164: step 1520, loss = 0.69, batch loss = 0.63 (29.5 examples/sec; 0.271 sec/batch; 24h:57m:17s remains)
INFO - root - 2017-12-11 03:03:16.879079: step 1530, loss = 0.71, batch loss = 0.65 (30.4 examples/sec; 0.263 sec/batch; 24h:10m:48s remains)
INFO - root - 2017-12-11 03:03:19.545850: step 1540, loss = 0.69, batch loss = 0.64 (30.6 examples/sec; 0.261 sec/batch; 24h:00m:23s remains)
INFO - root - 2017-12-11 03:03:22.217298: step 1550, loss = 0.70, batch loss = 0.64 (30.4 examples/sec; 0.263 sec/batch; 24h:09m:22s remains)
INFO - root - 2017-12-11 03:03:24.892433: step 1560, loss = 0.68, batch loss = 0.62 (30.2 examples/sec; 0.265 sec/batch; 24h:19m:47s remains)
INFO - root - 2017-12-11 03:03:27.559425: step 1570, loss = 0.70, batch loss = 0.64 (30.4 examples/sec; 0.263 sec/batch; 24h:09m:18s remains)
INFO - root - 2017-12-11 03:03:30.218725: step 1580, loss = 0.72, batch loss = 0.66 (30.0 examples/sec; 0.267 sec/batch; 24h:31m:17s remains)
INFO - root - 2017-12-11 03:03:32.849900: step 1590, loss = 0.69, batch loss = 0.64 (30.2 examples/sec; 0.265 sec/batch; 24h:22m:20s remains)
INFO - root - 2017-12-11 03:03:35.539577: step 1600, loss = 0.70, batch loss = 0.64 (31.5 examples/sec; 0.254 sec/batch; 23h:19m:41s remains)
2017-12-11 03:03:35.987928: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.19982728 0.21060292 0.20439874 0.18570636 0.16595979 0.14833514 0.13383034 0.12861861 0.13830481 0.15838972 0.17641228 0.18682225 0.18967845 0.18431517 0.17540565][0.27799472 0.29575777 0.29153082 0.27142665 0.24925911 0.22811265 0.20931789 0.201235 0.21167149 0.23592995 0.25925654 0.27308547 0.27662268 0.26974639 0.25963569][0.32486641 0.34672621 0.34384596 0.3232424 0.29981542 0.27731118 0.25775751 0.24988316 0.26206708 0.28962284 0.3172031 0.334018 0.33893612 0.33219686 0.32281208][0.3545436 0.38008723 0.37874031 0.35846594 0.33443037 0.31190944 0.29390377 0.28794912 0.30148849 0.32953426 0.35765213 0.37526393 0.38021344 0.37338609 0.36486888][0.38552213 0.4151943 0.41788679 0.40077347 0.37702322 0.35335326 0.33549678 0.32950071 0.34177557 0.36813617 0.396721 0.41685948 0.42407569 0.41845486 0.41054365][0.40654817 0.4402532 0.449762 0.43981168 0.42046186 0.39992458 0.38557088 0.37981918 0.38811737 0.40992305 0.4379319 0.461053 0.46989229 0.46361971 0.45389345][0.41177025 0.45033044 0.46905655 0.46935117 0.45876211 0.44778895 0.44362354 0.44177544 0.44568351 0.46016586 0.48313895 0.50319415 0.50694257 0.49433339 0.47936603][0.40772647 0.45133224 0.47921365 0.49060851 0.49229348 0.49554053 0.5044477 0.50701255 0.5047518 0.508231 0.51908082 0.52669865 0.51605129 0.48983827 0.46480983][0.38004586 0.42579269 0.46110365 0.48299587 0.497352 0.5137223 0.531897 0.53473389 0.52402568 0.51581746 0.51317543 0.50542551 0.47822481 0.43775085 0.4027836][0.31750837 0.35643098 0.39098158 0.41686097 0.43847504 0.46203113 0.48356158 0.48480439 0.46887231 0.45450002 0.44397023 0.42589161 0.38837123 0.34045824 0.30107144][0.22764179 0.25134307 0.27687097 0.30069748 0.32459417 0.3508243 0.37249765 0.37300387 0.3564029 0.3407779 0.326355 0.30194458 0.25972927 0.21103363 0.17305885][0.13424805 0.14061065 0.15427712 0.17283574 0.19490217 0.21947984 0.23912491 0.24099134 0.22862165 0.21612193 0.20137547 0.1748492 0.13332748 0.088510506 0.054744016][0.060026895 0.05495565 0.060602229 0.074921191 0.093184024 0.11214202 0.12655757 0.12899159 0.12176237 0.11331457 0.1003776 0.076236226 0.040223796 0.0017906419 -0.027741559][0.0078745885 -0.00050317956 0.0035206873 0.017234864 0.033599146 0.048208408 0.059738733 0.065828755 0.066759512 0.064085804 0.054134667 0.033452541 0.0027223178 -0.031484362 -0.060060076][-0.014068082 -0.0185725 -0.0093977135 0.0097069843 0.030442057 0.048362918 0.065646358 0.08259809 0.095649727 0.099405192 0.089776218 0.06677568 0.032449357 -0.0072602942 -0.043290973]]...]
INFO - root - 2017-12-11 03:03:38.673556: step 1610, loss = 0.69, batch loss = 0.63 (30.5 examples/sec; 0.262 sec/batch; 24h:06m:53s remains)
INFO - root - 2017-12-11 03:03:41.306808: step 1620, loss = 0.70, batch loss = 0.64 (31.3 examples/sec; 0.256 sec/batch; 23h:30m:25s remains)
INFO - root - 2017-12-11 03:03:43.975669: step 1630, loss = 0.70, batch loss = 0.64 (29.5 examples/sec; 0.271 sec/batch; 24h:53m:50s remains)
INFO - root - 2017-12-11 03:03:46.614371: step 1640, loss = 0.69, batch loss = 0.63 (31.2 examples/sec; 0.256 sec/batch; 23h:33m:23s remains)
INFO - root - 2017-12-11 03:03:49.295356: step 1650, loss = 0.69, batch loss = 0.63 (29.9 examples/sec; 0.267 sec/batch; 24h:33m:28s remains)
INFO - root - 2017-12-11 03:03:51.963720: step 1660, loss = 0.69, batch loss = 0.64 (29.5 examples/sec; 0.271 sec/batch; 24h:52m:54s remains)
INFO - root - 2017-12-11 03:03:54.616074: step 1670, loss = 0.70, batch loss = 0.64 (30.5 examples/sec; 0.262 sec/batch; 24h:05m:10s remains)
INFO - root - 2017-12-11 03:03:57.306441: step 1680, loss = 0.70, batch loss = 0.64 (31.4 examples/sec; 0.255 sec/batch; 23h:26m:06s remains)
INFO - root - 2017-12-11 03:04:00.008330: step 1690, loss = 0.71, batch loss = 0.65 (26.9 examples/sec; 0.297 sec/batch; 27h:19m:21s remains)
INFO - root - 2017-12-11 03:04:02.683741: step 1700, loss = 0.69, batch loss = 0.63 (30.0 examples/sec; 0.267 sec/batch; 24h:31m:14s remains)
2017-12-11 03:04:03.102980: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.58175713 0.60124546 0.543979 0.42918575 0.28417683 0.14793082 0.049915276 -0.00076612859 -0.020128159 -0.027660897 -0.032084443 -0.029836249 -0.01811485 0.0049379044 0.038371265][0.62521607 0.64046758 0.5784418 0.4630349 0.32435724 0.1989944 0.11245643 0.068521336 0.046785906 0.027216837 0.0054255067 -0.007164116 -0.0027258608 0.021706369 0.061384678][0.589773 0.603566 0.55181926 0.45960638 0.35514107 0.26643008 0.20952639 0.17988338 0.15476274 0.11771468 0.071559139 0.038056444 0.031571321 0.05559212 0.099543586][0.51031786 0.52582294 0.49477926 0.441475 0.38775477 0.34938839 0.33061031 0.31813887 0.28827554 0.2302186 0.15653221 0.099570408 0.080064565 0.10048249 0.14394502][0.42501014 0.44279647 0.43691269 0.42985585 0.43323746 0.44756588 0.46466151 0.46455696 0.42535293 0.34424612 0.24442808 0.16702272 0.1362171 0.15072398 0.18719886][0.35782912 0.37778166 0.39674073 0.43543288 0.49317786 0.55441105 0.598437 0.60156912 0.54789996 0.44380993 0.32226485 0.22979543 0.19128272 0.19896822 0.22315952][0.31088924 0.33342573 0.37297842 0.44761726 0.54400331 0.63398904 0.68877631 0.68502837 0.6152826 0.49441904 0.36125308 0.26286629 0.22117217 0.22219497 0.23294178][0.2872442 0.30986208 0.36033186 0.45254347 0.56259429 0.65627384 0.70381385 0.68621916 0.60413074 0.47776097 0.34673375 0.25316259 0.21286626 0.20849758 0.20858978][0.27426875 0.29404539 0.34505913 0.43679044 0.53972834 0.61908424 0.64836675 0.61549008 0.52774733 0.40688679 0.28856483 0.20730558 0.17244388 0.16619161 0.16187237][0.24272805 0.25756678 0.30222473 0.38148189 0.46584225 0.52384233 0.53385097 0.49030453 0.40518355 0.29931122 0.20161079 0.13858134 0.11431157 0.11226815 0.1109753][0.17121574 0.18150912 0.21733806 0.27862814 0.34031448 0.37736702 0.3741149 0.32936239 0.25616035 0.17238386 0.099467888 0.056936998 0.04559591 0.051457111 0.058040529][0.0689873 0.076123551 0.10330047 0.14573994 0.18466434 0.20302793 0.19209734 0.15375885 0.099226579 0.04183742 -0.0043859673 -0.026391337 -0.025521593 -0.012237217 0.0019601937][-0.030728925 -0.025066203 -0.0054695569 0.020236075 0.039963007 0.044466775 0.030898806 0.0032074668 -0.030498067 -0.061964571 -0.083490267 -0.087705478 -0.077161916 -0.059020068 -0.040423375][-0.10556371 -0.10107807 -0.087677024 -0.074286796 -0.067074753 -0.069498807 -0.080721334 -0.096158653 -0.11092667 -0.12122983 -0.12408163 -0.11618706 -0.10043313 -0.081597306 -0.063747667][-0.14916556 -0.14788212 -0.14003566 -0.13418131 -0.13278724 -0.13630655 -0.14240851 -0.1470705 -0.14780787 -0.14382225 -0.13511264 -0.12118483 -0.10491727 -0.089506537 -0.076882623]]...]
INFO - root - 2017-12-11 03:04:05.770571: step 1710, loss = 0.72, batch loss = 0.66 (30.9 examples/sec; 0.259 sec/batch; 23h:49m:37s remains)
INFO - root - 2017-12-11 03:04:08.391171: step 1720, loss = 0.69, batch loss = 0.63 (31.1 examples/sec; 0.257 sec/batch; 23h:39m:27s remains)
INFO - root - 2017-12-11 03:04:11.122264: step 1730, loss = 0.70, batch loss = 0.64 (29.2 examples/sec; 0.274 sec/batch; 25h:08m:58s remains)
INFO - root - 2017-12-11 03:04:13.784409: step 1740, loss = 0.70, batch loss = 0.64 (30.2 examples/sec; 0.265 sec/batch; 24h:18m:32s remains)
INFO - root - 2017-12-11 03:04:16.533351: step 1750, loss = 0.69, batch loss = 0.64 (28.8 examples/sec; 0.278 sec/batch; 25h:32m:49s remains)
INFO - root - 2017-12-11 03:04:19.202791: step 1760, loss = 0.70, batch loss = 0.64 (29.7 examples/sec; 0.270 sec/batch; 24h:46m:11s remains)
INFO - root - 2017-12-11 03:04:21.859824: step 1770, loss = 0.71, batch loss = 0.65 (30.4 examples/sec; 0.263 sec/batch; 24h:08m:13s remains)
INFO - root - 2017-12-11 03:04:24.567087: step 1780, loss = 0.70, batch loss = 0.64 (30.2 examples/sec; 0.265 sec/batch; 24h:18m:57s remains)
INFO - root - 2017-12-11 03:04:27.246952: step 1790, loss = 0.68, batch loss = 0.62 (29.9 examples/sec; 0.268 sec/batch; 24h:35m:23s remains)
INFO - root - 2017-12-11 03:04:29.914326: step 1800, loss = 0.70, batch loss = 0.64 (30.4 examples/sec; 0.263 sec/batch; 24h:11m:32s remains)
2017-12-11 03:04:30.355533: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.02093921 0.025147539 0.094041556 0.17357215 0.24307042 0.28624466 0.30028102 0.2864241 0.25445974 0.22397695 0.21763386 0.23629205 0.2629469 0.28456268 0.29991657][-0.023688402 0.023200631 0.094752714 0.17917553 0.25506884 0.30380735 0.32321894 0.31756461 0.29548004 0.27150774 0.26441431 0.27763489 0.2974579 0.31048611 0.3150838][-0.026966775 0.016879259 0.084603347 0.16781746 0.24663389 0.30066872 0.32700506 0.33195174 0.32288978 0.30783656 0.30012923 0.30459398 0.3125135 0.31207791 0.30328402][-0.026005914 0.016667435 0.081486039 0.16316807 0.24355903 0.30132303 0.33244735 0.34382477 0.34237364 0.33227605 0.3220399 0.3167668 0.3126168 0.30082041 0.28384173][-0.022701578 0.021531323 0.087227747 0.17047535 0.25343078 0.31447592 0.34810898 0.36058012 0.3586334 0.34595847 0.32852525 0.31183559 0.29613912 0.2758396 0.25592351][-0.02163453 0.024505697 0.092838444 0.17937022 0.26560616 0.3297303 0.3651745 0.37680373 0.37095717 0.35272264 0.3262901 0.29832312 0.27248761 0.24646753 0.22754636][-0.023087952 0.023052942 0.0928041 0.18112171 0.26925921 0.33655745 0.37560359 0.38872293 0.38073131 0.35758081 0.32256386 0.28499013 0.25228027 0.22566397 0.2127668][-0.024824617 0.019861924 0.089049317 0.17585681 0.2623207 0.33033851 0.37228975 0.38776565 0.37908247 0.3515456 0.30969357 0.26632741 0.23326765 0.21377243 0.21286787][-0.025927225 0.016989801 0.084113985 0.16660464 0.24868436 0.3153708 0.35911304 0.37681156 0.36802572 0.336932 0.29027009 0.24463697 0.21607012 0.20921281 0.22387941][-0.027663324 0.012483529 0.075104646 0.1504143 0.22628324 0.2904278 0.33488947 0.35398841 0.34534514 0.31161723 0.26178703 0.21552128 0.19181526 0.19650522 0.22371495][-0.029785814 0.0057584154 0.061253253 0.12764239 0.19740044 0.25974062 0.30546162 0.32619667 0.31782719 0.28174207 0.22925377 0.18243733 0.16223869 0.17493454 0.21044935][-0.03200455 -0.002058449 0.045498289 0.10367609 0.16909586 0.23064341 0.27692837 0.29709229 0.28605852 0.24592143 0.19133683 0.14633027 0.13136017 0.15066639 0.19062214][-0.034831207 -0.009987534 0.030836049 0.083253331 0.14615324 0.20676841 0.25106373 0.26640603 0.24792941 0.20073836 0.14408551 0.10391514 0.097254544 0.12239084 0.16222592][-0.036757212 -0.014439248 0.023718745 0.075287707 0.13855347 0.19796629 0.23692638 0.24219605 0.21104115 0.15367897 0.094504304 0.060658552 0.062892653 0.091874361 0.12743379][-0.036638554 -0.013663461 0.026724635 0.082697161 0.14921212 0.2067422 0.23578736 0.22443853 0.17531337 0.10592641 0.044830956 0.017717076 0.027729109 0.05836606 0.087592222]]...]
INFO - root - 2017-12-11 03:04:33.018809: step 1810, loss = 0.69, batch loss = 0.63 (30.2 examples/sec; 0.265 sec/batch; 24h:17m:58s remains)
INFO - root - 2017-12-11 03:04:35.650951: step 1820, loss = 0.70, batch loss = 0.65 (30.4 examples/sec; 0.263 sec/batch; 24h:09m:13s remains)
INFO - root - 2017-12-11 03:04:38.307915: step 1830, loss = 0.69, batch loss = 0.63 (28.9 examples/sec; 0.277 sec/batch; 25h:24m:14s remains)
INFO - root - 2017-12-11 03:04:40.995977: step 1840, loss = 0.69, batch loss = 0.63 (29.6 examples/sec; 0.271 sec/batch; 24h:51m:11s remains)
INFO - root - 2017-12-11 03:04:43.659671: step 1850, loss = 0.71, batch loss = 0.65 (30.7 examples/sec; 0.260 sec/batch; 23h:53m:59s remains)
INFO - root - 2017-12-11 03:04:46.292181: step 1860, loss = 0.68, batch loss = 0.62 (31.0 examples/sec; 0.258 sec/batch; 23h:40m:16s remains)
INFO - root - 2017-12-11 03:04:48.969812: step 1870, loss = 0.69, batch loss = 0.63 (31.2 examples/sec; 0.256 sec/batch; 23h:31m:23s remains)
INFO - root - 2017-12-11 03:04:51.620187: step 1880, loss = 0.70, batch loss = 0.65 (30.7 examples/sec; 0.261 sec/batch; 23h:57m:20s remains)
INFO - root - 2017-12-11 03:04:54.285935: step 1890, loss = 0.71, batch loss = 0.65 (28.6 examples/sec; 0.280 sec/batch; 25h:43m:17s remains)
INFO - root - 2017-12-11 03:04:56.936789: step 1900, loss = 0.70, batch loss = 0.64 (30.0 examples/sec; 0.267 sec/batch; 24h:29m:00s remains)
2017-12-11 03:04:57.436388: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.32636917 0.35468093 0.32284415 0.2805346 0.26396176 0.27874026 0.2952475 0.28938678 0.27214473 0.25894693 0.25973696 0.26281959 0.26695764 0.26780564 0.26661229][0.44689897 0.49590087 0.47639412 0.43917012 0.42440742 0.44162446 0.45975095 0.45173562 0.42631087 0.396326 0.37671915 0.35503829 0.34119564 0.33656013 0.34114674][0.51730192 0.58106565 0.58019817 0.55948031 0.55449224 0.57504851 0.59321392 0.58395743 0.55475396 0.51296192 0.47661403 0.43367127 0.40277711 0.39214557 0.40145856][0.54167318 0.60784435 0.6220876 0.62091166 0.62897176 0.65371364 0.67314714 0.665081 0.63663077 0.59185737 0.55307865 0.50507551 0.46864632 0.45640144 0.46539885][0.51208204 0.57284009 0.59580183 0.610497 0.63158154 0.66355842 0.68979347 0.68804216 0.66527689 0.626534 0.59992641 0.56506938 0.53626794 0.52526569 0.52785194][0.41911608 0.47323939 0.50293064 0.53193253 0.56796461 0.61313576 0.65402347 0.66696084 0.65852249 0.63588482 0.63185608 0.62015337 0.60375553 0.590073 0.57721615][0.31271377 0.35767189 0.38823292 0.42315945 0.46745792 0.52311563 0.57864159 0.60905844 0.61976045 0.61848438 0.639291 0.6513235 0.64532238 0.62335271 0.58969754][0.24506761 0.27920979 0.30324703 0.33460212 0.37744588 0.43472549 0.49651608 0.53853017 0.56390387 0.57961756 0.61801088 0.6462931 0.64561158 0.61446017 0.56505632][0.21090293 0.23604254 0.25326923 0.27854523 0.31547448 0.36791503 0.42802754 0.47432241 0.50646943 0.52925891 0.57368922 0.60686219 0.606193 0.56883258 0.51284575][0.17312944 0.19177426 0.20511852 0.22551525 0.25568634 0.29931873 0.3508175 0.39343977 0.42370978 0.44334853 0.48219335 0.51162696 0.509579 0.47275221 0.42302477][0.11385459 0.12623525 0.13759561 0.15424947 0.17728664 0.20975655 0.24826792 0.28156385 0.30481192 0.31746554 0.34825191 0.37368357 0.37380537 0.34596431 0.3122035][0.05441305 0.057215948 0.064034253 0.074771553 0.088686317 0.10790776 0.13054575 0.15031092 0.16310816 0.16813274 0.19266741 0.21741521 0.22439612 0.21161796 0.19775754][0.0044511035 -0.0011003609 0.00078658824 0.0056021838 0.011473721 0.019367004 0.027963292 0.034535196 0.037010148 0.035707589 0.054916855 0.078967288 0.091842711 0.091783233 0.092730194][-0.0424377 -0.051144753 -0.050692875 -0.048537686 -0.046682589 -0.04498107 -0.044507485 -0.045890898 -0.049097512 -0.053035077 -0.038111724 -0.01636292 -0.0015650769 0.0041756411 0.0099867][-0.083024889 -0.091917239 -0.091903821 -0.091476575 -0.091938041 -0.093230076 -0.096029513 -0.10002302 -0.1038747 -0.10651167 -0.094034024 -0.07524956 -0.061021045 -0.0537963 -0.048595391]]...]
INFO - root - 2017-12-11 03:05:00.092413: step 1910, loss = 0.68, batch loss = 0.62 (30.2 examples/sec; 0.265 sec/batch; 24h:18m:02s remains)
INFO - root - 2017-12-11 03:05:02.744307: step 1920, loss = 0.71, batch loss = 0.66 (31.0 examples/sec; 0.258 sec/batch; 23h:41m:06s remains)
INFO - root - 2017-12-11 03:05:05.405345: step 1930, loss = 0.69, batch loss = 0.63 (30.1 examples/sec; 0.266 sec/batch; 24h:24m:36s remains)
INFO - root - 2017-12-11 03:05:08.051729: step 1940, loss = 0.71, batch loss = 0.65 (30.4 examples/sec; 0.263 sec/batch; 24h:08m:37s remains)
INFO - root - 2017-12-11 03:05:10.679290: step 1950, loss = 0.69, batch loss = 0.64 (30.9 examples/sec; 0.259 sec/batch; 23h:48m:22s remains)
INFO - root - 2017-12-11 03:05:13.333971: step 1960, loss = 0.70, batch loss = 0.64 (29.9 examples/sec; 0.268 sec/batch; 24h:34m:32s remains)
INFO - root - 2017-12-11 03:05:16.002962: step 1970, loss = 0.71, batch loss = 0.65 (30.4 examples/sec; 0.263 sec/batch; 24h:10m:10s remains)
INFO - root - 2017-12-11 03:05:18.707423: step 1980, loss = 0.69, batch loss = 0.63 (28.6 examples/sec; 0.280 sec/batch; 25h:41m:03s remains)
INFO - root - 2017-12-11 03:05:21.404319: step 1990, loss = 0.69, batch loss = 0.63 (30.8 examples/sec; 0.260 sec/batch; 23h:50m:49s remains)
INFO - root - 2017-12-11 03:05:24.044255: step 2000, loss = 0.70, batch loss = 0.64 (30.2 examples/sec; 0.264 sec/batch; 24h:16m:55s remains)
2017-12-11 03:05:24.474478: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.034150608 0.028498847 0.021472892 0.014266307 0.0072792484 0.0010601254 -0.0022053272 0.00096902222 0.011462002 0.025029074 0.041157823 0.059984457 0.07071808 0.067470945 0.056196589][0.019229764 0.022140965 0.024909824 0.026208334 0.027372209 0.030635377 0.035629328 0.043262538 0.055216432 0.071303576 0.091392063 0.1134912 0.12471522 0.11883773 0.10299926][0.01228822 0.026531264 0.042661645 0.056478418 0.069205426 0.084365666 0.098247483 0.10852884 0.11832485 0.13257559 0.15169235 0.17437132 0.18633057 0.17897083 0.15844639][0.034686487 0.061331581 0.091051959 0.11632968 0.13731265 0.15947303 0.17753498 0.18573435 0.18819873 0.19583015 0.21003221 0.23162428 0.24719405 0.24397621 0.22310795][0.078995474 0.11974218 0.16376728 0.19952792 0.22441499 0.24706383 0.26460159 0.2673845 0.25856954 0.25549793 0.2613703 0.27966729 0.29817656 0.30125234 0.28375033][0.13135391 0.18632579 0.24427442 0.28849658 0.31315732 0.33209985 0.34810328 0.34570789 0.32411864 0.30844113 0.3052755 0.31918311 0.33774012 0.34395385 0.32963172][0.17001088 0.23658338 0.30515531 0.3539612 0.37513712 0.38851193 0.40373448 0.39847714 0.36719197 0.34229231 0.33434859 0.34583047 0.36228141 0.3679131 0.35550451][0.18879783 0.26138777 0.3363592 0.38721785 0.40494856 0.41380569 0.42825091 0.42097464 0.38358369 0.35391632 0.34571615 0.35644066 0.3682113 0.36905518 0.35619354][0.19719279 0.26902786 0.34428844 0.39483485 0.41050589 0.41605362 0.42793861 0.41886616 0.37973195 0.34907424 0.34219638 0.35138649 0.35537019 0.34712836 0.33060816][0.19473007 0.2584466 0.32695672 0.37434566 0.3897315 0.39421722 0.40349904 0.39517352 0.36052784 0.33307853 0.32705036 0.33171698 0.3246716 0.30409205 0.28046095][0.18243214 0.23396549 0.2902661 0.33162326 0.34724715 0.35162956 0.35782161 0.35152406 0.32521921 0.30331364 0.29667854 0.29452223 0.27674556 0.24580635 0.21617283][0.15544562 0.19412833 0.23751484 0.27282935 0.29010195 0.29604003 0.29921195 0.29358983 0.27413684 0.25578079 0.24538557 0.23501359 0.21012153 0.17584229 0.14618286][0.11327312 0.14066203 0.1736476 0.20528211 0.22654462 0.23668572 0.23879308 0.23321119 0.21728139 0.19889548 0.18200395 0.16314492 0.13467266 0.10248157 0.077869847][0.069244348 0.085129194 0.10730996 0.13285373 0.1547201 0.16736872 0.16975768 0.16536433 0.15280694 0.13540217 0.11504224 0.091851763 0.064105608 0.037783906 0.021382531][0.027796106 0.031012338 0.040817332 0.05626167 0.0730307 0.084270887 0.086913936 0.084834337 0.077072345 0.064432591 0.047421396 0.027608735 0.0063527767 -0.011072823 -0.018528903]]...]
INFO - root - 2017-12-11 03:05:27.100713: step 2010, loss = 0.70, batch loss = 0.64 (29.4 examples/sec; 0.272 sec/batch; 24h:59m:59s remains)
INFO - root - 2017-12-11 03:05:29.767286: step 2020, loss = 0.70, batch loss = 0.65 (31.3 examples/sec; 0.255 sec/batch; 23h:27m:03s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-SGD-lr0.001-clip50-initfromfixqian30k-fixqian
INFO - root - 2017-12-11 03:05:32.401663: step 2030, loss = 0.68, batch loss = 0.63 (30.6 examples/sec; 0.262 sec/batch; 24h:02m:06s remains)
INFO - root - 2017-12-11 03:05:35.055882: step 2040, loss = 0.70, batch loss = 0.64 (31.6 examples/sec; 0.253 sec/batch; 23h:13m:54s remains)
INFO - root - 2017-12-11 03:05:37.724704: step 2050, loss = 0.71, batch loss = 0.65 (30.0 examples/sec; 0.267 sec/batch; 24h:29m:51s remains)
INFO - root - 2017-12-11 03:05:40.359428: step 2060, loss = 0.70, batch loss = 0.64 (30.9 examples/sec; 0.259 sec/batch; 23h:46m:27s remains)
INFO - root - 2017-12-11 03:05:43.027390: step 2070, loss = 0.70, batch loss = 0.65 (29.9 examples/sec; 0.268 sec/batch; 24h:35m:46s remains)
INFO - root - 2017-12-11 03:05:45.671089: step 2080, loss = 0.73, batch loss = 0.67 (29.5 examples/sec; 0.272 sec/batch; 24h:55m:50s remains)
INFO - root - 2017-12-11 03:05:48.297799: step 2090, loss = 0.69, batch loss = 0.63 (29.5 examples/sec; 0.271 sec/batch; 24h:54m:38s remains)
INFO - root - 2017-12-11 03:05:50.946317: step 2100, loss = 0.69, batch loss = 0.63 (29.3 examples/sec; 0.273 sec/batch; 25h:02m:49s remains)
2017-12-11 03:05:51.402054: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.17543247 0.22715694 0.27385688 0.29934534 0.30491939 0.30166543 0.29611921 0.28380841 0.26366434 0.2358889 0.2025277 0.16136384 0.11778837 0.084010221 0.07265126][0.18524387 0.23652375 0.27791175 0.29736784 0.29777578 0.28475273 0.26376984 0.23648302 0.2072937 0.17897604 0.15259938 0.12260377 0.091794178 0.071932711 0.074333057][0.19033571 0.2405872 0.27774686 0.29337153 0.29286283 0.27831307 0.2515572 0.21682084 0.18235163 0.15249179 0.12844066 0.10257294 0.0756299 0.059278525 0.065899208][0.20218454 0.24469016 0.27376607 0.28337353 0.28133664 0.26927307 0.24509013 0.2115017 0.1774925 0.14777744 0.12347202 0.096698962 0.066759817 0.045063779 0.046033263][0.23180769 0.25811982 0.27186459 0.27039135 0.2625486 0.25163096 0.23340636 0.20761903 0.18037003 0.15472443 0.13119346 0.10316616 0.068781435 0.038025238 0.027923463][0.27045166 0.27661994 0.27246216 0.25945005 0.24618663 0.23655155 0.22536838 0.20964442 0.19114074 0.17028211 0.14638227 0.11651523 0.07913591 0.042113375 0.023352837][0.29474121 0.28439057 0.26745439 0.2507607 0.24123514 0.23951444 0.23806895 0.23135224 0.2178899 0.19681531 0.16813059 0.13465972 0.096850239 0.060508013 0.041979387][0.29114369 0.26982874 0.24863008 0.2392261 0.24548988 0.26111177 0.27416027 0.27579108 0.26322523 0.23750843 0.20214526 0.16546229 0.12942109 0.097993031 0.084736228][0.2695826 0.2431106 0.2237844 0.22644387 0.25247169 0.28887478 0.31786653 0.32575959 0.30981886 0.27659991 0.23601362 0.19978034 0.16839392 0.14301208 0.13380881][0.24245995 0.21516329 0.20012496 0.21287979 0.25314653 0.30520496 0.34670272 0.35886556 0.33917031 0.3001622 0.25930756 0.23032992 0.20926572 0.19082759 0.18039151][0.21296158 0.18664059 0.17526495 0.193058 0.238895 0.29794672 0.34579059 0.36019415 0.3375456 0.29492024 0.25756374 0.24204841 0.23838401 0.23073304 0.21719843][0.19118342 0.16235258 0.14912994 0.1648241 0.20816885 0.26610261 0.31356165 0.32813454 0.30516273 0.26292062 0.23181061 0.23213656 0.24997963 0.25877026 0.24735016][0.18276918 0.14560771 0.12287176 0.12941007 0.16493946 0.21683626 0.26025128 0.27513191 0.25638813 0.21971548 0.19553722 0.20773861 0.24309501 0.26912484 0.26595017][0.18840785 0.14108786 0.10578416 0.09901987 0.12250713 0.16434635 0.20113799 0.21601911 0.20396982 0.17634715 0.15854418 0.17492425 0.21672848 0.25322723 0.2600708][0.20205504 0.15104409 0.10972766 0.092927441 0.10334279 0.13120773 0.15766379 0.16971397 0.16271032 0.1442575 0.13233903 0.14746635 0.18538417 0.22267093 0.23576716]]...]
INFO - root - 2017-12-11 03:05:54.069480: step 2110, loss = 0.71, batch loss = 0.65 (31.3 examples/sec; 0.256 sec/batch; 23h:28m:37s remains)
INFO - root - 2017-12-11 03:05:56.697918: step 2120, loss = 0.69, batch loss = 0.63 (29.4 examples/sec; 0.272 sec/batch; 24h:58m:59s remains)
INFO - root - 2017-12-11 03:05:59.345233: step 2130, loss = 0.70, batch loss = 0.64 (30.0 examples/sec; 0.266 sec/batch; 24h:26m:23s remains)
INFO - root - 2017-12-11 03:06:02.004546: step 2140, loss = 0.70, batch loss = 0.65 (29.5 examples/sec; 0.271 sec/batch; 24h:50m:41s remains)
INFO - root - 2017-12-11 03:06:04.662498: step 2150, loss = 0.70, batch loss = 0.64 (30.2 examples/sec; 0.265 sec/batch; 24h:19m:51s remains)
INFO - root - 2017-12-11 03:06:07.303539: step 2160, loss = 0.70, batch loss = 0.64 (31.3 examples/sec; 0.256 sec/batch; 23h:27m:15s remains)
INFO - root - 2017-12-11 03:06:09.925522: step 2170, loss = 0.71, batch loss = 0.65 (30.0 examples/sec; 0.267 sec/batch; 24h:28m:14s remains)
INFO - root - 2017-12-11 03:06:12.602024: step 2180, loss = 0.70, batch loss = 0.65 (28.7 examples/sec; 0.279 sec/batch; 25h:34m:38s remains)
INFO - root - 2017-12-11 03:06:15.250948: step 2190, loss = 0.69, batch loss = 0.63 (31.9 examples/sec; 0.251 sec/batch; 23h:00m:28s remains)
INFO - root - 2017-12-11 03:06:17.878155: step 2200, loss = 0.70, batch loss = 0.64 (30.8 examples/sec; 0.260 sec/batch; 23h:48m:43s remains)
2017-12-11 03:06:18.336126: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.076982744 0.071607023 0.067973211 0.06379316 0.057991769 0.051579934 0.047429185 0.045326214 0.0455167 0.047296204 0.049649995 0.051710088 0.0543493 0.0591719 0.064556189][0.0738273 0.0685706 0.064867862 0.060628574 0.054486398 0.047531132 0.042565838 0.039940875 0.039499894 0.040479433 0.042426892 0.044614192 0.047852252 0.053739037 0.060704324][0.060999572 0.060304515 0.061127964 0.060375776 0.05638691 0.050611604 0.046252411 0.044541594 0.04409932 0.043524835 0.042970162 0.042208631 0.041714944 0.043172546 0.046230935][0.058485687 0.0708435 0.0834487 0.090684511 0.091173396 0.087346904 0.0847233 0.086284831 0.087670259 0.085791923 0.080889463 0.073432058 0.062561333 0.049007412 0.036258273][0.081724413 0.11572097 0.14629209 0.1649249 0.17139675 0.16900134 0.1686641 0.17622899 0.18229042 0.18086076 0.17156544 0.15576077 0.1287896 0.089135177 0.046820678][0.12962797 0.18760496 0.23662895 0.26637903 0.27857876 0.27636239 0.27790943 0.29256356 0.30546057 0.30695182 0.29539278 0.27289835 0.22882396 0.15828355 0.079354592][0.18216096 0.25694588 0.3171297 0.35272068 0.36760074 0.36363739 0.36553097 0.38558504 0.40560469 0.412435 0.40271193 0.37860149 0.3232322 0.22842214 0.11916255][0.20789534 0.28443873 0.34269643 0.37517592 0.38755709 0.38011372 0.38027939 0.40196377 0.42684397 0.43971413 0.43559134 0.41610688 0.36039656 0.25816765 0.13791433][0.18267675 0.2449495 0.28916946 0.31109765 0.31706417 0.30609506 0.30316877 0.32181782 0.34682524 0.36293539 0.3645469 0.35298154 0.30781496 0.21859108 0.11223935][0.10963537 0.14830524 0.17332523 0.18273535 0.18215245 0.16995955 0.1645731 0.17700328 0.19694304 0.21134622 0.21534184 0.210552 0.18134516 0.11931388 0.045243479][0.021236256 0.036722451 0.045154121 0.045085117 0.040585671 0.029935323 0.02377723 0.029396487 0.041147783 0.049823124 0.052198578 0.05048633 0.0358435 0.0031641989 -0.034799531][-0.047927853 -0.048348147 -0.049663413 -0.054048732 -0.059269883 -0.066848919 -0.072346441 -0.072081037 -0.068632387 -0.067003213 -0.068356104 -0.069924384 -0.074889272 -0.085522294 -0.095901653][-0.082409821 -0.089845479 -0.094340019 -0.099013463 -0.1029771 -0.10735116 -0.11134931 -0.11397284 -0.11631502 -0.12017845 -0.12499306 -0.12768207 -0.12813404 -0.12703139 -0.12272145][-0.085855037 -0.093829885 -0.097738564 -0.10105843 -0.10283502 -0.10348215 -0.10403351 -0.10547808 -0.10865435 -0.11419429 -0.12102103 -0.1259677 -0.12694825 -0.12370789 -0.11640238][-0.072847091 -0.078629106 -0.081127204 -0.08309266 -0.081386968 -0.075412817 -0.06773226 -0.06147901 -0.058297705 -0.059934504 -0.066949159 -0.076484554 -0.0840222 -0.0874083 -0.086364992]]...]
INFO - root - 2017-12-11 03:06:21.098233: step 2210, loss = 0.72, batch loss = 0.66 (28.2 examples/sec; 0.284 sec/batch; 26h:03m:21s remains)
INFO - root - 2017-12-11 03:06:23.782828: step 2220, loss = 0.69, batch loss = 0.64 (29.9 examples/sec; 0.268 sec/batch; 24h:32m:42s remains)
INFO - root - 2017-12-11 03:06:26.458375: step 2230, loss = 0.70, batch loss = 0.64 (30.3 examples/sec; 0.264 sec/batch; 24h:11m:10s remains)
INFO - root - 2017-12-11 03:06:29.094248: step 2240, loss = 0.69, batch loss = 0.64 (31.2 examples/sec; 0.256 sec/batch; 23h:29m:14s remains)
INFO - root - 2017-12-11 03:06:31.728380: step 2250, loss = 0.69, batch loss = 0.63 (30.1 examples/sec; 0.266 sec/batch; 24h:22m:20s remains)
INFO - root - 2017-12-11 03:06:34.402267: step 2260, loss = 0.71, batch loss = 0.65 (29.9 examples/sec; 0.268 sec/batch; 24h:32m:52s remains)
INFO - root - 2017-12-11 03:06:37.103365: step 2270, loss = 0.70, batch loss = 0.64 (29.7 examples/sec; 0.269 sec/batch; 24h:42m:44s remains)
INFO - root - 2017-12-11 03:06:39.756149: step 2280, loss = 0.71, batch loss = 0.65 (30.5 examples/sec; 0.262 sec/batch; 24h:03m:55s remains)
INFO - root - 2017-12-11 03:06:42.407747: step 2290, loss = 0.70, batch loss = 0.64 (30.6 examples/sec; 0.262 sec/batch; 24h:00m:46s remains)
INFO - root - 2017-12-11 03:06:45.067753: step 2300, loss = 0.69, batch loss = 0.63 (29.5 examples/sec; 0.272 sec/batch; 24h:54m:14s remains)
2017-12-11 03:06:45.548721: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.150515 0.16830671 0.19555198 0.22725613 0.2675069 0.32505029 0.39470804 0.44586426 0.46874067 0.47270268 0.47022754 0.45436892 0.42063004 0.37778267 0.32971796][0.19791839 0.22047219 0.25185531 0.28737849 0.33193445 0.3951669 0.47000283 0.5234524 0.54505837 0.54390472 0.53203374 0.50206435 0.45226321 0.39455116 0.33373719][0.21476057 0.24583675 0.28476843 0.32551205 0.37207583 0.43474039 0.50561005 0.55388319 0.56879896 0.55833435 0.53339112 0.48801494 0.42341578 0.3530049 0.28330857][0.20982866 0.25093111 0.29940322 0.3459428 0.39249253 0.44915628 0.50906438 0.54676348 0.55306572 0.53497839 0.50131959 0.44628057 0.37182596 0.29211229 0.21651846][0.20634693 0.25346422 0.30911049 0.36054334 0.40530014 0.4523589 0.49714467 0.52128875 0.51904666 0.49700406 0.46027243 0.40060458 0.31954268 0.23263694 0.15316227][0.22741617 0.27387789 0.33085462 0.38344556 0.42492288 0.46201438 0.49245647 0.50470936 0.49565011 0.47075889 0.43115932 0.36668596 0.27948952 0.18624307 0.10354161][0.2801083 0.31708935 0.3645536 0.41051552 0.44621223 0.47595823 0.49789375 0.50447375 0.49239856 0.46517882 0.4203364 0.34848481 0.25438625 0.15569177 0.070792429][0.34584695 0.36665297 0.39564455 0.42719442 0.45436916 0.47919071 0.497802 0.50224751 0.48813081 0.45721582 0.40438607 0.32386667 0.2249158 0.12598759 0.043696139][0.39847896 0.40112466 0.40761369 0.4203366 0.43667549 0.45727828 0.47556111 0.4803293 0.46550924 0.43091115 0.37043136 0.28428674 0.18598275 0.093821041 0.019787759][0.41912258 0.40746558 0.39553782 0.39201686 0.39840031 0.4152073 0.43328434 0.43757069 0.42069241 0.38183713 0.31573129 0.22833562 0.13665929 0.057351604 -0.0035392153][0.40240458 0.38752839 0.36883271 0.35816371 0.35906464 0.37202895 0.38592678 0.38400406 0.35966837 0.31373483 0.2432784 0.15814206 0.077287428 0.014466996 -0.029436616][0.36262167 0.35384065 0.33764368 0.32739216 0.32574803 0.33307016 0.33789584 0.32453653 0.28906918 0.23583218 0.16458277 0.086303577 0.019235844 -0.025856057 -0.051565066][0.32226136 0.32066971 0.30929044 0.30079651 0.29531041 0.29294294 0.2840952 0.25674805 0.2111268 0.15465425 0.088853262 0.023293549 -0.027165361 -0.054836232 -0.064545706][0.29357392 0.29430416 0.28390968 0.27399647 0.261739 0.24633667 0.22213908 0.18270533 0.13232307 0.079612054 0.026537057 -0.020752663 -0.052872013 -0.065266617 -0.063587233][0.26829526 0.26675034 0.25359944 0.23917629 0.2182363 0.18929087 0.15206562 0.10650173 0.059031121 0.017020563 -0.018395497 -0.045120787 -0.05944575 -0.059972033 -0.051929135]]...]
INFO - root - 2017-12-11 03:06:48.237019: step 2310, loss = 0.70, batch loss = 0.64 (29.0 examples/sec; 0.276 sec/batch; 25h:18m:28s remains)
INFO - root - 2017-12-11 03:06:50.884293: step 2320, loss = 0.71, batch loss = 0.65 (29.7 examples/sec; 0.270 sec/batch; 24h:43m:51s remains)
INFO - root - 2017-12-11 03:06:53.590527: step 2330, loss = 0.68, batch loss = 0.63 (29.1 examples/sec; 0.275 sec/batch; 25h:10m:41s remains)
INFO - root - 2017-12-11 03:06:56.225783: step 2340, loss = 0.69, batch loss = 0.63 (31.0 examples/sec; 0.258 sec/batch; 23h:39m:16s remains)
INFO - root - 2017-12-11 03:06:58.891549: step 2350, loss = 0.69, batch loss = 0.64 (31.5 examples/sec; 0.254 sec/batch; 23h:15m:46s remains)
INFO - root - 2017-12-11 03:07:01.596495: step 2360, loss = 0.71, batch loss = 0.65 (30.8 examples/sec; 0.260 sec/batch; 23h:51m:08s remains)
INFO - root - 2017-12-11 03:07:04.248529: step 2370, loss = 0.69, batch loss = 0.64 (29.8 examples/sec; 0.268 sec/batch; 24h:37m:18s remains)
INFO - root - 2017-12-11 03:07:06.884656: step 2380, loss = 0.70, batch loss = 0.64 (31.3 examples/sec; 0.256 sec/batch; 23h:28m:10s remains)
INFO - root - 2017-12-11 03:07:09.569472: step 2390, loss = 0.70, batch loss = 0.64 (29.6 examples/sec; 0.270 sec/batch; 24h:44m:29s remains)
INFO - root - 2017-12-11 03:07:12.224176: step 2400, loss = 0.68, batch loss = 0.62 (29.1 examples/sec; 0.275 sec/batch; 25h:11m:42s remains)
2017-12-11 03:07:12.674423: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.36981013 0.33436903 0.27844697 0.22689933 0.1941811 0.19424783 0.22062287 0.25515106 0.27896833 0.27632356 0.24436395 0.18281755 0.10601049 0.035692446 -0.014620744][0.38017264 0.34339187 0.28837919 0.2425293 0.21620533 0.22016412 0.24689057 0.27858198 0.29710758 0.28752211 0.24926254 0.18407424 0.10608716 0.035447307 -0.014894578][0.37279648 0.34230769 0.29878163 0.26766178 0.25422561 0.26347694 0.2870492 0.30978152 0.31672236 0.29644367 0.25074312 0.18278819 0.10561994 0.0362332 -0.013386551][0.37842575 0.35995767 0.33472571 0.32336289 0.3256833 0.34166506 0.36117741 0.37168178 0.36171836 0.32479998 0.26679945 0.19214356 0.1119521 0.040263858 -0.011242875][0.38876265 0.38543117 0.38096413 0.39029658 0.40829718 0.43250716 0.450748 0.4518522 0.42500368 0.36873439 0.29509017 0.210525 0.12416634 0.047396075 -0.0077387854][0.38710457 0.39746052 0.41132975 0.43872383 0.470527 0.50464386 0.5274815 0.52731836 0.49036181 0.41830692 0.33058724 0.23572391 0.14158271 0.058079686 -0.0012632904][0.36067683 0.37958282 0.40439248 0.44220528 0.4812029 0.52105421 0.54902446 0.55228311 0.51379257 0.43626055 0.3444595 0.24700832 0.15054373 0.064746678 0.0043938067][0.30765849 0.32640669 0.351401 0.38840443 0.42463431 0.46178395 0.48939246 0.49492964 0.46090916 0.38966569 0.30712813 0.21969184 0.13242264 0.05456255 0.00053908542][0.22219227 0.23470311 0.25321689 0.28248891 0.31066406 0.34023058 0.36389196 0.3709836 0.3453072 0.28824034 0.22340479 0.15482865 0.085817158 0.02450593 -0.016601922][0.11289608 0.11784106 0.12865131 0.14890787 0.16873771 0.19062705 0.21000449 0.21853729 0.20229812 0.16162921 0.11635999 0.069066942 0.022004902 -0.018455613 -0.043030962][0.01356598 0.012034616 0.016408695 0.028693503 0.04174757 0.057391796 0.072762623 0.081578262 0.073226169 0.04726208 0.018987892 -0.0096605523 -0.036736913 -0.057703 -0.066920124][-0.048375659 -0.0538979 -0.054447841 -0.04912331 -0.042150706 -0.032728631 -0.02260478 -0.015795933 -0.018858647 -0.032527827 -0.046849128 -0.060618497 -0.072287008 -0.078868777 -0.077875935][-0.076510154 -0.083410166 -0.086251557 -0.08539214 -0.082805932 -0.0784009 -0.073026769 -0.068769068 -0.068988137 -0.074257217 -0.079264924 -0.083404772 -0.085590534 -0.084043041 -0.078249924][-0.0843355 -0.090276487 -0.092493184 -0.092764594 -0.091806859 -0.089734592 -0.086874589 -0.084316745 -0.083700754 -0.085290484 -0.086520314 -0.086662829 -0.084834278 -0.080217846 -0.073347852][-0.07631667 -0.0805625 -0.081336215 -0.081230804 -0.080490775 -0.079173185 -0.077504858 -0.076119877 -0.075813591 -0.0766122 -0.077191308 -0.076653413 -0.074281953 -0.069961 -0.064517573]]...]
INFO - root - 2017-12-11 03:07:15.317220: step 2410, loss = 0.69, batch loss = 0.64 (30.5 examples/sec; 0.262 sec/batch; 24h:02m:09s remains)
INFO - root - 2017-12-11 03:07:17.954386: step 2420, loss = 0.70, batch loss = 0.64 (28.6 examples/sec; 0.279 sec/batch; 25h:36m:47s remains)
INFO - root - 2017-12-11 03:07:20.590664: step 2430, loss = 0.71, batch loss = 0.65 (30.0 examples/sec; 0.266 sec/batch; 24h:24m:49s remains)
INFO - root - 2017-12-11 03:07:23.271258: step 2440, loss = 0.70, batch loss = 0.64 (30.3 examples/sec; 0.264 sec/batch; 24h:13m:24s remains)
INFO - root - 2017-12-11 03:07:25.972741: step 2450, loss = 0.69, batch loss = 0.63 (29.6 examples/sec; 0.270 sec/batch; 24h:46m:57s remains)
INFO - root - 2017-12-11 03:07:28.673892: step 2460, loss = 0.69, batch loss = 0.63 (27.5 examples/sec; 0.291 sec/batch; 26h:40m:47s remains)
INFO - root - 2017-12-11 03:07:31.374183: step 2470, loss = 0.71, batch loss = 0.65 (30.6 examples/sec; 0.262 sec/batch; 23h:59m:12s remains)
INFO - root - 2017-12-11 03:07:34.026111: step 2480, loss = 0.70, batch loss = 0.64 (30.5 examples/sec; 0.263 sec/batch; 24h:05m:00s remains)
INFO - root - 2017-12-11 03:07:36.743691: step 2490, loss = 0.71, batch loss = 0.65 (29.6 examples/sec; 0.270 sec/batch; 24h:46m:58s remains)
INFO - root - 2017-12-11 03:07:39.402449: step 2500, loss = 0.69, batch loss = 0.63 (31.1 examples/sec; 0.257 sec/batch; 23h:34m:35s remains)
2017-12-11 03:07:39.816891: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.032617778 -0.031513505 -0.029123275 -0.02918121 -0.030780431 -0.032753129 -0.036490578 -0.047653779 -0.062287051 -0.071117245 -0.073185928 -0.07237491 -0.069517322 -0.062360495 -0.057345442][0.012482712 0.021497536 0.033808433 0.04524041 0.054446537 0.060463771 0.060508512 0.044716544 0.018522779 -0.0015664783 -0.011871178 -0.017329883 -0.018626155 -0.012193551 -0.009030439][0.085014634 0.10686478 0.13311738 0.16041116 0.18500096 0.20419051 0.21274903 0.19441032 0.15527394 0.11896764 0.095921725 0.080170617 0.070489213 0.071113341 0.067573659][0.17155607 0.2127334 0.25827911 0.30429915 0.3445392 0.37609407 0.3931821 0.37208322 0.31864107 0.26259798 0.2224775 0.1920746 0.16763432 0.1556841 0.14033435][0.25036183 0.31626761 0.38512877 0.45214519 0.50753373 0.54809344 0.56970942 0.54380709 0.47568342 0.39857256 0.33853245 0.28951257 0.24570075 0.21545854 0.18488789][0.3211695 0.41356575 0.50451779 0.59035575 0.65864855 0.70548248 0.72818828 0.69793308 0.61989629 0.52694327 0.44924748 0.38148785 0.31625274 0.26410988 0.2155029][0.38659403 0.50543791 0.61529887 0.71495277 0.79107231 0.84086448 0.86153561 0.82759291 0.7459451 0.64786249 0.56286496 0.48388931 0.40123853 0.32753959 0.25920081][0.43652639 0.57656229 0.69885111 0.80439717 0.88013494 0.9253391 0.93976742 0.904483 0.83093792 0.74540144 0.66943216 0.59051442 0.49591333 0.40200746 0.3110652][0.46443683 0.61353457 0.73602456 0.83558095 0.89964569 0.92871743 0.92894679 0.89381754 0.84049881 0.78575808 0.73495233 0.66850662 0.57186151 0.4644706 0.35200831][0.45589367 0.59510177 0.7016421 0.78325433 0.82867521 0.83725238 0.82115388 0.78827459 0.760399 0.742927 0.72417247 0.6791597 0.59354007 0.48744589 0.36669204][0.41141197 0.52512687 0.60348332 0.65776861 0.68032527 0.66958141 0.64100623 0.61239529 0.60740888 0.62229872 0.6326161 0.61113662 0.54469919 0.45325315 0.34029621][0.31930417 0.39890218 0.445418 0.47064394 0.47077322 0.44425842 0.40839589 0.38478494 0.39573294 0.43139571 0.46156397 0.45962343 0.41465491 0.3448844 0.25168234][0.18474022 0.227445 0.24504216 0.24646735 0.22989285 0.19372471 0.15628245 0.13871002 0.15825745 0.20110364 0.2387258 0.24926756 0.22467601 0.17925805 0.11282379][0.051393196 0.063044608 0.059368774 0.045379329 0.020303676 -0.017607307 -0.051734794 -0.064638264 -0.046162464 -0.010113762 0.022290913 0.037204929 0.029317932 0.0075816121 -0.029215328][-0.051503133 -0.059311334 -0.073612213 -0.092917979 -0.117107 -0.14781703 -0.173315 -0.18266323 -0.17182453 -0.15065619 -0.13100381 -0.11912378 -0.11785333 -0.1224386 -0.13407567]]...]
INFO - root - 2017-12-11 03:07:42.506640: step 2510, loss = 0.69, batch loss = 0.63 (30.7 examples/sec; 0.261 sec/batch; 23h:54m:17s remains)
INFO - root - 2017-12-11 03:07:45.187012: step 2520, loss = 0.69, batch loss = 0.63 (26.6 examples/sec; 0.301 sec/batch; 27h:35m:20s remains)
INFO - root - 2017-12-11 03:07:47.849431: step 2530, loss = 0.70, batch loss = 0.64 (31.2 examples/sec; 0.256 sec/batch; 23h:29m:11s remains)
INFO - root - 2017-12-11 03:07:50.485405: step 2540, loss = 0.68, batch loss = 0.62 (30.4 examples/sec; 0.263 sec/batch; 24h:05m:54s remains)
INFO - root - 2017-12-11 03:07:53.173663: step 2550, loss = 0.69, batch loss = 0.64 (31.1 examples/sec; 0.257 sec/batch; 23h:35m:17s remains)
INFO - root - 2017-12-11 03:07:55.819575: step 2560, loss = 0.69, batch loss = 0.64 (30.5 examples/sec; 0.263 sec/batch; 24h:04m:24s remains)
INFO - root - 2017-12-11 03:07:58.466182: step 2570, loss = 0.69, batch loss = 0.63 (29.8 examples/sec; 0.269 sec/batch; 24h:37m:19s remains)
INFO - root - 2017-12-11 03:08:01.094531: step 2580, loss = 0.68, batch loss = 0.63 (29.6 examples/sec; 0.271 sec/batch; 24h:48m:27s remains)
INFO - root - 2017-12-11 03:08:03.724547: step 2590, loss = 0.69, batch loss = 0.63 (30.2 examples/sec; 0.265 sec/batch; 24h:16m:28s remains)
INFO - root - 2017-12-11 03:08:06.393529: step 2600, loss = 0.68, batch loss = 0.63 (30.5 examples/sec; 0.263 sec/batch; 24h:04m:18s remains)
2017-12-11 03:08:06.831902: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.017018929 0.044713 0.072979085 0.095492974 0.11047098 0.12059442 0.1272129 0.13145371 0.1300488 0.12166794 0.11000226 0.10862853 0.120536 0.13866976 0.15746015][0.0028906937 0.028694345 0.058842033 0.085460871 0.10573293 0.12218505 0.13618711 0.14750852 0.15077005 0.14488176 0.13348834 0.1301081 0.13910441 0.15609348 0.17848887][-0.0028915864 0.01839087 0.049219448 0.080756851 0.10848093 0.13336655 0.15542345 0.17232475 0.17654745 0.16839488 0.15276764 0.14340177 0.14571874 0.15795706 0.18122293][0.004958008 0.024596021 0.058411755 0.097431436 0.13510172 0.16972606 0.19855522 0.21660066 0.21581455 0.19913472 0.17423442 0.15458713 0.14605075 0.14898367 0.16808149][0.035480592 0.060819164 0.1014545 0.14910926 0.19630305 0.23845619 0.26862586 0.27971181 0.26593745 0.23413165 0.1953831 0.16257502 0.14205576 0.1350771 0.1481149][0.084223576 0.121787 0.17170805 0.22636196 0.27890053 0.3222366 0.34574831 0.34233966 0.31104058 0.26297453 0.21190169 0.16966465 0.14273731 0.13126785 0.14069319][0.14208372 0.19256935 0.24937487 0.30551943 0.35575497 0.39214167 0.40240222 0.38174057 0.33438137 0.27460048 0.2165641 0.17066479 0.14380619 0.13472512 0.145486][0.19797309 0.25581631 0.31278652 0.36277586 0.40273768 0.42614695 0.42119852 0.38623825 0.32954684 0.26645434 0.20924616 0.16650091 0.1462086 0.14543258 0.16173357][0.23429079 0.29127419 0.34089449 0.37736464 0.40014786 0.40642715 0.38759512 0.34524372 0.28922981 0.23320588 0.18559714 0.152498 0.14294256 0.15244904 0.17492652][0.23976518 0.2878432 0.32417268 0.3428151 0.34589198 0.33536056 0.3072187 0.26535895 0.21854544 0.17713135 0.14483353 0.12522824 0.1275962 0.14636911 0.17358723][0.20438451 0.2376404 0.25877252 0.26181614 0.25054204 0.23035215 0.20091298 0.16688581 0.13435596 0.11066561 0.095482349 0.090174787 0.10273246 0.12707913 0.15512997][0.13788064 0.1548747 0.16302763 0.15659636 0.13984892 0.11946724 0.096587606 0.074715592 0.056784891 0.047541529 0.044466466 0.047924493 0.064790659 0.089345679 0.11451986][0.059380602 0.062175032 0.061321497 0.051581182 0.036905389 0.02316292 0.011023675 0.0016790422 -0.0045883032 -0.0048471238 -0.0015055343 0.0060749217 0.023815744 0.046446543 0.067746833][-0.014379613 -0.019922225 -0.024285428 -0.032536093 -0.041595645 -0.047522988 -0.050457336 -0.051011324 -0.050212312 -0.046225436 -0.040282764 -0.031419635 -0.015234556 0.0037042629 0.020225102][-0.068827346 -0.075028189 -0.077288866 -0.0809105 -0.083962694 -0.084241807 -0.082272649 -0.079231493 -0.076312974 -0.072311372 -0.067248486 -0.060071897 -0.048061293 -0.034521427 -0.023349104]]...]
INFO - root - 2017-12-11 03:08:09.494816: step 2610, loss = 0.69, batch loss = 0.63 (30.7 examples/sec; 0.261 sec/batch; 23h:53m:57s remains)
INFO - root - 2017-12-11 03:08:12.149689: step 2620, loss = 0.70, batch loss = 0.64 (30.2 examples/sec; 0.265 sec/batch; 24h:17m:17s remains)
INFO - root - 2017-12-11 03:08:14.813804: step 2630, loss = 0.70, batch loss = 0.64 (30.0 examples/sec; 0.267 sec/batch; 24h:25m:49s remains)
INFO - root - 2017-12-11 03:08:17.457586: step 2640, loss = 0.71, batch loss = 0.65 (31.6 examples/sec; 0.253 sec/batch; 23h:11m:09s remains)
INFO - root - 2017-12-11 03:08:20.154831: step 2650, loss = 0.69, batch loss = 0.64 (28.9 examples/sec; 0.277 sec/batch; 25h:23m:09s remains)
INFO - root - 2017-12-11 03:08:22.809851: step 2660, loss = 0.71, batch loss = 0.66 (30.5 examples/sec; 0.263 sec/batch; 24h:03m:38s remains)
INFO - root - 2017-12-11 03:08:25.444253: step 2670, loss = 0.70, batch loss = 0.65 (29.7 examples/sec; 0.270 sec/batch; 24h:41m:40s remains)
INFO - root - 2017-12-11 03:08:28.104224: step 2680, loss = 0.70, batch loss = 0.64 (31.3 examples/sec; 0.256 sec/batch; 23h:25m:33s remains)
INFO - root - 2017-12-11 03:08:30.762467: step 2690, loss = 0.70, batch loss = 0.65 (30.1 examples/sec; 0.265 sec/batch; 24h:18m:45s remains)
INFO - root - 2017-12-11 03:08:33.396602: step 2700, loss = 0.68, batch loss = 0.62 (30.4 examples/sec; 0.263 sec/batch; 24h:04m:59s remains)
2017-12-11 03:08:33.816619: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.18535447 0.15591708 0.12124494 0.09235312 0.0729275 0.0620981 0.055904191 0.050468065 0.045303766 0.040631864 0.036639512 0.032965977 0.029750044 0.027864408 0.02821493][0.19111811 0.15712315 0.11937779 0.089783475 0.07114429 0.062519379 0.059245884 0.055818193 0.050631966 0.044875536 0.039103925 0.032820839 0.026703896 0.022552064 0.022253243][0.17715192 0.14263995 0.10681608 0.081468754 0.068411477 0.066295192 0.069618024 0.07065741 0.066475719 0.059375022 0.051034372 0.041271586 0.030778082 0.02266459 0.020373452][0.16095486 0.12916975 0.098590642 0.080047935 0.074477814 0.0810417 0.0936467 0.1014232 0.099612124 0.091950171 0.081404835 0.067949064 0.052542377 0.039702687 0.033911623][0.16051042 0.13362451 0.10888947 0.096220881 0.096455373 0.11044841 0.13187177 0.14633746 0.14729257 0.13979374 0.12787628 0.11197586 0.093075566 0.076910153 0.068017416][0.17796353 0.15643686 0.13613941 0.1264008 0.12873051 0.14657997 0.17436384 0.19425783 0.19805509 0.19182657 0.1799279 0.16363928 0.14380971 0.12659954 0.1159023][0.20184013 0.18584651 0.16925791 0.1601897 0.16119023 0.17887703 0.20915422 0.231883 0.23785414 0.23359339 0.22334132 0.20939466 0.19213875 0.17706561 0.16701801][0.21304469 0.20259254 0.19024636 0.18185586 0.1806079 0.19550042 0.22456388 0.24763502 0.25557417 0.25445104 0.24776027 0.23860829 0.22669549 0.21595307 0.20860903][0.20299324 0.19804536 0.19078431 0.18398449 0.18044741 0.1907593 0.2157689 0.23717456 0.24648517 0.24930245 0.24813466 0.24581198 0.24092248 0.23558274 0.23230706][0.17864908 0.17911831 0.17798801 0.17483716 0.17105386 0.17744938 0.1976693 0.21617518 0.22488844 0.2292821 0.23222023 0.23608112 0.23725121 0.23579443 0.23594056][0.15340188 0.15880628 0.16452633 0.16740136 0.16666508 0.17199516 0.18889052 0.20450346 0.21037884 0.21233927 0.21511939 0.2208773 0.22403681 0.22340822 0.22584][0.13946216 0.14764574 0.15847558 0.16728501 0.17094989 0.17725442 0.19259122 0.20632485 0.20882462 0.20584835 0.20445555 0.20668879 0.20635389 0.20263714 0.2052144][0.13949475 0.14708975 0.15926585 0.17111631 0.17772025 0.18478267 0.19914168 0.21205211 0.21257474 0.20525129 0.19829355 0.19364963 0.18590926 0.17594887 0.17577027][0.14354959 0.14892597 0.15908824 0.169643 0.17503685 0.17957698 0.19076012 0.2020662 0.20201655 0.19326606 0.18342717 0.17423423 0.16128871 0.14715257 0.14462993][0.14424549 0.14751352 0.15429698 0.16034602 0.16094023 0.15965495 0.16458805 0.17213745 0.1716999 0.16407329 0.15496656 0.14603049 0.13357191 0.12062234 0.11870556]]...]
INFO - root - 2017-12-11 03:08:36.485062: step 2710, loss = 0.72, batch loss = 0.66 (30.2 examples/sec; 0.265 sec/batch; 24h:16m:33s remains)
INFO - root - 2017-12-11 03:08:39.173108: step 2720, loss = 0.70, batch loss = 0.64 (30.1 examples/sec; 0.266 sec/batch; 24h:20m:20s remains)
INFO - root - 2017-12-11 03:08:41.836611: step 2730, loss = 0.69, batch loss = 0.63 (30.7 examples/sec; 0.261 sec/batch; 23h:51m:51s remains)
INFO - root - 2017-12-11 03:08:44.505650: step 2740, loss = 0.70, batch loss = 0.64 (30.8 examples/sec; 0.259 sec/batch; 23h:45m:33s remains)
INFO - root - 2017-12-11 03:08:47.164803: step 2750, loss = 0.69, batch loss = 0.64 (31.0 examples/sec; 0.258 sec/batch; 23h:40m:06s remains)
INFO - root - 2017-12-11 03:08:49.822256: step 2760, loss = 0.69, batch loss = 0.63 (30.6 examples/sec; 0.261 sec/batch; 23h:56m:05s remains)
INFO - root - 2017-12-11 03:08:52.479350: step 2770, loss = 0.70, batch loss = 0.64 (29.2 examples/sec; 0.274 sec/batch; 25h:06m:16s remains)
INFO - root - 2017-12-11 03:08:55.194591: step 2780, loss = 0.68, batch loss = 0.62 (29.7 examples/sec; 0.269 sec/batch; 24h:40m:45s remains)
INFO - root - 2017-12-11 03:08:57.861139: step 2790, loss = 0.69, batch loss = 0.63 (30.4 examples/sec; 0.263 sec/batch; 24h:05m:44s remains)
INFO - root - 2017-12-11 03:09:00.499122: step 2800, loss = 0.70, batch loss = 0.64 (30.4 examples/sec; 0.263 sec/batch; 24h:05m:29s remains)
2017-12-11 03:09:00.936722: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.19691032 0.21029924 0.22460538 0.23329508 0.23912823 0.25100103 0.27416065 0.30104157 0.31036457 0.28499854 0.22200419 0.13797078 0.056340385 -0.0055975 -0.043599583][0.22050579 0.23710603 0.25487185 0.26695 0.27598611 0.29098871 0.31643477 0.34384853 0.3516188 0.3226105 0.25353402 0.16142042 0.071604207 0.0033346217 -0.0391188][0.21372129 0.23177148 0.25172022 0.26800081 0.28238827 0.30259034 0.33041164 0.35624263 0.36041561 0.32825613 0.25725287 0.16417696 0.074505486 0.0071027032 -0.035212159][0.19609535 0.21014787 0.22834016 0.24828538 0.27064636 0.29984188 0.3316645 0.35354561 0.34819013 0.30661812 0.23112391 0.13977951 0.056968864 -0.0021039506 -0.038276248][0.18435834 0.19142492 0.20638384 0.2306833 0.2634275 0.30501229 0.34407508 0.36319381 0.34607059 0.29043785 0.20605069 0.11391924 0.03705107 -0.013832253 -0.043264687][0.18319838 0.18610606 0.20289725 0.23600066 0.28168836 0.33605111 0.38232356 0.39970839 0.3726784 0.30410528 0.20929155 0.11144853 0.033223584 -0.016604932 -0.044104211][0.18690354 0.19111112 0.21555844 0.26117975 0.31940183 0.38167435 0.42841187 0.43927109 0.40168738 0.3228727 0.22008207 0.11686423 0.03539028 -0.016127869 -0.04376423][0.20091017 0.20936611 0.2417732 0.29784411 0.36385515 0.42701179 0.46708164 0.46685749 0.4180513 0.33067554 0.22142553 0.11331868 0.029162988 -0.022743218 -0.049044885][0.24452907 0.26289871 0.30034426 0.35629177 0.41600472 0.46702594 0.49227807 0.47940522 0.4222317 0.33010411 0.2174498 0.10589714 0.019406084 -0.032983247 -0.057935216][0.30612063 0.34258875 0.38451302 0.43010339 0.46794567 0.49225685 0.49468246 0.46936187 0.40994132 0.32182282 0.21456584 0.10608417 0.019599892 -0.034768511 -0.061876178][0.37080324 0.42692322 0.47407275 0.50670445 0.5178073 0.51043731 0.48597977 0.44497666 0.38169575 0.29896882 0.20203699 0.10385874 0.02355819 -0.030204415 -0.06002707][0.44166028 0.50658381 0.55144095 0.56942809 0.55670607 0.52251184 0.47416261 0.4169153 0.34778261 0.269349 0.18385018 0.099308193 0.029044969 -0.021419572 -0.053209338][0.49628353 0.55382383 0.584361 0.58485782 0.55496258 0.50578952 0.44515681 0.37972528 0.30927297 0.23716694 0.16381638 0.093177073 0.033500291 -0.01234118 -0.0447697][0.50138205 0.543959 0.55467224 0.53684437 0.49460098 0.439445 0.37758502 0.31544265 0.25391167 0.1960026 0.13982978 0.0853015 0.036506847 -0.0044793626 -0.036640063][0.4354305 0.46253866 0.45834443 0.4305124 0.38567868 0.33450502 0.28165749 0.23260841 0.18839441 0.15036689 0.11421116 0.07660751 0.038645525 0.0023602524 -0.029369676]]...]
INFO - root - 2017-12-11 03:09:03.573129: step 2810, loss = 0.71, batch loss = 0.65 (30.1 examples/sec; 0.266 sec/batch; 24h:19m:17s remains)
INFO - root - 2017-12-11 03:09:06.216697: step 2820, loss = 0.70, batch loss = 0.64 (30.0 examples/sec; 0.267 sec/batch; 24h:26m:47s remains)
INFO - root - 2017-12-11 03:09:08.914564: step 2830, loss = 0.69, batch loss = 0.63 (30.3 examples/sec; 0.264 sec/batch; 24h:08m:58s remains)
INFO - root - 2017-12-11 03:09:11.612883: step 2840, loss = 0.69, batch loss = 0.63 (30.2 examples/sec; 0.265 sec/batch; 24h:15m:05s remains)
INFO - root - 2017-12-11 03:09:14.301224: step 2850, loss = 0.70, batch loss = 0.64 (29.7 examples/sec; 0.269 sec/batch; 24h:38m:07s remains)
INFO - root - 2017-12-11 03:09:16.977235: step 2860, loss = 0.69, batch loss = 0.63 (30.2 examples/sec; 0.265 sec/batch; 24h:16m:48s remains)
INFO - root - 2017-12-11 03:09:19.680742: step 2870, loss = 0.70, batch loss = 0.64 (30.6 examples/sec; 0.261 sec/batch; 23h:55m:07s remains)
INFO - root - 2017-12-11 03:09:22.371242: step 2880, loss = 0.70, batch loss = 0.64 (30.4 examples/sec; 0.263 sec/batch; 24h:04m:43s remains)
INFO - root - 2017-12-11 03:09:25.060652: step 2890, loss = 0.71, batch loss = 0.66 (31.9 examples/sec; 0.251 sec/batch; 22h:58m:03s remains)
INFO - root - 2017-12-11 03:09:27.718506: step 2900, loss = 0.69, batch loss = 0.63 (30.8 examples/sec; 0.260 sec/batch; 23h:45m:42s remains)
2017-12-11 03:09:28.130291: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.082685672 -0.089811429 -0.096171275 -0.1023452 -0.1073628 -0.11117148 -0.11502962 -0.12158622 -0.13273267 -0.14617755 -0.15647529 -0.15873289 -0.15069669 -0.13409367 -0.11337469][-0.08191517 -0.08737348 -0.091708034 -0.096163094 -0.10055351 -0.10523232 -0.11148633 -0.12044386 -0.1319423 -0.1433924 -0.15024514 -0.14889395 -0.13852826 -0.12209491 -0.10373429][-0.076603256 -0.075931251 -0.070594728 -0.061330494 -0.049396966 -0.038448431 -0.034132298 -0.03963859 -0.055004612 -0.07658989 -0.097473271 -0.1106744 -0.11267862 -0.10590786 -0.094460115][-0.066351958 -0.054616392 -0.031689018 0.0031412917 0.046135526 0.087500982 0.11299802 0.11393539 0.088257879 0.042021692 -0.011560434 -0.056974545 -0.084260143 -0.094160952 -0.09164954][-0.051770769 -0.025740946 0.020060616 0.088287167 0.17215239 0.254058 0.30816433 0.31749544 0.27674294 0.19595747 0.097870454 0.00957803 -0.050601106 -0.081416845 -0.08971905][-0.037548549 0.0026676562 0.071542084 0.17348155 0.29890141 0.42259678 0.50682396 0.52530205 0.46920925 0.35320413 0.21019523 0.078894548 -0.014345868 -0.066462055 -0.086183235][-0.0315307 0.017766962 0.10228357 0.22716495 0.38106582 0.53409249 0.64038473 0.66651136 0.60077733 0.46103135 0.28736895 0.12671354 0.010859826 -0.055878557 -0.0834383][-0.037715938 0.011057267 0.095965505 0.22187233 0.37766826 0.53388464 0.64399773 0.67290568 0.60767394 0.46615005 0.289353 0.12565166 0.0078704339 -0.059312046 -0.086300656][-0.046396997 -0.0076427045 0.060172886 0.16098946 0.28666443 0.4141607 0.50503111 0.52928162 0.47492847 0.35610858 0.20733134 0.070307165 -0.026205076 -0.078075513 -0.095134914][-0.029685434 -0.0034631006 0.035856217 0.091206349 0.16022415 0.23173945 0.28275189 0.2935597 0.25458068 0.17376356 0.073891975 -0.016065525 -0.075724855 -0.10252555 -0.10475618][0.040008157 0.060705788 0.070754752 0.07222724 0.070815913 0.070963018 0.06926392 0.058655076 0.032047532 -0.0098904762 -0.057820413 -0.097479254 -0.11856844 -0.1205776 -0.10959031][0.16840644 0.19729258 0.18707542 0.13914305 0.068476446 -0.0035588325 -0.060586575 -0.097754784 -0.12058795 -0.13546696 -0.14472476 -0.14657545 -0.13943145 -0.12468461 -0.10640449][0.33190286 0.38239655 0.36560458 0.28204304 0.15542166 0.022394707 -0.083682567 -0.14840324 -0.17672272 -0.18080398 -0.17129292 -0.15466766 -0.13501079 -0.11471247 -0.0962168][0.47896913 0.55470854 0.54210842 0.44046995 0.279423 0.10615753 -0.034319188 -0.12108399 -0.15888181 -0.16372736 -0.15123127 -0.13216414 -0.11315494 -0.096429221 -0.082955956][0.54995292 0.64014018 0.63387209 0.52960658 0.35876372 0.17226033 0.019472456 -0.076089241 -0.11899897 -0.12685 -0.11731574 -0.10258435 -0.089535318 -0.079477862 -0.072134]]...]
INFO - root - 2017-12-11 03:09:30.750899: step 2910, loss = 0.70, batch loss = 0.64 (31.4 examples/sec; 0.255 sec/batch; 23h:21m:12s remains)
INFO - root - 2017-12-11 03:09:33.449877: step 2920, loss = 0.69, batch loss = 0.63 (30.6 examples/sec; 0.261 sec/batch; 23h:53m:51s remains)
INFO - root - 2017-12-11 03:09:36.066694: step 2930, loss = 0.72, batch loss = 0.66 (29.6 examples/sec; 0.270 sec/batch; 24h:45m:48s remains)
INFO - root - 2017-12-11 03:09:38.812531: step 2940, loss = 0.69, batch loss = 0.63 (29.2 examples/sec; 0.274 sec/batch; 25h:05m:24s remains)
INFO - root - 2017-12-11 03:09:41.471350: step 2950, loss = 0.71, batch loss = 0.65 (31.1 examples/sec; 0.257 sec/batch; 23h:34m:00s remains)
INFO - root - 2017-12-11 03:09:44.178487: step 2960, loss = 0.74, batch loss = 0.68 (29.9 examples/sec; 0.267 sec/batch; 24h:29m:06s remains)
INFO - root - 2017-12-11 03:09:46.823934: step 2970, loss = 0.70, batch loss = 0.64 (30.6 examples/sec; 0.261 sec/batch; 23h:54m:15s remains)
INFO - root - 2017-12-11 03:09:49.467690: step 2980, loss = 0.69, batch loss = 0.64 (31.5 examples/sec; 0.254 sec/batch; 23h:12m:58s remains)
INFO - root - 2017-12-11 03:09:52.134650: step 2990, loss = 0.68, batch loss = 0.62 (31.3 examples/sec; 0.256 sec/batch; 23h:25m:45s remains)
INFO - root - 2017-12-11 03:09:54.776262: step 3000, loss = 0.70, batch loss = 0.64 (29.7 examples/sec; 0.269 sec/batch; 24h:38m:54s remains)
2017-12-11 03:09:55.195106: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.19799736 0.14295816 0.089867488 0.076740049 0.11771063 0.20057079 0.29589832 0.36816418 0.39839807 0.38605887 0.34086579 0.27727237 0.21157664 0.15286006 0.10328615][0.24265856 0.18204454 0.11797896 0.094644114 0.13349912 0.22214656 0.32660162 0.40582436 0.43735179 0.421777 0.37059146 0.3021161 0.2351103 0.17822041 0.13153842][0.28609762 0.22232299 0.14833046 0.11222016 0.14086521 0.22195035 0.32049906 0.3950845 0.42267439 0.40593606 0.35675839 0.29498389 0.23748939 0.18979897 0.14959486][0.33664015 0.27612486 0.20003144 0.15592517 0.17125423 0.2337193 0.3113713 0.36829397 0.38489926 0.36548659 0.32012057 0.26662946 0.21837342 0.17796968 0.1414151][0.38913813 0.3443912 0.28246748 0.24451284 0.25288397 0.29615635 0.34780231 0.38022134 0.37961453 0.35124233 0.30271071 0.24768901 0.19784285 0.15568043 0.11722886][0.42251283 0.41171712 0.38614103 0.37583345 0.39377955 0.43005055 0.4632203 0.47313938 0.45226815 0.40750676 0.34500247 0.27448609 0.21026948 0.15802255 0.11473196][0.41568062 0.44965327 0.47566432 0.50897729 0.55207813 0.59657961 0.62408966 0.61901945 0.57952946 0.51562685 0.43379611 0.34190327 0.25972971 0.19716008 0.15189137][0.35670105 0.42943695 0.50260323 0.5773285 0.64789647 0.70710492 0.7377283 0.72653234 0.67611045 0.60024041 0.50719249 0.40524319 0.31847981 0.25902718 0.22380276][0.2500141 0.34118944 0.43987474 0.53634322 0.62036514 0.68626034 0.71862429 0.70725989 0.65842509 0.58869016 0.50683 0.42052841 0.35360974 0.31785238 0.30781135][0.1183434 0.20099063 0.29407361 0.38341236 0.45868549 0.51644093 0.54543459 0.54021609 0.50920773 0.46932074 0.42692441 0.3859252 0.36488807 0.37344813 0.40103731][-0.005523514 0.049369402 0.11436587 0.17600147 0.22679283 0.26634791 0.28934273 0.295301 0.2925593 0.29679891 0.31090495 0.33184198 0.36968324 0.43096739 0.49813083][-0.090856113 -0.067092255 -0.035032481 -0.0053413738 0.01817595 0.038455613 0.056108821 0.074320309 0.099639408 0.14619367 0.21415779 0.29249787 0.38214126 0.48593897 0.58081359][-0.12794684 -0.12614289 -0.11806133 -0.11157402 -0.10780787 -0.1015164 -0.087696329 -0.06104159 -0.016799143 0.057551973 0.16065575 0.27517721 0.39451271 0.5178954 0.621335][-0.12880807 -0.13664989 -0.13985345 -0.14413548 -0.14951742 -0.15042123 -0.13974543 -0.11100284 -0.06012195 0.02322777 0.13639899 0.2590265 0.37962931 0.49477908 0.58529216][-0.1131335 -0.1223601 -0.12811729 -0.13452864 -0.14157237 -0.14504221 -0.138075 -0.11384577 -0.068111263 0.0068960232 0.1076813 0.21484552 0.3151603 0.40374345 0.46891519]]...]
INFO - root - 2017-12-11 03:09:57.798195: step 3010, loss = 0.68, batch loss = 0.63 (29.8 examples/sec; 0.269 sec/batch; 24h:36m:31s remains)
INFO - root - 2017-12-11 03:10:00.398347: step 3020, loss = 0.69, batch loss = 0.63 (30.3 examples/sec; 0.264 sec/batch; 24h:10m:02s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-SGD-lr0.001-clip50-initfromfixqian30k-fixqian
INFO - root - 2017-12-11 03:10:03.093445: step 3030, loss = 0.69, batch loss = 0.63 (32.1 examples/sec; 0.249 sec/batch; 22h:46m:50s remains)
INFO - root - 2017-12-11 03:10:05.717354: step 3040, loss = 0.69, batch loss = 0.64 (30.7 examples/sec; 0.261 sec/batch; 23h:51m:37s remains)
INFO - root - 2017-12-11 03:10:08.352423: step 3050, loss = 0.68, batch loss = 0.63 (29.9 examples/sec; 0.267 sec/batch; 24h:27m:20s remains)
INFO - root - 2017-12-11 03:10:10.975170: step 3060, loss = 0.70, batch loss = 0.64 (30.5 examples/sec; 0.262 sec/batch; 23h:59m:23s remains)
INFO - root - 2017-12-11 03:10:13.609533: step 3070, loss = 0.69, batch loss = 0.63 (30.1 examples/sec; 0.266 sec/batch; 24h:18m:17s remains)
INFO - root - 2017-12-11 03:10:16.245867: step 3080, loss = 0.72, batch loss = 0.66 (31.0 examples/sec; 0.258 sec/batch; 23h:35m:04s remains)
INFO - root - 2017-12-11 03:10:18.870400: step 3090, loss = 0.70, batch loss = 0.64 (30.2 examples/sec; 0.265 sec/batch; 24h:15m:29s remains)
INFO - root - 2017-12-11 03:10:21.584097: step 3100, loss = 0.70, batch loss = 0.64 (30.8 examples/sec; 0.259 sec/batch; 23h:44m:04s remains)
2017-12-11 03:10:22.096187: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.15576327 0.14762923 0.14634308 0.15368207 0.16524869 0.17841159 0.20002289 0.23081568 0.25993854 0.26800391 0.24599074 0.20894119 0.17358708 0.1532312 0.14834678][0.1606456 0.15203854 0.1488656 0.15237813 0.1592526 0.16903447 0.18856047 0.21683231 0.24445945 0.25334787 0.23455817 0.19960834 0.16225463 0.13687751 0.12775721][0.15783839 0.15259252 0.15036726 0.15211561 0.15675336 0.16713703 0.18797064 0.21560489 0.24121293 0.24981233 0.23386487 0.20002283 0.15826984 0.12335676 0.10476708][0.15418088 0.15952408 0.16432373 0.16864942 0.1747939 0.19044392 0.21667008 0.24602342 0.26946282 0.2753143 0.25796223 0.21975553 0.16899149 0.12113348 0.089885622][0.1545756 0.1770485 0.19410866 0.20453776 0.21495701 0.2381064 0.27050263 0.3014006 0.32136747 0.32164785 0.2986986 0.25267154 0.19152232 0.13097826 0.087077856][0.16123015 0.20325385 0.23467469 0.25320852 0.26951334 0.30003053 0.33660004 0.36637527 0.38034686 0.37209558 0.340164 0.28511372 0.214989 0.14505266 0.091340609][0.16689542 0.22467342 0.26840329 0.29433528 0.31548324 0.35022539 0.3866277 0.41085014 0.41572958 0.39697564 0.35525563 0.29330403 0.21906349 0.14658535 0.090270273][0.16089013 0.22352441 0.27097318 0.29819107 0.31842926 0.35000509 0.37954035 0.39358193 0.38774344 0.36025885 0.31411842 0.2543444 0.18703318 0.12378124 0.076012217][0.13869703 0.19251443 0.23177738 0.25159672 0.26416138 0.28559741 0.30372962 0.30703416 0.29372174 0.26413789 0.22307467 0.17688602 0.12806889 0.084554978 0.054375418][0.10385479 0.14090897 0.16610685 0.17554079 0.17935087 0.189969 0.19776456 0.19341148 0.1771356 0.15103069 0.12100326 0.093484379 0.067420304 0.046881255 0.036692545][0.067749567 0.089188524 0.10283924 0.10570617 0.10498793 0.10934962 0.11192042 0.10419409 0.087728962 0.066774964 0.047631029 0.035425935 0.027326368 0.024693618 0.029579893][0.039535634 0.051693805 0.06052782 0.062912449 0.062989645 0.066627316 0.069729924 0.063367158 0.049409114 0.033735726 0.022117581 0.017665723 0.017734738 0.022703726 0.032614291][0.026020352 0.035932329 0.04602344 0.052122243 0.055879939 0.06172597 0.068391129 0.066300213 0.056564212 0.044965692 0.036106039 0.031783435 0.031157486 0.03453194 0.040674668][0.031156654 0.043989982 0.058772951 0.069989085 0.077358186 0.084857211 0.093982689 0.095229514 0.088600248 0.078170322 0.067183807 0.05754799 0.050984751 0.048282117 0.047379229][0.052334238 0.069137305 0.087778695 0.10214645 0.11016183 0.11576477 0.1233307 0.124768 0.11911182 0.10798461 0.093380928 0.07811556 0.066063009 0.058272135 0.051885873]]...]
INFO - root - 2017-12-11 03:10:24.811996: step 3110, loss = 0.69, batch loss = 0.63 (29.4 examples/sec; 0.272 sec/batch; 24h:55m:50s remains)
INFO - root - 2017-12-11 03:10:27.430899: step 3120, loss = 0.69, batch loss = 0.64 (30.1 examples/sec; 0.266 sec/batch; 24h:21m:11s remains)
INFO - root - 2017-12-11 03:10:30.084404: step 3130, loss = 0.72, batch loss = 0.66 (31.0 examples/sec; 0.258 sec/batch; 23h:35m:45s remains)
INFO - root - 2017-12-11 03:10:32.788042: step 3140, loss = 0.71, batch loss = 0.65 (30.8 examples/sec; 0.260 sec/batch; 23h:47m:44s remains)
INFO - root - 2017-12-11 03:10:35.406929: step 3150, loss = 0.69, batch loss = 0.63 (29.7 examples/sec; 0.269 sec/batch; 24h:37m:20s remains)
INFO - root - 2017-12-11 03:10:38.082450: step 3160, loss = 0.69, batch loss = 0.63 (31.3 examples/sec; 0.255 sec/batch; 23h:20m:59s remains)
INFO - root - 2017-12-11 03:10:40.758597: step 3170, loss = 0.71, batch loss = 0.65 (29.1 examples/sec; 0.275 sec/batch; 25h:11m:05s remains)
INFO - root - 2017-12-11 03:10:43.425793: step 3180, loss = 0.70, batch loss = 0.64 (30.9 examples/sec; 0.259 sec/batch; 23h:41m:35s remains)
INFO - root - 2017-12-11 03:10:46.116068: step 3190, loss = 0.70, batch loss = 0.64 (27.8 examples/sec; 0.288 sec/batch; 26h:19m:45s remains)
INFO - root - 2017-12-11 03:10:48.760111: step 3200, loss = 0.69, batch loss = 0.63 (30.8 examples/sec; 0.260 sec/batch; 23h:46m:56s remains)
2017-12-11 03:10:49.211122: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.036255 -0.034049414 -0.033875085 -0.038891207 -0.048345271 -0.058508571 -0.060788814 -0.058073092 -0.057114277 -0.057434976 -0.057568952 -0.060843337 -0.07534495 -0.094386406 -0.10820878][0.02198736 0.034348007 0.041568082 0.040333107 0.029683232 0.015981928 0.014774468 0.019870436 0.019779813 0.015441353 0.011154239 -0.00019065 -0.031355605 -0.069593541 -0.0973832][0.10186635 0.13311101 0.15580469 0.1655533 0.15951222 0.14832428 0.15321387 0.16269621 0.16070658 0.14887 0.13399245 0.10388177 0.043035928 -0.025859248 -0.07511469][0.19238 0.25107479 0.29790395 0.32607964 0.33082533 0.32834333 0.34328827 0.35820231 0.35360634 0.33217937 0.30191079 0.24329014 0.14134391 0.031840336 -0.045494009][0.30037016 0.39381069 0.47050679 0.52071738 0.53696132 0.54395694 0.57010221 0.5925414 0.58828253 0.55823624 0.51001489 0.41397834 0.25952613 0.0991428 -0.012719452][0.41094276 0.54007214 0.64731532 0.71907091 0.74557292 0.76152796 0.79927355 0.83090764 0.82920045 0.79112029 0.72213411 0.58282286 0.37048122 0.15691656 0.011523468][0.48972195 0.6451565 0.77404314 0.86091727 0.8960613 0.92162 0.97249603 1.0161716 1.0213147 0.97922826 0.89155239 0.71342152 0.45168111 0.19549 0.024967011][0.51606166 0.67718107 0.8088792 0.89578521 0.93150437 0.962119 1.0220399 1.0749151 1.0865163 1.0442786 0.94715381 0.74984878 0.46672714 0.19587159 0.019767793][0.48260608 0.62679541 0.73998153 0.80920172 0.83319324 0.857741 0.91390204 0.96496242 0.97787637 0.93897831 0.84684587 0.65997356 0.39574814 0.14766212 -0.0092866216][0.39118934 0.50188929 0.58227479 0.6230337 0.62795192 0.63806546 0.67885369 0.717548 0.72629642 0.69339389 0.61829704 0.46655104 0.25381508 0.058149736 -0.060194958][0.2487971 0.31768557 0.36121589 0.37337515 0.36135641 0.35698274 0.378598 0.40053841 0.40288112 0.37777156 0.32539809 0.22082457 0.075534493 -0.053233676 -0.12361957][0.089924842 0.11955737 0.13256329 0.12441055 0.10275871 0.08893206 0.094191819 0.10134119 0.0986672 0.082089692 0.052309532 -0.0058899713 -0.0855602 -0.15066831 -0.17689414][-0.032525003 -0.029808188 -0.035179548 -0.053399734 -0.078320548 -0.097164162 -0.10395718 -0.10777719 -0.11372599 -0.12293671 -0.13485333 -0.15739259 -0.18720675 -0.2055966 -0.20153601][-0.10334785 -0.11362661 -0.12568405 -0.14477453 -0.16699626 -0.1854445 -0.19727503 -0.20573197 -0.21197791 -0.21592344 -0.21744004 -0.21988577 -0.2217392 -0.21538723 -0.19774395][-0.13497667 -0.14873908 -0.15959094 -0.17311853 -0.18753952 -0.19997068 -0.20941523 -0.21630439 -0.22047988 -0.22178692 -0.22019434 -0.21624568 -0.20890199 -0.19581473 -0.17734574]]...]
INFO - root - 2017-12-11 03:10:51.860717: step 3210, loss = 0.68, batch loss = 0.63 (29.8 examples/sec; 0.269 sec/batch; 24h:34m:06s remains)
INFO - root - 2017-12-11 03:10:54.499300: step 3220, loss = 0.70, batch loss = 0.64 (30.6 examples/sec; 0.262 sec/batch; 23h:56m:32s remains)
INFO - root - 2017-12-11 03:10:57.161055: step 3230, loss = 0.69, batch loss = 0.63 (29.9 examples/sec; 0.267 sec/batch; 24h:26m:31s remains)
INFO - root - 2017-12-11 03:10:59.823982: step 3240, loss = 0.69, batch loss = 0.64 (29.7 examples/sec; 0.269 sec/batch; 24h:36m:17s remains)
INFO - root - 2017-12-11 03:11:02.478665: step 3250, loss = 0.70, batch loss = 0.64 (30.3 examples/sec; 0.264 sec/batch; 24h:11m:04s remains)
INFO - root - 2017-12-11 03:11:05.162868: step 3260, loss = 0.71, batch loss = 0.65 (29.7 examples/sec; 0.269 sec/batch; 24h:37m:02s remains)
INFO - root - 2017-12-11 03:11:07.836133: step 3270, loss = 0.70, batch loss = 0.65 (30.2 examples/sec; 0.265 sec/batch; 24h:12m:45s remains)
INFO - root - 2017-12-11 03:11:10.515438: step 3280, loss = 0.67, batch loss = 0.61 (29.6 examples/sec; 0.270 sec/batch; 24h:43m:49s remains)
INFO - root - 2017-12-11 03:11:13.182985: step 3290, loss = 0.70, batch loss = 0.64 (30.2 examples/sec; 0.265 sec/batch; 24h:14m:31s remains)
INFO - root - 2017-12-11 03:11:15.833649: step 3300, loss = 0.69, batch loss = 0.63 (29.8 examples/sec; 0.269 sec/batch; 24h:34m:45s remains)
2017-12-11 03:11:16.310755: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.26950434 0.21407774 0.14329071 0.083273619 0.051012255 0.051119767 0.077464677 0.11300708 0.14570943 0.177674 0.20776457 0.2283193 0.23709166 0.24049874 0.24327178][0.20640668 0.15293382 0.087052613 0.03135334 -0.00042553141 -0.0061618672 0.007442534 0.027164133 0.044020474 0.060489498 0.07822448 0.095173679 0.11212522 0.13435972 0.16321558][0.15703149 0.10971359 0.052900139 0.0045411941 -0.024510019 -0.03372385 -0.029374434 -0.021425446 -0.016662804 -0.01376404 -0.0094542494 -0.00018533421 0.018547308 0.05156143 0.097953223][0.12507546 0.087388538 0.044228356 0.0086821578 -0.010644965 -0.013949914 -0.0076314188 -0.0010168534 -0.000943327 -0.0082259718 -0.018986057 -0.024948619 -0.017018195 0.011428292 0.059700657][0.10920309 0.08293312 0.057174236 0.040755447 0.04069965 0.056242198 0.079145253 0.09670046 0.098695986 0.081814781 0.050430149 0.017954694 0.00019003678 0.0084996913 0.044635672][0.10781712 0.092258751 0.084779941 0.091373555 0.11806355 0.16265666 0.21260609 0.2481395 0.25324711 0.22336513 0.16473283 0.097359605 0.045513131 0.026828324 0.045488149][0.11023205 0.1035657 0.11345898 0.1446615 0.20170033 0.27878088 0.3570478 0.40849978 0.4120943 0.36477834 0.2769638 0.17586383 0.093453377 0.052012455 0.055960692][0.10124516 0.10206769 0.12853156 0.18452662 0.27092031 0.37578949 0.47360158 0.53082705 0.525476 0.4593899 0.3485218 0.22523107 0.12523004 0.071323119 0.065556332][0.071564995 0.079245366 0.11969624 0.1952434 0.30134961 0.41973591 0.5209372 0.57143945 0.55342489 0.47498775 0.35625941 0.22985117 0.12927732 0.073867694 0.062445048][0.029399965 0.044634264 0.094607979 0.17851435 0.28651398 0.39707276 0.4825123 0.51613832 0.48842397 0.41169345 0.30562839 0.19643305 0.10987628 0.059784785 0.043784212][-0.0030506898 0.021624422 0.076071128 0.15477766 0.24454796 0.32569516 0.37891933 0.39006618 0.35966071 0.30049294 0.22590111 0.14991975 0.086867742 0.0448266 0.023438431][-0.0062718014 0.028568696 0.082654782 0.14524375 0.20274884 0.24188536 0.255911 0.24515456 0.21894765 0.18747877 0.15398575 0.11823949 0.082333133 0.049243364 0.022584969][0.024864295 0.067175969 0.11658201 0.15844671 0.18117821 0.17949077 0.15881453 0.13149627 0.11247106 0.10859145 0.11345673 0.11462703 0.10373389 0.0794168 0.048999522][0.076121755 0.11973261 0.16000123 0.18172489 0.17659155 0.14582868 0.10291515 0.066919073 0.054556936 0.070391715 0.10214943 0.12993546 0.13798152 0.12119248 0.088914879][0.12078312 0.15715678 0.18347007 0.18764451 0.16514225 0.12078347 0.070656419 0.034729451 0.028220173 0.054449655 0.099604592 0.1415347 0.1608303 0.14984831 0.11818905]]...]
INFO - root - 2017-12-11 03:11:18.974540: step 3310, loss = 0.70, batch loss = 0.64 (30.8 examples/sec; 0.260 sec/batch; 23h:45m:25s remains)
INFO - root - 2017-12-11 03:11:21.633407: step 3320, loss = 0.71, batch loss = 0.65 (30.4 examples/sec; 0.263 sec/batch; 24h:04m:53s remains)
INFO - root - 2017-12-11 03:11:24.331515: step 3330, loss = 0.70, batch loss = 0.64 (30.6 examples/sec; 0.262 sec/batch; 23h:56m:23s remains)
INFO - root - 2017-12-11 03:11:26.988534: step 3340, loss = 0.69, batch loss = 0.63 (30.8 examples/sec; 0.260 sec/batch; 23h:47m:01s remains)
INFO - root - 2017-12-11 03:11:29.647917: step 3350, loss = 0.70, batch loss = 0.64 (29.9 examples/sec; 0.267 sec/batch; 24h:25m:44s remains)
INFO - root - 2017-12-11 03:11:32.287360: step 3360, loss = 0.69, batch loss = 0.63 (31.5 examples/sec; 0.254 sec/batch; 23h:12m:43s remains)
INFO - root - 2017-12-11 03:11:34.973340: step 3370, loss = 0.69, batch loss = 0.64 (30.7 examples/sec; 0.261 sec/batch; 23h:51m:39s remains)
INFO - root - 2017-12-11 03:11:37.663642: step 3380, loss = 0.68, batch loss = 0.62 (29.9 examples/sec; 0.267 sec/batch; 24h:25m:37s remains)
INFO - root - 2017-12-11 03:11:40.303466: step 3390, loss = 0.70, batch loss = 0.64 (29.5 examples/sec; 0.271 sec/batch; 24h:47m:31s remains)
INFO - root - 2017-12-11 03:11:42.961469: step 3400, loss = 0.69, batch loss = 0.63 (29.6 examples/sec; 0.270 sec/batch; 24h:41m:54s remains)
2017-12-11 03:11:43.472875: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.31604296 0.29242516 0.2299272 0.16021386 0.11018333 0.0912392 0.092794329 0.10329181 0.12101713 0.14264597 0.16369459 0.18515022 0.21698537 0.26315513 0.31206486][0.4235343 0.40437403 0.33391297 0.25188121 0.19117926 0.16886108 0.17250247 0.18610245 0.20637426 0.23048586 0.25335163 0.27469471 0.30701616 0.35529208 0.40632755][0.5201298 0.51408052 0.44752473 0.36405745 0.29900563 0.27487996 0.27900746 0.29052243 0.30448559 0.32094881 0.33585507 0.34809723 0.37124276 0.411551 0.45626602][0.59607553 0.6077621 0.5537765 0.47861865 0.41696689 0.39465192 0.3986136 0.4027814 0.40100914 0.39840284 0.39532796 0.39054322 0.39698723 0.42264333 0.45685378][0.63385111 0.65886956 0.6181736 0.55646342 0.50574237 0.49204177 0.50074875 0.50047034 0.483176 0.4585301 0.43405333 0.40940145 0.39535835 0.40133363 0.4207989][0.62351835 0.6519531 0.6208446 0.57538712 0.54406238 0.54975444 0.57480568 0.58215666 0.55911022 0.51695979 0.47180468 0.4276135 0.39363605 0.37913361 0.38122296][0.57746571 0.60008776 0.57513267 0.54736143 0.54018128 0.5697186 0.61514479 0.63576555 0.61418295 0.56088996 0.49963561 0.44120851 0.39480042 0.36819166 0.3596698][0.50629413 0.51892728 0.49918428 0.48937386 0.50652063 0.55704689 0.61752737 0.64772147 0.62938273 0.57150489 0.50211948 0.43738002 0.38776508 0.35922241 0.34821361][0.42660838 0.42868826 0.41090626 0.41261947 0.44536397 0.5064373 0.56981325 0.60017115 0.58309132 0.52699685 0.4595345 0.39857581 0.35604456 0.33549991 0.32972568][0.34604695 0.33807808 0.31842688 0.32354489 0.361569 0.42267221 0.47861221 0.50140035 0.4828127 0.43158585 0.37253103 0.32294959 0.29505295 0.2888113 0.29288995][0.26121193 0.24668919 0.22477569 0.22712721 0.26014787 0.3109858 0.35249123 0.3642599 0.34301588 0.29797518 0.25063595 0.21615037 0.20553935 0.21526137 0.23070426][0.17519119 0.15861833 0.13689801 0.1340466 0.15534353 0.18927528 0.21321535 0.2134113 0.18932326 0.15048048 0.11534534 0.095665224 0.10017285 0.12274366 0.14728308][0.089927159 0.075247817 0.057192396 0.051039483 0.060407694 0.077500805 0.085784689 0.076759189 0.051235981 0.018587081 -0.0058369008 -0.013967019 -0.00010561753 0.028234098 0.055773947][0.023046548 0.011629894 -0.0020151301 -0.0099803964 -0.0099696154 -0.0058152764 -0.0079536382 -0.021059036 -0.04405459 -0.06882865 -0.083521396 -0.083268784 -0.0654988 -0.037754618 -0.012294474][-0.020499585 -0.028809711 -0.038113613 -0.045578312 -0.049902491 -0.052257679 -0.058298387 -0.07028389 -0.08739195 -0.10350586 -0.1103503 -0.10567929 -0.089212209 -0.066992112 -0.047279064]]...]
INFO - root - 2017-12-11 03:11:46.170308: step 3410, loss = 0.66, batch loss = 0.61 (30.8 examples/sec; 0.260 sec/batch; 23h:46m:54s remains)
INFO - root - 2017-12-11 03:11:48.895749: step 3420, loss = 0.69, batch loss = 0.63 (26.8 examples/sec; 0.299 sec/batch; 27h:18m:40s remains)
INFO - root - 2017-12-11 03:11:51.541323: step 3430, loss = 0.69, batch loss = 0.63 (30.7 examples/sec; 0.260 sec/batch; 23h:48m:27s remains)
INFO - root - 2017-12-11 03:11:54.216041: step 3440, loss = 0.70, batch loss = 0.65 (30.0 examples/sec; 0.267 sec/batch; 24h:24m:17s remains)
INFO - root - 2017-12-11 03:11:56.828138: step 3450, loss = 0.72, batch loss = 0.66 (29.8 examples/sec; 0.268 sec/batch; 24h:31m:51s remains)
INFO - root - 2017-12-11 03:11:59.451323: step 3460, loss = 0.67, batch loss = 0.61 (31.0 examples/sec; 0.258 sec/batch; 23h:34m:45s remains)
INFO - root - 2017-12-11 03:12:02.127521: step 3470, loss = 0.69, batch loss = 0.63 (29.1 examples/sec; 0.275 sec/batch; 25h:09m:47s remains)
INFO - root - 2017-12-11 03:12:04.862458: step 3480, loss = 0.69, batch loss = 0.63 (28.8 examples/sec; 0.278 sec/batch; 25h:23m:06s remains)
INFO - root - 2017-12-11 03:12:07.529374: step 3490, loss = 0.69, batch loss = 0.63 (30.3 examples/sec; 0.264 sec/batch; 24h:05m:49s remains)
INFO - root - 2017-12-11 03:12:10.189868: step 3500, loss = 0.68, batch loss = 0.62 (29.8 examples/sec; 0.268 sec/batch; 24h:31m:34s remains)
2017-12-11 03:12:10.646641: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.37823495 0.37676489 0.3594206 0.33893183 0.32868239 0.32675746 0.32268527 0.3129358 0.30471754 0.31015056 0.33250374 0.3607519 0.38610005 0.40684217 0.41224584][0.44070771 0.44292015 0.42561853 0.40274122 0.38893673 0.38424215 0.37816036 0.3661103 0.35586244 0.36222896 0.39125314 0.43082109 0.47002321 0.50339276 0.51632136][0.47292158 0.4789291 0.4616299 0.43526697 0.41564015 0.40654242 0.39843959 0.38558906 0.37474531 0.38178408 0.41466811 0.46228334 0.51175588 0.55371553 0.5707323][0.48323369 0.49009389 0.47087744 0.44042093 0.41514131 0.40255851 0.39429975 0.38261658 0.37247989 0.37975717 0.4132494 0.46479866 0.51993597 0.56708235 0.58702123][0.48011208 0.48498583 0.46225956 0.42698252 0.39691165 0.38292104 0.37651253 0.36781245 0.36078838 0.37056184 0.40462637 0.45699897 0.51366651 0.56332338 0.58671385][0.46712342 0.46886471 0.44287038 0.40414259 0.37189296 0.35860023 0.35493612 0.350001 0.34875718 0.36474869 0.40168002 0.45374995 0.50854933 0.55716616 0.58215922][0.4629294 0.46062306 0.43069577 0.38931361 0.35670424 0.34528866 0.34388721 0.34210634 0.347066 0.37086463 0.4122245 0.46350321 0.51435477 0.55961692 0.58425081][0.47446784 0.46755758 0.4307583 0.38477755 0.35177711 0.3428013 0.344017 0.34604558 0.35760316 0.38900423 0.43430945 0.48344815 0.52864772 0.56865728 0.59101444][0.48287055 0.4688594 0.42225227 0.37027425 0.33713466 0.33140647 0.33615094 0.34286544 0.36096212 0.39842707 0.44611052 0.49152446 0.52944708 0.56144512 0.57836729][0.46028319 0.4368397 0.37958112 0.32201239 0.28963324 0.28804123 0.29750246 0.31059083 0.33591172 0.37858731 0.42763472 0.46913302 0.49994516 0.52241069 0.53046572][0.40125692 0.36786437 0.30192891 0.24131718 0.21074864 0.21267612 0.22628736 0.24544236 0.27607691 0.31966323 0.36545885 0.40051797 0.42274839 0.43429616 0.43171427][0.34392184 0.3050029 0.23486188 0.17366235 0.14375548 0.14497842 0.15804203 0.17788461 0.20674491 0.24242857 0.27616468 0.29892409 0.31009784 0.31190795 0.302772][0.31088033 0.2706784 0.20174788 0.14352024 0.11483387 0.11323133 0.12251592 0.13796225 0.15856333 0.17972398 0.19576079 0.20280461 0.20194516 0.1957804 0.1843596][0.29407746 0.25670666 0.19397677 0.14254785 0.1169147 0.11248375 0.11629787 0.12384597 0.1324973 0.137179 0.135264 0.12734149 0.11631546 0.1052934 0.095310047][0.29607502 0.26389888 0.20763969 0.1618726 0.13850392 0.13160489 0.13015237 0.129567 0.12643106 0.11627202 0.099049553 0.0789088 0.060805462 0.048431911 0.043088321]]...]
INFO - root - 2017-12-11 03:12:13.274980: step 3510, loss = 0.70, batch loss = 0.64 (31.1 examples/sec; 0.257 sec/batch; 23h:31m:35s remains)
INFO - root - 2017-12-11 03:12:15.883301: step 3520, loss = 0.69, batch loss = 0.64 (31.1 examples/sec; 0.257 sec/batch; 23h:31m:44s remains)
INFO - root - 2017-12-11 03:12:18.537387: step 3530, loss = 0.69, batch loss = 0.63 (29.7 examples/sec; 0.269 sec/batch; 24h:34m:26s remains)
INFO - root - 2017-12-11 03:12:21.189183: step 3540, loss = 0.71, batch loss = 0.65 (29.4 examples/sec; 0.272 sec/batch; 24h:49m:22s remains)
INFO - root - 2017-12-11 03:12:23.863424: step 3550, loss = 0.69, batch loss = 0.63 (28.7 examples/sec; 0.279 sec/batch; 25h:28m:27s remains)
INFO - root - 2017-12-11 03:12:26.544729: step 3560, loss = 0.69, batch loss = 0.63 (30.1 examples/sec; 0.266 sec/batch; 24h:17m:21s remains)
INFO - root - 2017-12-11 03:12:29.197318: step 3570, loss = 0.70, batch loss = 0.64 (29.0 examples/sec; 0.276 sec/batch; 25h:14m:20s remains)
INFO - root - 2017-12-11 03:12:31.865860: step 3580, loss = 0.70, batch loss = 0.64 (30.2 examples/sec; 0.265 sec/batch; 24h:12m:16s remains)
INFO - root - 2017-12-11 03:12:34.512796: step 3590, loss = 0.70, batch loss = 0.64 (29.3 examples/sec; 0.273 sec/batch; 24h:54m:40s remains)
INFO - root - 2017-12-11 03:12:37.161062: step 3600, loss = 0.69, batch loss = 0.63 (28.6 examples/sec; 0.280 sec/batch; 25h:33m:02s remains)
2017-12-11 03:12:37.643304: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.19098775 0.11396734 0.039230864 -0.0034762688 -0.0075401193 0.020641012 0.069198057 0.12666194 0.18427131 0.23362157 0.26488286 0.27007821 0.24530908 0.19455382 0.12914494][0.24395108 0.16071714 0.075962164 0.02620524 0.022362081 0.057929903 0.11595359 0.17895253 0.23436205 0.27359721 0.28977218 0.27931952 0.24420701 0.19189665 0.13454358][0.26259112 0.18237616 0.098052844 0.048959844 0.049821902 0.094361253 0.16132249 0.22733259 0.27663124 0.30107382 0.29783532 0.27114567 0.22943835 0.18328711 0.14231266][0.23663701 0.17034978 0.099551231 0.061432939 0.072162859 0.12564774 0.19796719 0.26194558 0.30036092 0.30721027 0.28483987 0.24544357 0.20356575 0.16985947 0.14812689][0.17221472 0.12461402 0.076526292 0.058858655 0.08622019 0.15037052 0.2251396 0.28155726 0.30360773 0.29013017 0.25085241 0.20546827 0.1715987 0.15697753 0.15703648][0.094311967 0.065394275 0.045355164 0.0571625 0.11038858 0.19172315 0.27128914 0.31898141 0.32167897 0.28488925 0.22761883 0.17658839 0.15069483 0.15370603 0.17276454][0.033048525 0.02003373 0.026624678 0.068127237 0.14742298 0.24569362 0.3295525 0.36868626 0.35369548 0.29612428 0.2232067 0.16694352 0.14656246 0.16292118 0.19698092][-0.0070634694 -0.0056138765 0.0222156 0.085501224 0.180998 0.28683686 0.36905861 0.39958039 0.37277776 0.30388784 0.22476323 0.16918933 0.15519923 0.18215187 0.22834104][-0.03532825 -0.022936966 0.018168062 0.092411809 0.19219029 0.29505861 0.37003127 0.39360577 0.36363938 0.29620668 0.22303666 0.17489262 0.16768442 0.2009891 0.25471702][-0.05548222 -0.039227456 0.0050102696 0.078128047 0.17024989 0.26052541 0.32340303 0.3411434 0.31421962 0.25734845 0.19890521 0.1637504 0.16379246 0.19963612 0.25535235][-0.065061532 -0.051070087 -0.012519578 0.049166132 0.12406606 0.19502984 0.24259458 0.25441104 0.2322716 0.18826944 0.14630984 0.12510592 0.13234092 0.16834776 0.22097342][-0.065125048 -0.05740077 -0.030541655 0.013039774 0.0653646 0.11398817 0.14531662 0.15124945 0.133456 0.10075528 0.0725331 0.063224845 0.077317365 0.11371234 0.16165961][-0.062008236 -0.060539454 -0.045678351 -0.019455353 0.012683999 0.042622212 0.06134396 0.063336685 0.048865065 0.023596486 0.0029040033 -0.0011131426 0.015052804 0.049169533 0.090851374][-0.06274382 -0.064967841 -0.05748402 -0.042616222 -0.023593754 -0.0053090383 0.0066852565 0.007869726 -0.0035369399 -0.024468839 -0.043349486 -0.049782682 -0.039680056 -0.014669712 0.0162853][-0.067344777 -0.072602846 -0.069381386 -0.061077751 -0.049705796 -0.03787443 -0.028581815 -0.025808031 -0.033091661 -0.049224734 -0.0663839 -0.076608129 -0.075526722 -0.063595951 -0.046963979]]...]
INFO - root - 2017-12-11 03:12:40.283101: step 3610, loss = 0.70, batch loss = 0.65 (30.9 examples/sec; 0.259 sec/batch; 23h:39m:06s remains)
INFO - root - 2017-12-11 03:12:42.934418: step 3620, loss = 0.68, batch loss = 0.62 (30.6 examples/sec; 0.261 sec/batch; 23h:53m:06s remains)
INFO - root - 2017-12-11 03:12:45.568994: step 3630, loss = 0.70, batch loss = 0.64 (30.0 examples/sec; 0.267 sec/batch; 24h:22m:00s remains)
INFO - root - 2017-12-11 03:12:48.183831: step 3640, loss = 0.70, batch loss = 0.65 (30.5 examples/sec; 0.262 sec/batch; 23h:58m:44s remains)
INFO - root - 2017-12-11 03:12:50.868651: step 3650, loss = 0.70, batch loss = 0.64 (28.4 examples/sec; 0.282 sec/batch; 25h:45m:27s remains)
INFO - root - 2017-12-11 03:12:53.592145: step 3660, loss = 0.71, batch loss = 0.65 (30.8 examples/sec; 0.260 sec/batch; 23h:44m:51s remains)
INFO - root - 2017-12-11 03:12:56.224867: step 3670, loss = 0.71, batch loss = 0.65 (30.5 examples/sec; 0.262 sec/batch; 23h:56m:54s remains)
INFO - root - 2017-12-11 03:12:58.872095: step 3680, loss = 0.69, batch loss = 0.63 (30.1 examples/sec; 0.266 sec/batch; 24h:18m:03s remains)
INFO - root - 2017-12-11 03:13:01.536424: step 3690, loss = 0.68, batch loss = 0.62 (29.0 examples/sec; 0.275 sec/batch; 25h:09m:11s remains)
INFO - root - 2017-12-11 03:13:04.189089: step 3700, loss = 0.69, batch loss = 0.63 (30.1 examples/sec; 0.266 sec/batch; 24h:16m:03s remains)
2017-12-11 03:13:04.641143: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.12802711 0.13128047 0.12340899 0.10457065 0.081086442 0.071494065 0.094283439 0.15190832 0.21989317 0.25952747 0.24746659 0.18202628 0.086548232 -0.0044547981 -0.066459931][0.22124752 0.21765593 0.1980437 0.16820879 0.13893174 0.13277508 0.16996129 0.25188747 0.34361249 0.39513472 0.37811145 0.29168555 0.1660375 0.044272274 -0.041048624][0.30937296 0.29031616 0.25204772 0.20831662 0.17464149 0.1739932 0.22298574 0.3187435 0.41826126 0.46767861 0.43983328 0.33755556 0.19612586 0.062050633 -0.030342326][0.37665555 0.34140524 0.28534186 0.22833538 0.1899282 0.19228959 0.24667664 0.34471664 0.43768984 0.4735038 0.43089956 0.31864285 0.17526196 0.045550585 -0.039561067][0.43137625 0.38578707 0.3175092 0.25351441 0.21754833 0.23017666 0.29473636 0.39404932 0.47205386 0.48124698 0.41045693 0.28008026 0.13346747 0.011850022 -0.061016008][0.45240653 0.40453127 0.3380813 0.28611627 0.27577028 0.32177669 0.4124186 0.51562148 0.56769776 0.53009534 0.40859237 0.24261089 0.084177114 -0.030659791 -0.0893944][0.43789431 0.39283946 0.34052822 0.31825826 0.35626143 0.45758316 0.58735722 0.69377327 0.71040857 0.61393648 0.4312264 0.22291203 0.049310535 -0.060591128 -0.10757316][0.38068691 0.34136274 0.30987704 0.32577673 0.4211857 0.58350539 0.75066018 0.85200906 0.82773179 0.67580777 0.4423638 0.20327635 0.022205202 -0.079971872 -0.11672724][0.29080573 0.26007631 0.24818973 0.29372644 0.42841047 0.62483853 0.80190575 0.88272417 0.8238107 0.64364386 0.3970778 0.16056846 -0.0079284674 -0.095400609 -0.1224409][0.19243683 0.16936405 0.16872227 0.22493641 0.36651459 0.55652022 0.70945805 0.75684482 0.67702949 0.50414765 0.28886279 0.092095815 -0.043127351 -0.10977182 -0.12764643][0.092542514 0.0747298 0.078266792 0.12947562 0.25206676 0.40676308 0.51640737 0.53005433 0.44683945 0.30766627 0.15007293 0.011468514 -0.0820274 -0.12626165 -0.13491072][0.0037428171 -0.01269093 -0.010735315 0.027171852 0.11914615 0.22947083 0.29681581 0.28984413 0.22112 0.12789072 0.030563876 -0.054478496 -0.1130036 -0.13977852 -0.14140517][-0.061833885 -0.0802766 -0.082777545 -0.059190791 0.0032837621 0.076498754 0.11676352 0.10762671 0.064730607 0.014065634 -0.038547318 -0.088797338 -0.12660266 -0.1437279 -0.14214048][-0.089207433 -0.10503284 -0.10881131 -0.095667988 -0.057351105 -0.012183777 0.012732618 0.010527679 -0.00751932 -0.028825812 -0.056269243 -0.08987426 -0.1192806 -0.13375153 -0.1334534][-0.073115066 -0.083291337 -0.0853551 -0.077076346 -0.053405832 -0.024685441 -0.0067111268 -0.0026284398 -0.0080910968 -0.020253683 -0.043356247 -0.075852953 -0.10536794 -0.12059188 -0.12261919]]...]
INFO - root - 2017-12-11 03:13:07.295312: step 3710, loss = 0.69, batch loss = 0.63 (29.5 examples/sec; 0.271 sec/batch; 24h:47m:16s remains)
INFO - root - 2017-12-11 03:13:09.948242: step 3720, loss = 0.71, batch loss = 0.65 (30.2 examples/sec; 0.265 sec/batch; 24h:13m:39s remains)
INFO - root - 2017-12-11 03:13:12.601403: step 3730, loss = 0.69, batch loss = 0.63 (31.2 examples/sec; 0.256 sec/batch; 23h:23m:06s remains)
INFO - root - 2017-12-11 03:13:15.282436: step 3740, loss = 0.70, batch loss = 0.64 (30.3 examples/sec; 0.264 sec/batch; 24h:08m:13s remains)
INFO - root - 2017-12-11 03:13:17.914730: step 3750, loss = 0.69, batch loss = 0.64 (31.3 examples/sec; 0.256 sec/batch; 23h:21m:15s remains)
INFO - root - 2017-12-11 03:13:20.613806: step 3760, loss = 0.70, batch loss = 0.64 (30.1 examples/sec; 0.266 sec/batch; 24h:17m:51s remains)
INFO - root - 2017-12-11 03:13:23.309762: step 3770, loss = 0.70, batch loss = 0.64 (29.3 examples/sec; 0.273 sec/batch; 24h:56m:27s remains)
INFO - root - 2017-12-11 03:13:26.032266: step 3780, loss = 0.68, batch loss = 0.62 (28.9 examples/sec; 0.277 sec/batch; 25h:18m:36s remains)
INFO - root - 2017-12-11 03:13:28.707059: step 3790, loss = 0.68, batch loss = 0.63 (30.1 examples/sec; 0.265 sec/batch; 24h:13m:57s remains)
INFO - root - 2017-12-11 03:13:31.348210: step 3800, loss = 0.69, batch loss = 0.64 (29.8 examples/sec; 0.269 sec/batch; 24h:31m:53s remains)
2017-12-11 03:13:31.789555: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.48545462 0.58574921 0.61519879 0.58364666 0.51449245 0.43970558 0.38633183 0.36608 0.36879337 0.37519789 0.38005275 0.38443312 0.38527265 0.37801215 0.36168602][0.44335851 0.53066921 0.55176228 0.52057874 0.46144006 0.40286312 0.36676657 0.36110529 0.37086263 0.37508711 0.36988679 0.36035737 0.35038114 0.33829993 0.32411361][0.35414711 0.4245823 0.44271266 0.42519763 0.39507613 0.37320891 0.37105769 0.386486 0.39949298 0.38980293 0.35786745 0.3174502 0.282039 0.25604829 0.23964451][0.24166468 0.29788443 0.3208122 0.32713562 0.33834076 0.36632344 0.40855527 0.44839633 0.46147612 0.42988461 0.36051682 0.27780902 0.20667757 0.15846534 0.13391495][0.13245948 0.18207522 0.21864574 0.25639597 0.31510097 0.39713886 0.48393136 0.5428735 0.54688692 0.48376346 0.37051511 0.24391282 0.13847913 0.069401167 0.036167957][0.054256547 0.10702237 0.16701783 0.24527769 0.35648745 0.48976919 0.60811549 0.66468495 0.63599676 0.52491271 0.36460146 0.20232807 0.074840546 -0.0050834734 -0.042317506][0.024543641 0.088523388 0.17796244 0.30009732 0.45950106 0.62897593 0.75456172 0.78279787 0.70262563 0.53879708 0.34058222 0.15893303 0.024921723 -0.055824619 -0.093529135][0.026395876 0.10459708 0.22194505 0.38064802 0.57062137 0.74910951 0.85346889 0.83667713 0.70598376 0.50547278 0.29189393 0.11172446 -0.01339589 -0.085667893 -0.11947358][0.039705675 0.12846777 0.26303768 0.43881834 0.6306985 0.78691739 0.8487404 0.78741091 0.62745672 0.42096022 0.22013065 0.061626285 -0.041334055 -0.096054323 -0.11919222][0.049718853 0.13868864 0.27087182 0.43592691 0.5994578 0.71119481 0.72689629 0.63832521 0.47838348 0.294243 0.12691957 0.0031874392 -0.069015369 -0.0997833 -0.10706919][0.042502262 0.11874323 0.2295904 0.36179236 0.48022342 0.54404986 0.52588415 0.43110102 0.29100668 0.14237449 0.016429998 -0.067404367 -0.10551032 -0.11037426 -0.10018581][0.011394629 0.065061569 0.14215647 0.22990045 0.2999984 0.32471716 0.29048911 0.20779677 0.10103316 -0.0040575648 -0.084684089 -0.12821583 -0.13528627 -0.119362 -0.096605942][-0.036989763 -0.010918746 0.028270712 0.071280658 0.10074621 0.10143173 0.068377584 0.011080218 -0.054628532 -0.11301715 -0.1499072 -0.15937583 -0.14514427 -0.11895049 -0.09250889][-0.080426753 -0.077663824 -0.068833143 -0.059153974 -0.05597654 -0.065441452 -0.088165671 -0.11785629 -0.14636789 -0.16639109 -0.17153342 -0.16067201 -0.13827863 -0.11227962 -0.089300521][-0.10470098 -0.11459375 -0.12146705 -0.1279802 -0.13589093 -0.14669137 -0.15931229 -0.17010114 -0.17584264 -0.17451487 -0.16461954 -0.14720209 -0.12596032 -0.10526633 -0.088204667]]...]
INFO - root - 2017-12-11 03:13:34.456175: step 3810, loss = 0.70, batch loss = 0.64 (30.4 examples/sec; 0.263 sec/batch; 24h:00m:14s remains)
INFO - root - 2017-12-11 03:13:37.111978: step 3820, loss = 0.69, batch loss = 0.63 (30.0 examples/sec; 0.266 sec/batch; 24h:18m:29s remains)
INFO - root - 2017-12-11 03:13:39.752511: step 3830, loss = 0.69, batch loss = 0.63 (30.2 examples/sec; 0.265 sec/batch; 24h:12m:05s remains)
INFO - root - 2017-12-11 03:13:42.401570: step 3840, loss = 0.69, batch loss = 0.63 (30.3 examples/sec; 0.264 sec/batch; 24h:06m:30s remains)
INFO - root - 2017-12-11 03:13:45.057953: step 3850, loss = 0.71, batch loss = 0.65 (31.0 examples/sec; 0.258 sec/batch; 23h:31m:54s remains)
INFO - root - 2017-12-11 03:13:47.706020: step 3860, loss = 0.70, batch loss = 0.64 (29.6 examples/sec; 0.270 sec/batch; 24h:39m:59s remains)
INFO - root - 2017-12-11 03:13:50.372685: step 3870, loss = 0.70, batch loss = 0.64 (29.2 examples/sec; 0.274 sec/batch; 25h:01m:51s remains)
INFO - root - 2017-12-11 03:13:53.066943: step 3880, loss = 0.71, batch loss = 0.65 (28.2 examples/sec; 0.283 sec/batch; 25h:51m:15s remains)
INFO - root - 2017-12-11 03:13:55.688206: step 3890, loss = 0.69, batch loss = 0.63 (30.3 examples/sec; 0.264 sec/batch; 24h:04m:15s remains)
INFO - root - 2017-12-11 03:13:58.346058: step 3900, loss = 0.70, batch loss = 0.64 (30.4 examples/sec; 0.263 sec/batch; 24h:01m:18s remains)
2017-12-11 03:13:58.749249: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.023105249 -0.0026438313 0.021125119 0.046449952 0.06654086 0.073194325 0.062711492 0.038460162 0.0096172262 -0.016262705 -0.0333433 -0.041045032 -0.041547265 -0.038178563 -0.035360839][-0.012811327 0.016631193 0.05357844 0.0943024 0.12851495 0.14353663 0.13283396 0.10015962 0.058156468 0.016872369 -0.013261008 -0.028887937 -0.03246735 -0.029127538 -0.02548187][0.0035006411 0.0412629 0.091905996 0.14861417 0.19745898 0.22252688 0.21431507 0.17493097 0.12041362 0.062949933 0.017989228 -0.0076136896 -0.016007692 -0.013681839 -0.0093519595][0.01850689 0.06113828 0.1219072 0.19116071 0.25220644 0.2882081 0.28680041 0.24591115 0.18286523 0.1118438 0.053397425 0.017381329 0.0027343484 0.00246492 0.0065645222][0.027067259 0.069192812 0.13272345 0.20742802 0.27614647 0.323178 0.3330664 0.29757312 0.23220025 0.15323588 0.085393891 0.040384024 0.018989252 0.014853501 0.017248193][0.028934419 0.066663712 0.12710585 0.20126706 0.27365279 0.3308244 0.35474014 0.33052593 0.26908287 0.18819572 0.11534114 0.062718324 0.033445533 0.022925066 0.020772958][0.029154733 0.061324183 0.11529912 0.18489832 0.25717664 0.32102939 0.35705864 0.34575742 0.29258826 0.21549082 0.14251037 0.084861927 0.048028048 0.029921038 0.021833016][0.032594681 0.057633091 0.10174317 0.16210654 0.22884855 0.29282576 0.33537874 0.33586159 0.29451653 0.22688268 0.15895027 0.10021985 0.058165241 0.033588983 0.020139413][0.03572619 0.053831808 0.087300986 0.13617842 0.19408743 0.25340021 0.29785988 0.3085829 0.28197041 0.22813493 0.1680872 0.11007532 0.06410335 0.034077104 0.016295414][0.03752885 0.052443385 0.080317244 0.12246351 0.17397456 0.22795558 0.27073258 0.2871404 0.27181906 0.22898935 0.17429194 0.11608806 0.06711144 0.033784125 0.014362984][0.042327911 0.058422655 0.087476753 0.13026777 0.18053442 0.23075616 0.26913136 0.28448543 0.27159241 0.230782 0.17477393 0.11375754 0.062293421 0.02824227 0.010749054][0.050721776 0.069769137 0.10251565 0.1485467 0.19906254 0.24593973 0.27846235 0.28828496 0.27053094 0.22451819 0.16326305 0.099420369 0.048142288 0.017648609 0.0069347387][0.059033837 0.081179306 0.11822069 0.16838746 0.22052108 0.26564127 0.29341516 0.29622433 0.26932117 0.21344087 0.14475515 0.078834072 0.030414689 0.0069468045 0.006218018][0.063909374 0.088947877 0.1303174 0.1851006 0.24043553 0.28607875 0.31143349 0.30801669 0.27048969 0.20277861 0.12619931 0.059026219 0.014674756 -0.0014159089 0.0069653019][0.064868122 0.090762556 0.13410981 0.19096504 0.24766871 0.29309139 0.31665131 0.30827436 0.26123914 0.18323633 0.10124717 0.035984077 -0.0012324104 -0.008196976 0.0093507767]]...]
INFO - root - 2017-12-11 03:14:01.387064: step 3910, loss = 0.71, batch loss = 0.65 (30.2 examples/sec; 0.265 sec/batch; 24h:10m:41s remains)
INFO - root - 2017-12-11 03:14:04.087311: step 3920, loss = 0.68, batch loss = 0.63 (30.4 examples/sec; 0.263 sec/batch; 24h:00m:59s remains)
INFO - root - 2017-12-11 03:14:06.710500: step 3930, loss = 0.72, batch loss = 0.66 (29.7 examples/sec; 0.269 sec/batch; 24h:33m:15s remains)
INFO - root - 2017-12-11 03:14:09.373597: step 3940, loss = 0.69, batch loss = 0.64 (30.7 examples/sec; 0.261 sec/batch; 23h:49m:07s remains)
INFO - root - 2017-12-11 03:14:11.973688: step 3950, loss = 0.69, batch loss = 0.63 (31.5 examples/sec; 0.254 sec/batch; 23h:08m:57s remains)
INFO - root - 2017-12-11 03:14:14.608646: step 3960, loss = 0.69, batch loss = 0.63 (30.6 examples/sec; 0.262 sec/batch; 23h:52m:26s remains)
INFO - root - 2017-12-11 03:14:17.249446: step 3970, loss = 0.70, batch loss = 0.64 (30.1 examples/sec; 0.266 sec/batch; 24h:15m:24s remains)
INFO - root - 2017-12-11 03:14:19.843829: step 3980, loss = 0.70, batch loss = 0.64 (30.5 examples/sec; 0.262 sec/batch; 23h:55m:34s remains)
INFO - root - 2017-12-11 03:14:22.481305: step 3990, loss = 0.69, batch loss = 0.63 (28.8 examples/sec; 0.278 sec/batch; 25h:22m:07s remains)
INFO - root - 2017-12-11 03:14:25.130455: step 4000, loss = 0.70, batch loss = 0.64 (29.6 examples/sec; 0.271 sec/batch; 24h:41m:54s remains)
2017-12-11 03:14:25.547173: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.2103042 0.19551954 0.175318 0.15668774 0.13726294 0.11346758 0.089598149 0.075460531 0.070357777 0.062866524 0.053134777 0.054858029 0.067054324 0.076508507 0.077206135][0.19545212 0.18267407 0.16698973 0.1514308 0.13462375 0.11512621 0.096459009 0.085982807 0.082946651 0.078383945 0.073795244 0.082632408 0.09995196 0.11154583 0.1114196][0.1674047 0.16074304 0.15273878 0.14313322 0.1319456 0.11943175 0.10705442 0.099165365 0.095703341 0.091508634 0.090274908 0.10441967 0.12603341 0.13968894 0.13956194][0.15054545 0.15134519 0.15091638 0.14722434 0.14132218 0.13439494 0.12632477 0.11925335 0.11338584 0.10685616 0.1053495 0.12071885 0.14340012 0.15751958 0.15728194][0.1542248 0.16199626 0.16636868 0.16638923 0.16391529 0.16012326 0.15330356 0.14486334 0.1351421 0.12425317 0.1189658 0.13059853 0.15009846 0.16231865 0.16178569][0.17715599 0.18852681 0.19310603 0.19404362 0.19395286 0.19249451 0.1855312 0.17447825 0.16010225 0.14335717 0.13135129 0.13497397 0.14732219 0.15482254 0.15341122][0.20706019 0.21849433 0.21926056 0.21899638 0.22091633 0.22222468 0.21556166 0.20203304 0.18303871 0.15968892 0.13971378 0.13342197 0.13638464 0.13787143 0.13602653][0.22968212 0.24006516 0.2364618 0.23445298 0.23758964 0.24143003 0.235874 0.22063492 0.19764501 0.16805121 0.14104679 0.12588011 0.11965726 0.11531448 0.11367311][0.23239882 0.24188885 0.236281 0.2342895 0.23898911 0.24569319 0.24291699 0.22813645 0.2031133 0.16922273 0.13744132 0.11569585 0.10173269 0.0924077 0.091032036][0.20836692 0.21817146 0.21496107 0.21708781 0.22576603 0.23662157 0.23785761 0.22491224 0.19916837 0.16244122 0.12767868 0.10195198 0.083276115 0.07162232 0.07147263][0.16051842 0.17126404 0.17458133 0.18506892 0.20084396 0.2171814 0.22304831 0.21242283 0.18637465 0.14758641 0.11094471 0.0837644 0.064410768 0.054438025 0.057657644][0.10430816 0.11697605 0.12934208 0.15038002 0.17428605 0.19543175 0.20451596 0.19527677 0.16861387 0.12818034 0.090038195 0.0625144 0.045075595 0.039562307 0.047465306][0.058259416 0.072874889 0.093991019 0.12483177 0.15566328 0.17910683 0.18826142 0.17850444 0.15091704 0.10951668 0.069811381 0.041662671 0.026918748 0.026795128 0.039624996][0.033540782 0.049382877 0.076399282 0.11360352 0.14815743 0.17053653 0.17645673 0.16466536 0.13700178 0.0965125 0.056432188 0.027982652 0.016149001 0.021380257 0.03917706][0.031749256 0.049503908 0.07962095 0.11894134 0.15323447 0.1713368 0.1711269 0.15513617 0.12682255 0.087732971 0.04855578 0.021287113 0.013315499 0.024323685 0.047722172]]...]
INFO - root - 2017-12-11 03:14:28.249625: step 4010, loss = 0.70, batch loss = 0.64 (29.6 examples/sec; 0.270 sec/batch; 24h:39m:43s remains)
INFO - root - 2017-12-11 03:14:30.886558: step 4020, loss = 0.69, batch loss = 0.64 (28.5 examples/sec; 0.281 sec/batch; 25h:39m:02s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-SGD-lr0.001-clip50-initfromfixqian30k-fixqian
INFO - root - 2017-12-11 03:14:33.514544: step 4030, loss = 0.69, batch loss = 0.63 (30.6 examples/sec; 0.261 sec/batch; 23h:50m:27s remains)
INFO - root - 2017-12-11 03:14:36.177347: step 4040, loss = 0.68, batch loss = 0.62 (29.5 examples/sec; 0.271 sec/batch; 24h:45m:18s remains)
INFO - root - 2017-12-11 03:14:38.796577: step 4050, loss = 0.68, batch loss = 0.63 (31.1 examples/sec; 0.257 sec/batch; 23h:26m:28s remains)
INFO - root - 2017-12-11 03:14:41.401472: step 4060, loss = 0.69, batch loss = 0.63 (31.5 examples/sec; 0.254 sec/batch; 23h:08m:54s remains)
INFO - root - 2017-12-11 03:14:43.990526: step 4070, loss = 0.70, batch loss = 0.64 (30.4 examples/sec; 0.263 sec/batch; 23h:59m:46s remains)
INFO - root - 2017-12-11 03:14:46.622557: step 4080, loss = 0.70, batch loss = 0.64 (29.8 examples/sec; 0.268 sec/batch; 24h:28m:49s remains)
INFO - root - 2017-12-11 03:14:49.266658: step 4090, loss = 0.69, batch loss = 0.63 (30.5 examples/sec; 0.263 sec/batch; 23h:57m:11s remains)
INFO - root - 2017-12-11 03:14:51.959935: step 4100, loss = 0.69, batch loss = 0.64 (29.4 examples/sec; 0.272 sec/batch; 24h:47m:55s remains)
2017-12-11 03:14:52.416640: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.042730786 -0.044726647 -0.046784882 -0.050049394 -0.053759709 -0.057599861 -0.061338924 -0.06219963 -0.058603976 -0.052529126 -0.04738405 -0.044766407 -0.044571772 -0.046205346 -0.048977487][-0.0038197767 -0.0022401849 -0.0032662693 -0.0081720985 -0.016432351 -0.027612211 -0.039897434 -0.048054721 -0.048065286 -0.041584995 -0.034418955 -0.029972572 -0.028399995 -0.028437734 -0.028836435][0.046936527 0.060324121 0.067680478 0.065390788 0.052379023 0.029560998 0.0015352351 -0.023103919 -0.035849467 -0.036539126 -0.032273244 -0.028569194 -0.026226319 -0.023190681 -0.017563788][0.11315964 0.155195 0.18702717 0.1991798 0.18693198 0.15157309 0.10111602 0.048650809 0.010405203 -0.0089177629 -0.017225385 -0.022083296 -0.025525739 -0.02360261 -0.013304739][0.17899556 0.25997317 0.32862672 0.3677983 0.36789596 0.32992527 0.26334119 0.18410863 0.1167891 0.072010122 0.042946745 0.019658525 -0.00091329962 -0.010549408 -0.0033653527][0.21512572 0.330673 0.43792236 0.51287782 0.54024297 0.5191586 0.45614719 0.36503667 0.27560207 0.20550816 0.15211277 0.10378598 0.056181651 0.023586527 0.018293831][0.20203178 0.33137 0.46264455 0.56965125 0.63250238 0.64651966 0.61143178 0.53173709 0.43665275 0.35016891 0.27677718 0.20396298 0.12658288 0.067825355 0.047883518][0.14411105 0.25836948 0.38655934 0.50517333 0.59357548 0.64302772 0.6452105 0.59400088 0.51299834 0.428711 0.35178304 0.27010036 0.17849837 0.10742289 0.08269611][0.059153024 0.13713273 0.23676686 0.33967131 0.42883986 0.4939155 0.52150184 0.49994975 0.44658735 0.38591385 0.33009124 0.26666749 0.18997602 0.13248271 0.12080438][-0.028886415 0.005172539 0.062277328 0.12966338 0.19612455 0.25237724 0.28562486 0.285877 0.26513076 0.24218886 0.22612825 0.20375061 0.16781373 0.14629304 0.1614659][-0.089371137 -0.091054156 -0.073130056 -0.044040553 -0.010118315 0.022625025 0.046873294 0.058602646 0.065635622 0.080271356 0.10626286 0.12998706 0.14237922 0.16311409 0.20580332][-0.10601473 -0.12633796 -0.13248488 -0.13053259 -0.12379157 -0.11417754 -0.10263533 -0.087667435 -0.063747436 -0.023472155 0.031793639 0.089376524 0.13967814 0.19198838 0.24994582][-0.087170362 -0.11158121 -0.12666334 -0.13714978 -0.14403547 -0.14637846 -0.14176975 -0.12645699 -0.095560126 -0.044787109 0.022015851 0.094889283 0.1638988 0.22810878 0.28383878][-0.052816171 -0.0738222 -0.089126304 -0.10172766 -0.11016291 -0.11231072 -0.10573135 -0.086592436 -0.05072375 0.003667454 0.072149038 0.14612788 0.21410683 0.2689569 0.30570778][-0.015988374 -0.031477287 -0.043945979 -0.054000307 -0.057804145 -0.052604958 -0.035996486 -0.0039163553 0.046503335 0.11247975 0.18587184 0.25462237 0.30549476 0.33132187 0.3337999]]...]
INFO - root - 2017-12-11 03:14:55.050441: step 4110, loss = 0.69, batch loss = 0.63 (30.2 examples/sec; 0.265 sec/batch; 24h:08m:15s remains)
INFO - root - 2017-12-11 03:14:57.727371: step 4120, loss = 0.71, batch loss = 0.65 (29.4 examples/sec; 0.272 sec/batch; 24h:47m:58s remains)
INFO - root - 2017-12-11 03:15:00.348750: step 4130, loss = 0.69, batch loss = 0.63 (30.4 examples/sec; 0.263 sec/batch; 23h:59m:48s remains)
INFO - root - 2017-12-11 03:15:03.035264: step 4140, loss = 0.69, batch loss = 0.63 (29.7 examples/sec; 0.269 sec/batch; 24h:33m:47s remains)
INFO - root - 2017-12-11 03:15:05.684483: step 4150, loss = 0.70, batch loss = 0.64 (30.7 examples/sec; 0.260 sec/batch; 23h:44m:50s remains)
INFO - root - 2017-12-11 03:15:08.354664: step 4160, loss = 0.70, batch loss = 0.64 (31.9 examples/sec; 0.250 sec/batch; 22h:50m:18s remains)
INFO - root - 2017-12-11 03:15:11.002925: step 4170, loss = 0.70, batch loss = 0.64 (30.9 examples/sec; 0.259 sec/batch; 23h:36m:10s remains)
INFO - root - 2017-12-11 03:15:13.655096: step 4180, loss = 0.69, batch loss = 0.64 (30.4 examples/sec; 0.263 sec/batch; 23h:59m:06s remains)
INFO - root - 2017-12-11 03:15:16.283251: step 4190, loss = 0.70, batch loss = 0.64 (31.2 examples/sec; 0.257 sec/batch; 23h:24m:49s remains)
INFO - root - 2017-12-11 03:15:18.922853: step 4200, loss = 0.70, batch loss = 0.64 (30.6 examples/sec; 0.262 sec/batch; 23h:51m:48s remains)
2017-12-11 03:15:19.359201: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.21342993 0.2413871 0.24635634 0.22568747 0.18354619 0.13396806 0.098314814 0.089941554 0.10813285 0.13841239 0.16729082 0.18443216 0.18299277 0.16422987 0.14300179][0.21638016 0.24131165 0.24164638 0.21655351 0.175397 0.1349276 0.11293861 0.11528335 0.13577873 0.1596514 0.17670241 0.18028139 0.16841669 0.14688976 0.13030031][0.19961044 0.21740271 0.21516603 0.19376655 0.16518801 0.14534435 0.14529145 0.16171372 0.18376596 0.19794579 0.19917807 0.18634528 0.16271383 0.13948454 0.13147275][0.17324713 0.18213712 0.18056956 0.17011073 0.16241892 0.17010747 0.19611277 0.2289148 0.25453645 0.26079512 0.24684294 0.21714106 0.1812501 0.15608172 0.15685169][0.14300399 0.14782023 0.15234956 0.15747398 0.17341679 0.20839439 0.25791559 0.3044742 0.33268008 0.33095223 0.30142546 0.25520378 0.2088003 0.18385826 0.19410247][0.11358031 0.12269702 0.13862303 0.16102754 0.19870722 0.25511745 0.32048678 0.37383083 0.39925277 0.38613442 0.34067994 0.28192809 0.2313581 0.21170926 0.23253694][0.0837149 0.10316517 0.13397548 0.17322642 0.22799017 0.29804265 0.371192 0.42507008 0.44373307 0.41840497 0.35995585 0.29377705 0.24257293 0.22693704 0.25229648][0.058402482 0.0898108 0.13372226 0.1861867 0.25271469 0.33128813 0.40832254 0.4600572 0.46988571 0.432322 0.36253744 0.28969142 0.23617885 0.22012186 0.24423741][0.053167116 0.090219468 0.13798411 0.19538675 0.26714993 0.34986737 0.42837486 0.47657761 0.47710922 0.42854238 0.34948024 0.27037048 0.2135919 0.19614065 0.21883617][0.06970457 0.10111587 0.14118066 0.19424598 0.26382416 0.34425783 0.41924179 0.461188 0.45278504 0.39645779 0.31275904 0.23130137 0.17447032 0.15888385 0.18494089][0.11382318 0.12836251 0.1493971 0.18798755 0.24594259 0.31545377 0.38049641 0.41338003 0.39734992 0.33823335 0.25722823 0.18145573 0.13262625 0.12686348 0.16482677][0.18643816 0.17720714 0.17087324 0.18681467 0.22568081 0.27848864 0.33031258 0.35415286 0.33411071 0.27788046 0.20583388 0.14215943 0.10744593 0.11696826 0.17133281][0.26072139 0.22897469 0.19621214 0.1894204 0.20930682 0.24632661 0.28581387 0.30178192 0.27983266 0.22858647 0.16751876 0.11909007 0.1028558 0.13223042 0.20602737][0.312223 0.26484215 0.21296306 0.18768506 0.19124638 0.21431956 0.24246232 0.25206041 0.23131493 0.18884234 0.14237384 0.11207768 0.1154898 0.16257074 0.24951908][0.32658246 0.27666506 0.21774979 0.1815363 0.17171019 0.18045042 0.19564769 0.19843191 0.18071438 0.15062775 0.12265161 0.11289362 0.1348684 0.19328424 0.28125513]]...]
INFO - root - 2017-12-11 03:15:21.987811: step 4210, loss = 0.69, batch loss = 0.64 (31.2 examples/sec; 0.257 sec/batch; 23h:24m:09s remains)
INFO - root - 2017-12-11 03:15:24.647628: step 4220, loss = 0.69, batch loss = 0.64 (30.8 examples/sec; 0.260 sec/batch; 23h:43m:22s remains)
INFO - root - 2017-12-11 03:15:27.292826: step 4230, loss = 0.70, batch loss = 0.64 (30.8 examples/sec; 0.260 sec/batch; 23h:41m:31s remains)
INFO - root - 2017-12-11 03:15:29.970602: step 4240, loss = 0.69, batch loss = 0.64 (29.8 examples/sec; 0.268 sec/batch; 24h:26m:25s remains)
INFO - root - 2017-12-11 03:15:32.640503: step 4250, loss = 0.69, batch loss = 0.63 (29.9 examples/sec; 0.268 sec/batch; 24h:25m:48s remains)
INFO - root - 2017-12-11 03:15:35.285043: step 4260, loss = 0.70, batch loss = 0.64 (29.8 examples/sec; 0.269 sec/batch; 24h:30m:38s remains)
INFO - root - 2017-12-11 03:15:37.911568: step 4270, loss = 0.69, batch loss = 0.63 (30.9 examples/sec; 0.259 sec/batch; 23h:34m:14s remains)
INFO - root - 2017-12-11 03:15:40.550097: step 4280, loss = 0.69, batch loss = 0.63 (30.5 examples/sec; 0.262 sec/batch; 23h:53m:44s remains)
INFO - root - 2017-12-11 03:15:43.214009: step 4290, loss = 0.71, batch loss = 0.65 (28.7 examples/sec; 0.279 sec/batch; 25h:24m:07s remains)
INFO - root - 2017-12-11 03:15:45.860668: step 4300, loss = 0.68, batch loss = 0.63 (29.4 examples/sec; 0.272 sec/batch; 24h:46m:22s remains)
2017-12-11 03:15:46.285530: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.059745517 -0.060996991 -0.060728792 -0.060231972 -0.059954576 -0.060455877 -0.062184859 -0.064732954 -0.06739676 -0.069326624 -0.070022352 -0.069899455 -0.069036581 -0.067278132 -0.064463496][-0.068137556 -0.065927312 -0.060174331 -0.05365096 -0.048608821 -0.046871141 -0.048653916 -0.053273115 -0.060288623 -0.068667695 -0.076585695 -0.082331412 -0.084735766 -0.08391694 -0.079979591][-0.065636732 -0.055483602 -0.038938429 -0.020575337 -0.0043785889 0.0071566496 0.01292122 0.011824498 0.0023824242 -0.015621272 -0.039188296 -0.061722 -0.0771797 -0.084357262 -0.084449559][-0.05082665 -0.027377361 0.0064368737 0.044663515 0.082413927 0.11620864 0.14179604 0.15395434 0.14745213 0.11914755 0.071372978 0.017157303 -0.028517464 -0.058366314 -0.073064119][-0.024746843 0.016600473 0.0744786 0.14205801 0.21380419 0.28313696 0.34040919 0.37526676 0.37863094 0.3428382 0.2658903 0.16516179 0.068292551 -0.0037256586 -0.046477795][0.0037740632 0.063474163 0.14768864 0.25006104 0.36511782 0.48080271 0.57940507 0.64391029 0.6617654 0.62066007 0.5133189 0.35834125 0.19705969 0.069405369 -0.010740822][0.022030137 0.094161317 0.19845089 0.33084977 0.48788181 0.65211368 0.79563063 0.89090323 0.92082316 0.87201589 0.73678339 0.5344795 0.31589675 0.13735898 0.022181809][0.021859087 0.096353047 0.20661259 0.35127532 0.5312866 0.72830361 0.90659869 1.0251082 1.0594172 0.99850357 0.84192574 0.61266321 0.36517093 0.16215195 0.030678377][0.0024816284 0.0677728 0.16624719 0.29813132 0.46933243 0.66683477 0.85436457 0.98089558 1.0138862 0.94400334 0.780725 0.55320483 0.3144834 0.12253449 0.0014622193][-0.029943224 0.016611986 0.089228794 0.18817501 0.32182851 0.48532104 0.64951652 0.76351053 0.79147846 0.72458792 0.57835543 0.38443303 0.18899693 0.038747232 -0.049485147][-0.063718095 -0.040497452 0.0010343628 0.060136888 0.14406911 0.253174 0.36892861 0.45144802 0.47016826 0.41747025 0.30880797 0.17170002 0.04090986 -0.05143318 -0.096535221][-0.086328126 -0.084188163 -0.07038188 -0.046420526 -0.0075269085 0.047669441 0.10959151 0.1540076 0.16160038 0.1272548 0.062407963 -0.013827487 -0.079043664 -0.11529461 -0.12128492][-0.090879947 -0.10090505 -0.10494442 -0.10431151 -0.095860481 -0.078965679 -0.057939772 -0.044306967 -0.046090376 -0.065186493 -0.094730422 -0.12401414 -0.14072566 -0.13779242 -0.11908344][-0.082976617 -0.094037227 -0.10277332 -0.11047881 -0.11491637 -0.11614759 -0.11579737 -0.11825175 -0.1246533 -0.13449052 -0.14380197 -0.14730811 -0.13960521 -0.12018806 -0.094784565][-0.07356438 -0.0795893 -0.083910041 -0.089004122 -0.0940279 -0.099199928 -0.10453101 -0.11113044 -0.11716744 -0.12160672 -0.1219187 -0.11577118 -0.10208301 -0.083273627 -0.063838348]]...]
INFO - root - 2017-12-11 03:15:48.885909: step 4310, loss = 0.71, batch loss = 0.65 (30.5 examples/sec; 0.263 sec/batch; 23h:56m:58s remains)
INFO - root - 2017-12-11 03:15:51.539386: step 4320, loss = 0.71, batch loss = 0.65 (30.3 examples/sec; 0.264 sec/batch; 24h:06m:06s remains)
INFO - root - 2017-12-11 03:15:54.146557: step 4330, loss = 0.69, batch loss = 0.64 (30.5 examples/sec; 0.262 sec/batch; 23h:55m:14s remains)
INFO - root - 2017-12-11 03:15:56.777988: step 4340, loss = 0.69, batch loss = 0.63 (29.9 examples/sec; 0.267 sec/batch; 24h:22m:10s remains)
INFO - root - 2017-12-11 03:15:59.432586: step 4350, loss = 0.70, batch loss = 0.64 (30.8 examples/sec; 0.260 sec/batch; 23h:40m:59s remains)
INFO - root - 2017-12-11 03:16:02.091616: step 4360, loss = 0.70, batch loss = 0.64 (31.1 examples/sec; 0.257 sec/batch; 23h:24m:50s remains)
INFO - root - 2017-12-11 03:16:04.731182: step 4370, loss = 0.70, batch loss = 0.65 (28.5 examples/sec; 0.281 sec/batch; 25h:35m:23s remains)
INFO - root - 2017-12-11 03:16:07.472260: step 4380, loss = 0.69, batch loss = 0.63 (29.6 examples/sec; 0.270 sec/batch; 24h:38m:57s remains)
INFO - root - 2017-12-11 03:16:10.095572: step 4390, loss = 0.69, batch loss = 0.64 (31.1 examples/sec; 0.257 sec/batch; 23h:26m:18s remains)
INFO - root - 2017-12-11 03:16:12.746252: step 4400, loss = 0.70, batch loss = 0.64 (30.3 examples/sec; 0.264 sec/batch; 24h:02m:09s remains)
2017-12-11 03:16:13.209799: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.17496176 0.19919266 0.21937507 0.23354281 0.25333995 0.28465158 0.32616505 0.36472291 0.38917652 0.39169776 0.36865267 0.3333047 0.30932617 0.31717831 0.35519612][0.252478 0.29021329 0.31559336 0.32842153 0.34500533 0.37355098 0.41303378 0.44700131 0.46185762 0.45461172 0.42632157 0.39319485 0.37779027 0.39355952 0.432224][0.36133242 0.40200019 0.41974154 0.42215815 0.42909688 0.44904763 0.48031625 0.50468922 0.50795293 0.49335587 0.46780041 0.44776052 0.44943032 0.47483093 0.51004833][0.48399022 0.5151518 0.5172469 0.50812244 0.50634813 0.51755911 0.53977835 0.55595434 0.55281788 0.53766483 0.52197307 0.51992667 0.53914553 0.57041949 0.59800303][0.56867713 0.58884865 0.5825159 0.57553118 0.5781979 0.58990961 0.60871363 0.62147689 0.61720008 0.60401613 0.597334 0.60807914 0.63533026 0.66222876 0.67589074][0.578486 0.59840268 0.60340196 0.62032813 0.64646214 0.67296761 0.69633561 0.70818287 0.70183647 0.685168 0.67742693 0.68638492 0.70480925 0.71399975 0.70788664][0.51203221 0.54536939 0.57981056 0.63601309 0.69786668 0.74854523 0.78081191 0.78989691 0.774663 0.74401408 0.72136831 0.71209288 0.70600331 0.688294 0.66120267][0.39159128 0.44662488 0.51547307 0.60913271 0.70196635 0.77231884 0.80927223 0.81017727 0.77975845 0.729341 0.68475574 0.649404 0.61310732 0.56836295 0.52460086][0.25470072 0.32775745 0.42029271 0.53348196 0.6378749 0.71151042 0.74249083 0.73028529 0.68431693 0.61787945 0.5556131 0.49925217 0.44079813 0.37995902 0.3302297][0.13078043 0.20636451 0.30012202 0.40655598 0.49859285 0.55800653 0.57511771 0.55152887 0.49876842 0.42961007 0.36416262 0.30299756 0.24128398 0.18363257 0.14365868][0.036731593 0.096738316 0.16995981 0.24837281 0.31152603 0.34744978 0.35048237 0.32342628 0.27669531 0.22062126 0.16933148 0.12291726 0.079542406 0.045626421 0.031525925][-0.026731011 0.0086587258 0.05170368 0.095130496 0.12672164 0.14097711 0.13615631 0.11498723 0.08452744 0.051957 0.025633996 0.0057322239 -0.0073374189 -0.00653121 0.014782885][-0.0659227 -0.052856382 -0.036296103 -0.02113251 -0.012467049 -0.011150288 -0.016385403 -0.02649199 -0.037972249 -0.046319328 -0.047714405 -0.041485697 -0.025261257 0.00952612 0.067125894][-0.082363307 -0.083475515 -0.082626671 -0.082714312 -0.084661365 -0.087330081 -0.089212663 -0.089671813 -0.087647133 -0.080257811 -0.065849639 -0.04276865 -0.0069269603 0.0524994 0.13911799][-0.08478713 -0.0922273 -0.097473778 -0.10227851 -0.1059887 -0.10755184 -0.10606621 -0.10151426 -0.093872309 -0.08107657 -0.061857723 -0.033484038 0.0096569946 0.080342308 0.182631]]...]
INFO - root - 2017-12-11 03:16:15.865648: step 4410, loss = 0.71, batch loss = 0.66 (30.0 examples/sec; 0.267 sec/batch; 24h:18m:24s remains)
INFO - root - 2017-12-11 03:16:18.545720: step 4420, loss = 0.71, batch loss = 0.65 (31.0 examples/sec; 0.258 sec/batch; 23h:32m:32s remains)
INFO - root - 2017-12-11 03:16:21.236072: step 4430, loss = 0.70, batch loss = 0.64 (29.5 examples/sec; 0.271 sec/batch; 24h:40m:29s remains)
INFO - root - 2017-12-11 03:16:23.911880: step 4440, loss = 0.69, batch loss = 0.63 (30.8 examples/sec; 0.259 sec/batch; 23h:38m:01s remains)
INFO - root - 2017-12-11 03:16:26.588882: step 4450, loss = 0.69, batch loss = 0.63 (30.4 examples/sec; 0.263 sec/batch; 23h:56m:59s remains)
INFO - root - 2017-12-11 03:16:29.286278: step 4460, loss = 0.71, batch loss = 0.65 (31.3 examples/sec; 0.255 sec/batch; 23h:15m:14s remains)
INFO - root - 2017-12-11 03:16:31.942816: step 4470, loss = 0.70, batch loss = 0.64 (30.6 examples/sec; 0.262 sec/batch; 23h:50m:59s remains)
INFO - root - 2017-12-11 03:16:34.546456: step 4480, loss = 0.70, batch loss = 0.64 (31.2 examples/sec; 0.256 sec/batch; 23h:19m:48s remains)
INFO - root - 2017-12-11 03:16:37.171101: step 4490, loss = 0.70, batch loss = 0.65 (30.0 examples/sec; 0.267 sec/batch; 24h:17m:31s remains)
INFO - root - 2017-12-11 03:16:39.745208: step 4500, loss = 0.69, batch loss = 0.63 (31.9 examples/sec; 0.251 sec/batch; 22h:50m:29s remains)
2017-12-11 03:16:40.167517: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.13456167 0.14921229 0.15662715 0.15873064 0.16399918 0.18258356 0.21153899 0.2344588 0.23248261 0.20010908 0.14216189 0.071556859 0.0055522309 -0.0412097 -0.0637022][0.23215446 0.25308383 0.26172265 0.25916862 0.25619617 0.26693019 0.29114932 0.31146681 0.3046625 0.26367274 0.19270386 0.10626192 0.024787527 -0.033964667 -0.062925071][0.32497165 0.35952803 0.37802124 0.37722075 0.36601767 0.3616159 0.36901242 0.37423304 0.35468435 0.30421749 0.22458416 0.1289065 0.038620394 -0.027410623 -0.060151417][0.39561227 0.44772094 0.48416677 0.49484116 0.48244894 0.4646427 0.45182002 0.43578249 0.39752266 0.33410048 0.24600752 0.14370315 0.047604289 -0.023079835 -0.058036875][0.43814591 0.49935544 0.55006814 0.57511073 0.5694052 0.54718876 0.52163374 0.48900625 0.4344233 0.35936719 0.26444203 0.15742382 0.056968164 -0.017627068 -0.054839175][0.469253 0.52415687 0.57736939 0.61310774 0.61868167 0.60131472 0.57310975 0.53216279 0.466611 0.38297588 0.28340784 0.17322569 0.068981864 -0.0096948175 -0.050053194][0.49059629 0.530129 0.57616067 0.61617315 0.63271713 0.62581092 0.60330713 0.56223053 0.49218509 0.40384224 0.30148616 0.18907776 0.081760518 -0.0002474518 -0.043639008][0.49856609 0.51803738 0.54852128 0.58416229 0.60681349 0.61235756 0.60286343 0.57130218 0.50570565 0.4176293 0.31387579 0.19909112 0.088918589 0.004830658 -0.040335681][0.48707026 0.48149362 0.48790434 0.50913638 0.5299899 0.54492056 0.55048817 0.53471112 0.48179165 0.40084788 0.300564 0.18778563 0.079503953 -0.0020793153 -0.0451391][0.44356251 0.41481078 0.39620236 0.39864096 0.41114062 0.42817745 0.4425863 0.43968683 0.40161678 0.33389547 0.24531129 0.14414975 0.047790911 -0.022964083 -0.058158863][0.3573769 0.31865436 0.28697142 0.27636805 0.27910286 0.29087648 0.30409488 0.30541322 0.2783097 0.22522488 0.1538146 0.072389148 -0.0028568269 -0.054854289 -0.076699056][0.22910011 0.19515795 0.16559944 0.15200406 0.14838111 0.15253824 0.15919217 0.15880179 0.13858168 0.099279851 0.047241114 -0.010281437 -0.059781224 -0.089227714 -0.0953387][0.0887897 0.06652455 0.048609417 0.040433016 0.036443841 0.035803284 0.036210414 0.032576669 0.016750179 -0.010575818 -0.044125505 -0.0784886 -0.104038 -0.11372748 -0.10708527][-0.027305642 -0.038777061 -0.044545736 -0.044820424 -0.044955987 -0.04569459 -0.047506485 -0.052263163 -0.064170919 -0.081761435 -0.10032847 -0.11650153 -0.12452655 -0.12126712 -0.10775942][-0.09889213 -0.10427238 -0.10313032 -0.098695129 -0.095201313 -0.093514122 -0.093675867 -0.096294478 -0.10337815 -0.11311904 -0.12155909 -0.12632763 -0.12431838 -0.11496317 -0.099826254]]...]
INFO - root - 2017-12-11 03:16:42.818379: step 4510, loss = 0.67, batch loss = 0.61 (30.2 examples/sec; 0.265 sec/batch; 24h:07m:25s remains)
INFO - root - 2017-12-11 03:16:45.466676: step 4520, loss = 0.71, batch loss = 0.65 (28.2 examples/sec; 0.283 sec/batch; 25h:48m:30s remains)
INFO - root - 2017-12-11 03:16:48.116152: step 4530, loss = 0.69, batch loss = 0.63 (30.2 examples/sec; 0.265 sec/batch; 24h:07m:15s remains)
INFO - root - 2017-12-11 03:16:50.753170: step 4540, loss = 0.70, batch loss = 0.64 (30.2 examples/sec; 0.265 sec/batch; 24h:06m:36s remains)
INFO - root - 2017-12-11 03:16:53.388372: step 4550, loss = 0.69, batch loss = 0.63 (31.0 examples/sec; 0.258 sec/batch; 23h:32m:03s remains)
INFO - root - 2017-12-11 03:16:56.048106: step 4560, loss = 0.69, batch loss = 0.63 (30.2 examples/sec; 0.265 sec/batch; 24h:06m:08s remains)
INFO - root - 2017-12-11 03:16:58.670037: step 4570, loss = 0.68, batch loss = 0.62 (31.3 examples/sec; 0.256 sec/batch; 23h:18m:03s remains)
INFO - root - 2017-12-11 03:17:01.306164: step 4580, loss = 0.71, batch loss = 0.65 (29.1 examples/sec; 0.275 sec/batch; 25h:02m:15s remains)
INFO - root - 2017-12-11 03:17:03.962338: step 4590, loss = 0.70, batch loss = 0.64 (29.9 examples/sec; 0.268 sec/batch; 24h:24m:15s remains)
INFO - root - 2017-12-11 03:17:06.666909: step 4600, loss = 0.73, batch loss = 0.67 (29.4 examples/sec; 0.272 sec/batch; 24h:46m:30s remains)
2017-12-11 03:17:07.143431: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.038936809 -0.025887916 -0.012865104 -0.00089996296 0.0085747708 0.013039411 0.010608538 0.0022956077 -0.0079900222 -0.017582994 -0.025243558 -0.031144083 -0.033743784 -0.028372027 -0.014252448][-0.023226794 0.0003587208 0.025741983 0.050051104 0.068803117 0.07724449 0.073416263 0.059348971 0.040288866 0.021024317 0.0046196347 -0.0080171656 -0.01456676 -0.008452463 0.01171532][-0.0025608502 0.035680056 0.079921558 0.12234195 0.15346113 0.16661292 0.16107014 0.14001776 0.10910734 0.075738907 0.046045057 0.023367528 0.0098834289 0.013230296 0.036240947][0.016796967 0.071510144 0.13747373 0.19951268 0.24285069 0.26137441 0.25664872 0.23181397 0.19012579 0.14122523 0.095195368 0.058603458 0.033830605 0.028439492 0.04750618][0.031066194 0.099587679 0.18401346 0.26156777 0.31433311 0.34003732 0.34343496 0.32407343 0.27836677 0.21706663 0.15448414 0.10122227 0.060018618 0.038445137 0.044508][0.035683054 0.11122598 0.20566297 0.29099545 0.34975138 0.38567895 0.4054628 0.40067354 0.35827011 0.28966948 0.21389779 0.14559513 0.087840565 0.04783636 0.036769837][0.030044947 0.10469433 0.19910747 0.284012 0.34547123 0.39263555 0.43209538 0.44568112 0.4116216 0.34202787 0.26017019 0.18323216 0.11418993 0.059988506 0.034039292][0.018940361 0.086680815 0.17269059 0.25028259 0.31024942 0.36448678 0.41856286 0.4478201 0.42529976 0.36247921 0.2839767 0.20716509 0.13497315 0.074109174 0.03904644][0.011306969 0.072070546 0.14762571 0.21533315 0.26961115 0.32230663 0.37864172 0.41359219 0.40070292 0.34993196 0.28356868 0.21605176 0.14940779 0.090547785 0.053999186][0.012484368 0.0706406 0.13839869 0.19656226 0.24186726 0.28417519 0.32943961 0.35808396 0.34936404 0.3115375 0.26215345 0.21103874 0.15818955 0.10971047 0.078456625][0.018646562 0.077547356 0.14081544 0.19142169 0.22713792 0.25443929 0.28035873 0.29384133 0.28289533 0.25480071 0.222322 0.1907033 0.15693107 0.12425387 0.10241415][0.022842253 0.083052851 0.14399527 0.18992704 0.21855175 0.23239595 0.23860872 0.23447186 0.21712077 0.19399053 0.17431509 0.16001576 0.14491202 0.12772121 0.11453979][0.019354764 0.077352136 0.13480157 0.17696393 0.20127746 0.20699732 0.19961111 0.1819201 0.15823729 0.13690969 0.12491608 0.12199589 0.12001566 0.11358082 0.10573112][0.0050075818 0.055181198 0.10549968 0.14288825 0.16470776 0.16778211 0.15475708 0.13052732 0.10350312 0.082767509 0.074024357 0.0762793 0.080887616 0.080332562 0.075317718][-0.018023239 0.018364852 0.056740262 0.086398043 0.10499321 0.10825138 0.096151859 0.073040746 0.048072226 0.029836223 0.022962606 0.026543977 0.033182375 0.035401192 0.03214211]]...]
INFO - root - 2017-12-11 03:17:09.754559: step 4610, loss = 0.70, batch loss = 0.64 (31.0 examples/sec; 0.258 sec/batch; 23h:30m:00s remains)
INFO - root - 2017-12-11 03:17:12.402021: step 4620, loss = 0.69, batch loss = 0.63 (29.2 examples/sec; 0.274 sec/batch; 24h:57m:22s remains)
INFO - root - 2017-12-11 03:17:15.071976: step 4630, loss = 0.70, batch loss = 0.64 (31.5 examples/sec; 0.254 sec/batch; 23h:05m:50s remains)
INFO - root - 2017-12-11 03:17:17.673402: step 4640, loss = 0.69, batch loss = 0.63 (29.0 examples/sec; 0.276 sec/batch; 25h:07m:59s remains)
INFO - root - 2017-12-11 03:17:20.322097: step 4650, loss = 0.70, batch loss = 0.64 (29.7 examples/sec; 0.269 sec/batch; 24h:29m:55s remains)
INFO - root - 2017-12-11 03:17:22.999633: step 4660, loss = 0.71, batch loss = 0.65 (30.6 examples/sec; 0.262 sec/batch; 23h:49m:27s remains)
INFO - root - 2017-12-11 03:17:25.631779: step 4670, loss = 0.70, batch loss = 0.64 (30.6 examples/sec; 0.261 sec/batch; 23h:46m:30s remains)
INFO - root - 2017-12-11 03:17:28.293668: step 4680, loss = 0.68, batch loss = 0.62 (30.0 examples/sec; 0.266 sec/batch; 24h:15m:51s remains)
INFO - root - 2017-12-11 03:17:30.968537: step 4690, loss = 0.70, batch loss = 0.64 (29.8 examples/sec; 0.269 sec/batch; 24h:28m:35s remains)
INFO - root - 2017-12-11 03:17:33.589555: step 4700, loss = 0.69, batch loss = 0.63 (29.6 examples/sec; 0.271 sec/batch; 24h:38m:32s remains)
2017-12-11 03:17:34.009250: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.25839087 0.2348557 0.20487873 0.18267079 0.17434764 0.17794316 0.18602507 0.19324361 0.19943303 0.21058124 0.22474496 0.23670414 0.2407456 0.23890509 0.2413525][0.34061843 0.29186827 0.23190308 0.18531597 0.16280054 0.16107918 0.16788954 0.17376575 0.17822812 0.19085014 0.21088639 0.23043771 0.23886558 0.23311475 0.22206631][0.40129465 0.33500269 0.25460643 0.19310188 0.1641525 0.16159998 0.16858424 0.17202519 0.17143992 0.17902565 0.19847056 0.22093844 0.23162507 0.22154349 0.1971073][0.41253963 0.34519538 0.26305914 0.202749 0.17900327 0.18251418 0.19329473 0.19516502 0.18755206 0.18459211 0.19569334 0.21443698 0.22360292 0.2095768 0.17547548][0.35934088 0.3087554 0.24763542 0.21036372 0.20937033 0.23089139 0.25236368 0.25577533 0.2418018 0.22679798 0.22543614 0.23380722 0.23402555 0.21042374 0.163442][0.25873116 0.23377238 0.20743571 0.20830677 0.2426569 0.29157543 0.32881278 0.33617103 0.31720519 0.29225641 0.27923384 0.27593422 0.26406607 0.22661926 0.16287322][0.13423668 0.13192795 0.14155582 0.18447804 0.2585111 0.3387624 0.39391983 0.40697595 0.38529825 0.35379928 0.33249611 0.32035834 0.2984817 0.24842589 0.1692612][0.024532426 0.03692608 0.073416896 0.14839154 0.25293562 0.35750672 0.42713115 0.44674045 0.42725122 0.39582923 0.37268704 0.35690352 0.3290253 0.26968971 0.1789429][-0.033426072 -0.015933527 0.031189822 0.1173049 0.23107915 0.34275573 0.41750628 0.44259807 0.43043897 0.40649724 0.38853163 0.37390146 0.34325752 0.27855396 0.18181027][-0.043237872 -0.029023554 0.013143075 0.089762285 0.19109114 0.29109645 0.35923514 0.38529134 0.38107058 0.36811614 0.36006522 0.3514241 0.32317746 0.2596747 0.16492845][-0.037260361 -0.033239089 -0.0070868535 0.047116444 0.12308236 0.20023747 0.25425991 0.27676418 0.27731717 0.27345672 0.27546608 0.27571866 0.25595331 0.20260753 0.1206587][-0.033350468 -0.04253519 -0.036801584 -0.0094024474 0.036937822 0.087540686 0.12428716 0.14009166 0.1416714 0.14290194 0.15181655 0.16041927 0.15196888 0.11475123 0.053961139][-0.033982109 -0.052957438 -0.063520364 -0.058932155 -0.039416417 -0.014014456 0.0052312147 0.013036471 0.013556423 0.0166611 0.028270762 0.041252948 0.0418398 0.021027101 -0.016158419][-0.035900693 -0.057694253 -0.075736284 -0.084713064 -0.083796352 -0.077724978 -0.073286846 -0.073648423 -0.076383546 -0.075303383 -0.06630744 -0.054561444 -0.049577717 -0.057232358 -0.073549964][-0.038979091 -0.057885848 -0.074746393 -0.086491443 -0.092416264 -0.094855212 -0.097419694 -0.1021106 -0.10775407 -0.11097585 -0.10879381 -0.10349288 -0.099835433 -0.10065546 -0.10347028]]...]
INFO - root - 2017-12-11 03:17:36.674250: step 4710, loss = 0.70, batch loss = 0.64 (31.0 examples/sec; 0.258 sec/batch; 23h:28m:53s remains)
INFO - root - 2017-12-11 03:17:39.327057: step 4720, loss = 0.69, batch loss = 0.63 (29.9 examples/sec; 0.267 sec/batch; 24h:20m:43s remains)
INFO - root - 2017-12-11 03:17:41.988187: step 4730, loss = 0.70, batch loss = 0.64 (30.6 examples/sec; 0.262 sec/batch; 23h:48m:47s remains)
INFO - root - 2017-12-11 03:17:44.601719: step 4740, loss = 0.70, batch loss = 0.64 (30.2 examples/sec; 0.265 sec/batch; 24h:05m:21s remains)
INFO - root - 2017-12-11 03:17:47.235547: step 4750, loss = 0.69, batch loss = 0.64 (30.5 examples/sec; 0.262 sec/batch; 23h:51m:00s remains)
INFO - root - 2017-12-11 03:17:49.818507: step 4760, loss = 0.69, batch loss = 0.63 (30.2 examples/sec; 0.265 sec/batch; 24h:08m:23s remains)
INFO - root - 2017-12-11 03:17:52.471240: step 4770, loss = 0.69, batch loss = 0.63 (30.4 examples/sec; 0.263 sec/batch; 23h:56m:38s remains)
INFO - root - 2017-12-11 03:17:55.205034: step 4780, loss = 0.72, batch loss = 0.66 (28.9 examples/sec; 0.277 sec/batch; 25h:12m:28s remains)
INFO - root - 2017-12-11 03:17:57.821337: step 4790, loss = 0.70, batch loss = 0.64 (30.3 examples/sec; 0.264 sec/batch; 24h:03m:33s remains)
INFO - root - 2017-12-11 03:18:00.475792: step 4800, loss = 0.68, batch loss = 0.62 (29.9 examples/sec; 0.267 sec/batch; 24h:18m:54s remains)
2017-12-11 03:18:00.938669: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.072961904 0.063023776 0.05675552 0.0506117 0.0416671 0.029259508 0.011837287 -0.0078570973 -0.024973357 -0.036395974 -0.041462276 -0.042906515 -0.044360261 -0.050324317 -0.059432797][0.1626828 0.14927448 0.13719475 0.12325272 0.10427731 0.08130984 0.052645456 0.020162897 -0.0095889289 -0.031412214 -0.04339879 -0.04901452 -0.052179385 -0.058278162 -0.066931993][0.25328249 0.24256392 0.23040959 0.21411476 0.19004886 0.16109084 0.1244607 0.081466645 0.039857678 0.0071235127 -0.014004666 -0.027232591 -0.036158565 -0.0457706 -0.05601421][0.31314102 0.31389138 0.31242988 0.3053042 0.28706726 0.26099122 0.22260121 0.17356923 0.12276845 0.079732165 0.048336379 0.024876583 0.0064997179 -0.010622337 -0.025643976][0.32430726 0.3440448 0.36520669 0.38189369 0.38439319 0.37350813 0.34266225 0.29412964 0.23697443 0.18341002 0.14063689 0.1059403 0.077142082 0.051079955 0.029600939][0.29038584 0.33324462 0.38448682 0.434969 0.46935493 0.48369235 0.46932909 0.42804971 0.36798397 0.30434746 0.24915586 0.20243879 0.16306458 0.12825522 0.10009115][0.23284028 0.29476273 0.37362531 0.45753339 0.52656484 0.57141924 0.5803349 0.5524987 0.49412948 0.42328709 0.35641828 0.29766262 0.24871352 0.20761664 0.17500442][0.16979502 0.23731256 0.33169165 0.43987435 0.53886473 0.61332053 0.64763844 0.637162 0.58577639 0.51205921 0.43527764 0.36513868 0.3080909 0.26413131 0.23183274][0.11840534 0.17668177 0.26971513 0.38595706 0.50129831 0.59507942 0.64813358 0.6525628 0.60995907 0.53735411 0.45501205 0.37854764 0.31909698 0.27903003 0.25477865][0.093584165 0.13320872 0.20899491 0.31296447 0.42326614 0.51689947 0.57382971 0.58449662 0.54885238 0.48073402 0.399222 0.32500288 0.27233467 0.24577065 0.23969486][0.090758137 0.11013237 0.16056488 0.23828566 0.32589966 0.40162531 0.44732833 0.45420772 0.42222744 0.3614389 0.28778133 0.22415964 0.18588617 0.17932312 0.19778907][0.10623855 0.1097459 0.13573036 0.18401472 0.2420211 0.29091567 0.31593853 0.31091875 0.27768537 0.22371341 0.16144338 0.11278504 0.091932647 0.10598907 0.14865255][0.13985905 0.13657449 0.14521812 0.16815145 0.19755772 0.21840833 0.21965238 0.19867073 0.15988104 0.10972928 0.057779588 0.022771131 0.016035596 0.044572808 0.10401517][0.18670271 0.18552609 0.18485279 0.18894494 0.19500439 0.19215421 0.17293009 0.13831197 0.094474413 0.047304213 0.0035582783 -0.022599481 -0.02330303 0.0083171772 0.0713173][0.2431476 0.24803583 0.24293703 0.23351645 0.22192518 0.20192097 0.16966751 0.12824231 0.083906576 0.041567653 0.0046665119 -0.018148683 -0.021990702 0.00067964173 0.053179257]]...]
INFO - root - 2017-12-11 03:18:03.592470: step 4810, loss = 0.69, batch loss = 0.63 (31.0 examples/sec; 0.258 sec/batch; 23h:27m:46s remains)
INFO - root - 2017-12-11 03:18:06.267187: step 4820, loss = 0.70, batch loss = 0.64 (29.9 examples/sec; 0.268 sec/batch; 24h:23m:34s remains)
INFO - root - 2017-12-11 03:18:08.978094: step 4830, loss = 0.70, batch loss = 0.64 (31.1 examples/sec; 0.257 sec/batch; 23h:23m:53s remains)
INFO - root - 2017-12-11 03:18:11.664512: step 4840, loss = 0.70, batch loss = 0.64 (30.3 examples/sec; 0.264 sec/batch; 24h:01m:16s remains)
INFO - root - 2017-12-11 03:18:14.290855: step 4850, loss = 0.69, batch loss = 0.63 (31.8 examples/sec; 0.251 sec/batch; 22h:53m:18s remains)
INFO - root - 2017-12-11 03:18:16.861551: step 4860, loss = 0.71, batch loss = 0.65 (31.9 examples/sec; 0.251 sec/batch; 22h:51m:09s remains)
INFO - root - 2017-12-11 03:18:19.495687: step 4870, loss = 0.70, batch loss = 0.64 (29.9 examples/sec; 0.267 sec/batch; 24h:19m:35s remains)
INFO - root - 2017-12-11 03:18:22.164241: step 4880, loss = 0.69, batch loss = 0.63 (30.4 examples/sec; 0.264 sec/batch; 23h:59m:02s remains)
INFO - root - 2017-12-11 03:18:24.787719: step 4890, loss = 0.72, batch loss = 0.66 (29.7 examples/sec; 0.269 sec/batch; 24h:28m:35s remains)
INFO - root - 2017-12-11 03:18:27.421367: step 4900, loss = 0.71, batch loss = 0.65 (30.9 examples/sec; 0.259 sec/batch; 23h:33m:50s remains)
2017-12-11 03:18:27.901267: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.17030405 0.16154893 0.13000371 0.086968675 0.048170116 0.01671405 -0.011843719 -0.037912808 -0.054968596 -0.060600985 -0.054919574 -0.041848466 -0.030619595 -0.025564289 -0.026649293][0.14943737 0.1320578 0.097787365 0.057607073 0.022832349 -0.0050522932 -0.03086764 -0.056146074 -0.074833639 -0.083042346 -0.080072775 -0.069833361 -0.05992429 -0.05391755 -0.052821703][0.14776917 0.13163322 0.1022052 0.069891922 0.041165255 0.016224934 -0.0082787052 -0.033772517 -0.054403361 -0.065294921 -0.0661169 -0.061237525 -0.056437548 -0.053960018 -0.054358628][0.16456351 0.15842186 0.13976569 0.11745819 0.094602354 0.071219936 0.046697251 0.02126294 0.0011316872 -0.00961543 -0.013087624 -0.015108422 -0.019284714 -0.025430406 -0.032519218][0.19672744 0.20443925 0.19736889 0.1829159 0.16362756 0.14034556 0.1152888 0.091321416 0.075089969 0.068352506 0.064439945 0.055191971 0.039670166 0.020732002 0.0017733612][0.23332535 0.25287253 0.25307882 0.24118832 0.22124603 0.19642499 0.17147675 0.15223506 0.14433576 0.14552803 0.14357905 0.12864751 0.10250264 0.069395751 0.035383593][0.26077467 0.285355 0.28536251 0.26918316 0.24295655 0.21353464 0.18890744 0.17731775 0.18125284 0.19394162 0.19809021 0.18271974 0.15100935 0.10724587 0.060124118][0.27868104 0.30077487 0.29205921 0.26395881 0.22510312 0.18675724 0.15959543 0.15317342 0.16649891 0.1893741 0.20181178 0.19219527 0.16297536 0.11679081 0.064120464][0.29783869 0.31590182 0.29649997 0.25336689 0.19723925 0.14336658 0.10482033 0.093173236 0.10573736 0.13100244 0.14891127 0.14768809 0.12816697 0.090334646 0.04431485][0.32046181 0.34067273 0.31708059 0.26382264 0.19217356 0.12001047 0.062589154 0.03493391 0.035313681 0.053128917 0.069966905 0.075052232 0.068238772 0.046852954 0.019197404][0.33013991 0.35741761 0.33993819 0.28880465 0.21309775 0.13078369 0.058243833 0.013277184 -0.0030912706 0.0012068634 0.010168747 0.015457249 0.018203752 0.014809704 0.010076447][0.32069018 0.35501587 0.35043746 0.31281266 0.24688207 0.16890559 0.093558528 0.03867593 0.0086116036 -0.00065146643 -0.0029233475 -0.0035126726 0.0028624611 0.013722668 0.02962565][0.29681751 0.33495483 0.34361556 0.32363278 0.27608731 0.21470463 0.14983539 0.095866583 0.059741005 0.042272788 0.030825442 0.022058243 0.026692338 0.044841208 0.07313443][0.25841707 0.29699227 0.31535974 0.3116495 0.28593606 0.249113 0.20520081 0.16246629 0.12956144 0.11142353 0.095317133 0.078156896 0.0754506 0.091312371 0.1202371][0.2025218 0.23553285 0.25620514 0.26301572 0.25797468 0.24885716 0.23237434 0.20896018 0.18780461 0.17612529 0.16047449 0.13677973 0.12305319 0.12828226 0.14678343]]...]
INFO - root - 2017-12-11 03:18:30.530449: step 4910, loss = 0.69, batch loss = 0.63 (30.5 examples/sec; 0.262 sec/batch; 23h:52m:28s remains)
INFO - root - 2017-12-11 03:18:33.178145: step 4920, loss = 0.68, batch loss = 0.63 (28.8 examples/sec; 0.278 sec/batch; 25h:16m:25s remains)
INFO - root - 2017-12-11 03:18:35.779330: step 4930, loss = 0.70, batch loss = 0.64 (30.2 examples/sec; 0.265 sec/batch; 24h:05m:54s remains)
INFO - root - 2017-12-11 03:18:38.430336: step 4940, loss = 0.70, batch loss = 0.64 (30.1 examples/sec; 0.266 sec/batch; 24h:12m:47s remains)
INFO - root - 2017-12-11 03:18:41.060296: step 4950, loss = 0.70, batch loss = 0.64 (30.4 examples/sec; 0.263 sec/batch; 23h:55m:09s remains)
INFO - root - 2017-12-11 03:18:43.717966: step 4960, loss = 0.71, batch loss = 0.65 (30.0 examples/sec; 0.266 sec/batch; 24h:14m:48s remains)
INFO - root - 2017-12-11 03:18:46.362764: step 4970, loss = 0.70, batch loss = 0.64 (30.0 examples/sec; 0.266 sec/batch; 24h:13m:42s remains)
INFO - root - 2017-12-11 03:18:48.979521: step 4980, loss = 0.70, batch loss = 0.64 (30.7 examples/sec; 0.261 sec/batch; 23h:43m:35s remains)
INFO - root - 2017-12-11 03:18:51.567871: step 4990, loss = 0.70, batch loss = 0.64 (30.5 examples/sec; 0.262 sec/batch; 23h:51m:52s remains)
INFO - root - 2017-12-11 03:18:54.226127: step 5000, loss = 0.70, batch loss = 0.64 (31.1 examples/sec; 0.257 sec/batch; 23h:23m:13s remains)
2017-12-11 03:18:54.663798: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.031266 0.032484695 0.1151241 0.19240916 0.24463187 0.26788422 0.27629057 0.28468066 0.2952202 0.30148244 0.30143535 0.30253834 0.31640083 0.34448773 0.37303054][-0.042419542 0.014924904 0.091992788 0.16695121 0.22365467 0.26101136 0.29275918 0.331777 0.37260383 0.40096682 0.41090217 0.41068333 0.41778663 0.43940449 0.46222958][-0.047916263 0.0021372377 0.07324934 0.1475753 0.21259376 0.26716536 0.32271668 0.38969472 0.45494443 0.49774963 0.50889838 0.49842405 0.48894185 0.49300328 0.50059009][-0.044032838 0.0063294452 0.081595242 0.16635421 0.24787064 0.32154834 0.39433861 0.475334 0.54660428 0.58568996 0.58344835 0.55257255 0.51886922 0.4978691 0.48414016][-0.035266817 0.020400049 0.1072012 0.20979199 0.31222042 0.4042092 0.48667526 0.56717175 0.62580729 0.64413822 0.61699444 0.56017709 0.50042462 0.45293394 0.41743428][-0.027923105 0.033063516 0.13127531 0.25078893 0.37132528 0.47664469 0.56118095 0.62944949 0.66256267 0.65072268 0.59567434 0.51525694 0.43441898 0.36622858 0.31585383][-0.024016282 0.04085106 0.14673954 0.27689466 0.40751407 0.51776797 0.59552222 0.64170039 0.64193743 0.59785563 0.51845849 0.42285275 0.33197042 0.25688636 0.20565139][-0.023199877 0.043994021 0.15295078 0.28541127 0.41521636 0.51866043 0.57967228 0.59761512 0.56564039 0.49614206 0.40392092 0.30787846 0.22326042 0.15890932 0.12185233][-0.026225213 0.040348917 0.14600509 0.2710757 0.38887006 0.47494563 0.51334268 0.5041185 0.44906667 0.36780483 0.27805713 0.19676296 0.133508 0.0941469 0.080346316][-0.030610688 0.032722939 0.12954953 0.23985676 0.33793893 0.40084252 0.41603523 0.38675657 0.32219192 0.24455824 0.17116764 0.11568724 0.081951477 0.071907431 0.080740809][-0.033533033 0.025084583 0.11116505 0.20560488 0.28434521 0.32662839 0.32421005 0.284594 0.22246198 0.15908338 0.10961075 0.083087668 0.077637926 0.091345228 0.11418809][-0.034238979 0.019267427 0.09659747 0.18100746 0.25021493 0.28518066 0.2794255 0.24299574 0.19266555 0.14749908 0.12071005 0.11722329 0.13017614 0.15408853 0.17766622][-0.031899072 0.016577553 0.087623522 0.16861536 0.23943828 0.28147674 0.28663024 0.26481074 0.23230921 0.20596714 0.19767368 0.20890187 0.22945912 0.25141174 0.26629311][-0.026568292 0.018274644 0.085750371 0.16874337 0.24887437 0.306056 0.32844302 0.32502481 0.31052038 0.29998532 0.3040939 0.32205683 0.34248486 0.35650423 0.36007631][-0.022421097 0.019159852 0.083845519 0.16931489 0.25796881 0.32762712 0.36360544 0.37479535 0.37405735 0.37438321 0.384859 0.40381929 0.4207204 0.42706257 0.42247981]]...]
INFO - root - 2017-12-11 03:18:57.328922: step 5010, loss = 0.70, batch loss = 0.64 (30.9 examples/sec; 0.259 sec/batch; 23h:33m:45s remains)
INFO - root - 2017-12-11 03:18:59.981248: step 5020, loss = 0.68, batch loss = 0.62 (31.2 examples/sec; 0.256 sec/batch; 23h:19m:20s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-SGD-lr0.001-clip50-initfromfixqian30k-fixqian
INFO - root - 2017-12-11 03:19:02.709481: step 5030, loss = 0.69, batch loss = 0.63 (30.3 examples/sec; 0.264 sec/batch; 24h:00m:22s remains)
INFO - root - 2017-12-11 03:19:05.388283: step 5040, loss = 0.70, batch loss = 0.65 (30.8 examples/sec; 0.260 sec/batch; 23h:39m:14s remains)
INFO - root - 2017-12-11 03:19:08.022712: step 5050, loss = 0.69, batch loss = 0.63 (30.3 examples/sec; 0.264 sec/batch; 24h:02m:04s remains)
INFO - root - 2017-12-11 03:19:10.653842: step 5060, loss = 0.68, batch loss = 0.62 (30.4 examples/sec; 0.263 sec/batch; 23h:55m:05s remains)
INFO - root - 2017-12-11 03:19:13.279563: step 5070, loss = 0.70, batch loss = 0.64 (30.3 examples/sec; 0.264 sec/batch; 24h:02m:30s remains)
INFO - root - 2017-12-11 03:19:15.916434: step 5080, loss = 0.69, batch loss = 0.64 (30.9 examples/sec; 0.259 sec/batch; 23h:33m:16s remains)
INFO - root - 2017-12-11 03:19:18.589393: step 5090, loss = 0.70, batch loss = 0.64 (29.4 examples/sec; 0.272 sec/batch; 24h:46m:34s remains)
INFO - root - 2017-12-11 03:19:21.263019: step 5100, loss = 0.71, batch loss = 0.66 (29.4 examples/sec; 0.272 sec/batch; 24h:45m:37s remains)
2017-12-11 03:19:21.744682: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.047789458 0.078556016 0.11641419 0.1521664 0.18115152 0.19618481 0.18873462 0.16401194 0.12609373 0.089035325 0.060099535 0.035730653 0.017531209 0.0060647279 0.0073314137][0.077837132 0.1098683 0.14885588 0.18397398 0.21050066 0.22348736 0.21648556 0.19211228 0.15326084 0.11421173 0.08179564 0.053723019 0.031556506 0.016195271 0.014531499][0.10638297 0.12938403 0.15663636 0.17934896 0.19644369 0.20808454 0.21058787 0.19999617 0.17248453 0.13827655 0.10376748 0.071869045 0.046365585 0.028621458 0.025513822][0.14510979 0.15430366 0.16527051 0.17430268 0.18510099 0.2020358 0.22158881 0.23016813 0.2151181 0.1819602 0.13879636 0.097936407 0.067068838 0.047150079 0.043031015][0.17924374 0.17659307 0.17670117 0.18190153 0.19852018 0.23056996 0.27096969 0.29597333 0.28579104 0.24507138 0.18680575 0.13382356 0.096936271 0.07508754 0.070164554][0.19456199 0.18666264 0.18870324 0.20798613 0.24810545 0.30581641 0.3668794 0.39902133 0.3796941 0.31913349 0.23927356 0.17244583 0.13032989 0.10787952 0.10383921][0.19466382 0.18581419 0.1986953 0.24302934 0.31468129 0.39908698 0.47438648 0.5041039 0.46711573 0.3813234 0.27856788 0.19873285 0.15310349 0.13252494 0.1325178][0.18991359 0.18345441 0.20811476 0.27292088 0.36502168 0.45983103 0.53236353 0.54933006 0.49509525 0.39186066 0.27611271 0.1902146 0.14477418 0.128972 0.13521543][0.17949879 0.17942041 0.21210735 0.28247193 0.37133485 0.45048794 0.49962625 0.49613169 0.43285233 0.33055165 0.22172947 0.14303315 0.10318917 0.0931334 0.10374278][0.16598669 0.17688191 0.21106037 0.26723742 0.32569489 0.36397079 0.37373552 0.34722295 0.28647771 0.20648052 0.12723564 0.071389318 0.043811992 0.039477 0.05110104][0.15686616 0.18003201 0.21178043 0.24422008 0.26093966 0.25073135 0.21895409 0.17146896 0.11727566 0.064562745 0.019648373 -0.0097646564 -0.023271432 -0.022603136 -0.012168015][0.16208921 0.19687165 0.22473411 0.23485413 0.21642178 0.16889372 0.10763942 0.047004227 -0.001633566 -0.03637547 -0.059511028 -0.072166614 -0.076739155 -0.07352715 -0.065407507][0.19123316 0.23650828 0.25951567 0.2503618 0.20598991 0.13507642 0.059197504 -0.0048071025 -0.047817349 -0.073530816 -0.087698475 -0.094331883 -0.0961184 -0.093147606 -0.087767594][0.24086231 0.29505846 0.310569 0.28181586 0.21489665 0.12831461 0.048289314 -0.009920611 -0.043216713 -0.061355609 -0.071693793 -0.078419805 -0.082121052 -0.081753194 -0.07883469][0.29444098 0.35516539 0.36113015 0.31209126 0.22355685 0.12415741 0.044134773 -0.0043030474 -0.025464213 -0.034560595 -0.041197542 -0.04934733 -0.056165174 -0.058170725 -0.055354495]]...]
INFO - root - 2017-12-11 03:19:24.397328: step 5110, loss = 0.72, batch loss = 0.66 (31.2 examples/sec; 0.256 sec/batch; 23h:18m:46s remains)
INFO - root - 2017-12-11 03:19:26.962887: step 5120, loss = 0.70, batch loss = 0.64 (29.8 examples/sec; 0.269 sec/batch; 24h:26m:10s remains)
INFO - root - 2017-12-11 03:19:29.614753: step 5130, loss = 0.71, batch loss = 0.65 (30.9 examples/sec; 0.259 sec/batch; 23h:31m:49s remains)
INFO - root - 2017-12-11 03:19:32.245863: step 5140, loss = 0.69, batch loss = 0.63 (30.8 examples/sec; 0.260 sec/batch; 23h:37m:02s remains)
INFO - root - 2017-12-11 03:19:34.912149: step 5150, loss = 0.68, batch loss = 0.63 (30.5 examples/sec; 0.263 sec/batch; 23h:52m:41s remains)
INFO - root - 2017-12-11 03:19:37.569542: step 5160, loss = 0.71, batch loss = 0.65 (28.4 examples/sec; 0.281 sec/batch; 25h:34m:52s remains)
INFO - root - 2017-12-11 03:19:40.226912: step 5170, loss = 0.69, batch loss = 0.63 (28.9 examples/sec; 0.277 sec/batch; 25h:08m:47s remains)
INFO - root - 2017-12-11 03:19:42.861916: step 5180, loss = 0.69, batch loss = 0.63 (31.6 examples/sec; 0.254 sec/batch; 23h:02m:59s remains)
INFO - root - 2017-12-11 03:19:45.504167: step 5190, loss = 0.69, batch loss = 0.64 (30.7 examples/sec; 0.260 sec/batch; 23h:40m:32s remains)
INFO - root - 2017-12-11 03:19:48.100911: step 5200, loss = 0.70, batch loss = 0.65 (30.5 examples/sec; 0.262 sec/batch; 23h:49m:03s remains)
2017-12-11 03:19:48.567908: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.018279526 -0.01019336 0.007769818 0.046637435 0.11230222 0.19548672 0.27044648 0.30982891 0.30533037 0.26627314 0.21217713 0.16466288 0.1423797 0.1554167 0.1971142][-0.019166155 -0.011683804 0.0058282739 0.044178311 0.11058074 0.19690675 0.27741036 0.32158536 0.31843677 0.27806008 0.22241163 0.17346922 0.14990601 0.16090311 0.20003122][-0.019000825 -0.011879074 0.0062024957 0.0447722 0.11131741 0.19886097 0.28335309 0.33287826 0.33320957 0.29351172 0.23634052 0.18464732 0.15684599 0.16177519 0.19300064][-0.018867677 -0.0098768463 0.011469132 0.052893896 0.12094739 0.20924184 0.29592913 0.34957945 0.35416207 0.31688717 0.25895783 0.20422013 0.1708964 0.16785806 0.18840787][-0.011018109 0.0029196015 0.029092493 0.073043033 0.14032465 0.22481743 0.3069205 0.35805586 0.36336195 0.32813278 0.270688 0.21434507 0.17700696 0.16721344 0.17925943][0.0060351184 0.026114602 0.057045337 0.10191451 0.16589987 0.24307048 0.31576911 0.35982984 0.36312607 0.3297441 0.27421805 0.21813816 0.17879917 0.16359605 0.1696123][0.034786034 0.061826542 0.097458892 0.14242652 0.20162411 0.26950514 0.33099529 0.36716837 0.36803085 0.33716285 0.28493878 0.23099394 0.19140506 0.17172803 0.17289324][0.071760014 0.10406048 0.14132102 0.18321915 0.23438215 0.28978065 0.33787784 0.36597505 0.36560073 0.33928791 0.29286432 0.24455485 0.2088061 0.18879476 0.18784121][0.10910266 0.14341883 0.17851304 0.214537 0.25511724 0.29551649 0.32806942 0.34622511 0.34435874 0.32368475 0.28622106 0.24776359 0.22031486 0.20528954 0.20619194][0.1394538 0.17191838 0.20193018 0.23026082 0.2588819 0.28316721 0.29910395 0.30551377 0.3008351 0.28541207 0.258742 0.23290239 0.21718679 0.2123439 0.2205179][0.15328819 0.18184899 0.20603442 0.22661024 0.24416982 0.25433406 0.25589216 0.25176054 0.24414095 0.23377945 0.21903858 0.20712999 0.2041692 0.21115425 0.22868977][0.15941934 0.18322307 0.20093432 0.21368119 0.22181833 0.22155973 0.21353205 0.2032042 0.19446705 0.18800236 0.18228137 0.18060458 0.18625417 0.20062074 0.22381328][0.16016163 0.17966744 0.19166106 0.19808403 0.19967726 0.19353144 0.18111332 0.16871448 0.1607714 0.15785685 0.15962796 0.16675362 0.179909 0.19927488 0.22460814][0.15814587 0.17414375 0.18187614 0.18403645 0.18189819 0.1731862 0.15970773 0.14727384 0.14003648 0.13863474 0.14436924 0.15669398 0.17431727 0.19567956 0.22026072][0.15020369 0.16219591 0.16568592 0.16392536 0.15861784 0.14813252 0.13447113 0.12248566 0.11509733 0.11267402 0.11747979 0.12920997 0.14605306 0.16588821 0.18766753]]...]
INFO - root - 2017-12-11 03:19:51.163582: step 5210, loss = 0.71, batch loss = 0.65 (30.7 examples/sec; 0.261 sec/batch; 23h:43m:05s remains)
INFO - root - 2017-12-11 03:19:53.814755: step 5220, loss = 0.69, batch loss = 0.63 (29.2 examples/sec; 0.274 sec/batch; 24h:55m:15s remains)
INFO - root - 2017-12-11 03:19:56.452653: step 5230, loss = 0.69, batch loss = 0.63 (30.3 examples/sec; 0.264 sec/batch; 23h:59m:30s remains)
INFO - root - 2017-12-11 03:19:59.042081: step 5240, loss = 0.71, batch loss = 0.65 (30.4 examples/sec; 0.263 sec/batch; 23h:54m:38s remains)
INFO - root - 2017-12-11 03:20:01.719058: step 5250, loss = 0.70, batch loss = 0.64 (30.6 examples/sec; 0.262 sec/batch; 23h:47m:39s remains)
INFO - root - 2017-12-11 03:20:04.357121: step 5260, loss = 0.72, batch loss = 0.66 (29.8 examples/sec; 0.268 sec/batch; 24h:22m:53s remains)
INFO - root - 2017-12-11 03:20:07.046350: step 5270, loss = 0.70, batch loss = 0.64 (29.2 examples/sec; 0.274 sec/batch; 24h:53m:11s remains)
INFO - root - 2017-12-11 03:20:09.683779: step 5280, loss = 0.69, batch loss = 0.63 (27.2 examples/sec; 0.294 sec/batch; 26h:45m:12s remains)
INFO - root - 2017-12-11 03:20:12.306858: step 5290, loss = 0.68, batch loss = 0.62 (31.2 examples/sec; 0.257 sec/batch; 23h:18m:54s remains)
INFO - root - 2017-12-11 03:20:14.929743: step 5300, loss = 0.69, batch loss = 0.63 (31.5 examples/sec; 0.254 sec/batch; 23h:04m:17s remains)
2017-12-11 03:20:15.357777: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.036548968 0.0315939 0.029006409 0.034190554 0.048544466 0.068571545 0.088167451 0.10050493 0.10130786 0.092971377 0.081035115 0.069678061 0.057596553 0.043637026 0.031163584][0.080896907 0.071898825 0.064939156 0.069323078 0.087670617 0.115794 0.14513253 0.16511369 0.16828586 0.1571123 0.1393705 0.12203354 0.10423466 0.083689921 0.063906655][0.13711914 0.12186375 0.10723403 0.10750297 0.12730806 0.16219211 0.2009981 0.22892128 0.23467632 0.2201802 0.19513355 0.17030199 0.1467547 0.12154778 0.096939333][0.19315897 0.17061013 0.14624122 0.1400263 0.15934236 0.19963859 0.24679622 0.28159958 0.28878066 0.26951334 0.23530972 0.20103322 0.1709853 0.14271481 0.11637599][0.2343573 0.20459925 0.17005596 0.15624574 0.1734785 0.21718329 0.26998645 0.30887762 0.31565061 0.29087478 0.24779582 0.20467262 0.16932021 0.1406486 0.11654151][0.25205559 0.21686696 0.17404084 0.15314798 0.16710234 0.21141328 0.26589784 0.30525556 0.30994341 0.28003743 0.23032676 0.18126251 0.14350718 0.11748805 0.099354967][0.24623913 0.20936079 0.16268885 0.13733326 0.14797318 0.19018391 0.24230866 0.27860445 0.2798146 0.24585356 0.19266465 0.14180195 0.10506661 0.083556913 0.07255993][0.22356795 0.18985893 0.14511515 0.11906701 0.12696896 0.1649213 0.21170336 0.24284638 0.24061769 0.20521304 0.1528932 0.10453637 0.071376063 0.054079946 0.047939103][0.19185868 0.16483386 0.12665886 0.1034562 0.10958581 0.14210312 0.18198331 0.20739485 0.20282578 0.16906069 0.12157271 0.078954339 0.050494436 0.035913102 0.031112418][0.16136581 0.14114021 0.11102181 0.091654353 0.095500007 0.12109095 0.15297276 0.17278336 0.167438 0.1380202 0.0982275 0.06351988 0.040785007 0.028671613 0.024035206][0.1426522 0.12812085 0.10531543 0.089331187 0.0900964 0.10748167 0.13043055 0.14426355 0.1382807 0.11348911 0.081458606 0.054509889 0.037309807 0.027901955 0.024067797][0.13653645 0.12564898 0.10810523 0.094637744 0.092196673 0.10138025 0.11515028 0.12231445 0.11478952 0.09375336 0.068794467 0.04928242 0.037814565 0.0320869 0.030583696][0.13548008 0.12535527 0.11039109 0.09842144 0.0932296 0.095282629 0.10071974 0.10160845 0.0927336 0.075895652 0.058945704 0.048387423 0.04480271 0.045142587 0.04775973][0.12925404 0.1176075 0.10341625 0.092873216 0.086827025 0.0847887 0.085043415 0.082596332 0.074677959 0.063536078 0.055535242 0.054874986 0.060494319 0.0680576 0.0747913][0.11269204 0.099816434 0.086363383 0.078005709 0.073793776 0.072061032 0.072090887 0.071009658 0.067400843 0.063154832 0.063398667 0.070925489 0.083720841 0.096176229 0.10398695]]...]
INFO - root - 2017-12-11 03:20:17.993412: step 5310, loss = 0.71, batch loss = 0.65 (30.3 examples/sec; 0.264 sec/batch; 23h:59m:36s remains)
INFO - root - 2017-12-11 03:20:20.619936: step 5320, loss = 0.71, batch loss = 0.65 (31.0 examples/sec; 0.258 sec/batch; 23h:27m:54s remains)
INFO - root - 2017-12-11 03:20:23.288771: step 5330, loss = 0.71, batch loss = 0.65 (30.2 examples/sec; 0.265 sec/batch; 24h:03m:13s remains)
INFO - root - 2017-12-11 03:20:25.953927: step 5340, loss = 0.69, batch loss = 0.63 (29.5 examples/sec; 0.271 sec/batch; 24h:37m:42s remains)
INFO - root - 2017-12-11 03:20:28.554895: step 5350, loss = 0.71, batch loss = 0.65 (32.1 examples/sec; 0.249 sec/batch; 22h:38m:13s remains)
INFO - root - 2017-12-11 03:20:31.224362: step 5360, loss = 0.71, batch loss = 0.65 (30.2 examples/sec; 0.265 sec/batch; 24h:03m:09s remains)
INFO - root - 2017-12-11 03:20:33.868017: step 5370, loss = 0.69, batch loss = 0.64 (30.7 examples/sec; 0.261 sec/batch; 23h:42m:08s remains)
INFO - root - 2017-12-11 03:20:36.503551: step 5380, loss = 0.70, batch loss = 0.64 (29.3 examples/sec; 0.273 sec/batch; 24h:46m:38s remains)
INFO - root - 2017-12-11 03:20:39.137831: step 5390, loss = 0.71, batch loss = 0.65 (30.0 examples/sec; 0.267 sec/batch; 24h:13m:12s remains)
INFO - root - 2017-12-11 03:20:41.775737: step 5400, loss = 0.69, batch loss = 0.63 (30.5 examples/sec; 0.263 sec/batch; 23h:51m:15s remains)
2017-12-11 03:20:42.187606: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.014234887 0.037694857 0.055015583 0.062036905 0.062425535 0.057162259 0.04577646 0.039452538 0.048196323 0.066633627 0.081891939 0.086119294 0.081067689 0.060081515 0.021570541][0.047738627 0.084539905 0.11486869 0.13306692 0.14170134 0.13933466 0.12429643 0.1119551 0.11720957 0.13529126 0.150063 0.15190491 0.14329135 0.11416654 0.061780665][0.082017981 0.13483033 0.18388723 0.22142828 0.24671859 0.25402629 0.23897471 0.21815129 0.21061271 0.21486948 0.21676365 0.20797716 0.1913542 0.153387 0.089536108][0.10304842 0.17293721 0.245489 0.30988261 0.36024395 0.385497 0.37685803 0.34811112 0.32004261 0.29647955 0.26996821 0.2382755 0.20550297 0.15699086 0.086295456][0.10240126 0.18735477 0.28416198 0.37876922 0.46045709 0.51311839 0.5216679 0.49230638 0.44359756 0.38466647 0.31815371 0.25141084 0.19348043 0.13224597 0.058806986][0.091346063 0.18479042 0.29792255 0.4149093 0.52318251 0.60423791 0.63684195 0.61689037 0.55761665 0.47012398 0.3666558 0.26495919 0.18122567 0.1093297 0.036868319][0.0812692 0.17445037 0.29114017 0.41588345 0.5372836 0.6374591 0.69124067 0.68590415 0.62896878 0.53064466 0.40860957 0.28678828 0.18779436 0.11170901 0.042583086][0.075995743 0.16346008 0.27493674 0.39639804 0.5182969 0.62507731 0.69040841 0.69885623 0.65533131 0.56692433 0.44995564 0.32783315 0.22603151 0.14858674 0.07819289][0.078609146 0.15906307 0.26089537 0.37108979 0.48186353 0.58057755 0.64344209 0.65908736 0.63444436 0.57308686 0.48334205 0.3806338 0.28860843 0.21145524 0.13382365][0.090263195 0.1623823 0.24954829 0.33976448 0.42739651 0.5026179 0.54715836 0.55861562 0.54914689 0.52072215 0.47128725 0.4038159 0.33561829 0.26626566 0.18390954][0.10189578 0.16305168 0.23149648 0.29726943 0.35668325 0.40153012 0.41963536 0.41802156 0.41436893 0.41191304 0.40042239 0.37110516 0.33236781 0.27717286 0.19710937][0.092850953 0.1402099 0.18941288 0.2329504 0.268178 0.28682959 0.28103894 0.26420307 0.25753132 0.26744756 0.27990052 0.27870882 0.26411429 0.22395802 0.15323868][0.053132188 0.085530862 0.11806775 0.1448271 0.16308188 0.16445643 0.14371821 0.11624093 0.10328902 0.11392196 0.1343917 0.14648031 0.14537276 0.11766884 0.061851054][0.0013489571 0.01978014 0.038280815 0.051503085 0.057397928 0.049092717 0.023088662 -0.007071848 -0.023834556 -0.018228956 -0.0011975623 0.011824847 0.015206372 -0.0017031613 -0.038041964][-0.042545252 -0.036440715 -0.030181853 -0.02878481 -0.032508478 -0.045156412 -0.068633884 -0.093630381 -0.10888725 -0.10825669 -0.098921746 -0.090770908 -0.087366566 -0.094912827 -0.11188681]]...]
INFO - root - 2017-12-11 03:20:44.864172: step 5410, loss = 0.69, batch loss = 0.63 (30.9 examples/sec; 0.259 sec/batch; 23h:32m:41s remains)
INFO - root - 2017-12-11 03:20:47.460968: step 5420, loss = 0.71, batch loss = 0.65 (30.4 examples/sec; 0.263 sec/batch; 23h:54m:57s remains)
INFO - root - 2017-12-11 03:20:50.206643: step 5430, loss = 0.69, batch loss = 0.64 (30.2 examples/sec; 0.265 sec/batch; 24h:02m:55s remains)
INFO - root - 2017-12-11 03:20:52.909587: step 5440, loss = 0.71, batch loss = 0.65 (29.2 examples/sec; 0.274 sec/batch; 24h:52m:51s remains)
INFO - root - 2017-12-11 03:20:55.552845: step 5450, loss = 0.68, batch loss = 0.63 (30.9 examples/sec; 0.259 sec/batch; 23h:30m:16s remains)
INFO - root - 2017-12-11 03:20:58.216562: step 5460, loss = 0.69, batch loss = 0.64 (29.9 examples/sec; 0.268 sec/batch; 24h:19m:11s remains)
INFO - root - 2017-12-11 03:21:00.876725: step 5470, loss = 0.70, batch loss = 0.64 (29.9 examples/sec; 0.267 sec/batch; 24h:16m:40s remains)
INFO - root - 2017-12-11 03:21:03.524568: step 5480, loss = 0.71, batch loss = 0.65 (31.1 examples/sec; 0.257 sec/batch; 23h:21m:24s remains)
INFO - root - 2017-12-11 03:21:06.174077: step 5490, loss = 0.71, batch loss = 0.65 (29.6 examples/sec; 0.270 sec/batch; 24h:33m:49s remains)
INFO - root - 2017-12-11 03:21:08.882762: step 5500, loss = 0.69, batch loss = 0.63 (30.4 examples/sec; 0.263 sec/batch; 23h:53m:19s remains)
2017-12-11 03:21:09.390767: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.45633981 0.50031441 0.47555605 0.39141229 0.29191458 0.22847848 0.21960455 0.25283206 0.302352 0.35194406 0.3971535 0.45585537 0.54758161 0.63345408 0.63581765][0.53306592 0.5930081 0.57063723 0.48047304 0.37600568 0.3110221 0.30280164 0.33211851 0.36296982 0.37332803 0.3679266 0.38195494 0.44622815 0.52648252 0.53934705][0.6093511 0.68553692 0.66993982 0.5802598 0.47752157 0.41702834 0.41250452 0.43646526 0.44417927 0.4096179 0.34640861 0.30458567 0.323981 0.37926549 0.39284751][0.65969765 0.75163728 0.75187433 0.67672437 0.59005129 0.54708731 0.55555189 0.57841319 0.56550521 0.48923805 0.36947963 0.27062702 0.24112162 0.26458573 0.2707586][0.6760903 0.78294849 0.80758047 0.762704 0.70863169 0.69578284 0.72290152 0.74365532 0.70752674 0.59045279 0.4190082 0.26936531 0.19677848 0.19168505 0.18950571][0.66458094 0.78760308 0.84437853 0.84253311 0.83293748 0.85634035 0.90184194 0.91704816 0.85560006 0.70186222 0.48953241 0.30088106 0.19445048 0.16637245 0.15739596][0.6383785 0.77387565 0.85975784 0.89879447 0.93038249 0.98405665 1.0410855 1.0468582 0.96317655 0.78615093 0.55475789 0.34900519 0.22583956 0.18667999 0.17752288][0.57425404 0.71082062 0.81320196 0.88169074 0.94328022 1.0165014 1.0770924 1.0729465 0.97623968 0.79607683 0.57435441 0.38127422 0.26619649 0.23318677 0.23400246][0.46129885 0.58204788 0.68260169 0.76293141 0.83917385 0.91999823 0.97818732 0.96745551 0.86998391 0.70581585 0.51845753 0.36536554 0.28407225 0.27727672 0.30002573][0.32338321 0.41366464 0.49336979 0.56425536 0.63569695 0.70921123 0.75776982 0.74331683 0.65644294 0.52293557 0.38507503 0.28715041 0.25362748 0.28357121 0.33457571][0.18982519 0.24356379 0.29176113 0.33837736 0.38942337 0.44305283 0.47620815 0.46069106 0.39312911 0.29899612 0.21464516 0.17120235 0.18182287 0.24272659 0.31593764][0.092384964 0.11140978 0.125004 0.14003363 0.1618444 0.18874097 0.20393974 0.18831678 0.14172281 0.08515574 0.045961641 0.043920025 0.0842341 0.16146971 0.24421489][0.036765825 0.029819665 0.016300811 0.0054546008 0.0021454412 0.0058744415 0.00616027 -0.0088788839 -0.038440704 -0.067509234 -0.077683426 -0.057268113 -0.0057125925 0.070617385 0.1479663][0.027423665 0.0066129914 -0.021337144 -0.046348903 -0.064404637 -0.074924134 -0.083220616 -0.096422911 -0.11396758 -0.1268526 -0.12450332 -0.1000824 -0.054590147 0.0060551381 0.065592811][0.058483217 0.03569793 0.0047536967 -0.024542676 -0.048742451 -0.066067204 -0.078305349 -0.089801349 -0.1003369 -0.10565849 -0.10133542 -0.084066585 -0.05489818 -0.017671723 0.018314287]]...]
INFO - root - 2017-12-11 03:21:12.016712: step 5510, loss = 0.70, batch loss = 0.64 (30.1 examples/sec; 0.266 sec/batch; 24h:10m:11s remains)
INFO - root - 2017-12-11 03:21:14.747765: step 5520, loss = 0.71, batch loss = 0.65 (30.1 examples/sec; 0.266 sec/batch; 24h:08m:15s remains)
INFO - root - 2017-12-11 03:21:17.386141: step 5530, loss = 0.69, batch loss = 0.63 (29.8 examples/sec; 0.268 sec/batch; 24h:22m:03s remains)
INFO - root - 2017-12-11 03:21:20.028525: step 5540, loss = 0.69, batch loss = 0.64 (30.2 examples/sec; 0.265 sec/batch; 24h:04m:23s remains)
INFO - root - 2017-12-11 03:21:22.697176: step 5550, loss = 0.69, batch loss = 0.63 (30.6 examples/sec; 0.261 sec/batch; 23h:44m:31s remains)
INFO - root - 2017-12-11 03:21:25.388486: step 5560, loss = 0.71, batch loss = 0.65 (28.8 examples/sec; 0.278 sec/batch; 25h:14m:12s remains)
INFO - root - 2017-12-11 03:21:28.019541: step 5570, loss = 0.69, batch loss = 0.63 (31.8 examples/sec; 0.252 sec/batch; 22h:52m:50s remains)
INFO - root - 2017-12-11 03:21:30.630275: step 5580, loss = 0.70, batch loss = 0.64 (31.0 examples/sec; 0.258 sec/batch; 23h:24m:37s remains)
INFO - root - 2017-12-11 03:21:33.273486: step 5590, loss = 0.71, batch loss = 0.65 (30.0 examples/sec; 0.266 sec/batch; 24h:11m:18s remains)
INFO - root - 2017-12-11 03:21:35.873579: step 5600, loss = 0.71, batch loss = 0.65 (30.5 examples/sec; 0.263 sec/batch; 23h:50m:26s remains)
2017-12-11 03:21:36.299384: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.012364587 -0.0013188305 0.0074970936 0.013903528 0.019680474 0.020608388 0.013077337 -0.0034128849 -0.025413275 -0.046913117 -0.062136415 -0.067697585 -0.066209428 -0.064050011 -0.066079445][0.046856608 0.068504587 0.084947221 0.099626273 0.11607889 0.12608285 0.12196949 0.10178521 0.067336045 0.027334938 -0.0073664933 -0.026576208 -0.031008562 -0.031249529 -0.037457582][0.099386938 0.13070782 0.15634231 0.18458802 0.22007506 0.25005409 0.26062554 0.24369624 0.19653051 0.13173452 0.068466753 0.026942555 0.010826966 0.0061417241 -0.0042221034][0.13233882 0.17053556 0.20533684 0.25093457 0.3134855 0.37560058 0.4139505 0.40984991 0.35350165 0.260366 0.15949441 0.084451847 0.048064683 0.035350207 0.020798212][0.15529352 0.19628862 0.23540008 0.29422221 0.38261133 0.47954661 0.55148989 0.56788236 0.51063168 0.39395463 0.25543538 0.14319782 0.082176186 0.058713358 0.03915181][0.19646005 0.23706667 0.27254742 0.33418977 0.4386937 0.56374139 0.66841555 0.71213794 0.66664428 0.539153 0.37291861 0.22941744 0.14471735 0.10703744 0.076541394][0.25377974 0.29302967 0.31966949 0.37449747 0.4817836 0.62110555 0.74956566 0.820421 0.79476446 0.66982156 0.49118435 0.32932869 0.22782966 0.17612903 0.13111252][0.30209652 0.34116957 0.36001697 0.40426436 0.50349641 0.64190179 0.78032023 0.87086487 0.866529 0.75459981 0.57871938 0.41217607 0.3015691 0.23805681 0.17871556][0.31662968 0.35743716 0.37301666 0.40687549 0.48991287 0.61281866 0.74450183 0.84095377 0.85200894 0.75717157 0.5950796 0.43693674 0.32813334 0.26088947 0.19506279][0.28050381 0.3211064 0.3346574 0.35707274 0.417761 0.51480544 0.627031 0.71770263 0.73807228 0.66337162 0.52544868 0.388751 0.29357356 0.23256278 0.17060798][0.19730991 0.23341972 0.24513373 0.25791433 0.296855 0.36605337 0.45356682 0.5308069 0.55412173 0.4988243 0.39000344 0.28199545 0.20744035 0.15980078 0.11059296][0.0869945 0.11200479 0.11945136 0.12428953 0.14489616 0.18814941 0.24852166 0.30544129 0.3239848 0.28406379 0.20443426 0.12703198 0.076191746 0.046912763 0.01811881][-0.014963624 -0.0049927789 -0.0042412682 -0.0059511857 0.0016292831 0.024928272 0.061656062 0.098019324 0.1095169 0.082444414 0.029282648 -0.020933056 -0.051372405 -0.064504065 -0.073792905][-0.084258221 -0.0870732 -0.091984317 -0.097604431 -0.097376868 -0.086884327 -0.067189276 -0.046825878 -0.040947694 -0.057769097 -0.09016145 -0.12022657 -0.136803 -0.1399606 -0.13671286][-0.1204074 -0.13112423 -0.13907731 -0.14618675 -0.14937817 -0.14634176 -0.13773602 -0.12816086 -0.12572941 -0.13482153 -0.15220676 -0.1683487 -0.17614253 -0.17416926 -0.16505577]]...]
INFO - root - 2017-12-11 03:21:38.963558: step 5610, loss = 0.71, batch loss = 0.65 (29.6 examples/sec; 0.270 sec/batch; 24h:31m:43s remains)
INFO - root - 2017-12-11 03:21:41.599630: step 5620, loss = 0.71, batch loss = 0.65 (30.5 examples/sec; 0.262 sec/batch; 23h:47m:33s remains)
INFO - root - 2017-12-11 03:21:44.263700: step 5630, loss = 0.71, batch loss = 0.65 (30.5 examples/sec; 0.263 sec/batch; 23h:50m:32s remains)
INFO - root - 2017-12-11 03:21:46.958497: step 5640, loss = 0.69, batch loss = 0.63 (28.9 examples/sec; 0.277 sec/batch; 25h:09m:56s remains)
INFO - root - 2017-12-11 03:21:49.604210: step 5650, loss = 0.70, batch loss = 0.64 (31.2 examples/sec; 0.257 sec/batch; 23h:18m:31s remains)
INFO - root - 2017-12-11 03:21:52.247261: step 5660, loss = 0.70, batch loss = 0.64 (29.8 examples/sec; 0.268 sec/batch; 24h:21m:34s remains)
INFO - root - 2017-12-11 03:21:54.851278: step 5670, loss = 0.70, batch loss = 0.64 (30.4 examples/sec; 0.263 sec/batch; 23h:51m:44s remains)
INFO - root - 2017-12-11 03:21:57.467349: step 5680, loss = 0.68, batch loss = 0.62 (30.4 examples/sec; 0.263 sec/batch; 23h:53m:20s remains)
INFO - root - 2017-12-11 03:22:00.138675: step 5690, loss = 0.70, batch loss = 0.64 (30.2 examples/sec; 0.265 sec/batch; 24h:01m:30s remains)
INFO - root - 2017-12-11 03:22:02.788215: step 5700, loss = 0.69, batch loss = 0.63 (29.4 examples/sec; 0.273 sec/batch; 24h:44m:18s remains)
2017-12-11 03:22:03.207753: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.1106136 0.10854281 0.12266146 0.15090665 0.18530485 0.21646231 0.24018443 0.26280507 0.28095227 0.29454225 0.29750374 0.28918177 0.26684812 0.23105972 0.19621116][0.11955766 0.1147365 0.12679175 0.15571523 0.19354261 0.22725327 0.2500453 0.26723897 0.27787396 0.2830312 0.27850515 0.26339209 0.23572892 0.19866779 0.16824369][0.1174615 0.10424066 0.1055555 0.12519592 0.15756986 0.18889722 0.20907353 0.2200892 0.22279033 0.21954817 0.20886305 0.19049242 0.1639342 0.13293409 0.11235256][0.11648192 0.091914818 0.078217663 0.083428986 0.10542045 0.13100569 0.14733665 0.15199259 0.1472045 0.1365674 0.12152285 0.1029663 0.082276739 0.062608823 0.053720996][0.11683871 0.082991749 0.057323173 0.050961152 0.064455524 0.085270591 0.098528832 0.097532317 0.085582174 0.067991808 0.048769929 0.030226622 0.015114341 0.0055147326 0.0052034114][0.11470775 0.076621816 0.045986153 0.035785776 0.048134815 0.070266023 0.085228056 0.082719244 0.066017859 0.042284586 0.017523579 -0.0043267636 -0.019164674 -0.025697324 -0.024409246][0.1102044 0.073136292 0.044149987 0.037777647 0.056637388 0.086753219 0.10910961 0.10986262 0.090772785 0.060469132 0.027298773 -0.002384081 -0.023164041 -0.033546064 -0.035856631][0.10565444 0.072858222 0.049851526 0.051438957 0.079923958 0.12022947 0.15145943 0.15704988 0.13663357 0.099757284 0.056699343 0.016680889 -0.013087101 -0.030237714 -0.037118744][0.10172737 0.074152239 0.057706553 0.067058139 0.10347206 0.15096733 0.18790232 0.1965892 0.17491859 0.13285352 0.082473271 0.03492967 -0.0017274533 -0.023760034 -0.032913689][0.10021777 0.076296143 0.063859388 0.077119112 0.11633603 0.16557817 0.20325805 0.21217598 0.18947285 0.14523302 0.092375234 0.042858992 0.00462068 -0.018297035 -0.027066499][0.10125745 0.078800209 0.06704627 0.079406552 0.11559346 0.1606496 0.19394059 0.19998163 0.17615797 0.13247468 0.081972964 0.036358912 0.0023138886 -0.017250855 -0.023401085][0.10150965 0.079508439 0.06614802 0.074203156 0.10310727 0.13963751 0.1650853 0.16589145 0.14067578 0.099671058 0.055398416 0.017895341 -0.008235896 -0.021699941 -0.023919184][0.0984603 0.075937413 0.060035832 0.062116854 0.081487857 0.10722614 0.12322993 0.1181438 0.0920458 0.055691238 0.020515988 -0.0062892451 -0.022921765 -0.029773545 -0.028336938][0.094834171 0.071402058 0.052253105 0.04748816 0.056756515 0.071710669 0.079160757 0.069642685 0.0442462 0.014078731 -0.011008753 -0.02709895 -0.035163384 -0.036732666 -0.033047676][0.096700244 0.071983948 0.048739832 0.03656584 0.036444422 0.04261243 0.044279564 0.033012237 0.010024626 -0.013829876 -0.030477632 -0.038669627 -0.04123164 -0.040012993 -0.035800707]]...]
INFO - root - 2017-12-11 03:22:05.872681: step 5710, loss = 0.69, batch loss = 0.63 (30.5 examples/sec; 0.262 sec/batch; 23h:49m:30s remains)
INFO - root - 2017-12-11 03:22:08.531106: step 5720, loss = 0.70, batch loss = 0.64 (30.1 examples/sec; 0.266 sec/batch; 24h:08m:18s remains)
INFO - root - 2017-12-11 03:22:11.171663: step 5730, loss = 0.69, batch loss = 0.63 (30.1 examples/sec; 0.266 sec/batch; 24h:07m:48s remains)
INFO - root - 2017-12-11 03:22:13.824468: step 5740, loss = 0.70, batch loss = 0.64 (30.2 examples/sec; 0.265 sec/batch; 24h:00m:33s remains)
INFO - root - 2017-12-11 03:22:16.464471: step 5750, loss = 0.69, batch loss = 0.64 (30.9 examples/sec; 0.259 sec/batch; 23h:30m:53s remains)
INFO - root - 2017-12-11 03:22:19.104595: step 5760, loss = 0.71, batch loss = 0.65 (30.6 examples/sec; 0.261 sec/batch; 23h:43m:49s remains)
INFO - root - 2017-12-11 03:22:21.753027: step 5770, loss = 0.69, batch loss = 0.64 (29.3 examples/sec; 0.273 sec/batch; 24h:45m:18s remains)
INFO - root - 2017-12-11 03:22:24.416650: step 5780, loss = 0.71, batch loss = 0.66 (28.7 examples/sec; 0.279 sec/batch; 25h:19m:05s remains)
INFO - root - 2017-12-11 03:22:27.085397: step 5790, loss = 0.69, batch loss = 0.63 (28.9 examples/sec; 0.277 sec/batch; 25h:09m:06s remains)
INFO - root - 2017-12-11 03:22:29.706938: step 5800, loss = 0.69, batch loss = 0.63 (31.2 examples/sec; 0.257 sec/batch; 23h:16m:44s remains)
2017-12-11 03:22:30.149809: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.29153979 0.30301058 0.30602318 0.3065789 0.29651013 0.27507856 0.24447112 0.22326735 0.2191563 0.22489244 0.23863739 0.25598791 0.27378711 0.28881094 0.30777594][0.33151963 0.32363051 0.30725718 0.29398885 0.27657333 0.2569485 0.23294055 0.21640605 0.21303627 0.21840575 0.23392707 0.25122944 0.26275814 0.26736155 0.27430183][0.37561634 0.355958 0.32833418 0.30897632 0.28955764 0.27140215 0.24879816 0.22880314 0.2194526 0.22193989 0.23999304 0.2606377 0.27187333 0.27286789 0.27101967][0.44007277 0.43060568 0.41690534 0.41606587 0.41582137 0.41002467 0.385504 0.34632972 0.31019065 0.2889238 0.29022312 0.30020368 0.30580303 0.30490279 0.29823214][0.50146335 0.52533621 0.55239213 0.59475172 0.63802683 0.66362774 0.64499086 0.58122939 0.505244 0.44163477 0.40171969 0.37682375 0.36096576 0.35303757 0.34319007][0.50208229 0.56969178 0.65265995 0.75313193 0.85720974 0.93113482 0.93274748 0.85448784 0.74143112 0.62992054 0.53343946 0.45498306 0.40372786 0.38335919 0.37306488][0.42173758 0.52582604 0.65807694 0.80997378 0.97004157 1.0923152 1.123168 1.0479944 0.91514045 0.76701409 0.61837971 0.48667789 0.39843005 0.36522523 0.35926843][0.26613471 0.38650852 0.5427438 0.71912593 0.90723258 1.0565422 1.1127528 1.0569551 0.93162555 0.77592361 0.60348707 0.4427965 0.33270115 0.29292959 0.29417941][0.083898656 0.19410992 0.339313 0.50128144 0.67500812 0.8161447 0.88012725 0.84855688 0.75260085 0.62066019 0.46232361 0.30964857 0.20510189 0.17184511 0.1833847][-0.069257632 0.0089361006 0.11457038 0.2305824 0.35529363 0.45809928 0.50959474 0.49562705 0.4355039 0.34493357 0.22868378 0.11473417 0.039553452 0.023091378 0.043910645][-0.16861726 -0.13188072 -0.076590165 -0.01633881 0.049903303 0.10606489 0.13623479 0.13103391 0.10066335 0.051500507 -0.014945545 -0.078785844 -0.11534894 -0.11208581 -0.085195981][-0.20436044 -0.20087922 -0.18615967 -0.17025255 -0.15077494 -0.13309607 -0.12394951 -0.1284126 -0.14150193 -0.16131113 -0.18760121 -0.20912193 -0.21337421 -0.19553131 -0.16721015][-0.19112097 -0.20475921 -0.21124317 -0.21809681 -0.22310664 -0.22636031 -0.22986083 -0.23570494 -0.24132267 -0.24592979 -0.24955018 -0.24747813 -0.23560536 -0.21399558 -0.1904016][-0.15898146 -0.17646967 -0.18824561 -0.20022584 -0.21110135 -0.21973655 -0.226335 -0.23157072 -0.23415019 -0.23336217 -0.22926185 -0.2206507 -0.20717064 -0.19024692 -0.17441739][-0.12845312 -0.14226159 -0.15086856 -0.15930237 -0.16701929 -0.17323223 -0.17786314 -0.18128416 -0.1828936 -0.18190049 -0.17823765 -0.17197211 -0.16358286 -0.15400887 -0.14554644]]...]
INFO - root - 2017-12-11 03:22:32.788248: step 5810, loss = 0.70, batch loss = 0.64 (29.1 examples/sec; 0.275 sec/batch; 24h:57m:39s remains)
INFO - root - 2017-12-11 03:22:35.464835: step 5820, loss = 0.70, batch loss = 0.64 (30.0 examples/sec; 0.267 sec/batch; 24h:12m:05s remains)
INFO - root - 2017-12-11 03:22:38.138718: step 5830, loss = 0.70, batch loss = 0.64 (30.6 examples/sec; 0.262 sec/batch; 23h:44m:17s remains)
INFO - root - 2017-12-11 03:22:40.759835: step 5840, loss = 0.70, batch loss = 0.64 (30.8 examples/sec; 0.260 sec/batch; 23h:33m:15s remains)
INFO - root - 2017-12-11 03:22:43.447784: step 5850, loss = 0.69, batch loss = 0.63 (29.7 examples/sec; 0.270 sec/batch; 24h:28m:17s remains)
INFO - root - 2017-12-11 03:22:46.069583: step 5860, loss = 0.71, batch loss = 0.65 (31.2 examples/sec; 0.256 sec/batch; 23h:15m:16s remains)
INFO - root - 2017-12-11 03:22:48.756069: step 5870, loss = 0.69, batch loss = 0.64 (31.1 examples/sec; 0.257 sec/batch; 23h:19m:41s remains)
INFO - root - 2017-12-11 03:22:51.396123: step 5880, loss = 0.70, batch loss = 0.64 (31.7 examples/sec; 0.252 sec/batch; 22h:53m:36s remains)
INFO - root - 2017-12-11 03:22:54.024782: step 5890, loss = 0.70, batch loss = 0.64 (30.6 examples/sec; 0.261 sec/batch; 23h:43m:09s remains)
INFO - root - 2017-12-11 03:22:56.755752: step 5900, loss = 0.72, batch loss = 0.66 (30.0 examples/sec; 0.266 sec/batch; 24h:09m:51s remains)
2017-12-11 03:22:57.145394: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.0010836315 -0.019816814 -0.044892672 -0.064881042 -0.075158134 -0.07300476 -0.059929203 -0.046506938 -0.042619362 -0.050449297 -0.059026089 -0.060641095 -0.053064883 -0.035303477 -0.014860516][0.07284566 0.05135062 0.021400638 -0.0026046 -0.013154099 -0.0067723948 0.013818287 0.035434932 0.04244275 0.029761828 0.011400372 -6.0445313e-05 0.0015059463 0.019587489 0.046339385][0.16968113 0.15238051 0.12098109 0.095787391 0.088643789 0.10374753 0.13615143 0.16907862 0.18017964 0.16102695 0.12736386 0.098171711 0.08566343 0.098396204 0.12914805][0.25979316 0.25729731 0.23745872 0.2240175 0.23111796 0.26168203 0.30746412 0.34836915 0.35813087 0.32605428 0.2685608 0.21047239 0.17263295 0.17167501 0.20174156][0.32326815 0.34275424 0.34616911 0.35708058 0.38812009 0.43817037 0.49441612 0.53420544 0.53213406 0.47872695 0.39178738 0.30058363 0.23398386 0.21682256 0.24422498][0.35926494 0.402976 0.43693405 0.48113424 0.54308468 0.615604 0.68001825 0.71109295 0.68745053 0.6050818 0.48578128 0.36217138 0.26887062 0.23707789 0.26196095][0.38701043 0.45014328 0.51038551 0.583806 0.67239052 0.76425856 0.83446789 0.85409379 0.80667353 0.69577467 0.54867029 0.39997065 0.28865102 0.25012559 0.27720124][0.40650192 0.47848395 0.55093592 0.6382212 0.73940736 0.84107673 0.9134835 0.92453974 0.86118293 0.73319983 0.57194495 0.41204232 0.29562014 0.26000747 0.29449153][0.39652249 0.46655476 0.53733826 0.62295645 0.7228331 0.82455122 0.89683294 0.90558892 0.83856308 0.7085073 0.5483427 0.39093816 0.27993634 0.25275993 0.29607871][0.36405036 0.41969267 0.47340763 0.5414921 0.626547 0.71886575 0.7887764 0.801891 0.74442118 0.62828332 0.4849847 0.34487477 0.25080973 0.237032 0.28823367][0.31745014 0.35104367 0.37823573 0.41849759 0.47816163 0.55147707 0.61392152 0.63401395 0.59589458 0.507377 0.39497784 0.28478786 0.21575685 0.21683638 0.27232319][0.250647 0.26159304 0.26222223 0.27296704 0.30267996 0.35008183 0.39873302 0.42288038 0.405809 0.35024789 0.27589521 0.20275195 0.16241845 0.17725742 0.23230414][0.14724769 0.14042449 0.12371855 0.11473239 0.1222803 0.14795153 0.1830841 0.20791221 0.20674671 0.17865889 0.13753882 0.097272709 0.080485143 0.10266951 0.15059222][0.019978713 0.0045031933 -0.015570766 -0.028347006 -0.027714364 -0.011972509 0.014751824 0.037386231 0.043187432 0.030244367 0.00874042 -0.012023404 -0.017439706 0.0022188798 0.036572039][-0.0860524 -0.10188274 -0.11548641 -0.1220573 -0.1193274 -0.10700282 -0.0873016 -0.070894964 -0.065939561 -0.073077284 -0.084471196 -0.094745643 -0.096358694 -0.084236175 -0.0653929]]...]
INFO - root - 2017-12-11 03:22:59.806436: step 5910, loss = 0.69, batch loss = 0.63 (28.3 examples/sec; 0.282 sec/batch; 25h:37m:20s remains)
INFO - root - 2017-12-11 03:23:02.488172: step 5920, loss = 0.70, batch loss = 0.64 (30.2 examples/sec; 0.265 sec/batch; 23h:59m:56s remains)
INFO - root - 2017-12-11 03:23:05.101853: step 5930, loss = 0.71, batch loss = 0.65 (30.3 examples/sec; 0.264 sec/batch; 23h:54m:51s remains)
INFO - root - 2017-12-11 03:23:07.788093: step 5940, loss = 0.70, batch loss = 0.64 (30.3 examples/sec; 0.264 sec/batch; 23h:56m:33s remains)
INFO - root - 2017-12-11 03:23:10.384942: step 5950, loss = 0.69, batch loss = 0.63 (31.5 examples/sec; 0.254 sec/batch; 23h:02m:12s remains)
INFO - root - 2017-12-11 03:23:13.041765: step 5960, loss = 0.70, batch loss = 0.64 (29.1 examples/sec; 0.275 sec/batch; 24h:58m:22s remains)
INFO - root - 2017-12-11 03:23:15.669523: step 5970, loss = 0.70, batch loss = 0.64 (30.3 examples/sec; 0.264 sec/batch; 23h:56m:26s remains)
INFO - root - 2017-12-11 03:23:18.348253: step 5980, loss = 0.70, batch loss = 0.64 (30.2 examples/sec; 0.265 sec/batch; 23h:59m:51s remains)
INFO - root - 2017-12-11 03:23:20.985664: step 5990, loss = 0.67, batch loss = 0.62 (30.9 examples/sec; 0.259 sec/batch; 23h:27m:32s remains)
INFO - root - 2017-12-11 03:23:23.638456: step 6000, loss = 0.68, batch loss = 0.63 (29.1 examples/sec; 0.275 sec/batch; 24h:58m:19s remains)
2017-12-11 03:23:24.089168: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.32371739 0.28255537 0.21814725 0.15958875 0.12108774 0.10108825 0.094119005 0.093030758 0.096559606 0.11563944 0.16285276 0.23815557 0.32147989 0.38211527 0.39790678][0.44033426 0.38733456 0.30581042 0.23222287 0.18086419 0.15039527 0.13594551 0.1307494 0.13446696 0.1600178 0.21954259 0.30669308 0.39512497 0.45020327 0.44833747][0.49662131 0.44715995 0.37202322 0.30745661 0.2611382 0.22872341 0.20628636 0.1906199 0.18556666 0.20823845 0.27003747 0.35784113 0.44028 0.48291808 0.46391442][0.48649511 0.4510234 0.39917302 0.36094543 0.33421358 0.31050789 0.28635097 0.26224789 0.24662617 0.2604472 0.31320846 0.3864429 0.44768789 0.46915305 0.43512931][0.43121052 0.41315383 0.39077696 0.3851957 0.38567275 0.38107163 0.36756575 0.34654322 0.32830998 0.3343136 0.36975208 0.41302967 0.43721107 0.42792261 0.3788316][0.3674981 0.36303732 0.36213008 0.38070881 0.40495175 0.42423183 0.43358615 0.43247855 0.42734876 0.43329874 0.4474512 0.44786441 0.42122585 0.37185329 0.30569124][0.32156736 0.32089734 0.32334369 0.34743246 0.38332137 0.42347798 0.46169996 0.49326438 0.51594079 0.53240782 0.52881652 0.48438936 0.40339026 0.31027713 0.22478999][0.3037723 0.29254913 0.2779294 0.28641182 0.31803572 0.37056383 0.43628836 0.50546443 0.56497926 0.60265863 0.59357262 0.51792479 0.39486352 0.26478025 0.15979749][0.30370918 0.26976383 0.22307917 0.20024434 0.21453345 0.27112338 0.36085597 0.46782565 0.56758207 0.63388991 0.632024 0.5438959 0.39923859 0.24744582 0.12843275][0.31052598 0.24909692 0.16717872 0.11007349 0.10282637 0.15742932 0.2640852 0.40113905 0.53452551 0.62751508 0.6400525 0.55679321 0.412389 0.25757506 0.13346457][0.33378354 0.24667637 0.13422339 0.049098246 0.024163241 0.074361049 0.18876748 0.34066194 0.49019253 0.59612346 0.61904955 0.54796559 0.417988 0.27558067 0.15671659][0.39611486 0.29181448 0.1585108 0.056077123 0.01970382 0.06380707 0.17532371 0.3234098 0.46671426 0.5656563 0.587401 0.52635169 0.415425 0.29356244 0.18747413][0.48857424 0.37735927 0.23762706 0.13078901 0.088647835 0.12393118 0.22209349 0.349417 0.46579146 0.53813505 0.54524219 0.48852369 0.39778608 0.30226213 0.21670021][0.576007 0.47198495 0.34218094 0.24391499 0.2014939 0.22646381 0.30426803 0.39943793 0.47466263 0.50676775 0.48919043 0.43107772 0.3583273 0.28953809 0.22764312][0.6208294 0.53414291 0.42598397 0.34497187 0.30744451 0.32361153 0.37922704 0.43807304 0.46723104 0.45536539 0.4101001 0.34720141 0.28780076 0.24096492 0.20099033]]...]
INFO - root - 2017-12-11 03:23:26.730414: step 6010, loss = 0.70, batch loss = 0.64 (31.3 examples/sec; 0.256 sec/batch; 23h:12m:30s remains)
INFO - root - 2017-12-11 03:23:29.341637: step 6020, loss = 0.69, batch loss = 0.63 (32.1 examples/sec; 0.249 sec/batch; 22h:34m:30s remains)
INFO - root - 2017-12-11 03:23:31.992790: step 6030, loss = 0.69, batch loss = 0.63 (29.7 examples/sec; 0.270 sec/batch; 24h:27m:02s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-SGD-lr0.001-clip50-initfromfixqian30k-fixqian
INFO - root - 2017-12-11 03:23:34.649095: step 6040, loss = 0.69, batch loss = 0.64 (30.7 examples/sec; 0.261 sec/batch; 23h:38m:59s remains)
INFO - root - 2017-12-11 03:23:37.287421: step 6050, loss = 0.70, batch loss = 0.64 (28.9 examples/sec; 0.277 sec/batch; 25h:07m:44s remains)
INFO - root - 2017-12-11 03:23:39.998674: step 6060, loss = 0.70, batch loss = 0.64 (27.5 examples/sec; 0.291 sec/batch; 26h:24m:48s remains)
INFO - root - 2017-12-11 03:23:42.659345: step 6070, loss = 0.70, batch loss = 0.64 (30.1 examples/sec; 0.265 sec/batch; 24h:03m:35s remains)
INFO - root - 2017-12-11 03:23:45.297821: step 6080, loss = 0.70, batch loss = 0.64 (30.6 examples/sec; 0.261 sec/batch; 23h:40m:07s remains)
INFO - root - 2017-12-11 03:23:47.917760: step 6090, loss = 0.70, batch loss = 0.64 (30.7 examples/sec; 0.260 sec/batch; 23h:36m:08s remains)
INFO - root - 2017-12-11 03:23:50.529469: step 6100, loss = 0.69, batch loss = 0.63 (29.0 examples/sec; 0.276 sec/batch; 25h:01m:25s remains)
2017-12-11 03:23:50.972273: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0084251082 0.0069335415 0.020253846 0.030117838 0.035024483 0.033368394 0.027250508 0.018491132 0.010140542 0.0048409961 0.0030173094 0.0025266677 0.0003234644 0.00045290473 0.0063828654][0.014376914 0.041373372 0.066597596 0.088049307 0.10288564 0.1079224 0.10378827 0.091673344 0.078097247 0.068520844 0.06328062 0.059161954 0.052980516 0.052070465 0.060244426][0.036710031 0.076528363 0.11504709 0.15009257 0.17852733 0.19573697 0.19848399 0.18519837 0.16682628 0.15277591 0.14338931 0.13442937 0.12316102 0.1208525 0.13109118][0.053208597 0.10310509 0.15257469 0.20034903 0.24339516 0.27522042 0.28700548 0.27334681 0.249522 0.23027629 0.21584316 0.20144731 0.18546772 0.18217324 0.19329084][0.060395464 0.11562057 0.17215291 0.23004188 0.28641188 0.33282131 0.3540445 0.34001759 0.30914527 0.28271472 0.26232621 0.2428048 0.2231002 0.21879801 0.229865][0.064283058 0.12388086 0.18589754 0.25114477 0.31743959 0.37482807 0.40231344 0.38612208 0.34766236 0.3145082 0.29025251 0.26834819 0.24651246 0.24050738 0.25079447][0.069803827 0.13443409 0.2017379 0.27168182 0.34300306 0.40543467 0.43408188 0.41298315 0.36715427 0.32921118 0.30429974 0.28354824 0.26234645 0.25566348 0.26570606][0.077161893 0.14571317 0.21740595 0.28954273 0.36031532 0.41996515 0.44349813 0.41644722 0.36505011 0.32413709 0.29932258 0.28029281 0.2605722 0.25305596 0.26263711][0.081173688 0.15097354 0.22594416 0.29940212 0.36698171 0.4195348 0.43552396 0.40412435 0.34977761 0.30614278 0.27916855 0.25878429 0.23830257 0.22797725 0.23612092][0.080161721 0.148401 0.22447363 0.29767543 0.36048427 0.40413213 0.4126879 0.37989148 0.3262831 0.28108564 0.25087857 0.22746567 0.20506108 0.19167224 0.19834191][0.076288454 0.13998511 0.21244922 0.28058815 0.33500803 0.36756966 0.3683334 0.33564404 0.28595918 0.24241331 0.21139045 0.18714987 0.165414 0.15245487 0.15972295][0.062480964 0.11633708 0.17854697 0.23588273 0.27833235 0.29887885 0.29275197 0.26202351 0.21898411 0.18063019 0.15232155 0.13095261 0.1137804 0.1049867 0.1139028][0.03453647 0.074000396 0.12169149 0.16580403 0.19646131 0.20756827 0.19810423 0.17220728 0.13801804 0.10717501 0.083684988 0.0666945 0.054806609 0.0500055 0.058618903][-0.0016476241 0.02157959 0.053178858 0.0838826 0.1047805 0.11010598 0.10090722 0.081606142 0.05696927 0.034116082 0.016060231 0.0036625711 -0.0036043627 -0.0056519131 0.0010624828][-0.033547286 -0.024884643 -0.0084423563 0.0095715635 0.022321146 0.024592297 0.018069843 0.0063216435 -0.0085927853 -0.023036819 -0.034798142 -0.042246651 -0.045582455 -0.045631524 -0.040886119]]...]
INFO - root - 2017-12-11 03:23:53.669628: step 6110, loss = 0.70, batch loss = 0.64 (30.8 examples/sec; 0.259 sec/batch; 23h:30m:50s remains)
INFO - root - 2017-12-11 03:23:56.309476: step 6120, loss = 0.72, batch loss = 0.66 (29.5 examples/sec; 0.271 sec/batch; 24h:35m:24s remains)
INFO - root - 2017-12-11 03:23:58.897082: step 6130, loss = 0.69, batch loss = 0.63 (31.4 examples/sec; 0.255 sec/batch; 23h:07m:23s remains)
INFO - root - 2017-12-11 03:24:01.543319: step 6140, loss = 0.69, batch loss = 0.63 (29.7 examples/sec; 0.270 sec/batch; 24h:26m:53s remains)
INFO - root - 2017-12-11 03:24:04.211901: step 6150, loss = 0.67, batch loss = 0.62 (28.6 examples/sec; 0.279 sec/batch; 25h:19m:35s remains)
INFO - root - 2017-12-11 03:24:06.911780: step 6160, loss = 0.71, batch loss = 0.65 (31.0 examples/sec; 0.258 sec/batch; 23h:24m:23s remains)
INFO - root - 2017-12-11 03:24:09.543371: step 6170, loss = 0.72, batch loss = 0.66 (28.1 examples/sec; 0.285 sec/batch; 25h:49m:09s remains)
INFO - root - 2017-12-11 03:24:12.245580: step 6180, loss = 0.69, batch loss = 0.63 (29.4 examples/sec; 0.272 sec/batch; 24h:41m:50s remains)
INFO - root - 2017-12-11 03:24:14.842197: step 6190, loss = 0.71, batch loss = 0.65 (31.5 examples/sec; 0.254 sec/batch; 23h:01m:16s remains)
INFO - root - 2017-12-11 03:24:17.547316: step 6200, loss = 0.71, batch loss = 0.65 (30.2 examples/sec; 0.265 sec/batch; 23h:59m:17s remains)
2017-12-11 03:24:17.955628: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.24573165 0.22789094 0.20322102 0.18230805 0.17146842 0.18766183 0.23612903 0.30360478 0.36365238 0.3879557 0.36887747 0.31462169 0.24044074 0.16311763 0.094510883][0.2390987 0.20963918 0.17515033 0.14959086 0.14192115 0.16842555 0.23024406 0.30913362 0.37566146 0.39952841 0.37367034 0.308594 0.22106752 0.12977156 0.050773479][0.20127802 0.16177808 0.12215073 0.098018773 0.099540889 0.13914198 0.21260682 0.29815906 0.36678046 0.38940266 0.36016071 0.29019195 0.19616544 0.098031059 0.016566927][0.14172868 0.098870188 0.062376354 0.048354205 0.065981209 0.12120435 0.20471627 0.29310516 0.35903978 0.37639645 0.34180167 0.26785758 0.17175159 0.074214712 -0.0019637528][0.075811386 0.0384842 0.013567407 0.016418336 0.053491373 0.12312339 0.21244322 0.29798135 0.3547999 0.36113122 0.31681159 0.23772331 0.14347148 0.054893155 -0.0070975879][0.022805871 -0.0033471605 -0.012279931 0.00918661 0.063856073 0.14298947 0.23202302 0.308411 0.35103646 0.34355879 0.28850552 0.20493129 0.11570139 0.041238263 -0.0023825855][-0.0031626131 -0.015913239 -0.0079707569 0.031437729 0.10089649 0.18581571 0.26932073 0.32924357 0.34989005 0.32314226 0.25528648 0.16889545 0.088889912 0.0333817 0.01069421][0.0065562059 0.0057561113 0.027794054 0.081507295 0.16151892 0.24850865 0.3214736 0.35804364 0.34912461 0.29725781 0.21533775 0.12935884 0.063455082 0.030582311 0.029409764][0.041207444 0.04828256 0.078344055 0.1390022 0.22272375 0.30750585 0.3675926 0.38027468 0.34300515 0.26886854 0.17735112 0.09652289 0.048143502 0.038318902 0.057300515][0.079871342 0.092784494 0.12786639 0.1903709 0.27070129 0.34580532 0.38816848 0.37788594 0.31866539 0.23092605 0.13829395 0.068960227 0.039812755 0.0499593 0.085224941][0.11934521 0.13724202 0.1756072 0.23580384 0.30526575 0.36205056 0.38199219 0.35203919 0.28102612 0.19129455 0.10606081 0.05089806 0.037409492 0.060589384 0.10610648][0.16333549 0.18108502 0.21681881 0.26945373 0.32327369 0.35855806 0.35727829 0.31454751 0.24205035 0.16000755 0.0886753 0.049174592 0.048551258 0.080191 0.1319391][0.20561513 0.216144 0.24108009 0.27946094 0.31416091 0.32870904 0.31222725 0.26614818 0.20202664 0.13532764 0.082503863 0.059784506 0.07091637 0.10928872 0.16484351][0.23086438 0.22967234 0.23812273 0.2583169 0.27446437 0.27431887 0.2528671 0.21392936 0.16553196 0.11730769 0.082700983 0.074721821 0.09566544 0.13967465 0.19752702][0.22829759 0.21712339 0.21297371 0.21973689 0.22384334 0.21678774 0.19737512 0.16959995 0.13744025 0.10606043 0.087349668 0.091904916 0.1202609 0.16678262 0.22265819]]...]
INFO - root - 2017-12-11 03:24:20.571017: step 6210, loss = 0.69, batch loss = 0.64 (30.6 examples/sec; 0.261 sec/batch; 23h:40m:18s remains)
INFO - root - 2017-12-11 03:24:23.204132: step 6220, loss = 0.69, batch loss = 0.63 (30.1 examples/sec; 0.266 sec/batch; 24h:07m:39s remains)
INFO - root - 2017-12-11 03:24:25.896920: step 6230, loss = 0.69, batch loss = 0.64 (29.5 examples/sec; 0.272 sec/batch; 24h:36m:52s remains)
INFO - root - 2017-12-11 03:24:28.543663: step 6240, loss = 0.70, batch loss = 0.64 (29.4 examples/sec; 0.272 sec/batch; 24h:39m:09s remains)
INFO - root - 2017-12-11 03:24:31.185265: step 6250, loss = 0.69, batch loss = 0.63 (29.2 examples/sec; 0.274 sec/batch; 24h:50m:29s remains)
INFO - root - 2017-12-11 03:24:33.835990: step 6260, loss = 0.70, batch loss = 0.65 (29.6 examples/sec; 0.270 sec/batch; 24h:29m:59s remains)
INFO - root - 2017-12-11 03:24:36.473371: step 6270, loss = 0.69, batch loss = 0.64 (29.8 examples/sec; 0.268 sec/batch; 24h:17m:56s remains)
INFO - root - 2017-12-11 03:24:39.135011: step 6280, loss = 0.71, batch loss = 0.65 (30.6 examples/sec; 0.261 sec/batch; 23h:40m:56s remains)
INFO - root - 2017-12-11 03:24:41.802762: step 6290, loss = 0.71, batch loss = 0.65 (31.2 examples/sec; 0.256 sec/batch; 23h:13m:05s remains)
INFO - root - 2017-12-11 03:24:44.455379: step 6300, loss = 0.71, batch loss = 0.65 (31.0 examples/sec; 0.258 sec/batch; 23h:21m:26s remains)
2017-12-11 03:24:44.907547: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.02476977 -0.0049352245 0.024847629 0.059271216 0.091427378 0.11223204 0.1169604 0.10945097 0.098004311 0.08805915 0.082091868 0.078015216 0.071968213 0.062367182 0.051191509][-0.022726709 0.0019859124 0.039980542 0.085070252 0.12721355 0.15406708 0.15864959 0.14482063 0.12367726 0.10386983 0.090648375 0.08195401 0.072077684 0.058220532 0.042258482][-0.020664768 0.0070256256 0.050959293 0.10518027 0.15759312 0.19264293 0.20075592 0.18437019 0.15643871 0.12801988 0.10745358 0.093677334 0.079348013 0.060482092 0.039008096][-0.020095715 0.0076800734 0.0537758 0.1134107 0.17417018 0.21814947 0.233091 0.21882698 0.1887145 0.15479551 0.12836635 0.11018379 0.092252858 0.069768392 0.044396918][-0.020566972 0.0052270512 0.050221812 0.11147662 0.17781706 0.23012669 0.25351521 0.24454649 0.21552825 0.17877205 0.14796887 0.12683494 0.10821263 0.086740144 0.062502854][-0.021302335 0.0020465127 0.045141604 0.10746031 0.17994203 0.24295686 0.27836752 0.27836022 0.2519466 0.21112569 0.17291106 0.14666814 0.12834863 0.11271831 0.096320137][-0.02137013 -0.00016316987 0.040891517 0.10391364 0.18207954 0.25580117 0.30383998 0.31419814 0.29113778 0.24539287 0.1981367 0.16582981 0.14880292 0.14183618 0.13715932][-0.022421021 -0.0041610948 0.032877885 0.093727492 0.17380375 0.25455919 0.31247672 0.33193812 0.31190836 0.2608211 0.20423418 0.16585033 0.15118285 0.15462343 0.16385235][-0.025235819 -0.010413945 0.021759545 0.078562059 0.15713485 0.24082981 0.30454287 0.33001554 0.31191504 0.25595805 0.19109914 0.14697029 0.13463023 0.14805663 0.17094263][-0.02849672 -0.016498558 0.010702568 0.06111354 0.13315903 0.21297343 0.27621162 0.30422774 0.28872943 0.23230164 0.16466166 0.11810414 0.10786682 0.12828581 0.16107596][-0.031409793 -0.021773851 0.0002840462 0.041829944 0.1021274 0.17084482 0.22715916 0.25442493 0.24262489 0.1908522 0.12662379 0.081076615 0.071634211 0.093698479 0.12944058][-0.036302421 -0.03027826 -0.014579415 0.015858388 0.060649555 0.11299803 0.15706852 0.17957838 0.17056422 0.12659536 0.071106508 0.030770792 0.022429451 0.042411167 0.075223409][-0.043941963 -0.043300379 -0.035329934 -0.01663268 0.01260836 0.048317719 0.079078309 0.095070235 0.087693557 0.052721202 0.0088981576 -0.022951311 -0.028699141 -0.011668629 0.01573924][-0.049768191 -0.053701159 -0.052033707 -0.042674497 -0.025702395 -0.0036198653 0.015719427 0.025652185 0.019714929 -0.0054988991 -0.036315318 -0.058189113 -0.060983289 -0.047598522 -0.026858803][-0.051042959 -0.056892727 -0.058589481 -0.055318482 -0.047314767 -0.036183085 -0.026691526 -0.02241924 -0.02730483 -0.043240108 -0.061735347 -0.074224263 -0.074731268 -0.065453082 -0.051659066]]...]
INFO - root - 2017-12-11 03:24:47.557207: step 6310, loss = 0.69, batch loss = 0.63 (29.1 examples/sec; 0.275 sec/batch; 24h:53m:47s remains)
INFO - root - 2017-12-11 03:24:50.220943: step 6320, loss = 0.69, batch loss = 0.64 (29.5 examples/sec; 0.271 sec/batch; 24h:34m:22s remains)
INFO - root - 2017-12-11 03:24:52.876312: step 6330, loss = 0.70, batch loss = 0.64 (29.6 examples/sec; 0.271 sec/batch; 24h:30m:38s remains)
INFO - root - 2017-12-11 03:24:55.525323: step 6340, loss = 0.70, batch loss = 0.64 (30.2 examples/sec; 0.265 sec/batch; 24h:02m:19s remains)
INFO - root - 2017-12-11 03:24:58.206446: step 6350, loss = 0.68, batch loss = 0.62 (30.5 examples/sec; 0.262 sec/batch; 23h:45m:59s remains)
INFO - root - 2017-12-11 03:25:00.877808: step 6360, loss = 0.69, batch loss = 0.64 (29.7 examples/sec; 0.269 sec/batch; 24h:24m:12s remains)
INFO - root - 2017-12-11 03:25:03.551392: step 6370, loss = 0.70, batch loss = 0.64 (30.2 examples/sec; 0.265 sec/batch; 23h:59m:42s remains)
INFO - root - 2017-12-11 03:25:06.201214: step 6380, loss = 0.71, batch loss = 0.65 (29.9 examples/sec; 0.267 sec/batch; 24h:13m:53s remains)
INFO - root - 2017-12-11 03:25:08.880805: step 6390, loss = 0.70, batch loss = 0.64 (30.5 examples/sec; 0.262 sec/batch; 23h:45m:02s remains)
INFO - root - 2017-12-11 03:25:11.582986: step 6400, loss = 0.69, batch loss = 0.63 (28.9 examples/sec; 0.277 sec/batch; 25h:05m:03s remains)
2017-12-11 03:25:12.069023: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.0873441 0.075257763 0.069360025 0.079165109 0.10543454 0.14466746 0.18151325 0.20861384 0.22555043 0.22785535 0.21779913 0.20254587 0.19719113 0.20123625 0.20063454][0.086820632 0.075039461 0.073791847 0.091896482 0.1255573 0.16656718 0.20018336 0.2210826 0.23131979 0.22901686 0.21838626 0.20603305 0.20657167 0.21708427 0.22112741][0.098220818 0.0887686 0.092533909 0.11809266 0.15697758 0.19561142 0.22087945 0.23137724 0.23240548 0.22532475 0.21568705 0.2090034 0.2161389 0.23145393 0.23907939][0.12807435 0.12182121 0.12915492 0.15859155 0.19893901 0.23363167 0.25244913 0.25732833 0.25496873 0.24828349 0.24257106 0.24068484 0.24862762 0.25934723 0.26045886][0.16352911 0.1582713 0.16572043 0.19550337 0.23570693 0.26898581 0.28780743 0.29572365 0.29805854 0.29667911 0.29495937 0.29294184 0.29307795 0.28831097 0.27267814][0.19787143 0.19013618 0.19395103 0.22203261 0.26274344 0.29824781 0.32180357 0.33692619 0.34621969 0.34927833 0.34767696 0.33963156 0.32642725 0.30155811 0.26563853][0.22285889 0.21023665 0.20832738 0.23360419 0.27563611 0.31603646 0.347217 0.3706871 0.38557369 0.38895729 0.38149792 0.36109316 0.33054426 0.28567126 0.23290786][0.22287783 0.20562051 0.19860902 0.22020225 0.26296768 0.30928263 0.35002792 0.38249287 0.40159947 0.40300393 0.38714603 0.35384372 0.30905321 0.25215021 0.1928221][0.20043851 0.17763275 0.16473128 0.18090153 0.2231182 0.27620485 0.32961732 0.37426755 0.39961058 0.40047869 0.37814376 0.33637655 0.28532252 0.22797644 0.17367116][0.15252788 0.12625411 0.11008211 0.12202505 0.16304936 0.22120799 0.28467068 0.33879262 0.36921138 0.37076768 0.34580669 0.30305874 0.25661644 0.21141359 0.17344749][0.09357813 0.067257389 0.051727742 0.061395679 0.099088781 0.15575536 0.21922895 0.27303249 0.30310476 0.30589449 0.28489858 0.25251079 0.22489335 0.20563519 0.19494718][0.0405449 0.017475517 0.0051983418 0.014089318 0.047647122 0.098183982 0.15400825 0.19972229 0.2245941 0.22854462 0.21670365 0.20246385 0.2010832 0.21271737 0.23225202][-0.0025457917 -0.020506509 -0.027897134 -0.017968832 0.01272519 0.056466311 0.10212167 0.136333 0.15262172 0.1555517 0.15321143 0.15856673 0.18400535 0.22482392 0.27140772][-0.027999619 -0.040305156 -0.042175774 -0.02976197 -0.00053932 0.036893368 0.071898274 0.093516365 0.099319905 0.098009028 0.10122521 0.12063493 0.16531059 0.22583643 0.28975636][-0.041723322 -0.048118386 -0.045043875 -0.030321604 -0.002840149 0.028168572 0.05263951 0.06270659 0.059008542 0.052141022 0.05573101 0.08007063 0.1310668 0.19628152 0.26235834]]...]
INFO - root - 2017-12-11 03:25:14.734354: step 6410, loss = 0.70, batch loss = 0.64 (30.8 examples/sec; 0.259 sec/batch; 23h:29m:41s remains)
INFO - root - 2017-12-11 03:25:17.381409: step 6420, loss = 0.71, batch loss = 0.65 (30.4 examples/sec; 0.263 sec/batch; 23h:51m:59s remains)
INFO - root - 2017-12-11 03:25:20.079923: step 6430, loss = 0.69, batch loss = 0.63 (30.9 examples/sec; 0.259 sec/batch; 23h:28m:47s remains)
INFO - root - 2017-12-11 03:25:22.751947: step 6440, loss = 0.68, batch loss = 0.62 (30.9 examples/sec; 0.259 sec/batch; 23h:28m:22s remains)
INFO - root - 2017-12-11 03:25:25.407920: step 6450, loss = 0.70, batch loss = 0.64 (29.6 examples/sec; 0.270 sec/batch; 24h:27m:35s remains)
INFO - root - 2017-12-11 03:25:28.071149: step 6460, loss = 0.69, batch loss = 0.64 (30.4 examples/sec; 0.263 sec/batch; 23h:50m:25s remains)
INFO - root - 2017-12-11 03:25:30.706049: step 6470, loss = 0.70, batch loss = 0.64 (29.4 examples/sec; 0.273 sec/batch; 24h:40m:44s remains)
INFO - root - 2017-12-11 03:25:33.397027: step 6480, loss = 0.68, batch loss = 0.63 (31.3 examples/sec; 0.256 sec/batch; 23h:09m:11s remains)
INFO - root - 2017-12-11 03:25:36.057967: step 6490, loss = 0.70, batch loss = 0.64 (30.5 examples/sec; 0.262 sec/batch; 23h:46m:05s remains)
INFO - root - 2017-12-11 03:25:38.733134: step 6500, loss = 0.71, batch loss = 0.65 (30.6 examples/sec; 0.261 sec/batch; 23h:38m:25s remains)
2017-12-11 03:25:39.152107: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.04452762 -0.029563058 -0.00098652649 0.0409296 0.098928183 0.17605637 0.26263818 0.34504163 0.40712795 0.43281618 0.4031769 0.30934125 0.18043543 0.06084837 -0.0098475991][-0.035480715 -0.015175408 0.024546709 0.081824541 0.15829143 0.25547603 0.35915828 0.45036051 0.50918365 0.52215797 0.4760386 0.36509985 0.21926793 0.084466211 0.0041418839][-0.025812319 -0.0018491135 0.047505107 0.11915336 0.21097068 0.32069355 0.43105519 0.52229291 0.5741812 0.57496083 0.51662576 0.39700371 0.24417931 0.1021513 0.01656656][-0.021321947 0.001050995 0.054992281 0.13667192 0.23867737 0.35408616 0.46478134 0.55381316 0.60219318 0.59820688 0.53633225 0.41594717 0.26171514 0.11725738 0.03039174][-0.017929222 7.8697209e-05 0.054381039 0.14064503 0.24705122 0.3626523 0.46874911 0.5516448 0.59538925 0.58922112 0.52764136 0.41163132 0.26375356 0.12792714 0.050031696][-0.0088148583 0.0071971132 0.062735789 0.15253179 0.26214921 0.37594348 0.47256055 0.539739 0.56698859 0.54863614 0.48127341 0.370283 0.23714824 0.12364288 0.067017771][0.0080912858 0.030483004 0.097163916 0.20155241 0.32506484 0.443405 0.529083 0.56834537 0.55837554 0.50669867 0.42063814 0.3118476 0.19803138 0.11320177 0.082615428][0.032987688 0.071313128 0.16283613 0.29738289 0.44887331 0.58182067 0.66021782 0.66626006 0.60608584 0.50709295 0.39333797 0.28117782 0.18218623 0.12013213 0.10968493][0.059618846 0.11844084 0.23856696 0.4047651 0.58223557 0.72673541 0.79660648 0.77068084 0.664465 0.52318925 0.38515261 0.26833552 0.18016672 0.13509801 0.13822244][0.083560459 0.15918595 0.29780892 0.47885522 0.66124588 0.79931808 0.85371029 0.80497986 0.67493629 0.51509476 0.36928383 0.25627345 0.18218516 0.15313178 0.16607444][0.0977806 0.18123026 0.32163113 0.49505258 0.65835637 0.77109331 0.80421597 0.74517566 0.61619425 0.46330661 0.3291567 0.23289844 0.1785138 0.16455866 0.18332207][0.090654135 0.16931877 0.29539907 0.44509572 0.57707959 0.65870887 0.67293489 0.61461723 0.50175864 0.3707464 0.26018879 0.18831004 0.15477048 0.15219447 0.1720576][0.05512742 0.11452153 0.21085404 0.32480732 0.42189494 0.47758502 0.482853 0.43646798 0.35047138 0.25079566 0.16911814 0.12115993 0.10388414 0.10770596 0.12548657][0.0016022149 0.031770207 0.087366715 0.15716599 0.2176446 0.2519373 0.25455976 0.22527339 0.17071906 0.10651259 0.055031978 0.028580952 0.024238676 0.032877106 0.048804432][-0.047905426 -0.044804033 -0.026276151 0.003390261 0.031505112 0.047524814 0.048500396 0.03474291 0.0084323417 -0.023516586 -0.047721826 -0.055515856 -0.04902833 -0.035823777 -0.020337727]]...]
INFO - root - 2017-12-11 03:25:41.845844: step 6510, loss = 0.68, batch loss = 0.62 (30.4 examples/sec; 0.263 sec/batch; 23h:47m:48s remains)
INFO - root - 2017-12-11 03:25:44.554670: step 6520, loss = 0.70, batch loss = 0.64 (29.7 examples/sec; 0.270 sec/batch; 24h:24m:39s remains)
INFO - root - 2017-12-11 03:25:47.221043: step 6530, loss = 0.69, batch loss = 0.63 (29.4 examples/sec; 0.272 sec/batch; 24h:37m:56s remains)
INFO - root - 2017-12-11 03:25:50.041146: step 6540, loss = 0.69, batch loss = 0.63 (30.3 examples/sec; 0.264 sec/batch; 23h:55m:58s remains)
INFO - root - 2017-12-11 03:25:52.689842: step 6550, loss = 0.72, batch loss = 0.66 (30.2 examples/sec; 0.265 sec/batch; 24h:00m:11s remains)
INFO - root - 2017-12-11 03:25:56.941002: step 6560, loss = 0.70, batch loss = 0.64 (30.4 examples/sec; 0.263 sec/batch; 23h:48m:42s remains)
INFO - root - 2017-12-11 03:26:01.150819: step 6570, loss = 0.68, batch loss = 0.63 (14.6 examples/sec; 0.546 sec/batch; 49h:27m:09s remains)
INFO - root - 2017-12-11 03:26:06.679848: step 6580, loss = 0.71, batch loss = 0.65 (14.4 examples/sec; 0.555 sec/batch; 50h:14m:22s remains)
INFO - root - 2017-12-11 03:26:12.217611: step 6590, loss = 0.71, batch loss = 0.65 (14.0 examples/sec; 0.571 sec/batch; 51h:39m:34s remains)
INFO - root - 2017-12-11 03:26:17.851497: step 6600, loss = 0.70, batch loss = 0.64 (14.1 examples/sec; 0.568 sec/batch; 51h:27m:19s remains)
2017-12-11 03:26:18.468507: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.050417576 0.14504422 0.26214689 0.38474855 0.4919177 0.56358266 0.60420543 0.61697048 0.60459429 0.57770365 0.56168926 0.56842172 0.57646817 0.58350796 0.59134525][0.049469545 0.14791542 0.26978511 0.39551926 0.50185031 0.57187355 0.61179996 0.62728143 0.61917496 0.59543449 0.5786283 0.57679105 0.57141429 0.56532544 0.56820488][0.043594088 0.13842191 0.2566601 0.37737167 0.47690031 0.54049677 0.57555962 0.58981526 0.58461279 0.56613171 0.5514887 0.54463845 0.5294894 0.51245177 0.50759059][0.037191376 0.12616476 0.2396532 0.3561745 0.4510937 0.50963539 0.53904957 0.54907596 0.5433079 0.52802604 0.51599175 0.50773567 0.48851976 0.46420822 0.44977993][0.030956522 0.11493291 0.22776799 0.3469806 0.44570452 0.50675344 0.53448069 0.53923261 0.52792627 0.51019341 0.49730721 0.48817897 0.46893239 0.44105214 0.41697529][0.027023394 0.10903917 0.22716655 0.35794929 0.47034779 0.54273003 0.57484978 0.57644325 0.55618584 0.52786821 0.50610489 0.49159834 0.47178414 0.44192442 0.41023481][0.028006563 0.1111958 0.23825541 0.38504475 0.51641345 0.60613686 0.6488471 0.65268117 0.62497038 0.58264238 0.54608029 0.5202142 0.49503413 0.46146426 0.42491359][0.029812418 0.11342572 0.24661732 0.4054094 0.552896 0.65933073 0.71580935 0.72808939 0.69877744 0.64635533 0.59732854 0.56146216 0.53060985 0.49372798 0.45707175][0.030575365 0.11053618 0.24164529 0.4020887 0.55662668 0.67370158 0.74270523 0.766635 0.7412796 0.68531865 0.63080835 0.59200674 0.56127638 0.52635962 0.49723816][0.031413455 0.10374103 0.22341499 0.37218574 0.51939893 0.63402945 0.7062825 0.73731267 0.71748513 0.66552335 0.61655462 0.58720231 0.56809294 0.54556227 0.53253263][0.031538773 0.094401285 0.1971118 0.32443389 0.45138472 0.549484 0.61220461 0.64044213 0.6232028 0.57956004 0.5442394 0.5337193 0.53541249 0.53360647 0.54112542][0.022265214 0.072835267 0.15484986 0.25487611 0.35366863 0.426697 0.47148296 0.48946321 0.47188437 0.43846402 0.41946533 0.4293173 0.45232779 0.47233135 0.49914247][-0.00055735779 0.034746788 0.094188467 0.16553549 0.23446757 0.28139594 0.30740851 0.31412441 0.29586157 0.27201709 0.26604977 0.28951478 0.32538846 0.35911179 0.39684811][-0.030315125 -0.010391129 0.028428629 0.075113505 0.11899943 0.14555353 0.15765537 0.15647002 0.13876517 0.12138075 0.12044449 0.1448262 0.17954066 0.21319366 0.2486258][-0.057925537 -0.051151559 -0.029107047 -0.0012568847 0.024593325 0.0384595 0.043115836 0.038561214 0.023137227 0.00999566 0.0092600621 0.027073732 0.051976856 0.076563083 0.10079613]]...]
INFO - root - 2017-12-11 03:26:24.033847: step 6610, loss = 0.69, batch loss = 0.63 (14.4 examples/sec; 0.555 sec/batch; 50h:16m:34s remains)
INFO - root - 2017-12-11 03:26:29.534211: step 6620, loss = 0.68, batch loss = 0.62 (15.1 examples/sec; 0.530 sec/batch; 47h:59m:08s remains)
INFO - root - 2017-12-11 03:26:35.059191: step 6630, loss = 0.71, batch loss = 0.65 (14.1 examples/sec; 0.568 sec/batch; 51h:24m:38s remains)
INFO - root - 2017-12-11 03:26:40.588397: step 6640, loss = 0.70, batch loss = 0.64 (14.4 examples/sec; 0.556 sec/batch; 50h:18m:42s remains)
INFO - root - 2017-12-11 03:26:46.223029: step 6650, loss = 0.68, batch loss = 0.62 (14.3 examples/sec; 0.558 sec/batch; 50h:28m:59s remains)
INFO - root - 2017-12-11 03:26:51.525259: step 6660, loss = 0.70, batch loss = 0.64 (23.2 examples/sec; 0.344 sec/batch; 31h:10m:03s remains)
INFO - root - 2017-12-11 03:26:56.812152: step 6670, loss = 0.71, batch loss = 0.65 (14.7 examples/sec; 0.546 sec/batch; 49h:23m:04s remains)
INFO - root - 2017-12-11 03:27:02.406240: step 6680, loss = 0.70, batch loss = 0.64 (14.9 examples/sec; 0.538 sec/batch; 48h:39m:08s remains)
INFO - root - 2017-12-11 03:27:08.026296: step 6690, loss = 0.69, batch loss = 0.63 (14.0 examples/sec; 0.572 sec/batch; 51h:43m:24s remains)
INFO - root - 2017-12-11 03:27:13.520445: step 6700, loss = 0.71, batch loss = 0.66 (14.9 examples/sec; 0.538 sec/batch; 48h:39m:53s remains)
2017-12-11 03:27:14.171705: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.029598024 0.0086683389 0.052317802 0.090578526 0.11646508 0.12833892 0.12969336 0.12848359 0.1364347 0.16145569 0.19378485 0.21310981 0.20792103 0.17808837 0.12602958][-0.035889886 -0.00014206315 0.0420953 0.080615073 0.11075017 0.13208392 0.1468024 0.15993978 0.18211964 0.22189845 0.26834121 0.29692873 0.29345918 0.25879484 0.19495457][-0.038892768 -0.0048691407 0.035778932 0.07453046 0.10966133 0.14155665 0.17066544 0.19821271 0.23184371 0.28068283 0.334275 0.3667486 0.36274537 0.32452974 0.25417644][-0.036478162 -0.0025925904 0.038383212 0.080227993 0.1249091 0.17157336 0.21754783 0.25891951 0.29778749 0.34301144 0.38752517 0.41035059 0.39869088 0.35704285 0.28707388][-0.030207559 0.0050608828 0.04862804 0.097208619 0.15772229 0.2258368 0.29410169 0.35123119 0.39171243 0.42288864 0.44272962 0.44043809 0.41095924 0.36245045 0.29638308][-0.019298097 0.021637682 0.074658059 0.13920718 0.22523901 0.32154778 0.41438046 0.48417243 0.51802784 0.52341443 0.506217 0.46854538 0.41514078 0.35667202 0.29441753][-0.0033616181 0.04770454 0.11787736 0.2072241 0.32447594 0.44913879 0.56015456 0.63149309 0.64572805 0.61441976 0.554408 0.48093536 0.40628353 0.33980018 0.28180638][0.010225907 0.071210958 0.15953535 0.27438638 0.4198702 0.56501466 0.68146008 0.74149382 0.72691482 0.65657824 0.55889487 0.46005285 0.37509182 0.30671242 0.25390479][0.016002487 0.081866108 0.18238471 0.31452531 0.47622946 0.627601 0.73553419 0.775549 0.73319268 0.63500273 0.51770437 0.41288728 0.3326692 0.27089417 0.22494945][0.014508057 0.079933845 0.18226425 0.315749 0.47293624 0.6104939 0.69640297 0.71401721 0.65565419 0.55189776 0.440504 0.35065103 0.28741646 0.23770259 0.19890806][0.0094739841 0.070353352 0.16506582 0.28461367 0.41742438 0.52273655 0.57550395 0.57017875 0.506324 0.41422451 0.32620797 0.2641446 0.22457971 0.19057797 0.16064142][-0.0023864366 0.049281757 0.12798205 0.22260059 0.31985569 0.38578719 0.40473744 0.38127923 0.32001576 0.24717131 0.18702726 0.15364148 0.13732766 0.1201698 0.10151258][-0.019970642 0.019902024 0.078860506 0.14537099 0.20683476 0.237658 0.23072253 0.19688001 0.14346464 0.091197759 0.056095455 0.04556071 0.047212139 0.044767711 0.037338905][-0.033117112 -0.0018542754 0.041316394 0.085036032 0.11820794 0.12318441 0.099591166 0.060949519 0.016166689 -0.019419407 -0.036617119 -0.032973986 -0.020874435 -0.013283501 -0.011798223][-0.035362896 -0.006189418 0.028972343 0.05841779 0.072472461 0.060207997 0.026388463 -0.014183837 -0.051937211 -0.076493584 -0.084155 -0.075731918 -0.06147984 -0.050389484 -0.043963883]]...]
INFO - root - 2017-12-11 03:27:19.741950: step 6710, loss = 0.69, batch loss = 0.63 (14.4 examples/sec; 0.556 sec/batch; 50h:17m:10s remains)
INFO - root - 2017-12-11 03:27:25.259745: step 6720, loss = 0.70, batch loss = 0.64 (15.2 examples/sec; 0.525 sec/batch; 47h:32m:46s remains)
INFO - root - 2017-12-11 03:27:30.738667: step 6730, loss = 0.70, batch loss = 0.65 (15.2 examples/sec; 0.525 sec/batch; 47h:28m:53s remains)
INFO - root - 2017-12-11 03:27:36.303604: step 6740, loss = 0.69, batch loss = 0.64 (14.4 examples/sec; 0.555 sec/batch; 50h:14m:06s remains)
INFO - root - 2017-12-11 03:27:41.865834: step 6750, loss = 0.70, batch loss = 0.64 (14.6 examples/sec; 0.549 sec/batch; 49h:43m:16s remains)
INFO - root - 2017-12-11 03:27:46.957249: step 6760, loss = 0.70, batch loss = 0.64 (14.3 examples/sec; 0.558 sec/batch; 50h:31m:17s remains)
INFO - root - 2017-12-11 03:27:52.603768: step 6770, loss = 0.69, batch loss = 0.63 (13.7 examples/sec; 0.584 sec/batch; 52h:47m:47s remains)
INFO - root - 2017-12-11 03:27:58.086676: step 6780, loss = 0.70, batch loss = 0.64 (14.0 examples/sec; 0.573 sec/batch; 51h:50m:57s remains)
INFO - root - 2017-12-11 03:28:03.655108: step 6790, loss = 0.70, batch loss = 0.64 (14.4 examples/sec; 0.555 sec/batch; 50h:10m:40s remains)
INFO - root - 2017-12-11 03:28:09.208862: step 6800, loss = 0.71, batch loss = 0.65 (14.4 examples/sec; 0.554 sec/batch; 50h:09m:16s remains)
2017-12-11 03:28:09.835601: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.14074375 0.15231489 0.1766331 0.21977071 0.28065735 0.34391725 0.38370925 0.38151732 0.33094856 0.25476155 0.18486439 0.14625795 0.1475623 0.17707171 0.21872535][0.20624772 0.21820465 0.23898177 0.28026283 0.34276137 0.40780354 0.44442326 0.43197146 0.36616689 0.27354157 0.19205473 0.15023942 0.15469366 0.19069785 0.23877354][0.25481212 0.26585466 0.28130117 0.31697968 0.3755179 0.43626586 0.4658263 0.44305679 0.3666777 0.26599208 0.18275827 0.14675964 0.15991516 0.20410493 0.25664249][0.27583483 0.28193685 0.28958702 0.31691903 0.36841738 0.42381585 0.45020184 0.42589924 0.35020706 0.25191388 0.17410475 0.14713767 0.16889411 0.21950071 0.27421364][0.27108356 0.26866195 0.26755586 0.28753406 0.33443111 0.38901693 0.4201456 0.40533313 0.34177426 0.25407669 0.18381156 0.16205753 0.18513148 0.23395362 0.28397456][0.24987864 0.24182262 0.23705988 0.25568569 0.30425984 0.36355042 0.40354255 0.40160751 0.35327458 0.27817506 0.2134206 0.19013475 0.20523554 0.24240135 0.27949944][0.2082082 0.20074259 0.199954 0.22460553 0.28127283 0.34974515 0.39983997 0.40913823 0.37271175 0.30624831 0.24180822 0.21005513 0.21033739 0.23027596 0.25139657][0.14868324 0.14771493 0.15719473 0.193739 0.26372558 0.345416 0.406321 0.42297167 0.39064035 0.32329082 0.25002772 0.20206243 0.18319128 0.18456912 0.19094542][0.087062344 0.094091922 0.11485928 0.16358365 0.2468048 0.34172475 0.4124985 0.43310964 0.39896724 0.32378358 0.23596582 0.16811381 0.12981805 0.11500468 0.11079109][0.036033709 0.048450358 0.075919949 0.13058266 0.21898499 0.31918788 0.39407015 0.4161666 0.38078022 0.30082726 0.2033748 0.12116329 0.068938114 0.043574151 0.03406309][0.0037427065 0.016687891 0.043551028 0.0947142 0.17630561 0.26968434 0.33997798 0.36078334 0.32704869 0.25008512 0.15446775 0.070901841 0.016413907 -0.010427704 -0.019268617][-0.012887902 -0.0046180477 0.016192537 0.058411792 0.12673813 0.20585285 0.26520365 0.28159344 0.25073111 0.182755 0.099182241 0.026748493 -0.018460114 -0.037701275 -0.040152766][-0.0035445862 -0.0022691004 0.0098197833 0.04110669 0.095028691 0.15849419 0.20541193 0.21606383 0.18781641 0.13092345 0.06394279 0.008129782 -0.023360929 -0.031655602 -0.025145879][0.044674929 0.04046648 0.043507677 0.06302952 0.10269414 0.15161584 0.18805638 0.19505844 0.17151399 0.12794261 0.079233125 0.039982278 0.020003224 0.020331167 0.034900673][0.12812304 0.12345906 0.11850495 0.12430894 0.14678715 0.17943642 0.20652784 0.21336371 0.1994165 0.17306298 0.14330308 0.11669959 0.10059686 0.10141298 0.11791251]]...]
INFO - root - 2017-12-11 03:28:15.330152: step 6810, loss = 0.69, batch loss = 0.63 (14.8 examples/sec; 0.542 sec/batch; 49h:02m:53s remains)
INFO - root - 2017-12-11 03:28:20.798944: step 6820, loss = 0.70, batch loss = 0.64 (14.7 examples/sec; 0.543 sec/batch; 49h:05m:53s remains)
INFO - root - 2017-12-11 03:28:26.320129: step 6830, loss = 0.68, batch loss = 0.63 (14.4 examples/sec; 0.554 sec/batch; 50h:07m:35s remains)
INFO - root - 2017-12-11 03:28:31.795943: step 6840, loss = 0.70, batch loss = 0.64 (14.7 examples/sec; 0.543 sec/batch; 49h:04m:51s remains)
INFO - root - 2017-12-11 03:28:37.276927: step 6850, loss = 0.69, batch loss = 0.63 (14.4 examples/sec; 0.555 sec/batch; 50h:10m:16s remains)
INFO - root - 2017-12-11 03:28:42.411752: step 6860, loss = 0.70, batch loss = 0.64 (14.5 examples/sec; 0.551 sec/batch; 49h:49m:34s remains)
INFO - root - 2017-12-11 03:28:47.934368: step 6870, loss = 0.70, batch loss = 0.64 (14.6 examples/sec; 0.548 sec/batch; 49h:35m:10s remains)
INFO - root - 2017-12-11 03:28:53.416480: step 6880, loss = 0.71, batch loss = 0.65 (14.2 examples/sec; 0.562 sec/batch; 50h:49m:55s remains)
INFO - root - 2017-12-11 03:28:58.951639: step 6890, loss = 0.69, batch loss = 0.63 (14.3 examples/sec; 0.561 sec/batch; 50h:42m:42s remains)
INFO - root - 2017-12-11 03:29:04.419742: step 6900, loss = 0.68, batch loss = 0.62 (14.6 examples/sec; 0.547 sec/batch; 49h:26m:58s remains)
2017-12-11 03:29:05.076225: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.12654036 0.11776134 0.1079087 0.099169329 0.10057329 0.11834005 0.14343567 0.15830548 0.14910349 0.1212141 0.0821366 0.039613865 0.0049582962 -0.017801691 -0.023761788][0.13182288 0.11974332 0.10626049 0.09590935 0.096887246 0.11489458 0.14061113 0.15672611 0.15040456 0.12418662 0.081707038 0.033478577 -0.0021861726 -0.018147072 -0.014857182][0.14034693 0.12597132 0.11160526 0.10372571 0.11013509 0.13274167 0.16240872 0.18276361 0.18229258 0.16006917 0.11477876 0.060314741 0.022179822 0.011852342 0.023536671][0.15174681 0.13735494 0.12491026 0.12376747 0.14279541 0.17744119 0.21638598 0.2425518 0.24662431 0.22631101 0.17623506 0.11346462 0.069150388 0.059106529 0.074453332][0.16285411 0.15243916 0.14552329 0.15498634 0.19297175 0.24700685 0.29966542 0.33067366 0.3344937 0.31141511 0.25410825 0.18130748 0.12688552 0.11070003 0.12392901][0.17522581 0.1720183 0.17299461 0.19406982 0.25192228 0.32685763 0.39348537 0.42552176 0.42066404 0.38609016 0.3177112 0.23539165 0.17147642 0.14713182 0.15538222][0.18604071 0.19149092 0.20231225 0.23474053 0.30727229 0.39638507 0.47110897 0.49890947 0.47762319 0.42261502 0.33933219 0.2503033 0.18298793 0.15495326 0.16030723][0.19142805 0.20726277 0.22980647 0.27069187 0.34710112 0.43699074 0.50955337 0.52906531 0.49115461 0.417266 0.32292548 0.23287559 0.16911073 0.14332984 0.14950483][0.19418266 0.22191794 0.25599983 0.30052784 0.36894596 0.44493389 0.50364923 0.51239783 0.46458206 0.38352764 0.28962231 0.20625581 0.15118687 0.13083886 0.13928847][0.20727053 0.24568194 0.28784981 0.33022717 0.381111 0.43107194 0.4648113 0.45958737 0.40947482 0.3351343 0.25518841 0.18733409 0.14635369 0.13403854 0.14439982][0.23684593 0.28299147 0.32718384 0.36107016 0.38896835 0.40768662 0.41123787 0.3902801 0.34277529 0.28390548 0.22590013 0.17933267 0.15678945 0.15487199 0.16642867][0.27550748 0.32629433 0.36810571 0.39067715 0.39647329 0.38768405 0.364748 0.33020455 0.28719056 0.24454533 0.20752656 0.18120557 0.1767173 0.18512656 0.19664541][0.31541407 0.36740577 0.40403867 0.41485006 0.40212253 0.37278938 0.331756 0.28942519 0.25239757 0.22298232 0.20108323 0.18938749 0.1983735 0.21391816 0.2230731][0.34907097 0.39791238 0.42691159 0.42807093 0.40316516 0.36139339 0.31025252 0.26465198 0.23247762 0.21022695 0.19494058 0.19037455 0.20644069 0.22528091 0.23075236][0.36511275 0.40420577 0.42112735 0.4135417 0.38296083 0.33815867 0.28625056 0.24285759 0.21553384 0.19613984 0.18181579 0.1796876 0.19825599 0.21769032 0.21984147]]...]
INFO - root - 2017-12-11 03:29:10.598862: step 6910, loss = 0.69, batch loss = 0.64 (14.5 examples/sec; 0.553 sec/batch; 50h:00m:33s remains)
INFO - root - 2017-12-11 03:29:16.090445: step 6920, loss = 0.70, batch loss = 0.64 (14.5 examples/sec; 0.552 sec/batch; 49h:57m:00s remains)
INFO - root - 2017-12-11 03:29:21.598879: step 6930, loss = 0.70, batch loss = 0.64 (14.8 examples/sec; 0.539 sec/batch; 48h:47m:00s remains)
INFO - root - 2017-12-11 03:29:27.090238: step 6940, loss = 0.69, batch loss = 0.63 (14.6 examples/sec; 0.549 sec/batch; 49h:39m:42s remains)
INFO - root - 2017-12-11 03:29:32.406418: step 6950, loss = 0.71, batch loss = 0.65 (24.6 examples/sec; 0.325 sec/batch; 29h:25m:21s remains)
INFO - root - 2017-12-11 03:29:37.704952: step 6960, loss = 0.70, batch loss = 0.64 (13.8 examples/sec; 0.581 sec/batch; 52h:33m:07s remains)
INFO - root - 2017-12-11 03:29:43.235993: step 6970, loss = 0.72, batch loss = 0.66 (14.8 examples/sec; 0.539 sec/batch; 48h:43m:34s remains)
INFO - root - 2017-12-11 03:29:48.776676: step 6980, loss = 0.69, batch loss = 0.63 (14.2 examples/sec; 0.562 sec/batch; 50h:47m:52s remains)
INFO - root - 2017-12-11 03:29:54.342855: step 6990, loss = 0.70, batch loss = 0.64 (14.3 examples/sec; 0.561 sec/batch; 50h:43m:48s remains)
INFO - root - 2017-12-11 03:29:59.906587: step 7000, loss = 0.70, batch loss = 0.64 (14.3 examples/sec; 0.561 sec/batch; 50h:42m:33s remains)
2017-12-11 03:30:00.542387: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.046102941 0.040730469 0.043611329 0.05977875 0.080565482 0.1003553 0.1194497 0.14015296 0.16018052 0.16927695 0.15931499 0.13155387 0.091480374 0.042475771 -0.010136875][0.076730363 0.074333034 0.082802095 0.10770541 0.13650084 0.16305347 0.18955524 0.21860732 0.24481377 0.25550121 0.24165934 0.20554887 0.15388688 0.090153813 0.021284878][0.10339806 0.10466354 0.11837637 0.14950965 0.18208124 0.21012229 0.23897654 0.27212745 0.30140463 0.31279176 0.29685467 0.25612178 0.19670473 0.12207936 0.041198324][0.13272898 0.13725846 0.15317984 0.18445194 0.21329387 0.23545228 0.26072565 0.29369551 0.32312879 0.33351374 0.31559005 0.27190244 0.20763646 0.12657566 0.040164728][0.18536451 0.19526872 0.21202104 0.23886314 0.25930849 0.27262589 0.29386202 0.326907 0.35648751 0.36434108 0.34091815 0.28940064 0.21553031 0.12533498 0.033716623][0.24812986 0.27000484 0.29070973 0.31414041 0.32801244 0.33585513 0.35655648 0.39236829 0.42342317 0.428229 0.39573371 0.33064118 0.24114089 0.13688561 0.036657952][0.29685128 0.33513993 0.36300859 0.38525704 0.39645612 0.40209702 0.42198017 0.45753914 0.48728225 0.48797464 0.4457894 0.36853451 0.26719037 0.15355979 0.047704373][0.30500495 0.35552067 0.38883978 0.40961647 0.42001235 0.42481688 0.44165897 0.47195783 0.49668166 0.493441 0.44624835 0.36591554 0.26506624 0.15493464 0.053081948][0.26715651 0.31762531 0.34850457 0.36373073 0.37118459 0.37411141 0.38551328 0.40723464 0.42608437 0.42316785 0.38105515 0.31055316 0.2244221 0.13153839 0.04451146][0.18920586 0.22697622 0.24775326 0.25479192 0.25796297 0.25902745 0.26562107 0.28019917 0.29612252 0.29889479 0.270244 0.21844083 0.15520835 0.086973228 0.02114404][0.083932437 0.10246661 0.11023451 0.10932293 0.10821972 0.10699102 0.1091935 0.11827052 0.13258572 0.14172903 0.12807529 0.097558461 0.060179569 0.019929789 -0.020224286][-0.016342876 -0.014779458 -0.017053625 -0.023250991 -0.027751051 -0.03179922 -0.033796016 -0.029806348 -0.018610045 -0.0069963536 -0.0099494038 -0.023054877 -0.038129326 -0.053187679 -0.068623237][-0.08241234 -0.090753011 -0.097779684 -0.10551544 -0.11122657 -0.11626556 -0.11981685 -0.11857596 -0.11088375 -0.10067565 -0.098769888 -0.10209838 -0.10409841 -0.10379734 -0.10324059][-0.10771916 -0.1196464 -0.12783891 -0.13510911 -0.14070129 -0.14537455 -0.14856373 -0.14821252 -0.14337884 -0.13610542 -0.13293336 -0.13211335 -0.12898737 -0.12299572 -0.11626612][-0.10900799 -0.12076155 -0.12814389 -0.13413951 -0.13897046 -0.14278126 -0.14498679 -0.14462835 -0.14183 -0.13758381 -0.13480595 -0.13251187 -0.12824191 -0.12195864 -0.11496005]]...]
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-SGD-lr0.001-clip50-initfromfixqian30k-fixqian
INFO - root - 2017-12-11 03:30:06.134461: step 7010, loss = 0.69, batch loss = 0.64 (14.7 examples/sec; 0.544 sec/batch; 49h:12m:01s remains)
INFO - root - 2017-12-11 03:30:11.622585: step 7020, loss = 0.71, batch loss = 0.65 (14.7 examples/sec; 0.543 sec/batch; 49h:04m:52s remains)
INFO - root - 2017-12-11 03:30:17.154490: step 7030, loss = 0.69, batch loss = 0.63 (14.3 examples/sec; 0.558 sec/batch; 50h:24m:24s remains)
INFO - root - 2017-12-11 03:30:22.691827: step 7040, loss = 0.70, batch loss = 0.64 (14.6 examples/sec; 0.547 sec/batch; 49h:28m:40s remains)
INFO - root - 2017-12-11 03:30:27.861797: step 7050, loss = 0.70, batch loss = 0.64 (14.2 examples/sec; 0.563 sec/batch; 50h:52m:40s remains)
INFO - root - 2017-12-11 03:30:33.392574: step 7060, loss = 0.69, batch loss = 0.63 (14.9 examples/sec; 0.536 sec/batch; 48h:28m:53s remains)
INFO - root - 2017-12-11 03:30:38.942855: step 7070, loss = 0.70, batch loss = 0.65 (14.7 examples/sec; 0.543 sec/batch; 49h:04m:31s remains)
INFO - root - 2017-12-11 03:30:44.448451: step 7080, loss = 0.69, batch loss = 0.63 (14.0 examples/sec; 0.571 sec/batch; 51h:37m:05s remains)
INFO - root - 2017-12-11 03:30:49.922791: step 7090, loss = 0.69, batch loss = 0.63 (14.0 examples/sec; 0.573 sec/batch; 51h:47m:53s remains)
INFO - root - 2017-12-11 03:30:55.467745: step 7100, loss = 0.71, batch loss = 0.66 (14.1 examples/sec; 0.567 sec/batch; 51h:12m:45s remains)
2017-12-11 03:30:56.128908: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.07499221 0.066047564 0.059150532 0.061397269 0.080621578 0.11353498 0.15039179 0.17915565 0.19178788 0.19031556 0.17479667 0.15217774 0.13543108 0.13078338 0.13697878][0.10090298 0.091207936 0.080250844 0.075296052 0.08551874 0.10964418 0.13971739 0.16482154 0.17791344 0.18039054 0.16969034 0.15069064 0.13647161 0.13442819 0.14400198][0.14200313 0.13168162 0.11447725 0.097883932 0.091936454 0.098353736 0.11193547 0.12450615 0.1327216 0.1388272 0.13760172 0.12936674 0.12376177 0.1271722 0.13949084][0.1994487 0.18727994 0.16255045 0.13432284 0.1132791 0.10339683 0.10054635 0.099036209 0.09980233 0.10694148 0.1137085 0.11601462 0.11875433 0.1261 0.13743122][0.25959179 0.24381684 0.21188438 0.17580707 0.14703535 0.13084498 0.12197386 0.11511231 0.11311626 0.12061591 0.13004871 0.13529328 0.13878879 0.14415321 0.14968695][0.29939756 0.27872634 0.24088286 0.20197068 0.17477381 0.16521931 0.16534787 0.16737284 0.17214566 0.18176429 0.18836139 0.18684797 0.18183535 0.17928559 0.17713384][0.29569545 0.27079892 0.23086774 0.19545099 0.1778329 0.18401277 0.20384276 0.22565404 0.24516062 0.25988156 0.26180193 0.24899185 0.23062944 0.21720487 0.20753193][0.24267228 0.21592027 0.18015942 0.15529661 0.15206836 0.175129 0.21412663 0.25526464 0.28970134 0.31075373 0.31096008 0.2909072 0.26298705 0.24148503 0.22679505][0.16963404 0.14301179 0.11600371 0.10565194 0.11650953 0.150031 0.19772987 0.24766213 0.28955922 0.31483895 0.31737953 0.29854751 0.26994947 0.24640191 0.23057343][0.1231582 0.095287979 0.075107105 0.075868286 0.094151564 0.12736979 0.17022344 0.21508579 0.2540372 0.27927339 0.28629616 0.2761018 0.25576013 0.23696965 0.22369552][0.12792903 0.0960443 0.076656446 0.080583304 0.097379461 0.12056305 0.14827149 0.17751008 0.2046584 0.2248238 0.23586166 0.23835795 0.23313405 0.22590406 0.21903929][0.16903217 0.13533777 0.11504679 0.11772764 0.12894028 0.13985911 0.15097392 0.16168237 0.17175403 0.18029144 0.18929069 0.19973166 0.20756687 0.21231443 0.21241426][0.20956732 0.1839103 0.16782369 0.17053625 0.17785047 0.18167286 0.18287663 0.1803451 0.1742961 0.1667895 0.16519484 0.17220061 0.18236633 0.1913687 0.19440356][0.2291567 0.21934605 0.21211846 0.21648891 0.2222902 0.22493634 0.22479704 0.21864268 0.20407532 0.18404707 0.1696033 0.16609357 0.16950239 0.1745504 0.17495377][0.23125131 0.23524112 0.23566407 0.24032971 0.24435876 0.24734782 0.25020376 0.24860226 0.23675911 0.21586265 0.19712889 0.1871499 0.18371904 0.18206778 0.17641303]]...]
INFO - root - 2017-12-11 03:31:01.651832: step 7110, loss = 0.71, batch loss = 0.65 (14.1 examples/sec; 0.569 sec/batch; 51h:23m:11s remains)
INFO - root - 2017-12-11 03:31:07.195380: step 7120, loss = 0.69, batch loss = 0.63 (14.1 examples/sec; 0.567 sec/batch; 51h:17m:32s remains)
INFO - root - 2017-12-11 03:31:12.814952: step 7130, loss = 0.70, batch loss = 0.64 (14.1 examples/sec; 0.569 sec/batch; 51h:26m:50s remains)
INFO - root - 2017-12-11 03:31:18.326539: step 7140, loss = 0.72, batch loss = 0.66 (14.2 examples/sec; 0.562 sec/batch; 50h:45m:33s remains)
INFO - root - 2017-12-11 03:31:23.473893: step 7150, loss = 0.68, batch loss = 0.62 (14.0 examples/sec; 0.570 sec/batch; 51h:31m:52s remains)
INFO - root - 2017-12-11 03:31:28.986831: step 7160, loss = 0.70, batch loss = 0.64 (14.4 examples/sec; 0.554 sec/batch; 50h:03m:46s remains)
INFO - root - 2017-12-11 03:31:34.492233: step 7170, loss = 0.69, batch loss = 0.63 (14.4 examples/sec; 0.554 sec/batch; 50h:02m:57s remains)
INFO - root - 2017-12-11 03:31:40.046124: step 7180, loss = 0.70, batch loss = 0.65 (14.3 examples/sec; 0.560 sec/batch; 50h:36m:01s remains)
INFO - root - 2017-12-11 03:31:45.564163: step 7190, loss = 0.70, batch loss = 0.65 (14.9 examples/sec; 0.536 sec/batch; 48h:27m:54s remains)
INFO - root - 2017-12-11 03:31:50.954632: step 7200, loss = 0.69, batch loss = 0.63 (14.9 examples/sec; 0.535 sec/batch; 48h:22m:51s remains)
2017-12-11 03:31:51.649367: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.072526112 -0.06555675 -0.060579486 -0.061367068 -0.065397449 -0.068962723 -0.070274018 -0.069869034 -0.069698423 -0.07183668 -0.078579292 -0.09175051 -0.10890155 -0.1254461 -0.13848405][-0.0087283328 0.014640206 0.033790857 0.043826275 0.047199853 0.046848964 0.044396508 0.039425395 0.031676527 0.019462898 0.000315249 -0.027158979 -0.059560694 -0.091283835 -0.11949757][0.081721038 0.13113949 0.17599067 0.21036625 0.23415706 0.24525861 0.24327588 0.22944742 0.20782328 0.17855936 0.1393268 0.091220967 0.038655449 -0.014077279 -0.065486863][0.17902938 0.26004854 0.33846211 0.40766847 0.46291652 0.49324873 0.49404734 0.47056282 0.43443674 0.38708466 0.32557067 0.25395986 0.17819272 0.099479176 0.016625062][0.25640035 0.36619744 0.47683141 0.58133543 0.66950542 0.72191107 0.72791541 0.69768023 0.65135497 0.59150273 0.51249808 0.41978085 0.32219574 0.21758787 0.10245027][0.30441847 0.43316135 0.56402361 0.69131452 0.801228 0.86995471 0.88098317 0.84928286 0.80249405 0.7421751 0.65728194 0.55184859 0.43862826 0.3135992 0.17203489][0.324808 0.45968533 0.59561753 0.72936451 0.84637755 0.92217308 0.93724954 0.90949655 0.87046611 0.81897026 0.7375595 0.62762076 0.50577188 0.36916783 0.21293056][0.32234889 0.45193207 0.57746851 0.69973087 0.80639517 0.87643927 0.89124429 0.87066936 0.84537184 0.80882114 0.73801869 0.63115555 0.5086897 0.37066761 0.21311057][0.30666828 0.42226347 0.52474594 0.6192559 0.69842595 0.74797416 0.75453031 0.739163 0.72751296 0.70701045 0.65120631 0.55540943 0.44216871 0.31471071 0.17019929][0.27592313 0.37096921 0.44330081 0.50165 0.54434061 0.5641439 0.5554319 0.53984213 0.53632265 0.52779114 0.48821881 0.4113709 0.31850287 0.21391879 0.095406368][0.22292784 0.29466796 0.33705735 0.36093739 0.36979607 0.36102951 0.33616236 0.31644031 0.31548291 0.31420594 0.28987515 0.23575392 0.16891557 0.09293396 0.0062654307][0.14078882 0.18834725 0.20680863 0.20632024 0.19333039 0.16682395 0.13193424 0.10847612 0.10662439 0.1086556 0.095807545 0.062142462 0.020085679 -0.027831407 -0.082381956][0.043241482 0.06827458 0.070006989 0.055992518 0.0330646 0.00054714206 -0.035972487 -0.059096292 -0.060838558 -0.056070779 -0.0596862 -0.076586351 -0.098047994 -0.12250863 -0.15008567][-0.044888 -0.038267203 -0.045945 -0.064309113 -0.08778701 -0.11689192 -0.14736742 -0.16570918 -0.16591169 -0.15931556 -0.15660106 -0.1613701 -0.16785283 -0.1755009 -0.18405285][-0.11056607 -0.11602989 -0.12635452 -0.14222871 -0.16012427 -0.18008526 -0.19991378 -0.21116425 -0.20977481 -0.20307806 -0.1976696 -0.19589332 -0.19407071 -0.19207768 -0.18981037]]...]
INFO - root - 2017-12-11 03:31:57.089030: step 7210, loss = 0.70, batch loss = 0.64 (14.5 examples/sec; 0.550 sec/batch; 49h:44m:12s remains)
INFO - root - 2017-12-11 03:32:02.622887: step 7220, loss = 0.70, batch loss = 0.64 (14.8 examples/sec; 0.541 sec/batch; 48h:51m:38s remains)
INFO - root - 2017-12-11 03:32:08.146242: step 7230, loss = 0.70, batch loss = 0.64 (14.7 examples/sec; 0.543 sec/batch; 49h:06m:02s remains)
INFO - root - 2017-12-11 03:32:13.307977: step 7240, loss = 0.69, batch loss = 0.63 (27.4 examples/sec; 0.291 sec/batch; 26h:19m:56s remains)
INFO - root - 2017-12-11 03:32:18.894960: step 7250, loss = 0.70, batch loss = 0.64 (14.4 examples/sec; 0.554 sec/batch; 50h:04m:21s remains)
INFO - root - 2017-12-11 03:32:24.471474: step 7260, loss = 0.68, batch loss = 0.62 (13.3 examples/sec; 0.603 sec/batch; 54h:27m:49s remains)
INFO - root - 2017-12-11 03:32:29.912309: step 7270, loss = 0.70, batch loss = 0.65 (14.9 examples/sec; 0.536 sec/batch; 48h:26m:00s remains)
INFO - root - 2017-12-11 03:32:35.454407: step 7280, loss = 0.70, batch loss = 0.64 (14.3 examples/sec; 0.559 sec/batch; 50h:30m:35s remains)
INFO - root - 2017-12-11 03:32:40.990674: step 7290, loss = 0.69, batch loss = 0.63 (14.6 examples/sec; 0.549 sec/batch; 49h:33m:43s remains)
INFO - root - 2017-12-11 03:32:46.381274: step 7300, loss = 0.70, batch loss = 0.64 (15.0 examples/sec; 0.533 sec/batch; 48h:06m:35s remains)
2017-12-11 03:32:46.955606: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.060993511 0.074133635 0.073436461 0.060548328 0.041748114 0.022951212 0.0093908478 0.00602883 0.014759392 0.032897335 0.053630266 0.068108842 0.06974154 0.05504645 0.027799385][0.11990659 0.14707959 0.15181926 0.13455598 0.10438733 0.072054096 0.047940742 0.041494172 0.055012703 0.083025038 0.11348041 0.13284354 0.13206124 0.10693912 0.064629994][0.17315161 0.21602461 0.22907807 0.21047921 0.17106971 0.12584567 0.0905256 0.07925272 0.094844475 0.12921916 0.16627 0.18879658 0.1855353 0.15103516 0.095020957][0.20626894 0.26114228 0.28301683 0.26776361 0.2258141 0.17394125 0.13078636 0.11359047 0.12595555 0.1589658 0.19545218 0.21728559 0.21228941 0.17299429 0.10965329][0.21190913 0.27204546 0.30090272 0.29326087 0.25731793 0.20904426 0.16641065 0.14676683 0.15349244 0.17829095 0.20614125 0.22073558 0.21117595 0.1690373 0.10392298][0.19647604 0.25531614 0.28698307 0.28705102 0.26203012 0.22623067 0.19412005 0.17936713 0.18293543 0.19773491 0.21288325 0.21561238 0.19832835 0.15312339 0.088529639][0.17616092 0.22992457 0.25911516 0.26214719 0.24614663 0.22501859 0.2094005 0.20718408 0.21517672 0.22599672 0.23226899 0.22401409 0.19742365 0.14675164 0.080331549][0.16701081 0.21742366 0.24344361 0.24629158 0.23514725 0.22450623 0.22243696 0.2331012 0.25012991 0.26429695 0.26860973 0.25333357 0.21761587 0.15862505 0.086054437][0.1695503 0.22053626 0.24620448 0.24903737 0.23949857 0.23265901 0.2359474 0.25402036 0.28008857 0.30173081 0.30903822 0.2904996 0.24680531 0.1780744 0.096859284][0.17426457 0.22814964 0.25664812 0.26185316 0.25350958 0.24622892 0.24817351 0.26743957 0.29913795 0.32733071 0.33825618 0.31862202 0.269842 0.1936983 0.10526632][0.16616608 0.22074349 0.25240108 0.26230592 0.25745028 0.25001043 0.24868333 0.26465675 0.29558608 0.32355061 0.33348793 0.31301966 0.26411378 0.18796588 0.099989384][0.1348612 0.184572 0.21641929 0.23081955 0.23119822 0.22553779 0.22213075 0.23304468 0.25767735 0.27866057 0.2829186 0.26166031 0.21766776 0.149971 0.071943276][0.080234423 0.11922031 0.14650102 0.16192763 0.16591802 0.16238642 0.15831269 0.16459852 0.18105455 0.19287845 0.1906518 0.16993812 0.13444445 0.081407666 0.02092015][0.011949715 0.036632378 0.055923495 0.068949528 0.074284457 0.072697818 0.069014765 0.07187 0.080952778 0.085166022 0.078836322 0.060871616 0.035453778 -0.00024016191 -0.039403144][-0.045756526 -0.034286309 -0.023140451 -0.013982774 -0.00897605 -0.0089880405 -0.011586488 -0.010851658 -0.0072438512 -0.007972151 -0.015760455 -0.0299265 -0.046481747 -0.066707231 -0.086348131]]...]
INFO - root - 2017-12-11 03:32:52.352750: step 7310, loss = 0.69, batch loss = 0.64 (14.6 examples/sec; 0.547 sec/batch; 49h:25m:04s remains)
INFO - root - 2017-12-11 03:32:57.811314: step 7320, loss = 0.70, batch loss = 0.64 (14.5 examples/sec; 0.550 sec/batch; 49h:40m:10s remains)
INFO - root - 2017-12-11 03:33:03.387532: step 7330, loss = 0.69, batch loss = 0.64 (13.8 examples/sec; 0.580 sec/batch; 52h:21m:48s remains)
INFO - root - 2017-12-11 03:33:08.572038: step 7340, loss = 0.70, batch loss = 0.64 (14.8 examples/sec; 0.539 sec/batch; 48h:39m:50s remains)
INFO - root - 2017-12-11 03:33:14.049510: step 7350, loss = 0.70, batch loss = 0.64 (14.6 examples/sec; 0.546 sec/batch; 49h:20m:38s remains)
INFO - root - 2017-12-11 03:33:19.456575: step 7360, loss = 0.70, batch loss = 0.64 (14.6 examples/sec; 0.547 sec/batch; 49h:24m:58s remains)
INFO - root - 2017-12-11 03:33:24.989541: step 7370, loss = 0.70, batch loss = 0.64 (14.7 examples/sec; 0.543 sec/batch; 49h:01m:50s remains)
INFO - root - 2017-12-11 03:33:30.545062: step 7380, loss = 0.70, batch loss = 0.64 (14.6 examples/sec; 0.548 sec/batch; 49h:31m:31s remains)
INFO - root - 2017-12-11 03:33:35.980946: step 7390, loss = 0.67, batch loss = 0.61 (14.1 examples/sec; 0.569 sec/batch; 51h:22m:35s remains)
INFO - root - 2017-12-11 03:33:41.512529: step 7400, loss = 0.70, batch loss = 0.64 (14.6 examples/sec; 0.547 sec/batch; 49h:21m:29s remains)
2017-12-11 03:33:42.099311: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.10954704 0.079518646 0.079309314 0.13222833 0.23553154 0.36213538 0.47885263 0.54401124 0.52725017 0.43827149 0.31200525 0.18953459 0.1065615 0.084661655 0.1133322][0.083823264 0.067481853 0.087468483 0.16458808 0.29281709 0.44036388 0.57154709 0.64423269 0.62362826 0.51792759 0.36628011 0.21452121 0.10164945 0.053207081 0.062954925][0.045079194 0.049153734 0.092077851 0.1889791 0.33029196 0.48496431 0.61920452 0.69234306 0.66829604 0.55403996 0.38915762 0.2189275 0.081740953 0.005711609 -0.0083715748][0.011982666 0.030933885 0.086979881 0.19054258 0.33171651 0.48392496 0.6161626 0.68741113 0.66181087 0.54644507 0.38103437 0.20867951 0.064381771 -0.024501625 -0.052214723][-0.0047234194 0.0202343 0.077724785 0.17729217 0.3121509 0.46031454 0.59109694 0.66096759 0.63499343 0.52224869 0.36301675 0.19974628 0.062295742 -0.026674928 -0.059147045][0.0040324405 0.031664949 0.087666474 0.18253544 0.311725 0.45491618 0.580058 0.64253294 0.61016637 0.49610341 0.34277022 0.19127651 0.06532409 -0.019431511 -0.054255649][0.013200791 0.043182895 0.10073041 0.19479811 0.32030311 0.45592922 0.56888813 0.61760557 0.57641268 0.4612571 0.31350619 0.17135142 0.05415136 -0.027080597 -0.064225569][-0.0049189837 0.026316889 0.085758582 0.17906883 0.29946759 0.42528924 0.52523613 0.56457752 0.52434254 0.41826743 0.28092822 0.14656489 0.034291666 -0.044634387 -0.082676448][-0.035307184 -0.0047553792 0.052993987 0.14093931 0.25207913 0.36664903 0.45659706 0.49362659 0.46330363 0.37368968 0.25086752 0.12582506 0.019467201 -0.054570973 -0.089028716][-0.052384768 -0.024116421 0.028620169 0.10764707 0.20746721 0.31152111 0.39425796 0.43060434 0.40767267 0.32981867 0.21875191 0.10369475 0.0060855793 -0.060025919 -0.088180237][-0.05117083 -0.027686037 0.017206734 0.084093831 0.16919388 0.25878504 0.32913569 0.35702711 0.33131814 0.25717345 0.15640739 0.056617107 -0.023414835 -0.073823243 -0.090914495][-0.046355672 -0.031646669 7.4653632e-05 0.04831668 0.11077424 0.17722687 0.22788295 0.24337153 0.21509889 0.14895537 0.065599672 -0.010270962 -0.064192526 -0.092315495 -0.094947129][-0.042861164 -0.0393128 -0.023814609 0.0022676603 0.037464209 0.075833239 0.1040014 0.10889281 0.084608279 0.035265107 -0.022916822 -0.070770375 -0.098355763 -0.10567028 -0.094752222][-0.037896018 -0.045881741 -0.0454984 -0.039059818 -0.028393295 -0.016195409 -0.0092969919 -0.013818854 -0.031894434 -0.061043765 -0.091297857 -0.11125691 -0.11562578 -0.10587213 -0.084185183][-0.033253651 -0.052634828 -0.064719334 -0.072451249 -0.078855149 -0.08503513 -0.09283904 -0.10305128 -0.11440624 -0.12503202 -0.13054587 -0.12657535 -0.11216578 -0.089732684 -0.061597284]]...]
INFO - root - 2017-12-11 03:33:47.635468: step 7410, loss = 0.71, batch loss = 0.65 (14.6 examples/sec; 0.549 sec/batch; 49h:33m:20s remains)
INFO - root - 2017-12-11 03:33:53.028180: step 7420, loss = 0.69, batch loss = 0.63 (15.4 examples/sec; 0.520 sec/batch; 46h:58m:50s remains)
INFO - root - 2017-12-11 03:33:58.548262: step 7430, loss = 0.70, batch loss = 0.64 (13.9 examples/sec; 0.575 sec/batch; 51h:54m:31s remains)
INFO - root - 2017-12-11 03:34:03.691508: step 7440, loss = 0.69, batch loss = 0.63 (14.9 examples/sec; 0.536 sec/batch; 48h:22m:05s remains)
INFO - root - 2017-12-11 03:34:09.190041: step 7450, loss = 0.71, batch loss = 0.65 (14.4 examples/sec; 0.554 sec/batch; 50h:02m:55s remains)
INFO - root - 2017-12-11 03:34:14.742137: step 7460, loss = 0.70, batch loss = 0.64 (14.3 examples/sec; 0.559 sec/batch; 50h:30m:16s remains)
INFO - root - 2017-12-11 03:34:20.183320: step 7470, loss = 0.69, batch loss = 0.63 (14.8 examples/sec; 0.539 sec/batch; 48h:40m:19s remains)
INFO - root - 2017-12-11 03:34:25.663052: step 7480, loss = 0.70, batch loss = 0.64 (14.6 examples/sec; 0.546 sec/batch; 49h:19m:16s remains)
INFO - root - 2017-12-11 03:34:31.111079: step 7490, loss = 0.69, batch loss = 0.64 (14.4 examples/sec; 0.556 sec/batch; 50h:12m:17s remains)
INFO - root - 2017-12-11 03:34:36.604040: step 7500, loss = 0.69, batch loss = 0.63 (14.4 examples/sec; 0.554 sec/batch; 50h:01m:22s remains)
2017-12-11 03:34:37.180664: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.11516894 0.10096813 0.0975333 0.11073861 0.13564262 0.1626664 0.18461992 0.19497001 0.19017081 0.17120248 0.13887744 0.10740536 0.093671449 0.10401197 0.12917416][0.18331656 0.16908917 0.16830985 0.18709424 0.21844055 0.24792011 0.26487562 0.26034528 0.2341148 0.192431 0.13959433 0.095268652 0.078635894 0.093203515 0.12651624][0.25644165 0.24214509 0.24213827 0.26313114 0.29674122 0.3243829 0.33178911 0.30799598 0.25595826 0.18829134 0.11455971 0.059436534 0.041985828 0.0611487 0.10120539][0.32151192 0.30842751 0.30527127 0.32269436 0.35260236 0.37570038 0.3760612 0.34060395 0.27319142 0.19121176 0.10778057 0.049911372 0.035034258 0.058149096 0.10142899][0.36000076 0.34687454 0.33555612 0.34356683 0.36599702 0.38620657 0.38771072 0.35499159 0.28964689 0.2117331 0.13447158 0.083825707 0.075016484 0.099920079 0.14015169][0.36361825 0.34934309 0.32796934 0.32473475 0.34060806 0.36346996 0.37632108 0.35921234 0.30909309 0.24678041 0.18416032 0.14538085 0.14374827 0.16852498 0.20099647][0.34732509 0.33379486 0.30708212 0.2969121 0.31161219 0.34346321 0.37448692 0.37813795 0.34576622 0.297281 0.24471095 0.21201795 0.21309549 0.2356832 0.26083681][0.35891744 0.34609503 0.31758046 0.30408162 0.3195408 0.35878751 0.40316197 0.42054325 0.3973431 0.35181135 0.2992211 0.26486596 0.2649368 0.28681362 0.309955][0.40869954 0.39349136 0.36033228 0.34199673 0.35430652 0.3913784 0.43525 0.45411563 0.43279293 0.38702267 0.33491182 0.30327389 0.30955657 0.33918837 0.36746976][0.46998832 0.44769681 0.4060314 0.38190943 0.38877586 0.41651729 0.44801897 0.4585585 0.43499994 0.39012715 0.34299332 0.32122907 0.34224904 0.38702583 0.42437941][0.50260741 0.47246611 0.4232941 0.39591244 0.39853981 0.41652429 0.43422812 0.43735111 0.41594094 0.37866989 0.34235951 0.33371383 0.36899579 0.42506945 0.46611628][0.46554562 0.43570864 0.38668671 0.36019441 0.35964313 0.37000352 0.37873995 0.38052306 0.36684066 0.34129372 0.31720871 0.31829336 0.35873282 0.4141672 0.45048997][0.35755631 0.33382469 0.29071668 0.26615587 0.26274288 0.26892331 0.27491745 0.27837217 0.26997462 0.25033337 0.23163088 0.23488532 0.27114961 0.31736642 0.34694088][0.21048386 0.19488481 0.15862329 0.13415465 0.12704331 0.13090746 0.13775635 0.14263837 0.13466343 0.1149597 0.097308345 0.10061815 0.13242364 0.17208502 0.20075038][0.065963686 0.055492084 0.025740746 0.0024189511 -0.0062339683 -0.0022204572 0.0070371558 0.012949942 0.0044201347 -0.015413445 -0.031607322 -0.027940236 0.00070773845 0.037180491 0.067747958]]...]
INFO - root - 2017-12-11 03:34:42.641343: step 7510, loss = 0.69, batch loss = 0.63 (14.7 examples/sec; 0.545 sec/batch; 49h:11m:46s remains)
INFO - root - 2017-12-11 03:34:48.106984: step 7520, loss = 0.69, batch loss = 0.63 (15.0 examples/sec; 0.535 sec/batch; 48h:16m:19s remains)
INFO - root - 2017-12-11 03:34:53.339458: step 7530, loss = 0.69, batch loss = 0.64 (19.0 examples/sec; 0.422 sec/batch; 38h:03m:14s remains)
INFO - root - 2017-12-11 03:34:58.866167: step 7540, loss = 0.70, batch loss = 0.64 (14.5 examples/sec; 0.552 sec/batch; 49h:47m:23s remains)
INFO - root - 2017-12-11 03:35:04.409385: step 7550, loss = 0.68, batch loss = 0.63 (14.6 examples/sec; 0.548 sec/batch; 49h:29m:36s remains)
INFO - root - 2017-12-11 03:35:09.860016: step 7560, loss = 0.71, batch loss = 0.65 (14.9 examples/sec; 0.539 sec/batch; 48h:37m:26s remains)
INFO - root - 2017-12-11 03:35:15.329963: step 7570, loss = 0.71, batch loss = 0.65 (14.0 examples/sec; 0.572 sec/batch; 51h:38m:21s remains)
INFO - root - 2017-12-11 03:35:20.865696: step 7580, loss = 0.70, batch loss = 0.64 (14.4 examples/sec; 0.557 sec/batch; 50h:14m:20s remains)
INFO - root - 2017-12-11 03:35:26.355055: step 7590, loss = 0.69, batch loss = 0.63 (14.6 examples/sec; 0.549 sec/batch; 49h:34m:48s remains)
INFO - root - 2017-12-11 03:35:31.832590: step 7600, loss = 0.69, batch loss = 0.63 (14.3 examples/sec; 0.558 sec/batch; 50h:20m:50s remains)
2017-12-11 03:35:32.405983: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.037577696 -0.022619005 -0.010529604 -0.0022709656 0.0011342235 6.9161419e-05 7.6428689e-05 0.0044363337 0.0072076395 0.0064377636 0.0030930871 -0.0048061251 -0.017999116 -0.03554653 -0.050390355][-0.017405733 0.0098819742 0.034436878 0.052819457 0.062229186 0.063842081 0.066522129 0.075146407 0.081950672 0.085202284 0.085762106 0.077848934 0.05726824 0.026264645 -0.0036328337][0.0067653432 0.049366765 0.090288468 0.12196174 0.13902728 0.14430286 0.15018183 0.1636353 0.17756839 0.18983701 0.20014827 0.19663747 0.17038475 0.12497489 0.076334126][0.028124539 0.087394185 0.14818299 0.19748048 0.2255788 0.23571867 0.24292409 0.25836053 0.27970111 0.30456269 0.33007973 0.336443 0.30847517 0.25031564 0.18120204][0.041809916 0.11640994 0.19808778 0.26870126 0.31272468 0.3301954 0.33598897 0.34567434 0.36679092 0.40035793 0.44144547 0.46205086 0.43916261 0.37522897 0.29015407][0.047325809 0.13266346 0.232308 0.32599059 0.39168441 0.42240295 0.42837533 0.42644832 0.43559343 0.4651463 0.51205951 0.54415774 0.5318315 0.47368407 0.38527322][0.045172837 0.13356829 0.24357274 0.35646689 0.44559047 0.49520186 0.50759465 0.49641946 0.48709384 0.49858275 0.53498924 0.56826675 0.56797588 0.52720833 0.45151594][0.036112025 0.1190742 0.22811058 0.34917229 0.45408618 0.52025634 0.54322988 0.53092092 0.50657 0.49424177 0.50770742 0.531356 0.54119867 0.52543581 0.47542146][0.022793442 0.093984909 0.19204572 0.30808577 0.41462576 0.48604617 0.5160743 0.50796211 0.47691151 0.44563386 0.43469262 0.44395778 0.46088687 0.47150308 0.45445082][0.010322998 0.067918 0.14996047 0.25110933 0.34563914 0.40831137 0.43556157 0.42966861 0.39740083 0.35638815 0.32856274 0.32541603 0.34523928 0.37588394 0.39002943][0.0050848164 0.052009378 0.11869437 0.20065781 0.27460593 0.31780538 0.33203444 0.32226023 0.29043195 0.24918111 0.21726194 0.20974363 0.23038261 0.27041858 0.30455017][0.0057146074 0.045848291 0.10089023 0.16504905 0.21764033 0.23871107 0.23553562 0.21719772 0.18507749 0.14904587 0.12351236 0.12049217 0.14218971 0.18235244 0.22265267][0.007856545 0.045240093 0.094939686 0.1482306 0.18640499 0.19213529 0.1758682 0.14951649 0.11606255 0.085111812 0.068242542 0.072004214 0.09339916 0.12814985 0.16626512][0.0080256313 0.045063268 0.093987308 0.14319891 0.17558853 0.17639381 0.15644608 0.12770391 0.094153963 0.066295087 0.054669593 0.061081722 0.077890307 0.10385422 0.13655737][0.0021904411 0.036803283 0.082849041 0.12792952 0.15745029 0.15938461 0.1431946 0.11844633 0.088243507 0.063202783 0.053001706 0.056836922 0.065372363 0.0812633 0.10734064]]...]
INFO - root - 2017-12-11 03:35:37.935252: step 7610, loss = 0.70, batch loss = 0.65 (14.5 examples/sec; 0.552 sec/batch; 49h:46m:34s remains)
INFO - root - 2017-12-11 03:35:43.450484: step 7620, loss = 0.68, batch loss = 0.62 (14.6 examples/sec; 0.547 sec/batch; 49h:21m:15s remains)
INFO - root - 2017-12-11 03:35:48.670364: step 7630, loss = 0.70, batch loss = 0.64 (14.7 examples/sec; 0.546 sec/batch; 49h:15m:55s remains)
INFO - root - 2017-12-11 03:35:54.118782: step 7640, loss = 0.70, batch loss = 0.64 (14.7 examples/sec; 0.546 sec/batch; 49h:14m:04s remains)
INFO - root - 2017-12-11 03:35:59.596611: step 7650, loss = 0.70, batch loss = 0.64 (14.4 examples/sec; 0.556 sec/batch; 50h:07m:55s remains)
INFO - root - 2017-12-11 03:36:05.145382: step 7660, loss = 0.70, batch loss = 0.64 (14.0 examples/sec; 0.570 sec/batch; 51h:24m:32s remains)
INFO - root - 2017-12-11 03:36:10.652125: step 7670, loss = 0.68, batch loss = 0.63 (14.5 examples/sec; 0.552 sec/batch; 49h:47m:52s remains)
INFO - root - 2017-12-11 03:36:16.163698: step 7680, loss = 0.69, batch loss = 0.63 (14.2 examples/sec; 0.565 sec/batch; 50h:57m:11s remains)
INFO - root - 2017-12-11 03:36:21.738959: step 7690, loss = 0.69, batch loss = 0.64 (14.4 examples/sec; 0.554 sec/batch; 50h:00m:24s remains)
INFO - root - 2017-12-11 03:36:27.224541: step 7700, loss = 0.71, batch loss = 0.65 (14.5 examples/sec; 0.552 sec/batch; 49h:46m:03s remains)
2017-12-11 03:36:27.857003: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.16173711 0.14767176 0.13048241 0.12910457 0.14093119 0.14121133 0.1106583 0.05542656 0.002054581 -0.027275527 -0.025684591 0.004356171 0.052986015 0.10584952 0.14273591][0.29207343 0.27274862 0.24554756 0.23917991 0.25161111 0.25219625 0.21538322 0.14670354 0.078522466 0.037735686 0.033887651 0.063106067 0.11262209 0.16584599 0.20224589][0.40173289 0.37751848 0.34259081 0.3310982 0.34292886 0.34717226 0.31484333 0.24737163 0.17635131 0.12922676 0.11710661 0.13614997 0.1720998 0.20874707 0.230015][0.45729691 0.43206751 0.39895117 0.39043212 0.40783498 0.42385885 0.40933952 0.35929474 0.29851454 0.25073889 0.22734709 0.22636697 0.23433027 0.23765281 0.2289203][0.45824033 0.43965837 0.42326277 0.43301815 0.46915528 0.50858325 0.52234721 0.49799022 0.451028 0.40096447 0.36033762 0.3302502 0.3002561 0.26121941 0.21520756][0.42164236 0.41728002 0.42872605 0.46937436 0.53554672 0.60603833 0.64988971 0.64789224 0.60721326 0.54653323 0.48109302 0.41695225 0.34848031 0.27068672 0.19240601][0.36914641 0.38236696 0.42392874 0.49863002 0.59683084 0.69497317 0.75934654 0.76706582 0.72079575 0.64210707 0.550423 0.45873097 0.365465 0.26613453 0.1709674][0.3155466 0.34159803 0.40406314 0.50213814 0.619336 0.72854376 0.79547781 0.79832774 0.73930717 0.64291811 0.53251219 0.4279587 0.3290163 0.22896196 0.13521729][0.27124333 0.30008373 0.36570182 0.46523049 0.57631916 0.67110389 0.7187494 0.70409966 0.6315698 0.52584058 0.41172305 0.31235352 0.2262399 0.14348353 0.06743367][0.24991661 0.26762056 0.31423897 0.38879183 0.46791151 0.52750903 0.5432812 0.50828058 0.42876568 0.32684767 0.22533174 0.14638449 0.085832372 0.031238252 -0.017640145][0.257559 0.25190908 0.25727057 0.28109646 0.3081167 0.32269832 0.3082726 0.26298735 0.19157547 0.10977797 0.035153855 -0.014814897 -0.0465755 -0.07267841 -0.094727561][0.28753018 0.25448436 0.21003209 0.17338488 0.14288373 0.11477458 0.080611445 0.039521609 -0.0094894413 -0.060244326 -0.10228731 -0.12438674 -0.13332835 -0.13878992 -0.14150499][0.32852668 0.27444661 0.1897762 0.10373612 0.027253263 -0.032001939 -0.074668966 -0.10354777 -0.12775348 -0.15031733 -0.16714674 -0.17167887 -0.16882947 -0.16405171 -0.15676381][0.36581576 0.30127686 0.19568156 0.0834618 -0.017406795 -0.092206441 -0.13561347 -0.15267941 -0.15906914 -0.1638656 -0.1671416 -0.16481797 -0.15914428 -0.15216589 -0.14270508][0.37238508 0.30608696 0.19894667 0.0846613 -0.018291038 -0.093360268 -0.13200049 -0.14033625 -0.13695155 -0.13326578 -0.13121235 -0.12775438 -0.12313846 -0.11767621 -0.11032214]]...]
INFO - root - 2017-12-11 03:36:33.368747: step 7710, loss = 0.69, batch loss = 0.63 (14.8 examples/sec; 0.540 sec/batch; 48h:41m:24s remains)
INFO - root - 2017-12-11 03:36:38.869567: step 7720, loss = 0.71, batch loss = 0.65 (14.4 examples/sec; 0.554 sec/batch; 49h:58m:39s remains)
INFO - root - 2017-12-11 03:36:44.096125: step 7730, loss = 0.70, batch loss = 0.64 (14.6 examples/sec; 0.549 sec/batch; 49h:29m:40s remains)
INFO - root - 2017-12-11 03:36:49.568373: step 7740, loss = 0.70, batch loss = 0.64 (14.7 examples/sec; 0.543 sec/batch; 48h:56m:45s remains)
INFO - root - 2017-12-11 03:36:55.004070: step 7750, loss = 0.68, batch loss = 0.62 (14.5 examples/sec; 0.550 sec/batch; 49h:37m:29s remains)
INFO - root - 2017-12-11 03:37:00.606224: step 7760, loss = 0.70, batch loss = 0.64 (15.0 examples/sec; 0.532 sec/batch; 47h:59m:46s remains)
INFO - root - 2017-12-11 03:37:06.094357: step 7770, loss = 0.70, batch loss = 0.64 (15.0 examples/sec; 0.534 sec/batch; 48h:12m:46s remains)
INFO - root - 2017-12-11 03:37:11.633566: step 7780, loss = 0.70, batch loss = 0.64 (14.1 examples/sec; 0.569 sec/batch; 51h:17m:00s remains)
INFO - root - 2017-12-11 03:37:17.195178: step 7790, loss = 0.71, batch loss = 0.65 (14.2 examples/sec; 0.562 sec/batch; 50h:44m:03s remains)
INFO - root - 2017-12-11 03:37:22.606482: step 7800, loss = 0.71, batch loss = 0.65 (14.9 examples/sec; 0.537 sec/batch; 48h:26m:22s remains)
2017-12-11 03:37:23.206456: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.045074541 0.085227787 0.12455311 0.15120314 0.15929532 0.15684436 0.15327106 0.15160014 0.14898999 0.13970207 0.12155068 0.089072548 0.051953543 0.023346985 0.015452634][0.1506124 0.20750931 0.25845516 0.29229951 0.30333602 0.30168018 0.29784402 0.29733187 0.29777661 0.2903924 0.26787823 0.22362183 0.17172536 0.12821651 0.10864429][0.26431954 0.33635896 0.39475894 0.431972 0.44483122 0.44633007 0.44510317 0.44811746 0.45372939 0.45099783 0.4264186 0.37172413 0.305388 0.24631035 0.21298021][0.35220888 0.43252495 0.49212748 0.5285967 0.54344606 0.5527637 0.55915749 0.56829351 0.5794574 0.58085966 0.55359119 0.48856735 0.40845278 0.33497003 0.28880468][0.39873296 0.48570573 0.5495289 0.59050757 0.61354178 0.63595611 0.65309161 0.66801322 0.6809625 0.68142509 0.64723587 0.5686357 0.47210422 0.38287979 0.32321185][0.39119661 0.48574743 0.56142193 0.61764479 0.65891469 0.7006824 0.73208892 0.75291252 0.76402456 0.75770706 0.71180934 0.61695278 0.50208843 0.39577112 0.321407][0.34056953 0.43411711 0.51950783 0.5938465 0.65867567 0.7259025 0.77849966 0.80913639 0.8183648 0.80278569 0.74489772 0.63694865 0.50884259 0.39143068 0.30773658][0.27188259 0.35107049 0.43521968 0.52040821 0.604805 0.69509375 0.76769078 0.80733478 0.81367838 0.78761685 0.72039419 0.60718274 0.47683206 0.36001447 0.27804556][0.2001311 0.25905019 0.33426279 0.42215592 0.51706153 0.61846864 0.69895327 0.73838013 0.73622489 0.69752413 0.62297183 0.51214707 0.39044854 0.28547916 0.21481235][0.14310391 0.17819904 0.23790358 0.32043645 0.4155038 0.51485062 0.5909667 0.62277895 0.60983962 0.56027114 0.48264146 0.38051766 0.2740396 0.18585822 0.12894911][0.11722924 0.11845375 0.14636794 0.20616977 0.28638008 0.37302065 0.43905628 0.46414563 0.44737872 0.39708704 0.32580444 0.24000163 0.1547337 0.086865179 0.045220811][0.12910673 0.089755177 0.0727298 0.094106361 0.14532389 0.21001093 0.26169503 0.28169203 0.2679832 0.22694041 0.17141475 0.10873517 0.049779896 0.0058901981 -0.01795302][0.16212696 0.090892054 0.032894183 0.014481818 0.031686224 0.068560287 0.10115787 0.11400043 0.10426445 0.07559783 0.038420081 -0.00067057996 -0.034694497 -0.057031643 -0.065874562][0.19496027 0.10465874 0.017505316 -0.032781955 -0.045379579 -0.033957407 -0.018364174 -0.011317491 -0.016948575 -0.034302186 -0.056164104 -0.077239648 -0.093755119 -0.10220064 -0.10281668][0.22771372 0.12692139 0.020492977 -0.053328875 -0.088780142 -0.095929019 -0.090731025 -0.0845098 -0.0832537 -0.088901021 -0.097529262 -0.10554904 -0.11124079 -0.1125011 -0.11028696]]...]
INFO - root - 2017-12-11 03:37:28.739283: step 7810, loss = 0.69, batch loss = 0.63 (14.4 examples/sec; 0.555 sec/batch; 50h:02m:49s remains)
INFO - root - 2017-12-11 03:37:33.904500: step 7820, loss = 0.69, batch loss = 0.63 (30.2 examples/sec; 0.265 sec/batch; 23h:55m:22s remains)
INFO - root - 2017-12-11 03:37:39.475506: step 7830, loss = 0.69, batch loss = 0.63 (14.5 examples/sec; 0.551 sec/batch; 49h:42m:54s remains)
INFO - root - 2017-12-11 03:37:44.972524: step 7840, loss = 0.69, batch loss = 0.63 (14.7 examples/sec; 0.543 sec/batch; 48h:56m:44s remains)
INFO - root - 2017-12-11 03:37:50.476332: step 7850, loss = 0.68, batch loss = 0.62 (14.4 examples/sec; 0.556 sec/batch; 50h:07m:09s remains)
INFO - root - 2017-12-11 03:37:56.009171: step 7860, loss = 0.70, batch loss = 0.64 (14.4 examples/sec; 0.554 sec/batch; 49h:58m:46s remains)
INFO - root - 2017-12-11 03:38:01.582211: step 7870, loss = 0.69, batch loss = 0.63 (15.0 examples/sec; 0.532 sec/batch; 47h:58m:00s remains)
INFO - root - 2017-12-11 03:38:07.115729: step 7880, loss = 0.67, batch loss = 0.62 (14.6 examples/sec; 0.548 sec/batch; 49h:26m:31s remains)
INFO - root - 2017-12-11 03:38:12.561158: step 7890, loss = 0.70, batch loss = 0.64 (14.3 examples/sec; 0.560 sec/batch; 50h:27m:04s remains)
INFO - root - 2017-12-11 03:38:18.061694: step 7900, loss = 0.70, batch loss = 0.64 (14.3 examples/sec; 0.561 sec/batch; 50h:36m:35s remains)
2017-12-11 03:38:18.674663: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.077203132 0.075069375 0.061858013 0.044979874 0.03077016 0.026087137 0.027869951 0.032454066 0.034273654 0.031189278 0.025716122 0.021061234 0.021524552 0.026142148 0.030145993][0.14289576 0.14145809 0.12156986 0.095970735 0.075277567 0.069334827 0.073178239 0.080831155 0.08400701 0.079111531 0.07011582 0.062042817 0.061775856 0.0682896 0.074871935][0.19758548 0.19535954 0.16899607 0.13742004 0.11466809 0.11155807 0.12051512 0.13298079 0.13828309 0.13157193 0.11803535 0.10457116 0.1016414 0.10904577 0.1177567][0.22071041 0.21679917 0.18796916 0.15771161 0.14106962 0.14708415 0.1649397 0.18322437 0.18981712 0.17951241 0.15868802 0.13658392 0.12744635 0.13307583 0.14242966][0.20677994 0.20168187 0.17651837 0.15617974 0.15384403 0.17419966 0.20301798 0.22589119 0.23057044 0.21331036 0.18166587 0.14754532 0.12833337 0.12860294 0.13675249][0.1686248 0.16370432 0.14741014 0.14322393 0.16033158 0.19711263 0.2361756 0.26053876 0.25939941 0.23177704 0.18671745 0.13912366 0.10819183 0.10133932 0.1071672][0.12551068 0.12323427 0.11875748 0.13223729 0.16831093 0.21899818 0.26503736 0.28748167 0.27801734 0.23904434 0.18168423 0.12372568 0.083662271 0.070836045 0.07427799][0.098994531 0.10082348 0.10626151 0.13223775 0.18015851 0.23770688 0.284917 0.30274671 0.28582802 0.2402484 0.17864253 0.11981968 0.078333847 0.0630748 0.063483939][0.097083054 0.10319629 0.11401452 0.14408754 0.19371265 0.24915619 0.29104975 0.30266392 0.28258783 0.24010172 0.18697804 0.13892907 0.10460328 0.090896048 0.087664008][0.11531169 0.12437573 0.13488793 0.16042255 0.20218559 0.24818486 0.28116974 0.28876215 0.273814 0.24675746 0.21495292 0.18640715 0.16407354 0.15279216 0.14336167][0.1439888 0.1546111 0.160516 0.17560181 0.20341958 0.2356631 0.25895602 0.26656094 0.26452184 0.26127234 0.25667861 0.24933714 0.2386093 0.22836176 0.21082383][0.16574094 0.17651126 0.17630357 0.17960717 0.19263881 0.21178651 0.22804345 0.23963527 0.25353211 0.27368709 0.29216206 0.30023444 0.29669887 0.28528935 0.26000515][0.166582 0.17561497 0.17003137 0.16407426 0.16577488 0.17584172 0.18862404 0.20526308 0.2311338 0.26542664 0.29532105 0.3093718 0.30784014 0.29420039 0.26436129][0.13884698 0.14456764 0.13610807 0.12584364 0.12223648 0.12848142 0.14055143 0.15993989 0.18937275 0.2249624 0.25358182 0.26506856 0.2617121 0.24654384 0.21695696][0.085143387 0.086465769 0.077785917 0.068267688 0.0652972 0.072111487 0.084820382 0.10403021 0.12983926 0.15743878 0.17691971 0.18163291 0.17520262 0.15970355 0.13372384]]...]
INFO - root - 2017-12-11 03:38:24.247627: step 7910, loss = 0.69, batch loss = 0.64 (14.6 examples/sec; 0.548 sec/batch; 49h:22m:57s remains)
INFO - root - 2017-12-11 03:38:29.505071: step 7920, loss = 0.69, batch loss = 0.63 (14.3 examples/sec; 0.558 sec/batch; 50h:20m:17s remains)
INFO - root - 2017-12-11 03:38:34.996383: step 7930, loss = 0.69, batch loss = 0.63 (14.8 examples/sec; 0.540 sec/batch; 48h:40m:17s remains)
INFO - root - 2017-12-11 03:38:40.504599: step 7940, loss = 0.69, batch loss = 0.64 (14.1 examples/sec; 0.567 sec/batch; 51h:07m:01s remains)
INFO - root - 2017-12-11 03:38:46.059319: step 7950, loss = 0.72, batch loss = 0.66 (14.7 examples/sec; 0.543 sec/batch; 48h:56m:58s remains)
INFO - root - 2017-12-11 03:38:51.451443: step 7960, loss = 0.71, batch loss = 0.65 (14.6 examples/sec; 0.548 sec/batch; 49h:23m:29s remains)
INFO - root - 2017-12-11 03:38:56.965216: step 7970, loss = 0.70, batch loss = 0.64 (14.7 examples/sec; 0.543 sec/batch; 48h:59m:29s remains)
INFO - root - 2017-12-11 03:39:02.389999: step 7980, loss = 0.70, batch loss = 0.64 (14.9 examples/sec; 0.536 sec/batch; 48h:20m:02s remains)
INFO - root - 2017-12-11 03:39:07.922856: step 7990, loss = 0.71, batch loss = 0.65 (14.4 examples/sec; 0.556 sec/batch; 50h:06m:01s remains)
INFO - root - 2017-12-11 03:39:13.466345: step 8000, loss = 0.69, batch loss = 0.63 (14.6 examples/sec; 0.546 sec/batch; 49h:14m:40s remains)
2017-12-11 03:39:14.079254: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.09610372 0.072340734 0.043985017 0.021348404 0.013817792 0.023159185 0.041795604 0.05839723 0.065480016 0.060392916 0.043886442 0.022883536 0.0051901075 -0.0030148183 -0.0026223222][0.051980611 0.035843797 0.020377055 0.013478233 0.020606065 0.039849076 0.061475415 0.074604757 0.074458733 0.060997378 0.038245335 0.014329639 -0.0027474463 -0.0081057595 -0.0041545392][0.018159539 0.013185017 0.013767388 0.025320539 0.049026709 0.079440527 0.10508008 0.1154294 0.10760298 0.084312379 0.053075805 0.024075525 0.0052357726 1.8993378e-05 0.0044084569][-0.0012854881 0.0044891224 0.019211849 0.046223845 0.083718024 0.12413321 0.15507148 0.1656597 0.15298094 0.12110539 0.0809047 0.044936951 0.021375237 0.01294929 0.01503861][-0.0040576668 0.0096878493 0.033935893 0.070329078 0.11563435 0.16200191 0.19732928 0.20959848 0.19449082 0.15672722 0.10954513 0.066995688 0.03711481 0.02286591 0.020143952][0.0040134392 0.022949167 0.052754037 0.0938013 0.14230026 0.19075963 0.22853293 0.24245583 0.22624904 0.18471609 0.13291202 0.085539937 0.049724277 0.029076925 0.020777298][0.014197671 0.036676858 0.069511428 0.11190131 0.15990025 0.20710506 0.2450936 0.26067975 0.24590929 0.20500503 0.15266603 0.10300305 0.062814578 0.036677759 0.023497647][0.022334294 0.049200647 0.085166812 0.12756091 0.17177995 0.21266997 0.24525224 0.25884262 0.24580759 0.20951951 0.16206697 0.11523809 0.074920461 0.046487849 0.03056783][0.032274388 0.064944223 0.10478376 0.1469422 0.18550798 0.21625394 0.2384695 0.246404 0.23437969 0.20423591 0.16469675 0.12475121 0.088987157 0.062253844 0.046243414][0.046305306 0.083477952 0.12541209 0.16634934 0.19944794 0.22082965 0.23327617 0.23640816 0.22665775 0.20379631 0.17344142 0.14265202 0.11480062 0.092878394 0.078285381][0.056471236 0.095046006 0.13642566 0.17527962 0.20484056 0.22105061 0.22831631 0.23023759 0.22468083 0.20976271 0.18910167 0.16882822 0.1511389 0.13637684 0.12495576][0.058978904 0.096061483 0.13491315 0.17114262 0.19886252 0.2140123 0.22109953 0.22571883 0.22639924 0.22060208 0.21062087 0.20169461 0.19470671 0.18765022 0.18067215][0.059219416 0.092573412 0.12743819 0.16063835 0.1875758 0.20479953 0.21639961 0.22803685 0.23747133 0.2417884 0.24237218 0.24327618 0.24386604 0.24112284 0.23691852][0.06055373 0.088270508 0.11745635 0.14624164 0.17179485 0.19218624 0.2111403 0.23311435 0.25372487 0.26915827 0.27960256 0.28763166 0.29174709 0.28924683 0.28505528][0.063310541 0.084471308 0.10666467 0.12924689 0.15159862 0.17421608 0.20091921 0.23443961 0.26710543 0.29372555 0.3123982 0.32406262 0.32714915 0.32056272 0.31291616]]...]
INFO - root - 2017-12-11 03:39:19.598828: step 8010, loss = 0.72, batch loss = 0.66 (14.8 examples/sec; 0.540 sec/batch; 48h:38m:43s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-SGD-lr0.001-clip50-initfromfixqian30k-fixqian
INFO - root - 2017-12-11 03:39:24.636130: step 8020, loss = 0.70, batch loss = 0.64 (14.5 examples/sec; 0.554 sec/batch; 49h:53m:31s remains)
INFO - root - 2017-12-11 03:39:30.121106: step 8030, loss = 0.68, batch loss = 0.62 (13.9 examples/sec; 0.576 sec/batch; 51h:55m:54s remains)
INFO - root - 2017-12-11 03:39:35.612258: step 8040, loss = 0.70, batch loss = 0.64 (14.7 examples/sec; 0.545 sec/batch; 49h:09m:00s remains)
INFO - root - 2017-12-11 03:39:41.046296: step 8050, loss = 0.69, batch loss = 0.63 (15.1 examples/sec; 0.529 sec/batch; 47h:39m:26s remains)
INFO - root - 2017-12-11 03:39:46.472725: step 8060, loss = 0.71, batch loss = 0.65 (14.9 examples/sec; 0.537 sec/batch; 48h:22m:35s remains)
INFO - root - 2017-12-11 03:39:51.964101: step 8070, loss = 0.70, batch loss = 0.64 (14.4 examples/sec; 0.555 sec/batch; 50h:00m:57s remains)
INFO - root - 2017-12-11 03:39:57.552995: step 8080, loss = 0.70, batch loss = 0.64 (14.5 examples/sec; 0.551 sec/batch; 49h:36m:48s remains)
INFO - root - 2017-12-11 03:40:02.979436: step 8090, loss = 0.70, batch loss = 0.64 (14.6 examples/sec; 0.548 sec/batch; 49h:22m:40s remains)
INFO - root - 2017-12-11 03:40:08.455216: step 8100, loss = 0.71, batch loss = 0.65 (15.2 examples/sec; 0.527 sec/batch; 47h:30m:26s remains)
2017-12-11 03:40:09.066772: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.13438119 0.13088694 0.12262835 0.11541474 0.1105087 0.10774909 0.10760735 0.10940394 0.11555175 0.12665312 0.13695121 0.13566282 0.1168792 0.082387581 0.038264632][0.16490796 0.16288535 0.15189631 0.14127269 0.13361393 0.1294961 0.12764131 0.12588836 0.12664926 0.13140872 0.13603233 0.13069132 0.11043941 0.076660454 0.034392405][0.20277634 0.20096555 0.18718287 0.17424916 0.16518649 0.16008899 0.15587159 0.14927995 0.14309981 0.13967903 0.13705365 0.12708668 0.10563357 0.0733505 0.033555564][0.24129115 0.23746105 0.22098261 0.20745994 0.19951621 0.19525596 0.18979661 0.17925292 0.16713987 0.15651129 0.14690387 0.13183433 0.10815243 0.075864121 0.03658307][0.27015767 0.26281381 0.24364033 0.22995572 0.22425358 0.22261198 0.2184836 0.20745809 0.1934116 0.17975828 0.16609311 0.14680472 0.12019155 0.08591526 0.044930011][0.28739756 0.27769685 0.25778219 0.24528953 0.24330316 0.24634692 0.24641442 0.23819298 0.22569415 0.21246827 0.19728477 0.17426643 0.14305286 0.10403985 0.058212765][0.28906164 0.28233817 0.26781741 0.26177967 0.26781276 0.27888247 0.28557277 0.28153855 0.27135989 0.25869161 0.24160704 0.21381168 0.17561358 0.12859668 0.074348792][0.2746636 0.27502877 0.2706809 0.27512917 0.29161191 0.31146747 0.32423329 0.32360724 0.31470302 0.30143812 0.28173232 0.24879965 0.2033536 0.14753568 0.08432164][0.2439914 0.25155413 0.25776649 0.27206907 0.29639223 0.32102194 0.33591172 0.3364912 0.32854053 0.31596437 0.29635671 0.26230231 0.21410784 0.15369877 0.085425481][0.20330991 0.21553399 0.22920713 0.24979667 0.27716395 0.30136642 0.31414092 0.31389561 0.30705929 0.29689255 0.28026876 0.24927388 0.20336236 0.14399324 0.076207176][0.16397038 0.17719126 0.19397044 0.21696061 0.24343367 0.26333761 0.27073228 0.26755646 0.26170698 0.25529751 0.2440853 0.21934174 0.17922114 0.12475565 0.060869094][0.13459516 0.14627051 0.16213407 0.18326078 0.2053307 0.2186359 0.21954834 0.21291804 0.20841694 0.2069542 0.20259827 0.18537119 0.15226431 0.10389287 0.044837669][0.1176189 0.12489295 0.13463391 0.14821827 0.16216828 0.16787885 0.16355723 0.15514909 0.15359025 0.15854484 0.16209748 0.15279998 0.12687741 0.084887244 0.03105651][0.11263574 0.11256927 0.11196388 0.11413013 0.11831415 0.1177773 0.11135429 0.10368601 0.10589803 0.11660569 0.12671232 0.12384093 0.1040515 0.068569019 0.021073686][0.11868738 0.11167976 0.10154365 0.0939463 0.090833105 0.087341957 0.08184734 0.076234519 0.080750495 0.093366094 0.10515636 0.10396883 0.0867759 0.056009684 0.014767564]]...]
INFO - root - 2017-12-11 03:40:14.612326: step 8110, loss = 0.68, batch loss = 0.62 (14.4 examples/sec; 0.555 sec/batch; 49h:59m:23s remains)
INFO - root - 2017-12-11 03:40:19.851820: step 8120, loss = 0.68, batch loss = 0.62 (14.8 examples/sec; 0.542 sec/batch; 48h:49m:05s remains)
INFO - root - 2017-12-11 03:40:25.337362: step 8130, loss = 0.70, batch loss = 0.64 (14.5 examples/sec; 0.553 sec/batch; 49h:47m:24s remains)
INFO - root - 2017-12-11 03:40:30.804093: step 8140, loss = 0.66, batch loss = 0.60 (14.7 examples/sec; 0.545 sec/batch; 49h:05m:37s remains)
INFO - root - 2017-12-11 03:40:36.330208: step 8150, loss = 0.68, batch loss = 0.62 (14.6 examples/sec; 0.547 sec/batch; 49h:15m:13s remains)
INFO - root - 2017-12-11 03:40:41.862330: step 8160, loss = 0.71, batch loss = 0.65 (14.4 examples/sec; 0.554 sec/batch; 49h:53m:47s remains)
INFO - root - 2017-12-11 03:40:47.268159: step 8170, loss = 0.68, batch loss = 0.62 (14.9 examples/sec; 0.536 sec/batch; 48h:16m:59s remains)
INFO - root - 2017-12-11 03:40:52.879953: step 8180, loss = 0.70, batch loss = 0.64 (14.5 examples/sec; 0.552 sec/batch; 49h:42m:42s remains)
INFO - root - 2017-12-11 03:40:58.331998: step 8190, loss = 0.68, batch loss = 0.62 (14.4 examples/sec; 0.556 sec/batch; 50h:04m:17s remains)
INFO - root - 2017-12-11 03:41:03.884492: step 8200, loss = 0.71, batch loss = 0.65 (14.8 examples/sec; 0.540 sec/batch; 48h:39m:43s remains)
2017-12-11 03:41:04.369377: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0687571 -0.039250445 0.0060786195 0.063936286 0.12033216 0.16048166 0.16997214 0.15901999 0.14370155 0.12900604 0.11638951 0.1020724 0.083531924 0.058086228 0.027037103][-0.075059123 -0.047367197 -0.0014364568 0.058122531 0.11607821 0.15614188 0.1634105 0.1486726 0.12886582 0.10987636 0.093684755 0.076875582 0.057040378 0.030253667 -0.0017767869][-0.07260251 -0.043075446 0.0077949748 0.073851675 0.13794892 0.18014036 0.18351237 0.15783507 0.12122194 0.083865181 0.052486163 0.026787205 0.0047312616 -0.019569134 -0.045901973][-0.066464022 -0.03383147 0.023404215 0.098193444 0.17144738 0.21931133 0.22161199 0.18579242 0.1304611 0.07077723 0.019216402 -0.020255117 -0.048188012 -0.071056142 -0.090690583][-0.060875025 -0.0265286 0.034587327 0.11588108 0.19800888 0.25501463 0.26333809 0.22656754 0.16109765 0.085186221 0.015823012 -0.038313247 -0.074686877 -0.098768376 -0.11370606][-0.05784509 -0.02320862 0.039117508 0.12558798 0.21948591 0.29468378 0.32265249 0.29981768 0.23691139 0.1522439 0.066589907 -0.0050839828 -0.055204295 -0.086932011 -0.10409322][-0.056060854 -0.021714913 0.041612457 0.13537787 0.24696431 0.35003242 0.40814608 0.40793586 0.35393605 0.26403502 0.16319658 0.073207147 0.00697432 -0.036089532 -0.060380045][-0.055795755 -0.022335259 0.041933239 0.14398243 0.27479389 0.40714347 0.49522993 0.51604778 0.47055694 0.37721848 0.26476771 0.16021186 0.080370285 0.026576729 -0.0055799182][-0.056607768 -0.025195383 0.038294572 0.14500654 0.28794026 0.43917641 0.54601163 0.57951397 0.54028285 0.44758877 0.33150077 0.22058135 0.1333963 0.072744124 0.034644607][-0.055579204 -0.025850236 0.034681574 0.13796425 0.27734789 0.42563149 0.53016877 0.56336051 0.52822876 0.44308466 0.33501574 0.2297197 0.14528616 0.085479923 0.0471186][-0.053142115 -0.023976102 0.032123927 0.12413201 0.24419238 0.36792549 0.45021182 0.47121632 0.43746313 0.36447382 0.27327466 0.18401456 0.11279494 0.06342642 0.032640185][-0.053293597 -0.025743796 0.023436531 0.097936027 0.18862343 0.27584788 0.32616234 0.32955366 0.29510242 0.23501924 0.16402741 0.096561648 0.046364021 0.015903397 0.0004448891][-0.05719728 -0.032482553 0.00901346 0.065656237 0.12764177 0.18010981 0.2008335 0.18768004 0.15117194 0.10119151 0.048172176 0.0020534308 -0.026134178 -0.036315788 -0.034954749][-0.060353108 -0.03796643 -0.0016507371 0.042948268 0.086049959 0.11575577 0.11709233 0.092301473 0.052821513 0.0080673015 -0.033154916 -0.063418835 -0.075108945 -0.071136981 -0.058820352][-0.0606485 -0.039149065 -0.0043284781 0.036838356 0.07508754 0.098988414 0.095106229 0.065965161 0.023723062 -0.021122895 -0.058958896 -0.082583643 -0.087294847 -0.078633323 -0.063697226]]...]
INFO - root - 2017-12-11 03:41:09.598339: step 8210, loss = 0.69, batch loss = 0.64 (13.9 examples/sec; 0.577 sec/batch; 51h:58m:11s remains)
INFO - root - 2017-12-11 03:41:15.111650: step 8220, loss = 0.70, batch loss = 0.64 (14.7 examples/sec; 0.546 sec/batch; 49h:11m:16s remains)
INFO - root - 2017-12-11 03:41:20.519036: step 8230, loss = 0.70, batch loss = 0.64 (14.3 examples/sec; 0.561 sec/batch; 50h:29m:41s remains)
INFO - root - 2017-12-11 03:41:25.955424: step 8240, loss = 0.69, batch loss = 0.64 (14.7 examples/sec; 0.543 sec/batch; 48h:52m:48s remains)
INFO - root - 2017-12-11 03:41:31.469907: step 8250, loss = 0.68, batch loss = 0.62 (14.7 examples/sec; 0.543 sec/batch; 48h:54m:13s remains)
INFO - root - 2017-12-11 03:41:36.985262: step 8260, loss = 0.71, batch loss = 0.65 (14.6 examples/sec; 0.547 sec/batch; 49h:13m:26s remains)
INFO - root - 2017-12-11 03:41:42.456175: step 8270, loss = 0.71, batch loss = 0.65 (14.5 examples/sec; 0.552 sec/batch; 49h:42m:49s remains)
INFO - root - 2017-12-11 03:41:47.905612: step 8280, loss = 0.69, batch loss = 0.63 (14.3 examples/sec; 0.561 sec/batch; 50h:31m:19s remains)
INFO - root - 2017-12-11 03:41:53.512069: step 8290, loss = 0.70, batch loss = 0.64 (14.6 examples/sec; 0.548 sec/batch; 49h:21m:45s remains)
INFO - root - 2017-12-11 03:41:58.972826: step 8300, loss = 0.70, batch loss = 0.65 (14.4 examples/sec; 0.554 sec/batch; 49h:51m:53s remains)
2017-12-11 03:41:59.596037: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.054652765 -0.053647593 -0.052209333 -0.053300403 -0.056089524 -0.058397051 -0.058956679 -0.056544509 -0.052220896 -0.048539758 -0.046277132 -0.045831162 -0.047288574 -0.051642131 -0.059561398][-0.03151926 -0.024054814 -0.017993364 -0.016891588 -0.019348325 -0.021696398 -0.02109126 -0.015552795 -0.0063130958 0.001992417 0.0070217648 0.0078727817 0.0044332328 -0.0053771269 -0.023660008][0.0096299788 0.029382495 0.045501884 0.052837741 0.052523289 0.050006747 0.050237164 0.057875153 0.072586231 0.087233678 0.096321553 0.097415917 0.090301566 0.07133808 0.037539016][0.061867975 0.098202161 0.12851247 0.14535001 0.14882466 0.14593989 0.14429744 0.15275529 0.17330465 0.19554234 0.209771 0.21068028 0.19728673 0.16461565 0.11042399][0.11243434 0.16709433 0.21429853 0.24459469 0.2565017 0.25680879 0.25420567 0.26146168 0.28489417 0.31203151 0.32859406 0.32644406 0.30411619 0.25645429 0.1821478][0.16224413 0.23508458 0.2987839 0.34271193 0.36415023 0.36855665 0.36478278 0.36802539 0.38880715 0.41399819 0.42667359 0.41776404 0.38652596 0.32848656 0.24121287][0.2061467 0.29374519 0.36972129 0.4234277 0.45207796 0.45946914 0.45421773 0.45199442 0.46482733 0.48025021 0.48202193 0.46274117 0.42418557 0.36293775 0.27384129][0.23117188 0.32696182 0.40902197 0.46711129 0.49901223 0.506774 0.49926955 0.49084255 0.49254212 0.49457014 0.48428053 0.45678246 0.41554841 0.3584795 0.27734482][0.23069546 0.32513148 0.40492153 0.46086067 0.49129465 0.49700573 0.48694488 0.47218469 0.46259505 0.45297489 0.4356575 0.40704593 0.36990792 0.32373708 0.25760597][0.20478736 0.28804302 0.35751724 0.4055309 0.43048427 0.43202415 0.41857871 0.397646 0.37879276 0.362592 0.34557235 0.32385302 0.29767904 0.26784992 0.22083549][0.15923654 0.22362521 0.27683008 0.31226337 0.32786494 0.32287392 0.30516541 0.2799418 0.2563841 0.23934357 0.22848 0.21897444 0.20754954 0.1952471 0.16730608][0.095129386 0.13634562 0.17044058 0.19160108 0.19764265 0.18826661 0.16984785 0.1456133 0.12326719 0.11003337 0.1070366 0.10876264 0.10926409 0.10984339 0.097017862][0.02770113 0.047413472 0.064008519 0.072159804 0.070126273 0.058994528 0.043189485 0.024272259 0.0081381937 0.0021456138 0.0072433949 0.017490812 0.02561897 0.032935172 0.029277895][-0.027587507 -0.02329186 -0.019661928 -0.02116341 -0.028660748 -0.03989828 -0.051942181 -0.064119 -0.072186604 -0.07057222 -0.059501562 -0.044735312 -0.033340096 -0.024336878 -0.023806076][-0.068258382 -0.073011905 -0.076467372 -0.082498051 -0.091302775 -0.10083199 -0.10926785 -0.1160606 -0.11827593 -0.1126052 -0.1003953 -0.086087078 -0.075117707 -0.066900991 -0.064447932]]...]
INFO - root - 2017-12-11 03:42:04.781246: step 8310, loss = 0.71, batch loss = 0.65 (14.2 examples/sec; 0.563 sec/batch; 50h:42m:16s remains)
INFO - root - 2017-12-11 03:42:10.327704: step 8320, loss = 0.70, batch loss = 0.65 (14.4 examples/sec; 0.554 sec/batch; 49h:54m:38s remains)
INFO - root - 2017-12-11 03:42:15.827647: step 8330, loss = 0.68, batch loss = 0.62 (14.5 examples/sec; 0.551 sec/batch; 49h:38m:02s remains)
INFO - root - 2017-12-11 03:42:21.326711: step 8340, loss = 0.70, batch loss = 0.64 (14.2 examples/sec; 0.562 sec/batch; 50h:33m:56s remains)
INFO - root - 2017-12-11 03:42:26.898475: step 8350, loss = 0.72, batch loss = 0.66 (14.7 examples/sec; 0.543 sec/batch; 48h:54m:28s remains)
INFO - root - 2017-12-11 03:42:32.384813: step 8360, loss = 0.70, batch loss = 0.65 (14.9 examples/sec; 0.537 sec/batch; 48h:18m:36s remains)
INFO - root - 2017-12-11 03:42:37.941250: step 8370, loss = 0.70, batch loss = 0.65 (14.9 examples/sec; 0.538 sec/batch; 48h:27m:55s remains)
INFO - root - 2017-12-11 03:42:43.401424: step 8380, loss = 0.71, batch loss = 0.65 (14.8 examples/sec; 0.540 sec/batch; 48h:39m:25s remains)
INFO - root - 2017-12-11 03:42:48.866439: step 8390, loss = 0.70, batch loss = 0.64 (14.6 examples/sec; 0.548 sec/batch; 49h:18m:21s remains)
INFO - root - 2017-12-11 03:42:54.318191: step 8400, loss = 0.70, batch loss = 0.64 (14.7 examples/sec; 0.545 sec/batch; 49h:01m:47s remains)
2017-12-11 03:42:54.912245: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.030785363 0.011521949 0.091848835 0.19987458 0.31285718 0.39863539 0.42498672 0.37870589 0.27658367 0.15520975 0.051829875 -0.010959832 -0.031562105 -0.025487168 -0.0093006669][-0.045520715 -0.0076401294 0.066031605 0.16782516 0.2783446 0.36816239 0.40646768 0.38047573 0.29997215 0.19162239 0.088866778 0.016396856 -0.01845382 -0.02690967 -0.021422204][-0.06424202 -0.031052796 0.034422874 0.1275087 0.23334736 0.32678589 0.37988919 0.37990957 0.32811832 0.23951878 0.1402472 0.055433124 -0.0014370194 -0.033497017 -0.046427026][-0.080492415 -0.049936015 0.011580487 0.10177607 0.20976053 0.31379884 0.38707158 0.41534311 0.39209688 0.32110733 0.22095259 0.11701573 0.029732704 -0.03415725 -0.072139889][-0.089927629 -0.058915172 0.0041081468 0.099086352 0.21796681 0.3401086 0.43744045 0.49263021 0.49267387 0.4330757 0.32643518 0.19859929 0.077413976 -0.020417275 -0.08490213][-0.090391219 -0.056193553 0.012112382 0.11659771 0.25101879 0.39422408 0.51556706 0.59323472 0.60884923 0.55275303 0.43477908 0.2827203 0.13106437 0.0048781284 -0.08056438][-0.082672581 -0.042803057 0.033431519 0.14892811 0.2977652 0.45744437 0.59522855 0.68570405 0.70716119 0.64774233 0.51771194 0.34755927 0.17619805 0.034149744 -0.061236721][-0.070084006 -0.022281632 0.063636124 0.18938871 0.34744197 0.51334572 0.65359551 0.7420615 0.75772828 0.69055897 0.55307525 0.37710381 0.20239088 0.060762797 -0.031781092][-0.056757569 -0.0014244996 0.092438556 0.22328153 0.38042316 0.53743631 0.66202921 0.73158193 0.73075491 0.65451038 0.51822472 0.35280326 0.19456749 0.07157021 -0.0036968233][-0.049792286 0.0082884757 0.10205208 0.22630763 0.36768702 0.49936569 0.59277064 0.63221562 0.61106908 0.53067821 0.40808624 0.27049148 0.14722839 0.058979563 0.01481347][-0.053951 -0.0021907692 0.079484992 0.18306027 0.29457894 0.38948607 0.44501668 0.45343447 0.41673905 0.34141469 0.24411096 0.14676864 0.070308685 0.027328148 0.024142442][-0.0683693 -0.031876188 0.027165098 0.099564075 0.17343675 0.22918004 0.25058779 0.23674156 0.19560987 0.1358941 0.071745493 0.019385358 -0.0079624355 -0.0046712421 0.033262655][-0.087134831 -0.069714345 -0.037333064 0.0016969624 0.039475825 0.062782146 0.061426293 0.038648859 0.0045362362 -0.032431941 -0.061616335 -0.072296627 -0.057219263 -0.015880201 0.053417604][-0.10288447 -0.10300902 -0.094026655 -0.082679726 -0.072505131 -0.070358172 -0.081273094 -0.10065586 -0.11940889 -0.13085739 -0.12778971 -0.1044301 -0.058097545 0.0082481476 0.093758367][-0.11265065 -0.12488282 -0.13162355 -0.13801548 -0.14401017 -0.15100551 -0.15968019 -0.16585211 -0.16329606 -0.14806569 -0.11637887 -0.067423292 -0.00280649 0.0726986 0.15657294]]...]
INFO - root - 2017-12-11 03:43:00.048219: step 8410, loss = 0.69, batch loss = 0.63 (14.4 examples/sec; 0.557 sec/batch; 50h:06m:52s remains)
INFO - root - 2017-12-11 03:43:05.590161: step 8420, loss = 0.67, batch loss = 0.61 (14.9 examples/sec; 0.538 sec/batch; 48h:24m:03s remains)
INFO - root - 2017-12-11 03:43:11.097794: step 8430, loss = 0.68, batch loss = 0.63 (14.3 examples/sec; 0.558 sec/batch; 50h:13m:48s remains)
INFO - root - 2017-12-11 03:43:16.633333: step 8440, loss = 0.70, batch loss = 0.64 (14.0 examples/sec; 0.572 sec/batch; 51h:31m:43s remains)
INFO - root - 2017-12-11 03:43:22.094649: step 8450, loss = 0.67, batch loss = 0.61 (15.1 examples/sec; 0.530 sec/batch; 47h:41m:41s remains)
INFO - root - 2017-12-11 03:43:27.553547: step 8460, loss = 0.70, batch loss = 0.64 (15.1 examples/sec; 0.531 sec/batch; 47h:49m:57s remains)
INFO - root - 2017-12-11 03:43:33.026830: step 8470, loss = 0.69, batch loss = 0.63 (14.9 examples/sec; 0.535 sec/batch; 48h:10m:25s remains)
INFO - root - 2017-12-11 03:43:38.607171: step 8480, loss = 0.69, batch loss = 0.63 (14.7 examples/sec; 0.545 sec/batch; 49h:03m:37s remains)
INFO - root - 2017-12-11 03:43:44.056220: step 8490, loss = 0.69, batch loss = 0.63 (14.5 examples/sec; 0.550 sec/batch; 49h:31m:07s remains)
INFO - root - 2017-12-11 03:43:49.500600: step 8500, loss = 0.71, batch loss = 0.65 (14.5 examples/sec; 0.551 sec/batch; 49h:34m:29s remains)
2017-12-11 03:43:49.978162: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.043778036 -0.044930197 -0.0447416 -0.044662539 -0.044721574 -0.044692546 -0.0447258 -0.044802703 -0.045147423 -0.045744281 -0.046023242 -0.046356358 -0.046834283 -0.047329705 -0.047277097][-0.034136437 -0.034553811 -0.034475062 -0.035151053 -0.036517486 -0.037702255 -0.038495939 -0.038759813 -0.038864303 -0.038891714 -0.038403291 -0.03833336 -0.038814772 -0.039729465 -0.040127032][-0.016847832 -0.01539259 -0.015143874 -0.016502991 -0.019500244 -0.022850199 -0.025631692 -0.026927853 -0.027071377 -0.026637109 -0.025484029 -0.024925904 -0.025428876 -0.026945051 -0.028265497][0.012450749 0.017209092 0.018168632 0.016199917 0.010659082 0.0029629942 -0.0044827494 -0.0089392634 -0.010051354 -0.0097447615 -0.0081410892 -0.0068845092 -0.0070003336 -0.0088851452 -0.011885464][0.058872141 0.06877467 0.071197122 0.068513811 0.058842815 0.044067133 0.028925223 0.019083133 0.016111648 0.015789244 0.017939644 0.020703088 0.022130273 0.020572992 0.015225499][0.1165325 0.1321148 0.13609815 0.13216291 0.11686902 0.093165167 0.068692721 0.052520722 0.047586106 0.046779834 0.050051477 0.055590846 0.059949022 0.059554044 0.051777206][0.16903447 0.18820207 0.19231775 0.1859597 0.16440627 0.13197626 0.099260338 0.077957846 0.072000474 0.071508147 0.076577246 0.085712694 0.0936498 0.09505257 0.085696973][0.19718522 0.21564882 0.21710725 0.20664378 0.17919935 0.14058918 0.10314044 0.079674184 0.074146777 0.074755445 0.081752829 0.09395659 0.10494325 0.1083499 0.099125527][0.18819171 0.20162061 0.19809611 0.18294828 0.15193956 0.11186557 0.074960649 0.053166352 0.0493248 0.051235411 0.059142549 0.072224595 0.084080532 0.08875525 0.0814857][0.14065896 0.14630127 0.1378163 0.12016738 0.090684168 0.055736635 0.025471583 0.00925264 0.008142014 0.011264409 0.018614251 0.029869141 0.039911889 0.044534955 0.040236291][0.073041521 0.071193062 0.059917919 0.043217741 0.020141812 -0.0046850168 -0.024502749 -0.03328602 -0.031485531 -0.0275657 -0.021915076 -0.014327435 -0.0078226356 -0.0043924619 -0.005976439][0.0098462747 0.0036970084 -0.007000586 -0.019486325 -0.03348878 -0.046676882 -0.055925038 -0.058106869 -0.054302774 -0.050221179 -0.046521235 -0.042650744 -0.039692897 -0.038009513 -0.038223319][-0.030318346 -0.037331913 -0.045095529 -0.052190736 -0.05778268 -0.061431374 -0.062887028 -0.060878962 -0.056287456 -0.052464809 -0.050095417 -0.04852169 -0.047778923 -0.047575139 -0.047787331][-0.042116269 -0.047813378 -0.052275177 -0.055112023 -0.055449024 -0.053953808 -0.051684812 -0.048280679 -0.043998502 -0.040630233 -0.038717136 -0.037815873 -0.037795022 -0.038375948 -0.039213303][-0.033494212 -0.037223961 -0.039500229 -0.04036719 -0.039081663 -0.03669906 -0.034438647 -0.031984407 -0.029030202 -0.026502114 -0.024721293 -0.023507806 -0.023130521 -0.023657311 -0.024726536]]...]
INFO - root - 2017-12-11 03:43:55.297106: step 8510, loss = 0.71, batch loss = 0.65 (14.6 examples/sec; 0.548 sec/batch; 49h:17m:58s remains)
INFO - root - 2017-12-11 03:44:00.724586: step 8520, loss = 0.70, batch loss = 0.64 (14.6 examples/sec; 0.546 sec/batch; 49h:10m:16s remains)
INFO - root - 2017-12-11 03:44:06.288168: step 8530, loss = 0.70, batch loss = 0.64 (14.5 examples/sec; 0.551 sec/batch; 49h:33m:48s remains)
INFO - root - 2017-12-11 03:44:11.827850: step 8540, loss = 0.68, batch loss = 0.62 (14.4 examples/sec; 0.556 sec/batch; 50h:03m:21s remains)
INFO - root - 2017-12-11 03:44:17.279932: step 8550, loss = 0.68, batch loss = 0.63 (14.8 examples/sec; 0.540 sec/batch; 48h:37m:23s remains)
INFO - root - 2017-12-11 03:44:22.693935: step 8560, loss = 0.69, batch loss = 0.64 (15.1 examples/sec; 0.529 sec/batch; 47h:35m:41s remains)
INFO - root - 2017-12-11 03:44:28.117494: step 8570, loss = 0.70, batch loss = 0.64 (14.6 examples/sec; 0.546 sec/batch; 49h:10m:16s remains)
INFO - root - 2017-12-11 03:44:33.598010: step 8580, loss = 0.69, batch loss = 0.64 (14.2 examples/sec; 0.564 sec/batch; 50h:44m:06s remains)
INFO - root - 2017-12-11 03:44:39.091526: step 8590, loss = 0.68, batch loss = 0.62 (14.2 examples/sec; 0.562 sec/batch; 50h:35m:20s remains)
INFO - root - 2017-12-11 03:44:44.238818: step 8600, loss = 0.69, batch loss = 0.63 (31.5 examples/sec; 0.254 sec/batch; 22h:48m:51s remains)
2017-12-11 03:44:44.752624: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.056813944 0.056738604 0.055737682 0.054688685 0.054384694 0.055245418 0.056808215 0.058911458 0.061058071 0.063101649 0.065173924 0.0673028 0.069994658 0.085981779 0.11683388][0.052578151 0.053994324 0.053889282 0.052485373 0.051143557 0.050707385 0.050993513 0.052812167 0.055155993 0.05722094 0.058854304 0.060085025 0.0621623 0.077921748 0.10908005][0.051522821 0.057363097 0.060108542 0.05843015 0.054649696 0.050920893 0.048102621 0.048148178 0.049388919 0.050536916 0.050547015 0.049254335 0.048590165 0.0605772 0.087383211][0.060475979 0.072681755 0.079598442 0.078257956 0.072107092 0.065579891 0.0610261 0.060852323 0.062019389 0.063019991 0.061554931 0.05645306 0.049894128 0.053381532 0.070407584][0.080629356 0.098991685 0.11050776 0.11157171 0.10692535 0.1032269 0.10294566 0.10709423 0.11066273 0.11177453 0.10755033 0.095730647 0.078914508 0.069327839 0.072581463][0.11638376 0.13584164 0.14780603 0.1504755 0.15110987 0.15732539 0.16837677 0.18114245 0.1885009 0.18856288 0.17799881 0.15513648 0.1243306 0.10005103 0.089547507][0.1626863 0.17893092 0.18709587 0.18922852 0.19626707 0.21607944 0.24160552 0.26346916 0.27326703 0.2696844 0.24945661 0.21304631 0.16795905 0.13186403 0.11204453][0.21112509 0.22561538 0.22871695 0.22751451 0.23700117 0.26587403 0.30011863 0.32491761 0.33234835 0.32208073 0.29121169 0.24281514 0.18793136 0.14690293 0.12549815][0.2691364 0.28279203 0.28008437 0.27159584 0.27477822 0.30017957 0.3290008 0.34402156 0.3404468 0.31998318 0.28027052 0.2254594 0.1689496 0.13200861 0.11775102][0.33607492 0.34934217 0.34123576 0.32352063 0.31292951 0.32144815 0.32991937 0.32306111 0.30129072 0.2690258 0.22425567 0.17018482 0.12041025 0.094990529 0.094232231][0.38249537 0.3951728 0.38337928 0.35730883 0.33033937 0.31523696 0.29779786 0.267472 0.22914641 0.18927772 0.14594874 0.10024039 0.063647121 0.05367079 0.068819061][0.36717561 0.37801033 0.36558664 0.33599222 0.29849765 0.26616487 0.23166475 0.18925929 0.1455332 0.1073776 0.072976418 0.041340653 0.020692987 0.025956605 0.055241123][0.28214958 0.28894722 0.27761158 0.24985468 0.21179248 0.17605019 0.1410093 0.10392266 0.06990882 0.044250969 0.025271451 0.01087743 0.005859524 0.022551842 0.059871364][0.16108939 0.16089836 0.1499252 0.12719223 0.097051241 0.070388921 0.048434548 0.029020155 0.013919878 0.0059679551 0.0039316406 0.0059854509 0.01417331 0.037896857 0.07683184][0.057935368 0.051488735 0.041097872 0.0251396 0.0069484087 -0.0055378219 -0.010762318 -0.011501843 -0.0087243691 -0.0013961798 0.0099356519 0.024536736 0.042637464 0.070404753 0.10770376]]...]
INFO - root - 2017-12-11 03:44:50.187159: step 8610, loss = 0.70, batch loss = 0.64 (14.6 examples/sec; 0.549 sec/batch; 49h:21m:03s remains)
INFO - root - 2017-12-11 03:44:55.690083: step 8620, loss = 0.70, batch loss = 0.65 (14.5 examples/sec; 0.552 sec/batch; 49h:42m:03s remains)
INFO - root - 2017-12-11 03:45:01.210421: step 8630, loss = 0.69, batch loss = 0.64 (14.0 examples/sec; 0.570 sec/batch; 51h:16m:49s remains)
INFO - root - 2017-12-11 03:45:06.740304: step 8640, loss = 0.70, batch loss = 0.64 (14.6 examples/sec; 0.549 sec/batch; 49h:24m:33s remains)
INFO - root - 2017-12-11 03:45:12.208405: step 8650, loss = 0.69, batch loss = 0.63 (14.2 examples/sec; 0.564 sec/batch; 50h:46m:06s remains)
INFO - root - 2017-12-11 03:45:17.647192: step 8660, loss = 0.68, batch loss = 0.62 (14.8 examples/sec; 0.542 sec/batch; 48h:45m:11s remains)
INFO - root - 2017-12-11 03:45:23.203562: step 8670, loss = 0.70, batch loss = 0.64 (14.1 examples/sec; 0.567 sec/batch; 50h:58m:20s remains)
INFO - root - 2017-12-11 03:45:28.744036: step 8680, loss = 0.69, batch loss = 0.63 (14.9 examples/sec; 0.537 sec/batch; 48h:20m:23s remains)
INFO - root - 2017-12-11 03:45:34.262012: step 8690, loss = 0.69, batch loss = 0.63 (14.8 examples/sec; 0.540 sec/batch; 48h:35m:33s remains)
INFO - root - 2017-12-11 03:45:39.586855: step 8700, loss = 0.68, batch loss = 0.63 (14.5 examples/sec; 0.553 sec/batch; 49h:46m:45s remains)
2017-12-11 03:45:40.122227: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.043413237 -0.037198931 -0.030400163 -0.025394594 -0.016272618 -0.0018119579 0.0088323317 0.011927066 0.0053440747 -0.0072443285 -0.024223141 -0.042453524 -0.056346755 -0.063187934 -0.063765749][-0.0013600637 0.015137235 0.028981933 0.039556023 0.056902379 0.082720868 0.10333828 0.1125451 0.10680626 0.090973437 0.064391941 0.032008976 0.0063648084 -0.0070103495 -0.010870423][0.06477689 0.099242531 0.12664227 0.14793046 0.17582801 0.21238899 0.24187471 0.25681198 0.25357535 0.23712337 0.20297517 0.15674376 0.11649823 0.091197237 0.077646762][0.13473359 0.19181257 0.23905814 0.27753353 0.31804806 0.36121708 0.39352688 0.40931538 0.40667444 0.39011836 0.35190237 0.29599592 0.24110584 0.19997166 0.17168108][0.18915813 0.27134556 0.34579319 0.41050839 0.46807998 0.51542056 0.54538625 0.55503792 0.54571116 0.52321541 0.48135841 0.41895854 0.34932429 0.28922954 0.24326652][0.21876383 0.32654619 0.43552157 0.53808081 0.62383366 0.68436968 0.71910536 0.72277629 0.70078409 0.66284841 0.61052608 0.53744048 0.44835702 0.3651287 0.29802877][0.23636311 0.37022531 0.51723278 0.66304094 0.78246015 0.862669 0.90491474 0.89771461 0.852372 0.78584415 0.71185404 0.62160975 0.51282126 0.40970051 0.32532468][0.255048 0.41114363 0.58861184 0.76818275 0.91431212 1.0106744 1.0542468 1.0280877 0.95215571 0.85112864 0.7502898 0.64323491 0.52376139 0.41322109 0.32215345][0.24979268 0.41572225 0.60730404 0.80295718 0.96205872 1.0649611 1.101468 1.0534058 0.95077217 0.82365221 0.70498306 0.59261894 0.47844821 0.37625265 0.29051059][0.22682947 0.3890276 0.57498062 0.763076 0.91283453 1.0033804 1.0210638 0.95201319 0.83445865 0.69878566 0.57868594 0.4757618 0.38147962 0.3008551 0.23102786][0.20071396 0.34304336 0.49990952 0.65386719 0.77065527 0.83221996 0.82597035 0.74556535 0.63130116 0.50707608 0.39939052 0.31439698 0.24569227 0.19207579 0.14430724][0.163546 0.27394253 0.38696212 0.49086714 0.56120753 0.58511186 0.5545221 0.47193873 0.374774 0.27718794 0.19386198 0.13287836 0.090674333 0.062810488 0.037516773][0.10412955 0.17974783 0.24948721 0.30614844 0.33454248 0.32601449 0.27892098 0.20311145 0.12980093 0.0643477 0.010572693 -0.024780665 -0.043976068 -0.052039724 -0.059288848][0.026118934 0.070537642 0.10705829 0.13074549 0.13216294 0.10567471 0.055570453 -0.004766474 -0.052437443 -0.087754205 -0.1138026 -0.12702838 -0.12960702 -0.1255053 -0.12155651][-0.035400543 -0.015003369 -0.0011030789 0.0023306543 -0.00992376 -0.040802028 -0.08283893 -0.12380617 -0.14955451 -0.16312842 -0.17070729 -0.17068464 -0.16487128 -0.15546039 -0.14624003]]...]
INFO - root - 2017-12-11 03:45:45.542488: step 8710, loss = 0.69, batch loss = 0.63 (15.1 examples/sec; 0.531 sec/batch; 47h:43m:54s remains)
INFO - root - 2017-12-11 03:45:51.047661: step 8720, loss = 0.69, batch loss = 0.63 (14.4 examples/sec; 0.555 sec/batch; 49h:52m:41s remains)
INFO - root - 2017-12-11 03:45:56.558413: step 8730, loss = 0.69, batch loss = 0.63 (14.7 examples/sec; 0.545 sec/batch; 49h:02m:14s remains)
INFO - root - 2017-12-11 03:46:02.036553: step 8740, loss = 0.69, batch loss = 0.63 (14.5 examples/sec; 0.552 sec/batch; 49h:38m:09s remains)
INFO - root - 2017-12-11 03:46:07.506046: step 8750, loss = 0.70, batch loss = 0.64 (14.5 examples/sec; 0.550 sec/batch; 49h:27m:25s remains)
INFO - root - 2017-12-11 03:46:13.052807: step 8760, loss = 0.69, batch loss = 0.63 (14.7 examples/sec; 0.546 sec/batch; 49h:06m:12s remains)
INFO - root - 2017-12-11 03:46:18.552016: step 8770, loss = 0.70, batch loss = 0.65 (14.5 examples/sec; 0.553 sec/batch; 49h:42m:13s remains)
INFO - root - 2017-12-11 03:46:24.003606: step 8780, loss = 0.70, batch loss = 0.65 (14.6 examples/sec; 0.550 sec/batch; 49h:25m:38s remains)
INFO - root - 2017-12-11 03:46:29.501699: step 8790, loss = 0.68, batch loss = 0.62 (14.6 examples/sec; 0.549 sec/batch; 49h:20m:38s remains)
INFO - root - 2017-12-11 03:46:34.712092: step 8800, loss = 0.70, batch loss = 0.64 (13.7 examples/sec; 0.584 sec/batch; 52h:29m:19s remains)
2017-12-11 03:46:35.278845: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.073667578 -0.081777945 -0.092065968 -0.10213996 -0.10811077 -0.1080023 -0.10338221 -0.096815489 -0.090815328 -0.086918324 -0.086114764 -0.086933382 -0.087349094 -0.08751151 -0.088886343][-0.022397222 -0.032375984 -0.048764598 -0.06691701 -0.079866692 -0.08412116 -0.08158724 -0.0748446 -0.0667358 -0.05989448 -0.057079844 -0.056684628 -0.055904843 -0.0557923 -0.0592638][0.076444857 0.069466673 0.04854181 0.021424187 -0.00066991214 -0.012176136 -0.014667991 -0.0096722068 0.0004046364 0.011319711 0.017491059 0.019645508 0.020736653 0.017804956 0.00713262][0.21472938 0.212903 0.18853013 0.15470362 0.12681843 0.11258727 0.11065581 0.11980436 0.13728577 0.15629283 0.16864431 0.17338386 0.1719456 0.15873621 0.13050742][0.36189139 0.3644236 0.33863962 0.3053782 0.28165644 0.27558979 0.28465584 0.30559909 0.3334758 0.35855189 0.37346566 0.37821829 0.37245688 0.34711689 0.29808715][0.48094875 0.48116592 0.45444033 0.43048355 0.42353976 0.43866682 0.46986866 0.50867254 0.5459038 0.56968075 0.57834452 0.5773108 0.56571788 0.53160572 0.467416][0.54260635 0.53278023 0.50532472 0.49714136 0.51500595 0.55671757 0.61095989 0.6636939 0.70254856 0.715937 0.7108205 0.70018232 0.68409842 0.64841324 0.5807184][0.54628313 0.51992416 0.48756063 0.49199831 0.53074867 0.59250718 0.65950811 0.71506864 0.74749124 0.74862832 0.73264772 0.71764511 0.70406765 0.67592859 0.61685807][0.50927031 0.46464807 0.42216268 0.42782879 0.4736335 0.54078871 0.60630119 0.65372866 0.674525 0.6663416 0.64760321 0.6377337 0.63521856 0.62228471 0.58084911][0.45748588 0.40238386 0.34960267 0.34523672 0.38136819 0.43716604 0.48714766 0.51659423 0.52101284 0.50387216 0.4847908 0.48219582 0.492974 0.49840644 0.47966751][0.43207571 0.37945998 0.32316571 0.30478725 0.31947786 0.34914944 0.37067661 0.37341762 0.35718164 0.32974803 0.3087858 0.31140822 0.33421928 0.3580288 0.36278307][0.447782 0.4102239 0.36077815 0.33291417 0.32408255 0.32103062 0.30727962 0.27923885 0.24167588 0.20452484 0.18063948 0.18496467 0.2153587 0.25310037 0.27635473][0.49942908 0.48239169 0.44441426 0.41174921 0.38257918 0.34839222 0.30127442 0.24595805 0.19201726 0.14933445 0.12430179 0.12834819 0.16086525 0.20459348 0.23672603][0.54426563 0.54581827 0.519724 0.48765048 0.44753429 0.39358687 0.32437545 0.25109857 0.18703954 0.14203389 0.11759566 0.12029824 0.150009 0.19147031 0.22292736][0.52539539 0.53782243 0.52064723 0.49311635 0.45278957 0.39510092 0.3207272 0.24296047 0.17662919 0.13219242 0.10950566 0.11107767 0.1360236 0.17073841 0.19611213]]...]
INFO - root - 2017-12-11 03:46:40.966829: step 8810, loss = 0.71, batch loss = 0.65 (13.7 examples/sec; 0.584 sec/batch; 52h:33m:07s remains)
INFO - root - 2017-12-11 03:46:46.514146: step 8820, loss = 0.70, batch loss = 0.64 (14.2 examples/sec; 0.564 sec/batch; 50h:42m:12s remains)
INFO - root - 2017-12-11 03:46:52.140142: step 8830, loss = 0.69, batch loss = 0.63 (14.4 examples/sec; 0.556 sec/batch; 49h:57m:12s remains)
INFO - root - 2017-12-11 03:46:57.729456: step 8840, loss = 0.70, batch loss = 0.64 (14.1 examples/sec; 0.569 sec/batch; 51h:06m:57s remains)
INFO - root - 2017-12-11 03:47:03.287948: step 8850, loss = 0.70, batch loss = 0.64 (14.3 examples/sec; 0.558 sec/batch; 50h:09m:47s remains)
INFO - root - 2017-12-11 03:47:08.775883: step 8860, loss = 0.69, batch loss = 0.63 (14.9 examples/sec; 0.538 sec/batch; 48h:24m:12s remains)
INFO - root - 2017-12-11 03:47:14.224920: step 8870, loss = 0.68, batch loss = 0.63 (15.1 examples/sec; 0.530 sec/batch; 47h:38m:12s remains)
INFO - root - 2017-12-11 03:47:19.842041: step 8880, loss = 0.70, batch loss = 0.65 (14.4 examples/sec; 0.556 sec/batch; 49h:57m:33s remains)
INFO - root - 2017-12-11 03:47:25.332784: step 8890, loss = 0.70, batch loss = 0.64 (15.7 examples/sec; 0.509 sec/batch; 45h:44m:58s remains)
INFO - root - 2017-12-11 03:47:30.599081: step 8900, loss = 0.70, batch loss = 0.64 (14.5 examples/sec; 0.553 sec/batch; 49h:43m:26s remains)
2017-12-11 03:47:31.181065: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.038980305 -0.026526125 -0.0098199947 0.0078695528 0.020566681 0.019196976 0.00071618467 -0.027414385 -0.055125125 -0.078009821 -0.093043767 -0.096862458 -0.090621665 -0.072824694 -0.05202214][-0.035019435 -0.0070367567 0.028870123 0.06592235 0.095295109 0.10284703 0.0823848 0.041611105 -0.0053180335 -0.049383696 -0.084231131 -0.10275335 -0.10334482 -0.085642338 -0.061968889][-0.020109987 0.027720815 0.090959832 0.15849109 0.21637052 0.24306831 0.22743909 0.17458928 0.10254952 0.027403312 -0.038417 -0.082018413 -0.09741471 -0.084168725 -0.060102511][-0.0017146913 0.066374227 0.16022371 0.2649821 0.35997656 0.41569644 0.4146238 0.35646063 0.26036781 0.14885411 0.042659461 -0.036870286 -0.076912478 -0.075391054 -0.054739542][0.014268441 0.097781494 0.21664366 0.35407478 0.48433328 0.5730198 0.59654981 0.54682589 0.4399842 0.3021341 0.16029543 0.042096782 -0.031480867 -0.052543595 -0.043759905][0.024800692 0.11635997 0.2497551 0.40964088 0.56834084 0.68977922 0.74465996 0.71562076 0.61244315 0.46193704 0.29459897 0.14121224 0.030664438 -0.020455202 -0.030977236][0.02613489 0.11780855 0.25435993 0.42504019 0.60349578 0.75338918 0.84004921 0.83748674 0.74729347 0.594226 0.41047096 0.22915187 0.086585589 0.0076727145 -0.021476045][0.019390993 0.098118082 0.22159408 0.38524809 0.56712037 0.73159611 0.8414942 0.86607754 0.79901975 0.65830851 0.47444162 0.28184578 0.12113458 0.024085207 -0.01777382][0.016570665 0.065975145 0.15686619 0.29176617 0.45564398 0.61562222 0.73480242 0.78233856 0.74701649 0.63683975 0.47560734 0.2946094 0.13505036 0.032576632 -0.015481278][0.025768068 0.033145074 0.077724338 0.16858064 0.29721007 0.43520874 0.54798877 0.60801995 0.60245121 0.53003973 0.405627 0.2555038 0.11735015 0.025588915 -0.019234765][0.050589718 0.012515633 0.0082023395 0.050862119 0.1354387 0.23862165 0.33030519 0.38820726 0.40046251 0.35973963 0.27354735 0.1634751 0.060651597 -0.0061494145 -0.036416564][0.08684957 0.010106705 -0.034840517 -0.033764038 0.0078895073 0.070754215 0.13078101 0.17272654 0.18841654 0.16882896 0.11681735 0.048632383 -0.013596207 -0.04905137 -0.059061572][0.11974728 0.019190023 -0.05238831 -0.080357239 -0.071929723 -0.04364134 -0.014358882 0.006841118 0.016846944 0.0091946246 -0.015716277 -0.047819238 -0.074689336 -0.082847871 -0.075563915][0.12609713 0.020735789 -0.05858944 -0.099147357 -0.10922976 -0.10240585 -0.093728639 -0.088674292 -0.086402662 -0.089202791 -0.097422779 -0.10623983 -0.11030886 -0.10101318 -0.083127953][0.089025177 -0.0017225476 -0.068120517 -0.10257016 -0.11441187 -0.11500315 -0.11629883 -0.12121516 -0.12622969 -0.12968756 -0.13121985 -0.1292606 -0.12286963 -0.10564774 -0.083059005]]...]
INFO - root - 2017-12-11 03:47:36.729839: step 8910, loss = 0.71, batch loss = 0.65 (14.0 examples/sec; 0.572 sec/batch; 51h:27m:08s remains)
INFO - root - 2017-12-11 03:47:42.323752: step 8920, loss = 0.69, batch loss = 0.63 (14.9 examples/sec; 0.538 sec/batch; 48h:19m:53s remains)
INFO - root - 2017-12-11 03:47:47.758964: step 8930, loss = 0.70, batch loss = 0.64 (14.5 examples/sec; 0.551 sec/batch; 49h:31m:36s remains)
INFO - root - 2017-12-11 03:47:53.307908: step 8940, loss = 0.71, batch loss = 0.65 (14.4 examples/sec; 0.554 sec/batch; 49h:48m:20s remains)
INFO - root - 2017-12-11 03:47:58.841317: step 8950, loss = 0.69, batch loss = 0.63 (15.0 examples/sec; 0.535 sec/batch; 48h:03m:31s remains)
INFO - root - 2017-12-11 03:48:04.331958: step 8960, loss = 0.70, batch loss = 0.64 (14.8 examples/sec; 0.540 sec/batch; 48h:30m:25s remains)
INFO - root - 2017-12-11 03:48:09.894064: step 8970, loss = 0.70, batch loss = 0.65 (13.8 examples/sec; 0.579 sec/batch; 52h:00m:52s remains)
INFO - root - 2017-12-11 03:48:15.428907: step 8980, loss = 0.68, batch loss = 0.63 (13.5 examples/sec; 0.592 sec/batch; 53h:14m:03s remains)
INFO - root - 2017-12-11 03:48:20.583888: step 8990, loss = 0.69, batch loss = 0.63 (14.4 examples/sec; 0.554 sec/batch; 49h:46m:51s remains)
INFO - root - 2017-12-11 03:48:26.077056: step 9000, loss = 0.71, batch loss = 0.65 (14.7 examples/sec; 0.546 sec/batch; 49h:01m:45s remains)
2017-12-11 03:48:26.673214: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.039239418 0.0453414 0.052648511 0.058481336 0.064750917 0.070473112 0.070745453 0.063552268 0.052416354 0.045585115 0.052052781 0.0753698 0.11297727 0.15625244 0.19256355][0.027645066 0.032124467 0.039621007 0.047212239 0.0555406 0.062197506 0.061792541 0.05236344 0.037122659 0.025614705 0.028273731 0.050733354 0.092526808 0.14396009 0.18989527][0.031412065 0.033750728 0.043251675 0.055867549 0.068965383 0.077581838 0.075427517 0.061304096 0.038672235 0.018646928 0.013210719 0.030448107 0.07198289 0.12777613 0.18092287][0.054082565 0.058188565 0.075150929 0.098533161 0.12106983 0.13474827 0.13193059 0.11213861 0.0795433 0.047156256 0.028742468 0.035046831 0.069974624 0.12417056 0.18013459][0.095322922 0.10574976 0.13353626 0.16986813 0.20364346 0.22477864 0.22371274 0.19943932 0.15674646 0.11068946 0.076509967 0.066322558 0.086990438 0.13117573 0.18245594][0.14595307 0.16549635 0.20445885 0.25222641 0.2962302 0.32639173 0.33076707 0.30564818 0.25558221 0.19736625 0.14721234 0.11682864 0.11611675 0.14173949 0.17998448][0.1938431 0.22153956 0.26635957 0.3178429 0.36537567 0.40181658 0.41366759 0.39277425 0.34209371 0.27867922 0.21854688 0.17186643 0.15104578 0.15751845 0.18083088][0.23189944 0.26211423 0.30393288 0.34879339 0.39027432 0.42560554 0.44119224 0.42543855 0.37955245 0.31925195 0.25823539 0.2044325 0.17151639 0.16464628 0.17648554][0.2448909 0.27237377 0.30510402 0.33749372 0.36742741 0.39600459 0.41101837 0.39977083 0.3631967 0.31451204 0.26288447 0.21306767 0.17824097 0.16562614 0.17021298][0.22630653 0.2473129 0.2685774 0.28727323 0.30463111 0.32429078 0.33627632 0.32922018 0.3049601 0.27373084 0.23916346 0.20255104 0.17545685 0.16552983 0.16877647][0.18404827 0.19833155 0.21041711 0.2193711 0.22783946 0.23991907 0.24815767 0.24400952 0.23068891 0.21589325 0.19868873 0.17767148 0.16208872 0.15899961 0.16473599][0.13758722 0.14640298 0.15297218 0.15776448 0.16283101 0.17070338 0.17578931 0.17263052 0.16554996 0.160589 0.15469243 0.14565125 0.14020915 0.14385639 0.15327746][0.10609643 0.10952316 0.11149301 0.11411314 0.1182137 0.12396277 0.1269846 0.12410479 0.11966234 0.11852542 0.11771598 0.11549971 0.11607923 0.12283597 0.13325898][0.096210942 0.094468258 0.091725089 0.092414133 0.096292213 0.10106658 0.10315232 0.10078978 0.097326428 0.09629301 0.095655508 0.094728641 0.09589906 0.10103408 0.10883005][0.09939269 0.092089333 0.0844112 0.083019733 0.086955532 0.09189675 0.0945442 0.093938589 0.091710031 0.089708455 0.0868463 0.083473563 0.08100263 0.080749311 0.083070561]]...]
INFO - root - 2017-12-11 03:48:32.207751: step 9010, loss = 0.70, batch loss = 0.65 (14.8 examples/sec; 0.540 sec/batch; 48h:30m:55s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-SGD-lr0.001-clip50-initfromfixqian30k-fixqian
INFO - root - 2017-12-11 03:48:37.667196: step 9020, loss = 0.70, batch loss = 0.64 (14.3 examples/sec; 0.560 sec/batch; 50h:18m:01s remains)
INFO - root - 2017-12-11 03:48:43.160146: step 9030, loss = 0.69, batch loss = 0.64 (14.5 examples/sec; 0.552 sec/batch; 49h:33m:33s remains)
INFO - root - 2017-12-11 03:48:48.733141: step 9040, loss = 0.70, batch loss = 0.64 (15.0 examples/sec; 0.533 sec/batch; 47h:51m:56s remains)
INFO - root - 2017-12-11 03:48:54.208245: step 9050, loss = 0.68, batch loss = 0.63 (14.9 examples/sec; 0.536 sec/batch; 48h:11m:48s remains)
INFO - root - 2017-12-11 03:48:59.715656: step 9060, loss = 0.72, batch loss = 0.66 (14.3 examples/sec; 0.561 sec/batch; 50h:25m:25s remains)
INFO - root - 2017-12-11 03:49:05.291306: step 9070, loss = 0.69, batch loss = 0.63 (14.6 examples/sec; 0.546 sec/batch; 49h:04m:04s remains)
INFO - root - 2017-12-11 03:49:10.814297: step 9080, loss = 0.68, batch loss = 0.63 (13.5 examples/sec; 0.592 sec/batch; 53h:09m:36s remains)
INFO - root - 2017-12-11 03:49:16.117833: step 9090, loss = 0.69, batch loss = 0.63 (14.2 examples/sec; 0.564 sec/batch; 50h:42m:34s remains)
INFO - root - 2017-12-11 03:49:21.738579: step 9100, loss = 0.70, batch loss = 0.64 (14.6 examples/sec; 0.548 sec/batch; 49h:14m:23s remains)
2017-12-11 03:49:22.337984: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.017577194 -0.024264922 -0.028528342 -0.031343147 -0.032470293 -0.032056056 -0.030859202 -0.029623868 -0.028165856 -0.026267484 -0.023417898 -0.018178569 -0.0060956003 0.010070028 0.022333683][0.029791031 0.018574338 0.0095920162 0.0032030803 -0.00043287376 -0.0016697579 -0.0015532265 -0.0013617001 -0.00038260844 0.0024101511 0.0078741442 0.019003864 0.043643113 0.076271668 0.10340922][0.089685291 0.077287778 0.066494428 0.058378141 0.05263859 0.048758738 0.045822948 0.042137295 0.038921542 0.038522065 0.042859942 0.056952 0.092086233 0.14041056 0.18300724][0.1411207 0.13940199 0.1385712 0.1373803 0.13388424 0.12824541 0.12131508 0.11127727 0.0989835 0.087670878 0.0805929 0.085402273 0.11693739 0.16851977 0.21923511][0.16872717 0.19591716 0.22471425 0.24627061 0.25473213 0.25217572 0.24342573 0.22707403 0.20231178 0.1716962 0.1394936 0.11571589 0.11925165 0.1505098 0.19224958][0.17140244 0.24266292 0.31859574 0.37796253 0.40861052 0.41519886 0.40829542 0.38794172 0.35096616 0.29763028 0.23162809 0.16375886 0.11704508 0.10403401 0.11682224][0.1574156 0.27234328 0.39689878 0.49663454 0.55253935 0.571641 0.57023072 0.55058295 0.50736129 0.43743885 0.34165612 0.22901854 0.12592596 0.058344614 0.031080583][0.13134678 0.26814264 0.41943 0.54277629 0.61501139 0.6441946 0.64966255 0.6356231 0.59574527 0.52396965 0.41690812 0.27916485 0.13779321 0.0285609 -0.032644808][0.096830428 0.22270471 0.36479804 0.48255664 0.55341816 0.58475971 0.59481329 0.58837134 0.55942923 0.50113869 0.40671465 0.27552095 0.13138278 0.01194957 -0.061732456][0.061984591 0.15214494 0.25545248 0.34149274 0.39289057 0.41566306 0.42418253 0.42251131 0.40567434 0.36797494 0.30192694 0.2033184 0.09019579 -0.0060443273 -0.066669352][0.033018887 0.082491152 0.13845845 0.18271382 0.20523123 0.21146165 0.21184476 0.20939258 0.200129 0.18015492 0.1437981 0.0864993 0.020265637 -0.03421678 -0.0656892][0.014789137 0.03630444 0.057606824 0.06917125 0.067280844 0.057689812 0.048085436 0.04070022 0.032299966 0.020661511 0.0034537709 -0.021294374 -0.045980584 -0.060476631 -0.062284693][0.0065641818 0.018015064 0.025341615 0.02268707 0.0095811943 -0.0081503913 -0.024559578 -0.037584573 -0.049541116 -0.061561774 -0.072882511 -0.081729144 -0.082574755 -0.0723846 -0.05529917][-0.0035133765 0.0084788268 0.016181864 0.013932318 0.0018010808 -0.015225326 -0.032742124 -0.04850189 -0.063775763 -0.079085194 -0.092032857 -0.0982029 -0.092524119 -0.074872747 -0.052155711][-0.030487202 -0.018158449 -0.0075166649 -0.0049642567 -0.011021852 -0.022282237 -0.036082741 -0.050356992 -0.06537877 -0.081454612 -0.095910035 -0.10374012 -0.099909015 -0.084560536 -0.0642557]]...]
INFO - root - 2017-12-11 03:49:27.842976: step 9110, loss = 0.69, batch loss = 0.63 (14.4 examples/sec; 0.555 sec/batch; 49h:53m:50s remains)
INFO - root - 2017-12-11 03:49:33.340579: step 9120, loss = 0.70, batch loss = 0.64 (14.7 examples/sec; 0.545 sec/batch; 48h:57m:45s remains)
INFO - root - 2017-12-11 03:49:38.888792: step 9130, loss = 0.69, batch loss = 0.63 (14.7 examples/sec; 0.545 sec/batch; 48h:55m:17s remains)
INFO - root - 2017-12-11 03:49:44.385981: step 9140, loss = 0.70, batch loss = 0.64 (14.5 examples/sec; 0.551 sec/batch; 49h:30m:59s remains)
INFO - root - 2017-12-11 03:49:49.904011: step 9150, loss = 0.69, batch loss = 0.63 (14.7 examples/sec; 0.543 sec/batch; 48h:46m:52s remains)
INFO - root - 2017-12-11 03:49:55.468837: step 9160, loss = 0.68, batch loss = 0.63 (14.5 examples/sec; 0.551 sec/batch; 49h:28m:15s remains)
INFO - root - 2017-12-11 03:50:00.912305: step 9170, loss = 0.71, batch loss = 0.65 (15.1 examples/sec; 0.529 sec/batch; 47h:32m:38s remains)
INFO - root - 2017-12-11 03:50:06.356572: step 9180, loss = 0.71, batch loss = 0.65 (16.2 examples/sec; 0.494 sec/batch; 44h:19m:43s remains)
INFO - root - 2017-12-11 03:50:11.551098: step 9190, loss = 0.70, batch loss = 0.64 (15.0 examples/sec; 0.535 sec/batch; 48h:03m:03s remains)
INFO - root - 2017-12-11 03:50:16.998721: step 9200, loss = 0.69, batch loss = 0.63 (14.5 examples/sec; 0.551 sec/batch; 49h:27m:39s remains)
2017-12-11 03:50:17.586967: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.029581156 0.0033197519 -0.010494683 -0.0091236122 2.3732186e-05 0.034204096 0.084229663 0.12851571 0.14546768 0.13272327 0.10040865 0.045864996 -0.018599832 -0.073494516 -0.10681661][0.022255514 0.00032313156 -0.017426299 -0.026415456 -0.032129876 -0.019115448 0.007663691 0.037434924 0.056840632 0.054877624 0.035679996 -0.004028413 -0.050968457 -0.090322085 -0.11569344][0.085813351 0.0778182 0.061230693 0.042419605 0.019043911 0.0026411305 -0.0056681577 -0.0028078987 0.00856667 0.01094278 -0.0012723623 -0.031437159 -0.066490032 -0.094326109 -0.11421429][0.20092462 0.21958879 0.21389875 0.19247204 0.15511502 0.10970153 0.063277647 0.03133617 0.021857223 0.015378018 -6.3968662e-05 -0.029807102 -0.062552623 -0.087695569 -0.10639846][0.32864672 0.38055351 0.39479971 0.38378266 0.34699354 0.28835803 0.21909581 0.16016233 0.1257062 0.098238572 0.066050395 0.022903139 -0.02276323 -0.059489891 -0.086799689][0.41912216 0.49922347 0.53811038 0.55208325 0.53709769 0.4949562 0.43678939 0.37645847 0.32543033 0.27111635 0.21089503 0.13898832 0.062029269 -0.0038453143 -0.05190054][0.44037 0.54090357 0.60870516 0.65936881 0.68300855 0.68349737 0.66752082 0.62971658 0.571685 0.48855671 0.39419997 0.28403205 0.16444197 0.059182987 -0.015439293][0.39884639 0.51138943 0.6062578 0.69264585 0.755504 0.80616063 0.84382063 0.83860272 0.77968043 0.67169315 0.54716033 0.40210554 0.24351895 0.10320026 0.0071040345][0.30970821 0.4223845 0.53239709 0.63934094 0.72541404 0.81216896 0.89233196 0.91496569 0.85896122 0.73796678 0.60021889 0.44115865 0.26638302 0.11153957 0.0085625313][0.19183855 0.29022521 0.39751863 0.50293982 0.5891363 0.68535244 0.781347 0.81598723 0.76394576 0.64605093 0.51780349 0.37314686 0.21377142 0.073386952 -0.016403824][0.068724446 0.14065315 0.22713391 0.31084651 0.377168 0.45741212 0.5424363 0.57403308 0.52810794 0.42824006 0.3273868 0.21818821 0.098320045 -0.0049773869 -0.065315261][-0.033095434 0.0085249767 0.064310677 0.11615653 0.15355331 0.20372306 0.2609202 0.27957779 0.24085149 0.1664456 0.0996267 0.033557486 -0.03675966 -0.093297355 -0.11841842][-0.095209964 -0.0794497 -0.052944504 -0.030281885 -0.017845893 0.00346616 0.031190034 0.0358882 0.0055275578 -0.043120857 -0.079511449 -0.10846347 -0.13578638 -0.15253544 -0.15014653][-0.12034133 -0.12115341 -0.11450828 -0.11047454 -0.11259741 -0.11009835 -0.10266156 -0.10558339 -0.12552455 -0.15235059 -0.16776811 -0.17396134 -0.17552368 -0.16973825 -0.15448768][-0.12140613 -0.12925327 -0.13182469 -0.1358241 -0.14294173 -0.14838144 -0.15063024 -0.15559846 -0.16653296 -0.17876381 -0.18323176 -0.18001969 -0.17170487 -0.15867405 -0.1420548]]...]
INFO - root - 2017-12-11 03:50:23.136995: step 9210, loss = 0.69, batch loss = 0.63 (14.7 examples/sec; 0.543 sec/batch; 48h:45m:04s remains)
INFO - root - 2017-12-11 03:50:28.599697: step 9220, loss = 0.70, batch loss = 0.64 (14.5 examples/sec; 0.551 sec/batch; 49h:27m:28s remains)
INFO - root - 2017-12-11 03:50:34.102362: step 9230, loss = 0.70, batch loss = 0.65 (14.1 examples/sec; 0.568 sec/batch; 50h:59m:44s remains)
INFO - root - 2017-12-11 03:50:39.626696: step 9240, loss = 0.72, batch loss = 0.66 (14.3 examples/sec; 0.559 sec/batch; 50h:10m:14s remains)
INFO - root - 2017-12-11 03:50:45.221874: step 9250, loss = 0.70, batch loss = 0.64 (14.8 examples/sec; 0.541 sec/batch; 48h:36m:27s remains)
INFO - root - 2017-12-11 03:50:50.739300: step 9260, loss = 0.69, batch loss = 0.63 (14.3 examples/sec; 0.559 sec/batch; 50h:12m:41s remains)
INFO - root - 2017-12-11 03:50:56.321212: step 9270, loss = 0.68, batch loss = 0.62 (14.1 examples/sec; 0.566 sec/batch; 50h:47m:20s remains)
INFO - root - 2017-12-11 03:51:01.544106: step 9280, loss = 0.72, batch loss = 0.66 (14.1 examples/sec; 0.566 sec/batch; 50h:46m:39s remains)
INFO - root - 2017-12-11 03:51:07.151619: step 9290, loss = 0.71, batch loss = 0.65 (13.9 examples/sec; 0.576 sec/batch; 51h:44m:45s remains)
INFO - root - 2017-12-11 03:51:12.668955: step 9300, loss = 0.70, batch loss = 0.64 (14.8 examples/sec; 0.542 sec/batch; 48h:38m:47s remains)
2017-12-11 03:51:13.257390: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.18622328 0.19415852 0.19111808 0.1791189 0.16727018 0.16995259 0.18632747 0.20428766 0.21427217 0.2113412 0.19655681 0.16516224 0.1181147 0.065935634 0.02152543][0.2189554 0.22944529 0.22686076 0.21332689 0.19899723 0.19886969 0.21220472 0.22663465 0.23215935 0.22457954 0.20435557 0.16711569 0.11483137 0.059492629 0.014316041][0.23261446 0.24317029 0.23984392 0.22564097 0.21060547 0.2083084 0.21853364 0.22913888 0.23093931 0.2193125 0.19464955 0.15439115 0.1015934 0.048025087 0.0057709278][0.23652913 0.2448971 0.2398012 0.22617435 0.21299507 0.21062998 0.22010912 0.22946391 0.22976957 0.21591815 0.18838356 0.14635316 0.093707807 0.041590288 0.0015179673][0.23533198 0.23959562 0.23116371 0.21827106 0.2081438 0.2072126 0.218496 0.22998579 0.23198381 0.21846546 0.18972172 0.14684793 0.094251223 0.042412825 0.0028225787][0.24035332 0.23838532 0.22552437 0.21252437 0.20483522 0.20523518 0.21919538 0.23443756 0.23931266 0.22687438 0.19739893 0.15364988 0.1004993 0.047642443 0.0071292804][0.2613489 0.25342381 0.23485713 0.21864381 0.21010783 0.21010676 0.22600462 0.24436136 0.25069559 0.23787312 0.20682612 0.16163389 0.10748975 0.053273644 0.011600037][0.29317158 0.27978554 0.25370967 0.23046152 0.21692079 0.21399476 0.22904518 0.24814393 0.25440207 0.24106932 0.20904773 0.16313657 0.1092024 0.055292439 0.013768288][0.3124679 0.29558572 0.26407441 0.2344954 0.21496703 0.20712332 0.21775542 0.23469311 0.239702 0.22663201 0.19580419 0.15201133 0.10104559 0.050060976 0.010823418][0.29786098 0.27975467 0.24690129 0.21510589 0.19233863 0.18100348 0.18683927 0.20085247 0.2053657 0.19419651 0.16735493 0.12938717 0.084764019 0.039182536 0.0038631975][0.25285062 0.23596525 0.20672862 0.17805444 0.15654483 0.14507899 0.14847291 0.16093482 0.16659409 0.15888968 0.13773221 0.10680668 0.069011226 0.028633665 -0.0033325274][0.19607031 0.18421924 0.16242798 0.14086466 0.12389696 0.11489028 0.11858845 0.13148428 0.13974258 0.13582459 0.11980695 0.094597191 0.061440405 0.023879891 -0.0068424684][0.1457509 0.14006636 0.12685779 0.11407268 0.10363169 0.098862767 0.10549836 0.12083694 0.13243347 0.13158597 0.11923213 0.09752138 0.065912127 0.027921546 -0.0043326342][0.11150903 0.11305731 0.10844419 0.10448469 0.10134277 0.1022004 0.11423668 0.13393724 0.14914057 0.15000342 0.13903742 0.11760371 0.083664358 0.041342698 0.0045245592][0.094962858 0.10182296 0.10294797 0.10475402 0.1067455 0.11259843 0.13055988 0.15530258 0.17420918 0.17659606 0.16657245 0.14436568 0.10697318 0.059006635 0.01669988]]...]
INFO - root - 2017-12-11 03:51:18.844976: step 9310, loss = 0.71, batch loss = 0.65 (14.5 examples/sec; 0.550 sec/batch; 49h:25m:02s remains)
INFO - root - 2017-12-11 03:51:24.310845: step 9320, loss = 0.68, batch loss = 0.62 (14.7 examples/sec; 0.545 sec/batch; 48h:54m:29s remains)
INFO - root - 2017-12-11 03:51:29.726610: step 9330, loss = 0.70, batch loss = 0.64 (14.7 examples/sec; 0.543 sec/batch; 48h:47m:07s remains)
INFO - root - 2017-12-11 03:51:35.207116: step 9340, loss = 0.71, batch loss = 0.65 (14.5 examples/sec; 0.551 sec/batch; 49h:28m:51s remains)
INFO - root - 2017-12-11 03:51:40.810327: step 9350, loss = 0.69, batch loss = 0.63 (14.4 examples/sec; 0.557 sec/batch; 50h:01m:21s remains)
INFO - root - 2017-12-11 03:51:46.332090: step 9360, loss = 0.69, batch loss = 0.63 (14.1 examples/sec; 0.567 sec/batch; 50h:56m:19s remains)
INFO - root - 2017-12-11 03:51:51.885811: step 9370, loss = 0.70, batch loss = 0.64 (14.2 examples/sec; 0.563 sec/batch; 50h:30m:30s remains)
INFO - root - 2017-12-11 03:51:57.015333: step 9380, loss = 0.70, batch loss = 0.64 (14.6 examples/sec; 0.549 sec/batch; 49h:15m:55s remains)
INFO - root - 2017-12-11 03:52:02.562952: step 9390, loss = 0.70, batch loss = 0.64 (14.6 examples/sec; 0.548 sec/batch; 49h:09m:46s remains)
INFO - root - 2017-12-11 03:52:08.054968: step 9400, loss = 0.72, batch loss = 0.66 (14.8 examples/sec; 0.542 sec/batch; 48h:38m:59s remains)
2017-12-11 03:52:08.644035: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.67239362 0.66647315 0.63571334 0.59753966 0.56120378 0.5320614 0.5046376 0.49567777 0.52062112 0.56494242 0.59817547 0.58126122 0.508957 0.3826836 0.21843217][0.61819166 0.60483789 0.57389367 0.54048544 0.51343328 0.49243417 0.47263831 0.46943656 0.49228141 0.53084856 0.557999 0.54019451 0.47177225 0.35241404 0.19856164][0.51228505 0.49096689 0.46273172 0.43927389 0.4257617 0.41667786 0.40663874 0.4087123 0.42706081 0.45727938 0.48011538 0.46754718 0.41142708 0.30754486 0.17231046][0.42486152 0.40084463 0.38190484 0.37450558 0.37716326 0.3797974 0.37653702 0.37692893 0.38463107 0.403394 0.42230397 0.41637307 0.37241596 0.282139 0.16040227][0.37763691 0.36348343 0.3672415 0.38624716 0.4094103 0.4225485 0.42097577 0.41052321 0.40012953 0.4029966 0.41497549 0.41044095 0.37018406 0.28438565 0.16651411][0.39503565 0.40557331 0.44283485 0.49346104 0.53617287 0.55450451 0.54766166 0.51913065 0.48664778 0.47218281 0.47444484 0.46239677 0.41168386 0.31618944 0.19028889][0.48214552 0.52490574 0.59685683 0.67595756 0.73280281 0.75114346 0.73482758 0.68662709 0.63348669 0.60426414 0.59588665 0.56817943 0.49352133 0.37448823 0.22977853][0.5968076 0.6656459 0.76519996 0.86389446 0.926678 0.94065529 0.913233 0.84667766 0.774724 0.73181313 0.71118742 0.66395766 0.56202859 0.41782162 0.25504115][0.66420048 0.74176347 0.85196781 0.95720005 1.0187742 1.0287063 0.99601406 0.92042178 0.83780086 0.78369766 0.74890184 0.6814152 0.5571925 0.39693367 0.22639547][0.61851358 0.68018448 0.77799773 0.87277997 0.92658371 0.93609643 0.90936619 0.8413766 0.76248825 0.70593983 0.66189009 0.58331424 0.45332554 0.29763886 0.14052959][0.46102551 0.4879244 0.55272079 0.6215741 0.66179764 0.6734888 0.66049224 0.61275369 0.54944772 0.49890906 0.45356989 0.37738124 0.26185054 0.13380575 0.013024079][0.24531822 0.23310034 0.25787479 0.29494691 0.32010716 0.33504739 0.33765727 0.31474233 0.27400571 0.23720674 0.20083158 0.14133774 0.056674361 -0.030354807 -0.10586188][0.032666594 -0.0049499017 -0.0094991764 0.0025937492 0.01587224 0.031692643 0.044455029 0.041785106 0.025138039 0.0090902029 -0.006644831 -0.036525752 -0.0811773 -0.1265779 -0.16484198][-0.11778118 -0.15694466 -0.16969192 -0.16699506 -0.15818238 -0.14356199 -0.12822889 -0.11959531 -0.11646917 -0.11112443 -0.1032947 -0.10299199 -0.11311617 -0.12922306 -0.14857557][-0.18514802 -0.20892173 -0.21318218 -0.20736428 -0.19764899 -0.18557981 -0.173526 -0.16220814 -0.14676261 -0.12397134 -0.095048413 -0.070375204 -0.05692051 -0.057899542 -0.074828647]]...]
INFO - root - 2017-12-11 03:52:14.059223: step 9410, loss = 0.70, batch loss = 0.64 (16.1 examples/sec; 0.498 sec/batch; 44h:41m:33s remains)
INFO - root - 2017-12-11 03:52:19.467720: step 9420, loss = 0.69, batch loss = 0.63 (15.2 examples/sec; 0.526 sec/batch; 47h:10m:24s remains)
INFO - root - 2017-12-11 03:52:24.952938: step 9430, loss = 0.70, batch loss = 0.64 (14.6 examples/sec; 0.546 sec/batch; 49h:01m:53s remains)
INFO - root - 2017-12-11 03:52:30.588405: step 9440, loss = 0.70, batch loss = 0.64 (14.9 examples/sec; 0.538 sec/batch; 48h:17m:33s remains)
INFO - root - 2017-12-11 03:52:36.111663: step 9450, loss = 0.69, batch loss = 0.64 (14.4 examples/sec; 0.555 sec/batch; 49h:48m:56s remains)
INFO - root - 2017-12-11 03:52:41.602303: step 9460, loss = 0.70, batch loss = 0.64 (14.0 examples/sec; 0.571 sec/batch; 51h:12m:00s remains)
INFO - root - 2017-12-11 03:52:47.175534: step 9470, loss = 0.70, batch loss = 0.64 (14.9 examples/sec; 0.537 sec/batch; 48h:09m:18s remains)
INFO - root - 2017-12-11 03:52:52.487972: step 9480, loss = 0.72, batch loss = 0.66 (14.0 examples/sec; 0.571 sec/batch; 51h:11m:24s remains)
INFO - root - 2017-12-11 03:52:57.986939: step 9490, loss = 0.70, batch loss = 0.64 (14.8 examples/sec; 0.539 sec/batch; 48h:21m:24s remains)
INFO - root - 2017-12-11 03:53:03.503345: step 9500, loss = 0.70, batch loss = 0.64 (14.1 examples/sec; 0.568 sec/batch; 50h:57m:41s remains)
2017-12-11 03:53:04.114359: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.23921159 0.22203611 0.22310008 0.27213719 0.36151761 0.46473476 0.55447632 0.59806758 0.57498187 0.48846042 0.37279871 0.2654095 0.18417183 0.14045964 0.13248913][0.24286385 0.22703086 0.22650884 0.26573604 0.34422389 0.44978353 0.55670196 0.62463969 0.62196624 0.54198349 0.41773993 0.28760555 0.17463697 0.098650351 0.070742637][0.24620822 0.23590553 0.23597933 0.26345247 0.32409978 0.41998532 0.5284301 0.60489482 0.61342454 0.54248029 0.41995224 0.28120282 0.15079768 0.054298457 0.013364815][0.27121395 0.27676079 0.28986287 0.3204549 0.37457 0.46020705 0.55720741 0.62133193 0.62271428 0.55296206 0.4350825 0.29698884 0.16024241 0.05226507 0.0016539383][0.32226789 0.35381898 0.39335787 0.44442588 0.50849873 0.59183812 0.67395937 0.71496123 0.698328 0.62294865 0.50793427 0.37070754 0.22872676 0.10994522 0.045950633][0.37137437 0.42900783 0.49366447 0.5643332 0.64040154 0.72511309 0.79432279 0.81452239 0.78200489 0.70108265 0.58607721 0.44580239 0.29816234 0.17230919 0.097681843][0.38200516 0.45641068 0.53470016 0.61287236 0.69305474 0.77761459 0.83695656 0.84321123 0.80150163 0.71892691 0.60506147 0.46449551 0.31929654 0.19903673 0.12477046][0.343482 0.4174543 0.49098966 0.55808425 0.62636113 0.70053959 0.74974883 0.75127113 0.71291876 0.6416682 0.54206157 0.41616264 0.28867912 0.18761812 0.12393079][0.24476634 0.30423465 0.36112055 0.40872407 0.45943308 0.52003545 0.5616622 0.56519717 0.5385375 0.48736978 0.4123145 0.31422383 0.21688557 0.14410487 0.097696505][0.11201455 0.15120262 0.18812095 0.21645153 0.25013089 0.29550231 0.32757112 0.33166441 0.31673709 0.28731373 0.2405948 0.17582472 0.11321786 0.070380062 0.042633325][-0.0024092293 0.014258732 0.027681896 0.033692714 0.045743007 0.069306351 0.085757025 0.086510085 0.081177831 0.07263609 0.054103076 0.022583852 -0.0067734388 -0.022638673 -0.03241647][-0.068016089 -0.068959683 -0.073693655 -0.084353909 -0.089935273 -0.086037345 -0.0843749 -0.088217653 -0.088091128 -0.082578987 -0.080510668 -0.086648479 -0.091408335 -0.088712953 -0.085859269][-0.0989522 -0.10857712 -0.12058469 -0.13656944 -0.14896695 -0.15448219 -0.15925194 -0.16401371 -0.16141605 -0.15070936 -0.13968095 -0.13314256 -0.126259 -0.11631955 -0.10879359][-0.11434536 -0.12597251 -0.13751414 -0.15115844 -0.16242552 -0.16920938 -0.17451185 -0.17857839 -0.17675635 -0.16838664 -0.15809168 -0.14889435 -0.13888736 -0.12768199 -0.11925312][-0.11713611 -0.127282 -0.13559204 -0.14522794 -0.15400477 -0.16054685 -0.16569798 -0.16936684 -0.1692512 -0.16478015 -0.15776685 -0.14963551 -0.14018974 -0.13037851 -0.12290312]]...]
INFO - root - 2017-12-11 03:53:09.617698: step 9510, loss = 0.68, batch loss = 0.62 (14.7 examples/sec; 0.543 sec/batch; 48h:45m:14s remains)
INFO - root - 2017-12-11 03:53:15.191956: step 9520, loss = 0.69, batch loss = 0.64 (14.6 examples/sec; 0.550 sec/batch; 49h:18m:23s remains)
INFO - root - 2017-12-11 03:53:20.669227: step 9530, loss = 0.71, batch loss = 0.66 (14.6 examples/sec; 0.546 sec/batch; 48h:59m:43s remains)
INFO - root - 2017-12-11 03:53:26.182709: step 9540, loss = 0.69, batch loss = 0.63 (14.4 examples/sec; 0.555 sec/batch; 49h:48m:42s remains)
INFO - root - 2017-12-11 03:53:31.663119: step 9550, loss = 0.69, batch loss = 0.63 (14.8 examples/sec; 0.539 sec/batch; 48h:22m:25s remains)
INFO - root - 2017-12-11 03:53:37.244315: step 9560, loss = 0.70, batch loss = 0.64 (14.4 examples/sec; 0.554 sec/batch; 49h:44m:00s remains)
INFO - root - 2017-12-11 03:53:42.431299: step 9570, loss = 0.69, batch loss = 0.63 (31.0 examples/sec; 0.258 sec/batch; 23h:09m:46s remains)
INFO - root - 2017-12-11 03:53:47.950497: step 9580, loss = 0.69, batch loss = 0.63 (14.1 examples/sec; 0.567 sec/batch; 50h:50m:54s remains)
INFO - root - 2017-12-11 03:53:53.377803: step 9590, loss = 0.70, batch loss = 0.65 (14.7 examples/sec; 0.543 sec/batch; 48h:42m:35s remains)
INFO - root - 2017-12-11 03:53:58.795809: step 9600, loss = 0.71, batch loss = 0.65 (14.8 examples/sec; 0.541 sec/batch; 48h:32m:48s remains)
2017-12-11 03:53:59.374226: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.054785371 -0.055324182 -0.035724327 0.0055284505 0.065094158 0.13532345 0.20744221 0.27133483 0.31937736 0.34724954 0.34970933 0.32586819 0.28551385 0.23649599 0.1902853][-0.053200945 -0.064915471 -0.055965822 -0.021023683 0.038186617 0.11400324 0.19783022 0.27608883 0.33656153 0.37146896 0.37333521 0.34398445 0.29665071 0.24278392 0.19651039][-0.019615879 -0.040537637 -0.04304111 -0.017028738 0.037306946 0.1124928 0.20046924 0.28396025 0.34656087 0.37885949 0.37259167 0.33366987 0.27822587 0.22044685 0.1757637][0.045015857 0.01476156 -0.0017162858 0.011637147 0.056399982 0.12576178 0.21254447 0.29554072 0.3544732 0.37837356 0.35951149 0.30796993 0.24236308 0.17922422 0.13391268][0.13104489 0.088669345 0.054084323 0.050320424 0.08121483 0.14079136 0.22251607 0.30154711 0.35436812 0.3682946 0.33635589 0.27225477 0.19668569 0.12723437 0.0787037][0.23100723 0.17532329 0.12183221 0.10034511 0.11718632 0.16654368 0.24096589 0.312762 0.35640708 0.35821366 0.31227812 0.23523721 0.15028398 0.074844606 0.02274888][0.33086491 0.26872975 0.20431378 0.17247033 0.18108551 0.22418708 0.29175746 0.35496151 0.38745034 0.3760916 0.31560534 0.22398439 0.12724656 0.042798944 -0.015511521][0.40915939 0.35305151 0.29326913 0.2646102 0.27497432 0.31827539 0.38118032 0.43544331 0.45517293 0.42954987 0.35386744 0.24525279 0.13286886 0.03589084 -0.031061633][0.44308418 0.40532738 0.36687818 0.35648873 0.37983271 0.42986161 0.48970938 0.53358835 0.53812522 0.496648 0.40583473 0.28097674 0.15436195 0.046983652 -0.026315127][0.43134579 0.41935116 0.41078189 0.42536351 0.46568668 0.52410084 0.58100736 0.61411816 0.60420215 0.54907072 0.44721815 0.31177536 0.17697647 0.065367155 -0.009378342][0.38127285 0.39258447 0.40964037 0.44261068 0.49257579 0.5537318 0.60599047 0.63093638 0.61317986 0.55324322 0.45042995 0.31547329 0.18265907 0.075897753 0.0070984652][0.29216948 0.31566435 0.34507173 0.38252291 0.42945147 0.48372135 0.52795768 0.54745626 0.52996993 0.47641873 0.3857488 0.26638392 0.14985165 0.060215596 0.0068580019][0.18135446 0.20356746 0.23031904 0.2586225 0.29126215 0.32975283 0.36151412 0.37550935 0.36257914 0.32264429 0.25403216 0.16316171 0.076349705 0.015585801 -0.013317887][0.0798457 0.092371568 0.10635378 0.11806358 0.13188121 0.15186469 0.17037529 0.17960179 0.17311263 0.14909592 0.10569333 0.047943782 -0.003961056 -0.032005332 -0.034165096][0.012782277 0.013948297 0.013159997 0.0087456917 0.0066804197 0.012495706 0.022140104 0.029901458 0.030792616 0.021194806 -0.00072568859 -0.030301593 -0.05266694 -0.053891782 -0.034952804]]...]
INFO - root - 2017-12-11 03:54:04.954560: step 9610, loss = 0.70, batch loss = 0.64 (14.5 examples/sec; 0.552 sec/batch; 49h:30m:33s remains)
INFO - root - 2017-12-11 03:54:10.408659: step 9620, loss = 0.70, batch loss = 0.64 (14.5 examples/sec; 0.552 sec/batch; 49h:29m:57s remains)
INFO - root - 2017-12-11 03:54:16.002613: step 9630, loss = 0.69, batch loss = 0.64 (13.9 examples/sec; 0.577 sec/batch; 51h:42m:38s remains)
INFO - root - 2017-12-11 03:54:21.531253: step 9640, loss = 0.71, batch loss = 0.65 (14.4 examples/sec; 0.554 sec/batch; 49h:41m:23s remains)
INFO - root - 2017-12-11 03:54:26.991162: step 9650, loss = 0.69, batch loss = 0.63 (15.0 examples/sec; 0.534 sec/batch; 47h:51m:05s remains)
INFO - root - 2017-12-11 03:54:32.558987: step 9660, loss = 0.70, batch loss = 0.64 (14.4 examples/sec; 0.554 sec/batch; 49h:43m:27s remains)
INFO - root - 2017-12-11 03:54:37.867880: step 9670, loss = 0.68, batch loss = 0.63 (14.0 examples/sec; 0.571 sec/batch; 51h:11m:23s remains)
INFO - root - 2017-12-11 03:54:43.439710: step 9680, loss = 0.70, batch loss = 0.64 (14.4 examples/sec; 0.555 sec/batch; 49h:44m:47s remains)
INFO - root - 2017-12-11 03:54:48.889781: step 9690, loss = 0.71, batch loss = 0.65 (15.0 examples/sec; 0.535 sec/batch; 47h:56m:27s remains)
INFO - root - 2017-12-11 03:54:54.318855: step 9700, loss = 0.70, batch loss = 0.64 (14.4 examples/sec; 0.555 sec/batch; 49h:44m:38s remains)
2017-12-11 03:54:54.964674: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.10537006 0.10382209 0.09832032 0.090829611 0.08222305 0.074133821 0.068223536 0.065124713 0.063608646 0.061692711 0.058304597 0.054312833 0.050564237 0.04901867 0.053021837][0.15434055 0.15791249 0.15594287 0.1502331 0.14152582 0.1319225 0.12407601 0.12011227 0.11828945 0.11683895 0.11415637 0.11112902 0.10868859 0.10926534 0.11468287][0.19861706 0.21057558 0.21505897 0.2124445 0.20326331 0.19037603 0.1785396 0.17177317 0.16852242 0.16837378 0.16859056 0.16913636 0.17060336 0.17658491 0.18657428][0.22888643 0.24819605 0.25750604 0.25675032 0.24668333 0.2317996 0.21982954 0.21586594 0.21699503 0.223023 0.22963367 0.23461896 0.23804294 0.24603721 0.25812858][0.24232544 0.26674464 0.281048 0.28542849 0.28072384 0.27320459 0.27226278 0.28072974 0.29210976 0.30462641 0.31261182 0.31189242 0.30447131 0.3012737 0.30558005][0.23694813 0.26542425 0.2879577 0.30560926 0.31821582 0.33262604 0.35658509 0.38573408 0.40757462 0.41763356 0.41162562 0.38715616 0.35173625 0.32327029 0.31078234][0.21295244 0.24872546 0.28702331 0.3277643 0.36725065 0.41082528 0.46221504 0.50810391 0.53074551 0.52451843 0.49025691 0.43092036 0.36031586 0.30268615 0.272214][0.17175753 0.21963777 0.27917492 0.34523055 0.40907362 0.4738287 0.539697 0.587509 0.59824687 0.56789148 0.50400424 0.41495198 0.31795639 0.24004509 0.19776554][0.1253327 0.18284351 0.25682396 0.33609012 0.40811327 0.47435269 0.53385794 0.56729937 0.55900162 0.507563 0.42538613 0.32416871 0.22053368 0.13924989 0.095394582][0.086812921 0.14342275 0.2148935 0.28677163 0.34560189 0.39277792 0.42897895 0.43930334 0.4137682 0.35332236 0.27135316 0.17951982 0.090466313 0.023079148 -0.011279427][0.044460315 0.085370339 0.13690437 0.18573423 0.22026035 0.24226435 0.2543183 0.24673854 0.21444853 0.15979423 0.09417519 0.026739141 -0.034532838 -0.077127278 -0.093771458][0.0013694688 0.018180715 0.042022746 0.063666135 0.075262181 0.078669228 0.076653495 0.063081488 0.035677791 -0.00308517 -0.045097139 -0.084316641 -0.11625381 -0.13356966 -0.13244946][-0.032159295 -0.033983529 -0.030918529 -0.028296793 -0.030401055 -0.035761051 -0.042497724 -0.054261737 -0.07184276 -0.093290992 -0.11389091 -0.1303225 -0.1403089 -0.14028423 -0.1292523][-0.052819025 -0.062764943 -0.068679079 -0.073567174 -0.079524204 -0.085791595 -0.091524005 -0.098645851 -0.1073311 -0.11631324 -0.12331185 -0.12670192 -0.12540053 -0.11823083 -0.10592069][-0.05961474 -0.070213184 -0.077451892 -0.083171852 -0.088147745 -0.092123732 -0.095038213 -0.098002262 -0.10115653 -0.10372399 -0.1046301 -0.10305247 -0.098441988 -0.090831213 -0.08160653]]...]
INFO - root - 2017-12-11 03:55:00.386419: step 9710, loss = 0.69, batch loss = 0.63 (14.7 examples/sec; 0.545 sec/batch; 48h:50m:01s remains)
INFO - root - 2017-12-11 03:55:05.861974: step 9720, loss = 0.70, batch loss = 0.64 (14.5 examples/sec; 0.552 sec/batch; 49h:30m:45s remains)
INFO - root - 2017-12-11 03:55:11.394310: step 9730, loss = 0.69, batch loss = 0.63 (14.6 examples/sec; 0.546 sec/batch; 48h:59m:17s remains)
INFO - root - 2017-12-11 03:55:16.922939: step 9740, loss = 0.71, batch loss = 0.65 (14.7 examples/sec; 0.542 sec/batch; 48h:38m:02s remains)
INFO - root - 2017-12-11 03:55:22.344730: step 9750, loss = 0.70, batch loss = 0.64 (14.8 examples/sec; 0.541 sec/batch; 48h:27m:47s remains)
INFO - root - 2017-12-11 03:55:27.786760: step 9760, loss = 0.69, batch loss = 0.63 (14.9 examples/sec; 0.536 sec/batch; 48h:05m:21s remains)
INFO - root - 2017-12-11 03:55:32.982285: step 9770, loss = 0.72, batch loss = 0.66 (14.3 examples/sec; 0.558 sec/batch; 50h:01m:07s remains)
INFO - root - 2017-12-11 03:55:38.471211: step 9780, loss = 0.68, batch loss = 0.62 (14.7 examples/sec; 0.546 sec/batch; 48h:55m:47s remains)
INFO - root - 2017-12-11 03:55:43.945040: step 9790, loss = 0.69, batch loss = 0.63 (14.8 examples/sec; 0.541 sec/batch; 48h:30m:00s remains)
INFO - root - 2017-12-11 03:55:49.535661: step 9800, loss = 0.71, batch loss = 0.65 (14.5 examples/sec; 0.553 sec/batch; 49h:31m:58s remains)
2017-12-11 03:55:50.075542: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.19130261 0.19075465 0.18369848 0.17643757 0.17520158 0.18511796 0.20494512 0.22743498 0.24375315 0.24873221 0.24475195 0.23828451 0.23845129 0.25333968 0.28660983][0.13729328 0.13376492 0.1320322 0.13794163 0.15591019 0.18621598 0.22366874 0.25902969 0.28299996 0.29015046 0.28261176 0.26886055 0.26055303 0.26695359 0.2915076][0.10938721 0.10527945 0.11244676 0.13500492 0.17345437 0.22254832 0.27285376 0.31395587 0.33697492 0.33676142 0.31543526 0.28411922 0.25755572 0.24681057 0.25627363][0.10788642 0.10768042 0.12653208 0.16585176 0.22125682 0.28274646 0.33735317 0.37390694 0.38592783 0.37049776 0.3307372 0.27939326 0.23362733 0.20674242 0.2040029][0.10999017 0.11798917 0.15065075 0.20618181 0.2759999 0.34660488 0.40227193 0.43108723 0.42954257 0.39766732 0.34034759 0.2721858 0.21256723 0.17642558 0.16863099][0.11714016 0.1381817 0.18623558 0.25638416 0.33695224 0.41222942 0.46504459 0.48379117 0.46807435 0.42120844 0.35109964 0.2743437 0.211215 0.17609075 0.17099863][0.14030768 0.17547974 0.23440886 0.31121597 0.39383444 0.46699455 0.513877 0.5235337 0.49716538 0.44018313 0.36379352 0.28636408 0.22793624 0.20056288 0.20234489][0.17609766 0.21795765 0.27675074 0.34933394 0.42584294 0.49270773 0.53394312 0.5389694 0.50915217 0.45042402 0.37608892 0.30517611 0.25623628 0.23819673 0.24655916][0.21758075 0.25633371 0.30364493 0.36233035 0.42620429 0.48431605 0.52227908 0.53014684 0.50747317 0.45790872 0.39404052 0.33432791 0.29564777 0.28470671 0.29667389][0.26540995 0.29280481 0.32151932 0.36017296 0.40633157 0.45245558 0.48709127 0.50214368 0.49423507 0.4621194 0.41489935 0.369188 0.3399545 0.3321377 0.34177774][0.31305221 0.33074534 0.34177876 0.35841247 0.38174433 0.40936783 0.43524477 0.45519084 0.46286276 0.45026216 0.42114 0.38889888 0.36671221 0.35902232 0.36373726][0.35106647 0.36610726 0.36572906 0.36291093 0.36199605 0.36683995 0.37834549 0.39703351 0.41436923 0.41623098 0.40159103 0.38063094 0.36487454 0.35923409 0.36331448][0.36701375 0.38584402 0.38258663 0.36846974 0.34991929 0.33613154 0.33366346 0.34608144 0.36400351 0.37056527 0.36274448 0.34912616 0.33988532 0.34027857 0.35011643][0.35967615 0.3851786 0.38527277 0.36724991 0.33812502 0.31120259 0.29741427 0.3014808 0.31375825 0.31748161 0.31009844 0.30013752 0.29696655 0.3064487 0.32790649][0.32923549 0.3585358 0.36276507 0.34521365 0.31247917 0.27990937 0.26041076 0.25790772 0.26261082 0.25982255 0.24902174 0.24026571 0.24281621 0.26281935 0.29807666]]...]
INFO - root - 2017-12-11 03:55:55.665543: step 9810, loss = 0.70, batch loss = 0.64 (14.6 examples/sec; 0.548 sec/batch; 49h:07m:47s remains)
INFO - root - 2017-12-11 03:56:01.225083: step 9820, loss = 0.68, batch loss = 0.63 (13.9 examples/sec; 0.574 sec/batch; 51h:28m:01s remains)
INFO - root - 2017-12-11 03:56:06.696159: step 9830, loss = 0.69, batch loss = 0.63 (14.7 examples/sec; 0.545 sec/batch; 48h:48m:59s remains)
INFO - root - 2017-12-11 03:56:12.258946: step 9840, loss = 0.69, batch loss = 0.63 (14.5 examples/sec; 0.553 sec/batch; 49h:36m:11s remains)
INFO - root - 2017-12-11 03:56:17.778441: step 9850, loss = 0.69, batch loss = 0.63 (14.4 examples/sec; 0.555 sec/batch; 49h:42m:10s remains)
INFO - root - 2017-12-11 03:56:23.275221: step 9860, loss = 0.69, batch loss = 0.63 (14.8 examples/sec; 0.542 sec/batch; 48h:35m:41s remains)
INFO - root - 2017-12-11 03:56:28.458135: step 9870, loss = 0.70, batch loss = 0.65 (15.1 examples/sec; 0.529 sec/batch; 47h:23m:09s remains)
INFO - root - 2017-12-11 03:56:33.899945: step 9880, loss = 0.69, batch loss = 0.64 (14.5 examples/sec; 0.552 sec/batch; 49h:28m:50s remains)
INFO - root - 2017-12-11 03:56:39.360836: step 9890, loss = 0.69, batch loss = 0.63 (14.6 examples/sec; 0.548 sec/batch; 49h:04m:59s remains)
INFO - root - 2017-12-11 03:56:44.942555: step 9900, loss = 0.69, batch loss = 0.63 (14.6 examples/sec; 0.548 sec/batch; 49h:05m:43s remains)
2017-12-11 03:56:45.515219: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.010269114 0.01828764 0.021172985 0.019246081 0.014344807 0.0072688991 0.0040671867 0.0044578938 0.0032163262 -0.0017153156 -0.0084706666 -0.012779574 -0.01276557 -0.0086649293 -0.0037776625][0.059040368 0.077015892 0.086527526 0.089130744 0.087094009 0.080120869 0.07744728 0.07803423 0.075237215 0.065222047 0.049168207 0.034251746 0.025435196 0.025281884 0.030119853][0.11975686 0.15087968 0.16935398 0.17774922 0.17915463 0.17318881 0.17183143 0.17283095 0.16847703 0.15224615 0.1245175 0.094999261 0.072372884 0.063281566 0.065895237][0.17878218 0.22231908 0.24940644 0.26290643 0.2672517 0.26356003 0.26545009 0.26873267 0.26418796 0.24236473 0.202652 0.15663642 0.1163281 0.093879431 0.090875827][0.22895274 0.28204104 0.31573269 0.33314037 0.33981293 0.33947664 0.34683883 0.35513493 0.35234314 0.32619178 0.27553475 0.2131992 0.15408838 0.11541515 0.10308201][0.27420384 0.33538955 0.37478074 0.39519393 0.40237764 0.40358809 0.41518423 0.42756882 0.42529431 0.39405134 0.33324152 0.25610322 0.17915981 0.12396634 0.10099714][0.308006 0.3762967 0.42053095 0.44234475 0.44646505 0.44512156 0.45768759 0.47272134 0.47112972 0.43678334 0.3699657 0.28273547 0.19144246 0.12055822 0.085711941][0.32851297 0.40023544 0.44585055 0.46528617 0.46234417 0.45482647 0.46637079 0.48477736 0.48692137 0.45430952 0.38748881 0.29643697 0.19544356 0.11094192 0.063194439][0.33190268 0.40301645 0.44722566 0.4630093 0.45362648 0.44063589 0.45202175 0.47592524 0.48465729 0.45689139 0.39346224 0.30333644 0.19823633 0.10427427 0.045448449][0.32103088 0.39139554 0.43478176 0.4485555 0.43544978 0.4186919 0.43041402 0.46081907 0.47799405 0.45741358 0.40007496 0.31486672 0.21083024 0.11223969 0.045646615][0.30292016 0.37204176 0.41370586 0.42518735 0.4088724 0.38769814 0.39693588 0.43069926 0.455096 0.44318351 0.3950983 0.31898114 0.22138947 0.12452041 0.056137774][0.27018452 0.33280239 0.36831012 0.37477869 0.35454398 0.32895413 0.33356237 0.36675242 0.39652973 0.3954359 0.36065784 0.29798171 0.2121917 0.12416255 0.060941249][0.21344967 0.26376361 0.29021883 0.29105532 0.26844668 0.24085924 0.24110892 0.27123985 0.30415353 0.31347352 0.29269785 0.24514973 0.17531887 0.10265688 0.051126633][0.13201411 0.16778269 0.18584421 0.18378086 0.16359025 0.13970387 0.13962726 0.16705287 0.20055394 0.21637724 0.20609117 0.17237793 0.12038849 0.067105547 0.031686313][0.057660528 0.078945711 0.08822149 0.083327055 0.066327013 0.047702048 0.048347436 0.072241135 0.10286262 0.12059233 0.1177722 0.097241342 0.0637609 0.030149138 0.010338296]]...]
INFO - root - 2017-12-11 03:56:51.121810: step 9910, loss = 0.70, batch loss = 0.64 (14.8 examples/sec; 0.539 sec/batch; 48h:19m:47s remains)
INFO - root - 2017-12-11 03:56:56.582407: step 9920, loss = 0.70, batch loss = 0.64 (14.8 examples/sec; 0.539 sec/batch; 48h:18m:46s remains)
INFO - root - 2017-12-11 03:57:02.129564: step 9930, loss = 0.72, batch loss = 0.66 (14.6 examples/sec; 0.549 sec/batch; 49h:09m:59s remains)
INFO - root - 2017-12-11 03:57:07.624876: step 9940, loss = 0.68, batch loss = 0.62 (14.5 examples/sec; 0.551 sec/batch; 49h:24m:20s remains)
INFO - root - 2017-12-11 03:57:13.046694: step 9950, loss = 0.68, batch loss = 0.62 (15.5 examples/sec; 0.517 sec/batch; 46h:20m:29s remains)
INFO - root - 2017-12-11 03:57:18.189574: step 9960, loss = 0.71, batch loss = 0.65 (14.5 examples/sec; 0.550 sec/batch; 49h:19m:02s remains)
INFO - root - 2017-12-11 03:57:23.668022: step 9970, loss = 0.70, batch loss = 0.65 (14.9 examples/sec; 0.537 sec/batch; 48h:06m:29s remains)
INFO - root - 2017-12-11 03:57:29.209677: step 9980, loss = 0.69, batch loss = 0.63 (14.3 examples/sec; 0.561 sec/batch; 50h:15m:51s remains)
INFO - root - 2017-12-11 03:57:34.638337: step 9990, loss = 0.70, batch loss = 0.64 (14.4 examples/sec; 0.556 sec/batch; 49h:46m:57s remains)
INFO - root - 2017-12-11 03:57:40.226740: step 10000, loss = 0.72, batch loss = 0.67 (13.3 examples/sec; 0.603 sec/batch; 54h:00m:31s remains)
2017-12-11 03:57:40.813518: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.15889585 0.17716759 0.17513597 0.15415917 0.12599026 0.10110645 0.085098594 0.080051973 0.096400946 0.14178731 0.2019619 0.25226372 0.26951665 0.25376293 0.20132178][0.16522868 0.18747687 0.18641016 0.16374317 0.13053998 0.09808249 0.073395856 0.05993614 0.065737277 0.095769264 0.14059795 0.18194044 0.19954798 0.19203305 0.15294205][0.16804212 0.19547224 0.20004408 0.18399352 0.15489662 0.12172063 0.0924668 0.072297238 0.066849329 0.078470983 0.10258149 0.1265139 0.1356183 0.12894425 0.099665716][0.17727111 0.2152822 0.23518758 0.23895872 0.22751532 0.20532073 0.18021373 0.15703501 0.13940446 0.12990531 0.12828045 0.12672222 0.11577388 0.097557634 0.0676955][0.18841453 0.2435407 0.28859353 0.32375962 0.34139466 0.34110746 0.3302829 0.3107104 0.28447321 0.256724 0.22982079 0.19900337 0.15949418 0.11813839 0.074235454][0.19776252 0.27218643 0.34619108 0.41615433 0.4672958 0.49532038 0.506518 0.49734038 0.469358 0.4333795 0.39212057 0.3395611 0.27232435 0.20204489 0.13285264][0.20616478 0.29297632 0.38843 0.48475978 0.56425762 0.61969727 0.65405148 0.65856647 0.63660681 0.60702223 0.56876767 0.50928974 0.42330545 0.32627025 0.22748716][0.21978354 0.30324197 0.40274069 0.50832176 0.602433 0.67551893 0.72645146 0.7434963 0.73467761 0.72645581 0.70945 0.66039664 0.56857985 0.45227838 0.32619974][0.2400157 0.30191559 0.38177302 0.47180197 0.55876404 0.63338858 0.69103676 0.71890211 0.73076862 0.75679421 0.77654678 0.754501 0.67299771 0.55073392 0.40598726][0.27284694 0.29687807 0.33342323 0.38282961 0.44065794 0.49987715 0.55314159 0.58738774 0.61974674 0.67942083 0.73734647 0.74978483 0.69349039 0.58350873 0.43757355][0.3207632 0.30020383 0.28110304 0.27705655 0.29438129 0.32872206 0.36979005 0.40368184 0.44683951 0.52532154 0.60727537 0.64786124 0.62094414 0.53532404 0.4046343][0.37614015 0.31603035 0.24598865 0.19424805 0.17488028 0.18518932 0.21174195 0.23922141 0.28025565 0.35725775 0.44238827 0.49574196 0.49091005 0.42983016 0.32106352][0.43758309 0.35301471 0.24809578 0.16353883 0.11785325 0.10960967 0.12285796 0.13993421 0.16769211 0.22537014 0.29501155 0.3458603 0.35302919 0.31234664 0.22684515][0.50308609 0.41279766 0.29240254 0.19020374 0.12781359 0.10611013 0.10824998 0.11413842 0.12382102 0.15404367 0.19936134 0.23969011 0.2525593 0.22823966 0.16383022][0.55607158 0.47641018 0.35757795 0.24938242 0.17654654 0.14437842 0.13743997 0.13451773 0.12914573 0.13548215 0.15884922 0.18886597 0.20544517 0.19381426 0.14536847]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-SGD-lr0.001-clip50-initfromfixqian30k-fixqian/model.ckpt-10000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-SGD-lr0.001-clip50-initfromfixqian30k-fixqian/model.ckpt-10000 is not in all_model_checkpoint_paths. Manually adding it.
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-SGD-lr0.001-clip50-initfromfixqian30k-fixqian
INFO - root - 2017-12-11 03:57:47.130400: step 10010, loss = 0.69, batch loss = 0.63 (14.3 examples/sec; 0.558 sec/batch; 50h:00m:42s remains)
INFO - root - 2017-12-11 03:57:52.624785: step 10020, loss = 0.69, batch loss = 0.63 (14.4 examples/sec; 0.557 sec/batch; 49h:51m:04s remains)
INFO - root - 2017-12-11 03:57:58.159563: step 10030, loss = 0.70, batch loss = 0.65 (14.6 examples/sec; 0.548 sec/batch; 49h:03m:28s remains)
INFO - root - 2017-12-11 03:58:03.606583: step 10040, loss = 0.69, batch loss = 0.63 (14.8 examples/sec; 0.542 sec/batch; 48h:33m:55s remains)
INFO - root - 2017-12-11 03:58:09.070622: step 10050, loss = 0.70, batch loss = 0.64 (14.5 examples/sec; 0.552 sec/batch; 49h:28m:01s remains)
INFO - root - 2017-12-11 03:58:14.233862: step 10060, loss = 0.69, batch loss = 0.63 (14.3 examples/sec; 0.559 sec/batch; 50h:06m:22s remains)
INFO - root - 2017-12-11 03:58:19.734196: step 10070, loss = 0.69, batch loss = 0.63 (14.4 examples/sec; 0.555 sec/batch; 49h:44m:06s remains)
INFO - root - 2017-12-11 03:58:25.299444: step 10080, loss = 0.71, batch loss = 0.65 (14.7 examples/sec; 0.543 sec/batch; 48h:35m:55s remains)
INFO - root - 2017-12-11 03:58:30.783749: step 10090, loss = 0.68, batch loss = 0.62 (14.7 examples/sec; 0.545 sec/batch; 48h:46m:50s remains)
INFO - root - 2017-12-11 03:58:36.341539: step 10100, loss = 0.70, batch loss = 0.64 (14.8 examples/sec; 0.540 sec/batch; 48h:22m:01s remains)
2017-12-11 03:58:36.954983: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.11956458 0.11411993 0.085558437 0.044377804 0.0042763148 -0.024287043 -0.039320163 -0.038134106 -0.022320658 0.0014470617 0.024350073 0.0362614 0.035816081 0.02607308 0.013649079][0.13713214 0.1460883 0.13120052 0.099055246 0.061152879 0.026826009 -0.0016909014 -0.01681998 -0.01352311 0.0055014822 0.029359991 0.046490785 0.05429427 0.051934384 0.041301187][0.15478732 0.18413313 0.19208 0.18002136 0.15588708 0.12592655 0.089505427 0.05621646 0.038095813 0.039244317 0.050811168 0.063219979 0.072798163 0.074822463 0.066105179][0.17562498 0.22970189 0.26698548 0.28323939 0.28360555 0.27096641 0.23864454 0.19251268 0.14933173 0.12158909 0.10612848 0.097631074 0.094054691 0.090325087 0.0799798][0.19829662 0.27535069 0.34151733 0.38844332 0.42063469 0.43831724 0.42567343 0.37877986 0.31464985 0.2534886 0.19993888 0.1545825 0.12052384 0.0979044 0.079591878][0.21298444 0.30431765 0.39039388 0.46126249 0.52348524 0.57672393 0.59615016 0.56452334 0.49331769 0.40646511 0.31535125 0.22661522 0.15166612 0.1002014 0.068088166][0.21233939 0.30403426 0.39441422 0.4748759 0.55564493 0.63831192 0.69208682 0.68747556 0.62591338 0.53024483 0.41531959 0.29273647 0.18333046 0.10708276 0.063649051][0.20990033 0.28541741 0.35908416 0.42755613 0.50524 0.59743661 0.67301017 0.69486719 0.65370226 0.56750041 0.45100722 0.31888077 0.19932535 0.11837413 0.076465182][0.22161393 0.26977342 0.30984396 0.34615436 0.39669007 0.47169954 0.54629982 0.58338815 0.56594259 0.50464612 0.41033083 0.297755 0.19822839 0.13747975 0.11322879][0.2465052 0.26482636 0.26574543 0.26154876 0.27260536 0.31357229 0.3691172 0.40804672 0.41058937 0.38079506 0.32428712 0.25231317 0.19428866 0.17005354 0.17281096][0.26646042 0.2596494 0.22744788 0.18840364 0.16415495 0.171653 0.20399876 0.23798726 0.2551955 0.25598738 0.24127662 0.21583262 0.20312147 0.21521263 0.24156608][0.262599 0.23845969 0.18550834 0.12647632 0.083438292 0.072539642 0.090799242 0.12149126 0.14986393 0.17525038 0.19563533 0.20849285 0.22934911 0.2632834 0.29845479][0.22288036 0.18999611 0.13061666 0.069297522 0.027431672 0.017649435 0.036523752 0.071285494 0.11109156 0.15506083 0.19916297 0.23488863 0.2698465 0.30470198 0.32953382][0.14848128 0.11530463 0.06229249 0.012861184 -0.013617421 -0.0090888413 0.020820325 0.065499857 0.11701065 0.17358275 0.22912306 0.27195284 0.30390584 0.32394597 0.32621869][0.062488168 0.034879543 -0.0049503255 -0.0376264 -0.047311015 -0.029657133 0.0088952333 0.0598078 0.11713601 0.17753603 0.23329176 0.27269876 0.29441848 0.29702619 0.27830437]]...]
INFO - root - 2017-12-11 03:58:42.551729: step 10110, loss = 0.69, batch loss = 0.63 (14.8 examples/sec; 0.542 sec/batch; 48h:32m:14s remains)
INFO - root - 2017-12-11 03:58:48.039545: step 10120, loss = 0.70, batch loss = 0.64 (14.6 examples/sec; 0.549 sec/batch; 49h:11m:54s remains)
INFO - root - 2017-12-11 03:58:53.519247: step 10130, loss = 0.69, batch loss = 0.63 (14.3 examples/sec; 0.559 sec/batch; 50h:02m:03s remains)
INFO - root - 2017-12-11 03:58:59.121607: step 10140, loss = 0.70, batch loss = 0.64 (14.1 examples/sec; 0.569 sec/batch; 50h:54m:57s remains)
INFO - root - 2017-12-11 03:59:04.394864: step 10150, loss = 0.69, batch loss = 0.63 (29.2 examples/sec; 0.274 sec/batch; 24h:30m:16s remains)
INFO - root - 2017-12-11 03:59:09.862773: step 10160, loss = 0.68, batch loss = 0.62 (14.8 examples/sec; 0.540 sec/batch; 48h:21m:16s remains)
INFO - root - 2017-12-11 03:59:15.346950: step 10170, loss = 0.69, batch loss = 0.64 (14.9 examples/sec; 0.538 sec/batch; 48h:12m:40s remains)
INFO - root - 2017-12-11 03:59:20.851052: step 10180, loss = 0.71, batch loss = 0.66 (14.9 examples/sec; 0.536 sec/batch; 47h:59m:02s remains)
INFO - root - 2017-12-11 03:59:26.367385: step 10190, loss = 0.68, batch loss = 0.63 (15.6 examples/sec; 0.514 sec/batch; 45h:58m:44s remains)
INFO - root - 2017-12-11 03:59:31.958800: step 10200, loss = 0.70, batch loss = 0.64 (14.5 examples/sec; 0.553 sec/batch; 49h:30m:28s remains)
2017-12-11 03:59:32.561347: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.04710608 0.041864473 0.036908656 0.029988758 0.01726494 0.000884419 -0.014849249 -0.024396449 -0.025515355 -0.01909107 -0.0097431242 -0.0041510556 -0.0042233868 -0.0099858018 -0.017005514][0.013561385 0.0090004085 0.0077706915 0.0061236741 0.0026170628 -0.00049206929 -0.0024620506 -0.0020919761 0.00097557809 0.0068381648 0.012486655 0.013583413 0.0095642917 0.00055578427 -0.0096430751][-0.0023110067 -0.0042921705 -0.00036296464 0.0048654974 0.012452586 0.024139581 0.036068752 0.044765428 0.048978548 0.05039857 0.04870775 0.042688821 0.032883611 0.020072922 0.00731491][0.0064598983 0.0067465347 0.01518211 0.027114892 0.045517169 0.071080253 0.095513158 0.11032979 0.11281566 0.10708759 0.0959699 0.081525609 0.065115675 0.048910584 0.035304286][0.03351609 0.035423588 0.047335718 0.065395646 0.094091088 0.13247651 0.16781822 0.18693507 0.1860217 0.17215861 0.15163666 0.12930815 0.10725586 0.089364715 0.07757584][0.066886529 0.0712237 0.087889194 0.11320283 0.15165672 0.20019096 0.24228056 0.26151153 0.2539908 0.23048563 0.20107339 0.17341959 0.14993425 0.13495137 0.12936072][0.096614875 0.10559339 0.12938139 0.16379054 0.21091619 0.26514149 0.30767152 0.32128456 0.3033959 0.2684966 0.23067845 0.20063612 0.18081863 0.17414153 0.17939979][0.11788119 0.13243076 0.16487464 0.20889206 0.26228496 0.31717581 0.35428163 0.3580448 0.32840344 0.28290352 0.23884989 0.20943965 0.19742432 0.20227119 0.21928371][0.12534294 0.1447925 0.1843234 0.235334 0.29078513 0.34108597 0.36850813 0.36110458 0.32166293 0.26988333 0.22484955 0.20060788 0.19997421 0.21746762 0.24431296][0.10833302 0.13101131 0.1753322 0.23023115 0.28421542 0.32620016 0.34154692 0.32329321 0.27741304 0.22488733 0.18557177 0.17241007 0.18607867 0.21572721 0.24921408][0.068284847 0.093232423 0.14064035 0.19716693 0.24820402 0.28168252 0.28655666 0.26048389 0.21218158 0.1629533 0.13235243 0.13088655 0.15612742 0.19324146 0.22855645][0.023632288 0.050152875 0.097145654 0.15087584 0.19643675 0.22234049 0.22095954 0.19282654 0.14760768 0.10512191 0.083144322 0.088273622 0.115893 0.15068851 0.18106617][-0.0048835566 0.020776365 0.0629577 0.10930985 0.14728013 0.16716844 0.16429505 0.1402189 0.10335444 0.070093766 0.054945137 0.060229998 0.080283664 0.10270572 0.12173214][-0.0077356878 0.016267786 0.051698871 0.0895674 0.12035725 0.13646708 0.13586822 0.11958098 0.093166545 0.068510287 0.056602497 0.056036782 0.060901787 0.064573944 0.069164112][0.017424542 0.041031491 0.07119675 0.10245039 0.127668 0.14170751 0.14514896 0.13806745 0.12187815 0.10334892 0.089967921 0.078376979 0.063354157 0.044992205 0.033341877]]...]
INFO - root - 2017-12-11 03:59:38.096838: step 10210, loss = 0.70, batch loss = 0.64 (14.8 examples/sec; 0.542 sec/batch; 48h:29m:15s remains)
INFO - root - 2017-12-11 03:59:43.639130: step 10220, loss = 0.68, batch loss = 0.63 (13.4 examples/sec; 0.597 sec/batch; 53h:24m:39s remains)
INFO - root - 2017-12-11 03:59:49.197143: step 10230, loss = 0.71, batch loss = 0.66 (14.0 examples/sec; 0.570 sec/batch; 51h:03m:51s remains)
INFO - root - 2017-12-11 03:59:54.711077: step 10240, loss = 0.71, batch loss = 0.65 (14.6 examples/sec; 0.549 sec/batch; 49h:07m:35s remains)
INFO - root - 2017-12-11 03:59:59.891490: step 10250, loss = 0.69, batch loss = 0.63 (13.8 examples/sec; 0.581 sec/batch; 52h:01m:31s remains)
INFO - root - 2017-12-11 04:00:05.402793: step 10260, loss = 0.70, batch loss = 0.64 (14.8 examples/sec; 0.542 sec/batch; 48h:31m:25s remains)
INFO - root - 2017-12-11 04:00:10.923890: step 10270, loss = 0.70, batch loss = 0.64 (15.2 examples/sec; 0.528 sec/batch; 47h:15m:37s remains)
INFO - root - 2017-12-11 04:00:16.511966: step 10280, loss = 0.69, batch loss = 0.63 (14.6 examples/sec; 0.549 sec/batch; 49h:06m:28s remains)
INFO - root - 2017-12-11 04:00:21.956876: step 10290, loss = 0.71, batch loss = 0.65 (14.9 examples/sec; 0.536 sec/batch; 47h:55m:52s remains)
INFO - root - 2017-12-11 04:00:27.389999: step 10300, loss = 0.69, batch loss = 0.63 (14.9 examples/sec; 0.538 sec/batch; 48h:09m:51s remains)
2017-12-11 04:00:28.007450: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0896586 -0.0917565 -0.085616045 -0.0735212 -0.058069147 -0.04535377 -0.041991398 -0.052635297 -0.074682526 -0.10027283 -0.12098947 -0.13138145 -0.13094093 -0.12278406 -0.11236838][-0.090975568 -0.087266132 -0.069440559 -0.040141068 -0.0042573512 0.027664293 0.042815737 0.03207735 -0.0015983926 -0.046503589 -0.089525484 -0.12041735 -0.13411584 -0.13230257 -0.12262218][-0.091035761 -0.0777662 -0.04293735 0.011042153 0.0776929 0.14065933 0.17898655 0.17668575 0.13442215 0.065903 -0.010126271 -0.074578419 -0.11389618 -0.12601022 -0.12176432][-0.086366728 -0.06014109 -0.00525486 0.077509686 0.18158264 0.2841098 0.3550404 0.36968291 0.32379976 0.23038453 0.11371817 0.0045932145 -0.071491189 -0.10628674 -0.11333612][-0.06387604 -0.023964303 0.048813973 0.15711018 0.29608411 0.43756458 0.54283291 0.57758188 0.53102452 0.41468251 0.25815347 0.10273997 -0.014377313 -0.0771324 -0.099981889][-0.00930304 0.038023647 0.1190475 0.242084 0.40522182 0.57726014 0.71197158 0.76494658 0.71821916 0.58273059 0.39299917 0.19751963 0.042955577 -0.047169503 -0.086496115][0.07535471 0.1209736 0.19803412 0.32164866 0.49265742 0.67946917 0.83098572 0.89515984 0.8475256 0.69824672 0.48538843 0.26170883 0.080480792 -0.029545747 -0.080410115][0.17885184 0.21172392 0.26945215 0.37490442 0.53193408 0.71155095 0.86139143 0.92635691 0.87778336 0.72313392 0.50146192 0.26709023 0.076714478 -0.038788743 -0.090582125][0.27597433 0.28249836 0.30428308 0.37245405 0.49441066 0.64654958 0.7789802 0.83787477 0.79313552 0.64814532 0.4392983 0.21819697 0.040045291 -0.065613769 -0.10883445][0.32708564 0.29813698 0.27600503 0.29847676 0.37626144 0.49203813 0.60084361 0.65313894 0.61938506 0.49872431 0.32254168 0.13628273 -0.011336884 -0.095440872 -0.12471849][0.30189016 0.2411443 0.18288791 0.16670701 0.20508495 0.28568384 0.37007719 0.41619566 0.39751026 0.30986372 0.17804822 0.039595347 -0.066580035 -0.12240246 -0.13529837][0.20418452 0.12868065 0.055609025 0.018810185 0.0298153 0.080069415 0.1397862 0.17751609 0.17262736 0.11932729 0.035051953 -0.051861294 -0.11419062 -0.14120108 -0.13922183][0.078107662 0.0071226084 -0.059716273 -0.099813983 -0.10408057 -0.077721581 -0.040736649 -0.013138339 -0.0086104982 -0.032696657 -0.07487908 -0.11667695 -0.1422047 -0.14680769 -0.1354599][-0.034722745 -0.087884508 -0.1352665 -0.16644794 -0.17550366 -0.16495588 -0.14571498 -0.12794644 -0.1190925 -0.12323629 -0.13535996 -0.14583918 -0.14752267 -0.13972934 -0.12570749][-0.11019913 -0.14129017 -0.16537102 -0.18223505 -0.18908052 -0.18679596 -0.17972302 -0.17099006 -0.16326489 -0.15872261 -0.1556493 -0.15042078 -0.14105521 -0.12891652 -0.11600659]]...]
INFO - root - 2017-12-11 04:00:33.537506: step 10310, loss = 0.70, batch loss = 0.64 (14.8 examples/sec; 0.542 sec/batch; 48h:31m:52s remains)
INFO - root - 2017-12-11 04:00:38.998012: step 10320, loss = 0.70, batch loss = 0.65 (14.4 examples/sec; 0.556 sec/batch; 49h:46m:18s remains)
INFO - root - 2017-12-11 04:00:44.567892: step 10330, loss = 0.71, batch loss = 0.65 (14.5 examples/sec; 0.553 sec/batch; 49h:30m:19s remains)
INFO - root - 2017-12-11 04:00:50.064201: step 10340, loss = 0.70, batch loss = 0.64 (14.6 examples/sec; 0.548 sec/batch; 49h:01m:51s remains)
INFO - root - 2017-12-11 04:00:55.281193: step 10350, loss = 0.70, batch loss = 0.64 (14.6 examples/sec; 0.546 sec/batch; 48h:52m:32s remains)
INFO - root - 2017-12-11 04:01:00.782758: step 10360, loss = 0.70, batch loss = 0.64 (14.5 examples/sec; 0.551 sec/batch; 49h:16m:02s remains)
INFO - root - 2017-12-11 04:01:06.276275: step 10370, loss = 0.69, batch loss = 0.64 (14.5 examples/sec; 0.552 sec/batch; 49h:20m:57s remains)
INFO - root - 2017-12-11 04:01:11.739304: step 10380, loss = 0.71, batch loss = 0.65 (14.4 examples/sec; 0.554 sec/batch; 49h:36m:10s remains)
INFO - root - 2017-12-11 04:01:17.292285: step 10390, loss = 0.68, batch loss = 0.62 (14.3 examples/sec; 0.560 sec/batch; 50h:05m:41s remains)
INFO - root - 2017-12-11 04:01:22.731670: step 10400, loss = 0.70, batch loss = 0.64 (14.7 examples/sec; 0.545 sec/batch; 48h:43m:28s remains)
2017-12-11 04:01:23.349710: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0068885414 -0.000797659 2.2554398e-06 -0.0041393167 -0.0098081976 -0.015559045 -0.020748245 -0.024637993 -0.025745127 -0.022767521 -0.017664494 -0.013928014 -0.013958934 -0.018031191 -0.024613392][0.035872743 0.053895544 0.06074477 0.056713037 0.047999736 0.036465798 0.023876537 0.012957048 0.007767308 0.011862733 0.020915827 0.028099911 0.028570261 0.021808146 0.010164865][0.096363574 0.13292386 0.15062131 0.14936659 0.13873407 0.12085154 0.098699011 0.077549569 0.065437689 0.0695413 0.082899757 0.094607212 0.096344441 0.086782962 0.068922892][0.16187635 0.21992485 0.25187379 0.25701159 0.24807097 0.22675474 0.19655547 0.1649704 0.1443435 0.14656369 0.16252561 0.17764969 0.17996788 0.16725907 0.1427395][0.21182311 0.28934732 0.33670396 0.3521581 0.35055435 0.3315641 0.29809216 0.25867248 0.22955357 0.22738127 0.24149711 0.25547916 0.25531441 0.23847142 0.20782039][0.22686201 0.3168571 0.37737384 0.40542629 0.41607735 0.40561 0.37513062 0.33302429 0.2984463 0.29156429 0.3004148 0.30857721 0.30226874 0.28010085 0.24443899][0.21193571 0.30655774 0.37603694 0.41578788 0.43944177 0.43982497 0.41614676 0.37631992 0.34082308 0.33067349 0.33302492 0.3326776 0.31777266 0.28932393 0.24992137][0.18239853 0.274598 0.34732249 0.39450616 0.427645 0.43779188 0.4229039 0.39000666 0.35883254 0.34855533 0.34677404 0.33900985 0.31517842 0.27904743 0.23519114][0.1519264 0.2370315 0.30778676 0.35718691 0.39470389 0.411396 0.40511373 0.38215473 0.36006007 0.35523951 0.35658514 0.34840509 0.31995702 0.27656579 0.22623186][0.13135603 0.20962919 0.27667817 0.32492933 0.36202851 0.38028559 0.37843576 0.36324796 0.35070002 0.35500443 0.36693513 0.36737064 0.34168854 0.29438475 0.23715465][0.12256166 0.19454917 0.25634375 0.30053097 0.33348444 0.34953731 0.34881428 0.33886611 0.33446604 0.34872669 0.3748138 0.38934255 0.37232596 0.32573256 0.26432177][0.11644491 0.17981125 0.23278703 0.26896426 0.29465088 0.30709496 0.30732343 0.302317 0.30497634 0.32781857 0.36652151 0.39475772 0.38860151 0.34731278 0.28711757][0.095526613 0.1458343 0.18633121 0.21186624 0.22909227 0.238209 0.24028929 0.24036899 0.24844688 0.27612245 0.32094693 0.35674894 0.3594819 0.32672104 0.27387694][0.058162715 0.093459114 0.12091351 0.13599658 0.1452134 0.15043607 0.15257093 0.15499145 0.16461183 0.19114333 0.23266219 0.26695466 0.27389356 0.25057757 0.20964812][0.020638864 0.043338027 0.060479786 0.06767679 0.070179671 0.070417926 0.069480084 0.069915712 0.076054417 0.0949567 0.12473684 0.14960471 0.15559387 0.1402411 0.11253857]]...]
INFO - root - 2017-12-11 04:01:28.934328: step 10410, loss = 0.69, batch loss = 0.63 (14.5 examples/sec; 0.550 sec/batch; 49h:14m:44s remains)
INFO - root - 2017-12-11 04:01:34.374512: step 10420, loss = 0.69, batch loss = 0.64 (14.4 examples/sec; 0.556 sec/batch; 49h:45m:45s remains)
INFO - root - 2017-12-11 04:01:39.849096: step 10430, loss = 0.70, batch loss = 0.64 (14.4 examples/sec; 0.557 sec/batch; 49h:48m:59s remains)
INFO - root - 2017-12-11 04:01:45.453204: step 10440, loss = 0.70, batch loss = 0.65 (14.1 examples/sec; 0.566 sec/batch; 50h:39m:47s remains)
INFO - root - 2017-12-11 04:01:50.592735: step 10450, loss = 0.70, batch loss = 0.65 (14.1 examples/sec; 0.569 sec/batch; 50h:56m:12s remains)
INFO - root - 2017-12-11 04:01:56.113230: step 10460, loss = 0.70, batch loss = 0.64 (14.2 examples/sec; 0.562 sec/batch; 50h:13m:55s remains)
INFO - root - 2017-12-11 04:02:01.655760: step 10470, loss = 0.68, batch loss = 0.62 (13.8 examples/sec; 0.581 sec/batch; 51h:58m:32s remains)
INFO - root - 2017-12-11 04:02:07.127170: step 10480, loss = 0.71, batch loss = 0.65 (14.8 examples/sec; 0.542 sec/batch; 48h:26m:14s remains)
INFO - root - 2017-12-11 04:02:12.631168: step 10490, loss = 0.71, batch loss = 0.66 (14.6 examples/sec; 0.547 sec/batch; 48h:56m:28s remains)
INFO - root - 2017-12-11 04:02:18.196598: step 10500, loss = 0.71, batch loss = 0.65 (14.5 examples/sec; 0.552 sec/batch; 49h:20m:13s remains)
2017-12-11 04:02:18.808646: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.012680939 0.017741075 0.020583574 0.021398481 0.020257855 0.01566809 0.0095300255 0.003452532 0.0010145722 0.00077914906 0.0040281243 0.018028833 0.045602165 0.084227622 0.12064033][0.0048021968 0.007958916 0.0091767358 0.0085663972 0.0072987131 0.0046279375 0.0014129983 -0.0021805898 -0.0031637594 -0.00202792 0.0026232586 0.01702646 0.044158373 0.0824475 0.11926186][0.00868361 0.011371255 0.011936262 0.010624461 0.0097283395 0.0093129547 0.0090999324 0.0076018106 0.0070121065 0.0079257954 0.011650308 0.023778303 0.04691473 0.079835914 0.11193904][0.027458189 0.032331284 0.034003478 0.033315111 0.033294771 0.034599658 0.036565814 0.036535092 0.035698291 0.035096589 0.035976674 0.044099651 0.0613318 0.086460933 0.11114524][0.05838621 0.067058571 0.070641205 0.071551107 0.072816268 0.074964784 0.077861317 0.078669213 0.077477276 0.074873462 0.07181368 0.075173579 0.08618018 0.10318606 0.1194209][0.095255882 0.10786083 0.11350702 0.11688894 0.1194515 0.12114348 0.12315641 0.12368882 0.12187558 0.11734199 0.11067338 0.11011307 0.1162855 0.12686323 0.13573191][0.13083789 0.14648134 0.15379742 0.15989102 0.16316761 0.1627249 0.16171452 0.16038807 0.15733312 0.15127788 0.14241377 0.13998014 0.14344135 0.14991257 0.15352796][0.1587451 0.17570247 0.18343484 0.19137031 0.19384937 0.18932463 0.18362118 0.1788971 0.1737902 0.16612305 0.15617348 0.15362157 0.15622412 0.16062364 0.16137062][0.1733757 0.18927732 0.19611278 0.20424666 0.20412625 0.19411595 0.18247916 0.17339754 0.16565244 0.15610336 0.14570834 0.14397281 0.14758208 0.15229386 0.15327944][0.16519129 0.17839849 0.18373305 0.19126728 0.18863499 0.17444888 0.15810718 0.14555988 0.13599519 0.12542289 0.11510985 0.11379815 0.11893266 0.12582803 0.13019779][0.13145386 0.14089732 0.1451001 0.15239403 0.14970009 0.13627496 0.12069811 0.10930509 0.10156707 0.092540413 0.083372869 0.081781425 0.086964294 0.095566511 0.10405061][0.079680793 0.085171379 0.089324735 0.097707562 0.099207185 0.0936147 0.087028839 0.083924837 0.082954071 0.078122556 0.07089977 0.068422578 0.071992189 0.08043655 0.091226719][0.026374577 0.029056013 0.034808442 0.046454556 0.056179855 0.064145721 0.073001422 0.083789036 0.09281873 0.093071193 0.087344259 0.083029777 0.083663374 0.090009779 0.10058958][-0.012717987 -0.010452185 -0.001274561 0.015983898 0.036445569 0.059975084 0.085423537 0.11040999 0.12858173 0.13267456 0.12702394 0.11977 0.1164076 0.11895322 0.12682721][-0.03011339 -0.026770538 -0.013880944 0.0095760943 0.040237669 0.076975241 0.11521246 0.15025476 0.17382795 0.17925003 0.17184339 0.15995482 0.15098239 0.14790002 0.15094133]]...]
INFO - root - 2017-12-11 04:02:24.367935: step 10510, loss = 0.71, batch loss = 0.65 (14.6 examples/sec; 0.549 sec/batch; 49h:06m:38s remains)
INFO - root - 2017-12-11 04:02:29.884126: step 10520, loss = 0.67, batch loss = 0.62 (14.5 examples/sec; 0.553 sec/batch; 49h:26m:46s remains)
INFO - root - 2017-12-11 04:02:35.328980: step 10530, loss = 0.69, batch loss = 0.64 (14.8 examples/sec; 0.541 sec/batch; 48h:23m:48s remains)
INFO - root - 2017-12-11 04:02:40.426450: step 10540, loss = 0.71, batch loss = 0.65 (14.5 examples/sec; 0.552 sec/batch; 49h:19m:23s remains)
INFO - root - 2017-12-11 04:02:45.916416: step 10550, loss = 0.70, batch loss = 0.64 (15.1 examples/sec; 0.529 sec/batch; 47h:16m:09s remains)
INFO - root - 2017-12-11 04:02:51.444348: step 10560, loss = 0.69, batch loss = 0.63 (14.8 examples/sec; 0.540 sec/batch; 48h:14m:55s remains)
INFO - root - 2017-12-11 04:02:57.031369: step 10570, loss = 0.70, batch loss = 0.64 (13.2 examples/sec; 0.607 sec/batch; 54h:17m:24s remains)
INFO - root - 2017-12-11 04:03:02.598384: step 10580, loss = 0.70, batch loss = 0.64 (14.7 examples/sec; 0.544 sec/batch; 48h:37m:59s remains)
INFO - root - 2017-12-11 04:03:08.056599: step 10590, loss = 0.68, batch loss = 0.62 (14.8 examples/sec; 0.539 sec/batch; 48h:11m:51s remains)
INFO - root - 2017-12-11 04:03:13.538179: step 10600, loss = 0.69, batch loss = 0.63 (14.5 examples/sec; 0.551 sec/batch; 49h:14m:58s remains)
2017-12-11 04:03:14.115908: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.28061163 0.40997097 0.51583719 0.57981116 0.60539311 0.60705632 0.60128665 0.58887947 0.55466604 0.4840391 0.38259846 0.28818738 0.24373394 0.26409841 0.32104456][0.24825297 0.37114862 0.47282916 0.53781891 0.57242507 0.59148246 0.60668254 0.60832095 0.57396924 0.49177253 0.37382302 0.26049197 0.19586329 0.19792813 0.244039][0.17885375 0.28196681 0.37053576 0.43296334 0.47605744 0.51180214 0.54470766 0.55694324 0.52336562 0.43735361 0.31757966 0.20241714 0.13115019 0.12361273 0.16612382][0.10612419 0.18652757 0.25976533 0.31838718 0.3670876 0.41286659 0.4541651 0.47179726 0.44376081 0.36762336 0.26357803 0.16272306 0.09728168 0.089286923 0.13547722][0.052381549 0.11441198 0.17624305 0.23298475 0.2852684 0.33445841 0.37605494 0.39526185 0.37610343 0.31798074 0.23727888 0.15480171 0.095861286 0.086559713 0.13458641][0.041052204 0.10106708 0.16543394 0.22871837 0.28579012 0.33318472 0.36713937 0.38110822 0.36604035 0.3205725 0.25456 0.17976916 0.11745385 0.098662935 0.14034836][0.077530593 0.15272558 0.23461951 0.3147912 0.38232654 0.43042302 0.45753953 0.46438861 0.44613156 0.39818886 0.32566983 0.23723283 0.15553831 0.11631138 0.14261179][0.13864811 0.23725285 0.34350637 0.44503659 0.5263254 0.57914776 0.60461342 0.60765821 0.58289397 0.51955974 0.4212507 0.30125993 0.19040295 0.12916052 0.14282656][0.18691774 0.30404389 0.42733887 0.53992414 0.62464291 0.67488188 0.69649684 0.69690043 0.66761541 0.59151077 0.47192919 0.329268 0.20291632 0.13614619 0.1535877][0.19466144 0.31582528 0.44013682 0.5472315 0.62075752 0.65593559 0.66349351 0.65378952 0.6187225 0.54001951 0.42150742 0.28688991 0.17740424 0.13409838 0.17744531][0.16301072 0.2695334 0.37670848 0.46459809 0.52006131 0.5389353 0.53231722 0.51107329 0.46996915 0.39630142 0.297257 0.19710337 0.13113831 0.13174067 0.21106425][0.11290857 0.19048518 0.26681364 0.32809484 0.36673912 0.37823874 0.36839625 0.34411478 0.30215797 0.23819242 0.16471779 0.10546431 0.086128928 0.12778258 0.23418424][0.066712677 0.11280307 0.15616202 0.19193502 0.21844129 0.23038642 0.22600913 0.20601314 0.16811728 0.11626559 0.067496069 0.042905334 0.057688586 0.12235107 0.23555174][0.045706652 0.070662543 0.091642849 0.1103624 0.12881757 0.14114685 0.14098348 0.1255075 0.093952969 0.055663753 0.028105866 0.026955876 0.0585582 0.12564585 0.22426935][0.057963848 0.079150751 0.095296904 0.11016907 0.12598559 0.13617313 0.13380006 0.11679152 0.0873209 0.058045354 0.043723036 0.053565197 0.08705534 0.14128549 0.21084961]]...]
INFO - root - 2017-12-11 04:03:19.578833: step 10610, loss = 0.69, batch loss = 0.63 (15.1 examples/sec; 0.528 sec/batch; 47h:13m:48s remains)
INFO - root - 2017-12-11 04:03:25.062347: step 10620, loss = 0.69, batch loss = 0.63 (14.9 examples/sec; 0.538 sec/batch; 48h:05m:54s remains)
INFO - root - 2017-12-11 04:03:30.518462: step 10630, loss = 0.70, batch loss = 0.64 (14.5 examples/sec; 0.551 sec/batch; 49h:15m:04s remains)
INFO - root - 2017-12-11 04:03:35.688227: step 10640, loss = 0.67, batch loss = 0.62 (14.6 examples/sec; 0.549 sec/batch; 49h:04m:52s remains)
INFO - root - 2017-12-11 04:03:41.241914: step 10650, loss = 0.70, batch loss = 0.64 (14.7 examples/sec; 0.545 sec/batch; 48h:44m:33s remains)
INFO - root - 2017-12-11 04:03:46.742658: step 10660, loss = 0.70, batch loss = 0.64 (14.2 examples/sec; 0.563 sec/batch; 50h:20m:27s remains)
INFO - root - 2017-12-11 04:03:52.199136: step 10670, loss = 0.70, batch loss = 0.65 (14.5 examples/sec; 0.553 sec/batch; 49h:24m:29s remains)
INFO - root - 2017-12-11 04:03:57.743647: step 10680, loss = 0.71, batch loss = 0.65 (14.2 examples/sec; 0.562 sec/batch; 50h:14m:36s remains)
INFO - root - 2017-12-11 04:04:03.367139: step 10690, loss = 0.70, batch loss = 0.64 (14.4 examples/sec; 0.554 sec/batch; 49h:29m:58s remains)
INFO - root - 2017-12-11 04:04:08.834424: step 10700, loss = 0.70, batch loss = 0.64 (14.6 examples/sec; 0.547 sec/batch; 48h:54m:57s remains)
2017-12-11 04:04:09.487944: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.091676533 -0.054255947 0.010712105 0.090944849 0.16619839 0.22138764 0.2516343 0.2620846 0.24299192 0.18867244 0.11928411 0.053110708 0.010154316 0.01016208 0.052791428][-0.091277368 -0.047150165 0.034382522 0.14158134 0.25317505 0.34857631 0.41602552 0.44903749 0.42642114 0.34380257 0.23490991 0.13977544 0.090908818 0.10583103 0.17196172][-0.091243289 -0.041769505 0.05354153 0.18507905 0.33166358 0.46765018 0.5731135 0.62656546 0.59717405 0.48602796 0.34253055 0.22988081 0.18790673 0.22890043 0.32795593][-0.086908333 -0.032285385 0.0759078 0.22837925 0.40386853 0.57223219 0.70533693 0.76840311 0.72705728 0.59145975 0.42546663 0.30920169 0.28488842 0.35903859 0.49582735][-0.07782273 -0.016233185 0.10523637 0.27647412 0.4755325 0.66703117 0.81543106 0.87698078 0.82055438 0.66698629 0.48998597 0.37846962 0.372805 0.47689465 0.647871][-0.070647188 -0.0027517397 0.12893274 0.3138293 0.52936989 0.7355817 0.89245772 0.95278972 0.89139086 0.73702908 0.56385088 0.45934367 0.45895565 0.57004666 0.75184047][-0.066039979 0.0088600162 0.15149626 0.34997967 0.57952791 0.79645586 0.95902717 1.0209084 0.9631176 0.81954443 0.65890127 0.55955917 0.55131829 0.64841938 0.8192448][-0.062108494 0.021401094 0.17600153 0.38874885 0.63093412 0.85580826 1.0207937 1.0825176 1.0279268 0.89424443 0.74476218 0.64680672 0.62660217 0.70267487 0.85075057][-0.058166567 0.032083619 0.19558741 0.41737247 0.66518974 0.89052606 1.0508542 1.1065925 1.0505745 0.9219476 0.7794674 0.68017006 0.64839661 0.70159495 0.81955343][-0.058290683 0.034356311 0.20015003 0.42118371 0.66189229 0.87296778 1.014676 1.053902 0.99018937 0.86360973 0.72783744 0.62939847 0.58868605 0.61812627 0.70132309][-0.066638581 0.021067949 0.1776114 0.38241836 0.59784085 0.7759 0.88273615 0.89574516 0.82146084 0.7004568 0.57883489 0.49043253 0.44866639 0.4584291 0.50728446][-0.081714541 -0.0089513632 0.12366788 0.2937142 0.46470919 0.59327984 0.65381706 0.63609535 0.5523954 0.44260889 0.34438017 0.27924657 0.25155371 0.25676343 0.28403455][-0.10455985 -0.057126969 0.035342146 0.15213996 0.26310959 0.33468667 0.35085991 0.31176537 0.23131339 0.14398651 0.077314578 0.042078692 0.034165416 0.042088777 0.056888163][-0.12922762 -0.11157726 -0.064522907 -0.0049686073 0.046762142 0.069658324 0.05669545 0.013501168 -0.048907571 -0.10657378 -0.1429376 -0.15512146 -0.15136033 -0.14365147 -0.13685147][-0.14842969 -0.15585344 -0.14635623 -0.13260448 -0.12510365 -0.13377573 -0.159418 -0.19548462 -0.2354454 -0.26631728 -0.2806989 -0.28004968 -0.2722955 -0.26561013 -0.26095119]]...]
INFO - root - 2017-12-11 04:04:14.928940: step 10710, loss = 0.72, batch loss = 0.66 (14.6 examples/sec; 0.548 sec/batch; 49h:00m:40s remains)
INFO - root - 2017-12-11 04:04:20.472370: step 10720, loss = 0.68, batch loss = 0.62 (14.3 examples/sec; 0.559 sec/batch; 49h:55m:19s remains)
INFO - root - 2017-12-11 04:04:25.969757: step 10730, loss = 0.69, batch loss = 0.63 (14.6 examples/sec; 0.549 sec/batch; 49h:03m:59s remains)
INFO - root - 2017-12-11 04:04:31.168355: step 10740, loss = 0.69, batch loss = 0.63 (14.3 examples/sec; 0.558 sec/batch; 49h:50m:44s remains)
INFO - root - 2017-12-11 04:04:36.657394: step 10750, loss = 0.69, batch loss = 0.63 (14.4 examples/sec; 0.556 sec/batch; 49h:40m:46s remains)
INFO - root - 2017-12-11 04:04:42.154745: step 10760, loss = 0.70, batch loss = 0.65 (14.5 examples/sec; 0.553 sec/batch; 49h:27m:26s remains)
INFO - root - 2017-12-11 04:04:47.672072: step 10770, loss = 0.68, batch loss = 0.62 (14.3 examples/sec; 0.560 sec/batch; 50h:02m:01s remains)
INFO - root - 2017-12-11 04:04:53.175653: step 10780, loss = 0.69, batch loss = 0.63 (14.0 examples/sec; 0.571 sec/batch; 51h:01m:55s remains)
INFO - root - 2017-12-11 04:04:58.599779: step 10790, loss = 0.70, batch loss = 0.64 (14.7 examples/sec; 0.544 sec/batch; 48h:34m:52s remains)
INFO - root - 2017-12-11 04:05:04.060140: step 10800, loss = 0.70, batch loss = 0.64 (15.3 examples/sec; 0.524 sec/batch; 46h:47m:10s remains)
2017-12-11 04:05:04.633845: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.0054953769 0.016624497 0.029334359 0.044247083 0.059657473 0.072649345 0.085128941 0.099720493 0.11569196 0.13142082 0.14995585 0.17838404 0.21986841 0.276685 0.34000814][0.042980578 0.054932117 0.0718557 0.095351048 0.12104201 0.14297652 0.16017656 0.17395757 0.18265609 0.18750721 0.19679107 0.22088157 0.2631374 0.3226659 0.38760224][0.075858265 0.0876428 0.11043859 0.1467896 0.18796989 0.2232345 0.24785259 0.26106197 0.25886267 0.24556479 0.23737922 0.24971935 0.28521022 0.33776921 0.39359969][0.09712176 0.11070788 0.14338523 0.1981644 0.26004669 0.31242427 0.34684709 0.36101565 0.34815016 0.31548646 0.287915 0.28584629 0.30928737 0.34607929 0.38248402][0.10101177 0.12260134 0.17227934 0.25126785 0.33785379 0.40983886 0.45491743 0.47031185 0.4484683 0.40030971 0.35651582 0.33832878 0.34179634 0.35116127 0.35657406][0.096382052 0.13254702 0.20230745 0.30329889 0.40973184 0.49578312 0.54557073 0.55786061 0.52966434 0.47577816 0.42527 0.39337519 0.3721036 0.34693715 0.315587][0.10024572 0.15096265 0.23521914 0.34695625 0.45959687 0.54672122 0.59121156 0.59659207 0.56812835 0.5219844 0.47760388 0.43858492 0.39389613 0.33438754 0.26734719][0.11891396 0.17611735 0.26125929 0.36617133 0.46595669 0.53753984 0.56715733 0.56600893 0.54738045 0.52323413 0.49730036 0.46023417 0.40055147 0.31607759 0.22199337][0.1429162 0.19400874 0.26560783 0.34886175 0.42119724 0.46625727 0.47808963 0.47529197 0.47407889 0.47754633 0.47401461 0.4453811 0.38285244 0.28953055 0.18417747][0.16373834 0.19972251 0.24931322 0.30436611 0.34581837 0.36530989 0.36468133 0.36566108 0.38249356 0.40854511 0.42179072 0.40219009 0.34618893 0.25872728 0.15747952][0.18432601 0.20336616 0.23010181 0.25885031 0.27487457 0.27645269 0.27042243 0.27691528 0.30559248 0.34306386 0.36424085 0.35188556 0.30631498 0.23161475 0.14284702][0.20728485 0.2154727 0.22671215 0.23884907 0.24089478 0.23412395 0.22579047 0.23242164 0.2605727 0.29585725 0.31558338 0.30604434 0.26849422 0.20537694 0.12976269][0.23759194 0.24512942 0.25086546 0.2554723 0.25056618 0.23753428 0.22190347 0.21731587 0.23078218 0.25205895 0.26328072 0.2526229 0.21997012 0.16645898 0.10368302][0.28000936 0.29312739 0.29689705 0.29470626 0.28136849 0.25794786 0.22813012 0.20434245 0.19610302 0.19835478 0.1974058 0.18238859 0.15202935 0.10736852 0.058058031][0.33957765 0.35718122 0.35395393 0.33729589 0.30745924 0.26731592 0.22006959 0.17760579 0.15078659 0.13706186 0.1252832 0.10599685 0.078226194 0.0433633 0.0082460409]]...]
INFO - root - 2017-12-11 04:05:10.139154: step 10810, loss = 0.71, batch loss = 0.65 (14.9 examples/sec; 0.538 sec/batch; 48h:05m:42s remains)
INFO - root - 2017-12-11 04:05:15.740314: step 10820, loss = 0.69, batch loss = 0.63 (14.3 examples/sec; 0.558 sec/batch; 49h:54m:16s remains)
INFO - root - 2017-12-11 04:05:20.956935: step 10830, loss = 0.69, batch loss = 0.63 (30.7 examples/sec; 0.260 sec/batch; 23h:15m:15s remains)
INFO - root - 2017-12-11 04:05:26.380643: step 10840, loss = 0.71, batch loss = 0.65 (14.5 examples/sec; 0.553 sec/batch; 49h:26m:16s remains)
INFO - root - 2017-12-11 04:05:31.983252: step 10850, loss = 0.71, batch loss = 0.65 (14.3 examples/sec; 0.559 sec/batch; 49h:54m:20s remains)
INFO - root - 2017-12-11 04:05:37.497793: step 10860, loss = 0.69, batch loss = 0.63 (14.9 examples/sec; 0.538 sec/batch; 48h:03m:00s remains)
INFO - root - 2017-12-11 04:05:43.076128: step 10870, loss = 0.71, batch loss = 0.65 (14.3 examples/sec; 0.559 sec/batch; 49h:56m:05s remains)
INFO - root - 2017-12-11 04:05:48.559998: step 10880, loss = 0.70, batch loss = 0.64 (15.1 examples/sec; 0.528 sec/batch; 47h:12m:52s remains)
INFO - root - 2017-12-11 04:05:54.031679: step 10890, loss = 0.70, batch loss = 0.64 (14.3 examples/sec; 0.559 sec/batch; 49h:55m:34s remains)
INFO - root - 2017-12-11 04:05:59.562238: step 10900, loss = 0.71, batch loss = 0.66 (13.7 examples/sec; 0.583 sec/batch; 52h:04m:43s remains)
2017-12-11 04:06:00.147314: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.21883175 0.22221638 0.21200104 0.1891553 0.15609697 0.12081226 0.092978 0.08619573 0.10609429 0.15137842 0.20653425 0.25124985 0.26676109 0.24322066 0.18240713][0.29557011 0.30726996 0.30456594 0.28735781 0.25467563 0.2113837 0.166944 0.13911417 0.14063206 0.17662524 0.23177704 0.28097102 0.29900059 0.27218789 0.20294489][0.34818265 0.36959541 0.37992561 0.37931344 0.36218014 0.32574055 0.2740013 0.22475867 0.19919334 0.21321253 0.2561579 0.30090094 0.31819826 0.2907027 0.21874055][0.36493519 0.39275295 0.41576868 0.43565163 0.44302368 0.42666864 0.38230354 0.32329157 0.27662441 0.26769865 0.29160109 0.32254517 0.331276 0.29998451 0.22730768][0.36494911 0.39396897 0.42137879 0.45229384 0.47803178 0.48444992 0.46104333 0.41345766 0.36682349 0.34776565 0.35360962 0.36158106 0.34798419 0.30136722 0.2230337][0.36072645 0.38501456 0.40779847 0.43848407 0.47316146 0.50079018 0.5074625 0.48906997 0.46131998 0.44353676 0.43159071 0.40742105 0.35904831 0.2873925 0.19944745][0.34777692 0.36303616 0.3753868 0.3990947 0.43585569 0.481192 0.52003407 0.53815752 0.53719217 0.524501 0.49362829 0.43338528 0.34743434 0.25102854 0.15659425][0.33724841 0.33736458 0.33287588 0.3420276 0.37191048 0.42477772 0.48639637 0.5347113 0.55787379 0.5512647 0.506685 0.42022395 0.30911908 0.19936888 0.10588981][0.34149817 0.32460564 0.29829898 0.28457564 0.29544768 0.33976552 0.40671462 0.47100833 0.5108068 0.51174867 0.46432027 0.37013486 0.25362667 0.14505073 0.059298951][0.35938179 0.33172682 0.2886557 0.25342533 0.24117573 0.26590878 0.32164803 0.38422954 0.42709446 0.43100309 0.38691238 0.29952914 0.19377163 0.09793824 0.024534348][0.36695021 0.34017923 0.29426667 0.25104591 0.22531657 0.23257054 0.26954952 0.31553066 0.34577039 0.3427093 0.3008357 0.22644551 0.14062662 0.064723313 0.006104378][0.36222377 0.34248772 0.3036918 0.2642715 0.23710747 0.23538727 0.25567865 0.28089777 0.29256397 0.27888227 0.23885959 0.17960764 0.11687465 0.063078083 0.018021485][0.36052969 0.3478736 0.31729081 0.28528208 0.263253 0.25989327 0.26990509 0.27907833 0.27619284 0.25578383 0.22052757 0.17750385 0.13699445 0.10237636 0.065318249][0.37606123 0.36975709 0.3444829 0.31807381 0.30145687 0.29925868 0.30382136 0.30268168 0.29046673 0.26672742 0.23656577 0.20646009 0.18231113 0.16016473 0.12500906][0.39558864 0.3945497 0.37278008 0.35062772 0.33802393 0.33643216 0.33625227 0.32638967 0.30625764 0.2798301 0.25471222 0.23625252 0.22555721 0.21200177 0.17560761]]...]
INFO - root - 2017-12-11 04:06:05.690249: step 10910, loss = 0.70, batch loss = 0.64 (14.3 examples/sec; 0.558 sec/batch; 49h:53m:15s remains)
INFO - root - 2017-12-11 04:06:11.245793: step 10920, loss = 0.71, batch loss = 0.65 (14.6 examples/sec; 0.549 sec/batch; 49h:04m:18s remains)
INFO - root - 2017-12-11 04:06:16.431885: step 10930, loss = 0.69, batch loss = 0.63 (14.1 examples/sec; 0.568 sec/batch; 50h:44m:34s remains)
INFO - root - 2017-12-11 04:06:21.984168: step 10940, loss = 0.68, batch loss = 0.63 (14.6 examples/sec; 0.547 sec/batch; 48h:53m:00s remains)
INFO - root - 2017-12-11 04:06:27.451278: step 10950, loss = 0.70, batch loss = 0.64 (14.5 examples/sec; 0.551 sec/batch; 49h:15m:15s remains)
INFO - root - 2017-12-11 04:06:33.023555: step 10960, loss = 0.70, batch loss = 0.64 (14.7 examples/sec; 0.545 sec/batch; 48h:41m:38s remains)
INFO - root - 2017-12-11 04:06:38.569920: step 10970, loss = 0.70, batch loss = 0.64 (14.2 examples/sec; 0.563 sec/batch; 50h:19m:06s remains)
INFO - root - 2017-12-11 04:06:44.017112: step 10980, loss = 0.70, batch loss = 0.64 (14.5 examples/sec; 0.553 sec/batch; 49h:25m:31s remains)
INFO - root - 2017-12-11 04:06:49.525680: step 10990, loss = 0.72, batch loss = 0.66 (14.5 examples/sec; 0.552 sec/batch; 49h:16m:47s remains)
INFO - root - 2017-12-11 04:06:55.067486: step 11000, loss = 0.70, batch loss = 0.64 (14.5 examples/sec; 0.552 sec/batch; 49h:19m:28s remains)
2017-12-11 04:06:55.590769: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0331882 -0.017766781 -0.007443931 -0.0060151075 -0.012300498 -0.023962852 -0.03768358 -0.048354402 -0.052811876 -0.052882832 -0.049548678 -0.043855485 -0.0361615 -0.0315534 -0.035007481][0.012001961 0.044578414 0.067238972 0.076125383 0.073324993 0.061149064 0.044941913 0.033720192 0.030235957 0.029816415 0.029922673 0.031247236 0.034903336 0.034642164 0.023885649][0.07874427 0.13799384 0.1819493 0.20599519 0.21183756 0.20105784 0.18252631 0.17089172 0.16757727 0.16404907 0.15540126 0.14595997 0.13965714 0.12993738 0.10835042][0.16038343 0.25598648 0.33104733 0.37820375 0.39810506 0.39455611 0.3818751 0.37721133 0.3762137 0.36392629 0.33512202 0.29918051 0.2668615 0.23473938 0.19453828][0.23833649 0.37139815 0.48028603 0.55329669 0.59098834 0.60507166 0.6160174 0.63538188 0.64567471 0.62227505 0.56265873 0.48262593 0.40437496 0.33238235 0.26305342][0.29228714 0.45303628 0.58838975 0.68262309 0.73812604 0.77884805 0.82860184 0.88494962 0.91231269 0.87774175 0.78554481 0.657899 0.52835 0.41138095 0.31100422][0.31626642 0.49047494 0.6401608 0.74653214 0.81424075 0.87933153 0.9642157 1.0500475 1.0880675 1.0427636 0.92676967 0.76581866 0.60006618 0.45235786 0.33263594][0.3079845 0.47837874 0.62626594 0.73071694 0.79937696 0.87556571 0.97539067 1.0677681 1.1029477 1.0510056 0.92974395 0.76233745 0.58850443 0.43537378 0.31544349][0.26739174 0.41710156 0.54696316 0.6360563 0.69437313 0.76600355 0.8586303 0.93720621 0.96190083 0.91326922 0.80934584 0.664881 0.51254708 0.378295 0.27457729][0.20070143 0.3189871 0.42044088 0.48577896 0.52564132 0.57889807 0.64584738 0.69654995 0.706723 0.66882414 0.596711 0.495365 0.38631198 0.2896474 0.21447974][0.12145819 0.20496799 0.27415136 0.31257761 0.32992336 0.35613546 0.38757393 0.40484932 0.39897555 0.37209329 0.33261842 0.27864987 0.22048947 0.16971567 0.12982252][0.046281692 0.099088155 0.14033192 0.15641806 0.15486339 0.1556727 0.15454113 0.14308387 0.12353959 0.10352619 0.086966649 0.069363512 0.053221017 0.041741192 0.03309609][-0.014985468 0.014188986 0.035069205 0.036992416 0.025148686 0.01115006 -0.0080725849 -0.033559114 -0.058332719 -0.073717669 -0.078362979 -0.076378085 -0.068863429 -0.058006588 -0.04804033][-0.05512768 -0.04174488 -0.03318556 -0.037310619 -0.051157672 -0.068683781 -0.091112688 -0.11650727 -0.13844596 -0.15006076 -0.15075566 -0.1430451 -0.12894757 -0.11222726 -0.096580893][-0.0733321 -0.069011234 -0.066592835 -0.071857288 -0.083411656 -0.0983793 -0.11657703 -0.1358206 -0.15206181 -0.16082166 -0.16157566 -0.15545097 -0.14418268 -0.13055168 -0.11667005]]...]
INFO - root - 2017-12-11 04:07:01.119439: step 11010, loss = 0.69, batch loss = 0.63 (14.6 examples/sec; 0.550 sec/batch; 49h:04m:26s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-SGD-lr0.001-clip50-initfromfixqian30k-fixqian
INFO - root - 2017-12-11 04:07:06.709853: step 11020, loss = 0.70, batch loss = 0.64 (14.8 examples/sec; 0.542 sec/batch; 48h:25m:13s remains)
INFO - root - 2017-12-11 04:07:11.934468: step 11030, loss = 0.70, batch loss = 0.64 (14.2 examples/sec; 0.563 sec/batch; 50h:15m:29s remains)
INFO - root - 2017-12-11 04:07:17.492263: step 11040, loss = 0.70, batch loss = 0.64 (14.7 examples/sec; 0.546 sec/batch; 48h:43m:30s remains)
INFO - root - 2017-12-11 04:07:22.970336: step 11050, loss = 0.69, batch loss = 0.63 (14.4 examples/sec; 0.554 sec/batch; 49h:29m:40s remains)
INFO - root - 2017-12-11 04:07:28.410301: step 11060, loss = 0.69, batch loss = 0.63 (14.9 examples/sec; 0.537 sec/batch; 47h:55m:50s remains)
INFO - root - 2017-12-11 04:07:33.957763: step 11070, loss = 0.69, batch loss = 0.63 (14.4 examples/sec; 0.556 sec/batch; 49h:40m:06s remains)
INFO - root - 2017-12-11 04:07:39.479646: step 11080, loss = 0.69, batch loss = 0.63 (14.3 examples/sec; 0.561 sec/batch; 50h:03m:43s remains)
INFO - root - 2017-12-11 04:07:44.930616: step 11090, loss = 0.71, batch loss = 0.66 (14.5 examples/sec; 0.551 sec/batch; 49h:12m:51s remains)
INFO - root - 2017-12-11 04:07:50.444821: step 11100, loss = 0.71, batch loss = 0.65 (14.5 examples/sec; 0.552 sec/batch; 49h:15m:11s remains)
2017-12-11 04:07:51.061295: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.23381479 0.24195524 0.23388088 0.20940913 0.17175397 0.13314691 0.099277966 0.071196295 0.050888088 0.041452579 0.045733295 0.055854741 0.075514577 0.10914325 0.15025154][0.22424422 0.22889109 0.21869145 0.19533958 0.16196583 0.12809283 0.098151051 0.072961643 0.054077912 0.044675358 0.048434984 0.057466503 0.076235875 0.11045095 0.15443431][0.19065602 0.19296083 0.18595257 0.17230257 0.15254746 0.1307258 0.10887817 0.088514991 0.071521468 0.061976757 0.064261474 0.071284555 0.0869215 0.11636665 0.15561956][0.15511866 0.15973337 0.16204296 0.16450343 0.16390629 0.15820085 0.14710341 0.13325319 0.11846811 0.1079412 0.10592684 0.10599472 0.11095881 0.12616004 0.15101047][0.13406327 0.14608777 0.16239463 0.18329602 0.20177281 0.21122374 0.20985568 0.20164707 0.18782273 0.1746859 0.16646071 0.15576395 0.14356042 0.13658863 0.13995798][0.12699935 0.14772144 0.17695555 0.21245673 0.24436694 0.26418233 0.26993832 0.266275 0.2532866 0.23822802 0.22564667 0.20622422 0.17800978 0.14913432 0.13104898][0.12891106 0.15609565 0.19369228 0.23802292 0.27747273 0.30324048 0.31354755 0.31286329 0.30046815 0.28425941 0.26905742 0.24446949 0.20532215 0.16038986 0.12612125][0.13907035 0.16580027 0.2044986 0.25172731 0.29349813 0.32129115 0.33366677 0.33400261 0.32043314 0.30162525 0.28323162 0.25637773 0.21311784 0.16106373 0.1188895][0.14736597 0.16634251 0.1976364 0.24078321 0.27984485 0.30707064 0.32133955 0.32430092 0.31227553 0.29314756 0.2730979 0.24641648 0.20488019 0.15432927 0.11229753][0.14995091 0.1564118 0.17435583 0.20637456 0.23702304 0.26072943 0.27683297 0.28502303 0.27979425 0.26631421 0.24966572 0.22664879 0.19096187 0.14777708 0.11201348][0.14175223 0.13771127 0.14295185 0.16182947 0.18075691 0.19839036 0.21554099 0.23001198 0.23469129 0.23158054 0.22312391 0.20700626 0.17925139 0.14599973 0.1192475][0.12839533 0.11958695 0.11936091 0.13033591 0.13964881 0.1506464 0.16649678 0.18422022 0.19622022 0.20222385 0.20223856 0.19365215 0.17392103 0.15028885 0.13195002][0.11883228 0.11188 0.11404257 0.12392304 0.12773386 0.13272895 0.14404637 0.15871124 0.17036355 0.17896733 0.18379845 0.18036851 0.16589443 0.14824122 0.13469236][0.1186973 0.12042783 0.13151579 0.14542408 0.14746054 0.14733353 0.1506229 0.15513352 0.1583702 0.16318741 0.16932534 0.16917261 0.15785515 0.14297433 0.13061365][0.1295639 0.14274246 0.1644147 0.18421015 0.18679649 0.18203454 0.17436208 0.16356467 0.15324253 0.15123357 0.15861297 0.16403058 0.15863313 0.1477937 0.13615903]]...]
INFO - root - 2017-12-11 04:07:56.634007: step 11110, loss = 0.70, batch loss = 0.65 (14.7 examples/sec; 0.545 sec/batch; 48h:38m:16s remains)
INFO - root - 2017-12-11 04:08:02.122736: step 11120, loss = 0.71, batch loss = 0.65 (14.7 examples/sec; 0.546 sec/batch; 48h:44m:47s remains)
INFO - root - 2017-12-11 04:08:07.403717: step 11130, loss = 0.69, batch loss = 0.64 (14.6 examples/sec; 0.549 sec/batch; 48h:58m:41s remains)
INFO - root - 2017-12-11 04:08:12.899800: step 11140, loss = 0.69, batch loss = 0.63 (14.9 examples/sec; 0.537 sec/batch; 47h:57m:05s remains)
INFO - root - 2017-12-11 04:08:18.426381: step 11150, loss = 0.69, batch loss = 0.63 (14.7 examples/sec; 0.543 sec/batch; 48h:26m:26s remains)
INFO - root - 2017-12-11 04:08:23.895319: step 11160, loss = 0.69, batch loss = 0.63 (14.5 examples/sec; 0.552 sec/batch; 49h:18m:37s remains)
INFO - root - 2017-12-11 04:08:29.372592: step 11170, loss = 0.71, batch loss = 0.65 (14.6 examples/sec; 0.548 sec/batch; 48h:54m:11s remains)
INFO - root - 2017-12-11 04:08:34.844500: step 11180, loss = 0.71, batch loss = 0.65 (14.6 examples/sec; 0.549 sec/batch; 49h:00m:56s remains)
INFO - root - 2017-12-11 04:08:40.371164: step 11190, loss = 0.70, batch loss = 0.64 (14.4 examples/sec; 0.554 sec/batch; 49h:28m:01s remains)
INFO - root - 2017-12-11 04:08:45.923249: step 11200, loss = 0.70, batch loss = 0.64 (14.4 examples/sec; 0.557 sec/batch; 49h:40m:35s remains)
2017-12-11 04:08:46.478071: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.33744892 0.34478968 0.33798286 0.31656745 0.29332414 0.28529865 0.28873011 0.28521475 0.26764697 0.24356255 0.22269718 0.20536903 0.19725113 0.21396847 0.25726908][0.31074971 0.32035217 0.31859115 0.30533957 0.29194182 0.291802 0.29788691 0.29201624 0.26968506 0.24338605 0.22467518 0.21351862 0.21153776 0.23148741 0.27556056][0.29079971 0.2986199 0.29759508 0.2886501 0.28141108 0.28668138 0.29629585 0.29391921 0.27531955 0.25400874 0.24175556 0.23798235 0.24030516 0.2601904 0.30119514][0.29035121 0.29208753 0.28702027 0.27823073 0.27513924 0.28781655 0.30788767 0.31835932 0.31180143 0.29892024 0.29086995 0.28710198 0.28421298 0.29644021 0.32968217][0.29167417 0.28708398 0.27889186 0.27309081 0.27830413 0.30411348 0.34178352 0.37125343 0.37895608 0.37068832 0.35931027 0.34683976 0.3308498 0.32944313 0.35111532][0.28969237 0.28143761 0.27562502 0.27983448 0.29878688 0.33962926 0.39387655 0.43936697 0.45636049 0.44575593 0.42457879 0.39930907 0.36820537 0.35189614 0.36133784][0.28714457 0.28087109 0.28328767 0.30305222 0.33786258 0.39032865 0.45314267 0.50507653 0.522484 0.50297713 0.46785623 0.4293409 0.38637781 0.35974088 0.36109644][0.29485065 0.29478198 0.30753455 0.34200558 0.38828254 0.44318604 0.50160146 0.54655313 0.55565447 0.52530664 0.47800526 0.43008092 0.38107505 0.35060057 0.34970769][0.31658065 0.32496747 0.34403962 0.38372341 0.429268 0.47354683 0.51496023 0.54343265 0.54193193 0.50557178 0.45316887 0.40149567 0.35119545 0.3214317 0.32346946][0.34514722 0.35850334 0.37514228 0.4066309 0.43828291 0.46285251 0.482485 0.4940505 0.48573446 0.45076948 0.4013055 0.35221359 0.30530751 0.27973104 0.288488][0.37137431 0.38363674 0.38877216 0.40132791 0.41161111 0.41617662 0.41931522 0.42140841 0.41259921 0.38380754 0.34063581 0.29540658 0.25182754 0.23007841 0.24528818][0.38888693 0.39602369 0.3858507 0.37715515 0.36814448 0.36099336 0.35894284 0.36122537 0.3565245 0.33344609 0.29300863 0.24608535 0.20102452 0.18025951 0.19994822][0.40248641 0.40451461 0.38151705 0.35658053 0.33564895 0.3258009 0.3261466 0.33202165 0.33001661 0.307586 0.26388735 0.20964494 0.15939432 0.13824931 0.16099082][0.41427878 0.41580752 0.38714889 0.35447475 0.32851472 0.31878597 0.31987405 0.32442763 0.31875822 0.2914443 0.24187465 0.1813698 0.12937956 0.11199617 0.13928236][0.42195332 0.42839441 0.40229353 0.36957014 0.34204942 0.33005068 0.32619351 0.32318598 0.30898529 0.27511659 0.22232555 0.16198373 0.11579451 0.10787931 0.14130487]]...]
INFO - root - 2017-12-11 04:08:52.076804: step 11210, loss = 0.68, batch loss = 0.62 (14.6 examples/sec; 0.549 sec/batch; 48h:59m:19s remains)
INFO - root - 2017-12-11 04:08:57.305352: step 11220, loss = 0.70, batch loss = 0.64 (14.5 examples/sec; 0.552 sec/batch; 49h:15m:52s remains)
INFO - root - 2017-12-11 04:09:02.847483: step 11230, loss = 0.70, batch loss = 0.64 (14.1 examples/sec; 0.567 sec/batch; 50h:34m:39s remains)
INFO - root - 2017-12-11 04:09:08.309105: step 11240, loss = 0.70, batch loss = 0.64 (14.3 examples/sec; 0.560 sec/batch; 49h:59m:04s remains)
INFO - root - 2017-12-11 04:09:13.852993: step 11250, loss = 0.68, batch loss = 0.62 (14.4 examples/sec; 0.555 sec/batch; 49h:29m:15s remains)
INFO - root - 2017-12-11 04:09:19.425927: step 11260, loss = 0.70, batch loss = 0.64 (14.5 examples/sec; 0.551 sec/batch; 49h:09m:18s remains)
INFO - root - 2017-12-11 04:09:24.854600: step 11270, loss = 0.70, batch loss = 0.64 (14.5 examples/sec; 0.553 sec/batch; 49h:21m:20s remains)
INFO - root - 2017-12-11 04:09:30.385580: step 11280, loss = 0.70, batch loss = 0.64 (15.0 examples/sec; 0.532 sec/batch; 47h:27m:04s remains)
INFO - root - 2017-12-11 04:09:35.937907: step 11290, loss = 0.70, batch loss = 0.64 (14.9 examples/sec; 0.538 sec/batch; 48h:00m:47s remains)
INFO - root - 2017-12-11 04:09:41.414750: step 11300, loss = 0.68, batch loss = 0.62 (14.8 examples/sec; 0.542 sec/batch; 48h:20m:37s remains)
2017-12-11 04:09:42.013460: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.30606794 0.28205633 0.24899128 0.21812643 0.20014225 0.19694737 0.20187481 0.20666996 0.20561884 0.2056919 0.22457743 0.25481454 0.27082497 0.25862312 0.22186029][0.30798984 0.28384987 0.25014168 0.21774149 0.19879697 0.19615416 0.20384818 0.21404682 0.21880598 0.22317317 0.24500638 0.27749005 0.29918495 0.29208556 0.25697076][0.29929507 0.27826008 0.24672063 0.21446864 0.19334218 0.18748757 0.19375977 0.20590316 0.21508604 0.22252563 0.24440096 0.2764267 0.30035943 0.29646143 0.26315358][0.28480178 0.26998469 0.24338575 0.21336696 0.19125035 0.18287975 0.18817912 0.20203562 0.21511972 0.22598633 0.24763145 0.27529016 0.29325259 0.28392127 0.24581739][0.26425198 0.25934196 0.2406294 0.21543315 0.19472989 0.18578893 0.19141084 0.20776582 0.22464955 0.23843035 0.25787055 0.27742502 0.28446865 0.26608828 0.2220685][0.26021445 0.26156285 0.24538133 0.21985525 0.19623452 0.18332089 0.18625459 0.20281819 0.22213663 0.2385506 0.25654042 0.27083403 0.27239695 0.25185072 0.20855249][0.2712236 0.27013797 0.24750078 0.21470064 0.1844611 0.16600053 0.16495633 0.18039072 0.20206244 0.22278792 0.24445051 0.2614651 0.266605 0.25140071 0.21399911][0.28109455 0.27579248 0.24648653 0.20701924 0.17170817 0.15001358 0.14707494 0.16241717 0.18803646 0.21552029 0.24463007 0.26787439 0.27787161 0.26727226 0.23410323][0.28236493 0.27346087 0.24108897 0.1996619 0.16387977 0.1428761 0.14048254 0.1563044 0.18482207 0.21821003 0.25453997 0.28345785 0.29556009 0.28527197 0.25187647][0.28087413 0.27185211 0.24105568 0.20077646 0.16543362 0.14434925 0.1409059 0.1551775 0.18364999 0.21904057 0.2581231 0.28955036 0.30183718 0.28938898 0.2532773][0.29150572 0.28525275 0.25630933 0.21466888 0.17489329 0.14863983 0.14089653 0.15320167 0.18270752 0.21978919 0.25848371 0.28746125 0.29563984 0.27758008 0.23590524][0.30355093 0.29967594 0.27142042 0.22646986 0.18023469 0.14838897 0.13940109 0.15590695 0.19376765 0.23861717 0.27973107 0.30518061 0.30544251 0.27683917 0.22421302][0.30378136 0.3018606 0.27616665 0.2313886 0.18228355 0.14866835 0.14387512 0.17072299 0.22188199 0.27756411 0.32218063 0.34359878 0.3335287 0.29135194 0.22485654][0.30526844 0.30504969 0.28240132 0.2393381 0.18870482 0.1526482 0.14893131 0.18093458 0.23869818 0.29923263 0.34427139 0.36337563 0.34807634 0.29879558 0.22474127][0.32512248 0.32807955 0.30733058 0.26377741 0.2086837 0.16519657 0.15344161 0.17761558 0.22850437 0.28371173 0.32587415 0.34730536 0.338363 0.2961753 0.22756733]]...]
INFO - root - 2017-12-11 04:09:47.471756: step 11310, loss = 0.71, batch loss = 0.65 (14.8 examples/sec; 0.540 sec/batch; 48h:11m:20s remains)
INFO - root - 2017-12-11 04:09:52.684249: step 11320, loss = 0.70, batch loss = 0.64 (14.1 examples/sec; 0.567 sec/batch; 50h:35m:34s remains)
INFO - root - 2017-12-11 04:09:58.184889: step 11330, loss = 0.70, batch loss = 0.64 (14.3 examples/sec; 0.557 sec/batch; 49h:44m:10s remains)
INFO - root - 2017-12-11 04:10:03.655433: step 11340, loss = 0.70, batch loss = 0.64 (14.7 examples/sec; 0.545 sec/batch; 48h:36m:05s remains)
INFO - root - 2017-12-11 04:10:09.197815: step 11350, loss = 0.71, batch loss = 0.65 (14.2 examples/sec; 0.563 sec/batch; 50h:11m:28s remains)
INFO - root - 2017-12-11 04:10:14.743479: step 11360, loss = 0.71, batch loss = 0.65 (15.0 examples/sec; 0.533 sec/batch; 47h:32m:15s remains)
INFO - root - 2017-12-11 04:10:20.164621: step 11370, loss = 0.68, batch loss = 0.63 (14.8 examples/sec; 0.539 sec/batch; 48h:06m:38s remains)
INFO - root - 2017-12-11 04:10:25.594499: step 11380, loss = 0.71, batch loss = 0.65 (14.6 examples/sec; 0.546 sec/batch; 48h:43m:26s remains)
INFO - root - 2017-12-11 04:10:31.133189: step 11390, loss = 0.69, batch loss = 0.63 (14.4 examples/sec; 0.554 sec/batch; 49h:25m:03s remains)
INFO - root - 2017-12-11 04:10:36.655033: step 11400, loss = 0.69, batch loss = 0.63 (14.7 examples/sec; 0.543 sec/batch; 48h:27m:24s remains)
2017-12-11 04:10:37.273192: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.2679944 0.29693645 0.31887588 0.33589479 0.36408207 0.41217053 0.46288407 0.49447444 0.503159 0.48721939 0.45221621 0.40993708 0.37459132 0.358435 0.35120067][0.25849158 0.27908462 0.301073 0.32209358 0.35164937 0.39458662 0.4355517 0.45317769 0.44424787 0.41167697 0.36974853 0.33709538 0.32266411 0.32876948 0.33893681][0.21722484 0.22840813 0.25167114 0.27982411 0.31339136 0.34997481 0.37600845 0.37377316 0.3433561 0.29481891 0.24970873 0.23111072 0.24328114 0.27782089 0.31227618][0.15766262 0.16851817 0.20299387 0.24901292 0.29736879 0.33673775 0.35489681 0.33968598 0.29381132 0.23224886 0.18194257 0.16955705 0.19737582 0.24840413 0.29870552][0.094678171 0.12286199 0.18603177 0.26676327 0.34651896 0.40400821 0.42871341 0.41368702 0.36078513 0.28599337 0.22167414 0.19869095 0.21847402 0.25935113 0.30265188][0.048698742 0.11497254 0.22473726 0.35445362 0.47681949 0.56111586 0.60095829 0.59283191 0.53569227 0.44403219 0.35641983 0.307682 0.29677492 0.30134866 0.31367311][0.0360506 0.15198423 0.31568521 0.49524552 0.65524763 0.759205 0.80883509 0.80339277 0.73909241 0.6288752 0.51669055 0.43777004 0.38630882 0.34318632 0.31406379][0.052172169 0.21110709 0.4158749 0.62574184 0.80030924 0.9027372 0.94426364 0.92855215 0.85203183 0.72777689 0.60129738 0.50407243 0.42493686 0.3484529 0.29101887][0.075010017 0.25151893 0.46476158 0.66909713 0.82485366 0.90096653 0.91640997 0.88040978 0.79314727 0.668545 0.54802567 0.45571473 0.37632316 0.29765725 0.24081627][0.086579986 0.24746518 0.43151608 0.59555995 0.70677954 0.74408585 0.72930425 0.674542 0.58416921 0.47297975 0.37606874 0.30899006 0.2552205 0.20477372 0.17738712][0.09025681 0.21122858 0.34312746 0.45195398 0.51331031 0.51554334 0.47885519 0.4133974 0.32514885 0.23079167 0.16316822 0.13161497 0.12064186 0.11931638 0.1394408][0.1083406 0.18450794 0.26422897 0.32556239 0.34899563 0.32885966 0.2823889 0.21393256 0.12783551 0.044202663 -0.0002336054 0.0018571016 0.036764868 0.08844889 0.15500037][0.16581382 0.21026002 0.25238216 0.28313893 0.28387222 0.25227723 0.20372987 0.13621601 0.050293282 -0.030534135 -0.063466586 -0.039092947 0.0309601 0.12282883 0.2187973][0.27587706 0.30729771 0.32614759 0.33567128 0.31803441 0.27533364 0.22344492 0.15605043 0.07023707 -0.00887672 -0.036363795 -0.00027781678 0.089271 0.2011192 0.30418423][0.42457211 0.45252421 0.44976914 0.43295541 0.38985065 0.33013135 0.270553 0.20292595 0.1228627 0.05432051 0.037255619 0.082571737 0.17947625 0.29243696 0.38383156]]...]
INFO - root - 2017-12-11 04:10:42.883325: step 11410, loss = 0.69, batch loss = 0.63 (14.9 examples/sec; 0.535 sec/batch; 47h:45m:29s remains)
INFO - root - 2017-12-11 04:10:48.250059: step 11420, loss = 0.71, batch loss = 0.65 (14.6 examples/sec; 0.550 sec/batch; 49h:01m:35s remains)
INFO - root - 2017-12-11 04:10:53.752440: step 11430, loss = 0.70, batch loss = 0.64 (14.3 examples/sec; 0.558 sec/batch; 49h:44m:55s remains)
INFO - root - 2017-12-11 04:10:59.137545: step 11440, loss = 0.69, batch loss = 0.63 (14.7 examples/sec; 0.546 sec/batch; 48h:39m:41s remains)
INFO - root - 2017-12-11 04:11:04.651097: step 11450, loss = 0.69, batch loss = 0.63 (14.7 examples/sec; 0.544 sec/batch; 48h:29m:32s remains)
INFO - root - 2017-12-11 04:11:10.183551: step 11460, loss = 0.69, batch loss = 0.63 (14.3 examples/sec; 0.559 sec/batch; 49h:49m:01s remains)
INFO - root - 2017-12-11 04:11:15.597638: step 11470, loss = 0.70, batch loss = 0.64 (14.8 examples/sec; 0.540 sec/batch; 48h:10m:34s remains)
INFO - root - 2017-12-11 04:11:21.155852: step 11480, loss = 0.70, batch loss = 0.64 (14.2 examples/sec; 0.564 sec/batch; 50h:15m:56s remains)
INFO - root - 2017-12-11 04:11:26.707542: step 11490, loss = 0.69, batch loss = 0.63 (14.6 examples/sec; 0.549 sec/batch; 48h:59m:05s remains)
INFO - root - 2017-12-11 04:11:32.177601: step 11500, loss = 0.69, batch loss = 0.63 (14.4 examples/sec; 0.554 sec/batch; 49h:26m:02s remains)
2017-12-11 04:11:32.838102: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.083852984 -0.083263941 -0.070584007 -0.045263864 -0.0095767966 0.032053698 0.0701106 0.097543448 0.11322726 0.11906971 0.11635151 0.10679565 0.091418006 0.07020887 0.044652686][-0.078021318 -0.071376115 -0.050424796 -0.013541945 0.03718828 0.095939852 0.14673249 0.17759217 0.18274198 0.16472287 0.13007343 0.090176225 0.055360842 0.02818268 0.0088043083][-0.054701008 -0.036742888 -0.0015921135 0.053846795 0.12663184 0.20799594 0.27427536 0.30835938 0.29905245 0.24903756 0.17105235 0.089502044 0.02678863 -0.01090482 -0.025700375][-0.015491448 0.018047292 0.072194107 0.15148824 0.25139675 0.35881153 0.44182783 0.47880891 0.45413089 0.37110585 0.24892254 0.1242118 0.03033217 -0.022773888 -0.038726725][0.037712555 0.091152787 0.16741401 0.272316 0.39994404 0.53269947 0.63100785 0.66989017 0.6318301 0.52118683 0.36251357 0.20054783 0.07661552 0.0046567996 -0.01827359][0.10011499 0.17491141 0.27441341 0.40404403 0.55585814 0.7084263 0.8179028 0.85769176 0.80861318 0.67688632 0.49099404 0.300047 0.1493053 0.057235081 0.025651246][0.15937829 0.25429812 0.37547871 0.5243572 0.68949586 0.84904236 0.96107829 0.99868852 0.94176877 0.79924583 0.6014232 0.39581898 0.22696277 0.11890578 0.080637045][0.20437782 0.31625694 0.45453256 0.612428 0.77364725 0.919811 1.0178015 1.0442313 0.98091751 0.83913469 0.64808255 0.44764027 0.2765775 0.16294007 0.12235782][0.22326265 0.34331337 0.48843086 0.64140421 0.78099871 0.8940208 0.96090508 0.96682012 0.89867157 0.76823074 0.60005492 0.42327681 0.26831537 0.16317874 0.12642975][0.20783994 0.32040218 0.4552969 0.58769065 0.69345486 0.76442987 0.79464549 0.7791546 0.71039969 0.5988611 0.46250486 0.32016307 0.19365758 0.10780459 0.080749683][0.1601243 0.24856688 0.35529315 0.45386183 0.52065569 0.55131948 0.54972428 0.51880139 0.45627865 0.37033874 0.2718257 0.17036009 0.079537988 0.019127397 0.0046172184][0.092245564 0.1486014 0.21872443 0.2798681 0.31183907 0.31251082 0.290485 0.25319162 0.20288697 0.14579274 0.086728685 0.027367387 -0.026377266 -0.061140589 -0.065574326][0.023054834 0.048960727 0.084218413 0.11256891 0.11939864 0.10345731 0.073290564 0.037387818 0.000691453 -0.031781282 -0.058612224 -0.082805514 -0.10431796 -0.11676655 -0.11397836][-0.035536531 -0.03368064 -0.025747443 -0.02104374 -0.027886543 -0.047031567 -0.07294935 -0.099409595 -0.1218643 -0.13668831 -0.14354731 -0.14599669 -0.14628203 -0.14300431 -0.13456589][-0.073902823 -0.085951604 -0.093803108 -0.10136437 -0.11175065 -0.12512417 -0.13926367 -0.15160646 -0.16016655 -0.16336851 -0.16113877 -0.15548214 -0.14802684 -0.13899054 -0.12879308]]...]
INFO - root - 2017-12-11 04:11:37.886397: step 11510, loss = 0.70, batch loss = 0.64 (17.0 examples/sec; 0.472 sec/batch; 42h:03m:09s remains)
INFO - root - 2017-12-11 04:11:43.454052: step 11520, loss = 0.69, batch loss = 0.63 (14.7 examples/sec; 0.543 sec/batch; 48h:25m:26s remains)
INFO - root - 2017-12-11 04:11:48.975703: step 11530, loss = 0.68, batch loss = 0.62 (14.3 examples/sec; 0.561 sec/batch; 49h:58m:43s remains)
INFO - root - 2017-12-11 04:11:54.448538: step 11540, loss = 0.69, batch loss = 0.63 (14.7 examples/sec; 0.544 sec/batch; 48h:28m:31s remains)
INFO - root - 2017-12-11 04:11:59.990919: step 11550, loss = 0.70, batch loss = 0.64 (14.1 examples/sec; 0.569 sec/batch; 50h:45m:03s remains)
INFO - root - 2017-12-11 04:12:05.500527: step 11560, loss = 0.69, batch loss = 0.64 (14.4 examples/sec; 0.554 sec/batch; 49h:21m:43s remains)
INFO - root - 2017-12-11 04:12:10.984890: step 11570, loss = 0.68, batch loss = 0.62 (14.8 examples/sec; 0.541 sec/batch; 48h:13m:04s remains)
INFO - root - 2017-12-11 04:12:16.416902: step 11580, loss = 0.71, batch loss = 0.65 (14.8 examples/sec; 0.541 sec/batch; 48h:15m:35s remains)
INFO - root - 2017-12-11 04:12:21.963879: step 11590, loss = 0.69, batch loss = 0.63 (14.5 examples/sec; 0.553 sec/batch; 49h:15m:52s remains)
INFO - root - 2017-12-11 04:12:27.425410: step 11600, loss = 0.69, batch loss = 0.63 (14.7 examples/sec; 0.546 sec/batch; 48h:38m:44s remains)
2017-12-11 04:12:28.045227: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.0041482304 0.012508226 0.018644962 0.02139006 0.021721985 0.021368349 0.017708948 0.0082281362 -0.0026772 -0.0064337379 -0.0044830553 -0.0011964481 0.0002619052 -0.0019188243 -0.011059562][0.062623188 0.083278768 0.10028189 0.11179908 0.11833637 0.12299582 0.12234815 0.11072293 0.093944438 0.085694 0.0847809 0.084019557 0.078599274 0.067771718 0.047282767][0.14312331 0.18108805 0.21188261 0.2325515 0.24414305 0.25279936 0.25495133 0.24138707 0.21841502 0.20578291 0.20347652 0.20051996 0.1888628 0.16815449 0.13372307][0.23073319 0.28676546 0.32911548 0.35466391 0.36727566 0.37700808 0.38074392 0.36609116 0.33809745 0.3216919 0.31968027 0.31718037 0.30148524 0.27169663 0.22321349][0.30251968 0.37038061 0.41706136 0.44156173 0.45220134 0.46297902 0.47103536 0.46084744 0.43333966 0.41620377 0.41619062 0.41552565 0.39637685 0.35691231 0.293858][0.35716435 0.42775857 0.47006777 0.48721057 0.49333304 0.50669426 0.52388358 0.52603203 0.50728089 0.49394003 0.495587 0.493334 0.46698764 0.41441 0.33540535][0.40004477 0.46449935 0.49461368 0.49942943 0.49923012 0.51574928 0.54431629 0.56310743 0.55865824 0.55196404 0.55324674 0.54476124 0.50769931 0.44013596 0.34611613][0.41539267 0.46504074 0.47702 0.46677029 0.46011844 0.48045558 0.52127135 0.55918336 0.57284176 0.57514471 0.57436919 0.55657816 0.50762767 0.4266398 0.3224332][0.39768252 0.42801186 0.42152274 0.39871493 0.38783059 0.41166869 0.46274075 0.51775944 0.54859519 0.55855107 0.55298507 0.52338541 0.46249431 0.37188986 0.26454297][0.3507508 0.36218905 0.34209061 0.31348085 0.3035292 0.33218604 0.39102614 0.45694172 0.49794558 0.51000333 0.4967742 0.45537508 0.38542148 0.29166725 0.18904561][0.28661817 0.28526935 0.25861773 0.22936897 0.22185174 0.25292394 0.31403244 0.38259932 0.42618576 0.43597922 0.41494647 0.36502022 0.29146692 0.20190844 0.11130796][0.20596212 0.19832414 0.17063721 0.1431125 0.13639578 0.16474086 0.22083117 0.28428614 0.32541436 0.33276489 0.30865815 0.25819245 0.18973784 0.11240221 0.039873317][0.10977133 0.10016242 0.076018676 0.052562755 0.045378786 0.065855183 0.10922922 0.15985386 0.19401002 0.20022073 0.17954688 0.13801271 0.084289782 0.026976692 -0.022686712][0.02202611 0.013688635 -0.0036540825 -0.021228978 -0.029451773 -0.020358071 0.0045430139 0.035810724 0.057900403 0.061873715 0.04798742 0.021145744 -0.012110387 -0.045381512 -0.071038626][-0.040154282 -0.047040295 -0.058388822 -0.070913672 -0.079861216 -0.080309495 -0.0717648 -0.058768138 -0.04916276 -0.048060544 -0.055440582 -0.068378448 -0.082971223 -0.0953031 -0.10165365]]...]
INFO - root - 2017-12-11 04:12:33.249182: step 11610, loss = 0.68, batch loss = 0.62 (14.7 examples/sec; 0.544 sec/batch; 48h:29m:27s remains)
INFO - root - 2017-12-11 04:12:38.768541: step 11620, loss = 0.69, batch loss = 0.64 (13.8 examples/sec; 0.580 sec/batch; 51h:40m:49s remains)
INFO - root - 2017-12-11 04:12:44.317837: step 11630, loss = 0.70, batch loss = 0.64 (14.7 examples/sec; 0.544 sec/batch; 48h:27m:07s remains)
INFO - root - 2017-12-11 04:12:49.748083: step 11640, loss = 0.70, batch loss = 0.64 (14.9 examples/sec; 0.538 sec/batch; 47h:55m:01s remains)
INFO - root - 2017-12-11 04:12:55.292089: step 11650, loss = 0.70, batch loss = 0.64 (14.8 examples/sec; 0.542 sec/batch; 48h:16m:32s remains)
INFO - root - 2017-12-11 04:13:00.834683: step 11660, loss = 0.71, batch loss = 0.65 (14.5 examples/sec; 0.552 sec/batch; 49h:10m:41s remains)
INFO - root - 2017-12-11 04:13:06.293829: step 11670, loss = 0.69, batch loss = 0.64 (14.6 examples/sec; 0.549 sec/batch; 48h:56m:57s remains)
INFO - root - 2017-12-11 04:13:11.792370: step 11680, loss = 0.68, batch loss = 0.62 (14.7 examples/sec; 0.543 sec/batch; 48h:25m:19s remains)
INFO - root - 2017-12-11 04:13:17.340067: step 11690, loss = 0.71, batch loss = 0.65 (14.5 examples/sec; 0.552 sec/batch; 49h:10m:21s remains)
INFO - root - 2017-12-11 04:13:22.800462: step 11700, loss = 0.70, batch loss = 0.64 (14.7 examples/sec; 0.545 sec/batch; 48h:31m:41s remains)
2017-12-11 04:13:23.394721: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.25831527 0.28809431 0.29413667 0.28408614 0.27441981 0.28751928 0.31580952 0.34135091 0.35499832 0.34616429 0.3206383 0.3036333 0.30419 0.31248665 0.30099916][0.24628405 0.27833089 0.2877869 0.27965325 0.26741207 0.27578741 0.30364209 0.33499593 0.3574456 0.35498598 0.33096135 0.31274778 0.31354928 0.32360902 0.31578127][0.23625721 0.26361525 0.27178922 0.2644766 0.25058612 0.25331876 0.27703455 0.30947316 0.33577284 0.335651 0.31180093 0.29254749 0.29371911 0.30543411 0.3031272][0.24302056 0.26091838 0.26501209 0.25988606 0.24899629 0.25116447 0.27218977 0.30301532 0.32705614 0.3226482 0.29401875 0.26870638 0.2640419 0.27163208 0.27153093][0.26912391 0.27752277 0.27694428 0.27523845 0.27113441 0.27861598 0.30117768 0.33082229 0.34935066 0.33612922 0.29821271 0.26110795 0.24369846 0.24251096 0.24319306][0.302356 0.30392188 0.29985389 0.30218339 0.30663028 0.3232069 0.35128608 0.38159969 0.39425451 0.3705855 0.32059768 0.26822054 0.23516011 0.22439857 0.22719474][0.32250598 0.31888437 0.31079903 0.31523782 0.32713979 0.35318655 0.38836256 0.42083725 0.42988068 0.39888802 0.33983627 0.27557704 0.23160405 0.21580423 0.22318281][0.31448084 0.30372971 0.28832141 0.28897151 0.30226961 0.33302566 0.37345132 0.40864053 0.41825819 0.38809565 0.32988626 0.26422572 0.21953052 0.20646505 0.22184394][0.27391356 0.25293812 0.2265301 0.21758589 0.22689524 0.25872639 0.30430561 0.34531063 0.36158457 0.34055 0.29140082 0.23196538 0.1925047 0.18599643 0.20990157][0.21771649 0.18610072 0.1490777 0.12918857 0.13169371 0.16204184 0.21313454 0.26358986 0.29113483 0.28250051 0.24389403 0.19152939 0.15666988 0.15482546 0.18390255][0.16172174 0.12688525 0.087554939 0.061946396 0.057837002 0.084585488 0.14001627 0.20086192 0.24036916 0.24213429 0.21067041 0.16170859 0.12628964 0.12224009 0.14915171][0.11321277 0.086688787 0.0575562 0.035741556 0.029018123 0.052423049 0.10992698 0.1770903 0.2234432 0.2297772 0.20086971 0.15146372 0.11048846 0.09778396 0.11535238][0.084446624 0.076968394 0.068259314 0.058113772 0.052888416 0.073061407 0.1267093 0.18955018 0.23197164 0.23608205 0.20771556 0.15887006 0.11393418 0.093663394 0.10226677][0.090057589 0.10298944 0.11274042 0.11326696 0.10934015 0.12391071 0.16582596 0.21396638 0.24437852 0.24348737 0.21811873 0.17608295 0.13524407 0.11485367 0.12095227][0.13105674 0.15660852 0.17352471 0.17663614 0.16983308 0.17473875 0.19864029 0.22648452 0.24281354 0.23870143 0.22185495 0.19544826 0.16907035 0.15648746 0.16535345]]...]
INFO - root - 2017-12-11 04:13:28.512467: step 11710, loss = 0.69, batch loss = 0.63 (14.7 examples/sec; 0.544 sec/batch; 48h:28m:15s remains)
INFO - root - 2017-12-11 04:13:33.957303: step 11720, loss = 0.71, batch loss = 0.65 (14.8 examples/sec; 0.539 sec/batch; 48h:01m:26s remains)
INFO - root - 2017-12-11 04:13:39.445265: step 11730, loss = 0.72, batch loss = 0.66 (14.1 examples/sec; 0.569 sec/batch; 50h:42m:19s remains)
INFO - root - 2017-12-11 04:13:45.005649: step 11740, loss = 0.71, batch loss = 0.65 (14.8 examples/sec; 0.542 sec/batch; 48h:16m:33s remains)
INFO - root - 2017-12-11 04:13:50.438056: step 11750, loss = 0.70, batch loss = 0.64 (14.8 examples/sec; 0.540 sec/batch; 48h:08m:21s remains)
INFO - root - 2017-12-11 04:13:55.930107: step 11760, loss = 0.68, batch loss = 0.62 (14.6 examples/sec; 0.549 sec/batch; 48h:55m:51s remains)
INFO - root - 2017-12-11 04:14:01.517801: step 11770, loss = 0.70, batch loss = 0.64 (14.0 examples/sec; 0.570 sec/batch; 50h:45m:26s remains)
INFO - root - 2017-12-11 04:14:06.956653: step 11780, loss = 0.69, batch loss = 0.63 (14.8 examples/sec; 0.540 sec/batch; 48h:04m:41s remains)
INFO - root - 2017-12-11 04:14:12.493940: step 11790, loss = 0.71, batch loss = 0.65 (14.6 examples/sec; 0.549 sec/batch; 48h:54m:12s remains)
INFO - root - 2017-12-11 04:14:17.984051: step 11800, loss = 0.70, batch loss = 0.64 (14.8 examples/sec; 0.540 sec/batch; 48h:08m:16s remains)
2017-12-11 04:14:18.482235: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.31010243 0.26097164 0.22277257 0.21469034 0.22742249 0.25030491 0.28515324 0.32341284 0.351878 0.36844414 0.36291993 0.33056557 0.27393976 0.21091315 0.15912341][0.36512882 0.33868185 0.32125056 0.32426766 0.33222064 0.33472371 0.33410576 0.3309032 0.3219983 0.31242985 0.29395294 0.25606474 0.199003 0.13783927 0.089080557][0.3875466 0.39275703 0.40139309 0.41571161 0.41724432 0.39652261 0.35855192 0.31403908 0.27043512 0.23712209 0.20673689 0.16541041 0.11141022 0.057255123 0.017370408][0.37856194 0.41699046 0.45047554 0.47369108 0.47099912 0.43708375 0.3784565 0.31189352 0.25033325 0.2032751 0.16263527 0.11425186 0.057803318 0.005124962 -0.0307138][0.34919032 0.41472459 0.46593758 0.49397451 0.49136466 0.45968312 0.404656 0.34222138 0.28331497 0.23385632 0.1845054 0.12349808 0.054821298 -0.0067795413 -0.04719688][0.32018358 0.39967376 0.45543855 0.48087275 0.48052323 0.46313298 0.43059611 0.39059114 0.34618863 0.29865932 0.23987797 0.16316918 0.078798376 0.0053970646 -0.041641772][0.30077541 0.37755671 0.42379496 0.43920261 0.44222334 0.44657594 0.44775271 0.44066057 0.41692352 0.37274042 0.30237606 0.20811979 0.10869103 0.026144831 -0.024847666][0.29376683 0.35638994 0.384683 0.38705617 0.39400613 0.4214564 0.45817682 0.48458511 0.48065317 0.4374508 0.35281929 0.24076258 0.12901774 0.041910745 -0.010158234][0.29799062 0.34133992 0.3495976 0.34008887 0.35001868 0.3948127 0.45655483 0.50452614 0.51041353 0.46250874 0.36417904 0.24055201 0.12563543 0.042340595 -0.00571991][0.29826179 0.32206157 0.31210029 0.29239216 0.30189675 0.353717 0.42493793 0.47851479 0.48317257 0.42802912 0.32310292 0.200203 0.09334968 0.021897621 -0.016465165][0.27629665 0.28352842 0.26050675 0.23346119 0.23974556 0.28936291 0.356209 0.40277341 0.40044734 0.34163514 0.24045055 0.12989095 0.040766072 -0.012070573 -0.03564287][0.22210757 0.21894489 0.19140245 0.16274574 0.1654145 0.20612422 0.25900882 0.29201362 0.2835083 0.22957756 0.14399721 0.055761851 -0.00884755 -0.039506212 -0.046557449][0.15359132 0.14554776 0.11961582 0.093263477 0.091760553 0.11926287 0.15422085 0.17328964 0.16339086 0.12325444 0.062451538 0.0021682873 -0.036765154 -0.047612429 -0.0419194][0.091296159 0.080147736 0.057768583 0.035551552 0.03148656 0.0477452 0.067865148 0.077264667 0.070739917 0.048880778 0.01539543 -0.017600637 -0.035465974 -0.033585764 -0.021997364][0.037973572 0.024818627 0.0065336581 -0.010602526 -0.014439622 -0.0040683248 0.0081496546 0.014085731 0.013884922 0.0096842423 -0.00023084665 -0.011949725 -0.016058112 -0.0084402636 0.0040332032]]...]
INFO - root - 2017-12-11 04:14:23.775882: step 11810, loss = 0.70, batch loss = 0.64 (14.6 examples/sec; 0.547 sec/batch; 48h:42m:04s remains)
INFO - root - 2017-12-11 04:14:29.287328: step 11820, loss = 0.69, batch loss = 0.63 (14.6 examples/sec; 0.547 sec/batch; 48h:45m:24s remains)
INFO - root - 2017-12-11 04:14:34.908234: step 11830, loss = 0.70, batch loss = 0.65 (14.1 examples/sec; 0.569 sec/batch; 50h:42m:13s remains)
INFO - root - 2017-12-11 04:14:40.432124: step 11840, loss = 0.70, batch loss = 0.64 (15.2 examples/sec; 0.527 sec/batch; 46h:55m:59s remains)
INFO - root - 2017-12-11 04:14:45.968061: step 11850, loss = 0.71, batch loss = 0.65 (14.8 examples/sec; 0.541 sec/batch; 48h:13m:33s remains)
INFO - root - 2017-12-11 04:14:51.459924: step 11860, loss = 0.69, batch loss = 0.63 (14.1 examples/sec; 0.567 sec/batch; 50h:30m:59s remains)
INFO - root - 2017-12-11 04:14:56.962459: step 11870, loss = 0.67, batch loss = 0.62 (14.5 examples/sec; 0.552 sec/batch; 49h:09m:35s remains)
INFO - root - 2017-12-11 04:15:02.415929: step 11880, loss = 0.70, batch loss = 0.64 (14.9 examples/sec; 0.536 sec/batch; 47h:44m:21s remains)
INFO - root - 2017-12-11 04:15:07.935178: step 11890, loss = 0.68, batch loss = 0.62 (14.5 examples/sec; 0.552 sec/batch; 49h:11m:33s remains)
INFO - root - 2017-12-11 04:15:13.188082: step 11900, loss = 0.68, batch loss = 0.63 (14.6 examples/sec; 0.548 sec/batch; 48h:50m:06s remains)
2017-12-11 04:15:13.744821: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.0036958735 0.025285129 0.04871548 0.066888735 0.079251409 0.088170335 0.09789487 0.11090402 0.12309668 0.12765163 0.11715903 0.086619675 0.040952556 -0.0076960931 -0.048882462][-0.044507455 -0.027050847 0.0033509561 0.037945174 0.072447091 0.10442358 0.13270265 0.15438862 0.16407984 0.15608424 0.12686084 0.078172818 0.020886937 -0.030729888 -0.0686336][-0.0828293 -0.070481569 -0.03428917 0.016320843 0.073636994 0.13066624 0.17940052 0.21052949 0.21693109 0.19462448 0.14540638 0.078337304 0.010175493 -0.043536033 -0.077982947][-0.081360586 -0.067170039 -0.022578366 0.041507054 0.11421826 0.18610382 0.24567735 0.28018886 0.28136572 0.24655253 0.18219775 0.1021263 0.026122995 -0.03082226 -0.066286765][-0.036856234 -0.011781636 0.043876879 0.11704746 0.19525215 0.26910242 0.32783625 0.35905236 0.35325626 0.30855635 0.23463479 0.14792766 0.068210147 0.0077185519 -0.033397026][0.032252915 0.071231171 0.13637665 0.21384358 0.29275057 0.36631644 0.42603582 0.45882088 0.45122048 0.3997187 0.31531057 0.21796145 0.12971452 0.061452396 0.010559593][0.10917255 0.15799879 0.22521669 0.29929817 0.3740907 0.44618478 0.50836784 0.54442459 0.53602821 0.477296 0.38065749 0.27197817 0.17635392 0.1033762 0.045836285][0.17441253 0.22738436 0.28844026 0.35020643 0.41255915 0.47521445 0.53270739 0.5671401 0.55716676 0.49456766 0.39262021 0.28224018 0.18912503 0.11972214 0.062233135][0.20898926 0.26420215 0.31815547 0.36584663 0.41206044 0.45981255 0.50717419 0.537521 0.528253 0.46891144 0.37149984 0.26834941 0.18311955 0.11937489 0.063235134][0.2033705 0.25799337 0.30576265 0.34111679 0.37218311 0.40629974 0.44544157 0.4741919 0.46870241 0.4160074 0.32649958 0.23265821 0.15649003 0.099178821 0.046061587][0.16181257 0.20840043 0.2470344 0.27092585 0.28979713 0.31506324 0.35155472 0.38376942 0.3855018 0.34063044 0.25851643 0.17261097 0.10465741 0.0549133 0.0090104146][0.097440191 0.13208582 0.15893178 0.17065141 0.17774983 0.19450748 0.22706059 0.26020947 0.26700261 0.23067671 0.16071628 0.089451469 0.036079932 -0.00087054446 -0.034464527][0.02388561 0.045112826 0.061085682 0.063117534 0.061303437 0.069735616 0.094196081 0.12235264 0.13127473 0.10725839 0.058281872 0.010144676 -0.02380343 -0.046506818 -0.0680014][-0.041473437 -0.031608019 -0.022891348 -0.024841005 -0.02956328 -0.026288962 -0.010736724 0.0096132234 0.02021542 0.012709638 -0.0090094078 -0.03046608 -0.046195515 -0.05998043 -0.076845594][-0.078218363 -0.0750521 -0.070720859 -0.074144736 -0.079670653 -0.080069184 -0.072017491 -0.057958804 -0.044349369 -0.035264455 -0.03179178 -0.030774305 -0.034399148 -0.046436384 -0.065987512]]...]
INFO - root - 2017-12-11 04:15:19.194033: step 11910, loss = 0.70, batch loss = 0.64 (15.1 examples/sec; 0.529 sec/batch; 47h:07m:42s remains)
INFO - root - 2017-12-11 04:15:24.634319: step 11920, loss = 0.69, batch loss = 0.63 (14.5 examples/sec; 0.553 sec/batch; 49h:14m:54s remains)
INFO - root - 2017-12-11 04:15:30.154568: step 11930, loss = 0.70, batch loss = 0.64 (14.2 examples/sec; 0.565 sec/batch; 50h:18m:34s remains)
INFO - root - 2017-12-11 04:15:35.641638: step 11940, loss = 0.70, batch loss = 0.64 (14.4 examples/sec; 0.556 sec/batch; 49h:29m:52s remains)
INFO - root - 2017-12-11 04:15:41.082371: step 11950, loss = 0.68, batch loss = 0.62 (15.2 examples/sec; 0.528 sec/batch; 46h:59m:00s remains)
INFO - root - 2017-12-11 04:15:46.600717: step 11960, loss = 0.69, batch loss = 0.63 (14.3 examples/sec; 0.558 sec/batch; 49h:41m:00s remains)
INFO - root - 2017-12-11 04:15:52.131924: step 11970, loss = 0.68, batch loss = 0.63 (14.5 examples/sec; 0.552 sec/batch; 49h:08m:04s remains)
INFO - root - 2017-12-11 04:15:57.622568: step 11980, loss = 0.69, batch loss = 0.64 (15.1 examples/sec; 0.530 sec/batch; 47h:11m:46s remains)
INFO - root - 2017-12-11 04:16:03.112016: step 11990, loss = 0.70, batch loss = 0.64 (14.5 examples/sec; 0.551 sec/batch; 49h:04m:11s remains)
INFO - root - 2017-12-11 04:16:08.403208: step 12000, loss = 0.69, batch loss = 0.63 (14.1 examples/sec; 0.567 sec/batch; 50h:27m:43s remains)
2017-12-11 04:16:08.996636: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.049300455 0.071381837 0.085297279 0.091788426 0.094545856 0.099563479 0.1075329 0.11152615 0.10486387 0.0872304 0.068640649 0.0585905 0.059806574 0.067995168 0.0744065][0.087771885 0.12238515 0.14398322 0.15182188 0.151967 0.15723634 0.17189893 0.1847745 0.181461 0.1570415 0.12556088 0.10382646 0.098983437 0.10866198 0.12043699][0.1212916 0.16574626 0.19169648 0.19783729 0.19273509 0.19602445 0.21587691 0.23719886 0.23813246 0.20887674 0.16654173 0.13372797 0.1210931 0.12915699 0.14486796][0.13441926 0.18279955 0.21129692 0.21825673 0.21142368 0.21311696 0.23456381 0.25972375 0.26409441 0.23505685 0.19041105 0.1536305 0.13414165 0.13541837 0.14851935][0.1250709 0.17268026 0.20484984 0.21931745 0.2183437 0.22168605 0.24286325 0.26882327 0.27747419 0.2554763 0.21858139 0.18627806 0.16192143 0.1504306 0.15138063][0.10572997 0.15103075 0.18847401 0.21340726 0.22146922 0.22831064 0.25032485 0.2794356 0.29621285 0.28727794 0.26483008 0.24221885 0.21407877 0.18428297 0.16359213][0.090267152 0.13409822 0.17480488 0.20538557 0.21908012 0.23022018 0.25754669 0.29544392 0.32261524 0.32504749 0.31287175 0.29585961 0.26206502 0.21285887 0.16926095][0.08604598 0.12985355 0.16956136 0.19732597 0.20915303 0.22151054 0.25456268 0.30225509 0.33737305 0.34432966 0.33310586 0.31443214 0.27596202 0.21551326 0.15866074][0.098839633 0.15067229 0.19519068 0.22045676 0.22322737 0.22380883 0.24806377 0.29305369 0.32695773 0.33200106 0.31736445 0.29664639 0.26048294 0.20227461 0.14536667][0.13454224 0.20557675 0.26590976 0.29495519 0.28501728 0.25940666 0.25517014 0.27902198 0.29938453 0.29723889 0.28027678 0.26466802 0.2432225 0.203811 0.16190569][0.17744592 0.26910552 0.34809527 0.38529763 0.36627567 0.31401929 0.27536249 0.26914644 0.26929247 0.25875226 0.24435315 0.24171488 0.24263254 0.22958778 0.20822017][0.20998795 0.31292751 0.40305832 0.44777456 0.42639095 0.35750288 0.29097229 0.25628462 0.23738307 0.22206528 0.21580546 0.22953489 0.25270414 0.26543286 0.26548761][0.22108735 0.32170907 0.40937951 0.45442221 0.43302298 0.35781547 0.27510777 0.22102882 0.19089699 0.17750682 0.18316422 0.21091533 0.24852347 0.27789456 0.29308167][0.19642688 0.28263462 0.35679609 0.39557728 0.37627339 0.3051866 0.22060335 0.15903339 0.12492706 0.11528187 0.12862086 0.16185287 0.20299403 0.2384221 0.26057214][0.13774383 0.20433769 0.26121676 0.29065892 0.27454066 0.21509479 0.14099464 0.082922526 0.050200578 0.042880975 0.057411518 0.088931777 0.12725803 0.16358224 0.1878515]]...]
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-SGD-lr0.001-clip50-initfromfixqian30k-fixqian
INFO - root - 2017-12-11 04:16:14.548457: step 12010, loss = 0.69, batch loss = 0.64 (14.8 examples/sec; 0.542 sec/batch; 48h:13m:42s remains)
INFO - root - 2017-12-11 04:16:20.024368: step 12020, loss = 0.70, batch loss = 0.64 (14.8 examples/sec; 0.540 sec/batch; 48h:05m:34s remains)
INFO - root - 2017-12-11 04:16:25.452833: step 12030, loss = 0.68, batch loss = 0.63 (14.9 examples/sec; 0.538 sec/batch; 47h:51m:48s remains)
INFO - root - 2017-12-11 04:16:30.952411: step 12040, loss = 0.70, batch loss = 0.64 (14.6 examples/sec; 0.546 sec/batch; 48h:37m:29s remains)
INFO - root - 2017-12-11 04:16:36.463657: step 12050, loss = 0.70, batch loss = 0.64 (14.6 examples/sec; 0.549 sec/batch; 48h:54m:41s remains)
INFO - root - 2017-12-11 04:16:41.940135: step 12060, loss = 0.70, batch loss = 0.64 (14.6 examples/sec; 0.548 sec/batch; 48h:45m:05s remains)
INFO - root - 2017-12-11 04:16:47.402574: step 12070, loss = 0.70, batch loss = 0.64 (14.5 examples/sec; 0.552 sec/batch; 49h:08m:51s remains)
INFO - root - 2017-12-11 04:16:52.947517: step 12080, loss = 0.70, batch loss = 0.64 (14.6 examples/sec; 0.547 sec/batch; 48h:38m:44s remains)
INFO - root - 2017-12-11 04:16:58.455020: step 12090, loss = 0.70, batch loss = 0.64 (14.3 examples/sec; 0.561 sec/batch; 49h:53m:46s remains)
INFO - root - 2017-12-11 04:17:03.727454: step 12100, loss = 0.71, batch loss = 0.65 (14.3 examples/sec; 0.560 sec/batch; 49h:48m:34s remains)
2017-12-11 04:17:04.306877: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.38495922 0.37638515 0.35807925 0.34255391 0.32297239 0.27555785 0.19755012 0.11965293 0.072167791 0.061303288 0.069879889 0.084374078 0.096433006 0.098227873 0.09411642][0.42014229 0.42135349 0.402266 0.37943539 0.35216662 0.29983589 0.21818356 0.13549466 0.08083199 0.06002228 0.055723269 0.055631183 0.056262311 0.056447022 0.06373845][0.40658754 0.41695502 0.40221241 0.37965724 0.35472786 0.31189582 0.24300849 0.16907519 0.11516909 0.088148259 0.073160209 0.059895985 0.047827043 0.040380083 0.047548313][0.35716966 0.37509853 0.37060452 0.35902902 0.34869233 0.32829267 0.2837671 0.22655588 0.17813797 0.14851117 0.12759829 0.10559942 0.082936056 0.064924479 0.061596315][0.29856643 0.32612869 0.33913618 0.34882531 0.36216268 0.36968288 0.35087582 0.30771205 0.26064941 0.226718 0.20223019 0.17667517 0.14850086 0.12208498 0.10572068][0.24523747 0.28542936 0.32270736 0.36181149 0.40464193 0.44068232 0.44353232 0.4072316 0.35298896 0.30712613 0.27429512 0.24481224 0.21457586 0.18540524 0.16127516][0.19574836 0.2492383 0.31207755 0.38108993 0.4518998 0.51176661 0.53083414 0.49748114 0.43279797 0.37034962 0.3237054 0.2877439 0.25772956 0.23246761 0.20984654][0.15075198 0.21077177 0.29051942 0.3794201 0.46782976 0.54167068 0.57127279 0.54216623 0.47239387 0.39661077 0.33541939 0.29131874 0.26257911 0.24575654 0.23275639][0.11048909 0.16448493 0.24613616 0.33962721 0.43159634 0.50866139 0.54468656 0.52466691 0.46064547 0.38197476 0.31245068 0.2616092 0.23301516 0.22401302 0.22284046][0.075041823 0.11098523 0.1792172 0.263035 0.34714314 0.4194068 0.4591547 0.45235285 0.40429014 0.33523953 0.26704979 0.21384417 0.18406074 0.17794397 0.18477374][0.049850106 0.061517008 0.10706272 0.17268956 0.24393356 0.30959529 0.35310328 0.36121473 0.33351442 0.28118092 0.22104563 0.169015 0.13656761 0.12733218 0.13443623][0.04298076 0.036220163 0.059910145 0.10524712 0.16163807 0.21995905 0.26585835 0.28532693 0.27383566 0.2364431 0.18533486 0.13640904 0.10173095 0.086801805 0.08892969][0.060332511 0.048139833 0.059170283 0.086678863 0.12635723 0.17421427 0.21769395 0.24121378 0.23706377 0.20714955 0.16123213 0.11530193 0.080714956 0.062508464 0.059478365][0.087672338 0.087119423 0.098792106 0.11497969 0.13900372 0.17407952 0.20990321 0.22974683 0.22396906 0.19385751 0.1491081 0.10605176 0.074755535 0.057926733 0.052278042][0.11744874 0.13706891 0.15656023 0.16618332 0.17602083 0.19652481 0.2200838 0.23046525 0.21784663 0.18430181 0.1405313 0.10269346 0.079041094 0.068435632 0.062779337]]...]
INFO - root - 2017-12-11 04:17:09.814219: step 12110, loss = 0.71, batch loss = 0.65 (14.7 examples/sec; 0.543 sec/batch; 48h:19m:38s remains)
INFO - root - 2017-12-11 04:17:15.295331: step 12120, loss = 0.69, batch loss = 0.63 (14.6 examples/sec; 0.547 sec/batch; 48h:39m:49s remains)
INFO - root - 2017-12-11 04:17:20.767253: step 12130, loss = 0.70, batch loss = 0.64 (14.2 examples/sec; 0.564 sec/batch; 50h:10m:23s remains)
INFO - root - 2017-12-11 04:17:26.297599: step 12140, loss = 0.69, batch loss = 0.64 (14.1 examples/sec; 0.566 sec/batch; 50h:21m:08s remains)
INFO - root - 2017-12-11 04:17:31.740381: step 12150, loss = 0.71, batch loss = 0.65 (14.7 examples/sec; 0.542 sec/batch; 48h:16m:01s remains)
INFO - root - 2017-12-11 04:17:37.220763: step 12160, loss = 0.69, batch loss = 0.64 (15.1 examples/sec; 0.531 sec/batch; 47h:15m:34s remains)
INFO - root - 2017-12-11 04:17:42.878965: step 12170, loss = 0.69, batch loss = 0.64 (14.3 examples/sec; 0.558 sec/batch; 49h:39m:38s remains)
INFO - root - 2017-12-11 04:17:48.351189: step 12180, loss = 0.70, batch loss = 0.64 (14.5 examples/sec; 0.550 sec/batch; 48h:57m:54s remains)
INFO - root - 2017-12-11 04:17:53.837523: step 12190, loss = 0.68, batch loss = 0.63 (15.2 examples/sec; 0.525 sec/batch; 46h:42m:39s remains)
INFO - root - 2017-12-11 04:17:59.083109: step 12200, loss = 0.68, batch loss = 0.62 (14.6 examples/sec; 0.549 sec/batch; 48h:52m:40s remains)
2017-12-11 04:17:59.658849: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0033169931 -0.0030155962 -3.5796168e-06 0.0034752723 0.0056945928 0.0061388896 0.0044873375 0.0023438781 0.0013724522 0.0028854702 0.0054891645 0.0071293213 0.0056641554 0.0003052683 -0.0077817556][0.027930815 0.037240479 0.049726497 0.059965011 0.064922 0.066111714 0.063831724 0.059400849 0.054002948 0.0504412 0.048228376 0.045571387 0.039357521 0.027987111 0.012528788][0.084913433 0.10959189 0.13597211 0.15553519 0.16432509 0.16619572 0.16314264 0.15643242 0.14578389 0.13541362 0.12497518 0.11411779 0.099097125 0.077785067 0.051226389][0.16512424 0.21042314 0.25223413 0.28071737 0.29220352 0.29305267 0.28882965 0.2813637 0.26814163 0.25353631 0.23524727 0.21434441 0.18623459 0.14963749 0.10670456][0.25534257 0.32371163 0.37944898 0.4135386 0.42417893 0.4206647 0.41497186 0.41010794 0.40003863 0.38745749 0.36566895 0.33580619 0.29173958 0.23481111 0.17067005][0.33187813 0.42151564 0.48809049 0.52416456 0.53024989 0.51935589 0.51241606 0.51418453 0.51462942 0.51263738 0.49469966 0.46014687 0.40072179 0.32172513 0.23425227][0.37623018 0.48213679 0.55589688 0.59027773 0.58784527 0.56643581 0.55685955 0.567096 0.58398992 0.60084647 0.59565103 0.56366885 0.49423787 0.39676514 0.28827322][0.37974629 0.49270356 0.56812984 0.597562 0.58443934 0.551029 0.53585923 0.55175811 0.5855087 0.62472975 0.6388346 0.61696821 0.54634547 0.439589 0.31861654][0.3392272 0.44633454 0.51643157 0.53970504 0.51906043 0.47830218 0.45770717 0.47385567 0.51709026 0.57200062 0.60357904 0.59547782 0.53309697 0.43022469 0.31055158][0.26439267 0.35286555 0.41068283 0.42806953 0.40667933 0.3674781 0.34510139 0.35671571 0.39820981 0.45548862 0.49525198 0.49878445 0.45125765 0.36473671 0.26063967][0.17649534 0.23971575 0.28117415 0.29255965 0.27457148 0.24270879 0.2216562 0.22615445 0.25701413 0.3043412 0.34170195 0.3517397 0.32127503 0.25870544 0.180238][0.096587636 0.13564676 0.16104637 0.16648789 0.15222247 0.12832649 0.10979103 0.10742053 0.12460566 0.15582387 0.18341556 0.1939467 0.17754102 0.1391308 0.089017808][0.034779932 0.054664232 0.067095004 0.067735404 0.056750558 0.040039089 0.025491327 0.019613137 0.025741518 0.041383982 0.056899164 0.063868538 0.056298751 0.036832862 0.010973622][-0.0060356762 0.0007200804 0.0046442845 0.0029803663 -0.0041624061 -0.013989804 -0.023063459 -0.028408086 -0.028061898 -0.022813939 -0.016727071 -0.013923652 -0.017138179 -0.024998298 -0.034998987][-0.02493123 -0.024862656 -0.024658579 -0.02615492 -0.029412573 -0.033560771 -0.037532952 -0.040380668 -0.041388065 -0.0404154 -0.038978267 -0.038804788 -0.040479489 -0.043326102 -0.046180438]]...]
INFO - root - 2017-12-11 04:18:05.216044: step 12210, loss = 0.69, batch loss = 0.63 (14.4 examples/sec; 0.556 sec/batch; 49h:28m:39s remains)
INFO - root - 2017-12-11 04:18:10.753472: step 12220, loss = 0.69, batch loss = 0.63 (14.7 examples/sec; 0.545 sec/batch; 48h:30m:32s remains)
INFO - root - 2017-12-11 04:18:16.211891: step 12230, loss = 0.70, batch loss = 0.64 (14.9 examples/sec; 0.537 sec/batch; 47h:44m:11s remains)
INFO - root - 2017-12-11 04:18:21.643341: step 12240, loss = 0.69, batch loss = 0.64 (14.8 examples/sec; 0.539 sec/batch; 47h:59m:21s remains)
INFO - root - 2017-12-11 04:18:27.198178: step 12250, loss = 0.70, batch loss = 0.65 (14.0 examples/sec; 0.570 sec/batch; 50h:42m:44s remains)
INFO - root - 2017-12-11 04:18:32.758021: step 12260, loss = 0.70, batch loss = 0.65 (14.5 examples/sec; 0.553 sec/batch; 49h:11m:59s remains)
INFO - root - 2017-12-11 04:18:38.241769: step 12270, loss = 0.71, batch loss = 0.65 (14.5 examples/sec; 0.553 sec/batch; 49h:09m:38s remains)
INFO - root - 2017-12-11 04:18:43.726647: step 12280, loss = 0.69, batch loss = 0.64 (14.3 examples/sec; 0.559 sec/batch; 49h:45m:46s remains)
INFO - root - 2017-12-11 04:18:48.854449: step 12290, loss = 0.71, batch loss = 0.65 (14.7 examples/sec; 0.545 sec/batch; 48h:26m:07s remains)
INFO - root - 2017-12-11 04:18:54.371318: step 12300, loss = 0.69, batch loss = 0.63 (14.4 examples/sec; 0.554 sec/batch; 49h:18m:06s remains)
2017-12-11 04:18:54.978481: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.11021437 0.12488818 0.13888831 0.14603858 0.14613988 0.14756638 0.15194459 0.15486847 0.14658745 0.12677428 0.099848852 0.068225391 0.037257228 0.015530724 0.0058630276][0.16741791 0.18907894 0.20642549 0.21311052 0.20712337 0.19792436 0.19125083 0.18588096 0.17422627 0.15434007 0.12835868 0.097652167 0.066758126 0.043055046 0.028589906][0.21261124 0.24128598 0.25966245 0.26236793 0.24792969 0.22715151 0.20977682 0.19826151 0.18755963 0.1730306 0.15199403 0.12504239 0.096271142 0.07167086 0.053105697][0.24318181 0.2745035 0.28770337 0.28058082 0.25536552 0.22474156 0.20197324 0.19196983 0.18947291 0.18549149 0.17212321 0.15027566 0.12442137 0.099986583 0.079699546][0.27039823 0.29901576 0.30172613 0.28071845 0.24380134 0.20592257 0.18275437 0.18031262 0.19014306 0.19809411 0.19202426 0.17509495 0.1533858 0.13206065 0.11475239][0.28682262 0.31213757 0.30719838 0.27812141 0.23678392 0.19841786 0.17875348 0.18496025 0.20476799 0.22007945 0.21724989 0.20337953 0.18700497 0.17214526 0.16236025][0.28204986 0.30738574 0.30435902 0.28049514 0.24789008 0.21814229 0.20478971 0.21649455 0.23914826 0.25363058 0.24829149 0.23422439 0.22213887 0.21463104 0.21370851][0.25015688 0.27793443 0.28436884 0.2754029 0.260143 0.24412732 0.23808332 0.25180921 0.27172852 0.28043929 0.26978505 0.25335073 0.24286529 0.2400604 0.2449452][0.19504686 0.2240254 0.24028809 0.24723886 0.25000885 0.24823719 0.24934112 0.26310724 0.27852443 0.28159004 0.26709512 0.24846572 0.23707275 0.23437928 0.2389183][0.12416261 0.15024942 0.17116234 0.18823501 0.20305073 0.21129085 0.21730481 0.22953421 0.24017668 0.23906209 0.22300081 0.20343748 0.19036198 0.18519609 0.18556172][0.04728321 0.065580815 0.084867775 0.10473871 0.12434776 0.13758372 0.14641465 0.15669058 0.16345742 0.16030473 0.14565596 0.12786846 0.11464434 0.10752967 0.10381645][-0.019349344 -0.011161508 0.0018151436 0.018297385 0.036587656 0.050579455 0.06043252 0.0688844 0.072905585 0.0693327 0.057777431 0.043774534 0.032467816 0.025401944 0.020197071][-0.06419035 -0.063949019 -0.057916284 -0.047618236 -0.034450669 -0.023178723 -0.014794948 -0.0088971378 -0.0071640606 -0.010496703 -0.018433938 -0.027603904 -0.035034135 -0.039507091 -0.04272119][-0.080824845 -0.084556513 -0.083132759 -0.078180395 -0.070588976 -0.063454077 -0.057971604 -0.054756712 -0.054556917 -0.057141092 -0.061629061 -0.06599915 -0.068916149 -0.069675379 -0.069282733][-0.072946161 -0.078217372 -0.079797491 -0.078934938 -0.075956874 -0.07256493 -0.069609635 -0.067949221 -0.06784007 -0.068753831 -0.069827907 -0.069844067 -0.06854929 -0.065696985 -0.06200695]]...]
INFO - root - 2017-12-11 04:19:00.468265: step 12310, loss = 0.69, batch loss = 0.63 (14.8 examples/sec; 0.539 sec/batch; 47h:56m:31s remains)
INFO - root - 2017-12-11 04:19:05.995479: step 12320, loss = 0.69, batch loss = 0.64 (14.9 examples/sec; 0.539 sec/batch; 47h:54m:14s remains)
INFO - root - 2017-12-11 04:19:11.498281: step 12330, loss = 0.69, batch loss = 0.63 (14.6 examples/sec; 0.547 sec/batch; 48h:40m:47s remains)
INFO - root - 2017-12-11 04:19:17.032833: step 12340, loss = 0.70, batch loss = 0.64 (14.3 examples/sec; 0.559 sec/batch; 49h:40m:49s remains)
INFO - root - 2017-12-11 04:19:22.530830: step 12350, loss = 0.71, batch loss = 0.65 (14.4 examples/sec; 0.557 sec/batch; 49h:33m:04s remains)
INFO - root - 2017-12-11 04:19:27.979433: step 12360, loss = 0.70, batch loss = 0.64 (14.5 examples/sec; 0.552 sec/batch; 49h:04m:12s remains)
INFO - root - 2017-12-11 04:19:33.491491: step 12370, loss = 0.69, batch loss = 0.63 (14.3 examples/sec; 0.560 sec/batch; 49h:49m:18s remains)
INFO - root - 2017-12-11 04:19:38.994345: step 12380, loss = 0.70, batch loss = 0.64 (14.9 examples/sec; 0.538 sec/batch; 47h:50m:41s remains)
INFO - root - 2017-12-11 04:19:44.208508: step 12390, loss = 0.71, batch loss = 0.65 (14.1 examples/sec; 0.569 sec/batch; 50h:36m:04s remains)
INFO - root - 2017-12-11 04:19:49.845173: step 12400, loss = 0.70, batch loss = 0.64 (14.5 examples/sec; 0.553 sec/batch; 49h:11m:42s remains)
2017-12-11 04:19:50.453632: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.091127843 0.075536065 0.065382995 0.077855378 0.11540506 0.17059916 0.23395592 0.29143625 0.32895 0.33481196 0.30765364 0.26531038 0.2306077 0.21725377 0.2218394][0.1369312 0.10167811 0.073761053 0.08151415 0.12453766 0.1865449 0.2532478 0.31118011 0.3480441 0.35414308 0.32897466 0.29113176 0.26361445 0.26004696 0.27514297][0.19565843 0.13680778 0.089327343 0.091877908 0.13880546 0.20251667 0.26311922 0.30968365 0.33503523 0.33491114 0.31173438 0.28365076 0.27051669 0.28253511 0.31164461][0.24938942 0.17464934 0.11543576 0.11844917 0.17134687 0.23570481 0.28653187 0.31614614 0.32335159 0.31107953 0.28549215 0.26392433 0.26241112 0.28719953 0.32756817][0.28702107 0.21290219 0.15837958 0.17311607 0.23666115 0.30212605 0.34176937 0.3524659 0.33951148 0.31254768 0.28030092 0.25846693 0.26036558 0.29000518 0.33510202][0.31584671 0.25698811 0.22162119 0.25636426 0.33258879 0.39777461 0.4249773 0.41692486 0.38503641 0.34375644 0.30334249 0.27774769 0.27884492 0.30955195 0.35589829][0.33364466 0.2978937 0.28729674 0.34217507 0.4283011 0.49189985 0.50973225 0.48938844 0.44574934 0.39431462 0.34608254 0.31441545 0.31073603 0.33783317 0.38038039][0.33441696 0.32578558 0.34045959 0.41003883 0.49994755 0.56049371 0.57383907 0.54982865 0.50293106 0.44655806 0.39192373 0.35216194 0.33882013 0.35524693 0.38647044][0.30531323 0.32193089 0.35442781 0.42633298 0.508623 0.56104392 0.57236284 0.55203176 0.51063389 0.45723742 0.4027814 0.35987511 0.33907208 0.34375221 0.36089876][0.24880953 0.27706745 0.31319696 0.37354767 0.43746305 0.4757778 0.4836829 0.46987706 0.44057703 0.39918935 0.35425252 0.31620997 0.29410982 0.29148859 0.29809865][0.17813013 0.20317908 0.22990979 0.26936865 0.30893183 0.32907471 0.33004659 0.32051629 0.30363512 0.27763203 0.24750142 0.22066744 0.20403053 0.20068929 0.20299286][0.094567984 0.10809734 0.11973609 0.13705413 0.1538724 0.15826707 0.15301806 0.14589554 0.13810651 0.12503362 0.10869882 0.093597129 0.084134936 0.082372226 0.083456919][0.013306513 0.014391083 0.012777869 0.013011107 0.013337641 0.0074609634 -0.0011088358 -0.0064233197 -0.0086377813 -0.01295226 -0.019147322 -0.025013624 -0.028348405 -0.028325526 -0.027254678][-0.050449006 -0.058125682 -0.067503884 -0.076654486 -0.085114554 -0.095243551 -0.10367455 -0.10662075 -0.10514013 -0.10386442 -0.10359561 -0.10358199 -0.10292304 -0.10132638 -0.099435404][-0.089866474 -0.10154539 -0.11243774 -0.1232661 -0.13318862 -0.14265016 -0.14917332 -0.15066166 -0.14810252 -0.14470917 -0.14178406 -0.13958424 -0.13769196 -0.13556115 -0.13292691]]...]
INFO - root - 2017-12-11 04:19:55.986089: step 12410, loss = 0.70, batch loss = 0.64 (14.4 examples/sec; 0.554 sec/batch; 49h:16m:50s remains)
INFO - root - 2017-12-11 04:20:01.489362: step 12420, loss = 0.68, batch loss = 0.62 (14.0 examples/sec; 0.572 sec/batch; 50h:53m:17s remains)
INFO - root - 2017-12-11 04:20:07.025198: step 12430, loss = 0.69, batch loss = 0.63 (14.4 examples/sec; 0.554 sec/batch; 49h:14m:10s remains)
INFO - root - 2017-12-11 04:20:12.548281: step 12440, loss = 0.70, batch loss = 0.64 (14.7 examples/sec; 0.546 sec/batch; 48h:32m:36s remains)
INFO - root - 2017-12-11 04:20:18.018348: step 12450, loss = 0.70, batch loss = 0.64 (14.8 examples/sec; 0.540 sec/batch; 47h:59m:19s remains)
INFO - root - 2017-12-11 04:20:23.474770: step 12460, loss = 0.70, batch loss = 0.65 (14.9 examples/sec; 0.537 sec/batch; 47h:46m:12s remains)
INFO - root - 2017-12-11 04:20:28.918602: step 12470, loss = 0.71, batch loss = 0.65 (13.9 examples/sec; 0.576 sec/batch; 51h:13m:14s remains)
INFO - root - 2017-12-11 04:20:34.507903: step 12480, loss = 0.68, batch loss = 0.63 (14.5 examples/sec; 0.551 sec/batch; 48h:57m:35s remains)
INFO - root - 2017-12-11 04:20:39.739860: step 12490, loss = 0.71, batch loss = 0.65 (13.7 examples/sec; 0.583 sec/batch; 51h:47m:39s remains)
INFO - root - 2017-12-11 04:20:45.314121: step 12500, loss = 0.68, batch loss = 0.62 (14.4 examples/sec; 0.556 sec/batch; 49h:23m:21s remains)
2017-12-11 04:20:45.931209: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.22408617 0.24409521 0.24861115 0.2373874 0.21828791 0.20328102 0.18935609 0.17080565 0.15085499 0.13182186 0.10827871 0.076246127 0.050419062 0.040151611 0.046433222][0.28794959 0.30534878 0.30607417 0.29211307 0.26997566 0.25219235 0.23525111 0.21252275 0.18558496 0.15819278 0.12684159 0.088240437 0.057570342 0.044330295 0.050239336][0.32374811 0.33762318 0.33454457 0.3193846 0.29683119 0.278519 0.26048964 0.23552465 0.20396464 0.1709199 0.13606629 0.096267082 0.064248651 0.048491932 0.052412439][0.32104373 0.33326048 0.33133736 0.32083663 0.30407029 0.29025802 0.27399951 0.24780723 0.21190248 0.17463712 0.13922374 0.10224978 0.072488263 0.056302622 0.059076816][0.29098451 0.30574989 0.31127983 0.31159869 0.30553585 0.29936451 0.28575945 0.25736964 0.21612315 0.17545477 0.14247715 0.11247045 0.0887378 0.074625306 0.0771892][0.24966075 0.26837415 0.28442848 0.29766798 0.30304843 0.30469102 0.29430529 0.26402658 0.21802118 0.17545828 0.147404 0.12724483 0.11190058 0.10100452 0.10311101][0.20635696 0.22802407 0.25328282 0.27725217 0.29149503 0.2997784 0.2951228 0.26773265 0.22141248 0.17990455 0.1585416 0.1486042 0.14094205 0.13245508 0.13276404][0.16708483 0.19124316 0.22485173 0.25816676 0.2794314 0.29272914 0.29447937 0.27339411 0.23028928 0.19095783 0.17483476 0.17289972 0.17111208 0.1642714 0.16229232][0.13821827 0.16245702 0.2014503 0.24243696 0.26997158 0.28748867 0.29513812 0.28152442 0.24356028 0.20633283 0.19211204 0.19341286 0.19426982 0.18818462 0.18432987][0.13503268 0.15545292 0.193068 0.23619649 0.26694414 0.28635389 0.29711318 0.28960657 0.25837472 0.22466013 0.21102004 0.2121371 0.21304588 0.20722729 0.20202473][0.14493771 0.15864396 0.18913716 0.22891062 0.26002333 0.28002387 0.29190329 0.28926671 0.26703632 0.2408942 0.23036808 0.23108315 0.23116766 0.22541416 0.21923368][0.1628032 0.17021127 0.19097923 0.22301571 0.25109628 0.26996446 0.28087333 0.28083929 0.26758638 0.25125909 0.24616121 0.24722578 0.24629466 0.2400196 0.23199946][0.18398313 0.1861295 0.19635926 0.21857417 0.24174735 0.25899556 0.26836976 0.26890558 0.26221889 0.25445494 0.2543526 0.25551453 0.25370383 0.24737102 0.23848389][0.19085082 0.18727434 0.18709144 0.19891977 0.2165077 0.23263255 0.24205279 0.24397166 0.24287483 0.24213998 0.2456729 0.24593779 0.24264967 0.23697197 0.22958912][0.16794086 0.15942729 0.15108714 0.1539574 0.16604109 0.18098028 0.19137558 0.195753 0.19993532 0.20543495 0.21215537 0.21187861 0.20736116 0.20285863 0.19802558]]...]
INFO - root - 2017-12-11 04:20:51.368208: step 12510, loss = 0.69, batch loss = 0.63 (15.9 examples/sec; 0.503 sec/batch; 44h:42m:40s remains)
INFO - root - 2017-12-11 04:20:56.859758: step 12520, loss = 0.69, batch loss = 0.64 (14.4 examples/sec; 0.556 sec/batch; 49h:26m:21s remains)
INFO - root - 2017-12-11 04:21:02.401485: step 12530, loss = 0.68, batch loss = 0.62 (14.6 examples/sec; 0.550 sec/batch; 48h:51m:03s remains)
INFO - root - 2017-12-11 04:21:07.954856: step 12540, loss = 0.70, batch loss = 0.64 (14.5 examples/sec; 0.552 sec/batch; 49h:05m:27s remains)
INFO - root - 2017-12-11 04:21:13.414616: step 12550, loss = 0.71, batch loss = 0.65 (14.5 examples/sec; 0.553 sec/batch; 49h:10m:23s remains)
INFO - root - 2017-12-11 04:21:18.849228: step 12560, loss = 0.70, batch loss = 0.65 (15.0 examples/sec; 0.533 sec/batch; 47h:23m:38s remains)
INFO - root - 2017-12-11 04:21:24.285525: step 12570, loss = 0.67, batch loss = 0.61 (14.6 examples/sec; 0.547 sec/batch; 48h:34m:03s remains)
INFO - root - 2017-12-11 04:21:29.640518: step 12580, loss = 0.70, batch loss = 0.64 (23.4 examples/sec; 0.343 sec/batch; 30h:26m:43s remains)
INFO - root - 2017-12-11 04:21:34.955168: step 12590, loss = 0.70, batch loss = 0.64 (14.9 examples/sec; 0.537 sec/batch; 47h:44m:58s remains)
INFO - root - 2017-12-11 04:21:40.611137: step 12600, loss = 0.70, batch loss = 0.64 (14.3 examples/sec; 0.561 sec/batch; 49h:51m:39s remains)
2017-12-11 04:21:41.209515: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.40080854 0.3642939 0.33358955 0.32824808 0.33447823 0.34115127 0.35310355 0.3754496 0.38811868 0.37569606 0.33494741 0.28033584 0.21530448 0.14207903 0.073982239][0.4336282 0.40266773 0.37645736 0.37524676 0.38439411 0.39310902 0.40250909 0.41584182 0.41746309 0.39690632 0.35243207 0.29455966 0.22670384 0.15145233 0.081379779][0.43021351 0.40564618 0.38456845 0.38793063 0.40275276 0.41800338 0.42889571 0.434873 0.42591357 0.39763397 0.34984955 0.28911257 0.2193781 0.14539829 0.077919863][0.41638309 0.39213037 0.37179574 0.37867692 0.40303075 0.43176296 0.45243263 0.45845115 0.44335562 0.40717646 0.3527588 0.28470367 0.20851053 0.133406 0.068665028][0.4159742 0.38261971 0.35503682 0.36256784 0.40011182 0.44994587 0.4900355 0.5050987 0.48862883 0.44331682 0.37652516 0.29436085 0.20546296 0.12456667 0.0601955][0.4451423 0.39634648 0.35472465 0.35914791 0.40932295 0.48147494 0.54337043 0.57187378 0.55710012 0.50243735 0.42014912 0.32127133 0.21746732 0.12760064 0.059912875][0.49593556 0.431159 0.37498266 0.37539923 0.43407756 0.52166677 0.59983045 0.64151591 0.63170671 0.57155657 0.47584233 0.36232492 0.24493381 0.14446725 0.069402069][0.53871536 0.46388233 0.39972857 0.39841762 0.45962435 0.55066705 0.63469404 0.68608475 0.68525338 0.6273241 0.52564418 0.40426251 0.27832666 0.16863818 0.084543675][0.53153753 0.45806521 0.39639547 0.39630976 0.45312572 0.53503883 0.61380571 0.66954339 0.68014592 0.63352007 0.53874493 0.4224346 0.29889277 0.18701105 0.097817555][0.4592554 0.39719307 0.34657571 0.34877816 0.39506716 0.45824292 0.52225673 0.5754931 0.59579873 0.56612194 0.48993483 0.39295471 0.28551009 0.18294337 0.097699068][0.33589029 0.28956577 0.25385067 0.25792339 0.29029122 0.33052596 0.37490565 0.41952023 0.44429037 0.43073615 0.37829331 0.3091515 0.22817384 0.14615448 0.0755339][0.19315012 0.16182199 0.13982248 0.14475411 0.16450925 0.18541373 0.21222126 0.24565382 0.26891345 0.26540047 0.23313896 0.19024873 0.13753848 0.081613913 0.03315584][0.06900727 0.048986614 0.036256757 0.04038801 0.051059969 0.060000096 0.074693777 0.09722019 0.11465785 0.11471094 0.096029758 0.072653092 0.043553382 0.012274991 -0.013350205][-0.011666367 -0.024569182 -0.032581668 -0.03076602 -0.026008837 -0.022717237 -0.014979802 -0.0015753361 0.0085774735 0.0083322125 -0.0022934859 -0.013620031 -0.026863571 -0.040050227 -0.048157435][-0.045994058 -0.054225717 -0.059294894 -0.059230264 -0.057497453 -0.056128412 -0.052317441 -0.045943365 -0.042067192 -0.044157397 -0.051164925 -0.057262912 -0.062828116 -0.066380158 -0.065054595]]...]
INFO - root - 2017-12-11 04:21:46.760586: step 12610, loss = 0.71, batch loss = 0.65 (14.5 examples/sec; 0.553 sec/batch; 49h:07m:07s remains)
INFO - root - 2017-12-11 04:21:52.276514: step 12620, loss = 0.70, batch loss = 0.64 (15.3 examples/sec; 0.524 sec/batch; 46h:32m:31s remains)
INFO - root - 2017-12-11 04:21:57.820050: step 12630, loss = 0.70, batch loss = 0.64 (14.3 examples/sec; 0.559 sec/batch; 49h:37m:51s remains)
INFO - root - 2017-12-11 04:22:03.351429: step 12640, loss = 0.68, batch loss = 0.62 (14.4 examples/sec; 0.557 sec/batch; 49h:29m:43s remains)
INFO - root - 2017-12-11 04:22:08.865977: step 12650, loss = 0.68, batch loss = 0.62 (15.2 examples/sec; 0.528 sec/batch; 46h:54m:50s remains)
INFO - root - 2017-12-11 04:22:14.374945: step 12660, loss = 0.70, batch loss = 0.65 (13.7 examples/sec; 0.582 sec/batch; 51h:41m:54s remains)
INFO - root - 2017-12-11 04:22:19.940249: step 12670, loss = 0.69, batch loss = 0.63 (14.0 examples/sec; 0.573 sec/batch; 50h:52m:23s remains)
INFO - root - 2017-12-11 04:22:25.187700: step 12680, loss = 0.69, batch loss = 0.63 (14.4 examples/sec; 0.554 sec/batch; 49h:14m:34s remains)
INFO - root - 2017-12-11 04:22:30.739860: step 12690, loss = 0.70, batch loss = 0.64 (14.8 examples/sec; 0.542 sec/batch; 48h:10m:13s remains)
INFO - root - 2017-12-11 04:22:36.342133: step 12700, loss = 0.68, batch loss = 0.63 (14.6 examples/sec; 0.549 sec/batch; 48h:45m:48s remains)
2017-12-11 04:22:36.927782: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.081502385 0.053652521 0.023956658 0.0043106405 -0.0012964344 0.0051303864 0.013677632 0.018016031 0.020925252 0.025715418 0.039464831 0.069384724 0.12527524 0.19250843 0.24548653][0.048251651 0.012636193 -0.022074487 -0.04276168 -0.045417823 -0.032800775 -0.014076485 0.0048540044 0.024377054 0.044309497 0.070453323 0.10826965 0.16813786 0.23603413 0.28641546][0.043585222 0.0032462312 -0.032370333 -0.047720931 -0.0397518 -0.012680848 0.024288636 0.063454114 0.10026854 0.12946384 0.15620194 0.18574609 0.22867815 0.27547768 0.3067615][0.06561549 0.028197512 0.00015243531 -0.00043385316 0.0269162 0.074538447 0.13318278 0.19287877 0.24264076 0.27176344 0.28647023 0.29465863 0.30601045 0.31788126 0.32162547][0.099615149 0.076680452 0.066976674 0.089353777 0.14034271 0.20781904 0.28461909 0.36026448 0.4173533 0.439944 0.43693036 0.42096075 0.3989619 0.37589747 0.35560542][0.14096992 0.14253411 0.15829356 0.20523202 0.27640826 0.35681611 0.44389215 0.52811712 0.5865435 0.59937781 0.57897156 0.54349113 0.49614418 0.44946679 0.41747907][0.19185704 0.22225232 0.26380458 0.32961383 0.41128758 0.49440905 0.58207 0.66576409 0.71856976 0.71896994 0.68318117 0.63436604 0.57177472 0.514917 0.48468882][0.25243717 0.30880964 0.36934298 0.44156227 0.5189352 0.59224224 0.66915554 0.74096578 0.77972627 0.76452518 0.71509933 0.65838641 0.59085393 0.53727829 0.51972693][0.31770605 0.38950473 0.45506522 0.51739568 0.57541406 0.62820303 0.68476164 0.73413271 0.75049418 0.71720546 0.65595073 0.59498328 0.52971596 0.4869006 0.484806][0.36893666 0.43482646 0.48480451 0.52080947 0.54969937 0.57833314 0.61261672 0.63725185 0.63144445 0.58510834 0.51892245 0.45902461 0.40120032 0.37009373 0.37852091][0.386494 0.42568576 0.44256836 0.44284621 0.44123554 0.44913614 0.46609211 0.47222048 0.45297059 0.40439895 0.34375292 0.29237548 0.24681769 0.22658245 0.23887655][0.37294266 0.375623 0.35252604 0.317133 0.29054525 0.28490895 0.29257771 0.29039025 0.26961613 0.23078194 0.18604963 0.14989524 0.11952199 0.10790781 0.11768867][0.34840024 0.31791157 0.26002318 0.1972892 0.15510876 0.14361615 0.14918049 0.14745039 0.13455689 0.11360197 0.091465153 0.075155236 0.061748069 0.057706065 0.062163312][0.33577529 0.28748459 0.21063967 0.13490915 0.087196827 0.075251527 0.082382716 0.085158728 0.083068654 0.080209889 0.079302877 0.080427289 0.079834729 0.079931758 0.078184456][0.33591989 0.28659698 0.20772272 0.13205926 0.085685179 0.075420752 0.08453951 0.091880679 0.0984589 0.10826928 0.12134028 0.1329235 0.13862905 0.13926876 0.13242424]]...]
INFO - root - 2017-12-11 04:22:42.420977: step 12710, loss = 0.69, batch loss = 0.63 (14.3 examples/sec; 0.558 sec/batch; 49h:33m:58s remains)
INFO - root - 2017-12-11 04:22:47.940399: step 12720, loss = 0.69, batch loss = 0.63 (14.5 examples/sec; 0.550 sec/batch; 48h:53m:07s remains)
INFO - root - 2017-12-11 04:22:53.415209: step 12730, loss = 0.71, batch loss = 0.65 (14.9 examples/sec; 0.538 sec/batch; 47h:45m:04s remains)
INFO - root - 2017-12-11 04:22:58.926696: step 12740, loss = 0.71, batch loss = 0.66 (15.1 examples/sec; 0.531 sec/batch; 47h:10m:17s remains)
INFO - root - 2017-12-11 04:23:04.389504: step 12750, loss = 0.71, batch loss = 0.65 (14.7 examples/sec; 0.545 sec/batch; 48h:22m:43s remains)
INFO - root - 2017-12-11 04:23:09.910851: step 12760, loss = 0.70, batch loss = 0.64 (15.1 examples/sec; 0.531 sec/batch; 47h:11m:58s remains)
INFO - root - 2017-12-11 04:23:15.398626: step 12770, loss = 0.71, batch loss = 0.65 (15.0 examples/sec; 0.533 sec/batch; 47h:22m:31s remains)
INFO - root - 2017-12-11 04:23:20.584547: step 12780, loss = 0.69, batch loss = 0.63 (14.2 examples/sec; 0.565 sec/batch; 50h:09m:33s remains)
INFO - root - 2017-12-11 04:23:26.069255: step 12790, loss = 0.71, batch loss = 0.65 (14.6 examples/sec; 0.548 sec/batch; 48h:41m:54s remains)
INFO - root - 2017-12-11 04:23:31.570046: step 12800, loss = 0.71, batch loss = 0.65 (14.7 examples/sec; 0.543 sec/batch; 48h:12m:22s remains)
2017-12-11 04:23:32.159225: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.35954383 0.43382186 0.51772511 0.59032303 0.628957 0.61348265 0.54169452 0.42459512 0.28525764 0.14732838 0.034400787 -0.039627474 -0.078521878 -0.090285756 -0.0876441][0.33993897 0.38142672 0.43776259 0.49698177 0.53988606 0.54422927 0.50032872 0.41007242 0.28855824 0.15673038 0.042745247 -0.034679819 -0.076765887 -0.091369845 -0.089814372][0.29794005 0.30428925 0.32696894 0.36273396 0.39591241 0.40573245 0.38022137 0.31617731 0.22165586 0.11182811 0.013777207 -0.052272394 -0.086109407 -0.096007712 -0.091315039][0.25364235 0.24146363 0.24590987 0.26733363 0.28970236 0.29422536 0.27084941 0.2172498 0.13936542 0.049063604 -0.031116873 -0.0825221 -0.10400679 -0.10500506 -0.094243728][0.20913529 0.2034442 0.21592161 0.24557036 0.27197114 0.27558059 0.24793419 0.19044144 0.11167981 0.024813717 -0.050779406 -0.098173119 -0.11514468 -0.11163421 -0.096808679][0.16570778 0.18668988 0.22900565 0.28792986 0.33785385 0.3559756 0.33270267 0.27090955 0.18197972 0.082319476 -0.0074806558 -0.06986542 -0.09960784 -0.10469504 -0.094589733][0.13296708 0.18387507 0.25942507 0.35222495 0.43509355 0.48205271 0.47937459 0.42605513 0.3320682 0.21543933 0.10026371 0.0086296387 -0.049322911 -0.076946981 -0.082582705][0.12370292 0.18907943 0.27924281 0.38799638 0.49328002 0.56951064 0.59816384 0.57018983 0.48841518 0.36721188 0.23255713 0.11141252 0.020901566 -0.034796692 -0.061894044][0.14643002 0.20174587 0.2754963 0.367701 0.46768147 0.55579889 0.6111896 0.61668354 0.56522059 0.4621349 0.32950142 0.19537003 0.083027571 0.0051543582 -0.040386409][0.19860844 0.22456509 0.2554937 0.30144244 0.36654279 0.44066685 0.504035 0.53463227 0.51795024 0.44939381 0.34215814 0.2196493 0.10712092 0.023557808 -0.029114572][0.25293481 0.24550258 0.22740315 0.21871218 0.23589259 0.27793819 0.32788911 0.36330441 0.36794424 0.33203685 0.25971729 0.16692434 0.075845771 0.0065230411 -0.037476335][0.27518758 0.24447243 0.19445756 0.1499041 0.13179745 0.14323401 0.16955827 0.1914575 0.19662127 0.17646453 0.13036944 0.067752689 0.0056087342 -0.039554082 -0.065093383][0.24639297 0.21066609 0.15819709 0.11240465 0.089961156 0.0910991 0.10091532 0.10376046 0.093489721 0.06733264 0.026116511 -0.022810131 -0.066013873 -0.091777928 -0.10024832][0.17911665 0.15506181 0.1258685 0.10955672 0.11429293 0.13247973 0.14571421 0.13994744 0.11312704 0.068138719 0.010877456 -0.048567478 -0.095605627 -0.12006597 -0.12396143][0.10787348 0.10487785 0.110415 0.13566384 0.18164819 0.23362328 0.26905903 0.27334145 0.24348159 0.18317492 0.10209862 0.015367147 -0.057221994 -0.10229813 -0.12000414]]...]
INFO - root - 2017-12-11 04:23:37.659361: step 12810, loss = 0.69, batch loss = 0.63 (14.8 examples/sec; 0.540 sec/batch; 47h:59m:13s remains)
INFO - root - 2017-12-11 04:23:43.140516: step 12820, loss = 0.71, batch loss = 0.65 (14.8 examples/sec; 0.541 sec/batch; 48h:02m:01s remains)
INFO - root - 2017-12-11 04:23:48.666230: step 12830, loss = 0.70, batch loss = 0.64 (14.2 examples/sec; 0.565 sec/batch; 50h:10m:42s remains)
INFO - root - 2017-12-11 04:23:54.150673: step 12840, loss = 0.70, batch loss = 0.64 (14.1 examples/sec; 0.568 sec/batch; 50h:23m:39s remains)
INFO - root - 2017-12-11 04:23:59.611317: step 12850, loss = 0.70, batch loss = 0.65 (14.7 examples/sec; 0.546 sec/batch; 48h:28m:43s remains)
INFO - root - 2017-12-11 04:24:05.171097: step 12860, loss = 0.69, batch loss = 0.63 (14.6 examples/sec; 0.547 sec/batch; 48h:33m:58s remains)
INFO - root - 2017-12-11 04:24:10.683875: step 12870, loss = 0.72, batch loss = 0.66 (14.4 examples/sec; 0.554 sec/batch; 49h:11m:12s remains)
INFO - root - 2017-12-11 04:24:15.946150: step 12880, loss = 0.69, batch loss = 0.63 (14.5 examples/sec; 0.552 sec/batch; 48h:59m:50s remains)
INFO - root - 2017-12-11 04:24:21.423005: step 12890, loss = 0.69, batch loss = 0.63 (14.3 examples/sec; 0.560 sec/batch; 49h:41m:50s remains)
INFO - root - 2017-12-11 04:24:26.925497: step 12900, loss = 0.68, batch loss = 0.62 (15.0 examples/sec; 0.534 sec/batch; 47h:22m:10s remains)
2017-12-11 04:24:27.505692: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.015680224 0.00040062526 -0.0010521641 0.0059366762 0.01924764 0.037010331 0.052226827 0.056944761 0.048500694 0.029788241 0.0049979365 -0.017283823 -0.024071675 -0.0004552193 0.054217029][0.037155624 0.012969985 0.0062798397 0.014253208 0.035435311 0.065636449 0.093537092 0.10663022 0.10045467 0.078683108 0.045981716 0.0095006032 -0.016174996 -0.011812077 0.029690284][0.066929467 0.033526819 0.01952818 0.028353669 0.05977685 0.1061191 0.15032068 0.17460415 0.17265016 0.14829688 0.10785985 0.058312938 0.01406766 -0.0036669695 0.017366674][0.11509458 0.073912449 0.050818611 0.058170512 0.0981826 0.15933682 0.21894573 0.25449893 0.25813928 0.23153506 0.1827288 0.12150776 0.062296908 0.025577748 0.026047528][0.17339608 0.1274368 0.095714971 0.09997388 0.1460278 0.21894926 0.29130852 0.33717835 0.34758058 0.31985226 0.26284108 0.19102778 0.11939249 0.066333167 0.047474474][0.2237246 0.17749195 0.14100623 0.14394853 0.19524039 0.27765396 0.3603805 0.41491377 0.43077686 0.40088138 0.33532894 0.25412461 0.17338097 0.10898112 0.075793959][0.26796383 0.2257067 0.19027433 0.19450895 0.24796535 0.33283284 0.41757661 0.47307613 0.48788238 0.45310459 0.38036355 0.2939972 0.2108029 0.14430781 0.10644691][0.31246942 0.27633 0.2446357 0.24835005 0.29583284 0.3711555 0.4448179 0.48967946 0.49472651 0.45230317 0.37514699 0.28966063 0.2129375 0.15603043 0.12552485][0.35342249 0.32659909 0.30158272 0.30447415 0.34085366 0.39729542 0.44850841 0.47270939 0.46077362 0.40796053 0.32794365 0.24760655 0.1842501 0.14604169 0.13275687][0.38547492 0.36930898 0.35247651 0.35523134 0.37925467 0.41231856 0.43488446 0.43273994 0.39915046 0.33355007 0.2513347 0.17915156 0.13356362 0.11891504 0.12825848][0.41205156 0.40583956 0.39541507 0.3964889 0.40672004 0.4149445 0.4077763 0.37817159 0.32224581 0.24462669 0.16334736 0.10317601 0.077511482 0.085592531 0.11513349][0.43350759 0.43572414 0.43042874 0.42956209 0.42793846 0.41575697 0.38480982 0.33275446 0.25884449 0.17254142 0.093652092 0.043722332 0.032742593 0.055384081 0.096158385][0.42825362 0.43616176 0.43668759 0.43890387 0.43504944 0.41526827 0.37462941 0.31250161 0.2301558 0.13995989 0.062246233 0.015250351 0.0071825106 0.031015955 0.071309835][0.39541879 0.40637484 0.4138473 0.42503342 0.42968485 0.41663039 0.38246909 0.32581154 0.247408 0.15941228 0.080770873 0.027809022 0.0092335893 0.020380471 0.049058229][0.34540802 0.35385498 0.3656868 0.388404 0.40978169 0.41558939 0.40132076 0.363996 0.30235219 0.22527181 0.14766683 0.085298575 0.048447926 0.037282694 0.045475084]]...]
INFO - root - 2017-12-11 04:24:32.980035: step 12910, loss = 0.70, batch loss = 0.64 (14.8 examples/sec; 0.542 sec/batch; 48h:04m:26s remains)
INFO - root - 2017-12-11 04:24:38.473314: step 12920, loss = 0.70, batch loss = 0.64 (14.4 examples/sec; 0.554 sec/batch; 49h:12m:14s remains)
INFO - root - 2017-12-11 04:24:43.981527: step 12930, loss = 0.70, batch loss = 0.64 (14.0 examples/sec; 0.571 sec/batch; 50h:43m:53s remains)
INFO - root - 2017-12-11 04:24:49.434232: step 12940, loss = 0.70, batch loss = 0.64 (14.6 examples/sec; 0.549 sec/batch; 48h:43m:24s remains)
INFO - root - 2017-12-11 04:24:54.864702: step 12950, loss = 0.71, batch loss = 0.66 (15.0 examples/sec; 0.535 sec/batch; 47h:28m:56s remains)
INFO - root - 2017-12-11 04:25:00.350294: step 12960, loss = 0.69, batch loss = 0.63 (14.5 examples/sec; 0.551 sec/batch; 48h:54m:25s remains)
INFO - root - 2017-12-11 04:25:05.480000: step 12970, loss = 0.70, batch loss = 0.64 (14.3 examples/sec; 0.561 sec/batch; 49h:44m:58s remains)
INFO - root - 2017-12-11 04:25:10.926501: step 12980, loss = 0.68, batch loss = 0.62 (14.9 examples/sec; 0.538 sec/batch; 47h:44m:42s remains)
INFO - root - 2017-12-11 04:25:16.470756: step 12990, loss = 0.67, batch loss = 0.61 (14.9 examples/sec; 0.536 sec/batch; 47h:34m:59s remains)
INFO - root - 2017-12-11 04:25:21.996436: step 13000, loss = 0.69, batch loss = 0.63 (14.2 examples/sec; 0.562 sec/batch; 49h:50m:32s remains)
2017-12-11 04:25:22.594462: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.044430703 -0.04419234 -0.039241545 -0.034621309 -0.031332884 -0.028524339 -0.025513871 -0.020808244 -0.012308075 -0.00053328852 0.011120739 0.017895704 0.016749058 0.009058876 -0.0017913647][-0.042593773 -0.043089777 -0.03492447 -0.025001438 -0.015582877 -0.007071137 0.00012338687 0.0075366106 0.018933102 0.034933291 0.05139352 0.060958017 0.059383519 0.048387256 0.032787055][-0.026551023 -0.029684033 -0.01836019 -0.0014495107 0.017579831 0.0357634 0.049536381 0.059063118 0.07003206 0.085839815 0.10203741 0.10920649 0.10351865 0.08746668 0.067362644][0.00752808 0.00064442446 0.01416169 0.038488481 0.07012438 0.10243422 0.12613194 0.13754927 0.14355068 0.15088795 0.15644558 0.15152119 0.13476907 0.11086822 0.087002859][0.061190225 0.050503314 0.065686874 0.098630168 0.14701387 0.19966859 0.23886946 0.25408304 0.25079817 0.23889351 0.2190638 0.18777378 0.15029022 0.11477704 0.088147342][0.13029285 0.11554732 0.13199347 0.1757718 0.24604274 0.32603383 0.38773242 0.41085365 0.39595473 0.355337 0.29645714 0.22577952 0.15860561 0.10754586 0.078139715][0.2052296 0.18562797 0.20083527 0.25535178 0.34982157 0.4607701 0.5488742 0.58303547 0.55755532 0.48620591 0.38444594 0.27089632 0.17152055 0.10343847 0.069675319][0.26883632 0.24344088 0.25426582 0.31515357 0.42818263 0.56316507 0.6713081 0.714087 0.68112636 0.58687115 0.45319307 0.30871925 0.18624932 0.105606 0.0675702][0.29898944 0.27110711 0.27648327 0.33601022 0.4529261 0.59309071 0.70430189 0.74739069 0.71068668 0.60805833 0.46462154 0.31314209 0.18670624 0.10422441 0.065002196][0.2987535 0.27351728 0.27297434 0.32130331 0.42321077 0.54600585 0.64141071 0.67595184 0.63931811 0.5435403 0.41262969 0.27682996 0.16466911 0.091098376 0.055421229][0.28619602 0.2681984 0.26159579 0.29061329 0.36184859 0.44926435 0.51456821 0.53335011 0.49876642 0.42004496 0.31600559 0.20971449 0.12252212 0.064951271 0.037393909][0.27664837 0.26943275 0.25908756 0.26770541 0.30278164 0.34664056 0.37426674 0.37150753 0.33673581 0.27642483 0.20240477 0.12868728 0.069260716 0.031003565 0.015056931][0.27931136 0.2823374 0.27166647 0.264736 0.26805231 0.27063069 0.26074991 0.23390165 0.19429463 0.14706147 0.097747207 0.05221758 0.018289726 -0.00059632113 -0.0035299456][0.29497337 0.30311722 0.29023829 0.26953077 0.24658068 0.21570268 0.17596912 0.13024484 0.087097384 0.05024353 0.020523073 -0.0023002741 -0.015082028 -0.017456964 -0.0095234225][0.31927058 0.32628468 0.30507988 0.26800013 0.22155122 0.16584007 0.10791845 0.055092018 0.016601078 -0.0071028294 -0.019442964 -0.024695531 -0.022522103 -0.014514901 -0.0014688035]]...]
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-SGD-lr0.001-clip50-initfromfixqian30k-fixqian
INFO - root - 2017-12-11 04:25:28.093586: step 13010, loss = 0.67, batch loss = 0.61 (14.6 examples/sec; 0.546 sec/batch; 48h:28m:44s remains)
INFO - root - 2017-12-11 04:25:33.592626: step 13020, loss = 0.69, batch loss = 0.63 (14.6 examples/sec; 0.548 sec/batch; 48h:39m:40s remains)
INFO - root - 2017-12-11 04:25:39.068569: step 13030, loss = 0.69, batch loss = 0.63 (15.1 examples/sec; 0.530 sec/batch; 46h:59m:21s remains)
INFO - root - 2017-12-11 04:25:44.548244: step 13040, loss = 0.69, batch loss = 0.63 (14.3 examples/sec; 0.558 sec/batch; 49h:31m:35s remains)
INFO - root - 2017-12-11 04:25:50.113195: step 13050, loss = 0.71, batch loss = 0.65 (14.9 examples/sec; 0.536 sec/batch; 47h:34m:24s remains)
INFO - root - 2017-12-11 04:25:55.635797: step 13060, loss = 0.71, batch loss = 0.66 (14.8 examples/sec; 0.540 sec/batch; 47h:54m:30s remains)
INFO - root - 2017-12-11 04:26:00.900183: step 13070, loss = 0.70, batch loss = 0.64 (14.1 examples/sec; 0.566 sec/batch; 50h:14m:48s remains)
INFO - root - 2017-12-11 04:26:06.498037: step 13080, loss = 0.68, batch loss = 0.62 (13.7 examples/sec; 0.584 sec/batch; 51h:51m:01s remains)
INFO - root - 2017-12-11 04:26:11.936616: step 13090, loss = 0.70, batch loss = 0.64 (15.0 examples/sec; 0.535 sec/batch; 47h:27m:57s remains)
INFO - root - 2017-12-11 04:26:17.405732: step 13100, loss = 0.68, batch loss = 0.62 (14.7 examples/sec; 0.546 sec/batch; 48h:26m:15s remains)
2017-12-11 04:26:17.968284: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.038942564 -0.024745712 -0.0075672297 0.0082044667 0.017867921 0.018939231 0.013191067 0.0052266987 0.00085896684 0.0081982566 0.0206262 0.028112579 0.028034296 0.02098887 0.0045496104][-0.013390496 0.0209895 0.061332736 0.0988842 0.12530053 0.13548028 0.13233359 0.12238381 0.11855267 0.1396566 0.17036074 0.19051646 0.19093068 0.17465043 0.13989604][0.018508893 0.07841254 0.14803308 0.21271257 0.261645 0.2871494 0.29225722 0.28284124 0.28132081 0.32300472 0.38116086 0.42165753 0.42343122 0.39493954 0.3359338][0.050825421 0.14019708 0.24160989 0.33340448 0.40456277 0.446608 0.46275014 0.45599562 0.45910648 0.5225262 0.60918885 0.6696431 0.66916013 0.62283 0.534308][0.085305147 0.20654657 0.34129947 0.45976323 0.55099994 0.60792863 0.63549769 0.63479167 0.64437056 0.72189873 0.82424223 0.89131016 0.87764883 0.80314064 0.67970365][0.10834631 0.25232166 0.41215211 0.55184108 0.66006714 0.73384142 0.77976054 0.79436916 0.81307518 0.89355284 0.99304235 1.0477383 1.0077373 0.89811933 0.74241078][0.1091233 0.260194 0.43086246 0.58154225 0.70204669 0.79424936 0.86313039 0.89822513 0.92566705 0.99908453 1.0807177 1.1099014 1.0400388 0.90126067 0.72719461][0.094178759 0.23748566 0.40241304 0.54855007 0.66739941 0.76535064 0.845328 0.89157712 0.9193331 0.9741807 1.0279881 1.0287017 0.93907672 0.7907151 0.621833][0.072392873 0.19543546 0.33929726 0.46675408 0.5702489 0.65789092 0.7314201 0.77471918 0.79355562 0.821709 0.8424468 0.81739897 0.72211415 0.58480889 0.4414058][0.043675728 0.13682371 0.24766214 0.34579304 0.42459291 0.49130672 0.54660976 0.5773406 0.58260339 0.58524477 0.57872915 0.53919643 0.45412612 0.3446984 0.23981012][0.0052083591 0.064010315 0.136057 0.19937238 0.24925661 0.29086837 0.32392919 0.33880311 0.33195919 0.31912461 0.30043259 0.26278791 0.20128214 0.129383 0.066774845][-0.032588825 -0.0043924735 0.032800842 0.06458509 0.088180184 0.1060369 0.11755074 0.11701755 0.10164508 0.082486972 0.063135318 0.03787522 0.0052292291 -0.028808944 -0.054141313][-0.054493632 -0.046098102 -0.032626055 -0.023033794 -0.019132866 -0.020306714 -0.02641434 -0.039179955 -0.058931071 -0.07828296 -0.0928073 -0.10401529 -0.1127275 -0.11853065 -0.11880194][-0.063897066 -0.066019237 -0.065879032 -0.068852738 -0.075890169 -0.087282911 -0.10203812 -0.11901584 -0.1367709 -0.15137883 -0.15969932 -0.16161177 -0.15788591 -0.15039665 -0.13994537][-0.069303952 -0.076320671 -0.081722073 -0.0889636 -0.098412335 -0.11046361 -0.1240705 -0.13739111 -0.14872959 -0.15640907 -0.15887497 -0.15597318 -0.14844693 -0.13829792 -0.12691416]]...]
INFO - root - 2017-12-11 04:26:23.407871: step 13110, loss = 0.71, batch loss = 0.65 (15.1 examples/sec; 0.528 sec/batch; 46h:52m:46s remains)
INFO - root - 2017-12-11 04:26:29.005808: step 13120, loss = 0.71, batch loss = 0.65 (14.7 examples/sec; 0.544 sec/batch; 48h:13m:39s remains)
INFO - root - 2017-12-11 04:26:34.488649: step 13130, loss = 0.69, batch loss = 0.64 (14.4 examples/sec; 0.555 sec/batch; 49h:14m:17s remains)
INFO - root - 2017-12-11 04:26:39.944379: step 13140, loss = 0.71, batch loss = 0.65 (14.6 examples/sec; 0.548 sec/batch; 48h:38m:31s remains)
INFO - root - 2017-12-11 04:26:45.489967: step 13150, loss = 0.70, batch loss = 0.64 (13.9 examples/sec; 0.577 sec/batch; 51h:12m:25s remains)
INFO - root - 2017-12-11 04:26:50.990546: step 13160, loss = 0.70, batch loss = 0.64 (14.7 examples/sec; 0.543 sec/batch; 48h:11m:23s remains)
INFO - root - 2017-12-11 04:26:56.171569: step 13170, loss = 0.70, batch loss = 0.65 (14.4 examples/sec; 0.556 sec/batch; 49h:19m:17s remains)
INFO - root - 2017-12-11 04:27:01.682256: step 13180, loss = 0.70, batch loss = 0.64 (14.5 examples/sec; 0.550 sec/batch; 48h:49m:45s remains)
INFO - root - 2017-12-11 04:27:07.198608: step 13190, loss = 0.68, batch loss = 0.62 (14.9 examples/sec; 0.536 sec/batch; 47h:32m:36s remains)
INFO - root - 2017-12-11 04:27:12.686881: step 13200, loss = 0.71, batch loss = 0.65 (14.2 examples/sec; 0.564 sec/batch; 49h:58m:46s remains)
2017-12-11 04:27:13.297022: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.095879465 0.10428037 0.11311002 0.12242083 0.13381168 0.14365782 0.14865619 0.14601196 0.13627046 0.12173484 0.10519326 0.092121221 0.0908398 0.10445096 0.12605533][0.12758306 0.13318565 0.13908353 0.1503884 0.17009799 0.19320272 0.21350382 0.22170015 0.21412942 0.19104272 0.15894818 0.13005692 0.11871982 0.13091324 0.15655184][0.15922394 0.15960895 0.16182511 0.17763917 0.21143135 0.25641495 0.29988086 0.32327047 0.31678882 0.27865955 0.2226515 0.17062791 0.1447594 0.15307347 0.18132058][0.1873114 0.18126093 0.17918158 0.19889876 0.24690284 0.314761 0.38185066 0.42017055 0.41374063 0.35853273 0.27771688 0.20352675 0.16458698 0.16884114 0.196995][0.20561941 0.19312258 0.18590122 0.20615648 0.26293683 0.34707826 0.4318009 0.48197749 0.47618356 0.40833145 0.31062758 0.22268912 0.17561018 0.17470284 0.19749823][0.20535456 0.19031116 0.18102185 0.20268558 0.26626074 0.36239097 0.46002388 0.51968139 0.51597261 0.44061372 0.33202589 0.23362677 0.1769125 0.16522615 0.17561744][0.18814807 0.17774116 0.17545603 0.20807973 0.28654623 0.39917648 0.510568 0.57895768 0.57568347 0.49046388 0.36616114 0.25030631 0.17660373 0.1476157 0.14107563][0.16317989 0.16370183 0.17564096 0.22559763 0.32174304 0.44885868 0.56817538 0.63852638 0.63091975 0.53468353 0.3956947 0.26434809 0.17557368 0.13010345 0.10703554][0.1407356 0.15316562 0.17892593 0.24170762 0.34562117 0.47348195 0.58681983 0.64896756 0.6340459 0.53352219 0.39283 0.25985283 0.16640356 0.11053063 0.0749564][0.12584378 0.14819938 0.18325581 0.24958304 0.34739074 0.46011546 0.554256 0.60105944 0.57992947 0.48566502 0.35769567 0.23640031 0.1472327 0.087541133 0.04552969][0.10984255 0.13962018 0.18035582 0.24364299 0.32635784 0.4139801 0.4809778 0.50835621 0.48240945 0.40169239 0.29638624 0.19684312 0.12066746 0.06539461 0.02457731][0.087918669 0.11880688 0.15878883 0.21296769 0.27635616 0.33651581 0.37570864 0.38377467 0.35467282 0.29079059 0.21380515 0.14382279 0.090265505 0.049877711 0.019412851][0.062614389 0.086443417 0.1160704 0.152613 0.19185774 0.22425464 0.23857266 0.23130818 0.20328774 0.16021164 0.11593064 0.080519296 0.055823032 0.037621435 0.024405492][0.040103037 0.052555956 0.066673018 0.0822215 0.097546376 0.10724415 0.10571454 0.092841282 0.072424732 0.050776094 0.034924708 0.027514732 0.026018342 0.027319452 0.031019434][0.023636013 0.028209409 0.031228941 0.031958483 0.030690584 0.025884368 0.016176697 0.0032306607 -0.0084351161 -0.013911203 -0.011407487 -0.0026225587 0.0089750392 0.022517193 0.038109772]]...]
INFO - root - 2017-12-11 04:27:18.842281: step 13210, loss = 0.69, batch loss = 0.63 (14.7 examples/sec; 0.546 sec/batch; 48h:23m:15s remains)
INFO - root - 2017-12-11 04:27:24.361218: step 13220, loss = 0.70, batch loss = 0.65 (14.5 examples/sec; 0.553 sec/batch; 49h:04m:22s remains)
INFO - root - 2017-12-11 04:27:29.832840: step 13230, loss = 0.69, batch loss = 0.63 (14.4 examples/sec; 0.555 sec/batch; 49h:11m:29s remains)
INFO - root - 2017-12-11 04:27:35.390062: step 13240, loss = 0.71, batch loss = 0.65 (14.3 examples/sec; 0.558 sec/batch; 49h:31m:30s remains)
INFO - root - 2017-12-11 04:27:40.821965: step 13250, loss = 0.71, batch loss = 0.65 (15.2 examples/sec; 0.526 sec/batch; 46h:39m:11s remains)
INFO - root - 2017-12-11 04:27:46.207665: step 13260, loss = 0.70, batch loss = 0.64 (20.5 examples/sec; 0.389 sec/batch; 34h:31m:25s remains)
INFO - root - 2017-12-11 04:27:51.533869: step 13270, loss = 0.70, batch loss = 0.64 (14.6 examples/sec; 0.549 sec/batch; 48h:41m:42s remains)
INFO - root - 2017-12-11 04:27:57.020627: step 13280, loss = 0.71, batch loss = 0.66 (14.8 examples/sec; 0.542 sec/batch; 48h:03m:07s remains)
INFO - root - 2017-12-11 04:28:02.634106: step 13290, loss = 0.71, batch loss = 0.65 (14.4 examples/sec; 0.554 sec/batch; 49h:08m:46s remains)
INFO - root - 2017-12-11 04:28:08.116724: step 13300, loss = 0.68, batch loss = 0.62 (14.8 examples/sec; 0.539 sec/batch; 47h:48m:13s remains)
2017-12-11 04:28:08.709853: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.052513566 0.058821063 0.063696869 0.066689171 0.076558687 0.0923285 0.11031259 0.1358999 0.17155893 0.21014227 0.24885011 0.2905961 0.33170429 0.36765397 0.40113211][0.053465333 0.063622333 0.074220888 0.084502757 0.10326008 0.12758073 0.15296464 0.18394782 0.22263265 0.26104006 0.29569772 0.3319203 0.36784276 0.39980567 0.43091255][0.099287838 0.11030612 0.12192812 0.13398805 0.15657692 0.1856565 0.2147194 0.24571984 0.279498 0.30953631 0.33424103 0.36186627 0.39087471 0.41643372 0.44046247][0.1678198 0.17533852 0.18365698 0.19355342 0.21624979 0.24681526 0.27618775 0.30390757 0.33075985 0.35247576 0.36855263 0.388958 0.41112563 0.42793378 0.43894032][0.23143081 0.23468563 0.23939584 0.2477469 0.27027482 0.30020267 0.32586095 0.34664807 0.36595455 0.38236928 0.39440635 0.41105866 0.4274424 0.43469656 0.431297][0.2704823 0.27533031 0.28301945 0.29641378 0.32345417 0.35384804 0.373347 0.38339806 0.39296633 0.40427303 0.41446507 0.42916098 0.44141939 0.44143328 0.42728758][0.2903505 0.30231887 0.3185837 0.34237042 0.37811825 0.4105725 0.423139 0.41983309 0.41649881 0.42024094 0.428072 0.44141731 0.45152661 0.44825697 0.4287909][0.28317797 0.30500904 0.33336806 0.37117228 0.41901365 0.45651376 0.46430504 0.4478873 0.4292686 0.42158949 0.42531249 0.43914583 0.45209244 0.45241329 0.43485913][0.25481033 0.28899232 0.33122882 0.38282093 0.44075325 0.48178181 0.48488325 0.45634431 0.42207742 0.40145054 0.40038273 0.41716993 0.43854177 0.44969776 0.44118372][0.21357067 0.25932783 0.3129954 0.37359717 0.43507978 0.47465509 0.47247458 0.43519551 0.38892716 0.35637081 0.349651 0.3688525 0.39992169 0.42525643 0.43079421][0.16694795 0.21803898 0.27639377 0.33843172 0.39639747 0.43063325 0.42512104 0.38599911 0.33566055 0.29573745 0.28242677 0.29918769 0.33407271 0.3695685 0.38842398][0.12908512 0.17628384 0.22948311 0.28418806 0.33270839 0.35993138 0.35451365 0.32123202 0.27659255 0.23769066 0.22084504 0.23211025 0.26405483 0.30225837 0.32863376][0.10344572 0.14237045 0.18654469 0.23168764 0.27092186 0.29308912 0.28994867 0.26540107 0.23087305 0.19854853 0.18164597 0.18652323 0.21046686 0.24362858 0.26969418][0.10666732 0.13692357 0.170904 0.2058515 0.23604937 0.25310063 0.25035369 0.23102115 0.20349348 0.17685492 0.16055162 0.15936604 0.17349608 0.19765879 0.21944119][0.13160807 0.15430301 0.17855982 0.20310463 0.22385362 0.23462856 0.22988585 0.21185413 0.18699837 0.16259119 0.14543726 0.13883387 0.14412794 0.15969221 0.17741749]]...]
INFO - root - 2017-12-11 04:28:14.182501: step 13310, loss = 0.70, batch loss = 0.64 (14.8 examples/sec; 0.539 sec/batch; 47h:49m:42s remains)
INFO - root - 2017-12-11 04:28:19.662880: step 13320, loss = 0.70, batch loss = 0.64 (14.6 examples/sec; 0.548 sec/batch; 48h:34m:48s remains)
INFO - root - 2017-12-11 04:28:25.119071: step 13330, loss = 0.68, batch loss = 0.62 (14.6 examples/sec; 0.550 sec/batch; 48h:44m:30s remains)
INFO - root - 2017-12-11 04:28:30.656321: step 13340, loss = 0.70, batch loss = 0.65 (14.2 examples/sec; 0.564 sec/batch; 49h:59m:57s remains)
INFO - root - 2017-12-11 04:28:36.196270: step 13350, loss = 0.70, batch loss = 0.65 (14.7 examples/sec; 0.545 sec/batch; 48h:20m:03s remains)
INFO - root - 2017-12-11 04:28:41.427217: step 13360, loss = 0.71, batch loss = 0.65 (14.1 examples/sec; 0.566 sec/batch; 50h:08m:34s remains)
INFO - root - 2017-12-11 04:28:46.867847: step 13370, loss = 0.71, batch loss = 0.65 (14.8 examples/sec; 0.539 sec/batch; 47h:47m:19s remains)
INFO - root - 2017-12-11 04:28:52.381080: step 13380, loss = 0.70, batch loss = 0.64 (14.5 examples/sec; 0.550 sec/batch; 48h:47m:23s remains)
INFO - root - 2017-12-11 04:28:57.972289: step 13390, loss = 0.71, batch loss = 0.65 (14.2 examples/sec; 0.562 sec/batch; 49h:51m:12s remains)
INFO - root - 2017-12-11 04:29:03.528600: step 13400, loss = 0.69, batch loss = 0.63 (14.6 examples/sec; 0.548 sec/batch; 48h:36m:51s remains)
2017-12-11 04:29:04.145796: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.32501718 0.30781153 0.27932465 0.25503227 0.23350935 0.21137473 0.18175791 0.13864738 0.084294431 0.026015043 -0.025771417 -0.060716007 -0.067012832 -0.042205155 0.00078190619][0.25717694 0.23087779 0.19882172 0.17505454 0.1568777 0.13920255 0.11638431 0.084255032 0.04454777 0.0032479479 -0.031006142 -0.048624519 -0.039125644 -0.00047825626 0.054755747][0.16292991 0.137189 0.11447243 0.10509455 0.10509577 0.10733681 0.1052363 0.093119994 0.070737951 0.043627556 0.019736169 0.00948925 0.024743512 0.069815539 0.13275297][0.090746954 0.073044531 0.067033947 0.079479493 0.10606542 0.13768561 0.16465959 0.17649223 0.1688417 0.14738441 0.12136725 0.1041901 0.11175812 0.15244412 0.21486808][0.072918415 0.065333977 0.074228317 0.1052571 0.15466575 0.21171656 0.26299921 0.29236159 0.29126832 0.26662481 0.23023896 0.19865341 0.19170782 0.22165041 0.27725726][0.12227733 0.12134266 0.13556796 0.17290892 0.23112929 0.29878432 0.35955513 0.39270049 0.38710195 0.35183987 0.30178696 0.25561336 0.23508051 0.25438541 0.30209675][0.21921159 0.2176349 0.22426519 0.25262612 0.30281934 0.36410248 0.41846639 0.44226411 0.42387277 0.37468854 0.31229517 0.25569555 0.22618769 0.23793657 0.27938294][0.32131425 0.31034967 0.2987732 0.30611813 0.33482569 0.3764157 0.41334072 0.42147493 0.3899923 0.3310563 0.26325467 0.20409626 0.17200869 0.17923459 0.21446288][0.38979459 0.36360377 0.32899067 0.30977967 0.31054285 0.32635173 0.34222218 0.33580288 0.29768771 0.23889604 0.17635162 0.1240117 0.095742919 0.10084013 0.12891789][0.39621657 0.35407966 0.29864576 0.25526094 0.23022904 0.22239475 0.22051857 0.20548789 0.16963197 0.12151641 0.074244082 0.036950607 0.018322533 0.024459688 0.046612993][0.33730465 0.28491056 0.21773268 0.15995482 0.11920086 0.0973973 0.086680517 0.071588241 0.04584508 0.014800137 -0.013083718 -0.033160079 -0.040923014 -0.032908045 -0.015706651][0.23171091 0.1778864 0.11205658 0.054609053 0.012870896 -0.010230388 -0.020170758 -0.028680177 -0.040936217 -0.054675493 -0.065547235 -0.07199084 -0.072162189 -0.064308904 -0.052702304][0.10892822 0.062866241 0.010544862 -0.033671807 -0.064305916 -0.079168282 -0.082404539 -0.082366489 -0.082976155 -0.083789878 -0.083143331 -0.081136338 -0.077424 -0.071433634 -0.065593235][0.0068639051 -0.026307289 -0.060268071 -0.086710453 -0.10243589 -0.10670224 -0.10284313 -0.096346065 -0.089751959 -0.083251908 -0.076528855 -0.070167 -0.064567119 -0.060145088 -0.058272906][-0.051151115 -0.071184389 -0.088621065 -0.099818647 -0.10338571 -0.099668533 -0.091305569 -0.081643894 -0.072143555 -0.063011758 -0.054389887 -0.046694752 -0.0406306 -0.037343193 -0.037673965]]...]
INFO - root - 2017-12-11 04:29:09.697159: step 13410, loss = 0.70, batch loss = 0.64 (14.2 examples/sec; 0.563 sec/batch; 49h:55m:37s remains)
INFO - root - 2017-12-11 04:29:15.215563: step 13420, loss = 0.70, batch loss = 0.64 (14.6 examples/sec; 0.548 sec/batch; 48h:36m:03s remains)
INFO - root - 2017-12-11 04:29:20.729471: step 13430, loss = 0.69, batch loss = 0.64 (14.2 examples/sec; 0.563 sec/batch; 49h:52m:44s remains)
INFO - root - 2017-12-11 04:29:26.278501: step 13440, loss = 0.71, batch loss = 0.65 (14.4 examples/sec; 0.556 sec/batch; 49h:17m:27s remains)
INFO - root - 2017-12-11 04:29:31.787365: step 13450, loss = 0.69, batch loss = 0.63 (14.6 examples/sec; 0.547 sec/batch; 48h:30m:21s remains)
INFO - root - 2017-12-11 04:29:37.103053: step 13460, loss = 0.71, batch loss = 0.65 (14.8 examples/sec; 0.540 sec/batch; 47h:53m:18s remains)
INFO - root - 2017-12-11 04:29:42.591212: step 13470, loss = 0.70, batch loss = 0.64 (14.7 examples/sec; 0.543 sec/batch; 48h:08m:04s remains)
INFO - root - 2017-12-11 04:29:48.084800: step 13480, loss = 0.69, batch loss = 0.63 (14.0 examples/sec; 0.572 sec/batch; 50h:42m:43s remains)
INFO - root - 2017-12-11 04:29:53.683394: step 13490, loss = 0.70, batch loss = 0.64 (14.4 examples/sec; 0.557 sec/batch; 49h:21m:00s remains)
INFO - root - 2017-12-11 04:29:59.142285: step 13500, loss = 0.71, batch loss = 0.65 (14.6 examples/sec; 0.547 sec/batch; 48h:28m:35s remains)
2017-12-11 04:29:59.707873: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.036503024 0.031882148 0.024806457 0.018348519 0.018356988 0.028287761 0.042207025 0.051797487 0.061866052 0.078576 0.094504774 0.096136346 0.083392426 0.064890914 0.040740017][0.065581471 0.058289576 0.048865154 0.043529749 0.0485898 0.066352464 0.0882211 0.10332788 0.11749136 0.13877887 0.15925738 0.16192897 0.14505571 0.11832984 0.083011575][0.086304493 0.076160409 0.065262422 0.063599832 0.076746114 0.10426471 0.13428654 0.15432261 0.17095451 0.19329798 0.21448776 0.21668154 0.196594 0.16326581 0.11815665][0.090980351 0.078832567 0.068638839 0.072333954 0.094221748 0.13095538 0.16748768 0.19087012 0.20810322 0.22849144 0.24710283 0.24716716 0.22461173 0.18708041 0.13527507][0.0869539 0.075778283 0.06881357 0.078101747 0.10669909 0.14916092 0.18841556 0.21240021 0.22820958 0.24496742 0.25992316 0.25816095 0.23446576 0.19400141 0.13775916][0.084307089 0.076227069 0.072638877 0.0849617 0.11601815 0.15937251 0.19731456 0.21912846 0.23146209 0.24302061 0.25348809 0.25028095 0.22674336 0.18551661 0.1279642][0.091434665 0.087241113 0.084710285 0.095483184 0.12381173 0.16362745 0.19759458 0.21569827 0.22300456 0.22753999 0.23148496 0.22547014 0.20232728 0.16249086 0.10747921][0.10541008 0.10455326 0.10119939 0.10707328 0.12813929 0.16005191 0.18759866 0.20115256 0.20319614 0.20074366 0.19758211 0.18800019 0.16577512 0.12964961 0.080773659][0.13204364 0.13448104 0.12994617 0.12911828 0.13880171 0.15780088 0.17559129 0.183646 0.18180357 0.17552263 0.16812427 0.15663216 0.13602592 0.10381223 0.06095586][0.17472921 0.17997392 0.17400238 0.16609474 0.16317637 0.16703403 0.17226461 0.17258269 0.16720326 0.16038468 0.15307783 0.14315507 0.12582645 0.09717162 0.057914171][0.21674165 0.22223009 0.21445091 0.20113334 0.18872471 0.18005554 0.17330235 0.16504511 0.15637742 0.15105332 0.14675318 0.14059724 0.12741669 0.10233164 0.064997412][0.2385418 0.24270675 0.23445341 0.22003049 0.20440096 0.18937646 0.17474288 0.1599368 0.14938959 0.1468226 0.14657325 0.14407729 0.13393438 0.11097351 0.073754944][0.22705922 0.2295485 0.22265753 0.21092704 0.19729652 0.18173796 0.164581 0.14720255 0.1372222 0.13845441 0.14273508 0.14359143 0.1356446 0.11425783 0.076826051][0.19073196 0.19153981 0.18597314 0.17669888 0.16563682 0.15173128 0.13560939 0.11954929 0.11272119 0.11829113 0.12662484 0.12981434 0.1229943 0.1028851 0.066558704][0.13402472 0.1331737 0.12833664 0.12042638 0.11127807 0.099842384 0.086754076 0.074353918 0.071664572 0.080625981 0.091227867 0.094969705 0.088059865 0.069433063 0.037129771]]...]
INFO - root - 2017-12-11 04:30:05.200076: step 13510, loss = 0.69, batch loss = 0.63 (14.7 examples/sec; 0.543 sec/batch; 48h:08m:24s remains)
INFO - root - 2017-12-11 04:30:10.707634: step 13520, loss = 0.69, batch loss = 0.63 (14.5 examples/sec; 0.552 sec/batch; 48h:55m:14s remains)
INFO - root - 2017-12-11 04:30:16.144080: step 13530, loss = 0.71, batch loss = 0.65 (14.9 examples/sec; 0.537 sec/batch; 47h:34m:08s remains)
INFO - root - 2017-12-11 04:30:21.607789: step 13540, loss = 0.71, batch loss = 0.65 (14.6 examples/sec; 0.547 sec/batch; 48h:29m:15s remains)
INFO - root - 2017-12-11 04:30:27.167476: step 13550, loss = 0.69, batch loss = 0.63 (14.7 examples/sec; 0.543 sec/batch; 48h:05m:20s remains)
INFO - root - 2017-12-11 04:30:32.348191: step 13560, loss = 0.70, batch loss = 0.64 (14.1 examples/sec; 0.567 sec/batch; 50h:16m:36s remains)
INFO - root - 2017-12-11 04:30:37.863871: step 13570, loss = 0.68, batch loss = 0.62 (14.7 examples/sec; 0.543 sec/batch; 48h:05m:02s remains)
INFO - root - 2017-12-11 04:30:43.316741: step 13580, loss = 0.68, batch loss = 0.62 (14.8 examples/sec; 0.542 sec/batch; 48h:00m:10s remains)
INFO - root - 2017-12-11 04:30:48.822802: step 13590, loss = 0.70, batch loss = 0.64 (14.6 examples/sec; 0.547 sec/batch; 48h:29m:56s remains)
INFO - root - 2017-12-11 04:30:54.475670: step 13600, loss = 0.70, batch loss = 0.64 (13.5 examples/sec; 0.594 sec/batch; 52h:38m:18s remains)
2017-12-11 04:30:55.110554: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.088833317 0.10458449 0.13788281 0.1750517 0.20998967 0.23797181 0.255316 0.26639247 0.27658248 0.29117709 0.29932681 0.29149139 0.26535273 0.2387602 0.23340067][0.089176208 0.11199778 0.15530826 0.20378433 0.24902138 0.28334907 0.29915515 0.30185866 0.30343762 0.31203911 0.31609985 0.30598897 0.28233755 0.26380131 0.26580665][0.087939039 0.11713863 0.1695551 0.22931282 0.28533226 0.32610133 0.3389073 0.33074898 0.32011774 0.32015324 0.32086074 0.3139827 0.30024883 0.29471028 0.30460206][0.090517595 0.12527393 0.18611333 0.25659302 0.322803 0.37061819 0.3829667 0.36561015 0.34131593 0.33012185 0.32821122 0.32813582 0.32748416 0.33430317 0.34835231][0.094977282 0.1330774 0.19932741 0.27764973 0.35223046 0.40750557 0.42299736 0.40123057 0.36529183 0.34281674 0.33931062 0.34885421 0.36317623 0.38166314 0.39748305][0.10021883 0.13737763 0.20327349 0.28352812 0.36246049 0.42437741 0.44583109 0.42528835 0.38280547 0.35221308 0.34803063 0.3660841 0.39304376 0.42042086 0.43737635][0.104644 0.13851753 0.20005067 0.27784094 0.35768074 0.42405465 0.45214882 0.43659854 0.3928633 0.35751703 0.35178182 0.37302545 0.40479761 0.43404627 0.45061508][0.11150716 0.14203693 0.19828661 0.27142194 0.34854352 0.41421598 0.44582149 0.43605533 0.39443678 0.35697702 0.34896111 0.36837265 0.39654928 0.42040315 0.43629384][0.12329775 0.15223081 0.20351176 0.27039111 0.34129974 0.40111634 0.43223193 0.42770371 0.39127949 0.35473737 0.34326559 0.35464075 0.37085068 0.38399738 0.39930862][0.14306578 0.17176795 0.21757431 0.2754969 0.33580184 0.38471705 0.41039902 0.40809718 0.3774468 0.34480312 0.33063918 0.3320094 0.3335841 0.3354238 0.35148397][0.17528367 0.20647004 0.24811493 0.2955347 0.34092811 0.37401551 0.38888273 0.38278183 0.35393819 0.32319319 0.30487892 0.2952987 0.28330395 0.27697217 0.29627338][0.21554667 0.25352487 0.29612112 0.33706185 0.36938208 0.38768822 0.39135975 0.37783316 0.34531051 0.30965742 0.2815671 0.25863698 0.23425169 0.22347252 0.24822319][0.25670221 0.30255565 0.35013592 0.39017221 0.41530702 0.42504865 0.42244714 0.40278426 0.36366686 0.31776747 0.27574441 0.23845887 0.20314848 0.18969998 0.21946466][0.29838887 0.34736603 0.39551672 0.43267015 0.45228395 0.45839697 0.45513919 0.43391743 0.39137051 0.33786342 0.28522298 0.23783073 0.19559105 0.18039966 0.21139945][0.34026474 0.38053513 0.41632205 0.44232321 0.45494586 0.46079922 0.46225631 0.44657436 0.40877718 0.35633034 0.30277163 0.25475636 0.21171321 0.19445822 0.22058624]]...]
INFO - root - 2017-12-11 04:31:00.649968: step 13610, loss = 0.72, batch loss = 0.66 (14.4 examples/sec; 0.554 sec/batch; 49h:02m:32s remains)
INFO - root - 2017-12-11 04:31:06.125919: step 13620, loss = 0.71, batch loss = 0.66 (14.9 examples/sec; 0.539 sec/batch; 47h:42m:33s remains)
INFO - root - 2017-12-11 04:31:11.559698: step 13630, loss = 0.69, batch loss = 0.63 (14.7 examples/sec; 0.544 sec/batch; 48h:12m:24s remains)
INFO - root - 2017-12-11 04:31:17.077308: step 13640, loss = 0.69, batch loss = 0.63 (14.7 examples/sec; 0.543 sec/batch; 48h:07m:45s remains)
INFO - root - 2017-12-11 04:31:22.284980: step 13650, loss = 0.70, batch loss = 0.64 (16.8 examples/sec; 0.477 sec/batch; 42h:13m:28s remains)
INFO - root - 2017-12-11 04:31:27.792449: step 13660, loss = 0.71, batch loss = 0.65 (14.6 examples/sec; 0.546 sec/batch; 48h:22m:44s remains)
INFO - root - 2017-12-11 04:31:33.307496: step 13670, loss = 0.69, batch loss = 0.63 (15.0 examples/sec; 0.533 sec/batch; 47h:14m:44s remains)
INFO - root - 2017-12-11 04:31:38.797881: step 13680, loss = 0.71, batch loss = 0.65 (14.8 examples/sec; 0.541 sec/batch; 47h:54m:51s remains)
INFO - root - 2017-12-11 04:31:44.284791: step 13690, loss = 0.68, batch loss = 0.63 (14.3 examples/sec; 0.560 sec/batch; 49h:34m:41s remains)
INFO - root - 2017-12-11 04:31:49.784567: step 13700, loss = 0.71, batch loss = 0.65 (14.6 examples/sec; 0.550 sec/batch; 48h:40m:56s remains)
2017-12-11 04:31:50.490358: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.087325893 0.075825475 0.062641867 0.049611367 0.037803143 0.027033493 0.014949725 0.0015380002 -0.0098310988 -0.011512788 0.0003519516 0.024186578 0.064150512 0.11790121 0.17391342][0.11000938 0.11155955 0.10700829 0.096148185 0.08219377 0.066971987 0.049535923 0.030929897 0.014905798 0.010234061 0.021869928 0.048232097 0.0942405 0.15638371 0.22085102][0.1341828 0.1596372 0.17485438 0.1754505 0.16454072 0.14515831 0.1176893 0.085691273 0.055141326 0.037295144 0.038730022 0.058477107 0.10289007 0.16691686 0.23517132][0.16651237 0.22091126 0.26274994 0.28207129 0.27836454 0.25459304 0.21304198 0.16183831 0.11064132 0.073508024 0.058516033 0.065035746 0.099863894 0.15649031 0.22064474][0.19607128 0.28042302 0.35339549 0.39816305 0.4074083 0.3828553 0.32941964 0.26095539 0.19059877 0.13471486 0.1017802 0.089961283 0.10570531 0.14281645 0.191574][0.21098079 0.32181317 0.42461964 0.49545726 0.51984209 0.49895784 0.44194478 0.36754146 0.29039216 0.22584684 0.17983781 0.1477461 0.13534388 0.14098029 0.16302508][0.21363609 0.34126586 0.46465889 0.55449194 0.59095693 0.57709342 0.52724427 0.46293381 0.39659488 0.33785167 0.28625709 0.23298371 0.18573397 0.15225795 0.14148991][0.21190706 0.34533846 0.47779897 0.5766021 0.61949295 0.61352241 0.57814538 0.53537989 0.4926087 0.45009884 0.3990773 0.32650349 0.24408327 0.17109519 0.12818196][0.2059148 0.33314925 0.4598074 0.55235821 0.59027612 0.58626819 0.56494915 0.54718572 0.53365505 0.51436174 0.47290012 0.39268875 0.28838587 0.18895233 0.12453643][0.20812406 0.31749889 0.42151058 0.4894971 0.50740069 0.49496683 0.48095235 0.48294377 0.49488723 0.49908787 0.4744581 0.40190285 0.29630727 0.19154637 0.12180024][0.23578477 0.31746718 0.38419724 0.41241884 0.39747456 0.366309 0.35039687 0.36270502 0.39171627 0.41515633 0.410055 0.35670441 0.26825681 0.17742887 0.11653364][0.28429505 0.33196497 0.35475215 0.33912253 0.28925842 0.23634279 0.21211219 0.22490813 0.26059297 0.2956551 0.30782735 0.27840742 0.21737193 0.15162678 0.10797817][0.32254338 0.33330679 0.31472245 0.26302972 0.18751405 0.11813875 0.083992891 0.090284295 0.12402306 0.16401541 0.1907063 0.18724854 0.15852122 0.12258711 0.099348567][0.32528251 0.303008 0.25179803 0.17576145 0.086443529 0.0093943048 -0.031101342 -0.031927772 -0.0030911868 0.038929865 0.078339994 0.10018111 0.10371397 0.097511627 0.094355814][0.28246003 0.238466 0.16895044 0.08411029 -0.0042294352 -0.07643538 -0.11459889 -0.11780582 -0.093063727 -0.051336929 -0.00387072 0.036420505 0.064529322 0.081199192 0.09320391]]...]
INFO - root - 2017-12-11 04:31:56.079072: step 13710, loss = 0.69, batch loss = 0.63 (14.9 examples/sec; 0.536 sec/batch; 47h:28m:27s remains)
INFO - root - 2017-12-11 04:32:01.567215: step 13720, loss = 0.70, batch loss = 0.64 (14.4 examples/sec; 0.555 sec/batch; 49h:07m:02s remains)
INFO - root - 2017-12-11 04:32:07.057855: step 13730, loss = 0.70, batch loss = 0.64 (14.5 examples/sec; 0.554 sec/batch; 49h:00m:55s remains)
INFO - root - 2017-12-11 04:32:12.573810: step 13740, loss = 0.71, batch loss = 0.65 (14.6 examples/sec; 0.547 sec/batch; 48h:27m:49s remains)
INFO - root - 2017-12-11 04:32:17.962103: step 13750, loss = 0.69, batch loss = 0.63 (14.0 examples/sec; 0.572 sec/batch; 50h:38m:10s remains)
INFO - root - 2017-12-11 04:32:23.492427: step 13760, loss = 0.68, batch loss = 0.62 (14.6 examples/sec; 0.546 sec/batch; 48h:21m:04s remains)
INFO - root - 2017-12-11 04:32:28.947054: step 13770, loss = 0.70, batch loss = 0.64 (14.5 examples/sec; 0.552 sec/batch; 48h:54m:24s remains)
INFO - root - 2017-12-11 04:32:34.381954: step 13780, loss = 0.69, batch loss = 0.63 (14.7 examples/sec; 0.543 sec/batch; 48h:02m:41s remains)
INFO - root - 2017-12-11 04:32:39.872341: step 13790, loss = 0.69, batch loss = 0.63 (14.7 examples/sec; 0.545 sec/batch; 48h:12m:44s remains)
INFO - root - 2017-12-11 04:32:45.455080: step 13800, loss = 0.71, batch loss = 0.65 (14.6 examples/sec; 0.547 sec/batch; 48h:27m:22s remains)
2017-12-11 04:32:46.130328: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.052457705 0.086148746 0.10133641 0.1018611 0.10356099 0.11451089 0.1292814 0.13566044 0.12537289 0.1004393 0.071564652 0.054994017 0.056002989 0.062587276 0.0552869][0.11568157 0.16709208 0.19218892 0.19743069 0.20251963 0.2170402 0.23514794 0.24244808 0.22448763 0.18037117 0.12597547 0.087287217 0.075488225 0.078055255 0.0721208][0.17333972 0.24123169 0.27808565 0.29099762 0.30179834 0.32004747 0.33990639 0.34630048 0.31977162 0.25496602 0.17051777 0.10306687 0.072715238 0.069639668 0.069551177][0.21235879 0.29474467 0.34574673 0.37047446 0.39006621 0.41292635 0.43365967 0.4366301 0.39884114 0.3122817 0.19822156 0.10252513 0.0541756 0.047610071 0.057330113][0.24103446 0.34116358 0.41269821 0.45501107 0.48549697 0.51128733 0.52884865 0.523643 0.47068527 0.36212704 0.22242256 0.10423564 0.043352671 0.037335481 0.058035966][0.25013903 0.36704677 0.46240881 0.52690929 0.57045782 0.598014 0.608219 0.59006655 0.52041668 0.39470923 0.23928431 0.10993024 0.044739824 0.042949755 0.072705746][0.22792315 0.35247973 0.46813089 0.55656487 0.6168018 0.64882106 0.65265894 0.62366956 0.54557168 0.42021382 0.27052638 0.1488501 0.090292588 0.09347751 0.12398132][0.18661274 0.30899343 0.43726832 0.54758817 0.6261 0.66429925 0.66139364 0.62212604 0.54496682 0.44037288 0.32304823 0.23296772 0.19525136 0.20345467 0.22094361][0.15219711 0.27026424 0.40633246 0.53388506 0.62669212 0.66599351 0.65027714 0.59646684 0.52243876 0.44780505 0.37861606 0.33668569 0.33006674 0.34095588 0.33074123][0.13319229 0.24931857 0.39002544 0.526938 0.6240319 0.65503985 0.62094069 0.55173713 0.48266023 0.43907368 0.42042208 0.42943856 0.453605 0.46087569 0.41332749][0.10986417 0.21868843 0.3534852 0.48432836 0.57083035 0.58568972 0.53537506 0.45989531 0.40300319 0.39056459 0.41702077 0.46955422 0.51785946 0.51811171 0.43776754][0.073126674 0.16780835 0.28597412 0.39695114 0.46178359 0.458656 0.40076438 0.33273754 0.29826474 0.31883991 0.38363779 0.46746317 0.52838516 0.51806897 0.41431668][0.036778487 0.11307365 0.20698521 0.2889286 0.32641461 0.30610582 0.24634385 0.19006617 0.17556493 0.21851464 0.30419278 0.39943537 0.45814729 0.43591669 0.32408303][0.006650818 0.063323021 0.12931415 0.17867687 0.18845908 0.15388343 0.095110767 0.048254032 0.04301703 0.089526109 0.17203908 0.25653547 0.30124179 0.27281857 0.17415543][-0.0215253 0.016690953 0.056956798 0.078309722 0.067377567 0.026019711 -0.027006382 -0.064881168 -0.067990869 -0.031282891 0.031549271 0.093498372 0.12334076 0.099868909 0.030334102]]...]
INFO - root - 2017-12-11 04:32:51.608541: step 13810, loss = 0.71, batch loss = 0.65 (14.8 examples/sec; 0.541 sec/batch; 47h:51m:42s remains)
INFO - root - 2017-12-11 04:32:57.088826: step 13820, loss = 0.71, batch loss = 0.65 (14.6 examples/sec; 0.548 sec/batch; 48h:28m:50s remains)
INFO - root - 2017-12-11 04:33:02.658519: step 13830, loss = 0.70, batch loss = 0.64 (14.5 examples/sec; 0.553 sec/batch; 48h:58m:00s remains)
INFO - root - 2017-12-11 04:33:08.182729: step 13840, loss = 0.70, batch loss = 0.64 (14.5 examples/sec; 0.552 sec/batch; 48h:51m:52s remains)
INFO - root - 2017-12-11 04:33:13.411169: step 13850, loss = 0.70, batch loss = 0.64 (14.3 examples/sec; 0.559 sec/batch; 49h:29m:35s remains)
INFO - root - 2017-12-11 04:33:18.871689: step 13860, loss = 0.68, batch loss = 0.62 (14.7 examples/sec; 0.545 sec/batch; 48h:14m:28s remains)
INFO - root - 2017-12-11 04:33:24.358073: step 13870, loss = 0.69, batch loss = 0.63 (14.6 examples/sec; 0.548 sec/batch; 48h:29m:26s remains)
INFO - root - 2017-12-11 04:33:29.790302: step 13880, loss = 0.69, batch loss = 0.64 (14.7 examples/sec; 0.546 sec/batch; 48h:16m:58s remains)
INFO - root - 2017-12-11 04:33:35.412901: step 13890, loss = 0.72, batch loss = 0.66 (14.4 examples/sec; 0.555 sec/batch; 49h:07m:13s remains)
INFO - root - 2017-12-11 04:33:40.896695: step 13900, loss = 0.69, batch loss = 0.63 (15.0 examples/sec; 0.535 sec/batch; 47h:19m:21s remains)
2017-12-11 04:33:41.487753: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.093838885 0.10105355 0.094077691 0.073247023 0.04132054 0.0087618241 -0.015768109 -0.028660787 -0.033933084 -0.032651644 -0.026470574 -0.021772237 -0.020888014 -0.021550266 -0.020801704][0.110002 0.11683074 0.10961558 0.088327453 0.056055058 0.023911992 0.00013944531 -0.012052626 -0.017715035 -0.018284988 -0.015454949 -0.014464166 -0.017371133 -0.021350233 -0.022594349][0.13050719 0.13933161 0.13523224 0.11727398 0.088294677 0.059557818 0.037794907 0.024411839 0.014222776 0.0074635413 0.0040538353 -5.8362963e-05 -0.0071106111 -0.014635436 -0.018660633][0.14786308 0.16294153 0.16716133 0.15790683 0.13698655 0.11578365 0.098564491 0.082613617 0.063351057 0.044830181 0.030845743 0.018613007 0.0056979526 -0.005684502 -0.012300602][0.15537623 0.17954142 0.19619942 0.20055185 0.19307734 0.18558186 0.17878014 0.16291635 0.1333914 0.099084474 0.0697171 0.04440612 0.021510053 0.004707844 -0.0034342462][0.14978379 0.18213859 0.21110399 0.23041938 0.23963368 0.25128418 0.26153091 0.25157413 0.21540183 0.16614881 0.11961979 0.077476993 0.040528554 0.016159147 0.0074149556][0.13371263 0.16945104 0.20575455 0.23689909 0.2620284 0.29383662 0.32392603 0.32508507 0.28888631 0.23102473 0.17119126 0.11316318 0.061406426 0.028469438 0.019378716][0.11962164 0.15272437 0.18856668 0.22410896 0.25847507 0.30371043 0.34892103 0.36229503 0.33268327 0.27571261 0.21112004 0.14358488 0.081435233 0.04200355 0.03316291][0.12411927 0.15114409 0.17929991 0.2094958 0.24124637 0.28619233 0.33499759 0.35644677 0.336626 0.28851169 0.22879764 0.16192484 0.098615088 0.058425661 0.051320247][0.15323347 0.17438078 0.19130409 0.20839737 0.22585389 0.25601363 0.29523373 0.31722286 0.30562857 0.2693828 0.22120979 0.16468403 0.11025762 0.076182991 0.072760716][0.19430092 0.21239528 0.21954057 0.22138788 0.21856751 0.22522238 0.24574968 0.26125443 0.253926 0.22834678 0.19387937 0.15325333 0.11416617 0.090839192 0.092202835][0.22033067 0.23827341 0.24021883 0.2304727 0.20964436 0.19368327 0.1943679 0.20059563 0.1946176 0.17718895 0.1550733 0.13035251 0.10707884 0.0946958 0.099968068][0.20607288 0.22364973 0.22404063 0.2088628 0.17826705 0.14855459 0.13537705 0.13396847 0.12855878 0.11748651 0.10530099 0.093237787 0.082401574 0.078179643 0.085064165][0.14726932 0.16107756 0.1604269 0.14490156 0.11417845 0.082405888 0.064604416 0.059793141 0.055664673 0.0497339 0.044643655 0.040717363 0.03749796 0.037621848 0.043669261][0.06333302 0.070367083 0.068178192 0.055531349 0.031765588 0.0069650118 -0.0077960719 -0.012123322 -0.014336079 -0.016501144 -0.017415453 -0.017389335 -0.017273851 -0.015973212 -0.012189548]]...]
INFO - root - 2017-12-11 04:33:47.090209: step 13910, loss = 0.70, batch loss = 0.64 (14.4 examples/sec; 0.554 sec/batch; 49h:04m:07s remains)
INFO - root - 2017-12-11 04:33:52.626267: step 13920, loss = 0.70, batch loss = 0.64 (14.0 examples/sec; 0.572 sec/batch; 50h:38m:47s remains)
INFO - root - 2017-12-11 04:33:58.197996: step 13930, loss = 0.70, batch loss = 0.64 (14.0 examples/sec; 0.572 sec/batch; 50h:38m:30s remains)
INFO - root - 2017-12-11 04:34:03.405282: step 13940, loss = 0.70, batch loss = 0.64 (30.3 examples/sec; 0.264 sec/batch; 23h:22m:42s remains)
INFO - root - 2017-12-11 04:34:09.005173: step 13950, loss = 0.69, batch loss = 0.64 (14.3 examples/sec; 0.560 sec/batch; 49h:33m:46s remains)
INFO - root - 2017-12-11 04:34:14.484702: step 13960, loss = 0.71, batch loss = 0.66 (14.3 examples/sec; 0.559 sec/batch; 49h:26m:29s remains)
INFO - root - 2017-12-11 04:34:20.018741: step 13970, loss = 0.69, batch loss = 0.63 (14.5 examples/sec; 0.553 sec/batch; 48h:54m:35s remains)
INFO - root - 2017-12-11 04:34:25.558248: step 13980, loss = 0.71, batch loss = 0.65 (14.3 examples/sec; 0.561 sec/batch; 49h:39m:33s remains)
INFO - root - 2017-12-11 04:34:30.966506: step 13990, loss = 0.70, batch loss = 0.64 (14.6 examples/sec; 0.548 sec/batch; 48h:28m:12s remains)
INFO - root - 2017-12-11 04:34:36.498230: step 14000, loss = 0.69, batch loss = 0.63 (14.3 examples/sec; 0.560 sec/batch; 49h:32m:26s remains)
2017-12-11 04:34:37.150259: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.069006316 0.11102045 0.14626278 0.16458096 0.16247125 0.14562087 0.12249196 0.096096314 0.071438409 0.051747952 0.040570032 0.035357669 0.03190659 0.030272227 0.029860534][0.0791854 0.12427709 0.16050623 0.17788151 0.17257756 0.15007105 0.12179418 0.092716157 0.067114122 0.046394046 0.034168433 0.029149743 0.026974637 0.028353399 0.031680621][0.073201805 0.11553313 0.14889218 0.16489509 0.15957615 0.13712864 0.11072312 0.086742826 0.066937871 0.049215835 0.037167285 0.031856555 0.030024393 0.034027375 0.041676566][0.063586622 0.10365429 0.13810754 0.16010901 0.16459493 0.15258446 0.13739188 0.12600696 0.11659487 0.10311484 0.089800328 0.081007876 0.075199537 0.077613577 0.086201593][0.056872144 0.097050719 0.13738759 0.17280908 0.19738343 0.20752157 0.21347564 0.22048417 0.22242741 0.20946331 0.18889731 0.169999 0.15505327 0.15161638 0.15826766][0.05344177 0.094916694 0.14257661 0.19313942 0.24117704 0.27861854 0.31013915 0.33721277 0.34928811 0.33279273 0.29938304 0.2646963 0.23606865 0.2228274 0.22411802][0.051665943 0.095922388 0.15065633 0.21265575 0.27740979 0.33362225 0.38071957 0.41762191 0.43247816 0.40961966 0.36438242 0.31692702 0.2788119 0.26001719 0.25942224][0.048240427 0.094336964 0.15189698 0.21586353 0.28224897 0.33858603 0.38163632 0.4108108 0.41714048 0.38694477 0.33645913 0.28666347 0.2497887 0.23451005 0.23882933][0.047587406 0.095945746 0.1532414 0.21072501 0.26470619 0.30386066 0.32527775 0.33157393 0.32050946 0.2838465 0.2364144 0.19563919 0.17062992 0.16744901 0.18390316][0.051011197 0.10225385 0.15759715 0.20424122 0.23835371 0.25245297 0.24653831 0.22669868 0.19764704 0.15775175 0.11901449 0.092382386 0.081512161 0.090645753 0.11978945][0.053229112 0.10733968 0.16169603 0.20008583 0.21805216 0.21279429 0.18742992 0.14974448 0.10900472 0.0688137 0.038086556 0.021365102 0.017340584 0.029929815 0.062751986][0.054551158 0.11208063 0.16797383 0.20346415 0.21299082 0.19768512 0.16303946 0.11718936 0.07134705 0.032019287 0.0054331208 -0.0087454915 -0.014734093 -0.0080895694 0.01797395][0.058562543 0.12009144 0.17872751 0.2144074 0.22060217 0.20146015 0.16393223 0.11596595 0.069091879 0.030458149 0.003897215 -0.014152383 -0.028903916 -0.035409827 -0.025395779][0.067820407 0.13435692 0.19631936 0.2325252 0.23590122 0.21315303 0.17283298 0.12344491 0.07662037 0.038859665 0.011755322 -0.01040836 -0.032856714 -0.05100878 -0.05709441][0.080688804 0.15249035 0.21772581 0.25432071 0.25508812 0.2290125 0.18635219 0.1368432 0.091953218 0.056996427 0.031652126 0.0087928623 -0.016406219 -0.040202681 -0.056295417]]...]
INFO - root - 2017-12-11 04:34:42.728164: step 14010, loss = 0.68, batch loss = 0.62 (14.7 examples/sec; 0.545 sec/batch; 48h:14m:41s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-SGD-lr0.001-clip50-initfromfixqian30k-fixqian
INFO - root - 2017-12-11 04:34:48.239502: step 14020, loss = 0.71, batch loss = 0.65 (14.4 examples/sec; 0.556 sec/batch; 49h:13m:38s remains)
INFO - root - 2017-12-11 04:34:53.792183: step 14030, loss = 0.69, batch loss = 0.63 (14.3 examples/sec; 0.558 sec/batch; 49h:24m:17s remains)
INFO - root - 2017-12-11 04:34:58.850111: step 14040, loss = 0.69, batch loss = 0.64 (14.7 examples/sec; 0.543 sec/batch; 48h:04m:01s remains)
INFO - root - 2017-12-11 04:35:04.337103: step 14050, loss = 0.71, batch loss = 0.65 (14.7 examples/sec; 0.546 sec/batch; 48h:16m:13s remains)
INFO - root - 2017-12-11 04:35:09.903738: step 14060, loss = 0.70, batch loss = 0.64 (14.5 examples/sec; 0.553 sec/batch; 48h:54m:15s remains)
INFO - root - 2017-12-11 04:35:15.393639: step 14070, loss = 0.69, batch loss = 0.63 (14.7 examples/sec; 0.545 sec/batch; 48h:09m:51s remains)
INFO - root - 2017-12-11 04:35:20.863253: step 14080, loss = 0.70, batch loss = 0.64 (14.9 examples/sec; 0.538 sec/batch; 47h:35m:38s remains)
INFO - root - 2017-12-11 04:35:26.436691: step 14090, loss = 0.70, batch loss = 0.64 (14.0 examples/sec; 0.570 sec/batch; 50h:26m:07s remains)
INFO - root - 2017-12-11 04:35:31.922360: step 14100, loss = 0.69, batch loss = 0.63 (15.0 examples/sec; 0.532 sec/batch; 47h:05m:11s remains)
2017-12-11 04:35:32.518684: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.11771947 0.1162903 0.12440669 0.14931272 0.17678055 0.19949608 0.22043985 0.24429642 0.26255089 0.26090875 0.23532286 0.19368966 0.14530277 0.09086182 0.033991218][0.14300148 0.14792849 0.16775477 0.20833653 0.25028938 0.28471369 0.31580776 0.34876952 0.37149838 0.36813721 0.33531836 0.28253466 0.22064272 0.14991026 0.075651579][0.16249424 0.17538011 0.20723277 0.26278281 0.31709543 0.35954088 0.39588508 0.43299782 0.45654479 0.45014423 0.41144508 0.35064629 0.27856702 0.19455366 0.10619004][0.18488155 0.20401148 0.24393229 0.30775806 0.36563766 0.40646681 0.4391796 0.47340462 0.49360576 0.48267648 0.43904552 0.37371734 0.2968595 0.20653352 0.11192291][0.2297668 0.25646609 0.30114776 0.36603904 0.41951677 0.45275459 0.47950321 0.50996631 0.52745861 0.51263237 0.46298495 0.39053053 0.30657831 0.20965526 0.11035193][0.29118234 0.3285749 0.37705895 0.43876144 0.4858568 0.51330924 0.538606 0.57060009 0.59107167 0.57664663 0.52037561 0.43558711 0.33798212 0.22886287 0.12047792][0.34962669 0.402346 0.45468119 0.51254767 0.55542326 0.57951784 0.6029166 0.6344465 0.65660721 0.64172542 0.57700831 0.47901735 0.36937502 0.25170261 0.13725294][0.37646857 0.44004551 0.49298641 0.54438984 0.58315676 0.60384196 0.62237883 0.64811951 0.66736615 0.65069258 0.5802151 0.47690853 0.36662847 0.25314346 0.14353649][0.35781118 0.4192726 0.46448755 0.50404519 0.53507245 0.55092591 0.56371474 0.58213443 0.59768307 0.58277881 0.51681823 0.42203498 0.32510015 0.22840507 0.13440277][0.29963043 0.3465322 0.37668937 0.40120038 0.42224771 0.43346268 0.44291845 0.45734 0.47271195 0.46541873 0.41393989 0.33809212 0.26227742 0.187723 0.113819][0.20954934 0.23568758 0.24838395 0.25824213 0.26881897 0.27463844 0.28086305 0.29305372 0.30945203 0.31117958 0.27806157 0.22636867 0.17595877 0.12659118 0.076028459][0.11118487 0.11919194 0.11835212 0.11742301 0.11903651 0.11841462 0.11954483 0.12772727 0.14200239 0.14862281 0.13106367 0.10221618 0.0764946 0.051827569 0.025011182][0.029755322 0.025577804 0.017217584 0.010473335 0.0066639362 0.002276479 0.000663538 0.0061675659 0.017490543 0.025572177 0.018810315 0.0065005883 -0.0019659377 -0.00961584 -0.019375991][-0.018558821 -0.028834792 -0.040169489 -0.049066991 -0.055263743 -0.060687367 -0.062657923 -0.058431886 -0.049699709 -0.042087469 -0.042441025 -0.045143027 -0.044849932 -0.044318102 -0.045563113][-0.040847965 -0.052420221 -0.063313596 -0.071943358 -0.078606941 -0.083906032 -0.085716188 -0.08244396 -0.076064147 -0.070042692 -0.067659311 -0.065904848 -0.0624141 -0.059334069 -0.057675883]]...]
INFO - root - 2017-12-11 04:35:38.016014: step 14110, loss = 0.68, batch loss = 0.63 (14.4 examples/sec; 0.556 sec/batch; 49h:09m:59s remains)
INFO - root - 2017-12-11 04:35:43.530430: step 14120, loss = 0.70, batch loss = 0.65 (14.1 examples/sec; 0.569 sec/batch; 50h:19m:34s remains)
INFO - root - 2017-12-11 04:35:49.134173: step 14130, loss = 0.68, batch loss = 0.62 (14.3 examples/sec; 0.560 sec/batch; 49h:31m:53s remains)
INFO - root - 2017-12-11 04:35:54.430992: step 14140, loss = 0.70, batch loss = 0.64 (14.4 examples/sec; 0.556 sec/batch; 49h:10m:25s remains)
INFO - root - 2017-12-11 04:35:59.936665: step 14150, loss = 0.70, batch loss = 0.64 (13.8 examples/sec; 0.579 sec/batch; 51h:13m:55s remains)
INFO - root - 2017-12-11 04:36:05.430559: step 14160, loss = 0.70, batch loss = 0.64 (14.8 examples/sec; 0.541 sec/batch; 47h:49m:10s remains)
INFO - root - 2017-12-11 04:36:10.946528: step 14170, loss = 0.71, batch loss = 0.65 (14.2 examples/sec; 0.564 sec/batch; 49h:50m:03s remains)
INFO - root - 2017-12-11 04:36:16.382512: step 14180, loss = 0.69, batch loss = 0.63 (14.9 examples/sec; 0.537 sec/batch; 47h:28m:54s remains)
INFO - root - 2017-12-11 04:36:21.816142: step 14190, loss = 0.70, batch loss = 0.65 (14.5 examples/sec; 0.550 sec/batch; 48h:38m:11s remains)
INFO - root - 2017-12-11 04:36:27.365420: step 14200, loss = 0.70, batch loss = 0.64 (14.4 examples/sec; 0.557 sec/batch; 49h:13m:47s remains)
2017-12-11 04:36:27.909025: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.012189946 -0.01319287 -0.01975015 -0.028927557 -0.036004983 -0.039211374 -0.040370326 -0.042846411 -0.047338296 -0.053781409 -0.060931422 -0.067002475 -0.071500354 -0.073371947 -0.073155344][0.033807311 0.038353227 0.031463385 0.017727671 0.0055973455 -0.00030014804 -0.0010152006 -0.0020899768 -0.0060818228 -0.014466741 -0.02637824 -0.038692612 -0.050000645 -0.056660816 -0.059046175][0.099223331 0.11297128 0.10888235 0.09389466 0.081160493 0.078643069 0.086164005 0.094206944 0.09593799 0.086668372 0.067024171 0.04302492 0.017965825 0.00013055802 -0.0095181735][0.17248692 0.19708484 0.19857377 0.18716282 0.18107325 0.19152634 0.2186958 0.2472963 0.26285896 0.25539103 0.22548473 0.18385099 0.13726121 0.10105775 0.079099081][0.23449744 0.26742375 0.27507147 0.27181754 0.28037289 0.31421015 0.37299585 0.43405968 0.4717848 0.46881998 0.42660818 0.36289114 0.28928334 0.22918789 0.19108939][0.26951984 0.30516845 0.31753507 0.32564223 0.35447726 0.41823342 0.51546794 0.61579281 0.68029338 0.68331265 0.6286512 0.54208845 0.44054422 0.3543891 0.29838979][0.2753444 0.30659822 0.31987765 0.33839181 0.38674936 0.47845578 0.61096352 0.74764425 0.8368063 0.8456232 0.78131288 0.67654252 0.55267668 0.44451907 0.37374514][0.26188046 0.28259456 0.29104006 0.31327769 0.37185517 0.47829378 0.62878257 0.78438812 0.88647634 0.89941937 0.83254254 0.72085857 0.58877933 0.47222 0.39739674][0.24472791 0.25363451 0.25375497 0.27142319 0.32570127 0.42509115 0.56517226 0.71036679 0.80496389 0.81733084 0.75669122 0.65346658 0.53207523 0.42599037 0.36146566][0.23425771 0.23369028 0.22477597 0.23220743 0.2709223 0.34578684 0.4522053 0.56197023 0.63065577 0.63609916 0.58678663 0.50419027 0.40901297 0.329633 0.28810388][0.23070878 0.22418489 0.20747468 0.20339872 0.22352445 0.26957062 0.33673292 0.40451506 0.44191951 0.43720245 0.39914724 0.34067386 0.27649626 0.22984825 0.21657912][0.22887646 0.22045161 0.19882613 0.18535008 0.19056812 0.21478242 0.25292423 0.28913814 0.30265352 0.2895447 0.25966358 0.22107412 0.18260629 0.16467389 0.17731097][0.2207244 0.21482769 0.19300419 0.17501538 0.17266111 0.18689051 0.21251218 0.23391946 0.23553857 0.2180544 0.19252966 0.16571333 0.14273958 0.14395447 0.17551613][0.20483786 0.20443985 0.1866155 0.16880651 0.16487263 0.17782338 0.20316008 0.22348677 0.22383347 0.206694 0.18316361 0.1603342 0.14197709 0.15110856 0.19308634][0.17754456 0.18289989 0.17143811 0.15765394 0.15635774 0.1723045 0.20266694 0.22907457 0.2347763 0.22137135 0.19817971 0.17331944 0.15110631 0.15825348 0.20196275]]...]
INFO - root - 2017-12-11 04:36:33.450676: step 14210, loss = 0.71, batch loss = 0.65 (14.5 examples/sec; 0.550 sec/batch; 48h:36m:57s remains)
INFO - root - 2017-12-11 04:36:39.019711: step 14220, loss = 0.68, batch loss = 0.62 (14.7 examples/sec; 0.546 sec/batch; 48h:16m:25s remains)
INFO - root - 2017-12-11 04:36:44.564787: step 14230, loss = 0.71, batch loss = 0.65 (14.7 examples/sec; 0.543 sec/batch; 47h:57m:59s remains)
INFO - root - 2017-12-11 04:36:49.755290: step 14240, loss = 0.69, batch loss = 0.64 (14.9 examples/sec; 0.536 sec/batch; 47h:23m:20s remains)
INFO - root - 2017-12-11 04:36:55.274485: step 14250, loss = 0.68, batch loss = 0.63 (14.6 examples/sec; 0.547 sec/batch; 48h:20m:40s remains)
INFO - root - 2017-12-11 04:37:00.715939: step 14260, loss = 0.71, batch loss = 0.65 (14.5 examples/sec; 0.550 sec/batch; 48h:38m:29s remains)
INFO - root - 2017-12-11 04:37:06.218224: step 14270, loss = 0.68, batch loss = 0.62 (14.2 examples/sec; 0.563 sec/batch; 49h:48m:23s remains)
INFO - root - 2017-12-11 04:37:11.810959: step 14280, loss = 0.69, batch loss = 0.63 (14.4 examples/sec; 0.556 sec/batch; 49h:09m:12s remains)
INFO - root - 2017-12-11 04:37:17.268024: step 14290, loss = 0.71, batch loss = 0.65 (14.7 examples/sec; 0.544 sec/batch; 48h:02m:41s remains)
INFO - root - 2017-12-11 04:37:22.828509: step 14300, loss = 0.68, batch loss = 0.62 (14.3 examples/sec; 0.559 sec/batch; 49h:24m:00s remains)
2017-12-11 04:37:23.399952: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.010324738 0.041733079 0.065184675 0.080376059 0.081096396 0.062146436 0.036491152 0.025006115 0.038799878 0.076918051 0.13480633 0.20312986 0.26533222 0.30827856 0.33164188][0.0049760286 0.037166025 0.067632005 0.094720364 0.1079947 0.098010443 0.077921942 0.06992624 0.086516969 0.12970011 0.19475435 0.26953846 0.33272007 0.36739761 0.37471411][0.00079989625 0.036298044 0.080860406 0.12874134 0.16255578 0.16780275 0.15756245 0.1531343 0.1657953 0.20063755 0.25568059 0.31974128 0.37039953 0.38901696 0.37735084][-0.0006589127 0.04249268 0.10713083 0.18077265 0.23754723 0.25857508 0.25779009 0.25413975 0.25568897 0.26998127 0.3005605 0.34053317 0.36984554 0.36972091 0.34192336][-0.0013269502 0.049537875 0.13216355 0.22751543 0.30343905 0.33976009 0.35054478 0.34957457 0.3387275 0.32701507 0.3247644 0.33182275 0.33328792 0.31214124 0.27111685][-0.0041958392 0.050106935 0.14298643 0.25240996 0.34425959 0.39992446 0.43076515 0.44075409 0.42188212 0.38384306 0.345328 0.3144446 0.28289607 0.23866107 0.18825583][-0.0092141731 0.044195239 0.13949452 0.25576654 0.36133361 0.43962139 0.49548104 0.52020323 0.49595052 0.43398884 0.3622483 0.29638186 0.23427922 0.17241782 0.12268628][-0.015296815 0.033706453 0.12440815 0.23952104 0.35187739 0.44607157 0.51952881 0.55352324 0.524376 0.44624698 0.3547447 0.26965609 0.19267167 0.12858574 0.091928661][-0.02163028 0.020090539 0.099365175 0.20354848 0.31113288 0.40781233 0.48520151 0.52066153 0.49052876 0.41108614 0.31970555 0.23526198 0.1626583 0.11348035 0.10034666][-0.025181558 0.0095888218 0.076253057 0.16616787 0.2633813 0.3546263 0.4279677 0.46191445 0.43585432 0.3663955 0.28676265 0.21223137 0.15108889 0.12163609 0.13373071][-0.023383386 0.0093918648 0.070183538 0.15230179 0.24305162 0.32849252 0.39532992 0.4256404 0.40212649 0.34139541 0.27108544 0.20357069 0.15091822 0.13761015 0.17127979][-0.017282967 0.019000122 0.0831831 0.16809492 0.26057777 0.34292915 0.40216163 0.42456007 0.39585164 0.3343944 0.26473275 0.19923964 0.15247242 0.15191387 0.20229328][-0.010634835 0.031064386 0.10303331 0.19602817 0.29369697 0.37391388 0.42558768 0.43895674 0.4021861 0.33579153 0.26324362 0.1985888 0.15694398 0.16450875 0.22283329][-0.005814896 0.039545357 0.11631697 0.21367075 0.31236032 0.38793352 0.43312961 0.4417648 0.40252572 0.33531728 0.26289341 0.20086454 0.16289473 0.17094901 0.22571036][-0.005637581 0.039058339 0.11366969 0.20716883 0.29966748 0.36737525 0.40757126 0.41606724 0.37981552 0.31606373 0.247435 0.19051582 0.15499836 0.15713978 0.19991587]]...]
INFO - root - 2017-12-11 04:37:28.873433: step 14310, loss = 0.71, batch loss = 0.65 (14.5 examples/sec; 0.552 sec/batch; 48h:49m:33s remains)
INFO - root - 2017-12-11 04:37:34.365573: step 14320, loss = 0.69, batch loss = 0.64 (14.7 examples/sec; 0.545 sec/batch; 48h:09m:16s remains)
INFO - root - 2017-12-11 04:37:39.447806: step 14330, loss = 0.68, batch loss = 0.62 (15.0 examples/sec; 0.532 sec/batch; 47h:01m:56s remains)
INFO - root - 2017-12-11 04:37:44.928108: step 14340, loss = 0.71, batch loss = 0.65 (14.8 examples/sec; 0.541 sec/batch; 47h:49m:30s remains)
INFO - root - 2017-12-11 04:37:50.433597: step 14350, loss = 0.70, batch loss = 0.64 (14.2 examples/sec; 0.562 sec/batch; 49h:38m:12s remains)
INFO - root - 2017-12-11 04:37:55.902063: step 14360, loss = 0.70, batch loss = 0.65 (14.7 examples/sec; 0.546 sec/batch; 48h:15m:06s remains)
INFO - root - 2017-12-11 04:38:01.379875: step 14370, loss = 0.70, batch loss = 0.64 (14.6 examples/sec; 0.547 sec/batch; 48h:17m:44s remains)
INFO - root - 2017-12-11 04:38:06.935559: step 14380, loss = 0.69, batch loss = 0.64 (14.2 examples/sec; 0.564 sec/batch; 49h:48m:41s remains)
INFO - root - 2017-12-11 04:38:12.430911: step 14390, loss = 0.71, batch loss = 0.65 (14.4 examples/sec; 0.554 sec/batch; 48h:57m:37s remains)
INFO - root - 2017-12-11 04:38:17.882445: step 14400, loss = 0.69, batch loss = 0.64 (14.5 examples/sec; 0.554 sec/batch; 48h:54m:39s remains)
2017-12-11 04:38:18.484263: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.04910399 0.092904836 0.12758787 0.14497779 0.13778791 0.11024892 0.074683428 0.042610031 0.022703078 0.016268782 0.018778285 0.024142839 0.03179739 0.042940062 0.057291988][0.053310778 0.10184053 0.14193508 0.16610327 0.16671742 0.14544399 0.11249256 0.079552293 0.05729983 0.048824955 0.051227853 0.059328873 0.073019654 0.092646286 0.11643609][0.060782693 0.11692286 0.16709614 0.20428419 0.22016422 0.21259241 0.18684049 0.15368766 0.12700972 0.11324963 0.11239504 0.12127373 0.14038618 0.16899671 0.20265622][0.071759395 0.13804102 0.20221287 0.25631931 0.29138863 0.30127692 0.28551728 0.25313589 0.22092551 0.19967619 0.19295382 0.19959672 0.22133064 0.25736731 0.29961255][0.084561452 0.16100442 0.23868828 0.30868608 0.36180043 0.38905022 0.38470694 0.35503986 0.31796113 0.28863281 0.274879 0.27727619 0.29927132 0.34052479 0.38902247][0.093937792 0.17687324 0.26450071 0.34764451 0.41763803 0.46383122 0.47508734 0.45286238 0.41426593 0.37763298 0.35571986 0.35138088 0.37103754 0.41433579 0.46562698][0.093115166 0.17714338 0.27023888 0.36388972 0.45122731 0.52020043 0.55284029 0.54411322 0.50815219 0.46545106 0.43316591 0.41815427 0.43121284 0.47238246 0.52329069][0.080280051 0.16116667 0.25635615 0.35877231 0.46353334 0.55581743 0.61037213 0.61573416 0.582082 0.53125232 0.48409122 0.45274433 0.45306772 0.48704863 0.5355956][0.059903491 0.1342846 0.2266067 0.3312422 0.44409126 0.54755437 0.61219269 0.62311929 0.5875746 0.52757782 0.46610171 0.41947895 0.4067466 0.43177065 0.47666696][0.038168468 0.10345504 0.18685444 0.28360718 0.38992745 0.48776978 0.54809529 0.55652231 0.518526 0.45402741 0.38502038 0.32963008 0.30808687 0.3251034 0.36531481][0.016741043 0.070513368 0.13956682 0.21980159 0.30784914 0.387815 0.43517196 0.43878227 0.40252164 0.34202707 0.27556825 0.22062324 0.19662744 0.20824924 0.24299185][-0.0018785401 0.039005227 0.08971642 0.14685218 0.20843515 0.26289746 0.29286283 0.29160941 0.26137373 0.21236888 0.15720356 0.11090437 0.090592079 0.10069871 0.13131709][-0.0092937686 0.021888547 0.056297228 0.090823829 0.12503749 0.15248489 0.16344865 0.15558523 0.13028263 0.092886418 0.050803691 0.01591246 0.0016998359 0.011742198 0.038372006][-0.004049683 0.023172615 0.047419153 0.064895056 0.076139554 0.079866782 0.073564485 0.058743682 0.036618792 0.009113349 -0.02045933 -0.044437204 -0.053865593 -0.046316706 -0.026994694][0.0010136547 0.02642397 0.044916354 0.051682848 0.04758836 0.035497073 0.018310929 -2.8753282e-06 -0.018220002 -0.036645487 -0.05489454 -0.069765456 -0.076453432 -0.07342 -0.063313276]]...]
INFO - root - 2017-12-11 04:38:24.092648: step 14410, loss = 0.68, batch loss = 0.63 (13.7 examples/sec; 0.583 sec/batch; 51h:32m:11s remains)
INFO - root - 2017-12-11 04:38:29.535564: step 14420, loss = 0.69, batch loss = 0.63 (14.7 examples/sec; 0.543 sec/batch; 48h:00m:37s remains)
INFO - root - 2017-12-11 04:38:34.734537: step 14430, loss = 0.67, batch loss = 0.61 (14.7 examples/sec; 0.546 sec/batch; 48h:13m:22s remains)
INFO - root - 2017-12-11 04:38:40.270082: step 14440, loss = 0.70, batch loss = 0.64 (14.4 examples/sec; 0.557 sec/batch; 49h:12m:46s remains)
INFO - root - 2017-12-11 04:38:45.762287: step 14450, loss = 0.70, batch loss = 0.65 (14.3 examples/sec; 0.560 sec/batch; 49h:28m:48s remains)
INFO - root - 2017-12-11 04:38:51.298773: step 14460, loss = 0.69, batch loss = 0.63 (14.7 examples/sec; 0.544 sec/batch; 48h:01m:51s remains)
INFO - root - 2017-12-11 04:38:56.756689: step 14470, loss = 0.69, batch loss = 0.64 (14.6 examples/sec; 0.550 sec/batch; 48h:32m:39s remains)
INFO - root - 2017-12-11 04:39:02.288185: step 14480, loss = 0.69, batch loss = 0.63 (14.6 examples/sec; 0.547 sec/batch; 48h:21m:23s remains)
INFO - root - 2017-12-11 04:39:07.827156: step 14490, loss = 0.69, batch loss = 0.63 (15.0 examples/sec; 0.534 sec/batch; 47h:10m:07s remains)
INFO - root - 2017-12-11 04:39:13.285638: step 14500, loss = 0.71, batch loss = 0.65 (14.9 examples/sec; 0.535 sec/batch; 47h:16m:55s remains)
2017-12-11 04:39:13.871491: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.04091214 0.023721844 0.01756707 0.026266476 0.046698663 0.069553807 0.093256772 0.11339606 0.14364997 0.17419593 0.18686733 0.18199663 0.16146021 0.13086557 0.092028156][0.0402865 0.033086054 0.031371988 0.035081193 0.042856187 0.051567495 0.063857891 0.075520374 0.096339881 0.11846413 0.12503305 0.1154324 0.093741946 0.064750925 0.032849114][0.045844782 0.046401352 0.0488036 0.049032263 0.046995774 0.045787759 0.053312689 0.06650617 0.08676669 0.10483942 0.10694115 0.093552843 0.069387935 0.037130874 0.0049897255][0.057417925 0.063633911 0.070811965 0.071426809 0.066327333 0.06306807 0.075610459 0.10344156 0.13569638 0.15650119 0.1550605 0.13573033 0.10370366 0.060277425 0.018213354][0.074734226 0.087777741 0.10304534 0.11014373 0.11019223 0.11327977 0.13912626 0.19074039 0.2434859 0.27256989 0.2681711 0.23978096 0.19474439 0.13439475 0.076109707][0.095620655 0.11901982 0.14678165 0.16596714 0.17762667 0.19223575 0.23279701 0.3059361 0.37658548 0.41347018 0.40538678 0.36590937 0.30654341 0.22972146 0.15588589][0.12036444 0.15742722 0.1997342 0.23218152 0.25671217 0.28212342 0.33063889 0.411045 0.48503518 0.52096146 0.50595093 0.45540512 0.38533551 0.29921338 0.21855259][0.14045301 0.19005722 0.24272649 0.28217402 0.31254774 0.34086537 0.38569552 0.45625016 0.51776028 0.54409766 0.52158242 0.46525958 0.39323694 0.3086988 0.2329293][0.13530912 0.18953946 0.24284613 0.279801 0.30748585 0.33155569 0.36516237 0.415265 0.45556387 0.46905768 0.44279188 0.38886476 0.32425025 0.25144732 0.19049747][0.096059807 0.14357829 0.18695533 0.21378553 0.23332135 0.2496638 0.26967221 0.29677609 0.31473136 0.31634182 0.29081917 0.24645127 0.19676207 0.14356589 0.10405674][0.0313291 0.064010605 0.092182755 0.10691676 0.11749337 0.12588061 0.13379972 0.14166996 0.14184874 0.13470487 0.11287094 0.08161594 0.049799994 0.018589802 0.00096933939][-0.032521408 -0.01606288 -0.0020018064 0.0035265419 0.00755931 0.010047366 0.0098575437 0.006002977 -0.0037282035 -0.015126592 -0.031828087 -0.050385367 -0.066398412 -0.079406954 -0.08098153][-0.075898871 -0.072044194 -0.067734286 -0.067463428 -0.067317568 -0.068326063 -0.072093472 -0.079602793 -0.09092357 -0.10154824 -0.11233626 -0.12088875 -0.12600967 -0.12784246 -0.12174378][-0.095954694 -0.098784395 -0.09904398 -0.1003813 -0.10139479 -0.10331771 -0.10728476 -0.11377193 -0.12232247 -0.12982605 -0.13568991 -0.13828461 -0.13789038 -0.13500978 -0.12768406][-0.094765581 -0.099528454 -0.10072561 -0.10176143 -0.10255149 -0.10408527 -0.10696623 -0.1111498 -0.11621758 -0.12059917 -0.12341961 -0.12354753 -0.12153005 -0.11808152 -0.11277106]]...]
INFO - root - 2017-12-11 04:39:19.367321: step 14510, loss = 0.71, batch loss = 0.65 (15.1 examples/sec; 0.529 sec/batch; 46h:43m:37s remains)
INFO - root - 2017-12-11 04:39:24.965822: step 14520, loss = 0.67, batch loss = 0.62 (14.3 examples/sec; 0.558 sec/batch; 49h:15m:43s remains)
INFO - root - 2017-12-11 04:39:30.101471: step 14530, loss = 0.69, batch loss = 0.64 (14.8 examples/sec; 0.541 sec/batch; 47h:44m:35s remains)
INFO - root - 2017-12-11 04:39:35.602816: step 14540, loss = 0.69, batch loss = 0.64 (14.8 examples/sec; 0.540 sec/batch; 47h:41m:21s remains)
INFO - root - 2017-12-11 04:39:41.037768: step 14550, loss = 0.69, batch loss = 0.64 (14.5 examples/sec; 0.551 sec/batch; 48h:40m:15s remains)
INFO - root - 2017-12-11 04:39:46.573187: step 14560, loss = 0.68, batch loss = 0.63 (14.6 examples/sec; 0.549 sec/batch; 48h:27m:25s remains)
INFO - root - 2017-12-11 04:39:52.242076: step 14570, loss = 0.70, batch loss = 0.64 (14.7 examples/sec; 0.545 sec/batch; 48h:05m:44s remains)
INFO - root - 2017-12-11 04:39:57.719038: step 14580, loss = 0.71, batch loss = 0.65 (14.8 examples/sec; 0.542 sec/batch; 47h:52m:46s remains)
INFO - root - 2017-12-11 04:40:03.249580: step 14590, loss = 0.71, batch loss = 0.65 (14.4 examples/sec; 0.557 sec/batch; 49h:13m:46s remains)
INFO - root - 2017-12-11 04:40:08.697237: step 14600, loss = 0.71, batch loss = 0.65 (14.7 examples/sec; 0.544 sec/batch; 48h:03m:36s remains)
2017-12-11 04:40:09.302363: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.27674526 0.27057993 0.28275502 0.313128 0.34893006 0.37177983 0.37921965 0.38168103 0.39695013 0.44206172 0.49924934 0.54220158 0.54915971 0.52021754 0.45448613][0.2317269 0.2301285 0.2465734 0.2790938 0.31675819 0.34412873 0.35935566 0.37199661 0.39890963 0.45890248 0.53350896 0.59262723 0.61173922 0.58684033 0.51591712][0.1783798 0.1849004 0.21050259 0.25122193 0.29639316 0.33122584 0.35161388 0.36528528 0.38769323 0.43979827 0.50856233 0.56761867 0.59400284 0.57997346 0.52046889][0.13546607 0.15732126 0.20160119 0.26125032 0.3236514 0.3713184 0.39498755 0.39950302 0.3993887 0.41806033 0.45375738 0.49079379 0.51227742 0.50795704 0.46889168][0.094164893 0.13898438 0.21091147 0.29873541 0.3872529 0.45442876 0.48290682 0.47410151 0.44266957 0.41382086 0.3976343 0.39140958 0.38949379 0.38412061 0.36390328][0.039778583 0.11307868 0.21936992 0.34378 0.4679732 0.56395948 0.60388184 0.58573991 0.52601922 0.44991317 0.37477455 0.30977479 0.26550549 0.24306682 0.23106252][-0.0077563021 0.090363555 0.227325 0.38621053 0.54622185 0.67337382 0.72908586 0.70898479 0.6328131 0.52306926 0.39707416 0.27187666 0.17658749 0.12499401 0.10726599][-0.027356615 0.081525452 0.23254731 0.41133639 0.59667104 0.74938482 0.82129788 0.80590957 0.72692043 0.60298884 0.44600514 0.27531838 0.13549586 0.052360356 0.018713776][0.0040874332 0.096989587 0.22951609 0.39935696 0.58761483 0.75079435 0.8328613 0.82527196 0.75436842 0.634876 0.47118124 0.28081638 0.11677688 0.01182898 -0.038736392][0.085877612 0.13237222 0.20963822 0.33667234 0.49880087 0.64922678 0.72841275 0.72675383 0.67026818 0.56995654 0.42336789 0.24402305 0.085425094 -0.01935811 -0.074336752][0.19392741 0.17641841 0.17776136 0.24124223 0.35621694 0.47436264 0.53848106 0.53868639 0.4974328 0.42245534 0.30778882 0.16321726 0.036105853 -0.045767628 -0.087756135][0.28285354 0.2061464 0.13639615 0.13590291 0.19678196 0.27333546 0.31653395 0.31628042 0.28853658 0.23834115 0.16023874 0.061740708 -0.020166283 -0.066296421 -0.084490091][0.3062872 0.19350526 0.079444796 0.034156 0.051216908 0.091498315 0.11688246 0.11690333 0.10058757 0.071332827 0.026495574 -0.027458746 -0.0654752 -0.077548437 -0.073162533][0.256356 0.13649209 0.013558093 -0.047927849 -0.054398745 -0.036749177 -0.021998638 -0.020088689 -0.027258012 -0.041048542 -0.061273318 -0.082122438 -0.089006342 -0.079171129 -0.06217232][0.15228297 0.050031591 -0.051954012 -0.10519419 -0.1168716 -0.10926042 -0.099290006 -0.095127217 -0.095787816 -0.099544086 -0.10485037 -0.10715427 -0.098986492 -0.081138141 -0.061454844]]...]
INFO - root - 2017-12-11 04:40:14.775260: step 14610, loss = 0.69, batch loss = 0.63 (14.8 examples/sec; 0.540 sec/batch; 47h:42m:33s remains)
INFO - root - 2017-12-11 04:40:20.160411: step 14620, loss = 0.69, batch loss = 0.63 (17.0 examples/sec; 0.470 sec/batch; 41h:29m:00s remains)
INFO - root - 2017-12-11 04:40:25.423441: step 14630, loss = 0.69, batch loss = 0.63 (14.7 examples/sec; 0.544 sec/batch; 48h:02m:42s remains)
INFO - root - 2017-12-11 04:40:30.928785: step 14640, loss = 0.70, batch loss = 0.64 (14.8 examples/sec; 0.542 sec/batch; 47h:51m:46s remains)
INFO - root - 2017-12-11 04:40:36.363398: step 14650, loss = 0.70, batch loss = 0.64 (14.3 examples/sec; 0.558 sec/batch; 49h:15m:56s remains)
INFO - root - 2017-12-11 04:40:41.788033: step 14660, loss = 0.70, batch loss = 0.65 (14.5 examples/sec; 0.552 sec/batch; 48h:45m:07s remains)
INFO - root - 2017-12-11 04:40:47.336734: step 14670, loss = 0.70, batch loss = 0.65 (13.9 examples/sec; 0.576 sec/batch; 50h:50m:15s remains)
INFO - root - 2017-12-11 04:40:52.832677: step 14680, loss = 0.70, batch loss = 0.64 (14.6 examples/sec; 0.548 sec/batch; 48h:23m:13s remains)
INFO - root - 2017-12-11 04:40:58.388962: step 14690, loss = 0.71, batch loss = 0.65 (14.8 examples/sec; 0.542 sec/batch; 47h:49m:14s remains)
INFO - root - 2017-12-11 04:41:03.876183: step 14700, loss = 0.68, batch loss = 0.63 (14.5 examples/sec; 0.553 sec/batch; 48h:50m:20s remains)
2017-12-11 04:41:04.452299: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.081729 0.078335546 0.075517304 0.075398229 0.074243456 0.071484275 0.070942417 0.072943442 0.0748375 0.078195415 0.078382008 0.07565774 0.073667333 0.073568419 0.075899385][0.14635895 0.14104322 0.13210227 0.12523659 0.11857325 0.1116781 0.10790806 0.1075671 0.10820498 0.11223951 0.11469927 0.11395601 0.11319071 0.11267339 0.11334458][0.19256179 0.19018906 0.17924324 0.16892649 0.15961079 0.15067844 0.14473407 0.14123988 0.13839875 0.14043647 0.14270894 0.14277543 0.14385246 0.14502105 0.14613797][0.19851178 0.20207755 0.19541676 0.18828371 0.18250634 0.17669939 0.17280173 0.1693144 0.16437221 0.16277051 0.162163 0.16109318 0.16187255 0.16337262 0.16477133][0.16476594 0.17341518 0.17361106 0.17319374 0.17444387 0.1746711 0.17469548 0.17276745 0.16721933 0.16132872 0.15599991 0.15129578 0.14875935 0.14752078 0.14707007][0.1076456 0.11685057 0.12016445 0.12354481 0.12847279 0.13143657 0.13176566 0.12944248 0.12253527 0.11351759 0.10478401 0.097651675 0.092821687 0.088404268 0.085694857][0.057807535 0.062022805 0.062372994 0.063270189 0.065836564 0.067331955 0.067307696 0.064938433 0.058793902 0.051172007 0.043932535 0.03685078 0.03106159 0.025199197 0.020900723][0.025293812 0.023038303 0.018475173 0.016112715 0.016862625 0.018344143 0.020378225 0.021180471 0.018994814 0.015435603 0.010552442 0.0041252784 -0.002358655 -0.009602299 -0.015382375][0.0059417905 -0.0016342159 -0.009420881 -0.012644719 -0.010634406 -0.0061523756 0.00068891648 0.0076218955 0.011399356 0.01212394 0.0089075183 0.0021654016 -0.006324362 -0.015714468 -0.023055593][0.0071102888 -0.0017096329 -0.0098206159 -0.012282569 -0.0084662382 -0.0014230652 0.0084709041 0.018840531 0.025596373 0.028354881 0.025589148 0.017870244 0.0080624586 -0.0023560396 -0.010102463][0.019071966 0.011790078 0.003981662 0.0013464888 0.0047398745 0.011529456 0.020780182 0.030642891 0.037387416 0.040956773 0.039059579 0.0321702 0.023952169 0.014959498 0.0084152892][0.029714596 0.022215892 0.012500286 0.00770121 0.0090600187 0.01442734 0.022198148 0.030997595 0.038046304 0.04367486 0.04453472 0.0405926 0.036288843 0.030823564 0.026368668][0.03713106 0.028442135 0.016297413 0.0093430448 0.0091739716 0.013389869 0.019842921 0.027799347 0.0356408 0.043553241 0.04709401 0.046048019 0.045494147 0.043460596 0.040693138][0.037402064 0.031027528 0.020060088 0.013722774 0.013677326 0.016324086 0.01906025 0.022866771 0.028376149 0.035426743 0.038733363 0.038335014 0.03959759 0.039751377 0.038211346][0.036790002 0.0338387 0.026640687 0.023164751 0.024569096 0.025769945 0.023519415 0.020851851 0.021108657 0.024609301 0.02576012 0.024691366 0.026513498 0.028118733 0.028058721]]...]
INFO - root - 2017-12-11 04:41:09.990270: step 14710, loss = 0.72, batch loss = 0.66 (14.7 examples/sec; 0.543 sec/batch; 47h:55m:40s remains)
INFO - root - 2017-12-11 04:41:15.167135: step 14720, loss = 0.69, batch loss = 0.63 (14.3 examples/sec; 0.561 sec/batch; 49h:31m:43s remains)
INFO - root - 2017-12-11 04:41:20.771729: step 14730, loss = 0.70, batch loss = 0.64 (14.6 examples/sec; 0.548 sec/batch; 48h:20m:51s remains)
INFO - root - 2017-12-11 04:41:26.283039: step 14740, loss = 0.68, batch loss = 0.62 (14.7 examples/sec; 0.544 sec/batch; 48h:03m:10s remains)
INFO - root - 2017-12-11 04:41:31.763278: step 14750, loss = 0.70, batch loss = 0.64 (14.5 examples/sec; 0.550 sec/batch; 48h:33m:19s remains)
INFO - root - 2017-12-11 04:41:37.250671: step 14760, loss = 0.69, batch loss = 0.63 (14.2 examples/sec; 0.563 sec/batch; 49h:39m:50s remains)
INFO - root - 2017-12-11 04:41:42.784344: step 14770, loss = 0.69, batch loss = 0.63 (14.8 examples/sec; 0.541 sec/batch; 47h:45m:08s remains)
INFO - root - 2017-12-11 04:41:48.237683: step 14780, loss = 0.69, batch loss = 0.64 (15.0 examples/sec; 0.532 sec/batch; 46h:57m:33s remains)
INFO - root - 2017-12-11 04:41:53.709585: step 14790, loss = 0.71, batch loss = 0.65 (15.1 examples/sec; 0.529 sec/batch; 46h:43m:16s remains)
INFO - root - 2017-12-11 04:41:59.206828: step 14800, loss = 0.69, batch loss = 0.64 (14.3 examples/sec; 0.558 sec/batch; 49h:14m:57s remains)
2017-12-11 04:41:59.758655: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.057965867 -0.057501286 -0.055932567 -0.0556031 -0.056587894 -0.057956766 -0.051604435 -0.035869397 -0.013018756 0.01147579 0.028616926 0.033346821 0.021990396 -0.0012243005 -0.025484307][-0.060820498 -0.0638897 -0.064153612 -0.063231781 -0.062336382 -0.0621814 -0.055627394 -0.040334132 -0.017941853 0.006475532 0.02327477 0.027818009 0.01629675 -0.0069313366 -0.03103579][-0.048218604 -0.055987384 -0.058044765 -0.054793853 -0.049022261 -0.044105332 -0.034470621 -0.017737344 0.0044460529 0.027052276 0.040358759 0.04047963 0.024702489 -0.0023407827 -0.029656876][-0.013715655 -0.0253622 -0.0284794 -0.021642379 -0.0088686505 0.003400499 0.019243047 0.040680002 0.064822346 0.08511515 0.091853872 0.083115645 0.058322668 0.021776488 -0.014678193][0.042684145 0.029474644 0.025331624 0.034479141 0.053172566 0.072200753 0.094464555 0.12186904 0.14911993 0.16668159 0.16474427 0.1446989 0.10851692 0.059479464 0.010392067][0.10853746 0.096922725 0.091424063 0.099650078 0.11996142 0.14150445 0.16645148 0.19766031 0.22673552 0.24052724 0.22907826 0.19805063 0.15185584 0.0923408 0.032433778][0.16168699 0.1541681 0.14667478 0.15002659 0.1660765 0.18407279 0.20598526 0.23652901 0.26446411 0.27374002 0.25442359 0.21631303 0.1653745 0.10207433 0.038442608][0.18325537 0.17968854 0.16922846 0.1647926 0.17181064 0.18110956 0.19509116 0.22053328 0.2445972 0.25000623 0.22718409 0.18856658 0.1402685 0.081942007 0.023939332][0.1606428 0.15825947 0.1445215 0.13237444 0.12948304 0.12909395 0.13396017 0.15223189 0.17111622 0.17415988 0.15283228 0.11995807 0.08094205 0.035087161 -0.0091324123][0.09801311 0.093870007 0.07820075 0.061789453 0.052590318 0.046225466 0.045057956 0.056874253 0.070697688 0.072710089 0.056135889 0.032092627 0.0052068294 -0.024863571 -0.051524203][0.021463279 0.014505016 -0.00069278531 -0.016762527 -0.027100122 -0.034347307 -0.037194498 -0.029735211 -0.020119494 -0.018496843 -0.029581996 -0.045023311 -0.060864855 -0.076629616 -0.0875111][-0.039269432 -0.047846597 -0.060464755 -0.073009022 -0.080789283 -0.085673667 -0.087727025 -0.083204143 -0.077094167 -0.075870231 -0.082633488 -0.091811717 -0.099913105 -0.10575008 -0.10614134][-0.067804143 -0.07579127 -0.084643371 -0.092764266 -0.09710665 -0.099319749 -0.10053373 -0.098341815 -0.094863243 -0.093968466 -0.098094106 -0.10395853 -0.10818761 -0.10934758 -0.10569324][-0.0660654 -0.07182803 -0.077111058 -0.081807546 -0.083823964 -0.084682755 -0.085957311 -0.085441612 -0.083215043 -0.08193282 -0.084246166 -0.088613182 -0.09184096 -0.092693 -0.09037666][-0.047978181 -0.051423479 -0.0542304 -0.056864392 -0.057776269 -0.058431461 -0.060260832 -0.0605412 -0.058313884 -0.055940531 -0.056660358 -0.0602637 -0.064057395 -0.067142539 -0.06895192]]...]
INFO - root - 2017-12-11 04:42:05.273648: step 14810, loss = 0.68, batch loss = 0.62 (13.6 examples/sec; 0.586 sec/batch; 51h:43m:16s remains)
INFO - root - 2017-12-11 04:42:10.487326: step 14820, loss = 0.68, batch loss = 0.62 (14.8 examples/sec; 0.540 sec/batch; 47h:39m:40s remains)
INFO - root - 2017-12-11 04:42:16.006390: step 14830, loss = 0.70, batch loss = 0.64 (14.1 examples/sec; 0.566 sec/batch; 49h:57m:47s remains)
INFO - root - 2017-12-11 04:42:21.520979: step 14840, loss = 0.69, batch loss = 0.63 (14.0 examples/sec; 0.571 sec/batch; 50h:21m:19s remains)
INFO - root - 2017-12-11 04:42:27.012204: step 14850, loss = 0.69, batch loss = 0.63 (14.8 examples/sec; 0.541 sec/batch; 47h:42m:42s remains)
INFO - root - 2017-12-11 04:42:32.548586: step 14860, loss = 0.70, batch loss = 0.64 (14.3 examples/sec; 0.559 sec/batch; 49h:19m:32s remains)
INFO - root - 2017-12-11 04:42:38.055083: step 14870, loss = 0.70, batch loss = 0.64 (14.5 examples/sec; 0.551 sec/batch; 48h:36m:10s remains)
INFO - root - 2017-12-11 04:42:43.551463: step 14880, loss = 0.68, batch loss = 0.62 (14.2 examples/sec; 0.563 sec/batch; 49h:41m:49s remains)
INFO - root - 2017-12-11 04:42:49.139781: step 14890, loss = 0.69, batch loss = 0.63 (13.4 examples/sec; 0.598 sec/batch; 52h:46m:12s remains)
INFO - root - 2017-12-11 04:42:54.788685: step 14900, loss = 0.70, batch loss = 0.64 (14.8 examples/sec; 0.539 sec/batch; 47h:33m:48s remains)
2017-12-11 04:42:55.400263: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.078573205 0.1430185 0.19778776 0.21583833 0.1947051 0.15671074 0.12628455 0.11453087 0.10933877 0.096656479 0.076722287 0.059999984 0.048205808 0.039431933 0.0315692][0.11087915 0.17268412 0.2158929 0.21886729 0.18906116 0.15601349 0.14217089 0.14618696 0.14654708 0.12852702 0.099430971 0.077315405 0.064503081 0.054985587 0.043692667][0.13341631 0.18841913 0.21960297 0.2114798 0.17747028 0.15015988 0.14880189 0.16243345 0.16520749 0.14476545 0.11333983 0.092230372 0.082682893 0.07501737 0.0628645][0.14119588 0.18368413 0.20295624 0.19031246 0.15987223 0.14024217 0.14471276 0.15823817 0.15830687 0.13874434 0.11324848 0.0996785 0.096582994 0.093031839 0.08447621][0.13143885 0.15909424 0.16911185 0.15984806 0.14199154 0.13400224 0.14109138 0.14806214 0.14111793 0.1225706 0.10588893 0.10105401 0.10371688 0.10463602 0.1039519][0.11587117 0.13570745 0.14409766 0.1437595 0.1404155 0.14175113 0.14658463 0.14318486 0.12774429 0.11039384 0.10236286 0.10396242 0.10824688 0.1107564 0.11905601][0.10145092 0.12180332 0.13454367 0.14201477 0.14624806 0.14838147 0.14511813 0.13051972 0.1095101 0.0969744 0.09912958 0.10684503 0.1101918 0.11084693 0.12567094][0.099394754 0.12124048 0.1355738 0.14302543 0.14439091 0.13974705 0.12685288 0.1048771 0.083644129 0.078954682 0.09172947 0.10652116 0.10941479 0.10695112 0.12489744][0.11109497 0.13309188 0.14650436 0.15149912 0.14823057 0.13771358 0.11893634 0.09267357 0.069996059 0.067535862 0.084712937 0.10519905 0.11095113 0.10905033 0.12812056][0.12280767 0.14483926 0.15788487 0.16367187 0.16075206 0.15075077 0.13238987 0.10423616 0.075705923 0.065034993 0.07608562 0.0970491 0.10828095 0.11328258 0.13576142][0.11814441 0.13855913 0.15108036 0.15958916 0.16159956 0.15850839 0.14760831 0.12209103 0.087424 0.063441813 0.061809722 0.077969149 0.094047666 0.10838763 0.13587809][0.10695945 0.12492605 0.13593839 0.14533165 0.15028663 0.15209813 0.14869525 0.12848178 0.091527686 0.057030324 0.043287523 0.052734692 0.07138855 0.093107469 0.12413209][0.10836579 0.12471128 0.13523713 0.14570758 0.15196607 0.15409914 0.15219331 0.13375409 0.095346332 0.054020327 0.031306427 0.034268893 0.052300934 0.075979814 0.10558065][0.1142612 0.12967935 0.14239638 0.15861484 0.1706474 0.17352007 0.16764744 0.14386664 0.10091819 0.055055682 0.027475331 0.026504472 0.043014474 0.065305308 0.089761779][0.11090422 0.12205088 0.13370857 0.15482886 0.17503327 0.18262559 0.17497937 0.14652884 0.10115514 0.055572461 0.028520888 0.027526803 0.04417783 0.066025369 0.086736485]]...]
INFO - root - 2017-12-11 04:43:00.953353: step 14910, loss = 0.72, batch loss = 0.66 (14.7 examples/sec; 0.544 sec/batch; 48h:01m:05s remains)
INFO - root - 2017-12-11 04:43:06.158855: step 14920, loss = 0.69, batch loss = 0.63 (14.8 examples/sec; 0.542 sec/batch; 47h:46m:46s remains)
INFO - root - 2017-12-11 04:43:11.648701: step 14930, loss = 0.69, batch loss = 0.64 (14.3 examples/sec; 0.558 sec/batch; 49h:11m:59s remains)
INFO - root - 2017-12-11 04:43:17.140228: step 14940, loss = 0.70, batch loss = 0.64 (14.7 examples/sec; 0.543 sec/batch; 47h:52m:00s remains)
INFO - root - 2017-12-11 04:43:22.732425: step 14950, loss = 0.70, batch loss = 0.64 (14.5 examples/sec; 0.552 sec/batch; 48h:41m:54s remains)
INFO - root - 2017-12-11 04:43:28.225628: step 14960, loss = 0.71, batch loss = 0.65 (15.1 examples/sec; 0.528 sec/batch; 46h:36m:53s remains)
INFO - root - 2017-12-11 04:43:33.681748: step 14970, loss = 0.68, batch loss = 0.63 (14.5 examples/sec; 0.551 sec/batch; 48h:38m:25s remains)
INFO - root - 2017-12-11 04:43:39.249739: step 14980, loss = 0.70, batch loss = 0.64 (14.6 examples/sec; 0.548 sec/batch; 48h:20m:14s remains)
INFO - root - 2017-12-11 04:43:44.753351: step 14990, loss = 0.71, batch loss = 0.65 (14.8 examples/sec; 0.540 sec/batch; 47h:39m:04s remains)
INFO - root - 2017-12-11 04:43:50.178265: step 15000, loss = 0.71, batch loss = 0.65 (14.7 examples/sec; 0.545 sec/batch; 48h:04m:30s remains)
2017-12-11 04:43:50.759422: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.061985683 -0.065116048 -0.060663644 -0.045704693 -0.019756114 0.011809075 0.037681431 0.048860263 0.04188852 0.015627343 -0.020790646 -0.055284839 -0.077097379 -0.084356345 -0.078931436][-0.067338958 -0.067098938 -0.055823654 -0.026959559 0.020958444 0.077369325 0.12292528 0.14446357 0.13744207 0.099430807 0.041764271 -0.017187949 -0.059227481 -0.08039847 -0.08247859][-0.068006366 -0.061538476 -0.039975151 0.00795581 0.08399637 0.17121732 0.24089029 0.27528688 0.26959962 0.21844773 0.13388608 0.041824985 -0.028541662 -0.068225063 -0.080952823][-0.061257023 -0.044567421 -0.0082873916 0.061662666 0.16599888 0.28306705 0.37650612 0.42389247 0.41898122 0.35401565 0.24152744 0.11391789 0.011738016 -0.049647607 -0.07563372][-0.032732483 0.0035539477 0.061696153 0.15441082 0.28049171 0.41841161 0.52893704 0.58441359 0.57546771 0.49576986 0.3609958 0.20525734 0.073552549 -0.013453095 -0.058132488][0.026160035 0.092265986 0.1784416 0.29148006 0.42795575 0.57210416 0.68742591 0.74062431 0.719177 0.62441874 0.477683 0.30737728 0.15295458 0.039150223 -0.0275712][0.10717249 0.20903134 0.32557341 0.4551546 0.59116703 0.7264151 0.8315689 0.87001044 0.82987511 0.72306877 0.57618076 0.40447563 0.23479444 0.097293615 0.0097743534][0.19093028 0.3243539 0.46418613 0.59919208 0.72018194 0.82836843 0.90629923 0.92084265 0.86457682 0.75668937 0.62234241 0.46083915 0.28668365 0.13598159 0.036022067][0.24444024 0.39200526 0.5371483 0.66003203 0.74959707 0.81447178 0.85389131 0.84595269 0.78604794 0.69148195 0.58009112 0.43973243 0.27741298 0.13183813 0.034498185][0.24025953 0.37813029 0.50686759 0.60154355 0.65070111 0.6693238 0.67319256 0.65341139 0.60425472 0.53360683 0.45004243 0.33790451 0.20233017 0.080246709 0.00091818243][0.1760032 0.2823723 0.3766095 0.43399557 0.44547093 0.4296093 0.41181394 0.39130855 0.359328 0.31406802 0.25775635 0.1777553 0.079875194 -0.0051043322 -0.054983247][0.078178644 0.14274801 0.19578622 0.21784747 0.20445164 0.1734307 0.14951906 0.13400128 0.11673455 0.09142413 0.058308914 0.010180715 -0.047000617 -0.09139397 -0.10954315][-0.017877571 0.0058552553 0.021198688 0.0167979 -0.0068950905 -0.036739361 -0.055796869 -0.064027943 -0.070607178 -0.081448749 -0.096084341 -0.11705643 -0.1393207 -0.15018126 -0.14372011][-0.091435738 -0.097918235 -0.10774847 -0.12494074 -0.14675529 -0.1664201 -0.17621417 -0.17766404 -0.17737606 -0.17890699 -0.18101475 -0.18335196 -0.18225807 -0.17216621 -0.15234581][-0.12858023 -0.14846452 -0.16703337 -0.18485591 -0.19935499 -0.20853886 -0.21081632 -0.20831335 -0.20498411 -0.20230415 -0.19871244 -0.19256721 -0.18135856 -0.16375282 -0.14158995]]...]
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-SGD-lr0.001-clip50-initfromfixqian30k-fixqian
INFO - root - 2017-12-11 04:43:55.923963: step 15010, loss = 0.70, batch loss = 0.64 (14.7 examples/sec; 0.544 sec/batch; 47h:58m:56s remains)
INFO - root - 2017-12-11 04:44:01.420420: step 15020, loss = 0.71, batch loss = 0.65 (14.8 examples/sec; 0.540 sec/batch; 47h:35m:53s remains)
INFO - root - 2017-12-11 04:44:06.859860: step 15030, loss = 0.69, batch loss = 0.63 (15.1 examples/sec; 0.531 sec/batch; 46h:51m:06s remains)
INFO - root - 2017-12-11 04:44:12.306413: step 15040, loss = 0.71, batch loss = 0.65 (14.8 examples/sec; 0.542 sec/batch; 47h:47m:57s remains)
INFO - root - 2017-12-11 04:44:17.789744: step 15050, loss = 0.69, batch loss = 0.63 (14.0 examples/sec; 0.570 sec/batch; 50h:15m:31s remains)
INFO - root - 2017-12-11 04:44:23.390446: step 15060, loss = 0.68, batch loss = 0.63 (14.4 examples/sec; 0.555 sec/batch; 48h:57m:51s remains)
INFO - root - 2017-12-11 04:44:28.881844: step 15070, loss = 0.71, batch loss = 0.65 (14.9 examples/sec; 0.538 sec/batch; 47h:26m:26s remains)
INFO - root - 2017-12-11 04:44:34.319843: step 15080, loss = 0.69, batch loss = 0.63 (15.2 examples/sec; 0.527 sec/batch; 46h:30m:10s remains)
INFO - root - 2017-12-11 04:44:39.727994: step 15090, loss = 0.70, batch loss = 0.64 (14.6 examples/sec; 0.548 sec/batch; 48h:18m:53s remains)
INFO - root - 2017-12-11 04:44:45.218770: step 15100, loss = 0.71, batch loss = 0.65 (13.3 examples/sec; 0.601 sec/batch; 52h:57m:08s remains)
2017-12-11 04:44:45.810658: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.015845969 -0.010899133 -0.0026920387 0.0091826469 0.02108576 0.029765688 0.034933142 0.034018937 0.023165556 0.0054053897 -0.0086614769 -0.011299008 -0.0047680219 0.0039941329 0.010980157][0.007560452 0.018660078 0.034354348 0.055920549 0.079133756 0.099309184 0.11237537 0.11205781 0.0937589 0.063016988 0.03607814 0.026159853 0.033270471 0.046283469 0.056436393][0.03744144 0.057063419 0.083675407 0.11896148 0.15796223 0.1935284 0.21623045 0.21496448 0.1852513 0.13710718 0.093783833 0.073906817 0.079281926 0.0957908 0.10984626][0.0665039 0.094362095 0.13164186 0.17968042 0.23291965 0.28180566 0.31272596 0.31021848 0.26940078 0.20584063 0.14971174 0.12308631 0.1274204 0.14662963 0.16380875][0.092585117 0.12568466 0.16865835 0.2231082 0.28467384 0.3433165 0.38236091 0.38171217 0.33550063 0.26314628 0.19949895 0.16858564 0.17164072 0.19163401 0.20944962][0.11948278 0.15717919 0.20407017 0.2625756 0.33101925 0.39989153 0.44814444 0.45076582 0.40163839 0.32239291 0.2501711 0.21072741 0.20806816 0.22514811 0.24037163][0.15705727 0.20287497 0.25732583 0.32343319 0.40081227 0.48020652 0.53628504 0.54004407 0.48441458 0.39308614 0.30644092 0.25258538 0.23825304 0.24723376 0.25681463][0.19688258 0.25016332 0.3097637 0.37927514 0.45851249 0.5405634 0.60013819 0.60588169 0.54872566 0.45120388 0.3549394 0.28827491 0.259163 0.25556511 0.25714236][0.22488596 0.27976048 0.33631778 0.39921275 0.46941161 0.54546517 0.60604388 0.61941081 0.57305306 0.48398045 0.39034626 0.3172853 0.27394632 0.25495937 0.24605519][0.23979479 0.29113719 0.338501 0.38816988 0.44355354 0.50857544 0.56668484 0.58749318 0.55602044 0.48215362 0.39816281 0.32524166 0.27335763 0.24271844 0.225361][0.23424786 0.28042591 0.3186191 0.35608509 0.39812553 0.45170122 0.50362766 0.525715 0.5025093 0.43938553 0.3637206 0.29430237 0.24115133 0.20694867 0.18710166][0.2034719 0.2440649 0.27500385 0.30296254 0.33358297 0.37558472 0.41877595 0.43822533 0.41938576 0.36436465 0.29620278 0.23179321 0.18130565 0.14865018 0.13078788][0.14206158 0.17201394 0.19285214 0.20984565 0.22818582 0.2575483 0.29137418 0.30926827 0.29832697 0.25758839 0.20405561 0.15071803 0.1073148 0.079465985 0.06573315][0.069326736 0.08479999 0.092937641 0.097532704 0.10350122 0.12010744 0.1440292 0.16018929 0.15833808 0.13592841 0.10287446 0.066947475 0.036025424 0.016624283 0.0088107968][0.0071503394 0.010284788 0.0086379936 0.0045892941 0.002481949 0.0089325905 0.022353867 0.032982707 0.034427118 0.024549162 0.0080556171 -0.011019303 -0.027612729 -0.03651968 -0.037734743]]...]
INFO - root - 2017-12-11 04:44:51.017692: step 15110, loss = 0.70, batch loss = 0.64 (14.4 examples/sec; 0.556 sec/batch; 48h:59m:32s remains)
INFO - root - 2017-12-11 04:44:56.563050: step 15120, loss = 0.71, batch loss = 0.66 (14.5 examples/sec; 0.550 sec/batch; 48h:31m:44s remains)
INFO - root - 2017-12-11 04:45:02.051633: step 15130, loss = 0.69, batch loss = 0.64 (14.5 examples/sec; 0.550 sec/batch; 48h:30m:55s remains)
INFO - root - 2017-12-11 04:45:07.507324: step 15140, loss = 0.71, batch loss = 0.65 (14.2 examples/sec; 0.564 sec/batch; 49h:40m:38s remains)
INFO - root - 2017-12-11 04:45:13.078342: step 15150, loss = 0.69, batch loss = 0.63 (13.9 examples/sec; 0.574 sec/batch; 50h:38m:19s remains)
INFO - root - 2017-12-11 04:45:18.566820: step 15160, loss = 0.70, batch loss = 0.64 (14.8 examples/sec; 0.541 sec/batch; 47h:41m:23s remains)
INFO - root - 2017-12-11 04:45:24.068091: step 15170, loss = 0.71, batch loss = 0.65 (14.7 examples/sec; 0.545 sec/batch; 48h:00m:14s remains)
INFO - root - 2017-12-11 04:45:29.596780: step 15180, loss = 0.68, batch loss = 0.62 (14.6 examples/sec; 0.547 sec/batch; 48h:11m:21s remains)
INFO - root - 2017-12-11 04:45:35.115286: step 15190, loss = 0.71, batch loss = 0.65 (14.4 examples/sec; 0.555 sec/batch; 48h:53m:54s remains)
INFO - root - 2017-12-11 04:45:40.603946: step 15200, loss = 0.68, batch loss = 0.62 (14.4 examples/sec; 0.556 sec/batch; 49h:01m:31s remains)
2017-12-11 04:45:41.204380: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.12023035 0.18239191 0.24314435 0.28226942 0.29364333 0.28488487 0.27254906 0.26180696 0.24593423 0.21019916 0.14319426 0.057664134 -0.022775805 -0.082694128 -0.11370806][0.06898874 0.13666627 0.2048863 0.24993333 0.26798707 0.26464248 0.25301862 0.23779593 0.2163749 0.17611696 0.1073236 0.024238473 -0.049083661 -0.10017946 -0.12419283][0.041566078 0.10829542 0.17881273 0.22918233 0.257256 0.26425457 0.25757861 0.23926307 0.20986192 0.1597155 0.082899787 -0.0026605988 -0.071611039 -0.11437656 -0.13110913][0.039587375 0.098328158 0.16441947 0.218444 0.26078197 0.2861529 0.29317835 0.27852923 0.2435855 0.18116166 0.0899238 -0.0062186932 -0.0794503 -0.12078837 -0.13381989][0.053899378 0.09945441 0.15553504 0.21126363 0.27005407 0.32011592 0.34954417 0.34774241 0.31431177 0.24283598 0.1357156 0.02232147 -0.065050833 -0.1144955 -0.13051453][0.081895374 0.11388399 0.15872884 0.21455307 0.28766856 0.36151111 0.41491598 0.43029684 0.40328443 0.32696632 0.2058313 0.072562382 -0.03508731 -0.099597029 -0.12376519][0.12110941 0.14299756 0.17858186 0.2342703 0.31829277 0.41086164 0.48474893 0.51639491 0.49618429 0.41628778 0.28253242 0.12953077 -6.4605716e-05 -0.082442254 -0.11705287][0.15471363 0.16989683 0.19905815 0.2541807 0.34423506 0.44890475 0.53730458 0.58035851 0.5631671 0.47840077 0.33449611 0.16757001 0.022634804 -0.072549626 -0.11436199][0.16964637 0.18277036 0.21092674 0.26739559 0.35945454 0.46855935 0.56268996 0.60931462 0.58943993 0.49768075 0.34705746 0.17514414 0.026233872 -0.072146595 -0.11535113][0.17302805 0.18835582 0.22051512 0.28036946 0.37073869 0.47528017 0.56275839 0.60047126 0.56953216 0.46870813 0.316196 0.14977515 0.0099388128 -0.080473252 -0.11882922][0.16183828 0.18022011 0.21776246 0.28002289 0.36405006 0.45439535 0.52260607 0.54013884 0.49313515 0.38543338 0.23937157 0.090883672 -0.02654767 -0.09812101 -0.12504618][0.11617 0.13470039 0.17430653 0.23478884 0.30817136 0.37857279 0.42165768 0.41633353 0.35683167 0.251668 0.12428576 0.0061691171 -0.078195117 -0.12304233 -0.13360432][0.029605318 0.044509508 0.081752509 0.1348954 0.19338077 0.24214372 0.26228565 0.24083607 0.17987788 0.090892412 -0.0053965389 -0.084613316 -0.13144374 -0.14750172 -0.14095955][-0.065267116 -0.053572826 -0.021435481 0.019433891 0.059322178 0.086779945 0.08951737 0.061787836 0.010757226 -0.052182257 -0.11195556 -0.1525868 -0.16676377 -0.16026406 -0.14193422][-0.12869106 -0.120118 -0.095697276 -0.069323324 -0.047767088 -0.037715942 -0.045421507 -0.071389817 -0.10687096 -0.14308824 -0.17117713 -0.18275981 -0.17620008 -0.15820996 -0.13616937]]...]
INFO - root - 2017-12-11 04:45:46.487891: step 15210, loss = 0.71, batch loss = 0.65 (14.5 examples/sec; 0.551 sec/batch; 48h:31m:39s remains)
INFO - root - 2017-12-11 04:45:51.967222: step 15220, loss = 0.68, batch loss = 0.62 (14.1 examples/sec; 0.566 sec/batch; 49h:52m:20s remains)
INFO - root - 2017-12-11 04:45:57.454771: step 15230, loss = 0.69, batch loss = 0.63 (14.5 examples/sec; 0.550 sec/batch; 48h:27m:40s remains)
INFO - root - 2017-12-11 04:46:02.965952: step 15240, loss = 0.69, batch loss = 0.63 (14.5 examples/sec; 0.553 sec/batch; 48h:43m:30s remains)
INFO - root - 2017-12-11 04:46:08.571713: step 15250, loss = 0.71, batch loss = 0.65 (14.7 examples/sec; 0.544 sec/batch; 47h:58m:16s remains)
INFO - root - 2017-12-11 04:46:14.055453: step 15260, loss = 0.70, batch loss = 0.65 (14.8 examples/sec; 0.541 sec/batch; 47h:40m:17s remains)
INFO - root - 2017-12-11 04:46:19.587995: step 15270, loss = 0.69, batch loss = 0.63 (14.6 examples/sec; 0.549 sec/batch; 48h:25m:05s remains)
INFO - root - 2017-12-11 04:46:25.166106: step 15280, loss = 0.70, batch loss = 0.65 (14.5 examples/sec; 0.551 sec/batch; 48h:30m:35s remains)
INFO - root - 2017-12-11 04:46:30.601835: step 15290, loss = 0.71, batch loss = 0.65 (14.7 examples/sec; 0.543 sec/batch; 47h:49m:22s remains)
INFO - root - 2017-12-11 04:46:36.146808: step 15300, loss = 0.69, batch loss = 0.63 (14.5 examples/sec; 0.552 sec/batch; 48h:37m:56s remains)
2017-12-11 04:46:36.706581: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.032134429 0.048413493 0.075615086 0.10391022 0.12583265 0.13575502 0.13338022 0.11901661 0.096399605 0.074311391 0.06158977 0.060186923 0.066863909 0.076156043 0.0820824][0.0346746 0.053200755 0.084646329 0.11808115 0.14369425 0.1538426 0.14793244 0.12832758 0.10152248 0.077324174 0.063435391 0.061756868 0.069342576 0.08068984 0.089142807][0.042530086 0.063076749 0.0973373 0.13409987 0.16223066 0.1724578 0.16271891 0.13778806 0.10679853 0.080202058 0.06319005 0.057240203 0.062011324 0.073261172 0.084136844][0.051881213 0.074276768 0.10984097 0.14840476 0.17895108 0.19106625 0.1802467 0.15332857 0.12071222 0.092663959 0.071038522 0.056674793 0.053312693 0.059950784 0.070507452][0.059206273 0.08157251 0.11585142 0.15376541 0.18610217 0.20224145 0.19494165 0.17178538 0.14309406 0.11763804 0.093216427 0.069253147 0.053948827 0.050538309 0.055327758][0.063814759 0.084609084 0.11476923 0.1487733 0.18070965 0.20116955 0.20064418 0.18616541 0.16692226 0.14915925 0.12628214 0.0954119 0.068258323 0.051866084 0.046011459][0.069552846 0.088163473 0.11202995 0.13869996 0.16618289 0.18753041 0.19262312 0.1877396 0.18065046 0.17446424 0.15830955 0.12670286 0.092771567 0.065475568 0.047615476][0.084206194 0.10184786 0.11874264 0.13568339 0.15389499 0.16923396 0.17354314 0.17346783 0.17597072 0.18119442 0.17497721 0.14925826 0.11603735 0.08377555 0.057019871][0.11500861 0.13370143 0.14448163 0.15112787 0.15613614 0.1575872 0.15180854 0.14755504 0.15207951 0.16348423 0.1662638 0.15047242 0.12469272 0.095001876 0.06586954][0.16132264 0.18238601 0.18822612 0.18497117 0.1747403 0.15691721 0.1334548 0.11629656 0.11415934 0.12416969 0.13147177 0.12539649 0.11010247 0.088276632 0.06316749][0.20851935 0.23150499 0.23318818 0.22165656 0.19756043 0.1608032 0.11839014 0.085246749 0.071564533 0.074256495 0.080670811 0.080807507 0.074869573 0.062418941 0.045124739][0.23591714 0.25822183 0.25589687 0.23848659 0.20498914 0.15551923 0.10034411 0.055704597 0.03205004 0.026376294 0.028977346 0.031568896 0.0316256 0.026729375 0.017377473][0.22881384 0.24710841 0.24060559 0.21964805 0.18248148 0.12957074 0.072211668 0.025564028 -0.0013031884 -0.011396565 -0.011664532 -0.008497403 -0.0056713717 -0.0060766451 -0.0096915178][0.18438084 0.19595484 0.18554807 0.16348322 0.12852788 0.081576407 0.032646276 -0.0062376368 -0.028549265 -0.037289295 -0.037414756 -0.033705086 -0.029936898 -0.02834297 -0.028792048][0.1191735 0.12327701 0.11006938 0.089110494 0.060609553 0.025592515 -0.0087410538 -0.034427922 -0.047761697 -0.051501755 -0.049250543 -0.044601738 -0.040658679 -0.038622744 -0.037994709]]...]
INFO - root - 2017-12-11 04:46:42.009390: step 15310, loss = 0.70, batch loss = 0.64 (14.7 examples/sec; 0.544 sec/batch; 47h:55m:55s remains)
INFO - root - 2017-12-11 04:46:47.505718: step 15320, loss = 0.70, batch loss = 0.64 (14.4 examples/sec; 0.557 sec/batch; 49h:06m:00s remains)
INFO - root - 2017-12-11 04:46:53.055302: step 15330, loss = 0.71, batch loss = 0.65 (13.7 examples/sec; 0.586 sec/batch; 51h:35m:40s remains)
INFO - root - 2017-12-11 04:46:58.576836: step 15340, loss = 0.70, batch loss = 0.64 (14.5 examples/sec; 0.550 sec/batch; 48h:29m:48s remains)
INFO - root - 2017-12-11 04:47:04.065889: step 15350, loss = 0.70, batch loss = 0.64 (14.6 examples/sec; 0.546 sec/batch; 48h:06m:33s remains)
INFO - root - 2017-12-11 04:47:09.548036: step 15360, loss = 0.69, batch loss = 0.63 (14.0 examples/sec; 0.571 sec/batch; 50h:18m:08s remains)
INFO - root - 2017-12-11 04:47:15.067112: step 15370, loss = 0.70, batch loss = 0.65 (14.8 examples/sec; 0.542 sec/batch; 47h:43m:57s remains)
INFO - root - 2017-12-11 04:47:20.554448: step 15380, loss = 0.69, batch loss = 0.63 (14.1 examples/sec; 0.569 sec/batch; 50h:05m:24s remains)
INFO - root - 2017-12-11 04:47:26.055460: step 15390, loss = 0.69, batch loss = 0.64 (14.8 examples/sec; 0.542 sec/batch; 47h:43m:53s remains)
INFO - root - 2017-12-11 04:47:31.356176: step 15400, loss = 0.71, batch loss = 0.65 (16.1 examples/sec; 0.497 sec/batch; 43h:44m:46s remains)
2017-12-11 04:47:31.904458: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.034463715 0.023045855 0.010474049 -3.1081203e-05 -0.0053571067 -0.0040065371 0.0020518152 0.0086096833 0.011271279 0.0080814594 0.0011840392 -0.0076189875 -0.01903272 -0.03243864 -0.046160113][0.12151197 0.10256094 0.082024693 0.066852845 0.062514938 0.069219872 0.080052912 0.087177679 0.085691 0.076043911 0.062730618 0.048444554 0.031584568 0.011428361 -0.011097008][0.2259118 0.20410064 0.18173476 0.16817386 0.17022678 0.18476647 0.19917136 0.20245183 0.19117343 0.17043228 0.14783023 0.12724736 0.10496819 0.077895857 0.045224238][0.31458831 0.29627791 0.27918166 0.27289632 0.28367636 0.30448288 0.31875762 0.31399015 0.28998238 0.25826058 0.22927311 0.2067389 0.18333118 0.15291718 0.11207341][0.37598422 0.36153549 0.34931362 0.35010624 0.36978379 0.39840829 0.41580117 0.40665945 0.37293404 0.332721 0.29948023 0.27651185 0.25334954 0.22151881 0.17494385][0.42418072 0.40904006 0.39443883 0.39685804 0.42469174 0.46741143 0.49950761 0.49808413 0.46316713 0.4176935 0.37865937 0.35012531 0.3216565 0.28483906 0.23151264][0.46355829 0.44460693 0.42286441 0.42381278 0.45930576 0.51919472 0.57239783 0.58599776 0.55447513 0.50359815 0.4552519 0.41636121 0.37832528 0.33457565 0.2757383][0.48680487 0.4672617 0.44170716 0.44353226 0.48511508 0.55633485 0.62338209 0.64555836 0.61215252 0.55030257 0.48872286 0.43825886 0.39218593 0.34617779 0.28871986][0.48925596 0.47500116 0.45276541 0.45913962 0.502693 0.57380152 0.64082342 0.661456 0.62250811 0.55056536 0.47956753 0.42126107 0.36995661 0.32379973 0.27015841][0.48843655 0.48525453 0.472574 0.48531276 0.52608472 0.58766431 0.644398 0.6582166 0.61592376 0.54024172 0.4661485 0.40265602 0.34525403 0.29492387 0.24053346][0.49769226 0.509309 0.50835663 0.52343696 0.55241889 0.59331214 0.63024211 0.63412857 0.59327114 0.523418 0.4557184 0.39422578 0.33442545 0.27922109 0.22050676][0.505455 0.53112793 0.53961992 0.55125409 0.56156 0.57452846 0.58581257 0.57806361 0.54229265 0.48679745 0.43558279 0.38664663 0.33241391 0.27548403 0.21157582][0.50244564 0.53546226 0.54798561 0.55379325 0.54713017 0.5359056 0.52513546 0.50830609 0.48078007 0.44569632 0.41784033 0.38856843 0.34514493 0.28857306 0.21787842][0.50390077 0.53317243 0.54358023 0.54306144 0.52502555 0.49833488 0.47402439 0.45425636 0.44040537 0.43241444 0.43405831 0.42895168 0.39811802 0.34038129 0.258313][0.51947123 0.53832507 0.54139823 0.53561342 0.51301527 0.48110557 0.45359352 0.43813637 0.44205594 0.46309307 0.49329641 0.50840235 0.48388472 0.41818255 0.31903437]]...]
INFO - root - 2017-12-11 04:47:37.378924: step 15410, loss = 0.68, batch loss = 0.62 (14.7 examples/sec; 0.544 sec/batch; 47h:54m:32s remains)
INFO - root - 2017-12-11 04:47:42.848261: step 15420, loss = 0.69, batch loss = 0.63 (14.5 examples/sec; 0.553 sec/batch; 48h:41m:54s remains)
INFO - root - 2017-12-11 04:47:48.371833: step 15430, loss = 0.70, batch loss = 0.65 (13.1 examples/sec; 0.610 sec/batch; 53h:42m:16s remains)
INFO - root - 2017-12-11 04:47:53.933975: step 15440, loss = 0.70, batch loss = 0.64 (14.9 examples/sec; 0.538 sec/batch; 47h:23m:56s remains)
INFO - root - 2017-12-11 04:47:59.387388: step 15450, loss = 0.70, batch loss = 0.64 (14.4 examples/sec; 0.557 sec/batch; 49h:04m:44s remains)
INFO - root - 2017-12-11 04:48:04.915685: step 15460, loss = 0.69, batch loss = 0.63 (14.4 examples/sec; 0.556 sec/batch; 48h:59m:32s remains)
INFO - root - 2017-12-11 04:48:10.494900: step 15470, loss = 0.70, batch loss = 0.64 (15.0 examples/sec; 0.532 sec/batch; 46h:49m:41s remains)
INFO - root - 2017-12-11 04:48:16.009422: step 15480, loss = 0.68, batch loss = 0.62 (14.8 examples/sec; 0.541 sec/batch; 47h:40m:49s remains)
INFO - root - 2017-12-11 04:48:21.563777: step 15490, loss = 0.71, batch loss = 0.65 (14.5 examples/sec; 0.552 sec/batch; 48h:35m:10s remains)
INFO - root - 2017-12-11 04:48:26.711970: step 15500, loss = 0.70, batch loss = 0.64 (14.3 examples/sec; 0.558 sec/batch; 49h:09m:36s remains)
2017-12-11 04:48:27.304490: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.41052878 0.4320727 0.44136539 0.44381863 0.43498868 0.40424725 0.34891969 0.28569692 0.23018956 0.18390897 0.1533646 0.148666 0.17289928 0.20668639 0.2539914][0.529444 0.54749036 0.54000717 0.51701951 0.48272604 0.43169478 0.36185706 0.2921693 0.24080473 0.20645829 0.1891344 0.19473775 0.22432773 0.26023012 0.31324407][0.56107491 0.56899887 0.54405177 0.50088084 0.45282787 0.39949673 0.33820766 0.28366664 0.25256634 0.23935483 0.23613295 0.24461122 0.26614711 0.29065359 0.33672318][0.50121212 0.49758029 0.46452865 0.42059162 0.38683334 0.36290851 0.34039855 0.32184386 0.31785744 0.32145643 0.32038808 0.31507272 0.30955386 0.30436531 0.32558647][0.37349266 0.36975375 0.35211116 0.3385798 0.35328948 0.38885269 0.42588598 0.44927248 0.46097133 0.45851249 0.43350855 0.39112142 0.33993453 0.29080695 0.27321091][0.22131562 0.23949932 0.26634553 0.3118636 0.39684311 0.50232488 0.5973193 0.65043259 0.65532887 0.61929625 0.54548329 0.44869643 0.34219211 0.244431 0.18461852][0.097223654 0.15648444 0.24521744 0.35894531 0.51018924 0.66882354 0.79694182 0.85467875 0.83194 0.74751312 0.61716068 0.46504372 0.30846754 0.17067575 0.07857085][0.033866029 0.13913327 0.2858412 0.45217094 0.63995647 0.8126592 0.93349874 0.96576822 0.9023869 0.771544 0.59703285 0.40821505 0.22438957 0.070828021 -0.029617921][0.034948349 0.17656153 0.35980836 0.54788917 0.7322191 0.875931 0.95057011 0.93233836 0.82428265 0.66189456 0.47201139 0.28144312 0.10675647 -0.029541735 -0.10975411][0.089347877 0.2520656 0.44621706 0.62562746 0.77365023 0.8590315 0.865956 0.7874161 0.64136636 0.47005048 0.29960176 0.14613111 0.016827103 -0.074686691 -0.11639274][0.17419782 0.34661022 0.53457934 0.68684524 0.78181183 0.79923707 0.73752862 0.6064043 0.43961862 0.28482667 0.16389732 0.075715989 0.011410279 -0.026385164 -0.03054091][0.27009615 0.44508258 0.61796844 0.73586065 0.7750138 0.72994596 0.61429513 0.45261246 0.29047176 0.17554756 0.12084994 0.10575873 0.10362961 0.10703723 0.1225543][0.3475717 0.51371229 0.6630044 0.74500704 0.73644036 0.64698136 0.50545722 0.34669611 0.21818455 0.15950523 0.17076845 0.21479563 0.25192431 0.26943615 0.27973196][0.36756906 0.50908542 0.62660992 0.67577875 0.63643777 0.52961463 0.39535317 0.27117237 0.19620487 0.1957642 0.2565459 0.33227715 0.37933651 0.38542861 0.37265515][0.30592313 0.408314 0.48711103 0.50792533 0.45722461 0.36024788 0.25949702 0.18630315 0.16570829 0.20843214 0.2922582 0.37145385 0.40735805 0.39182729 0.35470229]]...]
INFO - root - 2017-12-11 04:48:32.818626: step 15510, loss = 0.69, batch loss = 0.63 (15.1 examples/sec; 0.531 sec/batch; 46h:44m:44s remains)
INFO - root - 2017-12-11 04:48:38.379710: step 15520, loss = 0.68, batch loss = 0.62 (14.0 examples/sec; 0.571 sec/batch; 50h:18m:11s remains)
INFO - root - 2017-12-11 04:48:43.866922: step 15530, loss = 0.69, batch loss = 0.63 (15.1 examples/sec; 0.531 sec/batch; 46h:43m:07s remains)
INFO - root - 2017-12-11 04:48:49.320339: step 15540, loss = 0.71, batch loss = 0.65 (14.9 examples/sec; 0.539 sec/batch; 47h:25m:42s remains)
INFO - root - 2017-12-11 04:48:54.795271: step 15550, loss = 0.70, batch loss = 0.64 (14.7 examples/sec; 0.546 sec/batch; 48h:03m:04s remains)
INFO - root - 2017-12-11 04:49:00.388715: step 15560, loss = 0.72, batch loss = 0.66 (14.4 examples/sec; 0.557 sec/batch; 49h:00m:03s remains)
INFO - root - 2017-12-11 04:49:05.950533: step 15570, loss = 0.69, batch loss = 0.63 (14.8 examples/sec; 0.541 sec/batch; 47h:37m:57s remains)
INFO - root - 2017-12-11 04:49:11.445527: step 15580, loss = 0.70, batch loss = 0.64 (14.4 examples/sec; 0.556 sec/batch; 48h:54m:23s remains)
INFO - root - 2017-12-11 04:49:16.962027: step 15590, loss = 0.72, batch loss = 0.66 (14.6 examples/sec; 0.549 sec/batch; 48h:20m:58s remains)
INFO - root - 2017-12-11 04:49:22.185642: step 15600, loss = 0.68, batch loss = 0.63 (14.5 examples/sec; 0.553 sec/batch; 48h:40m:31s remains)
2017-12-11 04:49:22.752539: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.031020194 0.048192423 0.069273427 0.093031846 0.11700716 0.13875705 0.14915241 0.1412926 0.12119163 0.099549361 0.08206398 0.0713367 0.071338169 0.082710624 0.095150575][0.079098672 0.10788916 0.14195134 0.1774485 0.21043794 0.23909499 0.25225562 0.24026877 0.21007715 0.17705308 0.1487406 0.12998129 0.12786219 0.14541633 0.16899531][0.12860288 0.17221372 0.22264774 0.27141476 0.31283897 0.34637246 0.36012927 0.34292158 0.30209142 0.25779572 0.21866371 0.19043297 0.18287192 0.20223463 0.23459487][0.17235175 0.23168178 0.29852107 0.35921192 0.4064537 0.44075662 0.4512046 0.42790926 0.37972239 0.33042097 0.28767744 0.25526768 0.24430764 0.26395223 0.30043489][0.21287628 0.29088709 0.37491456 0.44576117 0.49496156 0.52401537 0.524151 0.49107045 0.43756878 0.38999879 0.35234165 0.32267836 0.31229353 0.33169448 0.36768341][0.2457682 0.34557885 0.44800815 0.52901047 0.57970411 0.60227531 0.59068543 0.54801929 0.49282208 0.45243454 0.42487291 0.39927521 0.38659462 0.39853486 0.42414951][0.27465552 0.39319205 0.50922662 0.59654588 0.64767176 0.66706204 0.65133977 0.60919333 0.56242555 0.53587997 0.52047271 0.49504474 0.47176266 0.4646728 0.46780354][0.29045454 0.41791177 0.53712273 0.62241727 0.66923088 0.68631291 0.67396182 0.64369369 0.61581743 0.60941809 0.60832411 0.58248317 0.54579192 0.51512235 0.49031928][0.290665 0.415828 0.527271 0.60169816 0.63763392 0.648857 0.64034277 0.6249876 0.61868119 0.6332919 0.64720714 0.62497693 0.5813722 0.53419828 0.48729858][0.26853913 0.37995851 0.47495332 0.53350657 0.55663234 0.56208444 0.55778378 0.55539131 0.56435162 0.59011447 0.61035395 0.59046048 0.54568583 0.49184513 0.43500349][0.21052806 0.29565403 0.36622527 0.40687025 0.42047372 0.42570332 0.4303205 0.44118536 0.45987967 0.486721 0.502957 0.48077703 0.43631771 0.38286653 0.32657406][0.12751432 0.1786952 0.2192733 0.23927149 0.24317811 0.24830879 0.26072291 0.28172874 0.30586165 0.32961297 0.33945185 0.31650427 0.27686867 0.23099893 0.18378361][0.037828792 0.057562876 0.07077729 0.071714632 0.065444291 0.066159368 0.078236334 0.0997317 0.12202112 0.14025927 0.14576337 0.12770739 0.099351823 0.067693986 0.035617325][-0.036314745 -0.037107054 -0.040684957 -0.050577022 -0.062681809 -0.066876344 -0.060403872 -0.045940347 -0.031010509 -0.01970684 -0.016887141 -0.027547747 -0.042506959 -0.058547135 -0.07500542][-0.085010335 -0.09547931 -0.10538633 -0.11766079 -0.12980579 -0.13566753 -0.13412467 -0.12764376 -0.12099665 -0.11659463 -0.11657978 -0.12231911 -0.12804142 -0.13294758 -0.13792227]]...]
INFO - root - 2017-12-11 04:49:28.189668: step 15610, loss = 0.70, batch loss = 0.64 (14.7 examples/sec; 0.543 sec/batch; 47h:47m:06s remains)
INFO - root - 2017-12-11 04:49:33.745674: step 15620, loss = 0.69, batch loss = 0.63 (13.9 examples/sec; 0.574 sec/batch; 50h:30m:47s remains)
INFO - root - 2017-12-11 04:49:39.306963: step 15630, loss = 0.70, batch loss = 0.65 (14.1 examples/sec; 0.568 sec/batch; 49h:57m:38s remains)
INFO - root - 2017-12-11 04:49:44.795699: step 15640, loss = 0.69, batch loss = 0.64 (14.5 examples/sec; 0.551 sec/batch; 48h:30m:39s remains)
INFO - root - 2017-12-11 04:49:50.295963: step 15650, loss = 0.71, batch loss = 0.65 (14.4 examples/sec; 0.554 sec/batch; 48h:47m:43s remains)
INFO - root - 2017-12-11 04:49:55.829239: step 15660, loss = 0.71, batch loss = 0.65 (14.4 examples/sec; 0.556 sec/batch; 48h:54m:05s remains)
INFO - root - 2017-12-11 04:50:01.320439: step 15670, loss = 0.71, batch loss = 0.65 (14.7 examples/sec; 0.543 sec/batch; 47h:49m:01s remains)
INFO - root - 2017-12-11 04:50:06.880163: step 15680, loss = 0.69, batch loss = 0.64 (13.8 examples/sec; 0.578 sec/batch; 50h:53m:26s remains)
INFO - root - 2017-12-11 04:50:12.111405: step 15690, loss = 0.72, batch loss = 0.66 (27.4 examples/sec; 0.292 sec/batch; 25h:40m:48s remains)
INFO - root - 2017-12-11 04:50:17.639763: step 15700, loss = 0.70, batch loss = 0.65 (14.6 examples/sec; 0.547 sec/batch; 48h:06m:36s remains)
2017-12-11 04:50:18.212624: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.087019235 -0.082499586 -0.07711684 -0.073393069 -0.071585134 -0.070797175 -0.072222367 -0.0755891 -0.080563769 -0.086630784 -0.092512488 -0.098191753 -0.10396259 -0.10964967 -0.11406408][-0.072136365 -0.060871188 -0.049182314 -0.039067596 -0.02980837 -0.019494208 -0.012296501 -0.010863637 -0.016051056 -0.027042333 -0.039437059 -0.052676324 -0.067060962 -0.081558153 -0.094667539][-0.047886569 -0.027762128 -0.0075448286 0.012081055 0.03504185 0.06496805 0.092264883 0.10697121 0.10355518 0.083931133 0.058054116 0.029351771 -0.00096055033 -0.029381087 -0.054355409][-0.013558975 0.019495526 0.052420769 0.0851339 0.12803611 0.18800817 0.24750277 0.28388369 0.28405529 0.25119048 0.20232761 0.14641859 0.08960291 0.039924379 -0.0012750397][0.030048762 0.082829557 0.13631959 0.18781604 0.25519541 0.34916198 0.44395033 0.502199 0.50219011 0.44898245 0.36739987 0.27404529 0.18335907 0.10808259 0.049385674][0.07652428 0.15432404 0.23508173 0.30956674 0.39992276 0.52052063 0.63959479 0.70991349 0.70292556 0.62670088 0.51210636 0.38230115 0.26071164 0.16293594 0.089737087][0.12165289 0.22794129 0.34046796 0.439131 0.54453588 0.67373532 0.79435372 0.85925406 0.83911371 0.74512577 0.6093027 0.45641938 0.3155742 0.2028749 0.11917008][0.16318862 0.29691258 0.43823153 0.55349177 0.65728688 0.76986223 0.86524075 0.9076274 0.87382442 0.77565026 0.63865 0.48197442 0.33597744 0.21654654 0.12655212][0.19201122 0.3424004 0.4986164 0.61544591 0.69984686 0.77499735 0.82655627 0.83721733 0.79433006 0.70714283 0.58865488 0.44760013 0.31109607 0.1946466 0.10485388][0.19800191 0.34644169 0.49570376 0.59557545 0.64713818 0.67556316 0.67935479 0.65974504 0.61433339 0.54781914 0.4592821 0.34709615 0.23289393 0.13093929 0.051211428][0.16752686 0.29367957 0.41482392 0.48387155 0.49949798 0.48703271 0.45512104 0.41716737 0.37694269 0.33423132 0.27809706 0.20033136 0.11626366 0.038364109 -0.022073884][0.095499888 0.18611427 0.26908505 0.30616963 0.2960166 0.26109353 0.21437487 0.17324616 0.14336799 0.12123383 0.092359155 0.046376526 -0.0069721988 -0.057607543 -0.094791375][0.0080808951 0.060854029 0.10679694 0.11917082 0.097955175 0.060156703 0.017167436 -0.016119448 -0.034889109 -0.043296695 -0.054240361 -0.076738827 -0.10497437 -0.13141981 -0.14736025][-0.064106964 -0.041796885 -0.023551505 -0.02573348 -0.04693893 -0.07645037 -0.10693675 -0.12868598 -0.13866159 -0.13988553 -0.14130165 -0.14898041 -0.1596843 -0.16838403 -0.16920438][-0.11084641 -0.1072198 -0.10431767 -0.11156107 -0.12727518 -0.14595319 -0.1638537 -0.17578577 -0.18016386 -0.17867623 -0.17644608 -0.1766182 -0.1772805 -0.17544973 -0.16810811]]...]
INFO - root - 2017-12-11 04:50:23.776224: step 15710, loss = 0.70, batch loss = 0.64 (14.2 examples/sec; 0.562 sec/batch; 49h:29m:45s remains)
INFO - root - 2017-12-11 04:50:29.294880: step 15720, loss = 0.70, batch loss = 0.65 (15.1 examples/sec; 0.531 sec/batch; 46h:43m:05s remains)
INFO - root - 2017-12-11 04:50:34.836453: step 15730, loss = 0.69, batch loss = 0.63 (14.8 examples/sec; 0.540 sec/batch; 47h:31m:18s remains)
INFO - root - 2017-12-11 04:50:40.369647: step 15740, loss = 0.69, batch loss = 0.63 (14.4 examples/sec; 0.556 sec/batch; 48h:54m:40s remains)
INFO - root - 2017-12-11 04:50:45.834691: step 15750, loss = 0.69, batch loss = 0.63 (14.7 examples/sec; 0.543 sec/batch; 47h:46m:12s remains)
INFO - root - 2017-12-11 04:50:51.297240: step 15760, loss = 0.68, batch loss = 0.63 (14.3 examples/sec; 0.559 sec/batch; 49h:10m:28s remains)
INFO - root - 2017-12-11 04:50:56.736609: step 15770, loss = 0.69, batch loss = 0.63 (14.6 examples/sec; 0.548 sec/batch; 48h:11m:05s remains)
INFO - root - 2017-12-11 04:51:02.296714: step 15780, loss = 0.70, batch loss = 0.65 (14.0 examples/sec; 0.571 sec/batch; 50h:11m:42s remains)
INFO - root - 2017-12-11 04:51:07.560228: step 15790, loss = 0.71, batch loss = 0.65 (14.3 examples/sec; 0.561 sec/batch; 49h:18m:57s remains)
INFO - root - 2017-12-11 04:51:13.137809: step 15800, loss = 0.71, batch loss = 0.65 (14.4 examples/sec; 0.556 sec/batch; 48h:53m:17s remains)
2017-12-11 04:51:13.705654: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.045290772 -0.015624866 0.017149722 0.0387186 0.043188 0.035675127 0.018466244 -0.0027774994 -0.023964763 -0.039935689 -0.044627756 -0.03681168 -0.019040959 -0.0019575635 0.005433247][0.00092503359 0.05382935 0.11145304 0.15383618 0.16977501 0.1635607 0.13721162 0.10126668 0.065442309 0.040173534 0.037056327 0.056201652 0.088400789 0.11489632 0.12488104][0.051346958 0.12997386 0.21652453 0.28582576 0.31990597 0.32054427 0.28997761 0.24402815 0.19702791 0.1644979 0.16409563 0.19208486 0.2291939 0.24988978 0.24846335][0.0870376 0.18898682 0.30555531 0.4078503 0.4703835 0.48945364 0.46598488 0.41879085 0.36539719 0.32422274 0.31885198 0.34283888 0.36751124 0.36269698 0.33291432][0.10688791 0.23091722 0.38103712 0.52519959 0.63030112 0.684918 0.6861288 0.649522 0.59212857 0.53389043 0.50437921 0.49899796 0.48432714 0.43095046 0.35662419][0.1228445 0.27212775 0.46205497 0.65712762 0.81673527 0.92190653 0.96262234 0.94391555 0.87813431 0.78761023 0.70933712 0.64613378 0.56667924 0.44836479 0.32168567][0.13887852 0.31234539 0.539136 0.77921146 0.98651177 1.1377724 1.2152318 1.211068 1.1324327 1.0038642 0.86976922 0.74314076 0.59950209 0.42446312 0.2556299][0.13703813 0.32197067 0.56720489 0.82834572 1.0581353 1.2320747 1.3271228 1.326202 1.2341648 1.0786124 0.9075141 0.74138868 0.56333208 0.36515072 0.18386397][0.10514072 0.28099945 0.51746154 0.76856077 0.98984146 1.1580458 1.2477336 1.2388299 1.1384124 0.9771874 0.80187857 0.63309717 0.45852485 0.2742843 0.11232575][0.049846895 0.19541682 0.39466193 0.60421634 0.7869761 0.92423016 0.99105805 0.96981257 0.87111419 0.72694761 0.57793009 0.43841755 0.29724377 0.15377784 0.033414796][-0.019926682 0.079159133 0.21907637 0.36448976 0.48908925 0.58058006 0.61726028 0.58665287 0.50083214 0.38893282 0.28339583 0.19080658 0.1006243 0.013173798 -0.053842098][-0.090136111 -0.04305369 0.030565016 0.10574224 0.16774561 0.21111277 0.21984164 0.18677644 0.12273482 0.050788585 -0.0054833121 -0.046225633 -0.08160869 -0.11284871 -0.12999544][-0.13999747 -0.13323592 -0.11157644 -0.090658329 -0.076070614 -0.068041526 -0.076490924 -0.10488297 -0.14586653 -0.18271811 -0.2001877 -0.20228626 -0.19834092 -0.19125241 -0.17678307][-0.15979446 -0.17441222 -0.18126538 -0.18967016 -0.19979477 -0.20895638 -0.22261745 -0.24270512 -0.26555118 -0.28025252 -0.27788368 -0.26295221 -0.24273635 -0.22058298 -0.1951663][-0.15629633 -0.17741175 -0.19401042 -0.21233326 -0.23011854 -0.24408586 -0.25677797 -0.26936373 -0.28046718 -0.28393421 -0.27558851 -0.25856882 -0.23729888 -0.21441299 -0.1902082]]...]
INFO - root - 2017-12-11 04:51:19.233452: step 15810, loss = 0.70, batch loss = 0.64 (14.6 examples/sec; 0.546 sec/batch; 48h:03m:18s remains)
INFO - root - 2017-12-11 04:51:24.816544: step 15820, loss = 0.69, batch loss = 0.63 (14.1 examples/sec; 0.566 sec/batch; 49h:45m:37s remains)
INFO - root - 2017-12-11 04:51:30.314946: step 15830, loss = 0.69, batch loss = 0.63 (15.1 examples/sec; 0.531 sec/batch; 46h:44m:43s remains)
INFO - root - 2017-12-11 04:51:35.813782: step 15840, loss = 0.70, batch loss = 0.64 (14.4 examples/sec; 0.557 sec/batch; 48h:59m:38s remains)
INFO - root - 2017-12-11 04:51:41.300686: step 15850, loss = 0.69, batch loss = 0.64 (14.4 examples/sec; 0.554 sec/batch; 48h:44m:24s remains)
INFO - root - 2017-12-11 04:51:46.896374: step 15860, loss = 0.70, batch loss = 0.64 (14.5 examples/sec; 0.552 sec/batch; 48h:31m:45s remains)
INFO - root - 2017-12-11 04:51:52.394266: step 15870, loss = 0.69, batch loss = 0.63 (14.7 examples/sec; 0.543 sec/batch; 47h:44m:07s remains)
INFO - root - 2017-12-11 04:51:57.900100: step 15880, loss = 0.70, batch loss = 0.64 (14.8 examples/sec; 0.541 sec/batch; 47h:35m:36s remains)
INFO - root - 2017-12-11 04:52:03.117875: step 15890, loss = 0.70, batch loss = 0.64 (14.1 examples/sec; 0.566 sec/batch; 49h:47m:17s remains)
INFO - root - 2017-12-11 04:52:08.599059: step 15900, loss = 0.71, batch loss = 0.65 (14.9 examples/sec; 0.538 sec/batch; 47h:18m:44s remains)
2017-12-11 04:52:09.237654: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.065842211 -0.077108085 -0.080603093 -0.079046085 -0.076522812 -0.077487417 -0.081231475 -0.083280094 -0.078723729 -0.06665241 -0.052067567 -0.040050685 -0.035538051 -0.039948825 -0.052442651][-0.026374506 -0.039369293 -0.043290731 -0.040434472 -0.037224524 -0.040458187 -0.049293842 -0.055861276 -0.0515835 -0.033439722 -0.0093465094 0.011563787 0.020151634 0.013148494 -0.0080452319][0.031892218 0.023337258 0.024182316 0.032587592 0.038975779 0.033692621 0.017216595 0.0012558298 -0.00069933897 0.01915299 0.051096659 0.080241658 0.092168741 0.081460394 0.05013651][0.09194652 0.096110374 0.10953058 0.13006257 0.14424516 0.1397887 0.11660705 0.089347914 0.075823873 0.0904179 0.12339327 0.15497775 0.1648645 0.14648612 0.10381617][0.14082585 0.16321254 0.19487277 0.23289765 0.26049104 0.26350486 0.24023186 0.20510924 0.17790845 0.18039952 0.20514914 0.22901243 0.2277361 0.19602436 0.1412494][0.17361927 0.21092208 0.2584424 0.31356752 0.35832098 0.37756783 0.36561987 0.33172336 0.29455879 0.28274739 0.2930274 0.30085135 0.28085428 0.23151867 0.16465662][0.18820298 0.22994375 0.28347197 0.34986654 0.41201547 0.45359659 0.46300542 0.44015715 0.40060854 0.37750551 0.37367398 0.36444345 0.32504937 0.25823513 0.1801178][0.18585734 0.21794149 0.26453644 0.33245307 0.40673137 0.46957916 0.50289011 0.49651706 0.46258542 0.43586251 0.4243896 0.40472847 0.35340858 0.27546537 0.18950745][0.17899598 0.18986194 0.21760744 0.27546668 0.35094792 0.4255763 0.47647086 0.48650068 0.46530104 0.4460344 0.43816012 0.41885293 0.36649489 0.28651986 0.19734126][0.17694682 0.16247801 0.16482086 0.20291176 0.26628518 0.33774868 0.39301893 0.41351259 0.40768167 0.40381166 0.40917593 0.40043044 0.35862502 0.28724831 0.20148575][0.16776223 0.13267595 0.11054572 0.1251086 0.16723795 0.22288087 0.27027518 0.29392415 0.30077633 0.31118873 0.32935721 0.33423787 0.31039998 0.25675043 0.18255657][0.13233961 0.087779321 0.050684165 0.046308458 0.066604219 0.10131873 0.13406289 0.15455481 0.16719984 0.18342347 0.20535195 0.21828632 0.21179847 0.17950071 0.12386879][0.067702226 0.025425695 -0.013188628 -0.027307827 -0.02354173 -0.0076837596 0.010220982 0.024537761 0.036657013 0.051180393 0.068904653 0.082197286 0.085801713 0.070432886 0.034361497][-0.0095315631 -0.041667841 -0.071920879 -0.087109208 -0.092211515 -0.0890387 -0.082016058 -0.073671743 -0.064780943 -0.054449894 -0.042509831 -0.03240874 -0.026337715 -0.033098795 -0.054553472][-0.080254436 -0.10142272 -0.12012949 -0.13088769 -0.13740952 -0.13943668 -0.13811854 -0.13432002 -0.1294948 -0.12388453 -0.11702402 -0.11068923 -0.1058211 -0.10878362 -0.11984807]]...]
INFO - root - 2017-12-11 04:52:14.742603: step 15910, loss = 0.71, batch loss = 0.65 (13.7 examples/sec; 0.586 sec/batch; 51h:32m:14s remains)
INFO - root - 2017-12-11 04:52:20.221524: step 15920, loss = 0.69, batch loss = 0.63 (14.9 examples/sec; 0.537 sec/batch; 47h:13m:38s remains)
INFO - root - 2017-12-11 04:52:25.777454: step 15930, loss = 0.69, batch loss = 0.63 (14.9 examples/sec; 0.537 sec/batch; 47h:15m:25s remains)
INFO - root - 2017-12-11 04:52:31.249651: step 15940, loss = 0.71, batch loss = 0.65 (14.3 examples/sec; 0.559 sec/batch; 49h:07m:05s remains)
INFO - root - 2017-12-11 04:52:36.738402: step 15950, loss = 0.70, batch loss = 0.64 (14.2 examples/sec; 0.564 sec/batch; 49h:37m:31s remains)
INFO - root - 2017-12-11 04:52:42.247142: step 15960, loss = 0.71, batch loss = 0.65 (14.1 examples/sec; 0.566 sec/batch; 49h:44m:40s remains)
INFO - root - 2017-12-11 04:52:47.787832: step 15970, loss = 0.69, batch loss = 0.63 (14.7 examples/sec; 0.546 sec/batch; 47h:58m:34s remains)
INFO - root - 2017-12-11 04:52:53.190677: step 15980, loss = 0.69, batch loss = 0.64 (16.7 examples/sec; 0.479 sec/batch; 42h:04m:26s remains)
INFO - root - 2017-12-11 04:52:58.473173: step 15990, loss = 0.70, batch loss = 0.64 (14.4 examples/sec; 0.556 sec/batch; 48h:54m:10s remains)
INFO - root - 2017-12-11 04:53:03.929405: step 16000, loss = 0.70, batch loss = 0.64 (15.0 examples/sec; 0.535 sec/batch; 47h:02m:36s remains)
2017-12-11 04:53:04.559168: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.016965397 -0.030721692 -0.045260735 -0.055065203 -0.058128435 -0.05160464 -0.037688188 -0.024696665 -0.018133083 -0.018932234 -0.025279397 -0.037599325 -0.054097958 -0.068997569 -0.078904785][0.060817532 0.039801471 0.014115746 -0.0040053749 -0.011542916 -0.0025122543 0.018703446 0.0377658 0.045014367 0.040031347 0.026324689 0.00176642 -0.028771373 -0.055209104 -0.072431423][0.15756187 0.13189545 0.098631732 0.075210713 0.065757006 0.081793904 0.11421797 0.13892236 0.14299847 0.12859513 0.10167278 0.058951538 0.0088361977 -0.032694064 -0.059127476][0.25114396 0.22718404 0.19368024 0.17002232 0.16211323 0.1869489 0.22943556 0.2554929 0.25038698 0.2203652 0.1751076 0.11130117 0.041089304 -0.014839204 -0.049357761][0.33661541 0.31876063 0.29121748 0.273469 0.27164382 0.30572453 0.35492653 0.37770936 0.35826075 0.30676222 0.23901406 0.1534088 0.064844213 -0.00382605 -0.045646165][0.40629488 0.39779988 0.38208875 0.3760837 0.38498989 0.42952645 0.48495492 0.50473446 0.47083375 0.396792 0.30582795 0.19961338 0.093554765 0.010883301 -0.040877718][0.45437264 0.45653513 0.45228419 0.45662349 0.47492623 0.52718776 0.58577138 0.60248733 0.55736208 0.46590465 0.35746047 0.23686035 0.11848526 0.024577424 -0.036477219][0.47904953 0.48980671 0.49227366 0.50086552 0.52269632 0.57643455 0.63227731 0.64348072 0.59033984 0.48970118 0.37358057 0.24791013 0.12560692 0.027701279 -0.037524249][0.45903951 0.47397414 0.47797567 0.48557505 0.50579232 0.55516851 0.60435319 0.61127794 0.55829239 0.46170563 0.35165393 0.23283504 0.11650836 0.02255803 -0.041348848][0.37596938 0.3900249 0.39172718 0.39518422 0.41090155 0.45263764 0.49356315 0.49850345 0.45430624 0.37483808 0.28422606 0.18469822 0.085738912 0.0055965958 -0.049710855][0.24286212 0.25243184 0.2509622 0.24966502 0.25942639 0.29142851 0.32282656 0.32622921 0.29395762 0.23710978 0.17288294 0.10119266 0.029256327 -0.027909409 -0.066864192][0.093802385 0.096682973 0.091821931 0.085922815 0.089196257 0.1107766 0.13261744 0.13488203 0.11491781 0.081104875 0.044376824 0.0028506748 -0.038711026 -0.069476768 -0.088707492][-0.031335898 -0.035170462 -0.042134136 -0.050733715 -0.052500986 -0.040270235 -0.026473753 -0.024207834 -0.033036105 -0.047347613 -0.061198849 -0.077179343 -0.092930377 -0.1017307 -0.1047098][-0.11248714 -0.11971644 -0.12600155 -0.13373081 -0.13721308 -0.13170131 -0.12394834 -0.12141874 -0.12254952 -0.12424031 -0.12369634 -0.12306299 -0.12191009 -0.1170719 -0.11053841][-0.14783895 -0.15609306 -0.16046704 -0.16578378 -0.16912192 -0.16769785 -0.16407041 -0.16167235 -0.15920667 -0.15524125 -0.14844862 -0.14004362 -0.13068748 -0.11983441 -0.10948548]]...]
INFO - root - 2017-12-11 04:53:10.044720: step 16010, loss = 0.71, batch loss = 0.65 (14.9 examples/sec; 0.536 sec/batch; 47h:07m:23s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-SGD-lr0.001-clip50-initfromfixqian30k-fixqian
INFO - root - 2017-12-11 04:53:15.530510: step 16020, loss = 0.70, batch loss = 0.64 (14.9 examples/sec; 0.538 sec/batch; 47h:16m:34s remains)
INFO - root - 2017-12-11 04:53:21.102191: step 16030, loss = 0.69, batch loss = 0.63 (14.1 examples/sec; 0.568 sec/batch; 49h:58m:13s remains)
INFO - root - 2017-12-11 04:53:26.555255: step 16040, loss = 0.70, batch loss = 0.64 (14.7 examples/sec; 0.544 sec/batch; 47h:49m:39s remains)
INFO - root - 2017-12-11 04:53:32.008401: step 16050, loss = 0.69, batch loss = 0.63 (14.5 examples/sec; 0.550 sec/batch; 48h:21m:20s remains)
INFO - root - 2017-12-11 04:53:37.461035: step 16060, loss = 0.71, batch loss = 0.65 (14.1 examples/sec; 0.568 sec/batch; 49h:54m:30s remains)
INFO - root - 2017-12-11 04:53:42.992788: step 16070, loss = 0.71, batch loss = 0.65 (14.2 examples/sec; 0.562 sec/batch; 49h:22m:46s remains)
INFO - root - 2017-12-11 04:53:48.138444: step 16080, loss = 0.70, batch loss = 0.64 (14.5 examples/sec; 0.550 sec/batch; 48h:22m:12s remains)
INFO - root - 2017-12-11 04:53:53.654611: step 16090, loss = 0.71, batch loss = 0.65 (14.9 examples/sec; 0.538 sec/batch; 47h:18m:44s remains)
INFO - root - 2017-12-11 04:53:59.148877: step 16100, loss = 0.68, batch loss = 0.62 (14.6 examples/sec; 0.546 sec/batch; 48h:01m:23s remains)
2017-12-11 04:53:59.733353: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.029907743 -0.031954318 -0.035757843 -0.041396648 -0.046711888 -0.049895596 -0.050969265 -0.051311359 -0.050625157 -0.048519142 -0.045604084 -0.042848971 -0.041816898 -0.043414004 -0.047602136][-0.010229795 -0.00949234 -0.012454184 -0.019368934 -0.027281972 -0.033449396 -0.037286509 -0.040161643 -0.041113444 -0.039173156 -0.035168726 -0.030935908 -0.028877288 -0.030514471 -0.036134377][0.016402163 0.024978321 0.029029913 0.027257353 0.022740297 0.018201523 0.014273008 0.0093161473 0.0052264254 0.0041440944 0.0050678533 0.0059388364 0.0041430197 -0.0014969519 -0.011692128][0.046166331 0.067991905 0.086639151 0.099006534 0.10712667 0.11228006 0.11421646 0.11036335 0.10337569 0.09653008 0.088825934 0.078706026 0.064634338 0.047442522 0.026571717][0.074535616 0.11292019 0.15180579 0.18626246 0.21622816 0.23986124 0.25480035 0.25658619 0.24795222 0.23291107 0.21124299 0.18277155 0.14799368 0.1112214 0.073241323][0.094968796 0.14904805 0.20811754 0.26626843 0.3215518 0.36814824 0.40063748 0.41160011 0.4026562 0.37821755 0.33977693 0.28952253 0.23059902 0.17113003 0.11407453][0.10141072 0.16437446 0.23535563 0.30902272 0.3827756 0.44773448 0.49597871 0.51694745 0.5098806 0.47859389 0.4266175 0.35867721 0.28065392 0.20382468 0.1330661][0.094780937 0.15581077 0.22394143 0.29618135 0.37161687 0.44071972 0.49546427 0.52344477 0.52089775 0.4895317 0.43375087 0.3598572 0.27536446 0.1935702 0.1204704][0.086757466 0.13695505 0.18818396 0.24084072 0.2977027 0.35189071 0.39842561 0.4260008 0.42873612 0.40519312 0.35854343 0.29448411 0.2200121 0.14829601 0.085309744][0.086961813 0.1228929 0.15014555 0.17239125 0.19675352 0.22149858 0.24701743 0.26602992 0.27277502 0.26260987 0.23520158 0.19345893 0.14174759 0.090833791 0.045563992][0.0949759 0.11855933 0.12376112 0.11621386 0.10636462 0.09816587 0.09789823 0.10378321 0.11221658 0.11635286 0.11153138 0.09658955 0.071856327 0.044162065 0.016734812][0.09974248 0.11525716 0.10663339 0.079647869 0.045849241 0.013234347 -0.0076232357 -0.013837836 -0.0061468529 0.0078698276 0.020688498 0.027712872 0.025315687 0.016046826 0.0015078794][0.087100483 0.098598875 0.086184375 0.054249454 0.012992383 -0.028435405 -0.058341227 -0.071534559 -0.066350237 -0.049574282 -0.029294774 -0.01151468 -0.0018359431 -0.00089817907 -0.0083974414][0.054047789 0.06331116 0.054166336 0.028688267 -0.0059005455 -0.041850206 -0.0692244 -0.083039962 -0.080521643 -0.066750959 -0.048370767 -0.030695558 -0.01895564 -0.015171819 -0.019806895][0.010253224 0.016604083 0.012329023 -0.0028064081 -0.024638616 -0.047941256 -0.065994337 -0.075649157 -0.074532859 -0.065904595 -0.054149646 -0.042641118 -0.034801867 -0.032337468 -0.035872735]]...]
INFO - root - 2017-12-11 04:54:05.255784: step 16110, loss = 0.70, batch loss = 0.64 (14.5 examples/sec; 0.552 sec/batch; 48h:29m:19s remains)
INFO - root - 2017-12-11 04:54:10.767562: step 16120, loss = 0.69, batch loss = 0.63 (14.2 examples/sec; 0.562 sec/batch; 49h:22m:40s remains)
INFO - root - 2017-12-11 04:54:16.203636: step 16130, loss = 0.70, batch loss = 0.64 (14.6 examples/sec; 0.547 sec/batch; 48h:03m:33s remains)
INFO - root - 2017-12-11 04:54:21.729611: step 16140, loss = 0.71, batch loss = 0.65 (14.8 examples/sec; 0.542 sec/batch; 47h:36m:35s remains)
INFO - root - 2017-12-11 04:54:27.246553: step 16150, loss = 0.72, batch loss = 0.66 (14.9 examples/sec; 0.538 sec/batch; 47h:17m:37s remains)
INFO - root - 2017-12-11 04:54:32.736427: step 16160, loss = 0.69, batch loss = 0.63 (14.6 examples/sec; 0.547 sec/batch; 48h:04m:45s remains)
INFO - root - 2017-12-11 04:54:38.182367: step 16170, loss = 0.69, batch loss = 0.63 (14.7 examples/sec; 0.544 sec/batch; 47h:46m:25s remains)
INFO - root - 2017-12-11 04:54:43.416159: step 16180, loss = 0.68, batch loss = 0.62 (14.2 examples/sec; 0.562 sec/batch; 49h:23m:33s remains)
INFO - root - 2017-12-11 04:54:48.992306: step 16190, loss = 0.71, batch loss = 0.65 (14.5 examples/sec; 0.550 sec/batch; 48h:19m:25s remains)
INFO - root - 2017-12-11 04:54:54.430434: step 16200, loss = 0.68, batch loss = 0.63 (15.0 examples/sec; 0.532 sec/batch; 46h:42m:19s remains)
2017-12-11 04:54:55.003023: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.12898014 0.13909279 0.15331265 0.16757178 0.177692 0.18285456 0.18677223 0.19170475 0.19861917 0.20561515 0.20598508 0.1968552 0.18155296 0.16748255 0.1610848][0.1533217 0.16639842 0.18316562 0.19710617 0.2017713 0.19737898 0.19114055 0.18850529 0.19023927 0.19279225 0.1896348 0.17969309 0.16740432 0.15869187 0.15725133][0.1730689 0.18533079 0.20188375 0.21463026 0.21503013 0.20339006 0.18902928 0.17919554 0.17396015 0.16939221 0.16198072 0.1542186 0.14985409 0.15044449 0.15534407][0.18440329 0.19450346 0.20831318 0.21977003 0.22033748 0.20949557 0.19517134 0.18243104 0.1696337 0.15451999 0.13954 0.13248132 0.136439 0.14786695 0.16055034][0.1830415 0.1904183 0.20001771 0.21042366 0.21585537 0.21392734 0.20721568 0.19506963 0.17413916 0.14625047 0.12153439 0.11306032 0.12287786 0.14269936 0.16128005][0.18547674 0.18845731 0.1897534 0.1950081 0.20417754 0.21342316 0.21759616 0.20851733 0.18164615 0.14422257 0.11340072 0.1053442 0.11951973 0.14345783 0.16314942][0.1948285 0.19249699 0.18433122 0.18379061 0.19676735 0.21761318 0.23268509 0.22624631 0.19345824 0.14733064 0.11087747 0.10192038 0.11735482 0.14154601 0.15936366][0.20283914 0.19884522 0.18691634 0.18568064 0.20399559 0.23392089 0.25562266 0.2489814 0.21014351 0.15637161 0.11440955 0.10276052 0.1164654 0.13797584 0.152247][0.20263185 0.20128699 0.19323881 0.1978202 0.22254255 0.25674242 0.27830708 0.26769674 0.2236018 0.16528092 0.11995812 0.10527112 0.11532007 0.13246942 0.14292213][0.18743101 0.19034696 0.19031905 0.20373802 0.23379454 0.26715216 0.28259861 0.26494166 0.21672645 0.15737882 0.11173764 0.095075123 0.10116232 0.11376783 0.12080689][0.16156185 0.16665931 0.17161834 0.18881471 0.21738431 0.24352024 0.24953921 0.22600187 0.17879593 0.12555806 0.086324535 0.07268329 0.078492761 0.089635365 0.09611053][0.12723523 0.13187785 0.13747004 0.15275331 0.17474073 0.19145207 0.18974376 0.16472693 0.12378242 0.081401706 0.051983822 0.043300573 0.050131135 0.06158356 0.070276208][0.090498745 0.095051549 0.099835031 0.11021418 0.12312762 0.13039838 0.12376086 0.10140325 0.070422105 0.041312519 0.022995237 0.019737547 0.027747966 0.03975023 0.050940715][0.054809686 0.058805976 0.061669145 0.066278413 0.070842244 0.071223468 0.0631968 0.046633869 0.027113235 0.011264196 0.0034931842 0.0053061182 0.01430573 0.026219381 0.038202472][0.016701695 0.018970689 0.019697426 0.02025526 0.020167751 0.018045686 0.01195094 0.0022750564 -0.0074042277 -0.013411963 -0.014276447 -0.010154974 -0.0022373057 0.0074053267 0.017266845]]...]
INFO - root - 2017-12-11 04:55:00.476459: step 16210, loss = 0.71, batch loss = 0.65 (14.7 examples/sec; 0.546 sec/batch; 47h:57m:00s remains)
INFO - root - 2017-12-11 04:55:05.938380: step 16220, loss = 0.68, batch loss = 0.63 (14.6 examples/sec; 0.550 sec/batch; 48h:18m:04s remains)
INFO - root - 2017-12-11 04:55:11.456978: step 16230, loss = 0.69, batch loss = 0.64 (13.9 examples/sec; 0.577 sec/batch; 50h:42m:28s remains)
INFO - root - 2017-12-11 04:55:17.022936: step 16240, loss = 0.69, batch loss = 0.63 (14.3 examples/sec; 0.560 sec/batch; 49h:12m:11s remains)
INFO - root - 2017-12-11 04:55:22.577772: step 16250, loss = 0.70, batch loss = 0.64 (14.0 examples/sec; 0.571 sec/batch; 50h:10m:15s remains)
INFO - root - 2017-12-11 04:55:28.133000: step 16260, loss = 0.69, batch loss = 0.63 (14.3 examples/sec; 0.560 sec/batch; 49h:10m:39s remains)
INFO - root - 2017-12-11 04:55:33.593380: step 16270, loss = 0.69, batch loss = 0.63 (14.9 examples/sec; 0.536 sec/batch; 47h:05m:03s remains)
INFO - root - 2017-12-11 04:55:37.938171: step 16280, loss = 0.70, batch loss = 0.64 (14.1 examples/sec; 0.566 sec/batch; 49h:44m:21s remains)
INFO - root - 2017-12-11 04:55:43.501360: step 16290, loss = 0.70, batch loss = 0.65 (15.0 examples/sec; 0.535 sec/batch; 46h:58m:16s remains)
INFO - root - 2017-12-11 04:55:48.952809: step 16300, loss = 0.70, batch loss = 0.64 (14.5 examples/sec; 0.551 sec/batch; 48h:23m:12s remains)
2017-12-11 04:55:49.550302: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.05833469 0.088780984 0.12132464 0.15138777 0.17615739 0.20113477 0.23071951 0.2678467 0.30752292 0.33841437 0.34593457 0.31500232 0.24639577 0.16472971 0.10329261][0.057932772 0.083018012 0.11105256 0.13955669 0.16559853 0.19278866 0.22320427 0.25930074 0.29675913 0.3260839 0.3355397 0.31079048 0.24952966 0.17224793 0.10989328][0.062111624 0.08117304 0.10313712 0.12767071 0.15228194 0.17884801 0.20716636 0.23903279 0.27073225 0.29483637 0.30319518 0.28293711 0.22980222 0.15988474 0.10068101][0.0678861 0.081539005 0.098797768 0.12137637 0.14734009 0.17621522 0.20421647 0.23137885 0.25440809 0.26815575 0.26862642 0.24621385 0.19732055 0.13461967 0.081325628][0.074031338 0.083390489 0.099043444 0.12501946 0.15974839 0.19857413 0.23179415 0.25560218 0.26685914 0.26356488 0.24729787 0.21489638 0.16621783 0.11046989 0.065573551][0.07898131 0.085025623 0.1021581 0.13726215 0.18881306 0.24627748 0.2920962 0.31671828 0.31624523 0.29232123 0.25295758 0.20406309 0.15053394 0.098988786 0.061794821][0.080346078 0.083564535 0.10425729 0.15269528 0.22680473 0.30943558 0.37388384 0.40331522 0.3923946 0.346805 0.28167251 0.21277696 0.15130286 0.10199573 0.071483158][0.078723051 0.079943515 0.10443922 0.16539735 0.26007509 0.36572689 0.44772816 0.48290351 0.4642866 0.40101883 0.31419894 0.22796714 0.159324 0.11145165 0.086082645][0.078319214 0.078818873 0.10488661 0.17125401 0.27496716 0.39108655 0.48150182 0.52024782 0.49883893 0.427817 0.33060494 0.23568904 0.16372409 0.11710943 0.095293723][0.080904059 0.081559256 0.10540473 0.16683714 0.26348263 0.37263784 0.45852533 0.49673557 0.47772434 0.41096163 0.31795245 0.22626664 0.15705499 0.11287938 0.0940777][0.084116779 0.084998749 0.10253403 0.14873169 0.22301222 0.30919307 0.37916055 0.41389906 0.40360966 0.35436651 0.28125605 0.20533487 0.14510743 0.10461019 0.087999165][0.087732419 0.088956267 0.097857639 0.12283895 0.16579966 0.21951099 0.26675287 0.29619959 0.29901651 0.27637294 0.23376088 0.18211097 0.13548405 0.10016479 0.0855727][0.092112154 0.092941232 0.092150427 0.094573326 0.10469239 0.1242225 0.14739689 0.17107801 0.18811332 0.19456059 0.1853801 0.16089371 0.13011882 0.10166734 0.089888684][0.095197171 0.093802236 0.082806356 0.064982586 0.047250055 0.0383469 0.042178851 0.062140871 0.092366748 0.12439365 0.1445636 0.14476633 0.12930509 0.10829403 0.099677734][0.094531909 0.089286335 0.069090761 0.036308452 -0.00010236359 -0.027118888 -0.034375057 -0.014933444 0.025785524 0.0761286 0.11734734 0.13605019 0.13270429 0.11758024 0.1103226]]...]
INFO - root - 2017-12-11 04:55:55.060965: step 16310, loss = 0.70, batch loss = 0.64 (14.8 examples/sec; 0.539 sec/batch; 47h:19m:36s remains)
INFO - root - 2017-12-11 04:56:00.566071: step 16320, loss = 0.70, batch loss = 0.64 (14.5 examples/sec; 0.551 sec/batch; 48h:26m:02s remains)
INFO - root - 2017-12-11 04:56:06.146513: step 16330, loss = 0.69, batch loss = 0.63 (13.9 examples/sec; 0.574 sec/batch; 50h:25m:40s remains)
INFO - root - 2017-12-11 04:56:11.595431: step 16340, loss = 0.70, batch loss = 0.64 (14.8 examples/sec; 0.540 sec/batch; 47h:26m:15s remains)
INFO - root - 2017-12-11 04:56:17.113599: step 16350, loss = 0.71, batch loss = 0.65 (14.2 examples/sec; 0.562 sec/batch; 49h:22m:25s remains)
INFO - root - 2017-12-11 04:56:22.722973: step 16360, loss = 0.72, batch loss = 0.66 (14.3 examples/sec; 0.558 sec/batch; 49h:00m:10s remains)
INFO - root - 2017-12-11 04:56:28.215119: step 16370, loss = 0.70, batch loss = 0.64 (14.6 examples/sec; 0.547 sec/batch; 48h:01m:05s remains)
INFO - root - 2017-12-11 04:56:33.432433: step 16380, loss = 0.69, batch loss = 0.64 (13.5 examples/sec; 0.593 sec/batch; 52h:02m:13s remains)
INFO - root - 2017-12-11 04:56:38.944933: step 16390, loss = 0.68, batch loss = 0.63 (14.7 examples/sec; 0.546 sec/batch; 47h:56m:36s remains)
INFO - root - 2017-12-11 04:56:44.457967: step 16400, loss = 0.68, batch loss = 0.63 (14.4 examples/sec; 0.557 sec/batch; 48h:52m:38s remains)
2017-12-11 04:56:45.052358: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.0082789995 0.0043378463 0.0012415479 0.00017363697 0.00037412002 0.001049267 0.001881285 0.0027900783 0.0041695903 0.0055271522 0.0057963803 0.004952529 0.0038702679 0.0033476055 0.0037489794][0.011033019 0.007171276 0.0038120751 0.0024520508 0.0026648643 0.0036047574 0.0044322545 0.0050339806 0.0060248692 0.0072186207 0.0074948687 0.0067179813 0.00578416 0.0053408304 0.0057355193][0.013416232 0.011355172 0.009239044 0.0081967413 0.0082344972 0.0087965969 0.0088841291 0.0084761372 0.0085199056 0.009175295 0.0094811786 0.0090296436 0.0084293671 0.0079944795 0.0078766542][0.016241532 0.018164394 0.019399362 0.019952256 0.020239778 0.020129407 0.018697279 0.016443036 0.014686833 0.014315919 0.014599665 0.014542693 0.0142601 0.013462674 0.012198436][0.022302683 0.029932406 0.036158167 0.039613709 0.040994529 0.040267427 0.036647715 0.031605035 0.027231066 0.025478326 0.025763471 0.0260443 0.025842968 0.024285201 0.021424752][0.03220208 0.046182297 0.058159832 0.0655956 0.069040924 0.067961931 0.061703891 0.05310246 0.045473207 0.041938502 0.0419535 0.042162284 0.041515496 0.038901567 0.034420535][0.045729153 0.065218613 0.082104586 0.093121432 0.0982447 0.096229091 0.086804628 0.074538954 0.063862614 0.058736049 0.058356922 0.058212396 0.056892809 0.053458903 0.048031341][0.058240138 0.081130236 0.100929 0.11394311 0.11939117 0.11569665 0.10338888 0.088327684 0.075662553 0.069339074 0.068354346 0.067580573 0.065610141 0.062001687 0.056653865][0.061724823 0.08462894 0.10435558 0.11707021 0.12140913 0.1160126 0.10234819 0.086768359 0.073981479 0.067231223 0.065459311 0.063840479 0.061408438 0.058242563 0.054245312][0.053421095 0.0723403 0.088516109 0.098303489 0.1001826 0.093480162 0.0804072 0.066700839 0.05591144 0.049886543 0.047709655 0.045687821 0.043443896 0.041505434 0.039654009][0.035273943 0.047259808 0.05753427 0.063063763 0.06251137 0.055813704 0.045352962 0.035396356 0.027949164 0.02347008 0.021368565 0.019432263 0.017900383 0.017459251 0.017778425][0.013947457 0.018560406 0.022958089 0.024674188 0.022665873 0.017167218 0.010253622 0.00446309 0.00037410928 -0.002461263 -0.0041769156 -0.0055919988 -0.006225293 -0.0055382089 -0.0037956915][-0.0025364123 -0.0029505903 -0.0021509938 -0.0022596789 -0.0041310727 -0.007599256 -0.01126809 -0.013908247 -0.015815839 -0.017617078 -0.018995555 -0.019988757 -0.020129669 -0.019223336 -0.017400317][-0.0095669776 -0.011965482 -0.012219928 -0.012355782 -0.013178439 -0.014797974 -0.016424604 -0.01753526 -0.018688263 -0.020252472 -0.021584835 -0.022453962 -0.022683434 -0.022287922 -0.021294843][-0.0074679921 -0.0097735683 -0.0097423308 -0.0092503466 -0.0091289068 -0.0097647719 -0.010787051 -0.011882243 -0.013436324 -0.015360188 -0.016850453 -0.017717205 -0.018105794 -0.018178085 -0.018084196]]...]
INFO - root - 2017-12-11 04:56:50.551085: step 16410, loss = 0.69, batch loss = 0.64 (14.8 examples/sec; 0.542 sec/batch; 47h:36m:15s remains)
INFO - root - 2017-12-11 04:56:56.063934: step 16420, loss = 0.70, batch loss = 0.64 (14.2 examples/sec; 0.565 sec/batch; 49h:36m:41s remains)
INFO - root - 2017-12-11 04:57:01.616101: step 16430, loss = 0.70, batch loss = 0.64 (14.5 examples/sec; 0.550 sec/batch; 48h:17m:16s remains)
INFO - root - 2017-12-11 04:57:07.108691: step 16440, loss = 0.69, batch loss = 0.63 (14.9 examples/sec; 0.537 sec/batch; 47h:08m:35s remains)
INFO - root - 2017-12-11 04:57:12.560619: step 16450, loss = 0.71, batch loss = 0.65 (14.6 examples/sec; 0.547 sec/batch; 48h:03m:51s remains)
INFO - root - 2017-12-11 04:57:18.063639: step 16460, loss = 0.71, batch loss = 0.65 (14.1 examples/sec; 0.568 sec/batch; 49h:52m:33s remains)
INFO - root - 2017-12-11 04:57:23.442444: step 16470, loss = 0.69, batch loss = 0.63 (17.5 examples/sec; 0.456 sec/batch; 40h:01m:01s remains)
INFO - root - 2017-12-11 04:57:28.700647: step 16480, loss = 0.68, batch loss = 0.63 (14.7 examples/sec; 0.543 sec/batch; 47h:40m:27s remains)
INFO - root - 2017-12-11 04:57:34.163964: step 16490, loss = 0.70, batch loss = 0.64 (14.6 examples/sec; 0.549 sec/batch; 48h:13m:27s remains)
INFO - root - 2017-12-11 04:57:39.703903: step 16500, loss = 0.72, batch loss = 0.67 (14.6 examples/sec; 0.548 sec/batch; 48h:07m:10s remains)
2017-12-11 04:57:40.260003: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.12413607 0.12351995 0.10893621 0.098569736 0.091848619 0.092347018 0.097427636 0.098253116 0.092312515 0.079178318 0.072381079 0.079663843 0.095040977 0.10981587 0.12771788][0.25356749 0.25778523 0.23606989 0.21901378 0.20890708 0.20894808 0.21561094 0.21686658 0.2097881 0.19463764 0.18743688 0.19675876 0.21595429 0.23622988 0.26351294][0.37883413 0.38570356 0.35590023 0.33144623 0.319989 0.3237595 0.33650863 0.34250209 0.33776021 0.32225004 0.31293195 0.31917521 0.33530056 0.35480475 0.3882851][0.45749557 0.46442571 0.42856017 0.39974555 0.39263007 0.40820742 0.43525106 0.45288852 0.45509708 0.44036475 0.42602482 0.42373872 0.43153152 0.44684172 0.48413122][0.49411368 0.501259 0.46374398 0.435039 0.43706271 0.46927404 0.51365227 0.54382688 0.55187541 0.53507125 0.51110351 0.49523196 0.49147376 0.50208157 0.5423429][0.5124796 0.52487648 0.49086383 0.46539503 0.47646752 0.52023309 0.5725795 0.60569561 0.611825 0.58542705 0.54378051 0.50827026 0.49134672 0.49877483 0.54217583][0.50881493 0.534195 0.51273263 0.498537 0.52095926 0.57077688 0.6208576 0.647032 0.64266056 0.59738755 0.52790958 0.46548355 0.43289712 0.43647039 0.48042792][0.48593169 0.52973866 0.52942878 0.53375 0.56722254 0.61545086 0.65362793 0.66520643 0.64426172 0.5754441 0.47515208 0.38573423 0.33875695 0.33823469 0.37986058][0.47602189 0.54092568 0.56282586 0.58154315 0.6148473 0.64663005 0.65984827 0.64759916 0.60555828 0.51614845 0.3940376 0.28717563 0.23029664 0.22503245 0.26221949][0.47509566 0.55618817 0.59253931 0.61337334 0.63197184 0.63582718 0.61781609 0.57943594 0.51866812 0.41942874 0.29153967 0.17969865 0.11685945 0.10467783 0.13548741][0.45437428 0.54038686 0.57943362 0.59071267 0.58618003 0.56112933 0.51871783 0.46464431 0.39701948 0.30165291 0.18318433 0.077894568 0.014572099 -0.0030698243 0.022591157][0.39299682 0.46901873 0.49930015 0.49509916 0.46906331 0.42552003 0.3753885 0.32338363 0.26383224 0.18371828 0.084588394 -0.0054051592 -0.062062 -0.080016658 -0.057235554][0.30329162 0.35691431 0.36925232 0.34678754 0.30345613 0.25114641 0.20509362 0.16664648 0.12475246 0.066935614 -0.0064129033 -0.073656 -0.1154762 -0.126846 -0.10476663][0.19740376 0.223667 0.21587841 0.1782617 0.12548637 0.073351994 0.037537321 0.016213421 -0.00514505 -0.038218275 -0.083448753 -0.12519896 -0.14895436 -0.15109889 -0.12982342][0.078539379 0.080432758 0.059212849 0.017140726 -0.032320302 -0.074228458 -0.0961771 -0.10218155 -0.10639995 -0.11834598 -0.13853396 -0.15696709 -0.16406012 -0.15820985 -0.13921198]]...]
INFO - root - 2017-12-11 04:57:45.665573: step 16510, loss = 0.70, batch loss = 0.64 (14.6 examples/sec; 0.548 sec/batch; 48h:03m:27s remains)
INFO - root - 2017-12-11 04:57:51.211318: step 16520, loss = 0.72, batch loss = 0.66 (14.4 examples/sec; 0.557 sec/batch; 48h:55m:46s remains)
INFO - root - 2017-12-11 04:57:56.702407: step 16530, loss = 0.71, batch loss = 0.65 (14.0 examples/sec; 0.572 sec/batch; 50h:09m:59s remains)
INFO - root - 2017-12-11 04:58:02.164998: step 16540, loss = 0.71, batch loss = 0.65 (14.5 examples/sec; 0.551 sec/batch; 48h:23m:47s remains)
INFO - root - 2017-12-11 04:58:07.737432: step 16550, loss = 0.68, batch loss = 0.63 (14.2 examples/sec; 0.563 sec/batch; 49h:26m:37s remains)
INFO - root - 2017-12-11 04:58:13.260628: step 16560, loss = 0.70, batch loss = 0.65 (14.6 examples/sec; 0.547 sec/batch; 48h:02m:07s remains)
INFO - root - 2017-12-11 04:58:18.491616: step 16570, loss = 0.69, batch loss = 0.63 (14.5 examples/sec; 0.551 sec/batch; 48h:22m:42s remains)
INFO - root - 2017-12-11 04:58:24.076549: step 16580, loss = 0.69, batch loss = 0.63 (14.2 examples/sec; 0.564 sec/batch; 49h:27m:14s remains)
INFO - root - 2017-12-11 04:58:29.551903: step 16590, loss = 0.70, batch loss = 0.64 (14.7 examples/sec; 0.544 sec/batch; 47h:42m:48s remains)
INFO - root - 2017-12-11 04:58:34.991864: step 16600, loss = 0.69, batch loss = 0.63 (15.1 examples/sec; 0.529 sec/batch; 46h:24m:43s remains)
2017-12-11 04:58:35.573740: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.50101686 0.5634734 0.58615983 0.57546139 0.5217492 0.44912788 0.39325032 0.36210749 0.33781356 0.31274307 0.30391321 0.30086762 0.30091849 0.312123 0.32373792][0.47844243 0.550704 0.58667314 0.58699089 0.54309869 0.47523683 0.41585073 0.37274909 0.33347982 0.29723689 0.27628547 0.26280457 0.26328561 0.29165015 0.32797909][0.45159078 0.536361 0.58675945 0.60010427 0.57454371 0.52490914 0.47178933 0.42132863 0.36680228 0.31710476 0.2801401 0.25270393 0.25179431 0.30162933 0.37219021][0.44215745 0.53727317 0.5959717 0.61625832 0.60957205 0.58668882 0.55156791 0.50456011 0.44364786 0.38654593 0.337123 0.29464364 0.28738803 0.35208791 0.45111287][0.43807644 0.54121476 0.60561264 0.63308328 0.6510945 0.66802239 0.66906768 0.64143306 0.58279216 0.51757312 0.44695553 0.37347853 0.33742988 0.388921 0.48900831][0.45085776 0.56487054 0.63859105 0.67608732 0.71945804 0.7784121 0.82269406 0.819459 0.76205158 0.68271911 0.5812487 0.46245587 0.377709 0.39000788 0.46551052][0.4966687 0.62609524 0.71193594 0.75656831 0.8135342 0.899467 0.97635251 0.99035537 0.9290635 0.83290493 0.7016449 0.53881687 0.4007121 0.36041173 0.3948459][0.560472 0.7033239 0.79618412 0.83852363 0.88892269 0.97365981 1.059455 1.0813447 1.0209022 0.92070657 0.78000206 0.59754264 0.42486727 0.33725509 0.32508266][0.6195634 0.76983756 0.860089 0.88970917 0.91412491 0.96855742 1.0350574 1.0532781 1.0010334 0.91354436 0.78699237 0.6151216 0.43955547 0.32840121 0.28152165][0.661648 0.81057709 0.88872075 0.89782488 0.88539094 0.89322257 0.92266482 0.92908579 0.88900715 0.82416469 0.72548568 0.58379948 0.43024749 0.3208642 0.25913024][0.68142635 0.81800276 0.87441868 0.85757208 0.8065154 0.76641107 0.75534582 0.74658918 0.71782237 0.67739516 0.61317879 0.5141471 0.40069652 0.31413257 0.25720972][0.66256666 0.77268803 0.80011559 0.75837064 0.678724 0.60489762 0.56359673 0.54326385 0.52471244 0.50610006 0.47591913 0.42371809 0.35915115 0.30613375 0.26373971][0.5808664 0.65475774 0.65333909 0.59685075 0.50893396 0.42597517 0.37456393 0.35357967 0.34736142 0.34642512 0.34221694 0.32939705 0.31035224 0.29181513 0.2669833][0.43491235 0.47426268 0.45475674 0.39817038 0.32014981 0.24699834 0.2008177 0.18657051 0.19168349 0.20224339 0.21274537 0.22439164 0.23758568 0.24725923 0.2410951][0.25323337 0.26665351 0.24177708 0.19602691 0.13824204 0.085766256 0.054606531 0.050264988 0.062333696 0.076937251 0.091297485 0.11073957 0.136655 0.1601593 0.16707791]]...]
INFO - root - 2017-12-11 04:58:41.123217: step 16610, loss = 0.70, batch loss = 0.64 (14.5 examples/sec; 0.550 sec/batch; 48h:17m:26s remains)
INFO - root - 2017-12-11 04:58:46.599889: step 16620, loss = 0.70, batch loss = 0.64 (14.5 examples/sec; 0.551 sec/batch; 48h:21m:51s remains)
INFO - root - 2017-12-11 04:58:52.150330: step 16630, loss = 0.69, batch loss = 0.63 (14.4 examples/sec; 0.555 sec/batch; 48h:41m:33s remains)
INFO - root - 2017-12-11 04:58:57.658692: step 16640, loss = 0.71, batch loss = 0.65 (15.0 examples/sec; 0.534 sec/batch; 46h:49m:30s remains)
INFO - root - 2017-12-11 04:59:03.129507: step 16650, loss = 0.70, batch loss = 0.64 (15.1 examples/sec; 0.531 sec/batch; 46h:36m:50s remains)
INFO - root - 2017-12-11 04:59:08.590557: step 16660, loss = 0.69, batch loss = 0.63 (14.7 examples/sec; 0.545 sec/batch; 47h:49m:49s remains)
INFO - root - 2017-12-11 04:59:13.852021: step 16670, loss = 0.69, batch loss = 0.63 (14.4 examples/sec; 0.555 sec/batch; 48h:43m:19s remains)
INFO - root - 2017-12-11 04:59:19.387277: step 16680, loss = 0.69, batch loss = 0.63 (14.8 examples/sec; 0.541 sec/batch; 47h:28m:52s remains)
INFO - root - 2017-12-11 04:59:24.885478: step 16690, loss = 0.70, batch loss = 0.64 (14.1 examples/sec; 0.569 sec/batch; 49h:56m:38s remains)
INFO - root - 2017-12-11 04:59:30.431254: step 16700, loss = 0.70, batch loss = 0.64 (14.3 examples/sec; 0.561 sec/batch; 49h:10m:19s remains)
2017-12-11 04:59:31.012231: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.02727155 -0.009525504 0.0065102791 0.016467135 0.020514717 0.019899879 0.016543413 0.013070109 0.01274488 0.016797403 0.022226663 0.029585013 0.04431257 0.064196452 0.0857555][-0.020776894 0.00079127506 0.018513322 0.027000986 0.027308051 0.021557219 0.012618719 0.0044156765 0.00099949073 0.0039231349 0.0093196472 0.017440166 0.035104774 0.061168216 0.092468329][-0.020919038 0.0022015802 0.019707562 0.025852788 0.022281388 0.011679552 -0.0019783222 -0.013726784 -0.019630972 -0.018515715 -0.014513212 -0.0076362626 0.0095218727 0.037971284 0.076434329][-0.022356721 0.0019802304 0.020784121 0.02795078 0.024532361 0.012851369 -0.0011788076 -0.012273957 -0.01827503 -0.019398512 -0.018937912 -0.016473874 -0.0052592433 0.017781869 0.054969773][-0.018523162 0.010534411 0.037724018 0.056505986 0.06372074 0.05935644 0.053491458 0.049858987 0.045494452 0.038345933 0.029080411 0.020959757 0.018401103 0.0252577 0.048595659][-0.0073595853 0.031646077 0.077675477 0.12220556 0.15372065 0.16832747 0.18200101 0.19470996 0.19458143 0.17794517 0.15272379 0.12746689 0.10444709 0.086682595 0.085414574][0.0091716386 0.063336618 0.13864002 0.22174641 0.29057655 0.33700204 0.38038334 0.41398886 0.41635478 0.38425064 0.33541256 0.28725934 0.24237072 0.20067866 0.17332242][0.028238619 0.09963154 0.20836481 0.33399734 0.44500083 0.52874231 0.60290605 0.65328455 0.65044451 0.59495926 0.51610637 0.44394675 0.38312668 0.32719252 0.28168091][0.043369189 0.12695757 0.2598542 0.41482991 0.55485147 0.6629948 0.75283766 0.80648512 0.79158133 0.71450621 0.61425477 0.53219551 0.47325197 0.42193449 0.37342745][0.049208086 0.13460255 0.2713159 0.42874345 0.56948256 0.67452449 0.753965 0.79294592 0.76334363 0.67672122 0.57499808 0.50401282 0.46644548 0.43799686 0.40244722][0.039630968 0.11399639 0.23206472 0.36439431 0.47849771 0.55662489 0.60708725 0.62180269 0.58131862 0.49984494 0.4144173 0.36724472 0.35777673 0.35722467 0.3420414][0.0081423651 0.059520252 0.14258999 0.23346382 0.30810475 0.35230842 0.37307814 0.36806238 0.32599795 0.26047951 0.19936624 0.17574491 0.1868249 0.20526576 0.20697539][-0.037701711 -0.014038163 0.029824063 0.077892572 0.11519861 0.13171971 0.13256325 0.11813424 0.082801916 0.037266947 -0.00031933215 -0.0075891097 0.012177855 0.03745693 0.048723482][-0.077473409 -0.076440409 -0.063615918 -0.048073914 -0.037399188 -0.037800726 -0.046005 -0.061083585 -0.085381255 -0.1120098 -0.13109268 -0.12958427 -0.10994409 -0.086494893 -0.072347172][-0.097325958 -0.10784745 -0.11127358 -0.1125402 -0.11473524 -0.12119044 -0.13002436 -0.14085892 -0.15457517 -0.16751157 -0.17523679 -0.17127647 -0.15690738 -0.13982449 -0.12755881]]...]
INFO - root - 2017-12-11 04:59:36.644034: step 16710, loss = 0.70, batch loss = 0.64 (14.3 examples/sec; 0.558 sec/batch; 48h:58m:07s remains)
INFO - root - 2017-12-11 04:59:42.105367: step 16720, loss = 0.70, batch loss = 0.64 (14.8 examples/sec; 0.541 sec/batch; 47h:26m:09s remains)
INFO - root - 2017-12-11 04:59:47.552340: step 16730, loss = 0.68, batch loss = 0.62 (14.6 examples/sec; 0.549 sec/batch; 48h:11m:49s remains)
INFO - root - 2017-12-11 04:59:53.067518: step 16740, loss = 0.69, batch loss = 0.63 (14.3 examples/sec; 0.558 sec/batch; 48h:57m:00s remains)
INFO - root - 2017-12-11 04:59:58.614905: step 16750, loss = 0.69, batch loss = 0.63 (14.6 examples/sec; 0.546 sec/batch; 47h:55m:02s remains)
INFO - root - 2017-12-11 05:00:04.085455: step 16760, loss = 0.69, batch loss = 0.63 (15.0 examples/sec; 0.534 sec/batch; 46h:48m:36s remains)
INFO - root - 2017-12-11 05:00:09.323476: step 16770, loss = 0.69, batch loss = 0.64 (14.4 examples/sec; 0.555 sec/batch; 48h:41m:43s remains)
INFO - root - 2017-12-11 05:00:14.805497: step 16780, loss = 0.70, batch loss = 0.64 (14.8 examples/sec; 0.542 sec/batch; 47h:29m:34s remains)
INFO - root - 2017-12-11 05:00:20.240010: step 16790, loss = 0.71, batch loss = 0.66 (14.9 examples/sec; 0.537 sec/batch; 47h:06m:43s remains)
INFO - root - 2017-12-11 05:00:25.815248: step 16800, loss = 0.68, batch loss = 0.63 (14.2 examples/sec; 0.564 sec/batch; 49h:28m:31s remains)
2017-12-11 05:00:26.377119: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.014981378 -0.021350423 -0.019422337 -0.010649947 0.0024524047 0.017460881 0.032923661 0.04616395 0.053403232 0.052604344 0.044784617 0.033603329 0.022439931 0.013584366 0.0085889213][0.013297852 -6.3917163e-05 -0.00063687708 0.012801331 0.036400121 0.065966479 0.098432593 0.12799978 0.14661051 0.14912838 0.1359629 0.1133047 0.08942458 0.070573218 0.059772402][0.036848854 0.012826237 0.006211834 0.022479102 0.056663394 0.10225201 0.15386446 0.20200984 0.23357539 0.23978427 0.21991235 0.18334635 0.1448234 0.11622109 0.10233455][0.069402367 0.031518228 0.014477353 0.029677568 0.071588345 0.13078552 0.19870754 0.26345104 0.30836084 0.32101426 0.29836544 0.25115684 0.19978401 0.16307133 0.1492732][0.13347548 0.083495885 0.053391878 0.062706105 0.10659003 0.17303517 0.25020796 0.32575113 0.38221651 0.40441954 0.3858262 0.33543429 0.27665171 0.23417118 0.22088876][0.22940138 0.17076209 0.12449673 0.12175538 0.16048746 0.22696476 0.30591667 0.38525289 0.44876561 0.48029685 0.47024882 0.4236955 0.36377639 0.31880802 0.30564409][0.34893519 0.28643838 0.2215063 0.1986023 0.22121955 0.27584669 0.34478435 0.4166052 0.47917914 0.51770693 0.51953566 0.4836849 0.42928192 0.38504761 0.36997634][0.46055233 0.40087613 0.32077152 0.2746827 0.271385 0.30043662 0.3445248 0.39488164 0.44552255 0.48512185 0.49881056 0.47841236 0.435256 0.39490387 0.37712815][0.53516972 0.48747489 0.40124783 0.335669 0.30354738 0.298363 0.30731222 0.32587129 0.35531163 0.38930887 0.41220313 0.40864435 0.37995324 0.34597078 0.3258622][0.5586307 0.52821434 0.44459012 0.36578292 0.30740035 0.26763782 0.24014342 0.22486991 0.2288564 0.25066689 0.27542132 0.28343165 0.26790035 0.24186353 0.22203027][0.55229664 0.536958 0.45819506 0.36877626 0.28743213 0.21792905 0.16012453 0.11645603 0.09650071 0.10257025 0.12214966 0.1351437 0.13008249 0.11365648 0.097762026][0.54242522 0.5406518 0.46762097 0.36988294 0.26977435 0.17792563 0.10054004 0.040728364 0.0066375583 0.0015659257 0.015157815 0.02938392 0.031513721 0.023427026 0.011917267][0.54539144 0.55245584 0.48337919 0.3786459 0.26352772 0.15565924 0.068627656 0.0058372575 -0.02965497 -0.037288714 -0.026607789 -0.012432824 -0.00619072 -0.00871257 -0.016721161][0.58017945 0.58896244 0.51636 0.40067366 0.27042019 0.14966244 0.058706228 0.00056303409 -0.028610079 -0.033067919 -0.022846628 -0.0096768532 -0.0019070321 -0.00083978276 -0.0053029787][0.63128018 0.63844025 0.56014758 0.43410835 0.29188827 0.16255242 0.069926322 0.016159913 -0.0080576176 -0.01093863 -0.0029453125 0.0067902072 0.013235738 0.015807725 0.014504975]]...]
INFO - root - 2017-12-11 05:00:31.880769: step 16810, loss = 0.72, batch loss = 0.66 (14.0 examples/sec; 0.570 sec/batch; 50h:01m:25s remains)
INFO - root - 2017-12-11 05:00:37.378426: step 16820, loss = 0.70, batch loss = 0.64 (14.4 examples/sec; 0.554 sec/batch; 48h:36m:09s remains)
INFO - root - 2017-12-11 05:00:42.884727: step 16830, loss = 0.69, batch loss = 0.63 (14.4 examples/sec; 0.557 sec/batch; 48h:52m:24s remains)
INFO - root - 2017-12-11 05:00:48.288044: step 16840, loss = 0.71, batch loss = 0.65 (14.5 examples/sec; 0.552 sec/batch; 48h:23m:40s remains)
INFO - root - 2017-12-11 05:00:53.745480: step 16850, loss = 0.70, batch loss = 0.64 (13.9 examples/sec; 0.576 sec/batch; 50h:28m:19s remains)
INFO - root - 2017-12-11 05:00:59.003740: step 16860, loss = 0.69, batch loss = 0.64 (14.5 examples/sec; 0.552 sec/batch; 48h:23m:08s remains)
INFO - root - 2017-12-11 05:01:04.487972: step 16870, loss = 0.70, batch loss = 0.64 (15.1 examples/sec; 0.531 sec/batch; 46h:33m:04s remains)
INFO - root - 2017-12-11 05:01:09.960796: step 16880, loss = 0.72, batch loss = 0.66 (14.6 examples/sec; 0.547 sec/batch; 47h:55m:34s remains)
INFO - root - 2017-12-11 05:01:15.451319: step 16890, loss = 0.70, batch loss = 0.65 (14.8 examples/sec; 0.540 sec/batch; 47h:19m:14s remains)
INFO - root - 2017-12-11 05:01:20.968050: step 16900, loss = 0.71, batch loss = 0.66 (14.6 examples/sec; 0.547 sec/batch; 47h:58m:41s remains)
2017-12-11 05:01:21.596512: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.029162096 -0.017167598 -0.010461892 -0.01108782 -0.017126611 -0.027113149 -0.039774403 -0.051768679 -0.061394777 -0.066750281 -0.068867475 -0.068428054 -0.067721426 -0.068509862 -0.0699457][0.012350325 0.033363815 0.043939188 0.043963879 0.038042977 0.028185492 0.015234205 0.003618201 -0.0061528753 -0.010888875 -0.011510201 -0.0096489107 -0.0096996855 -0.013012951 -0.015325025][0.081112333 0.11958189 0.13801557 0.13844566 0.129098 0.11360995 0.094661213 0.080957048 0.07202442 0.06995897 0.074685015 0.083690681 0.08762338 0.083456635 0.0815349][0.17849633 0.24432582 0.27613792 0.27633557 0.2586098 0.23028196 0.1987025 0.17939726 0.17167398 0.17511673 0.18849275 0.20689271 0.21379921 0.20449653 0.19713154][0.28975484 0.38801295 0.43540621 0.43366256 0.4046813 0.36153382 0.31597805 0.29014912 0.28425881 0.29547712 0.31817672 0.34384078 0.34922725 0.32781327 0.30553296][0.3783268 0.50420958 0.5650956 0.56189686 0.52498585 0.47338521 0.42093772 0.39364761 0.39425349 0.41773975 0.44990045 0.47719783 0.47435939 0.435873 0.39336371][0.43058497 0.57306141 0.64317775 0.64111531 0.60285467 0.55151397 0.5002389 0.47566059 0.48515609 0.52204031 0.56169987 0.58577394 0.57261431 0.51978904 0.45997274][0.45872226 0.61041129 0.68832403 0.69070905 0.65675837 0.61213017 0.56729007 0.54647815 0.56219995 0.60852933 0.6526736 0.67201895 0.6498217 0.58796388 0.51766419][0.45371085 0.60565841 0.68833828 0.69783986 0.673762 0.64188832 0.60951436 0.59517747 0.61351252 0.66170126 0.70444584 0.71716958 0.68830067 0.62572867 0.55780947][0.42529622 0.56616324 0.64533079 0.65869659 0.64374489 0.62518656 0.60795242 0.60306168 0.62311667 0.66807032 0.70559573 0.71169412 0.68030775 0.62580389 0.57330877][0.40512303 0.5249446 0.58906084 0.59676749 0.58250487 0.5695706 0.56285721 0.56672984 0.58989334 0.63211745 0.66595316 0.66968668 0.64183134 0.60142559 0.56985933][0.40577102 0.50020742 0.54154634 0.53472072 0.5115633 0.4942151 0.4889327 0.49637008 0.52201319 0.56281376 0.59594345 0.60312891 0.585103 0.56147951 0.54979676][0.41078737 0.4802345 0.49765772 0.47287 0.43565249 0.40772513 0.39812428 0.40641922 0.43517348 0.47621205 0.50983483 0.52229446 0.5149473 0.50582612 0.51084757][0.39509392 0.43971884 0.43485597 0.39401916 0.34436402 0.30634969 0.29132396 0.29897431 0.32932776 0.36960694 0.40306452 0.42105502 0.4252066 0.43079245 0.4522512][0.36308107 0.3859382 0.36218184 0.30830893 0.24908015 0.2035106 0.18384871 0.19015661 0.22102652 0.26148155 0.29697505 0.32241735 0.33928335 0.35962731 0.39482844]]...]
INFO - root - 2017-12-11 05:01:27.052100: step 16910, loss = 0.67, batch loss = 0.61 (14.6 examples/sec; 0.549 sec/batch; 48h:09m:35s remains)
INFO - root - 2017-12-11 05:01:32.496995: step 16920, loss = 0.71, batch loss = 0.65 (14.8 examples/sec; 0.541 sec/batch; 47h:22m:51s remains)
INFO - root - 2017-12-11 05:01:38.073276: step 16930, loss = 0.69, batch loss = 0.64 (14.5 examples/sec; 0.551 sec/batch; 48h:15m:26s remains)
INFO - root - 2017-12-11 05:01:43.668727: step 16940, loss = 0.71, batch loss = 0.65 (14.7 examples/sec; 0.543 sec/batch; 47h:37m:01s remains)
INFO - root - 2017-12-11 05:01:49.168702: step 16950, loss = 0.70, batch loss = 0.65 (14.9 examples/sec; 0.538 sec/batch; 47h:08m:42s remains)
INFO - root - 2017-12-11 05:01:54.226764: step 16960, loss = 0.71, batch loss = 0.65 (14.0 examples/sec; 0.571 sec/batch; 50h:01m:52s remains)
INFO - root - 2017-12-11 05:01:59.756895: step 16970, loss = 0.68, batch loss = 0.63 (14.4 examples/sec; 0.555 sec/batch; 48h:39m:30s remains)
INFO - root - 2017-12-11 05:02:05.300765: step 16980, loss = 0.68, batch loss = 0.62 (14.5 examples/sec; 0.553 sec/batch; 48h:29m:12s remains)
INFO - root - 2017-12-11 05:02:10.800800: step 16990, loss = 0.71, batch loss = 0.65 (14.8 examples/sec; 0.542 sec/batch; 47h:29m:29s remains)
INFO - root - 2017-12-11 05:02:16.335611: step 17000, loss = 0.70, batch loss = 0.64 (14.6 examples/sec; 0.549 sec/batch; 48h:05m:52s remains)
2017-12-11 05:02:16.965589: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.10798357 0.095498413 0.084304936 0.079302885 0.080032609 0.090502292 0.10625246 0.11691254 0.12817585 0.14011283 0.14738202 0.14472635 0.13373047 0.11871068 0.093702488][0.10089556 0.090606742 0.086565986 0.089816347 0.093253314 0.09880881 0.10541814 0.10698443 0.10832833 0.11216113 0.11672818 0.11583794 0.10825464 0.0956772 0.07411579][0.11772987 0.11107966 0.11366268 0.12303842 0.12601405 0.12345344 0.11859842 0.11005022 0.10137792 0.097062416 0.098936893 0.099781387 0.095001891 0.083671525 0.062938482][0.16051555 0.15718345 0.16344707 0.17523266 0.1751724 0.16428275 0.15044303 0.13615528 0.12208284 0.11234853 0.11197393 0.1132449 0.10903055 0.0957493 0.071529604][0.20773154 0.20880052 0.21897836 0.23365258 0.23253775 0.21738307 0.19999997 0.18532535 0.1696505 0.15538636 0.1507531 0.14904159 0.14160585 0.12308216 0.092351004][0.23899427 0.24526578 0.26115069 0.28073451 0.28231123 0.26850733 0.25321764 0.2419191 0.22653508 0.20796196 0.19698738 0.1883366 0.17338148 0.14681293 0.10866297][0.23592076 0.24820386 0.27095464 0.29629925 0.30322433 0.2952489 0.28622255 0.28021404 0.26650527 0.24508791 0.22767128 0.21050984 0.18660322 0.15261723 0.10963122][0.19547765 0.21378297 0.24317354 0.27336317 0.28613007 0.28537405 0.28306717 0.28135595 0.26954365 0.24747749 0.22564487 0.20157705 0.17101638 0.13338098 0.090196617][0.12890676 0.15066561 0.18346605 0.21428636 0.22937576 0.23310173 0.23475899 0.23475015 0.22433323 0.20447434 0.18297079 0.15776896 0.12689875 0.091779836 0.054049537][0.055416651 0.075904034 0.10612062 0.13175778 0.14442059 0.1490543 0.15181635 0.15164948 0.14289768 0.12827496 0.11211826 0.091746591 0.066888645 0.039964341 0.012209456][-0.005681376 0.0089020943 0.031054994 0.047176417 0.054135289 0.056981713 0.058888774 0.057792712 0.051213071 0.043497987 0.03571441 0.024282213 0.0099433567 -0.0050108265 -0.02048975][-0.040076073 -0.032834526 -0.021107191 -0.015712338 -0.015824851 -0.016738689 -0.017387465 -0.019713845 -0.023573248 -0.024210142 -0.023238763 -0.025040355 -0.028446468 -0.031992041 -0.036956191][-0.047541589 -0.046405476 -0.043076847 -0.045498077 -0.050894618 -0.055168636 -0.05815478 -0.061072025 -0.062432826 -0.0582669 -0.051716637 -0.047455233 -0.044168562 -0.041089419 -0.040357169][-0.039890114 -0.04254929 -0.044192109 -0.050321456 -0.058029734 -0.06396915 -0.067958958 -0.070423387 -0.069750682 -0.063448176 -0.055126887 -0.049244761 -0.044264052 -0.039596733 -0.037588738][-0.029231725 -0.033699274 -0.037506875 -0.044279862 -0.051759329 -0.057512004 -0.0612307 -0.062772058 -0.060929906 -0.054622948 -0.047233675 -0.042513911 -0.038713228 -0.035227161 -0.034272272]]...]
INFO - root - 2017-12-11 05:02:22.575845: step 17010, loss = 0.70, batch loss = 0.64 (13.9 examples/sec; 0.577 sec/batch; 50h:33m:17s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-SGD-lr0.001-clip50-initfromfixqian30k-fixqian
INFO - root - 2017-12-11 05:02:28.062411: step 17020, loss = 0.70, batch loss = 0.65 (14.8 examples/sec; 0.540 sec/batch; 47h:20m:30s remains)
INFO - root - 2017-12-11 05:02:33.533438: step 17030, loss = 0.70, batch loss = 0.64 (14.0 examples/sec; 0.573 sec/batch; 50h:10m:12s remains)
INFO - root - 2017-12-11 05:02:39.145388: step 17040, loss = 0.68, batch loss = 0.62 (14.1 examples/sec; 0.567 sec/batch; 49h:41m:38s remains)
INFO - root - 2017-12-11 05:02:44.674187: step 17050, loss = 0.68, batch loss = 0.62 (14.9 examples/sec; 0.536 sec/batch; 46h:56m:55s remains)
INFO - root - 2017-12-11 05:02:49.921965: step 17060, loss = 0.70, batch loss = 0.64 (14.6 examples/sec; 0.550 sec/batch; 48h:09m:11s remains)
INFO - root - 2017-12-11 05:02:55.410810: step 17070, loss = 0.70, batch loss = 0.64 (15.0 examples/sec; 0.535 sec/batch; 46h:52m:18s remains)
INFO - root - 2017-12-11 05:03:00.829978: step 17080, loss = 0.71, batch loss = 0.65 (15.0 examples/sec; 0.534 sec/batch; 46h:46m:38s remains)
INFO - root - 2017-12-11 05:03:06.358287: step 17090, loss = 0.71, batch loss = 0.65 (13.6 examples/sec; 0.587 sec/batch; 51h:23m:14s remains)
INFO - root - 2017-12-11 05:03:11.908141: step 17100, loss = 0.69, batch loss = 0.64 (14.1 examples/sec; 0.566 sec/batch; 49h:35m:05s remains)
2017-12-11 05:03:12.501934: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.047312744 -0.021356154 0.011617399 0.034731776 0.040009052 0.024876419 -0.0063238386 -0.042099815 -0.070447363 -0.082557477 -0.0774962 -0.063277066 -0.048573464 -0.039195552 -0.035881784][-0.0488762 -0.0064992891 0.04967669 0.09521094 0.11501653 0.10214869 0.059952624 0.0036940996 -0.048781928 -0.080314256 -0.084405445 -0.069818914 -0.04989941 -0.035662286 -0.029244009][-0.042507093 0.018913949 0.10298695 0.17783989 0.22024775 0.2176666 0.16990969 0.093721949 0.013620888 -0.043099009 -0.062072281 -0.051519215 -0.029355016 -0.012662415 -0.0051079411][-0.026170557 0.05000544 0.15890068 0.26490566 0.33667409 0.35385492 0.30906704 0.21738467 0.11112534 0.029721331 -0.0031471911 0.0045357975 0.029341828 0.047545936 0.05477272][0.0049049645 0.084898166 0.20535655 0.33476484 0.43646955 0.48010287 0.44774649 0.34933698 0.22462641 0.12592435 0.086706012 0.098659761 0.13101056 0.15288292 0.15943931][0.059150614 0.13208917 0.24807377 0.38501674 0.50747484 0.577393 0.56443286 0.47022456 0.33849654 0.23195119 0.19201998 0.21180005 0.25431532 0.28198409 0.28686818][0.14031081 0.19927236 0.29571187 0.41964504 0.54367912 0.62929076 0.63666505 0.55793065 0.43392861 0.32998109 0.2913855 0.31607571 0.36692375 0.401787 0.40459377][0.22773153 0.27057359 0.33922082 0.43421727 0.53890473 0.62212235 0.64337564 0.58761156 0.48507455 0.39306644 0.35634372 0.38084441 0.43588734 0.47890964 0.48284653][0.29873928 0.32701254 0.36760345 0.4271504 0.49743143 0.55938107 0.58224273 0.55015284 0.47884685 0.40932643 0.38057405 0.4055512 0.46297452 0.51345336 0.52241147][0.34737521 0.36415556 0.38152951 0.4086135 0.43990645 0.46877819 0.47992897 0.46379176 0.42512408 0.3859092 0.37354445 0.40262702 0.46028742 0.51308173 0.52663088][0.37957004 0.39023718 0.39342752 0.39846784 0.39819375 0.39399514 0.38513431 0.37163603 0.35539016 0.34463403 0.35200831 0.38715336 0.4407832 0.48614916 0.49680319][0.40254223 0.40923339 0.40219444 0.39332071 0.37392458 0.34745619 0.32107159 0.30208409 0.2956202 0.30379662 0.32745951 0.36761659 0.41323927 0.44368356 0.44438732][0.40120226 0.40335348 0.3887507 0.37195554 0.34484169 0.30886567 0.272577 0.2466736 0.24070813 0.25593507 0.28762123 0.32954529 0.367222 0.38475779 0.37732252][0.35575172 0.35492817 0.33642894 0.31641811 0.28866902 0.252159 0.21327594 0.18336064 0.1750865 0.19078209 0.22436665 0.26542956 0.29774007 0.30937064 0.30046058][0.26433408 0.26579747 0.24993931 0.2315667 0.20662002 0.17433649 0.13824973 0.10868729 0.099312462 0.1130743 0.14405842 0.18020523 0.20606709 0.21351612 0.20473459]]...]
INFO - root - 2017-12-11 05:03:18.066068: step 17110, loss = 0.68, batch loss = 0.62 (14.6 examples/sec; 0.547 sec/batch; 47h:53m:13s remains)
INFO - root - 2017-12-11 05:03:23.530724: step 17120, loss = 0.70, batch loss = 0.64 (14.3 examples/sec; 0.560 sec/batch; 49h:05m:59s remains)
INFO - root - 2017-12-11 05:03:29.014672: step 17130, loss = 0.70, batch loss = 0.64 (14.1 examples/sec; 0.569 sec/batch; 49h:50m:31s remains)
INFO - root - 2017-12-11 05:03:34.561794: step 17140, loss = 0.71, batch loss = 0.65 (14.5 examples/sec; 0.553 sec/batch; 48h:27m:20s remains)
INFO - root - 2017-12-11 05:03:39.712253: step 17150, loss = 0.70, batch loss = 0.64 (29.1 examples/sec; 0.275 sec/batch; 24h:03m:21s remains)
INFO - root - 2017-12-11 05:03:45.278629: step 17160, loss = 0.71, batch loss = 0.65 (14.3 examples/sec; 0.558 sec/batch; 48h:51m:33s remains)
INFO - root - 2017-12-11 05:03:50.758282: step 17170, loss = 0.69, batch loss = 0.63 (15.0 examples/sec; 0.532 sec/batch; 46h:36m:39s remains)
INFO - root - 2017-12-11 05:03:56.289353: step 17180, loss = 0.68, batch loss = 0.62 (14.6 examples/sec; 0.549 sec/batch; 48h:03m:21s remains)
INFO - root - 2017-12-11 05:04:01.794972: step 17190, loss = 0.71, batch loss = 0.65 (14.9 examples/sec; 0.538 sec/batch; 47h:07m:27s remains)
INFO - root - 2017-12-11 05:04:07.343097: step 17200, loss = 0.71, batch loss = 0.65 (14.3 examples/sec; 0.559 sec/batch; 48h:57m:22s remains)
2017-12-11 05:04:07.921059: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.060138494 0.061950721 0.060507491 0.055648446 0.049982909 0.047945634 0.047639728 0.043871142 0.038970485 0.037519824 0.038943782 0.04433284 0.05898644 0.080797352 0.10355214][0.068408169 0.0754381 0.079965852 0.080107152 0.077248983 0.074613929 0.0706219 0.061078817 0.049543262 0.042352404 0.039862771 0.043226331 0.056846317 0.077778541 0.10076503][0.093568437 0.1075517 0.1201349 0.12764214 0.13004549 0.12798175 0.11973336 0.10348973 0.084150657 0.069401048 0.060037598 0.057400361 0.064663321 0.078284658 0.096125275][0.14158292 0.16244233 0.18159255 0.19498289 0.202098 0.2008625 0.18857743 0.16681804 0.14160438 0.12010554 0.10256247 0.090888858 0.087160245 0.087688312 0.095555149][0.2005349 0.22776939 0.25141358 0.26868141 0.27920932 0.2787452 0.26345748 0.23829496 0.21034637 0.18444669 0.1595456 0.13807321 0.12179541 0.10778923 0.10470435][0.24563922 0.2781426 0.30590346 0.32718885 0.34145454 0.34337217 0.32797921 0.3030161 0.27608812 0.24942786 0.21951246 0.18966997 0.1625182 0.13646288 0.12423715][0.25715694 0.29168135 0.32127029 0.34513023 0.36268806 0.36899948 0.35771289 0.33813673 0.31727362 0.29486811 0.26462159 0.23028046 0.19602226 0.16195756 0.14310087][0.23161916 0.26275995 0.28952926 0.31177887 0.32982796 0.34010291 0.33523104 0.32448167 0.31300861 0.29828152 0.27191642 0.23785442 0.20158836 0.16446026 0.14159699][0.17665623 0.20170408 0.22221705 0.23895724 0.25390875 0.26556271 0.26665974 0.26507246 0.26353648 0.2577059 0.23813464 0.20855211 0.17553346 0.14105879 0.11828044][0.10608524 0.12461624 0.1396841 0.15119767 0.16164933 0.17161641 0.17546867 0.1794675 0.18417673 0.18491805 0.1726 0.15047479 0.12570924 0.10014666 0.082801275][0.0399896 0.052354235 0.063525051 0.07148698 0.077872485 0.084339604 0.088093355 0.093923867 0.10095409 0.10551751 0.10018107 0.087279193 0.073513746 0.059379898 0.049252328][0.0019266201 0.00857003 0.015733205 0.019645218 0.021215066 0.022443384 0.023620965 0.028309129 0.034965828 0.041294873 0.041875388 0.037843812 0.034141481 0.0297772 0.025375908][-0.0018672023 -0.00026699758 0.0027113974 0.001868101 -0.0022018936 -0.0070534269 -0.0098280935 -0.0075427932 -0.0017065059 0.0063033947 0.011934595 0.014826466 0.017971609 0.019731514 0.018250694][0.016336096 0.015082403 0.014763284 0.010291512 0.0015525194 -0.0085284011 -0.015421264 -0.015856273 -0.010222505 0.00068574765 0.012094397 0.021332609 0.029764507 0.035748806 0.0359316][0.042096131 0.038690433 0.035947382 0.02976531 0.018686868 0.0053772815 -0.0046956972 -0.0075121233 -0.0018808338 0.011821753 0.028408716 0.042925373 0.055195294 0.063747205 0.064673528]]...]
INFO - root - 2017-12-11 05:04:13.432190: step 17210, loss = 0.69, batch loss = 0.64 (14.8 examples/sec; 0.541 sec/batch; 47h:22m:10s remains)
INFO - root - 2017-12-11 05:04:18.930129: step 17220, loss = 0.70, batch loss = 0.64 (14.7 examples/sec; 0.542 sec/batch; 47h:30m:27s remains)
INFO - root - 2017-12-11 05:04:24.381773: step 17230, loss = 0.69, batch loss = 0.63 (14.8 examples/sec; 0.542 sec/batch; 47h:26m:49s remains)
INFO - root - 2017-12-11 05:04:29.843429: step 17240, loss = 0.69, batch loss = 0.63 (14.7 examples/sec; 0.545 sec/batch; 47h:42m:24s remains)
INFO - root - 2017-12-11 05:04:35.023337: step 17250, loss = 0.72, batch loss = 0.66 (14.3 examples/sec; 0.558 sec/batch; 48h:49m:21s remains)
INFO - root - 2017-12-11 05:04:40.518134: step 17260, loss = 0.69, batch loss = 0.63 (14.4 examples/sec; 0.555 sec/batch; 48h:38m:30s remains)
INFO - root - 2017-12-11 05:04:45.935413: step 17270, loss = 0.70, batch loss = 0.64 (14.5 examples/sec; 0.551 sec/batch; 48h:12m:21s remains)
INFO - root - 2017-12-11 05:04:51.394915: step 17280, loss = 0.69, batch loss = 0.64 (14.6 examples/sec; 0.549 sec/batch; 48h:06m:11s remains)
INFO - root - 2017-12-11 05:04:56.921355: step 17290, loss = 0.70, batch loss = 0.64 (14.0 examples/sec; 0.573 sec/batch; 50h:09m:40s remains)
INFO - root - 2017-12-11 05:05:02.416608: step 17300, loss = 0.69, batch loss = 0.63 (14.6 examples/sec; 0.546 sec/batch; 47h:50m:16s remains)
2017-12-11 05:05:03.011337: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.07211262 -0.0734719 -0.072191022 -0.070794724 -0.0686129 -0.065475553 -0.060243562 -0.054315452 -0.050068151 -0.049970552 -0.055279948 -0.065101475 -0.078456447 -0.091541037 -0.10026643][-0.062327906 -0.0639885 -0.062476519 -0.059507784 -0.053756077 -0.044444337 -0.02928808 -0.012172725 5.9381488e-05 0.00245394 -0.0067194472 -0.025902173 -0.053579152 -0.082824819 -0.10425184][-0.032741103 -0.030814191 -0.024702298 -0.015515729 -0.0015495328 0.018564532 0.048750244 0.080727972 0.10155683 0.10328441 0.0845319 0.04902475 -0.0012574411 -0.05443063 -0.094502874][0.016981434 0.029733693 0.04834415 0.071182981 0.099357612 0.13460964 0.18247817 0.2294794 0.25581267 0.25068757 0.2147225 0.15498231 0.074010253 -0.010290557 -0.07429114][0.091952652 0.12292073 0.16128132 0.20358762 0.2496341 0.30176291 0.36679146 0.42660779 0.454784 0.43760759 0.37886459 0.28949597 0.17295131 0.052799843 -0.039560996][0.19515811 0.25098294 0.31423244 0.37882376 0.4423888 0.50804567 0.58425611 0.65050417 0.6753965 0.64244056 0.55864155 0.43887192 0.28713512 0.13111866 0.0091255419][0.29227006 0.3759236 0.46660545 0.55393279 0.63188845 0.7043854 0.78229314 0.84648138 0.86381841 0.81580907 0.71051836 0.5658322 0.38648811 0.2020787 0.055454593][0.34871706 0.45354652 0.56421119 0.66585183 0.74838543 0.81686711 0.88555151 0.93983895 0.94951993 0.89395446 0.77960968 0.62495005 0.43501547 0.23927654 0.081255235][0.35262018 0.46404222 0.57889551 0.67976367 0.75408441 0.80816454 0.85880595 0.89794558 0.90134072 0.84736514 0.73932987 0.59320313 0.41416347 0.22836493 0.076465577][0.30402222 0.40433809 0.50450373 0.58780193 0.64215374 0.67502815 0.7038843 0.72706032 0.72713614 0.68325675 0.59478492 0.47319296 0.32346594 0.16702311 0.038405519][0.21189581 0.28759605 0.35961789 0.41426474 0.44246942 0.45253798 0.46118996 0.47095624 0.47064742 0.44138148 0.37911165 0.29079452 0.18099616 0.066377819 -0.026403962][0.0946099 0.14053528 0.18139344 0.20671207 0.21136877 0.20381682 0.19827987 0.1986388 0.19845451 0.18230867 0.1450334 0.090211473 0.021884382 -0.047617014 -0.099927537][-0.006712147 0.012892839 0.027983567 0.031109026 0.020378917 0.00335252 -0.0096671758 -0.015208003 -0.016567439 -0.024690595 -0.043829568 -0.072229251 -0.1064882 -0.13803388 -0.15580665][-0.069399826 -0.067746527 -0.068935961 -0.078292087 -0.095448159 -0.11446223 -0.12876511 -0.13620536 -0.13881147 -0.14337452 -0.15238857 -0.16458134 -0.17694618 -0.18395606 -0.17989452][-0.10092845 -0.10850251 -0.11643545 -0.12887296 -0.14438662 -0.15898776 -0.16914932 -0.17419159 -0.17600653 -0.17869395 -0.18315901 -0.18827324 -0.19086368 -0.18718739 -0.17471386]]...]
INFO - root - 2017-12-11 05:05:08.558625: step 17310, loss = 0.70, batch loss = 0.65 (14.7 examples/sec; 0.543 sec/batch; 47h:32m:25s remains)
INFO - root - 2017-12-11 05:05:14.066651: step 17320, loss = 0.69, batch loss = 0.63 (14.3 examples/sec; 0.561 sec/batch; 49h:07m:33s remains)
INFO - root - 2017-12-11 05:05:19.637066: step 17330, loss = 0.67, batch loss = 0.61 (14.7 examples/sec; 0.543 sec/batch; 47h:33m:27s remains)
INFO - root - 2017-12-11 05:05:25.185578: step 17340, loss = 0.68, batch loss = 0.62 (14.7 examples/sec; 0.545 sec/batch; 47h:41m:37s remains)
INFO - root - 2017-12-11 05:05:30.370351: step 17350, loss = 0.70, batch loss = 0.64 (14.5 examples/sec; 0.551 sec/batch; 48h:16m:11s remains)
INFO - root - 2017-12-11 05:05:35.866597: step 17360, loss = 0.69, batch loss = 0.63 (15.0 examples/sec; 0.532 sec/batch; 46h:32m:59s remains)
INFO - root - 2017-12-11 05:05:41.418813: step 17370, loss = 0.68, batch loss = 0.62 (13.9 examples/sec; 0.575 sec/batch; 50h:19m:23s remains)
INFO - root - 2017-12-11 05:05:46.972051: step 17380, loss = 0.71, batch loss = 0.65 (14.6 examples/sec; 0.546 sec/batch; 47h:49m:48s remains)
INFO - root - 2017-12-11 05:05:52.524360: step 17390, loss = 0.70, batch loss = 0.64 (13.6 examples/sec; 0.587 sec/batch; 51h:21m:35s remains)
INFO - root - 2017-12-11 05:05:58.081109: step 17400, loss = 0.69, batch loss = 0.64 (15.0 examples/sec; 0.534 sec/batch; 46h:43m:23s remains)
2017-12-11 05:05:58.645027: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.32893264 0.3685661 0.39953175 0.41564298 0.40442637 0.35779139 0.28018537 0.19377907 0.12317704 0.073531434 0.0406207 0.023697838 0.023208268 0.03016985 0.035508592][0.2711876 0.30970389 0.34563211 0.36855555 0.36553663 0.32941714 0.2647489 0.18924604 0.12714185 0.085217915 0.056266189 0.03637648 0.029365266 0.031845462 0.03446994][0.19839206 0.23630466 0.27831197 0.31088647 0.32238376 0.30708805 0.26670679 0.21093604 0.16255067 0.1311467 0.1078857 0.086031236 0.072470464 0.069856443 0.0682973][0.14042583 0.17768657 0.22552584 0.2692458 0.29888034 0.31008118 0.29907954 0.2669763 0.23386721 0.21275909 0.19565783 0.17463951 0.15748909 0.15132578 0.14554147][0.097046942 0.13496061 0.19019397 0.24890421 0.30214816 0.34509897 0.36677742 0.3611204 0.34448159 0.33326602 0.32207143 0.30443 0.28718331 0.27833578 0.26634124][0.079905182 0.12566981 0.19597922 0.27642626 0.3565892 0.42914096 0.47762412 0.49137118 0.48398146 0.47447634 0.46291935 0.44625461 0.42988896 0.41873908 0.39829758][0.090389177 0.14933611 0.23905267 0.34205386 0.44404784 0.5350976 0.59762925 0.62039226 0.614438 0.59858721 0.57973117 0.56056035 0.54580218 0.53372282 0.50453883][0.10474033 0.17638363 0.28296584 0.40282917 0.51659942 0.61367184 0.67942554 0.70451725 0.69652748 0.67171717 0.6433267 0.61960316 0.60494226 0.59074724 0.5540272][0.12271325 0.198179 0.30729195 0.42754945 0.5369156 0.62598729 0.68545067 0.70884603 0.70001614 0.67120486 0.63968343 0.615182 0.59990346 0.58093846 0.53791183][0.14239295 0.21131054 0.30696276 0.41013414 0.49981129 0.56898063 0.6139515 0.63146269 0.62252641 0.59548414 0.56830257 0.54841429 0.53399819 0.51096129 0.46552509][0.15311877 0.2086834 0.28124806 0.35681006 0.41841432 0.46196353 0.48822045 0.49672467 0.48668644 0.46362168 0.44352657 0.42976296 0.41684034 0.39290023 0.35159847][0.14492005 0.18303119 0.2284908 0.27244353 0.30401376 0.3223916 0.33125576 0.33185372 0.32263026 0.30626032 0.2946901 0.28827345 0.27977118 0.26113302 0.23158138][0.11756355 0.13809456 0.15835811 0.17411518 0.18124409 0.18211676 0.18102302 0.17916882 0.17424241 0.16595057 0.16138186 0.15991782 0.15551454 0.14465727 0.12980561][0.0964146 0.10156303 0.10168595 0.097215414 0.088887855 0.080816671 0.076045372 0.074338369 0.072395071 0.067413166 0.062827431 0.05973665 0.05575094 0.051647548 0.050121609][0.086046807 0.078402191 0.065233849 0.051698778 0.03979788 0.03269913 0.030820809 0.031305786 0.029685464 0.021592507 0.0093250144 -0.0024929524 -0.011288147 -0.012603538 -0.0045899698]]...]
INFO - root - 2017-12-11 05:06:04.169644: step 17410, loss = 0.68, batch loss = 0.62 (14.6 examples/sec; 0.547 sec/batch; 47h:53m:48s remains)
INFO - root - 2017-12-11 05:06:09.677592: step 17420, loss = 0.70, batch loss = 0.64 (14.8 examples/sec; 0.540 sec/batch; 47h:18m:08s remains)
INFO - root - 2017-12-11 05:06:15.102428: step 17430, loss = 0.70, batch loss = 0.64 (14.8 examples/sec; 0.540 sec/batch; 47h:15m:57s remains)
INFO - root - 2017-12-11 05:06:20.593507: step 17440, loss = 0.69, batch loss = 0.63 (14.7 examples/sec; 0.545 sec/batch; 47h:40m:32s remains)
INFO - root - 2017-12-11 05:06:25.692182: step 17450, loss = 0.70, batch loss = 0.64 (14.5 examples/sec; 0.553 sec/batch; 48h:25m:34s remains)
INFO - root - 2017-12-11 05:06:31.166906: step 17460, loss = 0.69, batch loss = 0.63 (14.6 examples/sec; 0.547 sec/batch; 47h:54m:26s remains)
INFO - root - 2017-12-11 05:06:36.661776: step 17470, loss = 0.68, batch loss = 0.62 (14.6 examples/sec; 0.548 sec/batch; 47h:59m:39s remains)
INFO - root - 2017-12-11 05:06:42.122191: step 17480, loss = 0.70, batch loss = 0.64 (14.4 examples/sec; 0.554 sec/batch; 48h:27m:50s remains)
INFO - root - 2017-12-11 05:06:47.533370: step 17490, loss = 0.70, batch loss = 0.65 (14.8 examples/sec; 0.541 sec/batch; 47h:18m:43s remains)
INFO - root - 2017-12-11 05:06:53.117898: step 17500, loss = 0.69, batch loss = 0.64 (13.8 examples/sec; 0.581 sec/batch; 50h:52m:23s remains)
2017-12-11 05:06:53.694151: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.032243907 -0.038727228 -0.042800747 -0.044065073 -0.0432445 -0.045842621 -0.05622445 -0.07285288 -0.092690483 -0.11171762 -0.12534244 -0.12925404 -0.12382714 -0.11329036 -0.10216989][0.051507015 0.046141796 0.045054361 0.0493772 0.056665827 0.057029519 0.040324267 0.0071262228 -0.035808574 -0.078697637 -0.11126548 -0.12521827 -0.12047541 -0.10370027 -0.0836438][0.18610226 0.18040439 0.17896181 0.18579963 0.19904941 0.20574304 0.18681864 0.13740142 0.067170747 -0.0064538312 -0.065429993 -0.094605841 -0.091592669 -0.064826265 -0.028552033][0.36042404 0.34935617 0.33812252 0.33735135 0.35134023 0.36962086 0.361411 0.30756807 0.21588433 0.11191013 0.02289963 -0.025921511 -0.025377328 0.017108673 0.080072947][0.54359263 0.51933092 0.48721644 0.46894702 0.480839 0.52067435 0.54346567 0.50610095 0.4075332 0.28073329 0.16348676 0.093643539 0.092401393 0.15627475 0.25540397][0.6864478 0.64466119 0.58821255 0.55076969 0.56271988 0.63198328 0.69941872 0.69390911 0.60204172 0.46279129 0.32432574 0.23764837 0.23856461 0.32890254 0.46879408][0.76370507 0.70626324 0.62988079 0.57824689 0.59318221 0.69080144 0.802035 0.83247346 0.75778544 0.6193217 0.47146255 0.37433344 0.37775281 0.48967183 0.6612404][0.75886011 0.69473016 0.61070848 0.55487478 0.57375073 0.68745947 0.82609165 0.88547254 0.83531362 0.71331483 0.57193655 0.47249264 0.47422019 0.59118927 0.7710765][0.68049318 0.62049043 0.54114586 0.48860043 0.50692785 0.61594 0.75516635 0.82820392 0.80456072 0.71190774 0.59251434 0.50089008 0.49832076 0.60281223 0.76514542][0.55971509 0.51166046 0.44604248 0.40207627 0.41559675 0.50262362 0.61823821 0.68730152 0.68353635 0.621893 0.531137 0.45523646 0.44968802 0.53133696 0.65817934][0.41797706 0.38672835 0.34016237 0.30744568 0.31339997 0.36955184 0.4477731 0.49874866 0.50285071 0.46458846 0.40088478 0.34434417 0.33831573 0.394368 0.47974604][0.27614361 0.26250356 0.23536192 0.21322908 0.21085258 0.23648113 0.27634332 0.30382526 0.30647713 0.2830784 0.24220531 0.20546757 0.20120732 0.23455459 0.28273588][0.1327251 0.13256361 0.12240943 0.10999534 0.1021907 0.10499948 0.11481936 0.12147901 0.11913099 0.10488261 0.082255393 0.0635427 0.063332014 0.081497796 0.10427518][-0.00049318554 0.0059326659 0.0069571431 0.0017762347 -0.0075407051 -0.016464047 -0.023289287 -0.029104216 -0.035142977 -0.043767929 -0.05401244 -0.060082927 -0.056403752 -0.045664005 -0.035185996][-0.095731325 -0.089431144 -0.0840737 -0.085059062 -0.092092857 -0.10202242 -0.11221366 -0.1208713 -0.12723117 -0.13249983 -0.13651951 -0.13703115 -0.13252296 -0.12555234 -0.11957977]]...]
INFO - root - 2017-12-11 05:06:59.306521: step 17510, loss = 0.68, batch loss = 0.62 (14.1 examples/sec; 0.567 sec/batch; 49h:34m:28s remains)
INFO - root - 2017-12-11 05:07:04.754442: step 17520, loss = 0.69, batch loss = 0.63 (14.2 examples/sec; 0.564 sec/batch; 49h:20m:51s remains)
INFO - root - 2017-12-11 05:07:10.174548: step 17530, loss = 0.70, batch loss = 0.64 (14.9 examples/sec; 0.535 sec/batch; 46h:50m:01s remains)
INFO - root - 2017-12-11 05:07:15.310509: step 17540, loss = 0.71, batch loss = 0.65 (20.0 examples/sec; 0.400 sec/batch; 34h:59m:20s remains)
INFO - root - 2017-12-11 05:07:20.845766: step 17550, loss = 0.69, batch loss = 0.63 (14.7 examples/sec; 0.543 sec/batch; 47h:31m:55s remains)
INFO - root - 2017-12-11 05:07:26.388286: step 17560, loss = 0.69, batch loss = 0.63 (14.2 examples/sec; 0.562 sec/batch; 49h:10m:51s remains)
INFO - root - 2017-12-11 05:07:31.829879: step 17570, loss = 0.70, batch loss = 0.64 (14.5 examples/sec; 0.552 sec/batch; 48h:16m:45s remains)
INFO - root - 2017-12-11 05:07:37.297687: step 17580, loss = 0.70, batch loss = 0.64 (14.9 examples/sec; 0.536 sec/batch; 46h:55m:38s remains)
INFO - root - 2017-12-11 05:07:42.780445: step 17590, loss = 0.71, batch loss = 0.65 (14.5 examples/sec; 0.553 sec/batch; 48h:23m:10s remains)
INFO - root - 2017-12-11 05:07:48.302380: step 17600, loss = 0.71, batch loss = 0.65 (15.3 examples/sec; 0.523 sec/batch; 45h:43m:51s remains)
2017-12-11 05:07:48.827139: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.076744005 0.069036916 0.061920721 0.056859177 0.05126632 0.04478791 0.038476329 0.030500308 0.021242633 0.012983611 0.0041770278 -0.002737032 -0.0062571978 -0.0045682318 0.0027492391][0.064942852 0.062034108 0.060929433 0.060347021 0.057193175 0.052429557 0.04700258 0.039758485 0.029827375 0.019242154 0.0072601186 -0.0027543784 -0.0087881815 -0.0098100659 -0.0031401007][0.057875313 0.068095773 0.081400633 0.092748061 0.098902851 0.10175239 0.10123178 0.096092656 0.083321981 0.065293744 0.043958552 0.02455667 0.010749154 0.0038176691 0.010449658][0.074216828 0.1067381 0.14149933 0.16985871 0.1891167 0.2022111 0.20824656 0.20655604 0.19117692 0.16399185 0.1299056 0.096374564 0.068882629 0.050337691 0.054541949][0.11952235 0.18073641 0.24037617 0.28705123 0.31901625 0.34048176 0.35094762 0.35171726 0.334066 0.29832742 0.25225189 0.20483415 0.16216457 0.13018395 0.13195054][0.1881261 0.27848777 0.36154851 0.42387545 0.46425447 0.48890224 0.49966621 0.50070536 0.48108482 0.43952897 0.38598672 0.32880911 0.27294314 0.22823213 0.22720921][0.26089415 0.37111139 0.46780196 0.53685308 0.57763392 0.59862453 0.604971 0.60332471 0.58169878 0.53760827 0.48201105 0.42134866 0.35782138 0.30425915 0.29974139][0.30159652 0.41459894 0.50981957 0.5741992 0.60752237 0.61937839 0.61802179 0.61164832 0.58877373 0.54600835 0.49386063 0.43672377 0.37318075 0.31682631 0.30797267][0.2835283 0.38078821 0.45979655 0.50967377 0.53037357 0.53084356 0.52097708 0.50937134 0.48655024 0.4486241 0.40435591 0.35631537 0.30026516 0.24807538 0.23555137][0.20838889 0.27701074 0.33108816 0.36233106 0.37043929 0.36220863 0.34661487 0.33118716 0.30894622 0.27667865 0.24151923 0.20506622 0.16192731 0.12087791 0.10912245][0.10754978 0.14418358 0.17296329 0.18776041 0.18766785 0.17643742 0.16008624 0.14422518 0.12421852 0.097955629 0.071008027 0.045165367 0.015758455 -0.0117281 -0.019860627][0.021274162 0.032585423 0.042021569 0.045644104 0.042163957 0.032356881 0.01892795 0.005447424 -0.010821238 -0.031196188 -0.05148147 -0.069217317 -0.086972445 -0.10187847 -0.10490023][-0.025694659 -0.028888976 -0.030101744 -0.031494036 -0.034640849 -0.04069205 -0.04925555 -0.058619402 -0.070282213 -0.084913887 -0.099417388 -0.11112411 -0.12044005 -0.12593102 -0.12418674][-0.025625262 -0.032808572 -0.03694148 -0.038764056 -0.039817706 -0.041819796 -0.045545172 -0.050721239 -0.058209322 -0.067926012 -0.077499911 -0.084623449 -0.088495426 -0.088311389 -0.083569951][0.015303047 0.010582675 0.0075424546 0.00689715 0.007474245 0.0079950821 0.0074065593 0.0051551647 0.00072170072 -0.0051747421 -0.010641278 -0.014070847 -0.014622073 -0.011912717 -0.0065407879]]...]
INFO - root - 2017-12-11 05:07:54.349735: step 17610, loss = 0.70, batch loss = 0.64 (14.5 examples/sec; 0.552 sec/batch; 48h:16m:52s remains)
INFO - root - 2017-12-11 05:07:59.901242: step 17620, loss = 0.71, batch loss = 0.65 (14.5 examples/sec; 0.553 sec/batch; 48h:20m:32s remains)
INFO - root - 2017-12-11 05:08:05.389365: step 17630, loss = 0.70, batch loss = 0.64 (14.9 examples/sec; 0.538 sec/batch; 47h:02m:57s remains)
INFO - root - 2017-12-11 05:08:10.527408: step 17640, loss = 0.71, batch loss = 0.66 (14.8 examples/sec; 0.542 sec/batch; 47h:25m:00s remains)
INFO - root - 2017-12-11 05:08:16.022500: step 17650, loss = 0.71, batch loss = 0.65 (14.5 examples/sec; 0.552 sec/batch; 48h:15m:47s remains)
INFO - root - 2017-12-11 05:08:21.486730: step 17660, loss = 0.69, batch loss = 0.64 (14.4 examples/sec; 0.557 sec/batch; 48h:44m:56s remains)
INFO - root - 2017-12-11 05:08:26.934696: step 17670, loss = 0.71, batch loss = 0.65 (14.9 examples/sec; 0.538 sec/batch; 47h:05m:21s remains)
INFO - root - 2017-12-11 05:08:32.433913: step 17680, loss = 0.68, batch loss = 0.63 (14.5 examples/sec; 0.553 sec/batch; 48h:23m:29s remains)
INFO - root - 2017-12-11 05:08:38.028455: step 17690, loss = 0.70, batch loss = 0.64 (14.2 examples/sec; 0.563 sec/batch; 49h:11m:59s remains)
INFO - root - 2017-12-11 05:08:43.489999: step 17700, loss = 0.70, batch loss = 0.64 (15.2 examples/sec; 0.527 sec/batch; 46h:06m:23s remains)
2017-12-11 05:08:44.048672: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.18628623 0.18491349 0.18627229 0.19450124 0.20384617 0.20287493 0.18696217 0.16442518 0.14966032 0.14676362 0.14580597 0.13726531 0.12294047 0.11223559 0.11221897][0.22119084 0.2201492 0.22188994 0.23020875 0.23898266 0.23802216 0.22418259 0.20372112 0.18722151 0.17793925 0.1685645 0.15375333 0.13801035 0.13041022 0.1355008][0.24273585 0.24077156 0.24126831 0.24784777 0.25515783 0.25559914 0.2474121 0.2332176 0.21836782 0.20462126 0.1872614 0.16575529 0.14842252 0.14480188 0.15624999][0.25039747 0.24343666 0.23914193 0.24110848 0.24614386 0.24888322 0.24759464 0.24136494 0.23175852 0.21907918 0.20008765 0.17696552 0.16016565 0.16006583 0.17581683][0.24027315 0.23257107 0.22479518 0.22120762 0.22158632 0.22320768 0.22450984 0.22364995 0.22067451 0.21436693 0.20189135 0.18431574 0.17077133 0.17190251 0.18770027][0.21590507 0.21400723 0.20770177 0.20069794 0.19550544 0.19265817 0.19270492 0.19530122 0.20014107 0.20412178 0.20285656 0.1938199 0.18280576 0.18062474 0.1915178][0.1937093 0.2002378 0.1969759 0.18672068 0.17536627 0.16721062 0.16520041 0.17056397 0.1824438 0.19627921 0.20512614 0.20350333 0.19438423 0.18864024 0.19441162][0.18317296 0.19549479 0.19355138 0.17909847 0.1615396 0.14872018 0.14452659 0.15077199 0.16612613 0.18590134 0.20265321 0.20963776 0.20646244 0.20123981 0.20353083][0.17118038 0.18482096 0.18368626 0.16747357 0.14633265 0.13032489 0.12428489 0.12996356 0.14596158 0.1684562 0.19201081 0.20931895 0.2157784 0.21420465 0.21338233][0.14859851 0.16223919 0.16406837 0.15123317 0.13091314 0.11360232 0.10612675 0.11116096 0.12745966 0.15121125 0.17871271 0.20212257 0.21445234 0.21411826 0.20864546][0.12331095 0.13577698 0.14190458 0.13603874 0.12051298 0.10493755 0.098574631 0.10556292 0.12358132 0.14705651 0.17257996 0.19326225 0.20350437 0.20025344 0.18966581][0.10966682 0.11905604 0.12507354 0.12289005 0.11206716 0.1004754 0.097637452 0.10799939 0.12732357 0.14786457 0.16603746 0.17804044 0.18245018 0.17690419 0.16460864][0.11031629 0.11292388 0.11203215 0.1075554 0.099367276 0.092775062 0.095057026 0.10925085 0.12915269 0.1455663 0.15497632 0.15767512 0.15720841 0.15225731 0.14184316][0.11678752 0.11164197 0.10174187 0.092635415 0.086281568 0.085404225 0.093969561 0.11218327 0.13228427 0.14437009 0.14538643 0.13948812 0.13426627 0.12932822 0.12038867][0.12455609 0.1131475 0.0960301 0.083240844 0.07906989 0.084408343 0.099628754 0.12181858 0.14201733 0.15047243 0.14523259 0.13354497 0.12443282 0.11674099 0.10471851]]...]
INFO - root - 2017-12-11 05:08:49.517507: step 17710, loss = 0.71, batch loss = 0.65 (14.9 examples/sec; 0.538 sec/batch; 47h:00m:53s remains)
INFO - root - 2017-12-11 05:08:55.056210: step 17720, loss = 0.70, batch loss = 0.64 (14.1 examples/sec; 0.567 sec/batch; 49h:36m:14s remains)
INFO - root - 2017-12-11 05:09:00.555164: step 17730, loss = 0.70, batch loss = 0.64 (14.5 examples/sec; 0.551 sec/batch; 48h:10m:36s remains)
INFO - root - 2017-12-11 05:09:05.721930: step 17740, loss = 0.70, batch loss = 0.64 (14.2 examples/sec; 0.563 sec/batch; 49h:13m:38s remains)
INFO - root - 2017-12-11 05:09:11.177425: step 17750, loss = 0.73, batch loss = 0.67 (14.9 examples/sec; 0.536 sec/batch; 46h:53m:09s remains)
INFO - root - 2017-12-11 05:09:16.670540: step 17760, loss = 0.70, batch loss = 0.64 (14.6 examples/sec; 0.549 sec/batch; 48h:01m:50s remains)
INFO - root - 2017-12-11 05:09:22.162271: step 17770, loss = 0.69, batch loss = 0.63 (15.0 examples/sec; 0.535 sec/batch; 46h:44m:45s remains)
INFO - root - 2017-12-11 05:09:27.612306: step 17780, loss = 0.69, batch loss = 0.63 (14.8 examples/sec; 0.542 sec/batch; 47h:21m:29s remains)
INFO - root - 2017-12-11 05:09:33.078384: step 17790, loss = 0.71, batch loss = 0.65 (14.6 examples/sec; 0.548 sec/batch; 47h:53m:03s remains)
INFO - root - 2017-12-11 05:09:38.682294: step 17800, loss = 0.69, batch loss = 0.63 (14.6 examples/sec; 0.547 sec/batch; 47h:48m:34s remains)
2017-12-11 05:09:39.273850: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.022491632 0.025041653 0.021092487 0.01367256 0.007083816 0.0024052702 -0.0019493788 -0.0066223587 -0.0105922 -0.01216724 -0.011668911 -0.009470419 -0.0058479514 -0.0020828482 0.0014700299][0.048922732 0.052707765 0.046301536 0.034526497 0.024127029 0.017024027 0.010575469 0.0036828776 -0.0021271333 -0.0039549619 -0.00197526 0.0032853629 0.011137349 0.018513119 0.023772843][0.075400874 0.079913564 0.070086136 0.053388037 0.039139148 0.030214144 0.022881282 0.01504963 0.00836691 0.0072024055 0.011585363 0.020545317 0.032430038 0.042169876 0.047080651][0.095171481 0.098782972 0.08543735 0.064720467 0.047898151 0.0387241 0.032521475 0.025696419 0.01944074 0.019191446 0.025435694 0.036514595 0.049745843 0.05884283 0.060739327][0.10587104 0.10843032 0.093150139 0.070839867 0.053098973 0.044766687 0.041059092 0.036578652 0.031387679 0.031167299 0.037361983 0.047621485 0.058679931 0.064455189 0.062095836][0.10880649 0.11076155 0.095951937 0.074708931 0.057517998 0.050505593 0.049510546 0.047697444 0.043218143 0.041461654 0.0455267 0.052933894 0.060182121 0.062166747 0.056656674][0.10551052 0.10719871 0.094218127 0.075713024 0.060332004 0.055009279 0.056510508 0.057131875 0.052719485 0.047968019 0.048393592 0.052502915 0.056653339 0.057105847 0.05189123][0.098861039 0.1005038 0.08971294 0.074454658 0.061408803 0.05742259 0.060318086 0.062436741 0.057563175 0.049254138 0.045457434 0.046616022 0.049369387 0.051382616 0.05024216][0.09389209 0.094788164 0.085381106 0.072557047 0.061015479 0.056623407 0.058586407 0.060554393 0.05526261 0.044510733 0.037509639 0.036837686 0.040464506 0.047153741 0.05308399][0.094424911 0.093948677 0.084518738 0.072368443 0.060624477 0.053655323 0.05224213 0.05247033 0.047352 0.03655009 0.028574178 0.027731109 0.034255486 0.047605466 0.061920088][0.10312751 0.10041856 0.088992052 0.075126037 0.061628107 0.051219087 0.045368969 0.042724807 0.03783235 0.028862508 0.022186309 0.022836173 0.033065669 0.052812848 0.074727476][0.11778811 0.11218713 0.096356086 0.078105763 0.061885793 0.049249865 0.040655795 0.035732776 0.030859597 0.023850484 0.019019945 0.021368649 0.034432497 0.058848724 0.086874738][0.13397239 0.12615985 0.10505856 0.080985464 0.0613441 0.047605671 0.038369745 0.032643538 0.027899817 0.022274716 0.01867513 0.021613671 0.035350304 0.061698262 0.0938325][0.14113937 0.13163669 0.1069216 0.078651674 0.056660574 0.042822558 0.034390084 0.029241476 0.025179457 0.020737641 0.01796924 0.020540107 0.032843221 0.058043208 0.09108568][0.12848663 0.11814408 0.093531206 0.065419175 0.044000894 0.0314462 0.024697248 0.021082429 0.018574795 0.015893668 0.014293538 0.016372839 0.025997894 0.047300797 0.077256292]]...]
INFO - root - 2017-12-11 05:09:44.811729: step 17810, loss = 0.69, batch loss = 0.63 (14.5 examples/sec; 0.553 sec/batch; 48h:20m:33s remains)
INFO - root - 2017-12-11 05:09:50.281542: step 17820, loss = 0.69, batch loss = 0.63 (14.7 examples/sec; 0.545 sec/batch; 47h:38m:06s remains)
INFO - root - 2017-12-11 05:09:55.751876: step 17830, loss = 0.71, batch loss = 0.65 (14.3 examples/sec; 0.558 sec/batch; 48h:47m:46s remains)
INFO - root - 2017-12-11 05:10:00.864582: step 17840, loss = 0.70, batch loss = 0.65 (14.9 examples/sec; 0.538 sec/batch; 47h:01m:45s remains)
INFO - root - 2017-12-11 05:10:06.325614: step 17850, loss = 0.72, batch loss = 0.66 (15.1 examples/sec; 0.531 sec/batch; 46h:24m:37s remains)
INFO - root - 2017-12-11 05:10:11.852449: step 17860, loss = 0.71, batch loss = 0.65 (14.2 examples/sec; 0.564 sec/batch; 49h:17m:48s remains)
INFO - root - 2017-12-11 05:10:17.392498: step 17870, loss = 0.70, batch loss = 0.64 (14.1 examples/sec; 0.566 sec/batch; 49h:30m:07s remains)
INFO - root - 2017-12-11 05:10:22.928280: step 17880, loss = 0.71, batch loss = 0.65 (15.0 examples/sec; 0.532 sec/batch; 46h:32m:11s remains)
INFO - root - 2017-12-11 05:10:28.437034: step 17890, loss = 0.69, batch loss = 0.63 (13.5 examples/sec; 0.591 sec/batch; 51h:40m:15s remains)
INFO - root - 2017-12-11 05:10:33.884559: step 17900, loss = 0.70, batch loss = 0.64 (14.7 examples/sec; 0.545 sec/batch; 47h:38m:05s remains)
2017-12-11 05:10:34.441370: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.24163435 0.25389144 0.24142575 0.20988606 0.16827448 0.13875702 0.13777 0.16660219 0.21269019 0.26911989 0.31693685 0.33553696 0.3152684 0.26046094 0.18915781][0.2226402 0.23130186 0.2156522 0.18419696 0.14678538 0.12474265 0.13230766 0.16827042 0.21573445 0.26406464 0.29901618 0.30455452 0.27488813 0.21547005 0.14500545][0.19556551 0.19432771 0.17570305 0.15162855 0.12793006 0.11978777 0.13668758 0.17410347 0.2140702 0.24583498 0.26211384 0.25339255 0.21714514 0.15840122 0.094390109][0.17875928 0.16685233 0.14959513 0.14083824 0.13825312 0.14703383 0.17044669 0.20053129 0.22236674 0.22998969 0.22427911 0.20188901 0.16121708 0.10602503 0.050081775][0.18145 0.16382517 0.15380555 0.16549994 0.18736677 0.21345289 0.23975261 0.25506538 0.25071451 0.22963297 0.20087959 0.16573635 0.12186272 0.070601135 0.021417672][0.20625095 0.18971449 0.19183962 0.22479279 0.26927015 0.30963647 0.33489558 0.33104709 0.29802841 0.24957018 0.20095928 0.15586635 0.11011831 0.062537111 0.017617632][0.23757491 0.22794572 0.2426742 0.29065502 0.34872016 0.39534178 0.41519168 0.39275485 0.33677047 0.27051792 0.211501 0.16332808 0.11945005 0.076719284 0.034975152][0.25439706 0.25272143 0.2758624 0.32837513 0.38783577 0.43224689 0.44548196 0.41211149 0.34703514 0.27889028 0.22262742 0.17909688 0.13965823 0.10034936 0.05831968][0.24801528 0.24959086 0.27282318 0.31874967 0.3696332 0.407663 0.41895777 0.38898262 0.33428434 0.28332898 0.24405952 0.21262588 0.17888857 0.13959898 0.091598034][0.21628605 0.21432523 0.22919291 0.26078027 0.29804754 0.32942206 0.34459376 0.33070254 0.30268252 0.28473562 0.27454659 0.26270884 0.23782361 0.19808483 0.14076215][0.17219366 0.16286777 0.16582672 0.18203297 0.20599183 0.2320191 0.25328782 0.25977257 0.26448342 0.28548378 0.30909139 0.32022831 0.30624822 0.266627 0.19878383][0.13672693 0.12267052 0.11664657 0.12204207 0.13668287 0.15858781 0.18368116 0.20570642 0.23792201 0.29223323 0.34518635 0.37675774 0.37279421 0.33315632 0.25506583][0.11580625 0.10191662 0.093198031 0.094554551 0.10476209 0.12315387 0.14741355 0.17484355 0.22108234 0.29382759 0.36451504 0.41008216 0.41461509 0.37687197 0.29339543][0.1028714 0.0935066 0.087484248 0.089521177 0.098241545 0.11240198 0.13042375 0.15230997 0.19638129 0.26889816 0.34201565 0.39276862 0.40403685 0.37300912 0.29488343][0.0782511 0.074888773 0.073565081 0.077360228 0.084490947 0.092996418 0.10158578 0.11183772 0.14269055 0.20051582 0.26282516 0.30978864 0.32466581 0.30362189 0.24090633]]...]
INFO - root - 2017-12-11 05:10:39.850121: step 17910, loss = 0.69, batch loss = 0.63 (15.2 examples/sec; 0.525 sec/batch; 45h:50m:59s remains)
INFO - root - 2017-12-11 05:10:45.245795: step 17920, loss = 0.70, batch loss = 0.64 (14.8 examples/sec; 0.542 sec/batch; 47h:23m:14s remains)
INFO - root - 2017-12-11 05:10:50.409782: step 17930, loss = 0.70, batch loss = 0.64 (24.5 examples/sec; 0.327 sec/batch; 28h:34m:32s remains)
INFO - root - 2017-12-11 05:10:55.891398: step 17940, loss = 0.71, batch loss = 0.65 (14.8 examples/sec; 0.542 sec/batch; 47h:21m:29s remains)
INFO - root - 2017-12-11 05:11:01.310317: step 17950, loss = 0.69, batch loss = 0.63 (15.3 examples/sec; 0.521 sec/batch; 45h:33m:21s remains)
INFO - root - 2017-12-11 05:11:06.869955: step 17960, loss = 0.69, batch loss = 0.63 (14.6 examples/sec; 0.547 sec/batch; 47h:47m:38s remains)
INFO - root - 2017-12-11 05:11:12.420205: step 17970, loss = 0.70, batch loss = 0.65 (14.5 examples/sec; 0.553 sec/batch; 48h:18m:39s remains)
INFO - root - 2017-12-11 05:11:17.902231: step 17980, loss = 0.69, batch loss = 0.63 (14.8 examples/sec; 0.540 sec/batch; 47h:12m:36s remains)
INFO - root - 2017-12-11 05:11:23.381808: step 17990, loss = 0.70, batch loss = 0.64 (14.3 examples/sec; 0.559 sec/batch; 48h:51m:28s remains)
INFO - root - 2017-12-11 05:11:28.970477: step 18000, loss = 0.68, batch loss = 0.63 (14.2 examples/sec; 0.563 sec/batch; 49h:11m:06s remains)
2017-12-11 05:11:29.529737: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.071442232 -0.060084738 -0.036587935 -0.0058102133 0.027724363 0.059050735 0.0803595 0.085256688 0.076199353 0.060655497 0.049010888 0.0507985 0.0636291 0.082201108 0.10080135][-0.071838841 -0.057294 -0.028497824 0.0092591783 0.051716022 0.092992418 0.12291791 0.13280804 0.12532058 0.10844487 0.092156574 0.087069467 0.092042074 0.10347378 0.11665645][-0.069928959 -0.052865941 -0.020711664 0.022130474 0.072281562 0.1231835 0.16221461 0.17846629 0.17415872 0.15643705 0.13359059 0.11775725 0.11087035 0.11188045 0.11749405][-0.068281084 -0.049886897 -0.015498936 0.03181554 0.08986304 0.15085237 0.19931164 0.22233266 0.22150214 0.20274015 0.17179613 0.14216663 0.1203602 0.10824416 0.10369506][-0.06657619 -0.04710694 -0.010725949 0.040931933 0.1067067 0.17733602 0.23439644 0.26311457 0.2643249 0.24400559 0.20486464 0.1613643 0.12497976 0.10041016 0.08651267][-0.065006308 -0.044745222 -0.0067148553 0.04934698 0.1222916 0.20122702 0.26534608 0.29793537 0.2984744 0.27437678 0.2270871 0.17303377 0.12814181 0.098807417 0.083095767][-0.063574277 -0.043269411 -0.0033899804 0.057635102 0.13781698 0.2246971 0.29571894 0.33137143 0.32893941 0.29746723 0.23963957 0.17588636 0.12624122 0.098242037 0.086808488][-0.061685912 -0.040125176 0.0045365221 0.074765325 0.16636337 0.26453257 0.34450206 0.38341054 0.37493804 0.33083767 0.25844866 0.1850381 0.13361 0.11019588 0.10606952][-0.0595384 -0.036218505 0.014231972 0.094634905 0.19817042 0.30779994 0.39633757 0.43741605 0.4215022 0.36413538 0.27854598 0.19811708 0.14779627 0.13160087 0.1367442][-0.05893638 -0.03484511 0.018284196 0.10405757 0.21352433 0.32847792 0.42091388 0.46234131 0.44108084 0.37502629 0.28266829 0.2007187 0.15444832 0.1463479 0.16076677][-0.059353884 -0.036009278 0.015817475 0.099754095 0.20605919 0.31673652 0.40533847 0.44361848 0.420019 0.35208222 0.26105756 0.18354975 0.14350022 0.14275147 0.16398893][-0.060465664 -0.039010931 0.0084377183 0.083995596 0.17864017 0.27549604 0.35175583 0.38212764 0.35751247 0.29358876 0.21067603 0.14201686 0.1082857 0.11161254 0.13499634][-0.063368328 -0.0451599 -0.0050235577 0.057269152 0.1346115 0.21199268 0.27069375 0.2902317 0.26549947 0.21032874 0.14118555 0.085453346 0.059543114 0.065420948 0.088025011][-0.0691344 -0.057554219 -0.029812058 0.013213363 0.067569859 0.12151877 0.16096866 0.17091718 0.15019853 0.109538 0.059793245 0.020807985 0.0043384191 0.011844655 0.03102167][-0.075751044 -0.072176605 -0.058923621 -0.0374006 -0.0081383009 0.021764725 0.043317202 0.046618357 0.03318188 0.009884458 -0.018088195 -0.038921732 -0.045655075 -0.037356246 -0.023034947]]...]
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-SGD-lr0.001-clip50-initfromfixqian30k-fixqian
INFO - root - 2017-12-11 05:11:35.130220: step 18010, loss = 0.69, batch loss = 0.63 (14.2 examples/sec; 0.562 sec/batch; 49h:04m:07s remains)
INFO - root - 2017-12-11 05:11:40.586019: step 18020, loss = 0.70, batch loss = 0.64 (14.6 examples/sec; 0.548 sec/batch; 47h:49m:44s remains)
INFO - root - 2017-12-11 05:11:45.750251: step 18030, loss = 0.70, batch loss = 0.64 (14.6 examples/sec; 0.547 sec/batch; 47h:49m:09s remains)
INFO - root - 2017-12-11 05:11:51.293765: step 18040, loss = 0.71, batch loss = 0.65 (14.5 examples/sec; 0.551 sec/batch; 48h:06m:31s remains)
INFO - root - 2017-12-11 05:11:56.860825: step 18050, loss = 0.71, batch loss = 0.65 (14.6 examples/sec; 0.547 sec/batch; 47h:49m:15s remains)
INFO - root - 2017-12-11 05:12:02.357682: step 18060, loss = 0.70, batch loss = 0.64 (14.7 examples/sec; 0.545 sec/batch; 47h:38m:12s remains)
INFO - root - 2017-12-11 05:12:07.889023: step 18070, loss = 0.70, batch loss = 0.64 (14.4 examples/sec; 0.554 sec/batch; 48h:23m:23s remains)
INFO - root - 2017-12-11 05:12:13.337768: step 18080, loss = 0.71, batch loss = 0.65 (14.8 examples/sec; 0.542 sec/batch; 47h:20m:50s remains)
INFO - root - 2017-12-11 05:12:18.807817: step 18090, loss = 0.69, batch loss = 0.63 (14.8 examples/sec; 0.542 sec/batch; 47h:21m:17s remains)
INFO - root - 2017-12-11 05:12:24.199422: step 18100, loss = 0.70, batch loss = 0.64 (15.1 examples/sec; 0.531 sec/batch; 46h:20m:08s remains)
2017-12-11 05:12:24.789809: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.62940794 0.6148954 0.55914205 0.51904166 0.52627903 0.57611269 0.63772291 0.68807167 0.71887147 0.72247022 0.6936332 0.651637 0.63789004 0.664011 0.7028929][0.64823514 0.62637895 0.56225169 0.5172863 0.52674448 0.5870049 0.66036284 0.71508819 0.73952776 0.72578543 0.66890287 0.59566605 0.56263763 0.5864166 0.63355315][0.63755047 0.61352259 0.54758513 0.500223 0.50943822 0.5749712 0.6550321 0.71013033 0.72444582 0.689819 0.60471267 0.50360674 0.45271176 0.47198778 0.5241248][0.62776232 0.61053056 0.55185413 0.50801992 0.51889431 0.58834535 0.67192358 0.72327286 0.72437775 0.67033905 0.56552225 0.44901174 0.39008474 0.4083989 0.46023923][0.628143 0.62287414 0.57797205 0.54510158 0.564047 0.64065778 0.72667521 0.77054733 0.75693983 0.68835086 0.5754413 0.45918196 0.405409 0.42868277 0.47660574][0.653908 0.66953772 0.64661783 0.6345914 0.668455 0.74944019 0.82646257 0.84991926 0.81471109 0.73218793 0.61853921 0.51301527 0.47078592 0.49909744 0.53987432][0.70053607 0.74606174 0.75441653 0.77377212 0.82859743 0.9080739 0.9600395 0.9475016 0.88319427 0.78665417 0.67691773 0.58721888 0.55604625 0.584957 0.61750185][0.7399267 0.81566679 0.85868895 0.91091871 0.98287958 1.0508718 1.0655731 1.0100683 0.91675395 0.81178349 0.71282762 0.64258921 0.62233448 0.65041006 0.67677528][0.71846485 0.80983287 0.875272 0.947008 1.0238117 1.0736783 1.0550255 0.969954 0.86200404 0.7606861 0.67908704 0.62861091 0.61855227 0.64658111 0.67081326][0.6151939 0.70078671 0.76807928 0.83912981 0.90645379 0.93762189 0.90219939 0.81303006 0.71218336 0.62722683 0.56602609 0.53071791 0.52536297 0.5503065 0.57370871][0.4609358 0.52731448 0.58049643 0.63433337 0.68036455 0.6925264 0.65327621 0.57846117 0.49998251 0.43857881 0.39833993 0.37520859 0.37111628 0.39104646 0.41328421][0.28747168 0.32827812 0.36002871 0.39113292 0.41529843 0.41557354 0.38416895 0.33373475 0.28339249 0.24641371 0.22501637 0.2115979 0.20778483 0.22299287 0.24441597][0.12329765 0.13659348 0.14557469 0.15588515 0.16424961 0.16129565 0.14412645 0.11890705 0.09365584 0.076369211 0.068829209 0.062323537 0.059324339 0.071439043 0.092175066][-0.014833732 -0.022652751 -0.029497793 -0.031609427 -0.030440904 -0.031145021 -0.035168048 -0.041944154 -0.050802957 -0.0566174 -0.057380412 -0.060740467 -0.063603543 -0.054867212 -0.037221733][-0.098143369 -0.11436151 -0.12636292 -0.13265094 -0.13426234 -0.13397412 -0.13235398 -0.13099131 -0.1322374 -0.13323161 -0.13233435 -0.13491713 -0.13786717 -0.13196148 -0.118145]]...]
INFO - root - 2017-12-11 05:12:30.289968: step 18110, loss = 0.71, batch loss = 0.65 (14.7 examples/sec; 0.544 sec/batch; 47h:28m:10s remains)
INFO - root - 2017-12-11 05:12:35.776061: step 18120, loss = 0.71, batch loss = 0.65 (14.4 examples/sec; 0.555 sec/batch; 48h:25m:59s remains)
INFO - root - 2017-12-11 05:12:41.010901: step 18130, loss = 0.70, batch loss = 0.64 (14.2 examples/sec; 0.562 sec/batch; 49h:02m:40s remains)
INFO - root - 2017-12-11 05:12:46.550372: step 18140, loss = 0.69, batch loss = 0.63 (14.8 examples/sec; 0.540 sec/batch; 47h:09m:14s remains)
INFO - root - 2017-12-11 05:12:52.102285: step 18150, loss = 0.70, batch loss = 0.64 (14.2 examples/sec; 0.561 sec/batch; 49h:01m:39s remains)
INFO - root - 2017-12-11 05:12:57.669153: step 18160, loss = 0.69, batch loss = 0.63 (14.4 examples/sec; 0.555 sec/batch; 48h:26m:04s remains)
INFO - root - 2017-12-11 05:13:03.141967: step 18170, loss = 0.70, batch loss = 0.65 (14.6 examples/sec; 0.546 sec/batch; 47h:41m:35s remains)
INFO - root - 2017-12-11 05:13:08.578083: step 18180, loss = 0.68, batch loss = 0.62 (14.7 examples/sec; 0.543 sec/batch; 47h:25m:37s remains)
INFO - root - 2017-12-11 05:13:14.039483: step 18190, loss = 0.70, batch loss = 0.64 (14.5 examples/sec; 0.551 sec/batch; 48h:05m:13s remains)
INFO - root - 2017-12-11 05:13:19.539889: step 18200, loss = 0.70, batch loss = 0.64 (14.6 examples/sec; 0.549 sec/batch; 47h:54m:42s remains)
2017-12-11 05:13:20.150132: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.051824421 -0.044631734 -0.033821713 -0.023169912 -0.017672094 -0.019595142 -0.028426269 -0.037854563 -0.048329294 -0.060419787 -0.071738891 -0.078607939 -0.079469621 -0.079323187 -0.078440465][0.015723579 0.029098378 0.048143726 0.069535635 0.085839957 0.092273355 0.086108871 0.074631169 0.056597132 0.032915454 0.0080662677 -0.010674392 -0.019191161 -0.025047036 -0.028674908][0.11181001 0.13508129 0.16639112 0.20414911 0.23764995 0.25790143 0.25862139 0.24739464 0.22027837 0.18014997 0.13550262 0.098839931 0.076195352 0.057631657 0.044332068][0.21490964 0.25182271 0.30017489 0.36028406 0.41781828 0.45785993 0.47088504 0.46349508 0.42804989 0.37062576 0.30389038 0.24431142 0.20033497 0.16187708 0.13284528][0.29492244 0.34783289 0.41742197 0.50382876 0.58961171 0.65268487 0.68062079 0.67882937 0.63847917 0.56936538 0.48639485 0.40693131 0.34053996 0.2780337 0.22685087][0.33296016 0.40291262 0.4952364 0.60938221 0.72517264 0.81283981 0.8562215 0.85939294 0.81586379 0.74029946 0.64898205 0.55660635 0.47234437 0.38836923 0.31461665][0.32757631 0.41192067 0.52309179 0.65935516 0.79910916 0.90773648 0.96595305 0.97527969 0.93298781 0.859021 0.77014381 0.67578858 0.58312577 0.48568577 0.39412132][0.29446992 0.39010617 0.51360941 0.66043168 0.81001657 0.92897618 0.996948 1.011201 0.97431564 0.9124887 0.84087378 0.76011175 0.67241114 0.57298338 0.47023892][0.25874093 0.36136848 0.48862517 0.63168389 0.772594 0.88574952 0.95260674 0.9662714 0.936468 0.89523077 0.85494292 0.80440187 0.73771411 0.65037161 0.54637986][0.243639 0.34957674 0.47254562 0.5992189 0.71477032 0.80532134 0.85707146 0.86118627 0.8359313 0.81737655 0.81438929 0.80272144 0.76692528 0.70205653 0.6065768][0.25706002 0.36273178 0.47535986 0.577731 0.65767539 0.71347916 0.73900574 0.72691029 0.70285857 0.70310193 0.73274088 0.75652105 0.75094265 0.71099389 0.63059026][0.27900162 0.37881094 0.47585556 0.55043948 0.59231448 0.60888153 0.60246933 0.57052308 0.54432648 0.55698657 0.60899919 0.65791529 0.67478806 0.65585506 0.59316689][0.27455795 0.3602567 0.43699321 0.48528811 0.49569851 0.47962847 0.44509315 0.39574915 0.36573434 0.38321254 0.44472623 0.50567067 0.53514379 0.53041178 0.48435268][0.22312711 0.28599644 0.33821332 0.3637608 0.35410669 0.32005817 0.27072415 0.21361543 0.18150346 0.19692613 0.255061 0.31491196 0.34792531 0.35163954 0.32075459][0.13241656 0.16814385 0.19529676 0.20255034 0.18289824 0.14439707 0.094319284 0.040269166 0.0092106266 0.018036451 0.06331522 0.11279328 0.14370042 0.15318114 0.13707767]]...]
INFO - root - 2017-12-11 05:13:25.741758: step 18210, loss = 0.70, batch loss = 0.64 (14.4 examples/sec; 0.555 sec/batch; 48h:27m:29s remains)
INFO - root - 2017-12-11 05:13:31.203119: step 18220, loss = 0.71, batch loss = 0.65 (14.4 examples/sec; 0.557 sec/batch; 48h:37m:03s remains)
INFO - root - 2017-12-11 05:13:36.341351: step 18230, loss = 0.67, batch loss = 0.61 (15.3 examples/sec; 0.524 sec/batch; 45h:42m:43s remains)
INFO - root - 2017-12-11 05:13:41.841715: step 18240, loss = 0.70, batch loss = 0.64 (14.6 examples/sec; 0.547 sec/batch; 47h:46m:49s remains)
INFO - root - 2017-12-11 05:13:47.388322: step 18250, loss = 0.70, batch loss = 0.64 (14.6 examples/sec; 0.547 sec/batch; 47h:47m:23s remains)
INFO - root - 2017-12-11 05:13:53.001973: step 18260, loss = 0.69, batch loss = 0.63 (13.9 examples/sec; 0.575 sec/batch; 50h:09m:22s remains)
INFO - root - 2017-12-11 05:13:58.478654: step 18270, loss = 0.70, batch loss = 0.64 (14.8 examples/sec; 0.541 sec/batch; 47h:15m:49s remains)
INFO - root - 2017-12-11 05:14:03.945409: step 18280, loss = 0.69, batch loss = 0.64 (14.3 examples/sec; 0.559 sec/batch; 48h:48m:29s remains)
INFO - root - 2017-12-11 05:14:09.500925: step 18290, loss = 0.69, batch loss = 0.63 (14.4 examples/sec; 0.557 sec/batch; 48h:37m:10s remains)
INFO - root - 2017-12-11 05:14:14.981234: step 18300, loss = 0.70, batch loss = 0.64 (14.8 examples/sec; 0.542 sec/batch; 47h:18m:33s remains)
2017-12-11 05:14:15.576237: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.11590072 0.094369739 0.061340533 0.030599251 0.0048172944 -0.01713782 -0.031274367 -0.039138179 -0.047694344 -0.064588122 -0.08586403 -0.099436618 -0.093090117 -0.060187962 -0.00023311998][0.084718026 0.065269321 0.044662911 0.034048736 0.030808706 0.027281228 0.023436628 0.016306793 -6.6418652e-05 -0.030597135 -0.067339361 -0.093685865 -0.094722979 -0.063092031 0.00061508943][0.054484453 0.038606707 0.033996966 0.049783543 0.077048086 0.099566437 0.11121869 0.10667626 0.080265813 0.029934105 -0.02991222 -0.075999826 -0.089820243 -0.063735254 0.00032660869][0.03976265 0.030311629 0.0435945 0.087783694 0.14871775 0.20185338 0.23309499 0.2330437 0.1953795 0.12057652 0.030098155 -0.043605223 -0.076293677 -0.060304068 0.00076887518][0.042490181 0.043876994 0.077213049 0.15062973 0.24589039 0.33115065 0.38384473 0.38902083 0.33906898 0.23699751 0.11117637 0.0040961727 -0.053267188 -0.052114107 0.0025111237][0.062394373 0.078346252 0.13318746 0.234994 0.36200243 0.47660822 0.54868788 0.55724496 0.49348977 0.36363819 0.20266254 0.0617149 -0.021719353 -0.037780877 0.0078514786][0.090530612 0.12264822 0.19851616 0.32630971 0.48059919 0.61841643 0.70351881 0.709819 0.62946868 0.47336254 0.28241953 0.11382894 0.009596047 -0.020774644 0.01654976][0.12439875 0.17082742 0.26269692 0.40775648 0.57805789 0.72694719 0.8148157 0.8129831 0.71684122 0.54269224 0.33444586 0.15062657 0.034031548 -0.0065821232 0.021273179][0.16494538 0.22005962 0.31724465 0.46330416 0.62951005 0.77012646 0.84726262 0.83409095 0.73023129 0.55591017 0.35294127 0.17396474 0.057159297 0.0097321784 0.024380662][0.21777995 0.27466553 0.36378747 0.49118283 0.63003224 0.74129325 0.79506445 0.77164054 0.67424387 0.52334625 0.35361019 0.2042378 0.10284894 0.05291022 0.049779207][0.28290161 0.33414912 0.4010151 0.49085489 0.58252674 0.64843 0.67102444 0.64177066 0.56664014 0.46182552 0.35009959 0.25236875 0.18182127 0.13593309 0.11342847][0.35432711 0.39435774 0.42989826 0.47243139 0.50897765 0.52535987 0.51778173 0.48983872 0.44792551 0.39969856 0.353693 0.31302816 0.27654669 0.23638682 0.19520755][0.42540792 0.44989797 0.45161435 0.44802245 0.43447623 0.40866369 0.37868533 0.358847 0.35317841 0.35858929 0.36992088 0.376495 0.36629263 0.32887557 0.27196956][0.46769115 0.47635385 0.45003915 0.41274527 0.36524615 0.31234431 0.26982263 0.26018319 0.2844629 0.32957578 0.38013834 0.41719949 0.42325962 0.38777107 0.323577][0.4571794 0.45385087 0.41212013 0.36007693 0.30031195 0.23898894 0.19394094 0.19127113 0.23033127 0.29258752 0.35803872 0.40577772 0.41856432 0.38781923 0.32850635]]...]
INFO - root - 2017-12-11 05:14:21.047328: step 18310, loss = 0.70, batch loss = 0.64 (14.5 examples/sec; 0.551 sec/batch; 48h:02m:47s remains)
INFO - root - 2017-12-11 05:14:26.188957: step 18320, loss = 0.71, batch loss = 0.65 (15.5 examples/sec; 0.516 sec/batch; 45h:01m:35s remains)
INFO - root - 2017-12-11 05:14:31.773350: step 18330, loss = 0.69, batch loss = 0.63 (15.0 examples/sec; 0.533 sec/batch; 46h:30m:12s remains)
INFO - root - 2017-12-11 05:14:37.301735: step 18340, loss = 0.68, batch loss = 0.62 (15.0 examples/sec; 0.535 sec/batch; 46h:41m:24s remains)
INFO - root - 2017-12-11 05:14:42.767928: step 18350, loss = 0.68, batch loss = 0.62 (14.9 examples/sec; 0.538 sec/batch; 46h:57m:54s remains)
INFO - root - 2017-12-11 05:14:48.344618: step 18360, loss = 0.70, batch loss = 0.64 (14.6 examples/sec; 0.547 sec/batch; 47h:46m:19s remains)
INFO - root - 2017-12-11 05:14:53.854042: step 18370, loss = 0.70, batch loss = 0.64 (14.6 examples/sec; 0.548 sec/batch; 47h:46m:53s remains)
INFO - root - 2017-12-11 05:14:59.360201: step 18380, loss = 0.71, batch loss = 0.65 (14.5 examples/sec; 0.551 sec/batch; 48h:06m:10s remains)
INFO - root - 2017-12-11 05:15:04.860247: step 18390, loss = 0.70, batch loss = 0.64 (14.5 examples/sec; 0.552 sec/batch; 48h:12m:07s remains)
INFO - root - 2017-12-11 05:15:10.405280: step 18400, loss = 0.72, batch loss = 0.66 (14.8 examples/sec; 0.541 sec/batch; 47h:12m:45s remains)
2017-12-11 05:15:11.009237: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.22423449 0.25216073 0.2747741 0.28874373 0.29606709 0.31434542 0.34124655 0.36473683 0.37050465 0.34691322 0.28856465 0.19714876 0.09708713 0.014213818 -0.04263347][0.19023933 0.21972038 0.24206713 0.25248545 0.25244468 0.25994724 0.27676073 0.2960304 0.30755141 0.30088887 0.26718146 0.20015864 0.11451253 0.033510804 -0.030575296][0.15617844 0.17817946 0.19263276 0.19516486 0.18699305 0.18534961 0.19638251 0.21770245 0.24310769 0.26012996 0.25528446 0.21358384 0.14091624 0.058984112 -0.014842748][0.14471728 0.15256418 0.15490969 0.14861676 0.13504569 0.13038903 0.14362486 0.17378989 0.21522264 0.2520667 0.26723418 0.23974289 0.17023307 0.081076816 -0.0038695757][0.15646605 0.15407939 0.15017146 0.14287129 0.13330546 0.13659137 0.16110946 0.20258458 0.25235057 0.29246989 0.30729192 0.27489454 0.19490317 0.092335716 -0.0028196489][0.17908138 0.17755544 0.18147242 0.18899095 0.19935252 0.22385752 0.26591298 0.31424591 0.35611561 0.37629169 0.36708272 0.31093952 0.20955941 0.091075815 -0.01094677][0.19287306 0.20357464 0.23112464 0.27201271 0.31967527 0.37619221 0.43634656 0.480742 0.49501967 0.47326583 0.42213637 0.33193409 0.20639348 0.07509806 -0.028142221][0.18780088 0.21840268 0.27976063 0.36396891 0.45519766 0.5434553 0.6142146 0.64283943 0.61793596 0.54630589 0.4494769 0.32591048 0.18152668 0.045362156 -0.052108623][0.16845821 0.21973677 0.31228676 0.43246213 0.55456936 0.65860426 0.72524792 0.73108351 0.67148441 0.56326735 0.43677884 0.29494953 0.14473796 0.01316864 -0.074061707][0.14159064 0.20503563 0.31308693 0.44684047 0.57399172 0.670313 0.71844232 0.70238674 0.62504631 0.5069043 0.37825605 0.24161413 0.10258528 -0.01499962 -0.089753963][0.11051686 0.17261064 0.27424306 0.3937605 0.49857518 0.56758022 0.59061795 0.561635 0.48860696 0.38921145 0.28580996 0.17594297 0.061890095 -0.035719033 -0.098106235][0.075488836 0.12377306 0.20092405 0.2857981 0.35217971 0.38752115 0.39005351 0.3617551 0.311651 0.24991331 0.18698442 0.1140008 0.030059198 -0.046962008 -0.099113859][0.037211359 0.065457605 0.11036599 0.15386581 0.18021445 0.18758377 0.18055964 0.16486849 0.14688496 0.1274237 0.10509218 0.065984331 0.0080998614 -0.052142922 -0.096772894][-0.0019347764 0.0077950861 0.024738731 0.034936484 0.032425836 0.024003526 0.017843103 0.019929336 0.032003716 0.046816815 0.05441514 0.038453434 -0.0025564472 -0.052589249 -0.093336031][-0.038481731 -0.03981274 -0.038110573 -0.043876186 -0.057304043 -0.067677654 -0.066641614 -0.049876742 -0.01969742 0.012467681 0.033295151 0.02691786 -0.0068390802 -0.052281536 -0.091374807]]...]
INFO - root - 2017-12-11 05:15:16.534321: step 18410, loss = 0.69, batch loss = 0.63 (14.5 examples/sec; 0.551 sec/batch; 48h:02m:33s remains)
INFO - root - 2017-12-11 05:15:21.676758: step 18420, loss = 0.71, batch loss = 0.65 (14.8 examples/sec; 0.541 sec/batch; 47h:12m:22s remains)
INFO - root - 2017-12-11 05:15:27.238689: step 18430, loss = 0.70, batch loss = 0.64 (14.2 examples/sec; 0.565 sec/batch; 49h:18m:25s remains)
INFO - root - 2017-12-11 05:15:32.719323: step 18440, loss = 0.69, batch loss = 0.63 (14.5 examples/sec; 0.552 sec/batch; 48h:11m:21s remains)
INFO - root - 2017-12-11 05:15:38.269105: step 18450, loss = 0.70, batch loss = 0.64 (14.9 examples/sec; 0.536 sec/batch; 46h:45m:46s remains)
INFO - root - 2017-12-11 05:15:43.867757: step 18460, loss = 0.69, batch loss = 0.63 (14.6 examples/sec; 0.547 sec/batch; 47h:41m:38s remains)
INFO - root - 2017-12-11 05:15:49.359566: step 18470, loss = 0.69, batch loss = 0.63 (14.3 examples/sec; 0.558 sec/batch; 48h:38m:52s remains)
INFO - root - 2017-12-11 05:15:54.874387: step 18480, loss = 0.70, batch loss = 0.64 (14.2 examples/sec; 0.563 sec/batch; 49h:09m:08s remains)
INFO - root - 2017-12-11 05:16:00.418588: step 18490, loss = 0.70, batch loss = 0.64 (14.4 examples/sec; 0.555 sec/batch; 48h:25m:50s remains)
INFO - root - 2017-12-11 05:16:05.969562: step 18500, loss = 0.69, batch loss = 0.63 (14.8 examples/sec; 0.539 sec/batch; 47h:00m:38s remains)
2017-12-11 05:16:06.627211: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0025074026 -0.0082348911 -0.014582684 -0.02094695 -0.027181249 -0.032522395 -0.036117 -0.037193187 -0.035834238 -0.032826472 -0.029122472 -0.022138642 -0.00803689 0.01007977 0.03127566][0.0034744511 0.0012987157 -0.0016589185 -0.0057157595 -0.01185131 -0.019535758 -0.027213052 -0.032785024 -0.035447475 -0.035215966 -0.032888923 -0.025928557 -0.010646967 0.0092381025 0.032238513][0.01673531 0.023778008 0.029712213 0.03256461 0.029590383 0.020620272 0.0079654614 -0.0042848741 -0.013827834 -0.019638646 -0.021985559 -0.01833735 -0.0051250523 0.0136588 0.036579348][0.036380406 0.058200862 0.078675695 0.093268409 0.097221442 0.08930324 0.072349779 0.052453693 0.033592958 0.018077122 0.0063357824 0.0016666108 0.0083173951 0.022406755 0.042723663][0.06237866 0.10276502 0.14175397 0.17193006 0.18682641 0.18385933 0.16557926 0.13913505 0.1096881 0.081000581 0.054773364 0.036070123 0.030822232 0.036100652 0.050847065][0.091828682 0.14975378 0.20675257 0.25315827 0.28194663 0.28899926 0.27485329 0.24551013 0.20655876 0.16297959 0.11852617 0.081700064 0.061170902 0.054989047 0.061891157][0.1196835 0.18898202 0.25815198 0.31703341 0.3593083 0.37959698 0.37430874 0.34588978 0.29975265 0.2419856 0.17941423 0.12463596 0.089607947 0.073045813 0.072614707][0.13909064 0.21053798 0.28269064 0.34677875 0.39795247 0.42965186 0.43358016 0.40801224 0.35786885 0.29017669 0.21501252 0.14857727 0.10523895 0.083038613 0.077993974][0.1433388 0.20784798 0.27363664 0.33442611 0.38691944 0.42366773 0.43281651 0.409743 0.35934702 0.28924412 0.21144064 0.14423719 0.10261444 0.082471348 0.077728979][0.12788564 0.17948279 0.23249674 0.283175 0.32934597 0.36331698 0.37210923 0.35064384 0.30420423 0.23996365 0.170176 0.11340734 0.083382912 0.072889738 0.073920965][0.094524622 0.12954342 0.16618977 0.20243308 0.23672532 0.26201394 0.26664779 0.24705641 0.20868839 0.15718356 0.1035378 0.065266632 0.053595137 0.058141455 0.069509938][0.050968982 0.0680142 0.087575093 0.10850364 0.12935463 0.14441462 0.14487836 0.12908855 0.10182104 0.0666436 0.032678772 0.01539609 0.023715694 0.045172174 0.069703095][0.013135884 0.012812707 0.016373375 0.023600513 0.032730944 0.039640494 0.03873007 0.029309053 0.014350852 -0.0044961749 -0.020041142 -0.01853724 0.007898598 0.045992181 0.085122228][0.0018718644 -0.013291711 -0.024029771 -0.028973604 -0.030463243 -0.030418759 -0.031559598 -0.034648288 -0.03911892 -0.044761986 -0.045777865 -0.029567305 0.012314454 0.06655141 0.12127503][0.030552141 0.0053390963 -0.017730065 -0.03498948 -0.048700884 -0.057752155 -0.061836004 -0.062096287 -0.060464054 -0.057701197 -0.048908655 -0.022236826 0.031987719 0.10068932 0.16950801]]...]
INFO - root - 2017-12-11 05:16:12.152191: step 18510, loss = 0.69, batch loss = 0.64 (14.4 examples/sec; 0.556 sec/batch; 48h:31m:48s remains)
INFO - root - 2017-12-11 05:16:17.404619: step 18520, loss = 0.68, batch loss = 0.63 (14.0 examples/sec; 0.572 sec/batch; 49h:55m:41s remains)
INFO - root - 2017-12-11 05:16:22.951634: step 18530, loss = 0.72, batch loss = 0.66 (14.4 examples/sec; 0.554 sec/batch; 48h:19m:09s remains)
INFO - root - 2017-12-11 05:16:28.515778: step 18540, loss = 0.69, batch loss = 0.63 (14.3 examples/sec; 0.558 sec/batch; 48h:41m:41s remains)
INFO - root - 2017-12-11 05:16:34.032579: step 18550, loss = 0.68, batch loss = 0.63 (14.0 examples/sec; 0.573 sec/batch; 49h:56m:53s remains)
INFO - root - 2017-12-11 05:16:39.475983: step 18560, loss = 0.70, batch loss = 0.64 (14.6 examples/sec; 0.547 sec/batch; 47h:40m:03s remains)
INFO - root - 2017-12-11 05:16:44.906642: step 18570, loss = 0.70, batch loss = 0.64 (14.5 examples/sec; 0.553 sec/batch; 48h:13m:54s remains)
INFO - root - 2017-12-11 05:16:50.498028: step 18580, loss = 0.70, batch loss = 0.64 (14.4 examples/sec; 0.554 sec/batch; 48h:18m:58s remains)
INFO - root - 2017-12-11 05:16:55.883277: step 18590, loss = 0.69, batch loss = 0.63 (14.8 examples/sec; 0.542 sec/batch; 47h:14m:53s remains)
INFO - root - 2017-12-11 05:17:01.380485: step 18600, loss = 0.70, batch loss = 0.64 (13.9 examples/sec; 0.574 sec/batch; 50h:01m:04s remains)
2017-12-11 05:17:02.015446: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.1883284 0.200676 0.21438619 0.22973751 0.23353463 0.22168146 0.19996244 0.17786591 0.15966569 0.14792393 0.14511323 0.15048404 0.16066009 0.17610374 0.20774893][0.19667125 0.2169809 0.24226211 0.2713829 0.28569847 0.27790236 0.25298661 0.2223476 0.19343694 0.1722924 0.16290894 0.16561867 0.17707466 0.19470961 0.22824293][0.17025982 0.20081434 0.24165009 0.28977755 0.3225264 0.32914764 0.31138247 0.27936348 0.24203706 0.20832746 0.18473382 0.17537703 0.17996508 0.19415845 0.22445314][0.12997696 0.16779394 0.22003044 0.28382593 0.33576712 0.36262923 0.3606852 0.33522511 0.29399067 0.24694008 0.20290297 0.17184779 0.15975484 0.16371869 0.18702735][0.11509667 0.15281267 0.20353444 0.26932177 0.33003241 0.37118515 0.38349333 0.36641151 0.32542256 0.269557 0.20859465 0.15723249 0.12743865 0.12018055 0.13653095][0.14202061 0.17458655 0.21363413 0.27212882 0.33563024 0.387243 0.40991762 0.39649481 0.351941 0.28593928 0.21046394 0.14427271 0.1030891 0.090513825 0.10560615][0.22422987 0.25051615 0.27150193 0.31584224 0.37771538 0.4375149 0.46871439 0.4553611 0.40231511 0.32223538 0.23230672 0.15573724 0.10954358 0.097460009 0.11572735][0.34275717 0.36042371 0.35764605 0.37920135 0.42942563 0.48878583 0.52366042 0.51032406 0.45209521 0.36415762 0.26920205 0.19371873 0.15285306 0.14721847 0.17087968][0.45702222 0.46474287 0.43718326 0.43079075 0.45933327 0.50747693 0.54066241 0.53125721 0.47878891 0.39771485 0.3132861 0.25273412 0.22697541 0.23219241 0.26165971][0.52268755 0.52085435 0.47618157 0.44725898 0.45305195 0.48332784 0.50824171 0.50196558 0.46186703 0.39944908 0.3379654 0.30161279 0.29620308 0.31360623 0.34741744][0.525999 0.51680505 0.46472877 0.42326307 0.411459 0.42300123 0.43500721 0.42762709 0.39846894 0.35604322 0.31875759 0.30528003 0.31694102 0.34304675 0.37808535][0.48383242 0.46669877 0.41135189 0.36205226 0.33584148 0.32916358 0.32668898 0.31591335 0.29524741 0.2699627 0.25216517 0.25433719 0.27464864 0.30215955 0.33330426][0.39266792 0.36896446 0.31354856 0.26133895 0.22628635 0.20698024 0.19450587 0.18264776 0.16991182 0.15860218 0.15492915 0.16604365 0.18858989 0.21331351 0.23933128][0.27009815 0.24331777 0.19417328 0.14676835 0.1118933 0.088989019 0.073770739 0.063854389 0.05756136 0.055076346 0.058536857 0.071416572 0.0904115 0.10962694 0.13086034][0.15937817 0.13187586 0.091678552 0.053501021 0.025173439 0.0056870617 -0.0075684474 -0.015168742 -0.018741926 -0.01873127 -0.014768128 -0.0056566736 0.00613139 0.018448759 0.035246596]]...]
INFO - root - 2017-12-11 05:17:07.145641: step 18610, loss = 0.71, batch loss = 0.65 (21.9 examples/sec; 0.366 sec/batch; 31h:52m:18s remains)
INFO - root - 2017-12-11 05:17:12.600541: step 18620, loss = 0.68, batch loss = 0.62 (14.6 examples/sec; 0.547 sec/batch; 47h:44m:07s remains)
INFO - root - 2017-12-11 05:17:18.174373: step 18630, loss = 0.69, batch loss = 0.63 (14.0 examples/sec; 0.570 sec/batch; 49h:40m:53s remains)
INFO - root - 2017-12-11 05:17:23.725456: step 18640, loss = 0.70, batch loss = 0.65 (14.7 examples/sec; 0.544 sec/batch; 47h:25m:41s remains)
INFO - root - 2017-12-11 05:17:29.241642: step 18650, loss = 0.71, batch loss = 0.65 (14.3 examples/sec; 0.560 sec/batch; 48h:51m:44s remains)
INFO - root - 2017-12-11 05:17:34.756030: step 18660, loss = 0.71, batch loss = 0.65 (14.5 examples/sec; 0.552 sec/batch; 48h:09m:23s remains)
INFO - root - 2017-12-11 05:17:40.219022: step 18670, loss = 0.69, batch loss = 0.63 (14.3 examples/sec; 0.558 sec/batch; 48h:39m:10s remains)
INFO - root - 2017-12-11 05:17:45.696876: step 18680, loss = 0.70, batch loss = 0.64 (14.4 examples/sec; 0.556 sec/batch; 48h:27m:31s remains)
INFO - root - 2017-12-11 05:17:51.211727: step 18690, loss = 0.69, batch loss = 0.63 (13.9 examples/sec; 0.575 sec/batch; 50h:07m:24s remains)
INFO - root - 2017-12-11 05:17:56.799344: step 18700, loss = 0.71, batch loss = 0.65 (14.3 examples/sec; 0.558 sec/batch; 48h:37m:50s remains)
2017-12-11 05:17:57.366808: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.32251206 0.35672581 0.36636904 0.36173108 0.33793229 0.29168531 0.22921799 0.17781101 0.15520772 0.1571954 0.17182919 0.20413268 0.25096083 0.28740296 0.30763039][0.3846857 0.41751528 0.42199984 0.41619986 0.39287037 0.34560844 0.27839062 0.21429646 0.17212737 0.15384434 0.15495448 0.18273768 0.23448193 0.28428549 0.32149792][0.42865041 0.46042788 0.45957419 0.4517383 0.43030769 0.38743454 0.32302123 0.25403681 0.1978078 0.16078046 0.14573075 0.16547501 0.21916701 0.27986616 0.33152157][0.45182991 0.48525119 0.48228747 0.47316226 0.45419228 0.41883022 0.36369655 0.29875031 0.23736487 0.18778701 0.15785608 0.1663309 0.21584217 0.28042361 0.33918488][0.44696468 0.47951519 0.4755699 0.46623248 0.45146656 0.42768252 0.38976535 0.34095716 0.28850269 0.23812889 0.19876581 0.19374618 0.22982728 0.28471586 0.3375124][0.41332552 0.44118717 0.43665114 0.42993832 0.42446205 0.41994381 0.4090187 0.3874191 0.35579708 0.31381214 0.26862642 0.24627441 0.25833106 0.29012141 0.32556549][0.3630904 0.38348746 0.37934682 0.3803246 0.3910915 0.41192335 0.43302235 0.44380143 0.43794912 0.40731025 0.3559323 0.31202051 0.29269332 0.29328144 0.30469814][0.31067276 0.3223694 0.31974593 0.33126494 0.36017558 0.4054006 0.45626527 0.49743247 0.51613283 0.49613377 0.43862197 0.37419125 0.32470316 0.29486945 0.28184852][0.26560098 0.26964208 0.26841629 0.28904885 0.33084163 0.39045253 0.45900345 0.52000117 0.55612797 0.54502255 0.48663113 0.41298506 0.34621829 0.29593304 0.26331702][0.22491972 0.22157119 0.22002688 0.24456748 0.28990731 0.35101187 0.42117658 0.48603573 0.527178 0.52058947 0.46678114 0.39734927 0.33062664 0.27575248 0.23514265][0.17411695 0.16378361 0.15891431 0.17987588 0.21832931 0.26922652 0.32867122 0.38574281 0.42305174 0.418177 0.3730655 0.31653339 0.26172438 0.21484913 0.17812425][0.10805529 0.09274555 0.084406413 0.097950242 0.12430029 0.15974577 0.20289026 0.24603279 0.27425858 0.26912665 0.23394765 0.19288471 0.15431444 0.1211204 0.094665259][0.036031976 0.019508922 0.01023603 0.017009452 0.031507339 0.051750381 0.077501528 0.10379159 0.11996616 0.11318444 0.087636769 0.061519902 0.03942991 0.021732029 0.0087531349][-0.018834572 -0.03437205 -0.042467434 -0.039701175 -0.033141613 -0.023989858 -0.012631548 -0.0020552583 0.0018085407 -0.0077054738 -0.025670694 -0.039722189 -0.048265107 -0.052838918 -0.053938869][-0.049828161 -0.0633303 -0.06972339 -0.069335163 -0.067225546 -0.064231373 -0.061391458 -0.060601495 -0.064256333 -0.074468754 -0.086714312 -0.093542837 -0.094825514 -0.092568547 -0.0876173]]...]
INFO - root - 2017-12-11 05:18:02.499176: step 18710, loss = 0.71, batch loss = 0.65 (14.5 examples/sec; 0.553 sec/batch; 48h:11m:45s remains)
INFO - root - 2017-12-11 05:18:07.948418: step 18720, loss = 0.71, batch loss = 0.65 (14.7 examples/sec; 0.543 sec/batch; 47h:20m:00s remains)
INFO - root - 2017-12-11 05:18:13.464975: step 18730, loss = 0.70, batch loss = 0.64 (13.8 examples/sec; 0.581 sec/batch; 50h:37m:36s remains)
INFO - root - 2017-12-11 05:18:18.974698: step 18740, loss = 0.67, batch loss = 0.61 (14.7 examples/sec; 0.545 sec/batch; 47h:30m:36s remains)
INFO - root - 2017-12-11 05:18:24.454677: step 18750, loss = 0.70, batch loss = 0.64 (13.9 examples/sec; 0.577 sec/batch; 50h:17m:18s remains)
INFO - root - 2017-12-11 05:18:30.015191: step 18760, loss = 0.70, batch loss = 0.64 (14.4 examples/sec; 0.556 sec/batch; 48h:25m:02s remains)
INFO - root - 2017-12-11 05:18:35.608387: step 18770, loss = 0.70, batch loss = 0.64 (14.0 examples/sec; 0.571 sec/batch; 49h:47m:59s remains)
INFO - root - 2017-12-11 05:18:41.098172: step 18780, loss = 0.69, batch loss = 0.63 (14.7 examples/sec; 0.545 sec/batch; 47h:30m:08s remains)
INFO - root - 2017-12-11 05:18:46.619465: step 18790, loss = 0.71, batch loss = 0.66 (14.3 examples/sec; 0.560 sec/batch; 48h:50m:18s remains)
INFO - root - 2017-12-11 05:18:52.141367: step 18800, loss = 0.71, batch loss = 0.65 (14.9 examples/sec; 0.536 sec/batch; 46h:42m:32s remains)
2017-12-11 05:18:52.697883: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.15037835 0.14719047 0.15879485 0.1967835 0.24599478 0.28900576 0.31415689 0.32568631 0.32312268 0.30910915 0.2974776 0.30902943 0.34056446 0.374706 0.393177][0.15968028 0.15249588 0.15724723 0.18820915 0.23692018 0.28858894 0.32632405 0.34883153 0.3546595 0.34826568 0.34544656 0.36773968 0.40815744 0.44621083 0.46074468][0.16357465 0.15102307 0.14403059 0.15952109 0.19802305 0.25258785 0.303229 0.34015864 0.35876369 0.36359131 0.37191615 0.40446737 0.45181662 0.49100307 0.50028074][0.17744102 0.16163243 0.1452498 0.14529634 0.17017387 0.22134173 0.27838057 0.32372472 0.34970918 0.36086398 0.37482136 0.41087216 0.46021292 0.49922761 0.506299][0.20693426 0.20113255 0.19195545 0.19420923 0.21773173 0.26688355 0.32162496 0.36078554 0.37721133 0.37866414 0.38398254 0.412626 0.4586316 0.49793333 0.50682521][0.24226542 0.26148823 0.27996904 0.30574816 0.34368771 0.39526314 0.44239664 0.46457592 0.45810506 0.43762925 0.42388281 0.43742046 0.47494018 0.51275772 0.52400154][0.27075872 0.32238397 0.37820205 0.43652022 0.49534813 0.55157888 0.59089583 0.59533256 0.56523126 0.52177 0.48614585 0.48008671 0.50313652 0.53421718 0.54393369][0.28998208 0.36946177 0.45736688 0.54310161 0.61747646 0.673807 0.70304358 0.69203317 0.643512 0.58160913 0.52615529 0.50049776 0.50844496 0.532008 0.53960085][0.30617073 0.39209729 0.48667014 0.57687974 0.64858168 0.69375616 0.70940214 0.68827748 0.63229662 0.56318265 0.49738327 0.45982757 0.46013865 0.48365766 0.49776366][0.3231512 0.39584458 0.4725998 0.54379076 0.59424353 0.61714476 0.61476928 0.58576983 0.52988994 0.46327284 0.39746916 0.35750771 0.35891446 0.39097646 0.42162821][0.33049738 0.37706894 0.42241818 0.46157593 0.48306912 0.48281357 0.46561879 0.43340963 0.38401303 0.32725069 0.26966813 0.23412934 0.2400645 0.28070429 0.3264735][0.31139991 0.33166572 0.34751144 0.35794684 0.35695875 0.34361359 0.32226956 0.29454255 0.2571865 0.21516123 0.17114411 0.14349033 0.15164092 0.19271922 0.24275276][0.26590058 0.2728892 0.27431476 0.27103639 0.26106888 0.24566819 0.22740832 0.20689943 0.18101859 0.15233088 0.12186316 0.10284534 0.11142869 0.14738885 0.19316629][0.19064941 0.19365609 0.19238569 0.18704429 0.17795818 0.16698606 0.1550663 0.14231373 0.12643032 0.10881881 0.090124391 0.078892916 0.086866 0.11524066 0.15209936][0.091691494 0.0956928 0.098503456 0.097599857 0.093886189 0.088554278 0.081716351 0.0740899 0.0647978 0.054539867 0.043941446 0.037870917 0.043671459 0.062468261 0.086586945]]...]
INFO - root - 2017-12-11 05:18:57.891342: step 18810, loss = 0.70, batch loss = 0.64 (14.5 examples/sec; 0.552 sec/batch; 48h:08m:29s remains)
INFO - root - 2017-12-11 05:19:03.334366: step 18820, loss = 0.68, batch loss = 0.62 (14.7 examples/sec; 0.543 sec/batch; 47h:18m:46s remains)
INFO - root - 2017-12-11 05:19:08.800324: step 18830, loss = 0.68, batch loss = 0.63 (14.8 examples/sec; 0.539 sec/batch; 46h:58m:10s remains)
INFO - root - 2017-12-11 05:19:14.308025: step 18840, loss = 0.69, batch loss = 0.63 (14.7 examples/sec; 0.542 sec/batch; 47h:15m:27s remains)
INFO - root - 2017-12-11 05:19:19.846946: step 18850, loss = 0.69, batch loss = 0.63 (14.8 examples/sec; 0.542 sec/batch; 47h:13m:54s remains)
INFO - root - 2017-12-11 05:19:25.292493: step 18860, loss = 0.69, batch loss = 0.64 (13.9 examples/sec; 0.574 sec/batch; 50h:01m:56s remains)
INFO - root - 2017-12-11 05:19:30.680244: step 18870, loss = 0.70, batch loss = 0.64 (14.5 examples/sec; 0.550 sec/batch; 47h:54m:54s remains)
INFO - root - 2017-12-11 05:19:36.099406: step 18880, loss = 0.70, batch loss = 0.64 (14.6 examples/sec; 0.548 sec/batch; 47h:43m:51s remains)
INFO - root - 2017-12-11 05:19:41.658049: step 18890, loss = 0.69, batch loss = 0.63 (14.4 examples/sec; 0.556 sec/batch; 48h:26m:20s remains)
INFO - root - 2017-12-11 05:19:47.168144: step 18900, loss = 0.69, batch loss = 0.63 (14.4 examples/sec; 0.555 sec/batch; 48h:18m:17s remains)
2017-12-11 05:19:47.789301: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.067752369 -0.06908071 -0.067674257 -0.066579565 -0.066427566 -0.066928156 -0.068460681 -0.06967064 -0.070135877 -0.07048928 -0.071052082 -0.072016649 -0.07148584 -0.070524856 -0.070810981][-0.055768967 -0.051657513 -0.044293016 -0.039080728 -0.03745731 -0.038213938 -0.041403528 -0.0448243 -0.04726921 -0.048532106 -0.049218964 -0.050322849 -0.047741912 -0.043770779 -0.042966243][-0.027592283 -0.011192941 0.009810009 0.024873547 0.030339574 0.029812261 0.024860196 0.018547352 0.012951255 0.010376206 0.010060889 0.009851614 0.01716299 0.026569152 0.028827898][0.018042905 0.058611207 0.10637709 0.14113756 0.15528473 0.15655021 0.14951652 0.13888083 0.1278622 0.1228734 0.12298365 0.12473252 0.13963172 0.15597144 0.15844102][0.075434484 0.15137574 0.23760746 0.30075148 0.328365 0.33322924 0.32414189 0.3082889 0.28978088 0.2799235 0.27866584 0.28239986 0.30591869 0.32747939 0.32567036][0.13248001 0.24596021 0.37173086 0.4639563 0.5065096 0.51714736 0.50844932 0.48905307 0.46264943 0.44434932 0.43641183 0.43743575 0.46298429 0.48040295 0.46587503][0.17299823 0.31192371 0.46301371 0.57344329 0.6258744 0.64219648 0.63653541 0.61658365 0.58337659 0.55378169 0.53420937 0.52667874 0.54466254 0.547876 0.51599842][0.18559653 0.32736525 0.47844395 0.58748221 0.63916355 0.65725052 0.65493596 0.63676846 0.60026437 0.561526 0.53065056 0.51246828 0.51683784 0.503184 0.45921993][0.17081232 0.29382086 0.42143676 0.51059371 0.55026382 0.56366152 0.56154972 0.54485011 0.50905561 0.46778187 0.43213674 0.407617 0.40127951 0.37908071 0.33645144][0.13158011 0.22264159 0.31366777 0.37275961 0.39353207 0.39744493 0.39288956 0.37818807 0.34837294 0.31335133 0.28200606 0.25892866 0.25055811 0.23275092 0.20458436][0.079753816 0.13730401 0.19197962 0.22199169 0.22465748 0.2187811 0.21100093 0.19916323 0.17900416 0.15660109 0.13657472 0.12165324 0.11968549 0.11598917 0.10872239][0.031549353 0.063053414 0.090804473 0.1000569 0.09029644 0.076981522 0.065919518 0.056426913 0.04559825 0.036652643 0.029922293 0.0262564 0.03525551 0.048355918 0.060358495][-0.0051697753 0.0109763 0.024213118 0.023871249 0.0099745244 -0.005909326 -0.018396759 -0.026296204 -0.030812688 -0.030252984 -0.027047684 -0.021324903 -0.0032887079 0.02123123 0.044127811][-0.032491542 -0.024251422 -0.016424637 -0.017570416 -0.028469063 -0.041558541 -0.052370332 -0.058735594 -0.060587656 -0.05638263 -0.049040973 -0.039105173 -0.018452134 0.0080438023 0.031596504][-0.051984943 -0.048552744 -0.042901237 -0.042074617 -0.047744714 -0.055727214 -0.062857628 -0.067069739 -0.0679782 -0.064016968 -0.057327732 -0.048522569 -0.032054715 -0.011879978 0.004875069]]...]
INFO - root - 2017-12-11 05:19:52.979920: step 18910, loss = 0.70, batch loss = 0.65 (14.5 examples/sec; 0.550 sec/batch; 47h:56m:34s remains)
INFO - root - 2017-12-11 05:19:58.513721: step 18920, loss = 0.69, batch loss = 0.63 (14.4 examples/sec; 0.555 sec/batch; 48h:21m:24s remains)
INFO - root - 2017-12-11 05:20:04.037152: step 18930, loss = 0.70, batch loss = 0.64 (14.9 examples/sec; 0.536 sec/batch; 46h:42m:29s remains)
INFO - root - 2017-12-11 05:20:09.498068: step 18940, loss = 0.68, batch loss = 0.62 (14.4 examples/sec; 0.554 sec/batch; 48h:15m:41s remains)
INFO - root - 2017-12-11 05:20:14.997548: step 18950, loss = 0.69, batch loss = 0.63 (15.0 examples/sec; 0.534 sec/batch; 46h:29m:32s remains)
INFO - root - 2017-12-11 05:20:20.527087: step 18960, loss = 0.71, batch loss = 0.65 (14.8 examples/sec; 0.541 sec/batch; 47h:06m:08s remains)
INFO - root - 2017-12-11 05:20:25.994277: step 18970, loss = 0.70, batch loss = 0.65 (15.3 examples/sec; 0.524 sec/batch; 45h:40m:22s remains)
INFO - root - 2017-12-11 05:20:31.505744: step 18980, loss = 0.70, batch loss = 0.64 (14.5 examples/sec; 0.551 sec/batch; 48h:00m:47s remains)
INFO - root - 2017-12-11 05:20:37.154307: step 18990, loss = 0.69, batch loss = 0.63 (14.1 examples/sec; 0.569 sec/batch; 49h:34m:10s remains)
INFO - root - 2017-12-11 05:20:42.393737: step 19000, loss = 0.69, batch loss = 0.63 (28.0 examples/sec; 0.286 sec/batch; 24h:54m:47s remains)
2017-12-11 05:20:42.993777: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.058263 0.11895973 0.19221331 0.25722405 0.30064344 0.31417662 0.3047131 0.28265759 0.25608382 0.22807203 0.20001118 0.1690879 0.12743258 0.074707352 0.023174249][0.042818949 0.10153566 0.17370741 0.24144904 0.2920981 0.31526875 0.31382832 0.30118072 0.28503364 0.26295742 0.2337241 0.19600631 0.14564495 0.084049046 0.024924517][0.031439051 0.090897419 0.1644256 0.23695177 0.29728556 0.33459175 0.34674421 0.34571487 0.33791181 0.3172057 0.28133053 0.23080947 0.16611271 0.092689581 0.025587449][0.034592874 0.10294621 0.18607803 0.2685096 0.34054932 0.39335603 0.42048177 0.42921346 0.4239665 0.39858705 0.3516283 0.28417078 0.19948842 0.10882147 0.030172754][0.051298916 0.13368708 0.23146376 0.32543343 0.40684697 0.47053856 0.50805414 0.52102953 0.51285297 0.47990423 0.42194837 0.33859974 0.23427968 0.12619498 0.035972595][0.072528794 0.16936287 0.28227517 0.38685822 0.47354591 0.54121095 0.5820691 0.59343535 0.57841033 0.53782523 0.47202912 0.37764084 0.25834003 0.13697194 0.03858877][0.089678079 0.19776177 0.32284278 0.43545625 0.52324724 0.58829677 0.62583542 0.63173807 0.60924047 0.56332 0.49440104 0.39530262 0.26827741 0.13972567 0.037600573][0.096436992 0.20977691 0.34087062 0.456868 0.54160565 0.59930766 0.62981993 0.63048458 0.60400414 0.557325 0.4905673 0.3931064 0.26555195 0.13545252 0.033230808][0.091906652 0.20300286 0.33324903 0.44835189 0.52854556 0.57877922 0.60383582 0.603772 0.57840884 0.53389794 0.470697 0.37731746 0.25347912 0.12584755 0.026217423][0.07851018 0.18170698 0.30618232 0.41905269 0.49793321 0.54632169 0.57122356 0.57397813 0.55050278 0.50533241 0.44120219 0.34962413 0.23096293 0.1096618 0.016507326][0.063657679 0.15755188 0.27566209 0.38787434 0.46969178 0.52091223 0.54752374 0.55098051 0.52408218 0.47164503 0.40103036 0.30904505 0.19744153 0.087087683 0.0046791155][0.050992221 0.13633646 0.24779002 0.35808137 0.44167337 0.49360502 0.51740736 0.51459157 0.47823036 0.41571462 0.33928069 0.25039503 0.15115017 0.057410724 -0.0097665712][0.0373128 0.11298194 0.2143838 0.31673512 0.39506549 0.44062674 0.45483047 0.44039786 0.39396331 0.32610056 0.25140694 0.17352006 0.09365917 0.022056542 -0.02677395][0.017255189 0.078180894 0.16159624 0.24609019 0.30986643 0.34255865 0.34477994 0.32113111 0.27241141 0.21004029 0.14760764 0.088885032 0.03334802 -0.013865907 -0.043915775][-0.009549275 0.030699151 0.087907694 0.14583555 0.18821612 0.2060561 0.20003939 0.17554258 0.13535005 0.088740252 0.045853518 0.00928767 -0.022041284 -0.046388373 -0.059434231]]...]
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-SGD-lr0.001-clip50-initfromfixqian30k-fixqian
INFO - root - 2017-12-11 05:20:48.562116: step 19010, loss = 0.68, batch loss = 0.62 (14.9 examples/sec; 0.538 sec/batch; 46h:48m:50s remains)
INFO - root - 2017-12-11 05:20:54.113309: step 19020, loss = 0.69, batch loss = 0.63 (14.7 examples/sec; 0.545 sec/batch; 47h:26m:27s remains)
INFO - root - 2017-12-11 05:20:59.626616: step 19030, loss = 0.70, batch loss = 0.64 (14.5 examples/sec; 0.553 sec/batch; 48h:06m:46s remains)
INFO - root - 2017-12-11 05:21:05.231236: step 19040, loss = 0.69, batch loss = 0.64 (13.3 examples/sec; 0.603 sec/batch; 52h:29m:09s remains)
INFO - root - 2017-12-11 05:21:10.707686: step 19050, loss = 0.68, batch loss = 0.62 (14.6 examples/sec; 0.548 sec/batch; 47h:45m:23s remains)
INFO - root - 2017-12-11 05:21:16.341960: step 19060, loss = 0.69, batch loss = 0.63 (14.4 examples/sec; 0.557 sec/batch; 48h:30m:46s remains)
INFO - root - 2017-12-11 05:21:21.900764: step 19070, loss = 0.69, batch loss = 0.63 (14.7 examples/sec; 0.543 sec/batch; 47h:19m:08s remains)
INFO - root - 2017-12-11 05:21:27.398497: step 19080, loss = 0.71, batch loss = 0.65 (14.8 examples/sec; 0.541 sec/batch; 47h:06m:57s remains)
INFO - root - 2017-12-11 05:21:32.899480: step 19090, loss = 0.69, batch loss = 0.64 (14.3 examples/sec; 0.561 sec/batch; 48h:48m:07s remains)
INFO - root - 2017-12-11 05:21:38.031728: step 19100, loss = 0.70, batch loss = 0.64 (15.0 examples/sec; 0.533 sec/batch; 46h:22m:48s remains)
2017-12-11 05:21:38.643709: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.040982928 -0.036296312 -0.032768153 -0.031372167 -0.028614167 -0.022701247 -0.019230727 -0.020947646 -0.02771618 -0.039218508 -0.057751156 -0.0822197 -0.10406928 -0.1165874 -0.1201653][0.024202419 0.038769349 0.048320286 0.054394875 0.062602818 0.077294953 0.089259163 0.093871273 0.088087924 0.071146578 0.038108025 -0.00932085 -0.056299351 -0.09053462 -0.10889292][0.12557498 0.15530604 0.17372581 0.18497993 0.19762419 0.22112252 0.24296892 0.25566077 0.25043431 0.2249455 0.17295682 0.097414844 0.018872377 -0.044059295 -0.08294294][0.25487709 0.30600119 0.33697841 0.35423726 0.36809933 0.39454186 0.41967633 0.43444064 0.42437521 0.38561252 0.31241366 0.20908003 0.09985929 0.007870934 -0.052581675][0.39378121 0.47479111 0.52608246 0.55394953 0.56669182 0.58703285 0.6034776 0.60863227 0.58386016 0.52537441 0.43011361 0.3033475 0.16941151 0.053223751 -0.025512177][0.52415466 0.64000493 0.71720409 0.75930536 0.76853329 0.7733 0.76951617 0.75434881 0.70720053 0.62533188 0.50966 0.36575234 0.21487433 0.082280368 -0.0083991243][0.62847936 0.77797014 0.88095289 0.93612832 0.938467 0.92094356 0.88968313 0.84787422 0.77565944 0.67180836 0.54116166 0.38796389 0.22902855 0.089238115 -0.0058133397][0.678109 0.84825581 0.96613961 1.0261546 1.0182209 0.97881883 0.92336321 0.8599456 0.7712338 0.65626639 0.52159894 0.36902541 0.21151459 0.074029557 -0.017761994][0.65502149 0.82387972 0.93810177 0.99077559 0.9711104 0.91673392 0.84938532 0.77921623 0.69053954 0.58082956 0.45561665 0.31452468 0.16847166 0.04229166 -0.0397269][0.55245644 0.69655353 0.78863662 0.82322818 0.79334819 0.73539543 0.67258751 0.61288983 0.54075056 0.45096824 0.34690845 0.22748026 0.10289119 -0.0029321595 -0.068698309][0.38713241 0.48705855 0.54254919 0.55143094 0.51366889 0.46220168 0.41678533 0.38020468 0.33649507 0.27797192 0.2054185 0.11764228 0.024427416 -0.053094674 -0.097806253][0.19712918 0.24723922 0.26466003 0.24976851 0.209027 0.16946298 0.14490086 0.13337745 0.11978234 0.095081046 0.057597596 0.0061457828 -0.050796479 -0.0969647 -0.12014074][0.026275139 0.035741307 0.025923075 -0.0020945836 -0.039012849 -0.065937318 -0.07460665 -0.069092043 -0.061546791 -0.060506456 -0.069011286 -0.087908112 -0.11120388 -0.12876038 -0.1336305][-0.090935931 -0.10365307 -0.12430818 -0.15292405 -0.18125853 -0.1982307 -0.19948195 -0.18856126 -0.17327824 -0.15985636 -0.15125206 -0.1480622 -0.14725089 -0.14441657 -0.13709892][-0.14364961 -0.16147092 -0.17987552 -0.20098658 -0.21933424 -0.22932713 -0.22885124 -0.21990058 -0.20617622 -0.19112182 -0.17690772 -0.16430287 -0.1527423 -0.14145985 -0.13029552]]...]
INFO - root - 2017-12-11 05:21:44.104947: step 19110, loss = 0.69, batch loss = 0.63 (14.8 examples/sec; 0.540 sec/batch; 47h:01m:20s remains)
INFO - root - 2017-12-11 05:21:49.568147: step 19120, loss = 0.70, batch loss = 0.64 (14.6 examples/sec; 0.546 sec/batch; 47h:32m:43s remains)
INFO - root - 2017-12-11 05:21:54.988932: step 19130, loss = 0.70, batch loss = 0.64 (14.4 examples/sec; 0.554 sec/batch; 48h:12m:43s remains)
INFO - root - 2017-12-11 05:22:00.501449: step 19140, loss = 0.70, batch loss = 0.64 (14.7 examples/sec; 0.543 sec/batch; 47h:13m:28s remains)
INFO - root - 2017-12-11 05:22:06.041314: step 19150, loss = 0.70, batch loss = 0.64 (14.2 examples/sec; 0.563 sec/batch; 49h:00m:39s remains)
INFO - root - 2017-12-11 05:22:11.565617: step 19160, loss = 0.69, batch loss = 0.63 (14.6 examples/sec; 0.548 sec/batch; 47h:43m:43s remains)
INFO - root - 2017-12-11 05:22:17.127805: step 19170, loss = 0.70, batch loss = 0.65 (14.5 examples/sec; 0.553 sec/batch; 48h:05m:26s remains)
INFO - root - 2017-12-11 05:22:22.643299: step 19180, loss = 0.72, batch loss = 0.66 (14.6 examples/sec; 0.548 sec/batch; 47h:43m:10s remains)
INFO - root - 2017-12-11 05:22:28.116903: step 19190, loss = 0.71, batch loss = 0.65 (13.9 examples/sec; 0.574 sec/batch; 49h:54m:45s remains)
INFO - root - 2017-12-11 05:22:33.331839: step 19200, loss = 0.68, batch loss = 0.62 (14.7 examples/sec; 0.544 sec/batch; 47h:18m:03s remains)
2017-12-11 05:22:33.955728: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.025929045 -0.02673889 -0.028774239 -0.031366218 -0.034813952 -0.039434839 -0.043279268 -0.041398183 -0.0345872 -0.030027924 -0.033220049 -0.043826446 -0.055010762 -0.061485432 -0.062280886][0.0065801451 0.015821448 0.020044275 0.019509669 0.016119665 0.010390437 0.0052306303 0.0083506368 0.01802464 0.022643227 0.012608701 -0.011619655 -0.03808973 -0.057929166 -0.067539096][0.055614229 0.084308505 0.10352446 0.11001298 0.10927967 0.10359392 0.097410418 0.09996023 0.10862311 0.10808828 0.084982894 0.042022809 -0.0047059176 -0.042682815 -0.06498348][0.11138578 0.16438037 0.20344563 0.21983851 0.22330223 0.21829525 0.21284413 0.21599211 0.22194317 0.21206242 0.17147906 0.10806431 0.039901935 -0.01826793 -0.055744473][0.16537271 0.24320304 0.30340523 0.33146036 0.33984607 0.33490896 0.33299145 0.34306785 0.35223293 0.33480257 0.27537587 0.19155844 0.10188249 0.021644128 -0.035151005][0.20978467 0.3103826 0.39074531 0.43150875 0.44513893 0.43873921 0.44216359 0.4672325 0.49150193 0.47499049 0.40010157 0.2972118 0.18610626 0.081282511 -0.00028477478][0.23667382 0.35379145 0.45063969 0.50438881 0.52409422 0.51767349 0.5286476 0.57386255 0.62001592 0.61024958 0.52444106 0.40508735 0.27442786 0.14571223 0.039235085][0.24457768 0.36818042 0.47302389 0.53437239 0.55801421 0.55323893 0.57230347 0.63598734 0.70194483 0.70031136 0.60926896 0.47910926 0.33520392 0.18994215 0.066591062][0.22654153 0.34315825 0.44317281 0.50191921 0.52353114 0.51865083 0.53999013 0.60966814 0.68327177 0.68804288 0.60210109 0.47438288 0.33149618 0.18646669 0.0635874][0.18034363 0.27680069 0.35962489 0.40631041 0.42025203 0.41208333 0.42777672 0.48871174 0.5562135 0.56540918 0.49591973 0.38751242 0.26366022 0.13722393 0.0318584][0.11063413 0.17892927 0.23723093 0.26709014 0.27176026 0.25996628 0.26615992 0.30912033 0.36096323 0.37209556 0.3251003 0.24652544 0.15408747 0.059632864 -0.01610426][0.031271648 0.0711949 0.10516089 0.11947142 0.11729007 0.10434893 0.10229729 0.12570679 0.15799762 0.16719785 0.14005853 0.091091655 0.032630164 -0.025141599 -0.066712789][-0.033129644 -0.015731063 -0.0012579346 0.0010931091 -0.0056024129 -0.018604677 -0.026571393 -0.019549806 -0.0049635116 0.00014987517 -0.013069781 -0.038287029 -0.067358345 -0.0923667 -0.10409266][-0.070957541 -0.068470813 -0.067082077 -0.072015628 -0.080777176 -0.092803352 -0.1032527 -0.10604688 -0.10293099 -0.10103011 -0.10549842 -0.11417428 -0.12221468 -0.12447974 -0.11724947][-0.083674274 -0.088079683 -0.092392139 -0.099251829 -0.10715987 -0.11660367 -0.12571937 -0.13098785 -0.131948 -0.1311384 -0.13095829 -0.13105363 -0.12855162 -0.12086181 -0.1073949]]...]
INFO - root - 2017-12-11 05:22:39.502963: step 19210, loss = 0.70, batch loss = 0.64 (15.1 examples/sec; 0.529 sec/batch; 46h:02m:14s remains)
INFO - root - 2017-12-11 05:22:44.967824: step 19220, loss = 0.70, batch loss = 0.64 (14.0 examples/sec; 0.571 sec/batch; 49h:38m:51s remains)
INFO - root - 2017-12-11 05:22:50.498675: step 19230, loss = 0.68, batch loss = 0.63 (14.5 examples/sec; 0.551 sec/batch; 47h:56m:45s remains)
INFO - root - 2017-12-11 05:22:55.958599: step 19240, loss = 0.71, batch loss = 0.65 (14.5 examples/sec; 0.552 sec/batch; 48h:01m:06s remains)
INFO - root - 2017-12-11 05:23:01.411227: step 19250, loss = 0.70, batch loss = 0.64 (14.5 examples/sec; 0.551 sec/batch; 47h:58m:35s remains)
INFO - root - 2017-12-11 05:23:07.062698: step 19260, loss = 0.70, batch loss = 0.64 (13.9 examples/sec; 0.574 sec/batch; 49h:54m:29s remains)
INFO - root - 2017-12-11 05:23:12.538285: step 19270, loss = 0.69, batch loss = 0.63 (14.8 examples/sec; 0.541 sec/batch; 47h:02m:30s remains)
INFO - root - 2017-12-11 05:23:17.999918: step 19280, loss = 0.70, batch loss = 0.64 (15.1 examples/sec; 0.531 sec/batch; 46h:13m:30s remains)
INFO - root - 2017-12-11 05:23:23.527196: step 19290, loss = 0.69, batch loss = 0.63 (14.4 examples/sec; 0.556 sec/batch; 48h:22m:57s remains)
INFO - root - 2017-12-11 05:23:28.797105: step 19300, loss = 0.69, batch loss = 0.63 (14.5 examples/sec; 0.551 sec/batch; 47h:56m:39s remains)
2017-12-11 05:23:29.390351: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.020420182 0.022946151 0.020157399 0.013193647 0.003823553 -0.0048057023 -0.011354028 -0.017374769 -0.021480402 -0.021367151 -0.016674034 -0.0063831829 0.009079936 0.022273393 0.027193937][0.073810026 0.081831217 0.083917961 0.079443395 0.067612305 0.053541657 0.040803567 0.026521556 0.013488377 0.0072304946 0.0099833775 0.023440475 0.046633486 0.069531247 0.081690922][0.14007734 0.15547217 0.16418681 0.1649683 0.15550651 0.14114466 0.12528405 0.10277123 0.078165106 0.061628211 0.057696719 0.068935461 0.095006131 0.12440808 0.14391583][0.19748139 0.22235148 0.24129231 0.25301236 0.25478855 0.24980126 0.23841374 0.21176176 0.1755171 0.14444426 0.12661543 0.12687531 0.14605546 0.17438605 0.19744952][0.23225944 0.26817158 0.30102587 0.32864189 0.34716433 0.35651574 0.3537147 0.32663363 0.28111589 0.23395991 0.19731359 0.17995833 0.18542351 0.20660372 0.22868052][0.24368373 0.28962752 0.33703518 0.3817614 0.41745281 0.44153371 0.44912934 0.42627075 0.37708595 0.31685725 0.26179743 0.22493558 0.21325237 0.222797 0.23944822][0.23893535 0.29269323 0.35191715 0.4108237 0.46076906 0.49789554 0.51639587 0.50178504 0.454895 0.38762996 0.31819516 0.26366878 0.23413998 0.22847077 0.23523861][0.23297578 0.29077628 0.35555884 0.42118594 0.4790093 0.5248965 0.55267513 0.5470103 0.50570333 0.43771037 0.36184138 0.29776293 0.25554073 0.23607893 0.23168758][0.23511206 0.29249012 0.3562693 0.42077741 0.47833 0.52585375 0.55811244 0.5589717 0.52419096 0.46100631 0.38875473 0.32688305 0.2814115 0.25374886 0.23976985][0.25400236 0.30779439 0.36401621 0.41835082 0.46464205 0.50304765 0.53160185 0.53532219 0.50848418 0.45623261 0.39776865 0.34909895 0.31035164 0.28121588 0.26078939][0.28768933 0.33648619 0.38061377 0.41796562 0.44560477 0.46863151 0.48859519 0.49334747 0.4764775 0.44061476 0.40226191 0.37191328 0.34419307 0.31736043 0.29295146][0.31246924 0.35559267 0.38640696 0.40547934 0.41401035 0.42162222 0.43184245 0.43521237 0.4262844 0.40651149 0.38777742 0.37493533 0.35820547 0.33553392 0.3099955][0.30302238 0.33822432 0.35637239 0.36067662 0.35562009 0.35263741 0.35511902 0.35586149 0.35215998 0.34509835 0.34199008 0.34313166 0.33607894 0.31863654 0.29452461][0.24672605 0.27182308 0.27988255 0.27562433 0.26471081 0.257654 0.25649452 0.25607818 0.25611135 0.25726902 0.26331869 0.2720392 0.27109426 0.25767827 0.23551522][0.16288115 0.17824982 0.179881 0.1722376 0.16068374 0.1535136 0.15104163 0.14989808 0.15118632 0.15548716 0.16389644 0.17347966 0.17467025 0.16427104 0.14504737]]...]
INFO - root - 2017-12-11 05:23:34.869300: step 19310, loss = 0.70, batch loss = 0.64 (14.5 examples/sec; 0.553 sec/batch; 48h:07m:59s remains)
INFO - root - 2017-12-11 05:23:40.352474: step 19320, loss = 0.70, batch loss = 0.65 (14.5 examples/sec; 0.551 sec/batch; 47h:55m:13s remains)
INFO - root - 2017-12-11 05:23:45.923373: step 19330, loss = 0.69, batch loss = 0.63 (14.0 examples/sec; 0.573 sec/batch; 49h:50m:20s remains)
INFO - root - 2017-12-11 05:23:51.436235: step 19340, loss = 0.71, batch loss = 0.65 (14.2 examples/sec; 0.565 sec/batch; 49h:08m:49s remains)
INFO - root - 2017-12-11 05:23:56.904257: step 19350, loss = 0.71, batch loss = 0.65 (14.6 examples/sec; 0.548 sec/batch; 47h:39m:06s remains)
INFO - root - 2017-12-11 05:24:02.430272: step 19360, loss = 0.70, batch loss = 0.64 (14.2 examples/sec; 0.563 sec/batch; 48h:58m:59s remains)
INFO - root - 2017-12-11 05:24:08.024477: step 19370, loss = 0.71, batch loss = 0.65 (14.6 examples/sec; 0.548 sec/batch; 47h:37m:24s remains)
INFO - root - 2017-12-11 05:24:13.443524: step 19380, loss = 0.70, batch loss = 0.64 (14.2 examples/sec; 0.564 sec/batch; 49h:05m:35s remains)
INFO - root - 2017-12-11 05:24:18.705817: step 19390, loss = 0.70, batch loss = 0.64 (13.3 examples/sec; 0.603 sec/batch; 52h:26m:52s remains)
INFO - root - 2017-12-11 05:24:24.249468: step 19400, loss = 0.68, batch loss = 0.63 (14.3 examples/sec; 0.561 sec/batch; 48h:46m:33s remains)
2017-12-11 05:24:24.852873: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.16186635 0.17348175 0.17102107 0.1663375 0.17075424 0.18799381 0.21020335 0.22911134 0.24398477 0.25392264 0.25420341 0.24555029 0.23296958 0.22385748 0.22175169][0.19455947 0.20551039 0.1982899 0.18653853 0.18205097 0.19010499 0.20372736 0.21495393 0.22451659 0.23353529 0.23802315 0.23735994 0.2340951 0.23494746 0.24471489][0.22014475 0.22986561 0.21904215 0.20325224 0.19410492 0.19667104 0.20368919 0.206872 0.20866676 0.21293643 0.21725395 0.21957847 0.21963011 0.22326829 0.23637274][0.2351882 0.24693833 0.2401056 0.2310373 0.22900449 0.23640828 0.24375801 0.24155152 0.2337759 0.22760765 0.22306983 0.2175927 0.2090289 0.20305163 0.207425][0.23788285 0.25599155 0.26257583 0.27247909 0.29056495 0.31391725 0.32923695 0.32513478 0.30671158 0.2846503 0.26191819 0.23637174 0.20672856 0.18021329 0.16636243][0.22492845 0.25121844 0.27588892 0.31115448 0.35714877 0.40375364 0.43326461 0.43159994 0.4037599 0.36303407 0.31576216 0.26320082 0.20774625 0.15938658 0.12796466][0.20294772 0.23632605 0.27604669 0.33237463 0.40267122 0.4708333 0.51536673 0.51874685 0.4843488 0.42671674 0.35612038 0.27934194 0.20353502 0.14150494 0.10130778][0.17453948 0.21036702 0.25603777 0.32030725 0.40062851 0.47913909 0.53306252 0.54174578 0.50536436 0.43812948 0.3540338 0.26523072 0.18237418 0.11918682 0.08097028][0.13545552 0.16797727 0.20951243 0.2670345 0.340273 0.41416848 0.46812442 0.48024574 0.44685271 0.38003659 0.29591316 0.21005711 0.13447672 0.081618294 0.054050654][0.082663141 0.10673888 0.13670519 0.17703588 0.23001759 0.28601602 0.32982668 0.34160689 0.31476751 0.25774226 0.18600947 0.11584967 0.05866041 0.024256822 0.012462716][0.023834774 0.036706224 0.052333843 0.072428182 0.10070304 0.13291886 0.16055681 0.1685756 0.14947593 0.10769626 0.055805705 0.0084171621 -0.025316028 -0.038903683 -0.034801815][-0.023928799 -0.021128295 -0.017520502 -0.013728238 -0.0060317232 0.0050308127 0.016217373 0.018244557 0.0050851009 -0.021395074 -0.052440319 -0.077307127 -0.089994371 -0.087219805 -0.072685353][-0.054796208 -0.058840364 -0.062402211 -0.067721762 -0.071926154 -0.073928565 -0.074163914 -0.077213369 -0.086436793 -0.10091851 -0.11552352 -0.12397499 -0.12326997 -0.11242147 -0.095070653][-0.070891812 -0.078101233 -0.084069185 -0.091941506 -0.09981484 -0.10640113 -0.11125905 -0.11618154 -0.12235964 -0.12914164 -0.13405237 -0.1342468 -0.12857823 -0.11701312 -0.10217844][-0.074299887 -0.081598558 -0.086768582 -0.09344127 -0.10045806 -0.10675176 -0.11173788 -0.1159109 -0.11944471 -0.1219441 -0.12254015 -0.12028986 -0.11481246 -0.10618489 -0.095882155]]...]
INFO - root - 2017-12-11 05:24:30.406344: step 19410, loss = 0.69, batch loss = 0.63 (14.4 examples/sec; 0.556 sec/batch; 48h:20m:00s remains)
INFO - root - 2017-12-11 05:24:35.955092: step 19420, loss = 0.71, batch loss = 0.65 (14.0 examples/sec; 0.572 sec/batch; 49h:43m:20s remains)
INFO - root - 2017-12-11 05:24:41.455185: step 19430, loss = 0.70, batch loss = 0.64 (14.4 examples/sec; 0.555 sec/batch; 48h:17m:49s remains)
INFO - root - 2017-12-11 05:24:47.004971: step 19440, loss = 0.70, batch loss = 0.64 (14.7 examples/sec; 0.543 sec/batch; 47h:12m:40s remains)
INFO - root - 2017-12-11 05:24:52.523068: step 19450, loss = 0.71, batch loss = 0.65 (14.7 examples/sec; 0.543 sec/batch; 47h:14m:33s remains)
INFO - root - 2017-12-11 05:24:57.997960: step 19460, loss = 0.68, batch loss = 0.63 (14.3 examples/sec; 0.560 sec/batch; 48h:43m:57s remains)
INFO - root - 2017-12-11 05:25:03.545152: step 19470, loss = 0.69, batch loss = 0.63 (14.1 examples/sec; 0.568 sec/batch; 49h:23m:38s remains)
INFO - root - 2017-12-11 05:25:08.995034: step 19480, loss = 0.69, batch loss = 0.63 (14.5 examples/sec; 0.552 sec/batch; 47h:58m:47s remains)
INFO - root - 2017-12-11 05:25:14.126961: step 19490, loss = 0.71, batch loss = 0.65 (14.1 examples/sec; 0.567 sec/batch; 49h:18m:20s remains)
INFO - root - 2017-12-11 05:25:19.634560: step 19500, loss = 0.69, batch loss = 0.63 (14.6 examples/sec; 0.550 sec/batch; 47h:46m:43s remains)
2017-12-11 05:25:20.325353: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0635134 -0.063941963 -0.060248628 -0.055915255 -0.053025987 -0.052763812 -0.054589327 -0.056840539 -0.058324419 -0.058694888 -0.058142465 -0.057302851 -0.056672405 -0.056289975 -0.055724017][-0.055746149 -0.050452463 -0.041091226 -0.032915805 -0.029664287 -0.032762975 -0.040354386 -0.049134605 -0.056793503 -0.061744221 -0.063606031 -0.063206181 -0.061982546 -0.060845029 -0.059772179][-0.02967702 -0.016121369 0.0015231229 0.016515389 0.023134278 0.018943736 0.0063815177 -0.010672431 -0.028995411 -0.044862926 -0.055686332 -0.061204851 -0.063306481 -0.063716896 -0.063128948][0.021563625 0.043640543 0.070111774 0.094160363 0.10801868 0.10708587 0.092747793 0.067768469 0.035676278 0.0032146608 -0.023282425 -0.041154657 -0.052086778 -0.058439087 -0.061326623][0.096818194 0.12649134 0.16077812 0.19469242 0.21863665 0.2249594 0.21278466 0.18241836 0.13680924 0.085591584 0.039526224 0.0044766888 -0.020320836 -0.037352059 -0.047940698][0.185314 0.22014435 0.25864261 0.29929253 0.33169717 0.34600624 0.33933222 0.30896756 0.25570387 0.19030397 0.12667125 0.073516153 0.031735864 0.00012239839 -0.022087526][0.26739967 0.30404255 0.34104779 0.38112924 0.41476679 0.43261665 0.43173933 0.40699109 0.35566407 0.28645483 0.21358609 0.14733329 0.090585016 0.044497043 0.0095215][0.31697431 0.35219264 0.38243484 0.41325569 0.43776485 0.45033821 0.45060539 0.43295974 0.39134035 0.33006567 0.26083022 0.19307524 0.13057825 0.076591156 0.033069819][0.30936095 0.34075966 0.36202389 0.37889487 0.38750461 0.38720232 0.38180318 0.36771134 0.33754632 0.29149538 0.23719232 0.18121177 0.12648994 0.076841325 0.03491291][0.23848894 0.26363331 0.27633217 0.28017226 0.27357256 0.26001009 0.24679005 0.23329419 0.21318246 0.18417162 0.14993562 0.11352883 0.076173887 0.041126456 0.010635789][0.12869777 0.144722 0.14962183 0.14392145 0.1274671 0.10592357 0.088053875 0.075643882 0.064553581 0.051491085 0.0369143 0.020984536 0.0036090165 -0.013093431 -0.027850149][0.017985314 0.024042103 0.022864355 0.012406695 -0.0066954987 -0.028733075 -0.045795988 -0.055195987 -0.058965497 -0.060215421 -0.060227711 -0.060183894 -0.060874112 -0.061579458 -0.062216945][-0.061770942 -0.06368082 -0.068320461 -0.079004735 -0.094728172 -0.11115497 -0.12282036 -0.12769347 -0.1267028 -0.12208059 -0.11522614 -0.1071581 -0.098635592 -0.089849874 -0.081710622][-0.0970623 -0.10320102 -0.10870735 -0.11692361 -0.12662227 -0.13524015 -0.14004527 -0.14031364 -0.13679868 -0.13103321 -0.12375666 -0.11507976 -0.10505498 -0.094109654 -0.083525494][-0.095608346 -0.10249755 -0.1073674 -0.11279561 -0.11738884 -0.11990492 -0.11949512 -0.11644019 -0.11181378 -0.10704024 -0.10227034 -0.096714087 -0.089697391 -0.081442751 -0.0730186]]...]
INFO - root - 2017-12-11 05:25:25.765816: step 19510, loss = 0.70, batch loss = 0.64 (15.2 examples/sec; 0.525 sec/batch; 45h:37m:30s remains)
INFO - root - 2017-12-11 05:25:31.376996: step 19520, loss = 0.70, batch loss = 0.65 (14.3 examples/sec; 0.559 sec/batch; 48h:36m:39s remains)
INFO - root - 2017-12-11 05:25:36.893713: step 19530, loss = 0.69, batch loss = 0.64 (14.8 examples/sec; 0.540 sec/batch; 46h:58m:55s remains)
INFO - root - 2017-12-11 05:25:42.377050: step 19540, loss = 0.71, batch loss = 0.65 (14.7 examples/sec; 0.544 sec/batch; 47h:15m:04s remains)
INFO - root - 2017-12-11 05:25:47.946192: step 19550, loss = 0.71, batch loss = 0.65 (13.8 examples/sec; 0.581 sec/batch; 50h:30m:47s remains)
INFO - root - 2017-12-11 05:25:53.434093: step 19560, loss = 0.68, batch loss = 0.63 (14.9 examples/sec; 0.539 sec/batch; 46h:48m:58s remains)
INFO - root - 2017-12-11 05:25:58.937746: step 19570, loss = 0.69, batch loss = 0.63 (14.6 examples/sec; 0.548 sec/batch; 47h:38m:32s remains)
INFO - root - 2017-12-11 05:26:04.449114: step 19580, loss = 0.69, batch loss = 0.64 (14.2 examples/sec; 0.565 sec/batch; 49h:04m:25s remains)
INFO - root - 2017-12-11 05:26:09.551996: step 19590, loss = 0.70, batch loss = 0.64 (15.2 examples/sec; 0.527 sec/batch; 45h:50m:41s remains)
INFO - root - 2017-12-11 05:26:14.996108: step 19600, loss = 0.69, batch loss = 0.63 (14.7 examples/sec; 0.543 sec/batch; 47h:11m:56s remains)
2017-12-11 05:26:15.581680: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.11752853 0.08303383 0.056455243 0.045043696 0.04504833 0.046855342 0.046169057 0.041858021 0.031764381 0.01460826 -0.0066275382 -0.022045735 -0.023207547 -0.0097074509 0.01293538][0.10863598 0.065559223 0.029240318 0.0077109151 -0.0016545211 -0.0047486308 -0.0027005463 0.0034101049 0.0071083414 0.0026265078 -0.010994727 -0.023610936 -0.021432839 -0.0029537776 0.024763441][0.099893548 0.057343565 0.019818131 -0.0049171248 -0.018270211 -0.022089168 -0.01372876 0.0051523251 0.023599271 0.029981121 0.021107692 0.0079239011 0.010448266 0.030628527 0.059548244][0.096588492 0.062612556 0.032121636 0.012017215 0.0012082749 0.00058863644 0.016295793 0.047066696 0.078689992 0.093157277 0.085649021 0.06790325 0.065356225 0.0803234 0.10380024][0.093659408 0.075975448 0.061812669 0.05637344 0.056233898 0.061762813 0.081475884 0.1170908 0.15384793 0.16925389 0.15755677 0.13123481 0.11850663 0.122028 0.13376683][0.086744539 0.0916836 0.10170241 0.1202191 0.13958073 0.15487222 0.17393947 0.20297028 0.23015371 0.23359974 0.20915079 0.17105076 0.14667483 0.13839297 0.13957927][0.07764557 0.10602821 0.14185581 0.18731485 0.2306121 0.25836843 0.27371123 0.28625226 0.2898761 0.26903388 0.22512425 0.17571956 0.14410359 0.13062342 0.12894176][0.065187685 0.11071251 0.16616358 0.23280114 0.29620934 0.33573702 0.3478741 0.34278044 0.3198989 0.2729421 0.21097142 0.15565705 0.12496355 0.11555626 0.11932194][0.047346294 0.099649265 0.16414456 0.24067286 0.31439441 0.36169127 0.37324518 0.35645416 0.31527558 0.25215435 0.18299843 0.13165419 0.11041038 0.11171801 0.12494219][0.024531106 0.07196755 0.13372657 0.20694461 0.27825567 0.32580215 0.33749154 0.31740016 0.27208629 0.20935556 0.14843039 0.11176812 0.10617603 0.12082042 0.14313264][0.0031986886 0.03769239 0.087817334 0.14842464 0.20848396 0.2509107 0.26281026 0.24561137 0.20663069 0.15572697 0.11187027 0.093686379 0.10361907 0.12946332 0.15786728][-0.0096558845 0.00929892 0.042938415 0.085397452 0.12880237 0.162246 0.17411935 0.16401088 0.13766304 0.10349668 0.077836029 0.074821934 0.094178274 0.12475469 0.15417241][-0.010048795 -0.004917725 0.012315459 0.035796043 0.060925394 0.082471862 0.092390388 0.089443535 0.076577954 0.058430333 0.047125973 0.052660976 0.0732666 0.10040196 0.12473592][0.0038425256 0.0016629563 0.00940227 0.020411177 0.031093271 0.039802741 0.043052223 0.041196015 0.03593776 0.028293716 0.025214417 0.032861669 0.048161317 0.0651371 0.079158194][0.034884159 0.034066066 0.041078821 0.047460414 0.048285138 0.043931358 0.03542785 0.027253265 0.022628862 0.020043822 0.02017517 0.025156843 0.030846268 0.033099566 0.032613382]]...]
INFO - root - 2017-12-11 05:26:21.196790: step 19610, loss = 0.68, batch loss = 0.63 (14.7 examples/sec; 0.545 sec/batch; 47h:23m:06s remains)
INFO - root - 2017-12-11 05:26:26.666873: step 19620, loss = 0.69, batch loss = 0.63 (14.7 examples/sec; 0.543 sec/batch; 47h:12m:24s remains)
INFO - root - 2017-12-11 05:26:32.196849: step 19630, loss = 0.70, batch loss = 0.64 (14.3 examples/sec; 0.560 sec/batch; 48h:42m:40s remains)
INFO - root - 2017-12-11 05:26:37.757600: step 19640, loss = 0.70, batch loss = 0.64 (14.6 examples/sec; 0.546 sec/batch; 47h:27m:53s remains)
INFO - root - 2017-12-11 05:26:43.263373: step 19650, loss = 0.70, batch loss = 0.64 (14.7 examples/sec; 0.543 sec/batch; 47h:13m:02s remains)
INFO - root - 2017-12-11 05:26:48.763269: step 19660, loss = 0.68, batch loss = 0.62 (14.5 examples/sec; 0.553 sec/batch; 48h:01m:31s remains)
INFO - root - 2017-12-11 05:26:54.313226: step 19670, loss = 0.71, batch loss = 0.65 (14.3 examples/sec; 0.559 sec/batch; 48h:33m:19s remains)
INFO - root - 2017-12-11 05:26:59.457554: step 19680, loss = 0.69, batch loss = 0.63 (31.7 examples/sec; 0.253 sec/batch; 21h:57m:31s remains)
INFO - root - 2017-12-11 05:27:04.757063: step 19690, loss = 0.67, batch loss = 0.61 (14.6 examples/sec; 0.548 sec/batch; 47h:36m:04s remains)
INFO - root - 2017-12-11 05:27:10.254903: step 19700, loss = 0.68, batch loss = 0.62 (14.4 examples/sec; 0.557 sec/batch; 48h:23m:07s remains)
2017-12-11 05:27:10.857079: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.088824444 0.084992923 0.10066257 0.12548308 0.14890887 0.16660035 0.19039698 0.22720782 0.26884961 0.30388418 0.32022583 0.30717745 0.2591399 0.18473268 0.10992508][0.099084809 0.0884182 0.094875179 0.1136888 0.13818422 0.16660544 0.2113831 0.27290881 0.33421695 0.37679502 0.38898173 0.36486113 0.29999331 0.20619169 0.11307189][0.10680108 0.09158358 0.090021648 0.10373589 0.13017066 0.16901639 0.2321721 0.31207755 0.38486025 0.42726693 0.429788 0.39241767 0.31319386 0.20683381 0.10506724][0.12467568 0.11201589 0.11061283 0.1262064 0.15740091 0.20326695 0.27495405 0.35953239 0.43071225 0.46360543 0.45266593 0.402493 0.31283867 0.20081422 0.098809175][0.14761373 0.14519238 0.15343317 0.1792288 0.21801646 0.26623884 0.33353412 0.40584761 0.45955092 0.47238356 0.44496387 0.38460779 0.29156706 0.18248269 0.088616319][0.16804661 0.18109336 0.20596048 0.24812345 0.29636306 0.34343374 0.39584115 0.44188452 0.46382618 0.4470976 0.39979872 0.3316907 0.24270183 0.14563827 0.06774693][0.1811423 0.21079844 0.25491345 0.3152273 0.3735778 0.41830543 0.45256895 0.46793631 0.45286852 0.40282124 0.33375 0.259012 0.17821904 0.098513693 0.040806588][0.18187413 0.22370233 0.28291157 0.35685104 0.42258313 0.46597391 0.48714656 0.47920546 0.43491313 0.35821152 0.27135897 0.190908 0.11795665 0.0552391 0.016622681][0.16618522 0.2118883 0.27684534 0.35521108 0.42256323 0.46414363 0.47861877 0.46035078 0.40328756 0.31627065 0.22363353 0.14343189 0.078430913 0.029753327 0.0050938036][0.1342262 0.17500435 0.23586765 0.30881494 0.37138969 0.40939629 0.42118615 0.40209895 0.34679568 0.26631141 0.1824863 0.11235311 0.05959633 0.024718415 0.0098900609][0.090379573 0.12011243 0.16986181 0.23076721 0.28510591 0.31977719 0.33298117 0.32054397 0.27812573 0.21654531 0.15262808 0.10056859 0.063860044 0.04209733 0.0330128][0.045631122 0.06364271 0.10007663 0.14722532 0.19284251 0.22592683 0.24418439 0.24364473 0.22067244 0.18394248 0.14470133 0.11322896 0.092118151 0.07930243 0.070538148][0.0093454672 0.019624447 0.0440666 0.077748515 0.11416889 0.1459299 0.17026462 0.18280135 0.18068862 0.16968304 0.15555961 0.14338671 0.134054 0.12467927 0.11149955][-0.013648415 -0.005783251 0.010296433 0.031952422 0.058200885 0.086363479 0.11403589 0.13687922 0.1528479 0.16480261 0.17337854 0.1780342 0.17748885 0.16885754 0.1501483][-0.026636949 -0.017515935 -0.0052106441 0.00785785 0.024607336 0.046775181 0.073680013 0.10196979 0.13029809 0.15875423 0.18400668 0.20172706 0.20910282 0.20324743 0.18338673]]...]
INFO - root - 2017-12-11 05:27:16.389586: step 19710, loss = 0.70, batch loss = 0.64 (13.7 examples/sec; 0.582 sec/batch; 50h:33m:17s remains)
INFO - root - 2017-12-11 05:27:21.950319: step 19720, loss = 0.70, batch loss = 0.64 (14.8 examples/sec; 0.541 sec/batch; 47h:02m:39s remains)
INFO - root - 2017-12-11 05:27:27.371105: step 19730, loss = 0.68, batch loss = 0.63 (14.3 examples/sec; 0.559 sec/batch; 48h:31m:43s remains)
INFO - root - 2017-12-11 05:27:32.942948: step 19740, loss = 0.70, batch loss = 0.64 (14.0 examples/sec; 0.572 sec/batch; 49h:44m:07s remains)
INFO - root - 2017-12-11 05:27:38.422485: step 19750, loss = 0.68, batch loss = 0.62 (14.8 examples/sec; 0.541 sec/batch; 46h:58m:57s remains)
INFO - root - 2017-12-11 05:27:43.939846: step 19760, loss = 0.70, batch loss = 0.64 (14.6 examples/sec; 0.547 sec/batch; 47h:32m:04s remains)
INFO - root - 2017-12-11 05:27:49.381387: step 19770, loss = 0.71, batch loss = 0.65 (14.6 examples/sec; 0.547 sec/batch; 47h:31m:46s remains)
INFO - root - 2017-12-11 05:27:54.647702: step 19780, loss = 0.72, batch loss = 0.66 (14.5 examples/sec; 0.552 sec/batch; 47h:55m:23s remains)
INFO - root - 2017-12-11 05:28:00.170137: step 19790, loss = 0.70, batch loss = 0.65 (14.0 examples/sec; 0.570 sec/batch; 49h:29m:22s remains)
INFO - root - 2017-12-11 05:28:05.726594: step 19800, loss = 0.71, batch loss = 0.65 (14.5 examples/sec; 0.551 sec/batch; 47h:52m:08s remains)
2017-12-11 05:28:06.306838: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.275218 0.23342104 0.17430903 0.11199175 0.054424372 0.0074517271 -0.027623614 -0.051814936 -0.060672414 -0.043304536 0.010490656 0.098785006 0.20349102 0.29413274 0.345757][0.28808978 0.25458583 0.20141749 0.14216623 0.086829893 0.044142667 0.016311426 -0.00016039086 -0.0045673908 0.014022049 0.069705114 0.16574959 0.28670794 0.40084031 0.477031][0.26380616 0.24635978 0.20975438 0.16522712 0.12286291 0.093131788 0.078469671 0.072297558 0.070407279 0.081728727 0.12482791 0.21072935 0.33124113 0.45722604 0.55425382][0.22023329 0.22215296 0.20712349 0.1829333 0.15902172 0.14708808 0.14947373 0.15580149 0.15514652 0.15217796 0.16911912 0.22678208 0.32786682 0.45055979 0.55983424][0.18381399 0.20882049 0.21979508 0.22094816 0.22176427 0.23554337 0.26390374 0.29020947 0.29358524 0.27064133 0.24545042 0.25111517 0.30743232 0.40587446 0.5133251][0.16058299 0.21263394 0.25550655 0.2893537 0.32388479 0.37248349 0.43420991 0.48401156 0.49004048 0.43885845 0.3557744 0.2897588 0.28269917 0.34214881 0.43687031][0.1396382 0.21606825 0.29124239 0.3615281 0.43645647 0.52719092 0.62719673 0.70102739 0.70677966 0.62258452 0.47558782 0.33273685 0.25945184 0.27716154 0.35455531][0.11438378 0.20584965 0.30571187 0.40812063 0.52181983 0.65369529 0.78952807 0.8841393 0.887564 0.77376753 0.57276481 0.36737108 0.2410236 0.22454232 0.28535157][0.084605657 0.17767346 0.28780982 0.40830892 0.54735219 0.70713776 0.86602479 0.97187227 0.97160733 0.84049749 0.61120033 0.37465641 0.22197597 0.18685308 0.23550193][0.053888261 0.13547526 0.23932762 0.35905355 0.50267541 0.66833866 0.8291623 0.93106145 0.92456096 0.79148334 0.56515408 0.3341364 0.18595369 0.15117943 0.19585575][0.027378999 0.086431913 0.16884287 0.26934612 0.39545405 0.54180479 0.67965007 0.760051 0.74408537 0.62345034 0.42918855 0.23845805 0.12484427 0.11071301 0.16275302][0.01277105 0.0424942 0.092868432 0.16126001 0.25419536 0.36481923 0.4662655 0.51957583 0.49779055 0.40169322 0.25728479 0.12515925 0.060583491 0.076949082 0.14282767][0.011518052 0.013628945 0.031393845 0.065071441 0.11993688 0.19007328 0.25417691 0.28477857 0.26472884 0.19946739 0.10857268 0.035431992 0.017642578 0.062174484 0.14113602][0.020052705 0.0017620393 -0.0061813206 -0.00097205932 0.02170248 0.058140077 0.094078004 0.11140013 0.099245831 0.062838227 0.015627615 -0.012963002 0.0015709049 0.063298725 0.14948612][0.037234779 0.0065892031 -0.018097028 -0.031384006 -0.028915169 -0.012214318 0.0096402 0.023606546 0.021002389 0.0042454074 -0.017560476 -0.023689108 0.0033769628 0.068660252 0.15574673]]...]
INFO - root - 2017-12-11 05:28:11.839284: step 19810, loss = 0.69, batch loss = 0.63 (14.1 examples/sec; 0.567 sec/batch; 49h:14m:43s remains)
INFO - root - 2017-12-11 05:28:17.287969: step 19820, loss = 0.70, batch loss = 0.65 (14.7 examples/sec; 0.545 sec/batch; 47h:18m:24s remains)
INFO - root - 2017-12-11 05:28:22.711352: step 19830, loss = 0.69, batch loss = 0.64 (14.9 examples/sec; 0.538 sec/batch; 46h:42m:35s remains)
INFO - root - 2017-12-11 05:28:28.171206: step 19840, loss = 0.71, batch loss = 0.65 (14.4 examples/sec; 0.557 sec/batch; 48h:21m:30s remains)
INFO - root - 2017-12-11 05:28:33.707252: step 19850, loss = 0.70, batch loss = 0.64 (14.0 examples/sec; 0.570 sec/batch; 49h:28m:39s remains)
INFO - root - 2017-12-11 05:28:39.308949: step 19860, loss = 0.69, batch loss = 0.63 (13.9 examples/sec; 0.575 sec/batch; 49h:57m:10s remains)
INFO - root - 2017-12-11 05:28:44.784643: step 19870, loss = 0.70, batch loss = 0.64 (14.7 examples/sec; 0.543 sec/batch; 47h:10m:30s remains)
INFO - root - 2017-12-11 05:28:49.945799: step 19880, loss = 0.71, batch loss = 0.65 (14.5 examples/sec; 0.550 sec/batch; 47h:47m:14s remains)
INFO - root - 2017-12-11 05:28:55.420524: step 19890, loss = 0.72, batch loss = 0.66 (14.8 examples/sec; 0.541 sec/batch; 47h:00m:06s remains)
INFO - root - 2017-12-11 05:29:00.900381: step 19900, loss = 0.70, batch loss = 0.64 (14.6 examples/sec; 0.548 sec/batch; 47h:37m:35s remains)
2017-12-11 05:29:01.559226: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.31295726 0.36790866 0.39770311 0.39919892 0.386511 0.37928215 0.38223964 0.37650943 0.35267428 0.33180922 0.3222914 0.32452795 0.33021137 0.3365064 0.34227228][0.31294835 0.36663911 0.39752531 0.40081355 0.38878706 0.37836337 0.37639081 0.36943549 0.34919855 0.33500749 0.33301717 0.34071156 0.34995571 0.35821661 0.36404178][0.29125202 0.340845 0.37081435 0.37557024 0.36450124 0.35221279 0.34799385 0.34512854 0.33535811 0.33128494 0.33645475 0.34806216 0.36028835 0.37148696 0.37839565][0.27358225 0.31941742 0.34651545 0.35053846 0.34076509 0.33128628 0.33111817 0.336898 0.34027389 0.34720057 0.35797012 0.37089425 0.38488835 0.3999559 0.40890217][0.26389983 0.30674642 0.32986715 0.33208191 0.32596743 0.32626793 0.3368741 0.35230529 0.36589432 0.3807148 0.39416176 0.40541336 0.41890031 0.43764454 0.44914374][0.26245522 0.30474284 0.32552621 0.32785469 0.32814875 0.34153056 0.36384326 0.38526681 0.40201777 0.41795608 0.42947179 0.43640679 0.44719014 0.46698928 0.47895741][0.27599967 0.32132077 0.34397215 0.34905764 0.35533497 0.37883192 0.40873334 0.43035346 0.44162813 0.45022374 0.45478365 0.45545021 0.4612278 0.47744656 0.485664][0.309283 0.36114359 0.38863033 0.39539993 0.40107116 0.42520046 0.45504507 0.47137082 0.47152463 0.46811429 0.46387753 0.45811155 0.4578836 0.46747291 0.46931514][0.35923576 0.41754183 0.4476974 0.45055708 0.44626415 0.45986661 0.48185408 0.49026662 0.47979927 0.46627358 0.45568374 0.44488814 0.43840045 0.43983665 0.43526742][0.41152066 0.46982837 0.49466056 0.48718336 0.4678027 0.46544382 0.47598982 0.4779959 0.46247089 0.44463742 0.43208131 0.41912141 0.40785331 0.40180814 0.39268544][0.43862206 0.48663753 0.49727347 0.47600177 0.44267794 0.42617029 0.42686105 0.42642951 0.41259763 0.39687881 0.38710332 0.37580815 0.36336562 0.3533136 0.34282526][0.41143587 0.4406445 0.43435928 0.40269288 0.36309719 0.34047663 0.33727875 0.33865377 0.33022457 0.31937173 0.31403044 0.30648535 0.29635549 0.28720579 0.27932835][0.31606942 0.3250595 0.30697078 0.27372718 0.23851465 0.2198935 0.21976514 0.22566819 0.22297813 0.21649896 0.21435575 0.20986827 0.20273097 0.19651139 0.19220994][0.17350291 0.16714463 0.14608368 0.12025796 0.097675651 0.089734592 0.096278735 0.10650317 0.10749783 0.1029562 0.10101909 0.096998386 0.090805747 0.085916631 0.083369151][0.030148698 0.015776617 -0.001276949 -0.015596092 -0.024183191 -0.022190664 -0.011178695 0.00040867808 0.0026928151 -0.0014588762 -0.004793196 -0.0098920846 -0.016798593 -0.022268105 -0.025141347]]...]
INFO - root - 2017-12-11 05:29:07.031244: step 19910, loss = 0.71, batch loss = 0.65 (15.0 examples/sec; 0.535 sec/batch; 46h:25m:03s remains)
INFO - root - 2017-12-11 05:29:12.490990: step 19920, loss = 0.67, batch loss = 0.61 (14.3 examples/sec; 0.561 sec/batch; 48h:43m:48s remains)
INFO - root - 2017-12-11 05:29:17.949098: step 19930, loss = 0.69, batch loss = 0.63 (14.7 examples/sec; 0.543 sec/batch; 47h:07m:05s remains)
INFO - root - 2017-12-11 05:29:23.490251: step 19940, loss = 0.71, batch loss = 0.65 (14.3 examples/sec; 0.559 sec/batch; 48h:29m:52s remains)
INFO - root - 2017-12-11 05:29:28.910159: step 19950, loss = 0.70, batch loss = 0.64 (14.5 examples/sec; 0.553 sec/batch; 48h:01m:06s remains)
INFO - root - 2017-12-11 05:29:34.344826: step 19960, loss = 0.69, batch loss = 0.63 (14.9 examples/sec; 0.538 sec/batch; 46h:41m:58s remains)
INFO - root - 2017-12-11 05:29:39.913195: step 19970, loss = 0.69, batch loss = 0.63 (14.8 examples/sec; 0.542 sec/batch; 47h:01m:37s remains)
INFO - root - 2017-12-11 05:29:45.131659: step 19980, loss = 0.69, batch loss = 0.63 (14.5 examples/sec; 0.550 sec/batch; 47h:45m:37s remains)
INFO - root - 2017-12-11 05:29:50.730736: step 19990, loss = 0.68, batch loss = 0.62 (14.4 examples/sec; 0.557 sec/batch; 48h:22m:52s remains)
INFO - root - 2017-12-11 05:29:56.235038: step 20000, loss = 0.69, batch loss = 0.63 (14.3 examples/sec; 0.561 sec/batch; 48h:40m:42s remains)
2017-12-11 05:29:56.858922: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.016004475 0.0074739233 -0.0034949705 -0.011886227 -0.019380413 -0.026503371 -0.028795289 -0.022817859 -0.010664077 0.00024294329 0.0065356591 0.00722571 5.8905127e-05 -0.01776344 -0.043027658][0.10720216 0.10020015 0.084529415 0.069591373 0.053910505 0.038581353 0.031422514 0.038206145 0.055613186 0.072073944 0.081985541 0.084463939 0.074570544 0.047174804 0.0061733136][0.22214527 0.21884912 0.19893484 0.1758976 0.14994113 0.12555523 0.11241332 0.11839453 0.13943975 0.16078892 0.17528513 0.17982037 0.16654429 0.12770171 0.068607882][0.33437178 0.34154981 0.32456672 0.29800031 0.26524112 0.23459174 0.21505605 0.21584639 0.23432256 0.25670156 0.27281713 0.27635363 0.25651205 0.2051059 0.12798132][0.40643978 0.43484607 0.43679243 0.42348063 0.39895943 0.37331864 0.35329369 0.34895161 0.36063382 0.37833229 0.38967985 0.38410872 0.34950668 0.27829745 0.1783205][0.43652174 0.49439096 0.52848607 0.54410452 0.54321688 0.53466868 0.52250123 0.51802528 0.525851 0.5381279 0.53916627 0.51657766 0.45979977 0.36207497 0.2330945][0.45219329 0.54175925 0.60961759 0.65722221 0.68344051 0.69460613 0.69296181 0.69184136 0.70015615 0.70846868 0.69680983 0.65412712 0.57605618 0.45510763 0.29909578][0.45421749 0.56811172 0.66003251 0.73218095 0.78126115 0.80978727 0.81872493 0.82368803 0.83779347 0.84552187 0.82151151 0.7594434 0.66509783 0.52940875 0.35427225][0.43487865 0.55328542 0.6484533 0.727335 0.78447711 0.81901991 0.83210963 0.84178662 0.86420417 0.87641531 0.84809768 0.77836728 0.68294984 0.55012369 0.37331611][0.38182491 0.47901547 0.55276757 0.61629111 0.66312867 0.68988186 0.69844985 0.708102 0.73712713 0.75631106 0.7331298 0.6707347 0.5924623 0.48386708 0.32978207][0.2908558 0.35130042 0.39038661 0.4263525 0.45278573 0.46414 0.46319893 0.46913451 0.5004102 0.52448821 0.51036274 0.46386525 0.41178298 0.33903944 0.22496079][0.16801138 0.19327705 0.2014522 0.21089286 0.21632957 0.21179405 0.20057525 0.20034131 0.22754014 0.25153497 0.24577779 0.21675834 0.19001175 0.15261728 0.083266452][0.034689374 0.035945084 0.027063064 0.021104351 0.013940827 0.00038734486 -0.016256688 -0.021326622 -0.0018259936 0.01725499 0.015424778 -0.0013357159 -0.011850974 -0.024675855 -0.05723162][-0.057707552 -0.066907749 -0.080124676 -0.091113612 -0.10258146 -0.11806197 -0.13460861 -0.14170597 -0.13072367 -0.11924562 -0.12087122 -0.13121991 -0.13522044 -0.13692723 -0.1473335][-0.098254517 -0.10863249 -0.11902352 -0.12871173 -0.1386794 -0.15091726 -0.16336574 -0.16993296 -0.16575316 -0.16122289 -0.16381083 -0.17089066 -0.17334354 -0.17210731 -0.1724474]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-SGD-lr0.001-clip50-initfromfixqian30k-fixqian/model.ckpt-20000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-SGD-lr0.001-clip50-initfromfixqian30k-fixqian/model.ckpt-20000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-11 05:30:03.219958: step 20010, loss = 0.70, batch loss = 0.64 (14.7 examples/sec; 0.545 sec/batch; 47h:19m:34s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-SGD-lr0.001-clip50-initfromfixqian30k-fixqian
INFO - root - 2017-12-11 05:30:08.651574: step 20020, loss = 0.69, batch loss = 0.64 (14.4 examples/sec; 0.557 sec/batch; 48h:23m:23s remains)
INFO - root - 2017-12-11 05:30:14.154869: step 20030, loss = 0.70, batch loss = 0.64 (14.3 examples/sec; 0.561 sec/batch; 48h:41m:58s remains)
INFO - root - 2017-12-11 05:30:19.693235: step 20040, loss = 0.69, batch loss = 0.63 (14.4 examples/sec; 0.555 sec/batch; 48h:07m:49s remains)
INFO - root - 2017-12-11 05:30:25.241024: step 20050, loss = 0.69, batch loss = 0.63 (14.4 examples/sec; 0.556 sec/batch; 48h:13m:09s remains)
INFO - root - 2017-12-11 05:30:30.703299: step 20060, loss = 0.69, batch loss = 0.63 (15.2 examples/sec; 0.526 sec/batch; 45h:39m:48s remains)
INFO - root - 2017-12-11 05:30:35.912923: step 20070, loss = 0.71, batch loss = 0.65 (14.3 examples/sec; 0.560 sec/batch; 48h:37m:31s remains)
INFO - root - 2017-12-11 05:30:41.391266: step 20080, loss = 0.71, batch loss = 0.65 (15.0 examples/sec; 0.533 sec/batch; 46h:15m:20s remains)
INFO - root - 2017-12-11 05:30:46.877394: step 20090, loss = 0.71, batch loss = 0.65 (14.4 examples/sec; 0.555 sec/batch; 48h:10m:56s remains)
INFO - root - 2017-12-11 05:30:52.346312: step 20100, loss = 0.71, batch loss = 0.65 (14.8 examples/sec; 0.540 sec/batch; 46h:50m:53s remains)
2017-12-11 05:30:52.938572: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.14482155 0.16340488 0.17885542 0.18884195 0.18520863 0.17647065 0.18256177 0.20426469 0.21886836 0.20524797 0.17390092 0.14191717 0.10284139 0.055478424 0.014237885][0.1350414 0.1505921 0.16085471 0.16691856 0.16307704 0.15820955 0.17252968 0.2048317 0.22995795 0.22263172 0.19191685 0.15782025 0.11509483 0.063995592 0.01876248][0.12078287 0.12520297 0.1249762 0.1261535 0.12505855 0.12783203 0.15089986 0.19144325 0.22444026 0.22270614 0.19245888 0.15636569 0.11305861 0.063488409 0.019017389][0.12475288 0.11513599 0.10388261 0.10243122 0.10701737 0.11976794 0.15217078 0.19998163 0.23750339 0.23624149 0.200534 0.15750669 0.11072406 0.061253771 0.017615354][0.14939104 0.1296096 0.1118612 0.11100137 0.12176163 0.14363128 0.18529186 0.24071644 0.28011587 0.27436653 0.22816916 0.17339629 0.11757746 0.062918842 0.017174942][0.17914669 0.15708524 0.13904323 0.13955715 0.15267161 0.17879602 0.22707857 0.28786445 0.3250497 0.31139955 0.25487947 0.1903232 0.12545495 0.063336894 0.014120316][0.19111341 0.17439634 0.16146162 0.16284117 0.17316231 0.19682647 0.24522407 0.30457103 0.33416316 0.31184128 0.25048354 0.18442473 0.1180135 0.053538911 0.0040568621][0.19368324 0.18183671 0.17148799 0.16857664 0.16958955 0.18417332 0.22551136 0.27550039 0.29313147 0.26373208 0.20483997 0.14667445 0.088290952 0.030100526 -0.013634507][0.19574864 0.1840864 0.16968696 0.15714855 0.14635205 0.15087214 0.18386735 0.22343755 0.23132958 0.199399 0.14761049 0.10099431 0.054235529 0.0061774068 -0.029364577][0.19967501 0.1848539 0.16365062 0.14138801 0.12187659 0.1204482 0.14927918 0.18480478 0.19103867 0.16223243 0.11742201 0.078380711 0.038493212 -0.0034291537 -0.034363784][0.19938307 0.18294172 0.15972917 0.13617976 0.11826806 0.11914283 0.14999698 0.1884384 0.19873542 0.1736749 0.1293588 0.088524371 0.046779696 0.0037156602 -0.02837649][0.19995859 0.18734501 0.16950333 0.15440217 0.1471587 0.15376244 0.18520427 0.22470976 0.23704538 0.21201585 0.16187383 0.11303629 0.065113857 0.017727913 -0.018036317][0.22472022 0.22054939 0.20916334 0.20211406 0.20238563 0.20718421 0.22868724 0.25933564 0.2666578 0.23825856 0.1821031 0.12744041 0.076705523 0.027546877 -0.010484986][0.26286691 0.26809943 0.26019558 0.25444895 0.25182289 0.24415544 0.24658939 0.26043451 0.25741994 0.22502068 0.16782363 0.11459928 0.068157658 0.023462472 -0.011985238][0.2831361 0.29375374 0.28576094 0.27516097 0.26235816 0.23920335 0.2238864 0.22285277 0.21151377 0.17856114 0.12684788 0.08162044 0.044477943 0.0088536153 -0.019689813]]...]
INFO - root - 2017-12-11 05:30:58.433600: step 20110, loss = 0.70, batch loss = 0.64 (14.4 examples/sec; 0.557 sec/batch; 48h:17m:35s remains)
INFO - root - 2017-12-11 05:31:03.930841: step 20120, loss = 0.71, batch loss = 0.65 (14.3 examples/sec; 0.560 sec/batch; 48h:37m:19s remains)
INFO - root - 2017-12-11 05:31:09.429894: step 20130, loss = 0.69, batch loss = 0.64 (14.7 examples/sec; 0.544 sec/batch; 47h:11m:04s remains)
INFO - root - 2017-12-11 05:31:14.849577: step 20140, loss = 0.70, batch loss = 0.64 (14.6 examples/sec; 0.547 sec/batch; 47h:28m:47s remains)
INFO - root - 2017-12-11 05:31:20.376976: step 20150, loss = 0.69, batch loss = 0.63 (14.4 examples/sec; 0.556 sec/batch; 48h:13m:44s remains)
INFO - root - 2017-12-11 05:31:25.832800: step 20160, loss = 0.69, batch loss = 0.63 (14.7 examples/sec; 0.543 sec/batch; 47h:07m:48s remains)
INFO - root - 2017-12-11 05:31:31.011172: step 20170, loss = 0.69, batch loss = 0.63 (14.4 examples/sec; 0.554 sec/batch; 48h:04m:06s remains)
INFO - root - 2017-12-11 05:31:36.519160: step 20180, loss = 0.70, batch loss = 0.64 (14.6 examples/sec; 0.546 sec/batch; 47h:23m:28s remains)
INFO - root - 2017-12-11 05:31:42.040271: step 20190, loss = 0.71, batch loss = 0.65 (14.7 examples/sec; 0.544 sec/batch; 47h:14m:00s remains)
INFO - root - 2017-12-11 05:31:47.637031: step 20200, loss = 0.70, batch loss = 0.64 (15.1 examples/sec; 0.529 sec/batch; 45h:55m:48s remains)
2017-12-11 05:31:48.226491: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.09649796 0.080288075 0.070737533 0.0787669 0.10710984 0.14585952 0.18190177 0.2063503 0.21255921 0.19830613 0.17231625 0.14434949 0.11816623 0.096335083 0.088461459][0.090239778 0.07921534 0.080578178 0.10264765 0.1436522 0.18925554 0.22396223 0.23970385 0.23412086 0.21060304 0.18117714 0.15588264 0.13547757 0.11900197 0.11481698][0.095999561 0.089165956 0.095590509 0.12183338 0.16456938 0.20848508 0.23748702 0.24418691 0.22882518 0.19793284 0.16505024 0.14040728 0.12320345 0.1109413 0.11066628][0.12157448 0.11642438 0.12168729 0.14425191 0.18142822 0.21856788 0.24045634 0.23981065 0.21735451 0.17974311 0.1403738 0.11071241 0.091079168 0.0803495 0.0839814][0.15384181 0.1509444 0.15574898 0.17665413 0.21173918 0.24616621 0.2653093 0.26113135 0.23381612 0.18841398 0.13820727 0.0971382 0.069144353 0.056075275 0.061495047][0.17553489 0.17759182 0.18731946 0.21475466 0.25724232 0.29834622 0.32260966 0.32060823 0.29214045 0.23999692 0.17833555 0.12508878 0.088469654 0.072069213 0.077427082][0.17010444 0.18012026 0.20119074 0.2437878 0.30346423 0.3611677 0.39898059 0.40517581 0.37955743 0.32501525 0.25776491 0.19949731 0.16130657 0.14739729 0.1560778][0.1336273 0.15351006 0.18903728 0.24931566 0.32850373 0.40570736 0.46101984 0.48141542 0.46647561 0.4197323 0.35862696 0.30681396 0.27673522 0.27151349 0.28560054][0.067035891 0.095523126 0.14337103 0.21674763 0.30962 0.40136304 0.47244966 0.5110811 0.51638234 0.491678 0.45281103 0.4214932 0.40928295 0.41726619 0.43665677][-0.013323472 0.017020585 0.067846119 0.14182302 0.23452714 0.32891476 0.40861428 0.46463957 0.49523342 0.50155967 0.49470162 0.49091798 0.49922875 0.5188691 0.54049039][-0.079812385 -0.057088144 -0.016090386 0.043703195 0.12098827 0.20467469 0.28252625 0.34798589 0.39780045 0.4301278 0.45011124 0.46751127 0.48916855 0.51383317 0.53367656][-0.11924987 -0.10816765 -0.083439738 -0.045614116 0.0069720922 0.069146156 0.13311899 0.19440062 0.24892323 0.292657 0.32576862 0.3523396 0.37723288 0.39966878 0.41438076][-0.13148145 -0.12992252 -0.1198487 -0.10272335 -0.075698078 -0.039952878 0.00074901583 0.044485807 0.088175207 0.1276623 0.1604937 0.18659987 0.20844339 0.22508892 0.23353823][-0.12530816 -0.12933096 -0.12865293 -0.12546723 -0.1173888 -0.10397491 -0.086332582 -0.064418338 -0.03955527 -0.01451092 0.0081266919 0.02652069 0.041242983 0.050948229 0.053963527][-0.1106251 -0.11676513 -0.12036283 -0.12388644 -0.12589926 -0.1257153 -0.12334219 -0.11810014 -0.11014462 -0.10065182 -0.091077134 -0.083100937 -0.07689251 -0.073674835 -0.074482307]]...]
INFO - root - 2017-12-11 05:31:53.800619: step 20210, loss = 0.71, batch loss = 0.66 (14.7 examples/sec; 0.545 sec/batch; 47h:15m:33s remains)
INFO - root - 2017-12-11 05:31:59.285325: step 20220, loss = 0.70, batch loss = 0.64 (14.4 examples/sec; 0.554 sec/batch; 48h:03m:14s remains)
INFO - root - 2017-12-11 05:32:04.785733: step 20230, loss = 0.70, batch loss = 0.64 (14.2 examples/sec; 0.564 sec/batch; 48h:54m:49s remains)
INFO - root - 2017-12-11 05:32:10.352017: step 20240, loss = 0.71, batch loss = 0.65 (14.8 examples/sec; 0.541 sec/batch; 46h:57m:55s remains)
INFO - root - 2017-12-11 05:32:15.844856: step 20250, loss = 0.70, batch loss = 0.65 (14.3 examples/sec; 0.558 sec/batch; 48h:23m:48s remains)
INFO - root - 2017-12-11 05:32:21.325165: step 20260, loss = 0.70, batch loss = 0.65 (14.2 examples/sec; 0.563 sec/batch; 48h:52m:11s remains)
INFO - root - 2017-12-11 05:32:26.394135: step 20270, loss = 0.71, batch loss = 0.65 (14.1 examples/sec; 0.569 sec/batch; 49h:22m:21s remains)
INFO - root - 2017-12-11 05:32:31.881984: step 20280, loss = 0.71, batch loss = 0.65 (14.8 examples/sec; 0.540 sec/batch; 46h:52m:01s remains)
INFO - root - 2017-12-11 05:32:37.396669: step 20290, loss = 0.71, batch loss = 0.66 (14.0 examples/sec; 0.570 sec/batch; 49h:28m:33s remains)
INFO - root - 2017-12-11 05:32:42.940663: step 20300, loss = 0.71, batch loss = 0.65 (14.9 examples/sec; 0.538 sec/batch; 46h:40m:39s remains)
2017-12-11 05:32:43.549342: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.4044669 0.36423084 0.32411072 0.30559015 0.29532796 0.28732467 0.29560825 0.33273721 0.39325902 0.451054 0.49270803 0.5185197 0.50799489 0.47018898 0.44761854][0.36663842 0.33696011 0.30894119 0.3043538 0.30789766 0.30878723 0.31982845 0.35517725 0.41049227 0.46008727 0.49186277 0.50813478 0.49284658 0.45769313 0.44771388][0.295432 0.28156781 0.27235413 0.28598222 0.30705312 0.32176736 0.33995458 0.37415946 0.42035171 0.45401192 0.46465409 0.45824715 0.42675349 0.38533157 0.37698433][0.2137581 0.22220521 0.23942205 0.2775028 0.32164359 0.35619876 0.38733456 0.42373943 0.46065897 0.47477236 0.45717087 0.4175896 0.35780904 0.29660267 0.27136952][0.14008383 0.17608784 0.22586514 0.29368359 0.36440384 0.42072228 0.46611109 0.50559735 0.53327245 0.52732587 0.48039275 0.40467185 0.3111245 0.22165132 0.16666874][0.095539637 0.16004789 0.24075101 0.33525538 0.42711011 0.49772322 0.5501132 0.58765614 0.60419744 0.580032 0.509424 0.40521052 0.28493765 0.17198789 0.08996506][0.09825249 0.18034087 0.27624294 0.38176075 0.47913489 0.54964417 0.5973832 0.62559396 0.62876612 0.59004694 0.50622952 0.39085263 0.265069 0.15072221 0.062887356][0.15130512 0.23096363 0.31687257 0.40975603 0.49189803 0.54633838 0.57826638 0.59128577 0.58088982 0.53399664 0.45110494 0.34742329 0.24444634 0.15778586 0.0917442][0.24165484 0.29851624 0.35192165 0.41212776 0.46292293 0.49155331 0.50363779 0.50194448 0.48244393 0.43625906 0.36779857 0.29395017 0.23386309 0.19374278 0.16691679][0.33719343 0.36067969 0.37217152 0.39284408 0.40850365 0.41155887 0.40688094 0.39509565 0.3724384 0.33441982 0.28800005 0.25097486 0.23852605 0.24660826 0.26029384][0.39949897 0.38995484 0.36527303 0.35336128 0.34144384 0.32602015 0.31076175 0.29435268 0.2738457 0.248695 0.22583565 0.2221774 0.24791102 0.29085496 0.33262041][0.38908607 0.35741356 0.31254402 0.28307685 0.25722781 0.23346408 0.21449412 0.19856048 0.18388939 0.17227033 0.16859919 0.1870887 0.23325941 0.29094467 0.34330851][0.29479644 0.25623712 0.20990312 0.17822844 0.1510393 0.12729219 0.10942759 0.096848413 0.088971734 0.087726049 0.095287129 0.12215566 0.17080383 0.22534443 0.27379674][0.14484689 0.11188957 0.077175923 0.053261478 0.032633997 0.014182873 0.0003873629 -0.0074968152 -0.009176814 -0.0041480013 0.0071349246 0.031650964 0.070771478 0.11212087 0.14869219][-0.006556801 -0.027424583 -0.045657415 -0.058419254 -0.070128173 -0.08187066 -0.091062628 -0.094944932 -0.092658244 -0.085325919 -0.075006522 -0.057504769 -0.031673547 -0.0055378363 0.017077759]]...]
INFO - root - 2017-12-11 05:32:49.093943: step 20310, loss = 0.70, batch loss = 0.64 (14.0 examples/sec; 0.573 sec/batch; 49h:40m:21s remains)
INFO - root - 2017-12-11 05:32:54.514855: step 20320, loss = 0.70, batch loss = 0.65 (14.7 examples/sec; 0.545 sec/batch; 47h:13m:26s remains)
INFO - root - 2017-12-11 05:33:00.002660: step 20330, loss = 0.69, batch loss = 0.63 (15.0 examples/sec; 0.535 sec/batch; 46h:23m:58s remains)
INFO - root - 2017-12-11 05:33:05.456262: step 20340, loss = 0.68, batch loss = 0.62 (14.8 examples/sec; 0.542 sec/batch; 46h:59m:21s remains)
INFO - root - 2017-12-11 05:33:11.009224: step 20350, loss = 0.69, batch loss = 0.63 (14.3 examples/sec; 0.561 sec/batch; 48h:38m:04s remains)
INFO - root - 2017-12-11 05:33:16.182136: step 20360, loss = 0.69, batch loss = 0.63 (15.0 examples/sec; 0.533 sec/batch; 46h:14m:45s remains)
INFO - root - 2017-12-11 05:33:21.789412: step 20370, loss = 0.70, batch loss = 0.64 (13.8 examples/sec; 0.581 sec/batch; 50h:20m:46s remains)
INFO - root - 2017-12-11 05:33:27.261218: step 20380, loss = 0.69, batch loss = 0.63 (14.6 examples/sec; 0.547 sec/batch; 47h:23m:55s remains)
INFO - root - 2017-12-11 05:33:32.733452: step 20390, loss = 0.70, batch loss = 0.64 (14.8 examples/sec; 0.540 sec/batch; 46h:50m:03s remains)
INFO - root - 2017-12-11 05:33:38.220052: step 20400, loss = 0.72, batch loss = 0.66 (14.4 examples/sec; 0.557 sec/batch; 48h:16m:10s remains)
2017-12-11 05:33:38.830686: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.06581717 0.078251272 0.093787737 0.10519069 0.10732077 0.10057402 0.091539413 0.092479423 0.11245327 0.14893763 0.18398732 0.19993377 0.1886403 0.15682124 0.12039991][0.068210334 0.091938883 0.12191498 0.14679222 0.15641519 0.14890431 0.13344963 0.12911037 0.15048963 0.19509366 0.24184464 0.26866096 0.26409206 0.23248348 0.19116688][0.078744054 0.11370552 0.15999554 0.20102964 0.22046556 0.21285774 0.18906096 0.17286804 0.18395603 0.22283679 0.27071914 0.30530611 0.3127223 0.29113513 0.254629][0.1053232 0.14469227 0.2003828 0.25331107 0.28319806 0.2805227 0.25443247 0.22814752 0.22437172 0.24906364 0.28975338 0.3273603 0.34644604 0.33859256 0.31161356][0.14358194 0.17874989 0.23035546 0.28315583 0.31917164 0.32701907 0.30933434 0.28286314 0.26992676 0.28215334 0.31408033 0.3508178 0.37653825 0.37908193 0.36110964][0.1957216 0.22018678 0.25452504 0.29360947 0.32820645 0.34896892 0.34923133 0.33435145 0.32137156 0.32528931 0.3477906 0.37947804 0.40620682 0.41436023 0.40371484][0.26869091 0.27948329 0.28814334 0.30333996 0.32873467 0.36082694 0.38402238 0.3893342 0.38329744 0.38017148 0.38923806 0.40884647 0.42957273 0.4379755 0.43157184][0.35810536 0.35284492 0.33145019 0.31740203 0.32707715 0.36317781 0.40458924 0.42918834 0.43231106 0.42432198 0.42086819 0.42749751 0.44001 0.44565928 0.43968081][0.42712691 0.40776616 0.3603135 0.32052559 0.31281438 0.34385109 0.39151108 0.4272365 0.43920979 0.43275398 0.42539266 0.4256753 0.43238974 0.43405357 0.4253943][0.44365263 0.41745785 0.35675234 0.3031902 0.28181773 0.30097368 0.34177792 0.37792274 0.39661571 0.3986159 0.3968589 0.39732534 0.39984432 0.39651206 0.38607329][0.40938014 0.38159043 0.32009023 0.26482174 0.23543678 0.24097967 0.26916724 0.3010323 0.32590339 0.33968642 0.34652239 0.347527 0.34375727 0.33439553 0.32479483][0.344175 0.31908175 0.26335305 0.2124328 0.17882307 0.1726236 0.18952437 0.21748644 0.24723086 0.26983929 0.2807087 0.2777983 0.26475814 0.24929567 0.24210274][0.26602018 0.24413295 0.1947702 0.15031406 0.11695435 0.10413474 0.11326482 0.13659504 0.16575998 0.18990558 0.19999991 0.19349355 0.17619804 0.16092561 0.1591852][0.18191729 0.16212168 0.12080965 0.08648923 0.059745941 0.046294838 0.049650904 0.064780205 0.085692912 0.10416465 0.11199553 0.10711962 0.094848774 0.08735957 0.092956267][0.10728689 0.090890765 0.061753344 0.0406076 0.023557035 0.011860272 0.00906817 0.012597103 0.020048456 0.028479636 0.033909794 0.034171924 0.031407684 0.033535395 0.044591222]]...]
INFO - root - 2017-12-11 05:33:44.333942: step 20410, loss = 0.68, batch loss = 0.62 (13.9 examples/sec; 0.576 sec/batch; 49h:53m:42s remains)
INFO - root - 2017-12-11 05:33:49.893584: step 20420, loss = 0.70, batch loss = 0.64 (14.5 examples/sec; 0.553 sec/batch; 47h:56m:49s remains)
INFO - root - 2017-12-11 05:33:55.413609: step 20430, loss = 0.69, batch loss = 0.63 (14.6 examples/sec; 0.547 sec/batch; 47h:26m:04s remains)
INFO - root - 2017-12-11 05:34:00.878880: step 20440, loss = 0.67, batch loss = 0.61 (14.4 examples/sec; 0.557 sec/batch; 48h:15m:54s remains)
INFO - root - 2017-12-11 05:34:06.404254: step 20450, loss = 0.69, batch loss = 0.63 (14.4 examples/sec; 0.556 sec/batch; 48h:10m:09s remains)
INFO - root - 2017-12-11 05:34:11.648783: step 20460, loss = 0.70, batch loss = 0.64 (14.6 examples/sec; 0.546 sec/batch; 47h:20m:48s remains)
INFO - root - 2017-12-11 05:34:17.172055: step 20470, loss = 0.70, batch loss = 0.64 (14.8 examples/sec; 0.541 sec/batch; 46h:51m:02s remains)
INFO - root - 2017-12-11 05:34:22.698097: step 20480, loss = 0.70, batch loss = 0.64 (14.9 examples/sec; 0.539 sec/batch; 46h:40m:47s remains)
INFO - root - 2017-12-11 05:34:28.328908: step 20490, loss = 0.68, batch loss = 0.62 (14.5 examples/sec; 0.551 sec/batch; 47h:43m:40s remains)
INFO - root - 2017-12-11 05:34:33.899699: step 20500, loss = 0.69, batch loss = 0.63 (14.5 examples/sec; 0.551 sec/batch; 47h:43m:35s remains)
2017-12-11 05:34:34.521000: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.19885661 0.18528906 0.1646321 0.13752027 0.12063521 0.13389139 0.18012461 0.24010867 0.28732613 0.30854726 0.30720857 0.2956813 0.28212228 0.26184711 0.22779875][0.19069014 0.1746098 0.16296247 0.15352881 0.15896055 0.1949756 0.25872564 0.32609552 0.370549 0.38386366 0.37721357 0.36949071 0.36861828 0.36299744 0.338285][0.16752543 0.14897662 0.14574045 0.15387104 0.18079606 0.23662741 0.31350851 0.38484457 0.42527449 0.4299593 0.41680238 0.41081831 0.42090547 0.43146005 0.422261][0.14080708 0.11863697 0.12113456 0.14349252 0.18803348 0.25997749 0.34824526 0.42451423 0.46325234 0.46077606 0.43955323 0.42941117 0.44262031 0.46364874 0.46841854][0.11835402 0.092714861 0.09730424 0.12836887 0.1840203 0.26559114 0.36030862 0.43897161 0.47575411 0.46697176 0.43763331 0.42036167 0.43174464 0.45898402 0.47590286][0.10437194 0.078778535 0.0853184 0.12124772 0.18179424 0.26619795 0.36137307 0.43857488 0.47254476 0.45890048 0.42245033 0.39632457 0.40141323 0.42955923 0.45480362][0.10048542 0.081368744 0.092152476 0.13104352 0.1923715 0.27576178 0.36995742 0.44758677 0.48293221 0.46742687 0.42231125 0.38110751 0.37074441 0.39062917 0.41766977][0.10297161 0.094155625 0.110513 0.15111302 0.21151601 0.29362118 0.38961488 0.47344142 0.51645756 0.50256068 0.4471083 0.38550115 0.35345632 0.36009046 0.38534486][0.10845564 0.110067 0.13192923 0.17367128 0.23296674 0.31376067 0.41171139 0.50206435 0.552462 0.54026234 0.47635168 0.39869431 0.3500596 0.34660554 0.36966035][0.11600117 0.12690493 0.153921 0.19741845 0.25528932 0.33080539 0.42259249 0.50976908 0.55985475 0.54797441 0.48213595 0.40029672 0.34633672 0.33899906 0.36114308][0.13121133 0.14654703 0.17781347 0.22526948 0.28415757 0.35358164 0.43348166 0.50858653 0.55053127 0.53639364 0.47223067 0.39304417 0.33970582 0.33073702 0.35156304][0.15950739 0.17251185 0.20303425 0.251086 0.30884913 0.3706367 0.4364627 0.49650127 0.52824241 0.51240641 0.4527984 0.37993094 0.33068112 0.3227497 0.34406596][0.20482361 0.2146291 0.23876262 0.27881986 0.32738888 0.37689278 0.42712936 0.47196814 0.49433643 0.47887924 0.42776218 0.36656871 0.32713071 0.32501161 0.35015154][0.25956842 0.26829717 0.28276905 0.30790451 0.33999458 0.37262726 0.4057104 0.43577945 0.45040619 0.43760607 0.39851436 0.35264382 0.32460463 0.32623065 0.35044375][0.30645514 0.31734511 0.32501981 0.33644021 0.35100394 0.36482492 0.37925649 0.39455721 0.40333998 0.39594373 0.37143186 0.34112373 0.32109249 0.3198806 0.33501709]]...]
INFO - root - 2017-12-11 05:34:40.010941: step 20510, loss = 0.71, batch loss = 0.65 (14.8 examples/sec; 0.542 sec/batch; 46h:56m:26s remains)
INFO - root - 2017-12-11 05:34:45.499334: step 20520, loss = 0.70, batch loss = 0.64 (14.6 examples/sec; 0.549 sec/batch; 47h:32m:24s remains)
INFO - root - 2017-12-11 05:34:51.073033: step 20530, loss = 0.69, batch loss = 0.64 (14.3 examples/sec; 0.561 sec/batch; 48h:35m:49s remains)
INFO - root - 2017-12-11 05:34:56.638982: step 20540, loss = 0.70, batch loss = 0.64 (14.5 examples/sec; 0.551 sec/batch; 47h:45m:34s remains)
INFO - root - 2017-12-11 05:35:02.127100: step 20550, loss = 0.69, batch loss = 0.63 (14.7 examples/sec; 0.543 sec/batch; 47h:03m:08s remains)
INFO - root - 2017-12-11 05:35:07.324130: step 20560, loss = 0.69, batch loss = 0.63 (14.6 examples/sec; 0.548 sec/batch; 47h:31m:08s remains)
INFO - root - 2017-12-11 05:35:12.824448: step 20570, loss = 0.69, batch loss = 0.63 (14.9 examples/sec; 0.536 sec/batch; 46h:24m:49s remains)
INFO - root - 2017-12-11 05:35:18.402403: step 20580, loss = 0.69, batch loss = 0.64 (14.0 examples/sec; 0.572 sec/batch; 49h:31m:38s remains)
INFO - root - 2017-12-11 05:35:23.835223: step 20590, loss = 0.70, batch loss = 0.65 (14.3 examples/sec; 0.558 sec/batch; 48h:22m:36s remains)
INFO - root - 2017-12-11 05:35:29.383706: step 20600, loss = 0.72, batch loss = 0.66 (13.8 examples/sec; 0.579 sec/batch; 50h:08m:23s remains)
2017-12-11 05:35:29.921064: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.12569092 0.14818239 0.15411572 0.14539866 0.12895639 0.11043456 0.091591284 0.06946449 0.048044618 0.03530496 0.038915962 0.058403421 0.084263325 0.10236954 0.1009017][0.17734389 0.2033588 0.20668572 0.19197956 0.17102475 0.15240058 0.13740304 0.11987038 0.10042985 0.085006431 0.082309105 0.095079124 0.11709154 0.13503629 0.1357511][0.24559027 0.27074298 0.26577356 0.24094813 0.21417314 0.19670399 0.18846953 0.17917623 0.1637079 0.1440807 0.13019116 0.13053989 0.14506693 0.16286841 0.16993752][0.32819298 0.34989277 0.33302382 0.29573563 0.26275358 0.24731073 0.24664359 0.24599729 0.23386808 0.20808049 0.17960292 0.16396283 0.169251 0.18725388 0.20305584][0.40538982 0.42433792 0.39788631 0.35211733 0.31731382 0.30722567 0.31544295 0.32280982 0.31234524 0.27875447 0.2340325 0.20062405 0.19518031 0.21250947 0.23655432][0.44918609 0.46831527 0.44021112 0.39593288 0.36773756 0.36790681 0.38613755 0.39945957 0.38782528 0.34567067 0.28620142 0.23709059 0.22190931 0.23823966 0.26831317][0.4397012 0.4595696 0.43872616 0.40745446 0.39455411 0.40882364 0.436511 0.45236263 0.43622509 0.38523728 0.31448773 0.25479391 0.23367853 0.2507312 0.28642678][0.37812626 0.39863726 0.3913295 0.38038313 0.38681769 0.41485012 0.44841683 0.46279004 0.44026119 0.38192457 0.3053329 0.24222328 0.22074202 0.24136232 0.28301352][0.29039472 0.31253335 0.32100463 0.33087802 0.35434154 0.39141268 0.42530057 0.43381703 0.40363812 0.34039646 0.26373395 0.20420502 0.18765311 0.21390168 0.26031306][0.20759232 0.23367585 0.2567043 0.28237855 0.31598443 0.35493776 0.38321027 0.38221511 0.34396133 0.27810583 0.20555681 0.15493339 0.14763963 0.18127277 0.23081376][0.1455806 0.17662339 0.21024252 0.24457057 0.28057405 0.3146373 0.33237281 0.31925446 0.27298984 0.20667118 0.14130849 0.10339214 0.10896058 0.15146019 0.20313329][0.10079907 0.13366874 0.17118305 0.20647293 0.2383517 0.2634618 0.26961708 0.24627589 0.19596186 0.13334964 0.077820435 0.053683385 0.072169036 0.12279919 0.17662765][0.070493668 0.099664323 0.13245074 0.16017447 0.18188575 0.19609991 0.19348219 0.16651584 0.12028717 0.068169788 0.025967143 0.015297257 0.044266991 0.10071323 0.15668914][0.058973808 0.079481065 0.10013344 0.11339013 0.12050851 0.12309393 0.1150784 0.090762526 0.055343717 0.018690702 -0.00807967 -0.0065966095 0.029864131 0.0897837 0.1476832][0.06128712 0.071057625 0.076864406 0.0743973 0.067732476 0.061328124 0.051802237 0.034228317 0.012336709 -0.0081867008 -0.020634569 -0.0098382281 0.030631451 0.090997256 0.14849636]]...]
INFO - root - 2017-12-11 05:35:35.357245: step 20610, loss = 0.71, batch loss = 0.65 (14.5 examples/sec; 0.554 sec/batch; 47h:57m:38s remains)
INFO - root - 2017-12-11 05:35:40.821398: step 20620, loss = 0.70, batch loss = 0.64 (14.7 examples/sec; 0.544 sec/batch; 47h:06m:44s remains)
INFO - root - 2017-12-11 05:35:46.403163: step 20630, loss = 0.68, batch loss = 0.63 (13.8 examples/sec; 0.579 sec/batch; 50h:08m:34s remains)
INFO - root - 2017-12-11 05:35:51.908025: step 20640, loss = 0.69, batch loss = 0.63 (14.8 examples/sec; 0.542 sec/batch; 46h:58m:19s remains)
INFO - root - 2017-12-11 05:35:57.105330: step 20650, loss = 0.70, batch loss = 0.64 (27.9 examples/sec; 0.287 sec/batch; 24h:49m:46s remains)
INFO - root - 2017-12-11 05:36:02.662080: step 20660, loss = 0.69, batch loss = 0.63 (14.7 examples/sec; 0.546 sec/batch; 47h:17m:59s remains)
INFO - root - 2017-12-11 05:36:08.121396: step 20670, loss = 0.70, batch loss = 0.64 (15.2 examples/sec; 0.527 sec/batch; 45h:41m:15s remains)
INFO - root - 2017-12-11 05:36:13.655991: step 20680, loss = 0.69, batch loss = 0.63 (13.8 examples/sec; 0.579 sec/batch; 50h:09m:08s remains)
INFO - root - 2017-12-11 05:36:19.164907: step 20690, loss = 0.70, batch loss = 0.64 (14.1 examples/sec; 0.567 sec/batch; 49h:06m:56s remains)
INFO - root - 2017-12-11 05:36:24.597927: step 20700, loss = 0.70, batch loss = 0.65 (15.0 examples/sec; 0.532 sec/batch; 46h:04m:35s remains)
2017-12-11 05:36:25.221461: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.049862184 0.048403397 0.0476351 0.049084526 0.055814587 0.070791721 0.094367906 0.12032552 0.14405066 0.16269645 0.18027084 0.20211588 0.22373705 0.23982063 0.24278566][0.024269952 0.02347677 0.023057519 0.024527822 0.033040084 0.053090274 0.085721612 0.12520567 0.16447544 0.19806266 0.22729395 0.25532 0.2775034 0.2885699 0.28173029][0.0050439378 0.0046909144 0.0049256478 0.0062899934 0.014963517 0.037010375 0.074269272 0.12213989 0.17205876 0.2157816 0.25104108 0.27976114 0.29870671 0.3037928 0.28969285][0.0016517716 0.0011241875 0.0020918618 0.0036421434 0.011358819 0.032009043 0.067841 0.11593209 0.16787963 0.213723 0.24840942 0.27321512 0.28726274 0.28793374 0.27096704][0.0091728065 0.00789447 0.008745499 0.010225858 0.016589193 0.034390487 0.065834567 0.10951716 0.15801743 0.20127235 0.23281987 0.25354123 0.26490906 0.26510808 0.24992096][0.019108102 0.015565953 0.015207131 0.017084507 0.024275444 0.042610444 0.073205464 0.11436434 0.15899521 0.19773561 0.22456871 0.24117266 0.25186 0.25560248 0.24624909][0.029733224 0.021976685 0.019705838 0.023508543 0.035947945 0.061827354 0.099692158 0.14536652 0.19088295 0.22679693 0.24843001 0.25961733 0.26786196 0.27364138 0.2685478][0.041233163 0.029779047 0.026867094 0.035314739 0.056499775 0.093061514 0.14072572 0.19220847 0.23885365 0.27079031 0.28522775 0.28924486 0.29296547 0.29849154 0.29560226][0.051878527 0.037407845 0.035094872 0.049105279 0.078701653 0.12357441 0.17778802 0.23199531 0.27751938 0.30470183 0.31238666 0.31078282 0.3112604 0.31648389 0.3150571][0.063613892 0.045221251 0.04219424 0.058651764 0.091665581 0.13885273 0.19412637 0.24745378 0.29003173 0.31327546 0.31663686 0.31192836 0.31049356 0.31543112 0.31560364][0.075902641 0.05255089 0.046595935 0.061240748 0.091758065 0.13470969 0.18475214 0.23219365 0.26877594 0.28710017 0.28654218 0.27873757 0.274842 0.27854964 0.28028396][0.084121004 0.054719154 0.043968458 0.054026835 0.078639381 0.11361771 0.15425096 0.19226493 0.22103472 0.23386785 0.22960271 0.21871144 0.211799 0.21288642 0.21473655][0.08922343 0.051874265 0.034116842 0.038554531 0.056824632 0.082845524 0.11217573 0.13874783 0.15841295 0.16546211 0.15798698 0.14500104 0.13594413 0.13478337 0.1366652][0.087191179 0.043877169 0.020585708 0.020895949 0.034456324 0.053089153 0.072452582 0.088896088 0.10050827 0.10295604 0.093787 0.080264404 0.069950566 0.066419147 0.066903785][0.078303561 0.032348126 0.0069358465 0.0050759404 0.015557381 0.028989049 0.040951196 0.049653441 0.054895796 0.054064456 0.044857346 0.03221627 0.021415295 0.015184846 0.012581391]]...]
INFO - root - 2017-12-11 05:36:30.700334: step 20710, loss = 0.70, batch loss = 0.64 (14.8 examples/sec; 0.541 sec/batch; 46h:49m:27s remains)
INFO - root - 2017-12-11 05:36:36.168517: step 20720, loss = 0.71, batch loss = 0.65 (14.3 examples/sec; 0.559 sec/batch; 48h:24m:03s remains)
INFO - root - 2017-12-11 05:36:41.758885: step 20730, loss = 0.70, batch loss = 0.64 (14.3 examples/sec; 0.558 sec/batch; 48h:17m:17s remains)
INFO - root - 2017-12-11 05:36:47.262567: step 20740, loss = 0.71, batch loss = 0.66 (14.9 examples/sec; 0.535 sec/batch; 46h:22m:25s remains)
INFO - root - 2017-12-11 05:36:52.397007: step 20750, loss = 0.68, batch loss = 0.62 (14.6 examples/sec; 0.549 sec/batch; 47h:31m:43s remains)
INFO - root - 2017-12-11 05:36:58.010542: step 20760, loss = 0.68, batch loss = 0.62 (14.5 examples/sec; 0.552 sec/batch; 47h:45m:45s remains)
INFO - root - 2017-12-11 05:37:03.465837: step 20770, loss = 0.68, batch loss = 0.62 (13.9 examples/sec; 0.577 sec/batch; 49h:58m:50s remains)
INFO - root - 2017-12-11 05:37:08.987629: step 20780, loss = 0.70, batch loss = 0.64 (14.8 examples/sec; 0.539 sec/batch; 46h:41m:22s remains)
INFO - root - 2017-12-11 05:37:14.552627: step 20790, loss = 0.73, batch loss = 0.67 (13.3 examples/sec; 0.601 sec/batch; 52h:03m:15s remains)
INFO - root - 2017-12-11 05:37:20.059755: step 20800, loss = 0.71, batch loss = 0.65 (14.7 examples/sec; 0.543 sec/batch; 46h:59m:51s remains)
2017-12-11 05:37:20.641355: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.049635924 -0.051570952 -0.051581211 -0.05121161 -0.05161928 -0.053324014 -0.056691833 -0.061185185 -0.06537281 -0.067536332 -0.066804908 -0.064478725 -0.06212775 -0.060597803 -0.059644844][-0.04201477 -0.044438727 -0.0410665 -0.033450525 -0.025719784 -0.021891603 -0.0248751 -0.0344227 -0.046862368 -0.057274215 -0.062130682 -0.062104613 -0.059703786 -0.057547305 -0.056669168][-0.019377461 -0.020343682 -0.0095342686 0.01204029 0.03570943 0.051271636 0.050657082 0.0324901 0.0036428042 -0.02507926 -0.044277851 -0.052017238 -0.051630728 -0.048658442 -0.046922319][0.012443377 0.015325595 0.037896756 0.080551364 0.12896638 0.16397887 0.16923618 0.13979624 0.08635895 0.028487261 -0.015052665 -0.037322152 -0.041966639 -0.038342096 -0.034696925][0.042966824 0.051363297 0.087977111 0.15546857 0.23397505 0.29389989 0.30857632 0.26799974 0.18641166 0.093468726 0.01944023 -0.021941639 -0.033990808 -0.030141942 -0.023764146][0.060403682 0.074435651 0.12330901 0.21223484 0.31754908 0.40060914 0.42541122 0.37697065 0.27192914 0.14841168 0.047131222 -0.01153621 -0.030441556 -0.026396364 -0.017152581][0.0573752 0.074839413 0.1293274 0.22778484 0.34594658 0.44102368 0.47226796 0.42183304 0.30701265 0.1695082 0.055389956 -0.011211182 -0.032798544 -0.027725197 -0.016128685][0.035776805 0.052643623 0.10318434 0.19443236 0.30509302 0.39517567 0.42590642 0.38000757 0.27354702 0.14515586 0.0386413 -0.022666376 -0.041133638 -0.034202792 -0.021407396][0.00563126 0.017786095 0.055895794 0.12539682 0.21054038 0.2801649 0.30358085 0.26719588 0.18424784 0.084931307 0.0037906196 -0.0408335 -0.051520918 -0.042609196 -0.030121285][-0.021913191 -0.017024506 0.0044908361 0.045745447 0.097257875 0.1392554 0.1520593 0.12715802 0.074046366 0.012388527 -0.035706911 -0.059042156 -0.0606257 -0.050569285 -0.039779365][-0.040461861 -0.042802449 -0.03663798 -0.020802373 0.00044072917 0.017539863 0.02088871 0.0065400032 -0.019835606 -0.048046917 -0.067190222 -0.072683722 -0.067636684 -0.058075763 -0.049841173][-0.04948524 -0.056386191 -0.059983708 -0.060318995 -0.058443744 -0.057201046 -0.059465379 -0.066201039 -0.074788824 -0.081540786 -0.082954444 -0.078657061 -0.071255662 -0.063723005 -0.05822577][-0.052372947 -0.060050178 -0.066558778 -0.072669119 -0.078204758 -0.083076186 -0.087057836 -0.089730531 -0.090183444 -0.087948605 -0.082963817 -0.076430969 -0.070166886 -0.065358251 -0.062273838][-0.0527924 -0.058636051 -0.063440166 -0.068524338 -0.073680341 -0.078369617 -0.081766792 -0.083176523 -0.082216054 -0.079167731 -0.074687995 -0.069936387 -0.066070959 -0.063548669 -0.062108621][-0.052606951 -0.05632104 -0.058593515 -0.061136067 -0.063953377 -0.066829748 -0.069283478 -0.070786774 -0.0709587 -0.069803387 -0.067592472 -0.064986557 -0.062772617 -0.061338648 -0.060541157]]...]
INFO - root - 2017-12-11 05:37:26.129109: step 20810, loss = 0.70, batch loss = 0.64 (14.4 examples/sec; 0.555 sec/batch; 48h:04m:12s remains)
INFO - root - 2017-12-11 05:37:31.639710: step 20820, loss = 0.70, batch loss = 0.64 (14.9 examples/sec; 0.535 sec/batch; 46h:20m:08s remains)
INFO - root - 2017-12-11 05:37:37.149693: step 20830, loss = 0.69, batch loss = 0.63 (14.0 examples/sec; 0.572 sec/batch; 49h:29m:38s remains)
INFO - root - 2017-12-11 05:37:42.680968: step 20840, loss = 0.70, batch loss = 0.64 (14.6 examples/sec; 0.550 sec/batch; 47h:35m:27s remains)
INFO - root - 2017-12-11 05:37:47.789423: step 20850, loss = 0.67, batch loss = 0.61 (14.4 examples/sec; 0.555 sec/batch; 48h:05m:11s remains)
INFO - root - 2017-12-11 05:37:53.329419: step 20860, loss = 0.70, batch loss = 0.64 (14.6 examples/sec; 0.549 sec/batch; 47h:30m:13s remains)
INFO - root - 2017-12-11 05:37:58.769551: step 20870, loss = 0.70, batch loss = 0.64 (14.8 examples/sec; 0.540 sec/batch; 46h:45m:53s remains)
INFO - root - 2017-12-11 05:38:04.324367: step 20880, loss = 0.70, batch loss = 0.64 (14.7 examples/sec; 0.545 sec/batch; 47h:08m:43s remains)
INFO - root - 2017-12-11 05:38:09.768079: step 20890, loss = 0.68, batch loss = 0.63 (14.8 examples/sec; 0.540 sec/batch; 46h:44m:21s remains)
INFO - root - 2017-12-11 05:38:15.285400: step 20900, loss = 0.68, batch loss = 0.62 (14.1 examples/sec; 0.566 sec/batch; 48h:59m:15s remains)
2017-12-11 05:38:15.811485: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.015247921 -0.016976334 -0.016980123 -0.016197404 -0.014827671 -0.013210638 -0.011372846 -0.0098198606 -0.0090249181 -0.00839228 -0.0076642134 -0.0069216634 -0.0064708646 -0.0068041682 -0.0077346526][-0.016107513 -0.0175211 -0.017102163 -0.016069161 -0.014771317 -0.013388734 -0.01146135 -0.0091239093 -0.0070860037 -0.0057101608 -0.0055050976 -0.0064946236 -0.0080375 -0.0095964875 -0.010437992][-0.016270226 -0.017278211 -0.016557058 -0.015561005 -0.014568345 -0.013281503 -0.01047823 -0.0059666885 -0.0013016601 0.0013805812 0.00046872476 -0.0036224835 -0.0084591918 -0.011831679 -0.0125467][-0.015621589 -0.016453024 -0.015791677 -0.015196843 -0.014563664 -0.01270449 -0.0075721736 0.00099881762 0.0099502634 0.014588486 0.011928461 0.0032989481 -0.006281306 -0.012376643 -0.013346963][-0.014348936 -0.015381634 -0.015237468 -0.015303972 -0.01464144 -0.010844809 -0.0011935113 0.013797956 0.028588995 0.035442851 0.029981814 0.015038081 -0.00095481641 -0.010868345 -0.012852398][-0.013060099 -0.014655524 -0.01531992 -0.015827745 -0.01402154 -0.006212166 0.010492474 0.033973306 0.055293996 0.063558966 0.053551324 0.030445116 0.006635319 -0.0080247438 -0.011835393][-0.012529628 -0.014840151 -0.01614278 -0.016157497 -0.011328447 0.0029260488 0.028798215 0.061403371 0.087927863 0.09514533 0.078279309 0.045769617 0.013926775 -0.0054480429 -0.011285891][-0.012847989 -0.015733736 -0.017006511 -0.015071747 -0.0051739523 0.017323311 0.052681856 0.092402063 0.12032821 0.12286155 0.097326458 0.055902284 0.017678337 -0.0049523413 -0.011959326][-0.013417189 -0.01644833 -0.016881978 -0.011854555 0.0041424627 0.034826316 0.077534482 0.12003636 0.14434433 0.13887653 0.10459425 0.056984723 0.01596457 -0.0071637016 -0.013493468][-0.013545132 -0.016273603 -0.015390685 -0.0071283346 0.014185471 0.050614364 0.096299216 0.13620332 0.15260603 0.13786863 0.0972746 0.048175707 0.0090550669 -0.011151287 -0.014560144][-0.01296241 -0.015233605 -0.013199762 -0.0027909565 0.02116522 0.058853917 0.10196845 0.13456169 0.14124414 0.11898417 0.076762654 0.031740069 -0.00087164814 -0.015129624 -0.013858399][-0.012017684 -0.014074591 -0.011723042 -0.001263383 0.021501718 0.055333477 0.091081642 0.11408007 0.11249437 0.087050557 0.048614863 0.012303939 -0.010691328 -0.017329805 -0.010814232][-0.011490448 -0.013781022 -0.012186323 -0.0039007403 0.014207059 0.040348925 0.066334493 0.080401286 0.074388139 0.050926164 0.020627908 -0.0045752064 -0.01724647 -0.016557366 -0.0057719005][-0.0119201 -0.014777834 -0.014615322 -0.0097741131 0.0019987889 0.019264909 0.036018558 0.044022821 0.037946392 0.020351591 -3.5858156e-07 -0.014497538 -0.018521512 -0.012627339 0.00013504029][-0.013182145 -0.016626393 -0.017810058 -0.016213302 -0.010189513 -0.00037923278 0.0096521089 0.014836477 0.011615584 0.0013714512 -0.0096928859 -0.01582657 -0.014344575 -0.006449407 0.0053213886]]...]
INFO - root - 2017-12-11 05:38:21.244434: step 20910, loss = 0.69, batch loss = 0.63 (14.7 examples/sec; 0.545 sec/batch; 47h:08m:58s remains)
INFO - root - 2017-12-11 05:38:26.694688: step 20920, loss = 0.70, batch loss = 0.64 (14.9 examples/sec; 0.538 sec/batch; 46h:32m:07s remains)
INFO - root - 2017-12-11 05:38:32.132139: step 20930, loss = 0.70, batch loss = 0.64 (14.6 examples/sec; 0.547 sec/batch; 47h:20m:04s remains)
INFO - root - 2017-12-11 05:38:37.712076: step 20940, loss = 0.70, batch loss = 0.64 (14.1 examples/sec; 0.567 sec/batch; 49h:06m:44s remains)
INFO - root - 2017-12-11 05:38:42.929573: step 20950, loss = 0.72, batch loss = 0.66 (14.0 examples/sec; 0.571 sec/batch; 49h:26m:16s remains)
INFO - root - 2017-12-11 05:38:48.489185: step 20960, loss = 0.70, batch loss = 0.65 (14.8 examples/sec; 0.541 sec/batch; 46h:49m:09s remains)
INFO - root - 2017-12-11 05:38:53.964251: step 20970, loss = 0.68, batch loss = 0.62 (14.5 examples/sec; 0.553 sec/batch; 47h:50m:04s remains)
INFO - root - 2017-12-11 05:38:59.487628: step 20980, loss = 0.71, batch loss = 0.65 (14.3 examples/sec; 0.558 sec/batch; 48h:15m:59s remains)
INFO - root - 2017-12-11 05:39:05.060408: step 20990, loss = 0.70, batch loss = 0.64 (14.3 examples/sec; 0.561 sec/batch; 48h:33m:25s remains)
INFO - root - 2017-12-11 05:39:10.615295: step 21000, loss = 0.68, batch loss = 0.62 (14.6 examples/sec; 0.548 sec/batch; 47h:24m:08s remains)
2017-12-11 05:39:11.214922: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.16302283 0.17033765 0.17159934 0.17849204 0.18401895 0.17861137 0.16451469 0.14427368 0.12499372 0.11888192 0.12085801 0.12078603 0.10572091 0.079092719 0.041496441][0.31987965 0.33577716 0.33475855 0.33747628 0.33739638 0.32523584 0.30581859 0.27984723 0.25341776 0.24113367 0.24013838 0.23964116 0.22076373 0.18725888 0.13720936][0.48011026 0.50756931 0.50390947 0.4997279 0.49168429 0.4736439 0.45285547 0.42632595 0.39561585 0.37419546 0.36505452 0.35924494 0.33395621 0.29233062 0.23050007][0.61190045 0.64809686 0.63902813 0.6267004 0.6130048 0.59573638 0.58282471 0.56462467 0.53549075 0.50368053 0.47906318 0.45753407 0.41668695 0.36179563 0.28824016][0.71499753 0.74613225 0.72281194 0.69841874 0.68103433 0.67191654 0.67687166 0.67603028 0.65440542 0.61354274 0.569561 0.5239948 0.45882466 0.38444862 0.298845][0.79027611 0.80004239 0.75113833 0.70768958 0.68508804 0.68777394 0.71553165 0.736749 0.72718924 0.68311834 0.62454224 0.55622381 0.46609816 0.37110826 0.27477628][0.83127761 0.80918622 0.72915274 0.66307557 0.63309795 0.64442635 0.68867236 0.72581959 0.72788721 0.69037318 0.6318357 0.5547322 0.45051008 0.3420119 0.23971938][0.83241171 0.77949929 0.67329156 0.58682084 0.54629683 0.55704546 0.60498977 0.64772958 0.65996742 0.64050823 0.60035539 0.53395993 0.432229 0.32134089 0.21717878][0.788792 0.7196542 0.60155129 0.50259978 0.44963706 0.44913644 0.48679617 0.52362454 0.5414263 0.54578453 0.53600347 0.49540457 0.41093257 0.3094652 0.2092008][0.69593275 0.627733 0.514576 0.414072 0.3513802 0.33456802 0.35193831 0.3725251 0.38875711 0.41513538 0.43900827 0.43243447 0.37763411 0.29860598 0.21156114][0.541757 0.48824215 0.39678231 0.31058407 0.249011 0.21895319 0.21200837 0.20937201 0.21650735 0.25632843 0.3076376 0.33499035 0.31683487 0.2705301 0.20604146][0.34016672 0.3077662 0.2497602 0.19289105 0.14643903 0.11260698 0.086247653 0.061623991 0.05542076 0.095850609 0.15956715 0.20957899 0.22258621 0.20762636 0.16739853][0.13210373 0.12025705 0.097155362 0.073700346 0.049276 0.021663897 -0.011285201 -0.046130762 -0.061412971 -0.028793193 0.031672642 0.087036625 0.1151238 0.11872549 0.096093707][-0.031426929 -0.02870677 -0.026396573 -0.025328862 -0.030841501 -0.047071893 -0.074637912 -0.10699393 -0.12426897 -0.10239993 -0.055985112 -0.010256401 0.017076315 0.025924141 0.013341825][-0.12345072 -0.11555524 -0.10233201 -0.090928346 -0.086579673 -0.093037955 -0.11097319 -0.13462074 -0.14900282 -0.13679977 -0.10718602 -0.076863386 -0.05767088 -0.050481677 -0.05751713]]...]
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-SGD-lr0.001-clip50-initfromfixqian30k-fixqian
INFO - root - 2017-12-11 05:39:16.731664: step 21010, loss = 0.70, batch loss = 0.64 (14.6 examples/sec; 0.548 sec/batch; 47h:27m:09s remains)
INFO - root - 2017-12-11 05:39:22.229527: step 21020, loss = 0.69, batch loss = 0.63 (14.5 examples/sec; 0.553 sec/batch; 47h:52m:32s remains)
INFO - root - 2017-12-11 05:39:27.723029: step 21030, loss = 0.70, batch loss = 0.64 (14.3 examples/sec; 0.559 sec/batch; 48h:19m:53s remains)
INFO - root - 2017-12-11 05:39:33.095347: step 21040, loss = 0.70, batch loss = 0.64 (15.8 examples/sec; 0.505 sec/batch; 43h:40m:46s remains)
INFO - root - 2017-12-11 05:39:38.648280: step 21050, loss = 0.69, batch loss = 0.63 (14.1 examples/sec; 0.567 sec/batch; 49h:02m:28s remains)
INFO - root - 2017-12-11 05:39:44.178224: step 21060, loss = 0.72, batch loss = 0.66 (14.6 examples/sec; 0.546 sec/batch; 47h:14m:43s remains)
INFO - root - 2017-12-11 05:39:49.787033: step 21070, loss = 0.69, batch loss = 0.63 (14.3 examples/sec; 0.560 sec/batch; 48h:26m:00s remains)
INFO - root - 2017-12-11 05:39:55.312014: step 21080, loss = 0.72, batch loss = 0.66 (14.9 examples/sec; 0.539 sec/batch; 46h:35m:30s remains)
INFO - root - 2017-12-11 05:40:00.762806: step 21090, loss = 0.69, batch loss = 0.63 (14.7 examples/sec; 0.544 sec/batch; 47h:03m:16s remains)
INFO - root - 2017-12-11 05:40:06.316807: step 21100, loss = 0.68, batch loss = 0.62 (14.3 examples/sec; 0.561 sec/batch; 48h:31m:10s remains)
2017-12-11 05:40:06.928309: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.066690817 0.064039707 0.067052208 0.075934723 0.085192375 0.093239829 0.097753122 0.097735249 0.09638197 0.09662316 0.099036105 0.10162015 0.10227174 0.10090312 0.09773507][0.097800836 0.099687688 0.10818223 0.12232219 0.13394624 0.14144285 0.14306372 0.13862546 0.13469809 0.13639554 0.1450336 0.1556388 0.16306783 0.16640198 0.16551378][0.1155746 0.12485823 0.13911945 0.15617888 0.16630121 0.16868888 0.1636768 0.15211722 0.14364085 0.14467077 0.15705478 0.17401384 0.18861769 0.19936094 0.2052061][0.12308608 0.142399 0.1637644 0.18540721 0.19775186 0.1994835 0.19127803 0.17399599 0.15878423 0.15130417 0.15453897 0.16488425 0.17858332 0.19468229 0.20879176][0.13234332 0.16601114 0.19984344 0.23565885 0.26496419 0.28098533 0.27927098 0.25907302 0.23236153 0.2024108 0.17605628 0.15968516 0.15832914 0.1710626 0.18771489][0.15271921 0.20548131 0.26008165 0.32506722 0.39194557 0.4411585 0.45691517 0.43622604 0.39245009 0.32587245 0.25071445 0.18902089 0.15778531 0.15448499 0.16220334][0.18225884 0.25575453 0.33633411 0.44034645 0.55677068 0.64862126 0.68638724 0.6657809 0.60202777 0.49352205 0.36431086 0.25261113 0.18544728 0.15678498 0.14483717][0.20777009 0.2970517 0.39959091 0.53665888 0.69306177 0.81738877 0.87012315 0.84805709 0.76762652 0.62723976 0.4583433 0.30994487 0.21387628 0.16091211 0.12831813][0.21567862 0.30915815 0.41925782 0.56633359 0.73156118 0.85994589 0.91109073 0.88533282 0.79788357 0.64812982 0.46924537 0.31175703 0.20599943 0.14155918 0.098634817][0.20428938 0.28775236 0.3855021 0.512021 0.64797539 0.74754769 0.779058 0.74765861 0.66512477 0.53141814 0.37490615 0.23830016 0.14534138 0.087257206 0.050186846][0.18754749 0.25111714 0.32094106 0.40489578 0.48697206 0.53846514 0.54167068 0.50547278 0.43745843 0.33696592 0.22358608 0.12654257 0.061359819 0.023022348 0.0047435383][0.18437271 0.22625557 0.2636151 0.30084425 0.32780996 0.33333132 0.31328493 0.277757 0.22984876 0.16716446 0.099965036 0.043907192 0.0084246751 -0.0067860074 -0.0023502351][0.20422947 0.22949688 0.24126215 0.24436088 0.23481041 0.21315622 0.18449658 0.15814832 0.13275169 0.10307151 0.071496308 0.044473138 0.029525574 0.030926328 0.05191382][0.23401 0.25097042 0.25043276 0.23985353 0.22002973 0.19537385 0.17448969 0.16497241 0.16165769 0.15654643 0.14612815 0.13300574 0.12568478 0.1320194 0.15683085][0.24352397 0.25798023 0.25698322 0.24973091 0.2397448 0.22948653 0.22614892 0.2357747 0.25107127 0.26273665 0.26303226 0.25394315 0.24559501 0.24729417 0.26400155]]...]
INFO - root - 2017-12-11 05:40:12.500923: step 21110, loss = 0.71, batch loss = 0.65 (14.2 examples/sec; 0.565 sec/batch; 48h:53m:35s remains)
INFO - root - 2017-12-11 05:40:18.056305: step 21120, loss = 0.70, batch loss = 0.64 (14.7 examples/sec; 0.545 sec/batch; 47h:07m:50s remains)
INFO - root - 2017-12-11 05:40:23.638386: step 21130, loss = 0.68, batch loss = 0.62 (14.3 examples/sec; 0.560 sec/batch; 48h:27m:32s remains)
INFO - root - 2017-12-11 05:40:28.837857: step 21140, loss = 0.69, batch loss = 0.63 (14.4 examples/sec; 0.556 sec/batch; 48h:04m:12s remains)
INFO - root - 2017-12-11 05:40:34.364802: step 21150, loss = 0.70, batch loss = 0.64 (14.4 examples/sec; 0.555 sec/batch; 48h:01m:51s remains)
INFO - root - 2017-12-11 05:40:39.940105: step 21160, loss = 0.70, batch loss = 0.64 (14.4 examples/sec; 0.554 sec/batch; 47h:54m:35s remains)
INFO - root - 2017-12-11 05:40:45.499243: step 21170, loss = 0.69, batch loss = 0.63 (14.4 examples/sec; 0.556 sec/batch; 48h:05m:50s remains)
INFO - root - 2017-12-11 05:40:51.075892: step 21180, loss = 0.70, batch loss = 0.64 (14.7 examples/sec; 0.543 sec/batch; 46h:56m:18s remains)
INFO - root - 2017-12-11 05:40:56.581332: step 21190, loss = 0.69, batch loss = 0.64 (14.3 examples/sec; 0.560 sec/batch; 48h:24m:11s remains)
INFO - root - 2017-12-11 05:41:02.192153: step 21200, loss = 0.70, batch loss = 0.64 (15.0 examples/sec; 0.535 sec/batch; 46h:14m:41s remains)
2017-12-11 05:41:02.779648: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0050111925 0.062993966 0.14031406 0.20869052 0.25151747 0.26192409 0.24913213 0.22309953 0.19509965 0.17549604 0.16841809 0.17134753 0.17927565 0.18804936 0.1952519][-0.0029592135 0.066534147 0.14458364 0.213318 0.25742054 0.27411604 0.27507135 0.26814139 0.25829542 0.25143653 0.25052217 0.25355843 0.25847253 0.26393893 0.26751342][-0.0020154114 0.065158665 0.1411442 0.20959638 0.25750253 0.28507936 0.30540985 0.322558 0.333645 0.3383511 0.33826357 0.33417836 0.32808271 0.32262498 0.31778085][-0.0033025057 0.060700297 0.13600346 0.20777908 0.26471019 0.30857742 0.35140994 0.39244017 0.42073336 0.43076017 0.42412582 0.40492618 0.3799876 0.35667294 0.33920234][-0.0060950168 0.054314204 0.12963642 0.20690873 0.27609193 0.33836335 0.40236542 0.46277663 0.50123465 0.509257 0.48912191 0.44931743 0.40115666 0.35632479 0.32426584][-0.0082812961 0.049817897 0.12655254 0.21100445 0.29309779 0.37078 0.44859365 0.5183323 0.55633742 0.55322206 0.51305604 0.45028016 0.37928766 0.31513551 0.27233386][-0.0094416738 0.047962915 0.1268643 0.21777628 0.30959949 0.3959564 0.47685796 0.54237688 0.5679217 0.54534322 0.4823662 0.39926812 0.31269026 0.23948154 0.19610949][-0.011406075 0.045250084 0.12475397 0.218261 0.31353161 0.39990574 0.47278735 0.52260196 0.52824938 0.48469454 0.40407723 0.31031403 0.22063935 0.15072832 0.11534376][-0.014870377 0.039764818 0.11703429 0.20767231 0.2984395 0.37580779 0.43216 0.46025237 0.44590968 0.38870674 0.30325279 0.2130851 0.13357019 0.07712929 0.053990535][-0.017743081 0.034994602 0.10828952 0.19200684 0.27204028 0.33371562 0.36931902 0.37573433 0.34713691 0.28660512 0.20886829 0.13411634 0.073590666 0.035178583 0.024135293][-0.017400434 0.034099918 0.10320539 0.17875831 0.24598746 0.2904526 0.30643657 0.29556862 0.26029211 0.20578089 0.14534043 0.093585894 0.056431927 0.036842328 0.035594482][-0.0160411 0.034193933 0.10018894 0.17044179 0.22980799 0.26421809 0.26953647 0.25109088 0.21754146 0.17565396 0.13693564 0.10993463 0.095593892 0.09197434 0.096641414][-0.016006414 0.032522604 0.097066946 0.16666105 0.22614436 0.26135546 0.26831117 0.25363147 0.22844075 0.20091228 0.18133131 0.17336179 0.17429657 0.17887659 0.18414746][-0.016985871 0.029445669 0.09300454 0.16382696 0.22708189 0.26822817 0.28261754 0.2765027 0.26070783 0.24449791 0.23720442 0.23925965 0.24602883 0.25104463 0.25295725][-0.02011136 0.022307344 0.082065709 0.15083519 0.21459116 0.25837886 0.27725998 0.27722982 0.26793587 0.25878403 0.25756574 0.26280582 0.269333 0.27084303 0.26794326]]...]
INFO - root - 2017-12-11 05:41:08.309525: step 21210, loss = 0.69, batch loss = 0.63 (14.8 examples/sec; 0.541 sec/batch; 46h:48m:35s remains)
INFO - root - 2017-12-11 05:41:13.788356: step 21220, loss = 0.69, batch loss = 0.63 (14.7 examples/sec; 0.543 sec/batch; 46h:57m:06s remains)
INFO - root - 2017-12-11 05:41:19.264935: step 21230, loss = 0.69, batch loss = 0.63 (14.5 examples/sec; 0.552 sec/batch; 47h:41m:46s remains)
INFO - root - 2017-12-11 05:41:24.503331: step 21240, loss = 0.71, batch loss = 0.65 (14.8 examples/sec; 0.540 sec/batch; 46h:40m:09s remains)
INFO - root - 2017-12-11 05:41:29.992457: step 21250, loss = 0.68, batch loss = 0.63 (14.7 examples/sec; 0.546 sec/batch; 47h:10m:09s remains)
INFO - root - 2017-12-11 05:41:35.447875: step 21260, loss = 0.70, batch loss = 0.64 (14.3 examples/sec; 0.561 sec/batch; 48h:29m:51s remains)
INFO - root - 2017-12-11 05:41:40.993440: step 21270, loss = 0.72, batch loss = 0.66 (14.8 examples/sec; 0.541 sec/batch; 46h:43m:41s remains)
INFO - root - 2017-12-11 05:41:46.556978: step 21280, loss = 0.70, batch loss = 0.65 (14.2 examples/sec; 0.565 sec/batch; 48h:51m:05s remains)
INFO - root - 2017-12-11 05:41:52.067073: step 21290, loss = 0.70, batch loss = 0.64 (14.7 examples/sec; 0.545 sec/batch; 47h:08m:04s remains)
INFO - root - 2017-12-11 05:41:57.666826: step 21300, loss = 0.70, batch loss = 0.65 (14.1 examples/sec; 0.567 sec/batch; 49h:00m:21s remains)
2017-12-11 05:41:58.269616: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.057892088 0.083592363 0.11223635 0.13028781 0.13152212 0.12200036 0.10398266 0.081229724 0.057306655 0.0389135 0.027982747 0.019377239 0.014708009 0.02045087 0.042445675][0.06424541 0.086922124 0.11185395 0.1268689 0.12574835 0.11542272 0.099043429 0.0795293 0.060354657 0.0470578 0.040555391 0.03629332 0.033334691 0.037696008 0.059383769][0.071511671 0.0857702 0.1019287 0.11123575 0.10883215 0.10112555 0.090716533 0.078829259 0.067952417 0.062162023 0.061816894 0.063711971 0.063748568 0.067600004 0.088773258][0.074968934 0.077964939 0.08363378 0.08749263 0.086010486 0.0840634 0.083180867 0.081920259 0.080631152 0.082226872 0.087504476 0.095054284 0.097708367 0.10162296 0.12222434][0.077883556 0.069916211 0.06652505 0.067687325 0.069858588 0.075412855 0.084941059 0.094412513 0.10133808 0.107487 0.11479105 0.1244386 0.12726152 0.12996452 0.14792891][0.080581173 0.063326567 0.053492162 0.055363704 0.063704774 0.077681005 0.097313128 0.11649677 0.13017966 0.13811067 0.14369704 0.15133756 0.15236332 0.15294997 0.16556695][0.085376248 0.062796019 0.049703043 0.054599475 0.069826588 0.091195837 0.11820742 0.14406803 0.16254166 0.17128026 0.17344195 0.17595427 0.17350881 0.17132254 0.17594641][0.0929742 0.068303563 0.054421552 0.062670656 0.083190687 0.10912999 0.13996509 0.16983424 0.19218476 0.20230442 0.20145844 0.19766217 0.19099705 0.18562378 0.18083973][0.10094225 0.0777302 0.065609433 0.076928452 0.10041907 0.12759131 0.15903848 0.19138202 0.2182804 0.23218276 0.23078559 0.22182363 0.21072395 0.20093708 0.1856209][0.11152323 0.092962593 0.084424958 0.097804062 0.12138874 0.14622249 0.17500839 0.20775822 0.2384879 0.25676802 0.256424 0.24455337 0.23038691 0.21607383 0.19120421][0.11882924 0.10603217 0.10151186 0.11492172 0.13596459 0.15658945 0.18093415 0.21161458 0.24315214 0.26387918 0.2650148 0.25314483 0.23887849 0.22199637 0.19123402][0.11825162 0.11162142 0.11105897 0.12262917 0.13879187 0.15381578 0.17214704 0.1974178 0.22460794 0.24316353 0.24431373 0.23470668 0.2237504 0.20778877 0.17619279][0.11520267 0.11346591 0.11570697 0.12359996 0.13299949 0.14118147 0.15232927 0.16966455 0.18853521 0.2006914 0.19989158 0.19359736 0.18820646 0.17633811 0.14867875][0.11422298 0.11561757 0.11850853 0.12160542 0.12400715 0.12590192 0.1305884 0.13981818 0.14992161 0.15530783 0.15248396 0.14939521 0.14895813 0.14170976 0.12038656][0.11648735 0.11915667 0.12056796 0.11888505 0.11609762 0.11477297 0.11558292 0.11814139 0.12092438 0.12125594 0.11709661 0.11518516 0.1165041 0.11232875 0.098018274]]...]
INFO - root - 2017-12-11 05:42:03.817772: step 21310, loss = 0.69, batch loss = 0.63 (14.8 examples/sec; 0.539 sec/batch; 46h:37m:40s remains)
INFO - root - 2017-12-11 05:42:09.297262: step 21320, loss = 0.70, batch loss = 0.65 (14.6 examples/sec; 0.548 sec/batch; 47h:21m:56s remains)
INFO - root - 2017-12-11 05:42:14.396958: step 21330, loss = 0.69, batch loss = 0.64 (15.2 examples/sec; 0.526 sec/batch; 45h:30m:27s remains)
INFO - root - 2017-12-11 05:42:19.977593: step 21340, loss = 0.71, batch loss = 0.65 (13.9 examples/sec; 0.574 sec/batch; 49h:34m:46s remains)
INFO - root - 2017-12-11 05:42:25.479210: step 21350, loss = 0.69, batch loss = 0.63 (14.9 examples/sec; 0.538 sec/batch; 46h:27m:24s remains)
INFO - root - 2017-12-11 05:42:30.928270: step 21360, loss = 0.70, batch loss = 0.64 (14.6 examples/sec; 0.549 sec/batch; 47h:27m:04s remains)
INFO - root - 2017-12-11 05:42:36.564061: step 21370, loss = 0.68, batch loss = 0.63 (13.9 examples/sec; 0.574 sec/batch; 49h:34m:12s remains)
INFO - root - 2017-12-11 05:42:42.085729: step 21380, loss = 0.70, batch loss = 0.64 (14.8 examples/sec; 0.541 sec/batch; 46h:44m:13s remains)
INFO - root - 2017-12-11 05:42:47.658014: step 21390, loss = 0.71, batch loss = 0.65 (14.4 examples/sec; 0.555 sec/batch; 47h:58m:44s remains)
INFO - root - 2017-12-11 05:42:53.194802: step 21400, loss = 0.69, batch loss = 0.63 (14.5 examples/sec; 0.551 sec/batch; 47h:35m:50s remains)
2017-12-11 05:42:53.812044: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.091156118 0.10124378 0.097895607 0.080639705 0.050328717 0.015151775 -0.0058402349 -0.0085686361 0.0016100492 0.016038133 0.032903336 0.058316004 0.085677035 0.10020018 0.090689316][0.12575676 0.1429998 0.13782032 0.10773046 0.060681406 0.013629417 -0.012142277 -0.01721262 -0.010886377 0.00010587311 0.016148945 0.04340395 0.072902225 0.087872274 0.076262839][0.18348226 0.2202201 0.22631419 0.19169508 0.1282855 0.063013926 0.022147203 0.0031429406 -0.0049636252 -0.0074840281 -0.0027948685 0.01589776 0.040237695 0.054546092 0.04596705][0.24841267 0.31395748 0.34614417 0.32178774 0.25087011 0.16800794 0.10535917 0.061586786 0.027306169 -0.00010643768 -0.017791376 -0.017649133 -0.0051185763 0.0078433361 0.0080279922][0.29830575 0.39570808 0.46439555 0.46614692 0.40328652 0.31254461 0.22929756 0.1547561 0.085360296 0.024573732 -0.021955514 -0.044863343 -0.04632261 -0.03308915 -0.018521221][0.32452053 0.44808188 0.55290395 0.58877414 0.54751074 0.46216518 0.3666741 0.26390705 0.1590566 0.064766616 -0.0090185013 -0.052503452 -0.066086642 -0.052025095 -0.022904297][0.32786337 0.464095 0.5920577 0.65760952 0.6429401 0.57399374 0.47885066 0.35935906 0.22886609 0.10959434 0.01673549 -0.0398898 -0.060761493 -0.0459334 -0.006527985][0.321886 0.45734614 0.58992606 0.67070341 0.6760596 0.62466872 0.5377357 0.41467232 0.27386218 0.14416458 0.044644557 -0.015767748 -0.039582737 -0.026250489 0.01594973][0.31437644 0.44015726 0.56375343 0.64617717 0.66106188 0.6213308 0.54360145 0.42644212 0.29005075 0.16515061 0.070948765 0.013648149 -0.012224015 -0.0055713351 0.03046654][0.30227184 0.41304654 0.5204308 0.59481353 0.6091792 0.57360893 0.50456351 0.40221474 0.28458065 0.178121 0.098231889 0.04767203 0.01894513 0.014494126 0.037483737][0.27053049 0.3618558 0.44887915 0.509785 0.51814067 0.48383561 0.42673555 0.35063669 0.26710951 0.19183494 0.13285014 0.090446264 0.05800854 0.040551614 0.047554582][0.20949274 0.27554771 0.33881459 0.38451585 0.38808778 0.35911235 0.3204498 0.2797398 0.24086426 0.20593077 0.17358568 0.14193009 0.10748247 0.079064012 0.071159489][0.12904295 0.16807529 0.20799641 0.24016331 0.243551 0.2253885 0.20814167 0.20218301 0.20690466 0.21415156 0.21336819 0.19887531 0.16938487 0.13673475 0.11828127][0.045439865 0.06076467 0.082001038 0.10450695 0.11161144 0.10649029 0.10749745 0.12612866 0.16226402 0.20436226 0.23591679 0.24606702 0.232499 0.20676036 0.1858575][-0.019768717 -0.019035568 -0.0078686681 0.010155137 0.02172091 0.025195111 0.03306834 0.057417102 0.1028287 0.16194478 0.21852757 0.25738958 0.27139783 0.26636621 0.25429606]]...]
INFO - root - 2017-12-11 05:42:59.293784: step 21410, loss = 0.70, batch loss = 0.64 (14.7 examples/sec; 0.544 sec/batch; 47h:01m:40s remains)
INFO - root - 2017-12-11 05:43:04.804171: step 21420, loss = 0.69, batch loss = 0.63 (14.7 examples/sec; 0.544 sec/batch; 47h:02m:22s remains)
INFO - root - 2017-12-11 05:43:09.985689: step 21430, loss = 0.70, batch loss = 0.64 (14.7 examples/sec; 0.545 sec/batch; 47h:04m:07s remains)
INFO - root - 2017-12-11 05:43:15.456871: step 21440, loss = 0.69, batch loss = 0.63 (15.1 examples/sec; 0.530 sec/batch; 45h:50m:08s remains)
INFO - root - 2017-12-11 05:43:21.042467: step 21450, loss = 0.71, batch loss = 0.65 (13.9 examples/sec; 0.576 sec/batch; 49h:45m:36s remains)
INFO - root - 2017-12-11 05:43:26.588807: step 21460, loss = 0.71, batch loss = 0.65 (14.7 examples/sec; 0.543 sec/batch; 46h:55m:51s remains)
INFO - root - 2017-12-11 05:43:32.119241: step 21470, loss = 0.70, batch loss = 0.64 (15.4 examples/sec; 0.519 sec/batch; 44h:50m:35s remains)
INFO - root - 2017-12-11 05:43:37.676854: step 21480, loss = 0.70, batch loss = 0.64 (14.5 examples/sec; 0.553 sec/batch; 47h:46m:06s remains)
INFO - root - 2017-12-11 05:43:43.125662: step 21490, loss = 0.69, batch loss = 0.63 (14.5 examples/sec; 0.550 sec/batch; 47h:30m:11s remains)
INFO - root - 2017-12-11 05:43:48.520774: step 21500, loss = 0.70, batch loss = 0.64 (14.9 examples/sec; 0.538 sec/batch; 46h:29m:16s remains)
2017-12-11 05:43:49.093487: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.304337 0.27779078 0.22860335 0.17887983 0.14225656 0.12305955 0.11420647 0.10960961 0.10614498 0.10376815 0.10123844 0.098474018 0.099351704 0.11061878 0.13524944][0.28679419 0.24903114 0.1933115 0.1428685 0.11050504 0.099436879 0.10178589 0.10978029 0.11767454 0.12328225 0.12480793 0.12265088 0.12232439 0.1321464 0.15748382][0.23516494 0.19384903 0.14163926 0.099813357 0.078743719 0.081108019 0.099488758 0.12367969 0.14369327 0.15504606 0.15648636 0.15080982 0.14635575 0.15337701 0.17917584][0.17549303 0.138352 0.097409718 0.069987245 0.0632835 0.080311164 0.1144281 0.15248072 0.1801153 0.19094534 0.18667559 0.1742662 0.16484134 0.16974814 0.19625409][0.12763141 0.10209724 0.077693775 0.066574484 0.072761357 0.1003519 0.14343631 0.18694566 0.21432669 0.21878044 0.20597409 0.18651973 0.17325586 0.17642057 0.20116745][0.11162398 0.10284029 0.094654366 0.094653361 0.10585009 0.13542476 0.17826641 0.21844096 0.23963366 0.23570359 0.21496788 0.18973914 0.17317586 0.17324694 0.19130273][0.13444951 0.14008558 0.13960788 0.1395036 0.14495617 0.16854045 0.20532022 0.23865607 0.25371638 0.24502824 0.22034273 0.19079146 0.16919817 0.16214679 0.16841075][0.18796836 0.20098802 0.19839945 0.1887331 0.18119073 0.19445769 0.22340681 0.24950869 0.26029813 0.25073928 0.22563584 0.19290461 0.16456161 0.14701821 0.13945678][0.24854136 0.26348242 0.25450987 0.23275129 0.21133204 0.21404947 0.23575379 0.25634333 0.265587 0.25809127 0.23551914 0.20150721 0.1660416 0.13716678 0.11718804][0.29349425 0.30527133 0.28911209 0.25785396 0.22711881 0.22319631 0.24123871 0.25989676 0.26991928 0.26612327 0.24838486 0.21677123 0.17804757 0.14127265 0.11213749][0.31583285 0.31989822 0.2967023 0.26069167 0.22770022 0.22300334 0.24176313 0.26257911 0.27546591 0.27522394 0.26250309 0.23679347 0.20104136 0.16228883 0.12719496][0.31715479 0.31063798 0.28086829 0.2442112 0.21441236 0.21230967 0.23294607 0.25748426 0.27511308 0.27964088 0.27299336 0.25601709 0.22828072 0.19281782 0.15551358][0.30172893 0.28638113 0.25229889 0.21742533 0.19301011 0.19380602 0.21483395 0.24157856 0.26429877 0.27538472 0.27602911 0.26815706 0.24933998 0.21944283 0.18337582][0.27717718 0.25788742 0.22243094 0.19016722 0.17099266 0.17416331 0.19440222 0.22157148 0.2481318 0.26542082 0.27253971 0.27136591 0.2595731 0.23592143 0.20472264][0.24536952 0.22396533 0.18800741 0.15786602 0.14290971 0.14808871 0.16704509 0.19278798 0.22029583 0.24124664 0.2530804 0.25655362 0.25083864 0.2351706 0.21297638]]...]
INFO - root - 2017-12-11 05:43:54.617742: step 21510, loss = 0.69, batch loss = 0.63 (15.0 examples/sec; 0.533 sec/batch; 46h:01m:23s remains)
INFO - root - 2017-12-11 05:44:00.141456: step 21520, loss = 0.70, batch loss = 0.64 (14.2 examples/sec; 0.561 sec/batch; 48h:30m:04s remains)
INFO - root - 2017-12-11 05:44:05.409920: step 21530, loss = 0.69, batch loss = 0.63 (14.6 examples/sec; 0.547 sec/batch; 47h:16m:41s remains)
INFO - root - 2017-12-11 05:44:10.929270: step 21540, loss = 0.69, batch loss = 0.63 (14.6 examples/sec; 0.549 sec/batch; 47h:26m:36s remains)
INFO - root - 2017-12-11 05:44:16.355417: step 21550, loss = 0.69, batch loss = 0.63 (14.2 examples/sec; 0.563 sec/batch; 48h:37m:24s remains)
INFO - root - 2017-12-11 05:44:21.887755: step 21560, loss = 0.69, batch loss = 0.63 (14.3 examples/sec; 0.560 sec/batch; 48h:20m:29s remains)
INFO - root - 2017-12-11 05:44:27.426381: step 21570, loss = 0.70, batch loss = 0.64 (14.8 examples/sec; 0.540 sec/batch; 46h:37m:37s remains)
INFO - root - 2017-12-11 05:44:32.869552: step 21580, loss = 0.70, batch loss = 0.64 (15.0 examples/sec; 0.533 sec/batch; 45h:59m:37s remains)
INFO - root - 2017-12-11 05:44:38.322163: step 21590, loss = 0.70, batch loss = 0.64 (14.6 examples/sec; 0.547 sec/batch; 47h:14m:13s remains)
INFO - root - 2017-12-11 05:44:43.974664: step 21600, loss = 0.69, batch loss = 0.63 (13.9 examples/sec; 0.574 sec/batch; 49h:36m:36s remains)
2017-12-11 05:44:44.601939: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.042242214 0.032804091 0.031650305 0.043639876 0.069054388 0.10243016 0.12943445 0.13435645 0.11339295 0.074286744 0.02927644 -0.00964426 -0.0341783 -0.045022119 -0.046833262][0.050017931 0.042306405 0.042952061 0.056980908 0.087128565 0.12555851 0.1561541 0.16210814 0.13901959 0.095654577 0.04448957 -0.00092344958 -0.030469406 -0.044565592 -0.048039697][0.090624727 0.08636339 0.088424928 0.10354011 0.13756008 0.17883088 0.2087075 0.21004468 0.17914379 0.12687236 0.065852389 0.011669939 -0.023743333 -0.04139686 -0.046225883][0.16818209 0.16826485 0.1699191 0.18280065 0.21603197 0.25385442 0.27556282 0.26465455 0.21960743 0.15444049 0.0817922 0.018747201 -0.021487145 -0.040963732 -0.045807336][0.25647759 0.26007947 0.25927234 0.26786506 0.29738081 0.32821393 0.33796686 0.31187418 0.25118506 0.17305981 0.090037055 0.019921277 -0.023553757 -0.0432996 -0.046931811][0.32277384 0.32702973 0.32314295 0.32907978 0.35715285 0.38405642 0.38472822 0.34649009 0.27294591 0.1845158 0.093706608 0.018386774 -0.027179483 -0.046811312 -0.0492509][0.34314924 0.34303036 0.33543897 0.34248024 0.37590811 0.40836677 0.41003203 0.36889717 0.2904171 0.19690762 0.10086612 0.021060662 -0.027115656 -0.047709566 -0.050287828][0.3376222 0.32770148 0.31450453 0.32368496 0.36609626 0.41090602 0.42230409 0.38697267 0.31074867 0.21627823 0.11654546 0.031789575 -0.020644289 -0.044253144 -0.049028825][0.32475728 0.30432087 0.28451839 0.29376444 0.34191874 0.39697152 0.41966495 0.39476529 0.3268078 0.23606889 0.13594705 0.047951985 -0.0089512561 -0.036976915 -0.045730196][0.29870063 0.27156904 0.24630125 0.25268927 0.29977775 0.35778171 0.38828382 0.37484014 0.3195526 0.23825786 0.14408772 0.058591753 0.00086090091 -0.030058397 -0.042195465][0.24736255 0.21979563 0.19318613 0.19524507 0.23439857 0.28660753 0.31873351 0.31462881 0.27407789 0.20778954 0.12746197 0.052729081 0.00093773653 -0.028091867 -0.040572152][0.16606659 0.14279267 0.11899105 0.11712728 0.14518367 0.18692957 0.21695386 0.22058232 0.19577852 0.1486015 0.088456273 0.031357802 -0.00854453 -0.031057196 -0.040810049][0.085622564 0.068019569 0.048527062 0.043008577 0.058980167 0.087902762 0.11242086 0.12020571 0.10802972 0.078832425 0.039638162 0.0020770303 -0.023599783 -0.037415035 -0.042679358][0.032523945 0.020711489 0.0060128504 -0.0018357791 0.0037470644 0.019978082 0.0360915 0.042370614 0.0358082 0.018260788 -0.0048725479 -0.026311286 -0.03945699 -0.044966187 -0.045448415][0.015856853 0.0089964662 -0.00058332639 -0.0081280172 -0.0081584463 -0.00064230443 0.007236775 0.0075387275 -0.001185308 -0.016057758 -0.032286692 -0.04512047 -0.050780285 -0.050866876 -0.047826853]]...]
INFO - root - 2017-12-11 05:44:50.075492: step 21610, loss = 0.69, batch loss = 0.63 (14.4 examples/sec; 0.554 sec/batch; 47h:49m:42s remains)
INFO - root - 2017-12-11 05:44:55.263356: step 21620, loss = 0.70, batch loss = 0.65 (31.2 examples/sec; 0.257 sec/batch; 22h:09m:13s remains)
INFO - root - 2017-12-11 05:45:00.844406: step 21630, loss = 0.70, batch loss = 0.65 (14.6 examples/sec; 0.547 sec/batch; 47h:14m:36s remains)
INFO - root - 2017-12-11 05:45:06.296120: step 21640, loss = 0.70, batch loss = 0.64 (14.6 examples/sec; 0.547 sec/batch; 47h:13m:00s remains)
INFO - root - 2017-12-11 05:45:11.759064: step 21650, loss = 0.69, batch loss = 0.64 (14.6 examples/sec; 0.546 sec/batch; 47h:09m:56s remains)
INFO - root - 2017-12-11 05:45:17.327204: step 21660, loss = 0.70, batch loss = 0.64 (14.7 examples/sec; 0.544 sec/batch; 46h:56m:16s remains)
INFO - root - 2017-12-11 05:45:22.782789: step 21670, loss = 0.70, batch loss = 0.64 (14.6 examples/sec; 0.548 sec/batch; 47h:17m:25s remains)
INFO - root - 2017-12-11 05:45:28.324946: step 21680, loss = 0.69, batch loss = 0.63 (14.3 examples/sec; 0.558 sec/batch; 48h:12m:44s remains)
INFO - root - 2017-12-11 05:45:33.921933: step 21690, loss = 0.69, batch loss = 0.63 (14.6 examples/sec; 0.548 sec/batch; 47h:18m:31s remains)
INFO - root - 2017-12-11 05:45:39.391891: step 21700, loss = 0.70, batch loss = 0.64 (14.6 examples/sec; 0.549 sec/batch; 47h:22m:56s remains)
2017-12-11 05:45:39.985484: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.21432558 0.22808735 0.23160873 0.22503623 0.22349875 0.23071088 0.24543372 0.25407678 0.24803615 0.22872111 0.18964796 0.13339554 0.068966009 0.011860398 -0.024664948][0.27427191 0.29950088 0.30966237 0.30505875 0.30244014 0.30891097 0.32410493 0.33326471 0.32566378 0.30008826 0.24972324 0.17731397 0.0953143 0.023505162 -0.022348031][0.31631026 0.35371614 0.36967671 0.36665165 0.36217719 0.36768603 0.38381508 0.39524177 0.38861263 0.35917032 0.29939172 0.21298069 0.11630731 0.032672256 -0.020490052][0.331658 0.3779451 0.39785311 0.39720324 0.39325678 0.40142575 0.42274162 0.44062957 0.43800291 0.40566909 0.33701509 0.23772699 0.12815332 0.035198662 -0.023061097][0.33893356 0.38695389 0.40457058 0.40268791 0.39847243 0.4106473 0.4398531 0.46707767 0.4705489 0.43735096 0.36196163 0.25242493 0.13240671 0.032310687 -0.029320443][0.35032484 0.38996688 0.39642927 0.38660112 0.3780165 0.39152861 0.42668328 0.46280012 0.47369766 0.44320676 0.36638185 0.25304848 0.12894486 0.026046181 -0.036688171][0.3669394 0.38991463 0.37955287 0.35816464 0.34435952 0.3586283 0.39813465 0.44144124 0.4594 0.43386629 0.35902688 0.2464802 0.12309419 0.020750077 -0.04139806][0.38143587 0.38802955 0.36313555 0.33280441 0.3166779 0.33327094 0.37682056 0.42519918 0.44782749 0.42615438 0.35285366 0.24054223 0.11767369 0.016287545 -0.04496355][0.38840052 0.38366431 0.35143289 0.31943494 0.30675265 0.32731983 0.37341732 0.4237093 0.44694337 0.42445511 0.34888589 0.23446321 0.11121586 0.010655144 -0.04941906][0.3920275 0.38388693 0.35205135 0.32496217 0.31975251 0.34347236 0.3877013 0.43390048 0.45228976 0.42466736 0.34451419 0.22802925 0.10579751 0.007209328 -0.051708728][0.38257194 0.38045526 0.3570019 0.34154677 0.34835383 0.37678194 0.41791576 0.45705879 0.467502 0.43175292 0.34493756 0.22574277 0.10479175 0.0086087044 -0.049697038][0.35223073 0.360589 0.35011336 0.35007748 0.37180525 0.40877324 0.45112491 0.4862881 0.49019358 0.44668484 0.35280126 0.22950739 0.10786506 0.012585443 -0.046039294][0.29886684 0.31631464 0.3179667 0.33090654 0.36375186 0.40726385 0.45163009 0.48507351 0.48520753 0.43788651 0.34216261 0.21959747 0.10084362 0.00917141 -0.047260553][0.2186434 0.23834179 0.24629021 0.26508412 0.30056357 0.34367147 0.38567427 0.4160285 0.41461825 0.36998886 0.28287283 0.1731279 0.068718821 -0.010036858 -0.057153329][0.11983201 0.13423899 0.14129405 0.15743743 0.18621656 0.22061059 0.25392583 0.27787179 0.27653247 0.2414908 0.17407353 0.090286531 0.012776398 -0.043013226 -0.073729545]]...]
INFO - root - 2017-12-11 05:45:45.527046: step 21710, loss = 0.72, batch loss = 0.66 (14.6 examples/sec; 0.549 sec/batch; 47h:25m:36s remains)
INFO - root - 2017-12-11 05:45:50.686450: step 21720, loss = 0.69, batch loss = 0.63 (14.2 examples/sec; 0.564 sec/batch; 48h:39m:01s remains)
INFO - root - 2017-12-11 05:45:56.252650: step 21730, loss = 0.71, batch loss = 0.65 (14.2 examples/sec; 0.563 sec/batch; 48h:34m:28s remains)
INFO - root - 2017-12-11 05:46:01.662130: step 21740, loss = 0.70, batch loss = 0.65 (14.5 examples/sec; 0.551 sec/batch; 47h:34m:21s remains)
INFO - root - 2017-12-11 05:46:07.186731: step 21750, loss = 0.71, batch loss = 0.65 (14.7 examples/sec; 0.543 sec/batch; 46h:54m:01s remains)
INFO - root - 2017-12-11 05:46:12.622883: step 21760, loss = 0.70, batch loss = 0.64 (15.3 examples/sec; 0.524 sec/batch; 45h:16m:08s remains)
INFO - root - 2017-12-11 05:46:18.107131: step 21770, loss = 0.68, batch loss = 0.62 (14.5 examples/sec; 0.554 sec/batch; 47h:47m:10s remains)
INFO - root - 2017-12-11 05:46:23.574289: step 21780, loss = 0.70, batch loss = 0.64 (14.6 examples/sec; 0.550 sec/batch; 47h:25m:45s remains)
INFO - root - 2017-12-11 05:46:29.099731: step 21790, loss = 0.69, batch loss = 0.63 (14.6 examples/sec; 0.546 sec/batch; 47h:09m:26s remains)
INFO - root - 2017-12-11 05:46:34.613935: step 21800, loss = 0.69, batch loss = 0.63 (14.4 examples/sec; 0.556 sec/batch; 47h:59m:11s remains)
2017-12-11 05:46:35.198700: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.077516265 -0.08184965 -0.082093038 -0.076901525 -0.066215292 -0.052122209 -0.037231 -0.024225596 -0.013508269 -0.0038077347 0.0096510332 0.030252483 0.056065541 0.077963278 0.091640495][-0.078775868 -0.081475414 -0.078025319 -0.065874621 -0.04547165 -0.021711318 0.00013745308 0.015527963 0.021866323 0.021282367 0.022460259 0.033265755 0.0541262 0.075399481 0.091539115][-0.072925411 -0.068963222 -0.055811152 -0.030110165 0.0064652353 0.045594875 0.078182772 0.097008348 0.096793249 0.080477454 0.061040875 0.05278505 0.060426611 0.074967451 0.089799233][-0.059988625 -0.044198215 -0.015033608 0.030957839 0.090283461 0.15086198 0.19852716 0.22305661 0.21572827 0.17964612 0.13236275 0.096129112 0.081833 0.082155094 0.090458274][-0.041601077 -0.0098672258 0.041427325 0.11497391 0.20425041 0.29297844 0.36074623 0.39335951 0.37768885 0.317773 0.23610915 0.16357554 0.11882944 0.097522445 0.094228469][-0.022381043 0.026362054 0.10177054 0.20458993 0.32482931 0.44230238 0.53012317 0.56953704 0.54384696 0.45934579 0.34271464 0.23241761 0.15561388 0.1117487 0.096301846][-0.0094505316 0.052362025 0.14718564 0.27320495 0.41767192 0.556509 0.65833056 0.70036942 0.664666 0.56027079 0.41698352 0.27898389 0.17953549 0.12066561 0.097200766][-0.010713486 0.055240076 0.15795374 0.29403579 0.44908389 0.59690946 0.7035796 0.7443549 0.7023232 0.58897454 0.43564633 0.28795996 0.18178852 0.11961775 0.094396316][-0.026801636 0.031147355 0.12562604 0.2534833 0.40120557 0.54284316 0.64464027 0.68197834 0.64069337 0.53370142 0.39081168 0.25522122 0.16091712 0.10857318 0.088830642][-0.043937776 -0.0035880471 0.068356454 0.17097977 0.29432058 0.41527754 0.50327528 0.53575581 0.50169724 0.41347304 0.297397 0.19083576 0.12256164 0.089920014 0.081196286][-0.046253677 -0.026679674 0.014819932 0.080534413 0.16551736 0.25192574 0.31595355 0.34011915 0.31690046 0.25569817 0.17715628 0.11056866 0.076625645 0.068548955 0.073238261][-0.024142265 -0.022764914 -0.011909289 0.013411768 0.053235751 0.097072437 0.130319 0.14308265 0.13100742 0.099294275 0.061428118 0.037227906 0.037713449 0.052120477 0.067986719][0.016052561 0.0037883874 -0.010831174 -0.020352948 -0.020865254 -0.015725954 -0.010538689 -0.0082579479 -0.010815154 -0.017089486 -0.020356761 -0.010131308 0.016015155 0.044952285 0.066851377][0.05668902 0.036117181 0.0053509478 -0.026670557 -0.054003254 -0.074792147 -0.087957293 -0.092049 -0.087755382 -0.076722413 -0.057093777 -0.02575421 0.013763583 0.0480753 0.070799179][0.0807365 0.057862572 0.021973789 -0.018064719 -0.05654069 -0.089131728 -0.11083618 -0.11802883 -0.11042821 -0.090471119 -0.059215195 -0.01868028 0.024202518 0.057357881 0.077625938]]...]
INFO - root - 2017-12-11 05:46:40.775000: step 21810, loss = 0.69, batch loss = 0.63 (14.6 examples/sec; 0.550 sec/batch; 47h:25m:31s remains)
INFO - root - 2017-12-11 05:46:45.963455: step 21820, loss = 0.71, batch loss = 0.65 (15.1 examples/sec; 0.529 sec/batch; 45h:38m:00s remains)
INFO - root - 2017-12-11 05:46:51.481524: step 21830, loss = 0.69, batch loss = 0.63 (14.7 examples/sec; 0.546 sec/batch; 47h:06m:12s remains)
INFO - root - 2017-12-11 05:46:56.919988: step 21840, loss = 0.69, batch loss = 0.63 (14.2 examples/sec; 0.562 sec/batch; 48h:28m:10s remains)
INFO - root - 2017-12-11 05:47:02.409292: step 21850, loss = 0.70, batch loss = 0.64 (14.1 examples/sec; 0.568 sec/batch; 49h:03m:20s remains)
INFO - root - 2017-12-11 05:47:07.973486: step 21860, loss = 0.69, batch loss = 0.63 (14.5 examples/sec; 0.553 sec/batch; 47h:41m:26s remains)
INFO - root - 2017-12-11 05:47:13.465554: step 21870, loss = 0.71, batch loss = 0.65 (14.7 examples/sec; 0.545 sec/batch; 47h:03m:33s remains)
INFO - root - 2017-12-11 05:47:18.923782: step 21880, loss = 0.70, batch loss = 0.65 (14.6 examples/sec; 0.550 sec/batch; 47h:25m:40s remains)
INFO - root - 2017-12-11 05:47:24.516788: step 21890, loss = 0.71, batch loss = 0.65 (14.3 examples/sec; 0.558 sec/batch; 48h:07m:03s remains)
INFO - root - 2017-12-11 05:47:29.987784: step 21900, loss = 0.69, batch loss = 0.64 (14.2 examples/sec; 0.564 sec/batch; 48h:41m:28s remains)
2017-12-11 05:47:30.609738: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.45221743 0.52711171 0.56913322 0.58399671 0.56150019 0.50291657 0.42857313 0.36401948 0.30285719 0.23434724 0.16715941 0.12385486 0.11025823 0.11871499 0.14268662][0.4955577 0.56094825 0.59491068 0.6067853 0.58605754 0.53235662 0.46374926 0.40010622 0.32917595 0.24166368 0.1522302 0.084745139 0.044119693 0.025594903 0.028575251][0.54367143 0.58614028 0.59867167 0.59731507 0.57370585 0.52858609 0.47812551 0.43335649 0.37341744 0.28701136 0.18938962 0.10211344 0.030999376 -0.020694299 -0.044356775][0.60835391 0.62594235 0.61326444 0.5933246 0.56128228 0.52181154 0.49049267 0.471435 0.4364509 0.36866114 0.27656293 0.17567256 0.076571353 -0.0042328569 -0.047861498][0.67912263 0.68604165 0.660334 0.62680936 0.58265775 0.53785342 0.51113391 0.5067277 0.49463832 0.45139945 0.37259224 0.26582095 0.14972629 0.055411831 0.0091247261][0.743528 0.75928569 0.73923111 0.70244914 0.6444751 0.57990479 0.53512013 0.52315104 0.51974475 0.49640274 0.43258989 0.3281813 0.21043755 0.1222256 0.091485523][0.77897513 0.81378579 0.811949 0.78235781 0.7142151 0.62312227 0.54397428 0.50529808 0.49355936 0.47831106 0.42528108 0.32999861 0.22599664 0.16089292 0.15773553][0.75556934 0.80682123 0.8246426 0.80805254 0.73697585 0.62381047 0.51058209 0.44142786 0.4144173 0.39976475 0.35590464 0.27727139 0.19851558 0.16412714 0.18913847][0.65293509 0.70576638 0.73601723 0.7346859 0.67265379 0.55576819 0.42861006 0.34345815 0.30651429 0.29078662 0.25534707 0.19675943 0.14618729 0.13964048 0.18464436][0.49371204 0.53428763 0.5665279 0.5783034 0.53515625 0.43623221 0.32316256 0.24578251 0.211951 0.19835402 0.17092615 0.130398 0.10207618 0.11258391 0.16455692][0.3286486 0.34988406 0.37443936 0.392285 0.36960641 0.30102596 0.22172228 0.17145331 0.15333036 0.1470774 0.12793994 0.1014565 0.086395 0.10193551 0.14929405][0.20379867 0.20651761 0.21753718 0.23303306 0.22503941 0.18652017 0.14517245 0.1278317 0.12980607 0.13309009 0.12257893 0.1077552 0.10129555 0.11681839 0.15580732][0.13970098 0.1308383 0.12755153 0.13346685 0.12957479 0.1102016 0.096618854 0.10438938 0.12251803 0.13507538 0.13489424 0.1326433 0.13535547 0.15218537 0.18479237][0.11449259 0.10078131 0.086376049 0.079679854 0.071002364 0.057375032 0.05559485 0.074199818 0.09984386 0.11910746 0.12981784 0.14098282 0.15447764 0.17530753 0.20417298][0.091713086 0.079407088 0.060727961 0.044486377 0.027309632 0.009232739 0.0047023511 0.020154236 0.044017375 0.065947451 0.0857703 0.10876095 0.13219117 0.15708882 0.18260454]]...]
INFO - root - 2017-12-11 05:47:36.127841: step 21910, loss = 0.71, batch loss = 0.66 (15.4 examples/sec; 0.518 sec/batch; 44h:41m:48s remains)
INFO - root - 2017-12-11 05:47:41.355849: step 21920, loss = 0.71, batch loss = 0.65 (14.8 examples/sec; 0.539 sec/batch; 46h:29m:08s remains)
INFO - root - 2017-12-11 05:47:46.858851: step 21930, loss = 0.70, batch loss = 0.64 (15.1 examples/sec; 0.528 sec/batch; 45h:35m:11s remains)
INFO - root - 2017-12-11 05:47:52.328872: step 21940, loss = 0.70, batch loss = 0.64 (14.5 examples/sec; 0.551 sec/batch; 47h:33m:47s remains)
INFO - root - 2017-12-11 05:47:57.822651: step 21950, loss = 0.70, batch loss = 0.64 (14.5 examples/sec; 0.550 sec/batch; 47h:26m:14s remains)
INFO - root - 2017-12-11 05:48:03.303708: step 21960, loss = 0.70, batch loss = 0.64 (14.7 examples/sec; 0.543 sec/batch; 46h:48m:33s remains)
INFO - root - 2017-12-11 05:48:08.826178: step 21970, loss = 0.69, batch loss = 0.63 (14.8 examples/sec; 0.539 sec/batch; 46h:28m:30s remains)
INFO - root - 2017-12-11 05:48:14.366135: step 21980, loss = 0.70, batch loss = 0.65 (14.0 examples/sec; 0.572 sec/batch; 49h:21m:54s remains)
INFO - root - 2017-12-11 05:48:19.923862: step 21990, loss = 0.69, batch loss = 0.63 (14.1 examples/sec; 0.568 sec/batch; 49h:00m:05s remains)
INFO - root - 2017-12-11 05:48:25.425754: step 22000, loss = 0.68, batch loss = 0.62 (14.4 examples/sec; 0.555 sec/batch; 47h:50m:54s remains)
2017-12-11 05:48:26.008914: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.007047066 0.032892875 0.0906777 0.15507069 0.21439818 0.26574108 0.31439966 0.3604877 0.39065126 0.40695643 0.41367707 0.40218416 0.373636 0.353395 0.36410871][-0.014454042 0.022523347 0.0783991 0.14358194 0.20951964 0.27301648 0.33619556 0.39514548 0.43348706 0.45237392 0.45473382 0.43411392 0.39850783 0.3789643 0.39869356][-0.023779992 0.0097898562 0.064176336 0.13159697 0.20498402 0.27964786 0.35428345 0.42032114 0.45963687 0.47357458 0.46705535 0.43742192 0.39587316 0.37556428 0.39911678][-0.032692347 -0.0013020554 0.054231685 0.12663811 0.20759962 0.28992316 0.37026602 0.43693563 0.47179654 0.47811314 0.46435049 0.43150485 0.38901863 0.36594662 0.38376486][-0.037406761 -0.004417 0.057262804 0.1401566 0.23237765 0.32316786 0.40783864 0.47378397 0.50483561 0.5057494 0.48794881 0.45530143 0.41332778 0.38317323 0.38451672][-0.034360688 0.0044202427 0.075759865 0.17349815 0.28149551 0.38395405 0.47323871 0.53795421 0.56583089 0.56085706 0.53544289 0.49839553 0.455334 0.41888961 0.404716][-0.024571573 0.021723513 0.10339972 0.21746702 0.3437953 0.45985642 0.55254227 0.61245728 0.63295829 0.61480981 0.571464 0.52116358 0.47473812 0.43631843 0.41434646][-0.014488893 0.038020328 0.12763122 0.2553573 0.39755031 0.52438 0.61678237 0.66789824 0.6767475 0.640766 0.57545716 0.5098561 0.46170941 0.42747146 0.40647274][-0.0077407612 0.047378678 0.13844118 0.27035624 0.41814271 0.54689169 0.63266 0.67129236 0.66738242 0.61644983 0.53479576 0.45879304 0.41223782 0.3862735 0.37199059][-0.0065451665 0.046266116 0.13085684 0.25461259 0.39374259 0.51159453 0.58153361 0.60192132 0.58324981 0.522257 0.43462294 0.35760635 0.31716305 0.30209789 0.29770896][-0.017449662 0.02505132 0.093129255 0.19533047 0.31138748 0.40621114 0.45333681 0.45356384 0.42247829 0.35979971 0.27886853 0.21183622 0.18205003 0.17857823 0.18495338][-0.038891736 -0.013143593 0.031616665 0.10412296 0.18914197 0.25607446 0.28139606 0.26743579 0.2313441 0.1753855 0.11006133 0.0595925 0.041370824 0.046140291 0.059198648][-0.059628531 -0.049639408 -0.026874494 0.015384327 0.0671567 0.10579055 0.11409034 0.094854683 0.062580861 0.020902015 -0.022848168 -0.053928107 -0.062364437 -0.054815967 -0.041361369][-0.073244505 -0.074170046 -0.067628175 -0.050686777 -0.028569112 -0.014300836 -0.017283272 -0.034904495 -0.05764902 -0.082296357 -0.10494915 -0.11884776 -0.12043702 -0.11362415 -0.10328771][-0.081328683 -0.0892058 -0.092780568 -0.092546836 -0.090359136 -0.091332562 -0.098687276 -0.11061931 -0.12256229 -0.13260844 -0.13972636 -0.14248243 -0.1406457 -0.13572301 -0.12908415]]...]
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-SGD-lr0.001-clip50-initfromfixqian30k-fixqian
INFO - root - 2017-12-11 05:48:31.173664: step 22010, loss = 0.70, batch loss = 0.64 (14.4 examples/sec; 0.554 sec/batch; 47h:46m:32s remains)
INFO - root - 2017-12-11 05:48:36.694511: step 22020, loss = 0.70, batch loss = 0.64 (14.3 examples/sec; 0.558 sec/batch; 48h:05m:21s remains)
INFO - root - 2017-12-11 05:48:42.203216: step 22030, loss = 0.70, batch loss = 0.64 (14.4 examples/sec; 0.557 sec/batch; 48h:00m:39s remains)
INFO - root - 2017-12-11 05:48:47.658356: step 22040, loss = 0.69, batch loss = 0.63 (14.4 examples/sec; 0.556 sec/batch; 47h:58m:24s remains)
INFO - root - 2017-12-11 05:48:53.171387: step 22050, loss = 0.69, batch loss = 0.64 (14.6 examples/sec; 0.546 sec/batch; 47h:06m:01s remains)
INFO - root - 2017-12-11 05:48:58.793728: step 22060, loss = 0.69, batch loss = 0.63 (13.8 examples/sec; 0.579 sec/batch; 49h:54m:37s remains)
INFO - root - 2017-12-11 05:49:04.290111: step 22070, loss = 0.70, batch loss = 0.64 (14.5 examples/sec; 0.552 sec/batch; 47h:35m:55s remains)
INFO - root - 2017-12-11 05:49:09.773310: step 22080, loss = 0.67, batch loss = 0.61 (14.2 examples/sec; 0.563 sec/batch; 48h:32m:12s remains)
INFO - root - 2017-12-11 05:49:15.337365: step 22090, loss = 0.69, batch loss = 0.63 (14.8 examples/sec; 0.541 sec/batch; 46h:37m:03s remains)
INFO - root - 2017-12-11 05:49:20.822694: step 22100, loss = 0.71, batch loss = 0.65 (14.7 examples/sec; 0.545 sec/batch; 46h:57m:30s remains)
2017-12-11 05:49:21.410255: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0596497 -0.040050577 -0.019740527 -0.00088547426 0.01291705 0.019257739 0.015625525 0.0020081531 -0.0153872 -0.032006014 -0.044133507 -0.051029429 -0.051231597 -0.0455325 -0.038057774][-0.035562135 -0.0013583293 0.03553167 0.072532445 0.10378036 0.12260851 0.12363432 0.10588703 0.078896143 0.049743183 0.025040274 0.0075933938 0.00012360573 0.0016693593 0.0060599921][-0.013465905 0.035409223 0.0914432 0.15333097 0.21258248 0.2538192 0.26669502 0.24867438 0.21285892 0.16814467 0.12559016 0.092307881 0.073000871 0.067285739 0.066474691][-0.0036672289 0.057137422 0.13246237 0.22338215 0.32014236 0.3947027 0.42860404 0.41595998 0.37383866 0.31320229 0.25015473 0.19701573 0.1615226 0.14627442 0.14022572][-0.0021508331 0.069451764 0.16475932 0.28621393 0.42409158 0.53657424 0.5949887 0.589063 0.54024106 0.463451 0.38013333 0.30735224 0.25657269 0.23496866 0.22888707][0.0039493409 0.090158768 0.20938984 0.36115739 0.53434438 0.67652392 0.75030446 0.74248183 0.67964309 0.58497268 0.48588756 0.40185061 0.34620669 0.32922027 0.33395436][0.019973908 0.12548855 0.27227336 0.4500578 0.64268982 0.7939266 0.86417085 0.84143388 0.75736147 0.64613467 0.54136145 0.46201977 0.41971087 0.42332587 0.45119748][0.042571854 0.16847356 0.34036967 0.53299809 0.72176433 0.8548277 0.90005231 0.85236186 0.74601835 0.62451482 0.52526885 0.46427003 0.44853285 0.48111537 0.53610259][0.063830875 0.20340204 0.38760293 0.576902 0.73846644 0.83154494 0.83891076 0.76682472 0.64817959 0.5287053 0.44413283 0.40634078 0.41778749 0.47405121 0.54771096][0.071288593 0.20864277 0.38234523 0.54559654 0.66190153 0.70450193 0.67396748 0.58799613 0.4739081 0.37034717 0.30601561 0.28941777 0.31955296 0.38595498 0.46335232][0.056176744 0.17293745 0.31360671 0.43366373 0.49985838 0.49786428 0.44261509 0.35880134 0.26560441 0.18863012 0.14679255 0.14579053 0.18231019 0.2433158 0.30902773][0.025431981 0.11134664 0.20833786 0.28151631 0.3066977 0.27800912 0.21710368 0.14909886 0.085254192 0.038049486 0.016485563 0.023381578 0.054524608 0.0986895 0.14345114][-0.0058502932 0.051113091 0.10905457 0.14495546 0.1459503 0.11030713 0.059246562 0.013231839 -0.02255691 -0.044854034 -0.051954061 -0.043439217 -0.023748456 6.5713888e-05 0.022732381][-0.031058071 0.0053663761 0.038079888 0.053325754 0.047178011 0.018724009 -0.015136046 -0.040487256 -0.055793446 -0.061855968 -0.060093451 -0.051653042 -0.041300297 -0.032403085 -0.025526496][-0.052337173 -0.028807888 -0.0084627708 -0.00027490046 -0.0037065407 -0.020346578 -0.038275339 -0.049263913 -0.053350598 -0.051711451 -0.045983091 -0.037905894 -0.032268893 -0.030359488 -0.030789815]]...]
INFO - root - 2017-12-11 05:49:26.660779: step 22110, loss = 0.69, batch loss = 0.64 (14.1 examples/sec; 0.566 sec/batch; 48h:45m:29s remains)
INFO - root - 2017-12-11 05:49:32.207441: step 22120, loss = 0.70, batch loss = 0.64 (14.7 examples/sec; 0.544 sec/batch; 46h:52m:38s remains)
INFO - root - 2017-12-11 05:49:37.704766: step 22130, loss = 0.69, batch loss = 0.63 (14.5 examples/sec; 0.553 sec/batch; 47h:40m:36s remains)
INFO - root - 2017-12-11 05:49:43.167032: step 22140, loss = 0.69, batch loss = 0.63 (14.6 examples/sec; 0.546 sec/batch; 47h:06m:44s remains)
INFO - root - 2017-12-11 05:49:48.756855: step 22150, loss = 0.70, batch loss = 0.65 (14.2 examples/sec; 0.562 sec/batch; 48h:24m:29s remains)
INFO - root - 2017-12-11 05:49:54.230534: step 22160, loss = 0.72, batch loss = 0.66 (14.1 examples/sec; 0.566 sec/batch; 48h:50m:04s remains)
INFO - root - 2017-12-11 05:49:59.722662: step 22170, loss = 0.69, batch loss = 0.63 (14.6 examples/sec; 0.548 sec/batch; 47h:16m:27s remains)
INFO - root - 2017-12-11 05:50:05.388278: step 22180, loss = 0.68, batch loss = 0.63 (14.2 examples/sec; 0.562 sec/batch; 48h:24m:57s remains)
INFO - root - 2017-12-11 05:50:10.919649: step 22190, loss = 0.70, batch loss = 0.64 (14.8 examples/sec; 0.541 sec/batch; 46h:35m:48s remains)
INFO - root - 2017-12-11 05:50:16.357037: step 22200, loss = 0.67, batch loss = 0.61 (14.6 examples/sec; 0.546 sec/batch; 47h:06m:05s remains)
2017-12-11 05:50:16.944893: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.11883272 0.12518987 0.12559558 0.11308803 0.082120962 0.044290725 0.017268511 0.00914687 0.01529007 0.038657457 0.091555804 0.16486108 0.24176763 0.30683419 0.34488767][0.19093078 0.20147407 0.20327957 0.19147368 0.15612288 0.10866982 0.069278181 0.049677297 0.048642021 0.071696326 0.13215064 0.2153126 0.30109206 0.37077132 0.4067485][0.26797166 0.28527573 0.29066384 0.28156084 0.24467359 0.1894113 0.13683341 0.10352501 0.093107626 0.11395873 0.17581834 0.25852919 0.34057415 0.40321097 0.42988849][0.32875589 0.35762703 0.37033576 0.36751309 0.33509395 0.27884072 0.21816047 0.17375599 0.15589677 0.17377827 0.22885489 0.29768229 0.36239359 0.40858436 0.42246392][0.37875673 0.42188588 0.44323775 0.44683766 0.42181155 0.37095246 0.31077397 0.26502693 0.24909236 0.26765335 0.31039628 0.35393521 0.39017671 0.41172948 0.40948105][0.43494698 0.49070778 0.51676089 0.52261555 0.50334275 0.46113 0.40937668 0.37348703 0.36909309 0.39150134 0.41848409 0.42994142 0.43055 0.42125985 0.39984515][0.49763808 0.56073648 0.585511 0.58848161 0.57225353 0.54007316 0.50127387 0.4806118 0.48998597 0.51420194 0.52304548 0.50244194 0.46860978 0.43007556 0.39229217][0.54906523 0.61139727 0.6272155 0.62115633 0.60313594 0.57812393 0.54966444 0.53945565 0.55478084 0.57337213 0.56360686 0.51847953 0.46120977 0.40527943 0.3617751][0.55979055 0.61115384 0.612401 0.59418786 0.57155836 0.55056036 0.52936274 0.52543294 0.5415169 0.55256516 0.53053218 0.47343728 0.40614197 0.34397238 0.30165932][0.51723653 0.55059743 0.53660572 0.5075931 0.47924563 0.45850325 0.44175783 0.44219026 0.45771861 0.46406373 0.43939307 0.38344929 0.31794444 0.25794843 0.22084233][0.42176852 0.4358581 0.41048917 0.375053 0.3425433 0.32013378 0.30559298 0.30802479 0.32144079 0.32515529 0.30476704 0.25969118 0.20586014 0.15695743 0.1304421][0.28774822 0.28875 0.26005578 0.22506471 0.19343857 0.17240559 0.16211689 0.1675843 0.18005158 0.1835482 0.17042118 0.13940598 0.10090271 0.0672811 0.053985149][0.14846861 0.14136033 0.11533413 0.086333737 0.061398398 0.047512658 0.045756552 0.056945466 0.0707877 0.076224685 0.07026387 0.051533703 0.02648118 0.0068177911 0.0046942676][0.0378793 0.026013048 0.004828441 -0.015793955 -0.030381227 -0.033414673 -0.024544438 -0.0066461577 0.00989821 0.018285291 0.017887488 0.0072588241 -0.0097757159 -0.021757517 -0.019560926][-0.031438317 -0.045531709 -0.061478402 -0.073909156 -0.079149619 -0.073665686 -0.058239494 -0.03768846 -0.020760041 -0.0110864 -0.0083103748 -0.014230916 -0.026652884 -0.034814477 -0.031892586]]...]
INFO - root - 2017-12-11 05:50:22.342309: step 22210, loss = 0.71, batch loss = 0.65 (13.3 examples/sec; 0.603 sec/batch; 51h:58m:30s remains)
INFO - root - 2017-12-11 05:50:27.857764: step 22220, loss = 0.70, batch loss = 0.64 (14.2 examples/sec; 0.565 sec/batch; 48h:43m:38s remains)
INFO - root - 2017-12-11 05:50:33.288121: step 22230, loss = 0.69, batch loss = 0.64 (14.9 examples/sec; 0.539 sec/batch; 46h:24m:43s remains)
INFO - root - 2017-12-11 05:50:38.885097: step 22240, loss = 0.68, batch loss = 0.63 (13.8 examples/sec; 0.580 sec/batch; 50h:00m:59s remains)
INFO - root - 2017-12-11 05:50:44.390722: step 22250, loss = 0.69, batch loss = 0.63 (14.3 examples/sec; 0.558 sec/batch; 48h:03m:14s remains)
INFO - root - 2017-12-11 05:50:49.852600: step 22260, loss = 0.71, batch loss = 0.65 (14.9 examples/sec; 0.537 sec/batch; 46h:15m:47s remains)
INFO - root - 2017-12-11 05:50:55.502490: step 22270, loss = 0.70, batch loss = 0.64 (14.2 examples/sec; 0.563 sec/batch; 48h:32m:31s remains)
INFO - root - 2017-12-11 05:51:01.015862: step 22280, loss = 0.69, batch loss = 0.64 (14.5 examples/sec; 0.551 sec/batch; 47h:27m:34s remains)
INFO - root - 2017-12-11 05:51:06.554488: step 22290, loss = 0.68, batch loss = 0.63 (14.7 examples/sec; 0.543 sec/batch; 46h:45m:57s remains)
INFO - root - 2017-12-11 05:51:11.827205: step 22300, loss = 0.69, batch loss = 0.63 (26.4 examples/sec; 0.303 sec/batch; 26h:07m:59s remains)
2017-12-11 05:51:12.358157: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.026362997 0.058272637 0.087476291 0.10614801 0.11004448 0.10348491 0.099709429 0.10755658 0.12431693 0.14379634 0.15535045 0.15202014 0.12569252 0.075627707 0.012900687][0.042062797 0.0768919 0.11293542 0.14245769 0.1587227 0.16586854 0.17885689 0.2060523 0.23989576 0.27040088 0.28733978 0.28589565 0.25519234 0.18850541 0.097265832][0.052112207 0.086901039 0.12876004 0.17067777 0.20433761 0.23294258 0.26814204 0.31315845 0.35628694 0.38652417 0.39990324 0.39834478 0.36907539 0.29486114 0.18216825][0.053254489 0.086292125 0.13262805 0.18729024 0.24093373 0.2930958 0.34852377 0.40319705 0.44253457 0.45751482 0.45634848 0.45226279 0.43148923 0.36374286 0.2448886][0.048968945 0.079163752 0.12934156 0.1980475 0.27469003 0.35211521 0.42617989 0.48430917 0.50800151 0.49285841 0.46435848 0.45020637 0.43721765 0.38335356 0.27161488][0.042647921 0.069340043 0.12328433 0.20740092 0.30949774 0.41453144 0.50863171 0.57048243 0.57687938 0.52807021 0.46727455 0.43755427 0.42742434 0.38398847 0.27979633][0.035694595 0.059165776 0.11709811 0.21620139 0.34316477 0.47652033 0.59367347 0.66514128 0.66293055 0.58971769 0.50277466 0.45825097 0.4460071 0.40421665 0.29896176][0.031904969 0.05508047 0.11770152 0.22827409 0.37287933 0.52643645 0.6619854 0.74436855 0.74067962 0.653773 0.550545 0.49522841 0.47776589 0.4303489 0.31713277][0.036150806 0.062868088 0.12946287 0.24436583 0.39366388 0.5523017 0.6952405 0.7872355 0.79045951 0.70366043 0.59645325 0.53499913 0.50975716 0.45069796 0.32529408][0.049764071 0.081467837 0.14813258 0.25764838 0.39679623 0.5441789 0.68161851 0.77978534 0.797491 0.72399306 0.62331861 0.55910122 0.52426028 0.45101997 0.31445706][0.063290924 0.097366884 0.15766068 0.25037006 0.36478877 0.48532602 0.60330606 0.69865835 0.73075432 0.67890149 0.59440345 0.5343405 0.49425697 0.41387656 0.27689439][0.069015704 0.10331485 0.15345496 0.2221154 0.30206436 0.3842994 0.46966031 0.54850012 0.58559352 0.55420548 0.48975059 0.43866944 0.39925215 0.32286444 0.20075691][0.061485462 0.094154909 0.13292959 0.17674237 0.22105689 0.2627137 0.30931166 0.36051488 0.38925141 0.36835453 0.31932968 0.27645439 0.24121937 0.1784084 0.084627569][0.040201414 0.067874625 0.094469763 0.11655121 0.13172166 0.14083844 0.15384403 0.17642792 0.19063388 0.17303015 0.13476242 0.099178605 0.070860185 0.027013516 -0.033465136][0.017331563 0.037355173 0.052142534 0.057513051 0.053607382 0.043089509 0.034653474 0.035538964 0.036034685 0.019062947 -0.0105679 -0.037826229 -0.057055525 -0.0815399 -0.11261024]]...]
INFO - root - 2017-12-11 05:51:17.780505: step 22310, loss = 0.71, batch loss = 0.65 (14.6 examples/sec; 0.547 sec/batch; 47h:09m:59s remains)
INFO - root - 2017-12-11 05:51:23.230706: step 22320, loss = 0.70, batch loss = 0.64 (14.3 examples/sec; 0.559 sec/batch; 48h:09m:59s remains)
INFO - root - 2017-12-11 05:51:28.703495: step 22330, loss = 0.69, batch loss = 0.63 (14.7 examples/sec; 0.543 sec/batch; 46h:46m:27s remains)
INFO - root - 2017-12-11 05:51:34.281926: step 22340, loss = 0.71, batch loss = 0.65 (13.9 examples/sec; 0.574 sec/batch; 49h:27m:28s remains)
INFO - root - 2017-12-11 05:51:39.830779: step 22350, loss = 0.70, batch loss = 0.64 (15.0 examples/sec; 0.535 sec/batch; 46h:04m:59s remains)
INFO - root - 2017-12-11 05:51:45.326371: step 22360, loss = 0.70, batch loss = 0.64 (14.9 examples/sec; 0.538 sec/batch; 46h:22m:02s remains)
INFO - root - 2017-12-11 05:51:50.852270: step 22370, loss = 0.70, batch loss = 0.64 (14.4 examples/sec; 0.554 sec/batch; 47h:44m:08s remains)
INFO - root - 2017-12-11 05:51:56.372424: step 22380, loss = 0.69, batch loss = 0.63 (14.5 examples/sec; 0.552 sec/batch; 47h:32m:59s remains)
INFO - root - 2017-12-11 05:52:01.792630: step 22390, loss = 0.70, batch loss = 0.64 (14.4 examples/sec; 0.556 sec/batch; 47h:51m:11s remains)
INFO - root - 2017-12-11 05:52:07.006659: step 22400, loss = 0.68, batch loss = 0.62 (14.0 examples/sec; 0.572 sec/batch; 49h:17m:14s remains)
2017-12-11 05:52:07.542146: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.044468716 -0.044962298 -0.043031141 -0.039363414 -0.0346427 -0.030802881 -0.028636469 -0.026057245 -0.021491101 -0.016535053 -0.013338769 -0.012758709 -0.015150177 -0.022753796 -0.034107484][-0.033330396 -0.030313447 -0.023560148 -0.013445616 -0.0010306721 0.009528189 0.015546622 0.020260505 0.027663188 0.035133667 0.039895553 0.041003361 0.038206723 0.026662722 0.0078309681][-0.017431162 -0.0098470328 0.0021561929 0.019533737 0.041579463 0.061151177 0.073120154 0.0817659 0.093944386 0.10512419 0.11147065 0.11246418 0.10809126 0.090996772 0.062309358][0.0021024724 0.015620271 0.033839319 0.05935568 0.091776513 0.12121204 0.14028803 0.15401831 0.17238151 0.18808809 0.19539231 0.19418943 0.18591532 0.16202839 0.12226918][0.029509362 0.052677236 0.080649972 0.11607815 0.15763898 0.19387384 0.21617652 0.23134041 0.25349605 0.27377516 0.28352773 0.28026342 0.26763764 0.23626421 0.1840491][0.071401194 0.10742252 0.1461339 0.18943529 0.23447856 0.2704913 0.28964204 0.3018775 0.32487205 0.34942317 0.36363843 0.36027083 0.34407094 0.30469984 0.23874526][0.12646767 0.17426279 0.21969336 0.26485577 0.30598915 0.3354786 0.34842592 0.35778174 0.38198781 0.41127333 0.4301621 0.42641184 0.40449718 0.35453522 0.27319902][0.1878653 0.24183571 0.28868583 0.3316398 0.3655605 0.38658273 0.39412251 0.40293664 0.4285982 0.46076876 0.48111796 0.4730846 0.44093505 0.37685719 0.28021875][0.257834 0.31372169 0.35821483 0.39635995 0.42149085 0.43265685 0.43296376 0.43880826 0.46146041 0.49099812 0.50767136 0.49198419 0.44725588 0.36815283 0.25939921][0.32821158 0.38224286 0.42079344 0.45225972 0.46957693 0.47276348 0.4663673 0.46736044 0.4816725 0.50067943 0.50601858 0.478956 0.42125377 0.3288314 0.21299696][0.3908262 0.43799734 0.46489412 0.48603505 0.49611971 0.49488273 0.48612374 0.48446274 0.48860902 0.49012208 0.47542408 0.43271431 0.36317617 0.26283962 0.14772904][0.44433182 0.48198965 0.494413 0.50289994 0.50520682 0.49950328 0.48823372 0.48261008 0.4752681 0.456644 0.41896576 0.36060315 0.28384343 0.18398486 0.077991828][0.48979342 0.51960891 0.519693 0.51584053 0.50765878 0.49057931 0.46669188 0.44678584 0.42280647 0.38494638 0.3294071 0.26305249 0.18955481 0.10207301 0.014704209][0.52894831 0.55539042 0.54815412 0.53398168 0.5114162 0.47325981 0.42330897 0.37674162 0.33095169 0.27749273 0.21335155 0.14932896 0.089510396 0.024647919 -0.036736406][0.55770022 0.58301175 0.57261133 0.55028129 0.51152271 0.4484002 0.36824971 0.29187644 0.22474408 0.16098017 0.0964595 0.041288555 -0.0005079346 -0.039036896 -0.072304115]]...]
INFO - root - 2017-12-11 05:52:12.981170: step 22410, loss = 0.70, batch loss = 0.64 (14.4 examples/sec; 0.556 sec/batch; 47h:51m:31s remains)
INFO - root - 2017-12-11 05:52:18.482317: step 22420, loss = 0.71, batch loss = 0.65 (14.5 examples/sec; 0.552 sec/batch; 47h:32m:09s remains)
INFO - root - 2017-12-11 05:52:23.964341: step 22430, loss = 0.67, batch loss = 0.61 (15.3 examples/sec; 0.522 sec/batch; 44h:58m:41s remains)
INFO - root - 2017-12-11 05:52:29.488863: step 22440, loss = 0.69, batch loss = 0.63 (14.3 examples/sec; 0.560 sec/batch; 48h:15m:29s remains)
INFO - root - 2017-12-11 05:52:35.037054: step 22450, loss = 0.68, batch loss = 0.62 (14.5 examples/sec; 0.551 sec/batch; 47h:29m:40s remains)
INFO - root - 2017-12-11 05:52:40.545367: step 22460, loss = 0.71, batch loss = 0.65 (14.5 examples/sec; 0.552 sec/batch; 47h:31m:44s remains)
INFO - root - 2017-12-11 05:52:46.059619: step 22470, loss = 0.69, batch loss = 0.63 (14.1 examples/sec; 0.568 sec/batch; 48h:55m:46s remains)
INFO - root - 2017-12-11 05:52:51.630423: step 22480, loss = 0.70, batch loss = 0.64 (14.4 examples/sec; 0.554 sec/batch; 47h:41m:48s remains)
INFO - root - 2017-12-11 05:52:57.183347: step 22490, loss = 0.70, batch loss = 0.64 (14.6 examples/sec; 0.547 sec/batch; 47h:08m:26s remains)
INFO - root - 2017-12-11 05:53:02.250497: step 22500, loss = 0.68, batch loss = 0.62 (15.9 examples/sec; 0.504 sec/batch; 43h:23m:22s remains)
2017-12-11 05:53:02.817507: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.042637676 -0.042134069 -0.040137209 -0.038458314 -0.038515482 -0.039535005 -0.038170252 -0.029452108 -0.00890404 0.021490186 0.055095281 0.083940953 0.10405864 0.11747637 0.12890945][-0.048472185 -0.046607632 -0.041824169 -0.035433121 -0.029552003 -0.0237356 -0.015884714 -0.0027672034 0.018383149 0.044775356 0.070104182 0.087834932 0.097423144 0.10440379 0.11478405][-0.047882404 -0.04512633 -0.037794527 -0.026549704 -0.014098212 -0.00077972322 0.013595357 0.030188197 0.050030123 0.069526628 0.083140045 0.086327687 0.082607932 0.082030967 0.092414558][-0.046887048 -0.044212297 -0.035411466 -0.020485029 -0.0025130464 0.017288528 0.03704115 0.056116678 0.07468012 0.088893704 0.093835823 0.085930258 0.071773954 0.065017626 0.075469218][-0.047685415 -0.045696482 -0.036142536 -0.018603392 0.0036303464 0.02842352 0.052279923 0.073350541 0.091998212 0.10503072 0.10773517 0.096102647 0.077107728 0.06581948 0.073504858][-0.0487872 -0.047143579 -0.036277354 -0.015720569 0.011042733 0.040787488 0.06849283 0.0914151 0.11115778 0.12600195 0.13106965 0.12093025 0.100813 0.085369036 0.086390935][-0.04881667 -0.046553686 -0.032824077 -0.0078079151 0.024976451 0.061179534 0.094162069 0.11981113 0.14109738 0.15835911 0.16657116 0.1586805 0.13749576 0.11652502 0.10753746][-0.048534296 -0.044347 -0.02581415 0.0061800806 0.048613716 0.095847636 0.13910045 0.17136517 0.19577838 0.21449579 0.22273238 0.21291085 0.18635687 0.15617862 0.13394594][-0.049249735 -0.042270627 -0.016954359 0.025144314 0.081838094 0.14603755 0.20624135 0.25044119 0.27999467 0.29833224 0.30203664 0.28478113 0.24778059 0.20501344 0.16771869][-0.051059924 -0.041268151 -0.0091938935 0.043258935 0.11509508 0.19834687 0.27902576 0.33907455 0.3761135 0.39386091 0.39147413 0.36510962 0.3163799 0.26077819 0.2091596][-0.050743867 -0.039269891 -0.0030911104 0.055900425 0.13800217 0.23557466 0.33349758 0.40843743 0.45362374 0.47201285 0.46598396 0.43417826 0.37814608 0.31427315 0.25252879][-0.043756943 -0.032183602 0.0039396212 0.062782489 0.14534618 0.24561708 0.34970108 0.43176663 0.48186275 0.50173503 0.49661016 0.46676609 0.41307148 0.35085279 0.28781667][-0.025257349 -0.01435296 0.018171998 0.07017681 0.14215259 0.23057975 0.32519796 0.40171927 0.44960919 0.46991402 0.46973103 0.4499791 0.41035518 0.36276764 0.31073284][0.0042732395 0.013978341 0.039963081 0.079468437 0.13156335 0.19574565 0.26675203 0.32541025 0.36332905 0.381715 0.38851041 0.38474825 0.3692652 0.34861848 0.32010353][0.037662204 0.045142271 0.06218322 0.085516788 0.11333644 0.14816973 0.18950742 0.22467643 0.24884145 0.26376808 0.27726093 0.29052341 0.30284768 0.31545022 0.31858847]]...]
INFO - root - 2017-12-11 05:53:08.279977: step 22510, loss = 0.71, batch loss = 0.65 (14.7 examples/sec; 0.544 sec/batch; 46h:50m:04s remains)
INFO - root - 2017-12-11 05:53:13.830391: step 22520, loss = 0.70, batch loss = 0.64 (14.5 examples/sec; 0.551 sec/batch; 47h:24m:42s remains)
INFO - root - 2017-12-11 05:53:19.352159: step 22530, loss = 0.69, batch loss = 0.64 (14.4 examples/sec; 0.557 sec/batch; 47h:57m:42s remains)
INFO - root - 2017-12-11 05:53:24.746140: step 22540, loss = 0.69, batch loss = 0.63 (14.7 examples/sec; 0.545 sec/batch; 46h:54m:51s remains)
INFO - root - 2017-12-11 05:53:30.265985: step 22550, loss = 0.70, batch loss = 0.64 (14.7 examples/sec; 0.545 sec/batch; 46h:57m:27s remains)
INFO - root - 2017-12-11 05:53:35.830838: step 22560, loss = 0.69, batch loss = 0.64 (14.3 examples/sec; 0.558 sec/batch; 48h:03m:46s remains)
INFO - root - 2017-12-11 05:53:41.335438: step 22570, loss = 0.68, batch loss = 0.62 (14.7 examples/sec; 0.545 sec/batch; 46h:56m:58s remains)
INFO - root - 2017-12-11 05:53:46.836470: step 22580, loss = 0.68, batch loss = 0.62 (14.4 examples/sec; 0.557 sec/batch; 47h:54m:38s remains)
INFO - root - 2017-12-11 05:53:52.476487: step 22590, loss = 0.70, batch loss = 0.64 (13.8 examples/sec; 0.579 sec/batch; 49h:49m:58s remains)
INFO - root - 2017-12-11 05:53:57.774348: step 22600, loss = 0.70, batch loss = 0.64 (14.1 examples/sec; 0.566 sec/batch; 48h:40m:56s remains)
2017-12-11 05:53:58.360093: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.085092932 -0.08101587 -0.073855869 -0.068567328 -0.067535833 -0.070871249 -0.075923868 -0.080713138 -0.084723949 -0.086112157 -0.084574357 -0.0834733 -0.085155942 -0.08858484 -0.091794617][-0.077885434 -0.066038333 -0.049543452 -0.033915382 -0.023245625 -0.019788094 -0.021157864 -0.026271032 -0.034692351 -0.042754568 -0.04769063 -0.052284341 -0.059404362 -0.066833928 -0.072611064][-0.058763 -0.030626046 0.0065420852 0.046428211 0.08128649 0.1030943 0.11349604 0.11221914 0.099179596 0.079783387 0.061998788 0.045354471 0.024924593 0.0040195887 -0.013503633][-0.029843362 0.023907831 0.093961172 0.17313075 0.24733928 0.29937679 0.3308135 0.3389557 0.32187718 0.28503755 0.24577545 0.20823303 0.16389193 0.11717978 0.076904163][0.0045591737 0.090364628 0.20193085 0.33062288 0.45301142 0.54148304 0.5995093 0.62053579 0.59826326 0.5382275 0.47157133 0.40722096 0.3308177 0.24908449 0.17926942][0.037058413 0.15564549 0.30983132 0.48916027 0.65850443 0.7818374 0.86669552 0.90136355 0.87328517 0.78922844 0.69618011 0.60643935 0.49707434 0.37721351 0.27530676][0.0628431 0.20815594 0.3970677 0.61589891 0.818181 0.96426505 1.0665213 1.1085491 1.0741972 0.97338283 0.865116 0.76064736 0.62684619 0.4751631 0.34424421][0.077437416 0.23642457 0.44288406 0.67955738 0.89248741 1.0441344 1.1500703 1.1914122 1.1521882 1.0467209 0.93819964 0.83230633 0.68773293 0.5180099 0.36852551][0.0754087 0.22963831 0.43034017 0.65743929 0.85555673 0.99402767 1.0884427 1.1199187 1.0769038 0.97826236 0.88184804 0.78598487 0.64748865 0.48238736 0.33486098][0.055153552 0.18742599 0.36090058 0.55451089 0.71751028 0.82727873 0.89685446 0.91137773 0.86619073 0.78346282 0.70720285 0.63079119 0.51567662 0.37823996 0.25322908][0.021134218 0.12087389 0.25389895 0.40118721 0.52126944 0.59739774 0.63861436 0.63698637 0.59489655 0.53423303 0.48115554 0.42768508 0.34456685 0.24539827 0.15108976][-0.021037607 0.04251571 0.13104481 0.23028246 0.31051227 0.35811195 0.37768847 0.3668586 0.33260983 0.29343319 0.25990877 0.22559921 0.17255266 0.11037164 0.046507869][-0.062869065 -0.031892028 0.016805397 0.074606113 0.12293528 0.1506743 0.15871985 0.14686221 0.12210461 0.098030083 0.076305561 0.053517021 0.021533635 -0.012981471 -0.050626177][-0.0968855 -0.091362074 -0.074064374 -0.049451374 -0.027084269 -0.014962784 -0.013343816 -0.022535097 -0.038183961 -0.051618692 -0.064402707 -0.077966392 -0.094724573 -0.10991139 -0.12636891][-0.1176222 -0.12826705 -0.13174619 -0.13051337 -0.12785509 -0.1283374 -0.13169554 -0.13809019 -0.14579335 -0.15051405 -0.15442351 -0.1585504 -0.1635102 -0.16663238 -0.16906935]]...]
INFO - root - 2017-12-11 05:54:03.961017: step 22610, loss = 0.71, batch loss = 0.66 (14.9 examples/sec; 0.537 sec/batch; 46h:13m:55s remains)
INFO - root - 2017-12-11 05:54:09.453129: step 22620, loss = 0.70, batch loss = 0.64 (15.1 examples/sec; 0.530 sec/batch; 45h:39m:34s remains)
INFO - root - 2017-12-11 05:54:14.913946: step 22630, loss = 0.69, batch loss = 0.63 (14.4 examples/sec; 0.554 sec/batch; 47h:41m:02s remains)
INFO - root - 2017-12-11 05:54:20.430402: step 22640, loss = 0.71, batch loss = 0.65 (14.5 examples/sec; 0.552 sec/batch; 47h:32m:16s remains)
INFO - root - 2017-12-11 05:54:25.948355: step 22650, loss = 0.70, batch loss = 0.65 (14.2 examples/sec; 0.564 sec/batch; 48h:33m:08s remains)
INFO - root - 2017-12-11 05:54:31.447731: step 22660, loss = 0.70, batch loss = 0.65 (14.4 examples/sec; 0.556 sec/batch; 47h:53m:00s remains)
INFO - root - 2017-12-11 05:54:36.880928: step 22670, loss = 0.70, batch loss = 0.64 (14.8 examples/sec; 0.540 sec/batch; 46h:27m:15s remains)
INFO - root - 2017-12-11 05:54:42.354920: step 22680, loss = 0.69, batch loss = 0.63 (14.5 examples/sec; 0.552 sec/batch; 47h:28m:43s remains)
INFO - root - 2017-12-11 05:54:47.619977: step 22690, loss = 0.68, batch loss = 0.62 (16.0 examples/sec; 0.500 sec/batch; 43h:03m:24s remains)
INFO - root - 2017-12-11 05:54:53.158945: step 22700, loss = 0.68, batch loss = 0.62 (14.6 examples/sec; 0.548 sec/batch; 47h:08m:49s remains)
2017-12-11 05:54:53.746918: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.042280313 -0.038274162 -0.035150066 -0.035566848 -0.039796386 -0.046177119 -0.051955972 -0.054793019 -0.053559225 -0.049128022 -0.044481728 -0.042925764 -0.045635797 -0.049706187 -0.052773707][-0.023189172 -0.010102442 -0.0002014084 0.0026895877 -0.0015577234 -0.010861306 -0.020683965 -0.027104685 -0.02712832 -0.022037974 -0.01685117 -0.017749611 -0.026915107 -0.03868708 -0.047699124][0.010004953 0.037068658 0.057395194 0.066206239 0.0639374 0.053233117 0.040554024 0.031258255 0.030302998 0.035416391 0.038511053 0.029721903 0.0067536626 -0.019586716 -0.039329261][0.056628898 0.10208149 0.13636266 0.15334739 0.15527061 0.14654893 0.13566647 0.12845562 0.13037544 0.13716501 0.13541612 0.11084627 0.063274138 0.012335478 -0.024940839][0.11354042 0.18133378 0.23307447 0.26119345 0.27080062 0.26917514 0.26691315 0.26811129 0.27695784 0.28547457 0.27365363 0.22440512 0.1427971 0.059839617 -0.00041699602][0.16708003 0.25758252 0.32829669 0.37066638 0.39223695 0.40367812 0.41670859 0.43141049 0.4472326 0.452042 0.42162454 0.3404195 0.22140846 0.10599957 0.02361181][0.19734098 0.30387267 0.39057192 0.44735432 0.48389536 0.51409733 0.54863232 0.5795657 0.59868634 0.59205824 0.53728044 0.42409432 0.27339166 0.1333432 0.035930727][0.1961309 0.30827853 0.40424368 0.47265998 0.52457088 0.575712 0.63302559 0.67855978 0.69632238 0.67473829 0.59759295 0.46124238 0.29175562 0.13976552 0.036533695][0.17342179 0.28104872 0.37772578 0.45115811 0.512299 0.57668644 0.64792562 0.70076489 0.71411484 0.68060368 0.59122843 0.44774881 0.27733842 0.12829514 0.028815584][0.13579428 0.23159868 0.32228658 0.39472994 0.4562318 0.51920742 0.58643156 0.63257533 0.63684475 0.59582168 0.50677633 0.37481645 0.22414586 0.0952253 0.010580949][0.0844383 0.16258568 0.2419747 0.3081888 0.36138955 0.409393 0.456821 0.48491359 0.47744417 0.434113 0.35704866 0.25237653 0.13789484 0.042554766 -0.01784665][0.030512316 0.087642141 0.15063353 0.20465736 0.24307668 0.26969698 0.29171491 0.29893869 0.28207123 0.24313039 0.18653852 0.11675536 0.044076744 -0.013864335 -0.047636565][-0.0054152836 0.032835282 0.077080756 0.11421514 0.13478196 0.14030768 0.1396596 0.13004908 0.10852943 0.0781386 0.043117624 0.005246222 -0.031351864 -0.057720203 -0.069550321][-0.02396954 -0.0011179124 0.025276132 0.045278527 0.050656404 0.042025119 0.027851589 0.009700234 -0.011756963 -0.033471774 -0.052239355 -0.067967348 -0.07999099 -0.084880471 -0.081985317][-0.038655724 -0.026853791 -0.012984881 -0.0038562242 -0.00513756 -0.017121689 -0.033968359 -0.05273379 -0.071294419 -0.086898871 -0.096917823 -0.10129095 -0.1006126 -0.0946028 -0.084783286]]...]
INFO - root - 2017-12-11 05:54:59.273587: step 22710, loss = 0.68, batch loss = 0.62 (14.5 examples/sec; 0.552 sec/batch; 47h:31m:27s remains)
INFO - root - 2017-12-11 05:55:04.739653: step 22720, loss = 0.68, batch loss = 0.62 (14.4 examples/sec; 0.555 sec/batch; 47h:47m:49s remains)
INFO - root - 2017-12-11 05:55:10.175789: step 22730, loss = 0.70, batch loss = 0.64 (14.2 examples/sec; 0.562 sec/batch; 48h:23m:38s remains)
INFO - root - 2017-12-11 05:55:15.751876: step 22740, loss = 0.71, batch loss = 0.66 (14.3 examples/sec; 0.558 sec/batch; 47h:59m:10s remains)
INFO - root - 2017-12-11 05:55:21.285026: step 22750, loss = 0.69, batch loss = 0.64 (14.8 examples/sec; 0.542 sec/batch; 46h:35m:54s remains)
INFO - root - 2017-12-11 05:55:26.767369: step 22760, loss = 0.69, batch loss = 0.63 (14.9 examples/sec; 0.537 sec/batch; 46h:13m:54s remains)
INFO - root - 2017-12-11 05:55:32.353898: step 22770, loss = 0.71, batch loss = 0.65 (14.4 examples/sec; 0.555 sec/batch; 47h:47m:31s remains)
INFO - root - 2017-12-11 05:55:37.846920: step 22780, loss = 0.69, batch loss = 0.63 (14.4 examples/sec; 0.556 sec/batch; 47h:47m:46s remains)
INFO - root - 2017-12-11 05:55:42.977990: step 22790, loss = 0.67, batch loss = 0.61 (14.4 examples/sec; 0.556 sec/batch; 47h:49m:55s remains)
INFO - root - 2017-12-11 05:55:48.505150: step 22800, loss = 0.69, batch loss = 0.64 (14.5 examples/sec; 0.553 sec/batch; 47h:33m:08s remains)
2017-12-11 05:55:49.109497: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.028203376 0.022394985 0.019884886 0.023700181 0.036476228 0.056946993 0.078458212 0.091895424 0.093602754 0.085048735 0.069052525 0.051302098 0.0351773 0.0282515 0.036063515][0.032909766 0.018167889 0.0090167867 0.0096019525 0.020890741 0.039128289 0.055552833 0.060937688 0.055465054 0.044388264 0.03151897 0.021148613 0.013013886 0.011244572 0.021098722][0.045676477 0.032406788 0.027368212 0.033900134 0.050926942 0.071563035 0.084955163 0.08150854 0.064760916 0.044973072 0.028042065 0.017883111 0.011937571 0.012434811 0.023594685][0.064184174 0.06573841 0.079932742 0.1070842 0.14157547 0.17184357 0.18585478 0.17311186 0.13929273 0.10076413 0.068647519 0.047744095 0.034366038 0.030821325 0.040215854][0.088132367 0.11627182 0.16461055 0.2273957 0.29046944 0.33670878 0.35446829 0.33244556 0.27667493 0.21049604 0.15283917 0.10984657 0.077164643 0.059114005 0.059760205][0.11491232 0.17562932 0.26569203 0.3715274 0.46816087 0.53319311 0.55672604 0.52786809 0.45199931 0.35738337 0.26948318 0.19576801 0.13359192 0.091166563 0.075044684][0.13900873 0.2307328 0.35894784 0.50362533 0.63124454 0.71560562 0.74804586 0.71859533 0.63123012 0.51486504 0.3971996 0.2887933 0.19217171 0.12147842 0.085906684][0.15953867 0.27235159 0.42394713 0.59152341 0.73811746 0.83578652 0.87596047 0.84972817 0.75931793 0.6302194 0.48957655 0.35242689 0.22830194 0.13596922 0.085693881][0.16551563 0.2808058 0.43225181 0.59841496 0.74497 0.84461528 0.88836229 0.86777985 0.783005 0.65464431 0.50837338 0.3623029 0.23011115 0.13135764 0.075869158][0.14296959 0.24118675 0.36911386 0.50977629 0.63674021 0.72619939 0.76849741 0.75535613 0.68569976 0.57478374 0.44477218 0.31317177 0.19419631 0.10525637 0.054447588][0.10199405 0.16945212 0.25792032 0.35662556 0.44939193 0.51832145 0.55350846 0.54812354 0.50087172 0.42090291 0.32356471 0.22262861 0.13136692 0.064218268 0.026723063][0.06222276 0.097997092 0.14706185 0.20380265 0.2604208 0.30516967 0.32853872 0.32728523 0.30166754 0.25586176 0.19691186 0.1328316 0.074752778 0.03367437 0.012991288][0.031430904 0.045312896 0.068157047 0.095687442 0.12440163 0.1474029 0.15674502 0.15439841 0.14374729 0.12663734 0.10253852 0.072450966 0.044604022 0.026449731 0.020183992][0.013380294 0.018370239 0.030320819 0.043025758 0.054683242 0.062586054 0.061162766 0.056705706 0.055216692 0.058084108 0.059005175 0.05305282 0.04609476 0.043029245 0.04612894][0.0098325582 0.014949141 0.025701223 0.03372588 0.037719909 0.038428709 0.032812823 0.02936369 0.034522045 0.048927791 0.06294658 0.068554088 0.070997991 0.074391589 0.081665955]]...]
INFO - root - 2017-12-11 05:55:54.680899: step 22810, loss = 0.69, batch loss = 0.63 (14.1 examples/sec; 0.566 sec/batch; 48h:41m:43s remains)
INFO - root - 2017-12-11 05:56:00.138056: step 22820, loss = 0.71, batch loss = 0.65 (14.7 examples/sec; 0.546 sec/batch; 46h:58m:07s remains)
INFO - root - 2017-12-11 05:56:05.644713: step 22830, loss = 0.71, batch loss = 0.65 (14.3 examples/sec; 0.560 sec/batch; 48h:12m:50s remains)
INFO - root - 2017-12-11 05:56:11.194497: step 22840, loss = 0.70, batch loss = 0.64 (14.3 examples/sec; 0.560 sec/batch; 48h:07m:38s remains)
INFO - root - 2017-12-11 05:56:16.631166: step 22850, loss = 0.69, batch loss = 0.63 (14.6 examples/sec; 0.550 sec/batch; 47h:17m:30s remains)
INFO - root - 2017-12-11 05:56:22.047489: step 22860, loss = 0.70, batch loss = 0.64 (14.9 examples/sec; 0.537 sec/batch; 46h:10m:56s remains)
INFO - root - 2017-12-11 05:56:27.446134: step 22870, loss = 0.68, batch loss = 0.62 (14.5 examples/sec; 0.553 sec/batch; 47h:34m:48s remains)
INFO - root - 2017-12-11 05:56:32.975049: step 22880, loss = 0.71, batch loss = 0.66 (14.3 examples/sec; 0.558 sec/batch; 48h:00m:36s remains)
INFO - root - 2017-12-11 05:56:38.266350: step 22890, loss = 0.71, batch loss = 0.65 (14.2 examples/sec; 0.565 sec/batch; 48h:36m:09s remains)
INFO - root - 2017-12-11 05:56:43.817626: step 22900, loss = 0.67, batch loss = 0.62 (14.5 examples/sec; 0.552 sec/batch; 47h:30m:43s remains)
2017-12-11 05:56:44.419396: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.39582828 0.3794266 0.35452157 0.34284982 0.33603352 0.33058167 0.32528785 0.32596755 0.33729109 0.345997 0.33643407 0.31149319 0.2946935 0.29324463 0.29448712][0.43796131 0.41570476 0.38012481 0.35944939 0.34674504 0.33572182 0.32306606 0.31955716 0.33407563 0.35299969 0.35662019 0.34421852 0.33945063 0.35162812 0.36543006][0.45684794 0.43469515 0.39743775 0.37687668 0.3659313 0.35360983 0.33536166 0.32637995 0.34120235 0.36499086 0.37532932 0.36980373 0.37227011 0.39458928 0.41933009][0.45719719 0.43786758 0.40682161 0.39595598 0.39615029 0.38983041 0.3718867 0.36023554 0.37316567 0.39632466 0.40554011 0.39937967 0.40063858 0.42263463 0.44880655][0.43879449 0.43219697 0.422557 0.43525448 0.45726955 0.46592647 0.45728916 0.44807944 0.45790926 0.47273502 0.47130141 0.45410845 0.43974936 0.44251817 0.45059097][0.4008176 0.42351374 0.45439935 0.50504696 0.5589059 0.59357584 0.60693228 0.60767233 0.61154395 0.60812157 0.58422494 0.54166085 0.49257761 0.45345768 0.42368266][0.33267897 0.39702189 0.47968653 0.57548577 0.66644704 0.73578644 0.78066385 0.79576039 0.79027736 0.76020819 0.70392579 0.62221706 0.52167904 0.42453521 0.34599167][0.24952483 0.34862947 0.47324443 0.60437053 0.72357208 0.82200617 0.895569 0.9241603 0.9099142 0.85652918 0.77254719 0.654549 0.50437319 0.3531571 0.23206048][0.15915771 0.27288556 0.41594967 0.56002152 0.68584609 0.79201722 0.87602407 0.91066521 0.89226252 0.82902569 0.73497045 0.60182106 0.4282566 0.25148138 0.11233497][0.071167894 0.18106411 0.31976929 0.45448461 0.5653311 0.65682656 0.73144686 0.76286072 0.74358535 0.68123215 0.5912922 0.4632535 0.29601765 0.12819313 4.8141483e-05][-0.013674119 0.076292805 0.19140665 0.30049288 0.38493189 0.45229641 0.50873327 0.53252959 0.513548 0.45705432 0.37825197 0.268959 0.13098869 -0.0010866929 -0.0951296][-0.080357686 -0.022261688 0.05552483 0.1288107 0.18287794 0.22491537 0.26116157 0.27552471 0.25779191 0.21120231 0.1497376 0.069475323 -0.025099739 -0.10736185 -0.15700011][-0.11372013 -0.089017004 -0.050557841 -0.013541257 0.012513001 0.032121025 0.049485497 0.054526407 0.039451886 0.0065982891 -0.032852545 -0.079920352 -0.12945282 -0.16420279 -0.17454195][-0.12227933 -0.12155785 -0.11209253 -0.10182949 -0.09536016 -0.09119492 -0.0875149 -0.089205153 -0.10039709 -0.11919423 -0.13853122 -0.15842058 -0.17451438 -0.17730296 -0.16430551][-0.11385344 -0.1238389 -0.12843598 -0.13148694 -0.13429172 -0.13702108 -0.13971993 -0.14430939 -0.15234491 -0.16222189 -0.17036112 -0.17648813 -0.17712194 -0.1675684 -0.14859454]]...]
INFO - root - 2017-12-11 05:56:49.921597: step 22910, loss = 0.70, batch loss = 0.64 (14.5 examples/sec; 0.550 sec/batch; 47h:20m:24s remains)
INFO - root - 2017-12-11 05:56:55.388469: step 22920, loss = 0.70, batch loss = 0.64 (14.7 examples/sec; 0.544 sec/batch; 46h:44m:52s remains)
INFO - root - 2017-12-11 05:57:00.849764: step 22930, loss = 0.70, batch loss = 0.64 (14.5 examples/sec; 0.552 sec/batch; 47h:29m:00s remains)
INFO - root - 2017-12-11 05:57:06.380268: step 22940, loss = 0.71, batch loss = 0.65 (14.1 examples/sec; 0.567 sec/batch; 48h:45m:54s remains)
INFO - root - 2017-12-11 05:57:11.984137: step 22950, loss = 0.71, batch loss = 0.65 (14.5 examples/sec; 0.552 sec/batch; 47h:28m:00s remains)
INFO - root - 2017-12-11 05:57:17.490247: step 22960, loss = 0.70, batch loss = 0.64 (14.9 examples/sec; 0.536 sec/batch; 46h:06m:09s remains)
INFO - root - 2017-12-11 05:57:22.951763: step 22970, loss = 0.71, batch loss = 0.65 (14.3 examples/sec; 0.558 sec/batch; 47h:59m:34s remains)
INFO - root - 2017-12-11 05:57:28.298014: step 22980, loss = 0.70, batch loss = 0.64 (26.9 examples/sec; 0.297 sec/batch; 25h:33m:04s remains)
INFO - root - 2017-12-11 05:57:33.845116: step 22990, loss = 0.70, batch loss = 0.64 (14.8 examples/sec; 0.541 sec/batch; 46h:32m:39s remains)
INFO - root - 2017-12-11 05:57:39.352899: step 23000, loss = 0.69, batch loss = 0.63 (14.8 examples/sec; 0.542 sec/batch; 46h:37m:39s remains)
2017-12-11 05:57:39.949613: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.13804561 0.12474596 0.10926099 0.1138561 0.1496727 0.209113 0.27632502 0.33796719 0.37896711 0.38434413 0.35654432 0.31724432 0.28135183 0.23375908 0.14744376][0.10283769 0.092591777 0.082123034 0.091681466 0.12968342 0.18751813 0.25272343 0.31212819 0.35257119 0.35953206 0.33281124 0.29063275 0.24607545 0.18776463 0.098746516][0.1003269 0.086847171 0.0707317 0.076523423 0.1127116 0.16972935 0.23518142 0.29350048 0.33082813 0.33412483 0.30334237 0.25420874 0.1987855 0.13085376 0.043717824][0.13842709 0.11191091 0.079432085 0.075637288 0.1111858 0.17400654 0.24652943 0.30718613 0.34012273 0.33507937 0.29499558 0.23566782 0.16885151 0.092622 0.0074032331][0.19714989 0.15315211 0.10036992 0.085320756 0.12126898 0.19229916 0.27307382 0.33632278 0.365201 0.35280654 0.30628639 0.24184856 0.16979414 0.089179553 0.00385553][0.24856348 0.18994285 0.12221078 0.10051303 0.13912433 0.21875958 0.30712008 0.37373874 0.40148979 0.38487586 0.33455873 0.26824874 0.19522056 0.11309913 0.025990708][0.27135825 0.20628278 0.13498056 0.1152472 0.16065527 0.24993797 0.34632692 0.41800794 0.44723031 0.42717043 0.37117109 0.3011474 0.22798353 0.14732055 0.059824511][0.26111603 0.19859248 0.13504376 0.12516333 0.17985511 0.276879 0.37816304 0.45236591 0.48170257 0.45715341 0.39389318 0.32003805 0.24980842 0.17650944 0.093906678][0.23216443 0.18199772 0.13433552 0.13785845 0.19961707 0.29744056 0.39451811 0.46287349 0.48678741 0.45599595 0.38672587 0.31267452 0.25061807 0.19118702 0.11918436][0.2077291 0.17487119 0.14511918 0.15883207 0.21927196 0.30519745 0.38363075 0.43400127 0.44661769 0.4122608 0.34542525 0.28022614 0.23302996 0.19164905 0.13309516][0.20430833 0.1885656 0.17272258 0.1889091 0.23776661 0.2999174 0.34768888 0.37071785 0.36897957 0.33643618 0.28311384 0.23678926 0.2092592 0.18541178 0.13839796][0.22467305 0.22052397 0.21103984 0.22110745 0.25016329 0.28064197 0.29109189 0.28368184 0.26986316 0.2475775 0.21945021 0.20029399 0.19336684 0.18153365 0.13985519][0.24531001 0.24692854 0.2374188 0.2362231 0.24292824 0.24189097 0.2186086 0.18776996 0.17098716 0.16844657 0.17300279 0.18317303 0.19209316 0.18338169 0.1393476][0.24255784 0.24621092 0.23422642 0.22330165 0.21386206 0.19208312 0.14967112 0.11014444 0.10094097 0.12177962 0.1558038 0.18734236 0.20198537 0.18678483 0.1345865][0.20606053 0.20877045 0.1952621 0.17970243 0.16398649 0.13708654 0.095198259 0.064441696 0.071348868 0.11270268 0.16510367 0.20416044 0.2128817 0.1838855 0.12190191]]...]
INFO - root - 2017-12-11 05:57:45.461441: step 23010, loss = 0.70, batch loss = 0.64 (14.9 examples/sec; 0.537 sec/batch; 46h:07m:59s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-SGD-lr0.001-clip50-initfromfixqian30k-fixqian
INFO - root - 2017-12-11 05:57:50.994784: step 23020, loss = 0.69, batch loss = 0.63 (14.3 examples/sec; 0.558 sec/batch; 47h:57m:17s remains)
INFO - root - 2017-12-11 05:57:56.513188: step 23030, loss = 0.70, batch loss = 0.64 (14.8 examples/sec; 0.541 sec/batch; 46h:31m:21s remains)
INFO - root - 2017-12-11 05:58:02.008990: step 23040, loss = 0.68, batch loss = 0.63 (15.1 examples/sec; 0.531 sec/batch; 45h:41m:15s remains)
INFO - root - 2017-12-11 05:58:07.591821: step 23050, loss = 0.71, batch loss = 0.65 (14.4 examples/sec; 0.556 sec/batch; 47h:47m:17s remains)
INFO - root - 2017-12-11 05:58:13.161560: step 23060, loss = 0.70, batch loss = 0.64 (14.5 examples/sec; 0.553 sec/batch; 47h:31m:27s remains)
INFO - root - 2017-12-11 05:58:18.755100: step 23070, loss = 0.70, batch loss = 0.64 (14.7 examples/sec; 0.545 sec/batch; 46h:51m:15s remains)
INFO - root - 2017-12-11 05:58:23.964056: step 23080, loss = 0.69, batch loss = 0.63 (14.2 examples/sec; 0.564 sec/batch; 48h:28m:08s remains)
INFO - root - 2017-12-11 05:58:29.411083: step 23090, loss = 0.68, batch loss = 0.62 (14.7 examples/sec; 0.545 sec/batch; 46h:51m:09s remains)
INFO - root - 2017-12-11 05:58:34.923879: step 23100, loss = 0.68, batch loss = 0.63 (14.8 examples/sec; 0.541 sec/batch; 46h:28m:43s remains)
2017-12-11 05:58:35.612755: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.1059491 0.12950607 0.1654101 0.20242229 0.22553211 0.23101108 0.22386055 0.20905323 0.19077885 0.17092353 0.15467411 0.14810373 0.14996412 0.15778078 0.1680287][0.14999507 0.17484951 0.21278475 0.25474706 0.284047 0.29504895 0.29197562 0.27759725 0.25479668 0.2253208 0.19685423 0.17995393 0.17507641 0.17932242 0.18780027][0.18989833 0.21518949 0.2502144 0.28960079 0.31894255 0.33286607 0.33441046 0.32220405 0.29620776 0.25807992 0.21868077 0.19250447 0.18093336 0.18016641 0.18495111][0.2285099 0.25831258 0.28985232 0.32094729 0.34291035 0.35357729 0.35572726 0.34327579 0.31250122 0.26590493 0.21790905 0.18509823 0.16779427 0.16043061 0.1584467][0.25600094 0.29510385 0.32693562 0.34975898 0.36165971 0.3657355 0.36565742 0.35055372 0.3126311 0.25694147 0.20229182 0.16610545 0.14626668 0.13355309 0.12373119][0.26123539 0.31193951 0.34913716 0.3692928 0.37592494 0.37705433 0.3766377 0.35910815 0.31237206 0.24557984 0.18438953 0.14794359 0.13103847 0.1178882 0.10231464][0.23687592 0.29460248 0.33910573 0.36372915 0.37493414 0.38303167 0.38983536 0.37429968 0.31938472 0.24008922 0.17162342 0.13708827 0.12849787 0.12118105 0.10366844][0.19518873 0.25194359 0.3000741 0.33056721 0.35060742 0.37016702 0.38768592 0.37836418 0.32161909 0.23658447 0.16682474 0.13894503 0.14247705 0.14416514 0.12637568][0.16370857 0.21591175 0.2631233 0.29542482 0.31979129 0.34522337 0.36833861 0.36468247 0.31294408 0.23275922 0.17078312 0.15403084 0.16998768 0.17972457 0.1615991][0.15391332 0.20437118 0.25085872 0.28185323 0.30348307 0.3252396 0.34517446 0.34358266 0.30068159 0.2327788 0.18359229 0.1782939 0.20362717 0.2186562 0.20058651][0.15982929 0.21579026 0.26761439 0.30009073 0.31649831 0.32766017 0.33626515 0.3316271 0.29773334 0.24459654 0.20825239 0.21040504 0.23899803 0.25517064 0.23744178][0.17253402 0.23607397 0.2958802 0.33301508 0.34593755 0.3456361 0.3404665 0.33011195 0.30407095 0.26470903 0.23709299 0.23865348 0.26171523 0.27494073 0.25970039][0.18773767 0.25340942 0.31622395 0.35718742 0.37020409 0.36414102 0.34990406 0.33512536 0.31537777 0.28585348 0.25950187 0.249981 0.25752074 0.26306498 0.25185451][0.20767246 0.26972279 0.33004776 0.37456447 0.39464366 0.394894 0.38396138 0.37086251 0.35438469 0.32517445 0.28636739 0.25127703 0.23163496 0.22326656 0.215756][0.23103392 0.28447467 0.33844331 0.38414308 0.41209927 0.42415723 0.42545512 0.42088535 0.40747651 0.37283108 0.31424013 0.24857149 0.19948228 0.17633677 0.17208698]]...]
INFO - root - 2017-12-11 05:58:41.071175: step 23110, loss = 0.70, batch loss = 0.64 (14.5 examples/sec; 0.550 sec/batch; 47h:16m:34s remains)
INFO - root - 2017-12-11 05:58:46.577573: step 23120, loss = 0.70, batch loss = 0.65 (13.7 examples/sec; 0.583 sec/batch; 50h:03m:50s remains)
INFO - root - 2017-12-11 05:58:52.161608: step 23130, loss = 0.69, batch loss = 0.63 (15.1 examples/sec; 0.530 sec/batch; 45h:35m:07s remains)
INFO - root - 2017-12-11 05:58:57.682181: step 23140, loss = 0.72, batch loss = 0.67 (14.7 examples/sec; 0.543 sec/batch; 46h:41m:19s remains)
INFO - root - 2017-12-11 05:59:03.115200: step 23150, loss = 0.70, batch loss = 0.64 (14.9 examples/sec; 0.538 sec/batch; 46h:16m:20s remains)
INFO - root - 2017-12-11 05:59:08.691467: step 23160, loss = 0.70, batch loss = 0.64 (14.5 examples/sec; 0.551 sec/batch; 47h:23m:00s remains)
INFO - root - 2017-12-11 05:59:14.163932: step 23170, loss = 0.69, batch loss = 0.63 (14.7 examples/sec; 0.543 sec/batch; 46h:37m:30s remains)
INFO - root - 2017-12-11 05:59:19.418998: step 23180, loss = 0.71, batch loss = 0.65 (14.3 examples/sec; 0.560 sec/batch; 48h:07m:47s remains)
INFO - root - 2017-12-11 05:59:24.944639: step 23190, loss = 0.69, batch loss = 0.63 (14.5 examples/sec; 0.551 sec/batch; 47h:21m:23s remains)
INFO - root - 2017-12-11 05:59:30.419767: step 23200, loss = 0.70, batch loss = 0.64 (14.6 examples/sec; 0.547 sec/batch; 47h:00m:32s remains)
2017-12-11 05:59:31.025061: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.069946416 -0.065354824 -0.059714023 -0.055836152 -0.054753259 -0.057373334 -0.06317094 -0.070382319 -0.076297157 -0.079279639 -0.078942835 -0.07669165 -0.073993534 -0.072289914 -0.072200932][-0.073967747 -0.071452409 -0.067433864 -0.063654445 -0.06086009 -0.060743418 -0.064685024 -0.07237035 -0.081551515 -0.089032434 -0.092047676 -0.09043929 -0.085746907 -0.080793247 -0.0776439][-0.070198216 -0.069088064 -0.064660013 -0.057025746 -0.046991609 -0.038223293 -0.0358065 -0.042595092 -0.057381433 -0.074881591 -0.088291615 -0.09386985 -0.091736183 -0.0857541 -0.080026813][-0.05611448 -0.053764947 -0.04350492 -0.022633329 0.0079755336 0.040397692 0.06197704 0.062818684 0.040442035 0.0026475985 -0.036662735 -0.065647483 -0.079181127 -0.08065965 -0.076742239][-0.034888621 -0.028012749 -0.0059067635 0.038492884 0.10487878 0.17895503 0.23588677 0.25382367 0.22266051 0.15319633 0.070101634 -0.0012697468 -0.046009172 -0.065246448 -0.068651892][-0.012475754 0.00049123005 0.037881903 0.11243198 0.22561291 0.35524613 0.46078333 0.50410908 0.46436873 0.35543051 0.2156795 0.088207819 0.00098877144 -0.043634571 -0.058609262][0.0029082184 0.020557748 0.071426481 0.17358203 0.33034816 0.5121938 0.66390371 0.73185152 0.68528944 0.54078567 0.34953809 0.17097288 0.044694621 -0.023529634 -0.049437717][0.0055558477 0.023683289 0.079943918 0.19523405 0.37355646 0.58149773 0.75618446 0.83638883 0.78657937 0.62491953 0.40927926 0.20700893 0.062902912 -0.015727982 -0.045935746][-0.0049757846 0.0071097109 0.056055203 0.16187391 0.32828981 0.52355468 0.68785709 0.76318926 0.71650308 0.56539553 0.36501226 0.17863722 0.047382966 -0.022603197 -0.047881916][-0.021161042 -0.021298349 0.0083114095 0.084012687 0.20886518 0.35818279 0.48496225 0.54356617 0.50860167 0.39423886 0.24400841 0.10656869 0.012278682 -0.03539728 -0.050297219][-0.031212846 -0.046822593 -0.042146534 -0.0055277762 0.065491937 0.15575829 0.23529363 0.27467555 0.25751758 0.19254094 0.10702358 0.030354371 -0.020141091 -0.043449774 -0.048868671][-0.032125555 -0.061633132 -0.078612454 -0.075308822 -0.049690146 -0.0085818768 0.032868851 0.059159454 0.060669769 0.040104631 0.0096600959 -0.018095287 -0.036066517 -0.043809 -0.045256227][-0.029859032 -0.068408117 -0.098505318 -0.11428396 -0.11358234 -0.098116107 -0.074304186 -0.050254602 -0.033098068 -0.025401155 -0.025091724 -0.02926 -0.035081752 -0.040225226 -0.043792356][-0.024720158 -0.066549316 -0.10137026 -0.12268828 -0.12743224 -0.11567966 -0.091582216 -0.0623391 -0.036059748 -0.018777248 -0.012473592 -0.016442088 -0.02701007 -0.038688209 -0.047378603][-0.01021674 -0.050408129 -0.085011661 -0.10494009 -0.10517039 -0.086394913 -0.054798953 -0.020475529 0.0063757994 0.018553935 0.014868614 -0.0013411618 -0.023368439 -0.043385547 -0.056253228]]...]
INFO - root - 2017-12-11 05:59:36.501291: step 23210, loss = 0.70, batch loss = 0.64 (14.7 examples/sec; 0.545 sec/batch; 46h:47m:10s remains)
INFO - root - 2017-12-11 05:59:42.023797: step 23220, loss = 0.69, batch loss = 0.63 (14.4 examples/sec; 0.555 sec/batch; 47h:42m:54s remains)
INFO - root - 2017-12-11 05:59:47.538940: step 23230, loss = 0.70, batch loss = 0.64 (14.7 examples/sec; 0.544 sec/batch; 46h:46m:21s remains)
INFO - root - 2017-12-11 05:59:52.995490: step 23240, loss = 0.70, batch loss = 0.64 (14.5 examples/sec; 0.551 sec/batch; 47h:17m:31s remains)
INFO - root - 2017-12-11 05:59:58.558689: step 23250, loss = 0.66, batch loss = 0.61 (14.1 examples/sec; 0.566 sec/batch; 48h:34m:42s remains)
INFO - root - 2017-12-11 06:00:04.023867: step 23260, loss = 0.68, batch loss = 0.62 (14.7 examples/sec; 0.543 sec/batch; 46h:38m:35s remains)
INFO - root - 2017-12-11 06:00:09.463186: step 23270, loss = 0.70, batch loss = 0.64 (16.0 examples/sec; 0.501 sec/batch; 43h:04m:29s remains)
INFO - root - 2017-12-11 06:00:14.701379: step 23280, loss = 0.69, batch loss = 0.63 (14.6 examples/sec; 0.548 sec/batch; 47h:01m:55s remains)
INFO - root - 2017-12-11 06:00:20.182308: step 23290, loss = 0.69, batch loss = 0.64 (15.2 examples/sec; 0.526 sec/batch; 45h:08m:28s remains)
INFO - root - 2017-12-11 06:00:25.718370: step 23300, loss = 0.71, batch loss = 0.65 (14.0 examples/sec; 0.571 sec/batch; 49h:02m:47s remains)
2017-12-11 06:00:26.284508: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.19829035 0.1722775 0.16662239 0.17384745 0.18621798 0.20004362 0.21727084 0.23080885 0.22244109 0.19842766 0.17880668 0.17221828 0.16659221 0.15210763 0.12987083][0.22576642 0.21160012 0.20769587 0.2066533 0.20457563 0.20375465 0.2087201 0.21660185 0.21262778 0.19970624 0.19198656 0.19332923 0.19032937 0.17340624 0.14546332][0.22731803 0.23194078 0.23767278 0.23716661 0.2291957 0.21975227 0.21572843 0.21784754 0.21536268 0.20845194 0.207042 0.21131517 0.20715715 0.18600225 0.15289029][0.2104951 0.23753542 0.26024902 0.27094045 0.26911154 0.26123011 0.25465077 0.251274 0.244193 0.23351392 0.22814067 0.22695228 0.215982 0.18895467 0.1519303][0.20460713 0.25138357 0.29396498 0.3231692 0.33704358 0.34007949 0.3375583 0.32941481 0.31150138 0.28741211 0.26922494 0.25601625 0.23452976 0.2002946 0.15918085][0.2211456 0.28364986 0.34655988 0.398235 0.43333992 0.45338339 0.46039131 0.44933194 0.41718003 0.37389937 0.33785635 0.30900222 0.2740173 0.22971494 0.18204829][0.24850133 0.32351577 0.40530005 0.4794687 0.536202 0.57398307 0.59235132 0.58050662 0.53513026 0.47264639 0.41791666 0.37127942 0.31961206 0.26206383 0.2057938][0.276592 0.3573885 0.44972745 0.5375303 0.60745054 0.65636843 0.683726 0.67447829 0.62347734 0.55063444 0.48446333 0.4243561 0.35785645 0.28742421 0.22258699][0.28006276 0.35545793 0.44295609 0.52744603 0.59510416 0.64316994 0.67214578 0.6657809 0.6173023 0.54723275 0.48408175 0.42503804 0.35783955 0.28645471 0.22175787][0.23489988 0.29400808 0.36301607 0.42958742 0.48242751 0.52038288 0.54479074 0.540813 0.50131071 0.44556746 0.39950714 0.357456 0.30642763 0.24930406 0.19612123][0.14272965 0.18219745 0.23026671 0.27730873 0.31511909 0.34335074 0.36287269 0.36116645 0.33175039 0.29226163 0.26623428 0.24684745 0.220462 0.18663889 0.15191627][0.04037866 0.063600577 0.095718876 0.12856495 0.15644564 0.17857754 0.19387123 0.19241346 0.17014264 0.14346366 0.13389133 0.13505799 0.13408808 0.12598422 0.11111814][-0.034998927 -0.019560963 0.005251443 0.031775791 0.055519983 0.074528418 0.085362323 0.081041306 0.061632488 0.043486509 0.046043981 0.063765056 0.084924422 0.099517882 0.10061226][-0.064727217 -0.047781032 -0.022029992 0.0044626365 0.027310068 0.04372577 0.049004003 0.039115947 0.018801104 0.0058340989 0.017338753 0.047173258 0.084286489 0.11498553 0.12621844][-0.05458219 -0.029394444 0.0032800417 0.033990908 0.05751973 0.070896469 0.070122942 0.054120306 0.031587083 0.022161447 0.04034793 0.077822313 0.12332854 0.16164848 0.17647904]]...]
INFO - root - 2017-12-11 06:00:31.763378: step 23310, loss = 0.70, batch loss = 0.64 (14.3 examples/sec; 0.559 sec/batch; 48h:00m:59s remains)
INFO - root - 2017-12-11 06:00:37.371502: step 23320, loss = 0.70, batch loss = 0.64 (14.6 examples/sec; 0.549 sec/batch; 47h:09m:44s remains)
INFO - root - 2017-12-11 06:00:42.858055: step 23330, loss = 0.70, batch loss = 0.64 (14.0 examples/sec; 0.572 sec/batch; 49h:06m:51s remains)
INFO - root - 2017-12-11 06:00:48.361577: step 23340, loss = 0.70, batch loss = 0.64 (14.5 examples/sec; 0.551 sec/batch; 47h:17m:17s remains)
INFO - root - 2017-12-11 06:00:53.871905: step 23350, loss = 0.72, batch loss = 0.66 (14.1 examples/sec; 0.566 sec/batch; 48h:36m:48s remains)
INFO - root - 2017-12-11 06:00:59.401071: step 23360, loss = 0.70, batch loss = 0.64 (14.5 examples/sec; 0.551 sec/batch; 47h:20m:47s remains)
INFO - root - 2017-12-11 06:01:04.507297: step 23370, loss = 0.72, batch loss = 0.66 (14.5 examples/sec; 0.550 sec/batch; 47h:14m:27s remains)
INFO - root - 2017-12-11 06:01:10.086067: step 23380, loss = 0.70, batch loss = 0.64 (14.3 examples/sec; 0.558 sec/batch; 47h:55m:34s remains)
INFO - root - 2017-12-11 06:01:15.546640: step 23390, loss = 0.69, batch loss = 0.64 (14.7 examples/sec; 0.544 sec/batch; 46h:44m:58s remains)
INFO - root - 2017-12-11 06:01:21.019567: step 23400, loss = 0.69, batch loss = 0.64 (14.2 examples/sec; 0.565 sec/batch; 48h:28m:12s remains)
2017-12-11 06:01:21.663996: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.22698618 0.17828266 0.13519406 0.12085564 0.15214039 0.2195905 0.29592195 0.34704405 0.35466656 0.32860228 0.28761727 0.25001171 0.22466317 0.21593986 0.21900471][0.21280637 0.16596472 0.12913862 0.12770422 0.17377317 0.25344238 0.33528462 0.38527924 0.38466382 0.34440109 0.28631619 0.23073138 0.18864301 0.16511454 0.15907605][0.20152679 0.15832382 0.12856184 0.13860092 0.19587645 0.2820195 0.36320296 0.40759197 0.3984108 0.34700942 0.27429071 0.2021464 0.14414905 0.10743985 0.093607455][0.21467982 0.16783579 0.13669297 0.14892346 0.20963104 0.29727048 0.37655354 0.41767493 0.4048503 0.34731355 0.26362681 0.17795558 0.10739891 0.062033825 0.045111742][0.24000037 0.18821293 0.15186004 0.15970469 0.21649735 0.30028415 0.3765735 0.41682917 0.40515 0.34722346 0.25765485 0.16302451 0.084825046 0.036087282 0.02029397][0.26290432 0.21187171 0.17588252 0.18235087 0.23410603 0.31054297 0.38090348 0.41813707 0.406664 0.34923702 0.25704396 0.15714076 0.074105144 0.024127169 0.010789742][0.26916224 0.2287306 0.20447972 0.21865231 0.27000332 0.33964252 0.40279433 0.43436897 0.41997007 0.36103809 0.26692548 0.16228667 0.072185427 0.01710286 0.0027912371][0.25672042 0.23633666 0.2338441 0.26261094 0.31680429 0.37989342 0.43452549 0.45860958 0.43956292 0.37889481 0.28506118 0.17787918 0.079358838 0.01470379 -0.0053310473][0.23148781 0.23718591 0.25954652 0.30136251 0.35409147 0.40622804 0.44973862 0.4665378 0.44583222 0.38863444 0.30154312 0.19760428 0.093762264 0.019413354 -0.0082379077][0.20807208 0.24206816 0.28604397 0.33269218 0.3739433 0.4070918 0.43602216 0.44715732 0.43028116 0.38444081 0.31235722 0.21921296 0.11633071 0.035608284 0.0008079682][0.19940455 0.25740609 0.31378755 0.35441315 0.37476489 0.38307357 0.39541927 0.4027237 0.39361143 0.36337024 0.31041139 0.23321708 0.13797285 0.056411058 0.016628334][0.20266651 0.273752 0.33024368 0.35591748 0.35237306 0.33746284 0.33593744 0.3424958 0.34363565 0.32994223 0.29575679 0.23594755 0.1533902 0.076124325 0.033035904][0.20966126 0.279471 0.32379121 0.32919514 0.30409992 0.27252614 0.26205349 0.27025527 0.28199965 0.28331864 0.26515657 0.22239171 0.1569784 0.090120733 0.046825312][0.19752362 0.25284183 0.27794921 0.26517147 0.22798777 0.19162367 0.1810199 0.19546404 0.21891178 0.23347995 0.22797181 0.19992964 0.15273581 0.099576958 0.058190927][0.14781609 0.1824106 0.18994887 0.16854461 0.13313144 0.10544074 0.10499328 0.13203605 0.17049184 0.19961433 0.20684388 0.19188456 0.16032404 0.11890671 0.0777433]]...]
INFO - root - 2017-12-11 06:01:27.099794: step 23410, loss = 0.69, batch loss = 0.63 (14.6 examples/sec; 0.547 sec/batch; 46h:57m:29s remains)
INFO - root - 2017-12-11 06:01:32.583927: step 23420, loss = 0.69, batch loss = 0.63 (14.5 examples/sec; 0.553 sec/batch; 47h:30m:00s remains)
INFO - root - 2017-12-11 06:01:38.064714: step 23430, loss = 0.71, batch loss = 0.65 (14.6 examples/sec; 0.549 sec/batch; 47h:07m:41s remains)
INFO - root - 2017-12-11 06:01:43.584738: step 23440, loss = 0.69, batch loss = 0.63 (14.3 examples/sec; 0.559 sec/batch; 47h:57m:53s remains)
INFO - root - 2017-12-11 06:01:49.107397: step 23450, loss = 0.68, batch loss = 0.63 (14.7 examples/sec; 0.545 sec/batch; 46h:45m:59s remains)
INFO - root - 2017-12-11 06:01:54.608633: step 23460, loss = 0.70, batch loss = 0.65 (15.0 examples/sec; 0.534 sec/batch; 45h:51m:28s remains)
INFO - root - 2017-12-11 06:01:59.755516: step 23470, loss = 0.69, batch loss = 0.63 (14.7 examples/sec; 0.545 sec/batch; 46h:44m:53s remains)
INFO - root - 2017-12-11 06:02:05.211990: step 23480, loss = 0.69, batch loss = 0.64 (14.9 examples/sec; 0.537 sec/batch; 46h:05m:48s remains)
INFO - root - 2017-12-11 06:02:10.647196: step 23490, loss = 0.69, batch loss = 0.63 (14.8 examples/sec; 0.540 sec/batch; 46h:21m:31s remains)
INFO - root - 2017-12-11 06:02:16.195993: step 23500, loss = 0.71, batch loss = 0.65 (14.4 examples/sec; 0.554 sec/batch; 47h:32m:38s remains)
2017-12-11 06:02:16.753886: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.071673937 0.065997928 0.042641025 0.017593248 -0.0034967053 -0.020679079 -0.030842397 -0.032211341 -0.026497396 -0.015359059 -0.0012467817 0.014463968 0.027843479 0.03147969 0.01694376][0.17667434 0.18075831 0.15845005 0.13170338 0.10679872 0.084684804 0.071995459 0.072557546 0.083045185 0.098534174 0.11470945 0.13059284 0.14163049 0.13746794 0.10657904][0.29989171 0.31767264 0.30223432 0.2785832 0.25435448 0.23236717 0.22142582 0.22669114 0.24185252 0.25854567 0.2712684 0.28046262 0.28116304 0.25967798 0.20427711][0.39519638 0.4258301 0.42059422 0.40556967 0.39161575 0.38214332 0.38312262 0.39781117 0.41658476 0.43042338 0.43460378 0.432017 0.41579169 0.36956069 0.28427637][0.44235745 0.48090398 0.48677638 0.48559266 0.49057278 0.50211155 0.52029 0.54475206 0.56369507 0.57019693 0.56259036 0.54653859 0.51140672 0.43857387 0.32380152][0.48845536 0.532876 0.55078566 0.56744277 0.59518331 0.62803251 0.65970588 0.68490309 0.69151396 0.6790905 0.65254521 0.62091541 0.56863129 0.47363716 0.33560666][0.54753649 0.598565 0.62985796 0.66472906 0.71310294 0.76373094 0.80504268 0.82427251 0.80861056 0.76790178 0.71521646 0.66319996 0.59341514 0.48200911 0.3310011][0.59669632 0.65310723 0.69319338 0.73680055 0.79273772 0.84836358 0.88998175 0.89849854 0.8619678 0.79832667 0.72446209 0.65605462 0.57515627 0.45882666 0.30837458][0.6119352 0.66947663 0.70996815 0.74839127 0.79321015 0.83467436 0.86201984 0.85516053 0.80619287 0.73681384 0.66107905 0.5927856 0.51558614 0.40943229 0.27275911][0.58571404 0.63573891 0.66624051 0.68839186 0.70943588 0.723575 0.72567117 0.70027953 0.64627934 0.58589822 0.52650088 0.47542146 0.41730338 0.33460847 0.22241984][0.5099991 0.54402739 0.55782992 0.56119066 0.55974966 0.55011857 0.53096205 0.49197739 0.4381023 0.39099047 0.35255584 0.32356349 0.28948632 0.23549022 0.15349922][0.36444217 0.37827909 0.37396824 0.36062947 0.3440707 0.32279187 0.29622695 0.25701633 0.21231715 0.18113638 0.16215153 0.15231176 0.13903569 0.11099156 0.05915137][0.18079229 0.1768658 0.15826884 0.13166086 0.1043053 0.077749908 0.051859695 0.021463359 -0.0065298406 -0.018246649 -0.017601311 -0.010447985 -0.0066022268 -0.013523461 -0.039133321][0.015258592 -0.0001389122 -0.024232758 -0.054486994 -0.084340356 -0.11025562 -0.13138932 -0.15050727 -0.16215453 -0.15762487 -0.14189869 -0.12274922 -0.10774584 -0.10124554 -0.10861328][-0.10349302 -0.12355017 -0.144922 -0.16892682 -0.19173954 -0.21061122 -0.22458701 -0.234342 -0.23581016 -0.22410829 -0.20361577 -0.18155581 -0.16326271 -0.15103227 -0.14780724]]...]
INFO - root - 2017-12-11 06:02:22.338834: step 23510, loss = 0.70, batch loss = 0.65 (14.2 examples/sec; 0.565 sec/batch; 48h:29m:40s remains)
INFO - root - 2017-12-11 06:02:27.865703: step 23520, loss = 0.68, batch loss = 0.62 (14.7 examples/sec; 0.545 sec/batch; 46h:48m:12s remains)
INFO - root - 2017-12-11 06:02:33.301521: step 23530, loss = 0.69, batch loss = 0.63 (14.8 examples/sec; 0.539 sec/batch; 46h:14m:40s remains)
INFO - root - 2017-12-11 06:02:38.767819: step 23540, loss = 0.72, batch loss = 0.66 (14.6 examples/sec; 0.547 sec/batch; 46h:59m:07s remains)
INFO - root - 2017-12-11 06:02:44.332866: step 23550, loss = 0.69, batch loss = 0.64 (14.1 examples/sec; 0.568 sec/batch; 48h:46m:53s remains)
INFO - root - 2017-12-11 06:02:49.809219: step 23560, loss = 0.69, batch loss = 0.64 (14.8 examples/sec; 0.542 sec/batch; 46h:28m:32s remains)
INFO - root - 2017-12-11 06:02:55.080410: step 23570, loss = 0.69, batch loss = 0.63 (14.3 examples/sec; 0.561 sec/batch; 48h:09m:29s remains)
INFO - root - 2017-12-11 06:03:00.606787: step 23580, loss = 0.71, batch loss = 0.65 (14.4 examples/sec; 0.557 sec/batch; 47h:50m:10s remains)
INFO - root - 2017-12-11 06:03:06.032360: step 23590, loss = 0.71, batch loss = 0.65 (14.8 examples/sec; 0.541 sec/batch; 46h:23m:39s remains)
INFO - root - 2017-12-11 06:03:11.637983: step 23600, loss = 0.69, batch loss = 0.63 (14.2 examples/sec; 0.565 sec/batch; 48h:27m:33s remains)
2017-12-11 06:03:12.222676: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.030192941 0.027395425 0.023193151 0.020341134 0.019790238 0.021248657 0.022404153 0.02240859 0.02435071 0.028410859 0.031172959 0.030862059 0.028269347 0.024745481 0.019302361][0.049834531 0.044255614 0.036979821 0.032637671 0.032494396 0.035504755 0.038494159 0.040198032 0.044352654 0.051600542 0.057232283 0.058258917 0.055095125 0.049254023 0.040106665][0.06554094 0.057983175 0.047769602 0.042098384 0.042530283 0.047293883 0.052506443 0.056529079 0.063247353 0.073849253 0.082878821 0.085744344 0.082243718 0.073675215 0.060177356][0.073523611 0.0645829 0.053268444 0.047573276 0.049376722 0.055975735 0.062842645 0.068332613 0.076393344 0.089014135 0.10056385 0.1050259 0.10134605 0.090155624 0.072534226][0.07826405 0.06975954 0.059483025 0.054972988 0.058326762 0.065902375 0.072579257 0.077524424 0.085073933 0.0981281 0.11106224 0.1166133 0.11263202 0.098960429 0.077644885][0.083583377 0.076811507 0.068048507 0.064212047 0.067906983 0.075073458 0.0803362 0.08403644 0.090584718 0.10350997 0.11718952 0.12337548 0.11892739 0.10305013 0.078717723][0.091255158 0.086570516 0.078706577 0.074407063 0.076959826 0.08252009 0.085783444 0.087912768 0.092963144 0.10469295 0.1176377 0.12324752 0.11800761 0.10083198 0.075151823][0.1012957 0.098690666 0.09072262 0.084919013 0.085106589 0.088070408 0.08896517 0.089193814 0.092089437 0.10134072 0.11195023 0.11598366 0.11016188 0.093191884 0.068292178][0.11733378 0.1165475 0.10815518 0.10001913 0.096687064 0.096026242 0.094014823 0.0918987 0.09223827 0.098369688 0.10582067 0.10794805 0.10207748 0.086563677 0.063671865][0.13973455 0.14008299 0.13079575 0.11979816 0.11251102 0.10798265 0.10292383 0.098501951 0.096796572 0.10034172 0.10481558 0.10481707 0.098478638 0.083772659 0.061853789][0.1551342 0.15589008 0.1462047 0.13376285 0.12437531 0.11780594 0.11123794 0.10586912 0.10356744 0.10584234 0.10826574 0.10624262 0.098847225 0.083573125 0.060892891][0.1495688 0.14984034 0.14135258 0.13049263 0.12233179 0.11634019 0.10999605 0.10476679 0.10257034 0.10430039 0.10560475 0.10237067 0.094331428 0.078739382 0.055664461][0.12549108 0.1241552 0.11734127 0.10955873 0.10423701 0.10002044 0.094730265 0.090386435 0.088959232 0.09101969 0.092464253 0.08929351 0.081571318 0.06686835 0.045143567][0.0943004 0.091417134 0.0862269 0.081168935 0.078114152 0.0751688 0.070761919 0.067450278 0.06712468 0.069719195 0.071418785 0.0683571 0.061053902 0.048158556 0.029704539][0.062616192 0.059264764 0.055608112 0.052417781 0.050614458 0.04842877 0.044979855 0.042983014 0.043836955 0.046549954 0.047674961 0.043857124 0.036320668 0.025155984 0.010785452]]...]
INFO - root - 2017-12-11 06:03:17.816931: step 23610, loss = 0.68, batch loss = 0.62 (14.8 examples/sec; 0.540 sec/batch; 46h:22m:27s remains)
INFO - root - 2017-12-11 06:03:23.289807: step 23620, loss = 0.67, batch loss = 0.61 (14.7 examples/sec; 0.546 sec/batch; 46h:50m:34s remains)
INFO - root - 2017-12-11 06:03:28.838694: step 23630, loss = 0.69, batch loss = 0.63 (13.9 examples/sec; 0.575 sec/batch; 49h:21m:06s remains)
INFO - root - 2017-12-11 06:03:34.420813: step 23640, loss = 0.68, batch loss = 0.63 (14.3 examples/sec; 0.560 sec/batch; 48h:01m:44s remains)
INFO - root - 2017-12-11 06:03:39.925440: step 23650, loss = 0.71, batch loss = 0.65 (14.2 examples/sec; 0.562 sec/batch; 48h:11m:21s remains)
INFO - root - 2017-12-11 06:03:45.072135: step 23660, loss = 0.70, batch loss = 0.64 (25.5 examples/sec; 0.314 sec/batch; 26h:56m:59s remains)
INFO - root - 2017-12-11 06:03:50.610731: step 23670, loss = 0.70, batch loss = 0.64 (14.4 examples/sec; 0.554 sec/batch; 47h:33m:51s remains)
INFO - root - 2017-12-11 06:03:56.077405: step 23680, loss = 0.70, batch loss = 0.64 (15.0 examples/sec; 0.534 sec/batch; 45h:49m:12s remains)
INFO - root - 2017-12-11 06:04:01.541896: step 23690, loss = 0.70, batch loss = 0.64 (15.2 examples/sec; 0.527 sec/batch; 45h:14m:28s remains)
INFO - root - 2017-12-11 06:04:07.142994: step 23700, loss = 0.71, batch loss = 0.65 (14.9 examples/sec; 0.536 sec/batch; 46h:00m:17s remains)
2017-12-11 06:04:07.678881: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.31763026 0.31879893 0.3209891 0.31851813 0.30398637 0.2716659 0.22616273 0.18155387 0.15813921 0.17819571 0.23647875 0.30407608 0.342969 0.34151754 0.30503386][0.28275114 0.27423123 0.27046493 0.26921126 0.26199427 0.24298207 0.21657701 0.19040668 0.17630365 0.18862751 0.22549622 0.26481515 0.27973109 0.26567876 0.23048308][0.21698044 0.20421174 0.20438358 0.21664922 0.22977507 0.2354293 0.23548746 0.23215882 0.22956266 0.23396608 0.24779069 0.25995076 0.25661683 0.2364351 0.20602559][0.14184439 0.13640577 0.15253569 0.18865159 0.22912951 0.261803 0.28625116 0.30312049 0.31217846 0.3136856 0.31338841 0.3082909 0.29383904 0.27036855 0.2423383][0.082503006 0.094119169 0.1346788 0.1988942 0.266159 0.32052559 0.35988811 0.38750643 0.40342867 0.40476161 0.3984001 0.38513461 0.36555508 0.34035859 0.31251374][0.051711336 0.084232621 0.14854105 0.23544626 0.32063064 0.38567513 0.42736602 0.45406073 0.46960992 0.470704 0.46286604 0.44717398 0.42643312 0.40248054 0.37794515][0.046604767 0.097249627 0.17867579 0.27841154 0.37037724 0.43544254 0.47111171 0.48990524 0.5020417 0.50542384 0.50172848 0.48840138 0.46754336 0.4446733 0.42393211][0.06452731 0.1262728 0.21481571 0.31472513 0.40044716 0.45476779 0.47882789 0.48919916 0.50107133 0.51201087 0.51778311 0.50929135 0.48613438 0.46018368 0.4408367][0.10194442 0.16552429 0.24805354 0.33370966 0.40036422 0.43628785 0.44841 0.45387802 0.46949056 0.49212113 0.51131725 0.51044726 0.48560759 0.45626813 0.43951893][0.14698899 0.2028317 0.26850986 0.33113977 0.37384772 0.39270076 0.40052184 0.41074979 0.43700758 0.47316098 0.50430983 0.50936806 0.48243678 0.45095795 0.43821278][0.17752491 0.21674357 0.25889584 0.29726315 0.32069227 0.33235791 0.34798002 0.3741127 0.41709998 0.46511126 0.50109643 0.50505686 0.47403279 0.44349435 0.43797666][0.1831881 0.19945906 0.21585084 0.23361646 0.24612898 0.25976792 0.28892973 0.33333752 0.39121693 0.44485953 0.47760218 0.47508326 0.4410283 0.41655865 0.4224059][0.17133909 0.16276658 0.15578896 0.1586653 0.16704798 0.18555269 0.22392033 0.27740327 0.33937439 0.38982362 0.41426685 0.40427828 0.36987561 0.35357282 0.37064838][0.15077002 0.12236172 0.098542161 0.0922678 0.099484213 0.1196908 0.15639545 0.20368619 0.25565854 0.29599696 0.31289938 0.30078793 0.27157515 0.26287043 0.28427431][0.12157201 0.083258614 0.051697511 0.041517403 0.048268747 0.066575654 0.094688229 0.12788218 0.16397537 0.19344829 0.20684683 0.19874541 0.17810719 0.17195524 0.18506536]]...]
INFO - root - 2017-12-11 06:04:13.275361: step 23710, loss = 0.69, batch loss = 0.63 (14.7 examples/sec; 0.543 sec/batch; 46h:32m:15s remains)
INFO - root - 2017-12-11 06:04:18.726047: step 23720, loss = 0.72, batch loss = 0.66 (14.4 examples/sec; 0.554 sec/batch; 47h:30m:37s remains)
INFO - root - 2017-12-11 06:04:24.232432: step 23730, loss = 0.70, batch loss = 0.64 (14.6 examples/sec; 0.548 sec/batch; 46h:58m:41s remains)
INFO - root - 2017-12-11 06:04:29.758752: step 23740, loss = 0.68, batch loss = 0.62 (14.6 examples/sec; 0.548 sec/batch; 47h:01m:14s remains)
INFO - root - 2017-12-11 06:04:35.300097: step 23750, loss = 0.70, batch loss = 0.64 (14.6 examples/sec; 0.546 sec/batch; 46h:50m:44s remains)
INFO - root - 2017-12-11 06:04:40.460444: step 23760, loss = 0.71, batch loss = 0.65 (14.2 examples/sec; 0.562 sec/batch; 48h:12m:38s remains)
INFO - root - 2017-12-11 06:04:45.955725: step 23770, loss = 0.69, batch loss = 0.64 (14.6 examples/sec; 0.547 sec/batch; 46h:54m:57s remains)
INFO - root - 2017-12-11 06:04:51.422356: step 23780, loss = 0.69, batch loss = 0.64 (14.8 examples/sec; 0.540 sec/batch; 46h:16m:27s remains)
INFO - root - 2017-12-11 06:04:57.040380: step 23790, loss = 0.70, batch loss = 0.64 (14.5 examples/sec; 0.551 sec/batch; 47h:16m:47s remains)
INFO - root - 2017-12-11 06:05:02.524143: step 23800, loss = 0.68, batch loss = 0.62 (14.5 examples/sec; 0.551 sec/batch; 47h:15m:11s remains)
2017-12-11 06:05:03.130543: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.13427591 0.1307871 0.12754034 0.12468139 0.12486267 0.13117523 0.1426008 0.15387207 0.16338328 0.16991881 0.17019522 0.16200849 0.14597394 0.12646018 0.10782005][0.13756992 0.13419065 0.13095856 0.12961872 0.13305061 0.14393391 0.16016202 0.17503263 0.18570597 0.19029877 0.18615387 0.1723454 0.15117504 0.12841362 0.10941041][0.13302189 0.13302067 0.13236119 0.13403921 0.14006214 0.15267093 0.16895889 0.18233958 0.1892271 0.18868107 0.17973612 0.16375443 0.1440199 0.12476767 0.11059595][0.11818159 0.1199876 0.12109201 0.12483117 0.13259873 0.14589615 0.16106589 0.17180543 0.17448625 0.16926624 0.15744968 0.14291255 0.12949345 0.11864714 0.11268541][0.098547593 0.09954603 0.099903338 0.10369594 0.11312366 0.12846626 0.14472282 0.15531318 0.15640709 0.14879566 0.13554393 0.12233832 0.11504682 0.11315423 0.11576457][0.083652377 0.083667442 0.083592393 0.088203207 0.10027906 0.11847182 0.13643403 0.14808512 0.14962418 0.1420396 0.1289755 0.11676374 0.11325891 0.11709505 0.12448227][0.078689307 0.080833808 0.083197266 0.08996018 0.10375126 0.12244653 0.13940334 0.15021738 0.15257466 0.14745411 0.13717054 0.12796193 0.1276515 0.13447189 0.1422113][0.076893449 0.083773687 0.091001727 0.10078184 0.11517925 0.13200453 0.14605232 0.15537101 0.15907092 0.1570023 0.14963707 0.14274189 0.14309537 0.14899154 0.15307991][0.07065887 0.081896767 0.094267324 0.10760318 0.12304301 0.1383951 0.15053971 0.15925413 0.16442016 0.1641624 0.15783723 0.15099116 0.14972737 0.15268087 0.1529877][0.061599817 0.076376267 0.094044678 0.11261222 0.13152011 0.14803153 0.160078 0.16803013 0.17188711 0.16948894 0.16026682 0.15005079 0.14503647 0.1444239 0.14248793][0.052684467 0.069622651 0.091831319 0.11618918 0.14013262 0.16004618 0.17345314 0.18005592 0.17966786 0.1713983 0.15600327 0.14014438 0.12996815 0.12551963 0.12236813][0.045781467 0.063975185 0.088947847 0.11690784 0.14425689 0.16662037 0.18128897 0.1869545 0.18275468 0.1688567 0.14782478 0.12728561 0.11288819 0.10529429 0.10108869][0.047811348 0.064556763 0.087972127 0.11515524 0.14281055 0.1662406 0.18301103 0.19060053 0.1862489 0.16973625 0.14541562 0.12177487 0.10405666 0.09320084 0.086702384][0.057735521 0.06974484 0.087748237 0.1107358 0.13565058 0.15789638 0.17560916 0.18563443 0.18339683 0.16802941 0.1447785 0.12211253 0.10460019 0.093090743 0.08589603][0.068892673 0.076031566 0.088576958 0.10744029 0.12903361 0.14896482 0.16557582 0.17583692 0.17495023 0.16230784 0.1433671 0.12512456 0.11025066 0.098940171 0.090759553]]...]
INFO - root - 2017-12-11 06:05:08.624329: step 23810, loss = 0.70, batch loss = 0.65 (14.9 examples/sec; 0.538 sec/batch; 46h:08m:50s remains)
INFO - root - 2017-12-11 06:05:14.095782: step 23820, loss = 0.70, batch loss = 0.64 (14.8 examples/sec; 0.539 sec/batch; 46h:11m:32s remains)
INFO - root - 2017-12-11 06:05:19.577343: step 23830, loss = 0.70, batch loss = 0.64 (14.3 examples/sec; 0.557 sec/batch; 47h:48m:02s remains)
INFO - root - 2017-12-11 06:05:25.131629: step 23840, loss = 0.71, batch loss = 0.65 (14.6 examples/sec; 0.548 sec/batch; 47h:01m:31s remains)
INFO - root - 2017-12-11 06:05:30.644973: step 23850, loss = 0.69, batch loss = 0.63 (14.9 examples/sec; 0.538 sec/batch; 46h:08m:39s remains)
INFO - root - 2017-12-11 06:05:35.928506: step 23860, loss = 0.68, batch loss = 0.62 (14.1 examples/sec; 0.568 sec/batch; 48h:42m:59s remains)
INFO - root - 2017-12-11 06:05:41.365855: step 23870, loss = 0.69, batch loss = 0.63 (14.9 examples/sec; 0.536 sec/batch; 45h:55m:50s remains)
INFO - root - 2017-12-11 06:05:46.855306: step 23880, loss = 0.69, batch loss = 0.63 (14.8 examples/sec; 0.539 sec/batch; 46h:12m:13s remains)
INFO - root - 2017-12-11 06:05:52.420518: step 23890, loss = 0.70, batch loss = 0.64 (14.4 examples/sec; 0.557 sec/batch; 47h:46m:52s remains)
INFO - root - 2017-12-11 06:05:57.946368: step 23900, loss = 0.69, batch loss = 0.64 (14.4 examples/sec; 0.555 sec/batch; 47h:32m:53s remains)
2017-12-11 06:05:58.529701: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.1325842 0.14837801 0.16226248 0.16040568 0.15435345 0.15767616 0.16488789 0.16901033 0.16639453 0.16715065 0.16506208 0.14923029 0.12549086 0.10763257 0.10262872][0.12667163 0.14543818 0.16231166 0.1601117 0.15226686 0.15498023 0.16224171 0.16544221 0.161393 0.16276973 0.16276209 0.14775489 0.12340517 0.1048866 0.10122013][0.10755023 0.12708358 0.14395875 0.14045356 0.13164234 0.13433808 0.14133991 0.14301756 0.13787812 0.14001834 0.14307588 0.13142446 0.11029088 0.094304726 0.093025327][0.078573354 0.097263344 0.11274549 0.10905775 0.10081834 0.10349853 0.10958645 0.10971889 0.10388788 0.10633679 0.11267593 0.10723294 0.09333048 0.082945116 0.08429639][0.062708743 0.0810556 0.096136034 0.094658546 0.089400336 0.092843555 0.09781456 0.095856972 0.0876756 0.087539293 0.093768686 0.092517227 0.084992617 0.079478495 0.081434235][0.068810813 0.086844862 0.10172474 0.10361204 0.10290393 0.10843926 0.11326592 0.11002597 0.099396683 0.095507741 0.09848693 0.096819147 0.09076523 0.085522085 0.084469005][0.094490543 0.10970166 0.12164741 0.12532648 0.12911949 0.13784581 0.14378284 0.14047597 0.12819499 0.12097724 0.11909394 0.11307944 0.10399375 0.095772319 0.0913389][0.12536426 0.13578013 0.14184597 0.14444004 0.15110667 0.16270651 0.16985475 0.16709864 0.15479812 0.14655741 0.14123692 0.13033183 0.11725863 0.10613083 0.10046776][0.13784888 0.14373194 0.1432977 0.14307432 0.15041354 0.162677 0.16996354 0.16912559 0.16046092 0.15583937 0.1513932 0.13875477 0.12423169 0.11272942 0.1083942][0.1286941 0.13285542 0.12804535 0.12505007 0.1308222 0.14019598 0.14470355 0.14474186 0.14093132 0.14234315 0.14218432 0.13204338 0.12070619 0.11280128 0.11285095][0.10483987 0.1085685 0.10075602 0.094460346 0.096154168 0.10037392 0.10139873 0.10264298 0.1046385 0.11284468 0.1189532 0.1150799 0.11035483 0.10863137 0.11446964][0.093330726 0.097089984 0.086616017 0.075479545 0.069954284 0.066289388 0.061859474 0.061550286 0.066345721 0.078616656 0.0898927 0.092998885 0.095756978 0.10163721 0.11428276][0.10083736 0.10585988 0.095176749 0.08153078 0.0701039 0.058651477 0.0469565 0.040598258 0.042252224 0.053211693 0.065531582 0.072362415 0.0795292 0.090773344 0.10865372][0.10415579 0.10862584 0.099540345 0.088034041 0.077229068 0.064506963 0.050258372 0.039582644 0.036919877 0.044591907 0.054948337 0.061325476 0.068213522 0.080299631 0.099399611][0.089613073 0.092195369 0.085401557 0.079306811 0.075092077 0.068465583 0.058203004 0.047522429 0.042163569 0.046856288 0.054228991 0.057705939 0.061681014 0.072426677 0.090884835]]...]
INFO - root - 2017-12-11 06:06:04.044282: step 23910, loss = 0.71, batch loss = 0.65 (14.2 examples/sec; 0.564 sec/batch; 48h:22m:26s remains)
INFO - root - 2017-12-11 06:06:09.477310: step 23920, loss = 0.68, batch loss = 0.62 (15.0 examples/sec; 0.532 sec/batch; 45h:38m:02s remains)
INFO - root - 2017-12-11 06:06:15.004729: step 23930, loss = 0.69, batch loss = 0.63 (14.7 examples/sec; 0.545 sec/batch; 46h:42m:19s remains)
INFO - root - 2017-12-11 06:06:20.552678: step 23940, loss = 0.69, batch loss = 0.63 (14.3 examples/sec; 0.559 sec/batch; 47h:52m:32s remains)
INFO - root - 2017-12-11 06:06:26.010599: step 23950, loss = 0.69, batch loss = 0.63 (15.3 examples/sec; 0.525 sec/batch; 44h:57m:32s remains)
INFO - root - 2017-12-11 06:06:31.230262: step 23960, loss = 0.69, batch loss = 0.63 (14.7 examples/sec; 0.544 sec/batch; 46h:38m:14s remains)
INFO - root - 2017-12-11 06:06:36.690769: step 23970, loss = 0.69, batch loss = 0.63 (14.9 examples/sec; 0.537 sec/batch; 46h:02m:25s remains)
INFO - root - 2017-12-11 06:06:42.210813: step 23980, loss = 0.69, batch loss = 0.63 (14.7 examples/sec; 0.545 sec/batch; 46h:42m:12s remains)
INFO - root - 2017-12-11 06:06:47.844322: step 23990, loss = 0.71, batch loss = 0.65 (14.3 examples/sec; 0.559 sec/batch; 47h:54m:48s remains)
INFO - root - 2017-12-11 06:06:53.286118: step 24000, loss = 0.70, batch loss = 0.64 (14.4 examples/sec; 0.557 sec/batch; 47h:46m:08s remains)
2017-12-11 06:06:53.908730: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.3500987 0.36084157 0.363171 0.35285938 0.32569349 0.29047003 0.26214722 0.25072181 0.24976473 0.24849367 0.23802759 0.21959338 0.20040846 0.19105707 0.19221771][0.37369728 0.39142373 0.39520741 0.38245454 0.35186195 0.31682423 0.29312435 0.28620526 0.28396437 0.27276766 0.24658173 0.21337888 0.18764752 0.18257874 0.19599375][0.36592409 0.39018458 0.39559522 0.38380519 0.35750836 0.33212066 0.32208955 0.32578433 0.32446742 0.30120882 0.25431994 0.2001067 0.16156708 0.155312 0.17693913][0.36721408 0.39287543 0.3968384 0.38784519 0.37254989 0.36429152 0.37184569 0.3860558 0.38319474 0.34480476 0.27455014 0.19635838 0.14104246 0.12908278 0.15527031][0.39553186 0.41738087 0.41514859 0.40695751 0.40396109 0.41454232 0.43814403 0.45794436 0.44839773 0.39333084 0.30221021 0.2039865 0.13422504 0.11536006 0.142738][0.43414056 0.45119923 0.43989947 0.42870864 0.43387264 0.4595665 0.49472207 0.51548344 0.49740824 0.42992377 0.32739583 0.22014998 0.14403196 0.12061287 0.14591987][0.45645884 0.46473682 0.44226721 0.42583784 0.43701962 0.47606727 0.52164382 0.54365873 0.52004993 0.44692829 0.34276107 0.23746525 0.16289569 0.13733017 0.15751222][0.45892563 0.45478481 0.42119244 0.40038618 0.41771132 0.46971205 0.52603191 0.551425 0.52612627 0.45236221 0.35254452 0.25608993 0.18901481 0.16452882 0.179541][0.46117857 0.44420096 0.40210146 0.37808976 0.39820832 0.45693648 0.51906455 0.547447 0.5240497 0.4546 0.36361727 0.27927771 0.22198628 0.199247 0.20755589][0.46005842 0.4352389 0.39045534 0.36572665 0.385448 0.44402766 0.50632238 0.536951 0.51873356 0.45699316 0.37505293 0.2998946 0.2489256 0.22579923 0.22634344][0.43674952 0.41027823 0.36899877 0.34709936 0.36478215 0.4174203 0.47405964 0.50458622 0.49324489 0.44232485 0.37058875 0.30304292 0.25562608 0.23110273 0.22542253][0.3816047 0.35800511 0.32535881 0.31015211 0.32680872 0.37063187 0.41657525 0.44218159 0.43503913 0.39441505 0.33333173 0.27331826 0.2288999 0.20376243 0.19505669][0.2956605 0.28020924 0.26127589 0.2560524 0.27266341 0.30605128 0.33720851 0.35220107 0.34371468 0.31027547 0.26007408 0.20942114 0.17062216 0.14823206 0.1404849][0.19173564 0.18867818 0.18553177 0.18938531 0.2030843 0.22198509 0.23418897 0.2345154 0.2213084 0.19343255 0.15479353 0.11567066 0.085050248 0.067511186 0.061922684][0.083490819 0.092110224 0.10190048 0.11126383 0.11954409 0.12334935 0.11771937 0.10466308 0.087031081 0.064146094 0.03669174 0.010055128 -0.010373263 -0.021130685 -0.023457851]]...]
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-SGD-lr0.001-clip50-initfromfixqian30k-fixqian
INFO - root - 2017-12-11 06:06:59.339737: step 24010, loss = 0.71, batch loss = 0.65 (15.2 examples/sec; 0.528 sec/batch; 45h:13m:48s remains)
INFO - root - 2017-12-11 06:07:04.833996: step 24020, loss = 0.68, batch loss = 0.63 (14.7 examples/sec; 0.544 sec/batch; 46h:37m:00s remains)
INFO - root - 2017-12-11 06:07:10.368152: step 24030, loss = 0.69, batch loss = 0.63 (14.7 examples/sec; 0.543 sec/batch; 46h:29m:45s remains)
INFO - root - 2017-12-11 06:07:15.902289: step 24040, loss = 0.69, batch loss = 0.63 (14.3 examples/sec; 0.561 sec/batch; 48h:04m:51s remains)
INFO - root - 2017-12-11 06:07:21.043399: step 24050, loss = 0.69, batch loss = 0.63 (14.5 examples/sec; 0.553 sec/batch; 47h:20m:37s remains)
INFO - root - 2017-12-11 06:07:26.598645: step 24060, loss = 0.70, batch loss = 0.64 (14.4 examples/sec; 0.556 sec/batch; 47h:38m:42s remains)
INFO - root - 2017-12-11 06:07:32.113520: step 24070, loss = 0.69, batch loss = 0.63 (14.3 examples/sec; 0.560 sec/batch; 47h:57m:14s remains)
INFO - root - 2017-12-11 06:07:37.683860: step 24080, loss = 0.69, batch loss = 0.64 (14.2 examples/sec; 0.565 sec/batch; 48h:23m:19s remains)
INFO - root - 2017-12-11 06:07:43.222483: step 24090, loss = 0.70, batch loss = 0.64 (14.9 examples/sec; 0.537 sec/batch; 46h:01m:37s remains)
INFO - root - 2017-12-11 06:07:48.768290: step 24100, loss = 0.71, batch loss = 0.65 (14.6 examples/sec; 0.547 sec/batch; 46h:49m:56s remains)
2017-12-11 06:07:49.343198: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.056684725 -0.058467392 -0.060029645 -0.058838345 -0.046579503 -0.023587765 0.0019679919 0.022244381 0.029992029 0.02131775 -0.0019207021 -0.028500613 -0.044223268 -0.04028254 -0.018148305][-0.07553383 -0.08209978 -0.081909381 -0.068805404 -0.035499651 0.013617908 0.065254293 0.10619613 0.12450829 0.11235317 0.07309562 0.023600934 -0.016707689 -0.035752658 -0.032984342][-0.077447116 -0.092326194 -0.09363085 -0.070681669 -0.016818691 0.060250804 0.14217556 0.20812659 0.23944195 0.22413374 0.16742353 0.091982327 0.023031047 -0.022723649 -0.041652117][-0.053933293 -0.07730978 -0.082063444 -0.052893694 0.016272912 0.11538561 0.22212663 0.30832741 0.34953448 0.33099258 0.26010963 0.16322812 0.069704846 0.00043293 -0.038382653][-0.0034810489 -0.033286486 -0.042692218 -0.013403424 0.059194833 0.16459522 0.27926496 0.37204874 0.41651252 0.39803752 0.32536352 0.22377378 0.12189969 0.041527119 -0.010003382][0.054595027 0.02090477 0.0064112246 0.029445712 0.092724547 0.18700668 0.29124931 0.37728077 0.42182538 0.41177735 0.35356605 0.26685914 0.17504063 0.09757264 0.040944695][0.09027373 0.05799789 0.040787905 0.054262105 0.099987179 0.17138004 0.25287798 0.32377315 0.36759365 0.37321171 0.34223697 0.28583768 0.21919778 0.15653174 0.1025784][0.088355988 0.061727297 0.044865415 0.048516821 0.074347988 0.11943131 0.1749149 0.22855262 0.27079722 0.29230115 0.29001516 0.26715115 0.23080507 0.18905 0.14379095][0.054975856 0.036061037 0.022552049 0.019889111 0.029878134 0.052501183 0.084527515 0.12081411 0.15757678 0.18743587 0.20486595 0.20774214 0.19720556 0.17586353 0.14307356][0.012774045 0.00057348446 -0.0093989149 -0.015443004 -0.015960416 -0.0094671231 0.0042748568 0.024830747 0.052067623 0.080753565 0.10511512 0.12082931 0.12583275 0.11926924 0.099008888][-0.024847703 -0.032582171 -0.039898813 -0.046965063 -0.052427474 -0.054579869 -0.051756419 -0.042078141 -0.024114277 -0.0014906584 0.020605519 0.037721954 0.047076236 0.046804246 0.034808237][-0.053778943 -0.058380544 -0.063278213 -0.069179706 -0.075012773 -0.079416953 -0.080539607 -0.076011084 -0.064476088 -0.048655871 -0.032663353 -0.020084661 -0.013174035 -0.013281412 -0.021557987][-0.069847368 -0.072907582 -0.076161079 -0.080434896 -0.084741689 -0.08797735 -0.088647954 -0.085167132 -0.077342078 -0.067456521 -0.058380075 -0.052301455 -0.050253347 -0.052504413 -0.059322793][-0.0747139 -0.0775183 -0.079746321 -0.082123354 -0.083724879 -0.0838231 -0.08177451 -0.077155337 -0.070904344 -0.065087922 -0.061660435 -0.061511926 -0.064393654 -0.069625691 -0.076680973][-0.069491245 -0.07279209 -0.07504572 -0.076177649 -0.07495752 -0.07111457 -0.0653333 -0.05825993 -0.051400416 -0.0469395 -0.046660531 -0.050640754 -0.057540741 -0.066114493 -0.075273626]]...]
INFO - root - 2017-12-11 06:07:54.837237: step 24110, loss = 0.70, batch loss = 0.65 (15.1 examples/sec; 0.529 sec/batch; 45h:17m:25s remains)
INFO - root - 2017-12-11 06:08:00.226522: step 24120, loss = 0.69, batch loss = 0.63 (14.5 examples/sec; 0.553 sec/batch; 47h:23m:47s remains)
INFO - root - 2017-12-11 06:08:05.638357: step 24130, loss = 0.67, batch loss = 0.61 (14.9 examples/sec; 0.537 sec/batch; 45h:59m:39s remains)
INFO - root - 2017-12-11 06:08:11.214130: step 24140, loss = 0.72, batch loss = 0.66 (14.0 examples/sec; 0.573 sec/batch; 49h:03m:16s remains)
INFO - root - 2017-12-11 06:08:16.527153: step 24150, loss = 0.70, batch loss = 0.64 (13.7 examples/sec; 0.583 sec/batch; 49h:56m:36s remains)
INFO - root - 2017-12-11 06:08:21.990898: step 24160, loss = 0.70, batch loss = 0.64 (14.9 examples/sec; 0.536 sec/batch; 45h:54m:43s remains)
INFO - root - 2017-12-11 06:08:27.515490: step 24170, loss = 0.71, batch loss = 0.65 (14.8 examples/sec; 0.540 sec/batch; 46h:16m:48s remains)
INFO - root - 2017-12-11 06:08:32.972092: step 24180, loss = 0.70, batch loss = 0.64 (14.1 examples/sec; 0.566 sec/batch; 48h:26m:13s remains)
INFO - root - 2017-12-11 06:08:38.524433: step 24190, loss = 0.69, batch loss = 0.63 (14.9 examples/sec; 0.536 sec/batch; 45h:55m:20s remains)
INFO - root - 2017-12-11 06:08:44.096657: step 24200, loss = 0.71, batch loss = 0.65 (14.5 examples/sec; 0.551 sec/batch; 47h:12m:18s remains)
2017-12-11 06:08:44.688964: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.18336686 0.20595157 0.22107418 0.23264973 0.22917642 0.20798805 0.17599353 0.13782915 0.09849599 0.069312334 0.063191868 0.07728444 0.13363001 0.21579346 0.28323981][0.32805398 0.37210566 0.40786061 0.43812612 0.44174436 0.41448373 0.36765757 0.30907735 0.24748695 0.1997959 0.18653953 0.20468609 0.2794109 0.38798523 0.47442567][0.4342308 0.50419569 0.56924754 0.62940431 0.65224588 0.629689 0.57519633 0.49859035 0.41158643 0.33605611 0.30251563 0.31372458 0.39650095 0.52015311 0.61684483][0.48183033 0.58025652 0.67950869 0.77577704 0.82761663 0.82310647 0.7743718 0.68853474 0.57615846 0.46325114 0.39177597 0.37714705 0.44819936 0.56975323 0.66797793][0.49372798 0.61746246 0.74699455 0.87258965 0.94953054 0.96934026 0.943287 0.86941046 0.74648029 0.59989923 0.48323452 0.42610383 0.46265876 0.55994546 0.64680862][0.48968583 0.62656271 0.77111065 0.90710366 0.99698049 1.0477835 1.0697125 1.0410836 0.93451518 0.76974285 0.61082524 0.49769232 0.47443417 0.51894861 0.57402176][0.49939513 0.63085026 0.76729387 0.8892417 0.97853082 1.0658888 1.1549137 1.194376 1.1238343 0.9589892 0.77130568 0.60177183 0.50415826 0.47547683 0.48275223][0.53638572 0.64789361 0.75433463 0.84027642 0.91442645 1.0293254 1.1750628 1.2725855 1.2406861 1.0968153 0.90888882 0.70484966 0.54374862 0.44642651 0.4073329][0.58705157 0.67481279 0.74046737 0.77701712 0.81875616 0.93121636 1.0917501 1.2107359 1.2088408 1.1086775 0.95646954 0.75457245 0.56216419 0.42423141 0.35997346][0.60518926 0.67356026 0.70165479 0.69056529 0.69010943 0.7677328 0.89833122 1.0041389 1.0256971 0.98486519 0.89248955 0.72267419 0.53340024 0.38896766 0.32466939][0.54171038 0.5937804 0.59386748 0.54792666 0.5093801 0.53980738 0.62160587 0.70021564 0.73864442 0.749652 0.71114063 0.58075845 0.41785049 0.29607564 0.25353408][0.39753908 0.4348841 0.41980931 0.35877582 0.29818276 0.28896046 0.32451379 0.37279692 0.41186979 0.44351923 0.43061709 0.33440888 0.21451655 0.13813516 0.13153656][0.19874734 0.2204556 0.20151612 0.14549963 0.084024042 0.053556081 0.056286197 0.076949067 0.10151535 0.12543793 0.1164545 0.052059907 -0.015702082 -0.039851118 -0.012322079][0.0027784845 0.0086288629 -0.0080673 -0.047305368 -0.09299586 -0.12406084 -0.13507633 -0.13173658 -0.12357298 -0.11532949 -0.12507537 -0.16009146 -0.18308431 -0.1697216 -0.12506679][-0.13477181 -0.14275925 -0.15665863 -0.17845765 -0.20297258 -0.22144082 -0.2298903 -0.23079313 -0.23054904 -0.23174509 -0.24002397 -0.25468567 -0.25340861 -0.22712812 -0.18507829]]...]
INFO - root - 2017-12-11 06:08:50.197584: step 24210, loss = 0.70, batch loss = 0.64 (14.4 examples/sec; 0.556 sec/batch; 47h:37m:40s remains)
INFO - root - 2017-12-11 06:08:55.779659: step 24220, loss = 0.71, batch loss = 0.65 (14.3 examples/sec; 0.560 sec/batch; 47h:55m:47s remains)
INFO - root - 2017-12-11 06:09:01.320156: step 24230, loss = 0.69, batch loss = 0.63 (14.5 examples/sec; 0.552 sec/batch; 47h:14m:13s remains)
INFO - root - 2017-12-11 06:09:06.846277: step 24240, loss = 0.69, batch loss = 0.63 (14.5 examples/sec; 0.551 sec/batch; 47h:11m:23s remains)
INFO - root - 2017-12-11 06:09:12.021987: step 24250, loss = 0.71, batch loss = 0.65 (14.5 examples/sec; 0.552 sec/batch; 47h:16m:24s remains)
INFO - root - 2017-12-11 06:09:17.533623: step 24260, loss = 0.72, batch loss = 0.66 (15.0 examples/sec; 0.534 sec/batch; 45h:41m:33s remains)
INFO - root - 2017-12-11 06:09:22.949488: step 24270, loss = 0.70, batch loss = 0.64 (14.3 examples/sec; 0.559 sec/batch; 47h:51m:13s remains)
INFO - root - 2017-12-11 06:09:28.438988: step 24280, loss = 0.69, batch loss = 0.64 (14.6 examples/sec; 0.548 sec/batch; 46h:55m:25s remains)
INFO - root - 2017-12-11 06:09:34.013209: step 24290, loss = 0.70, batch loss = 0.64 (14.0 examples/sec; 0.570 sec/batch; 48h:46m:06s remains)
INFO - root - 2017-12-11 06:09:39.485421: step 24300, loss = 0.71, batch loss = 0.65 (14.5 examples/sec; 0.551 sec/batch; 47h:09m:33s remains)
2017-12-11 06:09:40.084271: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.04440622 0.12339801 0.21908879 0.3085224 0.36355573 0.38922256 0.39334309 0.37555009 0.34946382 0.329756 0.32362911 0.3203724 0.31270298 0.31116545 0.3246164][0.048243038 0.13102928 0.23053907 0.32340145 0.38031369 0.40381414 0.40327948 0.38175169 0.35461384 0.33469886 0.3269777 0.32330129 0.31755054 0.32067648 0.33805123][0.045998361 0.12552169 0.2208187 0.31066883 0.36448166 0.38226712 0.37535805 0.35201478 0.32966337 0.31633496 0.31207928 0.31098369 0.30885512 0.31566671 0.33336917][0.0483096 0.12655063 0.22010392 0.30840549 0.35833603 0.36841759 0.3533937 0.3278085 0.31068012 0.3052029 0.30719051 0.31275338 0.31812757 0.32999265 0.3471694][0.057647411 0.14161538 0.24231115 0.33609405 0.38561866 0.38890058 0.36655709 0.33883974 0.325231 0.32493991 0.33272579 0.34668562 0.36199331 0.38068935 0.39798421][0.074450091 0.1710455 0.28675643 0.39271006 0.44736177 0.4493376 0.4253813 0.39793453 0.3857263 0.38557327 0.39474383 0.41329467 0.43604422 0.46054879 0.47772628][0.097428873 0.21054438 0.34505644 0.46757668 0.5343262 0.54409903 0.52620232 0.50119936 0.48616979 0.477666 0.47821677 0.4914313 0.51400691 0.53971195 0.5546211][0.11829308 0.24582116 0.39688933 0.53581774 0.61848378 0.641479 0.63230246 0.60767716 0.582819 0.55720484 0.53989768 0.53880179 0.55247432 0.57266563 0.58272111][0.12535174 0.25751296 0.414189 0.56055665 0.65484768 0.69048041 0.68959075 0.66360682 0.62636185 0.58208728 0.5452621 0.52641618 0.52517456 0.53337389 0.53477407][0.11606082 0.23860373 0.383636 0.52109563 0.61500597 0.6574809 0.66334987 0.637711 0.59276944 0.53651655 0.48558781 0.45112869 0.43318316 0.42526263 0.41437575][0.094245613 0.1943329 0.31149361 0.42357093 0.50233442 0.54123944 0.54882836 0.52520388 0.48013142 0.42218778 0.36672717 0.32393244 0.29423454 0.2730881 0.25211415][0.063190676 0.13595933 0.21841148 0.29680553 0.35090011 0.37729257 0.38088369 0.3591761 0.31927717 0.26798958 0.21733002 0.17525926 0.14209342 0.11547129 0.091459483][0.029834481 0.078787327 0.1297484 0.17505111 0.20184259 0.21146803 0.20782608 0.18769282 0.1558437 0.11649791 0.07766144 0.043895338 0.014970559 -0.0091236271 -0.028878763][0.0059371265 0.039296675 0.068065375 0.0880773 0.092269994 0.086665243 0.075935066 0.057711788 0.034961797 0.0087980824 -0.016390447 -0.038859818 -0.059192434 -0.076196589 -0.088606849][-0.0032805672 0.023185747 0.040532727 0.046658307 0.037985891 0.022788513 0.0069090771 -0.010049798 -0.026126051 -0.042945735 -0.058557142 -0.072334416 -0.084793113 -0.094669044 -0.10035817]]...]
INFO - root - 2017-12-11 06:09:45.636606: step 24310, loss = 0.69, batch loss = 0.63 (14.1 examples/sec; 0.568 sec/batch; 48h:36m:58s remains)
INFO - root - 2017-12-11 06:09:51.176890: step 24320, loss = 0.70, batch loss = 0.64 (14.7 examples/sec; 0.544 sec/batch; 46h:32m:09s remains)
INFO - root - 2017-12-11 06:09:56.619798: step 24330, loss = 0.70, batch loss = 0.64 (14.2 examples/sec; 0.563 sec/batch; 48h:10m:23s remains)
INFO - root - 2017-12-11 06:10:01.878407: step 24340, loss = 0.69, batch loss = 0.64 (15.8 examples/sec; 0.505 sec/batch; 43h:15m:48s remains)
INFO - root - 2017-12-11 06:10:07.471917: step 24350, loss = 0.71, batch loss = 0.65 (14.6 examples/sec; 0.547 sec/batch; 46h:49m:03s remains)
INFO - root - 2017-12-11 06:10:12.988749: step 24360, loss = 0.69, batch loss = 0.63 (14.6 examples/sec; 0.546 sec/batch; 46h:44m:52s remains)
INFO - root - 2017-12-11 06:10:18.446144: step 24370, loss = 0.70, batch loss = 0.64 (14.6 examples/sec; 0.546 sec/batch; 46h:45m:26s remains)
INFO - root - 2017-12-11 06:10:23.884010: step 24380, loss = 0.69, batch loss = 0.64 (14.3 examples/sec; 0.560 sec/batch; 47h:55m:02s remains)
INFO - root - 2017-12-11 06:10:29.437435: step 24390, loss = 0.69, batch loss = 0.64 (14.6 examples/sec; 0.549 sec/batch; 46h:57m:05s remains)
INFO - root - 2017-12-11 06:10:35.007938: step 24400, loss = 0.70, batch loss = 0.64 (14.9 examples/sec; 0.536 sec/batch; 45h:50m:19s remains)
2017-12-11 06:10:35.594838: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.011706837 0.00077726057 -0.01184449 -0.021373093 -0.025794977 -0.021983132 -0.010465134 0.00084283587 0.0054468252 0.0018046171 -0.0023215686 -0.0037519184 -0.0030040513 -0.00053175975 0.0023001863][0.077172108 0.0688187 0.053452574 0.038314 0.028084969 0.026350982 0.032768022 0.041049469 0.045163892 0.041958429 0.0392026 0.040975828 0.046494968 0.053829968 0.061161458][0.15811299 0.15751842 0.14439708 0.12844609 0.11642291 0.11050649 0.10919283 0.10930829 0.10796709 0.10151415 0.097190462 0.10010406 0.10987911 0.12249767 0.13479932][0.231093 0.24531424 0.24459475 0.23903656 0.23525794 0.23223931 0.22548337 0.21458004 0.20125872 0.18398866 0.16976486 0.1651354 0.17180541 0.18464741 0.19900775][0.28047767 0.31555885 0.3379595 0.35535592 0.37159282 0.3812001 0.37533423 0.35431275 0.32417586 0.28777525 0.25311503 0.22900401 0.22094896 0.22463951 0.23511286][0.30466822 0.363213 0.41495761 0.46328402 0.50725889 0.53617543 0.536834 0.50862479 0.46031877 0.39955732 0.33678105 0.28416681 0.2519601 0.23835622 0.23948966][0.31910175 0.39477816 0.46744496 0.53757674 0.60119218 0.64430177 0.65092069 0.61892629 0.55764067 0.47782046 0.39202607 0.31590176 0.26426038 0.23782423 0.2328952][0.33976895 0.41942731 0.49298707 0.561474 0.62222439 0.66286772 0.66812503 0.63459927 0.56976795 0.48449472 0.3924216 0.31078762 0.25704187 0.23282753 0.23223804][0.36258352 0.43338144 0.48866409 0.53271371 0.56796449 0.58857423 0.584074 0.55118811 0.49361137 0.41899067 0.33988559 0.27273482 0.23482642 0.22656949 0.23869768][0.37068647 0.42362055 0.4500095 0.45853031 0.45815477 0.45095208 0.43323502 0.40322718 0.36062694 0.30813509 0.25549752 0.21627848 0.20499429 0.21911299 0.246159][0.3457461 0.37781268 0.37648812 0.35363638 0.322511 0.2913678 0.26281032 0.23730241 0.21187019 0.18455476 0.16190206 0.15287565 0.16773342 0.20110865 0.23864321][0.28026229 0.29340607 0.27374724 0.2337881 0.18698066 0.14438771 0.1125416 0.093115166 0.081857026 0.074542493 0.075730957 0.089206874 0.12028053 0.16250889 0.20204003][0.17644641 0.17594153 0.15151837 0.11306043 0.07017789 0.032443322 0.0071348539 -0.0048242607 -0.007971501 -0.0056735347 0.0067134118 0.02897024 0.061691873 0.099158406 0.13046575][0.052295376 0.043810382 0.02496567 0.0011790338 -0.023466438 -0.044059381 -0.055549528 -0.059085689 -0.058966473 -0.056104336 -0.044823185 -0.026827512 -0.0043779733 0.01861931 0.035480607][-0.056295216 -0.067970328 -0.077824347 -0.084841788 -0.089441717 -0.091311604 -0.088893421 -0.085599579 -0.085193142 -0.086275652 -0.082172044 -0.074346855 -0.065670028 -0.057890456 -0.054097675]]...]
INFO - root - 2017-12-11 06:10:41.147866: step 24410, loss = 0.71, batch loss = 0.65 (14.8 examples/sec; 0.541 sec/batch; 46h:17m:39s remains)
INFO - root - 2017-12-11 06:10:46.628370: step 24420, loss = 0.71, batch loss = 0.65 (14.8 examples/sec; 0.539 sec/batch; 46h:09m:39s remains)
INFO - root - 2017-12-11 06:10:52.069466: step 24430, loss = 0.72, batch loss = 0.66 (14.4 examples/sec; 0.554 sec/batch; 47h:26m:45s remains)
INFO - root - 2017-12-11 06:10:57.158920: step 24440, loss = 0.69, batch loss = 0.63 (14.0 examples/sec; 0.572 sec/batch; 48h:58m:37s remains)
INFO - root - 2017-12-11 06:11:02.687647: step 24450, loss = 0.69, batch loss = 0.63 (14.5 examples/sec; 0.552 sec/batch; 47h:12m:57s remains)
INFO - root - 2017-12-11 06:11:08.171402: step 24460, loss = 0.70, batch loss = 0.64 (14.3 examples/sec; 0.558 sec/batch; 47h:45m:11s remains)
INFO - root - 2017-12-11 06:11:13.712147: step 24470, loss = 0.69, batch loss = 0.63 (14.3 examples/sec; 0.561 sec/batch; 48h:00m:30s remains)
INFO - root - 2017-12-11 06:11:19.230953: step 24480, loss = 0.69, batch loss = 0.63 (14.4 examples/sec; 0.556 sec/batch; 47h:34m:12s remains)
INFO - root - 2017-12-11 06:11:24.719865: step 24490, loss = 0.70, batch loss = 0.64 (14.4 examples/sec; 0.555 sec/batch; 47h:30m:16s remains)
INFO - root - 2017-12-11 06:11:30.277113: step 24500, loss = 0.71, batch loss = 0.65 (14.3 examples/sec; 0.561 sec/batch; 47h:58m:12s remains)
2017-12-11 06:11:30.842880: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.15245926 0.13194469 0.095911808 0.062008992 0.042935647 0.042177446 0.050855279 0.058020953 0.060997248 0.057315469 0.04944003 0.04020115 0.031988 0.025788128 0.020531563][0.23063293 0.19741538 0.14458674 0.096188158 0.066925555 0.0594648 0.064074107 0.069463611 0.074274205 0.075457245 0.074084908 0.070223622 0.065170586 0.059328388 0.052524943][0.28333709 0.24272218 0.18181323 0.12799034 0.094746545 0.083526067 0.0849531 0.088757679 0.094147921 0.097482048 0.098864965 0.096927851 0.0933999 0.088539124 0.082052134][0.30102026 0.26374492 0.20808163 0.16049132 0.13108349 0.12029238 0.12008431 0.12258489 0.12711395 0.1296117 0.1298968 0.1264029 0.12238426 0.11785693 0.11191446][0.28251892 0.25922492 0.22156891 0.19150589 0.1747192 0.17029102 0.17163196 0.17387986 0.17593959 0.17404316 0.16940592 0.16145776 0.15504083 0.1497799 0.14363004][0.24876909 0.24688451 0.23496141 0.2284046 0.22787029 0.23124136 0.2331056 0.23207475 0.22792035 0.21835017 0.20695524 0.19450864 0.18629737 0.18093018 0.17485479][0.22360685 0.24406557 0.25702354 0.27145851 0.28339523 0.28991359 0.28696761 0.27735102 0.26340207 0.245395 0.22867645 0.21482964 0.20776969 0.20450032 0.19991741][0.21534891 0.25338003 0.28397635 0.31080294 0.32693106 0.32939321 0.31590083 0.29372543 0.26909795 0.24479024 0.22621131 0.21420339 0.21011484 0.20931004 0.20623653][0.21777478 0.26534078 0.30369979 0.3330355 0.34549159 0.338621 0.31239393 0.27803037 0.24536186 0.2184037 0.20075583 0.19171721 0.19030613 0.19067602 0.18778019][0.21166377 0.25984645 0.2978363 0.32340869 0.32884741 0.312157 0.27555704 0.23264648 0.19522715 0.16737117 0.15053502 0.14317667 0.14306468 0.14407867 0.14167927][0.18279947 0.22507182 0.25816444 0.27765137 0.27643996 0.25304705 0.21151498 0.16605751 0.12828663 0.10161915 0.085683525 0.079075918 0.079898991 0.08259476 0.082707152][0.12581654 0.15866041 0.18604292 0.20140672 0.19838618 0.17539716 0.13687214 0.096117333 0.063284218 0.04076333 0.026962671 0.020944547 0.02246552 0.027392102 0.031132566][0.054833177 0.077804342 0.10044869 0.11525572 0.11642502 0.10146079 0.0734879 0.043497615 0.019752275 0.0039746212 -0.0060559018 -0.010800727 -0.0088881766 -0.0028293545 0.0029579832][-0.0040733861 0.011158303 0.030704051 0.047474556 0.055758327 0.051989123 0.037200592 0.019335035 0.0049753687 -0.0039351471 -0.0093508353 -0.011620006 -0.00885688 -0.0029636328 0.0018382092][-0.035269879 -0.024802931 -0.0074543874 0.011126081 0.024868267 0.029955931 0.025844719 0.017617658 0.010603152 0.0077166124 0.0078097689 0.009870219 0.014922542 0.02044275 0.022254677]]...]
INFO - root - 2017-12-11 06:11:36.494503: step 24510, loss = 0.70, batch loss = 0.64 (14.1 examples/sec; 0.567 sec/batch; 48h:31m:07s remains)
INFO - root - 2017-12-11 06:11:41.982713: step 24520, loss = 0.69, batch loss = 0.63 (14.4 examples/sec; 0.555 sec/batch; 47h:28m:27s remains)
INFO - root - 2017-12-11 06:11:47.430764: step 24530, loss = 0.71, batch loss = 0.65 (15.3 examples/sec; 0.524 sec/batch; 44h:49m:39s remains)
INFO - root - 2017-12-11 06:11:52.710374: step 24540, loss = 0.70, batch loss = 0.64 (14.0 examples/sec; 0.571 sec/batch; 48h:49m:53s remains)
INFO - root - 2017-12-11 06:11:58.210915: step 24550, loss = 0.70, batch loss = 0.64 (14.9 examples/sec; 0.536 sec/batch; 45h:48m:40s remains)
INFO - root - 2017-12-11 06:12:03.735038: step 24560, loss = 0.69, batch loss = 0.63 (14.6 examples/sec; 0.546 sec/batch; 46h:42m:48s remains)
INFO - root - 2017-12-11 06:12:09.256097: step 24570, loss = 0.68, batch loss = 0.62 (14.1 examples/sec; 0.566 sec/batch; 48h:26m:28s remains)
INFO - root - 2017-12-11 06:12:14.921505: step 24580, loss = 0.71, batch loss = 0.65 (14.5 examples/sec; 0.552 sec/batch; 47h:11m:21s remains)
INFO - root - 2017-12-11 06:12:20.448550: step 24590, loss = 0.71, batch loss = 0.65 (14.4 examples/sec; 0.554 sec/batch; 47h:24m:42s remains)
INFO - root - 2017-12-11 06:12:25.963266: step 24600, loss = 0.70, batch loss = 0.64 (14.3 examples/sec; 0.561 sec/batch; 47h:57m:39s remains)
2017-12-11 06:12:26.538080: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.076833963 -0.065477557 -0.045735788 -0.025102098 -0.0054769749 0.0079932772 0.0097981906 0.00080695155 -0.01672351 -0.03776668 -0.057068069 -0.0686024 -0.068924934 -0.059169848 -0.041640591][-0.064280607 -0.039595749 -0.00065955642 0.041881915 0.084367894 0.11782235 0.13175468 0.12217243 0.09071923 0.048465725 0.0075900997 -0.020652229 -0.028215643 -0.014518525 0.015459881][-0.04779214 -0.0054041371 0.060206663 0.13608183 0.21550027 0.28327584 0.32180551 0.31754395 0.2709586 0.2005886 0.12815353 0.073390678 0.049784068 0.061719205 0.10090601][-0.02997265 0.030966256 0.12510729 0.23808339 0.359108 0.46550506 0.53251314 0.536668 0.47842684 0.38408622 0.28400561 0.20358485 0.1608258 0.16495331 0.20430763][-0.013688385 0.063197784 0.18218513 0.32755086 0.48208776 0.61790705 0.7077896 0.7223683 0.66141093 0.55838466 0.4493036 0.35996252 0.30556521 0.297596 0.3240549][-0.0025607606 0.08605206 0.22403812 0.39301765 0.56742948 0.71875757 0.82296759 0.85093641 0.80044466 0.70982718 0.61671215 0.54147512 0.48984751 0.47023171 0.47196653][0.0016769409 0.095265709 0.24459973 0.42828727 0.61328155 0.77213967 0.88601911 0.92977107 0.89808291 0.8312248 0.76674032 0.71882546 0.68292528 0.65795964 0.63334525][0.0020918886 0.094554856 0.2456229 0.43241274 0.61847359 0.77599704 0.89013028 0.94171721 0.9246347 0.87954313 0.84354877 0.82872146 0.82111847 0.80476546 0.76664847][0.0023419191 0.089160591 0.23187336 0.40773496 0.58061433 0.72180265 0.8207348 0.8661496 0.85343444 0.82231164 0.80972272 0.82957512 0.85873 0.86520046 0.83466339][0.0016809388 0.079941392 0.20543237 0.35729682 0.50290179 0.61479497 0.68781751 0.71804643 0.70289677 0.67868537 0.67986864 0.72253114 0.77860439 0.807881 0.79857671][-0.0036355287 0.06385114 0.16720168 0.28821504 0.40028048 0.47971755 0.52653748 0.541557 0.52322632 0.501258 0.5040592 0.54793304 0.60765409 0.64599925 0.65713787][-0.015376557 0.039988175 0.12079202 0.21098733 0.29051331 0.34064424 0.36482149 0.36674964 0.34571353 0.32303989 0.31861812 0.34758836 0.3928929 0.42759144 0.45223305][-0.032453876 0.010613641 0.071200319 0.13461044 0.18559165 0.21011434 0.21367595 0.20259817 0.17802861 0.15377912 0.14191067 0.15494049 0.18270768 0.20833647 0.23628367][-0.050615784 -0.01941034 0.023962663 0.066062912 0.095141612 0.10074142 0.0886269 0.067272432 0.040458959 0.016974481 0.0031293356 0.0069394056 0.021455031 0.036398523 0.057122625][-0.066330791 -0.046016406 -0.016601129 0.010211771 0.025802623 0.022304894 0.004086074 -0.020178093 -0.045008525 -0.06446217 -0.075737737 -0.075569548 -0.070112281 -0.066173948 -0.059295081]]...]
INFO - root - 2017-12-11 06:12:32.060253: step 24610, loss = 0.70, batch loss = 0.64 (14.5 examples/sec; 0.550 sec/batch; 47h:03m:31s remains)
INFO - root - 2017-12-11 06:12:37.569501: step 24620, loss = 0.68, batch loss = 0.62 (14.2 examples/sec; 0.564 sec/batch; 48h:12m:58s remains)
INFO - root - 2017-12-11 06:12:42.866997: step 24630, loss = 0.70, batch loss = 0.65 (23.0 examples/sec; 0.347 sec/batch; 29h:41m:34s remains)
INFO - root - 2017-12-11 06:12:48.231556: step 24640, loss = 0.70, batch loss = 0.64 (14.6 examples/sec; 0.549 sec/batch; 46h:55m:32s remains)
INFO - root - 2017-12-11 06:12:53.790659: step 24650, loss = 0.70, batch loss = 0.64 (14.6 examples/sec; 0.549 sec/batch; 46h:56m:35s remains)
INFO - root - 2017-12-11 06:12:59.269264: step 24660, loss = 0.70, batch loss = 0.64 (14.7 examples/sec; 0.545 sec/batch; 46h:33m:53s remains)
INFO - root - 2017-12-11 06:13:04.807648: step 24670, loss = 0.70, batch loss = 0.64 (14.4 examples/sec; 0.555 sec/batch; 47h:26m:47s remains)
INFO - root - 2017-12-11 06:13:10.326016: step 24680, loss = 0.70, batch loss = 0.64 (14.4 examples/sec; 0.556 sec/batch; 47h:33m:51s remains)
INFO - root - 2017-12-11 06:13:15.805049: step 24690, loss = 0.69, batch loss = 0.63 (14.5 examples/sec; 0.550 sec/batch; 47h:01m:05s remains)
INFO - root - 2017-12-11 06:13:21.300851: step 24700, loss = 0.68, batch loss = 0.62 (14.4 examples/sec; 0.554 sec/batch; 47h:20m:39s remains)
2017-12-11 06:13:21.846793: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.22430001 0.26362726 0.2772781 0.26958737 0.23536234 0.1976098 0.17940785 0.1892643 0.21685269 0.26434949 0.32833952 0.37290168 0.37486085 0.34317195 0.29607317][0.27745295 0.32722321 0.34642908 0.34139284 0.30674928 0.26971141 0.25474617 0.26972362 0.30461603 0.36413741 0.4428452 0.49441555 0.49003893 0.44274727 0.37765861][0.30085459 0.36267886 0.39252493 0.39665741 0.3710871 0.34462935 0.33825031 0.35591388 0.39102113 0.45117489 0.53000349 0.57520485 0.55535883 0.48767263 0.40319821][0.31297579 0.39202389 0.43952671 0.45804942 0.44721216 0.43674269 0.44102061 0.45773047 0.48558143 0.53462505 0.59772879 0.6208384 0.57336426 0.478425 0.37281576][0.34029365 0.44632712 0.52096707 0.55921555 0.56495887 0.56853724 0.57809454 0.586092 0.59645659 0.62276763 0.65849441 0.65070248 0.57077467 0.44854891 0.32591251][0.38855821 0.5298366 0.63893825 0.7017085 0.72417963 0.73624355 0.7421689 0.73258513 0.71626759 0.71128696 0.715336 0.67886215 0.57446378 0.43598974 0.30682179][0.43419588 0.60528213 0.74424636 0.82906485 0.86557043 0.88111782 0.87908876 0.8514173 0.80919081 0.77516067 0.753246 0.70002532 0.58806592 0.44948441 0.32423818][0.45573062 0.63929921 0.79085857 0.88474137 0.925669 0.93746328 0.92484003 0.8842724 0.82734305 0.7790907 0.74774581 0.69604373 0.59562808 0.47165993 0.35769176][0.43160355 0.60353565 0.74442095 0.82991606 0.86340386 0.86462742 0.84070021 0.79443753 0.73742545 0.69419712 0.67266154 0.63887954 0.564868 0.46680689 0.37132961][0.34957427 0.48723504 0.59770364 0.66207933 0.68192863 0.67168212 0.63943547 0.593633 0.54633248 0.51951981 0.51805031 0.509663 0.46808419 0.40121767 0.32990479][0.22009106 0.31028524 0.3806738 0.41883948 0.42530793 0.40890503 0.37610263 0.337409 0.30475685 0.29613465 0.31323749 0.32651624 0.312945 0.27422836 0.22660498][0.078877561 0.12249187 0.15472898 0.16820993 0.1641205 0.14778297 0.12274221 0.097239524 0.080611557 0.086249322 0.11374553 0.13849396 0.14231889 0.12362485 0.094813392][-0.030247435 -0.019747155 -0.014296934 -0.018015238 -0.028584458 -0.041954525 -0.056964602 -0.069246538 -0.073648714 -0.0614541 -0.033432633 -0.0075721783 0.0033882591 -0.0031463881 -0.018230127][-0.094032034 -0.10047388 -0.1077881 -0.11871699 -0.13063115 -0.14030412 -0.14747877 -0.15092294 -0.14923441 -0.13771904 -0.11649989 -0.096694142 -0.085934877 -0.086292863 -0.091954648][-0.12074992 -0.13145022 -0.1403091 -0.15028831 -0.15930328 -0.16525549 -0.16802053 -0.16773048 -0.16477931 -0.15730101 -0.14494953 -0.13318355 -0.12594686 -0.12444165 -0.1254106]]...]
INFO - root - 2017-12-11 06:13:27.371014: step 24710, loss = 0.69, batch loss = 0.63 (14.6 examples/sec; 0.547 sec/batch; 46h:46m:16s remains)
INFO - root - 2017-12-11 06:13:32.817940: step 24720, loss = 0.69, batch loss = 0.64 (14.7 examples/sec; 0.545 sec/batch; 46h:34m:36s remains)
INFO - root - 2017-12-11 06:13:38.028468: step 24730, loss = 0.69, batch loss = 0.63 (14.7 examples/sec; 0.545 sec/batch; 46h:36m:21s remains)
INFO - root - 2017-12-11 06:13:43.509551: step 24740, loss = 0.69, batch loss = 0.63 (14.5 examples/sec; 0.553 sec/batch; 47h:16m:32s remains)
INFO - root - 2017-12-11 06:13:48.983356: step 24750, loss = 0.68, batch loss = 0.62 (14.6 examples/sec; 0.547 sec/batch; 46h:47m:39s remains)
INFO - root - 2017-12-11 06:13:54.500963: step 24760, loss = 0.71, batch loss = 0.65 (14.5 examples/sec; 0.550 sec/batch; 47h:00m:58s remains)
INFO - root - 2017-12-11 06:14:00.020377: step 24770, loss = 0.71, batch loss = 0.66 (15.0 examples/sec; 0.533 sec/batch; 45h:32m:45s remains)
INFO - root - 2017-12-11 06:14:05.492351: step 24780, loss = 0.71, batch loss = 0.65 (14.8 examples/sec; 0.541 sec/batch; 46h:12m:21s remains)
INFO - root - 2017-12-11 06:14:10.961347: step 24790, loss = 0.69, batch loss = 0.63 (14.7 examples/sec; 0.545 sec/batch; 46h:35m:31s remains)
INFO - root - 2017-12-11 06:14:16.467046: step 24800, loss = 0.69, batch loss = 0.64 (14.5 examples/sec; 0.553 sec/batch; 47h:17m:24s remains)
2017-12-11 06:14:17.008726: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.033781774 0.024431234 0.016362725 0.013326477 0.018003143 0.030998906 0.0463557 0.058768108 0.064383484 0.063602068 0.059239574 0.053158224 0.050696019 0.053497717 0.059681177][0.075750962 0.062766075 0.052015398 0.049361844 0.057626009 0.076282613 0.097344674 0.11395559 0.12084915 0.11889766 0.11233901 0.10484493 0.10391805 0.11069931 0.12160355][0.10856637 0.092564546 0.079778343 0.078493975 0.091149919 0.11502808 0.14078215 0.16056803 0.16786036 0.16400595 0.15471287 0.14548081 0.14529112 0.15473393 0.16919716][0.12239145 0.10480805 0.091291226 0.092204 0.10910405 0.13683045 0.16611081 0.1883357 0.19575952 0.19016446 0.17837387 0.1668174 0.16468216 0.17258424 0.18638071][0.12350356 0.10513762 0.091580309 0.0948055 0.11504008 0.14562812 0.17869215 0.2045984 0.213907 0.20839931 0.1954181 0.18089209 0.17241409 0.17207022 0.17855214][0.1276177 0.1088218 0.094977416 0.099736452 0.12196509 0.15474331 0.19195272 0.22244509 0.23481528 0.23062959 0.21780199 0.20065467 0.18357438 0.17077564 0.16482382][0.1465971 0.12834235 0.11486074 0.12095492 0.14334887 0.17638575 0.21556731 0.24868421 0.263125 0.26035994 0.24946025 0.23207857 0.2085599 0.18500513 0.16702497][0.17727783 0.16103908 0.14997573 0.15852816 0.1802447 0.2113525 0.24852386 0.28015065 0.29441163 0.2931968 0.28697264 0.27416459 0.25120124 0.22459742 0.20078269][0.21555828 0.2035872 0.19610237 0.20715962 0.22736953 0.25389981 0.28383705 0.30852395 0.31937045 0.31973508 0.321078 0.31817979 0.30419838 0.2839537 0.26260862][0.25070685 0.24339223 0.23846339 0.2502217 0.26786581 0.28808138 0.30685839 0.31996948 0.32367983 0.32377517 0.33241794 0.34144154 0.34202528 0.33523631 0.32345039][0.26806355 0.26619971 0.26325986 0.27380615 0.28805307 0.3012073 0.30690902 0.3057538 0.299495 0.2968083 0.31026295 0.3301979 0.34708807 0.35690215 0.35818562][0.26736245 0.2695795 0.26680994 0.27372882 0.28365958 0.29053155 0.28523609 0.27224293 0.25761849 0.25203368 0.26731148 0.29428396 0.32432413 0.34879762 0.36261794][0.26142758 0.2630415 0.2566238 0.25799862 0.26447645 0.26876095 0.25881156 0.2405486 0.22224061 0.21499804 0.22885601 0.25689772 0.29325983 0.32666874 0.3492029][0.26199996 0.25876132 0.24645646 0.24343204 0.2498493 0.25654531 0.24828465 0.23009947 0.21119648 0.20236915 0.21125926 0.23418508 0.269253 0.30494341 0.33103967][0.26090029 0.25314707 0.23794904 0.23572817 0.24708119 0.26040375 0.25769922 0.2418261 0.22276506 0.21170445 0.2142607 0.2296605 0.25977325 0.29434514 0.32083023]]...]
INFO - root - 2017-12-11 06:14:22.482848: step 24810, loss = 0.70, batch loss = 0.64 (14.8 examples/sec; 0.540 sec/batch; 46h:10m:55s remains)
INFO - root - 2017-12-11 06:14:28.058302: step 24820, loss = 0.70, batch loss = 0.64 (14.4 examples/sec; 0.556 sec/batch; 47h:32m:01s remains)
INFO - root - 2017-12-11 06:14:33.330982: step 24830, loss = 0.70, batch loss = 0.64 (14.4 examples/sec; 0.556 sec/batch; 47h:33m:18s remains)
INFO - root - 2017-12-11 06:14:38.880416: step 24840, loss = 0.67, batch loss = 0.61 (14.5 examples/sec; 0.551 sec/batch; 47h:07m:43s remains)
INFO - root - 2017-12-11 06:14:44.343026: step 24850, loss = 0.70, batch loss = 0.64 (14.5 examples/sec; 0.551 sec/batch; 47h:04m:47s remains)
INFO - root - 2017-12-11 06:14:49.946893: step 24860, loss = 0.69, batch loss = 0.63 (13.8 examples/sec; 0.579 sec/batch; 49h:28m:13s remains)
INFO - root - 2017-12-11 06:14:55.457213: step 24870, loss = 0.70, batch loss = 0.65 (14.6 examples/sec; 0.547 sec/batch; 46h:42m:03s remains)
INFO - root - 2017-12-11 06:15:00.908931: step 24880, loss = 0.70, batch loss = 0.64 (14.3 examples/sec; 0.560 sec/batch; 47h:49m:13s remains)
INFO - root - 2017-12-11 06:15:06.427068: step 24890, loss = 0.69, batch loss = 0.64 (14.6 examples/sec; 0.548 sec/batch; 46h:49m:09s remains)
INFO - root - 2017-12-11 06:15:11.988080: step 24900, loss = 0.69, batch loss = 0.63 (14.5 examples/sec; 0.553 sec/batch; 47h:15m:49s remains)
2017-12-11 06:15:12.576394: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.082042016 0.1116346 0.15993299 0.21859239 0.2745074 0.3184126 0.34467769 0.35558623 0.35918123 0.35776407 0.34503618 0.31513876 0.27437675 0.23844235 0.21323749][0.10666209 0.14882109 0.20745032 0.27440625 0.33595613 0.38206288 0.40615171 0.41044736 0.40453142 0.39361104 0.37498856 0.343262 0.30415836 0.27640966 0.26800507][0.12465667 0.17418006 0.23818815 0.30904615 0.37218741 0.41701412 0.43626845 0.4316698 0.41438478 0.39332536 0.37086138 0.34248385 0.31251937 0.29869992 0.31041929][0.12142885 0.17609501 0.24632303 0.32383308 0.39254016 0.4401888 0.45854723 0.44866633 0.42293578 0.39382246 0.36878628 0.34558895 0.32591084 0.32394433 0.34924963][0.096059486 0.15408674 0.23185039 0.31840038 0.39631698 0.45164803 0.475659 0.46804681 0.44061109 0.40760735 0.3801024 0.35881311 0.34361428 0.34480757 0.37174782][0.067625232 0.12674984 0.20985651 0.30445603 0.39309433 0.46023956 0.49640447 0.49868748 0.4755539 0.44084182 0.40788984 0.38195822 0.3640216 0.36107779 0.38151875][0.06352029 0.12186913 0.20679866 0.30581769 0.40307871 0.48242089 0.53250808 0.54637456 0.52721107 0.48764151 0.44262722 0.40424049 0.37838563 0.3701435 0.38516122][0.0911526 0.14837275 0.23072816 0.32813266 0.42834836 0.51540691 0.57515532 0.59551519 0.57481313 0.52468956 0.46095684 0.403912 0.36759609 0.35804889 0.37590379][0.15381964 0.20841524 0.282277 0.36923712 0.46091524 0.54175013 0.59612185 0.60910463 0.57732421 0.51278538 0.43171793 0.36042121 0.31864351 0.31356025 0.34127483][0.22968824 0.27570277 0.33122218 0.39628509 0.46600839 0.52539831 0.55933928 0.55311596 0.50501627 0.42893332 0.34096822 0.26796618 0.23030855 0.23503673 0.2756786][0.28001282 0.3105967 0.34172007 0.37872538 0.41983709 0.45130244 0.45922261 0.43152687 0.36988187 0.29023743 0.20784439 0.14568482 0.12081068 0.13880055 0.19185954][0.26599523 0.27935734 0.288479 0.30041865 0.31534147 0.321795 0.30789679 0.26516268 0.19860554 0.12526762 0.058494464 0.015337976 0.0078880126 0.039366465 0.1007031][0.18468063 0.18393794 0.17950495 0.17546202 0.17263003 0.16261561 0.13713834 0.091471754 0.032297678 -0.024836585 -0.06959556 -0.090833485 -0.081763096 -0.042290255 0.018071625][0.085223638 0.0758897 0.065007925 0.052425478 0.039252337 0.020878816 -0.0065052654 -0.044779871 -0.087928452 -0.12443473 -0.14725475 -0.15018271 -0.13092393 -0.092202082 -0.042613119][0.0048882142 -0.0082442192 -0.020843843 -0.036329512 -0.053111561 -0.072192952 -0.094186939 -0.11977132 -0.14516647 -0.16354725 -0.17090294 -0.16469264 -0.14498065 -0.11511869 -0.081456535]]...]
INFO - root - 2017-12-11 06:15:18.149851: step 24910, loss = 0.71, batch loss = 0.65 (14.5 examples/sec; 0.550 sec/batch; 46h:59m:50s remains)
INFO - root - 2017-12-11 06:15:23.621107: step 24920, loss = 0.69, batch loss = 0.63 (14.7 examples/sec; 0.544 sec/batch; 46h:30m:04s remains)
INFO - root - 2017-12-11 06:15:28.851154: step 24930, loss = 0.70, batch loss = 0.64 (14.5 examples/sec; 0.552 sec/batch; 47h:08m:03s remains)
INFO - root - 2017-12-11 06:15:34.333810: step 24940, loss = 0.71, batch loss = 0.65 (14.9 examples/sec; 0.538 sec/batch; 45h:57m:53s remains)
INFO - root - 2017-12-11 06:15:39.859108: step 24950, loss = 0.69, batch loss = 0.64 (14.7 examples/sec; 0.546 sec/batch; 46h:38m:46s remains)
INFO - root - 2017-12-11 06:15:45.412855: step 24960, loss = 0.69, batch loss = 0.63 (14.3 examples/sec; 0.558 sec/batch; 47h:41m:08s remains)
INFO - root - 2017-12-11 06:15:50.905630: step 24970, loss = 0.69, batch loss = 0.63 (14.9 examples/sec; 0.538 sec/batch; 45h:55m:34s remains)
INFO - root - 2017-12-11 06:15:56.405571: step 24980, loss = 0.69, batch loss = 0.63 (14.7 examples/sec; 0.546 sec/batch; 46h:36m:02s remains)
INFO - root - 2017-12-11 06:16:01.994199: step 24990, loss = 0.70, batch loss = 0.64 (14.3 examples/sec; 0.558 sec/batch; 47h:41m:07s remains)
INFO - root - 2017-12-11 06:16:07.479876: step 25000, loss = 0.68, batch loss = 0.62 (14.1 examples/sec; 0.566 sec/batch; 48h:20m:52s remains)
2017-12-11 06:16:08.106370: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.43808377 0.37815925 0.27108353 0.15912561 0.071082056 0.037947033 0.066144511 0.14810981 0.25036705 0.33466467 0.37244722 0.33693519 0.2389968 0.12193938 0.02481035][0.54687959 0.45923644 0.32161921 0.1906617 0.099322796 0.078171104 0.12648629 0.22897691 0.34145486 0.41980806 0.43859634 0.37683108 0.25285834 0.11653532 0.011263962][0.59541214 0.4894515 0.34026381 0.21276158 0.1369812 0.1381433 0.20820501 0.32526255 0.4378249 0.49834639 0.48928136 0.39845333 0.25118062 0.10081159 -0.0072977147][0.59217048 0.4916794 0.36077186 0.26217461 0.21754368 0.24508753 0.33076626 0.44836086 0.54280335 0.57075953 0.526259 0.40703255 0.24094412 0.081569977 -0.025774278][0.55224288 0.4839296 0.39942515 0.3508212 0.34787214 0.40158847 0.49351019 0.59480733 0.6512512 0.63105661 0.54500842 0.39972895 0.22174749 0.05991219 -0.044366028][0.49736667 0.4792442 0.4585743 0.46961519 0.50834733 0.58000427 0.66500753 0.735167 0.7410894 0.666986 0.54045027 0.37529206 0.19415577 0.036617327 -0.062372562][0.44613266 0.4804005 0.52368373 0.59069127 0.66360271 0.74268609 0.80982596 0.8407979 0.79592812 0.67520881 0.51747 0.33969107 0.16147079 0.013493767 -0.077885151][0.39333761 0.46754482 0.5620364 0.67270535 0.76858467 0.84755206 0.89286858 0.88570046 0.800237 0.6495564 0.4764286 0.29571497 0.12565543 -0.008926102 -0.089760348][0.34320718 0.44435677 0.57532984 0.71502453 0.821253 0.89177775 0.91272444 0.87372637 0.76293409 0.60099536 0.428299 0.254345 0.09637247 -0.023599183 -0.093623444][0.29333761 0.40644467 0.5549081 0.70471609 0.80668855 0.86132139 0.85948151 0.8005451 0.68393558 0.53028172 0.37362495 0.21720272 0.076864891 -0.02772464 -0.088205524][0.25215134 0.36537459 0.510831 0.64924955 0.73126113 0.762225 0.73957843 0.67123181 0.56266832 0.43071193 0.30126569 0.17261061 0.056661028 -0.02978557 -0.080258653][0.21943524 0.32149598 0.44679508 0.55769306 0.61123234 0.61622977 0.57725769 0.50808376 0.41559944 0.31115758 0.21322741 0.11698441 0.029298373 -0.03703244 -0.076472558][0.17663236 0.25842845 0.35204273 0.42704543 0.45199367 0.43584797 0.38875607 0.32761446 0.25771865 0.18423264 0.11791316 0.05323102 -0.0064506228 -0.052490704 -0.079997472][0.11973028 0.17860955 0.23769203 0.27615148 0.27700648 0.24764962 0.20020655 0.15192997 0.10574976 0.06161423 0.022827851 -0.015092694 -0.049757097 -0.075998969 -0.09020777][0.049759764 0.085189886 0.1132159 0.12185722 0.10746958 0.075912431 0.037020586 0.0045080055 -0.020735811 -0.041527309 -0.05915739 -0.076396666 -0.090882957 -0.099920891 -0.10162392]]...]
INFO - root - 2017-12-11 06:16:13.552802: step 25010, loss = 0.69, batch loss = 0.63 (14.4 examples/sec; 0.556 sec/batch; 47h:29m:34s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-SGD-lr0.001-clip50-initfromfixqian30k-fixqian
INFO - root - 2017-12-11 06:16:18.633076: step 25020, loss = 0.69, batch loss = 0.63 (15.1 examples/sec; 0.530 sec/batch; 45h:14m:10s remains)
INFO - root - 2017-12-11 06:16:24.184842: step 25030, loss = 0.68, batch loss = 0.63 (14.3 examples/sec; 0.561 sec/batch; 47h:53m:31s remains)
INFO - root - 2017-12-11 06:16:29.704162: step 25040, loss = 0.70, batch loss = 0.64 (14.1 examples/sec; 0.568 sec/batch; 48h:32m:05s remains)
INFO - root - 2017-12-11 06:16:35.153308: step 25050, loss = 0.71, batch loss = 0.65 (14.6 examples/sec; 0.546 sec/batch; 46h:39m:02s remains)
INFO - root - 2017-12-11 06:16:40.597245: step 25060, loss = 0.70, batch loss = 0.65 (14.6 examples/sec; 0.546 sec/batch; 46h:39m:41s remains)
INFO - root - 2017-12-11 06:16:46.111065: step 25070, loss = 0.68, batch loss = 0.62 (14.1 examples/sec; 0.566 sec/batch; 48h:19m:12s remains)
INFO - root - 2017-12-11 06:16:51.697397: step 25080, loss = 0.69, batch loss = 0.63 (14.4 examples/sec; 0.555 sec/batch; 47h:25m:33s remains)
INFO - root - 2017-12-11 06:16:57.123186: step 25090, loss = 0.69, batch loss = 0.63 (14.6 examples/sec; 0.548 sec/batch; 46h:50m:01s remains)
INFO - root - 2017-12-11 06:17:02.696349: step 25100, loss = 0.70, batch loss = 0.64 (14.3 examples/sec; 0.559 sec/batch; 47h:41m:50s remains)
2017-12-11 06:17:03.290360: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.58611876 0.68150753 0.728533 0.713225 0.64333355 0.5384717 0.40573829 0.26512173 0.15805115 0.11471476 0.12526044 0.16454093 0.21878153 0.27039275 0.29393905][0.64408284 0.7506265 0.80794883 0.80178219 0.73593146 0.62874913 0.48601645 0.32298669 0.17719352 0.08279781 0.039995622 0.035342883 0.0603781 0.10083036 0.13262944][0.62145787 0.73801094 0.81117123 0.82531577 0.77894604 0.68548906 0.550615 0.38586807 0.2219907 0.092246406 0.0056406404 -0.03903174 -0.043868106 -0.017866883 0.019643022][0.54747337 0.67137611 0.76119083 0.80172759 0.787306 0.72415668 0.61687928 0.47367391 0.31682882 0.17480065 0.061739277 -0.012889558 -0.043480683 -0.031717941 0.0077512059][0.45581815 0.5853672 0.69234413 0.76319754 0.789741 0.77030569 0.70473796 0.59632456 0.45944583 0.31768239 0.18750021 0.086873904 0.029239168 0.021287903 0.053459842][0.3605901 0.496813 0.62218237 0.72381318 0.79244524 0.81933272 0.79713386 0.72221494 0.60384613 0.46346718 0.31894073 0.19394216 0.10824073 0.076531708 0.094628975][0.27569216 0.41803727 0.55932689 0.68368447 0.782768 0.8451823 0.85676771 0.80626047 0.70051885 0.561558 0.40747425 0.26381338 0.15483585 0.10169307 0.10396683][0.20670563 0.34580731 0.489533 0.6187886 0.72788453 0.80737042 0.83839524 0.80392915 0.709527 0.57715786 0.42329496 0.27196926 0.15082456 0.084557533 0.07507097][0.13368683 0.25466618 0.38316348 0.49879646 0.59815866 0.67504174 0.70989162 0.68420315 0.6029402 0.48578179 0.3459008 0.20385587 0.088356547 0.023935182 0.013151765][0.045377269 0.13782401 0.24031733 0.33200496 0.41000152 0.47030118 0.49604696 0.47169387 0.40301865 0.30724281 0.19402564 0.079194516 -0.011854096 -0.058739062 -0.060431849][-0.0430321 0.01641972 0.086308651 0.14750068 0.19731687 0.23381789 0.24526201 0.22140782 0.16834369 0.099781342 0.021980638 -0.054675967 -0.11153607 -0.13455276 -0.12450554][-0.10548031 -0.07779865 -0.041495644 -0.011563115 0.010157189 0.023468437 0.021857485 0.00070983416 -0.034715772 -0.075011455 -0.11713041 -0.15542337 -0.17888419 -0.18039408 -0.1619861][-0.13788621 -0.13490006 -0.12568872 -0.11980771 -0.11789472 -0.11948165 -0.12767586 -0.14354654 -0.16334452 -0.18191668 -0.1978322 -0.2083315 -0.20832512 -0.19607769 -0.17431349][-0.15052952 -0.16072842 -0.16578768 -0.17124394 -0.17694645 -0.18290763 -0.19055556 -0.1995867 -0.20801876 -0.21380007 -0.21612687 -0.21336968 -0.20359388 -0.18764067 -0.16827133][-0.14310437 -0.15525067 -0.16279018 -0.16930519 -0.17443919 -0.17875905 -0.18304716 -0.18688318 -0.18942937 -0.19012406 -0.18841697 -0.18327804 -0.17415714 -0.16213715 -0.149318]]...]
INFO - root - 2017-12-11 06:17:08.846228: step 25110, loss = 0.70, batch loss = 0.64 (14.6 examples/sec; 0.548 sec/batch; 46h:49m:56s remains)
INFO - root - 2017-12-11 06:17:13.990889: step 25120, loss = 0.70, batch loss = 0.64 (15.1 examples/sec; 0.529 sec/batch; 45h:11m:03s remains)
INFO - root - 2017-12-11 06:17:19.529041: step 25130, loss = 0.71, batch loss = 0.66 (14.4 examples/sec; 0.554 sec/batch; 47h:20m:21s remains)
INFO - root - 2017-12-11 06:17:25.011390: step 25140, loss = 0.69, batch loss = 0.63 (14.6 examples/sec; 0.549 sec/batch; 46h:54m:50s remains)
INFO - root - 2017-12-11 06:17:30.480005: step 25150, loss = 0.69, batch loss = 0.63 (14.2 examples/sec; 0.564 sec/batch; 48h:10m:14s remains)
INFO - root - 2017-12-11 06:17:36.027042: step 25160, loss = 0.70, batch loss = 0.64 (15.0 examples/sec; 0.534 sec/batch; 45h:36m:11s remains)
INFO - root - 2017-12-11 06:17:41.558664: step 25170, loss = 0.70, batch loss = 0.64 (14.7 examples/sec; 0.544 sec/batch; 46h:27m:52s remains)
INFO - root - 2017-12-11 06:17:47.030485: step 25180, loss = 0.70, batch loss = 0.65 (14.5 examples/sec; 0.553 sec/batch; 47h:10m:10s remains)
INFO - root - 2017-12-11 06:17:52.392004: step 25190, loss = 0.70, batch loss = 0.64 (14.2 examples/sec; 0.562 sec/batch; 48h:00m:03s remains)
INFO - root - 2017-12-11 06:17:57.941427: step 25200, loss = 0.69, batch loss = 0.63 (14.4 examples/sec; 0.556 sec/batch; 47h:28m:14s remains)
2017-12-11 06:17:58.515456: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0068957368 0.064254314 0.1586899 0.2678993 0.38014704 0.48692518 0.58566248 0.64441639 0.65107024 0.61177719 0.53466719 0.415329 0.25941932 0.11404404 0.018779298][-0.0012186738 0.079779059 0.18820354 0.31469896 0.44790378 0.57891691 0.70356059 0.78304535 0.80077 0.76578277 0.68501759 0.54817921 0.36323965 0.18989857 0.07840731][0.00035127261 0.087989293 0.20708118 0.34588772 0.49250779 0.63683659 0.77389139 0.86223716 0.88196486 0.84802455 0.76853019 0.63005942 0.43995577 0.26424757 0.15993732][-0.0015255128 0.091496877 0.21988128 0.36759108 0.52055234 0.66751969 0.803558 0.88737983 0.89787227 0.85785079 0.78114092 0.65374273 0.48178336 0.32949111 0.25272396][-0.0029114382 0.095537111 0.23310396 0.38757679 0.54068995 0.68143135 0.80562186 0.87326813 0.863669 0.80957741 0.73510969 0.62943208 0.49441394 0.38396773 0.34573504][-0.00088998419 0.10395622 0.24997759 0.40966195 0.559926 0.69040519 0.79744011 0.84342182 0.81184721 0.74622291 0.68047404 0.60518861 0.51380473 0.44749543 0.44404531][0.0042968597 0.11587078 0.2694726 0.43439981 0.58385539 0.70594811 0.79576623 0.81970656 0.77117449 0.70242286 0.65151757 0.60732573 0.55373365 0.51972789 0.53636855][0.0090792542 0.12498422 0.28210822 0.44836468 0.5946725 0.70642775 0.77920246 0.78726381 0.73558265 0.67747062 0.64825726 0.63242286 0.60449207 0.58470309 0.60270858][0.012619752 0.12684353 0.2774114 0.43325 0.56626987 0.66124845 0.71758389 0.72068679 0.684333 0.65378237 0.652279 0.65887505 0.64346439 0.62262094 0.62824667][0.010949204 0.11564961 0.24766161 0.38016787 0.4890891 0.56035078 0.59844267 0.602132 0.58980352 0.59181154 0.61520791 0.6366629 0.62703395 0.60176724 0.594734][0.00425618 0.093171291 0.19810089 0.29811028 0.376848 0.42252731 0.44317159 0.44823584 0.45698687 0.48449025 0.52351165 0.55196291 0.5459367 0.51951033 0.50605232][-0.003008896 0.069434419 0.14791551 0.21715304 0.26902229 0.29271236 0.29672498 0.29660952 0.31044424 0.34339362 0.38160488 0.40818796 0.40559536 0.38449845 0.37371463][-0.01029255 0.048691962 0.10686088 0.15297619 0.18448988 0.19067033 0.17824377 0.16523454 0.16815993 0.18943843 0.21612027 0.23742592 0.24060734 0.23238279 0.23374][-0.015888272 0.03341531 0.078270644 0.10948415 0.1271075 0.12035726 0.093668044 0.064971745 0.0503036 0.05381361 0.067099884 0.084071875 0.095383719 0.1047301 0.12507376][-0.019871784 0.02298172 0.060582526 0.084391557 0.094735675 0.080644272 0.044651158 0.0033668405 -0.026811006 -0.038270902 -0.034219302 -0.018871626 0.00014799119 0.025632136 0.0649102]]...]
INFO - root - 2017-12-11 06:18:04.053482: step 25210, loss = 0.70, batch loss = 0.64 (15.0 examples/sec; 0.534 sec/batch; 45h:34m:46s remains)
INFO - root - 2017-12-11 06:18:09.272495: step 25220, loss = 0.71, batch loss = 0.65 (14.8 examples/sec; 0.539 sec/batch; 46h:01m:01s remains)
INFO - root - 2017-12-11 06:18:14.841535: step 25230, loss = 0.68, batch loss = 0.62 (14.7 examples/sec; 0.543 sec/batch; 46h:21m:15s remains)
INFO - root - 2017-12-11 06:18:20.319234: step 25240, loss = 0.70, batch loss = 0.64 (14.6 examples/sec; 0.547 sec/batch; 46h:39m:28s remains)
INFO - root - 2017-12-11 06:18:25.773577: step 25250, loss = 0.69, batch loss = 0.63 (14.3 examples/sec; 0.559 sec/batch; 47h:44m:04s remains)
INFO - root - 2017-12-11 06:18:31.323530: step 25260, loss = 0.69, batch loss = 0.63 (14.5 examples/sec; 0.553 sec/batch; 47h:09m:55s remains)
INFO - root - 2017-12-11 06:18:36.879777: step 25270, loss = 0.69, batch loss = 0.64 (13.9 examples/sec; 0.576 sec/batch; 49h:06m:55s remains)
INFO - root - 2017-12-11 06:18:42.370681: step 25280, loss = 0.69, batch loss = 0.63 (14.9 examples/sec; 0.536 sec/batch; 45h:45m:25s remains)
INFO - root - 2017-12-11 06:18:47.834995: step 25290, loss = 0.68, batch loss = 0.62 (14.9 examples/sec; 0.537 sec/batch; 45h:50m:51s remains)
INFO - root - 2017-12-11 06:18:53.470430: step 25300, loss = 0.69, batch loss = 0.63 (14.1 examples/sec; 0.568 sec/batch; 48h:30m:37s remains)
2017-12-11 06:18:54.012744: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.15444764 0.15706928 0.13967313 0.11541909 0.092192233 0.085387334 0.1061495 0.14346185 0.17762846 0.18667348 0.16856465 0.12857373 0.073155694 0.016988577 -0.025770044][0.2498668 0.25678807 0.23382002 0.20098597 0.16885309 0.15988767 0.1892243 0.23939176 0.28294379 0.29072931 0.26029983 0.19870913 0.11717747 0.038122796 -0.019704439][0.33336946 0.34641269 0.32112032 0.28230208 0.24313922 0.23269764 0.26833296 0.32703042 0.37453517 0.37764126 0.33443281 0.25346255 0.14999747 0.052569441 -0.01642926][0.39444357 0.41859493 0.39880612 0.36136103 0.32104829 0.31121853 0.34930614 0.40826952 0.45161536 0.44596907 0.39002454 0.29243648 0.17173842 0.06125829 -0.014910386][0.42673495 0.46732032 0.4616915 0.43493217 0.40193591 0.39577994 0.431358 0.48115456 0.51182318 0.49424508 0.42745072 0.31736714 0.18402915 0.064571455 -0.015491379][0.4492631 0.511441 0.52727348 0.51790649 0.49718764 0.49408227 0.52002579 0.55161136 0.56320363 0.53242606 0.45729095 0.33906582 0.1973937 0.0714841 -0.011943474][0.46965313 0.55497962 0.59425789 0.60386896 0.59575176 0.5933007 0.6044004 0.612985 0.60366529 0.56025928 0.47912306 0.35739216 0.21265009 0.083467558 -0.0031332932][0.48202711 0.58345222 0.63979614 0.66230506 0.66098404 0.65437782 0.64804429 0.63452095 0.60803336 0.55648673 0.47431591 0.35575631 0.21552923 0.089150257 0.0026596452][0.47243124 0.57657146 0.6371696 0.66285735 0.66177434 0.64832389 0.6276117 0.59884828 0.56292289 0.51001149 0.43358064 0.32632583 0.19941483 0.0835145 0.0025045471][0.42105615 0.51018113 0.55953574 0.57729143 0.57088733 0.55127066 0.52299625 0.48926297 0.45362216 0.40754756 0.34503496 0.25879055 0.15559584 0.059695408 -0.0080800857][0.32752773 0.38977462 0.4186835 0.42250022 0.40876368 0.38604751 0.35780337 0.32787555 0.30000544 0.26675397 0.22342095 0.16376008 0.090683416 0.021348862 -0.027333237][0.20632462 0.23911791 0.24761385 0.23921803 0.2213423 0.20062897 0.17917135 0.15925092 0.14294623 0.12400156 0.09935993 0.064679317 0.020675397 -0.021693841 -0.050024934][0.088471182 0.097875945 0.092818707 0.078682974 0.061840784 0.047044951 0.035344351 0.026828256 0.021372918 0.013778049 0.002859232 -0.013519106 -0.035241358 -0.055990152 -0.067784645][0.00049942971 -0.0031064246 -0.012877966 -0.025968868 -0.037909247 -0.045745041 -0.048983578 -0.049126644 -0.047977291 -0.049208324 -0.052417122 -0.058293384 -0.066407338 -0.073303208 -0.074356638][-0.048140839 -0.055387605 -0.063348047 -0.071923785 -0.078385063 -0.081022821 -0.079520635 -0.075806111 -0.07229282 -0.071423985 -0.07169693 -0.073041312 -0.07483498 -0.075480551 -0.071982481]]...]
INFO - root - 2017-12-11 06:18:59.519555: step 25310, loss = 0.69, batch loss = 0.63 (14.3 examples/sec; 0.559 sec/batch; 47h:42m:15s remains)
INFO - root - 2017-12-11 06:19:04.657540: step 25320, loss = 0.70, batch loss = 0.64 (14.5 examples/sec; 0.550 sec/batch; 46h:56m:38s remains)
INFO - root - 2017-12-11 06:19:10.123834: step 25330, loss = 0.71, batch loss = 0.66 (15.0 examples/sec; 0.534 sec/batch; 45h:36m:06s remains)
INFO - root - 2017-12-11 06:19:15.588606: step 25340, loss = 0.70, batch loss = 0.65 (15.0 examples/sec; 0.534 sec/batch; 45h:32m:49s remains)
INFO - root - 2017-12-11 06:19:21.124989: step 25350, loss = 0.69, batch loss = 0.64 (13.8 examples/sec; 0.579 sec/batch; 49h:22m:13s remains)
INFO - root - 2017-12-11 06:19:26.650736: step 25360, loss = 0.70, batch loss = 0.65 (14.7 examples/sec; 0.545 sec/batch; 46h:29m:39s remains)
INFO - root - 2017-12-11 06:19:32.169289: step 25370, loss = 0.70, batch loss = 0.64 (14.7 examples/sec; 0.546 sec/batch; 46h:34m:45s remains)
INFO - root - 2017-12-11 06:19:37.693018: step 25380, loss = 0.69, batch loss = 0.63 (14.3 examples/sec; 0.560 sec/batch; 47h:45m:04s remains)
INFO - root - 2017-12-11 06:19:43.262101: step 25390, loss = 0.69, batch loss = 0.63 (14.7 examples/sec; 0.545 sec/batch; 46h:30m:34s remains)
INFO - root - 2017-12-11 06:19:48.742215: step 25400, loss = 0.69, batch loss = 0.64 (14.7 examples/sec; 0.543 sec/batch; 46h:20m:56s remains)
2017-12-11 06:19:49.305522: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.36025748 0.34519091 0.31927249 0.28812018 0.26072663 0.24095619 0.2293222 0.21590027 0.2059426 0.20373142 0.21130277 0.22713466 0.24012543 0.24826328 0.26139563][0.25291979 0.23673148 0.22545558 0.2168729 0.20908472 0.20382853 0.20363656 0.19963312 0.19605464 0.19900464 0.21271037 0.23282747 0.24501127 0.24659553 0.25030077][0.15443406 0.14134556 0.14465789 0.15633024 0.16492823 0.16958091 0.17459364 0.17542577 0.17670904 0.18525946 0.20683014 0.2338317 0.24752989 0.2443364 0.23977037][0.12049659 0.11644573 0.13176963 0.15732081 0.17516877 0.1820244 0.18235761 0.17918718 0.17979066 0.19235496 0.22196269 0.25688717 0.27424911 0.26878372 0.25846276][0.16818281 0.179536 0.20447834 0.23747745 0.25807047 0.26078454 0.24895161 0.23406766 0.22788845 0.23999526 0.27427551 0.31570369 0.33837673 0.3338061 0.32122502][0.27667454 0.30910677 0.34219074 0.376973 0.3953701 0.39090347 0.36463118 0.33528477 0.31951162 0.32745975 0.36117837 0.40403968 0.42974022 0.42547125 0.41145742][0.39429396 0.44948617 0.48957649 0.52340931 0.53755116 0.52599597 0.48806912 0.44680232 0.42237484 0.42414761 0.45173866 0.48797265 0.51000261 0.50232738 0.48675761][0.46042386 0.53395569 0.57968718 0.61164236 0.62125832 0.60465705 0.5613516 0.51493472 0.48694953 0.4840439 0.50233334 0.52394551 0.53340232 0.51764137 0.50117552][0.44367343 0.52375329 0.56965786 0.59701 0.60142726 0.58236378 0.54146206 0.49894822 0.47443578 0.47028431 0.47917703 0.48351291 0.47718236 0.4553397 0.44428289][0.35652333 0.4268221 0.46405825 0.4819124 0.47976616 0.46082625 0.42848891 0.39662343 0.37997454 0.37742251 0.38119459 0.37622339 0.36367956 0.346748 0.35014755][0.23955989 0.287698 0.30969459 0.315672 0.30748877 0.29168573 0.27150038 0.25263247 0.24441007 0.24619614 0.25338069 0.25360593 0.25011781 0.25004935 0.27330548][0.13581033 0.15851814 0.16429022 0.15944953 0.1470665 0.13616583 0.12895033 0.12341566 0.12417199 0.13516928 0.15727115 0.17799997 0.19553307 0.21583351 0.25467288][0.074861951 0.077760272 0.071114033 0.058122963 0.042302553 0.035179455 0.038670316 0.045420673 0.056347057 0.081476 0.12578388 0.17303127 0.21208875 0.24558648 0.28819922][0.065622143 0.060545638 0.048714377 0.031689633 0.012945401 0.006327576 0.015952924 0.032694947 0.053894315 0.093044914 0.15606433 0.22198756 0.27108598 0.30270344 0.33418703][0.095680028 0.095656745 0.087768316 0.072935171 0.053364657 0.045393027 0.057597257 0.080874212 0.10874113 0.15481704 0.2232938 0.29018179 0.33089289 0.34411433 0.35278583]]...]
INFO - root - 2017-12-11 06:19:54.573580: step 25410, loss = 0.68, batch loss = 0.63 (15.0 examples/sec; 0.533 sec/batch; 45h:26m:19s remains)
INFO - root - 2017-12-11 06:20:00.116422: step 25420, loss = 0.69, batch loss = 0.63 (14.7 examples/sec; 0.545 sec/batch; 46h:29m:45s remains)
INFO - root - 2017-12-11 06:20:05.540003: step 25430, loss = 0.69, batch loss = 0.63 (14.6 examples/sec; 0.548 sec/batch; 46h:43m:04s remains)
INFO - root - 2017-12-11 06:20:11.010720: step 25440, loss = 0.70, batch loss = 0.64 (14.7 examples/sec; 0.544 sec/batch; 46h:24m:15s remains)
INFO - root - 2017-12-11 06:20:16.510993: step 25450, loss = 0.68, batch loss = 0.62 (14.6 examples/sec; 0.549 sec/batch; 46h:47m:58s remains)
INFO - root - 2017-12-11 06:20:22.077675: step 25460, loss = 0.70, batch loss = 0.64 (14.7 examples/sec; 0.543 sec/batch; 46h:17m:12s remains)
INFO - root - 2017-12-11 06:20:27.582400: step 25470, loss = 0.69, batch loss = 0.64 (14.5 examples/sec; 0.550 sec/batch; 46h:56m:00s remains)
INFO - root - 2017-12-11 06:20:33.050266: step 25480, loss = 0.69, batch loss = 0.63 (14.6 examples/sec; 0.548 sec/batch; 46h:45m:49s remains)
INFO - root - 2017-12-11 06:20:38.661729: step 25490, loss = 0.71, batch loss = 0.65 (14.1 examples/sec; 0.566 sec/batch; 48h:13m:53s remains)
INFO - root - 2017-12-11 06:20:44.215726: step 25500, loss = 0.70, batch loss = 0.64 (14.6 examples/sec; 0.546 sec/batch; 46h:35m:57s remains)
2017-12-11 06:20:44.805683: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0047297669 -0.010826636 -0.020360127 -0.03160641 -0.042616166 -0.05119691 -0.056613017 -0.059026513 -0.059517261 -0.058438737 -0.05570104 -0.052792471 -0.050735604 -0.049491879 -0.047851633][0.043200776 0.042052623 0.03182159 0.014746182 -0.0049164 -0.022836141 -0.037233379 -0.047529329 -0.05393206 -0.056108028 -0.054613024 -0.052639604 -0.051648039 -0.050891321 -0.048299663][0.10647707 0.11809786 0.11353967 0.095006034 0.068984009 0.041315753 0.014509675 -0.0088441651 -0.026504133 -0.036744248 -0.040560715 -0.043046646 -0.045730587 -0.046779949 -0.043142688][0.16957045 0.2003752 0.20897251 0.19711559 0.17263886 0.14102259 0.10383616 0.065810382 0.033010263 0.0096740918 -0.0053087315 -0.018027447 -0.029135983 -0.034650065 -0.030744223][0.211447 0.26304153 0.29074603 0.29555458 0.28522682 0.26233059 0.2251109 0.17903367 0.1335185 0.095178425 0.063483961 0.032522257 0.0047250292 -0.011871099 -0.011622055][0.21858369 0.28509513 0.33166039 0.35851136 0.37199721 0.37046641 0.34707111 0.30539486 0.25600734 0.20679893 0.15759629 0.10366369 0.052205913 0.017606072 0.0090959938][0.19128481 0.26035014 0.31831452 0.36544126 0.40602633 0.43400687 0.4366743 0.41309619 0.37229109 0.3216415 0.2605769 0.1849969 0.10724453 0.049830165 0.02733198][0.13995305 0.1974301 0.25524637 0.31500414 0.37911013 0.43861789 0.47506231 0.48100623 0.46080112 0.41992372 0.356361 0.26555797 0.16419101 0.082942635 0.043209985][0.081957139 0.11724652 0.16257463 0.22240025 0.29825905 0.38102242 0.44896549 0.48857364 0.49825338 0.47982404 0.42793632 0.33534333 0.22071448 0.12092187 0.063883282][0.035788722 0.04569108 0.069691882 0.11610653 0.18778418 0.27818754 0.36570784 0.433197 0.47398391 0.48602813 0.4592512 0.38208789 0.27157319 0.16579865 0.09700606][0.013478677 0.0023656541 0.0025237885 0.025879571 0.078598887 0.15857969 0.24806862 0.33010384 0.39472392 0.43647054 0.44130704 0.39391029 0.30634052 0.21201196 0.14278205][0.011613305 -0.012161416 -0.031397957 -0.032363374 -0.0056094667 0.051525816 0.1276799 0.20962577 0.28631493 0.34986246 0.38454518 0.37360421 0.32276914 0.25603083 0.1999134][0.019678665 -0.0069382596 -0.035705339 -0.053693429 -0.050371438 -0.0200398 0.03283228 0.10132957 0.1766807 0.25051165 0.30764607 0.33230269 0.32358274 0.29422602 0.26235127][0.033028588 0.013361237 -0.01350251 -0.038128074 -0.051448587 -0.047205657 -0.023548448 0.020873044 0.082870096 0.15511055 0.22421537 0.27672893 0.30629194 0.31556556 0.31344169][0.045944758 0.038944889 0.021640962 -0.0017423554 -0.025076108 -0.042830013 -0.047649045 -0.030002054 0.013452061 0.076516338 0.14704548 0.21388383 0.2681419 0.3066448 0.33145553]]...]
INFO - root - 2017-12-11 06:20:49.970341: step 25510, loss = 0.68, batch loss = 0.62 (14.5 examples/sec; 0.552 sec/batch; 47h:06m:42s remains)
INFO - root - 2017-12-11 06:20:55.457549: step 25520, loss = 0.69, batch loss = 0.63 (14.7 examples/sec; 0.545 sec/batch; 46h:27m:10s remains)
INFO - root - 2017-12-11 06:21:00.973823: step 25530, loss = 0.69, batch loss = 0.63 (14.3 examples/sec; 0.559 sec/batch; 47h:40m:33s remains)
INFO - root - 2017-12-11 06:21:06.390727: step 25540, loss = 0.71, batch loss = 0.65 (14.6 examples/sec; 0.547 sec/batch; 46h:40m:43s remains)
INFO - root - 2017-12-11 06:21:11.899785: step 25550, loss = 0.70, batch loss = 0.64 (14.1 examples/sec; 0.566 sec/batch; 48h:14m:29s remains)
INFO - root - 2017-12-11 06:21:17.446256: step 25560, loss = 0.70, batch loss = 0.64 (14.6 examples/sec; 0.548 sec/batch; 46h:42m:23s remains)
INFO - root - 2017-12-11 06:21:22.923219: step 25570, loss = 0.68, batch loss = 0.63 (14.9 examples/sec; 0.535 sec/batch; 45h:37m:55s remains)
INFO - root - 2017-12-11 06:21:28.356731: step 25580, loss = 0.68, batch loss = 0.62 (14.5 examples/sec; 0.552 sec/batch; 47h:02m:48s remains)
INFO - root - 2017-12-11 06:21:33.904103: step 25590, loss = 0.71, batch loss = 0.65 (14.2 examples/sec; 0.562 sec/batch; 47h:53m:17s remains)
INFO - root - 2017-12-11 06:21:39.400331: step 25600, loss = 0.69, batch loss = 0.63 (14.9 examples/sec; 0.537 sec/batch; 45h:47m:15s remains)
2017-12-11 06:21:40.055698: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.31893727 0.36138853 0.38713607 0.40475154 0.41212559 0.39939874 0.35633862 0.28043148 0.17896844 0.06870082 -0.021932237 -0.07639651 -0.097030267 -0.094593532 -0.081048116][0.35590318 0.40107509 0.42629188 0.44149795 0.44702712 0.43413419 0.39126462 0.31354475 0.20640388 0.088353008 -0.01034253 -0.070880555 -0.095235392 -0.094557829 -0.0814934][0.34365276 0.39234537 0.4212186 0.43941 0.44901592 0.44233143 0.40699282 0.33499879 0.22849403 0.10722431 0.0031853335 -0.062218234 -0.0904397 -0.091569826 -0.079107992][0.28682429 0.34013754 0.37807634 0.40736282 0.42942616 0.43594632 0.41377762 0.35190052 0.24938948 0.12645873 0.017211014 -0.053996891 -0.086803325 -0.090333395 -0.078317739][0.19625664 0.25208387 0.30152816 0.34787208 0.38939264 0.41488123 0.40939057 0.36009103 0.26423439 0.14188258 0.028740678 -0.047771465 -0.085167572 -0.091016777 -0.079038344][0.096834943 0.15124287 0.20948488 0.27187112 0.33306265 0.37768379 0.38819525 0.35142666 0.26425171 0.14568616 0.03213267 -0.047144879 -0.087228157 -0.094626807 -0.082216874][0.011732865 0.058930665 0.1184966 0.1895684 0.26504028 0.32585457 0.35148257 0.32789078 0.25212607 0.14117424 0.03085129 -0.048143376 -0.089389518 -0.097800121 -0.085359544][-0.046291422 -0.011721083 0.040818881 0.11269477 0.19776094 0.27507019 0.31934023 0.31284237 0.25093624 0.14792307 0.039552014 -0.041809563 -0.086844154 -0.0981958 -0.087152131][-0.075191252 -0.054523632 -0.013437007 0.054209538 0.14541289 0.2387953 0.30330446 0.31555757 0.26730421 0.17003559 0.059986088 -0.027493661 -0.079176106 -0.095563643 -0.08743991][-0.079840533 -0.07066182 -0.041654158 0.018298361 0.1102167 0.21407977 0.29475453 0.3235186 0.28775367 0.19585209 0.083856322 -0.010284201 -0.068982422 -0.090744331 -0.08608482][-0.071467094 -0.069258332 -0.049985379 0.00045945265 0.086628444 0.19170927 0.28024781 0.32047281 0.29587618 0.21142077 0.10110015 0.0034793683 -0.06001978 -0.085879594 -0.084210493][-0.061413705 -0.062094677 -0.0490234 -0.0080034537 0.067638196 0.16497838 0.25186116 0.29685172 0.28138644 0.20730324 0.10448311 0.009309032 -0.054550465 -0.081790134 -0.081913084][-0.054042015 -0.055296168 -0.045889575 -0.013805506 0.04828139 0.13138056 0.2088684 0.2522746 0.24334779 0.181859 0.092087075 0.0058367839 -0.053265497 -0.078821234 -0.079486117][-0.049032167 -0.050589297 -0.0441965 -0.021266641 0.02508756 0.0894959 0.1516927 0.18793565 0.1826659 0.13500585 0.063010469 -0.0079401 -0.056799594 -0.077277571 -0.077090181][-0.044522908 -0.047559008 -0.044802777 -0.031149024 -0.00058531814 0.044501204 0.089996055 0.11725945 0.11424863 0.080384023 0.027732724 -0.025272863 -0.061747849 -0.07608176 -0.074725196]]...]
INFO - root - 2017-12-11 06:21:45.163710: step 25610, loss = 0.70, batch loss = 0.64 (14.6 examples/sec; 0.548 sec/batch; 46h:40m:32s remains)
INFO - root - 2017-12-11 06:21:50.704945: step 25620, loss = 0.69, batch loss = 0.63 (14.0 examples/sec; 0.571 sec/batch; 48h:40m:03s remains)
INFO - root - 2017-12-11 06:21:56.227635: step 25630, loss = 0.70, batch loss = 0.64 (14.4 examples/sec; 0.554 sec/batch; 47h:13m:33s remains)
INFO - root - 2017-12-11 06:22:01.807102: step 25640, loss = 0.69, batch loss = 0.63 (14.5 examples/sec; 0.552 sec/batch; 47h:03m:49s remains)
INFO - root - 2017-12-11 06:22:07.300155: step 25650, loss = 0.69, batch loss = 0.63 (14.9 examples/sec; 0.539 sec/batch; 45h:54m:51s remains)
INFO - root - 2017-12-11 06:22:12.722004: step 25660, loss = 0.72, batch loss = 0.66 (14.8 examples/sec; 0.541 sec/batch; 46h:05m:49s remains)
INFO - root - 2017-12-11 06:22:18.251346: step 25670, loss = 0.70, batch loss = 0.64 (14.4 examples/sec; 0.555 sec/batch; 47h:20m:06s remains)
INFO - root - 2017-12-11 06:22:23.790990: step 25680, loss = 0.68, batch loss = 0.62 (14.8 examples/sec; 0.541 sec/batch; 46h:07m:49s remains)
INFO - root - 2017-12-11 06:22:29.282803: step 25690, loss = 0.71, batch loss = 0.65 (14.6 examples/sec; 0.548 sec/batch; 46h:43m:45s remains)
INFO - root - 2017-12-11 06:22:34.715284: step 25700, loss = 0.69, batch loss = 0.63 (14.7 examples/sec; 0.546 sec/batch; 46h:31m:31s remains)
2017-12-11 06:22:35.212249: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.035228528 0.080345765 0.125155 0.15909746 0.17542183 0.17300898 0.1579299 0.13906425 0.12603246 0.12203228 0.12546524 0.1270047 0.11684123 0.09383478 0.06325081][0.037761711 0.084953181 0.13229802 0.16773927 0.18414308 0.18159154 0.16746993 0.15212922 0.14512235 0.14715913 0.15241806 0.14950448 0.13195881 0.10254254 0.07071504][0.037301645 0.085691854 0.13485484 0.17172912 0.18919016 0.18848231 0.17734994 0.16651772 0.16452435 0.16930345 0.172491 0.16398606 0.14282733 0.11503305 0.092082389][0.038350735 0.089162923 0.14202608 0.18311119 0.20537619 0.21058241 0.2052888 0.19885673 0.19780351 0.1983588 0.1930228 0.17680588 0.1554779 0.1367227 0.13122159][0.041267581 0.096038118 0.15529776 0.20482066 0.23734042 0.25365415 0.25712171 0.25393847 0.24803251 0.23612955 0.21583626 0.19080503 0.17350098 0.17028067 0.18808323][0.04420086 0.1036366 0.1714561 0.2332862 0.28106961 0.3128061 0.32668346 0.32455209 0.30833244 0.27856818 0.24121204 0.20992889 0.20167369 0.21818763 0.26048481][0.047308322 0.11135964 0.18835795 0.26375976 0.32825142 0.37572676 0.39827085 0.39355114 0.36352327 0.31579283 0.26557389 0.23433658 0.23967119 0.27631825 0.33844623][0.05043358 0.11784548 0.20180014 0.2874029 0.36374086 0.42016086 0.44392467 0.43125528 0.38724187 0.32808509 0.27606532 0.25541195 0.27941805 0.33463937 0.40856111][0.052075014 0.12045476 0.207128 0.2960372 0.37442872 0.4280453 0.44281086 0.41744959 0.36284146 0.3029134 0.26258218 0.26329866 0.31029215 0.38199726 0.4591684][0.05161481 0.11886247 0.20380032 0.28840625 0.3584038 0.39856228 0.39765334 0.35978156 0.30164436 0.25080514 0.23013984 0.25512373 0.32246962 0.40448707 0.47634298][0.050566439 0.11569536 0.19575961 0.27064168 0.32576415 0.3482751 0.33261752 0.28816953 0.23435198 0.19698617 0.19389722 0.23448078 0.3100476 0.39094239 0.45014033][0.05020402 0.11290918 0.18656768 0.24994473 0.29013184 0.29905823 0.276759 0.2346905 0.19035813 0.16362421 0.16706827 0.20677546 0.27355543 0.34102765 0.38401258][0.050666139 0.11129299 0.17882338 0.2326785 0.26327375 0.26774091 0.24927683 0.21719925 0.18304834 0.15901378 0.15470591 0.17674717 0.21966764 0.26375428 0.28870443][0.054940529 0.11610548 0.18144263 0.23175581 0.26049861 0.26847535 0.25935072 0.23810583 0.20961329 0.17975594 0.15776873 0.15255901 0.16356361 0.17815995 0.18214951][0.0644024 0.13026904 0.19931559 0.2529932 0.28544644 0.29866278 0.29554835 0.2782394 0.24860355 0.20961872 0.17098954 0.14202908 0.12451226 0.11073399 0.091824032]]...]
INFO - root - 2017-12-11 06:22:40.711876: step 25710, loss = 0.68, batch loss = 0.62 (14.5 examples/sec; 0.552 sec/batch; 47h:04m:47s remains)
INFO - root - 2017-12-11 06:22:46.187156: step 25720, loss = 0.68, batch loss = 0.62 (14.8 examples/sec; 0.539 sec/batch; 45h:57m:10s remains)
INFO - root - 2017-12-11 06:22:51.722238: step 25730, loss = 0.71, batch loss = 0.65 (14.5 examples/sec; 0.552 sec/batch; 46h:59m:58s remains)
INFO - root - 2017-12-11 06:22:57.330412: step 25740, loss = 0.70, batch loss = 0.64 (13.8 examples/sec; 0.581 sec/batch; 49h:31m:19s remains)
INFO - root - 2017-12-11 06:23:02.810472: step 25750, loss = 0.69, batch loss = 0.64 (14.8 examples/sec; 0.541 sec/batch; 46h:05m:31s remains)
INFO - root - 2017-12-11 06:23:08.297104: step 25760, loss = 0.70, batch loss = 0.64 (14.6 examples/sec; 0.548 sec/batch; 46h:40m:18s remains)
INFO - root - 2017-12-11 06:23:13.768490: step 25770, loss = 0.69, batch loss = 0.63 (14.7 examples/sec; 0.546 sec/batch; 46h:29m:45s remains)
INFO - root - 2017-12-11 06:23:19.312313: step 25780, loss = 0.69, batch loss = 0.64 (14.6 examples/sec; 0.549 sec/batch; 46h:48m:09s remains)
INFO - root - 2017-12-11 06:23:24.811276: step 25790, loss = 0.70, batch loss = 0.64 (14.8 examples/sec; 0.542 sec/batch; 46h:09m:57s remains)
INFO - root - 2017-12-11 06:23:29.829134: step 25800, loss = 0.71, batch loss = 0.65 (16.2 examples/sec; 0.494 sec/batch; 42h:06m:02s remains)
2017-12-11 06:23:30.413577: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.30492288 0.32048643 0.29950631 0.24761611 0.1811074 0.12939622 0.11772575 0.149799 0.20524688 0.26475722 0.32488593 0.38259062 0.42802191 0.44911116 0.43430403][0.36547109 0.39020908 0.36986846 0.31449115 0.24351902 0.18807001 0.17381918 0.20389937 0.2577734 0.31664571 0.37912449 0.44203079 0.49260694 0.51436555 0.493647][0.42053744 0.45299155 0.43312117 0.378521 0.31143036 0.26070365 0.24759667 0.27290615 0.31874618 0.36892897 0.423239 0.48169884 0.531652 0.55320293 0.52938879][0.47533664 0.51148188 0.49343753 0.44506404 0.38996756 0.35174128 0.34430078 0.36446565 0.39853197 0.43447012 0.4732917 0.52013171 0.56489176 0.58502007 0.55843085][0.52255279 0.55887842 0.54549873 0.510715 0.47738069 0.46186015 0.46674094 0.48175994 0.4978379 0.509913 0.52314407 0.54942018 0.58293229 0.59885693 0.57062167][0.550812 0.58362478 0.57930654 0.56635672 0.56349951 0.57811612 0.59760678 0.60332644 0.59209096 0.56994051 0.54967815 0.5503279 0.57075661 0.58371961 0.55993813][0.54574215 0.5765698 0.58647883 0.601349 0.63218117 0.67636138 0.70685118 0.69886452 0.65635318 0.59811014 0.54465723 0.52145231 0.53025186 0.542833 0.53031653][0.50732684 0.53949624 0.5707432 0.61762846 0.67983985 0.74481255 0.77635586 0.74816948 0.67196769 0.57924658 0.4986141 0.45999771 0.46499586 0.48569137 0.49488267][0.45889196 0.49872568 0.55620879 0.6344074 0.71895254 0.78986704 0.808069 0.75046659 0.63792038 0.51428956 0.4161092 0.37399787 0.38665062 0.4266139 0.46852577][0.41969723 0.47343531 0.5551815 0.65528208 0.7460866 0.80468571 0.79535365 0.7024954 0.55688953 0.41160855 0.30859223 0.27591929 0.30649367 0.37237203 0.44937867][0.39347303 0.46186721 0.55925089 0.66506261 0.74295044 0.77205926 0.72618324 0.60024846 0.43332925 0.28234455 0.1904503 0.17984465 0.23575541 0.32922328 0.43614352][0.36230657 0.44085309 0.54311675 0.63976753 0.69159812 0.68378973 0.6038143 0.4575586 0.28788406 0.14897181 0.08050026 0.09693151 0.17661518 0.29114816 0.417255][0.30208546 0.38303548 0.47930661 0.55757594 0.58132118 0.54358232 0.44481969 0.29806855 0.1441786 0.030019483 -0.011291696 0.026886048 0.11923035 0.24055994 0.37038749][0.21319082 0.28395137 0.36217025 0.41645142 0.41728482 0.36488295 0.26817238 0.14183521 0.020218415 -0.060774639 -0.077240817 -0.029388078 0.058403537 0.16726319 0.28186238][0.11245856 0.16329294 0.21596806 0.2459221 0.23271388 0.18093863 0.10263253 0.010471459 -0.070316523 -0.11669136 -0.11469906 -0.069069691 0.00024130632 0.081580833 0.16579364]]...]
INFO - root - 2017-12-11 06:23:35.895036: step 25810, loss = 0.70, batch loss = 0.65 (14.4 examples/sec; 0.557 sec/batch; 47h:25m:26s remains)
INFO - root - 2017-12-11 06:23:41.403283: step 25820, loss = 0.69, batch loss = 0.63 (14.3 examples/sec; 0.559 sec/batch; 47h:39m:14s remains)
INFO - root - 2017-12-11 06:23:46.916044: step 25830, loss = 0.69, batch loss = 0.64 (14.7 examples/sec; 0.544 sec/batch; 46h:21m:08s remains)
INFO - root - 2017-12-11 06:23:52.372262: step 25840, loss = 0.70, batch loss = 0.64 (14.6 examples/sec; 0.549 sec/batch; 46h:48m:08s remains)
INFO - root - 2017-12-11 06:23:57.842893: step 25850, loss = 0.67, batch loss = 0.62 (14.5 examples/sec; 0.551 sec/batch; 46h:55m:45s remains)
INFO - root - 2017-12-11 06:24:03.276070: step 25860, loss = 0.71, batch loss = 0.65 (14.7 examples/sec; 0.545 sec/batch; 46h:24m:32s remains)
INFO - root - 2017-12-11 06:24:08.728578: step 25870, loss = 0.69, batch loss = 0.63 (14.6 examples/sec; 0.548 sec/batch; 46h:42m:23s remains)
INFO - root - 2017-12-11 06:24:14.225856: step 25880, loss = 0.69, batch loss = 0.63 (14.3 examples/sec; 0.558 sec/batch; 47h:32m:42s remains)
INFO - root - 2017-12-11 06:24:19.745674: step 25890, loss = 0.70, batch loss = 0.65 (14.8 examples/sec; 0.541 sec/batch; 46h:06m:12s remains)
INFO - root - 2017-12-11 06:24:24.869782: step 25900, loss = 0.70, batch loss = 0.64 (14.3 examples/sec; 0.560 sec/batch; 47h:42m:20s remains)
2017-12-11 06:24:25.414231: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.0029390948 0.035176326 0.066172034 0.088656329 0.094999611 0.0792997 0.047629256 0.01249129 -0.017083088 -0.03930968 -0.052801769 -0.057549246 -0.058022112 -0.060839791 -0.06633006][0.051822551 0.10208861 0.14989351 0.1861165 0.19781615 0.17714719 0.13340656 0.0856162 0.044670116 0.011600275 -0.011041191 -0.022450879 -0.028431276 -0.039144926 -0.053778313][0.0952553 0.15889533 0.22016296 0.26879224 0.28764242 0.26837125 0.22253557 0.1734238 0.1307525 0.093251213 0.063593939 0.044228412 0.028784124 0.0046542515 -0.025093857][0.11855718 0.18761767 0.25600544 0.31362402 0.34260651 0.33470321 0.30107939 0.26655009 0.23670585 0.20594388 0.1744491 0.14660443 0.11735207 0.073177919 0.020905603][0.12640521 0.19727895 0.26969492 0.33477315 0.3781206 0.39034665 0.37972334 0.37146822 0.36582035 0.35093588 0.32111904 0.28292078 0.23494145 0.16412114 0.082305141][0.13763575 0.21539071 0.29496962 0.36898696 0.42910403 0.46303976 0.47533828 0.49294451 0.51201165 0.51312882 0.48269916 0.42979538 0.35900626 0.25818214 0.14449242][0.16802512 0.26225945 0.35486183 0.43931502 0.5128687 0.55993617 0.58293229 0.6130923 0.645471 0.65323061 0.61600578 0.54523206 0.45197117 0.32479164 0.18516214][0.21012162 0.3240124 0.42979804 0.52073705 0.59741569 0.64245278 0.65918785 0.68380189 0.71290088 0.71491754 0.66675723 0.58155471 0.47444034 0.33415663 0.18448621][0.23130909 0.35320288 0.46081471 0.54674053 0.61299735 0.64320534 0.64295137 0.64986366 0.66314262 0.65161681 0.5948146 0.50588846 0.40076086 0.26865962 0.13251019][0.19955513 0.30632362 0.39634967 0.46255964 0.50747311 0.51823342 0.50183493 0.49111372 0.48734865 0.46460688 0.40851808 0.33048445 0.2445388 0.14177002 0.040857997][0.11350306 0.1843065 0.24073774 0.27753341 0.29772314 0.29278451 0.269458 0.25119296 0.23947518 0.21577254 0.17210105 0.11703502 0.061248627 -0.0010873032 -0.057275686][0.0065807882 0.035840929 0.056599475 0.065441608 0.0658218 0.053184457 0.032402743 0.01678513 0.0069234716 -0.0084286015 -0.032977678 -0.061269183 -0.086115457 -0.11038932 -0.12730768][-0.077914208 -0.079181448 -0.082493782 -0.090185061 -0.099405281 -0.11214863 -0.12580922 -0.13439175 -0.1383086 -0.14321946 -0.15063936 -0.1576674 -0.16013563 -0.15922706 -0.15249313][-0.11416717 -0.12819906 -0.1398937 -0.15135813 -0.16100945 -0.16971618 -0.17662309 -0.17966911 -0.17934105 -0.1780047 -0.17620118 -0.17265671 -0.16522016 -0.15456416 -0.14064258][-0.10788485 -0.12130298 -0.13101073 -0.13938721 -0.14572977 -0.15049408 -0.15358634 -0.15447499 -0.15326914 -0.15069351 -0.14686283 -0.14136304 -0.13342491 -0.12367672 -0.11280757]]...]
INFO - root - 2017-12-11 06:24:30.930836: step 25910, loss = 0.69, batch loss = 0.63 (14.4 examples/sec; 0.555 sec/batch; 47h:18m:22s remains)
INFO - root - 2017-12-11 06:24:36.541356: step 25920, loss = 0.71, batch loss = 0.65 (14.4 examples/sec; 0.554 sec/batch; 47h:10m:09s remains)
INFO - root - 2017-12-11 06:24:42.055196: step 25930, loss = 0.70, batch loss = 0.64 (14.8 examples/sec; 0.540 sec/batch; 46h:00m:50s remains)
INFO - root - 2017-12-11 06:24:47.512916: step 25940, loss = 0.69, batch loss = 0.63 (14.7 examples/sec; 0.546 sec/batch; 46h:27m:08s remains)
INFO - root - 2017-12-11 06:24:52.994713: step 25950, loss = 0.68, batch loss = 0.62 (13.7 examples/sec; 0.585 sec/batch; 49h:47m:07s remains)
INFO - root - 2017-12-11 06:24:58.521107: step 25960, loss = 0.69, batch loss = 0.63 (14.7 examples/sec; 0.545 sec/batch; 46h:24m:06s remains)
INFO - root - 2017-12-11 06:25:04.016601: step 25970, loss = 0.69, batch loss = 0.63 (14.4 examples/sec; 0.554 sec/batch; 47h:10m:45s remains)
INFO - root - 2017-12-11 06:25:09.492541: step 25980, loss = 0.72, batch loss = 0.67 (14.3 examples/sec; 0.561 sec/batch; 47h:45m:47s remains)
INFO - root - 2017-12-11 06:25:14.950507: step 25990, loss = 0.70, batch loss = 0.64 (14.6 examples/sec; 0.547 sec/batch; 46h:32m:14s remains)
INFO - root - 2017-12-11 06:25:19.544121: step 26000, loss = 0.69, batch loss = 0.63 (14.7 examples/sec; 0.545 sec/batch; 46h:24m:56s remains)
2017-12-11 06:25:20.175442: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.16965477 0.15627024 0.14414957 0.14154993 0.14213106 0.13975157 0.13013478 0.11578161 0.099043332 0.081719428 0.067846775 0.066568255 0.079882868 0.09848316 0.11203431][0.129331 0.1223381 0.11739616 0.12188029 0.13011967 0.13558947 0.13269724 0.12345655 0.1100696 0.093240775 0.076375283 0.06877625 0.073847473 0.083850324 0.089691341][0.089631535 0.0907072 0.093505494 0.10365383 0.116883 0.12842101 0.13324207 0.13211641 0.12543371 0.11155881 0.092291512 0.076339781 0.069726758 0.068420827 0.065962777][0.07226909 0.083308011 0.09454751 0.10913555 0.12393354 0.13740057 0.14731321 0.15351878 0.15396555 0.14389823 0.1230065 0.098749608 0.080242284 0.06822063 0.059073459][0.080129243 0.10427354 0.12598127 0.1456546 0.16050044 0.17252208 0.18336113 0.19304422 0.19799873 0.19108056 0.16993913 0.1402677 0.11362157 0.0946306 0.081704766][0.10611057 0.14459345 0.17851777 0.20528986 0.22152863 0.23140474 0.23979092 0.24743739 0.25101203 0.24314713 0.22095457 0.18925944 0.1607454 0.14117821 0.12896518][0.13155536 0.1818004 0.22727241 0.26366767 0.285719 0.29662991 0.30223805 0.30377567 0.30017862 0.28667951 0.26166278 0.23039724 0.20547484 0.19087297 0.18253361][0.14652912 0.20492053 0.26002267 0.30630341 0.33600748 0.34896559 0.35068867 0.34359664 0.33042443 0.31004632 0.28213149 0.25321725 0.2350107 0.22806895 0.22473249][0.14854835 0.21163847 0.27327886 0.32718977 0.3631022 0.37661681 0.37282667 0.35647109 0.33490011 0.30995205 0.28105867 0.25612506 0.24560112 0.24651884 0.24812235][0.14109443 0.20367479 0.2669377 0.3238818 0.36190432 0.37305787 0.36266512 0.33817151 0.31115925 0.28594133 0.26033431 0.24241243 0.24064627 0.24880928 0.25488979][0.13110642 0.18692534 0.24497624 0.29851735 0.33367574 0.34110066 0.32683909 0.2992765 0.27151623 0.24935086 0.22928359 0.21931377 0.22573707 0.23951133 0.24890555][0.11956165 0.16296864 0.20961313 0.25339165 0.28103781 0.28468317 0.27118728 0.24745701 0.22447821 0.20777468 0.19337818 0.18939552 0.20079058 0.21683611 0.22721487][0.09826722 0.12761216 0.1601796 0.19008632 0.20656233 0.20602967 0.19675413 0.18255532 0.16915214 0.15939826 0.14949122 0.1481199 0.16014469 0.17489378 0.18401238][0.058482129 0.074452445 0.093632251 0.11011812 0.1160631 0.11235149 0.10817295 0.10436681 0.10107365 0.09789896 0.09142559 0.090478353 0.10041054 0.11194669 0.1187089][0.001311328 0.0057509616 0.014061398 0.020705473 0.020170204 0.015506222 0.015363339 0.018663969 0.022268113 0.023730844 0.020255473 0.019578166 0.026967399 0.035211928 0.039597984]]...]
INFO - root - 2017-12-11 06:25:25.737531: step 26010, loss = 0.71, batch loss = 0.65 (14.8 examples/sec; 0.541 sec/batch; 46h:05m:25s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-SGD-lr0.001-clip50-initfromfixqian30k-fixqian
INFO - root - 2017-12-11 06:25:31.165866: step 26020, loss = 0.71, batch loss = 0.65 (14.8 examples/sec; 0.542 sec/batch; 46h:09m:55s remains)
INFO - root - 2017-12-11 06:25:36.708419: step 26030, loss = 0.69, batch loss = 0.63 (14.3 examples/sec; 0.560 sec/batch; 47h:42m:17s remains)
INFO - root - 2017-12-11 06:25:42.253316: step 26040, loss = 0.71, batch loss = 0.66 (14.6 examples/sec; 0.547 sec/batch; 46h:35m:18s remains)
INFO - root - 2017-12-11 06:25:47.726408: step 26050, loss = 0.71, batch loss = 0.65 (14.6 examples/sec; 0.546 sec/batch; 46h:29m:56s remains)
INFO - root - 2017-12-11 06:25:53.167302: step 26060, loss = 0.71, batch loss = 0.65 (14.5 examples/sec; 0.550 sec/batch; 46h:51m:12s remains)
INFO - root - 2017-12-11 06:25:58.636787: step 26070, loss = 0.71, batch loss = 0.65 (14.5 examples/sec; 0.553 sec/batch; 47h:02m:44s remains)
INFO - root - 2017-12-11 06:26:04.230960: step 26080, loss = 0.69, batch loss = 0.63 (14.6 examples/sec; 0.548 sec/batch; 46h:39m:05s remains)
INFO - root - 2017-12-11 06:26:09.665983: step 26090, loss = 0.70, batch loss = 0.64 (14.9 examples/sec; 0.536 sec/batch; 45h:38m:04s remains)
INFO - root - 2017-12-11 06:26:14.884228: step 26100, loss = 0.68, batch loss = 0.62 (14.6 examples/sec; 0.547 sec/batch; 46h:35m:21s remains)
2017-12-11 06:26:15.492592: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.12306067 0.14641163 0.16410774 0.17039502 0.16672574 0.16514322 0.18251593 0.2159752 0.2466732 0.25162664 0.22872956 0.18687168 0.13178627 0.084413409 0.062693037][0.1063266 0.13301878 0.15334144 0.16310725 0.16206168 0.16074784 0.17464355 0.19945459 0.21783744 0.21188475 0.1852432 0.14776488 0.10327923 0.069212392 0.058655277][0.086813129 0.11396912 0.13740449 0.15463792 0.16287434 0.16812195 0.18096015 0.19548891 0.19748253 0.17601472 0.14096387 0.10324954 0.065663159 0.039901789 0.0339497][0.084743947 0.11019976 0.13475268 0.15923987 0.17908826 0.19441359 0.20852044 0.21361113 0.19874187 0.15965419 0.11199959 0.068986222 0.033454727 0.01065456 0.0037788393][0.11376602 0.13457386 0.15550482 0.18238261 0.21025562 0.23413353 0.24957225 0.24660008 0.2166464 0.16129723 0.10105168 0.052340407 0.018629944 -0.001572918 -0.0094778407][0.17230515 0.18558936 0.19687513 0.21803437 0.24603193 0.27304274 0.28777167 0.27810207 0.23698208 0.17097582 0.10364094 0.054602217 0.027407747 0.014064633 0.00922382][0.23534016 0.24041611 0.23964305 0.24966957 0.27124915 0.29605895 0.307581 0.29221442 0.24491115 0.17649488 0.1114853 0.0715702 0.058482636 0.058086909 0.061214425][0.26122874 0.26143691 0.25383386 0.25570896 0.27077156 0.29130772 0.29867524 0.27953655 0.23212148 0.1707527 0.11933316 0.098559953 0.10593992 0.12234418 0.13583741][0.23080918 0.23190407 0.22795987 0.23173442 0.24667 0.26536164 0.26988983 0.24919564 0.20643485 0.15889128 0.12834023 0.13080546 0.15827645 0.18859306 0.20909083][0.15929508 0.16451654 0.172423 0.18860392 0.21238475 0.23486751 0.24007142 0.22029367 0.18437964 0.15175706 0.14125443 0.16196403 0.20087254 0.23600139 0.25607455][0.083819181 0.092179812 0.11285323 0.1449725 0.18171589 0.211568 0.22047345 0.20357847 0.17429078 0.15282142 0.15476149 0.18212058 0.21947007 0.24889936 0.26196128][0.034545489 0.043784387 0.072756521 0.11684586 0.16382346 0.19987193 0.21267402 0.19911869 0.17452784 0.15781717 0.16126725 0.18245293 0.20747414 0.225526 0.23108433][0.024672249 0.03481571 0.067292176 0.11618548 0.16684611 0.20526497 0.22121149 0.21163264 0.19060215 0.17293799 0.16754957 0.17207438 0.17947306 0.18770747 0.19254945][0.046655532 0.059864428 0.092372715 0.1386577 0.18560117 0.22203514 0.24035856 0.23601921 0.21949229 0.19899301 0.18013941 0.16516502 0.15783386 0.16441773 0.17916015][0.08039318 0.098164678 0.12899084 0.16884267 0.20829017 0.24009028 0.25906277 0.25925955 0.24740988 0.22465327 0.19432442 0.16511463 0.15156586 0.1655331 0.19774035]]...]
INFO - root - 2017-12-11 06:26:21.002130: step 26110, loss = 0.69, batch loss = 0.63 (14.6 examples/sec; 0.546 sec/batch; 46h:28m:41s remains)
INFO - root - 2017-12-11 06:26:26.471960: step 26120, loss = 0.68, batch loss = 0.62 (14.9 examples/sec; 0.538 sec/batch; 45h:46m:40s remains)
INFO - root - 2017-12-11 06:26:31.900116: step 26130, loss = 0.70, batch loss = 0.64 (14.5 examples/sec; 0.553 sec/batch; 47h:03m:53s remains)
INFO - root - 2017-12-11 06:26:37.479001: step 26140, loss = 0.70, batch loss = 0.64 (13.5 examples/sec; 0.591 sec/batch; 50h:18m:13s remains)
INFO - root - 2017-12-11 06:26:42.991938: step 26150, loss = 0.70, batch loss = 0.64 (14.6 examples/sec; 0.549 sec/batch; 46h:41m:01s remains)
INFO - root - 2017-12-11 06:26:48.577124: step 26160, loss = 0.69, batch loss = 0.63 (14.6 examples/sec; 0.550 sec/batch; 46h:46m:35s remains)
INFO - root - 2017-12-11 06:26:54.032163: step 26170, loss = 0.69, batch loss = 0.63 (14.8 examples/sec; 0.541 sec/batch; 46h:00m:58s remains)
INFO - root - 2017-12-11 06:26:59.526376: step 26180, loss = 0.69, batch loss = 0.63 (14.9 examples/sec; 0.539 sec/batch; 45h:49m:38s remains)
INFO - root - 2017-12-11 06:27:05.001958: step 26190, loss = 0.71, batch loss = 0.66 (14.6 examples/sec; 0.546 sec/batch; 46h:28m:25s remains)
INFO - root - 2017-12-11 06:27:10.302637: step 26200, loss = 0.71, batch loss = 0.65 (14.7 examples/sec; 0.546 sec/batch; 46h:25m:25s remains)
2017-12-11 06:27:10.868240: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.088221289 0.060662072 0.042702507 0.038080562 0.040949341 0.043893382 0.045897793 0.045859788 0.043482553 0.041793309 0.043902095 0.051433142 0.062094517 0.073453486 0.086896189][0.14993146 0.11573531 0.093397573 0.089150459 0.0954242 0.10081422 0.10345589 0.10202698 0.096876763 0.093171082 0.093937717 0.10185356 0.11476552 0.13022992 0.15032315][0.20907119 0.17695589 0.15644017 0.15657668 0.16975407 0.18059534 0.18553105 0.18267813 0.17289376 0.16346949 0.15778309 0.16038969 0.16924031 0.18373127 0.20767811][0.26428673 0.24230339 0.22795148 0.23254541 0.25106946 0.26590991 0.271753 0.26556233 0.24911503 0.23213395 0.21818495 0.21180803 0.21167225 0.22054273 0.24494934][0.31571275 0.30504453 0.29482165 0.30049819 0.32247272 0.34243506 0.35125914 0.34399903 0.32354656 0.30113044 0.27937135 0.2622256 0.25032154 0.25092551 0.27247804][0.35621724 0.35516277 0.34744161 0.35427514 0.38213736 0.41248098 0.42973495 0.42452711 0.40155262 0.37369117 0.34228304 0.31195441 0.28833374 0.28412181 0.30689174][0.3758173 0.3836118 0.37933078 0.39001417 0.42613614 0.47020271 0.5005 0.50160813 0.478953 0.44614246 0.4046284 0.36043248 0.32657158 0.32156768 0.34980771][0.37424716 0.39035434 0.39111471 0.40576613 0.44559821 0.49605602 0.53340578 0.53952169 0.520542 0.48865506 0.44540378 0.39725292 0.36210328 0.36066332 0.39361504][0.37543562 0.39655536 0.39908504 0.41049966 0.4406631 0.48020095 0.50960737 0.51306057 0.49799252 0.47360763 0.44034237 0.40312591 0.37928325 0.386332 0.42093849][0.38800478 0.4086315 0.40700197 0.40628439 0.414543 0.428851 0.4382737 0.43176705 0.41715443 0.40062323 0.38087884 0.36035091 0.35211506 0.3674821 0.40072152][0.39114994 0.41121492 0.40678537 0.39411539 0.37939289 0.36559871 0.35166982 0.33069831 0.31036627 0.29541555 0.28432032 0.27744913 0.28216413 0.30390206 0.33618391][0.36778972 0.38942036 0.38801572 0.37161142 0.3425552 0.3073844 0.27370545 0.23755403 0.206152 0.18544024 0.17602722 0.17693037 0.18920772 0.21329395 0.24360026][0.32133114 0.34643421 0.35362959 0.34235549 0.30958205 0.2626982 0.21463209 0.16425125 0.12006969 0.0915872 0.081652135 0.087838717 0.10478343 0.12855589 0.15458632][0.27211165 0.29936758 0.31385103 0.30974355 0.27940279 0.22854808 0.1720826 0.11161176 0.057679515 0.02303317 0.011489369 0.019329751 0.037021447 0.058066383 0.07837826][0.22866632 0.25507605 0.27308297 0.27470317 0.24935827 0.19973189 0.14021131 0.075092122 0.016519848 -0.021682655 -0.036433235 -0.032118924 -0.018836519 -0.0029263536 0.012078881]]...]
INFO - root - 2017-12-11 06:27:16.516286: step 26210, loss = 0.68, batch loss = 0.63 (14.6 examples/sec; 0.547 sec/batch; 46h:31m:22s remains)
INFO - root - 2017-12-11 06:27:22.016247: step 26220, loss = 0.70, batch loss = 0.64 (14.4 examples/sec; 0.555 sec/batch; 47h:12m:22s remains)
INFO - root - 2017-12-11 06:27:27.586703: step 26230, loss = 0.69, batch loss = 0.63 (14.7 examples/sec; 0.545 sec/batch; 46h:20m:41s remains)
INFO - root - 2017-12-11 06:27:33.169968: step 26240, loss = 0.70, batch loss = 0.65 (14.5 examples/sec; 0.552 sec/batch; 46h:59m:40s remains)
INFO - root - 2017-12-11 06:27:38.642599: step 26250, loss = 0.69, batch loss = 0.63 (14.7 examples/sec; 0.543 sec/batch; 46h:10m:42s remains)
INFO - root - 2017-12-11 06:27:44.089041: step 26260, loss = 0.69, batch loss = 0.63 (14.5 examples/sec; 0.550 sec/batch; 46h:49m:32s remains)
INFO - root - 2017-12-11 06:27:49.596421: step 26270, loss = 0.70, batch loss = 0.64 (14.3 examples/sec; 0.559 sec/batch; 47h:33m:06s remains)
INFO - root - 2017-12-11 06:27:55.182473: step 26280, loss = 0.70, batch loss = 0.64 (14.5 examples/sec; 0.553 sec/batch; 47h:02m:33s remains)
INFO - root - 2017-12-11 06:28:00.258374: step 26290, loss = 0.71, batch loss = 0.65 (14.1 examples/sec; 0.569 sec/batch; 48h:22m:29s remains)
INFO - root - 2017-12-11 06:28:05.924034: step 26300, loss = 0.70, batch loss = 0.64 (14.8 examples/sec; 0.539 sec/batch; 45h:52m:46s remains)
2017-12-11 06:28:06.568815: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.5041616 0.49290222 0.45271954 0.40333867 0.36547258 0.3609153 0.39562055 0.4546628 0.52902722 0.59552336 0.6466983 0.67394435 0.66836137 0.62167287 0.53822297][0.54874551 0.52096295 0.46468806 0.40582356 0.36268708 0.3538973 0.3872017 0.45237923 0.53906643 0.62188238 0.68751049 0.7247436 0.72336769 0.67739332 0.59423459][0.53093779 0.49417228 0.43702424 0.38466358 0.34938204 0.34209412 0.36963513 0.42837641 0.51022887 0.5919981 0.65711111 0.6940909 0.69452381 0.654881 0.58564085][0.46909016 0.43936977 0.40339661 0.37938374 0.37143049 0.37910616 0.40510821 0.45093393 0.5121749 0.57158941 0.61447793 0.63388133 0.62681311 0.59461427 0.54806197][0.38184947 0.37687704 0.384044 0.40901205 0.44706374 0.48542675 0.52018654 0.5545038 0.58650088 0.6067422 0.60778248 0.5933637 0.56935167 0.54128224 0.5177592][0.27111396 0.30259639 0.36602503 0.45232761 0.54867095 0.63135678 0.68814653 0.71873271 0.721777 0.69612491 0.646524 0.59146291 0.54792064 0.52309906 0.51983345][0.13867934 0.20363824 0.32068372 0.46717161 0.62235379 0.75454259 0.84209269 0.8774575 0.85987657 0.79736269 0.70828778 0.62394106 0.56937122 0.5503996 0.56146353][0.0058663487 0.0895112 0.24240404 0.43159926 0.63061088 0.80203712 0.91691178 0.9614417 0.93801785 0.86189055 0.76086175 0.67125583 0.61956066 0.60765922 0.62118345][-0.082046881 0.00056143192 0.15862224 0.35690406 0.566249 0.74839371 0.87245649 0.92277139 0.906788 0.84296316 0.7609539 0.69198149 0.65610808 0.65012223 0.65306634][-0.10391009 -0.041324235 0.090321645 0.26050311 0.44148636 0.59962857 0.70817876 0.75513035 0.75268108 0.71762455 0.67327309 0.63938612 0.62635064 0.62422681 0.60974318][-0.072654411 -0.041237559 0.042235903 0.1574755 0.28296158 0.39412254 0.4721835 0.51090133 0.523959 0.52208841 0.51773673 0.51804274 0.52449244 0.52281326 0.49130788][-0.013028191 -0.014547368 0.015629945 0.068672076 0.13215782 0.19252522 0.2393941 0.2710655 0.29806063 0.32254755 0.3462317 0.36783284 0.3832649 0.3775543 0.33394784][0.034038786 0.0082531245 -0.0030447161 0.00032954791 0.014511468 0.034872998 0.057840906 0.0839979 0.11834177 0.1556167 0.19067542 0.21875209 0.23360084 0.22273223 0.17582572][0.033673365 -0.0035383473 -0.035964221 -0.059653204 -0.072799563 -0.074463338 -0.064112134 -0.041001976 -0.0056351931 0.033158965 0.068022773 0.093783475 0.10424469 0.090343058 0.047680024][-0.018009439 -0.054516468 -0.087903567 -0.11580041 -0.13439748 -0.14116429 -0.13457468 -0.11446111 -0.08410152 -0.051676195 -0.023453563 -0.003336115 0.003356436 -0.0096295644 -0.042845927]]...]
INFO - root - 2017-12-11 06:28:12.082472: step 26310, loss = 0.70, batch loss = 0.64 (14.2 examples/sec; 0.564 sec/batch; 48h:00m:39s remains)
INFO - root - 2017-12-11 06:28:17.571818: step 26320, loss = 0.70, batch loss = 0.64 (14.6 examples/sec; 0.548 sec/batch; 46h:36m:04s remains)
INFO - root - 2017-12-11 06:28:23.058967: step 26330, loss = 0.69, batch loss = 0.63 (14.5 examples/sec; 0.552 sec/batch; 46h:56m:35s remains)
INFO - root - 2017-12-11 06:28:28.602023: step 26340, loss = 0.70, batch loss = 0.64 (15.0 examples/sec; 0.532 sec/batch; 45h:15m:05s remains)
INFO - root - 2017-12-11 06:28:34.139328: step 26350, loss = 0.69, batch loss = 0.63 (14.4 examples/sec; 0.555 sec/batch; 47h:10m:58s remains)
INFO - root - 2017-12-11 06:28:39.615300: step 26360, loss = 0.70, batch loss = 0.64 (14.6 examples/sec; 0.549 sec/batch; 46h:38m:39s remains)
INFO - root - 2017-12-11 06:28:45.162605: step 26370, loss = 0.70, batch loss = 0.64 (14.5 examples/sec; 0.553 sec/batch; 47h:00m:11s remains)
INFO - root - 2017-12-11 06:28:50.646945: step 26380, loss = 0.69, batch loss = 0.63 (14.8 examples/sec; 0.541 sec/batch; 46h:00m:21s remains)
INFO - root - 2017-12-11 06:28:55.848741: step 26390, loss = 0.70, batch loss = 0.64 (14.3 examples/sec; 0.558 sec/batch; 47h:25m:56s remains)
INFO - root - 2017-12-11 06:29:01.389438: step 26400, loss = 0.70, batch loss = 0.64 (14.1 examples/sec; 0.567 sec/batch; 48h:14m:40s remains)
2017-12-11 06:29:02.023627: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.039329439 -0.036833841 -0.030356493 -0.025212552 -0.025191683 -0.030907579 -0.039264604 -0.047029357 -0.052762307 -0.054330662 -0.050411217 -0.043093562 -0.036236681 -0.031886045 -0.030531252][-0.032951761 -0.020328527 -0.002308243 0.012057072 0.016509425 0.0089178933 -0.0068738861 -0.025069395 -0.042075649 -0.053664044 -0.057161152 -0.054040574 -0.04863032 -0.043642458 -0.0400802][-0.021669449 0.0059432676 0.042119656 0.072515547 0.087134041 0.081164144 0.058388125 0.027493535 -0.005090334 -0.031947408 -0.04852565 -0.054958243 -0.055245284 -0.052551888 -0.048461802][-0.009541546 0.034251913 0.09141358 0.14237146 0.17336112 0.17606127 0.15231116 0.11159962 0.062643565 0.016628524 -0.018907098 -0.04141343 -0.053161129 -0.057088107 -0.055289183][0.00067124562 0.0571883 0.13260557 0.20413676 0.25576004 0.27569124 0.26165041 0.21967994 0.15867005 0.093534306 0.035321925 -0.0092894295 -0.038895484 -0.054574609 -0.05850457][0.0053354991 0.06703674 0.15300064 0.24083042 0.3146086 0.35929385 0.3660292 0.33365712 0.26845613 0.18850696 0.10807708 0.038525879 -0.013303887 -0.044935282 -0.057837162][0.0021638186 0.059595816 0.14502919 0.24085075 0.33359206 0.40551829 0.44005963 0.42672697 0.36734438 0.28122914 0.18481226 0.093605287 0.020233354 -0.02817923 -0.0516821][-0.0073620914 0.037956946 0.11212345 0.20457105 0.30646563 0.39918092 0.45982966 0.46942738 0.42480719 0.34385622 0.24282469 0.13952123 0.051701963 -0.00910109 -0.041440409][-0.019903673 0.0095038377 0.064997636 0.14291011 0.23980097 0.33892071 0.41516057 0.44500273 0.42116722 0.35685688 0.26534268 0.16417196 0.073525786 0.0081271362 -0.029093675][-0.031419016 -0.017963285 0.01591026 0.071252264 0.14932963 0.23770438 0.31381717 0.35490286 0.35159871 0.31085196 0.24100667 0.15692548 0.077465169 0.017528227 -0.018650331][-0.039430324 -0.03900858 -0.02472236 0.0062103434 0.057776254 0.12269524 0.18433277 0.22486302 0.23516347 0.21688372 0.17349735 0.11568197 0.058150839 0.013184927 -0.015123836][-0.042146154 -0.050314017 -0.050010949 -0.039171524 -0.013567648 0.023717191 0.062946267 0.093021475 0.1072831 0.10453269 0.084884346 0.054531313 0.022571381 -0.0031914026 -0.019879738][-0.040762551 -0.052881151 -0.059748176 -0.061210774 -0.054610897 -0.040784564 -0.023793535 -0.0084549291 0.0019031209 0.0059826346 0.002391842 -0.0065002218 -0.016888974 -0.025390431 -0.030725092][-0.037792582 -0.05002204 -0.058311138 -0.064280219 -0.067502566 -0.068299368 -0.067228854 -0.0647093 -0.061234806 -0.056999125 -0.053285062 -0.050205912 -0.047634609 -0.045214869 -0.042897087][-0.032287844 -0.042430036 -0.048732203 -0.053866409 -0.059162617 -0.065421633 -0.071896516 -0.077057347 -0.079770774 -0.079156116 -0.0753624 -0.069585755 -0.06279473 -0.05631087 -0.050670378]]...]
INFO - root - 2017-12-11 06:29:07.525831: step 26410, loss = 0.68, batch loss = 0.62 (14.6 examples/sec; 0.547 sec/batch; 46h:29m:46s remains)
INFO - root - 2017-12-11 06:29:12.997255: step 26420, loss = 0.70, batch loss = 0.64 (15.1 examples/sec; 0.530 sec/batch; 45h:01m:45s remains)
INFO - root - 2017-12-11 06:29:18.460929: step 26430, loss = 0.69, batch loss = 0.64 (15.0 examples/sec; 0.534 sec/batch; 45h:25m:15s remains)
INFO - root - 2017-12-11 06:29:23.983697: step 26440, loss = 0.71, batch loss = 0.65 (14.4 examples/sec; 0.557 sec/batch; 47h:22m:49s remains)
INFO - root - 2017-12-11 06:29:29.484253: step 26450, loss = 0.68, batch loss = 0.63 (15.0 examples/sec; 0.535 sec/batch; 45h:26m:36s remains)
INFO - root - 2017-12-11 06:29:35.000743: step 26460, loss = 0.70, batch loss = 0.64 (14.3 examples/sec; 0.559 sec/batch; 47h:32m:39s remains)
INFO - root - 2017-12-11 06:29:40.489386: step 26470, loss = 0.70, batch loss = 0.64 (14.7 examples/sec; 0.543 sec/batch; 46h:08m:30s remains)
INFO - root - 2017-12-11 06:29:46.009529: step 26480, loss = 0.69, batch loss = 0.64 (14.4 examples/sec; 0.556 sec/batch; 47h:16m:11s remains)
INFO - root - 2017-12-11 06:29:51.270710: step 26490, loss = 0.69, batch loss = 0.63 (14.6 examples/sec; 0.547 sec/batch; 46h:31m:50s remains)
INFO - root - 2017-12-11 06:29:56.759990: step 26500, loss = 0.70, batch loss = 0.64 (14.4 examples/sec; 0.555 sec/batch; 47h:12m:27s remains)
2017-12-11 06:29:57.393849: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.014074922 0.0063820193 0.05773044 0.12231787 0.1794924 0.21093339 0.21294978 0.19667 0.17158827 0.13865438 0.093663022 0.041816209 0.0022709523 0.0010297127 0.04871849][-0.041492831 -0.0022577744 0.076368652 0.16696191 0.23963559 0.27295759 0.26472014 0.23325731 0.19561382 0.15478972 0.10514405 0.051570505 0.013569999 0.017109837 0.070415057][-0.035927523 0.018772608 0.11489429 0.2198838 0.29989344 0.33339515 0.322144 0.2899912 0.25639397 0.22071184 0.1720462 0.11500537 0.0712417 0.068076454 0.1117463][-0.017003251 0.0450964 0.14623246 0.25330439 0.33438292 0.37166885 0.37153956 0.35768977 0.3431282 0.31860095 0.26843354 0.2011794 0.14401157 0.12740643 0.15561955][-0.00084019476 0.05904343 0.15351555 0.25270587 0.33198351 0.37973624 0.40338954 0.41851294 0.42533833 0.40723282 0.34953853 0.26896369 0.1997374 0.17313437 0.189793][0.012600007 0.063071109 0.14150266 0.22480269 0.299075 0.35945 0.40917635 0.44994536 0.46901271 0.44912553 0.38371077 0.29756862 0.22829325 0.20266844 0.21613429][0.034820415 0.074237846 0.13247147 0.1961855 0.26228273 0.32985514 0.39505929 0.44574413 0.46240014 0.434094 0.36574697 0.28651366 0.23059754 0.21580663 0.23233326][0.08017122 0.11113326 0.14968827 0.19286518 0.24565747 0.30882764 0.37233093 0.41568026 0.41947806 0.38196167 0.31750309 0.2544896 0.21866927 0.21794264 0.23911574][0.15244623 0.1774656 0.19984162 0.2245461 0.26033437 0.3085458 0.35658211 0.38281131 0.37137994 0.32766619 0.27084512 0.22464786 0.20533544 0.21304458 0.23487152][0.23843242 0.25958422 0.27169704 0.28329152 0.30356786 0.33448428 0.36382264 0.37159419 0.34559506 0.29565239 0.24254179 0.20458122 0.19020067 0.19546874 0.21105263][0.30563214 0.32472891 0.33284822 0.34025049 0.35413849 0.37581041 0.39335322 0.38776228 0.34937811 0.29075307 0.23390995 0.19403271 0.174571 0.17052083 0.17652982][0.32732326 0.34471479 0.356032 0.37032589 0.39090908 0.41538814 0.42964315 0.41524014 0.3658382 0.29660079 0.23140301 0.18506603 0.15902895 0.14781733 0.14743793][0.30002275 0.31675908 0.33451381 0.35886014 0.38924062 0.41943666 0.43336427 0.41469949 0.36071736 0.28806812 0.22148512 0.17634313 0.15332194 0.14522468 0.1456608][0.2421528 0.26089758 0.28292936 0.31028435 0.34107524 0.36899373 0.38004634 0.36196053 0.31415725 0.25189087 0.19799031 0.16748393 0.15995541 0.16531579 0.17272146][0.17977528 0.20208438 0.22295618 0.24172263 0.25911382 0.27396759 0.27842763 0.26469919 0.23225364 0.19230568 0.16265479 0.15575801 0.16932675 0.19075619 0.20597835]]...]
INFO - root - 2017-12-11 06:30:02.941187: step 26510, loss = 0.71, batch loss = 0.65 (14.2 examples/sec; 0.562 sec/batch; 47h:45m:50s remains)
INFO - root - 2017-12-11 06:30:08.376351: step 26520, loss = 0.71, batch loss = 0.65 (14.0 examples/sec; 0.569 sec/batch; 48h:23m:52s remains)
INFO - root - 2017-12-11 06:30:13.808494: step 26530, loss = 0.70, batch loss = 0.65 (15.1 examples/sec; 0.531 sec/batch; 45h:08m:02s remains)
INFO - root - 2017-12-11 06:30:19.345378: step 26540, loss = 0.71, batch loss = 0.65 (14.5 examples/sec; 0.551 sec/batch; 46h:47m:58s remains)
INFO - root - 2017-12-11 06:30:24.866709: step 26550, loss = 0.71, batch loss = 0.65 (14.7 examples/sec; 0.544 sec/batch; 46h:15m:24s remains)
INFO - root - 2017-12-11 06:30:30.381778: step 26560, loss = 0.68, batch loss = 0.62 (14.4 examples/sec; 0.554 sec/batch; 47h:03m:50s remains)
INFO - root - 2017-12-11 06:30:35.927775: step 26570, loss = 0.70, batch loss = 0.64 (14.4 examples/sec; 0.556 sec/batch; 47h:13m:44s remains)
INFO - root - 2017-12-11 06:30:41.150371: step 26580, loss = 0.71, batch loss = 0.66 (14.6 examples/sec; 0.548 sec/batch; 46h:35m:45s remains)
INFO - root - 2017-12-11 06:30:46.663151: step 26590, loss = 0.69, batch loss = 0.63 (14.4 examples/sec; 0.556 sec/batch; 47h:12m:55s remains)
INFO - root - 2017-12-11 06:30:52.134096: step 26600, loss = 0.70, batch loss = 0.64 (14.6 examples/sec; 0.546 sec/batch; 46h:24m:31s remains)
2017-12-11 06:30:52.726401: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.05889903 0.051105503 0.039120119 0.02789643 0.023121221 0.028953495 0.045945209 0.068612263 0.090008721 0.10395274 0.10697818 0.099168077 0.083703563 0.070578471 0.07728681][0.044298463 0.047932677 0.047892116 0.044998664 0.043165971 0.046897952 0.05998328 0.079566725 0.099546723 0.11357719 0.11778504 0.11126508 0.095176063 0.078697436 0.081164867][0.042654105 0.065295748 0.086431086 0.10132802 0.11052332 0.11734767 0.12727565 0.14039963 0.15339872 0.16191782 0.16212894 0.15187451 0.13044828 0.10621493 0.098939076][0.0610764 0.11042275 0.16174079 0.20385525 0.23279487 0.24928655 0.25841105 0.26297396 0.26443264 0.26289463 0.254895 0.2366783 0.20591873 0.17156604 0.1514951][0.08813341 0.16703311 0.25338677 0.3291913 0.38507834 0.41806367 0.43069631 0.42764241 0.41636372 0.40392449 0.387755 0.36217439 0.32287613 0.2810362 0.24963714][0.11316624 0.2169174 0.33360738 0.43930775 0.51980084 0.56909466 0.58626664 0.5771718 0.55582863 0.53730971 0.51946723 0.49303094 0.4509739 0.40847591 0.37195498][0.13330999 0.24777693 0.37859544 0.49871626 0.59068018 0.64728945 0.66564983 0.65222234 0.62619764 0.60909891 0.59825289 0.5794456 0.54218984 0.50590658 0.47185588][0.14353326 0.24595658 0.36578882 0.47768429 0.56251782 0.61325765 0.62723452 0.61123914 0.58642471 0.57689416 0.57867664 0.57334942 0.54761785 0.5232079 0.49853241][0.14407414 0.20972697 0.29282972 0.37537959 0.43771175 0.47286075 0.47975141 0.46474379 0.44635409 0.44657198 0.46046162 0.46805632 0.45530057 0.44328141 0.43059415][0.14006156 0.15581506 0.18888298 0.23166099 0.26529482 0.28283295 0.28448179 0.27417922 0.26431394 0.27176058 0.29185712 0.30643606 0.3029249 0.29915074 0.29654083][0.1340729 0.10529795 0.0929449 0.099548467 0.10860951 0.11295339 0.11260497 0.10800398 0.10446744 0.11290381 0.13053162 0.14411078 0.14421184 0.14349261 0.14737763][0.11315531 0.059849545 0.022900613 0.010358803 0.0082370294 0.0083881784 0.0093738372 0.0089258747 0.0071189082 0.010189889 0.018179884 0.024335634 0.023312695 0.022546479 0.030374002][0.0695646 0.016039107 -0.020801907 -0.033209581 -0.032926533 -0.029026791 -0.024878355 -0.023684472 -0.027170578 -0.031311154 -0.033633258 -0.035478707 -0.038452044 -0.038592007 -0.026874116][0.01815919 -0.018271189 -0.037839279 -0.038013428 -0.028806204 -0.018995877 -0.012805764 -0.012724847 -0.019473271 -0.029186597 -0.03782795 -0.043807957 -0.046272531 -0.042925768 -0.025914835][-0.022235774 -0.035041526 -0.032737482 -0.019321656 -0.0030641491 0.00936465 0.014420452 0.011202085 0.0012015755 -0.011161094 -0.021349182 -0.026722264 -0.025272749 -0.015730174 0.0073591247]]...]
INFO - root - 2017-12-11 06:30:58.190851: step 26610, loss = 0.70, batch loss = 0.64 (14.6 examples/sec; 0.547 sec/batch; 46h:27m:12s remains)
INFO - root - 2017-12-11 06:31:03.734199: step 26620, loss = 0.69, batch loss = 0.63 (13.1 examples/sec; 0.608 sec/batch; 51h:41m:46s remains)
INFO - root - 2017-12-11 06:31:09.349906: step 26630, loss = 0.69, batch loss = 0.63 (14.3 examples/sec; 0.560 sec/batch; 47h:35m:24s remains)
INFO - root - 2017-12-11 06:31:14.830695: step 26640, loss = 0.69, batch loss = 0.63 (14.8 examples/sec; 0.540 sec/batch; 45h:54m:22s remains)
INFO - root - 2017-12-11 06:31:20.425600: step 26650, loss = 0.69, batch loss = 0.63 (14.4 examples/sec; 0.557 sec/batch; 47h:19m:40s remains)
INFO - root - 2017-12-11 06:31:25.935027: step 26660, loss = 0.70, batch loss = 0.64 (14.7 examples/sec; 0.545 sec/batch; 46h:16m:24s remains)
INFO - root - 2017-12-11 06:31:31.408242: step 26670, loss = 0.71, batch loss = 0.65 (14.2 examples/sec; 0.562 sec/batch; 47h:46m:15s remains)
INFO - root - 2017-12-11 06:31:36.651480: step 26680, loss = 0.69, batch loss = 0.63 (13.9 examples/sec; 0.574 sec/batch; 48h:46m:42s remains)
INFO - root - 2017-12-11 06:31:42.175262: step 26690, loss = 0.69, batch loss = 0.63 (14.7 examples/sec; 0.544 sec/batch; 46h:13m:53s remains)
INFO - root - 2017-12-11 06:31:47.723517: step 26700, loss = 0.69, batch loss = 0.64 (14.4 examples/sec; 0.555 sec/batch; 47h:08m:46s remains)
2017-12-11 06:31:48.304735: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.037448756 -0.040190097 -0.033059061 -0.017565316 0.0019493466 0.018172603 0.024393747 0.015633676 -0.0078760339 -0.038140289 -0.065895021 -0.084192954 -0.0925314 -0.0921719 -0.08572869][-0.02177768 -0.022821644 -0.0072538704 0.023032129 0.060525116 0.09333387 0.1087813 0.097526059 0.060328167 0.0099112187 -0.039229859 -0.075087346 -0.095097966 -0.1007042 -0.094995812][0.0015767785 0.0028925973 0.028866207 0.0785309 0.14068171 0.19703443 0.22640668 0.2131031 0.159336 0.084095933 0.0077090431 -0.051771365 -0.0884609 -0.10358781 -0.10117169][0.032157566 0.033818234 0.068304382 0.13851489 0.22896656 0.31333748 0.36047769 0.34698138 0.27631265 0.17437372 0.067701466 -0.019114418 -0.075688921 -0.10255013 -0.10487199][0.079708561 0.074951395 0.10970879 0.19533943 0.31282803 0.42626992 0.49348512 0.48230824 0.39657333 0.268398 0.13096601 0.015752545 -0.061741069 -0.1006979 -0.10798377][0.16269706 0.1389944 0.15757935 0.24390355 0.3789008 0.51755035 0.60671449 0.60418779 0.51010591 0.361239 0.19661817 0.0538255 -0.045455776 -0.097228684 -0.10990664][0.27470925 0.21807826 0.20034504 0.26814193 0.40779549 0.56766492 0.68410176 0.70276845 0.61407167 0.45494556 0.26924843 0.1007489 -0.02167177 -0.088686496 -0.10882857][0.3926667 0.29563174 0.22958417 0.26444721 0.39651781 0.5720033 0.71877754 0.76985693 0.70086586 0.54419827 0.34574375 0.1544119 0.0076274341 -0.076356687 -0.1050734][0.49777234 0.36501366 0.24978344 0.24387731 0.35659084 0.53691715 0.70812225 0.79343218 0.75318873 0.61124021 0.41086012 0.2037981 0.036312386 -0.06341745 -0.10043735][0.5708136 0.41621119 0.26278946 0.22003147 0.31014469 0.48705462 0.67271876 0.78583682 0.77585942 0.65624678 0.46263215 0.246725 0.063468896 -0.049457766 -0.094133563][0.59471053 0.43532741 0.26452008 0.19901104 0.27037448 0.43837422 0.62699246 0.75614065 0.77111655 0.67491037 0.49245507 0.27382019 0.0818212 -0.038746707 -0.0881608][0.55334795 0.40902016 0.24543382 0.175288 0.23414737 0.38906717 0.56983536 0.70216578 0.73310244 0.65649217 0.48597732 0.27030987 0.07885801 -0.040051073 -0.087912358][0.45322767 0.33746564 0.20188794 0.14220707 0.19272779 0.32901955 0.49051315 0.61298907 0.64986724 0.58853322 0.43247488 0.22849284 0.04942492 -0.057161234 -0.096097462][0.32151008 0.23823808 0.13767116 0.092653766 0.13326536 0.24340086 0.37517285 0.47784218 0.51374489 0.46680653 0.33217829 0.15284114 -0.00056912994 -0.085334919 -0.11000308][0.18255723 0.12722917 0.058726609 0.027174886 0.0575115 0.13962738 0.23803806 0.31630173 0.34686995 0.31372875 0.20688407 0.06268511 -0.056464709 -0.11540323 -0.12470195]]...]
INFO - root - 2017-12-11 06:31:53.873745: step 26710, loss = 0.70, batch loss = 0.64 (14.9 examples/sec; 0.536 sec/batch; 45h:33m:29s remains)
INFO - root - 2017-12-11 06:31:59.377900: step 26720, loss = 0.70, batch loss = 0.64 (14.5 examples/sec; 0.551 sec/batch; 46h:49m:43s remains)
INFO - root - 2017-12-11 06:32:04.905269: step 26730, loss = 0.69, batch loss = 0.63 (14.4 examples/sec; 0.557 sec/batch; 47h:18m:35s remains)
INFO - root - 2017-12-11 06:32:10.484231: step 26740, loss = 0.70, batch loss = 0.64 (14.4 examples/sec; 0.557 sec/batch; 47h:16m:35s remains)
INFO - root - 2017-12-11 06:32:15.954682: step 26750, loss = 0.70, batch loss = 0.64 (14.9 examples/sec; 0.538 sec/batch; 45h:39m:05s remains)
INFO - root - 2017-12-11 06:32:21.425203: step 26760, loss = 0.70, batch loss = 0.64 (14.4 examples/sec; 0.555 sec/batch; 47h:09m:22s remains)
INFO - root - 2017-12-11 06:32:27.008452: step 26770, loss = 0.68, batch loss = 0.62 (14.4 examples/sec; 0.557 sec/batch; 47h:19m:26s remains)
INFO - root - 2017-12-11 06:32:32.078093: step 26780, loss = 0.70, batch loss = 0.64 (15.2 examples/sec; 0.527 sec/batch; 44h:44m:01s remains)
INFO - root - 2017-12-11 06:32:37.702114: step 26790, loss = 0.69, batch loss = 0.63 (13.9 examples/sec; 0.576 sec/batch; 48h:54m:36s remains)
INFO - root - 2017-12-11 06:32:43.222717: step 26800, loss = 0.71, batch loss = 0.65 (14.4 examples/sec; 0.556 sec/batch; 47h:12m:22s remains)
2017-12-11 06:32:43.819369: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0027259064 0.03616555 0.091756627 0.15646844 0.21482185 0.2568427 0.27723747 0.27538812 0.25311083 0.22631346 0.21337998 0.22576714 0.2659491 0.32430625 0.38771877][-0.00232811 0.038358059 0.097687386 0.1689924 0.23780161 0.29310164 0.32887965 0.34258786 0.33147526 0.30928218 0.2952798 0.30400893 0.33678591 0.38372383 0.43469459][-0.001353302 0.040981568 0.10469902 0.1843269 0.26652595 0.33806795 0.39161077 0.42208269 0.42198819 0.40137336 0.37910128 0.37454951 0.39050028 0.41976973 0.45571083][0.00068869023 0.046009783 0.11602685 0.20565607 0.30219433 0.38976002 0.46028605 0.50644386 0.51658446 0.49550819 0.4597286 0.43386921 0.42586485 0.4343209 0.45502967][0.0015422517 0.049884897 0.1262925 0.22583732 0.33667228 0.44045225 0.5273909 0.58533084 0.59901333 0.57026869 0.51584262 0.46679977 0.43755153 0.43169659 0.44302896][0.00067225652 0.052647874 0.13680167 0.24898848 0.37841645 0.50320113 0.60791135 0.67009282 0.67129374 0.61771381 0.53456616 0.46174356 0.418243 0.40801325 0.41756502][0.0012306061 0.059353374 0.1558117 0.28695983 0.44181794 0.59293532 0.71457642 0.77097231 0.74222958 0.645985 0.52225626 0.42107657 0.364221 0.3513802 0.36031529][0.0018121949 0.066044278 0.17579326 0.32699716 0.50545293 0.67705423 0.80528337 0.8443113 0.77635586 0.63422316 0.47353938 0.35068232 0.286022 0.27232566 0.28118655][0.0017239075 0.0699886 0.19083643 0.35762784 0.54913068 0.72488946 0.8417508 0.85292768 0.74858916 0.57436842 0.39429954 0.26312482 0.1973013 0.18488863 0.19674778][0.0011698303 0.071144722 0.19852847 0.37234053 0.56250918 0.72423917 0.81395596 0.79319358 0.6629163 0.47568026 0.29559964 0.17046906 0.11148107 0.10426497 0.12279744][-0.00015755463 0.067913473 0.19333437 0.36035424 0.53228581 0.66513783 0.72112668 0.67524213 0.53709757 0.35859811 0.19790226 0.094002634 0.052262317 0.056298893 0.084130943][-0.004266724 0.056988787 0.17027976 0.31660274 0.4582729 0.55793643 0.58712196 0.53316963 0.40861809 0.25878671 0.13289575 0.060502492 0.041369241 0.057434291 0.0917104][-0.012080872 0.038543995 0.13277289 0.25142303 0.361269 0.4345423 0.45237777 0.40889546 0.31376669 0.20360716 0.11842658 0.0785116 0.078460574 0.10116428 0.13677996][-0.019549897 0.021536805 0.09897919 0.19637495 0.2874811 0.35232362 0.37765914 0.35861182 0.29832974 0.2256255 0.17363064 0.1561044 0.16490738 0.18558496 0.21672331][-0.023519974 0.012269105 0.081674635 0.17262341 0.26439202 0.33995834 0.38618365 0.39555612 0.36369014 0.31432924 0.278243 0.26750946 0.27419159 0.28553379 0.30684158]]...]
INFO - root - 2017-12-11 06:32:49.368423: step 26810, loss = 0.68, batch loss = 0.62 (14.6 examples/sec; 0.547 sec/batch; 46h:28m:31s remains)
INFO - root - 2017-12-11 06:32:54.812452: step 26820, loss = 0.69, batch loss = 0.63 (15.0 examples/sec; 0.532 sec/batch; 45h:11m:27s remains)
INFO - root - 2017-12-11 06:33:00.380668: step 26830, loss = 0.70, batch loss = 0.64 (14.5 examples/sec; 0.550 sec/batch; 46h:44m:25s remains)
INFO - root - 2017-12-11 06:33:05.840071: step 26840, loss = 0.69, batch loss = 0.63 (14.8 examples/sec; 0.539 sec/batch; 45h:46m:29s remains)
INFO - root - 2017-12-11 06:33:11.418279: step 26850, loss = 0.69, batch loss = 0.63 (14.0 examples/sec; 0.572 sec/batch; 48h:33m:07s remains)
INFO - root - 2017-12-11 06:33:16.944339: step 26860, loss = 0.69, batch loss = 0.63 (14.9 examples/sec; 0.536 sec/batch; 45h:29m:31s remains)
INFO - root - 2017-12-11 06:33:22.075987: step 26870, loss = 0.70, batch loss = 0.64 (15.0 examples/sec; 0.532 sec/batch; 45h:07m:49s remains)
INFO - root - 2017-12-11 06:33:27.671251: step 26880, loss = 0.69, batch loss = 0.63 (14.3 examples/sec; 0.560 sec/batch; 47h:30m:28s remains)
INFO - root - 2017-12-11 06:33:33.148905: step 26890, loss = 0.70, batch loss = 0.64 (14.7 examples/sec; 0.546 sec/batch; 46h:19m:28s remains)
INFO - root - 2017-12-11 06:33:38.627662: step 26900, loss = 0.69, batch loss = 0.63 (14.2 examples/sec; 0.561 sec/batch; 47h:39m:48s remains)
2017-12-11 06:33:39.237404: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.019812888 -0.020278798 -0.019392751 -0.017002502 -0.013604268 -0.010400424 -0.00808275 -0.0076996707 -0.009638425 -0.0125298 -0.015280371 -0.017434571 -0.018118808 -0.016672887 -0.01359075][-0.012138722 -0.013052362 -0.011655877 -0.0076970886 -0.0021136298 0.0027976106 0.0054976428 0.0043278374 -0.001098506 -0.0083878087 -0.015636381 -0.021542719 -0.024276119 -0.022740506 -0.017953111][0.01667795 0.017466169 0.0222565 0.031519752 0.043278322 0.052653927 0.056226548 0.05209567 0.04044392 0.025139447 0.0095308525 -0.0028752775 -0.0090258084 -0.0077649979 -0.00084975822][0.074200742 0.07897529 0.089950666 0.10903668 0.13242517 0.15075761 0.15697309 0.1489087 0.1281565 0.10068027 0.072301418 0.0501146 0.038657662 0.038879462 0.047580805][0.15324868 0.16541478 0.18628383 0.22006828 0.26028219 0.29151478 0.3015267 0.28802767 0.25528306 0.21215187 0.16790473 0.13349195 0.11475322 0.11186884 0.11980376][0.23807636 0.25789833 0.28924873 0.33836681 0.39573085 0.44003838 0.45355424 0.43409577 0.38973057 0.33254087 0.27526292 0.23159277 0.20730506 0.20013084 0.20336415][0.29583243 0.32209265 0.3620232 0.42266163 0.4921135 0.54502016 0.56000257 0.53566223 0.48370546 0.41898832 0.35705575 0.31212232 0.28754717 0.27711788 0.27309963][0.30296022 0.3316246 0.37394673 0.4362781 0.50588381 0.55718172 0.56896681 0.54164928 0.48943636 0.42848343 0.37526926 0.34121376 0.32523745 0.31669927 0.3073945][0.25758505 0.28270033 0.31931829 0.3720279 0.42941719 0.46940273 0.47462142 0.44798884 0.40424758 0.35867772 0.32576388 0.31124648 0.3095037 0.30675098 0.29533958][0.18524775 0.20516457 0.23303539 0.27124959 0.31101796 0.33577108 0.33353561 0.30951297 0.27741116 0.25022423 0.23861256 0.24182552 0.25127685 0.25276586 0.24051374][0.12427054 0.14125548 0.16237423 0.18746266 0.21060634 0.22128367 0.21271938 0.19124573 0.16850254 0.15532656 0.15818733 0.17053957 0.18288177 0.18362211 0.17003405][0.095046 0.11503702 0.13539545 0.15348449 0.16586342 0.16761018 0.1558494 0.13652527 0.11871859 0.11136891 0.11717011 0.1258596 0.12982847 0.12305581 0.10643604][0.091506578 0.11918525 0.14418922 0.16171195 0.16977876 0.16817133 0.15637553 0.13880795 0.12191178 0.1129106 0.11166719 0.10669672 0.09434133 0.075436041 0.054280855][0.0974582 0.1312712 0.16073462 0.18008249 0.18806724 0.18738708 0.17826948 0.16307026 0.14583369 0.13191284 0.11948633 0.098043278 0.068343572 0.0369991 0.010855868][0.097339161 0.13019273 0.15817505 0.17601828 0.18309835 0.18364313 0.177924 0.16605262 0.14978923 0.13267735 0.11231254 0.080366336 0.040700074 0.0031631282 -0.024329504]]...]
INFO - root - 2017-12-11 06:33:44.647757: step 26910, loss = 0.69, batch loss = 0.63 (15.1 examples/sec; 0.528 sec/batch; 44h:49m:28s remains)
INFO - root - 2017-12-11 06:33:50.026381: step 26920, loss = 0.69, batch loss = 0.63 (15.0 examples/sec; 0.534 sec/batch; 45h:17m:25s remains)
INFO - root - 2017-12-11 06:33:55.543060: step 26930, loss = 0.70, batch loss = 0.65 (14.7 examples/sec; 0.543 sec/batch; 46h:03m:56s remains)
INFO - root - 2017-12-11 06:34:01.022339: step 26940, loss = 0.69, batch loss = 0.63 (14.6 examples/sec; 0.547 sec/batch; 46h:26m:52s remains)
INFO - root - 2017-12-11 06:34:06.523291: step 26950, loss = 0.70, batch loss = 0.64 (14.3 examples/sec; 0.559 sec/batch; 47h:26m:50s remains)
INFO - root - 2017-12-11 06:34:12.091665: step 26960, loss = 0.68, batch loss = 0.62 (14.5 examples/sec; 0.553 sec/batch; 46h:56m:47s remains)
INFO - root - 2017-12-11 06:34:17.352269: step 26970, loss = 0.70, batch loss = 0.65 (14.0 examples/sec; 0.570 sec/batch; 48h:20m:15s remains)
INFO - root - 2017-12-11 06:34:22.891801: step 26980, loss = 0.71, batch loss = 0.65 (14.1 examples/sec; 0.568 sec/batch; 48h:10m:58s remains)
INFO - root - 2017-12-11 06:34:28.328905: step 26990, loss = 0.70, batch loss = 0.64 (14.5 examples/sec; 0.553 sec/batch; 46h:54m:36s remains)
INFO - root - 2017-12-11 06:34:33.892939: step 27000, loss = 0.69, batch loss = 0.63 (14.4 examples/sec; 0.557 sec/batch; 47h:13m:34s remains)
2017-12-11 06:34:34.437486: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.40635842 0.35180318 0.29970255 0.27073359 0.27138004 0.29615265 0.33438221 0.36643788 0.368469 0.33731663 0.28154495 0.20281672 0.10908023 0.022420136 -0.035377335][0.39272398 0.33647785 0.29113674 0.27556208 0.28930068 0.32692519 0.374124 0.41013184 0.40967903 0.37191913 0.30867353 0.22308378 0.12349743 0.032354113 -0.028075304][0.37079117 0.31074685 0.27161208 0.27026519 0.29923454 0.35197785 0.41108778 0.45320088 0.45298904 0.41061917 0.33974183 0.24553859 0.13842985 0.041649509 -0.022124559][0.34154648 0.2832154 0.25766823 0.27772725 0.32889205 0.40367591 0.48131624 0.532985 0.53222841 0.48084444 0.39496621 0.28299642 0.15926537 0.049373645 -0.02122275][0.30574006 0.26550233 0.26918185 0.32268494 0.40485823 0.5078181 0.60698062 0.66650832 0.65805417 0.5874204 0.47542909 0.33467194 0.18478593 0.055577021 -0.024531556][0.28239724 0.27357069 0.31459686 0.40293941 0.51374829 0.6399318 0.75444704 0.8146401 0.78981519 0.69230127 0.549407 0.37801972 0.20231263 0.056457628 -0.030347703][0.27650195 0.30199525 0.37491983 0.4847559 0.60738587 0.73851395 0.85310674 0.90436625 0.85974175 0.73822 0.57335687 0.38423792 0.19632719 0.046045061 -0.039530337][0.26784992 0.31714723 0.40444425 0.51400113 0.62541193 0.73876661 0.83519328 0.86943239 0.80926681 0.67812943 0.513135 0.33122441 0.1545231 0.017882889 -0.055994205][0.2229182 0.27906254 0.36088148 0.45141453 0.53517962 0.61639237 0.68445909 0.70136029 0.63839406 0.51978081 0.38038588 0.23080744 0.08662115 -0.021766268 -0.076577462][0.13536389 0.1813834 0.24292311 0.30518514 0.35731533 0.40525734 0.44613746 0.45248219 0.40215558 0.31581631 0.22027422 0.11776329 0.017200997 -0.057086833 -0.091823891][0.033002764 0.057254512 0.090515323 0.12233158 0.14574997 0.16663739 0.18813331 0.1934814 0.16767624 0.12476555 0.0798474 0.027082684 -0.029834278 -0.073056519 -0.092175342][-0.049262375 -0.045587875 -0.03694294 -0.029515913 -0.027157519 -0.024074038 -0.013264305 -0.0025340558 -0.00095413881 -0.0022051211 -0.0031364511 -0.015884828 -0.039838377 -0.062464479 -0.074659087][-0.096727677 -0.10465342 -0.10943274 -0.11459223 -0.12204226 -0.12638332 -0.11907386 -0.1016997 -0.078840055 -0.050411332 -0.022512417 -0.011204251 -0.017369155 -0.031806063 -0.045637839][-0.11023571 -0.12157779 -0.13013954 -0.1383968 -0.14713356 -0.1514502 -0.14314136 -0.12078774 -0.085853904 -0.041824549 -0.00020635796 0.021024233 0.020210251 0.0050196708 -0.015226607][-0.098967187 -0.10944529 -0.11725318 -0.12444976 -0.13117538 -0.13321833 -0.12337593 -0.09902405 -0.059555881 -0.0098393867 0.036106877 0.059305597 0.057275306 0.037349936 0.0097404905]]...]
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-SGD-lr0.001-clip50-initfromfixqian30k-fixqian
INFO - root - 2017-12-11 06:34:40.014628: step 27010, loss = 0.70, batch loss = 0.65 (14.6 examples/sec; 0.550 sec/batch; 46h:38m:31s remains)
INFO - root - 2017-12-11 06:34:45.554240: step 27020, loss = 0.69, batch loss = 0.63 (14.6 examples/sec; 0.548 sec/batch; 46h:30m:32s remains)
INFO - root - 2017-12-11 06:34:51.050822: step 27030, loss = 0.68, batch loss = 0.63 (14.2 examples/sec; 0.562 sec/batch; 47h:40m:59s remains)
INFO - root - 2017-12-11 06:34:56.620858: step 27040, loss = 0.71, batch loss = 0.65 (14.7 examples/sec; 0.545 sec/batch; 46h:13m:23s remains)
INFO - root - 2017-12-11 06:35:02.164661: step 27050, loss = 0.68, batch loss = 0.62 (14.7 examples/sec; 0.545 sec/batch; 46h:16m:28s remains)
INFO - root - 2017-12-11 06:35:07.649936: step 27060, loss = 0.70, batch loss = 0.65 (14.9 examples/sec; 0.538 sec/batch; 45h:36m:25s remains)
INFO - root - 2017-12-11 06:35:12.804192: step 27070, loss = 0.68, batch loss = 0.63 (14.4 examples/sec; 0.557 sec/batch; 47h:14m:49s remains)
INFO - root - 2017-12-11 06:35:18.355824: step 27080, loss = 0.72, batch loss = 0.66 (14.6 examples/sec; 0.547 sec/batch; 46h:22m:56s remains)
INFO - root - 2017-12-11 06:35:23.829977: step 27090, loss = 0.70, batch loss = 0.64 (14.7 examples/sec; 0.546 sec/batch; 46h:17m:22s remains)
INFO - root - 2017-12-11 06:35:29.389614: step 27100, loss = 0.69, batch loss = 0.64 (14.2 examples/sec; 0.563 sec/batch; 47h:46m:13s remains)
2017-12-11 06:35:29.936726: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.047735851 -0.049205936 -0.048284981 -0.046347186 -0.044758562 -0.044146549 -0.043892067 -0.043450661 -0.043015525 -0.043043956 -0.043156456 -0.042847093 -0.042158388 -0.041649889 -0.041504923][-0.032818194 -0.035176586 -0.032801468 -0.027008178 -0.020695962 -0.016156372 -0.01305272 -0.010813152 -0.0093179857 -0.0093387244 -0.010223317 -0.011201327 -0.011418471 -0.012138716 -0.014558354][-0.0037895625 -0.0057542729 -0.00061515713 0.011226617 0.025297957 0.036911473 0.045960192 0.052013617 0.055047415 0.05451408 0.051420797 0.046979073 0.042219177 0.036303032 0.027880054][0.031999644 0.031729218 0.041098528 0.061324932 0.086246029 0.10849946 0.12722807 0.1392616 0.14356503 0.14032517 0.13238148 0.12184886 0.10851852 0.092794985 0.074623957][0.072586991 0.073358461 0.086499773 0.1155059 0.15245952 0.18708082 0.21700144 0.23519316 0.23923327 0.23096392 0.21551201 0.19640973 0.17227617 0.14525592 0.11685996][0.11067797 0.11170983 0.12780054 0.16413783 0.21134447 0.2564505 0.29491708 0.31645268 0.31782052 0.30271298 0.2782771 0.2499918 0.21626496 0.18049526 0.14539154][0.14342323 0.14556319 0.16286014 0.20287715 0.25529447 0.30494341 0.34516525 0.36430833 0.3591142 0.33500996 0.30112776 0.26523814 0.2260581 0.18770008 0.15293242][0.1625995 0.16691099 0.18364863 0.2231922 0.27497828 0.32224071 0.35641187 0.3670404 0.35215256 0.31852368 0.27664304 0.23628956 0.1968455 0.16218922 0.13501377][0.1732934 0.18020448 0.19576591 0.23152189 0.276821 0.31476668 0.33624268 0.33414817 0.30816004 0.26598012 0.21931526 0.17953339 0.14666621 0.12291525 0.10918683][0.17421064 0.18565199 0.2012642 0.2329053 0.27003896 0.29655954 0.30361485 0.28855896 0.25267303 0.20390004 0.15470427 0.11841623 0.096280091 0.0872874 0.088939711][0.16067803 0.17852509 0.19729221 0.22753292 0.25902236 0.27689058 0.27328461 0.24980123 0.20924468 0.15814619 0.10943593 0.078403838 0.067570865 0.0713323 0.083816931][0.14273168 0.16592255 0.1885532 0.22041698 0.25074762 0.26517507 0.25743505 0.23317951 0.19563469 0.14810337 0.10344158 0.077979594 0.075061224 0.084712572 0.099372737][0.12862955 0.1518928 0.176524 0.21136707 0.24457447 0.26144353 0.25695992 0.24002619 0.21330781 0.17577927 0.13891342 0.11827698 0.11762945 0.12430706 0.13172583][0.1249329 0.14301011 0.16450119 0.19844669 0.23263301 0.25370467 0.25723878 0.2534492 0.24277414 0.21979271 0.19351006 0.17771107 0.17640065 0.17598222 0.17233698][0.13220502 0.14129075 0.15305378 0.17854634 0.20757377 0.23072343 0.24395211 0.25535154 0.26177424 0.25464627 0.23998988 0.22902614 0.22548915 0.21684656 0.202861]]...]
INFO - root - 2017-12-11 06:35:35.499269: step 27110, loss = 0.69, batch loss = 0.63 (14.1 examples/sec; 0.566 sec/batch; 47h:59m:01s remains)
INFO - root - 2017-12-11 06:35:41.054280: step 27120, loss = 0.68, batch loss = 0.63 (14.3 examples/sec; 0.559 sec/batch; 47h:24m:20s remains)
INFO - root - 2017-12-11 06:35:46.508163: step 27130, loss = 0.69, batch loss = 0.63 (14.7 examples/sec; 0.546 sec/batch; 46h:17m:11s remains)
INFO - root - 2017-12-11 06:35:51.980689: step 27140, loss = 0.70, batch loss = 0.64 (14.6 examples/sec; 0.546 sec/batch; 46h:20m:28s remains)
INFO - root - 2017-12-11 06:35:57.517162: step 27150, loss = 0.68, batch loss = 0.62 (14.6 examples/sec; 0.549 sec/batch; 46h:34m:35s remains)
INFO - root - 2017-12-11 06:36:02.746284: step 27160, loss = 0.69, batch loss = 0.63 (27.5 examples/sec; 0.291 sec/batch; 24h:38m:30s remains)
INFO - root - 2017-12-11 06:36:08.290755: step 27170, loss = 0.70, batch loss = 0.64 (14.4 examples/sec; 0.556 sec/batch; 47h:07m:27s remains)
INFO - root - 2017-12-11 06:36:13.818910: step 27180, loss = 0.70, batch loss = 0.64 (15.0 examples/sec; 0.535 sec/batch; 45h:21m:29s remains)
INFO - root - 2017-12-11 06:36:19.272233: step 27190, loss = 0.70, batch loss = 0.64 (14.6 examples/sec; 0.548 sec/batch; 46h:26m:02s remains)
INFO - root - 2017-12-11 06:36:24.846253: step 27200, loss = 0.68, batch loss = 0.63 (14.1 examples/sec; 0.566 sec/batch; 48h:00m:50s remains)
2017-12-11 06:36:25.457584: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.26657265 0.26459131 0.26614326 0.27244002 0.2864494 0.30158857 0.31118095 0.31864759 0.32188648 0.31164393 0.28641632 0.25915664 0.24929482 0.26070356 0.28650525][0.25869605 0.26479119 0.27600265 0.29103914 0.31270772 0.33420163 0.34645543 0.35072422 0.34564242 0.326641 0.29556489 0.26691476 0.25910577 0.27559936 0.3115226][0.21650122 0.23187502 0.25481579 0.28214842 0.31538251 0.3470535 0.36586812 0.36965448 0.35631722 0.32524759 0.28356373 0.24958654 0.24075806 0.26013348 0.30384186][0.15269993 0.17662685 0.21202366 0.25422776 0.30120173 0.34429389 0.37146711 0.37682411 0.35646105 0.31333151 0.261007 0.22257629 0.21456715 0.23882984 0.28988764][0.0892847 0.11779313 0.16319478 0.22055641 0.28258446 0.3371374 0.37179291 0.37815639 0.35116109 0.29814467 0.24000233 0.20414287 0.20426495 0.24082762 0.30500618][0.049501255 0.080282226 0.13186927 0.20124118 0.27712715 0.34302491 0.38481814 0.39179996 0.35875654 0.2982069 0.23766512 0.20734678 0.21795517 0.26861843 0.34770334][0.028779207 0.059025485 0.11245321 0.18889055 0.27600497 0.354082 0.40580025 0.417014 0.38188878 0.31763178 0.25645742 0.22978155 0.24684669 0.30667707 0.39495528][0.021099854 0.047549173 0.097374737 0.17331171 0.26466981 0.35086134 0.41117671 0.42833295 0.39543763 0.33199555 0.27208218 0.24621382 0.26395956 0.32449302 0.41208541][0.014754777 0.034030978 0.074921049 0.14172725 0.22685197 0.31097755 0.3719759 0.3916541 0.36331543 0.30609336 0.25160807 0.22690625 0.24196073 0.29604161 0.37334371][0.014956818 0.026244668 0.055729266 0.10864304 0.18008888 0.25310349 0.30641603 0.3234632 0.29903039 0.25009534 0.20347102 0.1812422 0.19275452 0.23684621 0.2990154][0.032024935 0.037739452 0.05692149 0.095031083 0.14908226 0.20526844 0.24508601 0.25509492 0.23181479 0.18960615 0.14974229 0.12870856 0.13464026 0.16620059 0.21167487][0.058345862 0.061769217 0.072794832 0.097575523 0.13440789 0.17275181 0.19796938 0.20058821 0.1784173 0.14292313 0.11028399 0.091868237 0.093793012 0.11429674 0.14481424][0.097410977 0.10002854 0.10489725 0.11821058 0.13906118 0.16023542 0.17175941 0.16816416 0.1484701 0.1210518 0.0974286 0.084457509 0.085565552 0.09839952 0.11711416][0.12602529 0.12900627 0.12976594 0.13455318 0.14288196 0.15050319 0.15188189 0.14471516 0.12886144 0.10976426 0.0947116 0.087222628 0.088497393 0.096036963 0.10608029][0.1293962 0.13294242 0.13222288 0.13262965 0.13365151 0.13313778 0.12877381 0.12030838 0.10818677 0.095737249 0.08716961 0.083848126 0.085528992 0.08995644 0.09486904]]...]
INFO - root - 2017-12-11 06:36:30.985718: step 27210, loss = 0.69, batch loss = 0.63 (13.8 examples/sec; 0.580 sec/batch; 49h:11m:39s remains)
INFO - root - 2017-12-11 06:36:36.412336: step 27220, loss = 0.71, batch loss = 0.65 (14.7 examples/sec; 0.546 sec/batch; 46h:17m:30s remains)
INFO - root - 2017-12-11 06:36:41.920249: step 27230, loss = 0.69, batch loss = 0.63 (14.3 examples/sec; 0.558 sec/batch; 47h:19m:54s remains)
INFO - root - 2017-12-11 06:36:47.473541: step 27240, loss = 0.72, batch loss = 0.66 (14.1 examples/sec; 0.566 sec/batch; 47h:59m:04s remains)
INFO - root - 2017-12-11 06:36:53.015437: step 27250, loss = 0.70, batch loss = 0.64 (14.7 examples/sec; 0.545 sec/batch; 46h:13m:55s remains)
INFO - root - 2017-12-11 06:36:58.211755: step 27260, loss = 0.70, batch loss = 0.64 (14.5 examples/sec; 0.552 sec/batch; 46h:47m:51s remains)
INFO - root - 2017-12-11 06:37:03.687756: step 27270, loss = 0.68, batch loss = 0.62 (14.7 examples/sec; 0.543 sec/batch; 46h:00m:09s remains)
INFO - root - 2017-12-11 06:37:09.151830: step 27280, loss = 0.70, batch loss = 0.64 (14.3 examples/sec; 0.558 sec/batch; 47h:19m:23s remains)
INFO - root - 2017-12-11 06:37:14.700980: step 27290, loss = 0.68, batch loss = 0.62 (14.4 examples/sec; 0.555 sec/batch; 47h:04m:33s remains)
INFO - root - 2017-12-11 06:37:20.231886: step 27300, loss = 0.70, batch loss = 0.65 (14.7 examples/sec; 0.546 sec/batch; 46h:15m:42s remains)
2017-12-11 06:37:20.904421: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.14309086 0.17830727 0.20289704 0.21497624 0.22253402 0.22373551 0.20869562 0.17685412 0.12839819 0.077981718 0.024335908 -0.030897252 -0.075615495 -0.10074005 -0.10884644][0.13371354 0.14656201 0.14436488 0.12901486 0.11331568 0.10195002 0.086321995 0.062505096 0.029769197 -0.0015080948 -0.03483982 -0.073246717 -0.10494845 -0.1202908 -0.12167889][0.13160442 0.12167245 0.09486144 0.057686836 0.02722873 0.013369023 0.0069427476 0.00027970172 -0.011361277 -0.023206405 -0.041765571 -0.072504431 -0.10196645 -0.11778141 -0.11979853][0.15842703 0.13298669 0.091865852 0.046162304 0.015244262 0.011870425 0.022512168 0.033930272 0.036756624 0.032835849 0.014809526 -0.023700941 -0.065823518 -0.094385467 -0.10535868][0.20996973 0.18147013 0.14017619 0.10086285 0.08269763 0.0988196 0.1288676 0.1530038 0.15853399 0.14767575 0.11573116 0.056788761 -0.008791279 -0.058526982 -0.084262855][0.26282269 0.24245897 0.21387343 0.19306636 0.19627821 0.23488764 0.28226978 0.31225958 0.30956623 0.27937251 0.2230517 0.13748865 0.045314323 -0.026804049 -0.067204535][0.29259425 0.2867004 0.27907509 0.28366014 0.31159371 0.37120116 0.43206874 0.46252105 0.44600582 0.39200619 0.30852798 0.19736938 0.081929967 -0.0081699528 -0.059283238][0.292378 0.30119574 0.31613275 0.34500206 0.39228356 0.46444866 0.53095365 0.55628866 0.52484012 0.45044705 0.34669375 0.21979347 0.091842748 -0.0064169774 -0.061556514][0.26626763 0.28804451 0.3232376 0.3706997 0.42755932 0.4994674 0.55963421 0.57293844 0.52746862 0.44025108 0.32828933 0.20008713 0.074445 -0.019608682 -0.0713172][0.22367896 0.2568295 0.30977535 0.37166223 0.43221667 0.49585769 0.54161453 0.53912562 0.48196489 0.38898405 0.27786121 0.15816617 0.045145135 -0.03683906 -0.080886766][0.16870345 0.20916156 0.27444306 0.34679165 0.41023993 0.46675941 0.50015354 0.486642 0.42463943 0.333121 0.22807348 0.11975019 0.021369936 -0.048008226 -0.085056357][0.10307246 0.14483048 0.21501662 0.29322207 0.36124822 0.41781452 0.44813585 0.43387008 0.37644204 0.29301947 0.19669792 0.098640755 0.01196869 -0.048760165 -0.082773529][0.045828082 0.085804388 0.15508607 0.23391631 0.30470112 0.36272278 0.39326403 0.38287672 0.33448306 0.26210445 0.17586458 0.087488867 0.010538456 -0.04419037 -0.077285439][0.013306324 0.054363336 0.1211722 0.19691518 0.26614144 0.32132462 0.348967 0.34073219 0.29995844 0.23725536 0.1605283 0.081407733 0.013046624 -0.037099518 -0.070039496][0.013104107 0.059907522 0.12452751 0.19312882 0.25348642 0.29776534 0.31627485 0.3061448 0.26994494 0.21466053 0.14672144 0.077161148 0.017148968 -0.029151643 -0.062442381]]...]
INFO - root - 2017-12-11 06:37:26.369161: step 27310, loss = 0.69, batch loss = 0.63 (14.8 examples/sec; 0.541 sec/batch; 45h:54m:12s remains)
INFO - root - 2017-12-11 06:37:31.867529: step 27320, loss = 0.69, batch loss = 0.63 (14.7 examples/sec; 0.546 sec/batch; 46h:17m:18s remains)
INFO - root - 2017-12-11 06:37:37.415916: step 27330, loss = 0.70, batch loss = 0.64 (13.9 examples/sec; 0.574 sec/batch; 48h:39m:13s remains)
INFO - root - 2017-12-11 06:37:42.903424: step 27340, loss = 0.69, batch loss = 0.63 (14.6 examples/sec; 0.549 sec/batch; 46h:32m:22s remains)
INFO - root - 2017-12-11 06:37:48.370010: step 27350, loss = 0.69, batch loss = 0.63 (14.8 examples/sec; 0.542 sec/batch; 45h:55m:02s remains)
INFO - root - 2017-12-11 06:37:53.637102: step 27360, loss = 0.70, batch loss = 0.64 (14.0 examples/sec; 0.572 sec/batch; 48h:26m:51s remains)
INFO - root - 2017-12-11 06:37:59.167722: step 27370, loss = 0.68, batch loss = 0.63 (14.4 examples/sec; 0.557 sec/batch; 47h:12m:24s remains)
INFO - root - 2017-12-11 06:38:04.707340: step 27380, loss = 0.70, batch loss = 0.64 (14.8 examples/sec; 0.539 sec/batch; 45h:43m:15s remains)
INFO - root - 2017-12-11 06:38:10.229504: step 27390, loss = 0.70, batch loss = 0.64 (13.6 examples/sec; 0.588 sec/batch; 49h:47m:55s remains)
INFO - root - 2017-12-11 06:38:15.653465: step 27400, loss = 0.68, batch loss = 0.62 (14.7 examples/sec; 0.545 sec/batch; 46h:12m:13s remains)
2017-12-11 06:38:16.256188: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.17611463 0.22276874 0.27093339 0.29025266 0.26947236 0.22804609 0.19797578 0.18984795 0.18495554 0.17989671 0.18734473 0.20814277 0.22085142 0.21434876 0.20089348][0.17812964 0.22590843 0.27742356 0.30331478 0.29113734 0.25773469 0.22803533 0.21314964 0.19708797 0.17824158 0.17102511 0.175914 0.17994186 0.17586689 0.17067392][0.16786177 0.21019782 0.26084393 0.29623199 0.30223054 0.28868708 0.27047715 0.25732222 0.23547889 0.20393857 0.17937967 0.16386457 0.15649559 0.1553209 0.16049266][0.15992695 0.1945387 0.24288799 0.2895599 0.3180787 0.32953522 0.32934493 0.32412049 0.30056545 0.25713748 0.21440879 0.17863671 0.16042651 0.16235667 0.17812212][0.15478638 0.17910804 0.22262718 0.27940217 0.33053493 0.3684864 0.39032343 0.39731789 0.37525955 0.32171944 0.26220313 0.20896637 0.18130848 0.18512999 0.2095027][0.14752348 0.15936925 0.1941715 0.25664088 0.32674879 0.39057425 0.43759975 0.46090394 0.44436821 0.385524 0.31478572 0.24933854 0.21291423 0.21446982 0.24154149][0.13908072 0.13905962 0.16186246 0.22305925 0.30455825 0.38927802 0.46027359 0.50137657 0.49457 0.43734431 0.3628915 0.29125819 0.24690562 0.24156837 0.26394352][0.1331412 0.12587768 0.13787009 0.19360907 0.27799183 0.37292659 0.45816779 0.51133275 0.51416236 0.46469361 0.39527628 0.32594496 0.27863035 0.26631704 0.280237][0.1258744 0.11483857 0.12045612 0.17168641 0.25450319 0.35011658 0.43787822 0.4947446 0.50547528 0.46956813 0.41536459 0.35913438 0.31708357 0.30072084 0.30449772][0.1099473 0.095991962 0.098863132 0.14813206 0.22863221 0.32081571 0.40517545 0.46267524 0.48302615 0.46689492 0.43611282 0.40036771 0.36904967 0.35038784 0.34245402][0.091897808 0.077339821 0.081912749 0.13187332 0.21028528 0.29729462 0.3751373 0.43163204 0.46208128 0.46681613 0.45943448 0.44276723 0.42099911 0.39945936 0.38038138][0.07958255 0.069743887 0.081118964 0.1327147 0.2069294 0.2850149 0.3513011 0.40034011 0.43308583 0.45001635 0.45821676 0.45556507 0.44224891 0.42082825 0.39685091][0.077811636 0.076772578 0.096366011 0.14712743 0.21346231 0.27944386 0.33119637 0.36706686 0.39291555 0.4115015 0.42672294 0.43348911 0.4286249 0.41176471 0.38976356][0.091470249 0.099357188 0.12367015 0.16845158 0.22335714 0.27679113 0.31602737 0.33940351 0.35533169 0.36918244 0.38401276 0.39387038 0.39416718 0.38252884 0.36488613][0.11780727 0.13388963 0.15902317 0.1951603 0.23854835 0.28159982 0.31193268 0.32582289 0.33207056 0.33750063 0.34509674 0.35005891 0.34877437 0.33859518 0.32338917]]...]
INFO - root - 2017-12-11 06:38:21.796062: step 27410, loss = 0.68, batch loss = 0.62 (14.6 examples/sec; 0.550 sec/batch; 46h:35m:23s remains)
INFO - root - 2017-12-11 06:38:27.259330: step 27420, loss = 0.70, batch loss = 0.64 (14.7 examples/sec; 0.544 sec/batch; 46h:05m:37s remains)
INFO - root - 2017-12-11 06:38:32.649752: step 27430, loss = 0.69, batch loss = 0.63 (15.2 examples/sec; 0.525 sec/batch; 44h:29m:46s remains)
INFO - root - 2017-12-11 06:38:38.238110: step 27440, loss = 0.70, batch loss = 0.64 (14.0 examples/sec; 0.573 sec/batch; 48h:31m:17s remains)
INFO - root - 2017-12-11 06:38:43.731316: step 27450, loss = 0.67, batch loss = 0.61 (14.3 examples/sec; 0.558 sec/batch; 47h:17m:59s remains)
INFO - root - 2017-12-11 06:38:48.997268: step 27460, loss = 0.69, batch loss = 0.63 (13.8 examples/sec; 0.581 sec/batch; 49h:16m:18s remains)
INFO - root - 2017-12-11 06:38:54.487068: step 27470, loss = 0.69, batch loss = 0.63 (14.4 examples/sec; 0.554 sec/batch; 46h:55m:45s remains)
INFO - root - 2017-12-11 06:38:59.981466: step 27480, loss = 0.70, batch loss = 0.64 (15.2 examples/sec; 0.528 sec/batch; 44h:41m:40s remains)
INFO - root - 2017-12-11 06:39:05.414581: step 27490, loss = 0.69, batch loss = 0.63 (14.7 examples/sec; 0.544 sec/batch; 46h:06m:32s remains)
INFO - root - 2017-12-11 06:39:10.980205: step 27500, loss = 0.70, batch loss = 0.64 (14.3 examples/sec; 0.558 sec/batch; 47h:14m:45s remains)
2017-12-11 06:39:11.595425: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.17619488 0.16247953 0.14664997 0.14812024 0.1595666 0.17219386 0.1805965 0.19105114 0.20480634 0.2146821 0.21369106 0.21875487 0.23830052 0.24764906 0.22639741][0.18195267 0.15987059 0.13595577 0.13120726 0.13973249 0.15493023 0.17262448 0.19542128 0.21925822 0.2365236 0.24206595 0.24761528 0.25703293 0.25226143 0.21981817][0.18583795 0.15659882 0.12406182 0.11053564 0.11298071 0.12890044 0.155225 0.18892618 0.21964882 0.24072987 0.25155127 0.25807887 0.25806877 0.23987146 0.19858885][0.20097083 0.16916105 0.12981229 0.10587583 0.09888164 0.11159544 0.14252025 0.18257828 0.21549021 0.2357887 0.24818455 0.25531486 0.24927025 0.22223869 0.17580552][0.23259181 0.20175038 0.15746172 0.12262631 0.1039363 0.10986707 0.14101151 0.18320213 0.21516542 0.23076323 0.23926164 0.24450897 0.23560716 0.20543519 0.15748452][0.27117062 0.24425413 0.19794187 0.15451279 0.12430745 0.12039819 0.14656653 0.18654484 0.21606004 0.22586595 0.22753011 0.23010004 0.22271831 0.19604988 0.15062992][0.30413097 0.28302681 0.23748708 0.188944 0.14910471 0.13357852 0.15011883 0.18439096 0.21147269 0.21681947 0.21167015 0.21237709 0.21056755 0.19298497 0.1536518][0.31558055 0.30138475 0.2588912 0.20948867 0.16468823 0.13986646 0.14562571 0.1722165 0.19651432 0.19881463 0.1884342 0.18833414 0.19364844 0.18726228 0.15610787][0.28917998 0.28115749 0.24367341 0.19767779 0.15362349 0.12512559 0.12389408 0.14447653 0.16636182 0.16673677 0.15312162 0.1522086 0.16216616 0.16402778 0.14051379][0.22040294 0.21637534 0.18562667 0.14662306 0.10812925 0.081370488 0.07756076 0.094089635 0.11313947 0.11241066 0.098695733 0.098512746 0.11094883 0.11688142 0.098891579][0.12107781 0.1187932 0.095352389 0.064981952 0.03464786 0.013043632 0.0097368993 0.023719825 0.040169012 0.040512238 0.031023996 0.034363456 0.04876738 0.056066744 0.041869227][0.020033086 0.018181261 0.0021859552 -0.01906121 -0.040257305 -0.054836154 -0.055554833 -0.042920697 -0.028467974 -0.025684312 -0.029365592 -0.022746524 -0.0082018981 -0.0010462494 -0.011512805][-0.053291868 -0.05511903 -0.064021841 -0.076561622 -0.0893914 -0.097425632 -0.095549777 -0.0843756 -0.072311789 -0.068008505 -0.067519113 -0.060086116 -0.047817729 -0.041706111 -0.047929063][-0.089988478 -0.091716893 -0.095196657 -0.10075075 -0.10688337 -0.10994036 -0.10656172 -0.097486228 -0.088273108 -0.083918296 -0.081739619 -0.075802237 -0.067347795 -0.062798411 -0.065260015][-0.094137318 -0.095632136 -0.095837183 -0.097127527 -0.0989375 -0.098890051 -0.095083639 -0.088312 -0.081732363 -0.077818818 -0.07503514 -0.070821658 -0.065801069 -0.062909491 -0.063280188]]...]
INFO - root - 2017-12-11 06:39:17.113503: step 27510, loss = 0.71, batch loss = 0.65 (14.8 examples/sec; 0.541 sec/batch; 45h:51m:56s remains)
INFO - root - 2017-12-11 06:39:22.525160: step 27520, loss = 0.68, batch loss = 0.62 (15.2 examples/sec; 0.527 sec/batch; 44h:37m:21s remains)
INFO - root - 2017-12-11 06:39:27.947539: step 27530, loss = 0.71, batch loss = 0.65 (14.8 examples/sec; 0.539 sec/batch; 45h:40m:19s remains)
INFO - root - 2017-12-11 06:39:33.518462: step 27540, loss = 0.71, batch loss = 0.65 (14.3 examples/sec; 0.560 sec/batch; 47h:25m:05s remains)
INFO - root - 2017-12-11 06:39:38.627985: step 27550, loss = 0.69, batch loss = 0.63 (14.3 examples/sec; 0.560 sec/batch; 47h:26m:03s remains)
INFO - root - 2017-12-11 06:39:44.229506: step 27560, loss = 0.71, batch loss = 0.65 (14.5 examples/sec; 0.554 sec/batch; 46h:53m:15s remains)
INFO - root - 2017-12-11 06:39:49.712119: step 27570, loss = 0.70, batch loss = 0.64 (14.3 examples/sec; 0.558 sec/batch; 47h:13m:32s remains)
INFO - root - 2017-12-11 06:39:55.160896: step 27580, loss = 0.71, batch loss = 0.65 (14.7 examples/sec; 0.543 sec/batch; 45h:57m:45s remains)
INFO - root - 2017-12-11 06:40:00.625679: step 27590, loss = 0.70, batch loss = 0.64 (14.7 examples/sec; 0.544 sec/batch; 46h:06m:36s remains)
INFO - root - 2017-12-11 06:40:06.158459: step 27600, loss = 0.71, batch loss = 0.65 (14.1 examples/sec; 0.567 sec/batch; 48h:01m:28s remains)
2017-12-11 06:40:06.734251: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.1288736 0.10441066 0.082747959 0.08020106 0.096372835 0.12110928 0.14146331 0.15627928 0.17333077 0.19773033 0.22668104 0.24670072 0.24129686 0.20329136 0.14006539][0.16984861 0.13862425 0.10986026 0.10865137 0.13781314 0.18223664 0.21843818 0.23830591 0.24918342 0.26160964 0.28009841 0.2939648 0.28479612 0.24158815 0.17022227][0.20022486 0.16184233 0.12586135 0.12694593 0.17205991 0.24103333 0.29799512 0.32470393 0.32464021 0.31512788 0.31158584 0.31097803 0.29607302 0.25148126 0.17903763][0.22543184 0.1798365 0.13590677 0.1370455 0.19485731 0.28566387 0.3629874 0.39700118 0.38489112 0.34906447 0.31714416 0.29817802 0.2783719 0.23789415 0.17186204][0.25145078 0.20248744 0.1522564 0.15094003 0.21405943 0.3167834 0.40672243 0.44385487 0.41866627 0.35624593 0.29525819 0.25722736 0.23408474 0.20215583 0.14827253][0.27701595 0.23019411 0.17971149 0.17767113 0.24084851 0.34522355 0.43749279 0.47175273 0.43317819 0.34770942 0.26186654 0.20742922 0.18261997 0.16051963 0.12019681][0.3127571 0.27204347 0.22657958 0.22511707 0.2831642 0.37888327 0.46283045 0.48925522 0.44090542 0.34235281 0.24203819 0.17694889 0.1504299 0.13457893 0.10368341][0.36607286 0.33512044 0.29639405 0.29272461 0.33735871 0.41280675 0.47903773 0.4961755 0.44701141 0.35073745 0.25029081 0.18189254 0.15175006 0.13468683 0.10483678][0.41946012 0.40097022 0.36956552 0.36158019 0.38700294 0.43481991 0.47889686 0.48905942 0.44934243 0.37025037 0.28286427 0.21708792 0.18046385 0.15410712 0.11678936][0.44525608 0.43835941 0.41600055 0.40751728 0.41818142 0.44279292 0.46890023 0.47728994 0.45364854 0.39906031 0.3303656 0.2684969 0.22178857 0.17951593 0.12878275][0.43504155 0.4340027 0.4196023 0.41388282 0.4166998 0.42616403 0.44005445 0.44996476 0.44514212 0.41887042 0.37345383 0.31891257 0.26276526 0.20260654 0.13589467][0.39308366 0.39371443 0.38263407 0.37764305 0.37516829 0.37475172 0.3802253 0.39188918 0.40399697 0.40481389 0.38431862 0.34112772 0.28000343 0.205723 0.1262901][0.31582466 0.31355831 0.30119264 0.29394621 0.28746998 0.28195968 0.28385353 0.29850003 0.32420608 0.34680542 0.34821451 0.31794643 0.25760543 0.1773086 0.0935811][0.20782755 0.19775981 0.1814189 0.17167911 0.16450252 0.15959844 0.16303992 0.181585 0.2156135 0.25134176 0.26769122 0.24938361 0.19594036 0.1204019 0.043573197][0.093801 0.075647429 0.056976978 0.047371119 0.043057628 0.042300865 0.0486743 0.068137437 0.1024137 0.14074013 0.1640742 0.15654758 0.11610974 0.055153847 -0.0062198793]]...]
INFO - root - 2017-12-11 06:40:12.320205: step 27610, loss = 0.69, batch loss = 0.63 (14.0 examples/sec; 0.572 sec/batch; 48h:28m:43s remains)
INFO - root - 2017-12-11 06:40:17.892447: step 27620, loss = 0.70, batch loss = 0.64 (14.0 examples/sec; 0.572 sec/batch; 48h:25m:45s remains)
INFO - root - 2017-12-11 06:40:23.436263: step 27630, loss = 0.70, batch loss = 0.64 (14.3 examples/sec; 0.558 sec/batch; 47h:17m:09s remains)
INFO - root - 2017-12-11 06:40:28.963305: step 27640, loss = 0.71, batch loss = 0.65 (14.7 examples/sec; 0.545 sec/batch; 46h:07m:51s remains)
INFO - root - 2017-12-11 06:40:34.088738: step 27650, loss = 0.69, batch loss = 0.63 (14.6 examples/sec; 0.546 sec/batch; 46h:15m:25s remains)
INFO - root - 2017-12-11 06:40:39.670419: step 27660, loss = 0.70, batch loss = 0.64 (14.8 examples/sec; 0.539 sec/batch; 45h:37m:11s remains)
INFO - root - 2017-12-11 06:40:45.129035: step 27670, loss = 0.72, batch loss = 0.66 (14.5 examples/sec; 0.551 sec/batch; 46h:37m:27s remains)
INFO - root - 2017-12-11 06:40:50.704246: step 27680, loss = 0.68, batch loss = 0.62 (14.3 examples/sec; 0.558 sec/batch; 47h:14m:23s remains)
INFO - root - 2017-12-11 06:40:56.260007: step 27690, loss = 0.68, batch loss = 0.62 (14.8 examples/sec; 0.542 sec/batch; 45h:51m:58s remains)
INFO - root - 2017-12-11 06:41:01.719101: step 27700, loss = 0.70, batch loss = 0.64 (14.7 examples/sec; 0.543 sec/batch; 45h:56m:15s remains)
2017-12-11 06:41:02.352361: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.036597725 0.1004796 0.17635736 0.25834265 0.32378247 0.36406544 0.38205761 0.37120438 0.32826844 0.26911297 0.22193699 0.19265038 0.17196842 0.1625354 0.18790837][0.022135606 0.082319759 0.16374746 0.25732481 0.3367683 0.38752431 0.40824708 0.38879928 0.32732466 0.24560645 0.17914084 0.13993196 0.11999916 0.12049638 0.16727497][0.012636918 0.071850993 0.16057527 0.26503411 0.35695016 0.41897339 0.44664067 0.42665121 0.35520315 0.25788623 0.17241295 0.11469506 0.081287175 0.076557681 0.12900589][0.015076295 0.079196975 0.17845562 0.29349598 0.39721224 0.47451931 0.5189777 0.51352161 0.44825131 0.34775513 0.24708229 0.1628971 0.098664612 0.067188509 0.10093183][0.027262872 0.10319646 0.21775293 0.34610432 0.46411961 0.56235164 0.63303071 0.6545397 0.60843611 0.51541537 0.40522078 0.29207802 0.1863744 0.11237685 0.10754088][0.045733478 0.13984467 0.27494076 0.42124212 0.55665421 0.67815679 0.77707869 0.82776642 0.80401993 0.72143662 0.60457766 0.46462741 0.31817397 0.20099367 0.15465766][0.065865174 0.18003851 0.33716035 0.50269252 0.65410405 0.79317123 0.91073734 0.97955006 0.96970439 0.89202273 0.76866579 0.61037576 0.43888795 0.29632252 0.22270446][0.083557867 0.21551488 0.39096469 0.570105 0.72683513 0.86460227 0.97495067 1.0353843 1.0204855 0.93897039 0.81275868 0.65396953 0.48596776 0.34754041 0.26997578][0.096356615 0.23918867 0.42153087 0.59870344 0.74033219 0.84966922 0.92177516 0.9460274 0.91026849 0.82364017 0.70599806 0.56909841 0.432558 0.32356903 0.26085982][0.098863453 0.23987183 0.40988857 0.56322032 0.66867596 0.72973686 0.74688506 0.72348809 0.66351151 0.57752919 0.48039815 0.38014862 0.28888234 0.21987881 0.18022917][0.082654953 0.20458858 0.34037346 0.44944063 0.50568408 0.51455468 0.48168904 0.42088765 0.347867 0.27287555 0.20404416 0.14411283 0.097396135 0.066067323 0.04973][0.048213847 0.13765635 0.22552964 0.28114009 0.288208 0.25591689 0.19355494 0.11885075 0.050974429 -0.002505911 -0.040033434 -0.063761331 -0.075213648 -0.078394443 -0.076759055][0.00991462 0.064681552 0.1069909 0.11671478 0.087874278 0.033548746 -0.034903679 -0.10142083 -0.15061006 -0.17927417 -0.19065318 -0.18984124 -0.18100351 -0.16946274 -0.15849398][-0.016233273 0.012044511 0.023076741 0.0052176137 -0.040684734 -0.099275172 -0.15837401 -0.2060336 -0.23346767 -0.24228846 -0.23819071 -0.22679102 -0.21187195 -0.19719318 -0.18459828][-0.022559885 -0.0090689892 -0.01345443 -0.041623522 -0.090304367 -0.14368246 -0.18945993 -0.21945508 -0.23052613 -0.22785807 -0.2177787 -0.20508479 -0.19172673 -0.17944339 -0.16898628]]...]
INFO - root - 2017-12-11 06:41:07.828365: step 27710, loss = 0.70, batch loss = 0.64 (14.2 examples/sec; 0.564 sec/batch; 47h:45m:36s remains)
INFO - root - 2017-12-11 06:41:13.314574: step 27720, loss = 0.70, batch loss = 0.64 (14.6 examples/sec; 0.550 sec/batch; 46h:31m:31s remains)
INFO - root - 2017-12-11 06:41:18.939323: step 27730, loss = 0.70, batch loss = 0.65 (14.4 examples/sec; 0.557 sec/batch; 47h:10m:55s remains)
INFO - root - 2017-12-11 06:41:24.487434: step 27740, loss = 0.70, batch loss = 0.64 (14.6 examples/sec; 0.547 sec/batch; 46h:15m:58s remains)
INFO - root - 2017-12-11 06:41:29.693141: step 27750, loss = 0.68, batch loss = 0.62 (14.4 examples/sec; 0.556 sec/batch; 47h:03m:27s remains)
INFO - root - 2017-12-11 06:41:35.202078: step 27760, loss = 0.68, batch loss = 0.63 (14.6 examples/sec; 0.550 sec/batch; 46h:31m:54s remains)
INFO - root - 2017-12-11 06:41:40.755667: step 27770, loss = 0.71, batch loss = 0.65 (14.5 examples/sec; 0.551 sec/batch; 46h:36m:26s remains)
INFO - root - 2017-12-11 06:41:46.266728: step 27780, loss = 0.70, batch loss = 0.64 (14.3 examples/sec; 0.558 sec/batch; 47h:12m:23s remains)
INFO - root - 2017-12-11 06:41:51.751420: step 27790, loss = 0.70, batch loss = 0.64 (14.6 examples/sec; 0.548 sec/batch; 46h:24m:52s remains)
INFO - root - 2017-12-11 06:41:57.239800: step 27800, loss = 0.70, batch loss = 0.64 (14.4 examples/sec; 0.554 sec/batch; 46h:53m:54s remains)
2017-12-11 06:41:57.844087: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.079917014 -0.078090414 -0.07625632 -0.077762842 -0.079412982 -0.08010453 -0.080017112 -0.079882592 -0.081054322 -0.085414521 -0.091598071 -0.096785381 -0.098062813 -0.094392613 -0.086632952][-0.084561534 -0.084105864 -0.083066456 -0.083064914 -0.080093987 -0.0741458 -0.067286856 -0.062674373 -0.063337885 -0.071614034 -0.084785178 -0.096778363 -0.1025485 -0.10048936 -0.091586739][-0.086354256 -0.083949864 -0.0785492 -0.069246463 -0.052345905 -0.030740295 -0.010254944 0.0023246966 0.00055068784 -0.018068101 -0.047549151 -0.075863048 -0.094153449 -0.10044617 -0.094870567][-0.078637958 -0.0672281 -0.046743516 -0.014927899 0.02918628 0.077390954 0.1184996 0.14084505 0.1328447 0.093675978 0.035016678 -0.022811696 -0.065588266 -0.089867279 -0.094306231][-0.055327877 -0.025248235 0.023457032 0.091974661 0.17548445 0.25880665 0.32457528 0.35510731 0.33294308 0.26194936 0.1621294 0.06268391 -0.015970321 -0.066893376 -0.087205447][-0.016025459 0.041166443 0.12816319 0.24176371 0.36864975 0.48652992 0.57273394 0.6040557 0.55863285 0.44807523 0.30253112 0.15820187 0.040681411 -0.038640559 -0.075705715][0.031423263 0.11753385 0.242646 0.39568388 0.55412489 0.69098109 0.7816608 0.80124456 0.727628 0.58071864 0.40007803 0.22421922 0.079884134 -0.018298997 -0.066106796][0.07137125 0.17888521 0.32757026 0.49754798 0.66010147 0.78876996 0.86222982 0.85977983 0.76327372 0.5979833 0.40805963 0.22839896 0.081826471 -0.017187104 -0.065174215][0.086185813 0.19784556 0.34393635 0.49879125 0.63371068 0.72853017 0.77018988 0.74582946 0.642653 0.48561049 0.31671914 0.16327332 0.040693428 -0.039894138 -0.076485626][0.065473206 0.16020878 0.27633381 0.38855517 0.47503126 0.52521414 0.534931 0.49876583 0.40831777 0.28302294 0.15711075 0.049520761 -0.031785607 -0.080887787 -0.097914904][0.017305912 0.0800572 0.15077066 0.21011959 0.24661084 0.25844184 0.24735187 0.21240082 0.14800361 0.065976657 -0.010060328 -0.06813886 -0.10556121 -0.12148092 -0.11822538][-0.037569679 -0.0089248624 0.019087229 0.035235472 0.036992434 0.027169647 0.0092924163 -0.016153477 -0.052610561 -0.094624974 -0.12869121 -0.14771388 -0.15180843 -0.1435914 -0.12688328][-0.079981916 -0.076407447 -0.075535625 -0.082147159 -0.09430483 -0.10899899 -0.12403406 -0.13897605 -0.15514936 -0.17043601 -0.17836392 -0.17498556 -0.16163631 -0.14241771 -0.12154271][-0.10050263 -0.10912605 -0.11831148 -0.12977663 -0.14115082 -0.15141687 -0.16045162 -0.16816661 -0.17411083 -0.17697233 -0.17378421 -0.16254519 -0.14504926 -0.12536232 -0.10720259][-0.10087565 -0.11103777 -0.11934537 -0.12700403 -0.13276346 -0.13722761 -0.14151615 -0.14559184 -0.14809977 -0.14770932 -0.14282145 -0.13248627 -0.1184184 -0.10377861 -0.091254033]]...]
INFO - root - 2017-12-11 06:42:03.399825: step 27810, loss = 0.68, batch loss = 0.62 (14.1 examples/sec; 0.566 sec/batch; 47h:52m:39s remains)
INFO - root - 2017-12-11 06:42:08.860479: step 27820, loss = 0.69, batch loss = 0.64 (14.5 examples/sec; 0.553 sec/batch; 46h:47m:11s remains)
INFO - root - 2017-12-11 06:42:14.390424: step 27830, loss = 0.70, batch loss = 0.64 (14.6 examples/sec; 0.548 sec/batch; 46h:24m:14s remains)
INFO - root - 2017-12-11 06:42:19.591384: step 27840, loss = 0.71, batch loss = 0.65 (18.7 examples/sec; 0.428 sec/batch; 36h:12m:47s remains)
INFO - root - 2017-12-11 06:42:25.072618: step 27850, loss = 0.69, batch loss = 0.63 (14.5 examples/sec; 0.553 sec/batch; 46h:50m:09s remains)
INFO - root - 2017-12-11 06:42:30.605929: step 27860, loss = 0.71, batch loss = 0.65 (14.5 examples/sec; 0.550 sec/batch; 46h:32m:00s remains)
INFO - root - 2017-12-11 06:42:36.143590: step 27870, loss = 0.71, batch loss = 0.65 (14.2 examples/sec; 0.563 sec/batch; 47h:37m:50s remains)
INFO - root - 2017-12-11 06:42:41.579384: step 27880, loss = 0.69, batch loss = 0.63 (14.9 examples/sec; 0.537 sec/batch; 45h:26m:29s remains)
INFO - root - 2017-12-11 06:42:47.063089: step 27890, loss = 0.70, batch loss = 0.64 (14.2 examples/sec; 0.564 sec/batch; 47h:44m:17s remains)
INFO - root - 2017-12-11 06:42:52.624523: step 27900, loss = 0.71, batch loss = 0.65 (14.1 examples/sec; 0.566 sec/batch; 47h:53m:59s remains)
2017-12-11 06:42:53.255766: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.28964314 0.30629745 0.30978096 0.3031854 0.28577113 0.24735707 0.18926722 0.13105874 0.092588931 0.086685926 0.10936502 0.15283054 0.21036464 0.26444337 0.29639131][0.30985662 0.31814989 0.30065408 0.26752451 0.23098753 0.18547294 0.12944214 0.07368423 0.03554773 0.029745905 0.05434892 0.10375901 0.17058988 0.23578915 0.27864006][0.32202798 0.32868007 0.29713973 0.24260885 0.18885005 0.13839982 0.088429265 0.040702604 0.0069430149 0.00069059373 0.022227818 0.068913609 0.13303214 0.19591448 0.23663476][0.32232141 0.33850241 0.30949354 0.25134724 0.19361345 0.14495558 0.1031139 0.063572526 0.032566484 0.021796148 0.033670459 0.068027727 0.11838318 0.16731127 0.19502373][0.31114841 0.34272164 0.32940298 0.28472638 0.23767747 0.19897148 0.16686811 0.13420387 0.10401069 0.086406484 0.08524283 0.10278829 0.13389662 0.162232 0.1695074][0.29778352 0.34153759 0.34853792 0.32795531 0.30205017 0.27948812 0.25773835 0.22971629 0.19794975 0.1725105 0.15854694 0.16039991 0.17358918 0.18112135 0.16643077][0.29369658 0.33903936 0.36010811 0.36204293 0.35724303 0.35138229 0.34156403 0.32139 0.29254302 0.26441994 0.2426464 0.23306784 0.23104021 0.21945912 0.18472815][0.30324778 0.33913752 0.3614786 0.37453446 0.38171577 0.3881053 0.39237002 0.38739589 0.37156397 0.35140198 0.33149117 0.31721187 0.30194718 0.27133462 0.21792054][0.31712288 0.33946961 0.3534314 0.36535686 0.37374684 0.38571337 0.40227062 0.41484302 0.41834134 0.41556489 0.40770748 0.39670187 0.37254322 0.3256487 0.25659493][0.31729338 0.32808298 0.33073115 0.33471763 0.33838 0.35241076 0.37974295 0.40925166 0.43325582 0.45177937 0.46197322 0.45986208 0.43080714 0.3714866 0.29117867][0.29962286 0.30499929 0.2998361 0.29540652 0.29289502 0.30691463 0.34126461 0.38214231 0.42101693 0.45725057 0.48506898 0.49337336 0.46306679 0.39687392 0.31124502][0.279781 0.28633374 0.2784757 0.26744944 0.25795844 0.26777443 0.30127114 0.34405282 0.38823885 0.43353045 0.47240171 0.4876036 0.45763382 0.39008462 0.30523261][0.26732442 0.27747461 0.26956376 0.25420174 0.23907472 0.24261796 0.26900643 0.30556369 0.34631574 0.39094332 0.43099537 0.44605881 0.4158099 0.35087588 0.27101356][0.25771287 0.27190271 0.26743287 0.25301814 0.23663287 0.23412217 0.24966298 0.27402702 0.30387008 0.33878338 0.37035367 0.37905598 0.34796852 0.28875428 0.21722713][0.25113571 0.27024567 0.27295229 0.26551095 0.2532506 0.24775279 0.25419554 0.26621005 0.28209063 0.30106431 0.31660205 0.3134028 0.27908143 0.22526886 0.16276734]]...]
INFO - root - 2017-12-11 06:42:58.856215: step 27910, loss = 0.68, batch loss = 0.63 (14.6 examples/sec; 0.547 sec/batch; 46h:14m:35s remains)
INFO - root - 2017-12-11 06:43:04.269058: step 27920, loss = 0.70, batch loss = 0.64 (14.8 examples/sec; 0.542 sec/batch; 45h:51m:11s remains)
INFO - root - 2017-12-11 06:43:09.813840: step 27930, loss = 0.71, batch loss = 0.65 (14.3 examples/sec; 0.561 sec/batch; 47h:29m:06s remains)
INFO - root - 2017-12-11 06:43:15.031329: step 27940, loss = 0.71, batch loss = 0.65 (14.9 examples/sec; 0.537 sec/batch; 45h:24m:36s remains)
INFO - root - 2017-12-11 06:43:20.541867: step 27950, loss = 0.69, batch loss = 0.63 (14.4 examples/sec; 0.554 sec/batch; 46h:52m:59s remains)
INFO - root - 2017-12-11 06:43:25.976650: step 27960, loss = 0.70, batch loss = 0.65 (14.7 examples/sec; 0.543 sec/batch; 45h:55m:14s remains)
INFO - root - 2017-12-11 06:43:31.493749: step 27970, loss = 0.70, batch loss = 0.64 (14.6 examples/sec; 0.549 sec/batch; 46h:27m:39s remains)
INFO - root - 2017-12-11 06:43:37.041250: step 27980, loss = 0.72, batch loss = 0.66 (14.4 examples/sec; 0.555 sec/batch; 46h:58m:21s remains)
INFO - root - 2017-12-11 06:43:42.531469: step 27990, loss = 0.68, batch loss = 0.63 (14.8 examples/sec; 0.540 sec/batch; 45h:39m:31s remains)
INFO - root - 2017-12-11 06:43:48.083871: step 28000, loss = 0.71, batch loss = 0.65 (14.8 examples/sec; 0.542 sec/batch; 45h:50m:36s remains)
2017-12-11 06:43:48.608896: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.091265567 -0.090273604 -0.08588703 -0.083673835 -0.0824354 -0.08160127 -0.080552086 -0.08046174 -0.076706149 -0.061899368 -0.034003831 0.0037718927 0.0454139 0.085753761 0.11992203][-0.078189991 -0.077487044 -0.072780259 -0.068765156 -0.064619049 -0.060104113 -0.054270048 -0.048110154 -0.039331969 -0.02284419 0.0032306681 0.036690541 0.071463428 0.1032309 0.12991367][-0.04289376 -0.040409721 -0.033291213 -0.024516864 -0.014532181 -0.0042343638 0.0075027067 0.019166779 0.030245375 0.043905325 0.062602431 0.086431496 0.10944933 0.12820762 0.14407822][0.014702653 0.021788053 0.034369439 0.051005352 0.068249471 0.083557568 0.098556072 0.11181971 0.12051452 0.12671249 0.13470133 0.14692186 0.15670314 0.16037829 0.16289304][0.093258917 0.10655282 0.12643455 0.15246493 0.17661223 0.19457564 0.20904709 0.21962883 0.22187971 0.21756138 0.21335685 0.21347556 0.20952779 0.19709773 0.18430665][0.17417532 0.19393209 0.22279522 0.2601085 0.29219458 0.31278792 0.32547012 0.33147749 0.32523769 0.30852437 0.29069194 0.27736777 0.25898561 0.23002836 0.20150851][0.23369041 0.26105136 0.29995894 0.34895569 0.38938367 0.41328076 0.4242236 0.42497221 0.40982974 0.3810592 0.35031173 0.32378566 0.29206568 0.24932759 0.20828801][0.25260848 0.28768334 0.33599812 0.39388832 0.43985835 0.46551979 0.47397146 0.46950841 0.44741517 0.41026187 0.37072995 0.33480042 0.29490954 0.24585105 0.20016307][0.22607526 0.26733112 0.32052094 0.38036582 0.42586526 0.45005819 0.45552334 0.44789833 0.42382917 0.38564467 0.34519055 0.30715469 0.26687065 0.22060341 0.17936307][0.16386452 0.20478611 0.25430924 0.30645511 0.34459737 0.36440653 0.36736557 0.359516 0.33969787 0.30999306 0.27844307 0.24736343 0.21557738 0.18132922 0.15265676][0.083846673 0.11757147 0.15678541 0.19559737 0.22422373 0.24069627 0.24407291 0.23943579 0.22844316 0.21291526 0.19558726 0.17670412 0.15902238 0.14196037 0.12936677][0.0087288674 0.031482846 0.057162259 0.081355326 0.1018392 0.11777145 0.12437031 0.12492777 0.12444181 0.12473914 0.12289942 0.11799045 0.11586682 0.11625352 0.11809625][-0.047798928 -0.036552384 -0.023361458 -0.011066476 0.0046176682 0.022406621 0.033344358 0.039220154 0.048363473 0.062763192 0.074749179 0.082944795 0.094754234 0.10861968 0.11969049][-0.081349224 -0.078778282 -0.073967844 -0.069250077 -0.056193084 -0.037074577 -0.023689333 -0.014625888 0.0014972618 0.026782148 0.050315633 0.070066437 0.09305194 0.11536609 0.12996905][-0.094197676 -0.095902286 -0.094543025 -0.09332481 -0.082668722 -0.065459244 -0.053890843 -0.045195527 -0.02551743 0.0072260452 0.040270556 0.07016959 0.10189636 0.12894155 0.14340259]]...]
INFO - root - 2017-12-11 06:43:54.051196: step 28010, loss = 0.69, batch loss = 0.64 (15.0 examples/sec; 0.533 sec/batch; 45h:07m:06s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-SGD-lr0.001-clip50-initfromfixqian30k-fixqian
INFO - root - 2017-12-11 06:43:59.455081: step 28020, loss = 0.70, batch loss = 0.64 (15.0 examples/sec; 0.533 sec/batch; 45h:05m:28s remains)
INFO - root - 2017-12-11 06:44:05.017546: step 28030, loss = 0.69, batch loss = 0.63 (14.4 examples/sec; 0.554 sec/batch; 46h:52m:03s remains)
INFO - root - 2017-12-11 06:44:10.097763: step 28040, loss = 0.71, batch loss = 0.65 (14.6 examples/sec; 0.550 sec/batch; 46h:28m:49s remains)
INFO - root - 2017-12-11 06:44:15.529468: step 28050, loss = 0.70, batch loss = 0.64 (14.4 examples/sec; 0.554 sec/batch; 46h:49m:35s remains)
INFO - root - 2017-12-11 06:44:21.085099: step 28060, loss = 0.70, batch loss = 0.64 (14.5 examples/sec; 0.553 sec/batch; 46h:48m:11s remains)
INFO - root - 2017-12-11 06:44:26.572063: step 28070, loss = 0.69, batch loss = 0.63 (14.7 examples/sec; 0.543 sec/batch; 45h:57m:22s remains)
INFO - root - 2017-12-11 06:44:32.040970: step 28080, loss = 0.70, batch loss = 0.64 (14.6 examples/sec; 0.549 sec/batch; 46h:24m:24s remains)
INFO - root - 2017-12-11 06:44:37.597261: step 28090, loss = 0.71, batch loss = 0.65 (15.0 examples/sec; 0.535 sec/batch; 45h:12m:00s remains)
INFO - root - 2017-12-11 06:44:43.082819: step 28100, loss = 0.69, batch loss = 0.63 (14.3 examples/sec; 0.558 sec/batch; 47h:11m:20s remains)
2017-12-11 06:44:43.682658: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.024956821 0.021844277 0.013995633 0.0080309324 0.0058657643 0.006958934 0.0087749781 0.010080442 0.011462044 0.013104739 0.014037749 0.014302148 0.013679975 0.013118567 0.014444523][0.059902646 0.056268122 0.0441776 0.03493084 0.032260783 0.034770004 0.038016167 0.039785482 0.041985486 0.044693194 0.0451676 0.043322861 0.039871491 0.037527956 0.039259791][0.095867738 0.091515236 0.074597143 0.062362064 0.060232967 0.065209307 0.070721172 0.073830374 0.077788688 0.082001776 0.081163839 0.075314887 0.067192815 0.061848309 0.063098177][0.12429096 0.11845733 0.097068332 0.082734488 0.082221016 0.090275861 0.098655432 0.10409489 0.11040197 0.11567479 0.11224072 0.10119343 0.0883366 0.08093816 0.082347475][0.13607733 0.12892251 0.1047119 0.08964882 0.091136217 0.10201055 0.11324505 0.121503 0.12995391 0.13516895 0.12835568 0.11270229 0.097408757 0.091161892 0.095305011][0.12889768 0.12118632 0.096611358 0.08221779 0.085433185 0.098176464 0.11136173 0.12169542 0.1310782 0.13503212 0.12516011 0.10695598 0.092451967 0.090778239 0.10048051][0.10791982 0.10132249 0.079351224 0.067108542 0.071536578 0.084728278 0.098088339 0.10881436 0.11731578 0.11877517 0.10627478 0.087132238 0.075096026 0.078983858 0.095016226][0.081979774 0.078298867 0.061955422 0.053519487 0.059045631 0.071801856 0.084148429 0.093837984 0.099619284 0.096648671 0.080295794 0.059405148 0.0485793 0.055777162 0.07616739][0.060258981 0.060559373 0.051240936 0.047898244 0.055331785 0.068100274 0.079582579 0.088009357 0.090204947 0.08098539 0.058483493 0.033502817 0.021087434 0.027549192 0.048003152][0.051385958 0.055457376 0.0515543 0.052467924 0.061912898 0.075390734 0.08679644 0.094747536 0.094108261 0.078808591 0.049421985 0.018849105 0.0025647345 0.0048669768 0.021089038][0.060979288 0.066866666 0.063693739 0.064485282 0.073165044 0.086178474 0.097460218 0.10576839 0.10459167 0.086382523 0.052748214 0.017827816 -0.0024357091 -0.0050376323 0.0054356731][0.08606296 0.091121 0.08311674 0.078300007 0.08198636 0.091808282 0.10164357 0.11041295 0.1112735 0.094866283 0.062144123 0.026982229 0.0053328266 -5.8044436e-05 0.0064265635][0.11606846 0.11815157 0.10196349 0.088706456 0.085172705 0.08982338 0.096419349 0.10467321 0.10909252 0.098826513 0.0726976 0.042524893 0.02339451 0.018456448 0.023750227][0.1364294 0.13465537 0.11124857 0.09117876 0.08187215 0.0818908 0.084926106 0.091921389 0.10002054 0.09787976 0.081638291 0.059830107 0.045661654 0.042963095 0.048329189][0.13278945 0.12773366 0.10153891 0.079431906 0.06828057 0.066267736 0.067170694 0.072875842 0.083459787 0.0885024 0.081991114 0.0688243 0.059539307 0.058667038 0.063582569]]...]
INFO - root - 2017-12-11 06:44:49.245481: step 28110, loss = 0.70, batch loss = 0.64 (14.6 examples/sec; 0.549 sec/batch; 46h:25m:55s remains)
INFO - root - 2017-12-11 06:44:54.686870: step 28120, loss = 0.70, batch loss = 0.64 (14.7 examples/sec; 0.545 sec/batch; 46h:03m:33s remains)
INFO - root - 2017-12-11 06:45:00.070878: step 28130, loss = 0.71, batch loss = 0.66 (17.9 examples/sec; 0.446 sec/batch; 37h:44m:04s remains)
INFO - root - 2017-12-11 06:45:05.309369: step 28140, loss = 0.70, batch loss = 0.64 (14.9 examples/sec; 0.536 sec/batch; 45h:19m:15s remains)
INFO - root - 2017-12-11 06:45:10.791215: step 28150, loss = 0.70, batch loss = 0.64 (15.3 examples/sec; 0.521 sec/batch; 44h:05m:08s remains)
INFO - root - 2017-12-11 06:45:16.242169: step 28160, loss = 0.71, batch loss = 0.65 (14.6 examples/sec; 0.550 sec/batch; 46h:27m:25s remains)
INFO - root - 2017-12-11 06:45:21.707735: step 28170, loss = 0.69, batch loss = 0.63 (14.3 examples/sec; 0.558 sec/batch; 47h:12m:25s remains)
INFO - root - 2017-12-11 06:45:27.337295: step 28180, loss = 0.68, batch loss = 0.62 (14.5 examples/sec; 0.552 sec/batch; 46h:39m:17s remains)
INFO - root - 2017-12-11 06:45:32.823430: step 28190, loss = 0.70, batch loss = 0.65 (14.6 examples/sec; 0.547 sec/batch; 46h:14m:37s remains)
INFO - root - 2017-12-11 06:45:38.234507: step 28200, loss = 0.69, batch loss = 0.63 (15.0 examples/sec; 0.535 sec/batch; 45h:12m:01s remains)
2017-12-11 06:45:38.838740: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.23311418 0.21664307 0.19942048 0.19876441 0.22085445 0.25857136 0.29850668 0.32475314 0.32775635 0.30477107 0.27131495 0.24013728 0.21746793 0.20998897 0.21495549][0.20557883 0.18369509 0.16600665 0.17651896 0.23014505 0.31701541 0.40912876 0.47312775 0.4896774 0.45241258 0.380484 0.29621583 0.22100465 0.17709641 0.16498725][0.13806629 0.11849906 0.10922911 0.13776498 0.22787613 0.367087 0.51178163 0.60929823 0.63438177 0.57956588 0.46635395 0.3245016 0.19370323 0.11243463 0.082439363][0.055447832 0.048111878 0.058731258 0.11207656 0.23544288 0.41377279 0.59432948 0.71236312 0.7392922 0.66571319 0.51635051 0.32846889 0.15474375 0.043541111 -0.0024169618][-0.015625931 -0.00046261598 0.040898316 0.12526758 0.27652374 0.47657681 0.67182863 0.79391652 0.81284654 0.71804494 0.53730923 0.31600818 0.11485112 -0.014717164 -0.070977032][-0.053160541 -0.0076905824 0.071562976 0.1910332 0.36631545 0.57628363 0.77054304 0.8824566 0.882656 0.76223165 0.55229628 0.30589193 0.087359689 -0.052247591 -0.11350164][-0.057834826 0.01815941 0.13453431 0.28792891 0.48153925 0.68967247 0.8653385 0.949774 0.9194445 0.77149749 0.5403899 0.28410503 0.065589741 -0.070197023 -0.12816446][-0.033191133 0.06528227 0.20754533 0.38376948 0.58369464 0.77537942 0.91606021 0.959802 0.89415133 0.72457719 0.48806125 0.24276514 0.043855768 -0.0746397 -0.12212897][0.015946625 0.12369661 0.27256319 0.45112798 0.64022022 0.8030405 0.90230191 0.90543538 0.81140071 0.63310069 0.40832222 0.19181421 0.026319582 -0.067040913 -0.10043778][0.06762404 0.16951413 0.30493879 0.46455455 0.62671292 0.75454921 0.81771576 0.79336494 0.68631667 0.51337445 0.31271175 0.13440797 0.0087524187 -0.055051394 -0.070884116][0.1049544 0.18654023 0.29132789 0.41433969 0.53669906 0.62650269 0.66130549 0.62525809 0.52416432 0.37420267 0.2111427 0.077904828 -0.0059817964 -0.039875124 -0.03748659][0.1198253 0.1733593 0.2392171 0.3167153 0.39281288 0.4438141 0.4555853 0.4182674 0.33708468 0.22500591 0.11136527 0.028079523 -0.015052987 -0.022355027 -0.0053520128][0.11769374 0.14563836 0.17635587 0.21129911 0.24328022 0.25820515 0.2507512 0.21701622 0.16136622 0.09210936 0.028689461 -0.0098293116 -0.021298317 -0.01110975 0.012768372][0.10019635 0.11065933 0.11796199 0.12321042 0.12301903 0.11150281 0.090732947 0.062242534 0.029498791 -0.0039009668 -0.028714845 -0.036918 -0.030730562 -0.013930134 0.0093069924][0.064476632 0.062716387 0.056586236 0.046341632 0.030439673 0.0076005366 -0.016763834 -0.038184159 -0.054096058 -0.064087085 -0.065792643 -0.058057941 -0.044518106 -0.027095448 -0.0075630229]]...]
INFO - root - 2017-12-11 06:45:44.265324: step 28210, loss = 0.69, batch loss = 0.63 (14.6 examples/sec; 0.548 sec/batch; 46h:19m:43s remains)
INFO - root - 2017-12-11 06:45:49.801092: step 28220, loss = 0.69, batch loss = 0.63 (14.9 examples/sec; 0.538 sec/batch; 45h:30m:22s remains)
INFO - root - 2017-12-11 06:45:55.109027: step 28230, loss = 0.70, batch loss = 0.64 (14.5 examples/sec; 0.553 sec/batch; 46h:46m:36s remains)
INFO - root - 2017-12-11 06:46:00.696378: step 28240, loss = 0.70, batch loss = 0.64 (13.8 examples/sec; 0.582 sec/batch; 49h:09m:07s remains)
INFO - root - 2017-12-11 06:46:06.121692: step 28250, loss = 0.70, batch loss = 0.64 (14.8 examples/sec; 0.540 sec/batch; 45h:36m:42s remains)
INFO - root - 2017-12-11 06:46:11.580740: step 28260, loss = 0.70, batch loss = 0.64 (15.1 examples/sec; 0.531 sec/batch; 44h:50m:17s remains)
INFO - root - 2017-12-11 06:46:17.099640: step 28270, loss = 0.72, batch loss = 0.66 (14.5 examples/sec; 0.551 sec/batch; 46h:32m:18s remains)
INFO - root - 2017-12-11 06:46:22.674814: step 28280, loss = 0.69, batch loss = 0.63 (14.3 examples/sec; 0.561 sec/batch; 47h:22m:37s remains)
INFO - root - 2017-12-11 06:46:28.229335: step 28290, loss = 0.69, batch loss = 0.63 (14.7 examples/sec; 0.543 sec/batch; 45h:55m:11s remains)
INFO - root - 2017-12-11 06:46:33.750574: step 28300, loss = 0.70, batch loss = 0.64 (14.5 examples/sec; 0.552 sec/batch; 46h:40m:11s remains)
2017-12-11 06:46:34.351683: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0028984377 -0.011786637 0.00026690675 0.040875781 0.11271817 0.20588619 0.29979926 0.37641883 0.42182314 0.42694154 0.39846015 0.35367405 0.31197965 0.28815907 0.29259047][0.019369951 0.01521428 0.031574532 0.076541461 0.15300485 0.25124082 0.35054141 0.43283573 0.48305732 0.48945296 0.45627078 0.40281633 0.35260543 0.32633802 0.33952159][0.045876835 0.045916643 0.063546821 0.10774413 0.18254147 0.27868819 0.37613225 0.45685726 0.5057621 0.50802523 0.46560568 0.40042934 0.34129336 0.31478485 0.33879706][0.075051211 0.078769363 0.094815 0.13306092 0.19989829 0.28789827 0.37861019 0.4542419 0.49860182 0.49332669 0.43844497 0.36039948 0.29446962 0.27112529 0.30658993][0.1095062 0.11396243 0.12618816 0.15640683 0.21367158 0.29240403 0.37593493 0.44498217 0.48053774 0.46138895 0.38791612 0.2933993 0.21939893 0.19839226 0.24214324][0.1370807 0.14221096 0.15496586 0.18524417 0.24393675 0.32472011 0.40967855 0.47360137 0.49415651 0.45082682 0.34794295 0.22746497 0.13787909 0.11234315 0.1561131][0.13905337 0.15046971 0.17469971 0.21965556 0.29596567 0.3928687 0.48629594 0.54256684 0.53918183 0.46243453 0.32434484 0.17560984 0.069224566 0.035116084 0.070891984][0.11568433 0.13979298 0.18458796 0.25517073 0.36014363 0.48136169 0.58542055 0.63067907 0.59783506 0.48434114 0.31269723 0.14065476 0.021635877 -0.021339586 0.0015280992][0.079846516 0.11770782 0.18420015 0.28118202 0.41303974 0.55302143 0.65954262 0.68819815 0.62633944 0.48359573 0.29161468 0.1100008 -0.012198349 -0.060367297 -0.050350048][0.039211381 0.086137973 0.1660084 0.2772229 0.41891873 0.5582732 0.65134442 0.65842533 0.57680106 0.42505345 0.23732534 0.066918336 -0.045635693 -0.092265993 -0.090354733][-0.0011943512 0.046001229 0.12603129 0.23419304 0.36476484 0.48399091 0.55180317 0.53858072 0.45129848 0.3124938 0.15222983 0.011420075 -0.079384744 -0.11621117 -0.11512287][-0.036868479 0.002225342 0.069122858 0.15817615 0.26122516 0.34840068 0.38751242 0.36063644 0.28115445 0.17038472 0.051313549 -0.048275858 -0.10777561 -0.12711965 -0.12052682][-0.061480157 -0.035611089 0.010242751 0.071401142 0.14023313 0.19414356 0.21026716 0.18023378 0.11850493 0.041574657 -0.034824144 -0.093307592 -0.12157842 -0.12311309 -0.1103323][-0.072254911 -0.060139302 -0.036011674 -0.0036328738 0.031879459 0.056995481 0.058280308 0.033694342 -0.0054046796 -0.048881039 -0.087586582 -0.11183279 -0.11587165 -0.10478986 -0.0883335][-0.073553808 -0.072960205 -0.0660761 -0.056258082 -0.04566801 -0.039877374 -0.044762492 -0.05949774 -0.077547751 -0.094246782 -0.10533231 -0.10644974 -0.096317023 -0.079603896 -0.062546074]]...]
INFO - root - 2017-12-11 06:46:39.881344: step 28310, loss = 0.69, batch loss = 0.63 (14.7 examples/sec; 0.546 sec/batch; 46h:06m:56s remains)
INFO - root - 2017-12-11 06:46:45.293144: step 28320, loss = 0.68, batch loss = 0.62 (14.5 examples/sec; 0.551 sec/batch; 46h:34m:47s remains)
INFO - root - 2017-12-11 06:46:50.421132: step 28330, loss = 0.71, batch loss = 0.65 (14.3 examples/sec; 0.560 sec/batch; 47h:19m:20s remains)
INFO - root - 2017-12-11 06:46:55.894984: step 28340, loss = 0.69, batch loss = 0.63 (14.4 examples/sec; 0.554 sec/batch; 46h:48m:09s remains)
INFO - root - 2017-12-11 06:47:01.466014: step 28350, loss = 0.68, batch loss = 0.62 (14.6 examples/sec; 0.547 sec/batch; 46h:10m:55s remains)
INFO - root - 2017-12-11 06:47:07.022010: step 28360, loss = 0.70, batch loss = 0.64 (14.9 examples/sec; 0.537 sec/batch; 45h:21m:19s remains)
INFO - root - 2017-12-11 06:47:12.543109: step 28370, loss = 0.69, batch loss = 0.63 (14.9 examples/sec; 0.538 sec/batch; 45h:29m:04s remains)
INFO - root - 2017-12-11 06:47:18.006902: step 28380, loss = 0.69, batch loss = 0.64 (15.1 examples/sec; 0.529 sec/batch; 44h:42m:35s remains)
INFO - root - 2017-12-11 06:47:23.467143: step 28390, loss = 0.69, batch loss = 0.63 (14.6 examples/sec; 0.547 sec/batch; 46h:12m:21s remains)
INFO - root - 2017-12-11 06:47:29.030441: step 28400, loss = 0.70, batch loss = 0.64 (14.2 examples/sec; 0.563 sec/batch; 47h:34m:18s remains)
2017-12-11 06:47:29.622001: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.021845069 -0.0064006313 0.010713606 0.021429107 0.021939356 0.018387998 0.017521178 0.017799011 0.017445374 0.013032319 0.0049870349 -0.00805414 -0.02611744 -0.045637276 -0.059610356][0.0042745457 0.033615742 0.064823493 0.086491682 0.092425443 0.090377994 0.087338783 0.081227086 0.072171979 0.058796048 0.042557709 0.021314077 -0.0051394426 -0.033560824 -0.054758485][0.036479428 0.081819177 0.13118228 0.17059229 0.19060469 0.19677551 0.19339643 0.177089 0.15241253 0.12260377 0.090490311 0.055187665 0.017407995 -0.019235181 -0.046140645][0.07123787 0.13453117 0.2042058 0.26540548 0.30583665 0.32552204 0.32495528 0.29945254 0.25680834 0.20571412 0.15109991 0.095414013 0.041664913 -0.005251938 -0.03799982][0.10612728 0.1901702 0.28317291 0.37049422 0.43832183 0.48024619 0.49241194 0.46593344 0.40843076 0.333157 0.24893823 0.16357934 0.084106185 0.019424997 -0.02374991][0.13439582 0.23977153 0.35636678 0.46883151 0.56396073 0.63140762 0.66365349 0.64384067 0.57603616 0.47721273 0.36245584 0.24553093 0.13794664 0.052731663 -0.0028461916][0.1426999 0.26266286 0.39602479 0.52521461 0.63816661 0.7252537 0.77792525 0.76971674 0.69936329 0.58553427 0.45012763 0.31149551 0.18441261 0.084395587 0.019227585][0.13527553 0.26004389 0.4003945 0.53474236 0.65049648 0.74149847 0.80356425 0.80399555 0.73718911 0.62080181 0.48102653 0.33792406 0.20645557 0.10212704 0.033180177][0.11551404 0.23125586 0.3631686 0.4875237 0.58954364 0.66717541 0.7243048 0.72851568 0.67236733 0.56981766 0.44647396 0.31956363 0.20026697 0.10266072 0.036399309][0.0861967 0.18051969 0.28887805 0.38924479 0.46608597 0.52079517 0.56397367 0.56751442 0.52508265 0.44686496 0.35392672 0.25701246 0.16174243 0.080795862 0.025194375][0.046460018 0.11260319 0.18878202 0.25758708 0.30538723 0.33494997 0.35916322 0.35667422 0.32464576 0.27125543 0.21148165 0.1491154 0.084662706 0.028950891 -0.007538178][0.00025565148 0.039127305 0.084871538 0.1252192 0.14953971 0.16001585 0.16792126 0.15913257 0.13362466 0.098959386 0.065419063 0.032234255 -0.0034219134 -0.033242766 -0.049155418][-0.036572352 -0.017229177 0.0072468454 0.028315568 0.038089387 0.037445068 0.034913562 0.022701563 0.0021506862 -0.020160414 -0.037120391 -0.051063213 -0.066119969 -0.076776877 -0.078170665][-0.057543449 -0.050329532 -0.03893448 -0.02960485 -0.027857423 -0.033606876 -0.041002378 -0.053044345 -0.068173312 -0.081742026 -0.089140922 -0.092577823 -0.095738344 -0.095349491 -0.089202307][-0.065573245 -0.065275 -0.061060302 -0.057758793 -0.058975436 -0.065170735 -0.073033571 -0.082766175 -0.092908427 -0.10082401 -0.1037296 -0.10364296 -0.10239554 -0.097950034 -0.089630172]]...]
INFO - root - 2017-12-11 06:47:35.244541: step 28410, loss = 0.71, batch loss = 0.65 (14.1 examples/sec; 0.567 sec/batch; 47h:54m:55s remains)
INFO - root - 2017-12-11 06:47:40.801399: step 28420, loss = 0.70, batch loss = 0.64 (14.7 examples/sec; 0.543 sec/batch; 45h:50m:09s remains)
INFO - root - 2017-12-11 06:47:46.084498: step 28430, loss = 0.71, batch loss = 0.66 (14.1 examples/sec; 0.566 sec/batch; 47h:46m:06s remains)
INFO - root - 2017-12-11 06:47:51.535901: step 28440, loss = 0.70, batch loss = 0.64 (15.4 examples/sec; 0.518 sec/batch; 43h:45m:47s remains)
INFO - root - 2017-12-11 06:47:56.985771: step 28450, loss = 0.68, batch loss = 0.62 (14.7 examples/sec; 0.543 sec/batch; 45h:52m:02s remains)
INFO - root - 2017-12-11 06:48:02.496013: step 28460, loss = 0.69, batch loss = 0.64 (14.0 examples/sec; 0.569 sec/batch; 48h:05m:46s remains)
INFO - root - 2017-12-11 06:48:08.007673: step 28470, loss = 0.70, batch loss = 0.64 (14.4 examples/sec; 0.556 sec/batch; 46h:56m:51s remains)
INFO - root - 2017-12-11 06:48:13.483669: step 28480, loss = 0.71, batch loss = 0.65 (14.7 examples/sec; 0.543 sec/batch; 45h:51m:35s remains)
INFO - root - 2017-12-11 06:48:18.921904: step 28490, loss = 0.71, batch loss = 0.65 (14.7 examples/sec; 0.543 sec/batch; 45h:52m:36s remains)
INFO - root - 2017-12-11 06:48:24.489714: step 28500, loss = 0.71, batch loss = 0.65 (14.5 examples/sec; 0.550 sec/batch; 46h:28m:14s remains)
2017-12-11 06:48:25.077923: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.0065241684 -0.00068218232 -0.015431847 -0.034739513 -0.050268162 -0.056830361 -0.054530498 -0.04404828 -0.026832204 -0.0094247116 -0.00017145253 -0.00088658335 -0.0069332966 -0.019120786 -0.037809677][0.051365945 0.043953035 0.026448525 0.0051223445 -0.0088608433 -0.0094705783 0.0034253178 0.028138556 0.05772559 0.08072634 0.086363085 0.073916554 0.051159717 0.021002302 -0.013280137][0.11439691 0.11216936 0.097171932 0.078493543 0.0703035 0.0797625 0.10848679 0.15086836 0.19091977 0.21228614 0.204659 0.16965269 0.118054 0.060512278 0.0067109722][0.18886398 0.1984074 0.19222848 0.18141559 0.18430753 0.21028122 0.2612229 0.32298616 0.3681539 0.3794508 0.34933078 0.28186291 0.19052035 0.096442424 0.01901363][0.26720974 0.29555732 0.30665633 0.31124875 0.33150095 0.38015705 0.45770878 0.536728 0.57860756 0.57102621 0.51303786 0.40960848 0.27431485 0.13819352 0.032555252][0.33513623 0.39128041 0.43253714 0.46481419 0.5104574 0.58510149 0.68738115 0.77516258 0.80100042 0.76267266 0.66994643 0.52913445 0.35270336 0.17787544 0.046242405][0.37838355 0.46887472 0.55224794 0.62362713 0.69971651 0.79731411 0.91410005 0.99721634 0.99483293 0.9161678 0.78359234 0.6074149 0.40055528 0.20102638 0.0540497][0.39239749 0.51559514 0.64163548 0.75043786 0.84805596 0.95130736 1.0597193 1.119861 1.0819913 0.96497416 0.80028588 0.60205734 0.38577297 0.18554822 0.041622348][0.37738955 0.51841587 0.66868556 0.79456353 0.890361 0.97289258 1.0473858 1.0715597 1.0037829 0.86636114 0.69231415 0.49737892 0.29910415 0.12524128 0.0047185821][0.32419312 0.45703015 0.59999818 0.7143693 0.78606707 0.83060837 0.85987639 0.84906477 0.76868176 0.63809621 0.48448297 0.32109043 0.16541688 0.037927873 -0.04508359][0.23199506 0.33152857 0.4384625 0.51900744 0.55716664 0.56515479 0.5568316 0.52286965 0.4468677 0.343696 0.23148715 0.11852547 0.019121552 -0.053683054 -0.094539836][0.12430216 0.17754357 0.23357514 0.27014205 0.27536085 0.25818458 0.23034562 0.19171371 0.13587214 0.072471686 0.010227112 -0.048063476 -0.093462586 -0.11940206 -0.12702265][0.025075004 0.036458392 0.046958588 0.046368007 0.029445248 0.002676388 -0.025861248 -0.053525873 -0.082044572 -0.10719211 -0.1273385 -0.14322874 -0.15064809 -0.14750682 -0.13709737][-0.052843034 -0.0672667 -0.08297462 -0.10243488 -0.12553416 -0.14777799 -0.16502759 -0.17597723 -0.18171568 -0.18220805 -0.17874067 -0.17252421 -0.16180219 -0.14705659 -0.13141795][-0.098130733 -0.12035109 -0.14193428 -0.16315286 -0.1816591 -0.19439843 -0.19992647 -0.19871765 -0.19243322 -0.18285573 -0.17160954 -0.1592917 -0.14524244 -0.13093406 -0.1184037]]...]
INFO - root - 2017-12-11 06:48:30.572009: step 28510, loss = 0.69, batch loss = 0.64 (14.5 examples/sec; 0.553 sec/batch; 46h:42m:52s remains)
INFO - root - 2017-12-11 06:48:35.849814: step 28520, loss = 0.70, batch loss = 0.64 (29.3 examples/sec; 0.273 sec/batch; 23h:05m:16s remains)
INFO - root - 2017-12-11 06:48:41.182095: step 28530, loss = 0.70, batch loss = 0.64 (14.5 examples/sec; 0.550 sec/batch; 46h:28m:12s remains)
INFO - root - 2017-12-11 06:48:46.688507: step 28540, loss = 0.69, batch loss = 0.64 (14.7 examples/sec; 0.545 sec/batch; 45h:58m:37s remains)
INFO - root - 2017-12-11 06:48:52.170946: step 28550, loss = 0.69, batch loss = 0.63 (14.9 examples/sec; 0.537 sec/batch; 45h:19m:56s remains)
INFO - root - 2017-12-11 06:48:57.652554: step 28560, loss = 0.71, batch loss = 0.65 (13.9 examples/sec; 0.574 sec/batch; 48h:27m:03s remains)
INFO - root - 2017-12-11 06:49:03.124219: step 28570, loss = 0.70, batch loss = 0.64 (14.6 examples/sec; 0.546 sec/batch; 46h:07m:38s remains)
INFO - root - 2017-12-11 06:49:08.627561: step 28580, loss = 0.69, batch loss = 0.64 (14.9 examples/sec; 0.538 sec/batch; 45h:26m:50s remains)
INFO - root - 2017-12-11 06:49:14.140028: step 28590, loss = 0.70, batch loss = 0.64 (14.5 examples/sec; 0.550 sec/batch; 46h:25m:49s remains)
INFO - root - 2017-12-11 06:49:19.574737: step 28600, loss = 0.69, batch loss = 0.63 (14.6 examples/sec; 0.548 sec/batch; 46h:15m:07s remains)
2017-12-11 06:49:20.188828: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.16566524 0.18477076 0.25602388 0.37146005 0.50140435 0.60847485 0.65868193 0.63116288 0.53936875 0.42753455 0.3328591 0.26835775 0.22816731 0.19606091 0.15642969][0.1699719 0.18475065 0.25578335 0.37300557 0.50630456 0.6187225 0.6738736 0.64574146 0.54699928 0.422373 0.3132596 0.23796597 0.1933338 0.16259715 0.12730142][0.17360535 0.17901693 0.24000411 0.34854198 0.47639686 0.58675689 0.64249766 0.613999 0.51223403 0.38044971 0.26208302 0.18117587 0.13721007 0.11249538 0.085471369][0.16624369 0.16274527 0.21104163 0.30812997 0.42872366 0.53563422 0.59028739 0.56199551 0.46020645 0.32600477 0.20414864 0.12382569 0.086569913 0.071750484 0.054541629][0.12662518 0.12270624 0.16628107 0.25917187 0.37930495 0.48878464 0.54672331 0.52252293 0.42529413 0.29432204 0.17635953 0.1043865 0.079950154 0.078580171 0.070633821][0.059143335 0.061277 0.10854201 0.20573026 0.33306837 0.45187837 0.51870596 0.50406903 0.41785017 0.29802564 0.19318122 0.13792273 0.13205835 0.14636776 0.14549546][-0.016620697 -0.0088855252 0.044584125 0.15045805 0.29038623 0.42487332 0.50798535 0.51121634 0.44387564 0.34179065 0.25506812 0.21835279 0.2289463 0.25416088 0.25555354][-0.073722444 -0.0688049 -0.014277634 0.097483926 0.24930844 0.40216154 0.50869232 0.53953928 0.5000568 0.42075062 0.35128173 0.32532203 0.34008342 0.36401722 0.36112428][-0.09731169 -0.10457392 -0.05755695 0.051903192 0.20775831 0.37366432 0.50302637 0.56412435 0.5549919 0.49761152 0.43881825 0.41124335 0.41588235 0.42788073 0.41812912][-0.087812506 -0.11372834 -0.08216086 0.016112251 0.16625352 0.33519077 0.47847682 0.56276846 0.57891697 0.5395292 0.48527285 0.44692546 0.43139273 0.42439619 0.40585402][-0.036749024 -0.08908838 -0.084386706 -0.010190331 0.12035625 0.27755964 0.41998994 0.51379555 0.54475737 0.51761854 0.464583 0.41266152 0.37436786 0.34671193 0.31963381][0.040436778 -0.043310557 -0.072534844 -0.030648768 0.070379786 0.20333888 0.33046493 0.4191837 0.45304233 0.43223643 0.38024473 0.31843242 0.26234728 0.21843879 0.186396][0.11770215 0.0045742914 -0.058597758 -0.051025227 0.01567181 0.11633804 0.21743403 0.28897175 0.31548017 0.29657155 0.24848261 0.18604544 0.12455818 0.075478978 0.044954225][0.16721104 0.0350972 -0.05366331 -0.075551242 -0.042624749 0.022356419 0.091308534 0.13901648 0.15359895 0.13607457 0.097529896 0.046575654 -0.0049107592 -0.045924883 -0.068581805][0.17810088 0.0437305 -0.055063311 -0.094131351 -0.085996054 -0.051281579 -0.012197796 0.012604325 0.015935911 0.00044508744 -0.026138267 -0.05965561 -0.092975833 -0.1191669 -0.13236991]]...]
INFO - root - 2017-12-11 06:49:25.704413: step 28610, loss = 0.71, batch loss = 0.65 (14.4 examples/sec; 0.557 sec/batch; 47h:00m:41s remains)
INFO - root - 2017-12-11 06:49:30.916844: step 28620, loss = 0.69, batch loss = 0.63 (13.3 examples/sec; 0.601 sec/batch; 50h:42m:22s remains)
INFO - root - 2017-12-11 06:49:36.410850: step 28630, loss = 0.71, batch loss = 0.65 (14.3 examples/sec; 0.558 sec/batch; 47h:07m:36s remains)
INFO - root - 2017-12-11 06:49:41.912816: step 28640, loss = 0.70, batch loss = 0.64 (14.6 examples/sec; 0.549 sec/batch; 46h:21m:13s remains)
INFO - root - 2017-12-11 06:49:47.391169: step 28650, loss = 0.70, batch loss = 0.64 (14.5 examples/sec; 0.553 sec/batch; 46h:40m:13s remains)
INFO - root - 2017-12-11 06:49:52.998860: step 28660, loss = 0.69, batch loss = 0.63 (14.5 examples/sec; 0.553 sec/batch; 46h:39m:07s remains)
INFO - root - 2017-12-11 06:49:58.545992: step 28670, loss = 0.70, batch loss = 0.64 (14.1 examples/sec; 0.567 sec/batch; 47h:51m:39s remains)
INFO - root - 2017-12-11 06:50:04.010468: step 28680, loss = 0.70, batch loss = 0.64 (15.4 examples/sec; 0.518 sec/batch; 43h:44m:40s remains)
INFO - root - 2017-12-11 06:50:09.515868: step 28690, loss = 0.70, batch loss = 0.64 (14.1 examples/sec; 0.569 sec/batch; 47h:58m:51s remains)
INFO - root - 2017-12-11 06:50:15.054453: step 28700, loss = 0.69, batch loss = 0.63 (14.5 examples/sec; 0.550 sec/batch; 46h:26m:08s remains)
2017-12-11 06:50:15.673689: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0567514 -0.050859656 -0.036992114 -0.021486139 -0.0084409779 -0.0012543665 -0.0040099421 -0.017782411 -0.0359369 -0.051179718 -0.061185777 -0.067054056 -0.069098972 -0.0680475 -0.064979635][-0.047823794 -0.031397071 -0.0025700808 0.029088277 0.056623302 0.075705528 0.080134228 0.066370666 0.041874774 0.017460326 -0.0025077683 -0.019272035 -0.032812521 -0.041569583 -0.044713315][-0.031517286 -0.00087320711 0.048394777 0.10341632 0.15327325 0.193049 0.21454832 0.20912945 0.18176737 0.14648439 0.11209046 0.077864006 0.043965869 0.015783345 -0.0018049012][-0.01250441 0.032988138 0.1050828 0.18797961 0.26552474 0.33255231 0.3800576 0.3929061 0.36943647 0.32483283 0.27343819 0.21548127 0.15187894 0.093989529 0.053511608][0.004048523 0.061584268 0.15474302 0.26585987 0.37290686 0.4702211 0.54875875 0.58651841 0.57221681 0.51995105 0.45039225 0.36525136 0.26690891 0.17403618 0.10663633][0.014235063 0.079879612 0.18976566 0.32612181 0.46151847 0.58849818 0.69683611 0.75712574 0.74876231 0.68471122 0.59340566 0.47974434 0.34870157 0.22577822 0.13705587][0.015946504 0.084592231 0.20406187 0.35733891 0.51400894 0.66364515 0.79247361 0.86409366 0.85190696 0.76959592 0.65388006 0.51491177 0.36171412 0.22348543 0.12771276][0.0083644567 0.075142182 0.19524549 0.35271937 0.51760542 0.67611 0.80955058 0.87706023 0.85144365 0.74965274 0.61445707 0.46163696 0.30388361 0.17016073 0.084208958][-0.0063750157 0.053509403 0.16473418 0.31193855 0.4683331 0.61752808 0.73753375 0.78783697 0.74557364 0.6316849 0.49044907 0.3412948 0.19874904 0.087924413 0.025183808][-0.024633333 0.023883874 0.11812244 0.24325404 0.37662441 0.50073552 0.59362054 0.61996222 0.56449467 0.45104703 0.32060042 0.19333951 0.082658067 0.006711571 -0.027165795][-0.041990083 -0.008562848 0.062140606 0.15685365 0.25745192 0.34761781 0.40892825 0.4142662 0.35612318 0.25807244 0.15364261 0.060737804 -0.010738358 -0.0507108 -0.0594781][-0.054881766 -0.038397439 0.0049414579 0.064760618 0.12811396 0.18252 0.2157643 0.21000989 0.16274397 0.093286075 0.025220869 -0.028650057 -0.063547358 -0.076133706 -0.07040149][-0.061544761 -0.059272908 -0.041227479 -0.01412313 0.014616886 0.038068879 0.050952144 0.043904375 0.016401017 -0.019354019 -0.05006988 -0.069507577 -0.077627867 -0.074897118 -0.0654083][-0.06296239 -0.068996146 -0.068251267 -0.064770043 -0.061308157 -0.059630416 -0.05964265 -0.0641773 -0.0731727 -0.08136896 -0.084035218 -0.080505826 -0.073437333 -0.064739645 -0.057188362][-0.062115196 -0.070355885 -0.0767606 -0.08447244 -0.093544021 -0.1027734 -0.10947482 -0.11255325 -0.11138419 -0.1049901 -0.093913667 -0.080547385 -0.068202369 -0.058840428 -0.054106914]]...]
INFO - root - 2017-12-11 06:50:21.235377: step 28710, loss = 0.70, batch loss = 0.64 (14.4 examples/sec; 0.555 sec/batch; 46h:50m:27s remains)
INFO - root - 2017-12-11 06:50:26.463793: step 28720, loss = 0.69, batch loss = 0.64 (14.6 examples/sec; 0.549 sec/batch; 46h:19m:07s remains)
INFO - root - 2017-12-11 06:50:31.954014: step 28730, loss = 0.70, batch loss = 0.64 (14.5 examples/sec; 0.552 sec/batch; 46h:35m:01s remains)
INFO - root - 2017-12-11 06:50:37.403033: step 28740, loss = 0.71, batch loss = 0.65 (14.2 examples/sec; 0.563 sec/batch; 47h:29m:24s remains)
INFO - root - 2017-12-11 06:50:42.788627: step 28750, loss = 0.68, batch loss = 0.62 (14.6 examples/sec; 0.547 sec/batch; 46h:07m:09s remains)
INFO - root - 2017-12-11 06:50:48.346467: step 28760, loss = 0.68, batch loss = 0.63 (14.4 examples/sec; 0.554 sec/batch; 46h:46m:27s remains)
INFO - root - 2017-12-11 06:50:53.876930: step 28770, loss = 0.69, batch loss = 0.63 (14.4 examples/sec; 0.554 sec/batch; 46h:45m:32s remains)
INFO - root - 2017-12-11 06:50:59.326031: step 28780, loss = 0.68, batch loss = 0.62 (14.4 examples/sec; 0.557 sec/batch; 47h:01m:57s remains)
INFO - root - 2017-12-11 06:51:04.850763: step 28790, loss = 0.68, batch loss = 0.62 (15.0 examples/sec; 0.535 sec/batch; 45h:05m:33s remains)
INFO - root - 2017-12-11 06:51:10.420410: step 28800, loss = 0.71, batch loss = 0.65 (14.6 examples/sec; 0.549 sec/batch; 46h:20m:22s remains)
2017-12-11 06:51:11.042249: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.074868135 0.065133691 0.062103167 0.066901945 0.076436408 0.088181689 0.10321438 0.12537797 0.15552564 0.19065003 0.22045799 0.23674121 0.24045148 0.23430598 0.22193271][0.081352621 0.072968118 0.071585774 0.079510905 0.094213992 0.11318148 0.13681935 0.16715644 0.20290956 0.23997718 0.26841614 0.28267476 0.28817636 0.28917962 0.2876][0.07787209 0.072646454 0.07482326 0.088136122 0.11137059 0.14097181 0.17408621 0.20834669 0.24036103 0.26636904 0.27949867 0.28062642 0.28151217 0.28854129 0.30112615][0.074785255 0.073661849 0.0787514 0.096217193 0.1278885 0.16984852 0.21428439 0.25263181 0.27847806 0.28891113 0.28108233 0.26311922 0.25272557 0.25909066 0.28049594][0.077754736 0.082569748 0.090385884 0.1102901 0.14829417 0.20122102 0.25661278 0.30010054 0.32167032 0.31858504 0.29069385 0.25154492 0.22336231 0.21988355 0.24072517][0.093780629 0.105489 0.11437613 0.1321146 0.1694268 0.22567216 0.28647941 0.33357024 0.3532562 0.34150967 0.29941005 0.24275468 0.19602731 0.17748135 0.18971835][0.1262192 0.14541043 0.15266459 0.16109279 0.18702222 0.23503368 0.29245713 0.33915657 0.35841012 0.34385806 0.29667261 0.23149906 0.17230231 0.13962659 0.13937183][0.17967817 0.2054916 0.20592009 0.19517504 0.19758691 0.22584173 0.27128834 0.31316677 0.33201358 0.319119 0.27478257 0.21055342 0.14768204 0.10669903 0.095946372][0.24093553 0.27048123 0.26083022 0.22811872 0.20363325 0.20858155 0.23876148 0.273482 0.29061854 0.28035817 0.24248347 0.18607804 0.12868372 0.088620648 0.074666925][0.28772461 0.3189308 0.30251333 0.25726196 0.21734688 0.20802687 0.22743686 0.25519934 0.26804596 0.25675264 0.22241654 0.17409989 0.12662469 0.094489917 0.08392226][0.31268653 0.34206948 0.32315463 0.27732566 0.23769766 0.22766188 0.24473961 0.26854959 0.27599695 0.25923267 0.22197847 0.17575014 0.13538969 0.11175147 0.10656507][0.31293029 0.34061414 0.32399288 0.28611487 0.25634581 0.25331798 0.27278998 0.29475313 0.29716656 0.27374569 0.23061407 0.1819271 0.1442506 0.12622851 0.12508129][0.28150353 0.30522847 0.29358116 0.26774541 0.25089559 0.256131 0.27780256 0.29744637 0.29526123 0.26675457 0.21946034 0.16858739 0.13208896 0.11767812 0.11949351][0.21015716 0.22709148 0.22029342 0.20665349 0.20138262 0.21217568 0.23358345 0.25015834 0.24525443 0.21585032 0.16976133 0.12155665 0.088850491 0.078140371 0.082147174][0.11421665 0.12429144 0.12186663 0.11750141 0.11961504 0.13160199 0.14928995 0.16157505 0.15595745 0.13019453 0.090652585 0.0500111 0.023928035 0.0170813 0.022132326]]...]
INFO - root - 2017-12-11 06:51:16.560883: step 28810, loss = 0.68, batch loss = 0.62 (14.5 examples/sec; 0.551 sec/batch; 46h:30m:24s remains)
INFO - root - 2017-12-11 06:51:21.725027: step 28820, loss = 0.69, batch loss = 0.63 (14.5 examples/sec; 0.551 sec/batch; 46h:27m:04s remains)
INFO - root - 2017-12-11 06:51:27.257949: step 28830, loss = 0.69, batch loss = 0.63 (14.5 examples/sec; 0.551 sec/batch; 46h:27m:03s remains)
INFO - root - 2017-12-11 06:51:32.749143: step 28840, loss = 0.71, batch loss = 0.66 (14.8 examples/sec; 0.542 sec/batch; 45h:44m:50s remains)
INFO - root - 2017-12-11 06:51:38.247436: step 28850, loss = 0.69, batch loss = 0.63 (15.0 examples/sec; 0.535 sec/batch; 45h:06m:37s remains)
INFO - root - 2017-12-11 06:51:43.760770: step 28860, loss = 0.70, batch loss = 0.64 (14.4 examples/sec; 0.557 sec/batch; 46h:59m:16s remains)
INFO - root - 2017-12-11 06:51:49.370211: step 28870, loss = 0.69, batch loss = 0.63 (14.5 examples/sec; 0.551 sec/batch; 46h:29m:36s remains)
INFO - root - 2017-12-11 06:51:54.857770: step 28880, loss = 0.69, batch loss = 0.63 (14.6 examples/sec; 0.548 sec/batch; 46h:12m:45s remains)
INFO - root - 2017-12-11 06:52:00.322541: step 28890, loss = 0.69, batch loss = 0.63 (14.6 examples/sec; 0.548 sec/batch; 46h:13m:50s remains)
INFO - root - 2017-12-11 06:52:05.860663: step 28900, loss = 0.71, batch loss = 0.65 (14.7 examples/sec; 0.544 sec/batch; 45h:52m:18s remains)
2017-12-11 06:52:06.431776: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.082368411 -0.085644685 -0.087080657 -0.087286748 -0.086068004 -0.084246427 -0.082421489 -0.08174964 -0.08216837 -0.084030055 -0.086368822 -0.088045508 -0.08823508 -0.087196939 -0.085371032][-0.085113138 -0.089059949 -0.090354249 -0.089209 -0.084895052 -0.078289084 -0.07115683 -0.066321313 -0.065189 -0.068583243 -0.075389452 -0.082918234 -0.088238813 -0.090414144 -0.089773886][-0.083117463 -0.084541887 -0.0812464 -0.072527044 -0.057280246 -0.037727311 -0.018668128 -0.0058073909 -0.0023842002 -0.009891998 -0.027056003 -0.048927985 -0.068402484 -0.081279866 -0.087071285][-0.07613454 -0.070390247 -0.05576539 -0.030569097 0.0067074732 0.050807107 0.091076665 0.11665314 0.12231231 0.10556069 0.068902507 0.021079056 -0.02432378 -0.057664938 -0.077022][-0.064622179 -0.047193404 -0.014974748 0.034792036 0.10394971 0.18290293 0.25245059 0.29452881 0.30090758 0.26877236 0.20302866 0.11814521 0.036563314 -0.024901768 -0.06258516][-0.052295506 -0.02090526 0.031781085 0.1092127 0.21292149 0.3283453 0.42720529 0.48446068 0.48872778 0.4371551 0.33838212 0.21374878 0.095034838 0.0056280978 -0.049780175][-0.044542383 -0.0013480607 0.068567179 0.16781496 0.29679418 0.43687478 0.55317932 0.61663431 0.61439955 0.54461038 0.41945106 0.26621974 0.12322045 0.017325463 -0.047011577][-0.047116373 0.00019184113 0.076427661 0.18200706 0.31559262 0.45712948 0.57014179 0.62618285 0.61377984 0.53323936 0.39902884 0.24080062 0.097916812 -0.0038289491 -0.061872203][-0.058262013 -0.018747006 0.047314607 0.13839349 0.25189316 0.36970735 0.45954823 0.49805406 0.47715062 0.399849 0.28045264 0.14569728 0.02927324 -0.048338786 -0.087122843][-0.062520295 -0.040638797 0.00088529591 0.060483851 0.13589385 0.21351933 0.26904318 0.28656331 0.26221886 0.20095241 0.11442278 0.022323379 -0.051763255 -0.095156953 -0.11007384][-0.041953623 -0.040487107 -0.029240355 -0.007661181 0.023930829 0.057730198 0.078445315 0.0772067 0.053675238 0.013152919 -0.03652446 -0.083885111 -0.11612549 -0.12813011 -0.12358039][0.0094475141 -0.0044068662 -0.019062158 -0.030407604 -0.036058217 -0.038227685 -0.044575632 -0.0592707 -0.080911256 -0.10492618 -0.12710214 -0.14274451 -0.14721277 -0.14023869 -0.12589423][0.076555356 0.057700679 0.029151602 -0.0014285565 -0.029362284 -0.052724808 -0.074366353 -0.096293367 -0.11680537 -0.1324635 -0.14160065 -0.14371787 -0.13854103 -0.12754497 -0.11445934][0.13141382 0.11790961 0.0894919 0.055856187 0.022731161 -0.005982724 -0.031238718 -0.054589305 -0.074484862 -0.087876417 -0.094475441 -0.095861375 -0.093402088 -0.088513084 -0.084509447][0.14706804 0.14476386 0.12693788 0.10263252 0.0770142 0.054148879 0.033105958 0.012162956 -0.0067085461 -0.01949228 -0.02621622 -0.029365594 -0.031167442 -0.032847207 -0.0380468]]...]
INFO - root - 2017-12-11 06:52:11.548211: step 28910, loss = 0.69, batch loss = 0.63 (26.1 examples/sec; 0.306 sec/batch; 25h:48m:16s remains)
INFO - root - 2017-12-11 06:52:17.087457: step 28920, loss = 0.71, batch loss = 0.65 (14.3 examples/sec; 0.558 sec/batch; 47h:02m:32s remains)
INFO - root - 2017-12-11 06:52:22.633520: step 28930, loss = 0.71, batch loss = 0.65 (14.5 examples/sec; 0.552 sec/batch; 46h:32m:20s remains)
INFO - root - 2017-12-11 06:52:28.096185: step 28940, loss = 0.68, batch loss = 0.63 (15.1 examples/sec; 0.530 sec/batch; 44h:40m:09s remains)
INFO - root - 2017-12-11 06:52:33.618857: step 28950, loss = 0.70, batch loss = 0.64 (14.1 examples/sec; 0.566 sec/batch; 47h:41m:38s remains)
INFO - root - 2017-12-11 06:52:39.166781: step 28960, loss = 0.70, batch loss = 0.64 (14.5 examples/sec; 0.550 sec/batch; 46h:24m:42s remains)
INFO - root - 2017-12-11 06:52:44.654634: step 28970, loss = 0.69, batch loss = 0.63 (14.5 examples/sec; 0.554 sec/batch; 46h:40m:32s remains)
INFO - root - 2017-12-11 06:52:50.130110: step 28980, loss = 0.70, batch loss = 0.64 (14.8 examples/sec; 0.542 sec/batch; 45h:41m:08s remains)
INFO - root - 2017-12-11 06:52:55.757339: step 28990, loss = 0.70, batch loss = 0.64 (14.3 examples/sec; 0.560 sec/batch; 47h:12m:51s remains)
INFO - root - 2017-12-11 06:53:01.234725: step 29000, loss = 0.69, batch loss = 0.63 (14.4 examples/sec; 0.554 sec/batch; 46h:42m:56s remains)
2017-12-11 06:53:01.812124: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.063159928 0.061294407 0.054718036 0.047870509 0.043292429 0.042791113 0.047024418 0.052436061 0.053783797 0.055062417 0.062769227 0.075124972 0.082270458 0.077073812 0.05924144][0.14548041 0.1459552 0.13705216 0.12818454 0.12352147 0.12599158 0.13695377 0.15070167 0.15722696 0.16207434 0.17651857 0.19751579 0.20855747 0.1987744 0.16889763][0.22861512 0.23772053 0.23497204 0.233847 0.23860414 0.25174582 0.27310249 0.29448804 0.30151817 0.30202127 0.31424457 0.33641803 0.34692276 0.3310515 0.29033136][0.29801911 0.32453677 0.33980629 0.35749659 0.37912813 0.4060123 0.43456146 0.45495364 0.45114413 0.43332338 0.42811728 0.4374544 0.44033045 0.41916075 0.37428331][0.3347182 0.38547698 0.42802855 0.47137037 0.51291555 0.55307156 0.58297133 0.59203619 0.56589437 0.51826489 0.48354509 0.4686293 0.45863783 0.43474531 0.39413425][0.34747568 0.42223 0.49183849 0.5580048 0.61589295 0.66615736 0.69472647 0.69011551 0.64133513 0.56553262 0.5012961 0.45994565 0.43439743 0.40750721 0.37306532][0.35700247 0.44923258 0.53736675 0.61745912 0.68434459 0.73964828 0.76678383 0.75414121 0.6935541 0.60332793 0.52200371 0.46180341 0.42137048 0.38730794 0.35327804][0.37186623 0.47161877 0.56574833 0.64819968 0.71485204 0.77018583 0.79846692 0.78609282 0.72473317 0.63214922 0.54472864 0.47247681 0.41748795 0.37277937 0.33520126][0.39639303 0.49585676 0.58485353 0.65790218 0.71375757 0.76217008 0.79248 0.7885462 0.73765242 0.65456551 0.57174641 0.4949576 0.42689943 0.36841232 0.32248616][0.4150154 0.50633955 0.58083439 0.63568103 0.67356253 0.71002096 0.74156487 0.749663 0.71470296 0.6476115 0.57751566 0.50627393 0.43425858 0.36788657 0.31563416][0.39896384 0.47397012 0.52901304 0.5649246 0.58710241 0.61352074 0.64616048 0.6646198 0.64396179 0.59207582 0.53683025 0.47802773 0.41204819 0.34728861 0.29592291][0.33309233 0.38607106 0.42038557 0.44039655 0.45212629 0.47187191 0.50353968 0.52639115 0.51454967 0.4746114 0.43357605 0.3903158 0.33738124 0.28288019 0.24070491][0.22106972 0.25250679 0.27028814 0.27959558 0.28535908 0.29984233 0.3267194 0.34727177 0.33851922 0.30670163 0.27610496 0.24580616 0.20674159 0.16635606 0.13840352][0.099560812 0.11640023 0.12621534 0.13184682 0.13606125 0.1463843 0.1656725 0.17953375 0.17068918 0.14477491 0.12106165 0.098932974 0.070482858 0.042241652 0.026284492][0.0024292069 0.0093980916 0.015383474 0.020494893 0.025658304 0.034132671 0.047197811 0.054777261 0.045498583 0.023949727 0.0039694086 -0.013883868 -0.034447297 -0.052480433 -0.059387133]]...]
INFO - root - 2017-12-11 06:53:07.184671: step 29010, loss = 0.70, batch loss = 0.64 (13.5 examples/sec; 0.593 sec/batch; 49h:58m:52s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-SGD-lr0.001-clip50-initfromfixqian30k-fixqian
INFO - root - 2017-12-11 06:53:12.726748: step 29020, loss = 0.71, batch loss = 0.65 (14.6 examples/sec; 0.548 sec/batch; 46h:11m:25s remains)
INFO - root - 2017-12-11 06:53:18.154705: step 29030, loss = 0.70, batch loss = 0.64 (14.4 examples/sec; 0.554 sec/batch; 46h:42m:16s remains)
INFO - root - 2017-12-11 06:53:23.671688: step 29040, loss = 0.69, batch loss = 0.63 (14.6 examples/sec; 0.549 sec/batch; 46h:16m:31s remains)
INFO - root - 2017-12-11 06:53:29.227945: step 29050, loss = 0.71, batch loss = 0.65 (14.3 examples/sec; 0.559 sec/batch; 47h:09m:29s remains)
INFO - root - 2017-12-11 06:53:34.715319: step 29060, loss = 0.70, batch loss = 0.64 (14.5 examples/sec; 0.552 sec/batch; 46h:29m:54s remains)
INFO - root - 2017-12-11 06:53:40.162605: step 29070, loss = 0.69, batch loss = 0.63 (14.7 examples/sec; 0.545 sec/batch; 45h:58m:24s remains)
INFO - root - 2017-12-11 06:53:45.698183: step 29080, loss = 0.70, batch loss = 0.64 (14.0 examples/sec; 0.572 sec/batch; 48h:14m:38s remains)
INFO - root - 2017-12-11 06:53:51.191210: step 29090, loss = 0.68, batch loss = 0.62 (14.7 examples/sec; 0.546 sec/batch; 45h:59m:16s remains)
INFO - root - 2017-12-11 06:53:56.752700: step 29100, loss = 0.69, batch loss = 0.63 (14.4 examples/sec; 0.554 sec/batch; 46h:43m:25s remains)
2017-12-11 06:53:57.348634: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.057187006 -0.054958306 -0.053129442 -0.051583193 -0.050861172 -0.052234262 -0.055866282 -0.06107948 -0.065968044 -0.069678232 -0.071115449 -0.067980796 -0.060259454 -0.05095008 -0.0427784][-0.04260686 -0.034973256 -0.028016077 -0.022608636 -0.019716099 -0.021162739 -0.027241603 -0.036770843 -0.0479593 -0.059730977 -0.069600433 -0.073102541 -0.068580851 -0.059573926 -0.049105063][-0.021894235 -0.0055600759 0.010984187 0.025952986 0.0375602 0.041554779 0.035961259 0.022009594 0.0010125871 -0.025441155 -0.051945891 -0.070655949 -0.077126086 -0.074047834 -0.064108074][0.0053960439 0.031683832 0.060716584 0.090497568 0.11759154 0.13277385 0.13093996 0.11306705 0.080382526 0.035438973 -0.01313168 -0.05306888 -0.076397717 -0.084255025 -0.078662157][0.039498124 0.0765797 0.12029626 0.16879578 0.21581645 0.24611945 0.2516312 0.23296401 0.19098867 0.12855798 0.057014313 -0.0066193086 -0.049920451 -0.071965471 -0.074868381][0.082126178 0.13056467 0.18882957 0.255056 0.3197394 0.36374193 0.37842733 0.3647826 0.32151911 0.24882561 0.15901014 0.074143358 0.012352455 -0.023963632 -0.037700769][0.13167854 0.19036226 0.2588667 0.33537996 0.40903431 0.46155748 0.4866963 0.48497367 0.45111212 0.37941083 0.28078017 0.1816518 0.10595576 0.057979725 0.032913454][0.17811778 0.2422687 0.31267121 0.38825774 0.45916221 0.51336014 0.54884988 0.5642947 0.54884255 0.4893364 0.39409134 0.29223415 0.21112254 0.15653847 0.12201053][0.21534353 0.28116456 0.34674129 0.41146868 0.46813247 0.51391029 0.5525592 0.58191115 0.58736014 0.55020571 0.47384906 0.38392308 0.30698511 0.25033921 0.20814641][0.2404009 0.30463827 0.36126482 0.41028464 0.44726142 0.47763386 0.51064688 0.54638141 0.57012212 0.55996495 0.5125789 0.4441337 0.37591574 0.31753305 0.26778823][0.25616035 0.3146348 0.35987854 0.39349815 0.41229776 0.42599332 0.4476569 0.48335147 0.5218538 0.53836304 0.52092957 0.4738237 0.41116902 0.34586635 0.28481537][0.27402717 0.32427493 0.35832545 0.37997645 0.38502002 0.38360125 0.39071083 0.42063624 0.46792126 0.50461066 0.50976372 0.47774312 0.41606796 0.33948836 0.2627193][0.30033094 0.34231839 0.36818141 0.38403043 0.38228527 0.37040231 0.36297888 0.38185218 0.4274717 0.47059828 0.48500386 0.46025103 0.39879447 0.31305885 0.22218955][0.3327449 0.36788681 0.38825735 0.40261841 0.40053913 0.38524783 0.36841458 0.37475282 0.40915835 0.44456515 0.4547717 0.42992193 0.36912766 0.27894765 0.17900807][0.36436233 0.39344478 0.40883002 0.42288515 0.42482319 0.414175 0.39735878 0.39569283 0.4155589 0.43450674 0.4310199 0.39911819 0.3368552 0.24482584 0.14090525]]...]
INFO - root - 2017-12-11 06:54:02.557934: step 29110, loss = 0.72, batch loss = 0.66 (14.4 examples/sec; 0.556 sec/batch; 46h:52m:47s remains)
INFO - root - 2017-12-11 06:54:08.075477: step 29120, loss = 0.70, batch loss = 0.64 (14.4 examples/sec; 0.555 sec/batch; 46h:48m:30s remains)
INFO - root - 2017-12-11 06:54:13.544675: step 29130, loss = 0.70, batch loss = 0.65 (14.8 examples/sec; 0.541 sec/batch; 45h:37m:26s remains)
INFO - root - 2017-12-11 06:54:19.178954: step 29140, loss = 0.68, batch loss = 0.62 (13.8 examples/sec; 0.580 sec/batch; 48h:54m:01s remains)
INFO - root - 2017-12-11 06:54:24.675679: step 29150, loss = 0.69, batch loss = 0.64 (14.9 examples/sec; 0.537 sec/batch; 45h:15m:01s remains)
INFO - root - 2017-12-11 06:54:30.149384: step 29160, loss = 0.68, batch loss = 0.63 (14.9 examples/sec; 0.538 sec/batch; 45h:18m:28s remains)
INFO - root - 2017-12-11 06:54:35.610667: step 29170, loss = 0.69, batch loss = 0.63 (14.4 examples/sec; 0.555 sec/batch; 46h:44m:43s remains)
INFO - root - 2017-12-11 06:54:41.124356: step 29180, loss = 0.70, batch loss = 0.64 (14.6 examples/sec; 0.547 sec/batch; 46h:06m:29s remains)
INFO - root - 2017-12-11 06:54:46.630430: step 29190, loss = 0.70, batch loss = 0.64 (14.8 examples/sec; 0.542 sec/batch; 45h:40m:59s remains)
INFO - root - 2017-12-11 06:54:52.078207: step 29200, loss = 0.69, batch loss = 0.63 (14.5 examples/sec; 0.550 sec/batch; 46h:20m:35s remains)
2017-12-11 06:54:52.695284: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.04887981 0.11932316 0.2041253 0.2715303 0.31306055 0.34144509 0.35545036 0.3456198 0.31570521 0.29058924 0.28028896 0.28531343 0.29547551 0.30784437 0.35156479][0.079370961 0.17978612 0.30144578 0.40182325 0.46722478 0.51610154 0.54190797 0.52934414 0.48523816 0.4493815 0.43326387 0.43458873 0.44348538 0.46118647 0.52667934][0.097211659 0.22200324 0.37216809 0.49798453 0.58105892 0.64447963 0.67481828 0.65551972 0.59993893 0.56047964 0.547391 0.55080026 0.55865675 0.574922 0.64251667][0.10335451 0.24225929 0.40884668 0.55006164 0.64212853 0.71139371 0.74187714 0.71817327 0.65854043 0.62202853 0.61818194 0.62646949 0.62945193 0.62972474 0.66928446][0.1099797 0.2593188 0.4401727 0.59637922 0.69691575 0.77085209 0.80565786 0.78706962 0.73230153 0.69799745 0.6973604 0.70173085 0.687219 0.65258139 0.63827544][0.11674297 0.2772254 0.47504836 0.65050316 0.76317239 0.84604591 0.89383566 0.88924193 0.84356487 0.80798328 0.80237991 0.79204255 0.74651873 0.66218507 0.58136415][0.12217867 0.29604176 0.51234168 0.70858306 0.83555591 0.93200076 1.0012089 1.0161917 0.98192364 0.94407463 0.93092597 0.90292042 0.82172364 0.68588507 0.543746][0.12735574 0.31094649 0.53691745 0.743257 0.87905055 0.98835063 1.0789797 1.1102828 1.0839801 1.0435183 1.0254824 0.98509222 0.87527066 0.70064873 0.51906133][0.12753671 0.3040444 0.51677966 0.7103827 0.83993995 0.95098209 1.0511076 1.089532 1.0677098 1.0287864 1.0137724 0.97247356 0.85246843 0.666126 0.47966531][0.11017959 0.26157427 0.44037402 0.60131836 0.70969725 0.8064639 0.89657807 0.92866653 0.90695989 0.87385297 0.86836052 0.83704031 0.72706544 0.55707043 0.39498436][0.063187793 0.17730936 0.31148604 0.43123141 0.5109244 0.58183944 0.64686012 0.66372526 0.64147854 0.61755556 0.62293726 0.60614455 0.51920891 0.38362247 0.26081893][-0.0038034974 0.071274281 0.16184226 0.24259569 0.29467797 0.33861604 0.37530038 0.37518233 0.351479 0.33378634 0.34255624 0.33418965 0.27129972 0.17563812 0.09645731][-0.065350823 -0.025354344 0.026968556 0.073476419 0.10125785 0.12195271 0.13534693 0.12365787 0.10016189 0.0850925 0.08979284 0.083127864 0.041187413 -0.01681116 -0.05611404][-0.10241842 -0.0888818 -0.065653645 -0.04562391 -0.037193529 -0.034381259 -0.036870379 -0.053444903 -0.074408554 -0.08801911 -0.088582136 -0.094003245 -0.11658942 -0.14299698 -0.15302004][-0.11802725 -0.12124247 -0.11662479 -0.11294696 -0.11530886 -0.12154902 -0.13109379 -0.14699225 -0.1634599 -0.1748026 -0.17834528 -0.18166138 -0.1898064 -0.1964052 -0.19276433]]...]
INFO - root - 2017-12-11 06:54:57.755703: step 29210, loss = 0.68, batch loss = 0.62 (14.7 examples/sec; 0.545 sec/batch; 45h:56m:41s remains)
INFO - root - 2017-12-11 06:55:03.252802: step 29220, loss = 0.71, batch loss = 0.66 (14.6 examples/sec; 0.547 sec/batch; 46h:03m:27s remains)
INFO - root - 2017-12-11 06:55:08.767470: step 29230, loss = 0.69, batch loss = 0.63 (14.3 examples/sec; 0.558 sec/batch; 47h:01m:38s remains)
INFO - root - 2017-12-11 06:55:14.199871: step 29240, loss = 0.71, batch loss = 0.65 (14.9 examples/sec; 0.538 sec/batch; 45h:16m:54s remains)
INFO - root - 2017-12-11 06:55:19.729917: step 29250, loss = 0.67, batch loss = 0.61 (14.6 examples/sec; 0.549 sec/batch; 46h:16m:59s remains)
INFO - root - 2017-12-11 06:55:25.301706: step 29260, loss = 0.69, batch loss = 0.64 (14.6 examples/sec; 0.547 sec/batch; 46h:05m:45s remains)
INFO - root - 2017-12-11 06:55:30.794084: step 29270, loss = 0.71, batch loss = 0.65 (14.8 examples/sec; 0.542 sec/batch; 45h:40m:25s remains)
INFO - root - 2017-12-11 06:55:36.274108: step 29280, loss = 0.70, batch loss = 0.64 (14.8 examples/sec; 0.541 sec/batch; 45h:34m:17s remains)
INFO - root - 2017-12-11 06:55:41.764042: step 29290, loss = 0.71, batch loss = 0.65 (14.4 examples/sec; 0.555 sec/batch; 46h:42m:37s remains)
INFO - root - 2017-12-11 06:55:47.135355: step 29300, loss = 0.70, batch loss = 0.64 (26.4 examples/sec; 0.303 sec/batch; 25h:33m:18s remains)
2017-12-11 06:55:47.670076: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.055628758 -0.044747092 -0.032115549 -0.025315626 -0.024965219 -0.026761075 -0.029159809 -0.034511961 -0.042477991 -0.05001647 -0.056087565 -0.0658193 -0.079947546 -0.092174925 -0.09799692][-0.02332489 0.0044360543 0.036745533 0.060269579 0.069969408 0.073713429 0.073704183 0.063693926 0.042936109 0.019836947 0.00075501326 -0.022172889 -0.050851367 -0.076708063 -0.091558605][0.021802174 0.076811768 0.1428325 0.1980176 0.2281801 0.2453416 0.25383267 0.24104616 0.20292602 0.15492021 0.11385506 0.069084421 0.016423643 -0.032741677 -0.065122895][0.073670045 0.16417871 0.27473757 0.37307838 0.43306547 0.47223461 0.49787313 0.48823151 0.43342736 0.35598969 0.28601247 0.20961231 0.12108147 0.03686912 -0.022806527][0.126125 0.25485793 0.41282114 0.55596536 0.6468302 0.71172231 0.76077271 0.7591579 0.68841654 0.579826 0.47804463 0.36436236 0.23268694 0.10777248 0.018246949][0.16758735 0.33002955 0.52781922 0.70568615 0.81906259 0.9059875 0.97639042 0.98104084 0.8939898 0.75731194 0.62820417 0.48172227 0.31313655 0.1559217 0.044768665][0.1948377 0.38105935 0.60247695 0.79468548 0.91347319 1.0090142 1.086964 1.0876118 0.98383057 0.8277843 0.68281311 0.51817226 0.33126813 0.16187364 0.045450229][0.20769311 0.40433642 0.62865657 0.81020296 0.912423 0.99643546 1.0626408 1.0476242 0.92974871 0.76760685 0.62297457 0.45977321 0.27675587 0.11672298 0.012152802][0.20332733 0.39327815 0.597948 0.745867 0.81248903 0.8654241 0.90219241 0.86464477 0.74056876 0.58859026 0.46142808 0.31989095 0.16320947 0.032634981 -0.045637041][0.17194894 0.33607158 0.501349 0.60222661 0.6263715 0.64048171 0.64255035 0.58670938 0.46975654 0.34339079 0.24683268 0.14199995 0.028059252 -0.06035335 -0.10498493][0.10573052 0.2284651 0.34347332 0.39841452 0.39064652 0.37671125 0.35567823 0.29608461 0.20105428 0.10998919 0.047883917 -0.01728219 -0.0854552 -0.13151008 -0.14569132][0.020550417 0.09678293 0.16300315 0.18345486 0.16133802 0.13731512 0.11061381 0.062004678 -0.0031395131 -0.05894845 -0.09171363 -0.12418354 -0.15527861 -0.16924101 -0.16305596][-0.057049166 -0.02122617 0.007868153 0.008937085 -0.013143044 -0.033786975 -0.054025766 -0.0845265 -0.12102091 -0.14893354 -0.16135281 -0.17168131 -0.17872187 -0.174495 -0.15967506][-0.1104487 -0.10237587 -0.095187716 -0.10048591 -0.11461377 -0.12592883 -0.13571064 -0.14907368 -0.16365883 -0.17282322 -0.1733595 -0.17150512 -0.16661653 -0.15601847 -0.1414367][-0.13344753 -0.13973111 -0.14175965 -0.14611302 -0.15115969 -0.15336891 -0.15426902 -0.15610188 -0.15800662 -0.15762781 -0.15355685 -0.14782152 -0.14027132 -0.13079113 -0.12056439]]...]
INFO - root - 2017-12-11 06:55:53.137519: step 29310, loss = 0.70, batch loss = 0.64 (15.1 examples/sec; 0.531 sec/batch; 44h:42m:20s remains)
INFO - root - 2017-12-11 06:55:58.691446: step 29320, loss = 0.70, batch loss = 0.64 (14.2 examples/sec; 0.562 sec/batch; 47h:18m:02s remains)
INFO - root - 2017-12-11 06:56:04.260914: step 29330, loss = 0.71, batch loss = 0.65 (14.4 examples/sec; 0.556 sec/batch; 46h:48m:57s remains)
INFO - root - 2017-12-11 06:56:09.826194: step 29340, loss = 0.71, batch loss = 0.65 (14.3 examples/sec; 0.561 sec/batch; 47h:15m:19s remains)
INFO - root - 2017-12-11 06:56:15.337253: step 29350, loss = 0.70, batch loss = 0.64 (14.8 examples/sec; 0.540 sec/batch; 45h:25m:57s remains)
INFO - root - 2017-12-11 06:56:20.831756: step 29360, loss = 0.71, batch loss = 0.65 (14.3 examples/sec; 0.561 sec/batch; 47h:13m:29s remains)
INFO - root - 2017-12-11 06:56:26.242568: step 29370, loss = 0.72, batch loss = 0.66 (14.9 examples/sec; 0.536 sec/batch; 45h:07m:18s remains)
INFO - root - 2017-12-11 06:56:31.796254: step 29380, loss = 0.69, batch loss = 0.63 (14.4 examples/sec; 0.554 sec/batch; 46h:37m:14s remains)
INFO - root - 2017-12-11 06:56:37.217833: step 29390, loss = 0.71, batch loss = 0.65 (14.7 examples/sec; 0.545 sec/batch; 45h:53m:10s remains)
INFO - root - 2017-12-11 06:56:42.411324: step 29400, loss = 0.67, batch loss = 0.61 (14.0 examples/sec; 0.570 sec/batch; 48h:00m:41s remains)
2017-12-11 06:56:42.949315: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.15828183 0.13766341 0.11449102 0.098802604 0.093542367 0.095255911 0.10285955 0.11650544 0.13794763 0.16935697 0.21322529 0.26728979 0.31866181 0.3515774 0.36120516][0.26299608 0.24896397 0.22339827 0.20271364 0.19523926 0.19928223 0.21025693 0.22284581 0.2360383 0.25335497 0.280709 0.320361 0.36138833 0.3863439 0.38750333][0.33864114 0.33200037 0.30678591 0.28548756 0.28150326 0.29401681 0.31329957 0.32592818 0.32669666 0.3204115 0.31820279 0.32935828 0.34798312 0.35791144 0.34922543][0.3552075 0.35593441 0.33693781 0.32443795 0.33425468 0.36431247 0.3980903 0.41332605 0.39984709 0.36449769 0.32522902 0.30029389 0.29118532 0.28444898 0.26892874][0.30356532 0.31291041 0.30860052 0.31723031 0.35314271 0.40980798 0.46249565 0.48061445 0.44965938 0.3799811 0.29897809 0.23529078 0.19843128 0.1776637 0.16038309][0.19989756 0.22075288 0.23978099 0.2807503 0.35263443 0.44117534 0.51248688 0.5295949 0.4763822 0.36913228 0.24683155 0.14906311 0.09101674 0.062981412 0.049703319][0.083451971 0.11567257 0.16085173 0.23687352 0.34497461 0.46145651 0.54433852 0.55398607 0.4768106 0.33649457 0.18300444 0.064186864 -0.0031897279 -0.030402444 -0.036207803][-0.016018234 0.023893835 0.090381518 0.19464329 0.3296811 0.46295136 0.547175 0.54410523 0.44621441 0.28446236 0.11628689 -0.0072360309 -0.071585126 -0.091598906 -0.089219309][-0.080019578 -0.03738784 0.040735979 0.15999711 0.30714339 0.44369048 0.52039689 0.50333804 0.39216033 0.22331244 0.056776304 -0.05815421 -0.11137766 -0.12141734 -0.11193021][-0.10409207 -0.064748921 0.012147757 0.1285525 0.2685647 0.39282963 0.45475507 0.42646286 0.31383646 0.15493129 0.0063040471 -0.089417137 -0.12740286 -0.12790944 -0.11403194][-0.098827921 -0.068256132 -0.0047472348 0.091901414 0.20683052 0.30529994 0.34782752 0.31364971 0.21254712 0.079820834 -0.0371291 -0.10584081 -0.12645845 -0.11883399 -0.10321259][-0.082963154 -0.0648974 -0.022546867 0.043693323 0.12261704 0.18807819 0.21081756 0.17723431 0.098269269 0.0025640745 -0.075234339 -0.11429358 -0.11838539 -0.10488614 -0.089581683][-0.072450988 -0.066029623 -0.04498985 -0.0096267518 0.03319218 0.067001931 0.073666096 0.045926485 -0.00658089 -0.0639071 -0.1047359 -0.11863997 -0.11112378 -0.0953935 -0.082032122][-0.068759426 -0.070051767 -0.064156376 -0.051638313 -0.035896875 -0.025341535 -0.028818466 -0.048829548 -0.078134112 -0.10490577 -0.11870267 -0.11652724 -0.10385287 -0.089299023 -0.078805074][-0.067330189 -0.072648816 -0.075703681 -0.077309206 -0.078393266 -0.08172866 -0.089895181 -0.10248018 -0.11480172 -0.12136538 -0.11904316 -0.10903878 -0.0958565 -0.084327176 -0.076963656]]...]
INFO - root - 2017-12-11 06:56:48.468768: step 29410, loss = 0.69, batch loss = 0.63 (14.3 examples/sec; 0.559 sec/batch; 47h:02m:59s remains)
INFO - root - 2017-12-11 06:56:53.897553: step 29420, loss = 0.70, batch loss = 0.64 (14.6 examples/sec; 0.549 sec/batch; 46h:13m:50s remains)
INFO - root - 2017-12-11 06:56:59.466331: step 29430, loss = 0.69, batch loss = 0.63 (14.0 examples/sec; 0.570 sec/batch; 47h:59m:45s remains)
INFO - root - 2017-12-11 06:57:05.020531: step 29440, loss = 0.69, batch loss = 0.63 (14.3 examples/sec; 0.560 sec/batch; 47h:09m:34s remains)
INFO - root - 2017-12-11 06:57:10.516205: step 29450, loss = 0.69, batch loss = 0.63 (14.4 examples/sec; 0.554 sec/batch; 46h:36m:47s remains)
INFO - root - 2017-12-11 06:57:16.034367: step 29460, loss = 0.69, batch loss = 0.63 (14.1 examples/sec; 0.569 sec/batch; 47h:52m:06s remains)
INFO - root - 2017-12-11 06:57:21.565862: step 29470, loss = 0.69, batch loss = 0.63 (14.6 examples/sec; 0.548 sec/batch; 46h:08m:09s remains)
INFO - root - 2017-12-11 06:57:27.084028: step 29480, loss = 0.70, batch loss = 0.64 (14.4 examples/sec; 0.556 sec/batch; 46h:50m:27s remains)
INFO - root - 2017-12-11 06:57:32.615190: step 29490, loss = 0.70, batch loss = 0.64 (13.9 examples/sec; 0.576 sec/batch; 48h:28m:59s remains)
INFO - root - 2017-12-11 06:57:37.691088: step 29500, loss = 0.70, batch loss = 0.64 (14.9 examples/sec; 0.536 sec/batch; 45h:06m:20s remains)
2017-12-11 06:57:38.264149: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.19491422 0.24950916 0.27816793 0.28106871 0.26362526 0.24252264 0.23660658 0.25032517 0.26663357 0.27843446 0.28106865 0.27822956 0.27219948 0.26513839 0.27527803][0.2528837 0.32034895 0.35924658 0.3720459 0.36569062 0.35534993 0.35856855 0.37870273 0.39783192 0.40858987 0.405578 0.39551216 0.38112563 0.36438566 0.36341926][0.30842048 0.38881379 0.43776578 0.45980793 0.46391878 0.46437007 0.47645083 0.50225532 0.522766 0.52869344 0.51358151 0.48795021 0.4569861 0.42428127 0.40743214][0.35666171 0.44845638 0.50594664 0.53524375 0.54701591 0.55576861 0.57434213 0.6036371 0.62373519 0.62251759 0.59155512 0.54321986 0.48793343 0.43403059 0.39935726][0.38825506 0.48876053 0.55393583 0.58978617 0.60712034 0.62121743 0.64197296 0.66958892 0.68490177 0.67396784 0.62622 0.55312961 0.4712894 0.39627552 0.3472617][0.40456352 0.51083452 0.58355445 0.62791103 0.65321922 0.67438978 0.69671375 0.71816719 0.72184283 0.6945641 0.62633842 0.52741909 0.42138565 0.33164006 0.27810335][0.41391793 0.52183032 0.5989396 0.65012032 0.68248886 0.7092008 0.73035145 0.740826 0.72688484 0.67883021 0.59154052 0.47580454 0.36004066 0.27188662 0.22935757][0.41729772 0.52092427 0.59615141 0.64864844 0.6833505 0.7112059 0.72862065 0.72785419 0.69735235 0.63220561 0.53511196 0.41879681 0.31302094 0.24463028 0.22701962][0.40167487 0.4943192 0.5609659 0.60858971 0.64020938 0.66475934 0.67758024 0.66883355 0.62829787 0.55575776 0.46236578 0.36226386 0.28254804 0.2453351 0.25788155][0.35127375 0.42573261 0.47761968 0.5152027 0.54022932 0.55937833 0.56807083 0.55516142 0.511451 0.44051388 0.36032289 0.28606904 0.23946059 0.23551513 0.27496979][0.26026094 0.31123641 0.34458679 0.36985672 0.38834947 0.40364906 0.41058868 0.39778611 0.35773265 0.29657906 0.23574854 0.19004145 0.17469707 0.19720539 0.25273526][0.14628717 0.173178 0.18762361 0.19870359 0.20782603 0.21641533 0.21990661 0.2089716 0.17863551 0.13556834 0.099345267 0.081367545 0.090091847 0.12710145 0.18451951][0.0370436 0.044395067 0.044416223 0.043783486 0.043850902 0.045321856 0.045541041 0.038223613 0.020391641 -0.0028995529 -0.017383793 -0.01645359 0.00323893 0.040681515 0.088280745][-0.05081873 -0.056061517 -0.063331217 -0.069527127 -0.07405284 -0.076676138 -0.078243136 -0.082205877 -0.090220734 -0.099275969 -0.10073759 -0.092036381 -0.072824009 -0.044437528 -0.012094251][-0.10962143 -0.12234251 -0.1326717 -0.14079538 -0.14672394 -0.15075083 -0.15305333 -0.15516248 -0.15775609 -0.15929367 -0.15547414 -0.14598384 -0.13181417 -0.11410006 -0.095672265]]...]
INFO - root - 2017-12-11 06:57:43.822420: step 29510, loss = 0.69, batch loss = 0.63 (14.0 examples/sec; 0.570 sec/batch; 48h:00m:05s remains)
INFO - root - 2017-12-11 06:57:49.314696: step 29520, loss = 0.69, batch loss = 0.64 (14.2 examples/sec; 0.563 sec/batch; 47h:24m:12s remains)
INFO - root - 2017-12-11 06:57:54.879790: step 29530, loss = 0.69, batch loss = 0.64 (13.8 examples/sec; 0.580 sec/batch; 48h:49m:05s remains)
INFO - root - 2017-12-11 06:58:00.416871: step 29540, loss = 0.69, batch loss = 0.63 (14.9 examples/sec; 0.536 sec/batch; 45h:05m:27s remains)
INFO - root - 2017-12-11 06:58:05.923734: step 29550, loss = 0.70, batch loss = 0.64 (14.5 examples/sec; 0.553 sec/batch; 46h:29m:45s remains)
INFO - root - 2017-12-11 06:58:11.468750: step 29560, loss = 0.68, batch loss = 0.63 (14.4 examples/sec; 0.556 sec/batch; 46h:49m:42s remains)
INFO - root - 2017-12-11 06:58:16.965348: step 29570, loss = 0.70, batch loss = 0.64 (14.8 examples/sec; 0.540 sec/batch; 45h:25m:24s remains)
INFO - root - 2017-12-11 06:58:22.523643: step 29580, loss = 0.69, batch loss = 0.63 (14.6 examples/sec; 0.550 sec/batch; 46h:14m:21s remains)
INFO - root - 2017-12-11 06:58:28.064346: step 29590, loss = 0.69, batch loss = 0.63 (14.7 examples/sec; 0.545 sec/batch; 45h:50m:10s remains)
INFO - root - 2017-12-11 06:58:33.197980: step 29600, loss = 0.71, batch loss = 0.65 (14.6 examples/sec; 0.549 sec/batch; 46h:10m:09s remains)
2017-12-11 06:58:33.780571: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.022356691 0.030081106 0.029140649 0.0220652 0.018533185 0.017781597 0.018077787 0.022777097 0.033587806 0.046608225 0.050805192 0.044519309 0.0321515 0.015101253 -0.0070169657][0.070560291 0.088639483 0.093736105 0.089255288 0.086332843 0.082972474 0.078529648 0.080882907 0.09348461 0.11087084 0.11792051 0.11191424 0.097851753 0.075633571 0.043811686][0.12417589 0.15645112 0.17341794 0.17877986 0.18263912 0.1807909 0.1740098 0.17395592 0.18422709 0.19927244 0.20392986 0.19513527 0.17689146 0.14717239 0.10436718][0.16924512 0.21919729 0.25466198 0.27782676 0.29474896 0.29850528 0.29208216 0.28957555 0.29324725 0.29868048 0.29426903 0.27785048 0.25118047 0.2108852 0.15643653][0.20203267 0.27198061 0.33036488 0.37633047 0.41094267 0.42484158 0.42272508 0.41956589 0.41516474 0.40669671 0.38838932 0.35942024 0.31896675 0.26379037 0.19650492][0.23111279 0.31873596 0.39681411 0.46215877 0.51343679 0.53967285 0.54560608 0.54507536 0.53408521 0.51194781 0.47869393 0.43492776 0.37729153 0.30447438 0.22455262][0.25659123 0.35629445 0.44639638 0.522198 0.58327919 0.617485 0.62900895 0.630486 0.61473739 0.58273637 0.53794867 0.48214427 0.40966639 0.32221186 0.23329942][0.26536268 0.36827523 0.45986694 0.53525931 0.59685969 0.63176847 0.64335489 0.64393914 0.62604165 0.59123737 0.54298043 0.4832727 0.40515298 0.31283545 0.2228514][0.25297374 0.3494508 0.43224245 0.49708748 0.54957438 0.57680351 0.58150518 0.57683569 0.558865 0.52867752 0.48622951 0.43267787 0.36147866 0.2778126 0.19686848][0.21445549 0.29494715 0.36013037 0.40690127 0.4433699 0.45769489 0.45237535 0.44128159 0.42515779 0.40413636 0.37334055 0.33275118 0.2779997 0.21431939 0.15228993][0.15153788 0.21011004 0.25387642 0.28037924 0.29838994 0.29823494 0.28264642 0.26513463 0.25039816 0.237968 0.21945466 0.19403774 0.16006318 0.12239994 0.085674413][0.075504579 0.1114749 0.13557623 0.1450315 0.14709292 0.13522959 0.11259123 0.090600386 0.075938053 0.068637654 0.060120929 0.048646364 0.034289032 0.021260431 0.0093241138][0.0029258681 0.018988231 0.02768486 0.026096495 0.018838992 0.0026335532 -0.0195463 -0.040088575 -0.05298385 -0.057336826 -0.058907993 -0.060013004 -0.06027675 -0.056458265 -0.051244348][-0.049601726 -0.04773334 -0.048484478 -0.054955333 -0.065038748 -0.079603076 -0.09611737 -0.11088741 -0.12020493 -0.12289292 -0.12116417 -0.11692309 -0.11029001 -0.099352032 -0.086976223][-0.077249713 -0.082833171 -0.087421849 -0.094244584 -0.10304067 -0.11363882 -0.12419059 -0.13319883 -0.13895439 -0.14059879 -0.13819659 -0.13266352 -0.12450579 -0.11317455 -0.10063799]]...]
INFO - root - 2017-12-11 06:58:39.257949: step 29610, loss = 0.71, batch loss = 0.65 (14.6 examples/sec; 0.549 sec/batch; 46h:10m:14s remains)
INFO - root - 2017-12-11 06:58:44.719739: step 29620, loss = 0.69, batch loss = 0.64 (14.6 examples/sec; 0.550 sec/batch; 46h:14m:44s remains)
INFO - root - 2017-12-11 06:58:50.216436: step 29630, loss = 0.70, batch loss = 0.64 (14.8 examples/sec; 0.542 sec/batch; 45h:34m:49s remains)
INFO - root - 2017-12-11 06:58:55.818138: step 29640, loss = 0.71, batch loss = 0.65 (14.4 examples/sec; 0.556 sec/batch; 46h:48m:06s remains)
INFO - root - 2017-12-11 06:59:01.268145: step 29650, loss = 0.68, batch loss = 0.62 (14.7 examples/sec; 0.544 sec/batch; 45h:48m:17s remains)
INFO - root - 2017-12-11 06:59:06.765066: step 29660, loss = 0.70, batch loss = 0.64 (14.8 examples/sec; 0.542 sec/batch; 45h:34m:46s remains)
INFO - root - 2017-12-11 06:59:12.282370: step 29670, loss = 0.71, batch loss = 0.65 (14.4 examples/sec; 0.554 sec/batch; 46h:35m:08s remains)
INFO - root - 2017-12-11 06:59:17.813992: step 29680, loss = 0.69, batch loss = 0.63 (14.7 examples/sec; 0.545 sec/batch; 45h:48m:34s remains)
INFO - root - 2017-12-11 06:59:22.901467: step 29690, loss = 0.71, batch loss = 0.65 (16.8 examples/sec; 0.477 sec/batch; 40h:09m:12s remains)
INFO - root - 2017-12-11 06:59:28.501912: step 29700, loss = 0.70, batch loss = 0.65 (14.5 examples/sec; 0.553 sec/batch; 46h:30m:48s remains)
2017-12-11 06:59:29.105688: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.30214515 0.22956897 0.16494288 0.13498878 0.14365897 0.16819648 0.19058338 0.2101225 0.23581837 0.26321906 0.28341132 0.29523286 0.29317775 0.27513227 0.24283153][0.3270663 0.24418776 0.16946797 0.13386649 0.14229411 0.17056003 0.19805054 0.22197734 0.2491966 0.27987409 0.30856192 0.33168912 0.33888838 0.32428366 0.28948972][0.33093178 0.24650669 0.1713929 0.14003922 0.15669756 0.19553694 0.2328327 0.26124549 0.28384107 0.30693513 0.33195943 0.35819569 0.37265018 0.364764 0.33460635][0.30737463 0.23059151 0.1672696 0.15167958 0.18543129 0.24077559 0.29123628 0.32403731 0.33773389 0.344112 0.35340229 0.37128735 0.38649777 0.38604054 0.36617485][0.25548857 0.19165857 0.14599326 0.14974961 0.20122094 0.27223349 0.33540621 0.37330142 0.37924135 0.367276 0.3548536 0.35543784 0.36264563 0.36472631 0.35569516][0.19253923 0.14480896 0.11775796 0.13828921 0.20238373 0.28375459 0.35602295 0.39861721 0.39960688 0.37199721 0.33746529 0.31595504 0.30754891 0.30465096 0.30120102][0.13976477 0.10955342 0.099216767 0.13108243 0.20033756 0.28443909 0.35990182 0.4047372 0.40211836 0.36131698 0.30565253 0.2595911 0.22984636 0.21462668 0.21030645][0.10496633 0.092069216 0.096077181 0.13438872 0.20199916 0.281055 0.35296664 0.3965514 0.39181185 0.34302565 0.2718522 0.20414256 0.15220371 0.12055457 0.10859998][0.097197473 0.10011467 0.11505451 0.15518391 0.21553837 0.28266382 0.34363595 0.38062313 0.37392664 0.32340106 0.24633811 0.1661649 0.0977069 0.049985856 0.02583378][0.11489498 0.12638272 0.14556718 0.18325557 0.23452638 0.28869519 0.33702204 0.36568123 0.35757002 0.31002718 0.234928 0.15082416 0.072701819 0.012217454 -0.025015706][0.15145265 0.16787525 0.18851085 0.22345328 0.26690528 0.30945703 0.34490553 0.36361328 0.35231885 0.30838215 0.2391395 0.15683277 0.075023539 0.0069190068 -0.040187668][0.19053574 0.21072748 0.23298888 0.26671726 0.30445924 0.33689994 0.35935655 0.36644784 0.34897146 0.30635449 0.24362341 0.16689722 0.08725924 0.018151345 -0.032296214][0.21631078 0.23685715 0.25845802 0.28898829 0.32016781 0.34299946 0.35433221 0.3520712 0.33057162 0.29146346 0.23812981 0.17129119 0.099232167 0.03486767 -0.013132378][0.22516325 0.2460295 0.2668151 0.29369724 0.31872767 0.33386347 0.33745465 0.32940713 0.3065109 0.27164283 0.22705759 0.17017749 0.10704061 0.049944185 0.007884766][0.22577903 0.24531417 0.2633478 0.28402933 0.30107284 0.30863902 0.306855 0.29698762 0.27673969 0.24843724 0.21276826 0.16560759 0.11133239 0.061986752 0.027282342]]...]
INFO - root - 2017-12-11 06:59:34.622767: step 29710, loss = 0.70, batch loss = 0.64 (14.9 examples/sec; 0.536 sec/batch; 45h:03m:17s remains)
INFO - root - 2017-12-11 06:59:40.159665: step 29720, loss = 0.71, batch loss = 0.65 (14.5 examples/sec; 0.552 sec/batch; 46h:26m:58s remains)
INFO - root - 2017-12-11 06:59:45.639666: step 29730, loss = 0.71, batch loss = 0.65 (14.6 examples/sec; 0.549 sec/batch; 46h:11m:14s remains)
INFO - root - 2017-12-11 06:59:51.159988: step 29740, loss = 0.71, batch loss = 0.65 (14.4 examples/sec; 0.555 sec/batch; 46h:41m:27s remains)
INFO - root - 2017-12-11 06:59:56.795241: step 29750, loss = 0.71, batch loss = 0.65 (14.7 examples/sec; 0.544 sec/batch; 45h:47m:00s remains)
INFO - root - 2017-12-11 07:00:02.283077: step 29760, loss = 0.71, batch loss = 0.65 (15.0 examples/sec; 0.535 sec/batch; 44h:57m:03s remains)
INFO - root - 2017-12-11 07:00:07.760112: step 29770, loss = 0.69, batch loss = 0.63 (14.3 examples/sec; 0.558 sec/batch; 46h:53m:16s remains)
INFO - root - 2017-12-11 07:00:13.308397: step 29780, loss = 0.69, batch loss = 0.63 (14.2 examples/sec; 0.563 sec/batch; 47h:21m:28s remains)
INFO - root - 2017-12-11 07:00:18.526323: step 29790, loss = 0.69, batch loss = 0.63 (14.1 examples/sec; 0.569 sec/batch; 47h:50m:46s remains)
INFO - root - 2017-12-11 07:00:24.145619: step 29800, loss = 0.71, batch loss = 0.65 (14.4 examples/sec; 0.557 sec/batch; 46h:50m:17s remains)
2017-12-11 07:00:24.807067: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.064836614 -0.060170218 -0.052402332 -0.043452479 -0.034680407 -0.028602585 -0.026404565 -0.027967408 -0.03271905 -0.038439386 -0.043316431 -0.046352122 -0.046597056 -0.044686634 -0.042055145][-0.071097784 -0.06647303 -0.060134947 -0.053884067 -0.047858395 -0.042978428 -0.039622359 -0.037984356 -0.038472231 -0.040444285 -0.042983737 -0.045588206 -0.04687205 -0.046404283 -0.04471245][-0.074440174 -0.069951862 -0.064355709 -0.059492819 -0.054078966 -0.047163047 -0.038464364 -0.029543564 -0.022868145 -0.01984385 -0.020957647 -0.026205661 -0.033277083 -0.039149232 -0.04238832][-0.075314261 -0.069463074 -0.061325271 -0.0532008 -0.043259908 -0.029347219 -0.011212886 0.0077277892 0.022499939 0.029451448 0.026742233 0.014285451 -0.0040909904 -0.021853529 -0.034601718][-0.07005918 -0.060896453 -0.044890583 -0.025337994 -0.0013792173 0.02843692 0.062433209 0.094156817 0.11530678 0.1200155 0.1065629 0.076904684 0.038269166 0.0016479817 -0.025025534][-0.05758848 -0.042043652 -0.011661729 0.02953982 0.080054373 0.13716052 0.1935844 0.23756959 0.25711122 0.24587791 0.2062653 0.14661986 0.079877183 0.020951103 -0.019720407][-0.036045488 -0.0094003305 0.041614436 0.11259343 0.19841652 0.28902423 0.36770698 0.41575295 0.41928774 0.37577423 0.29649323 0.19958827 0.10460897 0.02788727 -0.021149965][-0.0054991841 0.035177805 0.10805582 0.20795433 0.325458 0.4421294 0.53168225 0.56987196 0.54441643 0.46062759 0.34059465 0.21203245 0.099008948 0.015938882 -0.032048218][0.023022417 0.076165326 0.16413343 0.27995807 0.41124019 0.53426611 0.61778927 0.63638639 0.5822497 0.46865985 0.32483962 0.18336472 0.06907703 -0.0074249273 -0.046187274][0.034554925 0.093677446 0.18301924 0.2940788 0.41492757 0.52242315 0.58720034 0.58706224 0.51902604 0.40027267 0.26010132 0.12945932 0.030207148 -0.030790636 -0.057103444][0.030882509 0.08718361 0.16276897 0.24918956 0.33924335 0.41640574 0.45877928 0.44943032 0.38756409 0.2886771 0.17585926 0.073578753 -0.0012708283 -0.044320606 -0.059984684][0.023136029 0.0695329 0.12201106 0.17408089 0.225334 0.26865926 0.29177287 0.28273305 0.23979369 0.17301171 0.096974388 0.028236531 -0.021395639 -0.048868511 -0.057594486][0.016994713 0.050245609 0.078742772 0.098811649 0.11597337 0.13137767 0.1408516 0.13625778 0.11357863 0.07672216 0.032920022 -0.00781608 -0.037446558 -0.053628054 -0.05838925][0.0092322482 0.029309504 0.03864013 0.036231324 0.030965636 0.028723624 0.03005472 0.029321846 0.020794084 0.0040552625 -0.018144352 -0.039875466 -0.055307169 -0.062666 -0.063205644][0.0039076656 0.013584185 0.010646727 -0.004096041 -0.020951157 -0.03239242 -0.035624076 -0.034369774 -0.035078034 -0.0405472 -0.05030844 -0.060646206 -0.066602439 -0.066450238 -0.061017048]]...]
INFO - root - 2017-12-11 07:00:30.225559: step 29810, loss = 0.69, batch loss = 0.63 (14.6 examples/sec; 0.547 sec/batch; 45h:59m:50s remains)
INFO - root - 2017-12-11 07:00:35.793076: step 29820, loss = 0.70, batch loss = 0.64 (14.7 examples/sec; 0.543 sec/batch; 45h:41m:39s remains)
INFO - root - 2017-12-11 07:00:41.297638: step 29830, loss = 0.70, batch loss = 0.64 (14.5 examples/sec; 0.550 sec/batch; 46h:15m:11s remains)
INFO - root - 2017-12-11 07:00:46.784433: step 29840, loss = 0.69, batch loss = 0.63 (14.6 examples/sec; 0.547 sec/batch; 45h:58m:23s remains)
INFO - root - 2017-12-11 07:00:52.261317: step 29850, loss = 0.71, batch loss = 0.65 (14.4 examples/sec; 0.554 sec/batch; 46h:36m:06s remains)
INFO - root - 2017-12-11 07:00:57.847056: step 29860, loss = 0.71, batch loss = 0.65 (14.6 examples/sec; 0.548 sec/batch; 46h:06m:10s remains)
INFO - root - 2017-12-11 07:01:03.373390: step 29870, loss = 0.70, batch loss = 0.64 (15.1 examples/sec; 0.530 sec/batch; 44h:34m:04s remains)
INFO - root - 2017-12-11 07:01:08.815389: step 29880, loss = 0.71, batch loss = 0.65 (15.2 examples/sec; 0.528 sec/batch; 44h:22m:39s remains)
INFO - root - 2017-12-11 07:01:14.058323: step 29890, loss = 0.71, batch loss = 0.65 (13.9 examples/sec; 0.576 sec/batch; 48h:23m:19s remains)
INFO - root - 2017-12-11 07:01:19.547961: step 29900, loss = 0.69, batch loss = 0.63 (14.9 examples/sec; 0.539 sec/batch; 45h:16m:00s remains)
2017-12-11 07:01:20.186736: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.20182709 0.19890557 0.17957695 0.16433148 0.17098774 0.20037208 0.23074359 0.24042729 0.22121961 0.1889305 0.17255022 0.18508829 0.2189347 0.251015 0.25954562][0.17142561 0.16579221 0.15945311 0.16919361 0.20397194 0.25665843 0.29980627 0.30994704 0.28107375 0.23590407 0.20931011 0.21760184 0.25267732 0.28649044 0.29244906][0.15572508 0.14106376 0.14015524 0.16645497 0.22157969 0.29170138 0.34441185 0.35491055 0.31870142 0.26430762 0.23026739 0.23515438 0.27100179 0.30599695 0.31023344][0.17699109 0.14863394 0.14216371 0.17125593 0.23439501 0.31391311 0.37343806 0.38590211 0.34599838 0.2847521 0.24205399 0.23971771 0.27150309 0.30497885 0.30976087][0.22684467 0.18626042 0.16736732 0.18825719 0.24780324 0.32739455 0.38881063 0.40299383 0.3621347 0.29644147 0.24482255 0.23137198 0.25378826 0.28328425 0.29213968][0.29398122 0.24975407 0.21843129 0.22411564 0.2695002 0.33803231 0.39255953 0.40344331 0.36060971 0.29168728 0.23304343 0.20973411 0.22265132 0.2480129 0.26236367][0.35367805 0.31808963 0.28346902 0.27741387 0.30759802 0.36136284 0.40534732 0.41031164 0.36433676 0.29156575 0.22595601 0.19395728 0.19875523 0.22087871 0.23938625][0.37717667 0.36090052 0.33660898 0.33102274 0.3547003 0.3987827 0.43485031 0.43486923 0.38557178 0.30767018 0.23357923 0.19173893 0.18796551 0.20629603 0.22633649][0.35344294 0.35725927 0.35036451 0.35562444 0.38294649 0.42521742 0.45855817 0.456836 0.40683085 0.32602885 0.245261 0.19482228 0.18356037 0.19890538 0.22050016][0.30429775 0.32360584 0.33383015 0.35271323 0.38696507 0.4295916 0.46116474 0.45880073 0.41090339 0.33197024 0.25041702 0.19694 0.18323444 0.19993146 0.22616054][0.25298461 0.2806806 0.30220082 0.33041564 0.36868134 0.41029739 0.43980572 0.4379701 0.3950983 0.32302126 0.24677052 0.1956024 0.183401 0.20346195 0.23498237][0.21236058 0.24048845 0.26506186 0.29512244 0.33218822 0.37140968 0.40087003 0.4037526 0.37044454 0.30987781 0.24269804 0.19538096 0.1844527 0.20727462 0.24484409][0.19502789 0.22172868 0.24419168 0.26831865 0.2952444 0.32476985 0.35018507 0.35726926 0.33549917 0.28964356 0.23420452 0.19115813 0.17970774 0.20346549 0.24800502][0.20180129 0.22862025 0.24676044 0.25957868 0.26809978 0.27809188 0.29032496 0.29574624 0.28454411 0.25624824 0.21722318 0.18244243 0.17194746 0.19571836 0.24574046][0.22646864 0.25712919 0.272166 0.27251589 0.25977179 0.24529044 0.23770845 0.23469962 0.22949053 0.21761523 0.19769908 0.17639655 0.17121159 0.19501007 0.24505007]]...]
INFO - root - 2017-12-11 07:01:25.705011: step 29910, loss = 0.73, batch loss = 0.67 (14.9 examples/sec; 0.538 sec/batch; 45h:14m:17s remains)
INFO - root - 2017-12-11 07:01:31.246649: step 29920, loss = 0.72, batch loss = 0.66 (14.2 examples/sec; 0.563 sec/batch; 47h:18m:59s remains)
INFO - root - 2017-12-11 07:01:36.768076: step 29930, loss = 0.70, batch loss = 0.64 (14.5 examples/sec; 0.552 sec/batch; 46h:22m:28s remains)
INFO - root - 2017-12-11 07:01:42.267624: step 29940, loss = 0.71, batch loss = 0.65 (14.9 examples/sec; 0.535 sec/batch; 44h:59m:00s remains)
INFO - root - 2017-12-11 07:01:47.906149: step 29950, loss = 0.71, batch loss = 0.65 (14.6 examples/sec; 0.547 sec/batch; 45h:59m:58s remains)
INFO - root - 2017-12-11 07:01:53.354671: step 29960, loss = 0.70, batch loss = 0.64 (14.8 examples/sec; 0.542 sec/batch; 45h:31m:57s remains)
INFO - root - 2017-12-11 07:01:58.911852: step 29970, loss = 0.69, batch loss = 0.63 (14.3 examples/sec; 0.558 sec/batch; 46h:51m:54s remains)
INFO - root - 2017-12-11 07:02:04.135665: step 29980, loss = 0.69, batch loss = 0.63 (18.1 examples/sec; 0.441 sec/batch; 37h:05m:28s remains)
INFO - root - 2017-12-11 07:02:09.661273: step 29990, loss = 0.69, batch loss = 0.64 (14.1 examples/sec; 0.569 sec/batch; 47h:46m:49s remains)
INFO - root - 2017-12-11 07:02:15.221253: step 30000, loss = 0.69, batch loss = 0.63 (14.5 examples/sec; 0.553 sec/batch; 46h:28m:40s remains)
2017-12-11 07:02:15.799058: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.30704916 0.29301685 0.25709403 0.21259053 0.16217382 0.11020394 0.068413854 0.039755233 0.019479128 -0.0044348831 -0.033861548 -0.05625093 -0.062790662 -0.049553875 -0.022835581][0.32184181 0.29192597 0.24136347 0.18545787 0.12617616 0.063074432 0.0015276376 -0.051916096 -0.092587315 -0.1248477 -0.14942603 -0.15741247 -0.14630187 -0.11861658 -0.083001532][0.32106534 0.2866323 0.23683602 0.18813927 0.14002939 0.083706744 0.016926553 -0.052407365 -0.11111576 -0.15547483 -0.18160774 -0.18349746 -0.16202812 -0.12430762 -0.081737436][0.29072034 0.26416227 0.23297565 0.21189515 0.19677089 0.17032222 0.12175465 0.055952817 -0.011219399 -0.069679543 -0.10896088 -0.12133104 -0.10697751 -0.0734405 -0.031047776][0.22616048 0.21822429 0.22090384 0.24410559 0.27813649 0.29784873 0.28502992 0.23887001 0.17242925 0.10038991 0.040799446 0.0043760836 -0.0053216023 0.0097860191 0.044151764][0.14304484 0.16179781 0.20748907 0.28111202 0.36737004 0.43553817 0.46255782 0.44161257 0.38095772 0.29939261 0.22155365 0.15999708 0.12116436 0.11290433 0.13581005][0.078139529 0.12538643 0.21161461 0.32730657 0.45296615 0.5560962 0.61124629 0.60847259 0.55380493 0.47165537 0.38968015 0.3165558 0.25813472 0.23222071 0.24424976][0.06299492 0.13199554 0.24481276 0.38406697 0.52725893 0.64260083 0.70458847 0.70532173 0.65424329 0.58271366 0.51600295 0.45296726 0.39367208 0.3602339 0.35952187][0.097416587 0.17685838 0.29656681 0.43742067 0.57561779 0.680455 0.72864246 0.71899432 0.66849208 0.61454225 0.57586068 0.53945094 0.49720272 0.46799672 0.45654169][0.15140808 0.225045 0.3299225 0.44945052 0.56139541 0.63902694 0.6633777 0.63907462 0.58930933 0.55414689 0.54604912 0.54409003 0.53230721 0.51925832 0.50490707][0.18188892 0.23332298 0.30510682 0.38590616 0.45821071 0.50235367 0.504469 0.47176859 0.42874989 0.4129439 0.43168098 0.46174967 0.48290512 0.4926092 0.48621148][0.17081223 0.18902019 0.2193065 0.25647759 0.2883912 0.3033154 0.29233724 0.26195973 0.23234175 0.23383334 0.26982704 0.31962305 0.36365798 0.39356673 0.40340677][0.14251058 0.12446588 0.11509704 0.11477968 0.11553031 0.11212046 0.098807305 0.077651307 0.060839184 0.070994996 0.11064251 0.16437842 0.21823779 0.26360303 0.29491088][0.14072248 0.093169816 0.055584107 0.0324745 0.017787721 0.010754934 0.0044179144 -0.0073799393 -0.021677291 -0.019521181 0.00644951 0.049085461 0.10253835 0.16032842 0.21544322][0.19707184 0.13654377 0.089597821 0.061721507 0.04709354 0.047018073 0.050109271 0.040961906 0.014925747 -0.0075484309 -0.0096566221 0.012016567 0.058835972 0.12478929 0.19785574]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-SGD-lr0.001-clip50-initfromfixqian30k-fixqian/model.ckpt-30000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-SGD-lr0.001-clip50-initfromfixqian30k-fixqian/model.ckpt-30000 is not in all_model_checkpoint_paths. Manually adding it.
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-SGD-lr0.001-clip50-initfromfixqian30k-fixqian
INFO - root - 2017-12-11 07:02:21.961207: step 30010, loss = 0.67, batch loss = 0.62 (14.6 examples/sec; 0.549 sec/batch; 46h:05m:57s remains)
INFO - root - 2017-12-11 07:02:27.446616: step 30020, loss = 0.71, batch loss = 0.65 (14.6 examples/sec; 0.547 sec/batch; 45h:57m:32s remains)
INFO - root - 2017-12-11 07:02:33.021267: step 30030, loss = 0.71, batch loss = 0.65 (13.6 examples/sec; 0.587 sec/batch; 49h:19m:18s remains)
INFO - root - 2017-12-11 07:02:38.630832: step 30040, loss = 0.70, batch loss = 0.64 (14.4 examples/sec; 0.555 sec/batch; 46h:35m:35s remains)
INFO - root - 2017-12-11 07:02:44.073451: step 30050, loss = 0.69, batch loss = 0.63 (15.2 examples/sec; 0.527 sec/batch; 44h:15m:38s remains)
INFO - root - 2017-12-11 07:02:49.517805: step 30060, loss = 0.68, batch loss = 0.62 (14.7 examples/sec; 0.544 sec/batch; 45h:39m:37s remains)
INFO - root - 2017-12-11 07:02:54.989559: step 30070, loss = 0.70, batch loss = 0.64 (14.3 examples/sec; 0.558 sec/batch; 46h:50m:17s remains)
INFO - root - 2017-12-11 07:03:00.190428: step 30080, loss = 0.71, batch loss = 0.65 (14.6 examples/sec; 0.549 sec/batch; 46h:07m:31s remains)
INFO - root - 2017-12-11 07:03:05.654481: step 30090, loss = 0.72, batch loss = 0.66 (14.7 examples/sec; 0.544 sec/batch; 45h:42m:05s remains)
INFO - root - 2017-12-11 07:03:11.111490: step 30100, loss = 0.70, batch loss = 0.64 (14.7 examples/sec; 0.545 sec/batch; 45h:45m:47s remains)
2017-12-11 07:03:11.710303: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.21578938 0.22435758 0.21985529 0.21428068 0.21442208 0.23021315 0.25351638 0.2688342 0.27442083 0.28158084 0.29162028 0.29825789 0.29486111 0.28567758 0.27564207][0.23591639 0.24832653 0.24644488 0.24369593 0.24900688 0.26984388 0.29764503 0.31737033 0.32716423 0.33744776 0.34899661 0.35565144 0.35275763 0.34366804 0.33122405][0.27537456 0.29532713 0.29646617 0.29202735 0.29342306 0.30890241 0.3334752 0.35355306 0.36653784 0.3782917 0.38976392 0.39676636 0.39649177 0.38999939 0.37604189][0.34978908 0.38313425 0.38930073 0.37790132 0.3647061 0.36336872 0.37746176 0.39512494 0.41074079 0.42376381 0.43569428 0.44464821 0.44784436 0.44238797 0.42428413][0.44254395 0.49157539 0.50528884 0.48559907 0.45253384 0.42768684 0.42658854 0.4396559 0.45712495 0.47225446 0.48676002 0.49982333 0.50515258 0.4958787 0.46986941][0.53927273 0.59909296 0.61828476 0.58998168 0.5371542 0.4895936 0.47438532 0.483759 0.50419468 0.52417654 0.54492408 0.56406289 0.56906033 0.55031163 0.51334113][0.61802548 0.67579746 0.69060683 0.65030664 0.5813759 0.52048016 0.50043607 0.51192379 0.53827667 0.56562054 0.59399313 0.61877215 0.6216262 0.59180617 0.54567409][0.6483888 0.69072551 0.68873817 0.63163835 0.55108839 0.48812813 0.47263491 0.489641 0.52126652 0.5552721 0.59039897 0.619699 0.62190384 0.58666456 0.53967595][0.603951 0.62500012 0.60140312 0.52785051 0.44108221 0.383253 0.37536827 0.39626655 0.4288967 0.46508989 0.50170928 0.53113306 0.53398514 0.50227869 0.46521211][0.47761595 0.48204058 0.44436 0.36454359 0.28072852 0.23206453 0.22977453 0.24898644 0.27564836 0.30549446 0.33422944 0.35573581 0.35719761 0.33507693 0.31449673][0.30646023 0.30378485 0.26548469 0.19416404 0.12343636 0.086287849 0.086298525 0.098871008 0.11390653 0.13014771 0.14365305 0.15109156 0.14806046 0.13517553 0.12967449][0.13070241 0.12679012 0.098735236 0.0490226 0.0009881116 -0.02222182 -0.021029668 -0.015350454 -0.011290313 -0.0087382533 -0.010341034 -0.0162446 -0.024514211 -0.031480648 -0.027809426][-0.013166333 -0.017252183 -0.031206924 -0.054584995 -0.076336585 -0.084427752 -0.080946796 -0.078337066 -0.079759695 -0.085082509 -0.094903409 -0.10703848 -0.11709855 -0.12086542 -0.11478027][-0.095359437 -0.0998475 -0.10208795 -0.1055527 -0.10765059 -0.10437618 -0.09783 -0.094322443 -0.095834851 -0.10251869 -0.11319894 -0.12488416 -0.13364327 -0.13660792 -0.13295941][-0.11107964 -0.11567356 -0.11232619 -0.10733014 -0.10145999 -0.094559632 -0.087885104 -0.084233813 -0.084728494 -0.08947596 -0.09692391 -0.10492634 -0.11116023 -0.11406162 -0.1134709]]...]
INFO - root - 2017-12-11 07:03:17.240829: step 30110, loss = 0.70, batch loss = 0.64 (14.5 examples/sec; 0.550 sec/batch; 46h:13m:56s remains)
INFO - root - 2017-12-11 07:03:22.725078: step 30120, loss = 0.70, batch loss = 0.64 (14.4 examples/sec; 0.554 sec/batch; 46h:34m:04s remains)
INFO - root - 2017-12-11 07:03:28.372789: step 30130, loss = 0.69, batch loss = 0.63 (13.8 examples/sec; 0.578 sec/batch; 48h:30m:54s remains)
INFO - root - 2017-12-11 07:03:33.908525: step 30140, loss = 0.68, batch loss = 0.62 (14.4 examples/sec; 0.555 sec/batch; 46h:34m:35s remains)
INFO - root - 2017-12-11 07:03:39.307702: step 30150, loss = 0.71, batch loss = 0.65 (14.7 examples/sec; 0.544 sec/batch; 45h:40m:05s remains)
INFO - root - 2017-12-11 07:03:44.736966: step 30160, loss = 0.70, batch loss = 0.64 (14.6 examples/sec; 0.549 sec/batch; 46h:06m:53s remains)
INFO - root - 2017-12-11 07:03:50.413204: step 30170, loss = 0.69, batch loss = 0.63 (14.1 examples/sec; 0.567 sec/batch; 47h:39m:31s remains)
INFO - root - 2017-12-11 07:03:55.763453: step 30180, loss = 0.70, batch loss = 0.64 (14.5 examples/sec; 0.552 sec/batch; 46h:21m:08s remains)
INFO - root - 2017-12-11 07:04:01.300549: step 30190, loss = 0.69, batch loss = 0.63 (14.4 examples/sec; 0.554 sec/batch; 46h:32m:17s remains)
INFO - root - 2017-12-11 07:04:06.749282: step 30200, loss = 0.70, batch loss = 0.64 (14.4 examples/sec; 0.557 sec/batch; 46h:45m:25s remains)
2017-12-11 07:04:07.400451: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.18522602 0.25355092 0.29610652 0.30765885 0.3016634 0.29475951 0.30190709 0.33695716 0.39518863 0.45492691 0.49057332 0.49654749 0.47870067 0.44702855 0.40553415][0.19220866 0.27101809 0.32366449 0.33983716 0.32851851 0.3051627 0.28879461 0.3000133 0.34186158 0.397066 0.44009128 0.45888531 0.45182991 0.42743993 0.39100954][0.17710233 0.26140538 0.32308784 0.34730428 0.33734456 0.30538681 0.27199355 0.2620036 0.28492868 0.32868588 0.37113565 0.39594907 0.39610159 0.37855804 0.34924567][0.15376964 0.24078897 0.31127283 0.34685135 0.34553549 0.31461343 0.27299225 0.24684538 0.25136933 0.28049955 0.31673664 0.34300068 0.3489455 0.33841425 0.3153218][0.1358261 0.2278589 0.312199 0.36798477 0.38700086 0.36820486 0.3254368 0.28526866 0.26980776 0.28052008 0.30644736 0.33232477 0.344587 0.34111851 0.32154617][0.13216993 0.23467076 0.33978581 0.424134 0.47129795 0.46953902 0.42714265 0.37278494 0.33677623 0.32892272 0.34479487 0.37129867 0.39011812 0.39176986 0.37146911][0.1422767 0.25942004 0.38776174 0.50004321 0.57080609 0.57991922 0.53439432 0.46746388 0.41842446 0.40329969 0.41943082 0.45157182 0.47566798 0.47687343 0.44811952][0.1593629 0.28911546 0.43297896 0.55938929 0.63718629 0.64454335 0.59297585 0.52153772 0.47448561 0.46853009 0.49644274 0.5386216 0.56702322 0.56369925 0.52205825][0.16896108 0.30026552 0.44154727 0.55937457 0.62388849 0.61926907 0.56469089 0.50161409 0.47037736 0.48211366 0.52295023 0.57253444 0.6034081 0.596418 0.5462786][0.14889923 0.26536727 0.38532674 0.47805843 0.52010232 0.50455523 0.45480311 0.40841615 0.39478704 0.41787177 0.46216252 0.51132286 0.54154104 0.533545 0.48342669][0.090983488 0.177672 0.264352 0.32631055 0.34803268 0.32943416 0.29167756 0.26329494 0.26201731 0.28743097 0.32660753 0.36843011 0.39382148 0.38537738 0.34147492][0.015587758 0.066375509 0.11682034 0.14961381 0.15618438 0.13956766 0.11511602 0.10143366 0.10812885 0.13301225 0.16616617 0.1996403 0.21870841 0.20991106 0.17477292][-0.047125559 -0.027024239 -0.00600097 0.0053531029 0.0031953088 -0.0091841808 -0.022667043 -0.026621431 -0.016731005 0.00452565 0.029862065 0.054023229 0.067155823 0.060112063 0.036350664][-0.083904043 -0.082918011 -0.0793513 -0.079346009 -0.084528923 -0.092538148 -0.098777331 -0.098178886 -0.089655764 -0.075429186 -0.060114656 -0.04644211 -0.039376266 -0.043914642 -0.056622542][-0.098189086 -0.10653841 -0.11102866 -0.11534466 -0.120157 -0.12422863 -0.12592669 -0.12369464 -0.11816631 -0.11071025 -0.10378887 -0.098457739 -0.096181251 -0.098846108 -0.10433501]]...]
INFO - root - 2017-12-11 07:04:12.978394: step 30210, loss = 0.69, batch loss = 0.63 (14.2 examples/sec; 0.562 sec/batch; 47h:09m:55s remains)
INFO - root - 2017-12-11 07:04:18.518139: step 30220, loss = 0.72, batch loss = 0.66 (14.1 examples/sec; 0.566 sec/batch; 47h:31m:14s remains)
INFO - root - 2017-12-11 07:04:24.021267: step 30230, loss = 0.68, batch loss = 0.62 (14.9 examples/sec; 0.536 sec/batch; 44h:58m:37s remains)
INFO - root - 2017-12-11 07:04:29.533135: step 30240, loss = 0.71, batch loss = 0.65 (14.0 examples/sec; 0.571 sec/batch; 47h:55m:42s remains)
INFO - root - 2017-12-11 07:04:35.045589: step 30250, loss = 0.71, batch loss = 0.66 (14.7 examples/sec; 0.545 sec/batch; 45h:46m:43s remains)
INFO - root - 2017-12-11 07:04:40.595769: step 30260, loss = 0.71, batch loss = 0.65 (13.6 examples/sec; 0.590 sec/batch; 49h:29m:40s remains)
INFO - root - 2017-12-11 07:04:45.754029: step 30270, loss = 0.70, batch loss = 0.64 (14.5 examples/sec; 0.553 sec/batch; 46h:23m:10s remains)
INFO - root - 2017-12-11 07:04:51.239685: step 30280, loss = 0.71, batch loss = 0.65 (14.4 examples/sec; 0.557 sec/batch; 46h:46m:56s remains)
INFO - root - 2017-12-11 07:04:56.825720: step 30290, loss = 0.70, batch loss = 0.64 (14.4 examples/sec; 0.555 sec/batch; 46h:35m:15s remains)
INFO - root - 2017-12-11 07:05:02.334720: step 30300, loss = 0.70, batch loss = 0.64 (14.8 examples/sec; 0.539 sec/batch; 45h:14m:54s remains)
2017-12-11 07:05:02.977980: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.18982792 0.17724535 0.15497366 0.1275568 0.099204153 0.073697992 0.058381207 0.050760806 0.054117732 0.078712553 0.1327512 0.21265124 0.29528302 0.35321787 0.37797576][0.23061249 0.23568109 0.22865048 0.20945199 0.18228297 0.15187867 0.12895021 0.11282435 0.10952399 0.13084693 0.18154827 0.25345716 0.32190213 0.36186206 0.37006637][0.26377267 0.28670692 0.29424173 0.28299293 0.25889185 0.22733381 0.20179664 0.18208629 0.17488465 0.19072796 0.2284047 0.27808386 0.3187128 0.33325639 0.32425073][0.28955549 0.3244037 0.33951229 0.33107582 0.30921268 0.28100264 0.26150587 0.24895689 0.24666402 0.26024887 0.28146929 0.30210385 0.30922362 0.29690602 0.27283624][0.31644136 0.35655546 0.37162217 0.36006019 0.33796585 0.3152228 0.30700243 0.30797371 0.31504151 0.32712087 0.33011892 0.32098469 0.29803789 0.26684448 0.2366408][0.34100798 0.38040823 0.39107677 0.37545943 0.35457289 0.34043002 0.34591782 0.36088035 0.37489733 0.38111386 0.36400807 0.32917655 0.28764868 0.25354674 0.23318632][0.35102266 0.38277304 0.3852551 0.36636677 0.35031265 0.34940678 0.37148854 0.40024626 0.41815096 0.41497028 0.37824216 0.32392853 0.27577651 0.25203151 0.25268644][0.33574954 0.35438722 0.34786484 0.32906774 0.32257444 0.33886057 0.37932417 0.42143494 0.44225156 0.43082428 0.38034779 0.31480139 0.26760513 0.25691724 0.2767334][0.29722035 0.30353951 0.29199791 0.27804098 0.28382233 0.31663564 0.371735 0.42207992 0.44246829 0.42416388 0.36656398 0.29733443 0.25470498 0.25371736 0.28391826][0.24208018 0.24073462 0.23017237 0.22559619 0.24439429 0.28938583 0.35117519 0.40212509 0.41890666 0.39652362 0.33870712 0.27251229 0.23540273 0.23843168 0.26902911][0.17933354 0.17429896 0.16815333 0.17417075 0.20407516 0.25610659 0.31766438 0.36484694 0.37892202 0.35795939 0.30714288 0.24975827 0.21849869 0.22119492 0.24571061][0.12362215 0.11671128 0.11394604 0.12677994 0.16248935 0.21608335 0.27330714 0.31652173 0.33189002 0.31856251 0.28066689 0.23587129 0.2104224 0.21005856 0.22459705][0.084735237 0.076492421 0.075182758 0.090722211 0.12746778 0.17831782 0.2290604 0.2682997 0.28604609 0.28128937 0.25718141 0.22622132 0.20795466 0.20588756 0.21164539][0.069274418 0.05901413 0.055900317 0.068984151 0.10169228 0.14607227 0.18922131 0.22374454 0.24169935 0.24212639 0.22853565 0.21039493 0.20148927 0.20209505 0.20463605][0.073317051 0.060043115 0.052283727 0.058730327 0.083392106 0.11888696 0.15427579 0.18373655 0.19986308 0.20252515 0.19653052 0.18979068 0.19134112 0.19795616 0.20260273]]...]
INFO - root - 2017-12-11 07:05:08.545131: step 30310, loss = 0.69, batch loss = 0.63 (14.3 examples/sec; 0.560 sec/batch; 46h:59m:15s remains)
INFO - root - 2017-12-11 07:05:14.010452: step 30320, loss = 0.70, batch loss = 0.64 (14.9 examples/sec; 0.539 sec/batch; 45h:12m:15s remains)
INFO - root - 2017-12-11 07:05:19.463124: step 30330, loss = 0.69, batch loss = 0.63 (14.5 examples/sec; 0.553 sec/batch; 46h:22m:47s remains)
INFO - root - 2017-12-11 07:05:25.025825: step 30340, loss = 0.70, batch loss = 0.64 (14.5 examples/sec; 0.553 sec/batch; 46h:25m:18s remains)
INFO - root - 2017-12-11 07:05:30.560324: step 30350, loss = 0.71, batch loss = 0.65 (14.6 examples/sec; 0.547 sec/batch; 45h:53m:31s remains)
INFO - root - 2017-12-11 07:05:36.015043: step 30360, loss = 0.70, batch loss = 0.64 (15.2 examples/sec; 0.527 sec/batch; 44h:13m:25s remains)
INFO - root - 2017-12-11 07:05:41.195863: step 30370, loss = 0.68, batch loss = 0.62 (14.5 examples/sec; 0.552 sec/batch; 46h:20m:18s remains)
INFO - root - 2017-12-11 07:05:46.720159: step 30380, loss = 0.71, batch loss = 0.65 (14.4 examples/sec; 0.557 sec/batch; 46h:46m:07s remains)
INFO - root - 2017-12-11 07:05:52.210613: step 30390, loss = 0.70, batch loss = 0.64 (15.0 examples/sec; 0.534 sec/batch; 44h:49m:10s remains)
INFO - root - 2017-12-11 07:05:57.746346: step 30400, loss = 0.69, batch loss = 0.64 (14.4 examples/sec; 0.554 sec/batch; 46h:28m:48s remains)
2017-12-11 07:05:58.342757: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.079426482 0.10401323 0.11320371 0.10650507 0.088999279 0.065790191 0.041168 0.018153798 0.00058897358 -0.0086472118 -0.0079403687 0.00032209017 0.012483012 0.023918062 0.031520728][0.10239036 0.1360369 0.15556516 0.1576297 0.14553104 0.12286585 0.0915907 0.054969929 0.020410212 -0.0056602061 -0.01822209 -0.016847303 -0.0046345103 0.011493954 0.024703395][0.10159962 0.14335562 0.17798866 0.19882761 0.20511802 0.19543351 0.1659947 0.11888728 0.065776832 0.018593339 -0.013600176 -0.025857113 -0.019499043 -0.0027311821 0.014610983][0.082375415 0.12906119 0.18056586 0.22900085 0.26757756 0.28602389 0.27038059 0.21953917 0.14851871 0.076052785 0.017235843 -0.016882928 -0.024407903 -0.013444035 0.0041692089][0.057270873 0.10634261 0.17387089 0.25386235 0.33253533 0.38844389 0.39548105 0.34789553 0.26276729 0.16598932 0.080376461 0.024247518 0.0030867846 0.0071637728 0.023024384][0.032826785 0.081007972 0.1607253 0.26950932 0.38774961 0.48291034 0.5172528 0.48006135 0.38926029 0.27619296 0.1717959 0.10197065 0.073652834 0.073515333 0.086050086][0.015930954 0.061682981 0.14909767 0.2793324 0.42871597 0.55615664 0.61586404 0.5931558 0.50563973 0.38767281 0.27709547 0.2052127 0.17756145 0.17483643 0.18099229][0.01119323 0.056095965 0.14723188 0.28880414 0.45435292 0.59979874 0.676938 0.6686371 0.59117228 0.47901785 0.37456691 0.31020305 0.28764755 0.28199548 0.27787971][0.013574853 0.058975734 0.14989783 0.29102126 0.4549112 0.60062772 0.68399638 0.68744725 0.625106 0.53038037 0.44535679 0.39731392 0.38404211 0.37708637 0.36171159][0.012346314 0.053987574 0.13632268 0.2623539 0.40700424 0.537644 0.61841708 0.63332772 0.593126 0.52994126 0.47985286 0.45983616 0.463042 0.46042293 0.43740928][0.0040561832 0.0332383 0.094440266 0.18992415 0.30114728 0.40653759 0.47949439 0.50681454 0.49573597 0.47551358 0.47210559 0.48984137 0.51631391 0.52426529 0.50051552][-0.0059369816 0.0056658788 0.03838376 0.095439821 0.16648652 0.24098316 0.30212709 0.34074849 0.36030012 0.38389137 0.42640543 0.48140189 0.53212231 0.55547625 0.54032731][-0.012691346 -0.016764024 -0.010916833 0.0084282616 0.0388183 0.080134459 0.12659796 0.17337865 0.21992885 0.2804524 0.35911357 0.44206387 0.51079571 0.54901797 0.5488655][-0.014104661 -0.027876467 -0.041130632 -0.051768862 -0.055934824 -0.044636995 -0.013279595 0.037408035 0.10349371 0.19048986 0.29257733 0.39210865 0.4698776 0.51630896 0.52831119][-0.010646042 -0.027440194 -0.050529938 -0.079933524 -0.10761988 -0.11783144 -0.098100804 -0.045664269 0.032929886 0.13519186 0.24795549 0.35289517 0.4303802 0.47470486 0.48779753]]...]
INFO - root - 2017-12-11 07:06:03.859941: step 30410, loss = 0.68, batch loss = 0.62 (14.5 examples/sec; 0.550 sec/batch; 46h:09m:03s remains)
INFO - root - 2017-12-11 07:06:09.336418: step 30420, loss = 0.70, batch loss = 0.64 (14.8 examples/sec; 0.542 sec/batch; 45h:29m:46s remains)
INFO - root - 2017-12-11 07:06:14.932787: step 30430, loss = 0.69, batch loss = 0.63 (13.9 examples/sec; 0.575 sec/batch; 48h:15m:36s remains)
INFO - root - 2017-12-11 07:06:20.516683: step 30440, loss = 0.69, batch loss = 0.64 (14.4 examples/sec; 0.555 sec/batch; 46h:33m:15s remains)
INFO - root - 2017-12-11 07:06:25.975934: step 30450, loss = 0.68, batch loss = 0.62 (14.6 examples/sec; 0.549 sec/batch; 46h:01m:56s remains)
INFO - root - 2017-12-11 07:06:31.443120: step 30460, loss = 0.68, batch loss = 0.63 (14.8 examples/sec; 0.541 sec/batch; 45h:21m:53s remains)
INFO - root - 2017-12-11 07:06:36.556976: step 30470, loss = 0.69, batch loss = 0.63 (14.5 examples/sec; 0.551 sec/batch; 46h:15m:11s remains)
INFO - root - 2017-12-11 07:06:42.050569: step 30480, loss = 0.68, batch loss = 0.62 (14.9 examples/sec; 0.539 sec/batch; 45h:11m:37s remains)
INFO - root - 2017-12-11 07:06:47.466469: step 30490, loss = 0.70, batch loss = 0.64 (14.4 examples/sec; 0.557 sec/batch; 46h:45m:34s remains)
INFO - root - 2017-12-11 07:06:53.077657: step 30500, loss = 0.70, batch loss = 0.65 (14.1 examples/sec; 0.568 sec/batch; 47h:38m:14s remains)
2017-12-11 07:06:53.598380: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.040876828 -0.046780556 -0.051908944 -0.055328481 -0.056313995 -0.056027308 -0.056690242 -0.059980579 -0.065466434 -0.070976429 -0.073754169 -0.071895376 -0.06537275 -0.056122229 -0.047137965][-0.029427374 -0.031607319 -0.0317736 -0.028721645 -0.022168471 -0.015486891 -0.013331114 -0.018975159 -0.031543337 -0.047498759 -0.061706726 -0.068996876 -0.067099616 -0.05729045 -0.044049107][-0.022015102 -0.014853491 -0.0025626631 0.016223168 0.041658018 0.067404725 0.084261343 0.084956363 0.067986496 0.036992442 -0.00051523774 -0.033196382 -0.052111972 -0.054409 -0.044834398][-0.018705994 0.00090037542 0.030671567 0.072754547 0.12859537 0.188701 0.23758885 0.25945446 0.24578044 0.1974877 0.12469831 0.048648171 -0.0098217716 -0.039621208 -0.045010872][-0.014307618 0.017299652 0.065008685 0.13324085 0.22606315 0.3306815 0.42385137 0.478102 0.47382796 0.40725529 0.29223061 0.16270569 0.054466657 -0.011703813 -0.039514758][-0.0070889592 0.03302221 0.094655909 0.18452074 0.30930459 0.45321327 0.58665466 0.67194682 0.67889661 0.59851795 0.44820622 0.27327755 0.12181567 0.022669526 -0.027350588][-0.0021163789 0.041201372 0.10950276 0.20967168 0.34855548 0.50880831 0.65893233 0.75841928 0.77307791 0.69031036 0.52800727 0.3353 0.16476454 0.048741318 -0.014944673][-0.00742717 0.031438965 0.095138617 0.18827587 0.31557247 0.46049413 0.595329 0.68577391 0.702238 0.63109076 0.48645285 0.31187055 0.15489464 0.045758232 -0.016378336][-0.025638726 0.00048026277 0.046888329 0.11551938 0.20905235 0.31457815 0.41207552 0.47829691 0.49274036 0.44276345 0.33663106 0.20677643 0.089511558 0.0086212847 -0.036044527][-0.049029469 -0.039159231 -0.016042942 0.020446191 0.072013326 0.13104789 0.18599479 0.2243381 0.23462549 0.20674531 0.14392225 0.0668644 -0.0010617266 -0.04457261 -0.064248689][-0.066975474 -0.070671871 -0.067435592 -0.0579471 -0.040613063 -0.018581372 0.0027816277 0.018234475 0.023013227 0.010700379 -0.017830327 -0.051105198 -0.076980934 -0.088337451 -0.086774848][-0.070064396 -0.081910148 -0.0910488 -0.097841591 -0.1002728 -0.098980159 -0.096222661 -0.094082095 -0.093971178 -0.098764256 -0.1076918 -0.11494317 -0.11573707 -0.10861337 -0.095940664][-0.054757014 -0.070022829 -0.085365094 -0.099902831 -0.11089248 -0.11782385 -0.12192213 -0.12466498 -0.12676772 -0.12908481 -0.13047726 -0.12788337 -0.11994375 -0.10755544 -0.093031667][-0.028112486 -0.04375618 -0.061557367 -0.07903064 -0.092616886 -0.10140014 -0.10655838 -0.10986179 -0.11235673 -0.11430357 -0.11457051 -0.11131809 -0.10417117 -0.094200127 -0.0830127][-0.0026470567 -0.017346753 -0.035871532 -0.054303523 -0.0682798 -0.076699667 -0.081115186 -0.083773293 -0.086053371 -0.088194236 -0.089276493 -0.088055104 -0.084272817 -0.078528859 -0.071657538]]...]
INFO - root - 2017-12-11 07:06:59.050545: step 30510, loss = 0.71, batch loss = 0.65 (14.5 examples/sec; 0.552 sec/batch; 46h:17m:49s remains)
INFO - root - 2017-12-11 07:07:04.430252: step 30520, loss = 0.71, batch loss = 0.65 (14.5 examples/sec; 0.550 sec/batch; 46h:07m:34s remains)
INFO - root - 2017-12-11 07:07:09.999046: step 30530, loss = 0.69, batch loss = 0.63 (14.6 examples/sec; 0.548 sec/batch; 46h:00m:14s remains)
INFO - root - 2017-12-11 07:07:15.597636: step 30540, loss = 0.68, batch loss = 0.62 (14.4 examples/sec; 0.557 sec/batch; 46h:43m:47s remains)
INFO - root - 2017-12-11 07:07:21.062181: step 30550, loss = 0.69, batch loss = 0.63 (14.3 examples/sec; 0.560 sec/batch; 46h:58m:33s remains)
INFO - root - 2017-12-11 07:07:26.454789: step 30560, loss = 0.70, batch loss = 0.64 (16.1 examples/sec; 0.498 sec/batch; 41h:48m:03s remains)
INFO - root - 2017-12-11 07:07:31.693535: step 30570, loss = 0.68, batch loss = 0.62 (14.2 examples/sec; 0.565 sec/batch; 47h:24m:14s remains)
INFO - root - 2017-12-11 07:07:37.220203: step 30580, loss = 0.68, batch loss = 0.63 (14.9 examples/sec; 0.538 sec/batch; 45h:06m:50s remains)
INFO - root - 2017-12-11 07:07:42.739466: step 30590, loss = 0.69, batch loss = 0.63 (15.0 examples/sec; 0.535 sec/batch; 44h:49m:48s remains)
INFO - root - 2017-12-11 07:07:48.294996: step 30600, loss = 0.72, batch loss = 0.67 (14.7 examples/sec; 0.544 sec/batch; 45h:38m:02s remains)
2017-12-11 07:07:48.896727: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.13033307 0.15598422 0.16151631 0.15342145 0.13189276 0.10292774 0.07806728 0.064740524 0.062291782 0.067214832 0.085019164 0.10968854 0.1284661 0.13377528 0.12688522][0.10408977 0.1222646 0.12186882 0.10910051 0.086110383 0.059209827 0.037997272 0.027881291 0.027435627 0.0338209 0.051833339 0.076098241 0.095543563 0.10378039 0.10126822][0.076130837 0.089111023 0.086084463 0.071890689 0.049964312 0.026691468 0.0098142875 0.0031160552 0.0047767949 0.012165286 0.029688874 0.052798431 0.072577424 0.083958283 0.085911065][0.063587233 0.077466868 0.077551223 0.066498332 0.048027575 0.028455704 0.014504006 0.0097649479 0.012856351 0.020821432 0.03797077 0.060077883 0.079559363 0.092270531 0.095268652][0.073040679 0.096169464 0.10625196 0.10374532 0.091281787 0.074931033 0.061031427 0.055048592 0.05727331 0.064145036 0.080405764 0.10163924 0.12056611 0.13306904 0.13363957][0.10581252 0.14464387 0.1690207 0.17796847 0.17203288 0.15706986 0.13948905 0.12812544 0.12596756 0.1289506 0.14306894 0.16360627 0.18243895 0.1945377 0.191565][0.14971879 0.20483224 0.24295057 0.26205006 0.26039618 0.24386534 0.21940114 0.19933315 0.18990408 0.18722044 0.19943018 0.22107258 0.24197772 0.25517583 0.24959932][0.18633316 0.25269797 0.29927137 0.32364392 0.32264638 0.30220386 0.26999414 0.24107648 0.22478202 0.2178902 0.23030806 0.25548828 0.28063947 0.29623029 0.28918162][0.20226239 0.27086547 0.317616 0.3409659 0.33754829 0.3130292 0.27541295 0.24115166 0.22134571 0.21345633 0.22788876 0.25712478 0.2862736 0.3036336 0.29572454][0.18870173 0.24851793 0.28620893 0.3024511 0.2948961 0.26845852 0.23049696 0.19653581 0.17742975 0.17126286 0.18798731 0.21929929 0.24952555 0.26635686 0.25816089][0.14111367 0.18409041 0.20768842 0.21455197 0.20369422 0.17879368 0.14589292 0.11747997 0.1023882 0.099403486 0.11709214 0.14674115 0.17363895 0.18689771 0.17848478][0.067434721 0.091795906 0.10266817 0.10297737 0.092336968 0.072676577 0.048319113 0.028019931 0.01806335 0.017848216 0.033759352 0.057627037 0.077373832 0.084850378 0.076070629][-0.00741384 0.0021670065 0.004881802 0.0028465462 -0.0044002715 -0.016703492 -0.031492818 -0.043280635 -0.048124418 -0.04661553 -0.034865007 -0.019324472 -0.0084429774 -0.0072612506 -0.016527299][-0.060106464 -0.05963666 -0.060596514 -0.062514685 -0.066046938 -0.071941316 -0.079009868 -0.084261894 -0.085619874 -0.0839018 -0.077215225 -0.069764279 -0.066498369 -0.069781631 -0.078612857][-0.087243557 -0.090569466 -0.091857336 -0.092570193 -0.093416288 -0.095163926 -0.097422108 -0.098898888 -0.098776661 -0.097815558 -0.095289417 -0.093452409 -0.094735 -0.099798925 -0.10728543]]...]
INFO - root - 2017-12-11 07:07:54.405434: step 30610, loss = 0.70, batch loss = 0.64 (14.7 examples/sec; 0.543 sec/batch; 45h:32m:24s remains)
INFO - root - 2017-12-11 07:07:59.881908: step 30620, loss = 0.69, batch loss = 0.63 (14.3 examples/sec; 0.558 sec/batch; 46h:45m:30s remains)
INFO - root - 2017-12-11 07:08:05.475014: step 30630, loss = 0.70, batch loss = 0.64 (14.7 examples/sec; 0.545 sec/batch; 45h:41m:19s remains)
INFO - root - 2017-12-11 07:08:10.984557: step 30640, loss = 0.69, batch loss = 0.64 (14.8 examples/sec; 0.540 sec/batch; 45h:15m:51s remains)
INFO - root - 2017-12-11 07:08:16.497623: step 30650, loss = 0.69, batch loss = 0.63 (14.8 examples/sec; 0.541 sec/batch; 45h:19m:31s remains)
INFO - root - 2017-12-11 07:08:21.773789: step 30660, loss = 0.69, batch loss = 0.63 (14.8 examples/sec; 0.539 sec/batch; 45h:10m:20s remains)
INFO - root - 2017-12-11 07:08:27.318630: step 30670, loss = 0.69, batch loss = 0.63 (14.5 examples/sec; 0.553 sec/batch; 46h:20m:59s remains)
INFO - root - 2017-12-11 07:08:32.785231: step 30680, loss = 0.68, batch loss = 0.62 (14.9 examples/sec; 0.539 sec/batch; 45h:09m:11s remains)
INFO - root - 2017-12-11 07:08:38.309316: step 30690, loss = 0.68, batch loss = 0.62 (14.1 examples/sec; 0.568 sec/batch; 47h:37m:55s remains)
INFO - root - 2017-12-11 07:08:43.889086: step 30700, loss = 0.69, batch loss = 0.63 (14.8 examples/sec; 0.540 sec/batch; 45h:14m:24s remains)
2017-12-11 07:08:44.462633: I tensorflow/core/kernels/logging_ops.cc:79] [[[1.2729407e-05 0.012426728 0.016351104 0.010547332 0.0015423467 -0.0073874895 -0.012374022 -0.011469956 -0.0046972181 0.0045951996 0.011874806 0.016330017 0.017014392 0.0119325 2.0097734e-05][0.06887202 0.094918266 0.10312169 0.093759 0.077912755 0.060936786 0.050039567 0.04914346 0.059066996 0.074476518 0.08759281 0.096483082 0.099199288 0.092462622 0.072919592][0.14402081 0.18308382 0.19387943 0.17972611 0.15735015 0.1346489 0.12204659 0.12434318 0.14182304 0.16624877 0.18685487 0.20051084 0.20308006 0.19135506 0.16052687][0.21546166 0.26760563 0.28121993 0.26247156 0.23420335 0.20793268 0.19714305 0.20590211 0.23313272 0.26820949 0.29804024 0.31662142 0.31589949 0.29371426 0.24605557][0.26964781 0.33350447 0.35050073 0.32890749 0.29736763 0.27123359 0.26580933 0.28187022 0.3164176 0.35801026 0.39334494 0.41396597 0.40775082 0.37370613 0.309768][0.29995698 0.37559018 0.39980525 0.38095137 0.3510392 0.32840237 0.32904416 0.34972683 0.38490677 0.42361468 0.45584983 0.47396055 0.46261024 0.42068729 0.34763628][0.30298623 0.39033297 0.42715028 0.41906992 0.39663076 0.37867758 0.38205776 0.40154931 0.42922339 0.45480508 0.47394198 0.4838334 0.46817037 0.42480496 0.35292602][0.2856968 0.3817192 0.43266389 0.43909004 0.42569813 0.41000074 0.41048709 0.42327932 0.4390229 0.44830123 0.45277622 0.45497975 0.43873483 0.39931965 0.33439741][0.25861335 0.35689083 0.41749203 0.43619645 0.43085822 0.41637844 0.41202891 0.41665256 0.42035988 0.41527486 0.40799305 0.40554047 0.39250973 0.36022675 0.30445889][0.21844991 0.30938649 0.37043592 0.39490041 0.3953523 0.3830671 0.37535504 0.37284854 0.36626491 0.34984848 0.33436102 0.32995358 0.32243165 0.29990977 0.25557563][0.16481407 0.24158266 0.29587153 0.32036823 0.32408002 0.31435212 0.30427456 0.29549611 0.28068891 0.25667477 0.23628235 0.23037817 0.22687861 0.21340281 0.18110897][0.09666416 0.15325572 0.19501536 0.21506867 0.21960804 0.21215627 0.20081913 0.18770948 0.16824524 0.14192496 0.12137376 0.11575206 0.11520589 0.10892535 0.087842286][0.02295205 0.056320213 0.082234688 0.095452107 0.099841006 0.0958702 0.087064281 0.0751805 0.057791505 0.036073029 0.020387486 0.016778186 0.017483104 0.014395266 0.00061709789][-0.039757617 -0.027060634 -0.015662244 -0.0091216424 -0.0050200205 -0.0047409507 -0.0082012042 -0.015088847 -0.026660003 -0.041344505 -0.051827379 -0.054657724 -0.054809909 -0.057022739 -0.065423921][-0.08290977 -0.084311865 -0.0832421 -0.082136631 -0.079608388 -0.077541739 -0.077679813 -0.081029586 -0.088059083 -0.097309761 -0.1041997 -0.10696224 -0.10794067 -0.10905785 -0.11225116]]...]
INFO - root - 2017-12-11 07:08:49.951848: step 30710, loss = 0.71, batch loss = 0.65 (14.7 examples/sec; 0.546 sec/batch; 45h:45m:23s remains)
INFO - root - 2017-12-11 07:08:55.411877: step 30720, loss = 0.70, batch loss = 0.64 (14.6 examples/sec; 0.547 sec/batch; 45h:53m:33s remains)
INFO - root - 2017-12-11 07:09:00.988822: step 30730, loss = 0.71, batch loss = 0.65 (12.8 examples/sec; 0.625 sec/batch; 52h:23m:12s remains)
INFO - root - 2017-12-11 07:09:06.478675: step 30740, loss = 0.68, batch loss = 0.62 (14.7 examples/sec; 0.545 sec/batch; 45h:41m:01s remains)
INFO - root - 2017-12-11 07:09:11.939225: step 30750, loss = 0.69, batch loss = 0.63 (14.6 examples/sec; 0.549 sec/batch; 46h:01m:56s remains)
INFO - root - 2017-12-11 07:09:17.120045: step 30760, loss = 0.72, batch loss = 0.66 (14.3 examples/sec; 0.560 sec/batch; 46h:58m:11s remains)
INFO - root - 2017-12-11 07:09:22.728357: step 30770, loss = 0.70, batch loss = 0.65 (14.3 examples/sec; 0.561 sec/batch; 47h:01m:06s remains)
INFO - root - 2017-12-11 07:09:28.217083: step 30780, loss = 0.71, batch loss = 0.65 (14.5 examples/sec; 0.552 sec/batch; 46h:14m:25s remains)
INFO - root - 2017-12-11 07:09:33.802885: step 30790, loss = 0.67, batch loss = 0.62 (14.4 examples/sec; 0.554 sec/batch; 46h:25m:35s remains)
INFO - root - 2017-12-11 07:09:39.277843: step 30800, loss = 0.69, batch loss = 0.63 (14.6 examples/sec; 0.549 sec/batch; 46h:02m:29s remains)
2017-12-11 07:09:39.853918: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.039903291 -0.025425892 -0.00695523 0.010524436 0.022062644 0.024231449 0.017908765 0.0070115761 -0.0039460515 -0.012634935 -0.017981058 -0.018390561 -0.014962909 -0.011205887 -0.010617277][-0.022408269 0.0041161557 0.037441634 0.071595453 0.098817416 0.11233296 0.11063194 0.098802872 0.085405476 0.073401414 0.064019792 0.061460949 0.065856688 0.069299415 0.064612009][-0.0032177353 0.035344437 0.084752232 0.13812125 0.18544391 0.21655843 0.22565503 0.2180178 0.20648463 0.19393253 0.18080164 0.17399013 0.17687348 0.1764697 0.16143382][0.017647706 0.069567606 0.13602777 0.20882456 0.27593556 0.324319 0.34346128 0.339433 0.32933271 0.31631362 0.2992951 0.28693631 0.28666329 0.2811268 0.25513721][0.040663097 0.10699283 0.18921608 0.27680495 0.35594231 0.41293645 0.43420374 0.42759061 0.41411474 0.398512 0.3785395 0.36239594 0.36059132 0.35273817 0.32100829][0.064477779 0.14709894 0.24587813 0.3466447 0.43290862 0.49232429 0.50893289 0.49272269 0.46979854 0.44944766 0.42904121 0.41345957 0.41314584 0.40542671 0.37055159][0.090902142 0.19240664 0.31135267 0.42832187 0.52275944 0.58381706 0.59309185 0.56254888 0.52576953 0.50023514 0.48341706 0.47372517 0.47846648 0.47351637 0.43538067][0.11311967 0.22992626 0.36639935 0.49826881 0.60081255 0.66344607 0.66673803 0.62337673 0.57325089 0.54360759 0.53265631 0.53179288 0.54302275 0.54242992 0.50183892][0.12451559 0.24819389 0.39360136 0.53375453 0.64022011 0.70295388 0.70313162 0.65238726 0.5929268 0.55981743 0.55353665 0.56002069 0.57617313 0.57978266 0.5392428][0.12668905 0.24924165 0.39366308 0.53229588 0.63416874 0.691565 0.68854827 0.63507158 0.57202566 0.53694141 0.53243595 0.54202461 0.56004727 0.56724095 0.530415][0.11282591 0.22537108 0.35888442 0.48669973 0.57729727 0.62516642 0.61857432 0.56577289 0.50377607 0.46820539 0.46240687 0.47069606 0.48764548 0.49727908 0.46660715][0.077572308 0.16858454 0.27937752 0.38609114 0.45987794 0.49624008 0.48799387 0.4416568 0.38784376 0.35627356 0.34912616 0.35367605 0.36714765 0.37732619 0.35340595][0.027017187 0.087657 0.16553245 0.24208198 0.29419252 0.31818086 0.31029773 0.27526656 0.2354712 0.21230137 0.2063479 0.20821969 0.21783191 0.22664161 0.2093299][-0.020132264 0.011144171 0.056168269 0.10240547 0.13319767 0.14630732 0.139585 0.11613443 0.09043853 0.075712241 0.072005317 0.072434857 0.078619592 0.085056193 0.074002743][-0.050467648 -0.039250206 -0.018283853 0.0043611359 0.018109728 0.021957178 0.014768607 -0.0011285386 -0.016755186 -0.025233628 -0.027436761 -0.028374009 -0.025539586 -0.021590717 -0.027126405]]...]
INFO - root - 2017-12-11 07:09:45.382732: step 30810, loss = 0.69, batch loss = 0.63 (14.9 examples/sec; 0.538 sec/batch; 45h:04m:54s remains)
INFO - root - 2017-12-11 07:09:50.850892: step 30820, loss = 0.69, batch loss = 0.64 (14.6 examples/sec; 0.549 sec/batch; 46h:00m:57s remains)
INFO - root - 2017-12-11 07:09:56.373425: step 30830, loss = 0.71, batch loss = 0.65 (14.7 examples/sec; 0.544 sec/batch; 45h:36m:39s remains)
INFO - root - 2017-12-11 07:10:01.915901: step 30840, loss = 0.68, batch loss = 0.62 (14.4 examples/sec; 0.557 sec/batch; 46h:41m:27s remains)
INFO - root - 2017-12-11 07:10:07.403743: step 30850, loss = 0.69, batch loss = 0.63 (15.3 examples/sec; 0.522 sec/batch; 43h:43m:16s remains)
INFO - root - 2017-12-11 07:10:12.720731: step 30860, loss = 0.70, batch loss = 0.64 (14.5 examples/sec; 0.553 sec/batch; 46h:20m:34s remains)
INFO - root - 2017-12-11 07:10:18.216494: step 30870, loss = 0.70, batch loss = 0.65 (15.3 examples/sec; 0.523 sec/batch; 43h:50m:48s remains)
INFO - root - 2017-12-11 07:10:23.757443: step 30880, loss = 0.71, batch loss = 0.65 (14.2 examples/sec; 0.563 sec/batch; 47h:11m:38s remains)
INFO - root - 2017-12-11 07:10:29.297512: step 30890, loss = 0.70, batch loss = 0.65 (14.4 examples/sec; 0.557 sec/batch; 46h:38m:24s remains)
INFO - root - 2017-12-11 07:10:34.809664: step 30900, loss = 0.70, batch loss = 0.64 (14.6 examples/sec; 0.547 sec/batch; 45h:49m:57s remains)
2017-12-11 07:10:35.410632: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.12853335 0.10830135 0.077196479 0.051868755 0.037775759 0.037196051 0.049723461 0.073504157 0.10178241 0.12584856 0.14065892 0.14529175 0.14484644 0.14193563 0.1392128][0.11982393 0.10653222 0.079587586 0.053387664 0.035088036 0.028489424 0.033141449 0.047173213 0.06632597 0.086471014 0.10446613 0.11773574 0.12649702 0.12863173 0.12521696][0.10562398 0.099038593 0.079217166 0.056345057 0.037709638 0.027874468 0.02650569 0.031366177 0.040632822 0.054857623 0.073590368 0.093161233 0.1084495 0.11372508 0.1087118][0.084009647 0.082506865 0.073195234 0.060534846 0.050142847 0.045282632 0.044441506 0.044317745 0.045725308 0.053666569 0.070145234 0.091370963 0.10900038 0.11481113 0.10708552][0.059557773 0.062709436 0.066608094 0.069549337 0.073830336 0.080281928 0.085453682 0.08439476 0.079874896 0.080849521 0.091428854 0.10875021 0.12373786 0.12714046 0.11563468][0.044973832 0.051432911 0.067145452 0.085853763 0.1059636 0.12550773 0.13923155 0.13975677 0.13043348 0.12394273 0.1265143 0.13664155 0.1454872 0.143259 0.12638532][0.048776511 0.053186562 0.073740989 0.10275883 0.13481304 0.16552821 0.18792623 0.19194248 0.17997392 0.16814536 0.16480009 0.16915448 0.17147809 0.16156191 0.13734968][0.072534613 0.071937338 0.090089068 0.12144844 0.15784666 0.19360057 0.22066696 0.22733305 0.21459201 0.20071332 0.19615322 0.19976082 0.19892783 0.18245855 0.15056992][0.10646331 0.1048682 0.1193548 0.14787601 0.18114141 0.21347336 0.23711351 0.24126016 0.2264992 0.21134549 0.20801803 0.21427916 0.21433429 0.19586949 0.16039088][0.14317554 0.14815681 0.1613588 0.18445237 0.20898405 0.22978473 0.24080317 0.23499537 0.21463142 0.19677232 0.19460066 0.20445722 0.20820332 0.1925102 0.15932278][0.17504609 0.19005765 0.20347413 0.21991429 0.23357546 0.23937014 0.2329831 0.21274339 0.18425089 0.16297129 0.16211554 0.17643632 0.18648924 0.1777323 0.15138993][0.19713277 0.21845002 0.2302494 0.23900367 0.24304388 0.23748216 0.21751592 0.18524545 0.14902481 0.12334609 0.12200142 0.13942945 0.15658677 0.15744288 0.140895][0.20788544 0.23376504 0.24536833 0.2494822 0.24870212 0.23850811 0.21199864 0.17250453 0.13002805 0.098372258 0.093280539 0.11148201 0.13543379 0.1467288 0.14050007][0.21029219 0.23967654 0.25307265 0.25645533 0.25490135 0.24445061 0.21726172 0.17653525 0.13221891 0.0960014 0.085742816 0.1021101 0.12934168 0.14691365 0.14652443][0.20802967 0.2390995 0.25467956 0.2600033 0.25984129 0.25093663 0.22704163 0.191407 0.15169641 0.11531847 0.10025667 0.11055341 0.1332404 0.14777094 0.14505205]]...]
INFO - root - 2017-12-11 07:10:40.984954: step 30910, loss = 0.70, batch loss = 0.64 (14.7 examples/sec; 0.546 sec/batch; 45h:44m:43s remains)
INFO - root - 2017-12-11 07:10:46.475144: step 30920, loss = 0.69, batch loss = 0.64 (14.6 examples/sec; 0.548 sec/batch; 45h:54m:37s remains)
INFO - root - 2017-12-11 07:10:51.931917: step 30930, loss = 0.70, batch loss = 0.64 (14.7 examples/sec; 0.544 sec/batch; 45h:32m:15s remains)
INFO - root - 2017-12-11 07:10:57.606172: step 30940, loss = 0.69, batch loss = 0.63 (14.1 examples/sec; 0.566 sec/batch; 47h:26m:39s remains)
INFO - root - 2017-12-11 07:11:02.874808: step 30950, loss = 0.70, batch loss = 0.64 (14.6 examples/sec; 0.547 sec/batch; 45h:48m:58s remains)
INFO - root - 2017-12-11 07:11:08.336485: step 30960, loss = 0.69, batch loss = 0.63 (14.4 examples/sec; 0.556 sec/batch; 46h:32m:37s remains)
INFO - root - 2017-12-11 07:11:13.782287: step 30970, loss = 0.69, batch loss = 0.63 (14.7 examples/sec; 0.544 sec/batch; 45h:35m:05s remains)
INFO - root - 2017-12-11 07:11:19.358935: step 30980, loss = 0.70, batch loss = 0.64 (14.4 examples/sec; 0.555 sec/batch; 46h:28m:02s remains)
INFO - root - 2017-12-11 07:11:24.884165: step 30990, loss = 0.70, batch loss = 0.64 (14.6 examples/sec; 0.546 sec/batch; 45h:45m:39s remains)
INFO - root - 2017-12-11 07:11:30.354870: step 31000, loss = 0.71, batch loss = 0.65 (14.3 examples/sec; 0.561 sec/batch; 47h:00m:16s remains)
2017-12-11 07:11:30.922672: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.082049809 -0.090153575 -0.095718265 -0.09514977 -0.088501289 -0.080250613 -0.076019205 -0.080233023 -0.095986433 -0.11786091 -0.13480097 -0.13840526 -0.12705003 -0.10434031 -0.076709539][-0.068148986 -0.066337451 -0.059372 -0.043255877 -0.019012589 0.0076693664 0.027957434 0.032626383 0.015236084 -0.018935071 -0.057058942 -0.086004168 -0.0977288 -0.09061376 -0.0701762][-0.041424729 -0.023547499 0.00376948 0.043168895 0.093594447 0.14906074 0.19568156 0.21684296 0.19951737 0.1469633 0.0743385 0.0034723589 -0.046587586 -0.065871149 -0.058173638][-0.0038586962 0.03290258 0.083567113 0.14821739 0.2277285 0.31935242 0.40373027 0.45256007 0.4418987 0.37053773 0.25587109 0.13073105 0.029451212 -0.026392419 -0.037099924][0.041243579 0.097363181 0.17098792 0.25886646 0.36571595 0.49655402 0.62670887 0.71166033 0.712828 0.62328869 0.46382731 0.27888516 0.12011175 0.022912927 -0.008998299][0.082122393 0.153907 0.24585935 0.352002 0.48087838 0.64711809 0.82111669 0.94039631 0.95164961 0.84386092 0.64354306 0.40547353 0.19618207 0.063079469 0.012509828][0.10200928 0.18058808 0.28215545 0.39963466 0.542851 0.73335415 0.93749321 1.0785003 1.0929122 0.968903 0.7401588 0.46889925 0.23082884 0.078600593 0.018201508][0.086969957 0.1580319 0.25530258 0.37228274 0.5167954 0.71184421 0.92254031 1.0663497 1.0772495 0.94606155 0.71232671 0.44017765 0.2054083 0.057758808 0.00046873477][0.04451073 0.095610924 0.17358126 0.27452046 0.40244356 0.57683629 0.76508033 0.88997293 0.89184719 0.766111 0.55438751 0.31577006 0.11655433 -0.0027446442 -0.04293352][-0.0054370272 0.021859834 0.07217887 0.14535254 0.24207026 0.37529305 0.51742792 0.6064344 0.59615225 0.48638847 0.31573206 0.13321099 -0.0099147875 -0.08557725 -0.0989311][-0.05045202 -0.044917922 -0.022193197 0.021000555 0.083262473 0.16949549 0.25835517 0.307735 0.28731135 0.19978525 0.076434605 -0.045204304 -0.12964793 -0.16132359 -0.14847204][-0.081857584 -0.091168627 -0.087901764 -0.0663075 -0.029268092 0.020501023 0.067095138 0.086014904 0.059997987 -0.0066276542 -0.090232991 -0.16329785 -0.20355324 -0.20471928 -0.17591263][-0.091648318 -0.10456426 -0.10730811 -0.093181305 -0.065332234 -0.030958356 -0.0034543 0.0023021388 -0.024431996 -0.079022452 -0.14296006 -0.19413529 -0.21719013 -0.20944576 -0.17894852][-0.0783599 -0.081849627 -0.075194918 -0.052816808 -0.016548688 0.025591599 0.059281658 0.07022579 0.047066532 -0.0071434826 -0.076495685 -0.13803141 -0.17450233 -0.18028875 -0.16095029][-0.051959582 -0.037040632 -0.0088838125 0.035514202 0.096460238 0.16764058 0.231131 0.26554111 0.25216007 0.18999341 0.094017141 -0.0061686672 -0.083481371 -0.12282309 -0.12558261]]...]
INFO - root - 2017-12-11 07:11:36.473133: step 31010, loss = 0.70, batch loss = 0.65 (14.9 examples/sec; 0.537 sec/batch; 45h:00m:46s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-SGD-lr0.001-clip50-initfromfixqian30k-fixqian
INFO - root - 2017-12-11 07:11:41.992009: step 31020, loss = 0.70, batch loss = 0.64 (14.7 examples/sec; 0.544 sec/batch; 45h:33m:44s remains)
INFO - root - 2017-12-11 07:11:47.506194: step 31030, loss = 0.70, batch loss = 0.64 (14.5 examples/sec; 0.552 sec/batch; 46h:12m:09s remains)
INFO - root - 2017-12-11 07:11:52.976660: step 31040, loss = 0.68, batch loss = 0.62 (14.7 examples/sec; 0.545 sec/batch; 45h:37m:37s remains)
INFO - root - 2017-12-11 07:11:58.176294: step 31050, loss = 0.68, batch loss = 0.62 (14.3 examples/sec; 0.558 sec/batch; 46h:42m:07s remains)
INFO - root - 2017-12-11 07:12:03.688137: step 31060, loss = 0.69, batch loss = 0.63 (14.9 examples/sec; 0.535 sec/batch; 44h:50m:06s remains)
INFO - root - 2017-12-11 07:12:09.188202: step 31070, loss = 0.71, batch loss = 0.65 (14.7 examples/sec; 0.544 sec/batch; 45h:33m:57s remains)
INFO - root - 2017-12-11 07:12:14.705889: step 31080, loss = 0.70, batch loss = 0.65 (14.5 examples/sec; 0.553 sec/batch; 46h:19m:17s remains)
INFO - root - 2017-12-11 07:12:20.267521: step 31090, loss = 0.70, batch loss = 0.64 (14.9 examples/sec; 0.536 sec/batch; 44h:54m:47s remains)
INFO - root - 2017-12-11 07:12:25.716966: step 31100, loss = 0.69, batch loss = 0.63 (14.6 examples/sec; 0.547 sec/batch; 45h:46m:47s remains)
2017-12-11 07:12:26.303526: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.37217239 0.40876579 0.44498459 0.45342892 0.42476267 0.36688131 0.3006438 0.256418 0.26227096 0.30019948 0.33121654 0.33152503 0.29635614 0.22891282 0.13842757][0.3828122 0.42589456 0.4615269 0.46526551 0.43456209 0.37934613 0.3154723 0.26826414 0.2613472 0.28269643 0.30164143 0.30036736 0.2741808 0.21926288 0.13818766][0.37786806 0.42341366 0.45408657 0.45163676 0.42058662 0.37243718 0.3169626 0.27216041 0.25413939 0.25822124 0.26496717 0.26355115 0.24910006 0.20965171 0.14109133][0.38194063 0.43077427 0.45807648 0.45135951 0.42089596 0.37991115 0.33342323 0.29196945 0.26401708 0.25235969 0.24950351 0.25074238 0.24819402 0.22214773 0.16344117][0.39157096 0.44233033 0.4663257 0.45749435 0.42996329 0.3973954 0.36040923 0.32272944 0.28737167 0.26525053 0.25916451 0.266795 0.27440181 0.25576997 0.20087856][0.39556554 0.44017461 0.45891631 0.45285237 0.43569577 0.41663694 0.39080256 0.35646918 0.31502849 0.28638068 0.28125349 0.29643014 0.31041232 0.29275495 0.23619044][0.37174678 0.40868452 0.43010646 0.4405345 0.4459528 0.44486049 0.4270466 0.39027548 0.33994222 0.3046546 0.30014008 0.31977016 0.33450118 0.31299666 0.25382233][0.32129785 0.35554442 0.39197126 0.43274453 0.46890065 0.48564205 0.46880981 0.42162123 0.35862923 0.31515718 0.30853042 0.326932 0.33612826 0.30759236 0.24658354][0.259838 0.29243493 0.34642342 0.41852644 0.48298895 0.51264238 0.49180886 0.433015 0.36157551 0.31489244 0.30696785 0.31966132 0.31743976 0.277795 0.21333162][0.21814308 0.24707557 0.31154829 0.40339178 0.48298132 0.51519984 0.4866859 0.42026025 0.34993574 0.30866194 0.30171922 0.3048315 0.286402 0.23264495 0.16321115][0.19849211 0.22083192 0.28450337 0.37866879 0.45664454 0.48185766 0.44623372 0.38038233 0.32149056 0.292201 0.28688145 0.27927649 0.24558572 0.18161738 0.11102707][0.18565015 0.20031653 0.25333962 0.33424622 0.39696154 0.41043475 0.37285265 0.31748775 0.27755079 0.26216182 0.25703469 0.23903123 0.19479866 0.12855393 0.064283445][0.18335372 0.18817984 0.22160478 0.27696735 0.31535831 0.31557596 0.28114778 0.24230571 0.22322634 0.22057557 0.21572956 0.1911841 0.14374264 0.083175614 0.030763734][0.19542375 0.19013965 0.20093463 0.22633423 0.23875673 0.22705092 0.197984 0.17512664 0.17248289 0.17871396 0.17495099 0.14947699 0.106387 0.0571683 0.018974923][0.20473517 0.19349451 0.18691093 0.18818882 0.18118525 0.16269062 0.14021713 0.12921287 0.13517737 0.14420076 0.14078332 0.11847288 0.085128441 0.050920047 0.027744973]]...]
INFO - root - 2017-12-11 07:12:31.786637: step 31110, loss = 0.71, batch loss = 0.65 (14.7 examples/sec; 0.546 sec/batch; 45h:40m:38s remains)
INFO - root - 2017-12-11 07:12:37.227432: step 31120, loss = 0.71, batch loss = 0.65 (15.0 examples/sec; 0.533 sec/batch; 44h:37m:46s remains)
INFO - root - 2017-12-11 07:12:42.755217: step 31130, loss = 0.68, batch loss = 0.62 (14.3 examples/sec; 0.558 sec/batch; 46h:43m:13s remains)
INFO - root - 2017-12-11 07:12:48.339113: step 31140, loss = 0.69, batch loss = 0.63 (14.7 examples/sec; 0.543 sec/batch; 45h:29m:07s remains)
INFO - root - 2017-12-11 07:12:53.623426: step 31150, loss = 0.70, batch loss = 0.64 (14.4 examples/sec; 0.557 sec/batch; 46h:39m:58s remains)
INFO - root - 2017-12-11 07:12:59.176674: step 31160, loss = 0.71, batch loss = 0.65 (14.5 examples/sec; 0.551 sec/batch; 46h:07m:24s remains)
INFO - root - 2017-12-11 07:13:04.665786: step 31170, loss = 0.70, batch loss = 0.64 (14.7 examples/sec; 0.545 sec/batch; 45h:36m:41s remains)
INFO - root - 2017-12-11 07:13:10.101116: step 31180, loss = 0.70, batch loss = 0.64 (14.8 examples/sec; 0.542 sec/batch; 45h:22m:02s remains)
INFO - root - 2017-12-11 07:13:15.666394: step 31190, loss = 0.69, batch loss = 0.63 (14.6 examples/sec; 0.547 sec/batch; 45h:44m:51s remains)
INFO - root - 2017-12-11 07:13:21.186695: step 31200, loss = 0.69, batch loss = 0.63 (14.5 examples/sec; 0.553 sec/batch; 46h:16m:59s remains)
2017-12-11 07:13:21.795336: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.066881232 0.072242983 0.085989788 0.098067313 0.10205846 0.09781418 0.089322828 0.079125643 0.074588269 0.0924067 0.137043 0.19886564 0.25204208 0.28957662 0.30996731][0.12509188 0.14460446 0.17156374 0.19371007 0.20187318 0.19531959 0.17826979 0.1531451 0.12908082 0.12592839 0.15400904 0.20735587 0.26068449 0.306402 0.3397238][0.16487509 0.19864839 0.24022603 0.27675185 0.29713449 0.299057 0.283092 0.24759251 0.20122832 0.16641214 0.16168173 0.18901145 0.23024322 0.28044051 0.33086646][0.18526484 0.22941191 0.28460783 0.34028634 0.38489425 0.41252485 0.41462883 0.38021559 0.3136678 0.23998939 0.18778586 0.17177528 0.18755867 0.23617256 0.30427593][0.19196738 0.23845941 0.30482769 0.38567364 0.4707869 0.54708672 0.58979106 0.57173282 0.4888514 0.36970913 0.25567734 0.17967364 0.15619849 0.19346027 0.27259481][0.19024274 0.23185863 0.30669963 0.41656488 0.55316442 0.69279659 0.78969944 0.79658961 0.69893765 0.53115553 0.34968784 0.20774694 0.13873978 0.1568277 0.23754534][0.18749608 0.2211812 0.300667 0.435228 0.61794323 0.81357205 0.95678985 0.9836247 0.87153691 0.662697 0.42707717 0.23428349 0.13085847 0.13326481 0.21278866][0.18537605 0.21144997 0.29214045 0.44170114 0.65298331 0.88107574 1.0483667 1.0830383 0.95974052 0.7270261 0.46303234 0.24648622 0.13005234 0.12858844 0.20967245][0.19093128 0.20708373 0.28196242 0.43236014 0.64960611 0.88257825 1.0497419 1.0807806 0.9525584 0.7165615 0.4526132 0.24140759 0.13622718 0.14685069 0.23594382][0.19418979 0.19408625 0.2528494 0.38858822 0.59041792 0.80457139 0.95247394 0.97201496 0.8465696 0.62727582 0.38987485 0.21062538 0.13823925 0.17528087 0.27884388][0.19263798 0.17325489 0.2090359 0.31844676 0.48856696 0.66697234 0.78323871 0.78759861 0.6722244 0.48508275 0.29306576 0.16245368 0.13322017 0.20077275 0.3189103][0.17956962 0.14507923 0.15833338 0.23691466 0.36725813 0.50280035 0.584367 0.57536316 0.47617391 0.32903156 0.18857765 0.10722454 0.11436865 0.20085484 0.32201821][0.13725434 0.094687961 0.09118963 0.14099102 0.23237041 0.32820994 0.38151729 0.36618721 0.28716052 0.17883666 0.082812071 0.03764658 0.063333392 0.14958335 0.25773758][0.067431912 0.025020387 0.011384987 0.036776338 0.093632273 0.15623461 0.18982701 0.17561409 0.11866811 0.044131961 -0.018658357 -0.042991746 -0.015731774 0.054438557 0.13774569][-0.018559702 -0.05330367 -0.069924809 -0.062831528 -0.034859512 -0.00033533719 0.018233769 0.0079512829 -0.028333081 -0.074983813 -0.11357579 -0.12708825 -0.10727503 -0.059987191 -0.0050198864]]...]
INFO - root - 2017-12-11 07:13:27.322921: step 31210, loss = 0.69, batch loss = 0.63 (13.7 examples/sec; 0.582 sec/batch; 48h:43m:33s remains)
INFO - root - 2017-12-11 07:13:32.789763: step 31220, loss = 0.71, batch loss = 0.65 (14.6 examples/sec; 0.548 sec/batch; 45h:50m:28s remains)
INFO - root - 2017-12-11 07:13:38.268276: step 31230, loss = 0.70, batch loss = 0.65 (14.5 examples/sec; 0.550 sec/batch; 46h:01m:24s remains)
INFO - root - 2017-12-11 07:13:43.553152: step 31240, loss = 0.68, batch loss = 0.62 (20.2 examples/sec; 0.395 sec/batch; 33h:04m:22s remains)
INFO - root - 2017-12-11 07:13:49.085726: step 31250, loss = 0.70, batch loss = 0.64 (14.1 examples/sec; 0.566 sec/batch; 47h:23m:38s remains)
INFO - root - 2017-12-11 07:13:54.728303: step 31260, loss = 0.70, batch loss = 0.64 (13.7 examples/sec; 0.582 sec/batch; 48h:43m:45s remains)
INFO - root - 2017-12-11 07:14:00.228003: step 31270, loss = 0.71, batch loss = 0.65 (14.7 examples/sec; 0.546 sec/batch; 45h:40m:19s remains)
INFO - root - 2017-12-11 07:14:05.768213: step 31280, loss = 0.70, batch loss = 0.64 (14.3 examples/sec; 0.559 sec/batch; 46h:43m:52s remains)
INFO - root - 2017-12-11 07:14:11.312724: step 31290, loss = 0.69, batch loss = 0.63 (14.3 examples/sec; 0.560 sec/batch; 46h:52m:41s remains)
INFO - root - 2017-12-11 07:14:16.746580: step 31300, loss = 0.70, batch loss = 0.64 (14.9 examples/sec; 0.538 sec/batch; 45h:00m:26s remains)
2017-12-11 07:14:17.359063: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.11115932 0.19150475 0.26739869 0.32268569 0.351234 0.37114456 0.37897828 0.36599824 0.33684951 0.30672756 0.28173634 0.25485155 0.23075432 0.21478058 0.20728531][0.12336523 0.21437536 0.30189291 0.3664349 0.40282995 0.42853126 0.43633774 0.41635263 0.37584707 0.3387455 0.31358263 0.29043862 0.27113137 0.25951543 0.25451845][0.1191887 0.21410863 0.30651176 0.37547576 0.41721466 0.44854721 0.45932576 0.43940517 0.39749452 0.36153781 0.3418442 0.32491609 0.30986747 0.300008 0.29532549][0.11263333 0.21200435 0.3101559 0.38416958 0.43091297 0.46622598 0.47973612 0.46364364 0.42802405 0.39965492 0.38750854 0.37481532 0.35838547 0.34408 0.33729941][0.10636368 0.21094353 0.31732893 0.39943022 0.45266813 0.49028996 0.50534862 0.49428871 0.46848792 0.45056662 0.44610623 0.43638292 0.4149839 0.39242849 0.38197821][0.098622419 0.20666718 0.32073206 0.41307718 0.47576839 0.51681381 0.53420156 0.52895194 0.51323122 0.5040893 0.50372136 0.4946368 0.46870863 0.44138551 0.43106428][0.091701977 0.19872195 0.31403869 0.410696 0.4781644 0.52004832 0.53996706 0.54091358 0.53361255 0.52864629 0.52658325 0.51496887 0.48700076 0.462405 0.46022838][0.081441022 0.18083593 0.28799161 0.37771457 0.43880028 0.47395134 0.49252844 0.49761266 0.49578524 0.49069279 0.48270696 0.46625504 0.43842977 0.42153761 0.43214628][0.0648533 0.15130396 0.2429285 0.31647089 0.36146048 0.38201687 0.39231074 0.39540279 0.3942095 0.38704973 0.37366557 0.35430643 0.3297236 0.32242355 0.3446618][0.028519228 0.094337456 0.16378348 0.21683785 0.2444136 0.25131232 0.25276205 0.25131258 0.24805813 0.23951674 0.22467229 0.20630755 0.18688644 0.18659416 0.21296971][-0.019988384 0.0213864 0.067115709 0.10190919 0.1179275 0.11837965 0.11515503 0.10958272 0.10242306 0.091402128 0.076241583 0.060296353 0.046139833 0.049327627 0.073357992][-0.057154574 -0.03795841 -0.013652042 0.0051850225 0.012322384 0.0093266079 0.0036440808 -0.0044814134 -0.014637559 -0.027500454 -0.041727494 -0.0541423 -0.063060604 -0.05851768 -0.03969191][-0.080229647 -0.0771196 -0.069578491 -0.064113177 -0.064503871 -0.069882 -0.076663092 -0.085071839 -0.095036723 -0.10657001 -0.11778709 -0.12638456 -0.13163467 -0.12812807 -0.11600872][-0.095064722 -0.10164838 -0.10441546 -0.10725088 -0.11158827 -0.11698686 -0.12253897 -0.12856893 -0.13527331 -0.14285657 -0.15018931 -0.15595643 -0.15990962 -0.15948361 -0.15462236][-0.10109446 -0.11108054 -0.11712527 -0.12208489 -0.1261597 -0.12936631 -0.13209809 -0.13474375 -0.1376209 -0.14105932 -0.14479341 -0.14835408 -0.15176322 -0.15398742 -0.15463154]]...]
INFO - root - 2017-12-11 07:14:22.913544: step 31310, loss = 0.70, batch loss = 0.64 (14.8 examples/sec; 0.539 sec/batch; 45h:06m:20s remains)
INFO - root - 2017-12-11 07:14:28.370542: step 31320, loss = 0.71, batch loss = 0.65 (14.3 examples/sec; 0.560 sec/batch; 46h:50m:26s remains)
INFO - root - 2017-12-11 07:14:33.955516: step 31330, loss = 0.70, batch loss = 0.64 (14.4 examples/sec; 0.555 sec/batch; 46h:27m:15s remains)
INFO - root - 2017-12-11 07:14:39.183145: step 31340, loss = 0.70, batch loss = 0.65 (14.2 examples/sec; 0.562 sec/batch; 47h:02m:34s remains)
INFO - root - 2017-12-11 07:14:44.715437: step 31350, loss = 0.70, batch loss = 0.64 (14.9 examples/sec; 0.538 sec/batch; 45h:00m:54s remains)
INFO - root - 2017-12-11 07:14:50.189391: step 31360, loss = 0.68, batch loss = 0.63 (14.4 examples/sec; 0.556 sec/batch; 46h:29m:04s remains)
INFO - root - 2017-12-11 07:14:55.563839: step 31370, loss = 0.69, batch loss = 0.64 (15.1 examples/sec; 0.528 sec/batch; 44h:11m:46s remains)
INFO - root - 2017-12-11 07:15:01.025812: step 31380, loss = 0.71, batch loss = 0.65 (14.8 examples/sec; 0.541 sec/batch; 45h:15m:20s remains)
INFO - root - 2017-12-11 07:15:06.574544: step 31390, loss = 0.69, batch loss = 0.63 (14.1 examples/sec; 0.568 sec/batch; 47h:31m:19s remains)
INFO - root - 2017-12-11 07:15:11.978691: step 31400, loss = 0.69, batch loss = 0.63 (15.0 examples/sec; 0.534 sec/batch; 44h:38m:34s remains)
2017-12-11 07:15:12.587089: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.013445389 0.052828263 0.095448 0.13310619 0.15947437 0.17136577 0.16929078 0.15650946 0.13460095 0.11123194 0.099126711 0.10475107 0.13657399 0.20255584 0.29113373][0.014218079 0.063044228 0.1206288 0.17526422 0.21530534 0.23124479 0.22196944 0.1874868 0.13404593 0.080342516 0.050282579 0.056441046 0.1076209 0.20880766 0.33787361][0.017948762 0.079529792 0.15733354 0.23557848 0.29784262 0.32740462 0.31981984 0.2696673 0.18632698 0.0995898 0.04442798 0.040340036 0.096784607 0.21727096 0.37304306][0.025791062 0.10361626 0.2065634 0.31403521 0.40646017 0.46057236 0.46659464 0.41161495 0.30541098 0.1869241 0.099055238 0.068564184 0.10866717 0.2253539 0.38889247][0.037421189 0.13344382 0.26260477 0.39927524 0.52285486 0.60528922 0.63081557 0.57893658 0.45813727 0.31399402 0.19328709 0.12837934 0.13698389 0.22998491 0.38156414][0.046986118 0.15775095 0.3081679 0.4677192 0.61635637 0.72299427 0.76750767 0.72242117 0.59490025 0.43491969 0.29047614 0.19642231 0.1725814 0.2345228 0.36186025][0.050442949 0.16948038 0.33343112 0.506932 0.67053753 0.79231805 0.8495943 0.8088997 0.67750823 0.51027745 0.3564468 0.24984641 0.20685329 0.24525297 0.34759137][0.048163071 0.16899511 0.33893475 0.51760775 0.68404347 0.80804706 0.86872935 0.829358 0.6982078 0.53530347 0.39205778 0.29789639 0.25832388 0.28817758 0.36985597][0.043874133 0.16141218 0.32942396 0.5031144 0.65854412 0.77028012 0.8250137 0.7871154 0.66633928 0.52504677 0.41556635 0.35881007 0.34421852 0.37956271 0.44615474][0.040974215 0.15173069 0.31035036 0.46873587 0.60014844 0.68667644 0.72780448 0.6937483 0.59550953 0.4919872 0.43276307 0.4278959 0.44900113 0.49548551 0.54946905][0.03535831 0.1347101 0.27740219 0.41383767 0.516023 0.57404947 0.60103118 0.57600224 0.50945896 0.45088583 0.44206959 0.4829745 0.53232318 0.58304781 0.62113309][0.024892641 0.11011435 0.23294316 0.34547383 0.41984963 0.45250893 0.46696195 0.45140791 0.41459486 0.39417854 0.42174417 0.48984432 0.55093032 0.59448123 0.6123274][0.010324998 0.079099335 0.177836 0.2639859 0.31303304 0.32610548 0.33171368 0.32509032 0.31260085 0.3200897 0.36768344 0.44299728 0.49954548 0.52728 0.52399814][-0.012019639 0.036979843 0.10653519 0.16260765 0.18758351 0.18627244 0.18634993 0.18679777 0.19092157 0.21375221 0.26462018 0.32918757 0.36904907 0.37814009 0.35921231][-0.043457728 -0.015539767 0.024692843 0.052506294 0.057776965 0.047115289 0.042944636 0.045641862 0.05578275 0.079959661 0.11998198 0.16311607 0.18260673 0.17661123 0.15089203]]...]
INFO - root - 2017-12-11 07:15:18.131443: step 31410, loss = 0.69, batch loss = 0.63 (14.9 examples/sec; 0.537 sec/batch; 44h:54m:57s remains)
INFO - root - 2017-12-11 07:15:23.662373: step 31420, loss = 0.70, batch loss = 0.65 (14.6 examples/sec; 0.549 sec/batch; 45h:56m:56s remains)
INFO - root - 2017-12-11 07:15:29.208642: step 31430, loss = 0.71, batch loss = 0.66 (14.6 examples/sec; 0.547 sec/batch; 45h:42m:52s remains)
INFO - root - 2017-12-11 07:15:34.427674: step 31440, loss = 0.70, batch loss = 0.65 (13.5 examples/sec; 0.592 sec/batch; 49h:31m:19s remains)
INFO - root - 2017-12-11 07:15:39.972819: step 31450, loss = 0.70, batch loss = 0.65 (14.7 examples/sec; 0.543 sec/batch; 45h:25m:58s remains)
INFO - root - 2017-12-11 07:15:45.392107: step 31460, loss = 0.70, batch loss = 0.64 (15.0 examples/sec; 0.534 sec/batch; 44h:38m:53s remains)
INFO - root - 2017-12-11 07:15:50.895248: step 31470, loss = 0.69, batch loss = 0.64 (14.5 examples/sec; 0.551 sec/batch; 46h:05m:29s remains)
INFO - root - 2017-12-11 07:15:56.452540: step 31480, loss = 0.68, batch loss = 0.62 (14.4 examples/sec; 0.556 sec/batch; 46h:29m:38s remains)
INFO - root - 2017-12-11 07:16:01.971871: step 31490, loss = 0.69, batch loss = 0.64 (14.4 examples/sec; 0.557 sec/batch; 46h:32m:20s remains)
INFO - root - 2017-12-11 07:16:07.479434: step 31500, loss = 0.70, batch loss = 0.64 (14.2 examples/sec; 0.562 sec/batch; 47h:00m:01s remains)
2017-12-11 07:16:08.054611: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.098294564 0.096453145 0.0934318 0.09023235 0.088101454 0.087677829 0.086915419 0.085715011 0.0822311 0.080097429 0.087646894 0.099655308 0.10598446 0.10178784 0.092243917][0.10640926 0.10658916 0.10377868 0.0995414 0.095347106 0.09161222 0.086632535 0.080778465 0.072746769 0.066392466 0.06899938 0.076458469 0.081556723 0.079777882 0.07462801][0.1140573 0.11506394 0.11079929 0.10415358 0.097326435 0.0901738 0.081228733 0.071604833 0.060786109 0.052274588 0.051879689 0.0570286 0.062804431 0.064210281 0.063091323][0.11864419 0.11856275 0.11204384 0.10319255 0.095042042 0.086225778 0.075368933 0.064043216 0.052560516 0.043976091 0.042861003 0.04752266 0.054413807 0.05772578 0.058413912][0.11675687 0.11575445 0.10803272 0.098538481 0.090964854 0.082644604 0.0719902 0.061166961 0.051079165 0.044012561 0.043892954 0.049139526 0.056135766 0.0587276 0.058184192][0.10829181 0.1086541 0.10202502 0.09382607 0.08816956 0.081868216 0.072990216 0.063952871 0.056252148 0.051318757 0.052760456 0.058569018 0.064584911 0.06505426 0.061843608][0.0990492 0.1019398 0.097009495 0.090488017 0.086450309 0.08183416 0.074601434 0.067152664 0.061199326 0.057555132 0.059981678 0.066305786 0.071393274 0.070229061 0.065037951][0.093560353 0.097793333 0.094249174 0.089113519 0.086011618 0.082126543 0.075858057 0.069445178 0.064410634 0.061257962 0.064189553 0.071572877 0.076908223 0.075473979 0.06943135][0.089679159 0.09414763 0.0909739 0.086499959 0.083920456 0.080822118 0.076121368 0.071499243 0.067851044 0.065608434 0.0697029 0.0792992 0.086045079 0.084741227 0.077461988][0.089961842 0.094930753 0.09240336 0.088965572 0.08736942 0.085574627 0.082792677 0.079997763 0.077584416 0.076135412 0.081621818 0.094066352 0.10242102 0.10044745 0.090610251][0.091509677 0.097448088 0.096145272 0.094272509 0.094017811 0.093682885 0.092687853 0.091373168 0.089780353 0.088687323 0.0952618 0.1100929 0.11943368 0.11602312 0.10317472][0.087132312 0.092621684 0.092003644 0.091530807 0.092536964 0.093321845 0.093401082 0.092769124 0.091510072 0.090540916 0.097692631 0.11355183 0.12283265 0.11817812 0.10379808][0.079721533 0.08404623 0.082962915 0.082623586 0.083816648 0.084730551 0.084751725 0.083856925 0.082641348 0.082042083 0.0895069 0.10507859 0.11384678 0.10940796 0.09618257][0.067284063 0.068461463 0.065083034 0.062912412 0.062872246 0.063395873 0.0634942 0.06302432 0.063154139 0.064676017 0.073685274 0.0896265 0.099597022 0.098510191 0.089693591][0.059968792 0.058801871 0.0537197 0.049806677 0.048339106 0.048314627 0.048230462 0.047843423 0.049038012 0.052567784 0.06272012 0.078465134 0.089848481 0.092865206 0.08879292]]...]
INFO - root - 2017-12-11 07:16:13.520609: step 31510, loss = 0.69, batch loss = 0.63 (15.0 examples/sec; 0.534 sec/batch; 44h:37m:04s remains)
INFO - root - 2017-12-11 07:16:19.035896: step 31520, loss = 0.69, batch loss = 0.63 (14.2 examples/sec; 0.564 sec/batch; 47h:08m:45s remains)
INFO - root - 2017-12-11 07:16:24.336572: step 31530, loss = 0.71, batch loss = 0.65 (29.1 examples/sec; 0.275 sec/batch; 22h:59m:45s remains)
INFO - root - 2017-12-11 07:16:29.896224: step 31540, loss = 0.68, batch loss = 0.63 (15.0 examples/sec; 0.533 sec/batch; 44h:31m:44s remains)
INFO - root - 2017-12-11 07:16:35.353478: step 31550, loss = 0.70, batch loss = 0.65 (14.6 examples/sec; 0.549 sec/batch; 45h:53m:32s remains)
INFO - root - 2017-12-11 07:16:40.899189: step 31560, loss = 0.72, batch loss = 0.66 (14.7 examples/sec; 0.546 sec/batch; 45h:37m:38s remains)
INFO - root - 2017-12-11 07:16:46.492483: step 31570, loss = 0.71, batch loss = 0.66 (13.8 examples/sec; 0.580 sec/batch; 48h:29m:51s remains)
INFO - root - 2017-12-11 07:16:51.982706: step 31580, loss = 0.69, batch loss = 0.63 (14.4 examples/sec; 0.555 sec/batch; 46h:25m:02s remains)
INFO - root - 2017-12-11 07:16:57.529806: step 31590, loss = 0.70, batch loss = 0.65 (14.3 examples/sec; 0.560 sec/batch; 46h:50m:46s remains)
INFO - root - 2017-12-11 07:17:03.053277: step 31600, loss = 0.69, batch loss = 0.63 (14.7 examples/sec; 0.545 sec/batch; 45h:34m:20s remains)
2017-12-11 07:17:03.654912: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.39175546 0.46492019 0.5142864 0.53235686 0.51199716 0.46249789 0.41092128 0.37156552 0.33851868 0.30102217 0.25758043 0.21697688 0.19207063 0.1950983 0.21914575][0.41014063 0.49112442 0.54954773 0.57735926 0.57010597 0.53391504 0.48972443 0.4478282 0.40426955 0.35428575 0.29715973 0.24286483 0.21146174 0.22128056 0.26156679][0.37253204 0.457656 0.52377224 0.561829 0.56956023 0.55079114 0.51859963 0.47785422 0.42680532 0.36765626 0.30117244 0.23793145 0.20335326 0.22060214 0.27740535][0.31334019 0.40286458 0.47794461 0.52791834 0.55260974 0.55418724 0.53740281 0.50231212 0.44826153 0.382711 0.30762276 0.23444773 0.19214551 0.2076962 0.27059919][0.26188692 0.36119232 0.45136479 0.51844215 0.56313223 0.58581609 0.58390683 0.55413741 0.49716079 0.42381206 0.337047 0.2488842 0.189697 0.18935379 0.24340877][0.23175795 0.3430357 0.45120949 0.53723025 0.60030091 0.64021587 0.64863223 0.61970091 0.55631131 0.4720231 0.37133759 0.26670748 0.18710768 0.16341127 0.19917214][0.20868181 0.3263022 0.44873878 0.55192709 0.6306951 0.68523806 0.70284873 0.6748687 0.6061824 0.51278645 0.40059656 0.28200006 0.18307774 0.13366106 0.14607733][0.18032016 0.29165539 0.41685972 0.53014028 0.62063074 0.68824184 0.7183972 0.69865686 0.63384116 0.53980511 0.42376229 0.29807219 0.18526143 0.1136646 0.1031213][0.146525 0.23696558 0.34949198 0.46077523 0.55532533 0.63275456 0.67955929 0.67836004 0.63018739 0.54720241 0.43712619 0.31146458 0.19002123 0.10108158 0.070054568][0.098018788 0.16000919 0.24970277 0.34820294 0.43740845 0.51799673 0.58015835 0.60150635 0.57750309 0.51462 0.41994774 0.30357003 0.18227668 0.084614366 0.039084483][0.030496614 0.06149438 0.12281273 0.20050935 0.27613318 0.35220602 0.42395869 0.4666335 0.47020632 0.43449131 0.36481088 0.26966536 0.16133866 0.066373795 0.01377771][-0.040119708 -0.037726875 -0.0051383823 0.048788674 0.10768598 0.17555134 0.25232714 0.3137587 0.34531292 0.33966208 0.29935974 0.23109825 0.14188702 0.055301651 0.00027197268][-0.079714879 -0.099902138 -0.092949145 -0.062397379 -0.02082107 0.035991829 0.11128563 0.18346785 0.23538208 0.25460169 0.24102272 0.19921051 0.1298845 0.052828673 -0.0032530043][-0.070934184 -0.10604017 -0.11804832 -0.10671766 -0.081106417 -0.037495576 0.029234597 0.1015431 0.16259339 0.19867207 0.20708784 0.18905526 0.13863657 0.070595771 0.013124287][-0.02046968 -0.063697144 -0.087652639 -0.088951237 -0.075207464 -0.04412527 0.0098973792 0.07372541 0.13359347 0.17803575 0.20261817 0.2043159 0.17137654 0.11308669 0.056253351]]...]
INFO - root - 2017-12-11 07:17:09.237194: step 31610, loss = 0.71, batch loss = 0.65 (14.5 examples/sec; 0.550 sec/batch; 45h:59m:47s remains)
INFO - root - 2017-12-11 07:17:14.749914: step 31620, loss = 0.69, batch loss = 0.63 (14.4 examples/sec; 0.555 sec/batch; 46h:21m:07s remains)
INFO - root - 2017-12-11 07:17:19.950280: step 31630, loss = 0.69, batch loss = 0.63 (14.6 examples/sec; 0.549 sec/batch; 45h:52m:03s remains)
INFO - root - 2017-12-11 07:17:25.467883: step 31640, loss = 0.68, batch loss = 0.62 (14.4 examples/sec; 0.555 sec/batch; 46h:21m:03s remains)
INFO - root - 2017-12-11 07:17:30.942965: step 31650, loss = 0.68, batch loss = 0.62 (14.8 examples/sec; 0.542 sec/batch; 45h:17m:22s remains)
INFO - root - 2017-12-11 07:17:36.413956: step 31660, loss = 0.69, batch loss = 0.63 (14.8 examples/sec; 0.541 sec/batch; 45h:10m:29s remains)
INFO - root - 2017-12-11 07:17:41.861741: step 31670, loss = 0.70, batch loss = 0.64 (14.2 examples/sec; 0.563 sec/batch; 47h:03m:24s remains)
INFO - root - 2017-12-11 07:17:47.452575: step 31680, loss = 0.70, batch loss = 0.64 (14.4 examples/sec; 0.556 sec/batch; 46h:25m:29s remains)
INFO - root - 2017-12-11 07:17:53.013616: step 31690, loss = 0.72, batch loss = 0.66 (15.0 examples/sec; 0.534 sec/batch; 44h:38m:14s remains)
INFO - root - 2017-12-11 07:17:58.427200: step 31700, loss = 0.70, batch loss = 0.64 (15.1 examples/sec; 0.529 sec/batch; 44h:13m:03s remains)
2017-12-11 07:17:58.984552: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.12959458 0.15100713 0.18423139 0.21682319 0.23257229 0.23128532 0.22220714 0.21353349 0.19820069 0.17659339 0.1530169 0.12739775 0.097584583 0.061974283 0.025445245][0.18206066 0.20867167 0.24109483 0.26699013 0.2709907 0.25772366 0.24116789 0.23055421 0.21653543 0.19737056 0.17877991 0.15736021 0.1269161 0.085610241 0.040698931][0.23764284 0.26834965 0.29496476 0.30590907 0.29115453 0.26266208 0.24008103 0.23364155 0.22963227 0.2220712 0.21422832 0.19908772 0.16726543 0.11749677 0.06176272][0.26702061 0.2991733 0.31895056 0.31478888 0.28199655 0.24008583 0.21530697 0.21951084 0.23290895 0.24109447 0.24328749 0.23126432 0.19556144 0.13754533 0.073310323][0.27387726 0.30425966 0.3186017 0.30454043 0.26043254 0.20967256 0.1829951 0.19391187 0.21944538 0.23942299 0.24925375 0.24046357 0.20452607 0.14467645 0.078331262][0.30436122 0.33456883 0.34665662 0.32826895 0.27952027 0.22419755 0.19338337 0.1996987 0.22180691 0.23996475 0.24952763 0.2414396 0.20759052 0.15077463 0.086505353][0.35570514 0.38750127 0.39667332 0.37253273 0.32114372 0.26801682 0.23876545 0.23907186 0.24957646 0.25568092 0.25612953 0.24238902 0.20802926 0.15470594 0.094521657][0.38271406 0.41300589 0.41606969 0.38473311 0.33484903 0.29376784 0.27726194 0.28043661 0.28401357 0.27925384 0.26871455 0.24680015 0.20928301 0.15706556 0.098980278][0.36719698 0.39584619 0.39574629 0.36126351 0.31629559 0.28800452 0.2828396 0.28772452 0.28487039 0.27173549 0.25408083 0.22808655 0.19033338 0.14188533 0.088493131][0.29700509 0.3227528 0.32183903 0.28843203 0.24980079 0.23002198 0.22920966 0.23033836 0.2208546 0.20548035 0.19021459 0.16868651 0.13697992 0.097512275 0.054564096][0.18844889 0.2097844 0.21014602 0.18365994 0.15589844 0.14568678 0.14790559 0.14498444 0.13133593 0.1173996 0.1079355 0.09211278 0.066355936 0.035661642 0.0043432084][0.075205632 0.090329438 0.091885425 0.074465729 0.060103152 0.061046444 0.067433178 0.063138612 0.048750293 0.037572462 0.032095071 0.019611387 -0.0016569329 -0.024006544 -0.043273583][-0.0051873093 0.0039678728 0.005592891 -0.0047459868 -0.0090512587 -0.00087711436 0.0082473028 0.005413271 -0.0058362437 -0.012761536 -0.015455462 -0.025148122 -0.041833688 -0.0564883 -0.065759711][-0.035413414 -0.030746289 -0.029472489 -0.035211422 -0.035239723 -0.026938325 -0.0194248 -0.021182468 -0.028604306 -0.031811118 -0.032716285 -0.039646357 -0.051596783 -0.060379189 -0.063972183][-0.049203731 -0.048080221 -0.046754394 -0.049110945 -0.048147339 -0.043079622 -0.039026093 -0.04019089 -0.044013958 -0.044613089 -0.044568744 -0.048787162 -0.05570225 -0.05969125 -0.060088631]]...]
INFO - root - 2017-12-11 07:18:04.524703: step 31710, loss = 0.70, batch loss = 0.64 (14.5 examples/sec; 0.553 sec/batch; 46h:10m:41s remains)
INFO - root - 2017-12-11 07:18:09.986590: step 31720, loss = 0.69, batch loss = 0.63 (15.1 examples/sec; 0.528 sec/batch; 44h:08m:10s remains)
INFO - root - 2017-12-11 07:18:15.060208: step 31730, loss = 0.68, batch loss = 0.62 (14.6 examples/sec; 0.548 sec/batch; 45h:46m:34s remains)
INFO - root - 2017-12-11 07:18:20.633667: step 31740, loss = 0.70, batch loss = 0.64 (14.1 examples/sec; 0.566 sec/batch; 47h:16m:59s remains)
INFO - root - 2017-12-11 07:18:26.080800: step 31750, loss = 0.69, batch loss = 0.64 (14.7 examples/sec; 0.543 sec/batch; 45h:21m:38s remains)
INFO - root - 2017-12-11 07:18:31.607620: step 31760, loss = 0.69, batch loss = 0.63 (14.2 examples/sec; 0.564 sec/batch; 47h:06m:38s remains)
INFO - root - 2017-12-11 07:18:37.165571: step 31770, loss = 0.69, batch loss = 0.63 (14.8 examples/sec; 0.541 sec/batch; 45h:13m:22s remains)
INFO - root - 2017-12-11 07:18:42.709998: step 31780, loss = 0.70, batch loss = 0.64 (14.5 examples/sec; 0.552 sec/batch; 46h:06m:10s remains)
INFO - root - 2017-12-11 07:18:48.143225: step 31790, loss = 0.68, batch loss = 0.63 (14.5 examples/sec; 0.554 sec/batch; 46h:14m:30s remains)
INFO - root - 2017-12-11 07:18:53.704381: step 31800, loss = 0.69, batch loss = 0.63 (14.7 examples/sec; 0.546 sec/batch; 45h:35m:30s remains)
2017-12-11 07:18:54.308722: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.039108828 0.04710139 0.051572893 0.051761266 0.04957566 0.048369274 0.049721457 0.053451423 0.057775792 0.058600936 0.054920513 0.046696588 0.035938457 0.025161726 0.016008129][0.075902268 0.09541595 0.10891076 0.11419087 0.1127849 0.10878053 0.10536215 0.1037929 0.10350693 0.099627182 0.09006647 0.074544944 0.056193985 0.038688038 0.023807297][0.12608613 0.1616758 0.18795666 0.19983338 0.19773033 0.18688773 0.17326824 0.16096547 0.15153009 0.14000437 0.12278593 0.099644318 0.074546978 0.051566761 0.032274526][0.17967966 0.23140673 0.27019721 0.28756261 0.2825264 0.26195768 0.23586425 0.21124637 0.19152534 0.17172343 0.14672765 0.11659313 0.085868105 0.058444642 0.035741303][0.22524405 0.29029799 0.33943415 0.36191878 0.35537255 0.32814837 0.2936832 0.260575 0.23274364 0.20534194 0.17256287 0.13490245 0.097706258 0.064654619 0.037762247][0.25958246 0.33480194 0.39243877 0.42074603 0.41614777 0.38765988 0.35086441 0.31493121 0.28259963 0.24884623 0.20830543 0.16187733 0.11587796 0.074594788 0.041648507][0.27847627 0.35964987 0.42175874 0.45362303 0.45133558 0.42406777 0.38813639 0.35258761 0.3187817 0.28163868 0.23682661 0.18484132 0.13206357 0.083708309 0.045445789][0.27508149 0.35469916 0.41498718 0.44668874 0.44658846 0.42370218 0.39271125 0.361334 0.32974342 0.29317597 0.24880104 0.19606449 0.14075205 0.08875943 0.047583245][0.24535817 0.31544983 0.36799219 0.39682111 0.39964357 0.38474411 0.36378303 0.34159321 0.31672284 0.28504989 0.24568826 0.19685304 0.14281452 0.090582572 0.048809756][0.19603607 0.25221589 0.29460928 0.31991762 0.32611758 0.32000574 0.30952114 0.29677144 0.27909526 0.25345102 0.22132616 0.18020138 0.13253525 0.085352361 0.047300648][0.13954426 0.1813221 0.21349339 0.23445606 0.24231125 0.24195057 0.23759082 0.22954762 0.21577941 0.19505736 0.17076443 0.14016566 0.10407578 0.068305105 0.039440818][0.083965831 0.11146688 0.1330134 0.1478876 0.15429258 0.1556754 0.15349843 0.14702642 0.13562366 0.11992307 0.10422309 0.085945278 0.064445116 0.04347628 0.02669185][0.037794884 0.052219722 0.0636568 0.071838021 0.075043887 0.075848944 0.074466884 0.06952972 0.061066311 0.050919481 0.043529429 0.036777988 0.028707126 0.020939087 0.014689916][0.0071268189 0.012083589 0.016003281 0.01889907 0.01921179 0.018630696 0.017154511 0.013370074 0.0075578005 0.0020096537 0.00082036451 0.0022117316 0.0037282503 0.0053036218 0.0064194021][-0.0079617249 -0.00820397 -0.0084434738 -0.00854192 -0.0097775292 -0.011453541 -0.013759358 -0.017541081 -0.022011952 -0.024753202 -0.022842525 -0.017339488 -0.01039392 -0.0034643386 0.0020564219]]...]
INFO - root - 2017-12-11 07:18:59.917090: step 31810, loss = 0.68, batch loss = 0.62 (14.7 examples/sec; 0.544 sec/batch; 45h:26m:26s remains)
INFO - root - 2017-12-11 07:19:05.414069: step 31820, loss = 0.71, batch loss = 0.65 (14.7 examples/sec; 0.546 sec/batch; 45h:36m:30s remains)
INFO - root - 2017-12-11 07:19:10.698904: step 31830, loss = 0.70, batch loss = 0.64 (14.3 examples/sec; 0.560 sec/batch; 46h:46m:10s remains)
INFO - root - 2017-12-11 07:19:16.219239: step 31840, loss = 0.71, batch loss = 0.65 (14.7 examples/sec; 0.545 sec/batch; 45h:32m:50s remains)
INFO - root - 2017-12-11 07:19:21.710246: step 31850, loss = 0.69, batch loss = 0.63 (14.7 examples/sec; 0.543 sec/batch; 45h:23m:09s remains)
INFO - root - 2017-12-11 07:19:27.262147: step 31860, loss = 0.71, batch loss = 0.65 (14.6 examples/sec; 0.549 sec/batch; 45h:51m:17s remains)
INFO - root - 2017-12-11 07:19:32.703865: step 31870, loss = 0.70, batch loss = 0.65 (14.7 examples/sec; 0.546 sec/batch; 45h:35m:27s remains)
INFO - root - 2017-12-11 07:19:38.202387: step 31880, loss = 0.71, batch loss = 0.65 (14.9 examples/sec; 0.538 sec/batch; 44h:53m:39s remains)
INFO - root - 2017-12-11 07:19:43.768850: step 31890, loss = 0.70, batch loss = 0.64 (14.4 examples/sec; 0.556 sec/batch; 46h:26m:16s remains)
INFO - root - 2017-12-11 07:19:49.286115: step 31900, loss = 0.69, batch loss = 0.63 (14.3 examples/sec; 0.560 sec/batch; 46h:44m:00s remains)
2017-12-11 07:19:49.870333: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.044463143 0.048018768 0.051703092 0.056713533 0.062167175 0.067762546 0.075793184 0.086699411 0.098682754 0.1084655 0.11091866 0.099819526 0.072795384 0.033958159 -0.0072344518][0.057301324 0.060870823 0.067332506 0.078817092 0.092758 0.10586699 0.11792789 0.12832566 0.13479856 0.13538398 0.12787691 0.10809125 0.074732892 0.032266457 -0.010189801][0.067690924 0.072067678 0.082341872 0.10161907 0.12600404 0.14939539 0.1677943 0.17781639 0.17619593 0.16318779 0.14079435 0.10882828 0.068441913 0.024307013 -0.016289987][0.081007466 0.088553682 0.103893 0.13070057 0.16529258 0.1993169 0.22428696 0.23303321 0.22159313 0.19244498 0.15179384 0.10514037 0.057029795 0.012104107 -0.025181653][0.098268084 0.11080302 0.13180034 0.16517939 0.20941673 0.25446716 0.28643918 0.29395959 0.27316263 0.22872998 0.17001402 0.10830908 0.051907241 0.0051227189 -0.0306][0.11991606 0.13761391 0.16336587 0.20162888 0.2544829 0.31070292 0.35080567 0.35947052 0.33322471 0.27785504 0.20449679 0.12919773 0.063930437 0.012625046 -0.025606802][0.14663029 0.16866288 0.19674879 0.23692736 0.29564404 0.36092582 0.40862697 0.42026109 0.3926152 0.33161488 0.24821119 0.16182691 0.087784685 0.030315241 -0.012928589][0.19506276 0.21764286 0.2405595 0.27361017 0.32812038 0.3929424 0.44129756 0.45248571 0.42424691 0.36267257 0.27680898 0.18623604 0.10813391 0.046935435 -0.00077532197][0.26879525 0.28755131 0.29651326 0.31091481 0.34671098 0.39592391 0.43268031 0.43648714 0.40644327 0.34903753 0.27056751 0.1876965 0.11584674 0.057808902 0.0094432076][0.35066292 0.36339742 0.35489467 0.34493738 0.35257459 0.37472418 0.38990292 0.38048831 0.34741566 0.29790014 0.23521258 0.17054154 0.11405764 0.065115035 0.019215623][0.4191747 0.42631343 0.40129668 0.36665475 0.34289539 0.33294871 0.32249606 0.29883349 0.264922 0.22754329 0.18589771 0.14410339 0.10570119 0.067122415 0.024669649][0.4432562 0.44633687 0.40977693 0.35653651 0.30687183 0.2695393 0.23827198 0.20583735 0.17615019 0.15397666 0.13415845 0.11385686 0.091011681 0.060946833 0.022072557][0.40028378 0.40010029 0.35944173 0.29897219 0.23699647 0.18615493 0.14659603 0.11516079 0.094988883 0.087846167 0.085579805 0.080693141 0.06807556 0.043888185 0.0091603324][0.29016078 0.28871998 0.25348052 0.19992854 0.14258386 0.094786629 0.060286868 0.038366023 0.030068025 0.034157556 0.0416005 0.044069022 0.036413278 0.016667584 -0.012238335][0.15061159 0.14892246 0.12471759 0.086954288 0.045465987 0.011515278 -0.010301512 -0.019787217 -0.017768255 -0.0075891423 0.0025513899 0.0061840862 0.00022435856 -0.01532676 -0.036959682]]...]
INFO - root - 2017-12-11 07:19:55.392026: step 31910, loss = 0.72, batch loss = 0.66 (14.5 examples/sec; 0.551 sec/batch; 46h:00m:39s remains)
INFO - root - 2017-12-11 07:20:00.481474: step 31920, loss = 0.68, batch loss = 0.63 (14.4 examples/sec; 0.556 sec/batch; 46h:27m:21s remains)
INFO - root - 2017-12-11 07:20:05.979163: step 31930, loss = 0.68, batch loss = 0.63 (14.7 examples/sec; 0.545 sec/batch; 45h:29m:33s remains)
INFO - root - 2017-12-11 07:20:11.460000: step 31940, loss = 0.69, batch loss = 0.63 (14.2 examples/sec; 0.563 sec/batch; 46h:59m:27s remains)
INFO - root - 2017-12-11 07:20:17.118702: step 31950, loss = 0.69, batch loss = 0.63 (13.8 examples/sec; 0.581 sec/batch; 48h:31m:55s remains)
INFO - root - 2017-12-11 07:20:22.619229: step 31960, loss = 0.70, batch loss = 0.64 (14.5 examples/sec; 0.552 sec/batch; 46h:03m:21s remains)
INFO - root - 2017-12-11 07:20:28.045298: step 31970, loss = 0.68, batch loss = 0.62 (14.4 examples/sec; 0.556 sec/batch; 46h:26m:09s remains)
INFO - root - 2017-12-11 07:20:33.604643: step 31980, loss = 0.70, batch loss = 0.65 (13.9 examples/sec; 0.576 sec/batch; 48h:04m:55s remains)
INFO - root - 2017-12-11 07:20:39.099007: step 31990, loss = 0.71, batch loss = 0.65 (14.1 examples/sec; 0.566 sec/batch; 47h:13m:34s remains)
INFO - root - 2017-12-11 07:20:44.524351: step 32000, loss = 0.70, batch loss = 0.65 (14.7 examples/sec; 0.546 sec/batch; 45h:33m:38s remains)
2017-12-11 07:20:45.106069: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.037041966 -0.024210714 -0.0065138275 0.0155626 0.035157472 0.046533417 0.052447114 0.056451261 0.053888641 0.043406036 0.030376324 0.020898713 0.01267408 0.0001376524 -0.016184483][-0.025629457 -0.0055162739 0.024508175 0.064178251 0.10221998 0.12848775 0.14602076 0.15957078 0.16127723 0.14815758 0.12770937 0.11052051 0.09382157 0.068160541 0.034733687][-0.012817071 0.014911084 0.059475534 0.12113074 0.18299714 0.22890064 0.261593 0.28760287 0.29714158 0.28447071 0.25680384 0.22783467 0.19544895 0.14924073 0.092951067][-0.0057462007 0.027456537 0.08436238 0.16567792 0.24946742 0.31359959 0.35981375 0.3967143 0.41491252 0.40570703 0.3727577 0.33016267 0.2785815 0.21031095 0.13272388][-0.0062225652 0.029826516 0.094603784 0.18840821 0.28666198 0.36324638 0.41908121 0.46363625 0.48916391 0.48464316 0.448662 0.39383784 0.32400793 0.23699071 0.14466269][-0.01184369 0.025276856 0.093350276 0.19199836 0.29683027 0.38110802 0.44581765 0.49798506 0.52972615 0.52860475 0.49006972 0.4253318 0.34053272 0.2402844 0.14124103][-0.02017823 0.015725449 0.08184728 0.17696284 0.27988082 0.36707374 0.44027233 0.50139612 0.539929 0.54224145 0.5033623 0.43381053 0.34059054 0.2342958 0.13582619][-0.027909921 0.0052554933 0.06564083 0.15105253 0.24489982 0.3287884 0.40584046 0.47342283 0.51803493 0.5251298 0.49108878 0.42603555 0.3360424 0.23391087 0.14290208][-0.030050233 0.0019787066 0.057962116 0.13444486 0.21798845 0.29399002 0.36688715 0.43272975 0.47732881 0.48675081 0.45972979 0.40648818 0.33045483 0.24230671 0.16454343][-0.024827912 0.010885957 0.068997823 0.14395049 0.22222658 0.28998181 0.3520214 0.4063029 0.44198948 0.448545 0.42751774 0.38912806 0.33295369 0.2643339 0.20185792][-0.015721215 0.027612824 0.093555979 0.17321336 0.25041735 0.31006041 0.35636634 0.3909623 0.41000178 0.4097043 0.39415866 0.37181631 0.33728728 0.28976882 0.24255046][-0.0069840397 0.044719163 0.11929813 0.2033762 0.27740464 0.32540482 0.35168141 0.36235189 0.36176974 0.35397509 0.34514216 0.3400124 0.32710525 0.29968724 0.26702845][-0.00016128541 0.059429847 0.14157073 0.22772987 0.29497656 0.32735884 0.33074397 0.31600326 0.29548267 0.28062886 0.27887037 0.29029837 0.2976703 0.28916016 0.27133492][0.0038535998 0.07073421 0.16066666 0.24973203 0.31169805 0.3302452 0.3130081 0.27389106 0.23211128 0.20649332 0.20621923 0.22787125 0.25021186 0.25872767 0.25770739][0.0052147754 0.078367695 0.17606473 0.26962334 0.33016187 0.34091675 0.31050733 0.25397667 0.19493431 0.15808284 0.15385796 0.17682524 0.20497166 0.22506803 0.24036831]]...]
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-SGD-lr0.001-clip50-initfromfixqian30k-fixqian
INFO - root - 2017-12-11 07:20:50.683371: step 32010, loss = 0.70, batch loss = 0.64 (14.7 examples/sec; 0.546 sec/batch; 45h:34m:39s remains)
INFO - root - 2017-12-11 07:20:55.816614: step 32020, loss = 0.70, batch loss = 0.64 (14.4 examples/sec; 0.554 sec/batch; 46h:16m:08s remains)
INFO - root - 2017-12-11 07:21:01.376146: step 32030, loss = 0.69, batch loss = 0.63 (14.3 examples/sec; 0.560 sec/batch; 46h:42m:28s remains)
INFO - root - 2017-12-11 07:21:06.916559: step 32040, loss = 0.73, batch loss = 0.67 (14.6 examples/sec; 0.548 sec/batch; 45h:43m:48s remains)
INFO - root - 2017-12-11 07:21:12.422572: step 32050, loss = 0.70, batch loss = 0.64 (14.8 examples/sec; 0.542 sec/batch; 45h:15m:29s remains)
INFO - root - 2017-12-11 07:21:17.959141: step 32060, loss = 0.69, batch loss = 0.63 (14.4 examples/sec; 0.557 sec/batch; 46h:30m:08s remains)
INFO - root - 2017-12-11 07:21:23.453694: step 32070, loss = 0.70, batch loss = 0.64 (15.0 examples/sec; 0.532 sec/batch; 44h:25m:32s remains)
INFO - root - 2017-12-11 07:21:28.858885: step 32080, loss = 0.71, batch loss = 0.65 (14.8 examples/sec; 0.541 sec/batch; 45h:07m:53s remains)
INFO - root - 2017-12-11 07:21:34.330328: step 32090, loss = 0.69, batch loss = 0.63 (14.1 examples/sec; 0.566 sec/batch; 47h:13m:30s remains)
INFO - root - 2017-12-11 07:21:39.959575: step 32100, loss = 0.70, batch loss = 0.64 (14.5 examples/sec; 0.552 sec/batch; 46h:01m:49s remains)
2017-12-11 07:21:40.567005: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.18591124 0.25481489 0.31184158 0.35172555 0.36661655 0.3545348 0.32500625 0.29337692 0.25694248 0.21239598 0.16478132 0.11796342 0.074570648 0.036348291 0.0082238624][0.2002479 0.27771774 0.33792546 0.37698081 0.39197794 0.38013241 0.35048425 0.3205038 0.28790486 0.24809377 0.20332664 0.15675691 0.11160587 0.06966795 0.036745623][0.20359996 0.28228658 0.33940631 0.373561 0.38821149 0.3802219 0.35590291 0.33347729 0.31050092 0.28174922 0.24568361 0.20328334 0.15858945 0.11360009 0.075290486][0.20848817 0.28704795 0.34053391 0.3695417 0.3840799 0.38027406 0.36061952 0.34350577 0.32799974 0.30889267 0.28092539 0.24352133 0.20184349 0.15751754 0.1168815][0.21980983 0.3016791 0.3555131 0.38242683 0.39773855 0.39554122 0.37403584 0.35271388 0.33611429 0.32012907 0.2960262 0.26352102 0.22924992 0.19223875 0.15514596][0.24106807 0.330701 0.38903323 0.41763496 0.43439054 0.42985547 0.39795873 0.36084628 0.33274928 0.31152081 0.28581524 0.25772315 0.23574574 0.21368349 0.18708003][0.26510575 0.36274496 0.42751729 0.46212924 0.48404145 0.47841939 0.43362868 0.37506616 0.32933426 0.29687977 0.26504636 0.23986623 0.23200344 0.22859867 0.21588026][0.27859747 0.37842926 0.4461875 0.48710933 0.51584363 0.5123108 0.45961457 0.38481566 0.32484975 0.28240481 0.245085 0.22281341 0.22747782 0.24014275 0.23896849][0.27166346 0.36689511 0.43303114 0.47740871 0.51065469 0.50999379 0.45597145 0.37451661 0.3074308 0.25886464 0.21903788 0.20108435 0.21620175 0.2406944 0.24780226][0.2384861 0.3241989 0.38494909 0.42822269 0.46057922 0.46029583 0.40898323 0.32863075 0.25957465 0.20787826 0.16876435 0.15687525 0.18072084 0.21376255 0.22790851][0.18224762 0.25334316 0.30386239 0.33999661 0.36562806 0.36358964 0.31893963 0.24750637 0.18281406 0.13251737 0.097385339 0.091533631 0.12009448 0.15714142 0.17654398][0.1106864 0.16109158 0.19545445 0.21868633 0.23390184 0.22996941 0.19658883 0.14250477 0.090981707 0.04944279 0.0222115 0.021149918 0.048454959 0.082845859 0.10338723][0.032768544 0.059994165 0.076095857 0.084606826 0.088552244 0.082422264 0.060785558 0.027095379 -0.0057833125 -0.032698467 -0.048815377 -0.045943681 -0.023930658 0.002737382 0.020233229][-0.034472402 -0.02407993 -0.019373046 -0.019307371 -0.021786334 -0.028936958 -0.042425893 -0.061134368 -0.079367884 -0.094187491 -0.10177086 -0.097247638 -0.081454 -0.062892437 -0.049208179][-0.074317843 -0.071649686 -0.069762446 -0.070081532 -0.072264478 -0.07721547 -0.085020162 -0.095296435 -0.10557339 -0.11431409 -0.11854415 -0.11560672 -0.1062779 -0.094716057 -0.08471112]]...]
INFO - root - 2017-12-11 07:21:46.205027: step 32110, loss = 0.70, batch loss = 0.64 (14.4 examples/sec; 0.557 sec/batch; 46h:29m:54s remains)
INFO - root - 2017-12-11 07:21:51.438941: step 32120, loss = 0.69, batch loss = 0.64 (14.6 examples/sec; 0.547 sec/batch; 45h:38m:45s remains)
INFO - root - 2017-12-11 07:21:56.944428: step 32130, loss = 0.69, batch loss = 0.63 (14.6 examples/sec; 0.546 sec/batch; 45h:34m:46s remains)
INFO - root - 2017-12-11 07:22:02.414081: step 32140, loss = 0.70, batch loss = 0.64 (14.5 examples/sec; 0.552 sec/batch; 46h:04m:22s remains)
INFO - root - 2017-12-11 07:22:07.859639: step 32150, loss = 0.71, batch loss = 0.65 (14.8 examples/sec; 0.540 sec/batch; 45h:01m:04s remains)
INFO - root - 2017-12-11 07:22:13.421227: step 32160, loss = 0.70, batch loss = 0.65 (13.9 examples/sec; 0.576 sec/batch; 48h:05m:20s remains)
INFO - root - 2017-12-11 07:22:18.943557: step 32170, loss = 0.69, batch loss = 0.63 (14.2 examples/sec; 0.565 sec/batch; 47h:06m:32s remains)
INFO - root - 2017-12-11 07:22:24.381253: step 32180, loss = 0.69, batch loss = 0.63 (14.6 examples/sec; 0.548 sec/batch; 45h:40m:36s remains)
INFO - root - 2017-12-11 07:22:29.889324: step 32190, loss = 0.71, batch loss = 0.65 (14.1 examples/sec; 0.566 sec/batch; 47h:14m:32s remains)
INFO - root - 2017-12-11 07:22:35.413477: step 32200, loss = 0.69, batch loss = 0.64 (14.3 examples/sec; 0.561 sec/batch; 46h:49m:29s remains)
2017-12-11 07:22:36.009425: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.21217385 0.15267524 0.10601386 0.087104268 0.089949146 0.099822722 0.10192006 0.090899281 0.075955063 0.068050675 0.07639946 0.102289 0.14082044 0.18096888 0.20380165][0.21554352 0.14976612 0.10334291 0.093940742 0.11094689 0.13314317 0.14225134 0.13208881 0.11307982 0.098753713 0.10195366 0.12620313 0.16846004 0.21811134 0.25390562][0.1962149 0.13456026 0.098128662 0.10345232 0.13658954 0.17251103 0.19062859 0.18420911 0.16386098 0.14514759 0.14507301 0.16987415 0.21834598 0.2805849 0.33291239][0.16726241 0.11635901 0.093679629 0.11262909 0.15809503 0.20443772 0.23040089 0.22887728 0.20965114 0.18994647 0.19028629 0.21890862 0.27508035 0.34926838 0.41640419][0.14347927 0.10697087 0.099662833 0.13094679 0.18487066 0.23744747 0.26881281 0.27211744 0.25589669 0.23837405 0.24232481 0.27659327 0.33842281 0.41788533 0.4893696][0.1388377 0.11731789 0.12450057 0.1677082 0.23038802 0.29103777 0.33150125 0.34392652 0.33307827 0.31604183 0.3182607 0.35080385 0.40941402 0.48187 0.54239327][0.15296891 0.14593366 0.1655502 0.21976638 0.29227972 0.36417434 0.41850021 0.44355831 0.43793428 0.41648394 0.40947914 0.43312728 0.48146743 0.53802878 0.57648522][0.18210796 0.18855466 0.21875548 0.28162819 0.36235443 0.44456246 0.51150042 0.54609305 0.54120374 0.51080048 0.49118021 0.50359792 0.5393827 0.57684988 0.58883452][0.21067238 0.22779244 0.26392 0.32885543 0.41031718 0.49489698 0.56556028 0.60038954 0.58962148 0.54832685 0.51753676 0.52088338 0.54683864 0.56984627 0.56229603][0.22070894 0.24329281 0.27974656 0.33902651 0.41178054 0.4881106 0.55126095 0.57763964 0.55801725 0.50908107 0.47223786 0.46958619 0.4884713 0.50305122 0.48721516][0.21237159 0.23270537 0.26162446 0.30610505 0.36026958 0.41866603 0.46685749 0.48250014 0.45773959 0.40887746 0.37271371 0.36677572 0.37874368 0.38639152 0.36859998][0.19778243 0.20832063 0.22137608 0.24272639 0.27085096 0.3054418 0.33637428 0.34444124 0.32195932 0.28167126 0.25224644 0.24552767 0.25068408 0.25148192 0.23400874][0.19177279 0.18742631 0.17999166 0.17633875 0.17939854 0.19323255 0.21210814 0.22011913 0.20747291 0.18056542 0.15863188 0.14892808 0.14400132 0.13540229 0.11782698][0.19223128 0.17308384 0.14637098 0.12246711 0.10865447 0.11195066 0.12914054 0.146011 0.14885536 0.13642342 0.11905652 0.10160217 0.082532614 0.062217776 0.043764692][0.18597977 0.15771671 0.12062124 0.08849185 0.071621865 0.077488422 0.10248052 0.13269162 0.15067866 0.14941852 0.13344319 0.10727847 0.075320423 0.044836655 0.025513764]]...]
INFO - root - 2017-12-11 07:22:41.340023: step 32210, loss = 0.69, batch loss = 0.64 (24.0 examples/sec; 0.333 sec/batch; 27h:47m:11s remains)
INFO - root - 2017-12-11 07:22:46.774882: step 32220, loss = 0.68, batch loss = 0.62 (14.6 examples/sec; 0.549 sec/batch; 45h:48m:42s remains)
INFO - root - 2017-12-11 07:22:52.313157: step 32230, loss = 0.72, batch loss = 0.66 (14.8 examples/sec; 0.541 sec/batch; 45h:09m:23s remains)
INFO - root - 2017-12-11 07:22:57.756244: step 32240, loss = 0.70, batch loss = 0.64 (14.5 examples/sec; 0.553 sec/batch; 46h:09m:19s remains)
INFO - root - 2017-12-11 07:23:03.295416: step 32250, loss = 0.68, batch loss = 0.62 (14.0 examples/sec; 0.571 sec/batch; 47h:36m:41s remains)
INFO - root - 2017-12-11 07:23:08.859311: step 32260, loss = 0.69, batch loss = 0.63 (14.4 examples/sec; 0.555 sec/batch; 46h:17m:32s remains)
INFO - root - 2017-12-11 07:23:14.348699: step 32270, loss = 0.69, batch loss = 0.63 (14.5 examples/sec; 0.551 sec/batch; 45h:58m:05s remains)
INFO - root - 2017-12-11 07:23:19.794403: step 32280, loss = 0.70, batch loss = 0.64 (14.6 examples/sec; 0.546 sec/batch; 45h:32m:35s remains)
INFO - root - 2017-12-11 07:23:25.285447: step 32290, loss = 0.70, batch loss = 0.65 (14.3 examples/sec; 0.560 sec/batch; 46h:41m:36s remains)
INFO - root - 2017-12-11 07:23:30.807962: step 32300, loss = 0.68, batch loss = 0.62 (14.4 examples/sec; 0.556 sec/batch; 46h:22m:31s remains)
2017-12-11 07:23:31.360568: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.24170448 0.1429918 0.078750409 0.065836228 0.088259235 0.12632094 0.16201757 0.18155046 0.17558192 0.14298311 0.08956071 0.031541966 -0.018353036 -0.044212159 -0.040603533][0.26804981 0.18072996 0.13169266 0.13448459 0.17295764 0.22647625 0.27489668 0.30194533 0.2973263 0.25927004 0.19156072 0.11105724 0.035010859 -0.013611802 -0.026140139][0.27166352 0.21064149 0.1901468 0.21616936 0.27315843 0.34151661 0.40029597 0.43080521 0.42338985 0.3771753 0.29455683 0.19239599 0.0908546 0.019040514 -0.010281327][0.25780264 0.23502049 0.25265771 0.30670592 0.38205016 0.46121565 0.5241918 0.55128539 0.53473163 0.47564384 0.3766363 0.25539553 0.13376008 0.044542469 0.0035467837][0.22617367 0.24799921 0.30708802 0.38974521 0.48074082 0.56469327 0.62458926 0.64248466 0.61257547 0.53793287 0.4227792 0.28729951 0.1548806 0.058490749 0.014402566][0.18030848 0.24457802 0.34005922 0.44586238 0.54693031 0.63010132 0.68202513 0.6878947 0.64370745 0.55452311 0.42701435 0.28446621 0.1519679 0.060073886 0.023036744][0.13056412 0.22610919 0.34500387 0.46304452 0.56635427 0.64378738 0.68440527 0.67680478 0.61885333 0.51816022 0.38471326 0.24487807 0.12567338 0.052958988 0.036017686][0.088368036 0.19689564 0.320676 0.43649849 0.53249854 0.59845585 0.624874 0.60413051 0.53617233 0.43143493 0.30275822 0.17840186 0.087109029 0.048505496 0.064376988][0.060917474 0.1625912 0.27279666 0.37232119 0.45124581 0.49941704 0.50881743 0.47580904 0.40272146 0.30301106 0.19140592 0.095468238 0.044161148 0.050513063 0.10785113][0.05079079 0.12971443 0.21200751 0.28441608 0.33855876 0.36478016 0.35691151 0.31457204 0.24208137 0.1551241 0.069306321 0.0091294022 0.0014673806 0.053460736 0.15090235][0.060689043 0.10753361 0.15535223 0.19712432 0.22526485 0.23095125 0.20951916 0.16268869 0.096126027 0.026091693 -0.032034114 -0.0583369 -0.029662073 0.05688969 0.18219431][0.08057671 0.095747657 0.11236164 0.12897158 0.13777651 0.13098894 0.10462356 0.060715571 0.0047289585 -0.048450816 -0.084269494 -0.0873482 -0.038697731 0.062436525 0.19607612][0.096482277 0.087595962 0.083859257 0.087128177 0.088873662 0.081868991 0.061803676 0.029216863 -0.012620633 -0.052055366 -0.075778767 -0.071386009 -0.023318008 0.069315925 0.19001287][0.10592217 0.085837379 0.074126147 0.074893706 0.07938882 0.080573514 0.073087394 0.055291083 0.027512807 -0.002717929 -0.024123687 -0.025568273 0.0060855756 0.073261037 0.16553174][0.11458261 0.094958521 0.084423348 0.0885815 0.099464223 0.11007805 0.11405946 0.1083176 0.09097895 0.065730534 0.040922523 0.027409485 0.036887076 0.073492751 0.13264652]]...]
INFO - root - 2017-12-11 07:23:36.653144: step 32310, loss = 0.71, batch loss = 0.65 (14.6 examples/sec; 0.547 sec/batch; 45h:36m:48s remains)
INFO - root - 2017-12-11 07:23:42.202294: step 32320, loss = 0.71, batch loss = 0.65 (14.3 examples/sec; 0.558 sec/batch; 46h:33m:20s remains)
INFO - root - 2017-12-11 07:23:47.719001: step 32330, loss = 0.71, batch loss = 0.65 (14.8 examples/sec; 0.542 sec/batch; 45h:09m:05s remains)
INFO - root - 2017-12-11 07:23:53.183838: step 32340, loss = 0.72, batch loss = 0.66 (14.3 examples/sec; 0.559 sec/batch; 46h:35m:02s remains)
INFO - root - 2017-12-11 07:23:58.716506: step 32350, loss = 0.70, batch loss = 0.64 (14.7 examples/sec; 0.545 sec/batch; 45h:26m:39s remains)
INFO - root - 2017-12-11 07:24:04.226511: step 32360, loss = 0.71, batch loss = 0.65 (14.4 examples/sec; 0.554 sec/batch; 46h:12m:00s remains)
INFO - root - 2017-12-11 07:24:09.656621: step 32370, loss = 0.68, batch loss = 0.63 (14.3 examples/sec; 0.560 sec/batch; 46h:42m:29s remains)
INFO - root - 2017-12-11 07:24:15.099726: step 32380, loss = 0.69, batch loss = 0.63 (15.0 examples/sec; 0.533 sec/batch; 44h:25m:36s remains)
INFO - root - 2017-12-11 07:24:20.654237: step 32390, loss = 0.70, batch loss = 0.64 (14.7 examples/sec; 0.544 sec/batch; 45h:23m:22s remains)
INFO - root - 2017-12-11 07:24:26.159923: step 32400, loss = 0.72, batch loss = 0.66 (14.5 examples/sec; 0.553 sec/batch; 46h:03m:31s remains)
2017-12-11 07:24:26.785404: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.25438175 0.23770137 0.20576508 0.17574598 0.16124772 0.16457005 0.17203376 0.16196375 0.13141838 0.10167371 0.092583977 0.11210968 0.15379214 0.20662162 0.25617048][0.27490368 0.25967374 0.22958027 0.20240577 0.19348654 0.20424165 0.21482271 0.20252928 0.16746894 0.13344346 0.11879572 0.13267469 0.17577268 0.24219863 0.31352645][0.29084578 0.28601328 0.26650149 0.25045577 0.25518152 0.27968702 0.29677859 0.28286383 0.24121104 0.19610064 0.1651454 0.16416933 0.20517415 0.28718731 0.38539919][0.31545302 0.32782331 0.32547152 0.32805911 0.35422859 0.39836654 0.42485225 0.40902522 0.357262 0.29353 0.23759343 0.21743596 0.25796622 0.35983834 0.48832363][0.33953518 0.37079695 0.3882257 0.41451889 0.46725547 0.53445327 0.57250452 0.555136 0.49072808 0.40388814 0.32060081 0.28353608 0.32727656 0.44938353 0.60413188][0.36426604 0.41042238 0.44498622 0.49465615 0.57331461 0.6625104 0.71164238 0.69056 0.6082415 0.49392128 0.38458064 0.33562669 0.38497266 0.52242118 0.692052][0.37828195 0.435263 0.48649338 0.55972475 0.66114128 0.76598012 0.81950617 0.78811288 0.68220115 0.53851038 0.40668827 0.34953108 0.40166327 0.543107 0.71116871][0.37732688 0.44113228 0.50678039 0.60022914 0.71619749 0.8240872 0.86984992 0.82163072 0.69313407 0.5265938 0.37932184 0.31440791 0.36000153 0.48825973 0.63558912][0.36163333 0.42262304 0.4914428 0.58960611 0.70104289 0.79358047 0.8199026 0.75487775 0.61516726 0.44420204 0.29854617 0.23297772 0.26727802 0.37098393 0.4864147][0.3309477 0.37679839 0.43207613 0.51350957 0.59935755 0.66137785 0.66402549 0.59103996 0.45927498 0.3075211 0.18343545 0.12770851 0.15173313 0.22756784 0.30923483][0.28956172 0.31468254 0.34623781 0.396827 0.44538563 0.47173464 0.45337731 0.38215783 0.27263913 0.15538631 0.065619022 0.028412646 0.047686916 0.10082556 0.15492734][0.22454913 0.23334339 0.24406458 0.26581147 0.28246853 0.28205234 0.25259483 0.19176105 0.11092588 0.031700332 -0.022853959 -0.040208016 -0.021904249 0.014822919 0.048555907][0.14763187 0.1440264 0.1390582 0.13958941 0.13593751 0.12257852 0.093900278 0.050109338 -0.001187275 -0.046485242 -0.07216081 -0.073988162 -0.056463338 -0.032127656 -0.01337629][0.068845667 0.055377766 0.040462106 0.029135088 0.017146967 0.0027359624 -0.016835889 -0.041329376 -0.066697024 -0.085476764 -0.090473942 -0.081926793 -0.065201558 -0.048738681 -0.038809195][0.0092825377 -0.0077987188 -0.024398768 -0.03771073 -0.048750788 -0.057835847 -0.0667394 -0.075818434 -0.083281465 -0.085515641 -0.079057612 -0.065535285 -0.049732532 -0.03715124 -0.030817136]]...]
INFO - root - 2017-12-11 07:24:32.017749: step 32410, loss = 0.69, batch loss = 0.63 (14.5 examples/sec; 0.553 sec/batch; 46h:07m:43s remains)
INFO - root - 2017-12-11 07:24:37.541587: step 32420, loss = 0.70, batch loss = 0.65 (14.4 examples/sec; 0.554 sec/batch; 46h:09m:45s remains)
INFO - root - 2017-12-11 07:24:43.099521: step 32430, loss = 0.71, batch loss = 0.65 (14.2 examples/sec; 0.564 sec/batch; 47h:01m:43s remains)
INFO - root - 2017-12-11 07:24:48.603708: step 32440, loss = 0.69, batch loss = 0.63 (14.8 examples/sec; 0.541 sec/batch; 45h:06m:14s remains)
INFO - root - 2017-12-11 07:24:54.089700: step 32450, loss = 0.68, batch loss = 0.62 (14.2 examples/sec; 0.563 sec/batch; 46h:53m:38s remains)
INFO - root - 2017-12-11 07:24:59.495907: step 32460, loss = 0.70, batch loss = 0.64 (14.8 examples/sec; 0.540 sec/batch; 45h:02m:14s remains)
INFO - root - 2017-12-11 07:25:05.124518: step 32470, loss = 0.70, batch loss = 0.65 (14.2 examples/sec; 0.564 sec/batch; 46h:58m:24s remains)
INFO - root - 2017-12-11 07:25:10.616192: step 32480, loss = 0.70, batch loss = 0.64 (14.8 examples/sec; 0.541 sec/batch; 45h:04m:08s remains)
INFO - root - 2017-12-11 07:25:16.111606: step 32490, loss = 0.70, batch loss = 0.64 (14.9 examples/sec; 0.536 sec/batch; 44h:38m:52s remains)
INFO - root - 2017-12-11 07:25:21.572452: step 32500, loss = 0.69, batch loss = 0.63 (14.5 examples/sec; 0.553 sec/batch; 46h:06m:20s remains)
2017-12-11 07:25:22.129531: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.047694445 0.16555104 0.31863528 0.46929923 0.57873809 0.62059206 0.60302836 0.54098296 0.46106082 0.40257272 0.38629836 0.40010968 0.40786824 0.39579177 0.3769922][0.028248666 0.13089226 0.26429951 0.3937909 0.48769021 0.52540946 0.51527238 0.46852711 0.40771461 0.36732796 0.3629472 0.3804982 0.38800552 0.37786567 0.36300173][0.012049866 0.098810904 0.21108451 0.31890056 0.39951211 0.43855551 0.4408403 0.41187158 0.37142676 0.34818941 0.35178712 0.36735696 0.37044683 0.36041805 0.34662002][0.0063064732 0.085784614 0.18752985 0.28484085 0.36125183 0.40676296 0.4211531 0.40589425 0.38136354 0.37197629 0.38130575 0.39341962 0.39048237 0.37742129 0.35848954][0.015254861 0.10180771 0.21242519 0.31910768 0.40469462 0.46096447 0.48400027 0.47438478 0.45609641 0.45292294 0.46433291 0.4712387 0.45780516 0.43325162 0.40001264][0.038154084 0.14628735 0.28572965 0.42191038 0.52964568 0.60065776 0.629345 0.6174888 0.59346342 0.58417773 0.58849913 0.58270621 0.54945427 0.50168544 0.44608632][0.06623923 0.20023194 0.37449837 0.54432082 0.67418319 0.75707656 0.78845656 0.77136433 0.7361514 0.71379387 0.70479506 0.68112797 0.62327737 0.54872411 0.472257][0.086911216 0.23744196 0.43483913 0.62499249 0.76421756 0.848508 0.87777251 0.85582685 0.81043935 0.77384317 0.74959904 0.70863676 0.63131881 0.53946513 0.4556233][0.09453585 0.24558674 0.44551054 0.63507646 0.7672177 0.84109026 0.86197221 0.83458263 0.78187227 0.73423988 0.69837159 0.64780962 0.56529981 0.47457215 0.40203878][0.095591724 0.23611479 0.42245203 0.59503907 0.70785648 0.76259059 0.769415 0.73502564 0.67814004 0.62486118 0.583658 0.53184891 0.45635909 0.38143703 0.33418849][0.092666335 0.21578705 0.37670028 0.52074522 0.6069445 0.63886654 0.63023162 0.59081954 0.53677654 0.48812446 0.45184258 0.40861 0.3504841 0.30113655 0.28577572][0.085918054 0.18850572 0.31869221 0.43123323 0.49223351 0.50586581 0.48719007 0.44967175 0.40672907 0.37188241 0.34845516 0.32103989 0.28602222 0.26513448 0.27980766][0.085352346 0.17186552 0.27661604 0.36572143 0.41130421 0.41660833 0.39683798 0.36893004 0.34107402 0.32083833 0.30840486 0.294431 0.27760121 0.27650777 0.30882722][0.095028438 0.17293236 0.26231486 0.34028897 0.38202542 0.38914925 0.37631685 0.36294541 0.3497985 0.3396315 0.3318229 0.32332191 0.31295514 0.31581247 0.34801182][0.10266583 0.17568247 0.25707686 0.33330989 0.38042441 0.39766976 0.39778107 0.39925915 0.39620602 0.38903362 0.3791213 0.3681244 0.35380849 0.34902591 0.37014797]]...]
INFO - root - 2017-12-11 07:25:27.360715: step 32510, loss = 0.72, batch loss = 0.66 (14.8 examples/sec; 0.539 sec/batch; 44h:54m:03s remains)
INFO - root - 2017-12-11 07:25:32.897901: step 32520, loss = 0.70, batch loss = 0.64 (14.6 examples/sec; 0.549 sec/batch; 45h:44m:11s remains)
INFO - root - 2017-12-11 07:25:38.451916: step 32530, loss = 0.69, batch loss = 0.64 (14.4 examples/sec; 0.555 sec/batch; 46h:13m:22s remains)
INFO - root - 2017-12-11 07:25:43.949304: step 32540, loss = 0.68, batch loss = 0.62 (14.9 examples/sec; 0.537 sec/batch; 44h:44m:24s remains)
INFO - root - 2017-12-11 07:25:49.476153: step 32550, loss = 0.69, batch loss = 0.63 (14.1 examples/sec; 0.568 sec/batch; 47h:19m:13s remains)
INFO - root - 2017-12-11 07:25:54.991264: step 32560, loss = 0.69, batch loss = 0.63 (14.6 examples/sec; 0.547 sec/batch; 45h:33m:41s remains)
INFO - root - 2017-12-11 07:26:00.512364: step 32570, loss = 0.70, batch loss = 0.64 (14.7 examples/sec; 0.543 sec/batch; 45h:16m:06s remains)
INFO - root - 2017-12-11 07:26:05.974798: step 32580, loss = 0.71, batch loss = 0.65 (14.1 examples/sec; 0.566 sec/batch; 47h:07m:30s remains)
INFO - root - 2017-12-11 07:26:11.555627: step 32590, loss = 0.70, batch loss = 0.64 (14.4 examples/sec; 0.554 sec/batch; 46h:09m:38s remains)
INFO - root - 2017-12-11 07:26:16.750654: step 32600, loss = 0.68, batch loss = 0.62 (16.0 examples/sec; 0.500 sec/batch; 41h:40m:12s remains)
2017-12-11 07:26:17.430957: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.13396366 0.10155985 0.094230257 0.12022702 0.17608595 0.24678808 0.30443093 0.31881663 0.27461386 0.18242201 0.067195177 -0.041404229 -0.11785617 -0.15358603 -0.15581658][0.21669735 0.17632133 0.1548173 0.17084552 0.22291739 0.29705244 0.36509714 0.39079097 0.35096973 0.25335759 0.12412994 -0.0040136948 -0.09898217 -0.1480245 -0.15811741][0.31659618 0.27715737 0.24673918 0.25549975 0.30373529 0.37881434 0.45379055 0.48725066 0.44934428 0.34633985 0.20524916 0.05792978 -0.058172945 -0.12462705 -0.14733754][0.41879222 0.39041242 0.36494637 0.38093635 0.43788564 0.52197009 0.60473651 0.639499 0.59198844 0.472943 0.312894 0.14176631 0.00029532626 -0.087926708 -0.12700255][0.51131409 0.5009349 0.49265462 0.52856034 0.605703 0.70658845 0.797342 0.82583165 0.75667191 0.60994881 0.42327109 0.22571824 0.059234668 -0.049444508 -0.10367925][0.58758265 0.59576225 0.60694575 0.66222948 0.75761747 0.87173235 0.96403873 0.9789831 0.88247818 0.70573449 0.49438846 0.27738035 0.095216 -0.02513469 -0.087684013][0.64949858 0.66986006 0.68946719 0.74682 0.8409493 0.95083076 1.0313506 1.0277569 0.90914041 0.71370178 0.4919301 0.27213755 0.091550834 -0.02606383 -0.0861594][0.70490408 0.7256847 0.73220539 0.76201022 0.823365 0.9021762 0.95427996 0.93047208 0.80367863 0.6113559 0.40198359 0.20182133 0.04304526 -0.055532612 -0.10144133][0.74614924 0.75145453 0.72202724 0.69724917 0.6995008 0.72631007 0.74103642 0.70316392 0.58808863 0.42400482 0.25034213 0.089568838 -0.032337815 -0.10185561 -0.12720576][0.74591887 0.72232288 0.64555532 0.55871189 0.49751616 0.47244266 0.45743203 0.41964558 0.3351582 0.21786055 0.094373628 -0.018230379 -0.09994138 -0.14104041 -0.1486894][0.67561722 0.61608076 0.49791956 0.36786383 0.2680887 0.21740431 0.19749793 0.17828503 0.1348884 0.067763932 -0.0079817129 -0.079544678 -0.13095529 -0.154003 -0.15336317][0.54224843 0.44673926 0.30422968 0.16449286 0.066263489 0.027212102 0.030131852 0.043269396 0.040829614 0.014116105 -0.029580668 -0.078481905 -0.11728539 -0.13684286 -0.13887405][0.38698319 0.26252553 0.11433235 -0.0055342317 -0.070260167 -0.071658522 -0.0318274 0.015087205 0.044458568 0.043341871 0.014304377 -0.0296086 -0.07139226 -0.099097379 -0.11155353][0.25097424 0.10981438 -0.02867493 -0.11397193 -0.13472047 -0.09487775 -0.023032526 0.0465451 0.093358956 0.10438193 0.079056412 0.031292662 -0.019676903 -0.058783542 -0.082686245][0.15361848 0.010470885 -0.10964904 -0.16158207 -0.14745498 -0.079973206 0.0077235834 0.083811969 0.13350037 0.14644291 0.12020939 0.068164274 0.01080951 -0.03485566 -0.064645812]]...]
INFO - root - 2017-12-11 07:26:22.857032: step 32610, loss = 0.69, batch loss = 0.63 (15.5 examples/sec; 0.516 sec/batch; 42h:59m:52s remains)
INFO - root - 2017-12-11 07:26:28.441805: step 32620, loss = 0.70, batch loss = 0.64 (14.3 examples/sec; 0.560 sec/batch; 46h:41m:05s remains)
INFO - root - 2017-12-11 07:26:33.922096: step 32630, loss = 0.70, batch loss = 0.64 (14.9 examples/sec; 0.536 sec/batch; 44h:38m:56s remains)
INFO - root - 2017-12-11 07:26:39.433413: step 32640, loss = 0.71, batch loss = 0.65 (14.1 examples/sec; 0.566 sec/batch; 47h:09m:16s remains)
INFO - root - 2017-12-11 07:26:44.936918: step 32650, loss = 0.69, batch loss = 0.63 (14.5 examples/sec; 0.553 sec/batch; 46h:05m:16s remains)
INFO - root - 2017-12-11 07:26:50.519067: step 32660, loss = 0.68, batch loss = 0.63 (14.7 examples/sec; 0.544 sec/batch; 45h:20m:20s remains)
INFO - root - 2017-12-11 07:26:55.983237: step 32670, loss = 0.69, batch loss = 0.63 (15.1 examples/sec; 0.531 sec/batch; 44h:15m:05s remains)
INFO - root - 2017-12-11 07:27:01.386455: step 32680, loss = 0.70, batch loss = 0.64 (14.5 examples/sec; 0.550 sec/batch; 45h:49m:30s remains)
INFO - root - 2017-12-11 07:27:06.904744: step 32690, loss = 0.71, batch loss = 0.65 (14.4 examples/sec; 0.556 sec/batch; 46h:19m:34s remains)
INFO - root - 2017-12-11 07:27:12.216718: step 32700, loss = 0.71, batch loss = 0.65 (13.3 examples/sec; 0.601 sec/batch; 50h:02m:13s remains)
2017-12-11 07:27:12.745053: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.11702176 0.12561607 0.13458134 0.1428192 0.14870431 0.15611152 0.17037608 0.18519147 0.19459556 0.20254984 0.21511665 0.23106951 0.24985443 0.27097753 0.28772458][0.14070326 0.15434724 0.17246267 0.19549155 0.22061147 0.24924067 0.28468338 0.317839 0.34170789 0.36031148 0.38068619 0.40304488 0.42944863 0.45813423 0.47488984][0.15030259 0.16604123 0.19143294 0.22874409 0.27423555 0.32711384 0.388358 0.44587845 0.4895106 0.52057093 0.54619169 0.56889135 0.59484386 0.62232339 0.63062334][0.14055316 0.15907691 0.19223192 0.24309029 0.30787802 0.38465646 0.47129479 0.55215019 0.6116991 0.64726871 0.66838646 0.6822114 0.69953865 0.71807569 0.71336985][0.10743302 0.13042049 0.17290238 0.23791958 0.32267702 0.42435056 0.53531057 0.63464528 0.69962335 0.72509879 0.72711128 0.719421 0.71599346 0.714173 0.69055372][0.070558779 0.097430259 0.14729981 0.22448066 0.32799423 0.45338246 0.58499575 0.69451237 0.75175875 0.75194192 0.72015971 0.67835462 0.64237493 0.61117095 0.56750327][0.045636781 0.074103184 0.12760021 0.21305479 0.33053106 0.47284132 0.6154092 0.72202837 0.757709 0.72142237 0.64730364 0.56643951 0.49737287 0.44240797 0.39062142][0.036563996 0.064249925 0.11772007 0.20683144 0.33115011 0.47949514 0.61992604 0.71110356 0.716894 0.64127755 0.52581513 0.41042566 0.31814224 0.25468355 0.21212149][0.03483687 0.061084561 0.11333423 0.20314012 0.32764786 0.47074336 0.59657764 0.66293925 0.63772881 0.53007716 0.38552913 0.24975969 0.15003495 0.093301415 0.070691578][0.033285569 0.057445958 0.10739356 0.19432306 0.31248653 0.4425143 0.548845 0.59186161 0.54476869 0.42007235 0.263675 0.12257923 0.026585661 -0.018234178 -0.022571297][0.033902131 0.054880671 0.099555321 0.1771663 0.2805334 0.39019978 0.47449696 0.49941507 0.4429425 0.31879786 0.17001167 0.040414218 -0.04193747 -0.074506655 -0.068491213][0.033485826 0.05053284 0.087529041 0.15080906 0.23350474 0.31808913 0.37913057 0.39137906 0.33849588 0.23326004 0.1116007 0.00893124 -0.052778948 -0.075438917 -0.068902574][0.032674603 0.04512224 0.072541431 0.11908682 0.17972617 0.24034631 0.282705 0.29203516 0.25802588 0.19002667 0.11181592 0.045309596 0.0032257463 -0.019634489 -0.027444344][0.032059375 0.040566172 0.05860763 0.089360476 0.13009687 0.17135897 0.20225789 0.21849215 0.21512483 0.19635847 0.17040195 0.14270578 0.11436562 0.0791821 0.042856317][0.036295939 0.043175515 0.055023361 0.074229784 0.10009471 0.1282049 0.1548894 0.18503633 0.21773748 0.25124565 0.27701211 0.28361166 0.26000753 0.20127593 0.12595403]]...]
INFO - root - 2017-12-11 07:27:18.412812: step 32710, loss = 0.69, batch loss = 0.63 (13.6 examples/sec; 0.588 sec/batch; 48h:56m:28s remains)
INFO - root - 2017-12-11 07:27:23.911227: step 32720, loss = 0.69, batch loss = 0.63 (14.8 examples/sec; 0.542 sec/batch; 45h:07m:54s remains)
INFO - root - 2017-12-11 07:27:29.391130: step 32730, loss = 0.70, batch loss = 0.64 (14.5 examples/sec; 0.553 sec/batch; 46h:03m:19s remains)
INFO - root - 2017-12-11 07:27:34.911030: step 32740, loss = 0.70, batch loss = 0.64 (14.1 examples/sec; 0.569 sec/batch; 47h:21m:28s remains)
INFO - root - 2017-12-11 07:27:40.460443: step 32750, loss = 0.70, batch loss = 0.64 (14.7 examples/sec; 0.543 sec/batch; 45h:14m:32s remains)
INFO - root - 2017-12-11 07:27:45.918759: step 32760, loss = 0.69, batch loss = 0.63 (14.8 examples/sec; 0.539 sec/batch; 44h:54m:30s remains)
INFO - root - 2017-12-11 07:27:51.483790: step 32770, loss = 0.70, batch loss = 0.64 (14.6 examples/sec; 0.548 sec/batch; 45h:38m:13s remains)
INFO - root - 2017-12-11 07:27:57.013198: step 32780, loss = 0.69, batch loss = 0.64 (14.2 examples/sec; 0.563 sec/batch; 46h:52m:52s remains)
INFO - root - 2017-12-11 07:28:02.632594: step 32790, loss = 0.69, batch loss = 0.63 (14.1 examples/sec; 0.569 sec/batch; 47h:21m:50s remains)
INFO - root - 2017-12-11 07:28:07.804856: step 32800, loss = 0.70, batch loss = 0.64 (14.3 examples/sec; 0.559 sec/batch; 46h:32m:38s remains)
2017-12-11 07:28:08.414828: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.020932332 0.0073033948 0.01000298 0.027136918 0.051170666 0.076186031 0.10523488 0.13537624 0.16253971 0.18355396 0.19719058 0.19883585 0.18632627 0.16729726 0.15576477][0.056841742 0.048946667 0.055218164 0.07481844 0.097612418 0.11591557 0.13571189 0.15649673 0.17588998 0.19244143 0.20529252 0.20868765 0.19481201 0.17178361 0.15962841][0.09256915 0.093540363 0.10506785 0.12617761 0.14551561 0.15454127 0.1618163 0.17002921 0.17980145 0.19216862 0.20559682 0.21292655 0.19981427 0.17469555 0.16182889][0.11695813 0.1267861 0.14325947 0.16602698 0.18313994 0.18568201 0.18354626 0.18231003 0.18546155 0.1954426 0.2093405 0.2192959 0.20648253 0.17844646 0.16168368][0.128165 0.14244792 0.16381195 0.1930012 0.21569665 0.22102396 0.21818289 0.21370231 0.21250312 0.21807075 0.22768116 0.23508382 0.21955384 0.18737093 0.16463487][0.12740177 0.14521466 0.17473911 0.21701472 0.25282955 0.2678158 0.26939356 0.26356122 0.25584844 0.25163987 0.25104868 0.25157195 0.23262584 0.19825974 0.17231271][0.10948865 0.13186355 0.17305019 0.2308093 0.28099617 0.30713162 0.3154301 0.31018144 0.29575014 0.27992609 0.26731604 0.26049528 0.24066883 0.20899904 0.18566822][0.078792512 0.10365165 0.1536338 0.22095926 0.27835494 0.3104465 0.32434583 0.32234341 0.30572158 0.2819427 0.26046109 0.24932222 0.23237473 0.20771827 0.19074106][0.046470758 0.070810005 0.1218073 0.18792836 0.24215648 0.27314091 0.29019192 0.29357326 0.27997467 0.25437483 0.2295384 0.21772215 0.20546052 0.18826063 0.17733414][0.014177358 0.034012832 0.0786543 0.13517553 0.18035612 0.20742016 0.22613312 0.23390046 0.22357449 0.1981955 0.17273688 0.16169797 0.15410747 0.14412482 0.13949418][-0.019233437 -0.0059624696 0.028547684 0.071998984 0.1070017 0.12973136 0.14791505 0.15660278 0.14740333 0.12307642 0.098692514 0.088544391 0.084224626 0.080007963 0.081129558][-0.045674071 -0.038902186 -0.014385998 0.01669048 0.042412356 0.060229387 0.074959747 0.081057616 0.0716984 0.049819224 0.02798049 0.018257795 0.015314873 0.01485341 0.019956488][-0.058558349 -0.056508392 -0.039709408 -0.01872135 -0.0017494704 0.0089201918 0.016392605 0.01711585 0.0077361763 -0.0093496041 -0.025752874 -0.033184707 -0.034720182 -0.032809276 -0.025783829][-0.061366346 -0.061510283 -0.050536625 -0.037926041 -0.028979605 -0.025561994 -0.025291903 -0.029296895 -0.037918847 -0.049186166 -0.058913063 -0.062927313 -0.063192815 -0.060686838 -0.054124895][-0.059175834 -0.060619626 -0.054729227 -0.049221598 -0.04642548 -0.047275737 -0.050259035 -0.055537697 -0.062284973 -0.068924166 -0.073878832 -0.075667039 -0.075584769 -0.0736723 -0.068509534]]...]
INFO - root - 2017-12-11 07:28:13.932086: step 32810, loss = 0.70, batch loss = 0.64 (14.7 examples/sec; 0.543 sec/batch; 45h:13m:57s remains)
INFO - root - 2017-12-11 07:28:19.401711: step 32820, loss = 0.68, batch loss = 0.63 (14.6 examples/sec; 0.547 sec/batch; 45h:32m:43s remains)
INFO - root - 2017-12-11 07:28:24.982928: step 32830, loss = 0.69, batch loss = 0.63 (14.1 examples/sec; 0.568 sec/batch; 47h:18m:59s remains)
INFO - root - 2017-12-11 07:28:30.449958: step 32840, loss = 0.71, batch loss = 0.65 (14.8 examples/sec; 0.540 sec/batch; 44h:58m:35s remains)
INFO - root - 2017-12-11 07:28:35.973277: step 32850, loss = 0.70, batch loss = 0.64 (14.5 examples/sec; 0.552 sec/batch; 45h:57m:06s remains)
INFO - root - 2017-12-11 07:28:41.481516: step 32860, loss = 0.69, batch loss = 0.63 (15.0 examples/sec; 0.534 sec/batch; 44h:26m:46s remains)
INFO - root - 2017-12-11 07:28:47.035602: step 32870, loss = 0.70, batch loss = 0.64 (14.4 examples/sec; 0.557 sec/batch; 46h:19m:39s remains)
INFO - root - 2017-12-11 07:28:52.614365: step 32880, loss = 0.71, batch loss = 0.65 (14.3 examples/sec; 0.558 sec/batch; 46h:28m:30s remains)
INFO - root - 2017-12-11 07:28:57.797167: step 32890, loss = 0.71, batch loss = 0.65 (30.0 examples/sec; 0.267 sec/batch; 22h:13m:44s remains)
INFO - root - 2017-12-11 07:29:03.380014: step 32900, loss = 0.71, batch loss = 0.65 (14.8 examples/sec; 0.541 sec/batch; 45h:00m:37s remains)
2017-12-11 07:29:03.976963: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.12638482 0.20767298 0.27769959 0.30949524 0.30531192 0.271774 0.2268315 0.17653474 0.13488287 0.11845438 0.12845366 0.14907981 0.16332391 0.17234758 0.18135816][0.16111079 0.24579298 0.31519347 0.34682271 0.34528127 0.31386915 0.26927757 0.21932846 0.18218702 0.17236128 0.18584861 0.2051173 0.21758887 0.22536196 0.2319167][0.17241424 0.25180605 0.31519869 0.34675807 0.351818 0.3308059 0.29553604 0.25572851 0.22961186 0.22738852 0.24150753 0.2564041 0.26775438 0.27598256 0.282205][0.16906761 0.24203119 0.30174136 0.33723873 0.35266614 0.34678325 0.3248325 0.29877573 0.28402224 0.28568244 0.29701972 0.30666378 0.31785482 0.32841143 0.33770949][0.16688031 0.23472108 0.29319167 0.33472908 0.36181226 0.37318507 0.36720541 0.35807526 0.35530281 0.35865876 0.3659738 0.37072858 0.38076949 0.38964185 0.39775479][0.17355329 0.23660219 0.29312122 0.33850032 0.37466791 0.40201247 0.4139308 0.42290962 0.4298102 0.43041047 0.43096426 0.43042263 0.43555844 0.43516678 0.43358684][0.19012113 0.24597684 0.29739308 0.34242237 0.38363388 0.42411011 0.45456335 0.48001486 0.49128535 0.48292178 0.47214028 0.46352574 0.45800838 0.44088107 0.42218733][0.2134271 0.2602677 0.3042447 0.34537306 0.38765052 0.43600783 0.48024276 0.51565212 0.52520216 0.50420821 0.47870353 0.45799619 0.43665653 0.39894372 0.35977539][0.22867621 0.26586503 0.30092317 0.33570912 0.37461263 0.42319581 0.47275448 0.51080668 0.51742911 0.48721656 0.44936508 0.41620314 0.37825412 0.32215583 0.26579687][0.22043891 0.24580117 0.27079022 0.29861176 0.33171943 0.37466249 0.42212152 0.45713413 0.46079037 0.42663351 0.38068652 0.33633465 0.28460035 0.21780762 0.15468101][0.1847856 0.1982801 0.21418862 0.23662038 0.2636947 0.29729578 0.33545256 0.360326 0.35747752 0.32118893 0.27160928 0.22113723 0.16338195 0.097807832 0.041192647][0.14236173 0.14660026 0.15507784 0.17275137 0.19285278 0.21418811 0.23731652 0.2469683 0.23467499 0.19762725 0.14987467 0.10062434 0.046534222 -0.0074939234 -0.048397753][0.11346299 0.11327251 0.11700661 0.12981267 0.14134766 0.1485351 0.15467316 0.14867403 0.12716351 0.090725042 0.048565183 0.006381161 -0.037121851 -0.074515961 -0.096922442][0.10510468 0.10439895 0.10455866 0.11025643 0.11010187 0.1018948 0.092520118 0.074973591 0.048289914 0.015384098 -0.017663097 -0.048190936 -0.077553861 -0.098516621 -0.104988][0.10907213 0.1091437 0.10720865 0.10623761 0.095187813 0.074230321 0.054080579 0.031052519 0.0036524697 -0.024730783 -0.049707476 -0.07024008 -0.087983251 -0.09670075 -0.091118544]]...]
INFO - root - 2017-12-11 07:29:09.543924: step 32910, loss = 0.70, batch loss = 0.64 (14.3 examples/sec; 0.561 sec/batch; 46h:41m:04s remains)
INFO - root - 2017-12-11 07:29:15.019497: step 32920, loss = 0.69, batch loss = 0.63 (14.8 examples/sec; 0.541 sec/batch; 45h:01m:06s remains)
INFO - root - 2017-12-11 07:29:20.540770: step 32930, loss = 0.70, batch loss = 0.64 (14.6 examples/sec; 0.550 sec/batch; 45h:44m:44s remains)
INFO - root - 2017-12-11 07:29:26.127044: step 32940, loss = 0.70, batch loss = 0.64 (14.8 examples/sec; 0.540 sec/batch; 44h:56m:10s remains)
INFO - root - 2017-12-11 07:29:31.675186: step 32950, loss = 0.67, batch loss = 0.62 (14.4 examples/sec; 0.556 sec/batch; 46h:17m:48s remains)
INFO - root - 2017-12-11 07:29:37.070477: step 32960, loss = 0.68, batch loss = 0.62 (15.2 examples/sec; 0.527 sec/batch; 43h:53m:22s remains)
INFO - root - 2017-12-11 07:29:42.640666: step 32970, loss = 0.69, batch loss = 0.64 (14.0 examples/sec; 0.571 sec/batch; 47h:31m:48s remains)
INFO - root - 2017-12-11 07:29:48.208498: step 32980, loss = 0.70, batch loss = 0.64 (14.5 examples/sec; 0.552 sec/batch; 45h:54m:21s remains)
INFO - root - 2017-12-11 07:29:53.307771: step 32990, loss = 0.70, batch loss = 0.64 (14.6 examples/sec; 0.548 sec/batch; 45h:33m:14s remains)
INFO - root - 2017-12-11 07:29:58.833454: step 33000, loss = 0.69, batch loss = 0.63 (14.4 examples/sec; 0.557 sec/batch; 46h:21m:46s remains)
2017-12-11 07:29:59.402798: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.068134524 0.080243059 0.067686222 0.042549532 0.019936791 0.0081613231 0.0036724294 0.0010583439 -0.0023420525 -0.0060098479 -0.0058770785 -0.0014363538 0.0049809404 0.0049297051 -0.0072587868][0.14807369 0.17157722 0.16117471 0.1325618 0.10430686 0.088336356 0.081917204 0.076444894 0.065611206 0.048960917 0.035824403 0.03181354 0.036263566 0.037250627 0.023805298][0.23113666 0.26906392 0.2664896 0.23934861 0.20810442 0.18917175 0.18274485 0.17606789 0.15587446 0.11925814 0.083151907 0.0625126 0.060751665 0.063002378 0.05180265][0.29391915 0.34991032 0.36423412 0.34920141 0.32315534 0.30483451 0.29763702 0.28563565 0.24968721 0.18629274 0.12161416 0.0810602 0.072303943 0.077003658 0.070991166][0.33124134 0.40581769 0.44292727 0.44945511 0.43912873 0.42817241 0.42009798 0.3964299 0.33615494 0.24037209 0.14557017 0.086031742 0.071774215 0.080413 0.081713267][0.34034646 0.42958471 0.48830992 0.52073622 0.5360021 0.54249978 0.53746045 0.49821737 0.406155 0.27419627 0.15050788 0.076434694 0.061005678 0.076088607 0.085940368][0.32300037 0.42146248 0.49792239 0.55598038 0.6016705 0.63267761 0.6346814 0.57927346 0.45303237 0.28628689 0.13937427 0.058312975 0.047205202 0.070681989 0.088745639][0.29106173 0.39702404 0.49050364 0.57411891 0.65000957 0.70475858 0.713003 0.6419847 0.48560339 0.29232118 0.1325331 0.053001255 0.049278155 0.078272618 0.099139042][0.25709432 0.369965 0.48117062 0.59059268 0.69290042 0.76359946 0.77027887 0.68183708 0.50317407 0.2976445 0.13986896 0.071260229 0.075799257 0.10418363 0.1193421][0.22632389 0.34043917 0.46168119 0.58669078 0.70227116 0.77603811 0.77430618 0.67407888 0.49162364 0.29600587 0.15734743 0.10674572 0.11726605 0.13749054 0.13862413][0.1999709 0.30648211 0.42283222 0.54406118 0.65296024 0.71514553 0.70080942 0.59860516 0.43368432 0.27075258 0.16639304 0.13808268 0.15182881 0.15970258 0.14353962][0.17064875 0.26184723 0.35972348 0.45915261 0.54407293 0.58413512 0.55653417 0.46109352 0.32728371 0.20844139 0.14306897 0.1352118 0.14980505 0.14703141 0.11850781][0.12413918 0.1960852 0.26915139 0.33826938 0.39199576 0.40805128 0.37235731 0.29234093 0.19629203 0.12239582 0.091227546 0.096864596 0.10871072 0.098170504 0.064711288][0.057767481 0.10593174 0.15139835 0.18863568 0.21196426 0.2089535 0.17417575 0.11804293 0.06240458 0.0290091 0.023535548 0.035337366 0.042403463 0.028228715 -0.0024821588][-0.0079943314 0.013362103 0.029955203 0.037325829 0.0357818 0.022342067 -0.0028831114 -0.031481117 -0.050904162 -0.053064067 -0.04136483 -0.0267683 -0.022497661 -0.035476021 -0.058791857]]...]
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-SGD-lr0.001-clip50-initfromfixqian30k-fixqian
INFO - root - 2017-12-11 07:30:04.967443: step 33010, loss = 0.71, batch loss = 0.66 (14.6 examples/sec; 0.549 sec/batch; 45h:40m:31s remains)
INFO - root - 2017-12-11 07:30:10.427534: step 33020, loss = 0.70, batch loss = 0.64 (14.7 examples/sec; 0.545 sec/batch; 45h:21m:13s remains)
INFO - root - 2017-12-11 07:30:15.983719: step 33030, loss = 0.68, batch loss = 0.62 (13.9 examples/sec; 0.574 sec/batch; 47h:46m:53s remains)
INFO - root - 2017-12-11 07:30:21.499349: step 33040, loss = 0.70, batch loss = 0.64 (14.9 examples/sec; 0.536 sec/batch; 44h:34m:36s remains)
INFO - root - 2017-12-11 07:30:26.975806: step 33050, loss = 0.71, batch loss = 0.65 (14.7 examples/sec; 0.545 sec/batch; 45h:18m:25s remains)
INFO - root - 2017-12-11 07:30:32.462883: step 33060, loss = 0.70, batch loss = 0.64 (14.4 examples/sec; 0.556 sec/batch; 46h:16m:42s remains)
INFO - root - 2017-12-11 07:30:38.038521: step 33070, loss = 0.69, batch loss = 0.63 (15.0 examples/sec; 0.534 sec/batch; 44h:24m:45s remains)
INFO - root - 2017-12-11 07:30:43.549618: step 33080, loss = 0.68, batch loss = 0.63 (14.5 examples/sec; 0.552 sec/batch; 45h:54m:28s remains)
INFO - root - 2017-12-11 07:30:48.659616: step 33090, loss = 0.71, batch loss = 0.65 (14.9 examples/sec; 0.537 sec/batch; 44h:37m:51s remains)
INFO - root - 2017-12-11 07:30:54.146294: step 33100, loss = 0.70, batch loss = 0.64 (14.9 examples/sec; 0.537 sec/batch; 44h:39m:53s remains)
2017-12-11 07:30:54.685104: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.16636066 0.17030115 0.15757981 0.13218476 0.10394315 0.079109728 0.063998185 0.062563427 0.074864469 0.091626547 0.10299446 0.10760558 0.11200381 0.1170968 0.12203941][0.1697101 0.17789902 0.16356778 0.13370234 0.10175747 0.074868605 0.059126917 0.05887223 0.073856428 0.093177788 0.10498049 0.10868607 0.11231939 0.1163515 0.11995203][0.16625451 0.17726752 0.16161112 0.12924899 0.096642718 0.070910387 0.056937486 0.058348991 0.074348524 0.0935983 0.10325121 0.10382923 0.10446937 0.10606111 0.1073368][0.16618362 0.17861676 0.16220731 0.12914138 0.098055787 0.07560166 0.064795263 0.067751147 0.08290033 0.09936031 0.10443853 0.099958055 0.095922895 0.094075456 0.093254723][0.171484 0.18385418 0.16672501 0.1340958 0.10618522 0.088389255 0.08132039 0.085217007 0.097631559 0.10887433 0.10753155 0.09727744 0.088746242 0.084315158 0.083279088][0.18407606 0.19501811 0.17657796 0.14474769 0.12053588 0.10727532 0.10276986 0.10537778 0.11197687 0.1150177 0.1058284 0.09051346 0.0799795 0.076900706 0.080116518][0.1988879 0.20733882 0.18764114 0.15721348 0.13665311 0.12713599 0.12372524 0.12325411 0.12245806 0.11667378 0.10080046 0.082917549 0.073646829 0.075676 0.086734451][0.20935085 0.21486884 0.19494855 0.16687031 0.14959623 0.1427522 0.13971578 0.13663246 0.13036537 0.11904372 0.10067885 0.083938763 0.078415364 0.086675562 0.10555978][0.21288247 0.21534778 0.1955746 0.16993764 0.15498812 0.14983653 0.14758442 0.14433625 0.13708723 0.1261404 0.11122288 0.099668935 0.099043749 0.11198928 0.13535637][0.20903662 0.2070391 0.18650508 0.16264383 0.14922106 0.14533079 0.14470142 0.14385259 0.14001735 0.13473149 0.12797181 0.1240935 0.12796308 0.14243881 0.16557524][0.1967818 0.18856905 0.16631266 0.14406887 0.13248023 0.13059913 0.1328166 0.13595486 0.13759336 0.13967223 0.14170957 0.14484672 0.15136592 0.16446844 0.18323326][0.17496094 0.16020517 0.13617671 0.11586806 0.10709944 0.10883285 0.11574563 0.12418205 0.13177527 0.14047404 0.14939237 0.15689158 0.1631251 0.17158371 0.18284708][0.14316048 0.1237879 0.099446915 0.081848331 0.076229066 0.081626348 0.093449369 0.10745041 0.12078811 0.13461995 0.14777537 0.1567616 0.16019413 0.16183323 0.16409554][0.10888817 0.0876778 0.06460885 0.049718753 0.046302568 0.053784952 0.068537563 0.086101614 0.10320788 0.11978989 0.13447748 0.14299735 0.14296561 0.13806342 0.13215926][0.081802778 0.061386853 0.040958256 0.028494015 0.02606293 0.033379626 0.047622796 0.064758822 0.081769869 0.09757188 0.1108304 0.11745033 0.11469183 0.10577115 0.094998129]]...]
INFO - root - 2017-12-11 07:31:00.290842: step 33110, loss = 0.70, batch loss = 0.64 (14.6 examples/sec; 0.547 sec/batch; 45h:30m:51s remains)
INFO - root - 2017-12-11 07:31:05.778727: step 33120, loss = 0.71, batch loss = 0.65 (14.2 examples/sec; 0.562 sec/batch; 46h:46m:37s remains)
INFO - root - 2017-12-11 07:31:11.218483: step 33130, loss = 0.71, batch loss = 0.65 (15.0 examples/sec; 0.532 sec/batch; 44h:14m:19s remains)
INFO - root - 2017-12-11 07:31:16.840874: step 33140, loss = 0.69, batch loss = 0.63 (14.0 examples/sec; 0.573 sec/batch; 47h:39m:50s remains)
INFO - root - 2017-12-11 07:31:22.423971: step 33150, loss = 0.67, batch loss = 0.61 (13.7 examples/sec; 0.586 sec/batch; 48h:41m:18s remains)
INFO - root - 2017-12-11 07:31:27.908668: step 33160, loss = 0.70, batch loss = 0.64 (14.7 examples/sec; 0.543 sec/batch; 45h:10m:40s remains)
INFO - root - 2017-12-11 07:31:33.435744: step 33170, loss = 0.69, batch loss = 0.63 (14.3 examples/sec; 0.561 sec/batch; 46h:37m:47s remains)
INFO - root - 2017-12-11 07:31:38.893597: step 33180, loss = 0.70, batch loss = 0.64 (15.5 examples/sec; 0.515 sec/batch; 42h:51m:38s remains)
INFO - root - 2017-12-11 07:31:44.232096: step 33190, loss = 0.70, batch loss = 0.64 (14.6 examples/sec; 0.547 sec/batch; 45h:28m:16s remains)
INFO - root - 2017-12-11 07:31:49.701098: step 33200, loss = 0.70, batch loss = 0.64 (15.0 examples/sec; 0.532 sec/batch; 44h:15m:53s remains)
2017-12-11 07:31:50.323809: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.031637352 0.027420877 0.034229852 0.061378196 0.11459146 0.18814787 0.266005 0.3276459 0.3600601 0.36783013 0.3557618 0.33079505 0.2962431 0.25255492 0.19355516][0.045028225 0.036532678 0.044219065 0.08011315 0.14936054 0.24276763 0.3385419 0.41094574 0.4444811 0.44626257 0.42596182 0.39211935 0.34871635 0.29722571 0.23064907][0.083267316 0.062550023 0.061111614 0.095123246 0.16929713 0.27085719 0.37306708 0.44733357 0.4788532 0.47783664 0.45822304 0.42639923 0.38316163 0.32913914 0.25780481][0.14719121 0.10622218 0.084256306 0.10391267 0.17091268 0.26980215 0.37057656 0.44337747 0.47515634 0.47739968 0.4668937 0.44578522 0.40933558 0.35570046 0.28019738][0.22854622 0.16785593 0.12173226 0.12079827 0.17376818 0.26435709 0.36066905 0.43165341 0.4646548 0.4699479 0.46608144 0.4521957 0.41919395 0.36395761 0.28414544][0.30950937 0.23852567 0.17825919 0.16556174 0.21234913 0.30124608 0.39674968 0.46461368 0.49101886 0.48597521 0.47193184 0.44989869 0.41004306 0.34813079 0.26338965][0.40396535 0.33097094 0.26391914 0.24566902 0.29177549 0.38372108 0.47996444 0.54059792 0.54989588 0.5208897 0.48305598 0.44320494 0.39158973 0.3216041 0.23240365][0.50306869 0.42883381 0.35414645 0.32668358 0.36553529 0.45288616 0.54390544 0.59535187 0.59106749 0.54762721 0.49785551 0.451739 0.39625522 0.32140255 0.22594526][0.5696981 0.49186131 0.40928113 0.37037128 0.39479947 0.46742404 0.54622549 0.59056622 0.58672953 0.55157268 0.51294792 0.47754446 0.42726687 0.35079953 0.24812436][0.55601722 0.47952151 0.39855134 0.35584071 0.36794528 0.42387775 0.48912781 0.53027844 0.53861165 0.52692509 0.51176596 0.49282452 0.44923931 0.37250412 0.2657204][0.46003547 0.39579561 0.33037788 0.2971999 0.30649957 0.35121989 0.40518558 0.44300821 0.46212089 0.46990082 0.47263759 0.46503079 0.42722133 0.35460231 0.25280544][0.34297976 0.29420978 0.24691162 0.22505125 0.23394538 0.27003524 0.313594 0.34521705 0.36822203 0.38589966 0.39804581 0.39718068 0.36602184 0.30280119 0.21360902][0.23596729 0.20203181 0.16959645 0.1542684 0.15935338 0.18532 0.2184969 0.24395661 0.26923811 0.29406208 0.3133615 0.31827736 0.29436085 0.24130543 0.1649081][0.14236495 0.123033 0.10382161 0.093089037 0.093643859 0.10948217 0.13257211 0.15218052 0.1781417 0.20715135 0.23032102 0.23810804 0.21936707 0.17475627 0.10976115][0.052005909 0.045342162 0.037288979 0.030935008 0.028810941 0.036617853 0.050724458 0.064467 0.087812223 0.11585758 0.13816816 0.14613321 0.13175988 0.096154332 0.044221722]]...]
INFO - root - 2017-12-11 07:31:55.910567: step 33210, loss = 0.68, batch loss = 0.62 (14.8 examples/sec; 0.540 sec/batch; 44h:52m:52s remains)
INFO - root - 2017-12-11 07:32:01.415392: step 33220, loss = 0.69, batch loss = 0.63 (14.6 examples/sec; 0.549 sec/batch; 45h:39m:51s remains)
INFO - root - 2017-12-11 07:32:06.946047: step 33230, loss = 0.71, batch loss = 0.65 (14.5 examples/sec; 0.551 sec/batch; 45h:50m:31s remains)
INFO - root - 2017-12-11 07:32:12.548046: step 33240, loss = 0.71, batch loss = 0.65 (13.2 examples/sec; 0.606 sec/batch; 50h:21m:16s remains)
INFO - root - 2017-12-11 07:32:18.049291: step 33250, loss = 0.70, batch loss = 0.64 (14.4 examples/sec; 0.554 sec/batch; 46h:03m:53s remains)
INFO - root - 2017-12-11 07:32:23.456125: step 33260, loss = 0.69, batch loss = 0.64 (14.9 examples/sec; 0.535 sec/batch; 44h:28m:54s remains)
INFO - root - 2017-12-11 07:32:28.986369: step 33270, loss = 0.71, batch loss = 0.65 (14.5 examples/sec; 0.550 sec/batch; 45h:43m:57s remains)
INFO - root - 2017-12-11 07:32:34.179560: step 33280, loss = 0.71, batch loss = 0.65 (14.7 examples/sec; 0.544 sec/batch; 45h:15m:03s remains)
INFO - root - 2017-12-11 07:32:39.818424: step 33290, loss = 0.71, batch loss = 0.65 (13.7 examples/sec; 0.583 sec/batch; 48h:25m:37s remains)
INFO - root - 2017-12-11 07:32:45.321591: step 33300, loss = 0.70, batch loss = 0.64 (14.7 examples/sec; 0.546 sec/batch; 45h:22m:41s remains)
2017-12-11 07:32:45.911540: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.1485531 0.13563091 0.11264446 0.10173043 0.11417361 0.1459882 0.18600418 0.22355872 0.24100374 0.21507734 0.14640272 0.065011673 0.00063622097 -0.034970343 -0.05045047][0.18021795 0.16434918 0.13500604 0.12113886 0.13696632 0.17796466 0.2309867 0.28195408 0.30779612 0.27965534 0.19763759 0.097209617 0.015909394 -0.029728815 -0.050102629][0.19026195 0.17299564 0.14098908 0.12659784 0.14558527 0.19352384 0.25592747 0.31516102 0.34484744 0.31505781 0.22670692 0.11656649 0.025600828 -0.026662843 -0.049867194][0.19111921 0.17535187 0.1444433 0.1304592 0.15045427 0.20128892 0.26831898 0.33061489 0.36075765 0.33045441 0.24116792 0.12747173 0.030734694 -0.02677913 -0.052219111][0.18633787 0.17364816 0.14751548 0.13703309 0.15888432 0.21115275 0.27927229 0.34035683 0.36772421 0.33629891 0.24817573 0.13309129 0.031576823 -0.030834101 -0.05762529][0.1690966 0.16198063 0.14473686 0.14290883 0.17057303 0.22435069 0.28887752 0.34184456 0.36006925 0.32481167 0.23914394 0.1258862 0.022766866 -0.0422084 -0.068282336][0.16254388 0.1576169 0.14791381 0.15755674 0.19626747 0.25457171 0.31235093 0.34899306 0.3481454 0.30157924 0.21653338 0.10937522 0.011127881 -0.052073915 -0.075976647][0.17910376 0.17236191 0.16705234 0.18734403 0.23850121 0.3028757 0.35272488 0.36896157 0.34338972 0.28051677 0.19344702 0.09352684 0.0037545054 -0.054963291 -0.077028319][0.20112589 0.19505952 0.19397819 0.22152367 0.27929854 0.34478033 0.38518241 0.38387057 0.33968747 0.26646951 0.17990781 0.0867668 0.0036262628 -0.052253146 -0.074314982][0.20433177 0.20250285 0.20749049 0.23851621 0.29443568 0.35342184 0.38368428 0.37308952 0.32399106 0.25189543 0.16998282 0.082231574 0.0030958634 -0.051031269 -0.072685406][0.18237849 0.1887036 0.20113429 0.23185855 0.27832004 0.32354677 0.34193546 0.32718584 0.28293842 0.22117411 0.14951542 0.070038423 -0.0027440798 -0.052572735 -0.071893908][0.15273546 0.1688613 0.19056281 0.2201446 0.25386971 0.28140438 0.28573543 0.26615071 0.22737598 0.177135 0.11771554 0.049321655 -0.013687916 -0.055682704 -0.070400223][0.12482341 0.14985543 0.17979191 0.20811619 0.23103386 0.24460147 0.23841862 0.21386957 0.17640872 0.13222145 0.08128202 0.023070939 -0.029387468 -0.061838064 -0.070317224][0.11137531 0.13879548 0.16983441 0.19360015 0.20670554 0.21027888 0.1981649 0.1712935 0.13499339 0.094886512 0.049786188 -0.0006003342 -0.044533096 -0.068751216 -0.071252756][0.11639481 0.14147563 0.16636036 0.18151237 0.18499738 0.18176365 0.16798177 0.14214395 0.10751907 0.069158271 0.02615422 -0.019750191 -0.057428073 -0.075050756 -0.072425053]]...]
INFO - root - 2017-12-11 07:32:51.427147: step 33310, loss = 0.69, batch loss = 0.63 (14.6 examples/sec; 0.547 sec/batch; 45h:29m:17s remains)
INFO - root - 2017-12-11 07:32:56.871845: step 33320, loss = 0.69, batch loss = 0.63 (14.8 examples/sec; 0.541 sec/batch; 44h:55m:15s remains)
INFO - root - 2017-12-11 07:33:02.330911: step 33330, loss = 0.69, batch loss = 0.64 (14.6 examples/sec; 0.550 sec/batch; 45h:40m:22s remains)
INFO - root - 2017-12-11 07:33:08.001127: step 33340, loss = 0.70, batch loss = 0.64 (13.7 examples/sec; 0.583 sec/batch; 48h:24m:21s remains)
INFO - root - 2017-12-11 07:33:13.406994: step 33350, loss = 0.70, batch loss = 0.64 (14.8 examples/sec; 0.540 sec/batch; 44h:54m:28s remains)
INFO - root - 2017-12-11 07:33:19.024187: step 33360, loss = 0.69, batch loss = 0.63 (14.1 examples/sec; 0.569 sec/batch; 47h:15m:06s remains)
INFO - root - 2017-12-11 07:33:24.582479: step 33370, loss = 0.69, batch loss = 0.63 (14.4 examples/sec; 0.557 sec/batch; 46h:14m:49s remains)
INFO - root - 2017-12-11 07:33:29.841444: step 33380, loss = 0.69, batch loss = 0.63 (14.3 examples/sec; 0.559 sec/batch; 46h:24m:32s remains)
INFO - root - 2017-12-11 07:33:35.296479: step 33390, loss = 0.70, batch loss = 0.64 (14.8 examples/sec; 0.539 sec/batch; 44h:48m:35s remains)
INFO - root - 2017-12-11 07:33:40.813174: step 33400, loss = 0.70, batch loss = 0.64 (14.7 examples/sec; 0.546 sec/batch; 45h:21m:02s remains)
2017-12-11 07:33:41.502562: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.41977376 0.44478431 0.45164284 0.44407922 0.42146835 0.37932879 0.31739783 0.2634837 0.22205448 0.18549393 0.15931158 0.15961289 0.18567243 0.21636221 0.25052315][0.537973 0.55694729 0.54299575 0.50828218 0.45896095 0.39411288 0.31673473 0.25853014 0.22370204 0.19718929 0.17934366 0.18355614 0.21029817 0.24246486 0.28244007][0.5641638 0.571168 0.53825969 0.48569185 0.42558697 0.36006072 0.29251546 0.2497264 0.23298243 0.22112486 0.20935795 0.20860946 0.22314526 0.24534425 0.28128436][0.49885938 0.495521 0.4585115 0.41374448 0.37649849 0.34574559 0.31848231 0.30773133 0.31096664 0.30662 0.28813222 0.26734939 0.25256622 0.24871656 0.26464817][0.37571082 0.37704903 0.36381289 0.36012238 0.38009906 0.41308784 0.44392538 0.46771786 0.47871071 0.46272531 0.41701722 0.35833174 0.29857832 0.25392848 0.23439093][0.24143986 0.27133206 0.31047389 0.37156096 0.4643122 0.566778 0.65206963 0.69932151 0.70118278 0.6554119 0.56669647 0.45800889 0.34449336 0.25091022 0.18851282][0.14244278 0.21847831 0.32287958 0.45036915 0.60422063 0.75502825 0.87055504 0.92186749 0.90256816 0.82088989 0.68750405 0.52979928 0.36630502 0.22832406 0.12912636][0.10130021 0.22370347 0.38209152 0.55305409 0.733418 0.89279056 1.0022122 1.0353811 0.9897812 0.87883443 0.71446878 0.52478039 0.33220351 0.17103887 0.056021839][0.11432564 0.26767555 0.45215172 0.63229114 0.80012053 0.92849582 0.99762112 0.99229056 0.91814655 0.79055 0.61881876 0.42660585 0.23748523 0.084364295 -0.018665466][0.16464248 0.32934296 0.51113373 0.67094654 0.79822785 0.87083733 0.87944305 0.82343435 0.72118676 0.59072769 0.43707171 0.27517715 0.1229043 0.0071287081 -0.061822038][0.23931269 0.40626639 0.57121068 0.69597745 0.76968426 0.77623886 0.71746248 0.60872042 0.48351771 0.36415404 0.25001076 0.14317332 0.049307894 -0.014142884 -0.042518541][0.34196979 0.51226878 0.65824234 0.74363917 0.75971293 0.69911492 0.57703727 0.4249939 0.28949672 0.1951382 0.13389452 0.092071556 0.058632694 0.04122017 0.041370563][0.45200902 0.623701 0.75000113 0.79672158 0.75909412 0.64276141 0.47931921 0.31220648 0.19119339 0.13666439 0.13192175 0.14735146 0.15628588 0.16110553 0.16664311][0.52405715 0.68698436 0.79139513 0.80572951 0.73004448 0.58395743 0.41446796 0.26832005 0.1872173 0.18159446 0.22629176 0.27964947 0.30499366 0.30432433 0.29185286][0.51138246 0.6479739 0.7247439 0.71586668 0.62599969 0.4835912 0.34288189 0.24434666 0.21467625 0.25307128 0.32935786 0.3976585 0.41924617 0.39775348 0.357673]]...]
INFO - root - 2017-12-11 07:33:47.092779: step 33410, loss = 0.69, batch loss = 0.63 (13.9 examples/sec; 0.574 sec/batch; 47h:41m:55s remains)
INFO - root - 2017-12-11 07:33:52.620558: step 33420, loss = 0.68, batch loss = 0.63 (14.4 examples/sec; 0.557 sec/batch; 46h:15m:54s remains)
INFO - root - 2017-12-11 07:33:58.183582: step 33430, loss = 0.68, batch loss = 0.63 (14.6 examples/sec; 0.549 sec/batch; 45h:34m:17s remains)
INFO - root - 2017-12-11 07:34:03.725942: step 33440, loss = 0.71, batch loss = 0.65 (14.6 examples/sec; 0.550 sec/batch; 45h:40m:15s remains)
INFO - root - 2017-12-11 07:34:09.144396: step 33450, loss = 0.71, batch loss = 0.65 (14.9 examples/sec; 0.536 sec/batch; 44h:29m:23s remains)
INFO - root - 2017-12-11 07:34:14.589997: step 33460, loss = 0.69, batch loss = 0.63 (14.5 examples/sec; 0.550 sec/batch; 45h:41m:34s remains)
INFO - root - 2017-12-11 07:34:20.142014: step 33470, loss = 0.71, batch loss = 0.66 (14.3 examples/sec; 0.558 sec/batch; 46h:18m:36s remains)
INFO - root - 2017-12-11 07:34:25.301593: step 33480, loss = 0.68, batch loss = 0.62 (14.3 examples/sec; 0.561 sec/batch; 46h:36m:22s remains)
INFO - root - 2017-12-11 07:34:30.756412: step 33490, loss = 0.69, batch loss = 0.63 (14.8 examples/sec; 0.539 sec/batch; 44h:47m:23s remains)
INFO - root - 2017-12-11 07:34:36.287340: step 33500, loss = 0.70, batch loss = 0.64 (14.4 examples/sec; 0.556 sec/batch; 46h:09m:50s remains)
2017-12-11 07:34:36.870296: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.058878023 -0.034640428 0.0049065365 0.053591076 0.10381273 0.14273803 0.16262029 0.16115731 0.14121027 0.10938992 0.0723749 0.037009846 0.009821808 -0.0046725771 -0.0070434879][-0.053296257 -0.022605773 0.026929192 0.088750266 0.1542342 0.20947318 0.2432528 0.24890447 0.22581451 0.18154913 0.12651686 0.072600968 0.031844117 0.011717934 0.010707105][-0.042786334 -0.0041156122 0.057174996 0.13469504 0.21868552 0.29305652 0.34223813 0.35417888 0.32494533 0.26481918 0.18976653 0.11863215 0.067989029 0.045670494 0.046833761][-0.031599429 0.015105126 0.088754773 0.18388028 0.28856325 0.38261923 0.44496834 0.45882535 0.41849732 0.33933029 0.24567817 0.1631643 0.10998172 0.090196557 0.093803316][-0.020214509 0.034612466 0.12137593 0.2353445 0.3607882 0.47113752 0.53985381 0.54704595 0.48809764 0.38662395 0.27691123 0.19051677 0.14336613 0.13138719 0.13766602][-0.0091035543 0.0554491 0.15687189 0.2893945 0.43185961 0.55020696 0.61424071 0.6046021 0.52208948 0.39988282 0.28025398 0.19770423 0.16164231 0.15760933 0.16319542][-0.00033350373 0.074163668 0.19012369 0.33836421 0.49090865 0.60703015 0.65572315 0.62247366 0.51693314 0.38055128 0.25891408 0.18498391 0.15943302 0.15882425 0.15931548][0.0019492188 0.08308699 0.20896481 0.36587068 0.51888293 0.62270081 0.64892691 0.59139478 0.46930444 0.32860637 0.21341547 0.1508548 0.13307215 0.13185681 0.12635647][-0.0045458758 0.076907657 0.20380214 0.35772243 0.4997085 0.58416152 0.58822143 0.51401246 0.38781196 0.25522244 0.15411016 0.10364641 0.090330414 0.086983308 0.078738935][-0.016145837 0.06060192 0.1798625 0.31944069 0.4408451 0.5033806 0.49128065 0.41195437 0.29462627 0.1801869 0.097324409 0.057691209 0.046133351 0.041164104 0.035268031][-0.027487103 0.040807694 0.14539817 0.26215369 0.35686117 0.39740577 0.37562418 0.30149066 0.20298328 0.11312397 0.051218584 0.022953829 0.01452356 0.012127991 0.014879044][-0.039060686 0.016871233 0.10195009 0.19261341 0.2605724 0.28274181 0.25751206 0.19559483 0.12085558 0.057510905 0.017686659 0.0036639406 0.004278393 0.011891454 0.028787751][-0.050831858 -0.0088228611 0.055732187 0.12181278 0.16758797 0.17758641 0.15417328 0.10723029 0.055872995 0.016886983 -0.0021424466 -0.00017700196 0.014544002 0.03734158 0.070302412][-0.058487467 -0.02802298 0.019326027 0.06586653 0.096046634 0.099993125 0.081108384 0.047614425 0.014956729 -0.0051812613 -0.0077686906 0.0082246633 0.036914248 0.074361339 0.1211057][-0.060452588 -0.036743067 -0.00034858705 0.034041476 0.055590149 0.057738848 0.042949807 0.01835721 -0.0024644567 -0.010624228 -0.002439375 0.023142299 0.06130581 0.10767707 0.16151284]]...]
INFO - root - 2017-12-11 07:34:42.477328: step 33510, loss = 0.69, batch loss = 0.63 (14.4 examples/sec; 0.557 sec/batch; 46h:17m:24s remains)
INFO - root - 2017-12-11 07:34:47.949603: step 33520, loss = 0.69, batch loss = 0.63 (14.8 examples/sec; 0.539 sec/batch; 44h:47m:26s remains)
INFO - root - 2017-12-11 07:34:53.374991: step 33530, loss = 0.70, batch loss = 0.64 (14.6 examples/sec; 0.548 sec/batch; 45h:32m:22s remains)
INFO - root - 2017-12-11 07:34:58.945395: step 33540, loss = 0.70, batch loss = 0.64 (14.5 examples/sec; 0.553 sec/batch; 45h:56m:51s remains)
INFO - root - 2017-12-11 07:35:04.461754: step 33550, loss = 0.71, batch loss = 0.65 (14.4 examples/sec; 0.556 sec/batch; 46h:07m:50s remains)
INFO - root - 2017-12-11 07:35:09.892483: step 33560, loss = 0.70, batch loss = 0.65 (14.3 examples/sec; 0.561 sec/batch; 46h:36m:08s remains)
INFO - root - 2017-12-11 07:35:14.929600: step 33570, loss = 0.71, batch loss = 0.65 (16.6 examples/sec; 0.481 sec/batch; 39h:54m:12s remains)
INFO - root - 2017-12-11 07:35:20.514374: step 33580, loss = 0.69, batch loss = 0.63 (14.7 examples/sec; 0.543 sec/batch; 45h:04m:32s remains)
INFO - root - 2017-12-11 07:35:25.970185: step 33590, loss = 0.71, batch loss = 0.65 (14.7 examples/sec; 0.545 sec/batch; 45h:16m:10s remains)
INFO - root - 2017-12-11 07:35:31.499337: step 33600, loss = 0.69, batch loss = 0.63 (14.1 examples/sec; 0.567 sec/batch; 47h:04m:50s remains)
2017-12-11 07:35:32.057150: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.042972643 -0.033963874 -0.023260029 -0.010329627 0.0072150892 0.026735721 0.04272525 0.049818274 0.044354472 0.027422037 0.0021775931 -0.022962268 -0.041727137 -0.052929584 -0.0580964][-0.021439226 -0.0066216937 0.00944001 0.028012918 0.053850889 0.083458789 0.10804233 0.11948849 0.11302647 0.0910154 0.05774907 0.024431264 -0.0012427159 -0.01880206 -0.030613029][0.01167529 0.031629175 0.051361226 0.074124508 0.10801452 0.14896412 0.18439902 0.20278379 0.19809067 0.17404696 0.13534555 0.0949325 0.061816663 0.036308195 0.015211564][0.049221367 0.07206443 0.0931751 0.11846514 0.15939042 0.21141322 0.25850415 0.28557804 0.28512186 0.26176205 0.22006027 0.17319028 0.13205887 0.097907521 0.067388013][0.088317938 0.11262463 0.13353556 0.15963964 0.20454632 0.26383013 0.31955904 0.35414821 0.35872003 0.33819148 0.2969442 0.24724199 0.20109825 0.1610446 0.12384401][0.12955844 0.15627556 0.17788567 0.20500185 0.2522113 0.31571615 0.37630022 0.41439167 0.42041665 0.40048897 0.35983583 0.3101235 0.26366013 0.22339232 0.18470007][0.167786 0.19723801 0.21911567 0.24598356 0.29324979 0.35816866 0.41937912 0.45512658 0.45677522 0.4331418 0.39213577 0.34548441 0.30463815 0.27192673 0.23919152][0.20207182 0.23372579 0.25422996 0.27852714 0.32334173 0.3870323 0.4453074 0.47379255 0.46612272 0.43514678 0.39331922 0.35228604 0.32216746 0.30327842 0.28300145][0.24481742 0.27943629 0.29794198 0.31797904 0.35813892 0.41772792 0.47017848 0.48914841 0.47062185 0.43062848 0.38604048 0.34872311 0.32744911 0.3208423 0.31352511][0.28900453 0.32668242 0.34368458 0.36026454 0.39620569 0.45116737 0.49775636 0.508784 0.48166227 0.43371388 0.38410953 0.34583992 0.32692677 0.32586727 0.32659847][0.32109681 0.36110246 0.37808064 0.39392653 0.4283562 0.4807286 0.52396131 0.53127289 0.49966371 0.44541061 0.38803712 0.34350264 0.32039031 0.31871668 0.32338837][0.32819787 0.36499661 0.37942374 0.39357612 0.42517903 0.47305253 0.51229751 0.51818323 0.48654437 0.43053475 0.36844197 0.31826338 0.29012844 0.28596216 0.29336283][0.30057225 0.32901862 0.33827198 0.34980661 0.37749302 0.41880912 0.4522709 0.45690247 0.42767856 0.37428072 0.3128556 0.26111031 0.23025841 0.22334501 0.23244001][0.23971272 0.25844535 0.26291919 0.27229458 0.2959848 0.32960647 0.35590646 0.35882449 0.33296984 0.28526035 0.22886881 0.17955054 0.14840351 0.13942453 0.14906852][0.14531283 0.15453383 0.15508054 0.16199033 0.18080805 0.20620741 0.22532073 0.22695057 0.20594649 0.16701834 0.12023995 0.078690037 0.052101817 0.044590935 0.055591039]]...]
INFO - root - 2017-12-11 07:35:37.528757: step 33610, loss = 0.68, batch loss = 0.62 (14.7 examples/sec; 0.545 sec/batch; 45h:13m:15s remains)
INFO - root - 2017-12-11 07:35:43.072399: step 33620, loss = 0.70, batch loss = 0.64 (14.2 examples/sec; 0.562 sec/batch; 46h:39m:07s remains)
INFO - root - 2017-12-11 07:35:48.664017: step 33630, loss = 0.70, batch loss = 0.64 (14.4 examples/sec; 0.555 sec/batch; 46h:02m:20s remains)
INFO - root - 2017-12-11 07:35:54.135073: step 33640, loss = 0.70, batch loss = 0.64 (14.9 examples/sec; 0.535 sec/batch; 44h:26m:30s remains)
INFO - root - 2017-12-11 07:35:59.695090: step 33650, loss = 0.69, batch loss = 0.63 (14.1 examples/sec; 0.568 sec/batch; 47h:09m:00s remains)
INFO - root - 2017-12-11 07:36:05.212586: step 33660, loss = 0.69, batch loss = 0.64 (14.8 examples/sec; 0.541 sec/batch; 44h:52m:29s remains)
INFO - root - 2017-12-11 07:36:10.430466: step 33670, loss = 0.71, batch loss = 0.65 (14.1 examples/sec; 0.566 sec/batch; 47h:00m:58s remains)
INFO - root - 2017-12-11 07:36:15.963624: step 33680, loss = 0.71, batch loss = 0.65 (14.0 examples/sec; 0.573 sec/batch; 47h:34m:15s remains)
INFO - root - 2017-12-11 07:36:21.466143: step 33690, loss = 0.69, batch loss = 0.63 (14.4 examples/sec; 0.555 sec/batch; 46h:04m:09s remains)
INFO - root - 2017-12-11 07:36:27.011477: step 33700, loss = 0.70, batch loss = 0.64 (14.0 examples/sec; 0.571 sec/batch; 47h:25m:55s remains)
2017-12-11 07:36:27.567131: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.0014620858 0.055130053 0.12950213 0.20756929 0.26271355 0.27479598 0.24471748 0.19167426 0.14147751 0.11033212 0.09902361 0.097146489 0.098088264 0.10115543 0.10223243][0.0055599483 0.063048683 0.14262359 0.22512628 0.28248015 0.29521546 0.26719269 0.21983123 0.17774293 0.15445609 0.14827427 0.14760077 0.14725843 0.14890441 0.14833497][0.0057716602 0.063526921 0.14315425 0.22479993 0.2819559 0.29784977 0.27807668 0.2432982 0.21420972 0.20031144 0.19830631 0.19742665 0.19593897 0.19780008 0.19847657][0.0051890491 0.062563464 0.14102989 0.22122608 0.27990791 0.30324966 0.29630128 0.27567482 0.25612339 0.24422954 0.23831502 0.2317764 0.22840315 0.2335465 0.23995255][-0.00016493989 0.05477016 0.13049455 0.20927364 0.27199513 0.30725092 0.3174234 0.31282118 0.30013379 0.28301752 0.26441559 0.24569081 0.23735186 0.24555214 0.25914016][-0.0096012652 0.041167561 0.11304189 0.19095854 0.26009339 0.31068087 0.34074625 0.35267469 0.34441122 0.31687281 0.279938 0.24496515 0.2288367 0.23784652 0.2572408][-0.014858209 0.033402637 0.10362711 0.18274638 0.25944778 0.32463044 0.37079403 0.39229456 0.381406 0.3396318 0.28469422 0.23612125 0.21369939 0.22241355 0.24496405][-0.013587929 0.034897219 0.10635079 0.18825343 0.27129 0.346025 0.40018961 0.42271742 0.40493816 0.35071504 0.28341028 0.22667757 0.19934428 0.20482077 0.22639787][-0.011674469 0.037446778 0.1097522 0.19289418 0.27800173 0.35497603 0.40847555 0.42565909 0.40063071 0.34048817 0.270183 0.21234117 0.18257135 0.18409573 0.20338811][-0.010997444 0.037810091 0.10887574 0.19053952 0.27313656 0.34563851 0.39188421 0.40141773 0.37325856 0.31608614 0.25181878 0.19888508 0.16929172 0.16716923 0.18363683][-0.0077688219 0.04229904 0.11407879 0.19565234 0.27504089 0.33987135 0.37562197 0.37701064 0.34859285 0.29957443 0.24608909 0.20098896 0.17315805 0.16808617 0.18135166][-0.0045587313 0.047922559 0.12287429 0.20689772 0.28467274 0.34212825 0.36774859 0.36183751 0.33351606 0.29260927 0.2500768 0.2135044 0.18883577 0.1824704 0.19344312][-0.0056277085 0.047740068 0.12476045 0.21052775 0.28699115 0.33892542 0.35755706 0.34714857 0.31987324 0.28612116 0.25330138 0.22474599 0.20386036 0.19753058 0.20686018][-0.010591133 0.041183136 0.11759699 0.20285571 0.27690813 0.32378072 0.33662051 0.32216665 0.295126 0.26678145 0.24204573 0.2206849 0.20405158 0.19926453 0.20831285][-0.016729027 0.031545535 0.10416638 0.18514024 0.25359446 0.29341543 0.29907849 0.27930298 0.25095421 0.22589871 0.20685488 0.19083637 0.17781536 0.17430308 0.1822951]]...]
INFO - root - 2017-12-11 07:36:33.032296: step 33710, loss = 0.71, batch loss = 0.65 (14.1 examples/sec; 0.567 sec/batch; 47h:03m:56s remains)
INFO - root - 2017-12-11 07:36:38.608064: step 33720, loss = 0.71, batch loss = 0.65 (14.9 examples/sec; 0.537 sec/batch; 44h:35m:29s remains)
INFO - root - 2017-12-11 07:36:44.108621: step 33730, loss = 0.69, batch loss = 0.63 (14.4 examples/sec; 0.556 sec/batch; 46h:08m:13s remains)
INFO - root - 2017-12-11 07:36:49.766932: step 33740, loss = 0.69, batch loss = 0.63 (13.2 examples/sec; 0.604 sec/batch; 50h:08m:59s remains)
INFO - root - 2017-12-11 07:36:55.320858: step 33750, loss = 0.70, batch loss = 0.64 (14.5 examples/sec; 0.551 sec/batch; 45h:42m:30s remains)
INFO - root - 2017-12-11 07:37:00.763034: step 33760, loss = 0.69, batch loss = 0.63 (14.7 examples/sec; 0.543 sec/batch; 45h:03m:45s remains)
INFO - root - 2017-12-11 07:37:06.035307: step 33770, loss = 0.71, batch loss = 0.65 (14.3 examples/sec; 0.558 sec/batch; 46h:19m:33s remains)
INFO - root - 2017-12-11 07:37:11.453215: step 33780, loss = 0.71, batch loss = 0.65 (15.0 examples/sec; 0.535 sec/batch; 44h:21m:20s remains)
INFO - root - 2017-12-11 07:37:16.786078: step 33790, loss = 0.69, batch loss = 0.63 (14.9 examples/sec; 0.535 sec/batch; 44h:24m:42s remains)
INFO - root - 2017-12-11 07:37:22.227460: step 33800, loss = 0.70, batch loss = 0.64 (15.3 examples/sec; 0.523 sec/batch; 43h:25m:55s remains)
2017-12-11 07:37:22.818665: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.10091083 0.12157064 0.15774006 0.20798878 0.26378754 0.31089613 0.3349784 0.32717353 0.28816834 0.22917427 0.16839941 0.11935166 0.0853785 0.065684542 0.055751458][0.12558451 0.15137076 0.19713309 0.25969416 0.3279922 0.38340133 0.40752876 0.38913649 0.33169612 0.25209352 0.17312519 0.1103477 0.068265609 0.047167383 0.040342752][0.14107768 0.17182255 0.22161312 0.28800088 0.36323661 0.4267967 0.45621648 0.43568552 0.36963311 0.27751046 0.18485765 0.11135993 0.063699946 0.04261554 0.037747476][0.14096405 0.17534913 0.226243 0.2921913 0.37010744 0.44056478 0.47909787 0.46513924 0.40054256 0.30508435 0.20710996 0.13067067 0.083442211 0.064475842 0.059552662][0.1303978 0.16541068 0.21713194 0.28397664 0.36454666 0.44215947 0.49225992 0.49022543 0.43416882 0.34276429 0.24678528 0.17397229 0.13214836 0.11753571 0.11267077][0.1166273 0.14746702 0.19972371 0.27144456 0.36133787 0.45413026 0.52359551 0.53921807 0.49347889 0.40291187 0.3023735 0.22557323 0.18390711 0.17270392 0.17125647][0.10169655 0.12521574 0.17701103 0.25703847 0.36386985 0.48122853 0.57687014 0.61202812 0.57282633 0.4741731 0.35543379 0.25997534 0.20751624 0.19533233 0.1992418][0.088235229 0.10190778 0.15112007 0.23928569 0.36428335 0.50637895 0.62560779 0.67514503 0.63683778 0.52445108 0.38119185 0.26058269 0.19250332 0.17676419 0.18559913][0.072753862 0.076917052 0.12255726 0.21461287 0.34780863 0.49895388 0.62489331 0.67699379 0.63685077 0.51745033 0.36262757 0.22952537 0.15325534 0.13516065 0.14682768][0.051152393 0.048643894 0.088842995 0.1756503 0.30028048 0.43902966 0.55248243 0.5974136 0.55781186 0.44503096 0.29903731 0.17272845 0.10000646 0.082671262 0.095595665][0.024354242 0.014422467 0.04351683 0.11425277 0.21698731 0.33094248 0.42311954 0.45813942 0.42286903 0.32598013 0.20063432 0.092173606 0.030790765 0.01875715 0.035132669][-0.0019595262 -0.020107839 -0.0072226319 0.039399896 0.11261187 0.19661601 0.26553488 0.29169694 0.26445913 0.18920466 0.090936683 0.0061913342 -0.039293669 -0.043177735 -0.022817822][-0.023035377 -0.04448827 -0.044918556 -0.022033116 0.020516805 0.072481751 0.11615946 0.13281867 0.11487089 0.06460917 -0.0016965171 -0.057938769 -0.084950782 -0.081150971 -0.060168672][-0.0382813 -0.056600556 -0.063097723 -0.05689479 -0.039024737 -0.01505073 0.0053491425 0.012244219 0.0017894255 -0.025592258 -0.061315037 -0.089840338 -0.099789724 -0.090934061 -0.072757311][-0.047232863 -0.061289549 -0.070449263 -0.074737906 -0.073203675 -0.067798086 -0.062780283 -0.062233362 -0.067473508 -0.078821413 -0.09272974 -0.1014185 -0.099671863 -0.088286139 -0.073032342]]...]
INFO - root - 2017-12-11 07:37:28.360398: step 33810, loss = 0.69, batch loss = 0.64 (14.9 examples/sec; 0.538 sec/batch; 44h:40m:27s remains)
INFO - root - 2017-12-11 07:37:33.843532: step 33820, loss = 0.69, batch loss = 0.63 (14.7 examples/sec; 0.543 sec/batch; 45h:02m:30s remains)
INFO - root - 2017-12-11 07:37:39.319843: step 33830, loss = 0.68, batch loss = 0.63 (14.8 examples/sec; 0.541 sec/batch; 44h:52m:46s remains)
INFO - root - 2017-12-11 07:37:44.864160: step 33840, loss = 0.69, batch loss = 0.64 (14.0 examples/sec; 0.572 sec/batch; 47h:27m:17s remains)
INFO - root - 2017-12-11 07:37:50.402763: step 33850, loss = 0.71, batch loss = 0.65 (14.5 examples/sec; 0.552 sec/batch; 45h:49m:49s remains)
INFO - root - 2017-12-11 07:37:55.803107: step 33860, loss = 0.69, batch loss = 0.63 (15.7 examples/sec; 0.509 sec/batch; 42h:11m:35s remains)
INFO - root - 2017-12-11 07:38:01.109276: step 33870, loss = 0.68, batch loss = 0.63 (14.7 examples/sec; 0.546 sec/batch; 45h:17m:34s remains)
INFO - root - 2017-12-11 07:38:06.658637: step 33880, loss = 0.70, batch loss = 0.64 (14.2 examples/sec; 0.563 sec/batch; 46h:41m:15s remains)
INFO - root - 2017-12-11 07:38:12.086623: step 33890, loss = 0.70, batch loss = 0.64 (14.5 examples/sec; 0.551 sec/batch; 45h:43m:15s remains)
INFO - root - 2017-12-11 07:38:17.746620: step 33900, loss = 0.71, batch loss = 0.65 (14.1 examples/sec; 0.568 sec/batch; 47h:04m:39s remains)
2017-12-11 07:38:18.327677: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.06563212 -0.062929638 -0.060441807 -0.060114 -0.060755398 -0.062860668 -0.066019118 -0.069316819 -0.07307028 -0.078445882 -0.084748521 -0.09213993 -0.099643253 -0.10490543 -0.10645339][-0.0485535 -0.039069086 -0.030703183 -0.02456131 -0.016699689 -0.00965429 -0.0057450305 -0.0055166469 -0.011669865 -0.023890687 -0.039460808 -0.057961993 -0.076098211 -0.090534061 -0.0987489][-0.020924358 -0.00072503096 0.018196544 0.036558442 0.064257331 0.095069453 0.12001706 0.13163459 0.12361555 0.09841761 0.062632412 0.021082528 -0.017375141 -0.047093544 -0.066457465][0.018402535 0.055482447 0.092043839 0.13020307 0.18941468 0.25936615 0.32034329 0.35245758 0.34333512 0.29727888 0.2266745 0.14580673 0.074445136 0.021800965 -0.013652493][0.070608415 0.13510695 0.20002405 0.26692647 0.36457613 0.47966281 0.5805431 0.6338588 0.61944556 0.542625 0.423007 0.28979608 0.17727141 0.098056071 0.045202412][0.12534991 0.22471742 0.32722753 0.42908907 0.562743 0.7144146 0.84430975 0.91085613 0.88614553 0.77552503 0.60526258 0.42038447 0.26860139 0.16453555 0.095914237][0.17771332 0.31533679 0.46077025 0.59832823 0.75565773 0.921156 1.055186 1.1173985 1.0766245 0.938284 0.73151058 0.51115477 0.33182147 0.20878309 0.12783505][0.21943602 0.38935757 0.5711481 0.73288935 0.89149737 1.0403712 1.1481488 1.1870058 1.1299171 0.98170352 0.76713938 0.53873831 0.34991169 0.2165847 0.12848809][0.24076749 0.42547879 0.62361073 0.78901279 0.92513818 1.0303108 1.0875125 1.0901659 1.0226337 0.88737029 0.69701147 0.49033585 0.31265542 0.18164155 0.095127627][0.23508617 0.41156131 0.5986324 0.74313343 0.83723271 0.88413352 0.88315451 0.84959817 0.78113931 0.67653149 0.53287709 0.37044644 0.22248749 0.1082951 0.034120012][0.18942881 0.33481827 0.48551729 0.59045684 0.63627 0.63036358 0.58562452 0.53109759 0.47320619 0.40632337 0.31548816 0.20518665 0.0975768 0.011684342 -0.041209627][0.10544306 0.20637834 0.30743605 0.36667308 0.37190121 0.33378884 0.27158862 0.21622752 0.17606528 0.14258149 0.096287087 0.032874513 -0.033575885 -0.086467057 -0.11368934][0.012706372 0.067860074 0.11990324 0.14010939 0.12165508 0.075602569 0.019611729 -0.0225826 -0.045285732 -0.056983106 -0.074486911 -0.10401455 -0.13669 -0.15991691 -0.16398562][-0.063182645 -0.043680418 -0.027560312 -0.031285983 -0.056329593 -0.093949758 -0.13231334 -0.15774383 -0.16789216 -0.16896954 -0.17173976 -0.18062904 -0.19021627 -0.19224092 -0.18091907][-0.11372067 -0.11492698 -0.116995 -0.12847397 -0.14847063 -0.17187443 -0.19268262 -0.20495756 -0.2082094 -0.20588923 -0.20313542 -0.20181742 -0.19865263 -0.1891378 -0.17121918]]...]
INFO - root - 2017-12-11 07:38:23.918807: step 33910, loss = 0.71, batch loss = 0.65 (14.2 examples/sec; 0.562 sec/batch; 46h:37m:36s remains)
INFO - root - 2017-12-11 07:38:29.458768: step 33920, loss = 0.68, batch loss = 0.62 (14.7 examples/sec; 0.544 sec/batch; 45h:08m:31s remains)
INFO - root - 2017-12-11 07:38:34.944600: step 33930, loss = 0.71, batch loss = 0.65 (14.4 examples/sec; 0.554 sec/batch; 45h:59m:04s remains)
INFO - root - 2017-12-11 07:38:40.495965: step 33940, loss = 0.69, batch loss = 0.63 (14.5 examples/sec; 0.552 sec/batch; 45h:46m:53s remains)
INFO - root - 2017-12-11 07:38:45.996986: step 33950, loss = 0.71, batch loss = 0.65 (14.7 examples/sec; 0.544 sec/batch; 45h:06m:11s remains)
INFO - root - 2017-12-11 07:38:51.247575: step 33960, loss = 0.71, batch loss = 0.65 (14.3 examples/sec; 0.559 sec/batch; 46h:23m:40s remains)
INFO - root - 2017-12-11 07:38:56.802733: step 33970, loss = 0.69, batch loss = 0.63 (14.6 examples/sec; 0.547 sec/batch; 45h:19m:20s remains)
INFO - root - 2017-12-11 07:39:02.335939: step 33980, loss = 0.70, batch loss = 0.64 (14.3 examples/sec; 0.560 sec/batch; 46h:27m:26s remains)
INFO - root - 2017-12-11 07:39:07.837413: step 33990, loss = 0.71, batch loss = 0.65 (14.7 examples/sec; 0.546 sec/batch; 45h:15m:50s remains)
INFO - root - 2017-12-11 07:39:13.390925: step 34000, loss = 0.68, batch loss = 0.63 (14.0 examples/sec; 0.573 sec/batch; 47h:33m:00s remains)
2017-12-11 07:39:13.995192: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.013655385 0.028637964 0.040370569 0.043272045 0.037150484 0.024350666 0.013503434 0.0058159945 -0.00093576434 -0.0022059004 0.0046767849 0.025334809 0.061473504 0.10839885 0.15426214][0.044158787 0.070336349 0.093147434 0.10390674 0.10059562 0.085688166 0.071949691 0.061581884 0.051228948 0.045974482 0.051298257 0.075960562 0.12119038 0.17889127 0.23502368][0.066126369 0.1048851 0.14111765 0.16275981 0.16491172 0.15049039 0.13557713 0.12325595 0.10860769 0.096291095 0.095723473 0.11892034 0.16652341 0.22657439 0.28486121][0.077178583 0.12774415 0.17798184 0.21188533 0.22181544 0.21202692 0.19984595 0.18794276 0.16902666 0.14734454 0.13611704 0.15028366 0.1909062 0.24346416 0.29569611][0.082378037 0.14461493 0.2085728 0.2549845 0.27494141 0.27531382 0.27149561 0.26338708 0.2413187 0.21033551 0.18703192 0.1875644 0.21492848 0.25404942 0.29542458][0.089613758 0.16200925 0.23624018 0.29135844 0.32013696 0.33393106 0.34291804 0.34211287 0.32066774 0.28603616 0.25660282 0.24726911 0.26312569 0.28980982 0.31935573][0.10246813 0.18302675 0.26256683 0.32057095 0.35434517 0.38003403 0.40132231 0.40836397 0.39135274 0.35992649 0.33158261 0.31855288 0.32745081 0.34463698 0.36189166][0.1185956 0.20399043 0.28364336 0.33921602 0.37356815 0.40563932 0.43348002 0.44482091 0.4336926 0.41094568 0.39022055 0.37988946 0.38685426 0.39816782 0.40458059][0.12909505 0.21292484 0.28654477 0.33503816 0.36507285 0.39549735 0.42152071 0.432516 0.42761689 0.41695735 0.40811557 0.40501338 0.4142178 0.42353886 0.42320761][0.12012662 0.1935076 0.25463438 0.29219052 0.31392166 0.33585179 0.353121 0.35975757 0.35960689 0.36041415 0.36315188 0.36815277 0.38146028 0.39206129 0.39057356][0.085278988 0.14064033 0.18477677 0.20959446 0.22151716 0.23185074 0.23749988 0.23788212 0.23987249 0.24772492 0.25749788 0.26793721 0.2844936 0.29766387 0.29909357][0.035279512 0.070737287 0.09830457 0.1121095 0.11605753 0.11677661 0.1133526 0.10858654 0.10946275 0.11717004 0.12613545 0.135292 0.14957541 0.1619058 0.16601011][-0.012633904 0.0055791475 0.020153416 0.026189018 0.025258189 0.020676117 0.012991527 0.0059764828 0.0045967037 0.0080534173 0.011678235 0.015688328 0.024699934 0.034216702 0.040048428][-0.046951726 -0.041178074 -0.035578113 -0.034659211 -0.038229052 -0.044632852 -0.052569285 -0.058937784 -0.061338175 -0.061351459 -0.061888684 -0.061870266 -0.057602845 -0.051436678 -0.045540225][-0.065007269 -0.066730596 -0.066274628 -0.067807145 -0.071622193 -0.076925658 -0.082569338 -0.086778916 -0.088878684 -0.090452 -0.092990026 -0.095315054 -0.09489283 -0.092441 -0.088476606]]...]
INFO - root - 2017-12-11 07:39:19.607222: step 34010, loss = 0.72, batch loss = 0.67 (14.6 examples/sec; 0.548 sec/batch; 45h:28m:36s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-SGD-lr0.001-clip50-initfromfixqian30k-fixqian
INFO - root - 2017-12-11 07:39:25.092080: step 34020, loss = 0.71, batch loss = 0.65 (14.1 examples/sec; 0.569 sec/batch; 47h:10m:57s remains)
INFO - root - 2017-12-11 07:39:30.677367: step 34030, loss = 0.70, batch loss = 0.64 (14.3 examples/sec; 0.558 sec/batch; 46h:14m:39s remains)
INFO - root - 2017-12-11 07:39:36.209018: step 34040, loss = 0.68, batch loss = 0.62 (14.5 examples/sec; 0.552 sec/batch; 45h:45m:42s remains)
INFO - root - 2017-12-11 07:39:41.643133: step 34050, loss = 0.69, batch loss = 0.63 (14.9 examples/sec; 0.538 sec/batch; 44h:35m:05s remains)
INFO - root - 2017-12-11 07:39:46.823648: step 34060, loss = 0.71, batch loss = 0.65 (14.8 examples/sec; 0.540 sec/batch; 44h:45m:17s remains)
INFO - root - 2017-12-11 07:39:52.266881: step 34070, loss = 0.70, batch loss = 0.64 (14.4 examples/sec; 0.556 sec/batch; 46h:05m:14s remains)
INFO - root - 2017-12-11 07:39:57.710379: step 34080, loss = 0.70, batch loss = 0.64 (14.7 examples/sec; 0.546 sec/batch; 45h:15m:30s remains)
INFO - root - 2017-12-11 07:40:03.204548: step 34090, loss = 0.70, batch loss = 0.64 (14.0 examples/sec; 0.573 sec/batch; 47h:31m:18s remains)
INFO - root - 2017-12-11 07:40:08.778562: step 34100, loss = 0.69, batch loss = 0.64 (14.4 examples/sec; 0.554 sec/batch; 45h:53m:28s remains)
2017-12-11 07:40:09.374843: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0045796572 -0.0028894707 -0.00024118902 0.0029649111 0.0065790168 0.0097535029 0.011331911 0.0096333837 0.0033970142 -0.0070279744 -0.018156169 -0.026506575 -0.030539768 -0.029739304 -0.024695257][-0.023728862 -0.019804047 -0.01297264 -0.0034075778 0.0075223143 0.016470272 0.020170281 0.016152207 0.0035790792 -0.016144043 -0.036849715 -0.052192118 -0.059219629 -0.058873653 -0.0541626][-0.038175717 -0.029639779 -0.014931134 0.0064640469 0.031091746 0.051404957 0.060673892 0.054715876 0.032921404 -0.001312831 -0.037334185 -0.064154193 -0.077105351 -0.078836851 -0.074955449][-0.039736964 -0.02409308 0.0021091376 0.040280435 0.084344774 0.12131491 0.13981885 0.13283154 0.099556707 0.046026848 -0.011050384 -0.054656036 -0.077734448 -0.084191613 -0.082208648][-0.026563043 -0.0012105503 0.039593037 0.097635128 0.16421461 0.22094686 0.25137016 0.24459623 0.19921063 0.12450329 0.043203279 -0.021406231 -0.059338562 -0.074315757 -0.076241642][-0.0075909351 0.028542809 0.084682159 0.16197698 0.24978538 0.32550564 0.36826426 0.36281043 0.30692559 0.21323691 0.10927477 0.023138551 -0.032207288 -0.058753431 -0.066914715][0.0093961563 0.054905824 0.12383992 0.21579905 0.31947085 0.41007182 0.4634921 0.46021184 0.39721835 0.29025 0.16968028 0.06624727 -0.0046192631 -0.042473491 -0.057493336][0.024973962 0.077024154 0.15416414 0.25373015 0.36452833 0.46182677 0.5204882 0.51886624 0.45338902 0.34174022 0.21443962 0.10203379 0.021035394 -0.02534784 -0.046234958][0.045226116 0.10111982 0.18092637 0.27928784 0.38557509 0.47827005 0.534735 0.53406715 0.47214296 0.36614454 0.24334523 0.13084455 0.045309298 -0.0068104784 -0.032318436][0.073340461 0.13136567 0.20900883 0.29848573 0.38976857 0.46694753 0.5126912 0.50975174 0.4543241 0.36114711 0.25180352 0.14734641 0.063803174 0.010461685 -0.016884286][0.10662092 0.16441517 0.23525243 0.31009907 0.37977049 0.43480873 0.464358 0.45660657 0.40856349 0.33132675 0.24002315 0.14879355 0.072303928 0.021658722 -0.0049801944][0.13783295 0.19251029 0.25309533 0.31078503 0.35788342 0.39084125 0.40469369 0.39246175 0.35164747 0.28930482 0.21527268 0.13802992 0.070512287 0.024542131 2.1606445e-05][0.15820351 0.20763648 0.2565701 0.29737446 0.32448211 0.33926517 0.34130135 0.32658979 0.29240498 0.24238387 0.1828447 0.11846703 0.060497358 0.020449961 -0.00088081363][0.15814903 0.19976257 0.23598731 0.2608006 0.27156636 0.27321994 0.26817921 0.25390726 0.22707292 0.18870527 0.14243333 0.09068884 0.043268509 0.010652092 -0.0062865834][0.13561894 0.16628401 0.18881188 0.19919182 0.19825214 0.19256321 0.18495831 0.17312083 0.15354471 0.12623353 0.0930809 0.055419166 0.02109083 -0.0016438751 -0.012806779]]...]
INFO - root - 2017-12-11 07:40:14.992207: step 34110, loss = 0.70, batch loss = 0.64 (15.0 examples/sec; 0.532 sec/batch; 44h:06m:27s remains)
INFO - root - 2017-12-11 07:40:20.454600: step 34120, loss = 0.69, batch loss = 0.64 (14.8 examples/sec; 0.542 sec/batch; 44h:54m:58s remains)
INFO - root - 2017-12-11 07:40:26.080945: step 34130, loss = 0.69, batch loss = 0.63 (14.1 examples/sec; 0.568 sec/batch; 47h:02m:17s remains)
INFO - root - 2017-12-11 07:40:31.530099: step 34140, loss = 0.70, batch loss = 0.64 (15.0 examples/sec; 0.532 sec/batch; 44h:03m:18s remains)
INFO - root - 2017-12-11 07:40:37.007477: step 34150, loss = 0.69, batch loss = 0.64 (14.5 examples/sec; 0.550 sec/batch; 45h:34m:42s remains)
INFO - root - 2017-12-11 07:40:42.197967: step 34160, loss = 0.69, batch loss = 0.63 (14.3 examples/sec; 0.561 sec/batch; 46h:30m:47s remains)
INFO - root - 2017-12-11 07:40:47.699303: step 34170, loss = 0.69, batch loss = 0.63 (14.1 examples/sec; 0.569 sec/batch; 47h:08m:48s remains)
INFO - root - 2017-12-11 07:40:53.215605: step 34180, loss = 0.70, batch loss = 0.64 (14.6 examples/sec; 0.549 sec/batch; 45h:29m:41s remains)
INFO - root - 2017-12-11 07:40:58.649846: step 34190, loss = 0.70, batch loss = 0.64 (14.6 examples/sec; 0.549 sec/batch; 45h:31m:49s remains)
INFO - root - 2017-12-11 07:41:04.213785: step 34200, loss = 0.69, batch loss = 0.63 (14.4 examples/sec; 0.557 sec/batch; 46h:09m:24s remains)
2017-12-11 07:41:04.792840: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.22602962 0.24573061 0.2620129 0.27592263 0.2822099 0.27923071 0.26503548 0.23803768 0.21029924 0.19551581 0.20055959 0.21826902 0.23630284 0.24392399 0.22794513][0.24352765 0.25404093 0.26413983 0.27663606 0.28487593 0.28532398 0.27544403 0.25349519 0.22923946 0.21391673 0.21207881 0.21755689 0.22257443 0.21969627 0.19789472][0.24909388 0.25008506 0.25345778 0.26417115 0.27471131 0.27910203 0.27368557 0.25709608 0.23780079 0.22464778 0.21941295 0.21683362 0.21185368 0.20007421 0.17281485][0.24622016 0.23859331 0.23575285 0.24402459 0.25630304 0.2643131 0.26340026 0.25229171 0.23871446 0.22892132 0.22354411 0.21873137 0.21040985 0.19511919 0.16645594][0.23606959 0.22308691 0.21613529 0.2223537 0.23617366 0.24847113 0.25389412 0.25022146 0.24312721 0.23690678 0.23293667 0.22887214 0.22156787 0.20733503 0.18168773][0.22884275 0.21431734 0.20572978 0.21163045 0.22795148 0.24559291 0.25764006 0.25963965 0.25443694 0.24652684 0.24021555 0.23581505 0.23103833 0.22236662 0.20639932][0.23551962 0.22162026 0.21277833 0.21904698 0.2385722 0.26177517 0.27934632 0.28368351 0.27468234 0.2584621 0.24339566 0.2342398 0.2308972 0.22971691 0.2272999][0.24913274 0.23680924 0.22821906 0.23457289 0.25641939 0.28390944 0.30543405 0.31086567 0.29758644 0.27281556 0.24887928 0.23475066 0.23247376 0.23736207 0.24495783][0.25468543 0.24596626 0.23885071 0.24529856 0.26778015 0.2973859 0.32119724 0.32759506 0.31204677 0.28208089 0.25238597 0.23402706 0.2309673 0.23760338 0.247942][0.2537652 0.24750516 0.240898 0.24620743 0.2670497 0.29599953 0.32012472 0.327347 0.31165341 0.28023544 0.24764359 0.22524929 0.21920839 0.22447084 0.23357639][0.24158026 0.23378685 0.22428127 0.22599585 0.24303065 0.26901862 0.29144129 0.29894632 0.2852011 0.25636756 0.22481681 0.20133173 0.19324003 0.19725592 0.20479655][0.22237663 0.21052411 0.19659422 0.19358948 0.20585746 0.22796305 0.24813151 0.25651211 0.24735954 0.22515406 0.19879438 0.17742172 0.16881482 0.17058198 0.17379765][0.20789991 0.19394983 0.17594641 0.16805559 0.17453483 0.19176628 0.20882294 0.21766479 0.21383646 0.19980381 0.18090017 0.16355631 0.15468533 0.15231048 0.14828545][0.19636872 0.18310618 0.16347264 0.15286741 0.15538649 0.16875997 0.18261339 0.19061948 0.18980552 0.18126149 0.16822973 0.15507823 0.14706016 0.14176355 0.13176887][0.18293254 0.17140822 0.15209498 0.14183292 0.14352715 0.15476622 0.16541167 0.17103676 0.17028013 0.1636098 0.15410343 0.14595139 0.14253463 0.13962528 0.12934804]]...]
INFO - root - 2017-12-11 07:41:10.373570: step 34210, loss = 0.69, batch loss = 0.64 (14.4 examples/sec; 0.556 sec/batch; 46h:06m:09s remains)
INFO - root - 2017-12-11 07:41:15.870138: step 34220, loss = 0.69, batch loss = 0.63 (14.7 examples/sec; 0.544 sec/batch; 45h:03m:13s remains)
INFO - root - 2017-12-11 07:41:21.290949: step 34230, loss = 0.69, batch loss = 0.64 (14.8 examples/sec; 0.541 sec/batch; 44h:50m:57s remains)
INFO - root - 2017-12-11 07:41:26.839311: step 34240, loss = 0.68, batch loss = 0.63 (14.8 examples/sec; 0.539 sec/batch; 44h:39m:04s remains)
INFO - root - 2017-12-11 07:41:32.029035: step 34250, loss = 0.71, batch loss = 0.65 (15.1 examples/sec; 0.528 sec/batch; 43h:46m:15s remains)
INFO - root - 2017-12-11 07:41:37.582829: step 34260, loss = 0.71, batch loss = 0.65 (14.6 examples/sec; 0.548 sec/batch; 45h:23m:32s remains)
INFO - root - 2017-12-11 07:41:43.094745: step 34270, loss = 0.68, batch loss = 0.62 (14.7 examples/sec; 0.543 sec/batch; 45h:01m:21s remains)
INFO - root - 2017-12-11 07:41:48.516353: step 34280, loss = 0.70, batch loss = 0.65 (14.5 examples/sec; 0.550 sec/batch; 45h:33m:13s remains)
INFO - root - 2017-12-11 07:41:54.081971: step 34290, loss = 0.71, batch loss = 0.65 (14.5 examples/sec; 0.553 sec/batch; 45h:47m:50s remains)
INFO - root - 2017-12-11 07:41:59.608746: step 34300, loss = 0.69, batch loss = 0.63 (14.5 examples/sec; 0.552 sec/batch; 45h:44m:01s remains)
2017-12-11 07:42:00.216854: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.051983293 -0.046330355 -0.037116908 -0.032352321 -0.03702705 -0.049586039 -0.065927692 -0.078885853 -0.083215378 -0.075697958 -0.058181282 -0.035126053 -0.01014924 0.0093663372 0.020309914][-0.02008483 -0.00039215758 0.021289976 0.032942519 0.026027277 0.0025712433 -0.029067904 -0.057114735 -0.074524939 -0.077108949 -0.065345339 -0.045283671 -0.022365762 -0.0029769784 0.0084796492][0.031486992 0.07322517 0.11541919 0.14010754 0.13256925 0.094832383 0.040528748 -0.011453739 -0.049873613 -0.069271341 -0.068556949 -0.054899104 -0.03576912 -0.017682107 -0.006646974][0.087172307 0.15503161 0.22212465 0.26382408 0.25860015 0.20855778 0.13210019 0.055257525 -0.0051370412 -0.042634137 -0.056548961 -0.054919206 -0.045037508 -0.032743067 -0.025084578][0.13104844 0.22364978 0.31650332 0.37923363 0.38500187 0.33468211 0.24756728 0.15279414 0.073069371 0.016884843 -0.016213154 -0.03415503 -0.041137323 -0.039367493 -0.0362334][0.14838526 0.26063654 0.37643903 0.46126002 0.48654726 0.4509927 0.36916414 0.26723671 0.17312488 0.099582843 0.046042435 0.0059647984 -0.019230278 -0.027026514 -0.023989975][0.13736932 0.26233414 0.39430788 0.4964883 0.54344475 0.53191656 0.46790457 0.37061659 0.27144954 0.18821113 0.1212611 0.065898657 0.028113931 0.013845032 0.019571649][0.10926513 0.23915297 0.37958035 0.49238038 0.55779314 0.57115263 0.53081352 0.45026222 0.36174324 0.28487808 0.21985276 0.1620772 0.11917875 0.098334983 0.10262311][0.072565034 0.1979308 0.33747816 0.45263088 0.52909648 0.56234026 0.54804546 0.49798825 0.44100308 0.39244503 0.34838879 0.30163044 0.2589367 0.2289032 0.22407493][0.032166049 0.14247614 0.26982838 0.37771538 0.45628324 0.50239861 0.5141995 0.50546914 0.49725223 0.49463534 0.48557606 0.45861179 0.41767025 0.37351719 0.34875754][-0.0096303262 0.076135226 0.17908597 0.26910403 0.34064677 0.39292839 0.42880598 0.46490896 0.511085 0.55838346 0.58484513 0.57519311 0.53173161 0.46822166 0.41562131][-0.047281984 0.0094429022 0.0803519 0.14476058 0.20194733 0.25416204 0.30887094 0.38237852 0.47150436 0.55349559 0.5991919 0.59345621 0.54059023 0.45798647 0.3799825][-0.073655404 -0.042364411 -0.0011741944 0.038050525 0.078664131 0.12592492 0.189849 0.28202528 0.38849127 0.47861513 0.5224461 0.5090974 0.44746813 0.35664761 0.2674655][-0.083939746 -0.070199408 -0.050198078 -0.030182505 -0.0049205781 0.032184146 0.091093458 0.17739375 0.27195126 0.34525537 0.37261909 0.34907535 0.2873466 0.20487262 0.12359401][-0.072636113 -0.067345113 -0.058977023 -0.051089078 -0.039027214 -0.016760791 0.024178922 0.085052751 0.14840168 0.19201182 0.1988177 0.16855536 0.11510886 0.051785015 -0.0091440342]]...]
INFO - root - 2017-12-11 07:42:05.814881: step 34310, loss = 0.67, batch loss = 0.61 (14.2 examples/sec; 0.563 sec/batch; 46h:40m:24s remains)
INFO - root - 2017-12-11 07:42:11.289669: step 34320, loss = 0.70, batch loss = 0.64 (14.8 examples/sec; 0.541 sec/batch; 44h:48m:36s remains)
INFO - root - 2017-12-11 07:42:16.687415: step 34330, loss = 0.70, batch loss = 0.64 (15.4 examples/sec; 0.521 sec/batch; 43h:06m:49s remains)
INFO - root - 2017-12-11 07:42:22.172333: step 34340, loss = 0.70, batch loss = 0.64 (14.4 examples/sec; 0.555 sec/batch; 45h:57m:20s remains)
INFO - root - 2017-12-11 07:42:27.295117: step 34350, loss = 0.72, batch loss = 0.66 (14.8 examples/sec; 0.539 sec/batch; 44h:37m:22s remains)
INFO - root - 2017-12-11 07:42:32.812498: step 34360, loss = 0.69, batch loss = 0.63 (14.5 examples/sec; 0.550 sec/batch; 45h:34m:56s remains)
INFO - root - 2017-12-11 07:42:38.380461: step 34370, loss = 0.69, batch loss = 0.63 (14.5 examples/sec; 0.552 sec/batch; 45h:45m:03s remains)
INFO - root - 2017-12-11 07:42:43.884545: step 34380, loss = 0.72, batch loss = 0.66 (14.8 examples/sec; 0.539 sec/batch; 44h:39m:12s remains)
INFO - root - 2017-12-11 07:42:49.388858: step 34390, loss = 0.69, batch loss = 0.63 (14.7 examples/sec; 0.545 sec/batch; 45h:09m:23s remains)
INFO - root - 2017-12-11 07:42:54.797170: step 34400, loss = 0.70, batch loss = 0.65 (14.8 examples/sec; 0.539 sec/batch; 44h:36m:59s remains)
2017-12-11 07:42:55.411574: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.069225729 -0.070813008 -0.072417133 -0.074031293 -0.0743257 -0.074650116 -0.076248996 -0.079916932 -0.084451564 -0.08866138 -0.092808604 -0.09733291 -0.1015989 -0.10377309 -0.10360799][-0.031529672 -0.0369596 -0.044459589 -0.050857745 -0.053129926 -0.052648373 -0.051810641 -0.052902013 -0.056652151 -0.062426735 -0.070351005 -0.080892429 -0.092362 -0.10112021 -0.10517588][0.028707787 0.018902492 0.0051645567 -0.0049318937 -0.0071382718 -0.0025301611 0.00552706 0.011539349 0.010145137 0.00072709232 -0.015846161 -0.038353462 -0.063420415 -0.084723741 -0.097474784][0.10195663 0.0923014 0.076876104 0.067205846 0.067967087 0.0786274 0.096063077 0.11039961 0.1101547 0.092475742 0.061635278 0.02151088 -0.022234043 -0.060455617 -0.085162885][0.18281971 0.18171068 0.17192365 0.16685098 0.17151646 0.18765479 0.21465687 0.23676121 0.23571785 0.20573542 0.15612338 0.093627974 0.026645968 -0.032061569 -0.071133137][0.27069476 0.28968573 0.29350173 0.29565737 0.30266249 0.3217282 0.35696545 0.38551286 0.38198 0.33733138 0.26639408 0.17831863 0.084246263 0.0015229264 -0.054506656][0.361348 0.40663025 0.4286063 0.43888545 0.4465248 0.46804103 0.51185268 0.54630655 0.53853774 0.47697446 0.38305014 0.26790005 0.14517413 0.037110209 -0.036271639][0.44005683 0.50852156 0.54620904 0.56198984 0.56697637 0.588743 0.63836849 0.67526215 0.66029167 0.58162212 0.46758828 0.33046943 0.18545784 0.058602657 -0.026403841][0.47956237 0.55878317 0.60213017 0.61616141 0.61315382 0.62937218 0.6776284 0.71290123 0.6932168 0.60687482 0.48556888 0.34192646 0.19063592 0.059192073 -0.027605142][0.46230203 0.53604966 0.57247353 0.57704705 0.56236953 0.56815118 0.60789943 0.6385988 0.61981475 0.53976089 0.42843762 0.29708377 0.15816475 0.037957698 -0.040268693][0.38698182 0.441782 0.46339476 0.45705813 0.43382934 0.42970505 0.456789 0.47920376 0.462548 0.39617959 0.30551612 0.19947587 0.087447479 -0.0078718569 -0.067405879][0.25896016 0.28776726 0.29282871 0.27905929 0.25421834 0.24514213 0.26043409 0.27393654 0.26009169 0.21046372 0.14445069 0.068840072 -0.0095054861 -0.072689123 -0.1073954][0.11434122 0.12046276 0.11271099 0.094834559 0.071562439 0.059450209 0.063913152 0.068570279 0.05744952 0.024792591 -0.016639665 -0.062087432 -0.10666466 -0.13752787 -0.14725147][-0.0041089365 -0.010579558 -0.023507638 -0.040963251 -0.060301684 -0.073255591 -0.076712526 -0.079033069 -0.088325866 -0.10776975 -0.13008958 -0.15216233 -0.1705853 -0.1771944 -0.16922958][-0.078079738 -0.08870697 -0.10036613 -0.11342385 -0.12669581 -0.13722272 -0.14355195 -0.14915085 -0.15729633 -0.16899644 -0.18022217 -0.18865809 -0.19192338 -0.18545263 -0.16947688]]...]
INFO - root - 2017-12-11 07:43:00.936413: step 34410, loss = 0.72, batch loss = 0.67 (14.3 examples/sec; 0.558 sec/batch; 46h:11m:22s remains)
INFO - root - 2017-12-11 07:43:06.386294: step 34420, loss = 0.69, batch loss = 0.63 (14.5 examples/sec; 0.551 sec/batch; 45h:37m:41s remains)
INFO - root - 2017-12-11 07:43:11.920886: step 34430, loss = 0.69, batch loss = 0.63 (14.8 examples/sec; 0.542 sec/batch; 44h:52m:01s remains)
INFO - root - 2017-12-11 07:43:17.448777: step 34440, loss = 0.70, batch loss = 0.64 (14.7 examples/sec; 0.544 sec/batch; 45h:03m:18s remains)
INFO - root - 2017-12-11 07:43:22.714492: step 34450, loss = 0.70, batch loss = 0.64 (13.6 examples/sec; 0.586 sec/batch; 48h:31m:50s remains)
INFO - root - 2017-12-11 07:43:28.309168: step 34460, loss = 0.70, batch loss = 0.64 (14.7 examples/sec; 0.545 sec/batch; 45h:07m:17s remains)
INFO - root - 2017-12-11 07:43:33.786323: step 34470, loss = 0.71, batch loss = 0.65 (14.5 examples/sec; 0.552 sec/batch; 45h:43m:38s remains)
INFO - root - 2017-12-11 07:43:39.304647: step 34480, loss = 0.69, batch loss = 0.63 (14.7 examples/sec; 0.545 sec/batch; 45h:06m:32s remains)
INFO - root - 2017-12-11 07:43:44.854891: step 34490, loss = 0.70, batch loss = 0.64 (13.7 examples/sec; 0.583 sec/batch; 48h:15m:58s remains)
INFO - root - 2017-12-11 07:43:50.463184: step 34500, loss = 0.70, batch loss = 0.65 (14.4 examples/sec; 0.557 sec/batch; 46h:08m:23s remains)
2017-12-11 07:43:51.035163: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.04995852 0.062635012 0.069195852 0.063583285 0.046474673 0.026420766 0.02035277 0.038604587 0.083729006 0.14694166 0.21044685 0.25249839 0.25312954 0.2122038 0.14704351][0.021164112 0.029006729 0.03182913 0.024942731 0.0095472336 -0.0068282569 -0.010543562 0.0062801009 0.044892915 0.097810768 0.15029286 0.18414958 0.18300642 0.14771298 0.0942386][0.003102235 0.0051813158 0.0043586744 -0.0013619023 -0.010496081 -0.018468218 -0.017686184 -0.0046714339 0.020404993 0.053147789 0.084948026 0.10492273 0.10261571 0.07892786 0.045117542][-0.0056466828 -0.0061562047 -0.0062197363 -0.0049402956 -0.00084827043 0.0064764535 0.016901845 0.027960233 0.037418146 0.044292673 0.04789022 0.047310647 0.03959167 0.025189705 0.0092093684][-0.0039075245 -3.5332683e-05 0.0089386618 0.025898397 0.051572643 0.081540257 0.10729069 0.11928602 0.11253956 0.089169025 0.057441 0.028302934 0.0074115754 -0.0053385394 -0.011168825][0.008625634 0.023687311 0.048816651 0.087524369 0.13908219 0.19424236 0.23665892 0.24992669 0.22646046 0.17145234 0.10233409 0.041760065 0.0032985478 -0.014058892 -0.017423924][0.032467462 0.061649125 0.10483569 0.16517355 0.23992531 0.31556332 0.36946857 0.38054666 0.34017819 0.2568514 0.15581192 0.06869977 0.014634198 -0.0087660756 -0.013521813][0.060703043 0.10027883 0.15568314 0.22955838 0.31697273 0.4014158 0.4569287 0.46116915 0.40675238 0.30515277 0.18631513 0.085890815 0.024887644 -0.0007095795 -0.0060980609][0.092244096 0.13341431 0.18946898 0.26330456 0.34791547 0.42554983 0.47044337 0.46284291 0.39892989 0.29281062 0.17527656 0.0798129 0.024775784 0.00407943 0.0016128846][0.12719473 0.16001754 0.20414095 0.26342523 0.32958767 0.38574046 0.40986428 0.38805351 0.32162479 0.22614646 0.12878577 0.055450831 0.018047944 0.0083112493 0.011155197][0.15418248 0.17398362 0.19927931 0.2353621 0.27408028 0.30181593 0.30300081 0.27076861 0.21089174 0.13819815 0.072812058 0.030361738 0.01497141 0.016998604 0.023972493][0.16340029 0.17295347 0.18270725 0.19778828 0.21108128 0.21343291 0.19669148 0.16040081 0.11299592 0.066569388 0.03313366 0.018783048 0.021293169 0.031044969 0.038889073][0.1505049 0.15678184 0.15965433 0.16241203 0.1591965 0.14526969 0.11906534 0.08473137 0.051393874 0.027336819 0.017526433 0.021496015 0.033704873 0.045849938 0.051806148][0.12301429 0.1310436 0.13379696 0.13218985 0.12174348 0.10124953 0.073626548 0.045190003 0.023945369 0.014472792 0.0170968 0.028437311 0.042506095 0.052967656 0.055899583][0.087790221 0.099143118 0.10499666 0.10419986 0.093443692 0.073677644 0.049908958 0.028639909 0.015803419 0.013248633 0.019176343 0.02996612 0.040912934 0.047774624 0.048106723]]...]
INFO - root - 2017-12-11 07:43:56.596691: step 34510, loss = 0.70, batch loss = 0.64 (14.4 examples/sec; 0.554 sec/batch; 45h:52m:50s remains)
INFO - root - 2017-12-11 07:44:02.079022: step 34520, loss = 0.70, batch loss = 0.64 (14.3 examples/sec; 0.559 sec/batch; 46h:15m:12s remains)
INFO - root - 2017-12-11 07:44:07.535792: step 34530, loss = 0.69, batch loss = 0.63 (14.1 examples/sec; 0.567 sec/batch; 46h:55m:28s remains)
INFO - root - 2017-12-11 07:44:12.954651: step 34540, loss = 0.71, batch loss = 0.65 (16.3 examples/sec; 0.491 sec/batch; 40h:36m:07s remains)
INFO - root - 2017-12-11 07:44:18.175554: step 34550, loss = 0.71, batch loss = 0.65 (14.4 examples/sec; 0.557 sec/batch; 46h:04m:37s remains)
INFO - root - 2017-12-11 07:44:23.687258: step 34560, loss = 0.71, batch loss = 0.65 (14.7 examples/sec; 0.546 sec/batch; 45h:09m:49s remains)
INFO - root - 2017-12-11 07:44:29.176423: step 34570, loss = 0.69, batch loss = 0.63 (14.7 examples/sec; 0.543 sec/batch; 44h:55m:02s remains)
INFO - root - 2017-12-11 07:44:34.641197: step 34580, loss = 0.70, batch loss = 0.64 (14.6 examples/sec; 0.547 sec/batch; 45h:13m:38s remains)
INFO - root - 2017-12-11 07:44:40.127851: step 34590, loss = 0.73, batch loss = 0.67 (14.1 examples/sec; 0.566 sec/batch; 46h:50m:14s remains)
INFO - root - 2017-12-11 07:44:45.648348: step 34600, loss = 0.70, batch loss = 0.64 (14.6 examples/sec; 0.549 sec/batch; 45h:27m:51s remains)
2017-12-11 07:44:46.265083: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.23083431 0.20756137 0.18286648 0.16088328 0.14717646 0.14210178 0.14206435 0.14234196 0.1365511 0.12885913 0.12706974 0.13554277 0.15419437 0.17539397 0.19879133][0.24217756 0.21905184 0.19520861 0.17668582 0.16876371 0.17098311 0.17692603 0.17975974 0.17443585 0.16631334 0.1635918 0.1722016 0.19453569 0.22746363 0.27118737][0.23590599 0.21346776 0.1932677 0.18410955 0.19047368 0.20913282 0.22987528 0.24294466 0.24401592 0.23892048 0.23527579 0.241485 0.26386052 0.30653241 0.37300882][0.21328178 0.19382295 0.18418331 0.19419493 0.22591178 0.27259761 0.3194131 0.35135934 0.36273244 0.35900944 0.34854603 0.3436802 0.35515895 0.39709425 0.47816885][0.1685009 0.15874633 0.17063954 0.21284245 0.28263465 0.36875445 0.45077193 0.50640196 0.525944 0.51544011 0.48773611 0.46013811 0.4475379 0.47485891 0.55661857][0.11474961 0.12382616 0.16782308 0.25225544 0.36810109 0.498385 0.61605692 0.69161761 0.71034878 0.68029922 0.62318069 0.56200838 0.515031 0.51377696 0.57628626][0.085544087 0.11584111 0.19019459 0.31318039 0.46995729 0.637443 0.782104 0.86909372 0.87858039 0.82099932 0.7281847 0.6292516 0.54459661 0.50593668 0.53153914][0.11074737 0.15927349 0.25167003 0.3960031 0.57413143 0.75827348 0.910732 0.99503595 0.98807168 0.90214139 0.776559 0.64519334 0.52928174 0.45574448 0.43954918][0.19565709 0.25775149 0.35221088 0.49172035 0.65802717 0.82240933 0.94920546 1.0079319 0.97820157 0.87261796 0.73099655 0.58708793 0.45993528 0.36811379 0.32127833][0.31504256 0.38267511 0.46238229 0.57072723 0.69274467 0.80354571 0.87565684 0.891597 0.83959609 0.73084927 0.59718555 0.46577597 0.35106656 0.26157495 0.20239654][0.42450327 0.48901415 0.54271317 0.604735 0.66601056 0.70948297 0.71892339 0.69059509 0.62307286 0.52503163 0.41601512 0.31414863 0.22842965 0.15793693 0.10391916][0.47569418 0.52692431 0.54893571 0.56174809 0.56377572 0.54739249 0.50767004 0.45058548 0.38131607 0.30529529 0.23058116 0.16688673 0.11751519 0.075628459 0.039981324][0.4478882 0.48034474 0.47504371 0.45031434 0.41130841 0.35820028 0.29426283 0.22973451 0.17196617 0.12331109 0.0845194 0.058586214 0.043494821 0.030088114 0.01630681][0.36058626 0.37659353 0.35671896 0.31526446 0.26135626 0.20013016 0.13809308 0.08484745 0.046717294 0.024001634 0.014245804 0.015904794 0.023657391 0.029152604 0.031033052][0.25557071 0.26149842 0.24106553 0.20397538 0.15975916 0.11464165 0.073254593 0.041386634 0.023052439 0.018210053 0.024321144 0.037549723 0.051753685 0.061297428 0.066903017]]...]
INFO - root - 2017-12-11 07:44:51.798595: step 34610, loss = 0.70, batch loss = 0.64 (14.7 examples/sec; 0.543 sec/batch; 44h:57m:41s remains)
INFO - root - 2017-12-11 07:44:57.266081: step 34620, loss = 0.69, batch loss = 0.63 (14.8 examples/sec; 0.539 sec/batch; 44h:36m:00s remains)
INFO - root - 2017-12-11 07:45:02.677255: step 34630, loss = 0.71, batch loss = 0.65 (14.3 examples/sec; 0.561 sec/batch; 46h:26m:07s remains)
INFO - root - 2017-12-11 07:45:07.696493: step 34640, loss = 0.70, batch loss = 0.64 (16.2 examples/sec; 0.495 sec/batch; 40h:58m:57s remains)
INFO - root - 2017-12-11 07:45:13.277569: step 34650, loss = 0.70, batch loss = 0.64 (14.0 examples/sec; 0.570 sec/batch; 47h:08m:18s remains)
INFO - root - 2017-12-11 07:45:18.735420: step 34660, loss = 0.70, batch loss = 0.64 (14.3 examples/sec; 0.559 sec/batch; 46h:17m:16s remains)
INFO - root - 2017-12-11 07:45:24.240370: step 34670, loss = 0.68, batch loss = 0.62 (14.4 examples/sec; 0.554 sec/batch; 45h:48m:16s remains)
INFO - root - 2017-12-11 07:45:29.809350: step 34680, loss = 0.70, batch loss = 0.65 (14.5 examples/sec; 0.550 sec/batch; 45h:30m:51s remains)
INFO - root - 2017-12-11 07:45:35.341167: step 34690, loss = 0.69, batch loss = 0.63 (14.1 examples/sec; 0.568 sec/batch; 46h:58m:10s remains)
INFO - root - 2017-12-11 07:45:40.816298: step 34700, loss = 0.69, batch loss = 0.63 (14.6 examples/sec; 0.548 sec/batch; 45h:19m:04s remains)
2017-12-11 07:45:41.398125: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.04223863 -0.043611586 -0.045329761 -0.047331925 -0.049124505 -0.050719544 -0.051545307 -0.051360019 -0.050446451 -0.04918113 -0.047606494 -0.046078522 -0.04489854 -0.044070177 -0.043002937][-0.041208241 -0.04017891 -0.040007379 -0.041168131 -0.043448739 -0.046283014 -0.048319079 -0.04902032 -0.048887666 -0.048529528 -0.047648441 -0.047082007 -0.047237832 -0.047758363 -0.047204889][-0.027519006 -0.019422775 -0.012713841 -0.0092769675 -0.0099729588 -0.013175751 -0.016487023 -0.019027587 -0.021805042 -0.025243811 -0.02833757 -0.032175064 -0.037037004 -0.0420935 -0.044536591][0.0029558088 0.025092127 0.045539625 0.059695911 0.064709969 0.063557327 0.059615664 0.053721726 0.044262122 0.032115337 0.019488024 0.0056211837 -0.0095413607 -0.024034586 -0.032988098][0.049646169 0.092477165 0.13343269 0.16378456 0.1785595 0.18247068 0.17931482 0.16890827 0.14872107 0.12207912 0.093439706 0.063535251 0.032838877 0.0049699214 -0.013036069][0.10068075 0.16597833 0.22920154 0.27691534 0.30264816 0.31301424 0.31161341 0.29623121 0.2635695 0.22007984 0.17288236 0.12497779 0.077907227 0.037192918 0.011174584][0.13893919 0.21992151 0.29843953 0.35741678 0.39019424 0.40472811 0.40403891 0.38409922 0.34188786 0.2862522 0.22610617 0.16619761 0.1092712 0.062542751 0.034616526][0.15441602 0.23692876 0.31577182 0.37318903 0.40409598 0.41701505 0.41412508 0.39100543 0.34615725 0.28922871 0.22878993 0.16978833 0.11588632 0.074719585 0.05353589][0.14973368 0.21813326 0.28084761 0.3231858 0.342964 0.34830514 0.34070987 0.31705409 0.27802595 0.23218542 0.18552367 0.14139692 0.10363556 0.078570217 0.07083302][0.13800412 0.18281482 0.21951811 0.23942773 0.24367647 0.23919174 0.22707279 0.20652793 0.1798296 0.15241408 0.12665756 0.10414957 0.088665619 0.083827585 0.090745136][0.13240227 0.15424004 0.16584346 0.16527244 0.1565149 0.14502652 0.13154045 0.11718781 0.10425045 0.09404505 0.085786924 0.080665573 0.083219364 0.094133817 0.11097227][0.13315403 0.14004673 0.13607861 0.12472844 0.11117607 0.099141546 0.08865042 0.0816028 0.078910269 0.078133635 0.077001989 0.078265183 0.088055246 0.10473909 0.12335363][0.13185991 0.13327028 0.12482629 0.11309174 0.10278854 0.095837496 0.09136159 0.090891212 0.093372568 0.09385372 0.090336353 0.088157013 0.095047779 0.10821308 0.12147996][0.13151456 0.13334246 0.12639068 0.11853294 0.11373455 0.11263034 0.11353925 0.11683367 0.11955771 0.11608477 0.10600605 0.096710391 0.096405677 0.10189521 0.10713251][0.14886399 0.15232781 0.14628862 0.13902846 0.13457654 0.13381293 0.13492072 0.13680863 0.13525663 0.12554458 0.1093935 0.094644658 0.088441648 0.087302387 0.085993566]]...]
INFO - root - 2017-12-11 07:45:46.840824: step 34710, loss = 0.70, batch loss = 0.64 (14.6 examples/sec; 0.548 sec/batch; 45h:19m:18s remains)
INFO - root - 2017-12-11 07:45:52.372732: step 34720, loss = 0.70, batch loss = 0.64 (14.5 examples/sec; 0.551 sec/batch; 45h:35m:32s remains)
INFO - root - 2017-12-11 07:45:57.913738: step 34730, loss = 0.69, batch loss = 0.64 (14.4 examples/sec; 0.557 sec/batch; 46h:06m:37s remains)
INFO - root - 2017-12-11 07:46:03.049510: step 34740, loss = 0.68, batch loss = 0.62 (14.7 examples/sec; 0.543 sec/batch; 44h:55m:40s remains)
INFO - root - 2017-12-11 07:46:08.588557: step 34750, loss = 0.70, batch loss = 0.64 (14.4 examples/sec; 0.556 sec/batch; 45h:57m:22s remains)
INFO - root - 2017-12-11 07:46:14.122320: step 34760, loss = 0.70, batch loss = 0.64 (14.2 examples/sec; 0.564 sec/batch; 46h:36m:18s remains)
INFO - root - 2017-12-11 07:46:19.597041: step 34770, loss = 0.69, batch loss = 0.63 (14.5 examples/sec; 0.552 sec/batch; 45h:39m:34s remains)
INFO - root - 2017-12-11 07:46:25.098933: step 34780, loss = 0.70, batch loss = 0.64 (14.9 examples/sec; 0.536 sec/batch; 44h:18m:07s remains)
INFO - root - 2017-12-11 07:46:30.756000: step 34790, loss = 0.69, batch loss = 0.64 (14.3 examples/sec; 0.559 sec/batch; 46h:14m:00s remains)
INFO - root - 2017-12-11 07:46:36.179980: step 34800, loss = 0.71, batch loss = 0.65 (15.4 examples/sec; 0.520 sec/batch; 42h:59m:54s remains)
2017-12-11 07:46:36.743874: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.070004933 0.074972928 0.090684824 0.10563388 0.11024629 0.099626347 0.075840838 0.050524846 0.036617052 0.038778305 0.059648648 0.092900917 0.12731245 0.15071248 0.1593056][0.0257138 0.025111455 0.036825709 0.047719818 0.049899485 0.04161422 0.028853631 0.023778936 0.036594268 0.066665545 0.11091714 0.16048828 0.20251416 0.22542918 0.22956279][-0.00069293217 0.0042976532 0.019830327 0.032571722 0.038487297 0.040114213 0.045931712 0.065543093 0.10201778 0.14867887 0.19797075 0.24220408 0.27202967 0.28023773 0.27091175][-0.0004724045 0.02117838 0.052235238 0.07899192 0.10212378 0.12690391 0.15997745 0.20336975 0.24971667 0.28766164 0.3101584 0.31782123 0.31140497 0.29127821 0.263322][0.016463989 0.060122523 0.11535233 0.16811246 0.22126225 0.27844721 0.33998519 0.39751655 0.433788 0.43746752 0.40854234 0.36004665 0.3056213 0.25341803 0.20887342][0.03317862 0.095788784 0.17698917 0.26119569 0.34802949 0.4349955 0.51417154 0.56854337 0.57668459 0.53399414 0.45051071 0.34972981 0.25550246 0.18025951 0.12678538][0.038908985 0.10814275 0.20663641 0.31649834 0.42953232 0.53471488 0.61669648 0.65391678 0.62693274 0.54171622 0.417772 0.28493685 0.17181426 0.09132465 0.042292245][0.03405948 0.093193106 0.19185941 0.31194142 0.4356927 0.54349118 0.615185 0.62906206 0.571718 0.45921507 0.31812134 0.18001872 0.072946809 0.0075840307 -0.022110796][0.016164459 0.054474369 0.14138225 0.25909224 0.38086855 0.48040694 0.53619695 0.52919167 0.45261931 0.32979909 0.19177988 0.068958461 -0.014437977 -0.052418284 -0.055846509][-0.009583436 0.0092437631 0.082503669 0.19239332 0.30642664 0.39487928 0.43695053 0.41649055 0.33128566 0.20838869 0.081835493 -0.018791825 -0.074164107 -0.084842913 -0.065813579][-0.022091089 -0.013591443 0.050196595 0.1521772 0.25876454 0.33953562 0.37395683 0.34706226 0.25824916 0.13578376 0.016494969 -0.0686127 -0.10392147 -0.096427269 -0.064220749][-0.0077370913 0.0022076324 0.065080017 0.16450889 0.2687504 0.3470895 0.377833 0.34623131 0.25192991 0.12398352 0.0020589447 -0.07971485 -0.10824803 -0.09567941 -0.061847139][0.028376192 0.052538194 0.12717532 0.23486204 0.34499812 0.42543069 0.4524042 0.41214657 0.30661 0.16692755 0.034751274 -0.054126862 -0.0888239 -0.0842924 -0.060580075][0.075342931 0.12532826 0.22366682 0.34832427 0.46787912 0.54887855 0.56705642 0.51250809 0.39284536 0.24196559 0.10027732 0.0012008438 -0.046451312 -0.057634488 -0.05126597][0.12285743 0.20229426 0.32600448 0.46583495 0.58942652 0.66445053 0.66915321 0.60004628 0.47110221 0.31733841 0.17380027 0.069092616 0.01063086 -0.01537233 -0.026734438]]...]
INFO - root - 2017-12-11 07:46:42.379939: step 34810, loss = 0.69, batch loss = 0.63 (14.5 examples/sec; 0.551 sec/batch; 45h:34m:19s remains)
INFO - root - 2017-12-11 07:46:47.882433: step 34820, loss = 0.72, batch loss = 0.67 (14.9 examples/sec; 0.537 sec/batch; 44h:23m:48s remains)
INFO - root - 2017-12-11 07:46:53.404997: step 34830, loss = 0.71, batch loss = 0.65 (15.0 examples/sec; 0.534 sec/batch; 44h:11m:23s remains)
INFO - root - 2017-12-11 07:46:58.561326: step 34840, loss = 0.68, batch loss = 0.62 (14.6 examples/sec; 0.548 sec/batch; 45h:18m:31s remains)
INFO - root - 2017-12-11 07:47:04.088240: step 34850, loss = 0.71, batch loss = 0.65 (14.7 examples/sec; 0.544 sec/batch; 45h:01m:04s remains)
INFO - root - 2017-12-11 07:47:09.608257: step 34860, loss = 0.68, batch loss = 0.63 (14.2 examples/sec; 0.562 sec/batch; 46h:27m:33s remains)
INFO - root - 2017-12-11 07:47:15.105648: step 34870, loss = 0.68, batch loss = 0.62 (14.1 examples/sec; 0.567 sec/batch; 46h:50m:43s remains)
INFO - root - 2017-12-11 07:47:20.643348: step 34880, loss = 0.70, batch loss = 0.64 (14.7 examples/sec; 0.546 sec/batch; 45h:06m:24s remains)
INFO - root - 2017-12-11 07:47:26.138569: step 34890, loss = 0.69, batch loss = 0.63 (14.4 examples/sec; 0.556 sec/batch; 45h:56m:01s remains)
INFO - root - 2017-12-11 07:47:31.609932: step 34900, loss = 0.70, batch loss = 0.65 (14.6 examples/sec; 0.546 sec/batch; 45h:09m:53s remains)
2017-12-11 07:47:32.196743: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.29951009 0.34538245 0.34909627 0.30959561 0.24352981 0.17697856 0.13672878 0.13973 0.18011533 0.23052697 0.25853333 0.24335282 0.19419281 0.13898712 0.1008497][0.3054758 0.35382849 0.36295721 0.32966167 0.26666915 0.19840671 0.15534474 0.15997405 0.20892163 0.27002174 0.30543369 0.28961214 0.23191343 0.16413651 0.11525062][0.29002309 0.336117 0.35148472 0.33031306 0.27921253 0.21680321 0.17350584 0.1761135 0.22405717 0.2841647 0.31773841 0.29901451 0.2365554 0.16281739 0.10972843][0.2694571 0.31236917 0.33622846 0.33182961 0.29784149 0.24483351 0.20054971 0.1944861 0.22960609 0.27647737 0.30046809 0.27931684 0.2193775 0.14942385 0.10033859][0.25346956 0.29514435 0.329597 0.34316403 0.32620087 0.28188953 0.23433229 0.21381114 0.22738333 0.25368392 0.26600587 0.24715137 0.19956854 0.14396192 0.1052882][0.24053785 0.28479621 0.33181939 0.36215246 0.3590124 0.32121807 0.2704553 0.23594527 0.22764622 0.233749 0.23652546 0.22431718 0.19473024 0.1584691 0.13196017][0.22152296 0.27140346 0.33103597 0.37516928 0.38188809 0.34868765 0.29701954 0.25452715 0.23221666 0.22591679 0.22476465 0.22169553 0.21007915 0.19159545 0.17433402][0.19877437 0.25457174 0.32301593 0.37536758 0.38748556 0.3576681 0.30779254 0.26413488 0.23721427 0.22791581 0.22901276 0.23558725 0.23772322 0.23047788 0.21615584][0.18577924 0.24375895 0.31238383 0.3634055 0.37433404 0.34586447 0.29978785 0.26001626 0.23592995 0.23095448 0.23894331 0.2541585 0.26487947 0.26254946 0.24646948][0.19444583 0.24834013 0.30645719 0.34551179 0.34817222 0.31809476 0.27583808 0.24093446 0.22073399 0.21976121 0.23273872 0.25248912 0.26734993 0.26823714 0.2537452][0.22254664 0.26590338 0.30515632 0.32564589 0.31721833 0.28530717 0.24691099 0.21483247 0.1938546 0.1897267 0.19978648 0.21733183 0.2326688 0.23849081 0.23337224][0.25917742 0.28892511 0.30631384 0.30673054 0.28783166 0.25539726 0.22089621 0.18878196 0.16068164 0.14485767 0.14396797 0.15445475 0.16974913 0.18437818 0.19570792][0.2870577 0.30679747 0.30761409 0.29297188 0.26722518 0.23592585 0.20509957 0.17114051 0.1324084 0.10060089 0.08545576 0.0882048 0.10499529 0.13036305 0.15878232][0.29012045 0.30519894 0.29889971 0.278791 0.25283387 0.22574684 0.1995308 0.16423438 0.11594135 0.069715314 0.042127877 0.039559934 0.060244959 0.097004965 0.13981292][0.27076578 0.282099 0.27633014 0.2608569 0.2426054 0.22418641 0.20423503 0.1686233 0.11277471 0.055234682 0.018137 0.012576207 0.038205791 0.08577355 0.14024067]]...]
INFO - root - 2017-12-11 07:47:37.659301: step 34910, loss = 0.68, batch loss = 0.63 (14.9 examples/sec; 0.536 sec/batch; 44h:17m:32s remains)
INFO - root - 2017-12-11 07:47:43.181642: step 34920, loss = 0.71, batch loss = 0.66 (14.5 examples/sec; 0.552 sec/batch; 45h:39m:14s remains)
INFO - root - 2017-12-11 07:47:48.476070: step 34930, loss = 0.70, batch loss = 0.64 (27.4 examples/sec; 0.292 sec/batch; 24h:07m:41s remains)
INFO - root - 2017-12-11 07:47:53.914639: step 34940, loss = 0.71, batch loss = 0.65 (14.1 examples/sec; 0.566 sec/batch; 46h:48m:10s remains)
INFO - root - 2017-12-11 07:47:59.423373: step 34950, loss = 0.72, batch loss = 0.66 (13.9 examples/sec; 0.575 sec/batch; 47h:33m:52s remains)
INFO - root - 2017-12-11 07:48:04.888804: step 34960, loss = 0.68, batch loss = 0.63 (14.6 examples/sec; 0.549 sec/batch; 45h:24m:36s remains)
INFO - root - 2017-12-11 07:48:10.462173: step 34970, loss = 0.71, batch loss = 0.66 (14.9 examples/sec; 0.538 sec/batch; 44h:30m:16s remains)
INFO - root - 2017-12-11 07:48:15.962670: step 34980, loss = 0.69, batch loss = 0.64 (14.8 examples/sec; 0.540 sec/batch; 44h:35m:28s remains)
INFO - root - 2017-12-11 07:48:21.420054: step 34990, loss = 0.71, batch loss = 0.65 (14.4 examples/sec; 0.555 sec/batch; 45h:53m:16s remains)
INFO - root - 2017-12-11 07:48:27.042359: step 35000, loss = 0.71, batch loss = 0.65 (13.5 examples/sec; 0.593 sec/batch; 49h:02m:27s remains)
2017-12-11 07:48:27.662013: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.59269387 0.54747742 0.47890472 0.42635152 0.41774032 0.4603436 0.52010536 0.55748612 0.55986392 0.57403564 0.61084557 0.62650138 0.58910042 0.50187618 0.39154354][0.70853186 0.64533281 0.55808461 0.49803281 0.49620765 0.548751 0.61467034 0.65124589 0.645112 0.644253 0.66752696 0.67318 0.62714612 0.52723086 0.4026][0.81511492 0.73830849 0.64098758 0.58182627 0.59133303 0.64977032 0.71361542 0.74183106 0.72462833 0.70835853 0.71833467 0.71842557 0.66973597 0.5620662 0.42606878][0.90794784 0.82963288 0.73523569 0.686408 0.71025419 0.77209079 0.83124208 0.85137624 0.82787877 0.80479383 0.81042415 0.81180447 0.76229995 0.64555955 0.49402705][0.9689759 0.90419823 0.82547504 0.79357582 0.83187288 0.89724392 0.95565528 0.97551727 0.95439959 0.93327016 0.93943256 0.94188613 0.88742167 0.75719005 0.58505595][0.96907377 0.92970073 0.876667 0.86534816 0.9169361 0.98890382 1.0539765 1.0827737 1.0709194 1.0560169 1.0620576 1.0605377 0.99639678 0.85120082 0.66003329][0.89389038 0.87861186 0.84829825 0.85173959 0.910887 0.99108356 1.0690922 1.1149518 1.119428 1.1156484 1.1232637 1.1148812 1.0400225 0.88372719 0.6817956][0.7715829 0.76626325 0.74204022 0.74428862 0.79878664 0.88229114 0.97140807 1.0346577 1.0586002 1.0721511 1.0902115 1.0819331 1.0040257 0.84669048 0.64562869][0.64469159 0.63219237 0.59625888 0.58289224 0.62195569 0.699376 0.78811717 0.85774904 0.89522487 0.927169 0.96360785 0.96579421 0.89659011 0.7518779 0.56503934][0.54569221 0.51482362 0.45915487 0.42493746 0.44269091 0.50276715 0.57547438 0.63568634 0.67644411 0.72406644 0.78251857 0.80370742 0.75373197 0.6312719 0.46764693][0.46966404 0.42053682 0.34879413 0.29794294 0.29416907 0.32915887 0.37513059 0.41519269 0.45083857 0.50898182 0.58712244 0.629495 0.60151052 0.50389105 0.36687177][0.38282594 0.32436696 0.24835353 0.19186333 0.17285475 0.18328907 0.20156856 0.21993119 0.24671926 0.3073726 0.39320445 0.44697481 0.43427876 0.35847226 0.24844758][0.25980467 0.20559537 0.1406879 0.091183759 0.066717528 0.060669322 0.058500119 0.059196915 0.074112311 0.12522393 0.20070535 0.25022683 0.24336739 0.18564126 0.10338792][0.1111476 0.070859283 0.026338967 -0.0084129972 -0.029655129 -0.041928444 -0.053777795 -0.063148633 -0.059814997 -0.027046679 0.024252448 0.057328992 0.050047241 0.0086436318 -0.045629632][-0.029050309 -0.053499941 -0.077617772 -0.097369589 -0.11179232 -0.123189 -0.13559681 -0.14679027 -0.15012315 -0.13506803 -0.10909244 -0.09415067 -0.10299503 -0.12998813 -0.16031198]]...]
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-SGD-lr0.001-clip50-initfromfixqian30k-fixqian
INFO - root - 2017-12-11 07:48:33.286274: step 35010, loss = 0.71, batch loss = 0.65 (14.2 examples/sec; 0.564 sec/batch; 46h:36m:23s remains)
INFO - root - 2017-12-11 07:48:38.766578: step 35020, loss = 0.70, batch loss = 0.64 (14.6 examples/sec; 0.549 sec/batch; 45h:20m:01s remains)
INFO - root - 2017-12-11 07:48:43.944769: step 35030, loss = 0.70, batch loss = 0.64 (14.4 examples/sec; 0.554 sec/batch; 45h:48m:13s remains)
INFO - root - 2017-12-11 07:48:49.403738: step 35040, loss = 0.70, batch loss = 0.64 (14.3 examples/sec; 0.558 sec/batch; 46h:07m:01s remains)
INFO - root - 2017-12-11 07:48:54.847744: step 35050, loss = 0.70, batch loss = 0.64 (14.8 examples/sec; 0.541 sec/batch; 44h:40m:44s remains)
INFO - root - 2017-12-11 07:49:00.298702: step 35060, loss = 0.72, batch loss = 0.66 (14.6 examples/sec; 0.550 sec/batch; 45h:24m:19s remains)
INFO - root - 2017-12-11 07:49:05.817026: step 35070, loss = 0.71, batch loss = 0.65 (14.4 examples/sec; 0.556 sec/batch; 45h:57m:51s remains)
INFO - root - 2017-12-11 07:49:11.364603: step 35080, loss = 0.69, batch loss = 0.63 (14.7 examples/sec; 0.542 sec/batch; 44h:48m:48s remains)
INFO - root - 2017-12-11 07:49:16.843518: step 35090, loss = 0.69, batch loss = 0.63 (14.4 examples/sec; 0.555 sec/batch; 45h:50m:31s remains)
INFO - root - 2017-12-11 07:49:22.267354: step 35100, loss = 0.70, batch loss = 0.64 (14.7 examples/sec; 0.546 sec/batch; 45h:05m:08s remains)
2017-12-11 07:49:22.875433: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.094429031 0.090675756 0.087451234 0.085284308 0.080412738 0.07928919 0.084153265 0.094806805 0.10855263 0.12076925 0.12632585 0.11233854 0.078805342 0.034465443 -0.0085568046][0.096028954 0.0886411 0.079790391 0.0714142 0.061315477 0.05587808 0.057786509 0.067022249 0.080953844 0.094375379 0.10239754 0.091773488 0.061164781 0.019557679 -0.02014266][0.10165399 0.093285725 0.081278235 0.06926617 0.0565651 0.049126636 0.049295109 0.055907402 0.066578649 0.076906361 0.08252427 0.070974708 0.040998798 0.002291441 -0.032458145][0.1238228 0.11873385 0.1084239 0.097567886 0.086654685 0.081442475 0.083561115 0.089263484 0.095271841 0.097863168 0.094250225 0.073369861 0.035517372 -0.0070797619 -0.041421771][0.15900652 0.16284712 0.16045679 0.15679747 0.1526219 0.15316859 0.15964778 0.16558711 0.1666778 0.15948179 0.14270884 0.10647286 0.053303923 -0.0008848267 -0.041579675][0.20271985 0.2214338 0.23359895 0.24197634 0.24663462 0.25264296 0.26150203 0.26487765 0.25830233 0.23899436 0.20691188 0.1529296 0.0816542 0.012552639 -0.0380943][0.24733511 0.28310174 0.31243449 0.33413589 0.346975 0.35630447 0.3648572 0.36292106 0.34611806 0.3130708 0.26485 0.19360563 0.10557871 0.022768293 -0.036951344][0.27388394 0.32193878 0.36426595 0.39575106 0.41380692 0.42416969 0.43144351 0.42477986 0.40012124 0.35715646 0.2978301 0.21583912 0.11783526 0.026777864 -0.038559794][0.26745003 0.31910971 0.36695027 0.40264595 0.42226642 0.4311817 0.43571821 0.42541862 0.39736408 0.35144529 0.29007423 0.20814891 0.11124567 0.021105599 -0.043696649][0.22482051 0.26981696 0.31415713 0.34787855 0.36581984 0.37185389 0.37341526 0.36216286 0.33692321 0.29697537 0.24364106 0.17223668 0.086602546 0.00564444 -0.053043026][0.16576254 0.19604768 0.22923872 0.25612307 0.27071479 0.27498305 0.27703577 0.27093127 0.25561857 0.22802125 0.18728524 0.12890704 0.055816203 -0.014783047 -0.065803424][0.11199521 0.12380436 0.14169376 0.15932441 0.17112166 0.17734978 0.18536462 0.19009712 0.18849777 0.17427771 0.14428084 0.094789885 0.029844796 -0.033204388 -0.07760486][0.072689138 0.070308447 0.076409005 0.088048406 0.10093811 0.11433128 0.13317898 0.15131028 0.1631235 0.15952574 0.13544889 0.087961867 0.023778198 -0.037769478 -0.080041155][0.0533052 0.045220036 0.047480259 0.059988763 0.079758167 0.10546236 0.13823585 0.16896299 0.18953443 0.18945952 0.16299632 0.10851329 0.036656767 -0.029820329 -0.074380353][0.053235244 0.045909371 0.051373098 0.07206089 0.10485908 0.1475134 0.19632085 0.23781654 0.26123703 0.2563203 0.21877919 0.14931376 0.062909938 -0.013891791 -0.064319037]]...]
INFO - root - 2017-12-11 07:49:28.372974: step 35110, loss = 0.70, batch loss = 0.64 (14.5 examples/sec; 0.553 sec/batch; 45h:39m:14s remains)
INFO - root - 2017-12-11 07:49:33.804833: step 35120, loss = 0.70, batch loss = 0.64 (15.0 examples/sec; 0.533 sec/batch; 44h:02m:58s remains)
INFO - root - 2017-12-11 07:49:38.883086: step 35130, loss = 0.70, batch loss = 0.64 (14.7 examples/sec; 0.543 sec/batch; 44h:53m:16s remains)
INFO - root - 2017-12-11 07:49:44.385928: step 35140, loss = 0.70, batch loss = 0.64 (14.1 examples/sec; 0.566 sec/batch; 46h:47m:10s remains)
INFO - root - 2017-12-11 07:49:49.910577: step 35150, loss = 0.71, batch loss = 0.65 (14.4 examples/sec; 0.557 sec/batch; 46h:00m:31s remains)
INFO - root - 2017-12-11 07:49:55.443735: step 35160, loss = 0.70, batch loss = 0.64 (14.4 examples/sec; 0.555 sec/batch; 45h:52m:39s remains)
INFO - root - 2017-12-11 07:50:00.941830: step 35170, loss = 0.67, batch loss = 0.62 (14.5 examples/sec; 0.552 sec/batch; 45h:34m:00s remains)
INFO - root - 2017-12-11 07:50:06.451322: step 35180, loss = 0.71, batch loss = 0.65 (14.4 examples/sec; 0.557 sec/batch; 45h:59m:39s remains)
INFO - root - 2017-12-11 07:50:11.979629: step 35190, loss = 0.69, batch loss = 0.63 (14.4 examples/sec; 0.556 sec/batch; 45h:52m:58s remains)
INFO - root - 2017-12-11 07:50:17.555812: step 35200, loss = 0.71, batch loss = 0.65 (14.3 examples/sec; 0.560 sec/batch; 46h:14m:12s remains)
2017-12-11 07:50:18.138702: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.1214345 0.10619994 0.082260221 0.059578478 0.041116152 0.033546928 0.041922465 0.059819162 0.079694 0.10480399 0.13504495 0.16159782 0.1807469 0.19706702 0.21211421][0.10849655 0.09116444 0.0673322 0.047220767 0.033458311 0.03200594 0.046410494 0.068617888 0.090782844 0.11841089 0.15532669 0.19161864 0.22184861 0.24656826 0.26796749][0.092546217 0.074540056 0.051839251 0.034638319 0.025820825 0.03064383 0.050913822 0.077718951 0.10293953 0.13375771 0.17705908 0.22378904 0.26626915 0.29944992 0.32557309][0.079702809 0.062578425 0.043101083 0.030493218 0.027542062 0.039021868 0.064657696 0.095219858 0.12232993 0.15413514 0.20017974 0.25228438 0.30103123 0.33730972 0.36328492][0.076562114 0.064021312 0.051371213 0.046363726 0.051330425 0.070478454 0.10104933 0.13359314 0.15998316 0.18925788 0.2324724 0.28214461 0.32819995 0.359767 0.37947738][0.086622328 0.084038541 0.083607063 0.091092959 0.10761954 0.13627836 0.17166698 0.20357105 0.22471318 0.24534309 0.27696857 0.31326705 0.34499952 0.36207628 0.36847389][0.1070748 0.11750855 0.13312575 0.15636706 0.18591739 0.22300594 0.26001045 0.2864545 0.29656494 0.30215567 0.31452042 0.32938465 0.33978945 0.33851284 0.33011886][0.13079184 0.15531953 0.18757737 0.22598918 0.26567793 0.30533728 0.33611733 0.34906304 0.34104696 0.32600993 0.315151 0.30665839 0.29608402 0.27916291 0.25965625][0.14711544 0.18163209 0.22507411 0.2723484 0.31483388 0.34881818 0.36597753 0.36036235 0.33250415 0.29771662 0.26718292 0.24053398 0.2153697 0.18974912 0.16564012][0.14813544 0.18313438 0.22557469 0.26961604 0.3051371 0.3277522 0.33110109 0.31219336 0.2739515 0.23083661 0.19277352 0.160178 0.1319903 0.10716606 0.08583618][0.13161109 0.1603134 0.19280748 0.22536269 0.24857166 0.258931 0.25274169 0.22957225 0.19280998 0.15384081 0.12038854 0.093183309 0.07198412 0.055969771 0.043841396][0.11083413 0.1309655 0.15079215 0.16947077 0.17949873 0.17887537 0.16686201 0.14553203 0.11779959 0.091068044 0.07082554 0.057771903 0.05165733 0.050823048 0.052125081][0.10307507 0.11614486 0.1243406 0.13028388 0.12921745 0.12111776 0.10779536 0.092681751 0.077435367 0.065098561 0.059345327 0.061220117 0.070042849 0.082727268 0.094383031][0.11576771 0.12612383 0.12702151 0.12432878 0.11591047 0.10404442 0.092805609 0.085873574 0.082942888 0.082580909 0.08691439 0.097572774 0.11341045 0.1303806 0.14321974][0.14315718 0.15593104 0.1544171 0.1476392 0.13503404 0.12083583 0.11099494 0.10885495 0.11233765 0.11695963 0.12443875 0.13718171 0.1534531 0.16846594 0.17696382]]...]
INFO - root - 2017-12-11 07:50:23.679625: step 35210, loss = 0.70, batch loss = 0.64 (14.8 examples/sec; 0.540 sec/batch; 44h:33m:40s remains)
INFO - root - 2017-12-11 07:50:29.159461: step 35220, loss = 0.70, batch loss = 0.64 (15.0 examples/sec; 0.535 sec/batch; 44h:10m:24s remains)
INFO - root - 2017-12-11 07:50:34.360404: step 35230, loss = 0.69, batch loss = 0.63 (14.6 examples/sec; 0.547 sec/batch; 45h:11m:15s remains)
INFO - root - 2017-12-11 07:50:39.896104: step 35240, loss = 0.68, batch loss = 0.62 (14.6 examples/sec; 0.548 sec/batch; 45h:14m:31s remains)
INFO - root - 2017-12-11 07:50:45.386384: step 35250, loss = 0.69, batch loss = 0.63 (14.4 examples/sec; 0.555 sec/batch; 45h:52m:01s remains)
INFO - root - 2017-12-11 07:50:50.911466: step 35260, loss = 0.69, batch loss = 0.63 (14.5 examples/sec; 0.553 sec/batch; 45h:41m:48s remains)
INFO - root - 2017-12-11 07:50:56.452349: step 35270, loss = 0.71, batch loss = 0.65 (14.5 examples/sec; 0.553 sec/batch; 45h:41m:37s remains)
INFO - root - 2017-12-11 07:51:01.857786: step 35280, loss = 0.70, batch loss = 0.64 (15.1 examples/sec; 0.529 sec/batch; 43h:39m:54s remains)
INFO - root - 2017-12-11 07:51:07.295638: step 35290, loss = 0.71, batch loss = 0.65 (14.4 examples/sec; 0.554 sec/batch; 45h:44m:44s remains)
INFO - root - 2017-12-11 07:51:12.760981: step 35300, loss = 0.70, batch loss = 0.64 (14.5 examples/sec; 0.551 sec/batch; 45h:27m:11s remains)
2017-12-11 07:51:13.336143: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.14025536 0.12840351 0.13449717 0.15072969 0.16851456 0.18566972 0.2070839 0.23768555 0.27250642 0.29920307 0.29729623 0.25508022 0.17510119 0.072336644 -0.023005769][0.16473079 0.15229224 0.15708643 0.17264605 0.19031495 0.20841138 0.23522443 0.27526325 0.31891719 0.34970918 0.34517708 0.294638 0.20255631 0.086927176 -0.01811062][0.19007368 0.18188304 0.18958519 0.20802328 0.22785003 0.24680741 0.27744502 0.32316563 0.371631 0.40304893 0.39367747 0.33383259 0.22957109 0.10178034 -0.012314912][0.21834034 0.22099882 0.23914592 0.26754549 0.29453394 0.31573668 0.34707874 0.39191115 0.43737063 0.46251664 0.44473726 0.37596682 0.26170331 0.1237651 0.0013746644][0.24390273 0.26297966 0.2976158 0.34081808 0.37776816 0.40149915 0.42936903 0.465895 0.49939686 0.511344 0.48213172 0.40556934 0.28538719 0.14238404 0.015714012][0.26258412 0.29760182 0.34870687 0.40548903 0.44952333 0.47182932 0.49046612 0.5115515 0.52607006 0.52040154 0.47903183 0.39749733 0.27727944 0.13726154 0.014394799][0.26710981 0.30874789 0.36754116 0.43108937 0.4771252 0.49464455 0.50125831 0.50479567 0.49964926 0.47733563 0.42701888 0.34561318 0.23248079 0.10381616 -0.0067846072][0.26141426 0.2923992 0.34261256 0.40016705 0.43985683 0.44854742 0.44198561 0.42952093 0.4090887 0.37652412 0.32499197 0.25214741 0.15498948 0.046651293 -0.043750275][0.24721155 0.25226128 0.27746761 0.31572559 0.34157088 0.34029126 0.32355282 0.30143911 0.27412045 0.24008307 0.19533908 0.13767645 0.062282465 -0.020962747 -0.087835729][0.22928883 0.19669093 0.18546434 0.195271 0.20415039 0.19705206 0.17944382 0.15884589 0.13469133 0.10603472 0.0709816 0.028102517 -0.027183767 -0.087189257 -0.13214014][0.20723754 0.14101264 0.095437057 0.078812189 0.0746365 0.068814233 0.060276553 0.051555369 0.0388857 0.019843681 -0.0059840884 -0.0386523 -0.081181638 -0.12645665 -0.15773281][0.174432 0.091596484 0.027901353 -0.00209997 -0.00957446 -0.0070603872 -0.00069178012 0.0075347391 0.01091905 0.0044984468 -0.012137979 -0.038527451 -0.076011114 -0.117195 -0.14681791][0.12478682 0.046425559 -0.013996935 -0.041627061 -0.043875147 -0.031308662 -0.012006897 0.010155921 0.027480314 0.033334892 0.026086761 0.0052205487 -0.030316114 -0.073197216 -0.10874148][0.062932976 0.0045181965 -0.038144216 -0.0542833 -0.048943341 -0.03006603 -0.00461233 0.024030827 0.049399342 0.064656876 0.066450037 0.052318215 0.020221094 -0.02367978 -0.065441579][-0.0018433305 -0.035372961 -0.056348272 -0.0599152 -0.049448576 -0.029709287 -0.0044806376 0.024642752 0.053580362 0.076440327 0.088185579 0.083855994 0.059023138 0.017576808 -0.027488125]]...]
INFO - root - 2017-12-11 07:51:18.827286: step 35310, loss = 0.69, batch loss = 0.63 (14.6 examples/sec; 0.547 sec/batch; 45h:10m:29s remains)
INFO - root - 2017-12-11 07:51:24.006017: step 35320, loss = 0.71, batch loss = 0.65 (25.6 examples/sec; 0.312 sec/batch; 25h:46m:24s remains)
INFO - root - 2017-12-11 07:51:29.483558: step 35330, loss = 0.67, batch loss = 0.61 (14.0 examples/sec; 0.570 sec/batch; 47h:05m:33s remains)
INFO - root - 2017-12-11 07:51:34.929606: step 35340, loss = 0.69, batch loss = 0.64 (14.9 examples/sec; 0.537 sec/batch; 44h:19m:22s remains)
INFO - root - 2017-12-11 07:51:40.330454: step 35350, loss = 0.69, batch loss = 0.63 (15.0 examples/sec; 0.533 sec/batch; 43h:59m:56s remains)
INFO - root - 2017-12-11 07:51:45.878913: step 35360, loss = 0.70, batch loss = 0.64 (14.7 examples/sec; 0.546 sec/batch; 45h:03m:02s remains)
INFO - root - 2017-12-11 07:51:51.432198: step 35370, loss = 0.67, batch loss = 0.61 (14.6 examples/sec; 0.549 sec/batch; 45h:17m:21s remains)
INFO - root - 2017-12-11 07:51:56.882313: step 35380, loss = 0.70, batch loss = 0.64 (14.9 examples/sec; 0.535 sec/batch; 44h:10m:38s remains)
INFO - root - 2017-12-11 07:52:02.454020: step 35390, loss = 0.71, batch loss = 0.65 (14.4 examples/sec; 0.557 sec/batch; 45h:59m:07s remains)
INFO - root - 2017-12-11 07:52:08.008629: step 35400, loss = 0.69, batch loss = 0.63 (14.8 examples/sec; 0.540 sec/batch; 44h:32m:48s remains)
2017-12-11 07:52:08.579151: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.06526608 0.066713385 0.074292243 0.082356356 0.085488908 0.0807727 0.070126109 0.060779959 0.061836366 0.077447727 0.10540282 0.13597411 0.15447941 0.15040006 0.12482535][0.078825668 0.080175251 0.08545804 0.090328895 0.091271654 0.086911559 0.078826755 0.072378196 0.074651293 0.088881835 0.11266726 0.13678902 0.14875072 0.1399179 0.11234523][0.080199666 0.084081575 0.089117721 0.092187047 0.091932513 0.089025274 0.0846239 0.080576345 0.080832444 0.088042848 0.10172609 0.1154925 0.12053933 0.11041965 0.086504221][0.064142682 0.070943817 0.077239186 0.0811297 0.083454683 0.085876666 0.087823339 0.086856656 0.084139071 0.0834294 0.08839412 0.096893869 0.10238784 0.098188423 0.082478814][0.0384135 0.046386484 0.054096211 0.061583377 0.072003543 0.086164564 0.099565335 0.10449661 0.099867716 0.091808818 0.089350305 0.095108651 0.10369798 0.10693654 0.098664112][0.020844094 0.029748915 0.040696442 0.055716813 0.078764759 0.10715079 0.13099952 0.13855201 0.12802415 0.10898805 0.096106589 0.096749239 0.1067842 0.11501223 0.11200263][0.022708401 0.031650208 0.045964509 0.070084356 0.10586185 0.14527756 0.17324705 0.17583668 0.15313868 0.11917736 0.09413927 0.088803746 0.09867955 0.10977661 0.11041268][0.037774011 0.043699265 0.05875361 0.0899984 0.13499133 0.1796916 0.20474173 0.19625093 0.15815781 0.10907087 0.073786795 0.064514577 0.07502313 0.088887744 0.093859285][0.05575512 0.058517963 0.071893089 0.1044259 0.15022987 0.19216059 0.20931026 0.18945836 0.14068055 0.084449291 0.046439324 0.037354603 0.049372539 0.066243321 0.076722711][0.065246128 0.067364596 0.078955665 0.10745189 0.14611766 0.17874005 0.18613236 0.15945478 0.11016297 0.059111111 0.027693214 0.022800367 0.036937188 0.057250585 0.074457012][0.064609833 0.066728584 0.074891604 0.094255164 0.12019841 0.1403264 0.13966395 0.11370686 0.074900776 0.040106412 0.023167221 0.026535571 0.044085085 0.067476325 0.089621507][0.06038402 0.06016301 0.060906906 0.067464866 0.0795081 0.089821234 0.088258885 0.072908841 0.053515948 0.040024705 0.038507707 0.048082307 0.064844944 0.085138641 0.10379523][0.062441777 0.057597719 0.049158797 0.043751489 0.045213204 0.050728858 0.05420344 0.054407142 0.0559408 0.060932986 0.069078647 0.077778533 0.085917622 0.093877643 0.098973043][0.071230166 0.063761353 0.049480863 0.03683757 0.031914044 0.034759775 0.042237658 0.053425822 0.0686885 0.084032394 0.094623744 0.097049057 0.09284281 0.085483521 0.075108148][0.083327353 0.07394743 0.05625359 0.039514083 0.03048118 0.030588 0.038242612 0.052801657 0.072521716 0.090345919 0.099455863 0.096669354 0.085160583 0.069675446 0.052195728]]...]
INFO - root - 2017-12-11 07:52:14.101463: step 35410, loss = 0.72, batch loss = 0.66 (14.5 examples/sec; 0.551 sec/batch; 45h:28m:19s remains)
INFO - root - 2017-12-11 07:52:19.163160: step 35420, loss = 0.68, batch loss = 0.62 (14.8 examples/sec; 0.541 sec/batch; 44h:40m:29s remains)
INFO - root - 2017-12-11 07:52:24.637629: step 35430, loss = 0.71, batch loss = 0.65 (14.7 examples/sec; 0.545 sec/batch; 44h:56m:04s remains)
INFO - root - 2017-12-11 07:52:30.222034: step 35440, loss = 0.71, batch loss = 0.66 (14.6 examples/sec; 0.546 sec/batch; 45h:05m:38s remains)
INFO - root - 2017-12-11 07:52:35.719289: step 35450, loss = 0.69, batch loss = 0.63 (14.6 examples/sec; 0.549 sec/batch; 45h:17m:40s remains)
INFO - root - 2017-12-11 07:52:41.276289: step 35460, loss = 0.70, batch loss = 0.64 (14.7 examples/sec; 0.543 sec/batch; 44h:48m:21s remains)
INFO - root - 2017-12-11 07:52:46.826930: step 35470, loss = 0.70, batch loss = 0.64 (14.6 examples/sec; 0.547 sec/batch; 45h:06m:18s remains)
INFO - root - 2017-12-11 07:52:52.309824: step 35480, loss = 0.70, batch loss = 0.64 (14.2 examples/sec; 0.563 sec/batch; 46h:28m:49s remains)
INFO - root - 2017-12-11 07:52:57.739880: step 35490, loss = 0.71, batch loss = 0.65 (14.6 examples/sec; 0.548 sec/batch; 45h:11m:54s remains)
INFO - root - 2017-12-11 07:53:03.250215: step 35500, loss = 0.68, batch loss = 0.62 (14.6 examples/sec; 0.548 sec/batch; 45h:13m:17s remains)
2017-12-11 07:53:03.822216: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0030194875 0.033147037 0.074534379 0.1155766 0.14903091 0.1710341 0.1744827 0.15827213 0.13007793 0.096577652 0.064477541 0.038745511 0.020237299 0.0046534771 -0.014981629][0.024608888 0.079194933 0.14396912 0.21228717 0.27153972 0.31429076 0.32994473 0.31510389 0.28087997 0.2359817 0.18983689 0.15263203 0.1253237 0.10160394 0.070333466][0.043790057 0.11530015 0.20148309 0.29479104 0.37759957 0.43941417 0.46816555 0.46010837 0.42990381 0.38626942 0.33674943 0.2945841 0.26082918 0.22817296 0.18220015][0.052092485 0.13643067 0.23990223 0.353352 0.45524928 0.534257 0.57915586 0.58687717 0.573589 0.54636782 0.50697726 0.46732154 0.42805135 0.3823303 0.31694245][0.0603872 0.15540595 0.27245626 0.40115732 0.51825613 0.61360306 0.67877454 0.71148509 0.72588092 0.72471982 0.70218974 0.66787636 0.62208 0.55997515 0.47267014][0.071997575 0.17751415 0.30617148 0.44786489 0.57987964 0.6926387 0.77912343 0.83694911 0.87521833 0.89182782 0.87747091 0.84266669 0.78956473 0.715246 0.6141237][0.075380541 0.18439277 0.31678531 0.46433333 0.60734183 0.73452365 0.83801651 0.9143582 0.965648 0.98525822 0.96482611 0.92133754 0.861678 0.78393483 0.68298525][0.06046683 0.16208199 0.28679633 0.42994991 0.57619894 0.7102614 0.82138014 0.90466279 0.95669925 0.96802336 0.93453223 0.88212991 0.82253182 0.75268483 0.66477507][0.033653017 0.12004451 0.22846736 0.35771874 0.49628115 0.62453777 0.72830933 0.80199957 0.841054 0.8369509 0.79075807 0.73510283 0.68367743 0.62977016 0.56193781][-0.00012130738 0.065266691 0.14997025 0.25550178 0.37407169 0.48382241 0.56765544 0.61967081 0.63718241 0.61522573 0.55954319 0.50514781 0.46577474 0.43042445 0.38487205][-0.039688151 0.00016222382 0.0551835 0.12825772 0.21532108 0.29574013 0.35177851 0.37771267 0.37348762 0.3381356 0.28019735 0.23218711 0.20633778 0.18923229 0.16597618][-0.075165026 -0.059542079 -0.033805691 0.0045238771 0.054699954 0.10093601 0.12825234 0.13242878 0.11539208 0.078501984 0.029390987 -0.0062963665 -0.019164547 -0.021785855 -0.026899626][-0.096294321 -0.096728764 -0.091090843 -0.078448385 -0.057861712 -0.039209466 -0.033025164 -0.04139705 -0.061309129 -0.090714648 -0.12480923 -0.14665006 -0.15002115 -0.14462164 -0.13956197][-0.10249705 -0.11071599 -0.11516502 -0.11671952 -0.11414592 -0.11233879 -0.11744695 -0.12985948 -0.14671698 -0.16629231 -0.18633778 -0.19776182 -0.19690958 -0.19008839 -0.18247385][-0.10109094 -0.11190664 -0.12016072 -0.12802541 -0.13419929 -0.1402223 -0.1488497 -0.15977861 -0.17118086 -0.18215315 -0.19206615 -0.19719121 -0.1955246 -0.19018947 -0.18339168]]...]
INFO - root - 2017-12-11 07:53:09.316236: step 35510, loss = 0.71, batch loss = 0.65 (14.7 examples/sec; 0.546 sec/batch; 45h:01m:17s remains)
INFO - root - 2017-12-11 07:53:14.476383: step 35520, loss = 0.70, batch loss = 0.64 (14.6 examples/sec; 0.547 sec/batch; 45h:05m:33s remains)
INFO - root - 2017-12-11 07:53:19.980100: step 35530, loss = 0.69, batch loss = 0.64 (14.1 examples/sec; 0.569 sec/batch; 46h:57m:50s remains)
INFO - root - 2017-12-11 07:53:25.544235: step 35540, loss = 0.70, batch loss = 0.64 (14.4 examples/sec; 0.555 sec/batch; 45h:48m:33s remains)
INFO - root - 2017-12-11 07:53:31.067364: step 35550, loss = 0.71, batch loss = 0.65 (14.9 examples/sec; 0.538 sec/batch; 44h:21m:30s remains)
INFO - root - 2017-12-11 07:53:36.530600: step 35560, loss = 0.68, batch loss = 0.63 (14.4 examples/sec; 0.557 sec/batch; 45h:54m:45s remains)
INFO - root - 2017-12-11 07:53:42.136705: step 35570, loss = 0.71, batch loss = 0.66 (14.6 examples/sec; 0.549 sec/batch; 45h:19m:15s remains)
INFO - root - 2017-12-11 07:53:47.614838: step 35580, loss = 0.68, batch loss = 0.63 (14.7 examples/sec; 0.544 sec/batch; 44h:51m:25s remains)
INFO - root - 2017-12-11 07:53:53.140673: step 35590, loss = 0.70, batch loss = 0.64 (14.9 examples/sec; 0.539 sec/batch; 44h:25m:17s remains)
INFO - root - 2017-12-11 07:53:58.741658: step 35600, loss = 0.70, batch loss = 0.65 (14.8 examples/sec; 0.539 sec/batch; 44h:26m:03s remains)
2017-12-11 07:53:59.349208: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0042500077 0.061373796 0.15770753 0.26666567 0.365048 0.42733228 0.43374154 0.37817958 0.27480078 0.15069298 0.03600223 -0.04527837 -0.090008348 -0.10579038 -0.10144411][0.01120089 0.092114754 0.2092576 0.34030285 0.45856541 0.53530622 0.54602259 0.48110145 0.35776529 0.21032429 0.073273011 -0.025408747 -0.08112704 -0.10301182 -0.10179329][0.022300333 0.11535896 0.24925581 0.39841536 0.53247672 0.62009257 0.63251835 0.55733293 0.41562626 0.2488918 0.095082141 -0.015287049 -0.076266959 -0.099361792 -0.098548405][0.023931101 0.12244906 0.26526123 0.42618111 0.57210785 0.66931474 0.68448806 0.60242456 0.44699594 0.26605737 0.10060272 -0.017052194 -0.07925979 -0.099684939 -0.096457772][0.020843057 0.11981119 0.26553157 0.4326233 0.58638561 0.69097704 0.70946014 0.62443095 0.46127257 0.2723141 0.10073945 -0.020751312 -0.082470655 -0.098822482 -0.091487631][0.016105698 0.11356507 0.2608721 0.43463889 0.598528 0.71309948 0.73819226 0.65381938 0.48560995 0.28950995 0.111885 -0.013737099 -0.076001585 -0.08956667 -0.078618109][0.013151185 0.107664 0.25652364 0.43974081 0.61957353 0.75116789 0.78907943 0.70922434 0.53587043 0.32896987 0.1408896 0.0079869544 -0.056484263 -0.06836313 -0.054665316][0.012078797 0.1006446 0.24584195 0.43307772 0.62586826 0.77505 0.82947993 0.75983489 0.58649272 0.37136036 0.17372975 0.034840938 -0.030038163 -0.039036289 -0.022181841][0.01408635 0.093361758 0.22773768 0.40853646 0.603679 0.76331145 0.83310223 0.77946043 0.616172 0.40235347 0.20201218 0.061938953 -0.00020300294 -0.0041683656 0.017408844][0.020482613 0.090261936 0.21012187 0.3759909 0.56102937 0.71847183 0.79538482 0.75738007 0.61176479 0.41047353 0.21779157 0.083736964 0.027789705 0.030409226 0.05824152][0.02547201 0.084384926 0.18553734 0.32754678 0.48866883 0.62822855 0.69974065 0.67261016 0.5495702 0.37286216 0.20142928 0.084025525 0.039822694 0.051078752 0.086058617][0.020259675 0.063625149 0.13946405 0.24784099 0.3722353 0.48023975 0.53521645 0.51391262 0.4173345 0.27681103 0.14059694 0.050896976 0.024561094 0.046439134 0.088377252][0.00523312 0.029452639 0.0754025 0.14452745 0.22554289 0.29535785 0.32836923 0.30979171 0.24107823 0.1434588 0.051560253 -0.0033260919 -0.0089296782 0.022439744 0.067876071][-0.0094358949 -0.0023407135 0.0169972 0.051083889 0.093327053 0.12904502 0.14233869 0.12570336 0.081771374 0.024100518 -0.025623117 -0.048036095 -0.036495689 -0.00070811465 0.042159081][-0.020614156 -0.024191026 -0.022555282 -0.01275381 0.002204156 0.014066138 0.013961173 -0.00078817946 -0.026606355 -0.054799609 -0.073392369 -0.07299424 -0.051991828 -0.017827807 0.017486496]]...]
INFO - root - 2017-12-11 07:54:04.887312: step 35610, loss = 0.69, batch loss = 0.63 (14.5 examples/sec; 0.553 sec/batch; 45h:36m:36s remains)
INFO - root - 2017-12-11 07:54:10.095057: step 35620, loss = 0.70, batch loss = 0.64 (14.5 examples/sec; 0.550 sec/batch; 45h:21m:48s remains)
INFO - root - 2017-12-11 07:54:15.591158: step 35630, loss = 0.69, batch loss = 0.63 (14.8 examples/sec; 0.539 sec/batch; 44h:27m:39s remains)
INFO - root - 2017-12-11 07:54:21.050107: step 35640, loss = 0.69, batch loss = 0.63 (14.6 examples/sec; 0.548 sec/batch; 45h:10m:15s remains)
INFO - root - 2017-12-11 07:54:26.522001: step 35650, loss = 0.69, batch loss = 0.64 (14.6 examples/sec; 0.549 sec/batch; 45h:16m:11s remains)
INFO - root - 2017-12-11 07:54:32.112261: step 35660, loss = 0.70, batch loss = 0.64 (14.8 examples/sec; 0.541 sec/batch; 44h:34m:58s remains)
INFO - root - 2017-12-11 07:54:37.581536: step 35670, loss = 0.67, batch loss = 0.62 (14.4 examples/sec; 0.554 sec/batch; 45h:39m:40s remains)
INFO - root - 2017-12-11 07:54:43.152150: step 35680, loss = 0.70, batch loss = 0.64 (14.3 examples/sec; 0.558 sec/batch; 46h:01m:38s remains)
INFO - root - 2017-12-11 07:54:48.689878: step 35690, loss = 0.70, batch loss = 0.64 (14.9 examples/sec; 0.536 sec/batch; 44h:12m:39s remains)
INFO - root - 2017-12-11 07:54:54.165124: step 35700, loss = 0.69, batch loss = 0.63 (14.6 examples/sec; 0.546 sec/batch; 45h:03m:11s remains)
2017-12-11 07:54:54.775424: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.2476166 0.28457931 0.32332331 0.34307775 0.32970375 0.29757056 0.27503756 0.26874831 0.26016071 0.22438206 0.16185197 0.099347562 0.060489435 0.051770944 0.06742157][0.266371 0.30568868 0.34545287 0.36387497 0.34920645 0.31971467 0.30623496 0.31376451 0.31774178 0.28687629 0.22256187 0.156602 0.11610114 0.1087498 0.12877023][0.27436545 0.31000763 0.34285656 0.35469398 0.33905166 0.31667063 0.31721923 0.34129414 0.35890898 0.33497635 0.2728706 0.20790711 0.16937779 0.16601866 0.19034235][0.27293274 0.29756391 0.3163352 0.31884536 0.3049911 0.29551646 0.31427297 0.3558639 0.38600194 0.36850691 0.30851498 0.24383415 0.20616369 0.20582655 0.23258059][0.2576021 0.26588848 0.266683 0.26042053 0.25314537 0.26322424 0.30497056 0.36426485 0.40301958 0.38749641 0.32667479 0.26057556 0.22212593 0.22207198 0.24770331][0.23174106 0.22347748 0.20863234 0.19716965 0.2002583 0.23196112 0.29532343 0.36622447 0.40445817 0.38330573 0.3186332 0.2523073 0.21547456 0.21608213 0.23966689][0.20715487 0.18629996 0.16009998 0.14883158 0.16630629 0.21927446 0.2988998 0.371902 0.39868557 0.36265844 0.288414 0.219943 0.18610303 0.19010875 0.21550788][0.19280863 0.16217977 0.13004148 0.12489353 0.16022922 0.232761 0.32212743 0.38856527 0.39566082 0.33784267 0.24779074 0.17184992 0.13782272 0.14469895 0.17401323][0.1832307 0.14727089 0.11447053 0.11844027 0.1694999 0.25422707 0.3439182 0.397235 0.38312855 0.30517828 0.20182575 0.12017625 0.085998878 0.09560807 0.13015208][0.17987427 0.14231095 0.11262956 0.1254988 0.18505405 0.26989272 0.34819141 0.38229313 0.34875539 0.25850525 0.15175328 0.072783068 0.043914445 0.058805894 0.10000685][0.17605329 0.13774528 0.11379889 0.13390332 0.19411311 0.26863593 0.32778114 0.34134582 0.29459965 0.20340082 0.10642923 0.0416412 0.024789041 0.045687947 0.089353368][0.15934177 0.12554881 0.11097209 0.13881435 0.19695576 0.25776076 0.29644662 0.2922051 0.23977372 0.15782483 0.080454148 0.036279779 0.032282751 0.054285388 0.090412967][0.12475627 0.10156663 0.10298505 0.14107354 0.19661039 0.24187349 0.25924653 0.23919991 0.18668172 0.12164188 0.070308641 0.049457438 0.056612719 0.074679121 0.095409229][0.078431189 0.074678153 0.098936327 0.14951877 0.20094538 0.22741313 0.22114018 0.18629937 0.13799219 0.094755284 0.071766473 0.073042527 0.089097984 0.1024003 0.10807644][0.04715668 0.06915547 0.1195017 0.18367544 0.23133732 0.23912694 0.20936476 0.16026492 0.1152561 0.089966953 0.088180415 0.10344218 0.12406727 0.13490392 0.13308093]]...]
INFO - root - 2017-12-11 07:54:59.520891: step 35710, loss = 0.68, batch loss = 0.62 (30.3 examples/sec; 0.264 sec/batch; 21h:46m:11s remains)
INFO - root - 2017-12-11 07:55:04.438652: step 35720, loss = 0.69, batch loss = 0.63 (15.1 examples/sec; 0.531 sec/batch; 43h:46m:59s remains)
INFO - root - 2017-12-11 07:55:09.924747: step 35730, loss = 0.69, batch loss = 0.63 (14.4 examples/sec; 0.554 sec/batch; 45h:40m:34s remains)
INFO - root - 2017-12-11 07:55:15.503767: step 35740, loss = 0.69, batch loss = 0.64 (14.1 examples/sec; 0.567 sec/batch; 46h:45m:24s remains)
INFO - root - 2017-12-11 07:55:20.998395: step 35750, loss = 0.69, batch loss = 0.64 (14.7 examples/sec; 0.543 sec/batch; 44h:43m:11s remains)
INFO - root - 2017-12-11 07:55:26.446302: step 35760, loss = 0.70, batch loss = 0.64 (14.8 examples/sec; 0.542 sec/batch; 44h:38m:29s remains)
INFO - root - 2017-12-11 07:55:31.956230: step 35770, loss = 0.70, batch loss = 0.64 (14.5 examples/sec; 0.551 sec/batch; 45h:22m:44s remains)
INFO - root - 2017-12-11 07:55:37.504031: step 35780, loss = 0.70, batch loss = 0.64 (14.5 examples/sec; 0.552 sec/batch; 45h:31m:02s remains)
INFO - root - 2017-12-11 07:55:42.986145: step 35790, loss = 0.72, batch loss = 0.66 (14.8 examples/sec; 0.541 sec/batch; 44h:34m:10s remains)
INFO - root - 2017-12-11 07:55:48.406481: step 35800, loss = 0.70, batch loss = 0.64 (15.1 examples/sec; 0.531 sec/batch; 43h:44m:16s remains)
2017-12-11 07:55:49.004388: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.42651364 0.32698569 0.21612763 0.12954478 0.089695528 0.106196 0.18355075 0.3016648 0.42219442 0.50319749 0.512251 0.4391337 0.31908062 0.20357102 0.12369269][0.30372748 0.21622062 0.13031672 0.07222753 0.047604952 0.053123254 0.090276919 0.14954972 0.20762263 0.24110712 0.23480685 0.18767323 0.12315173 0.070321776 0.043505512][0.16166925 0.099465571 0.054560807 0.043187957 0.056480847 0.077726737 0.098994479 0.11291985 0.10883968 0.08224757 0.03898203 -0.0075872079 -0.0407562 -0.050878171 -0.038327787][0.035792511 0.013652991 0.02544756 0.079686821 0.15618809 0.224478 0.260482 0.25095081 0.19340681 0.099624835 -0.00099916826 -0.078084581 -0.11527953 -0.11538491 -0.089496344][-0.051708963 -0.026336178 0.047817875 0.17522553 0.32895136 0.46618029 0.5438242 0.53594309 0.44156149 0.28366482 0.11482422 -0.015845967 -0.086754777 -0.1063156 -0.091837533][-0.089151 -0.021902421 0.10514002 0.2948918 0.51800543 0.72344822 0.85132462 0.86094952 0.74703777 0.53809929 0.30624786 0.11808631 0.0030020142 -0.048145875 -0.055704515][-0.086611822 0.0036412203 0.15831593 0.37960219 0.64122528 0.89143187 1.0610521 1.0973575 0.98577541 0.753569 0.48353505 0.25440174 0.10493166 0.029544206 0.0068799593][-0.0667011 0.022198122 0.17152278 0.38416246 0.64159429 0.89891404 1.0882777 1.1515336 1.0634713 0.84329319 0.57206827 0.33327189 0.17374459 0.094374835 0.074958548][-0.038640063 0.02922764 0.14644654 0.31802464 0.53393483 0.76054096 0.940314 1.01926 0.96568877 0.78748888 0.55158585 0.33555984 0.1903393 0.12517338 0.12437243][-0.014572694 0.020789346 0.091984458 0.2080186 0.36611044 0.54423976 0.69884473 0.78536844 0.77088422 0.65315396 0.47763458 0.30580929 0.18635719 0.1376387 0.1525479][0.0014552537 0.004027626 0.028318407 0.088719957 0.18937318 0.31961989 0.45031106 0.54723287 0.57848406 0.53184152 0.4275218 0.30493915 0.20582668 0.15763581 0.16591634][0.013671273 -0.0048081134 -0.014058503 0.0041983682 0.060289904 0.15425931 0.27057144 0.38440615 0.46500978 0.48938423 0.453223 0.37287143 0.28040954 0.20994337 0.18175025][0.024557171 0.000443718 -0.023681169 -0.028243272 0.001451706 0.074120656 0.18556871 0.31828263 0.44265831 0.52679372 0.5467841 0.49878469 0.40356418 0.29755527 0.21449751][0.040227145 0.021877954 -0.0017428132 -0.013291855 0.0030172884 0.060831595 0.16480185 0.30468416 0.45416 0.57656091 0.63501352 0.61117971 0.51491725 0.38055241 0.25021651][0.062193979 0.056011036 0.042491488 0.033484947 0.041565768 0.080777355 0.16269316 0.28663638 0.43408555 0.568751 0.64723235 0.64240235 0.55498487 0.41377646 0.26306897]]...]
INFO - root - 2017-12-11 07:55:54.255763: step 35810, loss = 0.68, batch loss = 0.63 (15.2 examples/sec; 0.526 sec/batch; 43h:22m:46s remains)
INFO - root - 2017-12-11 07:55:59.762240: step 35820, loss = 0.71, batch loss = 0.65 (14.6 examples/sec; 0.550 sec/batch; 45h:18m:41s remains)
INFO - root - 2017-12-11 07:56:05.217878: step 35830, loss = 0.71, batch loss = 0.65 (14.4 examples/sec; 0.554 sec/batch; 45h:38m:30s remains)
INFO - root - 2017-12-11 07:56:10.673098: step 35840, loss = 0.68, batch loss = 0.62 (15.0 examples/sec; 0.532 sec/batch; 43h:52m:15s remains)
INFO - root - 2017-12-11 07:56:16.208303: step 35850, loss = 0.70, batch loss = 0.64 (14.6 examples/sec; 0.549 sec/batch; 45h:13m:20s remains)
INFO - root - 2017-12-11 07:56:21.696278: step 35860, loss = 0.71, batch loss = 0.65 (15.0 examples/sec; 0.533 sec/batch; 43h:52m:47s remains)
INFO - root - 2017-12-11 07:56:27.163352: step 35870, loss = 0.70, batch loss = 0.64 (14.8 examples/sec; 0.542 sec/batch; 44h:37m:14s remains)
INFO - root - 2017-12-11 07:56:32.619419: step 35880, loss = 0.69, batch loss = 0.63 (14.6 examples/sec; 0.547 sec/batch; 45h:05m:46s remains)
INFO - root - 2017-12-11 07:56:38.176054: step 35890, loss = 0.69, batch loss = 0.64 (14.8 examples/sec; 0.542 sec/batch; 44h:39m:18s remains)
INFO - root - 2017-12-11 07:56:43.633250: step 35900, loss = 0.71, batch loss = 0.65 (14.7 examples/sec; 0.545 sec/batch; 44h:51m:46s remains)
2017-12-11 07:56:44.231325: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.40956312 0.41583088 0.4286935 0.44170654 0.45812771 0.47778383 0.48767653 0.465094 0.39839387 0.30002761 0.19474469 0.11082471 0.065602958 0.062583208 0.1013732][0.38644537 0.40119371 0.42316133 0.44707292 0.47153038 0.489284 0.48741543 0.45847762 0.39865908 0.31458449 0.22125143 0.14079401 0.091846481 0.076193549 0.095780537][0.33794338 0.36286604 0.39683768 0.43259946 0.464882 0.4815909 0.47174278 0.44220069 0.39736834 0.33941078 0.26926008 0.19686918 0.13860807 0.09872888 0.085226737][0.29736432 0.33308741 0.37885955 0.42560795 0.46591657 0.4866316 0.4777903 0.45376468 0.42465639 0.39063969 0.3411575 0.2735149 0.20068318 0.13187766 0.08156091][0.27020812 0.31103286 0.3627786 0.41620839 0.46353328 0.49361598 0.4970769 0.48764181 0.47493777 0.4586584 0.42321357 0.35873726 0.27339244 0.17977366 0.096659578][0.25441796 0.29015625 0.3383294 0.39243063 0.44558632 0.48900056 0.5123989 0.52219021 0.52391946 0.5181725 0.49156058 0.43326667 0.34558126 0.23999226 0.1350141][0.25496513 0.2768392 0.31251264 0.36193055 0.41879451 0.47478354 0.51721781 0.54223776 0.55213088 0.55062211 0.53099644 0.48539832 0.40937 0.30836767 0.19533442][0.27437162 0.27728632 0.29401162 0.33360639 0.38988897 0.45215145 0.50541949 0.53779727 0.55080789 0.55139953 0.53938752 0.51122987 0.45647365 0.37196237 0.26139924][0.311435 0.2966435 0.29281124 0.31703994 0.36467969 0.4225862 0.47570887 0.50910616 0.52523267 0.53191632 0.53137714 0.52283585 0.4921535 0.42868707 0.32606772][0.35719991 0.33274189 0.31254762 0.31800932 0.34874415 0.39324456 0.43892255 0.47072175 0.49156526 0.50753176 0.51994085 0.52914238 0.51951379 0.47561976 0.38327953][0.39516377 0.37180796 0.34416628 0.33296588 0.34385639 0.37088069 0.40531835 0.43310267 0.45573139 0.47673184 0.49680763 0.51661915 0.52149385 0.49452487 0.41661653][0.40354145 0.38978627 0.3653979 0.34565967 0.34080592 0.35161602 0.37357834 0.39430034 0.41339907 0.43279493 0.45414203 0.47807008 0.4927128 0.48182797 0.42429012][0.37101868 0.37164569 0.36073908 0.34461018 0.33383533 0.33425221 0.34448373 0.35549903 0.36599332 0.377982 0.39537641 0.41806573 0.43841213 0.44224373 0.40851015][0.31179613 0.3283039 0.33750483 0.33511043 0.32729414 0.32259342 0.3225387 0.32158688 0.3192814 0.31957087 0.3296749 0.34817526 0.37023389 0.38454321 0.37226644][0.24252114 0.2716 0.29999137 0.31479341 0.31503105 0.30966187 0.30221352 0.28999555 0.2736555 0.25950077 0.25868353 0.26932123 0.28827325 0.30660197 0.30895498]]...]
INFO - root - 2017-12-11 07:56:49.431533: step 35910, loss = 0.71, batch loss = 0.65 (14.6 examples/sec; 0.547 sec/batch; 45h:04m:12s remains)
INFO - root - 2017-12-11 07:56:54.974084: step 35920, loss = 0.71, batch loss = 0.65 (14.3 examples/sec; 0.560 sec/batch; 46h:10m:12s remains)
INFO - root - 2017-12-11 07:57:00.470931: step 35930, loss = 0.70, batch loss = 0.64 (14.9 examples/sec; 0.538 sec/batch; 44h:17m:49s remains)
INFO - root - 2017-12-11 07:57:05.974571: step 35940, loss = 0.71, batch loss = 0.65 (14.1 examples/sec; 0.566 sec/batch; 46h:37m:08s remains)
INFO - root - 2017-12-11 07:57:11.496277: step 35950, loss = 0.70, batch loss = 0.64 (14.6 examples/sec; 0.549 sec/batch; 45h:15m:53s remains)
INFO - root - 2017-12-11 07:57:17.063795: step 35960, loss = 0.69, batch loss = 0.64 (14.8 examples/sec; 0.541 sec/batch; 44h:34m:29s remains)
INFO - root - 2017-12-11 07:57:22.570487: step 35970, loss = 0.71, batch loss = 0.65 (14.4 examples/sec; 0.554 sec/batch; 45h:36m:21s remains)
INFO - root - 2017-12-11 07:57:28.107048: step 35980, loss = 0.71, batch loss = 0.65 (14.6 examples/sec; 0.548 sec/batch; 45h:07m:44s remains)
INFO - root - 2017-12-11 07:57:33.605674: step 35990, loss = 0.68, batch loss = 0.62 (14.7 examples/sec; 0.545 sec/batch; 44h:52m:45s remains)
INFO - root - 2017-12-11 07:57:39.076704: step 36000, loss = 0.69, batch loss = 0.63 (15.3 examples/sec; 0.522 sec/batch; 42h:57m:33s remains)
2017-12-11 07:57:39.662896: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.042533319 -0.026847677 0.003094496 0.042988103 0.087133072 0.12774435 0.15269905 0.15967724 0.15872762 0.1569737 0.15782857 0.16214597 0.1711742 0.18039156 0.1859155][-0.045737352 -0.030589765 0.00079362874 0.044588987 0.094473779 0.14039227 0.16938102 0.18060179 0.18597306 0.19190153 0.19958928 0.20784274 0.21758443 0.22385859 0.22419082][-0.046038613 -0.032485053 -0.0019138795 0.042590238 0.094666585 0.14309052 0.17500393 0.19003589 0.2003845 0.21208242 0.22527158 0.23833701 0.25047463 0.25557619 0.25238985][-0.045887865 -0.033452153 -0.0044238358 0.038357362 0.089177437 0.13782978 0.1725077 0.19213331 0.20718552 0.22277348 0.23879218 0.2545298 0.2675952 0.27056921 0.2635856][-0.045922637 -0.034330398 -0.0074018748 0.031644888 0.078400776 0.12559985 0.16368638 0.19081616 0.21346864 0.23369199 0.25096911 0.26572946 0.27523708 0.27178052 0.25787893][-0.045995243 -0.035053089 -0.0095638428 0.026503373 0.069571033 0.1144999 0.15563308 0.19131044 0.22262721 0.24761422 0.26467225 0.27547833 0.27723804 0.26308805 0.23894283][-0.045700297 -0.034918297 -0.0092723966 0.026304856 0.067787506 0.11111303 0.15343733 0.19360285 0.22997624 0.25819147 0.27503014 0.282074 0.27630863 0.25259843 0.22072819][-0.044955313 -0.034158137 -0.0078539969 0.028116331 0.068865463 0.110149 0.14993931 0.18780358 0.22319801 0.252455 0.27062458 0.27746126 0.26822522 0.23936045 0.204055][-0.044817481 -0.034308072 -0.0082906038 0.027199712 0.067178421 0.10686368 0.1434166 0.17608252 0.20720941 0.23629478 0.25703487 0.26595253 0.25640339 0.22630461 0.19079618][-0.0451301 -0.035108164 -0.0098413248 0.025058297 0.065178946 0.10510847 0.14058042 0.16948971 0.19591385 0.22196414 0.24196894 0.25108254 0.24222335 0.21429881 0.18067354][-0.044769049 -0.035097949 -0.010214043 0.024858303 0.065987647 0.10747342 0.14381345 0.17105658 0.19272591 0.21156658 0.22477302 0.22992851 0.22178061 0.1986234 0.16922957][-0.043469638 -0.034249824 -0.010344086 0.023840822 0.064494461 0.1062332 0.14312056 0.17032713 0.18966353 0.20287649 0.20923264 0.20997269 0.20320506 0.18672881 0.16453451][-0.041794069 -0.033184636 -0.011253617 0.020038754 0.057387881 0.096408606 0.13187037 0.15934484 0.17912608 0.19108467 0.1943889 0.19241852 0.18645774 0.17503113 0.15940507][-0.040458705 -0.03241156 -0.012411561 0.0157086 0.049182106 0.084817283 0.11870923 0.14716782 0.1692736 0.18344148 0.18788023 0.1860819 0.18089503 0.17229921 0.16097783][-0.038558368 -0.029770253 -0.0094961971 0.017849732 0.04909179 0.081810892 0.1143135 0.1445007 0.17072359 0.19003418 0.19956392 0.20120576 0.19722185 0.18874475 0.17773205]]...]
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-SGD-lr0.001-clip50-initfromfixqian30k-fixqian
INFO - root - 2017-12-11 07:57:44.793839: step 36010, loss = 0.71, batch loss = 0.65 (14.3 examples/sec; 0.558 sec/batch; 45h:55m:44s remains)
INFO - root - 2017-12-11 07:57:50.420193: step 36020, loss = 0.69, batch loss = 0.63 (14.3 examples/sec; 0.558 sec/batch; 45h:56m:08s remains)
INFO - root - 2017-12-11 07:57:55.934335: step 36030, loss = 0.70, batch loss = 0.64 (14.8 examples/sec; 0.540 sec/batch; 44h:29m:37s remains)
INFO - root - 2017-12-11 07:58:01.456606: step 36040, loss = 0.70, batch loss = 0.64 (15.1 examples/sec; 0.528 sec/batch; 43h:30m:43s remains)
INFO - root - 2017-12-11 07:58:06.920223: step 36050, loss = 0.70, batch loss = 0.64 (14.4 examples/sec; 0.556 sec/batch; 45h:46m:01s remains)
INFO - root - 2017-12-11 07:58:12.453014: step 36060, loss = 0.71, batch loss = 0.65 (14.7 examples/sec; 0.544 sec/batch; 44h:48m:26s remains)
INFO - root - 2017-12-11 07:58:17.996505: step 36070, loss = 0.68, batch loss = 0.62 (14.6 examples/sec; 0.548 sec/batch; 45h:07m:55s remains)
INFO - root - 2017-12-11 07:58:23.538102: step 36080, loss = 0.70, batch loss = 0.64 (13.7 examples/sec; 0.582 sec/batch; 47h:57m:28s remains)
INFO - root - 2017-12-11 07:58:29.104433: step 36090, loss = 0.70, batch loss = 0.64 (14.5 examples/sec; 0.551 sec/batch; 45h:22m:47s remains)
INFO - root - 2017-12-11 07:58:34.619776: step 36100, loss = 0.69, batch loss = 0.63 (14.2 examples/sec; 0.565 sec/batch; 46h:31m:19s remains)
2017-12-11 07:58:35.152400: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.06525638 0.061862536 0.057776727 0.05514583 0.054928269 0.05829519 0.070740804 0.090998888 0.11121766 0.12246644 0.12158214 0.1033573 0.065005466 0.016559521 -0.027332349][0.051895592 0.050400969 0.050223786 0.05318325 0.059302375 0.069792986 0.091119654 0.12106911 0.14855054 0.16158506 0.15696271 0.13052784 0.082338549 0.025336027 -0.023639414][0.025368595 0.029149255 0.03689139 0.049470872 0.065960668 0.088904463 0.12554397 0.16985707 0.20519252 0.21621706 0.20046662 0.15655397 0.09094546 0.02296496 -0.029025404][0.0067494321 0.019738676 0.040253211 0.068623967 0.10307876 0.1478864 0.20889328 0.27244759 0.31410989 0.31579849 0.27838382 0.20559107 0.1131229 0.028556503 -0.029262858][0.012841515 0.037904985 0.075043485 0.12522085 0.18538703 0.25942698 0.34958711 0.43271217 0.47592232 0.45942941 0.39058477 0.28209078 0.15811218 0.051890761 -0.018238325][0.044289462 0.08144334 0.13604526 0.21070777 0.2995761 0.40259671 0.51731914 0.61267054 0.64941847 0.60797477 0.50459 0.36254603 0.21048069 0.0820997 -0.0048926165][0.095504366 0.14307201 0.21284363 0.30867097 0.42052877 0.5418371 0.66649359 0.760564 0.78292561 0.71632 0.58504045 0.41991439 0.24863173 0.10222181 -0.00038723755][0.15467246 0.20977764 0.28870746 0.3950218 0.51432925 0.6330812 0.74426115 0.81856877 0.82103252 0.73895025 0.59836823 0.42951044 0.25391576 0.099581912 -0.011424576][0.1961927 0.25386745 0.33192426 0.43199727 0.53748447 0.63026845 0.70609421 0.74768406 0.73309225 0.65332085 0.527757 0.37861598 0.21850365 0.071863346 -0.036520671][0.20084347 0.25494051 0.32084355 0.39754042 0.46989509 0.52099353 0.55302072 0.56436223 0.545673 0.4877198 0.397571 0.28543013 0.15579465 0.029425187 -0.067048207][0.17352311 0.21798861 0.26361758 0.30634952 0.33608857 0.34317455 0.3384538 0.3327474 0.32320687 0.29785487 0.2515139 0.18262212 0.089219533 -0.010952248 -0.0904414][0.14144254 0.17541935 0.20000315 0.20860006 0.19772327 0.16874653 0.14169201 0.13409181 0.14350902 0.15392737 0.14981294 0.11953733 0.056415502 -0.023363436 -0.091571338][0.11458544 0.14020792 0.14925964 0.13349117 0.095300406 0.046551768 0.013812741 0.016133737 0.048315305 0.090206407 0.11841213 0.11379559 0.067278057 -0.0048128511 -0.072644889][0.098208189 0.11769974 0.11689162 0.087896906 0.037381563 -0.015281223 -0.041170146 -0.023128247 0.029886268 0.094538718 0.14232349 0.14969704 0.10712112 0.031930473 -0.043700308][0.10431135 0.1186432 0.1107073 0.075623311 0.022678368 -0.024776334 -0.038200349 -0.0038572638 0.065489687 0.14393891 0.19990492 0.20818433 0.15997781 0.074283257 -0.013886089]]...]
INFO - root - 2017-12-11 07:58:40.436713: step 36110, loss = 0.69, batch loss = 0.63 (14.8 examples/sec; 0.541 sec/batch; 44h:34m:51s remains)
INFO - root - 2017-12-11 07:58:46.010277: step 36120, loss = 0.70, batch loss = 0.64 (14.5 examples/sec; 0.551 sec/batch; 45h:23m:06s remains)
INFO - root - 2017-12-11 07:58:51.478031: step 36130, loss = 0.70, batch loss = 0.64 (14.7 examples/sec; 0.544 sec/batch; 44h:47m:20s remains)
INFO - root - 2017-12-11 07:58:56.983117: step 36140, loss = 0.70, batch loss = 0.65 (14.1 examples/sec; 0.566 sec/batch; 46h:33m:17s remains)
INFO - root - 2017-12-11 07:59:02.525231: step 36150, loss = 0.71, batch loss = 0.65 (14.7 examples/sec; 0.546 sec/batch; 44h:56m:39s remains)
INFO - root - 2017-12-11 07:59:07.984550: step 36160, loss = 0.68, batch loss = 0.62 (14.6 examples/sec; 0.546 sec/batch; 44h:58m:14s remains)
INFO - root - 2017-12-11 07:59:13.570144: step 36170, loss = 0.71, batch loss = 0.65 (14.6 examples/sec; 0.547 sec/batch; 45h:01m:35s remains)
INFO - root - 2017-12-11 07:59:19.121294: step 36180, loss = 0.71, batch loss = 0.65 (14.9 examples/sec; 0.537 sec/batch; 44h:10m:35s remains)
INFO - root - 2017-12-11 07:59:24.579136: step 36190, loss = 0.70, batch loss = 0.64 (15.2 examples/sec; 0.526 sec/batch; 43h:15m:54s remains)
INFO - root - 2017-12-11 07:59:29.797189: step 36200, loss = 0.70, batch loss = 0.64 (13.3 examples/sec; 0.602 sec/batch; 49h:31m:37s remains)
2017-12-11 07:59:30.379001: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.18855742 0.15640086 0.12873966 0.11595664 0.12672225 0.16105038 0.20376182 0.23402165 0.23111366 0.18590906 0.11064538 0.0296564 -0.032380562 -0.065844469 -0.074778587][0.15738858 0.12959388 0.10850746 0.10560915 0.12930875 0.17811394 0.23298632 0.2688542 0.2638348 0.2107446 0.12564607 0.0357002 -0.032000359 -0.067925833 -0.077498615][0.11458472 0.093588427 0.081170268 0.088241369 0.12185615 0.17912583 0.23879905 0.27461508 0.26558667 0.2072666 0.11866593 0.027803462 -0.038312577 -0.071750537 -0.078986764][0.067730233 0.053538535 0.049929738 0.066371575 0.10790742 0.16997874 0.23055384 0.26344168 0.24979404 0.18818906 0.099887736 0.01243812 -0.048655931 -0.077018313 -0.080398805][0.029635621 0.021475831 0.025756236 0.050174769 0.0975642 0.16175209 0.22068919 0.24929418 0.23099358 0.16726127 0.080397919 -0.0026361295 -0.058140781 -0.081665821 -0.081726491][0.014136163 0.010477471 0.01967703 0.048226792 0.097381309 0.16015415 0.21513805 0.23878151 0.21611167 0.15065476 0.065202013 -0.013970855 -0.065031692 -0.084955141 -0.082776859][0.018146649 0.015985485 0.025881521 0.054302357 0.10172158 0.16077036 0.21091279 0.22981648 0.20338349 0.13675962 0.052919719 -0.022723394 -0.069937579 -0.086916633 -0.083044909][0.031228162 0.028847525 0.038018212 0.066014439 0.11304265 0.17114496 0.21980262 0.2366862 0.20745298 0.13825455 0.052763283 -0.02337281 -0.070450231 -0.087082081 -0.082979664][0.053312439 0.050749369 0.059217013 0.087295264 0.13598685 0.19726972 0.24943851 0.2680648 0.23688519 0.16254762 0.070169635 -0.012570097 -0.064765215 -0.084687471 -0.082515649][0.080046751 0.077683374 0.084933572 0.11198531 0.16139048 0.22552574 0.28109872 0.30133405 0.26801851 0.18794894 0.087888889 -0.0019408189 -0.059181981 -0.082480088 -0.082443021][0.10467424 0.1021202 0.10747458 0.13230798 0.18010902 0.24353416 0.29854041 0.31724826 0.28101012 0.19682617 0.092512459 -0.00023434831 -0.058739208 -0.08244998 -0.082596816][0.11665528 0.11366069 0.11734991 0.14002585 0.18515004 0.24542889 0.29710481 0.31278956 0.27482674 0.1901641 0.086259663 -0.0051407819 -0.061879985 -0.08440236 -0.084006511][0.12606475 0.12202983 0.12457959 0.14635624 0.19020417 0.24814136 0.296482 0.30889925 0.26908752 0.18412885 0.080937237 -0.0089712832 -0.064236984 -0.085880682 -0.085077807][0.140024 0.13276322 0.13248359 0.15253243 0.19516928 0.25110108 0.29618305 0.30528054 0.26350388 0.17824998 0.076196611 -0.0115373 -0.064925037 -0.0858147 -0.08502996][0.15375157 0.14316392 0.14081743 0.15972002 0.20015192 0.25112826 0.28912318 0.29183114 0.246925 0.16261145 0.064911366 -0.017052708 -0.065971106 -0.084925212 -0.083975419]]...]
INFO - root - 2017-12-11 07:59:35.871967: step 36210, loss = 0.71, batch loss = 0.65 (14.3 examples/sec; 0.558 sec/batch; 45h:53m:50s remains)
INFO - root - 2017-12-11 07:59:41.363786: step 36220, loss = 0.69, batch loss = 0.63 (14.8 examples/sec; 0.541 sec/batch; 44h:30m:04s remains)
INFO - root - 2017-12-11 07:59:46.811038: step 36230, loss = 0.70, batch loss = 0.64 (15.1 examples/sec; 0.529 sec/batch; 43h:30m:06s remains)
INFO - root - 2017-12-11 07:59:52.371066: step 36240, loss = 0.72, batch loss = 0.66 (14.4 examples/sec; 0.554 sec/batch; 45h:37m:21s remains)
INFO - root - 2017-12-11 07:59:57.865315: step 36250, loss = 0.72, batch loss = 0.66 (14.9 examples/sec; 0.536 sec/batch; 44h:04m:15s remains)
INFO - root - 2017-12-11 08:00:03.373953: step 36260, loss = 0.70, batch loss = 0.64 (14.6 examples/sec; 0.549 sec/batch; 45h:09m:33s remains)
INFO - root - 2017-12-11 08:00:08.812173: step 36270, loss = 0.69, batch loss = 0.63 (14.7 examples/sec; 0.545 sec/batch; 44h:49m:23s remains)
INFO - root - 2017-12-11 08:00:14.415473: step 36280, loss = 0.70, batch loss = 0.64 (14.1 examples/sec; 0.569 sec/batch; 46h:48m:10s remains)
INFO - root - 2017-12-11 08:00:19.942908: step 36290, loss = 0.69, batch loss = 0.63 (14.6 examples/sec; 0.549 sec/batch; 45h:08m:14s remains)
INFO - root - 2017-12-11 08:00:25.138525: step 36300, loss = 0.71, batch loss = 0.65 (14.3 examples/sec; 0.558 sec/batch; 45h:55m:52s remains)
2017-12-11 08:00:25.711578: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.075012818 0.061916854 0.065344043 0.077181838 0.088290729 0.098667309 0.12711421 0.16144124 0.187925 0.19574845 0.18681437 0.16061515 0.10822213 0.044847559 -0.0083897859][0.088604756 0.078969814 0.081109621 0.089181051 0.092350416 0.090850636 0.10618681 0.12980646 0.14987879 0.15670344 0.1523612 0.13438016 0.090271153 0.033779636 -0.013650691][0.1187658 0.12018084 0.12828821 0.13786353 0.13542078 0.12083452 0.1188631 0.1263274 0.13373245 0.13392048 0.12964256 0.11702418 0.079055883 0.027498679 -0.015754059][0.17247988 0.1954933 0.22092268 0.24211119 0.24150774 0.21764125 0.1999331 0.19139381 0.18362845 0.17135932 0.15873584 0.14201914 0.099227726 0.040676061 -0.0086337207][0.24651627 0.30079755 0.35313898 0.39470029 0.40308 0.37527213 0.34666196 0.32637313 0.30503115 0.27758145 0.24941604 0.21872649 0.15901861 0.080248676 0.013169831][0.32508174 0.41008627 0.49064794 0.55520874 0.57765788 0.55327284 0.52179915 0.49689698 0.46719423 0.42567506 0.37844768 0.32642055 0.24117841 0.13387752 0.042043038][0.38382626 0.48695549 0.58379507 0.66290379 0.69740146 0.68158525 0.65596116 0.63533264 0.60583234 0.55723405 0.4947063 0.42197061 0.31142691 0.17715679 0.063231349][0.39499673 0.49557453 0.59083718 0.66991764 0.70806485 0.70029932 0.68431824 0.67339379 0.65202886 0.60652918 0.53926468 0.45497757 0.32996973 0.18200581 0.058464441][0.34437045 0.42477047 0.50220722 0.56694621 0.59849834 0.5943504 0.58683854 0.58646256 0.57761258 0.54430455 0.48488569 0.40308854 0.28128085 0.13989042 0.024360452][0.23989469 0.29060832 0.34159589 0.38361579 0.40206137 0.39763209 0.39510763 0.40180275 0.40373838 0.38562104 0.34263015 0.27662459 0.17647587 0.062189274 -0.028036568][0.11280736 0.13452475 0.15920021 0.17833763 0.18331462 0.17753777 0.17749386 0.18687895 0.19429643 0.18798716 0.16252707 0.11825445 0.049836565 -0.026281945 -0.082265921][0.00013190461 0.0025110932 0.0088964915 0.011998202 0.0078306161 0.00119059 0.0018095666 0.010362044 0.01825645 0.018229855 0.0063318447 -0.017543226 -0.054926977 -0.094523467 -0.11899117][-0.077295341 -0.082905643 -0.08339233 -0.086046606 -0.09220726 -0.09761738 -0.097183064 -0.091662906 -0.086390883 -0.0848044 -0.088712759 -0.097949423 -0.11236043 -0.12589943 -0.12978894][-0.11060851 -0.11586101 -0.11599789 -0.1178568 -0.12173972 -0.12493084 -0.12487796 -0.122236 -0.1194677 -0.117929 -0.11812585 -0.11968216 -0.12202634 -0.12268163 -0.11827718][-0.10685886 -0.11029778 -0.10899635 -0.10902139 -0.11031977 -0.11162906 -0.11181773 -0.11105948 -0.11004262 -0.10908614 -0.1082148 -0.1071753 -0.10565661 -0.10292142 -0.098057576]]...]
INFO - root - 2017-12-11 08:00:31.169172: step 36310, loss = 0.70, batch loss = 0.64 (14.6 examples/sec; 0.547 sec/batch; 45h:00m:12s remains)
INFO - root - 2017-12-11 08:00:36.715692: step 36320, loss = 0.70, batch loss = 0.64 (14.2 examples/sec; 0.562 sec/batch; 46h:12m:09s remains)
INFO - root - 2017-12-11 08:00:42.215523: step 36330, loss = 0.69, batch loss = 0.63 (14.5 examples/sec; 0.552 sec/batch; 45h:26m:00s remains)
INFO - root - 2017-12-11 08:00:47.727145: step 36340, loss = 0.70, batch loss = 0.64 (14.2 examples/sec; 0.562 sec/batch; 46h:12m:42s remains)
INFO - root - 2017-12-11 08:00:53.326931: step 36350, loss = 0.70, batch loss = 0.65 (14.7 examples/sec; 0.544 sec/batch; 44h:43m:36s remains)
INFO - root - 2017-12-11 08:00:58.799059: step 36360, loss = 0.70, batch loss = 0.64 (14.5 examples/sec; 0.553 sec/batch; 45h:29m:37s remains)
INFO - root - 2017-12-11 08:01:04.189261: step 36370, loss = 0.70, batch loss = 0.64 (14.8 examples/sec; 0.539 sec/batch; 44h:20m:39s remains)
INFO - root - 2017-12-11 08:01:09.777819: step 36380, loss = 0.69, batch loss = 0.64 (14.2 examples/sec; 0.564 sec/batch; 46h:24m:54s remains)
INFO - root - 2017-12-11 08:01:15.286842: step 36390, loss = 0.69, batch loss = 0.63 (14.6 examples/sec; 0.549 sec/batch; 45h:09m:32s remains)
INFO - root - 2017-12-11 08:01:20.513684: step 36400, loss = 0.69, batch loss = 0.63 (14.3 examples/sec; 0.558 sec/batch; 45h:56m:01s remains)
2017-12-11 08:01:21.128801: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.19186863 0.24035478 0.29456118 0.34768745 0.39324209 0.44002724 0.4848848 0.51046687 0.50558072 0.46310681 0.38718507 0.27977821 0.16093113 0.057045657 -0.016995911][0.26248509 0.28624645 0.30918097 0.33545217 0.36146897 0.39559817 0.43695584 0.46824166 0.4736954 0.44117486 0.37242278 0.26982448 0.15238354 0.0475476 -0.027213449][0.3357414 0.33123335 0.32328328 0.32913485 0.34365618 0.36914328 0.40534303 0.43460518 0.43879402 0.40596041 0.33886367 0.24160266 0.12947547 0.028849833 -0.042174503][0.39097333 0.36728311 0.34389448 0.34756753 0.36640102 0.39370075 0.42955428 0.45521176 0.44974989 0.40365237 0.32609159 0.22577366 0.11392194 0.014820481 -0.053447604][0.4142983 0.39017361 0.374059 0.3934845 0.42721096 0.46263766 0.50147659 0.5229733 0.50227284 0.43420339 0.337269 0.22407585 0.10417231 0.0017656861 -0.064432137][0.4179582 0.41197023 0.42416835 0.47232196 0.52664226 0.57542557 0.62185377 0.64039516 0.60169047 0.50615525 0.38219613 0.24446379 0.10413002 -0.010013809 -0.077316269][0.42696118 0.44917935 0.49530697 0.57273436 0.6488359 0.71838921 0.78256166 0.80590969 0.75393885 0.63173562 0.47604352 0.30287474 0.12961318 -0.0062644659 -0.0813508][0.44740179 0.48635519 0.548151 0.63722169 0.72708207 0.82240236 0.91795868 0.96328509 0.91571867 0.78085232 0.59994704 0.39064917 0.18039477 0.017379425 -0.071202151][0.47885373 0.50955182 0.55524671 0.62727505 0.712636 0.82606256 0.95469314 1.0342689 1.0116495 0.88647687 0.69824672 0.46637607 0.22966367 0.045417726 -0.055498812][0.50404239 0.50735307 0.51229668 0.54472905 0.60606617 0.71768546 0.86181414 0.96856552 0.97804314 0.88094562 0.70928383 0.48202187 0.24426271 0.057110034 -0.046951022][0.48800293 0.46430349 0.42893529 0.42022333 0.44970995 0.54013991 0.67343593 0.78463531 0.81559932 0.75043011 0.61190736 0.41619077 0.20637308 0.0393314 -0.054391071][0.40264359 0.36157843 0.30126223 0.26447168 0.26688191 0.32776636 0.43161342 0.52593213 0.561947 0.52225405 0.42188507 0.27333549 0.11201382 -0.015798623 -0.085154235][0.24992508 0.20274706 0.13798565 0.0923855 0.079860821 0.11392353 0.18326537 0.2508809 0.28068238 0.25741383 0.1910464 0.091498494 -0.015154046 -0.095865607 -0.13328831][0.079406232 0.036130734 -0.017668268 -0.057678107 -0.074530564 -0.060430426 -0.022184229 0.0177493 0.03609341 0.021819618 -0.018013662 -0.075374909 -0.13308182 -0.17057021 -0.17803863][-0.050808918 -0.083955936 -0.12026978 -0.14828855 -0.16316003 -0.16127063 -0.14594777 -0.12859248 -0.1215769 -0.13133855 -0.15269932 -0.17969501 -0.20243476 -0.20993309 -0.19808087]]...]
INFO - root - 2017-12-11 08:01:26.684485: step 36410, loss = 0.69, batch loss = 0.63 (14.4 examples/sec; 0.556 sec/batch; 45h:43m:43s remains)
INFO - root - 2017-12-11 08:01:32.124237: step 36420, loss = 0.68, batch loss = 0.62 (14.6 examples/sec; 0.547 sec/batch; 45h:01m:08s remains)
INFO - root - 2017-12-11 08:01:37.571388: step 36430, loss = 0.70, batch loss = 0.64 (15.0 examples/sec; 0.535 sec/batch; 43h:58m:47s remains)
INFO - root - 2017-12-11 08:01:43.134833: step 36440, loss = 0.70, batch loss = 0.64 (13.8 examples/sec; 0.578 sec/batch; 47h:31m:00s remains)
INFO - root - 2017-12-11 08:01:48.705386: step 36450, loss = 0.69, batch loss = 0.63 (14.8 examples/sec; 0.540 sec/batch; 44h:24m:57s remains)
INFO - root - 2017-12-11 08:01:54.213380: step 36460, loss = 0.68, batch loss = 0.62 (14.6 examples/sec; 0.547 sec/batch; 44h:57m:43s remains)
INFO - root - 2017-12-11 08:01:59.608838: step 36470, loss = 0.70, batch loss = 0.64 (14.9 examples/sec; 0.539 sec/batch; 44h:17m:27s remains)
INFO - root - 2017-12-11 08:02:05.177789: step 36480, loss = 0.68, batch loss = 0.63 (14.5 examples/sec; 0.550 sec/batch; 45h:14m:14s remains)
INFO - root - 2017-12-11 08:02:10.488530: step 36490, loss = 0.68, batch loss = 0.62 (30.3 examples/sec; 0.264 sec/batch; 21h:41m:20s remains)
INFO - root - 2017-12-11 08:02:16.028368: step 36500, loss = 0.69, batch loss = 0.63 (14.3 examples/sec; 0.560 sec/batch; 46h:01m:05s remains)
2017-12-11 08:02:16.603539: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.29150045 0.27747163 0.23989345 0.19570418 0.1644118 0.15362647 0.15417454 0.15411614 0.1526334 0.15133545 0.16532078 0.20650251 0.25602961 0.28711396 0.28309497][0.33381245 0.33042854 0.30009684 0.2592212 0.23025361 0.22221877 0.22373033 0.21860752 0.20703006 0.19417346 0.19751282 0.23206584 0.27856386 0.31000504 0.30766493][0.34303722 0.35321712 0.339794 0.31610408 0.30325896 0.30749208 0.31373298 0.3017793 0.27386856 0.24119349 0.22439466 0.24247929 0.27763608 0.30481184 0.30270258][0.32332298 0.34400252 0.35231689 0.35806513 0.37640846 0.4051902 0.42264757 0.40457433 0.35627776 0.29686335 0.25149432 0.24491179 0.26279286 0.28326413 0.28113085][0.29072338 0.31469139 0.34135208 0.37941909 0.43660337 0.49940842 0.53631991 0.51760393 0.45097506 0.36292613 0.28435284 0.24777812 0.24440366 0.25641903 0.25460908][0.25778261 0.27756417 0.31280103 0.37446952 0.46482283 0.561128 0.6223011 0.61142838 0.53613079 0.42766526 0.3218407 0.25797442 0.23305777 0.23413861 0.23068534][0.2329426 0.24249819 0.27331 0.3410717 0.44687533 0.56451297 0.64693117 0.65042603 0.57979482 0.46712607 0.34978327 0.26961049 0.2279643 0.21657063 0.20765229][0.22534908 0.22062457 0.2352123 0.28993058 0.38918397 0.50982749 0.6040833 0.62369531 0.56869423 0.46815956 0.357136 0.27559412 0.22755952 0.20711958 0.19157565][0.23484582 0.21653186 0.20961973 0.23823789 0.31243095 0.41706306 0.50894868 0.53943872 0.50371319 0.42509207 0.33377504 0.26584619 0.22592038 0.20699431 0.19058061][0.27382058 0.2457187 0.21718727 0.21463515 0.25452572 0.33111751 0.40798706 0.43970516 0.4183521 0.36149642 0.2945644 0.25000224 0.23048371 0.22457945 0.21484131][0.34718555 0.31464633 0.26801834 0.23626545 0.24238314 0.28864241 0.34439203 0.36891368 0.35362348 0.31199819 0.26650804 0.24743931 0.25263259 0.26422158 0.26289478][0.43036494 0.39964491 0.34132197 0.28733921 0.26651955 0.2872639 0.32286307 0.33718187 0.32289514 0.29076064 0.26205519 0.26544723 0.29232889 0.31848377 0.32229063][0.48440811 0.4593451 0.39706552 0.33077478 0.29288754 0.29633671 0.31749728 0.3239353 0.30994216 0.28427443 0.26777792 0.28708255 0.32776418 0.36148757 0.36599079][0.4695558 0.45062134 0.39181533 0.32416812 0.27959603 0.27351764 0.28593764 0.2881906 0.27604681 0.25641248 0.24876234 0.27650961 0.32154241 0.35502622 0.35680419][0.38354751 0.36948198 0.31913161 0.25863937 0.21555696 0.2053434 0.21236396 0.21271311 0.20355226 0.18999803 0.18825141 0.21681267 0.25727928 0.28430605 0.2821528]]...]
INFO - root - 2017-12-11 08:02:22.193902: step 36510, loss = 0.69, batch loss = 0.63 (14.7 examples/sec; 0.544 sec/batch; 44h:41m:43s remains)
INFO - root - 2017-12-11 08:02:27.691421: step 36520, loss = 0.69, batch loss = 0.63 (14.6 examples/sec; 0.549 sec/batch; 45h:07m:33s remains)
INFO - root - 2017-12-11 08:02:33.143079: step 36530, loss = 0.69, batch loss = 0.63 (14.4 examples/sec; 0.555 sec/batch; 45h:37m:33s remains)
INFO - root - 2017-12-11 08:02:38.665844: step 36540, loss = 0.71, batch loss = 0.65 (14.4 examples/sec; 0.556 sec/batch; 45h:41m:06s remains)
INFO - root - 2017-12-11 08:02:44.285726: step 36550, loss = 0.71, batch loss = 0.65 (14.7 examples/sec; 0.543 sec/batch; 44h:39m:26s remains)
INFO - root - 2017-12-11 08:02:49.755187: step 36560, loss = 0.69, batch loss = 0.63 (14.6 examples/sec; 0.547 sec/batch; 44h:58m:58s remains)
INFO - root - 2017-12-11 08:02:55.252894: step 36570, loss = 0.70, batch loss = 0.64 (14.6 examples/sec; 0.549 sec/batch; 45h:06m:05s remains)
INFO - root - 2017-12-11 08:03:00.765633: step 36580, loss = 0.70, batch loss = 0.64 (15.0 examples/sec; 0.534 sec/batch; 43h:54m:51s remains)
INFO - root - 2017-12-11 08:03:05.935403: step 36590, loss = 0.71, batch loss = 0.65 (14.1 examples/sec; 0.569 sec/batch; 46h:44m:57s remains)
INFO - root - 2017-12-11 08:03:11.461620: step 36600, loss = 0.69, batch loss = 0.63 (14.8 examples/sec; 0.541 sec/batch; 44h:26m:05s remains)
2017-12-11 08:03:12.078216: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0469256 -0.042681526 -0.03798433 -0.035871882 -0.03756953 -0.04263268 -0.048823185 -0.053539921 -0.055999454 -0.056564018 -0.055972096 -0.054388583 -0.052047636 -0.050472334 -0.050724816][-0.035852894 -0.023727478 -0.0094742095 0.0004037905 0.0015026322 -0.0066987453 -0.019022839 -0.030390691 -0.039933655 -0.047026459 -0.051154025 -0.05164583 -0.049299072 -0.047163025 -0.047751855][-0.017180325 0.0079564359 0.038832065 0.063203596 0.070841834 0.060193285 0.040900368 0.021270055 0.0011571413 -0.017956117 -0.031932507 -0.037930459 -0.038098678 -0.037451863 -0.039827768][0.0049462281 0.047304247 0.10072735 0.14527993 0.16387162 0.15469688 0.13250387 0.10800641 0.077737011 0.043834452 0.015953051 5.5027012e-05 -0.0079455795 -0.013764159 -0.021091569][0.028376017 0.089836933 0.16741963 0.23340073 0.26499081 0.2616283 0.243528 0.22128989 0.18538071 0.13871884 0.0970571 0.068967909 0.047122527 0.02741654 0.0099319955][0.047818627 0.12574302 0.22256817 0.304858 0.34719759 0.35219833 0.34494 0.334248 0.30310574 0.25483865 0.2083745 0.17098722 0.131452 0.090495855 0.056839772][0.058085956 0.14592783 0.25245455 0.3414661 0.38840237 0.40024248 0.40612376 0.41300577 0.39745224 0.36322322 0.32750925 0.29020271 0.23496565 0.16982716 0.11590333][0.060341045 0.1513278 0.25842932 0.34537151 0.39038852 0.40393838 0.417855 0.43895119 0.44112813 0.43002364 0.41746604 0.39173421 0.33027515 0.24728517 0.17581278][0.055531956 0.14286946 0.24231969 0.32017782 0.35822725 0.36811054 0.38213474 0.40832233 0.42232344 0.43310121 0.44673488 0.44037092 0.38580057 0.29910257 0.21964559][0.041095775 0.11715074 0.20171617 0.26595819 0.29520473 0.29940718 0.30760536 0.32865602 0.34329104 0.36446041 0.39520815 0.4071888 0.36967587 0.29534093 0.22184019][0.015650162 0.073242605 0.13748792 0.18607193 0.207678 0.20847552 0.21098599 0.22286913 0.23029964 0.24866502 0.28188705 0.30359328 0.28527516 0.23330137 0.17729048][-0.014296746 0.021903612 0.064014547 0.096613616 0.11179276 0.11196378 0.11214455 0.11678506 0.11568868 0.12342701 0.14628994 0.16516182 0.15811451 0.12686931 0.09200979][-0.040040411 -0.023369161 -0.001555061 0.015973201 0.024856022 0.025350126 0.025935359 0.027571008 0.022113603 0.020188436 0.028662339 0.036607079 0.030777486 0.011737712 -0.0067557013][-0.0588273 -0.057470679 -0.052449234 -0.048523493 -0.046901867 -0.047686495 -0.046668559 -0.044875756 -0.048737537 -0.052694652 -0.051922597 -0.051869966 -0.058809906 -0.0716323 -0.080993839][-0.068391226 -0.076333977 -0.0820607 -0.087681718 -0.092398867 -0.095744096 -0.095633946 -0.093328923 -0.093142182 -0.093373716 -0.092022426 -0.092109561 -0.096416481 -0.10374453 -0.10805289]]...]
INFO - root - 2017-12-11 08:03:17.608572: step 36610, loss = 0.69, batch loss = 0.63 (14.7 examples/sec; 0.546 sec/batch; 44h:51m:21s remains)
INFO - root - 2017-12-11 08:03:23.068551: step 36620, loss = 0.69, batch loss = 0.63 (15.6 examples/sec; 0.511 sec/batch; 42h:01m:37s remains)
INFO - root - 2017-12-11 08:03:28.600807: step 36630, loss = 0.71, batch loss = 0.65 (14.5 examples/sec; 0.550 sec/batch; 45h:14m:04s remains)
INFO - root - 2017-12-11 08:03:34.070287: step 36640, loss = 0.70, batch loss = 0.65 (14.8 examples/sec; 0.539 sec/batch; 44h:19m:38s remains)
INFO - root - 2017-12-11 08:03:39.596259: step 36650, loss = 0.71, batch loss = 0.65 (14.5 examples/sec; 0.553 sec/batch; 45h:26m:34s remains)
INFO - root - 2017-12-11 08:03:45.151233: step 36660, loss = 0.71, batch loss = 0.65 (14.7 examples/sec; 0.544 sec/batch; 44h:44m:14s remains)
INFO - root - 2017-12-11 08:03:50.655166: step 36670, loss = 0.68, batch loss = 0.62 (14.5 examples/sec; 0.550 sec/batch; 45h:11m:37s remains)
INFO - root - 2017-12-11 08:03:56.105695: step 36680, loss = 0.70, batch loss = 0.64 (14.3 examples/sec; 0.558 sec/batch; 45h:50m:27s remains)
INFO - root - 2017-12-11 08:04:01.242396: step 36690, loss = 0.69, batch loss = 0.63 (15.0 examples/sec; 0.532 sec/batch; 43h:41m:46s remains)
INFO - root - 2017-12-11 08:04:06.704642: step 36700, loss = 0.68, batch loss = 0.62 (14.6 examples/sec; 0.549 sec/batch; 45h:08m:54s remains)
2017-12-11 08:04:07.406621: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.28007695 0.26253164 0.23783778 0.23352242 0.25587964 0.29459113 0.331411 0.35593715 0.35749415 0.33324373 0.29877979 0.27592188 0.27363309 0.2790035 0.27854994][0.37309951 0.35996029 0.33073142 0.32037091 0.33922559 0.37855127 0.41693106 0.4405947 0.43770027 0.40447697 0.35799056 0.32216731 0.31053013 0.31245777 0.31374124][0.42037734 0.41990677 0.39694759 0.38851264 0.40710348 0.44581464 0.48001185 0.49299249 0.47494289 0.42506698 0.36257955 0.31341958 0.29394013 0.29504165 0.30123258][0.42238507 0.44503593 0.446673 0.45869887 0.49160084 0.53818375 0.56861049 0.56271297 0.51594347 0.4360649 0.34834403 0.28220779 0.25563636 0.25998724 0.27491876][0.37854823 0.43137228 0.4720605 0.52141309 0.58495939 0.65268356 0.686948 0.66322464 0.58306658 0.46454549 0.34182844 0.25213042 0.21675803 0.22620204 0.25197175][0.31382936 0.39463937 0.47429231 0.5630942 0.66159171 0.7555629 0.79942673 0.76471466 0.65777934 0.5039525 0.34581515 0.23127638 0.18755724 0.20385891 0.24171653][0.25652418 0.3513957 0.45280746 0.56522137 0.68564487 0.79606324 0.84694314 0.80927813 0.69198465 0.52130985 0.34468162 0.21986146 0.17996572 0.21167983 0.26591203][0.20847702 0.29957125 0.39997646 0.51294094 0.63342506 0.74229431 0.7929247 0.76008183 0.652423 0.49150723 0.32561266 0.21757929 0.20213667 0.26131219 0.33655468][0.16196291 0.23432784 0.31575435 0.41020066 0.5122416 0.60520953 0.65067112 0.62938732 0.54748869 0.41982833 0.29326895 0.22845301 0.25422043 0.34495625 0.43629646][0.11864156 0.16466202 0.21720879 0.28168336 0.35367885 0.42150885 0.45755425 0.4496206 0.40061033 0.31950042 0.2497474 0.24250712 0.31324553 0.42754036 0.52192408][0.088440739 0.10895537 0.13209803 0.16468275 0.20409389 0.24443147 0.26836482 0.2698479 0.24905436 0.21171111 0.19596463 0.23984888 0.34337813 0.46657607 0.5525288][0.068879552 0.0704832 0.07057932 0.076929756 0.088743754 0.10487575 0.11569443 0.11961141 0.11557598 0.10807566 0.12827304 0.20157008 0.31689495 0.43440288 0.50667244][0.042687021 0.034466073 0.022550812 0.014514351 0.010628632 0.011612557 0.012333924 0.013547409 0.014292764 0.018346531 0.049632929 0.1249094 0.23004743 0.3300727 0.387238][-0.00021767427 -0.01144732 -0.025718747 -0.03728921 -0.045752265 -0.05040415 -0.054446787 -0.056547668 -0.057257418 -0.054002512 -0.029795067 0.027031602 0.10551206 0.17903678 0.21945643][-0.04509544 -0.055977255 -0.067736283 -0.077328995 -0.084539749 -0.089351967 -0.094295613 -0.098120078 -0.1008998 -0.10133313 -0.088820837 -0.055596974 -0.0081082908 0.036146861 0.058433529]]...]
INFO - root - 2017-12-11 08:04:12.835413: step 36710, loss = 0.69, batch loss = 0.63 (14.9 examples/sec; 0.539 sec/batch; 44h:15m:21s remains)
INFO - root - 2017-12-11 08:04:18.365823: step 36720, loss = 0.68, batch loss = 0.62 (14.8 examples/sec; 0.541 sec/batch; 44h:28m:28s remains)
INFO - root - 2017-12-11 08:04:23.909694: step 36730, loss = 0.70, batch loss = 0.64 (13.9 examples/sec; 0.577 sec/batch; 47h:23m:21s remains)
INFO - root - 2017-12-11 08:04:29.441968: step 36740, loss = 0.70, batch loss = 0.64 (14.5 examples/sec; 0.552 sec/batch; 45h:18m:38s remains)
INFO - root - 2017-12-11 08:04:34.848875: step 36750, loss = 0.70, batch loss = 0.64 (14.7 examples/sec; 0.543 sec/batch; 44h:37m:09s remains)
INFO - root - 2017-12-11 08:04:40.442665: step 36760, loss = 0.70, batch loss = 0.64 (14.5 examples/sec; 0.553 sec/batch; 45h:25m:09s remains)
INFO - root - 2017-12-11 08:04:45.929679: step 36770, loss = 0.69, batch loss = 0.63 (15.3 examples/sec; 0.522 sec/batch; 42h:55m:15s remains)
INFO - root - 2017-12-11 08:04:51.399303: step 36780, loss = 0.69, batch loss = 0.63 (14.6 examples/sec; 0.550 sec/batch; 45h:08m:52s remains)
INFO - root - 2017-12-11 08:04:56.603699: step 36790, loss = 0.70, batch loss = 0.64 (14.7 examples/sec; 0.545 sec/batch; 44h:46m:16s remains)
INFO - root - 2017-12-11 08:05:02.094299: step 36800, loss = 0.69, batch loss = 0.63 (14.5 examples/sec; 0.550 sec/batch; 45h:10m:08s remains)
2017-12-11 08:05:02.679453: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.086344421 -0.091450505 -0.094576649 -0.10234266 -0.11230285 -0.12320011 -0.13361946 -0.14073655 -0.14295274 -0.1371735 -0.12002767 -0.095384255 -0.07494884 -0.072279893 -0.077503964][-0.089865342 -0.086779624 -0.081946872 -0.082637437 -0.089501433 -0.10160419 -0.11855716 -0.13918775 -0.15841193 -0.17199482 -0.17183121 -0.15781885 -0.1403089 -0.13429868 -0.13923168][-0.043168843 -0.021515161 0.0050931037 0.026694242 0.036238413 0.032331429 0.012869097 -0.023515023 -0.067192465 -0.10728879 -0.12999612 -0.13049549 -0.11784602 -0.1088951 -0.11382405][0.028922815 0.075501852 0.13409655 0.19125813 0.23222047 0.25031614 0.23801447 0.1888348 0.11862052 0.050544169 0.0074292417 -0.0027033121 0.00955584 0.023579447 0.016090516][0.09729927 0.16907148 0.26159024 0.35901082 0.44059584 0.4925068 0.49812174 0.44329512 0.35045433 0.26065341 0.20841955 0.20271264 0.22663081 0.25197715 0.24145684][0.13836128 0.22701834 0.34454635 0.47534153 0.59678692 0.68759942 0.71984649 0.67027152 0.56567478 0.46532738 0.41647109 0.42730391 0.47315198 0.51600587 0.50345945][0.15426712 0.24450022 0.36708796 0.51089042 0.65723181 0.77992094 0.84021425 0.80651581 0.70516604 0.60696489 0.56720382 0.59425503 0.66089207 0.72185051 0.71040159][0.16023755 0.23644957 0.34115085 0.47148457 0.61614555 0.7484346 0.82500404 0.81133807 0.72871256 0.64584237 0.61631864 0.65003395 0.72558451 0.79660279 0.78796154][0.16369657 0.21584213 0.28640676 0.38118461 0.49659792 0.610188 0.68313187 0.68533731 0.63112116 0.57419407 0.55589962 0.58654016 0.65475136 0.72097039 0.71339273][0.16286145 0.18961698 0.22298151 0.27398494 0.34468147 0.41963002 0.47141623 0.48005605 0.45396459 0.4261966 0.41981748 0.4417206 0.48932877 0.53608477 0.52689904][0.15084222 0.15850671 0.16346861 0.17751884 0.20455356 0.23667613 0.260389 0.26762256 0.26341531 0.26123375 0.26606479 0.27819091 0.29980478 0.31918508 0.3054018][0.12359854 0.12151939 0.11396528 0.10904789 0.10857581 0.10965706 0.11004497 0.11122256 0.1175451 0.12997013 0.14194566 0.1472588 0.14840022 0.14486866 0.12780021][0.086212263 0.081559666 0.075054623 0.06938085 0.062348172 0.052282359 0.041991808 0.037164439 0.043034483 0.056855891 0.0702083 0.074215047 0.068529442 0.057253391 0.043191142][0.055886775 0.051098246 0.05060225 0.052279923 0.051293571 0.045363411 0.036460955 0.029022966 0.0288179 0.035360772 0.045160715 0.051675994 0.051658861 0.047834061 0.043685939][0.052541059 0.047830485 0.050354958 0.057384506 0.063743711 0.066500261 0.063524708 0.055341121 0.047480617 0.043241207 0.046055883 0.054792732 0.064646535 0.073497161 0.080031224]]...]
INFO - root - 2017-12-11 08:05:08.189762: step 36810, loss = 0.70, batch loss = 0.64 (14.3 examples/sec; 0.561 sec/batch; 46h:05m:56s remains)
INFO - root - 2017-12-11 08:05:13.652461: step 36820, loss = 0.71, batch loss = 0.66 (15.0 examples/sec; 0.535 sec/batch; 43h:54m:47s remains)
INFO - root - 2017-12-11 08:05:19.262396: step 36830, loss = 0.71, batch loss = 0.66 (14.0 examples/sec; 0.570 sec/batch; 46h:51m:05s remains)
INFO - root - 2017-12-11 08:05:24.781880: step 36840, loss = 0.69, batch loss = 0.64 (14.8 examples/sec; 0.542 sec/batch; 44h:30m:33s remains)
INFO - root - 2017-12-11 08:05:30.315523: step 36850, loss = 0.69, batch loss = 0.63 (13.8 examples/sec; 0.581 sec/batch; 47h:40m:32s remains)
INFO - root - 2017-12-11 08:05:35.921859: step 36860, loss = 0.68, batch loss = 0.62 (14.1 examples/sec; 0.568 sec/batch; 46h:38m:40s remains)
INFO - root - 2017-12-11 08:05:41.358636: step 36870, loss = 0.69, batch loss = 0.63 (15.0 examples/sec; 0.534 sec/batch; 43h:53m:28s remains)
INFO - root - 2017-12-11 08:05:46.481681: step 36880, loss = 0.68, batch loss = 0.62 (14.5 examples/sec; 0.553 sec/batch; 45h:23m:50s remains)
INFO - root - 2017-12-11 08:05:51.957945: step 36890, loss = 0.70, batch loss = 0.64 (14.7 examples/sec; 0.545 sec/batch; 44h:43m:14s remains)
INFO - root - 2017-12-11 08:05:57.422035: step 36900, loss = 0.70, batch loss = 0.64 (14.9 examples/sec; 0.536 sec/batch; 44h:01m:26s remains)
2017-12-11 08:05:58.021388: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.027697107 -0.019477494 -0.0041353744 0.016470456 0.038289592 0.050346546 0.047473226 0.044596516 0.062817454 0.10748121 0.16481949 0.20964377 0.22894612 0.22641848 0.20819487][0.00073459628 -0.0074375994 -0.0088534169 -0.0038296434 0.0057136654 0.013827489 0.016416157 0.023798015 0.051751051 0.10140423 0.15864253 0.19875658 0.21284425 0.20662701 0.18756318][0.050981466 0.033927329 0.021669747 0.017301351 0.020627778 0.029685451 0.039303072 0.053909775 0.084201954 0.12917696 0.17567982 0.20201732 0.20403929 0.18998227 0.16779737][0.10202106 0.086640909 0.074183866 0.0701422 0.075531282 0.0910936 0.10916546 0.12849143 0.15635298 0.19202049 0.22356865 0.23122077 0.21673292 0.19130464 0.1637321][0.13214791 0.12955511 0.12881255 0.1354443 0.15086283 0.17684598 0.20266199 0.22300303 0.24421848 0.26789427 0.28321096 0.271316 0.2385948 0.20137605 0.16797687][0.13093986 0.14715926 0.16666245 0.19220011 0.22321171 0.26036328 0.29052153 0.30699134 0.31739154 0.32649934 0.32529581 0.29659948 0.25060007 0.20612468 0.16983081][0.11164293 0.14659351 0.18873291 0.23526131 0.28104165 0.32437697 0.35143742 0.35738921 0.35277429 0.34558848 0.329886 0.29160368 0.24299391 0.20203803 0.17145276][0.095326558 0.14381003 0.20418179 0.26727641 0.32199112 0.36366248 0.38008866 0.37109205 0.35021129 0.32791102 0.30188978 0.26241535 0.22171605 0.19372444 0.17664167][0.092579126 0.14623043 0.21595286 0.28718695 0.34371313 0.37858471 0.3833558 0.36312523 0.33190656 0.300378 0.26985103 0.23600107 0.20870246 0.19616069 0.19350354][0.10390644 0.15538807 0.22560857 0.29660195 0.34875327 0.3746489 0.3716628 0.3487505 0.31657436 0.28352854 0.25439623 0.23006533 0.21590927 0.21401064 0.21947587][0.12236157 0.16705976 0.23112249 0.29532704 0.33922294 0.35712352 0.35237789 0.33536428 0.31078082 0.28370449 0.26132339 0.24695234 0.24051677 0.23970288 0.24316098][0.13868247 0.17484067 0.22939411 0.28397611 0.31940529 0.33217084 0.32970327 0.32177037 0.30791453 0.29015222 0.27619979 0.26870957 0.2628282 0.25538951 0.24981342][0.14513011 0.17136677 0.2142186 0.25860548 0.28754726 0.29869235 0.3010121 0.302602 0.29936263 0.29050687 0.28267634 0.27681163 0.26583543 0.24820331 0.23117733][0.14142002 0.15707956 0.18680054 0.2205482 0.24394818 0.25456676 0.26094824 0.26985419 0.27514794 0.27313337 0.268224 0.26041204 0.2435106 0.21800938 0.19234708][0.13186829 0.13776939 0.15409227 0.17663629 0.19364588 0.20187363 0.20832393 0.21990724 0.23027433 0.2330496 0.22949134 0.21936311 0.19931614 0.17124191 0.14257865]]...]
INFO - root - 2017-12-11 08:06:03.464498: step 36910, loss = 0.69, batch loss = 0.63 (14.6 examples/sec; 0.549 sec/batch; 45h:06m:02s remains)
INFO - root - 2017-12-11 08:06:08.936066: step 36920, loss = 0.69, batch loss = 0.63 (14.5 examples/sec; 0.552 sec/batch; 45h:18m:23s remains)
INFO - root - 2017-12-11 08:06:14.396268: step 36930, loss = 0.70, batch loss = 0.65 (14.7 examples/sec; 0.543 sec/batch; 44h:33m:05s remains)
INFO - root - 2017-12-11 08:06:19.849824: step 36940, loss = 0.70, batch loss = 0.64 (14.4 examples/sec; 0.556 sec/batch; 45h:37m:41s remains)
INFO - root - 2017-12-11 08:06:25.453059: step 36950, loss = 0.70, batch loss = 0.64 (14.2 examples/sec; 0.564 sec/batch; 46h:17m:56s remains)
INFO - root - 2017-12-11 08:06:30.927165: step 36960, loss = 0.71, batch loss = 0.65 (14.7 examples/sec; 0.546 sec/batch; 44h:49m:36s remains)
INFO - root - 2017-12-11 08:06:36.330219: step 36970, loss = 0.71, batch loss = 0.65 (14.7 examples/sec; 0.545 sec/batch; 44h:42m:41s remains)
INFO - root - 2017-12-11 08:06:41.494023: step 36980, loss = 0.69, batch loss = 0.63 (14.6 examples/sec; 0.548 sec/batch; 45h:01m:16s remains)
INFO - root - 2017-12-11 08:06:47.014350: step 36990, loss = 0.70, batch loss = 0.64 (14.4 examples/sec; 0.554 sec/batch; 45h:29m:53s remains)
INFO - root - 2017-12-11 08:06:52.649612: step 37000, loss = 0.68, batch loss = 0.62 (12.8 examples/sec; 0.626 sec/batch; 51h:24m:22s remains)
2017-12-11 08:06:53.205136: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.074164279 0.082512684 0.09476795 0.10847402 0.11469805 0.11895119 0.13636456 0.16400042 0.18494138 0.19293351 0.19359843 0.19812638 0.21010159 0.22781144 0.25242186][0.09433724 0.12380318 0.15199223 0.17128028 0.1714099 0.16275734 0.16982405 0.19667204 0.22385021 0.2395785 0.24337751 0.24465199 0.24999952 0.25996542 0.27646139][0.1155098 0.16517451 0.20913751 0.2343033 0.22985035 0.20906059 0.20335749 0.22581488 0.25865102 0.28599507 0.29850233 0.30152351 0.30508721 0.30951759 0.31484455][0.12082145 0.1831976 0.23819891 0.26927486 0.26597914 0.24133828 0.22814313 0.24672823 0.28408852 0.32276273 0.34604612 0.35598585 0.36525097 0.37014115 0.36681968][0.1052772 0.17061488 0.23121907 0.26936287 0.27587184 0.26009196 0.24949886 0.26704061 0.30530646 0.34849772 0.37666777 0.39095554 0.40680721 0.41642481 0.40977889][0.077300906 0.1388101 0.2030731 0.25223833 0.27773398 0.28252798 0.28416693 0.30316329 0.3373788 0.3748844 0.39670312 0.405633 0.42022824 0.43147054 0.42483547][0.061356839 0.1191135 0.19011612 0.2550866 0.30398715 0.33298051 0.34879953 0.3667267 0.38928047 0.41003567 0.41349655 0.40477738 0.40497831 0.40865895 0.4007698][0.074194148 0.12916107 0.20872395 0.2906661 0.3605181 0.40782979 0.43083608 0.44087285 0.44467476 0.44164917 0.42010379 0.38693491 0.36438727 0.35242963 0.33839211][0.11537261 0.16518492 0.25021744 0.34376046 0.42499271 0.47792929 0.49669412 0.49182314 0.47372833 0.44631839 0.40091547 0.34463343 0.29839426 0.26715112 0.24305035][0.1629938 0.20064738 0.28084365 0.37375867 0.45269608 0.49819452 0.50399524 0.48191059 0.4451409 0.400475 0.3415024 0.27368242 0.21403682 0.17030637 0.13968562][0.18896921 0.20670468 0.26992971 0.34993246 0.41632223 0.44816834 0.44032794 0.40586695 0.35946766 0.30961984 0.25182858 0.18897043 0.13254717 0.089768521 0.061286692][0.18181993 0.17651072 0.21539743 0.27522528 0.325475 0.34555626 0.33089387 0.29400977 0.24925441 0.20574546 0.16107093 0.11596894 0.076774389 0.04823108 0.032134309][0.14727245 0.12365148 0.14028886 0.18058333 0.21741794 0.23234725 0.22042437 0.191558 0.15841089 0.12986216 0.10522558 0.083477832 0.066906877 0.057756551 0.0578565][0.10615075 0.076066077 0.0802029 0.10847024 0.13889195 0.15642317 0.15529709 0.14131507 0.1242535 0.11219116 0.10571171 0.10253226 0.10253695 0.10710943 0.11852566][0.086158469 0.062700041 0.065000445 0.088579625 0.11677897 0.13926254 0.15034784 0.15159974 0.14863928 0.14816733 0.15171055 0.15649849 0.16203587 0.17035858 0.18446782]]...]
INFO - root - 2017-12-11 08:06:58.704144: step 37010, loss = 0.71, batch loss = 0.65 (14.8 examples/sec; 0.539 sec/batch; 44h:14m:27s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-SGD-lr0.001-clip50-initfromfixqian30k-fixqian
INFO - root - 2017-12-11 08:07:04.298631: step 37020, loss = 0.69, batch loss = 0.64 (14.7 examples/sec; 0.545 sec/batch; 44h:42m:18s remains)
INFO - root - 2017-12-11 08:07:09.851226: step 37030, loss = 0.69, batch loss = 0.63 (14.5 examples/sec; 0.550 sec/batch; 45h:09m:16s remains)
INFO - root - 2017-12-11 08:07:15.425751: step 37040, loss = 0.71, batch loss = 0.65 (14.2 examples/sec; 0.564 sec/batch; 46h:19m:33s remains)
INFO - root - 2017-12-11 08:07:20.959263: step 37050, loss = 0.70, batch loss = 0.64 (14.5 examples/sec; 0.553 sec/batch; 45h:24m:20s remains)
INFO - root - 2017-12-11 08:07:26.439405: step 37060, loss = 0.70, batch loss = 0.64 (14.3 examples/sec; 0.560 sec/batch; 45h:54m:59s remains)
INFO - root - 2017-12-11 08:07:31.954189: step 37070, loss = 0.70, batch loss = 0.65 (14.6 examples/sec; 0.548 sec/batch; 45h:00m:13s remains)
INFO - root - 2017-12-11 08:07:37.237341: step 37080, loss = 0.69, batch loss = 0.63 (14.7 examples/sec; 0.544 sec/batch; 44h:36m:56s remains)
INFO - root - 2017-12-11 08:07:42.762565: step 37090, loss = 0.70, batch loss = 0.64 (14.9 examples/sec; 0.536 sec/batch; 43h:58m:07s remains)
INFO - root - 2017-12-11 08:07:48.207850: step 37100, loss = 0.69, batch loss = 0.63 (14.8 examples/sec; 0.541 sec/batch; 44h:25m:18s remains)
2017-12-11 08:07:48.832340: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.035976976 0.072736 0.13262652 0.212364 0.29617125 0.36945033 0.42383528 0.46409929 0.49171305 0.49501204 0.46184093 0.38761154 0.29232723 0.20492364 0.14906132][-0.031391978 -0.018112367 0.015105844 0.06953302 0.13765858 0.21193759 0.28627881 0.35814682 0.41653445 0.44169149 0.42054448 0.35015127 0.25085634 0.15267064 0.083220586][-0.032727718 -0.0264348 -0.011032448 0.012791188 0.044224206 0.085660785 0.13975503 0.20515829 0.26735839 0.30484602 0.30242136 0.2536397 0.17279175 0.086139008 0.020541416][0.041179609 0.069688611 0.09564472 0.10805478 0.1061905 0.099039406 0.098830737 0.11308785 0.13730672 0.15929869 0.16309509 0.1354672 0.080911078 0.018431611 -0.02998678][0.1662391 0.24768661 0.31846133 0.35011333 0.33480784 0.28356048 0.21486709 0.14979425 0.10418027 0.083281487 0.071470633 0.047397625 0.0071035768 -0.036065377 -0.06568981][0.30898613 0.45643604 0.59066659 0.66495389 0.65998983 0.58392632 0.45615867 0.31043532 0.18571974 0.10725013 0.061794721 0.022212403 -0.019857012 -0.054538187 -0.071065269][0.42692521 0.62766337 0.81756526 0.93852359 0.96118152 0.88611025 0.72802824 0.52771658 0.34195969 0.21115977 0.12568444 0.058172274 0.0015236359 -0.034922365 -0.04603586][0.46143588 0.68003803 0.89402175 1.0453374 1.0995978 1.0476751 0.89546782 0.68309557 0.47498339 0.3179853 0.20564717 0.1141213 0.041379306 -0.0013044587 -0.012455933][0.39179191 0.58539879 0.782763 0.93615061 1.0118589 0.99210066 0.87371564 0.69100946 0.504659 0.35794064 0.24543358 0.14884016 0.071956635 0.028216386 0.017395798][0.23774858 0.37467849 0.52310234 0.65050423 0.72840434 0.73450714 0.65985471 0.53095317 0.39661604 0.28867117 0.20099837 0.12144677 0.057777636 0.02390857 0.018804658][0.062529378 0.13480988 0.22244874 0.30717513 0.36817113 0.38488069 0.34751758 0.27423111 0.19788936 0.13656065 0.084452562 0.035786469 -0.00057913974 -0.013582543 -0.0061994861][-0.072320834 -0.052340526 -0.017309515 0.023971241 0.059107974 0.073915623 0.061420016 0.03255675 0.0045654825 -0.016731191 -0.03678783 -0.0561413 -0.066488221 -0.060703125 -0.043044336][-0.14148249 -0.15107407 -0.14915298 -0.13989973 -0.1283595 -0.12114306 -0.12136605 -0.12397193 -0.12318671 -0.12012874 -0.11932909 -0.11905745 -0.11308964 -0.097821936 -0.077083148][-0.15343772 -0.1726425 -0.18389787 -0.19064517 -0.19372062 -0.19436115 -0.19246784 -0.18641132 -0.17602591 -0.16437845 -0.15448874 -0.14466749 -0.13156219 -0.11446441 -0.096296713][-0.13139482 -0.14809319 -0.15908821 -0.1681681 -0.17510776 -0.17936254 -0.17983851 -0.17586432 -0.16811128 -0.15854715 -0.14874603 -0.13802893 -0.12551372 -0.11197391 -0.099411391]]...]
INFO - root - 2017-12-11 08:07:54.265625: step 37110, loss = 0.69, batch loss = 0.63 (14.7 examples/sec; 0.546 sec/batch; 44h:47m:05s remains)
INFO - root - 2017-12-11 08:07:59.775774: step 37120, loss = 0.68, batch loss = 0.63 (14.1 examples/sec; 0.566 sec/batch; 46h:27m:13s remains)
INFO - root - 2017-12-11 08:08:05.374922: step 37130, loss = 0.69, batch loss = 0.63 (14.3 examples/sec; 0.558 sec/batch; 45h:46m:10s remains)
INFO - root - 2017-12-11 08:08:10.861395: step 37140, loss = 0.70, batch loss = 0.64 (14.8 examples/sec; 0.541 sec/batch; 44h:24m:11s remains)
INFO - root - 2017-12-11 08:08:16.393639: step 37150, loss = 0.70, batch loss = 0.64 (13.7 examples/sec; 0.586 sec/batch; 48h:02m:16s remains)
INFO - root - 2017-12-11 08:08:21.942444: step 37160, loss = 0.70, batch loss = 0.64 (14.0 examples/sec; 0.571 sec/batch; 46h:52m:28s remains)
INFO - root - 2017-12-11 08:08:27.145045: step 37170, loss = 0.70, batch loss = 0.64 (30.4 examples/sec; 0.263 sec/batch; 21h:33m:56s remains)
INFO - root - 2017-12-11 08:08:32.715554: step 37180, loss = 0.70, batch loss = 0.64 (14.5 examples/sec; 0.552 sec/batch; 45h:18m:00s remains)
INFO - root - 2017-12-11 08:08:38.203572: step 37190, loss = 0.68, batch loss = 0.63 (14.7 examples/sec; 0.544 sec/batch; 44h:37m:52s remains)
INFO - root - 2017-12-11 08:08:43.684713: step 37200, loss = 0.71, batch loss = 0.65 (14.6 examples/sec; 0.548 sec/batch; 44h:59m:06s remains)
2017-12-11 08:08:44.296597: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.0829707 0.081669882 0.067269854 0.046148103 0.024327647 0.0076970458 0.000695761 0.0031847907 0.01139453 0.022812856 0.033747833 0.040955745 0.041081309 0.029647011 0.0060532363][0.18371995 0.18841004 0.17054084 0.14112885 0.1094183 0.085252836 0.0756511 0.079180866 0.088562384 0.10046498 0.11127874 0.11666606 0.11179329 0.090635665 0.05268728][0.27321166 0.28622472 0.26898423 0.23646329 0.20068647 0.17489998 0.16756861 0.17424811 0.1837129 0.19171607 0.19649397 0.19454853 0.18059257 0.14758569 0.095188767][0.32433107 0.3463628 0.33475184 0.307082 0.27669349 0.25773263 0.25804085 0.26943311 0.27757227 0.27747431 0.27037 0.25593576 0.22988653 0.18495922 0.12079258][0.33743843 0.36603487 0.36178213 0.34360102 0.32511568 0.31882581 0.3299562 0.34653693 0.35272154 0.34358242 0.32311675 0.29479983 0.25619382 0.20096648 0.12903672][0.33372822 0.36471012 0.36505765 0.35439175 0.34664527 0.35258979 0.37384737 0.39507979 0.40051827 0.38613266 0.35731819 0.31882516 0.2698614 0.20637666 0.12966853][0.33087456 0.36095089 0.36212054 0.35330018 0.3493818 0.36158106 0.38874865 0.41278243 0.419144 0.40586787 0.37761378 0.33616236 0.2809239 0.21140113 0.13172068][0.33053452 0.35703549 0.35517392 0.3423503 0.33447427 0.34523511 0.37247902 0.39678243 0.40535921 0.39871389 0.37918264 0.34238037 0.28639218 0.21478795 0.13496265][0.31463534 0.33580729 0.32875866 0.30936319 0.2941449 0.29922155 0.322623 0.34546906 0.357219 0.3597272 0.35213247 0.32431796 0.27328292 0.20534043 0.13005018][0.26270598 0.27693826 0.26494572 0.24068278 0.22036867 0.22061935 0.23991102 0.261516 0.27740064 0.28880042 0.29153121 0.27294567 0.23059486 0.17202507 0.10675475][0.17188057 0.17884371 0.16469334 0.14043115 0.12062766 0.11959271 0.13620777 0.15708096 0.17660327 0.19407962 0.20294593 0.19179612 0.16012366 0.11517134 0.06436833][0.065423779 0.065953664 0.053035181 0.033437785 0.018612895 0.018934755 0.03359291 0.053156406 0.073886283 0.09317185 0.10382046 0.098062277 0.077474661 0.047770549 0.013411698][-0.022358738 -0.026268544 -0.035784498 -0.048678633 -0.05722497 -0.055098169 -0.042689081 -0.025670854 -0.0064029894 0.011273471 0.02093651 0.018575506 0.0070948512 -0.0096016815 -0.029565424][-0.071430892 -0.077238478 -0.082663946 -0.089273877 -0.092746325 -0.089952342 -0.08088246 -0.068223141 -0.053200588 -0.039427977 -0.031881843 -0.03221425 -0.037505358 -0.04534591 -0.055197667][-0.0842265 -0.09020862 -0.092320316 -0.094593361 -0.095142707 -0.092763081 -0.087362252 -0.07976193 -0.070332959 -0.061392315 -0.05616121 -0.055379961 -0.057115566 -0.060013928 -0.0640127]]...]
INFO - root - 2017-12-11 08:08:49.848076: step 37210, loss = 0.67, batch loss = 0.61 (13.6 examples/sec; 0.586 sec/batch; 48h:05m:39s remains)
INFO - root - 2017-12-11 08:08:55.393568: step 37220, loss = 0.70, batch loss = 0.64 (14.2 examples/sec; 0.564 sec/batch; 46h:16m:42s remains)
INFO - root - 2017-12-11 08:09:00.903758: step 37230, loss = 0.71, batch loss = 0.65 (14.2 examples/sec; 0.563 sec/batch; 46h:12m:37s remains)
INFO - root - 2017-12-11 08:09:06.399152: step 37240, loss = 0.72, batch loss = 0.66 (15.2 examples/sec; 0.528 sec/batch; 43h:17m:00s remains)
INFO - root - 2017-12-11 08:09:11.841215: step 37250, loss = 0.69, batch loss = 0.63 (14.8 examples/sec; 0.540 sec/batch; 44h:15m:01s remains)
INFO - root - 2017-12-11 08:09:17.328119: step 37260, loss = 0.70, batch loss = 0.64 (14.1 examples/sec; 0.569 sec/batch; 46h:40m:41s remains)
INFO - root - 2017-12-11 08:09:22.496765: step 37270, loss = 0.69, batch loss = 0.64 (14.6 examples/sec; 0.548 sec/batch; 44h:54m:18s remains)
INFO - root - 2017-12-11 08:09:28.094378: step 37280, loss = 0.69, batch loss = 0.63 (14.5 examples/sec; 0.551 sec/batch; 45h:12m:46s remains)
INFO - root - 2017-12-11 08:09:33.558048: step 37290, loss = 0.70, batch loss = 0.64 (14.9 examples/sec; 0.536 sec/batch; 43h:57m:15s remains)
INFO - root - 2017-12-11 08:09:39.090840: step 37300, loss = 0.68, batch loss = 0.62 (14.4 examples/sec; 0.554 sec/batch; 45h:24m:23s remains)
2017-12-11 08:09:39.636414: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.012998181 -0.0077398303 0.0016726437 0.016212597 0.036797523 0.063257344 0.092780136 0.11463848 0.12003781 0.10790919 0.087398432 0.06865485 0.05708177 0.051398914 0.048728991][-0.0088256458 0.00250169 0.019364042 0.043734346 0.076807909 0.11711037 0.16025357 0.19414887 0.20796174 0.19914496 0.17708161 0.15291917 0.13552751 0.12497631 0.1180143][0.011290833 0.033472005 0.059633203 0.092900582 0.13577244 0.18611166 0.23835047 0.28057683 0.30148768 0.297496 0.27597103 0.24791718 0.22530892 0.21031356 0.19978149][0.056406617 0.096003711 0.13300796 0.17059781 0.21409686 0.26364374 0.31565529 0.36047572 0.38670248 0.38818112 0.36867771 0.33790225 0.30976027 0.28886557 0.27363148][0.11999097 0.18028675 0.22908318 0.26712129 0.30301026 0.34191269 0.38457173 0.4244473 0.45085666 0.45549497 0.43864781 0.40757373 0.37531969 0.34755853 0.32605603][0.19066843 0.26560634 0.32308251 0.35926226 0.38413179 0.408133 0.43735147 0.46873534 0.49267268 0.49880442 0.48447016 0.45373011 0.41643336 0.37889662 0.34802425][0.25479808 0.32816118 0.38487977 0.41632906 0.43003058 0.43865871 0.45299074 0.47403261 0.49459696 0.50372589 0.49593496 0.47029611 0.43132657 0.38476387 0.34315521][0.29423609 0.34817702 0.39115548 0.41253817 0.4157491 0.41170242 0.41294357 0.42397538 0.44136012 0.45571533 0.45936671 0.44569325 0.41247651 0.36404341 0.31666839][0.31338891 0.33697364 0.35418761 0.35836959 0.35027152 0.33685818 0.32848218 0.3312301 0.34537977 0.36462161 0.38042074 0.38174453 0.36085224 0.32001442 0.27623004][0.311852 0.3068586 0.29666206 0.28245234 0.26423571 0.24471943 0.22931753 0.22332765 0.23029685 0.24826841 0.27062568 0.28535932 0.28153396 0.25815582 0.22899508][0.28751969 0.266339 0.23910469 0.2137866 0.19016309 0.16786115 0.14703986 0.13082609 0.12479855 0.13216181 0.15104736 0.17264256 0.18539405 0.18449864 0.17786236][0.25146988 0.22623374 0.19615132 0.17209613 0.15220687 0.13281374 0.10902602 0.081731781 0.058354057 0.047541786 0.053507656 0.0729433 0.095262274 0.11216283 0.12570445][0.2190107 0.19818249 0.17622913 0.16338584 0.15496789 0.14318946 0.11994089 0.0849158 0.046763487 0.018050095 0.0078691524 0.018294381 0.040686652 0.064572394 0.088178471][0.19897456 0.18625602 0.17657875 0.17828713 0.18413965 0.18222502 0.16305958 0.12622775 0.080401555 0.039643932 0.01561448 0.014132546 0.029245256 0.050505679 0.074833371][0.18601242 0.18052931 0.17960487 0.19169068 0.20884058 0.21715583 0.20682466 0.17714146 0.1346646 0.091970786 0.060384925 0.047934532 0.051686373 0.063487552 0.081236906]]...]
INFO - root - 2017-12-11 08:09:45.067423: step 37310, loss = 0.69, batch loss = 0.63 (14.6 examples/sec; 0.549 sec/batch; 45h:00m:08s remains)
INFO - root - 2017-12-11 08:09:50.670054: step 37320, loss = 0.71, batch loss = 0.65 (14.5 examples/sec; 0.550 sec/batch; 45h:05m:08s remains)
INFO - root - 2017-12-11 08:09:56.161272: step 37330, loss = 0.71, batch loss = 0.65 (14.6 examples/sec; 0.547 sec/batch; 44h:53m:09s remains)
INFO - root - 2017-12-11 08:10:01.618224: step 37340, loss = 0.71, batch loss = 0.65 (14.9 examples/sec; 0.537 sec/batch; 43h:59m:26s remains)
INFO - root - 2017-12-11 08:10:07.168843: step 37350, loss = 0.70, batch loss = 0.64 (14.7 examples/sec; 0.543 sec/batch; 44h:30m:34s remains)
INFO - root - 2017-12-11 08:10:12.697262: step 37360, loss = 0.72, batch loss = 0.66 (14.6 examples/sec; 0.548 sec/batch; 44h:54m:53s remains)
INFO - root - 2017-12-11 08:10:17.954114: step 37370, loss = 0.71, batch loss = 0.65 (13.6 examples/sec; 0.586 sec/batch; 48h:03m:27s remains)
INFO - root - 2017-12-11 08:10:23.526005: step 37380, loss = 0.71, batch loss = 0.66 (14.6 examples/sec; 0.549 sec/batch; 44h:58m:25s remains)
INFO - root - 2017-12-11 08:10:29.001331: step 37390, loss = 0.69, batch loss = 0.63 (14.4 examples/sec; 0.555 sec/batch; 45h:27m:25s remains)
INFO - root - 2017-12-11 08:10:34.402010: step 37400, loss = 0.70, batch loss = 0.64 (14.8 examples/sec; 0.542 sec/batch; 44h:27m:28s remains)
2017-12-11 08:10:35.022873: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.13410515 0.10573026 0.0760268 0.053290144 0.039500911 0.027333189 0.014492032 0.008805546 0.023923988 0.057089984 0.10632654 0.16557722 0.23007144 0.28358003 0.31393233][0.072798938 0.046236042 0.019105179 -0.0022394487 -0.016454594 -0.027143644 -0.033780467 -0.0278169 0.0021971094 0.05044841 0.11434774 0.18202057 0.24679384 0.29558033 0.31872246][0.03078386 0.013264653 -0.0050255205 -0.020837765 -0.033093 -0.04123788 -0.042141903 -0.026865616 0.011543461 0.064209871 0.12733853 0.18740512 0.23883095 0.27431953 0.28984085][0.023995912 0.021322511 0.018414002 0.014387707 0.01007285 0.0090928124 0.015270352 0.037287276 0.076174572 0.120725 0.16710697 0.20426756 0.23183239 0.24917807 0.25850555][0.05658849 0.072843142 0.09080559 0.10582396 0.11795731 0.13076688 0.145774 0.17134616 0.20321409 0.22886333 0.24618837 0.2507371 0.24994588 0.24738789 0.25084004][0.12299145 0.16136381 0.203986 0.24318677 0.27776587 0.30820149 0.33149227 0.35634953 0.37452444 0.37473783 0.35772476 0.32842618 0.30201423 0.28257209 0.28123856][0.20730877 0.2679897 0.33463034 0.39683846 0.45253605 0.49824831 0.52650928 0.54668915 0.54713297 0.5211941 0.47390747 0.4193514 0.37782049 0.35072088 0.34990585][0.28495374 0.36089194 0.4419724 0.51839286 0.58749783 0.64157414 0.66986781 0.6819641 0.66423506 0.61717385 0.55104756 0.48555592 0.44295084 0.42109823 0.42943817][0.33445379 0.412133 0.49296996 0.57088882 0.6415785 0.69391191 0.71715057 0.72045338 0.68933254 0.63009065 0.55760652 0.49293932 0.45753279 0.44755751 0.4686746][0.34581867 0.41114497 0.47709388 0.54234111 0.60054797 0.63965178 0.65272659 0.64835751 0.61156547 0.55080146 0.48283547 0.42677736 0.400861 0.40159562 0.43122113][0.3174279 0.35975745 0.40118444 0.44364214 0.47951645 0.49846038 0.499566 0.49011794 0.45503742 0.40179881 0.3464691 0.30375037 0.28678405 0.29263166 0.32132795][0.2553831 0.27113867 0.28609735 0.30416769 0.31754696 0.31936005 0.31342524 0.30398142 0.2769435 0.23702842 0.19802763 0.1695549 0.15908754 0.16401012 0.18360169][0.17691702 0.16986685 0.16392417 0.16334878 0.16141224 0.15475914 0.14900748 0.14449598 0.12906232 0.10453623 0.081721336 0.066283435 0.060441893 0.061191708 0.067710541][0.10072695 0.080123365 0.062828481 0.052217539 0.043824546 0.036972634 0.036298297 0.038603134 0.034075823 0.023044273 0.013483855 0.0083919987 0.0059024603 0.002196312 -0.0031887933][0.037956253 0.01262494 -0.0080869226 -0.021690041 -0.029709151 -0.031377114 -0.024847992 -0.016030164 -0.012164545 -0.012960459 -0.012690648 -0.01023822 -0.0098259049 -0.015791973 -0.028075876]]...]
INFO - root - 2017-12-11 08:10:40.521876: step 37410, loss = 0.69, batch loss = 0.64 (14.8 examples/sec; 0.541 sec/batch; 44h:19m:52s remains)
INFO - root - 2017-12-11 08:10:46.043880: step 37420, loss = 0.71, batch loss = 0.65 (14.4 examples/sec; 0.556 sec/batch; 45h:34m:55s remains)
INFO - root - 2017-12-11 08:10:51.641046: step 37430, loss = 0.69, batch loss = 0.63 (14.2 examples/sec; 0.565 sec/batch; 46h:16m:07s remains)
INFO - root - 2017-12-11 08:10:57.172768: step 37440, loss = 0.71, batch loss = 0.65 (15.2 examples/sec; 0.525 sec/batch; 43h:00m:20s remains)
INFO - root - 2017-12-11 08:11:02.688162: step 37450, loss = 0.70, batch loss = 0.64 (14.7 examples/sec; 0.544 sec/batch; 44h:34m:21s remains)
INFO - root - 2017-12-11 08:11:08.263734: step 37460, loss = 0.69, batch loss = 0.63 (14.5 examples/sec; 0.551 sec/batch; 45h:07m:17s remains)
INFO - root - 2017-12-11 08:11:13.355264: step 37470, loss = 0.70, batch loss = 0.64 (14.4 examples/sec; 0.554 sec/batch; 45h:23m:01s remains)
INFO - root - 2017-12-11 08:11:18.845007: step 37480, loss = 0.69, batch loss = 0.63 (14.9 examples/sec; 0.536 sec/batch; 43h:56m:38s remains)
INFO - root - 2017-12-11 08:11:24.401924: step 37490, loss = 0.69, batch loss = 0.63 (14.4 examples/sec; 0.557 sec/batch; 45h:39m:56s remains)
INFO - root - 2017-12-11 08:11:29.854724: step 37500, loss = 0.70, batch loss = 0.64 (14.5 examples/sec; 0.551 sec/batch; 45h:09m:07s remains)
2017-12-11 08:11:30.429785: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.41814804 0.42664859 0.41842562 0.40760982 0.40010205 0.39783034 0.39726821 0.39430618 0.38270369 0.36334482 0.33807197 0.31233108 0.30060968 0.30973879 0.33013168][0.41326746 0.42135212 0.41273677 0.40155318 0.39400551 0.39634517 0.40676558 0.41671211 0.41320315 0.39267647 0.35902384 0.32073468 0.29621464 0.29700553 0.31748411][0.35698637 0.3643702 0.35724929 0.3483116 0.34451497 0.35523739 0.37893942 0.40189314 0.40458402 0.38044941 0.33666885 0.28621271 0.24988921 0.24094306 0.25695571][0.27799919 0.28298065 0.27973974 0.27855349 0.2859512 0.31123495 0.35067475 0.38528657 0.39035532 0.35817787 0.30105966 0.2377155 0.19038527 0.17184986 0.18071027][0.19912027 0.20110475 0.2023486 0.21326582 0.23901656 0.28508279 0.34229788 0.3860895 0.38888642 0.34448308 0.27105734 0.19353318 0.13581906 0.10940547 0.11134531][0.14049031 0.13949203 0.14292851 0.16562279 0.21109618 0.27748168 0.34748688 0.39210245 0.38521895 0.32529464 0.236959 0.15013705 0.088961974 0.061912827 0.062787786][0.11486276 0.108909 0.1113619 0.14140351 0.20040761 0.27860591 0.35148469 0.38826972 0.36706594 0.29252651 0.19463016 0.10586527 0.048964452 0.028365523 0.0344928][0.11685549 0.10367379 0.10072605 0.13020973 0.1912559 0.2688387 0.33522278 0.36078456 0.32805306 0.24602537 0.1475717 0.065630756 0.020015713 0.010724972 0.025972769][0.1364191 0.11615229 0.10581613 0.12698801 0.17696986 0.24074525 0.29328257 0.30894375 0.27330121 0.19639583 0.10974475 0.043822162 0.01490695 0.01942968 0.044537038][0.16216993 0.14221221 0.12863451 0.13739258 0.16488297 0.20152469 0.23137294 0.23643309 0.20539412 0.14602931 0.083501823 0.041880794 0.032780711 0.0500764 0.081472173][0.17448273 0.16369586 0.15310021 0.14977174 0.15047455 0.15395923 0.15642272 0.14961219 0.12629378 0.090831287 0.059273854 0.046002328 0.056145482 0.082035236 0.11392869][0.17292006 0.17584561 0.17032912 0.15515763 0.12992468 0.10264838 0.081138879 0.065852985 0.053087663 0.042866874 0.041049752 0.051440347 0.072720349 0.097938418 0.12210087][0.18120961 0.19671172 0.1920242 0.16101933 0.10920935 0.0545435 0.01478661 -0.003644045 -0.0033338643 0.0098279463 0.031289823 0.055820704 0.077680625 0.093135871 0.1039128][0.21685189 0.24392618 0.23650877 0.18744065 0.10956184 0.030442666 -0.023810513 -0.043412354 -0.032341596 -0.0026890261 0.032123778 0.060535431 0.074930921 0.075689875 0.071039714][0.2720561 0.30531448 0.29113325 0.22524197 0.12709944 0.031170446 -0.031684291 -0.051239416 -0.0344164 0.0022904435 0.040389139 0.065235659 0.069132373 0.055689968 0.037370712]]...]
INFO - root - 2017-12-11 08:11:35.937666: step 37510, loss = 0.70, batch loss = 0.64 (15.1 examples/sec; 0.530 sec/batch; 43h:25m:26s remains)
INFO - root - 2017-12-11 08:11:41.387334: step 37520, loss = 0.67, batch loss = 0.62 (14.8 examples/sec; 0.541 sec/batch; 44h:20m:18s remains)
INFO - root - 2017-12-11 08:11:46.930062: step 37530, loss = 0.70, batch loss = 0.64 (14.2 examples/sec; 0.562 sec/batch; 46h:04m:04s remains)
INFO - root - 2017-12-11 08:11:52.422579: step 37540, loss = 0.71, batch loss = 0.65 (14.2 examples/sec; 0.564 sec/batch; 46h:10m:47s remains)
INFO - root - 2017-12-11 08:11:57.859965: step 37550, loss = 0.68, batch loss = 0.62 (14.2 examples/sec; 0.564 sec/batch; 46h:12m:40s remains)
INFO - root - 2017-12-11 08:12:03.018164: step 37560, loss = 0.70, batch loss = 0.64 (21.7 examples/sec; 0.369 sec/batch; 30h:13m:51s remains)
INFO - root - 2017-12-11 08:12:08.584934: step 37570, loss = 0.72, batch loss = 0.66 (14.3 examples/sec; 0.560 sec/batch; 45h:53m:47s remains)
INFO - root - 2017-12-11 08:12:14.163201: step 37580, loss = 0.68, batch loss = 0.63 (14.6 examples/sec; 0.549 sec/batch; 44h:58m:44s remains)
INFO - root - 2017-12-11 08:12:19.601516: step 37590, loss = 0.69, batch loss = 0.63 (14.4 examples/sec; 0.555 sec/batch; 45h:27m:33s remains)
INFO - root - 2017-12-11 08:12:25.087963: step 37600, loss = 0.70, batch loss = 0.64 (14.9 examples/sec; 0.539 sec/batch; 44h:07m:44s remains)
2017-12-11 08:12:25.707758: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.15857345 0.1844321 0.21428081 0.25192508 0.28588352 0.3007555 0.27959213 0.22334041 0.14511073 0.0626941 -0.0076868823 -0.054633841 -0.077831678 -0.082817495 -0.07643497][0.1604632 0.19547431 0.2348347 0.28430519 0.33185935 0.35813209 0.34370071 0.2870293 0.20252925 0.10902572 0.02485914 -0.035347886 -0.068352796 -0.079591632 -0.076980047][0.13580164 0.17790361 0.22514355 0.28673989 0.34992442 0.3924239 0.39050704 0.33905002 0.25266314 0.15076743 0.054210939 -0.018106377 -0.059531435 -0.075694278 -0.075994454][0.1019331 0.14662763 0.19804755 0.26895681 0.34607551 0.40403086 0.41459584 0.3695991 0.283151 0.17511226 0.069446921 -0.011027627 -0.057231147 -0.07534451 -0.076017857][0.0772091 0.1179743 0.16609906 0.23750703 0.31930393 0.38475612 0.40274274 0.36373678 0.28057325 0.17223568 0.06491673 -0.016341828 -0.061571002 -0.078234717 -0.077349409][0.076406762 0.10871828 0.14700629 0.20977354 0.28421992 0.34514794 0.36193961 0.32611996 0.24887028 0.14629583 0.044929925 -0.02993218 -0.069338284 -0.082157061 -0.078868583][0.10677665 0.13235883 0.15986034 0.20998828 0.26908642 0.31518152 0.32107106 0.28188577 0.20830855 0.11280237 0.020452242 -0.044789746 -0.076452672 -0.084777966 -0.079305314][0.16023655 0.1838382 0.20418346 0.2427672 0.2846809 0.31214705 0.30202705 0.25372264 0.17826603 0.086605884 0.0017277985 -0.055280272 -0.080644973 -0.085587762 -0.07880073][0.21939988 0.24397792 0.26180556 0.29320046 0.32203412 0.33377761 0.30870089 0.2495283 0.16776757 0.075301848 -0.0060442737 -0.058668561 -0.081188276 -0.084931426 -0.07804729][0.27036139 0.29649597 0.31396779 0.34089497 0.36159346 0.36286464 0.32665741 0.25788546 0.1684655 0.07252004 -0.0081793368 -0.058940716 -0.080682375 -0.084466681 -0.078072071][0.30083039 0.3280693 0.34631237 0.37060165 0.38721204 0.3830336 0.3402873 0.26517358 0.16957384 0.069975607 -0.011168534 -0.060916387 -0.082126923 -0.085594021 -0.078873307][0.3043597 0.32923305 0.34598705 0.36731869 0.38236323 0.37753981 0.33446515 0.25930348 0.16308668 0.063360244 -0.016879784 -0.065296061 -0.085425414 -0.087963209 -0.080170564][0.2826888 0.30000508 0.31075963 0.32741019 0.34224653 0.34081149 0.30385515 0.23607929 0.14642262 0.051733728 -0.024789238 -0.070580624 -0.089140683 -0.090634719 -0.08180733][0.25022727 0.25674838 0.25878194 0.27114603 0.28844485 0.29454994 0.26824588 0.21198894 0.13215788 0.043937728 -0.028837163 -0.072616361 -0.090484485 -0.091827765 -0.082743824][0.22655667 0.22309694 0.21795589 0.23058674 0.25541475 0.27255166 0.25765035 0.21127607 0.13798347 0.051685631 -0.02209691 -0.067713045 -0.087615006 -0.090669751 -0.082651123]]...]
INFO - root - 2017-12-11 08:12:31.325553: step 37610, loss = 0.69, batch loss = 0.63 (14.5 examples/sec; 0.553 sec/batch; 45h:17m:39s remains)
INFO - root - 2017-12-11 08:12:36.797549: step 37620, loss = 0.71, batch loss = 0.65 (14.5 examples/sec; 0.553 sec/batch; 45h:15m:56s remains)
INFO - root - 2017-12-11 08:12:42.373013: step 37630, loss = 0.71, batch loss = 0.65 (14.1 examples/sec; 0.568 sec/batch; 46h:30m:31s remains)
INFO - root - 2017-12-11 08:12:47.915849: step 37640, loss = 0.69, batch loss = 0.63 (15.1 examples/sec; 0.530 sec/batch; 43h:24m:11s remains)
INFO - root - 2017-12-11 08:12:53.387056: step 37650, loss = 0.70, batch loss = 0.65 (14.5 examples/sec; 0.552 sec/batch; 45h:14m:48s remains)
INFO - root - 2017-12-11 08:12:58.565058: step 37660, loss = 0.70, batch loss = 0.65 (14.8 examples/sec; 0.541 sec/batch; 44h:16m:37s remains)
INFO - root - 2017-12-11 08:13:04.074297: step 37670, loss = 0.70, batch loss = 0.64 (14.7 examples/sec; 0.545 sec/batch; 44h:38m:07s remains)
INFO - root - 2017-12-11 08:13:09.632256: step 37680, loss = 0.68, batch loss = 0.62 (14.1 examples/sec; 0.569 sec/batch; 46h:34m:54s remains)
INFO - root - 2017-12-11 08:13:15.034202: step 37690, loss = 0.71, batch loss = 0.65 (14.5 examples/sec; 0.550 sec/batch; 45h:01m:43s remains)
INFO - root - 2017-12-11 08:13:20.497614: step 37700, loss = 0.70, batch loss = 0.64 (14.8 examples/sec; 0.540 sec/batch; 44h:15m:20s remains)
2017-12-11 08:13:21.103679: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.070705384 0.0701082 0.062023576 0.050779507 0.041493814 0.036158714 0.031382717 0.022024689 0.011428294 0.0041011688 -9.8432545e-05 -0.0027724968 -0.0047322144 -0.0068467259 -0.00997087][0.0752441 0.076253 0.0704619 0.062935941 0.058727145 0.059718844 0.061201639 0.055434547 0.045803156 0.0384438 0.033445597 0.028487921 0.021382261 0.012561648 0.0037457582][0.075442255 0.083989762 0.088441804 0.094183251 0.10418654 0.12008929 0.13483293 0.13635617 0.12904581 0.1214813 0.11707897 0.11257299 0.10244797 0.087801449 0.072808988][0.076544255 0.099096015 0.12352999 0.15456098 0.18973511 0.22922459 0.26186314 0.27019784 0.26226848 0.25171429 0.24781725 0.24595891 0.23688804 0.22202553 0.20395085][0.082976006 0.12526719 0.17793888 0.2434468 0.31052023 0.3768732 0.42558363 0.43456441 0.41908169 0.40080315 0.3955211 0.39706245 0.39289352 0.38463637 0.3693679][0.09151832 0.15373346 0.23603924 0.33695552 0.43452621 0.52354848 0.58187932 0.58580816 0.55796731 0.5277757 0.51641512 0.51743519 0.51750004 0.51896733 0.51200879][0.094621159 0.16910622 0.272137 0.39803243 0.51523787 0.61575079 0.67545587 0.67380422 0.63684171 0.59619385 0.5749104 0.56901968 0.56971949 0.580989 0.58601922][0.091861285 0.16555929 0.27196914 0.4023883 0.51974833 0.61502773 0.6679281 0.66557854 0.63078862 0.58918279 0.56265551 0.55220163 0.55717677 0.58147722 0.60137576][0.089681827 0.1495547 0.23998551 0.35179049 0.44851375 0.52325296 0.56443369 0.56894732 0.55092436 0.5247665 0.50874329 0.50883019 0.53195745 0.57798433 0.61627519][0.093385883 0.13129602 0.19221257 0.26905677 0.3315587 0.37721005 0.40519667 0.42151377 0.42990577 0.43225792 0.44268727 0.46942294 0.52280778 0.59566975 0.65251029][0.11222892 0.12892497 0.15742156 0.1944982 0.21897258 0.23319617 0.246162 0.27138075 0.30461934 0.33794728 0.381152 0.44056523 0.52545047 0.62361854 0.69704741][0.14254339 0.14602076 0.14942905 0.15292373 0.14441918 0.13051154 0.1265832 0.15081567 0.19664228 0.25216529 0.32135376 0.40444252 0.50745612 0.61959291 0.705052][0.16003607 0.15919256 0.14943534 0.13274622 0.10402742 0.072262093 0.054378361 0.069212794 0.11222208 0.17304023 0.24961847 0.33546656 0.43420845 0.54258597 0.63148636][0.14317079 0.14312473 0.13066962 0.10765658 0.07280717 0.034973703 0.00899617 0.011431004 0.0413 0.091796547 0.15628238 0.22445567 0.30047134 0.3906877 0.47416934][0.089954667 0.091193691 0.081823885 0.061936986 0.031466514 -0.0030256922 -0.030501841 -0.03784655 -0.023495944 0.0084099732 0.049089152 0.088772386 0.13346082 0.19628789 0.26381162]]...]
INFO - root - 2017-12-11 08:13:26.592863: step 37710, loss = 0.71, batch loss = 0.65 (14.4 examples/sec; 0.554 sec/batch; 45h:20m:06s remains)
INFO - root - 2017-12-11 08:13:32.032514: step 37720, loss = 0.72, batch loss = 0.66 (14.4 examples/sec; 0.557 sec/batch; 45h:38m:26s remains)
INFO - root - 2017-12-11 08:13:37.570532: step 37730, loss = 0.69, batch loss = 0.63 (14.4 examples/sec; 0.556 sec/batch; 45h:31m:49s remains)
INFO - root - 2017-12-11 08:13:43.079481: step 37740, loss = 0.70, batch loss = 0.64 (14.9 examples/sec; 0.537 sec/batch; 44h:00m:31s remains)
INFO - root - 2017-12-11 08:13:48.466438: step 37750, loss = 0.69, batch loss = 0.63 (15.8 examples/sec; 0.507 sec/batch; 41h:31m:32s remains)
INFO - root - 2017-12-11 08:13:53.564096: step 37760, loss = 0.71, batch loss = 0.66 (14.5 examples/sec; 0.552 sec/batch; 45h:13m:28s remains)
INFO - root - 2017-12-11 08:13:59.194737: step 37770, loss = 0.70, batch loss = 0.64 (14.3 examples/sec; 0.561 sec/batch; 45h:55m:55s remains)
INFO - root - 2017-12-11 08:14:04.726466: step 37780, loss = 0.69, batch loss = 0.63 (14.6 examples/sec; 0.549 sec/batch; 44h:58m:08s remains)
INFO - root - 2017-12-11 08:14:10.174168: step 37790, loss = 0.70, batch loss = 0.64 (14.6 examples/sec; 0.548 sec/batch; 44h:50m:32s remains)
INFO - root - 2017-12-11 08:14:15.655523: step 37800, loss = 0.68, batch loss = 0.62 (14.4 examples/sec; 0.557 sec/batch; 45h:37m:29s remains)
2017-12-11 08:14:16.264890: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.10600234 0.095963925 0.08881209 0.098382041 0.12548329 0.15434623 0.17251438 0.17459086 0.16542342 0.15138879 0.13430306 0.11955642 0.11701388 0.13164039 0.159062][0.13153346 0.11705817 0.10012454 0.097961739 0.11490409 0.13566536 0.14815989 0.14972536 0.1445355 0.13677333 0.12569349 0.11479293 0.11426019 0.1290364 0.15484828][0.14807229 0.1287878 0.10249065 0.08885181 0.09530776 0.10904495 0.11920109 0.12441795 0.12706473 0.12857965 0.12533453 0.11766811 0.11639251 0.12693639 0.14753389][0.15716849 0.13332474 0.10164589 0.081923731 0.084599763 0.099938005 0.11648406 0.13164458 0.1444076 0.1538808 0.15511115 0.14491786 0.13602322 0.13620692 0.14726737][0.15352881 0.12920308 0.098851115 0.080732569 0.0880175 0.11388405 0.14414328 0.17294052 0.19517837 0.20899861 0.2097472 0.19157051 0.17014368 0.15664828 0.15631725][0.13772964 0.12008521 0.098580129 0.088263921 0.10433587 0.14283496 0.18694839 0.22720852 0.25459418 0.2679159 0.26454 0.23732619 0.20416111 0.17899449 0.16934758][0.11946052 0.11584893 0.10800447 0.1071642 0.13020469 0.17668849 0.22904323 0.27435064 0.30051485 0.30796829 0.29691982 0.26127225 0.21977156 0.18758561 0.17325772][0.11430757 0.12780337 0.13328955 0.13835043 0.16206965 0.20846418 0.26140717 0.30471438 0.32368156 0.32044172 0.29862317 0.25564048 0.21018946 0.17681858 0.16413389][0.12888664 0.15640354 0.17003067 0.17490751 0.19214506 0.22972739 0.275389 0.31096852 0.31995502 0.30500007 0.27272987 0.22515537 0.1804097 0.15140606 0.14550558][0.15378387 0.18747625 0.20226136 0.2019991 0.2075417 0.22928014 0.26097804 0.28537941 0.28529608 0.26300457 0.22626349 0.18077572 0.14256632 0.12256975 0.12673602][0.17660768 0.20571281 0.21414119 0.20605838 0.19953667 0.20545459 0.22294436 0.23789164 0.23402238 0.21355641 0.18260525 0.14705442 0.11930826 0.10929576 0.12291159][0.19282997 0.20719749 0.20261627 0.18587089 0.17114697 0.16796049 0.17741188 0.188683 0.18821917 0.17850697 0.16161343 0.14013858 0.12275904 0.11985689 0.13947351][0.19904964 0.19165961 0.17052943 0.1461947 0.12900195 0.12554099 0.13546966 0.1498484 0.15863197 0.16461702 0.16515453 0.15771259 0.14782877 0.1472027 0.16749306][0.19365796 0.16431376 0.12833466 0.099550895 0.084738888 0.087510258 0.10413146 0.12581299 0.14593601 0.16763906 0.18418688 0.18735066 0.17967424 0.17430966 0.18711585][0.17820519 0.13412881 0.090438068 0.061473772 0.050830256 0.060339391 0.083618827 0.1114706 0.14046347 0.17421772 0.20269504 0.21220772 0.20249052 0.18731575 0.18563963]]...]
INFO - root - 2017-12-11 08:14:21.671880: step 37810, loss = 0.69, batch loss = 0.63 (14.7 examples/sec; 0.546 sec/batch; 44h:39m:34s remains)
INFO - root - 2017-12-11 08:14:27.248400: step 37820, loss = 0.70, batch loss = 0.64 (14.6 examples/sec; 0.547 sec/batch; 44h:47m:20s remains)
INFO - root - 2017-12-11 08:14:32.773342: step 37830, loss = 0.70, batch loss = 0.64 (14.4 examples/sec; 0.554 sec/batch; 45h:20m:14s remains)
INFO - root - 2017-12-11 08:14:38.308659: step 37840, loss = 0.69, batch loss = 0.63 (14.4 examples/sec; 0.555 sec/batch; 45h:25m:51s remains)
INFO - root - 2017-12-11 08:14:43.764949: step 37850, loss = 0.69, batch loss = 0.63 (14.5 examples/sec; 0.553 sec/batch; 45h:15m:04s remains)
INFO - root - 2017-12-11 08:14:48.986722: step 37860, loss = 0.69, batch loss = 0.63 (14.3 examples/sec; 0.560 sec/batch; 45h:52m:03s remains)
INFO - root - 2017-12-11 08:14:54.594370: step 37870, loss = 0.71, batch loss = 0.65 (14.7 examples/sec; 0.544 sec/batch; 44h:28m:53s remains)
INFO - root - 2017-12-11 08:15:00.045162: step 37880, loss = 0.68, batch loss = 0.63 (15.2 examples/sec; 0.525 sec/batch; 42h:59m:51s remains)
INFO - root - 2017-12-11 08:15:05.434211: step 37890, loss = 0.69, batch loss = 0.63 (14.7 examples/sec; 0.546 sec/batch; 44h:39m:59s remains)
INFO - root - 2017-12-11 08:15:10.976915: step 37900, loss = 0.70, batch loss = 0.64 (14.9 examples/sec; 0.536 sec/batch; 43h:52m:47s remains)
2017-12-11 08:15:11.517356: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.080396153 -0.0506106 -0.0032723849 0.073744483 0.1805656 0.28898463 0.36963949 0.4268848 0.46272063 0.4736464 0.47656161 0.49307004 0.526554 0.5425204 0.51359117][-0.083030045 -0.053322338 -0.0043654866 0.076493964 0.19229718 0.31394702 0.40820974 0.47432137 0.51384914 0.52396756 0.520085 0.52542704 0.54856813 0.55913347 0.52816373][-0.0846983 -0.0569043 -0.0091805579 0.070768826 0.18887271 0.31773776 0.42198172 0.49578133 0.53884667 0.54865366 0.536881 0.52715081 0.53425926 0.53497058 0.50386256][-0.0874013 -0.061423954 -0.014498131 0.065906428 0.18626712 0.31981698 0.42982358 0.50735319 0.5493179 0.55459148 0.53441936 0.5134756 0.51027346 0.50603849 0.4794988][-0.0895047 -0.063261442 -0.013626183 0.070975453 0.19511887 0.3312287 0.44197452 0.51703173 0.55166346 0.5476082 0.51840252 0.49060845 0.48380858 0.48149437 0.46409667][-0.090243481 -0.061662927 -0.006984429 0.083174005 0.21098569 0.34789824 0.45819119 0.531751 0.56399369 0.55831033 0.52990079 0.50544256 0.50285321 0.50478947 0.49305192][-0.091147467 -0.059756443 0.002266922 0.10245114 0.2407953 0.38655761 0.5043909 0.5825054 0.61705887 0.61400294 0.59046108 0.57361072 0.57922947 0.58744591 0.57703018][-0.092037633 -0.057774693 0.014365303 0.13162012 0.29011148 0.45451307 0.5866338 0.67134309 0.70583993 0.70195156 0.68239993 0.67416847 0.68835759 0.700155 0.68241715][-0.090430483 -0.051559176 0.032925218 0.17029801 0.35144094 0.53432131 0.67687732 0.75981677 0.7838909 0.76907712 0.746669 0.74242687 0.75911593 0.76737034 0.7344805][-0.087406434 -0.045985553 0.045252673 0.19275431 0.38258937 0.56942219 0.70975339 0.78077888 0.78628051 0.75229132 0.71750444 0.70631093 0.71465003 0.71225274 0.66541684][-0.08444652 -0.044702068 0.043054361 0.18364808 0.36069819 0.53063005 0.65280986 0.7042886 0.68981057 0.63703334 0.58667648 0.5617038 0.55591649 0.54299968 0.49378821][-0.079645775 -0.041166645 0.040661976 0.16731223 0.32017556 0.45982388 0.55111629 0.57480145 0.53651053 0.46400875 0.39732698 0.35807344 0.34129676 0.32714888 0.292487][-0.073848136 -0.035304189 0.041025616 0.15172617 0.2756446 0.37876439 0.4333787 0.42660946 0.36586645 0.27805772 0.20119332 0.15479323 0.13513547 0.12882563 0.12021902][-0.07192488 -0.036592767 0.029648431 0.11946802 0.21160556 0.27860528 0.30098027 0.27223182 0.20110248 0.1108538 0.034765683 -0.0095091732 -0.024077628 -0.016688423 0.0052805673][-0.074246451 -0.045184847 0.0075480063 0.0748269 0.13816014 0.17715321 0.17887168 0.14133051 0.073069751 -0.0075415242 -0.073637895 -0.10926153 -0.11348671 -0.09003628 -0.039798055]]...]
INFO - root - 2017-12-11 08:15:16.992833: step 37910, loss = 0.69, batch loss = 0.64 (15.0 examples/sec; 0.533 sec/batch; 43h:39m:04s remains)
INFO - root - 2017-12-11 08:15:22.500293: step 37920, loss = 0.69, batch loss = 0.63 (14.6 examples/sec; 0.550 sec/batch; 44h:58m:00s remains)
INFO - root - 2017-12-11 08:15:28.071024: step 37930, loss = 0.70, batch loss = 0.65 (14.3 examples/sec; 0.559 sec/batch; 45h:46m:50s remains)
INFO - root - 2017-12-11 08:15:33.603341: step 37940, loss = 0.70, batch loss = 0.64 (14.6 examples/sec; 0.548 sec/batch; 44h:52m:11s remains)
INFO - root - 2017-12-11 08:15:38.787842: step 37950, loss = 0.71, batch loss = 0.65 (14.9 examples/sec; 0.536 sec/batch; 43h:53m:34s remains)
INFO - root - 2017-12-11 08:15:44.354718: step 37960, loss = 0.70, batch loss = 0.65 (14.0 examples/sec; 0.572 sec/batch; 46h:47m:06s remains)
INFO - root - 2017-12-11 08:15:49.832821: step 37970, loss = 0.71, batch loss = 0.65 (14.7 examples/sec; 0.543 sec/batch; 44h:23m:30s remains)
INFO - root - 2017-12-11 08:15:55.432660: step 37980, loss = 0.69, batch loss = 0.64 (14.7 examples/sec; 0.544 sec/batch; 44h:30m:57s remains)
INFO - root - 2017-12-11 08:16:00.958787: step 37990, loss = 0.69, batch loss = 0.63 (14.5 examples/sec; 0.553 sec/batch; 45h:15m:59s remains)
INFO - root - 2017-12-11 08:16:06.434396: step 38000, loss = 0.70, batch loss = 0.64 (14.3 examples/sec; 0.559 sec/batch; 45h:45m:34s remains)
2017-12-11 08:16:07.034309: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.30212146 0.32387766 0.35031772 0.36841083 0.36844343 0.351439 0.32626498 0.30172244 0.28004962 0.26205274 0.25005075 0.24837248 0.25895306 0.28146523 0.31280756][0.34718868 0.3728461 0.40348366 0.42619416 0.429126 0.41239566 0.38292634 0.34898174 0.3160516 0.28815943 0.26643866 0.25517878 0.25656578 0.27157953 0.29839307][0.37706906 0.40191996 0.43131125 0.45304856 0.4554919 0.43918434 0.40899941 0.37168136 0.33455533 0.30380681 0.27882734 0.26190969 0.25537038 0.26216644 0.28302163][0.36929116 0.39344832 0.42344758 0.44774151 0.45501259 0.44652593 0.42586848 0.3980563 0.36972067 0.34620693 0.32435364 0.30389625 0.28855431 0.28610608 0.30164862][0.32414982 0.35572517 0.39708751 0.43660751 0.46180913 0.472867 0.47087786 0.45806208 0.43932644 0.420459 0.39793959 0.36952731 0.34176061 0.32866919 0.33924609][0.27035645 0.318254 0.38174504 0.44870809 0.50426769 0.54403996 0.56274551 0.55817562 0.53590685 0.50834233 0.47682089 0.43887156 0.40141919 0.3790561 0.3809981][0.22569205 0.28721842 0.37021172 0.46277332 0.54765308 0.61422622 0.64921111 0.64611989 0.6141243 0.5755524 0.53843796 0.50017273 0.46352869 0.43818778 0.43027577][0.17956862 0.24581671 0.33935887 0.44762149 0.55120051 0.63363552 0.67697108 0.67362136 0.63741773 0.59767646 0.56721252 0.54215372 0.51800978 0.49692637 0.48166147][0.11873097 0.18224411 0.27658162 0.387052 0.49317256 0.57632267 0.61951029 0.61757427 0.58671159 0.55679893 0.54154992 0.53516346 0.52713239 0.5142771 0.49708804][0.057152163 0.10903158 0.19053312 0.28666016 0.37827435 0.44840628 0.48351938 0.48125315 0.45680857 0.43647239 0.43277162 0.43891519 0.44276139 0.43886226 0.42602122][0.013431794 0.046942145 0.10516172 0.17549138 0.24216825 0.29115024 0.31214315 0.30487728 0.2829214 0.26805714 0.27023718 0.28238484 0.29316825 0.29676503 0.29173806][0.00612191 0.023335526 0.05855545 0.10271575 0.14360724 0.16946705 0.17317143 0.15688848 0.13429114 0.12358518 0.13073966 0.1472584 0.16168626 0.16965589 0.17184989][0.033251245 0.043795027 0.06633772 0.093670249 0.11491863 0.11959685 0.10503951 0.077353336 0.05236293 0.045979165 0.059330285 0.080065921 0.095180809 0.10270083 0.10664669][0.079053581 0.091736227 0.11275112 0.13461268 0.1454726 0.13515078 0.10525081 0.066707775 0.038467463 0.034886219 0.052337486 0.073888 0.084859714 0.087035753 0.088282831][0.13718422 0.15684235 0.18287121 0.20786655 0.21832556 0.20281827 0.16473114 0.11915874 0.088434413 0.085466713 0.10291365 0.12024771 0.12170902 0.11271149 0.10533309]]...]
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-SGD-lr0.001-clip50-initfromfixqian30k-fixqian
INFO - root - 2017-12-11 08:16:12.561911: step 38010, loss = 0.69, batch loss = 0.63 (14.3 examples/sec; 0.560 sec/batch; 45h:49m:37s remains)
INFO - root - 2017-12-11 08:16:18.168133: step 38020, loss = 0.70, batch loss = 0.64 (13.9 examples/sec; 0.577 sec/batch; 47h:11m:49s remains)
INFO - root - 2017-12-11 08:16:23.728923: step 38030, loss = 0.72, batch loss = 0.66 (14.7 examples/sec; 0.545 sec/batch; 44h:35m:47s remains)
INFO - root - 2017-12-11 08:16:29.202847: step 38040, loss = 0.71, batch loss = 0.65 (14.2 examples/sec; 0.562 sec/batch; 45h:55m:52s remains)
INFO - root - 2017-12-11 08:16:34.385694: step 38050, loss = 0.69, batch loss = 0.63 (14.9 examples/sec; 0.536 sec/batch; 43h:50m:37s remains)
INFO - root - 2017-12-11 08:16:39.837883: step 38060, loss = 0.71, batch loss = 0.65 (14.5 examples/sec; 0.550 sec/batch; 45h:00m:34s remains)
INFO - root - 2017-12-11 08:16:45.347643: step 38070, loss = 0.70, batch loss = 0.64 (14.4 examples/sec; 0.555 sec/batch; 45h:25m:00s remains)
INFO - root - 2017-12-11 08:16:50.969866: step 38080, loss = 0.70, batch loss = 0.65 (13.4 examples/sec; 0.598 sec/batch; 48h:52m:52s remains)
INFO - root - 2017-12-11 08:16:56.447765: step 38090, loss = 0.71, batch loss = 0.65 (14.8 examples/sec; 0.540 sec/batch; 44h:07m:23s remains)
INFO - root - 2017-12-11 08:17:01.901032: step 38100, loss = 0.70, batch loss = 0.65 (14.2 examples/sec; 0.565 sec/batch; 46h:13m:29s remains)
2017-12-11 08:17:02.544113: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.4049412 0.33204353 0.25949129 0.21234016 0.18750629 0.1764646 0.17468159 0.18629499 0.21995109 0.26902863 0.31537619 0.35351393 0.40569636 0.46686304 0.51362062][0.42203733 0.34697613 0.2787323 0.24029595 0.22710447 0.22984526 0.23791721 0.24871185 0.26830208 0.29278302 0.31182107 0.32617977 0.36030215 0.40892625 0.44728309][0.36968163 0.31468716 0.27524731 0.2681556 0.28508431 0.31218985 0.33229157 0.33794764 0.33197767 0.31536624 0.28891793 0.26333296 0.26391691 0.28661245 0.3082225][0.27493951 0.25810516 0.26801029 0.31111512 0.37259716 0.43164778 0.46551132 0.46460354 0.42923462 0.36513424 0.28700006 0.21506585 0.17298914 0.15987213 0.15646018][0.18596384 0.21502915 0.28039342 0.37731728 0.48456076 0.57513148 0.62327081 0.61856347 0.56037021 0.458838 0.3396894 0.22844462 0.14453763 0.091517925 0.056234714][0.12819088 0.1978664 0.30920613 0.44886535 0.59118986 0.70577461 0.76643437 0.76188332 0.69221282 0.57116383 0.4307864 0.29782769 0.18531467 0.10091819 0.036684863][0.10178809 0.197953 0.3361963 0.49823666 0.65791762 0.78466547 0.85323709 0.85155207 0.78280693 0.66373158 0.52505594 0.39034456 0.26733515 0.1663754 0.082315557][0.096410066 0.19995964 0.34157237 0.50088269 0.65499163 0.77643096 0.84345329 0.84448564 0.78673565 0.68885386 0.57412595 0.45778519 0.34349576 0.24217945 0.15042028][0.098410577 0.19059706 0.31313482 0.44704491 0.57507777 0.67521977 0.73047417 0.73157865 0.68983871 0.62361622 0.54580158 0.46191958 0.37225825 0.2860314 0.20041285][0.089954779 0.1584601 0.24817118 0.3439573 0.43483269 0.50503391 0.54238468 0.54088515 0.51407558 0.47921628 0.43948463 0.39228919 0.33482644 0.27287987 0.204097][0.061725039 0.1022725 0.15622061 0.21309061 0.26689154 0.30741024 0.32677233 0.321915 0.30550086 0.29221618 0.27965671 0.26127613 0.23264256 0.19623195 0.15019791][0.023704717 0.039465811 0.063645296 0.0896577 0.11438096 0.13195302 0.13812745 0.13199009 0.12329072 0.12289995 0.12685671 0.12807353 0.12159316 0.10687093 0.082849845][-0.010571827 -0.011126689 -0.005749051 0.0014887449 0.008650016 0.012800477 0.012535869 0.0083064316 0.0064756651 0.014106335 0.028554814 0.044107124 0.055094082 0.058151133 0.05223183][-0.037491105 -0.044730566 -0.046541337 -0.045818828 -0.043788172 -0.041819651 -0.040445019 -0.039233882 -0.034751572 -0.022828141 -0.0026407964 0.023112725 0.048195649 0.066163205 0.073957287][-0.05558775 -0.062081181 -0.063183315 -0.061385565 -0.056650668 -0.049521975 -0.040977776 -0.031538364 -0.019866332 -0.004196933 0.019686444 0.05184849 0.086124778 0.11372948 0.12912166]]...]
INFO - root - 2017-12-11 08:17:08.092063: step 38110, loss = 0.70, batch loss = 0.64 (14.4 examples/sec; 0.557 sec/batch; 45h:34m:58s remains)
INFO - root - 2017-12-11 08:17:13.647373: step 38120, loss = 0.68, batch loss = 0.62 (14.7 examples/sec; 0.544 sec/batch; 44h:28m:05s remains)
INFO - root - 2017-12-11 08:17:19.105913: step 38130, loss = 0.70, batch loss = 0.64 (14.8 examples/sec; 0.539 sec/batch; 44h:04m:38s remains)
INFO - root - 2017-12-11 08:17:24.630303: step 38140, loss = 0.70, batch loss = 0.65 (14.3 examples/sec; 0.560 sec/batch; 45h:47m:52s remains)
INFO - root - 2017-12-11 08:17:29.775137: step 38150, loss = 0.71, batch loss = 0.65 (14.5 examples/sec; 0.552 sec/batch; 45h:07m:17s remains)
INFO - root - 2017-12-11 08:17:35.276745: step 38160, loss = 0.70, batch loss = 0.65 (14.4 examples/sec; 0.555 sec/batch; 45h:21m:49s remains)
INFO - root - 2017-12-11 08:17:40.669920: step 38170, loss = 0.69, batch loss = 0.64 (14.8 examples/sec; 0.542 sec/batch; 44h:18m:43s remains)
INFO - root - 2017-12-11 08:17:46.206201: step 38180, loss = 0.70, batch loss = 0.64 (14.2 examples/sec; 0.562 sec/batch; 45h:57m:41s remains)
INFO - root - 2017-12-11 08:17:51.739194: step 38190, loss = 0.71, batch loss = 0.65 (14.0 examples/sec; 0.570 sec/batch; 46h:33m:56s remains)
INFO - root - 2017-12-11 08:17:57.217736: step 38200, loss = 0.69, batch loss = 0.63 (14.4 examples/sec; 0.554 sec/batch; 45h:17m:14s remains)
2017-12-11 08:17:57.812087: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.040473487 0.10275128 0.17454447 0.24279946 0.29570204 0.33465865 0.35246375 0.34237257 0.31379792 0.27785298 0.2437968 0.21285407 0.18916698 0.17615397 0.18176796][0.040150162 0.1060314 0.18307167 0.25801924 0.31851515 0.367477 0.39557576 0.39318493 0.36817372 0.33317751 0.29894391 0.26276526 0.22809789 0.20320813 0.20045292][0.03406138 0.097284481 0.17299794 0.24872398 0.31273505 0.36940876 0.40807179 0.41638687 0.39974189 0.37226647 0.34561056 0.31343141 0.27865487 0.25279954 0.24828818][0.027065232 0.0847768 0.15624355 0.23062962 0.29755622 0.36156002 0.41090512 0.4308179 0.42365447 0.40533784 0.38991308 0.36901197 0.34517214 0.329331 0.32939786][0.022535706 0.074805185 0.14169136 0.21445289 0.28418562 0.354131 0.41214871 0.44187784 0.44260743 0.43165874 0.42671797 0.41965452 0.41186208 0.41003302 0.41564155][0.022265473 0.071918219 0.13695496 0.21030922 0.28346547 0.35688367 0.41829562 0.45147955 0.45362577 0.44339737 0.44311979 0.446436 0.45286727 0.46265873 0.47234][0.024396501 0.074089132 0.14060386 0.21705295 0.29328179 0.36570382 0.42270875 0.45075929 0.44700745 0.43050283 0.42719942 0.43372041 0.44723269 0.46265832 0.47440684][0.027183976 0.078688323 0.14886817 0.22938986 0.3068845 0.37356532 0.41990253 0.4366411 0.42199805 0.39404818 0.3801949 0.38109651 0.3921845 0.40574944 0.41700125][0.031210946 0.085521519 0.15975341 0.2426313 0.31768027 0.374448 0.40690124 0.40994352 0.38346693 0.34365422 0.31670845 0.30734828 0.30985287 0.31665114 0.32548133][0.033881687 0.089650214 0.1650428 0.24564563 0.31354237 0.35775396 0.37579542 0.3657347 0.32946461 0.28153279 0.24500734 0.22710828 0.22130293 0.22104259 0.22696091][0.031416483 0.084945194 0.15665442 0.23016208 0.28828615 0.32048592 0.32672212 0.30680123 0.26437384 0.21333334 0.17349906 0.15304695 0.14432649 0.14106047 0.14611918][0.022937715 0.0713091 0.13590041 0.1999246 0.24821679 0.27082232 0.26864851 0.24262209 0.19867735 0.15133457 0.11699066 0.10231885 0.097964987 0.096896641 0.10283598][0.011351392 0.053619113 0.11026964 0.16553867 0.20623408 0.22280851 0.21616203 0.18848664 0.14815688 0.11023291 0.087083541 0.082022429 0.0847922 0.088134646 0.09505219][-0.00080580905 0.035211269 0.084165908 0.13250011 0.16841727 0.18264647 0.175898 0.15164156 0.11948268 0.0934292 0.081908822 0.08485616 0.092475407 0.097953357 0.10297259][-0.012665062 0.016535982 0.057537686 0.0996504 0.13248505 0.14730753 0.14467186 0.12833706 0.1070291 0.0919656 0.088520907 0.094434023 0.10153902 0.1041928 0.10310207]]...]
INFO - root - 2017-12-11 08:18:03.396143: step 38210, loss = 0.71, batch loss = 0.65 (14.1 examples/sec; 0.567 sec/batch; 46h:20m:32s remains)
INFO - root - 2017-12-11 08:18:08.857645: step 38220, loss = 0.70, batch loss = 0.64 (14.7 examples/sec; 0.544 sec/batch; 44h:26m:18s remains)
INFO - root - 2017-12-11 08:18:14.383540: step 38230, loss = 0.69, batch loss = 0.63 (14.7 examples/sec; 0.543 sec/batch; 44h:25m:10s remains)
INFO - root - 2017-12-11 08:18:19.646139: step 38240, loss = 0.70, batch loss = 0.64 (28.9 examples/sec; 0.277 sec/batch; 22h:38m:15s remains)
INFO - root - 2017-12-11 08:18:25.026437: step 38250, loss = 0.69, batch loss = 0.64 (14.6 examples/sec; 0.548 sec/batch; 44h:46m:42s remains)
INFO - root - 2017-12-11 08:18:30.563903: step 38260, loss = 0.69, batch loss = 0.63 (14.3 examples/sec; 0.558 sec/batch; 45h:38m:28s remains)
INFO - root - 2017-12-11 08:18:36.056578: step 38270, loss = 0.69, batch loss = 0.64 (14.8 examples/sec; 0.541 sec/batch; 44h:12m:11s remains)
INFO - root - 2017-12-11 08:18:41.522133: step 38280, loss = 0.70, batch loss = 0.65 (14.3 examples/sec; 0.558 sec/batch; 45h:34m:20s remains)
INFO - root - 2017-12-11 08:18:47.056892: step 38290, loss = 0.70, batch loss = 0.64 (14.5 examples/sec; 0.550 sec/batch; 44h:57m:20s remains)
INFO - root - 2017-12-11 08:18:52.543479: step 38300, loss = 0.68, batch loss = 0.62 (14.2 examples/sec; 0.562 sec/batch; 45h:55m:44s remains)
2017-12-11 08:18:53.134699: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.086963467 0.081092 0.057362124 0.032032054 0.02078221 0.028149758 0.047921252 0.072375417 0.090535492 0.093031868 0.075668767 0.046002962 0.01486419 -0.0091831321 -0.023240034][0.1897919 0.18304136 0.14553849 0.10363671 0.084290147 0.095352 0.12590371 0.16430469 0.19392537 0.20008078 0.17565091 0.13064705 0.079878867 0.035828836 0.0054813195][0.30430084 0.29779354 0.24688041 0.18804954 0.16007818 0.17492244 0.21686253 0.26957983 0.31080636 0.32128724 0.29165915 0.23300281 0.163612 0.09857855 0.04867202][0.40252385 0.39561793 0.334565 0.26345637 0.22949125 0.24756369 0.29922703 0.36335218 0.41313726 0.42538816 0.39137533 0.32448113 0.24297598 0.16132648 0.091930009][0.47827151 0.46959618 0.40180171 0.32404682 0.28704929 0.30712837 0.36547884 0.4368296 0.49148604 0.50207293 0.46452412 0.39544439 0.31005982 0.21787779 0.13103604][0.53959274 0.52628976 0.45469072 0.37766767 0.34463215 0.37102491 0.43677211 0.51405728 0.57303762 0.58392775 0.54754961 0.48058328 0.39426211 0.29124466 0.18306941][0.58575165 0.566302 0.49418581 0.42579463 0.40590382 0.44577995 0.52018911 0.59853637 0.65560061 0.66438591 0.62976784 0.56553054 0.4793165 0.3668091 0.23733473][0.59033889 0.56923562 0.50378782 0.45063218 0.44754761 0.50056678 0.57832676 0.649536 0.69714588 0.69978255 0.66441727 0.60180861 0.51739722 0.40085322 0.25910932][0.54667646 0.52742004 0.47521633 0.44284609 0.45835042 0.52061892 0.59397542 0.65122324 0.68621045 0.68244648 0.6467464 0.58646119 0.50650471 0.39253536 0.24927589][0.47873083 0.46113157 0.42472208 0.41401571 0.44642955 0.51400125 0.577618 0.6178053 0.63924134 0.63061082 0.59626544 0.53777063 0.46188956 0.35398629 0.21710329][0.40741369 0.39046744 0.36954811 0.37895525 0.42615181 0.49763745 0.55064255 0.57314456 0.57833129 0.56222427 0.52610135 0.4660702 0.39203203 0.29217854 0.16790357][0.32073566 0.30413398 0.295258 0.31849512 0.37412387 0.44369689 0.48354074 0.48742214 0.47369671 0.44585785 0.40394327 0.34253919 0.27395508 0.18955484 0.088970967][0.20976682 0.19351877 0.18967038 0.21456984 0.26560724 0.32305616 0.3476533 0.33769172 0.31218258 0.27897656 0.23753539 0.18322393 0.12847519 0.066777118 -0.0030158234][0.095664889 0.08249227 0.081149541 0.10055431 0.1375947 0.175871 0.18594106 0.1687042 0.14103681 0.11160259 0.079480506 0.040855236 0.0055149174 -0.031509921 -0.071424842][0.00095580297 -0.0050470163 -0.0013048516 0.013581534 0.0355632 0.054449469 0.052215543 0.031447466 0.0058462983 -0.016795667 -0.037641756 -0.05974821 -0.077124156 -0.093483187 -0.1099738]]...]
INFO - root - 2017-12-11 08:18:58.653046: step 38310, loss = 0.69, batch loss = 0.64 (15.4 examples/sec; 0.520 sec/batch; 42h:27m:50s remains)
INFO - root - 2017-12-11 08:19:04.174075: step 38320, loss = 0.70, batch loss = 0.64 (14.6 examples/sec; 0.547 sec/batch; 44h:41m:42s remains)
INFO - root - 2017-12-11 08:19:09.628811: step 38330, loss = 0.70, batch loss = 0.65 (14.6 examples/sec; 0.546 sec/batch; 44h:39m:16s remains)
INFO - root - 2017-12-11 08:19:14.736976: step 38340, loss = 0.70, batch loss = 0.65 (14.3 examples/sec; 0.558 sec/batch; 45h:37m:58s remains)
INFO - root - 2017-12-11 08:19:20.269044: step 38350, loss = 0.68, batch loss = 0.63 (14.8 examples/sec; 0.541 sec/batch; 44h:12m:29s remains)
INFO - root - 2017-12-11 08:19:25.665169: step 38360, loss = 0.68, batch loss = 0.62 (14.8 examples/sec; 0.542 sec/batch; 44h:18m:24s remains)
INFO - root - 2017-12-11 08:19:31.237786: step 38370, loss = 0.72, batch loss = 0.66 (14.0 examples/sec; 0.570 sec/batch; 46h:33m:57s remains)
INFO - root - 2017-12-11 08:19:36.778946: step 38380, loss = 0.72, batch loss = 0.66 (14.1 examples/sec; 0.569 sec/batch; 46h:27m:45s remains)
INFO - root - 2017-12-11 08:19:42.256087: step 38390, loss = 0.69, batch loss = 0.64 (14.9 examples/sec; 0.537 sec/batch; 43h:52m:19s remains)
INFO - root - 2017-12-11 08:19:47.742373: step 38400, loss = 0.69, batch loss = 0.63 (14.6 examples/sec; 0.550 sec/batch; 44h:54m:15s remains)
2017-12-11 08:19:48.295582: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.24634534 0.24053466 0.22197157 0.20266096 0.18524183 0.16578583 0.14542924 0.12844639 0.12153849 0.13135283 0.16874863 0.2334328 0.3065519 0.36922419 0.41595343][0.23793846 0.22873494 0.20753525 0.18629636 0.16608684 0.14229694 0.11522417 0.089051023 0.071718462 0.069785021 0.0938785 0.14585282 0.2110437 0.27385956 0.33178344][0.20736594 0.20013209 0.18570997 0.17237757 0.15703589 0.13392088 0.10218865 0.066159092 0.034685344 0.014514915 0.016512021 0.04613924 0.0946622 0.15101194 0.2150588][0.16969217 0.16927104 0.17111549 0.1766995 0.17620884 0.16085878 0.12799321 0.082971789 0.035559595 -0.0065538161 -0.031712856 -0.030392736 -0.0046837921 0.039000031 0.1020717][0.14079759 0.15169552 0.17697102 0.21073687 0.23477708 0.23555489 0.20773533 0.1576262 0.095139652 0.029587165 -0.025834609 -0.05769543 -0.061604682 -0.038949165 0.013731652][0.12850818 0.15510666 0.20831449 0.27623358 0.33306989 0.35943484 0.34569865 0.29702726 0.22188871 0.13191129 0.043265827 -0.026739258 -0.067673273 -0.075744174 -0.044372674][0.12497775 0.17040138 0.25140259 0.35239875 0.44370741 0.50191879 0.51250333 0.4757548 0.39599794 0.28665674 0.16701806 0.057605267 -0.025960077 -0.074014 -0.075452439][0.11432042 0.17614686 0.27706796 0.40001985 0.51729995 0.60550129 0.64565372 0.63004923 0.5568819 0.44014737 0.30106381 0.16133668 0.040163867 -0.047104906 -0.08475776][0.083098084 0.15257981 0.25863796 0.38470745 0.50967282 0.61436296 0.678135 0.68582684 0.62995011 0.5229125 0.38460422 0.23491676 0.094433673 -0.016995454 -0.081009395][0.032827377 0.09726081 0.19148704 0.30027682 0.41096786 0.51102632 0.58194727 0.60554457 0.5711714 0.48774037 0.37058443 0.2353722 0.10179118 -0.0087369541 -0.07864581][-0.019057335 0.028327791 0.0964115 0.17297362 0.25270259 0.32940528 0.38947988 0.41672584 0.40089238 0.34657782 0.26298925 0.16073692 0.056682125 -0.029754251 -0.085289359][-0.057083935 -0.032240327 0.0044317343 0.044734016 0.088089623 0.13241906 0.16980556 0.18964615 0.18475489 0.15639341 0.10830791 0.046483051 -0.016719712 -0.067072317 -0.097402669][-0.076072276 -0.070974506 -0.060447294 -0.048998661 -0.035234973 -0.019564839 -0.0055571026 0.0019804228 -0.00037438155 -0.012700565 -0.033959925 -0.061081696 -0.086649075 -0.10299415 -0.1086087][-0.078697212 -0.084903374 -0.089150444 -0.093447573 -0.095994845 -0.096964277 -0.09765134 -0.099498481 -0.10366856 -0.10999653 -0.11756182 -0.1242107 -0.12627342 -0.12139082 -0.1114248][-0.07128755 -0.080511451 -0.088820167 -0.097270727 -0.10434203 -0.11024505 -0.11542195 -0.12036572 -0.1256796 -0.13095424 -0.13449569 -0.134056 -0.12794426 -0.11634067 -0.1019772]]...]
INFO - root - 2017-12-11 08:19:53.793488: step 38410, loss = 0.72, batch loss = 0.66 (14.5 examples/sec; 0.551 sec/batch; 45h:00m:15s remains)
INFO - root - 2017-12-11 08:19:59.246485: step 38420, loss = 0.70, batch loss = 0.64 (14.7 examples/sec; 0.544 sec/batch; 44h:26m:14s remains)
INFO - root - 2017-12-11 08:20:04.788108: step 38430, loss = 0.69, batch loss = 0.63 (14.3 examples/sec; 0.560 sec/batch; 45h:44m:24s remains)
INFO - root - 2017-12-11 08:20:09.946919: step 38440, loss = 0.70, batch loss = 0.65 (14.3 examples/sec; 0.559 sec/batch; 45h:41m:18s remains)
INFO - root - 2017-12-11 08:20:15.550562: step 38450, loss = 0.70, batch loss = 0.65 (14.4 examples/sec; 0.555 sec/batch; 45h:18m:36s remains)
INFO - root - 2017-12-11 08:20:21.035402: step 38460, loss = 0.72, batch loss = 0.66 (14.9 examples/sec; 0.538 sec/batch; 43h:55m:54s remains)
INFO - root - 2017-12-11 08:20:26.563612: step 38470, loss = 0.69, batch loss = 0.63 (14.4 examples/sec; 0.554 sec/batch; 45h:16m:23s remains)
INFO - root - 2017-12-11 08:20:32.048670: step 38480, loss = 0.70, batch loss = 0.64 (14.6 examples/sec; 0.546 sec/batch; 44h:36m:44s remains)
INFO - root - 2017-12-11 08:20:37.611791: step 38490, loss = 0.70, batch loss = 0.65 (14.5 examples/sec; 0.551 sec/batch; 45h:01m:13s remains)
INFO - root - 2017-12-11 08:20:43.097816: step 38500, loss = 0.70, batch loss = 0.65 (14.8 examples/sec; 0.541 sec/batch; 44h:12m:21s remains)
2017-12-11 08:20:43.710200: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.026669823 0.018469518 0.0056936485 -0.0062697958 -0.012973596 -0.0145965 -0.012652815 -0.00963385 -0.0087810066 -0.013471175 -0.023169275 -0.034473356 -0.044208918 -0.050454814 -0.053646605][0.10173145 0.094911836 0.078844093 0.060344841 0.044988725 0.034135245 0.029407715 0.030786265 0.032142907 0.025873311 0.010824055 -0.0082370741 -0.026559217 -0.040921129 -0.05032656][0.19908337 0.19959091 0.18243296 0.15689206 0.13122602 0.10773274 0.0929873 0.090241387 0.091123439 0.082585432 0.060819592 0.032233857 0.0033372766 -0.021535695 -0.039989956][0.30209187 0.31900379 0.30740294 0.28006104 0.24826792 0.21420753 0.18925536 0.17948423 0.17446627 0.15767081 0.12369236 0.082309872 0.040880639 0.0037509461 -0.025298288][0.38103139 0.41850296 0.41871718 0.39756775 0.36778626 0.33127537 0.30158389 0.28621358 0.27243766 0.24234705 0.19180077 0.13452972 0.078928612 0.028652055 -0.011267372][0.41065055 0.46728379 0.48418388 0.47752413 0.4589521 0.43049312 0.40442255 0.38617551 0.36169717 0.31511432 0.24614304 0.17262003 0.10395081 0.043156266 -0.0042328644][0.40411904 0.47351131 0.50476974 0.51264662 0.50797284 0.49325734 0.47637251 0.456669 0.42018721 0.35624769 0.26886129 0.18074366 0.10225011 0.036603313 -0.011474121][0.39363238 0.46689224 0.504283 0.51954603 0.5255512 0.52528393 0.52071607 0.50325483 0.45952824 0.38259086 0.27924517 0.17625383 0.087359525 0.018587541 -0.027268426][0.40238321 0.47224119 0.50587046 0.51843226 0.52799565 0.53901058 0.5472067 0.53600055 0.49257496 0.411006 0.29792145 0.1811391 0.080509163 0.0062402575 -0.039442424][0.41503194 0.47761679 0.5009551 0.50221688 0.50552285 0.51832533 0.53312993 0.52786976 0.49138096 0.41683453 0.30662203 0.18512754 0.077963732 0.00031799317 -0.0458866][0.40791202 0.46357203 0.47797853 0.46789908 0.46180069 0.47013947 0.48616704 0.48538271 0.45884335 0.39879566 0.30131978 0.18480043 0.077439457 -0.00034461214 -0.046281252][0.36599439 0.41550478 0.42622206 0.41140768 0.3988885 0.40099454 0.41495395 0.41754007 0.4009645 0.35692492 0.27721491 0.17438944 0.075259477 0.0023459168 -0.041532647][0.27787539 0.32080436 0.33294451 0.32106519 0.30698362 0.30344385 0.31322396 0.31788006 0.31017742 0.28175449 0.22338086 0.14314161 0.062938169 0.0027749825 -0.035208426][0.1593091 0.19338135 0.20702519 0.2007883 0.18850833 0.18146124 0.18692167 0.19272481 0.19257988 0.17836231 0.14193417 0.088349558 0.033408213 -0.0078088571 -0.034922052][0.038603567 0.060432985 0.071674265 0.068734333 0.058978435 0.051167019 0.054314472 0.061677165 0.067894496 0.065674789 0.048768558 0.020364366 -0.0096068783 -0.030882742 -0.044644576]]...]
INFO - root - 2017-12-11 08:20:49.199669: step 38510, loss = 0.72, batch loss = 0.66 (14.5 examples/sec; 0.550 sec/batch; 44h:57m:20s remains)
INFO - root - 2017-12-11 08:20:54.652086: step 38520, loss = 0.69, batch loss = 0.64 (15.0 examples/sec; 0.534 sec/batch; 43h:38m:25s remains)
INFO - root - 2017-12-11 08:21:00.246152: step 38530, loss = 0.71, batch loss = 0.65 (14.1 examples/sec; 0.569 sec/batch; 46h:27m:56s remains)
INFO - root - 2017-12-11 08:21:05.387980: step 38540, loss = 0.68, batch loss = 0.62 (14.3 examples/sec; 0.561 sec/batch; 45h:50m:14s remains)
INFO - root - 2017-12-11 08:21:10.992179: step 38550, loss = 0.69, batch loss = 0.63 (14.3 examples/sec; 0.558 sec/batch; 45h:35m:54s remains)
INFO - root - 2017-12-11 08:21:16.528517: step 38560, loss = 0.68, batch loss = 0.63 (14.6 examples/sec; 0.548 sec/batch; 44h:45m:28s remains)
INFO - root - 2017-12-11 08:21:22.154615: step 38570, loss = 0.69, batch loss = 0.63 (14.4 examples/sec; 0.556 sec/batch; 45h:22m:38s remains)
INFO - root - 2017-12-11 08:21:27.660112: step 38580, loss = 0.68, batch loss = 0.62 (14.9 examples/sec; 0.537 sec/batch; 43h:52m:17s remains)
INFO - root - 2017-12-11 08:21:33.138892: step 38590, loss = 0.68, batch loss = 0.62 (14.8 examples/sec; 0.540 sec/batch; 44h:05m:35s remains)
INFO - root - 2017-12-11 08:21:38.683786: step 38600, loss = 0.69, batch loss = 0.63 (14.2 examples/sec; 0.562 sec/batch; 45h:51m:29s remains)
2017-12-11 08:21:39.235538: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.064208858 -0.018918037 0.059316836 0.16588673 0.27739087 0.36777744 0.41639069 0.41056514 0.35417238 0.2739253 0.20617892 0.17158403 0.17074299 0.18899445 0.20491898][-0.049758889 -0.0115611 0.061103586 0.16505297 0.27513084 0.36254165 0.40569583 0.39463368 0.3350164 0.25333282 0.18558769 0.15235676 0.15358379 0.17499009 0.1982086][-0.017909577 0.00082839205 0.055268075 0.1457727 0.24635176 0.32665169 0.36532789 0.35499197 0.30152759 0.22834547 0.16803996 0.13908185 0.14049588 0.16010277 0.18503636][0.032464236 0.025823807 0.05741239 0.13149908 0.22215563 0.29778147 0.33767995 0.33513752 0.29506907 0.23686573 0.18758553 0.16291229 0.16217731 0.1773359 0.20063947][0.10324663 0.069180652 0.076126486 0.13420804 0.21871214 0.29670352 0.34710312 0.36060396 0.33935407 0.2979413 0.25726387 0.23229997 0.22638448 0.23744027 0.25959045][0.18337023 0.12343856 0.10679228 0.15207861 0.23677123 0.3258841 0.39579567 0.43186516 0.43132821 0.40341395 0.36453751 0.33336315 0.32133874 0.33154464 0.355402][0.24547529 0.16695482 0.13327102 0.17142558 0.26145002 0.36621091 0.45810145 0.51616675 0.53235722 0.51172107 0.46888345 0.4285312 0.41083574 0.4220129 0.44870698][0.26186988 0.17678227 0.13620229 0.17263395 0.26958549 0.38895184 0.49915433 0.57348686 0.59970582 0.57995558 0.52899981 0.4776623 0.45296153 0.46245933 0.48805559][0.23104103 0.15188758 0.11303305 0.14967062 0.25014377 0.37725922 0.49630779 0.57697791 0.60483587 0.5802682 0.51904142 0.45616341 0.4223462 0.42492265 0.44380367][0.16850266 0.10336807 0.071591169 0.10731944 0.20407176 0.32762471 0.4421902 0.51706934 0.53818166 0.5059464 0.43672121 0.36570188 0.32337689 0.31590658 0.32518584][0.092418507 0.044070035 0.0209102 0.052252054 0.13612953 0.24352534 0.34057155 0.39910245 0.4080621 0.3693352 0.29895103 0.22777431 0.18208088 0.16627781 0.16745013][0.019779904 -0.013291123 -0.028829427 -0.0050492091 0.059412852 0.14217687 0.21403727 0.2510241 0.24628013 0.20389979 0.13888578 0.075246289 0.033199809 0.015954169 0.016238868][-0.029270627 -0.051067237 -0.062215958 -0.048211988 -0.0054902374 0.050242931 0.095942952 0.11258566 0.0969741 0.054994114 0.00011309815 -0.050273638 -0.081953242 -0.092200689 -0.084308326][-0.045421828 -0.061611172 -0.07277298 -0.06940116 -0.04698788 -0.015367167 0.0085477354 0.01060138 -0.010252159 -0.047554407 -0.089691669 -0.12415001 -0.14131382 -0.13904299 -0.118822][-0.039306734 -0.054372881 -0.06855043 -0.075063936 -0.069034748 -0.055609532 -0.046412174 -0.051619895 -0.071711846 -0.1007316 -0.12924553 -0.14792912 -0.15017097 -0.13476598 -0.10307502]]...]
INFO - root - 2017-12-11 08:21:44.725262: step 38610, loss = 0.70, batch loss = 0.64 (14.7 examples/sec; 0.545 sec/batch; 44h:30m:38s remains)
INFO - root - 2017-12-11 08:21:50.173412: step 38620, loss = 0.70, batch loss = 0.64 (14.9 examples/sec; 0.536 sec/batch; 43h:45m:32s remains)
INFO - root - 2017-12-11 08:21:55.275101: step 38630, loss = 0.70, batch loss = 0.64 (30.8 examples/sec; 0.260 sec/batch; 21h:12m:26s remains)
INFO - root - 2017-12-11 08:22:00.780894: step 38640, loss = 0.70, batch loss = 0.64 (14.5 examples/sec; 0.552 sec/batch; 45h:04m:49s remains)
INFO - root - 2017-12-11 08:22:06.306935: step 38650, loss = 0.71, batch loss = 0.65 (14.9 examples/sec; 0.537 sec/batch; 43h:50m:31s remains)
INFO - root - 2017-12-11 08:22:11.873204: step 38660, loss = 0.70, batch loss = 0.64 (14.8 examples/sec; 0.542 sec/batch; 44h:16m:06s remains)
INFO - root - 2017-12-11 08:22:17.412882: step 38670, loss = 0.69, batch loss = 0.64 (14.9 examples/sec; 0.538 sec/batch; 43h:55m:51s remains)
INFO - root - 2017-12-11 08:22:22.836299: step 38680, loss = 0.70, batch loss = 0.64 (14.7 examples/sec; 0.545 sec/batch; 44h:29m:06s remains)
INFO - root - 2017-12-11 08:22:28.314306: step 38690, loss = 0.71, batch loss = 0.65 (14.2 examples/sec; 0.563 sec/batch; 45h:59m:17s remains)
INFO - root - 2017-12-11 08:22:33.830714: step 38700, loss = 0.70, batch loss = 0.64 (14.2 examples/sec; 0.565 sec/batch; 46h:06m:34s remains)
2017-12-11 08:22:34.393936: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.30078277 0.31098291 0.30107948 0.28350142 0.26230562 0.24611308 0.24539296 0.2580362 0.27080691 0.26717064 0.24156617 0.19721663 0.14403109 0.096221119 0.065228917][0.28828815 0.29707682 0.28805068 0.27309951 0.25800005 0.25219736 0.26383492 0.28749672 0.30558246 0.30012 0.26630464 0.20924301 0.14118631 0.0800992 0.039776314][0.23392019 0.2421772 0.23790821 0.23071098 0.22619857 0.23350136 0.25856045 0.29355532 0.31862172 0.31408551 0.27503952 0.20809199 0.1278964 0.056213457 0.0086841965][0.16832849 0.17987546 0.1846132 0.18938826 0.19834095 0.21921399 0.25628883 0.30014133 0.33072177 0.32773125 0.28490907 0.20982045 0.11893566 0.037884243 -0.015645588][0.10484734 0.122697 0.14044647 0.16241863 0.19011554 0.22832428 0.27799177 0.32772955 0.35884237 0.3526473 0.30270562 0.21708862 0.11492559 0.024863053 -0.034315921][0.052571148 0.077964254 0.11183393 0.15553455 0.2065084 0.26452491 0.32558781 0.37601608 0.39893115 0.37978444 0.3150984 0.21520407 0.10319258 0.0082312133 -0.052697789][0.024608072 0.057277065 0.10642587 0.17100424 0.24354826 0.31820378 0.38582489 0.43065524 0.43791184 0.39771309 0.31261602 0.19855006 0.081977651 -0.010407349 -0.067234136][0.026586594 0.063452184 0.12208459 0.20115452 0.2883274 0.37265307 0.44099045 0.4766649 0.46653432 0.40461245 0.29990625 0.17452234 0.057955619 -0.027091004 -0.0757752][0.047544137 0.081889711 0.14075294 0.22435835 0.31681606 0.40295765 0.46683118 0.49204764 0.46726918 0.38962823 0.2730962 0.14452948 0.034114111 -0.039835621 -0.077575527][0.064796686 0.088208534 0.13703965 0.21451904 0.30410969 0.38723698 0.44515958 0.46194276 0.4281756 0.34394476 0.22679968 0.10547927 0.0084399078 -0.050676212 -0.075574428][0.065518811 0.073715664 0.10498948 0.16599061 0.24297439 0.3164807 0.36625913 0.37679619 0.34042114 0.25978521 0.15437976 0.051699661 -0.023887709 -0.06398429 -0.0745996][0.0483251 0.042337794 0.054043557 0.092100546 0.14744379 0.20312135 0.24056602 0.24607036 0.21342298 0.14626551 0.063391067 -0.011548626 -0.06004829 -0.078768753 -0.075157613][0.020379353 0.00424652 -0.00019272519 0.015675578 0.046974521 0.081117615 0.10393244 0.10520873 0.080432758 0.032739334 -0.02245477 -0.0670326 -0.089155018 -0.089208946 -0.074701034][-0.0076337988 -0.029092403 -0.043582231 -0.044085156 -0.032396197 -0.016856495 -0.0069039147 -0.0088365329 -0.025077149 -0.053219959 -0.082634233 -0.10147948 -0.10381074 -0.0919425 -0.072120287][-0.032233905 -0.054034244 -0.072301582 -0.08184541 -0.0819883 -0.077517122 -0.074523814 -0.076871023 -0.085619986 -0.098906606 -0.11028438 -0.11311025 -0.10512143 -0.088907443 -0.068993077]]...]
INFO - root - 2017-12-11 08:22:40.000942: step 38710, loss = 0.69, batch loss = 0.63 (14.2 examples/sec; 0.564 sec/batch; 45h:59m:24s remains)
INFO - root - 2017-12-11 08:22:45.477271: step 38720, loss = 0.71, batch loss = 0.65 (14.7 examples/sec; 0.543 sec/batch; 44h:17m:40s remains)
INFO - root - 2017-12-11 08:22:50.533492: step 38730, loss = 0.71, batch loss = 0.65 (14.8 examples/sec; 0.540 sec/batch; 44h:04m:23s remains)
INFO - root - 2017-12-11 08:22:56.030906: step 38740, loss = 0.68, batch loss = 0.63 (14.4 examples/sec; 0.557 sec/batch; 45h:24m:59s remains)
INFO - root - 2017-12-11 08:23:01.540631: step 38750, loss = 0.71, batch loss = 0.65 (14.7 examples/sec; 0.545 sec/batch; 44h:27m:52s remains)
INFO - root - 2017-12-11 08:23:07.174319: step 38760, loss = 0.68, batch loss = 0.63 (14.0 examples/sec; 0.571 sec/batch; 46h:37m:24s remains)
INFO - root - 2017-12-11 08:23:12.694706: step 38770, loss = 0.70, batch loss = 0.64 (14.5 examples/sec; 0.550 sec/batch; 44h:52m:57s remains)
INFO - root - 2017-12-11 08:23:18.136976: step 38780, loss = 0.71, batch loss = 0.65 (15.0 examples/sec; 0.534 sec/batch; 43h:32m:30s remains)
INFO - root - 2017-12-11 08:23:23.606403: step 38790, loss = 0.71, batch loss = 0.65 (14.6 examples/sec; 0.550 sec/batch; 44h:50m:19s remains)
INFO - root - 2017-12-11 08:23:29.138984: step 38800, loss = 0.69, batch loss = 0.63 (14.2 examples/sec; 0.563 sec/batch; 45h:56m:56s remains)
2017-12-11 08:23:29.782100: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.63487524 0.5768 0.49052295 0.41903213 0.38766426 0.3996866 0.42774653 0.44341177 0.43068221 0.38781884 0.33255598 0.28196567 0.24025209 0.2053839 0.17417558][0.56915122 0.5056476 0.4158847 0.34405783 0.31389406 0.32826957 0.35760427 0.37261805 0.35990947 0.31859824 0.26623595 0.21850975 0.18135588 0.15162498 0.12484769][0.42205423 0.37028402 0.30048123 0.25126496 0.2423867 0.27486813 0.31760836 0.34011269 0.33086231 0.2899757 0.23215519 0.17422073 0.12862432 0.09747842 0.073560767][0.26865456 0.24172486 0.20857957 0.19887263 0.22620362 0.28796095 0.35120067 0.38389435 0.37489411 0.32410052 0.24597929 0.16389331 0.10067125 0.064817652 0.045946188][0.15301304 0.15951441 0.17500092 0.21657166 0.28787389 0.38066337 0.46204013 0.50098044 0.48666489 0.41912723 0.31444111 0.20261277 0.11695874 0.072984613 0.058553983][0.088151984 0.12306502 0.1816183 0.26945588 0.37900755 0.49484807 0.58576506 0.62444812 0.60227728 0.52029216 0.39566061 0.26160347 0.15845737 0.10753307 0.096718468][0.073463 0.11941423 0.20085421 0.31654707 0.44956988 0.57685584 0.66820973 0.70131105 0.67093766 0.58003628 0.44635591 0.30323896 0.19183183 0.13536006 0.12516645][0.090556145 0.13013448 0.21180561 0.33543482 0.4784236 0.61000425 0.69850731 0.72559059 0.68814129 0.59101957 0.45266128 0.30671129 0.19182412 0.12889113 0.11404542][0.11003735 0.13339499 0.20218302 0.32110885 0.46494356 0.59612334 0.68069947 0.70327103 0.66218358 0.56351078 0.42588523 0.2817862 0.16448267 0.091300167 0.06456174][0.10268862 0.11309084 0.17113461 0.28457239 0.42803028 0.55981952 0.643193 0.66342497 0.62100089 0.524989 0.3929452 0.25308013 0.13253789 0.046246186 0.0035722353][0.070129931 0.07790494 0.13389537 0.24556316 0.38894367 0.5213871 0.60450685 0.62307131 0.58030778 0.49018556 0.36986977 0.24174821 0.12521882 0.032265954 -0.023070626][0.035637848 0.047497254 0.10728645 0.21879807 0.35904926 0.48735306 0.56679326 0.58365422 0.54516751 0.46959433 0.37219235 0.26795173 0.16757078 0.077770829 0.013998147][0.024194131 0.040614773 0.10165738 0.20743902 0.33506182 0.44787753 0.51480824 0.52785832 0.49943548 0.44932416 0.38757613 0.320251 0.24913438 0.17443091 0.1094862][0.044973474 0.058647946 0.11087225 0.19897647 0.30050075 0.38516968 0.43048745 0.43606433 0.41971937 0.40145674 0.38409445 0.36333385 0.33154938 0.28235555 0.22477953][0.0735523 0.076374419 0.10968817 0.17064518 0.23835035 0.29052785 0.31370822 0.31254086 0.30719826 0.31645149 0.33726546 0.35715836 0.36149618 0.33875948 0.29335463]]...]
INFO - root - 2017-12-11 08:23:35.395917: step 38810, loss = 0.71, batch loss = 0.65 (14.2 examples/sec; 0.565 sec/batch; 46h:03m:41s remains)
INFO - root - 2017-12-11 08:23:40.959406: step 38820, loss = 0.70, batch loss = 0.64 (14.4 examples/sec; 0.555 sec/batch; 45h:18m:12s remains)
INFO - root - 2017-12-11 08:23:46.164935: step 38830, loss = 0.70, batch loss = 0.64 (14.7 examples/sec; 0.543 sec/batch; 44h:17m:33s remains)
INFO - root - 2017-12-11 08:23:51.673321: step 38840, loss = 0.69, batch loss = 0.63 (15.0 examples/sec; 0.533 sec/batch; 43h:30m:06s remains)
INFO - root - 2017-12-11 08:23:57.185281: step 38850, loss = 0.71, batch loss = 0.66 (14.4 examples/sec; 0.556 sec/batch; 45h:22m:54s remains)
INFO - root - 2017-12-11 08:24:02.751254: step 38860, loss = 0.69, batch loss = 0.63 (14.7 examples/sec; 0.545 sec/batch; 44h:27m:37s remains)
INFO - root - 2017-12-11 08:24:08.203503: step 38870, loss = 0.68, batch loss = 0.63 (15.4 examples/sec; 0.520 sec/batch; 42h:25m:37s remains)
INFO - root - 2017-12-11 08:24:13.702522: step 38880, loss = 0.70, batch loss = 0.64 (14.3 examples/sec; 0.558 sec/batch; 45h:29m:22s remains)
INFO - root - 2017-12-11 08:24:19.279499: step 38890, loss = 0.70, batch loss = 0.64 (13.8 examples/sec; 0.580 sec/batch; 47h:19m:39s remains)
INFO - root - 2017-12-11 08:24:24.732581: step 38900, loss = 0.70, batch loss = 0.64 (14.7 examples/sec; 0.546 sec/batch; 44h:31m:54s remains)
2017-12-11 08:24:25.331345: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.23644903 0.26279503 0.30068782 0.33444062 0.34940124 0.34415376 0.3237873 0.29087076 0.24765082 0.20771919 0.18959665 0.20266569 0.22968934 0.24733943 0.23904473][0.265625 0.29034743 0.32413313 0.35439527 0.36799026 0.36305088 0.34391183 0.31108952 0.26281419 0.21274535 0.18156156 0.18083097 0.19528744 0.20513374 0.19704218][0.26372394 0.28816205 0.31647265 0.34061298 0.35178944 0.34808913 0.33202919 0.30161339 0.25107184 0.19240892 0.14636619 0.12678932 0.12327497 0.12222306 0.114455][0.24294215 0.27247986 0.29865155 0.31734991 0.32665598 0.32557961 0.31408089 0.28753293 0.23746361 0.17258263 0.11143812 0.070741884 0.047456827 0.035157852 0.027989946][0.22021021 0.2629551 0.29305911 0.30940866 0.31873563 0.32260165 0.31755286 0.29599684 0.2481017 0.17849897 0.10264405 0.0417519 0.0013787041 -0.018913355 -0.024759965][0.2065416 0.26543885 0.30375314 0.32227963 0.33560061 0.34764656 0.35078362 0.33432811 0.28845063 0.21512584 0.12788704 0.054118779 0.0058881231 -0.01583793 -0.020014307][0.21031389 0.28185326 0.3277545 0.35059085 0.3703872 0.39190036 0.40326056 0.39080718 0.34547853 0.26969269 0.17772816 0.10322911 0.059927378 0.045015138 0.043744411][0.22677948 0.3015849 0.3492687 0.37466747 0.40009591 0.42964202 0.44792202 0.43870994 0.39356393 0.31877553 0.23238954 0.17179881 0.14603676 0.14417705 0.14537866][0.25121689 0.31668907 0.35625896 0.37846148 0.40491912 0.43738824 0.45885012 0.4517988 0.40850088 0.3410925 0.27221853 0.23761357 0.23656899 0.2488759 0.25046811][0.27909169 0.32212964 0.3435066 0.35570472 0.37643281 0.40373403 0.42173743 0.41448256 0.37567538 0.32281026 0.28116187 0.27859697 0.30237609 0.32519692 0.32418805][0.30128372 0.31434393 0.31108382 0.30711359 0.31479281 0.32890558 0.33727208 0.32769087 0.29546908 0.2610732 0.24930522 0.27707389 0.3209869 0.35026193 0.34624583][0.30850485 0.29231307 0.26444015 0.24259733 0.23424006 0.23259118 0.22956227 0.21710418 0.19156484 0.17337243 0.18449293 0.23202829 0.28713411 0.3195329 0.31547859][0.28639138 0.24961995 0.20493937 0.17037989 0.1496342 0.13608089 0.12495147 0.11127552 0.091826178 0.084229007 0.10627146 0.15893723 0.2142304 0.24559739 0.24384964][0.22887333 0.18536301 0.13701788 0.0994675 0.074192561 0.055941988 0.042433042 0.030550821 0.017128505 0.014981395 0.036984377 0.082188651 0.12822482 0.15427366 0.15463549][0.1375868 0.1000427 0.060592502 0.030264163 0.0094216671 -0.0054504587 -0.015426531 -0.022845021 -0.030837942 -0.031517252 -0.016427277 0.014540215 0.046596963 0.065068789 0.0663428]]...]
INFO - root - 2017-12-11 08:24:30.872109: step 38910, loss = 0.69, batch loss = 0.63 (14.4 examples/sec; 0.556 sec/batch; 45h:19m:07s remains)
INFO - root - 2017-12-11 08:24:36.268524: step 38920, loss = 0.69, batch loss = 0.64 (15.0 examples/sec; 0.535 sec/batch; 43h:35m:26s remains)
INFO - root - 2017-12-11 08:24:41.533561: step 38930, loss = 0.68, batch loss = 0.63 (14.7 examples/sec; 0.545 sec/batch; 44h:27m:09s remains)
INFO - root - 2017-12-11 08:24:47.034582: step 38940, loss = 0.70, batch loss = 0.64 (14.9 examples/sec; 0.538 sec/batch; 43h:53m:38s remains)
INFO - root - 2017-12-11 08:24:52.446770: step 38950, loss = 0.70, batch loss = 0.65 (14.8 examples/sec; 0.542 sec/batch; 44h:11m:15s remains)
INFO - root - 2017-12-11 08:24:57.922448: step 38960, loss = 0.69, batch loss = 0.63 (14.7 examples/sec; 0.544 sec/batch; 44h:23m:37s remains)
INFO - root - 2017-12-11 08:25:03.487054: step 38970, loss = 0.70, batch loss = 0.64 (14.2 examples/sec; 0.562 sec/batch; 45h:51m:25s remains)
INFO - root - 2017-12-11 08:25:09.031048: step 38980, loss = 0.72, batch loss = 0.66 (14.6 examples/sec; 0.549 sec/batch; 44h:45m:49s remains)
INFO - root - 2017-12-11 08:25:14.551315: step 38990, loss = 0.70, batch loss = 0.64 (14.2 examples/sec; 0.564 sec/batch; 45h:58m:11s remains)
INFO - root - 2017-12-11 08:25:20.080408: step 39000, loss = 0.69, batch loss = 0.64 (14.4 examples/sec; 0.555 sec/batch; 45h:15m:25s remains)
2017-12-11 08:25:20.645551: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.063187905 -0.059194833 -0.052695043 -0.046769522 -0.043546367 -0.044494838 -0.049369656 -0.056628719 -0.063536346 -0.0677997 -0.068153553 -0.0648198 -0.059829559 -0.055266391 -0.052449115][-0.0622646 -0.055152692 -0.042396288 -0.028579839 -0.018478887 -0.015742734 -0.021278659 -0.032986537 -0.047416233 -0.06086785 -0.069971815 -0.073086984 -0.07135237 -0.067170553 -0.062638037][-0.041409191 -0.035276514 -0.01813071 0.0058852248 0.028229574 0.040791031 0.039194629 0.024596866 0.0010934997 -0.025942104 -0.049792808 -0.065603949 -0.072940186 -0.074274272 -0.07169348][0.010105763 0.0084086265 0.025518285 0.060141727 0.098866448 0.12717155 0.13526997 0.1218662 0.0905421 0.0481401 0.0052107396 -0.029147407 -0.052150954 -0.065681845 -0.0709375][0.095077932 0.081901282 0.0931971 0.13344131 0.18511347 0.22878422 0.2502448 0.24474934 0.21287853 0.15956782 0.097658664 0.040690273 -0.00469212 -0.037926886 -0.057821441][0.20008357 0.17712033 0.17833856 0.21575503 0.27075318 0.32345253 0.35890761 0.3688221 0.34755507 0.29323274 0.2180275 0.13831969 0.066011786 0.0066881487 -0.033889566][0.29545325 0.27139422 0.26368976 0.29196608 0.34033865 0.3937014 0.4397077 0.46679452 0.46215278 0.41478163 0.33350402 0.23598409 0.13893655 0.053997815 -0.0076854709][0.35837379 0.34506652 0.3355079 0.35441971 0.39118257 0.4378742 0.48623139 0.52292311 0.53038037 0.49082235 0.40956464 0.30298239 0.19006772 0.087439567 0.010535081][0.38723981 0.39655972 0.39580476 0.41055498 0.43527192 0.46835384 0.50537938 0.53450477 0.539314 0.50061476 0.42131796 0.31436545 0.19786766 0.0905801 0.0097735906][0.39131802 0.42598349 0.43952921 0.45425463 0.46795705 0.48194894 0.49482113 0.49945918 0.48619 0.439863 0.36227855 0.26202118 0.15430295 0.05710841 -0.014200516][0.38070485 0.43509147 0.46232674 0.47838843 0.4823046 0.47544169 0.45851347 0.43043855 0.39025062 0.32985219 0.2518841 0.16228355 0.073054031 -0.0014835816 -0.051550515][0.35380304 0.41631728 0.45132923 0.468654 0.46682128 0.44488657 0.40386215 0.34719735 0.28097811 0.2046833 0.12413563 0.045850497 -0.020399347 -0.065841883 -0.088435955][0.31236491 0.37018618 0.40584913 0.4255583 0.426629 0.40328875 0.35337338 0.28177798 0.19841512 0.10955439 0.026022036 -0.042869691 -0.089216873 -0.10973047 -0.10869826][0.27952081 0.32231951 0.35082459 0.3719162 0.38254333 0.37232125 0.33307481 0.26652491 0.18155274 0.088434331 0.0023509064 -0.063938454 -0.10303041 -0.11390506 -0.10274035][0.27775398 0.29861605 0.31276283 0.33156118 0.3533859 0.36498061 0.35146815 0.30753893 0.23628214 0.14748712 0.058521722 -0.014232911 -0.061084621 -0.0789052 -0.071619019]]...]
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-SGD-lr0.001-clip50-initfromfixqian30k-fixqian
INFO - root - 2017-12-11 08:25:26.135328: step 39010, loss = 0.69, batch loss = 0.63 (14.4 examples/sec; 0.556 sec/batch; 45h:19m:05s remains)
INFO - root - 2017-12-11 08:25:31.236660: step 39020, loss = 0.71, batch loss = 0.65 (14.6 examples/sec; 0.548 sec/batch; 44h:41m:57s remains)
INFO - root - 2017-12-11 08:25:36.771977: step 39030, loss = 0.70, batch loss = 0.64 (14.4 examples/sec; 0.557 sec/batch; 45h:23m:36s remains)
INFO - root - 2017-12-11 08:25:42.240146: step 39040, loss = 0.69, batch loss = 0.63 (14.8 examples/sec; 0.542 sec/batch; 44h:09m:24s remains)
INFO - root - 2017-12-11 08:25:47.798049: step 39050, loss = 0.68, batch loss = 0.62 (14.1 examples/sec; 0.567 sec/batch; 46h:13m:03s remains)
INFO - root - 2017-12-11 08:25:53.267728: step 39060, loss = 0.70, batch loss = 0.64 (14.3 examples/sec; 0.559 sec/batch; 45h:34m:38s remains)
INFO - root - 2017-12-11 08:25:58.738452: step 39070, loss = 0.71, batch loss = 0.65 (14.8 examples/sec; 0.542 sec/batch; 44h:09m:16s remains)
INFO - root - 2017-12-11 08:26:04.326157: step 39080, loss = 0.69, batch loss = 0.63 (14.5 examples/sec; 0.553 sec/batch; 45h:02m:17s remains)
INFO - root - 2017-12-11 08:26:09.851381: step 39090, loss = 0.68, batch loss = 0.63 (14.7 examples/sec; 0.545 sec/batch; 44h:22m:55s remains)
INFO - root - 2017-12-11 08:26:15.355315: step 39100, loss = 0.71, batch loss = 0.66 (14.8 examples/sec; 0.541 sec/batch; 44h:05m:12s remains)
2017-12-11 08:26:15.933252: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.10640945 0.19832505 0.29275116 0.3742229 0.4331719 0.46632665 0.46297649 0.43455532 0.40112588 0.37781355 0.37815762 0.38945314 0.39914826 0.40030047 0.39745536][0.097468331 0.19295853 0.29300252 0.37980959 0.44123656 0.47224519 0.46176615 0.42377517 0.38134536 0.35185269 0.34472498 0.34670407 0.34907416 0.34698465 0.34279644][0.081496745 0.17519917 0.27654511 0.36591876 0.42861331 0.45715785 0.44001698 0.39231369 0.3391293 0.30121663 0.28610653 0.2808255 0.27913925 0.27762508 0.27714571][0.060238712 0.1496426 0.25243407 0.34792104 0.41724834 0.44875619 0.42970896 0.37539628 0.31266743 0.26744369 0.24817878 0.24093267 0.23896776 0.23807849 0.23833463][0.049957614 0.13847779 0.24491982 0.34778693 0.42491192 0.46168849 0.44538942 0.39089018 0.32559413 0.27864474 0.26006648 0.25397375 0.25052482 0.24394257 0.23552229][0.055025477 0.14765574 0.25970256 0.36880681 0.45206591 0.49454489 0.48472849 0.43607771 0.3744711 0.32926524 0.31106135 0.30305159 0.29286739 0.27479541 0.2526339][0.061964441 0.15811485 0.27341709 0.38538197 0.47348094 0.52360582 0.5247311 0.48687923 0.43276954 0.39014682 0.36922869 0.35443702 0.33314058 0.30264223 0.26940754][0.064121582 0.15777694 0.26984465 0.37900153 0.46923843 0.52754176 0.54127651 0.51543349 0.4692485 0.42848146 0.4020347 0.37724704 0.34384909 0.30389649 0.2660397][0.061714184 0.14470352 0.24465905 0.34296444 0.42882481 0.48964956 0.5098449 0.4907223 0.44969922 0.41016939 0.37827837 0.34531692 0.30510846 0.26409829 0.23132317][0.057997882 0.12638111 0.2084337 0.28812146 0.35977882 0.41143864 0.42622182 0.40535682 0.3665027 0.32973295 0.29662135 0.26153126 0.22264695 0.18984696 0.17205667][0.053116746 0.10675359 0.1689672 0.22551274 0.27535194 0.30836678 0.30923945 0.28157258 0.24319957 0.21046019 0.18061616 0.15052216 0.12164559 0.10491002 0.10856741][0.048104063 0.088751562 0.13183026 0.16441965 0.18922718 0.19952852 0.1847851 0.15194178 0.11729854 0.092952073 0.072869509 0.055090457 0.0421823 0.043887522 0.068086438][0.048287451 0.081339866 0.11015094 0.12270876 0.12422939 0.11246465 0.084182575 0.050071094 0.023637254 0.012667512 0.008538601 0.0084662177 0.013799965 0.03152455 0.069543481][0.051036447 0.079466246 0.098595589 0.097192466 0.081315234 0.052867841 0.016458554 -0.014093186 -0.028214164 -0.021798696 -0.0053651705 0.015747774 0.039849628 0.070591144 0.11538646][0.055865817 0.081171863 0.0954869 0.088538662 0.064942792 0.02893368 -0.0089305388 -0.032790087 -0.034116913 -0.011233144 0.024559876 0.0647579 0.10360057 0.14154768 0.18458341]]...]
INFO - root - 2017-12-11 08:26:21.469883: step 39110, loss = 0.71, batch loss = 0.65 (14.9 examples/sec; 0.537 sec/batch; 43h:43m:40s remains)
INFO - root - 2017-12-11 08:26:26.583558: step 39120, loss = 0.70, batch loss = 0.64 (14.7 examples/sec; 0.544 sec/batch; 44h:22m:05s remains)
INFO - root - 2017-12-11 08:26:32.206716: step 39130, loss = 0.67, batch loss = 0.61 (15.0 examples/sec; 0.533 sec/batch; 43h:23m:45s remains)
INFO - root - 2017-12-11 08:26:37.690972: step 39140, loss = 0.70, batch loss = 0.64 (14.7 examples/sec; 0.545 sec/batch; 44h:22m:54s remains)
INFO - root - 2017-12-11 08:26:43.218419: step 39150, loss = 0.69, batch loss = 0.63 (14.8 examples/sec; 0.541 sec/batch; 44h:04m:30s remains)
INFO - root - 2017-12-11 08:26:48.773261: step 39160, loss = 0.70, batch loss = 0.64 (14.8 examples/sec; 0.542 sec/batch; 44h:10m:29s remains)
INFO - root - 2017-12-11 08:26:54.322493: step 39170, loss = 0.69, batch loss = 0.63 (14.2 examples/sec; 0.562 sec/batch; 45h:46m:07s remains)
INFO - root - 2017-12-11 08:26:59.842581: step 39180, loss = 0.70, batch loss = 0.64 (14.4 examples/sec; 0.555 sec/batch; 45h:14m:14s remains)
INFO - root - 2017-12-11 08:27:05.288351: step 39190, loss = 0.72, batch loss = 0.66 (15.0 examples/sec; 0.532 sec/batch; 43h:21m:05s remains)
INFO - root - 2017-12-11 08:27:10.876656: step 39200, loss = 0.71, batch loss = 0.65 (14.0 examples/sec; 0.571 sec/batch; 46h:32m:21s remains)
2017-12-11 08:27:11.449859: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.049965747 -0.052376706 -0.052526586 -0.0527904 -0.053056743 -0.052798312 -0.0518487 -0.050688863 -0.049765971 -0.049229771 -0.048718944 -0.048108783 -0.047998149 -0.04775269 -0.047229413][-0.050349239 -0.053097986 -0.053120714 -0.053097427 -0.052896585 -0.051824085 -0.049618352 -0.047213778 -0.045252092 -0.044386905 -0.04437881 -0.044873759 -0.045835517 -0.046233974 -0.046138853][-0.049501862 -0.052107889 -0.052032068 -0.05243998 -0.053039592 -0.052768778 -0.050735839 -0.047856729 -0.044661067 -0.042742204 -0.042376436 -0.043222614 -0.044656992 -0.045432 -0.04575358][-0.035668518 -0.035248637 -0.034602974 -0.037401028 -0.042739365 -0.048645105 -0.052223112 -0.052722365 -0.049877882 -0.04639446 -0.044126313 -0.043160394 -0.04282837 -0.0423065 -0.042537957][0.004174639 0.012501212 0.01518534 0.0080400389 -0.0070150835 -0.02654996 -0.043800019 -0.053886518 -0.054560065 -0.04981098 -0.044075672 -0.03868271 -0.033717331 -0.029665304 -0.029099958][0.070708841 0.09100011 0.097067036 0.084301151 0.05594584 0.017336925 -0.019818211 -0.044741608 -0.052188594 -0.0472282 -0.037360232 -0.025766481 -0.013937514 -0.0048063174 -0.0026814148][0.14582141 0.17785433 0.18707015 0.1686154 0.12723845 0.070219792 0.013489546 -0.026610564 -0.04179645 -0.03777026 -0.024646336 -0.00754743 0.010651318 0.024539188 0.028360602][0.19585179 0.2339033 0.24434814 0.22228916 0.17355967 0.10662998 0.03909209 -0.0099290665 -0.030350147 -0.027982492 -0.014123841 0.00495752 0.025644213 0.041503966 0.046304941][0.19229442 0.22724818 0.23616351 0.21401621 0.16679285 0.10296997 0.038501475 -0.0087607391 -0.029174289 -0.028000768 -0.0161223 0.00038853838 0.018330066 0.032125223 0.036459189][0.13173875 0.15548509 0.16067033 0.14192615 0.1042939 0.055092975 0.0061749918 -0.029090313 -0.043571047 -0.04160348 -0.032732416 -0.021856636 -0.0107914 -0.0027882578 -0.00079714204][0.044617098 0.054354351 0.055407368 0.04199253 0.017670956 -0.012062085 -0.039997451 -0.058195338 -0.062973365 -0.05796995 -0.051052961 -0.04577063 -0.042393468 -0.041715074 -0.04370993][-0.024795327 -0.026255518 -0.028388681 -0.036785994 -0.049139947 -0.061977051 -0.071538284 -0.074052021 -0.068695225 -0.058781765 -0.050665449 -0.047080584 -0.047539677 -0.05195909 -0.058051851][-0.056306608 -0.063326053 -0.066584475 -0.070883319 -0.074865095 -0.076459765 -0.07314565 -0.062966421 -0.046345167 -0.027235188 -0.011444773 -0.0023495066 0.00026478674 -0.0047529917 -0.014531778][-0.056202572 -0.063708335 -0.066009589 -0.06681823 -0.065380923 -0.059740812 -0.046881173 -0.024872225 0.0061031478 0.040123735 0.070516266 0.091976769 0.10390057 0.10223885 0.088927664][-0.040903062 -0.045966808 -0.046544649 -0.045010176 -0.040602446 -0.030088944 -0.0092631234 0.024812564 0.072923832 0.12570728 0.17385682 0.20976721 0.23277594 0.23593508 0.21952271]]...]
INFO - root - 2017-12-11 08:27:17.002406: step 39210, loss = 0.69, batch loss = 0.63 (14.7 examples/sec; 0.543 sec/batch; 44h:15m:36s remains)
INFO - root - 2017-12-11 08:27:22.296183: step 39220, loss = 0.70, batch loss = 0.64 (14.1 examples/sec; 0.566 sec/batch; 46h:08m:36s remains)
INFO - root - 2017-12-11 08:27:27.747939: step 39230, loss = 0.70, batch loss = 0.64 (14.8 examples/sec; 0.539 sec/batch; 43h:55m:32s remains)
INFO - root - 2017-12-11 08:27:33.198916: step 39240, loss = 0.69, batch loss = 0.63 (14.6 examples/sec; 0.549 sec/batch; 44h:44m:22s remains)
INFO - root - 2017-12-11 08:27:38.755883: step 39250, loss = 0.70, batch loss = 0.64 (14.5 examples/sec; 0.551 sec/batch; 44h:54m:18s remains)
INFO - root - 2017-12-11 08:27:44.270861: step 39260, loss = 0.70, batch loss = 0.64 (14.5 examples/sec; 0.552 sec/batch; 44h:56m:54s remains)
INFO - root - 2017-12-11 08:27:49.722400: step 39270, loss = 0.69, batch loss = 0.63 (14.5 examples/sec; 0.550 sec/batch; 44h:48m:24s remains)
INFO - root - 2017-12-11 08:27:55.245296: step 39280, loss = 0.71, batch loss = 0.65 (14.2 examples/sec; 0.565 sec/batch; 46h:00m:02s remains)
INFO - root - 2017-12-11 08:28:00.794852: step 39290, loss = 0.70, batch loss = 0.65 (14.6 examples/sec; 0.547 sec/batch; 44h:31m:07s remains)
INFO - root - 2017-12-11 08:28:06.274267: step 39300, loss = 0.70, batch loss = 0.64 (15.4 examples/sec; 0.520 sec/batch; 42h:22m:49s remains)
2017-12-11 08:28:07.061352: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0743258 -0.074090704 -0.073252335 -0.07212957 -0.070090458 -0.067047626 -0.063780278 -0.061099224 -0.060802922 -0.065099321 -0.073400065 -0.0826965 -0.089800946 -0.092525579 -0.0900712][-0.073875614 -0.07063657 -0.066672571 -0.061382707 -0.054004055 -0.045113366 -0.035996813 -0.028168194 -0.026152818 -0.034318741 -0.051413205 -0.071649618 -0.089159869 -0.1001165 -0.10144572][-0.055783406 -0.046732273 -0.037567604 -0.026138345 -0.011416087 0.0050195744 0.021404283 0.035480965 0.039346453 0.025717858 -0.0039544585 -0.040300913 -0.072897419 -0.094320647 -0.09877605][-0.014676923 0.0040506176 0.020952571 0.040276691 0.063455448 0.088043906 0.11191092 0.1320824 0.13726261 0.11681784 0.071893126 0.016187644 -0.033845887 -0.065833665 -0.071966283][0.052400555 0.08544299 0.11323783 0.14133634 0.17202981 0.20266484 0.23076722 0.25265849 0.25510061 0.22544828 0.16518268 0.092044406 0.027840853 -0.010599884 -0.015179952][0.14838497 0.19676802 0.23411717 0.26623404 0.29725006 0.32609156 0.35104367 0.36746264 0.36259589 0.32444814 0.25574991 0.17534685 0.10674843 0.068953983 0.069732659][0.2669504 0.32264265 0.36142421 0.38751951 0.40748873 0.42420605 0.43856117 0.44521067 0.43354061 0.39422169 0.33048055 0.25802931 0.19778231 0.16840504 0.17741196][0.38550976 0.43363014 0.46000555 0.46876633 0.46899024 0.46880332 0.47205666 0.47242817 0.46143898 0.43242487 0.38785738 0.33723384 0.29607418 0.28065446 0.29839852][0.476611 0.50259089 0.50529975 0.49327338 0.47667596 0.46698615 0.46827838 0.47125825 0.46816981 0.45547739 0.43585771 0.41260317 0.39446989 0.39361274 0.41713464][0.51984024 0.51484883 0.49419996 0.47151557 0.45475721 0.45332807 0.46649814 0.47946128 0.48440096 0.48329169 0.48214996 0.48064592 0.4806346 0.488633 0.51025432][0.50213128 0.47351468 0.44433868 0.43189076 0.43666849 0.45996287 0.49319685 0.51671314 0.52304018 0.52180153 0.52561271 0.5341751 0.54219252 0.54932743 0.55871975][0.42992643 0.39657944 0.38172141 0.40092203 0.44255435 0.49585018 0.54585576 0.57288772 0.571927 0.55974025 0.55528551 0.56038845 0.56366134 0.55884504 0.54740447][0.32355154 0.30690452 0.32487974 0.38621518 0.46649924 0.54336721 0.59952974 0.61999822 0.604116 0.5735746 0.55143666 0.54201323 0.53018421 0.50630933 0.47191864][0.20927058 0.22181891 0.27615771 0.37189648 0.47661093 0.56136477 0.61101592 0.61768007 0.58506852 0.536698 0.49581522 0.46844891 0.43907514 0.39718467 0.34495193][0.10822085 0.14601129 0.22273657 0.3307353 0.43740627 0.5140872 0.54940563 0.54161704 0.49725679 0.43847036 0.38544825 0.34485337 0.30393764 0.25262398 0.19330944]]...]
INFO - root - 2017-12-11 08:28:12.347853: step 39310, loss = 0.70, batch loss = 0.64 (18.4 examples/sec; 0.436 sec/batch; 35h:28m:35s remains)
INFO - root - 2017-12-11 08:28:17.748065: step 39320, loss = 0.70, batch loss = 0.65 (14.7 examples/sec; 0.546 sec/batch; 44h:25m:32s remains)
INFO - root - 2017-12-11 08:28:23.415998: step 39330, loss = 0.71, batch loss = 0.65 (13.9 examples/sec; 0.574 sec/batch; 46h:46m:03s remains)
INFO - root - 2017-12-11 08:28:28.906666: step 39340, loss = 0.71, batch loss = 0.65 (15.0 examples/sec; 0.535 sec/batch; 43h:32m:40s remains)
INFO - root - 2017-12-11 08:28:34.388751: step 39350, loss = 0.71, batch loss = 0.65 (14.6 examples/sec; 0.546 sec/batch; 44h:29m:13s remains)
INFO - root - 2017-12-11 08:28:39.919173: step 39360, loss = 0.70, batch loss = 0.64 (14.2 examples/sec; 0.563 sec/batch; 45h:48m:24s remains)
INFO - root - 2017-12-11 08:28:45.505661: step 39370, loss = 0.69, batch loss = 0.64 (14.6 examples/sec; 0.547 sec/batch; 44h:33m:14s remains)
INFO - root - 2017-12-11 08:28:50.963727: step 39380, loss = 0.69, batch loss = 0.63 (14.7 examples/sec; 0.546 sec/batch; 44h:25m:13s remains)
INFO - root - 2017-12-11 08:28:56.483871: step 39390, loss = 0.69, batch loss = 0.63 (14.6 examples/sec; 0.549 sec/batch; 44h:43m:23s remains)
INFO - root - 2017-12-11 08:29:01.999841: step 39400, loss = 0.70, batch loss = 0.65 (14.7 examples/sec; 0.545 sec/batch; 44h:24m:26s remains)
2017-12-11 08:29:02.570069: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.041791156 -0.030342909 -0.013133124 0.0031443969 0.012891513 0.014531469 0.011016865 0.0044125933 -0.0075386623 -0.021994298 -0.031802908 -0.036052581 -0.037123997 -0.038253911 -0.038095921][-0.024098147 -0.0021996605 0.027678929 0.056572903 0.076310724 0.084594414 0.084730215 0.078691661 0.062993482 0.041835688 0.026190061 0.017097196 0.011357402 0.0058687576 0.0040258239][-2.2575379e-05 0.036344204 0.082593992 0.12742025 0.16026327 0.17990111 0.19081883 0.19335786 0.18072383 0.15696207 0.13706641 0.12080784 0.1039377 0.085673921 0.075137518][0.023071932 0.076745786 0.14319937 0.20790935 0.25792938 0.29491028 0.32462025 0.34329689 0.33847937 0.31276074 0.28617236 0.25726375 0.22132069 0.18197638 0.15601197][0.038607463 0.1099855 0.19868635 0.2858654 0.35559148 0.41263375 0.46305355 0.49743569 0.49697319 0.46417451 0.42500758 0.3788746 0.32194778 0.26237562 0.22314472][0.043965351 0.12946571 0.23745909 0.34514952 0.43281984 0.50696349 0.5727697 0.61525452 0.6126045 0.56841469 0.51402 0.45120803 0.3789756 0.30965719 0.26644015][0.040002655 0.13160841 0.25047371 0.37098181 0.47004652 0.55367517 0.62542969 0.66737127 0.6590274 0.60524929 0.53938925 0.46566829 0.38660622 0.31767258 0.27833074][0.02663804 0.11461943 0.23197764 0.3525525 0.45186019 0.53360111 0.59937972 0.6321187 0.61712587 0.56073964 0.49367392 0.4200134 0.34438032 0.28403762 0.25365382][0.0044863438 0.078260548 0.17992005 0.28569803 0.37296551 0.44210416 0.4922626 0.50957876 0.48725361 0.43402955 0.37528116 0.3132256 0.25248075 0.20894401 0.19198485][-0.022922518 0.028651476 0.10311423 0.18175146 0.2467633 0.29606202 0.32674512 0.32877389 0.3022081 0.25723079 0.21329029 0.17076665 0.13260844 0.11045007 0.10825665][-0.05078863 -0.023951918 0.018796308 0.064761043 0.10269511 0.13032202 0.14439327 0.13840766 0.11567321 0.084899515 0.059332017 0.037870307 0.021523429 0.017069899 0.024853917][-0.072585821 -0.066570714 -0.050979216 -0.033781938 -0.020249311 -0.01126448 -0.0087792305 -0.016001673 -0.029392049 -0.043432374 -0.050993241 -0.054766446 -0.055553149 -0.05022306 -0.039245065][-0.084258445 -0.091286369 -0.092828661 -0.093950488 -0.096269466 -0.099399388 -0.10365934 -0.10955596 -0.11420701 -0.11493812 -0.10949123 -0.1018931 -0.094343357 -0.086014107 -0.076773733][-0.086924106 -0.099835925 -0.10932079 -0.11869115 -0.12782222 -0.13585249 -0.14224885 -0.14623329 -0.14584693 -0.1401213 -0.1293415 -0.11749382 -0.10730994 -0.099350832 -0.093138881][-0.082936265 -0.096355714 -0.10700607 -0.11749556 -0.12705754 -0.13480228 -0.14007588 -0.14202642 -0.13959768 -0.13264957 -0.12215602 -0.11111782 -0.10197748 -0.095869444 -0.092400685]]...]
INFO - root - 2017-12-11 08:29:07.749372: step 39410, loss = 0.70, batch loss = 0.64 (14.3 examples/sec; 0.561 sec/batch; 45h:41m:33s remains)
INFO - root - 2017-12-11 08:29:13.242311: step 39420, loss = 0.70, batch loss = 0.64 (14.9 examples/sec; 0.535 sec/batch; 43h:33m:59s remains)
INFO - root - 2017-12-11 08:29:18.817779: step 39430, loss = 0.69, batch loss = 0.63 (14.2 examples/sec; 0.562 sec/batch; 45h:45m:14s remains)
INFO - root - 2017-12-11 08:29:24.301483: step 39440, loss = 0.69, batch loss = 0.63 (14.5 examples/sec; 0.550 sec/batch; 44h:47m:44s remains)
INFO - root - 2017-12-11 08:29:29.711930: step 39450, loss = 0.70, batch loss = 0.64 (14.8 examples/sec; 0.539 sec/batch; 43h:52m:30s remains)
INFO - root - 2017-12-11 08:29:35.200103: step 39460, loss = 0.68, batch loss = 0.62 (14.5 examples/sec; 0.551 sec/batch; 44h:52m:42s remains)
INFO - root - 2017-12-11 08:29:40.685458: step 39470, loss = 0.70, batch loss = 0.64 (14.8 examples/sec; 0.541 sec/batch; 44h:01m:12s remains)
INFO - root - 2017-12-11 08:29:46.199025: step 39480, loss = 0.70, batch loss = 0.64 (14.6 examples/sec; 0.549 sec/batch; 44h:41m:58s remains)
INFO - root - 2017-12-11 08:29:51.807072: step 39490, loss = 0.70, batch loss = 0.64 (14.0 examples/sec; 0.572 sec/batch; 46h:31m:48s remains)
INFO - root - 2017-12-11 08:29:57.306352: step 39500, loss = 0.69, batch loss = 0.63 (14.7 examples/sec; 0.543 sec/batch; 44h:13m:57s remains)
2017-12-11 08:29:57.908893: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.48098752 0.43626443 0.34786502 0.23908687 0.13518028 0.054395311 0.0051779407 -0.014721246 -0.011821404 0.0091404961 0.034875888 0.052504819 0.064628147 0.072636306 0.074961506][0.60577339 0.55948561 0.46226749 0.341349 0.2247269 0.13096559 0.068154633 0.033153873 0.019819833 0.026199099 0.04178796 0.054623567 0.065743729 0.075152248 0.078941062][0.67564589 0.63251323 0.53849727 0.42156252 0.30978981 0.21879835 0.15312503 0.10783477 0.0770771 0.062598914 0.059059832 0.058667261 0.062790133 0.071112826 0.0766892][0.67520803 0.64132816 0.56330186 0.46697247 0.37857875 0.30837393 0.25407469 0.20741335 0.16261005 0.12496626 0.095095836 0.072624244 0.062140621 0.06491974 0.070732154][0.60195732 0.58174431 0.53063393 0.46972007 0.42149752 0.38886252 0.36030689 0.32258397 0.26916674 0.20856656 0.14765073 0.0958866 0.064028822 0.056983341 0.061437581][0.47154957 0.4685491 0.45215777 0.43609416 0.43769816 0.45158261 0.45642048 0.43325913 0.37531802 0.29450843 0.20305927 0.12087993 0.0660485 0.047462374 0.049518894][0.31562942 0.3334375 0.35462752 0.38420394 0.43419045 0.4927305 0.53011757 0.52210522 0.46258134 0.36665946 0.25033939 0.14193858 0.066635884 0.037725642 0.037928507][0.16623011 0.20472962 0.26080459 0.33003461 0.41863149 0.51102674 0.57312316 0.57815546 0.52095926 0.4187187 0.28903192 0.16466172 0.075804904 0.038906351 0.037107479][0.051915284 0.10854997 0.1914276 0.28824446 0.39962068 0.50940353 0.58395392 0.59739387 0.54570162 0.44640803 0.31643584 0.18898174 0.096011177 0.055302281 0.051647183][-0.010107628 0.058620784 0.15550107 0.26279315 0.37691641 0.48426992 0.55608976 0.57087016 0.52607119 0.43764552 0.32041097 0.20461157 0.12038735 0.083919644 0.0814925][-0.025477074 0.044882845 0.13959704 0.23920986 0.33783272 0.42637807 0.48380992 0.49530137 0.45972013 0.38959554 0.29651278 0.20539564 0.14188237 0.11750499 0.11920464][-0.019134721 0.039237391 0.11558343 0.19294938 0.2654798 0.32928547 0.37090158 0.38146892 0.3595902 0.3131288 0.25002342 0.18931136 0.15150413 0.14207226 0.14827971][-0.018792504 0.018099483 0.06709218 0.1159023 0.16026258 0.20078565 0.2301309 0.24333274 0.23742643 0.21570759 0.18307072 0.15279716 0.140363 0.14484945 0.15392564][-0.03367446 -0.017764181 0.0056152041 0.028640402 0.048892081 0.069838472 0.088808216 0.10320562 0.10894045 0.10637583 0.098362878 0.092664838 0.099760011 0.11354524 0.12263311][-0.06059343 -0.059710842 -0.053975876 -0.048618365 -0.044908874 -0.038568541 -0.029155603 -0.017760467 -0.0083501181 -0.001328518 0.0054201977 0.015266021 0.033880886 0.051854808 0.059248812]]...]
INFO - root - 2017-12-11 08:30:03.222831: step 39510, loss = 0.70, batch loss = 0.64 (14.6 examples/sec; 0.548 sec/batch; 44h:35m:35s remains)
INFO - root - 2017-12-11 08:30:08.753746: step 39520, loss = 0.68, batch loss = 0.62 (14.5 examples/sec; 0.552 sec/batch; 44h:55m:58s remains)
INFO - root - 2017-12-11 08:30:14.242960: step 39530, loss = 0.71, batch loss = 0.65 (14.3 examples/sec; 0.561 sec/batch; 45h:40m:42s remains)
INFO - root - 2017-12-11 08:30:19.779543: step 39540, loss = 0.70, batch loss = 0.64 (14.6 examples/sec; 0.547 sec/batch; 44h:30m:08s remains)
INFO - root - 2017-12-11 08:30:25.341274: step 39550, loss = 0.70, batch loss = 0.65 (14.8 examples/sec; 0.539 sec/batch; 43h:53m:53s remains)
INFO - root - 2017-12-11 08:30:30.898429: step 39560, loss = 0.70, batch loss = 0.65 (14.2 examples/sec; 0.565 sec/batch; 45h:58m:03s remains)
INFO - root - 2017-12-11 08:30:36.366128: step 39570, loss = 0.70, batch loss = 0.64 (15.1 examples/sec; 0.529 sec/batch; 43h:04m:36s remains)
INFO - root - 2017-12-11 08:30:41.807573: step 39580, loss = 0.69, batch loss = 0.64 (14.6 examples/sec; 0.550 sec/batch; 44h:43m:27s remains)
INFO - root - 2017-12-11 08:30:47.317728: step 39590, loss = 0.69, batch loss = 0.63 (14.1 examples/sec; 0.567 sec/batch; 46h:07m:09s remains)
INFO - root - 2017-12-11 08:30:52.847127: step 39600, loss = 0.71, batch loss = 0.65 (14.7 examples/sec; 0.543 sec/batch; 44h:11m:52s remains)
2017-12-11 08:30:53.371490: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.19194618 0.21637578 0.21788324 0.20109016 0.18046102 0.16838533 0.1710694 0.18803257 0.21267445 0.23888396 0.25364751 0.24672972 0.22097495 0.18824467 0.15783073][0.19596705 0.22186349 0.22635835 0.21359275 0.19874042 0.19380677 0.20393403 0.22680484 0.25439852 0.28069556 0.29177672 0.27707389 0.24085286 0.19896063 0.16439506][0.19126311 0.21858953 0.22613668 0.21731022 0.2075485 0.20809668 0.22300643 0.2484919 0.27559376 0.29929915 0.30687997 0.2881327 0.24727842 0.20293687 0.17109622][0.18730292 0.21573822 0.22477983 0.21783663 0.2106923 0.2131969 0.22806032 0.25071141 0.2723543 0.29015249 0.2943947 0.27607358 0.23911197 0.20259278 0.18250154][0.18421787 0.21304117 0.22225823 0.21556428 0.20899126 0.2107114 0.22167665 0.23698911 0.24900241 0.2579788 0.25822774 0.24321131 0.21637824 0.19478691 0.19206974][0.19035818 0.2220355 0.23235632 0.22507241 0.21659178 0.21471153 0.21885081 0.2245847 0.22667447 0.22761752 0.22376543 0.21101843 0.19321033 0.18497364 0.1962254][0.20527206 0.24436826 0.25852197 0.25105017 0.23917621 0.23205657 0.22799394 0.224097 0.2197406 0.21821536 0.21416233 0.20329377 0.19034278 0.18882188 0.20539787][0.22285885 0.27110437 0.29023537 0.28269807 0.2664189 0.25257063 0.23847996 0.22460036 0.21675624 0.21831758 0.21998446 0.21444197 0.2062455 0.207799 0.22218435][0.23311977 0.28766853 0.30975592 0.3016353 0.28217274 0.26401863 0.24392283 0.22508155 0.21873155 0.22780658 0.23831972 0.23898113 0.23413254 0.23490483 0.24034394][0.23892501 0.29729995 0.32078454 0.312131 0.29121223 0.27181244 0.2514748 0.23417653 0.2336742 0.25133038 0.26887807 0.27257684 0.26717609 0.26260176 0.25486705][0.23902577 0.2987864 0.32328171 0.31600392 0.29645613 0.27833232 0.26096511 0.24836288 0.25416887 0.27675438 0.29548004 0.29655612 0.28637677 0.27428311 0.25486505][0.22755373 0.28289372 0.30513489 0.2990638 0.28248626 0.26704904 0.25355139 0.24599215 0.25643271 0.28014687 0.2966055 0.29384014 0.27943879 0.26217195 0.23711292][0.21071751 0.25492519 0.26941338 0.2609612 0.24568152 0.23244041 0.22268713 0.2196832 0.23263747 0.254804 0.26769364 0.26237085 0.24660407 0.22815785 0.20293809][0.18847686 0.21849842 0.22204012 0.20809734 0.19191849 0.18029782 0.17526668 0.17830698 0.19361499 0.21297361 0.22112773 0.21304749 0.19634745 0.17778949 0.15490647][0.16024864 0.17904939 0.17445156 0.15602222 0.13733327 0.12418395 0.11972819 0.1244715 0.13790107 0.15165433 0.15470764 0.14540282 0.13046418 0.11502238 0.098208636]]...]
INFO - root - 2017-12-11 08:30:58.981626: step 39610, loss = 0.69, batch loss = 0.63 (14.0 examples/sec; 0.570 sec/batch; 46h:23m:47s remains)
INFO - root - 2017-12-11 08:31:04.497637: step 39620, loss = 0.70, batch loss = 0.64 (14.6 examples/sec; 0.547 sec/batch; 44h:28m:19s remains)
INFO - root - 2017-12-11 08:31:09.949216: step 39630, loss = 0.69, batch loss = 0.63 (14.8 examples/sec; 0.540 sec/batch; 43h:56m:17s remains)
INFO - root - 2017-12-11 08:31:15.484090: step 39640, loss = 0.70, batch loss = 0.64 (14.6 examples/sec; 0.548 sec/batch; 44h:35m:07s remains)
INFO - root - 2017-12-11 08:31:21.004949: step 39650, loss = 0.70, batch loss = 0.64 (13.8 examples/sec; 0.581 sec/batch; 47h:16m:21s remains)
INFO - root - 2017-12-11 08:31:26.493438: step 39660, loss = 0.71, batch loss = 0.66 (14.6 examples/sec; 0.548 sec/batch; 44h:34m:33s remains)
INFO - root - 2017-12-11 08:31:32.059014: step 39670, loss = 0.71, batch loss = 0.65 (13.8 examples/sec; 0.580 sec/batch; 47h:12m:05s remains)
INFO - root - 2017-12-11 08:31:37.627400: step 39680, loss = 0.70, batch loss = 0.64 (14.1 examples/sec; 0.566 sec/batch; 46h:04m:28s remains)
INFO - root - 2017-12-11 08:31:43.100671: step 39690, loss = 0.69, batch loss = 0.63 (15.3 examples/sec; 0.523 sec/batch; 42h:33m:39s remains)
INFO - root - 2017-12-11 08:31:48.175426: step 39700, loss = 0.68, batch loss = 0.62 (14.3 examples/sec; 0.560 sec/batch; 45h:33m:47s remains)
2017-12-11 08:31:48.770783: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.070866637 0.076176822 0.091770083 0.11014214 0.11966834 0.11246713 0.088212281 0.054402635 0.020531641 -0.0067058108 -0.024213089 -0.031387117 -0.027256129 -0.011930007 0.013181624][0.060383137 0.065696418 0.083129853 0.10662371 0.12512742 0.1291683 0.11502806 0.086723365 0.051570352 0.016971404 -0.010344734 -0.025275849 -0.023328003 -0.00403978 0.029279225][0.058507107 0.059766 0.072619058 0.0947423 0.11805201 0.13333485 0.13334306 0.1165624 0.085238285 0.045386314 0.0068252184 -0.019853076 -0.025385693 -0.0078462511 0.027544811][0.070160463 0.066107132 0.071200475 0.088645451 0.11490922 0.14233772 0.16017517 0.15964481 0.13571478 0.0910871 0.038088791 -0.0062280237 -0.026961241 -0.020190174 0.0076259808][0.086748369 0.079541169 0.079625189 0.09541481 0.12792769 0.1710971 0.21035521 0.22875918 0.21342625 0.1633589 0.09334743 0.027077945 -0.014876798 -0.02633273 -0.014502213][0.095248766 0.0900163 0.092533782 0.11485549 0.16096 0.22496429 0.28812006 0.32565987 0.3181957 0.2624447 0.17566952 0.087378539 0.022660611 -0.010420078 -0.019169824][0.087439425 0.088729426 0.10080141 0.13687995 0.20196827 0.28891438 0.3748683 0.42896461 0.42806292 0.36856392 0.269719 0.16480836 0.081643604 0.029581778 0.0017791596][0.066976525 0.075732395 0.0988356 0.14856388 0.22933574 0.33233783 0.43228504 0.49576876 0.49934086 0.44067368 0.33982703 0.2305496 0.14058977 0.079436 0.040951427][0.049150232 0.061981883 0.090505607 0.14486918 0.22810075 0.33073774 0.42839968 0.48983565 0.49461523 0.44248784 0.35234815 0.25445268 0.17305368 0.11637764 0.079244405][0.048716296 0.060625926 0.085733354 0.13193271 0.20087875 0.28437296 0.36261275 0.41065103 0.41324106 0.37194994 0.302521 0.22856244 0.16809425 0.12683329 0.10061938][0.071885809 0.079212725 0.093790211 0.12195206 0.16454059 0.21629649 0.26446885 0.29270136 0.29160836 0.26338747 0.21897162 0.17368881 0.13842759 0.11618307 0.10376534][0.11469106 0.11718053 0.11936088 0.12683438 0.1401996 0.15761545 0.17383167 0.18128756 0.17605177 0.16000776 0.13891922 0.11994766 0.10724548 0.10122612 0.099696152][0.1640005 0.16464159 0.15761212 0.14876805 0.13880131 0.12867256 0.1192255 0.10995457 0.10100422 0.09345106 0.08872892 0.087685779 0.089435354 0.092609838 0.095993608][0.20133904 0.20319729 0.19231842 0.17470783 0.15143983 0.12518685 0.10042718 0.080852754 0.068820164 0.064504795 0.06663081 0.072629869 0.0791657 0.084131442 0.087059863][0.21071079 0.21463889 0.2033654 0.18271776 0.15440232 0.12184269 0.09072528 0.06625504 0.051455911 0.045931734 0.047432765 0.052606363 0.057758104 0.060710445 0.0612749]]...]
INFO - root - 2017-12-11 08:31:54.374920: step 39710, loss = 0.68, batch loss = 0.62 (14.1 examples/sec; 0.569 sec/batch; 46h:15m:20s remains)
INFO - root - 2017-12-11 08:31:59.924623: step 39720, loss = 0.70, batch loss = 0.64 (14.4 examples/sec; 0.557 sec/batch; 45h:19m:45s remains)
INFO - root - 2017-12-11 08:32:05.445998: step 39730, loss = 0.69, batch loss = 0.64 (14.4 examples/sec; 0.555 sec/batch; 45h:05m:49s remains)
INFO - root - 2017-12-11 08:32:10.876797: step 39740, loss = 0.71, batch loss = 0.65 (14.8 examples/sec; 0.540 sec/batch; 43h:56m:15s remains)
INFO - root - 2017-12-11 08:32:16.440692: step 39750, loss = 0.70, batch loss = 0.65 (14.5 examples/sec; 0.550 sec/batch; 44h:44m:11s remains)
INFO - root - 2017-12-11 08:32:21.900504: step 39760, loss = 0.69, batch loss = 0.64 (15.2 examples/sec; 0.528 sec/batch; 42h:53m:59s remains)
INFO - root - 2017-12-11 08:32:27.372867: step 39770, loss = 0.70, batch loss = 0.65 (14.9 examples/sec; 0.538 sec/batch; 43h:46m:47s remains)
INFO - root - 2017-12-11 08:32:32.857704: step 39780, loss = 0.69, batch loss = 0.63 (14.6 examples/sec; 0.550 sec/batch; 44h:41m:13s remains)
INFO - root - 2017-12-11 08:32:38.343367: step 39790, loss = 0.69, batch loss = 0.63 (14.5 examples/sec; 0.552 sec/batch; 44h:52m:02s remains)
INFO - root - 2017-12-11 08:32:43.558507: step 39800, loss = 0.71, batch loss = 0.65 (14.5 examples/sec; 0.553 sec/batch; 44h:56m:33s remains)
2017-12-11 08:32:44.135851: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.40308362 0.38889471 0.356982 0.31543642 0.27185628 0.23965047 0.22088419 0.20729594 0.21440433 0.24479337 0.28588977 0.31640506 0.32805988 0.31410038 0.26758361][0.50156134 0.46582404 0.40936902 0.35295719 0.30519697 0.27895641 0.27537337 0.28271395 0.30898681 0.34950471 0.38995785 0.41061348 0.40522128 0.37113589 0.30695766][0.57279032 0.51501232 0.43539304 0.36962745 0.32677457 0.31573549 0.33537355 0.36958626 0.41614518 0.46401796 0.49855617 0.5035044 0.47399175 0.4136706 0.32935128][0.62224174 0.55297965 0.46083021 0.3939442 0.3628343 0.37226838 0.41706082 0.47548255 0.53314507 0.57684112 0.594968 0.57619727 0.51781964 0.43061998 0.32949454][0.64876246 0.58724183 0.50271136 0.44843325 0.43618947 0.46800029 0.53475386 0.60752177 0.66000247 0.68136048 0.6679526 0.61562616 0.52510214 0.41330317 0.30186146][0.65594047 0.62377113 0.570141 0.54223341 0.55132413 0.59988952 0.67732412 0.74767548 0.77711 0.76184767 0.70987535 0.62281889 0.50426203 0.37507057 0.26055795][0.63680077 0.64553154 0.63574857 0.6404106 0.66686463 0.72051257 0.79232109 0.8427332 0.83733916 0.78283364 0.69947153 0.59101313 0.46137309 0.33138794 0.22544155][0.59075797 0.63715321 0.66938663 0.70198548 0.73595309 0.78074759 0.83109814 0.84998488 0.8093195 0.72653812 0.63107169 0.52452034 0.40724343 0.2949236 0.20696773][0.52283561 0.59395558 0.65428507 0.70118409 0.72993219 0.75491482 0.77554756 0.76266736 0.69850647 0.60755122 0.52264 0.43943733 0.35170627 0.26773766 0.20081191][0.4247314 0.50317043 0.57309413 0.61999345 0.63622952 0.63903695 0.631729 0.59622818 0.52596116 0.44716784 0.38920814 0.34047782 0.28805238 0.23375405 0.1857556][0.29717353 0.36648342 0.42846194 0.46415675 0.46616951 0.45138514 0.42450294 0.37858149 0.3163572 0.26182148 0.23621935 0.22216471 0.20291325 0.17634635 0.14634205][0.15488806 0.2037587 0.24618718 0.26550642 0.25624812 0.23209661 0.19730619 0.15348676 0.10843731 0.081255637 0.084445804 0.098115355 0.10483961 0.10064364 0.086788774][0.02583804 0.051767565 0.073419563 0.078632839 0.064235479 0.0396039 0.007754901 -0.02533531 -0.050965611 -0.056154102 -0.035473555 -0.0079257414 0.011244078 0.018402822 0.014379892][-0.066461839 -0.057853486 -0.050711561 -0.053583972 -0.067636892 -0.086991519 -0.10961808 -0.12929825 -0.13970621 -0.13344333 -0.10915143 -0.081828937 -0.062504992 -0.053415027 -0.052983761][-0.11821507 -0.11981192 -0.11990383 -0.12523286 -0.13554788 -0.14740297 -0.15958619 -0.16800201 -0.16948295 -0.16058369 -0.14141279 -0.12168364 -0.10790724 -0.10107628 -0.099123426]]...]
INFO - root - 2017-12-11 08:32:49.639484: step 39810, loss = 0.68, batch loss = 0.62 (14.7 examples/sec; 0.544 sec/batch; 44h:12m:15s remains)
INFO - root - 2017-12-11 08:32:55.148137: step 39820, loss = 0.69, batch loss = 0.63 (14.7 examples/sec; 0.545 sec/batch; 44h:18m:53s remains)
INFO - root - 2017-12-11 08:33:00.681451: step 39830, loss = 0.70, batch loss = 0.64 (14.0 examples/sec; 0.570 sec/batch; 46h:21m:54s remains)
INFO - root - 2017-12-11 08:33:06.233449: step 39840, loss = 0.70, batch loss = 0.64 (14.4 examples/sec; 0.554 sec/batch; 45h:03m:31s remains)
INFO - root - 2017-12-11 08:33:11.705801: step 39850, loss = 0.69, batch loss = 0.63 (14.5 examples/sec; 0.553 sec/batch; 44h:57m:28s remains)
INFO - root - 2017-12-11 08:33:17.227847: step 39860, loss = 0.68, batch loss = 0.62 (14.3 examples/sec; 0.560 sec/batch; 45h:30m:47s remains)
INFO - root - 2017-12-11 08:33:22.705734: step 39870, loss = 0.70, batch loss = 0.64 (14.6 examples/sec; 0.550 sec/batch; 44h:41m:03s remains)
INFO - root - 2017-12-11 08:33:28.145887: step 39880, loss = 0.70, batch loss = 0.64 (14.7 examples/sec; 0.545 sec/batch; 44h:17m:21s remains)
INFO - root - 2017-12-11 08:33:33.654970: step 39890, loss = 0.70, batch loss = 0.64 (14.5 examples/sec; 0.550 sec/batch; 44h:44m:07s remains)
INFO - root - 2017-12-11 08:33:38.844554: step 39900, loss = 0.69, batch loss = 0.63 (15.1 examples/sec; 0.531 sec/batch; 43h:10m:25s remains)
2017-12-11 08:33:39.424568: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.086826414 -0.071695454 -0.039893623 -0.00071146776 0.034205422 0.0508152 0.041994654 0.0092293033 -0.040308084 -0.093551658 -0.13648906 -0.15931834 -0.16164142 -0.14954096 -0.13117319][-0.053462818 -0.028549926 0.024314849 0.091961123 0.15464485 0.19034538 0.18562752 0.14063676 0.065703131 -0.020535234 -0.097333938 -0.1477617 -0.16721752 -0.16227415 -0.14433035][-0.0018610269 0.035118897 0.11405447 0.2174526 0.315513 0.37695834 0.38135862 0.32544583 0.22305253 0.099173263 -0.018713573 -0.10616985 -0.15235439 -0.16271515 -0.15084125][0.055819698 0.10393427 0.20739713 0.34523416 0.47839224 0.56751859 0.58528447 0.52289641 0.39650306 0.23712727 0.078341588 -0.048389286 -0.12506884 -0.15512827 -0.15333571][0.10643429 0.16476154 0.2865212 0.44972146 0.61032182 0.72520912 0.76106346 0.70118546 0.56049258 0.37463239 0.18104146 0.017229494 -0.090719558 -0.14223181 -0.15241438][0.14493361 0.21501441 0.34851989 0.52485675 0.70200759 0.838945 0.89829826 0.85307026 0.71230984 0.51288223 0.29374555 0.095956169 -0.045456454 -0.12227689 -0.14797349][0.17304868 0.25522682 0.39069706 0.56242752 0.73888493 0.88664579 0.96773165 0.94496316 0.81863332 0.62246108 0.39318904 0.17278099 0.0031921084 -0.097574085 -0.13968883][0.18556626 0.27414304 0.39789322 0.54437369 0.69826347 0.83806276 0.929731 0.93086916 0.83276397 0.66083157 0.44440815 0.22193502 0.039456271 -0.075950474 -0.13018242][0.17761286 0.26242241 0.36199471 0.46807438 0.58224046 0.69657212 0.78515327 0.80757308 0.74802822 0.61891603 0.4370093 0.23358744 0.056149378 -0.061865222 -0.12186455][0.14817408 0.22208954 0.29511622 0.36062604 0.43158206 0.51142555 0.5853405 0.62124288 0.59989226 0.51956564 0.38242352 0.21135291 0.052802812 -0.057185397 -0.11647605][0.11351851 0.17328787 0.22478317 0.25911045 0.29284698 0.33633783 0.38595229 0.42142797 0.4245525 0.38288462 0.28664112 0.15244861 0.023091782 -0.067976184 -0.11818382][0.088859014 0.13202134 0.16571479 0.17806207 0.18361409 0.19454047 0.21477288 0.23519783 0.24238993 0.22031228 0.1539575 0.0569461 -0.035142768 -0.097035535 -0.12925707][0.079603568 0.10542729 0.1246913 0.1245345 0.1144096 0.10439485 0.099952735 0.098832227 0.094126634 0.073795572 0.024433175 -0.04186571 -0.098910421 -0.13166684 -0.14434738][0.077847689 0.090051137 0.100189 0.097021841 0.084361717 0.067934431 0.050386135 0.031667329 0.010522071 -0.017680462 -0.061222054 -0.10911984 -0.14351034 -0.15716532 -0.15597194][0.064870946 0.06955374 0.076783456 0.077331543 0.070490666 0.05761819 0.037508942 0.0097868405 -0.02314833 -0.059436932 -0.10226459 -0.1417724 -0.16557117 -0.17036802 -0.16157264]]...]
INFO - root - 2017-12-11 08:33:44.971403: step 39910, loss = 0.68, batch loss = 0.62 (14.2 examples/sec; 0.564 sec/batch; 45h:52m:10s remains)
INFO - root - 2017-12-11 08:33:50.463961: step 39920, loss = 0.71, batch loss = 0.66 (14.8 examples/sec; 0.542 sec/batch; 44h:03m:29s remains)
INFO - root - 2017-12-11 08:33:56.004155: step 39930, loss = 0.70, batch loss = 0.65 (14.6 examples/sec; 0.546 sec/batch; 44h:24m:30s remains)
INFO - root - 2017-12-11 08:34:01.525676: step 39940, loss = 0.71, batch loss = 0.65 (14.6 examples/sec; 0.546 sec/batch; 44h:24m:39s remains)
INFO - root - 2017-12-11 08:34:07.017923: step 39950, loss = 0.72, batch loss = 0.66 (15.3 examples/sec; 0.521 sec/batch; 42h:21m:22s remains)
INFO - root - 2017-12-11 08:34:12.511322: step 39960, loss = 0.69, batch loss = 0.63 (14.5 examples/sec; 0.551 sec/batch; 44h:48m:55s remains)
INFO - root - 2017-12-11 08:34:18.020289: step 39970, loss = 0.70, batch loss = 0.64 (14.6 examples/sec; 0.548 sec/batch; 44h:33m:25s remains)
INFO - root - 2017-12-11 08:34:23.608401: step 39980, loss = 0.71, batch loss = 0.65 (14.2 examples/sec; 0.565 sec/batch; 45h:54m:36s remains)
INFO - root - 2017-12-11 08:34:28.830728: step 39990, loss = 0.71, batch loss = 0.66 (30.5 examples/sec; 0.262 sec/batch; 21h:17m:59s remains)
INFO - root - 2017-12-11 08:34:34.396893: step 40000, loss = 0.70, batch loss = 0.64 (14.2 examples/sec; 0.564 sec/batch; 45h:50m:03s remains)
2017-12-11 08:34:35.010906: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.22503667 0.25766826 0.27564794 0.27312317 0.2517221 0.22325663 0.20125407 0.19306877 0.19959611 0.21934675 0.2463824 0.26925811 0.27509165 0.26230329 0.24366194][0.21572682 0.25691 0.28205198 0.28540438 0.26964882 0.24522254 0.22409022 0.21355854 0.21616179 0.23198259 0.25472721 0.27269033 0.27477992 0.25985351 0.2407344][0.1931801 0.24086851 0.27263597 0.28238991 0.27360919 0.25465333 0.23530209 0.22257429 0.22057609 0.23072502 0.24660535 0.25701386 0.25327185 0.23606059 0.21876882][0.17204954 0.22379234 0.26182503 0.27873921 0.27765894 0.26429096 0.24615639 0.23000775 0.22175007 0.22456814 0.2330844 0.23652716 0.22828834 0.21122505 0.19843431][0.15653589 0.20916156 0.25143582 0.27498522 0.28137568 0.2733731 0.25577775 0.23489647 0.21873161 0.21294282 0.21440849 0.21311036 0.20402542 0.19069798 0.18387389][0.14499588 0.19508705 0.23837097 0.26650822 0.28002679 0.27833271 0.26317623 0.23945136 0.21661347 0.20262696 0.19706503 0.19256651 0.1854437 0.17807373 0.17685606][0.13649698 0.18100291 0.22123171 0.25037834 0.269653 0.27568939 0.26728258 0.24665427 0.22218917 0.20287299 0.19040143 0.18235303 0.17683835 0.17415817 0.17584021][0.1267343 0.16260423 0.19599605 0.22286099 0.24635211 0.26160249 0.26487061 0.25420535 0.23436524 0.21329227 0.19408825 0.18018456 0.17232074 0.17005661 0.17154099][0.114494 0.14083792 0.16748407 0.19319458 0.22155024 0.24629495 0.26206726 0.2635563 0.25097877 0.22950913 0.2028141 0.17992598 0.16532804 0.15910569 0.15819411][0.10367291 0.12083089 0.1428352 0.17039919 0.20504548 0.23853026 0.26449025 0.27616572 0.27012223 0.24817644 0.21306187 0.17828009 0.15266733 0.13817443 0.13280669][0.099204682 0.10759731 0.12565766 0.15588169 0.19653922 0.2370666 0.27005756 0.28900686 0.28830367 0.2668061 0.22457962 0.17736781 0.13813083 0.11226197 0.10051659][0.10207365 0.10227407 0.11418409 0.14302099 0.18542327 0.22921726 0.26557973 0.28873628 0.29314527 0.27621821 0.23407844 0.18037809 0.13030714 0.093560718 0.07479737][0.10744086 0.10104363 0.10420244 0.12566078 0.16361207 0.20619608 0.24272731 0.26754725 0.27688166 0.26860362 0.23535764 0.18503711 0.13200419 0.089129411 0.065170132][0.11062609 0.10186581 0.096948244 0.10758293 0.13612007 0.17352766 0.20772092 0.2324831 0.24632767 0.24863319 0.23024046 0.19236457 0.14578012 0.10346561 0.077186286][0.10444719 0.098655917 0.090347923 0.091353089 0.10796437 0.13602662 0.16467647 0.18867759 0.2080763 0.2223089 0.22115424 0.20100957 0.16830219 0.13246991 0.10621773]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-SGD-lr0.001-clip50-initfromfixqian30k-fixqian/model.ckpt-40000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-SGD-lr0.001-clip50-initfromfixqian30k-fixqian/model.ckpt-40000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-11 08:34:41.514560: step 40010, loss = 0.70, batch loss = 0.65 (14.8 examples/sec; 0.541 sec/batch; 43h:57m:40s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-SGD-lr0.001-clip50-initfromfixqian30k-fixqian
INFO - root - 2017-12-11 08:34:47.055317: step 40020, loss = 0.70, batch loss = 0.64 (14.2 examples/sec; 0.562 sec/batch; 45h:39m:49s remains)
INFO - root - 2017-12-11 08:34:52.573759: step 40030, loss = 0.72, batch loss = 0.66 (15.0 examples/sec; 0.533 sec/batch; 43h:20m:26s remains)
INFO - root - 2017-12-11 08:34:58.032331: step 40040, loss = 0.70, batch loss = 0.64 (14.7 examples/sec; 0.545 sec/batch; 44h:15m:40s remains)
INFO - root - 2017-12-11 08:35:03.474331: step 40050, loss = 0.70, batch loss = 0.64 (14.6 examples/sec; 0.548 sec/batch; 44h:32m:59s remains)
INFO - root - 2017-12-11 08:35:09.051337: step 40060, loss = 0.72, batch loss = 0.66 (14.1 examples/sec; 0.569 sec/batch; 46h:10m:52s remains)
INFO - root - 2017-12-11 08:35:14.599448: step 40070, loss = 0.70, batch loss = 0.64 (14.3 examples/sec; 0.558 sec/batch; 45h:21m:27s remains)
INFO - root - 2017-12-11 08:35:20.111539: step 40080, loss = 0.71, batch loss = 0.65 (14.2 examples/sec; 0.562 sec/batch; 45h:36m:48s remains)
INFO - root - 2017-12-11 08:35:25.250070: step 40090, loss = 0.69, batch loss = 0.63 (14.5 examples/sec; 0.551 sec/batch; 44h:45m:12s remains)
INFO - root - 2017-12-11 08:35:30.723083: step 40100, loss = 0.69, batch loss = 0.63 (14.7 examples/sec; 0.544 sec/batch; 44h:09m:10s remains)
2017-12-11 08:35:31.331610: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.064538546 0.077861 0.092413805 0.10475893 0.11195011 0.12121481 0.14124705 0.17601909 0.2227429 0.27072096 0.30243298 0.300248 0.25896126 0.1947687 0.13786387][0.059963364 0.064981535 0.074156187 0.08370322 0.089650288 0.10107166 0.12812121 0.173159 0.23220935 0.292949 0.33608216 0.34237796 0.30379891 0.23671409 0.17271671][0.091555819 0.093613438 0.10081388 0.10780589 0.10739847 0.11044516 0.12995555 0.16861425 0.22286998 0.28245974 0.32956484 0.34419096 0.3142021 0.25306198 0.19043347][0.1685456 0.17843966 0.1907251 0.19731283 0.18711902 0.17294751 0.17225882 0.18915771 0.22166978 0.26497537 0.30632719 0.32569632 0.30578056 0.25447461 0.19773704][0.27274305 0.30069843 0.32566646 0.33717084 0.32044008 0.28936347 0.26521277 0.25335538 0.25381264 0.26979226 0.29722 0.31719628 0.30619466 0.26426584 0.21311836][0.36740479 0.41735598 0.46090376 0.48617062 0.47489548 0.43900195 0.39955825 0.36144343 0.32680023 0.30886018 0.31489789 0.32976821 0.32389429 0.28807232 0.23958401][0.40910459 0.47713035 0.54207575 0.59106046 0.60171109 0.57909554 0.53871244 0.48285806 0.41598442 0.36208922 0.34082547 0.3438943 0.33815807 0.30561686 0.25854397][0.37459353 0.45191976 0.53628421 0.61407894 0.65809923 0.66216916 0.63206488 0.5672 0.47492182 0.38822967 0.33741271 0.32348293 0.31393909 0.28441161 0.24151246][0.27788261 0.35385656 0.44945297 0.55116135 0.62770879 0.66056192 0.64486533 0.57819593 0.47068459 0.36022627 0.2834734 0.25088444 0.2353895 0.21022679 0.17580433][0.17025054 0.23533562 0.3285794 0.43860456 0.53283721 0.58396059 0.57923061 0.51562965 0.40492573 0.28437075 0.19162132 0.14406282 0.12308926 0.1035989 0.080060154][0.10343866 0.15197892 0.22858736 0.32646331 0.41540438 0.46595123 0.46267676 0.40339297 0.29994458 0.18408246 0.0893612 0.0359077 0.013118157 -0.0002909012 -0.014044625][0.10377112 0.13835996 0.19211531 0.26312271 0.32631722 0.35650191 0.34176815 0.28288153 0.19111356 0.090353414 0.0065822564 -0.041941356 -0.06118536 -0.068636633 -0.075748615][0.16150385 0.19006585 0.22279786 0.261864 0.288406 0.28539303 0.24871324 0.18400609 0.10245912 0.020606248 -0.044434886 -0.079645246 -0.090331316 -0.091714576 -0.095039681][0.23612735 0.26599592 0.28471163 0.29679278 0.28903955 0.25308764 0.19336055 0.12064682 0.046412852 -0.018394135 -0.0641871 -0.083906651 -0.0843969 -0.080241628 -0.081713766][0.29305598 0.32965896 0.34343061 0.33948138 0.30914408 0.25124604 0.17658032 0.099276945 0.030408651 -0.022981392 -0.055685543 -0.064559706 -0.057698712 -0.049978089 -0.050961502]]...]
INFO - root - 2017-12-11 08:35:36.863369: step 40110, loss = 0.70, batch loss = 0.64 (14.6 examples/sec; 0.549 sec/batch; 44h:34m:46s remains)
INFO - root - 2017-12-11 08:35:42.300144: step 40120, loss = 0.69, batch loss = 0.63 (14.9 examples/sec; 0.538 sec/batch; 43h:43m:38s remains)
INFO - root - 2017-12-11 08:35:47.824299: step 40130, loss = 0.69, batch loss = 0.63 (14.5 examples/sec; 0.553 sec/batch; 44h:53m:26s remains)
INFO - root - 2017-12-11 08:35:53.470139: step 40140, loss = 0.68, batch loss = 0.62 (14.0 examples/sec; 0.571 sec/batch; 46h:20m:18s remains)
INFO - root - 2017-12-11 08:35:58.935756: step 40150, loss = 0.70, batch loss = 0.64 (14.7 examples/sec; 0.544 sec/batch; 44h:09m:35s remains)
INFO - root - 2017-12-11 08:36:04.376034: step 40160, loss = 0.71, batch loss = 0.65 (14.6 examples/sec; 0.546 sec/batch; 44h:21m:06s remains)
INFO - root - 2017-12-11 08:36:09.782559: step 40170, loss = 0.69, batch loss = 0.64 (14.9 examples/sec; 0.538 sec/batch; 43h:42m:33s remains)
INFO - root - 2017-12-11 08:36:15.282092: step 40180, loss = 0.67, batch loss = 0.62 (14.6 examples/sec; 0.547 sec/batch; 44h:25m:55s remains)
INFO - root - 2017-12-11 08:36:20.511621: step 40190, loss = 0.71, batch loss = 0.65 (14.2 examples/sec; 0.564 sec/batch; 45h:47m:41s remains)
INFO - root - 2017-12-11 08:36:26.026465: step 40200, loss = 0.71, batch loss = 0.65 (14.4 examples/sec; 0.554 sec/batch; 44h:57m:32s remains)
2017-12-11 08:36:26.664228: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.027256066 -0.00442791 0.028140849 0.064033329 0.094340608 0.10976601 0.10415578 0.079635076 0.048485484 0.022225581 0.0064434456 0.0022861387 0.0083720433 0.023420813 0.0457344][-0.031504687 -0.010077607 0.021811079 0.057857554 0.0888499 0.10418896 0.096915022 0.070621662 0.03960361 0.016715871 0.0082696788 0.015377526 0.035399318 0.063044146 0.0912596][-0.034547672 -0.014094285 0.017820055 0.055569157 0.089790374 0.10812609 0.10241265 0.0764737 0.045276772 0.022993855 0.018055389 0.033111908 0.065025605 0.10428271 0.13843365][-0.03486101 -0.012804803 0.023393234 0.068687685 0.11296891 0.140927 0.14198014 0.118165 0.083427854 0.053467516 0.041356392 0.053805277 0.088593051 0.13312492 0.17017877][-0.031441186 -0.0044689486 0.040805858 0.0998583 0.16104154 0.20513572 0.21792473 0.19784285 0.1561905 0.11048871 0.079968065 0.077855773 0.10509205 0.14794436 0.18465634][-0.025813637 0.0080876239 0.06498792 0.14109315 0.22298631 0.28706369 0.31477302 0.299851 0.24997512 0.18391092 0.12765744 0.10276669 0.11506195 0.15145937 0.1861088][-0.019823015 0.020499391 0.087887742 0.17890283 0.27887264 0.36047402 0.4012678 0.39092457 0.33452836 0.25138479 0.17196739 0.12461423 0.1201309 0.14729457 0.17749453][-0.016220506 0.028151659 0.10180238 0.201239 0.31142455 0.4033252 0.45240945 0.44586867 0.38746926 0.29657075 0.20501646 0.14391237 0.12618312 0.14249553 0.16408895][-0.01696039 0.02819898 0.1031935 0.20430237 0.31677574 0.41194639 0.46543977 0.463684 0.4094063 0.32124802 0.23039591 0.1674415 0.14436716 0.15253066 0.16408008][-0.021815477 0.020716509 0.0922284 0.1885974 0.29582143 0.3875024 0.44179434 0.44640955 0.40347219 0.32953107 0.25259176 0.19971241 0.18036732 0.18571191 0.18895528][-0.028864549 0.0089065246 0.073705293 0.16105726 0.25783992 0.34099185 0.39290485 0.40456349 0.37805852 0.32696003 0.27339515 0.23830023 0.22789967 0.23388121 0.23126164][-0.035014696 -0.0026125147 0.055330437 0.13408516 0.22121766 0.29661053 0.34669521 0.36588463 0.35676235 0.33064878 0.30344135 0.28863168 0.28857532 0.29605624 0.28858578][-0.039401066 -0.011798607 0.040152173 0.11263747 0.19447832 0.26774159 0.320836 0.34942412 0.35608149 0.35157633 0.34823757 0.35306859 0.36398232 0.3737947 0.36222535][-0.041302737 -0.017570222 0.02942753 0.097270712 0.17691246 0.25262037 0.31292036 0.35212496 0.3715961 0.38255081 0.39690602 0.41843143 0.44155857 0.45710978 0.44428489][-0.040280581 -0.018501984 0.025576005 0.090076469 0.16752225 0.24399978 0.30793896 0.35212019 0.37719995 0.39560312 0.42049286 0.45483315 0.49053171 0.51528764 0.50576359]]...]
INFO - root - 2017-12-11 08:36:32.122812: step 40210, loss = 0.70, batch loss = 0.64 (14.9 examples/sec; 0.536 sec/batch; 43h:29m:47s remains)
INFO - root - 2017-12-11 08:36:37.527752: step 40220, loss = 0.69, batch loss = 0.63 (14.6 examples/sec; 0.546 sec/batch; 44h:21m:06s remains)
INFO - root - 2017-12-11 08:36:43.083704: step 40230, loss = 0.70, batch loss = 0.64 (14.7 examples/sec; 0.546 sec/batch; 44h:18m:17s remains)
INFO - root - 2017-12-11 08:36:48.590180: step 40240, loss = 0.70, batch loss = 0.64 (14.4 examples/sec; 0.557 sec/batch; 45h:10m:56s remains)
INFO - root - 2017-12-11 08:36:54.080432: step 40250, loss = 0.69, batch loss = 0.63 (14.6 examples/sec; 0.548 sec/batch; 44h:29m:00s remains)
INFO - root - 2017-12-11 08:36:59.672875: step 40260, loss = 0.70, batch loss = 0.64 (14.6 examples/sec; 0.549 sec/batch; 44h:34m:23s remains)
INFO - root - 2017-12-11 08:37:05.166642: step 40270, loss = 0.71, batch loss = 0.65 (14.9 examples/sec; 0.537 sec/batch; 43h:35m:34s remains)
INFO - root - 2017-12-11 08:37:10.326219: step 40280, loss = 0.68, batch loss = 0.62 (14.6 examples/sec; 0.550 sec/batch; 44h:36m:34s remains)
INFO - root - 2017-12-11 08:37:15.845742: step 40290, loss = 0.70, batch loss = 0.64 (14.8 examples/sec; 0.540 sec/batch; 43h:52m:02s remains)
INFO - root - 2017-12-11 08:37:21.276532: step 40300, loss = 0.71, batch loss = 0.65 (14.6 examples/sec; 0.548 sec/batch; 44h:26m:43s remains)
2017-12-11 08:37:21.900763: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.0037918929 0.02284308 0.039455924 0.048735149 0.049316693 0.042069662 0.030372255 0.019614208 0.013184441 0.01246604 0.016399419 0.025019331 0.037777171 0.047046591 0.041624505][0.027481761 0.058112405 0.08782953 0.10991924 0.12209114 0.12436649 0.11984187 0.11286823 0.10385517 0.095164157 0.090167277 0.095218658 0.11222328 0.12863141 0.12418147][0.05294605 0.096152544 0.14109501 0.18000913 0.21110167 0.23424363 0.24973425 0.25606129 0.24623412 0.22264464 0.19631119 0.18460427 0.19612199 0.21566959 0.21360736][0.072994545 0.12788424 0.18841001 0.24618419 0.3003943 0.35105386 0.3937315 0.41830635 0.40927395 0.36817342 0.31354213 0.27465862 0.26981512 0.28532308 0.28489324][0.080790944 0.14534563 0.22087996 0.29923648 0.37975752 0.4616662 0.53382081 0.57694763 0.5683955 0.50838625 0.42175123 0.34840125 0.31832111 0.32267141 0.32191014][0.084046267 0.15654323 0.24758995 0.34988409 0.46121848 0.57694083 0.67681462 0.73312235 0.7180934 0.6330561 0.51044261 0.40113217 0.3452557 0.33734024 0.33614388][0.08525034 0.16438405 0.27088419 0.39837083 0.54067361 0.68656617 0.80667996 0.86686224 0.83740729 0.72433287 0.56958026 0.43286484 0.35883978 0.34124923 0.33777437][0.082257636 0.16726635 0.28775191 0.43642804 0.60130984 0.76441914 0.89084113 0.94443768 0.89778388 0.7637856 0.59206831 0.44499466 0.3642295 0.339266 0.32864577][0.07971184 0.1696308 0.2995837 0.4601644 0.633669 0.79748124 0.91563153 0.955014 0.89380133 0.75166249 0.58184254 0.4420923 0.36453474 0.33290005 0.30992743][0.076122589 0.16581376 0.29461098 0.45217428 0.61824149 0.7682935 0.8684054 0.8919965 0.82416695 0.68874818 0.53658056 0.41561368 0.34564236 0.30659431 0.26975685][0.064270541 0.14456719 0.25817558 0.39594677 0.53931582 0.66481632 0.74323863 0.75591165 0.69349885 0.57941383 0.45592245 0.35844538 0.29569009 0.24874541 0.20075755][0.038537096 0.10090177 0.1882145 0.29350615 0.40267125 0.49615762 0.55215156 0.55998182 0.51396132 0.4323692 0.34419918 0.27102646 0.21454246 0.16169935 0.10786945][0.0048803026 0.044221986 0.099711627 0.16664754 0.236456 0.29590675 0.33161706 0.33929276 0.31469429 0.26805627 0.21422791 0.16312653 0.11406855 0.061362505 0.0093028033][-0.022937963 -0.005218449 0.021163471 0.053458713 0.088143848 0.11864483 0.13877088 0.14790437 0.14172946 0.1226995 0.095004946 0.061363969 0.02164549 -0.023905333 -0.066788986][-0.045043822 -0.043876994 -0.038321711 -0.030270781 -0.02003397 -0.0094833774 -7.4615484e-05 0.0090768691 0.013636039 0.011355366 0.00068437582 -0.01898656 -0.04702206 -0.08019162 -0.10968222]]...]
INFO - root - 2017-12-11 08:37:27.449437: step 40310, loss = 0.72, batch loss = 0.66 (13.9 examples/sec; 0.574 sec/batch; 46h:36m:46s remains)
INFO - root - 2017-12-11 08:37:32.868369: step 40320, loss = 0.70, batch loss = 0.65 (14.6 examples/sec; 0.549 sec/batch; 44h:32m:01s remains)
INFO - root - 2017-12-11 08:37:38.299174: step 40330, loss = 0.69, batch loss = 0.63 (14.9 examples/sec; 0.538 sec/batch; 43h:38m:50s remains)
INFO - root - 2017-12-11 08:37:43.868213: step 40340, loss = 0.70, batch loss = 0.64 (14.3 examples/sec; 0.558 sec/batch; 45h:18m:27s remains)
INFO - root - 2017-12-11 08:37:49.400568: step 40350, loss = 0.69, batch loss = 0.63 (14.6 examples/sec; 0.547 sec/batch; 44h:24m:05s remains)
INFO - root - 2017-12-11 08:37:54.892524: step 40360, loss = 0.68, batch loss = 0.62 (15.0 examples/sec; 0.535 sec/batch; 43h:22m:32s remains)
INFO - root - 2017-12-11 08:38:00.412599: step 40370, loss = 0.69, batch loss = 0.63 (14.4 examples/sec; 0.557 sec/batch; 45h:10m:39s remains)
INFO - root - 2017-12-11 08:38:05.557595: step 40380, loss = 0.70, batch loss = 0.64 (14.4 examples/sec; 0.557 sec/batch; 45h:10m:45s remains)
INFO - root - 2017-12-11 08:38:11.122039: step 40390, loss = 0.70, batch loss = 0.65 (14.5 examples/sec; 0.552 sec/batch; 44h:48m:16s remains)
INFO - root - 2017-12-11 08:38:16.730012: step 40400, loss = 0.68, batch loss = 0.62 (14.5 examples/sec; 0.553 sec/batch; 44h:54m:37s remains)
2017-12-11 08:38:17.332064: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.13210471 0.14453453 0.14400357 0.133065 0.12189101 0.11527479 0.10888872 0.096521229 0.082693011 0.076317586 0.088693656 0.11583053 0.14724579 0.17349494 0.19000272][0.15387067 0.17384359 0.17935571 0.17209907 0.16264115 0.15591922 0.14783093 0.13222349 0.11426193 0.10350291 0.11080831 0.13291372 0.16038114 0.18554358 0.20282365][0.16411506 0.19426128 0.21045262 0.21139625 0.20641187 0.20029636 0.19022164 0.17047229 0.1466648 0.12939389 0.129174 0.14328331 0.162543 0.18143144 0.19487587][0.16911373 0.2112139 0.24154823 0.25414506 0.25553897 0.25114843 0.24011008 0.21611206 0.18487397 0.15970625 0.15221882 0.15896228 0.1687663 0.17765732 0.1825882][0.1708176 0.22469014 0.2703048 0.29595113 0.30559555 0.30599049 0.29672608 0.26937172 0.23028636 0.19717652 0.18360434 0.18454593 0.18575484 0.18375744 0.17774695][0.17651258 0.23989031 0.29747859 0.33378935 0.35277602 0.36170986 0.3574591 0.32826802 0.28290516 0.24348359 0.22396974 0.21809421 0.21106058 0.19963343 0.18342137][0.19644514 0.26454496 0.32669216 0.36708719 0.39239913 0.40980768 0.41190058 0.38339889 0.3358852 0.29369727 0.26968074 0.25709936 0.24281166 0.22451758 0.20132875][0.22688295 0.29453933 0.35428521 0.39220849 0.41849285 0.44093502 0.44890383 0.42464888 0.38109562 0.341447 0.3169108 0.30096784 0.28342649 0.26238838 0.23570634][0.25905889 0.32296291 0.37658155 0.40800869 0.42931148 0.45002663 0.46016258 0.44252819 0.40863514 0.376986 0.35700339 0.34247693 0.32536 0.30359656 0.27443391][0.28609598 0.34495449 0.39147151 0.41427207 0.42523628 0.43723673 0.44457033 0.4333584 0.41116598 0.39051974 0.3796697 0.37198281 0.3590017 0.33768463 0.30643994][0.29791164 0.35135734 0.39125037 0.40564719 0.40468496 0.40525723 0.40751266 0.40072337 0.38712233 0.37534708 0.37490347 0.37734389 0.37096211 0.35166889 0.32059938][0.28330025 0.33066744 0.36505702 0.37337965 0.36359671 0.35554096 0.35379294 0.34877449 0.33741641 0.32783565 0.333877 0.34607607 0.34827426 0.33472732 0.30891222][0.2321714 0.27206969 0.30149683 0.30699959 0.29415524 0.28318331 0.28029749 0.27579135 0.26281664 0.25063482 0.25759357 0.27564839 0.28653687 0.28205195 0.26547855][0.15385701 0.18444873 0.20771275 0.21125062 0.1989481 0.18882322 0.18663138 0.18252744 0.16827156 0.15401758 0.15947761 0.17892586 0.19478132 0.1976209 0.18952526][0.070714556 0.089916736 0.10516484 0.10643984 0.096604757 0.08951807 0.089003682 0.0858369 0.0724471 0.058663104 0.062001444 0.078818515 0.09433715 0.0997323 0.096518621]]...]
INFO - root - 2017-12-11 08:38:22.849275: step 40410, loss = 0.71, batch loss = 0.65 (14.7 examples/sec; 0.544 sec/batch; 44h:10m:00s remains)
INFO - root - 2017-12-11 08:38:28.298497: step 40420, loss = 0.68, batch loss = 0.62 (14.6 examples/sec; 0.550 sec/batch; 44h:34m:59s remains)
INFO - root - 2017-12-11 08:38:33.836149: step 40430, loss = 0.70, batch loss = 0.65 (14.6 examples/sec; 0.547 sec/batch; 44h:23m:52s remains)
INFO - root - 2017-12-11 08:38:39.344135: step 40440, loss = 0.71, batch loss = 0.65 (14.7 examples/sec; 0.546 sec/batch; 44h:16m:40s remains)
INFO - root - 2017-12-11 08:38:44.784744: step 40450, loss = 0.69, batch loss = 0.63 (15.0 examples/sec; 0.535 sec/batch; 43h:24m:28s remains)
INFO - root - 2017-12-11 08:38:50.221156: step 40460, loss = 0.70, batch loss = 0.65 (16.4 examples/sec; 0.487 sec/batch; 39h:31m:22s remains)
INFO - root - 2017-12-11 08:38:55.613076: step 40470, loss = 0.70, batch loss = 0.64 (14.4 examples/sec; 0.555 sec/batch; 45h:02m:36s remains)
INFO - root - 2017-12-11 08:39:00.764083: step 40480, loss = 0.69, batch loss = 0.64 (15.3 examples/sec; 0.523 sec/batch; 42h:25m:31s remains)
INFO - root - 2017-12-11 08:39:06.267210: step 40490, loss = 0.70, batch loss = 0.64 (14.8 examples/sec; 0.542 sec/batch; 43h:58m:08s remains)
INFO - root - 2017-12-11 08:39:11.719197: step 40500, loss = 0.72, batch loss = 0.66 (15.2 examples/sec; 0.526 sec/batch; 42h:40m:42s remains)
2017-12-11 08:39:12.320237: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.31623274 0.34360152 0.34531838 0.30677205 0.23422617 0.15760075 0.10325154 0.084792838 0.097912528 0.1279497 0.15502863 0.16478956 0.15515262 0.13692829 0.12247106][0.3393428 0.36372924 0.366959 0.3354229 0.27122942 0.20254239 0.15412958 0.13697994 0.14519872 0.1667096 0.18642628 0.19417749 0.18912883 0.17770539 0.16509663][0.32841319 0.34308952 0.34720147 0.32907051 0.28490627 0.23596263 0.2022187 0.19063044 0.19485219 0.20667908 0.21722119 0.22176771 0.22183958 0.21911594 0.21168055][0.29505584 0.29648048 0.30085224 0.29831028 0.27838913 0.2533122 0.23720613 0.23373771 0.23734495 0.24329776 0.24767198 0.24966024 0.25282156 0.256158 0.25298047][0.24340415 0.23136766 0.23561449 0.24805421 0.25297341 0.25317702 0.25578541 0.26174819 0.26714513 0.27059138 0.27214295 0.27384543 0.28039935 0.28958204 0.2918337][0.18228531 0.15949741 0.16262865 0.18679956 0.21380745 0.23826396 0.2600497 0.2767038 0.28513125 0.28709349 0.28649557 0.28831995 0.29837239 0.31450918 0.32542145][0.11928209 0.088458151 0.08981961 0.12155456 0.16612722 0.21263275 0.25424102 0.28334102 0.29596779 0.2962912 0.29218134 0.29196632 0.30301383 0.32419536 0.34344709][0.053067219 0.020871187 0.024592465 0.0636564 0.12285352 0.1888507 0.24931815 0.29052332 0.30632505 0.30269328 0.29145855 0.2845144 0.29132608 0.31230161 0.33518839][-0.011412567 -0.036522821 -0.024319742 0.023960179 0.095427826 0.17609785 0.25015873 0.2987833 0.31380805 0.30302468 0.28187159 0.26585114 0.26598346 0.28333041 0.30507696][-0.064449348 -0.077451 -0.0542098 0.0026516877 0.08234293 0.17107145 0.2512759 0.30141315 0.31259194 0.29472926 0.26621374 0.24505791 0.24165227 0.25509563 0.27122954][-0.09746512 -0.097299077 -0.065063357 -0.0047301333 0.074975327 0.16149208 0.23799656 0.28388268 0.29180819 0.2739521 0.2495507 0.23510313 0.23630823 0.24724802 0.25214276][-0.10041107 -0.089098878 -0.052871536 0.0028761597 0.071527041 0.14332749 0.205304 0.24199341 0.25000566 0.24294642 0.23749644 0.24363759 0.25835383 0.2687262 0.25819933][-0.078052357 -0.059461158 -0.024294365 0.020972954 0.071561664 0.12127576 0.16264777 0.18784034 0.19840036 0.20827962 0.22970763 0.26389644 0.29758447 0.31049708 0.28573391][-0.042583719 -0.022002419 0.0074349749 0.03915919 0.070287034 0.097893208 0.11948977 0.13473633 0.14939652 0.17672615 0.22456583 0.28623 0.33968961 0.3583141 0.32460421][-0.0090451362 0.0088797761 0.029927088 0.048420411 0.063411094 0.07445728 0.082087956 0.0913402 0.11039825 0.15058756 0.21622285 0.29533672 0.36120322 0.38383165 0.34564906]]...]
INFO - root - 2017-12-11 08:39:17.813309: step 40510, loss = 0.71, batch loss = 0.65 (14.5 examples/sec; 0.553 sec/batch; 44h:49m:54s remains)
INFO - root - 2017-12-11 08:39:23.316293: step 40520, loss = 0.70, batch loss = 0.64 (13.8 examples/sec; 0.578 sec/batch; 46h:54m:24s remains)
INFO - root - 2017-12-11 08:39:28.858442: step 40530, loss = 0.69, batch loss = 0.63 (14.6 examples/sec; 0.548 sec/batch; 44h:25m:06s remains)
INFO - root - 2017-12-11 08:39:34.345584: step 40540, loss = 0.69, batch loss = 0.63 (14.7 examples/sec; 0.546 sec/batch; 44h:15m:59s remains)
INFO - root - 2017-12-11 08:39:39.773539: step 40550, loss = 0.69, batch loss = 0.63 (14.8 examples/sec; 0.541 sec/batch; 43h:50m:27s remains)
INFO - root - 2017-12-11 08:39:45.336262: step 40560, loss = 0.69, batch loss = 0.63 (14.4 examples/sec; 0.557 sec/batch; 45h:10m:58s remains)
INFO - root - 2017-12-11 08:39:50.857764: step 40570, loss = 0.70, batch loss = 0.64 (14.8 examples/sec; 0.541 sec/batch; 43h:51m:44s remains)
INFO - root - 2017-12-11 08:39:55.923979: step 40580, loss = 0.69, batch loss = 0.63 (14.5 examples/sec; 0.552 sec/batch; 44h:46m:46s remains)
INFO - root - 2017-12-11 08:40:01.450342: step 40590, loss = 0.68, batch loss = 0.63 (14.2 examples/sec; 0.565 sec/batch; 45h:48m:09s remains)
INFO - root - 2017-12-11 08:40:07.021348: step 40600, loss = 0.69, batch loss = 0.63 (14.5 examples/sec; 0.550 sec/batch; 44h:34m:58s remains)
2017-12-11 08:40:07.611197: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.051911745 -0.053909086 -0.050432071 -0.044881348 -0.040424295 -0.040850274 -0.045412876 -0.048968628 -0.047015741 -0.037909653 -0.022248117 -0.0028156082 0.013908127 0.021978965 0.022135407][-0.047299925 -0.045666866 -0.038919434 -0.031156452 -0.025338601 -0.025381718 -0.030326283 -0.034588005 -0.033382207 -0.024771307 -0.0090362877 0.011607806 0.030480955 0.039395582 0.0367877][-0.035907682 -0.02897005 -0.017515251 -0.0058380254 0.0040461761 0.0084483605 0.00786173 0.0061631692 0.0059717055 0.0081018843 0.013645167 0.024383564 0.037122238 0.043399259 0.038321387][-0.019730803 -0.0070016989 0.010481839 0.028985951 0.047988955 0.063880809 0.075481817 0.082320951 0.081756644 0.071333773 0.054206789 0.040693693 0.035706546 0.033556707 0.026248308][0.00025583268 0.018745808 0.0434173 0.071894594 0.10529675 0.14000928 0.17193235 0.19395475 0.19545381 0.16843736 0.11891082 0.069152109 0.036105845 0.019765694 0.009850827][0.018812593 0.042715121 0.075304784 0.11611108 0.16774711 0.22633576 0.28411585 0.32634547 0.33312264 0.2895124 0.20440258 0.11347468 0.047456034 0.013329663 -0.00044424058][0.030341027 0.058963709 0.098644033 0.15111309 0.22015889 0.30204436 0.38575265 0.44933531 0.46408471 0.40787178 0.29209146 0.16442652 0.068055868 0.016703798 -0.0015244981][0.035721544 0.066152856 0.10835721 0.16647495 0.24538746 0.34239292 0.44475693 0.5256846 0.54988956 0.48908383 0.35547408 0.20399669 0.086559504 0.022331011 8.804322e-05][0.042563982 0.068813331 0.105695 0.15956463 0.23611698 0.33462459 0.4430781 0.53339672 0.56697404 0.51141053 0.37740734 0.22026891 0.094942264 0.023966387 -0.0018351366][0.058461122 0.07324788 0.0963823 0.1366428 0.19988656 0.28711671 0.3888337 0.47919571 0.51972884 0.47663474 0.35744405 0.21186091 0.091797158 0.020500867 -0.0080581708][0.082883835 0.080111854 0.083762787 0.10513926 0.14899124 0.21679997 0.30255926 0.38541836 0.42984194 0.40299803 0.3085967 0.18715988 0.0828398 0.017141404 -0.012412842][0.10426361 0.0832367 0.06832619 0.072379895 0.0971857 0.14296763 0.20728147 0.27606618 0.31931937 0.30764318 0.24178889 0.15210572 0.072198644 0.019287916 -0.0066272165][0.10846321 0.075567536 0.050679758 0.045536857 0.056898791 0.082224481 0.12243802 0.17123097 0.20648421 0.20492259 0.16599004 0.11046935 0.060368024 0.026864734 0.010600006][0.095068805 0.061576426 0.038274925 0.032895952 0.036353845 0.043322779 0.058209576 0.082711235 0.10400729 0.10627259 0.0889059 0.064265355 0.043521419 0.031634372 0.028462864][0.080507711 0.056165867 0.04310929 0.043228414 0.042133816 0.032401033 0.021452447 0.018364767 0.020328183 0.019588884 0.016041413 0.014511628 0.016668795 0.022329437 0.030364228]]...]
INFO - root - 2017-12-11 08:40:13.127270: step 40610, loss = 0.71, batch loss = 0.65 (14.1 examples/sec; 0.566 sec/batch; 45h:51m:10s remains)
INFO - root - 2017-12-11 08:40:18.598104: step 40620, loss = 0.70, batch loss = 0.64 (14.1 examples/sec; 0.568 sec/batch; 46h:03m:32s remains)
INFO - root - 2017-12-11 08:40:24.093742: step 40630, loss = 0.72, batch loss = 0.66 (14.2 examples/sec; 0.563 sec/batch; 45h:36m:34s remains)
INFO - root - 2017-12-11 08:40:29.637189: step 40640, loss = 0.70, batch loss = 0.65 (14.9 examples/sec; 0.537 sec/batch; 43h:33m:32s remains)
INFO - root - 2017-12-11 08:40:35.085287: step 40650, loss = 0.69, batch loss = 0.64 (14.7 examples/sec; 0.545 sec/batch; 44h:13m:01s remains)
INFO - root - 2017-12-11 08:40:40.545364: step 40660, loss = 0.69, batch loss = 0.63 (14.4 examples/sec; 0.556 sec/batch; 45h:05m:14s remains)
INFO - root - 2017-12-11 08:40:45.684751: step 40670, loss = 0.69, batch loss = 0.63 (14.6 examples/sec; 0.547 sec/batch; 44h:21m:20s remains)
INFO - root - 2017-12-11 08:40:51.194331: step 40680, loss = 0.69, batch loss = 0.63 (14.6 examples/sec; 0.547 sec/batch; 44h:20m:22s remains)
INFO - root - 2017-12-11 08:40:56.744576: step 40690, loss = 0.70, batch loss = 0.64 (14.0 examples/sec; 0.570 sec/batch; 46h:14m:26s remains)
INFO - root - 2017-12-11 08:41:02.267350: step 40700, loss = 0.69, batch loss = 0.63 (14.3 examples/sec; 0.561 sec/batch; 45h:26m:44s remains)
2017-12-11 08:41:02.906760: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.340897 0.31933579 0.28398672 0.24918272 0.23611826 0.2412784 0.24975064 0.24272268 0.21378194 0.16670503 0.10423891 0.04076824 -0.0089747012 -0.038615931 -0.054543391][0.32395053 0.3017754 0.27341279 0.24965687 0.24735649 0.26004055 0.27104652 0.26051813 0.22347434 0.16552731 0.0918707 0.01977057 -0.034381274 -0.063780412 -0.075042151][0.2725099 0.25632647 0.243387 0.23996711 0.25590733 0.28177664 0.30029243 0.29133585 0.25078133 0.1845562 0.10058256 0.018439397 -0.04323265 -0.076527558 -0.08752808][0.21045652 0.20557706 0.21355629 0.23573753 0.27504715 0.3189097 0.34858969 0.3441667 0.30255106 0.23009138 0.13601506 0.04151262 -0.031612244 -0.0737598 -0.089809693][0.13995688 0.15028568 0.18157306 0.23005934 0.29295155 0.35540071 0.39681807 0.39707944 0.35437405 0.27666071 0.17478165 0.070131563 -0.013544182 -0.064709105 -0.086976074][0.0791643 0.10484398 0.15560172 0.22484961 0.30623317 0.3834354 0.43418831 0.43700486 0.39048213 0.30572867 0.19757441 0.087272272 -0.001855568 -0.05788869 -0.083591051][0.059977688 0.098766409 0.16155946 0.24192493 0.33163422 0.41281947 0.46313727 0.46103156 0.40529042 0.31050068 0.19704966 0.086427659 -0.0013818895 -0.056679986 -0.0820345][0.077587493 0.12425213 0.19040115 0.27106404 0.35767171 0.43171537 0.47163504 0.4576785 0.38947523 0.28486425 0.16894673 0.063656092 -0.015748365 -0.06403257 -0.084631443][0.10719025 0.15303537 0.21312331 0.28523746 0.36129823 0.4229309 0.45023373 0.4260062 0.34950057 0.23970459 0.12480755 0.028071733 -0.039513629 -0.077437937 -0.090764806][0.14521869 0.1832778 0.22994825 0.28724495 0.34847897 0.39658865 0.41365203 0.38496554 0.30795869 0.19946928 0.087968953 -0.0015431748 -0.059656188 -0.089006193 -0.096403867][0.1916301 0.21896189 0.24861608 0.287497 0.33173484 0.36667427 0.37628472 0.34771833 0.27624539 0.17439222 0.068154134 -0.016238969 -0.068739578 -0.093590721 -0.098396942][0.24368474 0.26048741 0.27321678 0.2929787 0.31945887 0.34124392 0.34409681 0.31634688 0.25196373 0.15921585 0.059973452 -0.019947762 -0.069403581 -0.09248019 -0.09692245][0.2802296 0.289022 0.28843579 0.2927148 0.30525616 0.31743538 0.3158724 0.28932089 0.23167133 0.14842217 0.057057895 -0.018406015 -0.065919973 -0.088359624 -0.093265906][0.27824098 0.28150326 0.27285117 0.26772621 0.27266115 0.28059652 0.27822635 0.2544896 0.20421986 0.13142097 0.049696237 -0.01948517 -0.063870192 -0.0849219 -0.089747548][0.23292992 0.23330578 0.2217903 0.21270613 0.21432649 0.22030061 0.21766271 0.19660997 0.15408137 0.09336435 0.024824513 -0.033510838 -0.070432819 -0.086799569 -0.089257047]]...]
INFO - root - 2017-12-11 08:41:08.373572: step 40710, loss = 0.71, batch loss = 0.65 (14.7 examples/sec; 0.546 sec/batch; 44h:14m:52s remains)
INFO - root - 2017-12-11 08:41:13.806270: step 40720, loss = 0.70, batch loss = 0.64 (14.9 examples/sec; 0.538 sec/batch; 43h:37m:59s remains)
INFO - root - 2017-12-11 08:41:19.271320: step 40730, loss = 0.69, batch loss = 0.64 (14.3 examples/sec; 0.559 sec/batch; 45h:19m:02s remains)
INFO - root - 2017-12-11 08:41:24.781129: step 40740, loss = 0.70, batch loss = 0.65 (14.1 examples/sec; 0.566 sec/batch; 45h:51m:55s remains)
INFO - root - 2017-12-11 08:41:30.307746: step 40750, loss = 0.70, batch loss = 0.64 (14.9 examples/sec; 0.536 sec/batch; 43h:23m:54s remains)
INFO - root - 2017-12-11 08:41:35.817162: step 40760, loss = 0.69, batch loss = 0.63 (14.4 examples/sec; 0.556 sec/batch; 45h:02m:37s remains)
INFO - root - 2017-12-11 08:41:41.039403: step 40770, loss = 0.70, batch loss = 0.65 (14.8 examples/sec; 0.539 sec/batch; 43h:41m:48s remains)
INFO - root - 2017-12-11 08:41:46.571381: step 40780, loss = 0.69, batch loss = 0.64 (14.6 examples/sec; 0.548 sec/batch; 44h:23m:54s remains)
INFO - root - 2017-12-11 08:41:52.120257: step 40790, loss = 0.69, batch loss = 0.63 (14.1 examples/sec; 0.566 sec/batch; 45h:51m:05s remains)
INFO - root - 2017-12-11 08:41:57.711070: step 40800, loss = 0.69, batch loss = 0.63 (14.4 examples/sec; 0.555 sec/batch; 44h:58m:14s remains)
2017-12-11 08:41:58.299182: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.20603232 0.2576423 0.30916765 0.33865762 0.33312336 0.3034645 0.27146345 0.24220838 0.21844941 0.21878535 0.2504499 0.29502895 0.32483998 0.33314797 0.32962495][0.27226689 0.32048747 0.36288434 0.38149098 0.368479 0.33535787 0.30318564 0.27621648 0.26069885 0.27419764 0.32022402 0.37580258 0.40800992 0.40941647 0.39101955][0.31538454 0.35587573 0.38274342 0.38682565 0.36774188 0.3364161 0.30755445 0.28498054 0.27861294 0.30370313 0.36037889 0.42299569 0.45673072 0.45394266 0.42444846][0.33558625 0.36842582 0.38317791 0.37733814 0.35690835 0.33166903 0.30894735 0.29096669 0.28900594 0.31733623 0.3753655 0.43778288 0.46975943 0.46447793 0.43212906][0.3221972 0.34956846 0.36072874 0.35530648 0.34179109 0.32864746 0.31731635 0.30706063 0.30579245 0.328679 0.37796149 0.43193892 0.45656869 0.44626179 0.41379312][0.28546968 0.31208876 0.33089224 0.33877218 0.340191 0.34246781 0.34510207 0.34467557 0.340976 0.35007486 0.38166171 0.41988221 0.43129194 0.41072422 0.37547785][0.24241582 0.27373737 0.30850759 0.338409 0.35845277 0.3745138 0.38887513 0.39707047 0.38790166 0.37679204 0.38274965 0.39790261 0.38978034 0.35476682 0.31572476][0.20310172 0.24230117 0.29463258 0.34643582 0.38179287 0.4056941 0.42605513 0.43915442 0.42459768 0.39187184 0.36861366 0.35663068 0.32840616 0.28216597 0.24257937][0.16802613 0.2122017 0.27578864 0.34167641 0.38529244 0.4110457 0.43301967 0.44976386 0.43521568 0.39004177 0.34523153 0.31077084 0.26794684 0.21877021 0.18233921][0.13692878 0.17816862 0.24020821 0.30606434 0.34868336 0.37236142 0.39481223 0.41673505 0.41040286 0.36901933 0.31965083 0.27729675 0.23067637 0.18567088 0.15530133][0.11591621 0.14465868 0.19003253 0.2404477 0.2739436 0.29456329 0.31855658 0.34641987 0.35429397 0.33169514 0.29603371 0.26140246 0.22134104 0.18645652 0.16362424][0.10510737 0.11443283 0.13420977 0.1616686 0.1837479 0.2034871 0.23141856 0.26599967 0.29036739 0.29365033 0.28151143 0.26308808 0.23460823 0.210692 0.19406687][0.10562424 0.095998891 0.093049459 0.1023437 0.11747324 0.13898671 0.171182 0.21186543 0.25077164 0.2770386 0.28614485 0.28255293 0.26403981 0.24650833 0.23105888][0.11220212 0.089592181 0.074590795 0.077542141 0.094013706 0.12046591 0.15648431 0.20172374 0.24982029 0.2894524 0.30995691 0.3143644 0.30132565 0.28474203 0.26571047][0.12704234 0.098389007 0.080188379 0.085062712 0.10750361 0.14016081 0.17855787 0.22454533 0.27460903 0.31692338 0.3383342 0.34261012 0.33038789 0.31281179 0.29074708]]...]
INFO - root - 2017-12-11 08:42:03.838888: step 40810, loss = 0.71, batch loss = 0.65 (14.3 examples/sec; 0.559 sec/batch; 45h:19m:30s remains)
INFO - root - 2017-12-11 08:42:09.348320: step 40820, loss = 0.69, batch loss = 0.63 (14.7 examples/sec; 0.544 sec/batch; 44h:05m:06s remains)
INFO - root - 2017-12-11 08:42:14.810511: step 40830, loss = 0.69, batch loss = 0.63 (14.6 examples/sec; 0.549 sec/batch; 44h:29m:59s remains)
INFO - root - 2017-12-11 08:42:20.297347: step 40840, loss = 0.69, batch loss = 0.63 (14.3 examples/sec; 0.558 sec/batch; 45h:12m:05s remains)
INFO - root - 2017-12-11 08:42:25.860103: step 40850, loss = 0.71, batch loss = 0.65 (14.4 examples/sec; 0.556 sec/batch; 45h:02m:11s remains)
INFO - root - 2017-12-11 08:42:31.418133: step 40860, loss = 0.72, batch loss = 0.66 (14.1 examples/sec; 0.567 sec/batch; 45h:57m:41s remains)
INFO - root - 2017-12-11 08:42:36.610751: step 40870, loss = 0.69, batch loss = 0.64 (14.6 examples/sec; 0.548 sec/batch; 44h:25m:30s remains)
INFO - root - 2017-12-11 08:42:42.116167: step 40880, loss = 0.71, batch loss = 0.65 (14.4 examples/sec; 0.557 sec/batch; 45h:05m:46s remains)
INFO - root - 2017-12-11 08:42:47.626862: step 40890, loss = 0.69, batch loss = 0.64 (14.0 examples/sec; 0.571 sec/batch; 46h:14m:02s remains)
INFO - root - 2017-12-11 08:42:53.125669: step 40900, loss = 0.69, batch loss = 0.63 (14.9 examples/sec; 0.537 sec/batch; 43h:28m:03s remains)
2017-12-11 08:42:53.688071: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.022778898 0.014300019 0.0063565867 0.0024307405 0.0035413706 0.012166531 0.033812892 0.071727484 0.11397182 0.14505966 0.15884404 0.15401897 0.13046046 0.088252231 0.039083578][0.025297109 0.018034123 0.014350911 0.018141676 0.028937487 0.047981359 0.082340956 0.13606611 0.19504151 0.238531 0.25608575 0.24478517 0.20440492 0.13848597 0.064537272][0.023667565 0.021281336 0.027088907 0.043688931 0.068360418 0.09923999 0.1447292 0.20978394 0.279849 0.33042753 0.34638092 0.32413107 0.26558492 0.17960966 0.087910824][0.025089182 0.032695137 0.05244568 0.084819019 0.12508495 0.16707997 0.21931903 0.28726888 0.35740125 0.40340549 0.40695581 0.36690176 0.29016206 0.19235407 0.0957538][0.033833832 0.056148738 0.092291556 0.14062896 0.19602741 0.24770191 0.301921 0.36305344 0.4204604 0.44874248 0.43014663 0.36951461 0.27999055 0.18255323 0.0955145][0.05273816 0.09149415 0.14383179 0.20664467 0.275828 0.33658388 0.38964167 0.43656886 0.47117761 0.47292864 0.42929393 0.35352293 0.26323342 0.18024753 0.1141922][0.086968377 0.14029819 0.20581938 0.27958849 0.35831985 0.42437291 0.47137827 0.49874571 0.50463235 0.47793111 0.41488233 0.33747643 0.2634435 0.20926823 0.17236689][0.12702453 0.19010651 0.26452839 0.34515923 0.42782491 0.49357682 0.53049386 0.53733021 0.51517439 0.46422812 0.39029345 0.32250607 0.27557942 0.25802046 0.25456914][0.16479823 0.23105882 0.30796525 0.38921547 0.46822336 0.5263592 0.54992962 0.53936774 0.49846035 0.436092 0.36397651 0.31427029 0.29748359 0.31492886 0.34142384][0.18785883 0.24938291 0.32081056 0.39493719 0.46286815 0.50765669 0.51699078 0.49430788 0.44621098 0.386714 0.328727 0.30329242 0.31552267 0.36050925 0.40758827][0.18750721 0.2390331 0.29941919 0.36102641 0.41390637 0.44375271 0.44089895 0.41095337 0.36249942 0.31213185 0.27129281 0.26739898 0.30001852 0.35967922 0.41515309][0.15940693 0.19735804 0.24240617 0.28688288 0.32196963 0.33695546 0.32522815 0.29297051 0.24822749 0.20809589 0.18210313 0.19218858 0.2333533 0.29437372 0.34789845][0.098040909 0.12062 0.14886481 0.17533961 0.19447821 0.19925331 0.1857179 0.15834688 0.12228766 0.091878876 0.074718475 0.087322637 0.12425213 0.17589016 0.22129124][0.026094165 0.036781937 0.051234934 0.062587425 0.068749689 0.066080078 0.053705074 0.034545358 0.010844911 -0.0087281428 -0.019385694 -0.010286764 0.015806757 0.052575067 0.086143181][-0.037468236 -0.034133151 -0.028307335 -0.026349572 -0.028465685 -0.036006007 -0.04730795 -0.0598906 -0.072998226 -0.082951739 -0.087975346 -0.082060963 -0.065647021 -0.042387165 -0.020143285]]...]
INFO - root - 2017-12-11 08:42:59.232216: step 40910, loss = 0.68, batch loss = 0.63 (14.3 examples/sec; 0.559 sec/batch; 45h:18m:45s remains)
INFO - root - 2017-12-11 08:43:04.759204: step 40920, loss = 0.71, batch loss = 0.65 (14.3 examples/sec; 0.561 sec/batch; 45h:26m:57s remains)
INFO - root - 2017-12-11 08:43:10.265428: step 40930, loss = 0.70, batch loss = 0.64 (14.9 examples/sec; 0.539 sec/batch; 43h:37m:12s remains)
INFO - root - 2017-12-11 08:43:15.715996: step 40940, loss = 0.69, batch loss = 0.63 (14.8 examples/sec; 0.539 sec/batch; 43h:40m:05s remains)
INFO - root - 2017-12-11 08:43:21.160046: step 40950, loss = 0.69, batch loss = 0.63 (14.7 examples/sec; 0.543 sec/batch; 43h:59m:14s remains)
INFO - root - 2017-12-11 08:43:26.538276: step 40960, loss = 0.69, batch loss = 0.63 (16.7 examples/sec; 0.480 sec/batch; 38h:53m:48s remains)
INFO - root - 2017-12-11 08:43:31.818879: step 40970, loss = 0.70, batch loss = 0.64 (13.7 examples/sec; 0.585 sec/batch; 47h:23m:35s remains)
INFO - root - 2017-12-11 08:43:37.292198: step 40980, loss = 0.70, batch loss = 0.64 (14.9 examples/sec; 0.536 sec/batch; 43h:24m:06s remains)
INFO - root - 2017-12-11 08:43:42.767903: step 40990, loss = 0.70, batch loss = 0.64 (14.8 examples/sec; 0.540 sec/batch; 43h:42m:48s remains)
INFO - root - 2017-12-11 08:43:48.255975: step 41000, loss = 0.68, batch loss = 0.62 (14.2 examples/sec; 0.562 sec/batch; 45h:31m:35s remains)
2017-12-11 08:43:48.834141: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.051133364 0.10849221 0.16917068 0.2132327 0.23111846 0.22640356 0.20025098 0.15216199 0.093138643 0.044366959 0.021691751 0.019206643 0.025901724 0.033796225 0.039803118][0.053898577 0.11459511 0.17883845 0.224942 0.24342839 0.23785079 0.20785795 0.1523259 0.084854878 0.031227939 0.010354928 0.015137494 0.031999148 0.051171903 0.06883321][0.051539775 0.11233637 0.1751807 0.21815391 0.23309118 0.22473817 0.19124058 0.1320326 0.063055128 0.011332941 -0.0043458915 0.0078184474 0.033213679 0.061810009 0.090805806][0.054323517 0.11797839 0.1814137 0.22243822 0.23406059 0.22293901 0.1858547 0.12438982 0.057640087 0.011731 0.001704423 0.016157845 0.041555706 0.070358157 0.10249405][0.066933922 0.14099051 0.21322888 0.26031861 0.2750155 0.26441798 0.22342567 0.15729572 0.090797156 0.049453449 0.041978877 0.050898217 0.065111704 0.0813829 0.1032718][0.084975228 0.17369843 0.26106638 0.3223033 0.3484031 0.3439129 0.30140829 0.23014665 0.16257086 0.12491672 0.11864728 0.1179063 0.11352143 0.10744904 0.10741609][0.10245496 0.20443116 0.30721623 0.38506258 0.42714137 0.43293115 0.39347202 0.3214792 0.25668207 0.22608759 0.22425069 0.21617882 0.19238213 0.15987176 0.13083729][0.11787309 0.22954735 0.34410986 0.43518549 0.49120089 0.50642765 0.47220552 0.40485308 0.34871042 0.330401 0.33833742 0.32966432 0.29193866 0.23571043 0.17772667][0.12682249 0.24304643 0.3636921 0.46185678 0.52610856 0.54746157 0.51897782 0.46044618 0.41713637 0.41417238 0.43550429 0.43346643 0.39069554 0.31839997 0.23733942][0.12582643 0.24150494 0.36264107 0.46232492 0.52962345 0.55471289 0.53209937 0.48369244 0.45281211 0.46287692 0.49628112 0.50445122 0.46460408 0.38538298 0.29047266][0.11645841 0.22496191 0.33939895 0.43396103 0.49861541 0.52461457 0.50694257 0.46736211 0.44480073 0.461048 0.50009936 0.51706237 0.48544943 0.40881217 0.31126797][0.096940607 0.19048798 0.28960705 0.37115169 0.42686537 0.45014089 0.43533731 0.401012 0.38029891 0.39361629 0.4292697 0.44950491 0.42758542 0.36158639 0.27295467][0.06530939 0.1370431 0.21302223 0.27378148 0.31389362 0.32980832 0.31526753 0.28383 0.2617631 0.26725307 0.29337484 0.31143424 0.29842922 0.24862337 0.17852561][0.019295841 0.065285631 0.11418752 0.15040955 0.17157389 0.17756517 0.16275603 0.13511707 0.11332753 0.11178693 0.12750661 0.14109114 0.13568862 0.10447141 0.058319751][-0.032658413 -0.011307719 0.012760604 0.027339933 0.032154765 0.029506372 0.015481891 -0.0062308805 -0.024109326 -0.028605359 -0.020671524 -0.011696461 -0.011690003 -0.026353793 -0.049674217]]...]
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-SGD-lr0.001-clip50-initfromfixqian30k-fixqian
INFO - root - 2017-12-11 08:43:54.340439: step 41010, loss = 0.70, batch loss = 0.64 (14.8 examples/sec; 0.541 sec/batch; 43h:46m:44s remains)
INFO - root - 2017-12-11 08:43:59.911430: step 41020, loss = 0.70, batch loss = 0.64 (14.5 examples/sec; 0.551 sec/batch; 44h:38m:20s remains)
INFO - root - 2017-12-11 08:44:05.487328: step 41030, loss = 0.71, batch loss = 0.65 (14.8 examples/sec; 0.540 sec/batch; 43h:45m:24s remains)
INFO - root - 2017-12-11 08:44:10.839670: step 41040, loss = 0.69, batch loss = 0.63 (14.5 examples/sec; 0.550 sec/batch; 44h:33m:33s remains)
INFO - root - 2017-12-11 08:44:16.362680: step 41050, loss = 0.68, batch loss = 0.62 (14.5 examples/sec; 0.552 sec/batch; 44h:42m:14s remains)
INFO - root - 2017-12-11 08:44:21.574814: step 41060, loss = 0.69, batch loss = 0.63 (14.3 examples/sec; 0.558 sec/batch; 45h:12m:40s remains)
INFO - root - 2017-12-11 08:44:27.071534: step 41070, loss = 0.71, batch loss = 0.66 (14.3 examples/sec; 0.558 sec/batch; 45h:11m:00s remains)
INFO - root - 2017-12-11 08:44:32.626976: step 41080, loss = 0.70, batch loss = 0.64 (14.5 examples/sec; 0.552 sec/batch; 44h:39m:13s remains)
INFO - root - 2017-12-11 08:44:38.108958: step 41090, loss = 0.69, batch loss = 0.63 (14.7 examples/sec; 0.545 sec/batch; 44h:05m:22s remains)
INFO - root - 2017-12-11 08:44:43.522407: step 41100, loss = 0.70, batch loss = 0.64 (14.7 examples/sec; 0.543 sec/batch; 43h:56m:09s remains)
2017-12-11 08:44:44.143552: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.12119759 0.16260682 0.19361366 0.21550511 0.2242163 0.21104591 0.17907761 0.13985117 0.10304271 0.070116431 0.05365894 0.0709957 0.11651836 0.16837406 0.20282887][0.08626201 0.1315612 0.1778814 0.22643808 0.26671502 0.2835277 0.27088124 0.23349044 0.17851891 0.11226086 0.054112293 0.027849622 0.037198991 0.068042636 0.099341996][0.055936642 0.10988422 0.17606466 0.25569162 0.33399856 0.39066842 0.40935057 0.38610619 0.3239466 0.23066005 0.13027269 0.051652271 0.009768609 0.0017307741 0.013780579][0.058513768 0.13229015 0.22763251 0.34299454 0.46058705 0.55737793 0.61014211 0.607535 0.54830313 0.43844742 0.30384293 0.17507738 0.075890049 0.016350739 -0.0061007389][0.090871327 0.19435725 0.32554403 0.47498551 0.6221332 0.74442351 0.8168183 0.82571542 0.77128446 0.656687 0.505429 0.34627292 0.20598909 0.1035834 0.0456084][0.13250124 0.26403248 0.42480955 0.59556973 0.7533735 0.87781048 0.94805479 0.954764 0.90344727 0.79523379 0.64859313 0.48768103 0.33516014 0.21262565 0.13211639][0.16354316 0.30388239 0.46906337 0.6338976 0.77581751 0.87815464 0.92789435 0.92343044 0.87659562 0.78783476 0.66961503 0.53919047 0.41000387 0.29930481 0.21882983][0.16002953 0.28179371 0.41965729 0.54942489 0.65269119 0.71778822 0.73915595 0.72198415 0.68190736 0.62075323 0.54529691 0.46453762 0.381311 0.30528173 0.2442335][0.10956909 0.19458048 0.28826228 0.3720589 0.43334448 0.46433914 0.46303427 0.43770429 0.40440378 0.36734402 0.32984817 0.29462793 0.25714746 0.22068505 0.18792152][0.028476637 0.071858145 0.11974061 0.15997416 0.18557632 0.19196977 0.17821872 0.15198745 0.12634265 0.10681258 0.095493071 0.091546409 0.088369429 0.085824423 0.081969708][-0.045482043 -0.034944687 -0.021990523 -0.013777432 -0.012693831 -0.020735955 -0.038283568 -0.059968188 -0.077408955 -0.085954316 -0.083720677 -0.072092943 -0.056566723 -0.038788702 -0.02310539][-0.0858862 -0.092931993 -0.098379806 -0.10664672 -0.11742721 -0.13066073 -0.14595418 -0.1605178 -0.17053257 -0.17337586 -0.16723661 -0.15242092 -0.13209334 -0.10832671 -0.085829049][-0.097073309 -0.1095942 -0.12036473 -0.13270578 -0.14537629 -0.15724704 -0.16759638 -0.17548212 -0.18005814 -0.18044776 -0.17533295 -0.16417921 -0.14816251 -0.12887526 -0.10979445][-0.093745232 -0.10576431 -0.11514809 -0.12488986 -0.13401404 -0.14152753 -0.14689863 -0.1502143 -0.15173569 -0.15149504 -0.14871539 -0.14259954 -0.13321805 -0.12133902 -0.10891405][-0.082513109 -0.091441177 -0.097116672 -0.10253632 -0.10728617 -0.11089425 -0.11313637 -0.11428399 -0.11465146 -0.1144268 -0.11330004 -0.11074183 -0.10653865 -0.10084803 -0.09448611]]...]
INFO - root - 2017-12-11 08:44:49.685347: step 41110, loss = 0.70, batch loss = 0.64 (14.6 examples/sec; 0.548 sec/batch; 44h:19m:29s remains)
INFO - root - 2017-12-11 08:44:55.210858: step 41120, loss = 0.69, batch loss = 0.63 (14.4 examples/sec; 0.557 sec/batch; 45h:07m:03s remains)
INFO - root - 2017-12-11 08:45:00.734053: step 41130, loss = 0.69, batch loss = 0.63 (14.4 examples/sec; 0.556 sec/batch; 45h:00m:32s remains)
INFO - root - 2017-12-11 08:45:06.199301: step 41140, loss = 0.69, batch loss = 0.64 (15.2 examples/sec; 0.527 sec/batch; 42h:36m:43s remains)
INFO - root - 2017-12-11 08:45:11.740139: step 41150, loss = 0.70, batch loss = 0.64 (14.2 examples/sec; 0.562 sec/batch; 45h:27m:20s remains)
INFO - root - 2017-12-11 08:45:17.004537: step 41160, loss = 0.68, batch loss = 0.62 (14.7 examples/sec; 0.543 sec/batch; 43h:55m:35s remains)
INFO - root - 2017-12-11 08:45:22.623427: step 41170, loss = 0.71, batch loss = 0.65 (14.4 examples/sec; 0.555 sec/batch; 44h:54m:46s remains)
INFO - root - 2017-12-11 08:45:28.095115: step 41180, loss = 0.70, batch loss = 0.64 (14.4 examples/sec; 0.554 sec/batch; 44h:48m:58s remains)
INFO - root - 2017-12-11 08:45:33.632058: step 41190, loss = 0.69, batch loss = 0.63 (14.3 examples/sec; 0.559 sec/batch; 45h:12m:44s remains)
INFO - root - 2017-12-11 08:45:39.165960: step 41200, loss = 0.69, batch loss = 0.63 (15.0 examples/sec; 0.534 sec/batch; 43h:13m:50s remains)
2017-12-11 08:45:39.805132: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.049723886 0.046751965 0.039152186 0.0289714 0.018567877 0.0081895357 -0.0012664325 -0.010314822 -0.019051982 -0.026543645 -0.031437088 -0.033595432 -0.034236353 -0.034661535 -0.035580292][0.040237855 0.037489448 0.030298095 0.02057372 0.011122884 0.001914333 -0.0065567037 -0.01481286 -0.022984009 -0.030047728 -0.034655411 -0.03665996 -0.036959741 -0.037064265 -0.037758317][0.019880541 0.019402834 0.016156122 0.010793847 0.0053336015 -0.0011809827 -0.0087636514 -0.017404137 -0.026672082 -0.034764782 -0.040033709 -0.042191744 -0.041983053 -0.04120459 -0.040930208][0.0014812261 0.0068165781 0.012785473 0.017492063 0.020147385 0.017691704 0.0095129889 -0.0030387721 -0.018023906 -0.031869084 -0.041606929 -0.046349909 -0.046929676 -0.04587543 -0.044751171][0.0009525271 0.015268698 0.034776621 0.054503836 0.068965457 0.071621768 0.061021544 0.040339865 0.013867866 -0.01192987 -0.0315211 -0.042867661 -0.047080114 -0.047658384 -0.046812266][0.025147535 0.049325541 0.083421342 0.12008817 0.14826673 0.15680522 0.14301847 0.11192062 0.070466518 0.02855454 -0.0050117848 -0.02654806 -0.037321635 -0.042022612 -0.043408245][0.067090295 0.098382927 0.14307642 0.19342867 0.23354647 0.24774525 0.23190962 0.19221228 0.13764137 0.081003264 0.0340289 0.0020201951 -0.016262621 -0.026440965 -0.03139019][0.10957722 0.14144501 0.18794096 0.24320027 0.28911498 0.30713534 0.29184133 0.24955311 0.19007987 0.12727034 0.07383237 0.035733033 0.012122956 -0.0026375924 -0.011052883][0.13252363 0.15723747 0.1950753 0.2438819 0.2868872 0.30532068 0.29315671 0.25603554 0.20304632 0.14676851 0.098171115 0.062390979 0.039010867 0.02304781 0.013086415][0.12359797 0.1364287 0.1592104 0.19377835 0.22736946 0.24363841 0.23672734 0.2109576 0.17351584 0.13398007 0.099575534 0.073571108 0.055762667 0.042491768 0.033722635][0.086563148 0.08745975 0.0950498 0.11368943 0.13567689 0.14872703 0.14787453 0.13589759 0.11752489 0.09846437 0.081694841 0.068376489 0.058450222 0.049950391 0.044148788][0.03833079 0.030610219 0.027238984 0.032950237 0.044543952 0.05427368 0.058495417 0.058532305 0.056674697 0.05503979 0.053128567 0.050492257 0.047244653 0.04313124 0.040407248][-0.0040915753 -0.015827263 -0.024432244 -0.026423296 -0.021965921 -0.014847423 -0.0074455431 0.00029078865 0.0086215325 0.017126538 0.023460157 0.026509689 0.026637128 0.024883773 0.023884762][-0.029953903 -0.0415423 -0.050677866 -0.055622041 -0.055104379 -0.050221946 -0.042221367 -0.03188603 -0.020289201 -0.0091643492 -0.0011652079 0.0026898033 0.0030394585 0.0015652973 0.00074382831][-0.037461046 -0.046875309 -0.0543586 -0.059757166 -0.061485425 -0.059143662 -0.05313864 -0.044451222 -0.034617204 -0.02565386 -0.019759949 -0.017585054 -0.018370299 -0.020400405 -0.021749353]]...]
INFO - root - 2017-12-11 08:45:45.325132: step 41210, loss = 0.68, batch loss = 0.62 (14.2 examples/sec; 0.564 sec/batch; 45h:38m:19s remains)
INFO - root - 2017-12-11 08:45:50.855873: step 41220, loss = 0.69, batch loss = 0.63 (14.5 examples/sec; 0.553 sec/batch; 44h:42m:32s remains)
INFO - root - 2017-12-11 08:45:56.392815: step 41230, loss = 0.69, batch loss = 0.63 (14.7 examples/sec; 0.544 sec/batch; 44h:03m:05s remains)
INFO - root - 2017-12-11 08:46:01.962578: step 41240, loss = 0.70, batch loss = 0.64 (14.5 examples/sec; 0.550 sec/batch; 44h:30m:40s remains)
INFO - root - 2017-12-11 08:46:07.479317: step 41250, loss = 0.71, batch loss = 0.65 (14.3 examples/sec; 0.559 sec/batch; 45h:12m:22s remains)
INFO - root - 2017-12-11 08:46:12.578768: step 41260, loss = 0.67, batch loss = 0.61 (14.7 examples/sec; 0.543 sec/batch; 43h:55m:43s remains)
INFO - root - 2017-12-11 08:46:18.028182: step 41270, loss = 0.70, batch loss = 0.64 (15.0 examples/sec; 0.533 sec/batch; 43h:04m:54s remains)
INFO - root - 2017-12-11 08:46:23.474104: step 41280, loss = 0.70, batch loss = 0.64 (14.3 examples/sec; 0.560 sec/batch; 45h:19m:11s remains)
INFO - root - 2017-12-11 08:46:28.997827: step 41290, loss = 0.71, batch loss = 0.65 (14.1 examples/sec; 0.569 sec/batch; 45h:59m:23s remains)
INFO - root - 2017-12-11 08:46:34.515133: step 41300, loss = 0.68, batch loss = 0.62 (14.5 examples/sec; 0.550 sec/batch; 44h:31m:23s remains)
2017-12-11 08:46:35.103549: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.076932348 -0.087997042 -0.094306767 -0.096950926 -0.094534129 -0.08732982 -0.079257347 -0.075336032 -0.072628863 -0.064106293 -0.051127724 -0.039121237 -0.03143362 -0.028134488 -0.035404652][-0.021597037 -0.037804332 -0.047807105 -0.051772196 -0.047292277 -0.033962373 -0.019546982 -0.013634415 -0.011264758 0.00029459811 0.019198131 0.035902552 0.046503015 0.051687188 0.040186476][0.05904977 0.043418497 0.037089959 0.039594363 0.053695895 0.080365986 0.10574408 0.11353617 0.11042362 0.11652599 0.13131443 0.14230631 0.14666435 0.1480702 0.13043585][0.14556894 0.14083058 0.15084586 0.17239915 0.20768137 0.25724891 0.29893917 0.30792731 0.29170525 0.2786499 0.2725915 0.26123291 0.24534166 0.23337804 0.20618014][0.23451652 0.2517314 0.28804451 0.33713546 0.40078327 0.47841096 0.53825945 0.5462411 0.50920594 0.46296263 0.41823575 0.3664133 0.31425327 0.2763091 0.23448995][0.32917812 0.37205029 0.4337239 0.50756466 0.59715044 0.69977397 0.77441937 0.77939361 0.71895313 0.63317323 0.53983808 0.43712312 0.33854446 0.26498622 0.20393178][0.42944416 0.48962942 0.56045586 0.64175057 0.742433 0.85708916 0.93819803 0.93910271 0.86086917 0.74347526 0.60896903 0.46282241 0.32396296 0.21741462 0.13824597][0.52933258 0.58742696 0.64023221 0.70097393 0.78771585 0.8934508 0.968596 0.96500689 0.88090432 0.75153577 0.59884953 0.43298587 0.27538356 0.15223703 0.063745871][0.59773695 0.6315794 0.64040315 0.65508485 0.70402223 0.78116596 0.83968717 0.8344655 0.76138812 0.64662689 0.50668156 0.35215053 0.20387639 0.085768312 0.00029150391][0.59626287 0.59346336 0.54950291 0.51011282 0.51156008 0.55113196 0.58922726 0.587176 0.53948516 0.46118864 0.35959524 0.24175923 0.12475818 0.028633684 -0.043594867][0.50157511 0.46549997 0.38243335 0.30359724 0.2665827 0.2731007 0.29420358 0.30015847 0.28515738 0.25373867 0.20306428 0.1340266 0.058190897 -0.0090246284 -0.064445227][0.34094387 0.28746265 0.19200779 0.10001839 0.04392568 0.029792726 0.040836252 0.056672655 0.072236739 0.084444016 0.082011156 0.058585629 0.019793725 -0.023070633 -0.066091254][0.17238832 0.11996648 0.038031925 -0.041514408 -0.096178107 -0.11685748 -0.10952755 -0.085860752 -0.049361505 -0.00740352 0.023103502 0.029854978 0.01372902 -0.017031362 -0.057180315][0.046471212 0.0071905176 -0.047181237 -0.10013751 -0.14031979 -0.15794633 -0.15055734 -0.12274881 -0.076246917 -0.020172752 0.025939185 0.046008609 0.037847016 0.0078643914 -0.037376307][-0.011278203 -0.034859862 -0.062584087 -0.089896247 -0.11436073 -0.12663387 -0.11934268 -0.092759944 -0.046901591 0.0094923759 0.057193294 0.078788422 0.070254415 0.036911294 -0.014264552]]...]
INFO - root - 2017-12-11 08:46:40.706693: step 41310, loss = 0.70, batch loss = 0.64 (14.1 examples/sec; 0.569 sec/batch; 46h:02m:15s remains)
INFO - root - 2017-12-11 08:46:46.240170: step 41320, loss = 0.71, batch loss = 0.66 (14.3 examples/sec; 0.558 sec/batch; 45h:08m:29s remains)
INFO - root - 2017-12-11 08:46:51.670605: step 41330, loss = 0.70, batch loss = 0.65 (14.3 examples/sec; 0.560 sec/batch; 45h:18m:33s remains)
INFO - root - 2017-12-11 08:46:57.203560: step 41340, loss = 0.70, batch loss = 0.64 (14.8 examples/sec; 0.542 sec/batch; 43h:51m:00s remains)
INFO - root - 2017-12-11 08:47:02.347773: step 41350, loss = 0.72, batch loss = 0.66 (17.6 examples/sec; 0.454 sec/batch; 36h:41m:21s remains)
INFO - root - 2017-12-11 08:47:07.869083: step 41360, loss = 0.69, batch loss = 0.63 (14.9 examples/sec; 0.537 sec/batch; 43h:26m:51s remains)
INFO - root - 2017-12-11 08:47:13.364227: step 41370, loss = 0.68, batch loss = 0.63 (14.8 examples/sec; 0.539 sec/batch; 43h:36m:35s remains)
INFO - root - 2017-12-11 08:47:18.883493: step 41380, loss = 0.71, batch loss = 0.65 (14.7 examples/sec; 0.544 sec/batch; 43h:57m:44s remains)
INFO - root - 2017-12-11 08:47:24.473271: step 41390, loss = 0.68, batch loss = 0.62 (14.2 examples/sec; 0.563 sec/batch; 45h:33m:10s remains)
INFO - root - 2017-12-11 08:47:29.968556: step 41400, loss = 0.70, batch loss = 0.64 (14.7 examples/sec; 0.543 sec/batch; 43h:55m:01s remains)
2017-12-11 08:47:30.598406: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.062867656 -0.043412089 -0.011481863 0.028307162 0.065722339 0.0909655 0.096129946 0.079923533 0.053190373 0.028446905 0.017598169 0.027926004 0.055318374 0.087396912 0.10824639][-0.043094307 -0.019082148 0.015069308 0.055261336 0.091795355 0.11617739 0.1214554 0.1060777 0.079334646 0.053599589 0.040516458 0.048083436 0.073086396 0.10334937 0.12303986][-0.0066400645 0.022053355 0.058022592 0.098458804 0.13499241 0.16101597 0.16976517 0.15702686 0.12943238 0.097890772 0.073517263 0.066071622 0.076526508 0.095902748 0.10940891][0.041921329 0.075429551 0.11411499 0.1578512 0.19937433 0.23210344 0.24778847 0.23824443 0.20763563 0.16491885 0.12130141 0.089308351 0.075767554 0.076754019 0.079661071][0.09769088 0.13721219 0.18195061 0.23517179 0.28950465 0.33512259 0.36046481 0.35382649 0.31853688 0.26201126 0.19653627 0.13707006 0.095162496 0.072328009 0.059935808][0.15006147 0.19626591 0.25022087 0.31774867 0.38987789 0.4517574 0.48820442 0.48477238 0.44493383 0.37541267 0.29121625 0.20898737 0.1417616 0.095325716 0.066054434][0.18534394 0.23577732 0.29837832 0.37920707 0.46669552 0.5422442 0.5890283 0.59058046 0.54997045 0.47302476 0.37867677 0.28447983 0.20200673 0.13881758 0.096376121][0.19563416 0.24514286 0.31179056 0.39984152 0.49490774 0.577091 0.63129812 0.64063621 0.60581195 0.52959144 0.43455398 0.33887526 0.25150323 0.17982185 0.12985025][0.18465026 0.2267914 0.28937092 0.3743737 0.46546286 0.54433531 0.60035318 0.61823297 0.59396362 0.52586544 0.43856868 0.34986898 0.26585668 0.19253045 0.13979654][0.16020685 0.18905516 0.23821214 0.30827478 0.38299888 0.44850838 0.49977779 0.52400851 0.51132631 0.45627195 0.38264018 0.30655709 0.23162812 0.16244757 0.11162468][0.12844513 0.14394183 0.17596324 0.22538219 0.27859545 0.3269662 0.36934254 0.3952176 0.39017805 0.34705013 0.28685829 0.22364466 0.15982386 0.099029727 0.054616787][0.096852161 0.1058124 0.12600423 0.15908583 0.19535078 0.22934967 0.26099452 0.28257525 0.27889773 0.24335946 0.19322495 0.14059165 0.087837011 0.037542704 0.0015006791][0.069904327 0.080315687 0.097591296 0.12269095 0.14952704 0.17354572 0.19473274 0.20842719 0.20215419 0.17160244 0.13007209 0.087686241 0.046517983 0.007859013 -0.019907659][0.041967362 0.057473216 0.077920705 0.10123479 0.12379245 0.14148077 0.15489066 0.1618953 0.15414058 0.1304152 0.10044806 0.071844 0.045304209 0.020615052 0.0020977154][0.0076548578 0.026522355 0.050898172 0.075006716 0.096553914 0.11214533 0.12318015 0.12906034 0.12479134 0.11155833 0.096147023 0.083729453 0.07351093 0.064166717 0.056674626]]...]
INFO - root - 2017-12-11 08:47:36.058016: step 41410, loss = 0.70, batch loss = 0.64 (14.7 examples/sec; 0.544 sec/batch; 43h:59m:52s remains)
INFO - root - 2017-12-11 08:47:41.482122: step 41420, loss = 0.69, batch loss = 0.64 (14.7 examples/sec; 0.544 sec/batch; 43h:59m:14s remains)
INFO - root - 2017-12-11 08:47:47.008191: step 41430, loss = 0.70, batch loss = 0.64 (14.2 examples/sec; 0.565 sec/batch; 45h:41m:27s remains)
INFO - root - 2017-12-11 08:47:52.635203: step 41440, loss = 0.71, batch loss = 0.65 (14.3 examples/sec; 0.561 sec/batch; 45h:20m:17s remains)
INFO - root - 2017-12-11 08:47:57.900025: step 41450, loss = 0.69, batch loss = 0.63 (14.5 examples/sec; 0.551 sec/batch; 44h:31m:01s remains)
INFO - root - 2017-12-11 08:48:03.465212: step 41460, loss = 0.71, batch loss = 0.65 (14.4 examples/sec; 0.556 sec/batch; 44h:55m:28s remains)
INFO - root - 2017-12-11 08:48:08.934151: step 41470, loss = 0.70, batch loss = 0.64 (14.6 examples/sec; 0.548 sec/batch; 44h:17m:44s remains)
INFO - root - 2017-12-11 08:48:14.437090: step 41480, loss = 0.72, batch loss = 0.66 (14.6 examples/sec; 0.548 sec/batch; 44h:18m:49s remains)
INFO - root - 2017-12-11 08:48:19.985765: step 41490, loss = 0.70, batch loss = 0.64 (14.4 examples/sec; 0.557 sec/batch; 45h:03m:32s remains)
INFO - root - 2017-12-11 08:48:25.506320: step 41500, loss = 0.67, batch loss = 0.62 (14.5 examples/sec; 0.552 sec/batch; 44h:36m:49s remains)
2017-12-11 08:48:26.068458: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.088590972 -0.073510461 -0.036891643 0.02387964 0.10128238 0.18836835 0.2750628 0.35061637 0.40412247 0.42657277 0.4156431 0.37157369 0.30918166 0.25178838 0.2186964][-0.091842659 -0.078526847 -0.041364413 0.02405509 0.11020765 0.20589644 0.29414046 0.35956571 0.39153734 0.38962686 0.36130965 0.31391978 0.26158881 0.22236766 0.20885488][-0.08845453 -0.07287024 -0.031199163 0.041824892 0.13731679 0.23886992 0.32266328 0.36890677 0.36956564 0.33356959 0.27977332 0.22448646 0.18288901 0.16677183 0.17932886][-0.08199054 -0.059285108 -0.0067158244 0.080831632 0.19180581 0.30299711 0.38359129 0.4103595 0.37801325 0.30334193 0.21629477 0.1433448 0.10243366 0.1005416 0.13308929][-0.076978065 -0.046754468 0.019204702 0.12579378 0.25828207 0.3860507 0.47062638 0.4841913 0.42307043 0.31036848 0.18657535 0.087674662 0.035101496 0.032839984 0.06997893][-0.075024359 -0.038544785 0.040855736 0.16914929 0.32833365 0.47960946 0.5753563 0.58148366 0.49417117 0.34320334 0.18043523 0.05125716 -0.019695736 -0.032154124 -0.0033138124][-0.074364915 -0.032304402 0.060238939 0.21076287 0.39821261 0.57488954 0.6829015 0.68173558 0.56859952 0.3808341 0.18188812 0.025906464 -0.061080277 -0.084488027 -0.06865555][-0.074449636 -0.02826735 0.075202964 0.24410512 0.45460194 0.65099639 0.76599663 0.75444269 0.6168893 0.39926183 0.17519888 0.0045602228 -0.088414118 -0.11694376 -0.11176761][-0.075573362 -0.028130161 0.0807372 0.25845063 0.47897664 0.68172073 0.79399687 0.7693119 0.61325026 0.37962008 0.14885792 -0.018218964 -0.10290369 -0.12661564 -0.12475034][-0.077905267 -0.03236552 0.074290164 0.24816535 0.46249875 0.65625519 0.75708717 0.7210238 0.55839515 0.32721773 0.10894033 -0.038716692 -0.10356186 -0.11348546 -0.10658032][-0.080403052 -0.039956726 0.057106014 0.21444206 0.40668032 0.57737184 0.6603716 0.61819828 0.46488047 0.25673115 0.068280704 -0.049640123 -0.089768134 -0.082672566 -0.066082336][-0.082560547 -0.048573483 0.034968071 0.16833828 0.32902205 0.46866843 0.53145027 0.48880056 0.35766131 0.18774354 0.040271584 -0.043783374 -0.060951374 -0.040289417 -0.015488114][-0.083295219 -0.054509778 0.016265376 0.12580255 0.25466943 0.36392868 0.40965053 0.37235463 0.27014589 0.14376862 0.038537003 -0.016128717 -0.019741349 0.0037996371 0.028499911][-0.082391359 -0.056710817 0.0044479202 0.095418863 0.19994234 0.28792462 0.32601315 0.30156112 0.23069926 0.14416614 0.0719937 0.033493221 0.028473452 0.041503992 0.056249842][-0.080358095 -0.05572246 0.00018354798 0.080293916 0.17131479 0.25035241 0.2913568 0.28531691 0.24428855 0.18903549 0.13646409 0.098593615 0.0776652 0.068562187 0.065179981]]...]
INFO - root - 2017-12-11 08:48:31.534402: step 41510, loss = 0.70, batch loss = 0.64 (14.9 examples/sec; 0.537 sec/batch; 43h:25m:55s remains)
INFO - root - 2017-12-11 08:48:37.025185: step 41520, loss = 0.70, batch loss = 0.64 (14.3 examples/sec; 0.561 sec/batch; 45h:20m:50s remains)
INFO - root - 2017-12-11 08:48:42.544499: step 41530, loss = 0.68, batch loss = 0.62 (14.7 examples/sec; 0.545 sec/batch; 44h:04m:14s remains)
INFO - root - 2017-12-11 08:48:48.027153: step 41540, loss = 0.70, batch loss = 0.64 (14.0 examples/sec; 0.572 sec/batch; 46h:12m:35s remains)
INFO - root - 2017-12-11 08:48:53.222025: step 41550, loss = 0.68, batch loss = 0.62 (14.0 examples/sec; 0.573 sec/batch; 46h:16m:23s remains)
INFO - root - 2017-12-11 08:48:58.712430: step 41560, loss = 0.68, batch loss = 0.62 (14.5 examples/sec; 0.551 sec/batch; 44h:32m:18s remains)
INFO - root - 2017-12-11 08:49:04.188445: step 41570, loss = 0.70, batch loss = 0.64 (14.9 examples/sec; 0.539 sec/batch; 43h:32m:09s remains)
INFO - root - 2017-12-11 08:49:09.626208: step 41580, loss = 0.69, batch loss = 0.63 (14.9 examples/sec; 0.538 sec/batch; 43h:30m:29s remains)
INFO - root - 2017-12-11 08:49:15.146704: step 41590, loss = 0.69, batch loss = 0.63 (14.3 examples/sec; 0.559 sec/batch; 45h:12m:13s remains)
INFO - root - 2017-12-11 08:49:20.619907: step 41600, loss = 0.68, batch loss = 0.62 (14.7 examples/sec; 0.545 sec/batch; 44h:01m:38s remains)
2017-12-11 08:49:21.276867: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.036497079 0.082498118 0.12816605 0.17013499 0.19765177 0.19921973 0.175533 0.1495419 0.12757486 0.10427985 0.07723137 0.055960726 0.047668692 0.035188418 0.0056371232][0.090056948 0.15952155 0.23640217 0.31485742 0.37254786 0.38883448 0.36524251 0.33230835 0.29418966 0.24476174 0.18892381 0.14669988 0.13144793 0.11578806 0.07605119][0.14824772 0.24211387 0.35104543 0.46463892 0.551953 0.58356017 0.56120235 0.51909572 0.45942959 0.38058114 0.29465356 0.2317638 0.20945881 0.19185898 0.1466237][0.19344828 0.31303656 0.45421681 0.60021913 0.71544904 0.7632764 0.74311423 0.68807274 0.60232526 0.49408254 0.38206789 0.30212134 0.2711032 0.24839613 0.19842002][0.233014 0.38167551 0.55378836 0.72509319 0.86010039 0.9175142 0.89260894 0.81587774 0.69976383 0.56624711 0.43728006 0.34877956 0.31122828 0.28189248 0.22713642][0.27797955 0.45533711 0.65040886 0.83233446 0.97108561 1.0245067 0.98576373 0.88480204 0.74625558 0.60164809 0.46907619 0.37981272 0.33700815 0.29887033 0.23744243][0.31949323 0.51974237 0.72839582 0.90858704 1.0371311 1.0739802 1.0151829 0.89565384 0.75244021 0.61801434 0.49675894 0.4109742 0.35940433 0.30686945 0.23406389][0.33474705 0.54862577 0.76424819 0.93835431 1.0528052 1.0714179 0.9965899 0.87207949 0.74427748 0.63949966 0.5401687 0.45680678 0.3885538 0.31282032 0.22184153][0.31489322 0.52913517 0.74402696 0.91122478 1.0138881 1.0214269 0.94280142 0.82878858 0.73002273 0.66294283 0.58981043 0.50959665 0.42385134 0.32201326 0.20846419][0.27602211 0.47589505 0.67600542 0.82606339 0.91119194 0.90985471 0.83670431 0.742773 0.67658746 0.64489782 0.598208 0.5266872 0.43282235 0.31447029 0.18594342][0.22371516 0.39294583 0.56051528 0.67964053 0.7405718 0.733866 0.67720354 0.61342436 0.58066714 0.57740045 0.55226022 0.49072313 0.39627719 0.27376014 0.144008][0.14313748 0.26748863 0.39037165 0.47352096 0.51307315 0.50883245 0.47662652 0.44617206 0.44121334 0.45462593 0.44061521 0.38563645 0.29655239 0.18431285 0.071232155][0.0459756 0.12070937 0.19679248 0.24691081 0.2710492 0.27234113 0.2612102 0.25479072 0.26529109 0.28341702 0.27359223 0.22673899 0.15273418 0.064957947 -0.017390748][-0.031836912 0.0022710115 0.040425681 0.064717062 0.076284014 0.077974841 0.074795656 0.074930511 0.0845942 0.096949734 0.08936803 0.056693431 0.0078127962 -0.046421066 -0.092903][-0.078760594 -0.069571435 -0.055050261 -0.047420066 -0.045915872 -0.048831064 -0.054456592 -0.059342463 -0.059436951 -0.057128474 -0.063687958 -0.081605434 -0.10515419 -0.12783618 -0.14313424]]...]
INFO - root - 2017-12-11 08:49:26.804056: step 41610, loss = 0.68, batch loss = 0.62 (14.4 examples/sec; 0.556 sec/batch; 44h:55m:30s remains)
INFO - root - 2017-12-11 08:49:32.331742: step 41620, loss = 0.68, batch loss = 0.63 (14.7 examples/sec; 0.545 sec/batch; 44h:03m:38s remains)
INFO - root - 2017-12-11 08:49:37.758061: step 41630, loss = 0.70, batch loss = 0.64 (14.9 examples/sec; 0.537 sec/batch; 43h:21m:36s remains)
INFO - root - 2017-12-11 08:49:43.075104: step 41640, loss = 0.67, batch loss = 0.61 (24.0 examples/sec; 0.333 sec/batch; 26h:55m:26s remains)
INFO - root - 2017-12-11 08:49:48.490340: step 41650, loss = 0.71, batch loss = 0.65 (14.8 examples/sec; 0.542 sec/batch; 43h:45m:34s remains)
INFO - root - 2017-12-11 08:49:54.026298: step 41660, loss = 0.70, batch loss = 0.64 (14.7 examples/sec; 0.543 sec/batch; 43h:54m:01s remains)
INFO - root - 2017-12-11 08:49:59.476317: step 41670, loss = 0.70, batch loss = 0.64 (14.4 examples/sec; 0.556 sec/batch; 44h:57m:23s remains)
INFO - root - 2017-12-11 08:50:05.023107: step 41680, loss = 0.71, batch loss = 0.65 (14.8 examples/sec; 0.540 sec/batch; 43h:38m:11s remains)
INFO - root - 2017-12-11 08:50:10.566310: step 41690, loss = 0.69, batch loss = 0.64 (14.5 examples/sec; 0.550 sec/batch; 44h:25m:47s remains)
INFO - root - 2017-12-11 08:50:16.077224: step 41700, loss = 0.72, batch loss = 0.66 (15.0 examples/sec; 0.533 sec/batch; 43h:02m:22s remains)
2017-12-11 08:50:16.665226: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.19695382 0.24590819 0.30536395 0.35403749 0.36964959 0.34599677 0.29080188 0.22011572 0.15936008 0.12619628 0.11817002 0.12003253 0.12465871 0.13357683 0.14572407][0.22030935 0.26184767 0.30702418 0.33907521 0.3438285 0.32218224 0.28077286 0.22845492 0.18206404 0.15463546 0.14311108 0.13439815 0.12559138 0.12347236 0.12808484][0.25065419 0.27898633 0.30226117 0.31173018 0.30314857 0.28588319 0.26478028 0.23957868 0.21534456 0.1985608 0.18450569 0.16312678 0.13722934 0.11950336 0.11282502][0.28290173 0.29589283 0.29604003 0.28414491 0.26480073 0.25446954 0.25530657 0.25826 0.25628513 0.24896665 0.2302928 0.1945008 0.15126656 0.11824681 0.10234621][0.30376396 0.30288112 0.28486356 0.25914866 0.23588578 0.23528679 0.25627285 0.282788 0.29768112 0.29468691 0.26795965 0.21731907 0.15896665 0.11471634 0.095875166][0.29449025 0.28271723 0.25496408 0.22623317 0.20839871 0.22187676 0.26195303 0.30703944 0.33285794 0.32925537 0.29250753 0.22821014 0.15813868 0.10766665 0.09163779][0.24708107 0.22866797 0.20068395 0.1791461 0.17428587 0.20448378 0.26168177 0.32023269 0.3521001 0.34550962 0.30052423 0.22709776 0.15022919 0.097547978 0.085962959][0.17454624 0.15495498 0.13440908 0.126194 0.1367953 0.18165232 0.25085026 0.31682485 0.35043317 0.34025788 0.29093653 0.21483476 0.13704795 0.085209861 0.0765569][0.10497694 0.088746354 0.07838463 0.08317975 0.10560109 0.1582913 0.23123011 0.29745662 0.32871991 0.31520319 0.26601672 0.19418706 0.12201312 0.074169412 0.0661064][0.06147131 0.050457187 0.047400914 0.058862183 0.084917828 0.13595533 0.20322016 0.26296434 0.28940529 0.27394924 0.22866879 0.1662596 0.10513885 0.064638391 0.056878541][0.0539314 0.049209937 0.049054179 0.059141893 0.079238586 0.11860589 0.17160285 0.22020869 0.24208318 0.22803746 0.19008465 0.13980909 0.091691725 0.059678946 0.052920029][0.079459772 0.08190725 0.080898553 0.082732759 0.088684469 0.10833997 0.14125942 0.17667176 0.19588369 0.18714397 0.15913694 0.12150463 0.085206136 0.060718685 0.0558995][0.12415108 0.13426635 0.13092649 0.1214955 0.10967923 0.10666326 0.11849518 0.14142609 0.159762 0.15823533 0.14034586 0.11369179 0.086594 0.0680714 0.06612137][0.17839055 0.19699778 0.19281578 0.17400718 0.1465867 0.12341908 0.1164992 0.1275458 0.14340007 0.14605962 0.13448799 0.11590714 0.096817926 0.084378332 0.085675068][0.22697566 0.25341269 0.25091138 0.22802949 0.19200398 0.15602446 0.13532002 0.13504687 0.14491594 0.1466037 0.13655509 0.12369674 0.11320817 0.10831673 0.11258886]]...]
INFO - root - 2017-12-11 08:50:22.171062: step 41710, loss = 0.70, batch loss = 0.64 (14.6 examples/sec; 0.546 sec/batch; 44h:07m:47s remains)
INFO - root - 2017-12-11 08:50:27.685651: step 41720, loss = 0.70, batch loss = 0.64 (14.4 examples/sec; 0.554 sec/batch; 44h:43m:53s remains)
INFO - root - 2017-12-11 08:50:33.351174: step 41730, loss = 0.69, batch loss = 0.63 (14.7 examples/sec; 0.543 sec/batch; 43h:50m:34s remains)
INFO - root - 2017-12-11 08:50:38.428920: step 41740, loss = 0.68, batch loss = 0.62 (14.5 examples/sec; 0.551 sec/batch; 44h:31m:36s remains)
INFO - root - 2017-12-11 08:50:44.000123: step 41750, loss = 0.69, batch loss = 0.63 (13.9 examples/sec; 0.575 sec/batch; 46h:26m:35s remains)
INFO - root - 2017-12-11 08:50:49.539112: step 41760, loss = 0.69, batch loss = 0.63 (14.5 examples/sec; 0.551 sec/batch; 44h:29m:45s remains)
INFO - root - 2017-12-11 08:50:55.007951: step 41770, loss = 0.70, batch loss = 0.65 (15.0 examples/sec; 0.535 sec/batch; 43h:10m:34s remains)
INFO - root - 2017-12-11 08:51:00.517211: step 41780, loss = 0.68, batch loss = 0.62 (15.1 examples/sec; 0.531 sec/batch; 42h:51m:25s remains)
INFO - root - 2017-12-11 08:51:06.076898: step 41790, loss = 0.70, batch loss = 0.64 (14.5 examples/sec; 0.550 sec/batch; 44h:24m:28s remains)
INFO - root - 2017-12-11 08:51:11.640164: step 41800, loss = 0.69, batch loss = 0.63 (14.4 examples/sec; 0.555 sec/batch; 44h:51m:04s remains)
2017-12-11 08:51:12.270870: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.15723997 0.16812554 0.17237 0.17383413 0.17564709 0.18077393 0.1932817 0.20987345 0.22578068 0.23788661 0.24558415 0.24938564 0.24652332 0.23662975 0.21977392][0.19023676 0.199497 0.19711915 0.1875239 0.17559779 0.1669817 0.16828056 0.17924911 0.1977804 0.22150205 0.24722353 0.27000037 0.28179669 0.27975604 0.2634294][0.21435499 0.22202908 0.21391514 0.1938415 0.16785435 0.14425144 0.13234763 0.1346117 0.15118884 0.18054 0.21776883 0.25428683 0.27918398 0.28792953 0.27917829][0.23714332 0.25042787 0.24322677 0.21675572 0.17767796 0.13823327 0.11131093 0.10191381 0.11082511 0.13655119 0.17411986 0.214407 0.24650431 0.26600873 0.27075022][0.25571334 0.28206113 0.28388885 0.25792971 0.21065915 0.15849257 0.11833375 0.097962484 0.097209215 0.11323496 0.1412387 0.1744031 0.20493145 0.23059426 0.24942723][0.25686473 0.29386288 0.30612472 0.28681135 0.24141596 0.18774006 0.14442939 0.12028444 0.11266366 0.11670465 0.12806612 0.14449169 0.16394196 0.18839078 0.21766852][0.23841883 0.27146485 0.28460708 0.27195019 0.23721449 0.19547321 0.16292153 0.14575391 0.13746503 0.13128813 0.12418374 0.11997447 0.12338647 0.14132924 0.17645776][0.22071007 0.23735408 0.24109553 0.2301046 0.20682889 0.18206227 0.16701704 0.1625822 0.15802351 0.14543638 0.12320021 0.10041913 0.08761438 0.095575333 0.12990603][0.225501 0.22953764 0.22357112 0.20928319 0.18918522 0.17238092 0.16736239 0.1708138 0.16867733 0.15166065 0.11921999 0.083459094 0.058098778 0.056039333 0.084142856][0.24450056 0.24934065 0.24197792 0.22414829 0.19908696 0.17794424 0.17036067 0.17254956 0.16966031 0.15125684 0.11595126 0.075507894 0.044158876 0.034114078 0.051889516][0.25390157 0.27047873 0.27026337 0.25240475 0.22107364 0.19188584 0.1778435 0.17729262 0.17606615 0.16202219 0.13101307 0.0921149 0.058144484 0.039756484 0.042840861][0.23421821 0.26318672 0.27218822 0.25755444 0.22475725 0.1928841 0.17728508 0.17833225 0.18336539 0.17902306 0.15833309 0.12660299 0.093973421 0.069138825 0.057463378][0.18402639 0.21876472 0.23437992 0.22525917 0.1980985 0.17235038 0.16307944 0.17132801 0.18656364 0.19546497 0.18897571 0.16809326 0.13961931 0.11042312 0.0861296][0.11943845 0.15316862 0.1716594 0.1684849 0.15049247 0.13517277 0.13548414 0.15290782 0.17962705 0.20306191 0.21235827 0.20379634 0.18075913 0.14892489 0.11550485][0.061294727 0.091528185 0.11158841 0.11436468 0.10562372 0.099733979 0.10755773 0.13168912 0.16698873 0.20204352 0.2249739 0.22806622 0.21168116 0.18076991 0.14466636]]...]
INFO - root - 2017-12-11 08:51:17.726411: step 41810, loss = 0.69, batch loss = 0.63 (13.8 examples/sec; 0.581 sec/batch; 46h:55m:52s remains)
INFO - root - 2017-12-11 08:51:23.113656: step 41820, loss = 0.71, batch loss = 0.65 (14.8 examples/sec; 0.539 sec/batch; 43h:32m:04s remains)
INFO - root - 2017-12-11 08:51:28.568407: step 41830, loss = 0.70, batch loss = 0.64 (15.1 examples/sec; 0.530 sec/batch; 42h:47m:13s remains)
INFO - root - 2017-12-11 08:51:33.783816: step 41840, loss = 0.71, batch loss = 0.65 (14.5 examples/sec; 0.553 sec/batch; 44h:38m:25s remains)
INFO - root - 2017-12-11 08:51:39.317484: step 41850, loss = 0.69, batch loss = 0.63 (14.7 examples/sec; 0.545 sec/batch; 43h:59m:52s remains)
INFO - root - 2017-12-11 08:51:44.727492: step 41860, loss = 0.71, batch loss = 0.65 (15.0 examples/sec; 0.532 sec/batch; 42h:58m:14s remains)
INFO - root - 2017-12-11 08:51:50.237781: step 41870, loss = 0.71, batch loss = 0.65 (14.3 examples/sec; 0.559 sec/batch; 45h:06m:06s remains)
INFO - root - 2017-12-11 08:51:55.855766: step 41880, loss = 0.70, batch loss = 0.64 (14.2 examples/sec; 0.562 sec/batch; 45h:20m:43s remains)
INFO - root - 2017-12-11 08:52:01.331249: step 41890, loss = 0.70, batch loss = 0.64 (14.7 examples/sec; 0.545 sec/batch; 43h:59m:29s remains)
INFO - root - 2017-12-11 08:52:06.791398: step 41900, loss = 0.68, batch loss = 0.62 (14.4 examples/sec; 0.556 sec/batch; 44h:52m:46s remains)
2017-12-11 08:52:07.437336: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.31548548 0.30036059 0.25884914 0.20814748 0.16239512 0.12889817 0.11239466 0.11859986 0.14726728 0.186564 0.21966203 0.24541126 0.27000576 0.29361975 0.30920064][0.30062714 0.29048747 0.25556883 0.21727489 0.18701769 0.16427405 0.15031141 0.15524693 0.18294306 0.22234312 0.2591635 0.29435024 0.32983872 0.35900447 0.37299594][0.28823531 0.28097954 0.25486192 0.23467644 0.22665192 0.2216389 0.21501522 0.21802989 0.23847046 0.26922095 0.3044115 0.34839857 0.39795315 0.43719584 0.45316648][0.28873968 0.28400007 0.26864234 0.26844314 0.28403047 0.29970145 0.30559814 0.31017533 0.32244849 0.34002659 0.36975992 0.42170316 0.48753622 0.539881 0.55851746][0.31690624 0.31457832 0.30653119 0.3191821 0.35099936 0.38454378 0.40634856 0.41704288 0.4232128 0.42647997 0.4461287 0.49852955 0.57258773 0.63245034 0.64856][0.39374423 0.39065641 0.38044143 0.39011583 0.42242506 0.46524996 0.50242394 0.52217811 0.5257237 0.51679635 0.52213049 0.5621205 0.626539 0.6793437 0.68518233][0.50475788 0.50236487 0.48426104 0.47876725 0.4978326 0.53981596 0.58752173 0.61570036 0.61711854 0.59613657 0.5810442 0.59500015 0.63334382 0.6683777 0.66407561][0.60276836 0.605423 0.58216196 0.55994231 0.55991197 0.59179622 0.639924 0.66966349 0.66563749 0.63252348 0.59635621 0.581605 0.590544 0.60780334 0.60036945][0.65240943 0.6670655 0.64893222 0.61749846 0.59771353 0.60898954 0.64310747 0.66470426 0.65383011 0.61507732 0.56785023 0.53427476 0.52105886 0.52423048 0.51782656][0.65048873 0.67842937 0.67080593 0.63781154 0.6029163 0.5915612 0.60583317 0.61541432 0.59945524 0.56097412 0.51036423 0.46526596 0.43495908 0.42544156 0.42120162][0.60695463 0.64458591 0.64981645 0.62502772 0.587495 0.56119847 0.55576587 0.55021936 0.52740341 0.48885864 0.43752676 0.38640571 0.3456333 0.32787874 0.32850617][0.53564906 0.57753837 0.59283143 0.58063614 0.549622 0.51563221 0.4918173 0.46974477 0.44051945 0.40500176 0.36013576 0.31267849 0.27085409 0.25000265 0.25493607][0.45007938 0.48886928 0.50770718 0.50506204 0.48155624 0.44322947 0.40457892 0.36919239 0.33772486 0.31019381 0.2789087 0.24314363 0.20698556 0.18533982 0.18968335][0.37322986 0.40702805 0.42382082 0.42232966 0.39895836 0.35463566 0.30516475 0.26251265 0.23326626 0.21534327 0.1977239 0.17386188 0.14447773 0.12276093 0.12312053][0.31231031 0.34114385 0.35167825 0.34305105 0.31187972 0.26076975 0.207107 0.16607727 0.14388385 0.13475877 0.12611353 0.10917252 0.083974116 0.062009938 0.057511035]]...]
INFO - root - 2017-12-11 08:52:12.984097: step 41910, loss = 0.70, batch loss = 0.64 (14.6 examples/sec; 0.548 sec/batch; 44h:15m:32s remains)
INFO - root - 2017-12-11 08:52:18.519174: step 41920, loss = 0.70, batch loss = 0.64 (14.6 examples/sec; 0.547 sec/batch; 44h:06m:56s remains)
INFO - root - 2017-12-11 08:52:24.064953: step 41930, loss = 0.69, batch loss = 0.63 (14.7 examples/sec; 0.543 sec/batch; 43h:51m:37s remains)
INFO - root - 2017-12-11 08:52:29.275705: step 41940, loss = 0.69, batch loss = 0.63 (14.4 examples/sec; 0.557 sec/batch; 44h:55m:38s remains)
INFO - root - 2017-12-11 08:52:34.793819: step 41950, loss = 0.71, batch loss = 0.65 (14.7 examples/sec; 0.543 sec/batch; 43h:51m:36s remains)
INFO - root - 2017-12-11 08:52:40.282331: step 41960, loss = 0.68, batch loss = 0.62 (14.3 examples/sec; 0.560 sec/batch; 45h:11m:21s remains)
INFO - root - 2017-12-11 08:52:45.841048: step 41970, loss = 0.69, batch loss = 0.63 (14.1 examples/sec; 0.569 sec/batch; 45h:57m:04s remains)
INFO - root - 2017-12-11 08:52:51.363167: step 41980, loss = 0.71, batch loss = 0.65 (15.4 examples/sec; 0.521 sec/batch; 42h:00m:26s remains)
INFO - root - 2017-12-11 08:52:56.946068: step 41990, loss = 0.70, batch loss = 0.64 (14.4 examples/sec; 0.555 sec/batch; 44h:46m:15s remains)
INFO - root - 2017-12-11 08:53:02.485571: step 42000, loss = 0.70, batch loss = 0.65 (14.8 examples/sec; 0.540 sec/batch; 43h:35m:42s remains)
2017-12-11 08:53:03.111215: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.082346089 -0.075792111 -0.065706111 -0.055853702 -0.04832207 -0.043901186 -0.043225717 -0.045616906 -0.051286511 -0.060660738 -0.072723627 -0.08487983 -0.093937442 -0.098298885 -0.09802857][-0.06940341 -0.062435925 -0.047428463 -0.02867482 -0.010475744 0.0045117419 0.012802049 0.013626227 0.0052817692 -0.012535302 -0.036997691 -0.063190155 -0.084484167 -0.097576648 -0.10194054][-0.038464617 -0.033289123 -0.013631687 0.016509429 0.049708772 0.080601573 0.10134686 0.1080498 0.096367329 0.066177957 0.023398586 -0.024027584 -0.064191595 -0.091159612 -0.10315317][0.0059642144 0.011373459 0.038751621 0.083560646 0.13371812 0.1816303 0.21570997 0.22876097 0.21332917 0.1686707 0.10408203 0.030512642 -0.034225248 -0.0799107 -0.10256184][0.058858927 0.068865612 0.10834324 0.17037083 0.23689313 0.29984078 0.34548324 0.36412379 0.345117 0.2863611 0.19959562 0.098040581 0.0054342197 -0.062661424 -0.099005088][0.12832789 0.14493047 0.19441779 0.26743391 0.34237832 0.41451395 0.47009724 0.49652418 0.47822243 0.4096438 0.3035531 0.17466013 0.052951258 -0.03939309 -0.091225885][0.22061855 0.23975456 0.29047164 0.36232653 0.43481147 0.51063669 0.57749176 0.61692029 0.60491604 0.53045964 0.40743575 0.25262353 0.1022939 -0.013606575 -0.080041587][0.32675964 0.345954 0.38891828 0.44872835 0.50981367 0.5844748 0.66210717 0.71560121 0.71065289 0.63067949 0.49231043 0.31541431 0.14128031 0.0071559912 -0.069320053][0.42061916 0.43532282 0.46531856 0.50800365 0.55419207 0.62355363 0.70684111 0.7693792 0.76899618 0.68551683 0.53818852 0.34953639 0.16268104 0.019556016 -0.061178315][0.47445303 0.47857556 0.491557 0.5154897 0.54643077 0.60696197 0.68841648 0.75214124 0.753393 0.67196035 0.5281992 0.34432662 0.16083555 0.020216661 -0.058701433][0.47093007 0.46169111 0.45742813 0.46471754 0.48338681 0.53444666 0.60903931 0.66807455 0.66946268 0.59603316 0.46738252 0.30238259 0.13561732 0.0069067846 -0.064883031][0.40969083 0.38883939 0.37210155 0.36845547 0.37962112 0.42236733 0.48777181 0.53894544 0.53958929 0.4773556 0.37034717 0.23254794 0.090959281 -0.019369477 -0.079896778][0.32375202 0.29729781 0.2747741 0.26509804 0.27018631 0.30199867 0.35312197 0.39239454 0.39159268 0.34234583 0.25944582 0.15157962 0.03826829 -0.050954148 -0.098939791][0.271602 0.24502672 0.21959503 0.20421319 0.20089842 0.21878007 0.25273836 0.27923661 0.27772197 0.24098103 0.17866695 0.094850488 0.0038234827 -0.069509782 -0.10980466][0.28706431 0.25735542 0.22242044 0.19533192 0.18102495 0.18811224 0.21101432 0.2301638 0.2297021 0.20157276 0.1509576 0.079600573 -0.00013449861 -0.066462934 -0.10579679]]...]
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-SGD-lr0.001-clip50-initfromfixqian30k-fixqian
INFO - root - 2017-12-11 08:53:08.695003: step 42010, loss = 0.70, batch loss = 0.64 (14.6 examples/sec; 0.548 sec/batch; 44h:11m:05s remains)
INFO - root - 2017-12-11 08:53:14.160396: step 42020, loss = 0.69, batch loss = 0.64 (14.7 examples/sec; 0.544 sec/batch; 43h:51m:35s remains)
INFO - root - 2017-12-11 08:53:19.271964: step 42030, loss = 0.69, batch loss = 0.64 (15.1 examples/sec; 0.531 sec/batch; 42h:51m:39s remains)
INFO - root - 2017-12-11 08:53:24.817077: step 42040, loss = 0.69, batch loss = 0.63 (14.7 examples/sec; 0.542 sec/batch; 43h:45m:59s remains)
INFO - root - 2017-12-11 08:53:30.287780: step 42050, loss = 0.70, batch loss = 0.64 (14.5 examples/sec; 0.553 sec/batch; 44h:39m:13s remains)
INFO - root - 2017-12-11 08:53:35.863545: step 42060, loss = 0.71, batch loss = 0.65 (14.7 examples/sec; 0.545 sec/batch; 43h:59m:20s remains)
INFO - root - 2017-12-11 08:53:41.358955: step 42070, loss = 0.67, batch loss = 0.61 (14.7 examples/sec; 0.545 sec/batch; 43h:56m:25s remains)
INFO - root - 2017-12-11 08:53:46.816721: step 42080, loss = 0.69, batch loss = 0.63 (14.7 examples/sec; 0.545 sec/batch; 43h:55m:49s remains)
INFO - root - 2017-12-11 08:53:52.257630: step 42090, loss = 0.69, batch loss = 0.63 (14.5 examples/sec; 0.552 sec/batch; 44h:31m:15s remains)
INFO - root - 2017-12-11 08:53:57.815010: step 42100, loss = 0.71, batch loss = 0.65 (14.2 examples/sec; 0.564 sec/batch; 45h:28m:06s remains)
2017-12-11 08:53:58.493758: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.2390535 0.27462247 0.30696315 0.32490429 0.32745132 0.32254294 0.31568563 0.30616513 0.30007154 0.30349916 0.31624317 0.32891327 0.33208504 0.31498018 0.2696653][0.25731954 0.29188943 0.32362276 0.34296119 0.34747431 0.34235084 0.33023205 0.30956972 0.2860896 0.26793092 0.25818607 0.25408211 0.25003371 0.23494749 0.19998851][0.26320747 0.29503822 0.323615 0.34257773 0.349441 0.34600019 0.3301014 0.29936704 0.25893751 0.21806398 0.1837458 0.1604711 0.14740992 0.13464485 0.11352237][0.26931354 0.29902223 0.32421839 0.34152958 0.34950238 0.34814614 0.33055696 0.29338509 0.2412995 0.18342979 0.12964597 0.089804769 0.068534411 0.058820568 0.051376984][0.27970728 0.3096028 0.33281082 0.34854382 0.35638851 0.3565771 0.3398551 0.3026481 0.24833453 0.18382196 0.1199284 0.069496952 0.042393561 0.034756783 0.036126018][0.29393718 0.32328632 0.34586018 0.3624579 0.37160683 0.37391773 0.35976431 0.32714149 0.2782197 0.21642016 0.15254569 0.10002493 0.072188355 0.066461131 0.070829749][0.29921371 0.32709563 0.35000661 0.37079042 0.38467905 0.39049456 0.37840876 0.34944978 0.30677789 0.25164041 0.19528796 0.15023218 0.12934558 0.127774 0.13118528][0.29259884 0.31703272 0.34063575 0.36755422 0.38882837 0.39900154 0.387332 0.35803664 0.31770438 0.26793611 0.22220178 0.19236779 0.18643294 0.1932935 0.19516098][0.2716026 0.29153353 0.31516004 0.34856874 0.37859246 0.39471534 0.38534409 0.35585555 0.31524861 0.26800847 0.23185731 0.21918605 0.23146385 0.24912587 0.2501719][0.24055897 0.255505 0.27872977 0.31833753 0.35785016 0.38190973 0.37871876 0.35325587 0.31396395 0.26805604 0.2383702 0.23940791 0.26648664 0.29181087 0.29064089][0.21005853 0.21904311 0.23982123 0.28248212 0.32898232 0.35987836 0.36415377 0.34650642 0.31343973 0.27280965 0.24993676 0.26088074 0.29666033 0.32430184 0.31838435][0.19285443 0.19677095 0.21200962 0.25040749 0.29517651 0.32538658 0.33295283 0.32344007 0.30191702 0.27564782 0.26723272 0.29049 0.33235964 0.35856131 0.34514028][0.19633785 0.1977807 0.20511267 0.23100212 0.26233524 0.28167495 0.28638092 0.28388888 0.27790204 0.27411756 0.28988254 0.33212808 0.381809 0.40455368 0.38061249][0.21460423 0.21726952 0.21719268 0.22602399 0.23560463 0.23622347 0.23240504 0.23423673 0.24444748 0.26686618 0.31284744 0.37870061 0.43804359 0.45626351 0.41931936][0.23324387 0.2414543 0.2376648 0.23042271 0.21656889 0.19563542 0.17962681 0.18171674 0.20489585 0.25097847 0.32557684 0.41447264 0.48432317 0.49984416 0.45197681]]...]
INFO - root - 2017-12-11 08:54:03.988284: step 42110, loss = 0.70, batch loss = 0.65 (14.6 examples/sec; 0.548 sec/batch; 44h:10m:46s remains)
INFO - root - 2017-12-11 08:54:09.536578: step 42120, loss = 0.70, batch loss = 0.64 (14.5 examples/sec; 0.552 sec/batch; 44h:30m:39s remains)
INFO - root - 2017-12-11 08:54:14.719986: step 42130, loss = 0.70, batch loss = 0.64 (14.0 examples/sec; 0.573 sec/batch; 46h:11m:31s remains)
INFO - root - 2017-12-11 08:54:20.181426: step 42140, loss = 0.71, batch loss = 0.65 (15.2 examples/sec; 0.528 sec/batch; 42h:34m:27s remains)
INFO - root - 2017-12-11 08:54:25.727284: step 42150, loss = 0.68, batch loss = 0.62 (14.3 examples/sec; 0.559 sec/batch; 45h:03m:24s remains)
INFO - root - 2017-12-11 08:54:31.308635: step 42160, loss = 0.70, batch loss = 0.64 (14.5 examples/sec; 0.551 sec/batch; 44h:27m:29s remains)
INFO - root - 2017-12-11 08:54:36.812683: step 42170, loss = 0.68, batch loss = 0.63 (14.5 examples/sec; 0.552 sec/batch; 44h:29m:49s remains)
INFO - root - 2017-12-11 08:54:42.304096: step 42180, loss = 0.70, batch loss = 0.64 (14.6 examples/sec; 0.549 sec/batch; 44h:15m:13s remains)
INFO - root - 2017-12-11 08:54:47.896262: step 42190, loss = 0.69, batch loss = 0.63 (13.7 examples/sec; 0.583 sec/batch; 46h:58m:39s remains)
INFO - root - 2017-12-11 08:54:53.441413: step 42200, loss = 0.70, batch loss = 0.64 (14.6 examples/sec; 0.549 sec/batch; 44h:14m:44s remains)
2017-12-11 08:54:54.082134: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.027646692 -0.02526786 -0.024235386 -0.026530065 -0.029948464 -0.033520106 -0.034732796 -0.034432311 -0.034333933 -0.03530056 -0.035903491 -0.036902148 -0.041438334 -0.047605533 -0.051251885][-0.00011033822 0.012291583 0.019253613 0.018623661 0.015883578 0.012379768 0.011570759 0.010600829 0.0070376843 0.0019164599 -0.0016271647 -0.0059138439 -0.01792134 -0.0348257 -0.04734081][0.044761483 0.075624056 0.095331743 0.10102151 0.10278742 0.10272957 0.1058367 0.10430026 0.094290748 0.080393292 0.068451159 0.055840373 0.029847391 -0.0055532 -0.033734635][0.10581073 0.16289705 0.2010486 0.21560128 0.2232534 0.22803861 0.23650873 0.23358761 0.2138285 0.18673536 0.16183953 0.1356965 0.090348132 0.031647116 -0.01574677][0.17611505 0.26284072 0.32243374 0.34693617 0.35993075 0.36843324 0.3823562 0.37835908 0.34874916 0.30737808 0.26733345 0.22479868 0.15816166 0.0752441 0.0076397634][0.23568238 0.34778491 0.42632121 0.45948958 0.47606325 0.48678803 0.50676411 0.50429261 0.46915391 0.41721827 0.36342239 0.30472749 0.21837085 0.1140506 0.028081262][0.27235067 0.39985749 0.49048671 0.52978116 0.54877192 0.56090516 0.58749533 0.5903278 0.55607516 0.50002855 0.43774012 0.36759362 0.26579449 0.14413956 0.043252602][0.28860706 0.41898119 0.51066375 0.54932654 0.56645411 0.57749778 0.60943973 0.61997831 0.59279239 0.53979939 0.47677451 0.4025839 0.29218227 0.15972944 0.049211077][0.28424269 0.40338361 0.48343888 0.5125159 0.52144426 0.52676117 0.5596233 0.57768983 0.56199831 0.51923597 0.46310446 0.39265081 0.2834174 0.15212178 0.0429487][0.250272 0.34729615 0.40800318 0.42370325 0.42260751 0.42092326 0.45108968 0.47538602 0.47306353 0.444219 0.39897481 0.33777058 0.23929028 0.12135539 0.024393518][0.18951054 0.25959456 0.2988162 0.30226141 0.29363486 0.28637776 0.30995429 0.33497518 0.3417533 0.32488582 0.29080236 0.24164064 0.16079316 0.06592682 -0.0095386133][0.11927811 0.16379623 0.18440427 0.17953639 0.16752186 0.15759316 0.1725671 0.19217329 0.20025153 0.1894442 0.16390571 0.12702626 0.066981532 -0.00042972184 -0.05030993][0.05076462 0.073521145 0.080076523 0.070767656 0.058547419 0.048172727 0.054439053 0.065253712 0.069570273 0.060915872 0.042052492 0.016898206 -0.021486484 -0.060789492 -0.085319139][-0.00981012 -0.0037745668 -0.0060975011 -0.01628761 -0.026378496 -0.035324831 -0.035637207 -0.0329304 -0.03314507 -0.040554386 -0.053438734 -0.067975134 -0.086766466 -0.10165064 -0.10516065][-0.053700808 -0.057489406 -0.0631906 -0.071181506 -0.077782243 -0.084121324 -0.087661758 -0.089871235 -0.092922993 -0.0992438 -0.10766064 -0.11476061 -0.12000788 -0.11893804 -0.10997811]]...]
INFO - root - 2017-12-11 08:54:59.574929: step 42210, loss = 0.68, batch loss = 0.63 (15.0 examples/sec; 0.532 sec/batch; 42h:53m:57s remains)
INFO - root - 2017-12-11 08:55:05.140477: step 42220, loss = 0.69, batch loss = 0.64 (14.6 examples/sec; 0.548 sec/batch; 44h:09m:35s remains)
INFO - root - 2017-12-11 08:55:10.416094: step 42230, loss = 0.69, batch loss = 0.63 (14.3 examples/sec; 0.561 sec/batch; 45h:13m:11s remains)
INFO - root - 2017-12-11 08:55:15.975096: step 42240, loss = 0.69, batch loss = 0.63 (14.5 examples/sec; 0.551 sec/batch; 44h:27m:05s remains)
INFO - root - 2017-12-11 08:55:21.491202: step 42250, loss = 0.69, batch loss = 0.64 (14.0 examples/sec; 0.573 sec/batch; 46h:11m:46s remains)
INFO - root - 2017-12-11 08:55:27.072331: step 42260, loss = 0.70, batch loss = 0.64 (14.5 examples/sec; 0.552 sec/batch; 44h:29m:09s remains)
INFO - root - 2017-12-11 08:55:32.519878: step 42270, loss = 0.69, batch loss = 0.63 (14.7 examples/sec; 0.543 sec/batch; 43h:47m:09s remains)
INFO - root - 2017-12-11 08:55:38.035335: step 42280, loss = 0.70, batch loss = 0.64 (14.2 examples/sec; 0.563 sec/batch; 45h:24m:26s remains)
INFO - root - 2017-12-11 08:55:43.486248: step 42290, loss = 0.70, batch loss = 0.64 (14.5 examples/sec; 0.553 sec/batch; 44h:36m:18s remains)
INFO - root - 2017-12-11 08:55:49.018660: step 42300, loss = 0.70, batch loss = 0.64 (14.6 examples/sec; 0.548 sec/batch; 44h:09m:50s remains)
2017-12-11 08:55:49.627565: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.018143687 0.010589724 0.0012797219 -0.0077844225 -0.016608261 -0.019582096 -0.00746416 0.010295794 0.025691746 0.035322968 0.040592089 0.039548971 0.028576355 0.010033175 -0.007706177][0.088478409 0.080888189 0.068204246 0.054950453 0.039721109 0.030742485 0.042395011 0.063898541 0.084224381 0.095952287 0.10110968 0.09626133 0.076689266 0.045592293 0.016226038][0.1850981 0.17913893 0.16329429 0.14568786 0.12200383 0.10219937 0.10639474 0.12612848 0.14792988 0.16060014 0.16420794 0.15368931 0.12447419 0.080646671 0.039599724][0.27911365 0.27634856 0.25999102 0.24195555 0.21428041 0.18542513 0.17895859 0.19158928 0.20947492 0.21840341 0.21561562 0.19626909 0.15684174 0.10249525 0.052143227][0.3482388 0.34911451 0.335463 0.32260454 0.29981118 0.270499 0.2562077 0.25906751 0.26710144 0.26594126 0.25049618 0.21722764 0.16647589 0.10450562 0.048633777][0.38443056 0.38876882 0.37985003 0.37573153 0.36535445 0.34532192 0.3297469 0.324704 0.3213113 0.30731741 0.27566788 0.22515939 0.16176783 0.093743347 0.035180368][0.39066398 0.39607522 0.39021677 0.39344496 0.39766151 0.39315602 0.3835794 0.37538746 0.36310285 0.33807772 0.2914249 0.2238733 0.14842273 0.076275781 0.017870545][0.37970191 0.37988746 0.37088656 0.375196 0.38999236 0.40209374 0.40403163 0.39974055 0.38576713 0.35644746 0.30138025 0.2223184 0.13824753 0.063487478 0.0063432087][0.36790285 0.35608858 0.33710241 0.33451164 0.35096389 0.37379345 0.38792792 0.39290506 0.38487077 0.35884419 0.30372047 0.22240162 0.1371832 0.0630524 0.0070493473][0.36473292 0.33791628 0.30452192 0.28847602 0.295872 0.31831181 0.3392126 0.35394827 0.35561556 0.33842817 0.29131147 0.21850498 0.14138792 0.072530746 0.018263109][0.35574183 0.31653675 0.2698237 0.23955317 0.23277657 0.24675007 0.26880309 0.29179654 0.30437043 0.29883409 0.26475248 0.20776235 0.1449751 0.084581837 0.032789644][0.31980363 0.27286249 0.21872197 0.18011384 0.16285805 0.16837005 0.1896138 0.21912619 0.24133191 0.24649133 0.2253217 0.18512757 0.13796717 0.087497681 0.039935075][0.24498308 0.1977997 0.14610463 0.10942674 0.090923637 0.093452625 0.11445767 0.14771108 0.17570002 0.18762651 0.17606437 0.14972302 0.11620604 0.075640708 0.034447022][0.13668436 0.09812136 0.059980638 0.035708085 0.025384985 0.031182747 0.053074822 0.08648774 0.11480783 0.12771954 0.12020764 0.10200077 0.07800547 0.046766434 0.014328801][0.023287684 -0.0013206044 -0.021019194 -0.029276559 -0.02790609 -0.016203549 0.0063836649 0.036864974 0.061621293 0.071953878 0.065070063 0.050542902 0.032397162 0.0092593441 -0.013596519]]...]
INFO - root - 2017-12-11 08:55:55.182040: step 42310, loss = 0.69, batch loss = 0.63 (14.6 examples/sec; 0.548 sec/batch; 44h:08m:51s remains)
INFO - root - 2017-12-11 08:56:00.351248: step 42320, loss = 0.68, batch loss = 0.62 (32.0 examples/sec; 0.250 sec/batch; 20h:08m:38s remains)
INFO - root - 2017-12-11 08:56:05.762051: step 42330, loss = 0.70, batch loss = 0.64 (14.2 examples/sec; 0.565 sec/batch; 45h:30m:20s remains)
INFO - root - 2017-12-11 08:56:11.371861: step 42340, loss = 0.69, batch loss = 0.64 (14.6 examples/sec; 0.548 sec/batch; 44h:08m:01s remains)
INFO - root - 2017-12-11 08:56:16.844586: step 42350, loss = 0.71, batch loss = 0.65 (14.4 examples/sec; 0.557 sec/batch; 44h:54m:43s remains)
INFO - root - 2017-12-11 08:56:22.412462: step 42360, loss = 0.70, batch loss = 0.64 (14.0 examples/sec; 0.570 sec/batch; 45h:54m:10s remains)
INFO - root - 2017-12-11 08:56:27.884979: step 42370, loss = 0.70, batch loss = 0.64 (14.7 examples/sec; 0.545 sec/batch; 43h:54m:31s remains)
INFO - root - 2017-12-11 08:56:33.400542: step 42380, loss = 0.70, batch loss = 0.64 (14.4 examples/sec; 0.557 sec/batch; 44h:54m:57s remains)
INFO - root - 2017-12-11 08:56:38.947204: step 42390, loss = 0.69, batch loss = 0.63 (14.1 examples/sec; 0.566 sec/batch; 45h:36m:38s remains)
INFO - root - 2017-12-11 08:56:44.517300: step 42400, loss = 0.70, batch loss = 0.64 (14.5 examples/sec; 0.552 sec/batch; 44h:30m:33s remains)
2017-12-11 08:56:45.136047: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.25039381 0.23378409 0.19069849 0.15497929 0.14398108 0.14706917 0.15306693 0.17606102 0.22221111 0.2787925 0.30685854 0.29343995 0.24586935 0.16810818 0.074376218][0.35155448 0.34392264 0.30442697 0.27147412 0.26139787 0.26557982 0.26997688 0.29351136 0.3430295 0.40393642 0.42834058 0.39944094 0.32915708 0.22677982 0.10895006][0.418006 0.42833126 0.40303639 0.37678674 0.36460257 0.36520308 0.36386997 0.38403231 0.43133253 0.4918392 0.51088071 0.46879128 0.38105604 0.26171678 0.12908164][0.445699 0.47715914 0.47149527 0.454001 0.43938577 0.43488276 0.42794052 0.44904816 0.50154305 0.56699419 0.58523208 0.53618973 0.43879643 0.30703196 0.15868035][0.48387161 0.53634191 0.54966062 0.54270542 0.5319683 0.52836972 0.51935697 0.54471523 0.60771757 0.68098831 0.6970365 0.63940579 0.5341742 0.38989878 0.21900569][0.54417044 0.61282229 0.64066184 0.64460611 0.64347625 0.64615762 0.63762444 0.66467303 0.73297989 0.8049733 0.80797482 0.73200548 0.61443305 0.45783409 0.26763591][0.60738164 0.69022828 0.73139405 0.74744046 0.75778097 0.76570851 0.75331765 0.77022523 0.82678074 0.881428 0.86069912 0.76106119 0.62884623 0.46427467 0.2696909][0.643528 0.72945136 0.7757774 0.79723662 0.8116625 0.81761086 0.79545546 0.79450023 0.82988173 0.86321718 0.82398278 0.71138382 0.57471883 0.4136838 0.23092464][0.61776429 0.69183475 0.73026991 0.74540222 0.75107205 0.7444315 0.70853961 0.68958819 0.704625 0.7222265 0.679825 0.576322 0.45544738 0.31540197 0.16013761][0.51730317 0.56725454 0.58886516 0.589695 0.57900172 0.55646843 0.51135719 0.4821752 0.48269382 0.48946887 0.45363241 0.37451467 0.28470919 0.1798591 0.0641084][0.32823887 0.35114387 0.35636353 0.3449499 0.32190511 0.29131463 0.24929221 0.22294156 0.21852332 0.21888775 0.19270973 0.14173579 0.087500729 0.02368545 -0.045991693][0.11470421 0.11734293 0.11232019 0.094981872 0.067455292 0.036816675 0.0045537911 -0.012363235 -0.013633206 -0.01262492 -0.026731335 -0.052429475 -0.076390788 -0.10470697 -0.1341427][-0.044870708 -0.05486555 -0.065862805 -0.084277071 -0.10933179 -0.13417085 -0.15501378 -0.16172615 -0.1563684 -0.14991687 -0.15166566 -0.15875594 -0.16297811 -0.1684282 -0.17221755][-0.12940627 -0.14385855 -0.15583108 -0.1708816 -0.18867739 -0.20489676 -0.21584669 -0.21578504 -0.20664546 -0.19660614 -0.19072063 -0.1877144 -0.18286179 -0.17791912 -0.17036729][-0.1593098 -0.17270283 -0.18140495 -0.19033094 -0.199468 -0.20711343 -0.21112728 -0.2086291 -0.20044047 -0.19139925 -0.18405958 -0.17832693 -0.1716532 -0.16401732 -0.15360811]]...]
INFO - root - 2017-12-11 08:56:50.608394: step 42410, loss = 0.70, batch loss = 0.65 (15.2 examples/sec; 0.527 sec/batch; 42h:28m:48s remains)
INFO - root - 2017-12-11 08:56:55.831508: step 42420, loss = 0.70, batch loss = 0.64 (13.8 examples/sec; 0.579 sec/batch; 46h:37m:48s remains)
INFO - root - 2017-12-11 08:57:01.411172: step 42430, loss = 0.70, batch loss = 0.64 (14.7 examples/sec; 0.543 sec/batch; 43h:44m:52s remains)
INFO - root - 2017-12-11 08:57:06.860084: step 42440, loss = 0.69, batch loss = 0.63 (14.1 examples/sec; 0.567 sec/batch; 45h:39m:24s remains)
INFO - root - 2017-12-11 08:57:12.414762: step 42450, loss = 0.70, batch loss = 0.64 (14.5 examples/sec; 0.552 sec/batch; 44h:26m:44s remains)
INFO - root - 2017-12-11 08:57:17.986990: step 42460, loss = 0.68, batch loss = 0.63 (14.8 examples/sec; 0.540 sec/batch; 43h:28m:11s remains)
INFO - root - 2017-12-11 08:57:23.545172: step 42470, loss = 0.67, batch loss = 0.61 (14.5 examples/sec; 0.552 sec/batch; 44h:30m:07s remains)
INFO - root - 2017-12-11 08:57:29.140482: step 42480, loss = 0.70, batch loss = 0.64 (14.4 examples/sec; 0.557 sec/batch; 44h:51m:39s remains)
INFO - root - 2017-12-11 08:57:34.638376: step 42490, loss = 0.68, batch loss = 0.63 (14.8 examples/sec; 0.542 sec/batch; 43h:37m:40s remains)
INFO - root - 2017-12-11 08:57:40.064428: step 42500, loss = 0.70, batch loss = 0.64 (15.1 examples/sec; 0.529 sec/batch; 42h:37m:50s remains)
2017-12-11 08:57:40.688348: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.1656591 0.16526301 0.1475208 0.12242016 0.10155674 0.088251516 0.08125902 0.07756722 0.082355291 0.099931471 0.12195051 0.13582256 0.13280827 0.11293565 0.083446331][0.21571468 0.22135936 0.2056836 0.18068324 0.15868863 0.14190415 0.12752682 0.11230403 0.10525417 0.11586186 0.13836218 0.15750198 0.1601515 0.14271142 0.11050884][0.24200249 0.25430581 0.24443693 0.22535206 0.20874274 0.19455838 0.17698292 0.15117271 0.12887248 0.1258852 0.14272702 0.16459349 0.17448841 0.16332154 0.13260591][0.25582442 0.273533 0.27035597 0.26018628 0.25397041 0.24907759 0.23511676 0.20350052 0.16656648 0.14619794 0.1517176 0.17211448 0.18781547 0.18402724 0.1579169][0.2580981 0.27716866 0.2779164 0.27599728 0.28251627 0.29271889 0.29021427 0.26042843 0.21362321 0.17593919 0.16603157 0.17990167 0.19819258 0.2014844 0.18265958][0.24291937 0.25873649 0.26082191 0.26648042 0.28654149 0.31410939 0.32723629 0.30554149 0.25552922 0.20433149 0.1773373 0.17957127 0.19579887 0.20507288 0.19663346][0.21634403 0.22688481 0.22992653 0.24258763 0.27326107 0.31279969 0.33666742 0.32241437 0.27348787 0.215105 0.17521068 0.16679621 0.18062727 0.19649152 0.20095128][0.19156715 0.1990467 0.20413376 0.22140884 0.25475189 0.29411292 0.31758314 0.30570412 0.26079252 0.20394768 0.16056815 0.14764185 0.16154206 0.18439326 0.20092325][0.18218978 0.1892473 0.19569992 0.2124549 0.23930302 0.26734596 0.2810511 0.26689216 0.22732651 0.17810507 0.13922352 0.1277101 0.14361124 0.17203769 0.19690679][0.18474321 0.19311005 0.19952708 0.21142519 0.22698686 0.24008982 0.24181245 0.22398017 0.18880032 0.14684485 0.11325146 0.10420825 0.12265556 0.15610972 0.18739341][0.18003929 0.19151729 0.19887173 0.2071711 0.21493077 0.21843994 0.21275853 0.19262627 0.15948427 0.12025318 0.087683171 0.078134283 0.097473681 0.13547055 0.17398179][0.16204138 0.17680344 0.18621457 0.19474354 0.20234954 0.20618156 0.20124896 0.18195425 0.14881028 0.10804821 0.073304094 0.062172584 0.082561016 0.12552917 0.17235088][0.13098016 0.14682226 0.15807217 0.16971874 0.18331541 0.19574201 0.19970824 0.18715124 0.15741234 0.11748765 0.083152167 0.072926879 0.095090017 0.14105314 0.19257444][0.10548104 0.11838527 0.12850872 0.14142556 0.16013041 0.18174742 0.1968707 0.19502948 0.17429337 0.14220294 0.1155238 0.11231647 0.13941377 0.18754612 0.23976065][0.0997564 0.10630654 0.11156072 0.12195992 0.14131071 0.16770355 0.19078907 0.19888045 0.1891719 0.16938154 0.15583518 0.16471949 0.20022988 0.25137636 0.30244702]]...]
INFO - root - 2017-12-11 08:57:46.199048: step 42510, loss = 0.71, batch loss = 0.65 (14.7 examples/sec; 0.545 sec/batch; 43h:54m:44s remains)
INFO - root - 2017-12-11 08:57:51.229497: step 42520, loss = 0.69, batch loss = 0.63 (14.3 examples/sec; 0.561 sec/batch; 45h:10m:23s remains)
INFO - root - 2017-12-11 08:57:56.871878: step 42530, loss = 0.69, batch loss = 0.63 (14.3 examples/sec; 0.561 sec/batch; 45h:08m:57s remains)
INFO - root - 2017-12-11 08:58:02.318253: step 42540, loss = 0.70, batch loss = 0.64 (14.6 examples/sec; 0.548 sec/batch; 44h:10m:12s remains)
INFO - root - 2017-12-11 08:58:07.749270: step 42550, loss = 0.71, batch loss = 0.65 (14.9 examples/sec; 0.536 sec/batch; 43h:09m:12s remains)
INFO - root - 2017-12-11 08:58:13.318858: step 42560, loss = 0.69, batch loss = 0.64 (13.8 examples/sec; 0.579 sec/batch; 46h:37m:19s remains)
INFO - root - 2017-12-11 08:58:18.897510: step 42570, loss = 0.70, batch loss = 0.64 (14.2 examples/sec; 0.563 sec/batch; 45h:21m:47s remains)
INFO - root - 2017-12-11 08:58:24.408494: step 42580, loss = 0.69, batch loss = 0.63 (14.8 examples/sec; 0.542 sec/batch; 43h:39m:03s remains)
INFO - root - 2017-12-11 08:58:29.973435: step 42590, loss = 0.69, batch loss = 0.63 (14.6 examples/sec; 0.549 sec/batch; 44h:14m:01s remains)
INFO - root - 2017-12-11 08:58:35.503858: step 42600, loss = 0.69, batch loss = 0.63 (14.5 examples/sec; 0.552 sec/batch; 44h:27m:51s remains)
2017-12-11 08:58:36.135314: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.035697412 -0.028017079 -0.014663013 0.0013494716 0.018210895 0.029509095 0.031749908 0.027779173 0.018635191 0.0055569867 -0.0034339679 -0.00082216412 0.0092445388 0.019609027 0.026638243][-0.029832553 -0.017447513 0.004171201 0.02981578 0.056184169 0.074364111 0.080336824 0.077804282 0.0672282 0.049744569 0.036082782 0.037498187 0.04955944 0.062717929 0.07119631][-0.022053823 -0.0038543702 0.027140331 0.062943928 0.098636955 0.1239256 0.13489154 0.1355629 0.12498089 0.10408396 0.085469179 0.084126048 0.096004307 0.11007669 0.11829495][-0.01624194 0.0070113414 0.046120349 0.090308771 0.13341157 0.1652125 0.18222183 0.18763734 0.1784558 0.15641195 0.13441734 0.12886821 0.1373169 0.14866757 0.15376207][-0.014548321 0.01215902 0.057715386 0.10910195 0.15858166 0.1962513 0.21898304 0.22869818 0.2205154 0.19837007 0.17438407 0.16399644 0.16625947 0.17201692 0.17239015][-0.015564461 0.013321 0.064423174 0.12364067 0.18033974 0.22386326 0.25143018 0.26443231 0.25696138 0.23540658 0.21061999 0.19577879 0.19078495 0.18908294 0.1833404][-0.016861146 0.013920483 0.070445888 0.1384376 0.20325425 0.25262868 0.28451893 0.30116463 0.296019 0.27610341 0.25139463 0.23273207 0.22025973 0.20950623 0.19522603][-0.018612519 0.013159593 0.073958494 0.15042131 0.22384931 0.28033313 0.31843203 0.34101093 0.3407402 0.3233467 0.29868105 0.2762289 0.25631738 0.23562656 0.21079792][-0.020587662 0.011186752 0.074415393 0.15781361 0.24008518 0.30545279 0.35236946 0.383453 0.390617 0.37662485 0.35199055 0.32639685 0.30045509 0.27106392 0.23541008][-0.022008935 0.0088880546 0.071531594 0.15749696 0.24492115 0.31640276 0.36969942 0.40735489 0.42147505 0.41109398 0.38722718 0.36064109 0.332478 0.29874676 0.25535694][-0.023945726 0.0044224095 0.06178461 0.14252011 0.22662571 0.29647982 0.3488262 0.38653845 0.40371469 0.39655465 0.37515959 0.35056469 0.32425666 0.29083952 0.24449593][-0.029386243 -0.0066502383 0.03946428 0.10550652 0.17535591 0.23360232 0.2763252 0.30672875 0.32190117 0.3166869 0.29890403 0.27791777 0.25563738 0.22635506 0.18362568][-0.039175961 -0.025433958 0.0045148088 0.048794735 0.09646263 0.1358306 0.16294611 0.18113938 0.1905673 0.18609452 0.1722623 0.15563533 0.1389062 0.11754773 0.085700363][-0.04894948 -0.044703491 -0.031235041 -0.0096220346 0.014299839 0.033014983 0.043357432 0.04871491 0.051342815 0.046971597 0.037011705 0.025462808 0.015385374 0.0037588722 -0.013907533][-0.055866182 -0.05936465 -0.058821265 -0.055212654 -0.050472341 -0.048248928 -0.050534561 -0.05445553 -0.057248566 -0.061779492 -0.068535268 -0.075425372 -0.079766087 -0.083281472 -0.088809974]]...]
INFO - root - 2017-12-11 08:58:41.643284: step 42610, loss = 0.71, batch loss = 0.65 (14.3 examples/sec; 0.558 sec/batch; 44h:57m:30s remains)
INFO - root - 2017-12-11 08:58:46.875435: step 42620, loss = 0.69, batch loss = 0.63 (14.8 examples/sec; 0.540 sec/batch; 43h:27m:22s remains)
INFO - root - 2017-12-11 08:58:52.350932: step 42630, loss = 0.70, batch loss = 0.64 (14.9 examples/sec; 0.535 sec/batch; 43h:06m:56s remains)
INFO - root - 2017-12-11 08:58:57.810519: step 42640, loss = 0.70, batch loss = 0.64 (14.6 examples/sec; 0.549 sec/batch; 44h:12m:32s remains)
INFO - root - 2017-12-11 08:59:03.417251: step 42650, loss = 0.70, batch loss = 0.64 (14.0 examples/sec; 0.571 sec/batch; 45h:58m:52s remains)
INFO - root - 2017-12-11 08:59:09.010054: step 42660, loss = 0.69, batch loss = 0.63 (13.8 examples/sec; 0.578 sec/batch; 46h:32m:43s remains)
INFO - root - 2017-12-11 08:59:14.518384: step 42670, loss = 0.70, batch loss = 0.64 (14.8 examples/sec; 0.541 sec/batch; 43h:33m:10s remains)
INFO - root - 2017-12-11 08:59:20.040733: step 42680, loss = 0.69, batch loss = 0.63 (14.4 examples/sec; 0.554 sec/batch; 44h:34m:21s remains)
INFO - root - 2017-12-11 08:59:25.451218: step 42690, loss = 0.68, batch loss = 0.63 (14.9 examples/sec; 0.538 sec/batch; 43h:17m:56s remains)
INFO - root - 2017-12-11 08:59:30.972993: step 42700, loss = 0.70, batch loss = 0.64 (14.9 examples/sec; 0.538 sec/batch; 43h:17m:20s remains)
2017-12-11 08:59:31.518915: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.17505695 0.1751042 0.18890755 0.21047783 0.22704783 0.23414423 0.24040739 0.25028735 0.26107094 0.26362777 0.25129682 0.23232158 0.222376 0.2353905 0.26880336][0.19532028 0.20146137 0.2230954 0.25373882 0.2771745 0.28517362 0.28548238 0.28664541 0.291586 0.2967672 0.29475173 0.28907326 0.28937709 0.30552444 0.33394679][0.22907127 0.2405754 0.26435629 0.29465646 0.31402418 0.31437469 0.30384377 0.29485005 0.29499871 0.30504528 0.31614438 0.32658407 0.33869559 0.3561413 0.37445217][0.27212068 0.28723764 0.30845448 0.33184448 0.34101018 0.3308011 0.31045595 0.29442063 0.2917574 0.3051199 0.32424691 0.34384012 0.36028779 0.37227333 0.37615684][0.29557559 0.31457865 0.33396089 0.35161489 0.3535651 0.33888963 0.3178561 0.30364355 0.30229115 0.31510741 0.33174184 0.34663963 0.35461935 0.35370111 0.34269112][0.28613892 0.31194767 0.33432919 0.35216537 0.35463771 0.3448697 0.33283657 0.32810879 0.33122119 0.33935177 0.34364015 0.34162426 0.3310464 0.312959 0.28954798][0.24752885 0.28197476 0.31096363 0.33364931 0.34247547 0.34342989 0.34430042 0.34951797 0.35468972 0.35303646 0.3393209 0.31686094 0.28914008 0.26043 0.23416921][0.19871254 0.23801456 0.2716161 0.297821 0.31199342 0.32201681 0.33314866 0.34434906 0.34792075 0.3352989 0.30655107 0.27102271 0.23635668 0.20785815 0.18736851][0.1581143 0.19416058 0.22570476 0.24973154 0.26423061 0.27849862 0.29586512 0.31058797 0.31264791 0.29433754 0.26125428 0.22603041 0.19631752 0.176211 0.16540879][0.1324688 0.15389398 0.17474513 0.19040525 0.20089747 0.21601519 0.23704743 0.25479871 0.25844216 0.24281038 0.21713969 0.19408789 0.17915332 0.17380735 0.17466649][0.11955709 0.11898895 0.1230566 0.12683548 0.1317092 0.1466503 0.17061961 0.19279963 0.20264062 0.19711582 0.185988 0.18028525 0.18377759 0.19450006 0.20560533][0.11044105 0.087585032 0.074507631 0.068076372 0.069655962 0.08528959 0.11161192 0.13803226 0.15530808 0.16191246 0.16635758 0.17793337 0.19956715 0.22574809 0.24583134][0.092048466 0.054839078 0.03020246 0.018100789 0.018741177 0.034594476 0.061366633 0.090671815 0.11554241 0.13521576 0.15612034 0.18485399 0.22238617 0.26050061 0.28671947][0.059052188 0.01948227 -0.00722609 -0.01986596 -0.019134797 -0.0043606721 0.021376038 0.052933756 0.0860454 0.12014119 0.15829755 0.20205401 0.24920426 0.29129946 0.31801489][0.0152304 -0.016164917 -0.036744706 -0.046238605 -0.045149181 -0.032187868 -0.0080609284 0.025385644 0.066292 0.11341736 0.16542113 0.21781681 0.26558036 0.30274341 0.32378519]]...]
INFO - root - 2017-12-11 08:59:36.649254: step 42710, loss = 0.68, batch loss = 0.63 (14.9 examples/sec; 0.538 sec/batch; 43h:20m:14s remains)
INFO - root - 2017-12-11 08:59:42.213336: step 42720, loss = 0.71, batch loss = 0.65 (14.8 examples/sec; 0.539 sec/batch; 43h:22m:18s remains)
INFO - root - 2017-12-11 08:59:47.709483: step 42730, loss = 0.68, batch loss = 0.62 (14.8 examples/sec; 0.540 sec/batch; 43h:28m:31s remains)
INFO - root - 2017-12-11 08:59:53.249563: step 42740, loss = 0.70, batch loss = 0.64 (14.2 examples/sec; 0.562 sec/batch; 45h:12m:59s remains)
INFO - root - 2017-12-11 08:59:58.801650: step 42750, loss = 0.70, batch loss = 0.64 (14.5 examples/sec; 0.550 sec/batch; 44h:15m:32s remains)
INFO - root - 2017-12-11 09:00:04.300178: step 42760, loss = 0.70, batch loss = 0.64 (14.3 examples/sec; 0.561 sec/batch; 45h:09m:38s remains)
INFO - root - 2017-12-11 09:00:09.783843: step 42770, loss = 0.69, batch loss = 0.63 (14.3 examples/sec; 0.560 sec/batch; 45h:04m:43s remains)
INFO - root - 2017-12-11 09:00:15.323363: step 42780, loss = 0.70, batch loss = 0.64 (14.3 examples/sec; 0.561 sec/batch; 45h:10m:38s remains)
INFO - root - 2017-12-11 09:00:20.815368: step 42790, loss = 0.71, batch loss = 0.65 (14.9 examples/sec; 0.538 sec/batch; 43h:18m:36s remains)
INFO - root - 2017-12-11 09:00:26.238948: step 42800, loss = 0.69, batch loss = 0.63 (14.7 examples/sec; 0.543 sec/batch; 43h:42m:37s remains)
2017-12-11 09:00:26.853183: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.24728435 0.2276216 0.20658623 0.18329889 0.15112834 0.11222146 0.073586658 0.043562412 0.031566173 0.045422778 0.081920013 0.12918159 0.17063084 0.2003054 0.21687147][0.25890911 0.2270484 0.19448575 0.16585028 0.13534749 0.10415386 0.076829933 0.058214892 0.054819535 0.072565332 0.10832323 0.15194178 0.18769042 0.20968455 0.21721759][0.25051636 0.21081 0.16914263 0.13884629 0.11575378 0.098819435 0.087897889 0.083271578 0.088853307 0.10791413 0.13691667 0.16832693 0.19052242 0.20076525 0.19896653][0.22100154 0.17816193 0.13526089 0.11165323 0.10430398 0.10850943 0.1175304 0.12672937 0.13819575 0.15395494 0.17032881 0.18297587 0.18676209 0.18334393 0.17338392][0.17532317 0.13515685 0.1005458 0.091370367 0.10472993 0.13193773 0.16008253 0.17997818 0.19214119 0.1999491 0.20098411 0.1943997 0.18077259 0.16522047 0.14894775][0.12608266 0.095095783 0.075966433 0.08440952 0.11705642 0.16256583 0.20429553 0.22949708 0.23703402 0.23329678 0.21854419 0.19580773 0.17006432 0.14804536 0.12962116][0.07859157 0.06148098 0.061142787 0.086952262 0.13396885 0.18955655 0.23648439 0.26085114 0.26087841 0.24552223 0.21843529 0.18640253 0.15696624 0.13608408 0.12056376][0.040205736 0.036102362 0.051960442 0.092110492 0.14839378 0.20682025 0.25118735 0.26927233 0.26103613 0.2373257 0.20438612 0.17088105 0.14495708 0.13024731 0.12020945][0.019437814 0.023431275 0.049445007 0.09881191 0.15947951 0.21560422 0.25216982 0.26047748 0.24438779 0.21660163 0.18478392 0.15669289 0.13815495 0.13018379 0.12422664][0.0199561 0.026776291 0.055522941 0.10717715 0.16697428 0.21792234 0.24587607 0.24559647 0.22471257 0.19727698 0.17119062 0.15128352 0.13991612 0.13545351 0.12978861][0.034129586 0.04188934 0.07006491 0.11935607 0.17400202 0.21771309 0.2380027 0.23239158 0.21057604 0.18663475 0.16757126 0.15470649 0.14731038 0.14186154 0.13234055][0.048470888 0.058018755 0.086195819 0.13173686 0.17950825 0.21609421 0.23207209 0.22667094 0.2085841 0.18948747 0.17475517 0.16351093 0.15380435 0.14210947 0.12481188][0.055352904 0.06729693 0.096223667 0.13840394 0.1800918 0.21124086 0.22559857 0.22350766 0.21100228 0.1956079 0.18078806 0.16528445 0.14802551 0.12736107 0.10188945][0.048667669 0.06231752 0.0915181 0.13036433 0.16681275 0.19341964 0.20626414 0.20645548 0.1976544 0.18349296 0.16602893 0.14482984 0.12035558 0.093037188 0.063102707][0.028217774 0.04189717 0.068889819 0.10270885 0.13372715 0.15622535 0.16707641 0.16754787 0.15998375 0.14550404 0.12535872 0.10030704 0.072174422 0.043136638 0.014723637]]...]
INFO - root - 2017-12-11 09:00:32.046989: step 42810, loss = 0.69, batch loss = 0.64 (14.5 examples/sec; 0.550 sec/batch; 44h:16m:35s remains)
INFO - root - 2017-12-11 09:00:37.558776: step 42820, loss = 0.69, batch loss = 0.64 (14.4 examples/sec; 0.554 sec/batch; 44h:36m:48s remains)
INFO - root - 2017-12-11 09:00:43.025765: step 42830, loss = 0.70, batch loss = 0.64 (14.3 examples/sec; 0.561 sec/batch; 45h:09m:39s remains)
INFO - root - 2017-12-11 09:00:48.523924: step 42840, loss = 0.72, batch loss = 0.66 (14.4 examples/sec; 0.555 sec/batch; 44h:40m:02s remains)
INFO - root - 2017-12-11 09:00:54.070418: step 42850, loss = 0.69, batch loss = 0.63 (14.3 examples/sec; 0.559 sec/batch; 44h:56m:48s remains)
INFO - root - 2017-12-11 09:00:59.592974: step 42860, loss = 0.71, batch loss = 0.65 (14.2 examples/sec; 0.564 sec/batch; 45h:21m:47s remains)
INFO - root - 2017-12-11 09:01:05.107399: step 42870, loss = 0.69, batch loss = 0.63 (14.7 examples/sec; 0.545 sec/batch; 43h:51m:43s remains)
INFO - root - 2017-12-11 09:01:10.538442: step 42880, loss = 0.70, batch loss = 0.64 (14.6 examples/sec; 0.549 sec/batch; 44h:08m:39s remains)
INFO - root - 2017-12-11 09:01:16.081615: step 42890, loss = 0.70, batch loss = 0.64 (14.5 examples/sec; 0.554 sec/batch; 44h:31m:49s remains)
INFO - root - 2017-12-11 09:01:21.625551: step 42900, loss = 0.69, batch loss = 0.63 (14.4 examples/sec; 0.556 sec/batch; 44h:45m:30s remains)
2017-12-11 09:01:22.312682: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.08243572 -0.063819095 -0.05553256 -0.063671961 -0.083076738 -0.10777716 -0.12134545 -0.1156621 -0.096865162 -0.077266119 -0.064718843 -0.061737537 -0.068568334 -0.082307748 -0.097516209][-0.049219705 -0.03107645 -0.029297566 -0.043919209 -0.065845594 -0.089822762 -0.10444588 -0.10668731 -0.10252365 -0.098081119 -0.096794635 -0.097577758 -0.10067824 -0.10587239 -0.11090893][0.053361062 0.068531804 0.061207578 0.039277386 0.016619796 -0.0023180333 -0.013389742 -0.020447429 -0.030432474 -0.045947559 -0.064615257 -0.079275325 -0.085924253 -0.08697097 -0.08537516][0.20612189 0.21670075 0.19896968 0.16965015 0.14799826 0.13770273 0.13418734 0.12535197 0.10271596 0.065269232 0.019834165 -0.019424977 -0.041837305 -0.049480312 -0.049836926][0.36554056 0.37355834 0.34829476 0.31511202 0.29815474 0.29966068 0.30494362 0.29291761 0.25572655 0.19570726 0.12303474 0.056289181 0.011386075 -0.010834893 -0.019451341][0.49927536 0.50831205 0.48087919 0.44839078 0.4379659 0.45007411 0.4603163 0.44010946 0.3854281 0.30559304 0.21330263 0.12675777 0.062580004 0.024743555 0.0057174838][0.592046 0.605358 0.58099478 0.5506863 0.54241168 0.55745316 0.56647027 0.53652662 0.46758553 0.376111 0.2758328 0.18141147 0.10603967 0.055257954 0.025396898][0.63090259 0.64850384 0.62798011 0.59598565 0.57946515 0.585882 0.58998495 0.55826336 0.48944736 0.40099442 0.30521604 0.21334223 0.13402264 0.073532157 0.032861497][0.60575479 0.62358993 0.60349452 0.56514114 0.53288919 0.52377862 0.52395475 0.50264281 0.45076749 0.37850899 0.29536891 0.21178816 0.13389549 0.068766706 0.021480607][0.52623582 0.53345567 0.50683349 0.45990419 0.41359982 0.39199752 0.39269918 0.38915294 0.36237916 0.31240475 0.2455616 0.17358947 0.10291815 0.041211572 -0.0036978305][0.42137721 0.40855053 0.36819875 0.31339845 0.26118183 0.23559967 0.24042718 0.25418243 0.25111496 0.22252674 0.17257185 0.11480437 0.057237346 0.0076764226 -0.025285006][0.30969527 0.27708918 0.22422205 0.16776861 0.12068889 0.1000524 0.11024981 0.13501841 0.14782606 0.13543732 0.10173844 0.060964622 0.021356199 -0.010299156 -0.026202669][0.19839594 0.155986 0.10162212 0.053658132 0.02080808 0.01091244 0.026272302 0.05539979 0.075825818 0.074417405 0.056415081 0.034522034 0.01456239 0.0012414093 0.0016701814][0.091522448 0.052955773 0.011312081 -0.019545689 -0.034588061 -0.032258563 -0.012721165 0.01687178 0.040209007 0.04740382 0.04479954 0.041430764 0.038530014 0.038577396 0.049055096][0.0097617023 -0.016982339 -0.041365776 -0.056097947 -0.057730328 -0.046268564 -0.023853531 0.0051650661 0.030367941 0.045304757 0.055780467 0.066563539 0.074594848 0.080438845 0.091938466]]...]
INFO - root - 2017-12-11 09:01:27.656142: step 42910, loss = 0.70, batch loss = 0.64 (14.3 examples/sec; 0.558 sec/batch; 44h:54m:33s remains)
INFO - root - 2017-12-11 09:01:33.150470: step 42920, loss = 0.70, batch loss = 0.64 (14.6 examples/sec; 0.547 sec/batch; 44h:01m:50s remains)
INFO - root - 2017-12-11 09:01:38.641203: step 42930, loss = 0.71, batch loss = 0.65 (15.0 examples/sec; 0.533 sec/batch; 42h:52m:37s remains)
INFO - root - 2017-12-11 09:01:44.113217: step 42940, loss = 0.72, batch loss = 0.66 (14.3 examples/sec; 0.559 sec/batch; 44h:55m:22s remains)
INFO - root - 2017-12-11 09:01:49.713197: step 42950, loss = 0.69, batch loss = 0.63 (14.0 examples/sec; 0.569 sec/batch; 45h:48m:12s remains)
INFO - root - 2017-12-11 09:01:55.194296: step 42960, loss = 0.70, batch loss = 0.65 (14.5 examples/sec; 0.550 sec/batch; 44h:13m:50s remains)
INFO - root - 2017-12-11 09:02:00.727318: step 42970, loss = 0.71, batch loss = 0.65 (14.3 examples/sec; 0.558 sec/batch; 44h:52m:36s remains)
INFO - root - 2017-12-11 09:02:06.216408: step 42980, loss = 0.70, batch loss = 0.64 (14.6 examples/sec; 0.548 sec/batch; 44h:03m:45s remains)
INFO - root - 2017-12-11 09:02:11.674915: step 42990, loss = 0.70, batch loss = 0.64 (14.9 examples/sec; 0.536 sec/batch; 43h:07m:20s remains)
INFO - root - 2017-12-11 09:02:17.130240: step 43000, loss = 0.68, batch loss = 0.62 (19.6 examples/sec; 0.408 sec/batch; 32h:50m:00s remains)
2017-12-11 09:02:17.658635: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.29764795 0.25843984 0.25885934 0.29095981 0.32570818 0.34418097 0.34377927 0.32487732 0.2775197 0.19510935 0.093051136 0.00079285435 -0.057803616 -0.084169582 -0.090553515][0.31736282 0.28283882 0.28349969 0.31394053 0.3438873 0.35673085 0.35664383 0.34870961 0.31912157 0.25012216 0.14975071 0.048019778 -0.024181139 -0.0631749 -0.080213629][0.30287826 0.27973181 0.28757837 0.32289779 0.35627821 0.37329105 0.38016349 0.38440371 0.36971575 0.31065467 0.2095376 0.096388295 0.00834964 -0.045067 -0.071831033][0.26707011 0.25150511 0.26786414 0.31464162 0.3624455 0.39573628 0.41740218 0.43323934 0.42526984 0.36553127 0.25643703 0.1295511 0.026367113 -0.039061297 -0.07141269][0.21670155 0.2029279 0.22552665 0.2862947 0.35606742 0.41578585 0.45996797 0.48795167 0.47843605 0.40613058 0.2808094 0.13882856 0.024034575 -0.04769263 -0.079643928][0.15433583 0.14068213 0.16823789 0.24313398 0.3374944 0.42866689 0.50124478 0.54454291 0.53073096 0.43913332 0.2916137 0.13279225 0.0087193381 -0.065139122 -0.092723511][0.098673373 0.082692176 0.11215087 0.19802615 0.31481472 0.43659523 0.53812516 0.59780449 0.58031392 0.46970513 0.30068842 0.1268907 -0.00427623 -0.078764074 -0.10220153][0.077633284 0.048557073 0.068497695 0.15431981 0.28449222 0.42980367 0.55632806 0.63231581 0.61637104 0.49481872 0.31118768 0.12678668 -0.00905072 -0.083872646 -0.10567691][0.089769356 0.039049219 0.037496079 0.11085114 0.24189647 0.39887464 0.54128981 0.6298672 0.6201725 0.49854168 0.31152925 0.12515943 -0.0098543437 -0.082812577 -0.10445672][0.11759718 0.0450788 0.016772103 0.068311267 0.18719658 0.3407228 0.48503485 0.57785523 0.57583815 0.46506804 0.28923064 0.11359809 -0.012251149 -0.079629831 -0.1008385][0.14321037 0.054771602 0.0012596665 0.026734354 0.12359708 0.26031461 0.39405465 0.48381445 0.49071622 0.40083137 0.24993736 0.0963398 -0.014339059 -0.0745068 -0.095483124][0.17505473 0.074461341 -0.0032682 -0.0070376284 0.060989 0.17220227 0.28730085 0.36917773 0.38423178 0.31842563 0.19833772 0.072255179 -0.020261308 -0.072064631 -0.091761693][0.23387313 0.11777602 0.011284173 -0.026652081 0.0079490859 0.090077482 0.18439966 0.25739723 0.27875444 0.23355812 0.14137529 0.041326609 -0.033537775 -0.076334335 -0.0926157][0.31295574 0.17724864 0.040825304 -0.028953787 -0.023947438 0.032849193 0.10901964 0.17375267 0.19779143 0.16559394 0.093219914 0.012545279 -0.049128063 -0.084437087 -0.096143939][0.38878641 0.23686647 0.077028438 -0.014513816 -0.028399996 0.011932522 0.075266041 0.13204755 0.15337394 0.12487284 0.062296655 -0.0077633155 -0.062039029 -0.092156544 -0.099354953]]...]
INFO - root - 2017-12-11 09:02:23.210381: step 43010, loss = 0.70, batch loss = 0.64 (14.6 examples/sec; 0.548 sec/batch; 44h:05m:26s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-SGD-lr0.001-clip50-initfromfixqian30k-fixqian
INFO - root - 2017-12-11 09:02:28.648425: step 43020, loss = 0.71, batch loss = 0.65 (14.8 examples/sec; 0.540 sec/batch; 43h:24m:54s remains)
INFO - root - 2017-12-11 09:02:34.156578: step 43030, loss = 0.70, batch loss = 0.64 (14.2 examples/sec; 0.562 sec/batch; 45h:09m:17s remains)
INFO - root - 2017-12-11 09:02:39.749488: step 43040, loss = 0.71, batch loss = 0.65 (14.3 examples/sec; 0.561 sec/batch; 45h:05m:50s remains)
INFO - root - 2017-12-11 09:02:45.288424: step 43050, loss = 0.69, batch loss = 0.63 (14.2 examples/sec; 0.562 sec/batch; 45h:10m:28s remains)
INFO - root - 2017-12-11 09:02:50.763261: step 43060, loss = 0.68, batch loss = 0.63 (14.4 examples/sec; 0.556 sec/batch; 44h:43m:23s remains)
INFO - root - 2017-12-11 09:02:56.323309: step 43070, loss = 0.70, batch loss = 0.64 (14.6 examples/sec; 0.550 sec/batch; 44h:11m:10s remains)
INFO - root - 2017-12-11 09:03:01.832607: step 43080, loss = 0.70, batch loss = 0.64 (14.5 examples/sec; 0.553 sec/batch; 44h:28m:45s remains)
INFO - root - 2017-12-11 09:03:07.340232: step 43090, loss = 0.71, batch loss = 0.66 (14.2 examples/sec; 0.562 sec/batch; 45h:08m:59s remains)
INFO - root - 2017-12-11 09:03:12.529137: step 43100, loss = 0.69, batch loss = 0.63 (14.7 examples/sec; 0.546 sec/batch; 43h:53m:22s remains)
2017-12-11 09:03:13.137206: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.090741649 -0.092761382 -0.09247075 -0.091284886 -0.089425914 -0.087628171 -0.086598635 -0.086597905 -0.087618358 -0.088894494 -0.0893318 -0.088461943 -0.086253576 -0.083423048 -0.080717139][-0.09511216 -0.097021684 -0.096234992 -0.094516851 -0.092729986 -0.092000395 -0.092852972 -0.095101826 -0.098216318 -0.10079648 -0.10128807 -0.099079691 -0.094478257 -0.088565014 -0.082786821][-0.091229878 -0.088878252 -0.082831554 -0.076055259 -0.071430013 -0.071584366 -0.076962881 -0.086247027 -0.097362891 -0.10729106 -0.11279723 -0.11247008 -0.10679044 -0.09769927 -0.0878823][-0.074168541 -0.060573269 -0.040122554 -0.017217539 0.0012563344 0.0083888276 0.0017336913 -0.016837258 -0.043742832 -0.07288979 -0.096510932 -0.10931329 -0.11083869 -0.10423496 -0.0936516][-0.046822507 -0.015460593 0.030474832 0.085365117 0.13584654 0.16626872 0.16802451 0.1411074 0.0902108 0.02691791 -0.031988144 -0.073347062 -0.094296396 -0.099558137 -0.094966613][-0.018361935 0.033989407 0.11390882 0.21503815 0.31513739 0.38585389 0.40774512 0.37535596 0.29394904 0.18395334 0.075688116 -0.0069267964 -0.057437859 -0.082588017 -0.090329118][0.0031005479 0.074316531 0.18743467 0.33575174 0.48847079 0.60476434 0.65310967 0.62063748 0.51200467 0.35654715 0.19831058 0.071931764 -0.011451775 -0.05952695 -0.082233496][0.011171479 0.092956088 0.22644214 0.40428767 0.59082025 0.73837078 0.80768728 0.78020048 0.65892655 0.477873 0.28860825 0.13228509 0.024555504 -0.041371964 -0.07598003][0.00045651247 0.079213373 0.21112847 0.38824087 0.57548523 0.72641814 0.80165821 0.78135061 0.66654879 0.48970512 0.30108967 0.14194672 0.029981095 -0.039780688 -0.076894946][-0.027782289 0.0331712 0.14026344 0.28604546 0.44132185 0.56777042 0.63244963 0.61805159 0.52423137 0.37752229 0.21987545 0.086634934 -0.0059008794 -0.061407991 -0.08848127][-0.061709635 -0.026925325 0.041926809 0.13898607 0.24398685 0.33011252 0.37403104 0.36321175 0.29692721 0.19402571 0.084584735 -0.0056581022 -0.064405583 -0.094822496 -0.10460986][-0.088001773 -0.077778563 -0.046242595 0.002642029 0.0574429 0.10241006 0.12382914 0.11475778 0.075229205 0.016708188 -0.043014079 -0.08886411 -0.11376647 -0.12056985 -0.11540312][-0.10088317 -0.10699384 -0.10166223 -0.087315619 -0.068877906 -0.053950656 -0.049114197 -0.056907702 -0.076271854 -0.10120887 -0.12335809 -0.13622816 -0.13737473 -0.12945049 -0.11667572][-0.10035785 -0.11372441 -0.12137897 -0.12493373 -0.12558568 -0.12615661 -0.12916039 -0.13492884 -0.14212564 -0.14814082 -0.14998302 -0.14601737 -0.13644877 -0.12362365 -0.11017977][-0.091968521 -0.10565086 -0.11597586 -0.12454081 -0.13114713 -0.1362666 -0.14047258 -0.14341119 -0.144259 -0.14243913 -0.13752836 -0.12976189 -0.11991455 -0.10957775 -0.10012713]]...]
INFO - root - 2017-12-11 09:03:18.690292: step 43110, loss = 0.68, batch loss = 0.62 (14.5 examples/sec; 0.551 sec/batch; 44h:17m:41s remains)
INFO - root - 2017-12-11 09:03:24.102500: step 43120, loss = 0.70, batch loss = 0.64 (14.8 examples/sec; 0.542 sec/batch; 43h:32m:22s remains)
INFO - root - 2017-12-11 09:03:29.627056: step 43130, loss = 0.69, batch loss = 0.63 (13.6 examples/sec; 0.590 sec/batch; 47h:23m:33s remains)
INFO - root - 2017-12-11 09:03:35.161733: step 43140, loss = 0.70, batch loss = 0.65 (14.7 examples/sec; 0.543 sec/batch; 43h:37m:22s remains)
INFO - root - 2017-12-11 09:03:40.629183: step 43150, loss = 0.70, batch loss = 0.64 (13.9 examples/sec; 0.574 sec/batch; 46h:05m:55s remains)
INFO - root - 2017-12-11 09:03:45.849217: step 43160, loss = 0.69, batch loss = 0.63 (32.2 examples/sec; 0.248 sec/batch; 19h:58m:04s remains)
INFO - root - 2017-12-11 09:03:48.583454: step 43170, loss = 0.70, batch loss = 0.64 (30.4 examples/sec; 0.263 sec/batch; 21h:07m:54s remains)
INFO - root - 2017-12-11 09:03:51.214223: step 43180, loss = 0.71, batch loss = 0.65 (31.2 examples/sec; 0.256 sec/batch; 20h:35m:43s remains)
INFO - root - 2017-12-11 09:03:53.845079: step 43190, loss = 0.69, batch loss = 0.63 (31.2 examples/sec; 0.256 sec/batch; 20h:36m:37s remains)
INFO - root - 2017-12-11 09:03:56.484585: step 43200, loss = 0.70, batch loss = 0.64 (30.2 examples/sec; 0.265 sec/batch; 21h:17m:42s remains)
2017-12-11 09:03:56.850183: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.14221241 0.1289351 0.10903045 0.090636753 0.079104021 0.07445807 0.071549341 0.068474919 0.070410363 0.080582775 0.094116561 0.10346699 0.10595074 0.10841709 0.11313491][0.14257884 0.12950541 0.10915548 0.092378609 0.084935293 0.084755242 0.084833793 0.083826005 0.0866733 0.094529577 0.10209796 0.10432981 0.10131198 0.09920682 0.098698273][0.13562553 0.12737632 0.11208521 0.10068537 0.098053545 0.10109001 0.10315806 0.10464818 0.10866811 0.11278807 0.1116429 0.10448143 0.09519539 0.088235363 0.082996555][0.12935656 0.125665 0.11603196 0.11007344 0.11040223 0.11385795 0.11607645 0.11909414 0.12329781 0.12316192 0.11473096 0.10082383 0.087784305 0.078101918 0.070557855][0.12292496 0.12116295 0.11499587 0.11272515 0.11435569 0.11697352 0.11867703 0.1219136 0.12455613 0.12068681 0.10952678 0.095174208 0.082952268 0.072446577 0.06300281][0.1168117 0.11378861 0.10772429 0.10643433 0.10830539 0.11038128 0.11200462 0.11506122 0.11549912 0.10887504 0.098410048 0.088235691 0.079716116 0.069280908 0.057771519][0.11638504 0.11018313 0.10150829 0.098602727 0.10005803 0.10268186 0.10569878 0.10983178 0.11003533 0.10330749 0.095111728 0.089007847 0.082548745 0.0709178 0.057269413][0.12153581 0.11193021 0.099157445 0.093177915 0.094181724 0.098504931 0.10367249 0.10953426 0.11124615 0.10652716 0.099820837 0.09418267 0.08614707 0.072499745 0.0584456][0.13255771 0.11929939 0.10184833 0.092042804 0.091862716 0.09759593 0.10478481 0.1123742 0.11606936 0.11358155 0.106855 0.0989337 0.088237017 0.07466168 0.063474528][0.14586708 0.1305355 0.11008631 0.097268678 0.09483505 0.099326976 0.10565597 0.11247066 0.11635356 0.11433972 0.10603476 0.094945855 0.082728431 0.072133042 0.0666083][0.15209229 0.13786143 0.1178629 0.10512631 0.10106922 0.10265102 0.10626236 0.11095405 0.11327852 0.10883356 0.0965244 0.081513382 0.068416491 0.0615345 0.061953254][0.14416243 0.1340953 0.11993414 0.11307761 0.11172141 0.11262508 0.11473734 0.11759654 0.1165553 0.10603031 0.086261339 0.065188862 0.049952839 0.045303497 0.050172918][0.12680422 0.12098554 0.11427164 0.11540483 0.11954215 0.1231567 0.12666765 0.12883088 0.12325422 0.10447466 0.075343587 0.046577025 0.027264988 0.022763597 0.030420335][0.10950726 0.10570621 0.10397061 0.11070324 0.11995067 0.12790117 0.13435422 0.13618998 0.12624672 0.10057996 0.0646726 0.030835787 0.0087936306 0.0042160437 0.013112457][0.092674226 0.089347005 0.09107291 0.10255024 0.11756483 0.13125938 0.14108133 0.14250384 0.12939307 0.10049227 0.063430458 0.030730993 0.010818703 0.007685387 0.015517254]]...]
INFO - root - 2017-12-11 09:03:59.454041: step 43210, loss = 0.69, batch loss = 0.64 (31.1 examples/sec; 0.257 sec/batch; 20h:38m:17s remains)
INFO - root - 2017-12-11 09:04:02.091879: step 43220, loss = 0.69, batch loss = 0.63 (31.9 examples/sec; 0.251 sec/batch; 20h:08m:48s remains)
INFO - root - 2017-12-11 09:04:04.686995: step 43230, loss = 0.69, batch loss = 0.64 (30.2 examples/sec; 0.265 sec/batch; 21h:17m:37s remains)
INFO - root - 2017-12-11 09:04:07.357989: step 43240, loss = 0.70, batch loss = 0.64 (28.1 examples/sec; 0.285 sec/batch; 22h:54m:35s remains)
INFO - root - 2017-12-11 09:04:09.953752: step 43250, loss = 0.71, batch loss = 0.66 (31.0 examples/sec; 0.258 sec/batch; 20h:43m:49s remains)
INFO - root - 2017-12-11 09:04:12.570308: step 43260, loss = 0.70, batch loss = 0.65 (30.8 examples/sec; 0.260 sec/batch; 20h:53m:55s remains)
INFO - root - 2017-12-11 09:04:15.205512: step 43270, loss = 0.70, batch loss = 0.64 (30.1 examples/sec; 0.266 sec/batch; 21h:22m:05s remains)
INFO - root - 2017-12-11 09:04:17.882226: step 43280, loss = 0.69, batch loss = 0.64 (28.9 examples/sec; 0.277 sec/batch; 22h:13m:00s remains)
INFO - root - 2017-12-11 09:04:20.505551: step 43290, loss = 0.70, batch loss = 0.64 (29.0 examples/sec; 0.276 sec/batch; 22h:10m:56s remains)
INFO - root - 2017-12-11 09:04:23.188209: step 43300, loss = 0.70, batch loss = 0.64 (29.7 examples/sec; 0.269 sec/batch; 21h:36m:38s remains)
2017-12-11 09:04:23.568630: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.16869719 0.16261469 0.16443406 0.17143974 0.17767607 0.18221439 0.18823259 0.19842593 0.21613951 0.24000511 0.26345626 0.27830267 0.27936542 0.26558477 0.23609567][0.19476227 0.18765521 0.18646103 0.19263746 0.20097834 0.20998588 0.2212974 0.23518376 0.25205526 0.26988593 0.285092 0.29375282 0.29352936 0.28324696 0.25961852][0.19596921 0.1885553 0.18533489 0.1925673 0.20620151 0.22415122 0.24459596 0.26285568 0.27402112 0.2758795 0.27062145 0.26265436 0.25588831 0.25036234 0.23966324][0.19496384 0.18583409 0.18101691 0.19225258 0.21599424 0.24847259 0.2825627 0.30693689 0.30958778 0.28811592 0.25268763 0.21961205 0.20238371 0.20266601 0.20932341][0.20084132 0.18683952 0.17934832 0.19548547 0.23193169 0.28239384 0.33396608 0.36820337 0.36513847 0.32131824 0.25539353 0.19726317 0.17128995 0.17936303 0.20370151][0.20681255 0.18622307 0.17468488 0.19431168 0.24212255 0.30915737 0.37783539 0.42428902 0.42175442 0.36485022 0.27899826 0.2057889 0.17814393 0.19596201 0.23419616][0.20565191 0.18154939 0.16666324 0.18567169 0.23703384 0.31134084 0.38929346 0.44495729 0.44782627 0.39078212 0.30235511 0.22968377 0.2086475 0.23587874 0.28153273][0.20411454 0.18331513 0.16795331 0.18069749 0.2231639 0.28913948 0.36184138 0.41770887 0.42646652 0.37997404 0.30501908 0.24647695 0.23654231 0.26950467 0.31655997][0.21036333 0.20017561 0.18754521 0.19089176 0.2150003 0.25864223 0.31125584 0.35606444 0.36802748 0.33943102 0.28992611 0.25437334 0.25525245 0.28701013 0.32924542][0.22024572 0.22249874 0.21543099 0.21236524 0.21885805 0.23702545 0.2632449 0.29006839 0.30205652 0.2941553 0.27588412 0.26688695 0.27662659 0.30108488 0.33231667][0.22572696 0.23713927 0.23683912 0.23399508 0.23228142 0.23379034 0.23915119 0.24890903 0.25812089 0.26604426 0.27372134 0.28597665 0.29989409 0.3123517 0.32714424][0.2211017 0.23792598 0.2445785 0.24747618 0.24702476 0.24250501 0.23550536 0.23171458 0.23482761 0.24884835 0.27046278 0.29404756 0.30717787 0.3058967 0.3021684][0.19517405 0.21460506 0.22756217 0.23827632 0.2437567 0.23985007 0.22749057 0.21478809 0.2107555 0.22275597 0.24624 0.27046344 0.27781853 0.26332524 0.24326728][0.13630822 0.15519732 0.17178799 0.18803181 0.198473 0.19674556 0.18339354 0.16715118 0.15886357 0.16667436 0.1857201 0.20402478 0.20352438 0.17961866 0.14972489][0.051915728 0.065646335 0.081190966 0.097710319 0.10901646 0.1083707 0.096185446 0.080625236 0.071740143 0.07618425 0.089080237 0.099589191 0.09301848 0.06638404 0.0357784]]...]
INFO - root - 2017-12-11 09:04:26.185497: step 43310, loss = 0.69, batch loss = 0.63 (30.5 examples/sec; 0.262 sec/batch; 21h:02m:53s remains)
INFO - root - 2017-12-11 09:04:28.870430: step 43320, loss = 0.69, batch loss = 0.64 (29.9 examples/sec; 0.268 sec/batch; 21h:31m:19s remains)
INFO - root - 2017-12-11 09:04:31.539118: step 43330, loss = 0.71, batch loss = 0.65 (31.5 examples/sec; 0.254 sec/batch; 20h:23m:32s remains)
INFO - root - 2017-12-11 09:04:34.148884: step 43340, loss = 0.68, batch loss = 0.63 (29.8 examples/sec; 0.268 sec/batch; 21h:32m:34s remains)
INFO - root - 2017-12-11 09:04:36.777970: step 43350, loss = 0.70, batch loss = 0.64 (30.5 examples/sec; 0.262 sec/batch; 21h:04m:38s remains)
INFO - root - 2017-12-11 09:04:39.427477: step 43360, loss = 0.71, batch loss = 0.66 (31.3 examples/sec; 0.256 sec/batch; 20h:32m:25s remains)
INFO - root - 2017-12-11 09:04:41.998584: step 43370, loss = 0.73, batch loss = 0.67 (31.3 examples/sec; 0.255 sec/batch; 20h:30m:40s remains)
INFO - root - 2017-12-11 09:04:44.669600: step 43380, loss = 0.69, batch loss = 0.64 (30.3 examples/sec; 0.264 sec/batch; 21h:10m:45s remains)
INFO - root - 2017-12-11 09:04:47.319267: step 43390, loss = 0.69, batch loss = 0.64 (30.0 examples/sec; 0.267 sec/batch; 21h:25m:03s remains)
INFO - root - 2017-12-11 09:04:50.016798: step 43400, loss = 0.71, batch loss = 0.65 (30.0 examples/sec; 0.267 sec/batch; 21h:26m:41s remains)
2017-12-11 09:04:50.386514: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.12160965 0.1680844 0.22113068 0.27359787 0.32933277 0.38585833 0.42400306 0.41958666 0.36558375 0.28119606 0.18784077 0.10479449 0.044935234 0.012876512 0.011116837][0.13936453 0.19423135 0.25753322 0.31986895 0.38218126 0.43989852 0.47498792 0.46472615 0.4017477 0.30468494 0.19784079 0.104182 0.038853325 0.0054577333 0.004875496][0.14575483 0.20329767 0.27416268 0.34513858 0.41132417 0.46407405 0.48744789 0.46342832 0.38868952 0.28269106 0.1720145 0.080908388 0.022074761 -0.0042499467 -2.0988466e-05][0.13550419 0.19415061 0.27389425 0.35712641 0.4300805 0.47757173 0.48454192 0.43992379 0.34882849 0.23435341 0.12522398 0.044008538 -0.0015319519 -0.016218927 -0.0057467273][0.1197653 0.17811729 0.26594025 0.36323762 0.44691417 0.49353835 0.48692873 0.42262733 0.31546864 0.19360411 0.087445453 0.017201295 -0.014584122 -0.016842423 0.001087143][0.10861168 0.16410606 0.25540808 0.363717 0.45887026 0.5096584 0.49636093 0.41863447 0.29978067 0.17332292 0.070780367 0.010052056 -0.0096531911 0.00076057436 0.028355906][0.10869994 0.1595861 0.24934609 0.36300838 0.46696937 0.5241881 0.51048893 0.42711574 0.30291805 0.17569344 0.077274911 0.023726786 0.012873802 0.033941172 0.071214415][0.11879871 0.16394311 0.24769092 0.36024946 0.46864313 0.53202641 0.52173418 0.438761 0.31462383 0.18983532 0.096318319 0.048409931 0.043271769 0.071602434 0.11630586][0.12872334 0.16757098 0.24281889 0.3498944 0.45942608 0.52894121 0.52568829 0.44850469 0.32894692 0.20914522 0.12152123 0.078410871 0.076476559 0.10862537 0.15767704][0.12651516 0.15859006 0.22404188 0.32196823 0.42839187 0.50242579 0.509204 0.44375324 0.33439556 0.2230549 0.1426059 0.10439783 0.1052986 0.1409435 0.19497523][0.10046022 0.12522532 0.17997174 0.26501545 0.36179823 0.43433681 0.44948602 0.39925233 0.30659223 0.21000041 0.14056095 0.10884181 0.11341825 0.15384814 0.21566546][0.05284135 0.070474088 0.1140377 0.18289134 0.26314521 0.32601479 0.34414154 0.3091872 0.23810731 0.16227666 0.10756198 0.083597936 0.092398085 0.13910569 0.212976][-0.0057529146 0.0051657413 0.036702927 0.0866687 0.14515379 0.19170748 0.20735078 0.18594953 0.13888876 0.0878782 0.051072285 0.036778733 0.051346369 0.10606758 0.19554707][-0.059463639 -0.054765873 -0.034664884 -0.0028308907 0.033893123 0.062986456 0.073954135 0.063796215 0.039381579 0.012717036 -0.0062861741 -0.010890489 0.0092775505 0.070608482 0.17337556][-0.093082339 -0.0933581 -0.083380193 -0.067496449 -0.0498344 -0.036342423 -0.030791989 -0.033659663 -0.04144232 -0.049699686 -0.055141464 -0.052527927 -0.029170129 0.034285143 0.14307284]]...]
INFO - root - 2017-12-11 09:04:53.120400: step 43410, loss = 0.69, batch loss = 0.63 (28.4 examples/sec; 0.282 sec/batch; 22h:37m:10s remains)
INFO - root - 2017-12-11 09:04:55.714991: step 43420, loss = 0.71, batch loss = 0.65 (30.0 examples/sec; 0.267 sec/batch; 21h:25m:44s remains)
INFO - root - 2017-12-11 09:04:58.376151: step 43430, loss = 0.69, batch loss = 0.63 (30.0 examples/sec; 0.267 sec/batch; 21h:25m:42s remains)
INFO - root - 2017-12-11 09:05:01.046836: step 43440, loss = 0.70, batch loss = 0.64 (30.1 examples/sec; 0.266 sec/batch; 21h:22m:20s remains)
INFO - root - 2017-12-11 09:05:03.656587: step 43450, loss = 0.70, batch loss = 0.64 (29.6 examples/sec; 0.270 sec/batch; 21h:42m:35s remains)
INFO - root - 2017-12-11 09:05:06.269805: step 43460, loss = 0.72, batch loss = 0.66 (31.3 examples/sec; 0.256 sec/batch; 20h:31m:45s remains)
INFO - root - 2017-12-11 09:05:08.984132: step 43470, loss = 0.70, batch loss = 0.64 (28.4 examples/sec; 0.282 sec/batch; 22h:36m:45s remains)
INFO - root - 2017-12-11 09:05:11.623010: step 43480, loss = 0.70, batch loss = 0.64 (29.9 examples/sec; 0.267 sec/batch; 21h:27m:10s remains)
INFO - root - 2017-12-11 09:05:14.256391: step 43490, loss = 0.72, batch loss = 0.66 (30.1 examples/sec; 0.266 sec/batch; 21h:20m:01s remains)
INFO - root - 2017-12-11 09:05:16.886810: step 43500, loss = 0.70, batch loss = 0.64 (31.0 examples/sec; 0.258 sec/batch; 20h:44m:59s remains)
2017-12-11 09:05:17.260877: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.015545182 0.011799389 0.031055944 0.035807908 0.032134585 0.028793497 0.025056634 0.019937344 0.011177969 -0.00470665 -0.032463636 -0.07073456 -0.10715165 -0.12990522 -0.13686216][0.062967591 0.11722448 0.15674992 0.17143227 0.17184071 0.17353864 0.17323871 0.16773179 0.15258963 0.12353402 0.073304735 0.0033785668 -0.065790176 -0.11409126 -0.13551258][0.165542 0.25017112 0.31319579 0.34155622 0.35068142 0.36362135 0.37325513 0.37179253 0.35010195 0.30356961 0.22328068 0.11298802 0.0025122988 -0.078256316 -0.11880841][0.28411642 0.39331266 0.47250503 0.50906187 0.52536732 0.551328 0.57672256 0.58620512 0.56145 0.49658969 0.38357911 0.23099156 0.077683538 -0.037317071 -0.098866582][0.40131682 0.521607 0.6033628 0.63949704 0.65937465 0.69746327 0.74143165 0.7660594 0.74126869 0.659465 0.5169605 0.32921776 0.14160721 -0.00087881472 -0.080182649][0.50649637 0.62487674 0.69897211 0.73227662 0.75794369 0.8095029 0.87269914 0.91335028 0.88989174 0.79250515 0.62518132 0.41057369 0.1975428 0.033368073 -0.06167287][0.5864374 0.68982708 0.74712437 0.77645546 0.811021 0.87744057 0.95887053 1.0148745 0.99495429 0.88727885 0.70370787 0.47328335 0.24499066 0.065618075 -0.043028932][0.6201365 0.6995225 0.73490179 0.75774449 0.79839855 0.87391984 0.96529639 1.0293297 1.0120616 0.90150541 0.71526796 0.4858824 0.25899059 0.0783669 -0.03450612][0.59006596 0.6437211 0.658836 0.67452335 0.71610659 0.79195148 0.88210833 0.945254 0.93135488 0.82811809 0.65497255 0.44444039 0.23664974 0.069839329 -0.036922853][0.48929974 0.52172244 0.5250966 0.53821939 0.57863343 0.64833 0.72775245 0.78252238 0.77187186 0.683985 0.53717178 0.36037746 0.18653147 0.046115976 -0.045524493][0.33601058 0.35353598 0.35378447 0.36830416 0.4065209 0.46686918 0.53132963 0.57453781 0.5662635 0.49714544 0.38288516 0.24662171 0.11400591 0.007217255 -0.062995724][0.16015731 0.16457354 0.16247413 0.17451642 0.20439436 0.25041658 0.29807803 0.32995072 0.32527807 0.27785018 0.19982792 0.10770871 0.020370988 -0.047795765 -0.090945117][0.00072892004 -0.0046621342 -0.0094268089 -0.0037176553 0.012982222 0.040482536 0.0696868 0.089874007 0.088599414 0.062479205 0.0195425 -0.029938916 -0.073736057 -0.10421645 -0.11991408][-0.10766489 -0.11861971 -0.1254001 -0.12581035 -0.12055306 -0.10882929 -0.0948145 -0.083765149 -0.081776492 -0.090652741 -0.10630625 -0.12340343 -0.13530491 -0.13904184 -0.13588247][-0.1606995 -0.1751308 -0.18354616 -0.18777491 -0.18895921 -0.18666905 -0.18239036 -0.1777963 -0.17481436 -0.17398582 -0.17364895 -0.17165181 -0.16527283 -0.15437104 -0.1414448]]...]
INFO - root - 2017-12-11 09:05:19.918705: step 43510, loss = 0.72, batch loss = 0.66 (30.0 examples/sec; 0.267 sec/batch; 21h:24m:12s remains)
INFO - root - 2017-12-11 09:05:22.589569: step 43520, loss = 0.70, batch loss = 0.64 (31.0 examples/sec; 0.258 sec/batch; 20h:42m:34s remains)
INFO - root - 2017-12-11 09:05:25.307798: step 43530, loss = 0.69, batch loss = 0.63 (27.0 examples/sec; 0.296 sec/batch; 23h:45m:35s remains)
INFO - root - 2017-12-11 09:05:27.937970: step 43540, loss = 0.70, batch loss = 0.64 (30.8 examples/sec; 0.260 sec/batch; 20h:51m:30s remains)
INFO - root - 2017-12-11 09:05:30.578994: step 43550, loss = 0.69, batch loss = 0.63 (29.4 examples/sec; 0.272 sec/batch; 21h:50m:20s remains)
INFO - root - 2017-12-11 09:05:33.187079: step 43560, loss = 0.70, batch loss = 0.64 (31.2 examples/sec; 0.256 sec/batch; 20h:33m:35s remains)
INFO - root - 2017-12-11 09:05:35.875502: step 43570, loss = 0.70, batch loss = 0.64 (31.2 examples/sec; 0.256 sec/batch; 20h:33m:03s remains)
INFO - root - 2017-12-11 09:05:38.519100: step 43580, loss = 0.68, batch loss = 0.62 (30.2 examples/sec; 0.265 sec/batch; 21h:14m:46s remains)
INFO - root - 2017-12-11 09:05:41.134057: step 43590, loss = 0.71, batch loss = 0.65 (29.8 examples/sec; 0.268 sec/batch; 21h:32m:12s remains)
INFO - root - 2017-12-11 09:05:43.730561: step 43600, loss = 0.70, batch loss = 0.64 (29.8 examples/sec; 0.268 sec/batch; 21h:31m:25s remains)
2017-12-11 09:05:44.156179: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.20752907 0.23272206 0.2584326 0.28576386 0.29830271 0.27849048 0.22435771 0.15478404 0.095985375 0.073502474 0.10082167 0.1682841 0.26430169 0.38126847 0.49190634][0.20283772 0.22201152 0.2432242 0.27024922 0.288168 0.28034872 0.24093507 0.18193197 0.12364144 0.089238606 0.095419481 0.1394987 0.21652615 0.32328039 0.43428627][0.18833828 0.20726457 0.23245662 0.26807344 0.29920241 0.30932826 0.28804511 0.24104404 0.18263349 0.13399339 0.11461303 0.12753494 0.17320649 0.25344235 0.34766662][0.17134379 0.19617115 0.23353346 0.28604665 0.33720297 0.36881691 0.36528066 0.32812971 0.268325 0.20648776 0.16315486 0.14440899 0.15436493 0.19849448 0.2630817][0.15800577 0.19366248 0.24887115 0.32346982 0.39789557 0.45046651 0.4609383 0.42943776 0.36653903 0.29305342 0.22907515 0.18194905 0.1574984 0.16243799 0.19061512][0.14783503 0.19391274 0.2652691 0.3597098 0.45419124 0.52282274 0.54228908 0.51359981 0.44859254 0.36757407 0.28925136 0.22150552 0.17024599 0.14076304 0.13403687][0.13753626 0.18700889 0.26487637 0.36908185 0.47444922 0.55153435 0.57598519 0.55054152 0.48781875 0.40594178 0.32202858 0.24506745 0.17998801 0.12840296 0.096272431][0.13398996 0.177041 0.24818963 0.34819964 0.45207429 0.52876127 0.55496895 0.53516704 0.4806737 0.40503573 0.32378674 0.24842669 0.18344401 0.12558052 0.082204223][0.15197623 0.18120331 0.23388878 0.31634203 0.4061932 0.47342446 0.4974857 0.48402774 0.44139114 0.37692872 0.3040691 0.23744954 0.18224734 0.1311384 0.089963354][0.18540104 0.19767874 0.22554468 0.28143483 0.34818083 0.39949873 0.41835889 0.41032529 0.38013673 0.32920697 0.26830325 0.21426994 0.1734418 0.13664626 0.10686408][0.22740039 0.22409374 0.2263535 0.25181016 0.29082081 0.32347369 0.33616409 0.33280024 0.31442183 0.27773935 0.2303167 0.18937717 0.16251311 0.1413884 0.126696][0.26593959 0.25294933 0.23526183 0.23328117 0.24569948 0.26054102 0.26692507 0.26615277 0.25640717 0.23193073 0.19728661 0.16791932 0.15187363 0.14331611 0.14211106][0.2795206 0.26396415 0.23534925 0.21408086 0.2057517 0.20628257 0.20695896 0.20650305 0.20178322 0.18673356 0.16360334 0.14430116 0.13598372 0.13535245 0.14246184][0.24744928 0.23412636 0.20341188 0.17294116 0.15253539 0.14431478 0.14165436 0.14128627 0.13983046 0.13229194 0.11936179 0.10864451 0.10531276 0.1079696 0.11723509][0.16646601 0.15713795 0.13133435 0.10175601 0.079216585 0.069030583 0.066230677 0.066662811 0.067332968 0.064900331 0.059302762 0.0544948 0.053505909 0.056018826 0.063001186]]...]
INFO - root - 2017-12-11 09:05:46.757619: step 43610, loss = 0.68, batch loss = 0.63 (31.5 examples/sec; 0.254 sec/batch; 20h:21m:13s remains)
INFO - root - 2017-12-11 09:05:49.391943: step 43620, loss = 0.69, batch loss = 0.64 (30.1 examples/sec; 0.266 sec/batch; 21h:19m:04s remains)
INFO - root - 2017-12-11 09:05:52.043041: step 43630, loss = 0.68, batch loss = 0.62 (30.0 examples/sec; 0.267 sec/batch; 21h:25m:08s remains)
INFO - root - 2017-12-11 09:05:54.705084: step 43640, loss = 0.71, batch loss = 0.65 (29.6 examples/sec; 0.270 sec/batch; 21h:39m:24s remains)
INFO - root - 2017-12-11 09:05:57.375194: step 43650, loss = 0.71, batch loss = 0.65 (30.5 examples/sec; 0.262 sec/batch; 21h:02m:00s remains)
INFO - root - 2017-12-11 09:05:59.991547: step 43660, loss = 0.70, batch loss = 0.65 (30.2 examples/sec; 0.265 sec/batch; 21h:17m:10s remains)
INFO - root - 2017-12-11 09:06:02.592798: step 43670, loss = 0.70, batch loss = 0.64 (31.8 examples/sec; 0.251 sec/batch; 20h:09m:59s remains)
INFO - root - 2017-12-11 09:06:05.206760: step 43680, loss = 0.70, batch loss = 0.64 (31.5 examples/sec; 0.254 sec/batch; 20h:20m:54s remains)
INFO - root - 2017-12-11 09:06:07.844082: step 43690, loss = 0.69, batch loss = 0.63 (32.2 examples/sec; 0.248 sec/batch; 19h:54m:29s remains)
INFO - root - 2017-12-11 09:06:10.490778: step 43700, loss = 0.71, batch loss = 0.65 (29.9 examples/sec; 0.268 sec/batch; 21h:28m:58s remains)
2017-12-11 09:06:10.874318: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.024752446 0.069323532 0.11843915 0.16604808 0.19773145 0.20445901 0.18580192 0.1510189 0.11352696 0.08100266 0.069904968 0.084810868 0.11690138 0.15576153 0.18694715][0.023563111 0.071985781 0.12595108 0.17757365 0.21203227 0.21979871 0.19958802 0.16131681 0.11851247 0.079556406 0.061193254 0.069871917 0.098193519 0.13555783 0.16838554][0.021407709 0.0729182 0.13202219 0.18898921 0.22963239 0.2443254 0.22916605 0.19167815 0.14560118 0.1015759 0.074944906 0.073424138 0.092491291 0.12260623 0.15223391][0.018110326 0.071255222 0.13457172 0.19682705 0.24462904 0.26885352 0.26296932 0.23036027 0.1836656 0.13650413 0.10298318 0.090397142 0.097583406 0.1175992 0.14075248][0.017830128 0.072421134 0.13832565 0.20345056 0.25636297 0.28957295 0.29468343 0.27015692 0.22632843 0.1787405 0.1393493 0.11433872 0.10717531 0.11533544 0.13062583][0.020126659 0.076288715 0.14263606 0.20808049 0.26347896 0.30341148 0.31808326 0.301663 0.26219332 0.21543708 0.17047222 0.13241905 0.11022186 0.10723484 0.11638416][0.020587433 0.076621689 0.14169683 0.20701669 0.26546785 0.31190553 0.33531594 0.32709652 0.29305449 0.24755622 0.1972425 0.14662176 0.11002292 0.097031 0.10114545][0.012881207 0.066181839 0.1289303 0.19536446 0.25940084 0.31353751 0.34537098 0.34539881 0.31772116 0.27431655 0.22011086 0.15937597 0.11027274 0.08770746 0.086040787][0.0006135712 0.050566435 0.1117562 0.18073724 0.251552 0.31291613 0.35137281 0.35829964 0.33696762 0.29648969 0.2399538 0.17196143 0.11298197 0.081278473 0.072625063][-0.0024331894 0.047697064 0.11061611 0.18255973 0.25668153 0.32022792 0.36232609 0.37679255 0.36484739 0.33149838 0.27723604 0.20585164 0.13745925 0.092204347 0.069780245][0.0094784321 0.065192871 0.13399549 0.20866729 0.28091413 0.34056112 0.38298336 0.40503585 0.40450621 0.38219741 0.33563086 0.2651493 0.18767564 0.12468737 0.082697846][0.029308686 0.0941933 0.1717574 0.24933366 0.31711888 0.370285 0.41081271 0.43783376 0.44614562 0.43463334 0.39898378 0.33409518 0.25118202 0.1723334 0.11130752][0.052037433 0.12847058 0.21667325 0.29827327 0.36178046 0.40781838 0.44342262 0.4689042 0.47828683 0.47231755 0.44751567 0.39277256 0.31209666 0.22661176 0.15470253][0.075744033 0.16466807 0.26414937 0.3509517 0.41187117 0.4511615 0.47810188 0.49307916 0.49236935 0.48219839 0.46291977 0.41939637 0.34941256 0.27185529 0.2048717][0.093044288 0.19165038 0.30051678 0.3933 0.45484677 0.48980537 0.50590354 0.50293684 0.48179075 0.45638344 0.43375823 0.39874738 0.34592375 0.29053915 0.24503247]]...]
INFO - root - 2017-12-11 09:06:13.586233: step 43710, loss = 0.69, batch loss = 0.64 (30.0 examples/sec; 0.266 sec/batch; 21h:21m:48s remains)
INFO - root - 2017-12-11 09:06:16.199849: step 43720, loss = 0.70, batch loss = 0.64 (31.3 examples/sec; 0.256 sec/batch; 20h:31m:40s remains)
INFO - root - 2017-12-11 09:06:18.834177: step 43730, loss = 0.68, batch loss = 0.62 (29.8 examples/sec; 0.269 sec/batch; 21h:33m:14s remains)
INFO - root - 2017-12-11 09:06:21.505259: step 43740, loss = 0.69, batch loss = 0.63 (30.3 examples/sec; 0.264 sec/batch; 21h:10m:11s remains)
INFO - root - 2017-12-11 09:06:24.145049: step 43750, loss = 0.69, batch loss = 0.63 (30.2 examples/sec; 0.265 sec/batch; 21h:15m:15s remains)
INFO - root - 2017-12-11 09:06:26.839390: step 43760, loss = 0.72, batch loss = 0.66 (30.2 examples/sec; 0.265 sec/batch; 21h:13m:48s remains)
INFO - root - 2017-12-11 09:06:29.455087: step 43770, loss = 0.70, batch loss = 0.64 (32.2 examples/sec; 0.249 sec/batch; 19h:57m:24s remains)
INFO - root - 2017-12-11 09:06:32.082319: step 43780, loss = 0.69, batch loss = 0.63 (30.3 examples/sec; 0.264 sec/batch; 21h:09m:11s remains)
INFO - root - 2017-12-11 09:06:34.762957: step 43790, loss = 0.69, batch loss = 0.64 (27.5 examples/sec; 0.291 sec/batch; 23h:20m:41s remains)
INFO - root - 2017-12-11 09:06:37.469835: step 43800, loss = 0.71, batch loss = 0.65 (27.7 examples/sec; 0.289 sec/batch; 23h:10m:38s remains)
2017-12-11 09:06:37.834504: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.060990337 -0.070873491 -0.07267385 -0.070924476 -0.068325311 -0.064230971 -0.059158679 -0.053208251 -0.046646416 -0.036749259 -0.016834293 0.0198525 0.078557648 0.1636492 0.27268392][-0.061577938 -0.065550841 -0.060564991 -0.051939577 -0.042785883 -0.032658756 -0.021871693 -0.0095696375 0.0017806932 0.012436699 0.027752934 0.056338679 0.10508205 0.18047044 0.28387573][-0.032655548 -0.028017335 -0.013599144 0.005294411 0.024622224 0.044045333 0.063494474 0.083875656 0.099206634 0.10599593 0.10799712 0.11564124 0.13698697 0.18244658 0.26026472][0.024022188 0.04218851 0.071668565 0.10813137 0.14382914 0.17653431 0.20638974 0.23415056 0.25017861 0.24699615 0.22811711 0.2057154 0.18925487 0.19336048 0.23485857][0.096446611 0.13669847 0.1907125 0.25395194 0.31179032 0.35905012 0.39652991 0.4255853 0.43529806 0.41499978 0.36998686 0.31497383 0.26077539 0.22540574 0.23299588][0.17343819 0.24090064 0.32506257 0.41868037 0.49828169 0.55564505 0.5929212 0.61428 0.6104688 0.56940657 0.49913198 0.41677442 0.33499104 0.27265018 0.2592147][0.23970953 0.33271828 0.44408998 0.56301272 0.6573469 0.71568769 0.74277157 0.74630934 0.722003 0.65897489 0.5677709 0.46756712 0.3713091 0.29851469 0.2797668][0.28227675 0.38918954 0.51363873 0.64240277 0.73755878 0.78599578 0.79468727 0.77451271 0.72786182 0.64762431 0.54566431 0.43999773 0.34282154 0.27274093 0.25805309][0.28903583 0.38960567 0.50530195 0.62320572 0.70440811 0.73594511 0.72525167 0.684215 0.62154472 0.53495985 0.43579063 0.33831757 0.25183952 0.19236845 0.18353741][0.25517073 0.32966074 0.41688666 0.50707328 0.56480461 0.5788815 0.554903 0.50315291 0.43508419 0.3529129 0.26664314 0.18633184 0.11800539 0.073928908 0.07097102][0.19174893 0.2283611 0.27564538 0.329121 0.35973385 0.35942206 0.33160207 0.28303602 0.22379453 0.15811561 0.093372792 0.035368219 -0.012537747 -0.041635118 -0.041460916][0.12340067 0.12361367 0.13242772 0.15129934 0.15777262 0.14751358 0.12262487 0.08647535 0.045439411 0.0032668668 -0.036542278 -0.071832173 -0.10168429 -0.11993542 -0.12034996][0.07352294 0.04810806 0.030049533 0.025092157 0.016246533 0.0017905293 -0.017007591 -0.039283477 -0.062039338 -0.082662925 -0.10088444 -0.11752143 -0.13316661 -0.14439918 -0.14725721][0.047455247 0.011063408 -0.018927103 -0.033710733 -0.046555612 -0.0585815 -0.069730438 -0.080540486 -0.090303294 -0.097130723 -0.10188828 -0.10634585 -0.11195724 -0.11806341 -0.12276527][0.044848789 0.010223625 -0.019554501 -0.034376211 -0.044767048 -0.052022692 -0.056815974 -0.059913982 -0.06216158 -0.062586918 -0.061966427 -0.06122249 -0.061484545 -0.064383343 -0.069792084]]...]
INFO - root - 2017-12-11 09:06:40.449894: step 43810, loss = 0.71, batch loss = 0.65 (30.6 examples/sec; 0.261 sec/batch; 20h:56m:11s remains)
INFO - root - 2017-12-11 09:06:43.080820: step 43820, loss = 0.69, batch loss = 0.63 (29.8 examples/sec; 0.269 sec/batch; 21h:31m:58s remains)
INFO - root - 2017-12-11 09:06:45.748525: step 43830, loss = 0.70, batch loss = 0.64 (30.2 examples/sec; 0.265 sec/batch; 21h:14m:22s remains)
INFO - root - 2017-12-11 09:06:48.355282: step 43840, loss = 0.69, batch loss = 0.64 (31.4 examples/sec; 0.254 sec/batch; 20h:24m:23s remains)
INFO - root - 2017-12-11 09:06:50.960693: step 43850, loss = 0.69, batch loss = 0.63 (30.7 examples/sec; 0.261 sec/batch; 20h:54m:03s remains)
INFO - root - 2017-12-11 09:06:53.629386: step 43860, loss = 0.69, batch loss = 0.63 (30.7 examples/sec; 0.261 sec/batch; 20h:54m:34s remains)
INFO - root - 2017-12-11 09:06:56.231917: step 43870, loss = 0.69, batch loss = 0.64 (30.5 examples/sec; 0.263 sec/batch; 21h:03m:29s remains)
INFO - root - 2017-12-11 09:06:58.832116: step 43880, loss = 0.68, batch loss = 0.62 (30.2 examples/sec; 0.265 sec/batch; 21h:12m:47s remains)
INFO - root - 2017-12-11 09:07:01.463697: step 43890, loss = 0.70, batch loss = 0.64 (31.3 examples/sec; 0.255 sec/batch; 20h:28m:08s remains)
INFO - root - 2017-12-11 09:07:04.099875: step 43900, loss = 0.70, batch loss = 0.64 (30.3 examples/sec; 0.264 sec/batch; 21h:08m:51s remains)
2017-12-11 09:07:04.507124: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.10005217 0.098369576 0.093300357 0.087455094 0.085364126 0.091034107 0.10347907 0.11689251 0.12420259 0.12247452 0.110413 0.088614427 0.061795775 0.041691925 0.036569994][0.095763922 0.091416858 0.083699867 0.075643718 0.071257614 0.076195136 0.089477956 0.10416639 0.11231878 0.11049927 0.098375738 0.077369742 0.05256477 0.036077194 0.036092993][0.10739525 0.098495558 0.084115855 0.068821356 0.057283718 0.057515949 0.069898531 0.08644326 0.096939519 0.096837439 0.085988156 0.06663385 0.044565868 0.032298408 0.038473666][0.13610469 0.12232359 0.09836302 0.071618184 0.049263965 0.04245748 0.053066146 0.07204219 0.08672072 0.090480991 0.082359426 0.064448662 0.043322846 0.032495532 0.041961096][0.17023551 0.15352906 0.12024052 0.081510991 0.047783893 0.033226393 0.041292407 0.062570989 0.082251169 0.091061987 0.086241849 0.069105089 0.0470894 0.034981623 0.04459361][0.20107615 0.18439591 0.14464103 0.096535668 0.053343836 0.031588454 0.036308318 0.058715314 0.082513355 0.095876977 0.093833722 0.076632045 0.052661266 0.037992731 0.045931913][0.21693733 0.20331869 0.16203366 0.11001658 0.061994981 0.035419725 0.036462694 0.058073893 0.083789781 0.10003072 0.099768788 0.082118519 0.056123644 0.038690958 0.04417849][0.21111912 0.20249099 0.16485068 0.11522469 0.067990266 0.039961405 0.038032323 0.057576083 0.083124988 0.10045068 0.10125347 0.083462864 0.055967975 0.035794355 0.038108069][0.18112147 0.17771903 0.14771393 0.10588852 0.064607978 0.038713578 0.035548221 0.052793395 0.076905616 0.094167426 0.095916569 0.079041764 0.051471446 0.029586727 0.028706273][0.13669409 0.13738063 0.11619994 0.084397711 0.051492095 0.029847387 0.02697752 0.042244744 0.064234458 0.080711752 0.083374724 0.068448193 0.042554278 0.020748412 0.017735295][0.090720564 0.093640521 0.080156714 0.057673197 0.033066414 0.016345587 0.014773017 0.028516607 0.048130315 0.063221194 0.066477053 0.053943343 0.030872758 0.010446055 0.0061653806][0.051711243 0.055199537 0.04718535 0.031760834 0.013889606 0.001723945 0.0019138373 0.014534016 0.031771474 0.0451056 0.048482969 0.0382034 0.018279344 -0.00019556619 -0.0053646872][0.025546703 0.029072834 0.024561899 0.013629725 2.0310104e-05 -0.009217523 -0.0080127912 0.0031604967 0.017786829 0.029005433 0.031946171 0.023375997 0.0064100879 -0.0097789327 -0.015247441][0.011649593 0.015460101 0.013210766 0.0048180721 -0.0067321686 -0.014879296 -0.013953688 -0.0048567406 0.0069747269 0.015968846 0.018154694 0.010747763 -0.0035913074 -0.017301157 -0.022309491][0.0081509072 0.012737132 0.012019732 0.0047829663 -0.0064027044 -0.015070736 -0.015740911 -0.0092912316 -0.0002750752 0.0066336663 0.0079870513 0.0013430773 -0.010816114 -0.022213304 -0.026339557]]...]
INFO - root - 2017-12-11 09:07:07.104122: step 43910, loss = 0.72, batch loss = 0.66 (31.2 examples/sec; 0.256 sec/batch; 20h:31m:28s remains)
INFO - root - 2017-12-11 09:07:09.720378: step 43920, loss = 0.70, batch loss = 0.64 (30.9 examples/sec; 0.259 sec/batch; 20h:46m:31s remains)
INFO - root - 2017-12-11 09:07:12.361921: step 43930, loss = 0.70, batch loss = 0.64 (31.6 examples/sec; 0.253 sec/batch; 20h:16m:40s remains)
INFO - root - 2017-12-11 09:07:14.961617: step 43940, loss = 0.69, batch loss = 0.63 (30.2 examples/sec; 0.265 sec/batch; 21h:13m:17s remains)
INFO - root - 2017-12-11 09:07:17.581501: step 43950, loss = 0.70, batch loss = 0.64 (29.3 examples/sec; 0.273 sec/batch; 21h:54m:48s remains)
INFO - root - 2017-12-11 09:07:20.187473: step 43960, loss = 0.70, batch loss = 0.64 (30.7 examples/sec; 0.260 sec/batch; 20h:51m:27s remains)
INFO - root - 2017-12-11 09:07:22.839305: step 43970, loss = 0.69, batch loss = 0.63 (30.3 examples/sec; 0.264 sec/batch; 21h:11m:08s remains)
INFO - root - 2017-12-11 09:07:25.468814: step 43980, loss = 0.70, batch loss = 0.65 (31.1 examples/sec; 0.257 sec/batch; 20h:35m:58s remains)
INFO - root - 2017-12-11 09:07:28.115185: step 43990, loss = 0.70, batch loss = 0.64 (31.0 examples/sec; 0.258 sec/batch; 20h:41m:16s remains)
INFO - root - 2017-12-11 09:07:30.777932: step 44000, loss = 0.66, batch loss = 0.61 (29.9 examples/sec; 0.268 sec/batch; 21h:26m:52s remains)
2017-12-11 09:07:31.148228: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.18134062 0.27227592 0.362988 0.43749857 0.48327467 0.49897724 0.48799032 0.43826467 0.34945628 0.24066502 0.13903941 0.052430477 -0.021718279 -0.0786712 -0.11502083][0.19851281 0.299036 0.40022811 0.48399761 0.53642672 0.556436 0.54710686 0.49591008 0.40212411 0.28479329 0.17251293 0.075146414 -0.0087011419 -0.073192827 -0.11335506][0.1859123 0.28374946 0.38474965 0.47002482 0.52591324 0.55232704 0.55203682 0.51104939 0.42580587 0.31314152 0.20021446 0.09787526 0.0067579197 -0.064528868 -0.10853606][0.15505025 0.24046363 0.33274269 0.41479167 0.47484466 0.51414174 0.53326094 0.51522273 0.45036483 0.35104164 0.2413636 0.13356265 0.032099679 -0.049562242 -0.10044284][0.11883673 0.18565522 0.26428828 0.34193507 0.40960261 0.47005606 0.51922053 0.53446412 0.49689519 0.4135015 0.30529761 0.18702488 0.069139466 -0.028272118 -0.089813][0.094334856 0.14241672 0.20813362 0.28356904 0.36221257 0.44802481 0.5300138 0.57887661 0.56615794 0.49436924 0.38270414 0.24886443 0.11042234 -0.0056041032 -0.079719916][0.089864857 0.12427717 0.18120758 0.25692543 0.34690893 0.4541932 0.561272 0.63331193 0.63576758 0.56835455 0.44904816 0.2989583 0.14215562 0.010812241 -0.073385671][0.10422616 0.13286701 0.18548284 0.26104996 0.35606405 0.47176832 0.58609378 0.66280746 0.667497 0.59847349 0.47252405 0.31305048 0.1480341 0.011398721 -0.075682491][0.1259315 0.15413739 0.20349438 0.27368495 0.36193395 0.46773797 0.56826812 0.63173068 0.62885016 0.55721951 0.43198597 0.2761237 0.11833671 -0.0097630545 -0.089720957][0.13720912 0.16640037 0.21183999 0.27158755 0.3429091 0.42430148 0.49663854 0.53654873 0.52264392 0.45249251 0.33803871 0.19950846 0.062798396 -0.046062794 -0.11189817][0.13084321 0.15953407 0.19972062 0.24666311 0.29698449 0.3490116 0.390431 0.40751863 0.38680005 0.32516566 0.23009504 0.11729258 0.0072130971 -0.0798895 -0.13139442][0.11604379 0.14320306 0.17844519 0.2144984 0.24648735 0.27282524 0.28867802 0.28921115 0.26760027 0.22006018 0.14872511 0.062994264 -0.023781862 -0.095396094 -0.1388236][0.11087461 0.13730793 0.1692553 0.19785528 0.21637966 0.22381611 0.22210442 0.21345232 0.19545023 0.16374241 0.1157345 0.053212415 -0.018730465 -0.085667849 -0.13069662][0.12865166 0.1543206 0.18282887 0.2056063 0.21445903 0.20944028 0.19767271 0.18581602 0.17292649 0.15391997 0.12381193 0.077410392 0.011895936 -0.058572281 -0.11168967][0.16163018 0.18773566 0.21322434 0.23225962 0.23545457 0.22368769 0.20760636 0.19554536 0.18578446 0.17270893 0.15210269 0.11382888 0.048975933 -0.02782961 -0.08984255]]...]
INFO - root - 2017-12-11 09:07:33.778393: step 44010, loss = 0.69, batch loss = 0.63 (29.0 examples/sec; 0.276 sec/batch; 22h:05m:38s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-SGD-lr0.001-clip50-initfromfixqian30k-fixqian
INFO - root - 2017-12-11 09:07:36.424244: step 44020, loss = 0.70, batch loss = 0.64 (27.8 examples/sec; 0.288 sec/batch; 23h:05m:22s remains)
INFO - root - 2017-12-11 09:07:39.078708: step 44030, loss = 0.69, batch loss = 0.63 (31.5 examples/sec; 0.254 sec/batch; 20h:19m:22s remains)
INFO - root - 2017-12-11 09:07:41.729722: step 44040, loss = 0.69, batch loss = 0.63 (30.7 examples/sec; 0.261 sec/batch; 20h:53m:45s remains)
INFO - root - 2017-12-11 09:07:44.354524: step 44050, loss = 0.70, batch loss = 0.65 (30.5 examples/sec; 0.262 sec/batch; 20h:59m:45s remains)
INFO - root - 2017-12-11 09:07:46.982687: step 44060, loss = 0.70, batch loss = 0.64 (30.3 examples/sec; 0.264 sec/batch; 21h:07m:20s remains)
INFO - root - 2017-12-11 09:07:49.628897: step 44070, loss = 0.69, batch loss = 0.63 (29.2 examples/sec; 0.274 sec/batch; 21h:57m:03s remains)
INFO - root - 2017-12-11 09:07:52.264285: step 44080, loss = 0.69, batch loss = 0.63 (30.1 examples/sec; 0.266 sec/batch; 21h:19m:20s remains)
INFO - root - 2017-12-11 09:07:54.856449: step 44090, loss = 0.69, batch loss = 0.64 (30.5 examples/sec; 0.262 sec/batch; 21h:01m:46s remains)
INFO - root - 2017-12-11 09:07:57.556924: step 44100, loss = 0.71, batch loss = 0.65 (30.1 examples/sec; 0.266 sec/batch; 21h:17m:55s remains)
2017-12-11 09:07:57.929025: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.45200789 0.50812596 0.534962 0.5404098 0.52467173 0.49203262 0.45655423 0.42108369 0.39304709 0.37432033 0.36079928 0.35725179 0.3572225 0.34955677 0.33672023][0.51733243 0.60589737 0.65644985 0.6687488 0.63357586 0.56742644 0.49254036 0.42046511 0.36695644 0.33726758 0.32700828 0.33020815 0.333378 0.32187727 0.30722871][0.560538 0.66843694 0.72959208 0.74028343 0.68649769 0.59398228 0.48979121 0.39276335 0.32529566 0.29526761 0.30012751 0.32281142 0.33940732 0.32990775 0.310932][0.57228917 0.68113154 0.73648417 0.73616385 0.66861892 0.56620765 0.45526943 0.35581154 0.2914578 0.27307245 0.30244088 0.35537559 0.397964 0.40224522 0.38339278][0.55662018 0.65241915 0.69401979 0.68310767 0.61528867 0.52649707 0.43518832 0.35415331 0.30291966 0.29809442 0.34998783 0.43355551 0.50676382 0.53204948 0.51904935][0.53744704 0.61576194 0.64328176 0.62896609 0.57663012 0.52179468 0.46893582 0.4195137 0.38724098 0.39496014 0.46218103 0.56606972 0.65965348 0.69844866 0.68688929][0.54380095 0.60736388 0.62293237 0.60968035 0.57796967 0.55836087 0.54036307 0.51800185 0.50110376 0.51553249 0.5853678 0.68978232 0.7821427 0.81796461 0.7997272][0.58644694 0.6353218 0.63703007 0.62030166 0.60031313 0.60145295 0.604098 0.59964955 0.5934912 0.60823143 0.66688472 0.75133932 0.8198052 0.8348437 0.80170816][0.62186635 0.65270835 0.63734037 0.611208 0.59136164 0.59655529 0.60550839 0.60952061 0.60926455 0.61982083 0.65867704 0.71048552 0.74268162 0.73031843 0.6832639][0.61303 0.62829107 0.59806705 0.55976194 0.52908581 0.52223217 0.52304441 0.5247345 0.52402407 0.52728724 0.54555345 0.5662905 0.5678888 0.53813183 0.48925057][0.53346336 0.53678226 0.49752748 0.45011407 0.40838748 0.38703555 0.37656155 0.3725931 0.36961046 0.36672422 0.37013227 0.37005746 0.35550737 0.32309419 0.28315529][0.38043579 0.37221596 0.32733136 0.27404833 0.22552273 0.19491869 0.17810923 0.17159182 0.16837764 0.16301084 0.15907191 0.15086995 0.13495031 0.11168449 0.087272085][0.19985291 0.18624209 0.14371647 0.092755206 0.045546975 0.013534661 -0.0040892772 -0.010344996 -0.012057591 -0.01612917 -0.020347523 -0.026656721 -0.035395816 -0.045967527 -0.055729214][0.056412458 0.043010212 0.0089492993 -0.031792875 -0.069630988 -0.09614893 -0.11115541 -0.11686196 -0.11798703 -0.11988614 -0.12087255 -0.12125499 -0.12100706 -0.12049218 -0.11980596][-0.030301563 -0.041477632 -0.065116413 -0.093063459 -0.11910543 -0.13798164 -0.14943996 -0.15458997 -0.1559625 -0.156633 -0.15516298 -0.15156065 -0.14600749 -0.13943774 -0.13321103]]...]
INFO - root - 2017-12-11 09:08:00.525347: step 44110, loss = 0.69, batch loss = 0.63 (30.9 examples/sec; 0.259 sec/batch; 20h:42m:35s remains)
INFO - root - 2017-12-11 09:08:03.129042: step 44120, loss = 0.69, batch loss = 0.63 (30.3 examples/sec; 0.264 sec/batch; 21h:10m:14s remains)
INFO - root - 2017-12-11 09:08:05.812065: step 44130, loss = 0.69, batch loss = 0.63 (30.9 examples/sec; 0.259 sec/batch; 20h:43m:19s remains)
INFO - root - 2017-12-11 09:08:08.461948: step 44140, loss = 0.71, batch loss = 0.65 (30.1 examples/sec; 0.266 sec/batch; 21h:17m:43s remains)
INFO - root - 2017-12-11 09:08:11.123908: step 44150, loss = 0.70, batch loss = 0.65 (29.9 examples/sec; 0.267 sec/batch; 21h:24m:52s remains)
INFO - root - 2017-12-11 09:08:13.836638: step 44160, loss = 0.69, batch loss = 0.63 (30.0 examples/sec; 0.267 sec/batch; 21h:21m:53s remains)
INFO - root - 2017-12-11 09:08:16.485765: step 44170, loss = 0.71, batch loss = 0.65 (29.1 examples/sec; 0.275 sec/batch; 22h:00m:32s remains)
INFO - root - 2017-12-11 09:08:19.193967: step 44180, loss = 0.71, batch loss = 0.65 (28.9 examples/sec; 0.277 sec/batch; 22h:09m:14s remains)
INFO - root - 2017-12-11 09:08:21.837254: step 44190, loss = 0.70, batch loss = 0.64 (30.7 examples/sec; 0.261 sec/batch; 20h:53m:33s remains)
INFO - root - 2017-12-11 09:08:24.448458: step 44200, loss = 0.71, batch loss = 0.65 (30.4 examples/sec; 0.264 sec/batch; 21h:06m:10s remains)
2017-12-11 09:08:24.825501: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.049914785 -0.058392085 -0.062271763 -0.064053945 -0.06527485 -0.06590192 -0.065238476 -0.063179113 -0.060298312 -0.057372682 -0.055664569 -0.057021417 -0.062087886 -0.069065042 -0.074568234][-0.010775685 -0.019154267 -0.021871589 -0.020166909 -0.01690001 -0.012947333 -0.0092944363 -0.0068132021 -0.0061596395 -0.007178775 -0.010903403 -0.020049909 -0.035761755 -0.054675143 -0.070232905][0.062725529 0.058330398 0.059944995 0.068464436 0.080097221 0.092498496 0.10171407 0.10441726 0.10007152 0.090277709 0.074955523 0.0508149 0.016466845 -0.021989254 -0.053844851][0.1617751 0.16611306 0.17543988 0.19359943 0.21710184 0.24172311 0.25874659 0.26116249 0.24819167 0.22417952 0.19016093 0.14342442 0.083406724 0.020044476 -0.031423323][0.2680136 0.28285939 0.30077934 0.32978895 0.36743188 0.40729144 0.43438935 0.43701741 0.41356915 0.37133434 0.313277 0.23894709 0.15012261 0.060850054 -0.010137971][0.36812562 0.39043567 0.41302943 0.44966269 0.49983168 0.55437416 0.59196818 0.59582913 0.5627591 0.50242549 0.42034164 0.31926125 0.20443366 0.093126372 0.0062126466][0.4530662 0.47657856 0.49549931 0.53155434 0.58720767 0.65050858 0.69559485 0.70118964 0.66230214 0.58926725 0.4895452 0.37007752 0.23905414 0.11480042 0.018298494][0.52437365 0.53838253 0.5415414 0.56495744 0.61574018 0.67945224 0.72730792 0.73509622 0.69621891 0.61946672 0.51220006 0.3853755 0.24989626 0.1231294 0.02415175][0.575817 0.569825 0.54710954 0.548573 0.58513367 0.64040411 0.68528163 0.69502163 0.66149861 0.58971709 0.48607105 0.36420205 0.23600712 0.11611761 0.021561021][0.59462428 0.56483424 0.51372391 0.48981068 0.5061456 0.54546827 0.58129728 0.59102422 0.565734 0.50587738 0.41644937 0.31152162 0.20167774 0.097350679 0.013142274][0.564766 0.5154556 0.44247139 0.39719388 0.39310035 0.41360769 0.43710738 0.44483578 0.42809048 0.3835848 0.31488827 0.23469374 0.15022898 0.067523688 -0.0012408753][0.47487503 0.41535288 0.33306798 0.27592066 0.256776 0.26142985 0.27378944 0.27917042 0.26978502 0.2403286 0.19354419 0.13978016 0.08256536 0.024319215 -0.025244461][0.33373103 0.27511162 0.19852783 0.14251405 0.11763123 0.11318801 0.11804777 0.12154922 0.11747711 0.10064968 0.073307388 0.042971693 0.010305497 -0.024441849 -0.054110415][0.16882181 0.12124005 0.062989466 0.019867936 -0.001440753 -0.0080353348 -0.0067713112 -0.0046505281 -0.0060257153 -0.014070665 -0.0263722 -0.038220447 -0.050895274 -0.065415636 -0.077433214][0.021635653 -0.010784232 -0.046492834 -0.072349966 -0.085220069 -0.089287266 -0.088303454 -0.086291969 -0.086029023 -0.088615514 -0.0913834 -0.0916076 -0.091093786 -0.091409653 -0.090721972]]...]
INFO - root - 2017-12-11 09:08:27.452554: step 44210, loss = 0.69, batch loss = 0.64 (30.8 examples/sec; 0.260 sec/batch; 20h:49m:22s remains)
INFO - root - 2017-12-11 09:08:30.075103: step 44220, loss = 0.70, batch loss = 0.64 (30.0 examples/sec; 0.267 sec/batch; 21h:21m:53s remains)
INFO - root - 2017-12-11 09:08:32.749728: step 44230, loss = 0.68, batch loss = 0.63 (30.0 examples/sec; 0.267 sec/batch; 21h:20m:50s remains)
INFO - root - 2017-12-11 09:08:35.411727: step 44240, loss = 0.69, batch loss = 0.64 (31.4 examples/sec; 0.255 sec/batch; 20h:25m:33s remains)
INFO - root - 2017-12-11 09:08:38.090978: step 44250, loss = 0.70, batch loss = 0.64 (29.9 examples/sec; 0.267 sec/batch; 21h:24m:38s remains)
INFO - root - 2017-12-11 09:08:40.722489: step 44260, loss = 0.69, batch loss = 0.63 (29.7 examples/sec; 0.269 sec/batch; 21h:34m:34s remains)
INFO - root - 2017-12-11 09:08:43.388167: step 44270, loss = 0.70, batch loss = 0.65 (31.4 examples/sec; 0.255 sec/batch; 20h:24m:18s remains)
INFO - root - 2017-12-11 09:08:45.982995: step 44280, loss = 0.71, batch loss = 0.65 (31.2 examples/sec; 0.256 sec/batch; 20h:30m:55s remains)
INFO - root - 2017-12-11 09:08:48.647159: step 44290, loss = 0.68, batch loss = 0.62 (30.3 examples/sec; 0.264 sec/batch; 21h:07m:39s remains)
INFO - root - 2017-12-11 09:08:51.294008: step 44300, loss = 0.68, batch loss = 0.62 (30.9 examples/sec; 0.259 sec/batch; 20h:43m:46s remains)
2017-12-11 09:08:51.695263: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.05490737 -0.058171932 -0.059710331 -0.06110229 -0.062318981 -0.063924529 -0.065845743 -0.067244664 -0.06683331 -0.064782746 -0.062131 -0.059815444 -0.058912188 -0.059127383 -0.060561065][-0.047297254 -0.049785782 -0.05166788 -0.053659793 -0.055436496 -0.05823984 -0.062738374 -0.067057557 -0.067752272 -0.0645638 -0.059302129 -0.054115184 -0.05126423 -0.051783681 -0.055679567][-0.024553968 -0.025162974 -0.028005637 -0.030817371 -0.031789087 -0.03372246 -0.039431896 -0.046485841 -0.048512846 -0.04452575 -0.036538839 -0.02765453 -0.022447169 -0.025072169 -0.035417736][0.016020309 0.021431236 0.020783488 0.021002561 0.025211075 0.027401984 0.021381279 0.0099756531 0.0033844938 0.0051182797 0.01474534 0.027738396 0.034718651 0.025716472 0.0017642141][0.075648941 0.094510943 0.1045887 0.11593351 0.13132231 0.14098497 0.13488317 0.11570963 0.098196305 0.090593755 0.096659511 0.11012537 0.1152187 0.094666146 0.05061895][0.1454328 0.1838281 0.21289033 0.2424922 0.27335903 0.29291245 0.287837 0.25968 0.22708081 0.20297974 0.19686827 0.20208396 0.19878852 0.16383387 0.099132448][0.21012671 0.26933637 0.31977114 0.36705658 0.40936404 0.43475416 0.43019634 0.39554483 0.35077828 0.31006917 0.28669029 0.27591726 0.25860757 0.20980635 0.13037632][0.25368372 0.3283295 0.39465529 0.45057416 0.49195477 0.5127539 0.50574434 0.46924248 0.42056227 0.37026197 0.33252615 0.30614567 0.27572966 0.21867923 0.13454832][0.25968802 0.3381072 0.40911677 0.462689 0.49296305 0.50170964 0.49001053 0.45569363 0.41125941 0.36145034 0.31853303 0.28463382 0.24897358 0.19337183 0.11601488][0.21949601 0.28638545 0.34934518 0.39379737 0.41308913 0.41319373 0.40027511 0.37200865 0.33714718 0.2959545 0.2574366 0.22555244 0.19331071 0.14761591 0.0860711][0.14669895 0.19183905 0.23808922 0.27146742 0.286294 0.28699982 0.27940932 0.26106367 0.23890342 0.2109569 0.18272679 0.15824488 0.13336875 0.10036164 0.057234086][0.07421723 0.097628407 0.1256761 0.14882208 0.16375898 0.17075722 0.17177668 0.16395916 0.15339258 0.13778487 0.11958656 0.1023454 0.084667623 0.063152559 0.036812291][0.030364515 0.039515246 0.054481886 0.070969149 0.0875913 0.10063169 0.10743519 0.10481703 0.099099569 0.089735247 0.077893659 0.066522658 0.055624906 0.043807995 0.030373402][0.024292294 0.029425746 0.038768202 0.051990338 0.06849397 0.082485966 0.088075258 0.0822692 0.072717354 0.062137738 0.052589297 0.046302211 0.042862363 0.040222529 0.036641795][0.044326331 0.050786685 0.058245983 0.068716668 0.081452541 0.090588339 0.08934851 0.075228795 0.05724138 0.041349649 0.032149851 0.0314226 0.037048556 0.044364594 0.049560037]]...]
INFO - root - 2017-12-11 09:08:54.308834: step 44310, loss = 0.69, batch loss = 0.63 (31.5 examples/sec; 0.254 sec/batch; 20h:18m:19s remains)
INFO - root - 2017-12-11 09:08:56.960552: step 44320, loss = 0.68, batch loss = 0.62 (30.2 examples/sec; 0.265 sec/batch; 21h:13m:01s remains)
INFO - root - 2017-12-11 09:08:59.621102: step 44330, loss = 0.69, batch loss = 0.63 (30.5 examples/sec; 0.262 sec/batch; 20h:59m:28s remains)
INFO - root - 2017-12-11 09:09:02.270075: step 44340, loss = 0.69, batch loss = 0.63 (31.2 examples/sec; 0.256 sec/batch; 20h:31m:21s remains)
INFO - root - 2017-12-11 09:09:04.863268: step 44350, loss = 0.69, batch loss = 0.63 (30.4 examples/sec; 0.263 sec/batch; 21h:04m:20s remains)
INFO - root - 2017-12-11 09:09:07.519772: step 44360, loss = 0.70, batch loss = 0.64 (31.3 examples/sec; 0.256 sec/batch; 20h:27m:27s remains)
INFO - root - 2017-12-11 09:09:10.137168: step 44370, loss = 0.70, batch loss = 0.65 (31.1 examples/sec; 0.257 sec/batch; 20h:35m:17s remains)
INFO - root - 2017-12-11 09:09:12.853257: step 44380, loss = 0.70, batch loss = 0.64 (28.0 examples/sec; 0.286 sec/batch; 22h:51m:46s remains)
INFO - root - 2017-12-11 09:09:15.551570: step 44390, loss = 0.71, batch loss = 0.65 (28.3 examples/sec; 0.282 sec/batch; 22h:36m:24s remains)
INFO - root - 2017-12-11 09:09:18.161448: step 44400, loss = 0.71, batch loss = 0.65 (31.0 examples/sec; 0.258 sec/batch; 20h:40m:18s remains)
2017-12-11 09:09:18.555487: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.033005767 0.029802697 0.022678016 0.01516959 0.010070111 0.0085826386 0.00840602 0.0085071838 0.008425015 0.0073416224 0.0057447683 0.0040002549 0.001767706 -0.00012096883 -0.00117733][0.042645168 0.042277094 0.037234344 0.030731535 0.025398511 0.022796407 0.021489946 0.020753432 0.019443754 0.016727291 0.01372306 0.010414925 0.0063057328 0.0025294477 0.00026332][0.050049145 0.053663582 0.051838335 0.046868518 0.040930923 0.036203209 0.032653559 0.03035979 0.027613761 0.023746347 0.020049602 0.015931321 0.010366376 0.0045682644 0.00079842476][0.052155361 0.059757974 0.061651859 0.058695644 0.052505136 0.045769311 0.039932344 0.036070544 0.032347124 0.027905928 0.02406675 0.019730713 0.013272392 0.0058350861 0.0003053589][0.047357935 0.057566363 0.062674046 0.062175751 0.057017826 0.049921192 0.043434724 0.039497964 0.0363402 0.032559223 0.029212996 0.024857538 0.017549289 0.0085513908 0.00072418887][0.038510468 0.049261242 0.055901356 0.057248477 0.053760476 0.0478986 0.042717967 0.040624712 0.039883625 0.038438946 0.036600716 0.032682564 0.02480494 0.014194272 0.0035521146][0.033369597 0.04276599 0.048546452 0.049644709 0.046641335 0.041870322 0.038540952 0.039066639 0.041811008 0.044246156 0.045217935 0.042550225 0.034313027 0.021911329 0.00799663][0.039137546 0.046809435 0.049892917 0.048378941 0.043340936 0.037184987 0.033464737 0.0348388 0.039991632 0.045803461 0.049941283 0.049412936 0.041709382 0.028316272 0.011501727][0.0573243 0.064403318 0.064214453 0.058695018 0.049327467 0.038304847 0.030085471 0.028530404 0.033001393 0.040239923 0.04730951 0.050316863 0.045546573 0.033834916 0.016681783][0.083370432 0.091613755 0.088727556 0.078890659 0.063892551 0.045557462 0.029687382 0.022111155 0.022960884 0.029264411 0.038319331 0.045703661 0.046117753 0.039004039 0.024922276][0.10990824 0.12061814 0.11641117 0.10300551 0.082611352 0.056966044 0.033066258 0.018474493 0.01379892 0.016858926 0.026134597 0.037392862 0.044090334 0.043931879 0.036101751][0.12653433 0.13874556 0.13396275 0.11847253 0.094736159 0.064643204 0.035859227 0.016712476 0.0074017453 0.006534508 0.014647851 0.028607672 0.041871946 0.050489489 0.051622134][0.12717952 0.13799444 0.13191697 0.11533284 0.090888843 0.060677189 0.032246947 0.013495238 0.0033337385 0.00079232408 0.008334225 0.024349637 0.043679837 0.061195705 0.072027616][0.11658799 0.12312654 0.11426128 0.096268654 0.072391272 0.04519815 0.021480475 0.0076392777 0.0010123405 0.0005869923 0.0097888652 0.028199213 0.052771643 0.078321747 0.098662227][0.1016192 0.10266969 0.090328254 0.070398331 0.047261361 0.02442752 0.007461471 0.0006034798 -0.00035030986 0.0033219892 0.015216479 0.03556988 0.063625932 0.095450863 0.12432449]]...]
INFO - root - 2017-12-11 09:09:21.155194: step 44410, loss = 0.69, batch loss = 0.63 (30.8 examples/sec; 0.260 sec/batch; 20h:46m:02s remains)
INFO - root - 2017-12-11 09:09:23.804485: step 44420, loss = 0.70, batch loss = 0.64 (31.4 examples/sec; 0.255 sec/batch; 20h:24m:17s remains)
INFO - root - 2017-12-11 09:09:26.402223: step 44430, loss = 0.70, batch loss = 0.64 (29.9 examples/sec; 0.268 sec/batch; 21h:25m:13s remains)
INFO - root - 2017-12-11 09:09:29.024506: step 44440, loss = 0.71, batch loss = 0.65 (31.1 examples/sec; 0.257 sec/batch; 20h:33m:36s remains)
INFO - root - 2017-12-11 09:09:31.650708: step 44450, loss = 0.69, batch loss = 0.63 (30.6 examples/sec; 0.262 sec/batch; 20h:56m:35s remains)
INFO - root - 2017-12-11 09:09:34.224744: step 44460, loss = 0.72, batch loss = 0.66 (31.1 examples/sec; 0.257 sec/batch; 20h:34m:16s remains)
INFO - root - 2017-12-11 09:09:36.861411: step 44470, loss = 0.70, batch loss = 0.64 (30.4 examples/sec; 0.263 sec/batch; 21h:04m:10s remains)
INFO - root - 2017-12-11 09:09:39.546227: step 44480, loss = 0.69, batch loss = 0.63 (28.9 examples/sec; 0.277 sec/batch; 22h:09m:00s remains)
INFO - root - 2017-12-11 09:09:42.152162: step 44490, loss = 0.70, batch loss = 0.64 (30.9 examples/sec; 0.258 sec/batch; 20h:40m:49s remains)
INFO - root - 2017-12-11 09:09:44.789552: step 44500, loss = 0.70, batch loss = 0.64 (30.3 examples/sec; 0.264 sec/batch; 21h:07m:22s remains)
2017-12-11 09:09:45.137900: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.11060135 0.1211069 0.13264142 0.1405372 0.14114158 0.13468771 0.12254818 0.11067594 0.11308521 0.14291193 0.19814013 0.25677022 0.30618474 0.34102768 0.35466433][0.15708648 0.17935225 0.20239775 0.22031944 0.22702515 0.22077321 0.20062529 0.17261346 0.15337467 0.16147222 0.20082942 0.25218385 0.30413452 0.34911779 0.37755427][0.18254036 0.2169722 0.2551201 0.29129115 0.31686679 0.32478982 0.30717066 0.26565918 0.21776311 0.18813224 0.19081147 0.21634734 0.25981829 0.31181672 0.35792568][0.18982527 0.23402074 0.29036355 0.35623202 0.41985714 0.46350387 0.46560761 0.41768807 0.33649355 0.25435236 0.20058122 0.18266551 0.2067122 0.26149434 0.32468948][0.18526205 0.2359886 0.31371889 0.42025313 0.53908324 0.63671768 0.67181253 0.6217165 0.502764 0.35797879 0.23551573 0.16341643 0.16045167 0.21267039 0.287838][0.17602047 0.23061144 0.32949048 0.47783619 0.65343279 0.80530292 0.8723675 0.81879443 0.66280472 0.4601337 0.27611086 0.15655771 0.12983839 0.1791762 0.26198712][0.16736718 0.22410777 0.33892575 0.51851213 0.73460394 0.921743 1.0061463 0.94507784 0.76126063 0.52134758 0.30112228 0.15629993 0.11852076 0.16909675 0.25700971][0.1681425 0.22219972 0.34221709 0.53425258 0.76502895 0.961296 1.0460682 0.9770475 0.78121454 0.53063369 0.30399507 0.15971121 0.12703866 0.18511473 0.27647883][0.17511494 0.21772639 0.32734793 0.50896496 0.7262001 0.90520388 0.9751718 0.90064389 0.71107966 0.47687203 0.27329481 0.15570231 0.14618754 0.22011048 0.31564382][0.17490137 0.19706799 0.28296745 0.43611813 0.618946 0.7631889 0.81024212 0.73500049 0.56737012 0.37036073 0.21108803 0.13736363 0.16148153 0.25442523 0.35305348][0.16868794 0.16818245 0.22349615 0.3383249 0.47736153 0.58173651 0.60651392 0.53702241 0.40152264 0.25195587 0.14396355 0.11531416 0.16884953 0.27335274 0.36863184][0.14674442 0.12911697 0.15677336 0.23475714 0.33323303 0.40392038 0.41293353 0.3535381 0.25076041 0.14478008 0.07921572 0.081904806 0.14908451 0.25039777 0.33309221][0.10465697 0.077665359 0.084614515 0.13184 0.19729093 0.24365397 0.24483904 0.19836901 0.12451477 0.052477434 0.014780767 0.030950552 0.094355144 0.17840247 0.24112484][0.043954514 0.015910018 0.010219564 0.0334924 0.072175771 0.10019018 0.098215759 0.065357886 0.016016752 -0.030303041 -0.051161323 -0.034041345 0.014196706 0.073301494 0.11391539][-0.021420244 -0.04347967 -0.053641234 -0.047033265 -0.029597016 -0.016453819 -0.020115899 -0.041086942 -0.070801288 -0.097978488 -0.10917886 -0.097509265 -0.068368994 -0.034576196 -0.01352958]]...]
INFO - root - 2017-12-11 09:09:47.763581: step 44510, loss = 0.70, batch loss = 0.64 (31.0 examples/sec; 0.258 sec/batch; 20h:40m:07s remains)
INFO - root - 2017-12-11 09:09:50.394272: step 44520, loss = 0.69, batch loss = 0.63 (30.9 examples/sec; 0.258 sec/batch; 20h:40m:40s remains)
INFO - root - 2017-12-11 09:09:53.052314: step 44530, loss = 0.71, batch loss = 0.65 (30.4 examples/sec; 0.263 sec/batch; 21h:03m:33s remains)
INFO - root - 2017-12-11 09:09:55.652288: step 44540, loss = 0.70, batch loss = 0.64 (31.4 examples/sec; 0.255 sec/batch; 20h:22m:36s remains)
INFO - root - 2017-12-11 09:09:58.278421: step 44550, loss = 0.68, batch loss = 0.62 (31.0 examples/sec; 0.258 sec/batch; 20h:40m:22s remains)
INFO - root - 2017-12-11 09:10:00.858813: step 44560, loss = 0.70, batch loss = 0.64 (30.8 examples/sec; 0.260 sec/batch; 20h:47m:56s remains)
INFO - root - 2017-12-11 09:10:03.518040: step 44570, loss = 0.68, batch loss = 0.62 (29.4 examples/sec; 0.272 sec/batch; 21h:45m:26s remains)
INFO - root - 2017-12-11 09:10:06.125463: step 44580, loss = 0.70, batch loss = 0.64 (31.6 examples/sec; 0.253 sec/batch; 20h:13m:32s remains)
INFO - root - 2017-12-11 09:10:08.719144: step 44590, loss = 0.71, batch loss = 0.65 (31.6 examples/sec; 0.253 sec/batch; 20h:14m:17s remains)
INFO - root - 2017-12-11 09:10:11.392125: step 44600, loss = 0.71, batch loss = 0.65 (29.2 examples/sec; 0.274 sec/batch; 21h:54m:24s remains)
2017-12-11 09:10:11.768432: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.050741356 -0.054095827 -0.056022543 -0.058961403 -0.062510975 -0.06585075 -0.068410069 -0.07000684 -0.070020877 -0.069146 -0.068198688 -0.06780152 -0.068458728 -0.070240155 -0.072668761][-0.027683835 -0.027366612 -0.026841152 -0.028966237 -0.033533361 -0.039321214 -0.045309275 -0.050365482 -0.05303454 -0.053930782 -0.054171935 -0.055268962 -0.057968691 -0.063136704 -0.070029587][0.012549718 0.021187495 0.028391456 0.030760214 0.027483439 0.019766828 0.0089657819 -0.0029882651 -0.012245943 -0.01804634 -0.021405622 -0.02500503 -0.031202687 -0.041715294 -0.055561766][0.062269762 0.084566027 0.10422821 0.11654115 0.11814724 0.10957076 0.0928341 0.070354022 0.049674794 0.034559857 0.025262168 0.017531594 0.0062036519 -0.01190285 -0.035397351][0.1144018 0.15399413 0.19058973 0.21700746 0.22658339 0.21838722 0.1953133 0.15974095 0.12377459 0.09557578 0.077696756 0.064562425 0.047437944 0.021055162 -0.012991391][0.16704673 0.22340286 0.27571529 0.3143639 0.33103457 0.323786 0.2954568 0.24748954 0.19690597 0.15641312 0.13110179 0.11357157 0.09146335 0.057141013 0.012246468][0.21335411 0.28111932 0.34191537 0.38617432 0.40706304 0.40190154 0.37178984 0.31524086 0.25386202 0.20485362 0.17553078 0.15651865 0.13157719 0.091241978 0.03683532][0.2435682 0.31511605 0.37588605 0.418465 0.43967339 0.43645737 0.40683442 0.34585866 0.27882314 0.22730824 0.19997987 0.18426228 0.15965797 0.11645623 0.055771984][0.25441802 0.3234123 0.37783411 0.41230673 0.42775595 0.42124397 0.38970673 0.32799354 0.26316091 0.21821144 0.20059715 0.19406971 0.17500098 0.13351919 0.070640169][0.24271394 0.30369565 0.34684414 0.368166 0.37128156 0.35499796 0.31896198 0.26238233 0.21058957 0.18207018 0.17983845 0.18548541 0.17618473 0.14209427 0.083014071][0.20439975 0.25291127 0.28188381 0.28868207 0.27769276 0.25042748 0.21166022 0.16649742 0.13502666 0.12668122 0.13934335 0.15518585 0.15643153 0.13396184 0.085187107][0.13775437 0.17084394 0.18613584 0.18246904 0.16336301 0.13218223 0.097365066 0.067113891 0.054112691 0.059208948 0.077721521 0.096365 0.1043361 0.094352774 0.0615008][0.058775168 0.076854333 0.082382955 0.074946128 0.056569468 0.030384546 0.0045700436 -0.013321731 -0.016771279 -0.0091368007 0.005741199 0.020369891 0.029939787 0.029520301 0.013376717][-0.011765961 -0.0060986751 -0.0067179538 -0.014269953 -0.027859941 -0.045785692 -0.06256628 -0.073081881 -0.074395895 -0.070225082 -0.062514938 -0.053805873 -0.045386843 -0.040267017 -0.04319993][-0.063253328 -0.066937834 -0.071602352 -0.078820884 -0.088410452 -0.099786684 -0.10992181 -0.11595853 -0.11666843 -0.11502434 -0.11185081 -0.107236 -0.1007405 -0.093757853 -0.089260228]]...]
INFO - root - 2017-12-11 09:10:14.468852: step 44610, loss = 0.71, batch loss = 0.65 (31.0 examples/sec; 0.258 sec/batch; 20h:36m:55s remains)
INFO - root - 2017-12-11 09:10:17.115268: step 44620, loss = 0.71, batch loss = 0.65 (30.3 examples/sec; 0.264 sec/batch; 21h:07m:40s remains)
INFO - root - 2017-12-11 09:10:19.733419: step 44630, loss = 0.69, batch loss = 0.63 (31.4 examples/sec; 0.255 sec/batch; 20h:23m:24s remains)
INFO - root - 2017-12-11 09:10:22.369823: step 44640, loss = 0.69, batch loss = 0.63 (30.9 examples/sec; 0.259 sec/batch; 20h:43m:00s remains)
INFO - root - 2017-12-11 09:10:25.023369: step 44650, loss = 0.70, batch loss = 0.64 (31.7 examples/sec; 0.252 sec/batch; 20h:10m:43s remains)
INFO - root - 2017-12-11 09:10:27.644112: step 44660, loss = 0.69, batch loss = 0.63 (29.9 examples/sec; 0.268 sec/batch; 21h:25m:30s remains)
INFO - root - 2017-12-11 09:10:30.279618: step 44670, loss = 0.70, batch loss = 0.64 (29.9 examples/sec; 0.268 sec/batch; 21h:25m:05s remains)
INFO - root - 2017-12-11 09:10:32.922629: step 44680, loss = 0.70, batch loss = 0.64 (31.4 examples/sec; 0.254 sec/batch; 20h:20m:18s remains)
INFO - root - 2017-12-11 09:10:35.538929: step 44690, loss = 0.70, batch loss = 0.64 (29.1 examples/sec; 0.275 sec/batch; 21h:57m:58s remains)
INFO - root - 2017-12-11 09:10:38.269099: step 44700, loss = 0.71, batch loss = 0.66 (28.5 examples/sec; 0.281 sec/batch; 22h:27m:47s remains)
2017-12-11 09:10:38.655773: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.069063783 0.068383276 0.066420756 0.063249186 0.060287241 0.058708765 0.058172893 0.056813195 0.054030225 0.050725885 0.048030455 0.047484394 0.048822295 0.051002327 0.053011987][0.0375107 0.036982644 0.035872195 0.034789402 0.034337509 0.035087924 0.036484178 0.0369201 0.0360733 0.03436438 0.032618709 0.032067616 0.032287639 0.032819681 0.033828951][0.011371744 0.0099022361 0.0080917012 0.0069720419 0.0068657612 0.0081548262 0.010583442 0.012962999 0.014784916 0.015440049 0.015310072 0.015745603 0.015446777 0.014223027 0.013468376][0.006236027 0.0038891372 0.00077791896 -0.0019116164 -0.0038403412 -0.0041329404 -0.0020529351 0.0019875758 0.0066270316 0.0096900389 0.0113346 0.013537111 0.013910674 0.011450919 0.0081683043][0.029351285 0.027576344 0.023501517 0.018391892 0.012765247 0.0084666712 0.0079899728 0.012905498 0.02003116 0.024992555 0.027994147 0.032032948 0.034177661 0.031027906 0.024194174][0.085242562 0.087055549 0.082672976 0.073705293 0.06119201 0.048528761 0.041424174 0.045532543 0.055113111 0.062507793 0.067318 0.073289223 0.077137269 0.071723074 0.058012437][0.15786119 0.1659895 0.16209938 0.14881897 0.12777774 0.10402684 0.087485038 0.089135669 0.10067131 0.11071447 0.11782826 0.12568076 0.13064753 0.12129314 0.098155595][0.21479127 0.22860222 0.22501954 0.20856978 0.18132405 0.14921269 0.12529249 0.12485151 0.13776709 0.14966668 0.1583986 0.16707571 0.17167622 0.15781052 0.1261445][0.22313733 0.23867577 0.23488539 0.21782812 0.18979093 0.15643311 0.13122177 0.13071433 0.14440498 0.15684862 0.16568206 0.17335989 0.17603284 0.15895444 0.12367333][0.17291424 0.18594347 0.18227388 0.16787699 0.14502873 0.11799211 0.097711414 0.0984574 0.11127097 0.12234384 0.12980729 0.13545069 0.13620201 0.11975924 0.088420294][0.089360736 0.097448081 0.094785377 0.085391514 0.071273617 0.054899942 0.04285742 0.044824112 0.054831844 0.062910616 0.068117946 0.071729153 0.072027035 0.06057765 0.039536651][0.014871416 0.01834072 0.017316168 0.013047647 0.0072004781 0.0010253137 -0.0030999922 -0.00036661912 0.0064123031 0.011697507 0.015135584 0.017725741 0.019133374 0.014409801 0.004957038][-0.024731908 -0.024176791 -0.023886979 -0.024840282 -0.025540449 -0.025227666 -0.024233628 -0.021302411 -0.016741345 -0.01266253 -0.009587924 -0.0067483867 -0.003459757 -0.0023385067 -0.0029984503][-0.030659987 -0.031447008 -0.030906113 -0.030769665 -0.029528776 -0.026487766 -0.023189226 -0.020258363 -0.01646933 -0.012060541 -0.0083883572 -0.0049786903 -0.00043037275 0.0039082491 0.0074431091][-0.02015198 -0.021769742 -0.022061097 -0.022376426 -0.021123018 -0.017532334 -0.013466443 -0.010252142 -0.0061164582 -0.00068539433 0.0036088526 0.00688906 0.011064312 0.015724918 0.019585038]]...]
INFO - root - 2017-12-11 09:10:41.283454: step 44710, loss = 0.71, batch loss = 0.65 (31.3 examples/sec; 0.256 sec/batch; 20h:27m:22s remains)
INFO - root - 2017-12-11 09:10:43.922118: step 44720, loss = 0.70, batch loss = 0.65 (31.3 examples/sec; 0.256 sec/batch; 20h:26m:38s remains)
INFO - root - 2017-12-11 09:10:46.578503: step 44730, loss = 0.69, batch loss = 0.63 (29.4 examples/sec; 0.272 sec/batch; 21h:44m:31s remains)
INFO - root - 2017-12-11 09:10:49.217571: step 44740, loss = 0.70, batch loss = 0.64 (27.5 examples/sec; 0.291 sec/batch; 23h:14m:54s remains)
INFO - root - 2017-12-11 09:10:51.877892: step 44750, loss = 0.70, batch loss = 0.64 (30.5 examples/sec; 0.263 sec/batch; 20h:59m:03s remains)
INFO - root - 2017-12-11 09:10:54.499285: step 44760, loss = 0.71, batch loss = 0.65 (30.4 examples/sec; 0.263 sec/batch; 21h:02m:29s remains)
INFO - root - 2017-12-11 09:10:57.191768: step 44770, loss = 0.68, batch loss = 0.62 (26.5 examples/sec; 0.302 sec/batch; 24h:06m:03s remains)
INFO - root - 2017-12-11 09:10:59.836544: step 44780, loss = 0.71, batch loss = 0.65 (31.0 examples/sec; 0.258 sec/batch; 20h:36m:59s remains)
INFO - root - 2017-12-11 09:11:02.466602: step 44790, loss = 0.68, batch loss = 0.63 (27.1 examples/sec; 0.295 sec/batch; 23h:34m:00s remains)
INFO - root - 2017-12-11 09:11:05.102731: step 44800, loss = 0.71, batch loss = 0.65 (30.5 examples/sec; 0.262 sec/batch; 20h:57m:29s remains)
2017-12-11 09:11:05.451456: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.50610638 0.519524 0.52625459 0.50568622 0.45727146 0.40007937 0.3613987 0.37486896 0.42700166 0.48880294 0.53298277 0.54283375 0.50885266 0.42902255 0.33537179][0.62153769 0.61742449 0.59902942 0.54930997 0.47499707 0.40171215 0.36320436 0.39322609 0.46979991 0.55553842 0.6132372 0.61976957 0.56660193 0.45940956 0.3400045][0.68133926 0.66547996 0.6330865 0.57341313 0.4955987 0.42560121 0.3976849 0.44142306 0.52867848 0.61764818 0.66809464 0.656507 0.57823259 0.44842732 0.31361377][0.66329491 0.6544199 0.63584673 0.59963858 0.54759341 0.49910724 0.48509124 0.53015226 0.60557866 0.67167741 0.69332451 0.65233678 0.550384 0.40832555 0.27050945][0.56857073 0.58108509 0.59987479 0.61324227 0.60964119 0.59890181 0.60463125 0.64486235 0.69490421 0.72352797 0.70611781 0.63273609 0.51085168 0.36339098 0.22919667][0.41704991 0.45755649 0.52112263 0.59015214 0.63893336 0.66905457 0.69700944 0.73434013 0.76197517 0.75848 0.7093327 0.61032814 0.47340885 0.32347429 0.19628385][0.25191942 0.3136231 0.41017628 0.51851666 0.604645 0.66767257 0.71889186 0.76341903 0.78636736 0.77229756 0.71070415 0.59857732 0.45264104 0.30254534 0.18477079][0.11802941 0.1868982 0.29177037 0.40991646 0.50882012 0.59170038 0.6670953 0.7342509 0.775856 0.7744264 0.71842057 0.60501969 0.45624802 0.30834818 0.20073242][0.043555696 0.10562675 0.1963596 0.29946491 0.392361 0.48462567 0.58210927 0.67675853 0.74397624 0.76027066 0.71285832 0.60200167 0.45597261 0.31548598 0.22074018][0.031485491 0.078678757 0.14433391 0.22121523 0.29972664 0.39324814 0.50325763 0.61432219 0.69470274 0.71701676 0.67168796 0.56573361 0.43040621 0.3064751 0.22974756][0.069952637 0.10675793 0.15089282 0.20340416 0.2641418 0.34530142 0.4442195 0.54303104 0.60950834 0.61868471 0.56808841 0.47101131 0.3567535 0.26001215 0.20724903][0.14779507 0.18224734 0.213939 0.24899398 0.29050666 0.34548852 0.40914053 0.46741581 0.4953633 0.47658274 0.41621342 0.33027136 0.24150179 0.17497212 0.1460772][0.26470849 0.2980594 0.31973904 0.33880731 0.35748819 0.37673 0.39185408 0.39588544 0.375531 0.32519776 0.25506023 0.17965628 0.11310998 0.070205063 0.057099208][0.38694739 0.41864738 0.42982405 0.43258241 0.42654637 0.4087503 0.3766253 0.32974544 0.2662814 0.19050594 0.11499988 0.050177578 0.0017012102 -0.024531258 -0.028669115][0.47914791 0.507461 0.50807768 0.49465168 0.464398 0.41415608 0.34511489 0.26226473 0.17292021 0.088004619 0.019002229 -0.029457834 -0.058674607 -0.069479294 -0.065658912]]...]
INFO - root - 2017-12-11 09:11:08.083046: step 44810, loss = 0.70, batch loss = 0.64 (31.2 examples/sec; 0.257 sec/batch; 20h:30m:39s remains)
INFO - root - 2017-12-11 09:11:10.771270: step 44820, loss = 0.69, batch loss = 0.63 (29.0 examples/sec; 0.276 sec/batch; 22h:02m:12s remains)
INFO - root - 2017-12-11 09:11:13.402868: step 44830, loss = 0.70, batch loss = 0.64 (29.5 examples/sec; 0.271 sec/batch; 21h:38m:16s remains)
INFO - root - 2017-12-11 09:11:15.988093: step 44840, loss = 0.70, batch loss = 0.64 (31.3 examples/sec; 0.255 sec/batch; 20h:24m:08s remains)
INFO - root - 2017-12-11 09:11:18.580110: step 44850, loss = 0.70, batch loss = 0.64 (30.7 examples/sec; 0.261 sec/batch; 20h:49m:09s remains)
INFO - root - 2017-12-11 09:11:21.260690: step 44860, loss = 0.69, batch loss = 0.63 (29.7 examples/sec; 0.270 sec/batch; 21h:32m:01s remains)
INFO - root - 2017-12-11 09:11:23.959721: step 44870, loss = 0.69, batch loss = 0.64 (31.1 examples/sec; 0.258 sec/batch; 20h:34m:59s remains)
INFO - root - 2017-12-11 09:11:26.599421: step 44880, loss = 0.68, batch loss = 0.62 (29.7 examples/sec; 0.269 sec/batch; 21h:29m:32s remains)
INFO - root - 2017-12-11 09:11:29.185466: step 44890, loss = 0.70, batch loss = 0.64 (31.1 examples/sec; 0.257 sec/batch; 20h:31m:36s remains)
INFO - root - 2017-12-11 09:11:31.830272: step 44900, loss = 0.68, batch loss = 0.62 (30.2 examples/sec; 0.265 sec/batch; 21h:09m:37s remains)
2017-12-11 09:11:32.191330: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.006766452 0.0013749414 -0.003220567 -0.0051708589 -0.005402057 -0.0058872267 -0.0086333547 -0.013502515 -0.018204302 -0.020891614 -0.020543611 -0.017365139 -0.011510327 -0.0036329329 0.0048101777][0.034869175 0.028521555 0.024093032 0.02497801 0.028960861 0.032067697 0.030056696 0.022298265 0.012318953 0.0033340594 -0.0020055724 -0.0028022013 0.0020091878 0.012178628 0.025606209][0.064402178 0.059175439 0.058322828 0.06609749 0.078575917 0.088961564 0.08998923 0.079516113 0.062227216 0.042959429 0.026730368 0.017178794 0.017782087 0.02895741 0.047132634][0.087676674 0.086477406 0.092142582 0.11007655 0.13498047 0.15686207 0.16427687 0.15269049 0.12781349 0.096213937 0.065478124 0.042846944 0.035359003 0.044588059 0.06519708][0.10568632 0.10997252 0.12286349 0.15151568 0.19048609 0.22664118 0.24326701 0.23274942 0.20170933 0.15867105 0.11347698 0.076717362 0.059212722 0.06417089 0.084037356][0.12232952 0.13233615 0.15065578 0.1874851 0.2387782 0.28894052 0.31561077 0.30772549 0.27290779 0.22211561 0.16691838 0.11957311 0.093712375 0.094292618 0.11115891][0.14555858 0.15940616 0.1792683 0.21902989 0.27762645 0.33818907 0.37331215 0.36874866 0.33326221 0.280559 0.22264351 0.171141 0.14109039 0.1389624 0.15195084][0.18119697 0.19541885 0.21193363 0.24810807 0.30649519 0.37038374 0.40918711 0.40697658 0.37359944 0.32546541 0.27377477 0.22674321 0.19884139 0.19718708 0.20668116][0.22234438 0.23618568 0.24924017 0.28098437 0.33569637 0.39665869 0.43270844 0.42899805 0.39691749 0.35541543 0.31425202 0.27693817 0.25530219 0.25580317 0.26178956][0.25948319 0.27737111 0.29293525 0.32532117 0.37759712 0.43199658 0.4587765 0.44665945 0.40978962 0.36962071 0.33465615 0.30480549 0.28861102 0.29052716 0.29299694][0.27796713 0.30659926 0.33252499 0.3716599 0.42363772 0.46945393 0.48237428 0.45634302 0.40897295 0.36360502 0.32760277 0.29917872 0.28428236 0.28509253 0.28414336][0.26901788 0.31184611 0.35309628 0.40261123 0.4557344 0.49306875 0.49216634 0.45193827 0.39248189 0.33795521 0.29529816 0.26272756 0.24478862 0.24204393 0.23830153][0.22796056 0.28197014 0.33759192 0.39718708 0.45178226 0.48265955 0.47216493 0.42251113 0.35399091 0.29027724 0.23899449 0.1998938 0.17709053 0.17016622 0.16526566][0.15967509 0.21650581 0.27971002 0.34454456 0.39873078 0.42509598 0.41073397 0.35932925 0.28920546 0.22171624 0.16500333 0.12110417 0.094605394 0.08484631 0.080697156][0.083667509 0.13245657 0.19128074 0.25121602 0.29904845 0.32027656 0.30599144 0.26013497 0.19778982 0.136386 0.083286546 0.041676287 0.016433904 0.0072762072 0.0059622577]]...]
INFO - root - 2017-12-11 09:11:34.785778: step 44910, loss = 0.70, batch loss = 0.64 (31.5 examples/sec; 0.254 sec/batch; 20h:15m:27s remains)
INFO - root - 2017-12-11 09:11:37.410063: step 44920, loss = 0.69, batch loss = 0.63 (30.4 examples/sec; 0.264 sec/batch; 21h:03m:08s remains)
INFO - root - 2017-12-11 09:11:40.002595: step 44930, loss = 0.70, batch loss = 0.64 (30.6 examples/sec; 0.261 sec/batch; 20h:52m:34s remains)
INFO - root - 2017-12-11 09:11:42.606950: step 44940, loss = 0.69, batch loss = 0.64 (30.0 examples/sec; 0.267 sec/batch; 21h:20m:07s remains)
INFO - root - 2017-12-11 09:11:45.225195: step 44950, loss = 0.72, batch loss = 0.66 (29.8 examples/sec; 0.268 sec/batch; 21h:25m:18s remains)
INFO - root - 2017-12-11 09:11:47.823934: step 44960, loss = 0.68, batch loss = 0.62 (30.5 examples/sec; 0.263 sec/batch; 20h:58m:47s remains)
INFO - root - 2017-12-11 09:11:50.438811: step 44970, loss = 0.69, batch loss = 0.64 (29.0 examples/sec; 0.276 sec/batch; 22h:01m:29s remains)
INFO - root - 2017-12-11 09:11:53.107288: step 44980, loss = 0.70, batch loss = 0.64 (30.7 examples/sec; 0.261 sec/batch; 20h:50m:41s remains)
INFO - root - 2017-12-11 09:11:55.734573: step 44990, loss = 0.71, batch loss = 0.65 (27.3 examples/sec; 0.293 sec/batch; 23h:22m:23s remains)
INFO - root - 2017-12-11 09:11:58.332962: step 45000, loss = 0.69, batch loss = 0.63 (30.5 examples/sec; 0.262 sec/batch; 20h:57m:00s remains)
2017-12-11 09:11:58.699164: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.22272244 0.22293901 0.21121202 0.19325151 0.17728603 0.16411278 0.15679565 0.14563854 0.13671482 0.12864052 0.11968679 0.11183048 0.10623974 0.10410319 0.095962688][0.28719521 0.29180419 0.2818563 0.26549935 0.25178188 0.24088645 0.23550726 0.22747855 0.22472355 0.22356796 0.22013478 0.21460831 0.2100452 0.20899808 0.20033011][0.30673388 0.31847051 0.31589735 0.30667037 0.29912147 0.292935 0.29011294 0.28558651 0.28964752 0.2979238 0.3043021 0.30604005 0.30633888 0.30893335 0.30394226][0.29070354 0.31470171 0.326592 0.33146831 0.33503488 0.33509663 0.33303189 0.32780808 0.333827 0.34846932 0.3640976 0.3751415 0.38266683 0.39167127 0.39419776][0.26953474 0.31368691 0.34866911 0.37449989 0.392833 0.39858708 0.39311036 0.37942091 0.3768743 0.38848874 0.40789503 0.42762551 0.44414723 0.46192154 0.47343192][0.26010388 0.32876796 0.39141932 0.44152406 0.47584897 0.48666129 0.47570488 0.44876063 0.42927557 0.42804995 0.44362205 0.46820891 0.4921031 0.5162912 0.53244346][0.25691369 0.34589243 0.43363011 0.50728458 0.55850774 0.57618833 0.561686 0.52160943 0.48195168 0.46159828 0.46647975 0.49067307 0.518275 0.54474515 0.56003618][0.24199119 0.33691257 0.43753704 0.52757883 0.59423822 0.62136996 0.60777718 0.55990756 0.50512105 0.4676556 0.46009979 0.48006147 0.50750726 0.532034 0.54089624][0.2041876 0.28760603 0.38291621 0.47503489 0.54887182 0.58480346 0.57803851 0.53292948 0.475635 0.43214631 0.41758955 0.43177417 0.45362726 0.47007322 0.46767086][0.15690686 0.21826087 0.29522067 0.37637332 0.44636348 0.48581129 0.48808578 0.45463547 0.40713003 0.36903402 0.3532289 0.359074 0.36708626 0.36621031 0.34717751][0.11832177 0.15521969 0.20822746 0.27035856 0.32746702 0.36446333 0.37580678 0.36019138 0.33243594 0.3090556 0.29664895 0.29145315 0.27793092 0.25173485 0.21284536][0.10714187 0.12643491 0.15881604 0.20145373 0.24197285 0.27110505 0.28753325 0.28867188 0.28200361 0.27486262 0.26636428 0.24935615 0.21319267 0.16207254 0.10603135][0.13224077 0.14415625 0.16337828 0.19030699 0.21414952 0.23113225 0.24577372 0.25625423 0.26371852 0.26727852 0.26046145 0.23437825 0.18257332 0.11541402 0.049539451][0.17179033 0.18283848 0.19502571 0.21094072 0.22120386 0.22579555 0.23394398 0.24533172 0.25711548 0.26344 0.25582463 0.22642249 0.17133588 0.1021436 0.036592629][0.18511003 0.19700022 0.20527531 0.21404661 0.21560216 0.21099131 0.21089926 0.21650428 0.22407493 0.22678007 0.21758249 0.19127171 0.1449101 0.08727178 0.032821495]]...]
INFO - root - 2017-12-11 09:12:01.268950: step 45010, loss = 0.70, batch loss = 0.64 (30.9 examples/sec; 0.259 sec/batch; 20h:41m:04s remains)
INFO - root - 2017-12-11 09:12:03.903437: step 45020, loss = 0.71, batch loss = 0.65 (31.9 examples/sec; 0.251 sec/batch; 20h:03m:21s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-SGD-lr0.001-clip50-initfromfixqian30k-fixqian
INFO - root - 2017-12-11 09:12:06.548595: step 45030, loss = 0.68, batch loss = 0.63 (30.8 examples/sec; 0.260 sec/batch; 20h:46m:09s remains)
INFO - root - 2017-12-11 09:12:09.193933: step 45040, loss = 0.68, batch loss = 0.62 (31.4 examples/sec; 0.255 sec/batch; 20h:21m:05s remains)
INFO - root - 2017-12-11 09:12:11.832284: step 45050, loss = 0.69, batch loss = 0.63 (30.2 examples/sec; 0.265 sec/batch; 21h:11m:03s remains)
INFO - root - 2017-12-11 09:12:14.498843: step 45060, loss = 0.71, batch loss = 0.66 (31.7 examples/sec; 0.252 sec/batch; 20h:07m:12s remains)
INFO - root - 2017-12-11 09:12:17.100371: step 45070, loss = 0.69, batch loss = 0.63 (31.0 examples/sec; 0.258 sec/batch; 20h:37m:03s remains)
INFO - root - 2017-12-11 09:12:19.751788: step 45080, loss = 0.69, batch loss = 0.64 (30.7 examples/sec; 0.260 sec/batch; 20h:47m:51s remains)
INFO - root - 2017-12-11 09:12:22.361667: step 45090, loss = 0.72, batch loss = 0.66 (31.9 examples/sec; 0.251 sec/batch; 20h:00m:34s remains)
INFO - root - 2017-12-11 09:12:24.948836: step 45100, loss = 0.69, batch loss = 0.63 (30.3 examples/sec; 0.264 sec/batch; 21h:05m:13s remains)
2017-12-11 09:12:25.318447: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.40783957 0.47920978 0.50688732 0.49392822 0.45707217 0.40959847 0.35888192 0.3131744 0.28409994 0.27792171 0.28288195 0.28623798 0.28959644 0.30073965 0.32664376][0.55100513 0.6214869 0.63306612 0.5903517 0.51081127 0.413607 0.31557977 0.23633793 0.19717734 0.20817815 0.25366837 0.31172085 0.37130222 0.42736408 0.47715443][0.60110021 0.65480131 0.64616871 0.581667 0.47921768 0.36205414 0.25272027 0.17535973 0.15430211 0.19540989 0.27807617 0.37562659 0.46988222 0.54799449 0.59852743][0.5339694 0.56496352 0.54476893 0.48098969 0.39029005 0.29444411 0.21346125 0.16800423 0.17824712 0.24238428 0.33724642 0.4388971 0.53154379 0.60222894 0.63427275][0.38179392 0.39858872 0.3867709 0.35279876 0.30796692 0.26572976 0.2363797 0.23153837 0.26324683 0.32436502 0.39441517 0.45974165 0.51490283 0.55350167 0.55842412][0.21066895 0.22985023 0.24749433 0.26601917 0.2867173 0.30979127 0.33231997 0.35588387 0.38509998 0.41345859 0.42928645 0.43182969 0.42755458 0.42009577 0.39711159][0.086509839 0.12497822 0.18573001 0.26407716 0.34873524 0.42545277 0.4808439 0.50963926 0.51288587 0.49022552 0.44084975 0.3755044 0.30971205 0.25565803 0.20913175][0.040358309 0.1083881 0.2128417 0.33857107 0.46260136 0.562739 0.62219888 0.63346016 0.59842896 0.52440345 0.4206886 0.30444759 0.19518684 0.10932809 0.048772007][0.055093095 0.14845689 0.28023225 0.42465553 0.55218488 0.64162332 0.68095756 0.66487604 0.59691787 0.48987702 0.35882032 0.22245301 0.099655092 0.0059761051 -0.054456241][0.083350241 0.18289985 0.31301767 0.44260198 0.54301995 0.60040975 0.6113804 0.57426113 0.49329022 0.3824372 0.25718933 0.13351355 0.026175745 -0.052917115 -0.10037552][0.088498749 0.17051065 0.27123964 0.36243019 0.42264798 0.44602045 0.4355391 0.39189145 0.31954175 0.23042774 0.1365975 0.048780505 -0.024112839 -0.0746939 -0.10137769][0.067112766 0.11677474 0.1744141 0.22067373 0.24350891 0.24256694 0.22271724 0.18517014 0.13459411 0.07953313 0.027042603 -0.017965633 -0.05155981 -0.070239224 -0.074696749][0.029938038 0.046210572 0.064473815 0.075278312 0.073797904 0.06168475 0.043160494 0.019069605 -0.0067691561 -0.029518234 -0.046348073 -0.056101847 -0.057888851 -0.050950486 -0.037807159][-0.00964536 -0.016530175 -0.022286523 -0.029930871 -0.040511642 -0.052478921 -0.063060522 -0.072398409 -0.078473441 -0.079335332 -0.074651912 -0.064471573 -0.048733387 -0.027686961 -0.0047241291][-0.038005847 -0.053992666 -0.067543589 -0.079431236 -0.089455813 -0.096431978 -0.098649934 -0.09704835 -0.091709189 -0.0825476 -0.070036165 -0.054002248 -0.03403395 -0.010246138 0.014302359]]...]
INFO - root - 2017-12-11 09:12:27.897558: step 45110, loss = 0.68, batch loss = 0.62 (30.3 examples/sec; 0.264 sec/batch; 21h:03m:27s remains)
INFO - root - 2017-12-11 09:12:30.517496: step 45120, loss = 0.70, batch loss = 0.64 (30.8 examples/sec; 0.260 sec/batch; 20h:45m:34s remains)
INFO - root - 2017-12-11 09:12:33.160594: step 45130, loss = 0.70, batch loss = 0.64 (27.7 examples/sec; 0.289 sec/batch; 23h:05m:10s remains)
INFO - root - 2017-12-11 09:12:35.847276: step 45140, loss = 0.71, batch loss = 0.65 (29.8 examples/sec; 0.268 sec/batch; 21h:24m:24s remains)
INFO - root - 2017-12-11 09:12:38.518681: step 45150, loss = 0.70, batch loss = 0.64 (30.8 examples/sec; 0.259 sec/batch; 20h:42m:36s remains)
INFO - root - 2017-12-11 09:12:41.149589: step 45160, loss = 0.71, batch loss = 0.65 (31.3 examples/sec; 0.256 sec/batch; 20h:24m:42s remains)
INFO - root - 2017-12-11 09:12:43.774983: step 45170, loss = 0.70, batch loss = 0.64 (31.1 examples/sec; 0.257 sec/batch; 20h:31m:45s remains)
INFO - root - 2017-12-11 09:12:46.403566: step 45180, loss = 0.68, batch loss = 0.63 (29.5 examples/sec; 0.271 sec/batch; 21h:36m:27s remains)
INFO - root - 2017-12-11 09:12:49.006049: step 45190, loss = 0.69, batch loss = 0.64 (31.2 examples/sec; 0.257 sec/batch; 20h:28m:48s remains)
INFO - root - 2017-12-11 09:12:51.665682: step 45200, loss = 0.70, batch loss = 0.64 (31.0 examples/sec; 0.258 sec/batch; 20h:34m:21s remains)
2017-12-11 09:12:52.025977: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.12215237 0.12390117 0.1146633 0.10264612 0.095727228 0.099236473 0.11144587 0.12268157 0.12568559 0.12000915 0.10951958 0.10553422 0.11176383 0.12418886 0.13690914][0.1211721 0.12400698 0.11842321 0.11198728 0.11326188 0.12715223 0.1494623 0.16712779 0.17267536 0.16660154 0.15306859 0.14248243 0.13952632 0.14181055 0.14376658][0.11546867 0.11878518 0.11891063 0.12194404 0.13433686 0.15827245 0.18748699 0.20850091 0.21537796 0.20897126 0.19222993 0.17171882 0.1525901 0.13681076 0.12384178][0.099409692 0.10435746 0.11278513 0.12772404 0.15174009 0.18337815 0.21560974 0.23700029 0.24330711 0.23494145 0.21342616 0.18155183 0.14529131 0.1121681 0.087853916][0.071955919 0.079467632 0.095964439 0.12099569 0.15346421 0.18932037 0.22129841 0.24071743 0.2447931 0.23326598 0.20710015 0.16675089 0.11910502 0.076300472 0.048936397][0.044678155 0.055648606 0.077015489 0.10618033 0.14039484 0.17517045 0.20321922 0.21823807 0.21927714 0.20610183 0.17926066 0.13749574 0.088156924 0.045806635 0.023663338][0.031707641 0.04587096 0.068062134 0.095040932 0.12424496 0.15228941 0.17259383 0.18092278 0.1790908 0.16770144 0.14575273 0.1099719 0.067509919 0.0333746 0.020662583][0.035628419 0.052111696 0.072764389 0.093676366 0.11316307 0.13042362 0.14103724 0.14246018 0.13879493 0.13155693 0.11796368 0.092692763 0.061731271 0.039127968 0.035953369][0.055148616 0.074395664 0.092479154 0.10560872 0.11350585 0.11935056 0.12149975 0.11838204 0.11419454 0.11080606 0.10478638 0.089669988 0.069636069 0.057089839 0.0608632][0.086328916 0.10735434 0.12266686 0.12914406 0.12768623 0.12521765 0.1222869 0.11742932 0.11280343 0.11011376 0.1074479 0.098870441 0.086250983 0.07976824 0.086827986][0.11768266 0.14020105 0.15491889 0.15896223 0.15297371 0.14581132 0.13984486 0.13380326 0.12743631 0.12269206 0.12022748 0.11565112 0.10785785 0.10453167 0.11212154][0.13908179 0.16287519 0.17950995 0.18497741 0.17824283 0.16815604 0.15891942 0.15017781 0.14070828 0.1331106 0.1303629 0.1296864 0.12676826 0.12580825 0.13199778][0.1447292 0.16857974 0.18677188 0.19330104 0.18555316 0.17193048 0.15856421 0.1464102 0.13436238 0.12551352 0.12430678 0.12917483 0.13312198 0.1361714 0.14148216][0.12916814 0.14976044 0.16604224 0.171215 0.16220722 0.14640543 0.13070878 0.11701835 0.10474632 0.097116292 0.098808505 0.10903954 0.11924371 0.12606548 0.13083751][0.091806084 0.10646754 0.11854001 0.12140633 0.11224906 0.096738622 0.081497289 0.06877628 0.058261089 0.052816402 0.056538384 0.069143087 0.082219712 0.090748049 0.095025606]]...]
INFO - root - 2017-12-11 09:12:54.615065: step 45210, loss = 0.69, batch loss = 0.63 (31.3 examples/sec; 0.255 sec/batch; 20h:22m:05s remains)
INFO - root - 2017-12-11 09:12:57.242797: step 45220, loss = 0.70, batch loss = 0.64 (30.8 examples/sec; 0.259 sec/batch; 20h:41m:54s remains)
INFO - root - 2017-12-11 09:12:59.853283: step 45230, loss = 0.72, batch loss = 0.66 (31.3 examples/sec; 0.256 sec/batch; 20h:24m:43s remains)
INFO - root - 2017-12-11 09:13:02.554275: step 45240, loss = 0.71, batch loss = 0.65 (30.2 examples/sec; 0.265 sec/batch; 21h:08m:50s remains)
INFO - root - 2017-12-11 09:13:05.198786: step 45250, loss = 0.69, batch loss = 0.63 (30.0 examples/sec; 0.266 sec/batch; 21h:14m:50s remains)
INFO - root - 2017-12-11 09:13:07.869324: step 45260, loss = 0.69, batch loss = 0.63 (27.9 examples/sec; 0.287 sec/batch; 22h:54m:04s remains)
INFO - root - 2017-12-11 09:13:10.522763: step 45270, loss = 0.71, batch loss = 0.65 (30.0 examples/sec; 0.266 sec/batch; 21h:14m:46s remains)
INFO - root - 2017-12-11 09:13:13.156495: step 45280, loss = 0.70, batch loss = 0.65 (30.2 examples/sec; 0.265 sec/batch; 21h:06m:57s remains)
INFO - root - 2017-12-11 09:13:15.788808: step 45290, loss = 0.70, batch loss = 0.65 (31.2 examples/sec; 0.256 sec/batch; 20h:26m:57s remains)
INFO - root - 2017-12-11 09:13:18.403206: step 45300, loss = 0.70, batch loss = 0.64 (30.1 examples/sec; 0.266 sec/batch; 21h:13m:16s remains)
2017-12-11 09:13:18.772169: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.27575725 0.26031017 0.22230193 0.19017613 0.17736417 0.17993659 0.18720417 0.1872938 0.16474681 0.11118753 0.044036694 -0.014329434 -0.050134715 -0.069041215 -0.078158371][0.38159817 0.3662385 0.32202736 0.28617224 0.27468202 0.28231198 0.2946018 0.29237506 0.25468853 0.17601268 0.083510578 0.00537933 -0.043343361 -0.069117337 -0.080952652][0.42879266 0.4214958 0.38505319 0.35820997 0.35681978 0.37265956 0.38693288 0.37625605 0.31908983 0.21547 0.10099796 0.0083536152 -0.047658496 -0.075798586 -0.086158842][0.39632547 0.39903185 0.37995616 0.37243274 0.38879582 0.4165985 0.43344826 0.41418558 0.34030467 0.21866035 0.090371244 -0.0088072438 -0.065894961 -0.090958364 -0.095293961][0.307933 0.31827986 0.31904891 0.33536917 0.37314859 0.41584998 0.43806836 0.41456112 0.33162582 0.20307899 0.0716468 -0.027416078 -0.082542181 -0.10343893 -0.10189668][0.24021052 0.251828 0.26641849 0.3022444 0.35686365 0.41033116 0.43501279 0.40730032 0.31954733 0.19160885 0.063471913 -0.033516757 -0.087779649 -0.10771248 -0.10459365][0.24117218 0.24722452 0.2684176 0.31990698 0.39061347 0.45550948 0.48392785 0.45371139 0.36180204 0.23161167 0.099163279 -0.0065829242 -0.071117483 -0.099690817 -0.10213947][0.28733674 0.28633374 0.31019935 0.37443092 0.46052638 0.53794831 0.57254416 0.542928 0.44813874 0.31267 0.16849966 0.04466214 -0.038489275 -0.082224704 -0.095000118][0.33954504 0.33069846 0.35000405 0.41503409 0.50358307 0.58270162 0.618116 0.59000641 0.49804708 0.36494622 0.21760331 0.084347188 -0.010359474 -0.064810827 -0.086435556][0.3610701 0.34406841 0.35273346 0.40601158 0.48253083 0.5514971 0.58265233 0.55823523 0.47749844 0.35843885 0.22206342 0.094119877 0.0002584 -0.055968173 -0.081266828][0.32015607 0.29818606 0.29714286 0.33519888 0.39511159 0.45086148 0.47744307 0.45981258 0.39653823 0.29935494 0.18358485 0.072274648 -0.0099757845 -0.059358377 -0.082287125][0.22512001 0.20232062 0.1966323 0.22271121 0.26837838 0.31312317 0.33694082 0.32771611 0.28291377 0.20882508 0.11689563 0.028027391 -0.036122363 -0.072725162 -0.088139817][0.10163455 0.08143279 0.075597122 0.092515647 0.12441521 0.15703294 0.17607754 0.1724405 0.14320228 0.091573067 0.026686264 -0.034153529 -0.074435763 -0.093204848 -0.096885368][-0.0064935838 -0.022570752 -0.027954431 -0.021060053 -0.00589341 0.0098817712 0.018874882 0.015775567 -0.0014028645 -0.031281162 -0.067143075 -0.0970881 -0.11135614 -0.11119072 -0.10298163][-0.075055636 -0.086532049 -0.091100514 -0.091515616 -0.089021973 -0.086080313 -0.085008413 -0.08816167 -0.096447892 -0.10964028 -0.12356161 -0.13118494 -0.12812375 -0.1166332 -0.10241336]]...]
INFO - root - 2017-12-11 09:13:21.412318: step 45310, loss = 0.71, batch loss = 0.65 (29.6 examples/sec; 0.270 sec/batch; 21h:33m:19s remains)
INFO - root - 2017-12-11 09:13:24.045924: step 45320, loss = 0.72, batch loss = 0.66 (31.7 examples/sec; 0.252 sec/batch; 20h:06m:24s remains)
INFO - root - 2017-12-11 09:13:26.627578: step 45330, loss = 0.71, batch loss = 0.65 (30.1 examples/sec; 0.265 sec/batch; 21h:10m:19s remains)
INFO - root - 2017-12-11 09:13:29.293651: step 45340, loss = 0.70, batch loss = 0.64 (29.1 examples/sec; 0.275 sec/batch; 21h:56m:10s remains)
INFO - root - 2017-12-11 09:13:31.911812: step 45350, loss = 0.70, batch loss = 0.64 (31.4 examples/sec; 0.255 sec/batch; 20h:20m:36s remains)
INFO - root - 2017-12-11 09:13:34.554975: step 45360, loss = 0.70, batch loss = 0.64 (29.9 examples/sec; 0.268 sec/batch; 21h:20m:15s remains)
INFO - root - 2017-12-11 09:13:37.165730: step 45370, loss = 0.70, batch loss = 0.64 (30.7 examples/sec; 0.260 sec/batch; 20h:45m:28s remains)
INFO - root - 2017-12-11 09:13:39.792857: step 45380, loss = 0.72, batch loss = 0.66 (30.6 examples/sec; 0.262 sec/batch; 20h:52m:16s remains)
INFO - root - 2017-12-11 09:13:42.439199: step 45390, loss = 0.69, batch loss = 0.63 (31.5 examples/sec; 0.254 sec/batch; 20h:15m:54s remains)
INFO - root - 2017-12-11 09:13:45.066724: step 45400, loss = 0.70, batch loss = 0.64 (30.9 examples/sec; 0.259 sec/batch; 20h:37m:35s remains)
2017-12-11 09:13:45.427068: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.057277415 -0.059011832 -0.060155597 -0.061070584 -0.060194444 -0.057783756 -0.055783313 -0.056239422 -0.05887356 -0.062236045 -0.064448752 -0.063364081 -0.057966113 -0.049170125 -0.039434031][-0.053625308 -0.054645065 -0.0549928 -0.05432725 -0.050597057 -0.044943485 -0.040650919 -0.040332727 -0.043944128 -0.049997721 -0.056215178 -0.059297878 -0.057155285 -0.051258113 -0.045077097][-0.049596358 -0.046446085 -0.040751573 -0.032119296 -0.019226246 -0.00512999 0.0047179447 0.006823054 0.00088469 -0.011251274 -0.026526216 -0.039686784 -0.0465321 -0.047841042 -0.047141917][-0.043991111 -0.033266131 -0.016543839 0.0056933262 0.033075549 0.059867784 0.078055657 0.082781874 0.073293917 0.05193403 0.022853592 -0.006086247 -0.02749495 -0.040068351 -0.046473775][-0.034255277 -0.014867957 0.015259503 0.054883342 0.10152125 0.14599495 0.17675629 0.18601523 0.17202455 0.13814917 0.090729512 0.041086406 0.00052706816 -0.026641948 -0.04203447][-0.019522382 0.00864048 0.052864473 0.11156878 0.17982459 0.24448976 0.28994855 0.30452615 0.28502402 0.23669782 0.16898465 0.096636094 0.03504223 -0.0083441092 -0.03424393][0.00414151 0.04104498 0.098472908 0.17488471 0.26285011 0.34525695 0.40276659 0.41967157 0.39164102 0.32685202 0.23830284 0.14450452 0.064065024 0.0067137913 -0.028139673][0.040594179 0.084015191 0.14990677 0.23776995 0.33792573 0.43011928 0.49188945 0.50442535 0.46329224 0.38055295 0.27334186 0.16344807 0.0714556 0.0075931326 -0.030128693][0.095044173 0.14169841 0.20855962 0.2973595 0.3973991 0.48698208 0.54205996 0.542813 0.48593813 0.38730693 0.2675004 0.15042873 0.056921855 -0.0046063694 -0.038509775][0.16336583 0.21278895 0.27599958 0.357157 0.44563898 0.52010792 0.55750132 0.54003894 0.46684608 0.35661963 0.23145075 0.11568723 0.028624605 -0.024624437 -0.050838724][0.23277335 0.28586856 0.34269172 0.40864035 0.47413927 0.52096987 0.53103811 0.4919053 0.40583324 0.29207781 0.17117481 0.065421052 -0.0087423557 -0.049211688 -0.064225517][0.29168656 0.34497377 0.39011377 0.43338138 0.46774957 0.48063424 0.46161109 0.40470815 0.3145808 0.20813125 0.10160108 0.013671556 -0.042078722 -0.065101951 -0.0640507][0.32323715 0.36862528 0.39634177 0.41389975 0.4181155 0.40275851 0.36497933 0.30417788 0.22546467 0.14048377 0.059026103 -0.0046094819 -0.039065331 -0.043509837 -0.025790894][0.32116097 0.35181311 0.36095124 0.35735482 0.34184015 0.31305537 0.27345452 0.22533539 0.17231581 0.11937842 0.069168672 0.031546593 0.016396744 0.026748328 0.057223994][0.28699556 0.30103683 0.29713118 0.28424114 0.26487854 0.23910049 0.21134314 0.18489598 0.1617244 0.14150034 0.12142413 0.10762513 0.10904747 0.13140924 0.17205973]]...]
INFO - root - 2017-12-11 09:13:48.094191: step 45410, loss = 0.71, batch loss = 0.66 (30.3 examples/sec; 0.264 sec/batch; 21h:04m:49s remains)
INFO - root - 2017-12-11 09:13:50.681310: step 45420, loss = 0.71, batch loss = 0.65 (30.3 examples/sec; 0.264 sec/batch; 21h:04m:20s remains)
INFO - root - 2017-12-11 09:13:53.365594: step 45430, loss = 0.69, batch loss = 0.63 (30.0 examples/sec; 0.267 sec/batch; 21h:16m:39s remains)
INFO - root - 2017-12-11 09:13:56.035386: step 45440, loss = 0.71, batch loss = 0.65 (31.6 examples/sec; 0.253 sec/batch; 20h:10m:32s remains)
INFO - root - 2017-12-11 09:13:58.605842: step 45450, loss = 0.70, batch loss = 0.65 (30.9 examples/sec; 0.259 sec/batch; 20h:40m:25s remains)
INFO - root - 2017-12-11 09:14:01.233895: step 45460, loss = 0.70, batch loss = 0.65 (30.8 examples/sec; 0.260 sec/batch; 20h:42m:17s remains)
INFO - root - 2017-12-11 09:14:03.873476: step 45470, loss = 0.69, batch loss = 0.63 (30.9 examples/sec; 0.259 sec/batch; 20h:37m:30s remains)
INFO - root - 2017-12-11 09:14:06.571175: step 45480, loss = 0.70, batch loss = 0.64 (27.7 examples/sec; 0.289 sec/batch; 23h:02m:23s remains)
INFO - root - 2017-12-11 09:14:09.230914: step 45490, loss = 0.69, batch loss = 0.63 (30.9 examples/sec; 0.259 sec/batch; 20h:37m:14s remains)
INFO - root - 2017-12-11 09:14:11.835951: step 45500, loss = 0.67, batch loss = 0.61 (29.9 examples/sec; 0.268 sec/batch; 21h:20m:40s remains)
2017-12-11 09:14:12.218084: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.092804775 0.12082329 0.12992534 0.11892215 0.094560832 0.069384575 0.055703297 0.059068587 0.08000011 0.11445677 0.15036595 0.17405933 0.17903672 0.1664581 0.14567658][0.10156841 0.13900942 0.15871802 0.15954502 0.1469717 0.12866521 0.1122551 0.10196766 0.1018559 0.11513143 0.13538098 0.14978123 0.15123579 0.1387589 0.11954511][0.11253016 0.15774032 0.18716386 0.20065397 0.20306392 0.19593559 0.17961082 0.15543421 0.13030542 0.11550777 0.11212522 0.11289454 0.11102063 0.10268706 0.090000711][0.12339676 0.17617433 0.21448335 0.2388747 0.25521567 0.26131409 0.25100076 0.22164439 0.18063326 0.14331031 0.11821476 0.10486758 0.098671786 0.093635857 0.088155888][0.13421644 0.19454812 0.24133314 0.27501386 0.30294478 0.32181779 0.32097048 0.29371205 0.24566868 0.19289921 0.1492997 0.1213678 0.10866793 0.10508239 0.10580238][0.14081654 0.20567021 0.25777853 0.29715118 0.33272073 0.36211336 0.3726733 0.35296997 0.30399594 0.23993137 0.17838617 0.1340044 0.11188715 0.10765061 0.11383723][0.14002623 0.2028715 0.25384432 0.29327464 0.33156416 0.3689355 0.39208803 0.38430631 0.33951613 0.26803192 0.18975523 0.12731922 0.093220659 0.0857085 0.095950015][0.12708367 0.18017612 0.22198534 0.25437453 0.28925356 0.32981357 0.36364222 0.36993995 0.33625495 0.26748207 0.18268472 0.10851894 0.062620118 0.04776787 0.056699648][0.10747545 0.14603844 0.17293079 0.19211544 0.21695365 0.25384697 0.293422 0.31454384 0.30052307 0.24924159 0.17463161 0.1006426 0.046472989 0.020211412 0.019360764][0.095350586 0.11795336 0.12766461 0.13084501 0.14167634 0.16955146 0.20990416 0.24458024 0.25423735 0.22991742 0.17769532 0.11486039 0.058783721 0.021915453 0.0069460222][0.091638595 0.10190359 0.098794222 0.090830587 0.091232434 0.11069865 0.14911377 0.19247136 0.22073683 0.22068287 0.19216438 0.1451388 0.092962682 0.049726129 0.022482352][0.099431537 0.10515415 0.097690143 0.086519577 0.082277469 0.095231421 0.12817496 0.17096719 0.20530398 0.2172485 0.20409162 0.17079929 0.12649924 0.083701111 0.051055346][0.12100619 0.13053037 0.12542027 0.11536292 0.10867742 0.11461981 0.1377295 0.17076471 0.198968 0.21043603 0.20285371 0.17871204 0.14351915 0.10636001 0.074973665][0.15218708 0.17256355 0.17399351 0.16584061 0.15594138 0.15396424 0.16621409 0.18662114 0.20280449 0.20481409 0.19293809 0.17055579 0.14129591 0.1108419 0.084435262][0.17974354 0.21298811 0.22142243 0.21375366 0.1996848 0.19147794 0.19715649 0.2091441 0.2139771 0.20206314 0.17835398 0.15075278 0.12297529 0.098222315 0.078571]]...]
INFO - root - 2017-12-11 09:14:14.836144: step 45510, loss = 0.70, batch loss = 0.64 (30.2 examples/sec; 0.265 sec/batch; 21h:08m:46s remains)
INFO - root - 2017-12-11 09:14:17.480006: step 45520, loss = 0.69, batch loss = 0.63 (30.9 examples/sec; 0.259 sec/batch; 20h:39m:38s remains)
INFO - root - 2017-12-11 09:14:20.106636: step 45530, loss = 0.69, batch loss = 0.63 (30.5 examples/sec; 0.262 sec/batch; 20h:53m:45s remains)
INFO - root - 2017-12-11 09:14:22.749998: step 45540, loss = 0.69, batch loss = 0.63 (30.3 examples/sec; 0.264 sec/batch; 21h:01m:32s remains)
INFO - root - 2017-12-11 09:14:25.386620: step 45550, loss = 0.69, batch loss = 0.64 (31.2 examples/sec; 0.256 sec/batch; 20h:24m:48s remains)
INFO - root - 2017-12-11 09:14:27.995099: step 45560, loss = 0.71, batch loss = 0.65 (30.3 examples/sec; 0.264 sec/batch; 21h:04m:12s remains)
INFO - root - 2017-12-11 09:14:30.589958: step 45570, loss = 0.70, batch loss = 0.65 (30.7 examples/sec; 0.261 sec/batch; 20h:47m:32s remains)
INFO - root - 2017-12-11 09:14:33.198904: step 45580, loss = 0.69, batch loss = 0.64 (30.5 examples/sec; 0.262 sec/batch; 20h:54m:51s remains)
INFO - root - 2017-12-11 09:14:35.797099: step 45590, loss = 0.70, batch loss = 0.64 (31.3 examples/sec; 0.255 sec/batch; 20h:20m:32s remains)
INFO - root - 2017-12-11 09:14:38.485966: step 45600, loss = 0.68, batch loss = 0.62 (30.0 examples/sec; 0.266 sec/batch; 21h:13m:16s remains)
2017-12-11 09:14:38.865583: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.13250253 0.11745676 0.099394515 0.079500206 0.060395658 0.046322361 0.040911734 0.040805459 0.042364027 0.045267358 0.049093716 0.053281639 0.056844451 0.0597039 0.069878511][0.17283864 0.16053039 0.14150321 0.11486146 0.085196652 0.060783241 0.048467733 0.0441258 0.042540178 0.043517854 0.047092471 0.053068768 0.059850246 0.066176467 0.08086399][0.17827067 0.18055677 0.17353383 0.15149802 0.11888625 0.08742018 0.066739716 0.05365362 0.042575072 0.034425903 0.030909861 0.033423394 0.039071836 0.044691708 0.058952838][0.14322147 0.17038728 0.19035088 0.19043864 0.1708127 0.1427622 0.11674464 0.091870166 0.064451844 0.038131759 0.018038755 0.0073903585 0.0027369463 0.00011346055 0.0062341234][0.086685278 0.13737744 0.18954237 0.22532161 0.23648888 0.22788312 0.20727471 0.17541404 0.13125373 0.081699304 0.037612323 0.0059168935 -0.016378151 -0.034421094 -0.04421746][0.034199979 0.096284278 0.17230771 0.24372464 0.29361218 0.31633365 0.31446832 0.28787965 0.23636216 0.16839975 0.10122045 0.047282577 0.0054562534 -0.030496141 -0.059391156][0.011171639 0.067666754 0.14737007 0.23663986 0.31581405 0.37054422 0.39730006 0.39073336 0.34802291 0.27598998 0.19658403 0.12750699 0.070562258 0.020282013 -0.024731744][0.035416849 0.073013678 0.13232602 0.20938054 0.29133692 0.36330038 0.41765624 0.4409675 0.42298129 0.36542487 0.29092366 0.21980794 0.15710443 0.10073756 0.048265688][0.11163129 0.12727065 0.14918576 0.18650159 0.24043794 0.30421102 0.36945069 0.41711324 0.42888242 0.39900678 0.34540689 0.28677839 0.23037882 0.17873919 0.13119523][0.22392805 0.22528769 0.2084509 0.19591478 0.2045957 0.23944293 0.29383692 0.34887704 0.38259006 0.38290676 0.35720366 0.31746188 0.27194762 0.22994676 0.19679256][0.33324856 0.33560508 0.29572177 0.24519154 0.21191478 0.21084218 0.24050444 0.28761441 0.33228946 0.35739082 0.35559189 0.32895577 0.28797776 0.25071564 0.2311641][0.39515957 0.41142648 0.37174067 0.30885634 0.25202653 0.22098704 0.22254241 0.25300124 0.29867089 0.33854 0.35107511 0.32896712 0.28597593 0.24708378 0.23380272][0.38831156 0.42171949 0.3997483 0.34905413 0.29104891 0.24453004 0.22428215 0.23673487 0.27588379 0.31938881 0.33691058 0.31538016 0.27033845 0.22805144 0.21258542][0.32184455 0.36290154 0.36192867 0.33590785 0.29429021 0.25014755 0.22191237 0.22185053 0.25108019 0.28943115 0.30536684 0.28428671 0.24067467 0.19710344 0.17590986][0.23175092 0.26582947 0.27522102 0.26900184 0.24765529 0.21744847 0.1945626 0.1904262 0.21023463 0.23917475 0.25042444 0.23109372 0.1927844 0.15264185 0.12950931]]...]
INFO - root - 2017-12-11 09:14:41.531585: step 45610, loss = 0.71, batch loss = 0.65 (29.9 examples/sec; 0.268 sec/batch; 21h:20m:43s remains)
INFO - root - 2017-12-11 09:14:44.210086: step 45620, loss = 0.70, batch loss = 0.64 (29.2 examples/sec; 0.274 sec/batch; 21h:49m:43s remains)
INFO - root - 2017-12-11 09:14:46.906044: step 45630, loss = 0.70, batch loss = 0.64 (31.2 examples/sec; 0.257 sec/batch; 20h:27m:49s remains)
INFO - root - 2017-12-11 09:14:49.729271: step 45640, loss = 0.69, batch loss = 0.63 (31.4 examples/sec; 0.255 sec/batch; 20h:19m:41s remains)
INFO - root - 2017-12-11 09:14:54.351453: step 45650, loss = 0.71, batch loss = 0.65 (14.2 examples/sec; 0.562 sec/batch; 44h:48m:53s remains)
INFO - root - 2017-12-11 09:14:59.779347: step 45660, loss = 0.70, batch loss = 0.64 (14.8 examples/sec; 0.542 sec/batch; 43h:10m:11s remains)
INFO - root - 2017-12-11 09:15:05.151357: step 45670, loss = 0.70, batch loss = 0.64 (15.0 examples/sec; 0.534 sec/batch; 42h:32m:52s remains)
INFO - root - 2017-12-11 09:15:10.490638: step 45680, loss = 0.67, batch loss = 0.62 (15.0 examples/sec; 0.535 sec/batch; 42h:37m:19s remains)
INFO - root - 2017-12-11 09:15:15.930095: step 45690, loss = 0.69, batch loss = 0.64 (14.6 examples/sec; 0.549 sec/batch; 43h:44m:18s remains)
INFO - root - 2017-12-11 09:15:21.503583: step 45700, loss = 0.71, batch loss = 0.65 (14.0 examples/sec; 0.571 sec/batch; 45h:30m:28s remains)
2017-12-11 09:15:22.095389: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.043181252 -0.040082119 -0.0347409 -0.028523665 -0.023380835 -0.021128716 -0.02115564 -0.021723391 -0.021862989 -0.021033013 -0.019214002 -0.017633896 -0.017242758 -0.017283881 -0.016400341][-0.035039838 -0.025385797 -0.011274478 0.0042909854 0.016887931 0.022818891 0.022938596 0.020244554 0.016396411 0.013142205 0.011476645 0.010681598 0.0095674479 0.0085676927 0.009011047][-0.021641692 -0.0017388545 0.026198097 0.056669924 0.082115479 0.096014813 0.098510563 0.09349741 0.083619356 0.072409153 0.063034005 0.056825459 0.052784253 0.0505292 0.050542973][-0.0071279756 0.025243012 0.070318162 0.11939199 0.16145124 0.18658248 0.19288832 0.18457596 0.16607964 0.14408232 0.12479958 0.11187544 0.10507027 0.10170463 0.1000843][0.0052958988 0.049146321 0.11032434 0.17715113 0.23579144 0.27287239 0.28348306 0.27196029 0.24540994 0.21412398 0.18718363 0.17047611 0.16489054 0.16397645 0.16202888][0.013922662 0.0659154 0.13905287 0.21933483 0.29138407 0.3388547 0.35382119 0.34052914 0.30846256 0.27116138 0.24066426 0.22591151 0.22965629 0.24091129 0.24765469][0.019230852 0.075525261 0.155768 0.24425556 0.32542953 0.381141 0.40140605 0.38956013 0.35581344 0.31666613 0.28721246 0.2800003 0.30015281 0.3326546 0.35776612][0.021380357 0.079022221 0.16307154 0.25602114 0.3427715 0.40503255 0.43214303 0.42472154 0.39200333 0.35360587 0.32810932 0.3314873 0.3699432 0.42506415 0.47066846][0.019510804 0.07475435 0.15738396 0.24865946 0.33445191 0.39821488 0.43013477 0.42784545 0.39862233 0.36449593 0.34620631 0.36075404 0.41313863 0.48290756 0.541482][0.012779847 0.061668538 0.13624221 0.21856703 0.29636773 0.3554773 0.38767353 0.38935217 0.36575106 0.33868808 0.32811853 0.34981507 0.4068203 0.47878355 0.53906494][0.0030216523 0.044657383 0.10959902 0.18175702 0.25020087 0.30250174 0.33190286 0.33509675 0.31566778 0.29329738 0.28555122 0.30616137 0.35663509 0.41857964 0.47024328][-0.0078235632 0.027508309 0.084224582 0.14829844 0.20921451 0.25535175 0.28104562 0.28404135 0.26681104 0.24624805 0.23712343 0.25040939 0.28553048 0.32702681 0.36006123][-0.019439721 0.0095582968 0.057856161 0.11360748 0.16702168 0.20711085 0.22861302 0.23029749 0.21412227 0.19413508 0.18247788 0.18670039 0.20408463 0.22229934 0.23367888][-0.030733574 -0.0090737632 0.028748851 0.073549449 0.11731657 0.15004236 0.1665684 0.16665938 0.15227738 0.13421232 0.12150202 0.11881944 0.12202398 0.12180653 0.11642931][-0.040403873 -0.026769703 -0.00033532525 0.032109324 0.064715087 0.088978492 0.10021336 0.098873995 0.086814106 0.071556218 0.059468303 0.053213853 0.048713937 0.0396849 0.027549461]]...]
INFO - root - 2017-12-11 09:15:27.675934: step 45710, loss = 0.70, batch loss = 0.64 (14.7 examples/sec; 0.545 sec/batch; 43h:24m:46s remains)
INFO - root - 2017-12-11 09:15:33.108527: step 45720, loss = 0.70, batch loss = 0.64 (14.8 examples/sec; 0.540 sec/batch; 43h:00m:07s remains)
INFO - root - 2017-12-11 09:15:38.573892: step 45730, loss = 0.69, batch loss = 0.63 (14.5 examples/sec; 0.552 sec/batch; 43h:56m:25s remains)
INFO - root - 2017-12-11 09:15:43.963327: step 45740, loss = 0.70, batch loss = 0.64 (15.0 examples/sec; 0.533 sec/batch; 42h:27m:32s remains)
INFO - root - 2017-12-11 09:15:48.960093: step 45750, loss = 0.68, batch loss = 0.62 (14.9 examples/sec; 0.538 sec/batch; 42h:48m:59s remains)
INFO - root - 2017-12-11 09:15:54.384498: step 45760, loss = 0.70, batch loss = 0.64 (14.3 examples/sec; 0.558 sec/batch; 44h:27m:26s remains)
INFO - root - 2017-12-11 09:15:59.922778: step 45770, loss = 0.69, batch loss = 0.63 (14.5 examples/sec; 0.551 sec/batch; 43h:53m:52s remains)
INFO - root - 2017-12-11 09:16:05.411742: step 45780, loss = 0.68, batch loss = 0.62 (14.9 examples/sec; 0.538 sec/batch; 42h:52m:10s remains)
INFO - root - 2017-12-11 09:16:10.834021: step 45790, loss = 0.70, batch loss = 0.64 (14.6 examples/sec; 0.546 sec/batch; 43h:30m:19s remains)
INFO - root - 2017-12-11 09:16:16.236097: step 45800, loss = 0.69, batch loss = 0.63 (15.1 examples/sec; 0.531 sec/batch; 42h:19m:09s remains)
2017-12-11 09:16:16.867321: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.032662418 0.017443925 0.094801083 0.1867215 0.268092 0.31451663 0.3239688 0.30840498 0.28368777 0.27727309 0.30230722 0.3572914 0.41927254 0.46607083 0.48695698][-0.028407961 0.02488981 0.10948824 0.21185222 0.30503914 0.35958126 0.37108991 0.35053653 0.31355536 0.29201624 0.30203885 0.34560457 0.40208593 0.44909808 0.47475767][-0.018231461 0.040926561 0.13610609 0.251776 0.3578833 0.42049652 0.43249983 0.40338367 0.34740102 0.30109483 0.28609383 0.31073037 0.35852918 0.40787381 0.44503248][-0.0041778567 0.064271592 0.17365228 0.30666661 0.42996755 0.5056293 0.52221233 0.48561671 0.40771225 0.32960802 0.2810621 0.27975562 0.3157112 0.36968595 0.42458516][0.006605057 0.083684139 0.20577642 0.35597169 0.49958473 0.59524113 0.62460172 0.58583474 0.48735043 0.37487671 0.28771931 0.25456244 0.2751596 0.33488089 0.41263849][0.013019776 0.097775593 0.23015657 0.39552066 0.55983073 0.67865616 0.72571969 0.68956393 0.57459426 0.43049514 0.30618593 0.24114813 0.24547072 0.3093735 0.40838814][0.016145723 0.10535047 0.24238335 0.41458091 0.58995217 0.72351772 0.78437215 0.75392228 0.63281465 0.47054031 0.32195035 0.23370661 0.22435224 0.28863746 0.40070447][0.011877427 0.098659776 0.23069005 0.39639324 0.56703407 0.70039916 0.76668483 0.74514949 0.63317782 0.47331685 0.31894264 0.21873784 0.19725996 0.25453919 0.36728722][4.7271729e-05 0.076809913 0.19349439 0.33941951 0.48973644 0.60769987 0.66926938 0.65683293 0.566266 0.42772496 0.28507119 0.1833296 0.14994761 0.19139926 0.2906428][-0.01314116 0.0499841 0.14505918 0.26222971 0.38068965 0.47123349 0.51855779 0.51131231 0.44536042 0.33842647 0.22092687 0.12969434 0.089969687 0.11281555 0.18922013][-0.021325678 0.030235345 0.10524475 0.19389415 0.27892169 0.33938193 0.3690519 0.36273992 0.31760785 0.24280544 0.15587997 0.083030626 0.042881709 0.047954593 0.09642487][-0.024392197 0.019594731 0.080603778 0.14785191 0.20654407 0.24243532 0.25682908 0.24969825 0.22022724 0.17295356 0.11524665 0.06272532 0.025566477 0.015437588 0.035703324][-0.020207779 0.024151439 0.082439512 0.14220268 0.18851396 0.21013997 0.21343471 0.20360912 0.18312564 0.15481818 0.11946867 0.083550006 0.049662054 0.027559575 0.023552632][-0.0065373387 0.047389865 0.11599035 0.18398423 0.23338988 0.25111392 0.24534759 0.22730885 0.20475228 0.18194605 0.15734209 0.13142876 0.10103567 0.072788559 0.05276607][0.0087266965 0.074810795 0.15878557 0.242952 0.30533767 0.32819328 0.31789076 0.29008222 0.25796625 0.22951622 0.20520313 0.18338591 0.15707187 0.12855117 0.1014095]]...]
INFO - root - 2017-12-11 09:16:22.310434: step 45810, loss = 0.71, batch loss = 0.65 (14.1 examples/sec; 0.567 sec/batch; 45h:09m:38s remains)
INFO - root - 2017-12-11 09:16:27.758689: step 45820, loss = 0.71, batch loss = 0.65 (14.7 examples/sec; 0.544 sec/batch; 43h:19m:56s remains)
INFO - root - 2017-12-11 09:16:33.159716: step 45830, loss = 0.70, batch loss = 0.64 (14.4 examples/sec; 0.554 sec/batch; 44h:06m:35s remains)
INFO - root - 2017-12-11 09:16:38.565962: step 45840, loss = 0.69, batch loss = 0.63 (14.4 examples/sec; 0.557 sec/batch; 44h:19m:02s remains)
INFO - root - 2017-12-11 09:16:43.537920: step 45850, loss = 0.70, batch loss = 0.64 (14.9 examples/sec; 0.537 sec/batch; 42h:45m:00s remains)
INFO - root - 2017-12-11 09:16:48.863166: step 45860, loss = 0.70, batch loss = 0.64 (15.1 examples/sec; 0.529 sec/batch; 42h:06m:38s remains)
INFO - root - 2017-12-11 09:16:54.275547: step 45870, loss = 0.68, batch loss = 0.63 (15.3 examples/sec; 0.524 sec/batch; 41h:40m:53s remains)
INFO - root - 2017-12-11 09:16:59.612426: step 45880, loss = 0.69, batch loss = 0.64 (15.3 examples/sec; 0.523 sec/batch; 41h:39m:16s remains)
INFO - root - 2017-12-11 09:17:05.002192: step 45890, loss = 0.69, batch loss = 0.63 (15.1 examples/sec; 0.531 sec/batch; 42h:17m:26s remains)
INFO - root - 2017-12-11 09:17:10.346544: step 45900, loss = 0.69, batch loss = 0.63 (14.8 examples/sec; 0.539 sec/batch; 42h:56m:13s remains)
2017-12-11 09:17:10.927113: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.045761079 -0.016432587 0.036853727 0.11846414 0.22364564 0.33033988 0.41007391 0.44271225 0.4284848 0.37632313 0.30539045 0.24809082 0.22261827 0.22978938 0.2504068][-0.050389636 -0.021992974 0.027597906 0.10110507 0.1973788 0.2957207 0.36600387 0.39092806 0.37505856 0.3295154 0.27013737 0.22456242 0.2094616 0.22483417 0.2505753][-0.050456759 -0.020189295 0.030769955 0.10377918 0.1999827 0.29668239 0.36043632 0.37445784 0.34738994 0.29481813 0.23356532 0.18944034 0.17826094 0.19823293 0.22766806][-0.044899333 -0.0083332947 0.052105159 0.13715379 0.24814656 0.35739434 0.42563203 0.43247193 0.38787824 0.31311738 0.23202257 0.17421086 0.15706512 0.1763218 0.20761189][-0.035247132 0.010267949 0.084403366 0.18823728 0.3211965 0.45019752 0.53036135 0.53549117 0.47391811 0.37212491 0.26383981 0.18590009 0.15832362 0.17452662 0.20728579][-0.026883524 0.026251297 0.1122963 0.23328839 0.38526669 0.5318445 0.62553614 0.63513172 0.56494117 0.44360241 0.31395495 0.21988142 0.1830688 0.19480412 0.22621721][-0.024983857 0.030660532 0.12185331 0.25097832 0.41099557 0.56622463 0.67117375 0.69196963 0.62694955 0.50221771 0.36519262 0.26319736 0.21854444 0.22210948 0.24564314][-0.031383302 0.020365387 0.10762419 0.23233525 0.38659602 0.53970069 0.65213943 0.68965787 0.64333868 0.53190726 0.40107045 0.29677022 0.24213578 0.23094997 0.2391219][-0.043328241 -0.00085509493 0.074740313 0.18458277 0.32246107 0.46540907 0.58179909 0.63855517 0.61938089 0.53198731 0.4149344 0.30893934 0.23907244 0.20562217 0.19251294][-0.055774797 -0.024362039 0.0361781 0.12660892 0.24370094 0.37223226 0.48769736 0.55934393 0.56587195 0.50422078 0.40272683 0.29521859 0.20901497 0.15251954 0.11899148][-0.062517442 -0.038945641 0.00970147 0.08413101 0.18327758 0.2973401 0.40582466 0.48079404 0.500371 0.45538294 0.36542684 0.25804138 0.16146779 0.090265177 0.044462286][-0.060318317 -0.03784015 0.0070492327 0.07464797 0.16362604 0.26616207 0.36229113 0.42694637 0.44186127 0.3981379 0.31162116 0.20641783 0.10962783 0.037150834 -0.0090121888][-0.051653087 -0.024016794 0.0255505 0.096552014 0.18494737 0.28183523 0.36560288 0.41357625 0.41263503 0.35842842 0.26833051 0.16633441 0.076932542 0.013097306 -0.025403054][-0.042542636 -0.0082740784 0.049328152 0.12864082 0.22221869 0.31870374 0.39460203 0.42897841 0.41316548 0.34843069 0.25587 0.16053224 0.083188727 0.0318964 0.0024838333][-0.038288441 -0.00062458043 0.06213586 0.14699435 0.24375318 0.3389115 0.40899917 0.43528277 0.41246754 0.3464852 0.26112056 0.18081053 0.1211978 0.084874943 0.063593686]]...]
INFO - root - 2017-12-11 09:17:16.388700: step 45910, loss = 0.71, batch loss = 0.65 (14.8 examples/sec; 0.542 sec/batch; 43h:09m:29s remains)
INFO - root - 2017-12-11 09:17:21.896198: step 45920, loss = 0.70, batch loss = 0.64 (14.8 examples/sec; 0.540 sec/batch; 42h:58m:25s remains)
INFO - root - 2017-12-11 09:17:27.347848: step 45930, loss = 0.70, batch loss = 0.64 (14.6 examples/sec; 0.548 sec/batch; 43h:36m:18s remains)
INFO - root - 2017-12-11 09:17:32.752355: step 45940, loss = 0.69, batch loss = 0.63 (14.9 examples/sec; 0.538 sec/batch; 42h:48m:00s remains)
INFO - root - 2017-12-11 09:17:37.752069: step 45950, loss = 0.67, batch loss = 0.61 (15.1 examples/sec; 0.528 sec/batch; 42h:03m:22s remains)
INFO - root - 2017-12-11 09:17:43.165476: step 45960, loss = 0.73, batch loss = 0.67 (14.7 examples/sec; 0.546 sec/batch; 43h:27m:41s remains)
INFO - root - 2017-12-11 09:17:48.558250: step 45970, loss = 0.70, batch loss = 0.64 (15.1 examples/sec; 0.529 sec/batch; 42h:05m:33s remains)
INFO - root - 2017-12-11 09:17:53.943841: step 45980, loss = 0.70, batch loss = 0.64 (14.6 examples/sec; 0.548 sec/batch; 43h:34m:56s remains)
INFO - root - 2017-12-11 09:17:59.346810: step 45990, loss = 0.68, batch loss = 0.63 (14.9 examples/sec; 0.537 sec/batch; 42h:44m:23s remains)
INFO - root - 2017-12-11 09:18:04.712347: step 46000, loss = 0.69, batch loss = 0.63 (14.5 examples/sec; 0.551 sec/batch; 43h:50m:52s remains)
2017-12-11 09:18:05.305397: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.50017488 0.48615703 0.46569362 0.4571777 0.46371779 0.47865766 0.4861747 0.47750184 0.46100497 0.44935533 0.44188365 0.4402217 0.44761661 0.4614262 0.47193015][0.49687403 0.47623679 0.44905734 0.44189584 0.45865586 0.49055523 0.51760215 0.52043456 0.49857768 0.4679009 0.43291757 0.40086833 0.37758279 0.36462098 0.35184556][0.50195312 0.47314355 0.43548185 0.42259663 0.44246322 0.48576364 0.5292803 0.5432142 0.51796681 0.46686971 0.39766669 0.32509974 0.2613363 0.21478748 0.17801267][0.50396049 0.46582192 0.41816372 0.39940518 0.42231721 0.47719821 0.536043 0.56057316 0.53301692 0.46209365 0.35757869 0.24258046 0.13927974 0.063154072 0.010191544][0.45887271 0.41731077 0.37359494 0.36569655 0.40639159 0.48227918 0.55868304 0.5925222 0.56108546 0.47055182 0.33477294 0.18484977 0.052274868 -0.042816035 -0.1015991][0.35539198 0.32429805 0.3074626 0.33505982 0.4127211 0.51943791 0.61379313 0.65326929 0.61431742 0.5044595 0.34411502 0.17100652 0.021885697 -0.082017168 -0.14005466][0.21843635 0.212614 0.24072923 0.31953946 0.44366187 0.58389121 0.6944319 0.73576325 0.68718606 0.56091934 0.38396385 0.19730565 0.039362054 -0.068838261 -0.12496006][0.0916655 0.11712144 0.19207726 0.31830433 0.4810217 0.64629179 0.76803756 0.80961883 0.75392592 0.6176005 0.43192559 0.23795524 0.073856026 -0.037939362 -0.092396624][0.0071634525 0.05876559 0.16603562 0.31902125 0.49850619 0.67207277 0.79720926 0.83958089 0.78339803 0.6460107 0.45953095 0.26367742 0.09692163 -0.01533133 -0.064941444][-0.030396456 0.03414496 0.15098463 0.30393037 0.47493994 0.63858867 0.75835645 0.80178678 0.75201845 0.62343 0.4455502 0.25602716 0.09487281 -0.0095579382 -0.047282413][-0.039401338 0.023115594 0.12798417 0.25872567 0.4029085 0.54370683 0.6500746 0.69157362 0.65023392 0.53661883 0.37691852 0.20652156 0.065472029 -0.017849306 -0.034539536][-0.04053387 0.00862032 0.0882575 0.18576422 0.29528865 0.40597606 0.49095339 0.52343667 0.48711324 0.39103606 0.25900814 0.1231363 0.019164303 -0.029514734 -0.018117448][-0.041265473 -0.010080987 0.04076276 0.10364673 0.17717472 0.25352889 0.31053227 0.32804695 0.29457515 0.21974415 0.12492795 0.036406174 -0.019289611 -0.027673556 0.011044984][-0.04068435 -0.025810516 0.00037396816 0.034002669 0.075279362 0.11855994 0.14813457 0.15232503 0.12519598 0.076584749 0.023850206 -0.014750403 -0.02412563 0.0011093712 0.05720377][-0.034475457 -0.030405127 -0.021037268 -0.0078800926 0.010076489 0.029801194 0.042482872 0.043082692 0.029047951 0.0083025359 -0.0070329965 -0.0066692862 0.015002696 0.057627596 0.11653326]]...]
INFO - root - 2017-12-11 09:18:10.820454: step 46010, loss = 0.69, batch loss = 0.63 (14.2 examples/sec; 0.563 sec/batch; 44h:47m:07s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-SGD-lr0.001-clip50-initfromfixqian30k-fixqian
INFO - root - 2017-12-11 09:18:16.275308: step 46020, loss = 0.70, batch loss = 0.64 (14.9 examples/sec; 0.536 sec/batch; 42h:37m:07s remains)
INFO - root - 2017-12-11 09:18:21.685467: step 46030, loss = 0.71, batch loss = 0.65 (14.5 examples/sec; 0.553 sec/batch; 43h:59m:26s remains)
INFO - root - 2017-12-11 09:18:27.054776: step 46040, loss = 0.69, batch loss = 0.63 (15.0 examples/sec; 0.535 sec/batch; 42h:33m:24s remains)
INFO - root - 2017-12-11 09:18:32.142648: step 46050, loss = 0.71, batch loss = 0.65 (14.8 examples/sec; 0.540 sec/batch; 42h:57m:27s remains)
INFO - root - 2017-12-11 09:18:37.662735: step 46060, loss = 0.69, batch loss = 0.63 (14.2 examples/sec; 0.562 sec/batch; 44h:44m:58s remains)
INFO - root - 2017-12-11 09:18:43.115555: step 46070, loss = 0.70, batch loss = 0.64 (15.0 examples/sec; 0.532 sec/batch; 42h:18m:55s remains)
INFO - root - 2017-12-11 09:18:48.617681: step 46080, loss = 0.70, batch loss = 0.64 (14.6 examples/sec; 0.547 sec/batch; 43h:33m:29s remains)
INFO - root - 2017-12-11 09:18:54.050476: step 46090, loss = 0.69, batch loss = 0.64 (14.9 examples/sec; 0.538 sec/batch; 42h:50m:27s remains)
INFO - root - 2017-12-11 09:18:59.471430: step 46100, loss = 0.68, batch loss = 0.63 (14.5 examples/sec; 0.551 sec/batch; 43h:49m:50s remains)
2017-12-11 09:19:00.104721: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.1349127 0.1320273 0.13245976 0.14599398 0.17821527 0.23213457 0.29342866 0.33863479 0.35369149 0.33342043 0.27935317 0.19876938 0.11208461 0.040637825 -0.0046592448][0.1364083 0.12972832 0.12642215 0.13696799 0.16788508 0.22231953 0.28673321 0.33810878 0.36140603 0.34797603 0.2968857 0.21540779 0.12510604 0.048576381 -0.00098258979][0.10686114 0.10310085 0.10355685 0.11667006 0.14853771 0.20208137 0.265911 0.31959563 0.34857461 0.34135225 0.29392225 0.21365541 0.12361034 0.04643736 -0.0040597385][0.068678431 0.076728523 0.090986855 0.11553005 0.1545534 0.20982353 0.27198073 0.32408792 0.35256991 0.34419438 0.29382974 0.21056204 0.11879773 0.040487081 -0.011001248][0.045126069 0.071556829 0.1063486 0.14877595 0.19981416 0.25872925 0.31760347 0.36320469 0.38284767 0.36389777 0.30324924 0.21287894 0.1175302 0.037064768 -0.016547715][0.04768081 0.092932105 0.14781627 0.20777903 0.27007851 0.33063269 0.38261145 0.41631228 0.42030105 0.38554594 0.31368938 0.21899903 0.12325533 0.042193979 -0.013914414][0.072351187 0.13027726 0.1975814 0.26742092 0.33299142 0.38678694 0.42380527 0.43961492 0.42642292 0.380803 0.30803084 0.22094552 0.1342929 0.057804842 0.00043360904][0.10866808 0.17067052 0.23930734 0.30641663 0.36109665 0.39391166 0.40509856 0.39866295 0.37331158 0.33052802 0.27483398 0.21178265 0.14558774 0.079950929 0.023594018][0.14757572 0.20513953 0.26444563 0.31701368 0.34807503 0.34843081 0.32706589 0.29857484 0.2695334 0.24344845 0.2194249 0.19174702 0.15332657 0.1031176 0.050244741][0.18407603 0.23032998 0.27301365 0.30393735 0.30580494 0.27204147 0.2210836 0.17730387 0.15345255 0.1519684 0.16424626 0.17303629 0.16188243 0.12624428 0.076314494][0.21553755 0.24608202 0.26854333 0.27723688 0.25462106 0.19742353 0.13037491 0.083650731 0.072209418 0.094713137 0.13549145 0.16960652 0.17538834 0.1469577 0.096416637][0.24002838 0.25328258 0.25634083 0.2482145 0.2136921 0.15202285 0.08863841 0.053228103 0.057699066 0.095337056 0.14669484 0.18548636 0.19074123 0.15863216 0.10367475][0.25586396 0.25273138 0.24020445 0.22361797 0.1922605 0.14627419 0.10577735 0.092665233 0.11112061 0.14963648 0.18995899 0.21085539 0.19792503 0.15284525 0.0925052][0.26310903 0.24583034 0.22237507 0.20506731 0.18878818 0.17270002 0.16646491 0.17896587 0.20445481 0.22899623 0.24042207 0.22773233 0.18709403 0.12687086 0.064864166][0.25873229 0.23134257 0.20243819 0.190589 0.19525428 0.21312311 0.24145006 0.27468243 0.29829556 0.29928493 0.27481556 0.2259832 0.15987623 0.090222351 0.03318435]]...]
INFO - root - 2017-12-11 09:19:05.575789: step 46110, loss = 0.69, batch loss = 0.63 (14.3 examples/sec; 0.559 sec/batch; 44h:27m:58s remains)
INFO - root - 2017-12-11 09:19:11.026786: step 46120, loss = 0.70, batch loss = 0.64 (14.4 examples/sec; 0.555 sec/batch; 44h:07m:33s remains)
INFO - root - 2017-12-11 09:19:16.529057: step 46130, loss = 0.72, batch loss = 0.66 (14.7 examples/sec; 0.544 sec/batch; 43h:16m:58s remains)
INFO - root - 2017-12-11 09:19:21.908929: step 46140, loss = 0.70, batch loss = 0.64 (14.9 examples/sec; 0.535 sec/batch; 42h:34m:48s remains)
INFO - root - 2017-12-11 09:19:26.896216: step 46150, loss = 0.69, batch loss = 0.63 (14.7 examples/sec; 0.544 sec/batch; 43h:16m:52s remains)
INFO - root - 2017-12-11 09:19:32.306355: step 46160, loss = 0.70, batch loss = 0.64 (14.7 examples/sec; 0.543 sec/batch; 43h:11m:38s remains)
INFO - root - 2017-12-11 09:19:37.682823: step 46170, loss = 0.70, batch loss = 0.64 (15.2 examples/sec; 0.528 sec/batch; 41h:59m:55s remains)
INFO - root - 2017-12-11 09:19:43.014900: step 46180, loss = 0.72, batch loss = 0.66 (15.0 examples/sec; 0.535 sec/batch; 42h:30m:43s remains)
INFO - root - 2017-12-11 09:19:48.403862: step 46190, loss = 0.69, batch loss = 0.63 (15.1 examples/sec; 0.529 sec/batch; 42h:02m:36s remains)
INFO - root - 2017-12-11 09:19:53.814593: step 46200, loss = 0.68, batch loss = 0.62 (15.1 examples/sec; 0.531 sec/batch; 42h:13m:30s remains)
2017-12-11 09:19:54.488844: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.20423442 0.22774778 0.23408537 0.21671779 0.18119016 0.13441923 0.091792762 0.065530553 0.064719222 0.091049746 0.13327917 0.1704793 0.18622975 0.17172381 0.13210297][0.30509636 0.33644453 0.34357634 0.32144 0.2805399 0.23071428 0.18851008 0.16628739 0.17084651 0.20120375 0.24832933 0.2905373 0.3062028 0.28001338 0.2188935][0.37548581 0.41137832 0.42227697 0.4050357 0.37274209 0.33458403 0.30492988 0.29508638 0.30770648 0.33773711 0.38099524 0.42012805 0.43092966 0.39226145 0.31151542][0.38139197 0.41798526 0.439788 0.44411346 0.44074148 0.43251681 0.42695424 0.43449271 0.45448092 0.4775129 0.50448227 0.52544117 0.51991326 0.46618137 0.37126034][0.31767109 0.35796958 0.40439841 0.45283955 0.50313991 0.54558736 0.57552421 0.60055828 0.61762881 0.61586 0.6050992 0.58735186 0.55215037 0.48205918 0.38204905][0.21534738 0.27158418 0.358777 0.46864349 0.58633 0.687333 0.75184643 0.78213823 0.77477163 0.72420448 0.65498364 0.58387041 0.51120341 0.42441446 0.32566157][0.10112308 0.18695715 0.32555869 0.50080961 0.68147564 0.82984591 0.9130953 0.92827582 0.87547565 0.76191038 0.62873518 0.50548971 0.39998904 0.3033312 0.21413256][0.012748612 0.13138551 0.31622943 0.54183251 0.76314259 0.93381268 1.0143478 0.99982023 0.89610362 0.72434491 0.53825027 0.37513739 0.24699777 0.14845617 0.075200915][-0.036431521 0.10773453 0.31929353 0.56552875 0.79390448 0.95584172 1.0138309 0.965717 0.8240847 0.61954433 0.40924025 0.23116432 0.098786168 0.0085644536 -0.04604166][-0.045895632 0.10144455 0.307731 0.53732145 0.73915905 0.868122 0.8943857 0.82133186 0.666456 0.46412897 0.26698145 0.10774335 -0.0037869494 -0.072277121 -0.10636309][-0.033697519 0.091181047 0.25942776 0.43999386 0.59132713 0.6765188 0.67437476 0.59233457 0.44944516 0.27924395 0.12557819 0.013048638 -0.056574427 -0.0931157 -0.10781321][-0.019327302 0.062941432 0.17271543 0.28863752 0.38380787 0.43218407 0.4184117 0.34828144 0.23729356 0.11604875 0.019323658 -0.0368791 -0.059536677 -0.063837767 -0.0632538][-0.0039779055 0.031370711 0.081689693 0.13672349 0.18495366 0.21063307 0.2010873 0.15586747 0.0843507 0.012751285 -0.031717092 -0.039849907 -0.024435414 -0.0041789059 0.0056909719][0.00813472 0.00489104 0.008367382 0.017896371 0.034302536 0.049461771 0.051548854 0.033046424 -0.0023208028 -0.033349182 -0.039022639 -0.014258989 0.025779603 0.061080363 0.074837677][0.012994141 -0.014813675 -0.040106516 -0.057303518 -0.057657182 -0.044776727 -0.030016357 -0.026723322 -0.035811219 -0.041218836 -0.026201097 0.013761868 0.064238943 0.10535837 0.11854278]]...]
INFO - root - 2017-12-11 09:19:59.926100: step 46210, loss = 0.69, batch loss = 0.63 (14.3 examples/sec; 0.560 sec/batch; 44h:31m:32s remains)
INFO - root - 2017-12-11 09:20:05.306214: step 46220, loss = 0.69, batch loss = 0.63 (14.9 examples/sec; 0.537 sec/batch; 42h:42m:34s remains)
INFO - root - 2017-12-11 09:20:10.763932: step 46230, loss = 0.71, batch loss = 0.65 (14.8 examples/sec; 0.539 sec/batch; 42h:53m:58s remains)
INFO - root - 2017-12-11 09:20:16.272938: step 46240, loss = 0.69, batch loss = 0.63 (14.5 examples/sec; 0.551 sec/batch; 43h:48m:13s remains)
INFO - root - 2017-12-11 09:20:21.360530: step 46250, loss = 0.71, batch loss = 0.65 (14.8 examples/sec; 0.539 sec/batch; 42h:51m:19s remains)
INFO - root - 2017-12-11 09:20:26.836942: step 46260, loss = 0.70, batch loss = 0.64 (14.8 examples/sec; 0.542 sec/batch; 43h:06m:31s remains)
INFO - root - 2017-12-11 09:20:32.267879: step 46270, loss = 0.70, batch loss = 0.64 (14.7 examples/sec; 0.545 sec/batch; 43h:20m:00s remains)
INFO - root - 2017-12-11 09:20:37.650514: step 46280, loss = 0.70, batch loss = 0.64 (14.9 examples/sec; 0.538 sec/batch; 42h:48m:27s remains)
INFO - root - 2017-12-11 09:20:43.051179: step 46290, loss = 0.69, batch loss = 0.63 (14.6 examples/sec; 0.547 sec/batch; 43h:29m:15s remains)
INFO - root - 2017-12-11 09:20:48.364137: step 46300, loss = 0.69, batch loss = 0.64 (15.5 examples/sec; 0.516 sec/batch; 41h:02m:33s remains)
2017-12-11 09:20:48.946087: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.2539508 0.24650657 0.23269323 0.21155174 0.18144152 0.15462458 0.16861759 0.21302488 0.25054845 0.25834364 0.24043779 0.20376471 0.14254935 0.065475889 -0.0027663957][0.256883 0.25570771 0.24567552 0.22596461 0.1966719 0.16978936 0.18121137 0.22151341 0.25509545 0.25745746 0.23112272 0.18513833 0.11756742 0.038303729 -0.028005395][0.25822052 0.26437026 0.25999394 0.24516605 0.22179919 0.19929413 0.2076357 0.23941191 0.26351404 0.25624928 0.21834557 0.16120654 0.087951362 0.0086144144 -0.053554408][0.27934107 0.29352251 0.29608443 0.2885105 0.27366725 0.2565065 0.26050445 0.28003648 0.28999794 0.26847175 0.21594656 0.14644623 0.067341164 -0.011812375 -0.07057973][0.32009345 0.34041297 0.34901473 0.34879109 0.34198323 0.32839143 0.32856211 0.33677438 0.33223584 0.29487067 0.22792614 0.14819998 0.064156868 -0.015851807 -0.073957965][0.36889094 0.3920995 0.40485778 0.4104861 0.40891197 0.39629543 0.39611045 0.40025562 0.38694882 0.33662441 0.25838721 0.17217423 0.083844259 -0.00020584108 -0.062418055][0.40898255 0.43216503 0.44803914 0.45881331 0.46007171 0.44709316 0.45110536 0.46077809 0.44777226 0.39102477 0.30700421 0.21855818 0.12590043 0.033406585 -0.038338333][0.41338325 0.43429181 0.45289445 0.46912411 0.47341096 0.46297079 0.477048 0.50086957 0.49621496 0.43969288 0.35538307 0.26837793 0.17210203 0.069509074 -0.013411423][0.36858368 0.38458803 0.40391964 0.42440534 0.43237606 0.42860019 0.45549789 0.49571338 0.50267273 0.45195377 0.37290254 0.29131621 0.19535667 0.087510623 -0.0012527162][0.29997617 0.30893525 0.32544082 0.3472701 0.35858375 0.36270365 0.39860049 0.44783223 0.46129596 0.41659009 0.34483126 0.27056602 0.17990217 0.075631559 -0.0092859352][0.24254228 0.24392743 0.25291651 0.26998711 0.2803238 0.28815842 0.32407197 0.37051821 0.38250288 0.3413462 0.276385 0.2093064 0.12798376 0.0364162 -0.034925647][0.22358903 0.21710435 0.21183994 0.21376322 0.213539 0.21573222 0.2414459 0.27551612 0.28118464 0.24301691 0.18509385 0.12507774 0.055565942 -0.017556073 -0.069567978][0.24919021 0.23430225 0.20777735 0.18305182 0.16008525 0.14547025 0.15330647 0.17172202 0.1721625 0.14031844 0.092331514 0.041108668 -0.015134446 -0.068483092 -0.10102138][0.30186328 0.27735376 0.22688571 0.17075448 0.11894085 0.081738047 0.070213161 0.0767616 0.078047231 0.057879273 0.023350067 -0.017195141 -0.060758807 -0.098160766 -0.11709309][0.35439983 0.31958932 0.24923785 0.16842945 0.09396486 0.03929168 0.014418419 0.016179154 0.023453461 0.016825395 -0.00434433 -0.035151213 -0.069654748 -0.098395772 -0.11264899]]...]
INFO - root - 2017-12-11 09:20:54.438361: step 46310, loss = 0.69, batch loss = 0.63 (14.2 examples/sec; 0.564 sec/batch; 44h:48m:11s remains)
INFO - root - 2017-12-11 09:20:59.863426: step 46320, loss = 0.69, batch loss = 0.63 (14.8 examples/sec; 0.541 sec/batch; 43h:02m:00s remains)
INFO - root - 2017-12-11 09:21:05.267763: step 46330, loss = 0.70, batch loss = 0.65 (14.8 examples/sec; 0.539 sec/batch; 42h:52m:48s remains)
INFO - root - 2017-12-11 09:21:10.671535: step 46340, loss = 0.70, batch loss = 0.64 (15.3 examples/sec; 0.524 sec/batch; 41h:38m:10s remains)
INFO - root - 2017-12-11 09:21:15.769597: step 46350, loss = 0.70, batch loss = 0.64 (14.1 examples/sec; 0.568 sec/batch; 45h:06m:42s remains)
INFO - root - 2017-12-11 09:21:21.260525: step 46360, loss = 0.67, batch loss = 0.62 (14.9 examples/sec; 0.537 sec/batch; 42h:38m:57s remains)
INFO - root - 2017-12-11 09:21:26.665161: step 46370, loss = 0.69, batch loss = 0.63 (14.7 examples/sec; 0.542 sec/batch; 43h:06m:32s remains)
INFO - root - 2017-12-11 09:21:32.087106: step 46380, loss = 0.70, batch loss = 0.65 (14.7 examples/sec; 0.546 sec/batch; 43h:21m:55s remains)
INFO - root - 2017-12-11 09:21:37.499079: step 46390, loss = 0.71, batch loss = 0.65 (15.2 examples/sec; 0.526 sec/batch; 41h:48m:02s remains)
INFO - root - 2017-12-11 09:21:42.945879: step 46400, loss = 0.71, batch loss = 0.65 (14.4 examples/sec; 0.557 sec/batch; 44h:17m:34s remains)
2017-12-11 09:21:43.561567: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.06952823 -0.054071251 -0.024040341 0.012209013 0.047067765 0.073675551 0.091566257 0.098610386 0.091508791 0.074940883 0.057105608 0.046135288 0.042197313 0.042639162 0.044471834][-0.059696354 -0.034161519 0.01078862 0.066522293 0.12435895 0.17495705 0.21422273 0.23521507 0.23095638 0.20928791 0.18549569 0.17263493 0.17004365 0.17019868 0.167416][-0.046169419 -0.0097351456 0.052233193 0.13190214 0.21883668 0.29958707 0.36436281 0.39987668 0.39623195 0.36750254 0.33877552 0.32683578 0.32599023 0.32230988 0.30890176][-0.037473276 0.0075974278 0.085100964 0.18826719 0.30451706 0.41501972 0.50255662 0.54774868 0.54077661 0.5048548 0.4743906 0.46627146 0.4666248 0.45417151 0.42370173][-0.034485139 0.017017968 0.10881908 0.23451389 0.37873688 0.51578665 0.61925054 0.66467643 0.645941 0.59856331 0.56483346 0.55819947 0.55476934 0.52712435 0.4755733][-0.036751244 0.019705972 0.12406822 0.27079576 0.44095847 0.60109514 0.71503353 0.75370914 0.71483696 0.64635712 0.59835964 0.58156335 0.56406063 0.516273 0.4465017][-0.041816607 0.016984552 0.13015552 0.2920329 0.48002681 0.654309 0.77197415 0.79888988 0.7340672 0.63482338 0.56068331 0.52298379 0.48595673 0.42092496 0.34249872][-0.047183305 0.011721978 0.12824425 0.29503566 0.48728493 0.66242474 0.77530116 0.78772157 0.69775248 0.56766629 0.46391612 0.40190852 0.34789658 0.275288 0.20081578][-0.051114473 0.0060184482 0.12002395 0.28088677 0.46313772 0.62582678 0.72600025 0.724177 0.616444 0.46485624 0.33777016 0.25681224 0.19569525 0.13076128 0.075424969][-0.054735653 -0.0011647263 0.10524672 0.25234121 0.41526788 0.55696464 0.6391 0.62450165 0.50837028 0.34848598 0.21124198 0.12397306 0.068884566 0.025108492 -0.0011959][-0.059465535 -0.01323703 0.07885246 0.20386748 0.33920944 0.45274541 0.51288211 0.48883003 0.37644288 0.2260481 0.098039635 0.022062169 -0.013999314 -0.029606286 -0.026908746][-0.064125143 -0.029487019 0.04134855 0.1354157 0.23423737 0.31219655 0.34704989 0.31753314 0.22165436 0.099636175 0.001085064 -0.047935244 -0.05807171 -0.048248243 -0.027140291][-0.069124885 -0.047412157 -0.0010812102 0.057998821 0.11675715 0.15760495 0.16847409 0.13869312 0.069050081 -0.011863251 -0.069991671 -0.087715216 -0.075787172 -0.051244412 -0.025125233][-0.077332608 -0.0686405 -0.045751441 -0.018660102 0.0050281011 0.015653878 0.0094835609 -0.015839117 -0.057475474 -0.098111488 -0.11919022 -0.11262383 -0.0888575 -0.061183244 -0.039175119][-0.089049593 -0.091927215 -0.087451473 -0.0834597 -0.082887255 -0.089464851 -0.10265549 -0.12088369 -0.14080615 -0.15377358 -0.15242907 -0.13535699 -0.11105444 -0.088366047 -0.073477134]]...]
INFO - root - 2017-12-11 09:21:48.988896: step 46410, loss = 0.69, batch loss = 0.63 (14.5 examples/sec; 0.552 sec/batch; 43h:54m:22s remains)
INFO - root - 2017-12-11 09:21:54.398535: step 46420, loss = 0.71, batch loss = 0.65 (15.0 examples/sec; 0.532 sec/batch; 42h:17m:59s remains)
INFO - root - 2017-12-11 09:21:59.856889: step 46430, loss = 0.71, batch loss = 0.65 (14.5 examples/sec; 0.551 sec/batch; 43h:47m:00s remains)
INFO - root - 2017-12-11 09:22:05.282411: step 46440, loss = 0.69, batch loss = 0.63 (14.6 examples/sec; 0.548 sec/batch; 43h:34m:06s remains)
INFO - root - 2017-12-11 09:22:10.322463: step 46450, loss = 0.72, batch loss = 0.66 (14.4 examples/sec; 0.557 sec/batch; 44h:13m:25s remains)
INFO - root - 2017-12-11 09:22:15.711697: step 46460, loss = 0.69, batch loss = 0.63 (15.2 examples/sec; 0.526 sec/batch; 41h:46m:08s remains)
INFO - root - 2017-12-11 09:22:21.188233: step 46470, loss = 0.70, batch loss = 0.64 (14.7 examples/sec; 0.542 sec/batch; 43h:05m:41s remains)
INFO - root - 2017-12-11 09:22:26.741610: step 46480, loss = 0.70, batch loss = 0.64 (14.4 examples/sec; 0.557 sec/batch; 44h:16m:41s remains)
INFO - root - 2017-12-11 09:22:32.180099: step 46490, loss = 0.68, batch loss = 0.62 (14.1 examples/sec; 0.566 sec/batch; 44h:57m:24s remains)
INFO - root - 2017-12-11 09:22:37.576523: step 46500, loss = 0.69, batch loss = 0.63 (14.6 examples/sec; 0.546 sec/batch; 43h:24m:12s remains)
2017-12-11 09:22:38.173742: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.086073965 -0.084542654 -0.07879097 -0.071388222 -0.06207066 -0.051921021 -0.042773888 -0.038119853 -0.038998894 -0.042746264 -0.04726699 -0.050606657 -0.053081919 -0.054615609 -0.056774091][-0.074289069 -0.064117968 -0.047591228 -0.029430889 -0.009050129 0.011255905 0.027238488 0.032421447 0.025857236 0.015184942 0.0044859555 -0.0029265687 -0.0077489302 -0.010601527 -0.014820823][-0.049846247 -0.023786342 0.011847569 0.048417278 0.086026661 0.1198376 0.14198133 0.14281566 0.12242048 0.096218631 0.071596183 0.054011881 0.042721793 0.036945138 0.030703424][-0.015117852 0.035277121 0.10107803 0.16689186 0.23043086 0.28291085 0.31152382 0.30330148 0.26043075 0.20739295 0.15688677 0.11801706 0.091551475 0.076964281 0.065870732][0.021919886 0.10191056 0.20491931 0.30838338 0.40752336 0.48851115 0.53020567 0.5145784 0.44579792 0.35655943 0.26633948 0.19065006 0.13642368 0.10540995 0.086718291][0.051110767 0.15908666 0.29717398 0.43898386 0.57908732 0.6967898 0.75877297 0.74001783 0.64643145 0.51644152 0.3777284 0.2560122 0.16860408 0.12007277 0.095520757][0.066067435 0.19323102 0.35567024 0.52721155 0.704533 0.85833883 0.94099665 0.92124677 0.807544 0.64117438 0.4579955 0.29526716 0.1808141 0.12011442 0.093491048][0.064355366 0.19776636 0.36856684 0.55472869 0.75620681 0.93468571 1.0305427 1.0100425 0.88558465 0.69852734 0.49036202 0.30607998 0.17896193 0.11292214 0.085534886][0.0460246 0.17020716 0.32976887 0.50961661 0.71357608 0.89718676 0.99478483 0.97478962 0.85375106 0.670295 0.46638045 0.2870129 0.16484442 0.10159489 0.074744739][0.012981583 0.1125138 0.24133347 0.39192513 0.57204014 0.73662853 0.8223387 0.80368233 0.69945645 0.54338181 0.37162489 0.22278382 0.12390674 0.074445881 0.053805314][-0.027952546 0.037829623 0.12429981 0.22948594 0.36350167 0.48708275 0.54838246 0.530765 0.45239389 0.3396447 0.21860611 0.11812261 0.056019586 0.029171349 0.020180887][-0.065122709 -0.032338645 0.013113357 0.070879042 0.1508645 0.2244944 0.25692481 0.2406425 0.19043095 0.1241885 0.057141416 0.007229751 -0.017634891 -0.022546846 -0.020431912][-0.091249511 -0.082868561 -0.067665257 -0.046818338 -0.012843192 0.017887849 0.026845995 0.013051603 -0.013538565 -0.042395245 -0.066693835 -0.078349389 -0.076882027 -0.0677527 -0.059093267][-0.10686367 -0.11367464 -0.11656959 -0.11819881 -0.11374353 -0.10964645 -0.113381 -0.12334025 -0.13403849 -0.13995005 -0.13907462 -0.12990421 -0.11555155 -0.10049205 -0.089138992][-0.11266224 -0.12675792 -0.13745047 -0.14846505 -0.15683888 -0.16357374 -0.17083813 -0.17702565 -0.17955536 -0.17557292 -0.16505845 -0.14928322 -0.13195711 -0.11636171 -0.10472873]]...]
INFO - root - 2017-12-11 09:22:43.658368: step 46510, loss = 0.71, batch loss = 0.65 (14.5 examples/sec; 0.552 sec/batch; 43h:51m:40s remains)
INFO - root - 2017-12-11 09:22:49.042629: step 46520, loss = 0.69, batch loss = 0.63 (15.0 examples/sec; 0.532 sec/batch; 42h:13m:45s remains)
INFO - root - 2017-12-11 09:22:54.433726: step 46530, loss = 0.69, batch loss = 0.63 (15.0 examples/sec; 0.533 sec/batch; 42h:21m:26s remains)
INFO - root - 2017-12-11 09:22:59.803480: step 46540, loss = 0.69, batch loss = 0.63 (15.1 examples/sec; 0.528 sec/batch; 41h:58m:06s remains)
INFO - root - 2017-12-11 09:23:04.923168: step 46550, loss = 0.71, batch loss = 0.65 (14.3 examples/sec; 0.560 sec/batch; 44h:29m:32s remains)
INFO - root - 2017-12-11 09:23:10.461829: step 46560, loss = 0.67, batch loss = 0.61 (14.7 examples/sec; 0.544 sec/batch; 43h:12m:57s remains)
INFO - root - 2017-12-11 09:23:15.908834: step 46570, loss = 0.70, batch loss = 0.64 (14.8 examples/sec; 0.540 sec/batch; 42h:54m:41s remains)
INFO - root - 2017-12-11 09:23:21.300943: step 46580, loss = 0.69, batch loss = 0.63 (15.1 examples/sec; 0.529 sec/batch; 42h:00m:26s remains)
INFO - root - 2017-12-11 09:23:26.674171: step 46590, loss = 0.69, batch loss = 0.63 (14.9 examples/sec; 0.537 sec/batch; 42h:37m:17s remains)
INFO - root - 2017-12-11 09:23:32.020997: step 46600, loss = 0.70, batch loss = 0.65 (15.2 examples/sec; 0.525 sec/batch; 41h:42m:33s remains)
2017-12-11 09:23:32.616925: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.078726448 0.10519279 0.15103079 0.22176293 0.30284286 0.36334437 0.37470797 0.32421908 0.22843 0.11852418 0.031025743 -0.013924527 -0.018485509 -0.00058338168 0.017801629][0.088874564 0.11334518 0.14905988 0.20143372 0.26158869 0.305882 0.31121388 0.26722273 0.18753235 0.099915169 0.036678132 0.012718575 0.022625031 0.0474041 0.066323921][0.1227496 0.15259863 0.18132435 0.21510179 0.25296634 0.28232107 0.28601265 0.25406328 0.1933974 0.1261429 0.083068684 0.077222906 0.10109299 0.13327239 0.15352179][0.17633741 0.21942647 0.24998854 0.27654597 0.30584943 0.33477607 0.34827188 0.33245078 0.283906 0.22105658 0.18101236 0.18115537 0.21424367 0.2534883 0.27563605][0.24858694 0.31101936 0.35312209 0.38805994 0.42835802 0.4760679 0.51309657 0.51618278 0.46862668 0.38866884 0.32977232 0.32170877 0.35944849 0.40631241 0.4319725][0.333473 0.41937679 0.48141751 0.53740716 0.60324895 0.68160194 0.74825549 0.767128 0.70794946 0.59220243 0.4954268 0.46670109 0.50374454 0.55832362 0.59078956][0.40983084 0.51588649 0.59824842 0.67755181 0.77162522 0.88108087 0.97495037 1.0041097 0.92805922 0.77308434 0.6330235 0.57517517 0.60178775 0.65869862 0.69868618][0.45737094 0.56852537 0.65681422 0.74549872 0.85614932 0.98717183 1.1007107 1.1377181 1.0521964 0.87269729 0.69951493 0.61096948 0.6159839 0.66339117 0.70398945][0.45124522 0.54764158 0.62184644 0.70011145 0.80931413 0.94649547 1.0688863 1.1129959 1.0322323 0.85290664 0.6677177 0.55584437 0.533298 0.55897194 0.58804685][0.38839942 0.45802829 0.5054639 0.55822277 0.64806777 0.77219963 0.8880502 0.93497312 0.87043011 0.71408468 0.54072869 0.42151442 0.37682953 0.37800089 0.38885835][0.28169906 0.32255873 0.3421053 0.36503804 0.42463002 0.51967382 0.61383414 0.65575421 0.61097491 0.49133402 0.34898263 0.24010533 0.18555494 0.16915396 0.16500641][0.15357126 0.16876163 0.16586477 0.16365589 0.19221117 0.25205398 0.31629074 0.34666115 0.31885806 0.23835038 0.13616969 0.051220849 0.0011437607 -0.021048851 -0.032092538][0.032890033 0.030179838 0.014343522 -0.0028102037 0.0019285565 0.029969778 0.064120091 0.079745986 0.062424649 0.013989809 -0.049735393 -0.10499167 -0.13964531 -0.15666324 -0.16630478][-0.059143588 -0.069882222 -0.088021867 -0.10859527 -0.1166309 -0.1107716 -0.099879295 -0.097188868 -0.10933774 -0.13576549 -0.16958982 -0.1981895 -0.21507679 -0.22238459 -0.22647978][-0.11009593 -0.12109499 -0.13445413 -0.15062465 -0.16180503 -0.16710688 -0.169651 -0.17491685 -0.1844828 -0.19776277 -0.21235843 -0.22257866 -0.2261271 -0.22519121 -0.22383632]]...]
INFO - root - 2017-12-11 09:23:38.156102: step 46610, loss = 0.70, batch loss = 0.64 (14.3 examples/sec; 0.559 sec/batch; 44h:23m:05s remains)
INFO - root - 2017-12-11 09:23:43.551464: step 46620, loss = 0.70, batch loss = 0.64 (14.7 examples/sec; 0.546 sec/batch; 43h:19m:34s remains)
INFO - root - 2017-12-11 09:23:49.043062: step 46630, loss = 0.70, batch loss = 0.64 (14.5 examples/sec; 0.551 sec/batch; 43h:46m:07s remains)
INFO - root - 2017-12-11 09:23:54.522704: step 46640, loss = 0.69, batch loss = 0.64 (14.7 examples/sec; 0.546 sec/batch; 43h:19m:12s remains)
INFO - root - 2017-12-11 09:23:59.613296: step 46650, loss = 0.68, batch loss = 0.62 (14.6 examples/sec; 0.546 sec/batch; 43h:22m:39s remains)
INFO - root - 2017-12-11 09:24:05.124147: step 46660, loss = 0.68, batch loss = 0.62 (14.7 examples/sec; 0.543 sec/batch; 43h:07m:43s remains)
INFO - root - 2017-12-11 09:24:10.589494: step 46670, loss = 0.69, batch loss = 0.64 (14.7 examples/sec; 0.545 sec/batch; 43h:18m:37s remains)
INFO - root - 2017-12-11 09:24:15.997605: step 46680, loss = 0.69, batch loss = 0.63 (15.0 examples/sec; 0.534 sec/batch; 42h:22m:33s remains)
INFO - root - 2017-12-11 09:24:21.381530: step 46690, loss = 0.68, batch loss = 0.62 (15.0 examples/sec; 0.532 sec/batch; 42h:13m:29s remains)
INFO - root - 2017-12-11 09:24:26.749164: step 46700, loss = 0.67, batch loss = 0.61 (14.6 examples/sec; 0.546 sec/batch; 43h:21m:43s remains)
2017-12-11 09:24:27.349475: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.18034375 0.24646758 0.31613719 0.3790814 0.42009097 0.43605185 0.43008393 0.40143624 0.355585 0.29757056 0.23432639 0.17945109 0.14481027 0.1349903 0.1404123][0.13890792 0.19061595 0.25871789 0.33147627 0.38941994 0.42299727 0.42819968 0.39883071 0.34114248 0.26627749 0.18667997 0.11898043 0.074588381 0.056201808 0.054444943][0.13144188 0.1603193 0.21929291 0.29788634 0.37293807 0.42621133 0.4436529 0.41319022 0.34128395 0.24457555 0.14327823 0.059760772 0.0060811159 -0.017793825 -0.02225348][0.14337757 0.15109201 0.19935735 0.28125221 0.37166151 0.44392759 0.47310421 0.4414539 0.35456288 0.23452276 0.11057744 0.011597371 -0.049131844 -0.074256353 -0.076357462][0.15201378 0.15155116 0.1956189 0.28028157 0.37996969 0.4635649 0.49843851 0.46389395 0.3646687 0.22777216 0.089852877 -0.015448404 -0.075328939 -0.095923908 -0.092000507][0.14427412 0.14990252 0.19824323 0.28483826 0.38441426 0.46651265 0.49746937 0.458125 0.35390449 0.21453227 0.079671569 -0.016296571 -0.063739613 -0.073209688 -0.061942689][0.13037331 0.15384904 0.21471851 0.30337796 0.39366296 0.45937771 0.47326812 0.42387372 0.32002389 0.19178927 0.076942921 0.0063725053 -0.015832368 -0.0060505909 0.015206567][0.1266108 0.17333567 0.25243887 0.3448709 0.42101118 0.45927757 0.44371182 0.375843 0.27158675 0.16086383 0.076957569 0.044163257 0.05951352 0.099043645 0.13668281][0.14643711 0.21227339 0.30597723 0.39901224 0.45730811 0.46263716 0.41101277 0.31835172 0.21052396 0.11867094 0.070900559 0.083018869 0.14445518 0.22267446 0.28298503][0.20676938 0.27813646 0.37168649 0.45448092 0.49036238 0.46385819 0.38006923 0.265688 0.15507779 0.08053121 0.065236174 0.11728071 0.21968932 0.33332035 0.41534504][0.28068447 0.34382153 0.42196846 0.48424435 0.49642313 0.44621995 0.34318855 0.21779652 0.10814943 0.046170942 0.050871998 0.12520315 0.24940616 0.38303345 0.47987193][0.3325294 0.37914029 0.43340668 0.47201869 0.46533105 0.40386844 0.29701546 0.17229404 0.066233754 0.00945485 0.01843733 0.094121918 0.21744475 0.35139576 0.45238698][0.34661472 0.37433517 0.40450335 0.42317176 0.40620008 0.34546247 0.24732359 0.13268021 0.032767817 -0.02363955 -0.021031065 0.041044261 0.14572938 0.2622036 0.35383835][0.33723262 0.3536298 0.36922017 0.37789121 0.36020234 0.30720919 0.22260839 0.121361 0.028844366 -0.028813692 -0.036156215 0.0077928472 0.087359868 0.1770016 0.24735983][0.33181819 0.34420955 0.35395294 0.36024389 0.34813461 0.30762267 0.24070622 0.15779573 0.078006729 0.023145441 0.008450577 0.0356343 0.0893605 0.14788924 0.18815632]]...]
INFO - root - 2017-12-11 09:24:32.878093: step 46710, loss = 0.70, batch loss = 0.64 (14.3 examples/sec; 0.559 sec/batch; 44h:22m:45s remains)
INFO - root - 2017-12-11 09:24:38.343213: step 46720, loss = 0.69, batch loss = 0.64 (14.9 examples/sec; 0.536 sec/batch; 42h:31m:21s remains)
INFO - root - 2017-12-11 09:24:43.817615: step 46730, loss = 0.70, batch loss = 0.64 (14.4 examples/sec; 0.555 sec/batch; 44h:04m:54s remains)
INFO - root - 2017-12-11 09:24:49.008174: step 46740, loss = 0.70, batch loss = 0.65 (30.2 examples/sec; 0.265 sec/batch; 21h:00m:16s remains)
INFO - root - 2017-12-11 09:24:54.478192: step 46750, loss = 0.70, batch loss = 0.64 (14.4 examples/sec; 0.555 sec/batch; 44h:03m:20s remains)
INFO - root - 2017-12-11 09:24:59.931679: step 46760, loss = 0.71, batch loss = 0.65 (14.5 examples/sec; 0.552 sec/batch; 43h:48m:20s remains)
INFO - root - 2017-12-11 09:25:05.372926: step 46770, loss = 0.71, batch loss = 0.65 (15.2 examples/sec; 0.527 sec/batch; 41h:51m:20s remains)
INFO - root - 2017-12-11 09:25:10.824831: step 46780, loss = 0.69, batch loss = 0.63 (15.1 examples/sec; 0.530 sec/batch; 42h:05m:35s remains)
INFO - root - 2017-12-11 09:25:16.280304: step 46790, loss = 0.68, batch loss = 0.62 (14.9 examples/sec; 0.537 sec/batch; 42h:37m:45s remains)
INFO - root - 2017-12-11 09:25:21.708049: step 46800, loss = 0.70, batch loss = 0.64 (14.4 examples/sec; 0.557 sec/batch; 44h:10m:44s remains)
2017-12-11 09:25:22.375873: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.028480925 -0.028787464 -0.028553613 -0.027827531 -0.026928896 -0.025902813 -0.024720013 -0.023759464 -0.023076871 -0.022938276 -0.022989811 -0.022870023 -0.022510305 -0.021458916 -0.020511543][-0.029405162 -0.029767426 -0.029481757 -0.02850851 -0.027234372 -0.025712868 -0.023953786 -0.022152845 -0.02072197 -0.020278236 -0.020532357 -0.021182701 -0.021708714 -0.021607211 -0.021400379][-0.025609288 -0.024548452 -0.023210047 -0.021680493 -0.020333624 -0.019086374 -0.017573966 -0.015453574 -0.013216285 -0.011914077 -0.01197488 -0.013305947 -0.015094718 -0.016883276 -0.018675158][-0.012779989 -0.007695043 -0.0035745972 -0.0012273296 -0.00098068709 -0.0022146336 -0.003271491 -0.0022320657 0.00055335427 0.0031603482 0.0039045764 0.0025620642 -0.00045344356 -0.0049539157 -0.010288713][0.012977079 0.025329309 0.034418523 0.038039252 0.035928003 0.029696768 0.023135541 0.021014577 0.023433153 0.027376221 0.029337453 0.028191091 0.023821443 0.015593577 0.0050245039][0.048907436 0.070710786 0.086357214 0.092069179 0.0873357 0.0746855 0.060646158 0.053624302 0.05424035 0.058401104 0.060824092 0.059363153 0.053082798 0.040201671 0.023570184][0.086020611 0.11636097 0.13790515 0.14577751 0.13885461 0.12034207 0.099346139 0.087388493 0.085437976 0.088559024 0.090307452 0.087806009 0.079223543 0.061821349 0.039581139][0.11224738 0.14704569 0.17132227 0.17993376 0.17131764 0.14924577 0.12442499 0.1099234 0.1065203 0.10866663 0.10978293 0.10663891 0.096471719 0.075987346 0.049724247][0.11733402 0.15042709 0.17269114 0.17961782 0.16987038 0.14754249 0.12348519 0.11046163 0.10844067 0.11174653 0.11432337 0.11268117 0.10321998 0.082356431 0.054696783][0.10257251 0.12892766 0.14557244 0.14928409 0.13950373 0.120279 0.10133737 0.093767643 0.096738957 0.10501447 0.11288798 0.11601691 0.10991149 0.090825126 0.06307821][0.080899596 0.0982296 0.10782468 0.10799798 0.099059738 0.084759422 0.073246494 0.07359305 0.084613025 0.10181575 0.11918513 0.13084097 0.13084 0.11508828 0.087157249][0.067516088 0.076040745 0.078528844 0.075079046 0.066800438 0.057153195 0.052922931 0.061543245 0.081923373 0.11046495 0.14013985 0.16298448 0.17122962 0.16021183 0.13192816][0.0691236 0.070977315 0.067747496 0.061159927 0.053015057 0.046640269 0.047891904 0.062870145 0.090642311 0.12879634 0.16959445 0.2031996 0.21995337 0.21447243 0.1871442][0.084042929 0.082112268 0.075218514 0.066484913 0.058105327 0.053270355 0.057333823 0.075220726 0.10587861 0.14828959 0.19527782 0.23604475 0.25954971 0.25985402 0.2363399][0.10450248 0.1015453 0.093581036 0.08470948 0.076854765 0.072819553 0.07763236 0.095093131 0.12376089 0.16406456 0.21085389 0.25335616 0.27980286 0.28466481 0.26748946]]...]
INFO - root - 2017-12-11 09:25:27.773629: step 46810, loss = 0.70, batch loss = 0.65 (14.8 examples/sec; 0.541 sec/batch; 42h:53m:55s remains)
INFO - root - 2017-12-11 09:25:33.141452: step 46820, loss = 0.69, batch loss = 0.63 (14.9 examples/sec; 0.537 sec/batch; 42h:38m:58s remains)
INFO - root - 2017-12-11 09:25:38.502653: step 46830, loss = 0.70, batch loss = 0.64 (15.1 examples/sec; 0.530 sec/batch; 42h:03m:31s remains)
INFO - root - 2017-12-11 09:25:43.570251: step 46840, loss = 0.68, batch loss = 0.63 (15.3 examples/sec; 0.523 sec/batch; 41h:31m:38s remains)
INFO - root - 2017-12-11 09:25:49.083091: step 46850, loss = 0.71, batch loss = 0.65 (14.6 examples/sec; 0.547 sec/batch; 43h:22m:35s remains)
INFO - root - 2017-12-11 09:25:54.478265: step 46860, loss = 0.71, batch loss = 0.65 (15.9 examples/sec; 0.503 sec/batch; 39h:54m:49s remains)
INFO - root - 2017-12-11 09:25:59.910681: step 46870, loss = 0.69, batch loss = 0.63 (14.8 examples/sec; 0.541 sec/batch; 42h:57m:32s remains)
INFO - root - 2017-12-11 09:26:05.338881: step 46880, loss = 0.68, batch loss = 0.62 (14.7 examples/sec; 0.543 sec/batch; 43h:04m:45s remains)
INFO - root - 2017-12-11 09:26:10.812388: step 46890, loss = 0.70, batch loss = 0.64 (15.1 examples/sec; 0.531 sec/batch; 42h:09m:09s remains)
INFO - root - 2017-12-11 09:26:16.209429: step 46900, loss = 0.69, batch loss = 0.63 (14.8 examples/sec; 0.541 sec/batch; 42h:55m:47s remains)
2017-12-11 09:26:16.854669: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.16426165 0.14593089 0.133742 0.128262 0.11682387 0.10217823 0.092184745 0.087244712 0.079242639 0.069895022 0.068055458 0.074540541 0.084640555 0.095842533 0.11196797][0.21996905 0.20105542 0.18623343 0.17875743 0.16500102 0.14800185 0.13660589 0.13019954 0.11955623 0.1064679 0.10329407 0.11209594 0.1269006 0.14373751 0.1655077][0.24959907 0.23248447 0.21700926 0.21044745 0.19875884 0.18423168 0.17479362 0.16886954 0.15722266 0.14189442 0.13785496 0.14881651 0.16799563 0.18950243 0.215206][0.24602959 0.23165753 0.21890227 0.21813928 0.21430036 0.20668623 0.20112871 0.19625874 0.18413846 0.16737537 0.16260411 0.17526548 0.19779626 0.22233112 0.24941573][0.21466474 0.20641702 0.20081294 0.20939982 0.21548523 0.2148668 0.21232069 0.20805722 0.19625303 0.17986481 0.17534232 0.18895303 0.21262483 0.23684907 0.2615045][0.18238203 0.18152824 0.18332991 0.19958723 0.21268673 0.21624655 0.21491575 0.21154861 0.20164424 0.18794592 0.18406159 0.19636416 0.21695848 0.23637617 0.25536627][0.17424931 0.17930482 0.18488398 0.20309825 0.21742871 0.22116743 0.21915226 0.21607308 0.20794068 0.19723573 0.19324279 0.20144421 0.21524018 0.22738412 0.24067478][0.19143052 0.19925927 0.20386846 0.21906608 0.23054063 0.23212993 0.22841592 0.22507203 0.21815991 0.20961075 0.20442609 0.20692152 0.21268877 0.21781449 0.22721855][0.2152791 0.22326024 0.22460823 0.23523314 0.24280314 0.24143353 0.23595037 0.23274857 0.22796865 0.22230048 0.21717054 0.21528038 0.21448164 0.21434699 0.22043033][0.23055689 0.23762128 0.23555566 0.24089654 0.24284849 0.23666646 0.22814921 0.22509153 0.22450697 0.22548071 0.22527948 0.22378197 0.22011833 0.21622494 0.21757258][0.23287876 0.23979945 0.23560198 0.23506889 0.22915645 0.21573065 0.2023451 0.19852196 0.20298932 0.21338783 0.22239953 0.22553647 0.22206792 0.21565284 0.21144193][0.22623396 0.23448709 0.22966813 0.22251676 0.20699669 0.18500179 0.16604026 0.16063765 0.16935553 0.1889803 0.20809095 0.21723786 0.21536265 0.20764966 0.1988422][0.21649644 0.22681504 0.222227 0.20910975 0.18523927 0.15605769 0.13265519 0.12571935 0.13774765 0.16465598 0.19182688 0.20579167 0.20538399 0.19703531 0.18515006][0.20406993 0.21717449 0.21282068 0.1952472 0.16643998 0.13381281 0.10833655 0.10015488 0.11423954 0.14671321 0.18087552 0.20021714 0.20305717 0.19620518 0.18371843][0.1932523 0.20750013 0.20232627 0.1815756 0.15127164 0.11902931 0.093569994 0.08366137 0.097117066 0.13204792 0.17171888 0.19773173 0.2071912 0.20591788 0.19660322]]...]
INFO - root - 2017-12-11 09:26:22.308619: step 46910, loss = 0.70, batch loss = 0.64 (14.2 examples/sec; 0.563 sec/batch; 44h:40m:51s remains)
INFO - root - 2017-12-11 09:26:27.734580: step 46920, loss = 0.70, batch loss = 0.64 (14.8 examples/sec; 0.542 sec/batch; 43h:00m:13s remains)
INFO - root - 2017-12-11 09:26:33.120013: step 46930, loss = 0.68, batch loss = 0.62 (15.0 examples/sec; 0.533 sec/batch; 42h:17m:39s remains)
INFO - root - 2017-12-11 09:26:38.227126: step 46940, loss = 0.69, batch loss = 0.63 (14.7 examples/sec; 0.545 sec/batch; 43h:12m:27s remains)
INFO - root - 2017-12-11 09:26:43.775468: step 46950, loss = 0.70, batch loss = 0.64 (14.5 examples/sec; 0.553 sec/batch; 43h:52m:10s remains)
INFO - root - 2017-12-11 09:26:49.271945: step 46960, loss = 0.70, batch loss = 0.65 (14.4 examples/sec; 0.556 sec/batch; 44h:07m:16s remains)
INFO - root - 2017-12-11 09:26:54.679493: step 46970, loss = 0.68, batch loss = 0.63 (14.6 examples/sec; 0.546 sec/batch; 43h:19m:43s remains)
INFO - root - 2017-12-11 09:27:00.113312: step 46980, loss = 0.68, batch loss = 0.63 (15.1 examples/sec; 0.531 sec/batch; 42h:05m:43s remains)
INFO - root - 2017-12-11 09:27:05.536368: step 46990, loss = 0.70, batch loss = 0.64 (14.1 examples/sec; 0.568 sec/batch; 45h:01m:17s remains)
INFO - root - 2017-12-11 09:27:10.973033: step 47000, loss = 0.71, batch loss = 0.65 (14.8 examples/sec; 0.542 sec/batch; 42h:58m:32s remains)
2017-12-11 09:27:11.599684: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.22403328 0.23172539 0.18874754 0.11209746 0.03540479 -0.012523037 -0.019797055 -0.0031180421 0.019858103 0.039054394 0.049735121 0.053314105 0.0542819 0.054920588 0.051364265][0.18343577 0.18979743 0.15528677 0.092544466 0.029234622 -0.010294324 -0.015587997 0.00069025997 0.024762563 0.049634743 0.073263489 0.096492179 0.11831482 0.1344972 0.13653144][0.13628088 0.13908306 0.11433475 0.072451584 0.0345189 0.017344831 0.027147761 0.05261977 0.083127163 0.11665441 0.15730788 0.20793021 0.26055762 0.29890883 0.30547765][0.11874212 0.12388499 0.11420976 0.10008707 0.096541077 0.11182208 0.14439525 0.18106218 0.21345457 0.24696541 0.29589236 0.36619145 0.44348392 0.49815086 0.50259793][0.12376586 0.13654178 0.14733009 0.16730502 0.20511766 0.25812134 0.31495532 0.35958359 0.38667303 0.408756 0.44873297 0.51460433 0.58872181 0.635823 0.62670696][0.13596104 0.16098548 0.19734852 0.25565591 0.33490685 0.41872016 0.48753268 0.5265922 0.53792661 0.54019755 0.55950016 0.60073918 0.64407057 0.659686 0.62800151][0.14925501 0.18944736 0.24927704 0.33566421 0.438163 0.53091449 0.59243345 0.61529505 0.61149025 0.60273784 0.6100387 0.6292702 0.63736755 0.61477166 0.55703741][0.15402034 0.20700966 0.27966151 0.37353331 0.47412074 0.55527151 0.59905869 0.60595953 0.59355241 0.58218592 0.58466572 0.58817697 0.56800842 0.514451 0.43699422][0.13741018 0.19305207 0.26246575 0.34203559 0.4192915 0.47536373 0.49842525 0.49297494 0.47624159 0.46425352 0.46413496 0.45954069 0.42668056 0.361145 0.2796042][0.09658277 0.13996628 0.19030353 0.2423867 0.28952873 0.3214272 0.33086225 0.32161751 0.3066133 0.296784 0.29681715 0.29182208 0.26021579 0.19871385 0.12575415][0.055746719 0.080104768 0.10544159 0.12846367 0.14918123 0.16349202 0.16617285 0.15762107 0.14565589 0.13753104 0.13745233 0.13450181 0.11059491 0.062489714 0.007133015][0.039382633 0.049387362 0.052979108 0.049818631 0.046530087 0.04525296 0.042308375 0.035294633 0.027386129 0.022121433 0.021733494 0.019084202 0.0019900724 -0.031042291 -0.0658277][0.044800084 0.048058454 0.037335373 0.014483909 -0.0085331788 -0.023196118 -0.03053573 -0.034281295 -0.035501659 -0.035210766 -0.034923341 -0.039342925 -0.053607877 -0.075901724 -0.094982624][0.057304606 0.057423946 0.038558878 0.003832021 -0.032047991 -0.055559777 -0.064235821 -0.061859615 -0.053776175 -0.045831691 -0.043439347 -0.049693808 -0.063765869 -0.080990635 -0.0924984][0.078428708 0.074040122 0.048631784 0.0068248264 -0.035825137 -0.063628539 -0.071474366 -0.063155249 -0.048131559 -0.036194582 -0.03482886 -0.044880915 -0.061016325 -0.076577947 -0.084486686]]...]
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-SGD-lr0.001-clip50-initfromfixqian30k-fixqian
INFO - root - 2017-12-11 09:27:17.033251: step 47010, loss = 0.69, batch loss = 0.63 (14.6 examples/sec; 0.547 sec/batch; 43h:20m:38s remains)
INFO - root - 2017-12-11 09:27:22.508986: step 47020, loss = 0.70, batch loss = 0.64 (14.2 examples/sec; 0.564 sec/batch; 44h:42m:42s remains)
INFO - root - 2017-12-11 09:27:27.971615: step 47030, loss = 0.69, batch loss = 0.63 (14.9 examples/sec; 0.535 sec/batch; 42h:27m:02s remains)
INFO - root - 2017-12-11 09:27:33.062961: step 47040, loss = 0.69, batch loss = 0.63 (14.7 examples/sec; 0.543 sec/batch; 43h:01m:53s remains)
INFO - root - 2017-12-11 09:27:38.610621: step 47050, loss = 0.71, batch loss = 0.65 (14.5 examples/sec; 0.553 sec/batch; 43h:51m:08s remains)
INFO - root - 2017-12-11 09:27:44.087095: step 47060, loss = 0.68, batch loss = 0.62 (14.6 examples/sec; 0.547 sec/batch; 43h:22m:01s remains)
INFO - root - 2017-12-11 09:27:49.584790: step 47070, loss = 0.70, batch loss = 0.64 (14.7 examples/sec; 0.546 sec/batch; 43h:16m:42s remains)
INFO - root - 2017-12-11 09:27:54.996900: step 47080, loss = 0.69, batch loss = 0.64 (15.0 examples/sec; 0.532 sec/batch; 42h:12m:11s remains)
INFO - root - 2017-12-11 09:28:00.391601: step 47090, loss = 0.70, batch loss = 0.64 (15.0 examples/sec; 0.532 sec/batch; 42h:11m:34s remains)
INFO - root - 2017-12-11 09:28:05.836813: step 47100, loss = 0.69, batch loss = 0.63 (14.7 examples/sec; 0.545 sec/batch; 43h:14m:35s remains)
2017-12-11 09:28:06.431663: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.10332538 -0.1063395 -0.10784165 -0.10914693 -0.11025196 -0.11168771 -0.11350858 -0.1153765 -0.1167776 -0.11758772 -0.11783432 -0.11775765 -0.11723725 -0.11587303 -0.11335956][-0.11112326 -0.11197464 -0.10994982 -0.10710842 -0.10467246 -0.10402175 -0.10537661 -0.10803348 -0.11133969 -0.11515052 -0.11934278 -0.12379165 -0.12751131 -0.12899569 -0.12702192][-0.11205947 -0.10562096 -0.093562476 -0.079468146 -0.066747442 -0.058128975 -0.054271631 -0.054593846 -0.059073392 -0.068183579 -0.08169277 -0.098591909 -0.11538103 -0.12760565 -0.13206336][-0.1000571 -0.08018215 -0.050539576 -0.016736625 0.015060662 0.040721513 0.058310606 0.066669255 0.063325509 0.046123769 0.015393506 -0.025631847 -0.068679735 -0.10386548 -0.12417898][-0.0657783 -0.029538164 0.02100612 0.079232976 0.13674787 0.18881285 0.23070855 0.2565403 0.25837362 0.23087758 0.17504069 0.098112188 0.015688878 -0.054445591 -0.10008933][-0.0081582116 0.041196048 0.10876893 0.18904261 0.27296454 0.35584018 0.42901832 0.47932297 0.49072966 0.45328817 0.36903423 0.25062469 0.12250969 0.011626099 -0.064279236][0.064471558 0.1168372 0.18908265 0.27945712 0.38065523 0.48880276 0.59118742 0.66650039 0.68998694 0.64637417 0.53847647 0.38440391 0.21680407 0.070820071 -0.031020304][0.13455504 0.1755095 0.23496257 0.31678027 0.41722193 0.53431588 0.65264124 0.74447453 0.77831227 0.73434967 0.61562812 0.44382769 0.256911 0.094888985 -0.0178565][0.17842457 0.19578773 0.22788659 0.28356275 0.36298054 0.46681309 0.58000344 0.672855 0.71165031 0.67446768 0.563434 0.40007436 0.22253038 0.070573241 -0.032770775][0.17242973 0.1626429 0.16369067 0.18636109 0.23341955 0.3076838 0.39714083 0.47520894 0.51152736 0.4856503 0.3983877 0.26781231 0.12674341 0.0092578055 -0.066246085][0.11767256 0.088457786 0.067632444 0.064110167 0.080792695 0.12115119 0.17734815 0.22980238 0.25618142 0.24112022 0.18488753 0.10046457 0.011724835 -0.056976926 -0.094432518][0.045928184 0.010819979 -0.016428277 -0.031118367 -0.032654844 -0.018911492 0.0060545849 0.030934473 0.043530107 0.035542943 0.0081150848 -0.030794526 -0.067183241 -0.08785031 -0.089636736][-0.0075567868 -0.03615661 -0.056109756 -0.067214891 -0.072307982 -0.071922392 -0.067640625 -0.063639157 -0.063249774 -0.068123795 -0.075320549 -0.080548972 -0.078300022 -0.065150931 -0.044457685][-0.024045547 -0.039566778 -0.045564994 -0.045375574 -0.044676516 -0.046435893 -0.05093503 -0.057509836 -0.063978218 -0.067687124 -0.064374819 -0.05204533 -0.031012362 -0.0041275187 0.022018218][-0.0067366832 -0.010244073 -0.0043901266 0.00585042 0.013663789 0.0148018 0.0094208829 0.00051881984 -0.0075442307 -0.010524786 -0.0040628458 0.012347651 0.035333131 0.059599124 0.078461431]]...]
INFO - root - 2017-12-11 09:28:11.890642: step 47110, loss = 0.67, batch loss = 0.62 (14.4 examples/sec; 0.555 sec/batch; 44h:01m:56s remains)
INFO - root - 2017-12-11 09:28:17.356601: step 47120, loss = 0.69, batch loss = 0.63 (14.8 examples/sec; 0.542 sec/batch; 42h:57m:54s remains)
INFO - root - 2017-12-11 09:28:22.721804: step 47130, loss = 0.70, batch loss = 0.65 (15.0 examples/sec; 0.534 sec/batch; 42h:21m:22s remains)
INFO - root - 2017-12-11 09:28:27.684482: step 47140, loss = 0.70, batch loss = 0.64 (14.9 examples/sec; 0.536 sec/batch; 42h:27m:11s remains)
INFO - root - 2017-12-11 09:28:33.015212: step 47150, loss = 0.71, batch loss = 0.66 (15.1 examples/sec; 0.531 sec/batch; 42h:03m:41s remains)
INFO - root - 2017-12-11 09:28:38.437642: step 47160, loss = 0.69, batch loss = 0.63 (14.8 examples/sec; 0.542 sec/batch; 42h:57m:26s remains)
INFO - root - 2017-12-11 09:28:43.805275: step 47170, loss = 0.69, batch loss = 0.63 (15.0 examples/sec; 0.532 sec/batch; 42h:11m:31s remains)
INFO - root - 2017-12-11 09:28:49.162665: step 47180, loss = 0.70, batch loss = 0.64 (15.1 examples/sec; 0.528 sec/batch; 41h:51m:40s remains)
INFO - root - 2017-12-11 09:28:54.491615: step 47190, loss = 0.70, batch loss = 0.64 (15.3 examples/sec; 0.524 sec/batch; 41h:31m:13s remains)
INFO - root - 2017-12-11 09:28:59.903100: step 47200, loss = 0.70, batch loss = 0.64 (14.9 examples/sec; 0.536 sec/batch; 42h:29m:04s remains)
2017-12-11 09:29:00.475282: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.04160757 0.042275112 0.048002049 0.059536923 0.071435183 0.077129222 0.076273836 0.070682622 0.065239705 0.06424661 0.0666131 0.067516759 0.06234213 0.0519424 0.040717945][0.064781033 0.066431336 0.0737978 0.087030873 0.099214345 0.10319623 0.099940293 0.09353289 0.09063866 0.09523011 0.10354181 0.10688767 0.098083153 0.078315079 0.055088267][0.10500173 0.10512046 0.11083747 0.12256502 0.13267799 0.13374296 0.12771569 0.120878 0.12166694 0.13337977 0.1490954 0.1563883 0.14596267 0.11883526 0.084395781][0.16557319 0.16319133 0.16660541 0.17678966 0.18622468 0.18660188 0.17946725 0.17278767 0.17665029 0.1937393 0.21371651 0.22152534 0.20762618 0.17354129 0.13048236][0.23892815 0.23869677 0.24471644 0.25910884 0.27489412 0.28164375 0.27812135 0.27248824 0.27608663 0.29171059 0.306568 0.30539748 0.28128821 0.23798922 0.18866032][0.31053495 0.31706214 0.33118874 0.35542148 0.38384727 0.40308315 0.40769136 0.40428418 0.40446842 0.41196212 0.41299152 0.3942045 0.35301486 0.29705569 0.24203941][0.36007133 0.37669295 0.40343717 0.44257894 0.4891285 0.52620429 0.54369593 0.5450353 0.54023045 0.53396422 0.51317394 0.46859506 0.40362257 0.33172822 0.2713882][0.35947871 0.38535377 0.42399445 0.47708562 0.54033369 0.59425372 0.62558776 0.63452923 0.62804115 0.61066258 0.570669 0.50335318 0.41770217 0.33255923 0.268274][0.30541736 0.33406019 0.37709859 0.43502444 0.50462496 0.56690735 0.60878921 0.62799734 0.62638485 0.60580581 0.55555463 0.47495663 0.3774249 0.2855095 0.22006041][0.21956672 0.24573728 0.28526717 0.337129 0.39884382 0.45503291 0.49706289 0.52248585 0.5280565 0.5114516 0.46221703 0.38185495 0.28559205 0.19694227 0.13613172][0.13822035 0.15884736 0.18900988 0.22584714 0.26688582 0.30234134 0.330546 0.35156387 0.35945356 0.348477 0.30875933 0.24206261 0.16206916 0.089921124 0.043631624][0.10408871 0.12208989 0.1425705 0.16163468 0.17638391 0.18331821 0.18813993 0.19502184 0.19814727 0.1896131 0.16033366 0.11146594 0.053518679 0.0035039485 -0.023865925][0.12092006 0.13892733 0.15197897 0.15611573 0.14831103 0.13029686 0.113104 0.10468345 0.10102476 0.092933357 0.071828239 0.038027026 -0.00062496186 -0.031041343 -0.042169385][0.14551508 0.16333027 0.17183186 0.16790202 0.14904356 0.11960138 0.092208423 0.076384075 0.069990717 0.063305415 0.048140291 0.024504712 -0.00084983447 -0.01750139 -0.017703][0.14199911 0.15781768 0.16392966 0.15882646 0.14097612 0.11480715 0.090912081 0.077782422 0.0742358 0.071045689 0.061291836 0.045625322 0.030572668 0.023933984 0.030136671]]...]
INFO - root - 2017-12-11 09:29:05.989754: step 47210, loss = 0.69, batch loss = 0.63 (14.8 examples/sec; 0.542 sec/batch; 42h:58m:03s remains)
INFO - root - 2017-12-11 09:29:11.447936: step 47220, loss = 0.70, batch loss = 0.64 (15.0 examples/sec; 0.534 sec/batch; 42h:18m:55s remains)
INFO - root - 2017-12-11 09:29:16.891804: step 47230, loss = 0.68, batch loss = 0.63 (14.9 examples/sec; 0.536 sec/batch; 42h:28m:11s remains)
INFO - root - 2017-12-11 09:29:21.884413: step 47240, loss = 0.69, batch loss = 0.64 (14.8 examples/sec; 0.539 sec/batch; 42h:42m:54s remains)
INFO - root - 2017-12-11 09:29:27.279805: step 47250, loss = 0.70, batch loss = 0.64 (14.8 examples/sec; 0.540 sec/batch; 42h:49m:13s remains)
INFO - root - 2017-12-11 09:29:32.635304: step 47260, loss = 0.72, batch loss = 0.66 (15.0 examples/sec; 0.532 sec/batch; 42h:07m:38s remains)
INFO - root - 2017-12-11 09:29:37.974630: step 47270, loss = 0.70, batch loss = 0.64 (14.9 examples/sec; 0.539 sec/batch; 42h:40m:20s remains)
INFO - root - 2017-12-11 09:29:43.347692: step 47280, loss = 0.72, batch loss = 0.66 (15.1 examples/sec; 0.530 sec/batch; 42h:01m:20s remains)
INFO - root - 2017-12-11 09:29:48.724692: step 47290, loss = 0.69, batch loss = 0.63 (14.0 examples/sec; 0.572 sec/batch; 45h:20m:55s remains)
INFO - root - 2017-12-11 09:29:54.183801: step 47300, loss = 0.71, batch loss = 0.65 (14.6 examples/sec; 0.550 sec/batch; 43h:33m:16s remains)
2017-12-11 09:29:54.813205: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.066800058 -0.069353253 -0.067374714 -0.064406969 -0.06327045 -0.066090807 -0.072313681 -0.077372961 -0.073736146 -0.060038976 -0.041282974 -0.025644934 -0.019095253 -0.023696078 -0.038743947][-0.029495317 -0.032205589 -0.029700581 -0.023211576 -0.018161908 -0.019846031 -0.029148053 -0.040480569 -0.041423231 -0.02831611 -0.0067004371 0.012794093 0.022059014 0.017007543 -0.0046711713][0.024000619 0.021883296 0.027805047 0.044754557 0.062902853 0.070822276 0.063279353 0.044679828 0.030575026 0.029017854 0.037805617 0.048460674 0.053296145 0.045965374 0.01995356][0.080385305 0.083452962 0.10158719 0.14206325 0.18778896 0.21882693 0.22318852 0.20033203 0.16574523 0.13301712 0.1083677 0.090616293 0.075874262 0.058064021 0.026409905][0.13390654 0.15159342 0.19356127 0.26908433 0.35315466 0.41843721 0.44610944 0.42745271 0.37331802 0.30206704 0.22989252 0.16527122 0.11191464 0.068385743 0.023310212][0.17536993 0.21258453 0.28242543 0.39292884 0.51529741 0.61907232 0.68015033 0.67910844 0.61817765 0.51807559 0.40148532 0.28528652 0.18174903 0.099163517 0.030649628][0.19848979 0.25158522 0.34069002 0.47175404 0.61867803 0.75340712 0.84882283 0.87500066 0.8251543 0.71952933 0.5809412 0.42825186 0.27998823 0.15669885 0.060046878][0.20287153 0.26151276 0.35311607 0.48252046 0.63240635 0.78100061 0.90023565 0.95301908 0.92764288 0.84248185 0.71287185 0.55170327 0.37964875 0.228384 0.1082949][0.18771364 0.23980469 0.31667793 0.42300734 0.5519917 0.6895287 0.80974662 0.8757571 0.877455 0.83083075 0.73829043 0.60060507 0.43605313 0.28202611 0.15426458][0.15100218 0.18703578 0.23777786 0.30734175 0.3973285 0.50093788 0.5982371 0.66005105 0.68145335 0.67512053 0.6303876 0.53672332 0.40754431 0.27859664 0.16534206][0.085436441 0.1001752 0.12104119 0.15151733 0.19769123 0.2586211 0.32222259 0.36986819 0.40091065 0.42295527 0.41743466 0.3690522 0.28741059 0.20100856 0.12003643][0.0067199348 0.0027446023 -4.4651035e-05 0.00039813996 0.010933213 0.034184482 0.065274939 0.09479174 0.12392303 0.1555732 0.17037342 0.15477653 0.11610459 0.073331885 0.029833883][-0.0574835 -0.072560288 -0.087864019 -0.10318512 -0.11328749 -0.11447933 -0.10704119 -0.094304 -0.075001583 -0.048846416 -0.030787384 -0.029462265 -0.039402612 -0.050436564 -0.0640182][-0.096516654 -0.11571898 -0.13420764 -0.15332471 -0.16992763 -0.18115008 -0.18561706 -0.18388596 -0.17528814 -0.16035154 -0.14777946 -0.14156355 -0.13821125 -0.13353537 -0.1304629][-0.1139274 -0.13223942 -0.14809456 -0.16395588 -0.17813127 -0.18910982 -0.19588962 -0.1984448 -0.19658531 -0.19069205 -0.18461402 -0.17970158 -0.17421201 -0.16667789 -0.15897171]]...]
INFO - root - 2017-12-11 09:30:00.309738: step 47310, loss = 0.69, batch loss = 0.64 (14.5 examples/sec; 0.552 sec/batch; 43h:43m:59s remains)
INFO - root - 2017-12-11 09:30:05.768522: step 47320, loss = 0.70, batch loss = 0.64 (14.6 examples/sec; 0.546 sec/batch; 43h:16m:48s remains)
INFO - root - 2017-12-11 09:30:11.172967: step 47330, loss = 0.71, batch loss = 0.65 (14.9 examples/sec; 0.536 sec/batch; 42h:29m:43s remains)
INFO - root - 2017-12-11 09:30:16.252777: step 47340, loss = 0.69, batch loss = 0.64 (15.0 examples/sec; 0.534 sec/batch; 42h:17m:34s remains)
INFO - root - 2017-12-11 09:30:21.829712: step 47350, loss = 0.71, batch loss = 0.65 (14.0 examples/sec; 0.573 sec/batch; 45h:23m:53s remains)
INFO - root - 2017-12-11 09:30:27.313755: step 47360, loss = 0.70, batch loss = 0.65 (15.1 examples/sec; 0.530 sec/batch; 41h:59m:56s remains)
INFO - root - 2017-12-11 09:30:32.700567: step 47370, loss = 0.71, batch loss = 0.65 (14.9 examples/sec; 0.536 sec/batch; 42h:26m:26s remains)
INFO - root - 2017-12-11 09:30:38.157771: step 47380, loss = 0.71, batch loss = 0.65 (15.0 examples/sec; 0.535 sec/batch; 42h:20m:14s remains)
INFO - root - 2017-12-11 09:30:43.620544: step 47390, loss = 0.70, batch loss = 0.64 (15.1 examples/sec; 0.530 sec/batch; 41h:58m:19s remains)
INFO - root - 2017-12-11 09:30:48.988463: step 47400, loss = 0.69, batch loss = 0.63 (14.9 examples/sec; 0.538 sec/batch; 42h:38m:26s remains)
2017-12-11 09:30:49.604306: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.0994024 0.1137407 0.1282014 0.13310722 0.12999052 0.12508273 0.11458254 0.098841347 0.087794043 0.088355541 0.10148825 0.12751971 0.16536747 0.21320783 0.268049][0.13654277 0.16224833 0.18736944 0.20255086 0.21065439 0.21793035 0.21629544 0.20270376 0.19108278 0.19289951 0.20595543 0.22682878 0.25778154 0.30744734 0.37299827][0.18372066 0.21922114 0.25259137 0.27819958 0.30063537 0.3233985 0.33270285 0.32268628 0.31287581 0.31737471 0.32818553 0.33902109 0.35628238 0.39939314 0.46401805][0.2311009 0.27416608 0.31395894 0.35076037 0.39048359 0.43061537 0.45044491 0.44355685 0.43491015 0.43935308 0.44348836 0.44055319 0.44058254 0.47071207 0.5239889][0.26700678 0.31686434 0.36447829 0.41486079 0.47332051 0.52987969 0.55789572 0.5523206 0.54192621 0.54117674 0.53282511 0.51243323 0.49327344 0.5050146 0.53814793][0.28210098 0.33777308 0.39407051 0.45677269 0.52898014 0.59340835 0.62140942 0.61120147 0.59335434 0.58104688 0.55580777 0.51782191 0.48173478 0.47371632 0.48415413][0.27006394 0.33162978 0.39760187 0.4692972 0.54594409 0.60643303 0.62496877 0.60443288 0.57419729 0.54536378 0.501874 0.44968507 0.40238038 0.37904817 0.37258089][0.23796226 0.30489808 0.37909329 0.45453915 0.526889 0.57546937 0.58005959 0.54709476 0.50235218 0.45564151 0.39709616 0.33767703 0.2877695 0.25852874 0.24730763][0.19595671 0.26561049 0.34380439 0.41782397 0.48094693 0.51554787 0.50666732 0.46176124 0.40213192 0.33940333 0.27129865 0.21166033 0.1673813 0.14357568 0.14234363][0.15980363 0.2269133 0.30089417 0.36625528 0.41608238 0.43649021 0.41620082 0.36148393 0.29079419 0.21909268 0.15031959 0.097821906 0.065724656 0.055397976 0.072154172][0.13786821 0.19522737 0.25617915 0.30689448 0.34079915 0.34710366 0.3177838 0.25795874 0.18476792 0.11519205 0.055829473 0.016862519 0.00024253083 0.0053832247 0.038920507][0.12536137 0.16663788 0.20976064 0.24487156 0.26519769 0.2621924 0.23043275 0.17381482 0.10774445 0.048949007 0.0041235238 -0.019961743 -0.02271615 -0.0051561855 0.039107762][0.11874399 0.14257674 0.16713721 0.18783604 0.19828822 0.19129899 0.1633281 0.11645636 0.063464925 0.018857107 -0.01192712 -0.023978654 -0.017098244 0.00768212 0.055389278][0.11251272 0.12402216 0.13555361 0.14645939 0.15138921 0.14443928 0.12299776 0.087619849 0.048294991 0.016735306 -0.0033646347 -0.008040553 0.0029794751 0.028411895 0.071669772][0.10860085 0.11498849 0.12120201 0.12761334 0.12995908 0.12385991 0.10756747 0.081164017 0.05280022 0.031812523 0.019921798 0.019750178 0.031186443 0.052800622 0.086591132]]...]
INFO - root - 2017-12-11 09:30:55.163663: step 47410, loss = 0.70, batch loss = 0.64 (14.5 examples/sec; 0.551 sec/batch; 43h:37m:31s remains)
INFO - root - 2017-12-11 09:31:00.602715: step 47420, loss = 0.70, batch loss = 0.64 (14.9 examples/sec; 0.538 sec/batch; 42h:37m:41s remains)
INFO - root - 2017-12-11 09:31:05.960280: step 47430, loss = 0.71, batch loss = 0.65 (14.9 examples/sec; 0.536 sec/batch; 42h:25m:09s remains)
INFO - root - 2017-12-11 09:31:11.013451: step 47440, loss = 0.70, batch loss = 0.64 (14.7 examples/sec; 0.546 sec/batch; 43h:13m:25s remains)
INFO - root - 2017-12-11 09:31:16.518749: step 47450, loss = 0.71, batch loss = 0.65 (14.6 examples/sec; 0.549 sec/batch; 43h:26m:00s remains)
INFO - root - 2017-12-11 09:31:22.064817: step 47460, loss = 0.70, batch loss = 0.64 (14.6 examples/sec; 0.547 sec/batch; 43h:17m:18s remains)
INFO - root - 2017-12-11 09:31:27.534334: step 47470, loss = 0.70, batch loss = 0.64 (14.8 examples/sec; 0.541 sec/batch; 42h:49m:19s remains)
INFO - root - 2017-12-11 09:31:32.889480: step 47480, loss = 0.70, batch loss = 0.64 (14.9 examples/sec; 0.536 sec/batch; 42h:26m:42s remains)
INFO - root - 2017-12-11 09:31:38.321611: step 47490, loss = 0.70, batch loss = 0.64 (14.7 examples/sec; 0.543 sec/batch; 43h:01m:21s remains)
INFO - root - 2017-12-11 09:31:43.728654: step 47500, loss = 0.70, batch loss = 0.64 (15.0 examples/sec; 0.532 sec/batch; 42h:06m:38s remains)
2017-12-11 09:31:44.310020: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.060563311 -0.058963817 -0.052583922 -0.04731572 -0.045815822 -0.048171937 -0.052139223 -0.05652849 -0.060082473 -0.061879192 -0.061601076 -0.058919713 -0.053460576 -0.04772925 -0.048004039][-0.053081464 -0.045257062 -0.030460993 -0.015520813 -0.0046965354 0.0013156476 0.0035690467 0.0023611111 -0.0019667461 -0.0092788292 -0.017814815 -0.02407149 -0.023463139 -0.018569358 -0.021131344][-0.037966691 -0.020229671 0.0073841442 0.037147295 0.0635049 0.084456176 0.098762169 0.10501779 0.1012837 0.086528949 0.063608505 0.041463356 0.031008406 0.030504141 0.022446506][-0.017682878 0.014797669 0.061882645 0.11421926 0.16389418 0.20633312 0.23629509 0.25023007 0.2441204 0.21726975 0.17442243 0.13075955 0.10461236 0.095949151 0.080607809][0.0048002549 0.055117588 0.12650198 0.20762022 0.28672329 0.35445809 0.40007547 0.41816881 0.4050712 0.362302 0.29714692 0.23089576 0.18857972 0.1713631 0.14860798][0.024413049 0.091697626 0.18700746 0.29800949 0.40841749 0.50218332 0.56213343 0.58091509 0.556637 0.49468553 0.40650845 0.3198756 0.2643553 0.24153285 0.21576247][0.038049914 0.11786522 0.23162846 0.36638227 0.50140095 0.61539268 0.68566072 0.70287037 0.66673994 0.58739763 0.48206618 0.38254166 0.32000849 0.29629847 0.27278984][0.043289468 0.12966068 0.25309891 0.39970261 0.54607707 0.66839564 0.74127251 0.7540226 0.70848793 0.61957759 0.50856984 0.40688038 0.34362775 0.32200631 0.30342448][0.037658021 0.12250318 0.2440504 0.38785225 0.52981627 0.64627945 0.71232808 0.71750963 0.66627908 0.57677644 0.47198209 0.37907302 0.32263404 0.30608189 0.29383659][0.02148501 0.095269881 0.20137462 0.32604292 0.44761986 0.54566395 0.598138 0.595727 0.54475236 0.46466845 0.37648264 0.30016005 0.2549119 0.24456401 0.23916298][-0.003365349 0.051616013 0.1323116 0.22718747 0.31905609 0.39230105 0.42893445 0.42138413 0.37604156 0.31071272 0.24249162 0.18438402 0.15113735 0.14705963 0.14878538][-0.031701371 0.0011267758 0.052687794 0.11407705 0.1733885 0.22041288 0.24244422 0.23379649 0.199361 0.15247689 0.10526522 0.064974651 0.04239687 0.041987848 0.04763779][-0.055331565 -0.042156998 -0.016749579 0.014210073 0.044072993 0.067611009 0.077468216 0.07010407 0.048363544 0.020031668 -0.0083196377 -0.033517029 -0.04782223 -0.04707709 -0.040475871][-0.07167615 -0.072305031 -0.065372914 -0.056109644 -0.047086094 -0.040046912 -0.038723063 -0.045025095 -0.057768889 -0.073293857 -0.08896631 -0.10344789 -0.1111302 -0.10899743 -0.10157856][-0.081384078 -0.090021268 -0.093617864 -0.096587718 -0.099093065 -0.10107106 -0.10422766 -0.10942489 -0.11650816 -0.12421393 -0.13189939 -0.13866101 -0.14074887 -0.13648495 -0.12815535]]...]
INFO - root - 2017-12-11 09:31:49.820696: step 47510, loss = 0.70, batch loss = 0.64 (14.6 examples/sec; 0.548 sec/batch; 43h:23m:07s remains)
INFO - root - 2017-12-11 09:31:55.308355: step 47520, loss = 0.71, batch loss = 0.65 (15.0 examples/sec; 0.532 sec/batch; 42h:05m:28s remains)
INFO - root - 2017-12-11 09:32:00.676378: step 47530, loss = 0.69, batch loss = 0.64 (14.8 examples/sec; 0.540 sec/batch; 42h:42m:45s remains)
INFO - root - 2017-12-11 09:32:05.717673: step 47540, loss = 0.71, batch loss = 0.65 (15.0 examples/sec; 0.535 sec/batch; 42h:19m:19s remains)
INFO - root - 2017-12-11 09:32:11.118843: step 47550, loss = 0.68, batch loss = 0.62 (15.2 examples/sec; 0.527 sec/batch; 41h:42m:08s remains)
INFO - root - 2017-12-11 09:32:16.541930: step 47560, loss = 0.69, batch loss = 0.63 (14.2 examples/sec; 0.563 sec/batch; 44h:31m:26s remains)
INFO - root - 2017-12-11 09:32:21.930199: step 47570, loss = 0.70, batch loss = 0.64 (14.4 examples/sec; 0.554 sec/batch; 43h:50m:28s remains)
INFO - root - 2017-12-11 09:32:27.325716: step 47580, loss = 0.71, batch loss = 0.65 (14.9 examples/sec; 0.536 sec/batch; 42h:25m:28s remains)
INFO - root - 2017-12-11 09:32:32.684272: step 47590, loss = 0.68, batch loss = 0.62 (15.0 examples/sec; 0.535 sec/batch; 42h:20m:38s remains)
INFO - root - 2017-12-11 09:32:38.117196: step 47600, loss = 0.69, batch loss = 0.63 (14.4 examples/sec; 0.556 sec/batch; 44h:00m:58s remains)
2017-12-11 09:32:38.725968: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.27970374 0.27699053 0.25323483 0.22811328 0.20080732 0.16816166 0.12740268 0.082737155 0.049860727 0.042780779 0.064240187 0.093196347 0.11144996 0.11801201 0.115343][0.30077246 0.2861743 0.2510775 0.21906124 0.18953709 0.16008523 0.12865947 0.09616679 0.074934743 0.078947283 0.110118 0.14278877 0.15688267 0.15667391 0.1504662][0.28069359 0.25354007 0.21511649 0.18850824 0.17106797 0.16018759 0.15192585 0.14146306 0.1354561 0.14709954 0.17876908 0.20382655 0.20483308 0.19327705 0.18461522][0.22746135 0.19890049 0.17348927 0.16810599 0.17612922 0.19523533 0.21726604 0.23026326 0.23500867 0.24473459 0.26453316 0.27110824 0.25287148 0.22863136 0.21967623][0.15852363 0.14788288 0.1548021 0.18384208 0.22438607 0.27580929 0.32494771 0.35320047 0.35831439 0.35570732 0.35442778 0.33779597 0.30145723 0.26950336 0.264413][0.10222416 0.12661821 0.17828947 0.2455591 0.31509805 0.38955617 0.45244667 0.48134157 0.47514522 0.45318741 0.42836002 0.3916539 0.34519431 0.31468514 0.31838581][0.078953721 0.14257681 0.23670171 0.33324787 0.41789868 0.49733487 0.55564064 0.57072216 0.5461309 0.50527751 0.46365571 0.41814265 0.37467295 0.35558242 0.37073445][0.0851255 0.17864253 0.30016002 0.409069 0.49129838 0.55728692 0.59582537 0.58837944 0.54420143 0.48949856 0.4416258 0.4004584 0.37220797 0.3716017 0.3986246][0.095480129 0.20077538 0.32907665 0.43319491 0.50028992 0.54305667 0.55633777 0.52644175 0.466273 0.40371382 0.35717398 0.32768014 0.31911409 0.33716214 0.37417185][0.08436656 0.18315488 0.30022061 0.38879085 0.43752205 0.45902622 0.45273635 0.40909982 0.34056759 0.27465504 0.23082215 0.21148868 0.21736796 0.24669527 0.28830233][0.043839082 0.12256751 0.21733622 0.28739572 0.3222889 0.33237779 0.31869367 0.274042 0.2062338 0.14003436 0.097174183 0.082884341 0.094381437 0.12472809 0.16370291][-0.0091530783 0.042901274 0.11176947 0.16611475 0.19524461 0.20518696 0.19606908 0.16114073 0.10199428 0.038885169 -0.0039603296 -0.018014314 -0.0087686926 0.013947293 0.043295778][-0.047283709 -0.020373574 0.025885312 0.07042551 0.10128293 0.11889652 0.12107553 0.10034409 0.052912958 -0.0056808772 -0.049288217 -0.066672213 -0.06464909 -0.054271419 -0.038244449][-0.054314878 -0.0472229 -0.018065732 0.021105703 0.056862548 0.084308423 0.099182941 0.092868321 0.057840291 0.0051504225 -0.038937196 -0.061353356 -0.068221 -0.070327714 -0.066771939][-0.029860964 -0.035793003 -0.018974083 0.015796214 0.054023053 0.087667592 0.11159574 0.11667529 0.093389094 0.048248902 0.0052881138 -0.021600159 -0.035796542 -0.046656743 -0.05108444]]...]
INFO - root - 2017-12-11 09:32:44.267134: step 47610, loss = 0.69, batch loss = 0.63 (14.3 examples/sec; 0.561 sec/batch; 44h:24m:56s remains)
INFO - root - 2017-12-11 09:32:49.704906: step 47620, loss = 0.72, batch loss = 0.66 (14.8 examples/sec; 0.540 sec/batch; 42h:43m:50s remains)
INFO - root - 2017-12-11 09:32:55.073612: step 47630, loss = 0.69, batch loss = 0.64 (14.7 examples/sec; 0.543 sec/batch; 42h:58m:49s remains)
INFO - root - 2017-12-11 09:33:00.187051: step 47640, loss = 0.69, batch loss = 0.63 (14.5 examples/sec; 0.551 sec/batch; 43h:37m:27s remains)
INFO - root - 2017-12-11 09:33:05.806904: step 47650, loss = 0.69, batch loss = 0.63 (15.2 examples/sec; 0.526 sec/batch; 41h:36m:37s remains)
INFO - root - 2017-12-11 09:33:11.242607: step 47660, loss = 0.70, batch loss = 0.64 (14.9 examples/sec; 0.537 sec/batch; 42h:28m:51s remains)
INFO - root - 2017-12-11 09:33:16.631722: step 47670, loss = 0.72, batch loss = 0.66 (15.2 examples/sec; 0.528 sec/batch; 41h:45m:28s remains)
INFO - root - 2017-12-11 09:33:22.070071: step 47680, loss = 0.72, batch loss = 0.66 (14.9 examples/sec; 0.537 sec/batch; 42h:28m:53s remains)
INFO - root - 2017-12-11 09:33:27.480646: step 47690, loss = 0.71, batch loss = 0.66 (14.7 examples/sec; 0.542 sec/batch; 42h:55m:03s remains)
INFO - root - 2017-12-11 09:33:32.854958: step 47700, loss = 0.70, batch loss = 0.64 (14.8 examples/sec; 0.541 sec/batch; 42h:48m:48s remains)
2017-12-11 09:33:33.422273: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.24258547 0.24060468 0.2340508 0.24154525 0.2551924 0.26583314 0.26728237 0.25924915 0.25726885 0.26558813 0.28737575 0.31833419 0.3447251 0.358781 0.34068298][0.29071659 0.29027313 0.27861166 0.27361384 0.26842535 0.25978118 0.2460774 0.22923945 0.22536126 0.24025935 0.27251711 0.30961016 0.33486256 0.34351712 0.32001719][0.34275526 0.3449308 0.32778433 0.30992562 0.28587407 0.26012269 0.23524852 0.2124996 0.20447792 0.2172617 0.24695115 0.27641824 0.29108119 0.29134485 0.26670173][0.39580747 0.39885476 0.37871656 0.35475963 0.32206407 0.29154482 0.26615089 0.24282087 0.22823827 0.22888389 0.24165325 0.24952598 0.2436782 0.23065604 0.20505406][0.43913469 0.44234395 0.42442074 0.40563077 0.37985528 0.36028254 0.34577602 0.32656229 0.30361193 0.28505743 0.27238595 0.25261059 0.22307929 0.19482486 0.16621819][0.46284834 0.46425369 0.45243052 0.44848105 0.44261712 0.44495231 0.44861218 0.43738896 0.4075987 0.36885169 0.329939 0.28505769 0.23618488 0.19561006 0.16335946][0.44893822 0.44558761 0.44034278 0.45488453 0.47353569 0.49976337 0.5223242 0.52150059 0.48987147 0.43611842 0.37695765 0.31588981 0.25781703 0.21239914 0.17885992][0.39037171 0.37968731 0.37869063 0.40827897 0.4467262 0.48949236 0.52355975 0.53065437 0.50219637 0.44378078 0.37721121 0.31418678 0.26049474 0.22082163 0.19149427][0.29348949 0.27663326 0.27747279 0.31462815 0.36311364 0.411348 0.44666281 0.45616248 0.4333401 0.38053811 0.32037565 0.26886275 0.23061121 0.20507082 0.18524995][0.17567317 0.15684797 0.1585432 0.1957064 0.24396063 0.28747749 0.31522956 0.3214446 0.30400148 0.26299173 0.21830708 0.18555537 0.16674317 0.15732603 0.14839062][0.061252054 0.044890955 0.047535311 0.078686386 0.1180156 0.14996186 0.16597784 0.16613486 0.15255769 0.124685 0.097888157 0.084301315 0.082850493 0.086407341 0.085879706][-0.024393229 -0.0349735 -0.030818477 -0.0085865716 0.017213028 0.035076272 0.039605495 0.034078348 0.022963876 0.0059944694 -0.0062605063 -0.0061922111 0.0026491014 0.012359308 0.015312346][-0.070059583 -0.075292192 -0.070686147 -0.057667073 -0.0449564 -0.038872004 -0.041697502 -0.049532026 -0.057874884 -0.066620335 -0.069471955 -0.062791422 -0.051119074 -0.041415267 -0.038878504][-0.080352545 -0.082481466 -0.078809224 -0.072816581 -0.068940483 -0.06968414 -0.0752514 -0.082484484 -0.088077247 -0.09174519 -0.090399072 -0.0829155 -0.073297955 -0.066520073 -0.065445282][-0.069790438 -0.071249589 -0.069500163 -0.067941107 -0.068475775 -0.071655355 -0.076830372 -0.081977449 -0.085240491 -0.086419389 -0.084427476 -0.079196423 -0.073410414 -0.069942951 -0.070003271]]...]
INFO - root - 2017-12-11 09:33:38.870764: step 47710, loss = 0.68, batch loss = 0.62 (14.2 examples/sec; 0.562 sec/batch; 44h:26m:36s remains)
INFO - root - 2017-12-11 09:33:44.340522: step 47720, loss = 0.71, batch loss = 0.65 (14.7 examples/sec; 0.544 sec/batch; 43h:02m:55s remains)
INFO - root - 2017-12-11 09:33:49.729470: step 47730, loss = 0.71, batch loss = 0.66 (14.7 examples/sec; 0.546 sec/batch; 43h:11m:38s remains)
INFO - root - 2017-12-11 09:33:54.895046: step 47740, loss = 0.72, batch loss = 0.66 (14.2 examples/sec; 0.565 sec/batch; 44h:40m:37s remains)
INFO - root - 2017-12-11 09:34:00.424198: step 47750, loss = 0.68, batch loss = 0.63 (14.4 examples/sec; 0.554 sec/batch; 43h:49m:20s remains)
INFO - root - 2017-12-11 09:34:05.992677: step 47760, loss = 0.69, batch loss = 0.63 (14.2 examples/sec; 0.562 sec/batch; 44h:26m:43s remains)
INFO - root - 2017-12-11 09:34:11.353077: step 47770, loss = 0.69, batch loss = 0.63 (14.9 examples/sec; 0.538 sec/batch; 42h:35m:21s remains)
INFO - root - 2017-12-11 09:34:16.768635: step 47780, loss = 0.68, batch loss = 0.62 (14.7 examples/sec; 0.545 sec/batch; 43h:08m:18s remains)
INFO - root - 2017-12-11 09:34:22.158394: step 47790, loss = 0.70, batch loss = 0.64 (14.9 examples/sec; 0.536 sec/batch; 42h:24m:37s remains)
INFO - root - 2017-12-11 09:34:27.574001: step 47800, loss = 0.71, batch loss = 0.65 (14.8 examples/sec; 0.542 sec/batch; 42h:50m:18s remains)
2017-12-11 09:34:28.158982: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.019952178 0.037110057 0.0535041 0.067114033 0.077508718 0.086011738 0.090948626 0.089857414 0.085778549 0.083776109 0.082792059 0.078675635 0.070398517 0.062113572 0.059653141][0.020703621 0.037870008 0.053580932 0.06639199 0.076212943 0.08499752 0.091941215 0.092805892 0.088646725 0.083751105 0.08047533 0.078010164 0.074214466 0.070741944 0.072217561][0.0178524 0.032947104 0.046048954 0.056138694 0.063446067 0.070430107 0.077599354 0.079876781 0.076117471 0.069203466 0.064770922 0.064964145 0.066065192 0.067537978 0.073448978][0.019025128 0.034634456 0.047750704 0.056389421 0.060595453 0.063910879 0.069380105 0.07215777 0.069676854 0.062613823 0.057453685 0.05791501 0.059905551 0.06279739 0.070793457][0.022383431 0.040892906 0.056419969 0.065055363 0.066690236 0.0664001 0.06982369 0.073072389 0.072595626 0.066791572 0.061078768 0.059555359 0.058702715 0.059271365 0.066070393][0.02376776 0.044431426 0.062301256 0.071575835 0.071881063 0.069411874 0.071749076 0.07630945 0.078838833 0.075780436 0.070824832 0.067491956 0.062600791 0.058961857 0.062962137][0.023456471 0.044849757 0.064591326 0.075805917 0.077401623 0.075587831 0.078395709 0.084386148 0.089763463 0.0907681 0.088654593 0.084585749 0.075436786 0.066591918 0.066456608][0.023570348 0.04486718 0.066220984 0.080636919 0.086002477 0.08733014 0.0916262 0.098217964 0.10495049 0.109667 0.11124197 0.10723682 0.093790419 0.078495845 0.0726513][0.024002787 0.044966493 0.067569919 0.085530974 0.095870592 0.10170262 0.10795153 0.11373333 0.11907032 0.12490961 0.12894909 0.12522058 0.10886935 0.088663183 0.07818459][0.023658536 0.043209217 0.06514629 0.084475495 0.09809871 0.10738892 0.11483262 0.1187352 0.12092948 0.12565625 0.13107346 0.12960987 0.11554429 0.096511833 0.085499153][0.022918528 0.039986003 0.058960304 0.076257169 0.089336656 0.0987795 0.10557835 0.10762925 0.10787009 0.11246509 0.12038587 0.12376349 0.11659887 0.10404713 0.095732763][0.021417657 0.035706095 0.050799847 0.064057514 0.073494792 0.07987158 0.084228411 0.085185125 0.085998192 0.092869282 0.10463317 0.11347233 0.11361912 0.10812376 0.10285976][0.01926663 0.031928483 0.044350535 0.054183058 0.059513461 0.06147512 0.062203638 0.061800472 0.063635491 0.07266175 0.086951047 0.098935641 0.10316949 0.10173244 0.09823323][0.015645564 0.027798198 0.0395085 0.048107628 0.05131641 0.050279845 0.048205875 0.046534702 0.048447542 0.057220329 0.070403658 0.081407458 0.085465446 0.084601663 0.08167506][0.010238859 0.022097556 0.034139045 0.043231037 0.046619035 0.04510786 0.042088877 0.039685298 0.040633272 0.046680275 0.055368118 0.061875232 0.062995486 0.061024394 0.058511287]]...]
INFO - root - 2017-12-11 09:34:33.629919: step 47810, loss = 0.70, batch loss = 0.64 (14.5 examples/sec; 0.550 sec/batch; 43h:28m:56s remains)
INFO - root - 2017-12-11 09:34:39.071704: step 47820, loss = 0.70, batch loss = 0.64 (14.7 examples/sec; 0.543 sec/batch; 42h:55m:22s remains)
INFO - root - 2017-12-11 09:34:44.551152: step 47830, loss = 0.70, batch loss = 0.64 (14.5 examples/sec; 0.551 sec/batch; 43h:35m:25s remains)
INFO - root - 2017-12-11 09:34:49.550777: step 47840, loss = 0.70, batch loss = 0.65 (15.0 examples/sec; 0.532 sec/batch; 42h:03m:41s remains)
INFO - root - 2017-12-11 09:34:54.941279: step 47850, loss = 0.68, batch loss = 0.62 (14.9 examples/sec; 0.537 sec/batch; 42h:27m:36s remains)
INFO - root - 2017-12-11 09:35:00.344373: step 47860, loss = 0.68, batch loss = 0.63 (14.7 examples/sec; 0.544 sec/batch; 43h:00m:17s remains)
INFO - root - 2017-12-11 09:35:05.711945: step 47870, loss = 0.71, batch loss = 0.65 (15.0 examples/sec; 0.533 sec/batch; 42h:06m:28s remains)
INFO - root - 2017-12-11 09:35:11.122493: step 47880, loss = 0.70, batch loss = 0.65 (15.0 examples/sec; 0.533 sec/batch; 42h:07m:06s remains)
INFO - root - 2017-12-11 09:35:16.522565: step 47890, loss = 0.71, batch loss = 0.65 (14.7 examples/sec; 0.542 sec/batch; 42h:53m:10s remains)
INFO - root - 2017-12-11 09:35:21.891066: step 47900, loss = 0.70, batch loss = 0.65 (15.2 examples/sec; 0.527 sec/batch; 41h:41m:43s remains)
2017-12-11 09:35:22.516529: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.17967415 0.15573475 0.12372579 0.099510454 0.089655191 0.091469333 0.096614562 0.098922953 0.094583519 0.082894817 0.067599542 0.057344005 0.061414439 0.086506322 0.12865147][0.21165669 0.19210544 0.16298 0.14037509 0.12962723 0.12512591 0.11698397 0.099022873 0.071121313 0.040114459 0.019339744 0.022025663 0.052186176 0.10642483 0.1699632][0.22001465 0.2104675 0.19326314 0.1828462 0.18240184 0.1818227 0.16923979 0.13767157 0.089642651 0.039148241 0.010327196 0.022257017 0.07376641 0.15051819 0.22725944][0.20177644 0.20927534 0.21387063 0.22643539 0.24751604 0.26251084 0.25732478 0.22292325 0.16212244 0.093708284 0.050894 0.0592692 0.11464436 0.19585596 0.27222836][0.16359666 0.19696692 0.23535742 0.28281409 0.3356795 0.375934 0.3875187 0.35819906 0.28939271 0.20238812 0.13706034 0.12399345 0.15872663 0.22024462 0.28001985][0.12337627 0.18961464 0.27087551 0.36169875 0.4520444 0.52254653 0.55530912 0.53438884 0.45902151 0.35149461 0.25512159 0.20240271 0.19186512 0.20975 0.23736888][0.099224679 0.19683671 0.31871814 0.44959772 0.57259256 0.66924429 0.72074449 0.70852453 0.62963861 0.50577569 0.37908411 0.27896032 0.20932752 0.1694334 0.15526991][0.09594439 0.2148342 0.36308286 0.51763523 0.65667892 0.76391912 0.82191241 0.81262624 0.73351568 0.60400921 0.45923084 0.32144478 0.19968458 0.10729125 0.054616731][0.10288042 0.22686963 0.38053176 0.53664106 0.67086989 0.768902 0.81718379 0.80248851 0.72554308 0.60269266 0.45852163 0.30615309 0.15746333 0.03562291 -0.039643403][0.10304388 0.21412846 0.35166535 0.48820868 0.5996052 0.67298549 0.69968265 0.67360848 0.60059726 0.4944841 0.36929193 0.22971942 0.087195933 -0.032444246 -0.10737609][0.080554664 0.16368392 0.26843366 0.37083486 0.44971073 0.49339417 0.49740073 0.46195385 0.39729658 0.31524792 0.22176237 0.11580069 0.0057117543 -0.086435013 -0.14259237][0.032897878 0.080579169 0.14561577 0.20971166 0.25652769 0.27584243 0.26552668 0.22912087 0.17821793 0.12266253 0.064031251 -0.00089534762 -0.067863241 -0.12204352 -0.1519202][-0.023093458 -0.0074865385 0.022294372 0.054263763 0.07747101 0.083687536 0.070808731 0.042156223 0.0073690359 -0.025651906 -0.056353662 -0.087284796 -0.1170468 -0.13820218 -0.14549229][-0.06708812 -0.072826207 -0.066947177 -0.056380428 -0.047049228 -0.04455578 -0.051858533 -0.067964911 -0.087238774 -0.1040746 -0.11736655 -0.12785092 -0.13519211 -0.13686961 -0.13169129][-0.089898638 -0.10578708 -0.11232459 -0.1136402 -0.11161049 -0.10912099 -0.10947648 -0.1139842 -0.12084758 -0.1269484 -0.13104422 -0.13257866 -0.13102034 -0.12585482 -0.11755656]]...]
INFO - root - 2017-12-11 09:35:28.035058: step 47910, loss = 0.70, batch loss = 0.64 (14.6 examples/sec; 0.547 sec/batch; 43h:15m:30s remains)
INFO - root - 2017-12-11 09:35:33.417958: step 47920, loss = 0.70, batch loss = 0.64 (14.9 examples/sec; 0.538 sec/batch; 42h:33m:36s remains)
INFO - root - 2017-12-11 09:35:38.829125: step 47930, loss = 0.70, batch loss = 0.64 (14.5 examples/sec; 0.553 sec/batch; 43h:41m:01s remains)
INFO - root - 2017-12-11 09:35:43.861251: step 47940, loss = 0.72, batch loss = 0.66 (15.4 examples/sec; 0.521 sec/batch; 41h:08m:59s remains)
INFO - root - 2017-12-11 09:35:49.238550: step 47950, loss = 0.70, batch loss = 0.64 (14.7 examples/sec; 0.544 sec/batch; 43h:00m:27s remains)
INFO - root - 2017-12-11 09:35:54.661064: step 47960, loss = 0.70, batch loss = 0.64 (15.3 examples/sec; 0.523 sec/batch; 41h:18m:34s remains)
INFO - root - 2017-12-11 09:36:00.076577: step 47970, loss = 0.70, batch loss = 0.64 (15.0 examples/sec; 0.533 sec/batch; 42h:07m:08s remains)
INFO - root - 2017-12-11 09:36:05.549629: step 47980, loss = 0.71, batch loss = 0.66 (14.5 examples/sec; 0.550 sec/batch; 43h:28m:55s remains)
INFO - root - 2017-12-11 09:36:10.918823: step 47990, loss = 0.69, batch loss = 0.63 (15.0 examples/sec; 0.533 sec/batch; 42h:06m:19s remains)
INFO - root - 2017-12-11 09:36:16.347797: step 48000, loss = 0.71, batch loss = 0.65 (14.6 examples/sec; 0.548 sec/batch; 43h:16m:42s remains)
2017-12-11 09:36:16.950434: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.18746342 0.18043448 0.17091587 0.16058478 0.1501247 0.14632791 0.15590397 0.17578623 0.19703571 0.21373901 0.224806 0.23068824 0.22972752 0.22415794 0.21978353][0.19385321 0.18693592 0.17773375 0.16906603 0.1590388 0.15276822 0.16057892 0.18197349 0.20935267 0.23248455 0.24660242 0.25225389 0.25080886 0.24941905 0.25101617][0.19811842 0.19136916 0.1810818 0.17279853 0.16258094 0.1539149 0.15846848 0.1787122 0.20865959 0.23602168 0.25319928 0.25944167 0.25810263 0.26002875 0.26742443][0.21001507 0.20592296 0.19514018 0.185757 0.17413004 0.1643557 0.16667072 0.18267368 0.20858222 0.23430203 0.25244147 0.26009977 0.25913405 0.26146394 0.2705006][0.22542699 0.22484885 0.21357831 0.20165087 0.18727759 0.17626418 0.17668669 0.18644115 0.2030005 0.2207635 0.23551649 0.24259397 0.24061663 0.24058823 0.24777129][0.24231902 0.24764451 0.23747015 0.22356898 0.20628701 0.19323054 0.19052272 0.19328776 0.19857349 0.20429203 0.21027309 0.21186116 0.20548952 0.20083247 0.20387961][0.25660762 0.26923409 0.26279053 0.24978887 0.23115718 0.2140146 0.20489307 0.19971015 0.19554015 0.1908372 0.18691844 0.18051532 0.16773087 0.15752845 0.15568078][0.26119947 0.28131896 0.28124914 0.27281731 0.25572076 0.23450606 0.21705796 0.20352319 0.19218819 0.18058486 0.16914682 0.15546343 0.13631983 0.12062633 0.1146526][0.25511464 0.28242084 0.29047132 0.28897139 0.27539727 0.25111264 0.22577451 0.2034352 0.18474527 0.16790532 0.15255241 0.13597701 0.11505564 0.097779043 0.090776086][0.24506232 0.27819863 0.294334 0.29889771 0.2870644 0.25882727 0.22631422 0.19589774 0.16984986 0.14910023 0.1345557 0.12246984 0.10685061 0.092558526 0.086565837][0.23264848 0.27016854 0.29271153 0.30084792 0.28845865 0.25603053 0.21836187 0.18321155 0.15286341 0.13061836 0.11988437 0.1167241 0.11127318 0.1032935 0.099425972][0.22007944 0.26001075 0.28664079 0.29648647 0.28330934 0.2488409 0.20942222 0.17422828 0.14467134 0.12407538 0.11749718 0.12244962 0.12746233 0.12761724 0.12774768][0.21244684 0.25294235 0.28201097 0.29371533 0.28225479 0.25034577 0.21331401 0.18186592 0.15681629 0.13956699 0.13482037 0.1419737 0.15218821 0.15876187 0.16399641][0.21046466 0.25167117 0.28317246 0.29847646 0.29246444 0.26790112 0.23770364 0.21303101 0.19416326 0.1801808 0.17437598 0.17753899 0.18530108 0.19263183 0.20058897][0.209894 0.25133732 0.28364617 0.30144581 0.30141255 0.28641358 0.26695642 0.25299835 0.24273956 0.2331447 0.226499 0.22406812 0.22463459 0.2259381 0.23065154]]...]
INFO - root - 2017-12-11 09:36:22.423614: step 48010, loss = 0.70, batch loss = 0.64 (15.1 examples/sec; 0.529 sec/batch; 41h:46m:48s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-SGD-lr0.001-clip50-initfromfixqian30k-fixqian
INFO - root - 2017-12-11 09:36:27.905599: step 48020, loss = 0.69, batch loss = 0.63 (14.5 examples/sec; 0.552 sec/batch; 43h:38m:07s remains)
INFO - root - 2017-12-11 09:36:33.314371: step 48030, loss = 0.69, batch loss = 0.63 (14.8 examples/sec; 0.539 sec/batch; 42h:34m:51s remains)
INFO - root - 2017-12-11 09:36:38.291299: step 48040, loss = 0.67, batch loss = 0.61 (14.6 examples/sec; 0.549 sec/batch; 43h:22m:54s remains)
INFO - root - 2017-12-11 09:36:43.642612: step 48050, loss = 0.70, batch loss = 0.64 (15.3 examples/sec; 0.524 sec/batch; 41h:24m:17s remains)
INFO - root - 2017-12-11 09:36:49.045146: step 48060, loss = 0.69, batch loss = 0.63 (13.9 examples/sec; 0.574 sec/batch; 45h:21m:05s remains)
INFO - root - 2017-12-11 09:36:54.414463: step 48070, loss = 0.71, batch loss = 0.65 (14.8 examples/sec; 0.542 sec/batch; 42h:47m:06s remains)
INFO - root - 2017-12-11 09:36:59.790415: step 48080, loss = 0.70, batch loss = 0.64 (14.8 examples/sec; 0.540 sec/batch; 42h:39m:45s remains)
INFO - root - 2017-12-11 09:37:05.124659: step 48090, loss = 0.70, batch loss = 0.64 (14.6 examples/sec; 0.550 sec/batch; 43h:26m:08s remains)
INFO - root - 2017-12-11 09:37:10.537552: step 48100, loss = 0.70, batch loss = 0.64 (14.4 examples/sec; 0.555 sec/batch; 43h:50m:45s remains)
2017-12-11 09:37:11.161052: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.23353238 0.23426731 0.22138928 0.21457948 0.22625525 0.25658441 0.29038575 0.31102619 0.31185639 0.2916764 0.25898615 0.22880137 0.21074247 0.20514113 0.21063468][0.28093582 0.29579976 0.30168638 0.30978537 0.33124885 0.36562482 0.39393374 0.39528304 0.36803618 0.32000336 0.26499814 0.22059949 0.19601923 0.18959561 0.19651176][0.35087019 0.38084322 0.40590376 0.42485285 0.44722274 0.47378302 0.48672116 0.46196359 0.40173551 0.32476565 0.25166547 0.1991428 0.17020768 0.16022669 0.16431697][0.42774487 0.46311626 0.49891353 0.52084404 0.5360018 0.55017632 0.55174977 0.51394826 0.43715146 0.3479678 0.27168563 0.21906878 0.18377927 0.15998177 0.14942381][0.48076424 0.512157 0.55157417 0.57489055 0.58536744 0.59530872 0.60106742 0.57432556 0.50715083 0.42629996 0.35779238 0.304631 0.25290892 0.19957912 0.15901481][0.48913798 0.51982337 0.56622988 0.5992223 0.61608416 0.63431948 0.65688163 0.65577096 0.6123966 0.54716945 0.48508519 0.42270654 0.34443218 0.25361598 0.1781041][0.46321756 0.49765873 0.55345803 0.60119808 0.63186818 0.66432893 0.70475894 0.72683609 0.70337379 0.64768767 0.58490312 0.50830239 0.40541714 0.28531027 0.18507959][0.42245874 0.46459779 0.52859133 0.58796692 0.63120717 0.67363584 0.72097743 0.75057667 0.73297054 0.67698163 0.6082896 0.52147067 0.40865919 0.28081113 0.17654635][0.37980819 0.42802981 0.49475515 0.55872804 0.60833514 0.65224993 0.69250542 0.71286464 0.68836391 0.62665778 0.55323964 0.4672507 0.36474332 0.25422224 0.16772012][0.33009472 0.38152924 0.44679281 0.51038569 0.56091225 0.59910631 0.62229151 0.62281221 0.58592212 0.519707 0.44885513 0.37720075 0.30233425 0.22659554 0.17020592][0.29302961 0.34517103 0.4043144 0.46005031 0.50299305 0.5282594 0.53020668 0.51033461 0.46562696 0.40346438 0.34643331 0.30079579 0.26203561 0.22365801 0.19341084][0.28883582 0.33407098 0.37971193 0.41952446 0.44808039 0.45804605 0.44259554 0.41070178 0.36829665 0.31954134 0.28413552 0.26911634 0.26434404 0.25409886 0.2368281][0.31799525 0.35188824 0.37999314 0.40079907 0.41301027 0.40920138 0.38238 0.34644225 0.311173 0.27605122 0.25912446 0.2689178 0.28842995 0.29513219 0.28178823][0.34954986 0.37399366 0.38918146 0.39637288 0.39610291 0.38227713 0.348412 0.31155425 0.28161964 0.25482777 0.24897328 0.27523261 0.31030086 0.32826796 0.31956187][0.34873286 0.37157363 0.38662311 0.39445686 0.39374441 0.37842703 0.34190217 0.30179557 0.26744875 0.23661713 0.22990681 0.26262689 0.30653468 0.33525014 0.33834276]]...]
INFO - root - 2017-12-11 09:37:16.638946: step 48110, loss = 0.70, batch loss = 0.64 (14.7 examples/sec; 0.544 sec/batch; 42h:57m:57s remains)
INFO - root - 2017-12-11 09:37:22.090956: step 48120, loss = 0.68, batch loss = 0.62 (14.7 examples/sec; 0.545 sec/batch; 43h:02m:47s remains)
INFO - root - 2017-12-11 09:37:27.541295: step 48130, loss = 0.68, batch loss = 0.62 (14.7 examples/sec; 0.543 sec/batch; 42h:53m:20s remains)
INFO - root - 2017-12-11 09:37:32.755448: step 48140, loss = 0.69, batch loss = 0.63 (14.9 examples/sec; 0.536 sec/batch; 42h:19m:38s remains)
INFO - root - 2017-12-11 09:37:38.200399: step 48150, loss = 0.71, batch loss = 0.65 (14.7 examples/sec; 0.543 sec/batch; 42h:53m:49s remains)
INFO - root - 2017-12-11 09:37:43.529730: step 48160, loss = 0.70, batch loss = 0.64 (15.2 examples/sec; 0.526 sec/batch; 41h:31m:10s remains)
INFO - root - 2017-12-11 09:37:48.942181: step 48170, loss = 0.70, batch loss = 0.65 (14.8 examples/sec; 0.540 sec/batch; 42h:38m:21s remains)
INFO - root - 2017-12-11 09:37:54.316214: step 48180, loss = 0.71, batch loss = 0.65 (15.5 examples/sec; 0.516 sec/batch; 40h:45m:46s remains)
INFO - root - 2017-12-11 09:37:59.731065: step 48190, loss = 0.69, batch loss = 0.63 (14.6 examples/sec; 0.548 sec/batch; 43h:18m:43s remains)
INFO - root - 2017-12-11 09:38:05.101687: step 48200, loss = 0.70, batch loss = 0.64 (14.9 examples/sec; 0.538 sec/batch; 42h:27m:38s remains)
2017-12-11 09:38:05.757420: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0289992 -0.050764978 -0.060683474 -0.063459195 -0.062238842 -0.057249065 -0.049385402 -0.043475177 -0.042347368 -0.044291355 -0.048932206 -0.056825351 -0.064155154 -0.064764254 -0.055970687][-0.018510228 -0.041470639 -0.051511467 -0.055112965 -0.053937122 -0.045852646 -0.031393882 -0.018286446 -0.012621967 -0.01431657 -0.023542786 -0.03944857 -0.054768819 -0.060515665 -0.053010743][-0.011942635 -0.03231987 -0.0405778 -0.043661993 -0.040543534 -0.026238976 -0.00068739988 0.024959052 0.04000378 0.041101485 0.026996827 0.00035522558 -0.028227849 -0.045961384 -0.04669597][-0.011080962 -0.027033644 -0.032884534 -0.033601627 -0.024667805 0.00136271 0.044664368 0.089860052 0.12025639 0.12712702 0.10748882 0.066220984 0.018055813 -0.019475404 -0.036209919][-0.011803273 -0.024036665 -0.028099092 -0.024772277 -0.0060074735 0.037085854 0.10397941 0.17448148 0.22462554 0.238636 0.21126142 0.15080976 0.077591844 0.015208722 -0.022068471][-0.0095166517 -0.021610361 -0.025965821 -0.017645769 0.013894183 0.077252083 0.17018238 0.26727796 0.33673212 0.35551798 0.31666973 0.23361942 0.13344941 0.046266686 -0.010610651][0.0035452882 -0.015257088 -0.024904016 -0.013136563 0.031482212 0.11509502 0.2321942 0.35161531 0.43396875 0.45045578 0.39533851 0.2894007 0.16671939 0.061746687 -0.0077755949][0.030966721 -0.0017883683 -0.022785448 -0.011449777 0.043124974 0.14290048 0.27740893 0.40923008 0.49328974 0.49875668 0.42465147 0.30082178 0.16651042 0.056426626 -0.014862893][0.063144773 0.012753907 -0.022136079 -0.014171678 0.045423545 0.15344428 0.29346544 0.423521 0.49689308 0.48549077 0.39543539 0.26423886 0.13291083 0.032109454 -0.029602662][0.085280053 0.019523386 -0.026133416 -0.021820594 0.037567098 0.14352827 0.274684 0.38828486 0.44140327 0.41273373 0.31543183 0.19037504 0.0760473 -0.0041513084 -0.048627157][0.087268174 0.014397535 -0.034805335 -0.032653924 0.021700246 0.1154984 0.2251009 0.31172249 0.34106773 0.30048269 0.20800519 0.10255246 0.015932286 -0.037282869 -0.061894085][0.07101012 0.00066542818 -0.045146942 -0.044451617 0.00041401674 0.074973986 0.15661222 0.21401665 0.22346827 0.18010551 0.10353787 0.026721744 -0.026979603 -0.051914528 -0.05836105][0.045867227 -0.014596808 -0.053099528 -0.0545898 -0.02249141 0.030423325 0.085247517 0.11899132 0.11736103 0.081008613 0.027874358 -0.017000332 -0.038710229 -0.039028779 -0.032503005][0.025114588 -0.023483766 -0.054958131 -0.059910975 -0.040943328 -0.0068943715 0.027702753 0.046573263 0.042390306 0.019309431 -0.0080633676 -0.022939563 -0.017966941 -0.00038605311 0.012424996][0.021418089 -0.018354356 -0.046300676 -0.055518903 -0.046327088 -0.023811033 -9.3948845e-05 0.011829007 0.0091891363 -0.0011443952 -0.00742643 0.0003805485 0.023795648 0.051802561 0.065568164]]...]
INFO - root - 2017-12-11 09:38:11.166779: step 48210, loss = 0.69, batch loss = 0.63 (14.6 examples/sec; 0.548 sec/batch; 43h:15m:45s remains)
INFO - root - 2017-12-11 09:38:16.599533: step 48220, loss = 0.70, batch loss = 0.64 (14.6 examples/sec; 0.547 sec/batch; 43h:12m:24s remains)
INFO - root - 2017-12-11 09:38:22.055084: step 48230, loss = 0.68, batch loss = 0.62 (15.4 examples/sec; 0.519 sec/batch; 40h:57m:33s remains)
INFO - root - 2017-12-11 09:38:27.237372: step 48240, loss = 0.69, batch loss = 0.63 (14.4 examples/sec; 0.556 sec/batch; 43h:56m:24s remains)
INFO - root - 2017-12-11 09:38:32.673706: step 48250, loss = 0.71, batch loss = 0.65 (14.9 examples/sec; 0.536 sec/batch; 42h:19m:47s remains)
INFO - root - 2017-12-11 09:38:38.143090: step 48260, loss = 0.68, batch loss = 0.62 (14.8 examples/sec; 0.542 sec/batch; 42h:46m:25s remains)
INFO - root - 2017-12-11 09:38:43.625485: step 48270, loss = 0.69, batch loss = 0.63 (14.0 examples/sec; 0.570 sec/batch; 45h:01m:58s remains)
INFO - root - 2017-12-11 09:38:49.032109: step 48280, loss = 0.70, batch loss = 0.65 (14.7 examples/sec; 0.544 sec/batch; 42h:56m:34s remains)
INFO - root - 2017-12-11 09:38:54.420405: step 48290, loss = 0.70, batch loss = 0.64 (14.8 examples/sec; 0.539 sec/batch; 42h:33m:15s remains)
INFO - root - 2017-12-11 09:38:59.754131: step 48300, loss = 0.68, batch loss = 0.63 (15.1 examples/sec; 0.531 sec/batch; 41h:55m:25s remains)
2017-12-11 09:39:00.413285: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0028429318 0.010143731 0.023199338 0.028968738 0.02868562 0.02646389 0.024691362 0.022026572 0.020921635 0.02678122 0.038179155 0.050597783 0.060679242 0.065937467 0.063920461][0.019137723 0.039672244 0.059215061 0.067595653 0.066906266 0.062894933 0.059036624 0.054762386 0.054394655 0.065375164 0.083995439 0.10264622 0.11710595 0.12487733 0.12255324][0.047751259 0.078323916 0.1066645 0.11939881 0.11922144 0.11283045 0.10516523 0.097253621 0.096054621 0.11087703 0.13568851 0.15953968 0.17735744 0.18664049 0.18423887][0.074833028 0.11716231 0.15645063 0.17607081 0.17913604 0.17262122 0.16185868 0.14914389 0.14442277 0.15990953 0.18849576 0.21604505 0.2358958 0.24482939 0.24047597][0.09307383 0.14594315 0.19554079 0.22298437 0.23201393 0.22932014 0.21970302 0.20520377 0.19744384 0.21233362 0.24382298 0.27550861 0.29680079 0.30268878 0.29219276][0.098593786 0.1581005 0.21355459 0.24607594 0.26057935 0.26345479 0.25925514 0.24764626 0.23982184 0.25457147 0.2887395 0.32541531 0.3480933 0.34947819 0.33076468][0.095832922 0.15892552 0.21639921 0.25074911 0.26791745 0.27497455 0.27618384 0.26811871 0.26045132 0.27221254 0.30442744 0.34278694 0.36729017 0.36772841 0.34644461][0.093353309 0.15952677 0.21802366 0.25289634 0.27090725 0.28087252 0.28720492 0.28299072 0.27508718 0.27924886 0.30025572 0.33020589 0.35136166 0.35221022 0.33421919][0.096527472 0.16670975 0.22711103 0.26260954 0.28114638 0.29474226 0.30829015 0.31088391 0.3049064 0.3002229 0.30262843 0.3123883 0.32007492 0.31668839 0.30132458][0.10149433 0.17460006 0.23615938 0.2714659 0.29035127 0.30806983 0.33003542 0.34211355 0.34150019 0.33170748 0.31788123 0.30583632 0.29419032 0.27946469 0.25985861][0.10070399 0.17170189 0.22989699 0.26219487 0.28032702 0.30025354 0.32676181 0.34496272 0.34900591 0.33821017 0.31646794 0.29031113 0.26216608 0.23460484 0.20728034][0.088060446 0.14932303 0.19762312 0.2231455 0.23820622 0.25534311 0.27720183 0.29152122 0.29335415 0.28137276 0.25857094 0.22944339 0.19647828 0.1652537 0.13671742][0.060913477 0.10504983 0.13838589 0.15506607 0.1658242 0.17718525 0.18972811 0.19504513 0.19085081 0.17671791 0.1550893 0.12874903 0.099459879 0.073262773 0.051445551][0.023582757 0.047708053 0.064753741 0.072077595 0.077165879 0.081762411 0.085477881 0.083186604 0.074552663 0.05956839 0.040531244 0.019550448 -0.0018880826 -0.018893892 -0.03068364][-0.016004326 -0.00845526 -0.0042129871 -0.0045100329 -0.005144977 -0.0063157743 -0.0079749729 -0.01291808 -0.021092122 -0.032818131 -0.046542626 -0.060600452 -0.07375735 -0.082886644 -0.087768175]]...]
INFO - root - 2017-12-11 09:39:05.844554: step 48310, loss = 0.70, batch loss = 0.64 (14.8 examples/sec; 0.542 sec/batch; 42h:45m:54s remains)
INFO - root - 2017-12-11 09:39:11.322985: step 48320, loss = 0.70, batch loss = 0.64 (15.1 examples/sec; 0.529 sec/batch; 41h:44m:59s remains)
INFO - root - 2017-12-11 09:39:16.439271: step 48330, loss = 0.69, batch loss = 0.63 (29.5 examples/sec; 0.271 sec/batch; 21h:24m:48s remains)
INFO - root - 2017-12-11 09:39:21.892518: step 48340, loss = 0.70, batch loss = 0.64 (14.2 examples/sec; 0.562 sec/batch; 44h:19m:54s remains)
INFO - root - 2017-12-11 09:39:27.314773: step 48350, loss = 0.70, batch loss = 0.65 (15.2 examples/sec; 0.527 sec/batch; 41h:37m:39s remains)
INFO - root - 2017-12-11 09:39:32.774017: step 48360, loss = 0.70, batch loss = 0.64 (14.7 examples/sec; 0.545 sec/batch; 43h:02m:48s remains)
INFO - root - 2017-12-11 09:39:38.337152: step 48370, loss = 0.69, batch loss = 0.63 (14.5 examples/sec; 0.551 sec/batch; 43h:30m:49s remains)
INFO - root - 2017-12-11 09:39:43.817113: step 48380, loss = 0.69, batch loss = 0.63 (14.5 examples/sec; 0.550 sec/batch; 43h:23m:47s remains)
INFO - root - 2017-12-11 09:39:49.227260: step 48390, loss = 0.69, batch loss = 0.63 (14.9 examples/sec; 0.536 sec/batch; 42h:19m:18s remains)
INFO - root - 2017-12-11 09:39:54.664587: step 48400, loss = 0.69, batch loss = 0.63 (14.8 examples/sec; 0.542 sec/batch; 42h:47m:32s remains)
2017-12-11 09:39:55.314813: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.030101182 -0.021714509 -0.0053513949 0.01864546 0.050707083 0.092854075 0.13715836 0.17439251 0.19560878 0.20244522 0.19781546 0.1897727 0.18101548 0.17161515 0.1672354][-0.030782133 -0.021839464 -0.0037980655 0.023254765 0.05905119 0.10428257 0.14971513 0.18439594 0.19939023 0.19809251 0.1868927 0.17629974 0.16857836 0.16066182 0.15508839][-0.030330095 -0.021093285 -0.001973572 0.026991067 0.064879887 0.11124668 0.15601526 0.18689616 0.19497597 0.184903 0.16642551 0.15308774 0.14696616 0.1420387 0.1369123][-0.029862283 -0.020787505 -0.0014411736 0.027928811 0.06595961 0.11194997 0.15617533 0.18579319 0.19142912 0.1772536 0.15496305 0.13983279 0.13396133 0.12951593 0.12257321][-0.029626623 -0.021097761 -0.0024131357 0.025652587 0.061726697 0.1054313 0.14829065 0.17760414 0.18370429 0.17002754 0.14838007 0.13414188 0.12895639 0.1238987 0.11367191][-0.029761877 -0.021936856 -0.0042897454 0.022159884 0.056053042 0.097230248 0.13843442 0.16730262 0.17403257 0.16149136 0.14142936 0.1292098 0.12549423 0.12058175 0.10853215][-0.030087221 -0.022878794 -0.0059903855 0.019479549 0.052307114 0.092288 0.13261573 0.16118965 0.16803138 0.15575153 0.13574243 0.12346415 0.11965577 0.11488803 0.10287382][-0.030561082 -0.023867544 -0.0073843617 0.017674351 0.050345588 0.090074882 0.13006763 0.15834852 0.16523546 0.15306798 0.13242128 0.11843628 0.11246847 0.10680602 0.095067494][-0.031128561 -0.024852421 -0.0086522773 0.015999135 0.048466776 0.08779081 0.12712999 0.15482068 0.1619333 0.15061237 0.13002194 0.11417145 0.1050809 0.097327448 0.085124888][-0.03161037 -0.025463067 -0.0095896441 0.014572795 0.046701167 0.085606039 0.12430417 0.15121983 0.15793757 0.14634626 0.12462566 0.10601227 0.093509838 0.083995454 0.072025575][-0.031752307 -0.025309082 -0.0089989761 0.016260864 0.05056579 0.092268206 0.13334912 0.16125654 0.16693261 0.15186711 0.12461208 0.099540159 0.081950508 0.070712343 0.060067855][-0.031508323 -0.024493134 -0.006947062 0.0207683 0.059266333 0.10595024 0.15109235 0.18086076 0.18553683 0.16596927 0.1311879 0.097534455 0.073545523 0.060026594 0.050690204][-0.031442694 -0.024294443 -0.0064176335 0.022292417 0.063189283 0.11292234 0.16076505 0.19272085 0.19905476 0.17909764 0.14093833 0.10198108 0.073613115 0.058464039 0.050166138][-0.031729959 -0.02500766 -0.0081601031 0.019239718 0.059281029 0.10842591 0.15610564 0.18946537 0.19953413 0.18355462 0.14707883 0.1068952 0.076624453 0.060926724 0.053966526][-0.032440167 -0.026583485 -0.011742036 0.012626538 0.048975877 0.0940152 0.13795969 0.16982131 0.18219152 0.17087707 0.13847557 0.099976994 0.070288263 0.055680543 0.050855808]]...]
INFO - root - 2017-12-11 09:40:00.770027: step 48410, loss = 0.71, batch loss = 0.65 (14.8 examples/sec; 0.539 sec/batch; 42h:30m:51s remains)
INFO - root - 2017-12-11 09:40:06.156731: step 48420, loss = 0.69, batch loss = 0.63 (14.4 examples/sec; 0.556 sec/batch; 43h:51m:49s remains)
INFO - root - 2017-12-11 09:40:11.204431: step 48430, loss = 0.68, batch loss = 0.63 (17.6 examples/sec; 0.455 sec/batch; 35h:54m:07s remains)
INFO - root - 2017-12-11 09:40:16.658806: step 48440, loss = 0.68, batch loss = 0.62 (14.6 examples/sec; 0.547 sec/batch; 43h:08m:45s remains)
INFO - root - 2017-12-11 09:40:22.172859: step 48450, loss = 0.69, batch loss = 0.63 (14.4 examples/sec; 0.557 sec/batch; 43h:57m:48s remains)
INFO - root - 2017-12-11 09:40:27.655840: step 48460, loss = 0.70, batch loss = 0.64 (14.4 examples/sec; 0.557 sec/batch; 43h:55m:51s remains)
INFO - root - 2017-12-11 09:40:33.073741: step 48470, loss = 0.70, batch loss = 0.64 (14.8 examples/sec; 0.540 sec/batch; 42h:36m:03s remains)
INFO - root - 2017-12-11 09:40:38.513810: step 48480, loss = 0.69, batch loss = 0.63 (14.8 examples/sec; 0.540 sec/batch; 42h:38m:04s remains)
INFO - root - 2017-12-11 09:40:43.985427: step 48490, loss = 0.68, batch loss = 0.62 (14.5 examples/sec; 0.553 sec/batch; 43h:39m:45s remains)
INFO - root - 2017-12-11 09:40:49.387528: step 48500, loss = 0.70, batch loss = 0.64 (15.1 examples/sec; 0.528 sec/batch; 41h:40m:22s remains)
2017-12-11 09:40:49.998431: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0097244838 0.052710067 0.1371133 0.22574787 0.28844959 0.30752021 0.27711079 0.21696696 0.14497642 0.081379116 0.048669793 0.0431845 0.049687251 0.051866565 0.044260882][-0.0035051005 0.073302887 0.17706856 0.28207418 0.35393852 0.37370431 0.33683252 0.26212409 0.16836132 0.081449144 0.027620286 0.006002083 0.0023620683 -0.00016933442 -0.0084630512][-0.01445459 0.062672339 0.16886024 0.27629927 0.3535074 0.38140595 0.35444278 0.28577667 0.18969065 0.0928172 0.022569444 -0.015061891 -0.030287646 -0.038496081 -0.046432916][-0.032762446 0.035467561 0.13355516 0.23780143 0.32446271 0.37385935 0.37553415 0.33259255 0.25263372 0.16001362 0.08047501 0.02777941 -0.00076959992 -0.017097557 -0.026696816][-0.048341777 0.009802701 0.097706839 0.19976032 0.30077749 0.3810423 0.42298648 0.41722253 0.36576456 0.28974617 0.21052952 0.14747915 0.10578582 0.078363091 0.06126003][-0.058215991 -0.0089725805 0.068585448 0.168564 0.28424004 0.39956605 0.49054363 0.53350878 0.52372777 0.476329 0.40836996 0.34214097 0.28994063 0.2496606 0.2190057][-0.05987215 -0.013807069 0.059764344 0.16134742 0.28952712 0.43430164 0.56993341 0.66229862 0.69791979 0.68505621 0.63683933 0.57545853 0.5174154 0.46564296 0.41921854][-0.053842928 -0.0053255311 0.072098516 0.18090825 0.31944716 0.4797143 0.63823354 0.75872463 0.82537287 0.84320182 0.82205379 0.77912617 0.72911304 0.67612678 0.61967605][-0.047883824 0.0011900635 0.081012346 0.19455762 0.3362914 0.49577427 0.65317458 0.77679312 0.85618305 0.89725381 0.90743077 0.89391834 0.86446267 0.82154638 0.76475751][-0.048524387 -0.0052057956 0.067646094 0.17364004 0.30500311 0.44884303 0.58856446 0.69994813 0.77985328 0.83578545 0.87250429 0.88719445 0.87990534 0.85239065 0.80484694][-0.056548923 -0.024626313 0.031978242 0.11660822 0.22209205 0.33630812 0.44591609 0.53486061 0.60447514 0.6615243 0.70892352 0.7378332 0.74411821 0.73156172 0.70117384][-0.070813768 -0.053435586 -0.017998246 0.037121713 0.10691808 0.18314128 0.2554951 0.31401697 0.36073491 0.40050679 0.43785194 0.46522903 0.47757021 0.47915339 0.46991915][-0.087363563 -0.084077567 -0.0692394 -0.043248713 -0.0086515434 0.030834381 0.068440266 0.09824387 0.11995579 0.13594133 0.15322958 0.17032658 0.18376477 0.19567896 0.20207407][-0.10049616 -0.10762192 -0.10853748 -0.10464985 -0.09639167 -0.08418031 -0.071392238 -0.061143808 -0.055858277 -0.05565406 -0.053796448 -0.047510721 -0.037896786 -0.024850437 -0.014367333][-0.10516182 -0.11687322 -0.12519646 -0.13192163 -0.13659436 -0.13871802 -0.13967071 -0.14067458 -0.14430195 -0.15130256 -0.15714875 -0.1584636 -0.1549515 -0.14705552 -0.13917145]]...]
INFO - root - 2017-12-11 09:40:55.532678: step 48510, loss = 0.70, batch loss = 0.64 (14.7 examples/sec; 0.545 sec/batch; 42h:57m:54s remains)
INFO - root - 2017-12-11 09:41:00.991504: step 48520, loss = 0.71, batch loss = 0.65 (14.8 examples/sec; 0.539 sec/batch; 42h:32m:49s remains)
INFO - root - 2017-12-11 09:41:06.111055: step 48530, loss = 0.71, batch loss = 0.65 (14.9 examples/sec; 0.538 sec/batch; 42h:26m:44s remains)
INFO - root - 2017-12-11 09:41:11.541053: step 48540, loss = 0.69, batch loss = 0.63 (14.1 examples/sec; 0.568 sec/batch; 44h:46m:18s remains)
INFO - root - 2017-12-11 09:41:16.916868: step 48550, loss = 0.70, batch loss = 0.64 (14.9 examples/sec; 0.537 sec/batch; 42h:20m:05s remains)
INFO - root - 2017-12-11 09:41:22.211497: step 48560, loss = 0.69, batch loss = 0.63 (15.3 examples/sec; 0.524 sec/batch; 41h:19m:20s remains)
INFO - root - 2017-12-11 09:41:27.633368: step 48570, loss = 0.72, batch loss = 0.66 (14.9 examples/sec; 0.538 sec/batch; 42h:23m:47s remains)
INFO - root - 2017-12-11 09:41:33.147406: step 48580, loss = 0.69, batch loss = 0.63 (14.2 examples/sec; 0.565 sec/batch; 44h:34m:39s remains)
INFO - root - 2017-12-11 09:41:38.551223: step 48590, loss = 0.69, batch loss = 0.63 (14.5 examples/sec; 0.551 sec/batch; 43h:28m:43s remains)
INFO - root - 2017-12-11 09:41:43.949332: step 48600, loss = 0.70, batch loss = 0.64 (14.8 examples/sec; 0.541 sec/batch; 42h:37m:46s remains)
2017-12-11 09:41:44.508521: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.069499113 -0.069621213 -0.066716328 -0.063177042 -0.059766594 -0.056829903 -0.051504847 -0.044239018 -0.035923008 -0.02474227 -0.019585282 -0.021754723 -0.031743646 -0.047430974 -0.061867215][-0.072427221 -0.073237307 -0.071162984 -0.068477936 -0.064932 -0.060158215 -0.051543742 -0.039148703 -0.025917765 -0.011360946 -0.0046565174 -0.0075254682 -0.020747574 -0.041927516 -0.060821954][-0.070060804 -0.068950385 -0.064892516 -0.060298946 -0.053918771 -0.044040825 -0.027858771 -0.0048926361 0.017684894 0.037505083 0.044299386 0.036517158 0.013607158 -0.020595888 -0.050493732][-0.053237658 -0.045871891 -0.036273997 -0.027570151 -0.017062651 -0.00057144952 0.025464788 0.061954342 0.095838688 0.12042914 0.12406512 0.10559434 0.065789066 0.011726233 -0.03449361][-0.016398555 0.0004962082 0.017207978 0.029833311 0.0435403 0.066061161 0.10129811 0.15015921 0.19346946 0.22017984 0.21720541 0.18369812 0.12306349 0.046817191 -0.016884703][0.037996437 0.066082284 0.089252084 0.10319319 0.11622489 0.13999595 0.1788224 0.23392729 0.28109995 0.30588552 0.2942591 0.24584034 0.16690485 0.072882131 -0.0041156616][0.096154846 0.13285664 0.15803306 0.1674763 0.17318407 0.19028376 0.22388391 0.27581611 0.319874 0.33996964 0.32138056 0.26450306 0.17746276 0.077814884 -0.0023968737][0.13176575 0.1695862 0.18967968 0.18846016 0.18115096 0.18520659 0.20647275 0.24743213 0.2832104 0.29794645 0.2771329 0.22185624 0.14065813 0.050900828 -0.019413449][0.12275788 0.15287739 0.16235872 0.14888707 0.12919703 0.12046853 0.12870127 0.15656669 0.18316641 0.19486104 0.17812443 0.13390711 0.06997063 0.0015759126 -0.049517006][0.07895764 0.096845992 0.095097616 0.073358707 0.047363855 0.030782457 0.028991297 0.044688117 0.062827043 0.07340543 0.064425737 0.036053132 -0.0056785671 -0.049028575 -0.078911774][0.033153355 0.03993978 0.030378839 0.0057866815 -0.021471582 -0.042701233 -0.052533556 -0.047941171 -0.037685204 -0.028180568 -0.029681493 -0.042962439 -0.06423565 -0.085514605 -0.097823538][0.0064689755 0.0061938632 -0.0064685787 -0.029462384 -0.054884516 -0.078497566 -0.094291918 -0.098746225 -0.0953387 -0.087720208 -0.084553793 -0.087620728 -0.095058523 -0.10197254 -0.10376611][0.024372038 0.021530436 0.0077181095 -0.014270185 -0.04052195 -0.068957768 -0.091881871 -0.10479417 -0.1079395 -0.10330126 -0.098755389 -0.097295523 -0.09840975 -0.099406712 -0.097972527][0.091876969 0.088579774 0.0711868 0.044023737 0.0096221147 -0.02912637 -0.062017888 -0.083375029 -0.092255816 -0.090320513 -0.085967042 -0.083341576 -0.083060794 -0.083777271 -0.083954051][0.18247321 0.1762391 0.14951929 0.10947275 0.060693506 0.0089542335 -0.033698428 -0.061202981 -0.073175453 -0.072182715 -0.067645781 -0.064436615 -0.064240441 -0.066438474 -0.069403596]]...]
INFO - root - 2017-12-11 09:41:50.028241: step 48610, loss = 0.70, batch loss = 0.64 (14.4 examples/sec; 0.556 sec/batch; 43h:50m:07s remains)
INFO - root - 2017-12-11 09:41:55.444661: step 48620, loss = 0.70, batch loss = 0.64 (14.3 examples/sec; 0.561 sec/batch; 44h:12m:16s remains)
INFO - root - 2017-12-11 09:42:00.531750: step 48630, loss = 0.71, batch loss = 0.65 (14.4 examples/sec; 0.556 sec/batch; 43h:48m:47s remains)
INFO - root - 2017-12-11 09:42:06.088317: step 48640, loss = 0.72, batch loss = 0.66 (14.7 examples/sec; 0.546 sec/batch; 43h:02m:01s remains)
INFO - root - 2017-12-11 09:42:11.590871: step 48650, loss = 0.71, batch loss = 0.65 (14.8 examples/sec; 0.540 sec/batch; 42h:32m:57s remains)
INFO - root - 2017-12-11 09:42:17.080467: step 48660, loss = 0.70, batch loss = 0.64 (14.9 examples/sec; 0.536 sec/batch; 42h:17m:24s remains)
INFO - root - 2017-12-11 09:42:22.471441: step 48670, loss = 0.70, batch loss = 0.64 (14.5 examples/sec; 0.553 sec/batch; 43h:35m:34s remains)
INFO - root - 2017-12-11 09:42:27.893333: step 48680, loss = 0.70, batch loss = 0.64 (14.8 examples/sec; 0.539 sec/batch; 42h:30m:44s remains)
INFO - root - 2017-12-11 09:42:33.359219: step 48690, loss = 0.67, batch loss = 0.61 (14.1 examples/sec; 0.567 sec/batch; 44h:39m:44s remains)
INFO - root - 2017-12-11 09:42:38.747059: step 48700, loss = 0.69, batch loss = 0.63 (15.0 examples/sec; 0.535 sec/batch; 42h:09m:00s remains)
2017-12-11 09:42:39.393624: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.024916451 -0.012719163 0.0066888584 0.038947053 0.085471787 0.14173466 0.19707304 0.23860867 0.25466993 0.23874915 0.19608338 0.13946559 0.087100953 0.047697961 0.016993323][-0.019622384 -0.0020242464 0.025054403 0.068358645 0.12782313 0.19711542 0.26309118 0.31005257 0.32416952 0.29626164 0.23203813 0.15006153 0.076221287 0.024480611 -0.0090736812][-0.012875809 0.00936612 0.043274235 0.096312024 0.16752534 0.24810228 0.32167041 0.36954102 0.37679738 0.33365417 0.24859679 0.14659278 0.059011508 0.0025631334 -0.027548123][-0.006892174 0.018315358 0.05756982 0.11908346 0.20149253 0.29270959 0.37225446 0.41861129 0.41656527 0.35803548 0.25576711 0.14027162 0.045886461 -0.010199319 -0.034504257][-0.0024272995 0.024017388 0.0668702 0.13423908 0.22407712 0.32081348 0.40108752 0.44222787 0.43074226 0.36175478 0.25171483 0.13349052 0.040665694 -0.010986504 -0.02906022][0.00055017858 0.027470747 0.072998174 0.14430039 0.23736598 0.3330195 0.40714112 0.43804061 0.41591996 0.34044424 0.23089088 0.11937521 0.03525874 -0.008643372 -0.020006189][0.0010845413 0.028081696 0.075872354 0.15007213 0.2434077 0.33415526 0.39863789 0.41720718 0.38357186 0.30301169 0.19773756 0.097282 0.024883972 -0.0096853413 -0.013415574][-0.0020873414 0.023823461 0.072893053 0.14882211 0.24119939 0.32751662 0.38514525 0.395762 0.35505545 0.27246833 0.17275412 0.082036078 0.01853724 -0.0094563523 -0.0073539508][-0.0077732396 0.016840741 0.065917507 0.14143313 0.23106296 0.31298232 0.36606371 0.3737407 0.33212531 0.25252575 0.16002512 0.077266529 0.019421345 -0.0050176089 0.00048889924][-0.013255075 0.010914194 0.059316952 0.13240087 0.21777913 0.29486781 0.34475243 0.35333037 0.31640792 0.24370442 0.15831995 0.080537535 0.025179479 0.0022225266 0.0094606634][-0.018697899 0.0060953465 0.054169476 0.12515755 0.20764017 0.28245822 0.33272386 0.3462978 0.31740388 0.25192192 0.16947535 0.09046834 0.032825846 0.0091595082 0.017150922][-0.024328867 0.00099395378 0.048844494 0.11892985 0.20111357 0.27808082 0.33381262 0.35596052 0.33586428 0.27481484 0.18961293 0.1037854 0.040929135 0.016463662 0.026057694][-0.028706789 -0.0027224428 0.045664065 0.11715037 0.20328552 0.28801823 0.35411498 0.38582632 0.37113908 0.30780825 0.21278685 0.11614451 0.047445878 0.023239885 0.035459232][-0.029086884 -0.0012146912 0.049726222 0.12584944 0.22011621 0.31570992 0.39115688 0.42541239 0.40477854 0.32796273 0.21730739 0.11115023 0.042128809 0.02383117 0.043030486][-0.026366258 0.0031818505 0.056980312 0.1389325 0.24271165 0.34814551 0.4274343 0.4546155 0.41711739 0.31954494 0.19349699 0.084282719 0.023315087 0.017466431 0.048250947]]...]
INFO - root - 2017-12-11 09:42:44.900021: step 48710, loss = 0.69, batch loss = 0.63 (15.0 examples/sec; 0.534 sec/batch; 42h:05m:45s remains)
INFO - root - 2017-12-11 09:42:50.339087: step 48720, loss = 0.71, batch loss = 0.65 (14.9 examples/sec; 0.537 sec/batch; 42h:22m:00s remains)
INFO - root - 2017-12-11 09:42:55.468465: step 48730, loss = 0.69, batch loss = 0.63 (14.6 examples/sec; 0.548 sec/batch; 43h:11m:37s remains)
INFO - root - 2017-12-11 09:43:00.951695: step 48740, loss = 0.69, batch loss = 0.63 (14.7 examples/sec; 0.545 sec/batch; 42h:59m:04s remains)
INFO - root - 2017-12-11 09:43:06.376920: step 48750, loss = 0.70, batch loss = 0.64 (14.9 examples/sec; 0.536 sec/batch; 42h:16m:27s remains)
INFO - root - 2017-12-11 09:43:11.831986: step 48760, loss = 0.71, batch loss = 0.65 (14.8 examples/sec; 0.541 sec/batch; 42h:37m:49s remains)
INFO - root - 2017-12-11 09:43:17.204572: step 48770, loss = 0.71, batch loss = 0.65 (15.1 examples/sec; 0.531 sec/batch; 41h:51m:05s remains)
INFO - root - 2017-12-11 09:43:22.573376: step 48780, loss = 0.69, batch loss = 0.63 (15.0 examples/sec; 0.534 sec/batch; 42h:03m:34s remains)
INFO - root - 2017-12-11 09:43:27.987446: step 48790, loss = 0.71, batch loss = 0.65 (14.6 examples/sec; 0.547 sec/batch; 43h:04m:22s remains)
INFO - root - 2017-12-11 09:43:33.412075: step 48800, loss = 0.69, batch loss = 0.64 (14.5 examples/sec; 0.552 sec/batch; 43h:28m:03s remains)
2017-12-11 09:43:34.041251: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.23093481 0.25978449 0.27982467 0.28370622 0.26885515 0.2341172 0.17483371 0.097219206 0.022241246 -0.030313611 -0.057055648 -0.064522736 -0.059174225 -0.047858004 -0.037754968][0.24112134 0.25080502 0.24756585 0.23024826 0.2017604 0.16411403 0.11322219 0.050524879 -0.010284035 -0.053460054 -0.075037815 -0.079176828 -0.071057178 -0.057079863 -0.04364378][0.25065771 0.2404106 0.21603309 0.18284111 0.14769319 0.11374468 0.076350451 0.031066526 -0.016800264 -0.05544921 -0.078759931 -0.086036853 -0.0804228 -0.067527138 -0.053155132][0.25630662 0.23282692 0.19820051 0.16303895 0.13561147 0.11648533 0.097712748 0.069101796 0.029364705 -0.012112744 -0.046413876 -0.067104958 -0.073098756 -0.068541452 -0.058405764][0.24893901 0.2207507 0.18843256 0.1661779 0.16058937 0.16696395 0.17178798 0.15950812 0.12412821 0.075015143 0.023428345 -0.018507931 -0.0442805 -0.05459087 -0.054101825][0.22457387 0.19864863 0.17675725 0.17501396 0.19708699 0.23287329 0.26304844 0.26745251 0.2367803 0.181004 0.11306474 0.048188385 -0.0016813508 -0.031742785 -0.044688351][0.1872362 0.16713542 0.15725666 0.17467695 0.22092573 0.28200498 0.33473927 0.355742 0.33316702 0.27660084 0.19904181 0.11671677 0.04548993 -0.0038478128 -0.030763116][0.14773114 0.13419063 0.13349247 0.16289984 0.22281983 0.2974461 0.36290354 0.39578292 0.382777 0.33157653 0.254016 0.1657802 0.083568878 0.022205282 -0.014957032][0.11665966 0.11116703 0.11734444 0.15165241 0.21342692 0.28612593 0.34858465 0.38176802 0.37329543 0.32839277 0.25748238 0.17541859 0.09681467 0.03612848 -0.0023505595][0.10356984 0.10766065 0.12045217 0.15566325 0.21024439 0.26763189 0.31159553 0.32993692 0.31423122 0.26936162 0.20602089 0.13798162 0.075484484 0.028773375 0.00042847064][0.10976864 0.12431248 0.14436665 0.17893526 0.22113363 0.25531143 0.27085817 0.2629835 0.22968839 0.17860885 0.12198675 0.071467139 0.031441651 0.0057597924 -0.0061778338][0.13067921 0.15549235 0.18235636 0.21475798 0.24188857 0.25047514 0.2357588 0.19977179 0.14713795 0.088758476 0.03843382 0.005189362 -0.012447583 -0.017690735 -0.014540391][0.15161479 0.18448764 0.21596946 0.24446248 0.25658083 0.24268927 0.2032852 0.14572594 0.079915226 0.018301988 -0.025243882 -0.044177845 -0.044268698 -0.034598336 -0.021738259][0.16076453 0.19668449 0.228627 0.25124782 0.25037789 0.22096999 0.16751648 0.10024865 0.0313079 -0.027363287 -0.063189477 -0.071818873 -0.061362695 -0.044205196 -0.02776457][0.15834706 0.19126029 0.21852008 0.23371069 0.2233018 0.18642689 0.13000737 0.064621307 0.0016294289 -0.048876151 -0.076158695 -0.077705629 -0.06257654 -0.043899652 -0.028438324]]...]
INFO - root - 2017-12-11 09:43:39.543537: step 48810, loss = 0.70, batch loss = 0.64 (15.1 examples/sec; 0.530 sec/batch; 41h:46m:06s remains)
INFO - root - 2017-12-11 09:43:44.983994: step 48820, loss = 0.69, batch loss = 0.63 (14.8 examples/sec; 0.539 sec/batch; 42h:28m:48s remains)
INFO - root - 2017-12-11 09:43:50.072662: step 48830, loss = 0.69, batch loss = 0.63 (14.5 examples/sec; 0.550 sec/batch; 43h:20m:25s remains)
INFO - root - 2017-12-11 09:43:55.587976: step 48840, loss = 0.71, batch loss = 0.65 (14.2 examples/sec; 0.563 sec/batch; 44h:21m:37s remains)
INFO - root - 2017-12-11 09:44:01.038821: step 48850, loss = 0.70, batch loss = 0.64 (14.7 examples/sec; 0.543 sec/batch; 42h:49m:03s remains)
INFO - root - 2017-12-11 09:44:06.525662: step 48860, loss = 0.69, batch loss = 0.63 (14.6 examples/sec; 0.547 sec/batch; 43h:06m:20s remains)
INFO - root - 2017-12-11 09:44:11.988154: step 48870, loss = 0.69, batch loss = 0.63 (14.8 examples/sec; 0.541 sec/batch; 42h:39m:35s remains)
INFO - root - 2017-12-11 09:44:17.390829: step 48880, loss = 0.71, batch loss = 0.66 (14.9 examples/sec; 0.536 sec/batch; 42h:14m:07s remains)
INFO - root - 2017-12-11 09:44:22.916195: step 48890, loss = 0.69, batch loss = 0.63 (14.8 examples/sec; 0.541 sec/batch; 42h:36m:00s remains)
INFO - root - 2017-12-11 09:44:28.410770: step 48900, loss = 0.72, batch loss = 0.66 (14.8 examples/sec; 0.539 sec/batch; 42h:27m:17s remains)
2017-12-11 09:44:29.018363: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.34191865 0.31132257 0.24794778 0.17157766 0.11051173 0.082193911 0.090129167 0.12072239 0.15902476 0.20082304 0.24033189 0.26365644 0.27044961 0.27242869 0.27450725][0.28081849 0.2553072 0.1994711 0.1302857 0.073163107 0.043736275 0.043854792 0.061575495 0.084326424 0.10944852 0.13513587 0.15272416 0.16358559 0.17789869 0.19997431][0.22536097 0.2054577 0.16013528 0.10189957 0.051837925 0.02344971 0.017819313 0.024933 0.034836266 0.04552909 0.057302684 0.06733837 0.079392672 0.10273979 0.14119053][0.18350644 0.17102419 0.14038144 0.098752692 0.062478755 0.043165807 0.040918779 0.046004664 0.049502127 0.049010977 0.045631174 0.042216502 0.046405721 0.068952948 0.11437561][0.15856823 0.15457757 0.14282279 0.12510319 0.11324843 0.11675256 0.13271761 0.14794867 0.15152842 0.14050215 0.11610498 0.087075517 0.068376042 0.075768925 0.11566825][0.14762229 0.15181361 0.16039041 0.1717363 0.19359745 0.23154862 0.27743652 0.31147355 0.31812367 0.29431269 0.24208583 0.17765024 0.12592153 0.10922907 0.13507198][0.13942592 0.14871588 0.17502758 0.21610618 0.27533573 0.35169202 0.43024731 0.48339143 0.49052206 0.45007306 0.36767945 0.26802289 0.18502073 0.14522766 0.15478408][0.12414109 0.13648432 0.17751533 0.24539936 0.33850697 0.44704926 0.54889005 0.6109404 0.6106894 0.55086696 0.44340298 0.31957716 0.21728498 0.162003 0.15606825][0.0961003 0.11063989 0.16190836 0.24783513 0.361955 0.486011 0.59309793 0.64986026 0.63662815 0.56181419 0.44295964 0.3133657 0.20901309 0.14971067 0.13349102][0.063840613 0.084051095 0.142491 0.23516347 0.35172662 0.46863118 0.55964947 0.59808773 0.57198924 0.49425036 0.38310647 0.26758814 0.17639174 0.12207551 0.10039466][0.04307422 0.07372234 0.13711695 0.22584587 0.32697582 0.41610885 0.47330397 0.48412588 0.44747266 0.37892351 0.29272577 0.20835319 0.14303492 0.10154849 0.079201452][0.039980408 0.080608524 0.14552125 0.22346771 0.29952851 0.35192862 0.36979267 0.3525742 0.31028116 0.25950104 0.20825876 0.16391748 0.13105458 0.10705651 0.087646663][0.060723994 0.10499135 0.16551009 0.22808205 0.277097 0.29525557 0.28034988 0.24311213 0.20216767 0.1732617 0.15774812 0.15167847 0.14882773 0.14058633 0.123368][0.092770182 0.13180344 0.17965783 0.22206968 0.24530876 0.23770146 0.20338061 0.15900193 0.12710194 0.12100945 0.13627054 0.16047359 0.17999598 0.18247998 0.16558789][0.11737214 0.14476225 0.17419116 0.19482434 0.19679341 0.17386132 0.13357145 0.093383059 0.075456388 0.0898447 0.12751512 0.1708551 0.20253596 0.20919016 0.19055454]]...]
INFO - root - 2017-12-11 09:44:34.486942: step 48910, loss = 0.69, batch loss = 0.63 (14.3 examples/sec; 0.561 sec/batch; 44h:09m:50s remains)
INFO - root - 2017-12-11 09:44:39.962591: step 48920, loss = 0.70, batch loss = 0.64 (14.9 examples/sec; 0.537 sec/batch; 42h:16m:38s remains)
INFO - root - 2017-12-11 09:44:45.111658: step 48930, loss = 0.69, batch loss = 0.63 (14.6 examples/sec; 0.550 sec/batch; 43h:17m:09s remains)
INFO - root - 2017-12-11 09:44:50.612950: step 48940, loss = 0.71, batch loss = 0.65 (14.8 examples/sec; 0.539 sec/batch; 42h:26m:33s remains)
INFO - root - 2017-12-11 09:44:56.095688: step 48950, loss = 0.69, batch loss = 0.63 (14.7 examples/sec; 0.545 sec/batch; 42h:57m:06s remains)
INFO - root - 2017-12-11 09:45:01.526920: step 48960, loss = 0.71, batch loss = 0.65 (15.2 examples/sec; 0.527 sec/batch; 41h:31m:46s remains)
INFO - root - 2017-12-11 09:45:06.968096: step 48970, loss = 0.69, batch loss = 0.63 (14.9 examples/sec; 0.538 sec/batch; 42h:23m:35s remains)
INFO - root - 2017-12-11 09:45:12.372354: step 48980, loss = 0.70, batch loss = 0.64 (14.8 examples/sec; 0.539 sec/batch; 42h:28m:29s remains)
INFO - root - 2017-12-11 09:45:17.791103: step 48990, loss = 0.70, batch loss = 0.64 (14.8 examples/sec; 0.542 sec/batch; 42h:40m:24s remains)
INFO - root - 2017-12-11 09:45:23.230298: step 49000, loss = 0.68, batch loss = 0.63 (14.6 examples/sec; 0.547 sec/batch; 43h:03m:32s remains)
2017-12-11 09:45:23.842209: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.13855237 0.12530953 0.10876992 0.10460969 0.11320843 0.12703231 0.13856503 0.14264131 0.13344102 0.10923819 0.078048632 0.048810288 0.030659836 0.024757965 0.033553116][0.13678147 0.12739159 0.11296843 0.10978452 0.12028401 0.1355028 0.14653768 0.14725593 0.13369569 0.10577686 0.0719187 0.041672397 0.025600387 0.024878239 0.040825725][0.1304338 0.12706287 0.11712317 0.11412735 0.12392671 0.13900276 0.15026902 0.14996882 0.13577689 0.10855616 0.075458579 0.04525318 0.030385811 0.033661503 0.055112239][0.12021868 0.12407455 0.11955108 0.116564 0.12485929 0.13900626 0.15012048 0.14917015 0.13640422 0.11393468 0.085391782 0.057172623 0.043189004 0.049626149 0.074970588][0.10848805 0.11818208 0.11895789 0.11687797 0.12447394 0.1375327 0.14718415 0.14427638 0.13294618 0.11749788 0.097442508 0.075375229 0.064236335 0.073480546 0.10045403][0.10596125 0.11860351 0.1230931 0.12200184 0.12886012 0.14079441 0.14939964 0.1449506 0.13468312 0.12586999 0.11577237 0.10291121 0.0965165 0.10818485 0.13502654][0.11633734 0.13136087 0.1397948 0.14064859 0.14646843 0.15679722 0.16538806 0.16086884 0.15071476 0.14578937 0.14397724 0.14100367 0.13998362 0.15337051 0.17912936][0.13323388 0.15157609 0.1654616 0.16877772 0.17168629 0.17733915 0.18399312 0.17992517 0.17064168 0.16934602 0.17507502 0.18152606 0.18464248 0.19745481 0.22114137][0.14509887 0.16802232 0.18714434 0.19236997 0.1907465 0.18915728 0.19111829 0.1861777 0.17826134 0.1808397 0.19361539 0.20820765 0.21436051 0.22435775 0.24339703][0.14309259 0.16727442 0.18878821 0.1951129 0.1898022 0.18175168 0.1782437 0.17166609 0.16556904 0.17155442 0.18917646 0.20843351 0.21575825 0.22130872 0.23331916][0.12175095 0.14313544 0.16345997 0.16984266 0.16277681 0.15120198 0.14418809 0.13724588 0.133294 0.14092399 0.15994276 0.18048429 0.18851236 0.19049202 0.1953914][0.084838584 0.10118691 0.11862238 0.12524903 0.11950947 0.10884635 0.10194483 0.096268363 0.09356498 0.099792942 0.11586261 0.13382806 0.14119445 0.14049648 0.13936117][0.040840913 0.051720444 0.064929046 0.070962533 0.067535721 0.06007963 0.055516351 0.05207625 0.049995419 0.0531467 0.063620143 0.0761127 0.081356294 0.079304688 0.075335316][0.0031572874 0.00831011 0.015736505 0.019231509 0.017067943 0.012939472 0.011452691 0.010700122 0.0099776732 0.011370634 0.01724153 0.024286194 0.026959989 0.024586517 0.020344455][-0.021568544 -0.020931652 -0.018142588 -0.016968055 -0.018293398 -0.019848133 -0.019072073 -0.017777212 -0.017023573 -0.016068447 -0.013251822 -0.010511123 -0.01046172 -0.013358837 -0.017343605]]...]
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-SGD-lr0.001-clip50-initfromfixqian30k-fixqian
INFO - root - 2017-12-11 09:45:29.398879: step 49010, loss = 0.70, batch loss = 0.64 (14.4 examples/sec; 0.555 sec/batch; 43h:43m:07s remains)
INFO - root - 2017-12-11 09:45:34.904627: step 49020, loss = 0.69, batch loss = 0.63 (14.6 examples/sec; 0.548 sec/batch; 43h:09m:27s remains)
INFO - root - 2017-12-11 09:45:40.087060: step 49030, loss = 0.68, batch loss = 0.62 (14.1 examples/sec; 0.566 sec/batch; 44h:35m:40s remains)
INFO - root - 2017-12-11 09:45:45.580389: step 49040, loss = 0.69, batch loss = 0.63 (15.0 examples/sec; 0.534 sec/batch; 42h:04m:17s remains)
INFO - root - 2017-12-11 09:45:51.051365: step 49050, loss = 0.71, batch loss = 0.65 (15.4 examples/sec; 0.520 sec/batch; 40h:57m:09s remains)
INFO - root - 2017-12-11 09:45:56.427332: step 49060, loss = 0.69, batch loss = 0.63 (14.5 examples/sec; 0.553 sec/batch; 43h:30m:50s remains)
INFO - root - 2017-12-11 09:46:01.858586: step 49070, loss = 0.69, batch loss = 0.63 (14.7 examples/sec; 0.542 sec/batch; 42h:42m:17s remains)
INFO - root - 2017-12-11 09:46:07.257734: step 49080, loss = 0.69, batch loss = 0.63 (14.9 examples/sec; 0.537 sec/batch; 42h:18m:07s remains)
INFO - root - 2017-12-11 09:46:12.651039: step 49090, loss = 0.70, batch loss = 0.64 (15.0 examples/sec; 0.534 sec/batch; 42h:02m:56s remains)
INFO - root - 2017-12-11 09:46:18.061088: step 49100, loss = 0.69, batch loss = 0.63 (14.5 examples/sec; 0.552 sec/batch; 43h:25m:56s remains)
2017-12-11 09:46:18.661399: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.17522217 0.20860353 0.21224684 0.19702937 0.16827603 0.14016242 0.13701178 0.16774735 0.20756888 0.22165552 0.19861376 0.15296461 0.096424781 0.038685061 -0.010188237][0.2022026 0.24169466 0.2490662 0.23593643 0.20878977 0.18480039 0.19219142 0.2361799 0.28512508 0.29920846 0.26730585 0.20619671 0.13136622 0.056889746 -0.0039043694][0.22521724 0.2746183 0.29061323 0.28292421 0.25929067 0.24017519 0.25716972 0.31118578 0.36457193 0.37527919 0.331682 0.2516847 0.15437356 0.060091607 -0.012309998][0.2609168 0.32886139 0.36070126 0.36083308 0.33757821 0.31545129 0.33163127 0.38641059 0.43955722 0.44678944 0.39338589 0.29511145 0.1739755 0.057692654 -0.027940633][0.32773215 0.42091423 0.47183329 0.47895858 0.45026332 0.41412917 0.41615683 0.46159059 0.51050419 0.5161587 0.45808041 0.34734821 0.20673311 0.06958802 -0.031155221][0.4204559 0.53861415 0.60462075 0.61427236 0.57467949 0.51752055 0.49848309 0.53133631 0.57587183 0.58136016 0.52112114 0.40254986 0.24814415 0.094576754 -0.020087816][0.51116252 0.64635992 0.7190308 0.72668946 0.67525995 0.59803313 0.56052482 0.58551514 0.62973368 0.63633353 0.573284 0.44866902 0.28552419 0.12190729 -0.0020843584][0.56662 0.70501256 0.7731505 0.77442324 0.71380866 0.62366414 0.575678 0.59878004 0.6460796 0.65453643 0.59015363 0.46537715 0.303096 0.1402674 0.015103334][0.55474234 0.67834234 0.73219085 0.72634226 0.66434968 0.57404608 0.52641547 0.55158496 0.60109985 0.61056495 0.54925025 0.43474266 0.28766707 0.1399505 0.024526628][0.47228917 0.56462866 0.59702754 0.58528256 0.53120935 0.45526454 0.41918641 0.44834724 0.49577847 0.50289577 0.44682229 0.34977132 0.22865145 0.10801041 0.013313782][0.3347311 0.38897496 0.398612 0.38216922 0.34042495 0.28623343 0.26676363 0.29908994 0.3410551 0.34443697 0.29572669 0.219241 0.12816294 0.040072836 -0.026652407][0.17521897 0.19851969 0.19342923 0.17639811 0.1477375 0.11440163 0.10848958 0.13912144 0.17231028 0.17222673 0.13289408 0.077100091 0.014634713 -0.042236056 -0.080406629][0.038275834 0.042740796 0.032284915 0.018028637 6.6398621e-05 -0.018291948 -0.018074445 0.0044669574 0.02599711 0.023393165 -0.0046442049 -0.040474217 -0.077548333 -0.10781925 -0.1223454][-0.053562585 -0.058493882 -0.068264768 -0.077724017 -0.087326549 -0.096083589 -0.094963893 -0.082907692 -0.072816469 -0.0768189 -0.0942465 -0.11397283 -0.13193829 -0.14325549 -0.14225002][-0.098452136 -0.1059634 -0.11258809 -0.11750142 -0.1214919 -0.12465176 -0.1241731 -0.12004265 -0.11786505 -0.12241367 -0.13267596 -0.142834 -0.15015475 -0.15142915 -0.14331073]]...]
INFO - root - 2017-12-11 09:46:24.196854: step 49110, loss = 0.70, batch loss = 0.64 (14.8 examples/sec; 0.540 sec/batch; 42h:30m:56s remains)
INFO - root - 2017-12-11 09:46:29.569078: step 49120, loss = 0.68, batch loss = 0.63 (14.8 examples/sec; 0.540 sec/batch; 42h:32m:31s remains)
INFO - root - 2017-12-11 09:46:34.801853: step 49130, loss = 0.71, batch loss = 0.65 (14.0 examples/sec; 0.571 sec/batch; 44h:55m:48s remains)
INFO - root - 2017-12-11 09:46:40.259833: step 49140, loss = 0.71, batch loss = 0.65 (15.3 examples/sec; 0.524 sec/batch; 41h:16m:43s remains)
INFO - root - 2017-12-11 09:46:45.705228: step 49150, loss = 0.71, batch loss = 0.65 (14.9 examples/sec; 0.536 sec/batch; 42h:09m:25s remains)
INFO - root - 2017-12-11 09:46:51.146327: step 49160, loss = 0.70, batch loss = 0.64 (15.0 examples/sec; 0.535 sec/batch; 42h:06m:40s remains)
INFO - root - 2017-12-11 09:46:54.709850: step 49170, loss = 0.70, batch loss = 0.64 (30.9 examples/sec; 0.259 sec/batch; 20h:23m:55s remains)
INFO - root - 2017-12-11 09:46:57.359396: step 49180, loss = 0.69, batch loss = 0.63 (31.5 examples/sec; 0.254 sec/batch; 19h:58m:26s remains)
INFO - root - 2017-12-11 09:46:59.966884: step 49190, loss = 0.71, batch loss = 0.65 (31.4 examples/sec; 0.255 sec/batch; 20h:02m:21s remains)
INFO - root - 2017-12-11 09:47:02.573280: step 49200, loss = 0.72, batch loss = 0.66 (29.4 examples/sec; 0.272 sec/batch; 21h:25m:47s remains)
2017-12-11 09:47:02.951747: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.064008176 -0.056236345 -0.040355075 -0.020551736 -0.0025088303 0.0059811673 0.00010755206 -0.017990457 -0.039969455 -0.057372592 -0.065970249 -0.06604138 -0.061821666 -0.057369433 -0.053902481][-0.073921658 -0.057762295 -0.027929088 0.0086046383 0.042733539 0.062464476 0.0592357 0.034162648 -0.00089881255 -0.032902453 -0.054918345 -0.0657336 -0.069126494 -0.069198139 -0.068185247][-0.073418446 -0.0475939 -0.0014839974 0.055561107 0.11140101 0.14995046 0.15796095 0.13305925 0.088438936 0.040994335 0.00025029923 -0.029731229 -0.04957775 -0.061275464 -0.067989871][-0.067656092 -0.032938443 0.028862508 0.10727743 0.18859673 0.25367579 0.28381395 0.269935 0.22280647 0.16163783 0.098217882 0.040725026 -0.0052382969 -0.0364405 -0.056301907][-0.062832914 -0.022356989 0.051427741 0.14834972 0.25524309 0.35147461 0.41256002 0.4196887 0.37745416 0.30556583 0.21788499 0.12789497 0.050073568 -0.0053384784 -0.041548304][-0.058061939 -0.015368401 0.065608628 0.17700449 0.30778524 0.43628743 0.53199244 0.56476074 0.52973574 0.44572756 0.33081177 0.20685786 0.09766715 0.019330606 -0.031336572][-0.053429566 -0.01102256 0.0733941 0.19513698 0.3453021 0.50026536 0.6234535 0.67502105 0.64119375 0.54045844 0.39775386 0.24516685 0.11409346 0.022664789 -0.033852205][-0.052344471 -0.012323854 0.071703531 0.19833189 0.35983157 0.52968556 0.66595817 0.72307241 0.68167323 0.56109291 0.39423451 0.22301558 0.084101781 -0.0060174027 -0.055315387][-0.05834882 -0.024779785 0.052628245 0.17526673 0.33587241 0.50571972 0.63995838 0.69191492 0.640527 0.50623262 0.32836765 0.15505116 0.024079572 -0.051916607 -0.0846911][-0.070209213 -0.04637612 0.018007489 0.12613989 0.2706534 0.42282453 0.53953874 0.57833427 0.52007669 0.3860417 0.21791297 0.063721359 -0.043208748 -0.096176222 -0.10978198][-0.079102144 -0.064130954 -0.014931929 0.071023457 0.18567429 0.30330485 0.38844743 0.40809417 0.34803408 0.22903965 0.089614086 -0.028872797 -0.10175175 -0.1285688 -0.12498519][-0.077881627 -0.066383161 -0.029979374 0.030955724 0.10740298 0.17962448 0.22463197 0.22235529 0.16504237 0.070998684 -0.029044302 -0.10527615 -0.1428591 -0.14584455 -0.12928827][-0.062910855 -0.045812234 -0.013550577 0.028230999 0.068893008 0.095272221 0.098433882 0.072835065 0.01821628 -0.050190974 -0.11157334 -0.14960623 -0.15910208 -0.14632221 -0.1236301][-0.0363072 -0.0036120226 0.03722563 0.073827393 0.092613451 0.084680624 0.052844808 0.0049289353 -0.051918466 -0.10467858 -0.14090659 -0.15517262 -0.150171 -0.13287559 -0.11186627][-0.0034061929 0.051650263 0.11294073 0.16137661 0.17886887 0.15605703 0.10190469 0.034480378 -0.032656174 -0.086878084 -0.11988898 -0.13101013 -0.12713633 -0.11558858 -0.10164527]]...]
INFO - root - 2017-12-11 09:47:05.551765: step 49210, loss = 0.70, batch loss = 0.64 (29.6 examples/sec; 0.270 sec/batch; 21h:16m:20s remains)
INFO - root - 2017-12-11 09:47:08.241194: step 49220, loss = 0.70, batch loss = 0.64 (29.6 examples/sec; 0.270 sec/batch; 21h:16m:04s remains)
INFO - root - 2017-12-11 09:47:10.874496: step 49230, loss = 0.70, batch loss = 0.64 (31.5 examples/sec; 0.254 sec/batch; 19h:59m:41s remains)
INFO - root - 2017-12-11 09:47:13.526156: step 49240, loss = 0.70, batch loss = 0.64 (29.9 examples/sec; 0.268 sec/batch; 21h:04m:36s remains)
INFO - root - 2017-12-11 09:47:16.209751: step 49250, loss = 0.70, batch loss = 0.64 (29.7 examples/sec; 0.270 sec/batch; 21h:13m:32s remains)
INFO - root - 2017-12-11 09:47:18.840471: step 49260, loss = 0.71, batch loss = 0.65 (31.0 examples/sec; 0.258 sec/batch; 20h:18m:25s remains)
INFO - root - 2017-12-11 09:47:21.506863: step 49270, loss = 0.69, batch loss = 0.64 (29.6 examples/sec; 0.270 sec/batch; 21h:13m:39s remains)
INFO - root - 2017-12-11 09:47:24.148824: step 49280, loss = 0.68, batch loss = 0.62 (30.4 examples/sec; 0.263 sec/batch; 20h:40m:47s remains)
INFO - root - 2017-12-11 09:47:26.803295: step 49290, loss = 0.70, batch loss = 0.64 (29.0 examples/sec; 0.276 sec/batch; 21h:42m:55s remains)
INFO - root - 2017-12-11 09:47:29.482869: step 49300, loss = 0.71, batch loss = 0.65 (30.9 examples/sec; 0.259 sec/batch; 20h:20m:07s remains)
2017-12-11 09:47:29.874314: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.3370724 0.39076081 0.43222865 0.44980294 0.433359 0.38760117 0.33140674 0.28760651 0.25870815 0.23165743 0.18852827 0.12284938 0.046051081 -0.021208238 -0.063962094][0.3307583 0.38980317 0.43669492 0.46087486 0.45106387 0.41247389 0.36204043 0.3202931 0.28958169 0.25978157 0.21679668 0.15220331 0.072169051 -0.003162079 -0.053877916][0.33864132 0.40139541 0.4486064 0.47423568 0.47025564 0.44374332 0.40584227 0.37004805 0.33648536 0.29872781 0.24961881 0.18255249 0.099300437 0.017159119 -0.04088369][0.36740023 0.42924002 0.47193855 0.4967716 0.50067705 0.49118361 0.47052789 0.44309163 0.40588233 0.35484907 0.29170609 0.21465276 0.12376686 0.033577722 -0.031532891][0.39021963 0.4439756 0.47624391 0.49554342 0.50345027 0.5077439 0.50446075 0.49067953 0.45863363 0.40124983 0.32673064 0.23920926 0.1405385 0.043701477 -0.026105287][0.4083136 0.44995311 0.46753123 0.47496045 0.47841251 0.48787808 0.49893194 0.50409573 0.48801768 0.43478113 0.35379282 0.25502113 0.14741468 0.04552665 -0.026466234][0.42391768 0.45400283 0.456833 0.44925034 0.44300351 0.45328167 0.47910711 0.50953263 0.51793617 0.47552121 0.3886 0.2737444 0.15196262 0.042604342 -0.031727023][0.4264378 0.44921419 0.44164935 0.42046508 0.40407455 0.41527244 0.45706514 0.51486641 0.54940218 0.52008528 0.42827347 0.29591462 0.15721934 0.038273524 -0.039760407][0.40957418 0.43206772 0.4211601 0.39111251 0.36671063 0.37736487 0.42912692 0.50472462 0.55659515 0.538722 0.44716555 0.30544165 0.1562513 0.031426653 -0.048453342][0.38164246 0.40662497 0.39625326 0.36201453 0.33231294 0.33942679 0.39097178 0.46985728 0.52896416 0.52306658 0.44159755 0.30340785 0.15291126 0.026525743 -0.054130968][0.34701711 0.37117276 0.36089846 0.32734123 0.29790178 0.3027074 0.3482956 0.42050156 0.47962502 0.48441505 0.41768786 0.29038271 0.14468281 0.020406876 -0.059198968][0.313979 0.33256209 0.32023415 0.2905826 0.26732209 0.27499756 0.31659788 0.38077834 0.43623337 0.44603091 0.38922316 0.27153543 0.13230567 0.012705682 -0.063552618][0.28643221 0.29665527 0.27975893 0.25239834 0.23509678 0.24746098 0.28867683 0.34810603 0.39939797 0.40836808 0.35436764 0.24298288 0.11162359 0.00049824908 -0.0688404][0.26154232 0.26290902 0.23865654 0.2087052 0.19383161 0.21219566 0.25888044 0.31963328 0.36791196 0.37124208 0.31350514 0.20460413 0.081735462 -0.017515663 -0.076575473][0.24223934 0.23536979 0.20243004 0.16532181 0.14870182 0.17244615 0.22759666 0.29464591 0.34428754 0.34441131 0.28315377 0.17497575 0.058206145 -0.031620193 -0.0826359]]...]
INFO - root - 2017-12-11 09:47:32.476724: step 49310, loss = 0.71, batch loss = 0.65 (31.4 examples/sec; 0.255 sec/batch; 20h:02m:02s remains)
INFO - root - 2017-12-11 09:47:35.089025: step 49320, loss = 0.70, batch loss = 0.64 (31.2 examples/sec; 0.256 sec/batch; 20h:08m:24s remains)
INFO - root - 2017-12-11 09:47:37.715960: step 49330, loss = 0.70, batch loss = 0.64 (31.0 examples/sec; 0.258 sec/batch; 20h:18m:05s remains)
INFO - root - 2017-12-11 09:47:40.338886: step 49340, loss = 0.69, batch loss = 0.63 (29.7 examples/sec; 0.269 sec/batch; 21h:11m:45s remains)
INFO - root - 2017-12-11 09:47:42.985594: step 49350, loss = 0.70, batch loss = 0.64 (29.5 examples/sec; 0.272 sec/batch; 21h:21m:25s remains)
INFO - root - 2017-12-11 09:47:45.639067: step 49360, loss = 0.69, batch loss = 0.63 (30.2 examples/sec; 0.265 sec/batch; 20h:50m:24s remains)
INFO - root - 2017-12-11 09:47:48.285046: step 49370, loss = 0.71, batch loss = 0.65 (29.4 examples/sec; 0.272 sec/batch; 21h:24m:53s remains)
INFO - root - 2017-12-11 09:47:50.945327: step 49380, loss = 0.69, batch loss = 0.63 (30.6 examples/sec; 0.261 sec/batch; 20h:32m:08s remains)
INFO - root - 2017-12-11 09:47:53.642098: step 49390, loss = 0.68, batch loss = 0.62 (30.1 examples/sec; 0.266 sec/batch; 20h:55m:44s remains)
INFO - root - 2017-12-11 09:47:56.294273: step 49400, loss = 0.70, batch loss = 0.64 (31.1 examples/sec; 0.257 sec/batch; 20h:14m:28s remains)
2017-12-11 09:47:56.677824: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.18720907 0.19650571 0.20511329 0.21263142 0.21742831 0.21731618 0.21779966 0.22983955 0.23603554 0.22964844 0.20705685 0.18040989 0.14175516 0.08049468 0.012646165][0.25162938 0.26713708 0.28350824 0.29903522 0.31032741 0.31559134 0.32280517 0.34263906 0.35027981 0.33859462 0.30389351 0.26092634 0.20205913 0.11983737 0.034269914][0.27316129 0.29801267 0.32829866 0.35722464 0.37917966 0.39422092 0.41269121 0.43985537 0.44519404 0.42596021 0.37974983 0.31875971 0.23703319 0.13425703 0.035567347][0.2430024 0.28353444 0.33654252 0.385872 0.42316082 0.45329735 0.48849112 0.52182961 0.51980525 0.48963693 0.43334323 0.35552558 0.2493584 0.12524827 0.015576684][0.19934133 0.26532626 0.35192174 0.43004215 0.48667562 0.53558755 0.59134805 0.62926811 0.61481947 0.56923622 0.5038569 0.40994924 0.27528781 0.12253323 -0.0044826432][0.19228387 0.28939441 0.41468275 0.52528322 0.6008141 0.66656339 0.74143887 0.78313017 0.7554642 0.69331717 0.61981648 0.51013118 0.34274977 0.15088347 -0.0052070771][0.23388013 0.35753724 0.51440054 0.65170819 0.74001676 0.8145594 0.90096235 0.94725591 0.91394621 0.84216762 0.76359504 0.64016777 0.44097796 0.20646088 0.014710557][0.30269077 0.44054079 0.61362749 0.76445037 0.85520864 0.92694628 1.0125053 1.0608711 1.0296288 0.95646906 0.87548649 0.74203509 0.51981926 0.25335485 0.033538211][0.34868592 0.48424062 0.65352577 0.7995379 0.88011265 0.93663144 1.0063466 1.0495516 1.0245216 0.95801824 0.8802827 0.74671072 0.52213913 0.25091088 0.026908524][0.33625349 0.45531186 0.6022411 0.7252087 0.78455436 0.81774211 0.86163062 0.89315712 0.87644428 0.82506889 0.75985992 0.64061624 0.43894678 0.19398646 -0.0077509005][0.25717035 0.35187441 0.46539995 0.55435765 0.58789027 0.59816754 0.6177693 0.63855886 0.63270295 0.60255295 0.55828506 0.46650612 0.30596906 0.10745221 -0.055842035][0.1457005 0.2160558 0.29542342 0.3498044 0.35971284 0.35347134 0.35919604 0.377287 0.38595307 0.38003293 0.36084679 0.30114388 0.18428391 0.033111934 -0.092340454][0.061586171 0.11292833 0.16478999 0.19184694 0.18444704 0.17022364 0.17570119 0.20232615 0.22879966 0.24574998 0.24872702 0.214117 0.12693049 0.0055970615 -0.0977966][0.028438738 0.065040894 0.096060522 0.10426383 0.086822987 0.072560579 0.089574955 0.13550745 0.18381643 0.22136705 0.24024218 0.21838602 0.1415821 0.028307794 -0.071690768][0.027417302 0.051604662 0.0678987 0.065321743 0.044615295 0.035061989 0.067143805 0.13363136 0.20078515 0.25186548 0.27755341 0.25769782 0.1796287 0.064237535 -0.039475776]]...]
INFO - root - 2017-12-11 09:47:59.311669: step 49410, loss = 0.71, batch loss = 0.66 (30.4 examples/sec; 0.263 sec/batch; 20h:41m:03s remains)
INFO - root - 2017-12-11 09:48:01.952132: step 49420, loss = 0.70, batch loss = 0.64 (29.4 examples/sec; 0.273 sec/batch; 21h:25m:55s remains)
INFO - root - 2017-12-11 09:48:04.569529: step 49430, loss = 0.70, batch loss = 0.64 (31.1 examples/sec; 0.257 sec/batch; 20h:12m:36s remains)
INFO - root - 2017-12-11 09:48:07.229773: step 49440, loss = 0.70, batch loss = 0.64 (30.2 examples/sec; 0.265 sec/batch; 20h:50m:34s remains)
INFO - root - 2017-12-11 09:48:09.894948: step 49450, loss = 0.69, batch loss = 0.63 (31.7 examples/sec; 0.252 sec/batch; 19h:50m:48s remains)
INFO - root - 2017-12-11 09:48:12.581963: step 49460, loss = 0.71, batch loss = 0.65 (29.8 examples/sec; 0.269 sec/batch; 21h:07m:28s remains)
INFO - root - 2017-12-11 09:48:15.262453: step 49470, loss = 0.71, batch loss = 0.65 (30.5 examples/sec; 0.263 sec/batch; 20h:38m:28s remains)
INFO - root - 2017-12-11 09:48:17.950547: step 49480, loss = 0.69, batch loss = 0.63 (30.0 examples/sec; 0.267 sec/batch; 20h:57m:20s remains)
INFO - root - 2017-12-11 09:48:20.594328: step 49490, loss = 0.71, batch loss = 0.65 (30.9 examples/sec; 0.259 sec/batch; 20h:20m:45s remains)
INFO - root - 2017-12-11 09:48:23.227720: step 49500, loss = 0.71, batch loss = 0.65 (30.8 examples/sec; 0.259 sec/batch; 20h:23m:43s remains)
2017-12-11 09:48:23.672435: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.63045549 0.62347972 0.54217321 0.42012191 0.28041041 0.14426355 0.031484522 -0.041837458 -0.082796924 -0.1057804 -0.11404471 -0.11001543 -0.099829517 -0.091160484 -0.0865239][0.49531251 0.49231943 0.42482725 0.32394788 0.20956129 0.1004604 0.01262278 -0.044262476 -0.07922101 -0.10280753 -0.11520345 -0.11575394 -0.10796753 -0.098657548 -0.09186887][0.30298316 0.31167963 0.27638659 0.21812794 0.1502552 0.085347176 0.033313714 -0.0039727786 -0.036821164 -0.0692818 -0.09525124 -0.10939378 -0.11117716 -0.10512723 -0.097250879][0.1240978 0.15162659 0.1599828 0.15616889 0.14440583 0.12984851 0.11543968 0.095329173 0.058609784 0.0086962646 -0.0407243 -0.078052752 -0.09792316 -0.10145857 -0.096173711][0.0044244118 0.054993968 0.10988594 0.1630742 0.20755172 0.23995204 0.25656256 0.24594557 0.19828711 0.12475494 0.045790963 -0.021465594 -0.066854544 -0.086323112 -0.088357553][-0.040833909 0.032629829 0.13110033 0.23668978 0.33134156 0.40240648 0.44000149 0.42935216 0.36357203 0.26138619 0.14898653 0.047157533 -0.028483938 -0.068087779 -0.080848083][-0.03474972 0.057463609 0.19044337 0.3367956 0.46922541 0.56704944 0.61592865 0.59988296 0.51530862 0.38600245 0.24173945 0.10781419 0.0041199494 -0.0547987 -0.078435622][-0.013711412 0.084178455 0.23291588 0.40035072 0.55372775 0.66626257 0.72012174 0.6997062 0.60456896 0.46052209 0.2985591 0.14634138 0.026451768 -0.04451723 -0.076937988][-0.0057807923 0.07990998 0.22027452 0.38508597 0.54133 0.65811831 0.71362007 0.69282484 0.59920812 0.45890832 0.30064443 0.15078507 0.032030344 -0.039868303 -0.076258413][-0.013246148 0.044312064 0.1531179 0.29161054 0.43264118 0.54480535 0.60175073 0.58807182 0.511026 0.3950721 0.2634975 0.13775387 0.0373294 -0.025992632 -0.063681446][-0.0063658147 0.01127752 0.069564275 0.16118418 0.27005714 0.36902139 0.42962083 0.43494308 0.3921631 0.31989968 0.233696 0.14836596 0.077843212 0.027790712 -0.012670724][0.04755516 0.021941479 0.01893929 0.049605533 0.11212675 0.18867187 0.25319445 0.28665498 0.29153895 0.27598009 0.24623398 0.20999798 0.17415214 0.13743217 0.090192162][0.16171296 0.096724339 0.031220697 -0.0041172565 0.0038640061 0.050839785 0.11747486 0.18250349 0.24061741 0.28683186 0.31677285 0.32994014 0.32580733 0.29756358 0.23841237][0.31878614 0.22372226 0.10402668 0.0086415559 -0.036438808 -0.021082327 0.044301003 0.13689011 0.24275926 0.34438449 0.42725691 0.48219898 0.50158685 0.47546253 0.40151471][0.46981552 0.35827786 0.20087373 0.061368532 -0.026107546 -0.039740272 0.019110009 0.12769668 0.26570502 0.40644023 0.52800924 0.61365646 0.64956886 0.62402886 0.5385533]]...]
INFO - root - 2017-12-11 09:48:26.335925: step 49510, loss = 0.70, batch loss = 0.64 (30.6 examples/sec; 0.262 sec/batch; 20h:34m:09s remains)
INFO - root - 2017-12-11 09:48:28.979806: step 49520, loss = 0.69, batch loss = 0.63 (29.1 examples/sec; 0.275 sec/batch; 21h:36m:18s remains)
INFO - root - 2017-12-11 09:48:31.624831: step 49530, loss = 0.69, batch loss = 0.63 (29.4 examples/sec; 0.272 sec/batch; 21h:23m:10s remains)
INFO - root - 2017-12-11 09:48:34.253025: step 49540, loss = 0.71, batch loss = 0.65 (31.1 examples/sec; 0.257 sec/batch; 20h:11m:48s remains)
INFO - root - 2017-12-11 09:48:36.928230: step 49550, loss = 0.70, batch loss = 0.64 (30.6 examples/sec; 0.261 sec/batch; 20h:31m:58s remains)
INFO - root - 2017-12-11 09:48:39.595115: step 49560, loss = 0.70, batch loss = 0.64 (30.0 examples/sec; 0.266 sec/batch; 20h:56m:29s remains)
INFO - root - 2017-12-11 09:48:42.739170: step 49570, loss = 0.70, batch loss = 0.64 (16.9 examples/sec; 0.474 sec/batch; 37h:14m:21s remains)
INFO - root - 2017-12-11 09:48:47.931965: step 49580, loss = 0.69, batch loss = 0.63 (15.0 examples/sec; 0.532 sec/batch; 41h:47m:51s remains)
INFO - root - 2017-12-11 09:48:53.418881: step 49590, loss = 0.70, batch loss = 0.65 (14.9 examples/sec; 0.537 sec/batch; 42h:11m:15s remains)
INFO - root - 2017-12-11 09:48:58.845684: step 49600, loss = 0.69, batch loss = 0.63 (14.6 examples/sec; 0.548 sec/batch; 43h:05m:45s remains)
2017-12-11 09:48:59.419118: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.069703087 -0.068400033 -0.064357586 -0.060149334 -0.057246525 -0.057643894 -0.061939221 -0.0683768 -0.074476667 -0.078589484 -0.080457844 -0.080650546 -0.080585882 -0.081413575 -0.083286591][-0.050857291 -0.0466494 -0.037902135 -0.027215134 -0.017679865 -0.013566018 -0.017383054 -0.027183056 -0.038232032 -0.047385439 -0.053509545 -0.056804981 -0.059423003 -0.063651331 -0.070894532][-0.01464193 -0.0058883689 0.0096788108 0.029218487 0.047358852 0.057042927 0.053475466 0.038549006 0.020791607 0.0066583552 -0.0013727088 -0.0038351303 -0.0055048908 -0.011595343 -0.025640776][0.046613745 0.061281506 0.084216468 0.11228592 0.1373415 0.1502935 0.14508183 0.12336027 0.09798421 0.080176026 0.075257659 0.080919631 0.086260274 0.081096657 0.059562158][0.14286526 0.16456442 0.19376765 0.22684877 0.25293764 0.26342431 0.25365731 0.2242505 0.19115527 0.1717191 0.17555799 0.19607724 0.21231733 0.20821899 0.1770055][0.25832093 0.28495422 0.31725854 0.35097188 0.37301522 0.37735367 0.36316639 0.32903156 0.29085422 0.27195984 0.28766072 0.32538182 0.35245964 0.34726194 0.30400428][0.36476955 0.39314204 0.42533907 0.45802543 0.47634003 0.47638986 0.46177173 0.42859682 0.38880363 0.36994863 0.3938691 0.44307464 0.47505575 0.46462035 0.40862784][0.4351325 0.46200347 0.49189004 0.52435714 0.543406 0.54550624 0.53674406 0.511032 0.47369063 0.45274782 0.47567689 0.52317905 0.5483945 0.52708614 0.45997787][0.44671386 0.46914169 0.4954142 0.52853847 0.55287832 0.56344277 0.5662781 0.55432874 0.52535 0.50280195 0.51594746 0.54794747 0.55549604 0.52007842 0.44685373][0.39747009 0.4140206 0.43613598 0.46915224 0.49978319 0.52089423 0.5364179 0.540069 0.5234912 0.50191581 0.50166368 0.512084 0.50029254 0.45530257 0.38524282][0.30091986 0.31210804 0.3295798 0.36015573 0.39409682 0.42263815 0.44688073 0.46270254 0.45869026 0.44100347 0.42977229 0.42219943 0.39878058 0.35474905 0.29836419][0.18524729 0.19233964 0.20513038 0.23019588 0.26135567 0.29014021 0.31561428 0.33612373 0.34077522 0.32897496 0.31347847 0.29759786 0.27334347 0.23940888 0.20209967][0.072520092 0.076094471 0.084338985 0.10140941 0.12409417 0.14607506 0.166174 0.18465717 0.19325416 0.18794976 0.17492892 0.16016981 0.14278007 0.12246911 0.10309959][-0.022953618 -0.022727925 -0.018056745 -0.0088174446 0.0035713259 0.015423643 0.026745925 0.039178323 0.04758599 0.047545243 0.040822681 0.032911938 0.025621746 0.018499084 0.013395699][-0.086299732 -0.088154584 -0.0858951 -0.08265914 -0.078977823 -0.076049596 -0.072771043 -0.067158848 -0.061758518 -0.059474986 -0.060414348 -0.061111629 -0.059829317 -0.057487879 -0.053648498]]...]
INFO - root - 2017-12-11 09:49:04.945854: step 49610, loss = 0.70, batch loss = 0.64 (14.8 examples/sec; 0.540 sec/batch; 42h:25m:36s remains)
INFO - root - 2017-12-11 09:49:10.406528: step 49620, loss = 0.69, batch loss = 0.63 (14.8 examples/sec; 0.540 sec/batch; 42h:27m:20s remains)
INFO - root - 2017-12-11 09:49:15.842165: step 49630, loss = 0.70, batch loss = 0.65 (15.2 examples/sec; 0.526 sec/batch; 41h:19m:01s remains)
INFO - root - 2017-12-11 09:49:21.316408: step 49640, loss = 0.72, batch loss = 0.66 (14.7 examples/sec; 0.544 sec/batch; 42h:45m:08s remains)
INFO - root - 2017-12-11 09:49:26.819838: step 49650, loss = 0.71, batch loss = 0.65 (14.8 examples/sec; 0.541 sec/batch; 42h:28m:18s remains)
INFO - root - 2017-12-11 09:49:31.961339: step 49660, loss = 0.69, batch loss = 0.63 (14.4 examples/sec; 0.555 sec/batch; 43h:34m:40s remains)
INFO - root - 2017-12-11 09:49:37.450169: step 49670, loss = 0.71, batch loss = 0.65 (14.0 examples/sec; 0.572 sec/batch; 44h:56m:08s remains)
INFO - root - 2017-12-11 09:49:42.922121: step 49680, loss = 0.67, batch loss = 0.61 (15.3 examples/sec; 0.522 sec/batch; 40h:59m:53s remains)
INFO - root - 2017-12-11 09:49:48.440806: step 49690, loss = 0.70, batch loss = 0.64 (14.9 examples/sec; 0.536 sec/batch; 42h:06m:31s remains)
INFO - root - 2017-12-11 09:49:53.921421: step 49700, loss = 0.70, batch loss = 0.65 (14.3 examples/sec; 0.558 sec/batch; 43h:48m:06s remains)
2017-12-11 09:49:54.498661: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.11476627 0.12697345 0.1328492 0.13694157 0.1434032 0.15476881 0.16942753 0.17967182 0.17213902 0.14014778 0.090478733 0.036090452 -0.00858904 -0.037746351 -0.05074811][0.18923829 0.21289532 0.22302556 0.22538905 0.22716185 0.23367842 0.24599123 0.25523984 0.24520805 0.20476222 0.14022654 0.069207348 0.01125021 -0.027491827 -0.046766937][0.25374261 0.28515992 0.29617164 0.29324311 0.2872434 0.28685811 0.29627684 0.30534789 0.29577529 0.25225088 0.17909981 0.096593969 0.028869947 -0.017015783 -0.041538473][0.27455568 0.3082788 0.31778842 0.31069562 0.30045056 0.29714057 0.30617833 0.31669527 0.30956385 0.26782402 0.19304208 0.10646351 0.035161268 -0.012554318 -0.037830658][0.25728735 0.28988087 0.29936129 0.29345992 0.28612512 0.28523281 0.29456848 0.30388367 0.29647297 0.25619262 0.18279523 0.097464956 0.028264921 -0.016033849 -0.038098406][0.21501651 0.2452369 0.2567704 0.25701058 0.25818512 0.26352808 0.2749607 0.28326309 0.27524036 0.23729075 0.16743085 0.08570867 0.019901449 -0.021024732 -0.040355213][0.15818417 0.18653941 0.20451127 0.21726948 0.23273572 0.24928193 0.26715973 0.27697793 0.26825586 0.23042308 0.16150852 0.0813465 0.016400147 -0.023734499 -0.042391084][0.098787926 0.1231131 0.14792722 0.17486849 0.2066417 0.23714437 0.26404163 0.27699769 0.26723993 0.22751516 0.15818085 0.078897804 0.013540551 -0.026957704 -0.045623992][0.046456896 0.066535726 0.096730612 0.13516156 0.18084019 0.22458586 0.26028451 0.27654952 0.26607895 0.22442985 0.15461427 0.076064609 0.010232869 -0.031015253 -0.04946956][0.021468252 0.0402968 0.0751758 0.1210602 0.17527936 0.22729687 0.26730251 0.28329763 0.26961303 0.22402076 0.1519895 0.073019318 0.0071724514 -0.033697855 -0.051736906][0.023204217 0.044222564 0.082389817 0.13042589 0.18581781 0.23875536 0.27821204 0.29266381 0.276631 0.22791426 0.15373673 0.073450029 0.0070273746 -0.03358053 -0.051511168][0.039043896 0.06356357 0.10185726 0.14557694 0.19427672 0.24124022 0.27661866 0.29002544 0.2742717 0.22505137 0.1505855 0.070036225 0.0042128107 -0.0349794 -0.05179875][0.053753473 0.080050454 0.11459746 0.14899732 0.18624687 0.22350891 0.25244063 0.26339659 0.2479257 0.2007333 0.13020346 0.054550231 -0.0057522855 -0.040214293 -0.053954229][0.058485582 0.08175873 0.10754033 0.12922555 0.15353484 0.18071862 0.20353632 0.21261308 0.19904843 0.15781917 0.096444875 0.030859 -0.019850243 -0.047260843 -0.056643106][0.053246997 0.067395724 0.080394991 0.088728927 0.10146091 0.11995567 0.13748804 0.14504398 0.13438085 0.10138006 0.052271288 0.00016360474 -0.038113631 -0.056353435 -0.059828207]]...]
INFO - root - 2017-12-11 09:49:59.942729: step 49710, loss = 0.70, batch loss = 0.64 (14.6 examples/sec; 0.548 sec/batch; 43h:01m:39s remains)
INFO - root - 2017-12-11 09:50:05.379174: step 49720, loss = 0.70, batch loss = 0.65 (14.5 examples/sec; 0.552 sec/batch; 43h:20m:54s remains)
INFO - root - 2017-12-11 09:50:10.874909: step 49730, loss = 0.70, batch loss = 0.65 (14.5 examples/sec; 0.551 sec/batch; 43h:18m:14s remains)
INFO - root - 2017-12-11 09:50:16.106227: step 49740, loss = 0.72, batch loss = 0.66 (14.3 examples/sec; 0.560 sec/batch; 43h:58m:29s remains)
INFO - root - 2017-12-11 09:50:21.530587: step 49750, loss = 0.69, batch loss = 0.63 (15.7 examples/sec; 0.511 sec/batch; 40h:05m:58s remains)
INFO - root - 2017-12-11 09:50:26.949756: step 49760, loss = 0.70, batch loss = 0.64 (15.1 examples/sec; 0.530 sec/batch; 41h:36m:49s remains)
INFO - root - 2017-12-11 09:50:32.419636: step 49770, loss = 0.70, batch loss = 0.64 (14.6 examples/sec; 0.547 sec/batch; 42h:59m:44s remains)
INFO - root - 2017-12-11 09:50:37.921352: step 49780, loss = 0.69, batch loss = 0.63 (14.6 examples/sec; 0.548 sec/batch; 43h:04m:27s remains)
INFO - root - 2017-12-11 09:50:43.453859: step 49790, loss = 0.70, batch loss = 0.64 (14.6 examples/sec; 0.546 sec/batch; 42h:54m:00s remains)
INFO - root - 2017-12-11 09:50:48.875888: step 49800, loss = 0.70, batch loss = 0.64 (14.6 examples/sec; 0.548 sec/batch; 43h:01m:36s remains)
2017-12-11 09:50:49.465675: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.078775719 0.091370657 0.10932633 0.13285385 0.15543444 0.16822666 0.17485619 0.18450865 0.19539754 0.20114875 0.20454584 0.21462095 0.22920051 0.24847524 0.2668229][0.10087206 0.10995098 0.12438382 0.14440189 0.16357313 0.17371292 0.17915253 0.18851738 0.19946864 0.20517837 0.20692852 0.21309894 0.22505772 0.24532853 0.26889741][0.13727979 0.14232463 0.15166639 0.16717079 0.18305784 0.1911796 0.19739775 0.20864925 0.22165556 0.22815765 0.22774303 0.22882904 0.23481114 0.25088277 0.27407318][0.18171065 0.18221736 0.18621641 0.19829132 0.21269745 0.21996988 0.22715732 0.23986247 0.2543368 0.26062697 0.256682 0.25136641 0.24957034 0.25816911 0.27585831][0.21605328 0.21295989 0.21236846 0.22152655 0.23429002 0.23955876 0.24486399 0.25538853 0.26800942 0.27205727 0.26437241 0.25389963 0.2458283 0.24717762 0.25757319][0.22853224 0.22479974 0.22181496 0.22883496 0.239226 0.24098034 0.24125826 0.24521157 0.25146315 0.2503038 0.23854861 0.22540739 0.21556768 0.21451344 0.22108556][0.21573466 0.21457587 0.21253751 0.21976411 0.22913449 0.22872044 0.22451185 0.22141498 0.21978584 0.21216078 0.19654207 0.18315452 0.17614906 0.17802878 0.18507142][0.19187281 0.19468828 0.19505754 0.20304725 0.21184856 0.21087141 0.20443186 0.19654958 0.18829718 0.17464219 0.15576145 0.14404453 0.14301758 0.15120204 0.16163963][0.17521772 0.18220167 0.18476194 0.19256711 0.20067744 0.20112278 0.19555888 0.1861594 0.17371748 0.15511423 0.13334733 0.12296456 0.12737718 0.14163829 0.15601663][0.18015763 0.19146807 0.19511047 0.2004474 0.20561165 0.20683481 0.20304795 0.1940235 0.17909728 0.15650383 0.13185315 0.12106217 0.12748373 0.14419302 0.16056383][0.20234424 0.21876311 0.22413042 0.2267572 0.22757809 0.22796531 0.22557849 0.21804269 0.20229653 0.17719007 0.14990498 0.13628134 0.13964331 0.15304294 0.16731073][0.21735032 0.23729736 0.24523409 0.24649541 0.24303898 0.24096166 0.23875801 0.23292896 0.2182357 0.19373655 0.16664797 0.15066445 0.14869475 0.15517077 0.16406596][0.20921308 0.22760588 0.23580983 0.23521534 0.2271947 0.22102833 0.21733308 0.21273482 0.20082478 0.18060154 0.15805501 0.14324105 0.13806711 0.1388393 0.14282624][0.17479582 0.18617411 0.19153732 0.18887092 0.17796311 0.16893314 0.16412263 0.16083625 0.15270241 0.13889064 0.12377527 0.1133573 0.10851666 0.10715604 0.10874078][0.12333757 0.12531476 0.12593587 0.1211708 0.10931578 0.099308535 0.094144285 0.091735534 0.086696021 0.078838058 0.071281485 0.066632427 0.0647485 0.064278051 0.065578178]]...]
INFO - root - 2017-12-11 09:50:55.006141: step 49810, loss = 0.68, batch loss = 0.62 (15.2 examples/sec; 0.527 sec/batch; 41h:21m:35s remains)
INFO - root - 2017-12-11 09:51:00.213631: step 49820, loss = 0.70, batch loss = 0.64 (18.7 examples/sec; 0.428 sec/batch; 33h:35m:33s remains)
INFO - root - 2017-12-11 09:51:05.683905: step 49830, loss = 0.70, batch loss = 0.64 (14.6 examples/sec; 0.547 sec/batch; 42h:57m:26s remains)
INFO - root - 2017-12-11 09:51:11.169994: step 49840, loss = 0.70, batch loss = 0.64 (14.7 examples/sec; 0.542 sec/batch; 42h:35m:29s remains)
INFO - root - 2017-12-11 09:51:16.622148: step 49850, loss = 0.72, batch loss = 0.66 (14.9 examples/sec; 0.536 sec/batch; 42h:07m:04s remains)
INFO - root - 2017-12-11 09:51:22.099153: step 49860, loss = 0.69, batch loss = 0.64 (14.1 examples/sec; 0.566 sec/batch; 44h:24m:29s remains)
INFO - root - 2017-12-11 09:51:27.612130: step 49870, loss = 0.69, batch loss = 0.63 (14.7 examples/sec; 0.543 sec/batch; 42h:38m:55s remains)
INFO - root - 2017-12-11 09:51:33.116929: step 49880, loss = 0.68, batch loss = 0.62 (14.6 examples/sec; 0.546 sec/batch; 42h:53m:38s remains)
INFO - root - 2017-12-11 09:51:38.587827: step 49890, loss = 0.68, batch loss = 0.62 (14.6 examples/sec; 0.549 sec/batch; 43h:05m:02s remains)
INFO - root - 2017-12-11 09:51:44.033130: step 49900, loss = 0.69, batch loss = 0.64 (15.0 examples/sec; 0.535 sec/batch; 41h:57m:45s remains)
2017-12-11 09:51:44.569256: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.33157963 0.34389076 0.32815963 0.28740418 0.22932652 0.17184074 0.12756684 0.10573146 0.11881033 0.17598225 0.2786943 0.40433386 0.5278582 0.6253736 0.66626859][0.25981089 0.28115666 0.25924921 0.19653311 0.1164315 0.053014871 0.029191103 0.050494358 0.11360729 0.21234657 0.33661482 0.45968157 0.55908662 0.61663795 0.61198878][0.20242935 0.22358687 0.19363353 0.1201264 0.037264962 -0.012907937 -0.0063929143 0.053971451 0.15039842 0.265582 0.3830162 0.48015931 0.54438561 0.56791705 0.53664368][0.15114368 0.16062056 0.12431081 0.058080766 0.0015834428 -0.0059737554 0.049652644 0.15086627 0.26501203 0.3667289 0.44160923 0.48320851 0.49869776 0.49145573 0.45057344][0.10246451 0.095028043 0.059680928 0.020292077 0.016456513 0.076184124 0.1963672 0.33999616 0.45762888 0.51663953 0.51341689 0.47071737 0.42401594 0.38741133 0.34915784][0.060069539 0.039753113 0.014021432 0.011951798 0.06949836 0.20075534 0.38356793 0.56134576 0.66861284 0.66721147 0.57034791 0.43471169 0.3254301 0.26380947 0.23393296][0.021407763 -0.0022875825 -0.011856126 0.023286836 0.13460481 0.32407013 0.55204874 0.74560195 0.82922989 0.7651912 0.5852446 0.3750706 0.22044891 0.14414352 0.12401766][-0.0096397558 -0.028717851 -0.021238672 0.042272381 0.18874402 0.4095673 0.65100026 0.83178753 0.87734818 0.75965518 0.528934 0.285406 0.11804713 0.043408114 0.033867549][-0.025627976 -0.037679859 -0.018928217 0.0574925 0.21342264 0.43115458 0.64880747 0.7884196 0.78877765 0.63917494 0.4010008 0.17042506 0.023071812 -0.034159526 -0.032170512][-0.026833773 -0.035090148 -0.016675824 0.0523276 0.18908305 0.3708581 0.53642011 0.6208604 0.58408093 0.43352109 0.22982244 0.04976311 -0.054460857 -0.0856177 -0.072802939][-0.025669206 -0.034792166 -0.025981287 0.019883882 0.11583645 0.24051081 0.34308869 0.37825412 0.32723925 0.20772041 0.067257352 -0.044628419 -0.10019986 -0.10791315 -0.089663438][-0.02603999 -0.036403712 -0.036340017 -0.014903708 0.036734898 0.10304341 0.15038024 0.15392621 0.11096796 0.040158249 -0.029959785 -0.077207267 -0.094419718 -0.091335788 -0.077753954][-0.018276438 -0.028285738 -0.031199526 -0.025431916 -0.0071939491 0.01464426 0.023957945 0.013254849 -0.01027686 -0.032464717 -0.043342061 -0.043160059 -0.0394962 -0.039603 -0.039059747][0.0055455067 -0.0012229071 -0.0007422276 0.00031362343 0.00034453109 -0.0042660655 -0.016422352 -0.030993564 -0.0351776 -0.020655645 0.0080832392 0.035592355 0.045460656 0.034673877 0.019378718][0.047457926 0.045777567 0.053546276 0.057802737 0.051822573 0.034567859 0.011241874 -0.0041014436 0.0046928269 0.040706445 0.089771278 0.12795965 0.13529466 0.11291055 0.084020391]]...]
INFO - root - 2017-12-11 09:51:49.992835: step 49910, loss = 0.70, batch loss = 0.64 (15.2 examples/sec; 0.525 sec/batch; 41h:13m:48s remains)
INFO - root - 2017-12-11 09:51:55.455734: step 49920, loss = 0.69, batch loss = 0.64 (14.7 examples/sec; 0.545 sec/batch; 42h:48m:25s remains)
INFO - root - 2017-12-11 09:52:00.881348: step 49930, loss = 0.70, batch loss = 0.64 (15.0 examples/sec; 0.535 sec/batch; 41h:57m:28s remains)
INFO - root - 2017-12-11 09:52:06.331699: step 49940, loss = 0.70, batch loss = 0.65 (15.2 examples/sec; 0.525 sec/batch; 41h:12m:53s remains)
INFO - root - 2017-12-11 09:52:11.747793: step 49950, loss = 0.70, batch loss = 0.65 (14.6 examples/sec; 0.547 sec/batch; 42h:56m:19s remains)
INFO - root - 2017-12-11 09:52:17.228586: step 49960, loss = 0.71, batch loss = 0.65 (14.4 examples/sec; 0.557 sec/batch; 43h:41m:57s remains)
INFO - root - 2017-12-11 09:52:22.715811: step 49970, loss = 0.69, batch loss = 0.63 (14.9 examples/sec; 0.538 sec/batch; 42h:14m:19s remains)
INFO - root - 2017-12-11 09:52:28.222545: step 49980, loss = 0.69, batch loss = 0.63 (14.0 examples/sec; 0.571 sec/batch; 44h:49m:17s remains)
INFO - root - 2017-12-11 09:52:33.477071: step 49990, loss = 0.69, batch loss = 0.63 (15.1 examples/sec; 0.529 sec/batch; 41h:31m:19s remains)
INFO - root - 2017-12-11 09:52:38.919329: step 50000, loss = 0.69, batch loss = 0.63 (14.6 examples/sec; 0.547 sec/batch; 42h:54m:21s remains)
2017-12-11 09:52:39.571562: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.058500297 -0.039319858 -0.0084708389 0.025085591 0.053789943 0.076525107 0.087811813 0.082857743 0.061663996 0.034528285 0.013276469 -0.0026089069 -0.01458978 -0.024162287 -0.028676461][-0.050513048 -0.02597191 0.01339032 0.057322361 0.097981349 0.13675497 0.16816105 0.18344453 0.17580552 0.15523897 0.13386144 0.10824921 0.078716449 0.049787313 0.029355539][-0.043119334 -0.014283955 0.033094436 0.088982075 0.14632207 0.20708498 0.26725042 0.31355768 0.32822144 0.31729093 0.29368713 0.25210932 0.19453202 0.13519473 0.091007739][-0.037541598 -0.0049255067 0.050927669 0.12053275 0.19879615 0.28479397 0.3743417 0.45055866 0.48535568 0.48032331 0.44853809 0.38558125 0.29589498 0.2037086 0.134826][-0.02722262 0.013938294 0.084789693 0.1761283 0.2845895 0.39998335 0.51337051 0.60582495 0.64469135 0.63105643 0.57887208 0.48878852 0.36938384 0.25185487 0.16477643][-0.0019053803 0.060185276 0.16113937 0.29065254 0.44268098 0.59300625 0.72421783 0.81650347 0.83958149 0.79955417 0.71371245 0.59183973 0.44623807 0.31076843 0.20821379][0.031996921 0.12086158 0.25843191 0.43117622 0.62689912 0.80811614 0.95014763 1.0349002 1.0373412 0.96911025 0.8529501 0.70764393 0.54554451 0.39733589 0.2766811][0.056380983 0.16679369 0.33199024 0.5344618 0.75658888 0.953381 1.0957618 1.1701534 1.1592201 1.0788522 0.95509523 0.80866742 0.64694208 0.493012 0.35388041][0.065061875 0.18364385 0.35728464 0.56569129 0.78882122 0.97969842 1.1082715 1.1678809 1.1524099 1.0809495 0.97623628 0.85275066 0.70954227 0.56100196 0.41075486][0.05784012 0.17080745 0.33249179 0.52160537 0.71705437 0.87522233 0.97085208 1.0070416 0.99032515 0.94141895 0.87426972 0.79245061 0.68480641 0.55770177 0.41334671][0.032447312 0.12679639 0.26006263 0.41113719 0.55911243 0.66867739 0.72310829 0.73379689 0.7164799 0.69075572 0.66220367 0.62508011 0.559845 0.46675929 0.34683618][-0.015667351 0.046449039 0.13661921 0.23628654 0.32815829 0.38809535 0.40737563 0.40107274 0.38706774 0.3802838 0.38207218 0.38145828 0.35593933 0.30165383 0.21838856][-0.067594782 -0.041133028 0.00360561 0.052475005 0.09426029 0.11483381 0.11022971 0.094208382 0.08225023 0.08527077 0.10378725 0.12595402 0.13099411 0.11269691 0.068835229][-0.098878093 -0.097634725 -0.085651949 -0.072559908 -0.064082794 -0.068076544 -0.086013094 -0.10749123 -0.1205063 -0.11603025 -0.093203813 -0.06206755 -0.039650287 -0.03320143 -0.046799537][-0.1101005 -0.1213883 -0.12654676 -0.13192049 -0.13996263 -0.15479049 -0.17654331 -0.1981985 -0.21123268 -0.20794828 -0.18745485 -0.15743653 -0.13061309 -0.11396921 -0.1111335]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-SGD-lr0.001-clip50-initfromfixqian30k-fixqian/model.ckpt-50000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-SGD-lr0.001-clip50-initfromfixqian30k-fixqian/model.ckpt-50000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-11 09:52:45.660364: step 50010, loss = 0.70, batch loss = 0.64 (14.4 examples/sec; 0.557 sec/batch; 43h:44m:02s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-SGD-lr0.001-clip50-initfromfixqian30k-fixqian
INFO - root - 2017-12-11 09:52:51.176648: step 50020, loss = 0.69, batch loss = 0.64 (13.5 examples/sec; 0.593 sec/batch; 46h:30m:03s remains)
INFO - root - 2017-12-11 09:52:56.682851: step 50030, loss = 0.70, batch loss = 0.64 (14.6 examples/sec; 0.548 sec/batch; 43h:01m:59s remains)
INFO - root - 2017-12-11 09:53:02.196254: step 50040, loss = 0.69, batch loss = 0.63 (14.9 examples/sec; 0.538 sec/batch; 42h:11m:42s remains)
INFO - root - 2017-12-11 09:53:07.722428: step 50050, loss = 0.69, batch loss = 0.63 (14.3 examples/sec; 0.561 sec/batch; 44h:01m:09s remains)
INFO - root - 2017-12-11 09:53:13.196315: step 50060, loss = 0.70, batch loss = 0.64 (15.0 examples/sec; 0.534 sec/batch; 41h:54m:34s remains)
INFO - root - 2017-12-11 09:53:18.384984: step 50070, loss = 0.69, batch loss = 0.63 (14.4 examples/sec; 0.556 sec/batch; 43h:36m:19s remains)
INFO - root - 2017-12-11 09:53:23.851314: step 50080, loss = 0.69, batch loss = 0.63 (15.1 examples/sec; 0.529 sec/batch; 41h:31m:00s remains)
INFO - root - 2017-12-11 09:53:29.308977: step 50090, loss = 0.70, batch loss = 0.64 (15.0 examples/sec; 0.532 sec/batch; 41h:42m:22s remains)
INFO - root - 2017-12-11 09:53:34.782648: step 50100, loss = 0.69, batch loss = 0.63 (14.8 examples/sec; 0.541 sec/batch; 42h:24m:42s remains)
2017-12-11 09:53:35.368872: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.25807893 0.29613161 0.28857952 0.23588121 0.16490528 0.11361066 0.10636488 0.14456302 0.20822389 0.26399204 0.29161835 0.29831055 0.29552731 0.29195204 0.28999841][0.24816424 0.27229342 0.25338706 0.1976424 0.13427316 0.0996087 0.11296062 0.16951445 0.24596711 0.3089121 0.33963147 0.345228 0.33709955 0.32439342 0.313272][0.23647392 0.24115339 0.20973057 0.15414858 0.10352986 0.089533716 0.12302338 0.19209926 0.27179953 0.3331435 0.36210251 0.36521026 0.35185176 0.33022884 0.30960843][0.25058886 0.24059227 0.20104262 0.14932184 0.1116254 0.11423083 0.15989256 0.23228021 0.30710667 0.36084425 0.38419747 0.3824684 0.363178 0.33366972 0.30545962][0.28435853 0.26674673 0.22489382 0.17966206 0.1522996 0.16340792 0.20995286 0.27460676 0.33642042 0.37786713 0.39404416 0.38795161 0.36570826 0.33391756 0.304952][0.32419807 0.3056291 0.26676893 0.22908959 0.20802984 0.2190918 0.25699255 0.30648777 0.35102913 0.37868071 0.3890107 0.38299307 0.36437833 0.33707222 0.312614][0.34790123 0.33336189 0.30070776 0.27097058 0.25505456 0.26540944 0.29631883 0.33473766 0.36643279 0.38241783 0.38637739 0.37928921 0.36379555 0.34032935 0.31853524][0.35124105 0.3418937 0.31478041 0.2913779 0.28154469 0.29642159 0.32946634 0.36748439 0.39432278 0.40036964 0.39268503 0.3762472 0.35591576 0.33026949 0.30679804][0.33835962 0.33628553 0.31618872 0.29993221 0.29767096 0.31990758 0.35923165 0.40118703 0.42585731 0.4211641 0.39544874 0.35919112 0.32230672 0.28520909 0.25469521][0.33693632 0.34144992 0.32808024 0.31794673 0.32137322 0.34709108 0.3874394 0.42787954 0.44581816 0.42752004 0.38056064 0.3193005 0.25991216 0.2070144 0.16759259][0.36265877 0.37108713 0.35988095 0.35178941 0.35678175 0.38020483 0.41411185 0.44550583 0.4516843 0.41852391 0.35139734 0.26663309 0.1853976 0.11730342 0.070139483][0.40443981 0.41201907 0.39677951 0.3850252 0.38690218 0.40411529 0.42783213 0.4468694 0.44049624 0.39528537 0.31396985 0.21250309 0.11490395 0.035672341 -0.016466904][0.44840378 0.44897413 0.42285022 0.40103263 0.39508736 0.40319777 0.41470242 0.41937336 0.40032911 0.3473025 0.26146862 0.1567432 0.05579726 -0.024809083 -0.076036684][0.45956019 0.45223239 0.41467738 0.3811017 0.36395612 0.35878518 0.35434657 0.34231782 0.31242386 0.25984627 0.18464424 0.095765986 0.0095423432 -0.059246331 -0.10199646][0.41040489 0.39481667 0.34812692 0.30387148 0.27505231 0.25680885 0.23915157 0.215775 0.18321387 0.14163424 0.090216257 0.032310143 -0.024494031 -0.070097655 -0.097779639]]...]
INFO - root - 2017-12-11 09:53:40.795082: step 50110, loss = 0.71, batch loss = 0.65 (14.4 examples/sec; 0.556 sec/batch; 43h:36m:24s remains)
INFO - root - 2017-12-11 09:53:46.257338: step 50120, loss = 0.68, batch loss = 0.62 (14.8 examples/sec; 0.540 sec/batch; 42h:22m:24s remains)
INFO - root - 2017-12-11 09:53:51.697994: step 50130, loss = 0.69, batch loss = 0.63 (14.6 examples/sec; 0.547 sec/batch; 42h:53m:14s remains)
INFO - root - 2017-12-11 09:53:57.231566: step 50140, loss = 0.70, batch loss = 0.64 (14.4 examples/sec; 0.555 sec/batch; 43h:33m:56s remains)
INFO - root - 2017-12-11 09:54:02.492048: step 50150, loss = 0.72, batch loss = 0.66 (15.2 examples/sec; 0.526 sec/batch; 41h:16m:22s remains)
INFO - root - 2017-12-11 09:54:07.947834: step 50160, loss = 0.70, batch loss = 0.64 (14.6 examples/sec; 0.548 sec/batch; 42h:58m:55s remains)
INFO - root - 2017-12-11 09:54:13.435746: step 50170, loss = 0.70, batch loss = 0.64 (14.7 examples/sec; 0.546 sec/batch; 42h:48m:27s remains)
INFO - root - 2017-12-11 09:54:18.928243: step 50180, loss = 0.70, batch loss = 0.64 (15.1 examples/sec; 0.530 sec/batch; 41h:32m:09s remains)
INFO - root - 2017-12-11 09:54:24.388928: step 50190, loss = 0.69, batch loss = 0.63 (14.8 examples/sec; 0.539 sec/batch; 42h:16m:29s remains)
INFO - root - 2017-12-11 09:54:29.848369: step 50200, loss = 0.70, batch loss = 0.64 (14.5 examples/sec; 0.553 sec/batch; 43h:22m:30s remains)
2017-12-11 09:54:30.467503: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.014394439 -0.021683225 -0.024442842 -0.019867498 -0.010314098 -0.000693116 0.0035730584 -0.0010668042 -0.01416553 -0.030538846 -0.042577233 -0.045318641 -0.039029151 -0.02679394 -0.013343083][0.0051395684 0.0029703381 0.010073944 0.030277306 0.058608994 0.085784607 0.10096438 0.09639205 0.072177939 0.038268298 0.0083323177 -0.0069861989 -0.0053373063 0.0082838656 0.026276402][0.025321169 0.033280335 0.056755904 0.10119483 0.15836039 0.21317875 0.2467218 0.24483256 0.206135 0.14535622 0.084444106 0.0434671 0.031207066 0.041641518 0.063957][0.045218676 0.066121861 0.10974704 0.18422972 0.27830318 0.37042981 0.43128988 0.438091 0.38683695 0.29585004 0.19469664 0.11496611 0.075312994 0.071668573 0.090348184][0.058661334 0.096636318 0.16386777 0.27140573 0.40583676 0.53907037 0.62954861 0.64565945 0.58225 0.46214664 0.32137117 0.20159437 0.13173527 0.10985562 0.11772566][0.056712512 0.11365709 0.20613441 0.3470242 0.52088 0.69218194 0.80707657 0.82801741 0.75270891 0.61126584 0.44446823 0.29929644 0.20994374 0.17295419 0.16471918][0.036322191 0.10468251 0.21506792 0.38156 0.58736628 0.78997189 0.92491645 0.95053071 0.87059355 0.72366697 0.55341762 0.40512776 0.31077534 0.26271856 0.23369116][0.011046845 0.077597171 0.18902113 0.36149451 0.57926381 0.79636347 0.94288051 0.97581595 0.90404093 0.77276808 0.62674797 0.50264788 0.4217101 0.36956185 0.31928557][0.00039373781 0.053220477 0.14739446 0.30142662 0.50281346 0.70762014 0.84837186 0.88508403 0.83078152 0.73404604 0.63709539 0.56196511 0.51320684 0.46854731 0.40319374][0.018735902 0.0480591 0.10901174 0.22245604 0.38043776 0.54559636 0.660361 0.692956 0.659293 0.60602677 0.56919378 0.55458629 0.54899663 0.52281523 0.45474038][0.079959363 0.078363448 0.091260381 0.1441644 0.23667851 0.34257796 0.41853565 0.44174671 0.4278532 0.41561142 0.4323774 0.47151503 0.50727922 0.50561821 0.44941834][0.1954741 0.16085932 0.11554154 0.093674459 0.10749685 0.14609763 0.17993635 0.19118279 0.19115898 0.20771204 0.25923622 0.33272997 0.39789423 0.42079073 0.38733202][0.36710268 0.30687752 0.2061739 0.10846844 0.041642852 0.012688368 0.0036710149 -0.0023004112 -0.00086441619 0.025585378 0.087714665 0.17225993 0.25147954 0.29464331 0.28866181][0.57137066 0.49720216 0.35561273 0.19791725 0.065642916 -0.018509645 -0.0651406 -0.093565114 -0.10592602 -0.091142841 -0.043132767 0.029020939 0.1049426 0.15926056 0.17786194][0.766028 0.68788075 0.52435732 0.33332345 0.16421744 0.048315492 -0.022659006 -0.071713746 -0.1063274 -0.11833508 -0.10266459 -0.060915012 -0.0038117296 0.049233004 0.08305265]]...]
INFO - root - 2017-12-11 09:54:35.887791: step 50210, loss = 0.69, batch loss = 0.63 (15.1 examples/sec; 0.531 sec/batch; 41h:40m:09s remains)
INFO - root - 2017-12-11 09:54:41.398363: step 50220, loss = 0.69, batch loss = 0.64 (14.8 examples/sec; 0.539 sec/batch; 42h:15m:12s remains)
INFO - root - 2017-12-11 09:54:46.661533: step 50230, loss = 0.68, batch loss = 0.63 (14.7 examples/sec; 0.545 sec/batch; 42h:42m:17s remains)
INFO - root - 2017-12-11 09:54:52.152576: step 50240, loss = 0.68, batch loss = 0.62 (14.8 examples/sec; 0.540 sec/batch; 42h:19m:02s remains)
INFO - root - 2017-12-11 09:54:57.640439: step 50250, loss = 0.70, batch loss = 0.64 (15.1 examples/sec; 0.531 sec/batch; 41h:35m:51s remains)
INFO - root - 2017-12-11 09:55:03.170552: step 50260, loss = 0.69, batch loss = 0.63 (14.0 examples/sec; 0.573 sec/batch; 44h:55m:11s remains)
INFO - root - 2017-12-11 09:55:08.652339: step 50270, loss = 0.71, batch loss = 0.65 (15.2 examples/sec; 0.528 sec/batch; 41h:22m:15s remains)
INFO - root - 2017-12-11 09:55:14.080861: step 50280, loss = 0.67, batch loss = 0.61 (14.1 examples/sec; 0.566 sec/batch; 44h:22m:34s remains)
INFO - root - 2017-12-11 09:55:19.576661: step 50290, loss = 0.70, batch loss = 0.64 (14.2 examples/sec; 0.565 sec/batch; 44h:15m:10s remains)
INFO - root - 2017-12-11 09:55:25.050579: step 50300, loss = 0.70, batch loss = 0.64 (14.6 examples/sec; 0.548 sec/batch; 42h:55m:46s remains)
2017-12-11 09:55:25.666657: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.26257545 0.21533942 0.15532067 0.11541679 0.10745358 0.12684093 0.15942405 0.19027619 0.20626676 0.20571762 0.18951239 0.16305861 0.13002309 0.096198976 0.063307136][0.30847585 0.26083741 0.20031355 0.16573231 0.16876671 0.20080493 0.24508935 0.28531343 0.30679393 0.30516812 0.27722278 0.23059754 0.17546989 0.12279695 0.075782657][0.333691 0.29538247 0.24701439 0.22507364 0.24112412 0.28439113 0.33696559 0.38120729 0.40340033 0.39855909 0.35858434 0.29170978 0.21408606 0.14129303 0.078039587][0.36139876 0.34212288 0.31073096 0.30019328 0.32493079 0.37412804 0.42889449 0.4693796 0.48615873 0.47619092 0.42752686 0.34756711 0.25490534 0.16628529 0.086511493][0.42020169 0.42826241 0.41376925 0.40820256 0.4332051 0.4807013 0.52985257 0.558363 0.56339121 0.54626811 0.49363998 0.4109582 0.313482 0.2127912 0.11317808][0.50563735 0.54159451 0.54212612 0.53745294 0.55700564 0.59723777 0.63402843 0.64452934 0.6330899 0.60613877 0.55331481 0.47750917 0.38489723 0.27439678 0.15218478][0.59287846 0.64701074 0.65460694 0.64786333 0.66027862 0.69236845 0.71756411 0.71418059 0.69075757 0.65607643 0.60545766 0.540146 0.45522752 0.33517653 0.19106159][0.66429126 0.71460414 0.71590877 0.7049855 0.71145761 0.73635429 0.75350225 0.746173 0.72356832 0.68994886 0.64493442 0.59019762 0.51339006 0.38782465 0.22796732][0.68630844 0.71521246 0.70238888 0.68697584 0.68666285 0.70120317 0.70933574 0.70414531 0.69390106 0.67308187 0.64141953 0.60085785 0.53838575 0.42052674 0.25983948][0.6519081 0.65506327 0.62856823 0.61019987 0.60261625 0.60437793 0.60161626 0.60036075 0.60639185 0.60245574 0.58498621 0.5568257 0.51011127 0.40865773 0.26041389][0.57528651 0.56015545 0.52337623 0.50294 0.48843458 0.477691 0.46426016 0.46529296 0.48493966 0.49680942 0.492796 0.4761588 0.44506446 0.36311248 0.2334896][0.46774441 0.43926075 0.39644906 0.37569651 0.35861117 0.34203416 0.32433659 0.32785258 0.3567636 0.38056609 0.38818437 0.3817845 0.36341932 0.29908118 0.18891001][0.34096387 0.30860713 0.26759705 0.24847965 0.23155619 0.21378538 0.19570373 0.19908144 0.22995442 0.25963163 0.27563265 0.27669269 0.26677015 0.21714815 0.12680477][0.20690793 0.17808969 0.14390109 0.12589304 0.10855456 0.090302438 0.072566353 0.073768735 0.10084618 0.12997949 0.1483099 0.15155737 0.14508624 0.10767942 0.038625397][0.079983868 0.057519224 0.032575727 0.016980803 0.0012214535 -0.014839306 -0.029680008 -0.028992098 -0.0073235477 0.016267076 0.029459158 0.027808547 0.018186102 -0.011824746 -0.061551437]]...]
INFO - root - 2017-12-11 09:55:30.896737: step 50310, loss = 0.69, batch loss = 0.63 (14.3 examples/sec; 0.559 sec/batch; 43h:50m:46s remains)
INFO - root - 2017-12-11 09:55:36.312303: step 50320, loss = 0.70, batch loss = 0.64 (14.9 examples/sec; 0.537 sec/batch; 42h:04m:35s remains)
INFO - root - 2017-12-11 09:55:41.811260: step 50330, loss = 0.69, batch loss = 0.63 (14.2 examples/sec; 0.565 sec/batch; 44h:14m:47s remains)
INFO - root - 2017-12-11 09:55:47.276392: step 50340, loss = 0.71, batch loss = 0.65 (14.4 examples/sec; 0.556 sec/batch; 43h:34m:28s remains)
INFO - root - 2017-12-11 09:55:52.791405: step 50350, loss = 0.68, batch loss = 0.62 (14.1 examples/sec; 0.566 sec/batch; 44h:19m:45s remains)
INFO - root - 2017-12-11 09:55:58.246255: step 50360, loss = 0.69, batch loss = 0.63 (14.8 examples/sec; 0.540 sec/batch; 42h:20m:07s remains)
INFO - root - 2017-12-11 09:56:03.740645: step 50370, loss = 0.70, batch loss = 0.65 (14.3 examples/sec; 0.561 sec/batch; 43h:58m:47s remains)
INFO - root - 2017-12-11 09:56:09.223763: step 50380, loss = 0.69, batch loss = 0.63 (14.4 examples/sec; 0.556 sec/batch; 43h:34m:03s remains)
INFO - root - 2017-12-11 09:56:14.446755: step 50390, loss = 0.71, batch loss = 0.65 (15.2 examples/sec; 0.527 sec/batch; 41h:17m:12s remains)
INFO - root - 2017-12-11 09:56:19.891936: step 50400, loss = 0.69, batch loss = 0.63 (14.6 examples/sec; 0.549 sec/batch; 43h:00m:29s remains)
2017-12-11 09:56:20.467789: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.31496003 0.31967741 0.31004861 0.3000713 0.29352573 0.28523979 0.27325952 0.25662854 0.24201865 0.23479877 0.23845485 0.25393316 0.27612787 0.30191633 0.326253][0.33854309 0.34157804 0.32841438 0.31303775 0.3014856 0.29005435 0.27702191 0.26005465 0.244768 0.23670164 0.23907787 0.25254893 0.2731843 0.29867926 0.32473543][0.33056355 0.33199486 0.31747648 0.30061078 0.28896648 0.27966565 0.27000397 0.25558966 0.24021475 0.22934498 0.22732295 0.23582348 0.25239727 0.27550107 0.30199656][0.3057808 0.3074069 0.29754031 0.28850695 0.28759131 0.29049909 0.29097694 0.28159547 0.26389465 0.2438412 0.22823848 0.22225684 0.22748946 0.24398316 0.26867598][0.27767354 0.28406098 0.28669161 0.29569533 0.31604952 0.33956516 0.35448965 0.3502284 0.32641962 0.29028937 0.2517637 0.22155774 0.20713145 0.21057233 0.22847188][0.25263739 0.26957831 0.29239798 0.32813117 0.37679633 0.42504385 0.45586979 0.4562811 0.42430106 0.36824277 0.30181628 0.24284516 0.2043 0.18941972 0.19482876][0.22988999 0.26162365 0.30786613 0.37186444 0.4471083 0.51659214 0.5617404 0.5688 0.53321826 0.46105894 0.36935887 0.28292626 0.21838485 0.18027353 0.16711378][0.209051 0.257436 0.32307807 0.4064467 0.4961682 0.57478505 0.62756437 0.64259946 0.61226839 0.5364368 0.43130586 0.32551324 0.23763801 0.17518415 0.14126691][0.19214405 0.25502935 0.33059153 0.41742617 0.50294292 0.57398528 0.6252144 0.64919692 0.63477117 0.57324159 0.47463736 0.36595565 0.26509815 0.18364811 0.13217585][0.17597187 0.24393959 0.31517738 0.38827249 0.45331916 0.50420016 0.54681396 0.5786745 0.58595824 0.55175823 0.47771895 0.38465104 0.28700575 0.19963281 0.14054053][0.16293122 0.22023658 0.27135327 0.317731 0.35481289 0.38312894 0.41630903 0.4556877 0.48682404 0.48704958 0.44894382 0.38491231 0.30545649 0.22669773 0.17101179][0.16243358 0.19467039 0.21481492 0.22980955 0.24158683 0.25433919 0.28354055 0.33224306 0.38649619 0.42045727 0.41931021 0.387518 0.33197176 0.26792377 0.21926454][0.18464325 0.1851525 0.17255451 0.16102469 0.15694962 0.16374876 0.19315305 0.24852233 0.31772074 0.37459993 0.40078926 0.39477623 0.36112422 0.31230679 0.27032661][0.21935044 0.1926261 0.15561229 0.1291341 0.12079475 0.12971051 0.16095637 0.21759355 0.29043269 0.35525239 0.39435086 0.40282226 0.38454893 0.348464 0.31245241][0.24980223 0.20945053 0.16316971 0.13567941 0.133015 0.14864814 0.18095833 0.23312376 0.29912177 0.35765973 0.39455438 0.4053669 0.3955195 0.37010932 0.34081289]]...]
INFO - root - 2017-12-11 09:56:25.876503: step 50410, loss = 0.70, batch loss = 0.65 (14.7 examples/sec; 0.543 sec/batch; 42h:30m:37s remains)
INFO - root - 2017-12-11 09:56:31.368213: step 50420, loss = 0.70, batch loss = 0.65 (14.7 examples/sec; 0.544 sec/batch; 42h:38m:30s remains)
INFO - root - 2017-12-11 09:56:35.620387: step 50430, loss = 0.71, batch loss = 0.65 (30.3 examples/sec; 0.264 sec/batch; 20h:41m:14s remains)
INFO - root - 2017-12-11 09:56:38.289957: step 50440, loss = 0.70, batch loss = 0.64 (30.3 examples/sec; 0.264 sec/batch; 20h:40m:05s remains)
INFO - root - 2017-12-11 09:56:40.934709: step 50450, loss = 0.69, batch loss = 0.63 (31.1 examples/sec; 0.257 sec/batch; 20h:08m:05s remains)
INFO - root - 2017-12-11 09:56:43.623310: step 50460, loss = 0.70, batch loss = 0.65 (29.1 examples/sec; 0.275 sec/batch; 21h:34m:12s remains)
INFO - root - 2017-12-11 09:56:46.281546: step 50470, loss = 0.69, batch loss = 0.64 (30.4 examples/sec; 0.263 sec/batch; 20h:37m:55s remains)
INFO - root - 2017-12-11 09:56:48.892205: step 50480, loss = 0.70, batch loss = 0.65 (30.0 examples/sec; 0.266 sec/batch; 20h:51m:33s remains)
INFO - root - 2017-12-11 09:56:51.461188: step 50490, loss = 0.71, batch loss = 0.65 (31.7 examples/sec; 0.252 sec/batch; 19h:45m:28s remains)
INFO - root - 2017-12-11 09:56:54.085724: step 50500, loss = 0.70, batch loss = 0.65 (30.4 examples/sec; 0.264 sec/batch; 20h:38m:35s remains)
2017-12-11 09:56:54.449834: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.15152843 0.17641656 0.17586824 0.16073149 0.15594126 0.17085735 0.20267743 0.23905326 0.26193258 0.26243386 0.23940547 0.21251926 0.20085984 0.2063275 0.22093619][0.13253446 0.17665996 0.20125392 0.212471 0.23221342 0.26539633 0.30832309 0.34701622 0.36194509 0.34429574 0.29750332 0.24906874 0.22100969 0.2161732 0.22475106][0.090060748 0.14762169 0.19684026 0.23875551 0.28975448 0.347649 0.40561086 0.44672525 0.45083582 0.41061991 0.33420515 0.25735262 0.20666413 0.18814372 0.19127314][0.041118089 0.10248418 0.16867131 0.23691487 0.31795955 0.40249181 0.47883692 0.5264504 0.5236724 0.46396676 0.35863134 0.24996136 0.171094 0.13395518 0.12952958][-0.0019159394 0.055848286 0.13016526 0.21640399 0.31999791 0.42558363 0.51640695 0.5688495 0.56010312 0.48524216 0.35701361 0.22105061 0.11633457 0.061975207 0.051874369][-0.031620547 0.020027291 0.096925437 0.19439106 0.31338552 0.43381679 0.53344524 0.58700591 0.57174438 0.4850682 0.34078413 0.18471648 0.059206739 -0.011133759 -0.027670503][-0.04693047 0.00018847657 0.078354217 0.18393585 0.31420958 0.44597158 0.55204982 0.60693842 0.589138 0.49762127 0.34627578 0.17793573 0.034997452 -0.055033464 -0.08718615][-0.051998317 -0.0064676362 0.073596314 0.18561597 0.32403296 0.46436909 0.57656682 0.63585633 0.62254286 0.53598475 0.38958815 0.22040473 0.0666277 -0.044802409 -0.10282782][-0.053179186 -0.0078642657 0.073612057 0.18867186 0.33001223 0.4743239 0.59181529 0.65870655 0.65689749 0.58517569 0.45635039 0.30012769 0.1469408 0.020283654 -0.064621828][-0.056855381 -0.013286492 0.066557407 0.17924467 0.31586915 0.45551607 0.57219428 0.64522892 0.65895003 0.61089271 0.51251966 0.3852056 0.24905294 0.12104263 0.017831055][-0.065732412 -0.028552065 0.043563541 0.14685233 0.27169421 0.39995465 0.51085413 0.58796585 0.61950403 0.6025455 0.54541224 0.45990646 0.35522592 0.24098606 0.13207588][-0.077662617 -0.051718492 0.006092682 0.093254693 0.20127116 0.31497839 0.41829184 0.49858823 0.54790545 0.56412607 0.55224508 0.51489836 0.45124051 0.3643741 0.26545376][-0.088092767 -0.074106023 -0.033011492 0.034307979 0.12159143 0.21676755 0.30778077 0.38578674 0.44635382 0.48921716 0.51697421 0.52659106 0.51026648 0.46446329 0.39411104][-0.094737113 -0.089822911 -0.063109808 -0.014756653 0.050455004 0.12307765 0.19445619 0.25978243 0.31859931 0.37395075 0.42828998 0.47680292 0.5085535 0.51363122 0.48865792][-0.097678974 -0.098312125 -0.081418477 -0.047436289 -0.0010567018 0.049320612 0.097014189 0.14075533 0.18410666 0.23481855 0.29810935 0.37108284 0.44264156 0.49690649 0.52232939]]...]
INFO - root - 2017-12-11 09:56:57.069325: step 50510, loss = 0.71, batch loss = 0.65 (31.5 examples/sec; 0.254 sec/batch; 19h:54m:52s remains)
INFO - root - 2017-12-11 09:56:59.711210: step 50520, loss = 0.68, batch loss = 0.63 (29.5 examples/sec; 0.271 sec/batch; 21h:15m:07s remains)
INFO - root - 2017-12-11 09:57:02.364835: step 50530, loss = 0.71, batch loss = 0.65 (30.9 examples/sec; 0.259 sec/batch; 20h:14m:50s remains)
INFO - root - 2017-12-11 09:57:05.041037: step 50540, loss = 0.69, batch loss = 0.64 (29.9 examples/sec; 0.267 sec/batch; 20h:55m:49s remains)
INFO - root - 2017-12-11 09:57:07.710693: step 50550, loss = 0.69, batch loss = 0.64 (30.1 examples/sec; 0.265 sec/batch; 20h:47m:21s remains)
INFO - root - 2017-12-11 09:57:10.333703: step 50560, loss = 0.70, batch loss = 0.64 (29.9 examples/sec; 0.268 sec/batch; 20h:57m:19s remains)
INFO - root - 2017-12-11 09:57:12.896533: step 50570, loss = 0.69, batch loss = 0.64 (31.8 examples/sec; 0.251 sec/batch; 19h:40m:42s remains)
INFO - root - 2017-12-11 09:57:15.588768: step 50580, loss = 0.71, batch loss = 0.65 (28.3 examples/sec; 0.282 sec/batch; 22h:06m:45s remains)
INFO - root - 2017-12-11 09:57:18.290747: step 50590, loss = 0.71, batch loss = 0.65 (27.3 examples/sec; 0.293 sec/batch; 22h:54m:24s remains)
INFO - root - 2017-12-11 09:57:20.949177: step 50600, loss = 0.69, batch loss = 0.63 (29.5 examples/sec; 0.271 sec/batch; 21h:12m:43s remains)
2017-12-11 09:57:21.345622: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.008142842 0.0069088568 0.0082139233 0.011572701 0.015171727 0.015961275 0.013268532 0.010094806 0.0081916275 0.007513307 0.0085324217 0.011947026 0.015684262 0.017509442 0.017692743][0.01997135 0.02067462 0.023371391 0.027579172 0.029727815 0.026279123 0.018334402 0.011108125 0.0072154989 0.0064635728 0.0097288229 0.017285174 0.025015615 0.028940108 0.029712565][0.036480449 0.038740695 0.041379854 0.044593394 0.044313561 0.037644923 0.02725124 0.0192069 0.015966624 0.016341273 0.021370448 0.031301077 0.040967278 0.045995355 0.047063172][0.054136958 0.056872424 0.057672303 0.057590868 0.05373643 0.045241654 0.035366755 0.029339334 0.029001595 0.032127768 0.039675914 0.051761013 0.063075714 0.070275277 0.073367357][0.076129563 0.078588061 0.07596866 0.070717379 0.062847659 0.05423883 0.047387309 0.045273513 0.049227361 0.056689214 0.068130173 0.0831372 0.096537389 0.10718223 0.11363526][0.10561509 0.10702463 0.10013477 0.089502245 0.079081781 0.073249824 0.072402179 0.076062471 0.085114107 0.097503394 0.11297875 0.1303812 0.14469895 0.15762897 0.16629575][0.13922217 0.13914597 0.1286079 0.11478816 0.1049449 0.10449994 0.11096475 0.12010704 0.13272789 0.14811319 0.16528249 0.18264997 0.19536591 0.20800145 0.21672712][0.16546834 0.16303268 0.15001832 0.13615175 0.12985544 0.13572028 0.1484715 0.16074899 0.1734602 0.18740807 0.20130368 0.21430212 0.22246468 0.23260473 0.24005884][0.16855015 0.16237798 0.14790912 0.13669334 0.13584217 0.14744993 0.16447255 0.17747447 0.18733357 0.19552556 0.20113482 0.20551302 0.20708144 0.21386509 0.22044235][0.14566901 0.13386025 0.11789555 0.11027345 0.11494333 0.13029017 0.14886798 0.16071846 0.16613452 0.16616116 0.1608153 0.15471666 0.1498692 0.15409671 0.16157806][0.11226534 0.093680695 0.074418269 0.06787198 0.074670933 0.0896377 0.10604119 0.11512478 0.11619181 0.10909426 0.095026061 0.08153978 0.073799 0.077539034 0.087052405][0.092944659 0.068109557 0.042950533 0.032633692 0.035900727 0.045450486 0.055845421 0.060800746 0.058774211 0.048149958 0.030923516 0.015974099 0.0099584106 0.01523226 0.026439011][0.10277069 0.07308545 0.039720174 0.020595167 0.014819658 0.015042488 0.017188646 0.017759113 0.014658729 0.0048058406 -0.0099474536 -0.021320086 -0.022869928 -0.015540459 -0.0042751161][0.13868079 0.10660154 0.064212024 0.033064056 0.01480385 0.0039793015 -0.0023123322 -0.0054474319 -0.007761511 -0.013622474 -0.022332845 -0.027603475 -0.024206568 -0.015392991 -0.005686501][0.18466707 0.15335295 0.10369834 0.06021725 0.028612295 0.0070244679 -0.0064612222 -0.012456562 -0.013347756 -0.01428134 -0.016307956 -0.015782118 -0.0090141343 -0.00016490174 0.0068920827]]...]
INFO - root - 2017-12-11 09:57:23.973428: step 50610, loss = 0.69, batch loss = 0.63 (31.3 examples/sec; 0.256 sec/batch; 20h:02m:19s remains)
INFO - root - 2017-12-11 09:57:26.594038: step 50620, loss = 0.69, batch loss = 0.64 (29.9 examples/sec; 0.268 sec/batch; 20h:57m:53s remains)
INFO - root - 2017-12-11 09:57:29.248117: step 50630, loss = 0.68, batch loss = 0.63 (31.2 examples/sec; 0.256 sec/batch; 20h:04m:43s remains)
INFO - root - 2017-12-11 09:57:31.885290: step 50640, loss = 0.72, batch loss = 0.66 (30.8 examples/sec; 0.259 sec/batch; 20h:18m:25s remains)
INFO - root - 2017-12-11 09:57:34.534218: step 50650, loss = 0.69, batch loss = 0.63 (31.5 examples/sec; 0.254 sec/batch; 19h:51m:21s remains)
INFO - root - 2017-12-11 09:57:37.218071: step 50660, loss = 0.70, batch loss = 0.64 (29.5 examples/sec; 0.271 sec/batch; 21h:13m:17s remains)
INFO - root - 2017-12-11 09:57:39.834010: step 50670, loss = 0.70, batch loss = 0.64 (30.4 examples/sec; 0.263 sec/batch; 20h:34m:25s remains)
INFO - root - 2017-12-11 09:57:42.469777: step 50680, loss = 0.71, batch loss = 0.65 (30.3 examples/sec; 0.264 sec/batch; 20h:41m:57s remains)
INFO - root - 2017-12-11 09:57:45.105264: step 50690, loss = 0.69, batch loss = 0.63 (30.7 examples/sec; 0.261 sec/batch; 20h:23m:52s remains)
INFO - root - 2017-12-11 09:57:47.769581: step 50700, loss = 0.68, batch loss = 0.62 (29.6 examples/sec; 0.270 sec/batch; 21h:08m:13s remains)
2017-12-11 09:57:48.128728: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.14880563 0.16724345 0.18484502 0.18913265 0.17631279 0.15063831 0.11854026 0.089750923 0.07615523 0.083888374 0.10574601 0.12827434 0.14462961 0.14992782 0.13977817][0.1679197 0.18247651 0.19258894 0.1904963 0.17400281 0.14779991 0.11793704 0.093691729 0.085293792 0.095719844 0.11719556 0.1362181 0.14815407 0.15061446 0.13949738][0.19038112 0.20080069 0.20433559 0.19716582 0.17787735 0.15107478 0.12345609 0.1034971 0.099380709 0.11154607 0.13264292 0.14931397 0.15730192 0.1563326 0.14318857][0.21296933 0.22013928 0.21837261 0.20762524 0.18612704 0.15850642 0.13259907 0.11618908 0.11549331 0.1288452 0.14931223 0.16382372 0.16726966 0.16168021 0.14575875][0.22914484 0.23333482 0.22702119 0.2137091 0.19091991 0.16296048 0.13869441 0.1258826 0.12928736 0.14517556 0.16592707 0.17874093 0.17740811 0.16612335 0.1469266][0.24138066 0.24603517 0.2383789 0.22416231 0.2006003 0.17156431 0.14718142 0.13651414 0.14346083 0.16229303 0.18346326 0.19450724 0.18805677 0.17015885 0.14680658][0.24393304 0.25371873 0.24931838 0.23723361 0.21430714 0.18401743 0.15792741 0.14718793 0.15496959 0.17414515 0.19332294 0.20142916 0.19056681 0.1675161 0.141431][0.2292694 0.24681135 0.250255 0.24478532 0.22639844 0.19722624 0.16990568 0.15757175 0.16260993 0.17773145 0.19154772 0.19580251 0.18289073 0.15875541 0.1336334][0.20577495 0.23093122 0.24336244 0.2469663 0.23648746 0.21199735 0.18593696 0.17261395 0.17304473 0.18117802 0.18779054 0.18810545 0.17573927 0.15454283 0.13348615][0.18712352 0.21512944 0.23255736 0.24272297 0.23980118 0.22179359 0.20000364 0.18820588 0.18577778 0.18876326 0.19156632 0.19197273 0.18441693 0.16971019 0.15370539][0.17492014 0.20152663 0.21866828 0.23080076 0.23217045 0.22000785 0.20387422 0.19608702 0.19489007 0.19871183 0.20497636 0.21192783 0.21384938 0.20845217 0.19716376][0.16347459 0.1850255 0.19803388 0.20899528 0.21262647 0.20657137 0.19787309 0.19620213 0.19941531 0.20869106 0.22341555 0.24055965 0.25378066 0.2585398 0.25138262][0.15653162 0.17224157 0.18022902 0.18874174 0.19369607 0.19417924 0.19371516 0.19812439 0.20595342 0.2217602 0.24634501 0.27376071 0.29639095 0.3084262 0.30270308][0.15652537 0.16822255 0.17240372 0.17776656 0.1821062 0.18713948 0.19344911 0.20301402 0.21530549 0.23809443 0.27297586 0.30945861 0.33750415 0.3511022 0.34256855][0.1615565 0.1724377 0.1750337 0.17725019 0.1791638 0.18576112 0.19629289 0.20966886 0.22541799 0.25343019 0.29579005 0.33749971 0.36584225 0.37515572 0.36071578]]...]
INFO - root - 2017-12-11 09:57:50.697085: step 50710, loss = 0.71, batch loss = 0.65 (30.5 examples/sec; 0.262 sec/batch; 20h:31m:59s remains)
INFO - root - 2017-12-11 09:57:53.355161: step 50720, loss = 0.70, batch loss = 0.64 (30.3 examples/sec; 0.264 sec/batch; 20h:40m:38s remains)
INFO - root - 2017-12-11 09:57:55.976412: step 50730, loss = 0.70, batch loss = 0.64 (28.7 examples/sec; 0.278 sec/batch; 21h:47m:24s remains)
INFO - root - 2017-12-11 09:57:58.613859: step 50740, loss = 0.70, batch loss = 0.64 (30.9 examples/sec; 0.259 sec/batch; 20h:17m:09s remains)
INFO - root - 2017-12-11 09:58:01.240948: step 50750, loss = 0.71, batch loss = 0.65 (30.1 examples/sec; 0.266 sec/batch; 20h:47m:51s remains)
INFO - root - 2017-12-11 09:58:03.811638: step 50760, loss = 0.71, batch loss = 0.65 (31.2 examples/sec; 0.256 sec/batch; 20h:02m:47s remains)
INFO - root - 2017-12-11 09:58:06.455628: step 50770, loss = 0.70, batch loss = 0.64 (29.3 examples/sec; 0.273 sec/batch; 21h:23m:10s remains)
INFO - root - 2017-12-11 09:58:09.078389: step 50780, loss = 0.70, batch loss = 0.64 (31.6 examples/sec; 0.253 sec/batch; 19h:49m:45s remains)
INFO - root - 2017-12-11 09:58:11.752559: step 50790, loss = 0.69, batch loss = 0.64 (30.3 examples/sec; 0.264 sec/batch; 20h:38m:27s remains)
INFO - root - 2017-12-11 09:58:14.369166: step 50800, loss = 0.71, batch loss = 0.65 (30.4 examples/sec; 0.263 sec/batch; 20h:34m:11s remains)
2017-12-11 09:58:14.750161: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.35736117 0.36815992 0.36933497 0.3742786 0.37600747 0.38356742 0.39870316 0.41117072 0.40575907 0.37024441 0.30932918 0.22251631 0.12603919 0.043410983 -0.010049881][0.4216896 0.45271116 0.46495754 0.47256264 0.47299063 0.48298281 0.502705 0.51613712 0.506127 0.45866245 0.38082042 0.27327493 0.15621549 0.057085857 -0.0071406406][0.42666805 0.46846434 0.48442978 0.48923066 0.48669 0.502201 0.53477287 0.55984783 0.5563553 0.50754255 0.42249718 0.30333894 0.17266354 0.061749384 -0.00977945][0.38011637 0.4206031 0.4336302 0.43415549 0.43158051 0.45850602 0.51405084 0.56304413 0.57735175 0.53697443 0.45216697 0.32703975 0.18563966 0.064153843 -0.013321267][0.32175571 0.35559967 0.3663587 0.367108 0.36955267 0.41175112 0.49397549 0.56999469 0.60287541 0.57077736 0.48508132 0.352232 0.1988406 0.066668905 -0.015713181][0.28755152 0.32012713 0.33712509 0.34682271 0.35976967 0.41775656 0.52220845 0.61729878 0.65784949 0.62318313 0.5274415 0.37988394 0.21094601 0.067667522 -0.018422181][0.29036129 0.3302103 0.36137646 0.38708216 0.41432795 0.4854506 0.60167044 0.70055413 0.73206085 0.68073553 0.56585431 0.40108377 0.2191902 0.069313467 -0.017630067][0.31974524 0.36893728 0.4143846 0.456744 0.49764785 0.57611644 0.69087589 0.77725554 0.78545928 0.70855522 0.57314664 0.39703935 0.21213755 0.064786747 -0.017864114][0.35460669 0.40421683 0.45333508 0.50402379 0.55165583 0.62793046 0.729101 0.79422027 0.77780485 0.68138379 0.53721237 0.36469781 0.19084169 0.055237506 -0.019564981][0.35856169 0.39419037 0.43182245 0.47713223 0.52040058 0.58427566 0.6642766 0.70879018 0.67958695 0.58293116 0.45125958 0.30148974 0.15377447 0.039371606 -0.02370978][0.31215626 0.3251611 0.34233144 0.3721194 0.402272 0.4471494 0.50398588 0.53419024 0.50773525 0.43088549 0.33011737 0.21683815 0.10478038 0.017697351 -0.03043792][0.21972515 0.21115415 0.20873852 0.220339 0.23423161 0.2589021 0.294688 0.3164562 0.30181423 0.25391597 0.19052304 0.11742001 0.043489192 -0.013843232 -0.044186059][0.11144925 0.090536058 0.076190919 0.073390052 0.07362923 0.082154334 0.10173072 0.11801109 0.11518251 0.093043886 0.061668579 0.022919115 -0.017681085 -0.048123073 -0.061314121][0.022584511 0.00073827745 -0.016136294 -0.025812674 -0.032920823 -0.034109145 -0.025966998 -0.015691174 -0.01318216 -0.020049322 -0.031436324 -0.047034137 -0.063562818 -0.073696919 -0.073633038][-0.033111483 -0.048503991 -0.060831677 -0.069917604 -0.077680916 -0.082209677 -0.080968075 -0.077085629 -0.07543239 -0.077041984 -0.079601713 -0.083120428 -0.085759625 -0.08384937 -0.076347016]]...]
INFO - root - 2017-12-11 09:58:17.354449: step 50810, loss = 0.71, batch loss = 0.65 (29.7 examples/sec; 0.270 sec/batch; 21h:05m:28s remains)
INFO - root - 2017-12-11 09:58:19.974102: step 50820, loss = 0.69, batch loss = 0.63 (31.5 examples/sec; 0.254 sec/batch; 19h:50m:31s remains)
INFO - root - 2017-12-11 09:58:22.623736: step 50830, loss = 0.69, batch loss = 0.64 (30.0 examples/sec; 0.267 sec/batch; 20h:52m:32s remains)
INFO - root - 2017-12-11 09:58:25.258175: step 50840, loss = 0.70, batch loss = 0.64 (32.2 examples/sec; 0.248 sec/batch; 19h:25m:47s remains)
INFO - root - 2017-12-11 09:58:27.924679: step 50850, loss = 0.68, batch loss = 0.63 (29.7 examples/sec; 0.269 sec/batch; 21h:04m:45s remains)
INFO - root - 2017-12-11 09:58:30.540225: step 50860, loss = 0.70, batch loss = 0.64 (31.2 examples/sec; 0.257 sec/batch; 20h:04m:43s remains)
INFO - root - 2017-12-11 09:58:33.173395: step 50870, loss = 0.68, batch loss = 0.63 (31.1 examples/sec; 0.257 sec/batch; 20h:08m:16s remains)
INFO - root - 2017-12-11 09:58:35.807912: step 50880, loss = 0.71, batch loss = 0.65 (30.8 examples/sec; 0.259 sec/batch; 20h:17m:54s remains)
INFO - root - 2017-12-11 09:58:38.461324: step 50890, loss = 0.70, batch loss = 0.65 (30.4 examples/sec; 0.263 sec/batch; 20h:35m:28s remains)
INFO - root - 2017-12-11 09:58:41.111946: step 50900, loss = 0.69, batch loss = 0.63 (30.7 examples/sec; 0.261 sec/batch; 20h:22m:43s remains)
2017-12-11 09:58:41.485071: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.16128495 0.20240372 0.2292407 0.24730678 0.25479946 0.24651217 0.23512693 0.23758507 0.26024836 0.283831 0.31147236 0.34270492 0.36436677 0.36856869 0.35921609][0.21378082 0.2541644 0.27070421 0.27291563 0.26493624 0.24632117 0.22999068 0.2317086 0.25479415 0.27789411 0.30462888 0.33524752 0.35636306 0.35979134 0.35272443][0.2446475 0.27747306 0.27989882 0.26362881 0.24105895 0.21845402 0.20550498 0.21216679 0.23689936 0.25873566 0.28205729 0.31034333 0.33053207 0.33286694 0.32718146][0.25216576 0.27871627 0.2746169 0.24945778 0.22289667 0.20696884 0.20560424 0.22102797 0.24629627 0.26288784 0.27605721 0.29482296 0.30795962 0.30443183 0.29507503][0.24478737 0.27589852 0.28264481 0.26734743 0.25295752 0.25378633 0.26686266 0.28787538 0.30656579 0.30971724 0.30357406 0.30301574 0.29930893 0.28189614 0.26256648][0.23945524 0.28972903 0.32451874 0.33558109 0.3448641 0.36590815 0.39008078 0.40881234 0.41167757 0.39384726 0.36387673 0.33940071 0.31226891 0.27489215 0.24150459][0.25296643 0.33127454 0.39957708 0.4390997 0.46949089 0.50359517 0.53031963 0.53866023 0.52054751 0.48120919 0.43284732 0.39012235 0.34185311 0.28508669 0.23832844][0.29181796 0.39521289 0.48669642 0.539838 0.57383454 0.60445279 0.62291414 0.61691362 0.5813427 0.52951884 0.47608757 0.42839622 0.37024218 0.30298808 0.24905936][0.34516758 0.45915872 0.552588 0.59568948 0.60980415 0.61953372 0.62330848 0.60659719 0.56533742 0.51594925 0.47276562 0.43464425 0.3812508 0.31663075 0.26443842][0.38552633 0.49006593 0.56462836 0.58007866 0.56076962 0.54276443 0.53377771 0.51609695 0.48404178 0.45110875 0.42835361 0.40804708 0.36884227 0.31636441 0.27222306][0.3907766 0.46937874 0.5120666 0.49481964 0.44419986 0.40504768 0.39188573 0.38427138 0.37334138 0.3651785 0.36455011 0.3604407 0.3358075 0.29791871 0.26345149][0.36036718 0.4063549 0.4164677 0.37468898 0.3072339 0.2606757 0.25153637 0.2586965 0.26997152 0.2840001 0.29837507 0.30250832 0.28749985 0.26197514 0.23628837][0.30903113 0.32731891 0.31502834 0.26422164 0.19782481 0.15708704 0.15656035 0.17645842 0.20173728 0.22680107 0.24483216 0.24859673 0.23782024 0.22095197 0.20262229][0.25362265 0.256174 0.236092 0.19105153 0.13860753 0.11073728 0.11871467 0.14547653 0.17373726 0.19739127 0.21004266 0.20956013 0.20080204 0.1896591 0.1771328][0.21077475 0.20960577 0.19308308 0.1612255 0.12577671 0.10920713 0.12068404 0.14676374 0.16913192 0.18382883 0.18830386 0.18503852 0.17950879 0.17315783 0.16546559]]...]
INFO - root - 2017-12-11 09:58:44.076858: step 50910, loss = 0.68, batch loss = 0.63 (30.4 examples/sec; 0.263 sec/batch; 20h:34m:29s remains)
INFO - root - 2017-12-11 09:58:46.735847: step 50920, loss = 0.69, batch loss = 0.64 (30.1 examples/sec; 0.266 sec/batch; 20h:48m:13s remains)
INFO - root - 2017-12-11 09:58:49.393425: step 50930, loss = 0.68, batch loss = 0.62 (30.4 examples/sec; 0.263 sec/batch; 20h:33m:10s remains)
INFO - root - 2017-12-11 09:58:51.980929: step 50940, loss = 0.70, batch loss = 0.65 (31.4 examples/sec; 0.255 sec/batch; 19h:55m:41s remains)
INFO - root - 2017-12-11 09:58:54.614173: step 50950, loss = 0.71, batch loss = 0.65 (31.0 examples/sec; 0.258 sec/batch; 20h:12m:25s remains)
INFO - root - 2017-12-11 09:58:57.245236: step 50960, loss = 0.71, batch loss = 0.65 (31.3 examples/sec; 0.255 sec/batch; 19h:57m:30s remains)
INFO - root - 2017-12-11 09:58:59.848017: step 50970, loss = 0.70, batch loss = 0.64 (29.8 examples/sec; 0.268 sec/batch; 20h:57m:41s remains)
INFO - root - 2017-12-11 09:59:02.458152: step 50980, loss = 0.71, batch loss = 0.65 (30.1 examples/sec; 0.265 sec/batch; 20h:45m:00s remains)
INFO - root - 2017-12-11 09:59:05.130893: step 50990, loss = 0.72, batch loss = 0.66 (30.4 examples/sec; 0.263 sec/batch; 20h:33m:13s remains)
INFO - root - 2017-12-11 09:59:07.794743: step 51000, loss = 0.69, batch loss = 0.63 (31.1 examples/sec; 0.257 sec/batch; 20h:06m:04s remains)
2017-12-11 09:59:08.161867: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.014653988 0.0097710053 0.0040437253 0.0018470584 0.00550003 0.014634476 0.024249095 0.030452298 0.027945869 0.012459185 -0.016099431 -0.050625585 -0.079041719 -0.095489144 -0.10174064][0.071670771 0.06903556 0.064637527 0.068091832 0.084000766 0.10803448 0.13068642 0.14301999 0.13799952 0.11042661 0.060233731 -0.00067608646 -0.052857485 -0.085356213 -0.10159881][0.14009753 0.14151753 0.14039691 0.15250713 0.18630406 0.23300441 0.27660307 0.30103505 0.29518428 0.25346062 0.17592037 0.08082068 -0.0032593193 -0.059171338 -0.089893661][0.21227568 0.21906182 0.22204341 0.24223542 0.29255295 0.36116454 0.42577064 0.46173319 0.45239431 0.39420071 0.28888333 0.16086985 0.046356555 -0.031694818 -0.075832382][0.2978023 0.31113845 0.31709817 0.34113249 0.40110394 0.48287204 0.55966216 0.59863579 0.57830358 0.49942449 0.36776388 0.21366754 0.077994578 -0.013484772 -0.064713426][0.38804343 0.41121477 0.42161724 0.44718465 0.5112257 0.59869045 0.68031353 0.716979 0.68279666 0.58326566 0.42814803 0.25377193 0.10309735 0.0025447619 -0.05315971][0.44828975 0.48547161 0.5056141 0.53543174 0.60017252 0.68646389 0.76451886 0.79347503 0.74441719 0.62766421 0.456827 0.27205688 0.11547735 0.011584168 -0.045138337][0.47443551 0.52640283 0.55903518 0.59272623 0.65025246 0.72191221 0.78056526 0.79022765 0.72383279 0.59673792 0.42436332 0.24576341 0.098224446 0.0018772127 -0.048808061][0.47060597 0.534345 0.57706189 0.61208659 0.65650237 0.70345634 0.73095995 0.71264285 0.629371 0.49955034 0.33876589 0.18107514 0.055720706 -0.023202302 -0.061951343][0.42087975 0.49016273 0.5386889 0.57350981 0.60584182 0.6305446 0.6309 0.59058267 0.49948621 0.37580055 0.23435816 0.10332863 0.0042824405 -0.054439142 -0.079511918][0.3143025 0.3831352 0.43451068 0.46886098 0.49264675 0.50264645 0.48766378 0.43939626 0.35440212 0.24784166 0.13222925 0.029881753 -0.043650605 -0.083721444 -0.096551768][0.17452064 0.23271617 0.279145 0.3081823 0.32282722 0.32160208 0.29899311 0.2527664 0.1840328 0.10417677 0.022285722 -0.046015326 -0.091104738 -0.11118459 -0.11167847][0.045846738 0.084691532 0.1172986 0.13520581 0.139376 0.12930253 0.10433692 0.065988407 0.018021317 -0.0320689 -0.0788404 -0.1128552 -0.13021325 -0.13158998 -0.12160721][-0.049712576 -0.03243196 -0.016812515 -0.01066322 -0.014104287 -0.027496774 -0.048988421 -0.075676255 -0.10329284 -0.12771176 -0.14596377 -0.15319134 -0.14970848 -0.13817568 -0.12228616][-0.10752375 -0.10673461 -0.10472144 -0.10732137 -0.11469249 -0.12702829 -0.14209467 -0.15703043 -0.1684671 -0.17447002 -0.17370933 -0.1645792 -0.14919706 -0.13185731 -0.11494518]]...]
INFO - root - 2017-12-11 09:59:10.833882: step 51010, loss = 0.70, batch loss = 0.65 (31.5 examples/sec; 0.254 sec/batch; 19h:52m:06s remains)
INFO - root - 2017-12-11 09:59:13.445968: step 51020, loss = 0.69, batch loss = 0.64 (30.3 examples/sec; 0.264 sec/batch; 20h:40m:10s remains)
INFO - root - 2017-12-11 09:59:16.057667: step 51030, loss = 0.70, batch loss = 0.64 (30.4 examples/sec; 0.263 sec/batch; 20h:35m:29s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-SGD-lr0.001-clip50-initfromfixqian30k-fixqian
INFO - root - 2017-12-11 09:59:18.681225: step 51040, loss = 0.69, batch loss = 0.63 (27.6 examples/sec; 0.290 sec/batch; 22h:40m:24s remains)
INFO - root - 2017-12-11 09:59:21.355280: step 51050, loss = 0.70, batch loss = 0.65 (29.5 examples/sec; 0.271 sec/batch; 21h:13m:23s remains)
INFO - root - 2017-12-11 09:59:24.055225: step 51060, loss = 0.70, batch loss = 0.64 (30.3 examples/sec; 0.264 sec/batch; 20h:38m:18s remains)
INFO - root - 2017-12-11 09:59:26.701331: step 51070, loss = 0.71, batch loss = 0.66 (31.0 examples/sec; 0.258 sec/batch; 20h:11m:16s remains)
INFO - root - 2017-12-11 09:59:29.350905: step 51080, loss = 0.70, batch loss = 0.64 (30.0 examples/sec; 0.266 sec/batch; 20h:49m:48s remains)
INFO - root - 2017-12-11 09:59:32.080274: step 51090, loss = 0.69, batch loss = 0.63 (28.8 examples/sec; 0.278 sec/batch; 21h:42m:49s remains)
INFO - root - 2017-12-11 09:59:35.027205: step 51100, loss = 0.70, batch loss = 0.64 (21.1 examples/sec; 0.379 sec/batch; 29h:37m:17s remains)
2017-12-11 09:59:35.545692: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.12909675 0.084384367 0.054968759 0.069840021 0.13867033 0.24390304 0.3563177 0.45019311 0.51180923 0.53323418 0.51018834 0.45107317 0.36889759 0.2793144 0.197978][0.09238524 0.070838727 0.058916379 0.078506358 0.13882779 0.22525291 0.31156915 0.37619352 0.40963736 0.40840083 0.37254179 0.31207335 0.23993577 0.16939589 0.115067][0.065230012 0.068045557 0.073160253 0.095544018 0.14461301 0.21000676 0.26965559 0.30768603 0.31847093 0.30102354 0.2560696 0.19340575 0.12637408 0.067990452 0.033576783][0.062539242 0.086638704 0.10723536 0.13340007 0.17588563 0.2290305 0.27392039 0.29795918 0.29727957 0.27047041 0.21675409 0.14437816 0.068733186 0.0064763566 -0.023730317][0.082728356 0.12568182 0.16380842 0.2004904 0.24741462 0.30219835 0.34679991 0.3674255 0.35984677 0.32238573 0.25411651 0.1628149 0.066972673 -0.011377236 -0.050244395][0.1216151 0.18345752 0.24386889 0.29997605 0.36105844 0.42558205 0.47484154 0.49124911 0.47045743 0.41369697 0.32358444 0.20903492 0.090579748 -0.0062133335 -0.058732022][0.17084196 0.2527765 0.33854455 0.41839179 0.49487948 0.56466657 0.608995 0.60907108 0.56360745 0.47973457 0.36598507 0.23333977 0.10188207 -0.0040769195 -0.065167554][0.21245128 0.31302431 0.42147902 0.52087992 0.60443389 0.66552097 0.68883413 0.66019517 0.584626 0.47518197 0.34589335 0.20901102 0.081081688 -0.019182969 -0.077617615][0.22861119 0.33970433 0.45931661 0.56480742 0.64135796 0.6802476 0.67287385 0.61395764 0.51580423 0.39515388 0.26774612 0.14487201 0.0374377 -0.043555256 -0.089122243][0.21680354 0.32402566 0.43607333 0.52890652 0.584981 0.59605712 0.5605002 0.48221353 0.37902585 0.26836768 0.16345417 0.071753316 -0.002677582 -0.056927897 -0.085975073][0.18670638 0.27412391 0.35838443 0.42101759 0.44882309 0.43624043 0.38593447 0.30754143 0.22105044 0.14148407 0.07608635 0.025833063 -0.012282349 -0.041819636 -0.060009822][0.15086588 0.20768704 0.25253862 0.27749777 0.27826661 0.2527433 0.20428233 0.14352097 0.0895077 0.053582385 0.035659533 0.028497269 0.022107353 0.0086541139 -0.0095262192][0.12301674 0.14864032 0.15600032 0.14863423 0.13138923 0.10490555 0.0708695 0.0376736 0.021201203 0.028809566 0.053669706 0.080795549 0.094447084 0.085143782 0.056068152][0.1219863 0.12482236 0.10697552 0.081660405 0.0592642 0.041311111 0.0266866 0.020550046 0.035263177 0.074939363 0.12899007 0.1770857 0.19950181 0.18618776 0.14094782][0.14577131 0.13930383 0.11314997 0.086875007 0.071477823 0.066789083 0.068506904 0.079676852 0.11048347 0.16383617 0.23051748 0.28861547 0.31562331 0.299575 0.2426115]]...]
INFO - root - 2017-12-11 09:59:40.871809: step 51110, loss = 0.70, batch loss = 0.64 (14.5 examples/sec; 0.550 sec/batch; 43h:00m:38s remains)
INFO - root - 2017-12-11 09:59:46.400796: step 51120, loss = 0.69, batch loss = 0.63 (14.1 examples/sec; 0.566 sec/batch; 44h:15m:13s remains)
INFO - root - 2017-12-11 09:59:51.948777: step 51130, loss = 0.69, batch loss = 0.63 (13.5 examples/sec; 0.591 sec/batch; 46h:13m:47s remains)
INFO - root - 2017-12-11 09:59:57.498954: step 51140, loss = 0.69, batch loss = 0.63 (14.6 examples/sec; 0.550 sec/batch; 42h:58m:11s remains)
INFO - root - 2017-12-11 10:00:02.970356: step 51150, loss = 0.69, batch loss = 0.63 (14.9 examples/sec; 0.537 sec/batch; 41h:56m:53s remains)
INFO - root - 2017-12-11 10:00:08.479069: step 51160, loss = 0.70, batch loss = 0.64 (14.4 examples/sec; 0.554 sec/batch; 43h:16m:55s remains)
INFO - root - 2017-12-11 10:00:13.981737: step 51170, loss = 0.68, batch loss = 0.62 (14.6 examples/sec; 0.547 sec/batch; 42h:43m:29s remains)
INFO - root - 2017-12-11 10:00:19.521641: step 51180, loss = 0.71, batch loss = 0.65 (14.1 examples/sec; 0.569 sec/batch; 44h:29m:14s remains)
INFO - root - 2017-12-11 10:00:24.757461: step 51190, loss = 0.70, batch loss = 0.64 (14.9 examples/sec; 0.536 sec/batch; 41h:50m:44s remains)
INFO - root - 2017-12-11 10:00:30.209535: step 51200, loss = 0.71, batch loss = 0.65 (14.8 examples/sec; 0.539 sec/batch; 42h:07m:20s remains)
2017-12-11 10:00:30.806555: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0069397874 -0.0094285188 -0.011354104 -0.012272421 -0.012349552 -0.012239929 -0.012165426 -0.012889452 -0.014837895 -0.017504284 -0.019520715 -0.0197612 -0.01848422 -0.016489951 -0.014220528][0.013541642 0.010322775 0.0077206916 0.0070020719 0.0080850739 0.0096786106 0.010666657 0.00948338 0.0056775445 0.0002707206 -0.0046265237 -0.0070120892 -0.0068300287 -0.0049498333 -0.0021948363][0.038351532 0.0351211 0.032567885 0.033318527 0.0372546 0.042405084 0.046310082 0.045536861 0.039194643 0.029253665 0.019797619 0.01428263 0.012701664 0.014786919 0.019097371][0.065748081 0.062908649 0.061191723 0.064608753 0.0727773 0.083515212 0.092642307 0.09370172 0.084970661 0.069517612 0.053976044 0.0434705 0.039407786 0.042135529 0.04921905][0.089191 0.0881007 0.089165874 0.097311251 0.11146948 0.12962195 0.14604326 0.15053563 0.13978934 0.11805293 0.095195547 0.079045534 0.072516792 0.076182194 0.086481966][0.10252398 0.10403308 0.11032892 0.12556113 0.1476246 0.17476161 0.2000995 0.20967121 0.19800486 0.17050962 0.14057593 0.11953983 0.1114764 0.1161485 0.12907599][0.11113877 0.11440669 0.12531365 0.14754537 0.17752919 0.21315458 0.24723975 0.26261893 0.25102273 0.21876679 0.18307135 0.15849119 0.15025035 0.15676832 0.17308544][0.12247329 0.12664163 0.1408356 0.16929317 0.20637287 0.24810103 0.28784412 0.30680612 0.29429823 0.25704548 0.21525711 0.18645279 0.17718785 0.18556923 0.20598371][0.13936697 0.1446517 0.15982731 0.191642 0.23302709 0.27723023 0.3182379 0.33752719 0.32284644 0.28029719 0.23081803 0.19517606 0.18237025 0.19172636 0.21657279][0.16444062 0.17007715 0.18212642 0.21177477 0.25199535 0.29388928 0.33225542 0.35036641 0.33428633 0.28747007 0.23041563 0.18715577 0.16972993 0.179166 0.20755336][0.20175835 0.20775686 0.21339321 0.2349679 0.26719674 0.30177528 0.33510974 0.35229936 0.33644 0.28685904 0.2240966 0.17461517 0.15305355 0.16128129 0.19055896][0.24529743 0.25367641 0.25299203 0.263018 0.28193507 0.30545071 0.33193344 0.34758273 0.33241853 0.28187984 0.21687558 0.16421147 0.13925923 0.14384705 0.16995598][0.28490168 0.29932079 0.29626247 0.29590911 0.30009633 0.31036264 0.32712165 0.33833268 0.32303438 0.27462697 0.21317159 0.16220909 0.13499741 0.13302042 0.1510417][0.30961251 0.33228138 0.33126947 0.32452241 0.316104 0.3130722 0.31791902 0.32165822 0.30595636 0.26444674 0.2142054 0.17088917 0.14328967 0.13327548 0.1399104][0.31124246 0.34080237 0.34412912 0.335133 0.31824195 0.30395278 0.29662821 0.29162532 0.27609321 0.2453654 0.21157931 0.18017708 0.154695 0.13797365 0.13320529]]...]
INFO - root - 2017-12-11 10:00:36.322490: step 51210, loss = 0.70, batch loss = 0.65 (14.6 examples/sec; 0.546 sec/batch; 42h:40m:34s remains)
INFO - root - 2017-12-11 10:00:41.782177: step 51220, loss = 0.71, batch loss = 0.65 (15.1 examples/sec; 0.530 sec/batch; 41h:23m:03s remains)
INFO - root - 2017-12-11 10:00:47.331181: step 51230, loss = 0.70, batch loss = 0.64 (14.3 examples/sec; 0.558 sec/batch; 43h:34m:50s remains)
INFO - root - 2017-12-11 10:00:52.890995: step 51240, loss = 0.71, batch loss = 0.65 (14.4 examples/sec; 0.554 sec/batch; 43h:15m:26s remains)
INFO - root - 2017-12-11 10:00:58.468966: step 51250, loss = 0.71, batch loss = 0.65 (14.5 examples/sec; 0.552 sec/batch; 43h:06m:27s remains)
INFO - root - 2017-12-11 10:01:03.941041: step 51260, loss = 0.69, batch loss = 0.63 (14.8 examples/sec; 0.542 sec/batch; 42h:20m:23s remains)
INFO - root - 2017-12-11 10:01:09.226055: step 51270, loss = 0.67, batch loss = 0.61 (15.0 examples/sec; 0.535 sec/batch; 41h:47m:42s remains)
INFO - root - 2017-12-11 10:01:14.797143: step 51280, loss = 0.70, batch loss = 0.64 (13.8 examples/sec; 0.579 sec/batch; 45h:11m:55s remains)
INFO - root - 2017-12-11 10:01:20.254900: step 51290, loss = 0.70, batch loss = 0.65 (14.4 examples/sec; 0.555 sec/batch; 43h:20m:12s remains)
INFO - root - 2017-12-11 10:01:25.780798: step 51300, loss = 0.69, batch loss = 0.63 (16.8 examples/sec; 0.475 sec/batch; 37h:08m:03s remains)
2017-12-11 10:01:26.350329: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.09457171 0.13100389 0.16799168 0.20716077 0.24436824 0.27454963 0.29841703 0.32258284 0.34101287 0.34745583 0.33979288 0.31936544 0.292128 0.25974029 0.23858435][0.12991764 0.16635393 0.20270164 0.24121341 0.2751931 0.29729873 0.31050661 0.32528988 0.33753243 0.34002998 0.33086973 0.3125343 0.29094177 0.26777127 0.255567][0.15606858 0.1902339 0.22366489 0.26021442 0.29080638 0.30636984 0.31153479 0.31817892 0.32279316 0.31768647 0.30217952 0.28236672 0.2657856 0.25321236 0.25101659][0.17589679 0.20402463 0.23292103 0.26878047 0.30042562 0.31663051 0.32194385 0.32580698 0.32515064 0.31202123 0.28688958 0.26053709 0.24352424 0.23762465 0.24280012][0.18749961 0.2061978 0.23019497 0.26737195 0.30418867 0.32724306 0.33834773 0.34231105 0.33801582 0.31905025 0.28717732 0.2558755 0.23779558 0.2369435 0.24785486][0.20049192 0.20975429 0.23097496 0.2714681 0.31511793 0.346452 0.36272156 0.36439809 0.35468185 0.33118722 0.29608902 0.26304632 0.24585353 0.2503891 0.2660014][0.21932325 0.22716533 0.25175187 0.29674727 0.3442724 0.37871253 0.39384291 0.38826838 0.37106198 0.34434813 0.30842665 0.27533767 0.26003772 0.26942292 0.2886585][0.24085787 0.26106721 0.29652661 0.34406966 0.38575709 0.41027075 0.41320291 0.39580169 0.37233961 0.34747061 0.31670621 0.28914043 0.27999341 0.29616663 0.31999782][0.26071382 0.30237189 0.35053724 0.39349952 0.41635352 0.41691566 0.39856717 0.36853415 0.34358028 0.32813686 0.31206894 0.299409 0.30306259 0.32836789 0.35743207][0.27308345 0.33542019 0.39006826 0.41937923 0.41483396 0.38681209 0.34889719 0.31345186 0.29479393 0.29516169 0.3003929 0.3078866 0.32523724 0.35582331 0.38582703][0.26689169 0.33936393 0.39032954 0.40079474 0.37108651 0.32363671 0.28000239 0.2529383 0.24937835 0.26806346 0.29420736 0.31902957 0.34319991 0.36944067 0.39232126][0.23171398 0.29750037 0.33504212 0.3286961 0.28702468 0.24027906 0.21228583 0.20990056 0.22985478 0.26635775 0.30680543 0.33849314 0.35749963 0.36951014 0.37975109][0.17110291 0.21673188 0.23679382 0.22215378 0.18602148 0.16022412 0.16544075 0.19907878 0.24707848 0.29804349 0.3416115 0.36523715 0.36585283 0.35664344 0.35291493][0.0994875 0.12362204 0.13071252 0.11766548 0.0984263 0.10179938 0.14298929 0.21095514 0.28314015 0.34157827 0.37646583 0.37904173 0.35225898 0.31901729 0.30186474][0.033697996 0.044842716 0.048312068 0.04351759 0.041849233 0.066863075 0.12938812 0.21484269 0.29725021 0.35367322 0.3734892 0.35258365 0.30009761 0.2465539 0.21824887]]...]
INFO - root - 2017-12-11 10:01:28.975278: step 51310, loss = 0.71, batch loss = 0.65 (30.2 examples/sec; 0.265 sec/batch; 20h:41m:45s remains)
INFO - root - 2017-12-11 10:01:31.621241: step 51320, loss = 0.70, batch loss = 0.64 (29.5 examples/sec; 0.271 sec/batch; 21h:10m:22s remains)
INFO - root - 2017-12-11 10:01:34.245129: step 51330, loss = 0.69, batch loss = 0.64 (30.4 examples/sec; 0.263 sec/batch; 20h:31m:59s remains)
INFO - root - 2017-12-11 10:01:36.913133: step 51340, loss = 0.69, batch loss = 0.63 (32.0 examples/sec; 0.250 sec/batch; 19h:31m:31s remains)
INFO - root - 2017-12-11 10:01:39.547124: step 51350, loss = 0.70, batch loss = 0.64 (31.2 examples/sec; 0.257 sec/batch; 20h:02m:51s remains)
INFO - root - 2017-12-11 10:01:42.194138: step 51360, loss = 0.70, batch loss = 0.65 (30.8 examples/sec; 0.260 sec/batch; 20h:16m:12s remains)
INFO - root - 2017-12-11 10:01:44.841409: step 51370, loss = 0.71, batch loss = 0.65 (30.7 examples/sec; 0.261 sec/batch; 20h:22m:14s remains)
INFO - root - 2017-12-11 10:01:47.542926: step 51380, loss = 0.71, batch loss = 0.66 (29.8 examples/sec; 0.268 sec/batch; 20h:56m:32s remains)
INFO - root - 2017-12-11 10:01:50.174946: step 51390, loss = 0.69, batch loss = 0.63 (27.7 examples/sec; 0.289 sec/batch; 22h:33m:43s remains)
INFO - root - 2017-12-11 10:01:52.817026: step 51400, loss = 0.71, batch loss = 0.65 (30.6 examples/sec; 0.262 sec/batch; 20h:25m:43s remains)
2017-12-11 10:01:53.180552: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.038700808 -0.0388907 -0.039373279 -0.040595017 -0.041437697 -0.0407298 -0.038576432 -0.036657993 -0.036028195 -0.037492886 -0.040663529 -0.044445336 -0.04767989 -0.049158618 -0.048627123][-0.029174374 -0.026919436 -0.026206803 -0.027738374 -0.029188583 -0.027739083 -0.023255963 -0.018464979 -0.016277988 -0.01841132 -0.024785116 -0.033528239 -0.042388722 -0.048722196 -0.051102564][-0.010098694 -0.0029007141 0.000979301 0.00039471532 -0.0012321137 0.0015250335 0.00963344 0.019216238 0.024725294 0.022319421 0.011499606 -0.0049140286 -0.023006584 -0.038069963 -0.046853822][0.014173876 0.028552327 0.038049083 0.040582 0.040500116 0.046402551 0.060331803 0.0769258 0.087265782 0.084836096 0.068382859 0.041817255 0.011064098 -0.016646568 -0.03560074][0.039139055 0.061490852 0.07781364 0.084875152 0.088057213 0.099406444 0.12167292 0.14696237 0.16271134 0.16023581 0.13774967 0.10015946 0.0553237 0.013003048 -0.018305883][0.058571815 0.087571874 0.10976495 0.12103695 0.12804975 0.14602973 0.17766242 0.21167104 0.23248617 0.23031136 0.20314415 0.1564001 0.099362023 0.043801341 0.00087250903][0.068571977 0.10157668 0.1270335 0.14056796 0.15065557 0.17430446 0.21352372 0.25362524 0.27809036 0.27723861 0.24832305 0.19652162 0.13174945 0.067388318 0.01640366][0.07084807 0.10520123 0.13055433 0.143223 0.15385215 0.17954449 0.22139384 0.26252416 0.2878828 0.28884968 0.26119977 0.20918165 0.14259848 0.076047331 0.022864632][0.06828966 0.10164773 0.12394129 0.13279545 0.14071082 0.16309333 0.20044911 0.23616929 0.25863823 0.26084945 0.2366395 0.18923242 0.12765788 0.066689 0.018306622][0.060027659 0.09017472 0.107696 0.1116323 0.11474364 0.12911835 0.15538785 0.18020421 0.19652644 0.19884692 0.17944452 0.14080094 0.09060245 0.042029757 0.0041741505][0.043098804 0.067764342 0.080122747 0.080053672 0.078539282 0.083550386 0.096227676 0.10843364 0.11727711 0.11853976 0.10416747 0.076310463 0.040989045 0.00827846 -0.01624525][0.019346425 0.037217151 0.045184527 0.042967469 0.038525958 0.036302354 0.037931249 0.040006366 0.042194162 0.041521519 0.03120474 0.013237241 -0.0080781784 -0.026003523 -0.037963502][-0.0068894941 0.0037393991 0.0079654614 0.0048794216 -0.00062095071 -0.0063690664 -0.010696462 -0.014108545 -0.016141851 -0.018612495 -0.02588631 -0.036091786 -0.046376564 -0.053008623 -0.055341572][-0.031156592 -0.027389314 -0.026502909 -0.029823869 -0.034859426 -0.040746812 -0.046359148 -0.051045876 -0.054355834 -0.057217922 -0.06155875 -0.065766893 -0.068141177 -0.067382477 -0.063848205][-0.046946935 -0.047835298 -0.048881471 -0.051629625 -0.055057131 -0.058970958 -0.06296137 -0.066253193 -0.0685969 -0.070433021 -0.072231613 -0.072800942 -0.071239114 -0.067616925 -0.062558934]]...]
INFO - root - 2017-12-11 10:01:55.818846: step 51410, loss = 0.69, batch loss = 0.63 (31.0 examples/sec; 0.258 sec/batch; 20h:08m:55s remains)
INFO - root - 2017-12-11 10:01:58.432923: step 51420, loss = 0.70, batch loss = 0.65 (30.5 examples/sec; 0.262 sec/batch; 20h:27m:29s remains)
INFO - root - 2017-12-11 10:02:01.073482: step 51430, loss = 0.69, batch loss = 0.63 (29.9 examples/sec; 0.267 sec/batch; 20h:52m:07s remains)
INFO - root - 2017-12-11 10:02:03.657158: step 51440, loss = 0.70, batch loss = 0.64 (30.7 examples/sec; 0.260 sec/batch; 20h:18m:46s remains)
INFO - root - 2017-12-11 10:02:06.342841: step 51450, loss = 0.69, batch loss = 0.63 (31.4 examples/sec; 0.255 sec/batch; 19h:53m:00s remains)
INFO - root - 2017-12-11 10:02:09.048682: step 51460, loss = 0.69, batch loss = 0.63 (30.4 examples/sec; 0.263 sec/batch; 20h:32m:17s remains)
INFO - root - 2017-12-11 10:02:13.411488: step 51470, loss = 0.69, batch loss = 0.63 (23.9 examples/sec; 0.335 sec/batch; 26h:07m:14s remains)
INFO - root - 2017-12-11 10:02:18.152694: step 51480, loss = 0.70, batch loss = 0.65 (14.7 examples/sec; 0.545 sec/batch; 42h:32m:10s remains)
INFO - root - 2017-12-11 10:02:23.598982: step 51490, loss = 0.69, batch loss = 0.63 (15.0 examples/sec; 0.534 sec/batch; 41h:39m:02s remains)
INFO - root - 2017-12-11 10:02:28.983582: step 51500, loss = 0.70, batch loss = 0.64 (14.6 examples/sec; 0.549 sec/batch; 42h:50m:55s remains)
2017-12-11 10:02:29.599838: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.28412494 0.29646319 0.33639833 0.3784903 0.39981619 0.38329953 0.33308625 0.27135709 0.21512301 0.17187083 0.14726335 0.14419214 0.15937904 0.18705155 0.22396027][0.34527883 0.34845075 0.37971184 0.41806218 0.44053048 0.42515779 0.37060446 0.29572359 0.21843852 0.1531413 0.10886308 0.09336178 0.10320761 0.13032202 0.17367448][0.37889236 0.38908198 0.42324889 0.46380886 0.48856705 0.47480172 0.418488 0.33371425 0.23932658 0.15577152 0.094296895 0.065666743 0.065935358 0.085114539 0.12456565][0.38309988 0.41865492 0.47041216 0.52096778 0.54970223 0.53724909 0.48151046 0.39177293 0.28613061 0.18783586 0.11116377 0.069166891 0.057216249 0.064438313 0.09278769][0.35443336 0.42476526 0.5042758 0.57115763 0.606964 0.59930372 0.55178887 0.46894392 0.36373812 0.2580266 0.16974689 0.11473552 0.089222871 0.081500337 0.093353622][0.30084077 0.40214521 0.50966036 0.59520495 0.64190143 0.64531422 0.6155557 0.55336148 0.46285489 0.3599804 0.26508063 0.19748366 0.1566074 0.1319861 0.12624764][0.23556116 0.34928185 0.46991456 0.56653434 0.62505049 0.6478672 0.64688164 0.61768776 0.55400497 0.4639675 0.36859906 0.29044485 0.23311457 0.19064534 0.16821739][0.17332153 0.27212822 0.38058087 0.47245997 0.53710377 0.57960045 0.6089285 0.61623013 0.58640665 0.52055496 0.4373976 0.35885006 0.29045174 0.23185591 0.19345197][0.13190943 0.19872302 0.27500653 0.34443131 0.40035281 0.44819716 0.493553 0.52674818 0.52863663 0.49567279 0.44077292 0.37877992 0.31247354 0.24558035 0.19504692][0.12402361 0.15714599 0.19342867 0.22679667 0.25623578 0.28839612 0.32885036 0.37079981 0.39592275 0.39660037 0.3778576 0.34452266 0.29428923 0.23202832 0.17905663][0.14873858 0.15645923 0.1573696 0.15390284 0.15027146 0.15444544 0.17392533 0.20719048 0.24012621 0.26222107 0.27187312 0.26615334 0.23882245 0.19247495 0.14772844][0.19015443 0.18404652 0.16354226 0.13790128 0.11254837 0.092793189 0.087233461 0.10052074 0.1244871 0.14796153 0.16643631 0.17458905 0.16530652 0.13765152 0.10683344][0.2220377 0.21419649 0.19355503 0.17052212 0.1448656 0.1149119 0.089076452 0.077384673 0.078512944 0.08458019 0.092900515 0.10018261 0.099807635 0.087150946 0.069830723][0.23378962 0.23659834 0.23769547 0.24181081 0.23657598 0.2106331 0.17128503 0.13340878 0.10387473 0.081382178 0.06719657 0.061833933 0.059672806 0.053599834 0.044077273][0.23253474 0.25170508 0.28348476 0.3247079 0.34944132 0.33679292 0.29212686 0.23438989 0.17702456 0.12558468 0.08556132 0.061546627 0.049036264 0.039714031 0.029827408]]...]
INFO - root - 2017-12-11 10:02:34.535503: step 51510, loss = 0.70, batch loss = 0.64 (16.4 examples/sec; 0.486 sec/batch; 37h:57m:54s remains)
INFO - root - 2017-12-11 10:02:37.186079: step 51520, loss = 0.71, batch loss = 0.65 (30.9 examples/sec; 0.259 sec/batch; 20h:13m:53s remains)
INFO - root - 2017-12-11 10:02:39.847851: step 51530, loss = 0.70, batch loss = 0.64 (28.5 examples/sec; 0.281 sec/batch; 21h:55m:22s remains)
INFO - root - 2017-12-11 10:02:42.573425: step 51540, loss = 0.69, batch loss = 0.63 (28.9 examples/sec; 0.277 sec/batch; 21h:35m:51s remains)
INFO - root - 2017-12-11 10:02:45.267665: step 51550, loss = 0.69, batch loss = 0.64 (30.4 examples/sec; 0.263 sec/batch; 20h:31m:53s remains)
INFO - root - 2017-12-11 10:02:47.947575: step 51560, loss = 0.69, batch loss = 0.63 (30.4 examples/sec; 0.263 sec/batch; 20h:33m:40s remains)
INFO - root - 2017-12-11 10:02:50.609307: step 51570, loss = 0.68, batch loss = 0.62 (31.8 examples/sec; 0.251 sec/batch; 19h:36m:09s remains)
INFO - root - 2017-12-11 10:02:53.231305: step 51580, loss = 0.69, batch loss = 0.63 (29.7 examples/sec; 0.269 sec/batch; 20h:59m:57s remains)
INFO - root - 2017-12-11 10:02:55.881384: step 51590, loss = 0.68, batch loss = 0.62 (29.9 examples/sec; 0.268 sec/batch; 20h:54m:28s remains)
INFO - root - 2017-12-11 10:02:58.534723: step 51600, loss = 0.69, batch loss = 0.63 (29.2 examples/sec; 0.274 sec/batch; 21h:20m:28s remains)
2017-12-11 10:02:58.916948: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.10108908 0.052277308 0.035860024 0.05904606 0.10876636 0.15981556 0.19459964 0.21161538 0.21978334 0.22623843 0.23168999 0.23720649 0.24291773 0.24238192 0.23709254][0.16509409 0.10926882 0.089147381 0.11530463 0.17339464 0.23264804 0.26892892 0.27916941 0.27615064 0.27377725 0.27779421 0.28944045 0.30547914 0.31338784 0.3139458][0.22338961 0.1686389 0.15077753 0.18192832 0.24754018 0.3136152 0.3506237 0.35178304 0.3330161 0.3155461 0.31105816 0.32398319 0.34826988 0.36567643 0.37465009][0.25110179 0.20895906 0.20273265 0.24399249 0.31951031 0.39394447 0.43390512 0.4285666 0.39493978 0.35856217 0.33661929 0.33793721 0.35839751 0.37933975 0.39621443][0.22947468 0.21266067 0.23050651 0.29107776 0.38273787 0.47099873 0.52036768 0.51571965 0.47221231 0.41609609 0.3675862 0.34107125 0.33862093 0.34787595 0.36337849][0.15622987 0.17108372 0.22084418 0.30926406 0.42513606 0.53554642 0.60395133 0.61108696 0.56761914 0.49646288 0.41785535 0.35050082 0.30587178 0.28317183 0.28019992][0.056910694 0.094498657 0.1712375 0.28718415 0.43035305 0.5686419 0.66432661 0.69345069 0.660933 0.58358365 0.47870806 0.36767778 0.27227831 0.20512718 0.16969271][-0.027639924 0.015329728 0.10099285 0.22958419 0.38942933 0.55040222 0.67439431 0.73292494 0.72298145 0.65225822 0.53320366 0.3890174 0.25038895 0.14109613 0.070304796][-0.065941833 -0.032535892 0.040200014 0.15724434 0.31038147 0.47459793 0.61377823 0.69712651 0.713143 0.66016388 0.54528838 0.39162651 0.23444007 0.10279781 0.008887032][-0.037434474 -0.021563249 0.021823106 0.10380695 0.22203386 0.36017212 0.488479 0.57776004 0.61066562 0.57904768 0.48558462 0.35038155 0.20663135 0.081387229 -0.013536591][0.0595019 0.058386326 0.064446039 0.0953091 0.15566935 0.2398 0.32870486 0.39956918 0.43334144 0.41845024 0.35439429 0.25610864 0.14966501 0.054877385 -0.019363016][0.2095716 0.19626747 0.16456547 0.13817811 0.1284847 0.14120039 0.17096457 0.20473506 0.22519886 0.21965288 0.18466687 0.12896046 0.069200091 0.016558159 -0.023581063][0.38285491 0.36545759 0.30250481 0.22418857 0.14727362 0.087775163 0.053917076 0.042679179 0.041154344 0.037760094 0.026438097 0.00950844 -0.0059865648 -0.016691735 -0.020395944][0.52252382 0.508738 0.42784816 0.31458339 0.18936403 0.075545155 -0.00815435 -0.056812946 -0.078505054 -0.082937516 -0.07827387 -0.067063786 -0.049669955 -0.028453577 -0.0039848024][0.59287065 0.58475477 0.49803862 0.36915115 0.21981825 0.077219285 -0.033151437 -0.10078903 -0.13021271 -0.1311239 -0.11476136 -0.087094262 -0.052471437 -0.015870728 0.020849533]]...]
INFO - root - 2017-12-11 10:03:01.553608: step 51610, loss = 0.70, batch loss = 0.65 (29.8 examples/sec; 0.268 sec/batch; 20h:56m:35s remains)
INFO - root - 2017-12-11 10:03:04.222207: step 51620, loss = 0.70, batch loss = 0.64 (28.0 examples/sec; 0.286 sec/batch; 22h:19m:16s remains)
INFO - root - 2017-12-11 10:03:06.864875: step 51630, loss = 0.71, batch loss = 0.66 (31.0 examples/sec; 0.258 sec/batch; 20h:09m:39s remains)
INFO - root - 2017-12-11 10:03:09.445044: step 51640, loss = 0.71, batch loss = 0.65 (30.3 examples/sec; 0.264 sec/batch; 20h:37m:49s remains)
INFO - root - 2017-12-11 10:03:12.049422: step 51650, loss = 0.71, batch loss = 0.65 (30.1 examples/sec; 0.266 sec/batch; 20h:45m:54s remains)
INFO - root - 2017-12-11 10:03:14.682635: step 51660, loss = 0.69, batch loss = 0.63 (29.9 examples/sec; 0.267 sec/batch; 20h:50m:35s remains)
INFO - root - 2017-12-11 10:03:17.363842: step 51670, loss = 0.70, batch loss = 0.64 (31.2 examples/sec; 0.256 sec/batch; 19h:59m:01s remains)
INFO - root - 2017-12-11 10:03:19.994677: step 51680, loss = 0.72, batch loss = 0.66 (30.9 examples/sec; 0.259 sec/batch; 20h:11m:42s remains)
INFO - root - 2017-12-11 10:03:22.687784: step 51690, loss = 0.69, batch loss = 0.63 (29.7 examples/sec; 0.270 sec/batch; 21h:01m:43s remains)
INFO - root - 2017-12-11 10:03:25.330262: step 51700, loss = 0.69, batch loss = 0.63 (31.5 examples/sec; 0.254 sec/batch; 19h:49m:44s remains)
2017-12-11 10:03:25.723423: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.054170877 -0.05462138 -0.056798451 -0.06181515 -0.068061158 -0.073958106 -0.076466508 -0.074224047 -0.06804987 -0.061332554 -0.057521161 -0.058114894 -0.062622629 -0.069760531 -0.078293778][-0.010393891 -0.0050329533 -0.0061818506 -0.013539755 -0.024299152 -0.034644715 -0.038249422 -0.033047065 -0.020929543 -0.0078916382 -0.00049430085 -0.0024027235 -0.013949169 -0.032492343 -0.054840997][0.051193468 0.066607669 0.069727749 0.062693939 0.049584955 0.036883973 0.034737594 0.04597307 0.066976123 0.08932206 0.10249178 0.099854544 0.079217836 0.044043936 0.00010915852][0.13633113 0.16820841 0.18162422 0.18037242 0.16934244 0.15652151 0.1551173 0.16914216 0.19522001 0.22443959 0.24228638 0.23924306 0.2101436 0.15806866 0.089077778][0.24502642 0.30199665 0.33514112 0.34866536 0.34669563 0.33576438 0.33023012 0.33575886 0.353547 0.37863618 0.39430207 0.38952333 0.35516617 0.2915107 0.20029721][0.35668221 0.446004 0.50846392 0.54600513 0.5598259 0.55239111 0.53798938 0.52318591 0.51688242 0.5225125 0.52531433 0.51513356 0.47855669 0.4110164 0.30556968][0.44608355 0.56884009 0.66477972 0.73087686 0.76407081 0.76249367 0.73863286 0.69868428 0.65869021 0.63228589 0.6108076 0.58862418 0.55002725 0.48455617 0.37499136][0.48787585 0.63200063 0.75187653 0.83999807 0.88904727 0.89385736 0.86446035 0.80531377 0.73630738 0.6790303 0.63188404 0.59485364 0.55269456 0.49176741 0.38715139][0.47462291 0.61857516 0.74162787 0.83544046 0.89031941 0.89900506 0.86892718 0.80265164 0.72038519 0.64627409 0.58352494 0.53534925 0.48921737 0.43287179 0.33946151][0.41424206 0.53687853 0.64069426 0.72067922 0.76834196 0.77620929 0.74928761 0.68789679 0.60908777 0.53515226 0.4711813 0.42040318 0.37371644 0.32348239 0.24648917][0.32333407 0.41135877 0.48065549 0.532131 0.56206423 0.56554615 0.54516423 0.4978933 0.43455726 0.37333024 0.31963709 0.27548039 0.23470768 0.19440162 0.13738272][0.21822901 0.26921043 0.30160007 0.32187411 0.33251637 0.33255365 0.32160792 0.29281333 0.2497784 0.2062282 0.16754633 0.13530169 0.10550795 0.077996455 0.040831383][0.11240679 0.13292141 0.1373167 0.13450202 0.13136798 0.12955903 0.12676594 0.1137233 0.089088947 0.062413987 0.038830116 0.019876756 0.0030940783 -0.011014048 -0.031362757][0.018740106 0.019683987 0.0095774271 -0.0043681329 -0.013940798 -0.0168569 -0.015861951 -0.019571602 -0.031027077 -0.044359446 -0.055468004 -0.062950715 -0.068350136 -0.071601354 -0.079711519][-0.05045329 -0.059132472 -0.072552614 -0.087372459 -0.096672364 -0.098808415 -0.096242264 -0.095506445 -0.09884572 -0.10290202 -0.10535222 -0.10543241 -0.10430922 -0.10205909 -0.10367987]]...]
INFO - root - 2017-12-11 10:03:28.331208: step 51710, loss = 0.71, batch loss = 0.65 (29.8 examples/sec; 0.268 sec/batch; 20h:55m:08s remains)
INFO - root - 2017-12-11 10:03:30.982822: step 51720, loss = 0.69, batch loss = 0.63 (31.2 examples/sec; 0.256 sec/batch; 20h:00m:03s remains)
INFO - root - 2017-12-11 10:03:33.664998: step 51730, loss = 0.72, batch loss = 0.66 (31.9 examples/sec; 0.251 sec/batch; 19h:32m:30s remains)
INFO - root - 2017-12-11 10:03:36.302232: step 51740, loss = 0.71, batch loss = 0.65 (30.3 examples/sec; 0.264 sec/batch; 20h:35m:44s remains)
INFO - root - 2017-12-11 10:03:38.945021: step 51750, loss = 0.73, batch loss = 0.67 (30.8 examples/sec; 0.260 sec/batch; 20h:14m:43s remains)
INFO - root - 2017-12-11 10:03:41.526697: step 51760, loss = 0.71, batch loss = 0.65 (31.4 examples/sec; 0.255 sec/batch; 19h:52m:39s remains)
INFO - root - 2017-12-11 10:03:44.160535: step 51770, loss = 0.69, batch loss = 0.64 (30.0 examples/sec; 0.267 sec/batch; 20h:49m:15s remains)
INFO - root - 2017-12-11 10:03:46.804668: step 51780, loss = 0.69, batch loss = 0.63 (30.3 examples/sec; 0.264 sec/batch; 20h:33m:40s remains)
INFO - root - 2017-12-11 10:03:49.439214: step 51790, loss = 0.70, batch loss = 0.65 (30.1 examples/sec; 0.265 sec/batch; 20h:41m:42s remains)
INFO - root - 2017-12-11 10:03:52.124979: step 51800, loss = 0.70, batch loss = 0.64 (30.3 examples/sec; 0.264 sec/batch; 20h:34m:59s remains)
2017-12-11 10:03:52.512503: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.025846466 -0.018492339 -0.0063857236 0.011722405 0.0391934 0.075275667 0.11469292 0.14879712 0.16452529 0.16101289 0.1541352 0.15357219 0.15350251 0.1460091 0.132207][0.003047806 0.019050878 0.036160782 0.055989258 0.086595416 0.13235919 0.1865834 0.23566002 0.26241836 0.2662721 0.26185024 0.25804317 0.24821445 0.22566161 0.19503297][0.058180545 0.088755742 0.11507186 0.13673575 0.16411161 0.20845127 0.26568151 0.31996277 0.35305166 0.36469936 0.36599123 0.3622871 0.34519154 0.30906087 0.26180965][0.13607511 0.18122356 0.21821824 0.24192797 0.26046184 0.29143524 0.33772486 0.38519934 0.41821036 0.43690521 0.44699368 0.44815469 0.42926717 0.38553873 0.32586166][0.22436328 0.27455837 0.31736144 0.34302524 0.35238898 0.36575106 0.39498189 0.43120694 0.46218532 0.48569918 0.50212449 0.50658154 0.48655576 0.43976223 0.37419105][0.29972431 0.33678287 0.37399584 0.39970133 0.40395448 0.40406424 0.41997203 0.44891649 0.48057768 0.50832939 0.5282377 0.53311992 0.51080167 0.46238235 0.39588571][0.33819574 0.34613112 0.36377278 0.38328528 0.38547075 0.38102636 0.39294392 0.42145386 0.45561734 0.48525578 0.50652885 0.512805 0.49148959 0.44514334 0.38242081][0.34417796 0.32212317 0.31414175 0.32004768 0.31706694 0.31026021 0.32112387 0.34862727 0.380983 0.40802872 0.42928332 0.44072425 0.42775136 0.39098546 0.33776239][0.32952818 0.2899726 0.26151666 0.25200027 0.24101 0.23020886 0.23623064 0.25562012 0.27696308 0.293445 0.30991042 0.32701239 0.32802069 0.30894411 0.2727876][0.30358759 0.25929832 0.21976571 0.19899236 0.18136351 0.16600779 0.16419633 0.17142978 0.17767316 0.17924963 0.18519446 0.20141496 0.21243696 0.21064834 0.19368592][0.27830756 0.23903981 0.19831763 0.17263985 0.15217796 0.13464829 0.1273789 0.1258429 0.1208963 0.1098136 0.10246518 0.10921615 0.12022545 0.12765875 0.12440687][0.25292003 0.22720341 0.19470955 0.17211875 0.15525846 0.14140584 0.13402621 0.12803008 0.11582812 0.095756114 0.076388292 0.070109844 0.074240088 0.082778655 0.085241564][0.22535647 0.21781245 0.19972141 0.18563338 0.1769442 0.17203462 0.17055005 0.16524275 0.15011881 0.12550825 0.0986215 0.082097 0.077965856 0.082002595 0.08253435][0.19540071 0.2047205 0.20179529 0.19748378 0.19711061 0.20175126 0.20932497 0.21080415 0.20003483 0.17839253 0.15316334 0.13637586 0.12955515 0.1278342 0.12026752][0.15818366 0.17652276 0.18432797 0.1882785 0.19448386 0.20686294 0.2244951 0.23799998 0.23945042 0.22904508 0.21566264 0.21054314 0.21057044 0.20710796 0.18986897]]...]
INFO - root - 2017-12-11 10:03:55.125164: step 51810, loss = 0.69, batch loss = 0.63 (30.6 examples/sec; 0.262 sec/batch; 20h:23m:27s remains)
INFO - root - 2017-12-11 10:03:57.780552: step 51820, loss = 0.70, batch loss = 0.64 (29.8 examples/sec; 0.269 sec/batch; 20h:56m:21s remains)
INFO - root - 2017-12-11 10:04:00.415111: step 51830, loss = 0.69, batch loss = 0.63 (29.9 examples/sec; 0.268 sec/batch; 20h:53m:37s remains)
INFO - root - 2017-12-11 10:04:03.042617: step 51840, loss = 0.71, batch loss = 0.65 (29.1 examples/sec; 0.275 sec/batch; 21h:26m:18s remains)
INFO - root - 2017-12-11 10:04:05.684397: step 51850, loss = 0.69, batch loss = 0.63 (30.1 examples/sec; 0.266 sec/batch; 20h:42m:34s remains)
INFO - root - 2017-12-11 10:04:08.353192: step 51860, loss = 0.69, batch loss = 0.63 (28.5 examples/sec; 0.281 sec/batch; 21h:54m:33s remains)
INFO - root - 2017-12-11 10:04:10.970106: step 51870, loss = 0.70, batch loss = 0.64 (30.1 examples/sec; 0.266 sec/batch; 20h:41m:55s remains)
INFO - root - 2017-12-11 10:04:13.568730: step 51880, loss = 0.70, batch loss = 0.64 (30.9 examples/sec; 0.259 sec/batch; 20h:09m:44s remains)
INFO - root - 2017-12-11 10:04:16.124553: step 51890, loss = 0.69, batch loss = 0.63 (30.9 examples/sec; 0.259 sec/batch; 20h:10m:26s remains)
INFO - root - 2017-12-11 10:04:18.734694: step 51900, loss = 0.72, batch loss = 0.66 (30.0 examples/sec; 0.266 sec/batch; 20h:45m:38s remains)
2017-12-11 10:04:19.087432: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.16168033 0.17686203 0.19155666 0.19331339 0.1816825 0.17155153 0.17974754 0.18775861 0.17831032 0.15807109 0.13818845 0.11381838 0.074716464 0.029820738 -0.005313599][0.2378861 0.26055557 0.27691329 0.27135155 0.24378721 0.21734858 0.21470554 0.21253704 0.1921628 0.16123822 0.1341996 0.10567614 0.063605525 0.017832227 -0.015539475][0.31004658 0.33950678 0.35497478 0.33999223 0.29660591 0.25690222 0.24772374 0.23870429 0.20934172 0.16684884 0.1284032 0.089903556 0.041311808 -0.0059300847 -0.036034539][0.38245022 0.41677383 0.4301464 0.40803918 0.3543064 0.30813414 0.2996349 0.29042158 0.25626451 0.20162517 0.14644794 0.090878136 0.028400034 -0.026462052 -0.057537325][0.48341611 0.51605177 0.52323294 0.49620986 0.43845692 0.39219251 0.3896496 0.38674971 0.35245138 0.28523245 0.20818712 0.12718649 0.041398171 -0.030779328 -0.070809647][0.59687626 0.6205371 0.61944056 0.593248 0.54431313 0.511899 0.52674913 0.54119754 0.5133276 0.43383145 0.32912135 0.21322526 0.092123307 -0.010131287 -0.0697694][0.68687946 0.70091307 0.69309258 0.67391688 0.64635575 0.64055204 0.68064922 0.71615559 0.69401067 0.59994376 0.4650273 0.31269953 0.15534902 0.022076936 -0.058438998][0.7138108 0.72042507 0.713061 0.70980215 0.714344 0.7431848 0.8095901 0.8618269 0.83969426 0.72946197 0.56839335 0.38896203 0.20690481 0.052988041 -0.041781969][0.64252341 0.64602959 0.64685512 0.66509843 0.70404446 0.76564151 0.85200197 0.91408283 0.89162952 0.774747 0.60512429 0.41998389 0.23435946 0.076464757 -0.023150269][0.47479844 0.47707343 0.48810956 0.52274132 0.5845176 0.66553777 0.75951982 0.82421744 0.80722564 0.70280421 0.55116689 0.3868641 0.22204065 0.079555437 -0.012890488][0.25705194 0.26050967 0.27940828 0.31966293 0.38696879 0.470574 0.56041789 0.62254483 0.61750531 0.54221344 0.42858961 0.30290404 0.17404853 0.06011362 -0.015317223][0.0703107 0.075283341 0.09603706 0.12982029 0.18409772 0.25180739 0.32481495 0.37806788 0.38569155 0.34383357 0.27342767 0.19132039 0.10386551 0.024733739 -0.02821409][-0.041351039 -0.038375758 -0.022803774 -0.0031284648 0.027981794 0.07014025 0.12079483 0.16228482 0.17856821 0.16272058 0.12713392 0.082283087 0.032005448 -0.014603506 -0.045895938][-0.086352795 -0.087910905 -0.081888311 -0.077579141 -0.069127813 -0.051093318 -0.021175725 0.007569422 0.025043147 0.022121919 0.0072398274 -0.012140797 -0.034518138 -0.055123862 -0.0681338][-0.092495106 -0.098167747 -0.1003989 -0.10664308 -0.11259747 -0.11053647 -0.09660431 -0.081221946 -0.070330054 -0.072061725 -0.079183139 -0.085252628 -0.090379044 -0.093051739 -0.091998]]...]
INFO - root - 2017-12-11 10:04:21.682911: step 51910, loss = 0.69, batch loss = 0.63 (32.0 examples/sec; 0.250 sec/batch; 19h:27m:45s remains)
INFO - root - 2017-12-11 10:04:24.309244: step 51920, loss = 0.70, batch loss = 0.65 (31.1 examples/sec; 0.257 sec/batch; 20h:03m:40s remains)
INFO - root - 2017-12-11 10:04:26.925122: step 51930, loss = 0.72, batch loss = 0.66 (31.9 examples/sec; 0.251 sec/batch; 19h:33m:31s remains)
INFO - root - 2017-12-11 10:04:29.509393: step 51940, loss = 0.71, batch loss = 0.65 (30.9 examples/sec; 0.259 sec/batch; 20h:09m:02s remains)
INFO - root - 2017-12-11 10:04:32.234798: step 51950, loss = 0.70, batch loss = 0.64 (30.7 examples/sec; 0.261 sec/batch; 20h:18m:42s remains)
INFO - root - 2017-12-11 10:04:34.880648: step 51960, loss = 0.72, batch loss = 0.66 (28.2 examples/sec; 0.283 sec/batch; 22h:05m:25s remains)
INFO - root - 2017-12-11 10:04:37.525603: step 51970, loss = 0.71, batch loss = 0.66 (30.6 examples/sec; 0.262 sec/batch; 20h:23m:49s remains)
INFO - root - 2017-12-11 10:04:40.174259: step 51980, loss = 0.70, batch loss = 0.64 (29.4 examples/sec; 0.272 sec/batch; 21h:12m:42s remains)
INFO - root - 2017-12-11 10:04:42.830548: step 51990, loss = 0.70, batch loss = 0.64 (29.8 examples/sec; 0.268 sec/batch; 20h:54m:47s remains)
INFO - root - 2017-12-11 10:04:45.493785: step 52000, loss = 0.70, batch loss = 0.64 (27.9 examples/sec; 0.287 sec/batch; 22h:20m:05s remains)
2017-12-11 10:04:45.927799: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.043606509 -0.048712198 -0.049103741 -0.044656307 -0.03922873 -0.03652649 -0.036791049 -0.040874861 -0.049212605 -0.061313715 -0.073752128 -0.081408113 -0.080246642 -0.070886895 -0.056890957][-0.013800566 -0.014472906 -0.0052234144 0.012406846 0.030250298 0.041976769 0.047174815 0.043739397 0.028819373 0.0037075144 -0.024234639 -0.044919472 -0.051424123 -0.0447781 -0.029658392][0.022926712 0.032052454 0.059151269 0.099881507 0.14056346 0.17189002 0.19208246 0.19476755 0.17196122 0.126252 0.0721666 0.028343985 0.005809126 0.0027139473 0.013888478][0.055823941 0.077799246 0.12769383 0.19832532 0.2711401 0.33442125 0.38344786 0.403112 0.37538829 0.30329847 0.21240081 0.13454746 0.086592361 0.066620477 0.069232218][0.081818961 0.11389226 0.18251434 0.27956647 0.38523683 0.48754609 0.57694268 0.624452 0.59796196 0.50004733 0.36896625 0.25133032 0.17180254 0.13026252 0.12197024][0.092663363 0.12751192 0.20353279 0.31437019 0.44354147 0.58153152 0.713211 0.79361469 0.77643335 0.66259259 0.49930942 0.345745 0.2354233 0.17297083 0.15491244][0.097313575 0.12369986 0.18920793 0.29310292 0.42606685 0.58326173 0.74532658 0.85387039 0.85244155 0.73883253 0.56260365 0.38955766 0.26034597 0.18561286 0.16305625][0.11893214 0.12569836 0.16072953 0.23298083 0.34318376 0.49188888 0.65865082 0.77975506 0.79545081 0.69987357 0.53709811 0.37026504 0.24265566 0.17019694 0.15113984][0.1803516 0.16250256 0.15250245 0.17153175 0.2328313 0.34282357 0.48322767 0.59418756 0.62042862 0.55395216 0.42786738 0.29422939 0.19204444 0.13855466 0.13091825][0.29503006 0.25892013 0.20248912 0.15879148 0.15371361 0.20025834 0.2856434 0.36229017 0.38591707 0.34832716 0.26975349 0.18665572 0.12771343 0.10622992 0.11633188][0.45877934 0.41785312 0.32370284 0.21984591 0.14341043 0.11483864 0.12964338 0.15746963 0.16690992 0.1502942 0.11636275 0.085562944 0.073186494 0.085610919 0.11457835][0.64435923 0.60907775 0.48877636 0.33498842 0.19404674 0.094729178 0.041842639 0.0210253 0.011033646 0.0048757936 0.0034733736 0.013689606 0.037888933 0.075714663 0.11916399][0.7986834 0.77398396 0.6398201 0.45258138 0.26532814 0.11468185 0.013079209 -0.044062257 -0.072521761 -0.078692049 -0.065225087 -0.033590872 0.0097704548 0.060216662 0.11133143][0.8691557 0.85499138 0.71925038 0.52016854 0.31287667 0.1384462 0.014118996 -0.061106417 -0.10177286 -0.11473891 -0.10317434 -0.070818909 -0.026392702 0.024594713 0.077231511][0.82222 0.8161605 0.6930545 0.50815964 0.31272683 0.14655577 0.027764527 -0.045817323 -0.089440539 -0.11080126 -0.11231917 -0.095992006 -0.067042284 -0.027786789 0.018751558]]...]
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-SGD-lr0.001-clip50-initfromfixqian30k-fixqian
INFO - root - 2017-12-11 10:04:48.568250: step 52010, loss = 0.69, batch loss = 0.63 (30.5 examples/sec; 0.263 sec/batch; 20h:28m:07s remains)
INFO - root - 2017-12-11 10:04:51.201191: step 52020, loss = 0.67, batch loss = 0.61 (30.5 examples/sec; 0.262 sec/batch; 20h:25m:30s remains)
INFO - root - 2017-12-11 10:04:53.836082: step 52030, loss = 0.70, batch loss = 0.64 (30.1 examples/sec; 0.265 sec/batch; 20h:40m:23s remains)
INFO - root - 2017-12-11 10:04:56.468296: step 52040, loss = 0.68, batch loss = 0.62 (30.3 examples/sec; 0.264 sec/batch; 20h:32m:51s remains)
INFO - root - 2017-12-11 10:04:59.092233: step 52050, loss = 0.71, batch loss = 0.65 (28.4 examples/sec; 0.281 sec/batch; 21h:55m:32s remains)
INFO - root - 2017-12-11 10:05:01.744186: step 52060, loss = 0.71, batch loss = 0.65 (30.7 examples/sec; 0.261 sec/batch; 20h:19m:34s remains)
INFO - root - 2017-12-11 10:05:05.037364: step 52070, loss = 0.70, batch loss = 0.64 (15.4 examples/sec; 0.518 sec/batch; 40h:20m:38s remains)
INFO - root - 2017-12-11 10:05:08.903355: step 52080, loss = 0.71, batch loss = 0.65 (31.0 examples/sec; 0.258 sec/batch; 20h:07m:55s remains)
INFO - root - 2017-12-11 10:05:11.549638: step 52090, loss = 0.70, batch loss = 0.64 (29.7 examples/sec; 0.270 sec/batch; 20h:59m:57s remains)
INFO - root - 2017-12-11 10:05:14.241282: step 52100, loss = 0.71, batch loss = 0.65 (27.3 examples/sec; 0.293 sec/batch; 22h:48m:54s remains)
2017-12-11 10:05:14.610623: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.29292262 0.21860577 0.14244425 0.098083533 0.0845018 0.082914874 0.0784901 0.060374077 0.031767469 0.014628175 0.018636882 0.039302196 0.059363909 0.061582476 0.035735521][0.40944803 0.32663265 0.24334589 0.20291303 0.20281886 0.21652965 0.22066288 0.19783325 0.15642466 0.1287646 0.12937608 0.15170404 0.17188357 0.16623096 0.1203389][0.45573026 0.3803162 0.3101382 0.29358688 0.32584095 0.37178555 0.39693102 0.37572023 0.32152361 0.28093135 0.27508053 0.29605556 0.31334284 0.296292 0.22631328][0.40996286 0.355725 0.3156524 0.33846748 0.41767877 0.51009291 0.56886941 0.55915797 0.49682128 0.44204432 0.42534006 0.43947083 0.44970298 0.4207606 0.32903567][0.29986379 0.26949865 0.26534772 0.3320165 0.46168011 0.60472304 0.70260859 0.71102715 0.64525408 0.57641816 0.54485279 0.54683048 0.54726362 0.5093174 0.40383217][0.17363994 0.16336437 0.19326819 0.30208528 0.48092726 0.67428792 0.81229442 0.84038055 0.77009583 0.67908567 0.61828679 0.5919463 0.5715397 0.52306229 0.41349769][0.072760671 0.073870592 0.12689485 0.26570252 0.48181811 0.71546245 0.88653284 0.93118513 0.85414118 0.73441941 0.632745 0.56580639 0.51717871 0.45832464 0.35404053][0.044667955 0.044074144 0.099121533 0.24257359 0.4682906 0.71638894 0.90123564 0.95351738 0.86935008 0.72241694 0.57999969 0.47280794 0.39823106 0.33422562 0.2451697][0.099013872 0.085659027 0.12245746 0.24391331 0.44765243 0.67831135 0.85197449 0.89919627 0.80926549 0.64509642 0.47449818 0.33910689 0.24800597 0.18514141 0.11685294][0.18373862 0.15288554 0.16079155 0.24394739 0.40293172 0.59035373 0.73175675 0.76411313 0.673414 0.50932783 0.33375308 0.19158617 0.098409928 0.042962734 -0.0038359682][0.22635941 0.18357606 0.166844 0.21148241 0.31900162 0.45248935 0.55232531 0.56714773 0.48309878 0.33675244 0.17962086 0.052309908 -0.028911028 -0.071803465 -0.099121109][0.19326195 0.14723176 0.11599252 0.12956977 0.18920134 0.2696628 0.3290371 0.33043495 0.26243868 0.14994599 0.031268068 -0.062646031 -0.11937046 -0.14469208 -0.1549805][0.11531258 0.073101915 0.036452625 0.028004456 0.049106825 0.085348547 0.11210943 0.106522 0.0603071 -0.011421711 -0.083801508 -0.13734339 -0.1654156 -0.17279372 -0.17020476][0.025622385 -0.008307673 -0.041854549 -0.060401723 -0.06268093 -0.055350035 -0.049492374 -0.056705661 -0.08198633 -0.11737882 -0.14937244 -0.16866142 -0.17383359 -0.16880232 -0.1597278][-0.063373148 -0.0876294 -0.11196444 -0.12956382 -0.1397997 -0.14492261 -0.14785673 -0.15270142 -0.16179416 -0.17165214 -0.17670381 -0.17457233 -0.16677144 -0.15606482 -0.14526814]]...]
INFO - root - 2017-12-11 10:05:17.246469: step 52110, loss = 0.70, batch loss = 0.64 (30.7 examples/sec; 0.260 sec/batch; 20h:15m:54s remains)
INFO - root - 2017-12-11 10:05:19.880284: step 52120, loss = 0.68, batch loss = 0.62 (31.4 examples/sec; 0.255 sec/batch; 19h:52m:10s remains)
INFO - root - 2017-12-11 10:05:22.562365: step 52130, loss = 0.71, batch loss = 0.65 (31.5 examples/sec; 0.254 sec/batch; 19h:45m:54s remains)
INFO - root - 2017-12-11 10:05:25.134751: step 52140, loss = 0.69, batch loss = 0.63 (31.6 examples/sec; 0.253 sec/batch; 19h:41m:57s remains)
INFO - root - 2017-12-11 10:05:27.758422: step 52150, loss = 0.72, batch loss = 0.66 (30.2 examples/sec; 0.265 sec/batch; 20h:38m:06s remains)
INFO - root - 2017-12-11 10:05:30.409955: step 52160, loss = 0.70, batch loss = 0.64 (30.9 examples/sec; 0.259 sec/batch; 20h:11m:13s remains)
INFO - root - 2017-12-11 10:05:32.991823: step 52170, loss = 0.71, batch loss = 0.65 (31.3 examples/sec; 0.256 sec/batch; 19h:55m:52s remains)
INFO - root - 2017-12-11 10:05:35.643936: step 52180, loss = 0.72, batch loss = 0.66 (28.8 examples/sec; 0.278 sec/batch; 21h:39m:15s remains)
INFO - root - 2017-12-11 10:05:38.283009: step 52190, loss = 0.70, batch loss = 0.64 (31.3 examples/sec; 0.256 sec/batch; 19h:55m:58s remains)
INFO - root - 2017-12-11 10:05:40.903746: step 52200, loss = 0.68, batch loss = 0.63 (30.1 examples/sec; 0.266 sec/batch; 20h:40m:32s remains)
2017-12-11 10:05:41.266233: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.1728525 0.15178044 0.12843069 0.11069354 0.099269569 0.095714711 0.09958002 0.10708031 0.11256354 0.11279069 0.11083172 0.10303462 0.086154588 0.058081381 0.021096094][0.18734062 0.17710066 0.16663148 0.16345736 0.16841938 0.18175432 0.19687879 0.20523922 0.20214695 0.18922517 0.1741343 0.15280914 0.12462194 0.088401206 0.044974048][0.1795225 0.18274245 0.19232085 0.21371092 0.24603792 0.28671518 0.32228786 0.33718029 0.32654276 0.29687777 0.26277912 0.22252008 0.17823394 0.13040845 0.079006016][0.16822514 0.18443304 0.21627411 0.26544032 0.32878834 0.40150684 0.46327147 0.48948818 0.47249079 0.42319545 0.36324546 0.29705346 0.23058082 0.1680977 0.10964238][0.18632291 0.21755902 0.27185225 0.34568965 0.434243 0.533193 0.616963 0.6514861 0.62365413 0.54847032 0.45397878 0.352785 0.25675911 0.17669122 0.11463067][0.24373892 0.28932884 0.36094642 0.45138186 0.55632204 0.67597121 0.77921134 0.82100165 0.77890092 0.67203373 0.53477716 0.38846225 0.25454542 0.15410091 0.093036734][0.31933138 0.37653607 0.45740134 0.55474907 0.66771084 0.80161077 0.91925925 0.96524572 0.90754318 0.76925892 0.58998686 0.39896828 0.2285825 0.11144303 0.057695162][0.37339991 0.43409219 0.51355773 0.60784131 0.72142315 0.86216933 0.98742932 1.0348601 0.966716 0.80792367 0.6003744 0.37894326 0.18572429 0.0631962 0.024453569][0.37376559 0.4298127 0.49870551 0.58128971 0.68594182 0.82042527 0.94155383 0.98842865 0.92281365 0.76720184 0.560258 0.33761591 0.14687082 0.035738893 0.018180879][0.32962513 0.37155569 0.42015412 0.4808293 0.56437349 0.67753643 0.781241 0.82351065 0.771231 0.64143437 0.46471059 0.27218321 0.11061476 0.0261559 0.031010889][0.27684253 0.29306176 0.30917811 0.33693042 0.38936079 0.47215548 0.55253357 0.58944565 0.55679309 0.46497467 0.33548847 0.19262731 0.076580882 0.026196618 0.049943689][0.24594636 0.23442762 0.21464075 0.2050114 0.22057162 0.266809 0.31879884 0.34666651 0.3321186 0.27978253 0.202339 0.11603086 0.050003108 0.032069106 0.066438489][0.26121542 0.23159689 0.18372071 0.14049193 0.11821474 0.12362947 0.14175175 0.15419731 0.15052739 0.13147044 0.10016592 0.063883632 0.039903194 0.04565908 0.082690105][0.3097946 0.27577022 0.21462435 0.15002394 0.097532272 0.06649708 0.052504681 0.04713697 0.046432279 0.048944645 0.04944234 0.046796426 0.048895624 0.066648334 0.10151363][0.35730302 0.3311328 0.2716195 0.20041218 0.13076393 0.074788012 0.03641386 0.014112641 0.0084279217 0.017240833 0.030261358 0.040776603 0.051970869 0.072172128 0.10375673]]...]
INFO - root - 2017-12-11 10:05:43.875720: step 52210, loss = 0.70, batch loss = 0.64 (31.1 examples/sec; 0.258 sec/batch; 20h:03m:19s remains)
INFO - root - 2017-12-11 10:05:46.503193: step 52220, loss = 0.70, batch loss = 0.64 (30.9 examples/sec; 0.259 sec/batch; 20h:10m:21s remains)
INFO - root - 2017-12-11 10:05:49.133354: step 52230, loss = 0.70, batch loss = 0.64 (31.5 examples/sec; 0.254 sec/batch; 19h:47m:13s remains)
INFO - root - 2017-12-11 10:05:51.797814: step 52240, loss = 0.71, batch loss = 0.65 (30.6 examples/sec; 0.261 sec/batch; 20h:20m:45s remains)
INFO - root - 2017-12-11 10:05:54.448936: step 52250, loss = 0.69, batch loss = 0.63 (30.6 examples/sec; 0.261 sec/batch; 20h:21m:01s remains)
INFO - root - 2017-12-11 10:05:57.081668: step 52260, loss = 0.70, batch loss = 0.64 (29.7 examples/sec; 0.270 sec/batch; 20h:59m:30s remains)
INFO - root - 2017-12-11 10:05:59.719333: step 52270, loss = 0.71, batch loss = 0.65 (30.2 examples/sec; 0.265 sec/batch; 20h:36m:20s remains)
INFO - root - 2017-12-11 10:06:02.367655: step 52280, loss = 0.69, batch loss = 0.64 (30.2 examples/sec; 0.265 sec/batch; 20h:38m:29s remains)
INFO - root - 2017-12-11 10:06:05.030279: step 52290, loss = 0.70, batch loss = 0.64 (31.3 examples/sec; 0.256 sec/batch; 19h:53m:22s remains)
INFO - root - 2017-12-11 10:06:07.696030: step 52300, loss = 0.72, batch loss = 0.66 (28.7 examples/sec; 0.278 sec/batch; 21h:39m:37s remains)
2017-12-11 10:06:08.065685: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.089709282 0.066860385 0.051796984 0.055621386 0.074697837 0.099185795 0.1244077 0.14276487 0.15440995 0.16107689 0.15690394 0.15190348 0.15279904 0.15225656 0.13857827][0.13419583 0.10584641 0.087501585 0.097343341 0.13021369 0.17105219 0.21154065 0.24135274 0.26189813 0.2769534 0.27839753 0.27738297 0.2796686 0.27571508 0.25312769][0.17598188 0.14633381 0.12682725 0.141995 0.18512422 0.23729178 0.2877467 0.32495943 0.35059345 0.36913067 0.37249246 0.37234542 0.37214378 0.36143088 0.33120686][0.23878796 0.20654237 0.18226515 0.1982473 0.24572013 0.30202085 0.35465103 0.3915596 0.41503835 0.4298892 0.43099457 0.42857012 0.42342889 0.40543324 0.37021661][0.33737502 0.30416247 0.27252939 0.28555891 0.33291084 0.38849872 0.43793118 0.46916509 0.48697114 0.49685884 0.49687368 0.49162343 0.47979391 0.45270374 0.40915444][0.44423267 0.41242632 0.37366572 0.38165349 0.42557618 0.47765237 0.52161413 0.54589528 0.55863965 0.56543386 0.56466419 0.55361682 0.52990407 0.48884821 0.43127394][0.51310456 0.48559651 0.44599918 0.45368624 0.4974207 0.55100304 0.59571081 0.61893493 0.62965471 0.63272989 0.62571931 0.59982908 0.5560441 0.49677634 0.42302838][0.51378459 0.48880705 0.45402762 0.46725711 0.51634055 0.57669544 0.62745446 0.65357 0.66247743 0.65751535 0.63725543 0.59179217 0.52765983 0.45507464 0.3737615][0.43934679 0.41485965 0.3862538 0.40533194 0.45924851 0.5250845 0.58156681 0.61141241 0.62027639 0.61122149 0.58375 0.52877212 0.4570452 0.38371772 0.30613089][0.30988914 0.28610912 0.26437452 0.28706771 0.34190956 0.40847194 0.46687967 0.49967489 0.510334 0.5012449 0.4725638 0.41770506 0.34939218 0.28443122 0.21840267][0.15807764 0.13506044 0.11915223 0.14033671 0.18795374 0.24601138 0.29804963 0.32866928 0.33908564 0.33091724 0.30455604 0.2568447 0.200153 0.15044209 0.10315266][0.022238927 0.00042686847 -0.010668528 0.0060618138 0.04210934 0.086890616 0.12827031 0.15357572 0.16213341 0.1550232 0.13286014 0.095387354 0.053498995 0.020792449 -0.0064020464][-0.063758336 -0.083266817 -0.092014745 -0.082509167 -0.0605449 -0.032368295 -0.0054861368 0.010825422 0.015284476 0.0086804973 -0.0081825554 -0.033669338 -0.059780497 -0.076465718 -0.086172193][-0.10249358 -0.11884058 -0.12651344 -0.12385389 -0.11425667 -0.10094782 -0.087775633 -0.080399096 -0.079595447 -0.0844309 -0.0943038 -0.10738735 -0.11910465 -0.12347844 -0.12211915][-0.10686495 -0.11913147 -0.12574692 -0.12799908 -0.12722501 -0.12452561 -0.12133922 -0.12037627 -0.12184978 -0.12496813 -0.12942939 -0.13395308 -0.13659032 -0.13451426 -0.12879775]]...]
INFO - root - 2017-12-11 10:06:10.683873: step 52310, loss = 0.72, batch loss = 0.66 (30.6 examples/sec; 0.262 sec/batch; 20h:21m:38s remains)
INFO - root - 2017-12-11 10:06:13.384533: step 52320, loss = 0.70, batch loss = 0.64 (30.5 examples/sec; 0.262 sec/batch; 20h:24m:13s remains)
INFO - root - 2017-12-11 10:06:15.997593: step 52330, loss = 0.69, batch loss = 0.63 (29.1 examples/sec; 0.275 sec/batch; 21h:25m:27s remains)
INFO - root - 2017-12-11 10:06:18.594367: step 52340, loss = 0.70, batch loss = 0.64 (31.4 examples/sec; 0.255 sec/batch; 19h:49m:13s remains)
INFO - root - 2017-12-11 10:06:21.222497: step 52350, loss = 0.68, batch loss = 0.62 (31.2 examples/sec; 0.256 sec/batch; 19h:56m:36s remains)
INFO - root - 2017-12-11 10:06:23.859216: step 52360, loss = 0.71, batch loss = 0.65 (30.9 examples/sec; 0.259 sec/batch; 20h:10m:15s remains)
INFO - root - 2017-12-11 10:06:26.524299: step 52370, loss = 0.70, batch loss = 0.64 (30.1 examples/sec; 0.266 sec/batch; 20h:42m:22s remains)
INFO - root - 2017-12-11 10:06:29.218789: step 52380, loss = 0.69, batch loss = 0.64 (30.2 examples/sec; 0.265 sec/batch; 20h:37m:33s remains)
INFO - root - 2017-12-11 10:06:31.820656: step 52390, loss = 0.69, batch loss = 0.63 (30.3 examples/sec; 0.264 sec/batch; 20h:30m:56s remains)
INFO - root - 2017-12-11 10:06:34.500368: step 52400, loss = 0.70, batch loss = 0.64 (30.3 examples/sec; 0.264 sec/batch; 20h:31m:16s remains)
2017-12-11 10:06:34.877429: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0073869633 -0.023150919 -0.033365745 -0.039481297 -0.042613786 -0.040688843 -0.0327508 -0.018374776 0.00471484 0.0408735 0.091225393 0.15018377 0.20890297 0.25889459 0.29573804][-0.0022648773 -0.013933411 -0.020456109 -0.02258415 -0.021572851 -0.016282784 -0.0072555318 0.0047022174 0.023579523 0.058287613 0.11198911 0.17675139 0.24071833 0.29485092 0.33595678][0.0015088654 -0.0013338623 0.0017741929 0.010162823 0.020833421 0.032287117 0.042594232 0.050139464 0.060691785 0.087660991 0.13754712 0.20104086 0.26322195 0.31395671 0.35237539][0.0070958408 0.015813492 0.03420376 0.059635304 0.08533603 0.10630074 0.11900739 0.12120542 0.12078608 0.13659467 0.17893091 0.23725416 0.29285711 0.333837 0.3622115][0.018249558 0.039107662 0.076985389 0.12545516 0.17194504 0.2067235 0.22404951 0.22066891 0.20728886 0.20964256 0.24235912 0.29425338 0.3420091 0.37074432 0.3836498][0.042708568 0.073044822 0.12833481 0.19913559 0.26758814 0.31932446 0.34538496 0.33944309 0.31413221 0.30228016 0.32339552 0.36680397 0.40490454 0.41977909 0.41544539][0.077036761 0.11227435 0.1782382 0.26505989 0.35141203 0.42030892 0.4591442 0.45628291 0.423469 0.39887908 0.40644142 0.43714517 0.46171731 0.46209747 0.44332883][0.11207635 0.14615373 0.21262336 0.30481493 0.40112516 0.48353282 0.53570682 0.54077697 0.507639 0.4749946 0.46881968 0.48214686 0.48877519 0.47476703 0.44694483][0.13687222 0.16365342 0.22145212 0.30882338 0.40567425 0.49313712 0.55229789 0.5645169 0.53525674 0.49914455 0.47984007 0.47277907 0.4596031 0.43402341 0.40396729][0.13923246 0.15365253 0.19656703 0.27057958 0.35753065 0.4381642 0.49393767 0.50865126 0.48499835 0.45006368 0.42202139 0.3982521 0.36976352 0.33833647 0.31201953][0.11470791 0.1142334 0.13889784 0.19335933 0.26162004 0.32553211 0.36965895 0.38246977 0.36457166 0.3341696 0.3030664 0.26999339 0.23366809 0.20156017 0.18170311][0.070415668 0.055230327 0.062648252 0.097254492 0.14558992 0.19179641 0.22366214 0.23345906 0.22016175 0.19461593 0.16363883 0.12724157 0.089950241 0.061717492 0.049267095][0.023239767 -0.0025775263 -0.0067048036 0.014361502 0.04989573 0.085953467 0.11113326 0.11969225 0.11019658 0.088937394 0.060090695 0.025927939 -0.00482706 -0.023179462 -0.025976732][-0.012034683 -0.042764712 -0.051401127 -0.036039595 -0.0043055778 0.031499978 0.059002422 0.071611807 0.067244358 0.050351128 0.024992211 -0.0034049822 -0.023718756 -0.029235853 -0.020906454][-0.03014644 -0.05987408 -0.06734395 -0.052326404 -0.018798398 0.023158742 0.059803545 0.081718527 0.085346587 0.0741848 0.053324778 0.03155113 0.021456124 0.028229274 0.04800136]]...]
INFO - root - 2017-12-11 10:06:37.561452: step 52410, loss = 0.70, batch loss = 0.64 (30.6 examples/sec; 0.261 sec/batch; 20h:18m:58s remains)
INFO - root - 2017-12-11 10:06:40.202260: step 52420, loss = 0.70, batch loss = 0.64 (29.6 examples/sec; 0.271 sec/batch; 21h:02m:58s remains)
INFO - root - 2017-12-11 10:06:42.785930: step 52430, loss = 0.69, batch loss = 0.63 (30.2 examples/sec; 0.265 sec/batch; 20h:35m:12s remains)
INFO - root - 2017-12-11 10:06:45.463076: step 52440, loss = 0.69, batch loss = 0.64 (29.8 examples/sec; 0.268 sec/batch; 20h:51m:52s remains)
INFO - root - 2017-12-11 10:06:48.079909: step 52450, loss = 0.69, batch loss = 0.63 (29.5 examples/sec; 0.271 sec/batch; 21h:03m:57s remains)
INFO - root - 2017-12-11 10:06:50.703451: step 52460, loss = 0.70, batch loss = 0.64 (29.7 examples/sec; 0.270 sec/batch; 20h:58m:01s remains)
INFO - root - 2017-12-11 10:06:53.357414: step 52470, loss = 0.69, batch loss = 0.63 (30.1 examples/sec; 0.266 sec/batch; 20h:41m:48s remains)
INFO - root - 2017-12-11 10:06:56.029651: step 52480, loss = 0.71, batch loss = 0.65 (27.1 examples/sec; 0.296 sec/batch; 22h:59m:52s remains)
INFO - root - 2017-12-11 10:06:58.636316: step 52490, loss = 0.69, batch loss = 0.63 (31.2 examples/sec; 0.256 sec/batch; 19h:57m:00s remains)
INFO - root - 2017-12-11 10:07:01.344144: step 52500, loss = 0.70, batch loss = 0.64 (31.3 examples/sec; 0.256 sec/batch; 19h:54m:14s remains)
2017-12-11 10:07:01.732752: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.0061844182 0.0069132084 0.0054350705 0.0061670649 0.015960095 0.036729857 0.064846963 0.098059133 0.13489316 0.16828626 0.18812779 0.19401054 0.1889596 0.17338203 0.15277553][-0.00079298404 -0.0074571306 -0.010167671 -0.0034674113 0.017809477 0.052897993 0.093682922 0.1332389 0.16838208 0.19540519 0.2094048 0.21426018 0.21440315 0.20910038 0.20096788][-0.0025806658 -0.017555803 -0.022474267 -0.011008111 0.019814508 0.066428147 0.11635238 0.1576732 0.18541132 0.19959974 0.20144081 0.19997665 0.20171161 0.20586881 0.21247123][0.005565674 -0.016263405 -0.024029734 -0.010802809 0.024570283 0.077102713 0.13161322 0.17219785 0.19179668 0.19269261 0.18102308 0.1698426 0.1678426 0.17562495 0.19166294][0.025138224 -0.00056483463 -0.010598008 0.0022456781 0.038261 0.092815325 0.14981712 0.19018991 0.20390506 0.19395097 0.17011997 0.14823526 0.13903141 0.14514787 0.1644641][0.050987042 0.024939317 0.013876141 0.025278017 0.059604339 0.11343724 0.17095274 0.21131173 0.22192122 0.2056367 0.1740554 0.14430679 0.12863743 0.13105252 0.14954051][0.074283749 0.051013306 0.041469503 0.052935887 0.086603329 0.13982628 0.19742104 0.23858635 0.24970599 0.2330793 0.19967845 0.16641153 0.14674567 0.14538172 0.15997447][0.085691273 0.068578415 0.065499775 0.082275249 0.12028567 0.17581077 0.23427714 0.27698651 0.2907052 0.27658942 0.2434245 0.20888947 0.18775547 0.18425544 0.19455607][0.081693374 0.073171891 0.079951338 0.10607222 0.15252058 0.21269356 0.27198735 0.31484932 0.3293362 0.31555513 0.28086847 0.2451164 0.22435841 0.22103252 0.22894917][0.06675113 0.065299682 0.080610976 0.11463692 0.16801618 0.23110212 0.28875777 0.32764143 0.33746278 0.31873128 0.27965766 0.24282165 0.22450687 0.22413154 0.23255558][0.049539536 0.052532908 0.07130453 0.106566 0.15982141 0.21995415 0.27099568 0.3000026 0.29885417 0.27042559 0.22561164 0.18900827 0.17521358 0.18008338 0.19152777][0.040931139 0.046214297 0.064070515 0.094570942 0.14058875 0.19160753 0.23125172 0.24569526 0.22919233 0.18848817 0.13804081 0.10274569 0.094117463 0.10494328 0.12119236][0.046976615 0.054657761 0.070478104 0.094036959 0.12847675 0.16577937 0.19057839 0.188151 0.15539876 0.10334565 0.049717192 0.017620949 0.01540877 0.033861618 0.057904221][0.068260975 0.078757979 0.092229992 0.10770202 0.12817369 0.14995763 0.16015051 0.14336668 0.098940678 0.040970571 -0.011013696 -0.037185542 -0.031393368 -0.003479725 0.031104738][0.10231827 0.11592691 0.12656794 0.13363926 0.14032367 0.14808659 0.14688237 0.12086011 0.071098424 0.013090407 -0.032992564 -0.050770815 -0.036111008 0.0016344776 0.047127295]]...]
INFO - root - 2017-12-11 10:07:04.375207: step 52510, loss = 0.69, batch loss = 0.63 (31.3 examples/sec; 0.256 sec/batch; 19h:52m:25s remains)
INFO - root - 2017-12-11 10:07:07.045301: step 52520, loss = 0.69, batch loss = 0.63 (30.7 examples/sec; 0.261 sec/batch; 20h:16m:19s remains)
INFO - root - 2017-12-11 10:07:09.689559: step 52530, loss = 0.70, batch loss = 0.64 (30.4 examples/sec; 0.263 sec/batch; 20h:26m:24s remains)
INFO - root - 2017-12-11 10:07:12.316865: step 52540, loss = 0.70, batch loss = 0.64 (30.3 examples/sec; 0.264 sec/batch; 20h:29m:57s remains)
INFO - root - 2017-12-11 10:07:14.960565: step 52550, loss = 0.71, batch loss = 0.65 (28.5 examples/sec; 0.281 sec/batch; 21h:51m:48s remains)
INFO - root - 2017-12-11 10:07:17.627583: step 52560, loss = 0.67, batch loss = 0.61 (29.3 examples/sec; 0.273 sec/batch; 21h:15m:10s remains)
INFO - root - 2017-12-11 10:07:20.208425: step 52570, loss = 0.71, batch loss = 0.65 (31.2 examples/sec; 0.256 sec/batch; 19h:56m:09s remains)
INFO - root - 2017-12-11 10:07:22.847641: step 52580, loss = 0.71, batch loss = 0.65 (30.4 examples/sec; 0.263 sec/batch; 20h:28m:55s remains)
INFO - root - 2017-12-11 10:07:25.478702: step 52590, loss = 0.71, batch loss = 0.65 (30.9 examples/sec; 0.259 sec/batch; 20h:09m:04s remains)
INFO - root - 2017-12-11 10:07:28.121063: step 52600, loss = 0.70, batch loss = 0.64 (29.8 examples/sec; 0.268 sec/batch; 20h:50m:34s remains)
2017-12-11 10:07:28.523897: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.16384137 0.1359137 0.11692999 0.10985797 0.10429224 0.092431523 0.073996246 0.052386414 0.033320617 0.018528273 0.006908928 -0.0036160261 -0.012958752 -0.017510824 -0.014704125][0.18588059 0.15063928 0.12067744 0.10340106 0.090957932 0.076015361 0.057777561 0.03828758 0.02088796 0.0052689668 -0.010141791 -0.026599417 -0.040940963 -0.047119174 -0.042137064][0.18642545 0.15194468 0.11878219 0.096591033 0.08107578 0.067354254 0.055460159 0.04739695 0.042679183 0.036544435 0.024007326 0.0025300886 -0.021715228 -0.038949061 -0.042447705][0.18167804 0.15811424 0.13320512 0.1157651 0.10449171 0.098290764 0.099078245 0.10960244 0.12490807 0.13451347 0.12804954 0.10019223 0.059667632 0.021315491 -0.0014394608][0.1895425 0.18441595 0.17720002 0.17413023 0.17560601 0.18378706 0.20167351 0.23207918 0.26545656 0.2866168 0.28086475 0.2410489 0.17899436 0.11512153 0.06979847][0.20615198 0.22168262 0.23681003 0.25559559 0.27863458 0.30878371 0.34682447 0.39303187 0.43335804 0.45154288 0.43333209 0.37365085 0.2886602 0.20305331 0.1410687][0.20433256 0.23569025 0.27237073 0.31718555 0.36914188 0.42804372 0.48747027 0.54131585 0.57252485 0.56911588 0.52411413 0.4395363 0.33656755 0.24069633 0.174276][0.15892923 0.19783318 0.25176233 0.32284954 0.406366 0.4949899 0.5715251 0.62179375 0.62866676 0.58935928 0.51031524 0.40414932 0.29542068 0.20535322 0.14865988][0.069847658 0.10901613 0.17427365 0.26676595 0.37706161 0.48841298 0.57310867 0.6104424 0.58623344 0.50874805 0.39953804 0.2819812 0.18076444 0.1090123 0.070924439][-0.03452697 -0.00060742954 0.067866623 0.17058165 0.29430947 0.41401938 0.49552298 0.51520276 0.46388111 0.35923362 0.23421635 0.11929562 0.035725214 -0.012539017 -0.030930353][-0.11250861 -0.087724522 -0.026129136 0.07071501 0.18843855 0.29811311 0.36530533 0.3678633 0.30215663 0.19081941 0.07048548 -0.027802816 -0.08868885 -0.11514555 -0.11847199][-0.14255658 -0.12847635 -0.082117014 -0.0057868273 0.087800615 0.17182997 0.21774492 0.2084914 0.14438474 0.0476979 -0.048952837 -0.12031206 -0.15766455 -0.16739307 -0.16168569][-0.13252322 -0.12826371 -0.100835 -0.052631147 0.0069752391 0.058015805 0.081480443 0.066998735 0.016961094 -0.050653793 -0.11223476 -0.15201655 -0.16742103 -0.16534664 -0.15560505][-0.10711916 -0.11049904 -0.10049517 -0.079151295 -0.052044045 -0.030895604 -0.025279563 -0.039378379 -0.070102818 -0.10548396 -0.13218027 -0.14376491 -0.1420635 -0.13293442 -0.12258768][-0.086826138 -0.094982617 -0.097107872 -0.09491951 -0.0906902 -0.089502685 -0.0944725 -0.10578582 -0.12028684 -0.13135295 -0.1338075 -0.12747751 -0.11609714 -0.10394239 -0.094253272]]...]
INFO - root - 2017-12-11 10:07:31.183033: step 52610, loss = 0.70, batch loss = 0.64 (30.9 examples/sec; 0.259 sec/batch; 20h:06m:41s remains)
INFO - root - 2017-12-11 10:07:33.863328: step 52620, loss = 0.69, batch loss = 0.63 (30.7 examples/sec; 0.260 sec/batch; 20h:14m:40s remains)
INFO - root - 2017-12-11 10:07:36.480479: step 52630, loss = 0.70, batch loss = 0.64 (28.0 examples/sec; 0.286 sec/batch; 22h:14m:15s remains)
INFO - root - 2017-12-11 10:07:39.366010: step 52640, loss = 0.69, batch loss = 0.63 (18.0 examples/sec; 0.444 sec/batch; 34h:31m:16s remains)
INFO - root - 2017-12-11 10:07:43.711146: step 52650, loss = 0.71, batch loss = 0.65 (21.3 examples/sec; 0.376 sec/batch; 29h:13m:48s remains)
INFO - root - 2017-12-11 10:07:49.155311: step 52660, loss = 0.70, batch loss = 0.64 (14.7 examples/sec; 0.543 sec/batch; 42h:14m:32s remains)
INFO - root - 2017-12-11 10:07:54.657805: step 52670, loss = 0.68, batch loss = 0.62 (14.6 examples/sec; 0.549 sec/batch; 42h:38m:45s remains)
INFO - root - 2017-12-11 10:08:00.151956: step 52680, loss = 0.69, batch loss = 0.63 (14.5 examples/sec; 0.551 sec/batch; 42h:50m:08s remains)
INFO - root - 2017-12-11 10:08:05.693348: step 52690, loss = 0.70, batch loss = 0.64 (14.7 examples/sec; 0.544 sec/batch; 42h:18m:26s remains)
INFO - root - 2017-12-11 10:08:11.196617: step 52700, loss = 0.69, batch loss = 0.64 (14.4 examples/sec; 0.557 sec/batch; 43h:17m:37s remains)
2017-12-11 10:08:11.813442: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.41145259 0.3605729 0.3244012 0.30370966 0.27943379 0.24816926 0.22854863 0.2336856 0.25024161 0.26871732 0.27308917 0.25078896 0.19310455 0.10637748 0.01560936][0.48611042 0.4749665 0.47959366 0.48606679 0.46421468 0.41761744 0.37640092 0.36218426 0.35952452 0.35862964 0.34571382 0.30612618 0.23092894 0.12789644 0.026058687][0.57105756 0.59952837 0.6413936 0.66749918 0.64068323 0.57689321 0.51610172 0.4822247 0.45424318 0.42564911 0.38804275 0.32752821 0.23627363 0.12354285 0.02006424][0.6564607 0.71329892 0.77847385 0.81143594 0.774661 0.69983941 0.6321758 0.58990455 0.54512745 0.49339092 0.43266633 0.35068056 0.2421312 0.11880267 0.012608293][0.71901357 0.79296875 0.870334 0.90581858 0.86567944 0.79618728 0.74065614 0.70580482 0.65922463 0.59536016 0.51626354 0.41127342 0.27943146 0.13714193 0.019217011][0.74884474 0.82816136 0.90951347 0.94889235 0.91856217 0.87166339 0.84293449 0.82488042 0.7845158 0.71422136 0.61846519 0.48978043 0.33246449 0.16912261 0.03702347][0.7306878 0.80688506 0.88608515 0.93145293 0.92339635 0.91128558 0.91557574 0.91612077 0.88204074 0.80581981 0.69387949 0.54472429 0.36878812 0.19381003 0.055566806][0.65433043 0.72634971 0.80218542 0.854412 0.87315446 0.89663506 0.93025523 0.94777054 0.92234856 0.84663248 0.72759265 0.56847703 0.38503549 0.20860927 0.070805088][0.50824517 0.57984424 0.65575433 0.71745574 0.7627629 0.81532794 0.86744642 0.89540297 0.88006419 0.81392658 0.702137 0.54911476 0.3737109 0.2076081 0.077112488][0.32669389 0.39344218 0.46672562 0.53461474 0.59904212 0.66859007 0.72568619 0.75339139 0.7424022 0.68781036 0.5919981 0.45886984 0.30762306 0.1669206 0.056365732][0.15893297 0.21018293 0.27005762 0.33119887 0.39735782 0.46575904 0.514302 0.53310817 0.52132058 0.47789326 0.40314955 0.30045515 0.18688229 0.084868088 0.0066860584][0.025516557 0.055855907 0.094447419 0.13723531 0.18868218 0.24158244 0.27509809 0.28460908 0.27394986 0.2440335 0.19341163 0.12482776 0.052459124 -0.0087945946 -0.052414108][-0.064437121 -0.051303294 -0.031580381 -0.0086751692 0.02167033 0.053769372 0.072257221 0.07517565 0.067729086 0.050906871 0.022198543 -0.016548192 -0.054412797 -0.082938448 -0.099409357][-0.11104647 -0.1092656 -0.1026988 -0.095468841 -0.084013566 -0.07053452 -0.063685276 -0.06463147 -0.068774231 -0.0757439 -0.088224493 -0.10524155 -0.11918923 -0.12603113 -0.12556222][-0.12140746 -0.12758718 -0.13052487 -0.13442865 -0.13652609 -0.13649897 -0.13774574 -0.14115974 -0.14362872 -0.14499635 -0.14742325 -0.15091398 -0.15115018 -0.1464238 -0.13752437]]...]
INFO - root - 2017-12-11 10:08:17.322229: step 52710, loss = 0.68, batch loss = 0.63 (14.5 examples/sec; 0.551 sec/batch; 42h:51m:02s remains)
INFO - root - 2017-12-11 10:08:22.758855: step 52720, loss = 0.69, batch loss = 0.63 (15.0 examples/sec; 0.532 sec/batch; 41h:22m:56s remains)
INFO - root - 2017-12-11 10:08:28.229807: step 52730, loss = 0.68, batch loss = 0.62 (16.6 examples/sec; 0.481 sec/batch; 37h:21m:44s remains)
INFO - root - 2017-12-11 10:08:33.506525: step 52740, loss = 0.70, batch loss = 0.64 (14.7 examples/sec; 0.544 sec/batch; 42h:16m:28s remains)
INFO - root - 2017-12-11 10:08:39.033331: step 52750, loss = 0.71, batch loss = 0.65 (14.1 examples/sec; 0.569 sec/batch; 44h:11m:06s remains)
INFO - root - 2017-12-11 10:08:44.547137: step 52760, loss = 0.70, batch loss = 0.64 (14.9 examples/sec; 0.537 sec/batch; 41h:45m:39s remains)
INFO - root - 2017-12-11 10:08:50.071788: step 52770, loss = 0.70, batch loss = 0.64 (14.2 examples/sec; 0.562 sec/batch; 43h:41m:36s remains)
INFO - root - 2017-12-11 10:08:55.519517: step 52780, loss = 0.71, batch loss = 0.65 (14.5 examples/sec; 0.553 sec/batch; 42h:59m:46s remains)
INFO - root - 2017-12-11 10:09:01.104085: step 52790, loss = 0.69, batch loss = 0.63 (14.5 examples/sec; 0.551 sec/batch; 42h:46m:36s remains)
INFO - root - 2017-12-11 10:09:06.612936: step 52800, loss = 0.68, batch loss = 0.63 (15.0 examples/sec; 0.535 sec/batch; 41h:33m:36s remains)
2017-12-11 10:09:07.298646: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.012832616 0.013578853 0.045780025 0.076997593 0.098916404 0.10710944 0.10179197 0.086698368 0.066250332 0.046038561 0.034507934 0.03451287 0.042857513 0.053752664 0.066753618][0.0076369024 0.045583129 0.0926502 0.14172804 0.1812986 0.20278406 0.20364811 0.18593132 0.15561841 0.12405647 0.10720875 0.11073105 0.12825379 0.14808494 0.1654458][0.027471665 0.078177 0.1421452 0.21158485 0.27156925 0.30953687 0.31993815 0.3032262 0.26648933 0.22581339 0.20495065 0.21317896 0.24174348 0.27212623 0.29317978][0.032939378 0.090086229 0.16493292 0.24882939 0.32478407 0.3773357 0.39810494 0.38584182 0.34897456 0.3068794 0.28770262 0.30246076 0.34123614 0.38066506 0.40295047][0.030176835 0.086864278 0.1641867 0.25344071 0.33720952 0.39854252 0.42714643 0.41997468 0.38645807 0.34752554 0.332048 0.35075405 0.39320314 0.43460545 0.453776][0.026434846 0.081549942 0.15949708 0.25307566 0.34527257 0.41788003 0.45756772 0.45782813 0.42566592 0.3825717 0.35899395 0.36775675 0.40078956 0.43428534 0.44683492][0.026919222 0.083490074 0.16339409 0.26104707 0.36165902 0.44777578 0.50344461 0.517518 0.49038276 0.44076422 0.40025014 0.38725355 0.40072903 0.42036965 0.42542252][0.035166018 0.0963484 0.17826863 0.27489358 0.37506282 0.46600533 0.53243148 0.56045181 0.54373467 0.49460647 0.44190603 0.40921146 0.40545163 0.41592452 0.41960073][0.043768466 0.10794468 0.18845175 0.277493 0.36713374 0.451362 0.51871508 0.55539685 0.55018836 0.50744295 0.4500795 0.40511534 0.39216432 0.40314206 0.41515431][0.047874622 0.1118506 0.18833806 0.26786518 0.3438617 0.41595456 0.47701123 0.51577866 0.51862776 0.482687 0.42473087 0.37321889 0.3559778 0.37061116 0.39361373][0.041853197 0.10086679 0.16996427 0.23925894 0.3024148 0.36224532 0.41469494 0.45158052 0.45878324 0.42892745 0.37387735 0.32098114 0.30230191 0.32020593 0.35305244][0.024464326 0.0731605 0.13031201 0.18591684 0.2336943 0.27822691 0.31780341 0.34753358 0.35575119 0.33407074 0.28977305 0.24475282 0.22992016 0.25068027 0.2892876][-5.8982852e-05 0.03483149 0.076348774 0.11512607 0.14554289 0.17289393 0.19698773 0.21541026 0.22081648 0.20644794 0.17608365 0.14458258 0.13801408 0.1622048 0.20307258][-0.028475774 -0.009179322 0.0153233 0.037478045 0.053186607 0.067310385 0.079547919 0.088309377 0.089691713 0.079485051 0.060012173 0.040947266 0.042264931 0.068278506 0.10707502][-0.052232273 -0.04583583 -0.035164915 -0.025841806 -0.020661622 -0.015988225 -0.012237095 -0.010351786 -0.01172087 -0.018600121 -0.029395288 -0.038187813 -0.0318471 -0.0083927969 0.022152936]]...]
INFO - root - 2017-12-11 10:09:12.871472: step 52810, loss = 0.70, batch loss = 0.64 (14.2 examples/sec; 0.563 sec/batch; 43h:43m:33s remains)
INFO - root - 2017-12-11 10:09:18.093113: step 52820, loss = 0.71, batch loss = 0.65 (14.8 examples/sec; 0.541 sec/batch; 42h:01m:56s remains)
INFO - root - 2017-12-11 10:09:23.595347: step 52830, loss = 0.70, batch loss = 0.64 (14.7 examples/sec; 0.546 sec/batch; 42h:24m:03s remains)
INFO - root - 2017-12-11 10:09:29.109386: step 52840, loss = 0.71, batch loss = 0.65 (14.3 examples/sec; 0.559 sec/batch; 43h:24m:29s remains)
INFO - root - 2017-12-11 10:09:34.613734: step 52850, loss = 0.70, batch loss = 0.64 (14.8 examples/sec; 0.542 sec/batch; 42h:06m:55s remains)
INFO - root - 2017-12-11 10:09:40.110808: step 52860, loss = 0.67, batch loss = 0.61 (14.7 examples/sec; 0.545 sec/batch; 42h:21m:46s remains)
INFO - root - 2017-12-11 10:09:45.575757: step 52870, loss = 0.69, batch loss = 0.64 (14.6 examples/sec; 0.547 sec/batch; 42h:31m:25s remains)
INFO - root - 2017-12-11 10:09:51.119142: step 52880, loss = 0.69, batch loss = 0.63 (14.3 examples/sec; 0.558 sec/batch; 43h:19m:46s remains)
INFO - root - 2017-12-11 10:09:56.594198: step 52890, loss = 0.70, batch loss = 0.64 (15.0 examples/sec; 0.533 sec/batch; 41h:23m:20s remains)
INFO - root - 2017-12-11 10:10:01.878609: step 52900, loss = 0.70, batch loss = 0.65 (14.4 examples/sec; 0.557 sec/batch; 43h:13m:41s remains)
2017-12-11 10:10:02.436950: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.04038056 -0.034053508 -0.021539629 -0.0038349882 0.016888672 0.034845512 0.044050843 0.043091446 0.034438413 0.022447862 0.012401164 0.0086513953 0.010250813 0.013466162 0.016238438][-0.039977603 -0.029504664 -0.0092658987 0.019920472 0.054577239 0.086146653 0.10546851 0.10849292 0.096958093 0.076939367 0.057088129 0.045050386 0.041500282 0.042282511 0.043964837][-0.038070612 -0.023583109 0.0037956678 0.043726549 0.0916848 0.1365488 0.16637076 0.17451954 0.16192432 0.13561426 0.10717258 0.087545946 0.0779961 0.074069358 0.071479544][-0.036417954 -0.018493798 0.014175961 0.061598297 0.11832148 0.17157045 0.20775108 0.21894579 0.20628254 0.17698927 0.14407218 0.11990681 0.10553301 0.096475944 0.088163838][-0.034554042 -0.013786805 0.02278851 0.075056933 0.13649674 0.19323994 0.23134141 0.24285105 0.22998166 0.19998406 0.16574228 0.13901383 0.12079607 0.10787123 0.095829412][-0.032619935 -0.0087601021 0.032623507 0.091675207 0.16062242 0.22407819 0.26750639 0.28194827 0.26990643 0.2380151 0.19970772 0.16680466 0.14119659 0.12202414 0.10588878][-0.031634849 -0.0058444291 0.039626274 0.10524914 0.18283825 0.25602821 0.3092055 0.33114743 0.32258803 0.28915998 0.24532349 0.20329979 0.16636109 0.13681997 0.11376654][-0.032254893 -0.0068725781 0.03899603 0.10594047 0.18651383 0.26498273 0.32545856 0.3548032 0.35074022 0.31835508 0.2730462 0.22665684 0.18180792 0.143214 0.1139385][-0.032680288 -0.0092659341 0.033797927 0.096578039 0.17256775 0.24810006 0.30852678 0.34071517 0.33998793 0.31154794 0.27101681 0.22790004 0.18227051 0.13958023 0.10726683][-0.032504395 -0.010732976 0.029531103 0.087765411 0.15764213 0.22716585 0.2832076 0.31424755 0.31436774 0.28906572 0.25435933 0.2171948 0.17479341 0.13165678 0.098888323][-0.03262458 -0.012002682 0.026816964 0.082808129 0.14913182 0.21421874 0.26545942 0.29311982 0.29076222 0.26420093 0.23032635 0.1954651 0.15620081 0.11496524 0.083942458][-0.034209397 -0.01617243 0.019454492 0.070860051 0.13096087 0.18863285 0.23183438 0.25322914 0.24653962 0.21776396 0.18367372 0.15108268 0.11754888 0.083425187 0.059094653][-0.037133493 -0.022882149 0.0066192728 0.04884072 0.097267047 0.14192803 0.17243779 0.18483917 0.17419016 0.14621294 0.11550986 0.088949725 0.065114081 0.042762186 0.029095687][-0.039420541 -0.028551783 -0.0055149766 0.026436444 0.061682034 0.091590427 0.10779024 0.11012888 0.096630365 0.072507128 0.048549518 0.030034957 0.016108112 0.0045342371 -6.7440036e-05][-0.040799215 -0.032072335 -0.013858829 0.010304758 0.035786081 0.054894868 0.060158342 0.053685606 0.03746815 0.016757222 -0.0013932446 -0.013908307 -0.021198502 -0.025634065 -0.024329254]]...]
INFO - root - 2017-12-11 10:10:07.957201: step 52910, loss = 0.71, batch loss = 0.65 (14.3 examples/sec; 0.560 sec/batch; 43h:31m:46s remains)
INFO - root - 2017-12-11 10:10:13.400936: step 52920, loss = 0.70, batch loss = 0.64 (15.0 examples/sec; 0.533 sec/batch; 41h:25m:24s remains)
INFO - root - 2017-12-11 10:10:18.892691: step 52930, loss = 0.69, batch loss = 0.63 (14.8 examples/sec; 0.542 sec/batch; 42h:06m:27s remains)
INFO - root - 2017-12-11 10:10:24.425526: step 52940, loss = 0.70, batch loss = 0.64 (14.4 examples/sec; 0.554 sec/batch; 43h:02m:16s remains)
INFO - root - 2017-12-11 10:10:29.914393: step 52950, loss = 0.70, batch loss = 0.64 (14.4 examples/sec; 0.554 sec/batch; 43h:00m:46s remains)
INFO - root - 2017-12-11 10:10:35.474695: step 52960, loss = 0.71, batch loss = 0.65 (13.9 examples/sec; 0.577 sec/batch; 44h:48m:32s remains)
INFO - root - 2017-12-11 10:10:40.938077: step 52970, loss = 0.69, batch loss = 0.64 (14.9 examples/sec; 0.538 sec/batch; 41h:46m:08s remains)
INFO - root - 2017-12-11 10:10:46.176433: step 52980, loss = 0.71, batch loss = 0.65 (14.8 examples/sec; 0.541 sec/batch; 42h:01m:03s remains)
INFO - root - 2017-12-11 10:10:51.659673: step 52990, loss = 0.70, batch loss = 0.64 (15.0 examples/sec; 0.534 sec/batch; 41h:27m:02s remains)
INFO - root - 2017-12-11 10:10:57.159712: step 53000, loss = 0.71, batch loss = 0.65 (13.7 examples/sec; 0.585 sec/batch; 45h:23m:15s remains)
2017-12-11 10:10:57.774840: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0019517728 -0.0013863707 -0.0038947565 -0.0084937941 -0.012442255 -0.013350459 -0.010044922 -0.0033497126 0.0047298716 0.011507045 0.015912451 0.015951579 0.0087571545 -0.0042578154 -0.018379182][0.017210029 0.020174695 0.016438916 0.0085508516 0.001028328 -0.0017128385 0.0027307854 0.014289293 0.02990412 0.043962985 0.053883739 0.055823069 0.045083769 0.023502631 -0.001153967][0.039095003 0.046386838 0.043215014 0.033048566 0.021743769 0.016061975 0.020616349 0.036844622 0.061357476 0.0849361 0.10265645 0.10790895 0.093164966 0.060901374 0.02284674][0.061527047 0.074264362 0.073420957 0.062137391 0.047181025 0.038128279 0.0425032 0.06329181 0.097003736 0.13109794 0.15806937 0.16755784 0.14834198 0.1035884 0.050002415][0.08622326 0.10588111 0.10915381 0.098032214 0.080229469 0.068540782 0.073808074 0.099910878 0.14246304 0.18636976 0.22172712 0.23427829 0.20889403 0.14960815 0.078786768][0.11404468 0.14218013 0.15093593 0.14063169 0.11977553 0.10475703 0.10965505 0.13880318 0.186141 0.2352758 0.27519453 0.28884619 0.25781929 0.18605824 0.10082985][0.13117278 0.1671557 0.18298319 0.17635749 0.15518263 0.13841878 0.14295311 0.17278035 0.21972173 0.26794782 0.3069219 0.31853205 0.28244084 0.20298111 0.11021262][0.12928431 0.1714427 0.19555396 0.19644487 0.17964204 0.16468056 0.16975908 0.19780092 0.23953255 0.28106147 0.31363437 0.31978145 0.27888545 0.19705218 0.10446025][0.11747354 0.16053714 0.18878654 0.19486588 0.18257329 0.17052464 0.1760467 0.2007146 0.23476669 0.26683331 0.29050863 0.29024279 0.24718465 0.1691184 0.084591277][0.10611995 0.1455548 0.17245629 0.17872514 0.16779938 0.15634346 0.15973715 0.17871201 0.20368604 0.2255258 0.23962827 0.23368077 0.19201951 0.12352913 0.053376313][0.090183824 0.12253241 0.14452204 0.1482787 0.13666447 0.12345922 0.12206921 0.1330186 0.14767115 0.15888715 0.16395903 0.15438688 0.11817878 0.064189106 0.012931132][0.060324587 0.084014989 0.10084756 0.10375629 0.094161287 0.082366489 0.079166368 0.084124662 0.090112917 0.092306264 0.090019323 0.078375153 0.048747096 0.0095693935 -0.023465822][0.028848507 0.044909637 0.057510138 0.060968716 0.055253796 0.046802461 0.043421827 0.04397919 0.043656319 0.03979478 0.033216111 0.021906529 -7.2553637e-05 -0.02593796 -0.044668496][0.0097215585 0.019077256 0.02616892 0.027058683 0.021714227 0.013821005 0.0086962087 0.0052905134 0.0012321473 -0.0046670171 -0.01123399 -0.019107589 -0.032106109 -0.04603494 -0.0540058][-0.0032625296 0.00015357113 0.0015594064 -0.0017591869 -0.00897695 -0.018051697 -0.025444806 -0.031278282 -0.036179107 -0.040920403 -0.045127891 -0.048909303 -0.05411024 -0.058654938 -0.059151553]]...]
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-SGD-lr0.001-clip50-initfromfixqian30k-fixqian
INFO - root - 2017-12-11 10:11:03.348133: step 53010, loss = 0.70, batch loss = 0.64 (13.8 examples/sec; 0.579 sec/batch; 44h:58m:04s remains)
INFO - root - 2017-12-11 10:11:08.989869: step 53020, loss = 0.70, batch loss = 0.64 (14.6 examples/sec; 0.547 sec/batch; 42h:27m:42s remains)
INFO - root - 2017-12-11 10:11:14.507139: step 53030, loss = 0.70, batch loss = 0.64 (14.7 examples/sec; 0.544 sec/batch; 42h:12m:14s remains)
INFO - root - 2017-12-11 10:11:20.064672: step 53040, loss = 0.70, batch loss = 0.64 (14.0 examples/sec; 0.570 sec/batch; 44h:13m:53s remains)
INFO - root - 2017-12-11 10:11:25.568358: step 53050, loss = 0.70, batch loss = 0.64 (15.0 examples/sec; 0.535 sec/batch; 41h:32m:16s remains)
INFO - root - 2017-12-11 10:11:30.847202: step 53060, loss = 0.68, batch loss = 0.62 (14.5 examples/sec; 0.551 sec/batch; 42h:47m:46s remains)
INFO - root - 2017-12-11 10:11:36.361099: step 53070, loss = 0.70, batch loss = 0.64 (14.4 examples/sec; 0.557 sec/batch; 43h:12m:20s remains)
INFO - root - 2017-12-11 10:11:41.848740: step 53080, loss = 0.71, batch loss = 0.65 (15.0 examples/sec; 0.535 sec/batch; 41h:31m:37s remains)
INFO - root - 2017-12-11 10:11:47.325092: step 53090, loss = 0.69, batch loss = 0.63 (14.6 examples/sec; 0.547 sec/batch; 42h:25m:54s remains)
INFO - root - 2017-12-11 10:11:52.769804: step 53100, loss = 0.69, batch loss = 0.63 (15.1 examples/sec; 0.531 sec/batch; 41h:13m:05s remains)
2017-12-11 10:11:53.364131: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.058905654 -0.059459079 -0.059577212 -0.059850574 -0.060523447 -0.061603375 -0.062449541 -0.062793031 -0.062633187 -0.062929049 -0.064577676 -0.067500725 -0.070338279 -0.071315385 -0.068552725][-0.028957991 -0.030222803 -0.031564474 -0.032875955 -0.034745466 -0.037252884 -0.039158616 -0.039987836 -0.039705396 -0.039632548 -0.041710291 -0.045633439 -0.048447605 -0.047646694 -0.039810531][0.026914818 0.025312025 0.023802731 0.023411451 0.022006374 0.018917058 0.017282682 0.018158151 0.021265374 0.023685453 0.021293107 0.015361054 0.01054272 0.010362432 0.020752471][0.10575949 0.10580744 0.10771813 0.11270035 0.11542003 0.11400241 0.11510442 0.12090088 0.13029794 0.13691634 0.13340478 0.12266675 0.11145636 0.10491551 0.11211884][0.2051449 0.21036118 0.21994975 0.23426075 0.2431833 0.24325755 0.24682966 0.25762105 0.27357709 0.28453013 0.2789773 0.2610895 0.23975988 0.22255473 0.22155726][0.30613917 0.31761658 0.33621788 0.36064273 0.37540627 0.37536079 0.37959009 0.39355636 0.41467953 0.43001816 0.42376313 0.40011719 0.36911187 0.34051615 0.32809645][0.38672864 0.40271634 0.42916071 0.46199954 0.48070213 0.47880283 0.48126638 0.49489918 0.51689434 0.53400481 0.52771062 0.50099 0.46244213 0.42291287 0.39760858][0.43002433 0.44712186 0.47695047 0.51386833 0.53388155 0.52949172 0.52793044 0.53609514 0.551314 0.56293786 0.55400181 0.52598941 0.48331144 0.43620807 0.40093058][0.4235318 0.43955809 0.46953782 0.50622481 0.52510476 0.51785076 0.50961488 0.50622678 0.50641638 0.50560516 0.49209061 0.465762 0.4255451 0.37934044 0.34229246][0.36832991 0.38365635 0.412605 0.44558039 0.46031645 0.44838381 0.43020931 0.41053727 0.39094943 0.37454984 0.35568771 0.3328914 0.30058619 0.2633177 0.233043][0.2824367 0.29429203 0.31817573 0.34266523 0.34958249 0.33138597 0.30305603 0.26858968 0.23213094 0.2025843 0.17928992 0.16057429 0.13903522 0.11564436 0.098262213][0.19752572 0.20362127 0.21952723 0.23299101 0.23024677 0.20579594 0.17012495 0.12632318 0.079782844 0.04217134 0.016047139 0.00035828783 -0.01213255 -0.023288308 -0.028540017][0.13619852 0.13689187 0.14474152 0.14864147 0.13919826 0.11291716 0.076476827 0.031663176 -0.016070807 -0.055029351 -0.081545539 -0.096098341 -0.10475662 -0.1107674 -0.11157427][0.1055942 0.10481143 0.10876064 0.10741137 0.094965816 0.069989428 0.036931794 -0.0031186535 -0.04535605 -0.079987593 -0.10455256 -0.1195311 -0.12943439 -0.13679154 -0.13973503][0.098280072 0.0977631 0.099776909 0.095480993 0.081657812 0.059228092 0.032064278 0.00091936521 -0.031027587 -0.057193376 -0.077508256 -0.092958406 -0.10595815 -0.1168423 -0.12290021]]...]
INFO - root - 2017-12-11 10:11:58.849468: step 53110, loss = 0.68, batch loss = 0.62 (14.5 examples/sec; 0.552 sec/batch; 42h:51m:44s remains)
INFO - root - 2017-12-11 10:12:04.280165: step 53120, loss = 0.67, batch loss = 0.61 (14.7 examples/sec; 0.544 sec/batch; 42h:14m:11s remains)
INFO - root - 2017-12-11 10:12:09.757244: step 53130, loss = 0.70, batch loss = 0.64 (14.7 examples/sec; 0.544 sec/batch; 42h:14m:51s remains)
INFO - root - 2017-12-11 10:12:15.029797: step 53140, loss = 0.69, batch loss = 0.63 (14.8 examples/sec; 0.541 sec/batch; 41h:58m:21s remains)
INFO - root - 2017-12-11 10:12:20.514338: step 53150, loss = 0.70, batch loss = 0.64 (14.7 examples/sec; 0.543 sec/batch; 42h:09m:52s remains)
INFO - root - 2017-12-11 10:12:25.987942: step 53160, loss = 0.69, batch loss = 0.63 (14.7 examples/sec; 0.544 sec/batch; 42h:10m:45s remains)
INFO - root - 2017-12-11 10:12:31.584972: step 53170, loss = 0.68, batch loss = 0.62 (15.1 examples/sec; 0.530 sec/batch; 41h:05m:24s remains)
INFO - root - 2017-12-11 10:12:37.041606: step 53180, loss = 0.69, batch loss = 0.63 (14.9 examples/sec; 0.535 sec/batch; 41h:32m:21s remains)
INFO - root - 2017-12-11 10:12:42.517242: step 53190, loss = 0.70, batch loss = 0.64 (14.7 examples/sec; 0.544 sec/batch; 42h:12m:08s remains)
INFO - root - 2017-12-11 10:12:48.012968: step 53200, loss = 0.69, batch loss = 0.63 (14.8 examples/sec; 0.539 sec/batch; 41h:50m:32s remains)
2017-12-11 10:12:48.628175: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.02128238 -0.03250898 -0.043938514 -0.052006286 -0.056830954 -0.056696739 -0.044656161 -0.022880364 -0.0017388478 0.0068820105 0.0021826546 -0.013245811 -0.039382573 -0.070205189 -0.094664283][0.057178151 0.047476534 0.032071922 0.016995851 0.0028678305 -0.0039688023 0.009967139 0.040965974 0.072081424 0.084021114 0.075623751 0.050282206 0.0078046103 -0.042876679 -0.084565364][0.16873865 0.16422734 0.14671366 0.12432023 0.09848962 0.081574708 0.09386 0.12947568 0.16497032 0.17566535 0.15996097 0.12268277 0.062830493 -0.0083379121 -0.067454435][0.29194438 0.2992788 0.28691441 0.26302519 0.2296965 0.2043277 0.21255842 0.24668062 0.2790373 0.28194037 0.25428879 0.20127496 0.1216855 0.028734231 -0.048830263][0.39942563 0.42854759 0.43417293 0.42172164 0.39385957 0.37006867 0.37950873 0.41389236 0.44260168 0.4357349 0.39027956 0.31286189 0.20432043 0.080816 -0.022777069][0.47283816 0.52844179 0.55932975 0.56801718 0.55889165 0.54951465 0.56887209 0.60959029 0.638463 0.62293667 0.55501992 0.44705147 0.30457988 0.14631295 0.012409089][0.51589274 0.59774917 0.65265828 0.68345195 0.69687331 0.70688933 0.73851705 0.78556252 0.81355166 0.787704 0.69569904 0.55927104 0.38930136 0.20485736 0.04742296][0.52175379 0.6183477 0.68517733 0.72794753 0.757241 0.78272641 0.82390106 0.87544811 0.90400678 0.873366 0.76701504 0.61577445 0.4338603 0.23869839 0.070020571][0.47576675 0.56757808 0.62925661 0.67038858 0.70360547 0.73387271 0.77619535 0.82644808 0.85471439 0.8255868 0.720909 0.57708365 0.40731734 0.22477607 0.064549826][0.38046896 0.44961271 0.4917073 0.51924849 0.54445571 0.56860352 0.6032117 0.64567971 0.67147964 0.648602 0.56030113 0.44297317 0.30624574 0.15783402 0.026368158][0.24089679 0.27729732 0.29370877 0.30277872 0.31404191 0.32642543 0.34896046 0.38010207 0.40175769 0.38745466 0.32417515 0.24323991 0.150107 0.048437335 -0.040481851][0.085757047 0.0943435 0.09081459 0.084104218 0.080584764 0.079191655 0.087635815 0.10545264 0.12085196 0.1139724 0.076379932 0.031307764 -0.018993367 -0.072856486 -0.1162747][-0.040243927 -0.047722466 -0.060499031 -0.074851841 -0.087144516 -0.097510546 -0.099280253 -0.092544176 -0.084400825 -0.087742604 -0.10647094 -0.12528071 -0.14372203 -0.16105846 -0.16938548][-0.11223784 -0.12560038 -0.13941702 -0.15393516 -0.16750477 -0.17961448 -0.18525617 -0.18474388 -0.18216832 -0.18459646 -0.19248722 -0.19690382 -0.19804312 -0.19542977 -0.18533616][-0.13887987 -0.15184344 -0.16234224 -0.17321675 -0.18380173 -0.1934792 -0.19914353 -0.20110531 -0.20140205 -0.20321055 -0.20566516 -0.20421098 -0.19897038 -0.18950222 -0.17441158]]...]
INFO - root - 2017-12-11 10:12:54.150705: step 53210, loss = 0.70, batch loss = 0.64 (14.2 examples/sec; 0.563 sec/batch; 43h:42m:24s remains)
INFO - root - 2017-12-11 10:12:59.314028: step 53220, loss = 0.70, batch loss = 0.64 (19.3 examples/sec; 0.415 sec/batch; 32h:10m:35s remains)
INFO - root - 2017-12-11 10:13:04.838412: step 53230, loss = 0.70, batch loss = 0.64 (14.5 examples/sec; 0.552 sec/batch; 42h:48m:32s remains)
INFO - root - 2017-12-11 10:13:10.332183: step 53240, loss = 0.69, batch loss = 0.63 (14.5 examples/sec; 0.553 sec/batch; 42h:54m:32s remains)
INFO - root - 2017-12-11 10:13:15.811907: step 53250, loss = 0.70, batch loss = 0.64 (14.9 examples/sec; 0.535 sec/batch; 41h:31m:26s remains)
INFO - root - 2017-12-11 10:13:21.362383: step 53260, loss = 0.71, batch loss = 0.65 (14.8 examples/sec; 0.541 sec/batch; 42h:00m:04s remains)
INFO - root - 2017-12-11 10:13:26.843287: step 53270, loss = 0.69, batch loss = 0.63 (14.1 examples/sec; 0.567 sec/batch; 43h:58m:16s remains)
INFO - root - 2017-12-11 10:13:32.423868: step 53280, loss = 0.70, batch loss = 0.64 (13.2 examples/sec; 0.607 sec/batch; 47h:07m:05s remains)
INFO - root - 2017-12-11 10:13:38.000808: step 53290, loss = 0.70, batch loss = 0.64 (14.4 examples/sec; 0.556 sec/batch; 43h:06m:10s remains)
INFO - root - 2017-12-11 10:13:43.522488: step 53300, loss = 0.69, batch loss = 0.63 (15.1 examples/sec; 0.528 sec/batch; 40h:57m:40s remains)
2017-12-11 10:13:44.013218: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.0057098772 0.015329737 0.024491303 0.027567793 0.024471536 0.022307012 0.02680904 0.0345769 0.039257448 0.037605036 0.030691579 0.018728796 0.0031353512 -0.012762366 -0.024102816][0.047561251 0.073628463 0.097758226 0.10959443 0.10829822 0.10693333 0.11377966 0.12267001 0.1240987 0.11518034 0.098702043 0.073349267 0.041595243 0.0094298581 -0.014651589][0.091253318 0.13400593 0.1742955 0.19727403 0.20123222 0.20501582 0.21874596 0.23085865 0.22848475 0.21063328 0.1818791 0.13948853 0.0871679 0.035450023 -0.0029642868][0.12766385 0.18309981 0.23508362 0.2664777 0.27560169 0.28614116 0.31080624 0.33228156 0.3324832 0.30980682 0.27086845 0.21146989 0.13611031 0.061260492 0.006043179][0.16978812 0.23972444 0.30167013 0.338251 0.34927064 0.36187586 0.39362118 0.42454231 0.43163776 0.41005176 0.36535794 0.2910262 0.19187967 0.091387078 0.016483484][0.23219374 0.32225183 0.39721045 0.43895406 0.44871512 0.45547572 0.48391959 0.515825 0.5258677 0.50596887 0.45774758 0.37224403 0.25294194 0.12874478 0.03384237][0.29623029 0.40522236 0.49157438 0.5351885 0.53824544 0.52973479 0.54120117 0.562012 0.56993705 0.55379772 0.51011056 0.42619386 0.30137023 0.16520295 0.056635235][0.33722883 0.46039486 0.55706877 0.6039834 0.60174 0.5768609 0.564637 0.56402481 0.56132841 0.54551893 0.51051909 0.43919781 0.32363516 0.1898617 0.077923007][0.33812422 0.46315017 0.56347245 0.61309016 0.60940075 0.57411182 0.54166609 0.51924461 0.50254649 0.48427185 0.45927745 0.4072966 0.31274012 0.19471274 0.090804093][0.29063851 0.39973634 0.48874995 0.53282392 0.5284484 0.4915365 0.45064822 0.41585177 0.39001375 0.3718929 0.35871419 0.32914159 0.26239631 0.17076707 0.086309493][0.21220292 0.29465035 0.36221188 0.39525256 0.3916949 0.36172035 0.32413235 0.28721094 0.25786489 0.24075727 0.23591781 0.22341633 0.18216102 0.12019263 0.061931964][0.12054155 0.17323349 0.21640477 0.23696862 0.23476531 0.21554191 0.18895575 0.15984692 0.13528882 0.12232739 0.12257272 0.1197125 0.096752889 0.059896708 0.026326589][0.041273706 0.070579186 0.095393956 0.10743081 0.10724057 0.097962879 0.083573319 0.066051319 0.050232783 0.042371582 0.044086803 0.043721434 0.030120896 0.00937868 -0.0068622841][-0.0084750066 0.0051699737 0.018098285 0.024712464 0.025280159 0.021767141 0.015974533 0.00858562 0.0016182305 -0.0013423668 0.00025273062 -0.000695025 -0.010041736 -0.021834858 -0.028352814][-0.032199413 -0.028706605 -0.023354191 -0.020403584 -0.02016711 -0.021513188 -0.023097074 -0.024360636 -0.024887804 -0.023635209 -0.020782735 -0.02132538 -0.027486801 -0.034241863 -0.036497451]]...]
INFO - root - 2017-12-11 10:13:49.466547: step 53310, loss = 0.70, batch loss = 0.64 (14.2 examples/sec; 0.564 sec/batch; 43h:43m:29s remains)
INFO - root - 2017-12-11 10:13:54.965077: step 53320, loss = 0.72, batch loss = 0.66 (14.9 examples/sec; 0.537 sec/batch; 41h:36m:20s remains)
INFO - root - 2017-12-11 10:14:00.538311: step 53330, loss = 0.69, batch loss = 0.63 (14.7 examples/sec; 0.546 sec/batch; 42h:18m:22s remains)
INFO - root - 2017-12-11 10:14:06.021985: step 53340, loss = 0.70, batch loss = 0.64 (14.3 examples/sec; 0.558 sec/batch; 43h:14m:01s remains)
INFO - root - 2017-12-11 10:14:11.489505: step 53350, loss = 0.70, batch loss = 0.64 (14.2 examples/sec; 0.562 sec/batch; 43h:33m:59s remains)
INFO - root - 2017-12-11 10:14:17.001526: step 53360, loss = 0.69, batch loss = 0.63 (14.5 examples/sec; 0.551 sec/batch; 42h:43m:44s remains)
INFO - root - 2017-12-11 10:14:22.503412: step 53370, loss = 0.70, batch loss = 0.65 (15.0 examples/sec; 0.533 sec/batch; 41h:17m:39s remains)
INFO - root - 2017-12-11 10:14:28.020105: step 53380, loss = 0.71, batch loss = 0.65 (14.9 examples/sec; 0.538 sec/batch; 41h:40m:33s remains)
INFO - root - 2017-12-11 10:14:33.345836: step 53390, loss = 0.69, batch loss = 0.63 (14.4 examples/sec; 0.556 sec/batch; 43h:06m:25s remains)
INFO - root - 2017-12-11 10:14:38.843908: step 53400, loss = 0.70, batch loss = 0.64 (14.1 examples/sec; 0.568 sec/batch; 44h:01m:28s remains)
2017-12-11 10:14:39.482828: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.074641675 0.12091324 0.15984239 0.17736791 0.17164932 0.15177664 0.12365538 0.094234928 0.069226265 0.054484107 0.051627126 0.061239868 0.0812314 0.10604679 0.12788975][0.060803451 0.11082657 0.15907694 0.1905131 0.19982097 0.19313917 0.17393954 0.14752109 0.11922826 0.096322425 0.085159428 0.087994874 0.10272254 0.12173922 0.13698927][0.041109595 0.096392512 0.15644754 0.20616315 0.23682691 0.25060993 0.24716359 0.22938222 0.20188975 0.17314503 0.15512528 0.15111524 0.15829815 0.16651945 0.16825584][0.034410089 0.1009244 0.17758057 0.24816222 0.30229124 0.33927327 0.35491624 0.3497155 0.3273094 0.29702011 0.27594331 0.26720339 0.26633921 0.2608102 0.24413186][0.048934467 0.13375764 0.23197877 0.32427967 0.40078554 0.46002576 0.49468213 0.50341439 0.48962137 0.46282527 0.4431507 0.43222231 0.42372671 0.40296003 0.36462471][0.0815044 0.19123638 0.31493568 0.42866531 0.52368814 0.59940493 0.6470055 0.66465253 0.65707552 0.633923 0.61626518 0.60288078 0.58559668 0.54883426 0.48948878][0.12194838 0.25832623 0.40733549 0.53936255 0.64542043 0.72626555 0.77424228 0.79015642 0.7821312 0.75931585 0.74150652 0.72355193 0.69618046 0.64506763 0.56993419][0.15474917 0.30973905 0.47457397 0.61424488 0.71779984 0.78740543 0.81941319 0.82070905 0.80465615 0.77967441 0.76163703 0.74058914 0.70708305 0.64926946 0.56930494][0.15925714 0.31459796 0.47608063 0.60647839 0.69288087 0.73868042 0.74573785 0.72809267 0.70357913 0.67928654 0.66460252 0.64524168 0.611798 0.55579597 0.48124284][0.11902452 0.25151014 0.38794026 0.49358162 0.55480349 0.57494336 0.56016111 0.52881736 0.50024116 0.47967529 0.4704141 0.45618963 0.42784145 0.38032004 0.31878561][0.043001156 0.13610569 0.23327366 0.30583802 0.34113756 0.34104133 0.31311512 0.27570164 0.2466917 0.22997151 0.22525388 0.21729787 0.1976462 0.16341251 0.11986854][-0.036264919 0.014623918 0.070519418 0.11012705 0.12331641 0.11044917 0.078373492 0.042312574 0.016679551 0.0042266888 0.0033822991 0.0020604345 -0.007021891 -0.026052892 -0.050881509][-0.094658054 -0.077842809 -0.05546755 -0.041857831 -0.043638431 -0.061038334 -0.088808626 -0.11650177 -0.13456398 -0.14147151 -0.13888504 -0.13489014 -0.13514835 -0.14154796 -0.15156376][-0.12629874 -0.13115339 -0.12986358 -0.13143803 -0.13929792 -0.15382433 -0.17180066 -0.18768145 -0.19674844 -0.19870214 -0.1947584 -0.18931495 -0.18602034 -0.18607435 -0.1879968][-0.13222931 -0.14628504 -0.15374427 -0.16063116 -0.16820219 -0.17696351 -0.18567832 -0.19241554 -0.19564229 -0.19564988 -0.19299288 -0.1893751 -0.18653776 -0.18491776 -0.18364967]]...]
INFO - root - 2017-12-11 10:14:44.998685: step 53410, loss = 0.70, batch loss = 0.64 (14.5 examples/sec; 0.550 sec/batch; 42h:40m:01s remains)
INFO - root - 2017-12-11 10:14:50.529491: step 53420, loss = 0.70, batch loss = 0.64 (14.6 examples/sec; 0.546 sec/batch; 42h:20m:08s remains)
INFO - root - 2017-12-11 10:14:56.005173: step 53430, loss = 0.68, batch loss = 0.63 (15.0 examples/sec; 0.534 sec/batch; 41h:21m:32s remains)
INFO - root - 2017-12-11 10:15:01.505896: step 53440, loss = 0.68, batch loss = 0.63 (14.6 examples/sec; 0.547 sec/batch; 42h:22m:35s remains)
INFO - root - 2017-12-11 10:15:06.996936: step 53450, loss = 0.69, batch loss = 0.64 (13.9 examples/sec; 0.575 sec/batch; 44h:34m:55s remains)
INFO - root - 2017-12-11 10:15:12.526525: step 53460, loss = 0.70, batch loss = 0.64 (14.0 examples/sec; 0.570 sec/batch; 44h:11m:48s remains)
INFO - root - 2017-12-11 10:15:17.902752: step 53470, loss = 0.69, batch loss = 0.64 (13.8 examples/sec; 0.579 sec/batch; 44h:52m:33s remains)
INFO - root - 2017-12-11 10:15:23.373654: step 53480, loss = 0.70, batch loss = 0.65 (14.2 examples/sec; 0.563 sec/batch; 43h:36m:13s remains)
INFO - root - 2017-12-11 10:15:28.898696: step 53490, loss = 0.70, batch loss = 0.64 (15.0 examples/sec; 0.535 sec/batch; 41h:26m:00s remains)
INFO - root - 2017-12-11 10:15:34.322857: step 53500, loss = 0.70, batch loss = 0.64 (14.7 examples/sec; 0.544 sec/batch; 42h:09m:56s remains)
2017-12-11 10:15:34.963678: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.12388838 0.13284771 0.13777988 0.1345271 0.12621135 0.1216775 0.12504092 0.13091184 0.13503534 0.13900143 0.14831047 0.16248854 0.17365184 0.18725489 0.20587784][0.11723992 0.12599374 0.12811039 0.11985136 0.10716756 0.1020664 0.1075808 0.11650743 0.12249319 0.1262577 0.13341932 0.14682348 0.16234539 0.18548863 0.21551484][0.12085256 0.1333743 0.13658543 0.12680726 0.11197612 0.10611957 0.11093499 0.11833354 0.12088479 0.11870907 0.11782304 0.12593211 0.14411116 0.17679329 0.2172673][0.13564378 0.15293267 0.15865554 0.14985108 0.13632329 0.13289374 0.13892613 0.14444327 0.1413347 0.12978184 0.11670631 0.11521772 0.13249955 0.1721718 0.22064316][0.15583165 0.17356269 0.180551 0.17526554 0.16875507 0.17457195 0.1877659 0.19470762 0.1860853 0.16294788 0.13396718 0.11706837 0.12679833 0.16732685 0.21954498][0.17895423 0.19209115 0.19859682 0.19871089 0.2038568 0.22522542 0.25235975 0.26617941 0.25428003 0.21909977 0.17234501 0.13611302 0.13239345 0.16739722 0.219596][0.1980575 0.20369522 0.20950285 0.21687685 0.23657034 0.27690223 0.3222065 0.34730902 0.33534566 0.29020211 0.22738518 0.17237163 0.15315183 0.17904828 0.22862302][0.20105757 0.19909188 0.20373622 0.21774936 0.25001332 0.30652139 0.36846048 0.40562624 0.3963455 0.34569314 0.27315643 0.20519432 0.17222233 0.18639088 0.22927079][0.18137704 0.17380364 0.17714831 0.19520274 0.23459458 0.29961318 0.37069535 0.41633371 0.41245452 0.36512104 0.29523364 0.22587644 0.18555161 0.18812424 0.22050469][0.15368599 0.14398809 0.14641388 0.16475376 0.20304899 0.26466304 0.3319501 0.37694374 0.37703142 0.33883473 0.28194165 0.22252901 0.18297346 0.17644492 0.19660448][0.12702775 0.12026793 0.12353268 0.13944797 0.1697883 0.21798329 0.27085817 0.30740547 0.30916408 0.28289059 0.24470653 0.20327741 0.17235862 0.16214786 0.17173262][0.10807224 0.10555111 0.10884136 0.1191193 0.13658632 0.16518024 0.19740134 0.22028781 0.22099306 0.20613343 0.18752806 0.16762815 0.15149781 0.14441705 0.147896][0.10313986 0.10528592 0.10813885 0.11182997 0.11562991 0.12454977 0.13643052 0.14491175 0.14219482 0.13453254 0.13001242 0.12734798 0.12502055 0.12307962 0.12328884][0.10626364 0.11217539 0.11503483 0.11457523 0.11004679 0.1073273 0.10744227 0.10716581 0.10186674 0.097432725 0.099457525 0.10587337 0.11214777 0.11468497 0.11440606][0.11534086 0.12333555 0.12532289 0.12232154 0.11437522 0.10849906 0.10669908 0.10516654 0.09977676 0.09566585 0.097388461 0.10432877 0.11203198 0.1160578 0.11657429]]...]
INFO - root - 2017-12-11 10:15:40.528382: step 53510, loss = 0.71, batch loss = 0.65 (14.6 examples/sec; 0.547 sec/batch; 42h:23m:16s remains)
INFO - root - 2017-12-11 10:15:46.027263: step 53520, loss = 0.70, batch loss = 0.65 (14.2 examples/sec; 0.564 sec/batch; 43h:42m:57s remains)
INFO - root - 2017-12-11 10:15:51.488313: step 53530, loss = 0.70, batch loss = 0.64 (14.8 examples/sec; 0.541 sec/batch; 41h:55m:58s remains)
INFO - root - 2017-12-11 10:15:56.984527: step 53540, loss = 0.70, batch loss = 0.64 (14.6 examples/sec; 0.548 sec/batch; 42h:25m:33s remains)
INFO - root - 2017-12-11 10:16:02.280616: step 53550, loss = 0.71, batch loss = 0.65 (14.6 examples/sec; 0.546 sec/batch; 42h:19m:53s remains)
INFO - root - 2017-12-11 10:16:07.843559: step 53560, loss = 0.70, batch loss = 0.64 (14.9 examples/sec; 0.538 sec/batch; 41h:42m:09s remains)
INFO - root - 2017-12-11 10:16:13.376361: step 53570, loss = 0.71, batch loss = 0.65 (14.3 examples/sec; 0.558 sec/batch; 43h:15m:20s remains)
INFO - root - 2017-12-11 10:16:18.116864: step 53580, loss = 0.70, batch loss = 0.64 (31.3 examples/sec; 0.256 sec/batch; 19h:48m:03s remains)
INFO - root - 2017-12-11 10:16:20.733859: step 53590, loss = 0.70, batch loss = 0.64 (30.4 examples/sec; 0.263 sec/batch; 20h:22m:18s remains)
INFO - root - 2017-12-11 10:16:23.454100: step 53600, loss = 0.70, batch loss = 0.64 (31.1 examples/sec; 0.257 sec/batch; 19h:56m:28s remains)
2017-12-11 10:16:23.832867: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.16742529 0.20582701 0.22994359 0.22834864 0.20105389 0.15559824 0.10124682 0.048012216 0.0060744639 -0.017281441 -0.022568144 -0.0082174614 0.026928201 0.074989185 0.1252995][0.24658199 0.29835588 0.32881752 0.32730091 0.29853255 0.25127533 0.18967181 0.12305345 0.06310463 0.019717919 -0.0069085695 -0.013127587 0.0072963033 0.048114162 0.099197514][0.31303492 0.37194544 0.40664345 0.40968415 0.39260006 0.36134353 0.30938202 0.24064225 0.16841041 0.10360159 0.047537863 0.0089520114 0.0034896166 0.028813994 0.074545883][0.35821259 0.41675338 0.4551836 0.46970186 0.47813198 0.47989592 0.45337078 0.39395466 0.31539902 0.23018372 0.14115293 0.065181553 0.029747233 0.037650988 0.077904053][0.3771368 0.43376163 0.48044574 0.51568997 0.5603025 0.604028 0.60976106 0.56455576 0.48189098 0.3779749 0.25754622 0.14638627 0.083399594 0.077197261 0.11633882][0.35878432 0.41580769 0.47619304 0.53873342 0.62266529 0.70739824 0.74352711 0.71223187 0.62611371 0.505401 0.35866293 0.21988051 0.1360918 0.12095391 0.163661][0.30223411 0.36252508 0.43838772 0.52627695 0.64070159 0.75310433 0.81005043 0.78985357 0.70347804 0.57355583 0.4123888 0.2594817 0.16445194 0.14462756 0.19122517][0.23058006 0.29340383 0.37812304 0.47768673 0.60141319 0.71850508 0.77969837 0.7651934 0.68430912 0.5590573 0.402305 0.25316083 0.1579334 0.13562192 0.18112889][0.1567378 0.21810989 0.3016203 0.39704195 0.50885057 0.60959524 0.66027445 0.64634913 0.57529676 0.46626809 0.330657 0.20161222 0.11660394 0.093156889 0.13133894][0.08348491 0.13509876 0.20577857 0.28459206 0.37181997 0.44593093 0.47958717 0.46418813 0.40598324 0.31999129 0.21571714 0.11748832 0.051160958 0.029912798 0.057373751][0.018510891 0.051300921 0.099784747 0.1548118 0.21376289 0.2612392 0.27967191 0.26459724 0.22132251 0.16064374 0.09049283 0.026589219 -0.016393952 -0.030922346 -0.013050759][-0.024613649 -0.013305095 0.010810631 0.04166019 0.075393356 0.10212508 0.11096627 0.099004351 0.070589386 0.033412449 -0.0060512372 -0.03923656 -0.059888277 -0.065068521 -0.052815817][-0.037484858 -0.043747414 -0.0397038 -0.027843675 -0.012477661 0.00092890172 0.0054784035 -0.0020284872 -0.018287474 -0.037330713 -0.053921472 -0.065361805 -0.06990964 -0.066846728 -0.056683458][-0.032892648 -0.049305551 -0.056974038 -0.055121005 -0.048070528 -0.038860027 -0.0321444 -0.032647349 -0.038820092 -0.045696273 -0.048808169 -0.049462225 -0.047487769 -0.041545697 -0.033384085][-0.018112224 -0.036858127 -0.04849948 -0.050098691 -0.044589177 -0.033584248 -0.020870607 -0.013154618 -0.011147932 -0.010988415 -0.008880103 -0.0080940267 -0.0084343543 -0.0060870089 -0.00258445]]...]
INFO - root - 2017-12-11 10:16:26.472600: step 53610, loss = 0.69, batch loss = 0.63 (30.9 examples/sec; 0.259 sec/batch; 20h:02m:09s remains)
INFO - root - 2017-12-11 10:16:29.157836: step 53620, loss = 0.69, batch loss = 0.63 (29.9 examples/sec; 0.268 sec/batch; 20h:43m:50s remains)
INFO - root - 2017-12-11 10:16:31.830249: step 53630, loss = 0.71, batch loss = 0.65 (30.0 examples/sec; 0.266 sec/batch; 20h:38m:24s remains)
INFO - root - 2017-12-11 10:16:34.457066: step 53640, loss = 0.70, batch loss = 0.64 (31.2 examples/sec; 0.256 sec/batch; 19h:51m:27s remains)
INFO - root - 2017-12-11 10:16:37.154240: step 53650, loss = 0.68, batch loss = 0.62 (28.8 examples/sec; 0.277 sec/batch; 21h:28m:57s remains)
INFO - root - 2017-12-11 10:16:39.752609: step 53660, loss = 0.67, batch loss = 0.62 (31.2 examples/sec; 0.256 sec/batch; 19h:50m:35s remains)
INFO - root - 2017-12-11 10:16:42.391060: step 53670, loss = 0.70, batch loss = 0.65 (29.9 examples/sec; 0.268 sec/batch; 20h:44m:09s remains)
INFO - root - 2017-12-11 10:16:45.012608: step 53680, loss = 0.71, batch loss = 0.65 (31.4 examples/sec; 0.255 sec/batch; 19h:45m:46s remains)
INFO - root - 2017-12-11 10:16:47.660705: step 53690, loss = 0.70, batch loss = 0.64 (30.2 examples/sec; 0.265 sec/batch; 20h:30m:37s remains)
INFO - root - 2017-12-11 10:16:50.306033: step 53700, loss = 0.70, batch loss = 0.64 (30.4 examples/sec; 0.264 sec/batch; 20h:24m:44s remains)
2017-12-11 10:16:50.684810: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.24166493 0.26949704 0.31056678 0.34800729 0.35962448 0.33593258 0.27712977 0.20645747 0.16285443 0.16524646 0.20706053 0.25170678 0.26910767 0.24660671 0.19518101][0.22213694 0.24409792 0.27755311 0.30767575 0.3161962 0.29892704 0.25940064 0.21591154 0.19748908 0.21693529 0.26533237 0.3083393 0.31861463 0.28927791 0.23556937][0.20629539 0.21338265 0.22755231 0.24120513 0.24278904 0.23401453 0.21998468 0.21029848 0.22139938 0.25736675 0.30816188 0.34479252 0.34596196 0.31282455 0.26370221][0.20872182 0.20103838 0.19716981 0.19789295 0.19633979 0.19607089 0.2025611 0.21870187 0.25059393 0.29516277 0.34203786 0.3688353 0.3613559 0.32832852 0.28843024][0.22239834 0.20638381 0.1908748 0.18519257 0.18441664 0.19068281 0.20934124 0.24016051 0.28224608 0.32707039 0.36487862 0.3813403 0.36942023 0.34175789 0.3141923][0.24450599 0.22436109 0.20129338 0.19112596 0.1896717 0.19795951 0.22106257 0.25736082 0.30188453 0.3427076 0.37225044 0.38426456 0.37675351 0.36049998 0.34681797][0.27005562 0.24654621 0.21728007 0.20224519 0.19677778 0.20122804 0.22027519 0.25223637 0.29197693 0.32749057 0.35462153 0.37252823 0.37997383 0.38059178 0.38036036][0.28809175 0.26462775 0.23334418 0.21626051 0.2069557 0.20486221 0.214471 0.23542345 0.26498783 0.29402515 0.32197747 0.34978408 0.3735238 0.38851526 0.39652923][0.27420962 0.25350651 0.22711354 0.21437077 0.20648156 0.20117268 0.2025636 0.21246943 0.23107029 0.25323671 0.28096664 0.31444284 0.34681231 0.3682597 0.37891677][0.24507253 0.22853816 0.20818119 0.19911477 0.19233753 0.18553431 0.18316856 0.18890698 0.20402433 0.22494508 0.25271586 0.28610206 0.31745782 0.33594936 0.34314647][0.21315859 0.20053828 0.18508549 0.17681178 0.16861807 0.1602992 0.15728757 0.16417597 0.18196213 0.20666297 0.23592035 0.26697555 0.29270655 0.30415687 0.30649179][0.18447176 0.17459868 0.16232567 0.15349531 0.14333394 0.13331532 0.12997143 0.13845181 0.16010456 0.19073558 0.22393376 0.25452772 0.27500111 0.27914485 0.2766543][0.16747358 0.16037601 0.1496492 0.1388883 0.125609 0.1126183 0.10816306 0.11841702 0.14519666 0.18393588 0.22386855 0.25618935 0.2714971 0.26663342 0.25667498][0.16787706 0.16485649 0.15658268 0.14353165 0.12537608 0.10667457 0.0985317 0.10959012 0.14233683 0.19056903 0.2389802 0.27401704 0.28394479 0.26858923 0.24851695][0.18166344 0.18733343 0.18414144 0.17030415 0.14720288 0.12180068 0.10840117 0.11827944 0.1544468 0.20847639 0.26187211 0.29764625 0.30262592 0.27910814 0.2514708]]...]
INFO - root - 2017-12-11 10:16:53.341434: step 53710, loss = 0.70, batch loss = 0.64 (31.3 examples/sec; 0.256 sec/batch; 19h:49m:23s remains)
INFO - root - 2017-12-11 10:16:56.004654: step 53720, loss = 0.69, batch loss = 0.63 (29.9 examples/sec; 0.267 sec/batch; 20h:42m:13s remains)
INFO - root - 2017-12-11 10:16:58.632300: step 53730, loss = 0.70, batch loss = 0.64 (31.3 examples/sec; 0.256 sec/batch; 19h:49m:04s remains)
INFO - root - 2017-12-11 10:17:01.271680: step 53740, loss = 0.69, batch loss = 0.63 (30.9 examples/sec; 0.259 sec/batch; 20h:04m:47s remains)
INFO - root - 2017-12-11 10:17:03.846929: step 53750, loss = 0.70, batch loss = 0.64 (30.2 examples/sec; 0.265 sec/batch; 20h:31m:46s remains)
INFO - root - 2017-12-11 10:17:06.498465: step 53760, loss = 0.70, batch loss = 0.64 (29.2 examples/sec; 0.274 sec/batch; 21h:12m:04s remains)
INFO - root - 2017-12-11 10:17:09.194740: step 53770, loss = 0.70, batch loss = 0.64 (27.4 examples/sec; 0.292 sec/batch; 22h:37m:42s remains)
INFO - root - 2017-12-11 10:17:11.886431: step 53780, loss = 0.71, batch loss = 0.65 (27.8 examples/sec; 0.288 sec/batch; 22h:16m:01s remains)
INFO - root - 2017-12-11 10:17:14.545007: step 53790, loss = 0.70, batch loss = 0.64 (29.2 examples/sec; 0.274 sec/batch; 21h:13m:49s remains)
INFO - root - 2017-12-11 10:17:17.218296: step 53800, loss = 0.70, batch loss = 0.64 (29.6 examples/sec; 0.270 sec/batch; 20h:53m:32s remains)
2017-12-11 10:17:17.586464: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.20825028 0.24600475 0.27781573 0.29980484 0.30503944 0.29426694 0.27162823 0.24268965 0.21616462 0.20215906 0.19663009 0.21275494 0.25321177 0.3014915 0.3345392][0.21382959 0.24054995 0.26389897 0.28528091 0.29826429 0.30114684 0.29290432 0.2743172 0.2551367 0.24448836 0.23908825 0.2510781 0.28838325 0.33769438 0.37319309][0.21197316 0.22858506 0.2470282 0.27348807 0.3007994 0.32327408 0.33346719 0.32569486 0.31151825 0.30035684 0.29025379 0.28948447 0.3093915 0.3422178 0.36211872][0.20185693 0.21924406 0.24750906 0.29446578 0.348654 0.39825648 0.42918873 0.43085286 0.41826695 0.40131262 0.37982431 0.35754037 0.34714711 0.3465063 0.33189473][0.16239426 0.19211018 0.24713914 0.33173245 0.42705157 0.5144335 0.57297122 0.58710551 0.57461709 0.54816836 0.51257032 0.46858573 0.42709646 0.38884565 0.33183303][0.10598896 0.15504609 0.24689654 0.37737307 0.51948088 0.64892447 0.73852426 0.76734269 0.75390828 0.71721905 0.67071682 0.61419088 0.55228138 0.48365352 0.38838592][0.058759417 0.12374247 0.24601978 0.41230154 0.58948231 0.75040704 0.86379629 0.90491229 0.89098173 0.84880853 0.80094349 0.74748856 0.6838612 0.60289288 0.48664838][0.040942002 0.10940729 0.24004185 0.41445145 0.59848875 0.76567167 0.88425934 0.92939895 0.91632664 0.87820607 0.84180361 0.80666524 0.7592746 0.68710303 0.57363546][0.057387881 0.11488802 0.22714674 0.37518576 0.53148413 0.6754334 0.77939713 0.821002 0.811737 0.78687835 0.77127922 0.76123029 0.73740405 0.68653291 0.59628808][0.084326982 0.12214472 0.19980577 0.30016646 0.40687272 0.50932491 0.5877195 0.62345022 0.62186038 0.614512 0.62047631 0.63162571 0.62697917 0.59723306 0.53720069][0.091441475 0.10763685 0.14867964 0.19937278 0.25434148 0.31300911 0.36412838 0.39360064 0.40029538 0.40785536 0.42847279 0.45149833 0.45812118 0.44380018 0.41019034][0.066154666 0.063922577 0.075772636 0.088131227 0.10345814 0.12734565 0.15555884 0.17850539 0.19040668 0.2053276 0.2296371 0.25408357 0.26517045 0.26191163 0.24880819][0.018719783 0.0062614651 0.0043117069 -0.0019293863 -0.0073341955 -0.0046023866 0.0065523363 0.020510688 0.029772352 0.041042604 0.057565488 0.074174091 0.083907411 0.08761958 0.089586444][-0.032184333 -0.044297229 -0.044290397 -0.050537288 -0.059283756 -0.064832382 -0.06515567 -0.062180813 -0.062051341 -0.061721876 -0.058911081 -0.054770097 -0.050801102 -0.045442641 -0.034643717][-0.067634262 -0.071486562 -0.06057702 -0.056849465 -0.058973368 -0.06430158 -0.070957355 -0.077353857 -0.0866149 -0.0967322 -0.10574886 -0.11188629 -0.11385325 -0.11076351 -0.098403007]]...]
INFO - root - 2017-12-11 10:17:20.219973: step 53810, loss = 0.69, batch loss = 0.63 (29.8 examples/sec; 0.268 sec/batch; 20h:46m:38s remains)
INFO - root - 2017-12-11 10:17:22.843081: step 53820, loss = 0.69, batch loss = 0.63 (30.4 examples/sec; 0.263 sec/batch; 20h:23m:12s remains)
INFO - root - 2017-12-11 10:17:25.420717: step 53830, loss = 0.71, batch loss = 0.65 (31.1 examples/sec; 0.257 sec/batch; 19h:54m:23s remains)
INFO - root - 2017-12-11 10:17:27.994045: step 53840, loss = 0.69, batch loss = 0.63 (30.2 examples/sec; 0.265 sec/batch; 20h:29m:15s remains)
INFO - root - 2017-12-11 10:17:30.649302: step 53850, loss = 0.68, batch loss = 0.63 (27.7 examples/sec; 0.289 sec/batch; 22h:22m:48s remains)
INFO - root - 2017-12-11 10:17:33.270443: step 53860, loss = 0.70, batch loss = 0.64 (30.8 examples/sec; 0.260 sec/batch; 20h:06m:22s remains)
INFO - root - 2017-12-11 10:17:35.938573: step 53870, loss = 0.71, batch loss = 0.65 (30.2 examples/sec; 0.265 sec/batch; 20h:30m:32s remains)
INFO - root - 2017-12-11 10:17:38.618218: step 53880, loss = 0.69, batch loss = 0.63 (27.8 examples/sec; 0.287 sec/batch; 22h:15m:02s remains)
INFO - root - 2017-12-11 10:17:41.253472: step 53890, loss = 0.72, batch loss = 0.66 (31.5 examples/sec; 0.254 sec/batch; 19h:40m:00s remains)
INFO - root - 2017-12-11 10:17:43.816100: step 53900, loss = 0.70, batch loss = 0.64 (30.6 examples/sec; 0.261 sec/batch; 20h:12m:33s remains)
2017-12-11 10:17:44.165971: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.314855 0.31675068 0.30845642 0.29226765 0.27140164 0.24970053 0.22807038 0.21214385 0.21629758 0.24905708 0.31880984 0.40578035 0.47695646 0.50727224 0.49310428][0.2912364 0.30697727 0.31272295 0.31025165 0.30349982 0.29400703 0.27969617 0.26400745 0.2609342 0.28022504 0.33279002 0.40438423 0.4663628 0.49461567 0.48134243][0.25081986 0.27383059 0.28999805 0.30032936 0.30837885 0.3125751 0.30804878 0.29441071 0.28422496 0.28775954 0.31733662 0.36671236 0.41345814 0.43634883 0.42566267][0.20327058 0.22460292 0.2475058 0.26973754 0.29146487 0.3085224 0.31455305 0.3054859 0.29214486 0.28527626 0.29628807 0.32433796 0.35391223 0.36983237 0.36282107][0.14793816 0.16563496 0.19675699 0.23372157 0.27096856 0.30161765 0.31842896 0.31402352 0.29873836 0.28519997 0.28417218 0.29848951 0.31779712 0.3321979 0.33141425][0.099476434 0.11753178 0.15844907 0.21088822 0.26428923 0.30908051 0.33643523 0.33543819 0.31678662 0.29646692 0.28679627 0.29566139 0.31704876 0.34219581 0.35452682][0.069547854 0.094853669 0.14560483 0.20942885 0.27358145 0.32689911 0.3591617 0.35672948 0.33101612 0.30148932 0.28488347 0.29614457 0.33222714 0.37939718 0.40980434][0.069249 0.10651738 0.16417134 0.23025493 0.29364759 0.3442792 0.37200105 0.36233097 0.32770631 0.2900905 0.2702629 0.28906441 0.34398258 0.41346198 0.45903903][0.10921427 0.15759517 0.21557277 0.27343237 0.32476047 0.3627046 0.37829268 0.35803932 0.31446406 0.26970986 0.24653062 0.26961586 0.33686933 0.42048505 0.47574666][0.18247293 0.23625527 0.28787351 0.3315334 0.36569011 0.38691109 0.38733351 0.35539472 0.30264711 0.2501846 0.22091542 0.24216744 0.31332472 0.40353078 0.4661313][0.26395923 0.31530088 0.35591355 0.38501289 0.40449059 0.4120779 0.39867371 0.35444862 0.29002473 0.22703333 0.18897285 0.20417695 0.27380481 0.36662397 0.43723652][0.32214916 0.36770722 0.39905995 0.41904727 0.43050984 0.42940789 0.40404782 0.3465386 0.2673963 0.19042777 0.14091431 0.14635228 0.20961125 0.30078498 0.37798977][0.34068635 0.38114464 0.40954229 0.42959043 0.44148678 0.43746275 0.40341091 0.33410639 0.24048151 0.14888597 0.086782828 0.079651661 0.13222116 0.21709716 0.29677448][0.33341232 0.36873415 0.39774692 0.42273051 0.43842763 0.43276307 0.39097804 0.31175226 0.20741038 0.10611848 0.036118645 0.019075455 0.060867015 0.13723524 0.21514496][0.31263241 0.34051749 0.37000164 0.40039581 0.41925082 0.40999946 0.35918471 0.27067858 0.160951 0.059811581 -0.0070729107 -0.024949014 0.010131706 0.077036664 0.14731862]]...]
INFO - root - 2017-12-11 10:17:46.815480: step 53910, loss = 0.70, batch loss = 0.64 (30.4 examples/sec; 0.263 sec/batch; 20h:20m:20s remains)
INFO - root - 2017-12-11 10:17:49.483474: step 53920, loss = 0.70, batch loss = 0.64 (29.9 examples/sec; 0.268 sec/batch; 20h:43m:16s remains)
INFO - root - 2017-12-11 10:17:52.159050: step 53930, loss = 0.69, batch loss = 0.63 (30.5 examples/sec; 0.262 sec/batch; 20h:18m:21s remains)
INFO - root - 2017-12-11 10:17:54.779202: step 53940, loss = 0.70, batch loss = 0.64 (30.0 examples/sec; 0.267 sec/batch; 20h:39m:19s remains)
INFO - root - 2017-12-11 10:17:57.366595: step 53950, loss = 0.69, batch loss = 0.64 (31.2 examples/sec; 0.257 sec/batch; 19h:52m:13s remains)
INFO - root - 2017-12-11 10:17:59.991737: step 53960, loss = 0.70, batch loss = 0.64 (30.9 examples/sec; 0.259 sec/batch; 20h:02m:48s remains)
INFO - root - 2017-12-11 10:18:02.615961: step 53970, loss = 0.68, batch loss = 0.62 (30.7 examples/sec; 0.260 sec/batch; 20h:08m:30s remains)
INFO - root - 2017-12-11 10:18:05.220340: step 53980, loss = 0.70, batch loss = 0.65 (30.4 examples/sec; 0.264 sec/batch; 20h:23m:28s remains)
INFO - root - 2017-12-11 10:18:07.955655: step 53990, loss = 0.70, batch loss = 0.64 (25.4 examples/sec; 0.315 sec/batch; 24h:24m:05s remains)
INFO - root - 2017-12-11 10:18:10.609851: step 54000, loss = 0.70, batch loss = 0.64 (31.0 examples/sec; 0.258 sec/batch; 19h:58m:56s remains)
2017-12-11 10:18:10.975556: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.012409327 -0.013019642 -0.01287882 -0.011984059 -0.01067064 -0.0093987584 -0.0084305694 -0.008017756 -0.0082919942 -0.0092675993 -0.010554291 -0.011761446 -0.012783217 -0.013530903 -0.013964434][-0.012747956 -0.012833575 -0.011644565 -0.0092439028 -0.0060520433 -0.0028698603 -0.00036206498 0.00091245765 0.00043295769 -0.0016891743 -0.0047467481 -0.0079171071 -0.010607655 -0.012551098 -0.013518631][-0.011291827 -0.0096256677 -0.0060207178 -0.00096306449 0.0048424467 0.010140682 0.01384612 0.015086772 0.013318612 0.0090158451 0.0030565511 -0.0030414159 -0.0081106154 -0.011715171 -0.013577788][-0.0053978665 -0.0012976506 0.0052569625 0.013314745 0.021974323 0.029751152 0.035085838 0.036754955 0.034130737 0.027653072 0.018129084 0.00764267 -0.0016120578 -0.0085350554 -0.012664807][0.0035652372 0.010285706 0.020213243 0.032032102 0.044493537 0.055807605 0.063714296 0.066372789 0.062995531 0.054068528 0.040483188 0.025040414 0.010999155 2.1228791e-06 -0.0072556576][0.012279229 0.021097008 0.033924077 0.049101055 0.064955182 0.079380259 0.08967416 0.093535051 0.089943022 0.079539038 0.0635339 0.045301475 0.028399121 0.014691475 0.005127003][0.019951807 0.030312926 0.044813387 0.061431404 0.078330234 0.093471274 0.10449228 0.10908578 0.1061554 0.096441112 0.081260882 0.063942887 0.047645289 0.034064986 0.024256645][0.025567276 0.036596309 0.050624445 0.065331638 0.07917475 0.091122426 0.099974737 0.1039189 0.10218053 0.095526293 0.084869832 0.07257013 0.0606614 0.050626129 0.043444693][0.031767923 0.042628825 0.054257635 0.064211965 0.071665287 0.077374525 0.08164148 0.083280228 0.082069173 0.07881289 0.073727064 0.067643285 0.061400294 0.056341395 0.05338759][0.042601518 0.052193247 0.059888355 0.0636188 0.063649729 0.062551565 0.061847482 0.060427509 0.058365893 0.056453623 0.054497279 0.052247759 0.049789567 0.048462756 0.0490586][0.063080154 0.070766941 0.073505282 0.070574723 0.063785076 0.057228282 0.053120866 0.049727518 0.046608936 0.044081751 0.041784231 0.03912447 0.036290374 0.034967858 0.035987318][0.087551057 0.091434896 0.087488 0.078071065 0.066931769 0.05909583 0.056464076 0.055748835 0.05454934 0.051846966 0.047217917 0.040475339 0.033003431 0.027627932 0.025634933][0.10156889 0.10097092 0.090997912 0.077696778 0.066504672 0.062531859 0.066625766 0.073673047 0.078369342 0.0776556 0.070879169 0.058621448 0.044282116 0.032466196 0.025587818][0.10386376 0.10034321 0.08749944 0.073601477 0.064821549 0.065928251 0.077273168 0.092946291 0.10505696 0.10856411 0.10216761 0.086704329 0.067057513 0.049449358 0.038014092][0.089841887 0.086249821 0.074901126 0.064210631 0.059598021 0.065031067 0.080963857 0.10205766 0.11944001 0.12711334 0.12312571 0.10792302 0.086817369 0.066811539 0.053636122]]...]
INFO - root - 2017-12-11 10:18:13.603879: step 54010, loss = 0.69, batch loss = 0.63 (30.5 examples/sec; 0.263 sec/batch; 20h:18m:45s remains)
INFO - root - 2017-12-11 10:18:16.226307: step 54020, loss = 0.70, batch loss = 0.64 (32.3 examples/sec; 0.248 sec/batch; 19h:11m:13s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-SGD-lr0.001-clip50-initfromfixqian30k-fixqian
INFO - root - 2017-12-11 10:18:18.857891: step 54030, loss = 0.70, batch loss = 0.64 (31.3 examples/sec; 0.256 sec/batch; 19h:46m:04s remains)
INFO - root - 2017-12-11 10:18:21.513148: step 54040, loss = 0.71, batch loss = 0.65 (29.5 examples/sec; 0.271 sec/batch; 20h:57m:28s remains)
INFO - root - 2017-12-11 10:18:24.169754: step 54050, loss = 0.68, batch loss = 0.62 (29.1 examples/sec; 0.275 sec/batch; 21h:14m:56s remains)
INFO - root - 2017-12-11 10:18:26.818066: step 54060, loss = 0.67, batch loss = 0.62 (27.5 examples/sec; 0.291 sec/batch; 22h:29m:08s remains)
INFO - root - 2017-12-11 10:18:29.457245: step 54070, loss = 0.68, batch loss = 0.62 (30.4 examples/sec; 0.263 sec/batch; 20h:21m:27s remains)
INFO - root - 2017-12-11 10:18:32.123102: step 54080, loss = 0.69, batch loss = 0.63 (30.4 examples/sec; 0.263 sec/batch; 20h:22m:09s remains)
INFO - root - 2017-12-11 10:18:34.745846: step 54090, loss = 0.69, batch loss = 0.63 (29.8 examples/sec; 0.269 sec/batch; 20h:46m:58s remains)
INFO - root - 2017-12-11 10:18:37.408028: step 54100, loss = 0.70, batch loss = 0.64 (30.2 examples/sec; 0.265 sec/batch; 20h:30m:37s remains)
2017-12-11 10:18:37.786714: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.21753947 0.22928257 0.23204321 0.23331816 0.23139518 0.2191572 0.19855866 0.17340457 0.14996375 0.13348904 0.11953275 0.10667817 0.0924308 0.081403404 0.079472706][0.20980176 0.22464634 0.22518951 0.21795543 0.20270434 0.17659461 0.14625424 0.11815287 0.098989442 0.091292083 0.088017032 0.084884971 0.078509904 0.072249718 0.071067095][0.19550569 0.21614257 0.2179458 0.20619914 0.1819163 0.14614263 0.11003205 0.083107047 0.071579896 0.073416524 0.078671917 0.081841938 0.07979352 0.074893348 0.071263522][0.19055772 0.21813005 0.22492595 0.21499528 0.18903223 0.15051356 0.11455572 0.093466192 0.091549151 0.10143765 0.11041799 0.1131684 0.1084883 0.099338867 0.089636862][0.18997797 0.22508778 0.24147357 0.24086572 0.22158234 0.18762261 0.15672648 0.14359376 0.15012971 0.16413927 0.17094396 0.16697498 0.15480153 0.13892716 0.1228174][0.18730104 0.22993238 0.2590332 0.27315062 0.26649642 0.24206688 0.21855621 0.21230982 0.2232884 0.23617136 0.23639557 0.22348017 0.20478649 0.18617734 0.1689917][0.1851819 0.2311092 0.26974538 0.29703176 0.30328816 0.28990412 0.27420762 0.27249843 0.28326565 0.29041508 0.28236911 0.2623373 0.24233682 0.22858284 0.21815068][0.1876023 0.22957878 0.26928785 0.30151156 0.31489146 0.30943891 0.29925963 0.29889888 0.30643666 0.30717951 0.29410002 0.27293268 0.25853348 0.25603747 0.25730389][0.20016176 0.23042281 0.26039067 0.28597453 0.29712018 0.29312751 0.2840054 0.28140876 0.28420839 0.2806769 0.26816553 0.25250867 0.24859697 0.25934386 0.27234989][0.22552678 0.24009575 0.25401157 0.26531112 0.26704127 0.25804743 0.24456654 0.23558851 0.23220529 0.2266928 0.21925616 0.2135582 0.22190847 0.24441272 0.26637655][0.2548019 0.25541243 0.25305057 0.24854463 0.23820658 0.22060494 0.19984193 0.18355551 0.17552471 0.17168075 0.17253153 0.17809907 0.1966272 0.22590671 0.25154781][0.27541021 0.26790515 0.25439984 0.23790877 0.21730924 0.19144559 0.16429262 0.143594 0.13516203 0.13700387 0.1473563 0.16228427 0.18621825 0.21592668 0.23966978][0.27584323 0.26825586 0.25173691 0.23013486 0.20412578 0.17366335 0.14384201 0.12323876 0.1188192 0.12954962 0.14949858 0.1709867 0.19576696 0.22145833 0.23987865][0.24738115 0.24407408 0.23005681 0.20939772 0.1839934 0.15526725 0.12921427 0.1142969 0.11711857 0.13700269 0.16407514 0.18823628 0.21017472 0.22981733 0.24204326][0.18832545 0.18921573 0.17919368 0.16264267 0.1424856 0.12155353 0.1055911 0.10139741 0.11377838 0.14162654 0.17242554 0.19533855 0.21153308 0.22386713 0.22929707]]...]
INFO - root - 2017-12-11 10:18:40.405959: step 54110, loss = 0.70, batch loss = 0.65 (31.5 examples/sec; 0.254 sec/batch; 19h:37m:41s remains)
INFO - root - 2017-12-11 10:18:43.060226: step 54120, loss = 0.70, batch loss = 0.64 (30.3 examples/sec; 0.264 sec/batch; 20h:23m:17s remains)
INFO - root - 2017-12-11 10:18:45.691888: step 54130, loss = 0.70, batch loss = 0.64 (30.4 examples/sec; 0.263 sec/batch; 20h:20m:09s remains)
INFO - root - 2017-12-11 10:18:48.311326: step 54140, loss = 0.70, batch loss = 0.64 (30.9 examples/sec; 0.259 sec/batch; 20h:00m:35s remains)
INFO - root - 2017-12-11 10:18:50.936780: step 54150, loss = 0.70, batch loss = 0.65 (29.9 examples/sec; 0.267 sec/batch; 20h:39m:21s remains)
INFO - root - 2017-12-11 10:18:53.549386: step 54160, loss = 0.71, batch loss = 0.65 (30.9 examples/sec; 0.259 sec/batch; 20h:00m:48s remains)
INFO - root - 2017-12-11 10:18:56.189537: step 54170, loss = 0.67, batch loss = 0.61 (28.4 examples/sec; 0.282 sec/batch; 21h:45m:53s remains)
INFO - root - 2017-12-11 10:18:58.845080: step 54180, loss = 0.70, batch loss = 0.64 (30.6 examples/sec; 0.261 sec/batch; 20h:12m:44s remains)
INFO - root - 2017-12-11 10:19:01.425953: step 54190, loss = 0.69, batch loss = 0.63 (30.7 examples/sec; 0.261 sec/batch; 20h:08m:21s remains)
INFO - root - 2017-12-11 10:19:04.113171: step 54200, loss = 0.69, batch loss = 0.63 (30.2 examples/sec; 0.265 sec/batch; 20h:27m:18s remains)
2017-12-11 10:19:04.496977: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.10664179 0.18094988 0.27866971 0.37369335 0.44353107 0.46880391 0.44880068 0.39368671 0.32178962 0.25853634 0.21431813 0.18259715 0.15252109 0.12080133 0.083143786][0.016062809 0.088510856 0.19427308 0.30626559 0.39578882 0.43887776 0.43010622 0.37571919 0.29040262 0.19824226 0.11986608 0.054477386 -0.0035602876 -0.050283197 -0.086045094][-0.032891359 0.035983019 0.14394544 0.26451907 0.36623758 0.42521539 0.43381137 0.39204365 0.30567768 0.19543055 0.089208372 -0.007532448 -0.0930687 -0.15546207 -0.19022399][-0.013357515 0.060301282 0.17269261 0.29821327 0.40427449 0.47250819 0.49672207 0.47075582 0.3905735 0.27216861 0.14756159 0.025205521 -0.086976089 -0.16842954 -0.2098278][0.062223729 0.14885938 0.26882404 0.39726689 0.50209385 0.57323438 0.60812175 0.595163 0.52213174 0.40186325 0.26642776 0.12367028 -0.013751335 -0.11688252 -0.1709021][0.1747147 0.27616313 0.40182808 0.52885586 0.6273163 0.69566625 0.73368609 0.72464907 0.65140617 0.52722788 0.38348514 0.22579977 0.070593946 -0.047662325 -0.11053283][0.29555067 0.40800691 0.53365964 0.65315479 0.74019814 0.79862517 0.8276763 0.80790573 0.72166044 0.58609617 0.43269661 0.26724616 0.10841978 -0.0098204045 -0.069901384][0.38721254 0.50121909 0.618261 0.72416741 0.7967003 0.84125042 0.8545931 0.81630754 0.71150672 0.56225216 0.40059102 0.2353337 0.086498372 -0.017412644 -0.064363711][0.41844615 0.52040106 0.61849689 0.70476043 0.76134735 0.79228497 0.79221177 0.74075681 0.62619042 0.47309855 0.31261268 0.157811 0.028978776 -0.05267885 -0.082438804][0.37671927 0.45324484 0.52258688 0.58313107 0.62228078 0.64167857 0.634839 0.58313787 0.47651529 0.33789444 0.19498314 0.064325653 -0.034962 -0.090031527 -0.10203057][0.26969209 0.3117424 0.34717977 0.37999487 0.4022384 0.41315535 0.4057512 0.36422747 0.27945346 0.16973627 0.058913738 -0.035172775 -0.097230941 -0.12285554 -0.1173582][0.1272077 0.1354315 0.14056207 0.1494054 0.15731898 0.16187769 0.15694951 0.13070804 0.075147614 0.0030569651 -0.066441424 -0.11807098 -0.14304331 -0.1432815 -0.1254442][-0.013990569 -0.029585505 -0.043729793 -0.049920797 -0.050597474 -0.04849916 -0.047527455 -0.055954706 -0.080466859 -0.11383531 -0.14317197 -0.1585127 -0.15726659 -0.1434543 -0.12250358][-0.12342466 -0.14919163 -0.16957651 -0.18070172 -0.18396606 -0.18165348 -0.17560117 -0.1707385 -0.17179686 -0.17567079 -0.17570364 -0.16757342 -0.15229556 -0.1335654 -0.11484695][-0.17699638 -0.20200297 -0.21957454 -0.22951929 -0.23287913 -0.23127732 -0.22531068 -0.21696241 -0.20832634 -0.19817081 -0.18406253 -0.16555616 -0.14516585 -0.12602758 -0.11035026]]...]
INFO - root - 2017-12-11 10:19:07.140270: step 54210, loss = 0.70, batch loss = 0.64 (29.6 examples/sec; 0.270 sec/batch; 20h:52m:56s remains)
INFO - root - 2017-12-11 10:19:09.805163: step 54220, loss = 0.71, batch loss = 0.65 (29.3 examples/sec; 0.273 sec/batch; 21h:05m:38s remains)
INFO - root - 2017-12-11 10:19:12.478324: step 54230, loss = 0.70, batch loss = 0.65 (30.6 examples/sec; 0.261 sec/batch; 20h:11m:34s remains)
INFO - root - 2017-12-11 10:19:15.132735: step 54240, loss = 0.71, batch loss = 0.65 (31.2 examples/sec; 0.256 sec/batch; 19h:49m:30s remains)
INFO - root - 2017-12-11 10:19:17.766003: step 54250, loss = 0.69, batch loss = 0.63 (30.2 examples/sec; 0.265 sec/batch; 20h:29m:11s remains)
INFO - root - 2017-12-11 10:19:20.422704: step 54260, loss = 0.71, batch loss = 0.66 (30.7 examples/sec; 0.261 sec/batch; 20h:08m:45s remains)
INFO - root - 2017-12-11 10:19:23.143759: step 54270, loss = 0.70, batch loss = 0.64 (30.9 examples/sec; 0.258 sec/batch; 19h:58m:37s remains)
INFO - root - 2017-12-11 10:19:25.785765: step 54280, loss = 0.70, batch loss = 0.64 (30.0 examples/sec; 0.266 sec/batch; 20h:35m:07s remains)
INFO - root - 2017-12-11 10:19:28.447417: step 54290, loss = 0.71, batch loss = 0.65 (30.0 examples/sec; 0.267 sec/batch; 20h:38m:26s remains)
INFO - root - 2017-12-11 10:19:31.085480: step 54300, loss = 0.69, batch loss = 0.63 (28.5 examples/sec; 0.281 sec/batch; 21h:42m:36s remains)
2017-12-11 10:19:31.471801: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.03364728 0.0068506473 0.085094564 0.20124176 0.3352446 0.45065376 0.51463395 0.51162964 0.45716456 0.39558089 0.36480796 0.36801815 0.37545943 0.36405319 0.34143782][-0.038403612 -0.00079644017 0.074166447 0.18809572 0.32249054 0.44184953 0.51203126 0.51419473 0.45912582 0.38710749 0.33598223 0.31663483 0.31150219 0.30411762 0.29935959][-0.040022455 -0.0042005922 0.068104677 0.18001936 0.31503668 0.43926257 0.51710635 0.5256725 0.46931186 0.38298652 0.306395 0.26131138 0.24437678 0.24506669 0.26302353][-0.036791041 0.0012270737 0.07650923 0.19215253 0.33241183 0.4635101 0.54801363 0.55894858 0.4955126 0.38862038 0.28262445 0.21161462 0.18502331 0.19738218 0.24119467][-0.030798234 0.012078934 0.094357036 0.21800865 0.36722296 0.50733316 0.59843946 0.60925114 0.53463364 0.40356138 0.26690888 0.17076652 0.13499363 0.15973659 0.23124772][-0.024313463 0.024725983 0.1157153 0.25027287 0.41198623 0.56409752 0.66276515 0.67123878 0.58087385 0.42303765 0.25753498 0.14165761 0.10203547 0.14188148 0.24300687][-0.017028779 0.038261294 0.13872238 0.2856698 0.46073374 0.62308294 0.723847 0.72239578 0.6099208 0.4254365 0.2394947 0.11763232 0.088376746 0.15267918 0.28827363][-0.011107788 0.049186558 0.15751955 0.31448439 0.49879634 0.664445 0.75802928 0.73830879 0.60115582 0.39652652 0.20367225 0.091772586 0.086570211 0.1830117 0.35370931][-0.0088660279 0.052585971 0.16250904 0.32000631 0.50055629 0.65480119 0.72937196 0.68764055 0.534002 0.32701454 0.14752711 0.060813725 0.085732505 0.21078798 0.40442538][-0.010845765 0.047036044 0.15007088 0.29464176 0.45402417 0.57986891 0.62586147 0.565945 0.4133215 0.2272952 0.080297783 0.026655145 0.077498585 0.21726738 0.41277882][-0.017672563 0.0325947 0.12108094 0.24107683 0.36566353 0.45240727 0.46740103 0.39719468 0.26083893 0.1111185 0.0062218173 -0.013732079 0.055181012 0.19452186 0.37072662][-0.029620668 0.0092605595 0.077594712 0.16615891 0.25036916 0.29685798 0.28541258 0.21352139 0.10175927 -0.0073070815 -0.071065828 -0.063392244 0.013156581 0.13841532 0.28038356][-0.045293916 -0.020416513 0.024906389 0.081107423 0.12822726 0.14262687 0.11537348 0.050752755 -0.033050768 -0.10483214 -0.13623348 -0.1129919 -0.042579088 0.055618722 0.15494132][-0.059052721 -0.047444277 -0.022874875 0.006125256 0.025225619 0.019406758 -0.012753083 -0.063905984 -0.1199844 -0.16101623 -0.17149282 -0.14668094 -0.095795795 -0.033046037 0.024126275][-0.063822329 -0.058816776 -0.045525875 -0.03226272 -0.029421518 -0.045769807 -0.077709153 -0.11619393 -0.15091825 -0.17151198 -0.17280528 -0.1560037 -0.12873495 -0.098220848 -0.072967276]]...]
INFO - root - 2017-12-11 10:19:34.139663: step 54310, loss = 0.69, batch loss = 0.63 (28.3 examples/sec; 0.283 sec/batch; 21h:52m:48s remains)
INFO - root - 2017-12-11 10:19:36.766916: step 54320, loss = 0.70, batch loss = 0.64 (30.8 examples/sec; 0.260 sec/batch; 20h:06m:09s remains)
INFO - root - 2017-12-11 10:19:39.459917: step 54330, loss = 0.67, batch loss = 0.61 (27.9 examples/sec; 0.287 sec/batch; 22h:10m:59s remains)
INFO - root - 2017-12-11 10:19:42.095575: step 54340, loss = 0.70, batch loss = 0.64 (30.7 examples/sec; 0.261 sec/batch; 20h:07m:51s remains)
INFO - root - 2017-12-11 10:19:44.734727: step 54350, loss = 0.70, batch loss = 0.64 (30.4 examples/sec; 0.264 sec/batch; 20h:21m:49s remains)
INFO - root - 2017-12-11 10:19:47.377451: step 54360, loss = 0.70, batch loss = 0.64 (31.1 examples/sec; 0.257 sec/batch; 19h:51m:21s remains)
INFO - root - 2017-12-11 10:19:50.003306: step 54370, loss = 0.70, batch loss = 0.64 (30.8 examples/sec; 0.260 sec/batch; 20h:04m:04s remains)
INFO - root - 2017-12-11 10:19:52.602522: step 54380, loss = 0.69, batch loss = 0.63 (31.5 examples/sec; 0.254 sec/batch; 19h:38m:56s remains)
INFO - root - 2017-12-11 10:19:55.227343: step 54390, loss = 0.71, batch loss = 0.65 (29.7 examples/sec; 0.270 sec/batch; 20h:50m:12s remains)
INFO - root - 2017-12-11 10:19:57.891453: step 54400, loss = 0.70, batch loss = 0.64 (30.1 examples/sec; 0.266 sec/batch; 20h:30m:46s remains)
2017-12-11 10:19:58.256328: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.051619966 -0.032237627 0.0041973917 0.055149119 0.11822575 0.1889381 0.25346306 0.28664663 0.283316 0.24810332 0.19709739 0.1497549 0.118861 0.11218946 0.12815435][-0.053328052 -0.032142811 0.0095058372 0.068739727 0.14233431 0.22351491 0.29646724 0.33548933 0.33659944 0.30541071 0.25663611 0.20736699 0.17188649 0.15672065 0.16092484][-0.053145304 -0.03119427 0.013097897 0.077557854 0.15708722 0.24312463 0.31880087 0.36098808 0.36853096 0.34623984 0.30574816 0.26055178 0.22493759 0.20489793 0.19942284][-0.054178614 -0.031916764 0.013449967 0.08031331 0.16201517 0.24909553 0.32363349 0.36627492 0.37955007 0.3671535 0.33825815 0.3029002 0.27479964 0.25763878 0.2486739][-0.05599666 -0.033404 0.012549218 0.079262637 0.15964551 0.24481444 0.31637368 0.35810807 0.37537771 0.3716535 0.35524812 0.33323708 0.31752828 0.30844295 0.29931015][-0.057502337 -0.033375453 0.014373429 0.082887791 0.1648953 0.250824 0.32148722 0.3619743 0.37859362 0.37501383 0.36138487 0.34442633 0.33582217 0.33302221 0.32391116][-0.058348943 -0.03281131 0.017472619 0.090950057 0.18070179 0.2744126 0.34913066 0.38802442 0.39638367 0.379058 0.34955764 0.3189519 0.30359817 0.30152705 0.29546109][-0.058193538 -0.03195582 0.019974263 0.098027617 0.19640258 0.299335 0.3789809 0.41585544 0.41298255 0.37713286 0.32398036 0.27037263 0.23911363 0.23178346 0.22902195][-0.057208516 -0.030668553 0.021746328 0.10186626 0.20476636 0.31131673 0.39072832 0.42231157 0.4081687 0.35750461 0.28524175 0.21166353 0.16385131 0.14819251 0.14600065][-0.056002606 -0.029077107 0.023080187 0.10268656 0.20431511 0.3061507 0.37745896 0.39922717 0.37469342 0.31529322 0.23345914 0.14936863 0.090671055 0.067351311 0.062637515][-0.055932738 -0.029570119 0.0212838 0.098182634 0.19409874 0.28544348 0.34326187 0.35234919 0.31777596 0.25232056 0.16679002 0.0802173 0.018208338 -0.0089063309 -0.015557969][-0.058395885 -0.035663195 0.0095273806 0.077435918 0.16040578 0.23556864 0.27736837 0.27468526 0.23370259 0.16721302 0.08550825 0.0061280061 -0.049567118 -0.07317435 -0.076535881][-0.063456878 -0.047662683 -0.013956309 0.036748689 0.098578662 0.15285274 0.17974268 0.1713604 0.13369037 0.07754986 0.011870097 -0.049329884 -0.09080974 -0.10692594 -0.10677132][-0.068982646 -0.060594045 -0.039876942 -0.0089815464 0.029002218 0.061546039 0.075805955 0.066541307 0.038662348 -0.00014156711 -0.043321807 -0.081707492 -0.10666709 -0.11529246 -0.11359543][-0.07337904 -0.070690654 -0.059815019 -0.044413943 -0.025674639 -0.010600412 -0.0061058351 -0.015122331 -0.03341401 -0.05563641 -0.077743948 -0.09531334 -0.10522418 -0.1071852 -0.10477506]]...]
INFO - root - 2017-12-11 10:20:00.905331: step 54410, loss = 0.69, batch loss = 0.64 (31.8 examples/sec; 0.252 sec/batch; 19h:26m:20s remains)
INFO - root - 2017-12-11 10:20:03.526736: step 54420, loss = 0.69, batch loss = 0.63 (31.3 examples/sec; 0.256 sec/batch; 19h:46m:06s remains)
INFO - root - 2017-12-11 10:20:06.103770: step 54430, loss = 0.71, batch loss = 0.65 (31.1 examples/sec; 0.257 sec/batch; 19h:52m:44s remains)
INFO - root - 2017-12-11 10:20:08.750912: step 54440, loss = 0.71, batch loss = 0.65 (31.3 examples/sec; 0.255 sec/batch; 19h:43m:21s remains)
INFO - root - 2017-12-11 10:20:11.425899: step 54450, loss = 0.68, batch loss = 0.63 (30.6 examples/sec; 0.262 sec/batch; 20h:13m:21s remains)
INFO - root - 2017-12-11 10:20:14.015787: step 54460, loss = 0.70, batch loss = 0.64 (31.4 examples/sec; 0.255 sec/batch; 19h:42m:01s remains)
INFO - root - 2017-12-11 10:20:16.695354: step 54470, loss = 0.69, batch loss = 0.63 (28.7 examples/sec; 0.279 sec/batch; 21h:33m:50s remains)
INFO - root - 2017-12-11 10:20:19.311102: step 54480, loss = 0.70, batch loss = 0.64 (30.4 examples/sec; 0.263 sec/batch; 20h:17m:38s remains)
INFO - root - 2017-12-11 10:20:22.011692: step 54490, loss = 0.72, batch loss = 0.66 (27.6 examples/sec; 0.290 sec/batch; 22h:21m:38s remains)
INFO - root - 2017-12-11 10:20:24.690774: step 54500, loss = 0.70, batch loss = 0.64 (30.2 examples/sec; 0.265 sec/batch; 20h:28m:50s remains)
2017-12-11 10:20:25.052622: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.33853436 0.39171031 0.44464812 0.488304 0.51628011 0.52341574 0.48597482 0.41750869 0.33966815 0.26712313 0.21688984 0.19375257 0.20177387 0.23014891 0.26166585][0.39047956 0.43594447 0.48042908 0.51827627 0.54401207 0.55273634 0.51867092 0.4546755 0.38040009 0.31056121 0.26613173 0.25021684 0.26524171 0.29875717 0.33397549][0.37990925 0.41442129 0.44758487 0.47848374 0.50370574 0.518955 0.49845296 0.4508307 0.38985983 0.32900015 0.29226714 0.28174031 0.29811981 0.32987681 0.36352354][0.31786495 0.34337342 0.36990121 0.39981568 0.43199879 0.46202722 0.46437767 0.44017267 0.39527017 0.34156314 0.305316 0.28958002 0.29603812 0.31720746 0.34487581][0.22257777 0.24287251 0.27089095 0.30938941 0.358499 0.41163149 0.44286028 0.44424877 0.41404507 0.36267567 0.31679341 0.28274342 0.26670736 0.26847073 0.28606138][0.12667644 0.14706069 0.18483548 0.24058424 0.31406826 0.39465991 0.45412269 0.47541639 0.45242813 0.39574647 0.3306329 0.26808739 0.22142431 0.19874956 0.20337772][0.062980339 0.089776941 0.14378843 0.22177498 0.3218576 0.42840195 0.50889021 0.53786916 0.50957376 0.43822628 0.34736973 0.25281289 0.1757655 0.13193022 0.12563674][0.043189533 0.078106046 0.14764674 0.24613538 0.36994964 0.49736917 0.58974236 0.61444741 0.57059461 0.47941595 0.36347792 0.24246339 0.14326788 0.086616717 0.073863283][0.061463304 0.10157052 0.17984888 0.29082376 0.43010467 0.56988108 0.66522717 0.67899615 0.61685258 0.50792491 0.37459737 0.23922765 0.13042544 0.070145883 0.055005312][0.11173475 0.1510838 0.227134 0.33670032 0.47568044 0.61333984 0.70218873 0.70422673 0.63007486 0.51414561 0.3785252 0.24607734 0.1420659 0.086030751 0.071483217][0.18224955 0.21825221 0.28497019 0.38120964 0.50365937 0.62368095 0.69645536 0.68692529 0.60796386 0.49488181 0.36948773 0.25294808 0.1630652 0.11457052 0.10137434][0.257048 0.28947118 0.34315345 0.41884968 0.51280993 0.60256606 0.65085226 0.62903863 0.5498144 0.445401 0.33706185 0.24270897 0.17117323 0.13139017 0.12044533][0.30981183 0.33596504 0.3746213 0.42711711 0.48755178 0.54118091 0.56158751 0.52895892 0.45424324 0.36394033 0.27732453 0.2074922 0.15531766 0.12458425 0.11675916][0.31342909 0.3306393 0.35383949 0.38331077 0.41138861 0.43076992 0.42636544 0.38672614 0.32068378 0.24836515 0.18581504 0.14039744 0.10762379 0.087517306 0.084483825][0.25385204 0.26091677 0.27058908 0.28138286 0.28503656 0.27953088 0.25895619 0.21809812 0.16446629 0.11239945 0.073911145 0.051345021 0.037306748 0.029339803 0.03256331]]...]
INFO - root - 2017-12-11 10:20:27.641902: step 54510, loss = 0.70, batch loss = 0.64 (31.6 examples/sec; 0.253 sec/batch; 19h:32m:31s remains)
INFO - root - 2017-12-11 10:20:30.313866: step 54520, loss = 0.70, batch loss = 0.65 (29.6 examples/sec; 0.270 sec/batch; 20h:51m:17s remains)
INFO - root - 2017-12-11 10:20:32.972145: step 54530, loss = 0.69, batch loss = 0.63 (29.8 examples/sec; 0.268 sec/batch; 20h:43m:29s remains)
INFO - root - 2017-12-11 10:20:35.634807: step 54540, loss = 0.69, batch loss = 0.63 (31.2 examples/sec; 0.257 sec/batch; 19h:48m:47s remains)
INFO - root - 2017-12-11 10:20:38.283930: step 54550, loss = 0.71, batch loss = 0.65 (30.5 examples/sec; 0.262 sec/batch; 20h:15m:49s remains)
INFO - root - 2017-12-11 10:20:40.914214: step 54560, loss = 0.70, batch loss = 0.64 (30.2 examples/sec; 0.265 sec/batch; 20h:28m:17s remains)
INFO - root - 2017-12-11 10:20:43.582055: step 54570, loss = 0.68, batch loss = 0.62 (29.6 examples/sec; 0.270 sec/batch; 20h:51m:25s remains)
INFO - root - 2017-12-11 10:20:46.279197: step 54580, loss = 0.72, batch loss = 0.66 (29.5 examples/sec; 0.271 sec/batch; 20h:54m:46s remains)
INFO - root - 2017-12-11 10:20:48.864475: step 54590, loss = 0.70, batch loss = 0.65 (30.5 examples/sec; 0.262 sec/batch; 20h:12m:58s remains)
INFO - root - 2017-12-11 10:20:51.565160: step 54600, loss = 0.71, batch loss = 0.65 (29.8 examples/sec; 0.268 sec/batch; 20h:41m:42s remains)
2017-12-11 10:20:51.946034: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.1138747 0.11020705 0.10715622 0.102434 0.10346453 0.1133958 0.12256534 0.1200845 0.10480902 0.085322805 0.073545292 0.072823443 0.079487309 0.0846524 0.087529562][0.10881653 0.12108057 0.13326046 0.13795683 0.14093873 0.14966194 0.15894413 0.15787677 0.14073053 0.11284311 0.088425875 0.075734325 0.076453954 0.081610829 0.087359369][0.12622036 0.15045312 0.1714125 0.18104178 0.18629624 0.19904926 0.2151635 0.22028029 0.20169227 0.16086182 0.11588369 0.083154559 0.072399326 0.075468354 0.084217854][0.16310932 0.19191255 0.2140329 0.22555479 0.23694091 0.26194817 0.29266602 0.30734938 0.28533539 0.22586891 0.15400025 0.096597582 0.071279123 0.070184581 0.0809375][0.20675847 0.23324217 0.24941672 0.25989839 0.28016385 0.32303959 0.37245286 0.39656717 0.36741835 0.28548592 0.18565458 0.10551132 0.067934893 0.06358777 0.076080546][0.24845943 0.26794893 0.2723287 0.27765548 0.30639133 0.36906284 0.43881196 0.47187674 0.43437245 0.33122504 0.20797843 0.11120521 0.065715633 0.059467889 0.072712556][0.27632388 0.28852525 0.279629 0.27729139 0.31165895 0.39129147 0.47884005 0.5189755 0.47407576 0.35545534 0.21783677 0.11343496 0.065465853 0.058750294 0.071790233][0.29010126 0.29446039 0.27153859 0.25944683 0.29523268 0.38529298 0.48385429 0.52689332 0.47597632 0.34913802 0.20769042 0.10550271 0.061460849 0.057303514 0.070749909][0.29736951 0.29449195 0.25916371 0.23774181 0.27237189 0.36656287 0.468787 0.51054472 0.455158 0.326304 0.18870844 0.094796076 0.057902496 0.057008289 0.070233166][0.30261153 0.29796308 0.25738949 0.23145533 0.26436237 0.35694522 0.45571229 0.49280727 0.43563107 0.31110609 0.18288068 0.098991536 0.067748487 0.067045748 0.076461077][0.296269 0.29517123 0.25675523 0.23021889 0.25892437 0.34318763 0.43261537 0.46489584 0.41185224 0.29928488 0.18466455 0.11056196 0.083325014 0.08206556 0.087436244][0.27873653 0.28274074 0.24981484 0.22410548 0.24618939 0.31757694 0.3945297 0.42377755 0.38094184 0.28635502 0.18710567 0.12034287 0.094390385 0.092566058 0.096356027][0.26242402 0.27093348 0.24302065 0.21733087 0.23108628 0.28728917 0.35032386 0.37734398 0.3474758 0.27358288 0.19058026 0.13059118 0.10593326 0.10535788 0.11065914][0.24918668 0.26197463 0.24026628 0.21602122 0.22220783 0.26347509 0.31267259 0.3368423 0.31818822 0.26279041 0.1944896 0.14069837 0.11759709 0.11941472 0.1285897][0.23694153 0.253678 0.23939426 0.21828474 0.2179196 0.24465595 0.27968144 0.29894775 0.28762242 0.24681488 0.19129857 0.14423266 0.12440117 0.13040848 0.14534582]]...]
INFO - root - 2017-12-11 10:20:54.561956: step 54610, loss = 0.70, batch loss = 0.64 (30.1 examples/sec; 0.266 sec/batch; 20h:31m:47s remains)
INFO - root - 2017-12-11 10:20:57.211809: step 54620, loss = 0.71, batch loss = 0.65 (30.9 examples/sec; 0.259 sec/batch; 19h:57m:20s remains)
INFO - root - 2017-12-11 10:20:59.918563: step 54630, loss = 0.71, batch loss = 0.65 (30.3 examples/sec; 0.264 sec/batch; 20h:24m:26s remains)
INFO - root - 2017-12-11 10:21:02.535906: step 54640, loss = 0.71, batch loss = 0.65 (31.0 examples/sec; 0.258 sec/batch; 19h:55m:55s remains)
INFO - root - 2017-12-11 10:21:05.226557: step 54650, loss = 0.70, batch loss = 0.64 (30.2 examples/sec; 0.265 sec/batch; 20h:25m:26s remains)
INFO - root - 2017-12-11 10:21:07.848669: step 54660, loss = 0.69, batch loss = 0.63 (30.1 examples/sec; 0.266 sec/batch; 20h:29m:42s remains)
INFO - root - 2017-12-11 10:21:10.543180: step 54670, loss = 0.70, batch loss = 0.64 (27.8 examples/sec; 0.288 sec/batch; 22h:12m:23s remains)
INFO - root - 2017-12-11 10:21:13.123134: step 54680, loss = 0.70, batch loss = 0.64 (30.3 examples/sec; 0.264 sec/batch; 20h:23m:42s remains)
INFO - root - 2017-12-11 10:21:15.756014: step 54690, loss = 0.71, batch loss = 0.65 (30.1 examples/sec; 0.266 sec/batch; 20h:31m:11s remains)
INFO - root - 2017-12-11 10:21:18.375898: step 54700, loss = 0.69, batch loss = 0.63 (31.5 examples/sec; 0.254 sec/batch; 19h:37m:17s remains)
2017-12-11 10:21:18.761347: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.12979969 0.16476688 0.19620626 0.22422825 0.24337983 0.2509582 0.241178 0.21456252 0.18769398 0.16945344 0.17053907 0.18987916 0.22172439 0.26644039 0.32000822][0.11782043 0.14311716 0.15669923 0.16041932 0.15525536 0.14769343 0.13690311 0.11971564 0.10625488 0.10263464 0.11571393 0.13907306 0.16605577 0.20325898 0.25308082][0.11962616 0.13548884 0.13549802 0.12248804 0.10332277 0.0920895 0.090429306 0.088722691 0.089142762 0.0942032 0.10869867 0.1236978 0.13436522 0.15448821 0.19258077][0.14399895 0.15734358 0.15647301 0.14429097 0.13017239 0.13113153 0.1471419 0.16033281 0.16540039 0.16368848 0.16348298 0.15700181 0.1419964 0.13701481 0.15474282][0.18584409 0.20211369 0.21175215 0.21672685 0.22452226 0.24894829 0.2845417 0.30537647 0.30162805 0.27815339 0.25090131 0.21568313 0.17154562 0.13893625 0.13237432][0.22122023 0.24097143 0.26693597 0.29808038 0.33642587 0.38670263 0.43634731 0.45620394 0.43652096 0.38745138 0.33232027 0.27296975 0.2084749 0.15593857 0.12880971][0.22990385 0.25352082 0.29859719 0.35786384 0.42376038 0.49128696 0.544187 0.55499637 0.5185225 0.45049465 0.3793835 0.31217203 0.24521498 0.18787394 0.14970462][0.21034934 0.23758449 0.29959622 0.38077843 0.46201152 0.53101307 0.5740571 0.57139158 0.52478552 0.45336136 0.38633546 0.33233485 0.28224969 0.23544899 0.19509715][0.17116591 0.20047297 0.27201661 0.36315975 0.44601369 0.50366485 0.52937305 0.51493913 0.46901625 0.41103798 0.36682385 0.34212628 0.32028759 0.29163733 0.25365216][0.12137984 0.15153363 0.22449705 0.31511813 0.39254794 0.43730929 0.4493005 0.43136457 0.39675519 0.36292571 0.34942445 0.35710165 0.36178744 0.34716758 0.30877087][0.0736586 0.10542135 0.1755465 0.26062578 0.33174598 0.36824134 0.373514 0.35818195 0.33822259 0.32802808 0.34046155 0.37183368 0.39265835 0.38235486 0.33838391][0.042237591 0.079664692 0.14708027 0.22527786 0.28920454 0.3181386 0.31725118 0.30224571 0.29048836 0.29345655 0.31930134 0.36106589 0.38719523 0.3752836 0.32659897][0.034658197 0.081074066 0.14755243 0.21654868 0.26830959 0.28489527 0.27317926 0.25159743 0.23758353 0.24043013 0.26632723 0.30711633 0.33255726 0.32038492 0.27492157][0.047705512 0.10379425 0.16883272 0.22483242 0.25830671 0.25801942 0.23294005 0.20164581 0.17953357 0.17350768 0.18995427 0.22315688 0.24685861 0.23918329 0.20552249][0.077372134 0.13792774 0.19765648 0.23739715 0.24900883 0.23041505 0.1936678 0.15589301 0.12815961 0.11374514 0.1199134 0.14554606 0.16931839 0.16951154 0.15048444]]...]
INFO - root - 2017-12-11 10:21:21.377980: step 54710, loss = 0.69, batch loss = 0.63 (29.7 examples/sec; 0.269 sec/batch; 20h:46m:15s remains)
INFO - root - 2017-12-11 10:21:24.033991: step 54720, loss = 0.69, batch loss = 0.64 (31.3 examples/sec; 0.256 sec/batch; 19h:44m:07s remains)
INFO - root - 2017-12-11 10:21:26.689994: step 54730, loss = 0.71, batch loss = 0.65 (27.5 examples/sec; 0.291 sec/batch; 22h:26m:40s remains)
INFO - root - 2017-12-11 10:21:29.326443: step 54740, loss = 0.69, batch loss = 0.63 (30.7 examples/sec; 0.260 sec/batch; 20h:05m:41s remains)
INFO - root - 2017-12-11 10:21:31.944991: step 54750, loss = 0.70, batch loss = 0.64 (30.3 examples/sec; 0.264 sec/batch; 20h:23m:45s remains)
INFO - root - 2017-12-11 10:21:34.621427: step 54760, loss = 0.68, batch loss = 0.62 (30.9 examples/sec; 0.259 sec/batch; 19h:58m:44s remains)
INFO - root - 2017-12-11 10:21:37.234445: step 54770, loss = 0.69, batch loss = 0.63 (29.6 examples/sec; 0.270 sec/batch; 20h:49m:41s remains)
INFO - root - 2017-12-11 10:21:39.879434: step 54780, loss = 0.69, batch loss = 0.63 (29.6 examples/sec; 0.271 sec/batch; 20h:52m:12s remains)
INFO - root - 2017-12-11 10:21:42.541453: step 54790, loss = 0.70, batch loss = 0.65 (31.9 examples/sec; 0.251 sec/batch; 19h:19m:34s remains)
INFO - root - 2017-12-11 10:21:45.163630: step 54800, loss = 0.70, batch loss = 0.64 (29.6 examples/sec; 0.270 sec/batch; 20h:50m:08s remains)
2017-12-11 10:21:45.519063: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.23991601 0.25789294 0.2722019 0.26672071 0.23982634 0.20529078 0.16346058 0.11712489 0.085450612 0.088993363 0.12813938 0.17980418 0.22232434 0.25143352 0.26065698][0.23701972 0.26132935 0.28843504 0.29917103 0.28851756 0.26674598 0.22875634 0.17822917 0.14084855 0.14490642 0.19189534 0.25330296 0.30271456 0.3334341 0.33808967][0.21725273 0.2372821 0.268638 0.2940096 0.3041462 0.30129391 0.2746639 0.22800267 0.18967713 0.19122389 0.23503469 0.29313004 0.34001252 0.3677339 0.36826721][0.20901088 0.22085957 0.24964432 0.28498879 0.31263527 0.32543579 0.30933776 0.26839021 0.22977464 0.22314708 0.25236055 0.29501185 0.33028072 0.3495107 0.34489688][0.21465784 0.21910563 0.24278937 0.28178462 0.31849423 0.3388685 0.32941461 0.2948162 0.25692043 0.23988014 0.24939226 0.27068561 0.2882942 0.29386091 0.28254598][0.23163831 0.22998308 0.24498148 0.27913734 0.31485003 0.33526245 0.32975715 0.30302361 0.26887336 0.24378811 0.23473155 0.23399508 0.23154394 0.22098817 0.20303202][0.25932696 0.2536366 0.25722793 0.28039551 0.30866712 0.32485035 0.3206338 0.30023646 0.27037665 0.23995826 0.21577601 0.19553955 0.1745225 0.15030308 0.12895304][0.28671482 0.28177217 0.27895835 0.2936554 0.31468061 0.32536232 0.31851166 0.29914644 0.27066964 0.23739658 0.20476781 0.17224371 0.13893898 0.10733373 0.0875322][0.288024 0.289582 0.28891709 0.30184212 0.31989932 0.32718474 0.31759465 0.29819521 0.27257127 0.24363478 0.21353379 0.17915708 0.14110056 0.10719709 0.091522209][0.250615 0.26017404 0.26645991 0.28248006 0.30145183 0.3091664 0.30136427 0.28755972 0.27209413 0.25672096 0.23861381 0.21042632 0.17338225 0.14016545 0.12867057][0.18016046 0.19361059 0.20649809 0.22775809 0.25063062 0.26300964 0.26278195 0.26087075 0.26158854 0.26478425 0.2632221 0.24667864 0.21660614 0.18798551 0.18042381][0.10127616 0.11559179 0.13454139 0.16109681 0.18765055 0.20422883 0.21084757 0.21894588 0.23336965 0.25266868 0.267092 0.26482323 0.24667208 0.22687255 0.2232341][0.043416392 0.059540477 0.084154852 0.113975 0.14086753 0.15703186 0.16455272 0.17451936 0.19328208 0.22020842 0.24517152 0.25544992 0.24973199 0.23901692 0.23724842][0.014181062 0.033878785 0.06236871 0.092630543 0.11696742 0.12937871 0.13295053 0.13726631 0.15005705 0.17350288 0.19957732 0.21572249 0.21794833 0.21316448 0.21131398][0.018360188 0.039176006 0.067122638 0.094169475 0.11396655 0.12224795 0.1212941 0.11715242 0.11762741 0.12865001 0.14661695 0.16043277 0.16444664 0.16237462 0.16091563]]...]
INFO - root - 2017-12-11 10:21:48.137799: step 54810, loss = 0.69, batch loss = 0.63 (30.5 examples/sec; 0.262 sec/batch; 20h:13m:21s remains)
INFO - root - 2017-12-11 10:21:50.765150: step 54820, loss = 0.71, batch loss = 0.65 (30.6 examples/sec; 0.261 sec/batch; 20h:09m:16s remains)
INFO - root - 2017-12-11 10:21:53.337681: step 54830, loss = 0.69, batch loss = 0.63 (31.6 examples/sec; 0.253 sec/batch; 19h:30m:05s remains)
INFO - root - 2017-12-11 10:21:56.009377: step 54840, loss = 0.70, batch loss = 0.64 (31.2 examples/sec; 0.256 sec/batch; 19h:45m:26s remains)
INFO - root - 2017-12-11 10:21:58.622207: step 54850, loss = 0.70, batch loss = 0.64 (31.5 examples/sec; 0.254 sec/batch; 19h:34m:14s remains)
INFO - root - 2017-12-11 10:22:01.246697: step 54860, loss = 0.68, batch loss = 0.63 (29.7 examples/sec; 0.269 sec/batch; 20h:45m:13s remains)
INFO - root - 2017-12-11 10:22:03.857475: step 54870, loss = 0.70, batch loss = 0.65 (30.9 examples/sec; 0.259 sec/batch; 19h:56m:47s remains)
INFO - root - 2017-12-11 10:22:06.504266: step 54880, loss = 0.69, batch loss = 0.63 (29.9 examples/sec; 0.267 sec/batch; 20h:36m:35s remains)
INFO - root - 2017-12-11 10:22:09.176273: step 54890, loss = 0.70, batch loss = 0.64 (29.4 examples/sec; 0.272 sec/batch; 21h:00m:20s remains)
INFO - root - 2017-12-11 10:22:11.808771: step 54900, loss = 0.69, batch loss = 0.63 (31.5 examples/sec; 0.254 sec/batch; 19h:35m:23s remains)
2017-12-11 10:22:12.175403: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.40373275 0.39802113 0.36476162 0.32371354 0.28365767 0.26062167 0.26039049 0.28252289 0.31009766 0.33161354 0.35933906 0.3796199 0.38681862 0.38454878 0.37317881][0.35870287 0.35651967 0.33329314 0.30736446 0.28332484 0.27393466 0.28338891 0.31122947 0.3439334 0.37103018 0.40087143 0.41868624 0.42161265 0.4169569 0.4053019][0.29707351 0.29872465 0.28914484 0.28297481 0.277313 0.27968776 0.29457778 0.32397497 0.35899991 0.39213881 0.42595384 0.44481111 0.44706988 0.44269937 0.43365163][0.26175055 0.26452813 0.26690015 0.28112385 0.29689369 0.31531 0.34071815 0.37459609 0.41062006 0.4449876 0.47710824 0.49364579 0.49397781 0.49121642 0.48937917][0.2831693 0.28409773 0.29326957 0.32370684 0.36013675 0.39752808 0.43892527 0.47887564 0.51004 0.53323156 0.55097461 0.55562055 0.54974478 0.54928231 0.56027538][0.3539581 0.35347438 0.36559072 0.40467939 0.4546836 0.5073477 0.56353843 0.605834 0.62307179 0.62174314 0.61188114 0.59531486 0.577612 0.57855231 0.60475761][0.42616951 0.43010139 0.44339088 0.48211324 0.53437239 0.59190357 0.65341324 0.68997681 0.68713665 0.65659803 0.61674696 0.5776909 0.54668283 0.54530364 0.57967728][0.45626161 0.46621761 0.47757265 0.50615418 0.54603821 0.59188551 0.64272088 0.66487318 0.64306909 0.59124565 0.53210092 0.47949716 0.44026133 0.43439582 0.46683609][0.45845246 0.46849427 0.46941862 0.47576728 0.4870148 0.50340772 0.52766055 0.52856338 0.49172527 0.43005744 0.36540875 0.31093445 0.27099007 0.26145738 0.28603867][0.45615792 0.46003786 0.44207194 0.41588497 0.38606784 0.36036897 0.34776631 0.3245835 0.27786928 0.21714707 0.15929568 0.11344034 0.08041402 0.071013719 0.087977231][0.45629853 0.44777972 0.406335 0.34520859 0.27353835 0.2061718 0.15769623 0.11416912 0.064801261 0.01453782 -0.027620066 -0.058083232 -0.079229936 -0.084281266 -0.070923477][0.44457746 0.422394 0.36118233 0.27401331 0.17386749 0.08008755 0.0095452694 -0.043576602 -0.0875612 -0.12166642 -0.14489393 -0.15904538 -0.16787173 -0.16725667 -0.15475403][0.38639745 0.35383466 0.28292316 0.18721299 0.082002707 -0.012838044 -0.0827271 -0.1297982 -0.16036738 -0.17677481 -0.18376371 -0.18586512 -0.18620129 -0.18140294 -0.16860059][0.28271616 0.24447757 0.17461839 0.0871313 -0.002579632 -0.07799229 -0.12987719 -0.16002823 -0.17360809 -0.174969 -0.17130153 -0.16676043 -0.16220722 -0.15405846 -0.13927671][0.14178781 0.10441506 0.047101296 -0.017754421 -0.078020968 -0.12308245 -0.1492935 -0.15897436 -0.15650715 -0.14696229 -0.1370592 -0.12856647 -0.11986496 -0.10740821 -0.088625722]]...]
INFO - root - 2017-12-11 10:22:14.865798: step 54910, loss = 0.70, batch loss = 0.64 (24.5 examples/sec; 0.327 sec/batch; 25h:10m:55s remains)
INFO - root - 2017-12-11 10:22:17.598282: step 54920, loss = 0.70, batch loss = 0.64 (27.5 examples/sec; 0.291 sec/batch; 22h:27m:53s remains)
INFO - root - 2017-12-11 10:22:20.238589: step 54930, loss = 0.71, batch loss = 0.65 (29.9 examples/sec; 0.267 sec/batch; 20h:36m:10s remains)
INFO - root - 2017-12-11 10:22:22.913919: step 54940, loss = 0.68, batch loss = 0.62 (29.8 examples/sec; 0.268 sec/batch; 20h:40m:56s remains)
INFO - root - 2017-12-11 10:22:25.602400: step 54950, loss = 0.71, batch loss = 0.65 (30.2 examples/sec; 0.265 sec/batch; 20h:26m:14s remains)
INFO - root - 2017-12-11 10:22:28.197913: step 54960, loss = 0.70, batch loss = 0.65 (31.5 examples/sec; 0.254 sec/batch; 19h:35m:09s remains)
INFO - root - 2017-12-11 10:22:30.870103: step 54970, loss = 0.69, batch loss = 0.63 (27.3 examples/sec; 0.293 sec/batch; 22h:36m:28s remains)
INFO - root - 2017-12-11 10:22:33.537344: step 54980, loss = 0.69, batch loss = 0.63 (29.3 examples/sec; 0.273 sec/batch; 21h:01m:09s remains)
INFO - root - 2017-12-11 10:22:36.148618: step 54990, loss = 0.68, batch loss = 0.62 (31.5 examples/sec; 0.254 sec/batch; 19h:34m:12s remains)
INFO - root - 2017-12-11 10:22:38.802143: step 55000, loss = 0.70, batch loss = 0.65 (30.9 examples/sec; 0.259 sec/batch; 19h:57m:36s remains)
2017-12-11 10:22:39.160246: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.2398145 0.17723246 0.11340225 0.08222653 0.096083894 0.1402159 0.18663929 0.21326354 0.21336585 0.19447981 0.16487326 0.1328987 0.1042977 0.086631432 0.082603559][0.35834906 0.27747652 0.18994862 0.14171855 0.15103857 0.20202293 0.25998825 0.29738817 0.30502352 0.29017341 0.26091579 0.22520849 0.18979491 0.16448453 0.15371919][0.44917521 0.35497034 0.24916548 0.188339 0.19506559 0.25215933 0.31954408 0.36618122 0.381488 0.37176785 0.34425253 0.30726942 0.26757944 0.23561274 0.21632889][0.49803719 0.397607 0.28396547 0.2199658 0.22925413 0.29232997 0.3644869 0.41365674 0.43060386 0.42147079 0.39361838 0.35561189 0.31374085 0.27695987 0.24865256][0.49505782 0.3993291 0.29396307 0.24152857 0.26153442 0.33080921 0.40195134 0.44460103 0.45345432 0.43627939 0.40197071 0.35955542 0.31450132 0.27268383 0.23542468][0.4433006 0.36542892 0.28513095 0.25893745 0.29734826 0.37269998 0.43904582 0.46979237 0.46440616 0.43249124 0.38607237 0.33570546 0.28577945 0.23889551 0.19417496][0.35549608 0.30518925 0.26190123 0.27253449 0.33612376 0.42060685 0.48384252 0.50371277 0.4831388 0.43287075 0.36880496 0.30561146 0.24820915 0.19718929 0.1490878][0.25928417 0.24019942 0.23802145 0.28685966 0.37469721 0.46677181 0.52614784 0.53555518 0.50021279 0.43186238 0.34988266 0.27258644 0.20721588 0.15431343 0.10837869][0.175292 0.18771645 0.22408885 0.30340487 0.40508035 0.49550372 0.545579 0.54430449 0.49935928 0.42319429 0.33530977 0.25248379 0.18291503 0.12828718 0.084020011][0.10779244 0.14914104 0.21727486 0.31536505 0.41801643 0.49683961 0.53380424 0.52531374 0.48130283 0.41431078 0.33931848 0.26558027 0.19741371 0.13755924 0.086464137][0.061271358 0.12460644 0.21372719 0.31820473 0.41159475 0.47338623 0.49751619 0.48765621 0.45555535 0.41261843 0.36575076 0.3130264 0.25234267 0.18663016 0.12177422][0.04641005 0.12178149 0.22002336 0.32097402 0.39939925 0.44287044 0.45553318 0.44793078 0.4333047 0.42043126 0.40595523 0.37555215 0.32158473 0.24767859 0.16480273][0.0610565 0.13946351 0.23606685 0.32693151 0.3895148 0.41740695 0.42094806 0.41639858 0.41859692 0.43298322 0.44625348 0.43282467 0.38074836 0.29515228 0.19228362][0.088623255 0.16129686 0.24808 0.32619765 0.37638262 0.39481568 0.3929759 0.39011818 0.40271708 0.43397605 0.46438318 0.46103698 0.40884647 0.31406394 0.1973989][0.10105873 0.16060609 0.23129976 0.29583803 0.33851063 0.35518527 0.35377735 0.35254434 0.3679733 0.40268967 0.43673235 0.43645358 0.38564771 0.29063714 0.17399211]]...]
INFO - root - 2017-12-11 10:22:41.770514: step 55010, loss = 0.70, batch loss = 0.64 (30.5 examples/sec; 0.263 sec/batch; 20h:14m:08s remains)
INFO - root - 2017-12-11 10:22:44.472478: step 55020, loss = 0.69, batch loss = 0.63 (27.7 examples/sec; 0.289 sec/batch; 22h:14m:54s remains)
INFO - root - 2017-12-11 10:22:47.425262: step 55030, loss = 0.71, batch loss = 0.65 (28.8 examples/sec; 0.277 sec/batch; 21h:22m:46s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-SGD-lr0.001-clip50-initfromfixqian30k-fixqian
INFO - root - 2017-12-11 10:22:52.440420: step 55040, loss = 0.70, batch loss = 0.64 (14.3 examples/sec; 0.561 sec/batch; 43h:15m:28s remains)
INFO - root - 2017-12-11 10:22:58.018812: step 55050, loss = 0.70, batch loss = 0.64 (14.2 examples/sec; 0.562 sec/batch; 43h:16m:34s remains)
INFO - root - 2017-12-11 10:23:03.555372: step 55060, loss = 0.71, batch loss = 0.65 (14.6 examples/sec; 0.547 sec/batch; 42h:08m:27s remains)
INFO - root - 2017-12-11 10:23:09.083805: step 55070, loss = 0.68, batch loss = 0.62 (14.6 examples/sec; 0.549 sec/batch; 42h:17m:05s remains)
INFO - root - 2017-12-11 10:23:14.556461: step 55080, loss = 0.69, batch loss = 0.63 (14.6 examples/sec; 0.547 sec/batch; 42h:11m:27s remains)
INFO - root - 2017-12-11 10:23:20.052356: step 55090, loss = 0.69, batch loss = 0.63 (14.8 examples/sec; 0.542 sec/batch; 41h:47m:06s remains)
INFO - root - 2017-12-11 10:23:25.556315: step 55100, loss = 0.71, batch loss = 0.65 (14.6 examples/sec; 0.549 sec/batch; 42h:19m:15s remains)
2017-12-11 10:23:26.136387: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.068372279 -0.067940526 -0.066186681 -0.066861533 -0.068828419 -0.070604987 -0.071473822 -0.071460821 -0.071236454 -0.071182534 -0.072572358 -0.075985953 -0.081430316 -0.08750131 -0.089611165][-0.068514518 -0.07026 -0.070625894 -0.072409809 -0.073558383 -0.071995974 -0.067873761 -0.062617719 -0.05856923 -0.057537619 -0.061350722 -0.06939894 -0.079190835 -0.088827588 -0.095499732][-0.068870291 -0.071567371 -0.0723195 -0.071645267 -0.065687083 -0.052387133 -0.034270488 -0.016361568 -0.0046771728 -0.0039388794 -0.01623082 -0.037392531 -0.05997768 -0.078551821 -0.090624817][-0.06977044 -0.070541732 -0.066620231 -0.055424094 -0.030877361 0.00734917 0.051649634 0.090534747 0.11263738 0.10867002 0.077271089 0.029220965 -0.019374913 -0.057047054 -0.079937264][-0.067895085 -0.062438689 -0.046928946 -0.015190232 0.040493798 0.11636124 0.19660641 0.26105231 0.29195124 0.27529567 0.21242103 0.1241937 0.038276251 -0.026675567 -0.065313958][-0.062141985 -0.046110798 -0.012829811 0.047616273 0.14258857 0.26193067 0.37942946 0.46571016 0.49724677 0.45769215 0.35454521 0.22066943 0.095382929 0.0027411729 -0.051976725][-0.053256396 -0.024685349 0.028358294 0.11854985 0.25014183 0.40524104 0.54763055 0.64150661 0.66149467 0.59311634 0.45218614 0.282229 0.1298476 0.019987687 -0.044307169][-0.044163361 -0.0055989306 0.062387377 0.17257316 0.32408816 0.49256667 0.63572437 0.71727943 0.71563673 0.62213188 0.45969996 0.27714509 0.12140158 0.01328566 -0.047855105][-0.039294481 0.0020479204 0.073142827 0.18417403 0.32915705 0.48107228 0.59874415 0.65192437 0.62706292 0.5225991 0.36509559 0.20031705 0.068363689 -0.017428735 -0.061819118][-0.041312188 -0.0064127888 0.053723555 0.14519262 0.25907952 0.3705602 0.44659021 0.46696287 0.42692244 0.3306323 0.20329718 0.080862887 -0.0084882472 -0.059664942 -0.080314867][-0.049076762 -0.027239805 0.012442728 0.071944565 0.14264598 0.20569977 0.23957977 0.23441966 0.19202358 0.11917359 0.035669725 -0.035351809 -0.078643255 -0.095768385 -0.095229454][-0.058953054 -0.051546693 -0.033978455 -0.0071716653 0.023041654 0.045398347 0.049146295 0.031930622 -0.0015552517 -0.044919293 -0.085671671 -0.11257907 -0.12054325 -0.1143927 -0.10130268][-0.067466833 -0.07095547 -0.069968194 -0.066663295 -0.063555 -0.065359563 -0.075453833 -0.092663661 -0.1125764 -0.13081534 -0.14137378 -0.14109771 -0.1305629 -0.11462533 -0.0982591][-0.072803408 -0.081354193 -0.088161312 -0.095558569 -0.10377623 -0.11356048 -0.12479052 -0.13567217 -0.14349042 -0.1461072 -0.14190528 -0.13133669 -0.11685198 -0.10182344 -0.088932589][-0.074954614 -0.083463788 -0.090466984 -0.098404743 -0.10664281 -0.11451206 -0.12116399 -0.12544282 -0.12618011 -0.12279733 -0.11573447 -0.10617995 -0.095875591 -0.0866949 -0.079599038]]...]
INFO - root - 2017-12-11 10:23:31.601405: step 55110, loss = 0.69, batch loss = 0.63 (15.5 examples/sec; 0.515 sec/batch; 39h:39m:41s remains)
INFO - root - 2017-12-11 10:23:36.802962: step 55120, loss = 0.69, batch loss = 0.64 (15.0 examples/sec; 0.532 sec/batch; 40h:58m:35s remains)
INFO - root - 2017-12-11 10:23:42.309058: step 55130, loss = 0.70, batch loss = 0.65 (14.3 examples/sec; 0.560 sec/batch; 43h:07m:48s remains)
INFO - root - 2017-12-11 10:23:47.830661: step 55140, loss = 0.71, batch loss = 0.65 (14.5 examples/sec; 0.552 sec/batch; 42h:32m:09s remains)
INFO - root - 2017-12-11 10:23:53.326348: step 55150, loss = 0.69, batch loss = 0.63 (14.6 examples/sec; 0.548 sec/batch; 42h:13m:33s remains)
INFO - root - 2017-12-11 10:23:58.893709: step 55160, loss = 0.70, batch loss = 0.64 (14.1 examples/sec; 0.568 sec/batch; 43h:45m:00s remains)
INFO - root - 2017-12-11 10:24:04.441758: step 55170, loss = 0.69, batch loss = 0.63 (14.4 examples/sec; 0.557 sec/batch; 42h:53m:13s remains)
INFO - root - 2017-12-11 10:24:09.939778: step 55180, loss = 0.71, batch loss = 0.65 (14.6 examples/sec; 0.547 sec/batch; 42h:08m:23s remains)
INFO - root - 2017-12-11 10:24:15.447624: step 55190, loss = 0.71, batch loss = 0.65 (14.1 examples/sec; 0.566 sec/batch; 43h:37m:39s remains)
INFO - root - 2017-12-11 10:24:20.941887: step 55200, loss = 0.68, batch loss = 0.63 (14.8 examples/sec; 0.540 sec/batch; 41h:35m:18s remains)
2017-12-11 10:24:21.560479: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.13810517 0.17522663 0.18336202 0.16312823 0.12259106 0.07005474 0.022208227 -0.0060912156 -0.013575759 -0.014524609 -0.019146398 -0.026424116 -0.028689427 -0.016610812 0.011016099][0.18724921 0.23504698 0.25065589 0.23621823 0.20187111 0.15623885 0.11472772 0.090287253 0.081449464 0.0731116 0.056322493 0.03403762 0.018690214 0.023622647 0.049530812][0.20732284 0.26337293 0.29074684 0.29377034 0.28402725 0.26694778 0.24953088 0.23612194 0.22153056 0.19517882 0.15640566 0.11299752 0.081379488 0.076929525 0.098797061][0.19504379 0.25759819 0.30296221 0.33672068 0.36844882 0.39604762 0.41249764 0.40965217 0.38000244 0.32361636 0.2539615 0.18654621 0.13981715 0.12739918 0.14504197][0.17186913 0.24420512 0.31697342 0.39281654 0.47366184 0.54721934 0.59181255 0.58843666 0.53138053 0.43621096 0.33219311 0.24239808 0.18425992 0.16651787 0.17999][0.16530527 0.257625 0.3661221 0.48493886 0.60349697 0.70102608 0.74817222 0.7256152 0.63543409 0.50595659 0.37569875 0.27056289 0.20554155 0.18456282 0.19420882][0.1932961 0.31571776 0.46188551 0.61226887 0.74341559 0.83093023 0.84918624 0.790956 0.66949719 0.51795095 0.37534639 0.26597968 0.20109284 0.1803543 0.18706518][0.23584914 0.38337886 0.5522489 0.7109158 0.82874942 0.88316661 0.85937822 0.76602048 0.62525749 0.46820945 0.32826352 0.22522373 0.16693579 0.15037557 0.15709895][0.27121061 0.42429808 0.58658952 0.72261024 0.80457455 0.81761187 0.75907582 0.64773136 0.507789 0.36455679 0.24277443 0.15656751 0.11205732 0.10593525 0.11976987][0.27966565 0.41789848 0.54897392 0.64232194 0.68053567 0.65895081 0.58325124 0.47412583 0.352292 0.23612294 0.14179708 0.07777033 0.050322544 0.057034761 0.080956891][0.24727367 0.35590163 0.44384781 0.4902907 0.48976216 0.44604561 0.36892843 0.27609256 0.18279053 0.10055892 0.037741411 -0.0016421968 -0.010713082 0.0083650667 0.040217273][0.17416835 0.24599034 0.292709 0.30317217 0.27980104 0.23000652 0.1641088 0.095055848 0.033035588 -0.016094858 -0.049785107 -0.066784792 -0.060644589 -0.034524675 -0.0022028505][0.074626759 0.11049968 0.1262635 0.11769737 0.089633167 0.048605029 0.0024715429 -0.040635139 -0.074324377 -0.096693195 -0.1084253 -0.10969714 -0.095493764 -0.070080444 -0.044764012][-0.013046521 -0.0035782282 -0.0053123324 -0.019510411 -0.042080048 -0.069029592 -0.095765665 -0.11795463 -0.13148712 -0.13652661 -0.13471232 -0.12673834 -0.10964075 -0.088466555 -0.071893871][-0.067077138 -0.071026444 -0.078878991 -0.091578446 -0.10642431 -0.12141142 -0.13461921 -0.14407808 -0.14696006 -0.14399703 -0.13682643 -0.12654562 -0.11198192 -0.097633779 -0.089342646]]...]
INFO - root - 2017-12-11 10:24:26.689096: step 55210, loss = 0.70, batch loss = 0.65 (14.7 examples/sec; 0.543 sec/batch; 41h:51m:18s remains)
