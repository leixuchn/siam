INFO - Siam-FC - Running command 'main'
INFO - Siam-FC - Started run with ID "230"
INFO - root - Creating training directory: /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFCtest
INFO - root - preproces -- siamese_fc_color
WARNING - root - root is not explicitly specified, using default value: None
INFO - root - For training, we use instance size 255 - 2 * 8 ...
WARNING - root - init_method is not explicitly specified, using default value: None
INFO - root - Model moving average is disabled since decay factor is 0
111111111 Tensor("siamese_fc/conv5/concat:0", shape=(8, 6, 6, 256), dtype=float32) Tensor("siamese_fc/conv4/concat:0", shape=(8, 8, 8, 384), dtype=float32)
Tensor("siamese_fc/conv5/def/transpose:0", shape=(8, 384, 8, 8), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/db1/weights:0' shape=(256, 384, 3, 3) dtype=float32_ref> Tensor("siamese_fc/conv5/def/transpose_1:0", shape=(8, 18, 6, 6), dtype=float32)
111111111 Tensor("siamese_fc_1/conv5/concat:0", shape=(8, 20, 20, 256), dtype=float32) Tensor("siamese_fc_1/conv4/concat:0", shape=(8, 22, 22, 384), dtype=float32)
Tensor("siamese_fc_1/conv5/def/transpose:0", shape=(8, 384, 22, 22), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/db1/weights:0' shape=(256, 384, 3, 3) dtype=float32_ref> Tensor("siamese_fc_1/conv5/def/transpose_1:0", shape=(8, 18, 20, 20), dtype=float32)
Tensor("detection/add:0", shape=(8, 15, 15), dtype=float32)
2017-12-12 07:26:18.751399: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-12 07:26:18.751438: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-12 07:26:18.751445: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2017-12-12 07:26:18.751449: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-12 07:26:18.751453: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
2017-12-12 07:26:19.089448: I tensorflow/core/common_runtime/gpu/gpu_device.cc:887] Found device 0 with properties: 
name: Tesla K80
major: 3 minor: 7 memoryClockRate (GHz) 0.8235
pciBusID 5791:00:00.0
Total memory: 11.17GiB
Free memory: 11.10GiB
2017-12-12 07:26:19.089487: I tensorflow/core/common_runtime/gpu/gpu_device.cc:908] DMA: 0 
2017-12-12 07:26:19.089494: I tensorflow/core/common_runtime/gpu/gpu_device.cc:918] 0:   Y 
2017-12-12 07:26:19.089501: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla K80, pci bus id: 5791:00:00.0)
INFO:tensorflow:Restoring parameters from /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-SGD-lr0.1-decay7000-nosplit-clip50-shortcut1/model.ckpt-40000
INFO - tensorflow - Restoring parameters from /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-SGD-lr0.1-decay7000-nosplit-clip50-shortcut1/model.ckpt-40000
INFO - root - Starting threads 0 ...

INFO - root - training for 332500 steps
INFO - root - 2017-12-12 07:26:22.124576: step 0, loss = 0.64, batch loss = 0.59 (3.5 examples/sec; 2.317 sec/batch; 213h:58m:29s remains)
2017-12-12 07:26:22.464039: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.7684486 -2.7027414 -2.8191154 -3.069356 -3.3234389 -3.4376781 -3.2320607 -2.7146719 -2.1411519 -1.9086456 -2.2070353 -2.8010788 -3.3156047 -3.5960035 -3.658989][-2.7806773 -2.6659002 -2.7808478 -3.0532751 -3.2890406 -3.3053439 -2.9162569 -2.1447706 -1.3573644 -1.1000502 -1.5703053 -2.3939695 -3.0595059 -3.3976507 -3.5245841][-2.6916397 -2.5611086 -2.7159615 -2.9998267 -3.1398046 -2.9231346 -2.1962485 -1.0767574 -0.11875367 0.015535593 -0.75652909 -1.8677311 -2.7081947 -3.1037323 -3.2935946][-2.4528756 -2.3501878 -2.5797005 -2.8395085 -2.7689362 -2.1707664 -0.97758031 0.52656937 1.5514932 1.3912139 0.21370411 -1.2144074 -2.2612267 -2.7559381 -3.0249054][-2.0712519 -2.0121942 -2.3181698 -2.5145774 -2.1341782 -1.0355847 0.71502686 2.6081252 3.5928621 3.0039244 1.3490076 -0.43563843 -1.7380831 -2.3985929 -2.7730718][-1.6415112 -1.643697 -2.0198145 -2.1341825 -1.4167247 0.20623255 2.5174656 4.7166243 5.4789915 4.3159752 2.1546268 0.06469202 -1.410346 -2.1858463 -2.623312][-1.3554184 -1.437201 -1.840605 -1.8124769 -0.755049 1.3058062 3.9936881 6.2147322 6.4744596 4.6475887 2.0932879 -0.074578285 -1.4728725 -2.1949773 -2.6050467][-1.3798494 -1.5137084 -1.8700757 -1.6517317 -0.34245276 1.8806405 4.5144892 6.3259974 5.9244776 3.649508 1.0778689 -0.80983853 -1.864645 -2.3783634 -2.6892359][-1.5889287 -1.6936882 -1.9332049 -1.5562229 -0.18693924 1.8631825 4.0174422 5.1121054 4.1420794 1.8250008 -0.36648059 -1.7303531 -2.3504195 -2.6302562 -2.8383183][-1.7858994 -1.7412543 -1.8017876 -1.3458176 -0.12137222 1.5256124 2.9965887 3.3419433 2.053422 0.020123959 -1.6001451 -2.4247994 -2.712419 -2.8513038 -3.0119386][-1.9984186 -1.7819107 -1.6692936 -1.223963 -0.28400111 0.87905073 1.6974645 1.4988523 0.21322632 -1.324672 -2.3578005 -2.7734189 -2.9023285 -3.0135961 -3.1809072][-2.3005719 -2.0354688 -1.8585374 -1.5005095 -0.87798262 -0.15608501 0.18933344 -0.21853542 -1.1824617 -2.1188197 -2.6664941 -2.8550744 -2.9654169 -3.116009 -3.3140275][-2.6629686 -2.4720342 -2.3284805 -2.0830507 -1.7382479 -1.3805182 -1.3152928 -1.6558118 -2.1163077 -2.4634905 -2.6747565 -2.7905192 -2.9599972 -3.1700385 -3.3981016][-2.9018481 -2.8636022 -2.8392096 -2.7505291 -2.6638336 -2.5714035 -2.5796263 -2.6576333 -2.5657988 -2.4188557 -2.4531703 -2.6294804 -2.920908 -3.2015915 -3.4439318][-2.899241 -3.0003088 -3.1335778 -3.2401433 -3.3781064 -3.4120786 -3.2932615 -2.993464 -2.42036 -1.9553596 -1.9981107 -2.3735149 -2.8496161 -3.2073338 -3.444706]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFCtest/model.ckpt-0 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFCtest/model.ckpt-0 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-12 07:26:25.062146: step 10, loss = 0.44, batch loss = 0.38 (37.5 examples/sec; 0.213 sec/batch; 19h:42m:57s remains)
INFO - root - 2017-12-12 07:26:27.209692: step 20, loss = 0.34, batch loss = 0.28 (37.4 examples/sec; 0.214 sec/batch; 19h:45m:30s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFCtest
INFO - root - 2017-12-12 07:26:29.361506: step 30, loss = 0.36, batch loss = 0.31 (37.1 examples/sec; 0.216 sec/batch; 19h:55m:43s remains)
INFO - root - 2017-12-12 07:26:31.525188: step 40, loss = 0.46, batch loss = 0.40 (36.4 examples/sec; 0.220 sec/batch; 20h:17m:33s remains)
INFO - root - 2017-12-12 07:26:33.659374: step 50, loss = 0.42, batch loss = 0.36 (37.2 examples/sec; 0.215 sec/batch; 19h:50m:51s remains)
INFO - root - 2017-12-12 07:26:35.792556: step 60, loss = 0.45, batch loss = 0.39 (38.3 examples/sec; 0.209 sec/batch; 19h:18m:05s remains)
INFO - root - 2017-12-12 07:26:37.935428: step 70, loss = 0.27, batch loss = 0.21 (38.3 examples/sec; 0.209 sec/batch; 19h:18m:35s remains)
INFO - root - 2017-12-12 07:26:40.093518: step 80, loss = 0.35, batch loss = 0.29 (37.0 examples/sec; 0.216 sec/batch; 19h:56m:48s remains)
INFO - root - 2017-12-12 07:26:42.245374: step 90, loss = 0.32, batch loss = 0.26 (37.7 examples/sec; 0.212 sec/batch; 19h:34m:08s remains)
INFO - root - 2017-12-12 07:26:44.388113: step 100, loss = 0.41, batch loss = 0.35 (37.2 examples/sec; 0.215 sec/batch; 19h:50m:37s remains)
2017-12-12 07:26:44.685768: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.7597978 -3.9714794 -3.9541938 -3.7903256 -3.4790676 -2.9253526 -2.0796556 -1.0552256 -0.16204309 -0.01565814 -1.0802388 -2.6673217 -3.7609842 -3.7631631 -3.2946217][-4.7389746 -4.8260822 -4.5276322 -4.0534372 -3.6109703 -3.2560129 -2.8801255 -2.2843344 -1.6145475 -1.4621906 -2.2886105 -3.5803003 -4.5016193 -4.5617681 -4.1217456][-5.4412436 -5.2927542 -4.5977621 -3.7401123 -3.16756 -3.0849738 -3.2870266 -3.2554994 -2.9481921 -2.8751295 -3.4465957 -4.3753505 -5.0579948 -5.1469512 -4.8014627][-5.5012493 -5.0829329 -3.9660969 -2.6911612 -1.924583 -2.000802 -2.6770496 -3.2310448 -3.415946 -3.5951962 -4.0869207 -4.7786674 -5.277298 -5.4006138 -5.2155318][-4.916966 -4.2276411 -2.7231772 -1.0360489 0.038514137 0.02733922 -0.94657469 -2.0881658 -2.8986921 -3.5022502 -4.1194758 -4.7629924 -5.2188921 -5.4452562 -5.4811573][-3.8892095 -3.0059831 -1.2466466 0.76653743 2.1801093 2.3570163 1.1998341 -0.49574161 -1.9581977 -3.0352173 -3.8509114 -4.512877 -4.9841228 -5.3317561 -5.5853066][-2.9176266 -2.00735 -0.20495605 1.9718869 3.6228378 3.9579451 2.699126 0.62323213 -1.3181152 -2.7650447 -3.7276533 -4.3486166 -4.7532396 -5.1135507 -5.4582939][-2.770503 -2.0240145 -0.4089067 1.7026842 3.392493 3.7872756 2.5608604 0.45239043 -1.5802867 -3.0963638 -4.0117345 -4.4543419 -4.6627278 -4.8746157 -5.1257348][-3.7567649 -3.3296201 -2.1015882 -0.31187892 1.1907079 1.5899112 0.59790921 -1.1555877 -2.858556 -4.1023488 -4.7749424 -4.9545841 -4.904634 -4.8557553 -4.8681655][-5.1308422 -5.0110588 -4.20269 -2.8786578 -1.7227545 -1.3589704 -2.0211306 -3.2655668 -4.4646587 -5.2783685 -5.6331129 -5.5724931 -5.3065505 -4.9932547 -4.732233][-5.8502522 -5.9137425 -5.4281836 -4.5534825 -3.7717278 -3.4634261 -3.8196032 -4.5950837 -5.3254766 -5.7456331 -5.85606 -5.67861 -5.3076138 -4.8059006 -4.3204155][-5.7083769 -5.8052187 -5.4900408 -4.925849 -4.4181371 -4.142344 -4.2594357 -4.6801853 -5.07726 -5.2710276 -5.3056149 -5.1613431 -4.7946496 -4.1977806 -3.547488][-5.3176155 -5.3729954 -5.135735 -4.7504468 -4.4003158 -4.1464024 -4.1164818 -4.3000717 -4.4917736 -4.6138597 -4.6929808 -4.6342077 -4.3068814 -3.673316 -2.8971975][-5.3524532 -5.3864822 -5.2435255 -5.0194907 -4.8008003 -4.5979614 -4.5013428 -4.5238824 -4.5633788 -4.6271949 -4.6972666 -4.6277351 -4.30606 -3.6714985 -2.8139913][-5.8252773 -5.8754625 -5.8461809 -5.7777395 -5.692771 -5.5800972 -5.484684 -5.4067435 -5.3093581 -5.2641392 -5.2296181 -5.0628982 -4.7401352 -4.1531496 -3.2871578]]...]
INFO - root - 2017-12-12 07:26:46.851297: step 110, loss = 0.40, batch loss = 0.34 (37.0 examples/sec; 0.216 sec/batch; 19h:56m:46s remains)
INFO - root - 2017-12-12 07:26:49.000542: step 120, loss = 0.62, batch loss = 0.56 (37.2 examples/sec; 0.215 sec/batch; 19h:52m:19s remains)
INFO - root - 2017-12-12 07:26:51.179433: step 130, loss = 0.44, batch loss = 0.39 (35.8 examples/sec; 0.223 sec/batch; 20h:37m:35s remains)
INFO - root - 2017-12-12 07:26:53.376766: step 140, loss = 0.29, batch loss = 0.23 (35.9 examples/sec; 0.223 sec/batch; 20h:34m:33s remains)
INFO - root - 2017-12-12 07:26:55.555347: step 150, loss = 0.72, batch loss = 0.66 (36.7 examples/sec; 0.218 sec/batch; 20h:08m:19s remains)
INFO - root - 2017-12-12 07:26:57.694535: step 160, loss = 0.42, batch loss = 0.36 (37.0 examples/sec; 0.216 sec/batch; 19h:56m:30s remains)
INFO - root - 2017-12-12 07:26:59.840421: step 170, loss = 0.37, batch loss = 0.31 (36.5 examples/sec; 0.219 sec/batch; 20h:12m:49s remains)
INFO - root - 2017-12-12 07:27:01.997207: step 180, loss = 0.36, batch loss = 0.30 (36.2 examples/sec; 0.221 sec/batch; 20h:22m:31s remains)
INFO - root - 2017-12-12 07:27:04.177016: step 190, loss = 0.42, batch loss = 0.36 (35.6 examples/sec; 0.225 sec/batch; 20h:46m:16s remains)
INFO - root - 2017-12-12 07:27:06.341222: step 200, loss = 0.32, batch loss = 0.26 (37.0 examples/sec; 0.216 sec/batch; 19h:56m:58s remains)
2017-12-12 07:27:06.671362: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.6866832 -3.7311611 -3.7142792 -3.6339135 -3.5005891 -3.3573244 -3.2552748 -3.2415178 -3.3181102 -3.4417939 -3.5622048 -3.6436667 -3.6703064 -3.6382527 -3.5831342][-3.6483731 -3.6720624 -3.6194513 -3.4646144 -3.2494626 -3.0662732 -2.9926248 -3.0612249 -3.2424352 -3.4549408 -3.6204643 -3.6924791 -3.6559019 -3.5257652 -3.3811326][-3.3982222 -3.4066308 -3.3580313 -3.2014906 -3.0031972 -2.8855591 -2.9106932 -3.0726566 -3.3487179 -3.6463149 -3.8467255 -3.853307 -3.6574945 -3.3355756 -3.0558946][-3.0700755 -3.092134 -3.0810239 -2.9640212 -2.7884467 -2.6706359 -2.669569 -2.80169 -3.1306126 -3.5749934 -3.9198356 -3.9416344 -3.6119483 -3.1114964 -2.7190251][-2.870013 -2.9136691 -2.9274395 -2.8103142 -2.5307112 -2.1792245 -1.8900974 -1.8327038 -2.2023282 -2.8933768 -3.5433495 -3.7633932 -3.4792547 -2.9609878 -2.5668836][-2.8908381 -2.9035695 -2.8737831 -2.6902103 -2.1922638 -1.4177992 -0.66824961 -0.37752557 -0.84389997 -1.8400075 -2.8394871 -3.3516531 -3.2791882 -2.9216123 -2.6555018][-3.068202 -2.9865613 -2.84603 -2.5758367 -1.883842 -0.7311399 0.38626528 0.76407766 0.10251641 -1.1688814 -2.3969531 -3.1122158 -3.2433939 -3.0608525 -2.9237797][-3.2216911 -3.0775533 -2.8654411 -2.5992517 -1.9265233 -0.7237227 0.42081642 0.67759991 -0.15777898 -1.4905071 -2.651546 -3.3104064 -3.451721 -3.2962558 -3.1650662][-3.2177882 -3.138386 -2.9633775 -2.7972555 -2.3347082 -1.402195 -0.51406574 -0.41687274 -1.2086163 -2.2840023 -3.0970485 -3.5025086 -3.5232337 -3.3081605 -3.1212966][-3.1102419 -3.2092419 -3.1607594 -3.1068594 -2.8534286 -2.2326586 -1.5834589 -1.4640534 -1.9540548 -2.5988708 -3.0422862 -3.2769563 -3.3042748 -3.1678841 -3.0176926][-3.0619593 -3.3314679 -3.3908165 -3.3629289 -3.1717994 -2.6922741 -2.0950394 -1.7601987 -1.8284372 -2.0731919 -2.3234515 -2.6167548 -2.8822923 -3.0274756 -3.0585992][-3.1772304 -3.5056841 -3.5803721 -3.4959509 -3.2518032 -2.7567976 -2.082401 -1.5157664 -1.2750456 -1.3074455 -1.5508666 -2.0301745 -2.6098425 -3.075335 -3.3055012][-3.3257303 -3.628598 -3.667938 -3.528456 -3.2385476 -2.7117434 -1.9883717 -1.3517535 -1.0690968 -1.1344621 -1.4654088 -2.0441346 -2.7550983 -3.3735402 -3.6919551][-3.28849 -3.5650945 -3.5950203 -3.4693868 -3.2050197 -2.7292888 -2.0941832 -1.5978949 -1.4905798 -1.6905078 -2.0082221 -2.4273088 -2.9720361 -3.4942725 -3.7726076][-3.2043631 -3.4874649 -3.5549853 -3.5000179 -3.3303475 -3.0171647 -2.5888214 -2.3016992 -2.3442523 -2.5333085 -2.6179771 -2.6657856 -2.8919621 -3.199162 -3.3679004]]...]
INFO - root - 2017-12-12 07:27:08.854248: step 210, loss = 0.31, batch loss = 0.25 (36.2 examples/sec; 0.221 sec/batch; 20h:25m:12s remains)
INFO - root - 2017-12-12 07:27:11.047685: step 220, loss = 0.36, batch loss = 0.30 (37.6 examples/sec; 0.213 sec/batch; 19h:38m:30s remains)
INFO - root - 2017-12-12 07:27:13.229864: step 230, loss = 0.27, batch loss = 0.21 (37.0 examples/sec; 0.216 sec/batch; 19h:57m:15s remains)
INFO - root - 2017-12-12 07:27:15.396205: step 240, loss = 0.28, batch loss = 0.22 (37.5 examples/sec; 0.213 sec/batch; 19h:41m:25s remains)
INFO - root - 2017-12-12 07:27:17.574532: step 250, loss = 0.35, batch loss = 0.29 (37.0 examples/sec; 0.216 sec/batch; 19h:56m:12s remains)
INFO - root - 2017-12-12 07:27:19.738880: step 260, loss = 0.34, batch loss = 0.28 (37.7 examples/sec; 0.212 sec/batch; 19h:34m:34s remains)
INFO - root - 2017-12-12 07:27:21.909163: step 270, loss = 0.49, batch loss = 0.43 (35.3 examples/sec; 0.226 sec/batch; 20h:53m:27s remains)
INFO - root - 2017-12-12 07:27:24.078415: step 280, loss = 0.31, batch loss = 0.25 (37.1 examples/sec; 0.216 sec/batch; 19h:53m:36s remains)
INFO - root - 2017-12-12 07:27:26.284745: step 290, loss = 0.30, batch loss = 0.25 (36.5 examples/sec; 0.219 sec/batch; 20h:15m:10s remains)
INFO - root - 2017-12-12 07:27:28.476382: step 300, loss = 0.45, batch loss = 0.39 (36.8 examples/sec; 0.218 sec/batch; 20h:05m:15s remains)
2017-12-12 07:27:28.752560: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.7360163 -2.6791575 -2.5650892 -2.3142347 -2.0520873 -1.9161288 -1.7853184 -1.7115653 -1.815102 -1.9939561 -2.1195326 -1.9928194 -1.6348168 -1.3278558 -1.2953675][-1.7001427 -1.9780059 -2.3749652 -2.5928311 -2.6089516 -2.5466213 -2.4363387 -2.3814611 -2.4121866 -2.4050853 -2.2943358 -2.01776 -1.6596528 -1.4789493 -1.6173217][-0.40789795 -0.87878275 -1.6460446 -2.2932622 -2.6401587 -2.735949 -2.7001481 -2.6528904 -2.5619838 -2.3360162 -2.0318367 -1.6978074 -1.4020948 -1.3269675 -1.5911711][0.31730485 -0.1199379 -0.93644762 -1.7476912 -2.2678688 -2.4363499 -2.3868546 -2.2501295 -1.968048 -1.5067596 -1.0645797 -0.76939654 -0.64350867 -0.78925991 -1.2336102][-0.058730364 -0.30042672 -0.83577871 -1.4193738 -1.8100679 -1.9050694 -1.7509069 -1.4168653 -0.86312938 -0.15228796 0.38614678 0.56705809 0.40756369 -0.12688875 -0.904166][-1.4587007 -1.5255277 -1.6832224 -1.8017361 -1.8158226 -1.6928473 -1.3677721 -0.76673245 0.12949157 1.1070514 1.7456112 1.8259168 1.35215 0.36715174 -0.81820083][-3.05334 -3.0354385 -2.9507463 -2.733681 -2.4484713 -2.1790352 -1.8026775 -1.0765715 0.051535368 1.2556796 2.0789852 2.2675238 1.743166 0.54744339 -0.86337304][-4.1548781 -4.0610695 -3.8598235 -3.5415556 -3.1773906 -2.9326251 -2.6985607 -2.104104 -1.0060976 0.29160786 1.330092 1.8090591 1.5192485 0.44541669 -0.88206148][-4.9431806 -4.7697515 -4.4679708 -4.109551 -3.7456388 -3.5921664 -3.5542836 -3.1883769 -2.2725372 -1.0159061 0.13562202 0.83785534 0.82818985 0.073418379 -0.89684105][-5.3482513 -5.19503 -4.866756 -4.4967666 -4.165895 -4.1468949 -4.3185391 -4.178771 -3.4593635 -2.3113682 -1.1784604 -0.4274044 -0.24743056 -0.59329939 -1.00055][-4.9921875 -5.0121269 -4.818172 -4.5258636 -4.297904 -4.4517436 -4.846951 -4.933569 -4.3927503 -3.3762348 -2.3747578 -1.7238841 -1.4613078 -1.4092174 -1.212055][-4.1383104 -4.4203773 -4.4614258 -4.3179207 -4.2350416 -4.5301695 -5.0732536 -5.3266206 -4.930656 -4.061739 -3.2489648 -2.7414446 -2.4327288 -2.0781841 -1.4029298][-3.4343903 -3.9133015 -4.1259217 -4.0971742 -4.1002593 -4.4367127 -5.0237937 -5.3837347 -5.1452537 -4.4690824 -3.858314 -3.4251523 -3.0104966 -2.4463329 -1.5054958][-3.3940477 -3.8786454 -4.0217195 -3.8971155 -3.7917809 -4.0352 -4.566009 -4.9771709 -4.9235711 -4.5284114 -4.2012177 -3.8542907 -3.3414609 -2.6747532 -1.6861621][-3.8982058 -4.2432976 -4.163301 -3.8095655 -3.4718518 -3.51936 -3.9062488 -4.2931628 -4.3970881 -4.3164454 -4.3112936 -4.1231503 -3.5964584 -2.9372082 -2.0669193]]...]
INFO - root - 2017-12-12 07:27:30.936840: step 310, loss = 0.30, batch loss = 0.24 (37.7 examples/sec; 0.212 sec/batch; 19h:35m:32s remains)
INFO - root - 2017-12-12 07:27:33.107991: step 320, loss = 0.28, batch loss = 0.22 (36.3 examples/sec; 0.220 sec/batch; 20h:19m:47s remains)
INFO - root - 2017-12-12 07:27:35.287225: step 330, loss = 0.30, batch loss = 0.24 (36.1 examples/sec; 0.221 sec/batch; 20h:25m:52s remains)
INFO - root - 2017-12-12 07:27:37.507645: step 340, loss = 0.33, batch loss = 0.27 (36.6 examples/sec; 0.219 sec/batch; 20h:09m:45s remains)
INFO - root - 2017-12-12 07:27:39.692959: step 350, loss = 0.31, batch loss = 0.25 (36.6 examples/sec; 0.218 sec/batch; 20h:09m:31s remains)
INFO - root - 2017-12-12 07:27:41.891670: step 360, loss = 0.44, batch loss = 0.38 (36.2 examples/sec; 0.221 sec/batch; 20h:24m:23s remains)
INFO - root - 2017-12-12 07:27:44.082498: step 370, loss = 0.56, batch loss = 0.50 (35.8 examples/sec; 0.224 sec/batch; 20h:37m:47s remains)
INFO - root - 2017-12-12 07:27:46.256871: step 380, loss = 0.26, batch loss = 0.20 (35.1 examples/sec; 0.228 sec/batch; 21h:03m:12s remains)
INFO - root - 2017-12-12 07:27:48.423983: step 390, loss = 0.30, batch loss = 0.24 (36.6 examples/sec; 0.218 sec/batch; 20h:08m:14s remains)
INFO - root - 2017-12-12 07:27:50.609607: step 400, loss = 0.26, batch loss = 0.20 (35.7 examples/sec; 0.224 sec/batch; 20h:39m:52s remains)
2017-12-12 07:27:50.925743: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.6029716 -4.9053693 -5.0284252 -5.141211 -5.28801 -5.5704832 -5.5446253 -5.0985489 -4.9459119 -5.164196 -5.4073839 -5.4229383 -5.4771619 -5.7256188 -5.5345354][-4.9798303 -5.2866096 -5.3022828 -5.35289 -5.5464749 -5.825243 -5.6408372 -4.9656529 -4.6512671 -4.7092791 -4.6789093 -4.3864393 -4.3647089 -4.9058657 -5.1280222][-4.7381725 -4.9502568 -4.8059187 -4.7180643 -4.8757029 -5.1049938 -4.7968988 -3.9524853 -3.4851067 -3.4072978 -3.1717317 -2.6731005 -2.6501198 -3.4911442 -4.130836][-3.8953712 -3.8950713 -3.5461519 -3.28708 -3.4208097 -3.667608 -3.3861451 -2.4660063 -1.8126905 -1.5753384 -1.1642463 -0.5576129 -0.62361693 -1.8159326 -2.8879075][-2.9551225 -2.5802698 -1.8386263 -1.2002845 -1.1305187 -1.4009488 -1.3277991 -0.51077414 0.25366497 0.61522174 1.1349247 1.7523797 1.5185955 -0.069823265 -1.6181138][-2.4603894 -1.7820876 -0.62777662 0.49822783 0.97019362 0.79857135 0.64708924 1.2219861 1.9158704 2.2506359 2.7306163 3.254194 2.8234618 0.9606564 -0.90864325][-2.5872676 -1.8607926 -0.51993823 0.92021537 1.7316611 1.6989353 1.4664323 1.8889534 2.4349091 2.5744865 2.823375 3.1255648 2.5371287 0.64165711 -1.2301629][-2.6768947 -2.0275421 -0.782151 0.55984807 1.282789 1.1471479 0.88485122 1.3236082 1.839246 1.837074 1.8369148 1.8435528 1.0737274 -0.71784353 -2.3560009][-2.0554702 -1.3802524 -0.33211732 0.586632 0.75163817 0.1116178 -0.34769392 0.13882256 0.73555255 0.72766757 0.58797479 0.35257721 -0.56922317 -2.2156768 -3.5597723][-1.1897426 -0.40166998 0.45910287 0.87777638 0.31581473 -0.98480368 -1.767315 -1.3240135 -0.67947149 -0.63910961 -0.8652761 -1.2276266 -2.1490955 -3.51905 -4.4743071][-0.82972193 -0.016789675 0.64465785 0.62528586 -0.52610278 -2.3087494 -3.3354831 -3.0003295 -2.3759167 -2.2822249 -2.5684173 -3.0111923 -3.7751017 -4.703177 -5.1431351][-1.313086 -0.59307384 -0.027581453 -0.18368101 -1.4546151 -3.281621 -4.3356671 -4.1243711 -3.6135664 -3.5269005 -3.8331275 -4.2825212 -4.8398476 -5.3060904 -5.2974849][-2.2365861 -1.6149143 -0.98712182 -0.935189 -1.8575178 -3.3029289 -4.1879539 -4.1360178 -3.8638833 -3.8762805 -4.1665282 -4.5312548 -4.8751564 -5.0275512 -4.7963557][-2.8731549 -2.30124 -1.5973101 -1.2950187 -1.7367475 -2.6382813 -3.2764931 -3.3578811 -3.3304517 -3.457963 -3.7100482 -3.9533064 -4.1130548 -4.1052861 -3.8651519][-2.9812803 -2.4645569 -1.7971101 -1.4069142 -1.5212587 -1.9567865 -2.3044262 -2.3918853 -2.4667926 -2.6364846 -2.8470101 -3.0132701 -3.0803955 -3.0286212 -2.8755641]]...]
INFO - root - 2017-12-12 07:27:53.114961: step 410, loss = 0.32, batch loss = 0.27 (37.0 examples/sec; 0.216 sec/batch; 19h:55m:13s remains)
INFO - root - 2017-12-12 07:27:55.313387: step 420, loss = 0.50, batch loss = 0.44 (37.5 examples/sec; 0.214 sec/batch; 19h:42m:10s remains)
INFO - root - 2017-12-12 07:27:57.520696: step 430, loss = 0.36, batch loss = 0.30 (36.3 examples/sec; 0.221 sec/batch; 20h:21m:07s remains)
INFO - root - 2017-12-12 07:27:59.741053: step 440, loss = 0.27, batch loss = 0.21 (35.8 examples/sec; 0.224 sec/batch; 20h:38m:10s remains)
INFO - root - 2017-12-12 07:28:01.974408: step 450, loss = 0.34, batch loss = 0.28 (35.0 examples/sec; 0.228 sec/batch; 21h:03m:42s remains)
INFO - root - 2017-12-12 07:28:04.217411: step 460, loss = 0.28, batch loss = 0.22 (36.0 examples/sec; 0.222 sec/batch; 20h:29m:21s remains)
INFO - root - 2017-12-12 07:28:06.410531: step 470, loss = 0.25, batch loss = 0.20 (36.4 examples/sec; 0.220 sec/batch; 20h:16m:08s remains)
INFO - root - 2017-12-12 07:28:08.647900: step 480, loss = 0.28, batch loss = 0.22 (37.0 examples/sec; 0.216 sec/batch; 19h:56m:20s remains)
INFO - root - 2017-12-12 07:28:10.841955: step 490, loss = 0.34, batch loss = 0.28 (36.3 examples/sec; 0.221 sec/batch; 20h:20m:39s remains)
INFO - root - 2017-12-12 07:28:13.030280: step 500, loss = 0.33, batch loss = 0.27 (36.7 examples/sec; 0.218 sec/batch; 20h:04m:54s remains)
2017-12-12 07:28:13.318584: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.6511812 -2.8770514 -3.1842954 -3.3240237 -3.2299862 -3.2183163 -3.4339271 -3.7048144 -3.9571795 -3.8856215 -3.5993392 -3.1954236 -2.8152928 -2.7207017 -2.8216894][-2.2864406 -2.5964036 -2.9762583 -3.0876338 -2.9413247 -2.9131761 -3.1654403 -3.5101144 -3.7769284 -3.5293953 -2.8954425 -2.1832318 -1.660193 -1.6367264 -1.9637538][-2.6786675 -2.9112756 -3.1391144 -3.0961311 -2.9060473 -2.9431458 -3.2394018 -3.539643 -3.5863926 -2.9642274 -1.8735296 -0.84875917 -0.2742784 -0.46666527 -1.2602961][-3.1863551 -3.113425 -3.021657 -2.713397 -2.4692259 -2.5873394 -2.9030666 -3.0977974 -2.8251858 -1.7677662 -0.2002275 1.0802863 1.5528371 0.93557954 -0.51453972][-3.3004575 -2.8432369 -2.4209354 -1.7674901 -1.3183644 -1.2895668 -1.4229043 -1.4597948 -0.9995358 0.23775744 1.9490082 3.1267946 3.1847718 2.0174634 -0.03271842][-2.9406726 -2.153841 -1.5096339 -0.64139867 0.064612627 0.51391196 0.8694098 1.1106274 1.523957 2.3973773 3.6400201 4.2464724 3.7358139 2.1961529 -0.021344423][-2.2856369 -1.2728558 -0.5878849 0.23812199 1.0309994 1.9101431 2.8184626 3.2904341 3.3680112 3.4707468 3.8649075 3.7580845 2.8591273 1.3555882 -0.49968982][-1.6063008 -0.50622535 0.049217939 0.54648471 1.1124094 2.0879858 3.187938 3.5990932 3.254535 2.7547433 2.527374 1.9834836 1.0699828 -0.072635174 -1.3162618][-1.1718981 -0.1249373 0.17552376 0.14647484 0.22835922 0.883899 1.682441 1.8240011 1.3283603 0.77461648 0.42233443 -0.20477271 -0.94448328 -1.6940804 -2.37052][-0.85510087 -0.01671648 -0.022093534 -0.56643867 -1.019726 -0.88708925 -0.57187486 -0.63762617 -0.98327518 -1.2367802 -1.4387228 -1.9870321 -2.5432673 -3.0060744 -3.2816241][-0.83632445 -0.34256721 -0.58848953 -1.3674269 -2.080524 -2.3131456 -2.3363388 -2.4402237 -2.5052631 -2.4010863 -2.4098089 -2.8223755 -3.2194977 -3.4811702 -3.572233][-1.4242826 -1.2950714 -1.5975165 -2.228807 -2.7986588 -3.0918784 -3.203721 -3.199192 -3.0096302 -2.6819534 -2.5759344 -2.85043 -3.1404009 -3.3387752 -3.4596188][-2.3646853 -2.4389656 -2.5983336 -2.841711 -3.0579321 -3.2580638 -3.3516476 -3.2424543 -2.9330518 -2.5561731 -2.4244373 -2.5469482 -2.743186 -2.9494357 -3.1712747][-2.9983838 -3.0473328 -3.0210376 -2.9120228 -2.8486612 -2.984129 -3.0678844 -2.9288836 -2.6460662 -2.3562589 -2.2628884 -2.262022 -2.3583834 -2.5373523 -2.7430122][-2.9697099 -2.8999863 -2.7969241 -2.5952497 -2.5034266 -2.687006 -2.8057287 -2.7103806 -2.5538795 -2.3858671 -2.2880864 -2.1348565 -2.084424 -2.1458783 -2.2029881]]...]
INFO - root - 2017-12-12 07:28:15.527724: step 510, loss = 0.37, batch loss = 0.32 (33.6 examples/sec; 0.238 sec/batch; 21h:56m:23s remains)
INFO - root - 2017-12-12 07:28:17.746608: step 520, loss = 0.26, batch loss = 0.20 (37.1 examples/sec; 0.215 sec/batch; 19h:52m:17s remains)
INFO - root - 2017-12-12 07:28:19.979436: step 530, loss = 0.29, batch loss = 0.23 (36.3 examples/sec; 0.220 sec/batch; 20h:18m:39s remains)
INFO - root - 2017-12-12 07:28:22.209681: step 540, loss = 0.26, batch loss = 0.20 (35.4 examples/sec; 0.226 sec/batch; 20h:50m:02s remains)
INFO - root - 2017-12-12 07:28:24.430603: step 550, loss = 0.32, batch loss = 0.26 (35.6 examples/sec; 0.225 sec/batch; 20h:44m:22s remains)
INFO - root - 2017-12-12 07:28:26.697212: step 560, loss = 0.36, batch loss = 0.30 (37.0 examples/sec; 0.216 sec/batch; 19h:57m:23s remains)
INFO - root - 2017-12-12 07:28:28.907439: step 570, loss = 0.32, batch loss = 0.26 (35.3 examples/sec; 0.227 sec/batch; 20h:55m:14s remains)
INFO - root - 2017-12-12 07:28:31.101782: step 580, loss = 0.27, batch loss = 0.21 (36.0 examples/sec; 0.222 sec/batch; 20h:30m:14s remains)
INFO - root - 2017-12-12 07:28:33.318813: step 590, loss = 0.42, batch loss = 0.37 (36.4 examples/sec; 0.220 sec/batch; 20h:17m:17s remains)
INFO - root - 2017-12-12 07:28:35.508915: step 600, loss = 0.25, batch loss = 0.19 (37.5 examples/sec; 0.213 sec/batch; 19h:39m:30s remains)
2017-12-12 07:28:35.787025: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.65934 -3.7585552 -3.7536609 -3.5372584 -3.2147768 -2.9544079 -2.8731823 -2.9524877 -3.0987141 -3.2919044 -3.510474 -3.669384 -3.7984135 -3.7832491 -3.6845222][-3.9282746 -4.0587673 -4.0291333 -3.7935605 -3.3744812 -2.9335186 -2.7083693 -2.7841065 -3.0359821 -3.3862028 -3.7560575 -4.0190592 -4.2263527 -4.2308464 -4.1092682][-4.314992 -4.5313144 -4.5094047 -4.262352 -3.7009702 -2.9628692 -2.461926 -2.4678149 -2.8963652 -3.5106161 -4.0734067 -4.4736328 -4.7791591 -4.81515 -4.6452932][-4.6613979 -5.0633683 -5.1206913 -4.758563 -3.8603725 -2.6161466 -1.6565509 -1.5262458 -2.2461057 -3.3314853 -4.2761 -4.9274883 -5.38439 -5.4532509 -5.1840925][-4.8351226 -5.4572911 -5.6260915 -5.0607471 -3.6597276 -1.6536452 0.057031393 0.53401828 -0.47331309 -2.2186995 -3.809128 -4.9501395 -5.6989121 -5.9010506 -5.5942163][-4.8113236 -5.6372366 -5.9528947 -5.2234898 -3.3051493 -0.38774824 2.3572052 3.467581 2.3470528 -0.11135817 -2.5268583 -4.3222003 -5.4903545 -5.9692268 -5.7746773][-4.5753961 -5.498301 -5.9775448 -5.2925014 -3.1517532 0.45117331 4.2105026 6.0850792 5.170682 2.253897 -0.87642264 -3.2626667 -4.8440456 -5.6293945 -5.6175919][-4.3351903 -5.3072886 -5.9821386 -5.5825348 -3.6786137 0.061370611 4.3673677 6.7970495 6.28741 3.4081028 0.06930995 -2.5362685 -4.2971029 -5.2444816 -5.3369341][-4.2525949 -5.3016682 -6.2612171 -6.3617773 -5.0706282 -1.7969513 2.3570521 4.9032068 4.7835884 2.3126175 -0.70503783 -3.0523553 -4.6070557 -5.3886185 -5.3671637][-4.065321 -5.1455383 -6.3421822 -6.9558678 -6.4578362 -4.1583381 -0.87047482 1.2919018 1.4411275 -0.37792325 -2.723763 -4.5468693 -5.6457195 -5.9980774 -5.6503839][-3.6753616 -4.6391191 -5.8409748 -6.6986275 -6.8161478 -5.5888109 -3.5158818 -2.0906553 -1.9100217 -3.0784721 -4.64229 -5.8413534 -6.4325814 -6.3107328 -5.6330924][-3.1906247 -3.9330373 -4.9309096 -5.73374 -6.0925021 -5.627284 -4.6116319 -3.9250834 -3.8892076 -4.6250105 -5.5599222 -6.2059584 -6.3682079 -5.9177589 -5.0896363][-2.7573841 -3.259254 -4.0120845 -4.6878262 -5.0985527 -5.0374813 -4.6728969 -4.4600582 -4.5517759 -5.031229 -5.5659485 -5.81866 -5.6827106 -5.1096234 -4.3279405][-2.4115815 -2.6833751 -3.1922686 -3.7170532 -4.1231341 -4.2614789 -4.2214127 -4.2392197 -4.3876209 -4.6960392 -4.9666138 -4.9886436 -4.7309065 -4.2163463 -3.6123221][-2.1464496 -2.2199533 -2.4872484 -2.8072243 -3.1004777 -3.2643714 -3.3394527 -3.4422755 -3.60358 -3.8142953 -3.9586029 -3.9291577 -3.7240191 -3.3873894 -3.0056064]]...]
INFO - root - 2017-12-12 07:28:37.981334: step 610, loss = 0.32, batch loss = 0.26 (34.8 examples/sec; 0.230 sec/batch; 21h:11m:22s remains)
INFO - root - 2017-12-12 07:28:40.186262: step 620, loss = 0.19, batch loss = 0.13 (35.9 examples/sec; 0.223 sec/batch; 20h:33m:31s remains)
INFO - root - 2017-12-12 07:28:42.380680: step 630, loss = 0.28, batch loss = 0.23 (35.8 examples/sec; 0.223 sec/batch; 20h:35m:42s remains)
INFO - root - 2017-12-12 07:28:44.621349: step 640, loss = 0.29, batch loss = 0.23 (33.6 examples/sec; 0.238 sec/batch; 21h:56m:35s remains)
INFO - root - 2017-12-12 07:28:46.840080: step 650, loss = 0.35, batch loss = 0.29 (34.1 examples/sec; 0.234 sec/batch; 21h:36m:08s remains)
INFO - root - 2017-12-12 07:28:49.068982: step 660, loss = 0.28, batch loss = 0.22 (35.0 examples/sec; 0.229 sec/batch; 21h:04m:51s remains)
INFO - root - 2017-12-12 07:28:51.282014: step 670, loss = 0.41, batch loss = 0.35 (35.5 examples/sec; 0.226 sec/batch; 20h:47m:58s remains)
INFO - root - 2017-12-12 07:28:53.506887: step 680, loss = 0.34, batch loss = 0.28 (36.6 examples/sec; 0.219 sec/batch; 20h:08m:58s remains)
INFO - root - 2017-12-12 07:28:55.710448: step 690, loss = 0.34, batch loss = 0.28 (37.4 examples/sec; 0.214 sec/batch; 19h:44m:09s remains)
INFO - root - 2017-12-12 07:28:57.924320: step 700, loss = 0.29, batch loss = 0.23 (34.5 examples/sec; 0.232 sec/batch; 21h:20m:58s remains)
2017-12-12 07:28:58.205010: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.9184375 -3.7244282 -3.4567788 -3.1705883 -2.956727 -2.8085356 -2.6022797 -2.3393397 -2.0866938 -1.8772879 -1.6988934 -1.5517129 -1.2128325 -0.67856979 -0.72689939][-4.381454 -4.2126431 -3.9187315 -3.5596061 -3.2440634 -2.9550636 -2.5949464 -2.1878486 -1.7988976 -1.4855999 -1.2567213 -1.103941 -0.79887843 -0.30700064 -0.37758636][-4.871932 -4.7337742 -4.4798722 -4.1400275 -3.7815702 -3.3991106 -2.928874 -2.4185805 -1.938952 -1.5773379 -1.3598521 -1.276083 -1.0836656 -0.673188 -0.71091843][-4.7671356 -4.6289034 -4.4217324 -4.1613197 -3.8717589 -3.4738235 -2.9540854 -2.4102902 -1.9345338 -1.658576 -1.6062838 -1.7333121 -1.7370511 -1.4326382 -1.388375][-4.2557287 -4.0447626 -3.8275275 -3.6133983 -3.3950219 -3.003629 -2.4736819 -1.9323304 -1.5271726 -1.441736 -1.6891696 -2.1624908 -2.46374 -2.3125997 -2.2072177][-3.4857111 -3.1385598 -2.8650031 -2.6477737 -2.4573293 -2.0307631 -1.4363632 -0.84827495 -0.5047729 -0.62468529 -1.2231364 -2.1175609 -2.7847407 -2.8647647 -2.7972298][-2.6600142 -2.1862607 -1.8355732 -1.6030697 -1.408313 -0.88917208 -0.16540265 0.51673961 0.8003931 0.48108983 -0.42535782 -1.6807295 -2.6496985 -2.9400887 -2.9463658][-2.2364674 -1.7290421 -1.3629575 -1.0969956 -0.81893015 -0.15626216 0.72941113 1.5090742 1.7587872 1.3218875 0.26231837 -1.1620243 -2.2810826 -2.6983018 -2.7663326][-2.1677384 -1.7277911 -1.4003901 -1.1225402 -0.78619766 -0.0568974 0.8665719 1.6220531 1.8175192 1.3785868 0.38528848 -0.93903756 -1.9723662 -2.3326907 -2.3837123][-2.5012443 -2.1513829 -1.8552327 -1.6112443 -1.2971144 -0.66274762 0.11127257 0.69167185 0.77045441 0.36801863 -0.41537786 -1.4129467 -2.1146808 -2.2052035 -2.1175511][-3.0305676 -2.7044997 -2.3744915 -2.0791965 -1.805156 -1.3511124 -0.85432267 -0.56686759 -0.67765808 -1.0687182 -1.6287715 -2.2558634 -2.577836 -2.319777 -2.0601115][-3.1788988 -2.7467151 -2.2936664 -1.9226344 -1.6386049 -1.385499 -1.2073681 -1.2433345 -1.5718638 -2.0181527 -2.4481926 -2.8164952 -2.8478551 -2.3249462 -1.9885241][-3.2025995 -2.6461835 -2.1067262 -1.6870911 -1.3846695 -1.2681105 -1.3290882 -1.5834306 -2.0323989 -2.5162308 -2.9302642 -3.2252681 -3.1553602 -2.55399 -2.284755][-3.3925054 -2.872817 -2.3788452 -2.0155985 -1.7591571 -1.7164779 -1.8733797 -2.1845469 -2.6142735 -3.0461543 -3.4111757 -3.6657312 -3.565 -2.9792104 -2.8529878][-3.8343675 -3.4006295 -3.0369148 -2.7745345 -2.5914085 -2.6143777 -2.8084614 -3.0947604 -3.4259245 -3.7403686 -4.0089731 -4.2000856 -4.0745592 -3.5653884 -3.5729818]]...]
INFO - root - 2017-12-12 07:29:00.398864: step 710, loss = 0.41, batch loss = 0.35 (36.1 examples/sec; 0.222 sec/batch; 20h:26m:50s remains)
INFO - root - 2017-12-12 07:29:02.641691: step 720, loss = 0.36, batch loss = 0.30 (35.5 examples/sec; 0.226 sec/batch; 20h:47m:26s remains)
INFO - root - 2017-12-12 07:29:04.860582: step 730, loss = 0.41, batch loss = 0.35 (36.0 examples/sec; 0.222 sec/batch; 20h:28m:22s remains)
INFO - root - 2017-12-12 07:29:07.055969: step 740, loss = 0.32, batch loss = 0.27 (35.4 examples/sec; 0.226 sec/batch; 20h:51m:03s remains)
INFO - root - 2017-12-12 07:29:09.300465: step 750, loss = 0.29, batch loss = 0.24 (35.8 examples/sec; 0.223 sec/batch; 20h:35m:03s remains)
INFO - root - 2017-12-12 07:29:11.494507: step 760, loss = 0.32, batch loss = 0.27 (36.6 examples/sec; 0.218 sec/batch; 20h:07m:42s remains)
INFO - root - 2017-12-12 07:29:13.726427: step 770, loss = 0.24, batch loss = 0.18 (35.1 examples/sec; 0.228 sec/batch; 20h:59m:21s remains)
INFO - root - 2017-12-12 07:29:15.915973: step 780, loss = 0.47, batch loss = 0.41 (35.5 examples/sec; 0.226 sec/batch; 20h:46m:47s remains)
INFO - root - 2017-12-12 07:29:18.156537: step 790, loss = 0.42, batch loss = 0.36 (34.9 examples/sec; 0.229 sec/batch; 21h:08m:40s remains)
INFO - root - 2017-12-12 07:29:20.373995: step 800, loss = 0.48, batch loss = 0.42 (35.1 examples/sec; 0.228 sec/batch; 20h:58m:34s remains)
2017-12-12 07:29:20.650750: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.3859935 -4.5104218 -3.9062853 -3.10007 -2.816072 -3.3848047 -4.5069752 -5.3269644 -5.1691508 -4.294662 -3.4756784 -2.9074097 -3.0743423 -3.9148068 -5.1592221][-5.4347615 -5.7532444 -5.2692075 -4.3846722 -3.8147111 -3.9039664 -4.437654 -4.7111149 -4.2252769 -3.3923295 -2.735462 -2.1992636 -2.0839729 -2.5504265 -3.6281881][-6.3275065 -6.9802027 -6.65137 -5.7191429 -4.86489 -4.4677725 -4.4006553 -4.081284 -3.2653975 -2.4135895 -1.9013596 -1.5386035 -1.3440347 -1.555528 -2.3777685][-6.5653491 -7.61312 -7.5051641 -6.6440916 -5.5153651 -4.5007305 -3.7126706 -2.7777457 -1.7834876 -1.1171651 -0.95521522 -0.948472 -0.85917664 -0.99591637 -1.6189587][-5.9229016 -7.4226675 -7.70257 -7.0627542 -5.7704649 -4.1161261 -2.5105305 -0.95594311 0.070109129 0.32428527 -0.037997484 -0.47749329 -0.57652926 -0.7474432 -1.2636151][-4.6962357 -6.6643381 -7.4467282 -7.1426597 -5.722362 -3.3839505 -0.78330827 1.5206151 2.5312595 2.1944985 1.106534 0.095219135 -0.26555777 -0.46713448 -0.86100674][-3.4712062 -5.67879 -6.783989 -6.631072 -4.9388146 -1.8153803 1.7947626 4.715198 5.5347829 4.4443913 2.5608144 0.97191 0.29707193 -0.0519135 -0.46154118][-2.5826995 -4.7537465 -5.9630642 -5.812747 -3.8242965 -0.1205802 4.0303831 7.0553546 7.4782557 5.7431245 3.3451152 1.441236 0.51630831 -0.031799793 -0.54457426][-2.068152 -3.9241526 -5.0566158 -4.9689827 -3.0506988 0.57011223 4.4576869 6.9719586 6.8442073 4.674458 2.1193943 0.26423264 -0.6612308 -1.190769 -1.6129826][-1.7680576 -3.1883421 -4.17678 -4.2557135 -2.7557781 0.26293612 3.4210114 5.201838 4.6036191 2.3344159 -0.046172857 -1.6969525 -2.5630672 -3.0581176 -3.3814545][-1.0604174 -2.2190619 -3.1828477 -3.56767 -2.7396793 -0.66558552 1.5294986 2.5265632 1.5837951 -0.55070782 -2.5902855 -4.0081491 -4.8188977 -5.2099762 -5.3036709][0.024672508 -1.1268451 -2.2513006 -3.023622 -2.8367658 -1.6385785 -0.29780316 0.062877178 -1.0531664 -2.9491572 -4.6756954 -5.8899984 -6.6371126 -6.8823066 -6.7980266][1.134202 -0.30347157 -1.7807063 -2.9588568 -3.31803 -2.8162673 -2.0576115 -1.9965643 -3.0126605 -4.55656 -5.9606829 -6.986649 -7.5954862 -7.7251816 -7.5491][2.5063124 0.77425909 -1.0983167 -2.7184033 -3.6276879 -3.7933409 -3.4810195 -3.5740004 -4.3778667 -5.5164042 -6.4886961 -7.1670222 -7.5851383 -7.6804094 -7.5478582][3.2963929 1.260313 -1.0295162 -2.9894671 -4.2437477 -4.62534 -4.4286208 -4.4485712 -4.9184432 -5.5892243 -6.1087294 -6.4992375 -6.914855 -7.1775751 -7.251627]]...]
INFO - root - 2017-12-12 07:29:22.887549: step 810, loss = 0.32, batch loss = 0.26 (34.9 examples/sec; 0.229 sec/batch; 21h:08m:00s remains)
INFO - root - 2017-12-12 07:29:25.130433: step 820, loss = 0.25, batch loss = 0.20 (35.4 examples/sec; 0.226 sec/batch; 20h:48m:26s remains)
INFO - root - 2017-12-12 07:29:27.330860: step 830, loss = 0.42, batch loss = 0.36 (35.2 examples/sec; 0.227 sec/batch; 20h:56m:49s remains)
INFO - root - 2017-12-12 07:29:29.562222: step 840, loss = 0.25, batch loss = 0.19 (35.0 examples/sec; 0.229 sec/batch; 21h:04m:51s remains)
INFO - root - 2017-12-12 07:29:31.777861: step 850, loss = 0.27, batch loss = 0.21 (36.2 examples/sec; 0.221 sec/batch; 20h:21m:23s remains)
INFO - root - 2017-12-12 07:29:34.026240: step 860, loss = 0.50, batch loss = 0.44 (36.0 examples/sec; 0.222 sec/batch; 20h:28m:17s remains)
INFO - root - 2017-12-12 07:29:36.253924: step 870, loss = 0.28, batch loss = 0.22 (34.6 examples/sec; 0.231 sec/batch; 21h:18m:46s remains)
INFO - root - 2017-12-12 07:29:38.465456: step 880, loss = 0.34, batch loss = 0.28 (37.5 examples/sec; 0.214 sec/batch; 19h:40m:10s remains)
INFO - root - 2017-12-12 07:29:40.698483: step 890, loss = 0.33, batch loss = 0.27 (35.2 examples/sec; 0.227 sec/batch; 20h:55m:41s remains)
INFO - root - 2017-12-12 07:29:42.919506: step 900, loss = 0.27, batch loss = 0.22 (37.3 examples/sec; 0.214 sec/batch; 19h:44m:40s remains)
2017-12-12 07:29:43.219780: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.2118607 -1.6435943 -1.4643687 -1.6023273 -2.1971436 -2.9845033 -3.5712655 -4.0377421 -4.55999 -4.9563866 -4.7614193 -4.3149633 -4.2941084 -4.6433592 -5.0793142][1.2914038 1.1148334 0.53900361 -0.18557024 -1.1979594 -2.1747472 -2.9588566 -3.6844761 -4.4253478 -4.9795527 -4.6776772 -4.0577903 -4.0034423 -4.3739395 -4.85081][3.7167068 2.6287584 1.1993322 -0.20728135 -1.5051185 -2.2777863 -2.7719679 -3.3625922 -4.1071835 -4.7147541 -4.3722196 -3.7039714 -3.7395146 -4.274538 -4.8558536][3.4116383 1.7043128 -0.11175323 -1.6791152 -2.6364472 -2.5852919 -2.1857812 -2.1891704 -2.779465 -3.5032539 -3.3957 -3.0076795 -3.2923179 -4.0075421 -4.6761055][0.91523457 -0.55075574 -1.9359868 -2.781914 -2.796339 -1.6002903 -0.12215638 0.55443764 0.025259495 -0.96626735 -1.4330397 -1.6993165 -2.4168801 -3.2493525 -3.8822026][-1.4326324 -2.0391173 -2.4642627 -2.2738862 -1.287693 0.81352949 3.0463 4.0592079 3.3643842 1.9074073 0.6951046 -0.2928586 -1.3560052 -2.1311498 -2.4951947][-3.1450374 -2.6643691 -2.1516731 -0.97556925 0.71302128 3.2398443 5.731535 6.6839018 5.6336536 3.6121287 1.8209076 0.39697242 -0.70013642 -1.1603143 -1.0266483][-4.3437214 -3.3688591 -2.3728681 -0.75326681 1.1439447 3.6558156 6.0580292 6.7243128 5.3963022 3.0540032 1.0209484 -0.51541662 -1.4746838 -1.5669457 -1.001452][-5.0301008 -4.3620977 -3.7166016 -2.5225437 -1.0016801 1.111938 3.2591567 3.8328977 2.6753607 0.59574938 -1.0715013 -2.298986 -3.0491509 -3.0624919 -2.4781771][-5.3972526 -5.5623007 -5.7309556 -5.1994967 -4.102407 -2.3564939 -0.42013597 0.29061651 -0.36603761 -1.8112345 -2.8325329 -3.4819899 -3.9968436 -4.1193442 -3.876246][-5.6841941 -6.416173 -7.0631838 -7.0248742 -6.3801365 -5.1779203 -3.7842553 -3.206461 -3.4221439 -4.1049209 -4.3047738 -4.243751 -4.3260789 -4.3427935 -4.2490325][-5.9532051 -6.5671644 -7.1056795 -7.2156429 -6.9536719 -6.4099503 -5.8233919 -5.6615028 -5.7093029 -5.7668991 -5.3570957 -4.7255664 -4.2905416 -3.9001181 -3.6456482][-6.3486061 -6.5251093 -6.6366892 -6.5620093 -6.38978 -6.2528205 -6.2010236 -6.3898635 -6.5147724 -6.4012413 -5.9151649 -5.1706457 -4.4205608 -3.5934627 -2.9894514][-6.898056 -6.7237549 -6.4356918 -6.018589 -5.6713996 -5.5007572 -5.53365 -5.8261781 -6.1199627 -6.199873 -5.97914 -5.4723892 -4.6983476 -3.664495 -2.8154089][-6.7889853 -6.6163054 -6.2938557 -5.7777386 -5.297121 -4.9598989 -4.8029046 -4.8924317 -5.1813841 -5.4752865 -5.6146774 -5.4415479 -4.9161973 -4.0194039 -3.1726334]]...]
INFO - root - 2017-12-12 07:29:45.447266: step 910, loss = 0.33, batch loss = 0.27 (36.6 examples/sec; 0.218 sec/batch; 20h:07m:18s remains)
INFO - root - 2017-12-12 07:29:47.666623: step 920, loss = 0.46, batch loss = 0.41 (36.6 examples/sec; 0.218 sec/batch; 20h:06m:56s remains)
INFO - root - 2017-12-12 07:29:49.873227: step 930, loss = 0.41, batch loss = 0.36 (35.7 examples/sec; 0.224 sec/batch; 20h:39m:10s remains)
INFO - root - 2017-12-12 07:29:52.098961: step 940, loss = 0.28, batch loss = 0.22 (35.5 examples/sec; 0.225 sec/batch; 20h:45m:17s remains)
INFO - root - 2017-12-12 07:29:54.339036: step 950, loss = 0.32, batch loss = 0.26 (35.8 examples/sec; 0.223 sec/batch; 20h:34m:45s remains)
INFO - root - 2017-12-12 07:29:56.558855: step 960, loss = 0.35, batch loss = 0.29 (36.1 examples/sec; 0.222 sec/batch; 20h:23m:58s remains)
INFO - root - 2017-12-12 07:29:58.787693: step 970, loss = 0.27, batch loss = 0.22 (35.5 examples/sec; 0.226 sec/batch; 20h:46m:08s remains)
INFO - root - 2017-12-12 07:30:01.013426: step 980, loss = 0.33, batch loss = 0.27 (35.9 examples/sec; 0.223 sec/batch; 20h:29m:50s remains)
INFO - root - 2017-12-12 07:30:03.262162: step 990, loss = 0.28, batch loss = 0.22 (35.4 examples/sec; 0.226 sec/batch; 20h:50m:16s remains)
INFO - root - 2017-12-12 07:30:05.483290: step 1000, loss = 0.30, batch loss = 0.24 (36.7 examples/sec; 0.218 sec/batch; 20h:03m:55s remains)
2017-12-12 07:30:05.744850: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.8970447 -6.1231608 -6.2476444 -6.2762532 -6.0934305 -5.4077897 -5.1969037 -5.9464989 -6.8848276 -7.8430033 -8.6741009 -8.9544506 -8.2181873 -6.5081482 -4.967392][-4.3020148 -4.6802969 -4.9625931 -5.1577482 -5.1095157 -4.5003586 -4.3314352 -5.1098022 -6.140933 -7.2185459 -8.1974583 -8.623704 -8.0027113 -6.3827515 -4.9177094][-2.7795324 -3.3400838 -3.8003986 -4.1235204 -4.121511 -3.489248 -3.2224929 -3.8777628 -4.9044003 -6.0513706 -7.1807404 -7.7740383 -7.3463516 -6.0019922 -4.7784557][-1.8399026 -2.4607835 -2.9349861 -3.1757171 -2.9645739 -2.1123004 -1.5767347 -2.02748 -3.0471251 -4.3212905 -5.6762357 -6.5251684 -6.4065533 -5.4617109 -4.5662093][-1.6575627 -2.0933561 -2.3137522 -2.2192769 -1.6228861 -0.44074821 0.42428207 0.147192 -0.91451907 -2.3895288 -4.0147195 -5.1679754 -5.4510684 -4.961977 -4.4181132][-1.8506926 -2.040724 -1.9581422 -1.4594258 -0.43468547 1.1105628 2.2978182 2.1555619 0.99991703 -0.69214272 -2.579844 -4.0333519 -4.7222233 -4.6609449 -4.3994756][-2.0344493 -1.9094619 -1.5353478 -0.68499255 0.73462391 2.6116672 3.9985781 3.8481798 2.4898086 0.58110523 -1.5554363 -3.2771339 -4.2346692 -4.4754505 -4.4183507][-1.9510704 -1.5378747 -0.9873116 0.011756897 1.5902724 3.5633645 4.9595275 4.7302146 3.2424216 1.2081156 -1.0540066 -2.8800812 -3.9214942 -4.2945676 -4.3784895][-1.5853292 -1.0061641 -0.4511621 0.42065072 1.782217 3.4811082 4.6167226 4.2481084 2.7803679 0.8384943 -1.3125055 -3.0142922 -3.9211411 -4.2201371 -4.32697][-1.253473 -0.69901443 -0.26316929 0.31354666 1.2224631 2.4036746 3.1109037 2.5941992 1.2560229 -0.39953208 -2.222811 -3.6242292 -4.2508683 -4.3256707 -4.3256488][-1.295686 -0.91557765 -0.66560221 -0.37667632 0.08536911 0.73403406 1.0183754 0.39348817 -0.76378417 -2.1122148 -3.5707335 -4.5990229 -4.8813639 -4.6706481 -4.4556561][-1.8932289 -1.6830447 -1.5639622 -1.4992325 -1.4318359 -1.209955 -1.2376685 -1.9068544 -2.889178 -3.9406385 -5.0562739 -5.7268181 -5.6642752 -5.1584835 -4.6991043][-3.0922611 -3.0163639 -2.9813123 -3.0300741 -3.1475754 -3.1206024 -3.2789104 -3.9070106 -4.7307396 -5.5682545 -6.4068642 -6.7509065 -6.3977661 -5.6377645 -4.8983827][-4.0791373 -4.081913 -4.0751204 -4.1383538 -4.27681 -4.2951341 -4.4727883 -5.0357141 -5.7654076 -6.4833212 -7.1326485 -7.292943 -6.7744203 -5.8500614 -4.8945932][-4.5960808 -4.6162696 -4.6012974 -4.6354408 -4.7327037 -4.7344413 -4.8812914 -5.3511081 -5.9906487 -6.6172056 -7.1401405 -7.2249622 -6.6985474 -5.7388945 -4.6912985]]...]
INFO - root - 2017-12-12 07:30:07.982986: step 1010, loss = 0.27, batch loss = 0.22 (36.5 examples/sec; 0.219 sec/batch; 20h:12m:29s remains)
/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFCtest
INFO - root - 2017-12-12 07:30:10.228513: step 1020, loss = 0.24, batch loss = 0.18 (35.5 examples/sec; 0.225 sec/batch; 20h:44m:26s remains)
INFO - root - 2017-12-12 07:30:12.451135: step 1030, loss = 0.50, batch loss = 0.44 (36.4 examples/sec; 0.220 sec/batch; 20h:15m:01s remains)
INFO - root - 2017-12-12 07:30:14.688687: step 1040, loss = 0.28, batch loss = 0.22 (35.6 examples/sec; 0.225 sec/batch; 20h:41m:43s remains)
INFO - root - 2017-12-12 07:30:16.909132: step 1050, loss = 0.33, batch loss = 0.27 (36.2 examples/sec; 0.221 sec/batch; 20h:20m:41s remains)
INFO - root - 2017-12-12 07:30:19.136674: step 1060, loss = 0.31, batch loss = 0.25 (36.7 examples/sec; 0.218 sec/batch; 20h:02m:58s remains)
INFO - root - 2017-12-12 07:30:21.417310: step 1070, loss = 0.26, batch loss = 0.20 (36.0 examples/sec; 0.222 sec/batch; 20h:28m:30s remains)
INFO - root - 2017-12-12 07:30:23.669378: step 1080, loss = 0.27, batch loss = 0.21 (36.0 examples/sec; 0.222 sec/batch; 20h:26m:13s remains)
INFO - root - 2017-12-12 07:30:25.896416: step 1090, loss = 0.36, batch loss = 0.31 (36.1 examples/sec; 0.222 sec/batch; 20h:24m:53s remains)
INFO - root - 2017-12-12 07:30:28.116074: step 1100, loss = 0.25, batch loss = 0.19 (37.0 examples/sec; 0.216 sec/batch; 19h:55m:13s remains)
2017-12-12 07:30:28.382935: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.5825408 -2.4838414 -2.5044093 -2.5130303 -2.55751 -2.6369448 -2.7005761 -2.7234573 -2.7337766 -2.6935725 -2.6520548 -2.6557646 -2.7423906 -2.9014626 -3.1518698][-2.2931061 -2.2532384 -2.3638082 -2.4721172 -2.6211114 -2.7787709 -2.860178 -2.8579142 -2.8381042 -2.7594919 -2.6811826 -2.6915793 -2.8704789 -3.1430228 -3.4555442][-2.4705672 -2.4903758 -2.6294191 -2.7693112 -2.9340591 -3.0784948 -3.0928261 -3.0136008 -2.9478993 -2.8323879 -2.7159452 -2.7288163 -3.0236683 -3.4276192 -3.8213289][-3.0577703 -3.0727389 -3.1452088 -3.20949 -3.313242 -3.3933895 -3.2719305 -3.0715015 -2.9474635 -2.8229938 -2.696701 -2.7257853 -3.1618385 -3.709579 -4.1903343][-3.633508 -3.5069559 -3.3893626 -3.3074677 -3.3106236 -3.2933705 -2.9924545 -2.611371 -2.4132657 -2.3014512 -2.2379851 -2.3506284 -2.9575205 -3.7016273 -4.2974448][-3.8480039 -3.4329708 -3.0270567 -2.7477036 -2.6592333 -2.5660427 -2.0614405 -1.4125979 -1.07042 -0.990983 -1.0661263 -1.3233292 -2.0857182 -2.9912994 -3.7911105][-3.4767847 -2.6863151 -1.9625993 -1.5366626 -1.4442363 -1.357815 -0.69874263 0.18663335 0.6459341 0.64260292 0.34139323 -0.10785055 -0.98302722 -1.9570959 -2.836699][-2.6191926 -1.5617884 -0.66291165 -0.25551605 -0.29453945 -0.31594443 0.40794396 1.4724674 2.0257459 1.9436054 1.4187636 0.78985214 -0.1371851 -1.0855117 -1.9407607][-1.6409204 -0.5018754 0.37566566 0.63242245 0.35613537 0.10924625 0.70144129 1.7467775 2.3191767 2.2120914 1.6059422 0.94562292 0.13073158 -0.68674994 -1.4464164][-1.0186739 0.034818411 0.76663828 0.82754087 0.32521343 -0.17402697 0.10858154 0.90374136 1.3779817 1.2810631 0.74657774 0.23577428 -0.29390883 -0.82350254 -1.452173][-0.99457717 -0.15744328 0.36441398 0.25246263 -0.37481308 -1.0590878 -1.1370137 -0.72114968 -0.41443038 -0.42550921 -0.70560384 -0.89260316 -1.0562544 -1.2366579 -1.6522814][-1.4148781 -0.87184739 -0.56491446 -0.78592157 -1.4409373 -2.2111125 -2.5842142 -2.5411239 -2.4019179 -2.3093576 -2.3001697 -2.1628475 -2.0260644 -1.9226967 -2.0906215][-2.0849519 -1.8710026 -1.8003477 -2.0989792 -2.7039523 -3.3985753 -3.863132 -3.9846921 -3.9070115 -3.7386456 -3.5691276 -3.2854559 -3.0052066 -2.719784 -2.7418025][-2.563798 -2.6006622 -2.7043805 -3.0254679 -3.5173669 -4.0577545 -4.4723577 -4.6240082 -4.563242 -4.3865309 -4.1483436 -3.8222048 -3.4875112 -3.0971138 -3.0619876][-2.8163233 -2.9926946 -3.1812592 -3.4869692 -3.8634131 -4.2444139 -4.5364866 -4.636621 -4.571106 -4.4093943 -4.1763363 -3.8787775 -3.5914919 -3.2242 -3.226536]]...]
INFO - root - 2017-12-12 07:30:30.602397: step 1110, loss = 0.31, batch loss = 0.25 (36.2 examples/sec; 0.221 sec/batch; 20h:22m:08s remains)
INFO - root - 2017-12-12 07:30:32.849531: step 1120, loss = 0.42, batch loss = 0.36 (36.1 examples/sec; 0.222 sec/batch; 20h:24m:55s remains)
INFO - root - 2017-12-12 07:30:35.075500: step 1130, loss = 0.37, batch loss = 0.31 (35.6 examples/sec; 0.225 sec/batch; 20h:41m:24s remains)
INFO - root - 2017-12-12 07:30:37.305942: step 1140, loss = 0.40, batch loss = 0.35 (33.7 examples/sec; 0.237 sec/batch; 21h:49m:14s remains)
INFO - root - 2017-12-12 07:30:39.578019: step 1150, loss = 0.36, batch loss = 0.31 (36.3 examples/sec; 0.220 sec/batch; 20h:17m:33s remains)
INFO - root - 2017-12-12 07:30:41.818760: step 1160, loss = 0.33, batch loss = 0.28 (36.1 examples/sec; 0.221 sec/batch; 20h:22m:38s remains)
INFO - root - 2017-12-12 07:30:44.049660: step 1170, loss = 0.31, batch loss = 0.25 (34.9 examples/sec; 0.229 sec/batch; 21h:06m:44s remains)
INFO - root - 2017-12-12 07:30:46.271167: step 1180, loss = 0.23, batch loss = 0.17 (35.4 examples/sec; 0.226 sec/batch; 20h:49m:11s remains)
INFO - root - 2017-12-12 07:30:48.517888: step 1190, loss = 0.28, batch loss = 0.22 (34.9 examples/sec; 0.230 sec/batch; 21h:07m:21s remains)
INFO - root - 2017-12-12 07:30:50.767182: step 1200, loss = 0.23, batch loss = 0.18 (34.2 examples/sec; 0.234 sec/batch; 21h:30m:09s remains)
2017-12-12 07:30:51.113037: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.2730541 -4.80246 -4.2955976 -4.0107522 -3.9818761 -4.0822105 -4.2271385 -4.5866847 -4.9956942 -5.4672318 -5.9262667 -6.0107422 -5.7224331 -4.9621682 -4.1670771][-4.7015109 -4.3467932 -4.0676079 -3.9745932 -4.0504584 -4.1334844 -4.2349105 -4.6295242 -5.1393266 -5.7761221 -6.3254175 -6.3911729 -6.0120258 -5.1864085 -4.4148278][-3.7611945 -3.7254012 -3.7684922 -3.8710017 -3.979918 -3.9844959 -4.0040851 -4.33461 -4.8879733 -5.6878834 -6.4219422 -6.5862932 -6.1642551 -5.2384806 -4.4136353][-2.8134003 -3.0659034 -3.3368087 -3.4098928 -3.2520022 -2.9318371 -2.783865 -3.0889251 -3.7797239 -4.9086905 -6.0308442 -6.4748831 -6.1320238 -5.1072779 -4.1406422][-1.7503223 -2.1899414 -2.518373 -2.3276241 -1.6007308 -0.70111346 -0.32366467 -0.71848607 -1.7389034 -3.3242271 -4.9332514 -5.8076649 -5.7825089 -4.8947811 -3.8412533][-0.768348 -1.2221537 -1.4561795 -0.87097454 0.536181 2.0832412 2.7199805 2.1310508 0.63809228 -1.4262959 -3.4621861 -4.78735 -5.2674813 -4.7959676 -3.8431652][-0.55880213 -0.94107485 -0.97792554 0.045883894 2.0545275 4.2191715 5.1874065 4.4657774 2.6005557 0.20721555 -2.0305128 -3.6285455 -4.5716066 -4.6014881 -3.9240358][-1.2529359 -1.6617647 -1.5205836 -0.11351109 2.3273566 4.9672642 6.2676315 5.5600147 3.5450523 1.0437701 -1.1947391 -2.8474233 -4.0681329 -4.5274172 -4.26378][-2.0802069 -2.5156043 -2.33781 -0.89653277 1.5056894 4.1356392 5.470355 4.8013754 2.9398086 0.73280454 -1.1937747 -2.5936973 -3.8283141 -4.5751448 -4.7386427][-2.940757 -3.3628819 -3.2335076 -2.0680859 -0.16598272 1.893379 2.9146683 2.3302443 0.93986535 -0.5587244 -1.8232505 -2.7123299 -3.6946473 -4.5640922 -5.066956][-4.1162367 -4.3999853 -4.2510414 -3.4232838 -2.10214 -0.72622943 -0.14865661 -0.63211751 -1.4449104 -2.1422343 -2.6748831 -3.0224791 -3.6162128 -4.4283142 -5.1226912][-5.2585249 -5.3140817 -5.0736647 -4.5279074 -3.7637491 -3.0704618 -2.9342306 -3.3816993 -3.7445407 -3.7862856 -3.6825063 -3.4717243 -3.655484 -4.2782907 -4.9153314][-6.2316842 -6.0544567 -5.7334337 -5.4169149 -5.10057 -4.8796377 -5.0284743 -5.382535 -5.4151344 -4.999577 -4.482584 -3.9672039 -3.8880675 -4.3053284 -4.7577963][-6.5787554 -6.1765947 -5.8320718 -5.7388678 -5.72725 -5.7641611 -5.9995565 -6.2328033 -6.1332054 -5.5766106 -4.9810357 -4.4676805 -4.3065577 -4.5394011 -4.7705021][-6.6571445 -6.1110606 -5.7879157 -5.880682 -6.0132322 -6.116931 -6.3642316 -6.519381 -6.3760796 -5.8596339 -5.3809586 -5.0671024 -5.0081105 -5.1614571 -5.2651939]]...]
INFO - root - 2017-12-12 07:30:53.348489: step 1210, loss = 0.36, batch loss = 0.31 (36.1 examples/sec; 0.222 sec/batch; 20h:25m:11s remains)
INFO - root - 2017-12-12 07:30:55.555375: step 1220, loss = 0.28, batch loss = 0.22 (37.5 examples/sec; 0.213 sec/batch; 19h:36m:32s remains)
INFO - root - 2017-12-12 07:30:57.778028: step 1230, loss = 0.26, batch loss = 0.20 (35.6 examples/sec; 0.224 sec/batch; 20h:39m:05s remains)
INFO - root - 2017-12-12 07:31:00.014416: step 1240, loss = 0.28, batch loss = 0.22 (36.4 examples/sec; 0.220 sec/batch; 20h:12m:37s remains)
INFO - root - 2017-12-12 07:31:02.242846: step 1250, loss = 0.48, batch loss = 0.42 (36.8 examples/sec; 0.217 sec/batch; 19h:59m:41s remains)
INFO - root - 2017-12-12 07:31:04.468586: step 1260, loss = 0.26, batch loss = 0.21 (36.4 examples/sec; 0.220 sec/batch; 20h:14m:44s remains)
INFO - root - 2017-12-12 07:31:06.718557: step 1270, loss = 0.45, batch loss = 0.39 (34.7 examples/sec; 0.230 sec/batch; 21h:11m:03s remains)
