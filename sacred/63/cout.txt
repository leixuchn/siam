INFO - Siam-FC - Running command 'main'
INFO - Siam-FC - Started run with ID "63"
INFO - root - Creating training directory: /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2
INFO - root - preproces -- siamese_fc_color
WARNING - root - root is not explicitly specified, using default value: None
INFO - root - For training, we use instance size 255 - 2 * 8 ...
WARNING - root - init_method is not explicitly specified, using default value: None
sdiufhasudf Tensor("siamese_fc/conv5/split:0", shape=(8, 8, 8, 192), dtype=float32)
Tensor("siamese_fc/conv5/def/transpose:0", shape=(8, 192, 8, 8), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/b1/weights:0' shape=(128, 192, 3, 3) dtype=float32_ref> Tensor("siamese_fc/conv5/def/transpose_1:0", shape=(8, 72, 8, 8), dtype=float32)
ddd Tensor("siamese_fc/conv5/def/b1/transpose:0", shape=(8, 6, 6, 128), dtype=float32)
Tensor("siamese_fc/conv5/def/transpose_2:0", shape=(8, 192, 8, 8), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/b2/weights:0' shape=(128, 192, 3, 3) dtype=float32_ref> Tensor("siamese_fc/conv5/def/transpose_3:0", shape=(8, 72, 8, 8), dtype=float32)
sdiufhasudf Tensor("siamese_fc_1/conv5/split:0", shape=(8, 22, 22, 192), dtype=float32)
Tensor("siamese_fc_1/conv5/def/transpose:0", shape=(8, 192, 22, 22), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/b1/weights:0' shape=(128, 192, 3, 3) dtype=float32_ref> Tensor("siamese_fc_1/conv5/def/transpose_1:0", shape=(8, 72, 22, 22), dtype=float32)
ddd Tensor("siamese_fc_1/conv5/def/b1/transpose:0", shape=(8, 20, 20, 128), dtype=float32)
Tensor("siamese_fc_1/conv5/def/transpose_2:0", shape=(8, 192, 22, 22), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/b2/weights:0' shape=(128, 192, 3, 3) dtype=float32_ref> Tensor("siamese_fc_1/conv5/def/transpose_3:0", shape=(8, 72, 22, 22), dtype=float32)
Tensor("detection/add:0", shape=(8, 15, 15), dtype=float32)
INFO - root - Model moving average is disabled since decay factor is 0
2017-12-05 07:57:49.762020: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-05 07:57:49.762059: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-05 07:57:49.762065: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2017-12-05 07:57:49.762069: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-05 07:57:49.762074: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
2017-12-05 07:57:50.318099: I tensorflow/core/common_runtime/gpu/gpu_device.cc:887] Found device 0 with properties: 
name: Tesla K80
major: 3 minor: 7 memoryClockRate (GHz) 0.8235
pciBusID 5791:00:00.0
Total memory: 11.17GiB
Free memory: 11.11GiB
2017-12-05 07:57:50.318139: I tensorflow/core/common_runtime/gpu/gpu_device.cc:908] DMA: 0 
2017-12-05 07:57:50.318146: I tensorflow/core/common_runtime/gpu/gpu_device.cc:918] 0:   Y 
2017-12-05 07:57:50.318154: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla K80, pci bus id: 5791:00:00.0)
INFO:tensorflow:Restoring parameters from /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-only/model.ckpt-150000
INFO - tensorflow - Restoring parameters from /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-only/model.ckpt-150000
INFO - root - Starting threads 0 ...

INFO - root - training for 332500 steps
INFO - root - 2017-12-05 07:57:53.962700: step 0, loss = 2.03, batch loss = 1.97 (3.1 examples/sec; 2.546 sec/batch; 235h:07m:56s remains)
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2/model.ckpt-0 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2/model.ckpt-0 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-05 07:57:57.104996: step 10, loss = 2.04, batch loss = 1.98 (31.6 examples/sec; 0.254 sec/batch; 23h:24m:52s remains)
INFO - root - 2017-12-05 07:57:59.283653: step 20, loss = 2.07, batch loss = 2.01 (37.7 examples/sec; 0.212 sec/batch; 19h:35m:57s remains)
INFO - root - 2017-12-05 07:58:01.471672: step 30, loss = 2.04, batch loss = 1.98 (38.5 examples/sec; 0.208 sec/batch; 19h:12m:13s remains)
INFO - root - 2017-12-05 07:58:03.656574: step 40, loss = 2.03, batch loss = 1.98 (37.7 examples/sec; 0.212 sec/batch; 19h:34m:23s remains)
INFO - root - 2017-12-05 07:58:05.892523: step 50, loss = 2.04, batch loss = 1.98 (38.3 examples/sec; 0.209 sec/batch; 19h:17m:09s remains)
INFO - root - 2017-12-05 07:58:08.023627: step 60, loss = 2.01, batch loss = 1.95 (37.8 examples/sec; 0.212 sec/batch; 19h:34m:06s remains)
INFO - root - 2017-12-05 07:58:10.185840: step 70, loss = 2.00, batch loss = 1.94 (37.0 examples/sec; 0.216 sec/batch; 19h:58m:53s remains)
INFO - root - 2017-12-05 07:58:12.334050: step 80, loss = 2.04, batch loss = 1.98 (37.3 examples/sec; 0.215 sec/batch; 19h:48m:39s remains)
INFO - root - 2017-12-05 07:58:14.476973: step 90, loss = 2.03, batch loss = 1.97 (37.0 examples/sec; 0.216 sec/batch; 19h:56m:57s remains)
INFO - root - 2017-12-05 07:58:16.634458: step 100, loss = 2.03, batch loss = 1.97 (37.5 examples/sec; 0.213 sec/batch; 19h:41m:38s remains)
INFO - root - 2017-12-05 07:58:18.833798: step 110, loss = 2.02, batch loss = 1.96 (37.1 examples/sec; 0.216 sec/batch; 19h:55m:37s remains)
INFO - root - 2017-12-05 07:58:20.958357: step 120, loss = 2.04, batch loss = 1.98 (38.5 examples/sec; 0.208 sec/batch; 19h:12m:28s remains)
INFO - root - 2017-12-05 07:58:23.089687: step 130, loss = 2.02, batch loss = 1.96 (37.3 examples/sec; 0.214 sec/batch; 19h:47m:40s remains)
INFO - root - 2017-12-05 07:58:25.242809: step 140, loss = 2.00, batch loss = 1.94 (38.0 examples/sec; 0.211 sec/batch; 19h:27m:26s remains)
INFO - root - 2017-12-05 07:58:27.398608: step 150, loss = 1.97, batch loss = 1.92 (35.9 examples/sec; 0.223 sec/batch; 20h:34m:42s remains)
INFO - root - 2017-12-05 07:58:29.537091: step 160, loss = 2.02, batch loss = 1.96 (37.3 examples/sec; 0.214 sec/batch; 19h:48m:01s remains)
INFO - root - 2017-12-05 07:58:31.653273: step 170, loss = 1.98, batch loss = 1.92 (37.5 examples/sec; 0.214 sec/batch; 19h:43m:06s remains)
INFO - root - 2017-12-05 07:58:33.780587: step 180, loss = 1.98, batch loss = 1.92 (37.4 examples/sec; 0.214 sec/batch; 19h:45m:13s remains)
INFO - root - 2017-12-05 07:58:35.909971: step 190, loss = 1.92, batch loss = 1.86 (38.3 examples/sec; 0.209 sec/batch; 19h:16m:12s remains)
INFO - root - 2017-12-05 07:58:38.082470: step 200, loss = 1.95, batch loss = 1.89 (37.3 examples/sec; 0.215 sec/batch; 19h:49m:09s remains)
INFO - root - 2017-12-05 07:58:40.300915: step 210, loss = 1.94, batch loss = 1.88 (36.9 examples/sec; 0.217 sec/batch; 20h:00m:04s remains)
INFO - root - 2017-12-05 07:58:42.426675: step 220, loss = 1.97, batch loss = 1.92 (37.8 examples/sec; 0.211 sec/batch; 19h:30m:32s remains)
INFO - root - 2017-12-05 07:58:44.583015: step 230, loss = 1.96, batch loss = 1.91 (37.7 examples/sec; 0.212 sec/batch; 19h:35m:22s remains)
INFO - root - 2017-12-05 07:58:46.733644: step 240, loss = 1.97, batch loss = 1.91 (37.5 examples/sec; 0.213 sec/batch; 19h:42m:01s remains)
INFO - root - 2017-12-05 07:58:48.894058: step 250, loss = 1.92, batch loss = 1.86 (38.3 examples/sec; 0.209 sec/batch; 19h:17m:01s remains)
INFO - root - 2017-12-05 07:58:51.027582: step 260, loss = 1.94, batch loss = 1.88 (38.0 examples/sec; 0.211 sec/batch; 19h:25m:36s remains)
INFO - root - 2017-12-05 07:58:53.149744: step 270, loss = 1.92, batch loss = 1.86 (36.9 examples/sec; 0.217 sec/batch; 20h:01m:36s remains)
INFO - root - 2017-12-05 07:58:55.315967: step 280, loss = 1.87, batch loss = 1.82 (38.1 examples/sec; 0.210 sec/batch; 19h:23m:19s remains)
INFO - root - 2017-12-05 07:58:57.474543: step 290, loss = 1.80, batch loss = 1.74 (36.3 examples/sec; 0.221 sec/batch; 20h:21m:02s remains)
INFO - root - 2017-12-05 07:58:59.623989: step 300, loss = 1.82, batch loss = 1.76 (37.0 examples/sec; 0.216 sec/batch; 19h:57m:08s remains)
INFO - root - 2017-12-05 07:59:01.840088: step 310, loss = 1.88, batch loss = 1.82 (36.8 examples/sec; 0.217 sec/batch; 20h:03m:42s remains)
INFO - root - 2017-12-05 07:59:04.060444: step 320, loss = 1.85, batch loss = 1.80 (32.3 examples/sec; 0.248 sec/batch; 22h:52m:09s remains)
INFO - root - 2017-12-05 07:59:06.224472: step 330, loss = 1.82, batch loss = 1.76 (37.2 examples/sec; 0.215 sec/batch; 19h:50m:47s remains)
INFO - root - 2017-12-05 07:59:08.360950: step 340, loss = 1.80, batch loss = 1.74 (38.2 examples/sec; 0.209 sec/batch; 19h:18m:14s remains)
INFO - root - 2017-12-05 07:59:10.599302: step 350, loss = 1.86, batch loss = 1.80 (36.8 examples/sec; 0.217 sec/batch; 20h:01m:59s remains)
INFO - root - 2017-12-05 07:59:12.754789: step 360, loss = 1.85, batch loss = 1.80 (36.9 examples/sec; 0.217 sec/batch; 19h:59m:58s remains)
INFO - root - 2017-12-05 07:59:14.931400: step 370, loss = 1.87, batch loss = 1.82 (37.1 examples/sec; 0.215 sec/batch; 19h:52m:19s remains)
INFO - root - 2017-12-05 07:59:17.076213: step 380, loss = 1.89, batch loss = 1.83 (37.9 examples/sec; 0.211 sec/batch; 19h:27m:31s remains)
INFO - root - 2017-12-05 07:59:19.209774: step 390, loss = 1.78, batch loss = 1.73 (36.9 examples/sec; 0.217 sec/batch; 20h:01m:18s remains)
INFO - root - 2017-12-05 07:59:21.352612: step 400, loss = 1.70, batch loss = 1.64 (37.9 examples/sec; 0.211 sec/batch; 19h:29m:36s remains)
INFO - root - 2017-12-05 07:59:23.566358: step 410, loss = 1.78, batch loss = 1.72 (37.7 examples/sec; 0.212 sec/batch; 19h:34m:08s remains)
INFO - root - 2017-12-05 07:59:25.745783: step 420, loss = 1.82, batch loss = 1.77 (36.4 examples/sec; 0.220 sec/batch; 20h:16m:41s remains)
INFO - root - 2017-12-05 07:59:27.890925: step 430, loss = 1.75, batch loss = 1.69 (37.0 examples/sec; 0.216 sec/batch; 19h:56m:33s remains)
INFO - root - 2017-12-05 07:59:30.064093: step 440, loss = 1.74, batch loss = 1.68 (37.3 examples/sec; 0.215 sec/batch; 19h:47m:15s remains)
INFO - root - 2017-12-05 07:59:32.229448: step 450, loss = 1.80, batch loss = 1.74 (37.4 examples/sec; 0.214 sec/batch; 19h:43m:58s remains)
INFO - root - 2017-12-05 07:59:34.379690: step 460, loss = 1.63, batch loss = 1.57 (37.2 examples/sec; 0.215 sec/batch; 19h:49m:41s remains)
INFO - root - 2017-12-05 07:59:36.562991: step 470, loss = 1.75, batch loss = 1.69 (35.8 examples/sec; 0.223 sec/batch; 20h:35m:41s remains)
INFO - root - 2017-12-05 07:59:38.726697: step 480, loss = 1.72, batch loss = 1.66 (35.1 examples/sec; 0.228 sec/batch; 21h:02m:50s remains)
INFO - root - 2017-12-05 07:59:40.898453: step 490, loss = 1.61, batch loss = 1.55 (37.9 examples/sec; 0.211 sec/batch; 19h:29m:16s remains)
INFO - root - 2017-12-05 07:59:43.060468: step 500, loss = 1.74, batch loss = 1.68 (33.6 examples/sec; 0.238 sec/batch; 21h:56m:27s remains)
INFO - root - 2017-12-05 07:59:45.294148: step 510, loss = 1.67, batch loss = 1.61 (37.5 examples/sec; 0.213 sec/batch; 19h:41m:18s remains)
INFO - root - 2017-12-05 07:59:47.458482: step 520, loss = 1.65, batch loss = 1.59 (37.6 examples/sec; 0.213 sec/batch; 19h:38m:11s remains)
INFO - root - 2017-12-05 07:59:49.616789: step 530, loss = 1.54, batch loss = 1.48 (37.0 examples/sec; 0.216 sec/batch; 19h:55m:49s remains)
INFO - root - 2017-12-05 07:59:51.769151: step 540, loss = 1.62, batch loss = 1.57 (37.7 examples/sec; 0.212 sec/batch; 19h:33m:29s remains)
INFO - root - 2017-12-05 07:59:53.961207: step 550, loss = 1.51, batch loss = 1.45 (35.1 examples/sec; 0.228 sec/batch; 20h:59m:19s remains)
INFO - root - 2017-12-05 07:59:56.129479: step 560, loss = 1.65, batch loss = 1.59 (37.4 examples/sec; 0.214 sec/batch; 19h:43m:56s remains)
INFO - root - 2017-12-05 07:59:58.307973: step 570, loss = 1.60, batch loss = 1.54 (35.8 examples/sec; 0.224 sec/batch; 20h:37m:21s remains)
INFO - root - 2017-12-05 08:00:00.465451: step 580, loss = 1.50, batch loss = 1.44 (37.9 examples/sec; 0.211 sec/batch; 19h:27m:28s remains)
INFO - root - 2017-12-05 08:00:02.610229: step 590, loss = 1.63, batch loss = 1.58 (36.1 examples/sec; 0.221 sec/batch; 20h:24m:33s remains)
INFO - root - 2017-12-05 08:00:04.773777: step 600, loss = 1.58, batch loss = 1.52 (37.8 examples/sec; 0.212 sec/batch; 19h:31m:54s remains)
INFO - root - 2017-12-05 08:00:07.044820: step 610, loss = 1.80, batch loss = 1.74 (35.3 examples/sec; 0.227 sec/batch; 20h:54m:56s remains)
INFO - root - 2017-12-05 08:00:09.207043: step 620, loss = 1.51, batch loss = 1.46 (37.3 examples/sec; 0.214 sec/batch; 19h:46m:10s remains)
INFO - root - 2017-12-05 08:00:11.388109: step 630, loss = 1.51, batch loss = 1.46 (35.8 examples/sec; 0.224 sec/batch; 20h:36m:34s remains)
INFO - root - 2017-12-05 08:00:13.528723: step 640, loss = 1.39, batch loss = 1.34 (37.6 examples/sec; 0.213 sec/batch; 19h:36m:20s remains)
INFO - root - 2017-12-05 08:00:15.668796: step 650, loss = 1.48, batch loss = 1.42 (37.5 examples/sec; 0.213 sec/batch; 19h:38m:33s remains)
INFO - root - 2017-12-05 08:00:17.844773: step 660, loss = 1.40, batch loss = 1.34 (36.3 examples/sec; 0.220 sec/batch; 20h:18m:04s remains)
INFO - root - 2017-12-05 08:00:20.017997: step 670, loss = 1.40, batch loss = 1.34 (36.9 examples/sec; 0.217 sec/batch; 19h:59m:08s remains)
INFO - root - 2017-12-05 08:00:22.177973: step 680, loss = 1.53, batch loss = 1.47 (38.0 examples/sec; 0.211 sec/batch; 19h:25m:45s remains)
INFO - root - 2017-12-05 08:00:24.370804: step 690, loss = 1.33, batch loss = 1.27 (36.0 examples/sec; 0.222 sec/batch; 20h:30m:03s remains)
INFO - root - 2017-12-05 08:00:26.536085: step 700, loss = 1.23, batch loss = 1.17 (37.0 examples/sec; 0.216 sec/batch; 19h:55m:57s remains)
INFO - root - 2017-12-05 08:00:28.743139: step 710, loss = 1.48, batch loss = 1.42 (37.4 examples/sec; 0.214 sec/batch; 19h:41m:28s remains)
INFO - root - 2017-12-05 08:00:30.905703: step 720, loss = 1.43, batch loss = 1.37 (36.6 examples/sec; 0.218 sec/batch; 20h:08m:07s remains)
INFO - root - 2017-12-05 08:00:33.057667: step 730, loss = 1.50, batch loss = 1.44 (37.4 examples/sec; 0.214 sec/batch; 19h:41m:26s remains)
INFO - root - 2017-12-05 08:00:35.241778: step 740, loss = 1.35, batch loss = 1.29 (37.3 examples/sec; 0.214 sec/batch; 19h:45m:10s remains)
INFO - root - 2017-12-05 08:00:37.419804: step 750, loss = 1.43, batch loss = 1.37 (37.7 examples/sec; 0.212 sec/batch; 19h:34m:22s remains)
INFO - root - 2017-12-05 08:00:39.589090: step 760, loss = 1.69, batch loss = 1.64 (36.3 examples/sec; 0.220 sec/batch; 20h:18m:56s remains)
INFO - root - 2017-12-05 08:00:41.728586: step 770, loss = 1.43, batch loss = 1.37 (38.6 examples/sec; 0.207 sec/batch; 19h:05m:02s remains)
INFO - root - 2017-12-05 08:00:43.890006: step 780, loss = 1.45, batch loss = 1.39 (36.8 examples/sec; 0.218 sec/batch; 20h:03m:23s remains)
INFO - root - 2017-12-05 08:00:46.054522: step 790, loss = 1.59, batch loss = 1.53 (37.1 examples/sec; 0.216 sec/batch; 19h:53m:08s remains)
INFO - root - 2017-12-05 08:00:48.214506: step 800, loss = 1.37, batch loss = 1.31 (37.2 examples/sec; 0.215 sec/batch; 19h:49m:14s remains)
INFO - root - 2017-12-05 08:00:50.444973: step 810, loss = 1.32, batch loss = 1.26 (37.2 examples/sec; 0.215 sec/batch; 19h:48m:07s remains)
INFO - root - 2017-12-05 08:00:52.668019: step 820, loss = 1.51, batch loss = 1.46 (36.3 examples/sec; 0.221 sec/batch; 20h:18m:57s remains)
INFO - root - 2017-12-05 08:00:54.825109: step 830, loss = 1.39, batch loss = 1.34 (37.7 examples/sec; 0.212 sec/batch; 19h:33m:47s remains)
INFO - root - 2017-12-05 08:00:56.991517: step 840, loss = 1.35, batch loss = 1.30 (37.4 examples/sec; 0.214 sec/batch; 19h:40m:57s remains)
INFO - root - 2017-12-05 08:00:59.190757: step 850, loss = 1.44, batch loss = 1.38 (37.7 examples/sec; 0.212 sec/batch; 19h:33m:40s remains)
INFO - root - 2017-12-05 08:01:01.365019: step 860, loss = 1.39, batch loss = 1.33 (38.0 examples/sec; 0.210 sec/batch; 19h:22m:19s remains)
INFO - root - 2017-12-05 08:01:03.497206: step 870, loss = 1.54, batch loss = 1.48 (37.5 examples/sec; 0.213 sec/batch; 19h:38m:57s remains)
INFO - root - 2017-12-05 08:01:05.667171: step 880, loss = 1.68, batch loss = 1.62 (37.1 examples/sec; 0.215 sec/batch; 19h:50m:17s remains)
INFO - root - 2017-12-05 08:01:07.821582: step 890, loss = 1.32, batch loss = 1.26 (37.9 examples/sec; 0.211 sec/batch; 19h:26m:43s remains)
INFO - root - 2017-12-05 08:01:10.001085: step 900, loss = 1.41, batch loss = 1.35 (37.4 examples/sec; 0.214 sec/batch; 19h:42m:11s remains)
INFO - root - 2017-12-05 08:01:12.222334: step 910, loss = 1.66, batch loss = 1.60 (37.5 examples/sec; 0.213 sec/batch; 19h:37m:47s remains)
INFO - root - 2017-12-05 08:01:14.383203: step 920, loss = 1.57, batch loss = 1.51 (36.7 examples/sec; 0.218 sec/batch; 20h:03m:40s remains)
INFO - root - 2017-12-05 08:01:16.540969: step 930, loss = 1.59, batch loss = 1.53 (37.4 examples/sec; 0.214 sec/batch; 19h:42m:56s remains)
INFO - root - 2017-12-05 08:01:18.726457: step 940, loss = 1.53, batch loss = 1.47 (36.7 examples/sec; 0.218 sec/batch; 20h:04m:00s remains)
INFO - root - 2017-12-05 08:01:20.888963: step 950, loss = 1.49, batch loss = 1.43 (35.9 examples/sec; 0.223 sec/batch; 20h:31m:05s remains)
INFO - root - 2017-12-05 08:01:23.035866: step 960, loss = 1.80, batch loss = 1.74 (37.4 examples/sec; 0.214 sec/batch; 19h:41m:01s remains)
INFO - root - 2017-12-05 08:01:25.230298: step 970, loss = 1.43, batch loss = 1.37 (37.2 examples/sec; 0.215 sec/batch; 19h:47m:29s remains)
INFO - root - 2017-12-05 08:01:27.384650: step 980, loss = 1.77, batch loss = 1.71 (37.0 examples/sec; 0.216 sec/batch; 19h:53m:32s remains)
INFO - root - 2017-12-05 08:01:29.553310: step 990, loss = 1.68, batch loss = 1.62 (37.1 examples/sec; 0.216 sec/batch; 19h:51m:27s remains)
INFO - root - 2017-12-05 08:01:31.691007: step 1000, loss = 1.73, batch loss = 1.68 (37.7 examples/sec; 0.212 sec/batch; 19h:33m:07s remains)
INFO - root - 2017-12-05 08:01:33.926485: step 1010, loss = 1.65, batch loss = 1.59 (37.0 examples/sec; 0.216 sec/batch; 19h:54m:23s remains)
INFO - root - 2017-12-05 08:01:36.076816: step 1020, loss = 1.55, batch loss = 1.49 (38.2 examples/sec; 0.209 sec/batch; 19h:15m:30s remains)
INFO - root - 2017-12-05 08:01:38.248361: step 1030, loss = 1.64, batch loss = 1.58 (37.0 examples/sec; 0.216 sec/batch; 19h:54m:31s remains)
INFO - root - 2017-12-05 08:01:40.429079: step 1040, loss = 1.60, batch loss = 1.55 (36.4 examples/sec; 0.220 sec/batch; 20h:13m:35s remains)
INFO - root - 2017-12-05 08:01:42.598905: step 1050, loss = 1.81, batch loss = 1.76 (36.5 examples/sec; 0.219 sec/batch; 20h:11m:25s remains)
INFO - root - 2017-12-05 08:01:44.775043: step 1060, loss = 1.55, batch loss = 1.49 (37.0 examples/sec; 0.216 sec/batch; 19h:54m:38s remains)
INFO - root - 2017-12-05 08:01:46.953464: step 1070, loss = 1.61, batch loss = 1.55 (36.2 examples/sec; 0.221 sec/batch; 20h:19m:30s remains)
INFO - root - 2017-12-05 08:01:49.102247: step 1080, loss = 1.44, batch loss = 1.38 (37.0 examples/sec; 0.216 sec/batch; 19h:55m:15s remains)
INFO - root - 2017-12-05 08:01:51.300370: step 1090, loss = 1.22, batch loss = 1.16 (35.5 examples/sec; 0.225 sec/batch; 20h:44m:16s remains)
INFO - root - 2017-12-05 08:01:53.444917: step 1100, loss = 1.54, batch loss = 1.48 (36.8 examples/sec; 0.217 sec/batch; 19h:59m:58s remains)
INFO - root - 2017-12-05 08:01:55.738350: step 1110, loss = 1.17, batch loss = 1.11 (36.9 examples/sec; 0.217 sec/batch; 19h:57m:15s remains)
INFO - root - 2017-12-05 08:01:57.913613: step 1120, loss = 1.44, batch loss = 1.38 (36.2 examples/sec; 0.221 sec/batch; 20h:21m:08s remains)
INFO - root - 2017-12-05 08:02:00.095489: step 1130, loss = 1.34, batch loss = 1.28 (36.8 examples/sec; 0.217 sec/batch; 19h:59m:31s remains)
INFO - root - 2017-12-05 08:02:02.284541: step 1140, loss = 1.44, batch loss = 1.38 (36.6 examples/sec; 0.218 sec/batch; 20h:05m:35s remains)
INFO - root - 2017-12-05 08:02:04.454517: step 1150, loss = 2.05, batch loss = 2.00 (37.1 examples/sec; 0.216 sec/batch; 19h:52m:10s remains)
INFO - root - 2017-12-05 08:02:06.648865: step 1160, loss = 1.54, batch loss = 1.48 (35.6 examples/sec; 0.224 sec/batch; 20h:39m:40s remains)
INFO - root - 2017-12-05 08:02:08.829382: step 1170, loss = 1.59, batch loss = 1.53 (36.9 examples/sec; 0.217 sec/batch; 19h:58m:15s remains)
INFO - root - 2017-12-05 08:02:11.035443: step 1180, loss = 1.55, batch loss = 1.49 (37.2 examples/sec; 0.215 sec/batch; 19h:46m:51s remains)
INFO - root - 2017-12-05 08:02:13.216364: step 1190, loss = 1.80, batch loss = 1.74 (37.4 examples/sec; 0.214 sec/batch; 19h:41m:25s remains)
INFO - root - 2017-12-05 08:02:15.440371: step 1200, loss = 1.90, batch loss = 1.84 (35.6 examples/sec; 0.225 sec/batch; 20h:40m:10s remains)
INFO - root - 2017-12-05 08:02:17.715893: step 1210, loss = 1.24, batch loss = 1.18 (37.1 examples/sec; 0.215 sec/batch; 19h:49m:39s remains)
INFO - root - 2017-12-05 08:02:19.895622: step 1220, loss = 1.90, batch loss = 1.84 (36.9 examples/sec; 0.217 sec/batch; 19h:57m:17s remains)
INFO - root - 2017-12-05 08:02:22.078337: step 1230, loss = 1.81, batch loss = 1.75 (37.4 examples/sec; 0.214 sec/batch; 19h:40m:29s remains)
INFO - root - 2017-12-05 08:02:24.273739: step 1240, loss = 1.94, batch loss = 1.88 (36.4 examples/sec; 0.220 sec/batch; 20h:13m:00s remains)
INFO - root - 2017-12-05 08:02:26.495774: step 1250, loss = 1.81, batch loss = 1.75 (37.6 examples/sec; 0.213 sec/batch; 19h:34m:46s remains)
INFO - root - 2017-12-05 08:02:28.684371: step 1260, loss = 1.37, batch loss = 1.31 (34.8 examples/sec; 0.230 sec/batch; 21h:07m:37s remains)
INFO - root - 2017-12-05 08:02:30.870023: step 1270, loss = 1.48, batch loss = 1.42 (36.5 examples/sec; 0.219 sec/batch; 20h:08m:30s remains)
INFO - root - 2017-12-05 08:02:33.083443: step 1280, loss = 1.46, batch loss = 1.40 (35.0 examples/sec; 0.228 sec/batch; 21h:00m:23s remains)
INFO - root - 2017-12-05 08:02:35.270918: step 1290, loss = 2.07, batch loss = 2.01 (36.7 examples/sec; 0.218 sec/batch; 20h:01m:59s remains)
INFO - root - 2017-12-05 08:02:37.438245: step 1300, loss = 1.52, batch loss = 1.46 (36.3 examples/sec; 0.220 sec/batch; 20h:15m:37s remains)
INFO - root - 2017-12-05 08:02:39.719962: step 1310, loss = 1.84, batch loss = 1.78 (37.4 examples/sec; 0.214 sec/batch; 19h:40m:14s remains)
INFO - root - 2017-12-05 08:02:41.923020: step 1320, loss = 1.27, batch loss = 1.22 (36.5 examples/sec; 0.219 sec/batch; 20h:09m:34s remains)
INFO - root - 2017-12-05 08:02:44.120162: step 1330, loss = 2.03, batch loss = 1.97 (36.3 examples/sec; 0.220 sec/batch; 20h:15m:50s remains)
INFO - root - 2017-12-05 08:02:46.283890: step 1340, loss = 1.50, batch loss = 1.44 (37.0 examples/sec; 0.217 sec/batch; 19h:54m:56s remains)
INFO - root - 2017-12-05 08:02:48.455098: step 1350, loss = 1.99, batch loss = 1.93 (36.4 examples/sec; 0.220 sec/batch; 20h:13m:08s remains)
INFO - root - 2017-12-05 08:02:50.641052: step 1360, loss = 1.68, batch loss = 1.62 (36.9 examples/sec; 0.217 sec/batch; 19h:55m:43s remains)
INFO - root - 2017-12-05 08:02:52.814402: step 1370, loss = 1.79, batch loss = 1.74 (36.1 examples/sec; 0.222 sec/batch; 20h:22m:35s remains)
INFO - root - 2017-12-05 08:02:55.015839: step 1380, loss = 1.76, batch loss = 1.70 (35.2 examples/sec; 0.227 sec/batch; 20h:55m:08s remains)
INFO - root - 2017-12-05 08:02:57.174937: step 1390, loss = 1.31, batch loss = 1.25 (37.3 examples/sec; 0.214 sec/batch; 19h:42m:49s remains)
INFO - root - 2017-12-05 08:02:59.355156: step 1400, loss = 2.11, batch loss = 2.05 (37.2 examples/sec; 0.215 sec/batch; 19h:47m:09s remains)
INFO - root - 2017-12-05 08:03:01.599684: step 1410, loss = 1.64, batch loss = 1.58 (36.3 examples/sec; 0.221 sec/batch; 20h:16m:57s remains)
INFO - root - 2017-12-05 08:03:03.807941: step 1420, loss = 1.70, batch loss = 1.64 (36.9 examples/sec; 0.217 sec/batch; 19h:55m:17s remains)
INFO - root - 2017-12-05 08:03:05.978693: step 1430, loss = 1.63, batch loss = 1.57 (37.1 examples/sec; 0.216 sec/batch; 19h:49m:09s remains)
INFO - root - 2017-12-05 08:03:08.198804: step 1440, loss = 1.40, batch loss = 1.34 (37.5 examples/sec; 0.213 sec/batch; 19h:37m:32s remains)
INFO - root - 2017-12-05 08:03:10.398195: step 1450, loss = 2.10, batch loss = 2.04 (36.0 examples/sec; 0.223 sec/batch; 20h:27m:39s remains)
INFO - root - 2017-12-05 08:03:12.566443: step 1460, loss = 1.66, batch loss = 1.60 (37.2 examples/sec; 0.215 sec/batch; 19h:45m:27s remains)
INFO - root - 2017-12-05 08:03:14.757800: step 1470, loss = 1.61, batch loss = 1.56 (36.3 examples/sec; 0.220 sec/batch; 20h:14m:22s remains)
INFO - root - 2017-12-05 08:03:16.929227: step 1480, loss = 1.46, batch loss = 1.40 (36.5 examples/sec; 0.219 sec/batch; 20h:08m:30s remains)
INFO - root - 2017-12-05 08:03:19.103925: step 1490, loss = 1.55, batch loss = 1.49 (36.3 examples/sec; 0.220 sec/batch; 20h:15m:49s remains)
INFO - root - 2017-12-05 08:03:21.280777: step 1500, loss = 1.42, batch loss = 1.36 (36.2 examples/sec; 0.221 sec/batch; 20h:18m:17s remains)
INFO - root - 2017-12-05 08:03:23.513782: step 1510, loss = 1.44, batch loss = 1.38 (37.6 examples/sec; 0.213 sec/batch; 19h:34m:08s remains)
INFO - root - 2017-12-05 08:03:25.694936: step 1520, loss = 1.64, batch loss = 1.58 (36.9 examples/sec; 0.217 sec/batch; 19h:55m:37s remains)
INFO - root - 2017-12-05 08:03:27.887879: step 1530, loss = 1.63, batch loss = 1.57 (36.1 examples/sec; 0.222 sec/batch; 20h:23m:36s remains)
INFO - root - 2017-12-05 08:03:30.069628: step 1540, loss = 1.65, batch loss = 1.59 (36.9 examples/sec; 0.217 sec/batch; 19h:57m:10s remains)
INFO - root - 2017-12-05 08:03:32.225245: step 1550, loss = 1.43, batch loss = 1.37 (36.9 examples/sec; 0.217 sec/batch; 19h:57m:06s remains)
INFO - root - 2017-12-05 08:03:34.391827: step 1560, loss = 1.74, batch loss = 1.69 (37.5 examples/sec; 0.213 sec/batch; 19h:37m:15s remains)
INFO - root - 2017-12-05 08:03:36.576815: step 1570, loss = 1.41, batch loss = 1.36 (36.9 examples/sec; 0.217 sec/batch; 19h:56m:31s remains)
INFO - root - 2017-12-05 08:03:38.771586: step 1580, loss = 1.31, batch loss = 1.25 (33.7 examples/sec; 0.237 sec/batch; 21h:47m:53s remains)
INFO - root - 2017-12-05 08:03:40.964074: step 1590, loss = 1.65, batch loss = 1.60 (37.2 examples/sec; 0.215 sec/batch; 19h:47m:05s remains)
INFO - root - 2017-12-05 08:03:43.141827: step 1600, loss = 1.32, batch loss = 1.26 (38.0 examples/sec; 0.210 sec/batch; 19h:19m:48s remains)
INFO - root - 2017-12-05 08:03:45.379249: step 1610, loss = 1.54, batch loss = 1.48 (36.8 examples/sec; 0.217 sec/batch; 19h:58m:17s remains)
INFO - root - 2017-12-05 08:03:47.547761: step 1620, loss = 1.29, batch loss = 1.23 (37.9 examples/sec; 0.211 sec/batch; 19h:22m:54s remains)
INFO - root - 2017-12-05 08:03:49.731613: step 1630, loss = 1.56, batch loss = 1.51 (37.2 examples/sec; 0.215 sec/batch; 19h:44m:43s remains)
INFO - root - 2017-12-05 08:03:51.942104: step 1640, loss = 1.87, batch loss = 1.81 (36.6 examples/sec; 0.219 sec/batch; 20h:06m:38s remains)
INFO - root - 2017-12-05 08:03:54.118927: step 1650, loss = 1.73, batch loss = 1.67 (35.2 examples/sec; 0.228 sec/batch; 20h:54m:47s remains)
INFO - root - 2017-12-05 08:03:56.341359: step 1660, loss = 1.57, batch loss = 1.51 (34.6 examples/sec; 0.231 sec/batch; 21h:13m:09s remains)
INFO - root - 2017-12-05 08:03:58.507596: step 1670, loss = 1.70, batch loss = 1.64 (36.6 examples/sec; 0.219 sec/batch; 20h:05m:12s remains)
INFO - root - 2017-12-05 08:04:00.677250: step 1680, loss = 1.52, batch loss = 1.46 (37.1 examples/sec; 0.216 sec/batch; 19h:50m:20s remains)
INFO - root - 2017-12-05 08:04:02.892862: step 1690, loss = 1.69, batch loss = 1.63 (37.5 examples/sec; 0.213 sec/batch; 19h:34m:57s remains)
INFO - root - 2017-12-05 08:04:05.100728: step 1700, loss = 1.78, batch loss = 1.72 (37.0 examples/sec; 0.216 sec/batch; 19h:51m:28s remains)
INFO - root - 2017-12-05 08:04:07.309210: step 1710, loss = 1.58, batch loss = 1.52 (37.6 examples/sec; 0.213 sec/batch; 19h:33m:33s remains)
INFO - root - 2017-12-05 08:04:09.515119: step 1720, loss = 2.12, batch loss = 2.07 (36.1 examples/sec; 0.221 sec/batch; 20h:20m:34s remains)
INFO - root - 2017-12-05 08:04:11.690848: step 1730, loss = 1.48, batch loss = 1.42 (37.4 examples/sec; 0.214 sec/batch; 19h:38m:21s remains)
INFO - root - 2017-12-05 08:04:13.844185: step 1740, loss = 1.41, batch loss = 1.35 (37.1 examples/sec; 0.215 sec/batch; 19h:47m:17s remains)
INFO - root - 2017-12-05 08:04:16.040857: step 1750, loss = 1.86, batch loss = 1.80 (36.7 examples/sec; 0.218 sec/batch; 20h:02m:30s remains)
INFO - root - 2017-12-05 08:04:18.237836: step 1760, loss = 1.86, batch loss = 1.80 (36.7 examples/sec; 0.218 sec/batch; 20h:01m:35s remains)
INFO - root - 2017-12-05 08:04:20.428070: step 1770, loss = 1.74, batch loss = 1.68 (35.7 examples/sec; 0.224 sec/batch; 20h:35m:12s remains)
INFO - root - 2017-12-05 08:04:22.625810: step 1780, loss = 1.80, batch loss = 1.74 (36.4 examples/sec; 0.220 sec/batch; 20h:13m:02s remains)
INFO - root - 2017-12-05 08:04:24.808526: step 1790, loss = 1.72, batch loss = 1.66 (37.6 examples/sec; 0.213 sec/batch; 19h:33m:09s remains)
INFO - root - 2017-12-05 08:04:27.004001: step 1800, loss = 1.84, batch loss = 1.78 (36.8 examples/sec; 0.218 sec/batch; 19h:59m:23s remains)
INFO - root - 2017-12-05 08:04:29.250162: step 1810, loss = 1.42, batch loss = 1.36 (35.5 examples/sec; 0.225 sec/batch; 20h:41m:06s remains)
INFO - root - 2017-12-05 08:04:31.426056: step 1820, loss = 1.80, batch loss = 1.74 (38.1 examples/sec; 0.210 sec/batch; 19h:18m:06s remains)
INFO - root - 2017-12-05 08:04:33.570457: step 1830, loss = 1.48, batch loss = 1.42 (36.7 examples/sec; 0.218 sec/batch; 20h:02m:27s remains)
INFO - root - 2017-12-05 08:04:35.742433: step 1840, loss = 1.51, batch loss = 1.46 (36.5 examples/sec; 0.219 sec/batch; 20h:06m:54s remains)
INFO - root - 2017-12-05 08:04:37.914481: step 1850, loss = 2.40, batch loss = 2.34 (37.2 examples/sec; 0.215 sec/batch; 19h:45m:52s remains)
INFO - root - 2017-12-05 08:04:40.123809: step 1860, loss = 1.57, batch loss = 1.51 (36.5 examples/sec; 0.219 sec/batch; 20h:08m:18s remains)
INFO - root - 2017-12-05 08:04:42.302057: step 1870, loss = 1.63, batch loss = 1.57 (37.8 examples/sec; 0.212 sec/batch; 19h:27m:10s remains)
INFO - root - 2017-12-05 08:04:44.476812: step 1880, loss = 1.51, batch loss = 1.45 (37.6 examples/sec; 0.213 sec/batch; 19h:33m:46s remains)
INFO - root - 2017-12-05 08:04:46.662510: step 1890, loss = 2.03, batch loss = 1.97 (37.0 examples/sec; 0.216 sec/batch; 19h:51m:13s remains)
INFO - root - 2017-12-05 08:04:48.832461: step 1900, loss = 1.75, batch loss = 1.69 (36.3 examples/sec; 0.221 sec/batch; 20h:15m:57s remains)
INFO - root - 2017-12-05 08:04:51.064923: step 1910, loss = 1.70, batch loss = 1.64 (37.1 examples/sec; 0.216 sec/batch; 19h:49m:25s remains)
INFO - root - 2017-12-05 08:04:53.229578: step 1920, loss = 1.88, batch loss = 1.83 (36.7 examples/sec; 0.218 sec/batch; 20h:00m:37s remains)
INFO - root - 2017-12-05 08:04:55.417373: step 1930, loss = 1.62, batch loss = 1.56 (37.0 examples/sec; 0.216 sec/batch; 19h:51m:43s remains)
INFO - root - 2017-12-05 08:04:57.584321: step 1940, loss = 1.38, batch loss = 1.33 (37.0 examples/sec; 0.216 sec/batch; 19h:49m:40s remains)
INFO - root - 2017-12-05 08:04:59.799908: step 1950, loss = 1.75, batch loss = 1.69 (36.8 examples/sec; 0.218 sec/batch; 19h:58m:56s remains)
INFO - root - 2017-12-05 08:05:02.015331: step 1960, loss = 1.60, batch loss = 1.54 (37.3 examples/sec; 0.215 sec/batch; 19h:42m:42s remains)
INFO - root - 2017-12-05 08:05:04.167569: step 1970, loss = 1.72, batch loss = 1.66 (37.6 examples/sec; 0.213 sec/batch; 19h:32m:15s remains)
INFO - root - 2017-12-05 08:05:06.355695: step 1980, loss = 1.53, batch loss = 1.47 (36.7 examples/sec; 0.218 sec/batch; 20h:02m:19s remains)
INFO - root - 2017-12-05 08:05:08.542870: step 1990, loss = 1.74, batch loss = 1.69 (36.2 examples/sec; 0.221 sec/batch; 20h:17m:23s remains)
INFO - root - 2017-12-05 08:05:10.722147: step 2000, loss = 1.65, batch loss = 1.59 (36.5 examples/sec; 0.219 sec/batch; 20h:06m:05s remains)
INFO - root - 2017-12-05 08:05:12.989814: step 2010, loss = 2.17, batch loss = 2.11 (35.5 examples/sec; 0.225 sec/batch; 20h:39m:44s remains)
INFO - root - 2017-12-05 08:05:15.159969: step 2020, loss = 2.15, batch loss = 2.09 (36.7 examples/sec; 0.218 sec/batch; 19h:59m:18s remains)
INFO - root - 2017-12-05 08:05:17.343638: step 2030, loss = 1.69, batch loss = 1.64 (36.1 examples/sec; 0.222 sec/batch; 20h:22m:03s remains)
INFO - root - 2017-12-05 08:05:19.525061: step 2040, loss = 1.63, batch loss = 1.57 (35.9 examples/sec; 0.223 sec/batch; 20h:26m:50s remains)
INFO - root - 2017-12-05 08:05:21.727273: step 2050, loss = 2.26, batch loss = 2.20 (36.1 examples/sec; 0.222 sec/batch; 20h:20m:29s remains)
INFO - root - 2017-12-05 08:05:23.919296: step 2060, loss = 1.65, batch loss = 1.59 (35.9 examples/sec; 0.223 sec/batch; 20h:26m:50s remains)
INFO - root - 2017-12-05 08:05:26.134176: step 2070, loss = 1.48, batch loss = 1.42 (37.5 examples/sec; 0.213 sec/batch; 19h:34m:13s remains)
INFO - root - 2017-12-05 08:05:28.277489: step 2080, loss = 1.65, batch loss = 1.59 (36.8 examples/sec; 0.218 sec/batch; 19h:58m:22s remains)
INFO - root - 2017-12-05 08:05:30.443378: step 2090, loss = 1.66, batch loss = 1.60 (37.3 examples/sec; 0.214 sec/batch; 19h:39m:51s remains)
INFO - root - 2017-12-05 08:05:32.596472: step 2100, loss = 1.54, batch loss = 1.48 (36.8 examples/sec; 0.218 sec/batch; 19h:58m:11s remains)
INFO - root - 2017-12-05 08:05:34.874497: step 2110, loss = 1.68, batch loss = 1.63 (35.2 examples/sec; 0.227 sec/batch; 20h:50m:19s remains)
INFO - root - 2017-12-05 08:05:37.050461: step 2120, loss = 1.58, batch loss = 1.52 (36.7 examples/sec; 0.218 sec/batch; 20h:00m:31s remains)
INFO - root - 2017-12-05 08:05:39.224812: step 2130, loss = 1.88, batch loss = 1.82 (38.1 examples/sec; 0.210 sec/batch; 19h:14m:58s remains)
INFO - root - 2017-12-05 08:05:41.398010: step 2140, loss = 1.73, batch loss = 1.68 (36.1 examples/sec; 0.222 sec/batch; 20h:20m:15s remains)
INFO - root - 2017-12-05 08:05:43.569318: step 2150, loss = 1.81, batch loss = 1.76 (37.0 examples/sec; 0.216 sec/batch; 19h:49m:41s remains)
INFO - root - 2017-12-05 08:05:45.756146: step 2160, loss = 1.57, batch loss = 1.52 (36.3 examples/sec; 0.220 sec/batch; 20h:13m:29s remains)
INFO - root - 2017-12-05 08:05:47.926129: step 2170, loss = 1.71, batch loss = 1.65 (36.4 examples/sec; 0.220 sec/batch; 20h:11m:39s remains)
INFO - root - 2017-12-05 08:05:50.101373: step 2180, loss = 1.61, batch loss = 1.55 (37.2 examples/sec; 0.215 sec/batch; 19h:44m:22s remains)
INFO - root - 2017-12-05 08:05:52.288463: step 2190, loss = 1.72, batch loss = 1.66 (36.7 examples/sec; 0.218 sec/batch; 20h:01m:04s remains)
INFO - root - 2017-12-05 08:05:54.465452: step 2200, loss = 1.75, batch loss = 1.70 (36.8 examples/sec; 0.217 sec/batch; 19h:55m:41s remains)
INFO - root - 2017-12-05 08:05:56.732721: step 2210, loss = 1.51, batch loss = 1.45 (35.9 examples/sec; 0.223 sec/batch; 20h:27m:40s remains)
INFO - root - 2017-12-05 08:05:58.924998: step 2220, loss = 1.57, batch loss = 1.51 (36.9 examples/sec; 0.217 sec/batch; 19h:53m:15s remains)
INFO - root - 2017-12-05 08:06:01.100862: step 2230, loss = 1.67, batch loss = 1.62 (33.9 examples/sec; 0.236 sec/batch; 21h:40m:01s remains)
INFO - root - 2017-12-05 08:06:03.298657: step 2240, loss = 1.71, batch loss = 1.65 (35.0 examples/sec; 0.229 sec/batch; 20h:57m:46s remains)
INFO - root - 2017-12-05 08:06:05.536651: step 2250, loss = 1.51, batch loss = 1.45 (37.7 examples/sec; 0.212 sec/batch; 19h:28m:12s remains)
INFO - root - 2017-12-05 08:06:07.722147: step 2260, loss = 1.46, batch loss = 1.40 (36.4 examples/sec; 0.220 sec/batch; 20h:10m:49s remains)
INFO - root - 2017-12-05 08:06:09.925124: step 2270, loss = 1.97, batch loss = 1.91 (36.0 examples/sec; 0.222 sec/batch; 20h:23m:04s remains)
INFO - root - 2017-12-05 08:06:12.122234: step 2280, loss = 2.00, batch loss = 1.94 (35.0 examples/sec; 0.229 sec/batch; 20h:59m:06s remains)
INFO - root - 2017-12-05 08:06:14.288279: step 2290, loss = 1.91, batch loss = 1.85 (37.0 examples/sec; 0.216 sec/batch; 19h:50m:56s remains)
INFO - root - 2017-12-05 08:06:16.459680: step 2300, loss = 1.92, batch loss = 1.86 (37.2 examples/sec; 0.215 sec/batch; 19h:42m:36s remains)
INFO - root - 2017-12-05 08:06:18.755776: step 2310, loss = 1.81, batch loss = 1.75 (37.7 examples/sec; 0.212 sec/batch; 19h:26m:52s remains)
INFO - root - 2017-12-05 08:06:20.961206: step 2320, loss = 1.42, batch loss = 1.36 (34.9 examples/sec; 0.229 sec/batch; 21h:00m:06s remains)
INFO - root - 2017-12-05 08:06:23.140134: step 2330, loss = 1.66, batch loss = 1.60 (36.8 examples/sec; 0.217 sec/batch; 19h:56m:41s remains)
INFO - root - 2017-12-05 08:06:25.362039: step 2340, loss = 1.95, batch loss = 1.89 (35.5 examples/sec; 0.225 sec/batch; 20h:39m:11s remains)
INFO - root - 2017-12-05 08:06:27.534574: step 2350, loss = 1.48, batch loss = 1.42 (35.6 examples/sec; 0.225 sec/batch; 20h:36m:50s remains)
INFO - root - 2017-12-05 08:06:29.715288: step 2360, loss = 1.84, batch loss = 1.79 (37.4 examples/sec; 0.214 sec/batch; 19h:38m:21s remains)
INFO - root - 2017-12-05 08:06:31.892862: step 2370, loss = 1.78, batch loss = 1.72 (35.4 examples/sec; 0.226 sec/batch; 20h:43m:26s remains)
INFO - root - 2017-12-05 08:06:34.059022: step 2380, loss = 1.91, batch loss = 1.85 (37.3 examples/sec; 0.215 sec/batch; 19h:41m:23s remains)
INFO - root - 2017-12-05 08:06:36.223246: step 2390, loss = 2.20, batch loss = 2.14 (37.9 examples/sec; 0.211 sec/batch; 19h:19m:55s remains)
INFO - root - 2017-12-05 08:06:38.421360: step 2400, loss = 1.59, batch loss = 1.53 (36.5 examples/sec; 0.219 sec/batch; 20h:04m:32s remains)
INFO - root - 2017-12-05 08:06:40.687501: step 2410, loss = 1.89, batch loss = 1.84 (36.0 examples/sec; 0.222 sec/batch; 20h:21m:55s remains)
INFO - root - 2017-12-05 08:06:42.873877: step 2420, loss = 1.55, batch loss = 1.49 (36.5 examples/sec; 0.219 sec/batch; 20h:06m:42s remains)
INFO - root - 2017-12-05 08:06:45.040733: step 2430, loss = 1.77, batch loss = 1.71 (36.7 examples/sec; 0.218 sec/batch; 19h:59m:22s remains)
INFO - root - 2017-12-05 08:06:47.222116: step 2440, loss = 1.66, batch loss = 1.61 (36.4 examples/sec; 0.220 sec/batch; 20h:09m:52s remains)
INFO - root - 2017-12-05 08:06:49.397463: step 2450, loss = 1.53, batch loss = 1.48 (36.6 examples/sec; 0.219 sec/batch; 20h:03m:46s remains)
INFO - root - 2017-12-05 08:06:51.550525: step 2460, loss = 1.75, batch loss = 1.69 (37.1 examples/sec; 0.215 sec/batch; 19h:44m:51s remains)
INFO - root - 2017-12-05 08:06:53.694871: step 2470, loss = 1.93, batch loss = 1.87 (36.9 examples/sec; 0.217 sec/batch; 19h:52m:43s remains)
INFO - root - 2017-12-05 08:06:55.866931: step 2480, loss = 1.59, batch loss = 1.53 (36.0 examples/sec; 0.223 sec/batch; 20h:23m:51s remains)
INFO - root - 2017-12-05 08:06:58.047312: step 2490, loss = 1.62, batch loss = 1.56 (37.1 examples/sec; 0.215 sec/batch; 19h:45m:15s remains)
INFO - root - 2017-12-05 08:07:00.204772: step 2500, loss = 1.61, batch loss = 1.55 (37.8 examples/sec; 0.212 sec/batch; 19h:23m:56s remains)
INFO - root - 2017-12-05 08:07:02.458483: step 2510, loss = 1.76, batch loss = 1.70 (36.8 examples/sec; 0.217 sec/batch; 19h:54m:24s remains)
INFO - root - 2017-12-05 08:07:04.648918: step 2520, loss = 1.74, batch loss = 1.68 (36.8 examples/sec; 0.218 sec/batch; 19h:57m:02s remains)
INFO - root - 2017-12-05 08:07:06.823832: step 2530, loss = 1.71, batch loss = 1.65 (36.5 examples/sec; 0.219 sec/batch; 20h:06m:46s remains)
INFO - root - 2017-12-05 08:07:09.004072: step 2540, loss = 1.57, batch loss = 1.51 (35.7 examples/sec; 0.224 sec/batch; 20h:33m:24s remains)
INFO - root - 2017-12-05 08:07:11.191351: step 2550, loss = 1.68, batch loss = 1.62 (36.6 examples/sec; 0.219 sec/batch; 20h:02m:47s remains)
INFO - root - 2017-12-05 08:07:13.354742: step 2560, loss = 1.61, batch loss = 1.55 (36.4 examples/sec; 0.220 sec/batch; 20h:08m:53s remains)
INFO - root - 2017-12-05 08:07:15.601935: step 2570, loss = 1.62, batch loss = 1.57 (36.9 examples/sec; 0.217 sec/batch; 19h:51m:06s remains)
INFO - root - 2017-12-05 08:07:17.805039: step 2580, loss = 1.78, batch loss = 1.72 (35.6 examples/sec; 0.225 sec/batch; 20h:35m:37s remains)
INFO - root - 2017-12-05 08:07:19.964817: step 2590, loss = 1.83, batch loss = 1.78 (37.4 examples/sec; 0.214 sec/batch; 19h:35m:47s remains)
INFO - root - 2017-12-05 08:07:22.136407: step 2600, loss = 2.06, batch loss = 2.00 (37.0 examples/sec; 0.216 sec/batch; 19h:49m:17s remains)
INFO - root - 2017-12-05 08:07:24.394905: step 2610, loss = 1.48, batch loss = 1.42 (36.2 examples/sec; 0.221 sec/batch; 20h:13m:46s remains)
INFO - root - 2017-12-05 08:07:26.603379: step 2620, loss = 1.86, batch loss = 1.80 (35.6 examples/sec; 0.225 sec/batch; 20h:34m:44s remains)
INFO - root - 2017-12-05 08:07:28.775095: step 2630, loss = 1.91, batch loss = 1.85 (36.9 examples/sec; 0.217 sec/batch; 19h:52m:06s remains)
INFO - root - 2017-12-05 08:07:30.959499: step 2640, loss = 1.70, batch loss = 1.64 (36.3 examples/sec; 0.221 sec/batch; 20h:12m:26s remains)
INFO - root - 2017-12-05 08:07:33.136372: step 2650, loss = 1.77, batch loss = 1.72 (37.2 examples/sec; 0.215 sec/batch; 19h:43m:03s remains)
INFO - root - 2017-12-05 08:07:35.357174: step 2660, loss = 1.45, batch loss = 1.40 (36.8 examples/sec; 0.217 sec/batch; 19h:53m:31s remains)
INFO - root - 2017-12-05 08:07:37.577267: step 2670, loss = 1.95, batch loss = 1.89 (35.6 examples/sec; 0.225 sec/batch; 20h:34m:47s remains)
INFO - root - 2017-12-05 08:07:39.782222: step 2680, loss = 1.99, batch loss = 1.93 (35.8 examples/sec; 0.224 sec/batch; 20h:29m:30s remains)
INFO - root - 2017-12-05 08:07:41.966323: step 2690, loss = 1.51, batch loss = 1.45 (36.2 examples/sec; 0.221 sec/batch; 20h:15m:35s remains)
INFO - root - 2017-12-05 08:07:44.159440: step 2700, loss = 1.70, batch loss = 1.65 (35.8 examples/sec; 0.224 sec/batch; 20h:29m:40s remains)
INFO - root - 2017-12-05 08:07:46.421650: step 2710, loss = 1.51, batch loss = 1.45 (37.0 examples/sec; 0.216 sec/batch; 19h:49m:25s remains)
INFO - root - 2017-12-05 08:07:48.622261: step 2720, loss = 1.57, batch loss = 1.51 (35.2 examples/sec; 0.227 sec/batch; 20h:49m:37s remains)
INFO - root - 2017-12-05 08:07:50.775903: step 2730, loss = 1.96, batch loss = 1.90 (37.0 examples/sec; 0.216 sec/batch; 19h:49m:35s remains)
INFO - root - 2017-12-05 08:07:52.975231: step 2740, loss = 1.86, batch loss = 1.80 (35.5 examples/sec; 0.225 sec/batch; 20h:37m:54s remains)
INFO - root - 2017-12-05 08:07:55.177043: step 2750, loss = 1.75, batch loss = 1.70 (36.5 examples/sec; 0.219 sec/batch; 20h:04m:44s remains)
INFO - root - 2017-12-05 08:07:57.386211: step 2760, loss = 1.59, batch loss = 1.53 (35.7 examples/sec; 0.224 sec/batch; 20h:33m:04s remains)
INFO - root - 2017-12-05 08:07:59.602239: step 2770, loss = 1.51, batch loss = 1.45 (36.1 examples/sec; 0.222 sec/batch; 20h:17m:27s remains)
INFO - root - 2017-12-05 08:08:01.779901: step 2780, loss = 1.82, batch loss = 1.76 (37.8 examples/sec; 0.212 sec/batch; 19h:23m:06s remains)
INFO - root - 2017-12-05 08:08:03.983256: step 2790, loss = 1.51, batch loss = 1.45 (35.6 examples/sec; 0.225 sec/batch; 20h:35m:54s remains)
INFO - root - 2017-12-05 08:08:06.193414: step 2800, loss = 1.75, batch loss = 1.69 (38.1 examples/sec; 0.210 sec/batch; 19h:13m:29s remains)
INFO - root - 2017-12-05 08:08:08.455902: step 2810, loss = 1.98, batch loss = 1.92 (37.1 examples/sec; 0.215 sec/batch; 19h:43m:19s remains)
INFO - root - 2017-12-05 08:08:10.671850: step 2820, loss = 1.53, batch loss = 1.48 (36.4 examples/sec; 0.220 sec/batch; 20h:07m:34s remains)
INFO - root - 2017-12-05 08:08:12.848376: step 2830, loss = 1.68, batch loss = 1.62 (37.2 examples/sec; 0.215 sec/batch; 19h:40m:26s remains)
INFO - root - 2017-12-05 08:08:15.019942: step 2840, loss = 1.59, batch loss = 1.53 (36.5 examples/sec; 0.219 sec/batch; 20h:05m:51s remains)
INFO - root - 2017-12-05 08:08:17.212654: step 2850, loss = 1.85, batch loss = 1.79 (37.1 examples/sec; 0.216 sec/batch; 19h:44m:09s remains)
INFO - root - 2017-12-05 08:08:19.365658: step 2860, loss = 1.61, batch loss = 1.55 (36.1 examples/sec; 0.221 sec/batch; 20h:16m:07s remains)
INFO - root - 2017-12-05 08:08:21.551283: step 2870, loss = 1.79, batch loss = 1.73 (36.2 examples/sec; 0.221 sec/batch; 20h:13m:30s remains)
INFO - root - 2017-12-05 08:08:23.738664: step 2880, loss = 1.40, batch loss = 1.34 (35.6 examples/sec; 0.225 sec/batch; 20h:35m:20s remains)
INFO - root - 2017-12-05 08:08:25.915712: step 2890, loss = 1.57, batch loss = 1.51 (37.6 examples/sec; 0.213 sec/batch; 19h:27m:39s remains)
INFO - root - 2017-12-05 08:08:28.088298: step 2900, loss = 1.49, batch loss = 1.43 (37.3 examples/sec; 0.215 sec/batch; 19h:38m:27s remains)
INFO - root - 2017-12-05 08:08:30.332800: step 2910, loss = 1.73, batch loss = 1.67 (36.8 examples/sec; 0.217 sec/batch; 19h:54m:28s remains)
INFO - root - 2017-12-05 08:08:32.510007: step 2920, loss = 1.92, batch loss = 1.86 (37.2 examples/sec; 0.215 sec/batch; 19h:40m:29s remains)
INFO - root - 2017-12-05 08:08:34.709969: step 2930, loss = 1.87, batch loss = 1.81 (37.2 examples/sec; 0.215 sec/batch; 19h:40m:07s remains)
INFO - root - 2017-12-05 08:08:36.891292: step 2940, loss = 1.75, batch loss = 1.69 (37.4 examples/sec; 0.214 sec/batch; 19h:33m:33s remains)
INFO - root - 2017-12-05 08:08:39.105537: step 2950, loss = 1.36, batch loss = 1.30 (37.6 examples/sec; 0.213 sec/batch; 19h:30m:00s remains)
INFO - root - 2017-12-05 08:08:41.284613: step 2960, loss = 1.65, batch loss = 1.59 (36.1 examples/sec; 0.222 sec/batch; 20h:17m:49s remains)
INFO - root - 2017-12-05 08:08:43.466001: step 2970, loss = 1.55, batch loss = 1.49 (36.7 examples/sec; 0.218 sec/batch; 19h:56m:07s remains)
INFO - root - 2017-12-05 08:08:45.646584: step 2980, loss = 1.57, batch loss = 1.52 (36.6 examples/sec; 0.218 sec/batch; 19h:59m:43s remains)
INFO - root - 2017-12-05 08:08:47.847839: step 2990, loss = 1.63, batch loss = 1.57 (36.1 examples/sec; 0.221 sec/batch; 20h:16m:03s remains)
INFO - root - 2017-12-05 08:08:50.037852: step 3000, loss = 1.68, batch loss = 1.62 (37.1 examples/sec; 0.216 sec/batch; 19h:45m:40s remains)
INFO - root - 2017-12-05 08:08:52.304974: step 3010, loss = 1.70, batch loss = 1.64 (36.0 examples/sec; 0.222 sec/batch; 20h:19m:40s remains)
INFO - root - 2017-12-05 08:08:54.548675: step 3020, loss = 1.71, batch loss = 1.65 (35.1 examples/sec; 0.228 sec/batch; 20h:52m:53s remains)
INFO - root - 2017-12-05 08:08:56.721537: step 3030, loss = 1.76, batch loss = 1.70 (37.6 examples/sec; 0.213 sec/batch; 19h:29m:41s remains)
INFO - root - 2017-12-05 08:08:58.893224: step 3040, loss = 1.43, batch loss = 1.38 (38.1 examples/sec; 0.210 sec/batch; 19h:11m:57s remains)
INFO - root - 2017-12-05 08:09:01.056844: step 3050, loss = 2.02, batch loss = 1.96 (37.3 examples/sec; 0.214 sec/batch; 19h:37m:28s remains)
INFO - root - 2017-12-05 08:09:03.235343: step 3060, loss = 2.13, batch loss = 2.07 (36.8 examples/sec; 0.217 sec/batch; 19h:53m:42s remains)
INFO - root - 2017-12-05 08:09:05.426313: step 3070, loss = 1.47, batch loss = 1.41 (37.6 examples/sec; 0.213 sec/batch; 19h:29m:07s remains)
INFO - root - 2017-12-05 08:09:07.611095: step 3080, loss = 1.41, batch loss = 1.35 (36.6 examples/sec; 0.218 sec/batch; 19h:58m:27s remains)
INFO - root - 2017-12-05 08:09:09.827241: step 3090, loss = 1.56, batch loss = 1.50 (35.6 examples/sec; 0.225 sec/batch; 20h:33m:04s remains)
INFO - root - 2017-12-05 08:09:11.975666: step 3100, loss = 1.44, batch loss = 1.38 (36.6 examples/sec; 0.218 sec/batch; 19h:58m:25s remains)
INFO - root - 2017-12-05 08:09:14.227896: step 3110, loss = 1.83, batch loss = 1.78 (37.6 examples/sec; 0.213 sec/batch; 19h:29m:01s remains)
INFO - root - 2017-12-05 08:09:16.401080: step 3120, loss = 1.86, batch loss = 1.80 (36.4 examples/sec; 0.220 sec/batch; 20h:08m:07s remains)
INFO - root - 2017-12-05 08:09:18.578288: step 3130, loss = 2.00, batch loss = 1.94 (36.1 examples/sec; 0.221 sec/batch; 20h:15m:14s remains)
