INFO - Siam-FC - Running command 'main'
INFO - Siam-FC - Started run with ID "49"
INFO - root - preproces -- siamese_fc_color
WARNING - root - root is not explicitly specified, using default value: None
INFO - root - For training, we use instance size 255 - 2 * 8 ...
WARNING - root - init_method is not explicitly specified, using default value: None
sdiufhasudf Tensor("siamese_fc/conv5/split:0", shape=(8, 8, 8, 192), dtype=float32)
Tensor("siamese_fc/conv5/def/transpose:0", shape=(8, 192, 8, 8), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/b1/weights:0' shape=(128, 192, 3, 3) dtype=float32_ref> Tensor("siamese_fc/conv5/def/transpose_1:0", shape=(8, 72, 8, 8), dtype=float32)
ddd Tensor("siamese_fc/conv5/def/b1/transpose:0", shape=(8, 6, 6, 128), dtype=float32)
Tensor("siamese_fc/conv5/def/transpose_2:0", shape=(8, 192, 8, 8), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/b2/weights:0' shape=(128, 192, 3, 3) dtype=float32_ref> Tensor("siamese_fc/conv5/def/transpose_3:0", shape=(8, 72, 8, 8), dtype=float32)
sdiufhasudf Tensor("siamese_fc_1/conv5/split:0", shape=(8, 22, 22, 192), dtype=float32)
Tensor("siamese_fc_1/conv5/def/transpose:0", shape=(8, 192, 22, 22), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/b1/weights:0' shape=(128, 192, 3, 3) dtype=float32_ref> Tensor("siamese_fc_1/conv5/def/transpose_1:0", shape=(8, 72, 22, 22), dtype=float32)
ddd Tensor("siamese_fc_1/conv5/def/b1/transpose:0", shape=(8, 20, 20, 128), dtype=float32)
Tensor("siamese_fc_1/conv5/def/transpose_2:0", shape=(8, 192, 22, 22), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/b2/weights:0' shape=(128, 192, 3, 3) dtype=float32_ref> Tensor("siamese_fc_1/conv5/def/transpose_3:0", shape=(8, 72, 22, 22), dtype=float32)
INFO - root - Model moving average is disabled since decay factor is 0
2017-12-05 06:28:35.868187: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-05 06:28:35.868227: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-05 06:28:35.868235: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2017-12-05 06:28:35.868239: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-05 06:28:35.868244: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
2017-12-05 06:28:36.436750: I tensorflow/core/common_runtime/gpu/gpu_device.cc:887] Found device 0 with properties: 
name: Tesla K80
major: 3 minor: 7 memoryClockRate (GHz) 0.8235
pciBusID 5791:00:00.0
Total memory: 11.17GiB
Free memory: 11.11GiB
2017-12-05 06:28:36.436790: I tensorflow/core/common_runtime/gpu/gpu_device.cc:908] DMA: 0 
2017-12-05 06:28:36.436798: I tensorflow/core/common_runtime/gpu/gpu_device.cc:918] 0:   Y 
2017-12-05 06:28:36.436806: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla K80, pci bus id: 5791:00:00.0)
INFO:tensorflow:Restoring parameters from /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-only/model.ckpt-150000
INFO - tensorflow - Restoring parameters from /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-only/model.ckpt-150000
INFO - root - Starting threads 0 ...

INFO - root - training for 332500 steps
INFO - root - 2017-12-05 06:28:39.923351: step 0, loss = 2.03, batch loss = 1.98 (3.2 examples/sec; 2.526 sec/batch; 233h:18m:40s remains)
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC/model.ckpt-0 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC/model.ckpt-0 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-05 06:28:42.887652: step 10, loss = 2.06, batch loss = 2.00 (35.0 examples/sec; 0.229 sec/batch; 21h:08m:16s remains)
INFO - root - 2017-12-05 06:28:44.974267: step 20, loss = 2.08, batch loss = 2.02 (40.1 examples/sec; 0.200 sec/batch; 18h:26m:30s remains)
INFO - root - 2017-12-05 06:28:47.072833: step 30, loss = 2.04, batch loss = 1.98 (39.0 examples/sec; 0.205 sec/batch; 18h:56m:12s remains)
INFO - root - 2017-12-05 06:28:49.178967: step 40, loss = 2.04, batch loss = 1.98 (39.1 examples/sec; 0.204 sec/batch; 18h:53m:01s remains)
INFO - root - 2017-12-05 06:28:51.266708: step 50, loss = 2.05, batch loss = 2.00 (39.6 examples/sec; 0.202 sec/batch; 18h:39m:46s remains)
INFO - root - 2017-12-05 06:28:53.396054: step 60, loss = 2.03, batch loss = 1.98 (38.2 examples/sec; 0.210 sec/batch; 19h:20m:54s remains)
INFO - root - 2017-12-05 06:28:55.502502: step 70, loss = 2.03, batch loss = 1.97 (40.3 examples/sec; 0.199 sec/batch; 18h:19m:59s remains)
INFO - root - 2017-12-05 06:28:57.578070: step 80, loss = 2.07, batch loss = 2.01 (35.4 examples/sec; 0.226 sec/batch; 20h:52m:20s remains)
INFO - root - 2017-12-05 06:28:59.665906: step 90, loss = 2.06, batch loss = 2.00 (38.8 examples/sec; 0.206 sec/batch; 19h:01m:50s remains)
INFO - root - 2017-12-05 06:29:01.699043: step 100, loss = 2.06, batch loss = 2.00 (39.7 examples/sec; 0.201 sec/batch; 18h:35m:57s remains)
INFO - root - 2017-12-05 06:29:03.844959: step 110, loss = 2.05, batch loss = 1.99 (39.0 examples/sec; 0.205 sec/batch; 18h:57m:02s remains)
INFO - root - 2017-12-05 06:29:05.872821: step 120, loss = 2.08, batch loss = 2.03 (40.2 examples/sec; 0.199 sec/batch; 18h:21m:36s remains)
INFO - root - 2017-12-05 06:29:07.940048: step 130, loss = 2.06, batch loss = 2.00 (38.0 examples/sec; 0.211 sec/batch; 19h:26m:57s remains)
INFO - root - 2017-12-05 06:29:10.012766: step 140, loss = 2.05, batch loss = 1.99 (39.3 examples/sec; 0.203 sec/batch; 18h:46m:22s remains)
INFO - root - 2017-12-05 06:29:12.047859: step 150, loss = 2.06, batch loss = 2.00 (38.6 examples/sec; 0.208 sec/batch; 19h:09m:23s remains)
INFO - root - 2017-12-05 06:29:14.085039: step 160, loss = 2.08, batch loss = 2.02 (38.0 examples/sec; 0.210 sec/batch; 19h:25m:13s remains)
INFO - root - 2017-12-05 06:29:16.125919: step 170, loss = 2.06, batch loss = 2.00 (40.2 examples/sec; 0.199 sec/batch; 18h:22m:40s remains)
INFO - root - 2017-12-05 06:29:18.189058: step 180, loss = 2.06, batch loss = 2.00 (39.0 examples/sec; 0.205 sec/batch; 18h:56m:48s remains)
INFO - root - 2017-12-05 06:29:20.237293: step 190, loss = 2.03, batch loss = 1.97 (39.0 examples/sec; 0.205 sec/batch; 18h:56m:28s remains)
INFO - root - 2017-12-05 06:29:22.316417: step 200, loss = 2.04, batch loss = 1.98 (39.1 examples/sec; 0.205 sec/batch; 18h:53m:04s remains)
INFO - root - 2017-12-05 06:29:24.442226: step 210, loss = 2.06, batch loss = 2.01 (39.4 examples/sec; 0.203 sec/batch; 18h:45m:05s remains)
INFO - root - 2017-12-05 06:29:26.471917: step 220, loss = 2.06, batch loss = 2.00 (39.8 examples/sec; 0.201 sec/batch; 18h:34m:15s remains)
INFO - root - 2017-12-05 06:29:28.511252: step 230, loss = 2.05, batch loss = 2.00 (40.1 examples/sec; 0.200 sec/batch; 18h:25m:54s remains)
INFO - root - 2017-12-05 06:29:30.542324: step 240, loss = 2.10, batch loss = 2.04 (39.2 examples/sec; 0.204 sec/batch; 18h:49m:11s remains)
INFO - root - 2017-12-05 06:29:32.580478: step 250, loss = 2.06, batch loss = 2.00 (39.9 examples/sec; 0.200 sec/batch; 18h:29m:24s remains)
INFO - root - 2017-12-05 06:29:34.641713: step 260, loss = 2.08, batch loss = 2.02 (39.1 examples/sec; 0.205 sec/batch; 18h:54m:12s remains)
INFO - root - 2017-12-05 06:29:36.678170: step 270, loss = 2.07, batch loss = 2.01 (38.5 examples/sec; 0.208 sec/batch; 19h:11m:31s remains)
INFO - root - 2017-12-05 06:29:38.729609: step 280, loss = 2.06, batch loss = 2.00 (39.5 examples/sec; 0.203 sec/batch; 18h:41m:25s remains)
INFO - root - 2017-12-05 06:29:40.772430: step 290, loss = 2.05, batch loss = 1.99 (38.7 examples/sec; 0.206 sec/batch; 19h:03m:15s remains)
INFO - root - 2017-12-05 06:29:42.817699: step 300, loss = 2.06, batch loss = 2.00 (39.2 examples/sec; 0.204 sec/batch; 18h:48m:44s remains)
INFO - root - 2017-12-05 06:29:44.946193: step 310, loss = 2.05, batch loss = 1.99 (40.5 examples/sec; 0.198 sec/batch; 18h:14m:52s remains)
INFO - root - 2017-12-05 06:29:47.020159: step 320, loss = 2.08, batch loss = 2.02 (39.9 examples/sec; 0.200 sec/batch; 18h:28m:40s remains)
INFO - root - 2017-12-05 06:29:49.100644: step 330, loss = 2.07, batch loss = 2.01 (39.9 examples/sec; 0.201 sec/batch; 18h:30m:16s remains)
INFO - root - 2017-12-05 06:29:51.146528: step 340, loss = 2.06, batch loss = 2.00 (38.0 examples/sec; 0.211 sec/batch; 19h:26m:58s remains)
INFO - root - 2017-12-05 06:29:53.177314: step 350, loss = 2.07, batch loss = 2.01 (38.9 examples/sec; 0.206 sec/batch; 18h:57m:48s remains)
INFO - root - 2017-12-05 06:29:55.212476: step 360, loss = 2.09, batch loss = 2.03 (39.7 examples/sec; 0.202 sec/batch; 18h:35m:53s remains)
INFO - root - 2017-12-05 06:29:57.254734: step 370, loss = 2.10, batch loss = 2.04 (39.4 examples/sec; 0.203 sec/batch; 18h:44m:55s remains)
INFO - root - 2017-12-05 06:29:59.286875: step 380, loss = 2.09, batch loss = 2.03 (39.1 examples/sec; 0.205 sec/batch; 18h:52m:35s remains)
INFO - root - 2017-12-05 06:30:01.325831: step 390, loss = 2.05, batch loss = 2.00 (38.1 examples/sec; 0.210 sec/batch; 19h:20m:56s remains)
INFO - root - 2017-12-05 06:30:03.402272: step 400, loss = 2.04, batch loss = 1.98 (39.1 examples/sec; 0.204 sec/batch; 18h:51m:47s remains)
INFO - root - 2017-12-05 06:30:05.496657: step 410, loss = 2.05, batch loss = 2.00 (40.0 examples/sec; 0.200 sec/batch; 18h:27m:31s remains)
INFO - root - 2017-12-05 06:30:07.553456: step 420, loss = 2.07, batch loss = 2.02 (40.1 examples/sec; 0.200 sec/batch; 18h:24m:22s remains)
INFO - root - 2017-12-05 06:30:09.582367: step 430, loss = 2.07, batch loss = 2.01 (39.9 examples/sec; 0.200 sec/batch; 18h:29m:17s remains)
INFO - root - 2017-12-05 06:30:11.627948: step 440, loss = 2.06, batch loss = 2.00 (39.4 examples/sec; 0.203 sec/batch; 18h:44m:46s remains)
INFO - root - 2017-12-05 06:30:13.688227: step 450, loss = 2.08, batch loss = 2.02 (39.4 examples/sec; 0.203 sec/batch; 18h:44m:27s remains)
INFO - root - 2017-12-05 06:30:15.739168: step 460, loss = 2.02, batch loss = 1.96 (39.7 examples/sec; 0.201 sec/batch; 18h:34m:08s remains)
INFO - root - 2017-12-05 06:30:17.780507: step 470, loss = 2.04, batch loss = 1.98 (39.5 examples/sec; 0.203 sec/batch; 18h:41m:23s remains)
INFO - root - 2017-12-05 06:30:19.816813: step 480, loss = 2.09, batch loss = 2.03 (38.9 examples/sec; 0.206 sec/batch; 18h:58m:17s remains)
INFO - root - 2017-12-05 06:30:21.855865: step 490, loss = 2.06, batch loss = 2.00 (39.3 examples/sec; 0.203 sec/batch; 18h:45m:35s remains)
INFO - root - 2017-12-05 06:30:23.926274: step 500, loss = 2.07, batch loss = 2.01 (38.2 examples/sec; 0.210 sec/batch; 19h:20m:07s remains)
INFO - root - 2017-12-05 06:30:26.029890: step 510, loss = 2.06, batch loss = 2.00 (38.9 examples/sec; 0.205 sec/batch; 18h:57m:00s remains)
INFO - root - 2017-12-05 06:30:28.064528: step 520, loss = 2.09, batch loss = 2.03 (38.6 examples/sec; 0.207 sec/batch; 19h:05m:15s remains)
INFO - root - 2017-12-05 06:30:30.119663: step 530, loss = 2.05, batch loss = 1.99 (39.3 examples/sec; 0.204 sec/batch; 18h:45m:57s remains)
INFO - root - 2017-12-05 06:30:32.157440: step 540, loss = 2.03, batch loss = 1.97 (39.5 examples/sec; 0.202 sec/batch; 18h:39m:47s remains)
INFO - root - 2017-12-05 06:30:34.204345: step 550, loss = 2.04, batch loss = 1.98 (38.2 examples/sec; 0.210 sec/batch; 19h:19m:05s remains)
INFO - root - 2017-12-05 06:30:36.263812: step 560, loss = 2.07, batch loss = 2.02 (38.4 examples/sec; 0.209 sec/batch; 19h:13m:45s remains)
INFO - root - 2017-12-05 06:30:38.305224: step 570, loss = 2.06, batch loss = 2.00 (39.7 examples/sec; 0.201 sec/batch; 18h:34m:40s remains)
INFO - root - 2017-12-05 06:30:40.343015: step 580, loss = 2.07, batch loss = 2.01 (40.2 examples/sec; 0.199 sec/batch; 18h:21m:37s remains)
INFO - root - 2017-12-05 06:30:42.386965: step 590, loss = 2.04, batch loss = 1.98 (39.6 examples/sec; 0.202 sec/batch; 18h:38m:14s remains)
INFO - root - 2017-12-05 06:30:44.442634: step 600, loss = 2.09, batch loss = 2.03 (39.5 examples/sec; 0.203 sec/batch; 18h:40m:22s remains)
INFO - root - 2017-12-05 06:30:46.576467: step 610, loss = 2.07, batch loss = 2.02 (38.8 examples/sec; 0.206 sec/batch; 19h:01m:27s remains)
INFO - root - 2017-12-05 06:30:48.661707: step 620, loss = 2.05, batch loss = 1.99 (39.5 examples/sec; 0.202 sec/batch; 18h:38m:58s remains)
INFO - root - 2017-12-05 06:30:50.714397: step 630, loss = 2.06, batch loss = 2.00 (38.8 examples/sec; 0.206 sec/batch; 18h:59m:31s remains)
INFO - root - 2017-12-05 06:30:52.750571: step 640, loss = 2.04, batch loss = 1.98 (38.7 examples/sec; 0.207 sec/batch; 19h:04m:34s remains)
INFO - root - 2017-12-05 06:30:54.843967: step 650, loss = 2.07, batch loss = 2.01 (37.6 examples/sec; 0.212 sec/batch; 19h:35m:17s remains)
INFO - root - 2017-12-05 06:30:56.934764: step 660, loss = 2.04, batch loss = 1.98 (39.1 examples/sec; 0.204 sec/batch; 18h:50m:34s remains)
INFO - root - 2017-12-05 06:30:58.984225: step 670, loss = 2.04, batch loss = 1.99 (39.2 examples/sec; 0.204 sec/batch; 18h:48m:50s remains)
INFO - root - 2017-12-05 06:31:01.052534: step 680, loss = 2.06, batch loss = 2.00 (37.4 examples/sec; 0.214 sec/batch; 19h:42m:30s remains)
INFO - root - 2017-12-05 06:31:03.119834: step 690, loss = 2.04, batch loss = 1.98 (38.7 examples/sec; 0.207 sec/batch; 19h:03m:45s remains)
INFO - root - 2017-12-05 06:31:05.197377: step 700, loss = 2.07, batch loss = 2.01 (38.0 examples/sec; 0.211 sec/batch; 19h:24m:03s remains)
INFO - root - 2017-12-05 06:31:07.337664: step 710, loss = 2.07, batch loss = 2.01 (39.2 examples/sec; 0.204 sec/batch; 18h:49m:19s remains)
INFO - root - 2017-12-05 06:31:09.408160: step 720, loss = 2.06, batch loss = 2.01 (38.6 examples/sec; 0.207 sec/batch; 19h:06m:15s remains)
INFO - root - 2017-12-05 06:31:11.490802: step 730, loss = 2.07, batch loss = 2.01 (38.4 examples/sec; 0.208 sec/batch; 19h:10m:58s remains)
INFO - root - 2017-12-05 06:31:13.531337: step 740, loss = 2.05, batch loss = 2.00 (39.2 examples/sec; 0.204 sec/batch; 18h:48m:52s remains)
INFO - root - 2017-12-05 06:31:15.591577: step 750, loss = 2.03, batch loss = 1.97 (37.8 examples/sec; 0.212 sec/batch; 19h:30m:43s remains)
INFO - root - 2017-12-05 06:31:17.646498: step 760, loss = 2.04, batch loss = 1.98 (39.8 examples/sec; 0.201 sec/batch; 18h:32m:17s remains)
INFO - root - 2017-12-05 06:31:19.708363: step 770, loss = 2.07, batch loss = 2.01 (38.4 examples/sec; 0.208 sec/batch; 19h:10m:27s remains)
INFO - root - 2017-12-05 06:31:21.763476: step 780, loss = 2.06, batch loss = 2.00 (39.0 examples/sec; 0.205 sec/batch; 18h:55m:17s remains)
INFO - root - 2017-12-05 06:31:23.801403: step 790, loss = 2.08, batch loss = 2.02 (38.7 examples/sec; 0.207 sec/batch; 19h:01m:48s remains)
INFO - root - 2017-12-05 06:31:25.870840: step 800, loss = 2.06, batch loss = 2.00 (37.3 examples/sec; 0.215 sec/batch; 19h:47m:10s remains)
INFO - root - 2017-12-05 06:31:28.009314: step 810, loss = 2.06, batch loss = 2.00 (37.7 examples/sec; 0.212 sec/batch; 19h:32m:59s remains)
INFO - root - 2017-12-05 06:31:30.053944: step 820, loss = 2.06, batch loss = 2.00 (39.3 examples/sec; 0.203 sec/batch; 18h:44m:32s remains)
INFO - root - 2017-12-05 06:31:32.103549: step 830, loss = 2.04, batch loss = 1.98 (39.4 examples/sec; 0.203 sec/batch; 18h:41m:55s remains)
INFO - root - 2017-12-05 06:31:34.138708: step 840, loss = 2.08, batch loss = 2.02 (39.9 examples/sec; 0.200 sec/batch; 18h:28m:13s remains)
INFO - root - 2017-12-05 06:31:36.198886: step 850, loss = 2.05, batch loss = 1.99 (38.7 examples/sec; 0.207 sec/batch; 19h:02m:51s remains)
INFO - root - 2017-12-05 06:31:38.241483: step 860, loss = 2.06, batch loss = 2.00 (38.3 examples/sec; 0.209 sec/batch; 19h:14m:58s remains)
INFO - root - 2017-12-05 06:31:40.377725: step 870, loss = 2.07, batch loss = 2.01 (39.4 examples/sec; 0.203 sec/batch; 18h:41m:50s remains)
INFO - root - 2017-12-05 06:31:42.414458: step 880, loss = 2.08, batch loss = 2.02 (39.2 examples/sec; 0.204 sec/batch; 18h:49m:05s remains)
INFO - root - 2017-12-05 06:31:44.498974: step 890, loss = 2.02, batch loss = 1.96 (39.0 examples/sec; 0.205 sec/batch; 18h:54m:38s remains)
INFO - root - 2017-12-05 06:31:46.606192: step 900, loss = 2.07, batch loss = 2.01 (37.4 examples/sec; 0.214 sec/batch; 19h:41m:59s remains)
INFO - root - 2017-12-05 06:31:48.750036: step 910, loss = 2.06, batch loss = 2.00 (38.2 examples/sec; 0.210 sec/batch; 19h:18m:24s remains)
INFO - root - 2017-12-05 06:31:50.807079: step 920, loss = 2.08, batch loss = 2.02 (39.5 examples/sec; 0.203 sec/batch; 18h:39m:38s remains)
INFO - root - 2017-12-05 06:31:52.880664: step 930, loss = 2.09, batch loss = 2.04 (38.7 examples/sec; 0.207 sec/batch; 19h:03m:30s remains)
INFO - root - 2017-12-05 06:31:54.938000: step 940, loss = 2.08, batch loss = 2.02 (36.8 examples/sec; 0.218 sec/batch; 20h:02m:44s remains)
INFO - root - 2017-12-05 06:31:56.988416: step 950, loss = 2.06, batch loss = 2.00 (39.8 examples/sec; 0.201 sec/batch; 18h:30m:31s remains)
INFO - root - 2017-12-05 06:31:59.032802: step 960, loss = 2.09, batch loss = 2.03 (39.5 examples/sec; 0.202 sec/batch; 18h:38m:14s remains)
INFO - root - 2017-12-05 06:32:01.103161: step 970, loss = 2.07, batch loss = 2.01 (37.7 examples/sec; 0.212 sec/batch; 19h:32m:26s remains)
INFO - root - 2017-12-05 06:32:03.192949: step 980, loss = 2.04, batch loss = 1.99 (37.5 examples/sec; 0.213 sec/batch; 19h:38m:29s remains)
INFO - root - 2017-12-05 06:32:05.267792: step 990, loss = 2.07, batch loss = 2.01 (38.2 examples/sec; 0.209 sec/batch; 19h:16m:39s remains)
INFO - root - 2017-12-05 06:32:07.340886: step 1000, loss = 2.04, batch loss = 1.98 (38.8 examples/sec; 0.206 sec/batch; 18h:58m:14s remains)
INFO - root - 2017-12-05 06:32:09.498840: step 1010, loss = 2.05, batch loss = 2.00 (39.4 examples/sec; 0.203 sec/batch; 18h:42m:35s remains)
INFO - root - 2017-12-05 06:32:11.591380: step 1020, loss = 2.04, batch loss = 1.98 (37.0 examples/sec; 0.216 sec/batch; 19h:53m:45s remains)
INFO - root - 2017-12-05 06:32:13.662448: step 1030, loss = 2.05, batch loss = 1.99 (37.7 examples/sec; 0.212 sec/batch; 19h:31m:46s remains)
INFO - root - 2017-12-05 06:32:15.750193: step 1040, loss = 2.10, batch loss = 2.04 (40.0 examples/sec; 0.200 sec/batch; 18h:24m:55s remains)
INFO - root - 2017-12-05 06:32:17.792115: step 1050, loss = 2.07, batch loss = 2.01 (38.7 examples/sec; 0.207 sec/batch; 19h:01m:35s remains)
INFO - root - 2017-12-05 06:32:19.848899: step 1060, loss = 2.06, batch loss = 2.00 (38.6 examples/sec; 0.207 sec/batch; 19h:03m:46s remains)
INFO - root - 2017-12-05 06:32:21.961292: step 1070, loss = 2.04, batch loss = 1.98 (39.4 examples/sec; 0.203 sec/batch; 18h:41m:58s remains)
INFO - root - 2017-12-05 06:32:24.057454: step 1080, loss = 2.07, batch loss = 2.01 (37.1 examples/sec; 0.216 sec/batch; 19h:50m:33s remains)
INFO - root - 2017-12-05 06:32:26.132734: step 1090, loss = 2.04, batch loss = 1.98 (38.7 examples/sec; 0.207 sec/batch; 19h:02m:07s remains)
INFO - root - 2017-12-05 06:32:28.219952: step 1100, loss = 2.07, batch loss = 2.01 (38.3 examples/sec; 0.209 sec/batch; 19h:13m:51s remains)
INFO - root - 2017-12-05 06:32:30.370347: step 1110, loss = 2.05, batch loss = 1.99 (39.3 examples/sec; 0.204 sec/batch; 18h:45m:28s remains)
INFO - root - 2017-12-05 06:32:32.499123: step 1120, loss = 2.04, batch loss = 1.98 (36.7 examples/sec; 0.218 sec/batch; 20h:03m:53s remains)
INFO - root - 2017-12-05 06:32:34.579606: step 1130, loss = 2.06, batch loss = 2.00 (38.0 examples/sec; 0.211 sec/batch; 19h:23m:14s remains)
INFO - root - 2017-12-05 06:32:36.648495: step 1140, loss = 2.07, batch loss = 2.01 (39.2 examples/sec; 0.204 sec/batch; 18h:47m:09s remains)
INFO - root - 2017-12-05 06:32:38.745779: step 1150, loss = 2.04, batch loss = 1.98 (37.2 examples/sec; 0.215 sec/batch; 19h:47m:35s remains)
INFO - root - 2017-12-05 06:32:40.800458: step 1160, loss = 2.06, batch loss = 2.00 (39.5 examples/sec; 0.203 sec/batch; 18h:38m:34s remains)
INFO - root - 2017-12-05 06:32:42.874740: step 1170, loss = 2.06, batch loss = 2.00 (38.3 examples/sec; 0.209 sec/batch; 19h:14m:46s remains)
INFO - root - 2017-12-05 06:32:44.952790: step 1180, loss = 2.09, batch loss = 2.03 (37.2 examples/sec; 0.215 sec/batch; 19h:46m:13s remains)
INFO - root - 2017-12-05 06:32:47.016903: step 1190, loss = 2.05, batch loss = 1.99 (38.8 examples/sec; 0.206 sec/batch; 18h:58m:41s remains)
INFO - root - 2017-12-05 06:32:49.088732: step 1200, loss = 2.09, batch loss = 2.03 (38.9 examples/sec; 0.206 sec/batch; 18h:56m:18s remains)
INFO - root - 2017-12-05 06:32:51.227437: step 1210, loss = 2.06, batch loss = 2.00 (38.2 examples/sec; 0.209 sec/batch; 19h:16m:37s remains)
INFO - root - 2017-12-05 06:32:53.282265: step 1220, loss = 2.04, batch loss = 1.98 (39.6 examples/sec; 0.202 sec/batch; 18h:35m:21s remains)
INFO - root - 2017-12-05 06:32:55.380192: step 1230, loss = 2.08, batch loss = 2.02 (37.6 examples/sec; 0.213 sec/batch; 19h:33m:24s remains)
INFO - root - 2017-12-05 06:32:57.474318: step 1240, loss = 2.07, batch loss = 2.02 (36.8 examples/sec; 0.217 sec/batch; 20h:00m:34s remains)
INFO - root - 2017-12-05 06:32:59.532062: step 1250, loss = 2.06, batch loss = 2.01 (39.0 examples/sec; 0.205 sec/batch; 18h:53m:47s remains)
INFO - root - 2017-12-05 06:33:01.592982: step 1260, loss = 2.04, batch loss = 1.98 (39.3 examples/sec; 0.203 sec/batch; 18h:43m:00s remains)
INFO - root - 2017-12-05 06:33:03.653585: step 1270, loss = 2.04, batch loss = 1.98 (38.7 examples/sec; 0.207 sec/batch; 19h:00m:59s remains)
INFO - root - 2017-12-05 06:33:05.710591: step 1280, loss = 2.06, batch loss = 2.00 (38.7 examples/sec; 0.207 sec/batch; 19h:01m:48s remains)
INFO - root - 2017-12-05 06:33:07.786924: step 1290, loss = 2.06, batch loss = 2.00 (38.5 examples/sec; 0.208 sec/batch; 19h:06m:08s remains)
INFO - root - 2017-12-05 06:33:09.884993: step 1300, loss = 2.08, batch loss = 2.02 (36.5 examples/sec; 0.219 sec/batch; 20h:10m:50s remains)
INFO - root - 2017-12-05 06:33:12.046047: step 1310, loss = 2.06, batch loss = 2.00 (38.6 examples/sec; 0.207 sec/batch; 19h:04m:29s remains)
INFO - root - 2017-12-05 06:33:14.139938: step 1320, loss = 2.07, batch loss = 2.01 (38.7 examples/sec; 0.207 sec/batch; 19h:00m:38s remains)
INFO - root - 2017-12-05 06:33:16.217664: step 1330, loss = 2.07, batch loss = 2.01 (39.3 examples/sec; 0.203 sec/batch; 18h:42m:13s remains)
INFO - root - 2017-12-05 06:33:18.292192: step 1340, loss = 2.04, batch loss = 1.98 (37.4 examples/sec; 0.214 sec/batch; 19h:39m:34s remains)
INFO - root - 2017-12-05 06:33:20.387993: step 1350, loss = 2.06, batch loss = 2.00 (38.2 examples/sec; 0.209 sec/batch; 19h:15m:19s remains)
INFO - root - 2017-12-05 06:33:22.449078: step 1360, loss = 2.06, batch loss = 2.01 (37.4 examples/sec; 0.214 sec/batch; 19h:42m:01s remains)
INFO - root - 2017-12-05 06:33:24.510024: step 1370, loss = 2.06, batch loss = 2.00 (39.4 examples/sec; 0.203 sec/batch; 18h:41m:10s remains)
INFO - root - 2017-12-05 06:33:26.610610: step 1380, loss = 2.08, batch loss = 2.02 (37.7 examples/sec; 0.212 sec/batch; 19h:30m:53s remains)
INFO - root - 2017-12-05 06:33:28.678745: step 1390, loss = 2.04, batch loss = 1.98 (39.6 examples/sec; 0.202 sec/batch; 18h:33m:50s remains)
INFO - root - 2017-12-05 06:33:30.719143: step 1400, loss = 2.06, batch loss = 2.00 (39.5 examples/sec; 0.202 sec/batch; 18h:37m:01s remains)
INFO - root - 2017-12-05 06:33:32.878212: step 1410, loss = 2.07, batch loss = 2.01 (38.3 examples/sec; 0.209 sec/batch; 19h:12m:16s remains)
INFO - root - 2017-12-05 06:33:34.978894: step 1420, loss = 2.07, batch loss = 2.01 (37.2 examples/sec; 0.215 sec/batch; 19h:46m:50s remains)
INFO - root - 2017-12-05 06:33:37.064613: step 1430, loss = 2.06, batch loss = 2.00 (38.8 examples/sec; 0.206 sec/batch; 18h:57m:11s remains)
INFO - root - 2017-12-05 06:33:39.157586: step 1440, loss = 2.07, batch loss = 2.01 (37.5 examples/sec; 0.213 sec/batch; 19h:36m:57s remains)
INFO - root - 2017-12-05 06:33:41.231137: step 1450, loss = 2.04, batch loss = 1.98 (39.2 examples/sec; 0.204 sec/batch; 18h:46m:45s remains)
INFO - root - 2017-12-05 06:33:43.343836: step 1460, loss = 2.05, batch loss = 1.99 (37.1 examples/sec; 0.216 sec/batch; 19h:50m:23s remains)
INFO - root - 2017-12-05 06:33:45.449853: step 1470, loss = 2.06, batch loss = 2.00 (37.7 examples/sec; 0.212 sec/batch; 19h:30m:17s remains)
INFO - root - 2017-12-05 06:33:47.503629: step 1480, loss = 2.06, batch loss = 2.00 (38.9 examples/sec; 0.206 sec/batch; 18h:53m:57s remains)
INFO - root - 2017-12-05 06:33:49.610720: step 1490, loss = 2.08, batch loss = 2.02 (37.7 examples/sec; 0.212 sec/batch; 19h:31m:44s remains)
INFO - root - 2017-12-05 06:33:51.721821: step 1500, loss = 2.07, batch loss = 2.02 (38.7 examples/sec; 0.207 sec/batch; 19h:00m:50s remains)
INFO - root - 2017-12-05 06:33:53.886668: step 1510, loss = 2.06, batch loss = 2.00 (37.2 examples/sec; 0.215 sec/batch; 19h:47m:50s remains)
INFO - root - 2017-12-05 06:33:55.966712: step 1520, loss = 2.07, batch loss = 2.01 (39.8 examples/sec; 0.201 sec/batch; 18h:29m:00s remains)
INFO - root - 2017-12-05 06:33:58.049261: step 1530, loss = 2.07, batch loss = 2.01 (37.5 examples/sec; 0.213 sec/batch; 19h:36m:35s remains)
INFO - root - 2017-12-05 06:34:00.133481: step 1540, loss = 2.07, batch loss = 2.01 (38.7 examples/sec; 0.207 sec/batch; 19h:01m:03s remains)
INFO - root - 2017-12-05 06:34:02.238670: step 1550, loss = 2.07, batch loss = 2.01 (36.9 examples/sec; 0.217 sec/batch; 19h:57m:14s remains)
INFO - root - 2017-12-05 06:34:04.349767: step 1560, loss = 2.08, batch loss = 2.02 (38.8 examples/sec; 0.206 sec/batch; 18h:57m:44s remains)
INFO - root - 2017-12-05 06:34:06.433184: step 1570, loss = 2.06, batch loss = 2.00 (38.6 examples/sec; 0.207 sec/batch; 19h:04m:27s remains)
INFO - root - 2017-12-05 06:34:08.519124: step 1580, loss = 2.04, batch loss = 1.98 (38.6 examples/sec; 0.207 sec/batch; 19h:03m:38s remains)
INFO - root - 2017-12-05 06:34:10.607497: step 1590, loss = 2.07, batch loss = 2.01 (38.9 examples/sec; 0.206 sec/batch; 18h:54m:38s remains)
INFO - root - 2017-12-05 06:34:12.702104: step 1600, loss = 2.06, batch loss = 2.00 (38.8 examples/sec; 0.206 sec/batch; 18h:58m:27s remains)
INFO - root - 2017-12-05 06:34:14.858265: step 1610, loss = 2.06, batch loss = 2.00 (38.4 examples/sec; 0.208 sec/batch; 19h:08m:56s remains)
INFO - root - 2017-12-05 06:34:16.918993: step 1620, loss = 2.05, batch loss = 1.99 (39.1 examples/sec; 0.205 sec/batch; 18h:49m:01s remains)
INFO - root - 2017-12-05 06:34:18.997934: step 1630, loss = 2.11, batch loss = 2.05 (38.2 examples/sec; 0.209 sec/batch; 19h:14m:11s remains)
INFO - root - 2017-12-05 06:34:21.092628: step 1640, loss = 2.08, batch loss = 2.02 (36.4 examples/sec; 0.220 sec/batch; 20h:12m:26s remains)
INFO - root - 2017-12-05 06:34:23.152071: step 1650, loss = 2.06, batch loss = 2.00 (38.8 examples/sec; 0.206 sec/batch; 18h:58m:13s remains)
INFO - root - 2017-12-05 06:34:25.246223: step 1660, loss = 2.06, batch loss = 2.00 (38.8 examples/sec; 0.206 sec/batch; 18h:57m:38s remains)
INFO - root - 2017-12-05 06:34:27.309778: step 1670, loss = 2.06, batch loss = 2.00 (39.3 examples/sec; 0.203 sec/batch; 18h:41m:23s remains)
INFO - root - 2017-12-05 06:34:29.431579: step 1680, loss = 2.07, batch loss = 2.01 (35.5 examples/sec; 0.225 sec/batch; 20h:41m:56s remains)
INFO - root - 2017-12-05 06:34:31.516648: step 1690, loss = 2.08, batch loss = 2.02 (37.9 examples/sec; 0.211 sec/batch; 19h:23m:30s remains)
INFO - root - 2017-12-05 06:34:33.582010: step 1700, loss = 2.05, batch loss = 2.00 (39.0 examples/sec; 0.205 sec/batch; 18h:49m:53s remains)
INFO - root - 2017-12-05 06:34:35.737703: step 1710, loss = 2.06, batch loss = 2.00 (39.2 examples/sec; 0.204 sec/batch; 18h:45m:43s remains)
INFO - root - 2017-12-05 06:34:37.827689: step 1720, loss = 2.04, batch loss = 1.98 (39.3 examples/sec; 0.204 sec/batch; 18h:42m:10s remains)
INFO - root - 2017-12-05 06:34:39.892633: step 1730, loss = 2.04, batch loss = 1.98 (38.0 examples/sec; 0.211 sec/batch; 19h:20m:40s remains)
INFO - root - 2017-12-05 06:34:41.974413: step 1740, loss = 2.09, batch loss = 2.03 (38.1 examples/sec; 0.210 sec/batch; 19h:18m:56s remains)
INFO - root - 2017-12-05 06:34:44.040363: step 1750, loss = 2.08, batch loss = 2.02 (39.1 examples/sec; 0.204 sec/batch; 18h:46m:29s remains)
INFO - root - 2017-12-05 06:34:46.143415: step 1760, loss = 2.06, batch loss = 2.00 (39.6 examples/sec; 0.202 sec/batch; 18h:33m:24s remains)
INFO - root - 2017-12-05 06:34:48.219583: step 1770, loss = 2.07, batch loss = 2.01 (39.3 examples/sec; 0.203 sec/batch; 18h:40m:51s remains)
INFO - root - 2017-12-05 06:34:50.299454: step 1780, loss = 2.05, batch loss = 1.99 (37.9 examples/sec; 0.211 sec/batch; 19h:22m:44s remains)
INFO - root - 2017-12-05 06:34:52.376049: step 1790, loss = 2.05, batch loss = 1.99 (39.2 examples/sec; 0.204 sec/batch; 18h:45m:47s remains)
INFO - root - 2017-12-05 06:34:54.457499: step 1800, loss = 2.08, batch loss = 2.02 (37.2 examples/sec; 0.215 sec/batch; 19h:44m:18s remains)
INFO - root - 2017-12-05 06:34:56.611056: step 1810, loss = 2.07, batch loss = 2.01 (39.0 examples/sec; 0.205 sec/batch; 18h:51m:06s remains)
INFO - root - 2017-12-05 06:34:58.682152: step 1820, loss = 2.05, batch loss = 1.99 (39.3 examples/sec; 0.204 sec/batch; 18h:42m:56s remains)
INFO - root - 2017-12-05 06:35:00.743744: step 1830, loss = 2.04, batch loss = 1.98 (39.6 examples/sec; 0.202 sec/batch; 18h:33m:44s remains)
INFO - root - 2017-12-05 06:35:02.863592: step 1840, loss = 2.06, batch loss = 2.00 (37.6 examples/sec; 0.213 sec/batch; 19h:31m:26s remains)
INFO - root - 2017-12-05 06:35:04.953844: step 1850, loss = 2.05, batch loss = 1.99 (39.3 examples/sec; 0.204 sec/batch; 18h:41m:29s remains)
INFO - root - 2017-12-05 06:35:07.027787: step 1860, loss = 2.06, batch loss = 2.01 (38.2 examples/sec; 0.209 sec/batch; 19h:14m:20s remains)
INFO - root - 2017-12-05 06:35:09.116020: step 1870, loss = 2.04, batch loss = 1.98 (38.9 examples/sec; 0.206 sec/batch; 18h:52m:52s remains)
INFO - root - 2017-12-05 06:35:11.201287: step 1880, loss = 2.06, batch loss = 2.00 (37.5 examples/sec; 0.213 sec/batch; 19h:34m:25s remains)
INFO - root - 2017-12-05 06:35:13.284358: step 1890, loss = 2.09, batch loss = 2.03 (39.2 examples/sec; 0.204 sec/batch; 18h:44m:44s remains)
INFO - root - 2017-12-05 06:35:15.369866: step 1900, loss = 2.03, batch loss = 1.97 (38.1 examples/sec; 0.210 sec/batch; 19h:16m:13s remains)
INFO - root - 2017-12-05 06:35:17.483088: step 1910, loss = 2.06, batch loss = 2.00 (39.3 examples/sec; 0.203 sec/batch; 18h:40m:38s remains)
INFO - root - 2017-12-05 06:35:19.557069: step 1920, loss = 2.06, batch loss = 2.00 (37.9 examples/sec; 0.211 sec/batch; 19h:23m:40s remains)
INFO - root - 2017-12-05 06:35:21.635651: step 1930, loss = 2.04, batch loss = 1.98 (38.3 examples/sec; 0.209 sec/batch; 19h:10m:01s remains)
INFO - root - 2017-12-05 06:35:23.744674: step 1940, loss = 2.07, batch loss = 2.01 (38.5 examples/sec; 0.208 sec/batch; 19h:05m:55s remains)
INFO - root - 2017-12-05 06:35:25.832375: step 1950, loss = 2.08, batch loss = 2.02 (36.0 examples/sec; 0.222 sec/batch; 20h:23m:16s remains)
INFO - root - 2017-12-05 06:35:27.904615: step 1960, loss = 2.06, batch loss = 2.00 (39.8 examples/sec; 0.201 sec/batch; 18h:28m:27s remains)
INFO - root - 2017-12-05 06:35:29.969910: step 1970, loss = 2.07, batch loss = 2.02 (38.5 examples/sec; 0.208 sec/batch; 19h:04m:27s remains)
INFO - root - 2017-12-05 06:35:32.050521: step 1980, loss = 2.06, batch loss = 2.00 (37.4 examples/sec; 0.214 sec/batch; 19h:38m:26s remains)
INFO - root - 2017-12-05 06:35:34.106939: step 1990, loss = 2.08, batch loss = 2.02 (38.6 examples/sec; 0.207 sec/batch; 19h:02m:48s remains)
INFO - root - 2017-12-05 06:35:36.214464: step 2000, loss = 2.05, batch loss = 1.99 (38.9 examples/sec; 0.206 sec/batch; 18h:53m:04s remains)
INFO - root - 2017-12-05 06:35:38.349952: step 2010, loss = 2.09, batch loss = 2.03 (39.7 examples/sec; 0.202 sec/batch; 18h:30m:47s remains)
INFO - root - 2017-12-05 06:35:40.439750: step 2020, loss = 2.08, batch loss = 2.02 (39.0 examples/sec; 0.205 sec/batch; 18h:49m:57s remains)
INFO - root - 2017-12-05 06:35:42.488154: step 2030, loss = 2.05, batch loss = 1.99 (39.8 examples/sec; 0.201 sec/batch; 18h:28m:08s remains)
INFO - root - 2017-12-05 06:35:44.614229: step 2040, loss = 2.07, batch loss = 2.02 (38.9 examples/sec; 0.206 sec/batch; 18h:53m:16s remains)
INFO - root - 2017-12-05 06:35:46.685069: step 2050, loss = 2.06, batch loss = 2.00 (37.7 examples/sec; 0.212 sec/batch; 19h:29m:02s remains)
INFO - root - 2017-12-05 06:35:48.791455: step 2060, loss = 2.06, batch loss = 2.00 (37.5 examples/sec; 0.213 sec/batch; 19h:34m:42s remains)
INFO - root - 2017-12-05 06:35:50.867821: step 2070, loss = 2.05, batch loss = 1.99 (39.1 examples/sec; 0.205 sec/batch; 18h:48m:09s remains)
INFO - root - 2017-12-05 06:35:52.924713: step 2080, loss = 2.07, batch loss = 2.01 (39.0 examples/sec; 0.205 sec/batch; 18h:49m:21s remains)
INFO - root - 2017-12-05 06:35:55.022915: step 2090, loss = 2.08, batch loss = 2.02 (38.0 examples/sec; 0.211 sec/batch; 19h:20m:07s remains)
INFO - root - 2017-12-05 06:35:57.084146: step 2100, loss = 2.09, batch loss = 2.03 (38.2 examples/sec; 0.210 sec/batch; 19h:14m:15s remains)
INFO - root - 2017-12-05 06:35:59.247948: step 2110, loss = 2.05, batch loss = 1.99 (39.4 examples/sec; 0.203 sec/batch; 18h:39m:09s remains)
INFO - root - 2017-12-05 06:36:01.298550: step 2120, loss = 2.06, batch loss = 2.00 (37.9 examples/sec; 0.211 sec/batch; 19h:22m:23s remains)
INFO - root - 2017-12-05 06:36:03.392049: step 2130, loss = 2.06, batch loss = 2.00 (38.4 examples/sec; 0.208 sec/batch; 19h:07m:41s remains)
INFO - root - 2017-12-05 06:36:05.493158: step 2140, loss = 2.07, batch loss = 2.01 (38.5 examples/sec; 0.208 sec/batch; 19h:05m:01s remains)
INFO - root - 2017-12-05 06:36:07.550456: step 2150, loss = 2.08, batch loss = 2.02 (39.0 examples/sec; 0.205 sec/batch; 18h:48m:40s remains)
INFO - root - 2017-12-05 06:36:09.622212: step 2160, loss = 2.05, batch loss = 1.99 (39.5 examples/sec; 0.202 sec/batch; 18h:34m:24s remains)
INFO - root - 2017-12-05 06:36:11.699181: step 2170, loss = 2.05, batch loss = 1.99 (37.1 examples/sec; 0.216 sec/batch; 19h:46m:27s remains)
INFO - root - 2017-12-05 06:36:13.775092: step 2180, loss = 2.07, batch loss = 2.02 (38.8 examples/sec; 0.206 sec/batch; 18h:55m:09s remains)
INFO - root - 2017-12-05 06:36:15.864509: step 2190, loss = 2.07, batch loss = 2.01 (38.8 examples/sec; 0.206 sec/batch; 18h:56m:21s remains)
INFO - root - 2017-12-05 06:36:17.967216: step 2200, loss = 2.08, batch loss = 2.02 (37.4 examples/sec; 0.214 sec/batch; 19h:36m:06s remains)
INFO - root - 2017-12-05 06:36:20.106606: step 2210, loss = 2.05, batch loss = 1.99 (37.7 examples/sec; 0.212 sec/batch; 19h:27m:51s remains)
INFO - root - 2017-12-05 06:36:22.192410: step 2220, loss = 2.05, batch loss = 1.99 (39.0 examples/sec; 0.205 sec/batch; 18h:48m:26s remains)
INFO - root - 2017-12-05 06:36:24.295657: step 2230, loss = 2.07, batch loss = 2.01 (36.0 examples/sec; 0.222 sec/batch; 20h:24m:10s remains)
INFO - root - 2017-12-05 06:36:26.400473: step 2240, loss = 2.05, batch loss = 1.99 (36.3 examples/sec; 0.221 sec/batch; 20h:13m:47s remains)
INFO - root - 2017-12-05 06:36:28.465482: step 2250, loss = 2.05, batch loss = 1.99 (38.7 examples/sec; 0.207 sec/batch; 18h:57m:43s remains)
INFO - root - 2017-12-05 06:36:30.553361: step 2260, loss = 2.03, batch loss = 1.98 (37.3 examples/sec; 0.214 sec/batch; 19h:40m:30s remains)
INFO - root - 2017-12-05 06:36:32.627358: step 2270, loss = 2.07, batch loss = 2.01 (38.8 examples/sec; 0.206 sec/batch; 18h:54m:17s remains)
INFO - root - 2017-12-05 06:36:34.740318: step 2280, loss = 2.08, batch loss = 2.02 (39.1 examples/sec; 0.205 sec/batch; 18h:46m:04s remains)
INFO - root - 2017-12-05 06:36:36.822737: step 2290, loss = 2.07, batch loss = 2.02 (38.8 examples/sec; 0.206 sec/batch; 18h:54m:29s remains)
INFO - root - 2017-12-05 06:36:38.892785: step 2300, loss = 2.07, batch loss = 2.01 (38.4 examples/sec; 0.208 sec/batch; 19h:07m:12s remains)
INFO - root - 2017-12-05 06:36:41.050503: step 2310, loss = 2.07, batch loss = 2.01 (37.5 examples/sec; 0.213 sec/batch; 19h:33m:54s remains)
INFO - root - 2017-12-05 06:36:43.158092: step 2320, loss = 2.07, batch loss = 2.01 (36.4 examples/sec; 0.220 sec/batch; 20h:09m:11s remains)
INFO - root - 2017-12-05 06:36:45.230883: step 2330, loss = 2.06, batch loss = 2.00 (37.7 examples/sec; 0.212 sec/batch; 19h:27m:37s remains)
INFO - root - 2017-12-05 06:36:47.295706: step 2340, loss = 2.08, batch loss = 2.02 (38.7 examples/sec; 0.207 sec/batch; 18h:58m:21s remains)
INFO - root - 2017-12-05 06:36:49.367943: step 2350, loss = 2.06, batch loss = 2.00 (39.1 examples/sec; 0.205 sec/batch; 18h:46m:22s remains)
INFO - root - 2017-12-05 06:36:51.434817: step 2360, loss = 2.08, batch loss = 2.03 (37.9 examples/sec; 0.211 sec/batch; 19h:21m:30s remains)
INFO - root - 2017-12-05 06:36:53.551177: step 2370, loss = 2.06, batch loss = 2.00 (38.4 examples/sec; 0.208 sec/batch; 19h:06m:11s remains)
INFO - root - 2017-12-05 06:36:55.629684: step 2380, loss = 2.06, batch loss = 2.00 (39.1 examples/sec; 0.204 sec/batch; 18h:44m:57s remains)
INFO - root - 2017-12-05 06:36:57.698665: step 2390, loss = 2.06, batch loss = 2.01 (37.7 examples/sec; 0.212 sec/batch; 19h:29m:02s remains)
INFO - root - 2017-12-05 06:36:59.777411: step 2400, loss = 2.05, batch loss = 1.99 (38.8 examples/sec; 0.206 sec/batch; 18h:54m:13s remains)
INFO - root - 2017-12-05 06:37:01.978635: step 2410, loss = 2.05, batch loss = 1.99 (37.4 examples/sec; 0.214 sec/batch; 19h:36m:49s remains)
INFO - root - 2017-12-05 06:37:04.085201: step 2420, loss = 2.06, batch loss = 2.00 (38.7 examples/sec; 0.207 sec/batch; 18h:56m:04s remains)
INFO - root - 2017-12-05 06:37:06.180165: step 2430, loss = 2.05, batch loss = 1.99 (38.1 examples/sec; 0.210 sec/batch; 19h:13m:38s remains)
INFO - root - 2017-12-05 06:37:08.290057: step 2440, loss = 2.07, batch loss = 2.02 (37.9 examples/sec; 0.211 sec/batch; 19h:22m:08s remains)
INFO - root - 2017-12-05 06:37:10.374582: step 2450, loss = 2.06, batch loss = 2.01 (37.9 examples/sec; 0.211 sec/batch; 19h:21m:43s remains)
INFO - root - 2017-12-05 06:37:12.449965: step 2460, loss = 2.08, batch loss = 2.02 (39.1 examples/sec; 0.205 sec/batch; 18h:45m:27s remains)
INFO - root - 2017-12-05 06:37:14.723644: step 2470, loss = 2.07, batch loss = 2.01 (37.7 examples/sec; 0.212 sec/batch; 19h:28m:23s remains)
INFO - root - 2017-12-05 06:37:16.788494: step 2480, loss = 2.06, batch loss = 2.00 (38.8 examples/sec; 0.206 sec/batch; 18h:53m:39s remains)
INFO - root - 2017-12-05 06:37:18.860128: step 2490, loss = 2.03, batch loss = 1.98 (39.5 examples/sec; 0.202 sec/batch; 18h:33m:38s remains)
INFO - root - 2017-12-05 06:37:20.914026: step 2500, loss = 2.05, batch loss = 1.99 (39.0 examples/sec; 0.205 sec/batch; 18h:49m:18s remains)
INFO - root - 2017-12-05 06:37:23.081880: step 2510, loss = 2.07, batch loss = 2.01 (38.8 examples/sec; 0.206 sec/batch; 18h:55m:05s remains)
INFO - root - 2017-12-05 06:37:25.153885: step 2520, loss = 2.03, batch loss = 1.97 (37.9 examples/sec; 0.211 sec/batch; 19h:22m:03s remains)
INFO - root - 2017-12-05 06:37:27.267197: step 2530, loss = 2.08, batch loss = 2.03 (38.4 examples/sec; 0.208 sec/batch; 19h:04m:42s remains)
INFO - root - 2017-12-05 06:37:29.367400: step 2540, loss = 2.04, batch loss = 1.99 (36.9 examples/sec; 0.217 sec/batch; 19h:50m:49s remains)
INFO - root - 2017-12-05 06:37:31.456321: step 2550, loss = 2.07, batch loss = 2.02 (39.2 examples/sec; 0.204 sec/batch; 18h:43m:41s remains)
INFO - root - 2017-12-05 06:37:33.537030: step 2560, loss = 2.03, batch loss = 1.97 (39.3 examples/sec; 0.204 sec/batch; 18h:39m:29s remains)
INFO - root - 2017-12-05 06:37:35.605766: step 2570, loss = 2.07, batch loss = 2.01 (38.5 examples/sec; 0.208 sec/batch; 19h:01m:31s remains)
INFO - root - 2017-12-05 06:37:37.695224: step 2580, loss = 2.05, batch loss = 1.99 (38.7 examples/sec; 0.207 sec/batch; 18h:57m:33s remains)
INFO - root - 2017-12-05 06:37:39.785483: step 2590, loss = 2.08, batch loss = 2.02 (38.9 examples/sec; 0.206 sec/batch; 18h:51m:22s remains)
INFO - root - 2017-12-05 06:37:41.855096: step 2600, loss = 2.05, batch loss = 1.99 (38.9 examples/sec; 0.206 sec/batch; 18h:50m:39s remains)
INFO - root - 2017-12-05 06:37:44.022952: step 2610, loss = 2.05, batch loss = 1.99 (39.2 examples/sec; 0.204 sec/batch; 18h:40m:46s remains)
INFO - root - 2017-12-05 06:37:46.080755: step 2620, loss = 2.04, batch loss = 1.98 (38.7 examples/sec; 0.207 sec/batch; 18h:57m:07s remains)
INFO - root - 2017-12-05 06:37:48.165247: step 2630, loss = 2.06, batch loss = 2.00 (38.7 examples/sec; 0.207 sec/batch; 18h:56m:20s remains)
INFO - root - 2017-12-05 06:37:50.235164: step 2640, loss = 2.07, batch loss = 2.01 (39.1 examples/sec; 0.205 sec/batch; 18h:45m:01s remains)
INFO - root - 2017-12-05 06:37:52.283509: step 2650, loss = 2.05, batch loss = 2.00 (38.3 examples/sec; 0.209 sec/batch; 19h:08m:14s remains)
INFO - root - 2017-12-05 06:37:54.380024: step 2660, loss = 2.04, batch loss = 1.99 (37.9 examples/sec; 0.211 sec/batch; 19h:19m:01s remains)
INFO - root - 2017-12-05 06:37:56.500270: step 2670, loss = 2.07, batch loss = 2.01 (37.8 examples/sec; 0.212 sec/batch; 19h:22m:52s remains)
INFO - root - 2017-12-05 06:37:58.593736: step 2680, loss = 2.08, batch loss = 2.02 (38.7 examples/sec; 0.207 sec/batch; 18h:55m:52s remains)
INFO - root - 2017-12-05 06:38:00.672902: step 2690, loss = 2.07, batch loss = 2.01 (37.4 examples/sec; 0.214 sec/batch; 19h:36m:48s remains)
INFO - root - 2017-12-05 06:38:02.788237: step 2700, loss = 2.06, batch loss = 2.01 (39.0 examples/sec; 0.205 sec/batch; 18h:48m:16s remains)
INFO - root - 2017-12-05 06:38:04.964995: step 2710, loss = 2.06, batch loss = 2.00 (37.7 examples/sec; 0.212 sec/batch; 19h:27m:34s remains)
INFO - root - 2017-12-05 06:38:07.058212: step 2720, loss = 2.05, batch loss = 1.99 (37.1 examples/sec; 0.216 sec/batch; 19h:45m:33s remains)
INFO - root - 2017-12-05 06:38:09.201663: step 2730, loss = 2.10, batch loss = 2.04 (35.3 examples/sec; 0.227 sec/batch; 20h:46m:27s remains)
INFO - root - 2017-12-05 06:38:11.278472: step 2740, loss = 2.07, batch loss = 2.02 (37.5 examples/sec; 0.213 sec/batch; 19h:31m:28s remains)
INFO - root - 2017-12-05 06:38:13.375043: step 2750, loss = 2.10, batch loss = 2.04 (36.8 examples/sec; 0.217 sec/batch; 19h:53m:56s remains)
INFO - root - 2017-12-05 06:38:15.435112: step 2760, loss = 2.05, batch loss = 1.99 (39.0 examples/sec; 0.205 sec/batch; 18h:48m:26s remains)
INFO - root - 2017-12-05 06:38:17.530596: step 2770, loss = 2.06, batch loss = 2.00 (38.9 examples/sec; 0.206 sec/batch; 18h:50m:24s remains)
INFO - root - 2017-12-05 06:38:19.590724: step 2780, loss = 2.06, batch loss = 2.00 (39.5 examples/sec; 0.203 sec/batch; 18h:33m:37s remains)
INFO - root - 2017-12-05 06:38:21.649995: step 2790, loss = 2.03, batch loss = 1.97 (39.2 examples/sec; 0.204 sec/batch; 18h:40m:38s remains)
INFO - root - 2017-12-05 06:38:23.775132: step 2800, loss = 2.05, batch loss = 1.99 (37.8 examples/sec; 0.211 sec/batch; 19h:21m:46s remains)
INFO - root - 2017-12-05 06:38:25.916067: step 2810, loss = 2.05, batch loss = 1.99 (38.3 examples/sec; 0.209 sec/batch; 19h:07m:51s remains)
INFO - root - 2017-12-05 06:38:27.989012: step 2820, loss = 2.05, batch loss = 1.99 (39.8 examples/sec; 0.201 sec/batch; 18h:23m:44s remains)
INFO - root - 2017-12-05 06:38:30.044806: step 2830, loss = 2.07, batch loss = 2.01 (38.9 examples/sec; 0.205 sec/batch; 18h:48m:45s remains)
INFO - root - 2017-12-05 06:38:32.142994: step 2840, loss = 2.05, batch loss = 2.00 (39.0 examples/sec; 0.205 sec/batch; 18h:47m:24s remains)
INFO - root - 2017-12-05 06:38:34.193358: step 2850, loss = 2.06, batch loss = 2.00 (39.9 examples/sec; 0.200 sec/batch; 18h:21m:33s remains)
INFO - root - 2017-12-05 06:38:36.261329: step 2860, loss = 2.03, batch loss = 1.98 (38.7 examples/sec; 0.207 sec/batch; 18h:56m:03s remains)
INFO - root - 2017-12-05 06:38:38.294617: step 2870, loss = 2.07, batch loss = 2.01 (39.1 examples/sec; 0.204 sec/batch; 18h:42m:52s remains)
INFO - root - 2017-12-05 06:38:40.396678: step 2880, loss = 2.05, batch loss = 1.99 (36.3 examples/sec; 0.220 sec/batch; 20h:11m:02s remains)
INFO - root - 2017-12-05 06:38:42.470912: step 2890, loss = 2.08, batch loss = 2.03 (39.0 examples/sec; 0.205 sec/batch; 18h:46m:10s remains)
INFO - root - 2017-12-05 06:38:44.545733: step 2900, loss = 2.07, batch loss = 2.01 (39.1 examples/sec; 0.204 sec/batch; 18h:42m:49s remains)
INFO - root - 2017-12-05 06:38:46.755243: step 2910, loss = 2.07, batch loss = 2.01 (37.1 examples/sec; 0.216 sec/batch; 19h:44m:49s remains)
INFO - root - 2017-12-05 06:38:48.853183: step 2920, loss = 2.06, batch loss = 2.00 (37.9 examples/sec; 0.211 sec/batch; 19h:18m:15s remains)
INFO - root - 2017-12-05 06:38:50.965027: step 2930, loss = 2.06, batch loss = 2.00 (39.4 examples/sec; 0.203 sec/batch; 18h:36m:07s remains)
INFO - root - 2017-12-05 06:38:53.045153: step 2940, loss = 2.07, batch loss = 2.01 (39.2 examples/sec; 0.204 sec/batch; 18h:41m:35s remains)
INFO - root - 2017-12-05 06:38:55.182034: step 2950, loss = 2.06, batch loss = 2.01 (36.3 examples/sec; 0.221 sec/batch; 20h:11m:32s remains)
INFO - root - 2017-12-05 06:38:57.258445: step 2960, loss = 2.10, batch loss = 2.04 (39.5 examples/sec; 0.203 sec/batch; 18h:33m:21s remains)
INFO - root - 2017-12-05 06:38:59.347396: step 2970, loss = 2.03, batch loss = 1.98 (38.0 examples/sec; 0.210 sec/batch; 19h:15m:54s remains)
INFO - root - 2017-12-05 06:39:01.444206: step 2980, loss = 2.09, batch loss = 2.03 (37.9 examples/sec; 0.211 sec/batch; 19h:19m:33s remains)
INFO - root - 2017-12-05 06:39:03.560231: step 2990, loss = 2.10, batch loss = 2.04 (38.3 examples/sec; 0.209 sec/batch; 19h:06m:49s remains)
INFO - root - 2017-12-05 06:39:05.689264: step 3000, loss = 2.05, batch loss = 2.00 (37.8 examples/sec; 0.212 sec/batch; 19h:23m:33s remains)
INFO - root - 2017-12-05 06:39:07.849245: step 3010, loss = 2.06, batch loss = 2.00 (38.7 examples/sec; 0.207 sec/batch; 18h:55m:41s remains)
INFO - root - 2017-12-05 06:39:09.943751: step 3020, loss = 2.05, batch loss = 1.99 (37.3 examples/sec; 0.215 sec/batch; 19h:38m:01s remains)
INFO - root - 2017-12-05 06:39:12.033487: step 3030, loss = 2.07, batch loss = 2.01 (38.2 examples/sec; 0.209 sec/batch; 19h:09m:57s remains)
INFO - root - 2017-12-05 06:39:14.126772: step 3040, loss = 2.07, batch loss = 2.02 (38.5 examples/sec; 0.208 sec/batch; 19h:00m:09s remains)
INFO - root - 2017-12-05 06:39:16.200061: step 3050, loss = 2.08, batch loss = 2.02 (38.9 examples/sec; 0.206 sec/batch; 18h:48m:24s remains)
INFO - root - 2017-12-05 06:39:18.284225: step 3060, loss = 2.07, batch loss = 2.01 (38.4 examples/sec; 0.209 sec/batch; 19h:05m:17s remains)
INFO - root - 2017-12-05 06:39:20.364215: step 3070, loss = 2.06, batch loss = 2.00 (39.0 examples/sec; 0.205 sec/batch; 18h:46m:08s remains)
INFO - root - 2017-12-05 06:39:22.452982: step 3080, loss = 2.03, batch loss = 1.97 (38.9 examples/sec; 0.206 sec/batch; 18h:50m:33s remains)
INFO - root - 2017-12-05 06:39:24.507085: step 3090, loss = 2.07, batch loss = 2.02 (39.7 examples/sec; 0.202 sec/batch; 18h:27m:03s remains)
INFO - root - 2017-12-05 06:39:26.607369: step 3100, loss = 2.05, batch loss = 1.99 (37.0 examples/sec; 0.216 sec/batch; 19h:47m:16s remains)
INFO - root - 2017-12-05 06:39:28.759655: step 3110, loss = 2.07, batch loss = 2.01 (39.6 examples/sec; 0.202 sec/batch; 18h:27m:44s remains)
INFO - root - 2017-12-05 06:39:30.862274: step 3120, loss = 2.08, batch loss = 2.03 (36.9 examples/sec; 0.217 sec/batch; 19h:49m:22s remains)
INFO - root - 2017-12-05 06:39:32.920914: step 3130, loss = 2.06, batch loss = 2.00 (38.6 examples/sec; 0.207 sec/batch; 18h:58m:39s remains)
INFO - root - 2017-12-05 06:39:34.992004: step 3140, loss = 2.09, batch loss = 2.03 (38.2 examples/sec; 0.210 sec/batch; 19h:10m:07s remains)
INFO - root - 2017-12-05 06:39:37.075531: step 3150, loss = 2.06, batch loss = 2.00 (38.8 examples/sec; 0.206 sec/batch; 18h:52m:12s remains)
INFO - root - 2017-12-05 06:39:39.194954: step 3160, loss = 2.05, batch loss = 1.99 (39.8 examples/sec; 0.201 sec/batch; 18h:22m:01s remains)
INFO - root - 2017-12-05 06:39:41.292593: step 3170, loss = 2.06, batch loss = 2.00 (39.0 examples/sec; 0.205 sec/batch; 18h:46m:04s remains)
INFO - root - 2017-12-05 06:39:43.357367: step 3180, loss = 2.06, batch loss = 2.00 (38.9 examples/sec; 0.206 sec/batch; 18h:49m:32s remains)
INFO - root - 2017-12-05 06:39:45.459842: step 3190, loss = 2.07, batch loss = 2.01 (37.2 examples/sec; 0.215 sec/batch; 19h:40m:24s remains)
INFO - root - 2017-12-05 06:39:47.523911: step 3200, loss = 2.07, batch loss = 2.01 (39.6 examples/sec; 0.202 sec/batch; 18h:30m:08s remains)
INFO - root - 2017-12-05 06:39:49.704623: step 3210, loss = 2.07, batch loss = 2.01 (39.3 examples/sec; 0.204 sec/batch; 18h:38m:32s remains)
INFO - root - 2017-12-05 06:39:51.800178: step 3220, loss = 2.06, batch loss = 2.01 (38.0 examples/sec; 0.211 sec/batch; 19h:15m:26s remains)
INFO - root - 2017-12-05 06:39:53.885855: step 3230, loss = 2.04, batch loss = 1.98 (38.8 examples/sec; 0.206 sec/batch; 18h:52m:43s remains)
INFO - root - 2017-12-05 06:39:55.997171: step 3240, loss = 2.08, batch loss = 2.02 (37.3 examples/sec; 0.215 sec/batch; 19h:37m:24s remains)
INFO - root - 2017-12-05 06:39:58.077024: step 3250, loss = 2.05, batch loss = 1.99 (37.5 examples/sec; 0.213 sec/batch; 19h:29m:14s remains)
INFO - root - 2017-12-05 06:40:00.149100: step 3260, loss = 2.04, batch loss = 1.98 (39.1 examples/sec; 0.204 sec/batch; 18h:41m:50s remains)
INFO - root - 2017-12-05 06:40:02.234523: step 3270, loss = 2.07, batch loss = 2.01 (37.5 examples/sec; 0.213 sec/batch; 19h:30m:19s remains)
INFO - root - 2017-12-05 06:40:04.345346: step 3280, loss = 2.05, batch loss = 1.99 (38.1 examples/sec; 0.210 sec/batch; 19h:11m:44s remains)
