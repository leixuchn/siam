INFO - Siam-FC - Running command 'main'
INFO - Siam-FC - Started run with ID "177"
INFO - root - preproces -- siamese_fc_color
WARNING - root - root is not explicitly specified, using default value: None
INFO - root - For training, we use instance size 255 - 2 * 8 ...
WARNING - root - init_method is not explicitly specified, using default value: None
INFO - root - Model moving average is disabled since decay factor is 0
2017-12-09 06:59:31.335867: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-09 06:59:31.335907: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-09 06:59:31.335913: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2017-12-09 06:59:31.335919: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2017-12-09 06:59:31.335922: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
sdiufhasudf Tensor("siamese_fc/conv5/split:0", shape=(8, 8, 8, 192), dtype=float32)
Tensor("siamese_fc/conv5/def/transpose:0", shape=(8, 192, 8, 8), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/b1/weights:0' shape=(128, 192, 3, 3) dtype=float32_ref> Tensor("siamese_fc/conv5/def/transpose_1:0", shape=(8, 72, 8, 8), dtype=float32)
ddd Tensor("siamese_fc/conv5/def/b1/Relu:0", shape=(8, 6, 6, 128), dtype=float32)
Tensor("siamese_fc/conv5/def/transpose_2:0", shape=(8, 192, 8, 8), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/b2/weights:0' shape=(128, 192, 3, 3) dtype=float32_ref> Tensor("siamese_fc/conv5/def/transpose_3:0", shape=(8, 72, 8, 8), dtype=float32)
sdiufhasudf Tensor("siamese_fc_1/conv5/split:0", shape=(8, 22, 22, 192), dtype=float32)
Tensor("siamese_fc_1/conv5/def/transpose:0", shape=(8, 192, 22, 22), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/b1/weights:0' shape=(128, 192, 3, 3) dtype=float32_ref> Tensor("siamese_fc_1/conv5/def/transpose_1:0", shape=(8, 72, 22, 22), dtype=float32)
ddd Tensor("siamese_fc_1/conv5/def/b1/Relu:0", shape=(8, 20, 20, 128), dtype=float32)
Tensor("siamese_fc_1/conv5/def/transpose_2:0", shape=(8, 192, 22, 22), dtype=float32) <tf.Variable 'siamese_fc/conv5/def/b2/weights:0' shape=(128, 192, 3, 3) dtype=float32_ref> Tensor("siamese_fc_1/conv5/def/transpose_3:0", shape=(8, 72, 22, 22), dtype=float32)
Tensor("detection/add:0", shape=(8, 15, 15), dtype=float32)
2017-12-09 06:59:43.672755: I tensorflow/core/common_runtime/gpu/gpu_device.cc:887] Found device 0 with properties: 
name: Tesla K80
major: 3 minor: 7 memoryClockRate (GHz) 0.8235
pciBusID 5791:00:00.0
Total memory: 11.17GiB
Free memory: 4.02GiB
2017-12-09 06:59:43.672796: I tensorflow/core/common_runtime/gpu/gpu_device.cc:908] DMA: 0 
2017-12-09 06:59:43.672803: I tensorflow/core/common_runtime/gpu/gpu_device.cc:918] 0:   Y 
2017-12-09 06:59:43.672811: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla K80, pci bus id: 5791:00:00.0)
INFO:tensorflow:Restoring parameters from /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-only/model.ckpt-150000
INFO - tensorflow - Restoring parameters from /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-only/model.ckpt-150000
INFO - root - Starting threads 0 ...

INFO - root - training for 332500 steps
[<tf.Variable 'siamese_fc/conv1/weights:0' shape=(11, 11, 3, 96) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv1/BatchNorm/beta:0' shape=(96,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv1/BatchNorm/gamma:0' shape=(96,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv1/BatchNorm/moving_mean:0' shape=(96,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv1/BatchNorm/moving_variance:0' shape=(96,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv2/b1/weights:0' shape=(5, 5, 48, 128) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv2/b1/BatchNorm/beta:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv2/b1/BatchNorm/gamma:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv2/b1/BatchNorm/moving_mean:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv2/b1/BatchNorm/moving_variance:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv2/b2/weights:0' shape=(5, 5, 48, 128) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv2/b2/BatchNorm/beta:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv2/b2/BatchNorm/gamma:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv2/b2/BatchNorm/moving_mean:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv2/b2/BatchNorm/moving_variance:0' shape=(128,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv3/weights:0' shape=(3, 3, 256, 384) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv3/BatchNorm/beta:0' shape=(384,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv3/BatchNorm/gamma:0' shape=(384,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv3/BatchNorm/moving_mean:0' shape=(384,) dtype=float32_ref>, <tf.Variable 'siamese_fc/conv3/BatchNorm/moving_variance:0' shape=(384,) dtype=float32_ref>]
INFO - root - 2017-12-09 07:00:12.910052: step 0, loss = 0.82, batch loss = 0.69 (0.4 examples/sec; 18.547 sec/batch; 1712h:58m:46s remains)
2017-12-09 07:00:14.309654: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.00012333484 0.00013083941 0.00013708408 0.00014497043 0.00015612035 0.00016674981 0.00017326478 0.00017687795 0.00017426164 0.00016669215 0.00015576083 0.00014655168 0.00014091416 0.00013873207 0.00013919627][0.00012541116 0.00013717581 0.00015020747 0.00016714877 0.00018815346 0.00020683451 0.00021852706 0.00022437959 0.00021941777 0.0002062133 0.0001874669 0.00017176989 0.00015927413 0.00015155599 0.00014782352][0.00012808162 0.00014572803 0.00016728512 0.00019324837 0.00022153069 0.00024715276 0.00026918895 0.00028054035 0.00027252498 0.00025146443 0.00022380216 0.0002020509 0.00018173768 0.00016535922 0.00015532228][0.0001364921 0.00015959074 0.00018860165 0.00021844778 0.00025211519 0.0002878684 0.00032586011 0.00035076257 0.00034268908 0.00031256527 0.00027304387 0.00023994733 0.00020715292 0.00018092491 0.00016423094][0.00015116346 0.00017785798 0.00020950632 0.00023896195 0.000279053 0.00033079184 0.000394852 0.00044171457 0.00042924454 0.00037863193 0.0003208303 0.00027320354 0.00022874087 0.00019426018 0.00017270086][0.00016664893 0.00019545431 0.00022814758 0.00025933119 0.0003086513 0.00037871345 0.00047063411 0.00054152106 0.00050929916 0.00042711091 0.00034917 0.00029114864 0.00024147419 0.00020392895 0.00017958858][0.00018152046 0.0002105332 0.00024353925 0.00027793224 0.00033415222 0.00041825036 0.00053861988 0.00062970095 0.000556672 0.00044301595 0.00035528204 0.00029525376 0.00024763984 0.00021058871 0.00018480793][0.00019387402 0.00022263895 0.00025293804 0.00028630521 0.00034183284 0.00042689609 0.00054351735 0.00061089447 0.00051784015 0.00041600844 0.00034256341 0.00029004886 0.00024782802 0.0002122704 0.00018734302][0.00020543675 0.00023327085 0.0002588105 0.00028475226 0.00032891531 0.00038980087 0.00046300987 0.00048952317 0.00042784971 0.00036792262 0.00031996483 0.00028065295 0.00024377067 0.00021087147 0.00018702654][0.00021117182 0.00023433731 0.00025307277 0.0002744096 0.0003059876 0.00034101642 0.00038266243 0.00039684147 0.00036615209 0.00033283239 0.00030154779 0.00027060541 0.00023651574 0.00020613837 0.00018444205][0.00020637772 0.00022347893 0.00023909287 0.000257113 0.00027851315 0.00029804109 0.00032253715 0.00033332934 0.00032104913 0.00030058893 0.00027807348 0.00025389259 0.00022530578 0.00019982657 0.00018171826][0.00019979938 0.00021401703 0.0002272648 0.00023784061 0.0002491764 0.00025855505 0.00027471411 0.00028690076 0.00028492644 0.00027035768 0.00025379166 0.00023552724 0.00021232263 0.00019168807 0.0001785115][0.00018896264 0.0002016013 0.00021079404 0.00021603376 0.00022094572 0.00022592671 0.00024131572 0.00025758173 0.00026204879 0.00025219674 0.00023852823 0.00022170968 0.00020089834 0.00018416632 0.00017480443][0.00017723594 0.00018777527 0.00019362949 0.00019782405 0.00019953788 0.000204737 0.00022180617 0.00024252172 0.00025228862 0.0002446392 0.00023202119 0.00021662445 0.000195954 0.00018007567 0.00017217583][0.0001726507 0.00018193905 0.00018608902 0.00018998077 0.00019086339 0.00019897809 0.00021959124 0.00024294636 0.00025389303 0.00024560274 0.0002323651 0.00021673643 0.00019623779 0.00018021924 0.00017248576]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-momentum-lr0.0001-clip-zeroinit-conv1-3init-from-scratch/model.ckpt-0 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-momentum-lr0.0001-clip-zeroinit-conv1-3init-from-scratch/model.ckpt-0 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-09 07:00:24.423360: step 10, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.860 sec/batch; 79h:27m:22s remains)
INFO - root - 2017-12-09 07:00:33.017730: step 20, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.876 sec/batch; 80h:51m:33s remains)
INFO - root - 2017-12-09 07:00:41.446171: step 30, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.850 sec/batch; 78h:32m:38s remains)
INFO - root - 2017-12-09 07:00:50.157647: step 40, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.845 sec/batch; 78h:04m:47s remains)
INFO - root - 2017-12-09 07:00:58.832503: step 50, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.887 sec/batch; 81h:57m:08s remains)
INFO - root - 2017-12-09 07:01:07.402557: step 60, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.854 sec/batch; 78h:49m:05s remains)
INFO - root - 2017-12-09 07:01:15.889410: step 70, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.849 sec/batch; 78h:26m:09s remains)
INFO - root - 2017-12-09 07:01:24.365317: step 80, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.851 sec/batch; 78h:32m:08s remains)
INFO - root - 2017-12-09 07:01:32.942502: step 90, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 79h:44m:03s remains)
INFO - root - 2017-12-09 07:01:41.614098: step 100, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 80h:23m:09s remains)
2017-12-09 07:01:42.482741: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.00013192582 0.00013662154 0.00014599909 0.00015959328 0.00017123799 0.00017321231 0.00017040639 0.00016738812 0.00016326376 0.00016234862 0.00016233002 0.00016399997 0.00016519197 0.00016772636 0.00016758933][0.00013173837 0.00013897561 0.00015137537 0.00016812953 0.00017995993 0.00018092169 0.00017817467 0.00017714674 0.0001760852 0.00017594254 0.00017529727 0.00017248874 0.00017044357 0.00017116388 0.00016851502][0.00012929419 0.00013982321 0.00015543433 0.00017296399 0.0001835322 0.00018274499 0.00018089652 0.00018126243 0.00018338778 0.00018481379 0.00018358212 0.00017998715 0.00017600399 0.00017617457 0.00017134583][0.00012826853 0.00014023265 0.00015682727 0.00017128371 0.00017878487 0.0001778162 0.00017695976 0.00017983516 0.00018445226 0.00018620293 0.00018502434 0.00018155386 0.00017761033 0.00017886795 0.00017512472][0.00012719315 0.00013841128 0.00015438891 0.00016592826 0.0001721372 0.0001728534 0.0001736835 0.00017950509 0.00018448022 0.00018529905 0.00018343944 0.00017914012 0.00017572449 0.00017844548 0.00017713849][0.00012584732 0.00013522149 0.00014901541 0.00015941536 0.00016660817 0.00016973959 0.00017296108 0.00018155223 0.00018877418 0.00018849364 0.0001844371 0.00017893426 0.00017481591 0.00017650252 0.00017656269][0.00012563588 0.00013282034 0.00014444911 0.00015391705 0.00016232084 0.00016812261 0.00017495535 0.00018834045 0.00019974708 0.00019821333 0.00019082203 0.00018375597 0.0001783694 0.00017824843 0.00017771401][0.0001256176 0.00013073481 0.00014098246 0.00015135192 0.00015989374 0.00016605674 0.00017476975 0.00019032137 0.0002039565 0.00020258536 0.00019289681 0.00018518843 0.00018082859 0.00017918466 0.00017742458][0.00012527479 0.00012903697 0.00013808321 0.00014807822 0.00015522985 0.00016083701 0.0001683367 0.0001817981 0.00019435302 0.0001943095 0.00018777866 0.00018295347 0.00018100896 0.00017751768 0.00017372983][0.00012550263 0.00012933885 0.00013758382 0.00014701327 0.00015338229 0.00015934641 0.00016699545 0.0001769255 0.00018391901 0.00018264845 0.00017886826 0.00017877854 0.00018080219 0.00017741986 0.00017341254][0.00012632979 0.00013128963 0.0001407878 0.00015098022 0.00015789914 0.00016352338 0.00017039003 0.00017805128 0.00018065381 0.00017655836 0.00017270612 0.00017674812 0.00018107735 0.00017736776 0.00017355042][0.00012783294 0.00013314335 0.00014229615 0.00015191612 0.00015834538 0.00016303518 0.0001695364 0.00017415307 0.00017458304 0.00016947734 0.00016632094 0.00017092266 0.00017560847 0.00017292319 0.00016970064][0.00012983795 0.00013545243 0.00014289087 0.00014953308 0.0001537401 0.00015731914 0.0001619391 0.00016372427 0.00016386335 0.00016034745 0.00015701662 0.0001581878 0.0001616011 0.00016096812 0.00016031423][0.00013100245 0.0001357577 0.00014169328 0.00014640515 0.00014929398 0.00015102973 0.00015186358 0.00015066158 0.00015041677 0.00014817654 0.00014578449 0.00014572233 0.00014788806 0.00014921046 0.00014991548][0.00013105772 0.00013412273 0.00013794823 0.0001410658 0.00014334871 0.00014435373 0.00014420881 0.00014229414 0.00014049478 0.000138631 0.000137134 0.00013717021 0.00013900094 0.00014017157 0.00014054857]]...]
INFO - root - 2017-12-09 07:01:51.009405: step 110, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.848 sec/batch; 78h:20m:12s remains)
INFO - root - 2017-12-09 07:01:59.403396: step 120, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.872 sec/batch; 80h:31m:48s remains)
INFO - root - 2017-12-09 07:02:07.904352: step 130, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.859 sec/batch; 79h:16m:08s remains)
INFO - root - 2017-12-09 07:02:16.489271: step 140, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.861 sec/batch; 79h:28m:15s remains)
INFO - root - 2017-12-09 07:02:25.250219: step 150, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.882 sec/batch; 81h:22m:51s remains)
INFO - root - 2017-12-09 07:02:33.740560: step 160, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.843 sec/batch; 77h:50m:43s remains)
INFO - root - 2017-12-09 07:02:42.430712: step 170, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.897 sec/batch; 82h:47m:56s remains)
INFO - root - 2017-12-09 07:02:51.130677: step 180, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.904 sec/batch; 83h:25m:25s remains)
INFO - root - 2017-12-09 07:02:59.896825: step 190, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.875 sec/batch; 80h:48m:55s remains)
INFO - root - 2017-12-09 07:03:08.672508: step 200, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.891 sec/batch; 82h:16m:13s remains)
2017-12-09 07:03:09.505445: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.00030297367 0.00031058691 0.00030492875 0.00029195615 0.00027800311 0.0002735857 0.00027791169 0.00029031513 0.00030326162 0.00030295417 0.00028461966 0.000252907 0.00022591665 0.00021251361 0.00021454044][0.00035078308 0.00037258837 0.00037183735 0.000355435 0.0003401812 0.00033786171 0.00034837244 0.00036720379 0.00038449679 0.00038474993 0.0003599108 0.00031480088 0.00027507552 0.00025381724 0.00025265515][0.00038749477 0.00042192481 0.00042488144 0.00040660071 0.00039086395 0.00039182373 0.00041101928 0.0004350004 0.00045322071 0.00045363276 0.0004268745 0.00037586241 0.00032546825 0.00029423973 0.00028435915][0.00041924941 0.00046081148 0.00045888702 0.00043772743 0.00042042919 0.00042456581 0.00045023308 0.00047975229 0.00049902039 0.00049762521 0.00047144812 0.00041918349 0.00036386275 0.00032746152 0.00031697913][0.00044161125 0.00048770322 0.00048398995 0.00046175442 0.00044337957 0.00045086924 0.0004849293 0.00052474166 0.00054461655 0.00053393986 0.00050131383 0.00044655084 0.00038861172 0.000348715 0.00033756357][0.00044044093 0.00049079693 0.00049098249 0.00047121252 0.0004542582 0.00046948314 0.0005180788 0.00057101826 0.00059175026 0.00056513125 0.00051389961 0.0004521666 0.00039813993 0.00036155782 0.00034899791][0.00044150403 0.00049384724 0.00050124049 0.00048319908 0.00046803532 0.00049007352 0.00055312325 0.00063274271 0.00066327892 0.00061408797 0.00053866184 0.00046900666 0.00041795088 0.0003855062 0.00037314364][0.00046657739 0.00051997957 0.00052912236 0.00051311677 0.00049935264 0.00052855531 0.00061194855 0.00072813209 0.00077559776 0.00069302577 0.00058581686 0.00050745066 0.00046103983 0.00043268473 0.00041454419][0.00051713124 0.00056829792 0.00057817344 0.00056365604 0.00054514565 0.00056727027 0.000640058 0.00075013685 0.00079863862 0.000717597 0.000604692 0.00052736624 0.000488705 0.00046628903 0.00044707581][0.00054052757 0.00058437546 0.00059213472 0.00057483913 0.00055359723 0.00056253508 0.00061305932 0.00068335584 0.00071578147 0.00066696858 0.00058157439 0.00051753962 0.00048425124 0.00046432271 0.00045087436][0.00052639877 0.0005669751 0.00057872751 0.00056837371 0.00054959941 0.00055454019 0.000588756 0.00062707165 0.00064166472 0.0006101788 0.00055313762 0.00050512946 0.00048045372 0.00046028715 0.00044559353][0.00050199975 0.0005409001 0.00055245665 0.00055306219 0.00054302136 0.00054114 0.00056031835 0.00057875464 0.00057911064 0.0005566827 0.0005199911 0.00048570122 0.00046667163 0.00045043943 0.00043641572][0.00047323268 0.00050706771 0.00051706377 0.00051947858 0.00051543216 0.00051227084 0.00052110362 0.00052672328 0.00051881769 0.00050188246 0.00047818845 0.00045465914 0.00044068196 0.00042706481 0.00041608381][0.00042682979 0.00045603636 0.00046447714 0.00046205681 0.00045722025 0.00045629931 0.00045581628 0.00045516764 0.00044971728 0.00044254123 0.00043097636 0.00041231589 0.00040061155 0.00039048877 0.00038313819][0.00036454902 0.00038896079 0.00039305302 0.00038829411 0.00038379381 0.00037919293 0.00037384502 0.00037056988 0.00036834346 0.00036757922 0.00036340053 0.00035088728 0.00034038644 0.00033265725 0.00032796786]]...]
INFO - root - 2017-12-09 07:03:18.183760: step 210, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.872 sec/batch; 80h:26m:46s remains)
INFO - root - 2017-12-09 07:03:26.984899: step 220, loss = 0.82, batch loss = 0.69 (8.7 examples/sec; 0.917 sec/batch; 84h:38m:57s remains)
INFO - root - 2017-12-09 07:03:35.698097: step 230, loss = 0.82, batch loss = 0.69 (8.8 examples/sec; 0.908 sec/batch; 83h:46m:10s remains)
INFO - root - 2017-12-09 07:03:44.472699: step 240, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.880 sec/batch; 81h:10m:33s remains)
INFO - root - 2017-12-09 07:03:53.284096: step 250, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.878 sec/batch; 81h:03m:55s remains)
INFO - root - 2017-12-09 07:04:01.887802: step 260, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.890 sec/batch; 82h:08m:44s remains)
INFO - root - 2017-12-09 07:04:10.620644: step 270, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 80h:11m:18s remains)
INFO - root - 2017-12-09 07:04:19.304417: step 280, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.877 sec/batch; 80h:58m:19s remains)
INFO - root - 2017-12-09 07:04:27.790071: step 290, loss = 0.82, batch loss = 0.69 (9.7 examples/sec; 0.829 sec/batch; 76h:28m:24s remains)
INFO - root - 2017-12-09 07:04:36.257346: step 300, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.877 sec/batch; 80h:55m:19s remains)
2017-12-09 07:04:37.100267: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.00014521695 0.00017936711 0.00021908747 0.00026097719 0.00030455872 0.00034327968 0.00038011099 0.00041583608 0.00044259345 0.00044688242 0.00043436507 0.0004083325 0.00038187139 0.00036901448 0.00036771808][0.00016877962 0.00020450975 0.00024324091 0.00028443357 0.00032921054 0.00036653082 0.00039956515 0.00043322131 0.00046002807 0.00046636775 0.00045436452 0.0004261455 0.00039712014 0.00038005444 0.00037562629][0.0001994038 0.00023317858 0.00026767273 0.0003051302 0.00034667866 0.00037945577 0.00040297885 0.00042634792 0.00044368397 0.00045002619 0.00044196448 0.00042071484 0.00039691717 0.00037897882 0.00037438792][0.00024237123 0.00026917181 0.00029419753 0.00032249506 0.00035657303 0.00038093395 0.00039477093 0.00040685502 0.00041685603 0.00042395687 0.00042478446 0.0004182416 0.00040690508 0.00039450845 0.00038840654][0.00029727313 0.00031880496 0.00033632235 0.00035822237 0.0003834739 0.00039973255 0.00040623697 0.00041009267 0.00041582834 0.00042367348 0.00043107066 0.00043558163 0.00043637271 0.00043205556 0.00042576468][0.00037414816 0.00039087093 0.00039611969 0.00040522494 0.00041914731 0.00042811729 0.00043018541 0.00043039242 0.00043360755 0.00044230418 0.00045333852 0.00046826783 0.00048142837 0.00048721372 0.00048420738][0.00044298652 0.0004595365 0.00045841225 0.00045977524 0.00046934816 0.00047614757 0.00047427815 0.00047023306 0.00046891902 0.00047406915 0.00048549459 0.00050563674 0.00052591367 0.00053931563 0.00054193917][0.00046431395 0.00048260906 0.00047851319 0.00047419465 0.00048164433 0.00048694038 0.00048559235 0.00048208665 0.00047905097 0.00048422287 0.00049755082 0.00051884679 0.00054047868 0.00055918767 0.00056568522][0.00043862453 0.00045148758 0.00043973766 0.00042278162 0.00041530404 0.00041259889 0.00041353807 0.0004138606 0.00041553669 0.00042358469 0.00043843238 0.00045739385 0.00047663128 0.00049864029 0.0005143481][0.00038842138 0.000395228 0.00037906886 0.00035614046 0.00034065929 0.00033125386 0.00032944558 0.00033154705 0.00033435039 0.00033964479 0.00034879075 0.00035879036 0.00037057945 0.00038616333 0.00040444173][0.00031485196 0.00031796584 0.00030177907 0.00028066445 0.00026680971 0.00025884912 0.00025554767 0.00025551484 0.0002554293 0.00025574095 0.00025543157 0.00025477022 0.00025869004 0.00027013061 0.00029034208][0.00022609712 0.00022959492 0.00022041601 0.00020702893 0.00019932227 0.0001954648 0.0001932102 0.00019142548 0.00018914904 0.00018516535 0.00017983971 0.0001747308 0.00017461304 0.00018402381 0.00020410083][0.000155818 0.00016085072 0.00015857736 0.00015402792 0.00015137589 0.00015092273 0.00015192095 0.00015181862 0.00014900719 0.00014386221 0.00013790609 0.00013224894 0.00013088752 0.00013718259 0.00015386564][0.00011061475 0.00011466923 0.00011603392 0.00011623064 0.00011742491 0.00011924648 0.00012161949 0.00012245956 0.00012102549 0.00011764517 0.0001137388 0.0001102211 0.00010846024 0.00011165163 0.00012336954][9.3897492e-05 9.653742e-05 9.7252108e-05 9.7895645e-05 9.9317855e-05 0.00010104992 0.00010319044 0.0001044237 0.00010420474 0.00010263343 0.00010060481 9.863577e-05 9.7444135e-05 9.9742247e-05 0.00010730085]]...]
INFO - root - 2017-12-09 07:04:45.717792: step 310, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.865 sec/batch; 79h:48m:14s remains)
INFO - root - 2017-12-09 07:04:54.303396: step 320, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.896 sec/batch; 82h:38m:56s remains)
INFO - root - 2017-12-09 07:05:02.775714: step 330, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 79h:45m:18s remains)
INFO - root - 2017-12-09 07:05:11.392557: step 340, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.874 sec/batch; 80h:40m:35s remains)
INFO - root - 2017-12-09 07:05:20.111403: step 350, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.886 sec/batch; 81h:43m:37s remains)
INFO - root - 2017-12-09 07:05:28.718373: step 360, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.842 sec/batch; 77h:42m:37s remains)
INFO - root - 2017-12-09 07:05:37.292563: step 370, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.884 sec/batch; 81h:33m:05s remains)
INFO - root - 2017-12-09 07:05:45.894068: step 380, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.861 sec/batch; 79h:24m:28s remains)
INFO - root - 2017-12-09 07:05:54.415297: step 390, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.899 sec/batch; 82h:57m:30s remains)
INFO - root - 2017-12-09 07:06:03.111260: step 400, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.879 sec/batch; 81h:07m:00s remains)
2017-12-09 07:06:04.026575: I tensorflow/core/kernels/logging_ops.cc:79] [[[8.0613e-05 8.2914681e-05 8.2657076e-05 8.23605e-05 8.271508e-05 8.4433777e-05 8.876605e-05 9.1965936e-05 9.7168937e-05 0.000106992 0.00011465833 0.00011483933 0.00010525429 9.4030227e-05 8.5068496e-05][9.0688547e-05 9.3060364e-05 9.214662e-05 9.1993556e-05 9.2751456e-05 9.401729e-05 9.878516e-05 0.00010387792 0.00011281942 0.00012602229 0.00013743532 0.00013966639 0.00012773374 0.0001145189 0.00010264136][9.9316225e-05 0.00010339193 0.00010379613 0.0001030573 0.00010324164 0.00010540259 0.00011032099 0.00011541416 0.00012579387 0.00014141458 0.00015718433 0.00016128323 0.00014876165 0.00013408872 0.00012015923][0.00010595978 0.00011424783 0.00011621181 0.00011614653 0.00011700232 0.00011922064 0.0001238665 0.00012850645 0.00013823745 0.00015394947 0.00017031339 0.00017519895 0.00016279251 0.00014875588 0.00013384978][0.00011030417 0.00012593984 0.00013562608 0.00014074976 0.00014324761 0.00014341474 0.00014325726 0.00014464445 0.00015079045 0.00016479185 0.00017885285 0.0001816875 0.00016950727 0.00015652837 0.00014098512][0.00010967028 0.00012744413 0.00014257949 0.00015409522 0.00016369125 0.00016871055 0.00016757772 0.00016495418 0.00016555283 0.00017583286 0.00018550245 0.00018321093 0.00017044212 0.0001581437 0.00014196303][0.0001077478 0.0001234543 0.00013541033 0.00014670157 0.00015729001 0.00016675702 0.0001727129 0.0001756711 0.00017727962 0.00018433484 0.00018937963 0.00018144805 0.00016734567 0.00015543419 0.00013753091][0.0001126358 0.00012571136 0.00013536951 0.00014210999 0.00014834001 0.00015487087 0.00016053536 0.00016641148 0.00017118138 0.00018126785 0.00018730998 0.00017927634 0.00016415748 0.00015168464 0.00013226713][0.00012653595 0.00013453195 0.00013969534 0.00014101692 0.0001418367 0.00014280426 0.00014316566 0.00014592243 0.00015183857 0.00016427293 0.00017353722 0.00016850422 0.00015538205 0.00014351514 0.00012516721][0.00014085702 0.00014689773 0.00014521318 0.00013828036 0.00013161779 0.00012886465 0.00012644578 0.00012655773 0.00013014629 0.00014087842 0.00015099291 0.00014976824 0.00014060909 0.0001313987 0.00011613638][0.0001441062 0.00014909562 0.00014445662 0.00013306936 0.00012204068 0.00011449836 0.00011009003 0.00010934074 0.00011228375 0.00012048785 0.00012948069 0.00013066843 0.0001242619 0.00011745576 0.0001063736][0.00013686338 0.00014252357 0.00013952481 0.00012891543 0.00011585734 0.00010546247 9.8292e-05 9.52973e-05 9.7501957e-05 0.00010382835 0.00011114229 0.00011437519 0.00011068318 0.00010545933 9.6456271e-05][0.0001251478 0.00012906342 0.00012737453 0.00011984697 0.00010983097 0.00010008161 9.2881754e-05 8.8585846e-05 8.7750552e-05 9.0698552e-05 9.5279705e-05 9.8117453e-05 9.7223383e-05 9.4611554e-05 8.7639746e-05][0.00011099192 0.00011323429 0.00011196927 0.00010727497 0.00010023805 9.28812e-05 8.600919e-05 8.0839127e-05 7.8875608e-05 7.960981e-05 8.1883831e-05 8.3637184e-05 8.4009582e-05 8.3566723e-05 7.917636e-05][9.5989075e-05 9.7090146e-05 9.5796408e-05 9.2922674e-05 8.8283552e-05 8.2431456e-05 7.7362522e-05 7.3293326e-05 7.1346221e-05 7.1199574e-05 7.2115916e-05 7.3034622e-05 7.3647556e-05 7.3850984e-05 7.1543422e-05]]...]
INFO - root - 2017-12-09 07:06:12.651044: step 410, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.902 sec/batch; 83h:13m:19s remains)
INFO - root - 2017-12-09 07:06:21.273158: step 420, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.873 sec/batch; 80h:34m:10s remains)
INFO - root - 2017-12-09 07:06:29.661718: step 430, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.844 sec/batch; 77h:53m:02s remains)
INFO - root - 2017-12-09 07:06:38.134859: step 440, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 80h:05m:47s remains)
INFO - root - 2017-12-09 07:06:46.756972: step 450, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 80h:05m:33s remains)
INFO - root - 2017-12-09 07:06:55.309418: step 460, loss = 0.82, batch loss = 0.69 (10.7 examples/sec; 0.745 sec/batch; 68h:40m:25s remains)
INFO - root - 2017-12-09 07:07:03.938378: step 470, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.874 sec/batch; 80h:39m:14s remains)
INFO - root - 2017-12-09 07:07:12.627221: step 480, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.872 sec/batch; 80h:24m:44s remains)
INFO - root - 2017-12-09 07:07:21.275716: step 490, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.879 sec/batch; 81h:03m:23s remains)
INFO - root - 2017-12-09 07:07:29.933947: step 500, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.842 sec/batch; 77h:41m:43s remains)
2017-12-09 07:07:30.881203: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.0001098533 0.00012037444 0.00012875423 0.00013649453 0.00014494873 0.00015706377 0.0001773807 0.00020884404 0.00025205064 0.00030765613 0.0003699346 0.00043496562 0.000496579 0.000546622 0.00058253849][0.00013703173 0.00015528424 0.00016988128 0.00018312382 0.00019878468 0.00021924337 0.00024832058 0.0002925455 0.00034972638 0.00041639461 0.00048649753 0.00055812142 0.00061922462 0.00066004595 0.00068045297][0.00017948048 0.00020935069 0.00023229005 0.00024945103 0.00027114848 0.00029837756 0.00033729846 0.00039128156 0.00045662216 0.00052933162 0.00060528284 0.00067689957 0.00073090527 0.00075614639 0.00075385452][0.0002448645 0.0002874154 0.00031625631 0.00033485211 0.00035966476 0.00039158002 0.00043478757 0.00048909034 0.0005543892 0.00062565831 0.00070046779 0.00076613296 0.00080936961 0.0008190173 0.00079561712][0.00032999541 0.0003885435 0.00042382712 0.00044351903 0.00046770676 0.00049625157 0.00053345633 0.00057567569 0.00062633742 0.00068646611 0.00075356505 0.00081152288 0.00084449485 0.00084151421 0.00080583][0.00042638392 0.00049616268 0.00053900515 0.00055686984 0.00057236565 0.000588797 0.00060889235 0.00063254364 0.000661857 0.00070596026 0.00076566433 0.00082149263 0.00084835861 0.00084271084 0.00080550538][0.0005184536 0.00059068948 0.00063251203 0.00064172753 0.00064229179 0.000640977 0.00064141757 0.00065140805 0.00066736847 0.00069868623 0.00074996316 0.00080487737 0.00083111622 0.000824956 0.00078853866][0.00059406104 0.00065903884 0.00069091714 0.00068471505 0.00066535972 0.00064308185 0.00062770053 0.00063052366 0.000639083 0.00065487792 0.00069404073 0.00074193976 0.00076799624 0.00076349615 0.00073299336][0.00063564483 0.00068611238 0.00070452335 0.00068603334 0.000649094 0.000607013 0.0005750705 0.0005662379 0.00057122373 0.0005838605 0.00061539363 0.00065442204 0.00067576655 0.00066971779 0.00064235734][0.00064279238 0.00067525491 0.00067913026 0.00065152353 0.00060590229 0.00055559643 0.00051810767 0.00050314388 0.0005033178 0.00051256083 0.00053781935 0.00056517444 0.00057719479 0.00056768808 0.00053886668][0.00062484737 0.00063565787 0.00062118564 0.00058585283 0.00053997111 0.0004965103 0.00046634121 0.00045244236 0.0004485005 0.00045460594 0.00047062238 0.00048355837 0.00048555137 0.00047081197 0.00044066636][0.000575443 0.0005712008 0.00054363126 0.00050736108 0.00047180208 0.00044150287 0.00042001641 0.00040845902 0.00040394458 0.00040670944 0.00040985865 0.0004075511 0.0003984519 0.00037952125 0.00035093457][0.00050428056 0.0004945969 0.00046203643 0.00042903406 0.00040116918 0.00037911112 0.00036450615 0.00035493297 0.00034785434 0.00034650578 0.00034320916 0.0003328046 0.00031616379 0.00029544753 0.00027172125][0.00042856843 0.00041686936 0.00038503943 0.000354685 0.00032968406 0.000311557 0.00030112066 0.00029369837 0.0002864002 0.00028187729 0.00027619669 0.00026578602 0.00025068896 0.00023387588 0.00021932635][0.00035529307 0.00034338841 0.00031353539 0.00028660847 0.00026598465 0.00025226557 0.00024560318 0.00024129248 0.00023554999 0.0002318365 0.00022853338 0.00022339007 0.00021527159 0.00020532399 0.00019804567]]...]
INFO - root - 2017-12-09 07:07:39.540884: step 510, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.879 sec/batch; 81h:01m:22s remains)
INFO - root - 2017-12-09 07:07:48.042441: step 520, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.882 sec/batch; 81h:22m:45s remains)
INFO - root - 2017-12-09 07:07:56.674111: step 530, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.866 sec/batch; 79h:52m:05s remains)
INFO - root - 2017-12-09 07:08:05.436687: step 540, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.890 sec/batch; 82h:02m:49s remains)
INFO - root - 2017-12-09 07:08:14.074889: step 550, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.860 sec/batch; 79h:19m:26s remains)
INFO - root - 2017-12-09 07:08:22.607740: step 560, loss = 0.82, batch loss = 0.69 (10.2 examples/sec; 0.786 sec/batch; 72h:30m:21s remains)
INFO - root - 2017-12-09 07:08:31.200896: step 570, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.830 sec/batch; 76h:33m:36s remains)
INFO - root - 2017-12-09 07:08:39.801345: step 580, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 79h:28m:12s remains)
INFO - root - 2017-12-09 07:08:48.494388: step 590, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 79h:55m:08s remains)
INFO - root - 2017-12-09 07:08:57.215803: step 600, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.832 sec/batch; 76h:43m:14s remains)
2017-12-09 07:08:58.032109: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.00030257524 0.00031180945 0.00030312772 0.00028401241 0.00026211748 0.000248187 0.000243927 0.00024157054 0.00022146106 0.00018905572 0.00015786818 0.00013012724 0.00010689389 9.1893438e-05 8.01628e-05][0.00028269054 0.00029320785 0.00028854015 0.0002729072 0.00025247509 0.00023852264 0.00023431919 0.00023285067 0.00021535918 0.00018498331 0.00015570433 0.00012858385 0.00010575762 9.0523114e-05 7.9407408e-05][0.000259574 0.00027018809 0.00026833432 0.00025374562 0.00023367196 0.00021998656 0.00021672848 0.00021492931 0.00019894866 0.00017358361 0.00014751175 0.00012221361 0.00010114754 8.5289845e-05 7.4656389e-05][0.00022599392 0.00023575447 0.00023649253 0.00022704742 0.00021143408 0.00020070774 0.00019895897 0.00019635394 0.00018263161 0.00016015703 0.00013701052 0.00011472555 9.5341973e-05 7.991054e-05 6.9833259e-05][0.00018735029 0.00019839377 0.00020195353 0.00019712749 0.00018939623 0.00018564102 0.00018543209 0.00018264831 0.00016949041 0.00014895368 0.00012790105 0.00010813851 9.1419264e-05 7.7855206e-05 6.8188754e-05][0.00015886121 0.00017206764 0.00018001485 0.00018187371 0.00018299023 0.00018478847 0.00018548308 0.00018153137 0.00016679768 0.00014660806 0.00012645293 0.00010656775 9.0994763e-05 7.9664707e-05 7.1268623e-05][0.0001401241 0.00016156677 0.00017762795 0.00018628586 0.00019247973 0.00019594363 0.00019364223 0.00018608225 0.00017134839 0.00015255874 0.00013191751 0.00011050567 9.4215349e-05 8.3148829e-05 7.47622e-05][0.00012413121 0.00014909991 0.00017249527 0.00018699825 0.00019678434 0.00020222522 0.00019864345 0.00018960694 0.00017560048 0.00015765568 0.00013686405 0.00011543477 9.9195815e-05 8.7037086e-05 7.7912053e-05][0.00010175087 0.00012220768 0.00014268835 0.00015716243 0.00016768894 0.00017532897 0.0001753603 0.00016999869 0.00016030444 0.00014541307 0.00012819054 0.00011121321 9.7956057e-05 8.730543e-05 7.91839e-05][8.6164378e-05 9.9418343e-05 0.0001128505 0.00012367145 0.00013244098 0.00013771093 0.00013892222 0.00013761259 0.00013349962 0.00012523937 0.00011420832 0.00010406618 9.4941846e-05 8.6574029e-05 7.9583187e-05][7.7812918e-05 8.4991982e-05 9.27043e-05 0.00010050285 0.00010756608 0.00011142387 0.00011311411 0.00011348711 0.00011223767 0.00010808573 0.00010131747 9.4644922e-05 8.8764464e-05 8.31849e-05 7.8194746e-05][7.3200521e-05 7.7498386e-05 8.1476981e-05 8.6185406e-05 9.0711095e-05 9.2626193e-05 9.4756047e-05 9.5790914e-05 9.5541007e-05 9.3191629e-05 8.9542555e-05 8.5606567e-05 8.15702e-05 7.7830664e-05 7.4979383e-05][6.8943329e-05 7.1890914e-05 7.5195385e-05 7.90109e-05 8.2009137e-05 8.3008526e-05 8.4162108e-05 8.5098523e-05 8.54836e-05 8.4594125e-05 8.2048246e-05 7.8655154e-05 7.569154e-05 7.35942e-05 7.20213e-05][6.5238368e-05 6.7981324e-05 7.0937131e-05 7.4081086e-05 7.5983204e-05 7.654255e-05 7.7918041e-05 7.7750679e-05 7.8012985e-05 7.7999794e-05 7.6484423e-05 7.36608e-05 7.0771661e-05 6.9909904e-05 6.9447909e-05][6.2563304e-05 6.4178806e-05 6.6359527e-05 6.8246321e-05 6.9664529e-05 7.00153e-05 7.0499889e-05 6.9373287e-05 6.8953392e-05 6.9887239e-05 6.9977337e-05 6.8559522e-05 6.6724169e-05 6.6955545e-05 6.7427136e-05]]...]
INFO - root - 2017-12-09 07:09:06.644003: step 610, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.866 sec/batch; 79h:50m:36s remains)
INFO - root - 2017-12-09 07:09:15.047304: step 620, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.853 sec/batch; 78h:37m:12s remains)
INFO - root - 2017-12-09 07:09:23.461259: step 630, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.854 sec/batch; 78h:44m:46s remains)
INFO - root - 2017-12-09 07:09:32.154128: step 640, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.855 sec/batch; 78h:50m:57s remains)
INFO - root - 2017-12-09 07:09:40.761723: step 650, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.863 sec/batch; 79h:33m:50s remains)
INFO - root - 2017-12-09 07:09:49.514174: step 660, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.901 sec/batch; 83h:02m:36s remains)
INFO - root - 2017-12-09 07:09:58.080748: step 670, loss = 0.82, batch loss = 0.69 (8.8 examples/sec; 0.910 sec/batch; 83h:50m:09s remains)
INFO - root - 2017-12-09 07:10:06.804521: step 680, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.880 sec/batch; 81h:06m:06s remains)
INFO - root - 2017-12-09 07:10:15.477955: step 690, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.874 sec/batch; 80h:33m:30s remains)
INFO - root - 2017-12-09 07:10:24.096892: step 700, loss = 0.82, batch loss = 0.69 (9.8 examples/sec; 0.820 sec/batch; 75h:33m:03s remains)
2017-12-09 07:10:24.922752: I tensorflow/core/kernels/logging_ops.cc:79] [[[8.0281447e-05 8.0411737e-05 8.08207e-05 8.1463215e-05 8.1157043e-05 7.9002784e-05 7.7026685e-05 7.6610246e-05 7.7601821e-05 7.9165133e-05 8.0869213e-05 8.1579972e-05 8.1298218e-05 8.0862177e-05 8.0794896e-05][8.0526523e-05 8.0681217e-05 8.1334416e-05 8.2674713e-05 8.2093095e-05 7.8210578e-05 7.4106036e-05 7.3111791e-05 7.4289295e-05 7.6410666e-05 7.8812336e-05 8.04367e-05 8.1370934e-05 8.1764869e-05 8.200312e-05][7.9748686e-05 8.0548489e-05 8.2624007e-05 8.5557534e-05 8.5187865e-05 8.05438e-05 7.5248019e-05 7.3038493e-05 7.2904724e-05 7.3814394e-05 7.5973854e-05 7.8134464e-05 7.9636622e-05 8.0994367e-05 8.1891754e-05][7.9163161e-05 8.1624064e-05 8.6779692e-05 9.271504e-05 9.2910836e-05 8.8067791e-05 8.2157458e-05 7.8775083e-05 7.6519056e-05 7.4223557e-05 7.4205433e-05 7.5649892e-05 7.7126817e-05 7.8846628e-05 8.0791433e-05][7.9288337e-05 8.3582709e-05 9.1483882e-05 0.00010058902 0.0001013986 9.5841322e-05 8.845963e-05 8.3634426e-05 8.0445323e-05 7.5491953e-05 7.3174626e-05 7.3059651e-05 7.4035066e-05 7.567821e-05 7.8557212e-05][7.932404e-05 8.4829044e-05 9.4970572e-05 0.00010588006 0.00010769969 0.00010191706 9.3298258e-05 8.6691747e-05 8.2779392e-05 7.6423916e-05 7.1932525e-05 6.9818721e-05 7.035923e-05 7.2307615e-05 7.6040837e-05][7.8598292e-05 8.4031009e-05 9.4586525e-05 0.00010645697 0.00011022486 0.00010705881 9.9219294e-05 9.2431343e-05 8.8036264e-05 8.0173915e-05 7.2720446e-05 6.8357534e-05 6.7706707e-05 6.9423724e-05 7.3905467e-05][7.7676785e-05 8.2402774e-05 9.2233808e-05 0.00010381307 0.00010931052 0.000108831 0.00010357434 9.8781871e-05 9.49033e-05 8.6086053e-05 7.6263816e-05 6.9432564e-05 6.6816239e-05 6.7074929e-05 7.2091934e-05][7.7690529e-05 8.0590842e-05 8.7386135e-05 9.6225041e-05 0.00010182 0.0001025664 9.9287659e-05 9.7058226e-05 9.56596e-05 8.89232e-05 7.9089179e-05 7.1383387e-05 6.759938e-05 6.6452536e-05 7.1557988e-05][7.7899233e-05 7.8839934e-05 8.1945989e-05 8.7385335e-05 9.089175e-05 9.158859e-05 8.9754431e-05 8.9921254e-05 9.1029753e-05 8.7937107e-05 7.9716105e-05 7.2595089e-05 6.8929869e-05 6.7910645e-05 7.25362e-05][7.8392986e-05 7.8474492e-05 7.862263e-05 8.0423881e-05 8.2071456e-05 8.3069252e-05 8.22925e-05 8.3216044e-05 8.5469917e-05 8.5304528e-05 8.0566628e-05 7.4467345e-05 7.1754228e-05 7.1794304e-05 7.5807162e-05][7.9164529e-05 7.89077e-05 7.7555218e-05 7.6949174e-05 7.7059158e-05 7.7533579e-05 7.7636796e-05 7.870284e-05 8.0363621e-05 8.0551581e-05 7.8181693e-05 7.4707976e-05 7.3510622e-05 7.4815296e-05 7.8567151e-05][7.9954487e-05 7.9798439e-05 7.8215395e-05 7.6485048e-05 7.5086922e-05 7.4299431e-05 7.436399e-05 7.5501652e-05 7.5838114e-05 7.561958e-05 7.50402e-05 7.4055657e-05 7.5033371e-05 7.7642806e-05 8.1418635e-05][8.0578167e-05 8.042877e-05 7.8878824e-05 7.7134275e-05 7.5347525e-05 7.3432071e-05 7.2134528e-05 7.248934e-05 7.2775736e-05 7.269928e-05 7.3124393e-05 7.3956537e-05 7.6329547e-05 8.0194462e-05 8.3578067e-05][8.1012688e-05 8.1031576e-05 7.9729718e-05 7.841739e-05 7.7122007e-05 7.5037642e-05 7.2231793e-05 7.0636677e-05 7.0090355e-05 7.0610331e-05 7.2123905e-05 7.4109863e-05 7.7341771e-05 8.1388214e-05 8.4377345e-05]]...]
INFO - root - 2017-12-09 07:10:33.470188: step 710, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.857 sec/batch; 78h:58m:38s remains)
INFO - root - 2017-12-09 07:10:41.932914: step 720, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.876 sec/batch; 80h:46m:07s remains)
INFO - root - 2017-12-09 07:10:50.391343: step 730, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 79h:59m:35s remains)
INFO - root - 2017-12-09 07:10:58.992477: step 740, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.854 sec/batch; 78h:40m:34s remains)
INFO - root - 2017-12-09 07:11:07.532391: step 750, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.859 sec/batch; 79h:11m:59s remains)
INFO - root - 2017-12-09 07:11:16.034521: step 760, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 79h:34m:38s remains)
INFO - root - 2017-12-09 07:11:24.556117: step 770, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.863 sec/batch; 79h:30m:51s remains)
INFO - root - 2017-12-09 07:11:33.115579: step 780, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.851 sec/batch; 78h:27m:35s remains)
INFO - root - 2017-12-09 07:11:41.640004: step 790, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 79h:34m:22s remains)
INFO - root - 2017-12-09 07:11:50.233959: step 800, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 79h:57m:17s remains)
2017-12-09 07:11:51.133440: I tensorflow/core/kernels/logging_ops.cc:79] [[[7.942228e-05 7.9372723e-05 7.7071687e-05 7.4061936e-05 7.1599963e-05 7.1190247e-05 7.1638737e-05 7.2113238e-05 7.1656475e-05 6.8642337e-05 6.4897242e-05 6.2956176e-05 6.1693165e-05 6.2028623e-05 6.4663727e-05][7.9838246e-05 7.9983838e-05 7.7463781e-05 7.3944706e-05 7.1277209e-05 7.076747e-05 7.130106e-05 7.1583534e-05 7.1827148e-05 7.0526825e-05 6.858664e-05 6.8122004e-05 6.8646455e-05 6.9942034e-05 7.3108633e-05][7.7534853e-05 7.8042955e-05 7.6282187e-05 7.3161762e-05 7.0314949e-05 6.9317728e-05 6.9330548e-05 6.8576483e-05 6.84922e-05 6.8079687e-05 6.8700378e-05 7.0336988e-05 7.263841e-05 7.44389e-05 7.6890959e-05][7.4161355e-05 7.4843927e-05 7.3954958e-05 7.1924864e-05 6.9617709e-05 6.935632e-05 6.9395552e-05 6.8052177e-05 6.8273686e-05 6.8522786e-05 7.0244285e-05 7.252254e-05 7.5366253e-05 7.6887787e-05 7.8111021e-05][7.1826165e-05 7.2616524e-05 7.1933333e-05 7.016259e-05 6.8505404e-05 6.9150919e-05 6.9448346e-05 6.8471163e-05 6.8809743e-05 6.9967842e-05 7.2393574e-05 7.5039228e-05 7.8881661e-05 8.0849779e-05 8.2265506e-05][7.1174996e-05 7.2139876e-05 7.1260874e-05 6.9076916e-05 6.7706511e-05 6.9424204e-05 7.097458e-05 7.070959e-05 7.1030059e-05 7.2543167e-05 7.5307405e-05 7.7158606e-05 8.0084006e-05 8.2193561e-05 8.3802239e-05][7.2930052e-05 7.4419237e-05 7.3360614e-05 7.0921546e-05 6.9819143e-05 7.2104936e-05 7.5173521e-05 7.5065655e-05 7.4069234e-05 7.5462813e-05 7.82307e-05 7.8817851e-05 8.0457576e-05 8.2240338e-05 8.3065468e-05][7.249778e-05 7.3868723e-05 7.3538853e-05 7.2376359e-05 7.2144991e-05 7.55554e-05 7.9482714e-05 7.99825e-05 7.8178753e-05 7.9030506e-05 8.1997059e-05 8.3269617e-05 8.4203726e-05 8.5335276e-05 8.540893e-05][7.2286377e-05 7.4050433e-05 7.5093674e-05 7.5688193e-05 7.6230659e-05 7.9482845e-05 8.2962462e-05 8.3534862e-05 8.2589395e-05 8.31911e-05 8.470688e-05 8.4980245e-05 8.5528307e-05 8.6673012e-05 8.5436492e-05][6.9899623e-05 7.2751871e-05 7.4591204e-05 7.6116194e-05 7.7911485e-05 8.11428e-05 8.4344138e-05 8.5464089e-05 8.5536529e-05 8.6104366e-05 8.6750173e-05 8.6529377e-05 8.7146051e-05 8.8627581e-05 8.725057e-05][6.6464119e-05 6.9218171e-05 7.1057606e-05 7.3041789e-05 7.5461168e-05 7.913234e-05 8.3412611e-05 8.5859865e-05 8.7165434e-05 8.8426495e-05 8.8522829e-05 8.7414941e-05 8.7375236e-05 8.8867164e-05 8.8561777e-05][6.2863175e-05 6.5132415e-05 6.6452863e-05 6.8030866e-05 7.0331371e-05 7.39461e-05 7.75178e-05 7.9644182e-05 8.0682941e-05 8.1682454e-05 8.1471371e-05 8.0008176e-05 7.9331723e-05 8.04626e-05 8.1020095e-05][6.0179489e-05 6.2008992e-05 6.25619e-05 6.3707521e-05 6.5689157e-05 6.8523754e-05 7.1398383e-05 7.33487e-05 7.4419549e-05 7.4758609e-05 7.4259224e-05 7.277391e-05 7.1438524e-05 7.11904e-05 7.1329639e-05][5.9013859e-05 6.0398925e-05 6.0540369e-05 6.1344937e-05 6.2819483e-05 6.4750049e-05 6.6547334e-05 6.8093621e-05 6.8998255e-05 6.9295922e-05 6.8354057e-05 6.6930508e-05 6.5835637e-05 6.5322318e-05 6.4978107e-05][5.7889178e-05 5.9125588e-05 5.89451e-05 5.9163445e-05 5.9982012e-05 6.1214436e-05 6.2566149e-05 6.3706379e-05 6.43842e-05 6.4539512e-05 6.3970649e-05 6.30831e-05 6.2019695e-05 6.1306346e-05 6.0574472e-05]]...]
INFO - root - 2017-12-09 07:11:59.695631: step 810, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.873 sec/batch; 80h:23m:27s remains)
INFO - root - 2017-12-09 07:12:08.207778: step 820, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.860 sec/batch; 79h:14m:13s remains)
INFO - root - 2017-12-09 07:12:16.741493: step 830, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.848 sec/batch; 78h:09m:05s remains)
INFO - root - 2017-12-09 07:12:25.279094: step 840, loss = 0.82, batch loss = 0.69 (9.8 examples/sec; 0.812 sec/batch; 74h:49m:49s remains)
INFO - root - 2017-12-09 07:12:33.734961: step 850, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.840 sec/batch; 77h:20m:34s remains)
INFO - root - 2017-12-09 07:12:42.294980: step 860, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.879 sec/batch; 80h:57m:37s remains)
INFO - root - 2017-12-09 07:12:50.902992: step 870, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.881 sec/batch; 81h:08m:30s remains)
INFO - root - 2017-12-09 07:12:59.491544: step 880, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.872 sec/batch; 80h:22m:10s remains)
INFO - root - 2017-12-09 07:13:08.137396: step 890, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.872 sec/batch; 80h:18m:18s remains)
INFO - root - 2017-12-09 07:13:16.704776: step 900, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 80h:02m:23s remains)
2017-12-09 07:13:17.647818: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.00029142149 0.00035757502 0.00040482928 0.00043011745 0.00043241502 0.00041610282 0.00038602884 0.000347847 0.0003066647 0.00026653011 0.00022737923 0.00019155696 0.00016207474 0.00013999111 0.00012398753][0.00049006054 0.00061865716 0.00071593834 0.00077293231 0.00078442064 0.0007587655 0.00070713367 0.00064048165 0.00056467496 0.00048465855 0.00040257102 0.00032384149 0.00025618941 0.00020330331 0.00016424575][0.00073091843 0.00094103126 0.0011082527 0.0012153387 0.0012509092 0.0012248808 0.0011563619 0.0010617455 0.00094829081 0.00081992481 0.00068130187 0.00054020697 0.00041198076 0.00030765904 0.00022902442][0.000979517 0.0012840205 0.0015364707 0.0017117249 0.0017905997 0.0017830814 0.0017120669 0.0015991601 0.0014539651 0.0012772026 0.0010763179 0.00085974351 0.00064990477 0.00046974083 0.00033077865][0.0011829471 0.001578729 0.0019233539 0.0021791921 0.00231779 0.0023516973 0.0023037447 0.0021928973 0.0020307198 0.0018192012 0.0015619621 0.0012688611 0.00096917007 0.00069682754 0.00047753006][0.0012881889 0.0017524022 0.0021774285 0.0025165186 0.0027297435 0.0028263316 0.0028283256 0.0027467441 0.002590474 0.0023626776 0.0020664977 0.0017093307 0.001326672 0.00096332136 0.00065823569][0.0012810905 0.0017763639 0.0022541871 0.0026631455 0.0029558844 0.003133123 0.0032054004 0.0031737965 0.0030440344 0.0028204904 0.0025049539 0.0021024877 0.001653519 0.0012131649 0.00083208218][0.0011824662 0.0016666068 0.0021572995 0.0026022352 0.0029529829 0.0031991748 0.0033455568 0.0033749989 0.0032915415 0.0030979272 0.0027909973 0.0023734244 0.0018879459 0.001398234 0.0009641283][0.0010258278 0.0014630004 0.0019266076 0.0023699936 0.0027434074 0.0030262626 0.0032205509 0.0033054689 0.0032779907 0.0031273894 0.0028517088 0.0024525183 0.0019725861 0.0014770721 0.0010279273][0.000839423 0.0012041875 0.0016071809 0.0020100737 0.0023665442 0.0026448031 0.0028431141 0.0029519768 0.0029681416 0.0028697471 0.0026440637 0.0022956098 0.0018664938 0.0014114843 0.00099119276][0.00065798871 0.00093536847 0.0012537515 0.0015866887 0.0018924166 0.0021328579 0.0023041619 0.0024019789 0.0024318383 0.0023713117 0.0022050585 0.0019307643 0.0015835406 0.0012086231 0.00085525931][0.00048873777 0.00068107439 0.00090851082 0.0011551357 0.0013913062 0.0015794948 0.0017097923 0.0017826055 0.001804325 0.0017610485 0.0016428701 0.0014478978 0.0011984562 0.00092348637 0.00065880647][0.0003482858 0.00046613195 0.00060797896 0.00076795236 0.00092692667 0.001057233 0.0011478039 0.0011956989 0.0012072135 0.0011745333 0.001093794 0.00096772553 0.00080809224 0.00063086295 0.00045799019][0.00024607728 0.00030966534 0.00038637605 0.00047469253 0.00056381227 0.0006378023 0.00069028628 0.00071665546 0.00071926042 0.00069561845 0.00064719561 0.00057702936 0.00048921327 0.00039217426 0.00029720707][0.00017907418 0.00020991951 0.00024665071 0.00028839093 0.0003296604 0.00036236329 0.00038419059 0.0003927931 0.00038902904 0.00037302138 0.00034823889 0.00031637252 0.00027702184 0.00023350789 0.00019075928]]...]
INFO - root - 2017-12-09 07:13:26.306931: step 910, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.852 sec/batch; 78h:26m:48s remains)
INFO - root - 2017-12-09 07:13:34.899052: step 920, loss = 0.82, batch loss = 0.69 (8.8 examples/sec; 0.907 sec/batch; 83h:31m:52s remains)
INFO - root - 2017-12-09 07:13:43.432596: step 930, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.883 sec/batch; 81h:19m:42s remains)
INFO - root - 2017-12-09 07:13:51.960374: step 940, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.847 sec/batch; 78h:02m:27s remains)
INFO - root - 2017-12-09 07:14:00.349975: step 950, loss = 0.82, batch loss = 0.69 (9.8 examples/sec; 0.815 sec/batch; 75h:01m:37s remains)
INFO - root - 2017-12-09 07:14:08.989778: step 960, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.894 sec/batch; 82h:22m:37s remains)
INFO - root - 2017-12-09 07:14:17.401181: step 970, loss = 0.82, batch loss = 0.69 (9.7 examples/sec; 0.828 sec/batch; 76h:14m:58s remains)
INFO - root - 2017-12-09 07:14:26.068110: step 980, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.856 sec/batch; 78h:51m:23s remains)
INFO - root - 2017-12-09 07:14:34.760967: step 990, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.848 sec/batch; 78h:03m:14s remains)
INFO - root - 2017-12-09 07:14:43.460369: step 1000, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.856 sec/batch; 78h:51m:58s remains)
2017-12-09 07:14:44.436093: I tensorflow/core/kernels/logging_ops.cc:79] [[[7.0526563e-05 7.2635608e-05 7.06781e-05 6.7530062e-05 6.4680244e-05 6.2247891e-05 6.1792969e-05 6.5962588e-05 7.1147646e-05 7.2655457e-05 7.0961789e-05 6.9896741e-05 6.6231965e-05 5.9375641e-05 5.474516e-05][8.0084588e-05 8.3195388e-05 8.1015096e-05 7.7237994e-05 7.3202937e-05 6.96096e-05 6.8618283e-05 7.099365e-05 7.3358664e-05 7.2358373e-05 6.9611349e-05 6.87229e-05 6.5912485e-05 5.9649e-05 5.4618984e-05][8.9384914e-05 9.2979382e-05 9.0316636e-05 8.5107e-05 8.002371e-05 7.5497715e-05 7.4043048e-05 7.5243406e-05 7.5940356e-05 7.3453142e-05 6.9238173e-05 6.7401721e-05 6.4258056e-05 5.8229241e-05 5.2475909e-05][9.3422088e-05 9.7586235e-05 9.4731877e-05 8.8585977e-05 8.1977967e-05 7.6270226e-05 7.5210737e-05 7.6084281e-05 7.5780838e-05 7.2991541e-05 6.85827e-05 6.6058295e-05 6.2237239e-05 5.5882298e-05 4.991515e-05][9.4692383e-05 9.8040735e-05 9.420346e-05 8.7502813e-05 8.017302e-05 7.424772e-05 7.3269912e-05 7.5383607e-05 7.5364544e-05 7.2252049e-05 6.7615045e-05 6.4081774e-05 5.9736296e-05 5.3442345e-05 4.7614642e-05][9.03356e-05 9.3776078e-05 9.0529982e-05 8.5161e-05 7.8232624e-05 7.2177711e-05 7.088307e-05 7.2534414e-05 7.1491675e-05 6.8672125e-05 6.4256492e-05 6.0458187e-05 5.6611309e-05 5.1319974e-05 4.6576886e-05][8.3730061e-05 8.7082073e-05 8.5272819e-05 8.181767e-05 7.707953e-05 7.3306961e-05 7.4401607e-05 7.624297e-05 7.2609022e-05 6.80625e-05 6.3642961e-05 5.9887832e-05 5.6300269e-05 5.1134128e-05 4.6650966e-05][7.8190264e-05 8.080517e-05 7.89453e-05 7.5979187e-05 7.3381576e-05 7.2773139e-05 7.6945114e-05 7.9868565e-05 7.47166e-05 6.9122267e-05 6.36314e-05 5.9679376e-05 5.6300451e-05 5.147987e-05 4.6746009e-05][7.3059186e-05 7.5293217e-05 7.3774412e-05 7.1847055e-05 7.0290727e-05 7.1292372e-05 7.6370765e-05 7.995253e-05 7.5792268e-05 7.0495444e-05 6.4539032e-05 6.0129838e-05 5.642981e-05 5.176101e-05 4.7004512e-05][7.00776e-05 7.2204261e-05 7.0959337e-05 6.9378286e-05 6.8157933e-05 6.9333641e-05 7.3643416e-05 7.8017365e-05 7.6450204e-05 7.2724506e-05 6.743738e-05 6.2823907e-05 5.82539e-05 5.2926513e-05 4.8078513e-05][6.7531131e-05 7.0619557e-05 6.9867558e-05 6.9055663e-05 6.8049267e-05 6.9131434e-05 7.2698123e-05 7.7052318e-05 7.6951721e-05 7.453451e-05 6.9450442e-05 6.4881984e-05 5.9536869e-05 5.3850126e-05 4.9067283e-05][6.418371e-05 6.7388857e-05 6.6976005e-05 6.6717635e-05 6.6298344e-05 6.7802932e-05 7.0947513e-05 7.4646159e-05 7.5386284e-05 7.3203933e-05 6.7526737e-05 6.3214837e-05 5.7853205e-05 5.3027317e-05 4.9040424e-05][6.019781e-05 6.3624168e-05 6.3243417e-05 6.3129104e-05 6.2918829e-05 6.4145039e-05 6.685689e-05 7.0292728e-05 7.1126073e-05 6.91322e-05 6.3642234e-05 5.9519829e-05 5.4628959e-05 5.0863186e-05 4.7915579e-05][5.6017914e-05 5.9238155e-05 5.9308069e-05 5.9109225e-05 5.8902144e-05 5.9333055e-05 6.1735904e-05 6.4499094e-05 6.5107415e-05 6.3293548e-05 5.8631122e-05 5.5379391e-05 5.1562332e-05 4.8795257e-05 4.6528476e-05][5.0893686e-05 5.3753138e-05 5.3862772e-05 5.3721404e-05 5.3579897e-05 5.3832777e-05 5.5943081e-05 5.7881174e-05 5.8237412e-05 5.6779005e-05 5.343277e-05 5.1410541e-05 4.8479444e-05 4.6469377e-05 4.5109049e-05]]...]
INFO - root - 2017-12-09 07:14:52.953316: step 1010, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.832 sec/batch; 76h:34m:07s remains)
INFO - root - 2017-12-09 07:15:01.430666: step 1020, loss = 0.82, batch loss = 0.69 (10.7 examples/sec; 0.746 sec/batch; 68h:40m:27s remains)
INFO - root - 2017-12-09 07:15:09.798408: step 1030, loss = 0.82, batch loss = 0.69 (9.8 examples/sec; 0.817 sec/batch; 75h:14m:36s remains)
INFO - root - 2017-12-09 07:15:18.381321: step 1040, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.832 sec/batch; 76h:34m:28s remains)
INFO - root - 2017-12-09 07:15:27.106719: step 1050, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.861 sec/batch; 79h:15m:41s remains)
INFO - root - 2017-12-09 07:15:35.786270: step 1060, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.859 sec/batch; 79h:03m:52s remains)
INFO - root - 2017-12-09 07:15:44.307000: step 1070, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 79h:20m:09s remains)
INFO - root - 2017-12-09 07:15:53.123991: step 1080, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.863 sec/batch; 79h:28m:20s remains)
INFO - root - 2017-12-09 07:16:01.913822: step 1090, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.852 sec/batch; 78h:24m:01s remains)
INFO - root - 2017-12-09 07:16:10.450175: step 1100, loss = 0.82, batch loss = 0.69 (9.8 examples/sec; 0.814 sec/batch; 74h:55m:46s remains)
2017-12-09 07:16:11.255947: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.001769381 0.0020018008 0.0021608844 0.0022434541 0.0022481717 0.002171719 0.0020159748 0.0018072535 0.0015763842 0.0013379696 0.0011004515 0.00087732688 0.00067825813 0.00050663343 0.00035605027][0.0017755412 0.0020480931 0.0022531846 0.0023864983 0.0024340567 0.0023898319 0.0022592132 0.0020688504 0.001839368 0.001582604 0.0013078192 0.0010371958 0.00079123728 0.00057721639 0.00039690259][0.0016917816 0.0019972848 0.0022448814 0.0024310302 0.00253199 0.0025361837 0.0024472345 0.0022906021 0.0020804538 0.0018229734 0.001527078 0.0012180919 0.00092416554 0.00066253223 0.00044557758][0.0015853842 0.0019059276 0.0021811188 0.0024033566 0.0025486089 0.0026014484 0.0025616931 0.0024472817 0.0022679181 0.002026482 0.0017279258 0.0013967376 0.0010643617 0.00076194783 0.00050925894][0.0014808149 0.0017923058 0.002071515 0.0023097892 0.0024821877 0.002576258 0.0025878826 0.0025222248 0.0023821581 0.0021703278 0.0018885274 0.0015548866 0.0012014454 0.00087049708 0.00058651291][0.0013952423 0.0016849877 0.0019521981 0.0021869563 0.0023668378 0.0024812086 0.002530345 0.0025105409 0.0024144317 0.002243272 0.0019927595 0.0016723713 0.0013155052 0.00096769689 0.00066058547][0.0013329275 0.0015973989 0.0018419316 0.0020600562 0.0022337507 0.0023578384 0.0024315745 0.0024474077 0.00238859 0.0022553052 0.0020370344 0.0017396006 0.0013928993 0.0010407032 0.00071942247][0.001264274 0.0015023393 0.0017233983 0.0019209489 0.0020828038 0.0022096217 0.0022965001 0.00233509 0.0023060024 0.0022067707 0.0020220098 0.0017519087 0.0014223481 0.0010758667 0.000751729][0.0011830723 0.0013917503 0.0015863111 0.0017606767 0.0019064234 0.0020263859 0.002116299 0.00217029 0.0021677995 0.002100107 0.0019454757 0.001703552 0.0013964983 0.0010644066 0.00074845261][0.0011014182 0.0012814992 0.0014467948 0.0015950471 0.0017212683 0.0018308602 0.0019205362 0.0019838326 0.0020021868 0.001961184 0.0018324289 0.0016152576 0.0013301454 0.0010169569 0.00071569084][0.0010229255 0.0011771491 0.001314352 0.0014387155 0.0015492868 0.0016487495 0.0017338382 0.001798849 0.0018293582 0.00180674 0.0016991597 0.0015035449 0.0012395891 0.00094830804 0.00066725718][0.00095885055 0.0010850192 0.0011952205 0.0012975355 0.0013932973 0.0014827497 0.0015617979 0.0016247078 0.0016596755 0.0016479803 0.0015576117 0.0013836772 0.0011437702 0.00087772106 0.00062045181][0.00090571173 0.0010019992 0.0010863285 0.0011667412 0.0012452721 0.0013220995 0.0013937335 0.0014537208 0.0014926775 0.0014908165 0.0014174148 0.0012664953 0.0010534126 0.00081380433 0.00058131519][0.00086530659 0.00093310815 0.00099271943 0.0010529571 0.0011141966 0.0011766959 0.0012383352 0.0012929392 0.0013352548 0.0013459693 0.001293799 0.0011685085 0.00098198862 0.00076735823 0.00055758253][0.00084436836 0.00089306966 0.00093361811 0.00097615086 0.0010215309 0.0010692759 0.0011184046 0.0011634105 0.0012042408 0.0012255238 0.0011933858 0.0010935724 0.00093347667 0.00074186956 0.0005497766]]...]
INFO - root - 2017-12-09 07:16:20.042444: step 1110, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 79h:21m:58s remains)
INFO - root - 2017-12-09 07:16:28.695052: step 1120, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.872 sec/batch; 80h:13m:46s remains)
INFO - root - 2017-12-09 07:16:37.077982: step 1130, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.872 sec/batch; 80h:15m:12s remains)
INFO - root - 2017-12-09 07:16:45.448015: step 1140, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.839 sec/batch; 77h:15m:55s remains)
INFO - root - 2017-12-09 07:16:54.189196: step 1150, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.903 sec/batch; 83h:09m:25s remains)
INFO - root - 2017-12-09 07:17:02.757024: step 1160, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.881 sec/batch; 81h:05m:57s remains)
INFO - root - 2017-12-09 07:17:11.245774: step 1170, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.834 sec/batch; 76h:46m:01s remains)
INFO - root - 2017-12-09 07:17:19.847735: step 1180, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.887 sec/batch; 81h:39m:06s remains)
INFO - root - 2017-12-09 07:17:28.306360: step 1190, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.857 sec/batch; 78h:53m:13s remains)
INFO - root - 2017-12-09 07:17:36.974625: step 1200, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.875 sec/batch; 80h:33m:47s remains)
2017-12-09 07:17:37.795014: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.00046969813 0.00043852493 0.00038643985 0.00033168218 0.00028377434 0.00024399336 0.00021217411 0.00018646836 0.00016639936 0.00015092698 0.00014097162 0.00013241197 0.00012228348 0.00011316672 0.00010491104][0.00065326347 0.000627234 0.00056435837 0.00049038959 0.00042336277 0.00036929041 0.00032610534 0.0002898 0.00025721182 0.00022974321 0.00020770493 0.00018823108 0.00016647308 0.00014632943 0.0001292478][0.00084089435 0.00082440389 0.00075675093 0.000668803 0.00058725954 0.00052372483 0.00047658038 0.00043428698 0.00039349165 0.00035381707 0.00031594333 0.00027990574 0.00023964915 0.00020058389 0.00016579259][0.0010091277 0.0010044109 0.0009368998 0.000846488 0.00076170522 0.00069583883 0.0006489834 0.00060699682 0.00056608918 0.0005175402 0.00046282675 0.00040723311 0.00034367183 0.00027893565 0.00021906811][0.0011180906 0.0011369918 0.0010809619 0.00099977443 0.000924007 0.00086464814 0.00082335953 0.0007890811 0.00075439503 0.00070451084 0.00063647435 0.000559387 0.00046915005 0.00037459459 0.00028698787][0.0011550431 0.0011931937 0.0011613417 0.0011076329 0.0010561253 0.0010150719 0.00098954653 0.00096893235 0.00094185246 0.00089220505 0.00081467762 0.00071879936 0.00060395984 0.00047995267 0.00036555241][0.0011298106 0.0011850591 0.0011827708 0.0011665652 0.0011511357 0.0011404386 0.0011387415 0.0011341589 0.0011119976 0.001060613 0.000976142 0.00086799683 0.00073516788 0.00059005787 0.00045328686][0.0010578947 0.0011327486 0.0011659705 0.0011887134 0.0012053512 0.0012206478 0.0012450936 0.0012591679 0.0012376943 0.0011798613 0.0010954657 0.00098583312 0.00084662129 0.0006915977 0.00053985271][0.00096360216 0.0010613778 0.0011260263 0.0011750804 0.0012087256 0.0012331509 0.0012645316 0.0012898899 0.0012714179 0.0012163642 0.0011379682 0.0010367737 0.00090750225 0.00075815752 0.00060580805][0.00087175472 0.00098603079 0.0010673244 0.0011216475 0.0011520265 0.0011659607 0.0011771064 0.0011870012 0.001176103 0.0011373158 0.0010787965 0.0010005182 0.00089642685 0.00076882204 0.0006323235][0.0007898605 0.00090959179 0.00099112152 0.0010337951 0.001043388 0.0010344102 0.0010153892 0.00099356007 0.00097273861 0.0009462359 0.00091221434 0.00086405978 0.00079362164 0.00069940632 0.00059115543][0.000710947 0.00082464993 0.00089406245 0.000916835 0.00089767232 0.00085800415 0.00080644561 0.00075532764 0.00071771391 0.00069231557 0.00067263283 0.00064743665 0.00060911837 0.00055204169 0.00047974696][0.00061483291 0.00071335805 0.00076711242 0.0007704809 0.00072971394 0.00066884328 0.00059734687 0.00052834157 0.00047899067 0.000451125 0.00043702638 0.00042437142 0.00040510902 0.00037589271 0.0003365138][0.00047873383 0.00055616332 0.00059472315 0.00059173862 0.00055178505 0.00049336511 0.00042477457 0.00035887133 0.00030870098 0.00028165313 0.00027145108 0.00026533334 0.00025645338 0.00024213607 0.00022198092][0.00033068412 0.00038415592 0.00041093363 0.00040925489 0.00038227282 0.00034178823 0.00029398361 0.0002462714 0.00020740378 0.00018608346 0.00017860335 0.00017611444 0.00017164653 0.00016439985 0.00015548099]]...]
INFO - root - 2017-12-09 07:17:46.454832: step 1210, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.878 sec/batch; 80h:47m:58s remains)
INFO - root - 2017-12-09 07:17:55.198934: step 1220, loss = 0.82, batch loss = 0.69 (9.7 examples/sec; 0.824 sec/batch; 75h:47m:47s remains)
INFO - root - 2017-12-09 07:18:03.724611: step 1230, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.881 sec/batch; 81h:04m:44s remains)
INFO - root - 2017-12-09 07:18:12.573959: step 1240, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.870 sec/batch; 80h:00m:54s remains)
INFO - root - 2017-12-09 07:18:21.263716: step 1250, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.886 sec/batch; 81h:30m:24s remains)
INFO - root - 2017-12-09 07:18:29.980354: step 1260, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.872 sec/batch; 80h:16m:15s remains)
INFO - root - 2017-12-09 07:18:38.639905: step 1270, loss = 0.82, batch loss = 0.69 (10.6 examples/sec; 0.752 sec/batch; 69h:08m:48s remains)
INFO - root - 2017-12-09 07:18:47.266446: step 1280, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.855 sec/batch; 78h:39m:48s remains)
INFO - root - 2017-12-09 07:18:56.018451: step 1290, loss = 0.82, batch loss = 0.69 (8.8 examples/sec; 0.905 sec/batch; 83h:14m:24s remains)
INFO - root - 2017-12-09 07:19:04.749123: step 1300, loss = 0.82, batch loss = 0.69 (8.3 examples/sec; 0.966 sec/batch; 88h:54m:25s remains)
2017-12-09 07:19:05.638094: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.000498032 0.00076592644 0.0010778893 0.0014049141 0.0017301821 0.002009541 0.0021929794 0.0022574717 0.0021709914 0.0019527311 0.0016507978 0.0013308045 0.001029597 0.00083012989 0.000802074][0.00066784106 0.0009902569 0.0013529907 0.0017212752 0.0020655403 0.002351677 0.002544919 0.0026228889 0.0025547915 0.0023480498 0.0020385061 0.0016909509 0.0013460936 0.0010913956 0.0010019516][0.00083882123 0.0011922211 0.0015737755 0.0019498076 0.0022864791 0.0025597515 0.002745111 0.0028284257 0.0027842773 0.0026110224 0.0023239783 0.0019825629 0.0016289712 0.0013456268 0.0012046711][0.00098887517 0.0013562384 0.0017358278 0.0021004665 0.0024118519 0.0026628519 0.0028437243 0.00293191 0.0029184166 0.0027989463 0.0025662954 0.0022659772 0.0019363777 0.001655031 0.0014855145][0.0011108832 0.0014771201 0.0018423914 0.00218606 0.0024693783 0.0026970506 0.0028738466 0.0029679839 0.0029854022 0.0029188627 0.0027563174 0.0025304514 0.0022663346 0.0020283074 0.0018636408][0.0011879051 0.0015392433 0.0018789963 0.0021863307 0.002434592 0.0026375037 0.0028071443 0.0029060515 0.0029505864 0.0029408808 0.0028650579 0.0027315472 0.002553473 0.0023813196 0.0022435666][0.0012345314 0.0015711603 0.0018797903 0.0021425565 0.0023444735 0.0025094105 0.0026592885 0.0027547705 0.0028197092 0.0028530164 0.0028465462 0.0027947684 0.0027017172 0.0026037074 0.0025106831][0.0012305033 0.0015384076 0.0018084269 0.0020246734 0.0021772485 0.0022961167 0.0024187074 0.0025155458 0.0026038529 0.0026780795 0.0027298152 0.0027441797 0.0027202978 0.0026829082 0.0026354417][0.0011653289 0.0014363186 0.0016678422 0.0018443526 0.0019626054 0.0020535351 0.0021573887 0.002255375 0.0023584908 0.002460683 0.002553259 0.0026143813 0.0026356794 0.002637065 0.0026197578][0.001035299 0.0012712476 0.0014693548 0.0016239421 0.0017312979 0.001811541 0.0019035809 0.0020013489 0.0021121269 0.0022320696 0.0023519103 0.0024459725 0.0025036207 0.0025271471 0.0025199524][0.00085053721 0.0010403804 0.0011987261 0.0013220251 0.0014089119 0.001476911 0.0015580964 0.0016539729 0.0017746525 0.0019143235 0.002060039 0.0021870909 0.0022745109 0.002319053 0.002325074][0.0006274989 0.00076554512 0.00087834313 0.00096531876 0.0010276508 0.0010773285 0.0011441555 0.001231562 0.0013520771 0.0014991383 0.0016573427 0.0018007907 0.001907788 0.0019687628 0.0019848468][0.00040126251 0.00049151026 0.00056398223 0.00061749242 0.00065866642 0.00068961352 0.00073625933 0.00080463564 0.00090777385 0.0010426071 0.0011937866 0.0013353095 0.0014493358 0.0015199358 0.0015482284][0.00022679033 0.00028572135 0.0003292592 0.00036534478 0.00039376508 0.00041312532 0.00044157967 0.00048008491 0.00054986886 0.00064731465 0.00076547143 0.00088245049 0.00098748785 0.0010592155 0.0010955298][0.00012366544 0.00015944731 0.0001875537 0.00021152497 0.00023379135 0.00024951427 0.00026441456 0.00028243288 0.00032082005 0.00037368172 0.000440564 0.00051444175 0.0005897056 0.00064564316 0.00068272621]]...]
INFO - root - 2017-12-09 07:19:14.286991: step 1310, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.853 sec/batch; 78h:26m:05s remains)
INFO - root - 2017-12-09 07:19:22.843881: step 1320, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 79h:29m:45s remains)
INFO - root - 2017-12-09 07:19:31.143476: step 1330, loss = 0.82, batch loss = 0.69 (9.8 examples/sec; 0.817 sec/batch; 75h:10m:47s remains)
INFO - root - 2017-12-09 07:19:39.585496: step 1340, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.838 sec/batch; 77h:07m:01s remains)
INFO - root - 2017-12-09 07:19:48.169478: step 1350, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.883 sec/batch; 81h:15m:21s remains)
INFO - root - 2017-12-09 07:19:56.859463: step 1360, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.892 sec/batch; 82h:02m:37s remains)
INFO - root - 2017-12-09 07:20:05.446977: step 1370, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.832 sec/batch; 76h:32m:20s remains)
INFO - root - 2017-12-09 07:20:14.014812: step 1380, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.839 sec/batch; 77h:08m:28s remains)
INFO - root - 2017-12-09 07:20:22.625720: step 1390, loss = 0.82, batch loss = 0.69 (9.7 examples/sec; 0.826 sec/batch; 75h:59m:00s remains)
INFO - root - 2017-12-09 07:20:31.315751: step 1400, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.873 sec/batch; 80h:15m:13s remains)
2017-12-09 07:20:32.195544: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.0041157831 0.0043255128 0.0043955869 0.0043899221 0.0043122969 0.0041745277 0.0040128385 0.0038399631 0.0037295164 0.0037081221 0.0037500232 0.0038219891 0.0038946355 0.0039259824 0.0039015939][0.0046265828 0.0048690089 0.00495478 0.0049581937 0.0048852037 0.0047443109 0.0045689382 0.0043580872 0.0041909078 0.004107953 0.0040950887 0.0041333027 0.0042045829 0.0042717531 0.0043050894][0.0049597127 0.0052389377 0.0053555821 0.0053860531 0.0053365668 0.0052251318 0.0050630881 0.004836244 0.0046132859 0.004442533 0.0043261419 0.0042857667 0.0043206741 0.0043968437 0.0044771656][0.0050784242 0.0053968304 0.0055623753 0.005640496 0.0056385212 0.0055683092 0.0054259971 0.0051786257 0.0048879948 0.0046042148 0.0043560234 0.0042086239 0.00418188 0.0042593763 0.0043977709][0.0050235884 0.0053809644 0.0055985712 0.0057302788 0.0057866657 0.0057724449 0.0056566456 0.0053931121 0.0050333785 0.0046304483 0.0042448943 0.0039739506 0.0038704013 0.003943759 0.00414576][0.0047872751 0.0051621818 0.0054187272 0.0056092385 0.0057376595 0.0057955473 0.0057227449 0.0054550073 0.0050372374 0.004525769 0.0040097083 0.0036218117 0.0034515485 0.0035233642 0.0037858032][0.0044058375 0.0047822227 0.0050712381 0.0053171995 0.005509397 0.0056313421 0.0056065312 0.0053510177 0.0049002115 0.004310892 0.0036948908 0.003216973 0.0029949481 0.0030710881 0.0033906796][0.0038904562 0.0042559621 0.0045775333 0.0048888014 0.0051479461 0.0053315419 0.0053599472 0.0051340456 0.0046719033 0.0040317485 0.0033480341 0.0028088938 0.0025554474 0.0026380403 0.0030013693][0.0032977378 0.0036410459 0.0039822841 0.0043457928 0.0046682046 0.0049100616 0.0049865129 0.0048047439 0.0043592327 0.0037084233 0.0030036666 0.0024426652 0.0021796906 0.0022694457 0.0026581683][0.0026563695 0.0029698412 0.0033189422 0.0037231543 0.0040949476 0.0043936241 0.0045245895 0.0044054594 0.0040131556 0.0033994922 0.0027220252 0.002171105 0.0019067416 0.0019840884 0.0023529336][0.0020247295 0.0023052488 0.002635113 0.0030420397 0.0034288182 0.0037582596 0.00393808 0.0038894904 0.0035777634 0.0030494549 0.0024461327 0.0019435737 0.0016943126 0.0017487647 0.0020640881][0.0014290018 0.0016703262 0.0019651558 0.0023436733 0.0027176733 0.0030512526 0.0032603026 0.0032745507 0.0030538552 0.0026375302 0.0021396347 0.0017145523 0.0014973297 0.0015313621 0.0017797921][0.00092423084 0.0011178434 0.0013600494 0.0016805909 0.002005914 0.0023157441 0.0025305932 0.0025945969 0.0024695357 0.0021760135 0.0018002517 0.0014689533 0.00129245 0.0013066391 0.0014799655][0.00055653264 0.00069750857 0.00087885075 0.0011159985 0.0013682015 0.0016248783 0.0018179237 0.0019112737 0.0018681319 0.0016932867 0.0014427443 0.001208324 0.0010752227 0.0010705966 0.0011732025][0.00031138756 0.00040515835 0.0005286641 0.0006864086 0.00085824728 0.0010437542 0.0011930747 0.0012872282 0.001295916 0.0012150703 0.0010751052 0.00093386823 0.00084502017 0.0008321518 0.00087930128]]...]
INFO - root - 2017-12-09 07:20:40.894127: step 1410, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.851 sec/batch; 78h:14m:16s remains)
INFO - root - 2017-12-09 07:20:49.473795: step 1420, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.847 sec/batch; 77h:53m:25s remains)
INFO - root - 2017-12-09 07:20:57.760982: step 1430, loss = 0.82, batch loss = 0.69 (9.7 examples/sec; 0.826 sec/batch; 75h:56m:28s remains)
INFO - root - 2017-12-09 07:21:06.254874: step 1440, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.853 sec/batch; 78h:24m:46s remains)
INFO - root - 2017-12-09 07:21:14.896401: step 1450, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.846 sec/batch; 77h:48m:54s remains)
INFO - root - 2017-12-09 07:21:23.580687: step 1460, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.883 sec/batch; 81h:12m:41s remains)
INFO - root - 2017-12-09 07:21:32.264327: step 1470, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.875 sec/batch; 80h:28m:38s remains)
INFO - root - 2017-12-09 07:21:40.843147: step 1480, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.854 sec/batch; 78h:32m:49s remains)
INFO - root - 2017-12-09 07:21:49.555574: step 1490, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.859 sec/batch; 78h:59m:03s remains)
INFO - root - 2017-12-09 07:21:58.333737: step 1500, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.887 sec/batch; 81h:31m:39s remains)
2017-12-09 07:21:59.271237: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.000949929 0.0011537378 0.0013030758 0.0013832571 0.0013778724 0.0012986717 0.001164906 0.0010038487 0.00083843153 0.00068748143 0.00056113 0.00045249908 0.00035413142 0.00026354217 0.00018447764][0.0014252171 0.001703425 0.0018975441 0.0019971663 0.001984895 0.0018771274 0.0017015615 0.0014922121 0.0012752049 0.0010689315 0.00088426174 0.00071760645 0.00056209858 0.0004157623 0.00028337751][0.001902087 0.0022378846 0.002461544 0.0025715525 0.0025520972 0.002425378 0.0022271143 0.0019886224 0.0017369275 0.0014868026 0.0012483042 0.0010232173 0.00080708507 0.00059805281 0.00040533586][0.00231807 0.0026927686 0.0029316421 0.0030388108 0.0030022331 0.002856097 0.0026435535 0.002390929 0.0021231037 0.0018501332 0.0015775075 0.0013075899 0.0010388374 0.00077362522 0.00052467984][0.0026186469 0.0030201636 0.0032672721 0.0033684219 0.0033176632 0.0031607891 0.0029407246 0.0026824637 0.0024085364 0.0021268548 0.001837063 0.0015402223 0.001234565 0.00092606689 0.00063236448][0.0027905772 0.0032071106 0.0034578496 0.0035566632 0.0035030027 0.0033471424 0.0031268422 0.0028681813 0.0025950237 0.0023132332 0.0020178889 0.0017075811 0.0013774452 0.0010399849 0.00071555167][0.0028246855 0.0032396063 0.0034878347 0.0035853842 0.0035372635 0.0033894749 0.0031774149 0.0029286367 0.0026649642 0.002392034 0.0021042209 0.0017947635 0.0014564224 0.0011058577 0.00076674629][0.0027125736 0.0031121904 0.0033541298 0.0034477666 0.0034026462 0.0032595592 0.003055966 0.0028205968 0.0025747905 0.0023221546 0.0020536229 0.0017601942 0.0014353289 0.0010964588 0.00076667295][0.0024525262 0.0028246385 0.0030557485 0.0031437888 0.0030994832 0.0029651991 0.0027777343 0.0025606311 0.0023364313 0.0021078927 0.0018645584 0.0015972739 0.0013054713 0.0010031244 0.00070806133][0.0020627638 0.0023915381 0.0026063898 0.0026953486 0.0026626477 0.0025437505 0.0023711429 0.0021701031 0.0019639975 0.0017578211 0.0015461547 0.0013196399 0.0010794303 0.00083593075 0.00059881504][0.0015885225 0.001865483 0.0020583305 0.002150022 0.002134687 0.0020383555 0.0018849106 0.0017004657 0.0015102883 0.0013240194 0.0011467809 0.00096965733 0.0007929898 0.00061910803 0.0004506078][0.0010984188 0.0013169621 0.0014783721 0.0015653012 0.0015630808 0.0014896513 0.0013606853 0.0012017929 0.001036666 0.00088102213 0.00074653112 0.0006258004 0.00051461003 0.00040920716 0.00030768767][0.00066711649 0.00082036608 0.00094184896 0.0010113466 0.0010136627 0.00096090056 0.00086365256 0.00074228918 0.00061832007 0.0005126308 0.0004317569 0.00036641327 0.0003124974 0.00026176323 0.00021123845][0.00035852339 0.00045086973 0.00052622269 0.00056849309 0.00056914933 0.00053460209 0.00047172769 0.000396652 0.00032472418 0.00027005616 0.00023605068 0.00021382184 0.00019639474 0.00017654106 0.00015312927][0.00018034001 0.00022977336 0.00026872527 0.00028875945 0.00028465045 0.00026300011 0.00022933994 0.00019134374 0.00015746774 0.0001359304 0.00012866304 0.00012810486 0.00012892093 0.00012494778 0.00011553626]]...]
INFO - root - 2017-12-09 07:22:07.841472: step 1510, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.886 sec/batch; 81h:29m:54s remains)
INFO - root - 2017-12-09 07:22:16.635924: step 1520, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.876 sec/batch; 80h:32m:27s remains)
INFO - root - 2017-12-09 07:22:25.046395: step 1530, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.899 sec/batch; 82h:37m:39s remains)
INFO - root - 2017-12-09 07:22:33.877917: step 1540, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.874 sec/batch; 80h:22m:30s remains)
INFO - root - 2017-12-09 07:22:42.476165: step 1550, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.851 sec/batch; 78h:14m:35s remains)
INFO - root - 2017-12-09 07:22:51.124007: step 1560, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 79h:44m:26s remains)
INFO - root - 2017-12-09 07:22:59.812680: step 1570, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.884 sec/batch; 81h:18m:03s remains)
INFO - root - 2017-12-09 07:23:08.532178: step 1580, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.860 sec/batch; 79h:01m:15s remains)
INFO - root - 2017-12-09 07:23:17.160464: step 1590, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.893 sec/batch; 82h:03m:32s remains)
INFO - root - 2017-12-09 07:23:26.028041: step 1600, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.865 sec/batch; 79h:29m:22s remains)
2017-12-09 07:23:26.874432: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.0015009822 0.0015752495 0.0016279209 0.0016853155 0.0017400967 0.0017623771 0.0017628931 0.0017252864 0.0016596025 0.001569542 0.0014646888 0.0013694528 0.0013005214 0.0012748744 0.0012922616][0.0022963514 0.002410711 0.0024954202 0.00258445 0.0026648513 0.0026971698 0.0027050877 0.0026684471 0.0025976787 0.0024943107 0.0023696383 0.0022477091 0.0021541743 0.0021123828 0.0021227845][0.0031069804 0.003247513 0.0033587664 0.003475667 0.0035715341 0.0036129972 0.003631074 0.0036004973 0.003534758 0.0034325209 0.0033028044 0.003171176 0.0030622114 0.0030036347 0.0030032727][0.0038045505 0.0039697303 0.0040903208 0.0042084395 0.0043124179 0.0043659555 0.0044030524 0.0043935077 0.0043547247 0.00427349 0.0041610845 0.0040374342 0.003924815 0.0038553919 0.0038412525][0.0042932169 0.0044834153 0.0046023759 0.0047250111 0.0048448192 0.0049219392 0.004981163 0.0050038127 0.0050013112 0.0049508135 0.0048655937 0.0047614248 0.0046627796 0.0045946566 0.004573199][0.0045838584 0.0047985688 0.004915969 0.0050545037 0.0052082092 0.0053177006 0.0054001696 0.0054505635 0.0054747104 0.00544901 0.0053893509 0.0053109541 0.0052331989 0.00517335 0.0051379758][0.0047170455 0.0049405973 0.0050611352 0.0052198241 0.0054135439 0.0055653364 0.0056726676 0.0057498571 0.0057938471 0.0057831327 0.0057379697 0.005671646 0.0056029111 0.0055373833 0.0054872991][0.00468143 0.0049000136 0.0050257426 0.0051967651 0.0054206317 0.0056144265 0.0057399217 0.0058403024 0.0059025954 0.0059056194 0.0058722231 0.0058131684 0.0057431608 0.0056637325 0.0055870516][0.0044779414 0.0046597035 0.0047814441 0.0049680178 0.0052073691 0.0054210881 0.0055629024 0.0056794034 0.0057505895 0.0057616653 0.005725563 0.0056542959 0.00556289 0.0054535912 0.0053345766][0.0040351725 0.0041792961 0.004272162 0.0044416897 0.0046779681 0.0049038781 0.0050651389 0.0051842812 0.0052530682 0.0052598296 0.0052102031 0.0051136571 0.0049871914 0.0048389118 0.0046748375][0.0034127526 0.0034778735 0.0035215097 0.0036543503 0.0038633393 0.0040814131 0.004248396 0.0043676118 0.00442613 0.0044222693 0.0043579745 0.0042452426 0.004101838 0.0039379345 0.0037583569][0.0026880631 0.0026789622 0.002668506 0.0027621896 0.0029272856 0.0031116041 0.0032618039 0.0033669323 0.0034122886 0.0034013786 0.0033335148 0.0032230744 0.0030851234 0.0029329529 0.0027710327][0.0019905085 0.0019345767 0.0018854021 0.0019380039 0.00204505 0.0021766739 0.0022948554 0.0023783192 0.0024186682 0.002415908 0.0023645631 0.0022792458 0.0021696005 0.0020489148 0.0019155883][0.0013944247 0.0013269818 0.0012725715 0.0012949377 0.001359336 0.0014540628 0.0015544171 0.0016384687 0.001695006 0.0017149297 0.0016923581 0.0016342803 0.0015451784 0.0014415508 0.0013176074][0.0009389238 0.00088907534 0.00086072081 0.00089540088 0.00095907622 0.0010593866 0.001179072 0.0012999654 0.0014033869 0.0014705926 0.0014882382 0.0014502242 0.0013618896 0.0012327279 0.0010715583]]...]
INFO - root - 2017-12-09 07:23:35.493895: step 1610, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.861 sec/batch; 79h:06m:25s remains)
INFO - root - 2017-12-09 07:23:44.202217: step 1620, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.876 sec/batch; 80h:30m:25s remains)
INFO - root - 2017-12-09 07:23:52.609737: step 1630, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.892 sec/batch; 82h:01m:32s remains)
INFO - root - 2017-12-09 07:24:01.297314: step 1640, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.861 sec/batch; 79h:10m:05s remains)
INFO - root - 2017-12-09 07:24:09.933542: step 1650, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.888 sec/batch; 81h:39m:03s remains)
INFO - root - 2017-12-09 07:24:18.707888: step 1660, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.875 sec/batch; 80h:26m:41s remains)
INFO - root - 2017-12-09 07:24:27.504411: step 1670, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.883 sec/batch; 81h:06m:54s remains)
INFO - root - 2017-12-09 07:24:36.006172: step 1680, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.848 sec/batch; 77h:54m:34s remains)
INFO - root - 2017-12-09 07:24:44.646696: step 1690, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.837 sec/batch; 76h:52m:16s remains)
INFO - root - 2017-12-09 07:24:53.239273: step 1700, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.859 sec/batch; 78h:54m:24s remains)
2017-12-09 07:24:54.160746: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.0043078042 0.0045767957 0.0045203175 0.0042368891 0.0037381141 0.0031141883 0.0024496398 0.0018326419 0.0013056451 0.00091749919 0.00066500693 0.00049723161 0.00038569956 0.00030493885 0.00024672525][0.0047271168 0.0050671892 0.0050497563 0.0048025348 0.0043254811 0.0037074587 0.003027099 0.0023780109 0.001808214 0.0013624268 0.0010440811 0.00080923445 0.00063353346 0.00049362564 0.00038344556][0.0048816684 0.0052911942 0.0053343023 0.0051513235 0.0047526723 0.0042106383 0.0035900653 0.0029759665 0.0024143516 0.0019343799 0.0015540305 0.00123631 0.00096775312 0.00073777168 0.00054877572][0.0048021595 0.005261716 0.0053740619 0.0052879043 0.0050120614 0.0045998869 0.0040921345 0.0035630835 0.0030468742 0.0025611287 0.0021288444 0.0017244372 0.0013473727 0.0010041129 0.00071831845][0.0045657875 0.0050437693 0.0052142176 0.0052299262 0.0050919028 0.0048242803 0.0044565471 0.0040429574 0.0035981741 0.0031291987 0.0026610121 0.0021811102 0.0016997212 0.0012426809 0.00085776753][0.0042360527 0.0047018654 0.0049188426 0.0050275205 0.0050194026 0.0048982613 0.0046735862 0.0043757409 0.0040050796 0.0035583025 0.0030614769 0.0025130117 0.0019414314 0.0013915438 0.00092980335][0.0039012879 0.0043333811 0.0045740707 0.0047494587 0.0048427396 0.0048420718 0.004740803 0.0045370362 0.0042216228 0.0037911178 0.0032665627 0.0026610091 0.0020235707 0.0014162384 0.0009180932][0.0035563805 0.0039413441 0.0041759941 0.0043950081 0.0045568561 0.0046411925 0.004623746 0.0044789868 0.0041865 0.0037548253 0.0032118275 0.0025855247 0.001934986 0.0013274439 0.00084401754][0.003188188 0.0035016858 0.0037162909 0.0039406316 0.0041272915 0.0042470209 0.0042585931 0.0041328538 0.0038510344 0.0034248317 0.0028915682 0.002293718 0.0016946859 0.001150177 0.00072685396][0.0027882454 0.0030324752 0.003202497 0.0033967104 0.0035692938 0.0036866744 0.0036946435 0.0035644227 0.0032882064 0.0028832464 0.0023943244 0.0018662814 0.0013588407 0.000913294 0.00057734561][0.002297339 0.0024766652 0.0025944344 0.002743209 0.0028819588 0.0029796166 0.0029805442 0.0028573712 0.0026083158 0.0022543438 0.001842809 0.0014154051 0.0010231326 0.00068575412 0.00043864024][0.0017319454 0.0018567233 0.0019231739 0.0020153003 0.0021077746 0.0021745388 0.0021678633 0.0020693727 0.0018791513 0.0016129677 0.0013144516 0.0010130208 0.00074463635 0.00051134982 0.00034136622][0.001162105 0.00124815 0.0012752161 0.0013180594 0.0013650518 0.001400714 0.0013910198 0.0013247904 0.0012012721 0.0010364231 0.00085737987 0.00067777414 0.00052174396 0.00038234182 0.0002774451][0.00069599983 0.00075085473 0.00075352634 0.00076263363 0.000778124 0.000789836 0.000779283 0.00074403652 0.00068314129 0.00060496561 0.0005221765 0.00044040717 0.00036928215 0.00029572262 0.00023214465][0.00037888277 0.00041863078 0.00041089693 0.00040325784 0.0003997637 0.00039802748 0.00038799003 0.00037173845 0.00034809977 0.00032500044 0.00030403608 0.00028163838 0.00025850441 0.00022500717 0.0001886946]]...]
INFO - root - 2017-12-09 07:25:02.745708: step 1710, loss = 0.82, batch loss = 0.69 (9.8 examples/sec; 0.816 sec/batch; 74h:56m:11s remains)
INFO - root - 2017-12-09 07:25:11.363219: step 1720, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.830 sec/batch; 76h:15m:02s remains)
INFO - root - 2017-12-09 07:25:19.901000: step 1730, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.879 sec/batch; 80h:46m:52s remains)
INFO - root - 2017-12-09 07:25:28.571578: step 1740, loss = 0.82, batch loss = 0.69 (9.7 examples/sec; 0.829 sec/batch; 76h:07m:58s remains)
INFO - root - 2017-12-09 07:25:37.282441: step 1750, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.880 sec/batch; 80h:50m:33s remains)
INFO - root - 2017-12-09 07:25:45.888572: step 1760, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.883 sec/batch; 81h:05m:22s remains)
INFO - root - 2017-12-09 07:25:54.606668: step 1770, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.850 sec/batch; 78h:07m:36s remains)
INFO - root - 2017-12-09 07:26:03.221030: step 1780, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 79h:10m:41s remains)
INFO - root - 2017-12-09 07:26:11.944285: step 1790, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.866 sec/batch; 79h:32m:42s remains)
INFO - root - 2017-12-09 07:26:20.626549: step 1800, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.876 sec/batch; 80h:28m:42s remains)
2017-12-09 07:26:21.575587: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.0038902392 0.0042040516 0.0043267813 0.004221356 0.0038812652 0.0033339115 0.0026533848 0.0019497591 0.0013247621 0.00085060642 0.00055317307 0.00040047921 0.00034042509 0.00032576843 0.00032519872][0.0044819056 0.0049205786 0.0051751668 0.0052040708 0.0049678618 0.0044647455 0.003743296 0.0029182027 0.0021260453 0.001487645 0.0010695262 0.0008558798 0.00078266207 0.00077804475 0.00078120147][0.0047796182 0.0053322013 0.0057342756 0.0059414012 0.0058906171 0.0055468865 0.004916376 0.0040930104 0.003227233 0.0024820436 0.0019683836 0.0016990645 0.0016048842 0.0015909863 0.0015711811][0.0048495759 0.0054891543 0.0060261311 0.0064283162 0.0066106757 0.0065054931 0.0060758255 0.0053859814 0.0045738667 0.0038201825 0.0032636721 0.0029511799 0.0028187174 0.0027606077 0.0026757112][0.0047963285 0.0054820334 0.0061244946 0.0067098313 0.0071386737 0.0073134005 0.0071558906 0.006694762 0.0060405023 0.0053664353 0.004823491 0.004487128 0.0043105162 0.004189434 0.004018161][0.0047301762 0.0054247547 0.0061176107 0.0068382765 0.00748476 0.0079390723 0.0080853626 0.00790881 0.0074848491 0.0069584111 0.0064725121 0.0061192922 0.0058779116 0.0056667509 0.005385743][0.0046680495 0.0053517246 0.0060621058 0.0068580597 0.0076539209 0.0083230436 0.0087304879 0.0088228565 0.0086354492 0.008278097 0.0078697857 0.0075006871 0.0071804561 0.006868449 0.0064738006][0.0045452267 0.005201885 0.0059002 0.0067221196 0.0075925384 0.0083907945 0.0089788362 0.0092698382 0.0092606433 0.0090337619 0.0086880773 0.008306955 0.0079160649 0.0075136418 0.0070282007][0.0042623989 0.0048736129 0.005536187 0.0063439268 0.007230707 0.0080898283 0.0087811388 0.0091877608 0.009271238 0.00909872 0.0087634372 0.0083541181 0.00791239 0.0074527259 0.0069163861][0.003768418 0.0043106219 0.0049101547 0.005665103 0.0065129297 0.0073627341 0.0080786264 0.00852511 0.0086371237 0.0084626954 0.0081010489 0.0076547079 0.0071813604 0.0067056646 0.0061792783][0.0030902107 0.0035555398 0.0040685674 0.004727751 0.0054814317 0.0062496271 0.0069070472 0.0073298663 0.0074411831 0.0072628879 0.0068853414 0.00641689 0.0059317891 0.0054597184 0.0049718353][0.0023202016 0.0026983111 0.0031146756 0.0036513403 0.0042694253 0.0049006394 0.0054359538 0.0057799518 0.0058635743 0.0056966082 0.0053407163 0.0048978822 0.0044423416 0.0040129875 0.0035982076][0.0015665417 0.001847769 0.0021569573 0.0025565163 0.0030201548 0.0034941938 0.0038906503 0.0041436222 0.0041981144 0.004060383 0.0037629143 0.0033886526 0.0030030473 0.0026454348 0.002323491][0.00094841543 0.0011366239 0.0013409021 0.0016064333 0.0019188133 0.002238475 0.0025037769 0.0026693903 0.0027005142 0.0026021886 0.0023887365 0.00211787 0.0018371759 0.0015778509 0.0013575192][0.00054663245 0.00065208063 0.00076572166 0.00091705885 0.0011006478 0.0012946689 0.001456795 0.0015563321 0.001572125 0.0015094514 0.00137713 0.0012102771 0.0010372798 0.00088233739 0.00075835222]]...]
INFO - root - 2017-12-09 07:26:30.258521: step 1810, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.854 sec/batch; 78h:27m:38s remains)
INFO - root - 2017-12-09 07:26:38.921534: step 1820, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.856 sec/batch; 78h:39m:30s remains)
INFO - root - 2017-12-09 07:26:47.315955: step 1830, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.850 sec/batch; 78h:03m:25s remains)
INFO - root - 2017-12-09 07:26:55.903016: step 1840, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.865 sec/batch; 79h:27m:38s remains)
INFO - root - 2017-12-09 07:27:04.513464: step 1850, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 79h:47m:02s remains)
INFO - root - 2017-12-09 07:27:13.132854: step 1860, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.858 sec/batch; 78h:49m:24s remains)
INFO - root - 2017-12-09 07:27:21.690644: step 1870, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 79h:58m:58s remains)
INFO - root - 2017-12-09 07:27:30.146801: step 1880, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.870 sec/batch; 79h:54m:59s remains)
INFO - root - 2017-12-09 07:27:38.816613: step 1890, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.891 sec/batch; 81h:50m:20s remains)
INFO - root - 2017-12-09 07:27:47.460147: step 1900, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.850 sec/batch; 78h:03m:54s remains)
2017-12-09 07:27:48.313164: I tensorflow/core/kernels/logging_ops.cc:79] [[[9.6921045e-05 0.00010038834 9.8216573e-05 9.0938323e-05 8.1146871e-05 7.1300485e-05 6.2714738e-05 5.787141e-05 5.3684169e-05 5.0318682e-05 4.8073467e-05 4.7582107e-05 4.8534435e-05 5.101625e-05 5.552089e-05][0.00010535592 0.00010938328 0.0001066565 9.7994554e-05 8.6426553e-05 7.4893767e-05 6.5582259e-05 5.9835496e-05 5.5488359e-05 5.2168289e-05 5.0050126e-05 4.8792877e-05 4.9378861e-05 5.2370771e-05 5.7499819e-05][9.7367119e-05 0.00010190385 9.9732519e-05 9.1689973e-05 8.1170168e-05 7.0952367e-05 6.26141e-05 5.6699675e-05 5.2375159e-05 4.9514529e-05 4.789271e-05 4.631756e-05 4.5809182e-05 4.7347959e-05 5.1510491e-05][8.2450366e-05 8.8742432e-05 8.8410343e-05 8.29044e-05 7.5002157e-05 6.6768669e-05 5.8925471e-05 5.329532e-05 4.9489405e-05 4.7170128e-05 4.5868423e-05 4.4262095e-05 4.2809894e-05 4.288576e-05 4.5334709e-05][6.911988e-05 7.7367637e-05 7.9651007e-05 7.6861048e-05 7.1480972e-05 6.6049986e-05 6.0067796e-05 5.6416509e-05 5.4997938e-05 5.5014512e-05 5.523015e-05 5.478099e-05 5.2919786e-05 5.0825111e-05 4.9718336e-05][5.9820944e-05 6.8435045e-05 7.2930408e-05 7.5048687e-05 7.4992146e-05 7.3327181e-05 7.0181079e-05 6.97609e-05 7.2991548e-05 7.7816941e-05 8.1000951e-05 8.0306956e-05 7.62535e-05 6.9386289e-05 6.2700288e-05][5.7722893e-05 6.9672831e-05 7.8511192e-05 8.55889e-05 9.0979564e-05 9.3541188e-05 9.3533825e-05 9.634508e-05 0.00010306684 0.00011151795 0.00011767108 0.00011784307 0.00011171657 0.0001008364 8.8188448e-05][5.9297949e-05 7.4618591e-05 8.8067929e-05 0.00010087068 0.00011332271 0.00012283653 0.0001303855 0.00013847214 0.00014715662 0.00015601928 0.00016122937 0.00015961393 0.00015159043 0.00013723013 0.00011962703][6.267189e-05 7.8976882e-05 9.4903568e-05 0.00011131704 0.0001277571 0.0001415669 0.00015374534 0.00016449257 0.00017339971 0.00018147964 0.00018580866 0.00018298117 0.00017468943 0.0001598551 0.00014166125][6.2247105e-05 7.6058488e-05 9.0074544e-05 0.00010561916 0.00012186218 0.00013569969 0.00014680508 0.00015644669 0.00016572341 0.00017402992 0.00017803708 0.00017573759 0.00016945628 0.00015829546 0.00014385811][5.9873651e-05 6.9870672e-05 7.8795645e-05 8.8443274e-05 9.95646e-05 0.00010968946 0.00011761569 0.00012356888 0.00013020189 0.00013691874 0.00014095563 0.00014187714 0.00013935831 0.00013410843 0.00012726137][6.2235529e-05 6.7821034e-05 7.1260009e-05 7.5076343e-05 8.0299105e-05 8.5791406e-05 9.0442729e-05 9.4922332e-05 9.9783087e-05 0.00010555092 0.00011068923 0.00011406245 0.0001147852 0.00011277061 0.0001103961][6.5658154e-05 6.7768888e-05 6.6841167e-05 6.6911249e-05 6.8135421e-05 7.0759226e-05 7.4345691e-05 7.9034573e-05 8.3705592e-05 8.832795e-05 9.3802926e-05 9.8259807e-05 0.00010108057 0.00010156767 0.00010095099][6.8133457e-05 6.9279195e-05 6.664839e-05 6.5117965e-05 6.4885644e-05 6.7420362e-05 7.1174887e-05 7.5476448e-05 8.0106285e-05 8.4302046e-05 8.9587906e-05 9.4456133e-05 9.8145705e-05 9.9669407e-05 0.00010055194][7.1164817e-05 7.3213021e-05 7.0820308e-05 6.9215072e-05 6.9230235e-05 7.2752438e-05 7.7194e-05 8.1544982e-05 8.595307e-05 8.9136658e-05 9.2904957e-05 9.5834577e-05 9.8999764e-05 0.00010161055 0.00010468929]]...]
INFO - root - 2017-12-09 07:27:56.922543: step 1910, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.852 sec/batch; 78h:12m:44s remains)
INFO - root - 2017-12-09 07:28:05.571841: step 1920, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.904 sec/batch; 83h:00m:18s remains)
INFO - root - 2017-12-09 07:28:13.841994: step 1930, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.843 sec/batch; 77h:24m:33s remains)
INFO - root - 2017-12-09 07:28:22.256355: step 1940, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.851 sec/batch; 78h:08m:53s remains)
INFO - root - 2017-12-09 07:28:31.129937: step 1950, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 79h:39m:44s remains)
INFO - root - 2017-12-09 07:28:39.842735: step 1960, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 79h:59m:16s remains)
INFO - root - 2017-12-09 07:28:48.615238: step 1970, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.873 sec/batch; 80h:10m:41s remains)
INFO - root - 2017-12-09 07:28:57.140014: step 1980, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.865 sec/batch; 79h:22m:58s remains)
INFO - root - 2017-12-09 07:29:05.841867: step 1990, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.885 sec/batch; 81h:12m:33s remains)
INFO - root - 2017-12-09 07:29:14.527711: step 2000, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.898 sec/batch; 82h:26m:54s remains)
2017-12-09 07:29:15.381552: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.0002199095 0.00026140825 0.00029570219 0.000325827 0.00035548984 0.00039290552 0.00044983535 0.00053083547 0.00062910456 0.00072676007 0.00079784874 0.00081938843 0.00077880791 0.00068542123 0.00056211546][0.00034813464 0.00044529207 0.00053160859 0.0006046762 0.00066990522 0.00074202172 0.00084144593 0.000973863 0.0011278036 0.0012773564 0.0013824479 0.0014048207 0.0013223891 0.0011519532 0.00093111722][0.00058430817 0.00077985961 0.00095658854 0.0010998236 0.0012147026 0.0013230317 0.0014554024 0.0016222234 0.0018096401 0.0019862871 0.0021011664 0.0021028316 0.0019599278 0.0016969984 0.0013644998][0.000958577 0.0013004453 0.0016076592 0.0018468283 0.0020203448 0.0021551815 0.0022941085 0.0024555072 0.002631885 0.00279141 0.0028760852 0.0028287112 0.0026115482 0.0022520185 0.0018097148][0.0014534836 0.001975043 0.0024393303 0.0027876531 0.0030147154 0.0031537379 0.0032599375 0.0033667732 0.0034780479 0.0035669149 0.0035781614 0.0034561127 0.0031601249 0.0027154081 0.0021836956][0.0019945779 0.0027001621 0.0033183969 0.0037686443 0.0040369807 0.0041576009 0.0041952389 0.0042055403 0.004206459 0.00418052 0.0040856968 0.0038752351 0.0035090232 0.0030034494 0.0024163653][0.0024868099 0.0033500565 0.0040938165 0.00462187 0.0049108882 0.00499185 0.0049386327 0.0048283567 0.0046978272 0.0045392485 0.0043241908 0.0040261676 0.003609847 0.0030777333 0.0024749611][0.0028374041 0.0038053636 0.0046315352 0.0052096336 0.0054959878 0.0055180206 0.0053635263 0.0051291361 0.0048679914 0.0045865057 0.004269552 0.0039070733 0.0034699761 0.0029457014 0.0023647295][0.0029662927 0.0039705914 0.0048265108 0.0054169213 0.0056797815 0.0056364238 0.0053867977 0.0050427923 0.00467322 0.0043037347 0.0039284388 0.003539213 0.0031128812 0.0026276852 0.0021022775][0.0028250706 0.0037882021 0.0046132742 0.0051736515 0.0054034069 0.005315545 0.005001294 0.0045817378 0.0041429354 0.00372604 0.0033360231 0.0029621511 0.0025809158 0.0021662933 0.0017270001][0.0024413827 0.0032880646 0.0040201582 0.004514168 0.0047077714 0.0046039829 0.0042733592 0.0038333184 0.0033786159 0.0029629099 0.0025995467 0.0022757822 0.0019664839 0.0016437052 0.0013081611][0.0019006258 0.0025780702 0.0031739541 0.0035795521 0.0037358261 0.0036367914 0.0033353893 0.0029332368 0.0025180276 0.0021508038 0.0018491079 0.0015983658 0.0013737727 0.001148703 0.00091795024][0.0013224549 0.0018083287 0.0022465521 0.0025528944 0.0026747787 0.0025976226 0.0023582599 0.0020368572 0.0017057697 0.0014185582 0.0011947157 0.0010237157 0.0008819346 0.00074491242 0.00060537719][0.00081809534 0.001125464 0.0014107516 0.0016199655 0.0017104638 0.0016646003 0.0015034736 0.0012825562 0.0010533568 0.00085584348 0.00070725719 0.00060293853 0.00052529061 0.00045432613 0.00038347609][0.00046548166 0.00063439709 0.00079648686 0.00092420832 0.00098883559 0.00097202166 0.0008832672 0.00075461162 0.00061693683 0.00049597753 0.00040660091 0.00034816371 0.00031055533 0.00028036252 0.00025211874]]...]
INFO - root - 2017-12-09 07:29:24.005662: step 2010, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.866 sec/batch; 79h:27m:57s remains)
INFO - root - 2017-12-09 07:29:32.715769: step 2020, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.886 sec/batch; 81h:20m:15s remains)
INFO - root - 2017-12-09 07:29:41.040320: step 2030, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 79h:19m:50s remains)
INFO - root - 2017-12-09 07:29:49.501486: step 2040, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.847 sec/batch; 77h:44m:46s remains)
INFO - root - 2017-12-09 07:29:58.054712: step 2050, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.888 sec/batch; 81h:32m:50s remains)
INFO - root - 2017-12-09 07:30:06.762890: step 2060, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.866 sec/batch; 79h:26m:45s remains)
INFO - root - 2017-12-09 07:30:15.445859: step 2070, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.878 sec/batch; 80h:33m:28s remains)
INFO - root - 2017-12-09 07:30:23.914655: step 2080, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.881 sec/batch; 80h:52m:51s remains)
INFO - root - 2017-12-09 07:30:32.640672: step 2090, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.886 sec/batch; 81h:20m:12s remains)
INFO - root - 2017-12-09 07:30:41.317790: step 2100, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 79h:45m:14s remains)
2017-12-09 07:30:42.148056: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.0025179894 0.0029819168 0.0032836413 0.0034386814 0.0034822687 0.003465225 0.0034256356 0.0033771635 0.0032692458 0.003060176 0.0027060711 0.0021871142 0.0015937906 0.0010520868 0.00067899708][0.0026598771 0.003136077 0.0034419391 0.0036006535 0.0036544325 0.003645543 0.0036064794 0.0035497723 0.0034330203 0.0032022242 0.002814936 0.0022583182 0.001628815 0.0010723574 0.00070442026][0.0027611167 0.0032092221 0.0034806298 0.0036144359 0.0036672419 0.003675675 0.0036544343 0.0035994069 0.0034775375 0.0032393544 0.0028374079 0.002264261 0.0016210678 0.0010601814 0.00070938992][0.0028469497 0.0032492829 0.0034610727 0.0035431283 0.0035840324 0.0036229889 0.003646764 0.003626361 0.0035227858 0.0032933792 0.002897349 0.0023194256 0.0016613689 0.001083472 0.00073320564][0.0028509505 0.0031875223 0.0033189706 0.0033268265 0.0033335602 0.0034032781 0.0034966054 0.0035474766 0.0034955232 0.0033057041 0.0029422475 0.0023875423 0.0017364068 0.0011502869 0.00079594221][0.0027702977 0.0030377361 0.0030869171 0.003010784 0.0029704892 0.0030669896 0.0032533151 0.003411711 0.0034345586 0.0032959965 0.0029779056 0.0024672414 0.0018422725 0.0012605522 0.00089863356][0.002621084 0.0028154792 0.0027861907 0.0026299094 0.0025417183 0.0026597127 0.002939675 0.0032206455 0.0033423298 0.0032714631 0.0030145634 0.0025708571 0.0019957274 0.0014336305 0.0010658059][0.0024077878 0.0025302479 0.0024356358 0.0022206935 0.0020995147 0.0022405307 0.0025990193 0.0029887478 0.0032103497 0.0032218897 0.0030432073 0.0026807189 0.0021738503 0.0016519361 0.0012884047][0.0021375357 0.002201926 0.0020705622 0.0018312212 0.0017011767 0.0018590128 0.0022630477 0.002728818 0.0030358883 0.0031324618 0.0030416781 0.0027695464 0.0023490521 0.0018909412 0.0015495479][0.0018275199 0.0018504553 0.001713183 0.0014970989 0.0013932815 0.0015696104 0.0019903667 0.0024864317 0.0028480173 0.00301048 0.0029991185 0.0028169276 0.0024956812 0.0021209165 0.0018213668][0.0014855849 0.0014956949 0.0013858714 0.0012258661 0.0011743876 0.0013676028 0.0017765673 0.0022625253 0.0026453286 0.0028469812 0.0028925487 0.0027933624 0.002572404 0.00229794 0.0020667121][0.0011554015 0.0011523747 0.0010794875 0.00099334819 0.0010039191 0.0012066051 0.0015803354 0.0020254017 0.0023930527 0.0026126823 0.0026977072 0.0026708241 0.0025522267 0.0023925588 0.0022547806][0.000874146 0.00084997341 0.00080604473 0.00077960361 0.00084160041 0.0010447464 0.0013682368 0.0017471496 0.0020721634 0.0022852039 0.0023970611 0.0024380041 0.0024284769 0.0023925405 0.0023602273][0.00069591461 0.00063760829 0.0005978453 0.00060490944 0.00069642707 0.00089061441 0.0011614948 0.0014621114 0.0017237235 0.0019128527 0.0020407187 0.0021423136 0.0022345844 0.0023194973 0.0023969125][0.0006714107 0.00057684456 0.00052045914 0.00052962149 0.0006198237 0.00078875624 0.0010044166 0.0012313988 0.0014279211 0.0015773274 0.0017023765 0.0018471978 0.0020215414 0.0022049556 0.0023803744]]...]
INFO - root - 2017-12-09 07:30:50.717298: step 2110, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.855 sec/batch; 78h:27m:32s remains)
INFO - root - 2017-12-09 07:30:59.398427: step 2120, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 79h:34m:02s remains)
INFO - root - 2017-12-09 07:31:07.579282: step 2130, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 79h:47m:26s remains)
INFO - root - 2017-12-09 07:31:16.099966: step 2140, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.836 sec/batch; 76h:41m:21s remains)
INFO - root - 2017-12-09 07:31:24.610629: step 2150, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.884 sec/batch; 81h:04m:33s remains)
INFO - root - 2017-12-09 07:31:33.295486: step 2160, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.874 sec/batch; 80h:13m:55s remains)
INFO - root - 2017-12-09 07:31:41.984521: step 2170, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.874 sec/batch; 80h:10m:56s remains)
INFO - root - 2017-12-09 07:31:50.599809: step 2180, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 79h:34m:46s remains)
INFO - root - 2017-12-09 07:31:59.300132: step 2190, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 79h:57m:30s remains)
INFO - root - 2017-12-09 07:32:07.895849: step 2200, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.880 sec/batch; 80h:42m:43s remains)
2017-12-09 07:32:08.710117: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.00021320384 0.00031829745 0.00055037515 0.0010032926 0.0017163039 0.0026209024 0.0035490831 0.0042687966 0.0046098586 0.0045144223 0.0040370096 0.003286436 0.0024225791 0.0016560779 0.0011789032][0.00015781593 0.00023867184 0.00044466418 0.00088098808 0.0016046801 0.0025467481 0.0035052635 0.0042036031 0.0044494304 0.0042089308 0.0035916569 0.0027621915 0.0019044919 0.0012066318 0.00082725153][0.00011514132 0.00018377905 0.00037715351 0.00080686581 0.0015327254 0.0024814499 0.0034264992 0.0040608225 0.0041837003 0.0037935388 0.0030583818 0.0021887857 0.0013929211 0.00082290528 0.00057404465][0.000105309 0.00017628376 0.00038369425 0.00083584589 0.0015907331 0.0025591638 0.0034882256 0.0040496537 0.0040499694 0.0035163458 0.0026700627 0.001768113 0.001028096 0.00056735787 0.00042416318][0.00010915993 0.00019413175 0.00043316212 0.00093231059 0.0017400003 0.0027399003 0.0036477523 0.0041255881 0.0040017869 0.00333779 0.002402992 0.0014809864 0.00079125841 0.000416859 0.00035653848][0.00012213516 0.00022858096 0.00050746265 0.0010625308 0.0019273348 0.0029600055 0.0038441278 0.0042426935 0.0040056035 0.0032300847 0.0022234311 0.0012851784 0.00063529389 0.00032924153 0.00034247013][0.00013211821 0.000259533 0.00057747634 0.0011859597 0.0021040742 0.0031642753 0.0040323054 0.0043765516 0.0040572574 0.003199022 0.002134392 0.0011762368 0.00054698309 0.00029045183 0.0003650561][0.00014048658 0.00027584954 0.00061476283 0.0012532014 0.0022009907 0.0032757476 0.0041378494 0.004464685 0.0041184425 0.0032280786 0.0021321457 0.0011514915 0.00052124483 0.00028917711 0.0004021565][0.00013926404 0.00026970063 0.00060031825 0.0012269825 0.0021624104 0.0032254546 0.0040837466 0.0044247606 0.0041057682 0.0032418768 0.0021511232 0.0011655164 0.000529315 0.00030101134 0.0004252996][0.00012762731 0.00024643025 0.00054009963 0.0011045586 0.0019590983 0.0029465426 0.0037617672 0.0041180267 0.0038709897 0.0031017 0.0020844492 0.001146657 0.00053394731 0.00031029613 0.0004237938][0.00011245559 0.0002058259 0.00044143625 0.00090366526 0.0016184135 0.0024655049 0.0031860275 0.0035352493 0.003374204 0.0027473245 0.0018732246 0.0010500982 0.00050282071 0.00029573208 0.00038538998][9.48851e-05 0.00015880293 0.00032767761 0.000666326 0.0012089786 0.0018671668 0.0024560655 0.0027739299 0.0026952568 0.0022346103 0.0015529266 0.00089545822 0.0004476787 0.00026955275 0.00032655094][8.2005907e-05 0.00012022437 0.000223346 0.00043694975 0.00079866825 0.001259879 0.0016960851 0.0019532184 0.0019332707 0.0016313617 0.0011585081 0.000691691 0.00036703504 0.00023033572 0.00025711016][6.727341e-05 9.2001726e-05 0.00014500369 0.00025701997 0.00045921505 0.00073465914 0.0010151617 0.0011961783 0.0012052744 0.0010314101 0.00074600964 0.00046211565 0.00026167621 0.00017435712 0.00018273795][6.0709332e-05 7.4819705e-05 9.9401463e-05 0.00014471909 0.00023490394 0.00036403784 0.00050643767 0.0006041677 0.00061756308 0.00053621014 0.00039707447 0.00025845174 0.00015983355 0.00011605015 0.00011847352]]...]
INFO - root - 2017-12-09 07:32:17.426618: step 2210, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 79h:53m:57s remains)
INFO - root - 2017-12-09 07:32:26.091680: step 2220, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.880 sec/batch; 80h:42m:07s remains)
INFO - root - 2017-12-09 07:32:34.081020: step 2230, loss = 0.82, batch loss = 0.69 (12.3 examples/sec; 0.649 sec/batch; 59h:34m:14s remains)
INFO - root - 2017-12-09 07:32:42.689508: step 2240, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 79h:35m:46s remains)
INFO - root - 2017-12-09 07:32:51.388606: step 2250, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.850 sec/batch; 77h:59m:57s remains)
INFO - root - 2017-12-09 07:33:00.087423: step 2260, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.860 sec/batch; 78h:55m:34s remains)
INFO - root - 2017-12-09 07:33:08.787519: step 2270, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.873 sec/batch; 80h:07m:20s remains)
INFO - root - 2017-12-09 07:33:17.239912: step 2280, loss = 0.82, batch loss = 0.69 (9.7 examples/sec; 0.822 sec/batch; 75h:22m:40s remains)
INFO - root - 2017-12-09 07:33:25.759577: step 2290, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.847 sec/batch; 77h:39m:38s remains)
INFO - root - 2017-12-09 07:33:34.413910: step 2300, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 79h:35m:01s remains)
2017-12-09 07:33:35.285717: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.00066416437 0.00084789773 0.00095429335 0.0010021783 0.0010054943 0.00097684935 0.00092715112 0.00088101206 0.00084058772 0.00083351048 0.00086080743 0.00090729818 0.00095824915 0.0010076459 0.0010333619][0.00069637655 0.00087346177 0.00096938404 0.0010077484 0.00099916488 0.00095869828 0.0008972541 0.00084495597 0.00081322144 0.00081743882 0.00085450686 0.00090601813 0.00095595239 0.0010000665 0.0010180162][0.00069112849 0.000859154 0.00094518665 0.0009734531 0.00095529394 0.00090537581 0.00083993725 0.00078592292 0.00076240825 0.00077248411 0.00081015745 0.00085952354 0.00090622838 0.00094159093 0.000949306][0.00064217224 0.00080199429 0.00088350824 0.0009037311 0.00087997218 0.00082960387 0.00076995906 0.00072008936 0.00069875631 0.00070523046 0.00073625974 0.00077756273 0.00081821985 0.00084586011 0.000845842][0.00055706367 0.00070669531 0.00078767288 0.00080203125 0.00077431666 0.00072794623 0.00067852286 0.00063763751 0.00061734318 0.00061621552 0.00063416042 0.00066284445 0.00069207867 0.00071064255 0.0007041457][0.00045073978 0.00058772607 0.00066629227 0.00067476305 0.00064348377 0.00059974438 0.00056076789 0.00052956294 0.00051256572 0.00050737715 0.00051462481 0.00052976276 0.0005450481 0.00055315 0.00054087036][0.00035246808 0.00047349784 0.0005441496 0.0005491199 0.00051642844 0.00047445885 0.0004398679 0.00041202616 0.00039577018 0.00038879437 0.00038985678 0.00039371828 0.000395553 0.00039358367 0.00038066652][0.00028318726 0.00038766439 0.00044490924 0.00044429197 0.00040941336 0.00036618271 0.00033240826 0.00030648906 0.00028963585 0.00027911831 0.00027526764 0.00027226095 0.00026896477 0.00026597755 0.00025775618][0.00025107595 0.0003403039 0.00038228274 0.000374265 0.00033296546 0.00028458488 0.00024764391 0.00022391224 0.00021066976 0.00020215321 0.00019628699 0.00019131592 0.00018833253 0.00018914255 0.00019016064][0.00024320415 0.00032362976 0.00035393229 0.0003369326 0.00028699124 0.00023051191 0.0001894835 0.00016828327 0.00015995474 0.00015435937 0.0001507241 0.00015002844 0.00015375951 0.00016224115 0.00017395371][0.00023760065 0.00031412413 0.0003389396 0.00031592848 0.00026034386 0.0001994621 0.00015744286 0.00013805178 0.00013487035 0.00013578177 0.00013951979 0.00014673121 0.00015918488 0.00017669523 0.00019815886][0.00021834168 0.00029274571 0.00031743862 0.00029263561 0.00023680914 0.00017705222 0.00013797838 0.00012265301 0.00012404907 0.00013332008 0.00014731858 0.00016458856 0.0001869699 0.00021406535 0.00024180522][0.00018244564 0.00025079533 0.00027479584 0.00025101207 0.00020001603 0.00014870084 0.00011859667 0.00010923392 0.00011696664 0.00013389207 0.00015777457 0.00018777723 0.00022135011 0.00025657326 0.00028857929][0.00013829031 0.00019330261 0.00021330506 0.0001942408 0.00015550775 0.00011875812 0.00010017685 9.7505224e-05 0.00011016295 0.00013226552 0.00016348629 0.00020213563 0.00024280723 0.00028232572 0.00031614935][9.8748191e-05 0.00013795483 0.00015048782 0.00013768087 0.00011388423 9.22223e-05 8.1056074e-05 7.893602e-05 9.1552211e-05 0.00011490402 0.00014818789 0.00018902487 0.000233458 0.00027505562 0.00030948664]]...]
INFO - root - 2017-12-09 07:33:43.952435: step 2310, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 79h:40m:40s remains)
INFO - root - 2017-12-09 07:33:52.613501: step 2320, loss = 0.82, batch loss = 0.69 (9.7 examples/sec; 0.829 sec/batch; 76h:00m:41s remains)
INFO - root - 2017-12-09 07:34:00.953813: step 2330, loss = 0.82, batch loss = 0.69 (11.0 examples/sec; 0.726 sec/batch; 66h:36m:01s remains)
INFO - root - 2017-12-09 07:34:09.688585: step 2340, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 79h:50m:25s remains)
INFO - root - 2017-12-09 07:34:18.488521: step 2350, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.885 sec/batch; 81h:09m:54s remains)
INFO - root - 2017-12-09 07:34:27.237743: step 2360, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.861 sec/batch; 78h:58m:22s remains)
INFO - root - 2017-12-09 07:34:36.021551: step 2370, loss = 0.82, batch loss = 0.69 (8.8 examples/sec; 0.907 sec/batch; 83h:09m:14s remains)
INFO - root - 2017-12-09 07:34:44.855848: step 2380, loss = 0.82, batch loss = 0.69 (8.5 examples/sec; 0.936 sec/batch; 85h:50m:55s remains)
INFO - root - 2017-12-09 07:34:53.522460: step 2390, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.860 sec/batch; 78h:50m:54s remains)
INFO - root - 2017-12-09 07:35:02.154998: step 2400, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.854 sec/batch; 78h:19m:29s remains)
2017-12-09 07:35:03.022417: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.004735149 0.0052014347 0.0054166825 0.00538184 0.0051497952 0.004766569 0.0043218094 0.0038696788 0.0035059345 0.0032860041 0.0032189658 0.003313648 0.0035575891 0.00392906 0.0043417397][0.0049781809 0.0055677546 0.0059028566 0.0059745614 0.005832525 0.0055136895 0.0050990293 0.0046513337 0.0042868285 0.0040808017 0.0040417421 0.0041645155 0.0044242633 0.0048099831 0.0052322755][0.0049574357 0.0056361444 0.0060772276 0.0062560141 0.00622006 0.0060035777 0.0056662657 0.0052714688 0.0049424036 0.0047628754 0.0047527147 0.0048869578 0.0051478562 0.0055320249 0.0059382753][0.0048728222 0.00561423 0.0061497968 0.0064344169 0.0065022809 0.0063792686 0.0061230366 0.0057966649 0.0055137421 0.0053653782 0.0053771422 0.0055072145 0.0057543274 0.0061135693 0.0064788428][0.0047252993 0.0055068117 0.0061071757 0.0064768395 0.0066454378 0.0066319457 0.0064844363 0.0062480411 0.0060279146 0.005912073 0.0059232786 0.0060281218 0.0062393178 0.0065492387 0.0068529518][0.0045235935 0.0053278729 0.0059798234 0.0064246748 0.006675716 0.0067620361 0.006723512 0.0065929559 0.0064552897 0.0063827783 0.0063908836 0.0064607351 0.0066178441 0.0068557276 0.0070807366][0.0042450284 0.0050747353 0.0057800873 0.0062945737 0.0066176555 0.0067920024 0.0068538594 0.0068216487 0.0067605828 0.0067265956 0.0067283818 0.0067622336 0.0068606106 0.0070207003 0.007165445][0.0039299056 0.0047579138 0.0054955194 0.0060668415 0.0064515825 0.0066981059 0.0068415944 0.0068969857 0.0069094752 0.0069100633 0.0069068889 0.0069126114 0.0069589368 0.0070468565 0.0071181757][0.0036625804 0.004446323 0.0051665953 0.0057479134 0.0061680209 0.0064697824 0.0066814488 0.0068141241 0.0068985634 0.006944784 0.00695357 0.0069485647 0.006957036 0.0069845472 0.00699304][0.0035516478 0.0042481706 0.004895045 0.0054296236 0.00584485 0.0061701895 0.0064262948 0.0066202837 0.0067710252 0.0068722819 0.0069146412 0.006917729 0.006902263 0.0068760649 0.0068226727][0.0036266407 0.004210691 0.0047392631 0.0051795929 0.0055444972 0.0058496268 0.006111342 0.0063335854 0.0065234471 0.0066690948 0.0067532407 0.0067806346 0.0067593614 0.0066990759 0.0065973974][0.0038339463 0.0043035974 0.0046982886 0.005028428 0.005316346 0.0055691688 0.0057963459 0.0060000764 0.0061865849 0.0063467934 0.00645873 0.0065108431 0.00649885 0.0064249369 0.0062946649][0.0040527917 0.0044353441 0.0047205822 0.0049461522 0.0051513566 0.00533753 0.005502956 0.0056502786 0.0057902592 0.0059236726 0.0060336236 0.0060986984 0.0061031617 0.0060339924 0.0058947578][0.0041318755 0.0044597243 0.0046718032 0.0048286533 0.0049700704 0.0050956313 0.0051971315 0.0052735591 0.0053440118 0.0054233358 0.0055046738 0.0055592488 0.0055722133 0.0055175317 0.0053881803][0.0040146727 0.0042959214 0.0044576791 0.0045754225 0.0046824962 0.004770759 0.0048249071 0.00483731 0.0048308559 0.004831925 0.0048530088 0.0048739631 0.0048841368 0.0048458413 0.0047412477]]...]
INFO - root - 2017-12-09 07:35:11.624914: step 2410, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.832 sec/batch; 76h:19m:38s remains)
INFO - root - 2017-12-09 07:35:20.293968: step 2420, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.893 sec/batch; 81h:55m:06s remains)
INFO - root - 2017-12-09 07:35:28.785883: step 2430, loss = 0.82, batch loss = 0.69 (11.1 examples/sec; 0.723 sec/batch; 66h:18m:18s remains)
INFO - root - 2017-12-09 07:35:37.315040: step 2440, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.854 sec/batch; 78h:16m:56s remains)
INFO - root - 2017-12-09 07:35:45.933140: step 2450, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 79h:29m:59s remains)
INFO - root - 2017-12-09 07:35:54.499859: step 2460, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 79h:11m:19s remains)
INFO - root - 2017-12-09 07:36:03.102995: step 2470, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.832 sec/batch; 76h:15m:55s remains)
INFO - root - 2017-12-09 07:36:11.784884: step 2480, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.873 sec/batch; 80h:03m:05s remains)
INFO - root - 2017-12-09 07:36:20.231363: step 2490, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.865 sec/batch; 79h:18m:27s remains)
INFO - root - 2017-12-09 07:36:28.829064: step 2500, loss = 0.82, batch loss = 0.69 (9.8 examples/sec; 0.816 sec/batch; 74h:47m:14s remains)
2017-12-09 07:36:29.679755: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.0067247013 0.0075622471 0.0079194717 0.0078325989 0.007337864 0.0064964821 0.0054513095 0.0044407686 0.0037391777 0.0035801015 0.0040930896 0.0053019184 0.0070368559 0.0090362057 0.010934534][0.0083495192 0.00926926 0.0096571557 0.0095949518 0.0091374209 0.0083272476 0.007290788 0.0062632542 0.0055462974 0.0054121609 0.0060034571 0.0073629334 0.0093135582 0.011562482 0.013685649][0.0097801816 0.010709796 0.011123006 0.011125335 0.010787725 0.010135028 0.0092623364 0.0083683077 0.0077443742 0.0076659191 0.008281895 0.0096454611 0.011600733 0.013842756 0.015953707][0.011082779 0.012002808 0.012450513 0.012559609 0.01238744 0.011962784 0.011350068 0.010686388 0.010204672 0.010156608 0.010693192 0.011887383 0.013622878 0.015628655 0.017545445][0.012264993 0.013212697 0.013744166 0.014012444 0.014041935 0.013884646 0.013574122 0.013169891 0.012818554 0.012730043 0.013069923 0.01393787 0.01528068 0.016888918 0.018493541][0.013281154 0.014275776 0.014902346 0.01532553 0.015567446 0.015714508 0.015734755 0.015598764 0.015370185 0.01520176 0.015273494 0.015700698 0.016525619 0.017624669 0.018834395][0.014075287 0.01512129 0.015849408 0.016411047 0.016847845 0.017276311 0.017587358 0.017670374 0.017519761 0.017244026 0.017028652 0.017011803 0.017315391 0.01790054 0.018704111][0.014625153 0.015710345 0.016520133 0.017193157 0.017784884 0.018420815 0.01893484 0.019153453 0.019033832 0.018654777 0.018196521 0.017807594 0.017678708 0.017836163 0.018289538][0.014701336 0.015823849 0.016692253 0.017465064 0.018194735 0.018993866 0.019632163 0.01991209 0.019792002 0.019336741 0.018710552 0.018067015 0.017648136 0.017529601 0.017746998][0.014198391 0.015323545 0.016205698 0.017035557 0.017893588 0.018832386 0.019571729 0.019913321 0.019831372 0.01937479 0.018694034 0.017949989 0.017398788 0.017137295 0.017207285][0.013164227 0.014226757 0.015064414 0.015907798 0.016866008 0.017918414 0.018763619 0.019196877 0.019214317 0.018850341 0.018230427 0.017521629 0.016968807 0.016682053 0.016693885][0.01196897 0.012916693 0.01364433 0.014443214 0.015433962 0.01652473 0.017435547 0.017971378 0.018121112 0.017902173 0.017417153 0.016835388 0.016373068 0.016147075 0.016182682][0.010946228 0.011763957 0.01233107 0.013013206 0.013943162 0.015000346 0.015925104 0.016547816 0.016831648 0.016758889 0.016406773 0.015941938 0.015583376 0.015454609 0.015571776][0.010177172 0.010917285 0.011348391 0.011886766 0.012707507 0.013689125 0.014588589 0.015255207 0.01561964 0.015623574 0.015310799 0.014865428 0.014542558 0.014484928 0.014704643][0.0097444709 0.010469619 0.010820072 0.011239414 0.011942474 0.0128239 0.013669498 0.01432885 0.014697039 0.014679066 0.014284886 0.013730273 0.013326691 0.013252806 0.013525616]]...]
INFO - root - 2017-12-09 07:36:38.183652: step 2510, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.848 sec/batch; 77h:46m:20s remains)
INFO - root - 2017-12-09 07:36:46.847126: step 2520, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.852 sec/batch; 78h:06m:30s remains)
INFO - root - 2017-12-09 07:36:55.387940: step 2530, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 79h:10m:07s remains)
INFO - root - 2017-12-09 07:37:03.845618: step 2540, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.880 sec/batch; 80h:37m:11s remains)
INFO - root - 2017-12-09 07:37:12.331804: step 2550, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.841 sec/batch; 77h:02m:56s remains)
INFO - root - 2017-12-09 07:37:20.814377: step 2560, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.850 sec/batch; 77h:53m:43s remains)
INFO - root - 2017-12-09 07:37:29.326952: step 2570, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.849 sec/batch; 77h:47m:27s remains)
INFO - root - 2017-12-09 07:37:37.913701: step 2580, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.889 sec/batch; 81h:26m:03s remains)
INFO - root - 2017-12-09 07:37:46.582121: step 2590, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 79h:29m:43s remains)
INFO - root - 2017-12-09 07:37:55.225797: step 2600, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 79h:46m:41s remains)
2017-12-09 07:37:56.094509: I tensorflow/core/kernels/logging_ops.cc:79] [[[1.6006081e-05 1.7387989e-05 1.7071288e-05 1.5043926e-05 1.1378193e-05 1.1527663e-05 1.4061923e-05 1.7426286e-05 1.9819054e-05 2.1256907e-05 2.2541804e-05 2.2972061e-05 2.1076907e-05 1.81084e-05 1.7676193e-05][1.540341e-05 1.6595859e-05 1.6874041e-05 1.5759986e-05 1.1997963e-05 1.1359694e-05 1.3195513e-05 1.6348346e-05 1.8941493e-05 2.1010892e-05 2.4392422e-05 2.6915608e-05 2.5340079e-05 2.1361979e-05 1.9169151e-05][1.4270929e-05 1.4950365e-05 1.5321712e-05 1.4618385e-05 1.2131932e-05 1.2256281e-05 1.4021691e-05 1.6524278e-05 1.889807e-05 2.1578177e-05 2.5965157e-05 3.0098956e-05 2.9438852e-05 2.4914596e-05 2.129034e-05][1.2895336e-05 1.3034587e-05 1.2683053e-05 1.1693304e-05 1.0431879e-05 1.163956e-05 1.4246045e-05 1.7348837e-05 2.0425068e-05 2.3420616e-05 2.7520728e-05 3.2542455e-05 3.4127363e-05 3.0842682e-05 2.6419992e-05][1.2681983e-05 1.220949e-05 1.075201e-05 9.0501708e-06 8.5513493e-06 1.0333268e-05 1.3912391e-05 1.7442595e-05 2.1068168e-05 2.3386332e-05 2.6879869e-05 3.2220873e-05 3.6151738e-05 3.5647994e-05 3.2111027e-05][1.3583878e-05 1.2813951e-05 1.0886837e-05 9.0679605e-06 8.63793e-06 1.0217536e-05 1.4273144e-05 1.8255287e-05 2.1805878e-05 2.288579e-05 2.4654379e-05 2.9463205e-05 3.4644523e-05 3.65499e-05 3.4389348e-05][1.4763566e-05 1.3799319e-05 1.171113e-05 1.0525117e-05 1.0310305e-05 1.1673797e-05 1.6048798e-05 2.0742242e-05 2.4579262e-05 2.533244e-05 2.4861925e-05 2.6965783e-05 3.1775358e-05 3.5198849e-05 3.4354496e-05][1.6962251e-05 1.5748479e-05 1.2963319e-05 1.1321375e-05 1.1351694e-05 1.3076202e-05 1.7408056e-05 2.260341e-05 2.7549868e-05 2.9013398e-05 2.8019254e-05 2.8349255e-05 3.0945681e-05 3.4533339e-05 3.5305638e-05][2.1309395e-05 1.9369458e-05 1.571828e-05 1.3699591e-05 1.4108446e-05 1.6537218e-05 2.1048487e-05 2.620634e-05 3.1484509e-05 3.3679709e-05 3.3021679e-05 3.2643824e-05 3.3344972e-05 3.6135687e-05 3.7307866e-05][2.7258917e-05 2.510316e-05 2.0646956e-05 1.825278e-05 1.8975625e-05 2.1934844e-05 2.6943577e-05 3.16293e-05 3.5889352e-05 3.8000173e-05 3.7721577e-05 3.7017388e-05 3.6968748e-05 3.8624683e-05 3.936042e-05][3.1552954e-05 2.948998e-05 2.4750232e-05 2.3295717e-05 2.5052272e-05 2.8442628e-05 3.3553675e-05 3.7643476e-05 4.0159372e-05 4.133772e-05 4.1437728e-05 4.1729378e-05 4.1772764e-05 4.2164749e-05 4.231029e-05][3.5915844e-05 3.4852383e-05 2.9923242e-05 2.8339586e-05 3.0622876e-05 3.3753538e-05 3.8047758e-05 4.2698644e-05 4.4786088e-05 4.4589717e-05 4.4349006e-05 4.5175955e-05 4.5482651e-05 4.5342553e-05 4.5001158e-05][3.7317579e-05 3.7597049e-05 3.3319542e-05 3.1055577e-05 3.2379976e-05 3.4612596e-05 3.7811296e-05 4.1564337e-05 4.3786189e-05 4.3876578e-05 4.3546061e-05 4.4738532e-05 4.56294e-05 4.5579232e-05 4.4862747e-05][3.444418e-05 3.6348836e-05 3.4188786e-05 3.2523654e-05 3.2732998e-05 3.3777738e-05 3.5824996e-05 3.8206417e-05 3.9792649e-05 3.9599247e-05 3.91153e-05 4.0518287e-05 4.154662e-05 4.1538529e-05 4.1131454e-05][2.9925315e-05 3.2274729e-05 3.1772746e-05 3.102723e-05 3.1158779e-05 3.2020427e-05 3.314975e-05 3.4268647e-05 3.5312478e-05 3.4889395e-05 3.4247503e-05 3.5129902e-05 3.6017162e-05 3.5717814e-05 3.5369187e-05]]...]
INFO - root - 2017-12-09 07:38:04.874357: step 2610, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.863 sec/batch; 79h:05m:12s remains)
INFO - root - 2017-12-09 07:38:13.664333: step 2620, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.873 sec/batch; 80h:01m:46s remains)
INFO - root - 2017-12-09 07:38:22.177425: step 2630, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.865 sec/batch; 79h:16m:02s remains)
INFO - root - 2017-12-09 07:38:30.739029: step 2640, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.865 sec/batch; 79h:14m:43s remains)
INFO - root - 2017-12-09 07:38:39.443390: step 2650, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.877 sec/batch; 80h:23m:53s remains)
INFO - root - 2017-12-09 07:38:48.203313: step 2660, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.896 sec/batch; 82h:04m:47s remains)
INFO - root - 2017-12-09 07:38:56.878129: step 2670, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.874 sec/batch; 80h:03m:55s remains)
INFO - root - 2017-12-09 07:39:05.386074: step 2680, loss = 0.82, batch loss = 0.69 (9.7 examples/sec; 0.822 sec/batch; 75h:20m:20s remains)
INFO - root - 2017-12-09 07:39:13.770263: step 2690, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.834 sec/batch; 76h:22m:46s remains)
INFO - root - 2017-12-09 07:39:22.308611: step 2700, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.858 sec/batch; 78h:34m:01s remains)
2017-12-09 07:39:23.208162: I tensorflow/core/kernels/logging_ops.cc:79] [[[4.7504513e-05 5.1180494e-05 5.2659419e-05 5.3385284e-05 5.3920125e-05 5.4991197e-05 5.6364694e-05 5.7271693e-05 5.8169095e-05 5.8723443e-05 5.89613e-05 5.9910981e-05 6.1109531e-05 6.1338418e-05 6.1938161e-05][4.2414245e-05 4.6284065e-05 4.7511665e-05 4.7454803e-05 4.6901474e-05 4.6521647e-05 4.6750643e-05 4.6975791e-05 4.709078e-05 4.735195e-05 4.7658334e-05 4.8932194e-05 5.0322025e-05 5.076372e-05 5.1225881e-05][3.7911439e-05 4.1329247e-05 4.1655559e-05 4.0957002e-05 3.9799666e-05 3.86879e-05 3.7959271e-05 3.7446614e-05 3.6853879e-05 3.6785739e-05 3.6820111e-05 3.7719237e-05 3.8998693e-05 4.0167277e-05 4.139759e-05][3.3343586e-05 3.6022528e-05 3.5876237e-05 3.5590183e-05 3.4919733e-05 3.4153785e-05 3.3082109e-05 3.2301617e-05 3.2117434e-05 3.1930354e-05 3.1617532e-05 3.2538457e-05 3.4733243e-05 3.6605397e-05 3.7951435e-05][2.9237814e-05 3.1630236e-05 3.1254949e-05 3.1232728e-05 3.1324187e-05 3.1730731e-05 3.228449e-05 3.2610038e-05 3.3346998e-05 3.3429842e-05 3.2980093e-05 3.2174812e-05 3.273087e-05 3.3743181e-05 3.483961e-05][2.908472e-05 3.1915897e-05 3.1809297e-05 3.2339387e-05 3.4049106e-05 3.6016e-05 3.7813315e-05 3.9009803e-05 3.9803959e-05 3.9433213e-05 3.7615308e-05 3.4406305e-05 3.2187858e-05 3.1963926e-05 3.2921907e-05][2.896399e-05 3.2399283e-05 3.3063854e-05 3.4619206e-05 3.761108e-05 4.2146752e-05 4.6886908e-05 4.9356171e-05 4.8219317e-05 4.5448924e-05 4.2120042e-05 3.7590409e-05 3.3498869e-05 3.1252803e-05 3.1787746e-05][2.8528248e-05 3.2715016e-05 3.3942637e-05 3.57848e-05 3.9944371e-05 4.671697e-05 5.2963976e-05 5.40965e-05 5.009706e-05 4.6122896e-05 4.2794225e-05 3.9121089e-05 3.4926419e-05 3.1764412e-05 3.1351407e-05][2.7914339e-05 3.2388114e-05 3.451727e-05 3.6496935e-05 3.87586e-05 4.1575771e-05 4.4177443e-05 4.4231139e-05 4.2590797e-05 4.0993957e-05 3.9544426e-05 3.6928013e-05 3.374599e-05 3.0997337e-05 3.017097e-05][2.7037178e-05 3.1448366e-05 3.203256e-05 3.0725041e-05 2.8556136e-05 2.7407452e-05 2.8533945e-05 3.027276e-05 3.2687374e-05 3.4934437e-05 3.6348723e-05 3.5469853e-05 3.2851491e-05 2.9551342e-05 2.7783175e-05][2.0483687e-05 2.4253979e-05 2.3570381e-05 2.1389136e-05 1.9033596e-05 1.830047e-05 1.9110015e-05 2.1714393e-05 2.4659534e-05 2.7572551e-05 3.0336061e-05 3.1282867e-05 2.9568724e-05 2.654424e-05 2.4558929e-05][1.1337037e-05 1.4812627e-05 1.5779166e-05 1.5858488e-05 1.4682857e-05 1.3444835e-05 1.3004206e-05 1.3915011e-05 1.5687936e-05 1.9058669e-05 2.1984513e-05 2.3534278e-05 2.3189004e-05 2.1447388e-05 2.0045114e-05][8.5233987e-06 1.2580993e-05 1.4815541e-05 1.4943929e-05 1.2966873e-05 1.0330215e-05 8.725794e-06 8.4158492e-06 8.953e-06 1.0813543e-05 1.296102e-05 1.4600206e-05 1.5616202e-05 1.5822679e-05 1.5668753e-05][8.7804729e-06 1.2498305e-05 1.4132609e-05 1.3595498e-05 1.1646374e-05 9.2306545e-06 7.1204013e-06 5.9911545e-06 5.6191966e-06 7.0808819e-06 8.7487642e-06 9.78844e-06 1.0420506e-05 1.149474e-05 1.2897359e-05][9.5920805e-06 1.3133525e-05 1.4146033e-05 1.3324327e-05 1.1596705e-05 9.4357129e-06 7.5343632e-06 6.2881591e-06 6.2564941e-06 7.9917263e-06 9.9035824e-06 1.0888376e-05 1.1135231e-05 1.2068645e-05 1.3855533e-05]]...]
INFO - root - 2017-12-09 07:39:31.827532: step 2710, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.873 sec/batch; 79h:58m:37s remains)
INFO - root - 2017-12-09 07:39:40.445060: step 2720, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 78h:57m:02s remains)
INFO - root - 2017-12-09 07:39:49.168252: step 2730, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.874 sec/batch; 80h:02m:49s remains)
INFO - root - 2017-12-09 07:39:57.792982: step 2740, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.861 sec/batch; 78h:52m:23s remains)
INFO - root - 2017-12-09 07:40:06.447740: step 2750, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.898 sec/batch; 82h:17m:14s remains)
INFO - root - 2017-12-09 07:40:15.199546: step 2760, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 79h:35m:40s remains)
INFO - root - 2017-12-09 07:40:24.002829: step 2770, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.903 sec/batch; 82h:42m:58s remains)
INFO - root - 2017-12-09 07:40:32.699615: step 2780, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.900 sec/batch; 82h:25m:32s remains)
INFO - root - 2017-12-09 07:40:41.230662: step 2790, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.853 sec/batch; 78h:10m:07s remains)
INFO - root - 2017-12-09 07:40:49.908355: step 2800, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.879 sec/batch; 80h:27m:58s remains)
2017-12-09 07:40:50.741823: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.00026592665 0.00062012114 0.0011209352 0.0017177021 0.0023198384 0.0027827229 0.0029602633 0.0028111425 0.0023899991 0.0018183191 0.001221398 0.00071434467 0.00036375574 0.00016771868 7.7154604e-05][0.0003767653 0.0007801415 0.0013487705 0.0020282657 0.0027328741 0.0033100143 0.0035938565 0.0035179905 0.0031119243 0.002486171 0.0017755927 0.0011245564 0.00062972796 0.00031190572 0.0001386747][0.00062352174 0.0010453574 0.0016283749 0.0023295931 0.003067086 0.0037013264 0.0040813573 0.0041208146 0.0037993724 0.0031930243 0.0024303326 0.0016633562 0.0010138396 0.00054379768 0.00025724087][0.0011595302 0.0015837667 0.002138847 0.0028011617 0.0034986376 0.0041161752 0.0045316927 0.0046661212 0.0044597085 0.00393357 0.0031827705 0.002347877 0.0015659346 0.000932904 0.00049631647][0.0021103367 0.0025070542 0.0029718676 0.0035101862 0.0040719989 0.0045793881 0.0049509089 0.0051311916 0.0050425092 0.0046512224 0.0039926739 0.003168795 0.0023110884 0.0015346434 0.00092889467][0.0034363591 0.0037489154 0.0040505733 0.0043959152 0.0047635012 0.0051118443 0.0053892951 0.0055594114 0.0055535426 0.0053078225 0.00479029 0.0040483736 0.0031908 0.0023343661 0.0015895433][0.0047598453 0.0049356539 0.0050281696 0.0051592151 0.0053349296 0.0055291522 0.0057057077 0.0058395839 0.0058761085 0.005745268 0.0053823469 0.0047819461 0.0040173363 0.0031884268 0.0024040323][0.0055564293 0.0055908198 0.0054786205 0.005428513 0.0054626851 0.0055548907 0.0056665908 0.0057784282 0.0058514006 0.0058147125 0.0055960142 0.005166919 0.0045660757 0.0038652241 0.0031586953][0.0055136182 0.0054378314 0.0051911231 0.0050358535 0.0050062705 0.0050700079 0.0051878379 0.0053379778 0.0054869605 0.0055612209 0.0054898933 0.0052355584 0.0048248223 0.0043006456 0.0037336855][0.0047598127 0.0046103671 0.0043107467 0.0041503748 0.0041437582 0.0042548268 0.0044468082 0.0046953741 0.0049608564 0.0051661888 0.0052414937 0.0051508746 0.0049114656 0.0045455056 0.0041124234][0.0035195737 0.0033146972 0.0030386965 0.0029462785 0.0030267816 0.0032365872 0.0035386751 0.0039033687 0.0042896122 0.0046174969 0.0048233261 0.0048733694 0.0047777756 0.0045487224 0.0042335568][0.0022599692 0.0020028022 0.0017651634 0.0017408656 0.0018843868 0.0021514765 0.0025075267 0.0029229694 0.0033656976 0.0037625004 0.0040573762 0.0042230217 0.0042612078 0.0041742488 0.0039930795][0.0012443353 0.00099927 0.00082720886 0.00083259604 0.0009617082 0.0011836489 0.0014796875 0.0018367138 0.0022290119 0.0026066164 0.0029288451 0.0031695471 0.0033268742 0.0033950855 0.0033791913][0.00062061049 0.00043495098 0.0003286241 0.0003309866 0.00040510821 0.00053188036 0.00070466497 0.00092542509 0.0011835379 0.0014541431 0.0017141583 0.0019538626 0.0021676063 0.0023455445 0.0024803281][0.00032289291 0.00022395543 0.00017060984 0.0001669575 0.00018995201 0.00023220637 0.00029498112 0.00038518186 0.00049973931 0.00063457433 0.0007834704 0.000947914 0.0011300449 0.0013282695 0.0015319376]]...]
INFO - root - 2017-12-09 07:40:59.360281: step 2810, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.839 sec/batch; 76h:47m:50s remains)
INFO - root - 2017-12-09 07:41:08.320362: step 2820, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.882 sec/batch; 80h:49m:01s remains)
INFO - root - 2017-12-09 07:41:16.888920: step 2830, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.873 sec/batch; 79h:55m:54s remains)
INFO - root - 2017-12-09 07:41:25.417973: step 2840, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.896 sec/batch; 82h:00m:26s remains)
INFO - root - 2017-12-09 07:41:34.144225: step 2850, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.859 sec/batch; 78h:37m:28s remains)
INFO - root - 2017-12-09 07:41:42.789571: step 2860, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 79h:06m:41s remains)
INFO - root - 2017-12-09 07:41:51.501880: step 2870, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 79h:23m:16s remains)
INFO - root - 2017-12-09 07:42:00.281909: step 2880, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.903 sec/batch; 82h:42m:59s remains)
INFO - root - 2017-12-09 07:42:08.754051: step 2890, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.861 sec/batch; 78h:49m:26s remains)
INFO - root - 2017-12-09 07:42:17.339428: step 2900, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.851 sec/batch; 77h:52m:40s remains)
2017-12-09 07:42:18.192986: I tensorflow/core/kernels/logging_ops.cc:79] [[[3.5338584e-05 3.6059122e-05 3.6482154e-05 3.757346e-05 3.8382015e-05 3.7024445e-05 3.46534e-05 3.1593438e-05 2.9666517e-05 3.0055504e-05 3.2440541e-05 3.4974088e-05 3.6656114e-05 3.8881073e-05 4.0523941e-05][3.7203004e-05 3.6900063e-05 3.6274425e-05 3.5969417e-05 3.5730773e-05 3.3885692e-05 3.2019751e-05 2.9770868e-05 2.777906e-05 2.8222974e-05 3.0840281e-05 3.3800716e-05 3.5356286e-05 3.790098e-05 4.0552251e-05][3.720817e-05 3.6305726e-05 3.5111283e-05 3.4479533e-05 3.463505e-05 3.3732213e-05 3.3046686e-05 3.1311087e-05 2.9150629e-05 2.8850125e-05 3.0360228e-05 3.2168617e-05 3.2877484e-05 3.5109304e-05 3.7412887e-05][3.2270422e-05 3.1656396e-05 3.0393923e-05 2.9804622e-05 3.0309486e-05 3.054236e-05 3.147978e-05 3.1292722e-05 3.0789546e-05 3.1090654e-05 3.184165e-05 3.2261196e-05 3.1777963e-05 3.2718082e-05 3.3864584e-05][2.5408466e-05 2.4843721e-05 2.3796114e-05 2.3373315e-05 2.3863642e-05 2.4806919e-05 2.7069502e-05 2.8356313e-05 2.9673523e-05 3.1373667e-05 3.2520125e-05 3.1804004e-05 3.0617855e-05 3.1026204e-05 3.1062205e-05][1.958332e-05 1.9053303e-05 1.8171355e-05 1.7473452e-05 1.7619721e-05 1.9042578e-05 2.1718792e-05 2.4106972e-05 2.6434129e-05 2.9113355e-05 3.0520219e-05 2.9103408e-05 2.7508155e-05 2.7222275e-05 2.6395806e-05][1.4555568e-05 1.4014004e-05 1.3144861e-05 1.234407e-05 1.2371042e-05 1.3608565e-05 1.6200385e-05 1.8987441e-05 2.1577129e-05 2.4186673e-05 2.5417641e-05 2.3826455e-05 2.1596446e-05 2.0513347e-05 1.9750449e-05][8.8572015e-06 8.6837608e-06 8.15211e-06 7.8332014e-06 8.1730541e-06 9.6117219e-06 1.2031203e-05 1.4851736e-05 1.7334016e-05 1.9333354e-05 2.0338572e-05 1.9061663e-05 1.6879516e-05 1.5509882e-05 1.4708923e-05][3.1174823e-06 3.3392571e-06 3.4962104e-06 3.8056496e-06 4.75864e-06 6.6473076e-06 9.3232666e-06 1.2075296e-05 1.4132442e-05 1.5628597e-05 1.6570018e-05 1.559293e-05 1.3755773e-05 1.2500604e-05 1.1580436e-05][-3.7446807e-07 2.8303475e-08 7.1728573e-07 1.7783968e-06 3.4991317e-06 6.2942163e-06 9.6079566e-06 1.2625744e-05 1.4511563e-05 1.578412e-05 1.6726844e-05 1.5460362e-05 1.3575052e-05 1.2070574e-05 1.0660289e-05][-1.3728859e-06 -6.987284e-07 6.9719681e-07 2.6033413e-06 5.1626521e-06 8.6428336e-06 1.2383534e-05 1.5747428e-05 1.8054474e-05 1.9431907e-05 2.0448555e-05 1.9240957e-05 1.7282327e-05 1.4365243e-05 1.1480501e-05][-8.9522291e-07 2.6560519e-07 2.37333e-06 5.0436356e-06 8.5628817e-06 1.2846373e-05 1.7454433e-05 2.1987878e-05 2.5630048e-05 2.7799178e-05 2.8722308e-05 2.7399292e-05 2.4512607e-05 1.940527e-05 1.3945042e-05][3.3646575e-07 2.2522727e-06 5.6241915e-06 1.0072559e-05 1.5297504e-05 2.0741405e-05 2.6239919e-05 3.1166332e-05 3.5442121e-05 3.8440456e-05 3.9746454e-05 3.8191996e-05 3.4624252e-05 2.7948801e-05 1.9883162e-05][1.4919933e-06 4.4140179e-06 9.211486e-06 1.5763588e-05 2.3196371e-05 3.0571617e-05 3.7169506e-05 4.1776933e-05 4.5466222e-05 4.7921247e-05 4.8433489e-05 4.57801e-05 4.1386389e-05 3.3988232e-05 2.513572e-05][2.4815163e-06 6.6940374e-06 1.2713805e-05 2.0573832e-05 2.9094e-05 3.7757949e-05 4.4922606e-05 4.8780166e-05 5.1173767e-05 5.1792937e-05 5.0508657e-05 4.6782021e-05 4.2663953e-05 3.6734527e-05 2.986117e-05]]...]
INFO - root - 2017-12-09 07:42:26.845722: step 2910, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.846 sec/batch; 77h:27m:43s remains)
INFO - root - 2017-12-09 07:42:35.337472: step 2920, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 79h:22m:54s remains)
INFO - root - 2017-12-09 07:42:43.940209: step 2930, loss = 0.82, batch loss = 0.69 (9.7 examples/sec; 0.829 sec/batch; 75h:51m:09s remains)
INFO - root - 2017-12-09 07:42:52.256071: step 2940, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.848 sec/batch; 77h:35m:05s remains)
INFO - root - 2017-12-09 07:43:00.728011: step 2950, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.879 sec/batch; 80h:26m:41s remains)
INFO - root - 2017-12-09 07:43:09.456552: step 2960, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 79h:33m:16s remains)
INFO - root - 2017-12-09 07:43:18.164549: step 2970, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.860 sec/batch; 78h:43m:37s remains)
INFO - root - 2017-12-09 07:43:26.920579: step 2980, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.840 sec/batch; 76h:51m:23s remains)
INFO - root - 2017-12-09 07:43:35.475158: step 2990, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 79h:32m:48s remains)
INFO - root - 2017-12-09 07:43:44.121115: step 3000, loss = 0.82, batch loss = 0.69 (9.7 examples/sec; 0.824 sec/batch; 75h:26m:23s remains)
2017-12-09 07:43:44.946103: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.030172035 0.03099972 0.031219009 0.031024802 0.030837083 0.030626101 0.030344052 0.030017257 0.029648574 0.029060466 0.028277835 0.027393064 0.026398219 0.025313025 0.024224643][0.03135889 0.032306366 0.032503113 0.032332469 0.032152858 0.031941019 0.03160971 0.031216793 0.030753376 0.030065974 0.029193437 0.028185969 0.027083447 0.025942657 0.024858903][0.031819306 0.032960303 0.033291489 0.033196464 0.033079743 0.032907281 0.032556579 0.032114387 0.031550921 0.03071652 0.029672926 0.02847526 0.027247148 0.026049986 0.024985166][0.032047562 0.033437323 0.033925012 0.03394641 0.033933051 0.03385479 0.033577003 0.033150963 0.032541618 0.031645577 0.03052384 0.029236436 0.027938392 0.026714342 0.025679359][0.032246325 0.033894125 0.034571614 0.034748625 0.034878708 0.034947187 0.034773562 0.034398284 0.033786848 0.032912169 0.031832274 0.030593164 0.029354326 0.028200837 0.027253978][0.032425161 0.03428743 0.035113238 0.035402209 0.035632249 0.03580397 0.035782054 0.035518922 0.034992468 0.034234826 0.033311073 0.032257158 0.031190135 0.030196419 0.029384026][0.032472104 0.034537409 0.035521891 0.035894703 0.03618747 0.036423482 0.036505014 0.036340043 0.0359193 0.035327081 0.03465404 0.033907134 0.033147722 0.032423902 0.031817328][0.032106031 0.034300629 0.035376966 0.035792921 0.036109604 0.036402572 0.036590353 0.036577407 0.036366653 0.036069423 0.035777267 0.035455588 0.035101935 0.034714255 0.034347348][0.030869856 0.033098437 0.034131862 0.034552779 0.034843192 0.035143059 0.035437077 0.035660971 0.035810683 0.035982925 0.036231507 0.036483191 0.036651295 0.036668889 0.036592938][0.028619127 0.030731536 0.031627335 0.031967256 0.03214591 0.032387909 0.03277494 0.033308417 0.033946715 0.034728523 0.035643309 0.036561761 0.037318032 0.037781373 0.038033593][0.025145885 0.026986061 0.027685443 0.027901366 0.027943801 0.028126704 0.028627209 0.029524574 0.030728502 0.032192186 0.033823319 0.035440706 0.036803052 0.037738103 0.038360331][0.020669181 0.022124536 0.022598563 0.022688475 0.022638137 0.022776939 0.023390248 0.024634469 0.02638416 0.028505348 0.030825606 0.033112429 0.035068586 0.03649132 0.037516125][0.015619759 0.016644564 0.016886316 0.016892659 0.01680986 0.016937556 0.017626608 0.019101581 0.02123615 0.023826979 0.026665205 0.029500948 0.031995796 0.033904031 0.035366047][0.0107109 0.011279467 0.011301315 0.011210735 0.011082618 0.011166703 0.011821259 0.013323594 0.015571269 0.018339487 0.021406958 0.024533644 0.027383104 0.029676203 0.031531997][0.0065553822 0.0067501864 0.0066129728 0.0064582457 0.0062964731 0.0063029584 0.0068109715 0.0081104282 0.01013777 0.012718665 0.015651323 0.018721797 0.021622412 0.02407394 0.026159793]]...]
INFO - root - 2017-12-09 07:43:53.494142: step 3010, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.860 sec/batch; 78h:41m:41s remains)
INFO - root - 2017-12-09 07:44:02.084480: step 3020, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.876 sec/batch; 80h:12m:49s remains)
INFO - root - 2017-12-09 07:44:10.473365: step 3030, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.863 sec/batch; 78h:58m:48s remains)
INFO - root - 2017-12-09 07:44:18.989306: step 3040, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.874 sec/batch; 79h:56m:39s remains)
INFO - root - 2017-12-09 07:44:27.594648: step 3050, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.833 sec/batch; 76h:11m:37s remains)
INFO - root - 2017-12-09 07:44:36.039742: step 3060, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.836 sec/batch; 76h:29m:01s remains)
INFO - root - 2017-12-09 07:44:44.635102: step 3070, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.847 sec/batch; 77h:30m:27s remains)
INFO - root - 2017-12-09 07:44:53.194873: step 3080, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.877 sec/batch; 80h:17m:33s remains)
INFO - root - 2017-12-09 07:45:01.647622: step 3090, loss = 0.82, batch loss = 0.69 (10.2 examples/sec; 0.786 sec/batch; 71h:53m:02s remains)
INFO - root - 2017-12-09 07:45:10.171493: step 3100, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.873 sec/batch; 79h:51m:59s remains)
2017-12-09 07:45:11.024607: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.00033137234 0.00035754978 0.00037045259 0.00038208583 0.00038942465 0.00039241806 0.00039169079 0.00038871978 0.00038641898 0.00038268202 0.00037167888 0.00034904011 0.00031084332 0.00025717024 0.00019283904][0.0010323043 0.0011069875 0.0011443895 0.0011787037 0.0012012741 0.0012111482 0.0012089304 0.0011980527 0.0011846442 0.001165901 0.0011298843 0.0010647289 0.0009588509 0.00081092113 0.00063127221][0.0024029622 0.002573014 0.0026617525 0.0027444293 0.0028030889 0.0028339031 0.0028342844 0.0028096449 0.0027702756 0.0027141415 0.0026263711 0.0024820268 0.0022551131 0.0019392605 0.0015501555][0.0043403921 0.0046565705 0.004835187 0.0050058938 0.0051378291 0.0052195219 0.0052412949 0.0052067479 0.00512694 0.0050067711 0.0048396843 0.0045881928 0.004205992 0.0036747558 0.003006313][0.0062300377 0.0067291725 0.0070478027 0.0073646232 0.0076304846 0.0078186039 0.0079084123 0.0078946063 0.0077823354 0.007585865 0.0073176543 0.0069439779 0.0064041056 0.0056619649 0.0047144582][0.0073628891 0.0080599729 0.0085669514 0.0090863379 0.0095470138 0.0099031413 0.010117827 0.010169451 0.010048979 0.009781044 0.009399428 0.0088913357 0.0081972955 0.0072723674 0.0061021368][0.0073404363 0.0081929117 0.00888631 0.0096166115 0.010289147 0.010831645 0.011192698 0.0113322 0.011224668 0.010897967 0.010404432 0.0097565772 0.00891489 0.0078510316 0.0065567503][0.0061405515 0.0070316996 0.0078415414 0.008711176 0.0095292153 0.010205671 0.010672644 0.010876074 0.010777211 0.010406405 0.0098268287 0.009071297 0.0081319241 0.0070165559 0.0057485728][0.0041545704 0.0049272231 0.0057013663 0.0065466589 0.0073596891 0.0080443025 0.0085226456 0.0087322351 0.0086357463 0.0082570575 0.0076597836 0.0068924432 0.005987281 0.0049880613 0.003946851][0.0021288616 0.0026464607 0.0032189842 0.0038614327 0.0044990662 0.0050497218 0.005437389 0.0056046168 0.0055264872 0.00521383 0.0047148732 0.0040871468 0.0033912098 0.0026830437 0.0020129639][0.00075743953 0.0010064282 0.0013046036 0.0016523734 0.0020129234 0.002339653 0.0025728433 0.0026727165 0.0026259157 0.002437423 0.0021345846 0.0017595595 0.0013683727 0.0010040593 0.00069509289][0.00014823393 0.00022412365 0.00032130955 0.00044546559 0.00058470538 0.00071995961 0.00082008884 0.00086913828 0.00085624354 0.00078248704 0.00065895775 0.0005056262 0.00035695042 0.00023295425 0.00014162538][1.2214878e-05 2.2170076e-05 3.4386438e-05 5.1890049e-05 7.5546857e-05 0.0001021042 0.00012359471 0.00013676248 0.00013717372 0.00012639363 0.00010552823 7.7689845e-05 5.1855925e-05 3.199705e-05 1.9387589e-05][-5.084381e-06 -2.8643262e-06 -1.6004051e-06 -4.0911254e-07 1.6733393e-06 3.8886283e-06 5.6850549e-06 8.0324608e-06 9.21305e-06 1.0234762e-05 1.0760734e-05 9.6081276e-06 7.9088859e-06 5.679547e-06 3.403984e-06][-8.3152263e-06 -6.7899309e-06 -6.5152744e-06 -6.5765962e-06 -6.2532708e-06 -6.125505e-06 -6.0486927e-06 -5.2924079e-06 -4.7133144e-06 -3.5062549e-06 -2.1467349e-06 -1.3887475e-06 -7.9363963e-07 -1.0574586e-06 -1.5277328e-06]]...]
INFO - root - 2017-12-09 07:45:19.649634: step 3110, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.837 sec/batch; 76h:33m:32s remains)
INFO - root - 2017-12-09 07:45:28.444892: step 3120, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.894 sec/batch; 81h:49m:20s remains)
INFO - root - 2017-12-09 07:45:36.970130: step 3130, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.872 sec/batch; 79h:44m:46s remains)
INFO - root - 2017-12-09 07:45:45.392657: step 3140, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.859 sec/batch; 78h:34m:16s remains)
INFO - root - 2017-12-09 07:45:53.963241: step 3150, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.852 sec/batch; 77h:56m:12s remains)
INFO - root - 2017-12-09 07:46:02.499959: step 3160, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.851 sec/batch; 77h:48m:25s remains)
INFO - root - 2017-12-09 07:46:11.151252: step 3170, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.853 sec/batch; 78h:01m:35s remains)
INFO - root - 2017-12-09 07:46:19.735082: step 3180, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.865 sec/batch; 79h:06m:11s remains)
INFO - root - 2017-12-09 07:46:28.391618: step 3190, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.859 sec/batch; 78h:36m:28s remains)
INFO - root - 2017-12-09 07:46:36.888278: step 3200, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.856 sec/batch; 78h:19m:55s remains)
2017-12-09 07:46:37.701183: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.2184922e-05 -3.2261181e-05 -3.3019405e-05 -3.3591819e-05 -3.3737477e-05 -3.3347824e-05 -3.27834e-05 -3.2029311e-05 -3.1410429e-05 -3.1354321e-05 -3.1454849e-05 -3.1754251e-05 -3.179859e-05 -3.1479416e-05 -3.0309948e-05][-3.1730931e-05 -3.1090585e-05 -3.0949472e-05 -3.0221105e-05 -2.8485436e-05 -2.5401387e-05 -2.117435e-05 -1.5943329e-05 -1.1350759e-05 -8.9784517e-06 -9.3601993e-06 -1.2429344e-05 -1.73337e-05 -2.2405558e-05 -2.5818023e-05][-2.5104036e-05 -2.0591335e-05 -1.6083006e-05 -9.9943245e-06 -1.2469609e-06 1.1614349e-05 2.9806179e-05 5.1456882e-05 7.0620692e-05 7.9898688e-05 7.5769552e-05 5.8589911e-05 3.3177064e-05 7.7655277e-06 -1.1441873e-05][1.6794438e-06 2.1223532e-05 4.3064232e-05 6.9341753e-05 0.0001021403 0.00014602582 0.00020228278 0.00026427727 0.00031658902 0.00033919886 0.0003192398 0.00025873777 0.00017472812 9.2451839e-05 3.0073701e-05][9.2742135e-05 0.00015816333 0.00022882283 0.00030528154 0.00039405818 0.00050302391 0.00062985922 0.000758837 0.00086025434 0.00089321157 0.00082943344 0.00067385234 0.00046865857 0.00026975479 0.00011795214][0.00032077037 0.00048657734 0.00065225328 0.00081408123 0.00098540133 0.001171748 0.0013619019 0.0015337674 0.0016552367 0.0016694168 0.0015316493 0.0012461903 0.00087905733 0.00052276574 0.00024629413][0.00071462442 0.0010184846 0.0012964406 0.001541777 0.0017772689 0.0020036956 0.0021980233 0.0023437105 0.0024268914 0.0023906971 0.0021720461 0.0017651413 0.001250163 0.00075294351 0.00036590698][0.0011808213 0.0016051588 0.0019596918 0.0022408834 0.0024855987 0.0026840209 0.0028076689 0.002857341 0.0028555016 0.0027554086 0.0024770943 0.0019997116 0.0014076891 0.00084418373 0.00041084836][0.0015723242 0.0020600988 0.0024265444 0.002675435 0.0028531125 0.0029526106 0.0029562148 0.0028900052 0.0028026653 0.0026533571 0.0023524142 0.0018665693 0.001281202 0.00074378622 0.00034938438][0.0017889438 0.0022768974 0.0025966854 0.0027592643 0.0028180371 0.0027821071 0.0026604179 0.0024981562 0.0023501383 0.0021722126 0.001878113 0.0014404396 0.00094207004 0.0005130248 0.00022289915][0.0017811545 0.0022190344 0.0024559575 0.0025041904 0.0024228515 0.0022476895 0.0020203635 0.0017982185 0.001623033 0.0014473089 0.0012028894 0.00087512936 0.00053194049 0.00026092556 9.6464631e-05][0.0015360976 0.0018806192 0.0020170806 0.0019544866 0.0017583607 0.0014913874 0.0012186031 0.00099413423 0.00083911972 0.000709862 0.00055999425 0.00037960277 0.00020710532 8.3435953e-05 1.738155e-05][0.0010978051 0.0013236753 0.0013734634 0.0012527677 0.0010279319 0.00076966139 0.00054103683 0.00037719321 0.00028020481 0.00021551656 0.00015450126 9.0474714e-05 3.6171841e-05 2.7685892e-06 -1.2761713e-05][0.00060751534 0.00072422129 0.00072743191 0.00062137464 0.00045613968 0.00028602232 0.00015230292 6.9329268e-05 2.8950606e-05 1.0111915e-05 -2.2551467e-06 -1.1530654e-05 -1.7085978e-05 -1.9420859e-05 -2.0527357e-05][0.00023033166 0.00027746381 0.00027211348 0.00021740062 0.00013931256 6.5317363e-05 1.445313e-05 -1.1028264e-05 -2.0196159e-05 -2.2290009e-05 -2.3079592e-05 -2.3086897e-05 -2.2744145e-05 -2.2361335e-05 -2.2338256e-05]]...]
INFO - root - 2017-12-09 07:46:46.242281: step 3210, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.854 sec/batch; 78h:05m:13s remains)
INFO - root - 2017-12-09 07:46:54.874114: step 3220, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.848 sec/batch; 77h:31m:29s remains)
INFO - root - 2017-12-09 07:47:03.300313: step 3230, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.873 sec/batch; 79h:50m:18s remains)
INFO - root - 2017-12-09 07:47:11.679407: step 3240, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.856 sec/batch; 78h:19m:29s remains)
INFO - root - 2017-12-09 07:47:20.240685: step 3250, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 79h:25m:00s remains)
INFO - root - 2017-12-09 07:47:28.930654: step 3260, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.857 sec/batch; 78h:23m:47s remains)
INFO - root - 2017-12-09 07:47:37.523846: step 3270, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.863 sec/batch; 78h:55m:27s remains)
INFO - root - 2017-12-09 07:47:46.166303: step 3280, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.851 sec/batch; 77h:46m:41s remains)
INFO - root - 2017-12-09 07:47:54.839759: step 3290, loss = 0.82, batch loss = 0.69 (9.7 examples/sec; 0.823 sec/batch; 75h:13m:18s remains)
INFO - root - 2017-12-09 07:48:03.463900: step 3300, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.870 sec/batch; 79h:32m:11s remains)
2017-12-09 07:48:04.317788: I tensorflow/core/kernels/logging_ops.cc:79] [[[-9.8662676e-06 -6.0450984e-06 -5.036316e-06 -4.2488464e-06 -4.6888163e-06 -5.4240445e-06 -6.7178771e-06 -8.6929431e-06 -1.0705e-05 -1.204346e-05 -1.281625e-05 -1.247647e-05 -1.1577671e-05 -1.0167078e-05 -9.2837909e-06][-1.0217187e-05 -5.7566358e-06 -4.4275657e-06 -3.2616445e-06 -3.4348705e-06 -4.5131164e-06 -5.5516502e-06 -7.92776e-06 -1.0377669e-05 -1.1964024e-05 -1.277019e-05 -1.2473334e-05 -1.1378605e-05 -9.0827525e-06 -7.3909614e-06][-1.0979664e-05 -7.5797579e-06 -5.9000013e-06 -4.336056e-06 -4.0207524e-06 -4.4975386e-06 -5.2119067e-06 -7.2159528e-06 -9.2158516e-06 -1.0413529e-05 -1.1471835e-05 -1.2041473e-05 -1.1646654e-05 -9.5650794e-06 -8.2460683e-06][-1.2205837e-05 -9.7721568e-06 -8.627474e-06 -7.4548152e-06 -6.8121008e-06 -5.6567515e-06 -5.3582553e-06 -6.9966554e-06 -8.7153458e-06 -1.0048956e-05 -1.1538847e-05 -1.2999659e-05 -1.3138775e-05 -1.1871645e-05 -1.1545337e-05][-1.3416622e-05 -1.1775475e-05 -1.0892498e-05 -9.404499e-06 -8.2324768e-06 -6.3763146e-06 -4.8738584e-06 -5.8919322e-06 -7.2961411e-06 -8.918134e-06 -1.0581454e-05 -1.2069318e-05 -1.2834702e-05 -1.2749031e-05 -1.3248959e-05][-1.4003926e-05 -1.2365661e-05 -1.1063217e-05 -9.65879e-06 -8.2575352e-06 -5.6814024e-06 -2.5083136e-06 -2.394474e-06 -3.6798738e-06 -5.9251397e-06 -7.9821039e-06 -9.6544063e-06 -1.0930067e-05 -1.1884709e-05 -1.3429664e-05][-1.421856e-05 -1.2030952e-05 -1.0501404e-05 -8.3671548e-06 -6.9008092e-06 -3.6155106e-06 7.7838922e-07 2.7939459e-06 2.1048763e-06 -3.2972457e-07 -2.7042042e-06 -5.5308046e-06 -7.1206887e-06 -8.9755777e-06 -1.1313859e-05][-1.2318331e-05 -9.59761e-06 -7.7891746e-06 -5.573027e-06 -4.401576e-06 -8.950301e-07 3.9748047e-06 7.4682612e-06 8.1528269e-06 6.5053609e-06 3.3782999e-06 -9.3169365e-07 -3.4580371e-06 -5.6515346e-06 -8.1553808e-06][-8.8985325e-06 -5.3938929e-06 -3.80876e-06 -2.8753348e-06 -3.1064628e-06 3.1461968e-07 5.2682735e-06 8.7817243e-06 9.8117816e-06 9.0950125e-06 6.5545973e-06 2.9642397e-06 7.9540769e-08 -2.2294262e-06 -4.8710572e-06][-6.2183826e-06 -2.0767984e-06 -5.4590055e-07 -1.3691388e-06 -2.9067596e-06 5.6315912e-09 4.21649e-06 6.878865e-06 7.7199074e-06 9.143514e-06 8.9808818e-06 6.8935988e-06 4.5509296e-06 1.9114814e-06 -8.472125e-07][-2.6282796e-06 1.8818973e-06 3.0243827e-06 1.9076251e-06 -3.0510273e-07 1.6415943e-06 4.5691922e-06 5.1478419e-06 4.8661677e-06 6.7499714e-06 7.3110568e-06 6.2912077e-06 4.9554656e-06 2.6649941e-06 7.0911483e-08][7.7311415e-07 5.2250252e-06 6.1560422e-06 4.8843358e-06 2.7400965e-06 3.7713035e-06 5.7206053e-06 4.6681089e-06 2.8744907e-06 3.1184973e-06 2.6294001e-06 1.5507758e-06 1.3995741e-06 -8.787174e-08 -1.7283091e-06][4.3414548e-06 9.6588847e-06 9.9784229e-06 8.5006177e-06 6.0815291e-06 5.5447163e-06 5.6279823e-06 3.2401076e-06 1.3279423e-06 1.3737008e-06 1.3707904e-07 -1.6582198e-06 -1.736902e-06 -2.7323185e-06 -3.6718338e-06][4.1056264e-06 9.2603077e-06 9.8954406e-06 9.8455421e-06 8.3110644e-06 6.6010907e-06 4.7264475e-06 1.4790421e-06 -8.8360684e-07 -1.6536142e-06 -2.7517744e-06 -4.1117019e-06 -4.1783933e-06 -5.3743788e-06 -5.7951693e-06][8.03062e-07 5.6978461e-06 7.1922259e-06 7.89779e-06 6.9995804e-06 4.1554667e-06 8.684583e-07 -2.7173955e-06 -5.3902622e-06 -6.3798652e-06 -6.6504363e-06 -7.0854294e-06 -6.8888403e-06 -7.8436278e-06 -7.6339638e-06]]...]
INFO - root - 2017-12-09 07:48:13.111802: step 3310, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.865 sec/batch; 79h:05m:17s remains)
INFO - root - 2017-12-09 07:48:21.803661: step 3320, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.859 sec/batch; 78h:31m:38s remains)
INFO - root - 2017-12-09 07:48:30.521006: step 3330, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.875 sec/batch; 79h:58m:58s remains)
INFO - root - 2017-12-09 07:48:39.103190: step 3340, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.882 sec/batch; 80h:40m:38s remains)
INFO - root - 2017-12-09 07:48:47.779657: step 3350, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.849 sec/batch; 77h:38m:20s remains)
INFO - root - 2017-12-09 07:48:56.516153: step 3360, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.877 sec/batch; 80h:10m:05s remains)
INFO - root - 2017-12-09 07:49:05.150367: step 3370, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.870 sec/batch; 79h:30m:55s remains)
INFO - root - 2017-12-09 07:49:13.857882: step 3380, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.879 sec/batch; 80h:20m:40s remains)
INFO - root - 2017-12-09 07:49:22.578310: step 3390, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.890 sec/batch; 81h:22m:21s remains)
INFO - root - 2017-12-09 07:49:31.162717: step 3400, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.878 sec/batch; 80h:18m:03s remains)
2017-12-09 07:49:32.082132: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.010371696 0.01044457 0.010053818 0.0095204143 0.009039945 0.0087391855 0.0087897265 0.0093230139 0.010338467 0.011663647 0.013159066 0.014776001 0.016377995 0.017663363 0.018343948][0.01280915 0.012874847 0.012277923 0.011481079 0.010745612 0.010238708 0.010152708 0.010655993 0.011773002 0.013302746 0.015068838 0.016952034 0.018779587 0.020241892 0.02105286][0.014872923 0.014856378 0.014022998 0.012947564 0.011916919 0.011144866 0.010857143 0.01126143 0.012415178 0.014110617 0.016116489 0.018200573 0.020179629 0.02175501 0.022695392][0.01628329 0.016151814 0.015117143 0.013797623 0.012475716 0.01140666 0.01086981 0.011123622 0.012236767 0.014012749 0.016162774 0.018398358 0.020501323 0.022193858 0.023309499][0.016824741 0.016616095 0.015442251 0.013938462 0.012424296 0.011146054 0.010409915 0.010519728 0.011564704 0.013323847 0.015483452 0.017739495 0.019881271 0.021670179 0.02298652][0.016599199 0.016336795 0.015090972 0.013554322 0.011996789 0.010642161 0.0098120086 0.0098414561 0.010826637 0.012506939 0.014556413 0.016690167 0.018756272 0.020571664 0.022047063][0.015770428 0.015472785 0.014240661 0.012796232 0.011340388 0.010060095 0.00925948 0.0092986515 0.010258074 0.0118378 0.013703926 0.015602206 0.017480442 0.019217568 0.020747144][0.014559904 0.014240481 0.013120454 0.01187038 0.010628209 0.0095279105 0.0088251131 0.0088821705 0.0097761862 0.011205609 0.012833043 0.014439988 0.016061278 0.017640205 0.019126998][0.013116561 0.01287998 0.011964797 0.010962442 0.0099881617 0.0091261081 0.0085598454 0.008603313 0.0093353232 0.010511347 0.011827738 0.01314227 0.014489824 0.015869983 0.017253933][0.0117809 0.011630787 0.010913147 0.010143043 0.0094240252 0.0088337269 0.0084561426 0.0085102441 0.0090690255 0.0099347522 0.010896112 0.0118468 0.012868191 0.013991755 0.015201108][0.010568964 0.010531168 0.0099886535 0.0093920873 0.00888066 0.0085316319 0.0083670523 0.0084935557 0.008915199 0.0095013073 0.010117677 0.010703412 0.01136652 0.012170224 0.013141868][0.009354041 0.0093975132 0.0089940652 0.0085094264 0.0081194565 0.0079494668 0.0079860957 0.0082364045 0.0086260689 0.0090502817 0.0094217323 0.0097104395 0.010029433 0.010467302 0.011135377][0.0081285043 0.0082139159 0.0079033794 0.0074841827 0.0071497248 0.0070581376 0.0072044167 0.0075380108 0.0079283854 0.0082651256 0.0084935352 0.008599434 0.0086625554 0.0087745478 0.0091185095][0.0069291824 0.0069906521 0.0067313067 0.0063499818 0.0060292552 0.0059392848 0.0060972571 0.0064395084 0.0068009892 0.007060518 0.0071967226 0.0072085271 0.0071416656 0.0070535592 0.0071487715][0.0057610068 0.0057571833 0.0055104727 0.0051510166 0.0048305872 0.0047047916 0.0048225541 0.0051340563 0.0054547023 0.00565417 0.005722071 0.0056889625 0.0055923397 0.0054429411 0.0054199169]]...]
INFO - root - 2017-12-09 07:49:40.776791: step 3410, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.883 sec/batch; 80h:44m:45s remains)
INFO - root - 2017-12-09 07:49:49.483391: step 3420, loss = 0.82, batch loss = 0.69 (9.7 examples/sec; 0.827 sec/batch; 75h:33m:49s remains)
INFO - root - 2017-12-09 07:49:57.796170: step 3430, loss = 0.82, batch loss = 0.69 (9.7 examples/sec; 0.821 sec/batch; 75h:02m:03s remains)
INFO - root - 2017-12-09 07:50:06.316105: step 3440, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.903 sec/batch; 82h:30m:10s remains)
INFO - root - 2017-12-09 07:50:15.016297: step 3450, loss = 0.82, batch loss = 0.69 (8.8 examples/sec; 0.904 sec/batch; 82h:38m:29s remains)
INFO - root - 2017-12-09 07:50:23.716130: step 3460, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.861 sec/batch; 78h:40m:53s remains)
INFO - root - 2017-12-09 07:50:32.520643: step 3470, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.845 sec/batch; 77h:13m:48s remains)
INFO - root - 2017-12-09 07:50:41.410633: step 3480, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.858 sec/batch; 78h:24m:41s remains)
INFO - root - 2017-12-09 07:50:50.131504: step 3490, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.870 sec/batch; 79h:29m:52s remains)
INFO - root - 2017-12-09 07:50:58.750667: step 3500, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.873 sec/batch; 79h:47m:16s remains)
2017-12-09 07:50:59.719050: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.024419542 0.025784325 0.025905969 0.025101678 0.023724623 0.022418207 0.02162745 0.021272551 0.021068243 0.020921219 0.020940652 0.021104731 0.021177728 0.020911291 0.020177279][0.025905214 0.027227614 0.027134752 0.025908727 0.024040358 0.022320854 0.021282315 0.020827405 0.020622747 0.020545717 0.020692095 0.02099853 0.021180971 0.020970065 0.020226683][0.025754172 0.026765661 0.026262844 0.024496997 0.022093996 0.019950591 0.018638398 0.018046321 0.017835312 0.017856611 0.01814845 0.018582808 0.018864531 0.018742142 0.018084461][0.024318639 0.024843838 0.023817388 0.021554612 0.018732019 0.016296759 0.014791788 0.014086128 0.013851278 0.013922587 0.014278782 0.014768286 0.015110109 0.015083759 0.014589137][0.022152573 0.022062315 0.020499013 0.017846199 0.01482307 0.012335811 0.010814259 0.010059692 0.0097785247 0.0098309573 0.010176039 0.010666818 0.011057098 0.011167905 0.010900572][0.019702181 0.019097846 0.017077954 0.014165796 0.011091613 0.00869549 0.0072606811 0.006543559 0.0062845526 0.0063849208 0.00681617 0.0074330624 0.0080062347 0.0083517181 0.008352139][0.017857572 0.01691632 0.014581354 0.01149048 0.0084287012 0.0061552757 0.0048452774 0.0042359303 0.0040998608 0.0043646353 0.0049993964 0.0058497363 0.0066882842 0.0073257894 0.0076206336][0.017292548 0.016257143 0.013800034 0.010682045 0.0077200877 0.0056031481 0.0044541252 0.0040130336 0.0040666126 0.0045329407 0.0053626047 0.006409361 0.0074651451 0.0083487742 0.0088999309][0.018103104 0.017252838 0.014924188 0.011987389 0.0092545692 0.0073653581 0.0064012124 0.0061097285 0.0062993462 0.0068817926 0.0077904533 0.0088902675 0.010008002 0.010995075 0.011685185][0.019785235 0.019365065 0.017384481 0.014814372 0.012423642 0.010776013 0.009941753 0.009716481 0.0099412249 0.01052817 0.011411977 0.012471086 0.013568264 0.014578583 0.015338033][0.021650778 0.021840319 0.020418884 0.018345861 0.016331568 0.014876875 0.01408043 0.013827257 0.014008932 0.014545146 0.015370772 0.016365752 0.0174086 0.018383604 0.019140719][0.023366882 0.024194309 0.023401393 0.02186734 0.020231215 0.018934751 0.018137481 0.017820887 0.017917102 0.018346608 0.01904352 0.019896701 0.020795336 0.021632891 0.022287615][0.024452373 0.025790179 0.025537048 0.024512725 0.023247102 0.022133341 0.02137517 0.021013731 0.021010015 0.021286577 0.021785971 0.022415599 0.023080198 0.023686374 0.024151979][0.024350958 0.025945671 0.026061594 0.025444238 0.0245148 0.023600088 0.022916565 0.022542838 0.022453826 0.022570867 0.022850249 0.02322772 0.023633532 0.023992587 0.024256272][0.022853691 0.024508091 0.024827305 0.024489146 0.023828035 0.02309937 0.022498289 0.022131734 0.021970747 0.021948647 0.022031819 0.022197006 0.022400836 0.022565039 0.022668589]]...]
INFO - root - 2017-12-09 07:51:08.447741: step 3510, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 79h:25m:15s remains)
INFO - root - 2017-12-09 07:51:17.172284: step 3520, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.883 sec/batch; 80h:43m:31s remains)
INFO - root - 2017-12-09 07:51:25.784009: step 3530, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.883 sec/batch; 80h:40m:47s remains)
INFO - root - 2017-12-09 07:51:34.284007: step 3540, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.834 sec/batch; 76h:12m:27s remains)
INFO - root - 2017-12-09 07:51:42.819138: step 3550, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.880 sec/batch; 80h:22m:17s remains)
INFO - root - 2017-12-09 07:51:51.505870: step 3560, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.856 sec/batch; 78h:14m:11s remains)
INFO - root - 2017-12-09 07:52:00.353416: step 3570, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.898 sec/batch; 82h:04m:55s remains)
INFO - root - 2017-12-09 07:52:09.108784: step 3580, loss = 0.82, batch loss = 0.69 (8.2 examples/sec; 0.974 sec/batch; 89h:00m:19s remains)
INFO - root - 2017-12-09 07:52:17.724202: step 3590, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.870 sec/batch; 79h:27m:12s remains)
INFO - root - 2017-12-09 07:52:26.075852: step 3600, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 78h:46m:33s remains)
2017-12-09 07:52:26.959626: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.8392238e-05 -2.6935813e-05 -2.6392652e-05 -2.5999805e-05 -2.5778045e-05 -2.5814079e-05 -2.6061571e-05 -2.6061334e-05 -2.610523e-05 -2.6579208e-05 -2.7246282e-05 -2.8023911e-05 -2.8284408e-05 -2.7986742e-05 -2.7466929e-05][-2.9339837e-05 -2.802585e-05 -2.7657585e-05 -2.7395392e-05 -2.7294398e-05 -2.7528462e-05 -2.7793772e-05 -2.771019e-05 -2.7642986e-05 -2.8027545e-05 -2.8678372e-05 -2.9352534e-05 -2.9632371e-05 -2.943465e-05 -2.8860166e-05][-2.9825776e-05 -2.8544448e-05 -2.8053419e-05 -2.7595252e-05 -2.730234e-05 -2.7402391e-05 -2.7650145e-05 -2.7691887e-05 -2.7659178e-05 -2.7965663e-05 -2.8591861e-05 -2.91527e-05 -2.9245366e-05 -2.9073282e-05 -2.8623479e-05][-2.89358e-05 -2.776234e-05 -2.7394231e-05 -2.7089474e-05 -2.6824349e-05 -2.6797905e-05 -2.709866e-05 -2.7482361e-05 -2.7723258e-05 -2.7987469e-05 -2.8579219e-05 -2.9112271e-05 -2.9093273e-05 -2.8783168e-05 -2.8221533e-05][-2.7878275e-05 -2.6907815e-05 -2.6802925e-05 -2.6690148e-05 -2.6499121e-05 -2.6361584e-05 -2.6683563e-05 -2.7325277e-05 -2.7798076e-05 -2.8243339e-05 -2.8868199e-05 -2.9331826e-05 -2.9311745e-05 -2.904523e-05 -2.8417253e-05][-2.7216276e-05 -2.6269969e-05 -2.6260845e-05 -2.6220743e-05 -2.6012196e-05 -2.5887159e-05 -2.6346017e-05 -2.7182716e-05 -2.7837024e-05 -2.8556115e-05 -2.920692e-05 -2.9444953e-05 -2.937168e-05 -2.9281189e-05 -2.8652608e-05][-2.6994363e-05 -2.5941903e-05 -2.5836958e-05 -2.5740512e-05 -2.5466954e-05 -2.5389952e-05 -2.606817e-05 -2.7019352e-05 -2.7763486e-05 -2.857998e-05 -2.9141764e-05 -2.9211624e-05 -2.9141225e-05 -2.9084498e-05 -2.8450861e-05][-2.6945821e-05 -2.592483e-05 -2.5692389e-05 -2.5411231e-05 -2.5035304e-05 -2.496074e-05 -2.5664522e-05 -2.6540412e-05 -2.7312421e-05 -2.8146878e-05 -2.8680803e-05 -2.8787574e-05 -2.875027e-05 -2.8638158e-05 -2.8086932e-05][-2.7235033e-05 -2.6213445e-05 -2.5901918e-05 -2.5450005e-05 -2.5070607e-05 -2.5044174e-05 -2.5603793e-05 -2.6327969e-05 -2.7108694e-05 -2.7894326e-05 -2.8317245e-05 -2.8546274e-05 -2.8543927e-05 -2.8280265e-05 -2.7721519e-05][-2.8333707e-05 -2.7541075e-05 -2.7233822e-05 -2.6684538e-05 -2.6314239e-05 -2.6360143e-05 -2.669634e-05 -2.7287773e-05 -2.8055005e-05 -2.866095e-05 -2.8931438e-05 -2.915115e-05 -2.9162667e-05 -2.8788083e-05 -2.8102961e-05][-3.0163741e-05 -2.9430223e-05 -2.9139199e-05 -2.8752715e-05 -2.8579903e-05 -2.8650888e-05 -2.8758182e-05 -2.9166393e-05 -2.9713876e-05 -2.9883755e-05 -2.9969371e-05 -3.0113242e-05 -3.0008327e-05 -2.9625055e-05 -2.8996572e-05][-3.3269647e-05 -3.246692e-05 -3.2134318e-05 -3.2013209e-05 -3.2032262e-05 -3.2022035e-05 -3.2046028e-05 -3.232072e-05 -3.2553435e-05 -3.2365224e-05 -3.2395968e-05 -3.2560383e-05 -3.2376192e-05 -3.2007807e-05 -3.1468317e-05][-3.6325782e-05 -3.5501369e-05 -3.5197012e-05 -3.5070956e-05 -3.5094035e-05 -3.5116471e-05 -3.5208381e-05 -3.5423906e-05 -3.5480025e-05 -3.5248162e-05 -3.5215318e-05 -3.5238289e-05 -3.4986882e-05 -3.4610046e-05 -3.4197408e-05][-3.9123483e-05 -3.8264719e-05 -3.7946975e-05 -3.771091e-05 -3.7750564e-05 -3.7958016e-05 -3.8237718e-05 -3.847116e-05 -3.8400784e-05 -3.8064198e-05 -3.7828147e-05 -3.755308e-05 -3.7156129e-05 -3.6754667e-05 -3.6466477e-05][-4.1257135e-05 -4.0591352e-05 -4.0295443e-05 -4.004551e-05 -4.006056e-05 -4.0338335e-05 -4.0578463e-05 -4.0648378e-05 -4.0458857e-05 -4.0056526e-05 -3.9731945e-05 -3.9391944e-05 -3.8969509e-05 -3.8665898e-05 -3.851427e-05]]...]
INFO - root - 2017-12-09 07:52:35.660484: step 3610, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.835 sec/batch; 76h:19m:01s remains)
INFO - root - 2017-12-09 07:52:44.424932: step 3620, loss = 0.82, batch loss = 0.69 (8.8 examples/sec; 0.907 sec/batch; 82h:49m:03s remains)
INFO - root - 2017-12-09 07:52:52.999491: step 3630, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.873 sec/batch; 79h:46m:46s remains)
INFO - root - 2017-12-09 07:53:01.450674: step 3640, loss = 0.82, batch loss = 0.69 (10.2 examples/sec; 0.787 sec/batch; 71h:52m:47s remains)
INFO - root - 2017-12-09 07:53:10.042253: step 3650, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.865 sec/batch; 79h:02m:53s remains)
INFO - root - 2017-12-09 07:53:18.457844: step 3660, loss = 0.82, batch loss = 0.69 (9.7 examples/sec; 0.828 sec/batch; 75h:35m:40s remains)
INFO - root - 2017-12-09 07:53:27.079754: step 3670, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.841 sec/batch; 76h:47m:56s remains)
INFO - root - 2017-12-09 07:53:35.994415: step 3680, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.883 sec/batch; 80h:36m:32s remains)
INFO - root - 2017-12-09 07:53:44.708842: step 3690, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.856 sec/batch; 78h:09m:06s remains)
INFO - root - 2017-12-09 07:53:53.266458: step 3700, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.875 sec/batch; 79h:53m:24s remains)
2017-12-09 07:53:54.108064: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.00074029015 0.00079260167 0.000798789 0.00079607143 0.0007901467 0.00078948768 0.00079976482 0.00080466591 0.00080634997 0.00081029983 0.00081986329 0.00083013461 0.00082639081 0.00081094092 0.00078353519][0.00081197271 0.0008829243 0.00088183052 0.00087776873 0.00087302772 0.00086903688 0.00086713239 0.000865627 0.00086162589 0.0008590513 0.00085683982 0.00085463026 0.00084820762 0.00082780357 0.00079141831][0.00080757617 0.00088025804 0.00087502261 0.0008635245 0.00084526464 0.000826407 0.00081431295 0.00080762827 0.00079711189 0.00078014785 0.00075913273 0.0007427049 0.00073037058 0.00070820341 0.00066619454][0.00078155432 0.00085153437 0.00084081624 0.00082171796 0.00079314125 0.0007618707 0.0007414401 0.00073064113 0.00071325945 0.0006768828 0.00062746508 0.00058933668 0.00057090051 0.00055297813 0.00051418028][0.00074249512 0.00079184026 0.00076972169 0.00074477814 0.00071032112 0.00067054987 0.00063946279 0.000618106 0.00058943569 0.0005374715 0.00047237787 0.00042230438 0.00040078541 0.00038703461 0.00035388209][0.00066837121 0.00070466514 0.0006724128 0.00064578303 0.00061400415 0.00057187659 0.00052695652 0.00048627035 0.00044083543 0.0003815729 0.00031611868 0.00026646673 0.00024403038 0.00023096483 0.0002032542][0.00055267435 0.00058695959 0.00055666867 0.00053918752 0.00051832659 0.00047817116 0.00041685888 0.00035033817 0.0002854281 0.00022428858 0.00017144711 0.00013460917 0.00011517651 0.00010210579 8.0566671e-05][0.0004081314 0.00043905992 0.00042607065 0.00042813498 0.00042367505 0.00038878 0.00032029682 0.00023831148 0.00016214518 0.00010608168 7.0721937e-05 5.0552459e-05 3.8962426e-05 2.8499468e-05 1.2869714e-05][0.00023426689 0.00026251312 0.00026746551 0.00028570052 0.00029609021 0.00027496906 0.00021981669 0.00014820357 8.1683749e-05 3.9901788e-05 2.1467713e-05 1.563938e-05 1.3078206e-05 6.408045e-06 -5.4588454e-06][8.7285596e-05 0.00010363204 0.00011261501 0.00013124873 0.00014540681 0.0001385496 0.0001089177 6.789386e-05 3.0001342e-05 8.2226106e-06 2.1471496e-06 1.8739229e-06 2.5215268e-06 -6.2518666e-07 -9.351279e-06][9.36845e-07 9.4991556e-06 1.4769241e-05 2.6033937e-05 3.546456e-05 3.5415142e-05 2.6290523e-05 1.1035292e-05 -3.2644748e-06 -1.1344368e-05 -1.2672688e-05 -1.2157005e-05 -1.1127908e-05 -1.2235614e-05 -1.6988364e-05][-2.8392529e-05 -2.3016015e-05 -2.1308886e-05 -1.6947495e-05 -1.3288296e-05 -1.1475895e-05 -1.0814365e-05 -1.3561566e-05 -1.7047991e-05 -1.9594823e-05 -1.9878695e-05 -1.9692594e-05 -1.8777795e-05 -1.9629369e-05 -2.2384436e-05][-3.3763736e-05 -2.9754443e-05 -2.8580394e-05 -2.6803184e-05 -2.6151891e-05 -2.5316007e-05 -2.4689671e-05 -2.4625173e-05 -2.4820576e-05 -2.5299549e-05 -2.4631481e-05 -2.4199595e-05 -2.3705798e-05 -2.3867055e-05 -2.5105761e-05][-3.7492471e-05 -3.5236364e-05 -3.4309993e-05 -3.3063709e-05 -3.1884381e-05 -3.0902167e-05 -3.0047591e-05 -2.9667521e-05 -2.9668408e-05 -2.9553798e-05 -2.8994229e-05 -2.8842303e-05 -2.8519113e-05 -2.8808645e-05 -2.9043262e-05][-3.9525883e-05 -3.796475e-05 -3.7126745e-05 -3.6187408e-05 -3.5276858e-05 -3.4080891e-05 -3.3198841e-05 -3.2191503e-05 -3.1422722e-05 -3.0987787e-05 -3.0721585e-05 -3.065123e-05 -3.0409705e-05 -3.0835177e-05 -3.1222939e-05]]...]
INFO - root - 2017-12-09 07:54:02.752739: step 3710, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 79h:17m:34s remains)
INFO - root - 2017-12-09 07:54:11.268143: step 3720, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.852 sec/batch; 77h:47m:26s remains)
INFO - root - 2017-12-09 07:54:19.811685: step 3730, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.858 sec/batch; 78h:19m:19s remains)
INFO - root - 2017-12-09 07:54:28.243300: step 3740, loss = 0.82, batch loss = 0.69 (10.4 examples/sec; 0.771 sec/batch; 70h:26m:17s remains)
INFO - root - 2017-12-09 07:54:36.828355: step 3750, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.842 sec/batch; 76h:54m:05s remains)
INFO - root - 2017-12-09 07:54:45.330847: step 3760, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.859 sec/batch; 78h:26m:04s remains)
INFO - root - 2017-12-09 07:54:54.123350: step 3770, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.853 sec/batch; 77h:53m:48s remains)
INFO - root - 2017-12-09 07:55:02.892097: step 3780, loss = 0.82, batch loss = 0.69 (9.7 examples/sec; 0.828 sec/batch; 75h:38m:52s remains)
INFO - root - 2017-12-09 07:55:11.525998: step 3790, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.859 sec/batch; 78h:23m:49s remains)
INFO - root - 2017-12-09 07:55:19.995654: step 3800, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.865 sec/batch; 78h:59m:57s remains)
2017-12-09 07:55:20.871171: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.2855776e-05 -4.0942563e-05 -3.8812657e-05 -3.4275097e-05 -2.4858276e-05 -1.0350392e-05 6.0991442e-06 2.0136387e-05 2.6507019e-05 2.1888896e-05 8.9607056e-06 -5.6738281e-06 -1.672452e-05 -2.3699115e-05 -2.7429829e-05][-3.6601094e-05 -3.0479634e-05 -2.2853706e-05 -8.4206331e-06 1.9764942e-05 6.2461295e-05 0.00011140167 0.00015249592 0.0001694778 0.0001534498 0.00011240293 6.4304964e-05 2.6072397e-05 1.7596903e-06 -1.0361546e-05][-2.1978831e-05 -6.0543534e-06 1.3477926e-05 4.7908266e-05 0.00011301052 0.00021042573 0.00032233429 0.00041561641 0.00045386152 0.00041917319 0.00032523912 0.00020794087 0.00010877055 4.4176333e-05 1.1175376e-05][1.334089e-05 5.1389383e-05 9.1922229e-05 0.00015651464 0.0002745846 0.0004511126 0.00065512513 0.0008259819 0.00090051128 0.00084727688 0.0006844555 0.00046432138 0.000262073 0.00012180888 4.6407193e-05][7.9882615e-05 0.00015315137 0.00022102395 0.00031698807 0.00048870267 0.000746181 0.0010439776 0.0012952907 0.0014124811 0.0013511123 0.0011282178 0.00080176105 0.00047697508 0.00023475796 9.6469892e-05][0.00018150054 0.00029485262 0.00038187913 0.00048827913 0.00068349944 0.00098998658 0.0013519756 0.0016634859 0.0018194932 0.0017634019 0.0015072591 0.0011088194 0.00068694452 0.00035147063 0.00014795075][0.00029879424 0.00043665516 0.00051184831 0.00058661937 0.00075697957 0.0010599972 0.0014380725 0.0017761564 0.0019583371 0.0019189364 0.001664725 0.0012503936 0.00079443381 0.00041740623 0.0001782082][0.00038540186 0.00052069227 0.00055262767 0.00055683724 0.000649953 0.00089006254 0.0012347499 0.0015700952 0.001766463 0.0017480298 0.0015223812 0.0011493105 0.00073649606 0.00039131308 0.00016739996][0.00039798408 0.00050758879 0.0004947437 0.00043411556 0.00044159844 0.0005839279 0.00084215141 0.0011289746 0.0013186755 0.0013227083 0.0011475965 0.00085587229 0.00054202258 0.00028447842 0.00011798314][0.00031929364 0.00039345576 0.00035963856 0.00027569139 0.00023168104 0.00028434745 0.00043085814 0.00062155438 0.00076516473 0.00078375777 0.00067477522 0.00049084419 0.00030019807 0.00015028988 5.4908443e-05][0.00018897545 0.00022985123 0.00020098651 0.0001361297 8.7635381e-05 9.0946756e-05 0.00014756352 0.00023722689 0.00031374433 0.00033056937 0.00028215509 0.00019803722 0.00011317815 4.86598e-05 7.8587182e-06][7.2279385e-05 9.159989e-05 7.7086959e-05 4.3542444e-05 1.3189507e-05 1.421904e-06 1.0519318e-05 3.59005e-05 6.2011553e-05 6.9801616e-05 5.624105e-05 3.2746539e-05 9.4594216e-06 -7.5975695e-06 -1.88759e-05][1.4397447e-06 1.0270785e-05 7.0524766e-06 -3.4546101e-06 -1.4655518e-05 -2.332911e-05 -2.8129005e-05 -2.8759947e-05 -2.6148369e-05 -2.4622525e-05 -2.6022321e-05 -2.838529e-05 -3.0529034e-05 -3.1846987e-05 -3.3091146e-05][-2.7769223e-05 -2.3442375e-05 -2.236e-05 -2.4369056e-05 -2.7529943e-05 -3.0939078e-05 -3.4193174e-05 -3.7209298e-05 -3.9035946e-05 -3.9702762e-05 -4.004865e-05 -4.0297018e-05 -4.0135419e-05 -3.97831e-05 -3.9574952e-05][-3.8007547e-05 -3.5299588e-05 -3.3691336e-05 -3.3755226e-05 -3.4571756e-05 -3.5880788e-05 -3.7305083e-05 -3.8667677e-05 -3.9948161e-05 -4.0893672e-05 -4.1682481e-05 -4.2001819e-05 -4.192115e-05 -4.1810432e-05 -4.1759369e-05]]...]
INFO - root - 2017-12-09 07:55:29.528530: step 3810, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.853 sec/batch; 77h:51m:23s remains)
INFO - root - 2017-12-09 07:55:38.123786: step 3820, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.857 sec/batch; 78h:16m:22s remains)
INFO - root - 2017-12-09 07:55:46.586899: step 3830, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.872 sec/batch; 79h:36m:33s remains)
INFO - root - 2017-12-09 07:55:55.256676: step 3840, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.886 sec/batch; 80h:54m:16s remains)
INFO - root - 2017-12-09 07:56:03.631884: step 3850, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.842 sec/batch; 76h:53m:15s remains)
INFO - root - 2017-12-09 07:56:12.133623: step 3860, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.836 sec/batch; 76h:20m:37s remains)
INFO - root - 2017-12-09 07:56:20.919067: step 3870, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.859 sec/batch; 78h:22m:27s remains)
INFO - root - 2017-12-09 07:56:29.601825: step 3880, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.853 sec/batch; 77h:53m:08s remains)
INFO - root - 2017-12-09 07:56:38.397241: step 3890, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.872 sec/batch; 79h:37m:37s remains)
INFO - root - 2017-12-09 07:56:47.021036: step 3900, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.830 sec/batch; 75h:46m:11s remains)
2017-12-09 07:56:47.876968: I tensorflow/core/kernels/logging_ops.cc:79] [[[-9.7257e-07 1.488367e-05 2.3812659e-05 2.7575043e-05 3.0629933e-05 3.4833625e-05 4.0368628e-05 4.6373978e-05 5.2313713e-05 5.5571611e-05 5.4121134e-05 5.0568458e-05 4.4068191e-05 3.5765959e-05 2.4866305e-05][-1.3135377e-05 1.067936e-06 9.3524577e-06 1.2801574e-05 1.5026715e-05 1.8146631e-05 2.2691478e-05 2.9006776e-05 3.692889e-05 4.1686006e-05 4.1475279e-05 3.7542486e-05 3.0113515e-05 2.0971936e-05 9.9318131e-06][-2.4208741e-05 -1.2583685e-05 -5.3558542e-06 -2.9002331e-06 -2.0998268e-06 -4.9175287e-07 2.6151974e-06 8.7235385e-06 1.7728787e-05 2.357037e-05 2.4505956e-05 2.2185253e-05 1.6390055e-05 7.9854581e-06 -1.7283019e-06][-3.1538595e-05 -2.3267563e-05 -1.7733048e-05 -1.5246042e-05 -1.423568e-05 -1.406173e-05 -1.2461391e-05 -7.1539034e-06 1.8341307e-06 8.3582127e-06 1.0849217e-05 9.0466274e-06 4.9005102e-06 -1.387285e-06 -8.8036832e-06][-3.56028e-05 -2.9456649e-05 -2.5285834e-05 -2.2995944e-05 -2.1899486e-05 -2.2346467e-05 -2.1557291e-05 -1.774386e-05 -9.9561657e-06 -4.1232997e-06 -1.3776007e-06 -1.8121063e-06 -4.0117229e-06 -7.4734053e-06 -1.2097982e-05][-3.7174235e-05 -3.2601987e-05 -2.920493e-05 -2.6390568e-05 -2.5521429e-05 -2.6788159e-05 -2.6929694e-05 -2.3349705e-05 -1.6197351e-05 -1.0903605e-05 -8.04758e-06 -7.2657785e-06 -7.5134449e-06 -9.7021839e-06 -1.3811175e-05][-3.7776743e-05 -3.4609278e-05 -3.1577612e-05 -2.7848633e-05 -2.6776186e-05 -2.8353752e-05 -2.9130399e-05 -2.6472066e-05 -2.0738196e-05 -1.4998506e-05 -1.0963886e-05 -8.0284735e-06 -6.9433372e-06 -8.0759492e-06 -1.1605138e-05][-3.7724905e-05 -3.6245492e-05 -3.4157973e-05 -3.1266496e-05 -2.9895182e-05 -3.0779418e-05 -3.1490708e-05 -2.9583149e-05 -2.55636e-05 -2.051414e-05 -1.6222719e-05 -1.2021897e-05 -8.6717337e-06 -7.9607707e-06 -9.7635275e-06][-3.6996469e-05 -3.5788307e-05 -3.4539302e-05 -3.2883065e-05 -3.2317374e-05 -3.283755e-05 -3.3221386e-05 -3.1862553e-05 -2.9019269e-05 -2.4970923e-05 -2.1233467e-05 -1.7679082e-05 -1.4650519e-05 -1.2988716e-05 -1.2971843e-05][-3.6868569e-05 -3.5343473e-05 -3.4332352e-05 -3.3228745e-05 -3.3041266e-05 -3.3631979e-05 -3.3804994e-05 -3.289997e-05 -3.105382e-05 -2.82625e-05 -2.5278841e-05 -2.2482905e-05 -1.9844163e-05 -1.84649e-05 -1.799177e-05][-3.7989616e-05 -3.6483081e-05 -3.5416117e-05 -3.4189194e-05 -3.4171528e-05 -3.5051675e-05 -3.5460554e-05 -3.4881756e-05 -3.3411568e-05 -3.1144973e-05 -2.8940776e-05 -2.7073031e-05 -2.5113295e-05 -2.4186087e-05 -2.4317582e-05][-4.0039948e-05 -3.906116e-05 -3.8328108e-05 -3.7479334e-05 -3.7328082e-05 -3.7846949e-05 -3.8150276e-05 -3.7835296e-05 -3.675852e-05 -3.4876724e-05 -3.3214044e-05 -3.186247e-05 -3.0702333e-05 -3.0625954e-05 -3.1599338e-05][-4.2335105e-05 -4.178732e-05 -4.1245268e-05 -4.0399733e-05 -4.0143052e-05 -4.0454182e-05 -4.0790881e-05 -4.0630679e-05 -3.9840575e-05 -3.8301605e-05 -3.6814461e-05 -3.6004636e-05 -3.5224777e-05 -3.5543322e-05 -3.6763395e-05][-4.4363267e-05 -4.3699496e-05 -4.3118991e-05 -4.2433632e-05 -4.2165047e-05 -4.2371514e-05 -4.2828739e-05 -4.2731128e-05 -4.2277141e-05 -4.13406e-05 -4.0261497e-05 -3.9559236e-05 -3.9035487e-05 -3.9321771e-05 -4.0414438e-05][-4.5339773e-05 -4.4886103e-05 -4.436008e-05 -4.3738772e-05 -4.3569882e-05 -4.3893968e-05 -4.4289824e-05 -4.4388835e-05 -4.4138513e-05 -4.3622611e-05 -4.3111228e-05 -4.2646447e-05 -4.2142281e-05 -4.231463e-05 -4.3034983e-05]]...]
INFO - root - 2017-12-09 07:56:56.498441: step 3910, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 79h:07m:39s remains)
INFO - root - 2017-12-09 07:57:05.170669: step 3920, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.882 sec/batch; 80h:28m:02s remains)
INFO - root - 2017-12-09 07:57:13.937622: step 3930, loss = 0.82, batch loss = 0.69 (8.7 examples/sec; 0.921 sec/batch; 84h:05m:02s remains)
INFO - root - 2017-12-09 07:57:22.802945: step 3940, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.893 sec/batch; 81h:32m:45s remains)
INFO - root - 2017-12-09 07:57:31.362176: step 3950, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.860 sec/batch; 78h:30m:14s remains)
INFO - root - 2017-12-09 07:57:39.908381: step 3960, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.854 sec/batch; 77h:54m:34s remains)
INFO - root - 2017-12-09 07:57:48.302159: step 3970, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.830 sec/batch; 75h:44m:08s remains)
INFO - root - 2017-12-09 07:57:56.753019: step 3980, loss = 0.82, batch loss = 0.69 (10.0 examples/sec; 0.802 sec/batch; 73h:08m:29s remains)
INFO - root - 2017-12-09 07:58:05.301545: step 3990, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.855 sec/batch; 78h:03m:52s remains)
INFO - root - 2017-12-09 07:58:13.964462: step 4000, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.875 sec/batch; 79h:50m:55s remains)
2017-12-09 07:58:14.728233: I tensorflow/core/kernels/logging_ops.cc:79] [[[-2.2863707e-05 -2.3058969e-05 -2.6801164e-05 -3.2276293e-05 -3.6314905e-05 -4.0415729e-05 -4.3125397e-05 -4.5461464e-05 -4.6959474e-05 -4.7180642e-05 -4.6519621e-05 -4.5969089e-05 -4.4725406e-05 -4.3568711e-05 -4.2302738e-05][1.961604e-05 1.7633967e-05 2.8432987e-06 -1.2663419e-05 -2.5985257e-05 -3.6059155e-05 -4.0966217e-05 -4.4429289e-05 -4.5955312e-05 -4.6342444e-05 -4.5647066e-05 -4.5097891e-05 -4.368068e-05 -4.2355266e-05 -4.13424e-05][8.5991036e-05 7.7012781e-05 5.1792027e-05 2.1717977e-05 -7.8191006e-06 -2.8413546e-05 -3.8950187e-05 -4.3311189e-05 -4.4538585e-05 -4.5587833e-05 -4.4932189e-05 -4.4141656e-05 -4.2594409e-05 -4.1312582e-05 -4.0614977e-05][0.000143573 0.00014409289 0.00011294715 7.18362e-05 2.6020592e-05 -1.1485019e-05 -3.3945053e-05 -4.2234777e-05 -4.3262844e-05 -4.3859494e-05 -4.34791e-05 -4.2868327e-05 -4.1418341e-05 -4.0158313e-05 -3.9408937e-05][0.00020105265 0.00020863475 0.00017659039 0.00013206645 7.569461e-05 1.9230043e-05 -2.0857507e-05 -3.8113143e-05 -4.1324733e-05 -4.19119e-05 -4.1649939e-05 -4.0936189e-05 -3.9752467e-05 -3.9084345e-05 -3.9119197e-05][0.00025074725 0.00026873441 0.00023735013 0.00019297811 0.00013236044 6.2919135e-05 4.2396205e-06 -2.8702481e-05 -3.853739e-05 -3.990818e-05 -3.9768576e-05 -3.9337549e-05 -3.8433322e-05 -3.8304617e-05 -3.8688224e-05][0.00029574806 0.0003197763 0.00029534684 0.00025289657 0.00019319508 0.0001197715 4.7981252e-05 -2.8564464e-06 -2.7462411e-05 -3.5757606e-05 -3.7763581e-05 -3.7856273e-05 -3.7349695e-05 -3.802601e-05 -3.8518996e-05][0.00031715696 0.00035053625 0.00033488561 0.00030128524 0.00025212124 0.0001842731 0.00010962242 4.6960282e-05 3.8938379e-06 -2.0667765e-05 -3.2274645e-05 -3.6163208e-05 -3.6485297e-05 -3.7016471e-05 -3.6273013e-05][0.00032364717 0.0003646182 0.00035767292 0.0003324094 0.00029301032 0.00023717539 0.00017150289 0.00010737366 5.0284652e-05 6.2854233e-06 -2.136398e-05 -3.3499888e-05 -3.63752e-05 -3.7550213e-05 -3.7159807e-05][0.00031285948 0.00036558585 0.00036668364 0.00034738734 0.00031387265 0.00026886078 0.0002149887 0.00015526 9.2961665e-05 3.59255e-05 -8.1408507e-06 -3.0072537e-05 -3.5634806e-05 -3.6863235e-05 -3.7439971e-05][0.00029397215 0.00035164232 0.00035819469 0.00034375337 0.00031259324 0.00027121662 0.0002232782 0.00016852783 0.0001069173 4.5985689e-05 -1.3364552e-06 -2.7544182e-05 -3.5650057e-05 -3.7318525e-05 -3.7146354e-05][0.0002510997 0.00030545588 0.00031542493 0.00030610187 0.0002801661 0.00024379128 0.00020023079 0.00014944308 9.2735267e-05 3.67115e-05 -5.292386e-06 -2.8379909e-05 -3.5444264e-05 -3.7201171e-05 -3.6940008e-05][0.00018238735 0.00022515624 0.00023461723 0.00023013477 0.00021218655 0.0001834666 0.00014711029 0.00010441766 5.8029298e-05 1.4846191e-05 -1.5367863e-05 -3.1290216e-05 -3.5726091e-05 -3.6857989e-05 -3.6682581e-05][9.9393757e-05 0.00012682544 0.00013402058 0.00013322114 0.00012370366 0.00010495813 7.9408564e-05 4.9490438e-05 1.8886916e-05 -7.716546e-06 -2.5276386e-05 -3.4236637e-05 -3.6248111e-05 -3.6899572e-05 -3.7123442e-05][3.4853998e-05 4.8591726e-05 5.3135169e-05 5.4136879e-05 4.9727794e-05 3.879796e-05 2.3462017e-05 5.9266749e-06 -1.0903859e-05 -2.4570225e-05 -3.3045773e-05 -3.6278088e-05 -3.6707446e-05 -3.7113252e-05 -3.7785914e-05]]...]
INFO - root - 2017-12-09 07:58:23.195467: step 4010, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.850 sec/batch; 77h:36m:00s remains)
INFO - root - 2017-12-09 07:58:31.731396: step 4020, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.877 sec/batch; 80h:03m:51s remains)
INFO - root - 2017-12-09 07:58:40.317095: step 4030, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.852 sec/batch; 77h:43m:46s remains)
INFO - root - 2017-12-09 07:58:48.938859: step 4040, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.859 sec/batch; 78h:21m:39s remains)
INFO - root - 2017-12-09 07:58:57.253330: step 4050, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 78h:48m:54s remains)
INFO - root - 2017-12-09 07:59:05.884569: step 4060, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 78h:48m:42s remains)
INFO - root - 2017-12-09 07:59:14.553358: step 4070, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.882 sec/batch; 80h:27m:56s remains)
INFO - root - 2017-12-09 07:59:23.150073: step 4080, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 79h:07m:17s remains)
INFO - root - 2017-12-09 07:59:31.745003: step 4090, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.840 sec/batch; 76h:37m:47s remains)
INFO - root - 2017-12-09 07:59:40.500888: step 4100, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.886 sec/batch; 80h:46m:59s remains)
2017-12-09 07:59:41.448954: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.069804989 0.070849895 0.070673488 0.070418291 0.070108518 0.069794059 0.069522545 0.069260947 0.068959422 0.068471506 0.067842506 0.066719607 0.0654018 0.063556232 0.060830895][0.074776113 0.0759597 0.075698204 0.075302988 0.074797206 0.07429076 0.073818289 0.073378585 0.072899304 0.072243549 0.071407251 0.070029579 0.068421565 0.066216439 0.063156642][0.079528578 0.080810353 0.080554895 0.080097377 0.079455085 0.078734063 0.078001529 0.077316441 0.076593526 0.075670384 0.07457307 0.072891861 0.07093361 0.0683161 0.064851105][0.0838742 0.085424937 0.085288614 0.084819883 0.084067881 0.083154842 0.0821511 0.081131876 0.080088228 0.078877114 0.077528663 0.07555636 0.073222421 0.070200577 0.0663622][0.088149987 0.090062559 0.090163983 0.089816123 0.089108862 0.0881143 0.0868911 0.085533276 0.084125422 0.082556769 0.080847278 0.078470126 0.075609244 0.071990706 0.067534849][0.091896 0.094305642 0.094713986 0.0946154 0.094131358 0.093252651 0.091989584 0.090402134 0.0886696 0.086746328 0.084605537 0.08168219 0.078107655 0.073621057 0.068243183][0.094650142 0.097516552 0.098188318 0.098363578 0.098169386 0.097536355 0.096401274 0.09474697 0.092814431 0.090590954 0.088005461 0.084479734 0.080047555 0.074584149 0.068130106][0.096459068 0.099475659 0.10018127 0.10047638 0.10047341 0.10004507 0.099092461 0.097541623 0.095621072 0.093256295 0.090343207 0.086258993 0.081002787 0.074520059 0.066931255][0.0972538 0.10039538 0.10106256 0.10129315 0.10129236 0.10093693 0.1000982 0.098652989 0.096798591 0.094389096 0.091246009 0.086730406 0.080779329 0.073375784 0.064742543][0.09728542 0.100233 0.10073266 0.10088418 0.1008665 0.10053824 0.099779509 0.098504648 0.096759409 0.094313428 0.090940058 0.086060919 0.079565167 0.071389772 0.061902933][0.096546717 0.099144153 0.09933079 0.099352278 0.099248424 0.098902613 0.098206446 0.097031064 0.095300287 0.092706673 0.089019082 0.083788432 0.076863796 0.068208493 0.058249872][0.094603449 0.0967525 0.096598789 0.096516676 0.096330993 0.095952354 0.0952889 0.0941435 0.092370689 0.089510389 0.085441232 0.079862073 0.072601922 0.063750044 0.053721145][0.09224914 0.093909726 0.0934548 0.093124315 0.092694908 0.092239074 0.091517329 0.090261631 0.088310465 0.085199915 0.080849476 0.075006984 0.067626774 0.058851715 0.049118318][0.08925344 0.09058018 0.089980923 0.0894298 0.088763289 0.088079043 0.087158 0.08578001 0.0835888 0.080277495 0.07573799 0.0698275 0.062521949 0.054045737 0.044928938][0.08514294 0.086394578 0.085776918 0.08519759 0.084508784 0.083792187 0.082762033 0.081179239 0.0787662 0.075222485 0.070473351 0.064515196 0.057386432 0.049436197 0.041162688]]...]
INFO - root - 2017-12-09 07:59:50.011383: step 4110, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.863 sec/batch; 78h:45m:18s remains)
INFO - root - 2017-12-09 07:59:58.673237: step 4120, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.863 sec/batch; 78h:45m:40s remains)
INFO - root - 2017-12-09 08:00:07.123404: step 4130, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 78h:38m:15s remains)
INFO - root - 2017-12-09 08:00:15.691562: step 4140, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.870 sec/batch; 79h:22m:18s remains)
INFO - root - 2017-12-09 08:00:24.182392: step 4150, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 79h:16m:19s remains)
INFO - root - 2017-12-09 08:00:32.678091: step 4160, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.859 sec/batch; 78h:19m:07s remains)
INFO - root - 2017-12-09 08:00:41.385125: step 4170, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.880 sec/batch; 80h:13m:46s remains)
INFO - root - 2017-12-09 08:00:49.965386: step 4180, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.859 sec/batch; 78h:23m:07s remains)
INFO - root - 2017-12-09 08:00:58.743282: step 4190, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.890 sec/batch; 81h:10m:19s remains)
INFO - root - 2017-12-09 08:01:07.495307: step 4200, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.900 sec/batch; 82h:06m:14s remains)
2017-12-09 08:01:08.446603: I tensorflow/core/kernels/logging_ops.cc:79] [[[-1.8693776e-05 -1.975225e-05 -2.2057073e-05 -2.3434251e-05 -2.2806053e-05 -2.0645159e-05 -1.780867e-05 -1.573834e-05 -1.5692502e-05 -1.7777427e-05 -2.0306099e-05 -2.1345342e-05 -2.1636926e-05 -2.1135489e-05 -2.0172647e-05][-1.7891361e-05 -1.9112045e-05 -2.1486416e-05 -2.2475906e-05 -2.1454543e-05 -1.909057e-05 -1.660105e-05 -1.4951762e-05 -1.5019992e-05 -1.7143568e-05 -1.9827694e-05 -2.1181731e-05 -2.1635387e-05 -2.1099098e-05 -2.0176016e-05][-1.8078761e-05 -1.9098283e-05 -2.1298343e-05 -2.2148102e-05 -2.0961259e-05 -1.858922e-05 -1.6703834e-05 -1.5830436e-05 -1.6139864e-05 -1.7858998e-05 -2.0643773e-05 -2.2047825e-05 -2.2354685e-05 -2.1333679e-05 -2.0362091e-05][-1.9455831e-05 -1.9913812e-05 -2.167447e-05 -2.2172018e-05 -2.0614058e-05 -1.78855e-05 -1.6180456e-05 -1.5790592e-05 -1.6620426e-05 -1.8325329e-05 -2.0735115e-05 -2.1842174e-05 -2.1722357e-05 -2.0615455e-05 -2.0068765e-05][-2.1532625e-05 -2.148316e-05 -2.2768872e-05 -2.2881337e-05 -2.0816886e-05 -1.7445684e-05 -1.5050202e-05 -1.4429377e-05 -1.5581023e-05 -1.7707931e-05 -1.9894676e-05 -2.0808377e-05 -2.0536416e-05 -1.9810181e-05 -1.9935414e-05][-2.4065328e-05 -2.3426481e-05 -2.4039779e-05 -2.374806e-05 -2.1536911e-05 -1.7616629e-05 -1.4221532e-05 -1.2821991e-05 -1.3979985e-05 -1.6495494e-05 -1.8882474e-05 -1.985424e-05 -1.982657e-05 -1.9550778e-05 -2.0275296e-05][-2.7068847e-05 -2.5830333e-05 -2.5697605e-05 -2.4819368e-05 -2.2244654e-05 -1.7893279e-05 -1.3761804e-05 -1.1636061e-05 -1.2823846e-05 -1.5689977e-05 -1.795452e-05 -1.9128824e-05 -1.9414838e-05 -1.9521427e-05 -2.036383e-05][-2.9264138e-05 -2.7655875e-05 -2.6998645e-05 -2.5389214e-05 -2.2253298e-05 -1.7490933e-05 -1.3051496e-05 -1.0839016e-05 -1.187485e-05 -1.4644702e-05 -1.6960294e-05 -1.8461189e-05 -1.9308267e-05 -1.985579e-05 -2.0603395e-05][-3.0410003e-05 -2.8618964e-05 -2.7711616e-05 -2.5570156e-05 -2.1772303e-05 -1.6717146e-05 -1.2388678e-05 -1.021746e-05 -1.0963253e-05 -1.339657e-05 -1.5554742e-05 -1.7472314e-05 -1.9106643e-05 -2.0394567e-05 -2.1312222e-05][-3.0532545e-05 -2.8651066e-05 -2.7599359e-05 -2.53432e-05 -2.1355405e-05 -1.638972e-05 -1.2433087e-05 -1.0307318e-05 -1.0487602e-05 -1.2353248e-05 -1.461514e-05 -1.7098868e-05 -1.9347364e-05 -2.1073563e-05 -2.2263521e-05][-3.0694839e-05 -2.876536e-05 -2.7548129e-05 -2.5554458e-05 -2.199427e-05 -1.7507726e-05 -1.3797686e-05 -1.1695316e-05 -1.1637821e-05 -1.3041885e-05 -1.525064e-05 -1.7979084e-05 -2.0389314e-05 -2.2419765e-05 -2.3894794e-05][-3.1869811e-05 -2.9979867e-05 -2.8741233e-05 -2.715868e-05 -2.4222598e-05 -2.0323165e-05 -1.6979786e-05 -1.5026744e-05 -1.4955618e-05 -1.60145e-05 -1.7860209e-05 -2.0287593e-05 -2.2648957e-05 -2.477286e-05 -2.636305e-05][-3.3308777e-05 -3.1553231e-05 -3.0481635e-05 -2.9334056e-05 -2.7235052e-05 -2.4423738e-05 -2.194945e-05 -2.0420175e-05 -2.0289306e-05 -2.087055e-05 -2.204661e-05 -2.3814631e-05 -2.5713023e-05 -2.7475631e-05 -2.8946481e-05][-3.5275243e-05 -3.3748242e-05 -3.2886193e-05 -3.2118875e-05 -3.08781e-05 -2.9261715e-05 -2.7679645e-05 -2.6564147e-05 -2.623547e-05 -2.6371636e-05 -2.6952748e-05 -2.8031947e-05 -2.9325925e-05 -3.056967e-05 -3.1765263e-05][-3.709042e-05 -3.5947098e-05 -3.5488541e-05 -3.5079185e-05 -3.4501823e-05 -3.3753135e-05 -3.2963188e-05 -3.2321259e-05 -3.1930718e-05 -3.1887295e-05 -3.2196393e-05 -3.2734366e-05 -3.3339955e-05 -3.389542e-05 -3.4628069e-05]]...]
INFO - root - 2017-12-09 08:01:16.890528: step 4210, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.852 sec/batch; 77h:40m:40s remains)
INFO - root - 2017-12-09 08:01:25.542907: step 4220, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 79h:10m:33s remains)
INFO - root - 2017-12-09 08:01:34.010971: step 4230, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.858 sec/batch; 78h:14m:27s remains)
INFO - root - 2017-12-09 08:01:42.702771: step 4240, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.882 sec/batch; 80h:24m:38s remains)
INFO - root - 2017-12-09 08:01:51.059516: step 4250, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.879 sec/batch; 80h:07m:45s remains)
INFO - root - 2017-12-09 08:01:59.870467: step 4260, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.877 sec/batch; 80h:00m:18s remains)
INFO - root - 2017-12-09 08:02:08.572033: step 4270, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.891 sec/batch; 81h:14m:11s remains)
INFO - root - 2017-12-09 08:02:17.340982: step 4280, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.858 sec/batch; 78h:14m:50s remains)
INFO - root - 2017-12-09 08:02:25.978934: step 4290, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.833 sec/batch; 75h:56m:34s remains)
INFO - root - 2017-12-09 08:02:34.573005: step 4300, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 78h:45m:07s remains)
2017-12-09 08:02:35.468686: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.10406374 0.1058034 0.10587012 0.1057992 0.10566068 0.10548151 0.10531168 0.10518218 0.10512253 0.10478757 0.10359373 0.10187638 0.098990418 0.095322393 0.090848118][0.1037076 0.10547114 0.10551499 0.1054967 0.10541215 0.10528489 0.10515954 0.10504682 0.1049865 0.10458584 0.10339905 0.1017013 0.098864079 0.095268957 0.0909589][0.10213567 0.10383793 0.10392 0.10396374 0.10395502 0.10389022 0.10380387 0.10370119 0.10362338 0.10322704 0.10219442 0.10042164 0.097621836 0.094087504 0.089857869][0.10016093 0.10186476 0.10200056 0.10213269 0.10222506 0.10225271 0.10221735 0.10211375 0.10198512 0.10137987 0.10031818 0.098472834 0.095639832 0.092131287 0.088053577][0.098226048 0.099943712 0.10014452 0.1003951 0.10063335 0.10078901 0.10083017 0.10072859 0.10052117 0.099650472 0.0983868 0.09641251 0.093493849 0.089974262 0.086055078][0.096087031 0.097966939 0.098269008 0.098689869 0.099133752 0.099471256 0.099620551 0.0995172 0.099203363 0.098181143 0.09673728 0.094736494 0.091997318 0.088468723 0.084709212][0.094001971 0.095979854 0.09639556 0.096999474 0.097646445 0.098155826 0.098390289 0.098284982 0.097865209 0.096699081 0.095021918 0.092897058 0.090341084 0.086903736 0.083227195][0.092140809 0.094159149 0.09471067 0.095461123 0.096254453 0.096870355 0.097140335 0.096985057 0.096439004 0.095164545 0.093296319 0.091010563 0.088636413 0.085382819 0.081715383][0.089834087 0.091934696 0.09254244 0.09334065 0.094155923 0.094754919 0.094973087 0.094783433 0.094146982 0.09279342 0.090772286 0.088600487 0.086481981 0.083538979 0.080069095][0.087784886 0.090054646 0.0908231 0.091663033 0.092467435 0.092957616 0.09305767 0.0927618 0.092022337 0.090739839 0.088689566 0.086580209 0.084636539 0.081960469 0.078609325][0.085828252 0.088099904 0.088824332 0.089616105 0.090319358 0.090697512 0.090720557 0.090385854 0.089650504 0.08852683 0.086640432 0.08472921 0.0829921 0.080593124 0.077366352][0.083365768 0.085616343 0.086319558 0.086983606 0.087485768 0.087752827 0.087722629 0.087421358 0.086804576 0.085820042 0.084228873 0.082396232 0.080685094 0.078372777 0.075380519][0.081146561 0.083310649 0.083949246 0.0844826 0.084805943 0.084900528 0.084761523 0.084493853 0.083978824 0.083189987 0.081970029 0.080351405 0.07872431 0.07649453 0.073669717][0.078563489 0.080622457 0.081183828 0.081535995 0.081692711 0.081719987 0.081552468 0.081293866 0.080817677 0.080118872 0.07906732 0.077672549 0.076118663 0.074065156 0.071646415][0.075232573 0.077226467 0.07775192 0.078173772 0.078422628 0.078596883 0.078561679 0.078299426 0.077804893 0.077041611 0.076000512 0.074630387 0.073079139 0.071244784 0.069268808]]...]
INFO - root - 2017-12-09 08:02:44.061642: step 4310, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.855 sec/batch; 77h:55m:52s remains)
INFO - root - 2017-12-09 08:02:52.698336: step 4320, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 78h:43m:20s remains)
INFO - root - 2017-12-09 08:03:01.341603: step 4330, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.888 sec/batch; 80h:58m:31s remains)
INFO - root - 2017-12-09 08:03:10.112629: step 4340, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 78h:36m:13s remains)
INFO - root - 2017-12-09 08:03:18.622670: step 4350, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.861 sec/batch; 78h:30m:33s remains)
INFO - root - 2017-12-09 08:03:27.220888: step 4360, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 78h:59m:52s remains)
INFO - root - 2017-12-09 08:03:35.898399: step 4370, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.846 sec/batch; 77h:07m:36s remains)
INFO - root - 2017-12-09 08:03:44.568833: step 4380, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.883 sec/batch; 80h:27m:05s remains)
INFO - root - 2017-12-09 08:03:53.321770: step 4390, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.875 sec/batch; 79h:44m:07s remains)
INFO - root - 2017-12-09 08:04:01.998468: step 4400, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.874 sec/batch; 79h:40m:22s remains)
2017-12-09 08:04:02.924775: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.54148e-05 -4.4344531e-05 -4.4207743e-05 -4.4112159e-05 -4.40324e-05 -4.424488e-05 -4.46992e-05 -4.5271358e-05 -4.5716715e-05 -4.5720531e-05 -4.5372348e-05 -4.487393e-05 -4.4420907e-05 -4.4124481e-05 -4.411432e-05][-4.4811666e-05 -4.3623149e-05 -4.3335291e-05 -4.2946685e-05 -4.2683911e-05 -4.2892e-05 -4.3386666e-05 -4.4020533e-05 -4.4546956e-05 -4.4569446e-05 -4.4286109e-05 -4.3761167e-05 -4.3256441e-05 -4.28224e-05 -4.2691026e-05][-4.6010406e-05 -4.5016452e-05 -4.4669181e-05 -4.4022247e-05 -4.3386688e-05 -4.3177777e-05 -4.328903e-05 -4.3651671e-05 -4.4038654e-05 -4.4092507e-05 -4.4132204e-05 -4.4055079e-05 -4.39064e-05 -4.3493019e-05 -4.3150554e-05][-4.6621739e-05 -4.5736771e-05 -4.5425557e-05 -4.486883e-05 -4.4110937e-05 -4.3706998e-05 -4.3525535e-05 -4.3567372e-05 -4.3694286e-05 -4.3717235e-05 -4.4024997e-05 -4.4358709e-05 -4.4577755e-05 -4.4349621e-05 -4.4067205e-05][-4.6588066e-05 -4.5716806e-05 -4.5384382e-05 -4.4628476e-05 -4.3695498e-05 -4.3117383e-05 -4.2725678e-05 -4.2488435e-05 -4.2470965e-05 -4.2550844e-05 -4.3115586e-05 -4.3657812e-05 -4.4039931e-05 -4.4170243e-05 -4.4244742e-05][-4.5975481e-05 -4.4879198e-05 -4.4451168e-05 -4.3599433e-05 -4.2554246e-05 -4.1904845e-05 -4.1364692e-05 -4.0905081e-05 -4.0941657e-05 -4.1301977e-05 -4.2011237e-05 -4.2650488e-05 -4.3267843e-05 -4.3724453e-05 -4.4041175e-05][-4.469691e-05 -4.2995933e-05 -4.2124189e-05 -4.1138253e-05 -4.0618208e-05 -4.0622526e-05 -4.0357583e-05 -3.9989136e-05 -4.0020259e-05 -4.0517254e-05 -4.1102609e-05 -4.1631913e-05 -4.2339307e-05 -4.2948617e-05 -4.3360989e-05][-4.2930093e-05 -4.0792376e-05 -3.9793733e-05 -3.90447e-05 -3.9153241e-05 -3.9710034e-05 -4.0071434e-05 -4.027256e-05 -4.0415896e-05 -4.0814324e-05 -4.1104413e-05 -4.1416322e-05 -4.1946696e-05 -4.2342439e-05 -4.2575521e-05][-4.2404561e-05 -4.0408097e-05 -3.9629489e-05 -3.9159764e-05 -3.9511364e-05 -4.0345611e-05 -4.111453e-05 -4.1825013e-05 -4.2258191e-05 -4.2654112e-05 -4.2634212e-05 -4.2317744e-05 -4.2243424e-05 -4.2260192e-05 -4.2144067e-05][-4.3322121e-05 -4.183019e-05 -4.1487878e-05 -4.1453692e-05 -4.1797659e-05 -4.2559448e-05 -4.3330543e-05 -4.3980472e-05 -4.4430741e-05 -4.4740427e-05 -4.4476721e-05 -4.3864522e-05 -4.3468994e-05 -4.3051925e-05 -4.2534411e-05][-4.418048e-05 -4.2468884e-05 -4.2236552e-05 -4.267893e-05 -4.3475367e-05 -4.450095e-05 -4.5469224e-05 -4.6084067e-05 -4.6355097e-05 -4.6335914e-05 -4.5781606e-05 -4.4867338e-05 -4.4178138e-05 -4.3525321e-05 -4.2812495e-05][-4.4562606e-05 -4.2587304e-05 -4.2184816e-05 -4.2566309e-05 -4.3470922e-05 -4.4732307e-05 -4.5861834e-05 -4.6613139e-05 -4.6939142e-05 -4.6900866e-05 -4.6205107e-05 -4.5141984e-05 -4.446676e-05 -4.394021e-05 -4.3369721e-05][-4.6281304e-05 -4.4405977e-05 -4.4017164e-05 -4.42679e-05 -4.5018009e-05 -4.6167865e-05 -4.7093879e-05 -4.7629885e-05 -4.7837875e-05 -4.7720972e-05 -4.7048132e-05 -4.6006928e-05 -4.5320045e-05 -4.4828907e-05 -4.4402474e-05][-4.7378853e-05 -4.5986722e-05 -4.5884473e-05 -4.6094261e-05 -4.6611298e-05 -4.7355908e-05 -4.78788e-05 -4.8101705e-05 -4.8108996e-05 -4.7935202e-05 -4.7392925e-05 -4.657649e-05 -4.6082449e-05 -4.5790959e-05 -4.5565728e-05][-4.7454028e-05 -4.650624e-05 -4.6681773e-05 -4.69588e-05 -4.7324815e-05 -4.7651396e-05 -4.7851921e-05 -4.7845366e-05 -4.7745314e-05 -4.7588252e-05 -4.7254824e-05 -4.6725745e-05 -4.6433073e-05 -4.6357367e-05 -4.6293368e-05]]...]
INFO - root - 2017-12-09 08:04:11.377970: step 4410, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.846 sec/batch; 77h:03m:29s remains)
INFO - root - 2017-12-09 08:04:19.984559: step 4420, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 78h:32m:46s remains)
INFO - root - 2017-12-09 08:04:28.485322: step 4430, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 79h:02m:34s remains)
INFO - root - 2017-12-09 08:04:37.113249: step 4440, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.833 sec/batch; 75h:54m:42s remains)
INFO - root - 2017-12-09 08:04:45.460517: step 4450, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.860 sec/batch; 78h:21m:10s remains)
INFO - root - 2017-12-09 08:04:53.982399: step 4460, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.882 sec/batch; 80h:23m:57s remains)
INFO - root - 2017-12-09 08:05:02.610194: step 4470, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 78h:33m:02s remains)
INFO - root - 2017-12-09 08:05:11.106862: step 4480, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.843 sec/batch; 76h:50m:49s remains)
INFO - root - 2017-12-09 08:05:19.614923: step 4490, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.859 sec/batch; 78h:15m:51s remains)
INFO - root - 2017-12-09 08:05:28.274897: step 4500, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.851 sec/batch; 77h:31m:04s remains)
2017-12-09 08:05:29.146233: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.10101024 0.10593341 0.10874893 0.11083552 0.11235586 0.11311638 0.11366314 0.11384331 0.11377244 0.11260299 0.11021445 0.10739608 0.10371108 0.099222749 0.094411634][0.10219895 0.10726833 0.11029314 0.11251128 0.11396043 0.11444794 0.11499453 0.1152278 0.1153065 0.1145502 0.11280242 0.11076784 0.10749304 0.10345147 0.09872929][0.10099418 0.10604952 0.10915403 0.11123215 0.11235502 0.11245688 0.1127022 0.1131435 0.11357356 0.1133318 0.11253099 0.11163466 0.1092481 0.10574469 0.1012705][0.098161027 0.1029906 0.10576823 0.107426 0.1079856 0.10762283 0.10759531 0.10807712 0.1088452 0.10937431 0.10968252 0.11018723 0.10887919 0.10630112 0.10250516][0.094436377 0.098810144 0.10098933 0.10186084 0.10155106 0.10051613 0.10006607 0.10062458 0.10188261 0.10346532 0.1053357 0.10716584 0.10725124 0.10576787 0.10275189][0.089313485 0.093322411 0.094869331 0.094888434 0.093709245 0.0920017 0.091251239 0.091651537 0.093345553 0.096116304 0.099438392 0.10308012 0.10446865 0.10438045 0.10237218][0.083293051 0.086854488 0.087753437 0.087032795 0.085141823 0.083072469 0.082125865 0.082545318 0.084546775 0.088370025 0.092862688 0.0978877 0.10054076 0.10159657 0.10085338][0.076880157 0.079649881 0.079678148 0.078292504 0.075858034 0.07370694 0.072732732 0.073284775 0.075742356 0.080505326 0.086000718 0.092149861 0.096019432 0.098219417 0.098673791][0.069922537 0.071886949 0.071179494 0.069045857 0.06620764 0.064100236 0.06351205 0.064451404 0.067400321 0.072981358 0.0793459 0.086391404 0.091068015 0.094159424 0.095495209][0.062496521 0.064021759 0.063078672 0.060781643 0.058050279 0.056379382 0.056454659 0.057994727 0.061473392 0.067513838 0.074330688 0.081644945 0.08666686 0.090103924 0.09187638][0.055136275 0.056243826 0.055270616 0.053372055 0.051251952 0.05058017 0.05171283 0.05409617 0.058147937 0.064482167 0.071195662 0.078085631 0.08288274 0.086368896 0.088171467][0.048605788 0.049187791 0.04828449 0.046956208 0.045873489 0.046332207 0.048312593 0.051810145 0.056542583 0.062802218 0.069166966 0.075209953 0.079585783 0.082528107 0.084032968][0.04320702 0.043211289 0.042377442 0.041744355 0.041817307 0.043467842 0.046519071 0.050640166 0.055721011 0.061809942 0.067453384 0.072479039 0.076159008 0.078579515 0.079679884][0.039550245 0.039050605 0.038281869 0.038276549 0.039423112 0.042024698 0.045972504 0.050604958 0.055751223 0.061260834 0.06614098 0.07012856 0.072848439 0.074794225 0.075487785][0.038871787 0.038107216 0.037232254 0.037581697 0.039199844 0.042153396 0.046264544 0.050862726 0.055748153 0.060437266 0.064465791 0.067639641 0.069667816 0.071164362 0.071459308]]...]
INFO - root - 2017-12-09 08:05:37.600250: step 4510, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.870 sec/batch; 79h:17m:12s remains)
INFO - root - 2017-12-09 08:05:46.264051: step 4520, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 79h:19m:08s remains)
INFO - root - 2017-12-09 08:05:54.829565: step 4530, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.879 sec/batch; 80h:06m:12s remains)
INFO - root - 2017-12-09 08:06:03.330049: step 4540, loss = 0.82, batch loss = 0.69 (9.7 examples/sec; 0.827 sec/batch; 75h:19m:37s remains)
INFO - root - 2017-12-09 08:06:11.916682: step 4550, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.882 sec/batch; 80h:18m:39s remains)
INFO - root - 2017-12-09 08:06:20.722538: step 4560, loss = 0.82, batch loss = 0.69 (8.8 examples/sec; 0.911 sec/batch; 83h:01m:45s remains)
INFO - root - 2017-12-09 08:06:29.396308: step 4570, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.842 sec/batch; 76h:44m:15s remains)
INFO - root - 2017-12-09 08:06:38.259580: step 4580, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.883 sec/batch; 80h:25m:36s remains)
INFO - root - 2017-12-09 08:06:46.966919: step 4590, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.888 sec/batch; 80h:52m:48s remains)
INFO - root - 2017-12-09 08:06:55.725200: step 4600, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.873 sec/batch; 79h:29m:59s remains)
2017-12-09 08:06:56.658180: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.759341e-05 -4.6892175e-05 -4.6767309e-05 -4.6701651e-05 -4.6605124e-05 -4.64568e-05 -4.6295965e-05 -4.6241948e-05 -4.630394e-05 -4.645318e-05 -4.6544079e-05 -4.6708508e-05 -4.6871886e-05 -4.708764e-05 -4.7310576e-05][-4.7662241e-05 -4.6959234e-05 -4.6813497e-05 -4.6661495e-05 -4.6448258e-05 -4.6316945e-05 -4.6109581e-05 -4.5982782e-05 -4.6013603e-05 -4.6118963e-05 -4.62245e-05 -4.6468711e-05 -4.6858564e-05 -4.7337591e-05 -4.7779085e-05][-4.7627349e-05 -4.6608722e-05 -4.6353096e-05 -4.6421665e-05 -4.6471469e-05 -4.6561116e-05 -4.6470643e-05 -4.6371839e-05 -4.6381789e-05 -4.6463174e-05 -4.6557408e-05 -4.6822683e-05 -4.7292306e-05 -4.7935595e-05 -4.8512375e-05][-4.4629473e-05 -4.0464427e-05 -3.6537404e-05 -3.3798588e-05 -3.2979126e-05 -3.4262117e-05 -3.6438294e-05 -3.9426806e-05 -4.211751e-05 -4.4445802e-05 -4.5962723e-05 -4.6951205e-05 -4.7734458e-05 -4.8311314e-05 -4.8735274e-05][-4.2289539e-05 -3.5969973e-05 -2.8884806e-05 -2.2534663e-05 -1.8315044e-05 -1.75188e-05 -1.8794148e-05 -2.1348947e-05 -2.4653124e-05 -2.9441813e-05 -3.4262837e-05 -3.8952869e-05 -4.223323e-05 -4.3667245e-05 -4.3392556e-05][-3.37419e-05 -1.8059458e-05 1.0732256e-06 2.0326006e-05 3.5304176e-05 4.3435743e-05 4.6684778e-05 4.3208041e-05 3.5139165e-05 2.2278437e-05 7.1172835e-06 -7.6437427e-06 -2.0570013e-05 -3.0500541e-05 -3.569918e-05][-1.9632964e-05 1.506124e-05 6.0302271e-05 0.00010738655 0.00014625245 0.00017155462 0.00018433484 0.00018501241 0.00017464682 0.00015154161 0.00012114134 8.70765e-05 5.3507749e-05 2.4747504e-05 4.8140864e-06][1.489534e-06 6.1916428e-05 0.00013987825 0.00021893272 0.00028279042 0.00032519476 0.00034859349 0.00035481583 0.00034484241 0.00032035331 0.00028364852 0.00023791971 0.00018847888 0.00014023695 0.00010154777][3.0316551e-05 0.00011926018 0.00023158261 0.00034260415 0.0004292221 0.0004832331 0.00050886197 0.00051252509 0.00049877464 0.00046998836 0.00042842768 0.00037963691 0.00032652923 0.00027483035 0.00023117391][5.3802818e-05 0.00016053201 0.0002925708 0.00042189562 0.00052247412 0.0005846952 0.00061312469 0.00061682123 0.00060227129 0.00057167007 0.00052757125 0.00047527536 0.00042008396 0.00036906198 0.00032683532][4.7560658e-05 0.00015667011 0.00029134986 0.00042351181 0.00052798993 0.00059547613 0.00062855525 0.00063489063 0.00062098977 0.00058984396 0.00054498948 0.00049302477 0.0004405801 0.00039305937 0.0003526379][1.3928475e-05 0.00010083935 0.00021674761 0.00033686613 0.00043637477 0.00050473295 0.00054119783 0.00055030483 0.00053741905 0.00050698343 0.00046398895 0.00041577328 0.00036907595 0.00032829176 0.00029410282][-1.83294e-05 3.3064374e-05 0.00010924112 0.0001971056 0.00027767921 0.00033943486 0.0003765641 0.0003882194 0.00037753891 0.00034904404 0.0003096337 0.00026715125 0.0002278664 0.00019510937 0.00016949023][-3.6803307e-05 -1.336127e-05 2.42663e-05 7.283975e-05 0.00012250972 0.00016494782 0.00019312778 0.00020403418 0.00019858012 0.00017987366 0.00015345565 0.00012521504 9.9227262e-05 7.7745652e-05 6.2122628e-05][-4.651694e-05 -3.8219136e-05 -2.404863e-05 -3.0820956e-06 2.0871368e-05 4.3805841e-05 6.0927916e-05 6.9267677e-05 6.8678572e-05 6.0302649e-05 4.6981579e-05 3.1801421e-05 1.7507809e-05 5.7460493e-06 -2.7932183e-06]]...]
INFO - root - 2017-12-09 08:07:05.160041: step 4610, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 79h:02m:23s remains)
INFO - root - 2017-12-09 08:07:13.797267: step 4620, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 79h:06m:13s remains)
INFO - root - 2017-12-09 08:07:22.407114: step 4630, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.880 sec/batch; 80h:07m:07s remains)
INFO - root - 2017-12-09 08:07:31.088400: step 4640, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.874 sec/batch; 79h:38m:01s remains)
INFO - root - 2017-12-09 08:07:39.619039: step 4650, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.902 sec/batch; 82h:07m:00s remains)
INFO - root - 2017-12-09 08:07:48.285420: step 4660, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 78h:39m:16s remains)
INFO - root - 2017-12-09 08:07:56.988497: step 4670, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.870 sec/batch; 79h:15m:32s remains)
INFO - root - 2017-12-09 08:08:05.768105: step 4680, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.902 sec/batch; 82h:08m:05s remains)
INFO - root - 2017-12-09 08:08:14.559465: step 4690, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.865 sec/batch; 78h:46m:38s remains)
INFO - root - 2017-12-09 08:08:23.323917: step 4700, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.892 sec/batch; 81h:11m:52s remains)
2017-12-09 08:08:24.218295: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.0214727e-05 -4.0463128e-05 -4.1813368e-05 -4.361253e-05 -4.5147168e-05 -4.6250858e-05 -4.7304507e-05 -4.7970883e-05 -4.8002308e-05 -4.7842495e-05 -4.7713176e-05 -4.8162554e-05 -4.9067647e-05 -4.9930633e-05 -5.0985858e-05][-4.11372e-05 -4.0818028e-05 -4.1724186e-05 -4.2993463e-05 -4.377506e-05 -4.3936951e-05 -4.4372737e-05 -4.5221026e-05 -4.549549e-05 -4.5512574e-05 -4.5564891e-05 -4.6150464e-05 -4.6982117e-05 -4.76082e-05 -4.8490263e-05][-4.0337662e-05 -3.9240735e-05 -3.9431536e-05 -4.0434606e-05 -4.0492567e-05 -4.0071616e-05 -4.0149705e-05 -4.1353491e-05 -4.2164746e-05 -4.2525127e-05 -4.3058288e-05 -4.3619973e-05 -4.406906e-05 -4.4199714e-05 -4.4986042e-05][-3.6491227e-05 -3.4055327e-05 -3.3426582e-05 -3.39557e-05 -3.3529894e-05 -3.272088e-05 -3.2199347e-05 -3.3912398e-05 -3.5724075e-05 -3.6793241e-05 -3.7895243e-05 -3.7916678e-05 -3.7873997e-05 -3.7649879e-05 -3.843458e-05][-2.8866034e-05 -2.5277688e-05 -2.4083285e-05 -2.416093e-05 -2.323226e-05 -2.1698339e-05 -2.0538711e-05 -2.2954435e-05 -2.6602851e-05 -2.8699502e-05 -2.9806139e-05 -2.876544e-05 -2.8901846e-05 -2.9484108e-05 -3.0875617e-05][-2.1405325e-05 -1.7814185e-05 -1.7126731e-05 -1.7458151e-05 -1.6700211e-05 -1.5354482e-05 -1.4056342e-05 -1.679084e-05 -2.1676693e-05 -2.414922e-05 -2.4974095e-05 -2.2810513e-05 -2.2750384e-05 -2.3974251e-05 -2.6104968e-05][-1.9940926e-05 -1.6527192e-05 -1.6293168e-05 -1.7727369e-05 -1.8287315e-05 -1.7951883e-05 -1.7342551e-05 -1.9682921e-05 -2.3654466e-05 -2.5585072e-05 -2.6130208e-05 -2.3394005e-05 -2.2651315e-05 -2.4042183e-05 -2.63443e-05][-2.3274581e-05 -1.9933435e-05 -2.0293428e-05 -2.203964e-05 -2.2896857e-05 -2.3094573e-05 -2.2581004e-05 -2.4557241e-05 -2.7421334e-05 -2.8396993e-05 -2.8724677e-05 -2.6137e-05 -2.5129248e-05 -2.6248603e-05 -2.81346e-05][-2.7220376e-05 -2.4465731e-05 -2.5145113e-05 -2.6453345e-05 -2.6920432e-05 -2.7074057e-05 -2.6572132e-05 -2.7641661e-05 -2.9617557e-05 -3.0075109e-05 -3.0055158e-05 -2.8039816e-05 -2.7326747e-05 -2.8141501e-05 -2.9495299e-05][-3.0158157e-05 -2.8294697e-05 -2.8798429e-05 -2.9705738e-05 -3.0316831e-05 -3.0617193e-05 -3.0396499e-05 -3.077061e-05 -3.1851527e-05 -3.1834e-05 -3.1586907e-05 -3.012754e-05 -2.9533414e-05 -2.9878662e-05 -3.0546453e-05][-3.0497904e-05 -2.9352927e-05 -2.9933766e-05 -3.0771869e-05 -3.1590211e-05 -3.2169126e-05 -3.2546995e-05 -3.2760076e-05 -3.3133572e-05 -3.287684e-05 -3.2622887e-05 -3.1443909e-05 -3.0699193e-05 -3.0571482e-05 -3.0726613e-05][-2.9567e-05 -2.9147581e-05 -2.9857485e-05 -3.0581967e-05 -3.1113912e-05 -3.1705022e-05 -3.2493397e-05 -3.3015538e-05 -3.3054053e-05 -3.271781e-05 -3.241545e-05 -3.1654759e-05 -3.1091386e-05 -3.0712974e-05 -3.0579216e-05][-2.9070186e-05 -2.8992003e-05 -2.9765335e-05 -3.0386982e-05 -3.0711584e-05 -3.1144365e-05 -3.1882904e-05 -3.2404947e-05 -3.2375177e-05 -3.2109921e-05 -3.1981104e-05 -3.176208e-05 -3.1568998e-05 -3.1209765e-05 -3.0850417e-05][-2.8532595e-05 -2.8492621e-05 -2.9316976e-05 -2.9871462e-05 -3.0141357e-05 -3.0428851e-05 -3.0780375e-05 -3.102281e-05 -3.097226e-05 -3.0898533e-05 -3.0844265e-05 -3.0917803e-05 -3.0882144e-05 -3.0746054e-05 -3.0495201e-05][-2.8207869e-05 -2.7804635e-05 -2.8464947e-05 -2.8810704e-05 -2.8985462e-05 -2.9152488e-05 -2.9275365e-05 -2.9436596e-05 -2.9563107e-05 -2.9637202e-05 -2.9652558e-05 -2.9766343e-05 -2.9818239e-05 -2.9782324e-05 -2.9610452e-05]]...]
INFO - root - 2017-12-09 08:08:32.723441: step 4710, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.863 sec/batch; 78h:35m:31s remains)
INFO - root - 2017-12-09 08:08:41.327373: step 4720, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.877 sec/batch; 79h:50m:35s remains)
INFO - root - 2017-12-09 08:08:49.789838: step 4730, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.848 sec/batch; 77h:10m:31s remains)
INFO - root - 2017-12-09 08:08:58.357807: step 4740, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.865 sec/batch; 78h:42m:43s remains)
INFO - root - 2017-12-09 08:09:06.721835: step 4750, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.852 sec/batch; 77h:32m:58s remains)
INFO - root - 2017-12-09 08:09:15.298060: step 4760, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.849 sec/batch; 77h:19m:59s remains)
INFO - root - 2017-12-09 08:09:24.032763: step 4770, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.850 sec/batch; 77h:23m:30s remains)
INFO - root - 2017-12-09 08:09:32.781598: step 4780, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.844 sec/batch; 76h:52m:34s remains)
INFO - root - 2017-12-09 08:09:41.483401: step 4790, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.897 sec/batch; 81h:37m:04s remains)
INFO - root - 2017-12-09 08:09:50.046983: step 4800, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.884 sec/batch; 80h:28m:36s remains)
2017-12-09 08:09:51.024744: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.042342905 0.043376703 0.043267936 0.042602617 0.041116871 0.039049078 0.03692814 0.035587598 0.035032339 0.0349516 0.034892704 0.034648579 0.034247231 0.033417787 0.032003812][0.047190242 0.048449602 0.048375644 0.047688439 0.045971878 0.043287192 0.04047893 0.038628951 0.037842441 0.037668992 0.037678629 0.037635926 0.03753582 0.037115112 0.035998832][0.050151844 0.051547196 0.051497728 0.0506364 0.048566375 0.045405857 0.042084076 0.039720565 0.03864371 0.038428482 0.03854654 0.03857699 0.038682014 0.038565759 0.037613682][0.052032027 0.053346351 0.053007975 0.051714819 0.04913611 0.045427475 0.041581728 0.03888151 0.03765763 0.037432548 0.037675451 0.037940428 0.038231283 0.038206406 0.037242062][0.052924972 0.054016884 0.053252205 0.051342074 0.048135925 0.043819752 0.039475854 0.036321312 0.03474218 0.034429982 0.034697264 0.035114627 0.035556383 0.035712264 0.034852933][0.051932815 0.052558679 0.0511411 0.0485768 0.044771474 0.040045708 0.035498917 0.032151997 0.03042119 0.029847017 0.030036317 0.030455686 0.030956864 0.031132486 0.030526426][0.049939808 0.0502829 0.04831161 0.045081276 0.040671691 0.035558481 0.030845681 0.0275063 0.025771726 0.025174821 0.02542844 0.025882607 0.026440343 0.026673308 0.026272113][0.047883045 0.047938369 0.045612466 0.041968226 0.03710188 0.031762265 0.027094351 0.02388549 0.022364778 0.022010621 0.022493821 0.02316534 0.023793912 0.024114773 0.023735816][0.047095954 0.047107439 0.044580337 0.0405542 0.035359208 0.029983332 0.025491854 0.022577297 0.021235846 0.021090612 0.021778917 0.02263386 0.023298532 0.023643935 0.023411348][0.0478906 0.048210725 0.045867525 0.04202763 0.037029218 0.031890079 0.02765562 0.024955777 0.023504712 0.023276264 0.023807216 0.024607908 0.025291625 0.025739385 0.025916411][0.0485584 0.049588077 0.047952428 0.044697694 0.040442295 0.036149114 0.032670408 0.030407265 0.029144941 0.028849958 0.029221835 0.02982199 0.030419393 0.030768694 0.031074053][0.050097492 0.052043214 0.05144015 0.049354486 0.0462098 0.042755753 0.039898407 0.038091619 0.037069187 0.0369329 0.037274368 0.037779588 0.038280532 0.038550545 0.038749721][0.052048661 0.054925155 0.055424377 0.054499466 0.05250879 0.049928047 0.047701132 0.046086963 0.04516378 0.044951316 0.045110658 0.045515161 0.045760378 0.045951262 0.045963876][0.053638816 0.057427108 0.058773652 0.058730166 0.05762529 0.055819485 0.054075897 0.052749593 0.051882029 0.051484004 0.051383369 0.051345855 0.051168531 0.050947882 0.05074979][0.053576443 0.058041211 0.060028825 0.060828812 0.060556803 0.059344329 0.057910983 0.056682367 0.05576577 0.055146981 0.054619074 0.054144435 0.053651232 0.053095456 0.052716464]]...]
INFO - root - 2017-12-09 08:09:59.495460: step 4810, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.844 sec/batch; 76h:51m:36s remains)
INFO - root - 2017-12-09 08:10:08.076714: step 4820, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.852 sec/batch; 77h:31m:31s remains)
INFO - root - 2017-12-09 08:10:16.670306: step 4830, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.846 sec/batch; 77h:00m:54s remains)
INFO - root - 2017-12-09 08:10:25.215813: step 4840, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.850 sec/batch; 77h:21m:23s remains)
INFO - root - 2017-12-09 08:10:33.742445: step 4850, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.844 sec/batch; 76h:50m:07s remains)
INFO - root - 2017-12-09 08:10:42.458312: step 4860, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.875 sec/batch; 79h:38m:42s remains)
INFO - root - 2017-12-09 08:10:51.176826: step 4870, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.873 sec/batch; 79h:29m:25s remains)
INFO - root - 2017-12-09 08:10:59.698550: step 4880, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.846 sec/batch; 77h:01m:04s remains)
INFO - root - 2017-12-09 08:11:08.466122: step 4890, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.888 sec/batch; 80h:50m:06s remains)
INFO - root - 2017-12-09 08:11:17.173702: step 4900, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.852 sec/batch; 77h:34m:36s remains)
2017-12-09 08:11:18.117302: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.5617056e-05 -4.4607059e-05 -4.4548924e-05 -4.4500997e-05 -4.44391e-05 -4.4438511e-05 -4.4423497e-05 -4.4453613e-05 -4.4553253e-05 -4.4692883e-05 -4.4826837e-05 -4.4866279e-05 -4.4858232e-05 -4.4872122e-05 -4.4974062e-05][-4.3987566e-05 -4.3094082e-05 -4.3135009e-05 -4.3153494e-05 -4.3075255e-05 -4.3004609e-05 -4.29335e-05 -4.2959226e-05 -4.3080436e-05 -4.3242264e-05 -4.3376196e-05 -4.3391e-05 -4.339447e-05 -4.3458433e-05 -4.3579879e-05][-4.289467e-05 -4.2189724e-05 -4.2455576e-05 -4.262463e-05 -4.2629883e-05 -4.2606647e-05 -4.2525106e-05 -4.247492e-05 -4.2502594e-05 -4.25505e-05 -4.2546973e-05 -4.2470492e-05 -4.24667e-05 -4.2597552e-05 -4.2784821e-05][-4.2163269e-05 -4.1636973e-05 -4.210819e-05 -4.2477855e-05 -4.2676613e-05 -4.2766831e-05 -4.268506e-05 -4.2509571e-05 -4.2314347e-05 -4.2111704e-05 -4.1913456e-05 -4.1740441e-05 -4.1716339e-05 -4.1923617e-05 -4.2232481e-05][-4.2120766e-05 -4.1663596e-05 -4.2149655e-05 -4.2584474e-05 -4.2850177e-05 -4.2907723e-05 -4.2740019e-05 -4.2455718e-05 -4.2144842e-05 -4.1847907e-05 -4.1649244e-05 -4.1535252e-05 -4.1532905e-05 -4.1733056e-05 -4.206604e-05][-4.2404386e-05 -4.1836698e-05 -4.2170755e-05 -4.2487565e-05 -4.2675892e-05 -4.258129e-05 -4.2301239e-05 -4.2012536e-05 -4.1825959e-05 -4.1683048e-05 -4.1674804e-05 -4.1732732e-05 -4.1770021e-05 -4.1885694e-05 -4.2121046e-05][-4.2633259e-05 -4.1846804e-05 -4.1799842e-05 -4.1823911e-05 -4.1860294e-05 -4.1642492e-05 -4.1329949e-05 -4.1192852e-05 -4.1316907e-05 -4.158607e-05 -4.189121e-05 -4.2110536e-05 -4.2172851e-05 -4.2203741e-05 -4.2370513e-05][-4.2380096e-05 -4.1265554e-05 -4.0864321e-05 -4.0692459e-05 -4.0616869e-05 -4.0375005e-05 -4.0180352e-05 -4.0333161e-05 -4.0788764e-05 -4.1399118e-05 -4.1955725e-05 -4.2309071e-05 -4.2391137e-05 -4.2352189e-05 -4.2427593e-05][-4.1481911e-05 -4.0257491e-05 -3.9779206e-05 -3.9608687e-05 -3.9558425e-05 -3.9430503e-05 -3.9471037e-05 -3.9855986e-05 -4.0399878e-05 -4.1037925e-05 -4.1607167e-05 -4.1988445e-05 -4.2072003e-05 -4.2026993e-05 -4.2063726e-05][-4.0389466e-05 -3.9298302e-05 -3.9028069e-05 -3.8984188e-05 -3.9021754e-05 -3.9027011e-05 -3.9220333e-05 -3.960195e-05 -4.0019971e-05 -4.0425926e-05 -4.0837684e-05 -4.11974e-05 -4.1376094e-05 -4.142024e-05 -4.1456347e-05][-3.9600967e-05 -3.8711991e-05 -3.8663369e-05 -3.8698719e-05 -3.8757109e-05 -3.8886992e-05 -3.91554e-05 -3.9443672e-05 -3.9647308e-05 -3.9803228e-05 -4.0054558e-05 -4.0377385e-05 -4.0641062e-05 -4.0812643e-05 -4.0898143e-05][-3.9440281e-05 -3.8665479e-05 -3.8725175e-05 -3.8788698e-05 -3.8852504e-05 -3.9008417e-05 -3.9248745e-05 -3.9451865e-05 -3.9530958e-05 -3.9567174e-05 -3.9730574e-05 -4.0007602e-05 -4.02857e-05 -4.0491512e-05 -4.0577743e-05][-3.9556024e-05 -3.8792983e-05 -3.8858809e-05 -3.8867976e-05 -3.888203e-05 -3.8982438e-05 -3.9180159e-05 -3.9355225e-05 -3.942856e-05 -3.9471684e-05 -3.966505e-05 -3.9939136e-05 -4.0148909e-05 -4.0293038e-05 -4.0318962e-05][-3.9464663e-05 -3.8748331e-05 -3.8809048e-05 -3.8744824e-05 -3.8657272e-05 -3.8630864e-05 -3.8691716e-05 -3.8794205e-05 -3.8902985e-05 -3.9046539e-05 -3.9309154e-05 -3.957521e-05 -3.9717317e-05 -3.9774917e-05 -3.9722188e-05][-3.9148585e-05 -3.8449307e-05 -3.851435e-05 -3.8461123e-05 -3.8334798e-05 -3.8190276e-05 -3.8130132e-05 -3.8174894e-05 -3.8272359e-05 -3.8420618e-05 -3.8675011e-05 -3.8941642e-05 -3.91003e-05 -3.9219587e-05 -3.9267459e-05]]...]
INFO - root - 2017-12-09 08:11:26.667898: step 4910, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.857 sec/batch; 78h:01m:23s remains)
INFO - root - 2017-12-09 08:11:35.253909: step 4920, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.865 sec/batch; 78h:44m:34s remains)
INFO - root - 2017-12-09 08:11:43.835637: step 4930, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.889 sec/batch; 80h:53m:21s remains)
INFO - root - 2017-12-09 08:11:52.588182: step 4940, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.901 sec/batch; 81h:58m:50s remains)
INFO - root - 2017-12-09 08:12:01.145886: step 4950, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.891 sec/batch; 81h:03m:50s remains)
INFO - root - 2017-12-09 08:12:09.940106: step 4960, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.856 sec/batch; 77h:50m:48s remains)
INFO - root - 2017-12-09 08:12:18.648594: step 4970, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.845 sec/batch; 76h:50m:58s remains)
INFO - root - 2017-12-09 08:12:27.405734: step 4980, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.877 sec/batch; 79h:46m:28s remains)
INFO - root - 2017-12-09 08:12:36.216271: step 4990, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.870 sec/batch; 79h:06m:48s remains)
INFO - root - 2017-12-09 08:12:44.796929: step 5000, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.849 sec/batch; 77h:16m:02s remains)
2017-12-09 08:12:45.691525: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.4421937e-05 -4.359139e-05 -4.3436441e-05 -4.3599852e-05 -4.3911859e-05 -4.4171476e-05 -4.4350523e-05 -4.4996988e-05 -4.6420158e-05 -4.8060898e-05 -4.943191e-05 -5.0701121e-05 -5.1727213e-05 -5.14296e-05 -4.9499991e-05][-4.3883592e-05 -4.3130898e-05 -4.3213859e-05 -4.3638833e-05 -4.441871e-05 -4.5173183e-05 -4.5594312e-05 -4.6407284e-05 -4.7648795e-05 -4.8875238e-05 -4.9821789e-05 -5.0634622e-05 -5.1085441e-05 -5.0228024e-05 -4.7511276e-05][-4.4145352e-05 -4.3414628e-05 -4.3691831e-05 -4.4526289e-05 -4.5729215e-05 -4.6848e-05 -4.7642949e-05 -4.8582551e-05 -4.9544018e-05 -5.0336697e-05 -5.0767732e-05 -5.1134914e-05 -5.1128e-05 -4.9846807e-05 -4.6660098e-05][-4.5487093e-05 -4.4966971e-05 -4.5347031e-05 -4.6321205e-05 -4.7652989e-05 -4.8931244e-05 -4.9809525e-05 -5.0660383e-05 -5.1379342e-05 -5.1833933e-05 -5.178828e-05 -5.1671625e-05 -5.1417323e-05 -4.9744009e-05 -4.6249741e-05][-4.7399761e-05 -4.7388028e-05 -4.7948794e-05 -4.8834623e-05 -5.0092833e-05 -5.1249e-05 -5.1768322e-05 -5.2225849e-05 -5.2739044e-05 -5.3016825e-05 -5.2761825e-05 -5.2441028e-05 -5.1927069e-05 -4.9959279e-05 -4.6176287e-05][-4.9090617e-05 -4.9472965e-05 -4.9967766e-05 -5.0534221e-05 -5.14752e-05 -5.2442159e-05 -5.2525829e-05 -5.2729381e-05 -5.3132084e-05 -5.3513893e-05 -5.358566e-05 -5.3285872e-05 -5.2723608e-05 -5.05287e-05 -4.6448869e-05][-5.06651e-05 -5.1381779e-05 -5.1703319e-05 -5.1518895e-05 -5.1734496e-05 -5.2112267e-05 -5.1560593e-05 -5.1290932e-05 -5.2097457e-05 -5.3319956e-05 -5.411826e-05 -5.4148477e-05 -5.3667653e-05 -5.1182458e-05 -4.6816753e-05][-5.1767915e-05 -5.2664636e-05 -5.2803869e-05 -5.1730636e-05 -5.0985953e-05 -5.03452e-05 -4.9095477e-05 -4.8703507e-05 -5.0324128e-05 -5.2873351e-05 -5.4463715e-05 -5.4603559e-05 -5.3946143e-05 -5.124282e-05 -4.7105354e-05][-5.1946954e-05 -5.2756259e-05 -5.2771124e-05 -5.1392868e-05 -5.0094142e-05 -4.9043436e-05 -4.7800953e-05 -4.7886126e-05 -4.9947055e-05 -5.3155607e-05 -5.4943579e-05 -5.4815144e-05 -5.3636475e-05 -5.0620107e-05 -4.6716395e-05][-5.1511197e-05 -5.1919946e-05 -5.2072428e-05 -5.0964984e-05 -4.9808572e-05 -4.8949973e-05 -4.8172842e-05 -4.8788355e-05 -5.0847633e-05 -5.3734861e-05 -5.5406061e-05 -5.4990494e-05 -5.3460793e-05 -5.0300176e-05 -4.6758658e-05][-5.051199e-05 -5.0619645e-05 -5.1020452e-05 -5.0222439e-05 -4.9411105e-05 -4.9035163e-05 -4.8819584e-05 -4.9536149e-05 -5.1213145e-05 -5.352826e-05 -5.4861313e-05 -5.422673e-05 -5.2665237e-05 -4.9943581e-05 -4.7150388e-05][-4.9772276e-05 -4.9662292e-05 -5.0196941e-05 -4.9601309e-05 -4.9091213e-05 -4.91732e-05 -4.9403428e-05 -5.019522e-05 -5.1503652e-05 -5.3252166e-05 -5.4006945e-05 -5.31943e-05 -5.1752664e-05 -4.9665116e-05 -4.7575439e-05][-4.9413789e-05 -4.8953545e-05 -4.932093e-05 -4.9042508e-05 -4.8870726e-05 -4.9258651e-05 -4.9751572e-05 -5.0706636e-05 -5.1720359e-05 -5.2880579e-05 -5.3267566e-05 -5.2381951e-05 -5.0887873e-05 -4.929924e-05 -4.7830381e-05][-4.8771602e-05 -4.8173562e-05 -4.8521175e-05 -4.8626916e-05 -4.8783339e-05 -4.9241477e-05 -4.9774029e-05 -5.0565355e-05 -5.1161733e-05 -5.1944888e-05 -5.2082236e-05 -5.1270417e-05 -4.9916045e-05 -4.8935304e-05 -4.8017057e-05][-4.7947662e-05 -4.7341706e-05 -4.761029e-05 -4.7824964e-05 -4.8013128e-05 -4.8266153e-05 -4.8614169e-05 -4.9213639e-05 -4.955256e-05 -5.0064318e-05 -5.0230177e-05 -4.9833936e-05 -4.9041118e-05 -4.8527552e-05 -4.8187318e-05]]...]
INFO - root - 2017-12-09 08:12:54.249081: step 5010, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.857 sec/batch; 77h:58m:03s remains)
INFO - root - 2017-12-09 08:13:02.942973: step 5020, loss = 0.82, batch loss = 0.69 (9.9 examples/sec; 0.812 sec/batch; 73h:52m:42s remains)
INFO - root - 2017-12-09 08:13:11.709286: step 5030, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.842 sec/batch; 76h:33m:22s remains)
INFO - root - 2017-12-09 08:13:20.300289: step 5040, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.847 sec/batch; 77h:01m:46s remains)
INFO - root - 2017-12-09 08:13:28.909669: step 5050, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.863 sec/batch; 78h:29m:24s remains)
INFO - root - 2017-12-09 08:13:37.530626: step 5060, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.885 sec/batch; 80h:27m:35s remains)
INFO - root - 2017-12-09 08:13:46.105438: step 5070, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.872 sec/batch; 79h:16m:28s remains)
INFO - root - 2017-12-09 08:13:54.764752: step 5080, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.896 sec/batch; 81h:28m:21s remains)
INFO - root - 2017-12-09 08:14:03.452141: step 5090, loss = 0.82, batch loss = 0.69 (8.5 examples/sec; 0.940 sec/batch; 85h:28m:23s remains)
INFO - root - 2017-12-09 08:14:12.038308: step 5100, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 79h:04m:24s remains)
2017-12-09 08:14:12.857259: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.0889812e-05 -2.9727576e-05 -3.0694646e-05 -3.463246e-05 -3.8819959e-05 -4.2275278e-05 -4.3893284e-05 -4.360218e-05 -4.235806e-05 -4.0850726e-05 -3.9422383e-05 -3.8357874e-05 -3.7837344e-05 -3.7086123e-05 -3.6361635e-05][-8.4592393e-06 -4.9153459e-06 -8.5239444e-06 -1.7611987e-05 -2.7844912e-05 -3.6476686e-05 -4.1516741e-05 -4.3123029e-05 -4.2440239e-05 -4.106079e-05 -3.9654053e-05 -3.84069e-05 -3.7541246e-05 -3.6591468e-05 -3.5751975e-05][2.7821305e-05 3.7748323e-05 2.9714967e-05 1.3230536e-05 -6.4718261e-06 -2.3948189e-05 -3.58892e-05 -4.1138461e-05 -4.2282616e-05 -4.14347e-05 -3.9828141e-05 -3.8568411e-05 -3.77407e-05 -3.6586986e-05 -3.5654772e-05][7.617996e-05 9.6509015e-05 8.5582535e-05 6.1141414e-05 2.824542e-05 -2.5539121e-06 -2.5855094e-05 -3.7540507e-05 -4.1474363e-05 -4.1861858e-05 -4.0294293e-05 -3.9131359e-05 -3.8589922e-05 -3.7463684e-05 -3.630573e-05][0.0001262392 0.00015945639 0.00014997997 0.00011983648 7.3652365e-05 2.7619972e-05 -1.0913987e-05 -3.1664913e-05 -3.91836e-05 -4.082104e-05 -3.9684244e-05 -3.8586473e-05 -3.8374506e-05 -3.7794431e-05 -3.6847421e-05][0.00016341207 0.00021049392 0.00020816296 0.00017869254 0.00012318583 6.2379666e-05 7.6031065e-06 -2.4116198e-05 -3.6600995e-05 -3.9697545e-05 -3.8775976e-05 -3.8202663e-05 -3.8583417e-05 -3.859818e-05 -3.76084e-05][0.00017654852 0.00023568142 0.00024524372 0.00022270935 0.00016657196 9.8043252e-05 3.0245428e-05 -1.3973302e-05 -3.3390734e-05 -3.8986338e-05 -3.8314058e-05 -3.7741775e-05 -3.829769e-05 -3.8795188e-05 -3.8069691e-05][0.00016635508 0.00023385833 0.0002562447 0.00024356379 0.00019456941 0.00012695136 5.4090007e-05 6.1598985e-07 -2.7119044e-05 -3.7284313e-05 -3.7458125e-05 -3.6944049e-05 -3.7544985e-05 -3.8524126e-05 -3.8192455e-05][0.00013944467 0.00021082244 0.00024285555 0.00023955433 0.00020160363 0.00014279984 7.3053758e-05 1.5383834e-05 -1.9483996e-05 -3.4538691e-05 -3.646269e-05 -3.6728827e-05 -3.7325441e-05 -3.8302947e-05 -3.8539562e-05][0.00010797533 0.0001800124 0.00021614018 0.00021684956 0.00018654895 0.00013864825 7.8231533e-05 2.1886583e-05 -1.6329337e-05 -3.3360549e-05 -3.5644487e-05 -3.6459685e-05 -3.7371603e-05 -3.8204653e-05 -3.8782106e-05][7.9028556e-05 0.00014703537 0.00018277872 0.00018265605 0.00015463415 0.00011516945 6.5077475e-05 1.530593e-05 -2.0434083e-05 -3.5097513e-05 -3.7358193e-05 -3.8246864e-05 -3.8870341e-05 -3.9374525e-05 -3.9745439e-05][5.1187919e-05 0.00010848428 0.0001401998 0.00013849657 0.00011237629 7.916108e-05 3.8416416e-05 -6.2165054e-07 -2.8388982e-05 -3.7797316e-05 -3.9295672e-05 -3.96872e-05 -4.0074017e-05 -4.02317e-05 -4.0497154e-05][2.1551576e-05 6.37543e-05 8.9106456e-05 8.832349e-05 6.6647641e-05 3.9837265e-05 9.7003358e-06 -1.7028731e-05 -3.4049059e-05 -3.8756578e-05 -3.978124e-05 -4.0074126e-05 -4.0485611e-05 -4.083497e-05 -4.1100982e-05][-6.8269073e-06 1.9837469e-05 3.7324106e-05 3.8576763e-05 2.3912326e-05 4.8437214e-06 -1.4491387e-05 -2.9868213e-05 -3.7790684e-05 -3.95936e-05 -4.0020932e-05 -4.0157189e-05 -4.0716161e-05 -4.1103052e-05 -4.1428772e-05][-2.6896018e-05 -1.2635581e-05 -2.6044145e-06 -8.9553942e-07 -8.7079679e-06 -2.0001593e-05 -3.0408632e-05 -3.6940579e-05 -3.9769307e-05 -4.039715e-05 -4.0552353e-05 -4.0529176e-05 -4.0913244e-05 -4.1265383e-05 -4.1651187e-05]]...]
INFO - root - 2017-12-09 08:14:21.411926: step 5110, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.870 sec/batch; 79h:06m:49s remains)
INFO - root - 2017-12-09 08:14:29.864281: step 5120, loss = 0.82, batch loss = 0.69 (11.1 examples/sec; 0.720 sec/batch; 65h:28m:08s remains)
INFO - root - 2017-12-09 08:14:38.624834: step 5130, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.863 sec/batch; 78h:27m:50s remains)
INFO - root - 2017-12-09 08:14:47.257392: step 5140, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.865 sec/batch; 78h:40m:24s remains)
INFO - root - 2017-12-09 08:14:55.735868: step 5150, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.846 sec/batch; 76h:53m:30s remains)
INFO - root - 2017-12-09 08:15:04.210766: step 5160, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.842 sec/batch; 76h:31m:31s remains)
INFO - root - 2017-12-09 08:15:12.803168: step 5170, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.870 sec/batch; 79h:03m:52s remains)
INFO - root - 2017-12-09 08:15:21.456674: step 5180, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.850 sec/batch; 77h:14m:47s remains)
INFO - root - 2017-12-09 08:15:30.010850: step 5190, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.848 sec/batch; 77h:07m:59s remains)
INFO - root - 2017-12-09 08:15:38.601174: step 5200, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.874 sec/batch; 79h:25m:44s remains)
2017-12-09 08:15:39.432803: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.13603804 0.13710248 0.13537581 0.13320236 0.13026692 0.12692648 0.12403377 0.12009783 0.11645912 0.11320073 0.1101165 0.10724656 0.10390693 0.10163026 0.099534504][0.13928084 0.14055775 0.13901967 0.13704868 0.1343201 0.13125262 0.12811323 0.12405188 0.12040145 0.11713919 0.11421947 0.11119575 0.10800501 0.10564354 0.10326461][0.14055072 0.14171684 0.14010717 0.13810441 0.13545726 0.13272454 0.12942782 0.12544738 0.12165957 0.11836643 0.1153628 0.11207958 0.10903645 0.10665359 0.1043756][0.14196807 0.14327833 0.14182559 0.13999422 0.13748655 0.13485521 0.13151792 0.12780178 0.12388668 0.12029703 0.11695305 0.11357211 0.11052588 0.10777614 0.1053274][0.14331509 0.14494002 0.14371373 0.14203447 0.14003958 0.13769314 0.13441716 0.13066985 0.12650302 0.12258063 0.11857265 0.1148691 0.11164261 0.10876765 0.1063567][0.14338364 0.14526027 0.14430848 0.14277941 0.14099437 0.13913229 0.13652474 0.13311873 0.12898454 0.12484713 0.12039953 0.11607043 0.11227066 0.10917097 0.10656417][0.14151871 0.14351963 0.14282951 0.14178848 0.14066897 0.13939029 0.13741693 0.13466646 0.13082491 0.12664816 0.12187729 0.11713442 0.1129898 0.10960101 0.10684991][0.1387866 0.14052981 0.139601 0.13875654 0.13788591 0.13702725 0.13582815 0.13390954 0.13078949 0.12684159 0.12215136 0.11745594 0.11303888 0.1094337 0.10649338][0.13300353 0.13539533 0.13514349 0.13492705 0.13475125 0.1345378 0.13409492 0.1326974 0.13016933 0.12656184 0.12218558 0.11784125 0.11341794 0.10975238 0.10657069][0.12644979 0.12893949 0.12907232 0.12943222 0.12989235 0.13036565 0.13055904 0.12979738 0.12795471 0.1249911 0.12126675 0.11727633 0.11306119 0.10967015 0.10650179][0.11885367 0.12171291 0.12237901 0.1232634 0.12413 0.1252804 0.1262174 0.12622188 0.12522048 0.12309499 0.12031738 0.11677122 0.11278825 0.10936489 0.10604555][0.11144999 0.1146147 0.11575435 0.11714403 0.1187692 0.12034423 0.12160264 0.12247533 0.12251227 0.12140158 0.11943521 0.11655417 0.11312764 0.10961276 0.10614163][0.1034805 0.10682496 0.10844667 0.11001103 0.11187904 0.11391459 0.1159239 0.11767263 0.11879092 0.11917064 0.1185965 0.11662424 0.11368467 0.1102659 0.10665409][0.095537908 0.098995395 0.10093227 0.10279956 0.10510852 0.10773426 0.11061985 0.11334544 0.11557009 0.11731642 0.11798713 0.11693967 0.1145177 0.11128818 0.10768681][0.089183554 0.092022672 0.093475312 0.095203646 0.097571321 0.10058054 0.10430241 0.10830416 0.11194044 0.11512475 0.11713549 0.1170164 0.11512972 0.11206614 0.10849303]]...]
INFO - root - 2017-12-09 08:15:47.979202: step 5210, loss = 0.82, batch loss = 0.69 (11.4 examples/sec; 0.700 sec/batch; 63h:36m:22s remains)
INFO - root - 2017-12-09 08:15:56.466052: step 5220, loss = 0.82, batch loss = 0.69 (10.9 examples/sec; 0.737 sec/batch; 66h:57m:34s remains)
INFO - root - 2017-12-09 08:16:05.034724: step 5230, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.870 sec/batch; 79h:04m:36s remains)
INFO - root - 2017-12-09 08:16:13.503630: step 5240, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.840 sec/batch; 76h:23m:13s remains)
INFO - root - 2017-12-09 08:16:21.975062: step 5250, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 78h:47m:14s remains)
INFO - root - 2017-12-09 08:16:30.834255: step 5260, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.878 sec/batch; 79h:48m:42s remains)
INFO - root - 2017-12-09 08:16:39.389308: step 5270, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.845 sec/batch; 76h:47m:05s remains)
INFO - root - 2017-12-09 08:16:48.095789: step 5280, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.837 sec/batch; 76h:07m:24s remains)
INFO - root - 2017-12-09 08:16:56.795121: step 5290, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.883 sec/batch; 80h:17m:48s remains)
INFO - root - 2017-12-09 08:17:05.535249: step 5300, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.902 sec/batch; 81h:58m:31s remains)
2017-12-09 08:17:06.399409: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.1905743e-05 -5.2436513e-05 -5.3569896e-05 -5.4676137e-05 -5.5758563e-05 -5.6263983e-05 -5.61504e-05 -5.517454e-05 -5.390884e-05 -5.2639905e-05 -5.1123588e-05 -4.9696795e-05 -4.8681286e-05 -4.8031885e-05 -4.7587313e-05][-5.381586e-05 -5.4304692e-05 -5.5017757e-05 -5.5375182e-05 -5.5730281e-05 -5.5934623e-05 -5.5416906e-05 -5.4320524e-05 -5.3402211e-05 -5.2691397e-05 -5.1530027e-05 -4.993644e-05 -4.8724076e-05 -4.8012451e-05 -4.754393e-05][-5.5662545e-05 -5.6093239e-05 -5.6251305e-05 -5.5771703e-05 -5.5484426e-05 -5.5263521e-05 -5.4170981e-05 -5.2706291e-05 -5.2047646e-05 -5.2016174e-05 -5.1330931e-05 -4.9977574e-05 -4.8797385e-05 -4.8067152e-05 -4.7565951e-05][-5.6343146e-05 -5.638495e-05 -5.6078854e-05 -5.517661e-05 -5.4267628e-05 -5.3668693e-05 -5.2301923e-05 -5.0705163e-05 -4.9919559e-05 -4.9964077e-05 -4.97594e-05 -4.9029288e-05 -4.8193815e-05 -4.7503385e-05 -4.7038175e-05][-5.5897057e-05 -5.5285553e-05 -5.4442684e-05 -5.3006253e-05 -5.1455423e-05 -5.0285562e-05 -4.8549591e-05 -4.7076734e-05 -4.6626483e-05 -4.700249e-05 -4.7270365e-05 -4.7119564e-05 -4.6806726e-05 -4.6460667e-05 -4.6195313e-05][-5.4943928e-05 -5.3677588e-05 -5.2339819e-05 -5.0436385e-05 -4.8133581e-05 -4.5916237e-05 -4.3691234e-05 -4.2669421e-05 -4.3111606e-05 -4.4206288e-05 -4.5241519e-05 -4.5569515e-05 -4.5527406e-05 -4.5425877e-05 -4.5469369e-05][-5.36125e-05 -5.2111714e-05 -5.0593193e-05 -4.8376507e-05 -4.5241086e-05 -4.1316282e-05 -3.8654645e-05 -3.8710728e-05 -4.0422121e-05 -4.2251191e-05 -4.3837103e-05 -4.459151e-05 -4.4753513e-05 -4.4769062e-05 -4.4910244e-05][-5.2337917e-05 -5.0794741e-05 -4.9278147e-05 -4.70694e-05 -4.4122589e-05 -3.97119e-05 -3.6691716e-05 -3.7360245e-05 -3.9869417e-05 -4.1951975e-05 -4.3421467e-05 -4.4251745e-05 -4.4558696e-05 -4.46745e-05 -4.481299e-05][-5.1431885e-05 -4.9770348e-05 -4.8328962e-05 -4.6694822e-05 -4.47447e-05 -4.2141532e-05 -4.01758e-05 -4.030751e-05 -4.1616087e-05 -4.2821648e-05 -4.3815307e-05 -4.4363023e-05 -4.46994e-05 -4.4940232e-05 -4.5314246e-05][-5.0460563e-05 -4.8653252e-05 -4.7427533e-05 -4.6304252e-05 -4.5457382e-05 -4.4528573e-05 -4.3829434e-05 -4.3636574e-05 -4.3827793e-05 -4.404438e-05 -4.4373606e-05 -4.4760323e-05 -4.5253291e-05 -4.5863584e-05 -4.6844736e-05][-4.9414808e-05 -4.77166e-05 -4.6661989e-05 -4.5917415e-05 -4.5611203e-05 -4.5397042e-05 -4.5236913e-05 -4.5134795e-05 -4.5132376e-05 -4.5223231e-05 -4.5514313e-05 -4.6065874e-05 -4.7041965e-05 -4.8311671e-05 -4.9860733e-05][-4.8304126e-05 -4.6974241e-05 -4.6206573e-05 -4.5699118e-05 -4.5561872e-05 -4.5558045e-05 -4.5563113e-05 -4.560973e-05 -4.5771209e-05 -4.6153469e-05 -4.6963967e-05 -4.8194393e-05 -4.9823764e-05 -5.1550905e-05 -5.3175092e-05][-4.7087004e-05 -4.6040448e-05 -4.5662739e-05 -4.5401121e-05 -4.5367462e-05 -4.5505494e-05 -4.5673038e-05 -4.6004705e-05 -4.6580208e-05 -4.7584464e-05 -4.9179216e-05 -5.102373e-05 -5.3004424e-05 -5.4718512e-05 -5.5897141e-05][-4.6435824e-05 -4.5308931e-05 -4.5075249e-05 -4.4965003e-05 -4.500728e-05 -4.527417e-05 -4.5714987e-05 -4.6506539e-05 -4.7870952e-05 -4.9731323e-05 -5.1943771e-05 -5.4143427e-05 -5.60637e-05 -5.7169429e-05 -5.7269965e-05][-4.6091191e-05 -4.4972538e-05 -4.4728233e-05 -4.4651628e-05 -4.4773027e-05 -4.5192421e-05 -4.5982091e-05 -4.747833e-05 -4.9691513e-05 -5.2109081e-05 -5.455154e-05 -5.6694578e-05 -5.8016889e-05 -5.7928155e-05 -5.6374825e-05]]...]
INFO - root - 2017-12-09 08:17:15.094820: step 5310, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.888 sec/batch; 80h:40m:37s remains)
INFO - root - 2017-12-09 08:17:23.429411: step 5320, loss = 0.82, batch loss = 0.69 (10.6 examples/sec; 0.753 sec/batch; 68h:23m:42s remains)
INFO - root - 2017-12-09 08:17:32.298317: step 5330, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.844 sec/batch; 76h:40m:54s remains)
INFO - root - 2017-12-09 08:17:40.930073: step 5340, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.848 sec/batch; 77h:06m:12s remains)
INFO - root - 2017-12-09 08:17:49.397882: step 5350, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.830 sec/batch; 75h:24m:09s remains)
INFO - root - 2017-12-09 08:17:57.901666: step 5360, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 78h:59m:36s remains)
INFO - root - 2017-12-09 08:18:06.407741: step 5370, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 78h:45m:27s remains)
INFO - root - 2017-12-09 08:18:15.047504: step 5380, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 78h:58m:02s remains)
INFO - root - 2017-12-09 08:18:23.818072: step 5390, loss = 0.82, batch loss = 0.69 (8.2 examples/sec; 0.970 sec/batch; 88h:10m:18s remains)
INFO - root - 2017-12-09 08:18:32.441855: step 5400, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.860 sec/batch; 78h:06m:29s remains)
2017-12-09 08:18:33.432523: I tensorflow/core/kernels/logging_ops.cc:79] [[[-6.0255668e-05 -5.949667e-05 -6.0243776e-05 -6.0956561e-05 -6.148305e-05 -6.1788494e-05 -6.1796345e-05 -6.1778213e-05 -6.16736e-05 -6.159136e-05 -6.142263e-05 -6.0961469e-05 -6.0422768e-05 -5.9871345e-05 -5.9357084e-05][-5.88161e-05 -5.7904952e-05 -5.8634327e-05 -5.939963e-05 -5.9674916e-05 -5.9900038e-05 -5.9573889e-05 -5.9162208e-05 -5.9407914e-05 -6.0146307e-05 -6.0581559e-05 -6.0436469e-05 -6.0100763e-05 -5.9604885e-05 -5.9047525e-05][-5.7592195e-05 -5.5716362e-05 -5.5414261e-05 -5.501122e-05 -5.3729389e-05 -5.2669813e-05 -5.1055482e-05 -5.0199542e-05 -5.1862884e-05 -5.5159308e-05 -5.777453e-05 -5.9100304e-05 -5.9832091e-05 -5.9936807e-05 -5.955113e-05][-5.5850644e-05 -5.2148862e-05 -4.934036e-05 -4.5219243e-05 -3.9404877e-05 -3.3393091e-05 -2.8324146e-05 -2.7025315e-05 -3.2204636e-05 -4.1370731e-05 -4.9912112e-05 -5.5350061e-05 -5.852247e-05 -5.9983267e-05 -6.0274175e-05][-5.4790165e-05 -4.9640013e-05 -4.3669592e-05 -3.3522196e-05 -1.9025865e-05 -3.1596646e-06 9.977608e-06 1.3479366e-05 3.2597818e-06 -1.5198959e-05 -3.3670549e-05 -4.6982379e-05 -5.48607e-05 -5.9188344e-05 -6.0720155e-05][-5.4518874e-05 -4.8973721e-05 -4.011839e-05 -2.3819593e-05 1.4972247e-06 3.161835e-05 5.8317295e-05 6.79731e-05 5.3825483e-05 2.3457571e-05 -8.9543028e-06 -3.3965505e-05 -4.9149508e-05 -5.710643e-05 -6.0087994e-05][-5.4966378e-05 -4.952681e-05 -3.9477036e-05 -1.9552994e-05 1.4223922e-05 5.8346166e-05 0.00010088339 0.00012132127 0.00010798995 6.8857975e-05 2.2767832e-05 -1.5481477e-05 -4.0012404e-05 -5.3416115e-05 -5.87666e-05][-5.4852877e-05 -5.023017e-05 -4.1636544e-05 -2.317466e-05 1.1212069e-05 6.0884282e-05 0.00011339274 0.0001445722 0.00013930043 0.00010252219 5.2061572e-05 5.0479648e-06 -2.7682876e-05 -4.7075133e-05 -5.6184632e-05][-5.366774e-05 -4.9943981e-05 -4.4275017e-05 -3.1294534e-05 -5.4524062e-06 3.5328791e-05 8.4438827e-05 0.00012105267 0.00012753063 0.0001038824 6.268562e-05 1.7971252e-05 -1.7461512e-05 -4.0356139e-05 -5.2204156e-05][-5.2651147e-05 -4.9394566e-05 -4.6362857e-05 -3.9832288e-05 -2.5576053e-05 -1.130953e-06 3.2703749e-05 6.419346e-05 7.952447e-05 7.2447758e-05 4.8754824e-05 1.6302954e-05 -1.4080113e-05 -3.6330184e-05 -4.8958362e-05][-5.255625e-05 -4.9589511e-05 -4.8129703e-05 -4.6161353e-05 -4.0776871e-05 -2.9992607e-05 -1.3081713e-05 6.096423e-06 2.1174717e-05 2.482842e-05 1.7221661e-05 3.8815779e-07 -1.9302897e-05 -3.6433597e-05 -4.7667334e-05][-5.3061558e-05 -5.0487964e-05 -5.0085117e-05 -5.0266357e-05 -4.9628557e-05 -4.6481142e-05 -4.0588417e-05 -3.2384538e-05 -2.3639135e-05 -1.7930943e-05 -1.7220373e-05 -2.2041539e-05 -3.0823816e-05 -4.0886473e-05 -4.8928454e-05][-5.3865286e-05 -5.1711064e-05 -5.1800384e-05 -5.2400217e-05 -5.2940086e-05 -5.2602314e-05 -5.1835923e-05 -4.9887167e-05 -4.6700727e-05 -4.3404842e-05 -4.1517469e-05 -4.1479267e-05 -4.3264394e-05 -4.7276968e-05 -5.1614279e-05][-5.5537657e-05 -5.3842803e-05 -5.4016436e-05 -5.4487136e-05 -5.4868142e-05 -5.4785804e-05 -5.5043507e-05 -5.4995406e-05 -5.4323697e-05 -5.3525146e-05 -5.2962238e-05 -5.2521165e-05 -5.232578e-05 -5.3431075e-05 -5.5027307e-05][-5.6994126e-05 -5.5617118e-05 -5.5711036e-05 -5.5923865e-05 -5.6202338e-05 -5.6172285e-05 -5.6266221e-05 -5.6426605e-05 -5.6341618e-05 -5.61371e-05 -5.6177367e-05 -5.6285662e-05 -5.6398854e-05 -5.6959307e-05 -5.750218e-05]]...]
INFO - root - 2017-12-09 08:18:42.025618: step 5410, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.892 sec/batch; 81h:02m:19s remains)
INFO - root - 2017-12-09 08:18:50.382238: step 5420, loss = 0.82, batch loss = 0.69 (10.5 examples/sec; 0.760 sec/batch; 69h:05m:33s remains)
INFO - root - 2017-12-09 08:18:59.081115: step 5430, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 78h:53m:23s remains)
INFO - root - 2017-12-09 08:19:07.937837: step 5440, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.888 sec/batch; 80h:39m:51s remains)
INFO - root - 2017-12-09 08:19:16.585252: step 5450, loss = 0.82, batch loss = 0.69 (10.8 examples/sec; 0.738 sec/batch; 67h:02m:00s remains)
INFO - root - 2017-12-09 08:19:25.157510: step 5460, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.833 sec/batch; 75h:38m:31s remains)
INFO - root - 2017-12-09 08:19:33.747505: step 5470, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.838 sec/batch; 76h:07m:17s remains)
INFO - root - 2017-12-09 08:19:42.397875: step 5480, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.854 sec/batch; 77h:36m:38s remains)
INFO - root - 2017-12-09 08:19:51.141383: step 5490, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.902 sec/batch; 81h:58m:27s remains)
INFO - root - 2017-12-09 08:19:59.922200: step 5500, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.833 sec/batch; 75h:39m:51s remains)
2017-12-09 08:20:00.811225: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.00058099214 0.00044176722 0.00033380304 0.00029695139 0.00033203111 0.00045347743 0.00061688188 0.00077410816 0.00091047247 0.0010010217 0.0010095936 0.00091248727 0.00073272944 0.00052242266 0.00032187536][0.0012844637 0.0011701676 0.001065842 0.0010583935 0.0011667085 0.0013639267 0.0015566862 0.0016845678 0.0017582623 0.0017522437 0.0016348603 0.0014129921 0.001125767 0.00082055916 0.00052525883][0.0016351194 0.001636726 0.0017214215 0.00202439 0.0025225785 0.0030751768 0.0034196661 0.0034459953 0.0032491556 0.0028705476 0.0023496468 0.0017801614 0.0012647432 0.00084058236 0.00049551786][0.0021454184 0.0022988052 0.0026238034 0.0033206353 0.004347668 0.0054405592 0.006151828 0.0062181409 0.005726649 0.0048147305 0.0036788157 0.0025434701 0.0015972205 0.000914383 0.0004588486][0.0027706027 0.0033374827 0.0042327051 0.00563611 0.0074123149 0.0091537572 0.010267118 0.010402864 0.0096350834 0.0081713609 0.0063436748 0.0045001414 0.0029194574 0.0017291771 0.00091084861][0.0041210973 0.0052787536 0.0069085006 0.0091358358 0.011723177 0.014148687 0.015739679 0.016054662 0.015140121 0.013264988 0.010820064 0.0082306946 0.0058175023 0.0038055677 0.0022696564][0.006563927 0.008458579 0.010825171 0.01369333 0.016782578 0.019569149 0.02142553 0.021882391 0.020945001 0.018892022 0.016097257 0.012952963 0.0097709969 0.006848746 0.0043980763][0.0095487386 0.011977153 0.01474941 0.01784312 0.020997984 0.023754774 0.025582014 0.026056089 0.025205277 0.023259904 0.020466805 0.017121376 0.013493853 0.0098999515 0.0066550761][0.012266595 0.014843188 0.017541479 0.020368882 0.023138966 0.025521925 0.027142601 0.027640242 0.026996126 0.02534906 0.022868278 0.019709103 0.016040398 0.0121523 0.0084461309][0.013944385 0.016436942 0.018851131 0.021221165 0.023443883 0.02532422 0.026641343 0.027127726 0.026736122 0.025505265 0.023491926 0.020728769 0.017292889 0.013411472 0.009532935][0.014398707 0.016696548 0.018751744 0.020623215 0.022279229 0.023641879 0.024620339 0.025037339 0.02483524 0.023982894 0.022463135 0.020173024 0.017104635 0.013439222 0.0096439943][0.013330878 0.0153533 0.017042836 0.018463217 0.019637719 0.020558458 0.021197185 0.02145081 0.021280885 0.020641327 0.019492742 0.017662492 0.015078485 0.011875077 0.008499179][0.010719907 0.012298331 0.013563859 0.014577821 0.015380462 0.015987094 0.016384419 0.016501214 0.016314398 0.015793264 0.014912384 0.013500558 0.011474841 0.0089484258 0.0063038012][0.0073590185 0.0083844205 0.0091810348 0.0098090414 0.010306318 0.010682506 0.010922492 0.010972677 0.010813585 0.010424502 0.0097976634 0.0088099567 0.0074030859 0.00566857 0.0038865055][0.00426946 0.0048288577 0.0052553611 0.005598837 0.0058800848 0.0060946424 0.0062213619 0.0062255645 0.0061014849 0.0058442866 0.0054530231 0.0048536472 0.0040153665 0.0030002135 0.0019828933]]...]
INFO - root - 2017-12-09 08:20:09.539836: step 5510, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.847 sec/batch; 76h:57m:52s remains)
INFO - root - 2017-12-09 08:20:18.052312: step 5520, loss = 0.82, batch loss = 0.69 (10.1 examples/sec; 0.794 sec/batch; 72h:06m:54s remains)
INFO - root - 2017-12-09 08:20:26.951384: step 5530, loss = 0.82, batch loss = 0.69 (8.8 examples/sec; 0.907 sec/batch; 82h:20m:21s remains)
INFO - root - 2017-12-09 08:20:35.549490: step 5540, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.879 sec/batch; 79h:51m:14s remains)
INFO - root - 2017-12-09 08:20:44.414819: step 5550, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.860 sec/batch; 78h:06m:43s remains)
INFO - root - 2017-12-09 08:20:52.842924: step 5560, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.846 sec/batch; 76h:51m:09s remains)
INFO - root - 2017-12-09 08:21:01.450752: step 5570, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.860 sec/batch; 78h:06m:10s remains)
INFO - root - 2017-12-09 08:21:09.978776: step 5580, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.845 sec/batch; 76h:45m:15s remains)
INFO - root - 2017-12-09 08:21:18.651541: step 5590, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.863 sec/batch; 78h:21m:45s remains)
INFO - root - 2017-12-09 08:21:27.488247: step 5600, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.889 sec/batch; 80h:44m:04s remains)
2017-12-09 08:21:28.320889: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.024983311 0.025876636 0.026340356 0.026717726 0.026977515 0.02707845 0.027018569 0.026717527 0.026176661 0.025397517 0.024342965 0.023189176 0.021777742 0.020161696 0.018375097][0.027025072 0.028133545 0.028760862 0.029248822 0.029580679 0.02972419 0.029687431 0.029431814 0.028870104 0.028105158 0.026981708 0.025699876 0.024160253 0.022379233 0.020384084][0.028598875 0.029913133 0.030703191 0.031277 0.031636432 0.031782042 0.031747609 0.031511579 0.030974451 0.030276814 0.029176833 0.027854158 0.026229037 0.024334496 0.022173284][0.029773338 0.03125428 0.032142695 0.032730937 0.033043604 0.033138338 0.033089168 0.032882281 0.032443874 0.031884268 0.030890005 0.029639162 0.028024629 0.026121963 0.023844784][0.030639159 0.032219857 0.03311241 0.033624083 0.033804368 0.033800136 0.033689089 0.033517722 0.03325054 0.032925449 0.032193959 0.031083724 0.029564032 0.027628375 0.025275055][0.031177105 0.032812953 0.03363071 0.033963565 0.033935297 0.033750813 0.033542752 0.033408917 0.033356056 0.03336636 0.033001225 0.032146558 0.030798851 0.028920498 0.026552405][0.031223429 0.032914631 0.033658769 0.033805247 0.0335236 0.033103943 0.032745678 0.032657672 0.032820284 0.033198424 0.033261478 0.032766216 0.031681672 0.029907616 0.027562259][0.030827291 0.032471072 0.033116054 0.03310214 0.032606129 0.03199039 0.031523731 0.031472426 0.031855598 0.032540217 0.033006947 0.032846879 0.031993344 0.030401684 0.028088685][0.030028567 0.031623133 0.032156166 0.031994261 0.031331077 0.030575464 0.030046696 0.030059755 0.030593878 0.03149407 0.032245539 0.032398447 0.03178836 0.030343276 0.028098255][0.029009711 0.030540934 0.031002134 0.030768514 0.03004903 0.029213257 0.028649537 0.02868619 0.029271195 0.030242827 0.031146972 0.031465277 0.030995382 0.029672822 0.027539987][0.027511453 0.028968517 0.029414522 0.029214254 0.028549688 0.02774143 0.027243203 0.027337583 0.027941233 0.028846953 0.029689943 0.03004946 0.029607443 0.028362915 0.026356615][0.025598278 0.026966956 0.027429115 0.027312845 0.026744638 0.026052993 0.025652956 0.025795575 0.026357865 0.027140602 0.027896482 0.028177233 0.027711852 0.026526591 0.024675138][0.023496546 0.024724379 0.025171407 0.025147811 0.024749115 0.024234541 0.023959147 0.024104536 0.024582632 0.025191648 0.025744287 0.025878586 0.025359318 0.024250684 0.022578023][0.021166887 0.022244707 0.022677591 0.022767993 0.022561533 0.022246895 0.022091877 0.022202726 0.022533266 0.022926642 0.023256483 0.023271743 0.022745054 0.02174826 0.02033937][0.018668571 0.019593026 0.020006068 0.020215627 0.020195611 0.020051457 0.01998567 0.020038864 0.020237187 0.020423947 0.020555738 0.020451522 0.019955579 0.019152956 0.018041955]]...]
INFO - root - 2017-12-09 08:21:36.986842: step 5610, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.870 sec/batch; 79h:02m:12s remains)
INFO - root - 2017-12-09 08:21:45.313806: step 5620, loss = 0.82, batch loss = 0.69 (10.1 examples/sec; 0.794 sec/batch; 72h:05m:07s remains)
INFO - root - 2017-12-09 08:21:54.061047: step 5630, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.878 sec/batch; 79h:41m:38s remains)
INFO - root - 2017-12-09 08:22:02.864952: step 5640, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.893 sec/batch; 81h:04m:29s remains)
INFO - root - 2017-12-09 08:22:11.592490: step 5650, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.890 sec/batch; 80h:46m:58s remains)
INFO - root - 2017-12-09 08:22:20.300907: step 5660, loss = 0.82, batch loss = 0.69 (9.8 examples/sec; 0.819 sec/batch; 74h:21m:04s remains)
INFO - root - 2017-12-09 08:22:29.201014: step 5670, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.892 sec/batch; 81h:00m:01s remains)
INFO - root - 2017-12-09 08:22:37.939456: step 5680, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 79h:05m:53s remains)
INFO - root - 2017-12-09 08:22:46.491408: step 5690, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.853 sec/batch; 77h:24m:15s remains)
INFO - root - 2017-12-09 08:22:55.031651: step 5700, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.858 sec/batch; 77h:54m:41s remains)
2017-12-09 08:22:55.860936: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.022377452 0.020679828 0.017919721 0.014847755 0.01167401 0.0085371444 0.0055393027 0.0030205576 0.0012686397 0.00037526857 5.8477031e-05 -1.9804429e-06 -4.5022025e-06 1.5722253e-06 5.0932023e-05][0.023495328 0.021476861 0.018294638 0.014846575 0.011427533 0.0082183443 0.0052160439 0.0027536745 0.0010878126 0.00029239093 3.369055e-05 -1.1541983e-05 -1.5561011e-05 -1.3540652e-05 1.6415695e-05][0.023802623 0.021518974 0.018027591 0.014337434 0.010804812 0.0076523377 0.0047804639 0.0024557025 0.00090982928 0.00021248398 9.2221308e-06 -2.0665786e-05 -2.5365254e-05 -2.8210547e-05 -1.8198873e-05][0.024313534 0.021775054 0.018047977 0.014177524 0.010553483 0.0074440273 0.0046584266 0.0024119252 0.00090439338 0.00021824037 1.2169461e-05 -2.1230604e-05 -2.7912523e-05 -3.2897377e-05 -3.3094308e-05][0.024957716 0.022282187 0.018505622 0.014607358 0.010964285 0.0078608952 0.0050701159 0.0027734493 0.0011471593 0.00033593047 4.9549679e-05 -1.5791084e-05 -2.7336981e-05 -3.3389e-05 -3.7146474e-05][0.025730008 0.023052156 0.019372122 0.01556861 0.011952586 0.0087938616 0.0058898004 0.0034288801 0.001575432 0.00054637349 0.00011463986 -6.1453829e-06 -2.4993322e-05 -3.1449221e-05 -3.6562826e-05][0.026427 0.023930686 0.020491233 0.016875053 0.013307097 0.010054288 0.0069567082 0.0042359317 0.0020715785 0.00077276432 0.00017869053 2.8596914e-06 -2.2244356e-05 -2.9888055e-05 -3.5179997e-05][0.026906848 0.024660351 0.021487 0.01805724 0.014528083 0.011161625 0.0078629125 0.0049075265 0.0024829856 0.00095930404 0.00022899115 8.9219611e-06 -2.0453357e-05 -2.8323077e-05 -3.3430006e-05][0.02704726 0.025042728 0.022059048 0.018743342 0.01521313 0.011744627 0.0083011268 0.0052123563 0.0026701204 0.0010484549 0.00025676785 1.2239077e-05 -2.0330808e-05 -2.8319264e-05 -3.2996082e-05][0.026773872 0.024876174 0.021909744 0.018587295 0.01500816 0.011490569 0.0080350991 0.0049858312 0.0025287177 0.00098822708 0.00024362767 1.0779717e-05 -2.1441367e-05 -2.9233357e-05 -3.3522512e-05][0.025954366 0.024003891 0.02085986 0.017382082 0.013716466 0.010261877 0.0069948155 0.0042158426 0.002068677 0.0007808248 0.00018552551 3.9404404e-06 -2.3620509e-05 -3.143635e-05 -3.5918139e-05][0.024805812 0.022622645 0.019149872 0.015437916 0.0116976 0.0083974013 0.0054695504 0.0031298422 0.0014330541 0.00049763644 0.00010529767 -3.8899379e-06 -2.2917651e-05 -3.1807802e-05 -3.69195e-05][0.02361496 0.021079697 0.017246494 0.013303204 0.0095518576 0.0064769234 0.00395323 0.002099785 0.00086245453 0.00026348745 4.3933964e-05 -1.1038923e-05 -2.3824119e-05 -3.17873e-05 -3.6499881e-05][0.022406833 0.019505164 0.015376217 0.011301913 0.0076313014 0.0048241294 0.002709887 0.0013046056 0.00045943557 0.00011307751 8.4198473e-06 -1.5120408e-05 -2.3908993e-05 -3.1140233e-05 -3.6561873e-05][0.021183962 0.017939638 0.013615992 0.0095202774 0.0060079265 0.0034970301 0.0017667331 0.00074925768 0.00020980035 3.3566335e-05 -7.7101213e-06 -1.7998122e-05 -2.4529363e-05 -3.081694e-05 -3.5932448e-05]]...]
INFO - root - 2017-12-09 08:23:04.474163: step 5710, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.880 sec/batch; 79h:52m:01s remains)
INFO - root - 2017-12-09 08:23:12.929819: step 5720, loss = 0.82, batch loss = 0.69 (10.5 examples/sec; 0.764 sec/batch; 69h:19m:09s remains)
INFO - root - 2017-12-09 08:23:21.608880: step 5730, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.880 sec/batch; 79h:51m:35s remains)
INFO - root - 2017-12-09 08:23:30.516783: step 5740, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.886 sec/batch; 80h:22m:53s remains)
INFO - root - 2017-12-09 08:23:39.407255: step 5750, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.890 sec/batch; 80h:46m:14s remains)
INFO - root - 2017-12-09 08:23:47.923893: step 5760, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.891 sec/batch; 80h:53m:12s remains)
INFO - root - 2017-12-09 08:23:56.525875: step 5770, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 78h:42m:54s remains)
INFO - root - 2017-12-09 08:24:05.011789: step 5780, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.855 sec/batch; 77h:37m:14s remains)
INFO - root - 2017-12-09 08:24:13.706412: step 5790, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.874 sec/batch; 79h:17m:40s remains)
INFO - root - 2017-12-09 08:24:22.531436: step 5800, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 79h:01m:18s remains)
2017-12-09 08:24:23.377255: I tensorflow/core/kernels/logging_ops.cc:79] [[[-4.9206174e-05 -2.174345e-05 4.1962361e-05 0.00015703842 0.00033335577 0.00054794812 0.00076465926 0.00095488789 0.0011765604 0.0015387993 0.0021813784 0.0031960208 0.0045191711 0.0059593949 0.0072186412][-2.6670372e-05 5.547508e-05 0.00021479503 0.00046238585 0.00078058237 0.0011079594 0.0013624229 0.0014824283 0.001493778 0.0014959628 0.0016410935 0.002045305 0.0027132682 0.0035420558 0.0043418016][5.087164e-05 0.00026747512 0.00064492796 0.0011756491 0.0017925849 0.0023560943 0.0027104293 0.002753512 0.0025041667 0.0020797248 0.0016690457 0.0014448063 0.0014794079 0.0017340507 0.0020910278][0.00021193281 0.000678713 0.0014451374 0.0024622614 0.0035897174 0.0045639719 0.0051222509 0.005109082 0.0045592473 0.0036363725 0.0025925145 0.0016977987 0.0011081711 0.00084501895 0.00081787456][0.00052659307 0.0013258124 0.0026064997 0.0042745834 0.0060991012 0.0076586083 0.0085547687 0.0085588358 0.0077282875 0.0062796045 0.0045391307 0.00289977 0.0016258555 0.0008225498 0.00041299567][0.00097147358 0.0021097423 0.0039196056 0.0062793694 0.0088711549 0.011117729 0.01247016 0.012603511 0.011583082 0.00966741 0.00724168 0.0048201163 0.0027946024 0.0013865087 0.00055601943][0.0014155259 0.0027923656 0.0049871411 0.007887817 0.011124239 0.014016893 0.015884615 0.016292999 0.015270712 0.013075977 0.010112228 0.0069882283 0.0042276671 0.0021951732 0.00091955526][0.0016635627 0.003089797 0.0053827832 0.0084706768 0.011993503 0.015270718 0.017553976 0.018317344 0.017506203 0.015334369 0.012170607 0.0086560184 0.0054005333 0.0028900662 0.0012491328][0.0015935843 0.0028550352 0.0049095643 0.0077393022 0.011058511 0.014286811 0.016713966 0.017778812 0.017315622 0.015462913 0.012522752 0.0090909991 0.0057866047 0.0031488291 0.0013801026][0.0012494299 0.0021972628 0.0037650568 0.0059801252 0.008665082 0.011402979 0.013614837 0.014787416 0.014674651 0.013325975 0.010963315 0.0080741355 0.0051979688 0.0028418654 0.0012429277][0.000757223 0.0013599123 0.0023746786 0.0038458758 0.0056923293 0.0076654921 0.0093689431 0.010405752 0.010521453 0.0097030159 0.008081832 0.0060018809 0.0038722802 0.0020980397 0.00089490396][0.00032581363 0.00063618767 0.0011738414 0.0019779846 0.0030316513 0.0042157634 0.0053051733 0.0060450844 0.0062384182 0.0058395085 0.0049093873 0.0036557789 0.0023411259 0.0012366056 0.00049806247][5.4310767e-05 0.00017682681 0.00039891657 0.00074358773 0.0012184018 0.0017800456 0.0023340869 0.0027503888 0.0029094878 0.0027663396 0.0023416134 0.0017355895 0.0010866729 0.00054235983 0.00018808365][-4.8423473e-05 -1.9128493e-05 4.2114909e-05 0.00014609398 0.00029977187 0.00049494836 0.00070403173 0.00087633 0.00095935044 0.0009277119 0.00078501186 0.00056658016 0.00033026177 0.0001357033 1.5700847e-05][-6.26428e-05 -5.9801358e-05 -5.1296458e-05 -3.4541459e-05 -5.9096055e-06 3.5847719e-05 8.5608182e-05 0.00013147458 0.00015853066 0.00015863977 0.00012976627 7.9000682e-05 2.3731343e-05 -2.0687177e-05 -4.5584005e-05]]...]
INFO - root - 2017-12-09 08:24:31.932672: step 5810, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.876 sec/batch; 79h:27m:07s remains)
INFO - root - 2017-12-09 08:24:40.369018: step 5820, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.886 sec/batch; 80h:24m:50s remains)
INFO - root - 2017-12-09 08:24:49.176767: step 5830, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.882 sec/batch; 80h:04m:45s remains)
INFO - root - 2017-12-09 08:24:57.954339: step 5840, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 78h:25m:00s remains)
INFO - root - 2017-12-09 08:25:06.731035: step 5850, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.857 sec/batch; 77h:46m:49s remains)
INFO - root - 2017-12-09 08:25:15.341733: step 5860, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.882 sec/batch; 80h:02m:04s remains)
INFO - root - 2017-12-09 08:25:24.054320: step 5870, loss = 0.82, batch loss = 0.69 (8.8 examples/sec; 0.906 sec/batch; 82h:10m:39s remains)
INFO - root - 2017-12-09 08:25:32.788377: step 5880, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.891 sec/batch; 80h:52m:59s remains)
INFO - root - 2017-12-09 08:25:41.596789: step 5890, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.888 sec/batch; 80h:31m:38s remains)
INFO - root - 2017-12-09 08:25:50.361595: step 5900, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 78h:45m:18s remains)
2017-12-09 08:25:51.233475: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.05285361 0.062837318 0.069781989 0.073609278 0.074391663 0.072998881 0.070300378 0.0673081 0.064085975 0.060004186 0.054687452 0.047865368 0.039583892 0.030323872 0.021275109][0.067871593 0.0824651 0.093376316 0.10003928 0.10229664 0.10122662 0.098097 0.094320536 0.090027653 0.084625274 0.077415057 0.068082042 0.056631222 0.043645225 0.030863767][0.081779286 0.1018303 0.11780346 0.12842861 0.13277033 0.13220698 0.1281006 0.12284288 0.11676948 0.10957886 0.10038055 0.088604681 0.074112855 0.05755787 0.041106533][0.092883445 0.11833805 0.1395746 0.15448195 0.16165873 0.16243052 0.15793213 0.15120478 0.14308137 0.13381484 0.12235275 0.10795582 0.09039741 0.070409507 0.050521333][0.099759579 0.12927647 0.15486847 0.17360704 0.1834386 0.18571796 0.18145777 0.17409368 0.16473174 0.1539882 0.14080922 0.12436736 0.10440707 0.081546836 0.058565322][0.10180108 0.13378601 0.16229661 0.18371522 0.19586931 0.19978039 0.19621874 0.18888758 0.17902981 0.16769597 0.15360889 0.13584037 0.11419611 0.08936663 0.064293034][0.10010988 0.13333006 0.16364601 0.1870091 0.20100512 0.20627788 0.20355481 0.19663848 0.18693392 0.17555608 0.16106163 0.14271276 0.12008864 0.094046138 0.067601308][0.096257061 0.12938625 0.16020848 0.18457533 0.19986704 0.20640513 0.20463878 0.19850585 0.18943128 0.17842306 0.164035 0.14556573 0.12266979 0.096282907 0.069251642][0.092091091 0.12380206 0.15356599 0.17741618 0.19302031 0.20036943 0.19973345 0.19462782 0.18650343 0.17613035 0.16209882 0.14394309 0.12142757 0.095496528 0.068828955][0.08720509 0.11656662 0.14420338 0.16662747 0.18164767 0.18915407 0.18947604 0.18562248 0.17866401 0.16903532 0.15555149 0.13806158 0.1164699 0.091687158 0.066231176][0.080011405 0.10656602 0.13139841 0.15159194 0.16549492 0.17294386 0.17409846 0.1713385 0.16561751 0.15707698 0.14455312 0.12825535 0.1081818 0.085268989 0.061819874][0.070367374 0.093299173 0.11476367 0.1323356 0.14445052 0.15110077 0.15259868 0.15077712 0.14618759 0.13873091 0.12765406 0.11329176 0.095630772 0.075486 0.054900043][0.05881599 0.077344313 0.094572619 0.10866643 0.11851045 0.12407661 0.12553665 0.12425699 0.12058533 0.11439714 0.10516132 0.093192756 0.07869564 0.062276311 0.045543659][0.046123922 0.06001997 0.072760589 0.083100058 0.090258487 0.094350867 0.095538616 0.094712138 0.09195932 0.087116376 0.079919934 0.070674606 0.059567492 0.047080632 0.034504157][0.03274855 0.042433947 0.0511462 0.058092449 0.06283956 0.065552659 0.066417508 0.06595812 0.064139955 0.060811885 0.055819634 0.049355526 0.041591465 0.032882221 0.024161302]]...]
INFO - root - 2017-12-09 08:25:59.867262: step 5910, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.853 sec/batch; 77h:23m:52s remains)
INFO - root - 2017-12-09 08:26:08.247876: step 5920, loss = 0.82, batch loss = 0.69 (8.5 examples/sec; 0.946 sec/batch; 85h:47m:22s remains)
INFO - root - 2017-12-09 08:26:16.880188: step 5930, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.846 sec/batch; 76h:46m:05s remains)
INFO - root - 2017-12-09 08:26:25.498036: step 5940, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.896 sec/batch; 81h:18m:50s remains)
INFO - root - 2017-12-09 08:26:34.166905: step 5950, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.845 sec/batch; 76h:38m:40s remains)
INFO - root - 2017-12-09 08:26:42.608818: step 5960, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.843 sec/batch; 76h:27m:55s remains)
INFO - root - 2017-12-09 08:26:51.003866: step 5970, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.832 sec/batch; 75h:27m:17s remains)
INFO - root - 2017-12-09 08:26:59.758707: step 5980, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 78h:58m:55s remains)
INFO - root - 2017-12-09 08:27:08.366020: step 5990, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.854 sec/batch; 77h:25m:21s remains)
INFO - root - 2017-12-09 08:27:16.967806: step 6000, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.860 sec/batch; 77h:57m:46s remains)
2017-12-09 08:27:17.868226: I tensorflow/core/kernels/logging_ops.cc:79] [[[-8.1250379e-05 -7.9288031e-05 -7.8451194e-05 -7.7578916e-05 -7.6978991e-05 -7.6562719e-05 -7.6364988e-05 -7.5697128e-05 -7.4674535e-05 -7.3920441e-05 -7.3414376e-05 -7.3288567e-05 -7.3208634e-05 -7.322077e-05 -7.3304749e-05][-7.8757192e-05 -7.7183831e-05 -7.68766e-05 -7.6711338e-05 -7.6879078e-05 -7.7067823e-05 -7.709807e-05 -7.6416938e-05 -7.5401076e-05 -7.4489275e-05 -7.373259e-05 -7.33131e-05 -7.28584e-05 -7.2485047e-05 -7.2229144e-05][-7.6169323e-05 -7.473689e-05 -7.4693664e-05 -7.5313648e-05 -7.6454482e-05 -7.7490637e-05 -7.7866e-05 -7.7313663e-05 -7.6569653e-05 -7.564704e-05 -7.4822856e-05 -7.4053321e-05 -7.3228694e-05 -7.2488554e-05 -7.2170493e-05][-7.2305753e-05 -7.1279828e-05 -7.2141353e-05 -7.410621e-05 -7.6413256e-05 -7.8170087e-05 -7.8628305e-05 -7.8224672e-05 -7.7552555e-05 -7.6856857e-05 -7.6145596e-05 -7.5371383e-05 -7.4400486e-05 -7.3489166e-05 -7.3279924e-05][-6.9090755e-05 -6.8598725e-05 -6.9747854e-05 -7.26056e-05 -7.5900447e-05 -7.8373494e-05 -7.9229532e-05 -7.9264573e-05 -7.8743644e-05 -7.8030651e-05 -7.7328528e-05 -7.6709519e-05 -7.5672593e-05 -7.4645424e-05 -7.4274431e-05][-6.6692635e-05 -6.6647386e-05 -6.8298119e-05 -7.1281611e-05 -7.4568372e-05 -7.7107157e-05 -7.825176e-05 -7.8912533e-05 -7.9161844e-05 -7.8735393e-05 -7.8260433e-05 -7.7870318e-05 -7.6870041e-05 -7.5904209e-05 -7.5264652e-05][-6.5640961e-05 -6.5776105e-05 -6.7596193e-05 -6.9933514e-05 -7.2377639e-05 -7.427016e-05 -7.5314747e-05 -7.6270066e-05 -7.7529927e-05 -7.7944242e-05 -7.7897072e-05 -7.7639255e-05 -7.6650962e-05 -7.5548131e-05 -7.4517782e-05][-6.6531735e-05 -6.6320266e-05 -6.7542751e-05 -6.9116766e-05 -7.0603579e-05 -7.1768125e-05 -7.2050738e-05 -7.2829171e-05 -7.5179654e-05 -7.6666343e-05 -7.693356e-05 -7.66761e-05 -7.5495147e-05 -7.4105657e-05 -7.2741372e-05][-6.898999e-05 -6.8331923e-05 -6.911988e-05 -7.0183392e-05 -7.1154682e-05 -7.1823539e-05 -7.14576e-05 -7.2104347e-05 -7.4723182e-05 -7.6547993e-05 -7.6950178e-05 -7.6325661e-05 -7.499729e-05 -7.3368981e-05 -7.1525545e-05][-7.2651426e-05 -7.1897462e-05 -7.2263705e-05 -7.2599592e-05 -7.3267161e-05 -7.3729476e-05 -7.3660529e-05 -7.4209187e-05 -7.5759934e-05 -7.7006553e-05 -7.7493496e-05 -7.6941207e-05 -7.5233918e-05 -7.3212068e-05 -7.1145296e-05][-7.5183758e-05 -7.4658223e-05 -7.4868731e-05 -7.4899726e-05 -7.5300908e-05 -7.5403077e-05 -7.5391275e-05 -7.5719836e-05 -7.6717173e-05 -7.7977413e-05 -7.89092e-05 -7.8686979e-05 -7.6903292e-05 -7.456915e-05 -7.2612442e-05][-7.6052216e-05 -7.557396e-05 -7.5872958e-05 -7.578799e-05 -7.5958756e-05 -7.5908858e-05 -7.57747e-05 -7.6086122e-05 -7.7119614e-05 -7.88293e-05 -8.0101352e-05 -8.0009719e-05 -7.825088e-05 -7.5711774e-05 -7.3586707e-05][-7.586975e-05 -7.583847e-05 -7.6487348e-05 -7.6377444e-05 -7.63249e-05 -7.5909222e-05 -7.5380092e-05 -7.5829834e-05 -7.7081553e-05 -7.9068734e-05 -8.05867e-05 -8.0795915e-05 -7.9612226e-05 -7.7427714e-05 -7.5625067e-05][-7.5661395e-05 -7.560465e-05 -7.6368444e-05 -7.6484605e-05 -7.6220429e-05 -7.5293487e-05 -7.4292722e-05 -7.4599739e-05 -7.5886484e-05 -7.7921388e-05 -7.9655052e-05 -8.0629543e-05 -8.0606551e-05 -7.9158715e-05 -7.7715056e-05][-7.6254262e-05 -7.5942582e-05 -7.6477634e-05 -7.6398224e-05 -7.5627955e-05 -7.3962408e-05 -7.23554e-05 -7.2246679e-05 -7.346303e-05 -7.5429896e-05 -7.7252669e-05 -7.8815974e-05 -7.9631456e-05 -7.9163052e-05 -7.8202938e-05]]...]
INFO - root - 2017-12-09 08:27:26.721639: step 6010, loss = 0.82, batch loss = 0.69 (8.0 examples/sec; 1.004 sec/batch; 91h:01m:01s remains)
INFO - root - 2017-12-09 08:27:35.156888: step 6020, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.850 sec/batch; 77h:06m:04s remains)
INFO - root - 2017-12-09 08:27:43.818342: step 6030, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.866 sec/batch; 78h:29m:52s remains)
INFO - root - 2017-12-09 08:27:52.545473: step 6040, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.879 sec/batch; 79h:43m:37s remains)
INFO - root - 2017-12-09 08:28:01.156759: step 6050, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.859 sec/batch; 77h:52m:14s remains)
INFO - root - 2017-12-09 08:28:09.690308: step 6060, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.874 sec/batch; 79h:16m:49s remains)
INFO - root - 2017-12-09 08:28:18.216366: step 6070, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.850 sec/batch; 77h:04m:07s remains)
INFO - root - 2017-12-09 08:28:26.748114: step 6080, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.883 sec/batch; 80h:01m:36s remains)
INFO - root - 2017-12-09 08:28:35.275566: step 6090, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.858 sec/batch; 77h:48m:31s remains)
INFO - root - 2017-12-09 08:28:43.938661: step 6100, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.888 sec/batch; 80h:32m:59s remains)
2017-12-09 08:28:44.833700: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.00029024726 0.00073391869 0.0013031147 0.0019786274 0.0027393219 0.0035188347 0.0042020171 0.0047110505 0.005034538 0.0051649082 0.0051414487 0.0048894547 0.0044243378 0.0037909562 0.0030369975][0.00037305849 0.00095185847 0.001829839 0.0029652836 0.0042648665 0.0055949045 0.0067700786 0.0076350914 0.0081482707 0.0084006712 0.0084332805 0.0082374215 0.0077518774 0.0069160368 0.005803044][0.00052186375 0.0013259599 0.0026293474 0.00436369 0.0063600997 0.0084170615 0.010277718 0.011689903 0.012543613 0.012915893 0.012909684 0.012594656 0.011923479 0.010896949 0.0095068216][0.00073375815 0.0018293421 0.0036310544 0.0059941127 0.0086276457 0.011253218 0.013595402 0.015368828 0.016422313 0.01691106 0.016999308 0.01676455 0.016138516 0.015064179 0.013552347][0.00099054549 0.0023504684 0.004571863 0.0074466579 0.0105792 0.013596899 0.01614145 0.017963646 0.018976273 0.019395685 0.019470956 0.019309243 0.018869242 0.018036729 0.016774466][0.0012619202 0.0027070015 0.0050864955 0.008181124 0.011524708 0.014711771 0.01731886 0.019067451 0.019945791 0.020190278 0.020135397 0.019906968 0.019502381 0.018815313 0.017816626][0.0014853796 0.0028096489 0.0050252187 0.0079439962 0.011138883 0.01420804 0.016697207 0.018321054 0.019097418 0.019269193 0.019138683 0.018844936 0.018428087 0.017794924 0.016909592][0.0014113049 0.0025050389 0.0043203952 0.0067630108 0.0094694057 0.012084036 0.014211201 0.015604346 0.016305679 0.016478891 0.016368486 0.01610684 0.015751459 0.015223152 0.014455658][0.0010773303 0.001874323 0.0031781055 0.0049374006 0.0068918923 0.0087933326 0.010338169 0.011363245 0.011923912 0.012159086 0.012215534 0.012139362 0.011965105 0.01163705 0.011070133][0.00059278507 0.0010764361 0.0018694594 0.0029369558 0.0041131466 0.0052529732 0.0061733425 0.00679685 0.007189821 0.0074208523 0.0075737559 0.0076759919 0.0077156555 0.0076033515 0.0072715147][0.00017197173 0.00039244641 0.00076150079 0.0012651753 0.0018167982 0.00234572 0.0027807925 0.003093852 0.0033242919 0.0035000704 0.0036592362 0.0037952324 0.0038753191 0.003850254 0.0036793635][-3.2012133e-05 3.1350326e-05 0.00014606195 0.00031208381 0.00049536838 0.00067422871 0.00082863064 0.0009552174 0.0010698194 0.0011720301 0.0012708681 0.0013519539 0.0013955388 0.0013802511 0.0012968284][-6.8135079e-05 -6.2091451e-05 -4.9080369e-05 -2.5773705e-05 1.0232907e-07 2.5845322e-05 5.1348092e-05 8.0405633e-05 0.00011658941 0.00015361044 0.0001914621 0.0002219162 0.00023726998 0.00023062697 0.00020414703][-6.8400361e-05 -6.7252447e-05 -6.6007386e-05 -6.2964507e-05 -5.9816317e-05 -5.7399215e-05 -5.581796e-05 -5.5295768e-05 -5.4740995e-05 -5.577314e-05 -5.6226367e-05 -5.6730561e-05 -5.7275138e-05 -5.7354602e-05 -5.6425128e-05][-6.8590147e-05 -6.7597743e-05 -6.6431843e-05 -6.4440064e-05 -6.2273408e-05 -6.0462429e-05 -5.916095e-05 -5.9217684e-05 -6.0089878e-05 -6.198253e-05 -6.3152824e-05 -6.4064137e-05 -6.4691973e-05 -6.4711596e-05 -6.3530832e-05]]...]
INFO - root - 2017-12-09 08:28:53.653813: step 6110, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.885 sec/batch; 80h:14m:42s remains)
INFO - root - 2017-12-09 08:29:02.118891: step 6120, loss = 0.82, batch loss = 0.69 (9.7 examples/sec; 0.829 sec/batch; 75h:09m:02s remains)
INFO - root - 2017-12-09 08:29:10.774309: step 6130, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.858 sec/batch; 77h:46m:21s remains)
INFO - root - 2017-12-09 08:29:19.374130: step 6140, loss = 0.82, batch loss = 0.69 (8.8 examples/sec; 0.909 sec/batch; 82h:22m:30s remains)
INFO - root - 2017-12-09 08:29:27.993667: step 6150, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 78h:07m:34s remains)
INFO - root - 2017-12-09 08:29:36.645910: step 6160, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.884 sec/batch; 80h:08m:21s remains)
INFO - root - 2017-12-09 08:29:45.390802: step 6170, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.860 sec/batch; 77h:58m:09s remains)
INFO - root - 2017-12-09 08:29:54.033092: step 6180, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.885 sec/batch; 80h:12m:05s remains)
INFO - root - 2017-12-09 08:30:02.693229: step 6190, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.865 sec/batch; 78h:24m:33s remains)
INFO - root - 2017-12-09 08:30:11.354136: step 6200, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.843 sec/batch; 76h:23m:14s remains)
2017-12-09 08:30:12.251785: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.054025013 0.05704882 0.059522606 0.0615777 0.063337 0.064797387 0.066458412 0.068566948 0.071291454 0.074659996 0.07822407 0.081775554 0.0847076 0.086673886 0.08671584][0.046982005 0.050205331 0.053078238 0.055491861 0.057698827 0.059729993 0.062144287 0.0652859 0.069391996 0.074367955 0.079733454 0.084702045 0.08895196 0.091894768 0.092536435][0.038487446 0.041861925 0.044966158 0.04773156 0.050070047 0.05238222 0.055248607 0.059258375 0.064749613 0.0712748 0.078401268 0.08510299 0.090859875 0.09477032 0.095932581][0.031511206 0.035079569 0.038260065 0.04096188 0.043166343 0.045395121 0.048313059 0.05313994 0.05979966 0.068036146 0.07689596 0.085158564 0.092134722 0.096635543 0.098029405][0.027001677 0.030625204 0.033669785 0.036219046 0.037979428 0.040166542 0.04326085 0.048662566 0.056343168 0.066000529 0.076097056 0.085432395 0.093011752 0.097731031 0.099017046][0.024851797 0.028336195 0.031042902 0.033326276 0.034698851 0.036761232 0.040058304 0.046104439 0.054525491 0.065027893 0.07601878 0.086054027 0.093704358 0.098186746 0.099235289][0.024690541 0.02769023 0.029770017 0.031562563 0.032616962 0.034908354 0.038642336 0.045273673 0.054249257 0.065302841 0.07638821 0.086377218 0.09346354 0.097344689 0.098094933][0.026044147 0.028804142 0.030437468 0.031763617 0.032584757 0.034993827 0.038988922 0.045985322 0.0548574 0.065558 0.076161832 0.085626215 0.092037641 0.095217548 0.095583767][0.027769772 0.030250655 0.031722866 0.033062331 0.034063715 0.036735594 0.041036583 0.048027061 0.056402422 0.06624195 0.0755888 0.083851978 0.089076236 0.091446489 0.09139128][0.029037248 0.031291481 0.032692369 0.034124527 0.035609543 0.038716022 0.043285608 0.0499662 0.057496026 0.065995723 0.0736249 0.080193728 0.084156916 0.085849971 0.085553676][0.029213034 0.030939892 0.032158844 0.033708442 0.035723206 0.039219324 0.044071704 0.050531976 0.057267252 0.064228661 0.070174523 0.0749936 0.077726416 0.078783348 0.078347318][0.028545626 0.029724333 0.030667817 0.032175209 0.034430467 0.038140252 0.042924255 0.048883572 0.054601487 0.060208496 0.064633444 0.06788177 0.069616877 0.070217609 0.069769815][0.027244013 0.028163122 0.028983766 0.030388964 0.0325908 0.036008928 0.040316947 0.045420829 0.050031688 0.054265156 0.057456385 0.059592426 0.060664091 0.0609155 0.060466141][0.025294937 0.025896171 0.026620416 0.027948255 0.029988058 0.032917146 0.036438625 0.040448319 0.04397542 0.047100384 0.04931847 0.05065969 0.051199123 0.051263086 0.050829697][0.023252551 0.023603052 0.024088454 0.025122762 0.026789011 0.029119642 0.031845786 0.03479892 0.037308939 0.039447363 0.04094648 0.041763145 0.042020839 0.042004403 0.041685969]]...]
INFO - root - 2017-12-09 08:30:20.835897: step 6210, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.874 sec/batch; 79h:11m:05s remains)
INFO - root - 2017-12-09 08:30:29.240002: step 6220, loss = 0.82, batch loss = 0.69 (10.2 examples/sec; 0.784 sec/batch; 71h:03m:43s remains)
INFO - root - 2017-12-09 08:30:37.880839: step 6230, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 78h:33m:45s remains)
INFO - root - 2017-12-09 08:30:46.537332: step 6240, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 78h:17m:41s remains)
INFO - root - 2017-12-09 08:30:55.265585: step 6250, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.879 sec/batch; 79h:37m:15s remains)
INFO - root - 2017-12-09 08:31:03.770937: step 6260, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.891 sec/batch; 80h:44m:27s remains)
INFO - root - 2017-12-09 08:31:12.359125: step 6270, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 78h:34m:45s remains)
INFO - root - 2017-12-09 08:31:20.977783: step 6280, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.854 sec/batch; 77h:21m:42s remains)
INFO - root - 2017-12-09 08:31:29.506518: step 6290, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.887 sec/batch; 80h:21m:01s remains)
INFO - root - 2017-12-09 08:31:38.193156: step 6300, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.857 sec/batch; 77h:39m:07s remains)
2017-12-09 08:31:39.038571: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.037416756 0.037725359 0.036768619 0.035094891 0.032463409 0.02899156 0.025096094 0.021433238 0.018339377 0.016131276 0.014747616 0.014053266 0.013678917 0.013339588 0.013033736][0.037201948 0.03683342 0.035078354 0.0325488 0.029062811 0.024898395 0.020448577 0.016380569 0.013061531 0.010809267 0.0094952341 0.00886681 0.0085565224 0.0083198929 0.0081167715][0.036699429 0.035706546 0.033239696 0.029943841 0.025735471 0.021007121 0.016165204 0.011874078 0.0085170679 0.0063168621 0.0050920458 0.0045487597 0.0043289382 0.0041890326 0.0040757135][0.03660081 0.034948464 0.031788059 0.027714558 0.022820763 0.01769774 0.012682509 0.0084364563 0.0053127543 0.003382287 0.0023820424 0.0019457989 0.0017694526 0.0016868024 0.0016313987][0.036613621 0.034377348 0.030593047 0.025855398 0.020419402 0.015025348 0.010008575 0.0060226326 0.0033349856 0.0018599039 0.0011966638 0.00090597151 0.00077833742 0.00069409987 0.0006337876][0.036624651 0.034019463 0.029763944 0.024559367 0.01878777 0.013267362 0.0083609046 0.0047015809 0.0024742521 0.0014192496 0.0010370378 0.00087963557 0.00080125139 0.00070878927 0.00063713687][0.036761608 0.034004703 0.029534169 0.02412837 0.018251183 0.012761867 0.0080106491 0.0045991023 0.002645212 0.0018018184 0.0015276365 0.0013811507 0.0012977299 0.0011500926 0.0010292][0.036946271 0.034283925 0.029856745 0.024533194 0.018775249 0.013401934 0.00877492 0.0055064745 0.0036292982 0.0027652348 0.0024140009 0.0021704268 0.0020147059 0.001758162 0.0015137572][0.037101533 0.034701 0.030541835 0.025463874 0.019944025 0.014761726 0.010237596 0.0070367311 0.0050842264 0.004081897 0.0036042861 0.0032736568 0.0030517115 0.0026438923 0.0022646498][0.037472546 0.035557833 0.031928323 0.027348898 0.022287276 0.017346865 0.012939218 0.0096494174 0.0074281008 0.0061060027 0.0053454572 0.004815897 0.0044061807 0.0038248161 0.0032834266][0.03812566 0.036778964 0.03378148 0.02985679 0.025403881 0.020851182 0.016650343 0.01330513 0.010794954 0.0090733552 0.0078945272 0.0070557776 0.0064052013 0.0056689568 0.0049436325][0.038485505 0.037860531 0.035668995 0.032650385 0.0290674 0.025171049 0.02140826 0.018098727 0.015371626 0.013251535 0.011627793 0.010391048 0.0093894852 0.0083616506 0.0073822783][0.038418707 0.038512278 0.037094403 0.035013497 0.032346155 0.029213402 0.026019225 0.022967512 0.020267138 0.017948547 0.016046738 0.014508358 0.013189646 0.011954204 0.010781208][0.037627947 0.038314462 0.037593782 0.036428079 0.034679398 0.032455295 0.030046478 0.027497165 0.025080333 0.022820689 0.020866651 0.019199995 0.017722756 0.01639476 0.015190531][0.0366068 0.037761524 0.037609696 0.037169948 0.0362687 0.034895863 0.033276051 0.031274822 0.029303219 0.02733908 0.025493823 0.023820229 0.022314465 0.021035792 0.019833682]]...]
INFO - root - 2017-12-09 08:31:47.623145: step 6310, loss = 0.82, batch loss = 0.69 (9.7 examples/sec; 0.826 sec/batch; 74h:51m:52s remains)
INFO - root - 2017-12-09 08:31:56.019712: step 6320, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.842 sec/batch; 76h:19m:22s remains)
INFO - root - 2017-12-09 08:32:04.435995: step 6330, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.883 sec/batch; 79h:59m:01s remains)
INFO - root - 2017-12-09 08:32:13.051834: step 6340, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.851 sec/batch; 77h:05m:48s remains)
INFO - root - 2017-12-09 08:32:21.667762: step 6350, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.863 sec/batch; 78h:13m:47s remains)
INFO - root - 2017-12-09 08:32:30.230143: step 6360, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.866 sec/batch; 78h:27m:13s remains)
INFO - root - 2017-12-09 08:32:38.782118: step 6370, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.881 sec/batch; 79h:51m:07s remains)
INFO - root - 2017-12-09 08:32:47.252185: step 6380, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.855 sec/batch; 77h:26m:59s remains)
INFO - root - 2017-12-09 08:32:55.921049: step 6390, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.884 sec/batch; 80h:03m:00s remains)
INFO - root - 2017-12-09 08:33:04.583731: step 6400, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.875 sec/batch; 79h:13m:15s remains)
2017-12-09 08:33:05.435171: I tensorflow/core/kernels/logging_ops.cc:79] [[[-7.1006885e-05 -7.0845635e-05 -7.1126247e-05 -7.1047696e-05 -7.0421433e-05 -6.9429036e-05 -6.8513516e-05 -6.7596106e-05 -6.6577995e-05 -6.5843386e-05 -6.54047e-05 -6.496023e-05 -6.4356886e-05 -6.4051521e-05 -6.4034008e-05][-6.9812115e-05 -6.97027e-05 -7.01485e-05 -7.02729e-05 -6.9803049e-05 -6.897284e-05 -6.8248482e-05 -6.742424e-05 -6.636569e-05 -6.5578242e-05 -6.5054846e-05 -6.4519176e-05 -6.3826708e-05 -6.3575935e-05 -6.3700012e-05][-6.9148577e-05 -6.9142567e-05 -6.9732836e-05 -6.9978705e-05 -6.9693124e-05 -6.902426e-05 -6.84048e-05 -6.763e-05 -6.6553213e-05 -6.575857e-05 -6.5188731e-05 -6.4550055e-05 -6.3795313e-05 -6.359088e-05 -6.3797583e-05][-6.9107758e-05 -6.9186644e-05 -6.9819172e-05 -7.0005713e-05 -6.9775662e-05 -6.9266054e-05 -6.8608482e-05 -6.7659392e-05 -6.6544548e-05 -6.5777655e-05 -6.5253262e-05 -6.4565e-05 -6.3856831e-05 -6.3782514e-05 -6.4046166e-05][-6.996475e-05 -7.0070586e-05 -7.0546448e-05 -7.0544018e-05 -7.0214446e-05 -6.9613176e-05 -6.871036e-05 -6.7561552e-05 -6.656941e-05 -6.5930959e-05 -6.5494351e-05 -6.48202e-05 -6.4229222e-05 -6.4205175e-05 -6.4396583e-05][-7.1214992e-05 -7.1263181e-05 -7.14759e-05 -7.1236325e-05 -7.075147e-05 -6.98901e-05 -6.8683352e-05 -6.7405854e-05 -6.6516383e-05 -6.5905479e-05 -6.5482e-05 -6.487139e-05 -6.4380933e-05 -6.4426713e-05 -6.4556727e-05][-7.2573435e-05 -7.2425973e-05 -7.228936e-05 -7.1760136e-05 -7.1068396e-05 -6.9991816e-05 -6.8545771e-05 -6.7253568e-05 -6.6501831e-05 -6.59393e-05 -6.5510263e-05 -6.4948079e-05 -6.4502383e-05 -6.4513966e-05 -6.4541571e-05][-7.351744e-05 -7.3297335e-05 -7.2989627e-05 -7.2376184e-05 -7.1584123e-05 -7.0321141e-05 -6.8743e-05 -6.7474233e-05 -6.6809676e-05 -6.62895e-05 -6.5878994e-05 -6.541489e-05 -6.5004017e-05 -6.4838678e-05 -6.4684144e-05][-7.4288968e-05 -7.4084746e-05 -7.3811912e-05 -7.334238e-05 -7.2643074e-05 -7.1545444e-05 -7.0224967e-05 -6.8957757e-05 -6.8126319e-05 -6.7400273e-05 -6.6771929e-05 -6.6105014e-05 -6.5434782e-05 -6.5047992e-05 -6.47361e-05][-7.4949967e-05 -7.4962096e-05 -7.49818e-05 -7.4948985e-05 -7.4640775e-05 -7.393858e-05 -7.2973082e-05 -7.1759074e-05 -7.058237e-05 -6.9339323e-05 -6.81701e-05 -6.7014269e-05 -6.5882996e-05 -6.5133834e-05 -6.4738517e-05][-7.600135e-05 -7.647516e-05 -7.7037083e-05 -7.7582954e-05 -7.7874407e-05 -7.7645476e-05 -7.6977958e-05 -7.5782911e-05 -7.41948e-05 -7.2370851e-05 -7.05631e-05 -6.8580077e-05 -6.6685781e-05 -6.5422944e-05 -6.4864231e-05][-7.774579e-05 -7.8736994e-05 -7.9768986e-05 -8.08184e-05 -8.1645056e-05 -8.1823833e-05 -8.1346312e-05 -8.0130369e-05 -7.8206926e-05 -7.5861419e-05 -7.349778e-05 -7.0778733e-05 -6.8059249e-05 -6.6121873e-05 -6.5236716e-05][-7.9971847e-05 -8.1405116e-05 -8.2832412e-05 -8.42009e-05 -8.5273357e-05 -8.5620552e-05 -8.52722e-05 -8.39841e-05 -8.1867838e-05 -7.9139994e-05 -7.6382858e-05 -7.3133589e-05 -6.9687674e-05 -6.7082685e-05 -6.5722939e-05][-8.2929837e-05 -8.4494495e-05 -8.6120643e-05 -8.7560758e-05 -8.8571782e-05 -8.880056e-05 -8.8334513e-05 -8.7000881e-05 -8.4818908e-05 -8.1846461e-05 -7.87788e-05 -7.5126649e-05 -7.114539e-05 -6.7957088e-05 -6.6120541e-05][-8.5263804e-05 -8.6566761e-05 -8.8051616e-05 -8.9373068e-05 -9.0206966e-05 -9.03397e-05 -8.9834757e-05 -8.8677247e-05 -8.6607273e-05 -8.3583509e-05 -8.03678e-05 -7.6519318e-05 -7.2222392e-05 -6.8617854e-05 -6.6457418e-05]]...]
INFO - root - 2017-12-09 08:33:14.039297: step 6410, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.853 sec/batch; 77h:15m:30s remains)
INFO - root - 2017-12-09 08:33:22.716491: step 6420, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.883 sec/batch; 80h:01m:23s remains)
INFO - root - 2017-12-09 08:33:31.293631: step 6430, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.883 sec/batch; 79h:59m:51s remains)
INFO - root - 2017-12-09 08:33:40.149725: step 6440, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.882 sec/batch; 79h:55m:18s remains)
INFO - root - 2017-12-09 08:33:48.802812: step 6450, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.877 sec/batch; 79h:24m:30s remains)
INFO - root - 2017-12-09 08:33:57.423892: step 6460, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.890 sec/batch; 80h:34m:51s remains)
INFO - root - 2017-12-09 08:34:06.054406: step 6470, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.880 sec/batch; 79h:40m:18s remains)
INFO - root - 2017-12-09 08:34:14.700207: step 6480, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.858 sec/batch; 77h:42m:08s remains)
INFO - root - 2017-12-09 08:34:23.334515: step 6490, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.877 sec/batch; 79h:22m:52s remains)
INFO - root - 2017-12-09 08:34:32.056079: step 6500, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.878 sec/batch; 79h:28m:22s remains)
2017-12-09 08:34:32.901210: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.00010335681 -0.0001046236 -0.00010547525 -0.00010562517 -0.00010570106 -0.00010614264 -0.00010655633 -0.00010649458 -0.00010576857 -0.00010429734 -0.0001027982 -0.00010120252 -9.967247e-05 -9.8385346e-05 -9.7422468e-05][-0.00010383877 -0.00010497327 -0.00010566666 -0.00010562194 -0.00010526081 -0.00010514934 -0.00010526628 -0.00010534323 -0.00010500207 -0.00010371413 -0.00010235569 -0.00010083029 -9.9182907e-05 -9.7707394e-05 -9.6677271e-05][-0.0001045227 -0.00010571928 -0.00010641991 -0.00010608711 -0.00010503696 -0.00010432502 -0.00010417927 -0.00010423256 -0.00010402871 -0.00010294651 -0.00010172807 -0.00010024478 -9.85947e-05 -9.7373864e-05 -9.6747535e-05][-0.00010486553 -0.00010588227 -0.00010639954 -0.00010594374 -0.00010465566 -0.00010382585 -0.0001037716 -0.00010403241 -0.00010400842 -0.00010297447 -0.00010173733 -0.00010021338 -9.8702651e-05 -9.7921729e-05 -9.7686017e-05][-0.00010430934 -0.00010508853 -0.00010540308 -0.00010499384 -0.00010410032 -0.00010371137 -0.00010393055 -0.00010407908 -0.00010385314 -0.00010276512 -0.00010151697 -0.00010015538 -9.9187033e-05 -9.8866018e-05 -9.9018362e-05][-0.00010325193 -0.00010396402 -0.00010434407 -0.00010407103 -0.00010349666 -0.00010332637 -0.00010358906 -0.00010341741 -0.0001026411 -0.00010138131 -0.00010042326 -9.9604054e-05 -9.91969e-05 -9.9344281e-05 -9.9769684e-05][-0.00010206192 -0.0001028315 -0.00010334804 -0.00010332654 -0.00010305077 -0.00010286468 -0.00010280263 -0.000102139 -0.00010093709 -9.963839e-05 -9.878834e-05 -9.8293465e-05 -9.8250151e-05 -9.8696066e-05 -9.9326971e-05][-0.00010108686 -0.00010176155 -0.00010239099 -0.00010262411 -0.00010269895 -0.00010258123 -0.00010202698 -0.00010085072 -9.9455e-05 -9.826888e-05 -9.7463817e-05 -9.7329896e-05 -9.7796124e-05 -9.8640332e-05 -9.9608253e-05][-0.0001002235 -0.00010088408 -0.00010162593 -0.00010203397 -0.00010235766 -0.00010241983 -0.00010191526 -0.00010082845 -9.9636367e-05 -9.8581193e-05 -9.7813172e-05 -9.7831238e-05 -9.8534838e-05 -9.95999e-05 -0.00010052155][-9.9408622e-05 -9.9959943e-05 -0.00010070245 -0.00010125592 -0.00010162298 -0.00010181386 -0.00010161402 -0.00010083051 -9.9977646e-05 -9.9209887e-05 -9.8636738e-05 -9.8608471e-05 -9.92147e-05 -0.00010009739 -0.00010077636][-9.8563592e-05 -9.8860473e-05 -9.9449433e-05 -9.9903889e-05 -0.00010019515 -0.00010029995 -0.00010021097 -9.9720775e-05 -9.9140409e-05 -9.8662043e-05 -9.8455705e-05 -9.8595985e-05 -9.9104363e-05 -9.9754048e-05 -0.00010020904][-9.74803e-05 -9.7631972e-05 -9.803204e-05 -9.8315388e-05 -9.8458273e-05 -9.841827e-05 -9.8203185e-05 -9.7861535e-05 -9.7543329e-05 -9.7297052e-05 -9.7325908e-05 -9.7573029e-05 -9.8003387e-05 -9.8485871e-05 -9.883086e-05][-9.6667747e-05 -9.6615993e-05 -9.6795e-05 -9.6892931e-05 -9.6851989e-05 -9.6729476e-05 -9.6530748e-05 -9.6333788e-05 -9.6197327e-05 -9.6130068e-05 -9.6195705e-05 -9.6374279e-05 -9.6634525e-05 -9.6964446e-05 -9.7175507e-05][-9.61347e-05 -9.5911986e-05 -9.5911906e-05 -9.593484e-05 -9.58939e-05 -9.5813346e-05 -9.5742187e-05 -9.5669748e-05 -9.5593787e-05 -9.5572519e-05 -9.561317e-05 -9.5705676e-05 -9.5825388e-05 -9.595407e-05 -9.6041833e-05][-9.5541407e-05 -9.5140174e-05 -9.5035219e-05 -9.502226e-05 -9.4993047e-05 -9.4956609e-05 -9.4941031e-05 -9.494165e-05 -9.4932751e-05 -9.4936404e-05 -9.4956391e-05 -9.4992705e-05 -9.5028328e-05 -9.5072624e-05 -9.5099094e-05]]...]
INFO - root - 2017-12-09 08:34:41.621004: step 6510, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.851 sec/batch; 77h:01m:54s remains)
INFO - root - 2017-12-09 08:34:50.087283: step 6520, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 78h:38m:02s remains)
INFO - root - 2017-12-09 08:34:58.573372: step 6530, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.875 sec/batch; 79h:12m:18s remains)
INFO - root - 2017-12-09 08:35:07.404203: step 6540, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.900 sec/batch; 81h:30m:08s remains)
INFO - root - 2017-12-09 08:35:16.070136: step 6550, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.877 sec/batch; 79h:24m:13s remains)
INFO - root - 2017-12-09 08:35:24.553802: step 6560, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.839 sec/batch; 75h:59m:26s remains)
INFO - root - 2017-12-09 08:35:33.274468: step 6570, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.877 sec/batch; 79h:25m:36s remains)
INFO - root - 2017-12-09 08:35:42.007271: step 6580, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.877 sec/batch; 79h:25m:19s remains)
INFO - root - 2017-12-09 08:35:50.691940: step 6590, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.877 sec/batch; 79h:23m:01s remains)
INFO - root - 2017-12-09 08:35:59.328986: step 6600, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.870 sec/batch; 78h:47m:14s remains)
2017-12-09 08:36:00.206944: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.00034445102 0.00059323339 0.00097343908 0.0016191178 0.0026393463 0.0039385618 0.005243517 0.0062406659 0.0066970428 0.0067251772 0.0064778449 0.006234711 0.006012449 0.0058893114 0.0060359277][0.00032443841 0.00058165053 0.0010121721 0.0017512299 0.0028705802 0.0043101716 0.0058050295 0.0070053833 0.0076032486 0.0076649138 0.0074159629 0.0071799825 0.0070355544 0.0070119617 0.0072636427][0.00022218353 0.0004478858 0.00085050776 0.0015482449 0.00261867 0.0040343949 0.0055109123 0.0067198039 0.00735166 0.0074853264 0.0073736208 0.0073147435 0.0073396503 0.0075145224 0.0078814076][4.1462234e-05 0.00022118742 0.00052491412 0.0011085977 0.0020057273 0.0032224741 0.0045397393 0.005627702 0.0062239747 0.0064216913 0.0064835898 0.0066538425 0.0069191512 0.0072468878 0.0077159633][-5.22377e-05 1.8866122e-05 0.00018200927 0.00055310584 0.0011724775 0.0020745469 0.0030851334 0.0039347517 0.0044245757 0.0046582981 0.0048633 0.0051853266 0.0055821175 0.0060085491 0.0064383829][-5.0508446e-05 -3.7049962e-05 -2.0442676e-06 0.00014901513 0.00043262247 0.00090180489 0.0014883492 0.0020173469 0.0023689473 0.002606103 0.0028715034 0.0032206143 0.00361958 0.004045614 0.0044705556][-7.7238634e-05 -6.4353895e-05 -5.2853175e-05 -1.5625657e-05 4.5767214e-05 0.00018447282 0.00038286086 0.00059624424 0.000768482 0.00091975194 0.0011124744 0.0013579882 0.0016497944 0.0019780374 0.0023410483][-0.00011518259 -0.00011116399 -0.00010618065 -9.7948083e-05 -8.6315413e-05 -6.8881644e-05 -4.3357293e-05 -3.85553e-06 4.1980733e-05 9.2133807e-05 0.00016571541 0.000273769 0.00042227542 0.00061549083 0.00084580807][-0.00011674113 -0.00011506961 -0.00011427749 -0.00011330585 -0.00011186324 -0.00011013135 -0.00010797343 -0.00010367512 -0.00010067444 -0.0001032686 -9.9433608e-05 -8.43076e-05 -4.9713846e-05 1.3760029e-05 0.00010639956][-0.000116447 -0.00011514824 -0.00011466185 -0.00011412943 -0.00011331186 -0.00011309268 -0.00011262295 -0.00011061097 -0.00010839703 -0.00010602744 -0.00010542668 -0.00010409416 -9.9794648e-05 -9.0299327e-05 -7.5393706e-05][-0.00011608795 -0.00011504921 -0.00011460355 -0.00011403192 -0.00011350066 -0.00011355188 -0.00011318846 -0.00011236121 -0.00011118971 -0.00010935715 -0.00010742896 -0.00010574191 -0.0001043816 -0.00010303478 -0.0001017436][-0.00011559469 -0.00011477627 -0.00011431931 -0.00011388292 -0.00011354176 -0.00011361799 -0.00011356327 -0.00011321573 -0.00011246113 -0.00011123218 -0.00010969673 -0.00010786366 -0.00010590427 -0.00010450141 -0.00010328749][-0.00011535762 -0.00011465451 -0.00011412652 -0.00011344491 -0.00011308272 -0.0001129766 -0.00011313396 -0.00011295316 -0.00011278419 -0.00011251975 -0.00011197032 -0.00011075788 -0.00010877696 -0.00010725496 -0.00010606412][-0.00011549285 -0.00011482852 -0.00011426382 -0.00011347352 -0.00011267536 -0.00011241548 -0.00011250257 -0.00011240294 -0.00011264181 -0.00011257885 -0.00011229314 -0.00011185626 -0.00011097408 -0.00011002818 -0.00010928822][-0.0001153682 -0.00011474346 -0.00011412225 -0.00011335392 -0.0001126205 -0.00011219303 -0.00011206199 -0.00011211607 -0.00011242423 -0.00011248904 -0.00011265593 -0.00011277539 -0.00011282711 -0.00011276033 -0.00011268575]]...]
INFO - root - 2017-12-09 08:36:08.974980: step 6610, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.841 sec/batch; 76h:09m:42s remains)
INFO - root - 2017-12-09 08:36:17.559766: step 6620, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.886 sec/batch; 80h:12m:08s remains)
INFO - root - 2017-12-09 08:36:26.118694: step 6630, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.888 sec/batch; 80h:23m:57s remains)
INFO - root - 2017-12-09 08:36:34.756495: step 6640, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 78h:48m:16s remains)
INFO - root - 2017-12-09 08:36:43.516221: step 6650, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.853 sec/batch; 77h:11m:44s remains)
INFO - root - 2017-12-09 08:36:52.014047: step 6660, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.857 sec/batch; 77h:32m:43s remains)
INFO - root - 2017-12-09 08:37:00.529187: step 6670, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.853 sec/batch; 77h:14m:21s remains)
INFO - root - 2017-12-09 08:37:09.096137: step 6680, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.852 sec/batch; 77h:04m:15s remains)
INFO - root - 2017-12-09 08:37:17.740981: step 6690, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.850 sec/batch; 76h:55m:11s remains)
INFO - root - 2017-12-09 08:37:26.429384: step 6700, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.879 sec/batch; 79h:31m:57s remains)
2017-12-09 08:37:27.275209: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.0092601171 0.0093165766 0.0094536738 0.0095943166 0.0095247924 0.0094247479 0.0095518893 0.010017903 0.010221017 0.010141547 0.0097670956 0.0096002044 0.01002086 0.01060369 0.011022016][0.0097724777 0.0098313829 0.0099686487 0.010099423 0.010054266 0.0099424841 0.010043177 0.010583251 0.010949557 0.011105265 0.010877472 0.010741452 0.011025915 0.011506392 0.01193666][0.010001566 0.010140082 0.010316126 0.010455126 0.010444872 0.010381862 0.010502848 0.010971655 0.011325015 0.011605858 0.011569381 0.011483183 0.01166867 0.011984811 0.012352553][0.010264309 0.010555873 0.010767994 0.010976612 0.011072315 0.011054683 0.011146136 0.011426681 0.011624057 0.01185857 0.01190543 0.011910063 0.012070187 0.012311032 0.012604571][0.010430804 0.010925082 0.011169413 0.011434223 0.011608431 0.011620756 0.011605687 0.01169592 0.011730982 0.011847096 0.011890019 0.011928607 0.012124659 0.012359975 0.012619182][0.010559216 0.011203951 0.011445096 0.011703863 0.01191416 0.01189098 0.011735528 0.011638258 0.011572546 0.011635775 0.011626819 0.011652331 0.01184273 0.012110043 0.012357198][0.010504203 0.011237618 0.011403819 0.011582262 0.011700212 0.011563783 0.01130365 0.011108698 0.011050173 0.011124762 0.011160431 0.011192377 0.011333068 0.011602238 0.011848986][0.0102489 0.010910901 0.010988832 0.011086437 0.011087228 0.010802743 0.010456678 0.010298639 0.010379178 0.010532919 0.010593127 0.01058308 0.010666327 0.010916865 0.011174407][0.009601797 0.010159682 0.010186238 0.010248709 0.01019059 0.0098674241 0.0095570153 0.0095160706 0.0097369086 0.0099399388 0.0099855457 0.009914292 0.0099154133 0.010077311 0.010280079][0.0086928811 0.0090987682 0.0090829721 0.0091498839 0.00912317 0.0088892821 0.0087098982 0.0088027576 0.0090762908 0.0092533855 0.0092419721 0.0090749273 0.0089950012 0.0090825576 0.009200505][0.0075048967 0.0078532891 0.00781882 0.0078779608 0.0078961719 0.0078079887 0.0078118481 0.008002202 0.0082362844 0.0082972283 0.0081752082 0.0079794256 0.0078837266 0.0079064788 0.0079433247][0.006219707 0.0064788763 0.0064422688 0.0065123737 0.00661085 0.0066495691 0.0067830221 0.007020866 0.0071994243 0.0071763336 0.0069517824 0.0067386269 0.0066284533 0.0066136583 0.0065874858][0.0049430961 0.0051093 0.0050541013 0.0051249 0.0052625076 0.0054097269 0.0056371903 0.0058666794 0.0059858537 0.0059161228 0.0056911185 0.0054687052 0.0053217039 0.0052627232 0.0051723677][0.0035655918 0.0036709446 0.0036381667 0.0037099451 0.0038500431 0.004025924 0.0042530769 0.0044696624 0.0045636552 0.0044686482 0.0042810133 0.0040990021 0.0039501381 0.0038291889 0.0036753055][0.0021561082 0.0022278067 0.0022242034 0.0022867478 0.0024044432 0.0025601881 0.002742433 0.0028924521 0.0029419202 0.0028849717 0.0027689929 0.0026380639 0.0025146517 0.0023891185 0.0022460083]]...]
INFO - root - 2017-12-09 08:37:35.927936: step 6710, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.850 sec/batch; 76h:57m:03s remains)
INFO - root - 2017-12-09 08:37:44.607900: step 6720, loss = 0.82, batch loss = 0.69 (8.8 examples/sec; 0.908 sec/batch; 82h:10m:17s remains)
INFO - root - 2017-12-09 08:37:53.200770: step 6730, loss = 0.82, batch loss = 0.69 (8.8 examples/sec; 0.912 sec/batch; 82h:29m:52s remains)
INFO - root - 2017-12-09 08:38:01.814437: step 6740, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.847 sec/batch; 76h:39m:39s remains)
INFO - root - 2017-12-09 08:38:10.422809: step 6750, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.883 sec/batch; 79h:54m:23s remains)
INFO - root - 2017-12-09 08:38:18.883172: step 6760, loss = 0.82, batch loss = 0.69 (10.6 examples/sec; 0.756 sec/batch; 68h:26m:39s remains)
INFO - root - 2017-12-09 08:38:27.411729: step 6770, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.887 sec/batch; 80h:17m:18s remains)
INFO - root - 2017-12-09 08:38:36.030229: step 6780, loss = 0.82, batch loss = 0.69 (8.8 examples/sec; 0.907 sec/batch; 82h:05m:47s remains)
INFO - root - 2017-12-09 08:38:44.882915: step 6790, loss = 0.82, batch loss = 0.69 (8.5 examples/sec; 0.938 sec/batch; 84h:52m:14s remains)
INFO - root - 2017-12-09 08:38:53.586355: step 6800, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.872 sec/batch; 78h:55m:32s remains)
2017-12-09 08:38:54.410873: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.013970933 0.014063722 0.014014505 0.013980715 0.013974745 0.01391773 0.013668645 0.013230392 0.012668185 0.012015859 0.011301484 0.010392651 0.0092225028 0.007836001 0.0063452446][0.014500306 0.014965497 0.015250895 0.015506465 0.015738366 0.015819356 0.015619108 0.015156088 0.014554109 0.013919568 0.013274671 0.012453454 0.01138224 0.010066303 0.0085990662][0.014314476 0.015224269 0.015956527 0.016646925 0.0172988 0.017734138 0.017788269 0.017431049 0.016796142 0.016030336 0.015193441 0.014235661 0.01311361 0.011841724 0.010488551][0.014616041 0.015988294 0.017152442 0.018227264 0.019197997 0.019889308 0.020109665 0.019782839 0.019054748 0.018084919 0.016997127 0.015788786 0.014474364 0.013101546 0.011740859][0.01472857 0.016540496 0.018097272 0.019470358 0.020625308 0.021417392 0.021693699 0.021370014 0.020569999 0.019456206 0.018140616 0.016641412 0.015042256 0.013461174 0.01200367][0.013670594 0.015796598 0.017664175 0.019255839 0.020516273 0.021364285 0.021662734 0.021340454 0.020476311 0.019228091 0.01768226 0.015908478 0.014055403 0.012316356 0.010814697][0.011079755 0.013261896 0.01529486 0.017086685 0.018558541 0.019584164 0.020014847 0.019786092 0.018907897 0.017528102 0.015749605 0.013728412 0.011670161 0.0098368628 0.0083594061][0.0077405139 0.009695027 0.011654205 0.013497067 0.015108394 0.016349059 0.016996799 0.016941966 0.016142245 0.014748235 0.012873012 0.010753275 0.0086682774 0.006884513 0.0055135358][0.0045684967 0.0060771103 0.0077246851 0.0093904156 0.010942775 0.012198504 0.012922818 0.0129914 0.012345493 0.011091172 0.0093515385 0.0074073714 0.0055499291 0.0040184604 0.0028970435][0.0022103437 0.0031582196 0.0043137148 0.0055759507 0.0068108705 0.0078412676 0.0084620612 0.0085539045 0.0080594737 0.0070559941 0.005687363 0.0042039757 0.0028532359 0.0017983553 0.0010780322][0.00086164841 0.0013387734 0.0019695465 0.0027035023 0.0034592708 0.00410666 0.0045035807 0.0045679631 0.0042557265 0.0036165486 0.0027723524 0.0018989162 0.0011487685 0.00059716444 0.00025061611][0.00021482515 0.00038630515 0.0006321916 0.00093874248 0.0012782381 0.0015779033 0.0017688437 0.0018028726 0.0016530722 0.0013508082 0.00096975284 0.00060190441 0.00030889516 0.00011454563 7.4712152e-06][1.6387625e-05 4.6214918e-05 8.9177105e-05 0.00015313979 0.00023509676 0.00031020824 0.00035553542 0.00035722938 0.00030931679 0.00022492168 0.00013529396 6.0074803e-05 6.3403131e-06 -2.206309e-05 -3.6420228e-05][-2.8539842e-05 -2.4603563e-05 -2.7407368e-05 -3.3753124e-05 -3.6986872e-05 -3.9139122e-05 -4.2347376e-05 -4.9167189e-05 -5.8041805e-05 -6.8019064e-05 -7.7832614e-05 -8.4286017e-05 -8.7953333e-05 -8.857256e-05 -8.7728535e-05][-2.0116553e-05 -1.5010228e-05 -1.9332263e-05 -2.1970467e-05 -2.6143214e-05 -2.6925103e-05 -2.7917835e-05 -3.2419208e-05 -4.0100225e-05 -5.1987365e-05 -6.6459477e-05 -7.7600285e-05 -8.5486507e-05 -8.9187022e-05 -9.1085734e-05]]...]
INFO - root - 2017-12-09 08:39:03.129657: step 6810, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.856 sec/batch; 77h:25m:01s remains)
INFO - root - 2017-12-09 08:39:11.708018: step 6820, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.858 sec/batch; 77h:39m:15s remains)
INFO - root - 2017-12-09 08:39:20.168815: step 6830, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 78h:08m:30s remains)
INFO - root - 2017-12-09 08:39:28.955253: step 6840, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.866 sec/batch; 78h:20m:50s remains)
INFO - root - 2017-12-09 08:39:37.559105: step 6850, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.841 sec/batch; 76h:03m:58s remains)
INFO - root - 2017-12-09 08:39:46.199478: step 6860, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.860 sec/batch; 77h:47m:22s remains)
INFO - root - 2017-12-09 08:39:54.586930: step 6870, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.846 sec/batch; 76h:32m:57s remains)
INFO - root - 2017-12-09 08:40:03.239546: step 6880, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.873 sec/batch; 78h:59m:41s remains)
INFO - root - 2017-12-09 08:40:11.814937: step 6890, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.843 sec/batch; 76h:15m:37s remains)
INFO - root - 2017-12-09 08:40:20.336687: step 6900, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.833 sec/batch; 75h:20m:19s remains)
2017-12-09 08:40:21.249040: I tensorflow/core/kernels/logging_ops.cc:79] [[[-5.5888369e-05 -0.00011142113 -0.00013039105 -0.00011274578 1.7796541e-05 0.00024711908 0.00057071459 0.00098295347 0.0013888713 0.0016734713 0.0018048242 0.0019233348 0.0020472247 0.002139976 0.0021441807][0.00051216874 0.00027279108 0.00010871951 5.3252443e-05 0.0002138241 0.000580256 0.0011287483 0.0018620321 0.0025850642 0.0031488012 0.0034998076 0.0038065373 0.0041477708 0.0044298759 0.004506432][0.0014045803 0.0012378668 0.0011469121 0.0012798508 0.0018000407 0.0026478725 0.0037073463 0.0049346965 0.0060025845 0.0067171995 0.0070900004 0.0073616756 0.0077293348 0.0080269324 0.0079855649][0.0020921065 0.0022985539 0.0028287785 0.0038864666 0.0055649797 0.0076253237 0.0096798446 0.01149952 0.012629055 0.012976479 0.012698536 0.012256596 0.012022038 0.011810006 0.011253971][0.003142854 0.0038368765 0.0053756428 0.0079723662 0.011510985 0.015314857 0.01862097 0.020989776 0.021796748 0.021094905 0.019315926 0.017298201 0.015638562 0.014235261 0.012739665][0.0041059037 0.0055560847 0.0084085837 0.012832292 0.018478056 0.024185557 0.028749418 0.031425469 0.031565312 0.0294328 0.025728399 0.021622689 0.017981883 0.014896825 0.012187932][0.0048761247 0.0071397512 0.011320829 0.017587302 0.025286023 0.032788549 0.038486283 0.04142243 0.040920209 0.037281651 0.031497762 0.025113026 0.019281233 0.014339028 0.010364972][0.0055149938 0.0086980173 0.014178883 0.022039043 0.031346317 0.040206991 0.046690054 0.049662068 0.04851415 0.043632157 0.036125638 0.027712673 0.019861769 0.013248401 0.00824773][0.006127 0.010139412 0.016745081 0.025806187 0.036189832 0.04580123 0.052537415 0.055289648 0.0535297 0.047671683 0.038870953 0.029009905 0.019772444 0.012033204 0.0064104768][0.0063389139 0.011044436 0.018459544 0.028302873 0.039200023 0.048998326 0.055609755 0.057962324 0.055592246 0.04900042 0.039377183 0.02870634 0.018819122 0.010674094 0.0049840282][0.0059909658 0.01109208 0.018894946 0.028913692 0.039707188 0.049132109 0.05520786 0.0569811 0.054061864 0.047067057 0.037245546 0.026602976 0.016930776 0.009140566 0.0038955712][0.0050390856 0.0099212741 0.017345844 0.026722487 0.036682356 0.045216814 0.050511546 0.05168153 0.048449937 0.041569307 0.032314397 0.022585647 0.013971576 0.0072424747 0.002889598][0.0037717649 0.0078196069 0.014032377 0.021835322 0.030029612 0.036967125 0.04112361 0.041737381 0.038625158 0.03258393 0.024811435 0.016918601 0.010147125 0.0050463839 0.0018903886][0.002500437 0.0054307343 0.0099715218 0.015672356 0.021616969 0.026568297 0.029421719 0.029629147 0.027075451 0.02244876 0.016712252 0.011069109 0.0063875094 0.0030123836 0.0010320364][0.0014151641 0.0032822515 0.0061874711 0.0098040625 0.013517689 0.016542478 0.018193251 0.018134696 0.016327029 0.013272349 0.0096246758 0.0061590564 0.0033919576 0.0014908098 0.00043725452]]...]
INFO - root - 2017-12-09 08:40:29.835415: step 6910, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.861 sec/batch; 77h:53m:37s remains)
INFO - root - 2017-12-09 08:40:38.230499: step 6920, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 78h:31m:33s remains)
INFO - root - 2017-12-09 08:40:46.754301: step 6930, loss = 0.82, batch loss = 0.69 (9.7 examples/sec; 0.826 sec/batch; 74h:41m:17s remains)
INFO - root - 2017-12-09 08:40:55.278847: step 6940, loss = 0.82, batch loss = 0.69 (9.7 examples/sec; 0.823 sec/batch; 74h:26m:32s remains)
INFO - root - 2017-12-09 08:41:03.835761: step 6950, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.860 sec/batch; 77h:45m:22s remains)
INFO - root - 2017-12-09 08:41:12.449535: step 6960, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.879 sec/batch; 79h:27m:27s remains)
INFO - root - 2017-12-09 08:41:20.906991: step 6970, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.855 sec/batch; 77h:17m:24s remains)
INFO - root - 2017-12-09 08:41:29.479126: step 6980, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.835 sec/batch; 75h:29m:07s remains)
INFO - root - 2017-12-09 08:41:38.022398: step 6990, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.876 sec/batch; 79h:10m:16s remains)
INFO - root - 2017-12-09 08:41:46.667877: step 7000, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.858 sec/batch; 77h:37m:17s remains)
2017-12-09 08:41:47.517071: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.00016368352 -0.00016482583 -0.00016663896 -0.00016856848 -0.00017051793 -0.0001720116 -0.00017308525 -0.00017382606 -0.00017403794 -0.0001736088 -0.00017226221 -0.0001702787 -0.00016795544 -0.00016550266 -0.00016339714][-0.00016414432 -0.0001655763 -0.0001675996 -0.00016972667 -0.000171834 -0.00017337219 -0.00017439009 -0.00017503665 -0.00017498802 -0.00017414549 -0.00017247547 -0.00017031675 -0.0001679757 -0.00016563552 -0.00016363182][-0.00016565574 -0.00016728131 -0.00016929315 -0.00017132888 -0.00017323047 -0.00017459151 -0.00017547254 -0.00017590288 -0.00017557055 -0.00017433103 -0.00017238159 -0.00017016985 -0.00016791988 -0.00016575704 -0.00016392207][-0.00016801532 -0.00016965432 -0.0001713996 -0.00017300506 -0.00017445165 -0.00017550538 -0.00017608977 -0.00017616931 -0.00017555003 -0.00017407855 -0.00017205229 -0.00016999611 -0.00016811459 -0.00016622245 -0.00016462199][-0.00017067447 -0.0001719454 -0.00017314209 -0.0001742962 -0.00017520104 -0.00017574572 -0.00017594019 -0.00017563495 -0.00017482127 -0.00017339032 -0.00017157887 -0.00016992869 -0.00016854916 -0.00016721025 -0.00016609421][-0.00017312594 -0.00017382126 -0.00017442991 -0.00017498704 -0.00017533789 -0.00017525142 -0.00017481561 -0.00017411394 -0.00017340784 -0.00017241671 -0.00017120827 -0.00017019105 -0.00016944893 -0.00016874232 -0.00016801362][-0.00017495392 -0.0001750778 -0.00017514863 -0.00017506102 -0.00017469788 -0.00017391659 -0.00017291051 -0.00017204027 -0.00017171371 -0.0001714793 -0.00017109557 -0.0001708473 -0.00017077124 -0.00017050799 -0.00016996931][-0.00017575843 -0.00017530911 -0.00017483842 -0.00017406512 -0.00017294395 -0.00017165592 -0.00017039329 -0.00016958371 -0.00016988169 -0.0001705629 -0.00017101862 -0.00017154223 -0.00017205636 -0.00017219757 -0.00017175938][-0.00017507255 -0.00017413183 -0.00017311094 -0.00017172055 -0.00017020266 -0.00016882138 -0.00016769511 -0.00016724343 -0.00016819943 -0.00016971484 -0.00017089302 -0.00017213613 -0.000173155 -0.00017360563 -0.00017308306][-0.00017327524 -0.00017187334 -0.00017042735 -0.00016879138 -0.00016728768 -0.0001661189 -0.00016540231 -0.000165526 -0.00016704723 -0.00016899685 -0.00017068132 -0.00017252094 -0.00017403081 -0.00017470449 -0.00017405313][-0.00017077538 -0.00016926957 -0.00016777107 -0.0001661456 -0.00016480972 -0.00016395135 -0.00016367395 -0.00016415947 -0.00016583908 -0.00016800487 -0.0001701502 -0.00017235221 -0.00017406765 -0.00017477278 -0.0001740305][-0.00016796737 -0.00016672329 -0.00016548278 -0.00016405038 -0.00016292858 -0.00016239619 -0.0001624226 -0.00016307927 -0.00016473832 -0.00016691533 -0.00016925635 -0.00017152421 -0.00017325324 -0.00017390616 -0.00017301127][-0.00016512853 -0.00016422757 -0.00016334458 -0.00016228802 -0.00016162734 -0.00016146124 -0.00016158749 -0.00016216187 -0.00016363834 -0.00016557096 -0.00016760491 -0.00016961736 -0.00017119481 -0.0001717764 -0.00017093711][-0.00016270642 -0.00016209134 -0.00016153938 -0.000160939 -0.00016071956 -0.00016076386 -0.00016091013 -0.00016131457 -0.00016239846 -0.00016374869 -0.0001651011 -0.00016652379 -0.00016787708 -0.00016841217 -0.0001677909][-0.00016096307 -0.00016047932 -0.00016021266 -0.0001599563 -0.00016001351 -0.00016014489 -0.00016020219 -0.00016035262 -0.00016091412 -0.0001616621 -0.00016248606 -0.00016346942 -0.00016445259 -0.00016487119 -0.00016450451]]...]
INFO - root - 2017-12-09 08:41:56.124322: step 7010, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 78h:09m:01s remains)
INFO - root - 2017-12-09 08:42:04.716712: step 7020, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.888 sec/batch; 80h:15m:14s remains)
INFO - root - 2017-12-09 08:42:13.395204: step 7030, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 78h:45m:56s remains)
INFO - root - 2017-12-09 08:42:21.855571: step 7040, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.872 sec/batch; 78h:51m:51s remains)
INFO - root - 2017-12-09 08:42:30.559011: step 7050, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.878 sec/batch; 79h:24m:06s remains)
INFO - root - 2017-12-09 08:42:39.286259: step 7060, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 78h:33m:55s remains)
INFO - root - 2017-12-09 08:42:47.844946: step 7070, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.882 sec/batch; 79h:44m:06s remains)
INFO - root - 2017-12-09 08:42:56.494140: step 7080, loss = 0.82, batch loss = 0.69 (10.0 examples/sec; 0.799 sec/batch; 72h:14m:13s remains)
INFO - root - 2017-12-09 08:43:05.196275: step 7090, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.858 sec/batch; 77h:31m:03s remains)
INFO - root - 2017-12-09 08:43:13.936917: step 7100, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.883 sec/batch; 79h:51m:15s remains)
2017-12-09 08:43:14.805786: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.00016986372 -0.00016873502 -0.00016885527 -0.00016895194 -0.00016872719 -0.00016841872 -0.00016833787 -0.00016861863 -0.00016896048 -0.00016920763 -0.00016925044 -0.00016898863 -0.00016832849 -0.00016720177 -0.00016601107][-0.00016988869 -0.00016893866 -0.00016948274 -0.0001699447 -0.0001701108 -0.00017020605 -0.00017055284 -0.00017106737 -0.00017153591 -0.00017184399 -0.00017202237 -0.00017165362 -0.00017042636 -0.00016861499 -0.00016680264][-0.00017042486 -0.00016984847 -0.00017089456 -0.00017184753 -0.00017252282 -0.00017304948 -0.00017382711 -0.00017485977 -0.00017565423 -0.00017597931 -0.00017596685 -0.00017538242 -0.00017378171 -0.00017140343 -0.00016891242][-0.00017119986 -0.00017082355 -0.00017214246 -0.00017354038 -0.0001746479 -0.00017556756 -0.00017682246 -0.00017844369 -0.00017959152 -0.00018003711 -0.00017986735 -0.00017907635 -0.00017734768 -0.00017472741 -0.00017175959][-0.00017173972 -0.00017151117 -0.000172939 -0.00017459583 -0.00017603976 -0.0001772392 -0.00017883413 -0.0001809815 -0.00018263934 -0.00018341964 -0.00018334734 -0.00018248038 -0.00018068924 -0.0001780103 -0.00017474429][-0.00017219578 -0.00017191557 -0.00017345362 -0.00017515029 -0.00017670427 -0.00017813579 -0.00017998737 -0.00018226577 -0.00018417924 -0.00018533874 -0.00018576888 -0.0001852961 -0.00018355796 -0.00018085759 -0.00017757728][-0.0001734609 -0.0001730306 -0.00017445603 -0.00017593165 -0.00017728763 -0.00017861871 -0.00018027553 -0.00018229903 -0.00018391742 -0.00018537838 -0.00018644787 -0.00018673785 -0.00018557467 -0.00018332801 -0.00018050283][-0.00017598664 -0.00017531322 -0.00017604599 -0.0001768728 -0.00017780799 -0.00017902869 -0.00018048071 -0.00018210067 -0.00018346676 -0.00018500554 -0.00018643466 -0.00018714156 -0.00018672309 -0.00018522039 -0.00018309707][-0.00017841377 -0.00017735957 -0.00017735828 -0.00017757504 -0.00017823205 -0.000179549 -0.00018134924 -0.00018332354 -0.00018493788 -0.00018632037 -0.00018729692 -0.00018777477 -0.00018750367 -0.00018649972 -0.00018478381][-0.00017981972 -0.00017853016 -0.0001780341 -0.00017810879 -0.00017888407 -0.0001805667 -0.00018275771 -0.00018510778 -0.00018680797 -0.00018789138 -0.00018862105 -0.00018873821 -0.000188111 -0.00018708475 -0.00018563695][-0.00018068646 -0.00017916293 -0.00017841328 -0.00017841288 -0.00017915413 -0.00018088106 -0.00018305647 -0.00018528945 -0.00018693785 -0.00018790118 -0.00018870409 -0.0001889416 -0.00018835832 -0.00018734181 -0.00018589389][-0.00018147894 -0.00017983156 -0.00017897552 -0.000178797 -0.00017916456 -0.0001802857 -0.00018203715 -0.00018410049 -0.00018583002 -0.00018709034 -0.00018790383 -0.00018838749 -0.00018827597 -0.00018759204 -0.00018607193][-0.00018198654 -0.00018030024 -0.00017933939 -0.00017893891 -0.00017889083 -0.00017915056 -0.00018031582 -0.0001822967 -0.00018401234 -0.00018537765 -0.00018654788 -0.00018735985 -0.00018755243 -0.00018722471 -0.00018604384][-0.00018173149 -0.00018001602 -0.00017896036 -0.00017838317 -0.00017807183 -0.00017782567 -0.00017809175 -0.00017929646 -0.00018084992 -0.00018234245 -0.00018393836 -0.00018528801 -0.00018595719 -0.00018600872 -0.00018533769][-0.00018047745 -0.00017868973 -0.00017768591 -0.00017685739 -0.00017629149 -0.00017583098 -0.00017541839 -0.00017549031 -0.00017631851 -0.00017798087 -0.0001803614 -0.00018248397 -0.00018401009 -0.00018466749 -0.00018430172]]...]
INFO - root - 2017-12-09 08:43:23.535274: step 7110, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.903 sec/batch; 81h:39m:47s remains)
INFO - root - 2017-12-09 08:43:32.024312: step 7120, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 78h:28m:13s remains)
INFO - root - 2017-12-09 08:43:40.637316: step 7130, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.878 sec/batch; 79h:19m:59s remains)
INFO - root - 2017-12-09 08:43:49.060717: step 7140, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.878 sec/batch; 79h:18m:57s remains)
INFO - root - 2017-12-09 08:43:57.760095: step 7150, loss = 0.82, batch loss = 0.69 (8.8 examples/sec; 0.907 sec/batch; 81h:58m:49s remains)
INFO - root - 2017-12-09 08:44:06.351636: step 7160, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.858 sec/batch; 77h:29m:39s remains)
INFO - root - 2017-12-09 08:44:14.931514: step 7170, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.865 sec/batch; 78h:09m:39s remains)
INFO - root - 2017-12-09 08:44:23.649233: step 7180, loss = 0.82, batch loss = 0.69 (8.4 examples/sec; 0.954 sec/batch; 86h:11m:49s remains)
INFO - root - 2017-12-09 08:44:32.223219: step 7190, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.850 sec/batch; 76h:46m:15s remains)
INFO - root - 2017-12-09 08:44:40.947811: step 7200, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.854 sec/batch; 77h:07m:58s remains)
2017-12-09 08:44:41.840844: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.017764851 0.016828988 0.015360768 0.013844424 0.012326418 0.010830611 0.0097807189 0.0091583161 0.0087582329 0.00848652 0.0082634529 0.0082480647 0.00819238 0.00799224 0.0076256981][0.018043764 0.017058069 0.015427758 0.013742818 0.012040325 0.010396264 0.0091707222 0.008335242 0.0078291688 0.0075742272 0.0074228658 0.0076114349 0.0077620111 0.007943253 0.0079626739][0.018167106 0.017157281 0.015420659 0.013536946 0.011635435 0.0098221879 0.0083636083 0.0072410074 0.0065024989 0.0061168973 0.0059521003 0.006341903 0.0067962543 0.0074152611 0.0078879362][0.018123323 0.017228497 0.015467179 0.013442365 0.011363309 0.0093861073 0.0077050654 0.0062991623 0.0053491364 0.0048225145 0.0046665804 0.0050123143 0.005458456 0.0062827566 0.0071314438][0.018057974 0.017403565 0.015771246 0.013756724 0.011601424 0.0094843395 0.0075492533 0.0057867104 0.004486742 0.0037194265 0.0034453264 0.0037287094 0.0043021324 0.0052665179 0.0064350651][0.018229751 0.0177665 0.016306177 0.014481931 0.012432709 0.010228713 0.0079945764 0.0058656619 0.0041880435 0.0031836573 0.0027207716 0.0028740091 0.0033439773 0.0044855028 0.0059224307][0.018053198 0.017952627 0.0167898 0.015207479 0.013331613 0.011170273 0.008750814 0.0063015469 0.0043295873 0.0031186657 0.0025552453 0.0026351775 0.0030757091 0.0043217205 0.0059142597][0.017709278 0.01776091 0.016748516 0.015397916 0.013755819 0.011815337 0.0094395783 0.0068364209 0.004622994 0.0032653946 0.002606963 0.0026143542 0.0031250131 0.0044643404 0.0061734482][0.017208241 0.0173713 0.016472226 0.015352984 0.013941375 0.012217076 0.0099862041 0.0075534903 0.0053950632 0.0039071175 0.0031017922 0.0030096909 0.0034789785 0.0047571822 0.0064572245][0.016483428 0.016732002 0.015995618 0.015136354 0.014052825 0.012672365 0.010709033 0.0084916912 0.0063829022 0.0048239995 0.0038158589 0.0036012109 0.0040520187 0.0052671428 0.0068510352][0.015573301 0.015860394 0.015229694 0.014583735 0.013810603 0.01283217 0.011295993 0.0094523113 0.0075628608 0.006060252 0.0049769534 0.004648752 0.004979752 0.0060988739 0.0074612433][0.014626515 0.014908525 0.014340628 0.013790091 0.013193286 0.01248712 0.011336429 0.0098728836 0.0083120791 0.0070128781 0.0060303905 0.0057013421 0.0060195136 0.006977547 0.0080903936][0.01346206 0.01373369 0.013205242 0.012730218 0.012267886 0.011736186 0.010878749 0.0097528268 0.00847682 0.0074064266 0.0065637254 0.0062712305 0.0065426184 0.0073326137 0.0082215052][0.011785042 0.012075977 0.011658384 0.011294248 0.010953555 0.010539977 0.0098919477 0.0090659484 0.00810922 0.0072519537 0.0065121697 0.0062278407 0.0063881851 0.0069532692 0.0075877281][0.009611845 0.00991348 0.0096072629 0.0093364539 0.009084601 0.0087722484 0.0083046807 0.0076784994 0.006944675 0.006303072 0.0057160938 0.005431775 0.0054595382 0.0058126417 0.0062028216]]...]
INFO - root - 2017-12-09 08:44:50.629483: step 7210, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 78h:41m:58s remains)
INFO - root - 2017-12-09 08:44:59.230969: step 7220, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.870 sec/batch; 78h:35m:32s remains)
INFO - root - 2017-12-09 08:45:07.912795: step 7230, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.856 sec/batch; 77h:20m:55s remains)
INFO - root - 2017-12-09 08:45:16.529443: step 7240, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.866 sec/batch; 78h:13m:14s remains)
INFO - root - 2017-12-09 08:45:25.152364: step 7250, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.856 sec/batch; 77h:21m:42s remains)
INFO - root - 2017-12-09 08:45:33.818356: step 7260, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.893 sec/batch; 80h:38m:21s remains)
INFO - root - 2017-12-09 08:45:42.347564: step 7270, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 78h:18m:16s remains)
INFO - root - 2017-12-09 08:45:50.920334: step 7280, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.836 sec/batch; 75h:30m:24s remains)
INFO - root - 2017-12-09 08:45:59.499163: step 7290, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.851 sec/batch; 76h:50m:47s remains)
INFO - root - 2017-12-09 08:46:08.080725: step 7300, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.876 sec/batch; 79h:09m:00s remains)
2017-12-09 08:46:08.931972: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.00017541343 -0.00017299864 -0.00016950008 -0.00016456479 -0.0001589512 -0.0001539537 -0.0001520068 -0.00015270055 -0.00015381673 -0.0001588119 -0.00016449887 -0.00016850194 -0.00017102239 -0.00017397253 -0.00017484263][-0.00015984071 -0.00014273429 -0.00012207044 -0.00010156749 -9.0271475e-05 -9.1798509e-05 -0.00010172416 -0.00011349189 -0.00012000258 -0.00013229254 -0.00014611232 -0.00015619755 -0.0001642665 -0.00017128914 -0.00017403028][-8.9438108e-05 -9.6129661e-06 8.4781175e-05 0.0001656875 0.00019494459 0.00016077133 9.2259783e-05 2.6366062e-05 -2.0215739e-05 -6.2422987e-05 -9.9800855e-05 -0.00012466825 -0.00014430837 -0.00016052107 -0.00016941079][6.0178747e-05 0.00026987676 0.00051814958 0.00072440977 0.00080021867 0.00072553061 0.00057086773 0.00041526026 0.00028157581 0.00015822996 4.6552654e-05 -3.2826036e-05 -9.2655479e-05 -0.00013657288 -0.00016147966][0.00029106834 0.00069310406 0.0011671521 0.0015588197 0.0017151149 0.0016076333 0.0013584407 0.0010897239 0.00082526007 0.00055659923 0.00030423852 0.00011918775 -1.4925652e-05 -0.00010497895 -0.00015287161][0.00060903106 0.0012420465 0.0019776607 0.0025793018 0.0028272169 0.0026755589 0.0023140342 0.0019103212 0.0014894628 0.0010426412 0.00061231491 0.0002884795 6.1559258e-05 -7.9097561e-05 -0.00014675659][0.0010202046 0.0019027013 0.0029034726 0.0037025623 0.0040222169 0.003793465 0.0032838141 0.0027157925 0.0021234131 0.0015041436 0.0009093181 0.00045385293 0.00013297226 -5.8615624e-05 -0.00014288897][0.0014951294 0.0026206698 0.0038729496 0.0048542996 0.0052334918 0.0049101971 0.0042276583 0.0034701235 0.0026868032 0.0018972692 0.0011542033 0.00059033639 0.0001977268 -3.6963873e-05 -0.00013888162][0.001863936 0.0031354146 0.0045389915 0.0056389589 0.0060726423 0.0057145474 0.0049408511 0.0040647108 0.0031522589 0.0022336086 0.0013693762 0.000712062 0.00025651796 -1.6620572e-05 -0.00013693757][0.0020248713 0.003288442 0.0046717492 0.0057634106 0.0062063886 0.0058563482 0.0050738221 0.0041684066 0.0032293296 0.0022851648 0.001397674 0.00072094915 0.00025287468 -2.3526605e-05 -0.00014108772][0.0019422688 0.0030483035 0.0042285784 0.0051538935 0.0055299173 0.0052012322 0.0044792537 0.0036398915 0.002787306 0.0019561714 0.0011851961 0.00059368095 0.00018189961 -5.8208636e-05 -0.00015182859][0.001607438 0.0024807467 0.0033778534 0.0040635997 0.0043299962 0.0040366114 0.0034242726 0.0027146146 0.0020201807 0.0013836313 0.00081602274 0.00038787886 8.4578118e-05 -9.3986804e-05 -0.00015993493][0.001072677 0.0016719826 0.0022694394 0.0027158656 0.0028778526 0.0026550675 0.0022087742 0.0016919337 0.001201423 0.00078281842 0.00042981387 0.00017825246 -7.9172605e-07 -0.00011498201 -0.00016360593][0.00046945986 0.00079916418 0.0011272609 0.0013785502 0.0014820966 0.0013787431 0.001144642 0.00086065358 0.00058462343 0.00035078725 0.00015673193 2.6298803e-05 -6.7611458e-05 -0.00013076022 -0.00016472381][4.4912245e-05 0.00017138937 0.00030413282 0.00041314465 0.0004660784 0.00043786759 0.00035382522 0.00024304475 0.00013116273 3.6462108e-05 -4.2655607e-05 -9.0928312e-05 -0.0001254403 -0.00015074007 -0.00016730376]]...]
INFO - root - 2017-12-09 08:46:17.561298: step 7310, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 78h:30m:05s remains)
INFO - root - 2017-12-09 08:46:26.186199: step 7320, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 78h:38m:37s remains)
INFO - root - 2017-12-09 08:46:34.924320: step 7330, loss = 0.82, batch loss = 0.69 (8.6 examples/sec; 0.928 sec/batch; 83h:47m:45s remains)
INFO - root - 2017-12-09 08:46:43.553674: step 7340, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.852 sec/batch; 76h:58m:52s remains)
INFO - root - 2017-12-09 08:46:52.242075: step 7350, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.870 sec/batch; 78h:34m:35s remains)
INFO - root - 2017-12-09 08:47:00.959635: step 7360, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.841 sec/batch; 75h:56m:39s remains)
INFO - root - 2017-12-09 08:47:09.465835: step 7370, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.834 sec/batch; 75h:20m:03s remains)
INFO - root - 2017-12-09 08:47:18.158112: step 7380, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.879 sec/batch; 79h:21m:06s remains)
INFO - root - 2017-12-09 08:47:26.838597: step 7390, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.870 sec/batch; 78h:34m:22s remains)
INFO - root - 2017-12-09 08:47:35.439304: step 7400, loss = 0.82, batch loss = 0.69 (9.7 examples/sec; 0.828 sec/batch; 74h:45m:55s remains)
2017-12-09 08:47:36.321226: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.00016963731 -0.00014698954 -9.9114877e-05 -2.55411e-05 6.2571882e-05 0.00014912045 0.00021445887 0.00025390845 0.00026893208 0.00025858084 0.00021404815 0.00013360089 3.3675853e-05 -6.0623934e-05 -0.00012851681][-0.00017284717 -0.0001342365 -5.588826e-05 6.9490794e-05 0.00023310383 0.00040850614 0.00055549695 0.00065823994 0.00070256303 0.00068646623 0.00059935945 0.00044446706 0.00024614827 5.1297087e-05 -9.1798494e-05][-0.00016000585 -8.960822e-05 5.6786565e-05 0.00029584509 0.00060855318 0.00093978172 0.0012121755 0.0013889153 0.0014567212 0.001417966 0.0012634887 0.00099511945 0.00064426428 0.0002914631 1.8501029e-05][-0.00014545205 -3.8682992e-05 0.00018687676 0.00055504643 0.0010330046 0.0015346947 0.0019424356 0.0021938526 0.0022678059 0.0021728736 0.0019055045 0.0014916675 0.00099548046 0.00051480881 0.00013860319][-0.00014191624 -1.5442638e-05 0.0002492441 0.00067163026 0.0012142349 0.0017850975 0.0022660969 0.00258658 0.0027039386 0.0026010617 0.0022644978 0.0017518416 0.0011670422 0.00062636781 0.00021093119][-0.00015059576 -3.889724e-05 0.00018920626 0.0005452417 0.0010017277 0.0014954204 0.0019441011 0.0022866714 0.0024614311 0.0024165667 0.0021162541 0.0016278489 0.0010779492 0.00058188022 0.00020577257][-0.00015569135 -6.4966051e-05 0.00011042079 0.00037253828 0.00070129114 0.0010582926 0.0013947473 0.0016735799 0.0018411236 0.00184281 0.0016331914 0.0012581277 0.000827452 0.00044068351 0.00014519611][-0.00016330021 -9.29692e-05 4.1995358e-05 0.00023973196 0.00047920772 0.0007290364 0.000949918 0.001119433 0.0012161746 0.0012139259 0.001073862 0.00081666705 0.00051939464 0.00025287009 4.8713075e-05][-0.00016855687 -0.00011278951 -3.033987e-06 0.00015693709 0.00033956103 0.00051130855 0.00063432538 0.00069391605 0.00069621718 0.00065307523 0.00054803083 0.00038516958 0.00020685756 5.3989192e-05 -6.0141567e-05][-0.00017986933 -0.00014845858 -8.4450163e-05 1.0018819e-05 0.00011648679 0.00021150611 0.00027215848 0.00029179628 0.0002730597 0.00022851174 0.00015553182 6.017492e-05 -3.1972566e-05 -9.9721139e-05 -0.00014273771][-0.00018493303 -0.00017333394 -0.00014960053 -0.00011365657 -7.0709284e-05 -2.7705784e-05 6.8063673e-06 2.6455091e-05 2.3673376e-05 -1.9425497e-06 -4.8654823e-05 -0.00010400244 -0.0001478599 -0.00017070169 -0.0001788075][-0.00018723871 -0.00018683008 -0.00018590974 -0.00018104966 -0.00016838072 -0.00014438374 -0.00011066112 -7.8089375e-05 -6.151422e-05 -7.2379669e-05 -0.00010660429 -0.00014744719 -0.00017422887 -0.00018280844 -0.00018315861][-0.00018623979 -0.00018644026 -0.00018726783 -0.00018539168 -0.00017646514 -0.00015653625 -0.00012752332 -9.9527555e-05 -8.6315093e-05 -9.6922653e-05 -0.00012678109 -0.0001593871 -0.00017856757 -0.00018402071 -0.00018382209][-0.00018553346 -0.00018550281 -0.0001862917 -0.00018529933 -0.00017945105 -0.00016661463 -0.00014833649 -0.00013153128 -0.00012480133 -0.00013304139 -0.00015202785 -0.00017085727 -0.00018113575 -0.00018373378 -0.00018334642][-0.00018505585 -0.00018470515 -0.00018536026 -0.00018556442 -0.00018397035 -0.00018011651 -0.00017449375 -0.00016916954 -0.00016718361 -0.00017014853 -0.00017607202 -0.00018119802 -0.00018343692 -0.00018355364 -0.00018308964]]...]
INFO - root - 2017-12-09 08:47:44.925618: step 7410, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.860 sec/batch; 77h:42m:11s remains)
INFO - root - 2017-12-09 08:47:53.457626: step 7420, loss = 0.82, batch loss = 0.69 (9.8 examples/sec; 0.816 sec/batch; 73h:38m:23s remains)
INFO - root - 2017-12-09 08:48:01.915621: step 7430, loss = 0.82, batch loss = 0.69 (9.8 examples/sec; 0.813 sec/batch; 73h:24m:27s remains)
INFO - root - 2017-12-09 08:48:10.309668: step 7440, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.833 sec/batch; 75h:11m:02s remains)
INFO - root - 2017-12-09 08:48:18.922294: step 7450, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.859 sec/batch; 77h:30m:56s remains)
INFO - root - 2017-12-09 08:48:27.535556: step 7460, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.851 sec/batch; 76h:51m:08s remains)
INFO - root - 2017-12-09 08:48:36.022091: step 7470, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.876 sec/batch; 79h:04m:19s remains)
INFO - root - 2017-12-09 08:48:44.784157: step 7480, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.854 sec/batch; 77h:08m:29s remains)
INFO - root - 2017-12-09 08:48:53.294644: step 7490, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.853 sec/batch; 76h:59m:11s remains)
INFO - root - 2017-12-09 08:49:01.966201: step 7500, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.894 sec/batch; 80h:41m:46s remains)
2017-12-09 08:49:02.789457: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.11998125 0.12364488 0.12590192 0.12672994 0.12665381 0.12596695 0.12530693 0.12450797 0.12358832 0.12253583 0.12060568 0.11780509 0.113539 0.1079808 0.10128226][0.13323802 0.13825928 0.141406 0.14274475 0.14280957 0.14187223 0.14088067 0.13976069 0.13868377 0.13771468 0.13616608 0.13412739 0.13049108 0.12544337 0.11875641][0.14064814 0.14663938 0.15061212 0.15253361 0.15298367 0.15204141 0.15063849 0.14913891 0.14783131 0.14691865 0.14551269 0.14385222 0.14120981 0.13714494 0.13096441][0.14325789 0.14987172 0.15456727 0.15692274 0.1572331 0.15600866 0.15425165 0.15222567 0.15043175 0.14915362 0.14782947 0.14673872 0.14508526 0.14244822 0.13744603][0.14347097 0.15031037 0.15521358 0.15783322 0.15815392 0.15630585 0.15372534 0.1509601 0.1485105 0.14670861 0.14524731 0.14479527 0.14445741 0.14337638 0.13988574][0.14297223 0.15026127 0.15519954 0.15763807 0.1574406 0.15481088 0.15106213 0.14696646 0.14329667 0.14084075 0.13940768 0.13962698 0.14057587 0.14112595 0.13893443][0.14074093 0.14793091 0.15257947 0.15450092 0.15349129 0.14977691 0.14484936 0.1395565 0.13481554 0.1317358 0.13036431 0.1314756 0.13365798 0.13539372 0.13412802][0.13489978 0.14185531 0.14595273 0.14699529 0.14465673 0.13946795 0.13317923 0.12667757 0.12098673 0.11749335 0.11631421 0.11819895 0.12150963 0.12401262 0.12332212][0.12554426 0.13201593 0.13504611 0.13471378 0.13063239 0.1236773 0.11592072 0.10856234 0.10248118 0.098989852 0.098649509 0.10157139 0.10568722 0.10849619 0.10826208][0.11282254 0.11850587 0.12051291 0.11896161 0.11364275 0.10554146 0.096732885 0.088859208 0.082832143 0.079945005 0.080490023 0.084341481 0.088997133 0.091995351 0.091944173][0.0993631 0.10445309 0.10600062 0.10399782 0.098306611 0.090142563 0.081234373 0.073446043 0.067639358 0.06535773 0.066478774 0.0705119 0.07507991 0.07810095 0.0784028][0.087071985 0.091692708 0.093123458 0.091342859 0.086250953 0.078867093 0.070650071 0.063556738 0.058361486 0.056309458 0.057341445 0.060974266 0.065043978 0.06772244 0.068178669][0.077050231 0.0811859 0.082443543 0.080970146 0.076786272 0.070624225 0.063631311 0.057567865 0.053025991 0.051182147 0.051882453 0.054684255 0.058070451 0.060466006 0.06134107][0.067063585 0.07079877 0.071980052 0.071015768 0.06803292 0.063455582 0.058079366 0.053144075 0.049469594 0.047911983 0.048470724 0.050717421 0.053485561 0.055629428 0.056629367][0.0560601 0.059195042 0.060329419 0.060016666 0.058267511 0.055251803 0.051439784 0.047814842 0.04527019 0.044349097 0.044987667 0.046964291 0.049347959 0.051261157 0.052159581]]...]
INFO - root - 2017-12-09 08:49:11.242296: step 7510, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.873 sec/batch; 78h:49m:41s remains)
INFO - root - 2017-12-09 08:49:19.633258: step 7520, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.830 sec/batch; 74h:56m:44s remains)
INFO - root - 2017-12-09 08:49:28.250170: step 7530, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.848 sec/batch; 76h:31m:21s remains)
INFO - root - 2017-12-09 08:49:36.381007: step 7540, loss = 0.82, batch loss = 0.69 (9.9 examples/sec; 0.809 sec/batch; 73h:00m:40s remains)
INFO - root - 2017-12-09 08:49:44.901869: step 7550, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.841 sec/batch; 75h:55m:34s remains)
INFO - root - 2017-12-09 08:49:53.481008: step 7560, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.865 sec/batch; 78h:03m:43s remains)
INFO - root - 2017-12-09 08:50:02.115055: step 7570, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.861 sec/batch; 77h:45m:05s remains)
INFO - root - 2017-12-09 08:50:10.810929: step 7580, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.863 sec/batch; 77h:50m:51s remains)
INFO - root - 2017-12-09 08:50:19.480608: step 7590, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 78h:27m:26s remains)
INFO - root - 2017-12-09 08:50:28.304342: step 7600, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.897 sec/batch; 80h:57m:32s remains)
2017-12-09 08:50:29.174168: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.0018677685 0.0025853892 0.003564172 0.0047301492 0.0059856391 0.0072151241 0.0083617475 0.0094729112 0.010570085 0.011655722 0.012490171 0.012911008 0.012617468 0.011457142 0.0094375052][0.0027098642 0.003674285 0.0049221157 0.0063272305 0.0077591152 0.0091084382 0.010324243 0.011434852 0.012455733 0.0133778 0.013977502 0.014132771 0.013584911 0.012200298 0.0099704852][0.0036711106 0.0049910648 0.0066008596 0.0083160391 0.00997036 0.011482309 0.012825256 0.014011114 0.014994663 0.015708493 0.015953081 0.015626477 0.014572621 0.01274493 0.010188844][0.0044185002 0.006136809 0.0081899147 0.010274187 0.012172349 0.013808637 0.015196516 0.01639461 0.017301025 0.017836878 0.017788578 0.017018409 0.015438988 0.013083283 0.010157909][0.0050867628 0.0071705948 0.0096321795 0.012109049 0.014335681 0.016178042 0.017647546 0.018770542 0.019471839 0.019724563 0.01934997 0.018215198 0.016238078 0.013484438 0.010223847][0.0056820763 0.0081348643 0.011021248 0.013897148 0.016439948 0.018505614 0.020075686 0.021162361 0.021723835 0.021737983 0.021055149 0.019572241 0.017234825 0.014146454 0.010618043][0.0059067477 0.0086945361 0.012019301 0.015357456 0.018287322 0.020594092 0.0222357 0.023270637 0.023696143 0.023548454 0.022736441 0.021125464 0.018656462 0.015424903 0.011750726][0.0057809986 0.0087261675 0.012338021 0.016038312 0.019312903 0.021866009 0.023636874 0.02469366 0.025069008 0.024865095 0.024015045 0.02241605 0.019984107 0.016833896 0.013229951][0.0054759071 0.0084005883 0.012028278 0.015807522 0.019210353 0.021879382 0.023749923 0.024878146 0.025330139 0.025215307 0.024488064 0.023079611 0.020882417 0.018035671 0.014782926][0.0049958676 0.007800085 0.011275874 0.014915648 0.018221073 0.020835904 0.022710135 0.023889003 0.024452092 0.024506668 0.024021067 0.022913968 0.021105174 0.01874076 0.016018916][0.0042955396 0.0068782764 0.010071567 0.013431123 0.016487384 0.018889101 0.020603182 0.02168352 0.022237273 0.022385646 0.022126334 0.021394536 0.020105934 0.018349785 0.016326515][0.0034134705 0.0056347675 0.0084166536 0.01137826 0.014075221 0.016183035 0.017658923 0.018561298 0.019025097 0.019170741 0.019035256 0.01857912 0.01774757 0.016592165 0.015255461][0.0024463795 0.0041740108 0.0064303824 0.0089138029 0.011191976 0.012954773 0.014120992 0.014762934 0.01506507 0.015160124 0.015115365 0.014898323 0.014460056 0.013808775 0.01303534][0.001438632 0.0026198942 0.00425216 0.0061466023 0.0079461532 0.0093530351 0.010240224 0.010655607 0.010784566 0.01077375 0.010731929 0.010649705 0.01048623 0.010221147 0.0098846508][0.00061937206 0.0013080656 0.0023276452 0.0035793814 0.0048222826 0.005809322 0.0064109229 0.0066509885 0.0066774283 0.0066219913 0.0065832259 0.006563013 0.0065372172 0.0064757718 0.0063839038]]...]
INFO - root - 2017-12-09 08:50:37.774307: step 7610, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.875 sec/batch; 79h:00m:07s remains)
INFO - root - 2017-12-09 08:50:46.377656: step 7620, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.883 sec/batch; 79h:42m:42s remains)
INFO - root - 2017-12-09 08:50:55.083167: step 7630, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.844 sec/batch; 76h:08m:19s remains)
INFO - root - 2017-12-09 08:51:03.692538: step 7640, loss = 0.82, batch loss = 0.69 (9.9 examples/sec; 0.808 sec/batch; 72h:57m:21s remains)
INFO - root - 2017-12-09 08:51:12.285766: step 7650, loss = 0.82, batch loss = 0.69 (9.7 examples/sec; 0.829 sec/batch; 74h:47m:55s remains)
INFO - root - 2017-12-09 08:51:20.986266: step 7660, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.879 sec/batch; 79h:19m:59s remains)
INFO - root - 2017-12-09 08:51:29.559919: step 7670, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.889 sec/batch; 80h:14m:12s remains)
INFO - root - 2017-12-09 08:51:38.249190: step 7680, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.878 sec/batch; 79h:10m:38s remains)
INFO - root - 2017-12-09 08:51:47.023488: step 7690, loss = 0.82, batch loss = 0.69 (8.3 examples/sec; 0.969 sec/batch; 87h:23m:42s remains)
INFO - root - 2017-12-09 08:51:55.755984: step 7700, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.890 sec/batch; 80h:17m:08s remains)
2017-12-09 08:51:56.601049: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.0023327894 0.0022515815 0.0022780313 0.001857411 0.0015332728 0.0010319153 0.00062101218 0.00035637224 8.543086e-05 -2.0123611e-05 -6.9798058e-05 -0.0001649567 -0.00025652326 -0.00027914534 -0.00028080682][0.005231922 0.00463365 0.0038555469 0.0027765087 0.0018159762 0.00093382446 0.00043017531 0.000317649 0.00032741172 0.00038135887 0.00031252686 6.1779516e-05 -0.0001640226 -0.00026935356 -0.000281433][0.0090910941 0.0086206812 0.0076041911 0.0059578372 0.004041994 0.0021861305 0.0010231849 0.00069544232 0.00088131835 0.0011088157 0.0010315015 0.00051982864 1.8042629e-05 -0.00024232791 -0.00028119588][0.012717361 0.013035838 0.01236346 0.010794852 0.0081039388 0.0052343411 0.0030443196 0.001878807 0.0017156372 0.0019006747 0.0017679475 0.0010026377 0.00022417351 -0.0002102533 -0.00028169443][0.016981948 0.017635435 0.017139789 0.015773447 0.012958997 0.00931265 0.0059976256 0.00371528 0.0027112002 0.0024478475 0.0020938602 0.001186832 0.00030791087 -0.0001965937 -0.0002796473][0.020461192 0.022150408 0.021831131 0.020391671 0.017518397 0.013650056 0.0095685571 0.0060049356 0.0037445221 0.0026630168 0.0019226872 0.0010009437 0.00021906276 -0.00021194559 -0.00027685685][0.022852691 0.025013141 0.02523846 0.024289688 0.02156114 0.017441347 0.012669107 0.0080351718 0.004584169 0.0025474366 0.0014466601 0.0006341486 6.8002177e-05 -0.00023244912 -0.00027483256][0.025003215 0.027282642 0.0273328 0.026349597 0.023926819 0.020138286 0.015102765 0.00964028 0.0050489674 0.0021964707 0.00087154435 0.0002588839 -7.6416545e-05 -0.00025085002 -0.00027489645][0.026648888 0.029464828 0.029468099 0.028340058 0.025748933 0.021877352 0.016812088 0.010955252 0.0055171158 0.0019732765 0.00044341802 -6.3367479e-06 -0.00018016488 -0.00026430615 -0.00027461679][0.028079607 0.0314992 0.031281259 0.030183211 0.027367149 0.02321073 0.017840235 0.011608234 0.0057602753 0.0018444309 0.00016590685 -0.00015947109 -0.00023555047 -0.00026837309 -0.00027188976][0.029221388 0.033185147 0.032942224 0.032017495 0.028898884 0.024315227 0.018674677 0.01219762 0.0060883914 0.0018593931 3.6072684e-05 -0.00023039205 -0.00025489289 -0.00026404532 -0.0002660795][0.030159965 0.034749959 0.034291029 0.03314124 0.029668422 0.024858125 0.019018862 0.012482908 0.0062990719 0.0019220592 4.6853384e-06 -0.00025315673 -0.00025473125 -0.00025939377 -0.00026101168][0.031036325 0.036010146 0.035216931 0.033767145 0.029886069 0.024743592 0.018751156 0.01239327 0.0063817007 0.00200527 2.5832123e-05 -0.00026095827 -0.00025775345 -0.00025824839 -0.00025638635][0.032645542 0.036553565 0.035451274 0.033614196 0.029381907 0.024067774 0.018085554 0.012045819 0.006327332 0.0020970574 8.6057611e-05 -0.00026058155 -0.0002572418 -0.00025980431 -0.00025820572][0.032187145 0.034573179 0.033183634 0.031058095 0.02693042 0.022199238 0.016950278 0.011671403 0.0063330368 0.0022715922 0.00019507881 -0.00026317604 -0.00026003321 -0.00025895555 -0.00025629599]]...]
INFO - root - 2017-12-09 08:52:05.242779: step 7710, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 77h:46m:05s remains)
INFO - root - 2017-12-09 08:52:13.853114: step 7720, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.865 sec/batch; 78h:00m:18s remains)
INFO - root - 2017-12-09 08:52:22.537022: step 7730, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.853 sec/batch; 76h:59m:27s remains)
INFO - root - 2017-12-09 08:52:31.283217: step 7740, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.872 sec/batch; 78h:37m:50s remains)
INFO - root - 2017-12-09 08:52:39.810829: step 7750, loss = 0.81, batch loss = 0.69 (8.9 examples/sec; 0.896 sec/batch; 80h:48m:38s remains)
INFO - root - 2017-12-09 08:52:48.347195: step 7760, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 78h:23m:18s remains)
INFO - root - 2017-12-09 08:52:56.860564: step 7770, loss = 0.82, batch loss = 0.69 (8.3 examples/sec; 0.964 sec/batch; 86h:55m:39s remains)
INFO - root - 2017-12-09 08:53:05.386315: step 7780, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 78h:09m:50s remains)
INFO - root - 2017-12-09 08:53:14.111012: step 7790, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.878 sec/batch; 79h:10m:07s remains)
INFO - root - 2017-12-09 08:53:22.901744: step 7800, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.887 sec/batch; 79h:57m:39s remains)
2017-12-09 08:53:23.723538: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.00030471559 -0.00030350493 -0.00030238222 -0.00030130049 -0.00030019614 -0.0002992017 -0.00029845064 -0.0002975876 -0.00029668395 -0.00029605054 -0.00029560429 -0.00029533487 -0.00029535239 -0.00029581532 -0.00029660383][-0.00030487654 -0.00030356663 -0.00030234834 -0.00030138795 -0.00030063096 -0.00029999163 -0.00029944949 -0.0002987473 -0.0002979821 -0.0002974114 -0.00029696614 -0.00029666815 -0.00029672039 -0.00029730011 -0.00029819115][-0.00030487916 -0.00030352559 -0.00030222366 -0.00030121871 -0.00030073628 -0.00030042243 -0.0003000384 -0.00029944224 -0.00029874477 -0.00029819933 -0.00029780486 -0.00029761187 -0.00029787055 -0.00029874188 -0.00029988747][-0.00030420531 -0.00030277902 -0.00030144746 -0.00030046835 -0.00030011206 -0.0003000494 -0.00029982813 -0.00029929404 -0.00029865009 -0.00029811839 -0.00029778795 -0.00029775541 -0.00029827509 -0.0002994854 -0.00030093724][-0.00030265513 -0.00030113687 -0.00029977385 -0.00029888604 -0.00029861773 -0.00029862876 -0.00029854843 -0.00029817602 -0.00029758696 -0.00029714085 -0.00029698916 -0.00029724574 -0.00029809985 -0.00029964306 -0.00030140163][-0.00030057566 -0.00029905289 -0.00029767945 -0.00029679306 -0.00029644996 -0.00029643226 -0.00029635261 -0.00029604 -0.00029555094 -0.0002952409 -0.00029525 -0.00029575225 -0.00029690424 -0.0002987211 -0.00030074836][-0.00029836694 -0.00029690057 -0.00029559966 -0.00029474866 -0.00029429275 -0.00029415096 -0.00029396618 -0.00029365264 -0.00029324763 -0.00029296079 -0.00029302313 -0.00029362156 -0.00029485489 -0.00029677741 -0.00029893831][-0.00029619498 -0.00029481412 -0.00029366312 -0.00029291862 -0.00029243995 -0.00029218636 -0.00029196995 -0.00029175985 -0.00029149913 -0.00029127419 -0.00029132166 -0.00029184512 -0.00029293427 -0.00029468391 -0.00029678878][-0.000294374 -0.00029314877 -0.00029224006 -0.00029155816 -0.00029105583 -0.00029079628 -0.00029067512 -0.00029056726 -0.00029044767 -0.00029034575 -0.0002903899 -0.00029074319 -0.0002915425 -0.00029296623 -0.00029480405][-0.00029300625 -0.00029200374 -0.0002913526 -0.00029081356 -0.00029038033 -0.00029021088 -0.00029019418 -0.00029017951 -0.00029012948 -0.00029005771 -0.0002900697 -0.0002902502 -0.00029074604 -0.00029175926 -0.00029320703][-0.00029179253 -0.00029100006 -0.00029059948 -0.00029023527 -0.00028995323 -0.00028989319 -0.00028995582 -0.00029000055 -0.0002899901 -0.00028991885 -0.0002898906 -0.00028995963 -0.00029022119 -0.00029082212 -0.000291802][-0.00029072759 -0.00029008207 -0.00028986495 -0.00028964583 -0.00028945968 -0.00028945657 -0.00028955634 -0.0002896346 -0.00028964086 -0.00028958305 -0.00028957083 -0.00028959379 -0.00028970971 -0.00029003437 -0.00029060731][-0.00028988873 -0.00028938163 -0.00028929289 -0.00028916297 -0.00028907391 -0.00028910689 -0.00028919551 -0.00028925776 -0.00028925174 -0.00028921466 -0.00028921847 -0.00028922397 -0.00028926862 -0.00028942348 -0.00028973666][-0.00028937421 -0.0002889701 -0.00028888928 -0.00028882653 -0.00028880316 -0.0002888451 -0.00028889952 -0.000288933 -0.00028893427 -0.00028892298 -0.00028892574 -0.00028891698 -0.00028891224 -0.0002889696 -0.00028913002][-0.00028904041 -0.00028865135 -0.00028860662 -0.00028857708 -0.00028855959 -0.0002885802 -0.00028859975 -0.00028861529 -0.00028861285 -0.00028861262 -0.00028861366 -0.00028860554 -0.00028858898 -0.00028859251 -0.00028865755]]...]
INFO - root - 2017-12-09 08:53:32.349463: step 7810, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.837 sec/batch; 75h:28m:37s remains)
INFO - root - 2017-12-09 08:53:40.897065: step 7820, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.892 sec/batch; 80h:25m:45s remains)
INFO - root - 2017-12-09 08:53:49.464764: step 7830, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.860 sec/batch; 77h:31m:37s remains)
INFO - root - 2017-12-09 08:53:58.149376: step 7840, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.848 sec/batch; 76h:27m:05s remains)
INFO - root - 2017-12-09 08:54:06.568913: step 7850, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.875 sec/batch; 78h:51m:48s remains)
INFO - root - 2017-12-09 08:54:15.303713: step 7860, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.873 sec/batch; 78h:45m:08s remains)
INFO - root - 2017-12-09 08:54:23.824571: step 7870, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.861 sec/batch; 77h:37m:55s remains)
INFO - root - 2017-12-09 08:54:32.412873: step 7880, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.855 sec/batch; 77h:04m:50s remains)
INFO - root - 2017-12-09 08:54:40.866830: step 7890, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.839 sec/batch; 75h:39m:57s remains)
INFO - root - 2017-12-09 08:54:49.481046: step 7900, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.838 sec/batch; 75h:36m:00s remains)
2017-12-09 08:54:50.356080: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.0059842644 0.0078764232 0.0091731334 0.0098310066 0.0098319585 0.0093685165 0.0087432172 0.0081656985 0.0076626451 0.0070618391 0.006127222 0.0046969159 0.0031485863 0.0018283371 0.00083941937][0.010946672 0.014030618 0.016096083 0.017088285 0.017068366 0.016341345 0.015368249 0.014480383 0.013656972 0.012611859 0.010901902 0.0084294146 0.0057688714 0.0034799175 0.0017501935][0.019108303 0.024240809 0.027853066 0.029814998 0.030302277 0.029740537 0.028776938 0.027760271 0.026537806 0.024590025 0.021131985 0.016415589 0.01138447 0.0070083411 0.0036615981][0.030945398 0.039250825 0.045436379 0.049238179 0.050855555 0.050686162 0.049624469 0.04820225 0.04613889 0.042624712 0.036601923 0.028634327 0.020127134 0.012617231 0.0068116831][0.044876631 0.057589617 0.067717873 0.074788414 0.079079665 0.080733776 0.080519311 0.078912437 0.075434558 0.069124885 0.058898125 0.045953579 0.032331061 0.020269159 0.011030702][0.056241732 0.073174842 0.087779194 0.099155724 0.10735749 0.1119952 0.11354259 0.11216924 0.1073212 0.098099 0.083536826 0.065322682 0.046202861 0.029227082 0.01617869][0.06093137 0.0806128 0.098639987 0.11375557 0.12580629 0.13381626 0.13759503 0.13693491 0.13128375 0.11992531 0.10221323 0.08015357 0.057031315 0.036416348 0.020501198][0.057297941 0.077033378 0.096193627 0.11325642 0.12772445 0.13825844 0.14414127 0.14461011 0.13902557 0.12706295 0.1084898 0.08534804 0.061022747 0.039284445 0.022459548][0.046827577 0.063987307 0.0814839 0.0978959 0.11262784 0.12421344 0.13144279 0.13315834 0.12856993 0.11758434 0.10039664 0.078977786 0.056497257 0.036438435 0.020953208][0.033057634 0.0460284 0.059810054 0.073337406 0.086186185 0.097016186 0.10449648 0.10705613 0.10378596 0.094859838 0.0806826 0.063082352 0.044724431 0.028571147 0.016288348][0.019332847 0.027749885 0.037077144 0.046625409 0.056238353 0.064954281 0.071344063 0.073904045 0.071795091 0.065304436 0.0549639 0.042291261 0.02936024 0.018284393 0.010098748][0.0088468343 0.013296189 0.018436549 0.023919584 0.02976709 0.035402074 0.039783739 0.041656777 0.040353913 0.03627402 0.029934848 0.022412784 0.01496879 0.00884933 0.0045622555][0.002655149 0.0044078715 0.006547166 0.00894312 0.011637206 0.014371754 0.016598415 0.017585214 0.016913118 0.014879778 0.011895249 0.0085200006 0.0053200275 0.0028188052 0.0012066835][9.0963265e-05 0.00051398762 0.0010846178 0.0017655257 0.0025778895 0.003473216 0.0042466545 0.004594204 0.0043547708 0.0036775311 0.0027603826 0.0017821202 0.00090222718 0.00026004942 -9.0046873e-05][-0.00028985619 -0.00027367019 -0.00024276027 -0.00018395871 -8.36633e-05 5.0144416e-05 0.00018068159 0.00024924293 0.00022723747 0.00014642696 4.1388237e-05 -7.2198425e-05 -0.00017606065 -0.00024508632 -0.00027196063]]...]
INFO - root - 2017-12-09 08:54:59.066655: step 7910, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 78h:11m:33s remains)
INFO - root - 2017-12-09 08:55:07.619837: step 7920, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.887 sec/batch; 79h:56m:03s remains)
INFO - root - 2017-12-09 08:55:16.407802: step 7930, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 78h:16m:07s remains)
INFO - root - 2017-12-09 08:55:25.056227: step 7940, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.846 sec/batch; 76h:17m:03s remains)
INFO - root - 2017-12-09 08:55:33.608206: step 7950, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.858 sec/batch; 77h:21m:59s remains)
INFO - root - 2017-12-09 08:55:42.307946: step 7960, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.883 sec/batch; 79h:36m:03s remains)
INFO - root - 2017-12-09 08:55:50.732637: step 7970, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.850 sec/batch; 76h:36m:18s remains)
INFO - root - 2017-12-09 08:55:59.299896: step 7980, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.851 sec/batch; 76h:42m:23s remains)
INFO - root - 2017-12-09 08:56:08.001346: step 7990, loss = 0.82, batch loss = 0.69 (9.9 examples/sec; 0.807 sec/batch; 72h:45m:33s remains)
INFO - root - 2017-12-09 08:56:16.696561: step 8000, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.872 sec/batch; 78h:38m:25s remains)
2017-12-09 08:56:17.574999: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.19450727 0.18104389 0.16508326 0.1486382 0.13415617 0.1228683 0.11535962 0.11047217 0.10851979 0.10647237 0.10399979 0.10055266 0.096138053 0.09309911 0.091049328][0.19450085 0.18396632 0.17236133 0.16105409 0.15190022 0.14539547 0.14179946 0.13958016 0.13891058 0.13747714 0.13492638 0.13089578 0.12545148 0.1214266 0.11857707][0.19299746 0.18657194 0.18013835 0.17477341 0.17142795 0.17010474 0.17060041 0.17073591 0.17094314 0.16956314 0.16625826 0.16082641 0.15393057 0.14818086 0.14361767][0.19061428 0.18822467 0.187076 0.18742839 0.18973069 0.19325629 0.19711231 0.19935165 0.19977708 0.19799253 0.19332691 0.1862216 0.17760879 0.16970065 0.16315381][0.18832874 0.18952006 0.19255255 0.19756064 0.20433585 0.2117551 0.2185232 0.222378 0.22329545 0.22098005 0.21518348 0.20610981 0.1952648 0.18517458 0.17662118][0.18532121 0.1896002 0.19593546 0.20430753 0.21402498 0.22408335 0.23306228 0.23858218 0.24035379 0.23809132 0.2318365 0.22147742 0.20892259 0.19668394 0.18599164][0.18182279 0.18835323 0.19692707 0.20745201 0.2190505 0.23040397 0.24048829 0.24732436 0.25010648 0.24841389 0.24206662 0.23127323 0.2177756 0.20394897 0.19151431][0.17898312 0.1870179 0.19677465 0.20822011 0.22035959 0.232267 0.24264802 0.25028098 0.25384408 0.25291222 0.24696191 0.23643269 0.22291136 0.20837876 0.19497342][0.17770627 0.18657228 0.19672397 0.20812953 0.21983287 0.23108812 0.24080147 0.2483931 0.25218511 0.25187424 0.24660017 0.2370633 0.22442171 0.21051878 0.19745347][0.17732593 0.18723527 0.1974189 0.20843613 0.21922529 0.22889806 0.23718025 0.24353252 0.24679752 0.2466587 0.24222031 0.23429045 0.22334784 0.21127158 0.1995724][0.17495619 0.18570016 0.19598936 0.20652896 0.21646781 0.22484981 0.23188236 0.23737669 0.24020268 0.23992111 0.23594406 0.22919914 0.21984604 0.20903648 0.19824287][0.17089075 0.18192652 0.19195667 0.2019 0.2108136 0.21789578 0.22385101 0.22863591 0.23128058 0.2311659 0.22788428 0.222256 0.21411653 0.20424707 0.19409153][0.16472158 0.17557846 0.18508127 0.19392002 0.20161806 0.2073902 0.21237008 0.21646982 0.21893708 0.21927421 0.21690159 0.21250327 0.20551427 0.19643939 0.18684962][0.15472616 0.16503979 0.17379871 0.18160227 0.18836465 0.19314411 0.1975231 0.20091154 0.20320909 0.20390622 0.2021779 0.19855736 0.19219805 0.18373697 0.17462303][0.14133225 0.15038203 0.15776175 0.16438511 0.17028747 0.17449418 0.17867234 0.18196486 0.18441123 0.18527503 0.18392843 0.18055514 0.17429043 0.16595648 0.15712778]]...]
INFO - root - 2017-12-09 08:56:26.264871: step 8010, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.875 sec/batch; 78h:49m:38s remains)
INFO - root - 2017-12-09 08:56:34.748523: step 8020, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 78h:32m:05s remains)
INFO - root - 2017-12-09 08:56:43.408834: step 8030, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 78h:13m:01s remains)
INFO - root - 2017-12-09 08:56:52.052762: step 8040, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.877 sec/batch; 79h:04m:37s remains)
INFO - root - 2017-12-09 08:57:00.516995: step 8050, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.857 sec/batch; 77h:15m:06s remains)
INFO - root - 2017-12-09 08:57:09.335783: step 8060, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.888 sec/batch; 80h:01m:45s remains)
INFO - root - 2017-12-09 08:57:17.909757: step 8070, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.888 sec/batch; 80h:02m:09s remains)
INFO - root - 2017-12-09 08:57:26.537256: step 8080, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.873 sec/batch; 78h:40m:21s remains)
INFO - root - 2017-12-09 08:57:35.040454: step 8090, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.861 sec/batch; 77h:32m:38s remains)
INFO - root - 2017-12-09 08:57:43.681123: step 8100, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.877 sec/batch; 79h:00m:54s remains)
2017-12-09 08:57:44.580564: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.00043004428 -0.00043083544 -0.00043209366 -0.00043296505 -0.00043329218 -0.000432847 -0.00043198481 -0.00043177523 -0.00043278033 -0.0004345983 -0.00043706596 -0.00043965576 -0.000441399 -0.00044225366 -0.00044234787][-0.00041978827 -0.00042081776 -0.00042277022 -0.00042404066 -0.00042437419 -0.00042376819 -0.00042246853 -0.00042182353 -0.00042276768 -0.0004252721 -0.00042904849 -0.00043361346 -0.00043735935 -0.00043970154 -0.00044095312][-0.00041023552 -0.00041163262 -0.00041425729 -0.00041617136 -0.00041677966 -0.00041571743 -0.0004137205 -0.00041220096 -0.000412808 -0.00041580258 -0.0004208645 -0.00042743713 -0.00043347356 -0.00043778724 -0.00044028604][-0.00040410238 -0.00040572666 -0.00040846519 -0.00041047324 -0.0004107744 -0.00040923402 -0.00040666485 -0.00040419455 -0.000404211 -0.00040702464 -0.00041278548 -0.00042072183 -0.0004288349 -0.0004350839 -0.00043889895][-0.00040336349 -0.00040470276 -0.00040703636 -0.00040857744 -0.00040811618 -0.0004056865 -0.00040247705 -0.0003991928 -0.0003980929 -0.00040056417 -0.0004065971 -0.00041510092 -0.00042410666 -0.00043172156 -0.00043692128][-0.00040852703 -0.00040925512 -0.00041030228 -0.00041061439 -0.00040926304 -0.00040617504 -0.00040263319 -0.00039909602 -0.00039765853 -0.00039987406 -0.00040558574 -0.00041373231 -0.00042236506 -0.0004299162 -0.00043577325][-0.0004162509 -0.00041651147 -0.00041675108 -0.0004159621 -0.00041346246 -0.00040967204 -0.00040568344 -0.000402358 -0.00040094519 -0.00040283665 -0.00040821597 -0.00041555893 -0.00042344356 -0.000430227 -0.00043550506][-0.00042448446 -0.0004246186 -0.00042439246 -0.00042295479 -0.00042026295 -0.00041626231 -0.00041224828 -0.00040912771 -0.000407592 -0.00040883705 -0.00041304904 -0.00041907566 -0.00042585429 -0.00043164226 -0.000436043][-0.00043143527 -0.00043142601 -0.00043116833 -0.00042971803 -0.00042734138 -0.00042403294 -0.00042024837 -0.00041735973 -0.00041578183 -0.00041635236 -0.00041902903 -0.00042304181 -0.00042792942 -0.00043194435 -0.00043528984][-0.00043256479 -0.00043267445 -0.00043289969 -0.000432023 -0.00043062735 -0.00042867454 -0.00042609824 -0.0004239351 -0.00042265077 -0.00042295491 -0.00042478103 -0.00042767741 -0.00043107083 -0.00043380796 -0.00043567564][-0.00042609172 -0.00042634743 -0.00042760343 -0.00042735919 -0.00042644396 -0.0004255528 -0.00042429884 -0.00042347447 -0.00042337558 -0.00042442078 -0.00042639882 -0.00042912379 -0.0004322693 -0.00043482723 -0.00043663278][-0.00041204944 -0.00041220812 -0.00041433662 -0.00041513235 -0.00041530764 -0.00041527909 -0.00041529644 -0.00041559804 -0.00041629618 -0.00041814928 -0.00042053746 -0.00042345392 -0.0004265926 -0.00042936188 -0.00043187538][-0.00039160898 -0.00039209472 -0.00039459817 -0.00039635925 -0.00039781324 -0.00039920784 -0.00040078504 -0.00040238645 -0.00040403282 -0.00040615295 -0.00040853836 -0.00041136064 -0.00041439256 -0.0004183625 -0.00042256329][-0.00037083312 -0.00037046775 -0.00037293389 -0.00037507882 -0.00037687569 -0.00037894142 -0.00038165817 -0.0003841054 -0.00038612852 -0.00038821684 -0.00039031688 -0.00039293489 -0.00039622729 -0.00040113475 -0.0004067602][-0.00034972362 -0.00034901904 -0.00035136458 -0.00035319402 -0.00035494345 -0.00035758212 -0.00036066771 -0.00036350987 -0.00036609452 -0.000368556 -0.00037072992 -0.00037339749 -0.00037732857 -0.00038346357 -0.00039116084]]...]
INFO - root - 2017-12-09 08:57:53.041645: step 8110, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.889 sec/batch; 80h:04m:18s remains)
INFO - root - 2017-12-09 08:58:01.653425: step 8120, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.866 sec/batch; 78h:00m:01s remains)
INFO - root - 2017-12-09 08:58:10.429902: step 8130, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.882 sec/batch; 79h:29m:43s remains)
INFO - root - 2017-12-09 08:58:19.273061: step 8140, loss = 0.82, batch loss = 0.69 (9.7 examples/sec; 0.825 sec/batch; 74h:19m:55s remains)
INFO - root - 2017-12-09 08:58:27.965811: step 8150, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.887 sec/batch; 79h:57m:28s remains)
INFO - root - 2017-12-09 08:58:36.801986: step 8160, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.879 sec/batch; 79h:12m:15s remains)
INFO - root - 2017-12-09 08:58:45.354140: step 8170, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.881 sec/batch; 79h:23m:33s remains)
INFO - root - 2017-12-09 08:58:54.230080: step 8180, loss = 0.82, batch loss = 0.69 (8.2 examples/sec; 0.974 sec/batch; 87h:44m:43s remains)
INFO - root - 2017-12-09 08:59:02.812344: step 8190, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.860 sec/batch; 77h:29m:29s remains)
INFO - root - 2017-12-09 08:59:11.604195: step 8200, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.850 sec/batch; 76h:32m:35s remains)
2017-12-09 08:59:12.564615: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.0016965158 0.0023480342 0.0033918465 0.0045903712 0.0056311134 0.0062395246 0.0063007534 0.0058259363 0.0049789925 0.0040017236 0.0030483983 0.0021837689 0.001411201 0.00081822154 0.00038839725][0.0041117393 0.0057002925 0.0077719544 0.00994498 0.011644304 0.01229155 0.011710802 0.010073947 0.0078854738 0.0056672636 0.003801147 0.0024534483 0.001550647 0.00097081432 0.00055621675][0.0058309687 0.0089479024 0.012792442 0.016671699 0.019609133 0.020752102 0.019821605 0.017109096 0.013404019 0.0095220217 0.0061797951 0.0036813319 0.0020212904 0.0010403641 0.00054614333][0.0073093441 0.012033257 0.017860549 0.02393887 0.028773021 0.031128529 0.030481715 0.027181571 0.022127869 0.016390141 0.010984615 0.0066012042 0.0035081748 0.0015527126 0.00053269364][0.0096263429 0.015977843 0.0239454 0.032325178 0.0393474 0.043364622 0.043596517 0.040213678 0.034013126 0.026332052 0.018469088 0.011673223 0.0065024626 0.0030536766 0.0011352815][0.012861677 0.020507997 0.030116694 0.040482778 0.049607191 0.055636767 0.057496008 0.054783389 0.048076961 0.038655654 0.028249225 0.018660918 0.010942046 0.0056154365 0.0024824189][0.016202459 0.025330437 0.036510672 0.048581772 0.059178047 0.0666709 0.069888376 0.068181179 0.061800148 0.051610593 0.039537873 0.027521152 0.017206432 0.009614802 0.00479612][0.018324522 0.028475016 0.040909637 0.054179691 0.065985687 0.0746739 0.0789199 0.077985734 0.072023079 0.061804645 0.049072549 0.035763822 0.023850668 0.01460305 0.0082272757][0.018800618 0.029396405 0.042329766 0.056199193 0.068765596 0.078261018 0.083267547 0.083180018 0.077995472 0.068377472 0.055949885 0.042575158 0.030189898 0.020020967 0.012531277][0.017793752 0.028139563 0.040685382 0.05417756 0.066630863 0.076393232 0.08191368 0.082571931 0.078384086 0.069974504 0.058767546 0.046418078 0.034719251 0.024775151 0.017032392][0.015316019 0.024844928 0.036358383 0.048789009 0.060260423 0.069392808 0.074824341 0.076008871 0.072962627 0.066252656 0.057037711 0.046642661 0.03651049 0.027666636 0.020650623][0.011907057 0.019993102 0.029842472 0.040485498 0.050327022 0.058160327 0.062919542 0.064230636 0.062240675 0.057418983 0.050594892 0.042737465 0.035020538 0.028156025 0.022609575][0.008604506 0.014799602 0.022441698 0.030838385 0.038767844 0.045198537 0.049261928 0.050722063 0.049701631 0.046585549 0.041993804 0.036623918 0.031302609 0.02653675 0.022716407][0.0065915002 0.01151586 0.017708154 0.02451911 0.03100612 0.036349092 0.039750904 0.041002885 0.040325034 0.038041439 0.034652796 0.030759756 0.027027819 0.023752166 0.021225257][0.0058839866 0.01017371 0.015632914 0.021853914 0.02785985 0.032994956 0.03636492 0.037635792 0.037011947 0.034737904 0.031482842 0.027783286 0.024371712 0.02155122 0.019513641]]...]
INFO - root - 2017-12-09 08:59:21.174249: step 8210, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.881 sec/batch; 79h:21m:17s remains)
INFO - root - 2017-12-09 08:59:29.839969: step 8220, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.858 sec/batch; 77h:14m:33s remains)
INFO - root - 2017-12-09 08:59:38.723751: step 8230, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.897 sec/batch; 80h:47m:06s remains)
INFO - root - 2017-12-09 08:59:47.406064: step 8240, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.832 sec/batch; 74h:58m:10s remains)
INFO - root - 2017-12-09 08:59:56.042123: step 8250, loss = 0.81, batch loss = 0.68 (9.6 examples/sec; 0.837 sec/batch; 75h:24m:40s remains)
INFO - root - 2017-12-09 09:00:04.616116: step 8260, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.845 sec/batch; 76h:08m:11s remains)
INFO - root - 2017-12-09 09:00:13.311897: step 8270, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.851 sec/batch; 76h:39m:09s remains)
INFO - root - 2017-12-09 09:00:21.801046: step 8280, loss = 0.82, batch loss = 0.69 (9.7 examples/sec; 0.825 sec/batch; 74h:16m:22s remains)
INFO - root - 2017-12-09 09:00:30.363254: step 8290, loss = 0.82, batch loss = 0.69 (9.7 examples/sec; 0.825 sec/batch; 74h:17m:13s remains)
INFO - root - 2017-12-09 09:00:38.892271: step 8300, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.865 sec/batch; 77h:52m:13s remains)
2017-12-09 09:00:39.804442: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.00013811397 0.00038708153 0.00087072962 0.001489823 0.0022275785 0.0029738627 0.0035871784 0.0039448221 0.0039768592 0.0037357486 0.0032063818 0.0024864913 0.0017360677 0.0010261638 0.00043274654][0.0011447768 0.0017142601 0.0025882688 0.0036107553 0.0047475509 0.00586251 0.0067893104 0.0073559023 0.0074641155 0.0071642995 0.0063976683 0.005240927 0.0039049555 0.0025618116 0.0013915615][0.0031743622 0.0045188372 0.0062467335 0.0080603305 0.0098368013 0.011428033 0.012695992 0.013455463 0.013562601 0.013064956 0.011897226 0.010101764 0.0078658024 0.0054716244 0.0032981073][0.0062143626 0.0087983832 0.011802308 0.014733204 0.01732498 0.019411178 0.020937704 0.021741409 0.021664271 0.020776913 0.019045521 0.016430659 0.013086956 0.0094072968 0.0059855762][0.0094968323 0.01349032 0.017944405 0.022102768 0.025561688 0.028205724 0.030034576 0.030807072 0.030363735 0.028877808 0.026406543 0.022879397 0.018397128 0.01345487 0.0088289026][0.012355536 0.017524218 0.023188552 0.028399147 0.032630384 0.03580847 0.037963666 0.038753439 0.037987385 0.035893597 0.03266412 0.028273743 0.022796387 0.016830135 0.011235517][0.014207541 0.020069417 0.026478477 0.032357793 0.03707755 0.040611971 0.04300211 0.043801833 0.042791668 0.040266227 0.036512647 0.031562306 0.025454964 0.018875496 0.012716077][0.014788465 0.02079414 0.027364623 0.033402912 0.0382235 0.041777425 0.044124372 0.044816986 0.04365325 0.040941246 0.037018497 0.031947672 0.025727896 0.019077249 0.012860055][0.013901842 0.019530749 0.025691444 0.031350978 0.035816945 0.039006285 0.041010145 0.041446812 0.040170919 0.037485547 0.033731785 0.028975394 0.023201095 0.017090652 0.011429153][0.011578659 0.016389059 0.021645186 0.026467992 0.030234043 0.032817021 0.034312632 0.034448069 0.033155713 0.030706249 0.027409321 0.023329329 0.018463114 0.013405556 0.0088058375][0.0083432375 0.011983232 0.015966956 0.019637672 0.022487838 0.024374129 0.025370333 0.02529899 0.024138259 0.022116259 0.019503661 0.016363198 0.012715582 0.00901943 0.0057507223][0.0050706896 0.0074622259 0.010074472 0.012485561 0.014362263 0.015591583 0.016200667 0.016070344 0.015197692 0.013751554 0.011941577 0.0098289121 0.0074478472 0.0051018326 0.0030928997][0.0024728619 0.003816687 0.005275391 0.0066221221 0.0076720156 0.0083580092 0.0086808475 0.0085646855 0.0080097551 0.0071240729 0.0060519432 0.0048438367 0.0035276213 0.0022689363 0.0012272596][0.00079151249 0.0014234511 0.0021051203 0.0027337915 0.0032278849 0.0035586767 0.0037136539 0.003646614 0.0033559098 0.0029032084 0.0023724062 0.0017976142 0.0011939628 0.00063316937 0.00018353167][-7.2797149e-05 0.0001654501 0.00042004808 0.0006543686 0.00084157061 0.00097318663 0.0010387634 0.0010146401 0.00089757604 0.00071512634 0.0005062173 0.00028971356 7.0792215e-05 -0.00012733834 -0.00028292483]]...]
INFO - root - 2017-12-09 09:00:48.453197: step 8310, loss = 0.82, batch loss = 0.69 (8.7 examples/sec; 0.920 sec/batch; 82h:50m:54s remains)
INFO - root - 2017-12-09 09:00:57.057307: step 8320, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.882 sec/batch; 79h:23m:51s remains)
INFO - root - 2017-12-09 09:01:05.772551: step 8330, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.896 sec/batch; 80h:38m:34s remains)
INFO - root - 2017-12-09 09:01:14.518917: step 8340, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.855 sec/batch; 77h:00m:23s remains)
INFO - root - 2017-12-09 09:01:23.063890: step 8350, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.886 sec/batch; 79h:46m:46s remains)
INFO - root - 2017-12-09 09:01:31.766343: step 8360, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 78h:26m:00s remains)
INFO - root - 2017-12-09 09:01:40.348896: step 8370, loss = 0.82, batch loss = 0.69 (11.2 examples/sec; 0.713 sec/batch; 64h:11m:05s remains)
INFO - root - 2017-12-09 09:01:49.184669: step 8380, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.897 sec/batch; 80h:44m:15s remains)
INFO - root - 2017-12-09 09:01:57.903092: step 8390, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 78h:05m:58s remains)
INFO - root - 2017-12-09 09:02:06.473713: step 8400, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.849 sec/batch; 76h:26m:55s remains)
2017-12-09 09:02:07.388538: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.00052243622 -0.00052139192 -0.00052100531 -0.00052091171 -0.00052117021 -0.00052152871 -0.00052191404 -0.00052217342 -0.00052228558 -0.00052231614 -0.00052244071 -0.00052278943 -0.00052342564 -0.00052429776 -0.00052543014][-0.00052251294 -0.00052140519 -0.00052082492 -0.00052074814 -0.00052092393 -0.00052135333 -0.00052188471 -0.00052239385 -0.00052241154 -0.00052154512 -0.00052015233 -0.00051980023 -0.00052092818 -0.00052319403 -0.00052560447][-0.00052319677 -0.00052215095 -0.000521793 -0.00052174984 -0.0005219889 -0.000522304 -0.00052247761 -0.00052238052 -0.00052089011 -0.00051712157 -0.00051139173 -0.00050892081 -0.00051154359 -0.00051821297 -0.00052449817][-0.00052420673 -0.00052334822 -0.000523153 -0.00052325305 -0.00052276149 -0.00051969942 -0.00051310949 -0.0005037472 -0.00049226469 -0.00047928756 -0.00046684485 -0.00046500569 -0.00047683579 -0.0004975957 -0.000515478][-0.00052518887 -0.00052465271 -0.00052489672 -0.000524662 -0.00051813165 -0.000497524 -0.00045904724 -0.00040957981 -0.00036073482 -0.00032316055 -0.00030583015 -0.00032093166 -0.00036760879 -0.00042925123 -0.00048021579][-0.00052577665 -0.00052550575 -0.00052561145 -0.00052249868 -0.00049670594 -0.00042726781 -0.00030505293 -0.00015373586 -1.3560115e-05 7.81953e-05 9.6259231e-05 3.0905183e-05 -0.00010171303 -0.00025883262 -0.0003884419][-0.00052607921 -0.00052527944 -0.00052369246 -0.00051224779 -0.00044801674 -0.00029118944 -2.4545705e-05 0.00029831822 0.00059267058 0.00077377172 0.00078751653 0.00063140126 0.00035304495 3.7807506e-05 -0.00022449731][-0.0005255345 -0.0005229587 -0.000517135 -0.000491943 -0.00037894415 -0.00012015234 0.00031129381 0.00083096692 0.0013082451 0.001599564 0.001609802 0.0013433162 0.000891149 0.00038986729 -2.9495161e-05][-0.00052450347 -0.00051960826 -0.00050835009 -0.00046937441 -0.00032032133 4.9492228e-06 0.0005431021 0.0011941805 0.0018010263 0.0021701225 0.0021733625 0.0018215342 0.0012434653 0.000614711 9.1536436e-05][-0.0005235 -0.00051732187 -0.00050261623 -0.00045853472 -0.00031088578 1.0809163e-06 0.0005205315 0.0011589522 0.0017635645 0.0021300812 0.0021235705 0.0017587061 0.0011759691 0.00055725168 5.2360236e-05][-0.00052222871 -0.00051690615 -0.00050376606 -0.00046804029 -0.00035926478 -0.00013228034 0.00025367638 0.0007394256 0.001207056 0.0014880495 0.0014712333 0.0011728576 0.00071484951 0.00024562172 -0.00012486559][-0.00052122935 -0.00051833177 -0.000511309 -0.000491983 -0.00043463631 -0.00031329121 -9.8968274e-05 0.00018068583 0.0004544835 0.000616779 0.00059824536 0.00041163096 0.00013785751 -0.00012929938 -0.00032858824][-0.00052018254 -0.000519441 -0.00051782106 -0.00051204645 -0.00049253041 -0.00044812838 -0.00036413904 -0.0002492904 -0.00013575063 -7.0627284e-05 -8.3964755e-05 -0.00016760192 -0.00028280867 -0.00038860546 -0.00046141641][-0.00051932572 -0.00051889318 -0.00051896658 -0.00051898119 -0.00051625428 -0.00050795422 -0.00048898411 -0.00045962338 -0.00042968913 -0.00041291607 -0.00041784148 -0.00044097035 -0.00047063318 -0.0004957057 -0.00051096018][-0.00051878864 -0.00051822246 -0.00051830878 -0.00051845488 -0.00051859434 -0.00051866844 -0.00051778718 -0.00051578629 -0.00051351706 -0.00051209884 -0.00051268691 -0.0005145623 -0.000516556 -0.00051828608 -0.00051944808]]...]
INFO - root - 2017-12-09 09:02:16.111486: step 8410, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 78h:08m:07s remains)
INFO - root - 2017-12-09 09:02:24.647990: step 8420, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.832 sec/batch; 74h:56m:21s remains)
INFO - root - 2017-12-09 09:02:33.361863: step 8430, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 78h:03m:26s remains)
INFO - root - 2017-12-09 09:02:41.962023: step 8440, loss = 0.83, batch loss = 0.70 (9.3 examples/sec; 0.858 sec/batch; 77h:12m:09s remains)
INFO - root - 2017-12-09 09:02:50.509377: step 8450, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.857 sec/batch; 77h:06m:17s remains)
INFO - root - 2017-12-09 09:02:59.185174: step 8460, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.847 sec/batch; 76h:12m:08s remains)
INFO - root - 2017-12-09 09:03:07.741655: step 8470, loss = 0.82, batch loss = 0.69 (10.7 examples/sec; 0.748 sec/batch; 67h:19m:36s remains)
INFO - root - 2017-12-09 09:03:16.218969: step 8480, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.837 sec/batch; 75h:17m:34s remains)
INFO - root - 2017-12-09 09:03:24.916817: step 8490, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.879 sec/batch; 79h:08m:44s remains)
INFO - root - 2017-12-09 09:03:33.606779: step 8500, loss = 0.82, batch loss = 0.69 (8.3 examples/sec; 0.962 sec/batch; 86h:33m:59s remains)
2017-12-09 09:03:34.490354: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.00051809318 -0.00051724521 -0.00051729789 -0.00051750033 -0.00051760557 -0.00051767979 -0.00051781861 -0.00051797973 -0.00051810313 -0.00051808351 -0.00051789545 -0.00051767519 -0.000517504 -0.00051731669 -0.00051707303][-0.00051705132 -0.00051603926 -0.00051595719 -0.00051608175 -0.00051606429 -0.00051594852 -0.00051587354 -0.00051588408 -0.00051594188 -0.000515922 -0.0005157697 -0.00051561021 -0.00051549525 -0.00051536155 -0.00051517016][-0.00051735697 -0.00051624305 -0.00051600492 -0.00051598769 -0.00051580864 -0.00051548827 -0.00051523774 -0.00051515404 -0.00051519088 -0.00051519665 -0.00051505439 -0.00051488087 -0.00051473908 -0.00051458995 -0.00051440252][-0.0005184692 -0.00051734416 -0.00051688787 -0.00051662244 -0.00051617564 -0.0005155945 -0.00051511527 -0.00051487808 -0.00051485735 -0.0005148534 -0.00051469693 -0.000514486 -0.00051431917 -0.00051416538 -0.00051397987][-0.00052032078 -0.0005192192 -0.00051843835 -0.00051775988 -0.00051691715 -0.00051601877 -0.00051531324 -0.0005149346 -0.00051484158 -0.00051479466 -0.00051459763 -0.00051434844 -0.00051414967 -0.00051397271 -0.00051378249][-0.00052230473 -0.00052120158 -0.00052009214 -0.00051893073 -0.00051764381 -0.00051639957 -0.00051547744 -0.00051499542 -0.0005148701 -0.00051481475 -0.00051464705 -0.00051443465 -0.00051425974 -0.000514092 -0.00051389547][-0.00052368408 -0.00052259729 -0.00052125222 -0.00051972992 -0.00051815808 -0.00051672006 -0.00051564554 -0.00051509083 -0.0005149421 -0.00051489536 -0.000514773 -0.00051460339 -0.00051443774 -0.00051424303 -0.00051399943][-0.00052441622 -0.0005233346 -0.00052183383 -0.00052010233 -0.00051840156 -0.00051689619 -0.00051581621 -0.00051528693 -0.00051514828 -0.00051512173 -0.0005150853 -0.00051501114 -0.0005148768 -0.00051466806 -0.00051436713][-0.00052434736 -0.00052328844 -0.00052179472 -0.00052001432 -0.00051829318 -0.00051685126 -0.00051585952 -0.00051538309 -0.00051526166 -0.00051525555 -0.00051528012 -0.0005152761 -0.00051518559 -0.00051501003 -0.00051470607][-0.00052326784 -0.00052235078 -0.00052104285 -0.00051941507 -0.00051784213 -0.00051657029 -0.00051575684 -0.00051539991 -0.00051533279 -0.00051535532 -0.00051543233 -0.00051548815 -0.00051544653 -0.00051532628 -0.00051507843][-0.00052132254 -0.00052059017 -0.000519591 -0.00051833631 -0.00051712757 -0.00051617122 -0.00051557116 -0.00051532564 -0.0005152878 -0.00051531527 -0.00051540125 -0.00051546039 -0.00051542762 -0.00051534257 -0.0005151353][-0.00051900838 -0.00051845168 -0.00051785266 -0.00051706424 -0.00051627867 -0.00051563681 -0.0005152229 -0.00051503774 -0.00051499764 -0.00051502965 -0.00051512168 -0.000515199 -0.00051518768 -0.00051512197 -0.00051495386][-0.000517016 -0.00051652605 -0.00051615573 -0.00051563175 -0.00051508343 -0.00051465409 -0.00051436457 -0.00051420071 -0.00051416032 -0.00051419938 -0.00051429303 -0.00051437976 -0.00051440619 -0.00051439553 -0.000514319][-0.000515678 -0.0005151963 -0.00051488657 -0.00051446265 -0.00051400909 -0.00051365932 -0.00051341468 -0.00051325996 -0.0005131861 -0.00051317917 -0.00051321846 -0.0005132588 -0.00051327288 -0.00051327865 -0.00051326817][-0.00051435997 -0.00051389844 -0.00051372143 -0.00051351206 -0.00051325944 -0.00051304739 -0.00051288778 -0.00051275676 -0.00051265868 -0.00051260693 -0.00051258144 -0.00051255454 -0.00051253254 -0.00051250058 -0.00051248271]]...]
INFO - root - 2017-12-09 09:03:43.207811: step 8510, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.873 sec/batch; 78h:32m:16s remains)
INFO - root - 2017-12-09 09:03:51.553636: step 8520, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.875 sec/batch; 78h:46m:01s remains)
INFO - root - 2017-12-09 09:04:00.181006: step 8530, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.853 sec/batch; 76h:45m:56s remains)
INFO - root - 2017-12-09 09:04:09.023619: step 8540, loss = 0.82, batch loss = 0.69 (8.3 examples/sec; 0.963 sec/batch; 86h:39m:16s remains)
INFO - root - 2017-12-09 09:04:17.528322: step 8550, loss = 0.82, batch loss = 0.69 (10.1 examples/sec; 0.790 sec/batch; 71h:05m:24s remains)
INFO - root - 2017-12-09 09:04:26.124999: step 8560, loss = 0.81, batch loss = 0.68 (9.2 examples/sec; 0.865 sec/batch; 77h:52m:18s remains)
INFO - root - 2017-12-09 09:04:34.802403: step 8570, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 78h:22m:33s remains)
INFO - root - 2017-12-09 09:04:43.357201: step 8580, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.882 sec/batch; 79h:22m:05s remains)
INFO - root - 2017-12-09 09:04:52.066008: step 8590, loss = 0.82, batch loss = 0.69 (8.8 examples/sec; 0.910 sec/batch; 81h:52m:45s remains)
INFO - root - 2017-12-09 09:05:00.720027: step 8600, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.882 sec/batch; 79h:22m:17s remains)
2017-12-09 09:05:01.628324: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.0028386628 0.0020804233 0.0024071978 0.0031542131 0.0047639641 0.0067246282 0.0086802961 0.0096989153 0.0098945126 0.009636526 0.0090466095 0.0088102967 0.0089229122 0.0095458524 0.010847416][0.003775463 0.0034321016 0.0040537072 0.0054394943 0.0078550354 0.01059469 0.012637491 0.013905997 0.013952011 0.01326045 0.012679682 0.011945069 0.011993055 0.012936634 0.014863779][0.0056190724 0.0064677512 0.0082356129 0.011664267 0.015779562 0.019994507 0.022276277 0.023766018 0.023246583 0.022371978 0.022007281 0.021112101 0.020705123 0.021366363 0.02347987][0.0096512558 0.013178095 0.017888427 0.024896363 0.031900104 0.039079178 0.042540926 0.04434368 0.04283458 0.041555278 0.040571824 0.039272182 0.038427703 0.039120041 0.041984215][0.016509209 0.024331462 0.034007281 0.046337012 0.058224931 0.069338106 0.074737 0.077531993 0.075249486 0.072820991 0.070359848 0.068522856 0.067551054 0.06917575 0.073388256][0.024381539 0.036922846 0.052221972 0.069941573 0.08695212 0.10330443 0.11196584 0.11699934 0.11438745 0.11158745 0.10751724 0.10533957 0.10415049 0.1072704 0.11407176][0.030286457 0.046491716 0.066392064 0.0886689 0.1097345 0.12972191 0.14208877 0.14949045 0.14818878 0.14590617 0.14136122 0.14043534 0.14004096 0.14548841 0.15581092][0.031589121 0.04896827 0.070625335 0.094297536 0.11708026 0.13849136 0.15298232 0.16327366 0.16490328 0.16436052 0.16093282 0.16198318 0.1639695 0.17343497 0.18814309][0.027322859 0.043046419 0.063002087 0.084469296 0.10550423 0.12557812 0.14098939 0.1529815 0.15758489 0.15996929 0.15965785 0.16316108 0.16884941 0.18350941 0.20393966][0.01946497 0.03125396 0.046677407 0.0636144 0.080800906 0.0971159 0.11101744 0.12315504 0.13051836 0.13556169 0.1381534 0.14434591 0.15413065 0.17335548 0.19933169][0.011121475 0.018452248 0.028402006 0.039606538 0.051489536 0.063127987 0.074304119 0.085113168 0.0931926 0.099780381 0.1048748 0.11284822 0.12564839 0.14880219 0.17910056][0.0047991788 0.0085544856 0.013909149 0.020248901 0.027411023 0.034651957 0.04244335 0.050692942 0.057793155 0.0642674 0.069697313 0.077730879 0.091475725 0.11647823 0.14945394][0.001219894 0.0027701603 0.0051853503 0.0082720015 0.012039408 0.01592133 0.020590223 0.026097637 0.031280119 0.036404952 0.040729344 0.047367577 0.059827488 0.083886385 0.11701337][-0.0002004437 0.00029655604 0.001269819 0.0028026777 0.0045587867 0.0063615558 0.0086291144 0.011620123 0.014601876 0.0178438 0.02037034 0.024978878 0.035074614 0.056467988 0.087702543][-0.00052067608 -0.00039034506 5.0207018e-06 0.00088195765 0.0018578477 0.0026639267 0.0034035603 0.0046530417 0.0060464381 0.0077368747 0.0088992175 0.011837211 0.020001775 0.038441259 0.06657593]]...]
INFO - root - 2017-12-09 09:05:10.226395: step 8610, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 77h:43m:57s remains)
INFO - root - 2017-12-09 09:05:18.849173: step 8620, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.875 sec/batch; 78h:41m:38s remains)
INFO - root - 2017-12-09 09:05:27.664027: step 8630, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.876 sec/batch; 78h:47m:47s remains)
INFO - root - 2017-12-09 09:05:36.440548: step 8640, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.884 sec/batch; 79h:33m:16s remains)
INFO - root - 2017-12-09 09:05:44.984227: step 8650, loss = 0.82, batch loss = 0.69 (11.0 examples/sec; 0.728 sec/batch; 65h:27m:15s remains)
INFO - root - 2017-12-09 09:05:53.763357: step 8660, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.865 sec/batch; 77h:49m:43s remains)
INFO - root - 2017-12-09 09:06:02.461463: step 8670, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 78h:07m:49s remains)
INFO - root - 2017-12-09 09:06:11.200570: step 8680, loss = 0.81, batch loss = 0.68 (8.8 examples/sec; 0.914 sec/batch; 82h:11m:24s remains)
INFO - root - 2017-12-09 09:06:19.945914: step 8690, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.857 sec/batch; 77h:04m:55s remains)
INFO - root - 2017-12-09 09:06:28.544647: step 8700, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.852 sec/batch; 76h:39m:03s remains)
2017-12-09 09:06:29.435071: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.00066087791 -0.00064394542 -0.00061327161 -0.00057744788 -0.00053183053 -0.00047672121 -0.00041404806 -0.00034935796 -0.00030616595 -0.00028141896 -0.00029149686 -0.00032550626 -0.00039379435 -0.00050797767 -0.000597727][-0.00059836026 -0.00054119481 -0.00046446625 -0.00038171123 -0.00029467582 -0.00022291069 -0.00016200356 -0.00010468013 -6.22769e-05 -3.8536091e-05 -5.9226702e-05 -0.00010596425 -0.00020132581 -0.00036912868 -0.00052410661][-0.00022904819 3.1384523e-05 0.00031101244 0.00057076622 0.000753283 0.00080193515 0.00071569294 0.00057262491 0.00045382179 0.00035763433 0.00025971484 0.00015584525 2.9945921e-05 -0.00019165757 -0.00042822363][0.00069074222 0.0015030741 0.0023375379 0.0030653235 0.0034781296 0.0034361517 0.0029526085 0.0022822651 0.0017153993 0.0012968222 0.0010089083 0.00076703547 0.00054484786 0.00021197938 -0.00017816201][0.0023469653 0.0041560582 0.0060141222 0.00763887 0.0085614929 0.00847948 0.0073973648 0.0058726133 0.0045612417 0.0035755492 0.0028952763 0.0023172074 0.0018130527 0.0011787154 0.00042182818][0.0045028329 0.007543033 0.010747146 0.013638173 0.015413894 0.015526212 0.013937225 0.011527792 0.009380362 0.0076854797 0.0064290469 0.0052766209 0.0042024255 0.0029424212 0.0014972307][0.0065263123 0.010753996 0.015326199 0.019604355 0.02246307 0.023071917 0.021301534 0.018294826 0.015497175 0.013160031 0.011272037 0.0093891788 0.0074996883 0.0053308359 0.0029313837][0.00784251 0.012820288 0.018360112 0.023738952 0.027620403 0.028948057 0.027426645 0.0242773 0.021154627 0.01837847 0.015958805 0.013394327 0.010685089 0.0075916741 0.0042517465][0.0078634433 0.012924011 0.018699462 0.024508391 0.029013261 0.03103511 0.03009673 0.027273085 0.024198122 0.021263674 0.018556522 0.015590413 0.012370847 0.0087101338 0.0048433705][0.0066353292 0.011110809 0.016370932 0.021853341 0.026374947 0.028812686 0.028544756 0.026320385 0.023539472 0.020677259 0.017969141 0.01501149 0.011788332 0.0081631374 0.0044269436][0.0046077352 0.0080306549 0.012173538 0.016643068 0.020551343 0.022972416 0.023234144 0.021703608 0.019409917 0.016881425 0.014488807 0.01195295 0.0092232246 0.0062117022 0.0032026221][0.0025060468 0.00475701 0.00757238 0.010710746 0.013612002 0.015628312 0.016169216 0.015274632 0.013573945 0.01157181 0.0096965423 0.0078066997 0.0058343573 0.003729684 0.0017123448][0.00088422437 0.002125863 0.0037423333 0.0056032636 0.0074125174 0.0087991469 0.0093470113 0.0089287506 0.007846362 0.0064869779 0.0052240775 0.0040190737 0.0028115683 0.0015848284 0.00047966192][-8.7376975e-05 0.00046577334 0.0012271137 0.0021329566 0.0030579795 0.0038372308 0.0042358935 0.0041044839 0.0035401138 0.0027778163 0.0020706388 0.0014270504 0.00081013649 0.00022655673 -0.00025361736][-0.0005077782 -0.00031924769 -3.8253609e-05 0.00030771404 0.00067716261 0.0010177109 0.0012282941 0.0012124779 0.00098341657 0.00065057649 0.00033981673 6.6153472e-05 -0.00018496864 -0.00040101938 -0.00055857276]]...]
INFO - root - 2017-12-09 09:06:38.334848: step 8710, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.901 sec/batch; 81h:03m:39s remains)
INFO - root - 2017-12-09 09:06:46.908189: step 8720, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 77h:40m:41s remains)
INFO - root - 2017-12-09 09:06:55.730521: step 8730, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.879 sec/batch; 79h:02m:09s remains)
INFO - root - 2017-12-09 09:07:04.439285: step 8740, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.870 sec/batch; 78h:12m:05s remains)
INFO - root - 2017-12-09 09:07:13.019738: step 8750, loss = 0.82, batch loss = 0.69 (10.0 examples/sec; 0.796 sec/batch; 71h:35m:16s remains)
INFO - root - 2017-12-09 09:07:21.482348: step 8760, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.880 sec/batch; 79h:05m:50s remains)
INFO - root - 2017-12-09 09:07:30.282621: step 8770, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.884 sec/batch; 79h:31m:40s remains)
INFO - root - 2017-12-09 09:07:38.785202: step 8780, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.870 sec/batch; 78h:13m:22s remains)
INFO - root - 2017-12-09 09:07:47.445038: step 8790, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.846 sec/batch; 76h:03m:55s remains)
INFO - root - 2017-12-09 09:07:56.198568: step 8800, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.872 sec/batch; 78h:24m:37s remains)
2017-12-09 09:07:57.036912: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.00069105724 -0.00069079822 -0.0006911449 -0.00069138629 -0.00069127284 -0.00069082755 -0.00069030171 -0.00068986276 -0.00068938878 -0.00068880053 -0.00068845286 -0.0006887769 -0.00068971614 -0.00069105683 -0.00069239939][-0.00069044885 -0.00069036143 -0.00069087913 -0.00069126691 -0.00069121132 -0.00069072744 -0.00069012609 -0.00068964052 -0.00068908528 -0.00068841153 -0.00068794051 -0.00068800355 -0.00068852631 -0.00068943255 -0.00069038064][-0.00069018954 -0.000690324 -0.00069097854 -0.00069142011 -0.00069128542 -0.000690634 -0.00068983977 -0.00068925857 -0.0006887248 -0.00068819465 -0.00068786275 -0.00068790524 -0.00068818388 -0.00068872253 -0.00068932277][-0.00069004326 -0.00069036329 -0.00069112505 -0.00069153262 -0.0006912059 -0.00069031655 -0.0006893152 -0.00068863534 -0.00068823929 -0.00068800489 -0.00068794651 -0.00068807974 -0.00068825675 -0.00068854255 -0.00068885071][-0.00068991637 -0.00069032353 -0.00069109589 -0.00069146 -0.00069097214 -0.00068987242 -0.00068870606 -0.000687997 -0.00068771723 -0.00068775192 -0.00068794546 -0.00068818696 -0.00068836968 -0.00068852148 -0.00068863039][-0.0006897737 -0.00069011538 -0.00069078623 -0.00069104228 -0.00069051411 -0.00068933028 -0.00068805774 -0.000687303 -0.00068710576 -0.00068734831 -0.00068774226 -0.00068809435 -0.00068830175 -0.00068838528 -0.00068832742][-0.00068974035 -0.00068983459 -0.00069033634 -0.00069049047 -0.00068992615 -0.00068875618 -0.00068744534 -0.00068660913 -0.00068646274 -0.00068694045 -0.00068754732 -0.00068801729 -0.00068825006 -0.00068824284 -0.00068807078][-0.00068981404 -0.00068952248 -0.00068980694 -0.0006898662 -0.00068930566 -0.00068823807 -0.00068705325 -0.00068622467 -0.0006861164 -0.00068678794 -0.00068754837 -0.00068804319 -0.00068820524 -0.00068808038 -0.00068779394][-0.000689719 -0.00068916177 -0.00068925903 -0.00068926794 -0.00068885868 -0.00068804115 -0.00068714737 -0.00068650494 -0.000686386 -0.00068697287 -0.00068766624 -0.00068800291 -0.00068796449 -0.000687686 -0.00068736577][-0.00068942457 -0.00068877009 -0.00068879558 -0.00068883377 -0.0006886386 -0.00068813027 -0.00068748771 -0.00068700535 -0.00068680581 -0.00068708608 -0.00068749755 -0.00068761571 -0.0006874681 -0.00068714435 -0.0006868476][-0.0006892496 -0.00068855978 -0.00068857125 -0.00068863924 -0.00068857009 -0.00068825157 -0.00068776606 -0.0006872635 -0.0006869373 -0.00068696355 -0.00068713358 -0.00068718439 -0.00068707432 -0.00068682508 -0.0006865625][-0.00068931788 -0.00068865292 -0.00068866438 -0.00068876 -0.00068870181 -0.00068844738 -0.00068803923 -0.00068746647 -0.00068697415 -0.0006868256 -0.00068688678 -0.00068697636 -0.00068696862 -0.00068684091 -0.00068663486][-0.00068949244 -0.00068887789 -0.00068888033 -0.00068898796 -0.00068897195 -0.00068876374 -0.00068840088 -0.00068778661 -0.00068719487 -0.00068690273 -0.00068685878 -0.00068695436 -0.00068697409 -0.00068694062 -0.00068678986][-0.00068991439 -0.00068932964 -0.00068923173 -0.00068929448 -0.00068931957 -0.00068912643 -0.0006887482 -0.00068814738 -0.00068755826 -0.0006871972 -0.000687039 -0.00068707415 -0.000687069 -0.00068703614 -0.00068693113][-0.00069043005 -0.00068986363 -0.00068964623 -0.0006896194 -0.00068964454 -0.00068940478 -0.00068899407 -0.00068847771 -0.00068796112 -0.00068755768 -0.00068729941 -0.000687292 -0.00068725017 -0.00068715669 -0.00068704487]]...]
INFO - root - 2017-12-09 09:08:05.582325: step 8810, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.883 sec/batch; 79h:21m:41s remains)
INFO - root - 2017-12-09 09:08:14.311698: step 8820, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.875 sec/batch; 78h:42m:47s remains)
INFO - root - 2017-12-09 09:08:23.006399: step 8830, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.901 sec/batch; 81h:03m:07s remains)
INFO - root - 2017-12-09 09:08:31.691698: step 8840, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.886 sec/batch; 79h:40m:06s remains)
INFO - root - 2017-12-09 09:08:40.511022: step 8850, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.878 sec/batch; 78h:57m:00s remains)
INFO - root - 2017-12-09 09:08:49.209621: step 8860, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.880 sec/batch; 79h:09m:10s remains)
INFO - root - 2017-12-09 09:08:57.972581: step 8870, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.895 sec/batch; 80h:25m:35s remains)
INFO - root - 2017-12-09 09:09:06.538733: step 8880, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.902 sec/batch; 81h:04m:51s remains)
INFO - root - 2017-12-09 09:09:15.241083: step 8890, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.874 sec/batch; 78h:34m:57s remains)
INFO - root - 2017-12-09 09:09:23.897739: step 8900, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 78h:09m:02s remains)
2017-12-09 09:09:24.751552: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.0038400218 0.003417233 0.0029155223 0.0024938607 0.0021215926 0.0018270407 0.0016378728 0.0015434127 0.0015276819 0.0016325011 0.0018226295 0.0020922837 0.0023389508 0.0025563207 0.0027178344][0.0069704913 0.0062928493 0.0054044449 0.0045953938 0.0038399706 0.0032459022 0.0028245761 0.0025947245 0.0025386186 0.0026969661 0.003016413 0.0034585409 0.0039066048 0.0043299878 0.0046162349][0.01142787 0.01060445 0.0093625365 0.0081305867 0.0069359266 0.0059448555 0.0052147028 0.0047913659 0.0046546692 0.0048429044 0.005263445 0.0058630882 0.0064980006 0.0070942091 0.007422125][0.016644526 0.0157529 0.014230483 0.01266053 0.011112857 0.00975488 0.0087435273 0.0081452718 0.0079195481 0.0080937734 0.0085345563 0.0091928868 0.0098789465 0.010487917 0.010693759][0.021903768 0.021072244 0.019385228 0.01755739 0.015710954 0.014030947 0.012808008 0.012091691 0.011820119 0.011975058 0.012378361 0.012975023 0.013521248 0.013914952 0.013779258][0.026023747 0.025417028 0.023771059 0.021945031 0.020037441 0.018217312 0.016902722 0.016104199 0.0157593 0.015796479 0.016039174 0.016400615 0.016586743 0.016548282 0.015913961][0.027975988 0.027650615 0.026217513 0.024610627 0.02287716 0.021170905 0.019955855 0.019182824 0.018785197 0.018634664 0.018618768 0.018605784 0.018286778 0.017689472 0.016515313][0.027232988 0.027129941 0.025954464 0.024698423 0.023310209 0.021913009 0.020914771 0.020221066 0.019763472 0.01939819 0.019102208 0.018695345 0.017907975 0.016837243 0.015282413][0.023980362 0.024002057 0.023125803 0.022237943 0.021211423 0.020144764 0.019351542 0.018734351 0.018208524 0.017656907 0.017135456 0.016449541 0.015388642 0.014077092 0.012415024][0.01874475 0.018846866 0.0182795 0.01776026 0.017122533 0.01641316 0.015824353 0.015271761 0.014691368 0.014037332 0.013402907 0.012611021 0.011528765 0.010269918 0.0087996228][0.01279904 0.012882735 0.012579667 0.012366162 0.012073897 0.011685753 0.011295349 0.010848136 0.010288869 0.0096448585 0.009019427 0.008296527 0.0074032415 0.0064069177 0.0053157369][0.0072237216 0.0072477823 0.0071196286 0.0071065249 0.0070637353 0.0069373935 0.006741595 0.0064320266 0.0059798942 0.0054576993 0.0049561975 0.0044257129 0.0038338322 0.0032068607 0.0025552141][0.0029946216 0.0029577538 0.0029106939 0.0029699279 0.0030347363 0.0030462693 0.0029868255 0.0028235554 0.0025379623 0.0021994372 0.0018726627 0.001557461 0.0012431012 0.00093789044 0.00064290705][0.00051683647 0.00045652682 0.00044051948 0.00048984581 0.00055145303 0.00058852154 0.00058521557 0.00052112649 0.00039063784 0.00023463025 8.4004132e-05 -4.9111317e-05 -0.000167627 -0.00026789278 -0.0003551011][-0.00050190988 -0.00053935056 -0.00054853829 -0.00053025433 -0.00050205796 -0.00047839931 -0.00046592046 -0.00047359194 -0.00050396717 -0.00054738391 -0.00059187284 -0.00062969176 -0.00066066271 -0.00068211678 -0.00069667]]...]
INFO - root - 2017-12-09 09:09:33.480129: step 8910, loss = 0.81, batch loss = 0.68 (9.3 examples/sec; 0.862 sec/batch; 77h:29m:29s remains)
INFO - root - 2017-12-09 09:09:42.068792: step 8920, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.865 sec/batch; 77h:43m:01s remains)
INFO - root - 2017-12-09 09:09:50.670953: step 8930, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.878 sec/batch; 78h:54m:47s remains)
INFO - root - 2017-12-09 09:09:59.340328: step 8940, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.851 sec/batch; 76h:29m:27s remains)
INFO - root - 2017-12-09 09:10:08.223531: step 8950, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.893 sec/batch; 80h:15m:43s remains)
INFO - root - 2017-12-09 09:10:16.772532: step 8960, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.850 sec/batch; 76h:23m:50s remains)
INFO - root - 2017-12-09 09:10:25.524430: step 8970, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.854 sec/batch; 76h:47m:23s remains)
INFO - root - 2017-12-09 09:10:34.165601: step 8980, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 78h:06m:01s remains)
INFO - root - 2017-12-09 09:10:42.766422: step 8990, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 78h:07m:02s remains)
INFO - root - 2017-12-09 09:10:51.463962: step 9000, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.851 sec/batch; 76h:27m:40s remains)
2017-12-09 09:10:52.474512: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.00079762313 -0.0007958755 -0.00079580845 -0.00079726119 -0.00080035644 -0.0008040322 -0.00080732396 -0.000809341 -0.00081017322 -0.00081022159 -0.00081012206 -0.00080994639 -0.00080954033 -0.00080876116 -0.00080735219][-0.00079875049 -0.0007968946 -0.0007967705 -0.00079806615 -0.0008010923 -0.00080456882 -0.00080770167 -0.000809641 -0.00081053603 -0.00081056968 -0.00081039773 -0.0008101791 -0.0008096979 -0.00080877263 -0.00080728607][-0.00079862552 -0.0007967311 -0.0007965485 -0.0007977885 -0.00080080249 -0.00080433046 -0.00080762862 -0.0008098 -0.00081087468 -0.00081109512 -0.00081089919 -0.00081037643 -0.00080938928 -0.00080799038 -0.00080611609][-0.00079767127 -0.00079554168 -0.00079541217 -0.00079687074 -0.00080009096 -0.00080392958 -0.00080743915 -0.00081001513 -0.00081138464 -0.00081181811 -0.00081151386 -0.00081061519 -0.00080902979 -0.0008068232 -0.00080413884][-0.00079652789 -0.0007943582 -0.00079426175 -0.00079588161 -0.00079913519 -0.00080305559 -0.000806658 -0.00080942374 -0.00081104209 -0.0008116017 -0.00081122888 -0.00081003876 -0.00080803642 -0.00080533669 -0.00080210081][-0.00079546147 -0.00079350028 -0.00079357525 -0.00079528848 -0.00079852808 -0.00080249243 -0.00080607826 -0.00080878026 -0.00081030111 -0.00081080786 -0.00081047445 -0.00080910785 -0.00080688769 -0.00080401811 -0.00080067164][-0.00079495588 -0.00079320092 -0.00079346669 -0.00079521962 -0.00079826388 -0.00080209831 -0.00080557674 -0.00080807606 -0.00080949476 -0.00080996024 -0.00080963445 -0.000808159 -0.00080586 -0.00080314267 -0.00080009765][-0.0007957776 -0.00079407968 -0.00079414394 -0.00079570524 -0.00079850148 -0.00080197782 -0.00080520939 -0.00080753665 -0.0008088554 -0.00080929237 -0.0008089586 -0.00080755417 -0.00080533605 -0.00080279628 -0.0008000473][-0.00079777924 -0.00079612178 -0.000795954 -0.00079718139 -0.00079946173 -0.0008023355 -0.00080504268 -0.0008069778 -0.000808166 -0.00080859871 -0.000808434 -0.00080730475 -0.00080550916 -0.00080348091 -0.00080122007][-0.00080001308 -0.00079847354 -0.00079814671 -0.00079911394 -0.00080089655 -0.00080311281 -0.00080523262 -0.00080678461 -0.00080776337 -0.00080808357 -0.00080799405 -0.00080717873 -0.00080581865 -0.00080436387 -0.00080278068][-0.00080155826 -0.00080023048 -0.00079991197 -0.00080057484 -0.00080180913 -0.00080343022 -0.00080504658 -0.00080624653 -0.00080703408 -0.00080736162 -0.00080752897 -0.00080716982 -0.00080641324 -0.00080554554 -0.00080453244][-0.00080239307 -0.000801198 -0.0008009341 -0.00080142176 -0.00080224237 -0.00080328318 -0.0008042772 -0.00080516859 -0.00080581248 -0.00080612185 -0.00080638751 -0.00080642779 -0.00080627459 -0.00080607936 -0.00080578675][-0.00080222974 -0.00080116687 -0.00080089027 -0.00080123538 -0.00080175907 -0.00080239953 -0.00080298237 -0.00080361043 -0.000804217 -0.00080463896 -0.00080508157 -0.000805413 -0.00080573012 -0.00080590521 -0.00080600037][-0.00080099015 -0.00080009294 -0.00079991732 -0.00080011511 -0.00080039189 -0.000800758 -0.00080107566 -0.00080153986 -0.00080213591 -0.00080264261 -0.00080327271 -0.00080387876 -0.00080452254 -0.00080493133 -0.00080529583][-0.00079906319 -0.00079812348 -0.00079800776 -0.00079810695 -0.00079827051 -0.00079858623 -0.00079885771 -0.000799315 -0.00079992221 -0.000800577 -0.00080140115 -0.00080220058 -0.00080305926 -0.00080364128 -0.00080404489]]...]
INFO - root - 2017-12-09 09:11:01.174441: step 9010, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.851 sec/batch; 76h:29m:42s remains)
INFO - root - 2017-12-09 09:11:09.788599: step 9020, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.856 sec/batch; 76h:52m:50s remains)
INFO - root - 2017-12-09 09:11:18.371243: step 9030, loss = 0.83, batch loss = 0.70 (9.4 examples/sec; 0.852 sec/batch; 76h:35m:52s remains)
INFO - root - 2017-12-09 09:11:27.015187: step 9040, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.861 sec/batch; 77h:21m:19s remains)
INFO - root - 2017-12-09 09:11:35.707634: step 9050, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 77h:28m:58s remains)
INFO - root - 2017-12-09 09:11:44.323278: step 9060, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.893 sec/batch; 80h:13m:11s remains)
INFO - root - 2017-12-09 09:11:53.001627: step 9070, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.863 sec/batch; 77h:34m:19s remains)
INFO - root - 2017-12-09 09:12:01.597756: step 9080, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.853 sec/batch; 76h:39m:54s remains)
INFO - root - 2017-12-09 09:12:10.396169: step 9090, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.857 sec/batch; 76h:58m:18s remains)
INFO - root - 2017-12-09 09:12:19.018001: step 9100, loss = 0.83, batch loss = 0.70 (9.0 examples/sec; 0.885 sec/batch; 79h:31m:13s remains)
2017-12-09 09:12:19.822647: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.054981012 0.070966676 0.088994794 0.10824184 0.12660274 0.14192224 0.15272079 0.15853429 0.15841554 0.15219861 0.14088954 0.12730424 0.11404636 0.10421344 0.099364653][0.060495768 0.080898724 0.10461126 0.13008788 0.15455444 0.17505363 0.18856706 0.1950592 0.19342595 0.18449147 0.16957706 0.15281737 0.1373523 0.12587553 0.11960946][0.064481735 0.089295089 0.11871994 0.15103295 0.18199389 0.20754521 0.22372095 0.23038766 0.22708118 0.21514653 0.19689251 0.17723063 0.15977272 0.14687292 0.1390004][0.065706909 0.093938656 0.12807573 0.16625309 0.20301202 0.23348983 0.25223288 0.25966302 0.25524628 0.24115077 0.22044927 0.19841331 0.17948475 0.16514772 0.15599597][0.063820906 0.094079234 0.131609 0.17422172 0.21559593 0.25049955 0.27228865 0.2812666 0.27674213 0.26187965 0.23970383 0.21589492 0.19585648 0.18029636 0.17024408][0.059468929 0.090029523 0.12923658 0.17453757 0.21916069 0.25743076 0.28202125 0.29290298 0.28944209 0.27487138 0.25245363 0.22802404 0.20741792 0.19141884 0.1811645][0.053772315 0.08330863 0.12256978 0.16886652 0.215503 0.2562187 0.28319219 0.29605815 0.29373524 0.28015479 0.25873446 0.23452081 0.21391359 0.1979024 0.18794596][0.047238551 0.074746683 0.11251048 0.15821339 0.20522295 0.24712472 0.27625203 0.29128924 0.29097295 0.27884358 0.25856069 0.23532934 0.21493016 0.19913512 0.18986391][0.041176528 0.0659624 0.10113748 0.14452375 0.19021125 0.23194583 0.26219785 0.27931696 0.2813659 0.27142927 0.25297219 0.23088029 0.21094033 0.19556779 0.18706223][0.035097893 0.05728839 0.089608885 0.12986307 0.17311183 0.21344346 0.24407454 0.262488 0.26660255 0.25905466 0.24287584 0.22278306 0.20386219 0.18952864 0.18206559][0.0294349 0.048947558 0.077865653 0.11449464 0.15460087 0.19268259 0.22275437 0.24193831 0.24777724 0.24206553 0.22823821 0.21061106 0.19354451 0.18080455 0.17462155][0.025248697 0.041722551 0.066759929 0.099285305 0.13571958 0.17122038 0.20010522 0.21916853 0.22581027 0.22182265 0.21020408 0.19512987 0.18053108 0.16980787 0.165117][0.021606157 0.035089482 0.056318775 0.084584214 0.11687297 0.14924669 0.17619348 0.19436625 0.20116965 0.19812107 0.18812132 0.1754594 0.16350737 0.1552912 0.15246496][0.018277727 0.028973807 0.046318516 0.070260145 0.098417722 0.12750432 0.15217458 0.1691184 0.17551142 0.172869 0.16417278 0.15329218 0.14376204 0.13803709 0.13722008][0.014623506 0.022724966 0.036441665 0.056072094 0.079803638 0.10535524 0.12746751 0.14283079 0.14857227 0.146322 0.13883597 0.12969406 0.12247515 0.1190104 0.12006284]]...]
INFO - root - 2017-12-09 09:12:28.530943: step 9110, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.866 sec/batch; 77h:48m:27s remains)
INFO - root - 2017-12-09 09:12:37.065614: step 9120, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 77h:59m:22s remains)
INFO - root - 2017-12-09 09:12:45.705024: step 9130, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.874 sec/batch; 78h:33m:05s remains)
INFO - root - 2017-12-09 09:12:54.535118: step 9140, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.883 sec/batch; 79h:16m:30s remains)
INFO - root - 2017-12-09 09:13:03.256291: step 9150, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.865 sec/batch; 77h:40m:45s remains)
INFO - root - 2017-12-09 09:13:11.725742: step 9160, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.835 sec/batch; 75h:01m:44s remains)
INFO - root - 2017-12-09 09:13:20.491235: step 9170, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 77h:51m:08s remains)
INFO - root - 2017-12-09 09:13:28.815927: step 9180, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.845 sec/batch; 75h:54m:15s remains)
INFO - root - 2017-12-09 09:13:37.415299: step 9190, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.856 sec/batch; 76h:52m:15s remains)
INFO - root - 2017-12-09 09:13:46.049967: step 9200, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 77h:58m:02s remains)
2017-12-09 09:13:46.962187: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.0052326317 0.0066707181 0.00874929 0.010986859 0.013403917 0.01541588 0.01670007 0.016858136 0.015797425 0.014043075 0.011882793 0.0097641759 0.0077493084 0.0059705521 0.0043920479][0.0082294922 0.010540195 0.013747917 0.017251687 0.020774212 0.023460101 0.024930488 0.02485439 0.022979895 0.020139623 0.01692106 0.013984979 0.011236175 0.0087042609 0.0064538191][0.010971883 0.014676883 0.019730853 0.02503554 0.030153641 0.034103561 0.036147092 0.036105532 0.03380385 0.030293407 0.0263148 0.022678597 0.019134179 0.015491719 0.011909141][0.01356178 0.018921521 0.026293429 0.034319043 0.042045448 0.048152164 0.051689 0.052575249 0.050619673 0.046989802 0.042591985 0.038349573 0.033838551 0.028556861 0.022654062][0.01748183 0.025441352 0.0359404 0.047343004 0.058330048 0.067337111 0.072960816 0.075355306 0.074541092 0.071543232 0.067090422 0.06215274 0.056362666 0.048897471 0.039778661][0.022514401 0.033946697 0.048385143 0.063810065 0.078358538 0.090114735 0.097537175 0.1013649 0.10177226 0.099741422 0.095700078 0.090306081 0.083307147 0.073501527 0.060926378][0.027713656 0.042542227 0.060828928 0.08008977 0.097931832 0.11197925 0.12067991 0.12533782 0.12662624 0.12566808 0.12217116 0.11671153 0.10898004 0.097543992 0.08221674][0.032027632 0.049311291 0.0703836 0.0923126 0.11228501 0.12754729 0.13676707 0.14172499 0.14345558 0.143162 0.14020787 0.13488053 0.12686117 0.11468236 0.097938329][0.034439921 0.052730132 0.074816056 0.097684957 0.11826812 0.13370226 0.14295864 0.14797162 0.14993264 0.14995314 0.14719068 0.14184864 0.13366169 0.12139231 0.1044103][0.033936374 0.051826727 0.0733477 0.095533483 0.11530317 0.12999853 0.13886236 0.14375708 0.14574146 0.14574741 0.14289302 0.13738276 0.12900187 0.11696798 0.1006108][0.030061318 0.046232581 0.065692417 0.085780248 0.10365671 0.11697553 0.12516938 0.12977265 0.13163611 0.13145146 0.1284838 0.12292504 0.11459975 0.10315427 0.088074014][0.023653271 0.036949638 0.053085048 0.069876984 0.084913857 0.096252523 0.10342223 0.10753143 0.10918869 0.10888626 0.10609632 0.1009545 0.093308106 0.083096661 0.070037082][0.016317511 0.02618094 0.038252097 0.050915767 0.062325653 0.071042649 0.076680347 0.079960614 0.081290714 0.081018783 0.078809641 0.07467325 0.068451412 0.060214933 0.049920887][0.0095373616 0.01604601 0.024105005 0.032622002 0.040331263 0.046252187 0.050105281 0.05232694 0.053219922 0.053061329 0.051613133 0.048797358 0.044442527 0.038613919 0.031399604][0.0043615066 0.00804899 0.012706812 0.01767553 0.022193311 0.025660109 0.027902998 0.029164419 0.029668642 0.029631466 0.028912503 0.027380662 0.024861736 0.021358201 0.016999513]]...]
INFO - root - 2017-12-09 09:13:55.715192: step 9210, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.875 sec/batch; 78h:35m:18s remains)
INFO - root - 2017-12-09 09:14:04.367990: step 9220, loss = 0.83, batch loss = 0.70 (8.9 examples/sec; 0.897 sec/batch; 80h:34m:04s remains)
INFO - root - 2017-12-09 09:14:12.989447: step 9230, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.848 sec/batch; 76h:06m:25s remains)
INFO - root - 2017-12-09 09:14:21.812070: step 9240, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.884 sec/batch; 79h:22m:12s remains)
INFO - root - 2017-12-09 09:14:30.528633: step 9250, loss = 0.82, batch loss = 0.69 (8.8 examples/sec; 0.906 sec/batch; 81h:18m:51s remains)
INFO - root - 2017-12-09 09:14:39.227920: step 9260, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.879 sec/batch; 78h:54m:06s remains)
INFO - root - 2017-12-09 09:14:48.017052: step 9270, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.888 sec/batch; 79h:43m:38s remains)
INFO - root - 2017-12-09 09:14:56.556564: step 9280, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 78h:14m:38s remains)
INFO - root - 2017-12-09 09:15:05.167159: step 9290, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.860 sec/batch; 77h:11m:41s remains)
INFO - root - 2017-12-09 09:15:13.695336: step 9300, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.866 sec/batch; 77h:46m:55s remains)
2017-12-09 09:15:14.597475: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.00096921891 -0.00096821261 -0.00096778857 -0.00096776237 -0.00096767739 -0.00096744671 -0.00096681184 -0.00096558081 -0.00096342777 -0.00096055347 -0.00095732987 -0.00095393223 -0.00095058227 -0.00094753713 -0.00094498461][-0.00096896407 -0.00096788676 -0.00096744933 -0.00096750009 -0.00096743868 -0.0009671622 -0.00096635241 -0.00096482481 -0.00096237723 -0.00095927523 -0.00095584133 -0.00095238921 -0.00094914 -0.00094629737 -0.00094402052][-0.00096653512 -0.0009655293 -0.000965253 -0.00096545514 -0.00096544978 -0.00096515007 -0.00096423924 -0.000962534 -0.00095995347 -0.00095681229 -0.00095348805 -0.00095031509 -0.00094747078 -0.00094511383 -0.0009432988][-0.00096340437 -0.00096254051 -0.00096238265 -0.000962561 -0.00096244755 -0.00096191908 -0.00096077478 -0.00095893885 -0.00095641369 -0.00095350359 -0.00095057063 -0.0009479312 -0.000945679 -0.00094390067 -0.00094259082][-0.00096079451 -0.00096017495 -0.00096005254 -0.0009600012 -0.00095954968 -0.00095864909 -0.00095717848 -0.00095518091 -0.00095277932 -0.00095022068 -0.00094777666 -0.00094571104 -0.00094406982 -0.00094285072 -0.00094198028][-0.000959181 -0.00095875654 -0.00095844961 -0.00095798471 -0.00095707079 -0.000955747 -0.00095403282 -0.00095201086 -0.00094985147 -0.00094767672 -0.0009457118 -0.00094415544 -0.00094299245 -0.00094215217 -0.00094155036][-0.00095831591 -0.00095797831 -0.00095757691 -0.00095670071 -0.000955294 -0.00095357141 -0.00095168268 -0.00094968226 -0.0009477312 -0.00094589993 -0.0009443271 -0.00094314606 -0.00094230706 -0.00094171084 -0.00094126881][-0.000959064 -0.00095868239 -0.00095809519 -0.00095684553 -0.0009549617 -0.00095279387 -0.00095064 -0.00094853435 -0.00094663119 -0.00094496383 -0.00094359246 -0.00094258611 -0.00094190933 -0.00094142265 -0.00094106316][-0.0009609416 -0.00096054422 -0.00095970219 -0.00095801969 -0.00095564593 -0.00095300603 -0.00095051632 -0.00094821176 -0.00094623031 -0.000944577 -0.00094324473 -0.00094228447 -0.0009416524 -0.0009412092 -0.000940891][-0.00096304284 -0.00096269383 -0.00096162426 -0.00095951208 -0.000956676 -0.00095359795 -0.0009507566 -0.00094822 -0.00094609126 -0.00094437657 -0.000943015 -0.00094206049 -0.00094144937 -0.00094104651 -0.0009407528][-0.00096437382 -0.00096402381 -0.000962855 -0.00096060109 -0.00095761509 -0.00095434737 -0.00095128705 -0.00094856415 -0.0009462639 -0.00094442104 -0.0009429967 -0.00094203866 -0.00094144128 -0.00094104395 -0.00094075082][-0.00096408115 -0.0009637188 -0.00096262112 -0.0009605221 -0.00095772825 -0.0009545848 -0.0009515463 -0.00094882591 -0.00094650226 -0.00094463286 -0.0009432052 -0.00094224495 -0.0009416347 -0.00094121625 -0.00094089279][-0.00096195174 -0.00096176361 -0.00096095662 -0.00095927221 -0.00095691677 -0.0009541547 -0.00095138996 -0.00094885734 -0.000946656 -0.0009448622 -0.00094350975 -0.00094260566 -0.00094201288 -0.00094155985 -0.00094117894][-0.00095871033 -0.00095870841 -0.0009582247 -0.00095700967 -0.0009551628 -0.0009529026 -0.00095057051 -0.00094842562 -0.0009465263 -0.00094494311 -0.0009437119 -0.00094285945 -0.00094227638 -0.00094180333 -0.00094139873][-0.00095521391 -0.00095526385 -0.00095504208 -0.00095426355 -0.00095294422 -0.0009512438 -0.00094944716 -0.00094779907 -0.0009463146 -0.00094502838 -0.00094396679 -0.00094318239 -0.00094259519 -0.0009420765 -0.00094162632]]...]
INFO - root - 2017-12-09 09:15:23.277083: step 9310, loss = 0.82, batch loss = 0.69 (8.8 examples/sec; 0.904 sec/batch; 81h:11m:22s remains)
INFO - root - 2017-12-09 09:15:31.795202: step 9320, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.837 sec/batch; 75h:09m:14s remains)
INFO - root - 2017-12-09 09:15:40.549731: step 9330, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.880 sec/batch; 78h:59m:52s remains)
INFO - root - 2017-12-09 09:15:49.291297: step 9340, loss = 0.83, batch loss = 0.70 (9.7 examples/sec; 0.827 sec/batch; 74h:13m:50s remains)
INFO - root - 2017-12-09 09:15:58.135715: step 9350, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.888 sec/batch; 79h:40m:44s remains)
INFO - root - 2017-12-09 09:16:06.620139: step 9360, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.850 sec/batch; 76h:19m:15s remains)
INFO - root - 2017-12-09 09:16:15.403963: step 9370, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.865 sec/batch; 77h:37m:23s remains)
INFO - root - 2017-12-09 09:16:23.816029: step 9380, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.878 sec/batch; 78h:46m:50s remains)
INFO - root - 2017-12-09 09:16:32.450455: step 9390, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.872 sec/batch; 78h:16m:25s remains)
INFO - root - 2017-12-09 09:16:41.007057: step 9400, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.884 sec/batch; 79h:20m:12s remains)
2017-12-09 09:16:41.831240: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.19616066 0.19563645 0.19245647 0.1888198 0.184827 0.18060179 0.17641774 0.17113863 0.16616933 0.16066533 0.15453151 0.14853345 0.14298981 0.13839203 0.13414282][0.19768058 0.19837902 0.19663456 0.19412595 0.19107653 0.18772402 0.18382336 0.17847133 0.17301247 0.16691065 0.16021146 0.15333915 0.14725076 0.1422947 0.13770978][0.19628015 0.19777034 0.19762054 0.19666545 0.19538468 0.19350161 0.19016404 0.18534851 0.17941622 0.17262296 0.16510646 0.15738921 0.150573 0.14480127 0.13990645][0.19516926 0.197646 0.19932491 0.20005682 0.20044294 0.20007797 0.19756339 0.19326068 0.18689331 0.17937754 0.17092124 0.16231215 0.15488832 0.14823353 0.14280851][0.19461437 0.19842796 0.20166014 0.20423467 0.20643871 0.20720911 0.20509279 0.20083603 0.19385409 0.18577911 0.17637272 0.16707791 0.15915421 0.15203959 0.14615822][0.19604868 0.20129997 0.2055151 0.20915905 0.21221317 0.21401905 0.21269414 0.20824243 0.20066004 0.19166186 0.18159059 0.17124589 0.16234609 0.15480143 0.14862153][0.19785202 0.20385078 0.20863537 0.2133719 0.21720041 0.21952879 0.21853605 0.21423264 0.20622739 0.19641557 0.18555182 0.17452271 0.16490591 0.15688573 0.1503493][0.20083594 0.20708701 0.2115529 0.21611811 0.21978991 0.22187041 0.2207443 0.2163868 0.20805697 0.19790378 0.18647684 0.17521521 0.16543555 0.15708518 0.15036899][0.20225215 0.20967484 0.21446468 0.21891853 0.22211856 0.2236519 0.22203535 0.21695544 0.2080915 0.19726568 0.18568854 0.17431052 0.16442534 0.15595607 0.14895691][0.20077007 0.2088536 0.21360554 0.21815875 0.22134794 0.22244219 0.22052108 0.21491261 0.20565675 0.19437267 0.18260919 0.17133713 0.16147229 0.15309574 0.14617637][0.1963231 0.20482005 0.20971751 0.21428035 0.21748826 0.21860576 0.21678822 0.21115078 0.20193061 0.19043255 0.17863688 0.16712874 0.15696289 0.14858414 0.1418355][0.18799818 0.19653422 0.20164652 0.20665433 0.21049112 0.21179081 0.2100845 0.20487253 0.19615224 0.18505608 0.17337391 0.16206887 0.15210207 0.14371836 0.13713872][0.17721137 0.18566066 0.19068424 0.19544335 0.19932444 0.20070866 0.19923437 0.19441052 0.18641768 0.17640015 0.1656899 0.15522671 0.14603662 0.13826866 0.13226295][0.16481727 0.17282845 0.17770624 0.18249558 0.18632288 0.187947 0.18695123 0.18268687 0.1756314 0.16671346 0.15733263 0.14804919 0.13989358 0.13308984 0.12792939][0.15377671 0.16047528 0.16411284 0.16826542 0.17172667 0.1733426 0.17282936 0.16959848 0.16403189 0.1567772 0.14906363 0.1412247 0.134291 0.12862006 0.12434678]]...]
INFO - root - 2017-12-09 09:16:50.660949: step 9410, loss = 0.81, batch loss = 0.68 (8.7 examples/sec; 0.924 sec/batch; 82h:55m:35s remains)
INFO - root - 2017-12-09 09:16:59.374338: step 9420, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.883 sec/batch; 79h:13m:59s remains)
INFO - root - 2017-12-09 09:17:08.071516: step 9430, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 78h:12m:23s remains)
INFO - root - 2017-12-09 09:17:16.788323: step 9440, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.882 sec/batch; 79h:08m:19s remains)
INFO - root - 2017-12-09 09:17:25.515100: step 9450, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.891 sec/batch; 79h:54m:38s remains)
INFO - root - 2017-12-09 09:17:34.071763: step 9460, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.877 sec/batch; 78h:41m:10s remains)
INFO - root - 2017-12-09 09:17:42.631021: step 9470, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.879 sec/batch; 78h:51m:38s remains)
INFO - root - 2017-12-09 09:17:51.073742: step 9480, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 77h:30m:22s remains)
INFO - root - 2017-12-09 09:17:59.642114: step 9490, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.896 sec/batch; 80h:21m:33s remains)
INFO - root - 2017-12-09 09:18:08.411458: step 9500, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.890 sec/batch; 79h:49m:41s remains)
2017-12-09 09:18:09.192435: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.053761169 0.063002363 0.072552525 0.082182571 0.091337 0.098798931 0.10328172 0.10438026 0.10192771 0.095931068 0.08680287 0.075405926 0.063227668 0.051341698 0.040829461][0.062503651 0.073371477 0.084151685 0.094436005 0.10383794 0.11115044 0.11496908 0.11467737 0.11030954 0.10227755 0.091078654 0.077576078 0.063378826 0.049801353 0.038040146][0.068965159 0.081118383 0.09299963 0.1040416 0.11371669 0.12068845 0.12349474 0.12128726 0.11442399 0.10374842 0.090083115 0.074455 0.058569718 0.043992661 0.031811327][0.0726743 0.085880384 0.098704115 0.11043487 0.12050953 0.12738754 0.12945341 0.12566665 0.11662235 0.10351679 0.087531433 0.069909438 0.052582461 0.037319247 0.025103685][0.072785437 0.086843185 0.10059429 0.11313999 0.12383378 0.13101766 0.13299239 0.12843065 0.11792286 0.1029567 0.085081361 0.06591931 0.04758504 0.031941414 0.019822543][0.069506764 0.084574923 0.099493816 0.11319323 0.12493456 0.13287649 0.13524128 0.13046189 0.11903781 0.10266566 0.083333343 0.063137822 0.044231851 0.028462078 0.016522689][0.063089274 0.078997359 0.095008224 0.10970265 0.12227447 0.13093534 0.13393949 0.12952211 0.11798532 0.10118499 0.081333123 0.060897082 0.041948296 0.026335828 0.014687861][0.054498628 0.0706122 0.087068439 0.10227598 0.11534879 0.12453147 0.12815745 0.12436991 0.11334267 0.096948318 0.077540942 0.057734393 0.03946919 0.024566161 0.013583916][0.044758514 0.060132395 0.076083422 0.090961456 0.10393071 0.11337452 0.11757137 0.11471361 0.10480394 0.089704394 0.07162901 0.053143427 0.036127329 0.022380631 0.012415247][0.034813479 0.048652705 0.063308567 0.077199727 0.089580365 0.098935492 0.1035293 0.10165483 0.093246035 0.080011316 0.06389907 0.047288958 0.032013297 0.019829864 0.011131148][0.025563424 0.037356682 0.050260462 0.062831156 0.074231125 0.082969189 0.087423764 0.086127408 0.079180263 0.068088025 0.054485153 0.040359322 0.02737526 0.017079534 0.0097182123][0.017763192 0.027079668 0.037666369 0.048306808 0.05814939 0.065737173 0.069648989 0.068792783 0.063396811 0.054678638 0.043874428 0.032566935 0.022156013 0.013868924 0.0078697363][0.01163944 0.018511916 0.026569314 0.034888979 0.04257454 0.04838283 0.05125808 0.050569829 0.046601828 0.040252235 0.032420073 0.024202587 0.016576575 0.010396615 0.0058106878][0.0062872996 0.010776572 0.01615783 0.021819644 0.027069135 0.031043896 0.032991 0.03258561 0.030060468 0.02596618 0.020889899 0.015539587 0.010561464 0.0064783674 0.0034051375][0.0021470203 0.0045304904 0.0074951644 0.010679235 0.013616316 0.015832715 0.016941065 0.016796824 0.015504264 0.013345853 0.010636903 0.0077511822 0.0050567477 0.0028409068 0.0011891074]]...]
INFO - root - 2017-12-09 09:18:17.804968: step 9510, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.854 sec/batch; 76h:38m:43s remains)
INFO - root - 2017-12-09 09:18:26.286645: step 9520, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.875 sec/batch; 78h:30m:12s remains)
INFO - root - 2017-12-09 09:18:34.936581: step 9530, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.838 sec/batch; 75h:10m:23s remains)
INFO - root - 2017-12-09 09:18:43.571033: step 9540, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.860 sec/batch; 77h:09m:26s remains)
INFO - root - 2017-12-09 09:18:52.141827: step 9550, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.850 sec/batch; 76h:12m:46s remains)
INFO - root - 2017-12-09 09:19:00.720000: step 9560, loss = 0.82, batch loss = 0.69 (8.4 examples/sec; 0.949 sec/batch; 85h:07m:03s remains)
INFO - root - 2017-12-09 09:19:09.412238: step 9570, loss = 0.82, batch loss = 0.69 (9.9 examples/sec; 0.808 sec/batch; 72h:30m:44s remains)
INFO - root - 2017-12-09 09:19:17.846112: step 9580, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 77h:57m:42s remains)
INFO - root - 2017-12-09 09:19:26.375959: step 9590, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.849 sec/batch; 76h:11m:43s remains)
INFO - root - 2017-12-09 09:19:34.856945: step 9600, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.865 sec/batch; 77h:37m:05s remains)
2017-12-09 09:19:35.777619: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.20819174 0.21137556 0.21454574 0.21855529 0.22272114 0.22733 0.23284157 0.23925081 0.24279219 0.24187072 0.23596574 0.22873715 0.22041491 0.21256877 0.20705998][0.21720655 0.21916115 0.22151169 0.22443302 0.22770552 0.23133473 0.23644708 0.24265596 0.24627894 0.24472165 0.23736489 0.22795363 0.21739562 0.20799798 0.201428][0.22762284 0.22900711 0.23113807 0.23369884 0.23686756 0.23990364 0.24431777 0.25020236 0.25295725 0.24973463 0.23968914 0.22744375 0.21430463 0.20264411 0.1944834][0.23847853 0.24102138 0.24415743 0.2471993 0.25090763 0.25365728 0.25732425 0.26167837 0.2621685 0.2560361 0.24235804 0.22595957 0.20926745 0.1948767 0.18486814][0.24622911 0.25098345 0.25590879 0.26060075 0.26570919 0.26840562 0.27079287 0.27209905 0.26900035 0.25872046 0.24094686 0.22056739 0.20021342 0.18298887 0.17094645][0.24981003 0.25736368 0.26472974 0.27188492 0.27867252 0.28178838 0.28284776 0.28001964 0.27168906 0.25649583 0.2342128 0.2094036 0.18524736 0.16524623 0.15120134][0.24854888 0.25930637 0.26912645 0.2785081 0.28677386 0.28985482 0.28895053 0.28188458 0.26846603 0.24857052 0.22246011 0.19431956 0.16707514 0.1445803 0.1287985][0.24316093 0.25624287 0.26735726 0.27774087 0.286339 0.28872427 0.28576311 0.2750861 0.25769731 0.23454317 0.20565818 0.17561744 0.1465655 0.12267691 0.1059204][0.23410155 0.24888082 0.26064608 0.27107784 0.27889907 0.27999884 0.27527049 0.26189011 0.24213594 0.21754216 0.18794008 0.15722558 0.12725724 0.10274599 0.085241146][0.22119933 0.23658486 0.24828446 0.25828773 0.26522377 0.26548427 0.25943473 0.24485631 0.22442299 0.19967236 0.17100172 0.14134122 0.11251985 0.088695019 0.071364313][0.20448893 0.21946153 0.23053241 0.23957008 0.2455897 0.24597448 0.24020192 0.22615615 0.20667452 0.18328772 0.15667234 0.12857862 0.1013421 0.078817233 0.062309016][0.18460375 0.19858226 0.20893741 0.21719839 0.22268635 0.22331046 0.2182475 0.20594497 0.1887109 0.16790524 0.14420979 0.11892045 0.094357125 0.073637888 0.058327124][0.16317512 0.17547329 0.18470469 0.19221094 0.19707221 0.19802208 0.19390136 0.18344782 0.16891518 0.15134853 0.13152729 0.10980871 0.088757329 0.070871964 0.057671998][0.14086202 0.15124662 0.158945 0.16511756 0.16901527 0.17037787 0.16740032 0.15957111 0.1484551 0.13482368 0.11931746 0.10176445 0.084600821 0.070092663 0.059651949][0.12094685 0.128766 0.13436112 0.13910927 0.14227997 0.14405519 0.1424911 0.13751327 0.12996197 0.12057608 0.10976744 0.097086467 0.084539495 0.073960848 0.066710211]]...]
INFO - root - 2017-12-09 09:19:44.503477: step 9610, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.877 sec/batch; 78h:37m:16s remains)
INFO - root - 2017-12-09 09:19:53.095341: step 9620, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.879 sec/batch; 78h:47m:36s remains)
INFO - root - 2017-12-09 09:20:01.823218: step 9630, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.875 sec/batch; 78h:29m:50s remains)
INFO - root - 2017-12-09 09:20:10.536298: step 9640, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.840 sec/batch; 75h:19m:25s remains)
INFO - root - 2017-12-09 09:20:19.154547: step 9650, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.891 sec/batch; 79h:55m:51s remains)
INFO - root - 2017-12-09 09:20:27.692766: step 9660, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.881 sec/batch; 79h:00m:24s remains)
INFO - root - 2017-12-09 09:20:36.447508: step 9670, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 77h:45m:44s remains)
INFO - root - 2017-12-09 09:20:45.061791: step 9680, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.863 sec/batch; 77h:23m:47s remains)
INFO - root - 2017-12-09 09:20:53.871929: step 9690, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.876 sec/batch; 78h:35m:38s remains)
INFO - root - 2017-12-09 09:21:02.634524: step 9700, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.884 sec/batch; 79h:15m:08s remains)
2017-12-09 09:21:03.506615: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.0048773717 0.0038836729 0.0031927028 0.0029554078 0.0031157485 0.0034734085 0.0037591918 0.0037456187 0.0033430434 0.0026124152 0.0017159518 0.00081692229 5.027873e-05 -0.00050691841 -0.00084698317][0.0071436414 0.0058055161 0.0049090791 0.0046457215 0.0049361526 0.00554652 0.006092642 0.0062195961 0.0057568084 0.0047483463 0.0034206407 0.002027595 0.00080134568 -0.00010904134 -0.00067394419][0.008919403 0.0073043867 0.0063018482 0.0061251172 0.0067013018 0.0077677271 0.0088015143 0.0092730634 0.0088836923 0.0076473546 0.0058552874 0.0038703852 0.0020386667 0.00060835318 -0.00032972259][0.0095848478 0.007806608 0.0068956534 0.0070105544 0.0081060994 0.0098762494 0.01166309 0.012701798 0.012529008 0.01112346 0.008859314 0.0062303171 0.0037005753 0.001629367 0.00019539194][0.0090123732 0.0072318283 0.0065968717 0.0071793585 0.0089773275 0.011618862 0.014336951 0.016108342 0.01627887 0.014786607 0.012097083 0.0088456087 0.0056098262 0.0028562667 0.00086137326][0.0075842827 0.0059270565 0.0056769676 0.0068198121 0.0093744844 0.012889981 0.016536493 0.019061241 0.01965799 0.018213563 0.015254628 0.011508048 0.0076337936 0.0042043952 0.0016181928][0.0059612445 0.0045177271 0.0046320562 0.0062397635 0.0093419664 0.013428105 0.017672962 0.020723309 0.02170955 0.020447385 0.017467652 0.013514034 0.0092658959 0.0053635212 0.0023064162][0.0044235615 0.0032917843 0.0037243092 0.0056205043 0.0089295711 0.013137426 0.017490406 0.020686053 0.021911478 0.020924645 0.018190619 0.014364654 0.010083018 0.0060134791 0.0027229313][0.0031312294 0.0023226966 0.0029274556 0.004850361 0.0079829693 0.011847734 0.015783299 0.018691601 0.019919839 0.019235246 0.01698764 0.01365364 0.0097514428 0.0059119673 0.0027166919][0.0020169744 0.0015340315 0.0021637238 0.0038473271 0.0064733708 0.00965623 0.012845809 0.015196949 0.016236819 0.015800251 0.014131639 0.011521293 0.0083303656 0.0050751027 0.0022886784][0.00095696957 0.00072118384 0.0012735976 0.0025696051 0.0045209718 0.0068621547 0.0091852583 0.010901043 0.011678459 0.011416413 0.010282427 0.0084404293 0.0061164 0.0036724056 0.0015321053][6.119255e-05 -4.0840125e-05 0.00034373358 0.0011959965 0.0024657077 0.0039913696 0.0054956512 0.0066012735 0.0070936284 0.0069316528 0.0062297191 0.0050812778 0.0036094752 0.0020300944 0.00062753074][-0.00058217417 -0.00062714785 -0.00042213721 3.466662e-05 0.000734388 0.0015997391 0.0024553295 0.00306758 0.0033086929 0.0031826883 0.0027812342 0.0021685811 0.0013956584 0.00056086387 -0.00018496474][-0.00092544011 -0.00094676751 -0.00086822378 -0.00067674072 -0.00036315294 4.3095672e-05 0.00044742355 0.00072215346 0.00080052763 0.00069835735 0.00048444304 0.00020830997 -0.00011000503 -0.00044103846 -0.00073282653][-0.0010424275 -0.0010463074 -0.0010275216 -0.0009722262 -0.00086629251 -0.00070985919 -0.00054270419 -0.00042713614 -0.00040078862 -0.00046277413 -0.00057140784 -0.00069087732 -0.00080379669 -0.00090215052 -0.00098196138]]...]
INFO - root - 2017-12-09 09:21:12.197993: step 9710, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 78h:04m:05s remains)
INFO - root - 2017-12-09 09:21:20.778130: step 9720, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 77h:47m:33s remains)
INFO - root - 2017-12-09 09:21:29.460250: step 9730, loss = 0.82, batch loss = 0.69 (8.5 examples/sec; 0.945 sec/batch; 84h:43m:19s remains)
INFO - root - 2017-12-09 09:21:38.071279: step 9740, loss = 0.82, batch loss = 0.69 (8.8 examples/sec; 0.905 sec/batch; 81h:08m:07s remains)
INFO - root - 2017-12-09 09:21:46.874513: step 9750, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.841 sec/batch; 75h:23m:57s remains)
INFO - root - 2017-12-09 09:21:55.340483: step 9760, loss = 0.82, batch loss = 0.69 (10.3 examples/sec; 0.777 sec/batch; 69h:37m:42s remains)
INFO - root - 2017-12-09 09:22:03.877369: step 9770, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.857 sec/batch; 76h:51m:12s remains)
INFO - root - 2017-12-09 09:22:12.388865: step 9780, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.872 sec/batch; 78h:10m:06s remains)
INFO - root - 2017-12-09 09:22:21.105685: step 9790, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.856 sec/batch; 76h:44m:56s remains)
INFO - root - 2017-12-09 09:22:29.758954: step 9800, loss = 0.82, batch loss = 0.69 (9.7 examples/sec; 0.823 sec/batch; 73h:47m:47s remains)
2017-12-09 09:22:30.603209: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.00096765207 -0.00096658926 -0.00096394634 -0.00095618679 -0.00095006765 -0.00094471022 -0.0009433378 -0.00094412919 -0.00094810477 -0.00095355161 -0.00096155307 -0.00096945051 -0.00097575609 -0.00097748637 -0.00098171888][-0.0008307434 -0.00082804915 -0.00083151372 -0.00083493948 -0.000838469 -0.00083660684 -0.00083640136 -0.00083848444 -0.00084484508 -0.00085022941 -0.000855496 -0.00085901388 -0.00086063042 -0.00085994659 -0.00086297892][-0.000680845 -0.00065907941 -0.00064851152 -0.00064444827 -0.00064499688 -0.00064979756 -0.00065655372 -0.00066166971 -0.00066949247 -0.00067757134 -0.00068799726 -0.00069827831 -0.00070682156 -0.00071310881 -0.00071932818][-0.00053400471 -0.00051209424 -0.00050038344 -0.00048727432 -0.00047733053 -0.00047309336 -0.00047552941 -0.00048616913 -0.0005038309 -0.00052500638 -0.00054883631 -0.00057101267 -0.00059176277 -0.00060817145 -0.00062312314][-0.00044114457 -0.000414556 -0.00040116225 -0.00038783223 -0.00037674973 -0.00036722323 -0.00036410138 -0.00037248444 -0.00039599271 -0.00043302315 -0.00047584157 -0.000517953 -0.00055408047 -0.0005818081 -0.00060141337][-0.00043593353 -0.00038864237 -0.00035687513 -0.00033343432 -0.00031764212 -0.00030611345 -0.00030297885 -0.00031215849 -0.00033926818 -0.00038228463 -0.00043479586 -0.00049170246 -0.00054239464 -0.00058096263 -0.00060666847][-0.00051085814 -0.00044385553 -0.00039259496 -0.00035120628 -0.00032046612 -0.00029792532 -0.00028630265 -0.00029190432 -0.00032015418 -0.00036494731 -0.00042090414 -0.00047969463 -0.00053229555 -0.00057593815 -0.00060654193][-0.000642124 -0.00056723732 -0.00050078769 -0.00044097443 -0.00039206748 -0.00035316666 -0.00032747112 -0.00032043492 -0.00034029147 -0.00038226781 -0.00043608935 -0.00049460109 -0.00054972025 -0.00059814891 -0.00063875364][-0.00082899141 -0.00075258221 -0.0006739705 -0.00059978006 -0.00053499319 -0.00047964934 -0.00043760572 -0.00041667337 -0.00042531453 -0.00045877515 -0.00050720165 -0.00055750069 -0.00060807483 -0.00065737322 -0.00069895986][-0.00098044937 -0.00091299403 -0.00083460507 -0.00075416628 -0.000680255 -0.00061515387 -0.00055908441 -0.00051733968 -0.00049924647 -0.00050513528 -0.00053010369 -0.0005667549 -0.000611114 -0.00065703155 -0.00069668546][-0.0010666251 -0.0010192015 -0.00095708703 -0.00088706054 -0.0008176954 -0.00075166015 -0.00068764546 -0.00062890112 -0.00058310112 -0.0005557171 -0.00054576277 -0.00055137393 -0.0005749 -0.00061266363 -0.00065141951][-0.0011008917 -0.0010716284 -0.0010312025 -0.00098288758 -0.00093105924 -0.00087482453 -0.00081268436 -0.0007467973 -0.00068543694 -0.00063494657 -0.00059506326 -0.00056699989 -0.00055366021 -0.00055650552 -0.00057360571][-0.0011267855 -0.0011112072 -0.0010890923 -0.0010601876 -0.0010252335 -0.00098192634 -0.00092850148 -0.000866489 -0.00080380484 -0.00074512296 -0.00069119828 -0.00064284826 -0.00060345209 -0.0005776863 -0.00056410761][-0.0011415725 -0.0011360195 -0.001126828 -0.0011122284 -0.0010905981 -0.0010589571 -0.0010159247 -0.000963054 -0.00090531039 -0.00084567541 -0.00078606175 -0.00073091214 -0.00068384351 -0.0006456562 -0.00061703235][-0.0011455925 -0.0011438684 -0.0011410122 -0.0011353744 -0.0011247118 -0.0011063769 -0.0010780959 -0.0010399619 -0.00099313119 -0.00093790173 -0.00087693776 -0.00081568497 -0.00076091755 -0.00071597018 -0.00068187859]]...]
INFO - root - 2017-12-09 09:22:39.188517: step 9810, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.836 sec/batch; 74h:58m:16s remains)
INFO - root - 2017-12-09 09:22:47.619562: step 9820, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.860 sec/batch; 77h:03m:28s remains)
INFO - root - 2017-12-09 09:22:56.282598: step 9830, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.873 sec/batch; 78h:13m:41s remains)
INFO - root - 2017-12-09 09:23:04.989863: step 9840, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 77h:15m:11s remains)
INFO - root - 2017-12-09 09:23:13.790146: step 9850, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 77h:46m:12s remains)
INFO - root - 2017-12-09 09:23:22.441430: step 9860, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.854 sec/batch; 76h:33m:24s remains)
INFO - root - 2017-12-09 09:23:30.984583: step 9870, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 77h:26m:05s remains)
INFO - root - 2017-12-09 09:23:39.491462: step 9880, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.865 sec/batch; 77h:28m:58s remains)
INFO - root - 2017-12-09 09:23:48.065920: step 9890, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.872 sec/batch; 78h:08m:23s remains)
INFO - root - 2017-12-09 09:23:56.693820: step 9900, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.839 sec/batch; 75h:11m:19s remains)
2017-12-09 09:23:57.529284: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.017215258 0.017098766 0.016666466 0.016145743 0.015511325 0.014693137 0.013658224 0.012404042 0.010913463 0.00935296 0.0078676054 0.0067467596 0.0059655975 0.0053109387 0.0046093096][0.025729762 0.025769807 0.025410844 0.024997592 0.024423225 0.023539027 0.022235205 0.020513844 0.018420381 0.016173065 0.01406638 0.012461347 0.011399255 0.010558219 0.0095551824][0.034906477 0.035393327 0.035588607 0.035865821 0.035918348 0.035364814 0.033957228 0.031728715 0.028835375 0.025626704 0.022595307 0.020350832 0.019000007 0.018007644 0.01662828][0.042534269 0.043693975 0.044896059 0.046397217 0.04758627 0.0478419 0.046701275 0.044248186 0.040767424 0.036817171 0.033055648 0.030125914 0.028199462 0.026642811 0.024487441][0.046215266 0.048137054 0.050520964 0.053401515 0.05584833 0.056882434 0.056051321 0.053563796 0.049900733 0.045674376 0.041564062 0.038201146 0.035747133 0.033504948 0.030515941][0.045893028 0.04844822 0.051849931 0.055836029 0.059208229 0.060805902 0.060230561 0.057855695 0.054252662 0.050033398 0.045723908 0.041916385 0.038854942 0.035975754 0.032476224][0.04214894 0.045257673 0.049515992 0.054324932 0.058275782 0.060173646 0.059789903 0.057576694 0.054082088 0.049754493 0.045044221 0.040637128 0.036944881 0.033713549 0.030228375][0.03662622 0.040179625 0.045037515 0.050333291 0.054512113 0.056570828 0.05642508 0.054463193 0.051024824 0.046371005 0.041028328 0.035894491 0.031665593 0.028400889 0.025348561][0.031064784 0.034628093 0.039491817 0.044669233 0.048674218 0.050708167 0.050732188 0.048971288 0.045500644 0.04042726 0.034460776 0.028804291 0.024460543 0.021555636 0.019281607][0.026301082 0.02935328 0.033524416 0.038025014 0.041571438 0.043508023 0.043615803 0.041769467 0.038005151 0.032508463 0.026243854 0.020585189 0.016562374 0.01424265 0.012773036][0.022296753 0.024299113 0.027148655 0.030479284 0.033422023 0.035267659 0.035358973 0.033280917 0.029099816 0.023393612 0.0173773 0.012349519 0.0090713641 0.0074644051 0.0066863196][0.018693313 0.019830802 0.021432376 0.023584183 0.025819708 0.027341686 0.027123569 0.024561808 0.019993193 0.014474746 0.009341104 0.0055380464 0.0033437014 0.0024366418 0.0021224332][0.014904249 0.015670227 0.016477715 0.017658623 0.019027837 0.019814653 0.018990234 0.016155828 0.011888715 0.0073632905 0.00369062 0.0013372498 0.00018495659 -0.00019465643 -0.0002681656][0.011005337 0.011841647 0.012345101 0.01282203 0.013262467 0.013115098 0.011749507 0.00912335 0.0058796448 0.0028890884 0.00077272416 -0.0003927112 -0.00086943491 -0.00098916516 -0.00098841486][0.007101804 0.0080155293 0.0083954092 0.0083836373 0.0080951676 0.0073491624 0.0059495638 0.0040345788 0.0020560399 0.00046663021 -0.00051066355 -0.00096388324 -0.0011159708 -0.0011483254 -0.0011494894]]...]
INFO - root - 2017-12-09 09:24:06.078260: step 9910, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.852 sec/batch; 76h:20m:40s remains)
INFO - root - 2017-12-09 09:24:14.519057: step 9920, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.883 sec/batch; 79h:06m:23s remains)
INFO - root - 2017-12-09 09:24:23.212254: step 9930, loss = 0.82, batch loss = 0.69 (8.6 examples/sec; 0.928 sec/batch; 83h:06m:28s remains)
INFO - root - 2017-12-09 09:24:31.874914: step 9940, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.855 sec/batch; 76h:36m:23s remains)
INFO - root - 2017-12-09 09:24:40.614321: step 9950, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.850 sec/batch; 76h:08m:42s remains)
INFO - root - 2017-12-09 09:24:49.315877: step 9960, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.879 sec/batch; 78h:45m:56s remains)
INFO - root - 2017-12-09 09:24:57.842309: step 9970, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.841 sec/batch; 75h:19m:53s remains)
INFO - root - 2017-12-09 09:25:06.420475: step 9980, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 77h:37m:55s remains)
INFO - root - 2017-12-09 09:25:15.042154: step 9990, loss = 0.81, batch loss = 0.68 (9.6 examples/sec; 0.830 sec/batch; 74h:22m:52s remains)
INFO - root - 2017-12-09 09:25:23.643858: step 10000, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 77h:47m:04s remains)
2017-12-09 09:25:24.539379: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.013028446 0.019514846 0.026394969 0.032223273 0.035851561 0.037003834 0.036120042 0.033957373 0.031038335 0.027785419 0.024381446 0.020658977 0.016367843 0.011671317 0.0072181481][0.022231992 0.032271106 0.043077111 0.052626222 0.059091441 0.061894033 0.061514553 0.058893483 0.054545015 0.049051248 0.0429736 0.036323678 0.028908866 0.020965997 0.013461985][0.034301 0.048813336 0.064539135 0.078784555 0.088960521 0.094176486 0.094802707 0.091834918 0.08573243 0.077350013 0.067733161 0.057167146 0.045677707 0.033565864 0.022145191][0.047637943 0.067154273 0.088405907 0.10813024 0.12291051 0.13139509 0.13376227 0.13071398 0.12249453 0.11026159 0.095898367 0.0803852 0.064171851 0.047587771 0.032106116][0.059104249 0.083061755 0.10923908 0.13398035 0.15328635 0.16545029 0.17026582 0.16791777 0.1582128 0.14242345 0.12330053 0.10270652 0.0817877 0.060947277 0.041751422][0.065653615 0.0924489 0.12169658 0.14964475 0.17218465 0.18743375 0.19478545 0.19374257 0.18365458 0.16574629 0.14333974 0.11912742 0.094961882 0.071272224 0.049570408][0.0665078 0.093920432 0.12378766 0.15258457 0.17639743 0.19345541 0.20283948 0.20339784 0.19409458 0.17587411 0.15236983 0.12668628 0.10120772 0.07643757 0.053784564][0.06264586 0.088645995 0.11693543 0.14441645 0.16757014 0.18495964 0.19554566 0.19767295 0.19002302 0.17318466 0.15072541 0.12568833 0.10073118 0.076376937 0.054066505][0.055501144 0.078624457 0.10375852 0.12834293 0.14932355 0.16562964 0.1763411 0.17958817 0.17393638 0.15956092 0.13974509 0.11708052 0.094122812 0.07146503 0.05060374][0.046748511 0.066228643 0.087519847 0.1084576 0.12641348 0.14060047 0.15031536 0.15386805 0.14988983 0.13832924 0.12190569 0.10260779 0.08261437 0.062548332 0.043996163][0.037421063 0.053097118 0.070373356 0.087449655 0.10206134 0.11358164 0.12150924 0.12448057 0.12143248 0.11232667 0.099349894 0.083865069 0.067461818 0.050764486 0.035273749][0.028362855 0.040546615 0.054096632 0.067467965 0.078782953 0.087496586 0.093266338 0.095127828 0.0923731 0.085162744 0.075204268 0.063376188 0.05068149 0.037679736 0.025692703][0.01975432 0.02885451 0.039005656 0.048926264 0.057141937 0.063146 0.066709928 0.067282811 0.064556338 0.058890596 0.051546782 0.043090608 0.034056604 0.024871111 0.016525405][0.012078354 0.018377146 0.025410982 0.032161824 0.037574593 0.041232482 0.043010492 0.042681489 0.040222991 0.036071531 0.031075427 0.025551779 0.019773863 0.014044829 0.0089720255][0.0059834672 0.0097396038 0.013960468 0.017957432 0.021025511 0.022878034 0.023471117 0.022811394 0.021026285 0.018465944 0.015600104 0.012562801 0.0094564147 0.0064386493 0.0038136137]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-momentum-lr0.0001-clip-zeroinit-conv1-3init-from-scratch/model.ckpt-10000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-momentum-lr0.0001-clip-zeroinit-conv1-3init-from-scratch/model.ckpt-10000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-09 09:25:33.729163: step 10010, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.875 sec/batch; 78h:21m:54s remains)
INFO - root - 2017-12-09 09:25:42.348844: step 10020, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.876 sec/batch; 78h:26m:04s remains)
INFO - root - 2017-12-09 09:25:51.009768: step 10030, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 77h:45m:57s remains)
INFO - root - 2017-12-09 09:25:59.569451: step 10040, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.865 sec/batch; 77h:27m:18s remains)
INFO - root - 2017-12-09 09:26:08.101105: step 10050, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.863 sec/batch; 77h:19m:34s remains)
INFO - root - 2017-12-09 09:26:16.610160: step 10060, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.843 sec/batch; 75h:28m:35s remains)
INFO - root - 2017-12-09 09:26:25.039519: step 10070, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.852 sec/batch; 76h:17m:49s remains)
INFO - root - 2017-12-09 09:26:33.414085: step 10080, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.876 sec/batch; 78h:29m:28s remains)
INFO - root - 2017-12-09 09:26:42.199148: step 10090, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 77h:51m:14s remains)
INFO - root - 2017-12-09 09:26:50.890405: step 10100, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.866 sec/batch; 77h:35m:19s remains)
2017-12-09 09:26:51.723712: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0012396354 -0.0012383326 -0.0012381759 -0.0012381405 -0.0012381381 -0.0012382603 -0.0012383235 -0.0012382489 -0.0012380402 -0.0012377596 -0.0012375192 -0.0012373643 -0.0012372998 -0.0012373527 -0.001237506][-0.0012391844 -0.0012378823 -0.0012377471 -0.0012377739 -0.0012378908 -0.0012380768 -0.0012381577 -0.0012380349 -0.0012377765 -0.0012374317 -0.0012370718 -0.001236789 -0.0012366249 -0.0012366107 -0.0012367605][-0.0012392418 -0.0012381944 -0.0012381845 -0.0012383454 -0.0012386108 -0.0012389136 -0.0012390413 -0.0012389134 -0.0012386101 -0.0012381419 -0.0012375508 -0.0012370425 -0.0012367272 -0.0012366296 -0.0012367584][-0.0012393518 -0.0012385898 -0.0012387324 -0.0012390908 -0.0012395633 -0.0012400242 -0.0012402203 -0.0012400793 -0.001239689 -0.0012389892 -0.001238083 -0.0012373496 -0.0012369357 -0.0012368003 -0.0012368986][-0.0012396018 -0.0012389997 -0.0012392286 -0.0012397341 -0.0012403695 -0.0012409118 -0.0012411515 -0.0012410203 -0.0012406041 -0.0012397658 -0.0012387006 -0.001237845 -0.0012373192 -0.0012370822 -0.00123706][-0.0012399132 -0.0012393997 -0.0012396433 -0.0012401257 -0.0012407152 -0.0012411182 -0.0012412644 -0.0012411948 -0.0012409268 -0.0012402304 -0.0012392553 -0.0012384064 -0.0012377559 -0.0012373533 -0.0012371653][-0.0012401809 -0.0012397319 -0.001239959 -0.0012404014 -0.0012409027 -0.0012412161 -0.0012413947 -0.0012415433 -0.0012415095 -0.0012409547 -0.0012400057 -0.0012390874 -0.0012382516 -0.0012376729 -0.0012373747][-0.0012401763 -0.0012397413 -0.0012399743 -0.0012404782 -0.0012410671 -0.0012415658 -0.0012420127 -0.0012422833 -0.0012421631 -0.0012414759 -0.0012403732 -0.0012393148 -0.0012383771 -0.0012377512 -0.0012374533][-0.0012399538 -0.0012395325 -0.00123982 -0.0012403917 -0.0012410772 -0.0012417191 -0.0012422897 -0.0012424884 -0.0012421657 -0.001241319 -0.0012401687 -0.0012391 -0.0012381965 -0.0012376279 -0.0012373701][-0.001239783 -0.001239364 -0.0012395948 -0.001240054 -0.0012406071 -0.0012411581 -0.0012416271 -0.0012417469 -0.0012413971 -0.0012406346 -0.0012396548 -0.0012387309 -0.0012379427 -0.0012374155 -0.0012371698][-0.0012398165 -0.0012392914 -0.001239395 -0.0012396749 -0.0012400211 -0.0012404065 -0.0012407788 -0.0012409256 -0.00124071 -0.0012401761 -0.0012394122 -0.0012386244 -0.0012379107 -0.001237385 -0.0012371319][-0.0012398856 -0.0012393083 -0.0012393144 -0.001239452 -0.0012395992 -0.0012398231 -0.0012400884 -0.0012402164 -0.0012401134 -0.0012397799 -0.0012392082 -0.0012385602 -0.00123794 -0.0012374973 -0.0012372627][-0.0012400192 -0.0012394931 -0.001239437 -0.0012394192 -0.0012393253 -0.0012393005 -0.0012393583 -0.001239372 -0.0012392979 -0.0012391101 -0.001238777 -0.001238375 -0.0012380056 -0.0012377348 -0.0012375847][-0.001240278 -0.0012398447 -0.001239738 -0.0012395817 -0.0012392702 -0.0012390312 -0.0012388927 -0.001238779 -0.0012387175 -0.0012386484 -0.0012384834 -0.0012382846 -0.0012381452 -0.0012380872 -0.0012380728][-0.001240532 -0.0012401239 -0.0012399709 -0.0012397735 -0.0012393956 -0.0012390901 -0.0012388943 -0.0012387631 -0.001238722 -0.0012387208 -0.0012386536 -0.0012385696 -0.0012385452 -0.0012385892 -0.001238643]]...]
INFO - root - 2017-12-09 09:27:00.477035: step 10110, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.858 sec/batch; 76h:51m:26s remains)
INFO - root - 2017-12-09 09:27:09.007318: step 10120, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.876 sec/batch; 78h:25m:37s remains)
INFO - root - 2017-12-09 09:27:17.904175: step 10130, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.901 sec/batch; 80h:38m:29s remains)
INFO - root - 2017-12-09 09:27:26.596274: step 10140, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.850 sec/batch; 76h:06m:30s remains)
INFO - root - 2017-12-09 09:27:35.234660: step 10150, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.891 sec/batch; 79h:45m:10s remains)
INFO - root - 2017-12-09 09:27:44.082025: step 10160, loss = 0.81, batch loss = 0.68 (9.0 examples/sec; 0.889 sec/batch; 79h:36m:32s remains)
INFO - root - 2017-12-09 09:27:52.678434: step 10170, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.884 sec/batch; 79h:08m:13s remains)
INFO - root - 2017-12-09 09:28:01.262210: step 10180, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.892 sec/batch; 79h:50m:35s remains)
INFO - root - 2017-12-09 09:28:10.046324: step 10190, loss = 0.82, batch loss = 0.69 (8.8 examples/sec; 0.911 sec/batch; 81h:34m:00s remains)
INFO - root - 2017-12-09 09:28:18.841377: step 10200, loss = 0.81, batch loss = 0.68 (9.0 examples/sec; 0.892 sec/batch; 79h:53m:58s remains)
2017-12-09 09:28:19.681241: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0012324324 -0.0012344782 -0.0012379616 -0.0012412528 -0.0012433033 -0.0012433811 -0.0012421036 -0.0012405278 -0.0012393091 -0.0012391859 -0.0012398012 -0.0012404604 -0.0012402897 -0.0012389885 -0.0012368485][-0.0012324592 -0.001235498 -0.0012400537 -0.0012447913 -0.0012483315 -0.0012494404 -0.0012485308 -0.0012465346 -0.0012445671 -0.0012434974 -0.0012434894 -0.0012442276 -0.0012443414 -0.0012429588 -0.0012402795][-0.0012329209 -0.0012369858 -0.0012426608 -0.001248879 -0.0012538886 -0.0012560679 -0.0012555822 -0.0012532674 -0.0012502889 -0.0012477745 -0.0012468189 -0.0012477421 -0.0012484369 -0.0012472626 -0.0012443801][-0.0012332281 -0.0012378643 -0.0012444429 -0.0012516773 -0.0012576391 -0.0012604924 -0.0012603264 -0.0012573021 -0.0012525547 -0.0012481429 -0.0012465053 -0.0012481245 -0.0012499465 -0.001249668 -0.0012471562][-0.0012332473 -0.0012379788 -0.0012447165 -0.001252008 -0.0012579677 -0.0012610331 -0.0012605265 -0.0012560956 -0.0012489089 -0.0012422203 -0.0012401356 -0.0012436429 -0.0012480523 -0.0012494685 -0.0012478126][-0.0012333802 -0.0012380512 -0.0012444663 -0.0012508339 -0.0012558339 -0.0012582529 -0.0012565779 -0.0012501219 -0.0012404841 -0.0012319905 -0.0012301452 -0.001235754 -0.0012430273 -0.001246359 -0.0012456342][-0.0012339269 -0.0012383864 -0.0012439152 -0.0012485733 -0.001251874 -0.0012528085 -0.0012494099 -0.0012414935 -0.0012308414 -0.0012224721 -0.0012216906 -0.0012289002 -0.0012378674 -0.001242387 -0.0012421142][-0.0012346564 -0.001238788 -0.0012433821 -0.0012464257 -0.0012478131 -0.0012469115 -0.0012417895 -0.0012334477 -0.0012237474 -0.0012173206 -0.001218234 -0.0012260792 -0.0012349386 -0.0012390612 -0.0012384092][-0.0012354259 -0.0012394225 -0.0012434205 -0.0012455016 -0.0012455217 -0.0012429965 -0.0012370392 -0.0012293017 -0.0012219053 -0.0012183905 -0.0012206227 -0.001227596 -0.0012343739 -0.0012367941 -0.0012352691][-0.0012356495 -0.0012399392 -0.0012438667 -0.0012456859 -0.0012450756 -0.0012415659 -0.0012354843 -0.0012294146 -0.0012250462 -0.001224115 -0.0012264636 -0.0012310661 -0.0012347494 -0.0012351689 -0.0012327224][-0.0012350391 -0.001239358 -0.0012435337 -0.0012456613 -0.0012453096 -0.0012420717 -0.0012367801 -0.0012323917 -0.0012304098 -0.001230814 -0.0012322321 -0.0012340354 -0.0012348212 -0.0012334967 -0.0012305592][-0.001233682 -0.0012374062 -0.0012414999 -0.0012439591 -0.0012443441 -0.0012419082 -0.0012380859 -0.0012350947 -0.0012341053 -0.0012344631 -0.0012348213 -0.0012346706 -0.0012336503 -0.0012315518 -0.0012286684][-0.0012314477 -0.0012343555 -0.0012380475 -0.0012408923 -0.0012420663 -0.0012410345 -0.0012388687 -0.0012370243 -0.0012363261 -0.0012358932 -0.0012350668 -0.0012337641 -0.0012319001 -0.001229625 -0.0012270905][-0.0012293272 -0.0012312951 -0.0012343086 -0.0012372163 -0.0012390921 -0.0012392999 -0.0012386646 -0.0012377794 -0.0012368683 -0.0012358497 -0.0012344614 -0.0012325934 -0.0012302706 -0.0012279954 -0.0012261022][-0.0012277256 -0.0012287154 -0.0012308365 -0.0012332333 -0.0012352865 -0.0012364156 -0.0012368624 -0.0012368266 -0.0012363021 -0.0012350138 -0.001233191 -0.0012311678 -0.0012289606 -0.001227053 -0.0012256354]]...]
INFO - root - 2017-12-09 09:28:28.447137: step 10210, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.891 sec/batch; 79h:46m:31s remains)
INFO - root - 2017-12-09 09:28:37.099754: step 10220, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.899 sec/batch; 80h:27m:38s remains)
INFO - root - 2017-12-09 09:28:45.909210: step 10230, loss = 0.81, batch loss = 0.68 (8.9 examples/sec; 0.895 sec/batch; 80h:06m:39s remains)
INFO - root - 2017-12-09 09:28:54.800712: step 10240, loss = 0.82, batch loss = 0.69 (8.3 examples/sec; 0.962 sec/batch; 86h:08m:10s remains)
INFO - root - 2017-12-09 09:29:03.550463: step 10250, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 77h:18m:25s remains)
INFO - root - 2017-12-09 09:29:12.237104: step 10260, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.860 sec/batch; 76h:58m:41s remains)
INFO - root - 2017-12-09 09:29:21.001636: step 10270, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.887 sec/batch; 79h:23m:12s remains)
INFO - root - 2017-12-09 09:29:29.663816: step 10280, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.884 sec/batch; 79h:06m:16s remains)
INFO - root - 2017-12-09 09:29:38.457212: step 10290, loss = 0.82, batch loss = 0.69 (8.8 examples/sec; 0.912 sec/batch; 81h:37m:16s remains)
INFO - root - 2017-12-09 09:29:47.147813: step 10300, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.877 sec/batch; 78h:28m:41s remains)
2017-12-09 09:29:47.995944: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.30012524 0.29745018 0.29174173 0.28549007 0.27980986 0.27448833 0.26719591 0.25848648 0.24784043 0.23703648 0.22566079 0.21481155 0.20603974 0.19869746 0.19412784][0.30477077 0.3053242 0.30373821 0.30124256 0.2989893 0.29614854 0.29012144 0.28129962 0.2694326 0.25683904 0.24303746 0.2290656 0.21683244 0.2067765 0.20001091][0.30512217 0.31048188 0.31457952 0.31740659 0.32034108 0.32079241 0.31671125 0.30858281 0.29587489 0.28095379 0.26355329 0.24540447 0.22881833 0.21473219 0.20464137][0.30463347 0.31565073 0.32619071 0.3351416 0.34346741 0.34795648 0.346208 0.33813512 0.32461604 0.30762577 0.28676853 0.26443961 0.24358726 0.22518238 0.21120204][0.30723268 0.32248694 0.33774841 0.35165197 0.364814 0.37284642 0.37294212 0.36572236 0.35136664 0.33253953 0.30850446 0.28245825 0.25811717 0.23582384 0.21834205][0.31082428 0.32950372 0.34797981 0.36557218 0.38204184 0.39260003 0.39503169 0.38846508 0.37406462 0.35359544 0.32725754 0.29830498 0.27005336 0.24481881 0.22446094][0.31529009 0.33444944 0.35353065 0.37288788 0.39133108 0.40346593 0.40745789 0.40244073 0.38891375 0.36796984 0.34040764 0.30990267 0.27962577 0.25212237 0.22934774][0.32013148 0.33840719 0.355897 0.37457305 0.39250731 0.40423888 0.40844014 0.40436375 0.3918089 0.37117821 0.34352946 0.31307176 0.28246692 0.25424507 0.23051317][0.32252687 0.33919433 0.35405648 0.37010881 0.38582981 0.39633518 0.40007597 0.39598361 0.3840439 0.3644968 0.33806422 0.30913988 0.27985436 0.25274679 0.2294835][0.32172707 0.33482996 0.34530461 0.357464 0.36986694 0.37803948 0.38101596 0.37753755 0.36672947 0.3486228 0.32426211 0.29795986 0.27134061 0.24693102 0.22566622][0.31503275 0.32420677 0.33012894 0.33826494 0.34716976 0.35307232 0.3556335 0.35259062 0.34362659 0.32782042 0.30655456 0.28328305 0.25933346 0.23805608 0.2194194][0.30036247 0.30592671 0.30828544 0.31291145 0.31897876 0.32351363 0.32585454 0.32421204 0.31724176 0.30432636 0.28662434 0.26688859 0.24677277 0.22855525 0.21277909][0.281194 0.28499424 0.28486192 0.28656316 0.28995928 0.29266638 0.2939744 0.29247564 0.28730461 0.27761975 0.26407138 0.2484944 0.23289961 0.21868652 0.20618205][0.26075292 0.26310706 0.26150233 0.26141021 0.26304638 0.26444542 0.26505375 0.26359212 0.25960532 0.25238717 0.24253456 0.23128031 0.22017829 0.2100798 0.20121974][0.24278642 0.2437263 0.24057209 0.23896742 0.23898064 0.23939799 0.23968726 0.23894212 0.23661408 0.23204899 0.22577821 0.218295 0.21086678 0.20400393 0.19798125]]...]
INFO - root - 2017-12-09 09:29:56.555133: step 10310, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.877 sec/batch; 78h:29m:33s remains)
INFO - root - 2017-12-09 09:30:05.113339: step 10320, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.866 sec/batch; 77h:31m:33s remains)
INFO - root - 2017-12-09 09:30:13.858558: step 10330, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.875 sec/batch; 78h:18m:45s remains)
INFO - root - 2017-12-09 09:30:22.504649: step 10340, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.877 sec/batch; 78h:29m:14s remains)
INFO - root - 2017-12-09 09:30:31.204651: step 10350, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 77h:16m:48s remains)
INFO - root - 2017-12-09 09:30:39.777040: step 10360, loss = 0.83, batch loss = 0.70 (8.9 examples/sec; 0.896 sec/batch; 80h:10m:51s remains)
INFO - root - 2017-12-09 09:30:48.265798: step 10370, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.855 sec/batch; 76h:27m:57s remains)
INFO - root - 2017-12-09 09:30:56.881822: step 10380, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.860 sec/batch; 76h:56m:11s remains)
INFO - root - 2017-12-09 09:31:05.503262: step 10390, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.851 sec/batch; 76h:08m:51s remains)
INFO - root - 2017-12-09 09:31:14.290139: step 10400, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.870 sec/batch; 77h:48m:14s remains)
2017-12-09 09:31:15.204547: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.001028997 -0.0011185595 -0.0011572974 -0.001217495 -0.0012194179 -0.0010997301 -0.00082555704 -0.00058834848 -0.00051726407 -0.00059724425 -0.00065221055 -0.00063529838 -0.00055806438 -0.00052382943 -0.00057785172][-0.00091913971 -0.00097960792 -0.00099448 -0.0011208595 -0.0010862263 -0.00077875645 -0.00016366981 0.00030147028 0.00035894045 5.7739322e-05 -0.00027322618 -0.00043796509 -0.00043237366 -0.00040092214 -0.00044854492][-0.00085163495 -0.00087125751 -0.00090114272 -0.000997197 -0.00079823181 -0.0001655838 0.0009709096 0.0018807487 0.0019764658 0.0013807437 0.00063792465 0.00016484386 -2.1298649e-05 -0.00011580391 -0.00024768512][-0.00083492574 -0.00081032654 -0.00080929592 -0.00082944927 -0.00042839593 0.0007282265 0.0025587073 0.0040280623 0.0042666821 0.0034970318 0.0023600787 0.0015200707 0.0010719337 0.00079616439 0.00047655159][-0.00078699953 -0.00073134882 -0.00066749332 -0.00059276161 7.9752645e-05 0.0017703713 0.00425487 0.006355498 0.0069282828 0.0061967485 0.0048940694 0.0039024602 0.0032776638 0.0028369587 0.0022580514][-0.00073663134 -0.00065209693 -0.00053948181 -0.00035714707 0.00059004943 0.0027390036 0.0057440875 0.0083353668 0.0093241939 0.0089718569 0.00790772 0.0070502367 0.0064870268 0.0060287453 0.0052875476][-0.00066067814 -0.00055125449 -0.00039859075 -0.00017238257 0.00082759443 0.0031248245 0.006391922 0.00936441 0.010821979 0.011101326 0.010655324 0.010366187 0.01016539 0.0098811109 0.0091500646][-0.00063362025 -0.00048567198 -0.00030519674 -6.751914e-05 0.00083511695 0.0028862956 0.005987606 0.0090528242 0.010930521 0.011972357 0.012404023 0.012992119 0.013395406 0.013516987 0.012949034][-0.00071041344 -0.00053072494 -0.00034720771 -0.00013196934 0.000583948 0.0021761726 0.0047698682 0.0075326487 0.0096773384 0.011443391 0.012772679 0.014236381 0.015277792 0.015911166 0.015598837][-0.00086241669 -0.00069789094 -0.00052774354 -0.00034448237 0.00011991535 0.0011912973 0.0031041403 0.0053655766 0.0075035775 0.0096830269 0.011717551 0.013879696 0.015397606 0.016399058 0.016332928][-0.0010275524 -0.0009059225 -0.00077525113 -0.00064081379 -0.00037851639 0.00025239575 0.0014591215 0.0030893174 0.0049855048 0.0072337445 0.00954783 0.011954921 0.013696926 0.014889749 0.014998028][-0.0011544171 -0.0010835753 -0.00099546392 -0.00091179821 -0.00079000182 -0.00046686118 0.0001816717 0.0011999796 0.0026305087 0.0045630075 0.0067343097 0.0089776022 0.010651296 0.011797056 0.01198919][-0.001225753 -0.0011949869 -0.0011473441 -0.0010985507 -0.0010489419 -0.00090955803 -0.0006243083 -8.1348233e-05 0.00082698 0.00219549 0.0038563472 0.0056172088 0.0070247492 0.007998365 0.0082161613][-0.0012519873 -0.0012420198 -0.0012207398 -0.0011973281 -0.0011821886 -0.0011325262 -0.0010384964 -0.00080055476 -0.0003258303 0.00047101406 0.0015098581 0.0026563287 0.0036571857 0.004378804 0.0045831529][-0.0012562354 -0.0012532837 -0.0012453586 -0.0012342053 -0.0012284307 -0.0012169163 -0.0012014189 -0.0011228856 -0.000929776 -0.00056356069 -4.2960164e-05 0.00057297887 0.0011713356 0.0016313994 0.0017888655]]...]
INFO - root - 2017-12-09 09:31:23.922220: step 10410, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.877 sec/batch; 78h:28m:41s remains)
INFO - root - 2017-12-09 09:31:32.362963: step 10420, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.882 sec/batch; 78h:52m:09s remains)
INFO - root - 2017-12-09 09:31:41.012351: step 10430, loss = 0.82, batch loss = 0.69 (9.7 examples/sec; 0.828 sec/batch; 74h:03m:26s remains)
INFO - root - 2017-12-09 09:31:49.693585: step 10440, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.840 sec/batch; 75h:09m:45s remains)
INFO - root - 2017-12-09 09:31:58.444456: step 10450, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.874 sec/batch; 78h:09m:18s remains)
INFO - root - 2017-12-09 09:32:07.316461: step 10460, loss = 0.82, batch loss = 0.70 (9.7 examples/sec; 0.826 sec/batch; 73h:54m:21s remains)
INFO - root - 2017-12-09 09:32:15.875943: step 10470, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 77h:40m:46s remains)
INFO - root - 2017-12-09 09:32:24.311301: step 10480, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.894 sec/batch; 80h:00m:39s remains)
INFO - root - 2017-12-09 09:32:32.944266: step 10490, loss = 0.81, batch loss = 0.68 (9.3 examples/sec; 0.857 sec/batch; 76h:36m:50s remains)
INFO - root - 2017-12-09 09:32:41.627767: step 10500, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.849 sec/batch; 75h:54m:06s remains)
2017-12-09 09:32:42.482563: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.14764643 0.15107773 0.15196915 0.15101478 0.14724511 0.14061469 0.1338232 0.12711512 0.12246715 0.12076654 0.12242442 0.12800071 0.13574074 0.14249349 0.1471108][0.1542958 0.158422 0.16034491 0.16048233 0.15776318 0.15246046 0.1465669 0.14038618 0.13565128 0.13308297 0.13408317 0.13804697 0.14346845 0.14740524 0.14887899][0.15744999 0.16264676 0.16588549 0.16763037 0.16699117 0.16357443 0.15893322 0.15314694 0.14809889 0.14456107 0.14366476 0.14535971 0.14807409 0.14895727 0.14735064][0.15695688 0.16315033 0.16766413 0.17098556 0.17242382 0.17176507 0.16926533 0.16472925 0.15958832 0.15490071 0.15250202 0.15189904 0.15193383 0.15005539 0.14584115][0.15691312 0.16340262 0.16864137 0.17306915 0.1760747 0.17736298 0.176737 0.173594 0.16875403 0.16356099 0.1598876 0.15756841 0.15547614 0.15180969 0.14645033][0.1579387 0.1651316 0.17092456 0.17627417 0.18045215 0.18326174 0.18377428 0.18122123 0.17674157 0.17161642 0.16670017 0.162195 0.15786739 0.15280306 0.14676218][0.16036347 0.16807579 0.17459835 0.18084058 0.1860327 0.18956418 0.1904427 0.1883955 0.18460681 0.17979549 0.17466629 0.16914058 0.16383015 0.15786499 0.15161401][0.16648209 0.17535339 0.1822872 0.18904145 0.19476184 0.19821087 0.19847672 0.19590314 0.19215052 0.18792547 0.18302128 0.17726375 0.17108287 0.16429041 0.15776443][0.17433554 0.18368293 0.19059643 0.19751059 0.20339908 0.20702475 0.20725229 0.20443952 0.200921 0.19734983 0.19305466 0.18727444 0.18090655 0.17398709 0.16744846][0.18659088 0.19574197 0.2017214 0.20808893 0.21368851 0.2171199 0.21742226 0.21490358 0.21174997 0.20788728 0.20332031 0.19733915 0.19067234 0.18343718 0.17678857][0.2000283 0.20925061 0.21399117 0.21886392 0.22314833 0.22586213 0.22600622 0.22411703 0.22147389 0.21779205 0.21313488 0.20684147 0.19958836 0.19152366 0.18456447][0.21230587 0.22237092 0.22587933 0.22904144 0.23140071 0.23252222 0.23191877 0.22995989 0.22821124 0.22527611 0.22131869 0.21512336 0.20697598 0.19777547 0.18968865][0.21939614 0.23006597 0.23320565 0.23579584 0.23745856 0.23780756 0.23644894 0.23393674 0.23158593 0.2285119 0.22401944 0.2173809 0.20823817 0.19820972 0.18884966][0.22257349 0.23233432 0.23461072 0.23701966 0.23856762 0.23901649 0.23790638 0.23560148 0.2328313 0.22878367 0.22322635 0.21533404 0.20495039 0.1939736 0.18373698][0.22363541 0.23220715 0.23315087 0.23474333 0.23583198 0.23608257 0.23531601 0.23359619 0.23127499 0.22686706 0.22036946 0.21126419 0.1997052 0.1877024 0.17626925]]...]
INFO - root - 2017-12-09 09:32:51.039285: step 10510, loss = 0.81, batch loss = 0.69 (9.5 examples/sec; 0.839 sec/batch; 75h:03m:52s remains)
INFO - root - 2017-12-09 09:32:59.626660: step 10520, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.885 sec/batch; 79h:07m:25s remains)
INFO - root - 2017-12-09 09:33:08.267232: step 10530, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.863 sec/batch; 77h:13m:31s remains)
INFO - root - 2017-12-09 09:33:16.881514: step 10540, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.831 sec/batch; 74h:20m:11s remains)
INFO - root - 2017-12-09 09:33:25.545056: step 10550, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.836 sec/batch; 74h:45m:07s remains)
INFO - root - 2017-12-09 09:33:34.281774: step 10560, loss = 0.81, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 77h:42m:45s remains)
INFO - root - 2017-12-09 09:33:42.807626: step 10570, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.878 sec/batch; 78h:30m:20s remains)
INFO - root - 2017-12-09 09:33:51.245829: step 10580, loss = 0.82, batch loss = 0.69 (9.7 examples/sec; 0.826 sec/batch; 73h:50m:06s remains)
INFO - root - 2017-12-09 09:33:59.911543: step 10590, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.852 sec/batch; 76h:09m:14s remains)
INFO - root - 2017-12-09 09:34:08.615071: step 10600, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.896 sec/batch; 80h:06m:57s remains)
2017-12-09 09:34:09.544274: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0009193212 -0.00048148067 0.00018498511 0.00093400956 0.0015210878 0.0017672047 0.0016589785 0.0012915275 0.00079220661 0.00024209777 -0.00028438645 -0.00072263076 -0.0010244676 -0.001185076 -0.0012454948][-0.0010103985 -0.00063836685 -2.6655849e-05 0.00073386624 0.0014401592 0.0019002747 0.0020419862 0.0018830114 0.0014776156 0.00087693578 0.00018411828 -0.00044976076 -0.00090319768 -0.0011448364 -0.0012353982][-0.0011072417 -0.00084270164 -0.00035712519 0.00033261534 0.0011017068 0.0017744062 0.0022032876 0.0022929269 0.0020029894 0.0013698085 0.0005421784 -0.00024550059 -0.00081406883 -0.0011148162 -0.0012274518][-0.0011756525 -0.0009957368 -0.000623383 -2.3497385e-05 0.00074577343 0.0015345808 0.0021453588 0.0023944806 0.0021704319 0.0015128186 0.00061748282 -0.00022406923 -0.000815845 -0.0011177004 -0.0012279699][-0.001214848 -0.0010970749 -0.00081615429 -0.00030036189 0.00044384971 0.0012911981 0.0020136051 0.00236812 0.0021984302 0.0015435779 0.00062735355 -0.00023113738 -0.00082708325 -0.001125442 -0.0012314899][-0.0012272975 -0.0011363927 -0.00089786935 -0.00041442877 0.00034738157 0.0012811014 0.0021309187 0.0025933208 0.0024647354 0.0017811049 0.00079585717 -0.00013601664 -0.00078528165 -0.001113154 -0.0012299739][-0.0012242494 -0.0011245747 -0.00085840758 -0.00030796928 0.00057538366 0.0016816276 0.0027134027 0.0033061132 0.0032066517 0.002442529 0.0012986191 0.0001809845 -0.00062499364 -0.0010543243 -0.0012156479][-0.0012167785 -0.0010951374 -0.00076894404 -0.00010419718 0.00094243221 0.0022365465 0.0034370404 0.0041335486 0.0040332908 0.0031585665 0.0018292622 0.00050865917 -0.00046089321 -0.00099180918 -0.001196673][-0.0012033213 -0.0010667265 -0.0007022862 3.354589e-05 0.0011736819 0.0025599827 0.0038217297 0.0045356415 0.0043984288 0.0034469226 0.0020226641 0.00061653822 -0.00041031238 -0.00097152789 -0.0011864103][-0.001190622 -0.0010588019 -0.00071731274 -3.0011171e-05 0.0010342371 0.002321071 0.0034703766 0.0040876819 0.0039033317 0.0029831245 0.0016598046 0.00038656394 -0.0005231133 -0.0010105104 -0.0011926016][-0.0011967304 -0.0010951573 -0.00084204553 -0.00032785186 0.00048373989 0.0014761881 0.0023531225 0.0027907449 0.0025859345 0.0018285691 0.00080713315 -0.00012798572 -0.00076667912 -0.0010928501 -0.0012071159][-0.0012239192 -0.0011621274 -0.0010117765 -0.00070311513 -0.00020437292 0.000415612 0.00095781113 0.0012032519 0.001028001 0.00051897694 -0.00011631823 -0.00065947016 -0.0010077095 -0.001174117 -0.0012266008][-0.0012460343 -0.0012232511 -0.0011633164 -0.0010316508 -0.0008071952 -0.00051913376 -0.00026500609 -0.00015641982 -0.00025088096 -0.00049707253 -0.00078714651 -0.0010206914 -0.0011606867 -0.0012227912 -0.0012389635][-0.0012529535 -0.0012488184 -0.0012359715 -0.0012024179 -0.0011381193 -0.0010506868 -0.00097249565 -0.00094222149 -0.00097574794 -0.0010524663 -0.0011355104 -0.0011966544 -0.0012302629 -0.0012441298 -0.0012461708][-0.0012529059 -0.0012517078 -0.001250602 -0.0012486203 -0.0012444262 -0.0012390341 -0.0012345709 -0.0012336699 -0.0012365108 -0.0012408035 -0.0012445805 -0.001246797 -0.0012482338 -0.0012495165 -0.0012497695]]...]
INFO - root - 2017-12-09 09:34:18.246061: step 10610, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.882 sec/batch; 78h:50m:00s remains)
INFO - root - 2017-12-09 09:34:26.748745: step 10620, loss = 0.82, batch loss = 0.69 (9.7 examples/sec; 0.824 sec/batch; 73h:39m:39s remains)
INFO - root - 2017-12-09 09:34:35.482302: step 10630, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.856 sec/batch; 76h:33m:56s remains)
INFO - root - 2017-12-09 09:34:44.247451: step 10640, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.866 sec/batch; 77h:24m:55s remains)
INFO - root - 2017-12-09 09:34:52.985179: step 10650, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.838 sec/batch; 74h:54m:41s remains)
INFO - root - 2017-12-09 09:35:01.722791: step 10660, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.872 sec/batch; 77h:57m:39s remains)
INFO - root - 2017-12-09 09:35:10.330563: step 10670, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.880 sec/batch; 78h:40m:57s remains)
INFO - root - 2017-12-09 09:35:18.892513: step 10680, loss = 0.82, batch loss = 0.69 (9.7 examples/sec; 0.827 sec/batch; 73h:54m:59s remains)
INFO - root - 2017-12-09 09:35:27.551627: step 10690, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.847 sec/batch; 75h:45m:00s remains)
INFO - root - 2017-12-09 09:35:36.185569: step 10700, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.850 sec/batch; 75h:56m:19s remains)
2017-12-09 09:35:37.113915: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0012225469 -0.0012195718 -0.0012183582 -0.0012177338 -0.0012173704 -0.0012170285 -0.0012166708 -0.0012163618 -0.0012162105 -0.001216246 -0.0012164633 -0.0012166851 -0.0012168101 -0.0012168124 -0.0012165913][-0.0012213832 -0.001218637 -0.0012176265 -0.0012171008 -0.0012167199 -0.0012163402 -0.0012159646 -0.001215629 -0.0012154561 -0.0012155126 -0.0012156976 -0.0012158693 -0.0012159256 -0.0012158233 -0.0012155024][-0.0012209557 -0.0012184934 -0.0012176462 -0.0012172237 -0.0012168668 -0.0012164897 -0.0012160799 -0.0012157296 -0.0012155185 -0.0012155181 -0.0012156448 -0.0012157421 -0.0012157509 -0.0012156157 -0.0012152812][-0.0012202427 -0.0012180209 -0.0012173333 -0.0012170186 -0.0012167458 -0.0012164324 -0.0012160729 -0.0012157348 -0.0012154927 -0.0012154197 -0.0012154636 -0.0012155044 -0.0012154897 -0.0012153578 -0.001215068][-0.001219363 -0.0012172229 -0.0012166637 -0.0012164601 -0.0012163005 -0.0012161123 -0.0012158551 -0.0012155685 -0.0012153219 -0.001215194 -0.0012151659 -0.0012151502 -0.0012151058 -0.0012149775 -0.001214735][-0.0012184017 -0.0012163053 -0.0012158478 -0.001215757 -0.0012157094 -0.001215625 -0.0012154747 -0.0012152545 -0.0012150315 -0.001214872 -0.0012147839 -0.0012147264 -0.0012146602 -0.00121454 -0.001214351][-0.0012176057 -0.0012155375 -0.0012151317 -0.0012150932 -0.0012150913 -0.0012150621 -0.0012149856 -0.0012148385 -0.0012146663 -0.0012145161 -0.0012144068 -0.0012143286 -0.0012142672 -0.0012141675 -0.0012140304][-0.0012173472 -0.0012152427 -0.0012148321 -0.0012147656 -0.0012147408 -0.0012147136 -0.0012146635 -0.0012145599 -0.0012144262 -0.0012142906 -0.0012141686 -0.001214068 -0.0012139927 -0.0012138976 -0.001213789][-0.0012178703 -0.001215657 -0.0012151961 -0.0012150439 -0.0012149329 -0.0012148403 -0.0012147541 -0.0012146484 -0.0012145175 -0.0012143701 -0.0012142208 -0.0012140883 -0.0012139822 -0.0012138617 -0.0012137363][-0.0012190751 -0.0012169087 -0.0012163584 -0.0012160714 -0.0012158218 -0.0012155958 -0.0012153928 -0.0012152073 -0.0012150214 -0.0012148255 -0.0012146292 -0.00121446 -0.0012143287 -0.0012141855 -0.0012140394][-0.0012209989 -0.0012188945 -0.0012181543 -0.0012177028 -0.0012172678 -0.0012168441 -0.0012164576 -0.0012161247 -0.0012158233 -0.0012155476 -0.0012153022 -0.0012151155 -0.0012149864 -0.0012148544 -0.0012147325][-0.0012235623 -0.0012213454 -0.0012203899 -0.0012197371 -0.0012190699 -0.001218403 -0.0012177833 -0.0012172525 -0.0012167905 -0.0012163955 -0.0012160736 -0.0012158515 -0.00121572 -0.0012156118 -0.0012155419][-0.0012262526 -0.0012240298 -0.0012228929 -0.0012220517 -0.0012211432 -0.0012202068 -0.0012193113 -0.0012185237 -0.0012178388 -0.0012172648 -0.0012168206 -0.0012165236 -0.0012163586 -0.0012162618 -0.001216244][-0.001228838 -0.0012265473 -0.0012252955 -0.0012243127 -0.0012232073 -0.0012220226 -0.0012208524 -0.0012197893 -0.0012188478 -0.0012180553 -0.00121744 -0.0012170242 -0.0012167851 -0.0012166641 -0.0012166657][-0.0012308637 -0.0012286182 -0.0012273113 -0.0012262616 -0.0012250378 -0.0012236772 -0.0012222881 -0.0012209767 -0.0012197832 -0.0012187633 -0.0012179555 -0.0012173896 -0.001217037 -0.001216848 -0.0012168159]]...]
INFO - root - 2017-12-09 09:35:45.746394: step 10710, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.850 sec/batch; 76h:00m:52s remains)
INFO - root - 2017-12-09 09:35:54.467879: step 10720, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.854 sec/batch; 76h:20m:30s remains)
INFO - root - 2017-12-09 09:36:03.096428: step 10730, loss = 0.81, batch loss = 0.68 (9.4 examples/sec; 0.854 sec/batch; 76h:18m:52s remains)
INFO - root - 2017-12-09 09:36:11.910231: step 10740, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.891 sec/batch; 79h:39m:29s remains)
INFO - root - 2017-12-09 09:36:20.690631: step 10750, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.892 sec/batch; 79h:44m:43s remains)
INFO - root - 2017-12-09 09:36:29.461936: step 10760, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.859 sec/batch; 76h:44m:12s remains)
INFO - root - 2017-12-09 09:36:37.920090: step 10770, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.882 sec/batch; 78h:47m:05s remains)
INFO - root - 2017-12-09 09:36:46.412086: step 10780, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.857 sec/batch; 76h:34m:08s remains)
INFO - root - 2017-12-09 09:36:55.066582: step 10790, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.873 sec/batch; 77h:58m:29s remains)
INFO - root - 2017-12-09 09:37:03.817764: step 10800, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.872 sec/batch; 77h:55m:24s remains)
2017-12-09 09:37:04.741785: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.32216388 0.32743159 0.32634598 0.3229866 0.31680143 0.30658361 0.29403421 0.27949283 0.26762348 0.25859144 0.25155464 0.24943466 0.25104427 0.25704035 0.26557294][0.30735373 0.31244206 0.31300294 0.31387806 0.3126817 0.30788016 0.29914397 0.28737956 0.27663481 0.2679632 0.26068619 0.25788209 0.25905013 0.26551911 0.27502266][0.28508803 0.29036769 0.29363185 0.29866898 0.30282447 0.30359915 0.30024588 0.29221645 0.28316283 0.27462378 0.26712418 0.26332054 0.26361603 0.26970142 0.27920371][0.25830439 0.26432776 0.27037263 0.28017467 0.28981254 0.29669404 0.29932308 0.29634261 0.29020071 0.28317165 0.27648038 0.27250454 0.27200443 0.27652934 0.28445488][0.23013122 0.23677094 0.24482997 0.25762519 0.27145696 0.28454474 0.29380575 0.29743949 0.29650995 0.29274291 0.28744942 0.28364107 0.28222287 0.28431052 0.28944188][0.20472379 0.21192634 0.22144802 0.23667461 0.25397721 0.27179185 0.28660786 0.29681581 0.30238032 0.30317795 0.30040246 0.29724491 0.29501134 0.29439825 0.29578632][0.18406306 0.19144993 0.20194194 0.2188618 0.23902863 0.26067042 0.27983242 0.29543257 0.30632487 0.31200227 0.31319472 0.31158406 0.30944198 0.30620211 0.30377373][0.17129712 0.1796229 0.19097908 0.20851253 0.22991931 0.25375143 0.27575254 0.29415852 0.3084031 0.31763086 0.32166907 0.32154506 0.31935796 0.31391835 0.30781958][0.16745694 0.17619582 0.18779932 0.20514494 0.22648038 0.25071606 0.27324873 0.29344639 0.30989674 0.32164389 0.32766715 0.32875398 0.32600296 0.31838289 0.30901828][0.1728732 0.18156925 0.19245292 0.20797306 0.22720636 0.24948101 0.27083436 0.29062042 0.30779856 0.32081056 0.32801557 0.32984218 0.32633251 0.3176586 0.30635917][0.1861601 0.1943257 0.20305461 0.21581239 0.23182736 0.25109708 0.26966432 0.28761759 0.30379778 0.31589159 0.32277974 0.32419056 0.32046741 0.31189761 0.3003619][0.19939607 0.20779239 0.21501391 0.22509518 0.23753382 0.2529985 0.26787093 0.28295928 0.29657191 0.30665338 0.31271988 0.31381306 0.31035733 0.30237481 0.2917484][0.21658139 0.22515756 0.23082562 0.23823562 0.24709806 0.25801554 0.26843673 0.2791487 0.28894055 0.2956962 0.29920584 0.29907206 0.29534921 0.28830114 0.27906546][0.23515913 0.24339753 0.24727826 0.25203332 0.25715944 0.26335987 0.2691015 0.27542648 0.28158617 0.28519735 0.28685713 0.28588146 0.28205997 0.27603117 0.26843369][0.25239617 0.2609123 0.26326767 0.2653181 0.26666915 0.26822847 0.26935866 0.27144846 0.27418804 0.2751852 0.27525264 0.27373183 0.27052611 0.26579276 0.26006171]]...]
INFO - root - 2017-12-09 09:37:13.373585: step 10810, loss = 0.81, batch loss = 0.68 (9.2 examples/sec; 0.867 sec/batch; 77h:27m:45s remains)
INFO - root - 2017-12-09 09:37:21.919429: step 10820, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.859 sec/batch; 76h:45m:17s remains)
INFO - root - 2017-12-09 09:37:30.698597: step 10830, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 77h:47m:48s remains)
INFO - root - 2017-12-09 09:37:39.504466: step 10840, loss = 0.81, batch loss = 0.68 (9.0 examples/sec; 0.888 sec/batch; 79h:23m:05s remains)
INFO - root - 2017-12-09 09:37:48.386751: step 10850, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.894 sec/batch; 79h:50m:43s remains)
INFO - root - 2017-12-09 09:37:57.200555: step 10860, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.883 sec/batch; 78h:55m:09s remains)
INFO - root - 2017-12-09 09:38:05.795734: step 10870, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 77h:13m:42s remains)
INFO - root - 2017-12-09 09:38:14.337327: step 10880, loss = 0.81, batch loss = 0.68 (9.4 examples/sec; 0.855 sec/batch; 76h:22m:46s remains)
INFO - root - 2017-12-09 09:38:23.066898: step 10890, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.874 sec/batch; 78h:02m:52s remains)
INFO - root - 2017-12-09 09:38:31.564029: step 10900, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.845 sec/batch; 75h:26m:51s remains)
2017-12-09 09:38:32.475278: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.13492118 0.14834258 0.16200951 0.17257722 0.17980306 0.1837239 0.18659198 0.18949148 0.19363239 0.19880028 0.20391248 0.20639634 0.2033098 0.19272591 0.1734118][0.13121578 0.14585283 0.16146211 0.17470402 0.18487099 0.19183178 0.19770977 0.20333557 0.20967036 0.21672465 0.22336623 0.22697423 0.22474875 0.21452767 0.19457065][0.12885052 0.14358133 0.15970516 0.17432851 0.18691371 0.19729085 0.20712878 0.21651314 0.22565988 0.23485893 0.24273016 0.24678844 0.24435528 0.23347427 0.21217825][0.13222227 0.14542924 0.16024593 0.17495163 0.1892978 0.20284101 0.21714532 0.23095481 0.2437667 0.25531995 0.26424649 0.26854205 0.26520383 0.2526482 0.22905526][0.14425164 0.15476899 0.16629809 0.17918353 0.19368474 0.20963763 0.22755562 0.24524526 0.26129025 0.27481717 0.28450748 0.2884613 0.28385833 0.26934806 0.24310784][0.16018489 0.16722533 0.17446676 0.18451409 0.19815792 0.21549246 0.23637602 0.25737181 0.27617687 0.29127684 0.30117235 0.30396315 0.29735458 0.28061116 0.25194773][0.17413506 0.17716676 0.17959079 0.18651852 0.1993459 0.21797445 0.24128039 0.26544315 0.28721276 0.30388221 0.3138895 0.31549045 0.30671674 0.28723314 0.25575885][0.17912988 0.17918886 0.17844918 0.18370247 0.19660516 0.2171343 0.24300519 0.26961932 0.29334196 0.310858 0.32052177 0.32054588 0.30949283 0.28736466 0.25339282][0.17339839 0.171961 0.1701524 0.17597477 0.19056591 0.21342716 0.24148889 0.26997325 0.2947433 0.31212625 0.32061315 0.31873974 0.3056148 0.28135729 0.24581352][0.1566169 0.15594018 0.15585721 0.16405442 0.18132095 0.20676501 0.23626547 0.26525351 0.28962097 0.30602434 0.31289977 0.30908677 0.29443163 0.26896581 0.23298763][0.12943861 0.13038509 0.13393524 0.14617595 0.16705084 0.19474904 0.22529818 0.25423107 0.27744794 0.29245248 0.2979936 0.29331625 0.27815008 0.252635 0.21746454][0.09723898 0.10080327 0.1084215 0.12439311 0.14821929 0.17756197 0.20817629 0.23611715 0.25792715 0.27173179 0.27670085 0.27234086 0.25834891 0.23450677 0.20148806][0.067012817 0.072969221 0.084313728 0.10328837 0.128736 0.15807937 0.18735784 0.21283707 0.23178759 0.24398601 0.24889527 0.24602522 0.23476645 0.21462868 0.18568502][0.042175032 0.049921785 0.063752323 0.084279664 0.11002357 0.13796005 0.16476519 0.18694851 0.20249005 0.21244125 0.21684569 0.21597396 0.20836915 0.19305721 0.16944619][0.024527596 0.033137634 0.04792485 0.06820602 0.092421718 0.11773539 0.14124972 0.15944259 0.17144191 0.17916262 0.1832774 0.18407668 0.17975841 0.16938809 0.1512018]]...]
INFO - root - 2017-12-09 09:38:41.164422: step 10910, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.888 sec/batch; 79h:17m:10s remains)
INFO - root - 2017-12-09 09:38:49.823536: step 10920, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.884 sec/batch; 78h:59m:00s remains)
INFO - root - 2017-12-09 09:38:58.433198: step 10930, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.856 sec/batch; 76h:27m:10s remains)
INFO - root - 2017-12-09 09:39:07.227332: step 10940, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.835 sec/batch; 74h:33m:25s remains)
INFO - root - 2017-12-09 09:39:15.944354: step 10950, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.882 sec/batch; 78h:47m:45s remains)
INFO - root - 2017-12-09 09:39:24.623513: step 10960, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.865 sec/batch; 77h:16m:24s remains)
INFO - root - 2017-12-09 09:39:33.255926: step 10970, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.859 sec/batch; 76h:42m:50s remains)
INFO - root - 2017-12-09 09:39:41.800975: step 10980, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.865 sec/batch; 77h:12m:56s remains)
INFO - root - 2017-12-09 09:39:50.444847: step 10990, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.858 sec/batch; 76h:38m:25s remains)
INFO - root - 2017-12-09 09:39:58.941341: step 11000, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.857 sec/batch; 76h:30m:48s remains)
2017-12-09 09:39:59.898612: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0012496428 -0.0012467196 -0.0012457954 -0.0012454066 -0.0012456733 -0.0012460941 -0.0012465258 -0.0012469459 -0.0012455977 -0.0012449059 -0.0012462215 -0.0012488376 -0.0012516496 -0.001252384 -0.0012521247][-0.0012490995 -0.0012465862 -0.0012459778 -0.0012459796 -0.0012464575 -0.0012473701 -0.0012481946 -0.001248083 -0.001243783 -0.0012408568 -0.0012425731 -0.0012496847 -0.001255195 -0.0012560294 -0.0012554972][-0.0012496044 -0.0012476532 -0.0012473614 -0.0012475959 -0.0012484239 -0.0012484775 -0.0012457882 -0.0012394773 -0.0012273024 -0.0012218253 -0.0012275139 -0.0012446885 -0.0012563015 -0.001258894 -0.0012584773][-0.0012501894 -0.0012486418 -0.0012484767 -0.0012480918 -0.0012454325 -0.0012382603 -0.0012210637 -0.0011966577 -0.0011675633 -0.0011610485 -0.0011808926 -0.0012216029 -0.0012498519 -0.0012596996 -0.0012602619][-0.001249706 -0.0012483256 -0.0012478743 -0.0012433496 -0.0012288771 -0.0012010472 -0.0011519436 -0.0010934569 -0.0010347588 -0.0010275444 -0.0010763515 -0.0011640412 -0.0012285642 -0.0012565349 -0.0012600888][-0.0012468311 -0.0012449832 -0.0012431947 -0.0012273947 -0.0011888401 -0.0011258242 -0.001035318 -0.00093765394 -0.000844802 -0.00083585572 -0.00092145347 -0.0010745067 -0.001192622 -0.0012492443 -0.0012572568][-0.0012417505 -0.001238315 -0.0012323929 -0.0011993451 -0.0011320065 -0.0010345483 -0.00091117737 -0.00078548456 -0.00066676852 -0.00065440987 -0.0007700714 -0.00098406372 -0.00115554 -0.0012410752 -0.0012536399][-0.0012384026 -0.0012324076 -0.0012207214 -0.0011720981 -0.001084749 -0.00096949132 -0.00083365967 -0.00070108689 -0.00057521032 -0.00056250719 -0.00069262017 -0.000936014 -0.0011354517 -0.0012350878 -0.00125025][-0.0012388548 -0.0012311883 -0.0012170089 -0.0011669801 -0.0010801512 -0.00097012136 -0.00084310106 -0.00072525436 -0.00061447528 -0.00060748565 -0.00073282316 -0.0009587449 -0.0011439165 -0.0012339855 -0.0012484775][-0.0012417641 -0.0012350761 -0.0012232659 -0.0011862064 -0.0011181461 -0.0010320522 -0.00093108241 -0.0008426382 -0.00076365157 -0.00076665269 -0.00086886383 -0.0010388173 -0.0011755972 -0.0012378587 -0.0012487045][-0.0012450551 -0.0012409326 -0.0012340961 -0.0012145806 -0.0011739928 -0.0011205256 -0.0010555862 -0.001001299 -0.00095703546 -0.00096545485 -0.0010324244 -0.0011333853 -0.0012116731 -0.0012443549 -0.0012506089][-0.0012465273 -0.0012441035 -0.0012413912 -0.0012344936 -0.0012170083 -0.0011923232 -0.0011608454 -0.0011352685 -0.0011169537 -0.0011244819 -0.0011580556 -0.0012038327 -0.0012374581 -0.0012500776 -0.0012529636][-0.0012465812 -0.0012445521 -0.0012436939 -0.0012424778 -0.0012377709 -0.0012303235 -0.0012203525 -0.0012121593 -0.0012069811 -0.0012103623 -0.0012223679 -0.0012377853 -0.0012485305 -0.0012525418 -0.0012537984][-0.0012461684 -0.0012437534 -0.0012431211 -0.001243247 -0.0012428152 -0.0012420723 -0.0012411567 -0.0012403979 -0.0012401012 -0.0012413532 -0.0012447131 -0.0012487732 -0.0012517225 -0.0012530114 -0.0012533583][-0.0012460283 -0.0012433348 -0.0012424077 -0.0012423574 -0.0012425511 -0.0012429826 -0.001243864 -0.0012449026 -0.0012459585 -0.0012470991 -0.0012488503 -0.0012508222 -0.0012520987 -0.0012524344 -0.0012520654]]...]
INFO - root - 2017-12-09 09:40:08.678851: step 11010, loss = 0.81, batch loss = 0.68 (9.2 examples/sec; 0.871 sec/batch; 77h:45m:08s remains)
INFO - root - 2017-12-09 09:40:17.235244: step 11020, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.893 sec/batch; 79h:43m:14s remains)
INFO - root - 2017-12-09 09:40:26.016371: step 11030, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.863 sec/batch; 77h:01m:55s remains)
INFO - root - 2017-12-09 09:40:34.839285: step 11040, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.888 sec/batch; 79h:19m:45s remains)
INFO - root - 2017-12-09 09:40:43.546887: step 11050, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 77h:47m:23s remains)
INFO - root - 2017-12-09 09:40:52.285177: step 11060, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.873 sec/batch; 77h:57m:14s remains)
INFO - root - 2017-12-09 09:41:00.793658: step 11070, loss = 0.81, batch loss = 0.68 (8.8 examples/sec; 0.908 sec/batch; 81h:06m:33s remains)
INFO - root - 2017-12-09 09:41:09.402123: step 11080, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.866 sec/batch; 77h:19m:58s remains)
INFO - root - 2017-12-09 09:41:18.066206: step 11090, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 77h:08m:16s remains)
INFO - root - 2017-12-09 09:41:26.786964: step 11100, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.834 sec/batch; 74h:29m:46s remains)
2017-12-09 09:41:27.671837: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.23657298 0.24154241 0.23955648 0.23241466 0.22298612 0.21239445 0.20147167 0.19111878 0.18340221 0.17909285 0.17616637 0.17306677 0.16662438 0.15691447 0.14493613][0.25854808 0.26535636 0.26429284 0.25697079 0.24644271 0.23388414 0.22066936 0.20810491 0.19821985 0.19243248 0.18878405 0.18510905 0.17827643 0.16798732 0.15546444][0.27489793 0.28343108 0.28388187 0.2774905 0.26694968 0.25411183 0.24008049 0.22604077 0.21441923 0.20691326 0.20158224 0.19597286 0.1873432 0.17586897 0.16281311][0.28758207 0.29764363 0.29946989 0.2947554 0.28534719 0.27301273 0.25914806 0.24524069 0.23341867 0.22499157 0.21816741 0.21014209 0.19925514 0.18595125 0.17182469][0.29532969 0.30631673 0.30930576 0.30645576 0.29921257 0.28876811 0.27640769 0.26407948 0.25352731 0.24540758 0.23771182 0.2277552 0.21487513 0.19928963 0.18335725][0.29654059 0.307674 0.31148794 0.31060758 0.30584085 0.29872936 0.29001927 0.28097209 0.27275023 0.2657935 0.25786513 0.24609266 0.23099096 0.21346252 0.1956569][0.2911371 0.30174914 0.3059524 0.30688688 0.30469552 0.30094978 0.29632786 0.29170749 0.28718305 0.28198555 0.27471074 0.26245695 0.24596636 0.2271226 0.20789547][0.28098592 0.29083467 0.29503778 0.29702196 0.29690522 0.29643533 0.29558969 0.29480711 0.29340169 0.29043967 0.28460273 0.27298969 0.2566185 0.23776619 0.21834746][0.26553574 0.27436775 0.27798888 0.28037819 0.28189328 0.28352794 0.28537405 0.28746229 0.28846872 0.28761086 0.28371081 0.27458259 0.26029739 0.24323922 0.22510934][0.24518363 0.25254261 0.25515679 0.25741914 0.25990978 0.2629734 0.26671892 0.27055851 0.27298087 0.27354148 0.27137515 0.26497743 0.25357985 0.23939715 0.22365032][0.22094485 0.22702977 0.22859798 0.2302926 0.23278578 0.23657627 0.24158192 0.24647558 0.24992938 0.25150585 0.25082892 0.24659318 0.2375745 0.22612377 0.21287121][0.1950866 0.20028907 0.20117693 0.20214804 0.20438741 0.20807399 0.21325363 0.21874873 0.22282436 0.2249901 0.22536863 0.2229851 0.21638164 0.20714931 0.19614592][0.16875781 0.17332086 0.17377135 0.17430952 0.17631669 0.17975979 0.18481329 0.18988565 0.1936684 0.19605902 0.1967098 0.19524926 0.19049321 0.18351784 0.17498855][0.14206953 0.14547618 0.1453554 0.14554611 0.14723073 0.15019959 0.15483783 0.15951571 0.16303921 0.16524309 0.16602166 0.16540177 0.16227259 0.15751927 0.15139291][0.11715718 0.1196727 0.11895998 0.11862242 0.11964149 0.12176494 0.12556343 0.12956938 0.13287641 0.13486247 0.13565536 0.13550381 0.13347784 0.13031666 0.12606083]]...]
INFO - root - 2017-12-09 09:41:36.398237: step 11110, loss = 0.81, batch loss = 0.68 (9.3 examples/sec; 0.865 sec/batch; 77h:12m:01s remains)
INFO - root - 2017-12-09 09:41:44.977432: step 11120, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.840 sec/batch; 75h:00m:00s remains)
INFO - root - 2017-12-09 09:41:53.692002: step 11130, loss = 0.81, batch loss = 0.68 (9.2 examples/sec; 0.869 sec/batch; 77h:33m:32s remains)
INFO - root - 2017-12-09 09:42:02.427223: step 11140, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 77h:24m:29s remains)
INFO - root - 2017-12-09 09:42:11.100888: step 11150, loss = 0.81, batch loss = 0.68 (9.6 examples/sec; 0.833 sec/batch; 74h:22m:02s remains)
INFO - root - 2017-12-09 09:42:19.721197: step 11160, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.879 sec/batch; 78h:28m:26s remains)
INFO - root - 2017-12-09 09:42:28.259160: step 11170, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.884 sec/batch; 78h:52m:51s remains)
INFO - root - 2017-12-09 09:42:36.618726: step 11180, loss = 0.82, batch loss = 0.69 (10.2 examples/sec; 0.787 sec/batch; 70h:16m:06s remains)
INFO - root - 2017-12-09 09:42:45.330355: step 11190, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.838 sec/batch; 74h:48m:51s remains)
INFO - root - 2017-12-09 09:42:54.005320: step 11200, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.887 sec/batch; 79h:09m:39s remains)
2017-12-09 09:42:54.971989: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.0089729661 0.008186874 0.00751187 0.007093844 0.0069595845 0.0070606647 0.0071604345 0.0072281538 0.0072999047 0.0073906854 0.0075424165 0.0077532791 0.0080251312 0.0079258671 0.0071381638][0.0085907215 0.0080123283 0.007554194 0.0074091973 0.0076142475 0.00812468 0.0086970758 0.0092596272 0.0098151425 0.010277354 0.010639079 0.01088432 0.011004156 0.010591576 0.00937531][0.0079460954 0.0076992819 0.0076434873 0.0079140384 0.0085326266 0.0094399806 0.010459404 0.011512259 0.012529935 0.013316527 0.013846762 0.014040786 0.013912812 0.013090983 0.011402413][0.0071378034 0.0072212797 0.0075982045 0.0083548455 0.0094211735 0.010727932 0.012164479 0.013638532 0.015065393 0.016138213 0.016767891 0.016827986 0.01644602 0.015299731 0.013234575][0.0062185321 0.0066308537 0.0074617835 0.0086584687 0.01008438 0.011712824 0.013495465 0.015366075 0.017167149 0.018503001 0.019217808 0.01917327 0.018537451 0.017132916 0.014769466][0.0054115336 0.0060923276 0.0072714863 0.0087927617 0.010486729 0.012343696 0.014351008 0.0164985 0.018558666 0.02007856 0.020834023 0.020711882 0.019902181 0.01830177 0.015743408][0.0047971141 0.0056525832 0.0070298426 0.0087029189 0.010514174 0.012474948 0.014554442 0.016767826 0.018846748 0.020404505 0.021167966 0.021029931 0.020166103 0.018519692 0.015930992][0.0044938056 0.0054765306 0.0069431765 0.0086120879 0.010367587 0.012223593 0.014126045 0.016104851 0.017931214 0.019322475 0.020025741 0.019945577 0.019188896 0.017670793 0.015229092][0.0041837618 0.0052542714 0.0067361612 0.008326211 0.0099474266 0.011569017 0.013105907 0.014625077 0.016017254 0.017101692 0.017659847 0.01762159 0.017017551 0.015740454 0.013596854][0.0035762133 0.0046456009 0.0060427859 0.0074937837 0.0089383665 0.010270059 0.011404926 0.012422765 0.013337102 0.014078029 0.014475561 0.014451989 0.01396811 0.012934883 0.011128922][0.002640601 0.0035839246 0.00477844 0.0060167694 0.0072239293 0.0082455892 0.0090005025 0.0095757591 0.010080026 0.010515394 0.010748681 0.010698376 0.010333627 0.0095409378 0.0081329076][0.0015087815 0.0022475715 0.0031710158 0.0041343719 0.0050516734 0.0057608048 0.0062020393 0.0064558731 0.0066749761 0.0068924837 0.0070064478 0.0069499575 0.0066927643 0.0061461758 0.0051419633][0.0004662521 0.0009854699 0.00162342 0.0022888582 0.0029073472 0.0033414161 0.0035612111 0.0036343595 0.0037029048 0.0037814 0.0038088637 0.00374833 0.0035804973 0.0032438033 0.0026056676][-0.00037979143 -6.3698506e-05 0.00032882055 0.0007493326 0.0011317928 0.0013775006 0.0014798387 0.0014891899 0.0015030608 0.001520443 0.0015048138 0.0014376598 0.0013257209 0.0011348766 0.000773411][-0.00092448725 -0.0007728319 -0.00057590893 -0.00034941314 -0.00014535582 -1.4111167e-05 4.2797881e-05 4.9679889e-05 5.60442e-05 5.7114521e-05 3.2901182e-05 -2.1875603e-05 -9.0006157e-05 -0.00018830365 -0.00036670658]]...]
INFO - root - 2017-12-09 09:43:03.628750: step 11210, loss = 0.82, batch loss = 0.69 (10.3 examples/sec; 0.773 sec/batch; 69h:00m:59s remains)
INFO - root - 2017-12-09 09:43:12.218159: step 11220, loss = 0.81, batch loss = 0.68 (9.1 examples/sec; 0.876 sec/batch; 78h:09m:36s remains)
INFO - root - 2017-12-09 09:43:20.762192: step 11230, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.857 sec/batch; 76h:26m:56s remains)
INFO - root - 2017-12-09 09:43:29.260490: step 11240, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.865 sec/batch; 77h:12m:00s remains)
INFO - root - 2017-12-09 09:43:37.900090: step 11250, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.854 sec/batch; 76h:10m:13s remains)
INFO - root - 2017-12-09 09:43:46.764454: step 11260, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.873 sec/batch; 77h:53m:48s remains)
INFO - root - 2017-12-09 09:43:55.305245: step 11270, loss = 0.82, batch loss = 0.69 (10.8 examples/sec; 0.742 sec/batch; 66h:14m:35s remains)
INFO - root - 2017-12-09 09:44:03.738538: step 11280, loss = 0.82, batch loss = 0.69 (9.9 examples/sec; 0.808 sec/batch; 72h:08m:09s remains)
INFO - root - 2017-12-09 09:44:12.376378: step 11290, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.883 sec/batch; 78h:48m:00s remains)
INFO - root - 2017-12-09 09:44:20.925603: step 11300, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.874 sec/batch; 77h:59m:25s remains)
2017-12-09 09:44:21.825178: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.13477466 0.12525566 0.11204213 0.099288069 0.089191057 0.084410109 0.084990084 0.089768514 0.097649 0.10685216 0.11656425 0.12436944 0.12833169 0.12679531 0.11887329][0.1617555 0.15238205 0.13694942 0.12091813 0.10741875 0.10012015 0.09947525 0.10450689 0.11355528 0.1244479 0.1356094 0.14423922 0.14781061 0.14472844 0.13428122][0.18605408 0.17815258 0.16193239 0.14377689 0.12752184 0.11762651 0.11519521 0.1195281 0.12877566 0.13982776 0.15081994 0.15878955 0.16075997 0.15540905 0.14213963][0.2059399 0.20079544 0.18543442 0.16668862 0.14886095 0.13671152 0.13204941 0.13453691 0.14251333 0.15267767 0.16266933 0.1689489 0.16871829 0.16096263 0.1451048][0.22080334 0.21935041 0.20664698 0.18903258 0.17098793 0.15732291 0.15037538 0.15023956 0.15567513 0.16360882 0.17158334 0.17569898 0.17330429 0.16335323 0.14552978][0.23028585 0.23274274 0.22385709 0.20911953 0.19267294 0.17892104 0.17026243 0.16706657 0.16880222 0.17318481 0.17790692 0.17915523 0.17449206 0.16295716 0.14412987][0.23415606 0.24025021 0.23540471 0.2243266 0.21094476 0.19838281 0.1888566 0.18308403 0.1809638 0.18104275 0.18182619 0.1800769 0.17351995 0.16099912 0.14200078][0.23416527 0.2431535 0.24172144 0.23418613 0.2237214 0.21287216 0.20313145 0.1949511 0.18908872 0.18493088 0.18181668 0.17704193 0.16889745 0.1560684 0.13772979][0.23350573 0.24445941 0.24547115 0.24063382 0.23230608 0.22259484 0.21226089 0.20189479 0.19238327 0.18409319 0.17727455 0.16980642 0.16048057 0.14764942 0.13052657][0.2340724 0.24582864 0.24788238 0.24427216 0.23671848 0.22698253 0.21532592 0.20227975 0.18908781 0.17680055 0.16628659 0.15630469 0.14595065 0.1333407 0.11772971][0.236824 0.248155 0.24920198 0.24475183 0.23629251 0.225275 0.21175763 0.19615358 0.17986667 0.16418059 0.15033096 0.13778216 0.12597002 0.11338073 0.099189788][0.24157372 0.25088176 0.2491633 0.24195743 0.23060583 0.21714061 0.20139942 0.18394133 0.16577752 0.14809655 0.13213409 0.11760527 0.10440301 0.091404893 0.07810396][0.2467539 0.25333691 0.24806012 0.23717819 0.22209013 0.20563373 0.18766765 0.16868231 0.14952645 0.13111646 0.1142721 0.098640881 0.084527634 0.071246125 0.05862321][0.24942797 0.25347486 0.2447045 0.23019522 0.21165527 0.1924926 0.17266354 0.15289514 0.13363028 0.11541221 0.098416582 0.082320914 0.067682095 0.054285783 0.042278029][0.24653196 0.24857059 0.23724075 0.22004546 0.19894762 0.17766 0.15638322 0.13607098 0.11713583 0.099727243 0.083445571 0.067712672 0.053201124 0.040124424 0.028843991]]...]
INFO - root - 2017-12-09 09:44:30.432197: step 11310, loss = 0.82, batch loss = 0.69 (11.2 examples/sec; 0.712 sec/batch; 63h:30m:57s remains)
INFO - root - 2017-12-09 09:44:39.295050: step 11320, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.903 sec/batch; 80h:35m:33s remains)
INFO - root - 2017-12-09 09:44:47.905385: step 11330, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.861 sec/batch; 76h:51m:10s remains)
INFO - root - 2017-12-09 09:44:56.738951: step 11340, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.894 sec/batch; 79h:43m:14s remains)
INFO - root - 2017-12-09 09:45:05.500854: step 11350, loss = 0.81, batch loss = 0.68 (9.4 examples/sec; 0.854 sec/batch; 76h:13m:36s remains)
INFO - root - 2017-12-09 09:45:14.159376: step 11360, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 77h:19m:08s remains)
INFO - root - 2017-12-09 09:45:22.895743: step 11370, loss = 0.81, batch loss = 0.68 (8.9 examples/sec; 0.901 sec/batch; 80h:23m:19s remains)
INFO - root - 2017-12-09 09:45:31.337651: step 11380, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.848 sec/batch; 75h:40m:57s remains)
INFO - root - 2017-12-09 09:45:39.856576: step 11390, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 77h:43m:38s remains)
INFO - root - 2017-12-09 09:45:48.584746: step 11400, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.866 sec/batch; 77h:14m:30s remains)
2017-12-09 09:45:49.400624: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0012842934 -0.0012834826 -0.0012832959 -0.0012833113 -0.001283402 -0.0012833948 -0.0012831775 -0.0012828732 -0.0012826642 -0.0012826259 -0.0012826447 -0.0012825984 -0.0012826425 -0.0012825611 -0.0012825434][-0.0012844608 -0.0012838525 -0.0012838083 -0.0012839359 -0.0012841284 -0.0012840769 -0.0012832468 -0.0012817868 -0.001280424 -0.0012798704 -0.0012802086 -0.0012811371 -0.0012818794 -0.0012820016 -0.0012819656][-0.0012853837 -0.001284998 -0.0012850964 -0.0012853728 -0.0012853104 -0.0012836402 -0.0012788731 -0.0012719341 -0.0012667524 -0.0012661095 -0.0012699402 -0.0012755234 -0.001279943 -0.0012817857 -0.0012818513][-0.0012858699 -0.0012828476 -0.0012761774 -0.0012605523 -0.0012287109 -0.0011761803 -0.0011132311 -0.0010659654 -0.0010613971 -0.0011026142 -0.0011682281 -0.0012286459 -0.0012658102 -0.0012801279 -0.0012819045][-0.0012688087 -0.0012280731 -0.0011474473 -0.0010102559 -0.00080455013 -0.00054008537 -0.00028051622 -0.00013223058 -0.00018132222 -0.00041792152 -0.00073922239 -0.0010227468 -0.0011968294 -0.0012675765 -0.0012818812][-0.0012156721 -0.0010804714 -0.00081519957 -0.00038598181 0.00020568643 0.00089854363 0.0015238676 0.0018378036 0.0016580204 0.0010207234 0.00017958786 -0.00056521856 -0.0010326137 -0.0012325096 -0.0012801933][-0.0011331334 -0.0008664421 -0.00034710666 0.00048038433 0.0015821118 0.0028123995 0.0038740127 0.0043742838 0.004028467 0.0028969822 0.0014055058 6.6637411e-05 -0.00079470983 -0.0011772671 -0.0012762705][-0.0010757493 -0.00072649919 -4.839641e-05 0.0010317353 0.0024615345 0.004041411 0.0053966874 0.0060464363 0.0056347279 0.0042152405 0.0023041242 0.000552458 -0.00060216675 -0.0011296778 -0.0012720086][-0.0010994353 -0.00077932532 -0.00014819461 0.0008724361 0.0022429265 0.0037775752 0.0051209792 0.0058091613 0.0054828296 0.0041624024 0.0023189844 0.00058691972 -0.00057935121 -0.0011226389 -0.0012714313][-0.001177504 -0.00097299559 -0.00055418862 0.00014774478 0.0011271121 0.0022657388 0.003307872 0.0038977284 0.0037396797 0.0028187665 0.0014655007 0.00015904405 -0.00073735911 -0.0011601422 -0.0012754416][-0.0012485583 -0.0011608349 -0.000968326 -0.00062502 -0.00011507911 0.00051607785 0.0011341252 0.0015277037 0.0015056919 0.0010351475 0.00028949161 -0.00045258965 -0.00096985634 -0.0012147424 -0.0012796853][-0.001283125 -0.0012602576 -0.0012018436 -0.0010856939 -0.000895004 -0.00063837157 -0.00036591233 -0.00017103786 -0.00014663325 -0.00031950534 -0.00062273716 -0.00093414355 -0.0011532371 -0.0012562069 -0.0012818958][-0.0012899677 -0.0012878622 -0.001279042 -0.0012563592 -0.0012112295 -0.0011413525 -0.0010577777 -0.00098908274 -0.00096806814 -0.0010087232 -0.0010935966 -0.0011839484 -0.0012475282 -0.0012763125 -0.0012824028][-0.0012881665 -0.0012881125 -0.0012872145 -0.0012844735 -0.0012775807 -0.001265155 -0.0012486904 -0.0012335863 -0.0012268967 -0.0012325527 -0.0012481166 -0.0012651705 -0.0012768031 -0.0012817287 -0.0012822124][-0.0012859991 -0.0012857767 -0.0012856721 -0.0012855871 -0.0012847796 -0.0012828036 -0.0012800678 -0.0012775626 -0.0012766467 -0.0012771797 -0.0012789187 -0.0012808043 -0.001282081 -0.0012824085 -0.0012819142]]...]
INFO - root - 2017-12-09 09:45:57.985954: step 11410, loss = 0.82, batch loss = 0.69 (10.6 examples/sec; 0.752 sec/batch; 67h:06m:00s remains)
INFO - root - 2017-12-09 09:46:06.669087: step 11420, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.865 sec/batch; 77h:08m:07s remains)
INFO - root - 2017-12-09 09:46:15.405782: step 11430, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.882 sec/batch; 78h:41m:37s remains)
INFO - root - 2017-12-09 09:46:24.206408: step 11440, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 77h:32m:22s remains)
INFO - root - 2017-12-09 09:46:33.051012: step 11450, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.880 sec/batch; 78h:30m:03s remains)
INFO - root - 2017-12-09 09:46:41.920323: step 11460, loss = 0.82, batch loss = 0.69 (8.5 examples/sec; 0.940 sec/batch; 83h:51m:38s remains)
INFO - root - 2017-12-09 09:46:50.653016: step 11470, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.880 sec/batch; 78h:27m:22s remains)
INFO - root - 2017-12-09 09:46:59.251824: step 11480, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.889 sec/batch; 79h:14m:26s remains)
INFO - root - 2017-12-09 09:47:07.590512: step 11490, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.841 sec/batch; 75h:01m:33s remains)
INFO - root - 2017-12-09 09:47:16.070096: step 11500, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.870 sec/batch; 77h:32m:17s remains)
2017-12-09 09:47:16.956215: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.065904327 0.066013582 0.0654816 0.065276556 0.065412387 0.065474324 0.065193914 0.064160571 0.06205884 0.058609035 0.054062281 0.048680834 0.042442072 0.0353834 0.028082218][0.067241773 0.067291573 0.066519335 0.06612622 0.066159718 0.066492185 0.06652756 0.065878883 0.064278334 0.061473932 0.057583362 0.052468084 0.046509903 0.0396468 0.03257693][0.063916504 0.064126492 0.063275531 0.062827058 0.062891729 0.063391984 0.063835457 0.063654684 0.06250903 0.060321808 0.057050325 0.052756302 0.047675073 0.041775335 0.035921242][0.06070536 0.060959268 0.060107421 0.059624214 0.059795704 0.060560528 0.061357725 0.061615068 0.060995981 0.059384152 0.056657039 0.052910861 0.048618682 0.043839976 0.039267834][0.05750104 0.057441577 0.056126751 0.055474631 0.055734146 0.056741972 0.057992794 0.058948915 0.059078418 0.058040932 0.055925302 0.052868985 0.049333896 0.045438353 0.041992065][0.053995654 0.053557675 0.051567536 0.050212 0.050126314 0.051115941 0.052770283 0.054441955 0.05557964 0.055656709 0.054416891 0.052139118 0.049293227 0.046230517 0.043535613][0.050770331 0.049826387 0.047153756 0.045154285 0.044446159 0.04490966 0.046588726 0.048678908 0.050526306 0.051444404 0.051112317 0.049705483 0.047425453 0.04483306 0.042499162][0.046291206 0.045552395 0.0428145 0.040496148 0.039200447 0.039022688 0.0402376 0.042010162 0.043822631 0.044951897 0.045152366 0.044343315 0.04247088 0.040166691 0.038009159][0.039880123 0.039371561 0.036842104 0.0346803 0.033195693 0.032502189 0.032922044 0.033934746 0.035048395 0.035672419 0.035763416 0.035092928 0.033455666 0.031329066 0.02931454][0.030606315 0.029926643 0.027693197 0.025761515 0.024230208 0.023264922 0.02303445 0.023217663 0.023616016 0.023687199 0.023548823 0.022852592 0.02148129 0.019720659 0.017991491][0.019733053 0.018805861 0.016969223 0.01538163 0.014041225 0.013108879 0.012610823 0.012379169 0.012310075 0.012202443 0.012040082 0.011492522 0.010493846 0.009265014 0.00805389][0.0095916567 0.0087792939 0.0076059075 0.0065379869 0.0056048175 0.0049259635 0.0044724937 0.0042080726 0.0040695206 0.0039440021 0.0038292781 0.0035892236 0.0031493832 0.0025636011 0.0019743603][0.0025539068 0.0021039944 0.0015585318 0.0010851424 0.00068029528 0.00035083585 0.00010930875 -3.321527e-05 -0.00010840828 -0.00015732762 -0.00018416496 -0.00025046163 -0.00037842523 -0.00053406012 -0.0006971407][-0.00055922213 -0.00068208913 -0.00081784342 -0.00093717506 -0.0010445185 -0.0011326213 -0.0011917549 -0.001221839 -0.0012369484 -0.0012466108 -0.0012542139 -0.0012592979 -0.0012599953 -0.0012572793 -0.0012541036][-0.0012727822 -0.0012704388 -0.0012686431 -0.001268269 -0.0012667832 -0.0012651695 -0.001264545 -0.0012640316 -0.001264459 -0.0012646033 -0.0012646514 -0.0012651805 -0.0012626237 -0.0012583229 -0.0012537274]]...]
INFO - root - 2017-12-09 09:47:25.407939: step 11510, loss = 0.82, batch loss = 0.69 (10.8 examples/sec; 0.740 sec/batch; 65h:57m:03s remains)
INFO - root - 2017-12-09 09:47:34.089040: step 11520, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 77h:30m:08s remains)
INFO - root - 2017-12-09 09:47:42.941007: step 11530, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.870 sec/batch; 77h:34m:14s remains)
INFO - root - 2017-12-09 09:47:51.775806: step 11540, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.883 sec/batch; 78h:42m:08s remains)
INFO - root - 2017-12-09 09:48:00.407339: step 11550, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.863 sec/batch; 76h:54m:42s remains)
INFO - root - 2017-12-09 09:48:09.202704: step 11560, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.875 sec/batch; 77h:59m:45s remains)
INFO - root - 2017-12-09 09:48:17.869897: step 11570, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.884 sec/batch; 78h:45m:48s remains)
INFO - root - 2017-12-09 09:48:26.331581: step 11580, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.850 sec/batch; 75h:46m:21s remains)
INFO - root - 2017-12-09 09:48:34.818919: step 11590, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.860 sec/batch; 76h:39m:10s remains)
INFO - root - 2017-12-09 09:48:43.250884: step 11600, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.845 sec/batch; 75h:17m:49s remains)
2017-12-09 09:48:44.089717: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0012964727 -0.0012960571 -0.0012959989 -0.0012959345 -0.0012956988 -0.0012953846 -0.0012951127 -0.0012949123 -0.0012948313 -0.0012948576 -0.0012949453 -0.001295044 -0.0012950953 -0.0012951146 -0.001295078][-0.0012968712 -0.0012965157 -0.001296499 -0.0012963993 -0.0012960606 -0.0012956201 -0.0012952316 -0.0012949224 -0.0012948045 -0.0012948303 -0.0012949165 -0.0012950171 -0.0012950877 -0.0012951524 -0.0012951022][-0.0012974797 -0.0012971651 -0.0012971687 -0.0012970024 -0.0012965625 -0.0012960329 -0.0012955647 -0.0012951579 -0.0012949639 -0.0012949141 -0.0012949281 -0.0012950134 -0.0012951186 -0.0012952585 -0.0012952578][-0.0012980845 -0.0012977708 -0.0012977049 -0.001297468 -0.0012969672 -0.0012963645 -0.0012958525 -0.0012953472 -0.0012950471 -0.0012949165 -0.0012948656 -0.0012949225 -0.0012949927 -0.0012951398 -0.0012951915][-0.0012982667 -0.0012979151 -0.0012978103 -0.0012975718 -0.0012971244 -0.0012965631 -0.0012960538 -0.0012954865 -0.0012950856 -0.001294895 -0.0012948096 -0.001294788 -0.0012947151 -0.0012947724 -0.0012948575][-0.0012978723 -0.001297463 -0.0012973199 -0.0012970996 -0.0012967677 -0.0012963284 -0.0012959054 -0.0012953297 -0.0012948313 -0.0012945629 -0.0012944282 -0.0012943298 -0.0012941005 -0.0012940812 -0.0012942459][-0.0012973796 -0.0012969178 -0.0012967124 -0.0012964715 -0.0012962223 -0.0012958949 -0.001295561 -0.0012949714 -0.0012943622 -0.0012939835 -0.0012937634 -0.0012936263 -0.0012933509 -0.001293336 -0.0012935983][-0.0012969977 -0.0012965505 -0.0012963947 -0.001296218 -0.001296017 -0.0012957533 -0.0012954508 -0.0012948358 -0.0012941549 -0.0012937052 -0.0012934462 -0.0012933081 -0.0012930749 -0.0012930726 -0.0012933764][-0.0012964723 -0.0012960805 -0.0012960774 -0.0012960697 -0.0012960128 -0.0012958953 -0.0012956547 -0.0012950489 -0.0012943319 -0.0012938185 -0.0012934954 -0.0012933137 -0.0012931093 -0.0012931142 -0.0012933902][-0.0012958235 -0.0012954688 -0.0012955721 -0.0012956875 -0.0012957912 -0.0012958759 -0.0012958065 -0.0012953539 -0.0012946742 -0.0012941142 -0.0012937133 -0.0012934479 -0.0012932458 -0.0012932369 -0.0012934625][-0.0012954139 -0.0012950358 -0.0012951507 -0.0012952948 -0.0012954272 -0.0012955985 -0.0012956555 -0.0012953557 -0.0012947774 -0.0012942652 -0.0012938626 -0.0012935849 -0.0012933846 -0.0012933757 -0.0012935851][-0.0012951473 -0.0012948168 -0.0012949017 -0.0012949972 -0.0012950782 -0.0012952107 -0.0012952342 -0.0012949962 -0.0012945342 -0.0012941329 -0.0012938089 -0.001293577 -0.0012934435 -0.0012934781 -0.0012937051][-0.0012948378 -0.0012945278 -0.0012945439 -0.0012945582 -0.0012945927 -0.0012946978 -0.0012947269 -0.0012945671 -0.0012942586 -0.0012939938 -0.0012937938 -0.0012936112 -0.0012935195 -0.0012935724 -0.0012937915][-0.0012945319 -0.0012942146 -0.0012941456 -0.0012941 -0.0012941166 -0.0012942107 -0.0012942537 -0.0012941787 -0.0012940093 -0.0012938468 -0.0012937108 -0.0012935707 -0.0012935173 -0.0012935827 -0.00129378][-0.0012944661 -0.001294092 -0.0012939587 -0.0012938847 -0.0012938908 -0.0012939586 -0.0012940082 -0.0012939892 -0.0012939024 -0.0012937918 -0.0012936837 -0.0012935824 -0.0012935499 -0.0012936171 -0.0012937856]]...]
INFO - root - 2017-12-09 09:48:52.579777: step 11610, loss = 0.82, batch loss = 0.69 (10.8 examples/sec; 0.744 sec/batch; 66h:18m:41s remains)
INFO - root - 2017-12-09 09:49:01.322676: step 11620, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.842 sec/batch; 75h:01m:56s remains)
INFO - root - 2017-12-09 09:49:09.955818: step 11630, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.886 sec/batch; 79h:00m:49s remains)
INFO - root - 2017-12-09 09:49:18.572358: step 11640, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 77h:16m:02s remains)
INFO - root - 2017-12-09 09:49:27.355056: step 11650, loss = 0.81, batch loss = 0.68 (8.8 examples/sec; 0.908 sec/batch; 80h:52m:56s remains)
INFO - root - 2017-12-09 09:49:36.038705: step 11660, loss = 0.81, batch loss = 0.68 (9.5 examples/sec; 0.845 sec/batch; 75h:18m:58s remains)
INFO - root - 2017-12-09 09:49:44.765212: step 11670, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.880 sec/batch; 78h:25m:21s remains)
INFO - root - 2017-12-09 09:49:53.218484: step 11680, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.865 sec/batch; 77h:04m:46s remains)
INFO - root - 2017-12-09 09:50:01.602075: step 11690, loss = 0.83, batch loss = 0.70 (9.2 examples/sec; 0.869 sec/batch; 77h:25m:06s remains)
INFO - root - 2017-12-09 09:50:10.260475: step 11700, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.845 sec/batch; 75h:15m:29s remains)
2017-12-09 09:50:11.071454: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.035114117 0.033083022 0.030837396 0.028561713 0.02678673 0.025686419 0.024984507 0.024313929 0.024898076 0.028914763 0.034797266 0.040889181 0.043241806 0.040963933 0.034907244][0.049167097 0.0473845 0.045072332 0.042933226 0.0413782 0.040471666 0.03970807 0.039352521 0.040209047 0.044514976 0.050761767 0.057156727 0.059426345 0.05630748 0.048665918][0.064132906 0.063748822 0.062583216 0.061833493 0.061686486 0.061928719 0.061926723 0.061979234 0.062768951 0.066504076 0.071844712 0.077268906 0.078592412 0.074485555 0.065626927][0.076784566 0.078641444 0.079525851 0.081268847 0.083634637 0.08593864 0.087635346 0.088662021 0.089458466 0.0920914 0.095649473 0.099034593 0.098738685 0.093591265 0.083936021][0.082805023 0.087205656 0.090620078 0.095137954 0.10022203 0.10526393 0.10951988 0.11239722 0.11380509 0.11566397 0.11713284 0.11802781 0.11597929 0.11010684 0.10049514][0.079872482 0.086202964 0.091843672 0.098676667 0.10621969 0.11382063 0.12080957 0.12601978 0.12864403 0.13051733 0.13059896 0.12967981 0.1261868 0.11999881 0.11096266][0.067953341 0.075065158 0.08191555 0.0900067 0.099015124 0.10849193 0.1179369 0.12537496 0.12964706 0.13247721 0.13238963 0.13060929 0.12612763 0.12001044 0.1119722][0.050345279 0.056900583 0.063649252 0.071628362 0.080689646 0.090718649 0.10149329 0.11072253 0.11702053 0.1213293 0.12207242 0.12018114 0.11533453 0.10935332 0.10216963][0.031466827 0.036520824 0.042086646 0.048766069 0.05675517 0.066091917 0.076710992 0.086480267 0.0940933 0.099743776 0.1017013 0.10024133 0.095494218 0.090041079 0.084042937][0.015938859 0.019152768 0.02304855 0.027942987 0.034122661 0.041635148 0.050610378 0.05944068 0.067035608 0.073091768 0.076010264 0.075392976 0.071565591 0.067457005 0.06339433][0.0062862327 0.0079881838 0.010268523 0.01337399 0.01754926 0.02277104 0.029390972 0.036262214 0.042714685 0.048258737 0.052052379 0.052828893 0.050854959 0.048787132 0.047415145][0.0019929165 0.0027205311 0.0038597891 0.0056410441 0.0079955868 0.011070922 0.015238265 0.020367479 0.0258616 0.031121625 0.036089726 0.039055496 0.039766535 0.040122002 0.041670211][0.0013558152 0.0016081375 0.0020643242 0.002988765 0.0040946761 0.0057013859 0.00826834 0.012395318 0.017835096 0.023926336 0.030797152 0.03651084 0.040032744 0.042720467 0.046637878][0.0020934958 0.0021952682 0.0023734756 0.0029031234 0.0034122271 0.0044725719 0.0066198297 0.010897348 0.017426429 0.025510091 0.035053834 0.043942332 0.049892776 0.054320961 0.059310213][0.0029813619 0.0033431323 0.0036727176 0.0040842462 0.0045318 0.0057189758 0.0083281193 0.013536814 0.021908475 0.03269735 0.045352668 0.057286322 0.065120406 0.070147172 0.074573547]]...]
INFO - root - 2017-12-09 09:50:19.631832: step 11710, loss = 0.81, batch loss = 0.68 (9.4 examples/sec; 0.847 sec/batch; 75h:27m:58s remains)
INFO - root - 2017-12-09 09:50:28.157420: step 11720, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.845 sec/batch; 75h:15m:41s remains)
INFO - root - 2017-12-09 09:50:36.873494: step 11730, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 77h:25m:40s remains)
INFO - root - 2017-12-09 09:50:45.567313: step 11740, loss = 0.82, batch loss = 0.69 (8.6 examples/sec; 0.929 sec/batch; 82h:46m:14s remains)
INFO - root - 2017-12-09 09:50:54.262365: step 11750, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 77h:35m:31s remains)
INFO - root - 2017-12-09 09:51:02.797723: step 11760, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.852 sec/batch; 75h:55m:07s remains)
INFO - root - 2017-12-09 09:51:11.478450: step 11770, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.881 sec/batch; 78h:28m:32s remains)
INFO - root - 2017-12-09 09:51:20.046995: step 11780, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.855 sec/batch; 76h:10m:01s remains)
INFO - root - 2017-12-09 09:51:28.610802: step 11790, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.833 sec/batch; 74h:11m:37s remains)
INFO - root - 2017-12-09 09:51:37.249488: step 11800, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.865 sec/batch; 77h:04m:46s remains)
2017-12-09 09:51:38.135219: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.15366846 0.15744063 0.15955962 0.16078974 0.16251084 0.16531682 0.16918573 0.1730115 0.17486306 0.1725186 0.16435173 0.14761885 0.12317096 0.093954183 0.064849973][0.18526393 0.19191153 0.19546193 0.19737433 0.19911912 0.20165288 0.20511357 0.20875563 0.21023986 0.2072646 0.19743234 0.1777665 0.14887612 0.11413704 0.079186648][0.20793703 0.21790585 0.22351934 0.22657526 0.22900645 0.23202288 0.23555318 0.2390618 0.23998065 0.23588781 0.22384018 0.2009791 0.16812956 0.12912504 0.089922413][0.22057879 0.23313382 0.24047567 0.24477039 0.24794759 0.25153869 0.25552624 0.25935704 0.26030666 0.25589544 0.24262163 0.2176542 0.18180668 0.13946488 0.097040333][0.22411694 0.23778354 0.24574098 0.25078806 0.2547833 0.2590819 0.26375785 0.26814795 0.26956394 0.26545152 0.25205886 0.22649804 0.18937834 0.14546615 0.10133713][0.21995522 0.23338428 0.24058418 0.24518147 0.24933857 0.25442779 0.26005322 0.2650823 0.2673111 0.26405218 0.25145975 0.22668236 0.19001128 0.14640594 0.10233313][0.20879805 0.22099404 0.22624971 0.22925837 0.23270412 0.23774406 0.24386892 0.24980381 0.25330546 0.25144893 0.24055859 0.21770446 0.18302535 0.14145903 0.099189386][0.19369231 0.2036372 0.20636326 0.20703819 0.20869969 0.21275307 0.21857554 0.22464745 0.22905016 0.22881281 0.22016183 0.20013808 0.16881667 0.13092421 0.092145152][0.17365848 0.18125765 0.18134934 0.17964801 0.17945012 0.1818589 0.18658993 0.19234097 0.19713154 0.19807363 0.19157375 0.17482936 0.14786516 0.11494091 0.0810139][0.14874609 0.15376084 0.15184797 0.14857152 0.14690824 0.14785969 0.15147614 0.15655315 0.16122195 0.16269252 0.15774627 0.14406413 0.12173369 0.094490379 0.066411883][0.119004 0.12183516 0.1186773 0.11454039 0.11194934 0.11195573 0.11454716 0.11887802 0.12321357 0.12501293 0.12152899 0.11081442 0.093254618 0.071894206 0.049968317][0.087023877 0.088230833 0.085005715 0.081040934 0.078233175 0.077578455 0.079136148 0.082470879 0.086043544 0.087958552 0.085877508 0.078288823 0.06559591 0.050016329 0.034136433][0.056640014 0.056880828 0.054517116 0.051628672 0.049435668 0.048643708 0.049498171 0.051837593 0.0544634 0.056094695 0.054995816 0.050231807 0.041997463 0.031731084 0.021238696][0.030817909 0.030470869 0.028955668 0.02727209 0.026093539 0.025653033 0.026246773 0.027800275 0.029581053 0.030771265 0.030267173 0.027652258 0.022956757 0.0170759 0.011049826][0.013266074 0.012714588 0.011646393 0.010679848 0.010116098 0.010043007 0.010629517 0.011698464 0.012882537 0.013651358 0.013465947 0.01220854 0.0098708887 0.0069772275 0.0040534856]]...]
INFO - root - 2017-12-09 09:51:46.555350: step 11810, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.835 sec/batch; 74h:22m:10s remains)
INFO - root - 2017-12-09 09:51:55.141812: step 11820, loss = 0.81, batch loss = 0.68 (8.9 examples/sec; 0.900 sec/batch; 80h:11m:14s remains)
INFO - root - 2017-12-09 09:52:03.723324: step 11830, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.884 sec/batch; 78h:42m:34s remains)
INFO - root - 2017-12-09 09:52:12.551703: step 11840, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.848 sec/batch; 75h:34m:15s remains)
INFO - root - 2017-12-09 09:52:21.235005: step 11850, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.884 sec/batch; 78h:45m:38s remains)
INFO - root - 2017-12-09 09:52:29.916277: step 11860, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.888 sec/batch; 79h:07m:38s remains)
INFO - root - 2017-12-09 09:52:38.649868: step 11870, loss = 0.82, batch loss = 0.69 (8.8 examples/sec; 0.912 sec/batch; 81h:16m:10s remains)
INFO - root - 2017-12-09 09:52:47.234070: step 11880, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.881 sec/batch; 78h:30m:10s remains)
INFO - root - 2017-12-09 09:52:55.980301: step 11890, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.895 sec/batch; 79h:41m:12s remains)
INFO - root - 2017-12-09 09:53:04.698930: step 11900, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 77h:21m:23s remains)
2017-12-09 09:53:05.597420: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0013268803 -0.0013256415 -0.0013252593 -0.0013251567 -0.0013250273 -0.0013249927 -0.0013246286 -0.0013237525 -0.0013223612 -0.0013216006 -0.00132207 -0.0013234557 -0.001324335 -0.001324465 -0.0013247166][-0.0013267548 -0.0013255575 -0.0013253362 -0.0013254591 -0.0013253036 -0.0013239103 -0.0013199581 -0.0013136307 -0.0013078039 -0.0013068133 -0.0013116725 -0.0013186715 -0.0013231426 -0.0013243709 -0.0013245344][-0.0013277764 -0.0013269278 -0.0013269167 -0.0013268486 -0.0013243393 -0.0013129223 -0.0012859432 -0.0012468953 -0.0012162765 -0.0012177761 -0.0012511753 -0.0012917551 -0.0013164711 -0.0013243374 -0.0013248253][-0.0013285636 -0.0013280211 -0.001327858 -0.0013250943 -0.0013099923 -0.0012625502 -0.0011665446 -0.0010392524 -0.00094591186 -0.0009543316 -0.0010622875 -0.0011956221 -0.0012846139 -0.0013190048 -0.0013249492][-0.0013296057 -0.0013290219 -0.0013267478 -0.0013116986 -0.0012569034 -0.0011229556 -0.00089258351 -0.00061950146 -0.00043685146 -0.00046506652 -0.00069471891 -0.0009868443 -0.0012012019 -0.0012984419 -0.001323691][-0.001329028 -0.0013254272 -0.0013146009 -0.0012730303 -0.0011518965 -0.000896271 -0.00050466537 -8.3748251e-05 0.00017433846 0.00011657877 -0.0002391045 -0.00070475641 -0.0010738594 -0.0012609218 -0.0013195076][-0.0013254505 -0.0013125876 -0.0012823623 -0.0012013406 -0.001007268 -0.00064065575 -0.00012323237 0.00039482233 0.000694955 0.00061352225 0.00017327664 -0.0004225552 -0.00092818134 -0.0012093954 -0.0013108265][-0.0013237052 -0.0013033703 -0.0012552175 -0.0011419096 -0.00090178 -0.00048284337 7.7215722e-05 0.00061585195 0.00092146732 0.00083266129 0.0003711083 -0.00027186214 -0.00084137591 -0.0011751062 -0.0013038007][-0.0013229088 -0.0013024438 -0.0012527647 -0.0011350095 -0.00089295633 -0.0004881114 3.2940297e-05 0.00052095472 0.0007932086 0.00070661609 0.00028017571 -0.00031856156 -0.00085610285 -0.0011765254 -0.0013030777][-0.0013270618 -0.0013138287 -0.0012781606 -0.0011860518 -0.000989632 -0.00065938715 -0.00023620843 0.00015555765 0.00036874926 0.00029097311 -5.774328e-05 -0.00053853466 -0.00096316135 -0.0012128962 -0.001309918][-0.0013310006 -0.0013261852 -0.0013101083 -0.0012596538 -0.0011389439 -0.00092417584 -0.00063929975 -0.00037056196 -0.00022499938 -0.00027889665 -0.00051497039 -0.00083200471 -0.0011035105 -0.0012590619 -0.00131752][-0.0013317412 -0.0013307694 -0.0013259988 -0.0013064387 -0.0012519612 -0.0011460333 -0.00099556241 -0.00084514846 -0.00075809739 -0.000781803 -0.00090792269 -0.0010761434 -0.0012159315 -0.0012938577 -0.0013222584][-0.0013302928 -0.0013302775 -0.0013294993 -0.001324372 -0.0013070933 -0.0012708142 -0.0012165804 -0.0011600954 -0.0011250398 -0.0011307899 -0.001175186 -0.0012356235 -0.0012857632 -0.001313987 -0.0013245669][-0.0013282839 -0.001328118 -0.0013281725 -0.0013275235 -0.0013234713 -0.0013143577 -0.0013000374 -0.0012844906 -0.001274034 -0.0012736829 -0.0012840085 -0.0012992721 -0.0013128776 -0.0013216337 -0.0013253605][-0.0013264593 -0.0013261295 -0.001326317 -0.001326508 -0.0013257538 -0.0013236657 -0.0013203056 -0.0013163923 -0.0013133661 -0.001312086 -0.0013133567 -0.0013165522 -0.0013203571 -0.0013236218 -0.0013252662]]...]
INFO - root - 2017-12-09 09:53:14.168756: step 11910, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 77h:33m:16s remains)
INFO - root - 2017-12-09 09:53:22.957084: step 11920, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.885 sec/batch; 78h:47m:24s remains)
INFO - root - 2017-12-09 09:53:31.755263: step 11930, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.874 sec/batch; 77h:48m:53s remains)
INFO - root - 2017-12-09 09:53:40.398141: step 11940, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.878 sec/batch; 78h:09m:38s remains)
INFO - root - 2017-12-09 09:53:49.177508: step 11950, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 77h:11m:40s remains)
INFO - root - 2017-12-09 09:53:57.921288: step 11960, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.876 sec/batch; 78h:02m:00s remains)
INFO - root - 2017-12-09 09:54:06.622765: step 11970, loss = 0.82, batch loss = 0.69 (9.7 examples/sec; 0.825 sec/batch; 73h:26m:01s remains)
INFO - root - 2017-12-09 09:54:15.170320: step 11980, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 77h:35m:06s remains)
INFO - root - 2017-12-09 09:54:23.592700: step 11990, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.881 sec/batch; 78h:27m:04s remains)
INFO - root - 2017-12-09 09:54:32.163022: step 12000, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.857 sec/batch; 76h:19m:52s remains)
2017-12-09 09:54:33.031514: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0012902732 -0.0012878015 -0.0012873724 -0.0012874815 -0.0012875713 -0.0012875502 -0.0012872966 -0.0012869714 -0.0012867313 -0.0012867787 -0.0012870161 -0.0012871623 -0.001287222 -0.0012872682 -0.0012873475][-0.0012887031 -0.001285977 -0.0012854709 -0.0012855997 -0.0012857043 -0.0012856638 -0.0012853001 -0.0012847189 -0.0012843842 -0.0012845348 -0.0012849641 -0.0012852283 -0.0012853113 -0.001285392 -0.0012856781][-0.001288834 -0.0012860189 -0.0012854995 -0.001285637 -0.001285661 -0.0012853623 -0.0012846295 -0.001283758 -0.0012833469 -0.001283543 -0.0012840768 -0.0012843595 -0.0012844481 -0.0012846626 -0.0012852764][-0.0012895541 -0.001286766 -0.0012861693 -0.0012861997 -0.0012860443 -0.0012853416 -0.0012841598 -0.00128299 -0.0012824511 -0.0012826925 -0.0012832466 -0.0012835414 -0.0012836676 -0.0012841533 -0.0012852554][-0.0012919157 -0.0012893226 -0.0012886046 -0.0012883603 -0.001287648 -0.0012862299 -0.0012843056 -0.0012826439 -0.0012818145 -0.0012819351 -0.0012823622 -0.0012826284 -0.0012829774 -0.001283948 -0.0012857338][-0.0012962335 -0.0012942408 -0.0012935557 -0.0012928551 -0.0012913281 -0.0012889435 -0.0012860398 -0.0012835194 -0.001282127 -0.00128181 -0.0012819729 -0.0012822681 -0.0012829715 -0.0012845281 -0.0012869681][-0.0013020179 -0.0013006469 -0.0012998647 -0.0012985675 -0.0012961816 -0.0012926809 -0.0012886957 -0.0012851732 -0.0012830805 -0.0012823135 -0.001282272 -0.0012826927 -0.001283818 -0.0012859711 -0.0012888788][-0.0013077902 -0.0013068768 -0.0013060647 -0.0013042352 -0.0013011092 -0.001296659 -0.0012916303 -0.0012870131 -0.0012840418 -0.0012828492 -0.0012826897 -0.0012833744 -0.0012849328 -0.0012874923 -0.0012906198][-0.0013126576 -0.0013120823 -0.0013112123 -0.0013090356 -0.0013052154 -0.0012999937 -0.0012940482 -0.0012884605 -0.0012846921 -0.0012831219 -0.001283029 -0.0012840896 -0.0012859787 -0.0012886559 -0.0012917416][-0.0013162808 -0.0013159863 -0.0013149526 -0.0013124795 -0.0013081622 -0.0013024 -0.0012957689 -0.0012895371 -0.0012852714 -0.0012833806 -0.0012833965 -0.0012848086 -0.0012870568 -0.0012897403 -0.0012925183][-0.0013177212 -0.0013174994 -0.0013163863 -0.0013137861 -0.0013093415 -0.0013036483 -0.0012969158 -0.0012903274 -0.0012857741 -0.0012838694 -0.0012839872 -0.0012856042 -0.0012880468 -0.0012906841 -0.0012930884][-0.0013166947 -0.0013164728 -0.0013154292 -0.0013130427 -0.0013088107 -0.0013034443 -0.0012971337 -0.0012905719 -0.0012859065 -0.0012841199 -0.0012843689 -0.0012861702 -0.0012887157 -0.0012912941 -0.0012935401][-0.0013137446 -0.0013135374 -0.0013127369 -0.0013108987 -0.0013071478 -0.0013023457 -0.0012966382 -0.0012904144 -0.0012858236 -0.0012841057 -0.0012844622 -0.0012863593 -0.0012889048 -0.0012914159 -0.0012937549][-0.0013097499 -0.0013095636 -0.0013090825 -0.0013078015 -0.0013044055 -0.0013002531 -0.0012954811 -0.0012901435 -0.001285969 -0.00128434 -0.0012847818 -0.001286634 -0.0012890752 -0.0012915242 -0.0012939778][-0.0013055333 -0.0013053358 -0.0013050218 -0.0013039934 -0.0013012012 -0.0012978041 -0.0012939431 -0.0012896173 -0.0012862637 -0.0012849167 -0.0012853255 -0.0012869711 -0.0012891119 -0.0012913538 -0.0012937587]]...]
INFO - root - 2017-12-09 09:54:41.573484: step 12010, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.861 sec/batch; 76h:41m:12s remains)
INFO - root - 2017-12-09 09:54:50.255798: step 12020, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.882 sec/batch; 78h:32m:12s remains)
INFO - root - 2017-12-09 09:54:58.879426: step 12030, loss = 0.81, batch loss = 0.68 (9.2 examples/sec; 0.872 sec/batch; 77h:36m:25s remains)
INFO - root - 2017-12-09 09:55:07.580971: step 12040, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.893 sec/batch; 79h:28m:15s remains)
INFO - root - 2017-12-09 09:55:16.467784: step 12050, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.892 sec/batch; 79h:25m:14s remains)
INFO - root - 2017-12-09 09:55:25.246176: step 12060, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.896 sec/batch; 79h:45m:47s remains)
INFO - root - 2017-12-09 09:55:33.930436: step 12070, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.852 sec/batch; 75h:48m:07s remains)
INFO - root - 2017-12-09 09:55:42.485750: step 12080, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.842 sec/batch; 74h:57m:36s remains)
INFO - root - 2017-12-09 09:55:50.833549: step 12090, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.843 sec/batch; 75h:02m:48s remains)
INFO - root - 2017-12-09 09:55:59.468951: step 12100, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.853 sec/batch; 75h:56m:40s remains)
2017-12-09 09:56:00.333227: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.00131776 -0.0013197819 -0.0013229799 -0.001325891 -0.0013276134 -0.0013270674 -0.0013251001 -0.0013225564 -0.00131951 -0.0013162618 -0.0013135141 -0.0013113794 -0.001309715 -0.001308679 -0.0013081862][-0.0013178324 -0.0013201924 -0.0013237726 -0.0013269648 -0.0013289677 -0.0013283765 -0.0013264555 -0.0013239391 -0.0013209312 -0.001317762 -0.0013149627 -0.0013127871 -0.0013108905 -0.0013095225 -0.0013086459][-0.0013180925 -0.0013207619 -0.0013245575 -0.0013281466 -0.0013307246 -0.0013305335 -0.0013288073 -0.0013262923 -0.0013234514 -0.0013203805 -0.0013172737 -0.0013147492 -0.0013124821 -0.0013108766 -0.0013097143][-0.0013180636 -0.0013208429 -0.0013245344 -0.0013281804 -0.0013312245 -0.0013317941 -0.0013306384 -0.001328398 -0.0013258305 -0.0013227789 -0.0013193858 -0.0013163434 -0.0013136824 -0.0013118342 -0.0013106361][-0.0013175647 -0.0013197901 -0.0013230467 -0.0013263364 -0.0013295228 -0.00133069 -0.0013302688 -0.0013287227 -0.0013265566 -0.0013236558 -0.0013201241 -0.001316843 -0.0013140178 -0.0013120701 -0.0013109175][-0.001316143 -0.0013178078 -0.0013204759 -0.0013232607 -0.0013260833 -0.0013274918 -0.0013275095 -0.0013265018 -0.001324954 -0.0013226076 -0.0013195177 -0.0013164707 -0.0013138119 -0.0013118688 -0.0013107215][-0.0013142464 -0.0013153051 -0.0013174732 -0.0013197338 -0.0013218399 -0.0013230128 -0.0013230368 -0.0013223469 -0.0013213733 -0.0013197405 -0.0013174604 -0.0013150908 -0.001313039 -0.0013114793 -0.0013104682][-0.0013125489 -0.0013128705 -0.0013144427 -0.0013161902 -0.0013176242 -0.0013184545 -0.0013185086 -0.0013181431 -0.0013174714 -0.0013164354 -0.0013150034 -0.0013134863 -0.0013121316 -0.0013110068 -0.0013102829][-0.0013113408 -0.0013108862 -0.00131186 -0.0013129947 -0.0013137837 -0.0013142943 -0.0013144524 -0.0013145569 -0.001314379 -0.0013138539 -0.0013130128 -0.0013121882 -0.0013114646 -0.0013108004 -0.0013104051][-0.0013105707 -0.0013094723 -0.0013097649 -0.0013101996 -0.0013104664 -0.0013105791 -0.0013106455 -0.0013110331 -0.0013114728 -0.0013117206 -0.0013117464 -0.001311645 -0.0013113904 -0.0013110929 -0.0013109491][-0.001310582 -0.0013090551 -0.0013087826 -0.0013086438 -0.0013084892 -0.0013083272 -0.0013082629 -0.0013086584 -0.0013094041 -0.0013102135 -0.0013110281 -0.0013116272 -0.0013118653 -0.0013118654 -0.0013118825][-0.0013110915 -0.0013095627 -0.0013090124 -0.0013086263 -0.0013082492 -0.0013079159 -0.0013077375 -0.0013079188 -0.0013084789 -0.0013092632 -0.0013103753 -0.0013114826 -0.0013122675 -0.0013126404 -0.0013127837][-0.0013117907 -0.0013103416 -0.0013097094 -0.0013092766 -0.0013088497 -0.0013084825 -0.0013081931 -0.0013081962 -0.0013084986 -0.0013090679 -0.0013100507 -0.0013112266 -0.0013123212 -0.0013130794 -0.0013134896][-0.0013126668 -0.0013111759 -0.0013104451 -0.001309999 -0.0013096263 -0.0013092859 -0.0013089364 -0.0013087568 -0.0013088722 -0.0013093285 -0.0013101498 -0.0013111866 -0.0013122354 -0.0013131275 -0.0013137565][-0.001313592 -0.0013119772 -0.0013110206 -0.0013105449 -0.0013101847 -0.0013098996 -0.0013095731 -0.0013093075 -0.0013093427 -0.0013097593 -0.0013104882 -0.0013113433 -0.0013122158 -0.0013130242 -0.0013136831]]...]
INFO - root - 2017-12-09 09:56:08.718396: step 12110, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.859 sec/batch; 76h:27m:33s remains)
INFO - root - 2017-12-09 09:56:17.361802: step 12120, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.888 sec/batch; 79h:01m:11s remains)
INFO - root - 2017-12-09 09:56:26.049983: step 12130, loss = 0.83, batch loss = 0.70 (9.4 examples/sec; 0.850 sec/batch; 75h:37m:29s remains)
INFO - root - 2017-12-09 09:56:34.777629: step 12140, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.895 sec/batch; 79h:37m:42s remains)
INFO - root - 2017-12-09 09:56:43.515480: step 12150, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.885 sec/batch; 78h:47m:38s remains)
INFO - root - 2017-12-09 09:56:52.292412: step 12160, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.888 sec/batch; 78h:59m:53s remains)
INFO - root - 2017-12-09 09:57:01.021455: step 12170, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.876 sec/batch; 77h:58m:35s remains)
INFO - root - 2017-12-09 09:57:09.768285: step 12180, loss = 0.81, batch loss = 0.68 (8.7 examples/sec; 0.925 sec/batch; 82h:17m:21s remains)
INFO - root - 2017-12-09 09:57:18.443904: step 12190, loss = 0.82, batch loss = 0.69 (8.8 examples/sec; 0.906 sec/batch; 80h:38m:35s remains)
INFO - root - 2017-12-09 09:57:27.203800: step 12200, loss = 0.82, batch loss = 0.69 (9.7 examples/sec; 0.823 sec/batch; 73h:14m:22s remains)
2017-12-09 09:57:28.047700: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0013220084 -0.0013210922 -0.0013208429 -0.0013207643 -0.0013207978 -0.001320887 -0.0013209236 -0.0013207743 -0.0013205325 -0.0013202692 -0.0013200525 -0.0013198797 -0.0013197707 -0.0013197154 -0.0013197123][-0.0013219939 -0.0013210699 -0.0013208569 -0.0013207864 -0.0013208693 -0.0013209919 -0.0013209978 -0.0013207746 -0.0013204487 -0.0013201305 -0.0013198609 -0.0013196501 -0.0013195345 -0.0013194967 -0.001319495][-0.001322087 -0.001321465 -0.0013215134 -0.0013216226 -0.0013218275 -0.0013220019 -0.0013219535 -0.0013215939 -0.0013211089 -0.0013206525 -0.0013202741 -0.0013199893 -0.0013198467 -0.0013198166 -0.0013198399][-0.0013221737 -0.0013219649 -0.0013224137 -0.001322947 -0.0013234715 -0.0013238 -0.0013237189 -0.0013231637 -0.0013224171 -0.0013216981 -0.0013210828 -0.0013206318 -0.0013204139 -0.0013203697 -0.0013204151][-0.0013227831 -0.001322974 -0.0013239663 -0.0013251604 -0.0013263049 -0.0013270222 -0.001327033 -0.0013262831 -0.0013250635 -0.0013237848 -0.0013226485 -0.0013218381 -0.0013213751 -0.0013212179 -0.0013212523][-0.0013233968 -0.001324192 -0.0013258804 -0.0013277639 -0.0013295944 -0.0013308274 -0.0013310364 -0.0013301611 -0.0013284986 -0.0013266215 -0.0013249253 -0.0013237058 -0.001322919 -0.0013225506 -0.0013224939][-0.0013241573 -0.0013255095 -0.0013278351 -0.0013303004 -0.0013328387 -0.0013347111 -0.0013351594 -0.0013342022 -0.0013321833 -0.0013298361 -0.0013275731 -0.0013258271 -0.0013246731 -0.0013240637 -0.001323908][-0.0013253946 -0.0013272688 -0.001330192 -0.0013331753 -0.0013362879 -0.0013386753 -0.0013393248 -0.0013382785 -0.0013359805 -0.0013333163 -0.0013306403 -0.0013283755 -0.0013268116 -0.0013259387 -0.0013256519][-0.0013266471 -0.0013289533 -0.0013323799 -0.0013357629 -0.0013392435 -0.0013420072 -0.001342876 -0.00134183 -0.0013393797 -0.0013365331 -0.0013335338 -0.0013308866 -0.0013289711 -0.0013278784 -0.0013274819][-0.0013269194 -0.0013295193 -0.0013332886 -0.0013370003 -0.0013407362 -0.0013437256 -0.0013448409 -0.0013439577 -0.0013415988 -0.0013387015 -0.0013355564 -0.0013327277 -0.0013306581 -0.0013294489 -0.0013289719][-0.0013263426 -0.0013287724 -0.0013323454 -0.0013359701 -0.0013396295 -0.0013426696 -0.001343971 -0.0013433311 -0.0013413638 -0.001338843 -0.0013359461 -0.0013332063 -0.0013312973 -0.001330249 -0.001329865][-0.0013252919 -0.0013271844 -0.0013301225 -0.0013332642 -0.001336547 -0.0013393981 -0.0013407423 -0.0013403002 -0.0013387584 -0.0013368306 -0.001334501 -0.001332207 -0.0013307527 -0.0013301374 -0.0013300673][-0.0013237721 -0.0013251086 -0.001327283 -0.0013297379 -0.0013323537 -0.0013347475 -0.0013360142 -0.0013358295 -0.0013346947 -0.0013332469 -0.0013315264 -0.0013298696 -0.0013289552 -0.0013287861 -0.0013290104][-0.0013220622 -0.0013227924 -0.0013241261 -0.0013258256 -0.0013276862 -0.0013294695 -0.0013305701 -0.0013306766 -0.0013300098 -0.0013289963 -0.0013278229 -0.0013267325 -0.0013262229 -0.0013263144 -0.0013267036][-0.0013207201 -0.0013208961 -0.0013214874 -0.0013225282 -0.0013237012 -0.001324796 -0.0013255277 -0.0013256844 -0.0013253869 -0.00132477 -0.0013239987 -0.0013233444 -0.001323157 -0.0013234067 -0.0013238668]]...]
INFO - root - 2017-12-09 09:57:36.647894: step 12210, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.888 sec/batch; 78h:58m:29s remains)
INFO - root - 2017-12-09 09:57:45.338872: step 12220, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.863 sec/batch; 76h:49m:14s remains)
INFO - root - 2017-12-09 09:57:54.013821: step 12230, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.886 sec/batch; 78h:49m:10s remains)
INFO - root - 2017-12-09 09:58:02.764529: step 12240, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.865 sec/batch; 76h:57m:50s remains)
INFO - root - 2017-12-09 09:58:11.542588: step 12250, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.851 sec/batch; 75h:41m:15s remains)
INFO - root - 2017-12-09 09:58:20.221594: step 12260, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.873 sec/batch; 77h:39m:05s remains)
INFO - root - 2017-12-09 09:58:28.956272: step 12270, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.889 sec/batch; 79h:04m:00s remains)
INFO - root - 2017-12-09 09:58:37.420283: step 12280, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.860 sec/batch; 76h:32m:03s remains)
INFO - root - 2017-12-09 09:58:45.680117: step 12290, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.848 sec/batch; 75h:26m:31s remains)
INFO - root - 2017-12-09 09:58:54.277470: step 12300, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.842 sec/batch; 74h:53m:52s remains)
2017-12-09 09:58:55.131171: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.010396329 0.016390096 0.021720583 0.024954086 0.025257943 0.022639453 0.018034752 0.012656459 0.0075298138 0.0034208482 0.00071264827 -0.00068649766 -0.0011858506 -0.0012903637 -0.0013038869][0.014882013 0.02320523 0.030771919 0.035706215 0.036891147 0.034106214 0.028265191 0.020793766 0.01311523 0.0065409904 0.0020418831 -0.00031316944 -0.0011330862 -0.0012909636 -0.001303616][0.018795392 0.029093824 0.03869044 0.045466378 0.048057176 0.046045735 0.039993271 0.031165164 0.021055855 0.011562181 0.00451802 0.000543274 -0.00095083332 -0.0012844267 -0.0013044843][0.02106273 0.032913893 0.044522505 0.053558115 0.058386896 0.058048349 0.052569959 0.042779673 0.030312508 0.017765375 0.0078524807 0.0018752147 -0.00061890692 -0.0012539166 -0.0013010434][0.020791057 0.033673298 0.047457419 0.059649196 0.067999944 0.070559107 0.066411436 0.055787165 0.040668085 0.024717076 0.01165259 0.0034916538 -0.00014905259 -0.0011897916 -0.0012931385][0.019255044 0.032768756 0.048506 0.063806869 0.075651675 0.081085749 0.078340292 0.067232512 0.050139561 0.031450585 0.015624227 0.0053380346 0.00044107158 -0.0010773572 -0.001276269][0.017661976 0.031339373 0.048266184 0.065568157 0.079552844 0.086588696 0.084620856 0.073417008 0.055580713 0.03576687 0.018604338 0.0070046764 0.0010934498 -0.00092300889 -0.0012505446][0.016385822 0.029634718 0.046512268 0.064084716 0.078426875 0.085822791 0.084300794 0.073814847 0.05681897 0.037578128 0.020446597 0.0083500883 0.0017373078 -0.00073125737 -0.0012154321][0.015332745 0.027636277 0.043396473 0.059743684 0.073037639 0.080012009 0.079044446 0.070139319 0.055269632 0.037834685 0.021542653 0.0093584144 0.0022531911 -0.00056074612 -0.0011785517][0.013494361 0.024126286 0.037775729 0.051955014 0.063622922 0.070109285 0.070054196 0.063336879 0.051251475 0.036256388 0.021409322 0.0096588312 0.0024684146 -0.00048182957 -0.0011547384][0.010360403 0.018602857 0.029322987 0.040709764 0.050524514 0.056631364 0.057609744 0.053075533 0.043782596 0.031564347 0.018881612 0.0085081384 0.0020648635 -0.00057767861 -0.0011516551][0.0062997537 0.011786426 0.019215625 0.027566984 0.035345614 0.040866807 0.042640772 0.03997967 0.033242546 0.023903092 0.014001957 0.0059285788 0.0010437428 -0.0008295335 -0.0011333637][0.0026149803 0.0057053333 0.01016523 0.015580818 0.021166081 0.025612969 0.027547723 0.026183343 0.021671169 0.015177028 0.0082984455 0.002916408 -8.6213e-05 -0.0010391288 -0.0010124336][9.27666e-05 0.0015072506 0.003773981 0.0067903949 0.010265194 0.013349202 0.015001789 0.014515581 0.011925306 0.0080249952 0.003902728 0.00082791108 -0.00072773022 -0.0010453206 -0.00074070378][-0.0010399733 -0.00060023018 0.0002580462 0.0015872328 0.0032999937 0.0049482454 0.0059728026 0.0059173722 0.004751876 0.0028936004 0.00094241439 -0.00044130109 -0.0010474481 -0.00096052547 -0.00040486926]]...]
INFO - root - 2017-12-09 09:59:03.672303: step 12310, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.883 sec/batch; 78h:31m:45s remains)
INFO - root - 2017-12-09 09:59:12.307503: step 12320, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.840 sec/batch; 74h:44m:04s remains)
INFO - root - 2017-12-09 09:59:20.800491: step 12330, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.849 sec/batch; 75h:32m:25s remains)
INFO - root - 2017-12-09 09:59:29.454294: step 12340, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.883 sec/batch; 78h:30m:58s remains)
INFO - root - 2017-12-09 09:59:38.175323: step 12350, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.900 sec/batch; 80h:01m:35s remains)
INFO - root - 2017-12-09 09:59:46.928809: step 12360, loss = 0.82, batch loss = 0.69 (8.8 examples/sec; 0.913 sec/batch; 81h:13m:19s remains)
INFO - root - 2017-12-09 09:59:55.733513: step 12370, loss = 0.81, batch loss = 0.68 (7.9 examples/sec; 1.011 sec/batch; 89h:54m:13s remains)
INFO - root - 2017-12-09 10:00:04.372338: step 12380, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 77h:12m:07s remains)
INFO - root - 2017-12-09 10:00:12.862863: step 12390, loss = 0.83, batch loss = 0.70 (9.2 examples/sec; 0.867 sec/batch; 77h:08m:13s remains)
INFO - root - 2017-12-09 10:00:21.337862: step 12400, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.853 sec/batch; 75h:52m:10s remains)
2017-12-09 10:00:22.228357: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0013361506 -0.001335131 -0.0013347513 -0.0013344556 -0.0013342956 -0.0013344664 -0.0013350048 -0.0013354446 -0.0013359075 -0.0013372357 -0.0013394092 -0.0013416915 -0.0013443558 -0.0013468947 -0.0013482592][-0.001336913 -0.0013357163 -0.0013351014 -0.0013345085 -0.0013340417 -0.0013339272 -0.0013341361 -0.0013342211 -0.001334417 -0.0013357599 -0.0013383348 -0.001341208 -0.0013446059 -0.0013478968 -0.0013496879][-0.0013384911 -0.0013371622 -0.0013363395 -0.0013354266 -0.0013346514 -0.0013341865 -0.001333994 -0.0013337635 -0.001333798 -0.0013352219 -0.00133814 -0.0013414625 -0.0013453204 -0.0013490749 -0.0013511796][-0.0013408369 -0.001339453 -0.0013383796 -0.00133716 -0.0013361152 -0.0013353078 -0.0013346804 -0.0013340774 -0.0013339646 -0.0013354535 -0.0013385853 -0.0013421677 -0.0013462527 -0.0013501537 -0.0013522824][-0.0013436165 -0.0013423553 -0.0013412278 -0.0013399197 -0.0013387253 -0.001337592 -0.0013364747 -0.0013353721 -0.0013348943 -0.0013361892 -0.0013391012 -0.0013425418 -0.001346383 -0.0013498727 -0.0013516789][-0.0013456858 -0.0013449028 -0.0013441236 -0.0013430755 -0.0013418988 -0.0013404874 -0.0013388573 -0.0013371172 -0.0013360163 -0.0013368373 -0.0013393298 -0.0013423674 -0.0013455679 -0.0013482093 -0.0013494147][-0.0013465357 -0.0013463679 -0.0013461756 -0.0013456665 -0.0013446846 -0.0013431266 -0.0013411556 -0.0013389181 -0.0013372725 -0.001337476 -0.001339267 -0.0013416026 -0.0013439045 -0.0013455283 -0.0013460222][-0.0013462288 -0.0013467938 -0.001347373 -0.0013474815 -0.0013467271 -0.0013450895 -0.0013428039 -0.001340135 -0.0013378962 -0.001337382 -0.0013383641 -0.0013399146 -0.0013413783 -0.0013421653 -0.0013420827][-0.0013447007 -0.001346168 -0.0013476341 -0.001348403 -0.0013479602 -0.0013462583 -0.0013437654 -0.0013406854 -0.0013378592 -0.0013366229 -0.001336706 -0.0013374407 -0.0013381743 -0.0013384337 -0.001338003][-0.0013419351 -0.0013440135 -0.0013461935 -0.001347527 -0.001347415 -0.0013458253 -0.00134332 -0.0013402381 -0.0013372415 -0.0013355098 -0.0013348794 -0.0013348643 -0.0013349829 -0.0013347683 -0.0013341316][-0.0013385746 -0.001340696 -0.0013430186 -0.0013444729 -0.0013445187 -0.0013431557 -0.0013409397 -0.0013383023 -0.0013356821 -0.0013338565 -0.0013328809 -0.0013324278 -0.0013322099 -0.0013317856 -0.0013311799][-0.0013354402 -0.0013370644 -0.0013390881 -0.0013403848 -0.0013404539 -0.0013393172 -0.0013375112 -0.0013355131 -0.0013335134 -0.0013319466 -0.001330964 -0.0013304281 -0.0013300995 -0.0013296821 -0.0013292541][-0.0013329202 -0.0013338585 -0.0013353145 -0.001336196 -0.0013361893 -0.0013353712 -0.0013340812 -0.0013326801 -0.0013313012 -0.0013301926 -0.0013294625 -0.0013290627 -0.0013288057 -0.0013285358 -0.0013283308][-0.0013311364 -0.0013312943 -0.0013321119 -0.0013326434 -0.0013326899 -0.0013322422 -0.0013314543 -0.0013305676 -0.0013297977 -0.0013291731 -0.0013287531 -0.0013285512 -0.0013284051 -0.001328256 -0.0013281527][-0.0013300503 -0.0013296318 -0.0013299567 -0.0013302416 -0.0013303197 -0.0013301562 -0.001329766 -0.0013293133 -0.0013289349 -0.0013286134 -0.0013284098 -0.0013283339 -0.0013282823 -0.0013282137 -0.0013281684]]...]
INFO - root - 2017-12-09 10:00:30.675382: step 12410, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.841 sec/batch; 74h:44m:02s remains)
INFO - root - 2017-12-09 10:00:39.303585: step 12420, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.880 sec/batch; 78h:16m:19s remains)
INFO - root - 2017-12-09 10:00:47.989897: step 12430, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.881 sec/batch; 78h:20m:39s remains)
INFO - root - 2017-12-09 10:00:56.756894: step 12440, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.877 sec/batch; 77h:59m:01s remains)
INFO - root - 2017-12-09 10:01:05.284988: step 12450, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.858 sec/batch; 76h:18m:22s remains)
INFO - root - 2017-12-09 10:01:13.833100: step 12460, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 77h:15m:08s remains)
INFO - root - 2017-12-09 10:01:22.271351: step 12470, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.858 sec/batch; 76h:15m:38s remains)
INFO - root - 2017-12-09 10:01:30.760451: step 12480, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.881 sec/batch; 78h:16m:46s remains)
INFO - root - 2017-12-09 10:01:38.970609: step 12490, loss = 0.82, batch loss = 0.69 (9.7 examples/sec; 0.827 sec/batch; 73h:32m:16s remains)
INFO - root - 2017-12-09 10:01:47.627509: step 12500, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.848 sec/batch; 75h:22m:17s remains)
2017-12-09 10:01:48.560881: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0013236975 -0.0013225417 -0.0013228342 -0.0013230788 -0.0013227799 -0.0013217656 -0.0013204033 -0.0013194283 -0.0013191327 -0.00131953 -0.0013206815 -0.0013222754 -0.0013237665 -0.0013247892 -0.001325204][-0.001322248 -0.0013210906 -0.0013214271 -0.0013217896 -0.0013217052 -0.0013210184 -0.0013198841 -0.0013189139 -0.0013185244 -0.0013187237 -0.0013196236 -0.0013210974 -0.0013226224 -0.0013237423 -0.0013242543][-0.0013221717 -0.001321045 -0.0013212451 -0.0013215836 -0.0013216407 -0.0013212347 -0.0013202672 -0.0013193177 -0.0013187851 -0.0013187501 -0.001319334 -0.0013206356 -0.0013222018 -0.00132346 -0.0013240837][-0.0013225767 -0.0013213649 -0.0013212569 -0.0013214017 -0.0013214296 -0.0013211819 -0.0013203865 -0.0013194933 -0.0013189462 -0.0013188003 -0.0013191961 -0.0013203828 -0.0013219481 -0.0013232302 -0.0013239312][-0.0013231663 -0.0013218292 -0.0013214467 -0.0013213743 -0.001321347 -0.0013212153 -0.0013205895 -0.0013197627 -0.0013191989 -0.0013190112 -0.0013192603 -0.0013202961 -0.0013217737 -0.0013230281 -0.0013237637][-0.0013231637 -0.0013218691 -0.0013214999 -0.0013213758 -0.0013213406 -0.001321311 -0.0013208578 -0.0013201317 -0.0013195651 -0.0013193191 -0.0013193816 -0.0013201828 -0.0013215048 -0.0013227175 -0.0013234686][-0.0013227083 -0.0013215186 -0.0013212974 -0.0013212459 -0.0013212411 -0.001321309 -0.0013210643 -0.0013204846 -0.0013198702 -0.0013195201 -0.0013194196 -0.0013199686 -0.0013210503 -0.0013221352 -0.001322866][-0.0013221952 -0.0013209898 -0.0013209177 -0.0013210139 -0.0013210828 -0.0013212073 -0.0013211113 -0.001320695 -0.0013201904 -0.0013199231 -0.0013198106 -0.0013201159 -0.0013209073 -0.0013217286 -0.0013222569][-0.0013214446 -0.0013201864 -0.0013202548 -0.0013205019 -0.0013207463 -0.0013209406 -0.00132098 -0.0013207924 -0.0013205715 -0.0013205276 -0.0013204818 -0.0013206262 -0.0013210574 -0.0013215013 -0.0013216832][-0.0013208843 -0.001319643 -0.0013197946 -0.0013200891 -0.0013203634 -0.001320589 -0.001320739 -0.0013208091 -0.0013209202 -0.0013211808 -0.001321262 -0.0013212302 -0.0013212707 -0.0013212729 -0.0013210537][-0.0013208752 -0.0013195806 -0.0013196955 -0.001319908 -0.0013200656 -0.0013202023 -0.0013204203 -0.0013207052 -0.0013210797 -0.0013215558 -0.001321747 -0.0013215558 -0.0013212123 -0.0013207273 -0.0013201036][-0.0013212791 -0.0013198009 -0.0013198475 -0.0013199748 -0.0013200383 -0.0013200915 -0.001320316 -0.0013207271 -0.0013212613 -0.0013218108 -0.0013219794 -0.0013215836 -0.0013208515 -0.0013198908 -0.0013188367][-0.0013219791 -0.0013204141 -0.00132036 -0.001320433 -0.0013204454 -0.0013204549 -0.0013206449 -0.0013210202 -0.0013215221 -0.0013219605 -0.0013219252 -0.0013212601 -0.0013201106 -0.0013186771 -0.0013172398][-0.0013232868 -0.0013216138 -0.0013214705 -0.0013214846 -0.001321406 -0.0013212755 -0.0013212898 -0.0013214876 -0.0013217392 -0.0013218587 -0.0013214749 -0.0013204687 -0.0013189336 -0.0013171369 -0.0013154651][-0.0013247976 -0.0013230571 -0.0013227868 -0.0013227399 -0.0013225612 -0.0013223068 -0.0013221399 -0.0013221115 -0.0013220487 -0.0013217542 -0.0013209981 -0.001319662 -0.0013178256 -0.0013158036 -0.0013140406]]...]
INFO - root - 2017-12-09 10:01:57.097150: step 12510, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.853 sec/batch; 75h:51m:02s remains)
INFO - root - 2017-12-09 10:02:05.640078: step 12520, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.873 sec/batch; 77h:35m:13s remains)
INFO - root - 2017-12-09 10:02:14.204972: step 12530, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.835 sec/batch; 74h:11m:51s remains)
INFO - root - 2017-12-09 10:02:22.721796: step 12540, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.876 sec/batch; 77h:51m:07s remains)
INFO - root - 2017-12-09 10:02:31.398578: step 12550, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.835 sec/batch; 74h:13m:52s remains)
INFO - root - 2017-12-09 10:02:39.947591: step 12560, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.861 sec/batch; 76h:30m:50s remains)
INFO - root - 2017-12-09 10:02:48.490071: step 12570, loss = 0.82, batch loss = 0.69 (9.9 examples/sec; 0.809 sec/batch; 71h:55m:59s remains)
INFO - root - 2017-12-09 10:02:56.958222: step 12580, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.851 sec/batch; 75h:37m:24s remains)
INFO - root - 2017-12-09 10:03:05.279002: step 12590, loss = 0.82, batch loss = 0.69 (11.7 examples/sec; 0.686 sec/batch; 60h:56m:30s remains)
INFO - root - 2017-12-09 10:03:13.958274: step 12600, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.884 sec/batch; 78h:35m:24s remains)
2017-12-09 10:03:14.941451: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0013552125 -0.0013539358 -0.0013534997 -0.0013535211 -0.0013537072 -0.0013539628 -0.0013539735 -0.0013535703 -0.0013528174 -0.0013518335 -0.0013508501 -0.00134999 -0.0013494462 -0.0013493448 -0.00134948][-0.0013550943 -0.0013534292 -0.0013527581 -0.0013526919 -0.0013529267 -0.0013532944 -0.0013533962 -0.0013530347 -0.0013522424 -0.0013511973 -0.0013501404 -0.0013492241 -0.0013486666 -0.001348609 -0.0013488162][-0.0013547892 -0.0013529441 -0.0013521329 -0.0013520338 -0.0013523592 -0.0013528657 -0.0013530825 -0.0013527387 -0.0013519468 -0.0013509018 -0.0013498544 -0.0013489611 -0.00134847 -0.0013485088 -0.0013487755][-0.0013543232 -0.0013524212 -0.0013515475 -0.00135142 -0.001351763 -0.0013523198 -0.0013526011 -0.0013523141 -0.0013515907 -0.001350608 -0.0013496269 -0.0013488233 -0.0013484346 -0.0013485342 -0.0013488184][-0.0013539996 -0.0013521629 -0.0013512522 -0.0013510961 -0.0013513662 -0.0013518455 -0.0013520996 -0.0013518679 -0.0013512332 -0.0013503386 -0.0013494438 -0.0013487597 -0.0013484765 -0.0013486104 -0.0013488927][-0.0013539352 -0.0013522375 -0.0013513325 -0.0013510814 -0.0013512357 -0.0013515945 -0.0013517896 -0.0013515846 -0.0013510488 -0.0013502751 -0.0013494828 -0.0013488919 -0.0013486638 -0.0013487808 -0.0013490119][-0.0013541847 -0.0013527091 -0.001351826 -0.0013514616 -0.0013514401 -0.0013516201 -0.0013516812 -0.0013514821 -0.0013510286 -0.0013503605 -0.0013496762 -0.0013491785 -0.0013489726 -0.0013490086 -0.0013491421][-0.0013547308 -0.0013535677 -0.0013527494 -0.0013523309 -0.001352154 -0.001352102 -0.0013519839 -0.0013517214 -0.0013512759 -0.0013506453 -0.0013500006 -0.0013495418 -0.0013493135 -0.0013492457 -0.0013492731][-0.0013553507 -0.001354487 -0.0013538355 -0.0013534323 -0.0013531782 -0.0013529924 -0.0013527257 -0.0013523627 -0.0013518544 -0.0013511735 -0.0013504842 -0.0013499578 -0.0013496358 -0.001349457 -0.0013493779][-0.0013557018 -0.0013551759 -0.0013547566 -0.0013544447 -0.0013542143 -0.0013540173 -0.001353706 -0.0013532948 -0.0013527316 -0.0013519865 -0.0013512209 -0.0013505815 -0.0013501125 -0.0013497713 -0.0013495393][-0.0013559699 -0.0013557447 -0.0013555781 -0.0013553776 -0.0013552029 -0.0013550619 -0.0013548194 -0.0013544636 -0.001353911 -0.0013531384 -0.0013522953 -0.0013515262 -0.0013508599 -0.0013502734 -0.0013498124][-0.0013562059 -0.0013562022 -0.0013561944 -0.0013561049 -0.001355979 -0.0013558799 -0.0013556884 -0.0013554227 -0.0013549582 -0.0013542372 -0.0013533805 -0.0013525267 -0.0013516882 -0.0013508609 -0.0013501648][-0.0013562542 -0.0013564719 -0.0013565985 -0.0013566022 -0.0013565203 -0.0013564384 -0.0013562845 -0.0013560907 -0.0013557348 -0.0013551194 -0.0013543111 -0.0013534245 -0.0013524594 -0.0013514479 -0.0013505467][-0.001356125 -0.0013564792 -0.0013567243 -0.0013568325 -0.0013568285 -0.0013568071 -0.0013567158 -0.0013565907 -0.0013563394 -0.0013558398 -0.0013550813 -0.0013541816 -0.001353133 -0.0013519793 -0.0013508919][-0.0013560324 -0.001356413 -0.0013567135 -0.0013569063 -0.0013569645 -0.0013569901 -0.0013569662 -0.0013569263 -0.0013567834 -0.0013563803 -0.0013556657 -0.0013547644 -0.0013536613 -0.0013524066 -0.0013511772]]...]
INFO - root - 2017-12-09 10:03:23.385194: step 12610, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.887 sec/batch; 78h:49m:54s remains)
INFO - root - 2017-12-09 10:03:32.020516: step 12620, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.849 sec/batch; 75h:26m:01s remains)
INFO - root - 2017-12-09 10:03:40.717314: step 12630, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.854 sec/batch; 75h:51m:03s remains)
INFO - root - 2017-12-09 10:03:49.429939: step 12640, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.866 sec/batch; 76h:56m:40s remains)
INFO - root - 2017-12-09 10:03:57.956177: step 12650, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.846 sec/batch; 75h:08m:01s remains)
INFO - root - 2017-12-09 10:04:06.536819: step 12660, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.855 sec/batch; 75h:58m:51s remains)
INFO - root - 2017-12-09 10:04:15.154791: step 12670, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.879 sec/batch; 78h:05m:52s remains)
INFO - root - 2017-12-09 10:04:23.666680: step 12680, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.831 sec/batch; 73h:49m:58s remains)
INFO - root - 2017-12-09 10:04:32.032913: step 12690, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.844 sec/batch; 74h:59m:18s remains)
INFO - root - 2017-12-09 10:04:40.465203: step 12700, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.833 sec/batch; 73h:57m:38s remains)
2017-12-09 10:04:41.290246: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.13666348 0.16263498 0.18420795 0.20020618 0.20946771 0.21171246 0.20794177 0.1958735 0.17673708 0.15437874 0.13186248 0.1125533 0.099247769 0.096416213 0.10470521][0.13327193 0.15919477 0.18096577 0.19737639 0.20730509 0.21131831 0.20902453 0.19868965 0.18142556 0.16127896 0.14047824 0.12258482 0.11101013 0.1099459 0.11820664][0.12317436 0.14862689 0.17067598 0.18844432 0.19952931 0.2057239 0.20582224 0.19872443 0.18458094 0.16777052 0.15028653 0.13499963 0.12457104 0.12354045 0.13092774][0.11127961 0.13611376 0.15879135 0.17808144 0.19098523 0.19943327 0.20093711 0.19719318 0.1872227 0.17469877 0.16159518 0.14930069 0.14115676 0.14032947 0.14593375][0.099899352 0.12496302 0.14863615 0.16904604 0.18338934 0.19334003 0.19679676 0.1959345 0.19013941 0.18301095 0.17490835 0.1671679 0.16198511 0.16136947 0.16534781][0.092106521 0.1178293 0.14276768 0.16457361 0.17998929 0.1912501 0.1964087 0.198348 0.19688852 0.19547711 0.19212329 0.18818074 0.18555325 0.18559244 0.18809658][0.087419353 0.11392221 0.14017081 0.16318177 0.17956294 0.19163519 0.19759718 0.201685 0.20353687 0.20656137 0.20656958 0.20643006 0.20587006 0.20647407 0.20748554][0.088148907 0.11605334 0.14367405 0.16757953 0.1845271 0.19657598 0.20269918 0.20777459 0.21106584 0.21566352 0.21748948 0.21863025 0.21875434 0.21890716 0.21914974][0.091857769 0.12123144 0.14949109 0.17324521 0.19003783 0.20141 0.20708188 0.21174745 0.21527314 0.21996714 0.22208689 0.22381906 0.22429971 0.22434579 0.22400394][0.094197184 0.12375495 0.151377 0.17424038 0.18953755 0.19961859 0.20501916 0.20907278 0.21164064 0.21570475 0.217732 0.21971518 0.22016758 0.22021097 0.2200036][0.091942638 0.11976947 0.14532271 0.16612512 0.17990448 0.18921249 0.19375393 0.19735885 0.19947529 0.20250493 0.20428041 0.20561729 0.20609717 0.20617668 0.2059903][0.0843702 0.10888049 0.13104965 0.14871573 0.16060548 0.16882659 0.17303775 0.17646927 0.17801043 0.17994812 0.181149 0.18188424 0.182102 0.18203816 0.18211453][0.071286082 0.091452576 0.1095086 0.12371053 0.13336761 0.1402569 0.14399064 0.14676872 0.14767657 0.14866364 0.14940584 0.14962614 0.14958398 0.14952084 0.14999002][0.054035071 0.068996117 0.082232229 0.092570737 0.099661022 0.10495882 0.10800693 0.11018252 0.11081685 0.11156058 0.11220283 0.11263745 0.11293155 0.11326313 0.11402972][0.036483966 0.04634377 0.055111587 0.061900388 0.066576831 0.070274331 0.072471648 0.074136667 0.074657768 0.075387105 0.076197617 0.076768 0.077430129 0.078108683 0.0791018]]...]
INFO - root - 2017-12-09 10:04:49.846607: step 12710, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 76h:33m:25s remains)
INFO - root - 2017-12-09 10:04:58.304317: step 12720, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.841 sec/batch; 74h:42m:25s remains)
INFO - root - 2017-12-09 10:05:06.806455: step 12730, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.851 sec/batch; 75h:33m:35s remains)
INFO - root - 2017-12-09 10:05:15.363392: step 12740, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.833 sec/batch; 73h:59m:04s remains)
INFO - root - 2017-12-09 10:05:24.008667: step 12750, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.876 sec/batch; 77h:50m:02s remains)
INFO - root - 2017-12-09 10:05:32.707970: step 12760, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.865 sec/batch; 76h:47m:02s remains)
INFO - root - 2017-12-09 10:05:41.433659: step 12770, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.852 sec/batch; 75h:42m:13s remains)
INFO - root - 2017-12-09 10:05:50.228852: step 12780, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.886 sec/batch; 78h:40m:44s remains)
INFO - root - 2017-12-09 10:05:58.768302: step 12790, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.884 sec/batch; 78h:31m:12s remains)
INFO - root - 2017-12-09 10:06:07.314573: step 12800, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.865 sec/batch; 76h:46m:20s remains)
2017-12-09 10:06:08.242104: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0013695131 -0.0013689591 -0.001368976 -0.0013689747 -0.0013688882 -0.0013687224 -0.0013685181 -0.0013683317 -0.0013682755 -0.0013682981 -0.0013683308 -0.0013684047 -0.0013685693 -0.0013687044 -0.0013687373][-0.0013691385 -0.0013685486 -0.0013684802 -0.0013684027 -0.0013682528 -0.0013680141 -0.0013677679 -0.0013676003 -0.0013676411 -0.0013677606 -0.0013678775 -0.0013680225 -0.0013682622 -0.0013684301 -0.0013684918][-0.0013692933 -0.00136872 -0.0013685721 -0.0013683912 -0.0013681334 -0.0013677781 -0.0013674604 -0.0013672967 -0.0013674384 -0.001367701 -0.0013679453 -0.0013682035 -0.0013685395 -0.0013687279 -0.0013688018][-0.0013696606 -0.0013691239 -0.0013688976 -0.001368647 -0.0013682917 -0.0013678139 -0.0013673914 -0.0013672073 -0.0013674363 -0.0013677918 -0.0013681488 -0.0013685187 -0.0013689555 -0.0013691521 -0.0013691906][-0.0013701089 -0.0013695501 -0.0013692938 -0.0013690189 -0.0013686102 -0.0013680486 -0.0013675471 -0.0013673 -0.0013675453 -0.0013679675 -0.0013684016 -0.0013688465 -0.0013693335 -0.0013695207 -0.0013694983][-0.0013703537 -0.0013698169 -0.0013695278 -0.0013691992 -0.0013687111 -0.0013680833 -0.0013675231 -0.0013672417 -0.0013674868 -0.0013679616 -0.001368462 -0.001368962 -0.0013694755 -0.0013696543 -0.0013695973][-0.0013704975 -0.0013699298 -0.0013696348 -0.0013693066 -0.001368784 -0.001368116 -0.0013675115 -0.0013671783 -0.0013673679 -0.0013678387 -0.0013683663 -0.0013689004 -0.0013694208 -0.0013696003 -0.0013695396][-0.0013704264 -0.0013698596 -0.0013696261 -0.0013693969 -0.0013689549 -0.0013683531 -0.0013677668 -0.0013673663 -0.0013674186 -0.0013677559 -0.0013681952 -0.0013686888 -0.0013691668 -0.0013693406 -0.0013692691][-0.0013700569 -0.0013695001 -0.0013693853 -0.0013692722 -0.0013689526 -0.0013684951 -0.0013680012 -0.0013675936 -0.001367502 -0.001367654 -0.001367936 -0.0013683123 -0.0013686989 -0.0013688479 -0.0013687761][-0.0013695245 -0.0013689462 -0.0013688948 -0.0013688451 -0.0013686267 -0.0013683252 -0.0013679867 -0.0013676675 -0.0013675289 -0.0013675697 -0.0013677205 -0.0013679581 -0.0013682175 -0.0013683172 -0.0013682409][-0.0013689913 -0.0013684117 -0.0013683531 -0.0013683217 -0.001368174 -0.0013679875 -0.0013677852 -0.0013675995 -0.0013674961 -0.0013675051 -0.001367582 -0.0013677193 -0.0013678647 -0.0013679134 -0.0013678473][-0.0013687699 -0.0013681548 -0.0013680636 -0.0013680529 -0.0013679613 -0.0013678534 -0.00136774 -0.0013676446 -0.0013675843 -0.0013675883 -0.0013676271 -0.0013677 -0.0013677686 -0.0013677828 -0.0013677275][-0.0013687115 -0.0013681446 -0.0013680329 -0.0013680444 -0.0013680025 -0.0013679554 -0.001367906 -0.0013678614 -0.0013678271 -0.0013678168 -0.0013678243 -0.0013678645 -0.0013678983 -0.0013678973 -0.0013678451][-0.0013688462 -0.0013682904 -0.0013681602 -0.0013681835 -0.0013681771 -0.0013681719 -0.0013681659 -0.0013681517 -0.0013681337 -0.0013681132 -0.0013680983 -0.0013681069 -0.001368114 -0.0013680983 -0.0013680465][-0.0013690981 -0.0013684718 -0.0013682812 -0.0013682999 -0.0013683108 -0.0013683245 -0.0013683403 -0.0013683555 -0.0013683591 -0.0013683431 -0.0013683215 -0.0013683117 -0.0013682921 -0.0013682543 -0.0013682024]]...]
INFO - root - 2017-12-09 10:06:16.957735: step 12810, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.902 sec/batch; 80h:05m:29s remains)
INFO - root - 2017-12-09 10:06:25.747566: step 12820, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.854 sec/batch; 75h:51m:59s remains)
INFO - root - 2017-12-09 10:06:34.441450: step 12830, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.883 sec/batch; 78h:26m:24s remains)
INFO - root - 2017-12-09 10:06:43.178216: step 12840, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.863 sec/batch; 76h:38m:53s remains)
INFO - root - 2017-12-09 10:06:51.838682: step 12850, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.881 sec/batch; 78h:12m:29s remains)
INFO - root - 2017-12-09 10:07:00.495023: step 12860, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 77h:09m:43s remains)
INFO - root - 2017-12-09 10:07:09.264031: step 12870, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.849 sec/batch; 75h:21m:19s remains)
INFO - root - 2017-12-09 10:07:18.070266: step 12880, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.903 sec/batch; 80h:09m:39s remains)
INFO - root - 2017-12-09 10:07:26.502838: step 12890, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.866 sec/batch; 76h:50m:51s remains)
INFO - root - 2017-12-09 10:07:35.018957: step 12900, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.838 sec/batch; 74h:22m:32s remains)
2017-12-09 10:07:35.877159: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.00906701 0.0095692789 0.0099952007 0.01030219 0.010119162 0.010025746 0.010182837 0.010687869 0.011298891 0.011762713 0.012198701 0.011652612 0.011336627 0.010693831 0.010145239][0.012458081 0.013886255 0.015145962 0.016107172 0.016767841 0.017110612 0.017720826 0.018550392 0.019607525 0.020946883 0.021926265 0.021960743 0.021731012 0.020457882 0.0187965][0.01990246 0.022790436 0.025831342 0.028217152 0.029977784 0.030933559 0.032353643 0.034228113 0.036476113 0.038563579 0.039846007 0.039907709 0.039224152 0.037246287 0.034143057][0.031349011 0.036822144 0.041504622 0.045073766 0.047934972 0.049616568 0.051714446 0.054441497 0.057935897 0.061270028 0.063338041 0.063972361 0.063523605 0.060852882 0.056323789][0.04492043 0.052160278 0.057781376 0.062437281 0.065766841 0.067819133 0.070241876 0.073894106 0.078709409 0.083110668 0.086009227 0.087610625 0.088116594 0.086100347 0.081656806][0.056635443 0.065071896 0.070364118 0.074592583 0.077186294 0.079397738 0.082235754 0.086560383 0.091742367 0.096630231 0.10004043 0.10272206 0.10461853 0.10421027 0.10133938][0.062792949 0.071064606 0.075265452 0.078269817 0.079432905 0.081579372 0.084366411 0.089240737 0.094549425 0.0997153 0.10337374 0.10669537 0.10975107 0.1115005 0.11094377][0.062296249 0.069202311 0.071348034 0.072548486 0.072248928 0.074033916 0.076566681 0.082105353 0.087674804 0.093233764 0.097184367 0.10115556 0.10523127 0.10846741 0.10972838][0.056237739 0.061461993 0.061662227 0.060576804 0.058448818 0.059216786 0.061673455 0.067143373 0.0728317 0.079064265 0.083704814 0.088707633 0.093646817 0.098215528 0.10104058][0.046549615 0.049989674 0.048735008 0.045914456 0.042650729 0.041943211 0.042986989 0.0479417 0.053502917 0.060074288 0.065602295 0.071948864 0.077993236 0.083659552 0.087628268][0.034126535 0.036348194 0.034480177 0.031065822 0.027469818 0.025705479 0.025885694 0.029312864 0.033861987 0.040504668 0.046412013 0.053479582 0.060248207 0.066800289 0.071834385][0.020952728 0.022254836 0.02070028 0.017670874 0.014581305 0.012934392 0.012875984 0.015163206 0.018452674 0.023747463 0.029153224 0.036273759 0.042808659 0.049169578 0.054636143][0.0097968876 0.01010269 0.0088725723 0.0070842067 0.0055362177 0.0044899583 0.00436294 0.005798257 0.0079969084 0.011706836 0.015857305 0.021360312 0.026745424 0.032438785 0.037573829][0.0026267436 0.0026143631 0.0020015314 0.0011752932 0.00056136865 0.00027172605 0.00024782738 0.000998803 0.0022858891 0.0044007972 0.0070329895 0.010534022 0.014183208 0.018233232 0.022107992][-0.00059878221 -0.00067749253 -0.00083764823 -0.00098877249 -0.0010739175 -0.0011191277 -0.00118757 -0.0010064424 -0.00049136963 0.00051588239 0.0019019346 0.0036854525 0.0058766832 0.0082316762 0.010641993]]...]
INFO - root - 2017-12-09 10:07:44.463223: step 12910, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.878 sec/batch; 77h:58m:32s remains)
INFO - root - 2017-12-09 10:07:53.321501: step 12920, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.896 sec/batch; 79h:33m:07s remains)
INFO - root - 2017-12-09 10:08:02.107604: step 12930, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.872 sec/batch; 77h:23m:40s remains)
INFO - root - 2017-12-09 10:08:11.013081: step 12940, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.851 sec/batch; 75h:31m:08s remains)
INFO - root - 2017-12-09 10:08:19.664658: step 12950, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.879 sec/batch; 78h:00m:59s remains)
INFO - root - 2017-12-09 10:08:28.408169: step 12960, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.861 sec/batch; 76h:24m:45s remains)
INFO - root - 2017-12-09 10:08:37.218378: step 12970, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.891 sec/batch; 79h:03m:16s remains)
INFO - root - 2017-12-09 10:08:45.977984: step 12980, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.844 sec/batch; 74h:56m:05s remains)
INFO - root - 2017-12-09 10:08:54.639742: step 12990, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 76h:40m:53s remains)
INFO - root - 2017-12-09 10:09:03.258651: step 13000, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.877 sec/batch; 77h:50m:06s remains)
2017-12-09 10:09:04.060159: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0013971644 -0.001397297 -0.0013976297 -0.0013979404 -0.0013981931 -0.0013983941 -0.0013985307 -0.001398565 -0.001398516 -0.0013984467 -0.0013984668 -0.0013986133 -0.001398896 -0.0013992565 -0.0013995646][-0.0013970819 -0.0013972418 -0.0013976245 -0.0013980009 -0.0013983155 -0.0013985621 -0.0013987063 -0.0013987168 -0.0013986317 -0.0013985336 -0.0013985222 -0.0013986388 -0.0013988897 -0.0013992183 -0.001399488][-0.001397135 -0.0013973532 -0.0013977729 -0.0013981876 -0.001398541 -0.0013988218 -0.0013989841 -0.0013989942 -0.0013989098 -0.0013988129 -0.0013987842 -0.0013988465 -0.0013990083 -0.0013992438 -0.0013994372][-0.0013972264 -0.0013975137 -0.0013979614 -0.0013983898 -0.0013987467 -0.0013990274 -0.0013991902 -0.0013992303 -0.0013991995 -0.0013991575 -0.0013991382 -0.0013991477 -0.0013991909 -0.0013992778 -0.0013993481][-0.0013975124 -0.0013978489 -0.0013983003 -0.0013987041 -0.0013990083 -0.0013992266 -0.0013993569 -0.0013994438 -0.001399503 -0.0013995542 -0.0013995606 -0.001399516 -0.0013994204 -0.0013993286 -0.0013992453][-0.0013980021 -0.0013983479 -0.0013987661 -0.0013990945 -0.0013992968 -0.0013994054 -0.0013994757 -0.0013995888 -0.0013997332 -0.0013998833 -0.0013999203 -0.00139983 -0.0013996058 -0.0013993555 -0.001399138][-0.0013987323 -0.0013991392 -0.0013995253 -0.0013997465 -0.0013998045 -0.0013997413 -0.0013996838 -0.0013997477 -0.0013998988 -0.0014000782 -0.0014001125 -0.0013999753 -0.0013996456 -0.0013992717 -0.0013989747][-0.0013995939 -0.0014001362 -0.0014005462 -0.0014006793 -0.001400575 -0.0014003133 -0.00140006 -0.0013999827 -0.0014000424 -0.0014001577 -0.0014001156 -0.0013999109 -0.0013995256 -0.0013990956 -0.0013987698][-0.0014002164 -0.0014010341 -0.0014015979 -0.001401706 -0.0014014574 -0.0014009494 -0.0014004281 -0.001400081 -0.0013999243 -0.0013998965 -0.0013997661 -0.0013995225 -0.0013991558 -0.0013987717 -0.0013984863][-0.0014006593 -0.0014018399 -0.0014026413 -0.0014028114 -0.0014024475 -0.0014016614 -0.0014007509 -0.0014000026 -0.0013995303 -0.0013993039 -0.0013990783 -0.0013988161 -0.0013985164 -0.0013982563 -0.0013980931][-0.0014015441 -0.0014030377 -0.0014039718 -0.0014041027 -0.0014035382 -0.0014023973 -0.0014010415 -0.0013998604 -0.0013990495 -0.0013986066 -0.001398297 -0.0013980337 -0.0013978044 -0.0013976801 -0.00139766][-0.0014028692 -0.0014045693 -0.0014055466 -0.0014055212 -0.0014045959 -0.0014030129 -0.0014011757 -0.0013995809 -0.0013984761 -0.001397887 -0.0013975552 -0.0013973302 -0.0013971882 -0.0013971751 -0.0013972329][-0.0014043362 -0.0014060624 -0.0014069637 -0.0014066981 -0.0014053698 -0.0014033408 -0.0014010924 -0.00139918 -0.0013978775 -0.0013972325 -0.0013969357 -0.0013967633 -0.0013966711 -0.0013967013 -0.0013967517][-0.0014055464 -0.0014071001 -0.0014077578 -0.0014072348 -0.0014055867 -0.0014032631 -0.0014007719 -0.0013986751 -0.0013972998 -0.0013966666 -0.0013964111 -0.0013962442 -0.0013961082 -0.0013960926 -0.001396067][-0.0014061853 -0.001407326 -0.001407586 -0.0014067582 -0.0014048982 -0.0014024668 -0.0013999826 -0.0013979569 -0.0013967032 -0.0013961939 -0.0013959788 -0.0013957528 -0.0013955076 -0.0013953812 -0.0013952474]]...]
INFO - root - 2017-12-09 10:09:12.528652: step 13010, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.872 sec/batch; 77h:24m:07s remains)
INFO - root - 2017-12-09 10:09:21.288498: step 13020, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.884 sec/batch; 78h:24m:55s remains)
INFO - root - 2017-12-09 10:09:29.980002: step 13030, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.874 sec/batch; 77h:34m:57s remains)
INFO - root - 2017-12-09 10:09:38.537551: step 13040, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.836 sec/batch; 74h:12m:46s remains)
INFO - root - 2017-12-09 10:09:47.013263: step 13050, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.840 sec/batch; 74h:33m:21s remains)
INFO - root - 2017-12-09 10:09:55.503816: step 13060, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 77h:16m:08s remains)
INFO - root - 2017-12-09 10:10:04.107433: step 13070, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.881 sec/batch; 78h:09m:19s remains)
INFO - root - 2017-12-09 10:10:12.799972: step 13080, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 77h:04m:51s remains)
INFO - root - 2017-12-09 10:10:21.413522: step 13090, loss = 0.82, batch loss = 0.69 (10.1 examples/sec; 0.794 sec/batch; 70h:24m:36s remains)
INFO - root - 2017-12-09 10:10:29.844642: step 13100, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.881 sec/batch; 78h:07m:43s remains)
2017-12-09 10:10:30.665536: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0014079594 -0.0014009381 -0.0013868987 -0.0013642423 -0.0013378965 -0.0013175912 -0.0013096264 -0.0013157588 -0.0013331806 -0.0013553712 -0.0013753079 -0.0013890633 -0.0013959638 -0.0013980817 -0.0013978307][-0.0014117259 -0.0014049743 -0.0013901105 -0.0013644681 -0.0013317894 -0.0013024302 -0.001285805 -0.0012857299 -0.0013019509 -0.0013286326 -0.0013568326 -0.0013790961 -0.0013921569 -0.0013974452 -0.001398435][-0.0014132067 -0.0014077374 -0.0013941247 -0.0013688313 -0.0013333895 -0.0012970007 -0.0012709728 -0.0012621407 -0.0012728248 -0.0012990971 -0.001332585 -0.0013624845 -0.0013831656 -0.0013935129 -0.0013971309][-0.0014114885 -0.0014083635 -0.0013970855 -0.0013734082 -0.0013375764 -0.0012966188 -0.0012636913 -0.0012480603 -0.0012528565 -0.0012750623 -0.0013078861 -0.0013415266 -0.0013694001 -0.0013864054 -0.001394259][-0.0014058657 -0.0014063526 -0.0013977832 -0.001376578 -0.0013423456 -0.0013002998 -0.0012639022 -0.0012436443 -0.0012439572 -0.0012619382 -0.0012908234 -0.0013234764 -0.0013536711 -0.0013744995 -0.0013852748][-0.0013971084 -0.0014017819 -0.0013972906 -0.0013800102 -0.0013496069 -0.0013104536 -0.001273474 -0.0012506343 -0.001246965 -0.0012611389 -0.0012877485 -0.0013187022 -0.0013473197 -0.0013672975 -0.001377372][-0.0013867544 -0.0013959066 -0.0013967683 -0.0013849182 -0.0013607749 -0.0013271724 -0.0012924265 -0.0012689823 -0.0012618979 -0.0012721478 -0.0012958496 -0.001325133 -0.0013522075 -0.0013704667 -0.0013786977][-0.0013771735 -0.0013907403 -0.0013972067 -0.0013907424 -0.001373466 -0.0013479581 -0.0013190735 -0.0012969017 -0.0012875758 -0.0012942197 -0.001313831 -0.0013397193 -0.0013637115 -0.0013798074 -0.0013866641][-0.0013709558 -0.0013880881 -0.0013987969 -0.0013970742 -0.0013861335 -0.001369132 -0.0013485446 -0.0013305692 -0.0013211394 -0.0013240597 -0.0013379366 -0.0013576122 -0.0013762151 -0.001389017 -0.0013950682][-0.0013712158 -0.0013883051 -0.0014000732 -0.0014013934 -0.0013953099 -0.0013854972 -0.0013732041 -0.0013617645 -0.0013553079 -0.0013561541 -0.0013639167 -0.0013760374 -0.0013876997 -0.0013961203 -0.0014003795][-0.0013798869 -0.0013928477 -0.0014021201 -0.0014040777 -0.0014008435 -0.0013952792 -0.0013885973 -0.0013825778 -0.0013791216 -0.0013793586 -0.0013832202 -0.0013898389 -0.0013961375 -0.0014007193 -0.0014029698][-0.0013916985 -0.0013986485 -0.0014043686 -0.0014055155 -0.0014034226 -0.0013997449 -0.0013957333 -0.001393095 -0.0013920325 -0.0013920096 -0.0013934077 -0.0013961962 -0.0013994549 -0.0014024732 -0.001404454][-0.0013991989 -0.0014018074 -0.0014047589 -0.0014053714 -0.0014039694 -0.0014013591 -0.001398888 -0.0013975528 -0.0013975175 -0.0013977273 -0.0013982434 -0.001399071 -0.0014002798 -0.0014021213 -0.0014037594][-0.0014022073 -0.0014025365 -0.001403619 -0.0014040848 -0.0014030652 -0.0014012681 -0.0013996062 -0.0013990814 -0.0013993245 -0.0013996662 -0.0013997566 -0.0013998521 -0.0014001525 -0.0014011161 -0.0014024456][-0.0014022079 -0.0014016741 -0.0014023586 -0.0014027619 -0.0014021136 -0.0014009826 -0.0013997477 -0.0013993221 -0.0013992799 -0.0013993196 -0.0013995963 -0.0013997816 -0.0014002392 -0.0014008977 -0.0014020767]]...]
INFO - root - 2017-12-09 10:10:39.145401: step 13110, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.883 sec/batch; 78h:21m:41s remains)
INFO - root - 2017-12-09 10:10:47.692168: step 13120, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.859 sec/batch; 76h:13m:02s remains)
INFO - root - 2017-12-09 10:10:56.389312: step 13130, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.874 sec/batch; 77h:29m:52s remains)
INFO - root - 2017-12-09 10:11:04.977642: step 13140, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.892 sec/batch; 79h:06m:35s remains)
INFO - root - 2017-12-09 10:11:13.585911: step 13150, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 77h:07m:11s remains)
INFO - root - 2017-12-09 10:11:22.324480: step 13160, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 77h:02m:17s remains)
INFO - root - 2017-12-09 10:11:31.090397: step 13170, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.897 sec/batch; 79h:34m:18s remains)
INFO - root - 2017-12-09 10:11:39.864449: step 13180, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.889 sec/batch; 78h:48m:48s remains)
INFO - root - 2017-12-09 10:11:48.434043: step 13190, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.883 sec/batch; 78h:18m:33s remains)
INFO - root - 2017-12-09 10:11:56.839116: step 13200, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 77h:13m:33s remains)
2017-12-09 10:11:57.724351: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.001396396 -0.0013951918 -0.0013941132 -0.0013897432 -0.00137666 -0.0013424641 -0.0012652867 -0.0011232456 -0.00091180293 -0.00066320592 -0.00043937447 -0.00031317829 -0.0003220489 -0.00044285681 -0.000651971][-0.0013976383 -0.0013949539 -0.0013869371 -0.0013634184 -0.0013113823 -0.0012081481 -0.0010190855 -0.000712109 -0.00028552942 0.00019653013 0.00060995016 0.00081118918 0.00075291214 0.00048846251 9.4440184e-05][-0.001399377 -0.0013909052 -0.0013595846 -0.0012800541 -0.0011289957 -0.000874594 -0.00047636905 0.00011096336 0.00089718262 0.0017818275 0.0025304395 0.0028651825 0.0027068832 0.0021711739 0.001446126][-0.0013919659 -0.0013644532 -0.0012795518 -0.0010856029 -0.00074714661 -0.00022776029 0.00050888781 0.0015322589 0.0028735141 0.0043934239 0.0056877341 0.0062649837 0.005973896 0.0050124442 0.0037362457][-0.0013665654 -0.0013018146 -0.0011276081 -0.00075630069 -0.00014091947 0.00076027727 0.0019714735 0.0035854036 0.0056501338 0.00798352 0.0099720648 0.010864085 0.010411314 0.0089183748 0.0069341697][-0.0013220075 -0.0012031384 -0.00091082411 -0.00032224937 0.00062684948 0.0019893146 0.0037688555 0.0060637854 0.0089251781 0.012136159 0.014876484 0.016148252 0.015568887 0.013531264 0.01073882][-0.0012659387 -0.0010888043 -0.00068580866 9.5589668e-05 0.0013456326 0.0031421923 0.0054674074 0.0083987061 0.011936102 0.015829079 0.0191203 0.020685233 0.020034315 0.017646715 0.014257456][-0.0012176757 -0.0010028796 -0.00054701854 0.00031955633 0.0017194082 0.0037712108 0.0064648567 0.0098221675 0.013735703 0.017882556 0.021256989 0.022817226 0.022085279 0.019661866 0.016137596][-0.0012023821 -0.00098874548 -0.00055883313 0.00024451257 0.0015749452 0.003599602 0.0063567562 0.009818201 0.013737318 0.017691804 0.020715043 0.021997385 0.021191174 0.019006422 0.015789542][-0.0012509918 -0.0010820506 -0.00074701308 -0.00011958496 0.00095093739 0.0026626715 0.0051214164 0.0082763424 0.0118015 0.015187764 0.017550409 0.018386602 0.017520078 0.015744828 0.013177175][-0.0013201523 -0.0012137576 -0.00099340873 -0.00058221648 0.00015132595 0.0013911801 0.0032876309 0.0058206804 0.0086779455 0.011364661 0.013095056 0.013565226 0.012700574 0.011301592 0.0093783643][-0.0013678354 -0.0013197656 -0.0012141814 -0.001002169 -0.00059807685 0.0001490179 0.0013927147 0.0031617896 0.0052308636 0.0072032036 0.0084344056 0.0087033827 0.0079508014 0.0068878112 0.0055185109][-0.0013889599 -0.0013741766 -0.0013389089 -0.0012603879 -0.0010855661 -0.00071832055 -3.2702228e-05 0.001032765 0.0023650632 0.0037156034 0.0046000052 0.0048125507 0.0042736707 0.0034965712 0.0025358421][-0.0013925022 -0.0013891207 -0.001382437 -0.0013656407 -0.0013143711 -0.0011749123 -0.00087026728 -0.00033599802 0.00039355631 0.0011917248 0.0017800855 0.0019787471 0.0016990779 0.0012107331 0.00060119689][-0.0013926108 -0.0013914885 -0.0013911342 -0.0013906859 -0.0013831662 -0.0013477589 -0.0012479963 -0.0010482243 -0.000741631 -0.0003700899 -6.3588261e-05 7.2676572e-05 -3.0351337e-05 -0.00027027156 -0.00057525036]]...]
INFO - root - 2017-12-09 10:12:06.310862: step 13210, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.885 sec/batch; 78h:28m:57s remains)
INFO - root - 2017-12-09 10:12:14.909166: step 13220, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.876 sec/batch; 77h:41m:52s remains)
INFO - root - 2017-12-09 10:12:23.338605: step 13230, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.835 sec/batch; 74h:00m:49s remains)
INFO - root - 2017-12-09 10:12:32.010967: step 13240, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.875 sec/batch; 77h:38m:11s remains)
INFO - root - 2017-12-09 10:12:40.744922: step 13250, loss = 0.83, batch loss = 0.70 (8.9 examples/sec; 0.895 sec/batch; 79h:21m:23s remains)
INFO - root - 2017-12-09 10:12:49.508584: step 13260, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.842 sec/batch; 74h:39m:15s remains)
INFO - root - 2017-12-09 10:12:58.102392: step 13270, loss = 0.82, batch loss = 0.69 (9.7 examples/sec; 0.825 sec/batch; 73h:09m:21s remains)
INFO - root - 2017-12-09 10:13:06.892053: step 13280, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.878 sec/batch; 77h:51m:23s remains)
INFO - root - 2017-12-09 10:13:15.502810: step 13290, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 77h:15m:17s remains)
INFO - root - 2017-12-09 10:13:23.840482: step 13300, loss = 0.82, batch loss = 0.70 (9.2 examples/sec; 0.865 sec/batch; 76h:42m:00s remains)
2017-12-09 10:13:24.751229: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.0015718556 0.0015449509 0.0017257911 0.0018662064 0.0025698361 0.0041347863 0.0065992204 0.00975666 0.012513965 0.014459772 0.015805338 0.01721002 0.017956221 0.017031375 0.014905192][0.0021095295 0.0021496061 0.0023468863 0.0025549936 0.0033423249 0.0052146879 0.0082496125 0.0120494 0.015058875 0.016714236 0.017348539 0.017768141 0.0178344 0.016656816 0.014490198][0.0028369958 0.0028404174 0.003004076 0.0032231719 0.0041539627 0.0064601703 0.010112032 0.01442307 0.0176796 0.019280348 0.019367289 0.018929848 0.018242311 0.016838871 0.014681283][0.0034239125 0.0036537368 0.0039514722 0.0042594364 0.0054856176 0.0081127668 0.012176826 0.016887244 0.020458438 0.02220927 0.022276267 0.021249598 0.019910177 0.017903708 0.01552349][0.0035867305 0.0039674863 0.0044465177 0.0051330342 0.0068971636 0.0099931732 0.014471607 0.019433204 0.023104846 0.025009181 0.024973951 0.023612583 0.021701964 0.019530913 0.017142776][0.0037913378 0.004099282 0.0045458311 0.0055240332 0.00787138 0.011621701 0.016551279 0.021756103 0.025368139 0.027213603 0.027223919 0.025704432 0.023444952 0.02105258 0.018945105][0.0044737509 0.0044970759 0.0047924784 0.0058449232 0.0085088015 0.012746667 0.018008292 0.023279196 0.026864238 0.028968539 0.029153625 0.027855506 0.025759257 0.023577545 0.022081468][0.0059271203 0.0058944025 0.0060547171 0.0069911582 0.00967111 0.014013623 0.01936234 0.024621403 0.028298965 0.030746063 0.03153494 0.031120015 0.029746821 0.028142946 0.027027581][0.007804459 0.0079222983 0.00821722 0.0092720473 0.012002455 0.016253969 0.021451201 0.026583267 0.03029546 0.03309527 0.034516264 0.035061486 0.034568969 0.033632871 0.032831043][0.010671814 0.010923709 0.011346504 0.012448136 0.01508967 0.019255949 0.024301728 0.029232042 0.03284708 0.035794362 0.037573088 0.038826868 0.038851444 0.038269848 0.037578933][0.014959546 0.015574701 0.016275028 0.017519204 0.019959055 0.023676947 0.028108848 0.032393545 0.035630215 0.03833456 0.040101513 0.041477896 0.041680042 0.041003909 0.040112235][0.020514261 0.021471594 0.022474071 0.023854945 0.026078992 0.02927674 0.032779187 0.035983037 0.038345605 0.040363036 0.041700352 0.042868782 0.043028805 0.042344708 0.041137263][0.026854498 0.028069641 0.029298194 0.030741995 0.03251566 0.034929823 0.037391078 0.03953341 0.040932789 0.042004485 0.042611759 0.043265752 0.043220457 0.042545982 0.041233364][0.033208966 0.034341723 0.035526738 0.036864631 0.038171332 0.039713535 0.040986579 0.0418897 0.042114139 0.042168487 0.041905776 0.041921727 0.041598938 0.040959243 0.039817579][0.038265441 0.039118629 0.039996009 0.040918972 0.041543964 0.042128056 0.042238045 0.0418458 0.040870115 0.039780129 0.03865318 0.037977591 0.037339691 0.03669117 0.035707198]]...]
INFO - root - 2017-12-09 10:13:33.262782: step 13310, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.852 sec/batch; 75h:33m:35s remains)
INFO - root - 2017-12-09 10:13:41.931966: step 13320, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 76h:25m:53s remains)
INFO - root - 2017-12-09 10:13:50.577414: step 13330, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.892 sec/batch; 79h:03m:00s remains)
INFO - root - 2017-12-09 10:13:59.480775: step 13340, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.848 sec/batch; 75h:11m:45s remains)
INFO - root - 2017-12-09 10:14:08.139709: step 13350, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.840 sec/batch; 74h:26m:24s remains)
INFO - root - 2017-12-09 10:14:16.601272: step 13360, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.858 sec/batch; 76h:04m:14s remains)
INFO - root - 2017-12-09 10:14:25.273587: step 13370, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.881 sec/batch; 78h:07m:47s remains)
INFO - root - 2017-12-09 10:14:33.933694: step 13380, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.836 sec/batch; 74h:07m:10s remains)
INFO - root - 2017-12-09 10:14:42.631331: step 13390, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.879 sec/batch; 77h:55m:20s remains)
INFO - root - 2017-12-09 10:14:50.966025: step 13400, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.841 sec/batch; 74h:33m:01s remains)
2017-12-09 10:14:51.827168: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.0095985383 0.014944439 0.02243218 0.031944577 0.043582078 0.057872 0.073964737 0.089679442 0.10130237 0.10622526 0.1035855 0.093923628 0.079098955 0.061453793 0.043988768][0.0099261561 0.018154431 0.029814111 0.043896183 0.059208326 0.075377978 0.091533966 0.10578422 0.11513877 0.11797019 0.11352782 0.102125 0.085418679 0.065750577 0.046245284][0.010349555 0.021256687 0.036443636 0.054188937 0.072610356 0.090363026 0.10641486 0.11884052 0.12554993 0.12559503 0.11862455 0.10514464 0.086631045 0.065673389 0.045228619][0.010963399 0.023922432 0.04163542 0.061839107 0.082269646 0.10113943 0.11704961 0.12798297 0.13241473 0.12981842 0.12019785 0.10442986 0.084245153 0.062539667 0.042001449][0.011355813 0.025412228 0.044508573 0.065982573 0.087541059 0.10727843 0.12320337 0.13298962 0.13536523 0.13008383 0.1174385 0.09911409 0.077579834 0.055973861 0.036559038][0.011678763 0.025644846 0.044671658 0.066273682 0.088122159 0.10819643 0.12408572 0.13322119 0.13404514 0.12648468 0.11125998 0.090729631 0.068293206 0.047307611 0.029752487][0.011759303 0.024528632 0.04211878 0.062500075 0.083683647 0.10339974 0.11876553 0.12709068 0.12669098 0.11753155 0.1006232 0.0791029 0.056943428 0.037539974 0.022432417][0.011411157 0.0222847 0.0376806 0.056083772 0.075791322 0.0943193 0.1085154 0.11546514 0.11374863 0.10353111 0.086149052 0.0651051 0.044512432 0.027595526 0.015301873][0.010210766 0.018917689 0.031772021 0.047818989 0.065487124 0.082058415 0.094201222 0.09905985 0.09563648 0.084603958 0.067807391 0.0487986 0.031272076 0.017809583 0.0087482119][0.00791289 0.014464843 0.024660232 0.037961453 0.052941281 0.066894509 0.076458894 0.079021454 0.074070282 0.062887378 0.04780006 0.032137416 0.018820899 0.0094146784 0.0036587131][0.0051273741 0.0096040685 0.016899232 0.026884364 0.03833092 0.048950106 0.05583927 0.056757051 0.051480085 0.041548125 0.029492691 0.018031549 0.00917338 0.0035739874 0.00060047768][0.0022981153 0.0049853968 0.0094825048 0.015904482 0.023530195 0.030822819 0.035542563 0.03598382 0.031935949 0.024691433 0.016305495 0.0087895878 0.0034658739 0.000507504 -0.00077312428][5.0748349e-05 0.0013908076 0.0036891268 0.0071326243 0.011404107 0.015743712 0.01873792 0.019216109 0.016946144 0.01267808 0.0077357125 0.0033854765 0.00051342463 -0.00085581123 -0.0012847024][-0.0010788344 -0.00062444847 0.00023419538 0.0016879006 0.00361909 0.0057631377 0.0074083069 0.0079282261 0.0071151452 0.0052410932 0.0029223182 0.00079133268 -0.00058971258 -0.001199236 -0.0013508144][-0.0013896067 -0.0013164153 -0.0011365791 -0.00074933 -0.00013769197 0.00065891794 0.0013558303 0.00169587 0.0015584317 0.00099551328 0.00019482372 -0.00059870118 -0.0011104099 -0.0013283099 -0.0013706214]]...]
INFO - root - 2017-12-09 10:15:00.309532: step 13410, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 76h:33m:35s remains)
INFO - root - 2017-12-09 10:15:08.832321: step 13420, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.875 sec/batch; 77h:31m:47s remains)
INFO - root - 2017-12-09 10:15:17.488503: step 13430, loss = 0.82, batch loss = 0.70 (9.2 examples/sec; 0.868 sec/batch; 76h:58m:30s remains)
INFO - root - 2017-12-09 10:15:26.308499: step 13440, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.870 sec/batch; 77h:07m:54s remains)
INFO - root - 2017-12-09 10:15:35.032235: step 13450, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 76h:47m:41s remains)
INFO - root - 2017-12-09 10:15:43.571666: step 13460, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.888 sec/batch; 78h:41m:26s remains)
INFO - root - 2017-12-09 10:15:52.274884: step 13470, loss = 0.82, batch loss = 0.69 (9.7 examples/sec; 0.825 sec/batch; 73h:05m:50s remains)
INFO - root - 2017-12-09 10:16:01.023266: step 13480, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.866 sec/batch; 76h:46m:51s remains)
INFO - root - 2017-12-09 10:16:09.734535: step 13490, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.879 sec/batch; 77h:54m:58s remains)
INFO - root - 2017-12-09 10:16:18.065379: step 13500, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.852 sec/batch; 75h:30m:10s remains)
2017-12-09 10:16:18.927929: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0013884149 -0.0013884471 -0.0013915205 -0.0013954327 -0.0013997975 -0.0014037489 -0.0014059771 -0.0014065774 -0.0014053645 -0.0014025234 -0.0013988225 -0.0013961479 -0.0013940285 -0.0013924227 -0.0013911336][-0.001393122 -0.001393228 -0.0013965049 -0.0014007991 -0.0014053315 -0.001409039 -0.0014105656 -0.0014097191 -0.0014066574 -0.0014022668 -0.0013974229 -0.0013942049 -0.0013921103 -0.0013905565 -0.0013893193][-0.0013977017 -0.0013976936 -0.0014002291 -0.0014038977 -0.0014080128 -0.0014113358 -0.0014124624 -0.0014110932 -0.0014074247 -0.0014021891 -0.0013964562 -0.0013926812 -0.0013902549 -0.0013887172 -0.0013875462][-0.0013971635 -0.0013968371 -0.0013991784 -0.0014029024 -0.0014073255 -0.0014106806 -0.0014116155 -0.0014101628 -0.0014064163 -0.0014007151 -0.0013947067 -0.0013908974 -0.0013886171 -0.0013872586 -0.001386172][-0.0013946993 -0.0013948805 -0.0013974595 -0.0014010834 -0.0014051065 -0.0014077381 -0.0014081041 -0.0014064416 -0.0014030065 -0.001397801 -0.0013924892 -0.0013893045 -0.0013875435 -0.0013863752 -0.0013854187][-0.0013921147 -0.0013923227 -0.0013947228 -0.0013975359 -0.0014010118 -0.0014028675 -0.0014028171 -0.0014011861 -0.0013983782 -0.0013943957 -0.0013903893 -0.0013881093 -0.0013868854 -0.0013860264 -0.0013851046][-0.0013907253 -0.0013908966 -0.0013929263 -0.0013949707 -0.0013972297 -0.00139793 -0.0013970801 -0.0013952545 -0.0013932452 -0.0013908049 -0.0013884708 -0.0013873439 -0.0013867625 -0.0013862271 -0.0013854591][-0.0013901864 -0.001389962 -0.0013912506 -0.0013925838 -0.0013935716 -0.0013932115 -0.0013916536 -0.0013898924 -0.0013886578 -0.0013877955 -0.0013871316 -0.0013870774 -0.0013871006 -0.0013868483 -0.001386192][-0.0013895195 -0.0013892455 -0.0013900436 -0.0013907665 -0.001390962 -0.0013902321 -0.0013886144 -0.0013872093 -0.0013864523 -0.001386469 -0.0013867054 -0.0013872248 -0.0013876708 -0.0013876789 -0.0013870532][-0.0013894405 -0.001389093 -0.0013894493 -0.0013897329 -0.0013895551 -0.001388981 -0.0013877819 -0.0013868515 -0.0013863661 -0.0013865028 -0.0013869174 -0.0013875554 -0.0013880294 -0.0013880134 -0.0013872308][-0.0013899887 -0.0013898446 -0.0013902023 -0.0013904243 -0.0013902463 -0.0013896904 -0.0013886507 -0.0013878298 -0.0013873527 -0.0013873593 -0.00138761 -0.0013881448 -0.001388462 -0.0013881454 -0.0013869136][-0.0013919694 -0.0013921855 -0.0013927333 -0.0013932431 -0.0013930365 -0.0013923515 -0.0013911024 -0.0013898727 -0.0013889221 -0.0013885357 -0.0013885833 -0.0013888164 -0.0013887692 -0.001388029 -0.0013862466][-0.0013928441 -0.0013937134 -0.0013948603 -0.0013958572 -0.0013958296 -0.0013951981 -0.0013935785 -0.0013918257 -0.0013902423 -0.0013894163 -0.0013892641 -0.0013893681 -0.0013891598 -0.0013881547 -0.0013859954][-0.0013933845 -0.0013949961 -0.0013968996 -0.0013985949 -0.0013989205 -0.001398348 -0.00139645 -0.0013939649 -0.0013917065 -0.0013903794 -0.001390024 -0.0013900859 -0.0013899509 -0.0013889733 -0.0013866968][-0.0013936885 -0.0013959118 -0.0013985041 -0.0014008088 -0.0014013207 -0.0014006195 -0.0013983526 -0.0013952592 -0.0013923587 -0.0013907001 -0.001390285 -0.0013904517 -0.0013905312 -0.0013898395 -0.0013879503]]...]
INFO - root - 2017-12-09 10:16:27.452162: step 13510, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.848 sec/batch; 75h:09m:57s remains)
INFO - root - 2017-12-09 10:16:36.111170: step 13520, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.880 sec/batch; 77h:59m:10s remains)
INFO - root - 2017-12-09 10:16:44.705077: step 13530, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.833 sec/batch; 73h:45m:57s remains)
INFO - root - 2017-12-09 10:16:53.102481: step 13540, loss = 0.81, batch loss = 0.68 (9.6 examples/sec; 0.836 sec/batch; 74h:04m:59s remains)
INFO - root - 2017-12-09 10:17:01.699871: step 13550, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.854 sec/batch; 75h:38m:01s remains)
INFO - root - 2017-12-09 10:17:10.411754: step 13560, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.874 sec/batch; 77h:25m:57s remains)
INFO - root - 2017-12-09 10:17:19.155519: step 13570, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.859 sec/batch; 76h:04m:38s remains)
INFO - root - 2017-12-09 10:17:27.914408: step 13580, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.889 sec/batch; 78h:45m:59s remains)
INFO - root - 2017-12-09 10:17:36.602479: step 13590, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.894 sec/batch; 79h:12m:55s remains)
INFO - root - 2017-12-09 10:17:45.055451: step 13600, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.865 sec/batch; 76h:37m:59s remains)
2017-12-09 10:17:45.870296: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0011740972 -0.00099435577 -0.00076403032 -0.00054545421 -0.00036120624 -0.00024963624 -0.0002171814 -0.0002662983 -0.00037249457 -0.00046741206 -0.00052621739 -0.00055901794 -0.00059069274 -0.00063821528 -0.00069857412][-0.0011530857 -0.00095165859 -0.00069944467 -0.00046207313 -0.0002665004 -0.00015373121 -0.00011909788 -0.00015667221 -0.0002427717 -0.00032120058 -0.00036384386 -0.0003813362 -0.0004001694 -0.00044709677 -0.00052392134][-0.0011629879 -0.0009622206 -0.00072237721 -0.00049109908 -0.0002913553 -0.00014894281 -7.9066027e-05 -7.8075333e-05 -0.0001367759 -0.00020139385 -0.00024593971 -0.00026445766 -0.00028803921 -0.00033828488 -0.00042128563][-0.0012025943 -0.0010034826 -0.0007667297 -0.00052972167 -0.00031117571 -0.00012729515 -1.1761091e-05 2.3817061e-05 -1.7475802e-05 -8.9409761e-05 -0.00013665319 -0.00016333244 -0.00020093133 -0.00027721922 -0.00038500037][-0.0012475875 -0.0010573927 -0.00081957254 -0.0005734142 -0.00033127877 -0.00011180551 4.1856314e-05 0.00010038575 6.754545e-05 -5.3978292e-06 -6.7945453e-05 -0.00013079681 -0.00018906663 -0.00027616974 -0.00039025082][-0.0012703205 -0.0010996055 -0.00087053113 -0.00061816414 -0.0003550722 -0.0001153996 5.8741076e-05 0.0001227475 8.87597e-05 1.1550495e-05 -7.5401273e-05 -0.0001563878 -0.00023296359 -0.00031946937 -0.000425125][-0.0012742911 -0.0011355807 -0.00093341916 -0.00069582992 -0.00043019978 -0.00018474262 -7.5596618e-06 6.0029328e-05 3.2053562e-05 -4.3028966e-05 -0.0001420303 -0.00024741597 -0.00035781018 -0.00045332883 -0.00053666584][-0.0012899741 -0.0011823451 -0.0010129337 -0.00079371082 -0.00053398224 -0.0002938438 -0.0001243786 -5.8001024e-05 -8.5759093e-05 -0.00015962694 -0.0002528429 -0.00036592223 -0.000492078 -0.00059835654 -0.00066603418][-0.0013420089 -0.0012588294 -0.0011151307 -0.00091849733 -0.00066772522 -0.00043231528 -0.00026042724 -0.00018915604 -0.00020487618 -0.0002766226 -0.00037702057 -0.00049722631 -0.00062521338 -0.00072106742 -0.00077051768][-0.0013833158 -0.0013338367 -0.0012259821 -0.0010540562 -0.00081859669 -0.000580788 -0.00039146573 -0.00028900953 -0.00027179066 -0.0003290727 -0.00042831607 -0.00056020974 -0.00069129549 -0.00078510866 -0.00081962236][-0.0013891138 -0.0013622141 -0.0012915126 -0.0011629296 -0.00095936051 -0.00072757609 -0.00050285197 -0.00033563632 -0.00025421591 -0.00028307806 -0.00039567228 -0.00055337447 -0.00071038178 -0.00082498608 -0.00086057815][-0.0013856329 -0.0013626696 -0.0013164611 -0.0012328919 -0.001083766 -0.00086667226 -0.00060144725 -0.0003588153 -0.00022152439 -0.00022174511 -0.00035132677 -0.00054228149 -0.00074109063 -0.00089138449 -0.00095189328][-0.0013795546 -0.0013591673 -0.0013212579 -0.0012655287 -0.0011591145 -0.00096823159 -0.00068108697 -0.00037952233 -0.00020036742 -0.00019496982 -0.00034485 -0.00056878815 -0.00081112451 -0.00099681958 -0.0010778958][-0.0013764657 -0.0013639443 -0.0013362173 -0.001290224 -0.0011979587 -0.0010278916 -0.00075220369 -0.0004321275 -0.00022412825 -0.00020012329 -0.00035286206 -0.00059272512 -0.00086532068 -0.0010883142 -0.0012038714][-0.0013774359 -0.0013716961 -0.0013567324 -0.0013245024 -0.0012424021 -0.0010846427 -0.00082639523 -0.00052602129 -0.00032182375 -0.00028695387 -0.000423491 -0.000663614 -0.00093577517 -0.0011596726 -0.0012914818]]...]
INFO - root - 2017-12-09 10:17:54.465763: step 13610, loss = 0.83, batch loss = 0.70 (9.3 examples/sec; 0.860 sec/batch; 76h:10m:59s remains)
INFO - root - 2017-12-09 10:18:03.076390: step 13620, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.860 sec/batch; 76h:12m:14s remains)
INFO - root - 2017-12-09 10:18:11.742893: step 13630, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.881 sec/batch; 78h:01m:28s remains)
INFO - root - 2017-12-09 10:18:20.474563: step 13640, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.863 sec/batch; 76h:27m:36s remains)
INFO - root - 2017-12-09 10:18:29.086519: step 13650, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 76h:50m:13s remains)
INFO - root - 2017-12-09 10:18:37.816383: step 13660, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.858 sec/batch; 75h:59m:22s remains)
INFO - root - 2017-12-09 10:18:46.417012: step 13670, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.850 sec/batch; 75h:17m:43s remains)
INFO - root - 2017-12-09 10:18:55.073977: step 13680, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.874 sec/batch; 77h:24m:25s remains)
INFO - root - 2017-12-09 10:19:03.618461: step 13690, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.838 sec/batch; 74h:13m:55s remains)
INFO - root - 2017-12-09 10:19:11.972567: step 13700, loss = 0.82, batch loss = 0.69 (9.7 examples/sec; 0.822 sec/batch; 72h:46m:38s remains)
2017-12-09 10:19:12.881267: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0013934525 -0.0013923988 -0.001392977 -0.0013933619 -0.0013935466 -0.0013934663 -0.00139298 -0.001392395 -0.0013921767 -0.0013928775 -0.001394489 -0.0013968567 -0.0013994004 -0.0014016884 -0.0014027269][-0.0013926198 -0.0013923796 -0.0013936572 -0.0013945904 -0.0013949778 -0.0013947844 -0.0013939667 -0.0013929651 -0.0013923739 -0.0013928732 -0.0013947396 -0.001397677 -0.0014010381 -0.0014040257 -0.0014054043][-0.0013934739 -0.0013943407 -0.0013964724 -0.0013981935 -0.0013988834 -0.0013984706 -0.0013971254 -0.0013954917 -0.0013942281 -0.0013942464 -0.001396111 -0.001399509 -0.0014035195 -0.0014071011 -0.0014087259][-0.0013943213 -0.0013964042 -0.001399563 -0.0014021371 -0.0014033942 -0.0014029288 -0.0014011062 -0.0013988873 -0.0013969888 -0.001396552 -0.0013981899 -0.001401747 -0.0014059826 -0.0014097496 -0.001411289][-0.0013954048 -0.0013985372 -0.0014027612 -0.001406326 -0.0014082891 -0.0014080496 -0.0014059391 -0.0014031978 -0.0014007868 -0.0013999493 -0.0014013334 -0.001404705 -0.0014087519 -0.0014124608 -0.0014138465][-0.0013967345 -0.0014005973 -0.0014056243 -0.0014101061 -0.0014126109 -0.0014125444 -0.0014103242 -0.001407383 -0.0014046949 -0.0014036547 -0.0014049337 -0.0014081083 -0.0014118862 -0.0014153618 -0.0014165663][-0.0013977035 -0.001401762 -0.0014072554 -0.0014121486 -0.001414819 -0.0014148068 -0.0014126914 -0.0014099811 -0.0014075002 -0.0014066049 -0.001408142 -0.0014114381 -0.0014151732 -0.001418393 -0.0014192036][-0.0013976464 -0.0014013476 -0.0014067064 -0.0014114626 -0.0014139523 -0.0014139536 -0.0014122089 -0.0014100722 -0.0014081992 -0.0014078777 -0.0014099239 -0.0014136636 -0.0014177201 -0.0014209228 -0.0014213045][-0.0013965128 -0.0013993444 -0.0014040275 -0.0014082269 -0.0014104946 -0.0014106638 -0.0014094423 -0.0014079339 -0.001406886 -0.0014073858 -0.001410124 -0.0014143775 -0.001418813 -0.0014221487 -0.001422307][-0.0013942426 -0.0013960854 -0.001399728 -0.0014032419 -0.0014053305 -0.0014058318 -0.0014051718 -0.0014042603 -0.0014039903 -0.0014052545 -0.0014086467 -0.0014133469 -0.0014181144 -0.0014216249 -0.0014218539][-0.001392116 -0.0013927238 -0.0013951911 -0.0013978609 -0.0013995867 -0.0014001526 -0.0013998387 -0.0013994602 -0.0013998832 -0.0014018368 -0.0014058312 -0.0014109832 -0.0014162217 -0.0014200483 -0.0014205524][-0.0013903468 -0.0013899218 -0.0013913079 -0.0013931367 -0.0013944349 -0.0013949423 -0.0013948489 -0.0013948173 -0.0013957141 -0.0013981261 -0.0014024018 -0.001407791 -0.0014133961 -0.0014177156 -0.0014186194][-0.0013889049 -0.0013879414 -0.0013885482 -0.0013897873 -0.0013908409 -0.0013914215 -0.0013915941 -0.0013917855 -0.0013928178 -0.0013952376 -0.0013993949 -0.00140468 -0.0014102552 -0.0014149601 -0.0014164222][-0.0013879266 -0.0013866561 -0.0013869473 -0.0013878997 -0.001388905 -0.0013897002 -0.0013901655 -0.0013905396 -0.0013915346 -0.0013937969 -0.0013976095 -0.0014024955 -0.0014078314 -0.0014127207 -0.0014146578][-0.0013876007 -0.0013863553 -0.0013864618 -0.0013872603 -0.0013882323 -0.0013891907 -0.0013898632 -0.0013903603 -0.0013912803 -0.0013933819 -0.0013969042 -0.0014014178 -0.0014065172 -0.0014114605 -0.0014137005]]...]
INFO - root - 2017-12-09 10:19:21.373723: step 13710, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.888 sec/batch; 78h:40m:43s remains)
INFO - root - 2017-12-09 10:19:30.046181: step 13720, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 76h:59m:30s remains)
INFO - root - 2017-12-09 10:19:38.715604: step 13730, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.850 sec/batch; 75h:13m:55s remains)
INFO - root - 2017-12-09 10:19:47.345122: step 13740, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.875 sec/batch; 77h:30m:54s remains)
INFO - root - 2017-12-09 10:19:56.068380: step 13750, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 77h:07m:25s remains)
INFO - root - 2017-12-09 10:20:04.687057: step 13760, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.865 sec/batch; 76h:34m:41s remains)
INFO - root - 2017-12-09 10:20:13.361614: step 13770, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.882 sec/batch; 78h:03m:47s remains)
INFO - root - 2017-12-09 10:20:21.957462: step 13780, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.865 sec/batch; 76h:35m:33s remains)
INFO - root - 2017-12-09 10:20:30.613219: step 13790, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.866 sec/batch; 76h:40m:34s remains)
INFO - root - 2017-12-09 10:20:38.992134: step 13800, loss = 0.82, batch loss = 0.69 (9.9 examples/sec; 0.811 sec/batch; 71h:48m:43s remains)
2017-12-09 10:20:39.822772: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.20164533 0.19693293 0.19022247 0.18216322 0.17299567 0.16221812 0.15045688 0.13845347 0.12604561 0.11498009 0.10515895 0.098006405 0.0938876 0.0931924 0.095282651][0.22134468 0.22002998 0.21491109 0.20619918 0.19474284 0.17989783 0.16236576 0.14428617 0.12656979 0.11148347 0.099021316 0.090701737 0.086943164 0.08743576 0.090985194][0.23954274 0.24183531 0.23854914 0.22986704 0.217019 0.19904251 0.17711188 0.15417878 0.13221058 0.11403298 0.099404462 0.089709252 0.086030848 0.087250486 0.091611981][0.25653651 0.26307696 0.26233602 0.25416923 0.24033202 0.22020341 0.19503514 0.16893955 0.1445611 0.12508041 0.10990785 0.10017525 0.096915543 0.098318115 0.10208894][0.27671057 0.28805548 0.28963622 0.28208056 0.26753032 0.24563898 0.21774623 0.18925121 0.16346498 0.14337058 0.12786812 0.11842869 0.1161637 0.11796727 0.12037075][0.29807231 0.31428215 0.31810334 0.31127959 0.29644307 0.27358624 0.24410877 0.21420456 0.18816923 0.16823335 0.15249914 0.14269632 0.14032392 0.14191531 0.14272198][0.31260154 0.33376482 0.34022486 0.33474874 0.320422 0.29738787 0.26728654 0.23713908 0.21125162 0.19157252 0.17583656 0.16598643 0.16354994 0.16464679 0.16419531][0.3136237 0.33683702 0.34406906 0.33992723 0.32716703 0.30583853 0.27756992 0.24989305 0.2261924 0.20819539 0.19328086 0.18384254 0.18112625 0.18095826 0.17897877][0.300859 0.32450551 0.33238015 0.32997325 0.32002124 0.30191574 0.27712625 0.25283942 0.23208028 0.21628007 0.20279108 0.1941576 0.19125758 0.19017833 0.18673044][0.27965692 0.30122906 0.30844957 0.30756894 0.30066574 0.2864475 0.26622188 0.24603713 0.22905262 0.21614666 0.20484288 0.19726898 0.19418579 0.19223835 0.18753742][0.25496426 0.27414259 0.28023368 0.28006959 0.27508596 0.26427317 0.24849263 0.2325197 0.21946888 0.20959105 0.20114848 0.19502729 0.19183506 0.18871373 0.18279517][0.23051435 0.24686448 0.25185943 0.25202838 0.24823119 0.23943293 0.22696041 0.21482877 0.20534167 0.1981037 0.19221506 0.18759961 0.18445572 0.18033983 0.17351188][0.20610987 0.22032328 0.22459234 0.22496897 0.22210078 0.21514079 0.20499031 0.19539903 0.18854336 0.18356521 0.17992467 0.17668705 0.17391 0.16948827 0.16242097][0.180338 0.19248426 0.19609675 0.19652449 0.19447571 0.18923567 0.18163742 0.17466363 0.17012122 0.16681117 0.16454896 0.16214722 0.15970953 0.15556566 0.14912991][0.15538166 0.16492978 0.16757508 0.16805713 0.16669977 0.16292749 0.15745333 0.15283188 0.15003939 0.14815991 0.14698547 0.14562714 0.14398928 0.14057136 0.13534471]]...]
INFO - root - 2017-12-09 10:20:48.266917: step 13810, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.866 sec/batch; 76h:41m:32s remains)
INFO - root - 2017-12-09 10:20:56.781749: step 13820, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 76h:47m:00s remains)
INFO - root - 2017-12-09 10:21:05.397430: step 13830, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.874 sec/batch; 77h:20m:04s remains)
INFO - root - 2017-12-09 10:21:14.260515: step 13840, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.881 sec/batch; 77h:56m:48s remains)
INFO - root - 2017-12-09 10:21:22.953184: step 13850, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.837 sec/batch; 74h:07m:46s remains)
INFO - root - 2017-12-09 10:21:31.634388: step 13860, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.845 sec/batch; 74h:47m:20s remains)
INFO - root - 2017-12-09 10:21:40.387632: step 13870, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.890 sec/batch; 78h:45m:40s remains)
INFO - root - 2017-12-09 10:21:49.136839: step 13880, loss = 0.82, batch loss = 0.69 (8.8 examples/sec; 0.907 sec/batch; 80h:15m:19s remains)
INFO - root - 2017-12-09 10:21:57.899737: step 13890, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.876 sec/batch; 77h:32m:41s remains)
INFO - root - 2017-12-09 10:22:06.307189: step 13900, loss = 0.82, batch loss = 0.69 (11.1 examples/sec; 0.719 sec/batch; 63h:37m:51s remains)
2017-12-09 10:22:07.190414: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.001419225 -0.0014183667 -0.0014182003 -0.0014181745 -0.001418266 -0.0014185113 -0.0014188552 -0.0014191444 -0.0014193401 -0.0014194048 -0.0014193807 -0.0014192886 -0.0014190851 -0.0014189413 -0.0014188936][-0.001418461 -0.0014174995 -0.0014171617 -0.0014169243 -0.0014168123 -0.001416948 -0.0014172826 -0.0014176639 -0.0014179973 -0.0014182198 -0.0014183327 -0.0014183627 -0.0014182621 -0.0014181796 -0.0014181734][-0.0014184105 -0.0014172643 -0.0014167574 -0.0014163262 -0.0014160335 -0.0014160217 -0.0014162868 -0.0014166951 -0.0014171168 -0.0014174894 -0.0014177678 -0.001417934 -0.0014179726 -0.0014179714 -0.0014180031][-0.0014183187 -0.0014170465 -0.0014163842 -0.00141582 -0.0014154221 -0.0014152839 -0.0014154472 -0.0014158217 -0.0014162714 -0.0014167548 -0.0014172178 -0.0014175756 -0.0014177744 -0.0014178591 -0.0014179099][-0.0014181816 -0.0014168239 -0.0014160868 -0.0014154824 -0.0014150193 -0.0014147063 -0.0014146514 -0.0014148284 -0.0014151805 -0.0014157053 -0.00141637 -0.0014170307 -0.0014174965 -0.0014177472 -0.0014178518][-0.0014179039 -0.001416559 -0.0014158329 -0.0014152153 -0.0014146577 -0.0014141081 -0.0014136502 -0.0014134072 -0.0014134811 -0.0014139233 -0.0014147346 -0.0014157528 -0.0014165912 -0.0014171134 -0.0014173699][-0.0014174898 -0.0014160717 -0.0014153563 -0.0014147545 -0.0014141684 -0.0014133814 -0.0014124573 -0.0014117246 -0.001411419 -0.0014116619 -0.0014124967 -0.0014137942 -0.0014150069 -0.001415827 -0.0014162784][-0.0014170412 -0.0014153916 -0.0014146598 -0.0014140886 -0.0014135623 -0.0014126884 -0.0014114851 -0.0014103804 -0.0014096805 -0.001409638 -0.001410338 -0.0014116698 -0.0014131214 -0.0014141966 -0.0014148663][-0.0014164384 -0.0014146514 -0.0014138077 -0.0014131857 -0.0014126721 -0.0014118371 -0.0014106141 -0.0014093596 -0.0014084677 -0.0014082125 -0.0014087353 -0.0014099284 -0.0014113736 -0.0014125312 -0.0014133498][-0.0014156725 -0.0014137317 -0.0014127153 -0.0014119403 -0.0014113876 -0.0014106813 -0.0014096587 -0.0014085685 -0.0014077366 -0.0014074136 -0.0014077261 -0.0014086312 -0.0014098482 -0.0014108958 -0.0014116806][-0.0014147183 -0.0014125918 -0.001411391 -0.0014104941 -0.0014099592 -0.0014094617 -0.0014087902 -0.0014080509 -0.0014074615 -0.0014071704 -0.0014072958 -0.0014078429 -0.0014086846 -0.0014094821 -0.0014100989][-0.0014139677 -0.0014116311 -0.0014102677 -0.001409272 -0.0014087437 -0.0014084294 -0.0014080916 -0.0014077596 -0.0014075239 -0.001407394 -0.0014074033 -0.0014076357 -0.0014081008 -0.0014085659 -0.0014089498][-0.0014136141 -0.0014111885 -0.0014097005 -0.0014086496 -0.0014081026 -0.001407856 -0.0014076924 -0.0014076108 -0.0014076432 -0.0014076697 -0.0014076923 -0.0014077862 -0.0014080068 -0.0014082134 -0.0014084094][-0.0014136394 -0.0014112003 -0.0014096868 -0.0014086601 -0.0014081178 -0.0014078873 -0.0014077881 -0.0014078068 -0.0014079572 -0.0014080672 -0.0014081312 -0.0014081874 -0.0014082801 -0.0014083312 -0.0014083968][-0.0014143392 -0.0014120801 -0.0014107326 -0.0014098622 -0.0014093565 -0.0014091233 -0.0014090461 -0.0014090579 -0.0014091956 -0.0014093008 -0.0014093529 -0.0014093946 -0.0014094368 -0.0014094394 -0.0014094502]]...]
INFO - root - 2017-12-09 10:22:15.973964: step 13910, loss = 0.82, batch loss = 0.69 (8.2 examples/sec; 0.981 sec/batch; 86h:50m:31s remains)
INFO - root - 2017-12-09 10:22:24.739608: step 13920, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.877 sec/batch; 77h:35m:06s remains)
INFO - root - 2017-12-09 10:22:33.452679: step 13930, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.890 sec/batch; 78h:43m:24s remains)
INFO - root - 2017-12-09 10:22:42.183561: step 13940, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.889 sec/batch; 78h:41m:19s remains)
INFO - root - 2017-12-09 10:22:51.002261: step 13950, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.875 sec/batch; 77h:27m:50s remains)
INFO - root - 2017-12-09 10:22:59.697443: step 13960, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.873 sec/batch; 77h:14m:50s remains)
INFO - root - 2017-12-09 10:23:08.347374: step 13970, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.870 sec/batch; 76h:57m:37s remains)
INFO - root - 2017-12-09 10:23:17.074942: step 13980, loss = 0.81, batch loss = 0.68 (9.2 examples/sec; 0.874 sec/batch; 77h:17m:59s remains)
INFO - root - 2017-12-09 10:23:25.699853: step 13990, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.844 sec/batch; 74h:41m:07s remains)
INFO - root - 2017-12-09 10:23:34.089596: step 14000, loss = 0.82, batch loss = 0.69 (11.1 examples/sec; 0.720 sec/batch; 63h:39m:57s remains)
2017-12-09 10:23:34.977395: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.039070789 0.044099215 0.048261728 0.051487446 0.05355031 0.054316115 0.05353884 0.051407095 0.048660152 0.045911215 0.043541487 0.041539561 0.039879162 0.038314968 0.036626257][0.041430637 0.046641294 0.050768018 0.053767912 0.055523846 0.05579311 0.054315086 0.051490378 0.048088849 0.045161802 0.042815 0.041202389 0.040098507 0.039323572 0.03860281][0.042230316 0.047336567 0.051131558 0.053554814 0.054671086 0.054331131 0.052228451 0.048846018 0.04518697 0.042288464 0.040394567 0.039416578 0.039160173 0.039394196 0.039778456][0.042091236 0.046909969 0.050165869 0.051809691 0.052126814 0.051110692 0.048495278 0.044924274 0.041395292 0.038978223 0.03772945 0.037569247 0.038185868 0.039517995 0.041073237][0.0410192 0.045428909 0.04797883 0.04875204 0.048273712 0.046693493 0.043891512 0.040513568 0.037486218 0.035843976 0.035457045 0.036175311 0.037706736 0.040037971 0.042785857][0.039121889 0.043011311 0.044814989 0.044791441 0.043610793 0.0416331 0.038844671 0.035875127 0.033530887 0.032654211 0.033138722 0.034690417 0.037122481 0.040390849 0.044140179][0.036367569 0.039587725 0.04060642 0.039937209 0.038339071 0.036268078 0.033776186 0.031375736 0.029845539 0.029696388 0.030890569 0.033162888 0.036380015 0.040359855 0.044737138][0.033313394 0.0359826 0.036412243 0.035257161 0.033370342 0.03143942 0.029542213 0.027985437 0.027434094 0.028044511 0.029913004 0.032605685 0.036197778 0.040444396 0.044926561][0.030305684 0.032654557 0.0327871 0.031474836 0.029543778 0.027816597 0.026418529 0.025753599 0.026088513 0.02734237 0.029637784 0.032518383 0.036157042 0.040186062 0.044310633][0.027250301 0.029516436 0.029721875 0.028595913 0.02691712 0.025590369 0.024710445 0.024643268 0.025548572 0.027105646 0.029277802 0.03184348 0.035058238 0.038372412 0.041719634][0.024068451 0.026383448 0.026947863 0.026333984 0.025145369 0.024242219 0.023800865 0.024127152 0.025078531 0.026490668 0.028234571 0.030152675 0.032454602 0.034804646 0.037247591][0.020530172 0.022846339 0.023765588 0.023734085 0.023217741 0.02282325 0.022820018 0.023378672 0.024302963 0.025283625 0.026294058 0.027429447 0.028738206 0.030075733 0.03163353][0.017110912 0.019342193 0.020567283 0.021086497 0.021158386 0.021238195 0.021515107 0.022110099 0.022809308 0.0233503 0.023752552 0.024093609 0.02460281 0.025166154 0.026003532][0.013842301 0.015789747 0.017052641 0.017839214 0.018251587 0.018585598 0.018989852 0.019529963 0.019984627 0.020244505 0.020379635 0.020380519 0.020423871 0.020541409 0.020904176][0.010779291 0.012306601 0.013375511 0.014131715 0.014600798 0.01493859 0.015247709 0.01561021 0.015880218 0.01597845 0.015980493 0.015897067 0.015843002 0.015772406 0.015855791]]...]
INFO - root - 2017-12-09 10:23:43.529198: step 14010, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 76h:55m:09s remains)
INFO - root - 2017-12-09 10:23:52.167563: step 14020, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.874 sec/batch; 77h:20m:31s remains)
INFO - root - 2017-12-09 10:24:00.809109: step 14030, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 77h:00m:29s remains)
INFO - root - 2017-12-09 10:24:09.661646: step 14040, loss = 0.82, batch loss = 0.69 (9.8 examples/sec; 0.814 sec/batch; 72h:02m:53s remains)
INFO - root - 2017-12-09 10:24:18.290885: step 14050, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.859 sec/batch; 75h:57m:38s remains)
INFO - root - 2017-12-09 10:24:26.814122: step 14060, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 76h:53m:44s remains)
INFO - root - 2017-12-09 10:24:35.461547: step 14070, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.874 sec/batch; 77h:17m:41s remains)
INFO - root - 2017-12-09 10:24:44.148230: step 14080, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.856 sec/batch; 75h:40m:20s remains)
INFO - root - 2017-12-09 10:24:52.729416: step 14090, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.843 sec/batch; 74h:32m:11s remains)
INFO - root - 2017-12-09 10:25:01.170895: step 14100, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.879 sec/batch; 77h:43m:06s remains)
2017-12-09 10:25:01.944973: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.14459908 0.1484658 0.15088798 0.15110596 0.15144441 0.15102361 0.15057459 0.14975077 0.14793144 0.14523558 0.14026079 0.13312507 0.1254012 0.11674394 0.10817298][0.1549601 0.16123979 0.16488265 0.16627149 0.16711317 0.16718797 0.16754237 0.16734706 0.16664492 0.16478026 0.16058263 0.1529049 0.1441143 0.13447739 0.1244236][0.16274571 0.17067418 0.17619605 0.17823434 0.17931765 0.18005176 0.1812278 0.18235673 0.18298346 0.1824425 0.17883247 0.17129079 0.16150977 0.15073708 0.13911518][0.17037462 0.17801094 0.18340236 0.18552485 0.18624736 0.18665716 0.18736708 0.18923016 0.19164476 0.19326597 0.1910971 0.18416843 0.17457253 0.16319031 0.1502253][0.1751458 0.18062676 0.1838167 0.18394215 0.1832166 0.18276224 0.18319905 0.18586552 0.18987444 0.19392318 0.1938076 0.188332 0.17931618 0.1684968 0.15530843][0.17492063 0.17694481 0.17575154 0.17253461 0.16909902 0.16728877 0.16815117 0.17240296 0.17908381 0.18623969 0.1893433 0.18662597 0.17907879 0.16906281 0.15611477][0.17332256 0.17127624 0.16516557 0.15734302 0.15012269 0.14694364 0.14806013 0.15447205 0.16467544 0.17604041 0.18382676 0.18516967 0.18046452 0.17191359 0.15922268][0.17020686 0.16453464 0.15370543 0.14100911 0.12967964 0.1248954 0.12670897 0.13561799 0.14924882 0.1654785 0.17823826 0.18450381 0.18371783 0.17745166 0.16548991][0.16531533 0.15732539 0.14256711 0.12587267 0.11148665 0.1051422 0.10761806 0.11880684 0.13613082 0.15633146 0.17335908 0.18390648 0.1866561 0.18299402 0.17187941][0.15937755 0.15010579 0.13226856 0.11239918 0.09629868 0.089765318 0.093053468 0.10566724 0.12539083 0.14830017 0.16822502 0.18103485 0.18574838 0.18411015 0.17406178][0.15080306 0.14168102 0.1233127 0.10278337 0.086257368 0.080124423 0.0841365 0.097194947 0.11758049 0.14125906 0.16186434 0.17505464 0.1803107 0.17959894 0.17093082][0.14302488 0.13488844 0.11772959 0.09848927 0.083096668 0.077657729 0.081609093 0.094146043 0.11346371 0.13569643 0.15504897 0.16709691 0.17205261 0.17144327 0.16376439][0.13418695 0.12806953 0.11372774 0.097738177 0.084974125 0.080443665 0.083836593 0.094727941 0.11182603 0.13096078 0.14752583 0.15755424 0.16150419 0.16059808 0.15350239][0.12605928 0.12241503 0.11210354 0.10008333 0.090316668 0.086541079 0.089033172 0.097716309 0.11130312 0.12665726 0.14022945 0.14828178 0.15134661 0.15010853 0.14341685][0.11768762 0.11633793 0.10934809 0.10104195 0.094364427 0.091761574 0.0935218 0.10002042 0.11026848 0.1218551 0.13200505 0.13773146 0.13970265 0.13811393 0.1321667]]...]
INFO - root - 2017-12-09 10:25:10.311060: step 14110, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 76h:22m:37s remains)
INFO - root - 2017-12-09 10:25:18.920733: step 14120, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.855 sec/batch; 75h:34m:18s remains)
INFO - root - 2017-12-09 10:25:27.578135: step 14130, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.870 sec/batch; 76h:56m:37s remains)
INFO - root - 2017-12-09 10:25:36.332204: step 14140, loss = 0.82, batch loss = 0.69 (8.7 examples/sec; 0.915 sec/batch; 80h:52m:25s remains)
INFO - root - 2017-12-09 10:25:44.841968: step 14150, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.865 sec/batch; 76h:31m:03s remains)
INFO - root - 2017-12-09 10:25:53.484147: step 14160, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.865 sec/batch; 76h:29m:00s remains)
INFO - root - 2017-12-09 10:26:02.164818: step 14170, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.843 sec/batch; 74h:31m:20s remains)
INFO - root - 2017-12-09 10:26:10.910640: step 14180, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.890 sec/batch; 78h:41m:47s remains)
INFO - root - 2017-12-09 10:26:19.559636: step 14190, loss = 0.82, batch loss = 0.69 (9.7 examples/sec; 0.821 sec/batch; 72h:34m:06s remains)
INFO - root - 2017-12-09 10:26:28.180961: step 14200, loss = 0.82, batch loss = 0.69 (10.6 examples/sec; 0.753 sec/batch; 66h:32m:23s remains)
2017-12-09 10:26:29.051961: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0014223808 -0.0014197638 -0.0014185766 -0.0014171826 -0.0014161527 -0.0014154469 -0.0014150565 -0.0014151884 -0.0014155087 -0.0014163146 -0.0014173732 -0.0014186621 -0.001420041 -0.001421256 -0.0014221928][-0.0014233683 -0.0014212837 -0.0014204514 -0.0014194896 -0.0014189304 -0.0014185004 -0.0014183179 -0.0014186513 -0.0014192305 -0.001420259 -0.0014216442 -0.0014233142 -0.0014250389 -0.0014263697 -0.0014272173][-0.0014246171 -0.0014230682 -0.0014226645 -0.0014223609 -0.0014225615 -0.0014228136 -0.0014231424 -0.0014240602 -0.0014251248 -0.0014264497 -0.0014279814 -0.0014296917 -0.0014311504 -0.0014320426 -0.0014322997][-0.0014252361 -0.0014241192 -0.0014241664 -0.0014248075 -0.0014260962 -0.0014273593 -0.0014285481 -0.0014303838 -0.001432132 -0.0014336844 -0.0014348386 -0.0014355271 -0.001435434 -0.0014347733 -0.0014338976][-0.0014245636 -0.0014236884 -0.0014242714 -0.0014260294 -0.0014286911 -0.0014314152 -0.0014337481 -0.0014365964 -0.0014388025 -0.001439953 -0.0014396717 -0.0014382995 -0.0014359448 -0.0014328003 -0.0014300612][-0.0014230423 -0.0014223616 -0.0014233277 -0.0014259828 -0.0014297761 -0.0014337739 -0.001437292 -0.00144066 -0.0014425238 -0.0014421144 -0.0014395197 -0.0014354552 -0.0014301088 -0.0014242692 -0.001419677][-0.0014209686 -0.0014203893 -0.0014217054 -0.0014249536 -0.0014295063 -0.0014344831 -0.0014384631 -0.0014418082 -0.001442799 -0.0014404872 -0.0014351649 -0.0014277207 -0.0014188327 -0.0014101258 -0.0014038848][-0.0014184326 -0.0014177484 -0.0014193609 -0.0014229969 -0.0014280896 -0.0014337848 -0.0014382694 -0.0014412814 -0.0014410565 -0.0014368861 -0.0014289254 -0.0014183172 -0.001406291 -0.0013949611 -0.0013873231][-0.0014162611 -0.0014153086 -0.0014170432 -0.0014207265 -0.0014259221 -0.0014320476 -0.0014370668 -0.0014401255 -0.0014393703 -0.0014339432 -0.0014243103 -0.0014114487 -0.0013971148 -0.0013840132 -0.0013754549][-0.0014143258 -0.001413142 -0.0014146941 -0.0014180646 -0.0014229164 -0.0014288793 -0.0014342697 -0.0014377052 -0.001437104 -0.0014317104 -0.0014218293 -0.0014087472 -0.0013940337 -0.0013804388 -0.0013709839][-0.0014134685 -0.0014120735 -0.0014132459 -0.0014160613 -0.0014203605 -0.0014259304 -0.0014314633 -0.0014348208 -0.0014344594 -0.001429771 -0.0014210313 -0.0014093291 -0.0013958751 -0.0013836636 -0.0013747411][-0.0014133229 -0.0014115281 -0.0014122239 -0.0014145267 -0.0014181754 -0.0014235382 -0.0014293264 -0.0014331846 -0.0014337063 -0.00143044 -0.0014236955 -0.0014144625 -0.0014038134 -0.0013941029 -0.0013864851][-0.00141344 -0.0014112515 -0.0014114151 -0.0014132535 -0.0014166728 -0.0014219197 -0.0014280332 -0.001432634 -0.0014343165 -0.0014326142 -0.0014278768 -0.0014213509 -0.0014139173 -0.0014072307 -0.0014015748][-0.0014137335 -0.0014112515 -0.0014108783 -0.0014124636 -0.0014156807 -0.0014207533 -0.0014269516 -0.0014322086 -0.0014352384 -0.0014353367 -0.0014327826 -0.0014288167 -0.0014239182 -0.001419494 -0.0014152731][-0.0014142804 -0.0014115295 -0.0014107518 -0.0014122659 -0.0014154535 -0.0014202941 -0.0014261964 -0.0014315414 -0.0014352576 -0.0014368152 -0.0014359363 -0.0014337481 -0.001430265 -0.0014271543 -0.0014238058]]...]
INFO - root - 2017-12-09 10:26:37.485559: step 14210, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.851 sec/batch; 75h:16m:42s remains)
INFO - root - 2017-12-09 10:26:46.081919: step 14220, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.874 sec/batch; 77h:17m:18s remains)
INFO - root - 2017-12-09 10:26:54.754168: step 14230, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.847 sec/batch; 74h:54m:23s remains)
INFO - root - 2017-12-09 10:27:03.351028: step 14240, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.866 sec/batch; 76h:32m:48s remains)
INFO - root - 2017-12-09 10:27:12.055018: step 14250, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.874 sec/batch; 77h:14m:54s remains)
INFO - root - 2017-12-09 10:27:20.712961: step 14260, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.866 sec/batch; 76h:30m:37s remains)
INFO - root - 2017-12-09 10:27:29.430924: step 14270, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.899 sec/batch; 79h:26m:01s remains)
INFO - root - 2017-12-09 10:27:38.182928: step 14280, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.889 sec/batch; 78h:36m:53s remains)
INFO - root - 2017-12-09 10:27:46.925039: step 14290, loss = 0.82, batch loss = 0.69 (9.7 examples/sec; 0.828 sec/batch; 73h:13m:04s remains)
INFO - root - 2017-12-09 10:27:55.514570: step 14300, loss = 0.82, batch loss = 0.69 (10.4 examples/sec; 0.769 sec/batch; 67h:59m:24s remains)
2017-12-09 10:27:56.331886: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0014628279 -0.0014622295 -0.0014623461 -0.001462649 -0.0014629649 -0.0014631193 -0.001461986 -0.0014588701 -0.001454853 -0.0014526143 -0.0014537188 -0.0014567728 -0.0014597971 -0.0014615094 -0.0014620617][-0.0014617114 -0.0014610038 -0.0014611674 -0.0014616451 -0.001461867 -0.001460264 -0.0014542385 -0.0014429649 -0.0014311612 -0.0014262919 -0.0014314137 -0.0014419889 -0.0014516519 -0.0014573273 -0.0014596625][-0.0014619919 -0.0014612712 -0.0014615249 -0.0014620153 -0.0014604203 -0.0014519767 -0.0014322469 -0.0014037261 -0.0013804114 -0.0013769906 -0.0013938374 -0.001419013 -0.0014393543 -0.0014513887 -0.0014570766][-0.0014624349 -0.0014618061 -0.0014620851 -0.0014605849 -0.0014507099 -0.0014223246 -0.0013725074 -0.0013171788 -0.0012880906 -0.0013036694 -0.0013491781 -0.0013961798 -0.0014273837 -0.0014446874 -0.0014534076][-0.0014629811 -0.0014624551 -0.0014615813 -0.0014516528 -0.0014159647 -0.0013386484 -0.0012311568 -0.0011434682 -0.0011333501 -0.0012040059 -0.001302812 -0.0013785007 -0.0014193743 -0.0014396809 -0.0014504046][-0.0014635049 -0.001463084 -0.0014577734 -0.001426494 -0.0013346666 -0.0011659788 -0.000971069 -0.0008600892 -0.00091098889 -0.0010797941 -0.0012560312 -0.0013665876 -0.001416473 -0.0014379311 -0.0014495159][-0.0014638153 -0.0014628922 -0.0014494186 -0.0013829082 -0.0012069908 -0.00091567344 -0.00062002329 -0.00050416286 -0.0006495337 -0.0009405748 -0.0012060205 -0.001355543 -0.0014161138 -0.0014384768 -0.0014503738][-0.0014638323 -0.001461874 -0.0014383667 -0.0013341133 -0.0010762747 -0.00068023865 -0.00031514373 -0.00021816883 -0.0004552576 -0.00084414484 -0.0011746421 -0.0013511506 -0.001419234 -0.0014419346 -0.001452909][-0.0014634541 -0.0014602263 -0.001429593 -0.0013045755 -0.0010133425 -0.0005918204 -0.00023386662 -0.00016953552 -0.00044628792 -0.00085604197 -0.0011901848 -0.0013629993 -0.001427783 -0.0014477699 -0.0014564949][-0.0014629638 -0.0014582343 -0.0014272834 -0.0013116291 -0.0010571611 -0.00070497888 -0.00042980688 -0.00040385569 -0.00065173057 -0.000991414 -0.0012584669 -0.001392378 -0.001440738 -0.0014541771 -0.0014591243][-0.0014623639 -0.0014567435 -0.0014323456 -0.001350685 -0.0011799679 -0.00095168763 -0.00078803103 -0.00079079083 -0.0009628199 -0.0011817109 -0.0013466495 -0.0014261047 -0.0014530569 -0.0014593217 -0.0014609802][-0.0014618105 -0.001456499 -0.0014414452 -0.0013974457 -0.0013114194 -0.0012000358 -0.0011281966 -0.0011415052 -0.0012327322 -0.0013393689 -0.001415491 -0.0014496545 -0.0014598884 -0.0014612507 -0.001460967][-0.0014616166 -0.0014577719 -0.0014503868 -0.0014330877 -0.0014028559 -0.0013656593 -0.0013454712 -0.0013557366 -0.0013903882 -0.0014269296 -0.0014507964 -0.0014599069 -0.0014616289 -0.0014608759 -0.0014598547][-0.0014619358 -0.0014595872 -0.0014566164 -0.001451671 -0.0014451953 -0.0014385801 -0.0014367801 -0.0014412128 -0.001449562 -0.0014568346 -0.0014606029 -0.0014613357 -0.0014606653 -0.0014595917 -0.0014587647][-0.0014621629 -0.0014607721 -0.0014596197 -0.0014590384 -0.0014593645 -0.0014603871 -0.0014614958 -0.0014619549 -0.0014619244 -0.0014617133 -0.0014611972 -0.0014604506 -0.0014594928 -0.0014586117 -0.0014581459]]...]
INFO - root - 2017-12-09 10:28:04.815750: step 14310, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.887 sec/batch; 78h:23m:46s remains)
INFO - root - 2017-12-09 10:28:13.666416: step 14320, loss = 0.83, batch loss = 0.70 (8.8 examples/sec; 0.912 sec/batch; 80h:37m:49s remains)
INFO - root - 2017-12-09 10:28:22.325428: step 14330, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.881 sec/batch; 77h:51m:20s remains)
INFO - root - 2017-12-09 10:28:31.000076: step 14340, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.880 sec/batch; 77h:46m:24s remains)
INFO - root - 2017-12-09 10:28:39.727468: step 14350, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.874 sec/batch; 77h:12m:48s remains)
INFO - root - 2017-12-09 10:28:48.429653: step 14360, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.865 sec/batch; 76h:25m:29s remains)
INFO - root - 2017-12-09 10:28:57.073867: step 14370, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.843 sec/batch; 74h:27m:12s remains)
INFO - root - 2017-12-09 10:29:05.843338: step 14380, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.832 sec/batch; 73h:33m:07s remains)
INFO - root - 2017-12-09 10:29:14.605530: step 14390, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 76h:45m:38s remains)
INFO - root - 2017-12-09 10:29:23.201831: step 14400, loss = 0.82, batch loss = 0.69 (10.0 examples/sec; 0.802 sec/batch; 70h:53m:11s remains)
2017-12-09 10:29:24.116603: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.0030633744 0.0041821646 0.0052183392 0.0061595757 0.0068293381 0.0071575167 0.0071468567 0.0070694471 0.0070199361 0.0068410262 0.0065739313 0.0063253306 0.0061849076 0.0062542437 0.0062220348][0.0039716903 0.0052763587 0.0065291934 0.0076868674 0.0084696794 0.0088196266 0.0088200271 0.0088292388 0.0088218329 0.00869751 0.008401759 0.0082415715 0.0082208319 0.0082139708 0.0081483582][0.0047209803 0.0061566634 0.0076163029 0.0089867618 0.0098815747 0.010298762 0.010449948 0.010632638 0.010716782 0.010603835 0.010183692 0.010009524 0.0098607317 0.0099276742 0.009889394][0.0052724853 0.0069130473 0.0085666962 0.010120096 0.011222682 0.011839804 0.012205209 0.012622511 0.013044327 0.01313244 0.012848144 0.012631266 0.012540833 0.012383925 0.012015194][0.0057201376 0.0075664027 0.0094467774 0.011149011 0.012326328 0.013059998 0.013576615 0.01420399 0.014872819 0.015292862 0.015338957 0.015368279 0.015438594 0.015511185 0.015229985][0.005894254 0.007935714 0.0099337678 0.011741231 0.013013921 0.013760561 0.014267119 0.014981646 0.015868936 0.016690142 0.017169591 0.017569985 0.018042643 0.018473443 0.018536668][0.0058824727 0.0081002144 0.01017402 0.011950764 0.013191178 0.013873088 0.014305629 0.014923204 0.015956182 0.01703967 0.017859096 0.018736253 0.019596597 0.020408696 0.02082972][0.0057591209 0.0080280127 0.010110662 0.011847559 0.013039575 0.013631796 0.013933063 0.01451893 0.015557639 0.016856374 0.018000817 0.019199239 0.02027531 0.021259259 0.021912238][0.0049558543 0.0072336746 0.0093091708 0.010964497 0.012121123 0.012651114 0.012895998 0.013351891 0.014227223 0.015684968 0.017083187 0.01853973 0.019776005 0.020892179 0.021695718][0.003779439 0.0058769258 0.0077755088 0.0093186833 0.010390222 0.010834117 0.010989145 0.011371372 0.012166629 0.013486584 0.015040863 0.016684715 0.01807378 0.019220384 0.020059798][0.0024284942 0.004100753 0.0055260472 0.0066184504 0.0073585417 0.0076332558 0.0076897247 0.0079064686 0.0086234752 0.0098718191 0.011331465 0.012973312 0.014408367 0.015576059 0.016494583][0.0010613813 0.0022318377 0.0031919063 0.0038431457 0.0042274585 0.0043533975 0.0043741129 0.0045338869 0.0050985385 0.0060320115 0.0072326297 0.0085156038 0.0097103063 0.01078808 0.011650501][-0.00015558547 0.00051678577 0.0010356449 0.001366707 0.0015326054 0.0015378715 0.0015144979 0.0016407731 0.0020460014 0.0026553243 0.0034132905 0.0042644776 0.0050651729 0.0057689268 0.0064574229][-0.0009661697 -0.00067180407 -0.00044016796 -0.00029078673 -0.00022536376 -0.00023293484 -0.00025832851 -0.00022658089 -6.4613181e-05 0.00023928459 0.00057188957 0.00093493867 0.0012991023 0.0016551567 0.0020490848][-0.0013546192 -0.0012712459 -0.0012033273 -0.0011581212 -0.0011270128 -0.0011173431 -0.0011246199 -0.0011193109 -0.0010836532 -0.001027337 -0.00095990667 -0.00086855958 -0.00078963535 -0.00070054882 -0.0005708239]]...]
INFO - root - 2017-12-09 10:29:32.555363: step 14410, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 76h:19m:21s remains)
INFO - root - 2017-12-09 10:29:41.390389: step 14420, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.899 sec/batch; 79h:24m:55s remains)
INFO - root - 2017-12-09 10:29:50.351142: step 14430, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 76h:21m:04s remains)
INFO - root - 2017-12-09 10:29:58.983845: step 14440, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.884 sec/batch; 78h:04m:08s remains)
INFO - root - 2017-12-09 10:30:07.627451: step 14450, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 76h:57m:21s remains)
INFO - root - 2017-12-09 10:30:16.436370: step 14460, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.890 sec/batch; 78h:40m:05s remains)
INFO - root - 2017-12-09 10:30:25.043054: step 14470, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.847 sec/batch; 74h:47m:13s remains)
INFO - root - 2017-12-09 10:30:33.681385: step 14480, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 76h:44m:59s remains)
INFO - root - 2017-12-09 10:30:42.404201: step 14490, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.883 sec/batch; 78h:01m:30s remains)
INFO - root - 2017-12-09 10:30:51.107303: step 14500, loss = 0.82, batch loss = 0.69 (9.9 examples/sec; 0.806 sec/batch; 71h:10m:38s remains)
2017-12-09 10:30:51.928701: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0014685782 -0.0014687565 -0.0014691231 -0.0014697546 -0.0014703231 -0.0014709894 -0.0014713873 -0.0014711558 -0.0014707351 -0.0014705751 -0.0014708197 -0.0014712611 -0.0014717749 -0.0014721339 -0.0014726005][-0.0014708266 -0.0014708296 -0.0014710499 -0.0014715411 -0.0014718913 -0.0014721577 -0.0014723007 -0.001471799 -0.0014711943 -0.0014709836 -0.0014711524 -0.001471626 -0.0014722031 -0.0014726927 -0.0014730867][-0.0014732158 -0.0014730162 -0.0014730613 -0.0014730563 -0.0014727666 -0.0014722877 -0.0014718507 -0.0014710162 -0.0014704114 -0.0014704578 -0.0014709221 -0.0014714588 -0.0014720672 -0.0014725897 -0.001472819][-0.001475581 -0.0014750673 -0.0014745612 -0.0014737928 -0.0014723648 -0.0014708929 -0.0014700114 -0.0014690171 -0.0014684729 -0.0014690066 -0.0014699706 -0.0014708622 -0.001471606 -0.0014721631 -0.0014724121][-0.0014778976 -0.0014769442 -0.001475627 -0.0014739463 -0.0014712221 -0.0014684815 -0.0014668941 -0.0014659174 -0.0014655117 -0.0014663752 -0.0014678537 -0.0014692687 -0.0014702865 -0.0014709217 -0.0014714266][-0.001479667 -0.0014783411 -0.0014763693 -0.0014737907 -0.001469964 -0.0014661335 -0.0014636529 -0.0014625181 -0.0014622689 -0.0014632836 -0.0014650272 -0.0014667267 -0.0014680406 -0.0014689814 -0.0014698011][-0.0014802552 -0.001478781 -0.0014763811 -0.0014731167 -0.001468775 -0.0014643677 -0.0014613152 -0.0014599015 -0.0014596279 -0.0014605265 -0.0014620059 -0.0014635157 -0.0014647049 -0.0014659035 -0.0014671996][-0.001479573 -0.0014783463 -0.0014760706 -0.001472489 -0.0014682148 -0.0014637815 -0.0014605804 -0.0014589055 -0.0014582881 -0.0014590706 -0.0014599512 -0.0014606985 -0.0014613059 -0.001462376 -0.0014641786][-0.0014781004 -0.0014771768 -0.001475328 -0.0014719941 -0.0014681927 -0.0014644453 -0.0014617175 -0.0014599842 -0.0014590052 -0.0014593104 -0.0014593403 -0.0014589889 -0.0014588548 -0.0014597172 -0.0014619721][-0.0014762271 -0.0014754934 -0.0014741849 -0.0014715105 -0.0014685167 -0.0014657037 -0.0014634811 -0.0014619408 -0.0014609492 -0.0014607119 -0.001460062 -0.0014586626 -0.0014576814 -0.0014584218 -0.0014608285][-0.0014737068 -0.0014731752 -0.0014725096 -0.0014705034 -0.0014683502 -0.0014664083 -0.0014650119 -0.0014640637 -0.0014634422 -0.0014631023 -0.0014621663 -0.0014600909 -0.0014585012 -0.0014589552 -0.0014612069][-0.0014710692 -0.0014707043 -0.0014705241 -0.0014692859 -0.0014678952 -0.0014667636 -0.0014661154 -0.0014656928 -0.0014655694 -0.0014654056 -0.001464545 -0.0014625821 -0.0014608545 -0.0014611303 -0.0014629988][-0.0014685916 -0.0014681623 -0.0014682781 -0.0014678071 -0.0014672768 -0.0014669686 -0.0014669511 -0.0014670133 -0.0014672357 -0.0014672924 -0.0014666588 -0.0014651985 -0.0014635966 -0.0014636981 -0.0014650875][-0.0014667219 -0.0014660357 -0.0014660991 -0.0014661375 -0.0014661944 -0.0014665514 -0.0014672391 -0.0014679537 -0.0014685761 -0.0014690143 -0.0014687871 -0.0014677062 -0.0014664131 -0.0014663186 -0.001467192][-0.0014657327 -0.001464642 -0.0014643813 -0.0014644584 -0.0014648626 -0.0014656116 -0.0014667454 -0.0014681283 -0.0014693912 -0.0014704206 -0.0014707212 -0.0014701616 -0.001469348 -0.0014691275 -0.0014694147]]...]
INFO - root - 2017-12-09 10:31:00.194488: step 14510, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.858 sec/batch; 75h:49m:14s remains)
INFO - root - 2017-12-09 10:31:08.832222: step 14520, loss = 0.81, batch loss = 0.68 (9.0 examples/sec; 0.888 sec/batch; 78h:26m:53s remains)
INFO - root - 2017-12-09 10:31:17.431181: step 14530, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.861 sec/batch; 76h:00m:55s remains)
INFO - root - 2017-12-09 10:31:26.157865: step 14540, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.886 sec/batch; 78h:15m:00s remains)
INFO - root - 2017-12-09 10:31:34.999826: step 14550, loss = 0.82, batch loss = 0.69 (8.3 examples/sec; 0.962 sec/batch; 84h:58m:13s remains)
INFO - root - 2017-12-09 10:31:43.832775: step 14560, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.872 sec/batch; 76h:58m:29s remains)
INFO - root - 2017-12-09 10:31:52.534139: step 14570, loss = 0.83, batch loss = 0.70 (9.1 examples/sec; 0.883 sec/batch; 77h:56m:26s remains)
INFO - root - 2017-12-09 10:32:01.113241: step 14580, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 76h:33m:47s remains)
INFO - root - 2017-12-09 10:32:09.798829: step 14590, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.881 sec/batch; 77h:49m:46s remains)
INFO - root - 2017-12-09 10:32:18.488387: step 14600, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.855 sec/batch; 75h:31m:05s remains)
2017-12-09 10:32:19.287127: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0014378047 -0.0014365218 -0.0014361413 -0.0014355659 -0.0014346743 -0.001433928 -0.0014336584 -0.0014338503 -0.0014346152 -0.001435414 -0.0014353768 -0.0014344841 -0.0014330822 -0.0014316369 -0.001430661][-0.0014374007 -0.0014361348 -0.0014358019 -0.0014353597 -0.0014346384 -0.0014338802 -0.0014334508 -0.0014333461 -0.0014337659 -0.0014342734 -0.0014341477 -0.001433259 -0.0014319873 -0.0014307125 -0.0014299044][-0.0014379155 -0.0014368034 -0.0014366923 -0.0014363835 -0.001435732 -0.0014348836 -0.0014341441 -0.0014336326 -0.0014336684 -0.0014339541 -0.0014338244 -0.0014330851 -0.00143202 -0.001431052 -0.001430453][-0.0014388454 -0.0014378945 -0.0014379154 -0.0014377024 -0.0014369707 -0.0014359279 -0.0014348305 -0.001433894 -0.0014335945 -0.0014337393 -0.0014338171 -0.0014334321 -0.0014327611 -0.0014321904 -0.0014318639][-0.0014403398 -0.0014393694 -0.0014393582 -0.0014389787 -0.0014379747 -0.0014365772 -0.0014350656 -0.0014338139 -0.0014332982 -0.0014334726 -0.001433941 -0.0014341766 -0.0014341621 -0.0014341004 -0.0014340453][-0.0014417041 -0.0014406302 -0.00144051 -0.0014398975 -0.0014385941 -0.0014368653 -0.0014349986 -0.0014335257 -0.0014328826 -0.0014332093 -0.0014341148 -0.0014349797 -0.0014356795 -0.001436162 -0.0014363676][-0.001442316 -0.0014411183 -0.0014409071 -0.0014402089 -0.0014388005 -0.0014369019 -0.0014347988 -0.0014332059 -0.0014325144 -0.0014330401 -0.0014343113 -0.0014356553 -0.0014369573 -0.0014379303 -0.0014384454][-0.0014421466 -0.0014407715 -0.0014405217 -0.0014398993 -0.0014386325 -0.0014367915 -0.0014347121 -0.0014331442 -0.0014323993 -0.001433003 -0.0014344926 -0.0014360799 -0.0014377802 -0.0014391451 -0.0014399826][-0.001440946 -0.0014396018 -0.0014395539 -0.0014392482 -0.001438381 -0.001436897 -0.0014350933 -0.0014337093 -0.0014329858 -0.0014334327 -0.0014348255 -0.0014364342 -0.0014382415 -0.0014397622 -0.0014407865][-0.0014389604 -0.0014378707 -0.0014382353 -0.0014384553 -0.0014381644 -0.0014372275 -0.0014358528 -0.0014347222 -0.0014340979 -0.0014343277 -0.0014353897 -0.0014367738 -0.0014384002 -0.0014397913 -0.0014407811][-0.0014371942 -0.0014362955 -0.0014369471 -0.0014376367 -0.0014379043 -0.0014375591 -0.001436691 -0.0014358448 -0.0014353339 -0.00143539 -0.001436074 -0.0014370565 -0.0014382986 -0.0014393828 -0.0014401984][-0.0014362007 -0.00143538 -0.001436124 -0.0014370977 -0.0014377621 -0.0014378665 -0.0014374754 -0.0014369444 -0.0014365698 -0.0014365432 -0.0014369042 -0.0014374772 -0.0014382871 -0.0014390139 -0.0014395841][-0.0014360531 -0.0014351885 -0.0014358062 -0.0014368353 -0.0014376428 -0.0014379983 -0.0014379118 -0.0014376311 -0.0014373971 -0.0014373265 -0.0014374425 -0.0014376817 -0.0014381048 -0.0014385057 -0.0014388412][-0.0014365071 -0.0014354867 -0.001435834 -0.0014366929 -0.0014374092 -0.001437809 -0.001437858 -0.001437729 -0.0014376109 -0.0014375274 -0.0014374832 -0.0014375139 -0.0014376871 -0.0014378583 -0.0014380133][-0.0014373811 -0.0014362064 -0.0014361476 -0.0014366623 -0.001437127 -0.0014374147 -0.0014374848 -0.0014374285 -0.0014373753 -0.0014373083 -0.0014372218 -0.0014371733 -0.0014372295 -0.0014372988 -0.0014373567]]...]
INFO - root - 2017-12-09 10:32:27.730417: step 14610, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.884 sec/batch; 78h:02m:13s remains)
INFO - root - 2017-12-09 10:32:36.167189: step 14620, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.836 sec/batch; 73h:48m:10s remains)
INFO - root - 2017-12-09 10:32:44.712311: step 14630, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.870 sec/batch; 76h:48m:27s remains)
INFO - root - 2017-12-09 10:32:53.448423: step 14640, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.866 sec/batch; 76h:27m:49s remains)
INFO - root - 2017-12-09 10:33:02.045028: step 14650, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.856 sec/batch; 75h:34m:07s remains)
INFO - root - 2017-12-09 10:33:10.628053: step 14660, loss = 0.81, batch loss = 0.68 (9.0 examples/sec; 0.886 sec/batch; 78h:13m:41s remains)
INFO - root - 2017-12-09 10:33:19.110499: step 14670, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 76h:16m:24s remains)
INFO - root - 2017-12-09 10:33:27.660926: step 14680, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.877 sec/batch; 77h:23m:00s remains)
INFO - root - 2017-12-09 10:33:36.334774: step 14690, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.881 sec/batch; 77h:48m:18s remains)
INFO - root - 2017-12-09 10:33:44.915299: step 14700, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 76h:03m:37s remains)
2017-12-09 10:33:45.746064: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.00146542 -0.0014646978 -0.0014647153 -0.0014649401 -0.0014650937 -0.0014651512 -0.0014651004 -0.0014650562 -0.001465078 -0.0014651963 -0.0014653574 -0.0014655876 -0.0014657194 -0.0014657247 -0.0014656543][-0.0014652198 -0.0014645926 -0.001464635 -0.0014648848 -0.0014650763 -0.0014651479 -0.0014651139 -0.0014650801 -0.0014651261 -0.001465264 -0.0014654445 -0.0014656634 -0.0014657585 -0.0014657001 -0.0014655736][-0.0014657783 -0.0014652275 -0.0014652715 -0.0014655092 -0.001465693 -0.0014657279 -0.0014656439 -0.0014655696 -0.0014655758 -0.0014657276 -0.0014659498 -0.0014661854 -0.0014662563 -0.0014661533 -0.0014659566][-0.0014664113 -0.0014659744 -0.0014660746 -0.0014663844 -0.001466662 -0.0014667272 -0.0014666151 -0.0014665272 -0.0014664741 -0.0014665806 -0.001466766 -0.0014669531 -0.0014669403 -0.0014667233 -0.0014664026][-0.0014667613 -0.0014665427 -0.0014668326 -0.001467387 -0.0014679045 -0.0014681874 -0.0014682218 -0.0014682682 -0.0014682092 -0.0014682383 -0.0014682285 -0.0014681326 -0.001467812 -0.001467334 -0.0014667921][-0.0014669222 -0.0014669545 -0.0014674962 -0.0014683217 -0.0014690278 -0.0014694092 -0.0014695083 -0.0014695908 -0.0014695314 -0.001469497 -0.0014693388 -0.0014689947 -0.001468442 -0.0014677522 -0.0014670175][-0.0014670639 -0.0014672444 -0.0014679318 -0.001468903 -0.0014696501 -0.001470004 -0.0014700684 -0.0014700687 -0.0014699331 -0.0014698359 -0.0014695913 -0.0014691846 -0.0014686066 -0.001467874 -0.0014670563][-0.0014670186 -0.0014672283 -0.0014679165 -0.0014688431 -0.0014695444 -0.0014699851 -0.0014702522 -0.0014703537 -0.0014702675 -0.0014701365 -0.0014698021 -0.00146931 -0.0014686653 -0.0014678707 -0.001466983][-0.0014666967 -0.0014669885 -0.0014677885 -0.0014687233 -0.0014695233 -0.0014701115 -0.0014705287 -0.0014707767 -0.0014708634 -0.00147067 -0.0014701548 -0.0014695531 -0.0014688083 -0.0014678786 -0.0014668773][-0.001465901 -0.0014663783 -0.0014674198 -0.0014685612 -0.0014695922 -0.0014703256 -0.0014707994 -0.0014710359 -0.0014710695 -0.0014707127 -0.0014700539 -0.0014693451 -0.0014685622 -0.0014676818 -0.0014667407][-0.0014650061 -0.0014654385 -0.0014664773 -0.0014676513 -0.0014687376 -0.001469525 -0.0014700296 -0.0014702313 -0.0014701589 -0.0014697195 -0.0014691167 -0.0014685027 -0.0014678357 -0.0014671399 -0.0014664442][-0.0014641824 -0.001464459 -0.0014653512 -0.0014663544 -0.0014672974 -0.0014679559 -0.001468397 -0.0014684682 -0.0014683313 -0.0014679071 -0.0014674552 -0.0014671234 -0.0014667641 -0.0014663853 -0.0014659892][-0.0014636421 -0.0014637775 -0.0014645427 -0.0014654605 -0.0014663276 -0.0014668573 -0.0014670758 -0.0014669707 -0.0014667796 -0.0014663707 -0.0014660506 -0.0014659765 -0.001465936 -0.0014658375 -0.0014656523][-0.0014635847 -0.0014634352 -0.0014640471 -0.0014649255 -0.0014657343 -0.0014661376 -0.0014661765 -0.0014659682 -0.0014656783 -0.0014652797 -0.0014651171 -0.001465235 -0.0014653977 -0.0014654908 -0.0014654676][-0.0014638169 -0.0014634473 -0.0014638189 -0.0014645664 -0.0014652188 -0.001465489 -0.0014654611 -0.0014652153 -0.0014648614 -0.0014644812 -0.0014644131 -0.0014646277 -0.0014649157 -0.0014651598 -0.0014652661]]...]
INFO - root - 2017-12-09 10:33:53.913389: step 14710, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.879 sec/batch; 77h:37m:09s remains)
INFO - root - 2017-12-09 10:34:02.345968: step 14720, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.853 sec/batch; 75h:15m:25s remains)
INFO - root - 2017-12-09 10:34:10.885156: step 14730, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 76h:29m:28s remains)
INFO - root - 2017-12-09 10:34:19.314797: step 14740, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 76h:35m:25s remains)
INFO - root - 2017-12-09 10:34:27.909650: step 14750, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.870 sec/batch; 76h:47m:00s remains)
INFO - root - 2017-12-09 10:34:36.479711: step 14760, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.861 sec/batch; 75h:58m:38s remains)
INFO - root - 2017-12-09 10:34:45.127912: step 14770, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.863 sec/batch; 76h:07m:25s remains)
INFO - root - 2017-12-09 10:34:53.771303: step 14780, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.842 sec/batch; 74h:19m:31s remains)
INFO - root - 2017-12-09 10:35:02.471842: step 14790, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.886 sec/batch; 78h:12m:06s remains)
INFO - root - 2017-12-09 10:35:11.128028: step 14800, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.884 sec/batch; 77h:59m:09s remains)
2017-12-09 10:35:11.960745: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0014667971 -0.0014606237 -0.0014557391 -0.0014522305 -0.0014500327 -0.0014489027 -0.0014485423 -0.0014485586 -0.0014486039 -0.0014485492 -0.0014484384 -0.0014483106 -0.0014482139 -0.0014480979 -0.0014479837][-0.0014685469 -0.0014619444 -0.0014566523 -0.0014527371 -0.0014503128 -0.0014490812 -0.0014486894 -0.0014486217 -0.0014485683 -0.0014484272 -0.0014482423 -0.0014480416 -0.00144786 -0.0014476888 -0.0014475198][-0.0014703706 -0.001463728 -0.0014584367 -0.0014546446 -0.0014524402 -0.0014514043 -0.0014510135 -0.0014507109 -0.0014502865 -0.0014497166 -0.0014491224 -0.0014485684 -0.0014481469 -0.0014478263 -0.0014475329][-0.0014712262 -0.0014650151 -0.0014602713 -0.0014570497 -0.0014553661 -0.0014546919 -0.0014543526 -0.0014537069 -0.0014527058 -0.001451452 -0.0014502218 -0.0014491936 -0.0014485022 -0.0014480329 -0.001447647][-0.0014712054 -0.0014655681 -0.001461727 -0.0014595247 -0.0014587318 -0.0014585789 -0.0014582762 -0.0014572113 -0.0014554202 -0.0014532693 -0.0014512296 -0.0014496399 -0.0014487099 -0.0014481801 -0.0014477727][-0.0014703781 -0.0014650961 -0.0014621471 -0.0014610705 -0.0014612314 -0.0014616577 -0.0014614788 -0.0014600611 -0.0014575596 -0.0014545608 -0.0014517669 -0.0014497018 -0.0014486017 -0.0014481348 -0.0014478647][-0.0014687465 -0.001463806 -0.0014614653 -0.0014611355 -0.0014620328 -0.0014629513 -0.001462904 -0.0014612388 -0.0014584543 -0.0014550793 -0.0014518886 -0.0014495198 -0.0014483094 -0.0014479548 -0.0014479237][-0.0014661545 -0.0014617674 -0.0014600352 -0.0014602181 -0.0014615613 -0.0014628222 -0.0014628683 -0.0014611129 -0.0014582382 -0.0014547255 -0.0014513603 -0.0014488957 -0.0014476893 -0.0014475173 -0.0014478011][-0.0014625827 -0.0014588865 -0.0014577556 -0.0014582308 -0.0014595778 -0.001460857 -0.0014610278 -0.0014594847 -0.0014568859 -0.001453637 -0.0014504656 -0.0014481966 -0.0014472008 -0.0014472593 -0.0014477572][-0.0014587556 -0.0014559528 -0.0014552436 -0.0014557089 -0.0014568707 -0.0014579754 -0.0014581865 -0.001456956 -0.0014548793 -0.0014522066 -0.0014495745 -0.0014477738 -0.0014470638 -0.0014472795 -0.0014478681][-0.0014555037 -0.0014534622 -0.0014530476 -0.0014532597 -0.0014539425 -0.0014546351 -0.0014547521 -0.0014538653 -0.0014524583 -0.0014506888 -0.001448938 -0.0014477452 -0.0014472841 -0.0014475265 -0.0014480669][-0.0014532856 -0.0014517735 -0.0014515087 -0.0014515657 -0.0014518996 -0.001452222 -0.0014521499 -0.0014514868 -0.0014506655 -0.0014496844 -0.00144873 -0.0014480958 -0.001447826 -0.0014479511 -0.0014482834][-0.0014518842 -0.0014507328 -0.0014505744 -0.0014506371 -0.0014508088 -0.0014509119 -0.0014507484 -0.0014503534 -0.0014500013 -0.0014495605 -0.0014491305 -0.001448798 -0.0014485616 -0.0014485319 -0.0014486472][-0.0014513095 -0.0014502039 -0.0014500626 -0.0014501814 -0.001450283 -0.0014502743 -0.0014501348 -0.0014499367 -0.0014498446 -0.0014496748 -0.0014495035 -0.0014493444 -0.0014491719 -0.0014490918 -0.0014490772][-0.00145103 -0.0014498795 -0.0014496786 -0.0014498811 -0.0014500215 -0.0014500251 -0.0014499396 -0.0014498082 -0.0014497597 -0.001449646 -0.0014495505 -0.0014494885 -0.0014494063 -0.0014493359 -0.0014492829]]...]
INFO - root - 2017-12-09 10:35:19.938431: step 14810, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.861 sec/batch; 76h:00m:41s remains)
INFO - root - 2017-12-09 10:35:28.603232: step 14820, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.853 sec/batch; 75h:16m:14s remains)
INFO - root - 2017-12-09 10:35:37.368309: step 14830, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.850 sec/batch; 75h:00m:08s remains)
INFO - root - 2017-12-09 10:35:45.913389: step 14840, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.872 sec/batch; 76h:58m:18s remains)
INFO - root - 2017-12-09 10:35:54.648848: step 14850, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.870 sec/batch; 76h:43m:58s remains)
INFO - root - 2017-12-09 10:36:03.265636: step 14860, loss = 0.83, batch loss = 0.70 (9.2 examples/sec; 0.870 sec/batch; 76h:44m:54s remains)
INFO - root - 2017-12-09 10:36:12.030331: step 14870, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 76h:05m:36s remains)
INFO - root - 2017-12-09 10:36:20.797905: step 14880, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.889 sec/batch; 78h:27m:27s remains)
INFO - root - 2017-12-09 10:36:29.520924: step 14890, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.861 sec/batch; 75h:57m:28s remains)
INFO - root - 2017-12-09 10:36:38.128802: step 14900, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 76h:15m:41s remains)
2017-12-09 10:36:38.972018: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.00147902 -0.0014795711 -0.0014804251 -0.0014805418 -0.0014800052 -0.0014790392 -0.0014778482 -0.0014766104 -0.0014754655 -0.0014747281 -0.0014743969 -0.0014741851 -0.0014741227 -0.0014740154 -0.0014739265][-0.0014782132 -0.0014789175 -0.0014800292 -0.0014804079 -0.001480191 -0.0014795151 -0.0014784421 -0.0014769743 -0.0014754549 -0.0014742946 -0.0014736314 -0.0014733322 -0.0014731921 -0.0014730217 -0.0014728801][-0.0014785952 -0.001479405 -0.0014805135 -0.0014810323 -0.0014811762 -0.0014808648 -0.0014800251 -0.0014785446 -0.0014768712 -0.0014753704 -0.0014743152 -0.00147382 -0.0014735801 -0.0014733233 -0.0014730223][-0.0014798736 -0.0014805953 -0.001481371 -0.0014817698 -0.001482113 -0.0014821808 -0.001481593 -0.0014802411 -0.0014785455 -0.0014768962 -0.0014755318 -0.0014747567 -0.0014744733 -0.0014742202 -0.0014738918][-0.0014823406 -0.0014824995 -0.0014828077 -0.0014828434 -0.0014830317 -0.0014832212 -0.0014828256 -0.0014816674 -0.0014800737 -0.0014784008 -0.0014769654 -0.0014759916 -0.0014756387 -0.0014754336 -0.0014751208][-0.0014856638 -0.0014852172 -0.0014848637 -0.0014842883 -0.0014838632 -0.0014837825 -0.0014835162 -0.0014826057 -0.0014813149 -0.0014798671 -0.0014785368 -0.0014775378 -0.0014771536 -0.0014769443 -0.0014766019][-0.0014897718 -0.0014888521 -0.0014880255 -0.0014868627 -0.0014854772 -0.0014842509 -0.0014837175 -0.0014831482 -0.0014823301 -0.0014812156 -0.0014800555 -0.0014792249 -0.0014788376 -0.0014785836 -0.0014781783][-0.0014946647 -0.0014934868 -0.0014923298 -0.0014909268 -0.0014888045 -0.0014862824 -0.0014843114 -0.0014836323 -0.0014830914 -0.0014821765 -0.0014812943 -0.0014806842 -0.0014803602 -0.0014800311 -0.0014795633][-0.0014987336 -0.0014977427 -0.0014965401 -0.0014951481 -0.0014928541 -0.0014898458 -0.0014864346 -0.0014844851 -0.0014837305 -0.0014828128 -0.0014821172 -0.0014816808 -0.0014814268 -0.0014810268 -0.0014804698][-0.0015005879 -0.0014999581 -0.0014989671 -0.0014978746 -0.0014959394 -0.0014929542 -0.0014891116 -0.001485856 -0.0014844219 -0.001483416 -0.0014826989 -0.00148232 -0.0014820292 -0.0014815881 -0.0014808724][-0.0015005327 -0.0015004454 -0.0014998445 -0.0014989874 -0.0014973955 -0.0014948124 -0.0014911285 -0.0014872767 -0.0014852071 -0.0014840848 -0.0014833914 -0.0014829489 -0.001482526 -0.0014819467 -0.0014810647][-0.0014988382 -0.0014993839 -0.0014994441 -0.0014990568 -0.0014978732 -0.0014956525 -0.0014924404 -0.0014887628 -0.0014862046 -0.001484751 -0.0014840101 -0.0014833878 -0.0014828591 -0.0014821544 -0.0014811426][-0.0014969182 -0.0014977813 -0.001498218 -0.0014981863 -0.0014973518 -0.0014955023 -0.001492989 -0.0014898418 -0.0014873105 -0.0014855899 -0.0014846318 -0.0014838002 -0.001483185 -0.0014824121 -0.0014813052][-0.0014945498 -0.0014954541 -0.0014962222 -0.0014966489 -0.0014962605 -0.001494816 -0.0014930937 -0.0014906757 -0.0014883212 -0.0014865169 -0.001485339 -0.0014843519 -0.0014836253 -0.0014827283 -0.0014815056][-0.001492983 -0.0014937725 -0.0014947722 -0.0014954674 -0.0014954056 -0.0014943788 -0.0014931054 -0.0014912991 -0.0014892742 -0.0014874487 -0.0014861678 -0.0014851957 -0.0014843484 -0.0014832625 -0.0014818384]]...]
INFO - root - 2017-12-09 10:36:46.993308: step 14910, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 76h:39m:22s remains)
INFO - root - 2017-12-09 10:36:55.578069: step 14920, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.866 sec/batch; 76h:22m:03s remains)
INFO - root - 2017-12-09 10:37:04.113612: step 14930, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 76h:36m:18s remains)
INFO - root - 2017-12-09 10:37:12.721002: step 14940, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.870 sec/batch; 76h:45m:15s remains)
INFO - root - 2017-12-09 10:37:21.178004: step 14950, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.858 sec/batch; 75h:42m:51s remains)
INFO - root - 2017-12-09 10:37:29.845553: step 14960, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.886 sec/batch; 78h:09m:12s remains)
INFO - root - 2017-12-09 10:37:38.539954: step 14970, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 76h:40m:18s remains)
INFO - root - 2017-12-09 10:37:47.129510: step 14980, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 76h:02m:13s remains)
INFO - root - 2017-12-09 10:37:55.800923: step 14990, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 76h:03m:51s remains)
INFO - root - 2017-12-09 10:38:04.194245: step 15000, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.846 sec/batch; 74h:38m:37s remains)
2017-12-09 10:38:05.051446: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.27713454 0.27428684 0.27071431 0.26584038 0.26030684 0.25386903 0.24853356 0.24403459 0.24087049 0.23781379 0.2353283 0.23431247 0.23315731 0.23118955 0.22924781][0.283986 0.28189239 0.27850798 0.27474266 0.26965332 0.26388732 0.25939596 0.25623038 0.25463426 0.25335437 0.25246447 0.251758 0.25047761 0.24805866 0.24512064][0.28645 0.28598717 0.28414342 0.28138417 0.27751771 0.27295458 0.26956436 0.26741797 0.26712143 0.26749715 0.26800713 0.26756358 0.2660681 0.26298615 0.2585443][0.29095969 0.29305708 0.29346734 0.29238516 0.29005864 0.28696939 0.28445736 0.28281695 0.28290227 0.2840111 0.28511503 0.28468281 0.28255823 0.27873719 0.27318048][0.29485708 0.29993853 0.30258587 0.30401757 0.30350512 0.30148309 0.29970947 0.29790011 0.29760504 0.29820704 0.29983556 0.29959452 0.29754245 0.29310763 0.28677884][0.29671159 0.30414787 0.3082926 0.31142879 0.31273177 0.31226885 0.31153288 0.30957377 0.3085593 0.30851543 0.30959117 0.3084265 0.30552348 0.30090651 0.2944428][0.29716605 0.30594566 0.31014186 0.31360453 0.31550458 0.31582454 0.31523398 0.31278411 0.31093603 0.3100881 0.31004664 0.3084392 0.3053979 0.30095819 0.29525343][0.29552323 0.30539578 0.30949467 0.31189942 0.31340143 0.31339917 0.31247103 0.30921981 0.30563757 0.3032088 0.30189818 0.29985511 0.29671931 0.29283702 0.28879189][0.2895169 0.29931831 0.30234277 0.30446443 0.30536151 0.30466595 0.30295581 0.29901627 0.2940326 0.289979 0.28729352 0.28474647 0.28181571 0.27878582 0.27699772][0.28048548 0.28933161 0.29021832 0.29065204 0.2905696 0.28916711 0.28687072 0.28239179 0.27651793 0.27180046 0.268417 0.2656242 0.26358569 0.26186666 0.26248646][0.2688112 0.27586108 0.27457193 0.27298969 0.27173969 0.27019298 0.26781073 0.26379928 0.25824052 0.25343934 0.2495559 0.24592046 0.24368161 0.24303214 0.24555524][0.2581107 0.26324072 0.25947535 0.25590283 0.25313163 0.25077718 0.24812253 0.24487709 0.24007054 0.23561241 0.23171857 0.22846466 0.22703987 0.22698583 0.23053515][0.24778126 0.25177404 0.2466425 0.24207067 0.23832777 0.23546138 0.23247 0.22949295 0.22542836 0.22113498 0.2171645 0.2137977 0.21265678 0.21314262 0.21698922][0.23994827 0.24283096 0.2368232 0.2320566 0.22791988 0.22461209 0.22150862 0.21864702 0.21548502 0.21199454 0.20866606 0.20571113 0.20468383 0.2052699 0.20846912][0.23417209 0.23660547 0.23021382 0.22500491 0.22045079 0.2170148 0.2137256 0.21114781 0.20884529 0.20627426 0.20368712 0.20114662 0.20024273 0.20059419 0.20261231]]...]
INFO - root - 2017-12-09 10:38:13.139581: step 15010, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.859 sec/batch; 75h:44m:02s remains)
INFO - root - 2017-12-09 10:38:21.819532: step 15020, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 76h:46m:43s remains)
INFO - root - 2017-12-09 10:38:30.611280: step 15030, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.881 sec/batch; 77h:43m:05s remains)
INFO - root - 2017-12-09 10:38:39.377459: step 15040, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.889 sec/batch; 78h:23m:06s remains)
INFO - root - 2017-12-09 10:38:48.046504: step 15050, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 76h:38m:05s remains)
INFO - root - 2017-12-09 10:38:56.748511: step 15060, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.856 sec/batch; 75h:30m:25s remains)
INFO - root - 2017-12-09 10:39:05.455849: step 15070, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.902 sec/batch; 79h:30m:54s remains)
INFO - root - 2017-12-09 10:39:14.304039: step 15080, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.882 sec/batch; 77h:46m:25s remains)
INFO - root - 2017-12-09 10:39:23.026292: step 15090, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.840 sec/batch; 74h:05m:42s remains)
INFO - root - 2017-12-09 10:39:31.684909: step 15100, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.876 sec/batch; 77h:14m:21s remains)
2017-12-09 10:39:32.599753: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0014772934 -0.0014759728 -0.0014758403 -0.001476014 -0.0014761954 -0.0014763582 -0.0014764173 -0.001476338 -0.0014761316 -0.0014758734 -0.0014756866 -0.0014755828 -0.0014755491 -0.0014755608 -0.001475599][-0.001476211 -0.0014747402 -0.0014745332 -0.0014747597 -0.0014750639 -0.0014753228 -0.001475424 -0.0014753351 -0.0014750543 -0.0014747219 -0.001474519 -0.0014744796 -0.0014745545 -0.0014746815 -0.0014748266][-0.0014757592 -0.0014740016 -0.0014736674 -0.001473869 -0.0014742067 -0.0014745377 -0.0014746902 -0.0014746105 -0.0014742782 -0.0014739115 -0.0014737446 -0.0014738385 -0.0014741018 -0.0014744302 -0.0014747528][-0.0014750792 -0.001472984 -0.0014724089 -0.0014725314 -0.0014728296 -0.0014731996 -0.0014734644 -0.0014734755 -0.001473126 -0.0014727387 -0.001472621 -0.0014728924 -0.0014733952 -0.0014739963 -0.0014745864][-0.0014742457 -0.0014717645 -0.001470946 -0.0014709154 -0.0014711225 -0.0014714916 -0.0014718798 -0.0014719927 -0.0014716838 -0.0014713504 -0.0014713469 -0.0014718078 -0.0014725529 -0.0014734624 -0.0014743572][-0.0014734025 -0.0014706657 -0.0014696926 -0.0014694961 -0.0014695498 -0.0014698312 -0.0014702145 -0.0014703473 -0.0014700467 -0.0014697978 -0.0014699027 -0.001470542 -0.0014715288 -0.0014727737 -0.0014740039][-0.001472591 -0.0014697091 -0.0014686884 -0.0014683862 -0.0014683328 -0.001468537 -0.0014688899 -0.0014690027 -0.0014687169 -0.0014685026 -0.0014686269 -0.001469356 -0.0014705198 -0.0014720333 -0.0014735651][-0.0014719471 -0.0014689463 -0.0014680135 -0.0014677194 -0.0014676227 -0.0014677441 -0.0014680473 -0.0014681164 -0.0014678558 -0.0014676987 -0.0014678213 -0.0014685387 -0.0014697483 -0.0014714038 -0.00147314][-0.0014713005 -0.0014685054 -0.0014678417 -0.0014677498 -0.0014677782 -0.0014679062 -0.0014680787 -0.0014680021 -0.0014676513 -0.001467413 -0.0014674811 -0.0014681299 -0.0014692728 -0.001470934 -0.0014727896][-0.0014710131 -0.0014685447 -0.0014681878 -0.0014683421 -0.0014686637 -0.0014689326 -0.0014689941 -0.0014687189 -0.001468235 -0.0014679136 -0.0014678885 -0.0014684077 -0.001469405 -0.0014709295 -0.001472722][-0.0014713756 -0.001469221 -0.00146905 -0.0014694025 -0.0014699763 -0.001470434 -0.001470506 -0.0014701195 -0.0014695675 -0.0014691628 -0.0014690096 -0.0014693511 -0.0014701366 -0.0014714085 -0.0014729495][-0.0014721325 -0.0014701956 -0.0014700949 -0.0014705214 -0.0014712019 -0.0014718053 -0.0014719943 -0.0014716836 -0.0014711844 -0.0014707543 -0.0014705348 -0.0014706837 -0.0014711921 -0.0014721514 -0.0014733574][-0.0014731064 -0.0014714468 -0.0014713464 -0.0014717536 -0.0014724031 -0.0014730524 -0.0014733766 -0.0014732328 -0.0014728336 -0.0014724443 -0.0014722035 -0.0014721986 -0.0014724428 -0.0014730768 -0.0014738864][-0.0014744399 -0.0014730664 -0.0014728857 -0.0014731712 -0.0014736896 -0.0014742503 -0.0014745885 -0.0014745537 -0.0014743041 -0.0014740051 -0.0014737552 -0.0014736332 -0.0014736918 -0.0014740088 -0.0014744198][-0.001475907 -0.0014746475 -0.0014743367 -0.0014744666 -0.0014747558 -0.0014751113 -0.0014753547 -0.0014753743 -0.0014752755 -0.0014751165 -0.0014749169 -0.0014747513 -0.0014746886 -0.0014747676 -0.0014748612]]...]
INFO - root - 2017-12-09 10:39:41.042466: step 15110, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.899 sec/batch; 79h:17m:19s remains)
INFO - root - 2017-12-09 10:39:49.874509: step 15120, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.851 sec/batch; 75h:03m:43s remains)
INFO - root - 2017-12-09 10:39:58.706033: step 15130, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.859 sec/batch; 75h:45m:36s remains)
INFO - root - 2017-12-09 10:40:07.359534: step 15140, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.874 sec/batch; 77h:04m:00s remains)
INFO - root - 2017-12-09 10:40:16.055845: step 15150, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 76h:28m:45s remains)
INFO - root - 2017-12-09 10:40:24.834109: step 15160, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.866 sec/batch; 76h:22m:01s remains)
INFO - root - 2017-12-09 10:40:33.553337: step 15170, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.894 sec/batch; 78h:49m:21s remains)
INFO - root - 2017-12-09 10:40:42.192373: step 15180, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.830 sec/batch; 73h:09m:34s remains)
INFO - root - 2017-12-09 10:40:51.051112: step 15190, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.882 sec/batch; 77h:44m:16s remains)
INFO - root - 2017-12-09 10:40:59.750533: step 15200, loss = 0.81, batch loss = 0.68 (9.2 examples/sec; 0.871 sec/batch; 76h:47m:34s remains)
2017-12-09 10:41:00.605164: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0011225478 -0.0011788271 -0.0012574687 -0.0013409528 -0.0014129814 -0.001463769 -0.0014928596 -0.0015065186 -0.0015104819 -0.0015102292 -0.0015086784 -0.0015080665 -0.0015076771 -0.00150732 -0.0015072515][-0.001241661 -0.0012923377 -0.0013545195 -0.0014151679 -0.0014630118 -0.0014928693 -0.0015077327 -0.00151268 -0.0015124006 -0.0015103305 -0.0015083158 -0.0015071779 -0.0015066285 -0.0015062232 -0.0015060543][-0.0013576674 -0.0013941904 -0.0014343362 -0.0014702299 -0.0014956221 -0.0015095402 -0.0015151654 -0.0015155213 -0.0015134623 -0.0015108886 -0.0015088784 -0.0015077543 -0.0015071133 -0.0015063996 -0.0015057795][-0.0014412012 -0.0014612037 -0.001481395 -0.0014987235 -0.001510215 -0.0015157352 -0.0015176879 -0.0015167721 -0.0015144774 -0.0015121116 -0.0015102617 -0.0015090536 -0.0015080962 -0.0015068613 -0.0015058173][-0.0014887947 -0.0014951143 -0.0015024464 -0.0015096051 -0.0015152388 -0.0015179421 -0.0015186474 -0.0015181147 -0.0015162004 -0.0015140066 -0.0015122846 -0.0015110172 -0.0015096545 -0.0015078703 -0.0015063337][-0.001508993 -0.0015068698 -0.0015082811 -0.0015123223 -0.0015165391 -0.0015185153 -0.0015186066 -0.0015177391 -0.0015159809 -0.0015141751 -0.0015129076 -0.0015120435 -0.0015108516 -0.0015089205 -0.0015069846][-0.0015145655 -0.0015100406 -0.0015095889 -0.001513214 -0.001517007 -0.0015180067 -0.0015160698 -0.001513235 -0.001511147 -0.0015105172 -0.0015109113 -0.0015116633 -0.0015113279 -0.0015097156 -0.0015077061][-0.0015147626 -0.001510629 -0.0015104 -0.0015137587 -0.0015161081 -0.0015144895 -0.001509239 -0.00150357 -0.0015014497 -0.0015037664 -0.0015080842 -0.0015112433 -0.0015118893 -0.0015104816 -0.0015083459][-0.0015134237 -0.0015106441 -0.0015106893 -0.0015137881 -0.0015147056 -0.001510024 -0.0015007213 -0.0014930757 -0.0014924592 -0.0014991228 -0.0015071506 -0.0015119682 -0.0015129977 -0.0015113336 -0.0015090185][-0.0015120305 -0.001510262 -0.0015110562 -0.0015138466 -0.0015136135 -0.0015066711 -0.0014959162 -0.0014894946 -0.0014918967 -0.001500729 -0.0015087738 -0.0015131136 -0.0015138143 -0.0015120661 -0.001509574][-0.001510495 -0.0015092976 -0.0015101859 -0.001512356 -0.0015116713 -0.0015054727 -0.0014979431 -0.0014950744 -0.0014990352 -0.0015060649 -0.0015115757 -0.0015139375 -0.0015137603 -0.0015121022 -0.001509677][-0.001508749 -0.0015084449 -0.001509398 -0.0015109987 -0.0015105876 -0.001507048 -0.0015035574 -0.0015031626 -0.0015060214 -0.0015100192 -0.0015130226 -0.0015140182 -0.0015132476 -0.001511658 -0.0015094283][-0.0015077811 -0.0015076987 -0.0015085087 -0.0015095299 -0.0015094975 -0.0015082066 -0.0015073125 -0.0015081638 -0.0015099153 -0.0015117632 -0.0015129218 -0.0015135728 -0.0015127294 -0.0015109259 -0.0015087846][-0.0015072115 -0.0015067268 -0.0015070509 -0.001507744 -0.0015078463 -0.0015074809 -0.0015077697 -0.0015091179 -0.0015109098 -0.001512017 -0.0015124674 -0.0015128324 -0.0015118861 -0.0015099237 -0.0015077959][-0.0015066389 -0.0015059537 -0.0015058789 -0.0015061867 -0.0015063519 -0.0015064026 -0.0015067748 -0.0015080853 -0.0015097826 -0.0015108722 -0.0015110943 -0.0015110988 -0.00150999 -0.0015083657 -0.0015067523]]...]
INFO - root - 2017-12-09 10:41:09.033443: step 15210, loss = 0.82, batch loss = 0.69 (8.6 examples/sec; 0.925 sec/batch; 81h:34m:11s remains)
INFO - root - 2017-12-09 10:41:17.763262: step 15220, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.854 sec/batch; 75h:17m:36s remains)
INFO - root - 2017-12-09 10:41:26.606696: step 15230, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.889 sec/batch; 78h:19m:19s remains)
INFO - root - 2017-12-09 10:41:35.198786: step 15240, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.849 sec/batch; 74h:51m:45s remains)
INFO - root - 2017-12-09 10:41:43.766760: step 15250, loss = 0.81, batch loss = 0.68 (9.1 examples/sec; 0.881 sec/batch; 77h:36m:04s remains)
INFO - root - 2017-12-09 10:41:52.411492: step 15260, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.831 sec/batch; 73h:13m:27s remains)
INFO - root - 2017-12-09 10:42:01.127646: step 15270, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.878 sec/batch; 77h:20m:08s remains)
INFO - root - 2017-12-09 10:42:09.754597: step 15280, loss = 0.81, batch loss = 0.68 (9.2 examples/sec; 0.872 sec/batch; 76h:50m:10s remains)
INFO - root - 2017-12-09 10:42:18.455044: step 15290, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.887 sec/batch; 78h:06m:55s remains)
INFO - root - 2017-12-09 10:42:27.157225: step 15300, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.865 sec/batch; 76h:11m:46s remains)
2017-12-09 10:42:28.018282: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.0541097 0.054518212 0.054399014 0.054088417 0.053690095 0.05335157 0.0530852 0.052935585 0.052780244 0.052389607 0.0515459 0.050101455 0.0479456 0.045223165 0.04229565][0.056669489 0.057414863 0.057581615 0.057533477 0.057329822 0.057132397 0.056941416 0.056720857 0.056371681 0.055761758 0.054583609 0.052903391 0.050565388 0.047805794 0.044851828][0.058247607 0.05960115 0.060325593 0.060695227 0.060781624 0.060705014 0.060460504 0.059975956 0.059225153 0.058177583 0.0565763 0.054487366 0.051993579 0.049180277 0.04628158][0.059487008 0.061476205 0.062733755 0.063578621 0.063985363 0.064028613 0.063650042 0.062749013 0.0614481 0.059701044 0.05748326 0.054845218 0.052092213 0.049328912 0.046624582][0.06020458 0.062678367 0.064258114 0.065370463 0.065962456 0.06605196 0.065488331 0.064186007 0.062356334 0.05994121 0.057134956 0.054018185 0.051045336 0.04840773 0.046090476][0.060483377 0.063240588 0.064884208 0.06605605 0.06670659 0.066780142 0.06600818 0.06422165 0.061893802 0.058801115 0.055321984 0.051742766 0.048602015 0.046246346 0.044348512][0.06029281 0.062967978 0.064312451 0.065288909 0.065859273 0.065886922 0.0649813 0.062953956 0.06020771 0.05659182 0.0525773 0.048591644 0.045321837 0.043103144 0.0417554][0.058923095 0.061139975 0.061918322 0.06255959 0.063050523 0.063195378 0.062384825 0.060336582 0.057362236 0.053413238 0.049052887 0.044688754 0.041331772 0.039308213 0.038549263][0.056236155 0.057725661 0.057715043 0.057878397 0.058291491 0.058644004 0.058147196 0.056305628 0.053353749 0.049331389 0.044910327 0.040521737 0.03731095 0.03565881 0.035512038][0.053083204 0.053843442 0.053125966 0.05283561 0.053123232 0.053624336 0.053403802 0.05180442 0.049059335 0.045322381 0.041176002 0.037093986 0.034240481 0.033018541 0.033289235][0.050087374 0.050385002 0.049248952 0.048630472 0.048826519 0.049367942 0.049349695 0.048002824 0.045546167 0.042170953 0.038407989 0.034805663 0.032348443 0.031484552 0.031944286][0.047378268 0.047520012 0.04625925 0.045421015 0.045457687 0.0459045 0.045937393 0.044840705 0.04270862 0.039864738 0.036669463 0.03367494 0.031703379 0.031074416 0.031635191][0.04432448 0.044289179 0.042929061 0.041927509 0.041806497 0.042163629 0.042340003 0.0417062 0.040144376 0.037959587 0.035450947 0.033213958 0.031794265 0.031385891 0.031918179][0.040956751 0.040675737 0.039202232 0.038063608 0.037765954 0.038052239 0.038433794 0.038348045 0.037509184 0.036105778 0.034348611 0.032852218 0.031889882 0.031750884 0.032267042][0.037905518 0.037648983 0.036298897 0.035261642 0.034938883 0.03521695 0.035794005 0.036080703 0.035877787 0.035194892 0.03416181 0.033255968 0.032653205 0.032581083 0.032812666]]...]
INFO - root - 2017-12-09 10:42:36.297877: step 15310, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.895 sec/batch; 78h:52m:02s remains)
INFO - root - 2017-12-09 10:42:44.965946: step 15320, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.896 sec/batch; 78h:55m:21s remains)
INFO - root - 2017-12-09 10:42:53.759525: step 15330, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.893 sec/batch; 78h:38m:46s remains)
INFO - root - 2017-12-09 10:43:02.497501: step 15340, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 76h:20m:20s remains)
INFO - root - 2017-12-09 10:43:11.302608: step 15350, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.889 sec/batch; 78h:18m:55s remains)
INFO - root - 2017-12-09 10:43:20.170686: step 15360, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.894 sec/batch; 78h:46m:13s remains)
INFO - root - 2017-12-09 10:43:28.897926: step 15370, loss = 0.81, batch loss = 0.68 (8.8 examples/sec; 0.905 sec/batch; 79h:42m:09s remains)
INFO - root - 2017-12-09 10:43:37.618789: step 15380, loss = 0.81, batch loss = 0.68 (9.5 examples/sec; 0.840 sec/batch; 73h:58m:50s remains)
INFO - root - 2017-12-09 10:43:46.392199: step 15390, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.836 sec/batch; 73h:37m:32s remains)
INFO - root - 2017-12-09 10:43:55.207120: step 15400, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.880 sec/batch; 77h:32m:03s remains)
2017-12-09 10:43:56.081656: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.017958397 0.018713044 0.019276684 0.019693848 0.020044524 0.02040932 0.020582072 0.020608932 0.020246452 0.019438548 0.018261198 0.016808519 0.015270056 0.013703916 0.012391781][0.025448546 0.026330322 0.02696725 0.027487086 0.027958062 0.028530005 0.029015321 0.02945471 0.029443314 0.028910378 0.02782643 0.026161589 0.02420501 0.022139747 0.020236472][0.034953382 0.03609211 0.036894877 0.037562184 0.038156509 0.0388561 0.039596431 0.040443711 0.040874854 0.040656354 0.039550513 0.037621878 0.035197631 0.032509141 0.029848984][0.045392085 0.046968382 0.048057538 0.048943803 0.049655169 0.050423142 0.051380314 0.052506421 0.053288579 0.053233474 0.052089203 0.049819119 0.046804342 0.043510348 0.040087439][0.055545568 0.057462528 0.058817983 0.059906423 0.060744703 0.061611634 0.062760562 0.064107493 0.065141983 0.065257341 0.064046375 0.061455969 0.057895117 0.054021522 0.04999242][0.064349167 0.066665955 0.068281196 0.069522694 0.070434548 0.07135538 0.072548524 0.073884875 0.0748344 0.074993446 0.073525421 0.070644446 0.06670405 0.062471423 0.058048774][0.070809208 0.073508568 0.075384259 0.076802023 0.077792056 0.078674525 0.079693131 0.080880083 0.081533819 0.081544735 0.079809308 0.076723211 0.07262551 0.068125173 0.063518748][0.074280567 0.077275991 0.079298645 0.080841675 0.0819074 0.082774192 0.083621264 0.084498011 0.084694423 0.084434666 0.082565755 0.07949765 0.075369343 0.070822857 0.066346414][0.075132035 0.078226157 0.080228627 0.081774041 0.082846582 0.083718158 0.084424287 0.085041367 0.084883153 0.084400944 0.082577989 0.079629 0.075526223 0.071048819 0.0668655][0.073849477 0.0768347 0.0787173 0.08021003 0.08132489 0.0822049 0.082809009 0.083186775 0.082807392 0.082199924 0.080391362 0.077595949 0.073720478 0.069589831 0.065926865][0.071486883 0.07414899 0.07566385 0.076921113 0.077915013 0.078695282 0.079143509 0.079262987 0.078649744 0.077790163 0.076028734 0.073535249 0.0702533 0.066927463 0.064281441][0.068762772 0.070983015 0.072086729 0.073101364 0.073983751 0.074711092 0.075112529 0.075106628 0.074453585 0.073452026 0.071884751 0.069749318 0.067121647 0.064634845 0.0630017][0.066478759 0.068311796 0.069009662 0.069681339 0.070357122 0.070919067 0.071207225 0.071027361 0.070374377 0.069236279 0.067797884 0.065998547 0.063987173 0.062283095 0.061485369][0.064112075 0.065546326 0.065843023 0.066252671 0.066769242 0.067250662 0.067528218 0.067359194 0.0668382 0.065661751 0.06425561 0.062664367 0.061144274 0.060059123 0.05975027][0.062505327 0.063686267 0.063673556 0.063912615 0.064279966 0.064640716 0.064862512 0.064609773 0.064079471 0.062886626 0.06147616 0.059987884 0.058706284 0.058001883 0.057991579]]...]
INFO - root - 2017-12-09 10:44:04.373012: step 15410, loss = 0.82, batch loss = 0.69 (9.8 examples/sec; 0.813 sec/batch; 71h:35m:42s remains)
INFO - root - 2017-12-09 10:44:12.975051: step 15420, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.855 sec/batch; 75h:19m:11s remains)
INFO - root - 2017-12-09 10:44:21.581168: step 15430, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.849 sec/batch; 74h:45m:08s remains)
INFO - root - 2017-12-09 10:44:30.413758: step 15440, loss = 0.82, batch loss = 0.69 (8.4 examples/sec; 0.952 sec/batch; 83h:52m:41s remains)
INFO - root - 2017-12-09 10:44:39.151163: step 15450, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.882 sec/batch; 77h:40m:30s remains)
INFO - root - 2017-12-09 10:44:47.822461: step 15460, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.876 sec/batch; 77h:07m:41s remains)
INFO - root - 2017-12-09 10:44:56.805396: step 15470, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.892 sec/batch; 78h:32m:06s remains)
INFO - root - 2017-12-09 10:45:05.496409: step 15480, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.877 sec/batch; 77h:16m:03s remains)
INFO - root - 2017-12-09 10:45:14.199129: step 15490, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.880 sec/batch; 77h:30m:10s remains)
INFO - root - 2017-12-09 10:45:22.909215: step 15500, loss = 0.82, batch loss = 0.69 (8.8 examples/sec; 0.909 sec/batch; 80h:03m:56s remains)
2017-12-09 10:45:23.759316: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.001519878 -0.0015186985 -0.0015187536 -0.0015191066 -0.001519765 -0.001520575 -0.0015211799 -0.0015214706 -0.00152155 -0.0015213729 -0.0015208194 -0.0015201216 -0.0015193332 -0.001518533 -0.0015178946][-0.0015192572 -0.0015184131 -0.00151874 -0.0015195371 -0.0015209903 -0.0015227157 -0.0015240661 -0.0015247881 -0.0015249207 -0.0015245156 -0.0015234053 -0.001521963 -0.0015205313 -0.0015192357 -0.0015183118][-0.001519719 -0.0015192253 -0.0015200746 -0.0015215833 -0.0015239374 -0.0015265784 -0.0015285705 -0.0015295857 -0.0015295707 -0.0015287012 -0.001526825 -0.0015245108 -0.0015223955 -0.0015206866 -0.0015194861][-0.0015209788 -0.0015209751 -0.0015223281 -0.0015244544 -0.001527448 -0.0015306316 -0.0015329369 -0.0015339544 -0.001533673 -0.0015322417 -0.0015296967 -0.0015267998 -0.0015242297 -0.0015222074 -0.0015208668][-0.0015226488 -0.001523053 -0.0015246811 -0.0015271452 -0.0015303075 -0.0015334492 -0.0015356726 -0.0015364804 -0.0015357718 -0.0015339362 -0.0015312425 -0.0015283223 -0.001525647 -0.0015235511 -0.0015222338][-0.001524199 -0.0015247625 -0.0015264063 -0.0015288119 -0.0015316753 -0.0015344126 -0.0015362377 -0.0015366593 -0.0015356246 -0.0015337736 -0.0015313367 -0.0015286885 -0.0015262193 -0.001524315 -0.001523155][-0.0015253654 -0.0015256462 -0.0015269883 -0.0015289036 -0.0015310583 -0.0015330822 -0.001534329 -0.0015343182 -0.0015331877 -0.0015316815 -0.0015297148 -0.0015276022 -0.0015256766 -0.0015243283 -0.001523496][-0.0015258909 -0.0015256753 -0.0015263945 -0.0015275191 -0.0015286554 -0.0015296425 -0.0015301534 -0.0015296971 -0.0015287078 -0.0015279235 -0.0015268339 -0.0015255659 -0.0015244056 -0.0015236896 -0.0015232293][-0.0015253923 -0.0015246267 -0.0015247284 -0.0015250626 -0.0015252936 -0.0015253465 -0.0015253535 -0.0015251086 -0.0015247037 -0.001524427 -0.0015240406 -0.0015234944 -0.0015229051 -0.0015226037 -0.0015224268][-0.0015243791 -0.0015230752 -0.0015226165 -0.0015223845 -0.0015221025 -0.0015217542 -0.0015218331 -0.001522202 -0.0015224037 -0.0015223802 -0.0015221899 -0.0015218733 -0.0015215874 -0.0015214296 -0.0015213159][-0.0015232667 -0.0015215223 -0.0015206678 -0.0015200962 -0.0015196581 -0.0015192869 -0.0015194791 -0.0015201933 -0.0015208077 -0.001521019 -0.0015208969 -0.0015207356 -0.0015205988 -0.0015205406 -0.001520437][-0.0015224434 -0.0015203508 -0.0015192801 -0.0015185289 -0.001517992 -0.0015176951 -0.0015180248 -0.0015188357 -0.0015195477 -0.0015198718 -0.0015199073 -0.0015198588 -0.0015197948 -0.0015197766 -0.0015197633][-0.001521805 -0.0015196255 -0.0015185149 -0.0015178014 -0.0015172772 -0.0015171013 -0.001517613 -0.001518504 -0.0015192066 -0.0015194754 -0.0015194969 -0.001519455 -0.0015193672 -0.0015192981 -0.0015193219][-0.0015214323 -0.001519328 -0.0015183628 -0.0015178976 -0.0015175646 -0.0015175306 -0.0015180411 -0.0015188206 -0.0015193727 -0.0015195282 -0.0015195265 -0.0015194698 -0.0015193198 -0.0015191385 -0.0015191096][-0.0015213139 -0.0015194862 -0.0015186755 -0.0015184165 -0.0015182409 -0.0015182706 -0.0015186813 -0.0015192869 -0.0015196654 -0.0015196956 -0.0015196633 -0.0015196073 -0.0015194023 -0.0015191438 -0.0015190762]]...]
INFO - root - 2017-12-09 10:45:32.102810: step 15510, loss = 0.82, batch loss = 0.69 (10.4 examples/sec; 0.770 sec/batch; 67h:47m:20s remains)
INFO - root - 2017-12-09 10:45:40.665101: step 15520, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.887 sec/batch; 78h:08m:31s remains)
INFO - root - 2017-12-09 10:45:49.285222: step 15530, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.853 sec/batch; 75h:07m:45s remains)
INFO - root - 2017-12-09 10:45:57.936570: step 15540, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 76h:41m:23s remains)
INFO - root - 2017-12-09 10:46:06.735534: step 15550, loss = 0.82, batch loss = 0.70 (9.3 examples/sec; 0.861 sec/batch; 75h:47m:00s remains)
INFO - root - 2017-12-09 10:46:15.426981: step 15560, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.848 sec/batch; 74h:37m:13s remains)
INFO - root - 2017-12-09 10:46:24.197321: step 15570, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.877 sec/batch; 77h:10m:24s remains)
INFO - root - 2017-12-09 10:46:32.808649: step 15580, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.878 sec/batch; 77h:19m:50s remains)
INFO - root - 2017-12-09 10:46:41.576855: step 15590, loss = 0.83, batch loss = 0.70 (9.0 examples/sec; 0.891 sec/batch; 78h:27m:29s remains)
INFO - root - 2017-12-09 10:46:50.279900: step 15600, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.882 sec/batch; 77h:37m:49s remains)
2017-12-09 10:46:51.171849: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.010962969 0.010248916 0.0093915127 0.008449262 0.0075664544 0.0067523909 0.0061053988 0.0056222486 0.0054161916 0.0055439277 0.0060408264 0.006899822 0.0080404654 0.0091749057 0.0099837668][0.0075868866 0.0071229697 0.0066553964 0.0061629931 0.005771609 0.0054015475 0.0050858771 0.0047800825 0.0045641828 0.0045290524 0.004769267 0.0053508636 0.0062300148 0.0072097075 0.0079334509][0.0048369435 0.0045722127 0.0043801097 0.0041869548 0.0041367756 0.0040986403 0.0040804553 0.0039809188 0.0038138623 0.0037284088 0.0038379487 0.0042639081 0.00488603 0.0056478768 0.0062960354][0.0027926648 0.0026580687 0.0026417933 0.00265363 0.0028184666 0.0030016466 0.0031820759 0.0032508015 0.0032380233 0.0032611811 0.0033609732 0.0037875334 0.0044188052 0.0052107554 0.0059775254][0.0014128862 0.0012426332 0.001258618 0.0014119539 0.0017477165 0.0020701084 0.0023270296 0.0025947643 0.0026195436 0.002838084 0.0030907597 0.0037849792 0.0048348848 0.0059931418 0.0072491928][0.001349204 0.000973162 0.00084174168 0.00092576281 0.0011915574 0.0015151063 0.0018691474 0.0021621406 0.0023706509 0.0027707531 0.0032854304 0.0043638656 0.0058238576 0.0075530033 0.0093290713][0.0027690907 0.0022456029 0.0018846651 0.0017478392 0.0017589703 0.0018697891 0.0020919358 0.0024056407 0.0027610566 0.0034572207 0.0043793274 0.0058619068 0.0076735434 0.0097909719 0.011839205][0.0051179342 0.0045545008 0.0039736633 0.0035842904 0.0033188844 0.003145189 0.0031422111 0.0033494011 0.0038275402 0.0047302069 0.0060235029 0.0080074444 0.010233235 0.012594147 0.014743125][0.0078563839 0.0073459879 0.0066116643 0.0059755179 0.0054580565 0.0050313547 0.0047492282 0.0048578074 0.0054357536 0.0065363091 0.00814393 0.010271711 0.012716724 0.015280915 0.017530762][0.010885742 0.010476967 0.0096612554 0.0088842232 0.008160064 0.00749395 0.00695267 0.0069022016 0.0074130162 0.0086524822 0.010411527 0.012757856 0.01548483 0.018292218 0.02073534][0.01352155 0.013275363 0.01251355 0.011781208 0.011067821 0.010385118 0.0098392181 0.0096128825 0.010066532 0.011212654 0.012987282 0.015416026 0.018204644 0.021240233 0.023757868][0.014959749 0.014883913 0.014264936 0.013762243 0.01324808 0.012722902 0.012345714 0.012259809 0.012788843 0.013891786 0.01563821 0.017961716 0.020636499 0.023541249 0.025834505][0.015036013 0.015159935 0.014796756 0.014505722 0.014264215 0.014051743 0.013942709 0.014117455 0.014806576 0.015995974 0.017778859 0.020054363 0.02252228 0.025157437 0.027282985][0.013956381 0.014196983 0.014014028 0.013902238 0.013843631 0.013902999 0.01410391 0.014438976 0.015151778 0.016442522 0.018279415 0.020358212 0.022606064 0.025123764 0.0272743][0.012261152 0.012493649 0.012417503 0.012526449 0.012804486 0.013238622 0.013892353 0.01453669 0.015421355 0.016687447 0.018240422 0.02018358 0.022280203 0.024674039 0.026875244]]...]
INFO - root - 2017-12-09 10:46:59.546780: step 15610, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.874 sec/batch; 76h:56m:02s remains)
INFO - root - 2017-12-09 10:47:08.206617: step 15620, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 76h:30m:19s remains)
INFO - root - 2017-12-09 10:47:16.909659: step 15630, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 76h:01m:51s remains)
INFO - root - 2017-12-09 10:47:25.500599: step 15640, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.854 sec/batch; 75h:12m:24s remains)
INFO - root - 2017-12-09 10:47:33.909503: step 15650, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.849 sec/batch; 74h:45m:16s remains)
INFO - root - 2017-12-09 10:47:42.538730: step 15660, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.846 sec/batch; 74h:26m:30s remains)
INFO - root - 2017-12-09 10:47:51.119091: step 15670, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.853 sec/batch; 75h:02m:57s remains)
INFO - root - 2017-12-09 10:47:59.709723: step 15680, loss = 0.83, batch loss = 0.70 (9.7 examples/sec; 0.826 sec/batch; 72h:42m:53s remains)
INFO - root - 2017-12-09 10:48:08.331156: step 15690, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 76h:28m:03s remains)
INFO - root - 2017-12-09 10:48:16.892870: step 15700, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 76h:29m:11s remains)
2017-12-09 10:48:17.799509: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.40728876 0.40989083 0.40951383 0.40740883 0.40340868 0.39790592 0.39159548 0.38417268 0.37777531 0.3723723 0.36618268 0.36106265 0.35517755 0.34955278 0.34207249][0.42106354 0.42398691 0.42264104 0.420077 0.41551635 0.40950617 0.40281335 0.39561644 0.38925016 0.38334325 0.3765099 0.37055919 0.36460727 0.35904926 0.35214907][0.4264634 0.429233 0.42725992 0.42391825 0.41930789 0.41262656 0.40472761 0.39648429 0.3886894 0.38147795 0.37362644 0.36728886 0.36220169 0.35861319 0.35409227][0.43069953 0.43399724 0.43129686 0.42701772 0.42219129 0.41470063 0.40524441 0.39415175 0.3832725 0.37411967 0.36512181 0.35905313 0.35549179 0.35458103 0.35331789][0.43547007 0.43938828 0.4352105 0.42847481 0.42191613 0.41313377 0.40192157 0.38819206 0.37441465 0.36283749 0.35220033 0.34692052 0.34486678 0.34687448 0.34898564][0.43992308 0.44444457 0.43864897 0.43038052 0.42243484 0.41184705 0.3985073 0.38276395 0.36694542 0.353605 0.34209403 0.33716488 0.33673891 0.34047619 0.34462658][0.44801304 0.45373765 0.44701627 0.43692294 0.42703313 0.41438046 0.39860189 0.38060829 0.36326575 0.34951389 0.33896559 0.33533677 0.33663118 0.34133896 0.34584773][0.45380273 0.46283895 0.45743665 0.44694525 0.435982 0.42202774 0.4044733 0.38439015 0.36538172 0.3511675 0.3413398 0.33883703 0.34046879 0.34448811 0.34767556][0.45934597 0.47200888 0.46861655 0.45934153 0.44818228 0.4339402 0.41609439 0.39567465 0.37640056 0.36260977 0.3531509 0.35036454 0.35023212 0.35117152 0.35071582][0.45755079 0.47268835 0.47109768 0.4642089 0.45466191 0.44181189 0.42565927 0.40712371 0.38963267 0.37605092 0.36613002 0.36191332 0.35930905 0.35693625 0.35277286][0.44811979 0.46419474 0.46369866 0.45879111 0.45141277 0.44083634 0.42758763 0.41291863 0.39857212 0.38677928 0.3777464 0.37197819 0.36662644 0.36039948 0.35282671][0.42993611 0.44638315 0.44713071 0.44352371 0.437415 0.42891246 0.41839212 0.4074564 0.39615369 0.38662133 0.37914544 0.37318361 0.36646071 0.35766613 0.34823403][0.40187687 0.41812366 0.42008021 0.41846526 0.41452473 0.40835503 0.40073013 0.39275914 0.38427454 0.37656152 0.36966953 0.3634299 0.35591725 0.34666336 0.33708498][0.372173 0.38699529 0.38903341 0.38876542 0.38663688 0.38282704 0.37806892 0.37299797 0.36723837 0.36139426 0.35577035 0.34989145 0.34273297 0.33435392 0.32598311][0.34310281 0.35607249 0.35747781 0.35760087 0.35671544 0.35469833 0.35216352 0.34950557 0.34603715 0.34196922 0.33775094 0.33321521 0.32765117 0.32116866 0.31497797]]...]
INFO - root - 2017-12-09 10:48:26.255681: step 15710, loss = 0.82, batch loss = 0.69 (10.9 examples/sec; 0.731 sec/batch; 64h:21m:16s remains)
INFO - root - 2017-12-09 10:48:34.742681: step 15720, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.857 sec/batch; 75h:23m:04s remains)
INFO - root - 2017-12-09 10:48:43.383441: step 15730, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 76h:17m:38s remains)
INFO - root - 2017-12-09 10:48:52.140687: step 15740, loss = 0.82, batch loss = 0.69 (8.8 examples/sec; 0.908 sec/batch; 79h:53m:39s remains)
INFO - root - 2017-12-09 10:49:00.878148: step 15750, loss = 0.82, batch loss = 0.69 (8.8 examples/sec; 0.911 sec/batch; 80h:07m:14s remains)
INFO - root - 2017-12-09 10:49:09.596759: step 15760, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.892 sec/batch; 78h:31m:04s remains)
INFO - root - 2017-12-09 10:49:18.261608: step 15770, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 76h:39m:53s remains)
INFO - root - 2017-12-09 10:49:26.887288: step 15780, loss = 0.83, batch loss = 0.70 (9.1 examples/sec; 0.878 sec/batch; 77h:15m:23s remains)
INFO - root - 2017-12-09 10:49:35.647151: step 15790, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.875 sec/batch; 77h:00m:25s remains)
INFO - root - 2017-12-09 10:49:44.475872: step 15800, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.857 sec/batch; 75h:25m:10s remains)
2017-12-09 10:49:45.435883: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0015408373 -0.0015403556 -0.0015401852 -0.0015397293 -0.0015390055 -0.0015377771 -0.0015361247 -0.0015338117 -0.001531402 -0.0015294083 -0.0015279911 -0.0015277572 -0.0015287165 -0.001530071 -0.0015309377][-0.0015361184 -0.0015358542 -0.0015359799 -0.0015356966 -0.0015351431 -0.001534077 -0.0015324873 -0.0015303791 -0.0015281971 -0.0015263511 -0.001524987 -0.0015244548 -0.0015248596 -0.0015257483 -0.0015263016][-0.0015315849 -0.0015314596 -0.0015320251 -0.0015323929 -0.0015324554 -0.0015319352 -0.0015307001 -0.0015289263 -0.0015269748 -0.0015251802 -0.0015237839 -0.0015229521 -0.0015229037 -0.0015233661 -0.0015236287][-0.0015277155 -0.0015278931 -0.001529003 -0.0015302144 -0.0015312351 -0.0015316396 -0.0015311347 -0.0015298512 -0.001528053 -0.0015261475 -0.0015245808 -0.0015234764 -0.0015229564 -0.0015229199 -0.0015229146][-0.0015257498 -0.0015263247 -0.0015279048 -0.0015297502 -0.0015314292 -0.0015324826 -0.0015327295 -0.0015321858 -0.0015309198 -0.001529286 -0.0015277852 -0.0015265538 -0.0015255528 -0.0015248369 -0.0015242741][-0.0015249371 -0.0015255802 -0.0015271503 -0.0015291325 -0.0015311124 -0.0015326939 -0.0015337935 -0.0015342399 -0.0015339458 -0.0015330936 -0.0015320599 -0.0015309531 -0.0015297168 -0.0015285348 -0.0015275177][-0.0015241423 -0.0015245116 -0.0015257607 -0.0015275697 -0.0015295658 -0.0015314778 -0.001533292 -0.0015347418 -0.0015356092 -0.0015358491 -0.0015356449 -0.00153507 -0.0015341036 -0.0015330423 -0.0015321234][-0.0015230528 -0.0015230015 -0.0015239764 -0.0015256745 -0.0015277299 -0.0015299551 -0.001532324 -0.0015344932 -0.0015361381 -0.0015371228 -0.0015375585 -0.0015376249 -0.0015372464 -0.0015367126 -0.0015363609][-0.0015215763 -0.0015211999 -0.001522024 -0.0015235243 -0.0015255031 -0.0015279141 -0.0015307222 -0.0015334655 -0.0015356896 -0.0015372124 -0.0015381224 -0.0015386144 -0.0015386634 -0.0015386386 -0.0015389178][-0.0015201615 -0.0015196026 -0.0015202344 -0.0015213181 -0.0015228488 -0.0015249506 -0.0015276617 -0.0015304539 -0.001532944 -0.0015349593 -0.0015364403 -0.0015375136 -0.001538183 -0.0015388406 -0.0015397933][-0.0015193704 -0.0015188303 -0.0015193074 -0.0015199103 -0.001520763 -0.0015220809 -0.0015239452 -0.0015260058 -0.0015280645 -0.0015300844 -0.0015319153 -0.0015335164 -0.0015348321 -0.001536213 -0.0015379604][-0.0015196028 -0.0015190241 -0.0015193762 -0.0015195996 -0.0015198439 -0.0015203698 -0.0015212804 -0.0015223799 -0.0015236301 -0.0015250642 -0.0015266126 -0.0015281534 -0.0015295568 -0.0015311508 -0.0015332086][-0.0015199124 -0.0015193024 -0.00151944 -0.0015193573 -0.0015192432 -0.0015193064 -0.0015196146 -0.0015200826 -0.0015207097 -0.0015215633 -0.0015225864 -0.0015236625 -0.0015246907 -0.0015259048 -0.0015275802][-0.0015200631 -0.0015192276 -0.00151901 -0.001518685 -0.001518355 -0.001518179 -0.001518185 -0.0015183454 -0.0015186692 -0.0015192056 -0.00151989 -0.0015206165 -0.0015212628 -0.0015220053 -0.0015231367][-0.0015201814 -0.0015190602 -0.0015185995 -0.0015181988 -0.0015178331 -0.0015175729 -0.0015174162 -0.0015173703 -0.0015174593 -0.0015177573 -0.0015182021 -0.001518711 -0.0015191672 -0.0015196428 -0.0015203669]]...]
INFO - root - 2017-12-09 10:49:53.863808: step 15810, loss = 0.82, batch loss = 0.69 (11.0 examples/sec; 0.729 sec/batch; 64h:05m:54s remains)
INFO - root - 2017-12-09 10:50:02.392066: step 15820, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.873 sec/batch; 76h:48m:35s remains)
INFO - root - 2017-12-09 10:50:11.210998: step 15830, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 76h:24m:12s remains)
INFO - root - 2017-12-09 10:50:19.877348: step 15840, loss = 0.82, batch loss = 0.69 (8.8 examples/sec; 0.911 sec/batch; 80h:07m:29s remains)
INFO - root - 2017-12-09 10:50:28.707646: step 15850, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.874 sec/batch; 76h:54m:03s remains)
INFO - root - 2017-12-09 10:50:37.380708: step 15860, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.892 sec/batch; 78h:24m:50s remains)
INFO - root - 2017-12-09 10:50:46.195887: step 15870, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.881 sec/batch; 77h:30m:51s remains)
INFO - root - 2017-12-09 10:50:54.967844: step 15880, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.886 sec/batch; 77h:55m:40s remains)
INFO - root - 2017-12-09 10:51:03.714734: step 15890, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.874 sec/batch; 76h:49m:41s remains)
INFO - root - 2017-12-09 10:51:12.425995: step 15900, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.888 sec/batch; 78h:04m:43s remains)
2017-12-09 10:51:13.287851: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0015204474 -0.001517814 -0.0015172269 -0.0015171156 -0.0015169793 -0.0015168597 -0.0015167721 -0.0015167509 -0.0015167234 -0.0015166886 -0.0015165207 -0.0015161788 -0.0015156864 -0.0015151931 -0.001514842][-0.0015201566 -0.0015176215 -0.0015171872 -0.0015171063 -0.0015169875 -0.001516943 -0.0015169355 -0.0015169248 -0.0015168287 -0.0015166597 -0.0015161856 -0.0015153928 -0.0015144567 -0.001513636 -0.0015131978][-0.0015206285 -0.001518223 -0.0015178311 -0.0015176906 -0.0015174794 -0.0015174855 -0.0015175738 -0.0015175798 -0.0015174003 -0.0015170404 -0.0015162481 -0.0015149773 -0.0015136297 -0.0015126066 -0.0015121318][-0.0015206655 -0.0015184887 -0.00151819 -0.0015180053 -0.0015176423 -0.0015176024 -0.001517728 -0.0015177351 -0.0015175305 -0.0015170503 -0.0015159954 -0.0015143916 -0.0015127619 -0.0015116208 -0.0015111631][-0.0015201605 -0.0015181233 -0.0015179602 -0.0015178276 -0.001517339 -0.0015171139 -0.0015171649 -0.0015171317 -0.0015169142 -0.0015163556 -0.0015151707 -0.0015134069 -0.0015117527 -0.0015106492 -0.0015102518][-0.0015191956 -0.0015172459 -0.0015171965 -0.0015170692 -0.0015164312 -0.0015159453 -0.001515804 -0.001515722 -0.0015155388 -0.0015150959 -0.0015140421 -0.0015124187 -0.0015109576 -0.0015100347 -0.0015096995][-0.0015181287 -0.0015163533 -0.0015163907 -0.0015162225 -0.0015154217 -0.0015146411 -0.0015141785 -0.0015140224 -0.0015139477 -0.0015137747 -0.001513059 -0.0015117438 -0.0015105265 -0.0015098163 -0.0015095857][-0.0015171586 -0.0015156552 -0.0015158439 -0.0015157044 -0.0015148133 -0.0015137421 -0.0015128878 -0.0015125102 -0.0015125343 -0.0015125803 -0.0015121559 -0.0015111694 -0.0015101722 -0.0015096021 -0.0015094595][-0.0015161111 -0.0015149028 -0.0015153444 -0.0015153834 -0.0015145858 -0.0015134061 -0.0015122707 -0.0015115873 -0.0015114298 -0.0015113789 -0.0015109751 -0.0015101368 -0.0015092969 -0.0015088131 -0.0015088199][-0.0015145115 -0.0015136636 -0.0015143701 -0.0015146167 -0.0015140374 -0.0015129886 -0.001511876 -0.0015110909 -0.0015106334 -0.0015103134 -0.0015097469 -0.0015088737 -0.0015080749 -0.0015076853 -0.001507855][-0.0015132932 -0.0015125705 -0.0015132603 -0.0015136512 -0.0015133616 -0.0015126071 -0.0015117102 -0.0015109973 -0.0015103847 -0.0015098117 -0.0015090347 -0.0015080109 -0.0015071373 -0.0015067654 -0.0015070236][-0.0015127909 -0.0015118063 -0.0015124467 -0.0015129456 -0.0015129125 -0.0015124871 -0.0015118783 -0.0015113681 -0.0015107864 -0.0015100344 -0.0015090886 -0.0015080094 -0.0015070946 -0.0015067061 -0.0015069542][-0.0015129137 -0.001511578 -0.0015120156 -0.0015125109 -0.0015126253 -0.0015124538 -0.0015121185 -0.0015118279 -0.0015113782 -0.001510613 -0.0015096389 -0.0015086583 -0.0015078272 -0.0015074242 -0.0015075891][-0.0015134817 -0.0015116809 -0.0015118332 -0.0015122429 -0.0015124049 -0.0015124242 -0.0015123363 -0.0015122488 -0.0015119812 -0.0015113923 -0.0015105564 -0.0015097329 -0.0015090479 -0.0015086994 -0.0015087966][-0.0015142374 -0.0015121198 -0.0015119187 -0.0015121859 -0.0015123076 -0.0015124127 -0.0015125091 -0.001512595 -0.0015124955 -0.0015121452 -0.0015115977 -0.0015109933 -0.0015104822 -0.0015101818 -0.0015102011]]...]
INFO - root - 2017-12-09 10:51:21.564322: step 15910, loss = 0.82, batch loss = 0.69 (11.4 examples/sec; 0.704 sec/batch; 61h:56m:10s remains)
INFO - root - 2017-12-09 10:51:30.148844: step 15920, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 75h:58m:37s remains)
INFO - root - 2017-12-09 10:51:38.934491: step 15930, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.884 sec/batch; 77h:46m:17s remains)
INFO - root - 2017-12-09 10:51:47.550510: step 15940, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.903 sec/batch; 79h:23m:00s remains)
INFO - root - 2017-12-09 10:51:56.288081: step 15950, loss = 0.81, batch loss = 0.68 (9.2 examples/sec; 0.873 sec/batch; 76h:44m:08s remains)
INFO - root - 2017-12-09 10:52:04.897208: step 15960, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.856 sec/batch; 75h:17m:00s remains)
INFO - root - 2017-12-09 10:52:13.397750: step 15970, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 75h:45m:09s remains)
INFO - root - 2017-12-09 10:52:21.959609: step 15980, loss = 0.82, batch loss = 0.69 (9.9 examples/sec; 0.811 sec/batch; 71h:18m:49s remains)
INFO - root - 2017-12-09 10:52:30.622178: step 15990, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.849 sec/batch; 74h:40m:38s remains)
INFO - root - 2017-12-09 10:52:39.253902: step 16000, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.857 sec/batch; 75h:18m:46s remains)
2017-12-09 10:52:40.122706: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0015482976 -0.0015458491 -0.0015451693 -0.0015457646 -0.0015477543 -0.0015513493 -0.0015546388 -0.0015561568 -0.0015550428 -0.001553054 -0.0015502927 -0.001547416 -0.0015453112 -0.0015442879 -0.001544469][-0.0015548747 -0.001552658 -0.0015521287 -0.0015526174 -0.0015540757 -0.0015568372 -0.0015594027 -0.0015601644 -0.0015586519 -0.0015560865 -0.0015531181 -0.0015501704 -0.0015476999 -0.001546614 -0.0015460316][-0.0015613082 -0.0015594925 -0.001559248 -0.0015593799 -0.0015602529 -0.0015620578 -0.0015638492 -0.0015642357 -0.0015627766 -0.0015603036 -0.0015574411 -0.0015546607 -0.0015518415 -0.0015497769 -0.0015481985][-0.0015658501 -0.0015646158 -0.0015644321 -0.0015643592 -0.0015646999 -0.0015654935 -0.0015662138 -0.0015661457 -0.001564785 -0.0015627906 -0.0015602477 -0.0015573967 -0.0015543612 -0.0015516853 -0.0015491828][-0.001567688 -0.0015670264 -0.001566894 -0.0015667249 -0.0015666242 -0.0015665353 -0.0015662272 -0.0015653876 -0.0015638947 -0.0015620609 -0.0015595588 -0.0015567376 -0.0015535321 -0.0015504182 -0.0015475583][-0.0015671192 -0.0015665918 -0.0015660974 -0.0015654324 -0.0015646876 -0.0015639334 -0.001562888 -0.001561304 -0.0015594243 -0.0015574162 -0.0015549395 -0.0015520873 -0.0015486829 -0.00154559 -0.0015430915][-0.0015651974 -0.0015637763 -0.0015624507 -0.0015609892 -0.0015595061 -0.0015578767 -0.0015562065 -0.001554362 -0.001552266 -0.001550395 -0.0015483601 -0.0015458126 -0.0015427062 -0.0015400222 -0.0015384095][-0.0015625 -0.0015595431 -0.0015566613 -0.0015541441 -0.0015518107 -0.0015495935 -0.0015476866 -0.0015459475 -0.0015441014 -0.0015425817 -0.0015408404 -0.00153896 -0.0015368863 -0.0015351364 -0.0015342355][-0.0015587884 -0.0015541811 -0.0015499863 -0.0015467384 -0.0015439891 -0.001541617 -0.0015397964 -0.001538392 -0.0015369261 -0.0015357272 -0.0015343896 -0.0015331792 -0.0015321655 -0.0015314036 -0.0015311372][-0.0015551742 -0.0015494493 -0.0015446394 -0.0015409987 -0.0015382677 -0.0015362037 -0.0015347209 -0.0015337234 -0.0015326932 -0.0015317996 -0.0015307328 -0.0015299803 -0.0015296374 -0.0015294978 -0.0015295562][-0.0015513253 -0.0015454422 -0.0015407064 -0.0015370765 -0.0015345399 -0.0015330333 -0.0015320152 -0.0015314291 -0.001530889 -0.0015304087 -0.0015297398 -0.0015293313 -0.0015293388 -0.0015294187 -0.0015295215][-0.0015472057 -0.0015419298 -0.001537972 -0.0015348819 -0.0015327372 -0.0015315179 -0.0015307541 -0.001530425 -0.0015302353 -0.0015300565 -0.0015298631 -0.0015299633 -0.0015302877 -0.0015304832 -0.0015304886][-0.0015436314 -0.0015389954 -0.0015359238 -0.0015334347 -0.0015317828 -0.0015309636 -0.001530611 -0.0015305376 -0.0015304615 -0.0015303958 -0.0015304459 -0.0015307942 -0.0015312191 -0.0015313601 -0.0015311801][-0.0015409047 -0.0015370928 -0.0015349719 -0.0015331678 -0.0015320476 -0.0015315776 -0.0015313487 -0.0015312457 -0.0015311118 -0.0015310798 -0.0015312409 -0.0015316484 -0.0015319504 -0.0015318517 -0.0015314334][-0.0015385858 -0.0015358758 -0.0015346588 -0.0015334686 -0.001532806 -0.0015323717 -0.0015320105 -0.0015317046 -0.001531526 -0.0015315815 -0.0015318078 -0.0015322134 -0.0015323851 -0.0015320338 -0.0015313365]]...]
INFO - root - 2017-12-09 10:52:48.626302: step 16010, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.831 sec/batch; 73h:02m:09s remains)
INFO - root - 2017-12-09 10:52:56.950027: step 16020, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.854 sec/batch; 75h:06m:05s remains)
INFO - root - 2017-12-09 10:53:05.682667: step 16030, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 76h:13m:03s remains)
INFO - root - 2017-12-09 10:53:14.473216: step 16040, loss = 0.82, batch loss = 0.69 (8.8 examples/sec; 0.910 sec/batch; 80h:01m:39s remains)
INFO - root - 2017-12-09 10:53:23.208027: step 16050, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.884 sec/batch; 77h:40m:30s remains)
INFO - root - 2017-12-09 10:53:32.043952: step 16060, loss = 0.82, batch loss = 0.69 (8.6 examples/sec; 0.928 sec/batch; 81h:32m:23s remains)
INFO - root - 2017-12-09 10:53:40.781122: step 16070, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.870 sec/batch; 76h:30m:38s remains)
INFO - root - 2017-12-09 10:53:49.468758: step 16080, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.865 sec/batch; 75h:59m:43s remains)
INFO - root - 2017-12-09 10:53:58.211447: step 16090, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.881 sec/batch; 77h:24m:00s remains)
INFO - root - 2017-12-09 10:54:06.792281: step 16100, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.863 sec/batch; 75h:49m:56s remains)
2017-12-09 10:54:07.650984: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.001519884 -0.0014190766 -0.00095743145 0.00024370232 0.0022348708 0.0046445234 0.0067626629 0.0079653226 0.0079186205 0.0067584794 0.0049704034 0.003106829 0.0015721382 0.0003765272 -0.00049154949][-0.001344344 -0.0012409261 -0.00051145628 0.0013563097 0.0044931173 0.0084585436 0.012166788 0.014468531 0.014635415 0.012689971 0.0094257453 0.005905252 0.0029760494 0.00088689034 -0.00038336334][-0.00078769861 -0.00059206993 0.000725135 0.0039846841 0.0094245523 0.01635973 0.023007257 0.027305482 0.027878571 0.024602635 0.018789729 0.012280121 0.006681907 0.0026974597 0.00032806944][0.00027135294 0.0009254897 0.0035439704 0.009352155 0.018605243 0.030057151 0.040880367 0.047901183 0.049037065 0.044017427 0.034739789 0.023964059 0.014249672 0.0069759022 0.0023600503][0.0018730271 0.0035995336 0.0085016293 0.018122189 0.032578744 0.049850378 0.06585779 0.076220743 0.078129716 0.071164891 0.057784397 0.041711465 0.026609281 0.014664252 0.006507297][0.0037266456 0.0070862612 0.015005034 0.029059617 0.049013004 0.072051018 0.09311381 0.10694982 0.11025707 0.1024092 0.085942023 0.064990729 0.044067346 0.026361143 0.0133339][0.0054724114 0.010541474 0.021370789 0.039312836 0.06365028 0.090950221 0.11565416 0.13215317 0.13703734 0.12954746 0.11194096 0.088158086 0.062929131 0.040135875 0.022169868][0.0065684384 0.012793737 0.025388187 0.0454024 0.071733035 0.100679 0.1267688 0.14456795 0.15087296 0.1448334 0.12823032 0.10436473 0.077579811 0.051949888 0.030512109][0.0067631109 0.013039886 0.025509857 0.045027539 0.070337377 0.097894967 0.12278491 0.14019138 0.14731328 0.14323398 0.12924364 0.10786654 0.082687594 0.057418536 0.035242815][0.0060422719 0.011354585 0.02185276 0.038366169 0.059795182 0.083172306 0.10444847 0.1196702 0.12654719 0.12422637 0.11364968 0.096599251 0.07573951 0.05399847 0.034185067][0.0042227721 0.0080810748 0.01565904 0.027733777 0.043549586 0.060964826 0.076966867 0.088606335 0.094168343 0.092991129 0.085804664 0.073788889 0.058732133 0.042615972 0.027516346][0.0020892422 0.0044462997 0.0090212543 0.016465165 0.026406178 0.037553012 0.047928993 0.055559177 0.05926691 0.058650292 0.054281358 0.046898365 0.037581865 0.027473509 0.017839877][8.7167951e-05 0.0013230699 0.0036119351 0.007362335 0.012481621 0.018373206 0.023969173 0.028142257 0.030181244 0.029857941 0.027543817 0.023666125 0.018826971 0.013593264 0.008598676][-0.0011486865 -0.00068665348 0.00020049571 0.0016810491 0.0037431058 0.00618485 0.0085653635 0.010381959 0.0112843 0.011146332 0.010139533 0.0084726429 0.0064453678 0.0043070829 0.0023127354][-0.0015192053 -0.0014470769 -0.0012634578 -0.00089847896 -0.0003306194 0.00039413222 0.0011333716 0.0017202217 0.00202504 0.0019894978 0.0016684432 0.0011441768 0.000533266 -7.7575911e-05 -0.00061516045]]...]
INFO - root - 2017-12-09 10:54:16.217621: step 16110, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.872 sec/batch; 76h:36m:46s remains)
INFO - root - 2017-12-09 10:54:24.702949: step 16120, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.889 sec/batch; 78h:06m:14s remains)
INFO - root - 2017-12-09 10:54:33.191333: step 16130, loss = 0.83, batch loss = 0.70 (9.4 examples/sec; 0.852 sec/batch; 74h:54m:49s remains)
INFO - root - 2017-12-09 10:54:41.825649: step 16140, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.860 sec/batch; 75h:34m:05s remains)
INFO - root - 2017-12-09 10:54:50.408740: step 16150, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.865 sec/batch; 75h:59m:51s remains)
INFO - root - 2017-12-09 10:54:58.936569: step 16160, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.849 sec/batch; 74h:38m:30s remains)
INFO - root - 2017-12-09 10:55:07.506595: step 16170, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.833 sec/batch; 73h:14m:17s remains)
INFO - root - 2017-12-09 10:55:16.072357: step 16180, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.865 sec/batch; 76h:00m:07s remains)
INFO - root - 2017-12-09 10:55:24.774255: step 16190, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.887 sec/batch; 77h:57m:23s remains)
INFO - root - 2017-12-09 10:55:33.412528: step 16200, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.872 sec/batch; 76h:35m:14s remains)
2017-12-09 10:55:34.397350: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.0021066207 0.0032098123 0.0037331989 0.0038247912 0.0036636728 0.0033452667 0.0028087043 0.00210068 0.0015015807 0.0011859476 0.001039366 0.0008548093 0.00051605213 5.4606702e-05 -0.00048379821][0.00010506285 0.00064796419 0.00096560339 0.0012596729 0.0018194779 0.0026728965 0.0033319218 0.0033454855 0.0027232261 0.0018174509 0.00082582864 -0.00012656127 -0.00082624261 -0.0011961538 -0.0013494203][-0.0010914516 -0.00080673426 -0.00039053743 0.00040904107 0.001909391 0.0040737204 0.0060140872 0.0067407805 0.0060182912 0.00441729 0.0025102089 0.0006566837 -0.00070163788 -0.0013647504 -0.0015246375][-0.0014152896 -0.001073312 -0.00028202357 0.0012782444 0.0039212545 0.007548186 0.01089734 0.012332828 0.011315926 0.0086809164 0.0055096122 0.0024165248 6.5128668e-05 -0.0011690255 -0.0015133552][-0.0014073113 -0.00087420072 0.00042557716 0.0029136378 0.0068306196 0.01195935 0.016696895 0.018822188 0.017438732 0.013609036 0.0089675654 0.0044427132 0.00095381052 -0.00093975104 -0.0014996857][-0.0013861255 -0.00071585953 0.00096762436 0.0041536614 0.0090117017 0.015204606 0.020922443 0.023581363 0.022030735 0.017389782 0.011634368 0.0059915939 0.0016263286 -0.00076356449 -0.0014892062][-0.0013874049 -0.00068217359 0.0011497641 0.0046393834 0.009885367 0.016451195 0.022505039 0.025427803 0.023936575 0.019013055 0.012745412 0.0065930029 0.0018689104 -0.000698625 -0.0014860048][-0.0014056424 -0.00077339041 0.0009326248 0.0042540738 0.0092500309 0.015428123 0.021101812 0.023895752 0.022571586 0.017904764 0.011861671 0.005987471 0.0015726364 -0.00077831256 -0.001491281][-0.0014404057 -0.00095884193 0.00040456397 0.0031294713 0.0072599016 0.012315388 0.016895311 0.019105861 0.017955517 0.014051921 0.0090259807 0.0042575579 0.00079452922 -0.00098382751 -0.0015049293][-0.0014807848 -0.0011850912 -0.00030271756 0.0015136888 0.0043009315 0.0076854154 0.010675141 0.012024943 0.011129097 0.00843641 0.005059991 0.0019646655 -0.000180315 -0.0012313949 -0.0015227393][-0.0015163642 -0.0013887881 -0.00098221144 -0.00011689193 0.0012437778 0.0029032135 0.0043383846 0.0049335593 0.0044241259 0.0030678425 0.0014243671 -2.5208923e-05 -0.00097977056 -0.0014202028 -0.0015330969][-0.0015319556 -0.0014974013 -0.0013812041 -0.0011165058 -0.00068190321 -0.00014245987 0.00031371939 0.00048524817 0.00029767305 -0.00015108171 -0.000674263 -0.0011168702 -0.001391356 -0.0015096986 -0.0015365831][-0.0015346345 -0.0015323652 -0.0015248237 -0.0015004907 -0.0014501414 -0.0013771012 -0.0013108904 -0.0012841696 -0.0013087372 -0.0013686891 -0.0014371736 -0.0014928122 -0.0015240577 -0.0015348317 -0.0015353004][-0.0015307047 -0.001530708 -0.0015304628 -0.0015303072 -0.0015306656 -0.0015284371 -0.001526132 -0.001523571 -0.0015218733 -0.0015206123 -0.001520838 -0.001524157 -0.0015271278 -0.0015292915 -0.0015300596][-0.001488082 -0.0015000305 -0.0015074941 -0.0015110604 -0.0015148121 -0.0015040503 -0.0014574761 -0.001350651 -0.0012142204 -0.0010976163 -0.0010429563 -0.0010811861 -0.0011769301 -0.0012902984 -0.0013933815]]...]
INFO - root - 2017-12-09 10:55:42.893553: step 16210, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 75h:55m:50s remains)
INFO - root - 2017-12-09 10:55:51.312783: step 16220, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.855 sec/batch; 75h:08m:37s remains)
INFO - root - 2017-12-09 10:56:00.025562: step 16230, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.873 sec/batch; 76h:39m:14s remains)
INFO - root - 2017-12-09 10:56:08.761869: step 16240, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.888 sec/batch; 78h:00m:56s remains)
INFO - root - 2017-12-09 10:56:17.431093: step 16250, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.855 sec/batch; 75h:06m:53s remains)
INFO - root - 2017-12-09 10:56:26.071390: step 16260, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.856 sec/batch; 75h:12m:44s remains)
INFO - root - 2017-12-09 10:56:34.627797: step 16270, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.844 sec/batch; 74h:09m:27s remains)
INFO - root - 2017-12-09 10:56:43.038383: step 16280, loss = 0.82, batch loss = 0.69 (9.8 examples/sec; 0.820 sec/batch; 72h:03m:15s remains)
INFO - root - 2017-12-09 10:56:51.621098: step 16290, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.865 sec/batch; 75h:57m:41s remains)
INFO - root - 2017-12-09 10:57:00.311618: step 16300, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 76h:18m:04s remains)
2017-12-09 10:57:01.219858: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.11034767 0.10180912 0.09264113 0.08315891 0.074228704 0.066973351 0.06215816 0.060608678 0.06215246 0.067063391 0.074436329 0.082833074 0.090638578 0.097010128 0.10276119][0.10237489 0.092894934 0.083609819 0.07473734 0.066619955 0.0607124 0.057075 0.05676844 0.059064493 0.063751593 0.070087589 0.076423466 0.082276359 0.087823279 0.093936533][0.095201544 0.085477881 0.076408833 0.06778115 0.060281735 0.055096172 0.05213641 0.052419893 0.055134516 0.059814695 0.06529139 0.0700208 0.074257575 0.078822233 0.08479283][0.090972476 0.081463888 0.072449751 0.064513206 0.057777859 0.053206965 0.051000606 0.051555045 0.053815663 0.057503659 0.06146235 0.06451489 0.067504756 0.071481965 0.077365719][0.088454053 0.079657063 0.071369395 0.064212188 0.058420118 0.054452047 0.052727029 0.053224429 0.054608453 0.056721412 0.058903661 0.060816571 0.062998936 0.06651786 0.07209228][0.088709638 0.080812767 0.073272854 0.067114338 0.06244199 0.059923045 0.058734998 0.058610838 0.058530875 0.058729351 0.059289563 0.060108941 0.061791234 0.064664863 0.069435664][0.091876857 0.085260659 0.078300275 0.072426677 0.068136707 0.0661679 0.065645069 0.065641321 0.065558411 0.065131396 0.065054268 0.064890794 0.065470792 0.067005821 0.070423074][0.098207742 0.092894726 0.086677052 0.080930807 0.076809071 0.074668415 0.073614962 0.073190436 0.072892994 0.072837666 0.073078729 0.072773792 0.072534353 0.072679155 0.074589722][0.10374577 0.09939602 0.093909107 0.088964649 0.085730582 0.083992533 0.082904793 0.082116432 0.081436232 0.081467792 0.082096621 0.08222384 0.082043484 0.081537612 0.082543425][0.10908359 0.10576789 0.10098406 0.096894473 0.094535924 0.093603827 0.092901289 0.092360705 0.09222988 0.092541784 0.09327063 0.093450584 0.09298674 0.091761664 0.091979377][0.11350425 0.11124172 0.10786474 0.10547131 0.10446399 0.10497487 0.105078 0.1046504 0.10417993 0.10387877 0.1037825 0.10280459 0.10135397 0.099549085 0.099631406][0.11992945 0.11882516 0.1169333 0.1162411 0.11688317 0.11818621 0.11841562 0.11762893 0.11644728 0.11508922 0.11364616 0.11128559 0.10877498 0.10658465 0.10671931][0.12670274 0.12730119 0.12759659 0.12875524 0.13072884 0.13242406 0.13220318 0.13015759 0.12735932 0.12451242 0.12161025 0.11815615 0.11493666 0.11266553 0.11290407][0.13183153 0.13434976 0.1365726 0.13979205 0.14328596 0.1453439 0.14478764 0.14167604 0.13708061 0.1323199 0.12780666 0.12356305 0.12013453 0.11796741 0.1182918][0.13442722 0.13837345 0.14262888 0.1478586 0.15269536 0.1550888 0.15409978 0.14978068 0.14361995 0.13727549 0.13144265 0.12638664 0.12261216 0.12064184 0.12108143]]...]
INFO - root - 2017-12-09 10:57:09.717555: step 16310, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.877 sec/batch; 76h:59m:40s remains)
INFO - root - 2017-12-09 10:57:18.158058: step 16320, loss = 0.81, batch loss = 0.68 (10.1 examples/sec; 0.793 sec/batch; 69h:40m:46s remains)
INFO - root - 2017-12-09 10:57:26.847188: step 16330, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.882 sec/batch; 77h:29m:56s remains)
INFO - root - 2017-12-09 10:57:35.538397: step 16340, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.849 sec/batch; 74h:36m:16s remains)
INFO - root - 2017-12-09 10:57:44.189579: step 16350, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 76h:28m:43s remains)
INFO - root - 2017-12-09 10:57:52.947424: step 16360, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.859 sec/batch; 75h:27m:16s remains)
INFO - root - 2017-12-09 10:58:01.623752: step 16370, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.875 sec/batch; 76h:50m:40s remains)
INFO - root - 2017-12-09 10:58:10.391375: step 16380, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.880 sec/batch; 77h:17m:41s remains)
INFO - root - 2017-12-09 10:58:19.073787: step 16390, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.873 sec/batch; 76h:38m:48s remains)
INFO - root - 2017-12-09 10:58:27.872022: step 16400, loss = 0.82, batch loss = 0.69 (8.7 examples/sec; 0.916 sec/batch; 80h:23m:36s remains)
2017-12-09 10:58:28.703046: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.00138644 -0.0011130206 -0.00060951035 0.00015267159 0.0010822832 0.0019567814 0.0026249541 0.0028732242 0.0026456369 0.001959336 0.00093214167 -0.00014853943 -0.00098551274 -0.001425852 -0.0015667796][-0.0012767115 -0.000973151 -0.00041306077 0.00042062323 0.0013921892 0.0022137729 0.0027431147 0.0028347261 0.002517208 0.0018233215 0.00083884317 -0.00019711745 -0.0010018791 -0.0014271161 -0.0015656196][-0.00097597355 -0.0006674446 -0.0001163152 0.0006851363 0.0015869392 0.0022908144 0.0026685852 0.0026222365 0.0022267958 0.0015197415 0.00058575324 -0.00037059223 -0.0010890011 -0.0014537369 -0.0015677329][-0.00038997515 -7.78042e-05 0.00041433377 0.0011007101 0.00183118 0.0023404155 0.0025367513 0.0023594932 0.0019003912 0.0011875248 0.00031285151 -0.00055190152 -0.0011772575 -0.0014811662 -0.0015696187][0.00042218249 0.00079456158 0.0012292294 0.0017682684 0.002278008 0.0025466811 0.002532139 0.0022172236 0.0016932229 0.000969067 0.00014248048 -0.00065572344 -0.0012226806 -0.0014937316 -0.0015706713][0.0012168228 0.0017032532 0.0021424617 0.0025930284 0.0029267152 0.0029759612 0.0027520237 0.0022740946 0.0016494398 0.00088159018 7.3226518e-05 -0.00068984355 -0.0012291787 -0.0014914933 -0.0015707508][0.0016975885 0.0023157056 0.0028096386 0.0032508806 0.0035081427 0.0034332788 0.0030692634 0.0024542268 0.0017171535 0.0008699405 4.1171908e-05 -0.00070751092 -0.0012297233 -0.0014865935 -0.0015698157][0.0017642474 0.0024700169 0.0030306624 0.0035094474 0.0037740176 0.0036765761 0.003263142 0.0025627534 0.0017302618 0.00080018351 -4.8707821e-05 -0.00077005511 -0.0012528819 -0.0014883539 -0.0015681637][0.0014254865 0.0021528949 0.00276904 0.0032994132 0.0036122822 0.0035554804 0.0031652441 0.0024549295 0.0015878014 0.00062374957 -0.0002135681 -0.00087995967 -0.0012991304 -0.0014987506 -0.0015680954][0.00081343786 0.0014937781 0.0021206476 0.0026750723 0.0030303816 0.0030342489 0.0027039493 0.0020415389 0.0012081508 0.00029192679 -0.00046728726 -0.0010338954 -0.0013647723 -0.0015162007 -0.0015690669][9.183248e-05 0.0006493968 0.001216752 0.0017388717 0.0020966525 0.0021513873 0.0019040911 0.0013510007 0.00063478877 -0.00014211121 -0.00076108065 -0.0011978 -0.0014333797 -0.0015348638 -0.0015699754][-0.00055899203 -0.00016255933 0.00027428195 0.00069481367 0.0010026959 0.0010850194 0.00092588551 0.00051887077 -2.4898211e-05 -0.00060613884 -0.0010505635 -0.0013465794 -0.0014942264 -0.0015532303 -0.0015719015][-0.0010298234 -0.00078429712 -0.00049468211 -0.00020587037 1.5876722e-05 9.2314207e-05 3.6880374e-06 -0.00025883527 -0.00062028004 -0.0010019785 -0.0012821067 -0.0014580633 -0.0015380169 -0.0015673842 -0.0015755803][-0.0013268993 -0.0011912738 -0.0010210581 -0.000847052 -0.00070824986 -0.00065180106 -0.0006955312 -0.00084645313 -0.0010599177 -0.0012804433 -0.0014358531 -0.0015277081 -0.0015655566 -0.0015776625 -0.0015802662][-0.0014860817 -0.0014192016 -0.0013322986 -0.0012413787 -0.0011674877 -0.0011361524 -0.0011588811 -0.0012384695 -0.0013490259 -0.0014568954 -0.0015277069 -0.0015660053 -0.0015799531 -0.001583688 -0.001583801]]...]
INFO - root - 2017-12-09 10:58:37.429459: step 16410, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.897 sec/batch; 78h:47m:17s remains)
INFO - root - 2017-12-09 10:58:45.874954: step 16420, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 76h:30m:26s remains)
INFO - root - 2017-12-09 10:58:54.548284: step 16430, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.848 sec/batch; 74h:25m:08s remains)
INFO - root - 2017-12-09 10:59:03.283854: step 16440, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.881 sec/batch; 77h:19m:34s remains)
INFO - root - 2017-12-09 10:59:11.916058: step 16450, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 75h:39m:23s remains)
INFO - root - 2017-12-09 10:59:20.415256: step 16460, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.859 sec/batch; 75h:22m:19s remains)
INFO - root - 2017-12-09 10:59:28.952921: step 16470, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.849 sec/batch; 74h:29m:29s remains)
INFO - root - 2017-12-09 10:59:37.607952: step 16480, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.866 sec/batch; 76h:02m:54s remains)
INFO - root - 2017-12-09 10:59:46.374411: step 16490, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.874 sec/batch; 76h:45m:38s remains)
INFO - root - 2017-12-09 10:59:55.128706: step 16500, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.884 sec/batch; 77h:37m:37s remains)
2017-12-09 10:59:55.974799: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.054448918 0.05669244 0.059073787 0.059990745 0.060954984 0.061160717 0.061268833 0.061296325 0.061369214 0.061093953 0.057474118 0.053524338 0.049381081 0.045155376 0.042999681][0.063078187 0.065650024 0.068907052 0.071106531 0.0736832 0.0764273 0.079472274 0.082088523 0.084325694 0.084344186 0.080766365 0.075618617 0.070217706 0.065519691 0.063768916][0.080596827 0.083898939 0.088486291 0.093490854 0.099233575 0.10464478 0.11006559 0.11368025 0.1164376 0.11643352 0.11317036 0.10790993 0.10253102 0.098555386 0.097789973][0.11359568 0.11599874 0.11996666 0.12586685 0.13305396 0.14032951 0.14763556 0.15255953 0.15627873 0.1566844 0.15424754 0.15011032 0.14588071 0.14268129 0.14249046][0.15657313 0.15731497 0.15944837 0.16460121 0.17094807 0.17841728 0.18620104 0.1924236 0.19748072 0.19862732 0.19745122 0.19476715 0.1916465 0.19005711 0.19039872][0.20248176 0.20253618 0.20244093 0.20561291 0.21057844 0.21741048 0.22506016 0.23196192 0.23844466 0.24130198 0.24137895 0.23978859 0.23768443 0.23683974 0.23723628][0.24708176 0.2456577 0.24342971 0.24432302 0.24651396 0.25238311 0.25952828 0.26651111 0.27367666 0.27807167 0.28024578 0.28021997 0.27873605 0.27836472 0.27813813][0.28489196 0.28277 0.27836645 0.27694285 0.27674675 0.28092039 0.287385 0.29367232 0.30043077 0.30541554 0.30847791 0.30912355 0.30844289 0.3082583 0.30754119][0.31302404 0.31153032 0.30546361 0.30179825 0.29964083 0.30242053 0.30748814 0.31341764 0.32030576 0.32524994 0.32824358 0.32875127 0.32830876 0.32787535 0.32704625][0.32931852 0.32901257 0.32164642 0.3162325 0.31243077 0.31377536 0.31822315 0.32451972 0.33156529 0.33755261 0.34172463 0.34287915 0.34285641 0.34270695 0.34239775][0.3354111 0.33661595 0.32839093 0.32140622 0.31645635 0.31664243 0.32079715 0.32757574 0.335784 0.34377417 0.35009453 0.3532992 0.35489783 0.35575193 0.35617012][0.33136976 0.33405045 0.32572442 0.31845289 0.31294438 0.31251347 0.31640631 0.32354796 0.33251861 0.34168267 0.34996834 0.35533509 0.35903525 0.36160767 0.36312398][0.32050017 0.32453233 0.31685305 0.30954888 0.30339402 0.30152184 0.30398968 0.3104544 0.31906059 0.32911497 0.33909178 0.34652653 0.35296819 0.35786468 0.36142206][0.30111986 0.30569628 0.29891112 0.2921508 0.28574517 0.28264502 0.28357291 0.28881133 0.29696727 0.30686668 0.31749022 0.32678908 0.33518872 0.34204817 0.3471579][0.27718952 0.28188267 0.27635583 0.27021813 0.26423353 0.26067924 0.26029649 0.26372704 0.27051651 0.2792694 0.28917706 0.29892406 0.30822551 0.31596 0.32154393]]...]
INFO - root - 2017-12-09 11:00:04.635789: step 16510, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.863 sec/batch; 75h:46m:56s remains)
INFO - root - 2017-12-09 11:00:12.968871: step 16520, loss = 0.82, batch loss = 0.70 (10.0 examples/sec; 0.797 sec/batch; 69h:58m:24s remains)
INFO - root - 2017-12-09 11:00:21.412358: step 16530, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.834 sec/batch; 73h:12m:37s remains)
INFO - root - 2017-12-09 11:00:30.087621: step 16540, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.865 sec/batch; 75h:55m:42s remains)
INFO - root - 2017-12-09 11:00:38.779638: step 16550, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.892 sec/batch; 78h:14m:58s remains)
INFO - root - 2017-12-09 11:00:47.513646: step 16560, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.850 sec/batch; 74h:35m:03s remains)
INFO - root - 2017-12-09 11:00:56.173400: step 16570, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.849 sec/batch; 74h:30m:47s remains)
INFO - root - 2017-12-09 11:01:04.909082: step 16580, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 75h:48m:13s remains)
INFO - root - 2017-12-09 11:01:13.530861: step 16590, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.846 sec/batch; 74h:16m:13s remains)
INFO - root - 2017-12-09 11:01:22.364462: step 16600, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 76h:03m:31s remains)
2017-12-09 11:01:23.217997: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0013980169 -0.0013629049 -0.0013751214 -0.0014222959 -0.0014818849 -0.0015319862 -0.0015647092 -0.0015809389 -0.0015877355 -0.0015908604 -0.0015923788 -0.0015932792 -0.0015933239 -0.0015933185 -0.0015933834][-0.0012555784 -0.0012051762 -0.0012245101 -0.0013009167 -0.0013986228 -0.0014826757 -0.0015354875 -0.0015597611 -0.001571332 -0.0015786551 -0.0015850099 -0.0015899303 -0.0015932472 -0.0015947961 -0.0015946861][-0.0010743839 -0.00098961138 -0.0010009478 -0.0011020517 -0.0012494915 -0.0013910945 -0.0014946938 -0.0015535649 -0.0015793203 -0.0015885751 -0.0015912681 -0.001592707 -0.0015936882 -0.0015945061 -0.0015952145][-0.0008546905 -0.0007224601 -0.00071877346 -0.00084877259 -0.0010608719 -0.0012714268 -0.0014293951 -0.001521388 -0.0015640359 -0.0015814716 -0.0015884134 -0.0015927923 -0.0015952162 -0.0015962759 -0.0015968669][-0.00065937982 -0.00045570929 -0.00041112024 -0.00055836036 -0.00083143561 -0.0011155303 -0.0013344985 -0.0014658219 -0.00153058 -0.0015608429 -0.0015771086 -0.0015879811 -0.0015942625 -0.0015969292 -0.0015978649][-0.00052710029 -0.00027308776 -0.00019312766 -0.00034001737 -0.00064171269 -0.00096146436 -0.0012057933 -0.001354233 -0.0014366083 -0.0014906863 -0.0015319766 -0.0015642814 -0.0015850598 -0.0015952059 -0.0015985683][-0.00046048826 -0.00019850535 -0.0001102176 -0.000248853 -0.00054315187 -0.00084709487 -0.0010712408 -0.0012075682 -0.0012952016 -0.0013753267 -0.0014524426 -0.0015180105 -0.0015626209 -0.0015860733 -0.0015952578][-0.00049639842 -0.00024577195 -0.00015604426 -0.00027225423 -0.00052264356 -0.00077210605 -0.00093850761 -0.0010309434 -0.0011049635 -0.0012029815 -0.0013182728 -0.0014274962 -0.0015101224 -0.0015594305 -0.0015833606][-0.00060913677 -0.0003830638 -0.00029528548 -0.00037985924 -0.00057491881 -0.000766908 -0.00088819 -0.0009491783 -0.0010050687 -0.0011002214 -0.0012294836 -0.0013626619 -0.0014696617 -0.0015385716 -0.0015744092][-0.00085580262 -0.00066351146 -0.00057738693 -0.000625534 -0.00075953116 -0.0008964084 -0.00098395743 -0.0010263888 -0.0010666819 -0.0011406271 -0.0012501428 -0.0013697312 -0.0014696311 -0.001536414 -0.0015727594][-0.0011325272 -0.00099237007 -0.00092479226 -0.00095166452 -0.0010398475 -0.0011322404 -0.001193674 -0.0012248494 -0.0012513613 -0.0012972559 -0.0013670402 -0.0014448245 -0.0015108021 -0.0015554306 -0.0015795638][-0.0013623561 -0.0012758799 -0.0012320714 -0.0012451131 -0.0012956199 -0.0013510509 -0.0013899837 -0.0014093888 -0.0014221505 -0.0014418863 -0.001473714 -0.0015110427 -0.0015446458 -0.0015683814 -0.0015823424][-0.0014858048 -0.0014391221 -0.0014134106 -0.0014177172 -0.0014426828 -0.0014715805 -0.0014931154 -0.0015039444 -0.0015095078 -0.0015172738 -0.0015300704 -0.001546405 -0.0015626026 -0.0015751554 -0.0015835718][-0.0015430019 -0.001516089 -0.0014980109 -0.0014956651 -0.001506116 -0.0015212052 -0.0015327872 -0.0015375469 -0.0015389352 -0.0015419791 -0.0015486705 -0.0015584386 -0.0015689485 -0.0015779862 -0.0015845499][-0.0015713021 -0.001555032 -0.0015423242 -0.0015376526 -0.0015408712 -0.0015482907 -0.0015549413 -0.0015570369 -0.0015567567 -0.0015577159 -0.0015616597 -0.0015680459 -0.0015749189 -0.0015812798 -0.0015866429]]...]
INFO - root - 2017-12-09 11:01:31.678081: step 16610, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.866 sec/batch; 75h:57m:21s remains)
INFO - root - 2017-12-09 11:01:40.110028: step 16620, loss = 0.81, batch loss = 0.68 (9.4 examples/sec; 0.850 sec/batch; 74h:35m:39s remains)
INFO - root - 2017-12-09 11:01:48.559497: step 16630, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.851 sec/batch; 74h:40m:14s remains)
INFO - root - 2017-12-09 11:01:57.290615: step 16640, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.878 sec/batch; 77h:00m:50s remains)
INFO - root - 2017-12-09 11:02:05.784637: step 16650, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.830 sec/batch; 72h:49m:49s remains)
INFO - root - 2017-12-09 11:02:14.340217: step 16660, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.896 sec/batch; 78h:35m:11s remains)
INFO - root - 2017-12-09 11:02:22.989024: step 16670, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.884 sec/batch; 77h:31m:37s remains)
INFO - root - 2017-12-09 11:02:31.856911: step 16680, loss = 0.82, batch loss = 0.69 (9.7 examples/sec; 0.829 sec/batch; 72h:43m:30s remains)
INFO - root - 2017-12-09 11:02:40.671044: step 16690, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.861 sec/batch; 75h:32m:36s remains)
INFO - root - 2017-12-09 11:02:49.467400: step 16700, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.886 sec/batch; 77h:44m:44s remains)
2017-12-09 11:02:50.334895: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.036311749 0.04959115 0.063680716 0.076684527 0.086801261 0.093207024 0.095708735 0.095484488 0.093506366 0.09071549 0.087814227 0.085058786 0.081901729 0.077334605 0.070040368][0.0419881 0.0577467 0.074601568 0.09026771 0.1028102 0.11090602 0.11428588 0.11416751 0.11181165 0.10838753 0.10461306 0.10092279 0.096903905 0.0914395 0.083063468][0.045853846 0.063397124 0.082389213 0.10028931 0.11487992 0.12451185 0.12913477 0.12962121 0.12726361 0.12340742 0.11889192 0.11419999 0.10898725 0.10233197 0.092868969][0.048883494 0.067347087 0.087181009 0.10625581 0.1222517 0.13327049 0.13943452 0.14140622 0.14020945 0.13665737 0.13181913 0.12623349 0.11975688 0.11160545 0.10072445][0.050086223 0.068885051 0.088903904 0.10818125 0.12463492 0.13695027 0.14512852 0.14921534 0.14991762 0.14763866 0.14340341 0.13752311 0.13002066 0.12048748 0.10816146][0.050928336 0.069542 0.089090116 0.10781933 0.12399592 0.13702592 0.14689586 0.15346952 0.15661982 0.15647948 0.15374076 0.14833228 0.14041166 0.12971811 0.11599823][0.052256029 0.070569508 0.08936508 0.10705222 0.12231603 0.13529982 0.14613178 0.15453693 0.15972453 0.1618319 0.16089436 0.15651098 0.14863983 0.13717 0.12231199][0.054132082 0.072188184 0.090019085 0.10626887 0.12004209 0.13213933 0.14283414 0.15191013 0.158098 0.16160351 0.16198835 0.15859391 0.15097393 0.1391879 0.12369251][0.054761361 0.072427876 0.089134626 0.10363645 0.11557466 0.12613037 0.13572924 0.144438 0.15081766 0.15505213 0.15613034 0.15334494 0.14609495 0.13446255 0.11895727][0.051800448 0.068183668 0.083156154 0.095615469 0.10549924 0.11433934 0.12262512 0.13049723 0.1365764 0.14111567 0.14271238 0.14042325 0.13363656 0.12263305 0.10788128][0.045212641 0.0593745 0.071877934 0.081851684 0.089503683 0.096292019 0.10281728 0.10955501 0.11510957 0.1195623 0.12142345 0.11972322 0.11386567 0.10403213 0.090838894][0.0356271 0.046813831 0.056434274 0.0638357 0.069295466 0.074076161 0.078785978 0.083915956 0.088416025 0.092315435 0.094109431 0.092953131 0.088286005 0.0803097 0.069540463][0.024887089 0.032832976 0.039488047 0.044449229 0.047978923 0.05102925 0.054113977 0.05769543 0.061037939 0.064020991 0.065465383 0.064722918 0.061348636 0.055485539 0.047572482][0.014842368 0.019804647 0.023897426 0.026860513 0.028917808 0.030722888 0.032630727 0.034998909 0.037348323 0.039508935 0.0406331 0.040248439 0.038091477 0.034224343 0.028986905][0.0071069496 0.00973906 0.011863398 0.01336096 0.014365598 0.015286834 0.016360389 0.01782608 0.019379342 0.02087542 0.021768222 0.021725604 0.020587321 0.018366011 0.015326899]]...]
INFO - root - 2017-12-09 11:02:58.899431: step 16710, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.838 sec/batch; 73h:32m:22s remains)
INFO - root - 2017-12-09 11:03:07.416045: step 16720, loss = 0.81, batch loss = 0.68 (9.3 examples/sec; 0.860 sec/batch; 75h:23m:43s remains)
INFO - root - 2017-12-09 11:03:15.810392: step 16730, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 76h:00m:32s remains)
INFO - root - 2017-12-09 11:03:24.487442: step 16740, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.839 sec/batch; 73h:35m:50s remains)
INFO - root - 2017-12-09 11:03:33.190227: step 16750, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.861 sec/batch; 75h:28m:56s remains)
INFO - root - 2017-12-09 11:03:41.941682: step 16760, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 76h:21m:41s remains)
INFO - root - 2017-12-09 11:03:50.788055: step 16770, loss = 0.82, batch loss = 0.69 (8.3 examples/sec; 0.966 sec/batch; 84h:42m:48s remains)
INFO - root - 2017-12-09 11:03:59.508247: step 16780, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 76h:09m:56s remains)
INFO - root - 2017-12-09 11:04:08.258340: step 16790, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.877 sec/batch; 76h:52m:47s remains)
INFO - root - 2017-12-09 11:04:17.010027: step 16800, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 75h:43m:51s remains)
2017-12-09 11:04:17.855737: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0016129136 -0.0016110585 -0.0016100452 -0.0016089716 -0.001607959 -0.0016069676 -0.0016060747 -0.0016055927 -0.0016054519 -0.0016055914 -0.0016055058 -0.0016051384 -0.0016044345 -0.00160328 -0.0016020116][-0.0016146168 -0.0016135066 -0.0016129985 -0.001612148 -0.0016110671 -0.0016097435 -0.0016083175 -0.0016072281 -0.001606522 -0.001606121 -0.0016055831 -0.0016050409 -0.0016042732 -0.001603153 -0.0016018647][-0.0016161829 -0.0016160089 -0.0016159361 -0.0016153156 -0.0016141712 -0.0016125623 -0.0016107727 -0.0016091189 -0.0016077586 -0.0016068079 -0.0016060479 -0.0016053922 -0.0016045684 -0.0016034668 -0.0016021528][-0.0016166922 -0.0016169776 -0.0016173349 -0.0016170518 -0.0016160381 -0.001614476 -0.0016126754 -0.001610769 -0.0016088719 -0.0016074829 -0.0016065631 -0.0016059106 -0.0016050498 -0.0016039287 -0.0016025692][-0.001615686 -0.001616239 -0.0016168037 -0.0016167781 -0.0016161995 -0.0016151848 -0.0016135044 -0.0016112664 -0.0016091978 -0.0016078693 -0.0016071326 -0.0016065802 -0.0016056907 -0.001604484 -0.001603007][-0.0016133954 -0.0016133616 -0.0016135104 -0.0016136155 -0.0016139261 -0.0016141634 -0.0016130939 -0.0016107871 -0.0016087863 -0.0016076888 -0.0016072725 -0.0016070307 -0.0016063126 -0.0016050881 -0.0016034936][-0.0016076885 -0.0016050447 -0.0016031939 -0.0016035511 -0.0016062496 -0.0016098403 -0.0016107128 -0.0016091447 -0.0016073942 -0.0016065545 -0.0016064708 -0.0016065845 -0.0016062345 -0.0016051393 -0.0016035637][-0.0015884181 -0.0015775522 -0.0015720051 -0.0015756987 -0.0015869434 -0.0015993523 -0.0016059472 -0.0016069015 -0.0016055454 -0.0016049467 -0.0016051031 -0.0016054055 -0.0016052785 -0.0016044361 -0.0016031261][-0.0015536742 -0.001533607 -0.0015268329 -0.0015382555 -0.0015621798 -0.0015866003 -0.0016005613 -0.0016044864 -0.0016037066 -0.0016032631 -0.0016034298 -0.0016037442 -0.0016036879 -0.0016031368 -0.0016022718][-0.001513997 -0.0014912499 -0.0014913867 -0.0015140965 -0.0015488308 -0.001580257 -0.0015975597 -0.0016022611 -0.0016016875 -0.001601362 -0.0016015085 -0.001601735 -0.0016016833 -0.0016013988 -0.0016010386][-0.0014863844 -0.0014700987 -0.0014823667 -0.001514685 -0.0015533579 -0.0015830473 -0.0015974565 -0.0016003314 -0.0015996803 -0.0015993119 -0.0015994045 -0.0015996498 -0.0015997716 -0.0015997813 -0.0015998259][-0.0014962554 -0.0014913732 -0.0015102075 -0.0015408122 -0.0015711602 -0.0015907771 -0.0015986131 -0.001599257 -0.0015986012 -0.0015983086 -0.0015982633 -0.0015983918 -0.0015985458 -0.0015986913 -0.0015989679][-0.0015331815 -0.0015337516 -0.0015487645 -0.0015690491 -0.0015866114 -0.0015961434 -0.001598958 -0.0015985908 -0.0015981193 -0.0015978994 -0.0015977989 -0.0015978316 -0.0015980004 -0.0015982211 -0.0015984977][-0.0015795912 -0.0015796524 -0.0015857689 -0.0015926792 -0.0015971526 -0.0015987342 -0.0015986082 -0.0015981254 -0.0015978245 -0.0015976562 -0.0015975512 -0.0015975741 -0.0015977315 -0.0015979372 -0.0015981615][-0.0016017298 -0.0015998504 -0.0015996402 -0.001599542 -0.0015990078 -0.0015984776 -0.0015981185 -0.0015978245 -0.0015976111 -0.001597486 -0.001597434 -0.0015974604 -0.0015975931 -0.0015977599 -0.0015979144]]...]
INFO - root - 2017-12-09 11:04:26.634439: step 16810, loss = 0.81, batch loss = 0.68 (9.2 examples/sec; 0.874 sec/batch; 76h:36m:50s remains)
INFO - root - 2017-12-09 11:04:35.258859: step 16820, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.881 sec/batch; 77h:16m:38s remains)
INFO - root - 2017-12-09 11:04:43.811866: step 16830, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.870 sec/batch; 76h:17m:32s remains)
INFO - root - 2017-12-09 11:04:52.504030: step 16840, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.894 sec/batch; 78h:21m:23s remains)
INFO - root - 2017-12-09 11:05:01.133851: step 16850, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 75h:32m:45s remains)
INFO - root - 2017-12-09 11:05:09.848327: step 16860, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.866 sec/batch; 75h:58m:21s remains)
INFO - root - 2017-12-09 11:05:18.381118: step 16870, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.860 sec/batch; 75h:22m:39s remains)
INFO - root - 2017-12-09 11:05:27.062820: step 16880, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.860 sec/batch; 75h:23m:22s remains)
INFO - root - 2017-12-09 11:05:35.751883: step 16890, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 76h:07m:30s remains)
INFO - root - 2017-12-09 11:05:44.391419: step 16900, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.898 sec/batch; 78h:42m:44s remains)
2017-12-09 11:05:45.176083: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.00037628924 0.00071584573 0.0012135974 0.0016989729 0.0022598654 0.0028960698 0.0036740846 0.0045989715 0.0057425415 0.0069043171 0.007893783 0.0084963832 0.0086701186 0.00853265 0.0081099477][0.00012897386 0.00045538461 0.00095102633 0.0014245403 0.0020817795 0.00281656 0.0037718012 0.0048808437 0.0062750867 0.0076990118 0.0088619087 0.00960637 0.009830676 0.0096951276 0.0092112189][2.80143e-05 0.00037603406 0.00091851992 0.0014826059 0.0022636957 0.0031318197 0.0041729119 0.0054504136 0.0070021134 0.0085027516 0.0097210435 0.010451349 0.010689061 0.010559114 0.010015022][4.22186e-05 0.00051342929 0.0011668592 0.0018271299 0.0026652783 0.0036061553 0.0047672056 0.0061076349 0.0076988032 0.0091813849 0.010360906 0.011008975 0.01122597 0.011070753 0.010521459][0.00013362663 0.00073061627 0.0015065307 0.0022910389 0.0031813413 0.0041489787 0.0053367885 0.0066896398 0.0081504714 0.009537233 0.010557446 0.011109961 0.011236922 0.011128167 0.010624059][0.0002944133 0.0010932684 0.0019817411 0.0028324437 0.0036796762 0.0046184408 0.0056880647 0.0069676936 0.0083335536 0.0095065059 0.010339917 0.010727461 0.010754394 0.010627791 0.010238402][0.00037373859 0.0013413192 0.0023394085 0.003231352 0.0040579112 0.0049327314 0.0058584688 0.0069566714 0.0080704167 0.0091221 0.0097484915 0.010001616 0.00989844 0.0097324066 0.0093966639][0.00045373733 0.0015029365 0.0025662214 0.0035247274 0.0043439986 0.0051566726 0.0059126364 0.0068142377 0.0076713907 0.0084520848 0.0088295 0.008970757 0.0087829251 0.0086151827 0.00832906][0.00052030222 0.0015869723 0.0026546617 0.0036252241 0.0044353781 0.0052440092 0.0059041288 0.00661695 0.0072451597 0.0078575462 0.0080391848 0.0080393543 0.0077696815 0.0075568715 0.0073078293][0.00052216626 0.0015406718 0.0025804767 0.0035641335 0.0044104513 0.005229542 0.00589919 0.0065568062 0.0070483936 0.0074949646 0.0075179739 0.0074331118 0.007032047 0.0067536924 0.0065489281][0.0003963327 0.0013607799 0.0023546906 0.0033176795 0.0041850768 0.0050535658 0.0058035757 0.0065235011 0.00706398 0.0074163806 0.0073447172 0.007120966 0.0066949008 0.0063995617 0.0061687334][6.1190454e-05 0.00088790455 0.0018071358 0.0027008143 0.0035538704 0.0044311676 0.0052751331 0.0060620895 0.0066805249 0.0071467804 0.00713389 0.0068930322 0.0064237248 0.0061843442 0.006033293][-0.00029444625 0.00034545735 0.0010704896 0.0018303667 0.0025749521 0.003393743 0.0042492673 0.005096742 0.0057974821 0.0063264119 0.006397781 0.0063000917 0.0059698587 0.005831629 0.0057816058][-0.00068038865 -0.00022184814 0.00030695181 0.00089284265 0.0014841836 0.0021837526 0.002980557 0.0038062476 0.0044989726 0.0050393092 0.005217311 0.0052261762 0.0050245486 0.0050239516 0.0050873859][-0.0010292846 -0.00072370382 -0.00035829935 5.5311597e-05 0.00050633936 0.0010689911 0.0017300416 0.0024437143 0.0030273867 0.0035052665 0.00369181 0.0037582449 0.0036762289 0.0037644932 0.0038815786]]...]
INFO - root - 2017-12-09 11:05:53.797456: step 16910, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.857 sec/batch; 75h:06m:48s remains)
INFO - root - 2017-12-09 11:06:02.261275: step 16920, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.857 sec/batch; 75h:09m:45s remains)
INFO - root - 2017-12-09 11:06:10.775920: step 16930, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.857 sec/batch; 75h:05m:51s remains)
INFO - root - 2017-12-09 11:06:19.287803: step 16940, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.832 sec/batch; 72h:55m:54s remains)
INFO - root - 2017-12-09 11:06:27.904532: step 16950, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.880 sec/batch; 77h:05m:40s remains)
INFO - root - 2017-12-09 11:06:36.558453: step 16960, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.876 sec/batch; 76h:48m:30s remains)
INFO - root - 2017-12-09 11:06:45.303935: step 16970, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.853 sec/batch; 74h:46m:29s remains)
INFO - root - 2017-12-09 11:06:54.089167: step 16980, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.879 sec/batch; 77h:00m:47s remains)
INFO - root - 2017-12-09 11:07:02.850078: step 16990, loss = 0.81, batch loss = 0.68 (8.8 examples/sec; 0.912 sec/batch; 79h:53m:11s remains)
INFO - root - 2017-12-09 11:07:11.574225: step 17000, loss = 0.82, batch loss = 0.69 (10.1 examples/sec; 0.792 sec/batch; 69h:23m:22s remains)
2017-12-09 11:07:12.514802: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.0011191103 0.001000742 0.00067994278 0.0004195373 0.00019629602 -1.154549e-05 -0.00024201232 -0.00052415556 -0.0008486578 -0.0011732636 -0.0014315867 -0.0015713917 -0.0016110281 -0.0016135121 -0.0016127456][0.0029802506 0.0029192863 0.0024741672 0.0021108997 0.0017759046 0.0014184229 0.00097674481 0.00044146809 -0.00016130263 -0.00074326817 -0.0012150756 -0.0014976086 -0.0015991168 -0.0016130342 -0.0016121123][0.0053715706 0.0053836303 0.0047939476 0.0042920196 0.0038265942 0.003345717 0.0027347978 0.0019687831 0.0010352819 5.7075289e-05 -0.00080134976 -0.0013456104 -0.0015667918 -0.001611814 -0.0016113983][0.0079137664 0.0081266705 0.0075518666 0.0070366664 0.006432537 0.0057227276 0.0048222858 0.003776625 0.0025124757 0.0011425293 -0.00015621597 -0.0010703619 -0.0015021216 -0.0016095104 -0.0016107][0.0096822623 0.010234524 0.0099388622 0.0096131191 0.0090536112 0.0082910545 0.0072056809 0.0058905659 0.0042475779 0.0024110673 0.00060829963 -0.00072133145 -0.0014033378 -0.0016044242 -0.0016099198][0.010351221 0.011289611 0.01137698 0.011361629 0.010951851 0.010206417 0.0090286406 0.0075723454 0.0057082446 0.0035601386 0.0013569773 -0.00035454135 -0.0012926051 -0.0015940198 -0.0016077746][0.0097520277 0.011123936 0.01169837 0.012007804 0.01176435 0.011071629 0.0098373108 0.0082824742 0.0063048359 0.0040560393 0.0017243794 -0.00013536145 -0.0012087473 -0.0015794702 -0.0016040184][0.0080486238 0.0097198253 0.010740883 0.011357264 0.01132923 0.010730739 0.0095181214 0.0079691214 0.0060064271 0.0038238566 0.001595288 -0.00016064697 -0.0011927059 -0.0015654026 -0.0015995792][0.0055492045 0.0071571721 0.0083778705 0.0091968635 0.00938623 0.0089847678 0.0079840524 0.00663542 0.0048956163 0.002975113 0.0010514427 -0.00041489897 -0.0012624215 -0.0015652745 -0.0015979899][0.0027539169 0.0039930968 0.005092442 0.0059116418 0.006248638 0.0061007263 0.0054597529 0.0045044245 0.0031993829 0.0017198778 0.00026735419 -0.00079534744 -0.001381126 -0.0015784269 -0.0016003953][0.00042964425 0.0011796455 0.0019161718 0.0025271412 0.002854347 0.002876573 0.0025659783 0.0020311105 0.0012384257 0.00031463057 -0.00057313696 -0.0011930193 -0.0015043146 -0.0015954418 -0.0016049544][-0.00095423713 -0.00062998012 -0.00026921986 5.6247343e-05 0.00026298838 0.0003252388 0.00020940392 -2.2438588e-05 -0.00039651431 -0.00082795654 -0.0012200574 -0.0014711694 -0.001578834 -0.0016043953 -0.0016071231][-0.0015006406 -0.0014089404 -0.0012918395 -0.0011669518 -0.0010711122 -0.0010356526 -0.0010746898 -0.0011590321 -0.0012903214 -0.0014235697 -0.0015317494 -0.0015876456 -0.001604425 -0.0016064424 -0.0016065805][-0.0016030686 -0.0015942509 -0.0015802034 -0.0015592717 -0.0015382139 -0.0015250605 -0.0015272036 -0.0015441945 -0.0015692863 -0.0015892878 -0.0016014097 -0.0016054733 -0.0016065495 -0.0016069775 -0.001606854][-0.0016051884 -0.001601964 -0.0016009462 -0.0016002413 -0.0016003152 -0.0016010962 -0.0016018578 -0.0016022726 -0.0016029248 -0.0016044166 -0.0016057941 -0.0016067842 -0.0016071266 -0.0016074089 -0.0016068472]]...]
INFO - root - 2017-12-09 11:07:21.290460: step 17010, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 75h:42m:24s remains)
INFO - root - 2017-12-09 11:07:29.970767: step 17020, loss = 0.81, batch loss = 0.68 (9.3 examples/sec; 0.862 sec/batch; 75h:31m:31s remains)
INFO - root - 2017-12-09 11:07:38.462949: step 17030, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.838 sec/batch; 73h:23m:30s remains)
INFO - root - 2017-12-09 11:07:47.147863: step 17040, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.850 sec/batch; 74h:31m:18s remains)
INFO - root - 2017-12-09 11:07:55.772606: step 17050, loss = 0.82, batch loss = 0.69 (9.7 examples/sec; 0.829 sec/batch; 72h:37m:18s remains)
INFO - root - 2017-12-09 11:08:04.418464: step 17060, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.845 sec/batch; 74h:00m:23s remains)
INFO - root - 2017-12-09 11:08:12.994516: step 17070, loss = 0.82, batch loss = 0.69 (9.7 examples/sec; 0.824 sec/batch; 72h:12m:18s remains)
INFO - root - 2017-12-09 11:08:21.701675: step 17080, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.843 sec/batch; 73h:49m:58s remains)
INFO - root - 2017-12-09 11:08:30.223539: step 17090, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.878 sec/batch; 76h:53m:21s remains)
INFO - root - 2017-12-09 11:08:38.815175: step 17100, loss = 0.82, batch loss = 0.69 (10.5 examples/sec; 0.761 sec/batch; 66h:41m:17s remains)
2017-12-09 11:08:39.684305: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.11339478 0.11524818 0.11532895 0.11534157 0.11521259 0.11433751 0.11338572 0.11206033 0.10977099 0.10745297 0.10524905 0.10284404 0.10017376 0.09730389 0.094626188][0.12362841 0.1268978 0.12784941 0.12839934 0.12841183 0.12772439 0.12622634 0.12396139 0.12094951 0.11804198 0.11521287 0.11205494 0.10897219 0.10586084 0.10267355][0.12486269 0.12970099 0.1314628 0.13282162 0.13326667 0.13261431 0.13085806 0.12810236 0.12442458 0.12066706 0.11702025 0.11314609 0.10945222 0.10588674 0.10264944][0.12357597 0.13006261 0.13301498 0.13494895 0.13550982 0.13456573 0.13223325 0.12864542 0.12428468 0.11978054 0.11529569 0.11071141 0.10634921 0.10229777 0.098456174][0.12028295 0.1281566 0.13192913 0.13437937 0.13511805 0.13390578 0.13124472 0.12737302 0.1226485 0.11758787 0.11221806 0.1067887 0.10128897 0.096043408 0.091177285][0.11349468 0.12232623 0.12650448 0.12894838 0.12959681 0.12829222 0.12568216 0.12200497 0.11748917 0.11235411 0.10640524 0.099925809 0.092856176 0.085833818 0.079229467][0.10161855 0.10965597 0.11327066 0.11523093 0.11552747 0.11406963 0.11194834 0.10912388 0.10550091 0.10103939 0.095290788 0.088465892 0.080490932 0.072227076 0.06423872][0.083838053 0.089789741 0.0918836 0.092685439 0.0922726 0.090702429 0.089149557 0.087411143 0.085196257 0.081876822 0.077006765 0.070758231 0.063058659 0.054805942 0.046621844][0.060898289 0.064560995 0.065012738 0.064414814 0.06304922 0.061317198 0.060190037 0.059303731 0.058286235 0.056315027 0.05288671 0.047999062 0.0418355 0.035208747 0.028561601][0.037026331 0.038420625 0.03780067 0.036443993 0.034654967 0.032873541 0.031882823 0.0315333 0.031320885 0.030395346 0.028333373 0.025212755 0.02127702 0.017003862 0.01274408][0.017892215 0.018170226 0.017441353 0.016172417 0.014604453 0.013132075 0.012291249 0.012052491 0.012030867 0.011675201 0.010716816 0.0092058983 0.0073422408 0.0053547844 0.0034141652][0.0057504037 0.0057029384 0.0052551348 0.0045341225 0.003646059 0.0028027804 0.0022914186 0.0021444361 0.0021565447 0.0020519742 0.001718516 0.0012143489 0.00062448578 8.5763168e-07 -0.0006008679][0.00014529226 0.00012971985 -1.6298844e-05 -0.00027909665 -0.00062797137 -0.000968613 -0.0011888446 -0.0012755516 -0.001296781 -0.0013281107 -0.0013857081 -0.0014466661 -0.0014982244 -0.0015361899 -0.0015608869][-0.001518451 -0.0015140843 -0.0015141648 -0.0015214147 -0.0015412029 -0.0015620184 -0.0015764302 -0.0015794 -0.0015778269 -0.0015777302 -0.001579579 -0.0015830151 -0.0015853412 -0.00158548 -0.0015826836][-0.0015917493 -0.0015890818 -0.001588232 -0.0015866975 -0.0015859165 -0.0015840644 -0.0015824022 -0.0015801999 -0.0015772785 -0.0015754402 -0.001574148 -0.0015755957 -0.0015768348 -0.0015774504 -0.0015763982]]...]
INFO - root - 2017-12-09 11:08:48.221337: step 17110, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.853 sec/batch; 74h:45m:55s remains)
INFO - root - 2017-12-09 11:08:56.725051: step 17120, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.860 sec/batch; 75h:20m:16s remains)
INFO - root - 2017-12-09 11:09:05.098988: step 17130, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.888 sec/batch; 77h:48m:42s remains)
INFO - root - 2017-12-09 11:09:13.593334: step 17140, loss = 0.83, batch loss = 0.70 (9.6 examples/sec; 0.836 sec/batch; 73h:11m:26s remains)
INFO - root - 2017-12-09 11:09:22.044734: step 17150, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.845 sec/batch; 73h:59m:38s remains)
INFO - root - 2017-12-09 11:09:30.717620: step 17160, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.897 sec/batch; 78h:36m:47s remains)
INFO - root - 2017-12-09 11:09:39.411677: step 17170, loss = 0.81, batch loss = 0.68 (9.5 examples/sec; 0.845 sec/batch; 73h:59m:50s remains)
INFO - root - 2017-12-09 11:09:48.117545: step 17180, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.870 sec/batch; 76h:11m:19s remains)
INFO - root - 2017-12-09 11:09:56.846612: step 17190, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.894 sec/batch; 78h:19m:36s remains)
INFO - root - 2017-12-09 11:10:05.495826: step 17200, loss = 0.82, batch loss = 0.69 (10.7 examples/sec; 0.748 sec/batch; 65h:31m:07s remains)
2017-12-09 11:10:06.355475: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.5585587 0.55450791 0.54996073 0.54697406 0.5447644 0.54441434 0.54134083 0.53580296 0.52693748 0.51471812 0.500923 0.48679537 0.47516057 0.46604136 0.45917022][0.56645626 0.56302267 0.55912364 0.55675304 0.55593443 0.55745333 0.55645347 0.55257845 0.54440379 0.53110826 0.51535523 0.49883053 0.48477393 0.47393671 0.4661316][0.56578749 0.56452125 0.56244081 0.56149942 0.56242293 0.56521016 0.5652141 0.562545 0.55457616 0.540789 0.52304566 0.50400579 0.48787254 0.4752458 0.46634015][0.561962 0.56370133 0.563851 0.5649206 0.5683285 0.57350254 0.57575548 0.5734427 0.565583 0.55113512 0.53194404 0.51054531 0.49200305 0.47762513 0.46745765][0.55493748 0.55903488 0.56098628 0.56437218 0.5706262 0.57845515 0.5836423 0.58242011 0.57406473 0.55835456 0.53722823 0.51385885 0.49299869 0.47765532 0.46725428][0.54683632 0.55351794 0.55654794 0.56194705 0.57085395 0.58084631 0.58825034 0.58970624 0.58210832 0.5655545 0.54287738 0.51816571 0.49547514 0.47833389 0.46745753][0.54046404 0.54917592 0.55280882 0.55965245 0.57044125 0.5820325 0.59125984 0.59453523 0.58828586 0.5728333 0.55050242 0.52557194 0.501521 0.48257455 0.47027537][0.53712094 0.54645413 0.54836518 0.55454928 0.5652445 0.57682419 0.58670419 0.59119785 0.58607274 0.57209885 0.550833 0.52566659 0.50099164 0.48104253 0.46787831][0.53253722 0.54269981 0.54375446 0.54935503 0.55979747 0.57065588 0.58046871 0.58457881 0.57934469 0.56566024 0.54520589 0.52108055 0.49684954 0.47687519 0.46376568][0.52927744 0.53805685 0.53635478 0.540234 0.54940724 0.55982071 0.57022983 0.57528555 0.57112145 0.558265 0.53855425 0.51548129 0.49173877 0.472206 0.45924243][0.52352327 0.53106135 0.52647883 0.52758259 0.53402144 0.54350692 0.55290246 0.55857992 0.55682307 0.54698533 0.530401 0.50934589 0.48748773 0.46893871 0.4558045][0.51636285 0.52334493 0.51697582 0.5157423 0.5197202 0.52696931 0.53470051 0.53968114 0.53902525 0.53181481 0.51844376 0.50031316 0.48111519 0.46435478 0.45169321][0.50771004 0.51406568 0.50682396 0.50436592 0.50662291 0.5117529 0.51758051 0.5214017 0.52072471 0.51456439 0.50304139 0.48725939 0.47080633 0.45633888 0.44530308][0.49572837 0.50314385 0.49676895 0.49461493 0.496418 0.5005042 0.50508958 0.50760335 0.50658333 0.5011341 0.4912442 0.47814292 0.46450946 0.45243588 0.44315702][0.48445973 0.49245358 0.48641711 0.48428357 0.48524204 0.4890818 0.49301523 0.49528793 0.49483278 0.49050874 0.48259413 0.47152057 0.46008381 0.44993821 0.44197315]]...]
INFO - root - 2017-12-09 11:10:14.975771: step 17210, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.881 sec/batch; 77h:08m:12s remains)
INFO - root - 2017-12-09 11:10:23.440240: step 17220, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.897 sec/batch; 78h:34m:57s remains)
INFO - root - 2017-12-09 11:10:31.894685: step 17230, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.858 sec/batch; 75h:09m:55s remains)
INFO - root - 2017-12-09 11:10:40.571361: step 17240, loss = 0.81, batch loss = 0.68 (9.3 examples/sec; 0.856 sec/batch; 74h:56m:40s remains)
INFO - root - 2017-12-09 11:10:49.446297: step 17250, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.841 sec/batch; 73h:40m:54s remains)
INFO - root - 2017-12-09 11:10:58.119183: step 17260, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.853 sec/batch; 74h:43m:55s remains)
INFO - root - 2017-12-09 11:11:06.996596: step 17270, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.889 sec/batch; 77h:51m:47s remains)
INFO - root - 2017-12-09 11:11:15.858404: step 17280, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.904 sec/batch; 79h:07m:13s remains)
INFO - root - 2017-12-09 11:11:24.452883: step 17290, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.870 sec/batch; 76h:11m:40s remains)
INFO - root - 2017-12-09 11:11:33.039667: step 17300, loss = 0.82, batch loss = 0.69 (11.0 examples/sec; 0.729 sec/batch; 63h:50m:56s remains)
2017-12-09 11:11:33.866401: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.06130486 0.050661743 0.0376649 0.025358904 0.014961027 0.007296673 0.0023179573 -0.0001418998 -0.0010948732 -0.0013604305 -0.0014912193 -0.0015685189 -0.0016007908 -0.0016088102 -0.0016143415][0.067498162 0.057182122 0.04409913 0.030997807 0.019346597 0.010317006 0.00411952 0.00070991367 -0.00082972692 -0.00134504 -0.0015248697 -0.0015803917 -0.0016028164 -0.0016092221 -0.0016142996][0.074774675 0.065166287 0.052234598 0.038607139 0.025725691 0.015096253 0.0073120492 0.0025221144 1.1875411e-05 -0.0010652912 -0.0014659917 -0.0015749772 -0.0016048715 -0.0016087987 -0.001610727][0.081138216 0.072809771 0.060568329 0.0469721 0.033382498 0.021493344 0.012113262 0.0056917211 0.0018039898 -0.00025191088 -0.0011834648 -0.0015190141 -0.0016068743 -0.001611675 -0.0016102293][0.0860042 0.0792886 0.068566121 0.055690687 0.041950792 0.029173402 0.018337639 0.010220711 0.0047375076 0.0014219899 -0.00035843055 -0.0012031334 -0.001520253 -0.0015974119 -0.0016124533][0.087476127 0.0831595 0.074574172 0.063353807 0.050534341 0.037711717 0.025916345 0.016198466 0.0089255925 0.0040172795 0.0010102885 -0.00063139026 -0.001358401 -0.001579973 -0.0016177442][0.08621835 0.083686925 0.076930739 0.067491718 0.056194287 0.044255815 0.032573119 0.022163471 0.013648681 0.0073105814 0.0029576251 0.00029467139 -0.0010475155 -0.0015185468 -0.001615608][0.083787426 0.082046747 0.07599438 0.067442961 0.057162635 0.046152044 0.034965228 0.024604514 0.015730077 0.0088438075 0.0038982776 0.00075193541 -0.00088943169 -0.0014923074 -0.0016179292][0.08167927 0.080169454 0.074058764 0.065624371 0.055577021 0.044818174 0.033932604 0.023973363 0.015391149 0.0086854482 0.0038128763 0.00070549245 -0.0009091117 -0.0014967745 -0.0016159456][0.079333574 0.0776738 0.070970319 0.061953761 0.051476937 0.040529784 0.029751731 0.020264512 0.012445142 0.0066838609 0.0026500227 0.00016760186 -0.0010823705 -0.0015207727 -0.0016074904][0.077453755 0.074928 0.067207 0.05712783 0.0458106 0.034483764 0.023904264 0.015151659 0.0084434673 0.0039938791 0.0011362098 -0.00049736188 -0.0012850912 -0.0015506494 -0.0015998933][0.074886337 0.070902184 0.06179782 0.050703261 0.038853057 0.027545014 0.017607648 0.0099544814 0.0046127969 0.0015425771 -0.00016727799 -0.0010300167 -0.0014238041 -0.0015560605 -0.0015840547][0.07062538 0.064726867 0.054487146 0.042901319 0.031230519 0.020737844 0.012055495 0.00581442 0.0018953471 -4.648813e-06 -0.00087735435 -0.0012532818 -0.001429096 -0.0015058229 -0.0015448721][0.062012039 0.054790467 0.044521265 0.033577185 0.023095543 0.014200969 0.0072629321 0.0026824384 0.00015417032 -0.0008024461 -0.0011071069 -0.0012181418 -0.0012919958 -0.0013579424 -0.0014341733][0.050151512 0.042591095 0.033112489 0.023739513 0.015315445 0.0085578095 0.003614051 0.00066111027 -0.00070494437 -0.0010200803 -0.00099922158 -0.0009699323 -0.0010102412 -0.0011049421 -0.0012466391]]...]
INFO - root - 2017-12-09 11:11:42.598874: step 17310, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.856 sec/batch; 74h:55m:49s remains)
INFO - root - 2017-12-09 11:11:51.027994: step 17320, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.852 sec/batch; 74h:34m:11s remains)
INFO - root - 2017-12-09 11:11:59.716752: step 17330, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.892 sec/batch; 78h:05m:15s remains)
INFO - root - 2017-12-09 11:12:08.435054: step 17340, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 75h:38m:18s remains)
INFO - root - 2017-12-09 11:12:17.094540: step 17350, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.860 sec/batch; 75h:17m:23s remains)
INFO - root - 2017-12-09 11:12:25.750970: step 17360, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.860 sec/batch; 75h:16m:35s remains)
INFO - root - 2017-12-09 11:12:34.449327: step 17370, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 75h:29m:49s remains)
INFO - root - 2017-12-09 11:12:43.205223: step 17380, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.865 sec/batch; 75h:43m:26s remains)
INFO - root - 2017-12-09 11:12:51.817479: step 17390, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.832 sec/batch; 72h:47m:21s remains)
INFO - root - 2017-12-09 11:13:00.389700: step 17400, loss = 0.82, batch loss = 0.69 (10.3 examples/sec; 0.776 sec/batch; 67h:56m:29s remains)
2017-12-09 11:13:01.271135: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0016180316 -0.0016084125 -0.001593706 -0.0015740854 -0.0015532087 -0.0015341437 -0.0015178331 -0.0015058301 -0.0014980377 -0.0014928039 -0.0014895062 -0.0014862146 -0.0014833603 -0.0014797503 -0.0014756907][-0.0016192292 -0.0016123325 -0.001602078 -0.0015879635 -0.001572025 -0.0015569319 -0.0015440772 -0.0015345437 -0.0015284669 -0.0015248433 -0.0015224151 -0.001520218 -0.0015179857 -0.0015146678 -0.0015109334][-0.0016216788 -0.0016181181 -0.001612542 -0.0016044271 -0.0015946132 -0.0015846873 -0.0015755341 -0.0015684497 -0.0015637132 -0.0015604381 -0.0015582582 -0.0015563815 -0.0015548358 -0.0015529303 -0.0015509216][-0.0016230287 -0.0016220555 -0.0016198634 -0.0016157405 -0.0016108752 -0.0016054416 -0.0015997058 -0.0015950114 -0.0015918177 -0.001589509 -0.0015875523 -0.0015856468 -0.0015842512 -0.0015822049 -0.0015804102][-0.0016230165 -0.0016233907 -0.001623488 -0.0016225501 -0.0016208044 -0.0016180741 -0.0016140265 -0.0016111492 -0.0016092465 -0.0016079679 -0.0016071682 -0.0016071355 -0.0016072618 -0.0016062771 -0.0016050638][-0.0016226387 -0.0016231373 -0.0016240459 -0.0016239275 -0.0016241224 -0.001622911 -0.0016195672 -0.0016179958 -0.0016169325 -0.0016164992 -0.001615922 -0.001616476 -0.0016174013 -0.001617363 -0.0016169205][-0.0016225209 -0.0016227113 -0.0016233339 -0.0016226053 -0.0016225568 -0.0016222304 -0.0016198867 -0.0016189058 -0.0016175952 -0.0016168951 -0.0016165571 -0.0016168295 -0.0016176504 -0.0016178914 -0.0016185737][-0.0016224697 -0.0016217139 -0.0016214476 -0.0016198052 -0.0016188517 -0.0016183441 -0.0016166037 -0.0016153831 -0.0016146034 -0.0016142559 -0.0016140001 -0.0016141173 -0.0016149025 -0.0016157342 -0.0016162563][-0.0016217459 -0.0016201718 -0.0016191754 -0.001616762 -0.0016149313 -0.0016139729 -0.001612204 -0.0016106055 -0.001610178 -0.0016105484 -0.0016109049 -0.0016113111 -0.0016123848 -0.0016137025 -0.0016140195][-0.0016209909 -0.001619402 -0.0016184049 -0.0016161994 -0.0016138861 -0.0016123444 -0.0016102283 -0.0016083238 -0.0016078501 -0.0016083834 -0.0016094761 -0.0016101744 -0.0016115344 -0.0016130381 -0.0016132888][-0.0016203125 -0.0016189812 -0.0016181121 -0.001615974 -0.0016140433 -0.0016127257 -0.0016111525 -0.0016093587 -0.0016090589 -0.0016092764 -0.001610074 -0.0016101942 -0.0016112531 -0.0016128692 -0.0016133989][-0.0016191661 -0.0016181329 -0.0016176555 -0.0016155698 -0.0016134803 -0.0016123857 -0.0016115187 -0.0016097857 -0.0016091475 -0.0016093941 -0.0016096986 -0.0016097957 -0.0016104176 -0.001612142 -0.0016126516][-0.0016181187 -0.0016172049 -0.001616787 -0.0016151927 -0.0016137168 -0.001612826 -0.0016120165 -0.0016103723 -0.0016101123 -0.0016110216 -0.0016107613 -0.0016108893 -0.0016111949 -0.0016125329 -0.0016124272][-0.00161735 -0.0016167769 -0.0016165773 -0.0016152629 -0.0016144039 -0.0016138443 -0.0016132114 -0.001611935 -0.0016116016 -0.0016119773 -0.0016105016 -0.0016106162 -0.0016110481 -0.0016117967 -0.0016111865][-0.0016173448 -0.0016172091 -0.0016171456 -0.0016162574 -0.0016157196 -0.00161515 -0.0016144924 -0.0016136314 -0.0016131272 -0.0016131466 -0.0016114952 -0.0016114055 -0.0016111712 -0.0016106754 -0.0016101159]]...]
INFO - root - 2017-12-09 11:13:09.910374: step 17410, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.896 sec/batch; 78h:27m:45s remains)
INFO - root - 2017-12-09 11:13:18.296775: step 17420, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.843 sec/batch; 73h:47m:58s remains)
INFO - root - 2017-12-09 11:13:26.859930: step 17430, loss = 0.82, batch loss = 0.69 (10.7 examples/sec; 0.747 sec/batch; 65h:24m:27s remains)
INFO - root - 2017-12-09 11:13:35.609463: step 17440, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.837 sec/batch; 73h:15m:16s remains)
INFO - root - 2017-12-09 11:13:44.278131: step 17450, loss = 0.82, batch loss = 0.69 (9.9 examples/sec; 0.807 sec/batch; 70h:37m:45s remains)
INFO - root - 2017-12-09 11:13:53.011603: step 17460, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.895 sec/batch; 78h:17m:40s remains)
INFO - root - 2017-12-09 11:14:01.640181: step 17470, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.856 sec/batch; 74h:52m:53s remains)
INFO - root - 2017-12-09 11:14:10.262030: step 17480, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 75h:51m:03s remains)
INFO - root - 2017-12-09 11:14:18.810440: step 17490, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 76h:01m:23s remains)
INFO - root - 2017-12-09 11:14:27.417468: step 17500, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.860 sec/batch; 75h:15m:06s remains)
2017-12-09 11:14:28.285304: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0016628618 -0.0016617033 -0.0016604523 -0.0016593471 -0.0016589658 -0.0016594121 -0.0016603553 -0.0016612567 -0.0016617886 -0.0016620033 -0.001662087 -0.0016621862 -0.0016623859 -0.0016627786 -0.0016632099][-0.001661167 -0.0016575642 -0.0016535466 -0.00165081 -0.0016505641 -0.0016526806 -0.0016558331 -0.0016586734 -0.0016606519 -0.0016616093 -0.0016616572 -0.0016613494 -0.0016613049 -0.001661578 -0.0016619324][-0.0016539929 -0.001644263 -0.0016340562 -0.0016279033 -0.0016289449 -0.0016359681 -0.001645062 -0.0016526445 -0.0016572168 -0.0016589148 -0.0016584467 -0.0016572716 -0.0016564558 -0.0016563142 -0.0016565414][-0.0016371786 -0.0016140222 -0.001590531 -0.0015769142 -0.0015798988 -0.0015963817 -0.0016180077 -0.0016366475 -0.0016483597 -0.0016528681 -0.0016519094 -0.0016484241 -0.0016448695 -0.0016426644 -0.0016418331][-0.0016087583 -0.0015644165 -0.0015194591 -0.0014918745 -0.0014939397 -0.0015220434 -0.0015619291 -0.0015986431 -0.0016236255 -0.0016348254 -0.0016342865 -0.0016267204 -0.0016171334 -0.0016098177 -0.001606061][-0.0015735993 -0.0015062939 -0.001438944 -0.0013954224 -0.0013912247 -0.0014227697 -0.0014749981 -0.001529325 -0.0015714695 -0.0015939204 -0.0015952366 -0.001581608 -0.0015622574 -0.0015464384 -0.0015374388][-0.0015455445 -0.0014628058 -0.0013819027 -0.0013281928 -0.001315654 -0.001340715 -0.0013903467 -0.0014491589 -0.0015018875 -0.0015337493 -0.0015368637 -0.0015168359 -0.0014874209 -0.0014626265 -0.0014474898][-0.0015420346 -0.0014589539 -0.0013785663 -0.0013239471 -0.0013057361 -0.0013194624 -0.0013557054 -0.0014050495 -0.0014551047 -0.0014877309 -0.0014898479 -0.0014654653 -0.0014300199 -0.0013991422 -0.0013786766][-0.0015671805 -0.00149984 -0.0014333556 -0.0013868808 -0.0013685978 -0.0013739082 -0.0013958639 -0.001429291 -0.0014657606 -0.0014895637 -0.0014879401 -0.0014634703 -0.0014298439 -0.0013998905 -0.0013779744][-0.0016059701 -0.001563966 -0.0015204585 -0.0014882784 -0.0014740878 -0.0014748534 -0.0014861134 -0.0015045033 -0.0015246872 -0.001536336 -0.0015312994 -0.0015114055 -0.0014861622 -0.0014629374 -0.0014436609][-0.0016376871 -0.0016182764 -0.0015972216 -0.0015804524 -0.0015722064 -0.0015713554 -0.0015756343 -0.001583098 -0.0015910451 -0.0015946937 -0.0015900293 -0.0015781103 -0.0015640163 -0.0015501888 -0.0015367954][-0.0016540129 -0.0016474401 -0.0016401388 -0.0016338662 -0.0016304109 -0.0016296293 -0.0016308784 -0.0016331567 -0.0016350647 -0.0016351689 -0.0016323702 -0.0016272846 -0.00162147 -0.0016154565 -0.0016087779][-0.001658799 -0.0016569009 -0.0016551337 -0.0016534204 -0.0016523532 -0.0016520075 -0.0016523016 -0.0016528065 -0.001653287 -0.0016532309 -0.0016522767 -0.0016507222 -0.0016491191 -0.001647226 -0.0016449088][-0.0016588577 -0.00165772 -0.0016571615 -0.0016567349 -0.0016565027 -0.00165645 -0.0016565479 -0.0016567345 -0.0016569293 -0.0016570506 -0.0016570224 -0.0016569669 -0.0016568592 -0.0016564161 -0.0016558273][-0.0016584378 -0.0016572977 -0.0016567956 -0.0016565061 -0.0016563878 -0.0016562915 -0.0016563017 -0.0016563962 -0.0016565484 -0.0016567238 -0.0016568066 -0.0016569068 -0.0016569416 -0.0016568924 -0.0016567833]]...]
INFO - root - 2017-12-09 11:14:36.888506: step 17510, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.856 sec/batch; 74h:51m:18s remains)
INFO - root - 2017-12-09 11:14:45.380249: step 17520, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 75h:52m:47s remains)
INFO - root - 2017-12-09 11:14:53.966930: step 17530, loss = 0.82, batch loss = 0.69 (10.3 examples/sec; 0.775 sec/batch; 67h:50m:40s remains)
INFO - root - 2017-12-09 11:15:02.468100: step 17540, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.896 sec/batch; 78h:20m:57s remains)
INFO - root - 2017-12-09 11:15:11.197542: step 17550, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.855 sec/batch; 74h:46m:19s remains)
INFO - root - 2017-12-09 11:15:19.752735: step 17560, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.837 sec/batch; 73h:15m:59s remains)
INFO - root - 2017-12-09 11:15:28.489064: step 17570, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.876 sec/batch; 76h:35m:59s remains)
INFO - root - 2017-12-09 11:15:37.116776: step 17580, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.880 sec/batch; 77h:00m:14s remains)
INFO - root - 2017-12-09 11:15:45.765506: step 17590, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 75h:54m:33s remains)
INFO - root - 2017-12-09 11:15:54.197825: step 17600, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.854 sec/batch; 74h:43m:46s remains)
2017-12-09 11:15:55.076320: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0016428516 -0.0016234238 -0.0015775044 -0.0014806596 -0.0013376132 -0.0011857469 -0.0010726664 -0.0010399604 -0.0010920726 -0.0012062918 -0.0013396621 -0.0014493981 -0.0015186562 -0.0015500381 -0.0015382163][-0.0016311503 -0.0015817215 -0.001480059 -0.0013051574 -0.0010690547 -0.00082496164 -0.00063917448 -0.00057387573 -0.00064406288 -0.00082249404 -0.001042329 -0.001229774 -0.0013514538 -0.0013998327 -0.0013545784][-0.0015934026 -0.001471233 -0.001237228 -0.00088611862 -0.00045362953 -2.3246976e-05 0.00030068285 0.00042144139 0.00030498463 -6.7751389e-06 -0.00040623976 -0.00076697592 -0.0010085917 -0.0010972124 -0.00098785246][-0.0015114091 -0.0012463146 -0.000758924 -7.0386333e-05 0.00073539373 0.0015130464 0.0020886236 0.0023151278 0.0021263598 0.0015846582 0.00086017232 0.00016856543 -0.00032577023 -0.00053128612 -0.00036381802][-0.0013759454 -0.00088476739 3.3712713e-06 0.001227526 0.002630638 0.003957619 0.0049128304 0.0052748411 0.0049442262 0.0040334584 0.0028160741 0.0016329277 0.00076483446 0.00035858643 0.00052495347][-0.0012255148 -0.00049898808 0.00080619636 0.0026046303 0.0046623521 0.0066047367 0.0080086617 0.0085678631 0.0081313662 0.0068441685 0.0050898856 0.003358393 0.0020571679 0.0013829619 0.0014486494][-0.0011311246 -0.00029067253 0.0012340418 0.0033761505 0.0058609983 0.0082332464 0.0099756625 0.010708276 0.010240476 0.0087357461 0.0066670412 0.0046283826 0.003079419 0.0022150339 0.002106162][-0.0011454233 -0.00038397429 0.0010438082 0.0031387093 0.0056443019 0.0080999779 0.0099510178 0.010785134 0.010380776 0.0088888854 0.0068185339 0.0047959643 0.0032535829 0.0023371028 0.0020692006][-0.0012629686 -0.00072038232 0.00036385423 0.0020687808 0.0041946247 0.0063532852 0.0080413381 0.008876144 0.0086211227 0.0073776245 0.0055908007 0.0038368409 0.0024714535 0.0016011014 0.0012386814][-0.0013989646 -0.0010786122 -0.00038910634 0.00077768671 0.002291129 0.0038784593 0.0051670512 0.0058605564 0.0057519893 0.0048858239 0.0035771625 0.0022519259 0.0011853527 0.00045308843 8.9958427e-05][-0.0015166173 -0.0013540832 -0.00099111 -0.00033693644 0.00054132496 0.0014881941 0.0022833045 0.0027479925 0.002733795 0.0022410867 0.0014471519 0.00061422982 -8.3987368e-05 -0.000588791 -0.00085518567][-0.0015947059 -0.0015259275 -0.0013752973 -0.001101685 -0.00073081732 -0.00032750284 2.3992034e-05 0.00024809525 0.00026920554 7.2424649e-05 -0.000273727 -0.00065508112 -0.0009901179 -0.0012425582 -0.0013862213][-0.0016347792 -0.0016155184 -0.0015718414 -0.0014950964 -0.001391579 -0.0012799394 -0.0011777383 -0.0011048031 -0.001087169 -0.0011364383 -0.0012355286 -0.001350609 -0.001455239 -0.0015337544 -0.0015780961][-0.0016396798 -0.0016377155 -0.001635205 -0.0016314341 -0.0016256102 -0.0016184337 -0.0016081857 -0.001597568 -0.0015892922 -0.0015887258 -0.0015949863 -0.0016049385 -0.0016157478 -0.0016238113 -0.0016288137][-0.0016401879 -0.0016390611 -0.0016380599 -0.0016370681 -0.0016355134 -0.0016335287 -0.0016312024 -0.001629803 -0.0016289696 -0.0016300419 -0.0016316335 -0.0016338071 -0.0016360447 -0.0016369387 -0.0016371885]]...]
INFO - root - 2017-12-09 11:16:03.779708: step 17610, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 75h:32m:19s remains)
INFO - root - 2017-12-09 11:16:12.285029: step 17620, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.873 sec/batch; 76h:21m:52s remains)
INFO - root - 2017-12-09 11:16:20.845717: step 17630, loss = 0.82, batch loss = 0.69 (10.4 examples/sec; 0.771 sec/batch; 67h:27m:39s remains)
INFO - root - 2017-12-09 11:16:29.016299: step 17640, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.861 sec/batch; 75h:20m:32s remains)
INFO - root - 2017-12-09 11:16:37.464444: step 17650, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.865 sec/batch; 75h:36m:34s remains)
INFO - root - 2017-12-09 11:16:46.129607: step 17660, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.885 sec/batch; 77h:22m:53s remains)
INFO - root - 2017-12-09 11:16:54.900022: step 17670, loss = 0.82, batch loss = 0.69 (8.2 examples/sec; 0.974 sec/batch; 85h:11m:50s remains)
INFO - root - 2017-12-09 11:17:03.454315: step 17680, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.842 sec/batch; 73h:37m:36s remains)
INFO - root - 2017-12-09 11:17:12.064851: step 17690, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.876 sec/batch; 76h:37m:08s remains)
INFO - root - 2017-12-09 11:17:20.554674: step 17700, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.836 sec/batch; 73h:08m:35s remains)
2017-12-09 11:17:21.471851: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.047924835 0.044584095 0.04175951 0.039364409 0.037132468 0.035531178 0.035251446 0.037256941 0.040293798 0.042447653 0.041228231 0.035482213 0.025626604 0.014923573 0.0063105943][0.07675615 0.072335452 0.067331836 0.061843749 0.055727903 0.050046232 0.045845836 0.044800963 0.045805044 0.046985392 0.045128919 0.038695645 0.027929461 0.016288711 0.0069846585][0.10806014 0.10348072 0.096601717 0.088233091 0.078009941 0.067467988 0.05793386 0.052246638 0.049621638 0.048496038 0.0454299 0.038680304 0.027948091 0.016339339 0.0070009697][0.13652353 0.13438612 0.12686129 0.11593934 0.10166694 0.086469755 0.071546234 0.060643245 0.053229943 0.048662405 0.043821823 0.036601737 0.026150569 0.015102139 0.0062672989][0.15925758 0.16090658 0.15437838 0.14285526 0.12618488 0.10722547 0.087607764 0.071722478 0.059188344 0.050419576 0.042730633 0.034191534 0.023649864 0.013079927 0.0049474789][0.17325419 0.17971614 0.17536293 0.1645679 0.14745587 0.12708722 0.10491102 0.085325211 0.068416923 0.055625007 0.044649426 0.033978563 0.02237691 0.01156876 0.0037887108][0.17542973 0.18758789 0.18694739 0.17865227 0.16291222 0.14297611 0.12060098 0.099760808 0.080447793 0.064508863 0.050259765 0.03684568 0.023231922 0.011331905 0.0032328141][0.16584326 0.18275395 0.18638231 0.18181773 0.16922309 0.15155032 0.13076159 0.11055077 0.090880834 0.073470481 0.056846548 0.040797 0.024906714 0.011552968 0.002900901][0.14597528 0.16576105 0.17351721 0.17310545 0.16458161 0.15034771 0.13241437 0.11389402 0.09483505 0.077169478 0.059478544 0.04210408 0.025023567 0.01107064 0.0024480296][0.11952344 0.1393937 0.14980486 0.15303972 0.14870125 0.13825858 0.12373088 0.10769506 0.090264574 0.073480926 0.056042012 0.038990263 0.022439603 0.0094576394 0.0017721397][0.090176426 0.10771035 0.11854129 0.12375659 0.12290631 0.11676522 0.1066137 0.093996018 0.07931067 0.06448897 0.0485901 0.032999635 0.018175678 0.0071874764 0.00097165979][0.06196918 0.075785458 0.085225865 0.090610452 0.091553122 0.0884309 0.0821057 0.07347437 0.062697582 0.051097777 0.038138706 0.025260616 0.013213344 0.0047199894 0.00016261882][0.038150709 0.047759198 0.054938637 0.059459779 0.061035026 0.059822649 0.056266334 0.050819267 0.043590978 0.035495147 0.026161233 0.016807679 0.0081534674 0.0023478919 -0.00058143225][0.020960316 0.026860565 0.031576224 0.034685995 0.036056295 0.03570823 0.0338935 0.030764498 0.026367387 0.021273989 0.015331581 0.0093787471 0.0039262334 0.0004813578 -0.0011219296][0.009881461 0.01312991 0.015879383 0.017738869 0.018654816 0.018611446 0.017744625 0.01604503 0.013632318 0.010782302 0.0074480698 0.0041032126 0.0010899671 -0.00068011705 -0.001429109]]...]
INFO - root - 2017-12-09 11:17:30.011550: step 17710, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.863 sec/batch; 75h:26m:16s remains)
INFO - root - 2017-12-09 11:17:38.413991: step 17720, loss = 0.82, batch loss = 0.70 (9.6 examples/sec; 0.830 sec/batch; 72h:35m:21s remains)
INFO - root - 2017-12-09 11:17:46.928683: step 17730, loss = 0.82, batch loss = 0.69 (9.8 examples/sec; 0.820 sec/batch; 71h:44m:10s remains)
INFO - root - 2017-12-09 11:17:55.355701: step 17740, loss = 0.82, batch loss = 0.69 (9.7 examples/sec; 0.825 sec/batch; 72h:09m:06s remains)
INFO - root - 2017-12-09 11:18:03.993330: step 17750, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.863 sec/batch; 75h:24m:57s remains)
INFO - root - 2017-12-09 11:18:12.601277: step 17760, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.865 sec/batch; 75h:38m:18s remains)
INFO - root - 2017-12-09 11:18:21.291835: step 17770, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.857 sec/batch; 74h:52m:49s remains)
INFO - root - 2017-12-09 11:18:29.967374: step 17780, loss = 0.82, batch loss = 0.69 (8.3 examples/sec; 0.968 sec/batch; 84h:34m:54s remains)
INFO - root - 2017-12-09 11:18:38.784873: step 17790, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.877 sec/batch; 76h:38m:02s remains)
INFO - root - 2017-12-09 11:18:47.362815: step 17800, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.875 sec/batch; 76h:31m:26s remains)
2017-12-09 11:18:48.262567: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0016277061 -0.001625077 -0.0016249262 -0.0016255254 -0.0016264195 -0.0016274458 -0.0016283843 -0.001629089 -0.0016294168 -0.0016294033 -0.0016290443 -0.0016285067 -0.0016277973 -0.0016270611 -0.0016262858][-0.0016259043 -0.0016232021 -0.0016231637 -0.0016239665 -0.0016250787 -0.0016262932 -0.0016273551 -0.0016281474 -0.0016286144 -0.001628723 -0.0016285139 -0.0016280455 -0.0016273404 -0.0016264378 -0.0016254791][-0.001625552 -0.0016228876 -0.0016229228 -0.0016238763 -0.0016250737 -0.0016262493 -0.001627259 -0.0016280515 -0.0016286125 -0.0016288416 -0.0016288103 -0.0016284935 -0.0016278643 -0.0016269468 -0.0016258761][-0.0016253914 -0.0016228163 -0.0016229317 -0.0016238667 -0.0016249374 -0.0016259125 -0.0016267638 -0.0016274519 -0.0016280594 -0.0016285126 -0.0016287735 -0.0016287429 -0.0016283464 -0.0016275552 -0.0016265493][-0.001625462 -0.0016228935 -0.0016229979 -0.001623751 -0.0016245274 -0.0016251658 -0.0016257355 -0.0016262996 -0.0016270122 -0.0016277783 -0.0016284626 -0.0016288979 -0.0016289176 -0.0016284236 -0.001627624][-0.001625599 -0.0016229728 -0.001622972 -0.0016234961 -0.0016239849 -0.0016243581 -0.0016247501 -0.0016253375 -0.0016263063 -0.001627494 -0.0016286359 -0.0016295013 -0.0016298861 -0.0016296839 -0.0016291048][-0.0016257075 -0.0016231069 -0.0016229553 -0.0016232872 -0.0016235802 -0.0016238142 -0.0016241767 -0.001624911 -0.0016261843 -0.0016277551 -0.0016292407 -0.0016304245 -0.0016310811 -0.0016311212 -0.0016307692][-0.0016258676 -0.0016232599 -0.0016229572 -0.0016231161 -0.0016232746 -0.001623466 -0.0016238973 -0.0016248297 -0.0016263351 -0.0016281159 -0.0016297881 -0.0016311895 -0.00163207 -0.0016323191 -0.0016321507][-0.0016260984 -0.0016234815 -0.0016230652 -0.001623107 -0.0016232645 -0.0016235444 -0.0016241615 -0.0016252813 -0.0016268359 -0.0016286339 -0.0016303024 -0.0016317468 -0.0016327208 -0.0016331251 -0.001633091][-0.0016261974 -0.0016237152 -0.0016232788 -0.0016233437 -0.0016236529 -0.001624126 -0.0016249472 -0.0016261581 -0.0016276586 -0.0016293143 -0.0016308205 -0.0016321298 -0.0016330573 -0.0016334695 -0.001633444][-0.0016264904 -0.0016241174 -0.0016235927 -0.0016237188 -0.0016242195 -0.0016249097 -0.0016258656 -0.0016270792 -0.0016284439 -0.0016298475 -0.0016310654 -0.0016321037 -0.0016328228 -0.0016331227 -0.0016329861][-0.0016268941 -0.0016244135 -0.0016238034 -0.0016239663 -0.0016246039 -0.0016254575 -0.0016264926 -0.0016276339 -0.0016287661 -0.0016298458 -0.0016307409 -0.0016314725 -0.0016319383 -0.0016321023 -0.0016318785][-0.0016274493 -0.0016248695 -0.0016240935 -0.0016241461 -0.0016247638 -0.0016256091 -0.0016266037 -0.0016276117 -0.0016284828 -0.0016292447 -0.0016298493 -0.0016303275 -0.0016306094 -0.0016306953 -0.0016304721][-0.0016277707 -0.0016250883 -0.0016242503 -0.0016242291 -0.0016247599 -0.0016255587 -0.0016264876 -0.0016273584 -0.0016280125 -0.0016285214 -0.0016288992 -0.0016291875 -0.0016293364 -0.0016293727 -0.0016291981][-0.0016278216 -0.0016251316 -0.0016242617 -0.0016242466 -0.0016246955 -0.0016254138 -0.0016262452 -0.0016269711 -0.0016274562 -0.0016277666 -0.0016279778 -0.0016281182 -0.0016281486 -0.0016281242 -0.0016280081]]...]
INFO - root - 2017-12-09 11:18:56.888201: step 17810, loss = 0.81, batch loss = 0.68 (9.2 examples/sec; 0.867 sec/batch; 75h:48m:02s remains)
INFO - root - 2017-12-09 11:19:05.315438: step 17820, loss = 0.83, batch loss = 0.70 (9.4 examples/sec; 0.850 sec/batch; 74h:17m:30s remains)
INFO - root - 2017-12-09 11:19:13.882301: step 17830, loss = 0.83, batch loss = 0.70 (9.6 examples/sec; 0.836 sec/batch; 73h:05m:45s remains)
INFO - root - 2017-12-09 11:19:22.243801: step 17840, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.856 sec/batch; 74h:48m:16s remains)
INFO - root - 2017-12-09 11:19:30.880732: step 17850, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.858 sec/batch; 74h:57m:33s remains)
INFO - root - 2017-12-09 11:19:39.392238: step 17860, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 75h:49m:45s remains)
INFO - root - 2017-12-09 11:19:48.109969: step 17870, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.849 sec/batch; 74h:10m:35s remains)
INFO - root - 2017-12-09 11:19:56.876414: step 17880, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.838 sec/batch; 73h:12m:55s remains)
INFO - root - 2017-12-09 11:20:05.627442: step 17890, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.856 sec/batch; 74h:49m:40s remains)
INFO - root - 2017-12-09 11:20:14.242475: step 17900, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.879 sec/batch; 76h:49m:48s remains)
2017-12-09 11:20:15.099042: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.12784627 0.13765164 0.14203814 0.13870798 0.12845221 0.11415552 0.095974483 0.075863153 0.056159664 0.0406385 0.030055182 0.022949308 0.017983351 0.014466706 0.012209722][0.13309555 0.14496732 0.15123139 0.14926331 0.14005753 0.12636197 0.10817308 0.087683357 0.066670716 0.049559109 0.037066836 0.02801414 0.020884607 0.015238273 0.01115878][0.14392799 0.1581852 0.16673149 0.16708405 0.1602993 0.14853093 0.13151672 0.11127197 0.089326814 0.07054951 0.055444293 0.043418158 0.032955218 0.024123924 0.017247913][0.16309486 0.18004833 0.19109184 0.19413173 0.19005328 0.18016507 0.16456716 0.14495485 0.12264647 0.10249821 0.084810749 0.069539204 0.055137351 0.042546131 0.032073911][0.18824345 0.20737858 0.22054896 0.22626154 0.22522911 0.21808027 0.20480731 0.18673335 0.16502385 0.1438833 0.123416 0.10427982 0.085496426 0.068658583 0.054413896][0.21403736 0.23390232 0.24733895 0.25445747 0.25578272 0.2512427 0.2409758 0.22566317 0.20620598 0.18576871 0.16417123 0.14253898 0.12038016 0.099980496 0.081920579][0.23476183 0.25373685 0.26524192 0.27192521 0.27415943 0.27138773 0.26357946 0.251118 0.23451014 0.21569696 0.19450136 0.1721988 0.14868204 0.12660317 0.10644276][0.24786712 0.26422095 0.27171394 0.27621958 0.27791208 0.27585492 0.26960996 0.25905725 0.24451129 0.22721952 0.20697144 0.18498322 0.16177557 0.13966809 0.11918141][0.25217339 0.26411906 0.26604563 0.26675555 0.26620021 0.2633892 0.25724584 0.24750771 0.23413314 0.21787775 0.19870923 0.17785081 0.15604901 0.13535857 0.11629585][0.24714619 0.25381151 0.24981934 0.24602748 0.24208289 0.23705009 0.22942363 0.21890575 0.20524536 0.18918653 0.1709789 0.15186268 0.13267668 0.1147199 0.098594576][0.232588 0.23354091 0.2242796 0.21617576 0.20848724 0.20049544 0.19051257 0.17830668 0.16366349 0.14739756 0.13026577 0.11346649 0.097583242 0.083444804 0.071227551][0.20944448 0.20526855 0.19219483 0.18084031 0.17017789 0.15974234 0.14775573 0.13411397 0.11882713 0.10303672 0.0876859 0.073746838 0.061569564 0.051257558 0.042925432][0.17985646 0.17167723 0.15660019 0.1434278 0.1311395 0.11952342 0.10693278 0.093350567 0.079001509 0.065041058 0.052352771 0.041644655 0.033006143 0.026173629 0.021020023][0.14414966 0.13360457 0.11847693 0.10540136 0.093350045 0.082299352 0.070896372 0.059084028 0.047269315 0.036416262 0.027135976 0.019840404 0.014429868 0.01051473 0.00780808][0.10583143 0.0951975 0.081656367 0.070182614 0.059845414 0.0506992 0.041703105 0.03283884 0.024460563 0.017193189 0.011395978 0.0071988311 0.0044273138 0.0026773193 0.0016147161]]...]
INFO - root - 2017-12-09 11:20:23.947056: step 17910, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.885 sec/batch; 77h:19m:43s remains)
INFO - root - 2017-12-09 11:20:32.588771: step 17920, loss = 0.82, batch loss = 0.69 (10.9 examples/sec; 0.732 sec/batch; 63h:56m:24s remains)
INFO - root - 2017-12-09 11:20:41.282634: step 17930, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.861 sec/batch; 75h:12m:04s remains)
INFO - root - 2017-12-09 11:20:49.934561: step 17940, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 75h:56m:27s remains)
INFO - root - 2017-12-09 11:20:58.648889: step 17950, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.846 sec/batch; 73h:54m:57s remains)
INFO - root - 2017-12-09 11:21:07.339063: step 17960, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.858 sec/batch; 74h:57m:15s remains)
INFO - root - 2017-12-09 11:21:15.885544: step 17970, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 75h:45m:33s remains)
INFO - root - 2017-12-09 11:21:24.433042: step 17980, loss = 0.82, batch loss = 0.69 (9.8 examples/sec; 0.813 sec/batch; 71h:01m:47s remains)
INFO - root - 2017-12-09 11:21:32.938226: step 17990, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.873 sec/batch; 76h:14m:47s remains)
INFO - root - 2017-12-09 11:21:41.500145: step 18000, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.861 sec/batch; 75h:11m:52s remains)
2017-12-09 11:21:42.362823: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0016577034 -0.0016551584 -0.0016171124 -0.0014346523 -0.0010931131 -0.0006460892 -0.0003510922 -0.00021970225 -0.00019011088 -0.00024454517 -0.00051184092 -0.00097294169 -0.0013671081 -0.0015688855 -0.0016179474][-0.0016577081 -0.0016408623 -0.0014975459 -0.0010182622 -0.0002171346 0.00064236752 0.0011331883 0.0013137729 0.0013200144 0.0011177693 0.00051982293 -0.00037390157 -0.001109002 -0.0014915041 -0.0015917358][-0.0016575626 -0.0016003851 -0.0012368266 -0.00022118608 0.0014118442 0.0031052097 0.0041590137 0.0046060113 0.004568445 0.0039853915 0.0026913229 0.00094211416 -0.00050819479 -0.0013061317 -0.0015433835][-0.0016575668 -0.0015123587 -0.00078133208 0.0010406546 0.0038967808 0.0069361911 0.0090623163 0.010018568 0.0098650251 0.0085791135 0.00617223 0.0031364849 0.00055669027 -0.00093572645 -0.0014465818][-0.0016439173 -0.0013509552 -0.00010745821 0.0027497918 0.0071238712 0.011923772 0.01564884 0.017505586 0.017244048 0.014952148 0.011044646 0.0063562435 0.0022822884 -0.00023714011 -0.0012240503][-0.0015044434 -0.0010005289 0.00087919424 0.0049450714 0.011024433 0.017832339 0.023475863 0.026480582 0.026172763 0.022701437 0.017049668 0.010509139 0.0047349795 0.00096144632 -0.00072119583][-0.0012288543 -0.00046302762 0.002078644 0.0073412568 0.015037463 0.023745049 0.031306554 0.035654824 0.035525817 0.031026771 0.023721224 0.015376494 0.0078818537 0.0027015973 0.00018169254][-0.0008293764 0.00019068702 0.0032550078 0.00943185 0.018264426 0.028329283 0.037404168 0.043012757 0.043377232 0.038418557 0.029993778 0.020265019 0.011333963 0.00487622 0.0015259088][-0.00040485687 0.00081462751 0.0041614156 0.010770275 0.020073719 0.030734265 0.040584631 0.047103167 0.04812203 0.0433129 0.034504335 0.0240759 0.014258699 0.0068743783 0.0028869505][-9.238068e-05 0.001192212 0.0044777826 0.010915555 0.019988565 0.030432632 0.040189944 0.047003061 0.048581675 0.044391703 0.03594451 0.025608601 0.015682325 0.0080269249 0.0038553961][-3.993162e-06 0.0011931305 0.0040883892 0.0098018022 0.017960783 0.027422033 0.036256965 0.042608295 0.044404428 0.041027367 0.033567254 0.024156477 0.014990321 0.0078704711 0.00405201][-0.00041327113 0.00054587063 0.0028083371 0.0073401625 0.013994821 0.021834647 0.029140934 0.034432579 0.036043469 0.03349204 0.027464628 0.019681022 0.012088247 0.0062615988 0.0033050952][-0.00091637723 -0.00029334705 0.0012110701 0.0043177605 0.0090450291 0.014757947 0.020090247 0.023917193 0.025052454 0.023198413 0.018806616 0.013146785 0.0076931627 0.0036489828 0.0017719507][-0.0013937661 -0.0010828251 -0.00026728236 0.0014958725 0.0043397932 0.0079012327 0.011216017 0.013530442 0.014114958 0.01285588 0.010073784 0.0065846615 0.0033408427 0.0010515623 0.00011031551][-0.0016267664 -0.0015422739 -0.0012376152 -0.00046934211 0.00087025773 0.0026397794 0.004291153 0.0053791464 0.0055545559 0.0048335879 0.0034223385 0.00172817 0.00024133758 -0.00073408132 -0.0010628329]]...]
INFO - root - 2017-12-09 11:21:50.949820: step 18010, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.837 sec/batch; 73h:08m:02s remains)
INFO - root - 2017-12-09 11:21:59.684429: step 18020, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.894 sec/batch; 78h:06m:46s remains)
INFO - root - 2017-12-09 11:22:08.224401: step 18030, loss = 0.81, batch loss = 0.68 (9.1 examples/sec; 0.876 sec/batch; 76h:32m:39s remains)
INFO - root - 2017-12-09 11:22:16.771949: step 18040, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.875 sec/batch; 76h:28m:23s remains)
INFO - root - 2017-12-09 11:22:25.487548: step 18050, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.859 sec/batch; 75h:01m:55s remains)
INFO - root - 2017-12-09 11:22:34.172132: step 18060, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.878 sec/batch; 76h:41m:30s remains)
INFO - root - 2017-12-09 11:22:42.841796: step 18070, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.866 sec/batch; 75h:39m:10s remains)
INFO - root - 2017-12-09 11:22:51.560519: step 18080, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.863 sec/batch; 75h:24m:33s remains)
INFO - root - 2017-12-09 11:23:00.295347: step 18090, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.861 sec/batch; 75h:13m:33s remains)
INFO - root - 2017-12-09 11:23:08.785450: step 18100, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.836 sec/batch; 73h:01m:26s remains)
2017-12-09 11:23:09.724591: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.00016071997 8.1183389e-05 -1.0329881e-05 -7.2008814e-05 -0.00013075781 -0.00017192378 -0.00024212105 -0.0003035788 -0.00036515261 -0.00038648513 -0.00037750253 -0.00035257661 -0.00040335907 -0.000492914 -0.00060593977][0.0010035875 0.0009027886 0.00078525383 0.00070937106 0.00063551415 0.00057322287 0.00045912631 0.00034856366 0.00023325812 0.00017454068 0.00017020083 0.00020236662 0.00019832689 0.00013363082 3.4055905e-05][0.0020679007 0.0019458189 0.0018029761 0.001691091 0.0015804743 0.0014853053 0.0013179994 0.0011378987 0.00095242437 0.00084940332 0.00083681743 0.0008967818 0.00095866004 0.000953284 0.000872014][0.0031805937 0.0030516027 0.0028928826 0.0027493164 0.0026038103 0.0024651662 0.0022441023 0.0020153145 0.001788241 0.0016527738 0.001638203 0.0017078436 0.0018125492 0.0018368737 0.0017610643][0.0042063654 0.0040588742 0.003884946 0.0037298496 0.0035631047 0.0033823843 0.0031149429 0.0028383038 0.0025822497 0.0024204026 0.0024088821 0.0024867179 0.0026166947 0.0026398669 0.0025680293][0.0048355297 0.0046838531 0.0045112544 0.0043688528 0.0042033261 0.004004037 0.0037113773 0.0033967532 0.0031176922 0.0029421845 0.0029310524 0.0029982463 0.0031133131 0.0031185211 0.0030242545][0.0049628536 0.0048156637 0.0046592187 0.0045559811 0.0044260505 0.0042548929 0.0039874888 0.0036806394 0.0034093014 0.0032159225 0.0031812317 0.0031934804 0.003208328 0.0031308881 0.0029951283][0.0047831787 0.004653485 0.0045277989 0.004468923 0.0043900651 0.0042734556 0.0040643038 0.0038268245 0.0036129514 0.0034462218 0.0033624992 0.0032161684 0.0030120644 0.0027649766 0.0025202078][0.0046835481 0.0045707934 0.0044681816 0.0044369651 0.00440033 0.0043218038 0.0041552354 0.0039936872 0.0038489811 0.0037229336 0.0035937945 0.0032763383 0.0028519034 0.0023918515 0.002006114][0.0048597078 0.0047489456 0.0046480903 0.0046167076 0.0045960862 0.0045282356 0.0043749427 0.0042332783 0.0041213576 0.0040345592 0.0038892073 0.0035042227 0.0029349653 0.0023322562 0.0018305051][0.0052793073 0.0051679132 0.0050681271 0.0050210762 0.0049883928 0.0049087862 0.0047426485 0.004590393 0.0044851787 0.0044335825 0.0042945873 0.0039103855 0.0033267452 0.0026585772 0.0020954781][0.0058793756 0.0057594464 0.0056542275 0.00558511 0.0055272975 0.0054152333 0.0052126581 0.0050321179 0.0049497979 0.0049194307 0.0048050908 0.0044632591 0.0039539766 0.0033232411 0.0027774563][0.0064384891 0.0063165389 0.0062157661 0.0061250664 0.0060203136 0.0058625261 0.0056205527 0.0053871474 0.0052751373 0.0052599953 0.0052091526 0.0049586347 0.0046063745 0.0041187322 0.0036723223][0.0067728348 0.0066572288 0.006563683 0.0064545376 0.0063028783 0.0061020805 0.0058341804 0.0055632535 0.0054027075 0.0053822482 0.0053687897 0.0052557597 0.005091764 0.0047973511 0.0044877017][0.0067009823 0.0065847775 0.006503908 0.0064115655 0.0062638996 0.0060697165 0.0058170264 0.0055457228 0.0053691654 0.0053261737 0.005334415 0.005277087 0.0052417731 0.0051335967 0.004957967]]...]
INFO - root - 2017-12-09 11:23:18.389488: step 18110, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.878 sec/batch; 76h:40m:18s remains)
INFO - root - 2017-12-09 11:23:27.123936: step 18120, loss = 0.83, batch loss = 0.70 (9.1 examples/sec; 0.877 sec/batch; 76h:34m:33s remains)
INFO - root - 2017-12-09 11:23:35.567528: step 18130, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.861 sec/batch; 75h:10m:16s remains)
INFO - root - 2017-12-09 11:23:44.275771: step 18140, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.882 sec/batch; 76h:59m:03s remains)
INFO - root - 2017-12-09 11:23:52.916087: step 18150, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.873 sec/batch; 76h:15m:43s remains)
INFO - root - 2017-12-09 11:24:01.695888: step 18160, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.854 sec/batch; 74h:35m:48s remains)
INFO - root - 2017-12-09 11:24:10.466145: step 18170, loss = 0.83, batch loss = 0.70 (8.3 examples/sec; 0.963 sec/batch; 84h:07m:35s remains)
INFO - root - 2017-12-09 11:24:19.174186: step 18180, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.855 sec/batch; 74h:37m:14s remains)
INFO - root - 2017-12-09 11:24:27.885225: step 18190, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 75h:51m:03s remains)
INFO - root - 2017-12-09 11:24:36.465977: step 18200, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.844 sec/batch; 73h:39m:19s remains)
2017-12-09 11:24:37.417804: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.028072711 0.029347694 0.030862851 0.0327687 0.034840927 0.036454961 0.0374466 0.03796424 0.038111437 0.038097557 0.037748944 0.037175231 0.036202084 0.03512818 0.034028485][0.028721113 0.030194571 0.031833641 0.033746302 0.035833668 0.037380558 0.038224082 0.038713019 0.038865522 0.038925458 0.038679983 0.03822517 0.037334681 0.036125369 0.034908585][0.028243743 0.029778698 0.031437229 0.033191092 0.035055678 0.036281824 0.036767643 0.036882166 0.036777664 0.036805928 0.036611248 0.036240555 0.035498116 0.034496486 0.033370469][0.027582698 0.029136891 0.030771898 0.032381255 0.033964258 0.03482138 0.034951754 0.034569152 0.034031134 0.033707127 0.03331884 0.033030413 0.032481786 0.031647503 0.030652473][0.026988732 0.028462542 0.03000156 0.031466115 0.032733317 0.0332435 0.0330575 0.032278143 0.031297635 0.030539235 0.029940708 0.029664313 0.029248539 0.028610699 0.027633617][0.026291955 0.027619604 0.028960465 0.030171497 0.031008961 0.031081269 0.030522319 0.029459143 0.028175831 0.027085239 0.026395403 0.026266556 0.026141059 0.025706032 0.024710294][0.025555026 0.026583407 0.027501296 0.028275203 0.028564755 0.028063899 0.02703031 0.025641834 0.024117539 0.022863204 0.022184301 0.022232708 0.022406805 0.022127509 0.021090992][0.024551013 0.025222473 0.025592012 0.025729017 0.025327636 0.02422221 0.02271726 0.021032689 0.0193697 0.018071914 0.017466644 0.017567061 0.017801929 0.017555285 0.016434884][0.023622669 0.02391595 0.023643795 0.023041887 0.021877332 0.020149663 0.018243263 0.016455228 0.014871314 0.013654839 0.013062343 0.013050441 0.013099071 0.012558468 0.011222124][0.023173824 0.023195535 0.022405926 0.021151485 0.019295117 0.016944041 0.01459286 0.012634968 0.011042861 0.0099302931 0.0093481019 0.0091258073 0.0088312542 0.00799259 0.0065795109][0.023144009 0.023152813 0.022100311 0.02041477 0.018088374 0.015222244 0.012400912 0.010089629 0.0082879364 0.0070945704 0.0063926522 0.0059335991 0.0053943167 0.0044418788 0.0031823581][0.0231636 0.023329854 0.02221656 0.020391162 0.017871283 0.014768314 0.011675422 0.0090385154 0.0069485754 0.0055293464 0.0046245349 0.0039622514 0.0032991329 0.0023863432 0.0013684669][0.022585187 0.023074511 0.022169489 0.020584431 0.018259078 0.015359756 0.012370754 0.0096633174 0.0073845489 0.005663963 0.0044380166 0.0034176377 0.0024714759 0.001508606 0.000594834][0.021540049 0.022374285 0.021824796 0.020707058 0.019020505 0.01678884 0.014427792 0.012146958 0.010116196 0.0084229773 0.0070034452 0.00565202 0.0043120007 0.0029765773 0.001761251][0.021035602 0.022053275 0.021709835 0.021058295 0.019960944 0.01855601 0.017026637 0.015359057 0.013758868 0.012246999 0.010781863 0.0091645531 0.0074075488 0.0055916295 0.0038914029]]...]
INFO - root - 2017-12-09 11:24:46.026034: step 18210, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 75h:51m:06s remains)
INFO - root - 2017-12-09 11:24:54.516104: step 18220, loss = 0.82, batch loss = 0.69 (9.7 examples/sec; 0.829 sec/batch; 72h:20m:25s remains)
INFO - root - 2017-12-09 11:25:02.765508: step 18230, loss = 0.82, batch loss = 0.69 (9.8 examples/sec; 0.815 sec/batch; 71h:08m:18s remains)
INFO - root - 2017-12-09 11:25:11.215616: step 18240, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.852 sec/batch; 74h:24m:42s remains)
INFO - root - 2017-12-09 11:25:19.835550: step 18250, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.875 sec/batch; 76h:24m:55s remains)
INFO - root - 2017-12-09 11:25:28.465873: step 18260, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 75h:16m:39s remains)
INFO - root - 2017-12-09 11:25:37.342848: step 18270, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.877 sec/batch; 76h:35m:19s remains)
INFO - root - 2017-12-09 11:25:45.904774: step 18280, loss = 0.83, batch loss = 0.70 (9.2 examples/sec; 0.869 sec/batch; 75h:50m:37s remains)
INFO - root - 2017-12-09 11:25:54.656750: step 18290, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.851 sec/batch; 74h:18m:17s remains)
INFO - root - 2017-12-09 11:26:03.192006: step 18300, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.848 sec/batch; 74h:02m:09s remains)
2017-12-09 11:26:04.170931: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0016537161 -0.0016513653 -0.0016508651 -0.0016503374 -0.0016491991 -0.0016474917 -0.0016457925 -0.0016447172 -0.0016446129 -0.0016455436 -0.0016471209 -0.0016488779 -0.0016502219 -0.0016509256 -0.0016510219][-0.0016519089 -0.001649449 -0.0016488832 -0.0016482985 -0.0016470707 -0.0016453116 -0.0016435408 -0.0016424067 -0.0016423933 -0.0016435079 -0.0016453771 -0.00164738 -0.0016489119 -0.0016496826 -0.0016497277][-0.0016516777 -0.0016492241 -0.0016485399 -0.0016478777 -0.0016465669 -0.0016447597 -0.0016429856 -0.0016418565 -0.0016418921 -0.0016430952 -0.0016451109 -0.0016472635 -0.0016489179 -0.0016497452 -0.0016497533][-0.0016514804 -0.0016490745 -0.001648281 -0.0016475492 -0.0016462283 -0.0016444726 -0.0016428406 -0.0016418294 -0.0016419031 -0.0016430602 -0.0016450448 -0.0016471973 -0.0016488407 -0.0016496812 -0.0016497402][-0.0016514023 -0.0016489608 -0.0016480873 -0.0016473227 -0.0016460655 -0.0016444725 -0.0016430867 -0.0016422656 -0.0016424009 -0.0016434453 -0.0016452504 -0.0016471945 -0.0016486703 -0.0016494648 -0.001649611][-0.00165134 -0.0016488866 -0.0016479836 -0.0016472375 -0.001646116 -0.0016447722 -0.0016436748 -0.0016430515 -0.0016432449 -0.0016441661 -0.0016456799 -0.0016472803 -0.0016484899 -0.0016491836 -0.0016493862][-0.0016510586 -0.0016487124 -0.0016478545 -0.0016472009 -0.0016462688 -0.0016451997 -0.0016443992 -0.001643981 -0.0016442069 -0.0016450053 -0.001646186 -0.0016474021 -0.0016483283 -0.0016488988 -0.0016491312][-0.0016507113 -0.0016484425 -0.0016476475 -0.0016471047 -0.0016463457 -0.0016455565 -0.0016450651 -0.0016448601 -0.0016450984 -0.0016457728 -0.0016466732 -0.0016475212 -0.0016481526 -0.0016485848 -0.0016488422][-0.0016501846 -0.0016480478 -0.0016473822 -0.0016469646 -0.0016463986 -0.0016458802 -0.0016456471 -0.0016456005 -0.0016458477 -0.0016464348 -0.0016471262 -0.0016477037 -0.001648134 -0.0016484698 -0.0016487056][-0.0016495724 -0.001647656 -0.001647122 -0.0016468308 -0.001646464 -0.0016461698 -0.0016460996 -0.0016461574 -0.0016464107 -0.0016469251 -0.0016474462 -0.0016478639 -0.0016482185 -0.0016484963 -0.001648678][-0.0016492505 -0.0016474351 -0.0016469592 -0.0016468024 -0.0016466142 -0.0016464989 -0.0016465388 -0.0016466499 -0.0016468824 -0.0016472951 -0.001647647 -0.0016479273 -0.0016482094 -0.0016484336 -0.0016485529][-0.0016491278 -0.0016473125 -0.0016469938 -0.0016469486 -0.0016468556 -0.001646817 -0.0016468849 -0.0016469967 -0.0016471953 -0.0016475025 -0.0016477243 -0.0016478933 -0.0016480737 -0.0016482209 -0.0016482789][-0.0016492807 -0.0016474928 -0.0016472256 -0.0016472376 -0.0016471755 -0.0016471419 -0.0016471702 -0.0016472571 -0.0016474366 -0.0016476657 -0.0016478059 -0.001647874 -0.0016479484 -0.001648002 -0.0016479916][-0.0016496758 -0.0016478582 -0.0016475985 -0.0016476189 -0.0016475356 -0.001647439 -0.0016474002 -0.0016474364 -0.0016475611 -0.0016477068 -0.0016477725 -0.0016477668 -0.0016477497 -0.0016477254 -0.0016476742][-0.0016502308 -0.0016483505 -0.0016479848 -0.0016479827 -0.0016478802 -0.0016477404 -0.0016476589 -0.0016476734 -0.0016477617 -0.001647812 -0.001647766 -0.0016476443 -0.0016474985 -0.001647382 -0.0016473057]]...]
INFO - root - 2017-12-09 11:26:12.819949: step 18310, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 75h:43m:42s remains)
INFO - root - 2017-12-09 11:26:21.405074: step 18320, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.884 sec/batch; 77h:07m:28s remains)
INFO - root - 2017-12-09 11:26:29.847963: step 18330, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.856 sec/batch; 74h:43m:03s remains)
INFO - root - 2017-12-09 11:26:38.284393: step 18340, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.847 sec/batch; 73h:55m:49s remains)
INFO - root - 2017-12-09 11:26:46.813102: step 18350, loss = 0.81, batch loss = 0.68 (9.3 examples/sec; 0.861 sec/batch; 75h:10m:30s remains)
INFO - root - 2017-12-09 11:26:55.424920: step 18360, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.860 sec/batch; 75h:01m:32s remains)
INFO - root - 2017-12-09 11:27:03.969843: step 18370, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 75h:13m:30s remains)
INFO - root - 2017-12-09 11:27:12.655860: step 18380, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.892 sec/batch; 77h:48m:23s remains)
INFO - root - 2017-12-09 11:27:21.376191: step 18390, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 75h:24m:01s remains)
INFO - root - 2017-12-09 11:27:29.724697: step 18400, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.844 sec/batch; 73h:37m:29s remains)
2017-12-09 11:27:30.588654: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.11399832 0.11582974 0.1141546 0.10957897 0.10298887 0.095120318 0.086245328 0.076465741 0.066528857 0.056800932 0.047656897 0.038695529 0.029882658 0.021493208 0.013941767][0.11581959 0.11849317 0.11716978 0.11308672 0.10722748 0.10043158 0.092980951 0.084899478 0.076731868 0.068219379 0.059671007 0.050314188 0.040238865 0.029873326 0.020013753][0.10868724 0.11156993 0.11089433 0.10814609 0.1043919 0.10029203 0.096045248 0.091297664 0.085939735 0.079398349 0.071737304 0.062260792 0.051097736 0.03882413 0.026711684][0.09728279 0.10042455 0.10073649 0.09987969 0.09890306 0.098364212 0.098339967 0.097783506 0.095827013 0.09142983 0.084526606 0.074523181 0.061698429 0.047154818 0.032739408][0.081908874 0.085938126 0.088062 0.089792326 0.092015952 0.095048994 0.098801576 0.10190821 0.10311829 0.10095818 0.095039867 0.084778018 0.070695169 0.054262586 0.037752889][0.063329943 0.06871625 0.07355988 0.078847565 0.084831908 0.091343641 0.09814436 0.1037841 0.10686847 0.10593604 0.10058015 0.090385064 0.075784266 0.058363978 0.040713388][0.043773942 0.050398171 0.058073238 0.067058817 0.076868162 0.086722828 0.096031778 0.10345668 0.10766393 0.10727502 0.1019274 0.091641329 0.076830804 0.05912485 0.041148592][0.026523374 0.033704549 0.04325388 0.054822765 0.0672767 0.079417907 0.090362072 0.098974146 0.10389758 0.10393474 0.098759308 0.088684447 0.074185438 0.056891661 0.039319985][0.01462267 0.02127108 0.0309804 0.043003924 0.05610422 0.068800427 0.080107145 0.089065529 0.094212413 0.0945735 0.089839034 0.080535896 0.067083552 0.051026754 0.03479711][0.0080622965 0.013443703 0.02183608 0.032382429 0.043971796 0.055273343 0.065412559 0.073456854 0.078004815 0.078339949 0.07425002 0.066381492 0.054997638 0.041432217 0.027789313][0.0048295213 0.0088618966 0.01531039 0.0234442 0.032319736 0.040877651 0.048496015 0.054423686 0.057445098 0.057210021 0.053734884 0.0476205 0.038989704 0.028873663 0.018896274][0.0023252645 0.005165752 0.00975579 0.015479224 0.021561885 0.027215667 0.032076806 0.03560001 0.036953066 0.036083423 0.033210095 0.028849635 0.023093544 0.016548948 0.010282724][0.00019950222 0.0019633796 0.0048696157 0.0084205652 0.012122606 0.015373673 0.017984034 0.019620897 0.019824164 0.018768344 0.016710646 0.014056716 0.010862489 0.0073990431 0.0041780947][-0.0011156255 -0.00034401554 0.0010559149 0.002804117 0.0046431124 0.0061658621 0.0072881924 0.00781631 0.0075582005 0.0067467 0.0055432869 0.0042650476 0.00291643 0.0015734809 0.00037639076][-0.0015922876 -0.001413309 -0.00101678 -0.00045862957 0.00016140507 0.0006596325 0.0010183314 0.0011489957 0.00098968321 0.00059818965 9.0683461e-05 -0.00034106325 -0.00071576517 -0.0010275827 -0.0012810249]]...]
INFO - root - 2017-12-09 11:27:39.400399: step 18410, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 75h:24m:40s remains)
INFO - root - 2017-12-09 11:27:48.034822: step 18420, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 75h:23m:41s remains)
INFO - root - 2017-12-09 11:27:56.451015: step 18430, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.857 sec/batch; 74h:44m:50s remains)
INFO - root - 2017-12-09 11:28:05.000782: step 18440, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.849 sec/batch; 74h:01m:40s remains)
INFO - root - 2017-12-09 11:28:13.537042: step 18450, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.852 sec/batch; 74h:17m:34s remains)
INFO - root - 2017-12-09 11:28:22.181616: step 18460, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 75h:59m:54s remains)
INFO - root - 2017-12-09 11:28:30.806165: step 18470, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 75h:43m:11s remains)
INFO - root - 2017-12-09 11:28:39.646643: step 18480, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.851 sec/batch; 74h:15m:47s remains)
INFO - root - 2017-12-09 11:28:48.232006: step 18490, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.884 sec/batch; 77h:06m:13s remains)
INFO - root - 2017-12-09 11:28:56.639482: step 18500, loss = 0.82, batch loss = 0.69 (11.6 examples/sec; 0.689 sec/batch; 60h:04m:34s remains)
2017-12-09 11:28:57.524249: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.00168842 -0.001686999 -0.0016867168 -0.0016867497 -0.0016869621 -0.0016871643 -0.0016874842 -0.0016879445 -0.0016884757 -0.001689125 -0.0016897216 -0.0016902778 -0.0016909108 -0.0016914476 -0.0016917182][-0.0016861954 -0.0016848259 -0.0016846976 -0.001684897 -0.0016852625 -0.0016855835 -0.0016859921 -0.0016865238 -0.001687107 -0.0016878215 -0.0016885389 -0.0016892352 -0.001690006 -0.0016906433 -0.0016909281][-0.0016852417 -0.0016841459 -0.0016841774 -0.0016845347 -0.0016850112 -0.0016853968 -0.0016858042 -0.0016863246 -0.0016868917 -0.0016876182 -0.0016883923 -0.0016891775 -0.0016900392 -0.0016907417 -0.0016909898][-0.0016851778 -0.0016843867 -0.0016845842 -0.0016850944 -0.0016856573 -0.0016860956 -0.001686443 -0.0016868801 -0.0016873633 -0.0016879782 -0.0016887133 -0.0016895101 -0.0016903508 -0.0016909995 -0.0016911427][-0.001686239 -0.0016856056 -0.0016859141 -0.0016865029 -0.0016870771 -0.0016874856 -0.0016876683 -0.0016878503 -0.001688157 -0.0016885927 -0.0016891559 -0.0016898215 -0.0016905493 -0.0016911075 -0.0016911672][-0.0016878068 -0.0016873347 -0.0016876474 -0.0016881963 -0.0016886509 -0.0016888876 -0.0016888476 -0.001688813 -0.0016889462 -0.001689208 -0.0016895676 -0.0016900705 -0.00169066 -0.0016910922 -0.0016910798][-0.0016897524 -0.0016892517 -0.0016893671 -0.001689604 -0.0016897033 -0.0016896061 -0.0016893259 -0.0016891846 -0.001689314 -0.0016895576 -0.0016898651 -0.0016903077 -0.0016907888 -0.0016910939 -0.001691005][-0.0016915547 -0.0016909869 -0.001690862 -0.0016907172 -0.0016903898 -0.0016898952 -0.0016893583 -0.0016890905 -0.0016892507 -0.0016895959 -0.0016899888 -0.00169046 -0.0016908671 -0.0016910713 -0.001690932][-0.0016929716 -0.0016923213 -0.0016920712 -0.00169167 -0.0016910917 -0.0016903359 -0.0016896225 -0.0016892343 -0.0016893285 -0.0016896517 -0.0016900103 -0.001690447 -0.0016907899 -0.001690958 -0.0016908622][-0.0016940876 -0.001693388 -0.0016930319 -0.0016924695 -0.0016917587 -0.0016909001 -0.0016901211 -0.0016896895 -0.0016897015 -0.0016899562 -0.0016902502 -0.0016905991 -0.0016908739 -0.001691025 -0.0016909929][-0.0016947237 -0.0016939931 -0.0016935085 -0.0016928306 -0.0016920352 -0.0016911837 -0.0016904882 -0.0016901138 -0.001690113 -0.0016903259 -0.0016905835 -0.0016908915 -0.0016911111 -0.0016912147 -0.0016912025][-0.0016946117 -0.0016938149 -0.0016933368 -0.0016927267 -0.0016920435 -0.0016913473 -0.0016908298 -0.0016905731 -0.00169059 -0.0016907767 -0.0016909792 -0.0016912114 -0.0016913249 -0.001691348 -0.0016913115][-0.0016939886 -0.0016931957 -0.0016928244 -0.0016924312 -0.0016920124 -0.0016915794 -0.0016912698 -0.001691132 -0.0016911302 -0.0016912274 -0.0016913312 -0.0016914508 -0.0016914778 -0.0016914418 -0.0016913955][-0.0016935107 -0.0016927052 -0.0016924066 -0.001692228 -0.0016920584 -0.0016918652 -0.0016917257 -0.0016916556 -0.0016916131 -0.0016915987 -0.001691575 -0.0016915732 -0.001691544 -0.001691476 -0.0016914065][-0.0016933284 -0.001692399 -0.00169206 -0.0016919398 -0.0016918572 -0.0016918009 -0.0016917898 -0.0016917848 -0.0016917373 -0.001691692 -0.0016916345 -0.0016915798 -0.0016915073 -0.0016914071 -0.0016912905]]...]
INFO - root - 2017-12-09 11:29:06.228445: step 18510, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.890 sec/batch; 77h:35m:07s remains)
INFO - root - 2017-12-09 11:29:14.963059: step 18520, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.879 sec/batch; 76h:41m:23s remains)
INFO - root - 2017-12-09 11:29:23.519759: step 18530, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 75h:41m:00s remains)
INFO - root - 2017-12-09 11:29:32.316107: step 18540, loss = 0.82, batch loss = 0.69 (8.8 examples/sec; 0.904 sec/batch; 78h:52m:32s remains)
INFO - root - 2017-12-09 11:29:40.895901: step 18550, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.865 sec/batch; 75h:26m:49s remains)
INFO - root - 2017-12-09 11:29:49.605959: step 18560, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.876 sec/batch; 76h:24m:49s remains)
INFO - root - 2017-12-09 11:29:58.049684: step 18570, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.855 sec/batch; 74h:35m:08s remains)
INFO - root - 2017-12-09 11:30:06.542260: step 18580, loss = 0.82, batch loss = 0.70 (9.2 examples/sec; 0.872 sec/batch; 76h:03m:00s remains)
INFO - root - 2017-12-09 11:30:15.199498: step 18590, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.856 sec/batch; 74h:40m:45s remains)
INFO - root - 2017-12-09 11:30:24.022639: step 18600, loss = 0.82, batch loss = 0.69 (10.3 examples/sec; 0.778 sec/batch; 67h:51m:38s remains)
2017-12-09 11:30:24.886076: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.00062598754 -0.000709514 -0.00085785659 -0.0010192487 -0.0012220477 -0.0014199526 -0.0015616692 -0.0016247822 -0.0016270187 -0.0015941765 -0.0015186432 -0.0013833903 -0.00120018 -0.0010192797 -0.00091681164][-5.270273e-05 -0.00010788441 -0.00030549348 -0.00053429743 -0.00083588762 -0.0011555668 -0.0013962202 -0.0015160585 -0.0015260204 -0.0014770508 -0.0013695993 -0.0011719707 -0.00089010259 -0.00059312384 -0.00040225871][0.00068780349 0.00070700806 0.00047462818 0.0001544687 -0.00030163489 -0.0007826137 -0.0011384918 -0.001326502 -0.0013501602 -0.0012745226 -0.0011161614 -0.00082658691 -0.0004117093 3.3336342e-05 0.00032451528][0.0015289999 0.0016289545 0.001358929 0.00095595361 0.00037014589 -0.00027808791 -0.00078051729 -0.0010550502 -0.001083954 -0.000964775 -0.00073519838 -0.00033719768 0.00022208376 0.0008271368 0.0012421279][0.0024291882 0.0026113912 0.0022927946 0.0017983423 0.0010744535 0.00027800922 -0.00035600341 -0.00071380753 -0.00075968215 -0.00057834783 -0.00024346146 0.00028631592 0.00099161256 0.0017469955 0.0022786316][0.0031420947 0.0034141815 0.0030698273 0.0025291662 0.0017298326 0.00082224363 5.9128972e-05 -0.00038276904 -0.00042608776 -0.00015612354 0.00031368819 0.00099259859 0.0018254187 0.0026910393 0.0033070678][0.0035164692 0.0038518715 0.0034751552 0.0029238304 0.0021353397 0.0012098515 0.0003759776 -0.00012374052 -0.00016994798 0.00020171714 0.0008430219 0.0016766762 0.0026040403 0.0035135038 0.0041466295][0.0034421785 0.0037964247 0.0033927145 0.0028469828 0.0021162047 0.0012727111 0.00048276864 -1.5963335e-05 -4.3980661e-05 0.00041997724 0.0012025776 0.0021550241 0.0031372304 0.0040368871 0.004631245][0.0028553428 0.0032013026 0.0027954839 0.0022769556 0.0016635229 0.0009728783 0.00029683148 -0.00013985997 -0.00013137632 0.00039293512 0.0012743635 0.0022873962 0.0032530143 0.0040739588 0.004568391][0.0019013617 0.0021812525 0.0018021878 0.0013187575 0.00083680393 0.00036055536 -0.00011049025 -0.00043807598 -0.00039861037 0.00012807315 0.0009989104 0.0019748886 0.0028628604 0.003560889 0.0039237747][0.00077494315 0.00099472457 0.00067587628 0.00027244433 -8.6120213e-05 -0.00038076355 -0.00065278565 -0.00083218969 -0.00074380077 -0.00027096004 0.00048492721 0.001324423 0.0020522764 0.0025859298 0.002807912][-0.00025244453 -0.00011909357 -0.00034347828 -0.00063314568 -0.00086388248 -0.0010199839 -0.0011474309 -0.0012072665 -0.0010898954 -0.00071187667 -0.00014687586 0.00047042139 0.00098201761 0.0013277802 0.0014243923][-0.0010086484 -0.00095898617 -0.0010875532 -0.0012435884 -0.001368427 -0.0014419478 -0.0014890864 -0.0014877648 -0.0013849038 -0.0011338911 -0.00078047591 -0.00039921585 -0.00010218099 7.4425363e-05 8.2889339e-05][-0.0014592559 -0.0014502758 -0.0015075356 -0.0015713639 -0.0016168616 -0.001642823 -0.0016610825 -0.0016516218 -0.0015917551 -0.0014634612 -0.001292857 -0.0011121013 -0.00098062644 -0.000918494 -0.00094456883][-0.0016520335 -0.0016553442 -0.0016732479 -0.0016905637 -0.0017017787 -0.0017071482 -0.0017113799 -0.0017104057 -0.0016926598 -0.0016481208 -0.0015869421 -0.0015245423 -0.0014850141 -0.0014749095 -0.0014967215]]...]
INFO - root - 2017-12-09 11:30:33.498922: step 18610, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.846 sec/batch; 73h:47m:43s remains)
INFO - root - 2017-12-09 11:30:42.216185: step 18620, loss = 0.82, batch loss = 0.69 (9.7 examples/sec; 0.828 sec/batch; 72h:09m:34s remains)
INFO - root - 2017-12-09 11:30:50.649043: step 18630, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.860 sec/batch; 74h:59m:36s remains)
INFO - root - 2017-12-09 11:30:59.305973: step 18640, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.857 sec/batch; 74h:40m:51s remains)
INFO - root - 2017-12-09 11:31:07.859116: step 18650, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.860 sec/batch; 74h:59m:07s remains)
INFO - root - 2017-12-09 11:31:16.558486: step 18660, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.873 sec/batch; 76h:05m:42s remains)
INFO - root - 2017-12-09 11:31:25.189920: step 18670, loss = 0.82, batch loss = 0.69 (9.7 examples/sec; 0.825 sec/batch; 71h:55m:14s remains)
INFO - root - 2017-12-09 11:31:34.083875: step 18680, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.887 sec/batch; 77h:20m:43s remains)
INFO - root - 2017-12-09 11:31:42.757557: step 18690, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.840 sec/batch; 73h:12m:26s remains)
INFO - root - 2017-12-09 11:31:51.341416: step 18700, loss = 0.82, batch loss = 0.69 (10.7 examples/sec; 0.745 sec/batch; 64h:58m:02s remains)
2017-12-09 11:31:52.203347: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0017118677 -0.0017158259 -0.0017200868 -0.0017234751 -0.0017250411 -0.0017251201 -0.0017242329 -0.0017230484 -0.0017222697 -0.0017225773 -0.0017235471 -0.0017237295 -0.0017226654 -0.0017204843 -0.0017170457][-0.0017111626 -0.0017150667 -0.0017196077 -0.0017234026 -0.0017256336 -0.0017263122 -0.0017256669 -0.0017245669 -0.0017235917 -0.0017234645 -0.0017238959 -0.0017237085 -0.0017226328 -0.0017208033 -0.001717778][-0.0017097302 -0.0017135257 -0.0017182935 -0.001722418 -0.0017252883 -0.0017266808 -0.0017263731 -0.0017252365 -0.001724044 -0.0017235499 -0.0017234773 -0.0017230898 -0.0017221424 -0.0017206676 -0.0017180421][-0.0017079508 -0.0017112276 -0.0017160655 -0.0017204843 -0.0017239781 -0.00172608 -0.0017261353 -0.0017249474 -0.0017237222 -0.0017231124 -0.0017226905 -0.001722164 -0.001721335 -0.001720174 -0.0017177851][-0.00170611 -0.0017086136 -0.0017131995 -0.0017176719 -0.0017215237 -0.001724124 -0.0017246946 -0.0017238536 -0.0017228312 -0.0017223522 -0.0017218164 -0.0017211514 -0.0017201413 -0.0017188294 -0.0017162955][-0.0017044297 -0.0017060908 -0.0017101596 -0.0017143645 -0.0017181427 -0.0017209771 -0.0017219981 -0.0017218662 -0.0017213451 -0.001721159 -0.0017206302 -0.0017196744 -0.0017182075 -0.0017163849 -0.0017135642][-0.0017031296 -0.0017038287 -0.0017069208 -0.0017104186 -0.0017136639 -0.00171635 -0.0017177442 -0.0017183172 -0.001718572 -0.0017188644 -0.0017185534 -0.0017173862 -0.0017154071 -0.0017130033 -0.0017099447][-0.0017017802 -0.0017017358 -0.0017037645 -0.0017063387 -0.0017087919 -0.0017110301 -0.0017125852 -0.0017135435 -0.0017142842 -0.0017150475 -0.0017150333 -0.0017139078 -0.0017118212 -0.0017091432 -0.0017061377][-0.0017006225 -0.001700002 -0.0017011052 -0.0017027687 -0.0017044189 -0.0017060032 -0.001707238 -0.0017081561 -0.0017090312 -0.0017100046 -0.0017103124 -0.0017095231 -0.0017078199 -0.0017054586 -0.0017028722][-0.0016997395 -0.0016989163 -0.0016993501 -0.0017002703 -0.0017011438 -0.0017019376 -0.001702623 -0.0017032208 -0.001703865 -0.001704663 -0.0017051658 -0.0017048159 -0.0017037852 -0.0017020687 -0.0017000675][-0.0016992239 -0.0016983594 -0.0016983886 -0.001698748 -0.0016990646 -0.0016992787 -0.0016994986 -0.001699754 -0.0017000635 -0.0017005078 -0.0017010035 -0.0017009367 -0.0017004194 -0.0016993607 -0.001698046][-0.0016988511 -0.001697856 -0.0016977637 -0.001697761 -0.0016977269 -0.0016976811 -0.0016976784 -0.0016976657 -0.0016976845 -0.0016978463 -0.0016982444 -0.0016983023 -0.0016980461 -0.0016974902 -0.0016967715][-0.0016984675 -0.001697427 -0.0016973166 -0.0016971768 -0.0016970083 -0.0016968691 -0.0016967502 -0.0016966561 -0.0016966027 -0.0016966378 -0.0016969063 -0.0016969652 -0.0016968236 -0.0016965525 -0.0016962385][-0.0016982517 -0.0016970728 -0.0016969763 -0.0016968148 -0.00169666 -0.0016965186 -0.0016964026 -0.0016963363 -0.0016962794 -0.0016962815 -0.0016964345 -0.0016964606 -0.0016963742 -0.001696244 -0.0016961182][-0.0016979486 -0.0016968281 -0.0016967792 -0.0016966659 -0.0016965492 -0.0016964324 -0.0016963778 -0.0016963627 -0.0016963246 -0.0016962894 -0.0016963356 -0.00169633 -0.0016962786 -0.001696226 -0.0016961725]]...]
INFO - root - 2017-12-09 11:32:00.961780: step 18710, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.879 sec/batch; 76h:38m:00s remains)
INFO - root - 2017-12-09 11:32:09.740152: step 18720, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.888 sec/batch; 77h:22m:08s remains)
INFO - root - 2017-12-09 11:32:18.205649: step 18730, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.874 sec/batch; 76h:10m:07s remains)
INFO - root - 2017-12-09 11:32:26.955068: step 18740, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.874 sec/batch; 76h:10m:40s remains)
INFO - root - 2017-12-09 11:32:35.467319: step 18750, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.865 sec/batch; 75h:21m:40s remains)
INFO - root - 2017-12-09 11:32:44.113701: step 18760, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.875 sec/batch; 76h:17m:56s remains)
INFO - root - 2017-12-09 11:32:52.792655: step 18770, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.841 sec/batch; 73h:16m:53s remains)
INFO - root - 2017-12-09 11:33:01.564271: step 18780, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.847 sec/batch; 73h:48m:29s remains)
INFO - root - 2017-12-09 11:33:10.323386: step 18790, loss = 0.82, batch loss = 0.69 (8.8 examples/sec; 0.905 sec/batch; 78h:50m:12s remains)
INFO - root - 2017-12-09 11:33:19.028620: step 18800, loss = 0.82, batch loss = 0.69 (10.4 examples/sec; 0.766 sec/batch; 66h:43m:24s remains)
2017-12-09 11:33:19.866602: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.0011854994 0.0022084755 0.0028569545 0.0029405365 0.0025038435 0.0018282649 0.0011743184 0.00071583083 0.00042150728 0.00015453517 -0.00018433458 -0.00059444725 -0.001013829 -0.0013601294 -0.0015851972][0.0037955684 0.0055111828 0.0065524969 0.0066482071 0.0058926945 0.0047312551 0.0036012926 0.0027819006 0.0022200602 0.001714064 0.0010786813 0.00031544105 -0.00045518542 -0.0010790403 -0.0014802706][0.0071672918 0.00965008 0.011101193 0.011183704 0.010025439 0.0082292575 0.0064046634 0.0050153281 0.004012791 0.0031546163 0.0021872497 0.001084741 2.2526365e-07 -0.00086449611 -0.0014082465][0.010617081 0.01384437 0.015723985 0.015882257 0.014436298 0.012066463 0.0095006162 0.0073679853 0.005709535 0.0043107546 0.0029170527 0.0014954132 0.0001945321 -0.00079674018 -0.0013962361][0.013540045 0.017554354 0.019955698 0.020274546 0.018568236 0.015601913 0.012219556 0.0092421211 0.0068444423 0.004888854 0.0031402702 0.0015322713 0.00017056172 -0.0008260032 -0.0014113204][0.015110112 0.019751031 0.02266166 0.023236558 0.021439545 0.018060021 0.01402727 0.010334324 0.0073090764 0.0049217418 0.0029561226 0.0013088055 6.6598877e-06 -0.00091157423 -0.0014388079][0.014877289 0.019729126 0.022890054 0.023657449 0.021886874 0.018351246 0.014033855 0.010024928 0.0067638932 0.0042874687 0.0023836265 0.00087947166 -0.00025730429 -0.0010415803 -0.0014847106][0.012815326 0.017287157 0.020293511 0.021122288 0.019556701 0.016272273 0.012199168 0.0083988914 0.0053375447 0.0030661738 0.0014166574 0.0001908082 -0.00068062171 -0.0012562433 -0.0015653168][0.0098016132 0.013540881 0.016148567 0.01698941 0.01580403 0.013098818 0.0096554207 0.0063848617 0.0037357733 0.0017782669 0.00041262549 -0.00052724418 -0.0011244904 -0.0014784571 -0.0016465414][0.0064774454 0.0093550021 0.0114727 0.012276167 0.011495218 0.0094736982 0.0068006795 0.0042056404 0.0020900646 0.00054557063 -0.00047689 -0.0011127674 -0.001454884 -0.0016262305 -0.0016919265][0.0034770863 0.0055070384 0.0070971064 0.0077752667 0.0072905542 0.0058720093 0.0039519938 0.0020751781 0.00056541641 -0.0004909886 -0.0011300859 -0.001477281 -0.0016298797 -0.0016908457 -0.0017067924][0.0010740689 0.0023268145 0.0033707242 0.0038455506 0.0035407068 0.002606872 0.0013629051 0.00019222184 -0.00068522629 -0.0012396413 -0.0015253898 -0.0016537758 -0.0016966792 -0.0017110268 -0.001713298][-0.000563171 4.3192529e-05 0.00057666353 0.00082505983 0.00065630884 0.00015484658 -0.00048425852 -0.001040795 -0.0014119289 -0.0016108471 -0.0016883578 -0.0017117547 -0.0017145971 -0.0017151758 -0.0017149711][-0.0013829926 -0.0011667234 -0.00097062194 -0.00087834214 -0.00094857952 -0.0011430314 -0.0013768544 -0.0015600104 -0.0016636418 -0.0017061759 -0.0017156961 -0.0017165454 -0.0017157075 -0.0017153246 -0.0017148346][-0.0016842014 -0.0016453048 -0.0016040662 -0.0015818473 -0.001594466 -0.001632413 -0.0016749775 -0.0017024078 -0.0017139603 -0.0017175012 -0.0017183245 -0.0017178541 -0.0017167128 -0.0017155746 -0.0017144234]]...]
INFO - root - 2017-12-09 11:33:28.694120: step 18810, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 75h:17m:18s remains)
INFO - root - 2017-12-09 11:33:37.433186: step 18820, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.896 sec/batch; 78h:02m:37s remains)
INFO - root - 2017-12-09 11:33:46.073903: step 18830, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.849 sec/batch; 73h:58m:53s remains)
INFO - root - 2017-12-09 11:33:54.889392: step 18840, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.872 sec/batch; 75h:57m:36s remains)
INFO - root - 2017-12-09 11:34:03.461376: step 18850, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.855 sec/batch; 74h:27m:17s remains)
INFO - root - 2017-12-09 11:34:12.088094: step 18860, loss = 0.82, batch loss = 0.69 (9.7 examples/sec; 0.828 sec/batch; 72h:09m:50s remains)
INFO - root - 2017-12-09 11:34:20.756324: step 18870, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.844 sec/batch; 73h:31m:38s remains)
INFO - root - 2017-12-09 11:34:29.503231: step 18880, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.874 sec/batch; 76h:09m:50s remains)
INFO - root - 2017-12-09 11:34:38.183753: step 18890, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.889 sec/batch; 77h:25m:28s remains)
INFO - root - 2017-12-09 11:34:46.751757: step 18900, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.840 sec/batch; 73h:11m:01s remains)
2017-12-09 11:34:47.734990: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0017290383 -0.0017284745 -0.0017264184 -0.0017165743 -0.0016859991 -0.0016438524 -0.0015996301 -0.0015785991 -0.0015621363 -0.0015657557 -0.0015938083 -0.0016428538 -0.0016901869 -0.0017178116 -0.0017280775][-0.0017278612 -0.0017261757 -0.0017170061 -0.0016883931 -0.0016185093 -0.0015382974 -0.0014598685 -0.0014232256 -0.0013950113 -0.0014056978 -0.0014578811 -0.0015479303 -0.0016398303 -0.0016989564 -0.0017231837][-0.0017261443 -0.0017217135 -0.0016953456 -0.0016248034 -0.0014727344 -0.00131308 -0.0011675098 -0.001105705 -0.0010710645 -0.0011058461 -0.0012026699 -0.0013653429 -0.001534991 -0.0016553068 -0.0017090092][-0.0017232527 -0.0017141755 -0.0016555616 -0.0015109133 -0.0012217641 -0.00092503655 -0.00066724513 -0.00056335283 -0.00052501797 -0.00059450383 -0.00074764097 -0.001023375 -0.0013271708 -0.001564606 -0.0016793453][-0.0017203282 -0.0017051463 -0.0016002741 -0.0013446145 -0.00085974246 -0.00035886362 5.9507322e-05 0.00021814322 0.00024976162 0.0001333229 -8.8440254e-05 -0.00049951265 -0.00098000444 -0.0013964542 -0.0016189211][-0.0017181587 -0.0016977857 -0.0015469661 -0.0011751248 -0.0004945203 0.00022043101 0.00080428133 0.00104064 0.0010909731 0.00097543397 0.00071921106 0.00016926706 -0.00052364718 -0.0011636483 -0.0015321228][-0.0017139685 -0.0016912066 -0.0015131042 -0.001067636 -0.00026579341 0.00059314026 0.0012959486 0.0016224978 0.0017413227 0.0016957861 0.0014484257 0.00081150769 -6.2467181e-05 -0.00091315678 -0.0014346798][-0.0017029205 -0.0016811454 -0.0015104006 -0.001075157 -0.00028766308 0.00058426429 0.0013216359 0.0017359939 0.0019688173 0.0020441152 0.0018373022 0.0011690669 0.00019427529 -0.00076647673 -0.0013753071][-0.0016895782 -0.0016701702 -0.0015383646 -0.0011953996 -0.00055142015 0.00020499784 0.00089769275 0.0013787444 0.0017264665 0.0019151319 0.0017555791 0.0011115428 0.00014401658 -0.000794172 -0.0013868168][-0.0016849699 -0.001667358 -0.0015860202 -0.0013713614 -0.00093488512 -0.00036882272 0.00021918933 0.00072469003 0.0011386415 0.0013872944 0.0012553469 0.00067691691 -0.00018142827 -0.00097715482 -0.0014599554][-0.0017007829 -0.0016855592 -0.0016464486 -0.0015425711 -0.0013078828 -0.000945105 -0.00049449387 -1.7832732e-05 0.00039221044 0.00062365807 0.00049176556 5.1428797e-06 -0.00066398887 -0.0012371058 -0.0015594886][-0.001720164 -0.0017107095 -0.0016955865 -0.0016565404 -0.0015566179 -0.0013534687 -0.0010462773 -0.00065622549 -0.00031586178 -0.00014588656 -0.00028234441 -0.00065678114 -0.0011156675 -0.0014648438 -0.0016415881][-0.0017236086 -0.0017229562 -0.001719909 -0.0017104649 -0.0016804474 -0.0015855947 -0.0014089593 -0.0011511196 -0.00092982542 -0.00083638565 -0.00094995991 -0.0011900323 -0.0014469197 -0.0016155939 -0.0016910763][-0.0017229629 -0.0017220898 -0.0017219156 -0.0017207527 -0.001712734 -0.0016774271 -0.0015994406 -0.0014753751 -0.0013737334 -0.0013404948 -0.0014079427 -0.0015211926 -0.0016269336 -0.0016873891 -0.0017118629][-0.0017213714 -0.0017201665 -0.0017201547 -0.0017199082 -0.0017190134 -0.0017111556 -0.0016887347 -0.0016498493 -0.0016188882 -0.0016120354 -0.0016379434 -0.0016729633 -0.001700804 -0.0017144821 -0.0017193821]]...]
INFO - root - 2017-12-09 11:34:56.435867: step 18910, loss = 0.82, batch loss = 0.69 (8.7 examples/sec; 0.915 sec/batch; 79h:41m:01s remains)
INFO - root - 2017-12-09 11:35:05.271213: step 18920, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.872 sec/batch; 75h:56m:42s remains)
INFO - root - 2017-12-09 11:35:13.790039: step 18930, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.898 sec/batch; 78h:12m:58s remains)
INFO - root - 2017-12-09 11:35:22.614814: step 18940, loss = 0.82, batch loss = 0.69 (8.2 examples/sec; 0.981 sec/batch; 85h:24m:27s remains)
INFO - root - 2017-12-09 11:35:31.038587: step 18950, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.863 sec/batch; 75h:08m:30s remains)
INFO - root - 2017-12-09 11:35:39.525239: step 18960, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.858 sec/batch; 74h:44m:38s remains)
INFO - root - 2017-12-09 11:35:48.100875: step 18970, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.873 sec/batch; 76h:01m:05s remains)
INFO - root - 2017-12-09 11:35:56.591484: step 18980, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.879 sec/batch; 76h:34m:44s remains)
INFO - root - 2017-12-09 11:36:05.204924: step 18990, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.863 sec/batch; 75h:11m:32s remains)
INFO - root - 2017-12-09 11:36:13.800120: step 19000, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.860 sec/batch; 74h:55m:23s remains)
2017-12-09 11:36:14.733176: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.00099687313 0.0039917785 0.0087604607 0.014591867 0.020216888 0.024452943 0.026592907 0.026409008 0.024223493 0.020737896 0.016605856 0.012104648 0.0076026628 0.0037704236 0.0010232584][0.0031279395 0.00738532 0.014381824 0.023234254 0.032105017 0.0391475 0.043287903 0.04407353 0.04180048 0.037106082 0.03074686 0.023310164 0.015563382 0.0087130116 0.0036321157][0.0065712444 0.012511323 0.022155477 0.034453273 0.047050253 0.057491284 0.064383611 0.067009009 0.065405525 0.059873447 0.051080748 0.039971463 0.027897328 0.016779639 0.0081772357][0.011119455 0.019304093 0.032024182 0.047993209 0.064542748 0.078712158 0.088783175 0.093678892 0.0929671 0.086591743 0.075132884 0.059958152 0.043042097 0.027066441 0.014295401][0.015728081 0.026462777 0.042380214 0.062070768 0.082664929 0.10087679 0.11456744 0.1221135 0.122423 0.11504235 0.10056016 0.080944441 0.058949362 0.037980817 0.020940932][0.019384053 0.032554418 0.051415969 0.0744396 0.098688275 0.12061962 0.13753787 0.14722499 0.14806794 0.13943954 0.12211052 0.098603822 0.072313175 0.047212686 0.026657244][0.021288296 0.036285322 0.057379462 0.083016738 0.11009875 0.13475251 0.15374252 0.16436136 0.16479541 0.15457121 0.13487124 0.1086555 0.079697542 0.05221666 0.029735098][0.0213232 0.037237287 0.059464764 0.0864501 0.11488309 0.14058678 0.15984415 0.16978505 0.16883649 0.15694565 0.13579856 0.10858243 0.079133213 0.051543783 0.029187955][0.019836148 0.035563793 0.057417426 0.083819643 0.11137771 0.13580523 0.15332626 0.16127481 0.15855746 0.14561081 0.12449599 0.098311633 0.070668764 0.045282248 0.025100462][0.017233619 0.031632178 0.051546067 0.07535366 0.099738792 0.12073985 0.13490759 0.14015661 0.13597119 0.12306607 0.10356543 0.080275625 0.056421127 0.035132878 0.018703541][0.014035899 0.026159614 0.042766385 0.062347647 0.081940539 0.098190516 0.10831844 0.11087119 0.10583534 0.094065152 0.077511176 0.05853425 0.039802451 0.023721628 0.011794469][0.010451533 0.019862844 0.032569479 0.0472426 0.061508488 0.072809115 0.079118349 0.079547793 0.074410021 0.064548448 0.051636089 0.037557632 0.024323896 0.013528252 0.0059236484][0.0067016976 0.013400065 0.02231862 0.032391936 0.0418952 0.049016982 0.052388914 0.051529672 0.046910007 0.039311789 0.030090669 0.020666866 0.012371265 0.0060368543 0.001862862][0.0032362044 0.0074848728 0.013135618 0.019448344 0.025277898 0.029400006 0.030948346 0.029694019 0.026111145 0.020851051 0.014937019 0.0093358206 0.0047900127 0.0015879615 -0.00035249931][0.000597922 0.002903658 0.0060473979 0.0095860455 0.012826997 0.015005806 0.015597958 0.014511826 0.012138565 0.0089809224 0.0057151834 0.0028849072 0.00079163944 -0.00055283168 -0.0012942394]]...]
INFO - root - 2017-12-09 11:36:23.402184: step 19010, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.853 sec/batch; 74h:14m:44s remains)
INFO - root - 2017-12-09 11:36:32.039168: step 19020, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.849 sec/batch; 73h:57m:28s remains)
INFO - root - 2017-12-09 11:36:40.600090: step 19030, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.897 sec/batch; 78h:06m:48s remains)
INFO - root - 2017-12-09 11:36:49.253641: step 19040, loss = 0.81, batch loss = 0.68 (9.0 examples/sec; 0.887 sec/batch; 77h:14m:20s remains)
INFO - root - 2017-12-09 11:36:57.813829: step 19050, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 75h:40m:33s remains)
INFO - root - 2017-12-09 11:37:06.487933: step 19060, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.833 sec/batch; 72h:33m:02s remains)
INFO - root - 2017-12-09 11:37:15.206584: step 19070, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.870 sec/batch; 75h:46m:01s remains)
INFO - root - 2017-12-09 11:37:24.039218: step 19080, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.858 sec/batch; 74h:44m:06s remains)
INFO - root - 2017-12-09 11:37:32.785362: step 19090, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.847 sec/batch; 73h:42m:31s remains)
INFO - root - 2017-12-09 11:37:41.381186: step 19100, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.861 sec/batch; 74h:57m:21s remains)
2017-12-09 11:37:42.240064: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0015973173 -0.0013544947 -0.00099250767 -0.00058787141 -0.00022383081 -8.7706139e-06 3.1279633e-06 -0.00017503626 -0.0004768261 -0.00083755597 -0.0011641331 -0.0014201291 -0.0015875959 -0.0016800053 -0.0017221932][-0.0013741259 -0.00090909295 -0.00028880057 0.00032256229 0.00079780573 0.0010203494 0.00095043646 0.0006252093 0.00013807497 -0.00041113805 -0.00090227212 -0.0012795682 -0.0015230749 -0.0016562069 -0.0017162791][-0.00081071287 9.2837028e-05 0.0011993818 0.0021957592 0.0028561563 0.0030683875 0.0028682062 0.0023406586 0.0015882548 0.00071456248 -0.0001408559 -0.00084435148 -0.0013212569 -0.0015833409 -0.0016969554][0.00028978602 0.0019204993 0.0038044318 0.0054467493 0.0064958837 0.0068524862 0.0066381795 0.0059383819 0.0047957147 0.0033042342 0.0016824052 0.00025432336 -0.00076946535 -0.0013581802 -0.0016253052][0.0018933868 0.0044678496 0.0073498562 0.0098761627 0.011581096 0.012358663 0.012369505 0.011615168 0.010010083 0.0076395161 0.0048631583 0.002293516 0.00034755969 -0.00084456574 -0.0014333319][0.0036446638 0.0071807345 0.011092476 0.014614605 0.017217675 0.018747736 0.019255331 0.018565521 0.016459772 0.01306781 0.008945426 0.0050255894 0.0019426731 -4.5532826e-05 -0.0011032936][0.0050312453 0.0093083642 0.014057637 0.01847755 0.022002636 0.024387155 0.025469203 0.024870383 0.022301329 0.018011251 0.012749777 0.0076721944 0.0035625477 0.00081079861 -0.00072837935][0.0056494661 0.010242122 0.015407688 0.020361166 0.024520142 0.027549857 0.029062854 0.028532084 0.025663525 0.020845389 0.01496053 0.0092528788 0.0045595607 0.0013515066 -0.0004860569][0.0053179977 0.0096822269 0.014652343 0.019524084 0.0237435 0.02694457 0.02861982 0.028185947 0.025354035 0.020569075 0.014749686 0.0091131506 0.0044767545 0.0013050708 -0.00050970085][0.0040920433 0.0077061662 0.011856303 0.015983354 0.01963507 0.022507217 0.024099045 0.023851832 0.02145485 0.017290004 0.012205463 0.0073102247 0.00334863 0.00069603918 -0.00078106846][0.0023019081 0.0048365635 0.0077610789 0.010709203 0.013385281 0.015603133 0.016945839 0.016928202 0.015224311 0.012090011 0.0082274722 0.0045532519 0.0016733349 -0.00017501623 -0.0011514556][0.00050797116 0.001971941 0.0036725174 0.0054246406 0.0070807105 0.0085610859 0.009561236 0.0097046709 0.0086941822 0.0066777905 0.0041736332 0.0018436498 0.00010279648 -0.00094626512 -0.0014586779][-0.00080964924 -0.0001508604 0.00062977977 0.001473266 0.0023254827 0.0031656129 0.0037998119 0.003977105 0.0034872834 0.0024121022 0.0010777953 -0.0001212717 -0.00096316804 -0.0014305878 -0.001636393][-0.0014822199 -0.0012644005 -0.00099671539 -0.00067918596 -0.0003212291 7.1979361e-05 0.00039448438 0.00051281403 0.00031911593 -0.0001406502 -0.00070096378 -0.0011803904 -0.0014922427 -0.0016487731 -0.0017090263][-0.0017118047 -0.0016683062 -0.0016062669 -0.0015185482 -0.0014055709 -0.0012692523 -0.0011511808 -0.0011010142 -0.0011557487 -0.0012967241 -0.0014642654 -0.0016002762 -0.0016816721 -0.0017177442 -0.0017296957]]...]
INFO - root - 2017-12-09 11:37:50.826419: step 19110, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.885 sec/batch; 77h:00m:29s remains)
INFO - root - 2017-12-09 11:37:59.497562: step 19120, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.880 sec/batch; 76h:36m:48s remains)
INFO - root - 2017-12-09 11:38:08.106703: step 19130, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 75h:30m:25s remains)
INFO - root - 2017-12-09 11:38:16.908718: step 19140, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.873 sec/batch; 76h:00m:05s remains)
INFO - root - 2017-12-09 11:38:25.494085: step 19150, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.846 sec/batch; 73h:38m:06s remains)
INFO - root - 2017-12-09 11:38:34.250858: step 19160, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.860 sec/batch; 74h:51m:00s remains)
INFO - root - 2017-12-09 11:38:43.074080: step 19170, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 75h:48m:21s remains)
INFO - root - 2017-12-09 11:38:51.754343: step 19180, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.861 sec/batch; 74h:56m:02s remains)
INFO - root - 2017-12-09 11:39:00.487279: step 19190, loss = 0.81, batch loss = 0.68 (9.2 examples/sec; 0.868 sec/batch; 75h:31m:44s remains)
INFO - root - 2017-12-09 11:39:09.091619: step 19200, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.904 sec/batch; 78h:39m:29s remains)
2017-12-09 11:39:09.917847: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.005535183 0.0062174816 0.0066008149 0.0065988535 0.0064519355 0.0061614877 0.0058117351 0.0052998904 0.0045097722 0.0034583216 0.0021503768 0.00080756831 -0.00034950615 -0.0011397896 -0.0015599901][0.0084778126 0.0092091514 0.00969969 0.0099519519 0.010209876 0.010349661 0.010284511 0.0098224441 0.0087179439 0.0069974419 0.0048072343 0.0025149677 0.00050437462 -0.00086373131 -0.0015252852][0.012516916 0.013222991 0.013796426 0.014404392 0.015225103 0.015999733 0.016479637 0.016297238 0.014974182 0.012527399 0.0092430357 0.0056291083 0.0023178575 -6.3594664e-05 -0.0012885613][0.019882418 0.021215158 0.022491936 0.023962894 0.025492383 0.026541194 0.026780937 0.025812523 0.023289764 0.01941108 0.014602873 0.0094538219 0.0046803877 0.0011091343 -0.00087926764][0.032388352 0.035061996 0.037541423 0.040055659 0.042175815 0.043126892 0.042390212 0.039684415 0.034958489 0.028696269 0.021561768 0.01428766 0.0076404344 0.0026149461 -0.00033245608][0.048519235 0.053306013 0.057395168 0.060758032 0.0627917 0.062658004 0.059969768 0.0547066 0.047246423 0.038357683 0.028764416 0.01933253 0.010807666 0.0043058931 0.00031356688][0.063726038 0.070355177 0.075850554 0.079965271 0.081889689 0.080589004 0.075837128 0.067940772 0.057852793 0.046530947 0.034715608 0.0234202 0.013359859 0.0057038451 0.00086878275][0.07421463 0.081754349 0.087927476 0.092391342 0.094214194 0.092144012 0.086086728 0.076561965 0.064820066 0.051878929 0.038541831 0.025962573 0.01489475 0.0065150065 0.0011830908][0.079099543 0.086919732 0.093159832 0.097553954 0.099110745 0.0965372 0.089842282 0.079681091 0.0673403 0.053808834 0.039839525 0.026692128 0.015213914 0.0065999636 0.001194182][0.080043919 0.087711059 0.093443878 0.097232953 0.098104 0.094956733 0.088005356 0.077902675 0.065748535 0.052355584 0.03845467 0.025419027 0.014154397 0.0058680982 0.00085636193][0.079477765 0.086450055 0.09095411 0.093677394 0.093668118 0.09012007 0.08325386 0.073569059 0.061903164 0.048829541 0.035231844 0.022605209 0.011979719 0.004490756 0.00024638243][0.078213826 0.084034286 0.086883314 0.088137984 0.087031722 0.083096996 0.076401129 0.067278609 0.056218702 0.043645855 0.030653758 0.018810274 0.009231722 0.0028590127 -0.0004477629][0.076149032 0.080438666 0.081586532 0.081484593 0.079497583 0.0753068 0.068781793 0.060078822 0.0494665 0.03749273 0.025384404 0.014697749 0.0064771259 0.0013830563 -0.0010005068][0.072056964 0.07482481 0.074678957 0.073680766 0.071322419 0.067306876 0.061220616 0.053026021 0.042930815 0.031680152 0.020595487 0.011193989 0.0043286579 0.00036558125 -0.0013193163][0.065660954 0.067092739 0.06603317 0.0644997 0.062051557 0.058365315 0.052877925 0.045421664 0.036158375 0.025953813 0.016140826 0.00812697 0.002582219 -0.00037699076 -0.0015103183]]...]
INFO - root - 2017-12-09 11:39:18.759476: step 19210, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.878 sec/batch; 76h:22m:49s remains)
INFO - root - 2017-12-09 11:39:27.371782: step 19220, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.893 sec/batch; 77h:44m:19s remains)
INFO - root - 2017-12-09 11:39:35.903950: step 19230, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.861 sec/batch; 74h:53m:00s remains)
INFO - root - 2017-12-09 11:39:44.674868: step 19240, loss = 0.83, batch loss = 0.70 (8.7 examples/sec; 0.922 sec/batch; 80h:13m:08s remains)
INFO - root - 2017-12-09 11:39:53.369762: step 19250, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.893 sec/batch; 77h:42m:16s remains)
INFO - root - 2017-12-09 11:40:02.055297: step 19260, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.879 sec/batch; 76h:27m:42s remains)
INFO - root - 2017-12-09 11:40:10.711458: step 19270, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.848 sec/batch; 73h:46m:18s remains)
INFO - root - 2017-12-09 11:40:19.411737: step 19280, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.858 sec/batch; 74h:36m:30s remains)
INFO - root - 2017-12-09 11:40:28.151310: step 19290, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.884 sec/batch; 76h:53m:15s remains)
INFO - root - 2017-12-09 11:40:36.685034: step 19300, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.837 sec/batch; 72h:48m:30s remains)
2017-12-09 11:40:37.677320: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0017391981 -0.0017381249 -0.0017379739 -0.0017380975 -0.0017383116 -0.0017385599 -0.0017388366 -0.0017390663 -0.0017392676 -0.001739406 -0.0017394644 -0.0017394371 -0.0017393213 -0.0017391548 -0.0017389054][-0.0017346203 -0.0017333607 -0.0017331685 -0.0017332366 -0.0017333502 -0.0017335002 -0.0017336971 -0.0017338834 -0.0017341081 -0.0017343122 -0.0017344393 -0.0017344558 -0.0017343587 -0.0017341798 -0.0017339061][-0.0017306488 -0.0017293403 -0.0017291675 -0.0017292107 -0.001729255 -0.0017293072 -0.0017293906 -0.0017294987 -0.0017297321 -0.0017299837 -0.001730164 -0.0017302244 -0.001730165 -0.0017299745 -0.0017296741][-0.0017276161 -0.0017263648 -0.0017262709 -0.0017263032 -0.0017262643 -0.0017262014 -0.0017261509 -0.0017261488 -0.0017263632 -0.0017266655 -0.0017269219 -0.0017270532 -0.0017270672 -0.0017269222 -0.0017266383][-0.0017260524 -0.0017249224 -0.0017249063 -0.0017249399 -0.0017248058 -0.001724571 -0.0017243146 -0.0017241554 -0.0017243064 -0.0017246201 -0.0017249447 -0.0017251865 -0.0017253434 -0.0017253225 -0.0017251169][-0.0017250144 -0.0017239453 -0.0017239973 -0.0017240411 -0.001723815 -0.0017233954 -0.0017229163 -0.0017225639 -0.0017225906 -0.0017228267 -0.0017231572 -0.0017235108 -0.0017238694 -0.0017240493 -0.0017239816][-0.0017239996 -0.0017230164 -0.001723112 -0.0017231812 -0.0017229089 -0.0017223624 -0.001721708 -0.0017212062 -0.0017210981 -0.001721187 -0.001721461 -0.0017218952 -0.0017224208 -0.001722774 -0.0017228646][-0.0017235047 -0.0017225508 -0.0017227466 -0.001722903 -0.0017226398 -0.0017220384 -0.0017212937 -0.0017206564 -0.0017203806 -0.0017202708 -0.0017204394 -0.0017208417 -0.0017214395 -0.0017219451 -0.0017222108][-0.0017233435 -0.0017223641 -0.0017226171 -0.0017228009 -0.0017225272 -0.0017219012 -0.0017211337 -0.0017204508 -0.0017200923 -0.0017198543 -0.0017199197 -0.0017202825 -0.0017208954 -0.0017214887 -0.0017218795][-0.0017231626 -0.0017221555 -0.0017224082 -0.0017225782 -0.0017223071 -0.0017217023 -0.0017209906 -0.0017203791 -0.0017200431 -0.0017197507 -0.0017197427 -0.0017200682 -0.001720649 -0.0017212452 -0.0017216633][-0.0017229711 -0.0017219719 -0.0017221752 -0.0017223574 -0.0017221468 -0.0017216494 -0.0017210759 -0.0017205967 -0.001720323 -0.0017200329 -0.001720011 -0.0017203159 -0.001720825 -0.0017213554 -0.0017217319][-0.0017227179 -0.0017217924 -0.0017220298 -0.0017222667 -0.0017221782 -0.0017218566 -0.0017214492 -0.0017210807 -0.0017208536 -0.0017205867 -0.0017205377 -0.0017207537 -0.0017211146 -0.0017215235 -0.0017218555][-0.0017228074 -0.0017219089 -0.0017221449 -0.0017223728 -0.0017223413 -0.001722137 -0.0017218411 -0.0017215405 -0.0017213261 -0.0017210996 -0.0017210197 -0.0017211256 -0.0017213067 -0.00172158 -0.0017218562][-0.0017231536 -0.0017222367 -0.0017224409 -0.0017226215 -0.0017226037 -0.0017224411 -0.0017222029 -0.0017219406 -0.0017217103 -0.0017214766 -0.0017213506 -0.0017213569 -0.0017214017 -0.0017215409 -0.0017217537][-0.0017237236 -0.0017228114 -0.0017229137 -0.0017230749 -0.0017230598 -0.0017229165 -0.0017227002 -0.0017224196 -0.0017221358 -0.0017218699 -0.0017216851 -0.0017215891 -0.0017215459 -0.0017215989 -0.0017217514]]...]
INFO - root - 2017-12-09 11:40:46.305631: step 19310, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.853 sec/batch; 74h:11m:38s remains)
INFO - root - 2017-12-09 11:40:55.083580: step 19320, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.877 sec/batch; 76h:17m:20s remains)
INFO - root - 2017-12-09 11:41:03.629543: step 19330, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.882 sec/batch; 76h:45m:36s remains)
INFO - root - 2017-12-09 11:41:12.205676: step 19340, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.842 sec/batch; 73h:15m:38s remains)
INFO - root - 2017-12-09 11:41:20.548793: step 19350, loss = 0.81, batch loss = 0.68 (9.3 examples/sec; 0.862 sec/batch; 74h:56m:32s remains)
INFO - root - 2017-12-09 11:41:29.172227: step 19360, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.872 sec/batch; 75h:51m:27s remains)
INFO - root - 2017-12-09 11:41:37.600463: step 19370, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.847 sec/batch; 73h:40m:13s remains)
INFO - root - 2017-12-09 11:41:46.206615: step 19380, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.882 sec/batch; 76h:44m:46s remains)
INFO - root - 2017-12-09 11:41:55.040834: step 19390, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.876 sec/batch; 76h:13m:48s remains)
INFO - root - 2017-12-09 11:42:03.558020: step 19400, loss = 0.82, batch loss = 0.69 (9.8 examples/sec; 0.816 sec/batch; 71h:00m:11s remains)
2017-12-09 11:42:04.447539: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.028354611 0.032784816 0.038200222 0.043599222 0.047665015 0.049739055 0.050612576 0.05035007 0.04893164 0.045650903 0.039998308 0.032157421 0.023311807 0.015109069 0.0081871394][0.019451374 0.022147002 0.026215766 0.031302013 0.035755578 0.038979847 0.041239079 0.042548411 0.042593591 0.040513188 0.036042847 0.029397478 0.021613985 0.014123126 0.0077171368][0.017941631 0.018568821 0.020771118 0.024775881 0.029027047 0.0329382 0.03647681 0.039185002 0.04050732 0.039353646 0.035722584 0.029747898 0.022297902 0.014790344 0.0082917064][0.024044126 0.023019666 0.02366652 0.026677 0.030718392 0.035171054 0.039838158 0.043678287 0.0458414 0.044951446 0.041344021 0.034959938 0.026646717 0.017976211 0.010435986][0.034297053 0.032611348 0.032645922 0.035310954 0.039652646 0.044872474 0.050672017 0.0553213 0.057804789 0.056500591 0.052072696 0.044239026 0.033974137 0.023153087 0.013792618][0.044771075 0.04313048 0.043368392 0.046582397 0.052032705 0.058387574 0.065414362 0.070692845 0.0731559 0.07106398 0.06525895 0.05540064 0.042632833 0.029273327 0.017771086][0.052424349 0.051356561 0.052166514 0.05616843 0.062982336 0.070775419 0.07913328 0.084959924 0.08725553 0.084276 0.076910742 0.064959146 0.049825389 0.034302555 0.021030039][0.055489 0.055414204 0.057105582 0.061929446 0.069653206 0.078384988 0.0875918 0.093821026 0.0959924 0.092365958 0.083715223 0.070125878 0.053389728 0.03665074 0.022466788][0.054069094 0.054945953 0.057398293 0.062798388 0.070988663 0.08016856 0.089527845 0.095681116 0.097511269 0.093410179 0.083935812 0.069565915 0.052383661 0.035656083 0.021647802][0.050376363 0.051826335 0.054621838 0.060009893 0.067830421 0.076521352 0.085111022 0.090574764 0.091776371 0.087327659 0.077552989 0.063355707 0.046964437 0.031502567 0.018737802][0.04722321 0.048503976 0.050904013 0.055556912 0.062233593 0.069681175 0.076791726 0.080947563 0.081103347 0.076247029 0.066628791 0.053437456 0.038802054 0.025465302 0.014646831][0.046261653 0.046676196 0.047942761 0.051069651 0.055892881 0.061409879 0.066534862 0.069151215 0.068231575 0.063027307 0.05389455 0.042202786 0.029849565 0.018987888 0.010365333][0.047674075 0.046435587 0.045783486 0.046771657 0.049293056 0.0525376 0.055597808 0.056769878 0.054992605 0.049693458 0.041350082 0.031454697 0.021551488 0.013131858 0.0066111279][0.050100084 0.046783742 0.043764003 0.042218812 0.042215411 0.043159258 0.044363838 0.044462427 0.04233925 0.037425924 0.030222205 0.022187866 0.014546613 0.008282626 0.0035956362][0.051379733 0.046287663 0.0411611 0.037338413 0.035039593 0.033930004 0.033580769 0.032902956 0.030798228 0.026666878 0.020950144 0.014857643 0.00926004 0.0047416515 0.0014513071]]...]
INFO - root - 2017-12-09 11:42:13.106167: step 19410, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.849 sec/batch; 73h:52m:31s remains)
INFO - root - 2017-12-09 11:42:21.843979: step 19420, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.876 sec/batch; 76h:10m:18s remains)
INFO - root - 2017-12-09 11:42:30.274898: step 19430, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.838 sec/batch; 72h:54m:12s remains)
INFO - root - 2017-12-09 11:42:38.873984: step 19440, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.830 sec/batch; 72h:09m:36s remains)
INFO - root - 2017-12-09 11:42:47.307699: step 19450, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.856 sec/batch; 74h:28m:25s remains)
INFO - root - 2017-12-09 11:42:55.987058: step 19460, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 75h:05m:46s remains)
INFO - root - 2017-12-09 11:43:04.687992: step 19470, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 75h:41m:35s remains)
INFO - root - 2017-12-09 11:43:13.344986: step 19480, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.865 sec/batch; 75h:10m:17s remains)
INFO - root - 2017-12-09 11:43:21.934502: step 19490, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.838 sec/batch; 72h:51m:22s remains)
INFO - root - 2017-12-09 11:43:30.405496: step 19500, loss = 0.82, batch loss = 0.69 (10.1 examples/sec; 0.792 sec/batch; 68h:53m:08s remains)
2017-12-09 11:43:31.338589: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0017824887 -0.001781064 -0.0017793332 -0.0017777687 -0.0017769638 -0.0017772645 -0.0017784963 -0.001779962 -0.0017809924 -0.0017813729 -0.0017813256 -0.0017812063 -0.0017810701 -0.0017809308 -0.0017807819][-0.0017782132 -0.0017720563 -0.0017645294 -0.0017579987 -0.0017551376 -0.001757293 -0.0017632314 -0.0017700016 -0.001775148 -0.0017781124 -0.0017794499 -0.0017801238 -0.0017804046 -0.001780487 -0.0017803879][-0.001763244 -0.0017424636 -0.0017176296 -0.0016965661 -0.0016878032 -0.001695219 -0.0017142987 -0.0017362132 -0.0017537073 -0.0017652955 -0.0017718807 -0.0017758386 -0.0017781835 -0.0017795647 -0.0017800358][-0.0017281396 -0.0016755748 -0.0016138495 -0.0015617815 -0.0015393767 -0.0015551175 -0.001598375 -0.0016502363 -0.0016943672 -0.0017268701 -0.0017479003 -0.0017617227 -0.0017706696 -0.0017763674 -0.0017790624][-0.0016626167 -0.0015581215 -0.0014368674 -0.0013336377 -0.0012845462 -0.0013049855 -0.0013773001 -0.0014714516 -0.0015591877 -0.0016314224 -0.001683946 -0.0017218229 -0.0017483529 -0.0017661498 -0.0017756941][-0.0015558811 -0.0013829813 -0.0011862344 -0.0010179335 -0.00092940242 -0.000943574 -0.0010377066 -0.0011744522 -0.0013151383 -0.0014434012 -0.001547293 -0.0016301321 -0.0016935966 -0.0017390547 -0.0017655758][-0.001404281 -0.0011595506 -0.00088981359 -0.0006603084 -0.00053050346 -0.00052788504 -0.00062810909 -0.00079264084 -0.00097829278 -0.0011630065 -0.0013277021 -0.0014722827 -0.0015931545 -0.0016859045 -0.001743942][-0.0012366187 -0.00093961722 -0.00062703481 -0.00036705914 -0.00021610793 -0.00019826845 -0.00028883817 -0.00045336469 -0.00065329461 -0.00086808996 -0.0010780881 -0.0012804996 -0.0014640582 -0.0016139499 -0.0017131129][-0.0011258211 -0.00082044193 -0.000517174 -0.00027608383 -0.000140563 -0.00012087484 -0.00019222125 -0.0003276827 -0.000500927 -0.0007008547 -0.00091500318 -0.0011416525 -0.001363501 -0.0015547518 -0.001686731][-0.0011502451 -0.00088810438 -0.00064260338 -0.00045901549 -0.00036526273 -0.00035789143 -0.00041138346 -0.00050448254 -0.00062340044 -0.0007694026 -0.00094165554 -0.001143046 -0.0013549107 -0.0015463413 -0.0016821015][-0.0013122472 -0.0011268744 -0.00096065254 -0.00084445754 -0.0007948135 -0.00080167165 -0.00084146281 -0.00089721062 -0.0009626078 -0.001045903 -0.00115404 -0.0012935963 -0.0014501668 -0.0015971739 -0.0017031366][-0.0015220162 -0.0014173875 -0.0013253205 -0.0012644269 -0.0012443161 -0.0012566753 -0.0012841546 -0.0013140645 -0.0013427038 -0.0013784468 -0.0014293689 -0.0015024133 -0.0015896668 -0.0016744038 -0.0017357856][-0.0016798968 -0.0016348533 -0.0015944666 -0.0015682277 -0.0015617625 -0.0015713038 -0.0015874604 -0.0016023533 -0.0016131304 -0.0016242951 -0.0016409572 -0.0016679358 -0.0017024321 -0.001737057 -0.0017621339][-0.0017550755 -0.0017414473 -0.0017285537 -0.001719941 -0.0017182881 -0.0017227891 -0.0017298909 -0.0017360012 -0.0017394823 -0.0017417854 -0.0017449404 -0.0017509989 -0.0017595022 -0.0017683922 -0.0017748384][-0.0017753276 -0.0017728271 -0.0017702662 -0.0017683805 -0.0017680711 -0.0017694867 -0.0017717734 -0.0017738069 -0.001774905 -0.0017752739 -0.0017753828 -0.0017757759 -0.0017765446 -0.0017774936 -0.001778223]]...]
INFO - root - 2017-12-09 11:43:40.131439: step 19510, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 75h:05m:46s remains)
INFO - root - 2017-12-09 11:43:48.839074: step 19520, loss = 0.83, batch loss = 0.70 (9.0 examples/sec; 0.884 sec/batch; 76h:51m:58s remains)
INFO - root - 2017-12-09 11:43:57.432071: step 19530, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.883 sec/batch; 76h:43m:51s remains)
INFO - root - 2017-12-09 11:44:05.971242: step 19540, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.859 sec/batch; 74h:37m:58s remains)
INFO - root - 2017-12-09 11:44:14.590684: step 19550, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 75h:08m:47s remains)
INFO - root - 2017-12-09 11:44:23.119170: step 19560, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.836 sec/batch; 72h:39m:29s remains)
INFO - root - 2017-12-09 11:44:31.865589: step 19570, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.878 sec/batch; 76h:18m:28s remains)
INFO - root - 2017-12-09 11:44:40.578065: step 19580, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.842 sec/batch; 73h:12m:31s remains)
INFO - root - 2017-12-09 11:44:49.309144: step 19590, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.865 sec/batch; 75h:08m:37s remains)
INFO - root - 2017-12-09 11:44:57.895977: step 19600, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.854 sec/batch; 74h:15m:40s remains)
2017-12-09 11:44:58.705261: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.062152687 0.062734365 0.062186871 0.062090646 0.062254217 0.062693089 0.063190393 0.063631572 0.063996725 0.063602529 0.062688909 0.061515886 0.059444007 0.057744525 0.055876166][0.066766448 0.067618541 0.067497261 0.0677818 0.06843774 0.069520921 0.070714518 0.071804672 0.072459422 0.072330222 0.071323924 0.069377407 0.066609532 0.06386856 0.061141137][0.066262834 0.067772806 0.068127476 0.068829171 0.069896415 0.071427673 0.073113762 0.074619234 0.0755371 0.075480185 0.074245676 0.071765438 0.068452612 0.064778052 0.0613635][0.0655574 0.068224445 0.069354184 0.0707432 0.072195895 0.073735483 0.075362511 0.076844327 0.077663735 0.077476777 0.076031424 0.073186822 0.069283627 0.06484361 0.060612295][0.062833816 0.06678506 0.069093779 0.071271747 0.073131658 0.074631862 0.075659908 0.076434612 0.076487295 0.075675823 0.073742233 0.07065957 0.066601761 0.061951142 0.057436205][0.054307692 0.059305757 0.062816687 0.0661127 0.068621226 0.07030198 0.070950054 0.070921317 0.069953993 0.068156943 0.065490864 0.062021267 0.057956189 0.053502273 0.049246356][0.040236749 0.045179967 0.049153585 0.052994508 0.056144446 0.058275677 0.058933947 0.058546532 0.057120711 0.054713827 0.051411245 0.047703143 0.043864608 0.039933506 0.036238529][0.02374791 0.027471265 0.030937204 0.034507178 0.037551876 0.039758731 0.040711243 0.040624324 0.039427292 0.037119657 0.034026489 0.030626019 0.027198223 0.02392119 0.020982312][0.00991657 0.012140255 0.014472445 0.016968193 0.019186834 0.020878343 0.021705747 0.021831822 0.021079732 0.019536104 0.017343782 0.014947052 0.012566958 0.010342496 0.0083841523][0.0014029917 0.0022769491 0.0033439635 0.0045656851 0.0056778933 0.0065520531 0.0070459084 0.007219261 0.0069310018 0.0061822492 0.005097304 0.0039022225 0.0027810233 0.0017500677 0.00084748177][-0.0013129691 -0.0011264716 -0.00085327384 -0.00050477427 -0.00016833981 0.00010654517 0.00028455968 0.00037780416 0.00033297727 0.00012808922 -0.0001878523 -0.00053123769 -0.00085066119 -0.0011560894 -0.0014222662][-0.0017304909 -0.0017280014 -0.0017212558 -0.0017114587 -0.0016986077 -0.0016868344 -0.0016808474 -0.0016775411 -0.0016814676 -0.0016917734 -0.0017051188 -0.0017140167 -0.0017189626 -0.0017238314 -0.0017276009][-0.0017332711 -0.0017331593 -0.0017349091 -0.0017380367 -0.0017394621 -0.0017382941 -0.0017371407 -0.0017363816 -0.0017360048 -0.0017356981 -0.0017355763 -0.0017351247 -0.0017350148 -0.0017344257 -0.0017333644][-0.0017363081 -0.0017357843 -0.0017364632 -0.0017373277 -0.0017377585 -0.0017371438 -0.0017363756 -0.0017356637 -0.0017351193 -0.0017345214 -0.0017339553 -0.001733914 -0.0017341701 -0.0017339825 -0.0017328765][-0.0017370707 -0.0017359505 -0.0017358281 -0.0017357128 -0.0017356662 -0.0017358372 -0.0017359349 -0.0017353368 -0.001734777 -0.0017334984 -0.00173225 -0.0017317525 -0.0017315674 -0.001731654 -0.0017318758]]...]
INFO - root - 2017-12-09 11:45:07.374320: step 19610, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.850 sec/batch; 73h:53m:35s remains)
INFO - root - 2017-12-09 11:45:16.045766: step 19620, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.856 sec/batch; 74h:22m:28s remains)
INFO - root - 2017-12-09 11:45:24.536319: step 19630, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.881 sec/batch; 76h:33m:54s remains)
INFO - root - 2017-12-09 11:45:33.389343: step 19640, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.832 sec/batch; 72h:19m:16s remains)
INFO - root - 2017-12-09 11:45:41.901557: step 19650, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.831 sec/batch; 72h:13m:32s remains)
INFO - root - 2017-12-09 11:45:50.571547: step 19660, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.878 sec/batch; 76h:19m:53s remains)
INFO - root - 2017-12-09 11:45:59.231360: step 19670, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.846 sec/batch; 73h:29m:50s remains)
INFO - root - 2017-12-09 11:46:07.839541: step 19680, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.856 sec/batch; 74h:20m:21s remains)
INFO - root - 2017-12-09 11:46:16.420311: step 19690, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 75h:23m:01s remains)
INFO - root - 2017-12-09 11:46:25.006876: step 19700, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.848 sec/batch; 73h:42m:23s remains)
2017-12-09 11:46:25.819223: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0017768326 -0.0017773939 -0.0017783509 -0.0017793823 -0.0017802919 -0.0017808857 -0.00178093 -0.001780667 -0.0017804101 -0.0017801597 -0.0017799456 -0.0017796155 -0.0017790645 -0.0017785296 -0.0017777302][-0.0017765551 -0.0017771823 -0.0017780657 -0.0017788509 -0.0017793588 -0.0017795396 -0.0017792329 -0.0017788213 -0.0017787211 -0.0017788573 -0.0017790677 -0.0017790867 -0.0017788776 -0.0017785666 -0.0017778509][-0.0017763736 -0.0017766259 -0.0017770143 -0.0017770574 -0.0017767991 -0.0017762409 -0.0017754339 -0.0017748597 -0.0017751479 -0.0017760188 -0.0017769278 -0.0017776776 -0.0017780238 -0.0017781454 -0.0017777592][-0.0017760094 -0.00177548 -0.0017748122 -0.0017737364 -0.0017721985 -0.0017704759 -0.0017688918 -0.0017683883 -0.0017694868 -0.0017715035 -0.0017736658 -0.0017755412 -0.0017768259 -0.0017774408 -0.0017771655][-0.0017750874 -0.0017732827 -0.0017711249 -0.0017686831 -0.0017656292 -0.0017625269 -0.001760286 -0.0017601741 -0.0017623532 -0.0017657775 -0.0017693917 -0.0017728131 -0.0017754117 -0.0017767495 -0.0017767876][-0.0017738156 -0.0017703868 -0.0017665207 -0.0017625898 -0.0017581023 -0.0017538917 -0.0017511662 -0.001751523 -0.0017548479 -0.0017597539 -0.0017648134 -0.0017694609 -0.0017731085 -0.0017752888 -0.0017757692][-0.0017728818 -0.0017679242 -0.0017623296 -0.0017569332 -0.0017513032 -0.0017463573 -0.0017435328 -0.0017442589 -0.00174851 -0.0017548735 -0.0017614444 -0.0017672444 -0.0017717486 -0.00177459 -0.0017754714][-0.0017727596 -0.0017667895 -0.0017598573 -0.0017532243 -0.0017471387 -0.0017419279 -0.0017392283 -0.0017402939 -0.0017451495 -0.0017525795 -0.0017602209 -0.0017667833 -0.0017718179 -0.0017750605 -0.0017761067][-0.0017736026 -0.0017674937 -0.0017600458 -0.001752833 -0.0017467624 -0.0017419958 -0.0017395841 -0.0017409964 -0.0017461764 -0.0017540022 -0.0017617161 -0.0017679723 -0.0017728115 -0.0017761567 -0.0017773565][-0.0017746179 -0.0017692174 -0.0017623137 -0.0017556457 -0.0017501452 -0.0017460798 -0.0017442961 -0.0017458 -0.00175074 -0.0017580985 -0.0017651756 -0.0017709653 -0.0017750578 -0.0017778176 -0.0017789891][-0.0017758708 -0.0017716335 -0.0017663105 -0.0017608333 -0.001756266 -0.0017528888 -0.0017514776 -0.001752897 -0.0017572126 -0.0017634598 -0.0017693695 -0.0017740721 -0.0017771992 -0.0017793736 -0.0017802854][-0.0017767784 -0.0017742331 -0.0017708895 -0.0017672034 -0.001763926 -0.0017613376 -0.0017601467 -0.0017609753 -0.0017640615 -0.0017687416 -0.001773334 -0.0017768637 -0.0017792336 -0.0017807857 -0.0017814994][-0.0017771457 -0.0017759709 -0.001774448 -0.0017724638 -0.0017706172 -0.0017690603 -0.0017683139 -0.0017685166 -0.0017701443 -0.0017729208 -0.0017759847 -0.0017785473 -0.0017803604 -0.0017814283 -0.0017819026][-0.0017774017 -0.0017767489 -0.0017763694 -0.0017756321 -0.0017747465 -0.0017739685 -0.0017736477 -0.0017737392 -0.0017744494 -0.0017757334 -0.0017773141 -0.0017790329 -0.0017804203 -0.0017813038 -0.0017817692][-0.0017781991 -0.0017774588 -0.0017772019 -0.0017769323 -0.0017765668 -0.0017760951 -0.0017759367 -0.0017760543 -0.0017763284 -0.0017768379 -0.0017775986 -0.0017788803 -0.0017800156 -0.0017807303 -0.0017811538]]...]
INFO - root - 2017-12-09 11:46:34.492204: step 19710, loss = 0.81, batch loss = 0.68 (9.5 examples/sec; 0.841 sec/batch; 73h:06m:47s remains)
INFO - root - 2017-12-09 11:46:43.188641: step 19720, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.848 sec/batch; 73h:43m:01s remains)
INFO - root - 2017-12-09 11:46:51.733566: step 19730, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.895 sec/batch; 77h:43m:17s remains)
INFO - root - 2017-12-09 11:47:00.440249: step 19740, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 75h:24m:40s remains)
INFO - root - 2017-12-09 11:47:08.931295: step 19750, loss = 0.82, batch loss = 0.69 (9.7 examples/sec; 0.828 sec/batch; 71h:56m:15s remains)
INFO - root - 2017-12-09 11:47:17.410310: step 19760, loss = 0.82, batch loss = 0.69 (9.8 examples/sec; 0.813 sec/batch; 70h:35m:27s remains)
INFO - root - 2017-12-09 11:47:25.979693: step 19770, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 75h:25m:16s remains)
INFO - root - 2017-12-09 11:47:34.498336: step 19780, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.863 sec/batch; 74h:57m:32s remains)
INFO - root - 2017-12-09 11:47:43.049340: step 19790, loss = 0.81, batch loss = 0.68 (9.4 examples/sec; 0.854 sec/batch; 74h:10m:19s remains)
INFO - root - 2017-12-09 11:47:51.673018: step 19800, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.903 sec/batch; 78h:27m:37s remains)
2017-12-09 11:47:52.558445: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0017256454 -0.0017140391 -0.0017088514 -0.0017031387 -0.0016989711 -0.0016996162 -0.0017051202 -0.0017065449 -0.0017035437 -0.0016954555 -0.0016794552 -0.0016638689 -0.0016505044 -0.0016482952 -0.0016649289][-0.0017748026 -0.0017735657 -0.0017734347 -0.0017737903 -0.0017743933 -0.0017749423 -0.0017754276 -0.0017758497 -0.001776326 -0.0017766103 -0.0017765411 -0.0017764383 -0.0017762168 -0.0017757004 -0.0017748958][-0.0017738107 -0.0017728118 -0.0017729971 -0.0017735839 -0.0017742952 -0.0017749838 -0.0017755795 -0.0017762095 -0.0017768255 -0.0017771092 -0.0017769393 -0.0017765503 -0.0017761994 -0.0017754616 -0.0017744289][-0.001773396 -0.0017725155 -0.0017729138 -0.0017737804 -0.0017747683 -0.0017759134 -0.0017769323 -0.0017777707 -0.0017783726 -0.0017785926 -0.0017781806 -0.001777348 -0.0017766111 -0.0017755752 -0.0017741824][-0.001773292 -0.0017725131 -0.0017731261 -0.0017743036 -0.0017756653 -0.0017772774 -0.0017785486 -0.0017794097 -0.0017799407 -0.0017801992 -0.0017796818 -0.0017785827 -0.0017773439 -0.0017758999 -0.0017741661][-0.0017729651 -0.0017724267 -0.0017732373 -0.0017747666 -0.0017765117 -0.0017781511 -0.0017790286 -0.0017794747 -0.0017797741 -0.0017803257 -0.0017800971 -0.0017791102 -0.0017778004 -0.001776142 -0.0017743558][-0.0017725497 -0.0017720629 -0.0017730435 -0.0017749452 -0.0017769607 -0.0017780949 -0.0017777348 -0.0017770029 -0.0017772228 -0.0017785119 -0.0017793063 -0.001778805 -0.0017776891 -0.0017761813 -0.0017745969][-0.0017722076 -0.0017716695 -0.0017727267 -0.001774799 -0.0017766521 -0.0017767628 -0.0017748473 -0.0017724374 -0.0017723549 -0.0017747286 -0.0017773407 -0.001777722 -0.0017769277 -0.0017757193 -0.0017745555][-0.0017718669 -0.0017712259 -0.0017723567 -0.0017744115 -0.001775782 -0.0017747807 -0.0017713403 -0.0017673761 -0.0017668743 -0.0017704658 -0.0017747509 -0.0017763583 -0.0017760815 -0.0017752375 -0.0017745271][-0.0017715371 -0.001770844 -0.0017718013 -0.0017735646 -0.0017743412 -0.0017726825 -0.0017688725 -0.0017647039 -0.0017639466 -0.001767514 -0.0017720924 -0.0017745665 -0.0017750849 -0.0017748773 -0.0017747055][-0.0017712747 -0.0017704283 -0.0017711279 -0.0017725417 -0.001772975 -0.0017715186 -0.0017685597 -0.0017656011 -0.0017653103 -0.0017680895 -0.0017713932 -0.0017734072 -0.0017742403 -0.0017746616 -0.0017751328][-0.0017707405 -0.0017696969 -0.0017703441 -0.0017714959 -0.001772037 -0.0017714294 -0.0017700163 -0.0017686633 -0.0017687023 -0.0017704496 -0.0017720123 -0.0017729002 -0.0017735387 -0.0017744411 -0.0017755197][-0.0017705099 -0.001769181 -0.0017696347 -0.0017703782 -0.0017710179 -0.0017712165 -0.0017708844 -0.0017704697 -0.0017705596 -0.0017713596 -0.0017719746 -0.0017722388 -0.0017728379 -0.0017742119 -0.001774821][-0.0017704717 -0.001769144 -0.0017692409 -0.0017696251 -0.0017700276 -0.0017703709 -0.0017704956 -0.0017704837 -0.0017705206 -0.0017707461 -0.001771071 -0.0017713236 -0.0017723035 -0.0017736127 -0.0017683019][-0.0017705676 -0.0017692652 -0.0017690531 -0.001769178 -0.0017693038 -0.0017694483 -0.001769523 -0.0017696456 -0.0017697571 -0.0017698887 -0.0017700489 -0.0017704902 -0.0017718679 -0.0017712612 -0.0017442049]]...]
INFO - root - 2017-12-09 11:48:01.190821: step 19810, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 75h:27m:02s remains)
INFO - root - 2017-12-09 11:48:09.809515: step 19820, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.840 sec/batch; 72h:56m:13s remains)
INFO - root - 2017-12-09 11:48:18.375105: step 19830, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.848 sec/batch; 73h:37m:55s remains)
INFO - root - 2017-12-09 11:48:27.102832: step 19840, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.874 sec/batch; 75h:52m:56s remains)
INFO - root - 2017-12-09 11:48:35.818605: step 19850, loss = 0.82, batch loss = 0.69 (10.7 examples/sec; 0.748 sec/batch; 64h:57m:08s remains)
INFO - root - 2017-12-09 11:48:44.598796: step 19860, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.882 sec/batch; 76h:34m:15s remains)
INFO - root - 2017-12-09 11:48:53.349193: step 19870, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.878 sec/batch; 76h:15m:32s remains)
INFO - root - 2017-12-09 11:49:02.084525: step 19880, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 74h:51m:28s remains)
INFO - root - 2017-12-09 11:49:10.790218: step 19890, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 75h:37m:45s remains)
INFO - root - 2017-12-09 11:49:19.407741: step 19900, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.841 sec/batch; 73h:02m:35s remains)
2017-12-09 11:49:20.386072: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.34911674 0.36563256 0.38005182 0.39030111 0.39927432 0.40442711 0.40871909 0.41166392 0.41569141 0.41948202 0.42216262 0.42331395 0.42298326 0.42224449 0.42174348][0.35136339 0.37237826 0.39075664 0.40536183 0.41708887 0.42457354 0.43078271 0.43618554 0.4421232 0.44703177 0.45035422 0.4506003 0.44799554 0.44345507 0.43904993][0.34496608 0.37001055 0.39213169 0.4113242 0.42568663 0.43656388 0.44474852 0.45153239 0.4599604 0.46557212 0.46877638 0.46696994 0.46142259 0.4535062 0.44497973][0.33947128 0.36805102 0.39391854 0.41717538 0.43436179 0.44731212 0.45735466 0.46539804 0.47360048 0.47904924 0.48145285 0.47759327 0.46959972 0.45863578 0.44773307][0.33285928 0.36581731 0.39574438 0.4230924 0.44363913 0.45948815 0.47157627 0.47972229 0.48657036 0.49057445 0.49061087 0.48377749 0.47290921 0.46007031 0.44778708][0.32593781 0.36223528 0.3948929 0.425361 0.44910398 0.46775147 0.48049253 0.48831692 0.49374613 0.49539921 0.49296305 0.48359019 0.4710764 0.45735207 0.44469553][0.31711397 0.35483184 0.38934714 0.42136523 0.44575194 0.46479249 0.47747919 0.48505339 0.48896867 0.48906907 0.48507902 0.47577354 0.4635888 0.45037371 0.43860197][0.30673718 0.34444559 0.37840146 0.41017863 0.434799 0.45290604 0.46311802 0.4686898 0.47070104 0.4695769 0.4648419 0.45615879 0.44641468 0.4357672 0.42629313][0.29272625 0.32890615 0.36100218 0.39033556 0.41337878 0.42908671 0.43693089 0.43956795 0.43895251 0.43700662 0.43245718 0.42609093 0.42018712 0.4139114 0.40830526][0.27723926 0.31030023 0.33832788 0.36362043 0.38331419 0.39604253 0.40047035 0.39981115 0.39745584 0.39450115 0.39099395 0.38706529 0.38482839 0.38350645 0.38229141][0.26119581 0.28972876 0.31232449 0.33235416 0.3467097 0.3546049 0.35479465 0.35148948 0.34793049 0.34492314 0.34320536 0.34247592 0.34449148 0.34719491 0.35029104][0.24642456 0.27035308 0.28767893 0.30198893 0.31018612 0.31194767 0.30736202 0.30061838 0.29474646 0.2912316 0.29106808 0.29355198 0.29920921 0.30585384 0.31275767][0.23265101 0.25204524 0.26401469 0.27254847 0.27491108 0.27117807 0.26202786 0.25125533 0.2425736 0.23787557 0.2380055 0.24219567 0.25049427 0.2604799 0.27052805][0.21984014 0.2343841 0.240798 0.24317712 0.2393683 0.23002015 0.2167169 0.20292291 0.19215865 0.18648686 0.18689962 0.19232379 0.2020047 0.21383525 0.22588846][0.20887685 0.21866329 0.21955788 0.2154914 0.20537113 0.19035754 0.1732038 0.15685619 0.14445138 0.13833165 0.13908075 0.14534082 0.15577491 0.16834408 0.18122032]]...]
INFO - root - 2017-12-09 11:49:29.062985: step 19910, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.872 sec/batch; 75h:44m:59s remains)
INFO - root - 2017-12-09 11:49:37.761736: step 19920, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.873 sec/batch; 75h:49m:36s remains)
INFO - root - 2017-12-09 11:49:46.251154: step 19930, loss = 0.82, batch loss = 0.69 (9.9 examples/sec; 0.809 sec/batch; 70h:12m:48s remains)
INFO - root - 2017-12-09 11:49:54.929260: step 19940, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.877 sec/batch; 76h:07m:32s remains)
INFO - root - 2017-12-09 11:50:03.468581: step 19950, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.830 sec/batch; 72h:03m:55s remains)
INFO - root - 2017-12-09 11:50:11.952593: step 19960, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.838 sec/batch; 72h:47m:39s remains)
INFO - root - 2017-12-09 11:50:20.668440: step 19970, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.858 sec/batch; 74h:29m:41s remains)
INFO - root - 2017-12-09 11:50:29.224473: step 19980, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.832 sec/batch; 72h:13m:54s remains)
INFO - root - 2017-12-09 11:50:37.781439: step 19990, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.860 sec/batch; 74h:38m:51s remains)
INFO - root - 2017-12-09 11:50:46.212450: step 20000, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.831 sec/batch; 72h:10m:14s remains)
2017-12-09 11:50:47.097971: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0017811465 -0.0017805006 -0.0017803202 -0.0017802523 -0.0017801761 -0.0017801133 -0.001780085 -0.0017801337 -0.0017802225 -0.0017802764 -0.001780285 -0.001780255 -0.0017802173 -0.0017801813 -0.0017801492][-0.0017806453 -0.0017799222 -0.0017796537 -0.0017795133 -0.0017793906 -0.0017792875 -0.0017792404 -0.001779295 -0.001779419 -0.0017795231 -0.0017795857 -0.0017796004 -0.0017795936 -0.0017795899 -0.0017795891][-0.001780514 -0.0017797118 -0.0017793281 -0.0017790766 -0.0017788652 -0.0017787416 -0.0017786747 -0.0017787298 -0.0017788877 -0.0017790631 -0.0017791992 -0.0017792705 -0.0017793092 -0.0017793576 -0.0017794117][-0.0017803737 -0.0017795013 -0.0017789844 -0.0017785982 -0.0017782577 -0.0017780509 -0.0017779677 -0.0017780296 -0.0017782021 -0.0017784538 -0.0017786965 -0.0017788608 -0.0017789626 -0.0017790698 -0.0017791933][-0.0017801663 -0.0017793152 -0.0017787155 -0.0017782415 -0.00177781 -0.0017775052 -0.0017773851 -0.0017774022 -0.0017775586 -0.0017778213 -0.00177814 -0.0017783947 -0.0017785578 -0.0017787388 -0.0017789485][-0.0017799499 -0.001779148 -0.0017785818 -0.0017781019 -0.0017776263 -0.0017772428 -0.0017770694 -0.001777056 -0.0017771579 -0.0017773727 -0.0017777074 -0.0017780048 -0.0017782027 -0.0017784419 -0.001778743][-0.0017798601 -0.0017791279 -0.0017786089 -0.0017781364 -0.0017776033 -0.0017771769 -0.00177699 -0.0017769957 -0.0017770898 -0.0017772735 -0.0017775537 -0.0017778068 -0.0017779865 -0.0017782481 -0.0017786081][-0.0017799392 -0.0017792813 -0.0017789468 -0.0017786273 -0.0017781068 -0.0017775735 -0.001777327 -0.0017773273 -0.0017773985 -0.001777499 -0.0017776699 -0.0017778024 -0.0017778936 -0.0017781152 -0.0017784884][-0.0017799283 -0.0017793964 -0.001779307 -0.0017792938 -0.0017790551 -0.0017786121 -0.0017782554 -0.0017780908 -0.001778001 -0.0017778981 -0.0017778621 -0.0017778154 -0.0017777911 -0.0017779458 -0.0017783165][-0.0017798661 -0.0017794048 -0.0017794333 -0.0017796629 -0.0017798087 -0.0017797182 -0.0017794911 -0.0017792289 -0.0017788884 -0.0017784946 -0.0017781568 -0.0017778731 -0.0017777169 -0.0017778076 -0.0017781609][-0.001779838 -0.0017793679 -0.0017793793 -0.0017796713 -0.0017800082 -0.0017801999 -0.0017802082 -0.0017800526 -0.0017796741 -0.0017790856 -0.0017784438 -0.0017779247 -0.0017776677 -0.0017777093 -0.0017780366][-0.0017798018 -0.0017793076 -0.0017792579 -0.0017794825 -0.0017798322 -0.0017801088 -0.0017802345 -0.0017801913 -0.0017799152 -0.0017793371 -0.0017785912 -0.001777916 -0.0017775563 -0.0017775622 -0.0017778774][-0.0017798393 -0.0017793447 -0.0017792478 -0.00177938 -0.0017796545 -0.0017799049 -0.0017800311 -0.0017800225 -0.0017798309 -0.0017793606 -0.001778655 -0.0017779403 -0.0017775117 -0.0017774596 -0.0017777344][-0.0017800172 -0.001779475 -0.0017792771 -0.0017793144 -0.0017794837 -0.0017796559 -0.0017797337 -0.0017797 -0.0017795501 -0.0017791924 -0.0017786218 -0.0017779855 -0.0017775575 -0.0017774701 -0.0017777007][-0.0017803364 -0.0017796712 -0.0017793488 -0.0017792872 -0.0017793516 -0.0017794339 -0.0017794794 -0.0017794492 -0.0017793524 -0.0017791179 -0.0017787003 -0.001778192 -0.0017778079 -0.0017776935 -0.0017778639]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-momentum-lr0.0001-clip-zeroinit-conv1-3init-from-scratch/model.ckpt-20000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-momentum-lr0.0001-clip-zeroinit-conv1-3init-from-scratch/model.ckpt-20000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-09 11:50:56.487710: step 20010, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 75h:18m:00s remains)
INFO - root - 2017-12-09 11:51:05.113207: step 20020, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.836 sec/batch; 72h:35m:27s remains)
INFO - root - 2017-12-09 11:51:13.782356: step 20030, loss = 0.82, batch loss = 0.69 (8.3 examples/sec; 0.961 sec/batch; 83h:23m:59s remains)
INFO - root - 2017-12-09 11:51:22.550792: step 20040, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.880 sec/batch; 76h:22m:46s remains)
INFO - root - 2017-12-09 11:51:31.312735: step 20050, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.886 sec/batch; 76h:54m:35s remains)
INFO - root - 2017-12-09 11:51:39.798133: step 20060, loss = 0.81, batch loss = 0.68 (9.4 examples/sec; 0.855 sec/batch; 74h:12m:55s remains)
INFO - root - 2017-12-09 11:51:48.471046: step 20070, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.863 sec/batch; 74h:53m:40s remains)
INFO - root - 2017-12-09 11:51:57.172516: step 20080, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.853 sec/batch; 74h:01m:12s remains)
INFO - root - 2017-12-09 11:52:06.087069: step 20090, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.883 sec/batch; 76h:35m:36s remains)
INFO - root - 2017-12-09 11:52:14.642382: step 20100, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.883 sec/batch; 76h:39m:17s remains)
2017-12-09 11:52:15.570172: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.04907386 0.055700995 0.060638305 0.063148066 0.063495569 0.061666053 0.058204848 0.053182632 0.047046736 0.039993513 0.031958543 0.023442367 0.015291156 0.0083793737 0.0032904255][0.07651028 0.086644664 0.094472133 0.099309966 0.10117304 0.099705473 0.095508464 0.088375278 0.07916037 0.067693472 0.054450184 0.0404503 0.027075011 0.015641542 0.0070246332][0.10846808 0.12301882 0.13446687 0.14223759 0.14609605 0.14543878 0.14095095 0.13198367 0.11955627 0.10326599 0.084082268 0.063397363 0.043298971 0.025895346 0.012541452][0.14164872 0.16179094 0.17819557 0.19035131 0.19736482 0.19798158 0.19288631 0.18131374 0.16480249 0.14281215 0.11686318 0.088848382 0.061551604 0.037771016 0.019212995][0.17021018 0.19651902 0.21894112 0.23678973 0.24849959 0.25204408 0.24767151 0.23425859 0.21361302 0.18535249 0.15177107 0.11554898 0.080378123 0.049813334 0.025909567][0.18773176 0.21927844 0.24724872 0.27071643 0.28740877 0.29451203 0.29171443 0.2777383 0.25423482 0.22104228 0.18119743 0.13815528 0.096409738 0.060141906 0.031709477][0.19106871 0.22628495 0.25833422 0.28605247 0.30679008 0.31705034 0.31623343 0.30282354 0.27804732 0.24198548 0.19816224 0.15087199 0.10510624 0.065485656 0.034535445][0.18151703 0.21888624 0.25391281 0.2848348 0.30855525 0.32125932 0.32207319 0.30939147 0.28423265 0.24695879 0.20146884 0.15260868 0.10563561 0.065324992 0.034070462][0.1634794 0.2007335 0.23663282 0.26914519 0.294565 0.30869278 0.31048685 0.29847813 0.27373376 0.23671873 0.19160554 0.14385715 0.098574996 0.060142934 0.030643068][0.14189819 0.17761913 0.21275447 0.24489121 0.2701866 0.28436616 0.28642625 0.27496046 0.25078651 0.21488531 0.17169386 0.12707244 0.085646018 0.051104654 0.025104994][0.11805724 0.15106362 0.184165 0.21455206 0.23804918 0.25070783 0.25186202 0.2404092 0.21719056 0.18356182 0.14410664 0.1046613 0.068977542 0.039880108 0.018519953][0.094244383 0.12409116 0.15473156 0.18281658 0.20390394 0.21425587 0.21370414 0.20168541 0.17927146 0.14841068 0.11364163 0.080414325 0.051419128 0.028427152 0.012107149][0.071579769 0.097829573 0.1254517 0.15062732 0.16888335 0.17671224 0.17425834 0.16161714 0.14041112 0.11314496 0.083929427 0.057468437 0.035398804 0.018493028 0.0069638458][0.052264884 0.074344546 0.098159984 0.11981385 0.13504721 0.14059669 0.13681281 0.124351 0.10500228 0.081702754 0.058207572 0.038262371 0.022472242 0.010822421 0.0032653613][0.036256004 0.053947065 0.073250011 0.09060052 0.10243521 0.10599433 0.10165765 0.090348646 0.073929913 0.05531561 0.0376261 0.023534667 0.012943909 0.0054939762 0.00093985861]]...]
INFO - root - 2017-12-09 11:52:24.318273: step 20110, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.870 sec/batch; 75h:31m:51s remains)
INFO - root - 2017-12-09 11:52:33.056964: step 20120, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.884 sec/batch; 76h:42m:18s remains)
INFO - root - 2017-12-09 11:52:41.648771: step 20130, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.840 sec/batch; 72h:53m:16s remains)
INFO - root - 2017-12-09 11:52:50.310798: step 20140, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.848 sec/batch; 73h:32m:36s remains)
INFO - root - 2017-12-09 11:52:59.007903: step 20150, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.875 sec/batch; 75h:54m:07s remains)
INFO - root - 2017-12-09 11:53:07.678766: step 20160, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.872 sec/batch; 75h:40m:29s remains)
INFO - root - 2017-12-09 11:53:16.376515: step 20170, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.855 sec/batch; 74h:10m:36s remains)
INFO - root - 2017-12-09 11:53:24.978154: step 20180, loss = 0.82, batch loss = 0.69 (9.7 examples/sec; 0.823 sec/batch; 71h:25m:03s remains)
INFO - root - 2017-12-09 11:53:33.659985: step 20190, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.861 sec/batch; 74h:42m:45s remains)
INFO - root - 2017-12-09 11:53:42.116140: step 20200, loss = 0.81, batch loss = 0.68 (9.6 examples/sec; 0.835 sec/batch; 72h:25m:52s remains)
2017-12-09 11:53:42.980611: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.015766008 0.018062182 0.020340066 0.023989685 0.027400792 0.028861346 0.028534768 0.027099263 0.025767922 0.024582388 0.023981716 0.023235234 0.021586653 0.019981038 0.017903917][0.017417505 0.019299794 0.021500695 0.025355386 0.029322175 0.032511078 0.034206841 0.035089292 0.035983682 0.036425129 0.036624916 0.035261236 0.032707982 0.029641634 0.026628586][0.019281983 0.02113756 0.023857398 0.028784921 0.034238376 0.039770652 0.044389185 0.048486862 0.052156579 0.054493021 0.055304572 0.0536857 0.050667856 0.046745908 0.043454643][0.026253644 0.028081879 0.031182462 0.037234515 0.044268422 0.052218013 0.059608545 0.066791117 0.0730745 0.0779873 0.079883352 0.078624725 0.075943179 0.07191249 0.068807341][0.039223656 0.041562766 0.045182917 0.051530942 0.059110779 0.068615966 0.077863038 0.087207578 0.095510729 0.1018125 0.10484535 0.10453802 0.1030287 0.10017972 0.098393925][0.058557853 0.061251529 0.065256335 0.071698979 0.078949936 0.088390216 0.098036326 0.1084776 0.11778039 0.12525825 0.1294152 0.13054755 0.1304335 0.12904264 0.1285347][0.081332661 0.084625714 0.088444293 0.094340973 0.10114022 0.10928283 0.11805933 0.12818059 0.1376812 0.14579353 0.15107113 0.15402959 0.15610269 0.15680169 0.15701543][0.10316452 0.10651293 0.11009277 0.11562988 0.12126654 0.12814476 0.13586396 0.14555967 0.15495002 0.16297279 0.16904874 0.17351346 0.17692731 0.17898676 0.17876276][0.12052336 0.12426934 0.12749533 0.13242838 0.13738173 0.14341187 0.15028216 0.1592055 0.16760144 0.1752272 0.18104261 0.18601862 0.18994777 0.19264095 0.19212055][0.13060248 0.13469899 0.13786171 0.14223522 0.14669846 0.1520887 0.15830067 0.16660783 0.17395094 0.18084674 0.18644017 0.19163689 0.19595522 0.19925165 0.19886822][0.13242748 0.13708793 0.14050996 0.14492474 0.14931545 0.15452126 0.16047823 0.16813588 0.17483288 0.18156548 0.18720959 0.19274916 0.19747005 0.20098965 0.20084877][0.12889099 0.13395707 0.13730665 0.14171837 0.14644012 0.1516612 0.15759486 0.16494398 0.1710915 0.17730755 0.18274964 0.18823588 0.19309969 0.19674997 0.19711585][0.12353375 0.12856413 0.13164903 0.13583857 0.14043728 0.14568028 0.15116529 0.15778753 0.16330117 0.16881187 0.17374885 0.17889857 0.18360242 0.18711796 0.1880575][0.11771457 0.12193078 0.12396692 0.12737247 0.13151421 0.13621841 0.14088826 0.14640847 0.15078498 0.1555687 0.16000834 0.16479336 0.16922985 0.17262971 0.17418784][0.11304617 0.11631228 0.11754321 0.11998994 0.12291062 0.12638985 0.12984216 0.134027 0.13730061 0.1406931 0.14417805 0.147716 0.15091825 0.15328023 0.15476876]]...]
INFO - root - 2017-12-09 11:53:51.576744: step 20210, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.849 sec/batch; 73h:36m:37s remains)
INFO - root - 2017-12-09 11:54:00.204872: step 20220, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.833 sec/batch; 72h:16m:20s remains)
INFO - root - 2017-12-09 11:54:08.611751: step 20230, loss = 0.82, batch loss = 0.69 (10.2 examples/sec; 0.786 sec/batch; 68h:12m:32s remains)
INFO - root - 2017-12-09 11:54:17.216600: step 20240, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 75h:33m:11s remains)
INFO - root - 2017-12-09 11:54:25.815304: step 20250, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 74h:56m:46s remains)
INFO - root - 2017-12-09 11:54:34.381706: step 20260, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.872 sec/batch; 75h:38m:48s remains)
INFO - root - 2017-12-09 11:54:43.064854: step 20270, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.870 sec/batch; 75h:27m:47s remains)
INFO - root - 2017-12-09 11:54:51.828648: step 20280, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 74h:45m:48s remains)
INFO - root - 2017-12-09 11:55:00.529446: step 20290, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.865 sec/batch; 74h:58m:39s remains)
INFO - root - 2017-12-09 11:55:09.158991: step 20300, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 74h:46m:33s remains)
2017-12-09 11:55:10.023077: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.26773551 0.26962346 0.2705659 0.26956773 0.2665036 0.26046318 0.25223464 0.24191405 0.23067831 0.2191231 0.20815282 0.19843175 0.19019875 0.18285769 0.17562111][0.28092021 0.28411976 0.28648064 0.28647816 0.28410271 0.27865496 0.27007467 0.25914919 0.2464013 0.23306422 0.2203097 0.20825043 0.19793278 0.18900996 0.18083921][0.29059002 0.29583651 0.30020046 0.30107066 0.29933256 0.29410303 0.28498924 0.27360269 0.25976774 0.24506593 0.23070072 0.21654108 0.20426825 0.19341995 0.18386772][0.29689038 0.30440769 0.31086916 0.31313553 0.31249577 0.30787954 0.29878354 0.28731325 0.27294442 0.2574819 0.24205659 0.22636762 0.21223603 0.19913706 0.18770441][0.29971305 0.308457 0.31552446 0.31913328 0.32026273 0.31671861 0.30837265 0.29740927 0.28333437 0.26764712 0.25162053 0.23516171 0.21966295 0.20455699 0.19119361][0.2991969 0.30830726 0.31506729 0.31950119 0.32169181 0.32000369 0.31368166 0.30413979 0.29125851 0.27610311 0.25994134 0.24265923 0.22571976 0.20914523 0.1938192][0.29568404 0.30497059 0.3110806 0.31582236 0.3187404 0.31850576 0.31414554 0.30661467 0.29564023 0.28180802 0.26630595 0.24912716 0.23150294 0.21368404 0.19640626][0.29032937 0.29888722 0.30353341 0.3079032 0.31127635 0.31215507 0.30937496 0.30391079 0.2948561 0.28278878 0.26836863 0.25217584 0.23470765 0.21629442 0.19772795][0.28148383 0.28809392 0.29034474 0.29365507 0.29738659 0.29984805 0.29961509 0.29697263 0.29078564 0.28121129 0.26875642 0.2541292 0.2373929 0.21887581 0.1995589][0.27020422 0.27434856 0.27372321 0.27497935 0.27809983 0.28147897 0.2838397 0.28441396 0.28196719 0.2759769 0.26633596 0.25377283 0.2381707 0.22027443 0.20093998][0.25822818 0.26062027 0.25760496 0.25666305 0.25829118 0.26192749 0.26606017 0.26942313 0.27029577 0.26793593 0.261584 0.2512905 0.23719838 0.22036047 0.20153348][0.24660014 0.24737395 0.24181688 0.23844138 0.23820877 0.24110697 0.24571763 0.2511071 0.25486434 0.25582209 0.2525911 0.24534646 0.23373999 0.21842712 0.20083241][0.23542716 0.23502825 0.22766997 0.22188957 0.21891542 0.21967052 0.22327404 0.22916603 0.2349052 0.23893188 0.23911604 0.23525645 0.22675376 0.21391323 0.19826628][0.22308102 0.22177045 0.21320945 0.20603399 0.20140858 0.20039555 0.20300481 0.20868465 0.21568385 0.22175655 0.22475944 0.22406033 0.21868752 0.20864433 0.19504881][0.21137017 0.209865 0.20072557 0.19286363 0.18703087 0.184456 0.18571989 0.19060002 0.19766791 0.20456271 0.20931269 0.21082425 0.20791633 0.20020232 0.18863647]]...]
INFO - root - 2017-12-09 11:55:18.716751: step 20310, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.895 sec/batch; 77h:36m:12s remains)
INFO - root - 2017-12-09 11:55:27.337093: step 20320, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 75h:11m:35s remains)
INFO - root - 2017-12-09 11:55:35.816894: step 20330, loss = 0.82, batch loss = 0.69 (11.1 examples/sec; 0.721 sec/batch; 62h:28m:44s remains)
INFO - root - 2017-12-09 11:55:44.320783: step 20340, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.832 sec/batch; 72h:07m:05s remains)
INFO - root - 2017-12-09 11:55:52.820961: step 20350, loss = 0.81, batch loss = 0.68 (9.5 examples/sec; 0.841 sec/batch; 72h:53m:07s remains)
INFO - root - 2017-12-09 11:56:01.231668: step 20360, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.863 sec/batch; 74h:50m:30s remains)
INFO - root - 2017-12-09 11:56:09.903264: step 20370, loss = 0.83, batch loss = 0.70 (9.1 examples/sec; 0.876 sec/batch; 75h:55m:22s remains)
INFO - root - 2017-12-09 11:56:18.466772: step 20380, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.863 sec/batch; 74h:47m:16s remains)
INFO - root - 2017-12-09 11:56:27.033329: step 20390, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.861 sec/batch; 74h:36m:22s remains)
INFO - root - 2017-12-09 11:56:35.445425: step 20400, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.859 sec/batch; 74h:26m:40s remains)
2017-12-09 11:56:36.281748: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.001768756 -0.0017734654 -0.0017793543 -0.0017841319 -0.0017877638 -0.0017897164 -0.0017904 -0.0017907516 -0.0017910361 -0.0017913298 -0.0017909928 -0.001790647 -0.0017901215 -0.0017894242 -0.0017892432][-0.001769786 -0.0017739722 -0.0017795683 -0.0017844647 -0.0017883581 -0.0017909558 -0.0017921973 -0.0017928951 -0.0017936727 -0.0017940452 -0.0017938215 -0.0017932253 -0.0017923606 -0.0017914041 -0.0017905924][-0.0017735006 -0.0017772438 -0.0017822802 -0.0017870511 -0.0017912112 -0.001793863 -0.0017952861 -0.0017958633 -0.0017964602 -0.0017967462 -0.0017964621 -0.0017958372 -0.0017947458 -0.0017934216 -0.0017922914][-0.0017779012 -0.0017814541 -0.0017857648 -0.0017900118 -0.0017937035 -0.0017960039 -0.0017969159 -0.0017971931 -0.0017976072 -0.0017979204 -0.0017978596 -0.0017975174 -0.0017967571 -0.0017957326 -0.0017944898][-0.0017820933 -0.0017852962 -0.0017891768 -0.0017928026 -0.0017957415 -0.0017972485 -0.0017973684 -0.0017968118 -0.0017967134 -0.0017969996 -0.0017973813 -0.0017977398 -0.0017978607 -0.0017974182 -0.0017966127][-0.0017861627 -0.001788741 -0.0017921231 -0.0017950464 -0.0017971706 -0.0017977464 -0.0017970135 -0.0017956825 -0.0017947904 -0.0017949925 -0.0017959322 -0.001797131 -0.0017983047 -0.0017988198 -0.0017986207][-0.0017900905 -0.0017923104 -0.0017949595 -0.0017970086 -0.0017980635 -0.0017974642 -0.0017956363 -0.0017933639 -0.0017918232 -0.0017918797 -0.0017933233 -0.0017954968 -0.0017978932 -0.0017994704 -0.0018000208][-0.0017935901 -0.0017956907 -0.0017978322 -0.0017990001 -0.0017989343 -0.0017971923 -0.0017943782 -0.0017911691 -0.0017890929 -0.0017892323 -0.0017911203 -0.0017941518 -0.0017974309 -0.0017999243 -0.0018012247][-0.0017970656 -0.0017986707 -0.0018001343 -0.0018003994 -0.0017994833 -0.00179685 -0.0017934473 -0.0017899632 -0.0017879551 -0.0017882729 -0.001790503 -0.001793919 -0.0017976653 -0.0018007072 -0.0018025272][-0.0017997804 -0.0018008996 -0.0018018339 -0.0018015546 -0.0018003595 -0.0017975617 -0.0017942898 -0.0017909204 -0.0017890471 -0.001789541 -0.0017916885 -0.0017949496 -0.0017984271 -0.0018013775 -0.0018032508][-0.0018020769 -0.0018027164 -0.0018033262 -0.0018028818 -0.0018017552 -0.0017993662 -0.0017965693 -0.0017936724 -0.0017921889 -0.0017926823 -0.0017943931 -0.0017971614 -0.0018000731 -0.0018025371 -0.0018040068][-0.001803461 -0.0018038277 -0.0018043085 -0.0018038261 -0.0018028874 -0.0018010563 -0.0017991071 -0.0017968423 -0.0017956427 -0.0017961214 -0.001797441 -0.0017993904 -0.0018013369 -0.001803071 -0.0018039151][-0.0018031837 -0.0018033158 -0.0018037139 -0.0018034704 -0.001803029 -0.0018018233 -0.0018006652 -0.0017993612 -0.001798769 -0.0017991632 -0.0018000014 -0.001801236 -0.0018021032 -0.0018027463 -0.0018028246][-0.0018019537 -0.0018018595 -0.0018021849 -0.001802194 -0.0018021656 -0.0018017552 -0.0018013434 -0.0018008095 -0.0018008662 -0.0018012815 -0.0018017424 -0.0018021223 -0.00180215 -0.0018018719 -0.0018010137][-0.0018003575 -0.0018001016 -0.0018003193 -0.0018006287 -0.0018009193 -0.0018008505 -0.0018008049 -0.0018005623 -0.0018008034 -0.0018010229 -0.0018011552 -0.0018011881 -0.0018008137 -0.0018001368 -0.00179886]]...]
INFO - root - 2017-12-09 11:56:44.836864: step 20410, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.892 sec/batch; 77h:18m:29s remains)
INFO - root - 2017-12-09 11:56:53.553871: step 20420, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.880 sec/batch; 76h:14m:54s remains)
INFO - root - 2017-12-09 11:57:02.172387: step 20430, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 74h:43m:56s remains)
INFO - root - 2017-12-09 11:57:10.603779: step 20440, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.882 sec/batch; 76h:28m:50s remains)
INFO - root - 2017-12-09 11:57:19.230060: step 20450, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.853 sec/batch; 73h:58m:02s remains)
INFO - root - 2017-12-09 11:57:27.750990: step 20460, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 75h:10m:38s remains)
INFO - root - 2017-12-09 11:57:36.407682: step 20470, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.859 sec/batch; 74h:29m:02s remains)
INFO - root - 2017-12-09 11:57:45.045891: step 20480, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.873 sec/batch; 75h:42m:22s remains)
INFO - root - 2017-12-09 11:57:53.822482: step 20490, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 75h:06m:21s remains)
INFO - root - 2017-12-09 11:58:02.322772: step 20500, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.876 sec/batch; 75h:55m:45s remains)
2017-12-09 11:58:03.181195: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0018033037 -0.0018019424 -0.0018004073 -0.00179874 -0.0017977731 -0.0017974175 -0.0017973974 -0.001797518 -0.0017981435 -0.0017992598 -0.0017994543 -0.001799291 -0.0017992738 -0.0017996373 -0.0018002694][-0.001800605 -0.0017989567 -0.0017974761 -0.0017961444 -0.001795563 -0.0017957827 -0.0017962951 -0.0017969005 -0.0017978123 -0.0017989231 -0.0017992654 -0.0017987938 -0.0017982855 -0.0017983137 -0.0017988125][-0.0017983337 -0.0017967207 -0.0017956619 -0.0017950034 -0.0017950088 -0.0017958662 -0.0017967671 -0.0017978398 -0.0017988513 -0.0017997731 -0.0018000386 -0.001799587 -0.0017988713 -0.0017985834 -0.0017988218][-0.0017964073 -0.0017949191 -0.0017944081 -0.0017945273 -0.0017950898 -0.001796275 -0.0017974786 -0.0017988349 -0.0017999134 -0.0018007933 -0.0018010575 -0.0018006794 -0.00180006 -0.0017996633 -0.0017998376][-0.0017950454 -0.0017938433 -0.0017937908 -0.0017946383 -0.0017958946 -0.0017973577 -0.0017986223 -0.0017999467 -0.0018010127 -0.0018018193 -0.0018021561 -0.00180193 -0.0018015425 -0.0018012276 -0.0018014219][-0.0017951175 -0.0017940714 -0.0017942541 -0.0017955356 -0.0017971955 -0.0017987508 -0.0018000376 -0.0018012462 -0.001802283 -0.0018028739 -0.0018031701 -0.0018030703 -0.0018029286 -0.0018027683 -0.0018030432][-0.0017955711 -0.0017946324 -0.0017949517 -0.0017961871 -0.0017977336 -0.0017990225 -0.0018000469 -0.0018012563 -0.0018024128 -0.0018031596 -0.0018037959 -0.0018040702 -0.0018040999 -0.0018040326 -0.0018043895][-0.0017957545 -0.0017948997 -0.0017952356 -0.0017963921 -0.0017976096 -0.0017984858 -0.0017992152 -0.0018005259 -0.0018018264 -0.0018025676 -0.0018032728 -0.0018038761 -0.0018042348 -0.0018044184 -0.001805043][-0.0017959564 -0.0017948932 -0.0017950571 -0.0017961286 -0.0017969512 -0.0017973672 -0.0017978627 -0.0017990773 -0.0018004784 -0.0018011398 -0.0018015206 -0.0018019489 -0.0018025157 -0.0018031799 -0.001804178][-0.0017954666 -0.0017945979 -0.001794656 -0.0017954712 -0.001796 -0.0017961356 -0.0017963527 -0.0017970626 -0.001798188 -0.0017987451 -0.0017988306 -0.0017989975 -0.0017995074 -0.0018006802 -0.0018021147][-0.0017945109 -0.0017937353 -0.0017937426 -0.0017944791 -0.001794924 -0.0017947889 -0.0017946832 -0.0017949165 -0.0017955077 -0.0017957449 -0.0017955899 -0.0017955248 -0.0017957947 -0.0017970931 -0.0017989752][-0.0017926438 -0.001791983 -0.0017921004 -0.0017928953 -0.0017934692 -0.0017934206 -0.0017931607 -0.0017929126 -0.0017929509 -0.0017927956 -0.0017924556 -0.0017922603 -0.0017926026 -0.0017938794 -0.0017958938][-0.0017907907 -0.0017901243 -0.0017903133 -0.0017910034 -0.0017915857 -0.0017917908 -0.0017917141 -0.0017913736 -0.0017909867 -0.0017904473 -0.0017898445 -0.0017894813 -0.0017897144 -0.0017908156 -0.0017926842][-0.001789586 -0.0017887386 -0.0017887398 -0.0017890652 -0.001789398 -0.0017896739 -0.0017898434 -0.0017897886 -0.0017894424 -0.0017888903 -0.0017883646 -0.0017879383 -0.0017877945 -0.0017882865 -0.0017896269][-0.001788527 -0.0017875249 -0.0017873381 -0.0017873247 -0.0017873606 -0.0017876324 -0.0017879549 -0.0017881064 -0.0017879446 -0.0017876705 -0.0017875704 -0.0017873873 -0.0017872142 -0.0017873154 -0.0017879605]]...]
INFO - root - 2017-12-09 11:58:11.869598: step 20510, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.853 sec/batch; 73h:53m:27s remains)
INFO - root - 2017-12-09 11:58:20.521543: step 20520, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.852 sec/batch; 73h:51m:03s remains)
INFO - root - 2017-12-09 11:58:29.201004: step 20530, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.886 sec/batch; 76h:45m:52s remains)
INFO - root - 2017-12-09 11:58:37.545915: step 20540, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.860 sec/batch; 74h:33m:35s remains)
INFO - root - 2017-12-09 11:58:46.103618: step 20550, loss = 0.82, batch loss = 0.69 (9.7 examples/sec; 0.826 sec/batch; 71h:32m:55s remains)
INFO - root - 2017-12-09 11:58:54.460631: step 20560, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 75h:08m:11s remains)
INFO - root - 2017-12-09 11:59:03.184199: step 20570, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.889 sec/batch; 77h:01m:41s remains)
INFO - root - 2017-12-09 11:59:11.836432: step 20580, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.851 sec/batch; 73h:45m:40s remains)
INFO - root - 2017-12-09 11:59:20.238219: step 20590, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.847 sec/batch; 73h:24m:28s remains)
INFO - root - 2017-12-09 11:59:28.620177: step 20600, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 74h:53m:05s remains)
2017-12-09 11:59:29.515355: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.00176497 -0.0017633862 -0.0017631962 -0.001763714 -0.0017649608 -0.0017666167 -0.0017685904 -0.0017706236 -0.001772415 -0.0017735796 -0.0017742263 -0.0017745716 -0.0017745119 -0.0017744248 -0.0017744763][-0.0017664013 -0.0017654715 -0.0017658012 -0.0017668339 -0.0017685009 -0.0017705063 -0.0017727077 -0.001774666 -0.0017758613 -0.0017760502 -0.0017756334 -0.0017749392 -0.001774166 -0.0017738338 -0.0017742433][-0.0017690202 -0.0017692434 -0.0017703868 -0.0017720702 -0.0017739279 -0.0017757241 -0.0017771454 -0.0017781304 -0.0017782249 -0.0017772141 -0.0017756077 -0.0017740114 -0.0017728256 -0.001772479 -0.0017733691][-0.001772171 -0.0017736477 -0.0017757011 -0.0017778402 -0.0017796732 -0.0017808622 -0.0017813969 -0.0017813154 -0.0017803033 -0.0017780934 -0.0017754043 -0.0017730166 -0.0017712445 -0.0017706143 -0.0017717185][-0.0017752636 -0.0017777602 -0.001780387 -0.0017826696 -0.0017841761 -0.0017846243 -0.0017842255 -0.0017831572 -0.0017811545 -0.001778014 -0.0017743009 -0.0017708081 -0.0017681357 -0.001766879 -0.0017676151][-0.00177778 -0.0017809629 -0.0017840645 -0.0017864886 -0.0017876679 -0.0017874575 -0.0017861736 -0.0017845235 -0.0017820233 -0.0017784224 -0.0017742817 -0.0017699827 -0.0017662266 -0.0017636297 -0.0017632046][-0.001779014 -0.0017826603 -0.0017861208 -0.0017885615 -0.0017893014 -0.0017885822 -0.0017867801 -0.001784671 -0.0017820089 -0.0017788869 -0.001775271 -0.0017709529 -0.0017665987 -0.0017630864 -0.0017614983][-0.0017789797 -0.0017827057 -0.0017861482 -0.0017882993 -0.0017888166 -0.0017878775 -0.001785922 -0.0017839903 -0.0017822683 -0.0017805622 -0.0017784038 -0.0017750342 -0.0017708519 -0.001766803 -0.001764028][-0.0017772258 -0.0017804027 -0.0017835 -0.0017852224 -0.0017855859 -0.001784954 -0.0017839598 -0.0017833675 -0.0017832813 -0.0017835266 -0.0017832431 -0.0017810502 -0.001777158 -0.0017728282 -0.0017693614][-0.0017741602 -0.0017763628 -0.0017788555 -0.0017802893 -0.0017806418 -0.0017807381 -0.0017811625 -0.0017823419 -0.0017841461 -0.0017863115 -0.0017876923 -0.0017864686 -0.0017828796 -0.0017786177 -0.0017747792][-0.0017708262 -0.0017722608 -0.001774197 -0.0017753766 -0.0017759101 -0.0017767964 -0.0017783056 -0.0017807023 -0.0017840805 -0.0017879232 -0.0017908404 -0.0017907611 -0.0017879133 -0.0017841986 -0.0017801371][-0.0017674298 -0.0017681274 -0.0017693288 -0.001769949 -0.0017704322 -0.001771983 -0.0017745757 -0.0017781333 -0.0017827229 -0.0017879069 -0.0017921884 -0.0017932561 -0.001791438 -0.0017885024 -0.001784745][-0.0017646635 -0.0017645407 -0.0017651181 -0.0017652626 -0.001765652 -0.0017674892 -0.0017707263 -0.001775165 -0.0017805853 -0.0017864063 -0.0017913464 -0.0017932482 -0.0017922885 -0.0017899865 -0.001786469][-0.0017623842 -0.0017616941 -0.0017617823 -0.0017617756 -0.001762197 -0.0017641793 -0.0017676289 -0.0017723163 -0.0017779723 -0.0017837236 -0.0017884588 -0.0017904532 -0.0017899442 -0.0017880851 -0.0017847392][-0.0017610457 -0.0017600094 -0.0017598333 -0.0017598533 -0.0017603453 -0.0017621774 -0.0017652469 -0.0017694217 -0.0017745458 -0.0017795746 -0.0017836805 -0.0017856191 -0.0017854068 -0.0017838927 -0.0017809405]]...]
INFO - root - 2017-12-09 11:59:37.952942: step 20610, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.874 sec/batch; 75h:41m:55s remains)
INFO - root - 2017-12-09 11:59:46.439922: step 20620, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.861 sec/batch; 74h:35m:51s remains)
INFO - root - 2017-12-09 11:59:54.946183: step 20630, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 75h:05m:39s remains)
INFO - root - 2017-12-09 12:00:03.530196: step 20640, loss = 0.81, batch loss = 0.68 (9.1 examples/sec; 0.876 sec/batch; 75h:53m:42s remains)
INFO - root - 2017-12-09 12:00:12.208955: step 20650, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.877 sec/batch; 75h:56m:35s remains)
INFO - root - 2017-12-09 12:00:20.672623: step 20660, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.873 sec/batch; 75h:35m:23s remains)
INFO - root - 2017-12-09 12:00:29.351112: step 20670, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.863 sec/batch; 74h:45m:31s remains)
INFO - root - 2017-12-09 12:00:38.012697: step 20680, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.874 sec/batch; 75h:43m:20s remains)
INFO - root - 2017-12-09 12:00:46.720349: step 20690, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.843 sec/batch; 73h:00m:24s remains)
INFO - root - 2017-12-09 12:00:55.263375: step 20700, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 75h:25m:35s remains)
2017-12-09 12:00:56.115326: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.44787231 0.43633297 0.42811275 0.42309546 0.42012936 0.41905192 0.41981494 0.41804853 0.41296059 0.40576583 0.39780751 0.38997024 0.38275224 0.3790127 0.37818408][0.44706294 0.437115 0.42943314 0.42565027 0.42412293 0.42473 0.42671394 0.42655453 0.42206946 0.4136799 0.40343374 0.39269367 0.3827484 0.37634841 0.37365606][0.44394851 0.4355213 0.42924795 0.42666456 0.42640451 0.4281427 0.43076167 0.43216938 0.42835528 0.41957271 0.40723351 0.39348945 0.38040245 0.37080082 0.36517912][0.44232577 0.43807271 0.43509859 0.43469575 0.43652937 0.43941298 0.44212008 0.44352621 0.43946016 0.43006262 0.41635346 0.40018 0.38450494 0.371378 0.36258823][0.4415451 0.44114694 0.44119331 0.44359678 0.44809815 0.45292634 0.45679802 0.45794019 0.45347139 0.4427003 0.42725542 0.40996724 0.39226377 0.37713256 0.36596379][0.43899083 0.44254577 0.44524574 0.45057434 0.45808941 0.46479386 0.46967003 0.47128958 0.46712795 0.45593929 0.43975288 0.422008 0.40370771 0.38754544 0.37511596][0.432991 0.44043642 0.44526693 0.45323548 0.46323666 0.47170055 0.47759494 0.47925633 0.475983 0.46617556 0.45155478 0.4353219 0.41786611 0.4023754 0.38953152][0.42480496 0.4345389 0.44000876 0.44883859 0.45977551 0.46904311 0.47529665 0.47740394 0.47475883 0.46635768 0.45379841 0.43984571 0.4241412 0.41010991 0.3983829][0.41019139 0.42293879 0.43006527 0.44036821 0.45294032 0.46330008 0.47054651 0.47186166 0.46846735 0.46021017 0.44851702 0.43642282 0.42259023 0.41059557 0.4008601][0.3979969 0.41003606 0.41578791 0.4263505 0.43972299 0.45126468 0.45942056 0.46070248 0.4567596 0.44769013 0.43569052 0.42459327 0.41255403 0.40317905 0.39611614][0.38740855 0.39794013 0.40133724 0.41010529 0.42225406 0.43323705 0.440691 0.44156218 0.43724874 0.42878196 0.417999 0.40785372 0.39790693 0.39094341 0.38629872][0.38290471 0.39156532 0.39240503 0.39835396 0.40795389 0.41697147 0.4227533 0.42200544 0.41626859 0.40778735 0.3975724 0.38844198 0.38000533 0.37482393 0.37206891][0.38492 0.39181885 0.39062345 0.3940419 0.40087435 0.40703246 0.40997776 0.40664247 0.39866507 0.38847354 0.37730572 0.36784378 0.36008155 0.35591036 0.35431519][0.39064547 0.39791554 0.39611953 0.39847934 0.40343404 0.40718672 0.40744546 0.4009195 0.38996565 0.37730843 0.36440188 0.35386476 0.34600952 0.3420448 0.34084123][0.40052328 0.40842581 0.40641743 0.40774822 0.41062531 0.41217571 0.40936711 0.4002513 0.38690618 0.37195724 0.35785422 0.34608907 0.33768153 0.33337304 0.33201918]]...]
INFO - root - 2017-12-09 12:01:04.826890: step 20710, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.883 sec/batch; 76h:29m:56s remains)
INFO - root - 2017-12-09 12:01:13.510280: step 20720, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.883 sec/batch; 76h:28m:57s remains)
INFO - root - 2017-12-09 12:01:22.101506: step 20730, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.856 sec/batch; 74h:07m:58s remains)
INFO - root - 2017-12-09 12:01:30.563696: step 20740, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.847 sec/batch; 73h:21m:21s remains)
INFO - root - 2017-12-09 12:01:39.057276: step 20750, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.858 sec/batch; 74h:19m:08s remains)
INFO - root - 2017-12-09 12:01:47.516978: step 20760, loss = 0.81, batch loss = 0.68 (11.0 examples/sec; 0.726 sec/batch; 62h:52m:56s remains)
INFO - root - 2017-12-09 12:01:56.164147: step 20770, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 74h:40m:36s remains)
INFO - root - 2017-12-09 12:02:04.804774: step 20780, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.879 sec/batch; 76h:04m:31s remains)
INFO - root - 2017-12-09 12:02:13.570044: step 20790, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 75h:04m:29s remains)
INFO - root - 2017-12-09 12:02:22.047871: step 20800, loss = 0.83, batch loss = 0.70 (9.2 examples/sec; 0.871 sec/batch; 75h:22m:25s remains)
2017-12-09 12:02:22.854982: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.001683281 -0.0014289484 -0.00087528856 0.00010459847 0.0015505195 0.0033415169 0.0051218313 0.0064345207 0.0069246115 0.0064400751 0.0051169666 0.0033067195 0.0015257268 0.00010221254 -0.00085206935][-0.0016526047 -0.0013058723 -0.00053154142 0.00085692026 0.0029100603 0.0054510417 0.0079669878 0.0098347785 0.010566279 0.0099856406 0.0082828868 0.0058951867 0.0034610936 0.0014248589 -4.0788553e-05][-0.0016334881 -0.0012108089 -0.00024897105 0.0014861539 0.0040509095 0.007207592 0.010309541 0.012611708 0.013534458 0.01292112 0.011012211 0.0082863066 0.0054152389 0.002904423 0.00098027673][-0.0016300097 -0.0011688529 -0.00010404398 0.0018200428 0.0046618092 0.0081535289 0.011576258 0.014126673 0.015188798 0.014619665 0.012688665 0.0099003706 0.0069055827 0.0042055128 0.0020321757][-0.0016402017 -0.0011941984 -0.00015466625 0.0017327949 0.0045270431 0.00796683 0.011353762 0.013913972 0.015045838 0.014616707 0.012883183 0.010319029 0.0075164344 0.0049356227 0.0027825083][-0.0016538022 -0.0012665652 -0.00036766706 0.0012670251 0.0037005511 0.0067204442 0.009731018 0.01206009 0.013166238 0.012903897 0.011491499 0.009350189 0.007000383 0.0048212954 0.0029678578][-0.0016738066 -0.00137351 -0.00068688113 0.00056198251 0.0024355319 0.0047929804 0.007191428 0.00910435 0.010082028 0.0099692419 0.0089144167 0.0072672246 0.0054721343 0.0038289889 0.0024387073][-0.0017030489 -0.0014971542 -0.0010380847 -0.00020647701 0.0010517373 0.0026604873 0.004334332 0.005713739 0.0064695152 0.0064572971 0.0057583614 0.0046226983 0.0033905907 0.0022909339 0.0013896859][-0.001736044 -0.0016165684 -0.0013564953 -0.00088615419 -0.00016610126 0.000772382 0.0017730057 0.0026227897 0.0031167441 0.0031455215 0.0027391491 0.002054892 0.0013169263 0.00068223255 0.00018834963][-0.0017674017 -0.0017134368 -0.0015983484 -0.0013880644 -0.001058656 -0.00061699993 -0.00012963905 0.00029863964 0.00056171382 0.0005927434 0.00039592723 5.0377566e-05 -0.00031900452 -0.00062061264 -0.00083636085][-0.0017885942 -0.0017721109 -0.0017367003 -0.0016697263 -0.0015601579 -0.0014062749 -0.0012279088 -0.0010636417 -0.00095600239 -0.00093682215 -0.0010094543 -0.0011430291 -0.001285108 -0.0013937497 -0.0014614908][-0.0017937863 -0.00179109 -0.0017856265 -0.0017746626 -0.0017547745 -0.0017237827 -0.0016839709 -0.0016436037 -0.0016139777 -0.0016056484 -0.0016225151 -0.0016571217 -0.0016943846 -0.0017214009 -0.0017361146][-0.001793461 -0.0017927532 -0.0017921379 -0.0017916581 -0.0017912732 -0.0017903477 -0.0017883087 -0.0017851235 -0.0017817 -0.0017788315 -0.0017776937 -0.0017787189 -0.0017810464 -0.0017828235 -0.0017837586][-0.0017936792 -0.0017931251 -0.0017927756 -0.0017926164 -0.001792851 -0.0017934004 -0.0017937651 -0.0017938501 -0.0017939575 -0.0017939226 -0.0017937893 -0.0017935046 -0.0017932476 -0.0017932452 -0.0017933082][-0.0017941765 -0.0017936986 -0.0017934701 -0.0017934006 -0.0017934866 -0.0017937783 -0.0017941131 -0.0017939974 -0.001793924 -0.0017941387 -0.0017943558 -0.0017942211 -0.0017940106 -0.0017940674 -0.0017942548]]...]
INFO - root - 2017-12-09 12:02:31.568802: step 20810, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.865 sec/batch; 74h:53m:01s remains)
INFO - root - 2017-12-09 12:02:40.373596: step 20820, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.877 sec/batch; 75h:57m:59s remains)
INFO - root - 2017-12-09 12:02:48.965488: step 20830, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.855 sec/batch; 74h:03m:24s remains)
INFO - root - 2017-12-09 12:02:57.424366: step 20840, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.874 sec/batch; 75h:37m:52s remains)
INFO - root - 2017-12-09 12:03:06.122829: step 20850, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.893 sec/batch; 77h:20m:35s remains)
INFO - root - 2017-12-09 12:03:14.774344: step 20860, loss = 0.82, batch loss = 0.69 (9.7 examples/sec; 0.823 sec/batch; 71h:17m:02s remains)
INFO - root - 2017-12-09 12:03:23.473344: step 20870, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.885 sec/batch; 76h:37m:22s remains)
INFO - root - 2017-12-09 12:03:32.245826: step 20880, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.844 sec/batch; 73h:05m:13s remains)
INFO - root - 2017-12-09 12:03:40.978043: step 20890, loss = 0.81, batch loss = 0.68 (9.0 examples/sec; 0.886 sec/batch; 76h:40m:17s remains)
INFO - root - 2017-12-09 12:03:49.513858: step 20900, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.870 sec/batch; 75h:18m:21s remains)
2017-12-09 12:03:50.380912: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0018034007 -0.0018027329 -0.0018020823 -0.0018013167 -0.0018004936 -0.0017996669 -0.0017989129 -0.0017983685 -0.0017982406 -0.0017983754 -0.0017985823 -0.0017987787 -0.0017990344 -0.0017993146 -0.0017995323][-0.0018034419 -0.0018026894 -0.001801935 -0.0018010673 -0.0018001681 -0.0017993144 -0.0017985819 -0.0017981109 -0.0017980756 -0.0017982695 -0.0017984933 -0.0017986906 -0.0017989351 -0.0017991876 -0.0017993549][-0.0018037457 -0.0018029285 -0.0018020602 -0.001801068 -0.0018000795 -0.0017992085 -0.0017985175 -0.0017981436 -0.0017982201 -0.0017984931 -0.0017987421 -0.0017989351 -0.0017991522 -0.0017993496 -0.0017994404][-0.0018041363 -0.0018032601 -0.0018022627 -0.0018011195 -0.0018000148 -0.0017991103 -0.0017984612 -0.0017981924 -0.0017983848 -0.0017987436 -0.0017990294 -0.001799223 -0.0017994045 -0.0017995241 -0.001799527][-0.0018045916 -0.0018036561 -0.0018025247 -0.001801215 -0.0017999801 -0.0017990263 -0.0017984099 -0.0017982385 -0.0017985335 -0.0017989753 -0.0017993005 -0.0017994981 -0.001799632 -0.0017996607 -0.0017995803][-0.0018049941 -0.0018039936 -0.0018027411 -0.0018012776 -0.0017999334 -0.0017989472 -0.0017983753 -0.0017982898 -0.0017986617 -0.0017991657 -0.0017995156 -0.0017996974 -0.0017997631 -0.001799696 -0.0017995418][-0.0018052857 -0.0018042166 -0.0018028683 -0.001801305 -0.0017999077 -0.0017989286 -0.0017984167 -0.0017983984 -0.0017988103 -0.0017993316 -0.0017996752 -0.0017998159 -0.0017997874 -0.0017996217 -0.0017994001][-0.0018054398 -0.0018043164 -0.0018029262 -0.0018013455 -0.0017999806 -0.0017990528 -0.0017985913 -0.0017985926 -0.0017989741 -0.0017994413 -0.0017997267 -0.0017997939 -0.0017996692 -0.0017994299 -0.001799167][-0.0018054498 -0.0018042934 -0.0018029267 -0.0018014312 -0.0018001912 -0.0017993596 -0.0017989392 -0.0017989194 -0.0017992002 -0.0017995214 -0.0017996622 -0.0017996156 -0.0017994053 -0.0017991284 -0.0017988752][-0.0018053603 -0.0018042218 -0.0018029393 -0.0018016032 -0.0018005338 -0.0017998123 -0.0017994171 -0.0017993518 -0.0017995074 -0.0017996299 -0.0017995649 -0.0017993671 -0.0017990747 -0.0017987884 -0.0017985816][-0.0018052161 -0.0018041419 -0.0018029991 -0.0018018639 -0.0018009744 -0.0018003458 -0.0017999483 -0.0017998144 -0.0017998363 -0.0017997606 -0.0017994915 -0.0017991505 -0.0017988009 -0.0017985298 -0.0017983791][-0.0018050905 -0.0018041192 -0.0018031524 -0.0018022185 -0.0018014761 -0.0018008929 -0.0018004437 -0.0018002004 -0.0018000751 -0.0017998394 -0.0017994259 -0.0017989996 -0.0017986391 -0.0017984024 -0.0017982958][-0.0018050182 -0.0018041792 -0.0018033991 -0.0018026504 -0.0018019978 -0.0018014018 -0.0018008484 -0.0018004372 -0.0018001293 -0.0017997624 -0.0017992831 -0.0017988468 -0.0017985312 -0.0017983471 -0.0017982794][-0.0018050075 -0.0018043186 -0.0018037077 -0.0018031017 -0.0018024746 -0.0018018069 -0.0018011031 -0.0018004818 -0.0017999744 -0.0017995168 -0.001799065 -0.001798695 -0.0017984502 -0.0017983183 -0.0017982821][-0.001804969 -0.001804452 -0.0018040143 -0.0018035145 -0.0018028733 -0.0018021005 -0.001801216 -0.0018003602 -0.0017996531 -0.0017991541 -0.0017988046 -0.0017985485 -0.0017983889 -0.0017983105 -0.0017982959]]...]
INFO - root - 2017-12-09 12:03:59.040785: step 20910, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.861 sec/batch; 74h:29m:49s remains)
INFO - root - 2017-12-09 12:04:07.713798: step 20920, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.850 sec/batch; 73h:35m:48s remains)
INFO - root - 2017-12-09 12:04:16.360549: step 20930, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.840 sec/batch; 72h:42m:18s remains)
INFO - root - 2017-12-09 12:04:24.973982: step 20940, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 75h:06m:03s remains)
INFO - root - 2017-12-09 12:04:33.711811: step 20950, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 75h:11m:11s remains)
INFO - root - 2017-12-09 12:04:42.448879: step 20960, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.883 sec/batch; 76h:22m:29s remains)
INFO - root - 2017-12-09 12:04:51.004151: step 20970, loss = 0.83, batch loss = 0.70 (9.7 examples/sec; 0.827 sec/batch; 71h:31m:19s remains)
INFO - root - 2017-12-09 12:04:59.662765: step 20980, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.858 sec/batch; 74h:13m:24s remains)
INFO - root - 2017-12-09 12:05:08.315667: step 20990, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.888 sec/batch; 76h:52m:18s remains)
INFO - root - 2017-12-09 12:05:16.823352: step 21000, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.879 sec/batch; 76h:03m:17s remains)
2017-12-09 12:05:17.774657: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.016312625 0.014437624 0.012335356 0.010612324 0.0099205542 0.010403907 0.011664876 0.013168631 0.014658249 0.015957927 0.016711203 0.016553123 0.01507634 0.012317039 0.0086433347][0.022888279 0.021009447 0.018414844 0.015968943 0.014750454 0.015136638 0.01673221 0.0188234 0.020977139 0.022874398 0.024028189 0.02396128 0.022190321 0.018748332 0.013932112][0.030131856 0.028494567 0.025636587 0.022575609 0.020753589 0.020872818 0.022620467 0.025091834 0.027683707 0.029973911 0.031399142 0.031413771 0.029445564 0.025476146 0.01969341][0.036990684 0.035656 0.032665789 0.029152695 0.026791088 0.026503665 0.028138466 0.030746548 0.033580102 0.036122154 0.037751324 0.037849758 0.035755552 0.031419031 0.024934009][0.04253035 0.041525714 0.038514584 0.034692012 0.031839941 0.031063212 0.032383751 0.034874819 0.0377149 0.040399369 0.042287294 0.042624708 0.040607288 0.036105879 0.029185038][0.046077181 0.04538466 0.042509407 0.0386129 0.035456959 0.034238551 0.035118561 0.03730192 0.040025134 0.042798366 0.044978786 0.045664188 0.043890506 0.039428998 0.03234826][0.047420733 0.047063351 0.044491351 0.040790364 0.037596643 0.036069408 0.036505144 0.038301453 0.040818524 0.043634318 0.046100352 0.047153451 0.045696191 0.041441638 0.034450736][0.046615496 0.046567079 0.044484485 0.041260064 0.038301989 0.036658797 0.036702089 0.038076602 0.04031276 0.043082055 0.045701794 0.047019586 0.045861315 0.041914202 0.035240609][0.043978952 0.044151135 0.042566009 0.03996576 0.03746888 0.035880327 0.035610177 0.036566954 0.038450915 0.041014642 0.043535043 0.044875097 0.04390458 0.04031153 0.034164988][0.039723862 0.040014621 0.038937613 0.037001763 0.035018623 0.033606134 0.03316509 0.033743836 0.035167094 0.037266485 0.039368555 0.040457942 0.03954123 0.036334407 0.030896759][0.034001436 0.034289982 0.0335805 0.032199226 0.030684249 0.029480016 0.028963478 0.029243024 0.030216103 0.031735569 0.033242043 0.033924129 0.032992989 0.030200964 0.025600037][0.026913963 0.027122611 0.026648879 0.025676018 0.024533525 0.02354694 0.023038631 0.023113968 0.023709815 0.0246958 0.025650399 0.025970709 0.025074733 0.022762993 0.019089041][0.018958449 0.019049406 0.018713653 0.018012829 0.017158363 0.016393645 0.015973747 0.015959704 0.016306337 0.016923724 0.017525215 0.017684897 0.016977256 0.015241169 0.012532398][0.011245954 0.011230207 0.010967319 0.010465694 0.0098601477 0.0093222084 0.0090411967 0.0090355007 0.009274845 0.009698255 0.010121582 0.010266089 0.0098309573 0.0087002516 0.0069187991][0.005021655 0.0049569681 0.0047578709 0.004425901 0.0040466003 0.0037254479 0.0035781462 0.0035986735 0.0037823638 0.0040948763 0.0044316454 0.0046134312 0.0044295667 0.0038112032 0.002797744]]...]
INFO - root - 2017-12-09 12:05:26.446181: step 21010, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.889 sec/batch; 76h:54m:39s remains)
INFO - root - 2017-12-09 12:05:35.195997: step 21020, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 74h:43m:50s remains)
INFO - root - 2017-12-09 12:05:43.873936: step 21030, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.858 sec/batch; 74h:11m:30s remains)
INFO - root - 2017-12-09 12:05:52.456006: step 21040, loss = 0.81, batch loss = 0.68 (9.1 examples/sec; 0.877 sec/batch; 75h:54m:56s remains)
INFO - root - 2017-12-09 12:06:01.097127: step 21050, loss = 0.81, batch loss = 0.68 (9.5 examples/sec; 0.839 sec/batch; 72h:36m:44s remains)
INFO - root - 2017-12-09 12:06:09.673410: step 21060, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.860 sec/batch; 74h:25m:43s remains)
INFO - root - 2017-12-09 12:06:18.123075: step 21070, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.837 sec/batch; 72h:24m:06s remains)
INFO - root - 2017-12-09 12:06:26.851116: step 21080, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.885 sec/batch; 76h:33m:23s remains)
INFO - root - 2017-12-09 12:06:35.582903: step 21090, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.866 sec/batch; 74h:53m:38s remains)
INFO - root - 2017-12-09 12:06:44.074151: step 21100, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.839 sec/batch; 72h:34m:17s remains)
2017-12-09 12:06:44.912234: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0017783734 -0.001771084 -0.0017662155 -0.0017648073 -0.0017671346 -0.0017725133 -0.0017788913 -0.0017838591 -0.0017865751 -0.0017870378 -0.0017859463 -0.0017841191 -0.0017826778 -0.0017823186 -0.0017825597][-0.0017751218 -0.0017676803 -0.001763464 -0.0017628849 -0.0017658239 -0.0017716195 -0.0017782464 -0.0017833416 -0.0017859936 -0.0017863436 -0.0017852715 -0.0017835288 -0.0017820081 -0.0017814508 -0.0017814932][-0.0017758652 -0.0017693031 -0.0017660456 -0.0017661918 -0.0017692356 -0.0017745452 -0.0017801208 -0.0017841959 -0.0017860819 -0.0017859476 -0.0017847612 -0.0017830953 -0.0017816413 -0.0017810018 -0.0017808846][-0.0017798196 -0.00177453 -0.0017722722 -0.0017727092 -0.0017752036 -0.0017791492 -0.0017830341 -0.0017856265 -0.0017863661 -0.0017856068 -0.0017842441 -0.0017827227 -0.0017814363 -0.0017807808 -0.0017805449][-0.0017839213 -0.0017797982 -0.0017782912 -0.0017787396 -0.0017804786 -0.0017830245 -0.0017854121 -0.0017866134 -0.0017865106 -0.0017853637 -0.0017839081 -0.0017825457 -0.0017814043 -0.0017808087 -0.001780532][-0.0017877925 -0.0017845745 -0.0017833288 -0.0017834075 -0.0017840728 -0.0017851057 -0.0017859682 -0.001786132 -0.001785495 -0.0017842642 -0.0017831107 -0.0017820074 -0.0017812153 -0.0017807084 -0.001780526][-0.0017905879 -0.0017880639 -0.0017867984 -0.0017861754 -0.001785702 -0.0017854613 -0.0017851281 -0.0017844224 -0.0017835968 -0.0017826008 -0.0017818318 -0.0017810869 -0.0017807026 -0.0017805481 -0.0017804627][-0.0017912838 -0.0017889894 -0.0017874979 -0.0017862403 -0.0017851207 -0.0017842519 -0.0017834499 -0.001782581 -0.0017817924 -0.0017811722 -0.0017807299 -0.0017803991 -0.0017803995 -0.0017804481 -0.0017804515][-0.0017909126 -0.0017884533 -0.0017866382 -0.0017851272 -0.0017837894 -0.0017828138 -0.0017819485 -0.0017811593 -0.0017805687 -0.0017802054 -0.0017800144 -0.0017799373 -0.0017801827 -0.0017804429 -0.0017804767][-0.0017896865 -0.0017869786 -0.0017849643 -0.001783387 -0.0017822153 -0.0017814055 -0.0017807254 -0.0017801375 -0.0017797649 -0.0017795376 -0.0017795302 -0.0017796768 -0.0017800184 -0.0017803222 -0.0017803649][-0.0017892143 -0.0017864276 -0.001784306 -0.0017826853 -0.0017816235 -0.0017809719 -0.0017803581 -0.0017798126 -0.00177959 -0.00177957 -0.0017797665 -0.0017799767 -0.0017802643 -0.0017805587 -0.0017804907][-0.0017893759 -0.0017865995 -0.0017845259 -0.0017830201 -0.001782032 -0.001781445 -0.00178094 -0.0017804557 -0.0017802917 -0.0017802251 -0.0017803942 -0.0017805683 -0.0017807285 -0.0017809091 -0.0017807572][-0.0017893739 -0.0017866524 -0.0017846397 -0.0017832712 -0.0017824692 -0.0017819562 -0.0017815363 -0.0017812129 -0.0017810671 -0.0017809601 -0.0017810197 -0.0017811022 -0.001781114 -0.0017811061 -0.0017809335][-0.0017888149 -0.0017862603 -0.0017843612 -0.0017831256 -0.0017823334 -0.0017818885 -0.0017815502 -0.0017812955 -0.0017812012 -0.0017811033 -0.0017812405 -0.0017813378 -0.0017813045 -0.0017812037 -0.0017810024][-0.0017876589 -0.0017854053 -0.0017836732 -0.0017825008 -0.0017817391 -0.001781288 -0.0017810479 -0.0017809595 -0.0017809897 -0.0017809761 -0.001781147 -0.0017813352 -0.0017813634 -0.0017812593 -0.0017810486]]...]
INFO - root - 2017-12-09 12:06:53.629876: step 21110, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.856 sec/batch; 74h:03m:34s remains)
INFO - root - 2017-12-09 12:07:02.065389: step 21120, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.855 sec/batch; 73h:55m:02s remains)
INFO - root - 2017-12-09 12:07:10.506132: step 21130, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.854 sec/batch; 73h:50m:43s remains)
INFO - root - 2017-12-09 12:07:19.049754: step 21140, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 74h:45m:59s remains)
INFO - root - 2017-12-09 12:07:27.626272: step 21150, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.889 sec/batch; 76h:53m:35s remains)
INFO - root - 2017-12-09 12:07:36.191042: step 21160, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.877 sec/batch; 75h:49m:00s remains)
INFO - root - 2017-12-09 12:07:44.683382: step 21170, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.880 sec/batch; 76h:06m:49s remains)
INFO - root - 2017-12-09 12:07:53.201697: step 21180, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.854 sec/batch; 73h:50m:19s remains)
INFO - root - 2017-12-09 12:08:01.839863: step 21190, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.873 sec/batch; 75h:28m:34s remains)
INFO - root - 2017-12-09 12:08:10.271414: step 21200, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.830 sec/batch; 71h:44m:19s remains)
2017-12-09 12:08:11.202093: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0017597569 -0.0017573669 -0.0017563982 -0.0017557602 -0.0017557535 -0.0017562136 -0.0017568428 -0.0017573317 -0.001757525 -0.001757692 -0.0017579426 -0.0017585222 -0.00175916 -0.001759699 -0.0017599545][-0.0017569785 -0.0017543824 -0.0017533753 -0.001753011 -0.0017533975 -0.0017543951 -0.0017555965 -0.0017564101 -0.001756825 -0.0017572187 -0.0017577936 -0.0017586085 -0.0017593771 -0.0017599734 -0.0017601708][-0.0017553754 -0.0017529716 -0.0017522721 -0.0017524156 -0.0017531682 -0.0017543688 -0.0017557954 -0.0017566971 -0.0017571292 -0.0017575684 -0.0017582726 -0.0017591582 -0.0017600198 -0.0017607448 -0.001761043][-0.0017543738 -0.001752657 -0.0017525374 -0.0017531347 -0.0017540259 -0.0017551759 -0.0017564718 -0.0017571806 -0.0017574311 -0.0017578648 -0.0017587063 -0.001759719 -0.0017608408 -0.0017618411 -0.0017623822][-0.0017544035 -0.0017535468 -0.001754064 -0.0017548222 -0.0017555 -0.0017562903 -0.0017571389 -0.001757492 -0.0017577294 -0.0017584561 -0.0017597101 -0.0017610547 -0.0017625983 -0.0017640351 -0.0017648722][-0.0017558243 -0.0017557646 -0.00175676 -0.001757308 -0.0017572941 -0.0017572274 -0.0017573193 -0.0017573659 -0.0017578967 -0.0017593361 -0.0017613643 -0.0017631913 -0.0017651012 -0.00176701 -0.0017681972][-0.0017582861 -0.0017586274 -0.0017596571 -0.0017596447 -0.001758641 -0.0017574341 -0.0017565717 -0.0017563108 -0.0017572457 -0.0017595851 -0.0017626593 -0.0017651779 -0.0017674527 -0.0017698315 -0.0017713849][-0.0017610955 -0.0017613376 -0.0017620274 -0.001761342 -0.0017594725 -0.0017572935 -0.0017556315 -0.0017552045 -0.0017566199 -0.001759775 -0.0017637952 -0.0017670799 -0.00176977 -0.0017723748 -0.0017740303][-0.0017631435 -0.0017631527 -0.0017635175 -0.0017623596 -0.0017600446 -0.0017575214 -0.0017557143 -0.001755484 -0.0017573267 -0.00176099 -0.0017656321 -0.0017696392 -0.0017728157 -0.0017755484 -0.0017768971][-0.001763824 -0.001763747 -0.0017638833 -0.0017625722 -0.0017603155 -0.001757988 -0.0017565793 -0.0017568668 -0.0017591466 -0.0017630913 -0.0017679282 -0.0017722838 -0.0017757856 -0.0017783697 -0.0017792521][-0.0017637877 -0.0017635735 -0.0017635531 -0.0017623957 -0.0017605039 -0.0017586094 -0.0017576122 -0.0017582617 -0.0017606965 -0.0017645517 -0.0017692142 -0.0017735237 -0.0017769529 -0.0017791388 -0.0017796098][-0.0017635229 -0.0017631993 -0.0017631701 -0.0017623316 -0.0017609571 -0.0017595682 -0.0017587799 -0.0017593637 -0.0017614883 -0.0017648594 -0.0017689319 -0.0017727828 -0.0017758753 -0.0017777851 -0.0017781557][-0.0017631014 -0.0017626996 -0.0017627173 -0.0017623558 -0.0017615897 -0.0017607028 -0.001760073 -0.0017604076 -0.0017619876 -0.0017646075 -0.0017678165 -0.0017708839 -0.0017734499 -0.0017750775 -0.0017752883][-0.0017624679 -0.0017619545 -0.0017620046 -0.0017619935 -0.0017617294 -0.0017613169 -0.0017609523 -0.0017612342 -0.0017624465 -0.0017645434 -0.001767015 -0.0017692451 -0.0017710526 -0.0017720216 -0.0017715668][-0.0017618239 -0.0017610884 -0.0017610888 -0.0017612643 -0.0017612758 -0.0017611668 -0.0017610311 -0.0017613503 -0.0017623665 -0.0017641346 -0.0017661345 -0.0017676187 -0.001768564 -0.0017686698 -0.0017674408]]...]
INFO - root - 2017-12-09 12:08:19.819607: step 21210, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 75h:00m:37s remains)
INFO - root - 2017-12-09 12:08:28.614178: step 21220, loss = 0.82, batch loss = 0.69 (7.9 examples/sec; 1.009 sec/batch; 87h:16m:08s remains)
INFO - root - 2017-12-09 12:08:37.318684: step 21230, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.882 sec/batch; 76h:15m:50s remains)
INFO - root - 2017-12-09 12:08:45.848914: step 21240, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.878 sec/batch; 75h:55m:53s remains)
INFO - root - 2017-12-09 12:08:54.549463: step 21250, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 74h:57m:40s remains)
INFO - root - 2017-12-09 12:09:03.229374: step 21260, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.860 sec/batch; 74h:19m:35s remains)
INFO - root - 2017-12-09 12:09:11.672988: step 21270, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.852 sec/batch; 73h:37m:27s remains)
INFO - root - 2017-12-09 12:09:20.469051: step 21280, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.863 sec/batch; 74h:34m:38s remains)
INFO - root - 2017-12-09 12:09:29.150456: step 21290, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.854 sec/batch; 73h:49m:01s remains)
INFO - root - 2017-12-09 12:09:37.630573: step 21300, loss = 0.82, batch loss = 0.69 (9.7 examples/sec; 0.826 sec/batch; 71h:26m:32s remains)
2017-12-09 12:09:38.576110: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0017736787 -0.0017706653 -0.0017690754 -0.0017676394 -0.0017666493 -0.0017660339 -0.0017657391 -0.0017656374 -0.0017657541 -0.0017665565 -0.0017678714 -0.0017694853 -0.0017718994 -0.0017751144 -0.001778439][-0.0017718896 -0.0017689777 -0.0017673759 -0.0017661569 -0.0017655676 -0.0017655413 -0.0017657873 -0.0017660179 -0.0017663008 -0.0017671159 -0.0017684435 -0.0017701952 -0.0017729683 -0.0017766908 -0.0017805151][-0.0017707376 -0.0017683374 -0.0017671023 -0.0017664353 -0.0017664608 -0.0017669321 -0.0017674936 -0.001767832 -0.0017680925 -0.0017687664 -0.001769982 -0.0017718074 -0.0017748121 -0.0017790126 -0.0017831686][-0.0017692188 -0.0017673915 -0.0017667167 -0.0017668257 -0.0017676194 -0.0017686598 -0.0017694969 -0.0017698113 -0.0017698255 -0.0017702306 -0.0017712824 -0.0017731051 -0.0017763329 -0.0017809081 -0.0017853897][-0.0017682208 -0.0017670658 -0.0017670443 -0.0017678948 -0.0017693829 -0.0017708809 -0.0017718584 -0.0017721101 -0.001771931 -0.0017720876 -0.0017729988 -0.0017748764 -0.0017782768 -0.0017829977 -0.0017874719][-0.001767538 -0.0017670606 -0.0017676997 -0.0017691836 -0.0017712156 -0.0017731438 -0.0017743348 -0.001774673 -0.0017746331 -0.0017748644 -0.0017757759 -0.0017776926 -0.0017810069 -0.0017853985 -0.001789327][-0.0017672296 -0.0017670608 -0.0017681851 -0.0017701015 -0.001772541 -0.0017748955 -0.0017764985 -0.0017772834 -0.001777753 -0.0017783949 -0.0017795044 -0.0017814125 -0.0017844015 -0.0017881481 -0.0017913977][-0.0017670271 -0.0017669379 -0.0017682469 -0.0017703525 -0.0017731319 -0.0017760174 -0.0017783217 -0.0017799028 -0.0017811783 -0.0017823577 -0.0017835632 -0.0017852379 -0.0017875468 -0.0017903927 -0.0017928215][-0.0017667882 -0.0017664776 -0.0017678144 -0.0017700277 -0.0017731712 -0.0017767736 -0.001780039 -0.0017826561 -0.0017848401 -0.001786354 -0.0017874222 -0.0017884549 -0.0017897645 -0.0017916246 -0.0017933935][-0.0017665899 -0.0017662667 -0.0017676393 -0.0017699248 -0.0017733357 -0.0017775284 -0.0017816698 -0.0017853036 -0.0017881548 -0.001789752 -0.0017902867 -0.0017905228 -0.0017909113 -0.0017920173 -0.0017932045][-0.001766732 -0.0017663637 -0.0017676984 -0.0017700152 -0.001773554 -0.0017781208 -0.0017828399 -0.0017871558 -0.001790421 -0.0017920105 -0.0017921982 -0.001791818 -0.0017913425 -0.0017915408 -0.0017921246][-0.0017672621 -0.0017666846 -0.0017679154 -0.0017701533 -0.0017736114 -0.0017781411 -0.0017829546 -0.0017874744 -0.0017908815 -0.0017924838 -0.0017925765 -0.0017918653 -0.0017908149 -0.001790266 -0.0017901097][-0.0017680475 -0.0017671821 -0.0017681567 -0.0017701632 -0.0017732618 -0.0017772961 -0.0017816216 -0.0017857404 -0.0017888775 -0.0017902533 -0.0017901715 -0.0017892621 -0.0017878446 -0.0017867774 -0.0017861776][-0.0017694393 -0.0017680097 -0.0017684081 -0.0017697795 -0.0017721672 -0.0017752933 -0.0017787406 -0.0017820812 -0.0017847187 -0.001785949 -0.0017859488 -0.0017852671 -0.0017839894 -0.001782845 -0.0017819882][-0.0017719266 -0.0017696851 -0.0017691167 -0.001769378 -0.001770613 -0.0017725828 -0.001775012 -0.001777576 -0.0017798044 -0.0017811793 -0.001781637 -0.0017814357 -0.0017805821 -0.0017796309 -0.0017786415]]...]
INFO - root - 2017-12-09 12:09:47.303835: step 21310, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 75h:15m:43s remains)
INFO - root - 2017-12-09 12:09:55.823717: step 21320, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.836 sec/batch; 72h:17m:59s remains)
INFO - root - 2017-12-09 12:10:04.271296: step 21330, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.872 sec/batch; 75h:22m:15s remains)
INFO - root - 2017-12-09 12:10:12.661022: step 21340, loss = 0.83, batch loss = 0.70 (9.2 examples/sec; 0.869 sec/batch; 75h:07m:18s remains)
INFO - root - 2017-12-09 12:10:21.441732: step 21350, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.899 sec/batch; 77h:41m:52s remains)
INFO - root - 2017-12-09 12:10:30.199930: step 21360, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.881 sec/batch; 76h:09m:07s remains)
INFO - root - 2017-12-09 12:10:38.690878: step 21370, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.855 sec/batch; 73h:55m:10s remains)
INFO - root - 2017-12-09 12:10:47.302213: step 21380, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.843 sec/batch; 72h:51m:31s remains)
INFO - root - 2017-12-09 12:10:56.019070: step 21390, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.858 sec/batch; 74h:07m:15s remains)
INFO - root - 2017-12-09 12:11:04.540596: step 21400, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.843 sec/batch; 72h:52m:52s remains)
2017-12-09 12:11:05.399441: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0018023585 -0.0018026653 -0.0018031114 -0.0018033634 -0.001803436 -0.0018031992 -0.0018027432 -0.0018020811 -0.0018011782 -0.0018003319 -0.0017995129 -0.0017986745 -0.0017979223 -0.0017971635 -0.0017962472][-0.0018035672 -0.0018038198 -0.0018042787 -0.0018047196 -0.0018050411 -0.0018051538 -0.0018049513 -0.0018044084 -0.0018035026 -0.0018024842 -0.0018013569 -0.0018000385 -0.0017988952 -0.0017978398 -0.0017966894][-0.0018036994 -0.0018035194 -0.0018037637 -0.0018043522 -0.0018050829 -0.0018057674 -0.0018060984 -0.0018059305 -0.0018051945 -0.0018041962 -0.0018028966 -0.0018013373 -0.0017999155 -0.0017987075 -0.0017974201][-0.001802284 -0.0018013488 -0.001801105 -0.0018018122 -0.0018030885 -0.0018045162 -0.0018058136 -0.0018064536 -0.0018062589 -0.0018054896 -0.0018041702 -0.001802474 -0.0018007733 -0.0017994042 -0.0017979931][-0.0017998007 -0.0017978179 -0.001796775 -0.0017973527 -0.0017991664 -0.0018013696 -0.0018035602 -0.0018050587 -0.0018054901 -0.0018050768 -0.0018040178 -0.0018025094 -0.0018008499 -0.0017994936 -0.0017981146][-0.0017969749 -0.001794135 -0.0017924728 -0.001792873 -0.001794948 -0.0017977373 -0.0018005957 -0.0018028342 -0.0018038496 -0.0018039431 -0.0018032696 -0.001801945 -0.0018004803 -0.0017992446 -0.0017979778][-0.0017947684 -0.0017914447 -0.0017894676 -0.0017897084 -0.001791743 -0.0017945467 -0.0017974789 -0.0017998179 -0.001801007 -0.0018014481 -0.0018014092 -0.0018006451 -0.0017996789 -0.0017988019 -0.0017977607][-0.0017940528 -0.0017907317 -0.0017888561 -0.0017889878 -0.0017908326 -0.0017931964 -0.0017955505 -0.0017971933 -0.0017980182 -0.0017987248 -0.0017992483 -0.0017990872 -0.0017986502 -0.0017981201 -0.0017972915][-0.0017951214 -0.001792292 -0.0017907628 -0.0017907551 -0.0017919285 -0.0017933943 -0.0017948765 -0.0017957474 -0.0017962472 -0.0017970063 -0.0017977766 -0.0017979289 -0.0017977916 -0.0017974565 -0.0017967995][-0.0017973062 -0.0017952344 -0.0017940712 -0.0017938869 -0.001794177 -0.0017944968 -0.0017949814 -0.0017951013 -0.0017951827 -0.0017958218 -0.0017967127 -0.0017970076 -0.0017970848 -0.0017969258 -0.0017964349][-0.0017988882 -0.001797431 -0.0017965722 -0.0017961591 -0.0017957535 -0.0017951453 -0.0017947813 -0.0017943344 -0.001794205 -0.0017947388 -0.0017955847 -0.0017960464 -0.0017963735 -0.0017964159 -0.0017960499][-0.0017994222 -0.0017983732 -0.0017975814 -0.0017968477 -0.0017959037 -0.0017948136 -0.0017940188 -0.0017933657 -0.0017931114 -0.0017935105 -0.001794324 -0.0017949176 -0.0017953751 -0.0017955203 -0.0017953295][-0.0017991617 -0.0017981597 -0.0017972776 -0.0017963477 -0.0017952275 -0.0017940614 -0.001793306 -0.0017927814 -0.0017925849 -0.0017929259 -0.0017936081 -0.0017941069 -0.0017944687 -0.0017945921 -0.0017944615][-0.0017982351 -0.0017971479 -0.0017962278 -0.00179533 -0.0017943316 -0.0017933424 -0.0017927471 -0.0017924163 -0.0017923415 -0.001792628 -0.0017931358 -0.0017935123 -0.0017937777 -0.0017938488 -0.0017937416][-0.0017971799 -0.0017961282 -0.0017951594 -0.0017943387 -0.0017935251 -0.0017927296 -0.0017923079 -0.0017921051 -0.0017921051 -0.0017923285 -0.0017926913 -0.0017929665 -0.0017931696 -0.0017932344 -0.0017931452]]...]
INFO - root - 2017-12-09 12:11:14.183494: step 21410, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.851 sec/batch; 73h:33m:09s remains)
INFO - root - 2017-12-09 12:11:23.011738: step 21420, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.853 sec/batch; 73h:44m:11s remains)
INFO - root - 2017-12-09 12:11:31.744614: step 21430, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 74h:37m:28s remains)
INFO - root - 2017-12-09 12:11:40.264681: step 21440, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.881 sec/batch; 76h:05m:52s remains)
INFO - root - 2017-12-09 12:11:48.991024: step 21450, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.882 sec/batch; 76h:11m:10s remains)
INFO - root - 2017-12-09 12:11:57.695507: step 21460, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.831 sec/batch; 71h:49m:54s remains)
INFO - root - 2017-12-09 12:12:06.284614: step 21470, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.851 sec/batch; 73h:30m:12s remains)
INFO - root - 2017-12-09 12:12:14.880619: step 21480, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.872 sec/batch; 75h:20m:42s remains)
INFO - root - 2017-12-09 12:12:23.452486: step 21490, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.845 sec/batch; 72h:58m:36s remains)
INFO - root - 2017-12-09 12:12:31.948660: step 21500, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.857 sec/batch; 74h:00m:33s remains)
2017-12-09 12:12:32.802895: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0017955678 -0.0017943796 -0.0017939693 -0.0017905362 -0.0016388209 -0.0010289148 9.2385104e-05 0.0012851428 0.0021559154 0.0025269222 0.0022290777 0.0013761633 0.00026277464 -0.00073901715 -0.0013674186][-0.0017915232 -0.0017924472 -0.0017913062 -0.0017829839 -0.0015460511 -0.00063741603 0.0010966485 0.0030231443 0.004457131 0.0050144126 0.0045255693 0.0032187877 0.001492828 -7.3889853e-05 -0.0010816449][-0.0017684117 -0.0017726492 -0.0017652003 -0.0017247602 -0.0013483472 -0.00011290482 0.0022272551 0.0049119126 0.0070293513 0.0079255132 0.0073713562 0.0056335512 0.0032010493 0.00090952415 -0.00063953607][-0.0017126977 -0.0017009376 -0.001638148 -0.0014542951 -0.00079431233 0.00086379878 0.0037982161 0.0071763196 0.0099833515 0.011310414 0.010805369 0.0086742649 0.0054516508 0.0022758814 6.4880587e-06][-0.0015414888 -0.0015132212 -0.0013437432 -0.00085058727 0.00037902629 0.0027354644 0.006357267 0.010371268 0.013753962 0.015438135 0.014932244 0.012342233 0.0082340119 0.0040395809 0.00088757451][-0.0012539199 -0.0011862773 -0.00086072052 8.3188643e-05 0.0021273657 0.005487774 0.010002088 0.0146613 0.018467538 0.020346191 0.019706786 0.01652432 0.011418807 0.0061061285 0.0019748188][-0.00089804351 -0.00078108103 -0.00031952222 0.001054003 0.0039217686 0.0083423629 0.013773094 0.019011496 0.023055386 0.024932189 0.024062367 0.020330757 0.014391418 0.0081126085 0.0031077503][-0.00064286555 -0.00044277415 0.00010988873 0.0017237837 0.0050797365 0.010192189 0.016230635 0.021820953 0.025919082 0.027709154 0.026680861 0.022657581 0.016326731 0.00954437 0.0040203719][-0.00060747319 -0.00034711102 0.00022690848 0.0018275649 0.0051338775 0.010227458 0.016224397 0.021754816 0.025759792 0.027542097 0.02663156 0.022800721 0.016695384 0.010023496 0.0044713672][-0.00078158651 -0.00051927834 -2.8351205e-05 0.0012974065 0.0040763263 0.008484317 0.013776233 0.018782014 0.022512745 0.024348328 0.023801666 0.020604748 0.015296015 0.0093345428 0.0042638909][-0.0011155133 -0.00088495528 -0.00051740254 0.00041811576 0.0023901355 0.0056716944 0.0098144291 0.013975074 0.017292157 0.019168468 0.019065646 0.016713388 0.012518505 0.0076540424 0.0034343982][-0.001430166 -0.001274152 -0.0010492043 -0.00049709377 0.00069903338 0.00282021 0.0056851613 0.0088107577 0.011533767 0.013287947 0.013493573 0.011923648 0.00888502 0.0052848058 0.0021358128][-0.0016542743 -0.0015648903 -0.0014421311 -0.0011649304 -0.00055365055 0.00062846357 0.0023832889 0.0044963784 0.0065023643 0.0079189632 0.0082144551 0.0072290269 0.0052024727 0.0028015971 0.00072102749][-0.0017585665 -0.0017317228 -0.0016834341 -0.0015638492 -0.0012937309 -0.00072931463 0.00020023028 0.0014397599 0.0027102493 0.0036652898 0.0038975356 0.0033204025 0.002111739 0.000710335 -0.00046610006][-0.0017893877 -0.0017781904 -0.0017696718 -0.0017416412 -0.001652734 -0.0014330448 -0.0010231416 -0.00041638454 0.00025050214 0.00076871982 0.00088397751 0.00056702003 -5.3680269e-05 -0.0007285329 -0.0012578449]]...]
INFO - root - 2017-12-09 12:12:41.529695: step 21510, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.872 sec/batch; 75h:19m:23s remains)
INFO - root - 2017-12-09 12:12:50.067766: step 21520, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.860 sec/batch; 74h:15m:19s remains)
INFO - root - 2017-12-09 12:12:58.710180: step 21530, loss = 0.82, batch loss = 0.69 (9.7 examples/sec; 0.822 sec/batch; 70h:57m:53s remains)
INFO - root - 2017-12-09 12:13:07.138284: step 21540, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.872 sec/batch; 75h:21m:00s remains)
INFO - root - 2017-12-09 12:13:15.704695: step 21550, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.847 sec/batch; 73h:09m:53s remains)
INFO - root - 2017-12-09 12:13:24.345841: step 21560, loss = 0.82, batch loss = 0.70 (9.5 examples/sec; 0.846 sec/batch; 73h:04m:19s remains)
INFO - root - 2017-12-09 12:13:32.788231: step 21570, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.894 sec/batch; 77h:10m:49s remains)
INFO - root - 2017-12-09 12:13:41.576975: step 21580, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.849 sec/batch; 73h:20m:04s remains)
INFO - root - 2017-12-09 12:13:50.273208: step 21590, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.853 sec/batch; 73h:40m:19s remains)
INFO - root - 2017-12-09 12:13:58.782610: step 21600, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.857 sec/batch; 74h:02m:10s remains)
2017-12-09 12:13:59.754125: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0017845347 -0.0017780721 -0.001772817 -0.0017688732 -0.001766694 -0.0017657767 -0.0017656188 -0.0017659803 -0.0017664171 -0.001766926 -0.0017671046 -0.0017671153 -0.0017667076 -0.0017661026 -0.0017653329][-0.0017837955 -0.0017773161 -0.0017720347 -0.0017680817 -0.001765786 -0.0017647482 -0.0017645045 -0.0017647289 -0.001765226 -0.0017658437 -0.0017661334 -0.0017662153 -0.0017658829 -0.0017653469 -0.0017645634][-0.0017822181 -0.001776106 -0.0017712271 -0.0017676407 -0.0017655219 -0.0017645246 -0.0017642555 -0.0017644232 -0.0017648803 -0.0017654869 -0.0017658216 -0.0017659281 -0.001765649 -0.0017651536 -0.0017644006][-0.0017790823 -0.0017737108 -0.0017696541 -0.0017667421 -0.0017649767 -0.001764102 -0.0017638555 -0.0017640064 -0.0017644436 -0.0017650421 -0.0017654266 -0.0017655636 -0.0017653959 -0.0017649443 -0.001764238][-0.0017754523 -0.0017709143 -0.0017678046 -0.0017656441 -0.0017643084 -0.0017636159 -0.0017634337 -0.0017635974 -0.0017640549 -0.0017646507 -0.0017650526 -0.0017652183 -0.001765141 -0.0017647598 -0.0017641122][-0.0017718994 -0.0017682225 -0.0017659388 -0.0017644281 -0.0017634947 -0.0017629975 -0.0017629014 -0.0017631016 -0.0017635829 -0.0017642123 -0.0017646598 -0.0017648652 -0.0017648721 -0.0017645769 -0.0017640006][-0.001768968 -0.0017660514 -0.0017644144 -0.0017633531 -0.0017626997 -0.0017623847 -0.0017624046 -0.0017626864 -0.0017632325 -0.0017639133 -0.0017644211 -0.0017646477 -0.001764667 -0.0017643998 -0.001763886][-0.0017673002 -0.0017647744 -0.0017634955 -0.0017626553 -0.0017621178 -0.0017619012 -0.0017619941 -0.0017623305 -0.0017628917 -0.0017635834 -0.0017641425 -0.0017643886 -0.0017644295 -0.0017641889 -0.0017637487][-0.0017666327 -0.0017641905 -0.0017630121 -0.0017622263 -0.0017617223 -0.0017615684 -0.0017616835 -0.0017620113 -0.0017625572 -0.0017632471 -0.001763841 -0.0017641146 -0.0017641906 -0.0017639993 -0.0017636175][-0.0017669307 -0.001764446 -0.0017632084 -0.0017623754 -0.00176183 -0.0017616538 -0.0017617238 -0.0017620011 -0.0017624873 -0.0017631084 -0.0017636529 -0.0017638953 -0.001763973 -0.0017638167 -0.0017634896][-0.0017677047 -0.0017650934 -0.0017636752 -0.001762751 -0.0017621692 -0.0017619649 -0.0017620097 -0.0017622426 -0.0017626719 -0.0017631897 -0.0017636252 -0.0017637761 -0.0017638042 -0.0017636578 -0.0017633761][-0.001768434 -0.0017655775 -0.0017640177 -0.001763057 -0.0017624911 -0.0017623405 -0.0017624394 -0.0017626777 -0.0017630361 -0.0017634138 -0.0017637339 -0.0017637813 -0.0017637191 -0.0017635408 -0.0017632773][-0.0017688242 -0.0017658984 -0.0017642603 -0.0017633423 -0.0017628394 -0.001762745 -0.0017628786 -0.0017631379 -0.0017634431 -0.0017637063 -0.0017639028 -0.0017638505 -0.0017636971 -0.001763469 -0.0017631999][-0.0017690238 -0.0017661141 -0.0017645186 -0.001763711 -0.0017633121 -0.0017632734 -0.0017634593 -0.0017637372 -0.0017640186 -0.0017642045 -0.0017642702 -0.0017640839 -0.0017637935 -0.0017634874 -0.0017631728][-0.0017690089 -0.0017662875 -0.0017648354 -0.0017641895 -0.0017639168 -0.001763969 -0.0017642194 -0.0017645017 -0.0017647471 -0.0017648538 -0.0017647836 -0.0017644487 -0.0017640049 -0.0017635945 -0.0017632074]]...]
INFO - root - 2017-12-09 12:14:08.342630: step 21610, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.872 sec/batch; 75h:15m:50s remains)
INFO - root - 2017-12-09 12:14:16.993857: step 21620, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.866 sec/batch; 74h:47m:18s remains)
INFO - root - 2017-12-09 12:14:25.613038: step 21630, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 75h:10m:20s remains)
INFO - root - 2017-12-09 12:14:34.086878: step 21640, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.849 sec/batch; 73h:19m:26s remains)
INFO - root - 2017-12-09 12:14:42.676414: step 21650, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.870 sec/batch; 75h:05m:20s remains)
INFO - root - 2017-12-09 12:14:51.198996: step 21660, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 74h:23m:23s remains)
INFO - root - 2017-12-09 12:14:59.742925: step 21670, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.863 sec/batch; 74h:29m:37s remains)
INFO - root - 2017-12-09 12:15:08.457299: step 21680, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.889 sec/batch; 76h:44m:33s remains)
INFO - root - 2017-12-09 12:15:17.242656: step 21690, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.884 sec/batch; 76h:21m:16s remains)
INFO - root - 2017-12-09 12:15:26.029076: step 21700, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 74h:57m:28s remains)
2017-12-09 12:15:26.843431: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.001784409 -0.0017854065 -0.0017880565 -0.0017913296 -0.001794164 -0.0017961805 -0.0017979705 -0.0017985422 -0.0017975983 -0.0017953794 -0.0017926011 -0.0017895601 -0.0017863993 -0.0017837426 -0.0017814244][-0.0017876456 -0.0017894058 -0.0017925048 -0.0017958902 -0.0017985635 -0.0018001437 -0.0018014621 -0.0018016967 -0.0018004462 -0.0017979327 -0.0017950726 -0.0017919457 -0.0017883624 -0.0017850574 -0.0017821139][-0.0017910139 -0.0017931351 -0.0017962848 -0.0017995199 -0.0018019821 -0.0018031697 -0.001804031 -0.00180392 -0.0018023034 -0.0017996951 -0.0017968195 -0.0017936771 -0.0017899838 -0.0017862211 -0.0017828204][-0.0017933685 -0.0017955105 -0.0017984789 -0.0018014533 -0.0018036497 -0.0018046587 -0.0018052722 -0.0018049836 -0.001803243 -0.0018005098 -0.0017975187 -0.0017942225 -0.0017904486 -0.0017865348 -0.0017830557][-0.00179441 -0.0017960174 -0.0017984814 -0.0018010465 -0.0018028863 -0.0018036725 -0.0018043123 -0.0018043882 -0.0018028605 -0.0018001666 -0.0017969946 -0.0017934975 -0.0017897253 -0.0017858555 -0.0017826183][-0.0017937509 -0.0017948203 -0.0017967219 -0.0017986916 -0.0018000366 -0.0018007484 -0.00180156 -0.0018019453 -0.0018007536 -0.0017984015 -0.0017952261 -0.0017916652 -0.0017880192 -0.0017844803 -0.0017816814][-0.001791417 -0.0017920419 -0.0017933927 -0.0017948558 -0.001796021 -0.0017969832 -0.0017980975 -0.0017986734 -0.0017977378 -0.0017959641 -0.0017931594 -0.0017898243 -0.0017862893 -0.001783056 -0.0017807053][-0.0017879806 -0.0017881765 -0.0017891324 -0.0017903821 -0.0017917644 -0.0017933148 -0.0017947201 -0.0017954353 -0.0017947623 -0.0017936812 -0.0017915214 -0.0017884722 -0.0017850752 -0.0017819933 -0.0017799059][-0.0017841092 -0.0017839889 -0.0017847785 -0.0017860394 -0.0017878065 -0.0017899181 -0.0017917359 -0.0017927443 -0.0017923904 -0.0017918273 -0.001790321 -0.0017875326 -0.0017842569 -0.0017812181 -0.0017792361][-0.0017803131 -0.0017801535 -0.0017809127 -0.0017822044 -0.0017841452 -0.0017865875 -0.0017888555 -0.0017902292 -0.0017902763 -0.0017901944 -0.0017891618 -0.0017867747 -0.0017836338 -0.0017806788 -0.0017788311][-0.0017772815 -0.0017770968 -0.001777823 -0.0017790997 -0.00178099 -0.0017834987 -0.0017859384 -0.0017874286 -0.0017877264 -0.0017879691 -0.0017874104 -0.0017856308 -0.0017829863 -0.0017803373 -0.0017786289][-0.0017753306 -0.0017747955 -0.0017754041 -0.0017766545 -0.0017784657 -0.0017807857 -0.0017831518 -0.0017846823 -0.0017851706 -0.0017856344 -0.0017854179 -0.00178425 -0.0017823323 -0.0017801157 -0.0017785217][-0.0017747341 -0.0017738937 -0.0017743268 -0.001775532 -0.0017771671 -0.0017791056 -0.0017811863 -0.0017825842 -0.0017831225 -0.0017836376 -0.0017835483 -0.0017827998 -0.001781471 -0.0017796978 -0.0017782646][-0.001775198 -0.0017742476 -0.0017746396 -0.001775653 -0.0017769873 -0.001778518 -0.0017801746 -0.0017812967 -0.0017817627 -0.0017821596 -0.001782115 -0.0017815734 -0.0017805507 -0.0017791021 -0.0017778785][-0.0017759564 -0.0017750721 -0.0017754054 -0.0017761842 -0.0017770956 -0.0017781167 -0.0017792508 -0.00177999 -0.0017803143 -0.0017806041 -0.0017806105 -0.0017802372 -0.001779431 -0.0017783286 -0.0017774103]]...]
INFO - root - 2017-12-09 12:15:35.575062: step 21710, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.848 sec/batch; 73h:10m:20s remains)
INFO - root - 2017-12-09 12:15:44.235255: step 21720, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.850 sec/batch; 73h:22m:08s remains)
INFO - root - 2017-12-09 12:15:52.902747: step 21730, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.882 sec/batch; 76h:06m:57s remains)
INFO - root - 2017-12-09 12:16:01.464184: step 21740, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.832 sec/batch; 71h:50m:45s remains)
INFO - root - 2017-12-09 12:16:10.190822: step 21750, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.858 sec/batch; 74h:05m:40s remains)
INFO - root - 2017-12-09 12:16:18.898547: step 21760, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 75h:10m:15s remains)
INFO - root - 2017-12-09 12:16:27.417662: step 21770, loss = 0.82, batch loss = 0.69 (9.8 examples/sec; 0.813 sec/batch; 70h:08m:56s remains)
INFO - root - 2017-12-09 12:16:36.185085: step 21780, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.844 sec/batch; 72h:51m:32s remains)
INFO - root - 2017-12-09 12:16:44.942522: step 21790, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.889 sec/batch; 76h:41m:22s remains)
INFO - root - 2017-12-09 12:16:53.563279: step 21800, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.854 sec/batch; 73h:43m:01s remains)
2017-12-09 12:16:54.437496: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.16681384 0.16518927 0.1626174 0.15903881 0.15547168 0.15094881 0.14669082 0.14172474 0.13592546 0.12763904 0.11545592 0.098402537 0.0772771 0.055107564 0.034302559][0.17182407 0.17236526 0.17188697 0.17011639 0.16776821 0.16392583 0.159676 0.15432373 0.14775141 0.13818246 0.12445903 0.10567311 0.0827202 0.0587324 0.036382385][0.1742408 0.17741098 0.17965384 0.18023755 0.17963958 0.17652678 0.17217146 0.16593456 0.15800275 0.14678557 0.13129365 0.11096354 0.086564653 0.061350279 0.037961476][0.17513061 0.18102117 0.18618561 0.18932226 0.19068536 0.18871266 0.18464573 0.177727 0.16852072 0.15572922 0.13856721 0.11651466 0.090567157 0.064069077 0.039581798][0.17474185 0.18308343 0.1907685 0.19640163 0.19978985 0.198757 0.19497736 0.18791865 0.17812769 0.16422524 0.14582333 0.12255991 0.095326416 0.06742236 0.041705009][0.17255309 0.18311529 0.19284074 0.20059866 0.20578165 0.20600732 0.20275535 0.19556105 0.18552372 0.17115209 0.15218121 0.12815101 0.10006788 0.071142711 0.044308674][0.16878018 0.18080866 0.19184177 0.20109013 0.20768401 0.20922783 0.2070698 0.20077203 0.19129774 0.17701088 0.15763739 0.13293742 0.10398393 0.074142151 0.046433952][0.16421562 0.176963 0.18838143 0.19827247 0.20555526 0.20806146 0.20710652 0.20193391 0.19351135 0.18008192 0.16101773 0.13599242 0.10635915 0.075849637 0.047493283][0.15952761 0.17230672 0.18340626 0.1933144 0.2009584 0.20432387 0.20431967 0.20023298 0.19303484 0.1806989 0.16237974 0.13764352 0.10800081 0.077196732 0.048407406][0.15573554 0.1676717 0.1775813 0.18693361 0.19437255 0.19774866 0.19821033 0.19522461 0.18909037 0.1777806 0.16029896 0.13659133 0.10756846 0.077013612 0.0483977][0.151148 0.16210884 0.17072706 0.179094 0.18592143 0.18923624 0.18998721 0.18752873 0.18205713 0.17163816 0.15509182 0.13240235 0.10439633 0.074907668 0.047161762][0.1465379 0.15581521 0.16263337 0.16948873 0.17517856 0.17812081 0.1793288 0.17770186 0.17304166 0.16359726 0.14812237 0.12657489 0.099694893 0.071480907 0.044926379][0.14191602 0.14906113 0.15340766 0.15833081 0.16244589 0.16479695 0.16610712 0.16540012 0.16197778 0.15368015 0.13980429 0.11980935 0.094522767 0.067744136 0.042473629][0.13618524 0.14174719 0.14423668 0.1471532 0.14942001 0.15080445 0.15198022 0.15194263 0.14969856 0.14310291 0.13123703 0.11326279 0.089895971 0.064691909 0.0407245][0.12992598 0.13408253 0.13499428 0.13636565 0.1371413 0.13750283 0.13824835 0.1385261 0.13689065 0.13180758 0.12203713 0.10625088 0.0849738 0.061653137 0.039282795]]...]
INFO - root - 2017-12-09 12:17:03.120616: step 21810, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.902 sec/batch; 77h:53m:05s remains)
INFO - root - 2017-12-09 12:17:11.761629: step 21820, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.845 sec/batch; 72h:57m:18s remains)
INFO - root - 2017-12-09 12:17:20.453138: step 21830, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.872 sec/batch; 75h:16m:32s remains)
INFO - root - 2017-12-09 12:17:29.004889: step 21840, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.846 sec/batch; 72h:58m:47s remains)
INFO - root - 2017-12-09 12:17:37.806172: step 21850, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.888 sec/batch; 76h:38m:36s remains)
INFO - root - 2017-12-09 12:17:46.470989: step 21860, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.853 sec/batch; 73h:35m:57s remains)
INFO - root - 2017-12-09 12:17:54.914399: step 21870, loss = 0.82, batch loss = 0.69 (10.0 examples/sec; 0.803 sec/batch; 69h:17m:52s remains)
INFO - root - 2017-12-09 12:18:03.533529: step 21880, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.838 sec/batch; 72h:17m:20s remains)
INFO - root - 2017-12-09 12:18:12.029962: step 21890, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.835 sec/batch; 72h:02m:08s remains)
INFO - root - 2017-12-09 12:18:20.400921: step 21900, loss = 0.82, batch loss = 0.69 (9.9 examples/sec; 0.807 sec/batch; 69h:39m:08s remains)
2017-12-09 12:18:21.243526: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0017811406 -0.0017810309 -0.0017816201 -0.0017818135 -0.0017815878 -0.0017811088 -0.0017803776 -0.0017794607 -0.001778275 -0.001777193 -0.0017761056 -0.0017753338 -0.0017746843 -0.0017742161 -0.001773818][-0.0017840699 -0.0017837862 -0.0017841178 -0.0017841201 -0.0017836643 -0.0017828579 -0.001781745 -0.0017803198 -0.0017786571 -0.0017771041 -0.0017756504 -0.001774685 -0.0017739243 -0.0017733661 -0.0017729364][-0.0017882072 -0.001787884 -0.0017880272 -0.0017879256 -0.0017873552 -0.0017863339 -0.0017846939 -0.001782425 -0.0017800551 -0.001777841 -0.0017760627 -0.0017748608 -0.0017738598 -0.0017732272 -0.001772774][-0.0017916027 -0.0017910018 -0.0017911193 -0.0017911849 -0.0017908 -0.0017897363 -0.0017877995 -0.0017850221 -0.001781884 -0.0017790319 -0.0017769641 -0.0017755141 -0.0017742616 -0.0017734421 -0.0017729459][-0.0017926035 -0.0017913108 -0.0017911225 -0.0017913326 -0.0017915658 -0.0017911297 -0.0017896867 -0.0017873259 -0.0017840446 -0.0017807804 -0.0017783479 -0.0017765067 -0.0017748876 -0.0017737971 -0.0017732113][-0.0017913515 -0.0017894983 -0.0017892276 -0.001789546 -0.0017902574 -0.001790626 -0.0017903032 -0.0017885556 -0.0017854482 -0.0017819029 -0.0017792296 -0.0017771926 -0.0017752617 -0.0017740082 -0.0017735172][-0.0017884974 -0.001786552 -0.0017866369 -0.001787457 -0.0017888745 -0.0017901494 -0.0017907729 -0.0017897731 -0.00178681 -0.001783179 -0.0017802842 -0.0017778686 -0.0017756083 -0.0017739648 -0.0017733541][-0.0017847422 -0.0017826669 -0.0017830731 -0.0017844633 -0.001786893 -0.001789395 -0.0017915418 -0.0017916515 -0.0017890298 -0.0017853081 -0.0017820789 -0.0017793367 -0.0017766947 -0.0017747091 -0.0017738392][-0.0017814189 -0.00177916 -0.001779697 -0.0017815497 -0.0017848492 -0.001788466 -0.0017918113 -0.0017930748 -0.001791104 -0.0017874788 -0.0017838243 -0.0017805832 -0.0017775276 -0.001775142 -0.0017740477][-0.0017805482 -0.0017784156 -0.0017789988 -0.0017809573 -0.0017843989 -0.0017882671 -0.0017919801 -0.001793595 -0.0017920902 -0.0017886274 -0.0017848947 -0.0017815056 -0.0017781557 -0.001775511 -0.0017742327][-0.0017815433 -0.001779682 -0.0017801482 -0.0017818968 -0.001784832 -0.0017880852 -0.0017913326 -0.001792888 -0.0017916653 -0.0017884671 -0.001784912 -0.0017816011 -0.001778258 -0.0017756317 -0.0017743761][-0.0017828814 -0.0017813358 -0.0017816763 -0.0017830321 -0.0017851975 -0.0017874365 -0.0017895923 -0.0017907573 -0.0017897701 -0.0017871702 -0.001784034 -0.0017809895 -0.001777866 -0.0017753041 -0.001774064][-0.001783925 -0.001782691 -0.0017828452 -0.0017837667 -0.001785095 -0.0017863831 -0.0017877022 -0.0017881928 -0.0017871731 -0.001784918 -0.0017822748 -0.0017797373 -0.0017771045 -0.001774953 -0.0017738766][-0.0017832905 -0.0017824864 -0.0017827777 -0.0017835537 -0.001784326 -0.0017849801 -0.0017854979 -0.001785211 -0.0017839411 -0.0017819154 -0.0017797274 -0.001777699 -0.0017757571 -0.0017742229 -0.0017733783][-0.0017812813 -0.0017804698 -0.0017806633 -0.0017812216 -0.001781684 -0.0017819823 -0.0017820874 -0.0017816748 -0.0017806224 -0.0017791544 -0.0017776258 -0.001776198 -0.001774843 -0.0017737924 -0.0017731322]]...]
INFO - root - 2017-12-09 12:18:29.746494: step 21910, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.863 sec/batch; 74h:26m:58s remains)
INFO - root - 2017-12-09 12:18:38.513207: step 21920, loss = 0.82, batch loss = 0.69 (8.7 examples/sec; 0.918 sec/batch; 79h:12m:54s remains)
INFO - root - 2017-12-09 12:18:47.030141: step 21930, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.863 sec/batch; 74h:25m:16s remains)
INFO - root - 2017-12-09 12:18:55.627091: step 21940, loss = 0.81, batch loss = 0.68 (10.2 examples/sec; 0.784 sec/batch; 67h:37m:46s remains)
INFO - root - 2017-12-09 12:19:04.253579: step 21950, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.865 sec/batch; 74h:37m:02s remains)
INFO - root - 2017-12-09 12:19:12.975430: step 21960, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.834 sec/batch; 71h:57m:24s remains)
INFO - root - 2017-12-09 12:19:21.707548: step 21970, loss = 0.82, batch loss = 0.69 (8.8 examples/sec; 0.910 sec/batch; 78h:29m:04s remains)
INFO - root - 2017-12-09 12:19:30.261546: step 21980, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.881 sec/batch; 76h:01m:16s remains)
INFO - root - 2017-12-09 12:19:38.826584: step 21990, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.890 sec/batch; 76h:44m:30s remains)
INFO - root - 2017-12-09 12:19:47.362056: step 22000, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.865 sec/batch; 74h:34m:12s remains)
2017-12-09 12:19:48.242307: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.0081668664 0.010621955 0.01345489 0.015938912 0.018023564 0.018925587 0.018304408 0.01645094 0.01361295 0.010507997 0.0074778218 0.0051431004 0.0035727595 0.0026979756 0.0022169987][0.011715984 0.015610091 0.01997154 0.024075959 0.027344519 0.028743092 0.027805904 0.024824491 0.020470971 0.015540203 0.010718415 0.006854211 0.0041963821 0.002691604 0.0018718201][0.014525639 0.020217091 0.02658275 0.032712568 0.03757076 0.040032372 0.039378837 0.035883877 0.030493774 0.024154419 0.017749954 0.012214027 0.008115625 0.0054822695 0.0038000008][0.016336044 0.023617603 0.032112788 0.040851954 0.048122451 0.052522816 0.05300083 0.049898122 0.044060919 0.03683842 0.029227715 0.022371817 0.016940424 0.012984667 0.010095364][0.017904054 0.026553053 0.036967818 0.048071072 0.057789084 0.064862743 0.067555048 0.066095315 0.061261404 0.0544129 0.046511233 0.038725294 0.032038886 0.026587125 0.022193266][0.019590443 0.02972728 0.04202557 0.055295888 0.067293286 0.076664336 0.081459485 0.082118295 0.078938976 0.073286861 0.065889493 0.05785745 0.050214894 0.043308236 0.037329938][0.020973854 0.032456841 0.046512906 0.061717656 0.075752027 0.087183006 0.094030239 0.096703611 0.095200911 0.091015786 0.084499553 0.076716565 0.068765864 0.061086398 0.054165676][0.022051938 0.034295525 0.04934828 0.065725073 0.081141047 0.094020166 0.10249018 0.10683572 0.10676757 0.10361043 0.097725183 0.090563275 0.082787395 0.075279765 0.068419032][0.022280009 0.034855261 0.050105162 0.066762142 0.082708277 0.096238218 0.10567176 0.1111934 0.11228962 0.1099586 0.10464665 0.098109573 0.090795174 0.083890654 0.077538721][0.021296086 0.033643816 0.048623882 0.065021954 0.0807665 0.0942181 0.10389578 0.10980872 0.11133871 0.10924194 0.10414844 0.097954065 0.091019727 0.084532253 0.0785505][0.018331585 0.029721163 0.043610234 0.058958061 0.073882744 0.086723976 0.096013516 0.10174375 0.10327493 0.1011508 0.096108779 0.090009958 0.083296813 0.076960005 0.071041435][0.013799708 0.023233552 0.035000645 0.0482729 0.061363686 0.072735265 0.081034452 0.086098075 0.08741188 0.085315347 0.080587961 0.074818812 0.06854815 0.062546089 0.0569189][0.0087933447 0.015562046 0.02422782 0.034245946 0.04435223 0.053298093 0.059909325 0.063934922 0.064985871 0.063280076 0.059465948 0.054734871 0.049567748 0.044535212 0.039803091][0.0043324046 0.0085215084 0.014055826 0.0205929 0.02730765 0.033341162 0.037836913 0.040554237 0.041242622 0.040049754 0.037425406 0.034114257 0.030495714 0.026933983 0.02358296][0.0011848584 0.0033609935 0.0063511077 0.0099826725 0.013785441 0.017254444 0.019869145 0.02143647 0.021824596 0.021121476 0.01959282 0.017648023 0.015508855 0.013389029 0.01139595]]...]
INFO - root - 2017-12-09 12:19:56.891420: step 22010, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.849 sec/batch; 73h:15m:49s remains)
INFO - root - 2017-12-09 12:20:05.505214: step 22020, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.849 sec/batch; 73h:14m:32s remains)
INFO - root - 2017-12-09 12:20:14.188244: step 22030, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.863 sec/batch; 74h:26m:17s remains)
INFO - root - 2017-12-09 12:20:22.952689: step 22040, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.901 sec/batch; 77h:40m:43s remains)
INFO - root - 2017-12-09 12:20:31.431086: step 22050, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.863 sec/batch; 74h:24m:45s remains)
INFO - root - 2017-12-09 12:20:39.959836: step 22060, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.855 sec/batch; 73h:43m:30s remains)
INFO - root - 2017-12-09 12:20:48.541737: step 22070, loss = 0.81, batch loss = 0.68 (9.3 examples/sec; 0.861 sec/batch; 74h:13m:12s remains)
INFO - root - 2017-12-09 12:20:57.007545: step 22080, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.865 sec/batch; 74h:34m:09s remains)
INFO - root - 2017-12-09 12:21:05.649672: step 22090, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.849 sec/batch; 73h:13m:31s remains)
INFO - root - 2017-12-09 12:21:13.996253: step 22100, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.839 sec/batch; 72h:21m:16s remains)
2017-12-09 12:21:14.879711: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0017125618 -0.0017472857 -0.0017765719 -0.0017949775 -0.0018041719 -0.001807445 -0.0018084254 -0.00180866 -0.0018077372 -0.0018057262 -0.0018036349 -0.0018030561 -0.0018032099 -0.0018032982 -0.0018029502][-0.0014649744 -0.0016129556 -0.0017204697 -0.0017777894 -0.0018012931 -0.0018085295 -0.001810296 -0.0018103569 -0.0018090772 -0.0018064784 -0.0018038183 -0.0018028907 -0.0018031239 -0.0018031648 -0.0018027102][-0.00082521955 -0.001197882 -0.0014956621 -0.0016767115 -0.0017649438 -0.0017992135 -0.001809499 -0.0018113337 -0.001810079 -0.0018070103 -0.0018044261 -0.0018033524 -0.0018034041 -0.0018033944 -0.0018028321][-2.1105167e-05 -0.00063674082 -0.001174183 -0.0015283425 -0.0017118566 -0.0017857199 -0.0018076613 -0.0018112645 -0.0018099716 -0.0018068147 -0.0018039637 -0.0018026832 -0.0018030229 -0.0018033141 -0.001802862][0.0004260944 -0.00028414733 -0.00095068244 -0.0014181865 -0.0016718833 -0.0017754697 -0.0018052929 -0.0018096031 -0.0018080606 -0.0018051225 -0.0018030165 -0.0018024944 -0.0018030057 -0.00180334 -0.0018029099][0.00016541628 -0.00044346752 -0.0010337238 -0.00145617 -0.0016869281 -0.0017801319 -0.0018056928 -0.0018080892 -0.0018057162 -0.0018027193 -0.0018010933 -0.0018014886 -0.0018026492 -0.0018031958 -0.0018028119][-0.00052971928 -0.000929739 -0.0013142447 -0.0015868328 -0.0017334741 -0.0017913915 -0.0018062391 -0.0018067088 -0.0018042753 -0.0018019793 -0.0018012321 -0.0018018661 -0.001802634 -0.001803037 -0.0018026495][-0.0012184326 -0.0014069085 -0.0015836705 -0.0017071591 -0.0017732048 -0.0017993075 -0.0018058581 -0.0018052285 -0.0018032712 -0.0018020051 -0.0018018336 -0.0018025398 -0.0018028138 -0.0018028019 -0.0018023808][-0.0016063371 -0.0016680635 -0.0017254843 -0.0017665158 -0.0017904679 -0.0018014266 -0.0018040346 -0.001803169 -0.0018020344 -0.0018015269 -0.0018017355 -0.0018022393 -0.0018023342 -0.0018023211 -0.0018021051][-0.0017397708 -0.0017530615 -0.00176926 -0.0017838647 -0.0017951467 -0.0018010217 -0.0018023171 -0.0018016784 -0.0018009325 -0.001801105 -0.0018013774 -0.0018016619 -0.0018019 -0.0018020326 -0.001801991][-0.001773651 -0.0017761232 -0.0017829572 -0.0017910659 -0.0017980816 -0.0018013328 -0.0018016875 -0.0018010264 -0.0018004775 -0.0018008242 -0.0018013894 -0.0018015889 -0.0018015567 -0.0018017 -0.0018017333][-0.0017886098 -0.0017890468 -0.0017923353 -0.001796731 -0.001800413 -0.0018017101 -0.001801451 -0.0018009602 -0.001800707 -0.0018011184 -0.0018015846 -0.0018016507 -0.0018016335 -0.0018015433 -0.0018015463][-0.0017986615 -0.0017981991 -0.0017991529 -0.0018008415 -0.0018019518 -0.0018018903 -0.001801266 -0.0018007698 -0.0018007462 -0.0018012164 -0.0018015702 -0.001801605 -0.0018015399 -0.0018014029 -0.0018013803][-0.0018029351 -0.0018022456 -0.0018023248 -0.0018025922 -0.0018024661 -0.001802001 -0.0018014305 -0.0018010344 -0.0018010235 -0.0018012644 -0.0018014221 -0.0018015027 -0.0018014518 -0.0018013372 -0.0018012957][-0.0018035765 -0.0018029331 -0.0018028063 -0.0018026523 -0.0018023304 -0.0018019588 -0.0018016084 -0.0018013584 -0.0018012928 -0.0018013333 -0.0018013098 -0.0018013046 -0.0018013195 -0.0018012801 -0.0018012791]]...]
INFO - root - 2017-12-09 12:21:23.616787: step 22110, loss = 0.81, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 74h:45m:52s remains)
INFO - root - 2017-12-09 12:21:32.268749: step 22120, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 74h:52m:54s remains)
INFO - root - 2017-12-09 12:21:40.947409: step 22130, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.890 sec/batch; 76h:42m:10s remains)
INFO - root - 2017-12-09 12:21:49.599707: step 22140, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 74h:56m:29s remains)
INFO - root - 2017-12-09 12:21:58.029465: step 22150, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.840 sec/batch; 72h:27m:06s remains)
INFO - root - 2017-12-09 12:22:06.581062: step 22160, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.877 sec/batch; 75h:35m:34s remains)
INFO - root - 2017-12-09 12:22:15.190332: step 22170, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.845 sec/batch; 72h:52m:46s remains)
INFO - root - 2017-12-09 12:22:23.719053: step 22180, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.856 sec/batch; 73h:44m:40s remains)
INFO - root - 2017-12-09 12:22:32.435102: step 22190, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.883 sec/batch; 76h:04m:23s remains)
INFO - root - 2017-12-09 12:22:41.034300: step 22200, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.901 sec/batch; 77h:39m:37s remains)
2017-12-09 12:22:41.989205: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0017842746 -0.0017829898 -0.0017824849 -0.0017822329 -0.0017822178 -0.0017823441 -0.0017826247 -0.0017830119 -0.0017832812 -0.0017832633 -0.0017831208 -0.0017830159 -0.0017829518 -0.00178294 -0.0017829657][-0.0017833728 -0.0017821952 -0.0017817094 -0.0017814417 -0.0017814022 -0.0017815856 -0.0017819221 -0.0017823749 -0.0017827319 -0.0017828216 -0.0017827493 -0.0017826476 -0.0017825481 -0.0017824685 -0.0017824295][-0.0017829435 -0.0017819576 -0.0017814984 -0.0017812196 -0.0017811414 -0.0017813477 -0.0017817181 -0.0017822069 -0.0017826504 -0.0017828486 -0.0017828814 -0.0017828322 -0.0017827131 -0.0017826061 -0.0017825442][-0.0017826825 -0.0017817214 -0.0017812393 -0.001780898 -0.0017807286 -0.0017809208 -0.0017813154 -0.0017818337 -0.0017823627 -0.0017827018 -0.0017828719 -0.0017829149 -0.001782804 -0.0017827 -0.0017826403][-0.0017825721 -0.0017815182 -0.0017809928 -0.0017805913 -0.0017803257 -0.0017804377 -0.0017807957 -0.0017813139 -0.001781888 -0.0017823343 -0.0017826434 -0.0017827876 -0.0017827153 -0.0017826251 -0.0017825864][-0.0017824143 -0.0017813185 -0.001780806 -0.001780393 -0.0017801063 -0.0017801172 -0.0017803513 -0.0017807711 -0.0017813197 -0.0017818141 -0.0017822052 -0.0017824445 -0.0017824338 -0.0017823613 -0.0017823434][-0.0017823675 -0.0017811576 -0.0017806641 -0.0017802935 -0.0017800205 -0.0017799167 -0.0017799662 -0.0017802075 -0.0017806656 -0.0017811414 -0.0017815712 -0.0017819055 -0.0017820033 -0.001782004 -0.001782037][-0.0017825549 -0.0017811771 -0.0017806963 -0.0017803492 -0.0017800778 -0.0017798388 -0.0017796657 -0.0017796878 -0.001780003 -0.0017804264 -0.0017808675 -0.001781304 -0.0017815408 -0.0017816612 -0.0017817611][-0.0017828324 -0.001781332 -0.0017808945 -0.0017805588 -0.001780267 -0.0017799033 -0.0017795375 -0.001779347 -0.0017794884 -0.0017798323 -0.0017802687 -0.0017807708 -0.0017811287 -0.0017813595 -0.0017815202][-0.0017828192 -0.0017814633 -0.0017811373 -0.0017808916 -0.0017806435 -0.0017802488 -0.0017797801 -0.0017794028 -0.001779345 -0.001779557 -0.0017799357 -0.0017804526 -0.0017808852 -0.0017811714 -0.001781349][-0.0017827369 -0.001781567 -0.0017814002 -0.0017813189 -0.001781182 -0.0017808495 -0.0017803829 -0.0017799201 -0.0017796857 -0.0017797282 -0.0017799967 -0.0017804778 -0.0017809246 -0.0017812233 -0.0017813769][-0.0017829002 -0.0017818544 -0.0017817932 -0.0017818457 -0.0017818223 -0.0017816145 -0.0017812093 -0.0017807201 -0.0017803618 -0.0017802662 -0.0017804098 -0.0017807896 -0.0017811882 -0.0017814842 -0.0017816053][-0.0017832039 -0.0017823252 -0.0017823016 -0.0017824281 -0.0017824851 -0.0017823768 -0.0017820481 -0.0017815803 -0.0017811718 -0.0017809856 -0.001781024 -0.0017812768 -0.0017815948 -0.0017818337 -0.0017818962][-0.0017835936 -0.0017827997 -0.0017827818 -0.0017829452 -0.0017830555 -0.001783028 -0.0017827844 -0.0017823768 -0.001781997 -0.001781777 -0.0017817231 -0.001781823 -0.0017820093 -0.001782146 -0.0017821377][-0.0017841597 -0.0017833036 -0.0017832047 -0.001783369 -0.0017835007 -0.001783527 -0.0017833643 -0.0017830523 -0.0017827431 -0.0017825174 -0.0017823803 -0.0017823239 -0.0017823544 -0.0017823696 -0.0017823043]]...]
INFO - root - 2017-12-09 12:22:50.651536: step 22210, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.847 sec/batch; 73h:02m:34s remains)
INFO - root - 2017-12-09 12:22:59.329859: step 22220, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.866 sec/batch; 74h:38m:05s remains)
INFO - root - 2017-12-09 12:23:08.035297: step 22230, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.861 sec/batch; 74h:13m:21s remains)
INFO - root - 2017-12-09 12:23:16.697654: step 22240, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.851 sec/batch; 73h:18m:25s remains)
INFO - root - 2017-12-09 12:23:25.239924: step 22250, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.856 sec/batch; 73h:46m:53s remains)
INFO - root - 2017-12-09 12:23:33.733399: step 22260, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.848 sec/batch; 73h:06m:05s remains)
INFO - root - 2017-12-09 12:23:42.377352: step 22270, loss = 0.82, batch loss = 0.69 (8.2 examples/sec; 0.981 sec/batch; 84h:32m:58s remains)
INFO - root - 2017-12-09 12:23:51.011098: step 22280, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.883 sec/batch; 76h:06m:47s remains)
INFO - root - 2017-12-09 12:23:59.754288: step 22290, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.842 sec/batch; 72h:34m:56s remains)
INFO - root - 2017-12-09 12:24:08.228976: step 22300, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.861 sec/batch; 74h:13m:00s remains)
2017-12-09 12:24:09.059468: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.001770121 -0.0017543557 -0.0017083737 -0.00161344 -0.0014780451 -0.0013482504 -0.0012604111 -0.0012512235 -0.00132756 -0.001453958 -0.0015835547 -0.0016831204 -0.001744569 -0.0017696192 -0.0017736157][-0.0017462797 -0.0016973655 -0.0015567644 -0.0012854183 -0.00092144986 -0.00058732566 -0.00039558322 -0.0004202472 -0.00065238832 -0.00098649482 -0.0013065503 -0.0015449296 -0.0016895954 -0.0017546676 -0.0017718491][-0.0017080618 -0.0015791507 -0.0012286492 -0.00058932404 0.00023642089 0.00098330108 0.0013944018 0.0013246536 0.00080837426 7.09194e-05 -0.00064634241 -0.0011971248 -0.0015430715 -0.0017110562 -0.0017646335][-0.0016542745 -0.0013772245 -0.00066424778 0.00057443976 0.0021311638 0.0035377305 0.0043301769 0.0042443788 0.0033296284 0.0019613078 0.0005702523 -0.00054295454 -0.0012649347 -0.0016265301 -0.0017494224][-0.0015822775 -0.0010872527 0.00011320226 0.0021116396 0.0045692241 0.0067942003 0.0080876593 0.0080250511 0.0066455761 0.0044830507 0.0022045888 0.00033287005 -0.000897331 -0.001516198 -0.0017296273][-0.0014854192 -0.00073999446 0.0009705096 0.0037212239 0.00705126 0.010077361 0.011886282 0.011870289 0.010033422 0.0070558442 0.0038549628 0.0012001423 -0.00054402708 -0.00141335 -0.0017111432][-0.001381004 -0.00043365976 0.0016370553 0.0048763957 0.0087631447 0.012323782 0.014514622 0.014560229 0.012412393 0.008850812 0.0049830861 0.0017702156 -0.00032714719 -0.0013560069 -0.0017010826][-0.0013102847 -0.00029561867 0.0018293601 0.0050890315 0.0089941258 0.012615724 0.014903882 0.014997316 0.012810954 0.00913297 0.0051232157 0.0018030684 -0.00034013914 -0.0013676094 -0.0017021333][-0.0013064174 -0.00039111602 0.0014597764 0.0042652441 0.0076385569 0.010811799 0.01286197 0.012974508 0.011054383 0.00778859 0.0042295307 0.0013019838 -0.00056216354 -0.0014343727 -0.0017102018][-0.001379265 -0.0006918147 0.00065989722 0.0027033484 0.005190854 0.0075802663 0.0091682924 0.0092911869 0.0078538842 0.0053758454 0.0026788025 0.00047819619 -0.00090087234 -0.001529153 -0.0017218575][-0.0014931102 -0.0010778208 -0.00027189276 0.0009628993 0.0025178071 0.0040771123 0.0051802434 0.0053463848 0.0044759586 0.0028942258 0.0011460376 -0.00028790126 -0.0011863646 -0.0015937034 -0.0017215965][-0.0016103205 -0.0014136514 -0.0010300078 -0.00041755871 0.00041233585 0.0013185893 0.0020451776 0.0022739682 0.0018906612 0.0010480878 5.2905059e-05 -0.000799365 -0.001354817 -0.0016203542 -0.0017148448][-0.0016786556 -0.0016013039 -0.0014396616 -0.0011450407 -0.00066963013 -5.4171775e-05 0.00055598444 0.00092376466 0.0008901367 0.00047773006 -0.00014994177 -0.00077565317 -0.0012459191 -0.0015208707 -0.0016556467][-0.0016944429 -0.0016464448 -0.0015334014 -0.0012819363 -0.00079365307 -4.7457055e-05 0.00083323475 0.0015614762 0.0018451526 0.0015919632 0.00092134136 9.8202843e-05 -0.00063368632 -0.0011516493 -0.0014625631][-0.0016943391 -0.001631038 -0.001474899 -0.0010948905 -0.00030617381 0.00098940544 0.0026198381 0.0040995255 0.0048765494 0.0046946425 0.0036768748 0.0022452821 0.00084921927 -0.00024182489 -0.00096856285]]...]
INFO - root - 2017-12-09 12:24:17.808841: step 22310, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 74h:44m:16s remains)
INFO - root - 2017-12-09 12:24:26.453464: step 22320, loss = 0.81, batch loss = 0.68 (9.2 examples/sec; 0.869 sec/batch; 74h:52m:27s remains)
INFO - root - 2017-12-09 12:24:35.144048: step 22330, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.884 sec/batch; 76h:11m:29s remains)
INFO - root - 2017-12-09 12:24:43.775320: step 22340, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.865 sec/batch; 74h:31m:27s remains)
INFO - root - 2017-12-09 12:24:51.938685: step 22350, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 74h:28m:14s remains)
INFO - root - 2017-12-09 12:25:00.496000: step 22360, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.866 sec/batch; 74h:37m:26s remains)
INFO - root - 2017-12-09 12:25:08.953657: step 22370, loss = 0.81, batch loss = 0.68 (9.1 examples/sec; 0.879 sec/batch; 75h:43m:53s remains)
INFO - root - 2017-12-09 12:25:17.443946: step 22380, loss = 0.82, batch loss = 0.69 (9.7 examples/sec; 0.822 sec/batch; 70h:48m:46s remains)
INFO - root - 2017-12-09 12:25:26.205384: step 22390, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.875 sec/batch; 75h:24m:05s remains)
INFO - root - 2017-12-09 12:25:34.823325: step 22400, loss = 0.82, batch loss = 0.69 (8.8 examples/sec; 0.908 sec/batch; 78h:10m:25s remains)
2017-12-09 12:25:35.696260: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0017855782 -0.0017852738 -0.0017856875 -0.0017861105 -0.001786266 -0.0017860883 -0.0017856833 -0.0017853046 -0.00178505 -0.0017849229 -0.001784873 -0.0017848579 -0.0017848138 -0.0017846616 -0.0017844642][-0.0017852676 -0.0017849298 -0.0017853937 -0.0017858644 -0.0017860148 -0.0017857575 -0.0017852088 -0.0017846997 -0.0017843522 -0.0017841614 -0.0017840895 -0.0017840713 -0.0017840179 -0.0017838402 -0.0017836201][-0.0017854113 -0.0017849759 -0.0017853901 -0.0017858215 -0.0017859112 -0.0017855313 -0.0017848344 -0.0017842281 -0.0017838145 -0.0017835642 -0.0017834631 -0.0017834293 -0.0017833506 -0.0017831528 -0.0017829528][-0.0017854349 -0.0017847772 -0.0017849865 -0.0017852472 -0.0017851546 -0.0017846161 -0.0017838248 -0.00178315 -0.0017826658 -0.0017823571 -0.0017822451 -0.001782186 -0.0017820986 -0.0017819549 -0.0017818493][-0.0017854052 -0.0017844747 -0.0017844151 -0.0017844348 -0.0017841099 -0.0017833866 -0.001782508 -0.0017817824 -0.001781242 -0.0017808917 -0.0017807639 -0.0017807184 -0.0017806601 -0.0017805937 -0.0017806272][-0.0017852589 -0.0017841036 -0.0017837922 -0.0017835625 -0.0017830497 -0.0017821906 -0.0017812544 -0.0017804611 -0.0017798851 -0.0017795285 -0.001779423 -0.0017794318 -0.0017794026 -0.0017794111 -0.0017795705][-0.0017850514 -0.0017837359 -0.0017832168 -0.0017827768 -0.0017821133 -0.0017812063 -0.0017803316 -0.0017796217 -0.0017791546 -0.001778924 -0.0017788983 -0.0017789479 -0.0017788956 -0.0017789007 -0.0017791141][-0.0017848953 -0.0017834407 -0.0017827904 -0.0017822114 -0.0017814682 -0.0017805786 -0.0017798148 -0.0017792559 -0.0017789699 -0.0017789361 -0.0017790177 -0.0017790671 -0.0017789805 -0.0017789619 -0.0017791704][-0.0017846565 -0.0017832461 -0.0017826159 -0.001782018 -0.0017812934 -0.0017805025 -0.0017798885 -0.0017794352 -0.0017792331 -0.0017793104 -0.0017794961 -0.0017795719 -0.0017794652 -0.0017794321 -0.0017795835][-0.0017844589 -0.0017831592 -0.0017826478 -0.0017821335 -0.001781528 -0.0017808796 -0.001780372 -0.0017799389 -0.0017797053 -0.0017797468 -0.0017799392 -0.0017800396 -0.0017799541 -0.0017799045 -0.0017799663][-0.001784292 -0.0017831263 -0.0017827142 -0.001782333 -0.0017818977 -0.0017814477 -0.0017810813 -0.0017807189 -0.0017804706 -0.0017804548 -0.0017805798 -0.0017806638 -0.0017805891 -0.0017805367 -0.0017805219][-0.0017842299 -0.0017831135 -0.0017827519 -0.0017825114 -0.0017822558 -0.0017820342 -0.001781897 -0.0017817346 -0.0017815842 -0.0017815672 -0.0017816615 -0.0017816732 -0.0017815204 -0.001781371 -0.0017812161][-0.0017841001 -0.0017831364 -0.0017828131 -0.001782694 -0.0017825979 -0.0017825679 -0.0017826202 -0.0017826419 -0.0017826329 -0.0017826504 -0.0017826862 -0.0017826095 -0.0017823593 -0.001782089 -0.0017817995][-0.0017841053 -0.0017832296 -0.0017829406 -0.0017828911 -0.0017828546 -0.0017828748 -0.0017829688 -0.0017830476 -0.0017830947 -0.001783144 -0.0017831564 -0.0017830542 -0.0017828001 -0.0017825151 -0.0017822145][-0.0017842525 -0.0017833365 -0.0017830259 -0.0017829916 -0.0017829431 -0.001782914 -0.0017829478 -0.0017830082 -0.0017830709 -0.0017831475 -0.0017832031 -0.0017831905 -0.0017830759 -0.0017829097 -0.0017827054]]...]
INFO - root - 2017-12-09 12:25:44.550732: step 22410, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.857 sec/batch; 73h:46m:54s remains)
INFO - root - 2017-12-09 12:25:53.269635: step 22420, loss = 0.82, batch loss = 0.69 (8.8 examples/sec; 0.905 sec/batch; 77h:54m:43s remains)
INFO - root - 2017-12-09 12:26:02.027769: step 22430, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.896 sec/batch; 77h:09m:49s remains)
INFO - root - 2017-12-09 12:26:10.711511: step 22440, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 74h:52m:01s remains)
INFO - root - 2017-12-09 12:26:19.079008: step 22450, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.857 sec/batch; 73h:50m:29s remains)
INFO - root - 2017-12-09 12:26:27.631171: step 22460, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.848 sec/batch; 73h:03m:11s remains)
INFO - root - 2017-12-09 12:26:36.093803: step 22470, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.835 sec/batch; 71h:53m:18s remains)
INFO - root - 2017-12-09 12:26:44.552376: step 22480, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.856 sec/batch; 73h:40m:44s remains)
INFO - root - 2017-12-09 12:26:53.072164: step 22490, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.857 sec/batch; 73h:47m:08s remains)
INFO - root - 2017-12-09 12:27:01.556581: step 22500, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.849 sec/batch; 73h:08m:24s remains)
2017-12-09 12:27:02.519453: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0017707932 -0.0017692174 -0.0017692709 -0.0017695876 -0.0017694117 -0.0017687879 -0.0017680014 -0.0017673288 -0.0017669051 -0.0017667469 -0.0017669655 -0.0017673535 -0.0017677327 -0.001767827 -0.0017677015][-0.0017695607 -0.0017680548 -0.0017682073 -0.0017685967 -0.0017682865 -0.0017673252 -0.0017660831 -0.0017650626 -0.0017645261 -0.0017644297 -0.0017649191 -0.0017656646 -0.0017663992 -0.001766635 -0.0017665244][-0.0017694695 -0.0017680902 -0.0017682447 -0.0017686012 -0.0017680001 -0.0017665743 -0.0017647404 -0.0017633315 -0.0017627998 -0.0017629545 -0.0017638716 -0.0017650843 -0.0017661611 -0.0017665687 -0.0017664741][-0.0017694101 -0.0017681632 -0.0017682783 -0.0017685103 -0.0017676306 -0.001765797 -0.001763482 -0.0017617403 -0.0017612188 -0.0017617267 -0.0017630878 -0.0017647103 -0.0017659973 -0.0017665303 -0.0017665025][-0.0017694267 -0.0017681523 -0.0017682058 -0.0017682662 -0.0017671996 -0.0017652101 -0.0017627541 -0.0017609684 -0.0017605292 -0.0017612262 -0.0017627573 -0.0017645143 -0.0017658659 -0.0017664258 -0.0017664762][-0.0017694554 -0.0017681272 -0.0017680752 -0.0017679692 -0.0017668894 -0.0017650642 -0.0017629096 -0.0017614441 -0.0017611681 -0.001761872 -0.0017632802 -0.0017648403 -0.0017660054 -0.0017664663 -0.0017665242][-0.0017694253 -0.001768087 -0.0017680019 -0.0017678487 -0.0017669587 -0.0017655502 -0.0017639513 -0.0017629971 -0.0017628965 -0.0017635056 -0.0017645756 -0.0017656666 -0.0017664308 -0.0017666672 -0.0017666508][-0.001769373 -0.0017680168 -0.0017679181 -0.0017678005 -0.0017672706 -0.0017664542 -0.0017655046 -0.0017649883 -0.0017650027 -0.0017654282 -0.001766038 -0.0017665606 -0.0017668395 -0.0017668306 -0.0017667242][-0.0017692212 -0.001767878 -0.0017678087 -0.0017677232 -0.0017675767 -0.0017673324 -0.0017669533 -0.0017667112 -0.0017667526 -0.0017669915 -0.0017671747 -0.0017671933 -0.0017670961 -0.0017668948 -0.0017667268][-0.0017689775 -0.0017677278 -0.001767709 -0.001767674 -0.0017677466 -0.0017678114 -0.0017677977 -0.0017677397 -0.001767776 -0.0017678584 -0.0017678014 -0.001767547 -0.0017672229 -0.0017668925 -0.0017666616][-0.0017688927 -0.0017676729 -0.0017676026 -0.0017675918 -0.0017677379 -0.0017679259 -0.0017680844 -0.0017681654 -0.001768244 -0.0017682579 -0.0017680549 -0.001767683 -0.0017672797 -0.0017668855 -0.0017665959][-0.001768934 -0.0017676398 -0.0017675044 -0.0017675204 -0.0017677183 -0.001767964 -0.0017681939 -0.0017683458 -0.0017684432 -0.0017684076 -0.0017681044 -0.0017676873 -0.0017672989 -0.001766913 -0.0017666024][-0.0017690107 -0.0017676331 -0.0017674343 -0.0017674898 -0.0017677344 -0.0017680161 -0.0017682654 -0.0017683905 -0.001768437 -0.0017683329 -0.0017679729 -0.0017675717 -0.0017672359 -0.0017669176 -0.0017666458][-0.0017691435 -0.0017676626 -0.0017674228 -0.0017675039 -0.0017677363 -0.0017680119 -0.0017682149 -0.0017682619 -0.0017682271 -0.0017680689 -0.0017677099 -0.0017673535 -0.0017670911 -0.0017668536 -0.0017666545][-0.0017692891 -0.001767749 -0.0017674373 -0.001767533 -0.001767717 -0.0017679406 -0.0017680443 -0.0017680029 -0.0017678968 -0.0017676889 -0.0017673454 -0.0017670244 -0.0017668288 -0.0017666856 -0.0017665781]]...]
INFO - root - 2017-12-09 12:27:11.063318: step 22510, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.859 sec/batch; 73h:55m:27s remains)
INFO - root - 2017-12-09 12:27:19.611435: step 22520, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 74h:58m:08s remains)
INFO - root - 2017-12-09 12:27:28.188361: step 22530, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.849 sec/batch; 73h:05m:21s remains)
INFO - root - 2017-12-09 12:27:36.831713: step 22540, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.884 sec/batch; 76h:08m:36s remains)
INFO - root - 2017-12-09 12:27:45.396020: step 22550, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.880 sec/batch; 75h:47m:40s remains)
INFO - root - 2017-12-09 12:27:54.032247: step 22560, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.873 sec/batch; 75h:11m:44s remains)
INFO - root - 2017-12-09 12:28:02.695929: step 22570, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.876 sec/batch; 75h:25m:53s remains)
INFO - root - 2017-12-09 12:28:11.224559: step 22580, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.889 sec/batch; 76h:29m:27s remains)
INFO - root - 2017-12-09 12:28:19.881448: step 22590, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.875 sec/batch; 75h:17m:26s remains)
INFO - root - 2017-12-09 12:28:28.314388: step 22600, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.847 sec/batch; 72h:56m:45s remains)
2017-12-09 12:28:29.151407: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.043366943 0.039034717 0.04102226 0.047502868 0.058118436 0.06971477 0.081391387 0.091403618 0.09824647 0.10142018 0.10046103 0.096619368 0.090299316 0.082022719 0.073046386][0.051481172 0.046337783 0.0480169 0.054605514 0.065802217 0.077859864 0.089948155 0.10035941 0.10777365 0.11171168 0.11164875 0.10909957 0.10391446 0.096300311 0.0873242][0.062252678 0.056950662 0.058555584 0.065174073 0.076278195 0.08788193 0.099323571 0.10886609 0.11568954 0.11929736 0.11943243 0.1176108 0.11327295 0.10663031 0.098120183][0.073887125 0.069205061 0.070949353 0.0774663 0.088139661 0.09878701 0.10911752 0.1170351 0.12258046 0.12512037 0.12489909 0.1231319 0.11923891 0.11341459 0.10525275][0.084402487 0.080782756 0.082736686 0.089072354 0.099044263 0.10879917 0.11785452 0.12394485 0.12776367 0.12863438 0.12724274 0.12465763 0.12051252 0.11502945 0.10727375][0.093094528 0.090424754 0.092386462 0.098452121 0.10760064 0.11624351 0.12404756 0.12838238 0.13030705 0.12914483 0.12600426 0.12175608 0.11646868 0.11061136 0.10305045][0.098744996 0.09688247 0.098592423 0.10433644 0.11259619 0.12012062 0.12656941 0.12938917 0.12956965 0.12626114 0.12106882 0.11481299 0.10786582 0.10088443 0.0930714][0.10079653 0.099256925 0.10041247 0.10561863 0.11308336 0.1199536 0.12557366 0.1272475 0.12583125 0.12058096 0.11328807 0.10461088 0.095546871 0.087072149 0.07885097][0.099368565 0.097375393 0.097581685 0.10202825 0.10877849 0.11513895 0.12015067 0.12135383 0.11906363 0.11256833 0.10356269 0.092884846 0.081918165 0.071829483 0.062958419][0.09578377 0.093140572 0.0922778 0.095734194 0.10146099 0.1070826 0.1113805 0.11231239 0.10935285 0.10222846 0.092034861 0.080007166 0.067662127 0.056463733 0.047152288][0.090820014 0.087486163 0.0855362 0.087829076 0.09227784 0.096703067 0.099964611 0.10033605 0.096952006 0.0895343 0.078915544 0.06649781 0.053704292 0.042104986 0.032795321][0.085308075 0.081481174 0.078527957 0.079416625 0.082291886 0.08537288 0.087404326 0.087157227 0.08348123 0.076092936 0.0656387 0.05354302 0.041126013 0.029899595 0.021002833][0.080851607 0.076198675 0.072067656 0.071389236 0.07247214 0.073855288 0.074495666 0.0735468 0.069680057 0.062705182 0.053014845 0.04194504 0.030696636 0.020612502 0.012754721][0.07809677 0.072419629 0.0666429 0.063806027 0.062566817 0.061804764 0.060742505 0.058908179 0.055051956 0.048928987 0.04068087 0.031409513 0.022090923 0.013813858 0.0074282493][0.076042965 0.069850035 0.06250789 0.057238 0.05324693 0.050035536 0.047064651 0.044134621 0.040236793 0.035086773 0.028645027 0.021585811 0.014623939 0.0085470919 0.0039403429]]...]
INFO - root - 2017-12-09 12:28:37.862760: step 22610, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.878 sec/batch; 75h:36m:14s remains)
INFO - root - 2017-12-09 12:28:46.452207: step 22620, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 74h:11m:28s remains)
INFO - root - 2017-12-09 12:28:55.087144: step 22630, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.880 sec/batch; 75h:44m:06s remains)
INFO - root - 2017-12-09 12:29:03.791745: step 22640, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.856 sec/batch; 73h:39m:14s remains)
INFO - root - 2017-12-09 12:29:12.245696: step 22650, loss = 0.82, batch loss = 0.70 (9.5 examples/sec; 0.841 sec/batch; 72h:21m:42s remains)
INFO - root - 2017-12-09 12:29:20.861337: step 22660, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.855 sec/batch; 73h:35m:10s remains)
INFO - root - 2017-12-09 12:29:29.543758: step 22670, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 74h:58m:57s remains)
INFO - root - 2017-12-09 12:29:38.026017: step 22680, loss = 0.81, batch loss = 0.68 (9.1 examples/sec; 0.877 sec/batch; 75h:28m:01s remains)
INFO - root - 2017-12-09 12:29:46.709998: step 22690, loss = 0.81, batch loss = 0.68 (9.0 examples/sec; 0.884 sec/batch; 76h:06m:57s remains)
INFO - root - 2017-12-09 12:29:55.292125: step 22700, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.901 sec/batch; 77h:33m:19s remains)
2017-12-09 12:29:56.239238: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0011687254 -0.00068743108 -0.00010165724 0.00048048189 0.0010067353 0.0014475209 0.0017125951 0.0017267352 0.001451741 0.001023636 0.00054174173 7.2851195e-05 -0.00041652692 -0.00089066132 -0.0013006513][-0.0011889301 -0.00073134538 -0.00015151023 0.00047779642 0.001090673 0.0016124959 0.0019370189 0.0019884214 0.0017474538 0.0013415462 0.00085108262 0.00034924084 -0.00018959679 -0.00072518759 -0.0011933728][-0.0012759027 -0.00085981371 -0.000352239 0.00022044429 0.00081243529 0.0013484063 0.0017202951 0.0018512388 0.001719313 0.0014142604 0.00098394859 0.0004919034 -5.4789474e-05 -0.0006016579 -0.00108595][-0.0014441167 -0.0011051921 -0.00067548535 -0.00016004941 0.00039869826 0.000935395 0.0013501858 0.0015822898 0.0016073887 0.0014418927 0.0010892549 0.00061155157 6.0850172e-05 -0.00048248097 -0.00096061616][-0.0016256528 -0.0014107567 -0.0010915641 -0.00064121769 -9.5748459e-05 0.00046603498 0.00093422178 0.0012376748 0.0013537216 0.0012781974 0.0010120112 0.00059340149 8.0729253e-05 -0.00042706844 -0.00087372842][-0.0017315084 -0.001623202 -0.0014232697 -0.001074811 -0.00058259012 -1.9369763e-05 0.00048156176 0.00083101075 0.00099416985 0.00096560107 0.00077164848 0.00044093258 1.352618e-05 -0.00041800574 -0.00080366852][-0.0017493768 -0.0017118427 -0.0016132108 -0.0013786366 -0.00097839581 -0.00046636222 2.7239788e-05 0.00039168936 0.00057774875 0.00058223889 0.00045024673 0.00019795704 -0.00014752138 -0.00049647957 -0.00080175139][-0.00176645 -0.0017338051 -0.0016685359 -0.0015172539 -0.0012226556 -0.00081740436 -0.00042366982 -0.00013132463 2.175977e-05 3.7398073e-05 -3.94386e-05 -0.00020754558 -0.00045476959 -0.00070135749 -0.000902119][-0.0017907387 -0.0017676435 -0.0017190973 -0.0016174371 -0.0014242555 -0.0011510248 -0.00089887134 -0.00073759176 -0.00067686825 -0.00068030506 -0.00072577433 -0.00081670622 -0.00094586075 -0.0010576674 -0.0011122965][-0.0018023003 -0.001793034 -0.0017686401 -0.0017063573 -0.0015961977 -0.0014600903 -0.001353503 -0.0013116424 -0.0013275933 -0.0013585746 -0.0013904733 -0.0014215445 -0.0014529277 -0.0014351319 -0.0013429457][-0.001802734 -0.0018024361 -0.0017991437 -0.0017780286 -0.0017389081 -0.001689903 -0.0016538119 -0.0016482518 -0.0016698653 -0.0016929312 -0.0017058576 -0.0017149056 -0.0017043574 -0.0016350533 -0.0014853913][-0.0018017845 -0.0018013981 -0.0018010649 -0.0017974026 -0.0017884409 -0.0017773769 -0.0017706102 -0.0017698886 -0.0017759183 -0.0017778742 -0.001784121 -0.0017848285 -0.0017729203 -0.0017153871 -0.0015831412][-0.0018005768 -0.0018000397 -0.0017995809 -0.0017984009 -0.0017976223 -0.0017975307 -0.0017977238 -0.001797967 -0.0017971857 -0.0017949268 -0.0017942543 -0.0017913352 -0.001787809 -0.0017580006 -0.0016829591][-0.001799394 -0.0017987862 -0.0017986981 -0.0017980335 -0.0017975973 -0.001797296 -0.0017972975 -0.0017975401 -0.0017978128 -0.001798106 -0.0017982994 -0.0017983702 -0.0017950454 -0.0017852612 -0.0017581149][-0.0017989223 -0.0017980997 -0.0017978516 -0.0017978125 -0.0017980668 -0.0017980328 -0.0017977878 -0.0017976334 -0.0017974458 -0.0017973656 -0.0017973828 -0.0017979468 -0.0017981644 -0.0017971244 -0.0017933564]]...]
INFO - root - 2017-12-09 12:30:04.815815: step 22710, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.835 sec/batch; 71h:49m:55s remains)
INFO - root - 2017-12-09 12:30:13.429781: step 22720, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.866 sec/batch; 74h:30m:56s remains)
INFO - root - 2017-12-09 12:30:22.120265: step 22730, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.838 sec/batch; 72h:07m:04s remains)
INFO - root - 2017-12-09 12:30:30.707957: step 22740, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.885 sec/batch; 76h:08m:08s remains)
INFO - root - 2017-12-09 12:30:39.149631: step 22750, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.879 sec/batch; 75h:38m:10s remains)
INFO - root - 2017-12-09 12:30:47.785295: step 22760, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.845 sec/batch; 72h:39m:39s remains)
INFO - root - 2017-12-09 12:30:56.428956: step 22770, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.874 sec/batch; 75h:10m:19s remains)
INFO - root - 2017-12-09 12:31:05.157704: step 22780, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 74h:20m:56s remains)
INFO - root - 2017-12-09 12:31:13.820746: step 22790, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.873 sec/batch; 75h:07m:13s remains)
INFO - root - 2017-12-09 12:31:22.363467: step 22800, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.833 sec/batch; 71h:37m:43s remains)
2017-12-09 12:31:23.352376: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0017994726 -0.0017990631 -0.0017995274 -0.0018006499 -0.0018021425 -0.0018035946 -0.0018048583 -0.0018057661 -0.0018060368 -0.0018056416 -0.0018048026 -0.0018036999 -0.0018023514 -0.0018012745 -0.001800532][-0.0017986252 -0.0017982878 -0.0017988872 -0.0018001053 -0.0018016003 -0.0018030296 -0.0018042575 -0.0018050729 -0.0018055011 -0.0018052649 -0.0018045357 -0.0018034549 -0.0018020885 -0.0018009776 -0.0018001042][-0.0017989815 -0.0017989462 -0.0018000533 -0.0018016408 -0.0018033552 -0.0018046383 -0.001805407 -0.0018058756 -0.0018060934 -0.0018058356 -0.0018050917 -0.0018037852 -0.001802367 -0.0018011653 -0.0018001307][-0.0017999073 -0.0018003616 -0.0018019245 -0.0018038661 -0.0018052598 -0.0018057125 -0.0018054104 -0.001805104 -0.0018048938 -0.0018048704 -0.0018046176 -0.001803788 -0.0018027308 -0.0018017262 -0.0018006688][-0.0018014193 -0.0018023369 -0.0018039538 -0.0018054604 -0.0018057474 -0.00180461 -0.0018027722 -0.0018009687 -0.0018001224 -0.0018006287 -0.0018017648 -0.001802489 -0.0018025363 -0.0018021271 -0.0018012716][-0.0018027901 -0.001804048 -0.0018055054 -0.0018063382 -0.0018053619 -0.0018024659 -0.0017985096 -0.0017949737 -0.0017935787 -0.001794541 -0.0017969243 -0.0017994776 -0.0018011732 -0.0018019235 -0.0018016429][-0.0018037206 -0.0018051557 -0.0018065055 -0.0018069365 -0.0018052338 -0.0018010424 -0.0017955096 -0.0017910025 -0.0017896531 -0.0017907507 -0.0017933899 -0.0017966218 -0.0017996789 -0.0018012964 -0.0018013716][-0.001804243 -0.0018057978 -0.0018072476 -0.0018073882 -0.0018055656 -0.0018009157 -0.0017946193 -0.0017897877 -0.0017887701 -0.0017903303 -0.0017928907 -0.0017959741 -0.0017990022 -0.0018006801 -0.0018007651][-0.0018045707 -0.0018060801 -0.0018075682 -0.0018078524 -0.0018062727 -0.0018023322 -0.0017969847 -0.0017926437 -0.00179147 -0.0017926424 -0.0017947417 -0.0017971902 -0.0017996418 -0.0018009652 -0.0018009166][-0.0018042233 -0.0018055832 -0.0018069478 -0.0018072882 -0.001806225 -0.0018036739 -0.0018004642 -0.0017977993 -0.0017964672 -0.0017968577 -0.0017979697 -0.0017992813 -0.0018006264 -0.0018013864 -0.0018013621][-0.0018033 -0.0018044139 -0.0018056115 -0.0018062158 -0.0018060053 -0.0018050754 -0.0018038078 -0.001802575 -0.0018017868 -0.001801634 -0.0018016443 -0.0018016627 -0.0018016798 -0.001801625 -0.0018014258][-0.0018018652 -0.0018026772 -0.0018038835 -0.0018048303 -0.0018052758 -0.0018054122 -0.0018054229 -0.0018052353 -0.001804933 -0.001804693 -0.0018041986 -0.001803409 -0.0018025606 -0.0018018342 -0.0018011993][-0.0018005089 -0.0018008257 -0.001802011 -0.0018032163 -0.0018041251 -0.0018048992 -0.0018056388 -0.0018059715 -0.0018059732 -0.0018056968 -0.0018049086 -0.0018037707 -0.0018025377 -0.001801515 -0.0018007648][-0.0017997158 -0.001799396 -0.0018002184 -0.0018014559 -0.0018024568 -0.0018034242 -0.0018045058 -0.0018052881 -0.0018055182 -0.0018052567 -0.0018044771 -0.0018032818 -0.0018019509 -0.0018007942 -0.0018000926][-0.0017993369 -0.0017985627 -0.0017988067 -0.0017996732 -0.0018006023 -0.0018014418 -0.0018023942 -0.0018032386 -0.0018036789 -0.0018036185 -0.0018030697 -0.0018021103 -0.0018010178 -0.0018000555 -0.0017994726]]...]
INFO - root - 2017-12-09 12:31:32.175454: step 22810, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.853 sec/batch; 73h:21m:04s remains)
INFO - root - 2017-12-09 12:31:40.909940: step 22820, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.892 sec/batch; 76h:41m:21s remains)
INFO - root - 2017-12-09 12:31:49.533098: step 22830, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.865 sec/batch; 74h:21m:54s remains)
INFO - root - 2017-12-09 12:31:58.222812: step 22840, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.840 sec/batch; 72h:16m:24s remains)
INFO - root - 2017-12-09 12:32:06.740748: step 22850, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.876 sec/batch; 75h:21m:17s remains)
INFO - root - 2017-12-09 12:32:15.486149: step 22860, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.880 sec/batch; 75h:42m:12s remains)
INFO - root - 2017-12-09 12:32:24.279512: step 22870, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.861 sec/batch; 74h:01m:43s remains)
INFO - root - 2017-12-09 12:32:32.909595: step 22880, loss = 0.82, batch loss = 0.69 (11.0 examples/sec; 0.729 sec/batch; 62h:39m:30s remains)
INFO - root - 2017-12-09 12:32:41.592179: step 22890, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.858 sec/batch; 73h:49m:32s remains)
INFO - root - 2017-12-09 12:32:50.118187: step 22900, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.837 sec/batch; 71h:56m:34s remains)
2017-12-09 12:32:50.980616: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.001784016 -0.0017833678 -0.001783459 -0.0017834632 -0.0017832646 -0.0017829976 -0.00178269 -0.0017824383 -0.0017820087 -0.0017816494 -0.0017813256 -0.0017810506 -0.0017807513 -0.0017805442 -0.0017804325][-0.0017846581 -0.0017839774 -0.0017837364 -0.0017832968 -0.0017827726 -0.0017823636 -0.0017822079 -0.00178201 -0.0017815669 -0.0017811578 -0.001780759 -0.0017803821 -0.0017800435 -0.0017798271 -0.0017797048][-0.0017877214 -0.001786637 -0.0017853909 -0.0017834943 -0.0017817812 -0.0017812136 -0.0017814557 -0.0017816423 -0.0017812303 -0.0017808613 -0.0017805503 -0.001780238 -0.0017799952 -0.0017798075 -0.0017796922][-0.0017904212 -0.0017882199 -0.0017848456 -0.0017804195 -0.0017770578 -0.001776744 -0.0017788972 -0.0017808462 -0.0017811122 -0.0017806891 -0.0017804581 -0.0017802686 -0.0017801169 -0.0017799804 -0.0017798541][-0.001790609 -0.0017859322 -0.0017799257 -0.0017726602 -0.0017671577 -0.0017671739 -0.0017730669 -0.0017788627 -0.001781176 -0.0017807714 -0.0017805286 -0.0017803742 -0.001780268 -0.0017801509 -0.0017800136][-0.0017877037 -0.0017796623 -0.0017714538 -0.0017618826 -0.0017541241 -0.0017536851 -0.0017637134 -0.0017751602 -0.0017811283 -0.0017810146 -0.0017807118 -0.0017805485 -0.0017804503 -0.0017803228 -0.0017801378][-0.0017833223 -0.0017717236 -0.0017625232 -0.0017526948 -0.0017436524 -0.0017412865 -0.0017536556 -0.0017700386 -0.001780417 -0.0017809863 -0.0017807117 -0.0017806145 -0.0017805935 -0.0017805113 -0.0017802827][-0.0017790045 -0.0017652879 -0.0017571165 -0.0017493026 -0.0017407914 -0.0017364236 -0.0017485305 -0.0017668955 -0.0017796112 -0.0017808421 -0.0017806012 -0.0017805874 -0.0017806297 -0.0017805352 -0.0017803335][-0.0017759204 -0.0017621889 -0.0017564188 -0.0017519514 -0.0017460515 -0.001741209 -0.0017510809 -0.001767935 -0.0017797656 -0.0017811449 -0.0017808283 -0.0017807543 -0.0017807489 -0.0017806212 -0.001780352][-0.0017768191 -0.001764914 -0.0017612483 -0.0017592125 -0.0017559277 -0.0017521215 -0.001759073 -0.0017718333 -0.0017805452 -0.0017814634 -0.00178113 -0.0017809721 -0.0017808428 -0.0017806106 -0.0017803089][-0.0017822798 -0.0017731658 -0.0017703142 -0.0017689375 -0.0017673399 -0.001764758 -0.0017687764 -0.0017763262 -0.0017810366 -0.0017812352 -0.001780962 -0.0017808815 -0.00178078 -0.0017805514 -0.0017802485][-0.001789004 -0.0017827571 -0.0017802401 -0.0017784428 -0.0017771497 -0.0017754394 -0.0017769528 -0.001779826 -0.001781184 -0.0017807126 -0.0017804481 -0.0017804047 -0.0017803276 -0.0017802045 -0.0017800371][-0.0017930182 -0.0017891267 -0.0017871342 -0.0017852482 -0.0017838217 -0.001782371 -0.0017819556 -0.0017817877 -0.0017811789 -0.0017803684 -0.0017800801 -0.001780014 -0.0017799323 -0.0017798535 -0.0017797677][-0.0017940302 -0.0017916956 -0.0017899284 -0.0017879778 -0.0017862269 -0.0017846471 -0.0017833046 -0.0017819303 -0.0017808268 -0.0017800976 -0.001779858 -0.0017797818 -0.0017796996 -0.0017796272 -0.0017795652][-0.0017929781 -0.0017915504 -0.0017898744 -0.0017879587 -0.0017860032 -0.0017843519 -0.0017828017 -0.0017813311 -0.001780352 -0.0017798147 -0.0017796479 -0.0017795956 -0.0017795369 -0.0017794868 -0.0017794459]]...]
INFO - root - 2017-12-09 12:32:59.742612: step 22910, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.892 sec/batch; 76h:44m:16s remains)
INFO - root - 2017-12-09 12:33:08.304085: step 22920, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.848 sec/batch; 72h:54m:02s remains)
INFO - root - 2017-12-09 12:33:17.030740: step 22930, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.852 sec/batch; 73h:15m:39s remains)
INFO - root - 2017-12-09 12:33:25.637787: step 22940, loss = 0.82, batch loss = 0.69 (9.7 examples/sec; 0.822 sec/batch; 70h:41m:24s remains)
INFO - root - 2017-12-09 12:33:34.164593: step 22950, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.877 sec/batch; 75h:25m:37s remains)
INFO - root - 2017-12-09 12:33:42.752663: step 22960, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.859 sec/batch; 73h:49m:41s remains)
INFO - root - 2017-12-09 12:33:51.389930: step 22970, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.843 sec/batch; 72h:30m:46s remains)
INFO - root - 2017-12-09 12:33:59.970726: step 22980, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.861 sec/batch; 74h:02m:18s remains)
INFO - root - 2017-12-09 12:34:08.404884: step 22990, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.880 sec/batch; 75h:39m:32s remains)
INFO - root - 2017-12-09 12:34:16.786919: step 23000, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.888 sec/batch; 76h:21m:49s remains)
2017-12-09 12:34:17.683783: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0017675728 -0.0017683785 -0.0017727879 -0.001779363 -0.0017873144 -0.0017956247 -0.0018028617 -0.0018073367 -0.0018079574 -0.0018050087 -0.0017988182 -0.0017910214 -0.0017830748 -0.0017765758 -0.0017721386][-0.0017675931 -0.0017680398 -0.0017720186 -0.0017782954 -0.001786101 -0.0017942094 -0.0018012749 -0.001805716 -0.0018063125 -0.0018032201 -0.0017970059 -0.0017892087 -0.0017814271 -0.0017751816 -0.0017710114][-0.0017682966 -0.001768404 -0.0017719016 -0.0017777854 -0.0017853779 -0.0017930698 -0.001799615 -0.0018036577 -0.0018040015 -0.0018006666 -0.0017943894 -0.0017869225 -0.0017796877 -0.0017741456 -0.0017706086][-0.0017688727 -0.0017688341 -0.0017720396 -0.0017775672 -0.0017847101 -0.001791658 -0.0017972694 -0.00180059 -0.0018005192 -0.0017970303 -0.0017910006 -0.001784204 -0.0017779316 -0.0017733743 -0.0017706502][-0.0017694624 -0.0017693224 -0.0017724158 -0.0017775832 -0.0017838479 -0.0017894848 -0.0017937452 -0.0017960331 -0.0017954244 -0.0017920409 -0.0017867936 -0.0017811718 -0.0017763075 -0.0017728596 -0.0017709037][-0.0017698186 -0.0017696922 -0.0017726889 -0.0017773121 -0.0017822462 -0.0017862123 -0.001788977 -0.0017902188 -0.0017892091 -0.001786434 -0.0017825215 -0.0017784601 -0.0017751448 -0.0017727335 -0.0017712803][-0.001769564 -0.0017697256 -0.001772692 -0.0017765742 -0.0017799247 -0.0017820339 -0.0017832653 -0.0017835944 -0.0017826519 -0.0017809402 -0.0017786905 -0.0017764287 -0.0017744618 -0.001772806 -0.0017716145][-0.0017690705 -0.0017696838 -0.0017725553 -0.0017754778 -0.0017772274 -0.0017776589 -0.001777526 -0.0017771708 -0.0017765614 -0.0017759597 -0.0017752526 -0.0017745297 -0.0017736332 -0.0017725601 -0.0017715944][-0.0017687969 -0.0017698234 -0.0017723998 -0.0017742395 -0.0017745803 -0.0017737735 -0.0017726924 -0.0017719845 -0.0017718251 -0.0017721895 -0.0017726398 -0.0017728899 -0.0017726684 -0.0017719977 -0.0017711712][-0.0017688568 -0.0017698994 -0.0017718137 -0.0017725542 -0.0017718646 -0.0017703831 -0.0017689577 -0.0017683408 -0.0017687622 -0.0017698521 -0.0017710084 -0.001771747 -0.0017718425 -0.001771339 -0.0017706259][-0.0017690255 -0.0017695853 -0.0017705199 -0.0017703969 -0.0017692287 -0.0017677298 -0.0017665299 -0.0017664 -0.0017673483 -0.0017688514 -0.0017702137 -0.0017709212 -0.001770985 -0.0017704852 -0.0017699777][-0.0017690168 -0.0017686924 -0.0017688385 -0.0017683354 -0.0017673121 -0.0017663494 -0.0017658733 -0.0017664165 -0.0017676839 -0.001769134 -0.0017701468 -0.0017703708 -0.0017700886 -0.0017695664 -0.0017694015][-0.0017687428 -0.0017676705 -0.0017673097 -0.0017667564 -0.0017661662 -0.0017659796 -0.0017664458 -0.0017676891 -0.0017690379 -0.0017700123 -0.0017702477 -0.0017697732 -0.0017690831 -0.0017686569 -0.0017688705][-0.0017683683 -0.0017667125 -0.0017660789 -0.0017657375 -0.0017657464 -0.0017664523 -0.0017677641 -0.00176943 -0.0017705988 -0.0017708612 -0.0017702661 -0.0017691915 -0.0017683293 -0.0017681411 -0.0017686479][-0.0017683865 -0.0017666111 -0.0017659342 -0.0017659917 -0.0017666983 -0.0017682104 -0.00177006 -0.0017716773 -0.001772268 -0.00177162 -0.0017701973 -0.0017687132 -0.0017679364 -0.0017680718 -0.0017686791]]...]
INFO - root - 2017-12-09 12:34:26.195638: step 23010, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.847 sec/batch; 72h:50m:45s remains)
INFO - root - 2017-12-09 12:34:34.728226: step 23020, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.872 sec/batch; 74h:55m:25s remains)
INFO - root - 2017-12-09 12:34:43.488114: step 23030, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.854 sec/batch; 73h:23m:37s remains)
INFO - root - 2017-12-09 12:34:52.197750: step 23040, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.831 sec/batch; 71h:27m:30s remains)
INFO - root - 2017-12-09 12:35:00.791752: step 23050, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 74h:38m:48s remains)
INFO - root - 2017-12-09 12:35:09.411652: step 23060, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.877 sec/batch; 75h:23m:01s remains)
INFO - root - 2017-12-09 12:35:17.972649: step 23070, loss = 0.81, batch loss = 0.68 (9.4 examples/sec; 0.855 sec/batch; 73h:30m:51s remains)
INFO - root - 2017-12-09 12:35:26.460219: step 23080, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.843 sec/batch; 72h:29m:42s remains)
INFO - root - 2017-12-09 12:35:34.926838: step 23090, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.846 sec/batch; 72h:41m:56s remains)
INFO - root - 2017-12-09 12:35:43.449921: step 23100, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.861 sec/batch; 74h:00m:35s remains)
2017-12-09 12:35:44.304771: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0018005212 -0.0018016283 -0.0018033979 -0.0018051909 -0.0018069732 -0.0018083955 -0.0018096516 -0.0018103468 -0.0018104055 -0.0018107467 -0.0018111668 -0.0018114481 -0.0018116954 -0.0018121524 -0.0018123275][-0.0017905511 -0.0017934582 -0.001798583 -0.0018024883 -0.0018048832 -0.0018065398 -0.00180812 -0.001809191 -0.0018098417 -0.0018105186 -0.0018112204 -0.0018117025 -0.0018120155 -0.001812311 -0.001812538][-0.0017839981 -0.0017860323 -0.0017908028 -0.0017951131 -0.0017991461 -0.0018025097 -0.0018053501 -0.0018072251 -0.0018081948 -0.0018090683 -0.0018101506 -0.0018109208 -0.001811252 -0.0018114836 -0.0018118613][-0.0017699276 -0.0017728637 -0.0017811778 -0.0017873902 -0.0017926466 -0.0017966448 -0.0018003577 -0.0018034603 -0.0018059398 -0.0018075511 -0.0018090254 -0.0018098934 -0.0018104366 -0.0018108902 -0.0018113919][-0.0017561639 -0.0017579172 -0.0017674917 -0.0017756354 -0.0017836086 -0.0017903122 -0.0017960396 -0.0018002879 -0.0018035969 -0.0018060808 -0.0018082137 -0.0018094683 -0.001810329 -0.0018108502 -0.0018112565][-0.0017451665 -0.001744734 -0.0017551447 -0.0017650854 -0.0017754866 -0.0017838747 -0.0017908332 -0.0017967806 -0.0018017795 -0.0018051497 -0.0018075801 -0.001809035 -0.0018099209 -0.001810269 -0.001810595][-0.0017430276 -0.0017414121 -0.0017519837 -0.0017628687 -0.0017742008 -0.0017833713 -0.0017902438 -0.0017963103 -0.0018017533 -0.0018054864 -0.0018080319 -0.0018093764 -0.0018101097 -0.0018102718 -0.0018104904][-0.0017497798 -0.0017466304 -0.0017562374 -0.0017673535 -0.0017789609 -0.0017882462 -0.0017946525 -0.001799983 -0.0018048028 -0.0018083496 -0.0018104347 -0.0018113708 -0.0018118818 -0.001812128 -0.0018123756][-0.0017634571 -0.0017625386 -0.0017704805 -0.0017795347 -0.001789261 -0.0017971561 -0.0018022858 -0.0018065725 -0.0018101222 -0.0018123714 -0.0018134832 -0.0018139208 -0.0018142916 -0.0018141681 -0.0018142553][-0.0017790857 -0.0017811108 -0.0017877267 -0.0017940339 -0.0018007276 -0.0018065695 -0.0018102006 -0.0018126224 -0.0018140796 -0.0018151096 -0.0018154351 -0.0018152279 -0.0018151813 -0.0018148324 -0.0018146137][-0.0017958689 -0.0017978408 -0.0018027525 -0.0018063349 -0.001809808 -0.0018131175 -0.00181492 -0.001815695 -0.0018157137 -0.0018157747 -0.0018157096 -0.0018153454 -0.0018150145 -0.0018144719 -0.0018137895][-0.0018084993 -0.0018089255 -0.0018119249 -0.0018135094 -0.0018146557 -0.0018159146 -0.0018163397 -0.0018159538 -0.0018151072 -0.0018146868 -0.0018142292 -0.0018137506 -0.0018134882 -0.0018129317 -0.0018119485][-0.0018155675 -0.0018152447 -0.0018164609 -0.0018165726 -0.0018165987 -0.0018166626 -0.001816325 -0.0018152262 -0.001813954 -0.0018132459 -0.001812502 -0.0018118563 -0.0018114449 -0.0018107877 -0.0018094108][-0.0018185645 -0.0018178484 -0.0018176528 -0.0018175074 -0.0018174956 -0.0018168587 -0.0018159045 -0.0018142563 -0.0018127523 -0.0018116644 -0.001810469 -0.0018094983 -0.0018086977 -0.001807777 -0.0018062493][-0.0018199394 -0.0018189364 -0.0018181728 -0.0018179362 -0.0018177693 -0.0018167936 -0.0018152018 -0.0018130622 -0.0018111877 -0.0018095999 -0.001807967 -0.0018065053 -0.0018050659 -0.0018038653 -0.0018023645]]...]
INFO - root - 2017-12-09 12:35:53.090094: step 23110, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.886 sec/batch; 76h:10m:30s remains)
INFO - root - 2017-12-09 12:36:01.742099: step 23120, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.853 sec/batch; 73h:18m:49s remains)
INFO - root - 2017-12-09 12:36:10.444171: step 23130, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.873 sec/batch; 74h:59m:56s remains)
INFO - root - 2017-12-09 12:36:19.080136: step 23140, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.866 sec/batch; 74h:27m:22s remains)
INFO - root - 2017-12-09 12:36:27.715691: step 23150, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 74h:41m:47s remains)
INFO - root - 2017-12-09 12:36:36.334165: step 23160, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.877 sec/batch; 75h:20m:57s remains)
INFO - root - 2017-12-09 12:36:44.964063: step 23170, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.850 sec/batch; 73h:02m:42s remains)
INFO - root - 2017-12-09 12:36:53.492393: step 23180, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 74h:15m:48s remains)
INFO - root - 2017-12-09 12:37:02.056391: step 23190, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 74h:34m:04s remains)
INFO - root - 2017-12-09 12:37:10.731149: step 23200, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 74h:41m:37s remains)
2017-12-09 12:37:11.567154: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0017916106 -0.0017922021 -0.0017936374 -0.0017950173 -0.0017959934 -0.0017963026 -0.0017959622 -0.0017950409 -0.0017936658 -0.0017921338 -0.0017906826 -0.001789576 -0.0017887986 -0.0017883275 -0.0017880799][-0.0017925117 -0.0017938218 -0.0017960501 -0.0017980402 -0.0017992844 -0.0017996586 -0.001799242 -0.0017979902 -0.0017961656 -0.0017941367 -0.0017920944 -0.0017903821 -0.0017891228 -0.0017883406 -0.0017879007][-0.0017944832 -0.0017965381 -0.0017991563 -0.0018013184 -0.0018025151 -0.0018028319 -0.0018023038 -0.0018008067 -0.0017988081 -0.0017964928 -0.0017940241 -0.0017916891 -0.0017899282 -0.0017888683 -0.0017882646][-0.0017965797 -0.0017988995 -0.0018014725 -0.0018034994 -0.0018044154 -0.0018045666 -0.0018040347 -0.0018028171 -0.0018010119 -0.0017985983 -0.0017958833 -0.0017931352 -0.0017909503 -0.0017896 -0.0017887821][-0.0017984573 -0.0018007038 -0.0018029962 -0.0018045743 -0.0018051047 -0.0018050765 -0.0018046643 -0.0018039504 -0.0018026209 -0.0018004879 -0.0017977199 -0.001794716 -0.0017922563 -0.0017905663 -0.0017894468][-0.0017997199 -0.0018017736 -0.0018036487 -0.0018047282 -0.0018048466 -0.0018045924 -0.0018042602 -0.0018039765 -0.0018030222 -0.0018011964 -0.0017986351 -0.0017956914 -0.0017931869 -0.0017913359 -0.0017900271][-0.0018002585 -0.0018020301 -0.0018035566 -0.0018042098 -0.0018039454 -0.001803393 -0.00180281 -0.0018026561 -0.0018019264 -0.0018005446 -0.0017985 -0.0017959767 -0.0017936819 -0.0017918476 -0.0017904632][-0.0017998834 -0.0018013347 -0.001802573 -0.0018029534 -0.0018024214 -0.0018014859 -0.001800565 -0.0018002966 -0.001799767 -0.0017988387 -0.001797489 -0.0017957223 -0.0017938795 -0.001792205 -0.001790816][-0.0017988851 -0.0017999313 -0.0018010574 -0.0018013542 -0.0018006628 -0.0017994237 -0.0017982232 -0.0017978541 -0.0017974072 -0.0017968158 -0.0017960502 -0.0017949821 -0.0017936936 -0.0017923177 -0.0017910211][-0.0017975481 -0.001798322 -0.0017995549 -0.0017999338 -0.0017992669 -0.0017979065 -0.0017966634 -0.0017961505 -0.0017956301 -0.0017951614 -0.0017947474 -0.0017941613 -0.0017932958 -0.001792194 -0.0017910108][-0.0017962827 -0.001796781 -0.0017979315 -0.001798543 -0.0017981494 -0.0017970508 -0.001795933 -0.0017953467 -0.0017946031 -0.0017940016 -0.0017937192 -0.0017933472 -0.0017926705 -0.0017917771 -0.0017907572][-0.0017946267 -0.0017948254 -0.0017957685 -0.0017964785 -0.0017963503 -0.0017956733 -0.0017948177 -0.0017941829 -0.0017933676 -0.0017927742 -0.001792607 -0.0017923849 -0.0017918549 -0.0017911752 -0.0017903429][-0.0017926504 -0.0017925322 -0.0017930634 -0.0017935905 -0.0017935922 -0.0017932763 -0.001792843 -0.0017924086 -0.0017918217 -0.0017914157 -0.0017913493 -0.0017912184 -0.0017908545 -0.0017903815 -0.0017897383][-0.0017910441 -0.0017904877 -0.0017906158 -0.0017909559 -0.0017910561 -0.0017910317 -0.0017909664 -0.0017908881 -0.0017906546 -0.001790424 -0.0017903602 -0.0017902303 -0.0017899398 -0.0017895636 -0.0017890548][-0.0017902196 -0.0017893934 -0.0017892524 -0.0017895453 -0.0017897617 -0.0017898942 -0.0017900022 -0.0017900859 -0.0017899899 -0.0017898194 -0.0017897032 -0.0017894956 -0.0017891984 -0.0017888416 -0.0017884301]]...]
INFO - root - 2017-12-09 12:37:20.288907: step 23210, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.866 sec/batch; 74h:23m:33s remains)
INFO - root - 2017-12-09 12:37:28.977813: step 23220, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.887 sec/batch; 76h:11m:05s remains)
INFO - root - 2017-12-09 12:37:37.819495: step 23230, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.873 sec/batch; 75h:00m:28s remains)
INFO - root - 2017-12-09 12:37:46.530414: step 23240, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.897 sec/batch; 77h:03m:00s remains)
INFO - root - 2017-12-09 12:37:55.140085: step 23250, loss = 0.82, batch loss = 0.69 (10.7 examples/sec; 0.747 sec/batch; 64h:08m:57s remains)
INFO - root - 2017-12-09 12:38:03.663745: step 23260, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.874 sec/batch; 75h:07m:04s remains)
INFO - root - 2017-12-09 12:38:12.276758: step 23270, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.866 sec/batch; 74h:20m:54s remains)
INFO - root - 2017-12-09 12:38:20.925229: step 23280, loss = 0.83, batch loss = 0.70 (9.3 examples/sec; 0.858 sec/batch; 73h:41m:05s remains)
INFO - root - 2017-12-09 12:38:29.638354: step 23290, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.884 sec/batch; 75h:55m:09s remains)
INFO - root - 2017-12-09 12:38:38.217706: step 23300, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 74h:50m:24s remains)
2017-12-09 12:38:39.084195: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0018080559 -0.0018073209 -0.0018072273 -0.001807395 -0.0018078187 -0.0018084317 -0.0018089897 -0.0018094301 -0.0018095592 -0.0018096033 -0.0018094369 -0.0018090025 -0.0018086593 -0.0018085309 -0.0018087537][-0.0018088478 -0.0018083845 -0.0018086816 -0.0018092272 -0.0018099108 -0.0018107068 -0.0018114584 -0.0018120076 -0.0018120592 -0.0018117571 -0.0018113055 -0.0018105044 -0.0018097244 -0.0018091273 -0.0018090532][-0.0018104862 -0.0018106322 -0.0018115052 -0.0018125488 -0.0018134915 -0.001814386 -0.0018151749 -0.0018156804 -0.0018155496 -0.0018148225 -0.0018140228 -0.0018128116 -0.0018115635 -0.0018104271 -0.0018099709][-0.0018125258 -0.0018131621 -0.0018143079 -0.0018154049 -0.0018162219 -0.0018169383 -0.00181751 -0.0018177979 -0.0018175346 -0.0018166883 -0.0018158108 -0.0018144688 -0.0018130407 -0.0018117087 -0.00181105][-0.0018145342 -0.0018153271 -0.0018163166 -0.0018169112 -0.0018171833 -0.0018172527 -0.0018173888 -0.0018173859 -0.0018171613 -0.0018165667 -0.0018160543 -0.0018151371 -0.001814083 -0.0018131391 -0.0018125798][-0.0018161311 -0.0018166624 -0.0018170131 -0.0018168421 -0.0018161841 -0.0018152543 -0.0018145867 -0.0018141581 -0.0018139364 -0.0018136256 -0.0018137092 -0.0018136523 -0.001813514 -0.0018134863 -0.0018136365][-0.0018171867 -0.0018170632 -0.0018165115 -0.0018152941 -0.0018134973 -0.0018115006 -0.0018100041 -0.0018091219 -0.0018088592 -0.0018090137 -0.0018099917 -0.0018111741 -0.001812216 -0.0018134052 -0.0018144973][-0.0018179954 -0.0018171342 -0.0018156737 -0.0018134426 -0.0018107794 -0.0018082137 -0.0018064751 -0.0018053335 -0.0018050072 -0.0018057111 -0.0018076235 -0.0018098041 -0.0018119884 -0.0018141445 -0.0018158739][-0.0018184971 -0.0018170265 -0.001814915 -0.0018121367 -0.0018091764 -0.0018064626 -0.0018046995 -0.001803458 -0.0018030673 -0.0018041398 -0.0018067083 -0.0018095062 -0.0018122875 -0.0018149056 -0.0018168358][-0.0018187829 -0.0018171585 -0.0018151677 -0.0018128997 -0.0018104832 -0.0018083515 -0.0018066396 -0.0018051218 -0.0018045547 -0.0018053771 -0.0018077118 -0.0018103917 -0.0018130171 -0.0018154503 -0.0018172197][-0.0018181999 -0.0018167755 -0.0018153675 -0.0018139131 -0.0018124747 -0.0018112648 -0.0018102318 -0.0018091033 -0.0018087124 -0.0018093142 -0.0018110246 -0.001813003 -0.0018147519 -0.0018163068 -0.0018172088][-0.0018164315 -0.0018152959 -0.0018146596 -0.0018140613 -0.001813516 -0.0018131674 -0.0018128835 -0.0018125013 -0.0018125754 -0.0018132814 -0.0018145046 -0.0018155077 -0.0018161606 -0.001816703 -0.0018166709][-0.001814477 -0.0018137879 -0.0018137429 -0.00181393 -0.0018141258 -0.0018144401 -0.00181469 -0.0018146459 -0.001814832 -0.0018153872 -0.001816334 -0.0018169126 -0.0018170596 -0.0018169368 -0.0018163155][-0.0018133274 -0.001813032 -0.0018133526 -0.0018139507 -0.0018145387 -0.0018152361 -0.0018156056 -0.0018156203 -0.0018158067 -0.0018161905 -0.0018168084 -0.0018171252 -0.0018170702 -0.0018168128 -0.0018161812][-0.0018130183 -0.0018129736 -0.0018134615 -0.001814217 -0.001815081 -0.0018159166 -0.0018163151 -0.0018163121 -0.0018163605 -0.0018166295 -0.001816983 -0.0018170624 -0.0018168513 -0.0018165251 -0.0018159549]]...]
INFO - root - 2017-12-09 12:38:47.822547: step 23310, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.896 sec/batch; 76h:58m:29s remains)
INFO - root - 2017-12-09 12:38:56.358270: step 23320, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.880 sec/batch; 75h:37m:08s remains)
INFO - root - 2017-12-09 12:39:05.082619: step 23330, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.891 sec/batch; 76h:31m:06s remains)
INFO - root - 2017-12-09 12:39:13.918123: step 23340, loss = 0.82, batch loss = 0.69 (8.8 examples/sec; 0.913 sec/batch; 78h:24m:50s remains)
INFO - root - 2017-12-09 12:39:22.557326: step 23350, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.853 sec/batch; 73h:15m:10s remains)
INFO - root - 2017-12-09 12:39:31.080166: step 23360, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.875 sec/batch; 75h:09m:46s remains)
INFO - root - 2017-12-09 12:39:39.632711: step 23370, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.830 sec/batch; 71h:14m:25s remains)
INFO - root - 2017-12-09 12:39:48.159564: step 23380, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.863 sec/batch; 74h:05m:55s remains)
INFO - root - 2017-12-09 12:39:56.621536: step 23390, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.856 sec/batch; 73h:29m:16s remains)
INFO - root - 2017-12-09 12:40:05.147801: step 23400, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.892 sec/batch; 76h:33m:09s remains)
2017-12-09 12:40:06.013494: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.064370476 0.083497286 0.11121077 0.14762916 0.18821518 0.22690241 0.25642139 0.27015734 0.26459906 0.23945217 0.19952257 0.15123834 0.1020404 0.05914966 0.027507626][0.050265409 0.070822164 0.10101097 0.13980578 0.1824265 0.22196357 0.25007746 0.26134202 0.25251573 0.22486605 0.18356381 0.1357047 0.089213461 0.049758431 0.021610714][0.03629598 0.056855153 0.087315485 0.12581126 0.16799223 0.20676219 0.2335138 0.24278392 0.23189829 0.20309004 0.16190434 0.11645795 0.074125804 0.039483659 0.015705828][0.028022572 0.047019143 0.075848177 0.11258076 0.15287979 0.18958911 0.21509416 0.22337344 0.2118907 0.18327 0.14334254 0.10056642 0.061821967 0.031441737 0.011453516][0.024402887 0.042001039 0.068732783 0.10286526 0.14070554 0.17521548 0.198816 0.20570706 0.19380395 0.16584489 0.12773409 0.087776795 0.052335944 0.02546626 0.0084535507][0.022067837 0.037933853 0.062712766 0.09447594 0.12976164 0.16223283 0.18469176 0.19095023 0.17883334 0.15166211 0.11525671 0.077754579 0.045056529 0.02093228 0.00628087][0.017957598 0.031826627 0.053933911 0.082780987 0.11547706 0.14593925 0.16734469 0.17363617 0.16226502 0.13661695 0.10257108 0.068065673 0.038253672 0.016761638 0.0043151951][0.012141802 0.023364503 0.042045034 0.066959172 0.09583503 0.12373155 0.14456151 0.15195808 0.14295636 0.12070365 0.090506017 0.059646536 0.032829922 0.013648855 0.0029770047][0.0073645534 0.015709411 0.030355651 0.050576389 0.074581131 0.098569714 0.11750574 0.12572658 0.11985113 0.10225197 0.077439941 0.05142552 0.028290182 0.011447653 0.0021874271][0.0048362883 0.010886634 0.021694545 0.036878865 0.05523603 0.074148118 0.089720562 0.097373009 0.09409184 0.081367329 0.062536523 0.042060222 0.02330637 0.009350922 0.0016008394][0.0029052543 0.0072414931 0.014875014 0.025545679 0.038491752 0.05219597 0.063867 0.069991209 0.06824974 0.059532907 0.046239264 0.031294629 0.017252527 0.0066611525 0.00074768416][0.00088304584 0.0035868282 0.008272659 0.014925017 0.0231909 0.032175563 0.040070303 0.044473164 0.043833829 0.038467564 0.029892221 0.020043161 0.010705149 0.0037013674 -0.0001709254][-0.00078956829 0.00044191326 0.0027560261 0.0062551834 0.01085342 0.016098589 0.020973157 0.023925535 0.02401907 0.021269973 0.016404103 0.010627352 0.0051396429 0.0011683239 -0.0009321135][-0.0016000856 -0.0012607679 -0.0005426089 0.00073569966 0.0026819846 0.0050586131 0.00734076 0.0088461153 0.0091596236 0.0081322845 0.0060073528 0.0034086369 0.00098974211 -0.00066320517 -0.0014780329][-0.0017359091 -0.0017281333 -0.0016494857 -0.0013926507 -0.00087596185 -0.00015305402 0.00062767393 0.0012128251 0.0014571159 0.0012607125 0.00064700912 -0.00018315658 -0.00097232958 -0.001458433 -0.0016727594]]...]
INFO - root - 2017-12-09 12:40:14.622085: step 23410, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.872 sec/batch; 74h:51m:24s remains)
INFO - root - 2017-12-09 12:40:23.337716: step 23420, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.891 sec/batch; 76h:30m:36s remains)
INFO - root - 2017-12-09 12:40:31.860039: step 23430, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.846 sec/batch; 72h:39m:03s remains)
INFO - root - 2017-12-09 12:40:40.336672: step 23440, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.856 sec/batch; 73h:31m:29s remains)
INFO - root - 2017-12-09 12:40:48.991672: step 23450, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.877 sec/batch; 75h:15m:27s remains)
INFO - root - 2017-12-09 12:40:57.464945: step 23460, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.866 sec/batch; 74h:21m:17s remains)
INFO - root - 2017-12-09 12:41:06.055839: step 23470, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.844 sec/batch; 72h:28m:26s remains)
INFO - root - 2017-12-09 12:41:14.638680: step 23480, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.853 sec/batch; 73h:12m:53s remains)
INFO - root - 2017-12-09 12:41:23.217896: step 23490, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.887 sec/batch; 76h:10m:38s remains)
INFO - root - 2017-12-09 12:41:31.839547: step 23500, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.873 sec/batch; 74h:55m:20s remains)
2017-12-09 12:41:32.668651: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0018140615 -0.0018136549 -0.0018132525 -0.0018111708 -0.0018045006 -0.0017934934 -0.0017836352 -0.0017781595 -0.0017766819 -0.0017798553 -0.0017865244 -0.0017950521 -0.0018016173 -0.0018054878 -0.0018073046][-0.0018148599 -0.0018127706 -0.001809183 -0.0018002696 -0.0017816728 -0.0017560617 -0.0017362267 -0.0017269437 -0.0017266242 -0.0017373236 -0.0017539925 -0.0017719641 -0.0017861942 -0.0017968711 -0.0018027652][-0.0018114656 -0.0018023173 -0.0017876042 -0.0017610042 -0.0017193346 -0.001670426 -0.0016360262 -0.0016222232 -0.0016258822 -0.0016488056 -0.0016814098 -0.0017160018 -0.0017464499 -0.0017726797 -0.0017902666][-0.0017836364 -0.0017520906 -0.001707964 -0.0016428852 -0.0015591733 -0.0014738126 -0.0014174724 -0.0013970378 -0.0014137777 -0.0014655793 -0.0015369456 -0.0016113381 -0.0016764459 -0.0017311559 -0.0017691469][-0.0016753362 -0.0015907476 -0.0014918216 -0.0013657075 -0.0012216045 -0.0010893325 -0.0010096654 -0.00099263643 -0.0010507819 -0.0011686254 -0.0013229254 -0.0014756359 -0.001598086 -0.0016900811 -0.001750716][-0.0014122103 -0.0012404928 -0.0010724992 -0.000879962 -0.00067446288 -0.000504083 -0.00042577751 -0.00045058224 -0.00059598649 -0.0008223328 -0.0011012059 -0.0013622958 -0.001551985 -0.0016749783 -0.0017466213][-0.000979211 -0.00071274897 -0.0004989818 -0.00027900364 -5.3021475e-05 0.00010948209 0.00012940809 2.9196963e-06 -0.00026542833 -0.00060826703 -0.00099730748 -0.0013392775 -0.0015669263 -0.0016949404 -0.0017588692][-0.00051166571 -0.00019078015 1.0226155e-05 0.00018738478 0.00036649487 0.0004583319 0.000365476 0.00010252686 -0.00027659629 -0.0006802571 -0.0010908036 -0.0014271861 -0.0016329221 -0.0017343006 -0.0017771119][-0.00023113051 7.872167e-05 0.0002090662 0.00028221135 0.00035508734 0.00033851375 0.00013445842 -0.00022230274 -0.00063494977 -0.0010045184 -0.0013306264 -0.0015744589 -0.0017108986 -0.001770044 -0.0017911794][-0.00027625042 -2.7579954e-05 1.0306481e-05 -3.6196085e-05 -8.3016115e-05 -0.00019068678 -0.00043864816 -0.00078601204 -0.0011263071 -0.00138427 -0.0015763466 -0.0017028457 -0.0017671679 -0.0017916204 -0.0017988267][-0.0006120645 -0.00044210325 -0.00047438347 -0.00060203648 -0.00072607631 -0.0008649847 -0.0010694878 -0.0013116546 -0.0015163878 -0.0016482139 -0.0017293636 -0.0017742899 -0.0017943134 -0.0018007369 -0.0018020293][-0.0010723303 -0.00097223232 -0.0010273538 -0.0011635907 -0.0012906108 -0.0013999464 -0.0015178933 -0.0016358693 -0.0017222057 -0.0017687134 -0.0017907482 -0.0017996649 -0.0018025826 -0.0018031454 -0.0018030539][-0.0014679107 -0.0014191588 -0.0014592471 -0.0015488678 -0.0016280781 -0.0016854866 -0.0017317043 -0.0017692589 -0.0017920897 -0.0018015223 -0.0018040172 -0.001804063 -0.001803731 -0.0018033403 -0.0018028597][-0.0016996148 -0.0016806517 -0.0016968681 -0.0017337129 -0.0017646987 -0.0017838971 -0.0017949119 -0.0018013505 -0.0018041927 -0.0018048042 -0.0018045657 -0.001803773 -0.0018032292 -0.001802886 -0.0018024456][-0.0017880305 -0.0017822541 -0.0017844976 -0.0017921173 -0.0017983322 -0.001801634 -0.0018028917 -0.0018033236 -0.0018037383 -0.0018039756 -0.0018039244 -0.0018033818 -0.0018028552 -0.0018025028 -0.0018021818]]...]
INFO - root - 2017-12-09 12:41:41.342930: step 23510, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 74h:31m:20s remains)
INFO - root - 2017-12-09 12:41:50.004336: step 23520, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.879 sec/batch; 75h:26m:37s remains)
INFO - root - 2017-12-09 12:41:58.594091: step 23530, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.878 sec/batch; 75h:21m:06s remains)
INFO - root - 2017-12-09 12:42:07.303303: step 23540, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.860 sec/batch; 73h:48m:57s remains)
INFO - root - 2017-12-09 12:42:16.012282: step 23550, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.852 sec/batch; 73h:09m:11s remains)
INFO - root - 2017-12-09 12:42:24.477902: step 23560, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.857 sec/batch; 73h:34m:46s remains)
INFO - root - 2017-12-09 12:42:32.898463: step 23570, loss = 0.82, batch loss = 0.69 (9.8 examples/sec; 0.818 sec/batch; 70h:12m:12s remains)
INFO - root - 2017-12-09 12:42:41.444341: step 23580, loss = 0.83, batch loss = 0.70 (9.4 examples/sec; 0.848 sec/batch; 72h:47m:28s remains)
INFO - root - 2017-12-09 12:42:49.979503: step 23590, loss = 0.82, batch loss = 0.69 (11.0 examples/sec; 0.727 sec/batch; 62h:22m:12s remains)
INFO - root - 2017-12-09 12:42:58.617356: step 23600, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 73h:59m:02s remains)
2017-12-09 12:42:59.555478: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0017887515 -0.0017867308 -0.0017866263 -0.0017875719 -0.0017894467 -0.0017916345 -0.0017936241 -0.0017955712 -0.0017969728 -0.0017981194 -0.0017987154 -0.0017989695 -0.0017991229 -0.0017993493 -0.0018000144][-0.0017884495 -0.0017869188 -0.0017871571 -0.0017888288 -0.0017916055 -0.001794675 -0.0017972268 -0.0017994127 -0.0018007156 -0.0018014677 -0.0018014094 -0.0018011719 -0.0018010418 -0.0018012755 -0.0018021115][-0.0017887896 -0.0017879631 -0.0017888356 -0.001791349 -0.0017948729 -0.0017986214 -0.0018014504 -0.0018034371 -0.0018042796 -0.0018042127 -0.0018033388 -0.0018022979 -0.0018017514 -0.001801877 -0.0018028192][-0.0017891609 -0.00178898 -0.00179062 -0.0017938714 -0.001798028 -0.0018022548 -0.0018050739 -0.0018066072 -0.0018065379 -0.0018052169 -0.0018029879 -0.0018010936 -0.0018001013 -0.0018000142 -0.0018008372][-0.0017895568 -0.0017898199 -0.0017919377 -0.0017958136 -0.0018004759 -0.0018049286 -0.0018075728 -0.001808501 -0.0018072795 -0.0018044199 -0.0018006141 -0.0017974849 -0.0017958982 -0.0017955955 -0.0017963253][-0.0017897458 -0.0017903593 -0.0017929126 -0.0017971771 -0.0018020587 -0.0018065018 -0.0018089861 -0.0018094766 -0.0018075086 -0.0018033871 -0.0017982431 -0.0017938195 -0.0017914493 -0.0017906657 -0.0017911061][-0.0017897106 -0.0017907997 -0.0017938673 -0.0017983125 -0.0018031829 -0.0018074871 -0.0018097544 -0.0018098656 -0.0018074126 -0.0018029298 -0.0017975186 -0.0017924894 -0.001789422 -0.0017881322 -0.0017882679][-0.0017896151 -0.0017911001 -0.0017946198 -0.0017994047 -0.0018042799 -0.001808406 -0.0018103598 -0.0018104138 -0.0018081754 -0.0018042597 -0.0017994489 -0.0017946408 -0.0017915204 -0.0017900009 -0.0017896441][-0.0017893707 -0.0017911585 -0.0017951619 -0.0018003279 -0.0018051617 -0.0018091063 -0.0018110555 -0.0018112243 -0.0018094976 -0.0018067604 -0.001803132 -0.001799086 -0.0017962785 -0.0017947106 -0.0017942217][-0.001789157 -0.0017912104 -0.0017956691 -0.0018010475 -0.0018057463 -0.001809684 -0.0018119714 -0.0018126534 -0.0018115767 -0.001809882 -0.0018075728 -0.0018047752 -0.0018025995 -0.0018011101 -0.0018003626][-0.0017885229 -0.0017907706 -0.0017957329 -0.0018013166 -0.0018059792 -0.0018099177 -0.0018125076 -0.0018134849 -0.0018129458 -0.0018121876 -0.0018112914 -0.0018098099 -0.0018083976 -0.0018072426 -0.0018064597][-0.0017876636 -0.0017898072 -0.0017950366 -0.0018009035 -0.0018057567 -0.0018096509 -0.0018123949 -0.0018135632 -0.0018134165 -0.0018133679 -0.001813598 -0.0018131104 -0.0018122739 -0.0018116047 -0.0018110037][-0.0017865205 -0.0017881868 -0.0017932262 -0.0017991933 -0.0018044641 -0.001808469 -0.0018114235 -0.0018128291 -0.0018130963 -0.001813615 -0.001814275 -0.0018144406 -0.0018141866 -0.0018138894 -0.0018134065][-0.0017854205 -0.0017864739 -0.0017910557 -0.0017967818 -0.0018020428 -0.0018061741 -0.0018092218 -0.0018108438 -0.0018115627 -0.001812265 -0.001812816 -0.001813006 -0.001812903 -0.0018129032 -0.0018127499][-0.0017848038 -0.0017849325 -0.0017885196 -0.0017936082 -0.0017986264 -0.0018028606 -0.001806007 -0.0018079678 -0.0018090829 -0.0018099443 -0.0018103652 -0.0018104493 -0.0018103265 -0.0018102427 -0.0018097988]]...]
INFO - root - 2017-12-09 12:43:08.188869: step 23610, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.839 sec/batch; 71h:57m:23s remains)
INFO - root - 2017-12-09 12:43:16.807909: step 23620, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.874 sec/batch; 74h:57m:08s remains)
INFO - root - 2017-12-09 12:43:25.372636: step 23630, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.849 sec/batch; 72h:53m:03s remains)
INFO - root - 2017-12-09 12:43:34.031811: step 23640, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.859 sec/batch; 73h:41m:08s remains)
INFO - root - 2017-12-09 12:43:42.664004: step 23650, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 74h:45m:57s remains)
INFO - root - 2017-12-09 12:43:51.190446: step 23660, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.863 sec/batch; 74h:04m:16s remains)
INFO - root - 2017-12-09 12:43:59.645267: step 23670, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.842 sec/batch; 72h:14m:48s remains)
INFO - root - 2017-12-09 12:44:08.268765: step 23680, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.863 sec/batch; 74h:02m:49s remains)
INFO - root - 2017-12-09 12:44:16.928295: step 23690, loss = 0.82, batch loss = 0.69 (9.7 examples/sec; 0.828 sec/batch; 71h:02m:20s remains)
INFO - root - 2017-12-09 12:44:25.171087: step 23700, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.861 sec/batch; 73h:50m:16s remains)
2017-12-09 12:44:26.079600: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.032781575 0.045757633 0.060850289 0.075300552 0.086439662 0.094162494 0.099796891 0.10584006 0.11388753 0.1253462 0.1380887 0.1503554 0.15806293 0.15977596 0.15361415][0.026225274 0.035809297 0.047607191 0.059994448 0.071740694 0.081776835 0.090756953 0.10037997 0.11086057 0.12263077 0.1340746 0.14402431 0.1494184 0.14929292 0.14176036][0.023111923 0.029422302 0.038033422 0.049409453 0.062713794 0.076629028 0.089855962 0.10350051 0.11659697 0.12881236 0.13852589 0.14566411 0.14797105 0.14481543 0.13519453][0.023039518 0.028253615 0.036491677 0.048487689 0.063991889 0.081967622 0.099917516 0.11816645 0.13324609 0.14575069 0.15429287 0.15880883 0.15829995 0.15240532 0.14112934][0.024749381 0.030866288 0.040507521 0.054895353 0.074013524 0.09631484 0.11821518 0.13959274 0.15652858 0.169838 0.17760524 0.18115903 0.17873216 0.1713319 0.15897718][0.026427928 0.034367118 0.046508055 0.064008288 0.086717926 0.11253864 0.13743161 0.16131982 0.17947872 0.19405106 0.20203704 0.20589699 0.20319094 0.19569707 0.18337248][0.028916392 0.038651634 0.053172268 0.073411942 0.098655246 0.1266128 0.15290916 0.17773041 0.19638631 0.21229842 0.22230309 0.22804677 0.22701955 0.22107045 0.20927925][0.031787734 0.043778535 0.060872886 0.082592383 0.10878777 0.136786 0.16260993 0.1862852 0.20373201 0.21944933 0.2303718 0.2389186 0.24041048 0.23738682 0.22781318][0.035278462 0.049355239 0.068518221 0.0911745 0.1170191 0.14369249 0.16819778 0.18968394 0.20564829 0.22074571 0.23203774 0.24187566 0.24502906 0.24438708 0.23681338][0.035977345 0.051195413 0.070905492 0.093433715 0.11844488 0.14315473 0.16580088 0.18533672 0.20060652 0.21436797 0.22495469 0.23486112 0.23869343 0.24027626 0.23506384][0.033632204 0.048119552 0.066304684 0.08732637 0.11045705 0.13321294 0.15431136 0.1729774 0.18751317 0.20044218 0.21058373 0.21945651 0.2231946 0.22572614 0.22270824][0.02901279 0.041620564 0.057121094 0.075218521 0.095071927 0.11527777 0.13462675 0.15220687 0.16635811 0.17876726 0.18798347 0.19576438 0.19964446 0.2039593 0.20354143][0.022616165 0.03246332 0.04434678 0.058632415 0.074956879 0.09212698 0.10946066 0.12558125 0.13902372 0.15062349 0.15844673 0.16537812 0.16988222 0.17581986 0.17918673][0.014909266 0.021667944 0.0299648 0.040506214 0.053122669 0.067345612 0.082515761 0.096805088 0.10941067 0.12018209 0.12732717 0.13364783 0.13844652 0.14586799 0.1526019][0.0072863577 0.01120769 0.016254686 0.02343015 0.032784861 0.0437766 0.056284674 0.06840276 0.079170056 0.088381819 0.094797 0.10072967 0.10627008 0.11514904 0.12461942]]...]
INFO - root - 2017-12-09 12:44:34.738767: step 23710, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 73h:57m:12s remains)
INFO - root - 2017-12-09 12:44:43.352528: step 23720, loss = 0.83, batch loss = 0.70 (9.5 examples/sec; 0.841 sec/batch; 72h:10m:21s remains)
INFO - root - 2017-12-09 12:44:52.110873: step 23730, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.885 sec/batch; 75h:51m:51s remains)
INFO - root - 2017-12-09 12:45:00.775885: step 23740, loss = 0.82, batch loss = 0.69 (8.8 examples/sec; 0.905 sec/batch; 77h:37m:01s remains)
INFO - root - 2017-12-09 12:45:09.485473: step 23750, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.893 sec/batch; 76h:32m:47s remains)
INFO - root - 2017-12-09 12:45:18.083887: step 23760, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.855 sec/batch; 73h:19m:20s remains)
INFO - root - 2017-12-09 12:45:26.804230: step 23770, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.859 sec/batch; 73h:39m:56s remains)
INFO - root - 2017-12-09 12:45:35.512997: step 23780, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.877 sec/batch; 75h:13m:30s remains)
INFO - root - 2017-12-09 12:45:44.224730: step 23790, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.853 sec/batch; 73h:06m:52s remains)
INFO - root - 2017-12-09 12:45:52.805588: step 23800, loss = 0.82, batch loss = 0.69 (9.8 examples/sec; 0.820 sec/batch; 70h:18m:58s remains)
2017-12-09 12:45:53.682029: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0017975513 -0.0017979324 -0.0017985611 -0.0017991224 -0.0018001082 -0.0018018777 -0.0018047489 -0.0018087185 -0.0018129669 -0.0018163052 -0.0018184329 -0.0018195448 -0.001819818 -0.0018196983 -0.0018195086][-0.0017905391 -0.0017903203 -0.0017907874 -0.0017916552 -0.0017932733 -0.0017958814 -0.0017997514 -0.0018048646 -0.0018102896 -0.0018146378 -0.0018174697 -0.0018188939 -0.001819249 -0.0018189707 -0.0018184809][-0.001784927 -0.0017840834 -0.0017844547 -0.0017857153 -0.0017880555 -0.0017914404 -0.0017961054 -0.0018020737 -0.0018082819 -0.0018131423 -0.0018161681 -0.0018176121 -0.0018177754 -0.0018170753 -0.0018161623][-0.0017807853 -0.0017797704 -0.0017801808 -0.0017819314 -0.0017849151 -0.0017889021 -0.0017939149 -0.0018002233 -0.0018067111 -0.0018115784 -0.0018144649 -0.0018158905 -0.0018160993 -0.0018152252 -0.0018139185][-0.0017785964 -0.0017773671 -0.001777829 -0.0017799067 -0.0017832469 -0.0017875023 -0.0017925609 -0.0017987045 -0.0018051022 -0.0018097862 -0.001812571 -0.0018140324 -0.0018142222 -0.0018130552 -0.0018110983][-0.0017775842 -0.0017763838 -0.0017768836 -0.00177911 -0.0017824977 -0.0017866507 -0.0017913658 -0.0017969528 -0.0018028899 -0.0018074262 -0.0018103616 -0.0018119705 -0.001812169 -0.0018107115 -0.0018080913][-0.0017770777 -0.0017757597 -0.0017762833 -0.0017783832 -0.0017814246 -0.0017850781 -0.0017891066 -0.0017939343 -0.0017992781 -0.0018035833 -0.0018067053 -0.0018085788 -0.0018088232 -0.0018071862 -0.0018040075][-0.0017767524 -0.0017754316 -0.0017758474 -0.0017775656 -0.0017800396 -0.0017830166 -0.0017863294 -0.0017903351 -0.0017949783 -0.0017989583 -0.0018020914 -0.0018041348 -0.0018044437 -0.0018027478 -0.0017993194][-0.0017765367 -0.0017752119 -0.0017754784 -0.0017766458 -0.0017783367 -0.0017804893 -0.0017829768 -0.0017861187 -0.0017900225 -0.0017937255 -0.0017968763 -0.0017990752 -0.0017996033 -0.0017981868 -0.0017950026][-0.0017763865 -0.0017751389 -0.0017753718 -0.0017761576 -0.0017772656 -0.0017787422 -0.0017805905 -0.001782956 -0.0017859888 -0.0017891309 -0.0017919728 -0.0017938917 -0.0017942844 -0.0017930192 -0.0017902678][-0.0017763182 -0.0017751078 -0.0017752749 -0.0017758687 -0.0017766905 -0.0017777921 -0.0017792416 -0.0017810952 -0.0017833194 -0.0017856435 -0.0017876951 -0.0017889529 -0.0017889999 -0.0017879104 -0.0017858079][-0.0017764362 -0.0017751514 -0.0017752553 -0.0017756242 -0.0017761925 -0.0017770171 -0.0017781522 -0.0017795663 -0.0017811217 -0.0017826156 -0.0017838231 -0.0017845083 -0.0017844306 -0.0017836579 -0.0017823493][-0.0017767677 -0.0017754844 -0.0017754617 -0.0017756282 -0.0017759824 -0.0017765878 -0.0017774664 -0.0017785252 -0.0017796106 -0.0017805446 -0.001781154 -0.0017814103 -0.0017812734 -0.0017808063 -0.0017801046][-0.0017772651 -0.0017759558 -0.0017759405 -0.0017760788 -0.0017763656 -0.0017768607 -0.0017775398 -0.0017783138 -0.0017790744 -0.0017796897 -0.0017800612 -0.0017801693 -0.0017800813 -0.0017798331 -0.0017794906][-0.001777707 -0.0017764668 -0.0017765281 -0.0017767766 -0.0017771532 -0.0017776908 -0.0017782947 -0.0017788636 -0.0017793608 -0.001779716 -0.0017799002 -0.0017799193 -0.0017798537 -0.0017797396 -0.0017795898]]...]
INFO - root - 2017-12-09 12:46:02.292946: step 23810, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.887 sec/batch; 76h:02m:33s remains)
INFO - root - 2017-12-09 12:46:11.087364: step 23820, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.889 sec/batch; 76h:12m:26s remains)
INFO - root - 2017-12-09 12:46:19.849406: step 23830, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.896 sec/batch; 76h:48m:02s remains)
INFO - root - 2017-12-09 12:46:28.555687: step 23840, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.881 sec/batch; 75h:31m:07s remains)
INFO - root - 2017-12-09 12:46:37.118911: step 23850, loss = 0.81, batch loss = 0.68 (9.3 examples/sec; 0.861 sec/batch; 73h:51m:11s remains)
INFO - root - 2017-12-09 12:46:45.565734: step 23860, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.857 sec/batch; 73h:30m:15s remains)
INFO - root - 2017-12-09 12:46:54.230348: step 23870, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 74h:38m:11s remains)
INFO - root - 2017-12-09 12:47:02.973880: step 23880, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.841 sec/batch; 72h:04m:59s remains)
INFO - root - 2017-12-09 12:47:11.660408: step 23890, loss = 0.82, batch loss = 0.69 (9.7 examples/sec; 0.828 sec/batch; 70h:59m:57s remains)
INFO - root - 2017-12-09 12:47:20.074739: step 23900, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.865 sec/batch; 74h:06m:53s remains)
2017-12-09 12:47:20.962837: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0018135312 -0.0018130164 -0.001813733 -0.001814566 -0.001815407 -0.0018157345 -0.0018156663 -0.0018157319 -0.0018162305 -0.0018173005 -0.0018184319 -0.0018191378 -0.0018193788 -0.0018194284 -0.0018193143][-0.0018129967 -0.0018123501 -0.0018129908 -0.0018136409 -0.0018142973 -0.0018146464 -0.0018145951 -0.001814482 -0.0018149483 -0.0018161549 -0.0018178098 -0.0018193159 -0.0018200947 -0.0018205392 -0.0018206417][-0.001812835 -0.0018122187 -0.0018128639 -0.0018134477 -0.0018139046 -0.0018141038 -0.0018142397 -0.001814121 -0.0018140365 -0.0018136054 -0.0018137724 -0.0018153389 -0.0018178616 -0.0018202208 -0.0018215972][-0.0018125388 -0.0018119959 -0.0018126479 -0.001813141 -0.0018135905 -0.0018136972 -0.0018137642 -0.0018123818 -0.0018048738 -0.0017895148 -0.0017731753 -0.0017676064 -0.0017783165 -0.0017966067 -0.0018118372][-0.0018122473 -0.0018117643 -0.0018123088 -0.001812732 -0.0018130859 -0.0018130961 -0.0018130059 -0.0018074986 -0.0017805108 -0.0017276881 -0.0016688441 -0.0016433017 -0.0016698566 -0.0017260772 -0.0017774643][-0.0018119498 -0.0018115735 -0.0018121774 -0.001812562 -0.0018130044 -0.0018129743 -0.0018125919 -0.001795307 -0.0017245478 -0.0015883683 -0.0014396427 -0.0013724971 -0.0014305289 -0.0015666428 -0.0016974313][-0.0018116444 -0.0018113736 -0.0018121839 -0.0018126657 -0.0018133069 -0.0018136495 -0.001812669 -0.001773557 -0.0016321 -0.0013689474 -0.0010897683 -0.00096746429 -0.0010764604 -0.0013326479 -0.0015797949][-0.0018115661 -0.0018111913 -0.001812089 -0.0018128159 -0.0018136513 -0.0018141466 -0.0018117663 -0.0017408222 -0.0015079667 -0.0010887897 -0.0006592056 -0.00048197981 -0.00065769569 -0.0010560346 -0.0014405652][-0.0018113892 -0.0018108573 -0.0018118287 -0.0018128033 -0.0018137846 -0.0018143957 -0.0018099344 -0.0017061369 -0.0013914765 -0.00084346312 -0.00030326715 -0.00010027084 -0.00034278259 -0.00085653085 -0.0013429972][-0.0018111359 -0.0018104687 -0.0018115165 -0.0018125795 -0.0018135933 -0.0018141868 -0.0018084596 -0.0016838969 -0.0013276218 -0.00072755921 -0.00016306213 2.2654305e-05 -0.00026413426 -0.00082180288 -0.0013324793][-0.0018109739 -0.0018101021 -0.0018110719 -0.0018120715 -0.0018130107 -0.0018135795 -0.0018072813 -0.0016845209 -0.0013508457 -0.00080383103 -0.0003118685 -0.00017584243 -0.00046189316 -0.00096888811 -0.0014132381][-0.0018108742 -0.0018098957 -0.0018106727 -0.0018115696 -0.0018123949 -0.0018129068 -0.001807555 -0.0017120531 -0.0014610509 -0.0010586199 -0.00070907932 -0.00062918232 -0.00085899432 -0.0012341363 -0.0015463951][-0.0018108689 -0.0018097173 -0.0018103119 -0.0018110677 -0.0018116529 -0.0018119976 -0.0018089634 -0.0017538546 -0.0016074644 -0.0013732668 -0.0011729989 -0.0011319735 -0.0012726721 -0.0014915989 -0.001667326][-0.0018108505 -0.0018094507 -0.0018098922 -0.001810525 -0.0018109117 -0.0018111597 -0.0018102749 -0.0017889382 -0.0017283999 -0.001629484 -0.0015439683 -0.0015254304 -0.0015857425 -0.001678683 -0.0017518181][-0.0018108545 -0.0018092453 -0.0018094725 -0.0018099401 -0.0018101946 -0.0018103529 -0.0018105851 -0.0018062221 -0.0017901089 -0.0017614248 -0.0017355257 -0.0017277794 -0.0017437623 -0.0017706868 -0.0017926974]]...]
INFO - root - 2017-12-09 12:47:29.560205: step 23910, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.892 sec/batch; 76h:28m:14s remains)
INFO - root - 2017-12-09 12:47:38.348578: step 23920, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 73h:54m:41s remains)
INFO - root - 2017-12-09 12:47:47.380929: step 23930, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.897 sec/batch; 76h:52m:20s remains)
INFO - root - 2017-12-09 12:47:56.156745: step 23940, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.875 sec/batch; 74h:59m:09s remains)
INFO - root - 2017-12-09 12:48:04.863225: step 23950, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.840 sec/batch; 72h:01m:22s remains)
INFO - root - 2017-12-09 12:48:13.647975: step 23960, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.883 sec/batch; 75h:41m:35s remains)
INFO - root - 2017-12-09 12:48:22.421784: step 23970, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 74h:40m:11s remains)
INFO - root - 2017-12-09 12:48:31.047345: step 23980, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.846 sec/batch; 72h:29m:07s remains)
INFO - root - 2017-12-09 12:48:39.578210: step 23990, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.848 sec/batch; 72h:42m:16s remains)
INFO - root - 2017-12-09 12:48:47.972276: step 24000, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 74h:21m:44s remains)
2017-12-09 12:48:48.834878: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.03218995 0.040819578 0.050379686 0.059556469 0.067356274 0.072830923 0.075813226 0.075510241 0.07190831 0.064603552 0.053733341 0.040882006 0.027733682 0.016335942 0.0075541073][0.028679086 0.037232533 0.047084287 0.056804493 0.065327406 0.071635105 0.075497068 0.076151833 0.073356755 0.066755131 0.0563739 0.043539405 0.030020183 0.018034592 0.00865642][0.026335195 0.033998325 0.043059558 0.052366048 0.06086266 0.067629784 0.072319582 0.07406757 0.072420649 0.066909209 0.057469368 0.045182064 0.031773172 0.019512547 0.0097233355][0.025172433 0.031696044 0.03949333 0.0478236 0.055796638 0.062636986 0.067916222 0.070734717 0.070355855 0.066124961 0.05782713 0.046243154 0.033115439 0.020746792 0.010706215][0.023569265 0.028955687 0.035387553 0.042475879 0.049601942 0.056202717 0.061822265 0.065568529 0.066487558 0.063739769 0.056873627 0.046360597 0.033858113 0.021631811 0.01150707][0.020526301 0.024877843 0.030071029 0.035958089 0.042185739 0.048384503 0.054117106 0.058512688 0.06047035 0.059051588 0.053659797 0.044485368 0.033078045 0.021544347 0.011769935][0.015932273 0.019400334 0.023486203 0.028213503 0.033499986 0.0391934 0.044872425 0.049680866 0.052465245 0.05221352 0.048193306 0.040473819 0.030492416 0.020142106 0.011211318][0.010655342 0.013318526 0.016500549 0.020247633 0.024616877 0.029630648 0.03493645 0.039738134 0.042912759 0.04348281 0.040687107 0.034506828 0.026214868 0.017446741 0.0097632082][0.0056657526 0.0076023871 0.009962474 0.012807896 0.016297176 0.020503728 0.025146151 0.029544076 0.032684423 0.033703797 0.031883433 0.027214913 0.020729143 0.013760176 0.0075888466][0.0019025155 0.0031318506 0.0047277021 0.0067005064 0.0092211477 0.012408261 0.016064428 0.019627133 0.022327077 0.023470456 0.022451114 0.01924335 0.014603682 0.0095492005 0.005024774][-0.0004072045 0.00023370353 0.00112739 0.0023008091 0.0038921568 0.0060166875 0.0085806912 0.011172071 0.013246712 0.014281079 0.013826232 0.011847674 0.0088324584 0.0055123717 0.002529454][-0.0014553946 -0.0012111334 -0.0008412993 -0.00031155779 0.00048694364 0.0016618539 0.0031951861 0.0048407805 0.0062663071 0.0071026757 0.0070363237 0.0060153129 0.004303284 0.0023665316 0.00060831569][-0.0017687031 -0.0017217759 -0.0016308199 -0.0014652058 -0.0011640982 -0.00065131136 9.7457785e-05 0.0009672814 0.0017812687 0.002320617 0.0024109068 0.0019964881 0.0011866288 0.00022782921 -0.00065581372][-0.0017931673 -0.0017900753 -0.0017834863 -0.0017638818 -0.0017051648 -0.0015599822 -0.0012989022 -0.0009549798 -0.00059448986 -0.00032066146 -0.00022139132 -0.00033957 -0.00064339361 -0.0010228541 -0.0013766687][-0.0017939454 -0.0017931766 -0.0017911434 -0.0017870095 -0.0017807612 -0.0017535887 -0.0016942924 -0.0016090523 -0.0015125193 -0.0014330767 -0.0013899716 -0.0014093398 -0.0014903687 -0.0015964489 -0.0016943759]]...]
INFO - root - 2017-12-09 12:48:57.580119: step 24010, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.887 sec/batch; 76h:01m:33s remains)
INFO - root - 2017-12-09 12:49:06.218284: step 24020, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 74h:27m:59s remains)
INFO - root - 2017-12-09 12:49:14.869191: step 24030, loss = 0.82, batch loss = 0.69 (9.8 examples/sec; 0.820 sec/batch; 70h:16m:19s remains)
INFO - root - 2017-12-09 12:49:23.423344: step 24040, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.830 sec/batch; 71h:04m:39s remains)
INFO - root - 2017-12-09 12:49:32.280127: step 24050, loss = 0.82, batch loss = 0.69 (8.7 examples/sec; 0.919 sec/batch; 78h:46m:46s remains)
INFO - root - 2017-12-09 12:49:40.709062: step 24060, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.886 sec/batch; 75h:56m:40s remains)
INFO - root - 2017-12-09 12:49:49.225595: step 24070, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 74h:35m:06s remains)
INFO - root - 2017-12-09 12:49:57.822138: step 24080, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.857 sec/batch; 73h:25m:34s remains)
INFO - root - 2017-12-09 12:50:06.537021: step 24090, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.889 sec/batch; 76h:10m:55s remains)
INFO - root - 2017-12-09 12:50:15.010444: step 24100, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.842 sec/batch; 72h:08m:30s remains)
2017-12-09 12:50:15.835844: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0017962464 -0.0017943223 -0.0017937828 -0.0017936174 -0.0017936211 -0.0017937124 -0.0017937631 -0.0017938704 -0.0017940113 -0.0017940815 -0.0017941275 -0.0017941581 -0.0017941216 -0.0017940233 -0.0017938592][-0.0017950233 -0.0017930692 -0.0017926177 -0.0017925695 -0.0017926424 -0.0017927426 -0.0017927906 -0.0017928374 -0.0017929233 -0.001792972 -0.0017929398 -0.0017929212 -0.001792831 -0.0017926154 -0.0017924195][-0.0017948272 -0.0017929842 -0.0017925882 -0.0017926353 -0.001792748 -0.0017928146 -0.0017928445 -0.0017929054 -0.0017930004 -0.0017930587 -0.0017930585 -0.0017930071 -0.0017928375 -0.0017925769 -0.0017923493][-0.0017945992 -0.0017928203 -0.0017924563 -0.0017925202 -0.0017926343 -0.0017926842 -0.0017927169 -0.0017927811 -0.0017928879 -0.0017930003 -0.0017930489 -0.0017930444 -0.0017928852 -0.001792646 -0.0017924438][-0.0017944411 -0.0017926472 -0.0017923133 -0.0017923932 -0.0017924996 -0.0017925324 -0.001792553 -0.0017925758 -0.0017926537 -0.0017927998 -0.0017929138 -0.0017929861 -0.0017928984 -0.0017927326 -0.0017925728][-0.001794336 -0.0017925968 -0.0017923069 -0.0017924131 -0.0017924987 -0.0017924801 -0.0017924425 -0.0017923987 -0.0017924163 -0.0017925515 -0.0017927041 -0.0017928423 -0.0017928352 -0.0017927603 -0.0017926492][-0.0017941499 -0.0017925199 -0.0017922814 -0.0017923812 -0.0017924383 -0.0017923946 -0.0017923347 -0.001792234 -0.0017922127 -0.0017923368 -0.0017925142 -0.0017926944 -0.0017927181 -0.0017926747 -0.001792588][-0.0017940415 -0.0017924695 -0.0017922743 -0.0017923414 -0.0017923632 -0.0017923049 -0.0017922367 -0.0017921095 -0.0017920583 -0.0017921643 -0.0017923382 -0.0017925107 -0.0017925468 -0.0017925384 -0.001792484][-0.0017938861 -0.0017923644 -0.0017922859 -0.0017923175 -0.0017923061 -0.0017922615 -0.0017921909 -0.0017920483 -0.0017919699 -0.0017920608 -0.0017922048 -0.0017923242 -0.0017923701 -0.0017924017 -0.0017923987][-0.001793661 -0.0017922764 -0.0017922606 -0.0017922728 -0.0017922586 -0.0017922474 -0.0017921871 -0.0017920658 -0.0017920105 -0.0017920654 -0.0017921517 -0.0017922025 -0.0017922141 -0.0017922528 -0.0017922682][-0.0017936482 -0.0017922893 -0.001792183 -0.001792167 -0.0017921655 -0.0017921771 -0.0017921431 -0.0017920827 -0.0017920629 -0.0017920923 -0.0017921141 -0.0017921124 -0.0017921228 -0.0017921329 -0.0017921223][-0.001793612 -0.0017922671 -0.0017920604 -0.0017920131 -0.0017920026 -0.0017920164 -0.00179202 -0.0017920119 -0.0017920417 -0.0017920763 -0.0017920939 -0.0017920962 -0.0017921152 -0.0017921026 -0.0017920653][-0.0017933705 -0.0017921036 -0.001791851 -0.0017918007 -0.0017917933 -0.0017918233 -0.001791865 -0.0017919028 -0.0017919564 -0.001792005 -0.0017920295 -0.0017920616 -0.0017920975 -0.0017920893 -0.0017920561][-0.0017931783 -0.00179192 -0.00179165 -0.0017916256 -0.0017916457 -0.001791702 -0.0017917581 -0.0017917949 -0.0017918263 -0.0017918519 -0.001791873 -0.0017919221 -0.0017919709 -0.0017919949 -0.0017919886][-0.0017931247 -0.0017918702 -0.001791558 -0.0017915701 -0.0017916142 -0.0017916693 -0.0017917033 -0.0017917113 -0.0017917168 -0.0017917216 -0.0017917426 -0.0017917872 -0.001791822 -0.001791838 -0.0017918388]]...]
INFO - root - 2017-12-09 12:50:24.480563: step 24110, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.863 sec/batch; 73h:54m:58s remains)
INFO - root - 2017-12-09 12:50:33.194197: step 24120, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 74h:03m:01s remains)
INFO - root - 2017-12-09 12:50:41.846558: step 24130, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.850 sec/batch; 72h:50m:38s remains)
INFO - root - 2017-12-09 12:50:50.508461: step 24140, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.840 sec/batch; 71h:59m:29s remains)
INFO - root - 2017-12-09 12:50:59.191935: step 24150, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.861 sec/batch; 73h:43m:10s remains)
INFO - root - 2017-12-09 12:51:07.809172: step 24160, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 74h:20m:37s remains)
INFO - root - 2017-12-09 12:51:16.577720: step 24170, loss = 0.82, batch loss = 0.69 (8.3 examples/sec; 0.968 sec/batch; 82h:53m:17s remains)
INFO - root - 2017-12-09 12:51:25.224509: step 24180, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.872 sec/batch; 74h:43m:08s remains)
INFO - root - 2017-12-09 12:51:33.820574: step 24190, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.858 sec/batch; 73h:31m:13s remains)
INFO - root - 2017-12-09 12:51:42.322379: step 24200, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.874 sec/batch; 74h:50m:19s remains)
2017-12-09 12:51:43.220453: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0017903039 -0.0017883494 -0.0017877156 -0.0017877154 -0.0017878708 -0.0017881012 -0.0017884559 -0.0017889122 -0.0017892716 -0.0017895226 -0.0017897073 -0.0017897791 -0.0017897321 -0.0017896544 -0.0017896347][-0.0017896096 -0.0017875923 -0.0017868815 -0.0017868316 -0.0017869284 -0.0017870975 -0.0017874078 -0.0017878249 -0.0017881615 -0.0017884123 -0.0017886233 -0.0017886916 -0.0017886468 -0.0017885296 -0.0017884797][-0.0017895551 -0.0017875012 -0.00178673 -0.0017866174 -0.0017866357 -0.001786714 -0.0017869385 -0.0017872712 -0.0017875381 -0.0017877561 -0.0017879867 -0.0017880783 -0.0017880485 -0.0017879446 -0.0017878846][-0.0017895215 -0.0017874654 -0.001786668 -0.0017864795 -0.0017863819 -0.0017863245 -0.0017864225 -0.0017866283 -0.0017868035 -0.0017869959 -0.001787247 -0.0017873956 -0.0017874497 -0.0017874262 -0.0017874028][-0.0017898043 -0.0017876759 -0.0017868548 -0.0017866025 -0.0017863825 -0.0017861899 -0.0017861492 -0.0017862159 -0.0017862784 -0.0017864124 -0.0017866647 -0.0017868888 -0.0017870785 -0.0017871619 -0.0017872079][-0.0017899288 -0.00178782 -0.0017870048 -0.001786725 -0.0017864777 -0.001786252 -0.0017861653 -0.0017861448 -0.0017861129 -0.0017861926 -0.0017864539 -0.0017867581 -0.0017870545 -0.0017872443 -0.0017873631][-0.0017897919 -0.0017877229 -0.0017869285 -0.0017866642 -0.0017864589 -0.0017862966 -0.0017862768 -0.0017862569 -0.0017861817 -0.0017862314 -0.0017864923 -0.0017868334 -0.001787171 -0.0017874071 -0.0017875581][-0.0017896785 -0.0017876696 -0.0017869113 -0.001786679 -0.0017865381 -0.0017864624 -0.0017865294 -0.0017865556 -0.0017864748 -0.00178651 -0.0017867483 -0.0017870635 -0.0017873697 -0.0017875693 -0.0017876942][-0.0017894461 -0.001787569 -0.0017869156 -0.0017867535 -0.0017866843 -0.001786696 -0.0017868574 -0.0017869652 -0.0017869361 -0.0017869731 -0.0017871584 -0.0017873887 -0.0017876055 -0.0017877071 -0.0017877616][-0.0017890579 -0.0017873853 -0.0017868313 -0.0017867361 -0.0017867377 -0.00178682 -0.001787047 -0.001787218 -0.0017872644 -0.0017873052 -0.0017874285 -0.001787512 -0.001787553 -0.0017875001 -0.001787464][-0.001788701 -0.001787137 -0.0017866417 -0.0017865442 -0.0017865492 -0.0017866438 -0.0017868914 -0.0017871096 -0.0017872406 -0.0017873291 -0.0017874179 -0.0017873605 -0.0017872186 -0.0017870174 -0.0017869031][-0.0017886076 -0.0017869526 -0.0017864008 -0.0017862476 -0.0017862071 -0.0017862916 -0.0017865634 -0.0017868426 -0.0017870554 -0.0017872135 -0.0017873052 -0.0017871768 -0.0017869187 -0.0017866038 -0.0017864144][-0.0017882695 -0.0017865514 -0.0017859206 -0.0017857478 -0.0017857089 -0.0017858051 -0.001786098 -0.001786419 -0.0017866869 -0.0017868858 -0.0017869771 -0.0017868201 -0.0017865166 -0.001786161 -0.0017859531][-0.0017877853 -0.0017859041 -0.0017852023 -0.0017850433 -0.0017850357 -0.0017851375 -0.0017854031 -0.0017857072 -0.0017859774 -0.0017861847 -0.0017862646 -0.0017861068 -0.0017858245 -0.0017855101 -0.0017853526][-0.0017879473 -0.0017859567 -0.0017851912 -0.0017850571 -0.0017850786 -0.0017851695 -0.0017853706 -0.0017856174 -0.0017858435 -0.0017860304 -0.0017860935 -0.0017859474 -0.0017857044 -0.0017854442 -0.0017853337]]...]
INFO - root - 2017-12-09 12:51:51.833702: step 24210, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.866 sec/batch; 74h:08m:02s remains)
INFO - root - 2017-12-09 12:52:00.544129: step 24220, loss = 0.82, batch loss = 0.70 (9.2 examples/sec; 0.865 sec/batch; 74h:05m:11s remains)
INFO - root - 2017-12-09 12:52:09.242892: step 24230, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.870 sec/batch; 74h:27m:46s remains)
INFO - root - 2017-12-09 12:52:18.181886: step 24240, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.901 sec/batch; 77h:10m:26s remains)
INFO - root - 2017-12-09 12:52:26.839691: step 24250, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.884 sec/batch; 75h:44m:00s remains)
INFO - root - 2017-12-09 12:52:35.584438: step 24260, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.896 sec/batch; 76h:45m:24s remains)
INFO - root - 2017-12-09 12:52:44.367170: step 24270, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.893 sec/batch; 76h:27m:01s remains)
INFO - root - 2017-12-09 12:52:52.926000: step 24280, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 74h:14m:23s remains)
INFO - root - 2017-12-09 12:53:01.506012: step 24290, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 74h:20m:33s remains)
INFO - root - 2017-12-09 12:53:10.050214: step 24300, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 74h:22m:46s remains)
2017-12-09 12:53:10.963631: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.29237875 0.34024739 0.38603145 0.42393118 0.44701284 0.45796725 0.45797813 0.44570145 0.41914693 0.37839952 0.32995772 0.2755298 0.22046044 0.17220396 0.136994][0.30670205 0.36050621 0.41103214 0.4530566 0.47996622 0.49403942 0.49674743 0.48807412 0.465773 0.42845082 0.38252738 0.32869884 0.27300459 0.22161183 0.18275367][0.30756977 0.36579368 0.42041764 0.46680889 0.49718863 0.51542842 0.52301461 0.519603 0.50339687 0.47212332 0.43167609 0.38142478 0.32891423 0.27850917 0.23794995][0.30103242 0.3612825 0.41773552 0.4669905 0.50165915 0.52510422 0.536751 0.53940791 0.53083372 0.50728422 0.47399923 0.4298152 0.38314152 0.33597371 0.296931][0.28752974 0.3481926 0.40563735 0.45580834 0.49264157 0.51951444 0.53525758 0.54199862 0.5383938 0.52250004 0.49665952 0.4603864 0.42141461 0.38124484 0.34698591][0.27301875 0.33185378 0.38688362 0.43600279 0.47339919 0.50074524 0.51713508 0.52581209 0.52585679 0.51568937 0.49693969 0.47009626 0.4402239 0.40861028 0.38068289][0.25623125 0.31090966 0.36174703 0.40744469 0.44232303 0.46904129 0.48492455 0.49382931 0.49586052 0.49118155 0.47939089 0.45999455 0.439048 0.41628122 0.39496508][0.23944879 0.28861386 0.33336958 0.37377134 0.40396637 0.42606291 0.43943876 0.44855928 0.45223519 0.45105794 0.44524634 0.43411368 0.4210335 0.40627447 0.39168948][0.222295 0.26534036 0.30325693 0.33639786 0.36076787 0.37776431 0.38749245 0.39540312 0.40054697 0.40308285 0.40121573 0.39615434 0.38992098 0.38165489 0.37282798][0.20512076 0.24275063 0.27494711 0.30204248 0.32049218 0.33153394 0.33768332 0.34250697 0.3466191 0.35008654 0.35111684 0.35046706 0.3482728 0.34519172 0.34126434][0.19053572 0.22343661 0.25089335 0.27353263 0.28738356 0.29370782 0.29564548 0.2969678 0.29904377 0.30044571 0.30124202 0.30240029 0.30287695 0.3029108 0.30142948][0.17562744 0.20447288 0.22839758 0.2474409 0.25786197 0.2612479 0.26051226 0.25911063 0.2587066 0.25830197 0.25841653 0.25912631 0.25978354 0.26022342 0.25972119][0.16133744 0.18680699 0.20804349 0.22491042 0.2337254 0.23531689 0.23317038 0.23015751 0.22796535 0.22508217 0.22331353 0.2226734 0.22251359 0.2221515 0.22093998][0.14414328 0.16698533 0.18643199 0.20245016 0.2113447 0.21390532 0.21195464 0.20861559 0.20537408 0.20136073 0.19817601 0.19552663 0.19389546 0.19205621 0.18971007][0.12687722 0.14767522 0.16576624 0.1814384 0.1914565 0.19542193 0.19500631 0.19211823 0.18830638 0.18357852 0.17918248 0.17514423 0.1722831 0.16933556 0.16656768]]...]
INFO - root - 2017-12-09 12:53:19.632883: step 24310, loss = 0.81, batch loss = 0.68 (9.6 examples/sec; 0.835 sec/batch; 71h:26m:49s remains)
INFO - root - 2017-12-09 12:53:28.213249: step 24320, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.847 sec/batch; 72h:32m:32s remains)
INFO - root - 2017-12-09 12:53:36.745954: step 24330, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.865 sec/batch; 74h:01m:46s remains)
INFO - root - 2017-12-09 12:53:45.484602: step 24340, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.880 sec/batch; 75h:17m:33s remains)
INFO - root - 2017-12-09 12:53:54.276225: step 24350, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.876 sec/batch; 74h:57m:28s remains)
INFO - root - 2017-12-09 12:54:02.828774: step 24360, loss = 0.82, batch loss = 0.69 (9.7 examples/sec; 0.826 sec/batch; 70h:44m:22s remains)
INFO - root - 2017-12-09 12:54:11.583229: step 24370, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.897 sec/batch; 76h:44m:32s remains)
INFO - root - 2017-12-09 12:54:20.217101: step 24380, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.866 sec/batch; 74h:06m:14s remains)
INFO - root - 2017-12-09 12:54:28.912354: step 24390, loss = 0.82, batch loss = 0.69 (8.8 examples/sec; 0.909 sec/batch; 77h:46m:19s remains)
INFO - root - 2017-12-09 12:54:37.228760: step 24400, loss = 0.83, batch loss = 0.70 (9.2 examples/sec; 0.871 sec/batch; 74h:31m:17s remains)
2017-12-09 12:54:38.148611: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.14329176 0.15241085 0.1545496 0.15052372 0.14182441 0.13050298 0.11889416 0.10906119 0.10257953 0.098948106 0.096910112 0.094913624 0.091477014 0.084843181 0.074071504][0.17322429 0.18796076 0.19468115 0.19397703 0.1868635 0.1754853 0.1622697 0.14998215 0.14059658 0.13416828 0.12946112 0.12478361 0.11825029 0.10791968 0.092963271][0.19549546 0.21721235 0.23058383 0.2356265 0.23250544 0.22331208 0.21001984 0.19603828 0.18404463 0.17426334 0.16563953 0.15665403 0.14552945 0.13023312 0.1101617][0.20881857 0.23759955 0.25899202 0.27172944 0.27507111 0.2705074 0.25913638 0.24475773 0.22989625 0.21592686 0.20230241 0.18771335 0.17047027 0.14881361 0.12290954][0.21657069 0.25222084 0.28152743 0.30248114 0.31336355 0.31483689 0.30698925 0.29367775 0.27729604 0.25953957 0.24019051 0.21911183 0.19490607 0.16608028 0.13356179][0.22063993 0.2625542 0.29946119 0.32829589 0.3466711 0.35456082 0.35098547 0.3396824 0.32236871 0.30150136 0.27709463 0.24959877 0.21818066 0.18210755 0.14317729][0.22310582 0.27048922 0.31335926 0.34837657 0.37289271 0.38614577 0.38670713 0.37803781 0.36132565 0.33888534 0.31065437 0.27797264 0.24039997 0.19796093 0.15321335][0.22440077 0.27498856 0.32148328 0.36006704 0.38806492 0.4045004 0.40755087 0.40095162 0.38554806 0.36353126 0.33425289 0.29918262 0.25825298 0.21207057 0.16361807][0.22540309 0.27647054 0.32322559 0.3618426 0.39019167 0.40714157 0.41095358 0.40522987 0.39056689 0.36952826 0.34095794 0.30651352 0.2658644 0.21958588 0.17094336][0.22267212 0.27162 0.31563714 0.35142696 0.3773005 0.39218658 0.39494875 0.38919184 0.375585 0.3564899 0.33028924 0.2987076 0.26124251 0.21840174 0.17280854][0.21153359 0.25550735 0.29390791 0.32424781 0.34558833 0.35702968 0.35813016 0.35224563 0.34008655 0.3238728 0.3018893 0.27551425 0.24387692 0.20721079 0.1674512][0.19169463 0.22783256 0.25811779 0.281086 0.29629323 0.30333903 0.30227971 0.29633743 0.28639919 0.27412641 0.25773659 0.23822376 0.21454087 0.18627948 0.15433271][0.16345888 0.19028214 0.21141472 0.22634412 0.23522343 0.23795708 0.234926 0.22900304 0.22100943 0.21253672 0.20197636 0.18971439 0.17449994 0.15545106 0.13262253][0.12844484 0.14638759 0.1593172 0.16746457 0.17136595 0.17105128 0.16707969 0.1616821 0.15561311 0.15019096 0.14409281 0.1375915 0.12936582 0.11826546 0.10371205][0.091307223 0.10192999 0.10856126 0.11192765 0.11267621 0.11092889 0.10734665 0.10338591 0.099582963 0.096767 0.094080321 0.091572061 0.088044845 0.082457811 0.073966756]]...]
INFO - root - 2017-12-09 12:54:46.820284: step 24410, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.854 sec/batch; 73h:04m:11s remains)
INFO - root - 2017-12-09 12:54:55.524751: step 24420, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.872 sec/batch; 74h:36m:59s remains)
INFO - root - 2017-12-09 12:55:04.204979: step 24430, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.875 sec/batch; 74h:53m:07s remains)
INFO - root - 2017-12-09 12:55:12.839168: step 24440, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.870 sec/batch; 74h:24m:26s remains)
INFO - root - 2017-12-09 12:55:21.579829: step 24450, loss = 0.81, batch loss = 0.68 (9.1 examples/sec; 0.879 sec/batch; 75h:14m:57s remains)
INFO - root - 2017-12-09 12:55:30.119797: step 24460, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.873 sec/batch; 74h:42m:31s remains)
INFO - root - 2017-12-09 12:55:38.758324: step 24470, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.852 sec/batch; 72h:53m:44s remains)
INFO - root - 2017-12-09 12:55:47.347411: step 24480, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.865 sec/batch; 74h:00m:28s remains)
INFO - root - 2017-12-09 12:55:56.067254: step 24490, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.846 sec/batch; 72h:22m:56s remains)
INFO - root - 2017-12-09 12:56:04.441227: step 24500, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.838 sec/batch; 71h:39m:59s remains)
2017-12-09 12:56:05.341817: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0016862252 -0.0017148069 -0.0017464226 -0.0017746831 -0.0017949868 -0.0018088388 -0.0018169121 -0.0018203564 -0.0018207907 -0.0018197168 -0.0018178161 -0.0018158482 -0.0018138421 -0.0018121566 -0.0018108425][-0.0015723847 -0.0016227658 -0.0016765334 -0.0017257355 -0.0017633869 -0.0017907456 -0.0018068921 -0.0018130727 -0.0018132154 -0.001811307 -0.0018084699 -0.0018056673 -0.0018041877 -0.0018053008 -0.0018073495][-0.0014224292 -0.0014969331 -0.0015746235 -0.0016499747 -0.0017102681 -0.0017562876 -0.0017840069 -0.0017927329 -0.0017898412 -0.0017850311 -0.001781386 -0.0017787904 -0.0017793222 -0.0017860441 -0.0017960982][-0.0012550433 -0.0013489 -0.001450334 -0.0015533071 -0.0016380791 -0.0017035456 -0.0017430681 -0.0017506604 -0.0017385752 -0.0017272041 -0.001724132 -0.001724735 -0.0017309091 -0.0017477026 -0.001771918][-0.0011058826 -0.0012041843 -0.0013206003 -0.0014491004 -0.0015568031 -0.0016370851 -0.0016820251 -0.0016803351 -0.0016502342 -0.0016269392 -0.0016275065 -0.0016390457 -0.001659289 -0.001692276 -0.0017364221][-0.0010051407 -0.0010937753 -0.0012126202 -0.0013558046 -0.0014787321 -0.0015649552 -0.0016050283 -0.0015873651 -0.0015342094 -0.0014963228 -0.0015040868 -0.0015355689 -0.0015789899 -0.0016326302 -0.0016977058][-0.00098582916 -0.0010498301 -0.0011521806 -0.0012949607 -0.0014244596 -0.0015099705 -0.0015379881 -0.0015005195 -0.0014247086 -0.0013731443 -0.0013886138 -0.001444194 -0.0015143866 -0.0015880686 -0.0016687295][-0.0010618264 -0.0010928931 -0.0011649347 -0.0012909404 -0.0014179049 -0.0015007572 -0.0015142094 -0.001456455 -0.0013635758 -0.0013040471 -0.0013245516 -0.0013993457 -0.0014898534 -0.0015747971 -0.0016598849][-0.0012223215 -0.0012256432 -0.001266466 -0.0013645786 -0.0014770998 -0.0015546795 -0.0015583769 -0.001485982 -0.0013810886 -0.0013184472 -0.0013408193 -0.0014221886 -0.0015178989 -0.0015998085 -0.0016755356][-0.0014263636 -0.0014163149 -0.001435688 -0.0015005396 -0.0015825421 -0.0016449873 -0.0016454477 -0.0015763849 -0.0014742594 -0.0014115396 -0.0014300945 -0.0015049712 -0.001589412 -0.0016550481 -0.0017106965][-0.0016107555 -0.0015989743 -0.0016077682 -0.0016449079 -0.001691934 -0.0017293446 -0.0017289114 -0.001682107 -0.0016065711 -0.0015538575 -0.0015625192 -0.001616917 -0.0016770564 -0.0017182118 -0.0017497016][-0.0017369341 -0.0017282394 -0.0017311822 -0.0017468286 -0.0017654249 -0.0017815428 -0.0017817323 -0.0017602012 -0.0017210098 -0.0016882961 -0.0016878429 -0.0017160604 -0.0017483551 -0.0017681266 -0.0017813947][-0.0017960777 -0.0017905555 -0.0017905297 -0.0017953799 -0.0018002124 -0.0018038366 -0.0018039999 -0.0017984256 -0.0017860028 -0.0017735473 -0.0017708279 -0.0017794854 -0.0017900952 -0.0017958514 -0.0017994529][-0.0018138898 -0.0018112506 -0.0018108749 -0.0018113399 -0.0018120799 -0.0018124877 -0.0018123399 -0.0018114105 -0.0018091424 -0.0018065784 -0.0018047289 -0.0018049384 -0.0018060264 -0.0018065087 -0.001806792][-0.0018093695 -0.001810557 -0.0018136343 -0.0018156539 -0.0018168357 -0.0018170155 -0.0018167181 -0.0018162313 -0.0018156098 -0.0018148747 -0.0018138695 -0.0018127179 -0.001811543 -0.0018105565 -0.0018101746]]...]
INFO - root - 2017-12-09 12:56:14.133043: step 24510, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.874 sec/batch; 74h:48m:51s remains)
INFO - root - 2017-12-09 12:56:22.734091: step 24520, loss = 0.82, batch loss = 0.69 (9.7 examples/sec; 0.822 sec/batch; 70h:18m:49s remains)
INFO - root - 2017-12-09 12:56:31.445295: step 24530, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.866 sec/batch; 74h:06m:02s remains)
INFO - root - 2017-12-09 12:56:40.172836: step 24540, loss = 0.82, batch loss = 0.69 (8.3 examples/sec; 0.968 sec/batch; 82h:47m:44s remains)
INFO - root - 2017-12-09 12:56:48.878623: step 24550, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.863 sec/batch; 73h:47m:58s remains)
INFO - root - 2017-12-09 12:56:57.397088: step 24560, loss = 0.82, batch loss = 0.69 (9.7 examples/sec; 0.828 sec/batch; 70h:47m:10s remains)
INFO - root - 2017-12-09 12:57:06.062832: step 24570, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.838 sec/batch; 71h:42m:59s remains)
INFO - root - 2017-12-09 12:57:14.632286: step 24580, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.849 sec/batch; 72h:38m:20s remains)
INFO - root - 2017-12-09 12:57:22.996162: step 24590, loss = 0.82, batch loss = 0.69 (9.8 examples/sec; 0.819 sec/batch; 70h:01m:45s remains)
INFO - root - 2017-12-09 12:57:31.347859: step 24600, loss = 0.81, batch loss = 0.68 (9.4 examples/sec; 0.853 sec/batch; 72h:55m:05s remains)
2017-12-09 12:57:32.259796: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0018140847 -0.001813921 -0.0018138484 -0.0018136377 -0.0018132399 -0.0018127634 -0.0018123094 -0.0018120429 -0.0018120647 -0.001812271 -0.0018123842 -0.0018122748 -0.0018120006 -0.0018117292 -0.0018115896][-0.001814083 -0.0018140202 -0.0018140092 -0.0018138284 -0.0018133919 -0.0018128211 -0.0018122684 -0.0018119044 -0.0018118606 -0.0018120398 -0.0018121167 -0.0018119534 -0.0018116523 -0.0018113378 -0.0018111686][-0.0018142324 -0.0018142646 -0.0018143486 -0.0018142641 -0.0018138736 -0.0018133125 -0.0018127256 -0.0018123228 -0.0018121386 -0.0018120637 -0.001811967 -0.0018117621 -0.0018115004 -0.0018112423 -0.0018111155][-0.0018142775 -0.0018143499 -0.0018145214 -0.00181456 -0.0018143404 -0.001813942 -0.0018134714 -0.0018130395 -0.0018126654 -0.0018122939 -0.0018119279 -0.0018116406 -0.0018113898 -0.0018112146 -0.0018111508][-0.0018141441 -0.0018142279 -0.0018144911 -0.001814678 -0.0018146551 -0.0018144442 -0.0018141255 -0.0018137293 -0.0018132397 -0.0018126096 -0.0018120442 -0.0018116507 -0.0018114184 -0.0018113519 -0.0018114015][-0.0018140554 -0.0018142014 -0.0018145794 -0.0018149099 -0.0018149391 -0.0018146416 -0.0018142675 -0.0018138523 -0.0018132991 -0.0018125862 -0.0018119167 -0.0018115388 -0.0018113918 -0.0018115389 -0.001811792][-0.0018139859 -0.001814226 -0.001814787 -0.001815229 -0.0018151968 -0.0018146224 -0.0018138776 -0.0018132059 -0.0018126053 -0.0018119641 -0.001811464 -0.0018112224 -0.0018112634 -0.0018115761 -0.0018120157][-0.0018137617 -0.0018140434 -0.0018146702 -0.0018152019 -0.0018152317 -0.0018145775 -0.0018135146 -0.0018124457 -0.0018117707 -0.0018113045 -0.0018110146 -0.001810937 -0.0018110144 -0.0018113658 -0.0018119171][-0.0018134005 -0.001813574 -0.0018140718 -0.0018144937 -0.0018146004 -0.0018141654 -0.0018133124 -0.0018122897 -0.001811506 -0.0018111097 -0.0018109332 -0.0018108769 -0.0018108995 -0.0018111722 -0.0018117188][-0.0018128139 -0.0018128596 -0.0018131363 -0.0018133667 -0.0018134005 -0.0018131529 -0.0018126492 -0.0018120246 -0.001811508 -0.0018111663 -0.0018110035 -0.0018109538 -0.0018109204 -0.0018111173 -0.0018116157][-0.0018124968 -0.0018123795 -0.0018124647 -0.0018125492 -0.0018125563 -0.0018124052 -0.0018120885 -0.0018117347 -0.0018114765 -0.0018112939 -0.0018111705 -0.0018111164 -0.0018111179 -0.0018112948 -0.0018117209][-0.0018123594 -0.0018121446 -0.0018121187 -0.001812153 -0.0018122053 -0.0018121402 -0.0018119181 -0.0018116346 -0.0018114169 -0.0018112648 -0.0018111683 -0.0018111273 -0.0018111583 -0.0018113779 -0.0018117804][-0.0018125059 -0.0018122391 -0.0018121637 -0.0018121627 -0.0018122569 -0.001812267 -0.0018121414 -0.0018118968 -0.0018116479 -0.0018114165 -0.0018112173 -0.0018111229 -0.0018111833 -0.0018114372 -0.0018118172][-0.0018126562 -0.0018123522 -0.0018122516 -0.0018122476 -0.0018123623 -0.0018124451 -0.0018124146 -0.0018122564 -0.0018120402 -0.001811775 -0.0018114962 -0.0018113445 -0.0018113713 -0.0018115826 -0.0018119101][-0.0018127903 -0.0018124717 -0.0018123445 -0.0018123508 -0.0018124526 -0.0018125367 -0.0018125366 -0.0018124386 -0.0018122763 -0.0018120416 -0.0018117653 -0.0018115987 -0.0018115961 -0.0018117651 -0.0018120348]]...]
INFO - root - 2017-12-09 12:57:40.814280: step 24610, loss = 0.83, batch loss = 0.70 (9.5 examples/sec; 0.839 sec/batch; 71h:43m:45s remains)
INFO - root - 2017-12-09 12:57:49.376621: step 24620, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.851 sec/batch; 72h:47m:00s remains)
INFO - root - 2017-12-09 12:57:57.889339: step 24630, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.861 sec/batch; 73h:39m:17s remains)
INFO - root - 2017-12-09 12:58:06.545626: step 24640, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.850 sec/batch; 72h:43m:16s remains)
INFO - root - 2017-12-09 12:58:15.038372: step 24650, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.829 sec/batch; 70h:53m:44s remains)
INFO - root - 2017-12-09 12:58:23.461968: step 24660, loss = 0.81, batch loss = 0.68 (10.3 examples/sec; 0.777 sec/batch; 66h:27m:23s remains)
INFO - root - 2017-12-09 12:58:32.089544: step 24670, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.851 sec/batch; 72h:48m:14s remains)
INFO - root - 2017-12-09 12:58:40.769555: step 24680, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.878 sec/batch; 75h:04m:49s remains)
INFO - root - 2017-12-09 12:58:49.247311: step 24690, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.830 sec/batch; 71h:00m:15s remains)
INFO - root - 2017-12-09 12:58:57.767167: step 24700, loss = 0.82, batch loss = 0.69 (9.7 examples/sec; 0.825 sec/batch; 70h:30m:30s remains)
2017-12-09 12:58:58.531466: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0018317051 -0.0018311301 -0.0018274128 -0.0018171408 -0.0017983028 -0.0017683181 -0.0017217153 -0.0016572484 -0.0015834367 -0.0015236516 -0.0014905536 -0.001483219 -0.001493277 -0.0015150713 -0.0015410419][-0.0018307596 -0.0018283045 -0.0018193334 -0.0017995479 -0.001769674 -0.0017307227 -0.0016777288 -0.0016070178 -0.001525461 -0.0014605054 -0.0014294345 -0.0014324063 -0.0014566779 -0.0014947118 -0.0015402464][-0.0018276725 -0.001818781 -0.0017960327 -0.0017552131 -0.0017034392 -0.0016507129 -0.0015961166 -0.0015316331 -0.001455705 -0.0013945724 -0.0013687368 -0.0013864435 -0.001433783 -0.0015005447 -0.0015753803][-0.0018184197 -0.0017945203 -0.0017443441 -0.0016672416 -0.001582021 -0.0015113722 -0.001460357 -0.0014159734 -0.0013639971 -0.0013210396 -0.0013068273 -0.0013426468 -0.0014190507 -0.0015221502 -0.0016270169][-0.0017992344 -0.0017501306 -0.0016587905 -0.0015332205 -0.0014096545 -0.0013215572 -0.0012777552 -0.0012602962 -0.0012439974 -0.0012322932 -0.0012387878 -0.0012977113 -0.0014063899 -0.0015469191 -0.001676064][-0.0017705623 -0.0016913221 -0.0015554163 -0.001383564 -0.0012282231 -0.0011255215 -0.0010851681 -0.0010911499 -0.0011134013 -0.0011405207 -0.0011755406 -0.0012610874 -0.0014007408 -0.0015716525 -0.0017137831][-0.001740662 -0.001637619 -0.0014704685 -0.0012695433 -0.0010976633 -0.00098615675 -0.00094510411 -0.00096467452 -0.0010155104 -0.0010753267 -0.0011373674 -0.001248071 -0.0014124619 -0.0016007639 -0.0017420761][-0.0017230056 -0.0016114534 -0.0014361867 -0.0012299371 -0.0010580118 -0.0009450948 -0.00090227404 -0.00092809991 -0.00099594914 -0.0010766903 -0.0011552171 -0.0012790605 -0.0014519335 -0.0016383737 -0.0017650853][-0.001727782 -0.0016257566 -0.0014671847 -0.001280118 -0.0011254263 -0.0010246866 -0.00098913186 -0.001019465 -0.0010910132 -0.0011754946 -0.0012541227 -0.0013720632 -0.0015295348 -0.001688765 -0.0017864049][-0.0017516043 -0.0016726884 -0.0015492946 -0.0014010016 -0.0012757867 -0.0011967041 -0.0011746043 -0.0012088363 -0.0012744223 -0.001348459 -0.0014125118 -0.0015052215 -0.0016251298 -0.0017397928 -0.0018038406][-0.0017835745 -0.0017332677 -0.0016532565 -0.0015550774 -0.0014698959 -0.0014190172 -0.0014106575 -0.0014425651 -0.0014920896 -0.0015449183 -0.0015867889 -0.0016445648 -0.0017171614 -0.0017826927 -0.0018158623][-0.0018088027 -0.0017840333 -0.0017432444 -0.0016916661 -0.0016448729 -0.0016183329 -0.0016176996 -0.0016406493 -0.0016701671 -0.0016996472 -0.0017203907 -0.0017473075 -0.0017802918 -0.0018089714 -0.001822084][-0.0018228708 -0.001813877 -0.0017983282 -0.0017782407 -0.0017597359 -0.0017496148 -0.0017505456 -0.0017616451 -0.0017741008 -0.0017864183 -0.0017940319 -0.0018029221 -0.0018132498 -0.0018218287 -0.0018250394][-0.0018271882 -0.0018251551 -0.0018212235 -0.0018156328 -0.0018102545 -0.0018074277 -0.0018079476 -0.0018115274 -0.0018151836 -0.0018189078 -0.0018209708 -0.0018227199 -0.0018244404 -0.0018256757 -0.0018257846][-0.0018274916 -0.0018269414 -0.001826111 -0.0018250594 -0.0018239104 -0.0018232646 -0.0018229877 -0.0018234561 -0.0018241414 -0.0018251305 -0.0018259449 -0.0018263709 -0.0018263801 -0.001826248 -0.0018259336]]...]
INFO - root - 2017-12-09 12:59:07.018556: step 24710, loss = 0.82, batch loss = 0.69 (9.8 examples/sec; 0.819 sec/batch; 70h:02m:09s remains)
INFO - root - 2017-12-09 12:59:15.642955: step 24720, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.854 sec/batch; 73h:00m:25s remains)
INFO - root - 2017-12-09 12:59:24.413295: step 24730, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 74h:26m:06s remains)
INFO - root - 2017-12-09 12:59:33.163701: step 24740, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.853 sec/batch; 72h:53m:22s remains)
INFO - root - 2017-12-09 12:59:41.810074: step 24750, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.840 sec/batch; 71h:47m:48s remains)
INFO - root - 2017-12-09 12:59:50.496193: step 24760, loss = 0.82, batch loss = 0.69 (10.9 examples/sec; 0.737 sec/batch; 63h:00m:04s remains)
INFO - root - 2017-12-09 12:59:59.287927: step 24770, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.888 sec/batch; 75h:52m:49s remains)
INFO - root - 2017-12-09 13:00:07.951077: step 24780, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.888 sec/batch; 75h:54m:34s remains)
INFO - root - 2017-12-09 13:00:16.639752: step 24790, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.852 sec/batch; 72h:50m:11s remains)
INFO - root - 2017-12-09 13:00:25.091855: step 24800, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 74h:08m:33s remains)
2017-12-09 13:00:25.947224: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.017237805 0.017537419 0.018001633 0.018665932 0.019339645 0.020070672 0.020625953 0.020729516 0.020779584 0.020904602 0.020898666 0.020890782 0.020798119 0.020656761 0.020426091][0.021793306 0.022082906 0.022428552 0.022950368 0.02375285 0.024603924 0.025271442 0.025710927 0.026048787 0.026402378 0.026523292 0.026696466 0.026713721 0.02700875 0.027108392][0.025781594 0.026189767 0.026573617 0.027006656 0.027655149 0.028249128 0.028864382 0.029456768 0.029974524 0.030626141 0.03123121 0.031750496 0.032093596 0.032548167 0.032935783][0.028096171 0.028572315 0.028931664 0.029239368 0.029726963 0.030069273 0.030631028 0.031100992 0.031656358 0.032498635 0.033292983 0.034321796 0.03498638 0.0356572 0.036149155][0.029019274 0.029227052 0.029198805 0.029241877 0.029479422 0.029711671 0.030172795 0.030583341 0.031219702 0.032144766 0.033145696 0.034317955 0.035314851 0.036202796 0.036903754][0.028592315 0.028554108 0.028166646 0.027781893 0.027648974 0.027842518 0.028283769 0.028589755 0.029222636 0.03020303 0.031310871 0.032471582 0.033536743 0.034732223 0.035537519][0.026852559 0.026524957 0.025766119 0.025093965 0.024694884 0.024573212 0.024912065 0.025304249 0.025959592 0.026829395 0.027865347 0.029009115 0.030172534 0.031395748 0.032328371][0.024355698 0.023820413 0.022783758 0.021721317 0.021039957 0.020833295 0.021076467 0.021557324 0.022270519 0.023126299 0.024086289 0.025049813 0.026081832 0.027144549 0.027973417][0.022034511 0.021306012 0.019863395 0.018435273 0.017390309 0.01682107 0.016807344 0.017174259 0.01786169 0.01892535 0.019942068 0.020847874 0.021691769 0.02254696 0.023130611][0.020829242 0.019753693 0.017867139 0.015992844 0.014550556 0.013658836 0.013367269 0.013604145 0.014419212 0.015588143 0.016661404 0.017615022 0.018313987 0.018850019 0.019048432][0.020648364 0.01923761 0.017003294 0.014793086 0.012985383 0.011799014 0.011251556 0.011330902 0.012159185 0.013395183 0.014596143 0.015590294 0.016114589 0.01650998 0.016444078][0.020953823 0.019067207 0.016408775 0.013932659 0.011977067 0.010596403 0.0098244241 0.00970346 0.010369969 0.011540811 0.012679612 0.013616434 0.014201107 0.014569315 0.014548655][0.020588489 0.018313756 0.015290185 0.012531788 0.010440097 0.0090567144 0.0082389563 0.0080070943 0.0084617147 0.0094456114 0.01045275 0.011289217 0.011859716 0.0122503 0.012367706][0.018744566 0.016378041 0.013276228 0.01048305 0.0084362421 0.0071337628 0.0064410805 0.0062812534 0.0066931457 0.0074975048 0.0083164722 0.0090989722 0.0096089588 0.0099684559 0.010076702][0.016247937 0.013863373 0.010875602 0.0081823068 0.0061739809 0.0049455562 0.00433241 0.0042808168 0.004710258 0.0054420126 0.0062105288 0.0069270856 0.0074638226 0.0078054494 0.0079856515]]...]
INFO - root - 2017-12-09 13:00:34.375076: step 24810, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 74h:10m:38s remains)
INFO - root - 2017-12-09 13:00:43.065042: step 24820, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.859 sec/batch; 73h:24m:54s remains)
INFO - root - 2017-12-09 13:00:51.643090: step 24830, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.880 sec/batch; 75h:12m:52s remains)
INFO - root - 2017-12-09 13:01:00.121879: step 24840, loss = 0.82, batch loss = 0.69 (9.8 examples/sec; 0.814 sec/batch; 69h:35m:45s remains)
INFO - root - 2017-12-09 13:01:08.682611: step 24850, loss = 0.82, batch loss = 0.69 (8.8 examples/sec; 0.907 sec/batch; 77h:30m:24s remains)
INFO - root - 2017-12-09 13:01:17.361937: step 24860, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.865 sec/batch; 73h:54m:38s remains)
INFO - root - 2017-12-09 13:01:25.931556: step 24870, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 74h:25m:30s remains)
INFO - root - 2017-12-09 13:01:34.561958: step 24880, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 73h:51m:59s remains)
INFO - root - 2017-12-09 13:01:43.110325: step 24890, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.837 sec/batch; 71h:32m:18s remains)
INFO - root - 2017-12-09 13:01:51.718146: step 24900, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.866 sec/batch; 73h:59m:38s remains)
2017-12-09 13:01:52.554218: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.25363782 0.25289369 0.25115764 0.25097826 0.25164154 0.25349781 0.25563782 0.25757244 0.2588886 0.25840729 0.25591773 0.24983099 0.23950309 0.22412859 0.20308837][0.2624256 0.26297835 0.26156041 0.2617268 0.26234409 0.26424018 0.26630196 0.26823145 0.26961842 0.26986384 0.26849389 0.26329857 0.25323835 0.2371797 0.2146222][0.26064271 0.26342967 0.26312318 0.26411778 0.26498222 0.26669014 0.26800403 0.26925495 0.269995 0.27023077 0.26927185 0.26475942 0.25507128 0.23866105 0.2152326][0.25744098 0.2638346 0.26616439 0.26877588 0.27032733 0.27196294 0.272318 0.2720921 0.27081677 0.26943257 0.26716271 0.26203737 0.25178483 0.23471777 0.21055466][0.25344917 0.26341891 0.26842964 0.27315271 0.27604181 0.27749908 0.27651954 0.2743634 0.27037707 0.26612183 0.261052 0.25393626 0.24216577 0.22401424 0.19911283][0.24649741 0.26023489 0.26827607 0.27454212 0.27822536 0.27945226 0.277141 0.27220508 0.26431033 0.25591949 0.246814 0.23664016 0.22279271 0.20381273 0.17907698][0.23553488 0.25241596 0.26244566 0.26971036 0.27361688 0.27347314 0.26866284 0.26026148 0.24807866 0.23505719 0.22161627 0.20829616 0.19279017 0.17385606 0.15061092][0.21837045 0.23675616 0.24768364 0.25465095 0.25754175 0.25559133 0.2481671 0.23582873 0.21928385 0.20186269 0.18470077 0.1689404 0.15272158 0.13507552 0.1149719][0.19482452 0.21301287 0.22297943 0.22834517 0.22906937 0.22461325 0.21441677 0.1989297 0.17930804 0.15903616 0.13995568 0.1234623 0.10818437 0.0933146 0.077649][0.16483541 0.18088885 0.18849322 0.19095419 0.18857263 0.18146406 0.16917306 0.15224323 0.13212979 0.11199129 0.093884289 0.079022467 0.066367514 0.055182386 0.044370465][0.13141389 0.14379993 0.14838755 0.14760736 0.14228226 0.13329455 0.12039634 0.10441498 0.086746931 0.069845833 0.055398162 0.04399639 0.034945853 0.02764312 0.021162359][0.096593276 0.10493219 0.1068676 0.10384185 0.097053558 0.087676547 0.075915739 0.062755466 0.049265686 0.037124619 0.027356071 0.0200389 0.014666934 0.010695141 0.0075016594][0.063663431 0.068384193 0.068328351 0.064541683 0.058120623 0.050061103 0.040943515 0.031628974 0.02279811 0.01543213 0.00993635 0.0061197034 0.0036065252 0.0019897926 0.00087651622][0.035305161 0.03763777 0.036862679 0.033584625 0.028713638 0.023138084 0.017404027 0.011981808 0.0073105791 0.0037747878 0.0014036236 -3.249757e-05 -0.00082357228 -0.0012144158 -0.0014062512][0.014725219 0.015731713 0.015086611 0.013084574 0.010312222 0.0073722866 0.0046119345 0.0022786967 0.00051782478 -0.00061784766 -0.00123012 -0.0015131183 -0.0016245937 -0.0016642136 -0.0016842558]]...]
INFO - root - 2017-12-09 13:02:00.960499: step 24910, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 73h:50m:22s remains)
INFO - root - 2017-12-09 13:02:09.538736: step 24920, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 73h:51m:26s remains)
INFO - root - 2017-12-09 13:02:18.189620: step 24930, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.858 sec/batch; 73h:18m:22s remains)
INFO - root - 2017-12-09 13:02:26.960951: step 24940, loss = 0.83, batch loss = 0.70 (9.5 examples/sec; 0.840 sec/batch; 71h:43m:59s remains)
INFO - root - 2017-12-09 13:02:35.764453: step 24950, loss = 0.81, batch loss = 0.68 (8.7 examples/sec; 0.918 sec/batch; 78h:24m:09s remains)
INFO - root - 2017-12-09 13:02:44.363648: step 24960, loss = 0.82, batch loss = 0.69 (9.9 examples/sec; 0.806 sec/batch; 68h:49m:48s remains)
INFO - root - 2017-12-09 13:02:52.796712: step 24970, loss = 0.81, batch loss = 0.68 (9.6 examples/sec; 0.835 sec/batch; 71h:20m:51s remains)
INFO - root - 2017-12-09 13:03:01.271370: step 24980, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.857 sec/batch; 73h:10m:40s remains)
INFO - root - 2017-12-09 13:03:09.922203: step 24990, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.842 sec/batch; 71h:54m:56s remains)
INFO - root - 2017-12-09 13:03:18.351716: step 25000, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.854 sec/batch; 72h:56m:36s remains)
2017-12-09 13:03:19.226089: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.00064928737 0.00079923379 0.00089835771 0.00092232996 0.00089571555 0.00087394589 0.00089610368 0.00093563669 0.00093069975 0.00081683556 0.00055876607 0.00014213426 -0.00035626022 -0.00082994252 -0.0011851168][0.0021478226 0.0023868568 0.002529595 0.0025551771 0.0024960823 0.0024285861 0.0024313335 0.0025040419 0.0025663134 0.0024919622 0.0021665669 0.0015520491 0.0007821992 6.6902721e-06 -0.0006226328][0.003739648 0.0039631794 0.004009862 0.0038815574 0.0036806883 0.0035293815 0.0035382542 0.0036658412 0.003789675 0.0037882752 0.0035025612 0.0028525968 0.0019003819 0.000869191 -1.5212572e-05][0.0049086562 0.0050118528 0.0048474427 0.0044890107 0.004103302 0.0038510873 0.0038614701 0.0040889825 0.0043461178 0.0044572507 0.00422957 0.0036455467 0.0027169934 0.0015961784 0.00052015693][0.0052007278 0.0051713469 0.0048074005 0.0042221863 0.0036360682 0.003270403 0.00326525 0.003541436 0.0038952818 0.0041210195 0.0040100096 0.0035826159 0.0028152722 0.0018186085 0.00074015441][0.0048969937 0.0047389241 0.0042650192 0.0035466626 0.0028145933 0.0023490498 0.0023302296 0.0026567136 0.0030463869 0.0033025597 0.0032586311 0.0029727365 0.0023905011 0.0015628727 0.000596361][0.0040069646 0.00373653 0.0032154243 0.0024691839 0.0016754291 0.0011748446 0.0011319926 0.0014347597 0.0018362869 0.0021065213 0.0020991093 0.0019022643 0.0014751919 0.0008516917 8.3068851e-05][0.0027233982 0.0023800372 0.0018699993 0.0011927006 0.00049194461 4.2386935e-05 -2.6944559e-05 0.00021205819 0.00054742163 0.0007978559 0.00082776533 0.00069349748 0.00037646294 -6.1042141e-05 -0.00057946157][0.0011898859 0.00086226687 0.00044455961 -8.5887732e-05 -0.00060207711 -0.0009308508 -0.0010117556 -0.00088500854 -0.00066333089 -0.00045830163 -0.00040936994 -0.000480845 -0.00067619525 -0.00092422444 -0.0011920884][-0.00020330772 -0.00046103681 -0.00071743515 -0.0010211209 -0.00132552 -0.0015275053 -0.0015900081 -0.0015374669 -0.0014301115 -0.0013136056 -0.0012721929 -0.0012971577 -0.0013864835 -0.0014851576 -0.0015802595][-0.0011132553 -0.0012576386 -0.0013654509 -0.0014959583 -0.0016387714 -0.0017392394 -0.0017792764 -0.001772176 -0.0017394036 -0.0016966211 -0.0016814189 -0.0016889003 -0.0017130304 -0.0017338363 -0.0017523299][-0.0015660846 -0.0016220186 -0.0016519171 -0.0016941858 -0.0017516781 -0.0017902994 -0.0018016119 -0.0018028133 -0.0018012135 -0.0017944245 -0.0017881558 -0.0017860926 -0.0017903296 -0.0017943908 -0.0017962301][-0.0017476879 -0.0017597293 -0.0017653807 -0.0017743479 -0.0017897456 -0.0018004506 -0.0018031476 -0.0018040723 -0.001804441 -0.0018033213 -0.0018009833 -0.0017995718 -0.0018004905 -0.0018018303 -0.0018020503][-0.0018026117 -0.0018023466 -0.0018023037 -0.0018024144 -0.0018036191 -0.0018040746 -0.001804293 -0.0018046307 -0.0018054581 -0.0018060725 -0.0018056531 -0.0018054091 -0.0018056275 -0.0018055949 -0.0018051158][-0.0018111985 -0.001809474 -0.0018088698 -0.0018083649 -0.0018081275 -0.0018074238 -0.0018069232 -0.001806555 -0.0018066447 -0.0018070411 -0.0018073886 -0.0018074932 -0.0018076384 -0.001807412 -0.001807037]]...]
INFO - root - 2017-12-09 13:03:27.621372: step 25010, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.876 sec/batch; 74h:51m:29s remains)
INFO - root - 2017-12-09 13:03:36.376464: step 25020, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.887 sec/batch; 75h:44m:25s remains)
INFO - root - 2017-12-09 13:03:45.165011: step 25030, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.845 sec/batch; 72h:11m:48s remains)
INFO - root - 2017-12-09 13:03:53.949294: step 25040, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.900 sec/batch; 76h:50m:29s remains)
INFO - root - 2017-12-09 13:04:02.605830: step 25050, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.870 sec/batch; 74h:20m:16s remains)
INFO - root - 2017-12-09 13:04:11.402212: step 25060, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 73h:45m:47s remains)
INFO - root - 2017-12-09 13:04:19.884400: step 25070, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.880 sec/batch; 75h:07m:34s remains)
INFO - root - 2017-12-09 13:04:28.729028: step 25080, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.841 sec/batch; 71h:50m:05s remains)
INFO - root - 2017-12-09 13:04:37.505532: step 25090, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.877 sec/batch; 74h:53m:23s remains)
INFO - root - 2017-12-09 13:04:46.231702: step 25100, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 74h:06m:29s remains)
2017-12-09 13:04:47.109679: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.20811954 0.20815238 0.20705697 0.20452289 0.20082803 0.19629221 0.19032799 0.18411681 0.17717376 0.16972587 0.16230217 0.15459973 0.14671247 0.13872574 0.13130879][0.21547177 0.21846358 0.22013956 0.22007778 0.21835697 0.21475391 0.20900585 0.20202444 0.19368987 0.18449888 0.17551245 0.16678374 0.15777817 0.14877588 0.14000016][0.21993838 0.22632182 0.23126185 0.23427504 0.23518191 0.23315449 0.22814764 0.22056414 0.21086857 0.19984396 0.18899165 0.17877702 0.16841486 0.15795223 0.14752066][0.22290149 0.23297271 0.24165562 0.24827968 0.25228846 0.25231287 0.24828102 0.24040729 0.22943132 0.21663141 0.2040135 0.19195431 0.1800307 0.16791084 0.15561219][0.22471207 0.2379427 0.24979986 0.25955725 0.26640069 0.26856366 0.26580718 0.25794208 0.24590732 0.23201518 0.21778379 0.20410508 0.19075723 0.17728482 0.16356961][0.22430554 0.24023734 0.25437465 0.26648691 0.27532104 0.27947325 0.27814135 0.27119854 0.2593109 0.24484991 0.22955503 0.21445881 0.19968225 0.18476792 0.16976596][0.22065717 0.23817107 0.25351936 0.26697066 0.27731133 0.28276557 0.28291 0.27786344 0.26752704 0.25407887 0.23894809 0.22351272 0.20790198 0.19147618 0.17508458][0.21628177 0.23465188 0.25023723 0.26372451 0.27413464 0.28009486 0.28101251 0.27739972 0.26886302 0.25740588 0.24379984 0.22856587 0.21274915 0.19547653 0.17812279][0.20934162 0.22761023 0.24255708 0.25487456 0.26414347 0.26951849 0.27039662 0.26787907 0.26120445 0.25246111 0.24134558 0.22777505 0.21301526 0.19621089 0.17868073][0.20033301 0.2176739 0.23108286 0.24167383 0.24922194 0.25313613 0.25364533 0.25168815 0.24657258 0.24015158 0.23153244 0.22047214 0.20753171 0.19222984 0.17574166][0.18961197 0.20531498 0.21660019 0.22485922 0.23014052 0.23233125 0.23205127 0.23052292 0.2270285 0.22279979 0.21692285 0.20858949 0.19803837 0.18459533 0.16964984][0.17697009 0.19048813 0.19951671 0.20583285 0.20935908 0.20981888 0.20849799 0.20692177 0.20458665 0.20209451 0.19851559 0.19292279 0.18496925 0.1740184 0.16118053][0.16383868 0.1753331 0.18232508 0.18663436 0.18831941 0.18712221 0.18495862 0.18288454 0.18099762 0.17975366 0.17815754 0.17485958 0.16916697 0.16093993 0.15072474][0.14957105 0.15890393 0.16395833 0.16673261 0.16723579 0.1654458 0.16310067 0.16081876 0.15906 0.15841721 0.15781376 0.15631644 0.1527016 0.14728858 0.14009562][0.13734281 0.14477195 0.14815652 0.14976199 0.14958304 0.14756696 0.14530641 0.14311336 0.14155093 0.14105697 0.14088756 0.14029993 0.13807322 0.13464679 0.12998207]]...]
INFO - root - 2017-12-09 13:04:55.615804: step 25110, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.872 sec/batch; 74h:25m:51s remains)
INFO - root - 2017-12-09 13:05:04.175228: step 25120, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.854 sec/batch; 72h:52m:58s remains)
INFO - root - 2017-12-09 13:05:12.757542: step 25130, loss = 0.82, batch loss = 0.69 (9.8 examples/sec; 0.820 sec/batch; 70h:00m:24s remains)
INFO - root - 2017-12-09 13:05:21.466888: step 25140, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.873 sec/batch; 74h:30m:52s remains)
INFO - root - 2017-12-09 13:05:30.105092: step 25150, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.843 sec/batch; 71h:58m:09s remains)
INFO - root - 2017-12-09 13:05:38.745966: step 25160, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.884 sec/batch; 75h:30m:30s remains)
INFO - root - 2017-12-09 13:05:47.136281: step 25170, loss = 0.82, batch loss = 0.69 (9.7 examples/sec; 0.825 sec/batch; 70h:23m:41s remains)
INFO - root - 2017-12-09 13:05:55.724524: step 25180, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 73h:33m:57s remains)
INFO - root - 2017-12-09 13:06:04.226237: step 25190, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 74h:21m:34s remains)
INFO - root - 2017-12-09 13:06:12.828261: step 25200, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.860 sec/batch; 73h:23m:32s remains)
2017-12-09 13:06:13.694392: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0018178575 -0.0018182008 -0.0018179795 -0.0018119977 -0.0017973958 -0.0017746311 -0.0017493221 -0.0017335421 -0.0017370046 -0.0017564998 -0.0017801562 -0.0017989806 -0.001810603 -0.0018169173 -0.0018200653][-0.0018172953 -0.0018168888 -0.0018129891 -0.0017986768 -0.0017686039 -0.0017264463 -0.001683478 -0.0016601603 -0.0016680679 -0.0017023683 -0.0017442092 -0.0017781223 -0.0017988432 -0.0018091697 -0.00181384][-0.0018165972 -0.0018133805 -0.0017995546 -0.0017659544 -0.0017032623 -0.0016236429 -0.0015457718 -0.0015052017 -0.0015184557 -0.0015798339 -0.0016582889 -0.001726397 -0.0017691629 -0.0017884953 -0.0017941737][-0.0018155017 -0.0018068771 -0.0017735249 -0.0017017659 -0.0015799587 -0.0014382649 -0.0013067895 -0.0012370602 -0.0012507499 -0.0013474269 -0.0014843729 -0.0016144097 -0.0017015584 -0.0017420214 -0.0017509813][-0.0018144191 -0.0018007675 -0.0017500655 -0.0016406102 -0.0014584713 -0.0012410526 -0.0010339506 -0.00091202289 -0.00091209763 -0.00104077 -0.0012423075 -0.0014494542 -0.0015971456 -0.0016700873 -0.0016871423][-0.0018144348 -0.0018013804 -0.0017518894 -0.0016392572 -0.0014396237 -0.0011775934 -0.00090680149 -0.0007166221 -0.00067271118 -0.00079810945 -0.0010350692 -0.0012958322 -0.0014939776 -0.0015976109 -0.0016242348][-0.0018148152 -0.0018061127 -0.0017744593 -0.0016957362 -0.0015396471 -0.0013035182 -0.0010299755 -0.00080427364 -0.00071412092 -0.00079284096 -0.00099913718 -0.0012446183 -0.0014436439 -0.0015534734 -0.0015835154][-0.0018156127 -0.0018118739 -0.0017991202 -0.0017621563 -0.0016751578 -0.0015191892 -0.0013111002 -0.0011117251 -0.0010021585 -0.0010171842 -0.0011365952 -0.0012943237 -0.0014352705 -0.0015208232 -0.0015500903][-0.0018159682 -0.0018145512 -0.0018115881 -0.0018008378 -0.0017681343 -0.0016990546 -0.0015919261 -0.0014729258 -0.0013857019 -0.0013495969 -0.0013625219 -0.0013961087 -0.0014420022 -0.001483321 -0.0015138391][-0.0018161208 -0.0018151532 -0.0018146783 -0.0018129873 -0.0018047988 -0.0017849894 -0.0017498911 -0.0017062092 -0.0016633329 -0.0016189658 -0.00156912 -0.0015101967 -0.0014710771 -0.0014659339 -0.0014979137][-0.0018157314 -0.001814829 -0.0018144579 -0.00181418 -0.0018134963 -0.0018113285 -0.0018072211 -0.0017998507 -0.001786391 -0.0017573605 -0.0017054492 -0.0016304323 -0.0015623661 -0.0015324708 -0.0015562997][-0.0018154272 -0.001814629 -0.0018141632 -0.0018137397 -0.0018132047 -0.0018125633 -0.0018120155 -0.0018117192 -0.0018104413 -0.0018015435 -0.0017772975 -0.0017345924 -0.0016885763 -0.0016623988 -0.0016730861][-0.0018152893 -0.0018146683 -0.0018141818 -0.0018137021 -0.0018131587 -0.001812667 -0.0018121118 -0.001811812 -0.0018116077 -0.0018107231 -0.00180546 -0.0017936135 -0.0017785236 -0.0017683008 -0.0017706951][-0.0018156129 -0.0018150457 -0.0018146585 -0.0018141973 -0.0018136809 -0.0018131314 -0.0018125302 -0.0018120743 -0.0018117336 -0.0018117696 -0.0018116824 -0.0018104684 -0.0018080574 -0.0018060835 -0.0018062966][-0.0018160014 -0.0018154592 -0.0018151815 -0.0018148192 -0.0018143863 -0.0018137452 -0.0018130818 -0.0018125753 -0.0018121891 -0.0018120828 -0.0018121846 -0.0018123289 -0.0018122662 -0.0018121224 -0.0018123052]]...]
INFO - root - 2017-12-09 13:06:22.118999: step 25210, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.873 sec/batch; 74h:28m:32s remains)
INFO - root - 2017-12-09 13:06:30.761174: step 25220, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.888 sec/batch; 75h:48m:25s remains)
INFO - root - 2017-12-09 13:06:39.470242: step 25230, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.858 sec/batch; 73h:13m:16s remains)
INFO - root - 2017-12-09 13:06:48.183782: step 25240, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.856 sec/batch; 73h:04m:38s remains)
INFO - root - 2017-12-09 13:06:57.059017: step 25250, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.859 sec/batch; 73h:17m:31s remains)
INFO - root - 2017-12-09 13:07:05.763225: step 25260, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.893 sec/batch; 76h:15m:07s remains)
INFO - root - 2017-12-09 13:07:14.397797: step 25270, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 73h:42m:37s remains)
INFO - root - 2017-12-09 13:07:23.050498: step 25280, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.854 sec/batch; 72h:50m:23s remains)
INFO - root - 2017-12-09 13:07:31.765129: step 25290, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 74h:03m:44s remains)
INFO - root - 2017-12-09 13:07:40.372442: step 25300, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 74h:10m:38s remains)
2017-12-09 13:07:41.222171: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0017514528 -0.0016843078 -0.0015797709 -0.00143267 -0.0012367652 -0.00096723693 -0.00059902854 -0.00013393315 0.00037491031 0.00086191518 0.0012858784 0.0016500481 0.0019710776 0.0022580251 0.002482926][-0.0017827073 -0.0017499538 -0.0016927633 -0.0016018373 -0.0014616388 -0.0012362924 -0.00089296658 -0.000435599 7.8952871e-05 0.00057282567 0.0009981232 0.0013572298 0.0016645327 0.0019187605 0.0020926921][-0.0018041646 -0.0017898589 -0.0017614258 -0.0017083802 -0.0016133561 -0.0014435453 -0.0011673571 -0.00078509573 -0.00034316164 9.1424445e-05 0.00047934719 0.00081022747 0.0010863451 0.0012938337 0.0014133506][-0.001813717 -0.0018086747 -0.0017966324 -0.0017688189 -0.0017096878 -0.0015935852 -0.0013958672 -0.0011147608 -0.00078145496 -0.00044401432 -0.00013281766 0.00013500138 0.00034993549 0.00049013353 0.00055154611][-0.0018156238 -0.0018138792 -0.0018085823 -0.0017948474 -0.0017618703 -0.001691227 -0.001563777 -0.0013764442 -0.001152212 -0.00092584285 -0.00071726658 -0.00054228189 -0.0004109554 -0.00033957919 -0.000320044][-0.0018162722 -0.0018149788 -0.0018113527 -0.0018023903 -0.001780515 -0.0017332536 -0.0016515355 -0.0015374189 -0.0014099329 -0.0012884425 -0.0011804265 -0.0010920165 -0.0010308647 -0.0010058463 -0.0010064233][-0.0018172337 -0.0018163022 -0.0018133254 -0.0018035948 -0.0017797531 -0.0017351528 -0.0016749023 -0.0016119564 -0.0015615609 -0.0015231436 -0.0014893143 -0.0014595746 -0.0014405488 -0.0014361823 -0.0014392047][-0.0018181833 -0.001816768 -0.0018127138 -0.001797923 -0.0017634915 -0.0017084545 -0.0016541535 -0.0016252896 -0.0016294769 -0.001646926 -0.0016589093 -0.0016616947 -0.0016619801 -0.001663622 -0.0016655094][-0.0018169874 -0.001815456 -0.0018091004 -0.0017875467 -0.0017408719 -0.0016755767 -0.0016247131 -0.0016196585 -0.0016573763 -0.0017042519 -0.0017372917 -0.0017533969 -0.0017603711 -0.0017631338 -0.0017640684][-0.001817197 -0.0018156398 -0.0018064838 -0.0017771218 -0.0017210559 -0.0016535181 -0.0016126618 -0.0016238105 -0.0016738159 -0.0017283133 -0.0017667264 -0.0017870551 -0.0017955186 -0.0017975284 -0.0017974835][-0.0018183545 -0.0018156682 -0.0018042205 -0.0017722507 -0.001717814 -0.0016610634 -0.0016354207 -0.0016553137 -0.0017018212 -0.0017474034 -0.0017790314 -0.0017971284 -0.0018050525 -0.0018073288 -0.0018073992][-0.0018203149 -0.001817261 -0.0018055798 -0.0017771875 -0.0017333759 -0.0016930898 -0.0016800902 -0.0016989686 -0.0017330516 -0.0017645539 -0.0017871322 -0.00180091 -0.0018074316 -0.0018096899 -0.00180985][-0.0018219767 -0.0018196468 -0.001810509 -0.0017901233 -0.0017611638 -0.0017370812 -0.0017321932 -0.0017454819 -0.0017655952 -0.0017835397 -0.001796599 -0.0018054489 -0.0018098869 -0.0018117673 -0.0018123168][-0.0018227341 -0.0018220015 -0.001817074 -0.0018059184 -0.0017908508 -0.0017782699 -0.0017753929 -0.0017811082 -0.0017898037 -0.0017975153 -0.0018035901 -0.0018090392 -0.0018121809 -0.0018138037 -0.0018144452][-0.001822482 -0.0018227466 -0.0018210193 -0.0018162887 -0.0018094789 -0.0018036715 -0.0018016442 -0.0018030235 -0.0018055693 -0.0018081294 -0.0018104921 -0.0018130253 -0.0018148129 -0.0018158789 -0.0018164705]]...]
INFO - root - 2017-12-09 13:07:49.737618: step 25310, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.849 sec/batch; 72h:24m:22s remains)
INFO - root - 2017-12-09 13:07:58.462116: step 25320, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.859 sec/batch; 73h:17m:03s remains)
INFO - root - 2017-12-09 13:08:07.224506: step 25330, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.890 sec/batch; 75h:57m:21s remains)
INFO - root - 2017-12-09 13:08:15.828204: step 25340, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 74h:08m:56s remains)
INFO - root - 2017-12-09 13:08:24.493564: step 25350, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.860 sec/batch; 73h:24m:21s remains)
INFO - root - 2017-12-09 13:08:33.304241: step 25360, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.863 sec/batch; 73h:35m:16s remains)
INFO - root - 2017-12-09 13:08:41.895382: step 25370, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.873 sec/batch; 74h:27m:14s remains)
INFO - root - 2017-12-09 13:08:50.672681: step 25380, loss = 0.82, batch loss = 0.69 (8.8 examples/sec; 0.905 sec/batch; 77h:10m:17s remains)
INFO - root - 2017-12-09 13:08:59.475353: step 25390, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.899 sec/batch; 76h:41m:56s remains)
INFO - root - 2017-12-09 13:09:08.145915: step 25400, loss = 0.82, batch loss = 0.69 (8.8 examples/sec; 0.905 sec/batch; 77h:10m:41s remains)
2017-12-09 13:09:08.997043: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.025353039 0.023436625 0.020257128 0.016398307 0.012254725 0.0084147509 0.0053278892 0.0032533072 0.001913987 0.00098986633 0.0001499363 -0.00063660275 -0.0012342646 -0.0015925488 -0.0017561993][0.036767628 0.034666896 0.030793121 0.025928123 0.020442592 0.015029326 0.010326449 0.006809074 0.0043532485 0.0026396783 0.0011814187 -7.9384656e-05 -0.0010087541 -0.0015298265 -0.0017427793][0.048051741 0.046344217 0.042454258 0.037295993 0.031151233 0.02471336 0.01870879 0.013813348 0.01000636 0.0070088324 0.0042757881 0.0018143303 -0.00010891189 -0.0012211184 -0.0016820568][0.056911334 0.056001965 0.052737109 0.048072081 0.042153444 0.03548174 0.028802609 0.022925513 0.017926207 0.013584176 0.00925482 0.0051410119 0.001694288 -0.00047027727 -0.0014791875][0.061564185 0.061579272 0.059473705 0.0560549 0.051279552 0.045346308 0.038966957 0.03288402 0.027167408 0.021613566 0.015566891 0.00953384 0.0042315652 0.00070915942 -0.0010965217][0.06057528 0.061994195 0.061796248 0.060548887 0.05799669 0.053932916 0.048852168 0.04322987 0.037186068 0.030562961 0.022783164 0.014719641 0.007396339 0.0023100241 -0.00050302921][0.054437909 0.057513081 0.059550472 0.060892075 0.061140068 0.059736766 0.056892231 0.052623179 0.046888452 0.039470468 0.030043876 0.019981859 0.010675754 0.0040254989 0.0001655136][0.045926888 0.050502717 0.054513659 0.05807101 0.060733661 0.061786659 0.061237834 0.058680139 0.053782627 0.046179518 0.035734419 0.024225937 0.013420501 0.0055189608 0.00078086916][0.036608744 0.042180803 0.047510169 0.052444864 0.056581203 0.059266552 0.060427163 0.059352942 0.05545003 0.048261967 0.037761506 0.025912803 0.014629878 0.0062302533 0.0010930937][0.02727822 0.033150088 0.038995028 0.044489339 0.049285192 0.0528212 0.054984912 0.05490296 0.051844705 0.045315173 0.035451405 0.024269553 0.013642743 0.0057563493 0.0009340836][0.018503377 0.023966374 0.029552503 0.034885511 0.03966799 0.043447956 0.046043366 0.046550341 0.044232387 0.038649622 0.0300671 0.020336995 0.01117845 0.0044983421 0.00046268513][0.011118758 0.015430411 0.019968798 0.024401205 0.028495079 0.031933311 0.03448369 0.035337809 0.033768956 0.029434746 0.022662554 0.015000159 0.0078917732 0.0028198576 -0.00017279712][0.0057861572 0.0087222354 0.01188983 0.015048824 0.018044215 0.020668736 0.022703812 0.023522403 0.022553852 0.019565422 0.014817541 0.009453414 0.00454485 0.0011447909 -0.00079565751][0.0020796964 0.0038016937 0.0057235505 0.0076961382 0.0096253026 0.011376481 0.012769326 0.013376363 0.012823248 0.010973466 0.0080264574 0.004722178 0.0017466591 -0.00022216688 -0.0012893755][-0.00023157557 0.00060089596 0.0015903198 0.0026743608 0.003796645 0.0048693405 0.0057569155 0.0061824559 0.0059216418 0.0048936638 0.0032568998 0.0014417263 -0.00013633387 -0.0011096306 -0.0015927268]]...]
INFO - root - 2017-12-09 13:09:17.536156: step 25410, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.833 sec/batch; 71h:01m:11s remains)
INFO - root - 2017-12-09 13:09:26.172379: step 25420, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.854 sec/batch; 72h:52m:33s remains)
INFO - root - 2017-12-09 13:09:34.780931: step 25430, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.880 sec/batch; 75h:04m:11s remains)
INFO - root - 2017-12-09 13:09:43.381506: step 25440, loss = 0.82, batch loss = 0.69 (9.8 examples/sec; 0.816 sec/batch; 69h:36m:04s remains)
INFO - root - 2017-12-09 13:09:52.014607: step 25450, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.850 sec/batch; 72h:28m:37s remains)
INFO - root - 2017-12-09 13:10:00.694656: step 25460, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 73h:39m:04s remains)
INFO - root - 2017-12-09 13:10:09.289733: step 25470, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.883 sec/batch; 75h:19m:43s remains)
INFO - root - 2017-12-09 13:10:17.781080: step 25480, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 73h:31m:16s remains)
INFO - root - 2017-12-09 13:10:26.476221: step 25490, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.873 sec/batch; 74h:25m:42s remains)
INFO - root - 2017-12-09 13:10:35.035331: step 25500, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.843 sec/batch; 71h:52m:57s remains)
2017-12-09 13:10:35.922107: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.001829926 -0.001830574 -0.0018312407 -0.0018313708 -0.0018306358 -0.0018294472 -0.0018281658 -0.0018264978 -0.0018245787 -0.001823452 -0.0018243861 -0.0018264739 -0.0018283718 -0.0018292251 -0.0018290209][-0.001829761 -0.0018302535 -0.0018303792 -0.0018292329 -0.0018262884 -0.0018220574 -0.0018176552 -0.0018124449 -0.0018072376 -0.0018050232 -0.0018085922 -0.0018157117 -0.0018225176 -0.0018265649 -0.0018279726][-0.0018298734 -0.0018298377 -0.0018287216 -0.0018241439 -0.0018158535 -0.0018045263 -0.0017926848 -0.0017794415 -0.0017672223 -0.0017627138 -0.0017716202 -0.0017896506 -0.0018077857 -0.001819686 -0.0018252387][-0.0018295769 -0.0018278033 -0.0018227792 -0.0018105807 -0.0017917961 -0.0017683724 -0.0017447995 -0.0017197137 -0.0016972456 -0.0016892105 -0.0017065026 -0.0017426687 -0.0017810414 -0.0018072805 -0.0018202815][-0.0018289186 -0.0018234514 -0.0018102034 -0.0017846504 -0.0017501342 -0.001711795 -0.0016757065 -0.0016387701 -0.0016059061 -0.0015950596 -0.0016237439 -0.0016828544 -0.0017467994 -0.001791356 -0.0018140348][-0.0018279239 -0.0018173649 -0.0017930019 -0.0017511847 -0.0017005354 -0.0016509054 -0.0016081111 -0.0015652187 -0.001526497 -0.0015162596 -0.0015569259 -0.001636759 -0.001721801 -0.0017804777 -0.0018100746][-0.001826979 -0.0018129038 -0.0017809946 -0.0017284366 -0.0016687106 -0.0016163286 -0.0015753971 -0.0015347078 -0.0014973754 -0.001491437 -0.0015399321 -0.0016286923 -0.0017191069 -0.0017799059 -0.0018100179][-0.001826389 -0.0018131157 -0.001782949 -0.0017336792 -0.0016792277 -0.0016348179 -0.0016030776 -0.001571725 -0.0015426185 -0.0015415985 -0.0015873109 -0.0016653062 -0.0017402923 -0.0017888403 -0.001812401][-0.0018258223 -0.001816596 -0.0017956482 -0.001761397 -0.0017237441 -0.0016941756 -0.001674349 -0.0016551504 -0.0016374949 -0.0016391433 -0.0016717048 -0.0017241513 -0.0017720247 -0.0018019426 -0.0018162713][-0.001825442 -0.0018204934 -0.0018097034 -0.0017921862 -0.0017729934 -0.0017584051 -0.0017489322 -0.0017398895 -0.0017317997 -0.0017334715 -0.0017500736 -0.0017757601 -0.0017984021 -0.0018125157 -0.0018194262][-0.001824844 -0.0018228067 -0.0018190289 -0.0018128525 -0.0018060567 -0.0018010495 -0.001797772 -0.0017946091 -0.0017919196 -0.0017925368 -0.0017979551 -0.0018061682 -0.0018135284 -0.0018184751 -0.0018212094][-0.0018240981 -0.0018231923 -0.0018223487 -0.0018209005 -0.0018194529 -0.0018184545 -0.0018177163 -0.0018170079 -0.0018164458 -0.0018165184 -0.0018173631 -0.0018186489 -0.0018199013 -0.0018210117 -0.0018218204][-0.0018233279 -0.0018227475 -0.0018225674 -0.0018223159 -0.0018221936 -0.0018221323 -0.0018220424 -0.0018219547 -0.0018218949 -0.0018218997 -0.001821881 -0.0018217388 -0.0018215838 -0.0018216221 -0.0018217894][-0.001822789 -0.0018221346 -0.0018219862 -0.0018219594 -0.001821976 -0.0018220139 -0.0018219632 -0.0018218971 -0.0018218271 -0.0018218455 -0.0018218782 -0.0018218069 -0.0018217085 -0.0018217028 -0.0018217235][-0.0018223523 -0.0018217178 -0.0018215933 -0.0018216248 -0.0018217148 -0.0018218422 -0.001821845 -0.0018218134 -0.0018217327 -0.0018216963 -0.0018217032 -0.001821654 -0.0018215758 -0.0018215267 -0.0018214755]]...]
INFO - root - 2017-12-09 13:10:44.400272: step 25510, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.859 sec/batch; 73h:16m:37s remains)
INFO - root - 2017-12-09 13:10:53.005811: step 25520, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.850 sec/batch; 72h:29m:00s remains)
INFO - root - 2017-12-09 13:11:01.533677: step 25530, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.839 sec/batch; 71h:31m:49s remains)
INFO - root - 2017-12-09 13:11:10.242430: step 25540, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.858 sec/batch; 73h:10m:55s remains)
INFO - root - 2017-12-09 13:11:18.841195: step 25550, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.882 sec/batch; 75h:10m:09s remains)
INFO - root - 2017-12-09 13:11:27.462025: step 25560, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.838 sec/batch; 71h:28m:26s remains)
INFO - root - 2017-12-09 13:11:36.042391: step 25570, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.853 sec/batch; 72h:44m:59s remains)
INFO - root - 2017-12-09 13:11:44.682267: step 25580, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 73h:30m:11s remains)
INFO - root - 2017-12-09 13:11:53.354140: step 25590, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.857 sec/batch; 73h:04m:53s remains)
INFO - root - 2017-12-09 13:12:01.839183: step 25600, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.832 sec/batch; 70h:56m:02s remains)
2017-12-09 13:12:02.705918: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0018080217 -0.0018086607 -0.0018082486 -0.0018069998 -0.0018046887 -0.0018017372 -0.0017994093 -0.0017985612 -0.0017991676 -0.0018012675 -0.0018043829 -0.0018076563 -0.0018104332 -0.0018126545 -0.0018143001][-0.0018076748 -0.001807831 -0.0018069567 -0.0018055019 -0.0018032024 -0.0018003188 -0.0017981082 -0.0017975691 -0.0017986867 -0.0018013837 -0.0018051014 -0.0018085941 -0.0018110685 -0.0018125734 -0.00181344][-0.0018074487 -0.0018070972 -0.0018059126 -0.0018044993 -0.0018023785 -0.0017998259 -0.0017979763 -0.0017981241 -0.0018002375 -0.0018037995 -0.0018081096 -0.0018115549 -0.0018134605 -0.0018140461 -0.0018139369][-0.0018060721 -0.0018053608 -0.0018040373 -0.0018028736 -0.0018011932 -0.0017993621 -0.001798321 -0.0017993985 -0.0018025227 -0.001806788 -0.0018114299 -0.001814961 -0.0018167429 -0.0018167131 -0.001815655][-0.0018043548 -0.0018031726 -0.0018018369 -0.0018008451 -0.0017996615 -0.001798683 -0.0017985832 -0.0018005511 -0.0018044249 -0.001809109 -0.001813955 -0.0018175527 -0.0018191392 -0.0018185028 -0.0018165981][-0.0018025889 -0.0018011691 -0.0018000153 -0.0017992799 -0.0017985338 -0.0017982672 -0.001798921 -0.001801437 -0.001805841 -0.0018108669 -0.0018156779 -0.0018191135 -0.0018204261 -0.0018195938 -0.0018175726][-0.0018013561 -0.0017996489 -0.0017986248 -0.0017981454 -0.0017977344 -0.0017979224 -0.0017990018 -0.0018017836 -0.0018064774 -0.0018115299 -0.0018159754 -0.0018189313 -0.0018198126 -0.0018188523 -0.001816724][-0.0018003745 -0.0017984083 -0.0017974623 -0.0017971615 -0.0017969445 -0.0017973321 -0.0017985064 -0.0018012325 -0.0018056624 -0.0018102194 -0.0018140062 -0.0018163827 -0.0018170169 -0.0018160268 -0.0018138891][-0.0017996094 -0.0017977756 -0.0017971118 -0.0017969718 -0.001796829 -0.0017971897 -0.0017981365 -0.0018001822 -0.0018034967 -0.0018070225 -0.0018098836 -0.0018117301 -0.0018121338 -0.0018111881 -0.001809283][-0.0017989664 -0.0017972392 -0.00179694 -0.0017970479 -0.0017970171 -0.0017973075 -0.0017979383 -0.0017991414 -0.0018011639 -0.0018034395 -0.0018051867 -0.0018062874 -0.0018064111 -0.0018055864 -0.0018041825][-0.0017987577 -0.0017973081 -0.0017971328 -0.00179734 -0.0017973623 -0.0017974647 -0.0017976218 -0.0017981178 -0.0017992064 -0.0018005278 -0.0018014329 -0.0018018474 -0.0018017109 -0.0018010028 -0.0018000377][-0.0017991364 -0.0017979401 -0.001797854 -0.0017980197 -0.0017979972 -0.0017978789 -0.0017975431 -0.0017974629 -0.0017978017 -0.0017983076 -0.0017986282 -0.00179872 -0.0017985168 -0.0017979115 -0.0017972551][-0.0017995185 -0.0017985675 -0.0017985618 -0.0017986576 -0.0017985284 -0.001798305 -0.0017978087 -0.0017973644 -0.0017972143 -0.0017971783 -0.001797115 -0.0017970491 -0.0017968863 -0.0017963219 -0.0017959452][-0.0017997569 -0.0017988922 -0.0017988845 -0.0017989806 -0.0017988622 -0.0017986341 -0.0017982147 -0.0017977179 -0.0017974349 -0.0017972775 -0.0017971154 -0.0017969793 -0.001796853 -0.0017963572 -0.0017960363][-0.0018000772 -0.0017991737 -0.0017990362 -0.0017990912 -0.0017990075 -0.001798895 -0.0017986512 -0.0017983214 -0.0017981492 -0.0017980877 -0.0017980338 -0.0017979461 -0.0017978656 -0.0017974869 -0.0017971671]]...]
INFO - root - 2017-12-09 13:12:11.156183: step 25610, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.849 sec/batch; 72h:22m:03s remains)
INFO - root - 2017-12-09 13:12:19.825541: step 25620, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.854 sec/batch; 72h:49m:33s remains)
INFO - root - 2017-12-09 13:12:28.520922: step 25630, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.881 sec/batch; 75h:06m:16s remains)
INFO - root - 2017-12-09 13:12:37.272462: step 25640, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.882 sec/batch; 75h:12m:31s remains)
INFO - root - 2017-12-09 13:12:45.867058: step 25650, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.844 sec/batch; 71h:57m:07s remains)
INFO - root - 2017-12-09 13:12:54.641217: step 25660, loss = 0.81, batch loss = 0.68 (9.3 examples/sec; 0.860 sec/batch; 73h:16m:28s remains)
INFO - root - 2017-12-09 13:13:03.104364: step 25670, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.846 sec/batch; 72h:06m:11s remains)
INFO - root - 2017-12-09 13:13:11.795225: step 25680, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.851 sec/batch; 72h:30m:49s remains)
INFO - root - 2017-12-09 13:13:20.378146: step 25690, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 74h:13m:07s remains)
INFO - root - 2017-12-09 13:13:29.054940: step 25700, loss = 0.81, batch loss = 0.68 (8.1 examples/sec; 0.991 sec/batch; 84h:29m:36s remains)
2017-12-09 13:13:29.951934: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.28345519 0.29613033 0.30603009 0.31225404 0.31596494 0.31758624 0.31817952 0.31701332 0.31440306 0.30985811 0.30444866 0.29816779 0.2922729 0.2867201 0.28180686][0.28065744 0.29723823 0.31099549 0.32121289 0.32759082 0.33078918 0.33239603 0.3310602 0.32809746 0.3233268 0.31832653 0.31203103 0.30573756 0.3000426 0.29466718][0.27219787 0.29163918 0.30882087 0.32313511 0.33294719 0.33885846 0.34230191 0.34192455 0.33946002 0.33474582 0.32987684 0.32337797 0.31663719 0.3098537 0.30284902][0.26180282 0.28414106 0.30441639 0.32264447 0.33624902 0.34497651 0.34927657 0.35034466 0.34884322 0.34487182 0.34079871 0.33430132 0.32745877 0.31882039 0.30964771][0.25107166 0.27542138 0.29748791 0.31846946 0.33519089 0.3469328 0.35368204 0.35597315 0.35518113 0.35261232 0.34861949 0.34175658 0.33402124 0.32411176 0.31349194][0.24102646 0.26613984 0.28916249 0.31175044 0.33036259 0.34459838 0.35318452 0.35701734 0.3575086 0.3555769 0.35142952 0.3438535 0.33506477 0.32391027 0.31198451][0.23212221 0.2569606 0.27999389 0.30323237 0.32298955 0.33840975 0.34832641 0.35358557 0.354964 0.35359791 0.34921557 0.34132689 0.33188832 0.31981751 0.30717975][0.22622006 0.25091916 0.27330536 0.29519141 0.31440067 0.32949865 0.33864278 0.34380198 0.34514898 0.34385374 0.33896729 0.33085713 0.32196248 0.31044543 0.29875934][0.22049306 0.24487238 0.26622057 0.28665251 0.30450419 0.31801507 0.32546753 0.3287774 0.32857433 0.32671627 0.32137311 0.31368551 0.30628967 0.29750586 0.28892863][0.21696709 0.23987019 0.25917637 0.27735412 0.29249671 0.30354711 0.30882782 0.30900776 0.306439 0.30261078 0.29649523 0.28977096 0.28433558 0.27987286 0.27578861][0.21240574 0.23346278 0.2504974 0.26591545 0.27782017 0.28572148 0.2877624 0.28526995 0.28052923 0.2741029 0.26749849 0.2616587 0.25862041 0.25739422 0.25682592][0.2084455 0.22812223 0.24290633 0.25512567 0.26333478 0.2672129 0.265536 0.25982752 0.25223842 0.243836 0.23622534 0.2309155 0.2292442 0.23021451 0.23227522][0.20334135 0.22148809 0.23419775 0.24328631 0.24810229 0.24847013 0.24352588 0.23456749 0.22391528 0.21324575 0.20406492 0.19801952 0.196416 0.19827504 0.20167641][0.1963174 0.21272695 0.22313964 0.22976409 0.23173325 0.22930296 0.22209735 0.21097383 0.19813348 0.18542385 0.17445464 0.16689563 0.16369657 0.16443373 0.1672273][0.18829332 0.20310293 0.21180493 0.21643509 0.21625963 0.21180263 0.20268385 0.19020249 0.17558677 0.16087525 0.14766234 0.13760005 0.13175768 0.12966424 0.13010775]]...]
INFO - root - 2017-12-09 13:13:38.412167: step 25710, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.849 sec/batch; 72h:20m:39s remains)
INFO - root - 2017-12-09 13:13:47.005590: step 25720, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.848 sec/batch; 72h:16m:51s remains)
INFO - root - 2017-12-09 13:13:55.650813: step 25730, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.875 sec/batch; 74h:35m:48s remains)
INFO - root - 2017-12-09 13:14:04.273471: step 25740, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.843 sec/batch; 71h:51m:37s remains)
INFO - root - 2017-12-09 13:14:12.904169: step 25750, loss = 0.83, batch loss = 0.70 (9.4 examples/sec; 0.853 sec/batch; 72h:39m:07s remains)
INFO - root - 2017-12-09 13:14:21.487790: step 25760, loss = 0.82, batch loss = 0.69 (9.8 examples/sec; 0.818 sec/batch; 69h:42m:31s remains)
INFO - root - 2017-12-09 13:14:30.108096: step 25770, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.893 sec/batch; 76h:04m:25s remains)
INFO - root - 2017-12-09 13:14:38.824431: step 25780, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.894 sec/batch; 76h:10m:31s remains)
INFO - root - 2017-12-09 13:14:47.581198: step 25790, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 73h:38m:13s remains)
INFO - root - 2017-12-09 13:14:56.132328: step 25800, loss = 0.83, batch loss = 0.70 (9.3 examples/sec; 0.857 sec/batch; 73h:00m:34s remains)
2017-12-09 13:14:57.088048: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.045434821 0.045405343 0.04480375 0.043285217 0.041270774 0.03878247 0.03609458 0.03393627 0.03224409 0.031727403 0.031979691 0.033387303 0.035713553 0.037767041 0.039889146][0.043872122 0.043254748 0.04184043 0.039587285 0.03704248 0.034120031 0.031285428 0.028855762 0.027328532 0.0273449 0.028370118 0.030707425 0.03378208 0.03701089 0.039827347][0.041774325 0.040527984 0.038242716 0.035211775 0.032077841 0.028909406 0.02621112 0.023962028 0.022899805 0.023260986 0.025059985 0.028063644 0.031773895 0.0357923 0.039340727][0.040146437 0.03806334 0.034801695 0.031005843 0.027423223 0.024059294 0.021528717 0.019784948 0.019444248 0.020341117 0.022915421 0.026413824 0.030691573 0.035065405 0.038832758][0.039508458 0.036764693 0.0325812 0.028171392 0.024328712 0.021160869 0.01903927 0.017824478 0.01813532 0.019820407 0.023109889 0.02704804 0.031579562 0.035957173 0.039733436][0.039528068 0.03641871 0.031817771 0.027062435 0.023062954 0.020070309 0.018217508 0.017415874 0.01815797 0.02051531 0.024445359 0.028903093 0.03356282 0.037747283 0.041467022][0.03977542 0.036773376 0.032166991 0.02749061 0.023482952 0.020481266 0.018644255 0.017917896 0.018765865 0.021356273 0.025578216 0.030379152 0.035047118 0.039165869 0.042550519][0.040005792 0.03776988 0.033873256 0.029727658 0.025979498 0.023008838 0.021080079 0.020359874 0.021162827 0.023607679 0.027467651 0.031854577 0.036041945 0.03954307 0.042229764][0.040297389 0.038988195 0.035850424 0.032501023 0.02936168 0.0265852 0.024635185 0.023849502 0.024461158 0.026446793 0.029590925 0.033162016 0.036463968 0.039021891 0.04070228][0.039548796 0.039036382 0.036666684 0.034093186 0.031573582 0.029398067 0.027762318 0.026929758 0.027248714 0.028629445 0.030777534 0.0331641 0.035302363 0.036925815 0.037748564][0.036986578 0.037091948 0.035414357 0.033553917 0.031710837 0.030101435 0.028852675 0.028097466 0.028023684 0.028582009 0.029683257 0.030985394 0.032092158 0.032860305 0.033041965][0.032069344 0.03254756 0.03143565 0.030133145 0.028793545 0.027640162 0.026633553 0.025844486 0.025391771 0.025273694 0.025525333 0.026102792 0.026692457 0.027107183 0.027001986][0.02506314 0.025714226 0.025020806 0.024161849 0.023290362 0.022431463 0.021564808 0.020728484 0.020042241 0.01961053 0.019493274 0.01977879 0.020169284 0.020491406 0.020390838][0.017271824 0.017875388 0.017478853 0.01696985 0.016425582 0.015866924 0.015201583 0.014472096 0.013823631 0.013340222 0.013139504 0.013296761 0.013616135 0.013915592 0.013862674][0.010324784 0.010827547 0.010626734 0.010359766 0.010065746 0.0097430591 0.0093140928 0.0088144494 0.00835021 0.0080038412 0.0078475922 0.007891031 0.00804072 0.0082244547 0.008187972]]...]
INFO - root - 2017-12-09 13:15:05.550288: step 25810, loss = 0.82, batch loss = 0.69 (10.4 examples/sec; 0.769 sec/batch; 65h:30m:09s remains)
INFO - root - 2017-12-09 13:15:14.147368: step 25820, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.863 sec/batch; 73h:32m:54s remains)
INFO - root - 2017-12-09 13:15:22.885944: step 25830, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 74h:01m:53s remains)
INFO - root - 2017-12-09 13:15:31.455818: step 25840, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.855 sec/batch; 72h:48m:42s remains)
INFO - root - 2017-12-09 13:15:40.044564: step 25850, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.854 sec/batch; 72h:46m:00s remains)
INFO - root - 2017-12-09 13:15:48.763215: step 25860, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 73h:22m:54s remains)
INFO - root - 2017-12-09 13:15:57.191414: step 25870, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.886 sec/batch; 75h:26m:21s remains)
INFO - root - 2017-12-09 13:16:05.828591: step 25880, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.838 sec/batch; 71h:21m:05s remains)
INFO - root - 2017-12-09 13:16:14.474063: step 25890, loss = 0.81, batch loss = 0.68 (9.1 examples/sec; 0.876 sec/batch; 74h:35m:36s remains)
INFO - root - 2017-12-09 13:16:22.923109: step 25900, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.859 sec/batch; 73h:09m:11s remains)
2017-12-09 13:16:23.790826: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0017805599 -0.0017790868 -0.0017792017 -0.0017794551 -0.0017797084 -0.0017799202 -0.0017799439 -0.0017799117 -0.0017799365 -0.0017800839 -0.0017802509 -0.0017804118 -0.0017804395 -0.0017803273 -0.0017801158][-0.0017790318 -0.0017775835 -0.0017778195 -0.0017780695 -0.0017782542 -0.0017783559 -0.0017782801 -0.0017781615 -0.0017782267 -0.0017785389 -0.0017788816 -0.0017792183 -0.0017793913 -0.0017793258 -0.0017791137][-0.0017786024 -0.0017772289 -0.0017775608 -0.0017778591 -0.0017780052 -0.0017779619 -0.0017776698 -0.001777412 -0.001777496 -0.0017778523 -0.0017783007 -0.0017787991 -0.0017791209 -0.0017791226 -0.0017789243][-0.0017788995 -0.0017776837 -0.001778017 -0.0017783096 -0.001778452 -0.0017783022 -0.0017777185 -0.0017771836 -0.0017771348 -0.0017774088 -0.001777827 -0.0017783628 -0.00177878 -0.0017788683 -0.0017787667][-0.001779786 -0.0017787978 -0.00177916 -0.0017794162 -0.0017795494 -0.0017793217 -0.0017785014 -0.0017776586 -0.001777296 -0.0017773282 -0.0017774587 -0.0017777418 -0.001777952 -0.0017779968 -0.0017780168][-0.0017804881 -0.0017798617 -0.0017804698 -0.0017809425 -0.0017812601 -0.0017810399 -0.0017800468 -0.0017788922 -0.001778069 -0.0017776816 -0.0017773805 -0.0017771845 -0.0017769434 -0.0017767911 -0.0017768824][-0.001781988 -0.001781862 -0.0017828742 -0.0017839267 -0.0017847677 -0.0017847288 -0.0017836536 -0.0017820711 -0.0017806763 -0.0017795656 -0.0017785443 -0.0017776357 -0.0017767961 -0.0017762819 -0.0017762688][-0.0017854423 -0.0017857888 -0.0017874277 -0.0017893184 -0.0017909366 -0.0017913602 -0.0017903824 -0.0017883806 -0.0017862043 -0.001784089 -0.0017819938 -0.0017800593 -0.0017784152 -0.0017772298 -0.0017767679][-0.0017898029 -0.0017905711 -0.0017927248 -0.0017952665 -0.0017976059 -0.0017985664 -0.0017980227 -0.0017960869 -0.0017932191 -0.0017899835 -0.0017866348 -0.0017834574 -0.0017806705 -0.0017785536 -0.0017774468][-0.0017939083 -0.0017946528 -0.0017968601 -0.0017997591 -0.001802509 -0.0018039154 -0.0018038347 -0.001802051 -0.0017987876 -0.0017947096 -0.0017903292 -0.0017861259 -0.0017823863 -0.0017795473 -0.0017779288][-0.0017955401 -0.0017959859 -0.0017981054 -0.0018012207 -0.0018043082 -0.0018060276 -0.0018061858 -0.0018044402 -0.0018010422 -0.0017964928 -0.0017915536 -0.0017869135 -0.0017828004 -0.0017797374 -0.0017779262][-0.0017953378 -0.0017954274 -0.0017972321 -0.001800266 -0.0018034887 -0.0018053864 -0.0018055681 -0.0018038087 -0.0018004618 -0.001795826 -0.0017907009 -0.0017860645 -0.0017820671 -0.0017791237 -0.0017774802][-0.0017940179 -0.0017938107 -0.0017952448 -0.0017978025 -0.0018007255 -0.0018026679 -0.0018029545 -0.0018014171 -0.0017983422 -0.001794121 -0.0017894729 -0.0017852047 -0.0017814894 -0.0017787685 -0.0017773209][-0.0017913876 -0.0017906798 -0.001791489 -0.0017932492 -0.0017954346 -0.0017970986 -0.001797401 -0.0017961758 -0.0017937127 -0.0017903891 -0.001786792 -0.0017833755 -0.0017802485 -0.0017780303 -0.001776946][-0.0017875518 -0.0017864574 -0.0017867379 -0.0017879116 -0.0017894974 -0.0017907348 -0.0017908395 -0.0017897728 -0.0017879136 -0.0017855304 -0.0017830194 -0.0017806214 -0.0017784188 -0.00177698 -0.0017764427]]...]
INFO - root - 2017-12-09 13:16:32.444174: step 25910, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.851 sec/batch; 72h:25m:59s remains)
INFO - root - 2017-12-09 13:16:40.986683: step 25920, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.853 sec/batch; 72h:37m:46s remains)
INFO - root - 2017-12-09 13:16:49.673486: step 25930, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.884 sec/batch; 75h:16m:47s remains)
INFO - root - 2017-12-09 13:16:58.202919: step 25940, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 74h:01m:01s remains)
INFO - root - 2017-12-09 13:17:07.023741: step 25950, loss = 0.83, batch loss = 0.70 (9.2 examples/sec; 0.872 sec/batch; 74h:16m:57s remains)
INFO - root - 2017-12-09 13:17:15.696934: step 25960, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.883 sec/batch; 75h:12m:51s remains)
INFO - root - 2017-12-09 13:17:23.965944: step 25970, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.876 sec/batch; 74h:36m:48s remains)
INFO - root - 2017-12-09 13:17:32.573564: step 25980, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.851 sec/batch; 72h:24m:56s remains)
INFO - root - 2017-12-09 13:17:41.176073: step 25990, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.835 sec/batch; 71h:05m:10s remains)
INFO - root - 2017-12-09 13:17:49.683386: step 26000, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.866 sec/batch; 73h:45m:19s remains)
2017-12-09 13:17:50.503855: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.010558506 0.019151324 0.029911708 0.041663866 0.052781224 0.061770037 0.067719854 0.070852816 0.07200256 0.072572052 0.073585652 0.074717432 0.074089944 0.069213659 0.059161942][0.0080557633 0.016494256 0.028142227 0.041856155 0.055563327 0.066863865 0.073857427 0.076164812 0.074730352 0.071516179 0.068329833 0.065760955 0.062738024 0.057248347 0.048286706][0.0056785848 0.013838411 0.026290163 0.041985016 0.058356471 0.07199309 0.079895742 0.080938153 0.076143578 0.06821657 0.059951976 0.052962273 0.047067516 0.040881615 0.033372149][0.0038375759 0.011868436 0.02530878 0.043246362 0.062623292 0.078902818 0.087911457 0.087677933 0.079248339 0.066140965 0.052403688 0.040807389 0.031999543 0.025131512 0.019071793][0.0030640641 0.011140179 0.025720639 0.046055708 0.068596333 0.087776132 0.098213986 0.097021632 0.085212983 0.06717398 0.048287135 0.032465693 0.021106981 0.01369892 0.008846404][0.0034178847 0.011498869 0.02682887 0.048834872 0.073650926 0.0950128 0.10662466 0.10484756 0.090549283 0.06880632 0.046126671 0.027279031 0.014222226 0.0066482378 0.0028276748][0.0041900994 0.011953101 0.02699619 0.049005471 0.07416375 0.096069142 0.10807329 0.10610727 0.090829991 0.067608632 0.043469716 0.023587275 0.010184743 0.0029902782 2.394279e-05][0.0044183042 0.011373952 0.024933426 0.045011584 0.068212286 0.088642463 0.099962085 0.098125964 0.083590291 0.061472297 0.038545448 0.019807164 0.0074600847 0.0011937714 -0.0010316316][0.0037876491 0.0094059221 0.020380875 0.036842339 0.056086965 0.073232822 0.08285021 0.081399873 0.069172561 0.050472196 0.031102162 0.015365689 0.0051812287 0.00020988262 -0.0013907873][0.0026628322 0.0066914465 0.014526294 0.026383806 0.040383562 0.052987896 0.060113955 0.05908256 0.050068993 0.03624291 0.02192628 0.010361918 0.0030119808 -0.00045752258 -0.0014994252][0.0012403 0.0037331812 0.0085112993 0.01580248 0.024498938 0.03239876 0.036876436 0.036187623 0.03043982 0.021664528 0.012629598 0.0054252893 0.00097407121 -0.0010250246 -0.0015716669][-0.00023750274 0.0010467318 0.003445064 0.007143653 0.011622243 0.015746957 0.0180904 0.017707676 0.014657945 0.010025441 0.0053040013 0.0016315165 -0.00052584417 -0.0014188983 -0.0016284654][-0.0012930464 -0.00080652616 0.00012018532 0.0015919842 0.0034331467 0.0051800162 0.0061927 0.0060564028 0.0047957245 0.0028690214 0.00092325639 -0.00053911412 -0.0013340572 -0.0016205795 -0.0016692063][-0.0017088641 -0.0016135984 -0.001396487 -0.000997867 -0.00045862631 7.8172656e-05 0.000401285 0.00038241432 2.7908129e-05 -0.00052848167 -0.0010820634 -0.0014743489 -0.0016578165 -0.0017043005 -0.0017016532][-0.0017523476 -0.0017501173 -0.0017340435 -0.0016895779 -0.0016130994 -0.001524767 -0.0014629809 -0.0014512488 -0.0014928679 -0.0015708935 -0.0016533703 -0.0017096782 -0.0017298392 -0.0017292635 -0.0017238616]]...]
INFO - root - 2017-12-09 13:17:59.143832: step 26010, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.873 sec/batch; 74h:17m:39s remains)
INFO - root - 2017-12-09 13:18:07.788491: step 26020, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.872 sec/batch; 74h:15m:38s remains)
INFO - root - 2017-12-09 13:18:16.490309: step 26030, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.877 sec/batch; 74h:37m:10s remains)
INFO - root - 2017-12-09 13:18:25.248437: step 26040, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.873 sec/batch; 74h:21m:25s remains)
INFO - root - 2017-12-09 13:18:33.886860: step 26050, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.877 sec/batch; 74h:40m:03s remains)
INFO - root - 2017-12-09 13:18:42.601435: step 26060, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.831 sec/batch; 70h:42m:49s remains)
INFO - root - 2017-12-09 13:18:51.174900: step 26070, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.852 sec/batch; 72h:32m:34s remains)
INFO - root - 2017-12-09 13:18:59.673054: step 26080, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.856 sec/batch; 72h:51m:43s remains)
INFO - root - 2017-12-09 13:19:08.263298: step 26090, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.877 sec/batch; 74h:39m:01s remains)
INFO - root - 2017-12-09 13:19:16.673296: step 26100, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.873 sec/batch; 74h:16m:57s remains)
2017-12-09 13:19:17.573669: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.03698507 0.039293416 0.042440578 0.046591755 0.049420312 0.051088382 0.050865732 0.048121627 0.044287082 0.040637922 0.038485423 0.037621144 0.038820691 0.0441331 0.052966435][0.048126489 0.050958604 0.054157339 0.058660552 0.062461372 0.065507494 0.066366829 0.064296164 0.060172882 0.0552231 0.05152899 0.049488325 0.050753903 0.0569926 0.067614265][0.061941568 0.065437041 0.069740593 0.07554552 0.081388496 0.087067515 0.089805424 0.08891356 0.084184676 0.076921247 0.070542313 0.066788889 0.06744574 0.073512651 0.084644027][0.077940293 0.082607836 0.087764718 0.095392138 0.10346704 0.11185147 0.11666068 0.11696414 0.11222681 0.10384791 0.095715694 0.090117164 0.089814574 0.095526785 0.10598113][0.093892112 0.10007861 0.10685171 0.11608238 0.12584992 0.13624315 0.14232528 0.14337888 0.13895707 0.12999003 0.12075993 0.11403503 0.11280569 0.11791407 0.12744437][0.1075759 0.115377 0.12415799 0.13548167 0.1468824 0.15803865 0.16417238 0.16504434 0.16040336 0.15150766 0.14252904 0.13582063 0.13479681 0.13949975 0.14795355][0.11720687 0.12723432 0.13842405 0.15175733 0.16445561 0.17564927 0.1813352 0.18117364 0.17586094 0.16734737 0.1592484 0.15390074 0.15399313 0.15881988 0.16665028][0.12128606 0.13362947 0.14709146 0.16244295 0.17627129 0.18742338 0.19228545 0.19115596 0.18543775 0.17726327 0.17038184 0.16662614 0.16825236 0.17383064 0.181083][0.1234307 0.13761017 0.15310623 0.16975465 0.18419771 0.19517812 0.19957075 0.19771399 0.19123113 0.18341886 0.17728099 0.17475344 0.17740175 0.183365 0.19079554][0.12365386 0.13936651 0.15640774 0.17414351 0.18901844 0.19931076 0.20258071 0.19984709 0.19238997 0.18455434 0.17920877 0.17795034 0.18177749 0.18861485 0.19703563][0.12237705 0.13984403 0.15788867 0.1759285 0.19071729 0.2003561 0.20253859 0.19870524 0.19091935 0.18366957 0.1797173 0.18019289 0.18566345 0.19387099 0.20320471][0.12102468 0.13899578 0.15696061 0.17463747 0.18856074 0.19706003 0.198334 0.19409111 0.18659987 0.18044452 0.17831148 0.18064681 0.18758167 0.19699645 0.20705293][0.11962865 0.13691162 0.15380214 0.17035869 0.18317345 0.19027385 0.1906679 0.18636517 0.17946385 0.17463799 0.17436676 0.1788412 0.18690445 0.1968769 0.20686975][0.11776941 0.13246086 0.14635728 0.1603473 0.17156932 0.17776223 0.17785069 0.17422783 0.16870606 0.16541208 0.16634052 0.17173336 0.1803581 0.19050775 0.20029762][0.11627763 0.12786695 0.13810009 0.14875251 0.15740529 0.16197662 0.16187347 0.15882994 0.15491429 0.15321541 0.15551805 0.16159779 0.17013179 0.17984067 0.18852155]]...]
INFO - root - 2017-12-09 13:19:26.254718: step 26110, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.855 sec/batch; 72h:47m:10s remains)
INFO - root - 2017-12-09 13:19:34.717494: step 26120, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 73h:22m:41s remains)
INFO - root - 2017-12-09 13:19:43.242433: step 26130, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.841 sec/batch; 71h:32m:29s remains)
INFO - root - 2017-12-09 13:19:51.947860: step 26140, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.863 sec/batch; 73h:24m:21s remains)
INFO - root - 2017-12-09 13:20:00.545257: step 26150, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.845 sec/batch; 71h:53m:31s remains)
INFO - root - 2017-12-09 13:20:09.291119: step 26160, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.890 sec/batch; 75h:44m:06s remains)
INFO - root - 2017-12-09 13:20:17.758480: step 26170, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.836 sec/batch; 71h:10m:12s remains)
INFO - root - 2017-12-09 13:20:26.453203: step 26180, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.834 sec/batch; 70h:59m:57s remains)
INFO - root - 2017-12-09 13:20:35.110290: step 26190, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 73h:20m:45s remains)
INFO - root - 2017-12-09 13:20:43.544552: step 26200, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.843 sec/batch; 71h:44m:53s remains)
2017-12-09 13:20:44.414212: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.00183212 -0.0018317498 -0.0018317265 -0.0018317027 -0.0018316484 -0.001831588 -0.0018315477 -0.0018315382 -0.0018315314 -0.001831534 -0.001831517 -0.0018314961 -0.0018314562 -0.0018313848 -0.001831318][-0.0018320627 -0.0018316363 -0.0018315576 -0.0018314818 -0.0018313694 -0.0018312584 -0.0018312298 -0.0018312604 -0.0018312966 -0.0018313547 -0.0018313703 -0.0018313146 -0.0018312106 -0.0018310806 -0.0018309598][-0.0018325852 -0.0018321293 -0.0018320059 -0.001831886 -0.0018317132 -0.0018315801 -0.0018315909 -0.0018317178 -0.0018318584 -0.0018320004 -0.0018320589 -0.0018319577 -0.0018317574 -0.0018315044 -0.0018312759][-0.0018334053 -0.0018328672 -0.0018326711 -0.0018325192 -0.0018323285 -0.001832213 -0.0018322846 -0.0018325029 -0.0018327679 -0.0018330013 -0.0018330965 -0.0018329371 -0.0018325953 -0.0018321387 -0.0018317616][-0.0018340732 -0.001833552 -0.0018333369 -0.0018332349 -0.0018331693 -0.0018331531 -0.0018332951 -0.0018335219 -0.0018338129 -0.0018340786 -0.0018341293 -0.0018338679 -0.0018333301 -0.001832673 -0.0018321495][-0.0018344689 -0.0018340941 -0.0018340392 -0.0018340859 -0.0018341197 -0.0018340957 -0.0018342189 -0.0018343917 -0.0018346597 -0.0018349442 -0.0018349518 -0.0018346393 -0.0018340631 -0.0018333128 -0.001832697][-0.0018347602 -0.0018345453 -0.0018346369 -0.0018348626 -0.0018349823 -0.0018348808 -0.0018349107 -0.001835013 -0.0018352407 -0.0018354317 -0.0018354747 -0.0018352567 -0.0018347222 -0.0018340509 -0.0018335267][-0.0018351172 -0.0018349303 -0.0018351049 -0.0018355075 -0.0018357809 -0.0018356983 -0.0018357189 -0.0018359735 -0.0018363585 -0.001836502 -0.001836546 -0.0018363995 -0.0018358873 -0.0018351643 -0.0018346508][-0.0018352692 -0.001835265 -0.0018356197 -0.0018362689 -0.0018368706 -0.0018370731 -0.0018372288 -0.0018375192 -0.0018379627 -0.0018380388 -0.0018379244 -0.0018376078 -0.0018370223 -0.0018363089 -0.0018357623][-0.0018347951 -0.0018351898 -0.0018360267 -0.0018370417 -0.001838037 -0.0018385326 -0.0018388462 -0.0018390174 -0.0018392208 -0.0018390657 -0.0018387181 -0.0018382929 -0.0018376722 -0.0018370727 -0.0018365664][-0.0018338675 -0.0018345655 -0.0018357447 -0.0018371217 -0.0018383658 -0.0018390609 -0.0018393827 -0.0018394237 -0.0018393223 -0.0018389454 -0.0018385907 -0.0018383252 -0.0018380195 -0.0018377472 -0.0018374276][-0.0018327731 -0.0018335509 -0.0018348717 -0.0018364479 -0.0018378751 -0.0018387289 -0.0018391449 -0.0018393026 -0.0018391829 -0.0018386807 -0.0018383688 -0.0018383285 -0.0018383123 -0.0018382637 -0.0018381372][-0.0018317489 -0.0018324968 -0.0018337672 -0.0018353882 -0.0018369894 -0.0018380858 -0.0018387792 -0.0018392794 -0.001839421 -0.0018391191 -0.0018388978 -0.0018389691 -0.0018390225 -0.0018388956 -0.0018386224][-0.0018312875 -0.0018319907 -0.0018331073 -0.0018345907 -0.0018362092 -0.001837504 -0.0018384575 -0.001839148 -0.0018395265 -0.0018394404 -0.001839245 -0.0018392155 -0.0018391211 -0.0018388506 -0.0018384104][-0.0018315042 -0.0018321306 -0.0018330889 -0.0018343291 -0.0018357572 -0.0018370757 -0.0018382042 -0.0018389593 -0.00183933 -0.0018393451 -0.0018391772 -0.001839007 -0.001838788 -0.0018384503 -0.0018379866]]...]
INFO - root - 2017-12-09 13:20:53.136778: step 26210, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.854 sec/batch; 72h:37m:44s remains)
INFO - root - 2017-12-09 13:21:01.730600: step 26220, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.855 sec/batch; 72h:43m:33s remains)
INFO - root - 2017-12-09 13:21:10.352594: step 26230, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.893 sec/batch; 75h:58m:46s remains)
INFO - root - 2017-12-09 13:21:19.007104: step 26240, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.836 sec/batch; 71h:07m:01s remains)
INFO - root - 2017-12-09 13:21:27.615019: step 26250, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 73h:46m:18s remains)
INFO - root - 2017-12-09 13:21:36.330153: step 26260, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.884 sec/batch; 75h:09m:45s remains)
INFO - root - 2017-12-09 13:21:44.881634: step 26270, loss = 0.82, batch loss = 0.69 (9.7 examples/sec; 0.828 sec/batch; 70h:26m:00s remains)
INFO - root - 2017-12-09 13:21:53.462069: step 26280, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.879 sec/batch; 74h:46m:54s remains)
INFO - root - 2017-12-09 13:22:02.033927: step 26290, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.841 sec/batch; 71h:32m:59s remains)
INFO - root - 2017-12-09 13:22:10.579773: step 26300, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.836 sec/batch; 71h:07m:35s remains)
2017-12-09 13:22:11.436615: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0018293934 -0.0018285437 -0.0018255989 -0.0018157721 -0.0018021147 -0.0017910741 -0.0017811342 -0.0017605198 -0.001728221 -0.0016992345 -0.0016999292 -0.0017295282 -0.0017690941 -0.0017992525 -0.001816389][-0.0018286364 -0.0018275238 -0.0018233181 -0.0018132671 -0.0017993507 -0.0017685952 -0.0017331337 -0.0016848814 -0.0016336291 -0.0016006099 -0.0016135749 -0.001665635 -0.001730946 -0.0017798764 -0.0018071337][-0.0018283966 -0.0018247422 -0.0018147663 -0.0017927475 -0.0017623048 -0.001717345 -0.0016659765 -0.0015906147 -0.0015113939 -0.0014561313 -0.0014748189 -0.0015587122 -0.0016658587 -0.0017484942 -0.0017949098][-0.0018290041 -0.001820809 -0.0017980419 -0.0017492847 -0.0016897882 -0.0016098497 -0.0015300654 -0.0014365903 -0.0013575077 -0.0013153045 -0.0013546416 -0.0014647244 -0.0016040301 -0.0017177056 -0.0017837293][-0.0018300568 -0.0018203654 -0.0017886547 -0.0017173134 -0.0016254956 -0.0015091816 -0.0013986592 -0.0012843988 -0.0012103496 -0.0011865839 -0.0012492981 -0.0013856234 -0.0015500098 -0.0016905804 -0.001773527][-0.0018305589 -0.0018232977 -0.0017941385 -0.0017201782 -0.0016154886 -0.001477331 -0.0013461539 -0.0012254587 -0.0011701649 -0.0011763353 -0.0012609104 -0.0014036875 -0.0015615856 -0.0016968828 -0.001776512][-0.0018307485 -0.0018267818 -0.0018110239 -0.0017625216 -0.0016767083 -0.0015427943 -0.0014076113 -0.0012875183 -0.001240795 -0.001268032 -0.0013642439 -0.0014982419 -0.001628343 -0.0017327353 -0.0017914303][-0.0018304909 -0.0018274166 -0.0018188421 -0.0017877263 -0.0017235831 -0.0016062506 -0.0014764054 -0.0013681022 -0.0013383515 -0.0013852811 -0.0014873912 -0.0016079933 -0.0017071503 -0.0017755805 -0.0018095403][-0.0018291764 -0.0018222234 -0.0018111339 -0.0017866208 -0.0017454635 -0.0016649534 -0.0015680061 -0.0014823186 -0.0014617122 -0.0015079388 -0.0015965838 -0.0016928171 -0.0017622039 -0.0018023171 -0.0018194261][-0.001827663 -0.0018166348 -0.0017995308 -0.0017730655 -0.0017436642 -0.0016936703 -0.0016378938 -0.0015894653 -0.0015849745 -0.0016246249 -0.0016897907 -0.0017541707 -0.0017952715 -0.0018159479 -0.0018237261][-0.0018264353 -0.0018137258 -0.0017951501 -0.0017732366 -0.0017610888 -0.001743837 -0.0017265147 -0.0017094457 -0.001712109 -0.0017344333 -0.0017675477 -0.0017977185 -0.0018153507 -0.0018234432 -0.0018263137][-0.0018276455 -0.0018192622 -0.0018085184 -0.0017974682 -0.0017935523 -0.001788932 -0.0017861102 -0.001783983 -0.0017894254 -0.0018002463 -0.0018120424 -0.0018206491 -0.0018248464 -0.001826594 -0.0018272246][-0.0018289785 -0.0018260428 -0.0018218461 -0.001817803 -0.0018166448 -0.0018164114 -0.0018172283 -0.0018183945 -0.0018211188 -0.0018240038 -0.0018261196 -0.0018271464 -0.0018274805 -0.0018276556 -0.0018277757][-0.0018294216 -0.0018290543 -0.001828883 -0.0018285813 -0.0018280025 -0.0018272882 -0.0018267776 -0.0018268537 -0.0018273219 -0.0018276845 -0.0018278288 -0.0018278479 -0.0018278086 -0.0018277866 -0.0018278295][-0.0018293704 -0.0018289888 -0.0018289248 -0.0018287528 -0.0018284029 -0.0018279162 -0.001827533 -0.0018274814 -0.0018276548 -0.0018277883 -0.0018278337 -0.001827858 -0.0018278648 -0.0018278535 -0.0018278495]]...]
INFO - root - 2017-12-09 13:22:20.125234: step 26310, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.880 sec/batch; 74h:49m:10s remains)
INFO - root - 2017-12-09 13:22:28.660750: step 26320, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.874 sec/batch; 74h:18m:27s remains)
INFO - root - 2017-12-09 13:22:37.397240: step 26330, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.845 sec/batch; 71h:53m:30s remains)
INFO - root - 2017-12-09 13:22:46.071913: step 26340, loss = 0.82, batch loss = 0.69 (9.7 examples/sec; 0.827 sec/batch; 70h:19m:06s remains)
INFO - root - 2017-12-09 13:22:54.714392: step 26350, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.866 sec/batch; 73h:36m:33s remains)
INFO - root - 2017-12-09 13:23:03.382930: step 26360, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.883 sec/batch; 75h:03m:33s remains)
INFO - root - 2017-12-09 13:23:11.889664: step 26370, loss = 0.81, batch loss = 0.68 (10.7 examples/sec; 0.747 sec/batch; 63h:33m:43s remains)
INFO - root - 2017-12-09 13:23:20.607292: step 26380, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.870 sec/batch; 73h:59m:57s remains)
INFO - root - 2017-12-09 13:23:29.119465: step 26390, loss = 0.82, batch loss = 0.69 (10.0 examples/sec; 0.802 sec/batch; 68h:10m:41s remains)
INFO - root - 2017-12-09 13:23:37.502178: step 26400, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.875 sec/batch; 74h:25m:09s remains)
2017-12-09 13:23:38.358801: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0018216321 -0.001821326 -0.0018216583 -0.0018220025 -0.001822124 -0.0018220105 -0.0018219185 -0.001821826 -0.0018218035 -0.0018219854 -0.0018222855 -0.0018226092 -0.001822743 -0.0018227875 -0.0018227292][-0.0018188963 -0.0018185182 -0.0018187285 -0.0018189756 -0.0018191072 -0.0018191043 -0.0018190774 -0.0018190291 -0.001819042 -0.0018192232 -0.0018194901 -0.0018197553 -0.0018198186 -0.0018197694 -0.001819624][-0.0018170053 -0.0018165675 -0.0018166732 -0.0018168731 -0.0018170364 -0.0018171209 -0.0018171754 -0.0018171951 -0.0018172619 -0.001817422 -0.0018176237 -0.0018178009 -0.0018177921 -0.0018176767 -0.0018174783][-0.001816261 -0.0018157434 -0.0018156874 -0.0018157579 -0.0018158217 -0.0018158675 -0.0018159234 -0.001815978 -0.0018160852 -0.0018162359 -0.0018164071 -0.0018165287 -0.0018164928 -0.0018163555 -0.0018161777][-0.0018161281 -0.0018155444 -0.0018153068 -0.001815174 -0.0018150597 -0.0018149813 -0.0018149728 -0.0018150174 -0.0018151529 -0.0018153318 -0.0018155153 -0.0018156363 -0.001815634 -0.0018155486 -0.0018154448][-0.00181626 -0.0018156503 -0.0018152765 -0.0018149681 -0.0018147061 -0.0018145046 -0.0018144017 -0.0018144131 -0.0018145777 -0.001814806 -0.0018150363 -0.0018151932 -0.0018152379 -0.001815212 -0.0018151702][-0.0018164231 -0.0018158267 -0.0018154064 -0.0018150157 -0.0018146588 -0.0018143709 -0.001814212 -0.0018142093 -0.0018143707 -0.0018146168 -0.0018148769 -0.0018150655 -0.0018151387 -0.0018151585 -0.0018151713][-0.0018164435 -0.0018158756 -0.0018154947 -0.0018151082 -0.0018147316 -0.0018144032 -0.001814209 -0.0018141875 -0.0018143191 -0.0018145462 -0.0018148116 -0.0018150235 -0.001815129 -0.0018151769 -0.0018152179][-0.0018164259 -0.0018158939 -0.0018155755 -0.0018152249 -0.0018148719 -0.0018145611 -0.0018143579 -0.0018142953 -0.0018143744 -0.0018145618 -0.0018148101 -0.0018150179 -0.0018151514 -0.0018152384 -0.0018152983][-0.0018163986 -0.0018158733 -0.0018155888 -0.0018152801 -0.0018149959 -0.001814753 -0.001814599 -0.0018145523 -0.0018146081 -0.0018147638 -0.0018149711 -0.0018151625 -0.0018153095 -0.0018154222 -0.001815503][-0.0018163198 -0.0018158043 -0.0018155481 -0.0018152875 -0.0018150788 -0.0018149321 -0.0018148768 -0.0018148979 -0.001814966 -0.0018150921 -0.0018152586 -0.0018154279 -0.001815573 -0.0018156972 -0.001815786][-0.001816269 -0.0018157992 -0.0018155815 -0.0018153751 -0.0018152345 -0.0018151781 -0.0018152138 -0.0018152946 -0.0018153803 -0.0018154713 -0.0018155841 -0.0018157181 -0.0018158617 -0.0018159915 -0.0018160853][-0.001816254 -0.0018158515 -0.001815673 -0.0018155134 -0.0018154375 -0.0018154548 -0.001815556 -0.0018156808 -0.0018157768 -0.0018158309 -0.0018158748 -0.0018159511 -0.0018160767 -0.001816205 -0.0018162936][-0.001816184 -0.0018158627 -0.0018157171 -0.0018156022 -0.0018155804 -0.0018156612 -0.0018158171 -0.0018159876 -0.0018161081 -0.0018161507 -0.0018161603 -0.0018162 -0.0018162973 -0.0018164023 -0.0018164646][-0.0018161112 -0.0018158578 -0.00181576 -0.001815702 -0.0018157345 -0.0018158561 -0.0018160421 -0.0018162301 -0.0018163506 -0.00181639 -0.0018163975 -0.0018164399 -0.0018165229 -0.0018165915 -0.0018165959]]...]
INFO - root - 2017-12-09 13:23:46.903213: step 26410, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 73h:50m:57s remains)
INFO - root - 2017-12-09 13:23:55.395926: step 26420, loss = 0.82, batch loss = 0.69 (10.1 examples/sec; 0.791 sec/batch; 67h:15m:13s remains)
INFO - root - 2017-12-09 13:24:04.012437: step 26430, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.866 sec/batch; 73h:38m:05s remains)
INFO - root - 2017-12-09 13:24:12.645933: step 26440, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 73h:17m:07s remains)
INFO - root - 2017-12-09 13:24:21.310709: step 26450, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.852 sec/batch; 72h:28m:03s remains)
INFO - root - 2017-12-09 13:24:29.950595: step 26460, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.902 sec/batch; 76h:40m:42s remains)
INFO - root - 2017-12-09 13:24:38.602040: step 26470, loss = 0.83, batch loss = 0.70 (9.3 examples/sec; 0.861 sec/batch; 73h:13m:17s remains)
INFO - root - 2017-12-09 13:24:47.191095: step 26480, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.844 sec/batch; 71h:42m:49s remains)
INFO - root - 2017-12-09 13:24:55.869695: step 26490, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.865 sec/batch; 73h:31m:26s remains)
INFO - root - 2017-12-09 13:25:04.411465: step 26500, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 73h:18m:44s remains)
2017-12-09 13:25:05.259224: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0018010763 -0.0017999059 -0.0017992377 -0.0017984798 -0.0017975941 -0.0017967894 -0.001796335 -0.0017963938 -0.0017968131 -0.0017973197 -0.0017977001 -0.0017978465 -0.0017975975 -0.0017969521 -0.0017958535][-0.0017996352 -0.0017983876 -0.0017977151 -0.001797001 -0.0017962852 -0.0017958455 -0.0017958049 -0.0017960815 -0.0017963931 -0.001796608 -0.0017967451 -0.001796781 -0.0017964859 -0.0017957977 -0.0017947733][-0.0017994867 -0.0017982464 -0.0017976064 -0.0017969951 -0.0017964866 -0.0017964569 -0.0017968552 -0.0017974137 -0.0017978022 -0.0017980079 -0.0017982044 -0.0017983316 -0.0017980433 -0.0017973757 -0.0017966495][-0.0017995508 -0.0017983348 -0.0017977955 -0.001797448 -0.0017973573 -0.0017978575 -0.0017987239 -0.0017996493 -0.0018003235 -0.0018008904 -0.0018014994 -0.0018019495 -0.0018018944 -0.0018014972 -0.0018011909][-0.001799167 -0.0017981101 -0.0017979363 -0.0017982693 -0.0017990974 -0.0018005067 -0.0018020846 -0.0018036715 -0.0018050241 -0.0018061947 -0.0018071416 -0.0018076983 -0.0018077351 -0.0018075141 -0.0018074756][-0.0017983877 -0.0017976016 -0.0017979773 -0.0017992192 -0.0018011563 -0.0018035398 -0.0018060151 -0.0018084578 -0.0018105229 -0.0018121488 -0.0018131906 -0.00181363 -0.0018136614 -0.0018136102 -0.0018138427][-0.0017974378 -0.0017967384 -0.0017974121 -0.0017992869 -0.0018020201 -0.0018052512 -0.0018087591 -0.0018123394 -0.0018154697 -0.001817776 -0.001819051 -0.0018195105 -0.0018195548 -0.0018195774 -0.0018198985][-0.0017959318 -0.0017951804 -0.0017960211 -0.0017983037 -0.001801661 -0.0018057908 -0.0018103969 -0.0018150796 -0.0018189772 -0.0018217781 -0.0018231834 -0.0018236266 -0.0018235677 -0.0018235333 -0.0018236922][-0.0017943453 -0.0017935749 -0.0017947495 -0.0017975534 -0.0018016704 -0.0018065458 -0.0018116933 -0.0018165059 -0.0018202116 -0.001822693 -0.001823685 -0.001823787 -0.0018234435 -0.0018231017 -0.0018228011][-0.0017933545 -0.0017926594 -0.0017939813 -0.0017968946 -0.0018010765 -0.0018057926 -0.001810565 -0.0018146872 -0.0018175067 -0.0018191031 -0.0018194451 -0.0018191547 -0.0018185293 -0.0018179095 -0.0018172826][-0.0017930306 -0.001792149 -0.0017930472 -0.0017953168 -0.0017986189 -0.0018022804 -0.0018060275 -0.0018092246 -0.0018112519 -0.0018122287 -0.0018123662 -0.0018120711 -0.0018114436 -0.0018107595 -0.0018100574][-0.0017925533 -0.001791312 -0.0017917639 -0.0017932741 -0.0017954868 -0.0017979641 -0.0018005845 -0.0018028687 -0.0018043354 -0.0018049771 -0.0018050259 -0.0018048307 -0.0018043939 -0.0018038281 -0.0018031774][-0.0017920152 -0.0017906827 -0.0017909254 -0.0017919098 -0.0017932266 -0.0017946208 -0.0017960538 -0.0017972209 -0.0017978639 -0.0017979455 -0.0017976988 -0.0017974534 -0.0017972118 -0.0017969201 -0.0017966197][-0.0017918933 -0.0017904777 -0.0017904968 -0.0017910766 -0.0017916682 -0.0017921366 -0.0017925287 -0.0017927188 -0.00179265 -0.0017923235 -0.0017919163 -0.0017916651 -0.0017916375 -0.0017917207 -0.0017918526][-0.0017917213 -0.0017903037 -0.0017900531 -0.0017902619 -0.0017903495 -0.0017902795 -0.001790137 -0.00178986 -0.0017895349 -0.0017891192 -0.0017887447 -0.0017885356 -0.0017886227 -0.0017889154 -0.001789316]]...]
INFO - root - 2017-12-09 13:25:14.037703: step 26510, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.884 sec/batch; 75h:05m:51s remains)
INFO - root - 2017-12-09 13:25:22.520760: step 26520, loss = 0.82, batch loss = 0.69 (10.2 examples/sec; 0.786 sec/batch; 66h:47m:49s remains)
INFO - root - 2017-12-09 13:25:31.195540: step 26530, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.886 sec/batch; 75h:16m:33s remains)
INFO - root - 2017-12-09 13:25:39.850992: step 26540, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.887 sec/batch; 75h:24m:03s remains)
INFO - root - 2017-12-09 13:25:48.393726: step 26550, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.843 sec/batch; 71h:37m:29s remains)
INFO - root - 2017-12-09 13:25:56.949109: step 26560, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.847 sec/batch; 72h:01m:06s remains)
INFO - root - 2017-12-09 13:26:05.561330: step 26570, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.845 sec/batch; 71h:49m:08s remains)
INFO - root - 2017-12-09 13:26:13.929025: step 26580, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.860 sec/batch; 73h:05m:17s remains)
INFO - root - 2017-12-09 13:26:22.565808: step 26590, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.859 sec/batch; 73h:01m:26s remains)
INFO - root - 2017-12-09 13:26:31.169135: step 26600, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.870 sec/batch; 73h:57m:37s remains)
2017-12-09 13:26:32.026185: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.229252 0.22548416 0.2212417 0.21687877 0.21290553 0.20821986 0.20316082 0.19769441 0.1922299 0.18565238 0.17676443 0.16636461 0.15487075 0.14286946 0.13147129][0.23491858 0.23327594 0.23092306 0.22790854 0.22441757 0.21973059 0.21442792 0.20852113 0.20232427 0.19519874 0.18581846 0.17460302 0.161863 0.14830294 0.13530172][0.2366707 0.2376007 0.23728979 0.2357792 0.23288682 0.22825898 0.22264014 0.21620737 0.2094672 0.20179625 0.19208777 0.18015176 0.16676064 0.15228142 0.13809791][0.23662688 0.24055931 0.24277513 0.2432929 0.24183549 0.23755422 0.23192054 0.22495513 0.21736662 0.20882572 0.19833767 0.18595658 0.17211182 0.15724376 0.14276305][0.23582236 0.24235009 0.24685404 0.24921195 0.24908642 0.24552113 0.24005717 0.23323889 0.225231 0.21615933 0.20547703 0.19310182 0.17941242 0.16441871 0.14983284][0.23419747 0.24346478 0.25005606 0.25386864 0.25455329 0.25176576 0.24696797 0.2400866 0.231826 0.22258876 0.21185622 0.19924422 0.18541853 0.17110471 0.15739889][0.23127313 0.24180806 0.2490402 0.25366941 0.25503534 0.25318286 0.24929829 0.24340187 0.23595497 0.22695042 0.21650767 0.20412537 0.19058055 0.17689449 0.16404325][0.22925779 0.24035835 0.24707656 0.25119361 0.2522445 0.25061998 0.24720594 0.24223766 0.23600814 0.22781341 0.21786782 0.20610484 0.19312777 0.18011107 0.16814679][0.2260977 0.23719019 0.24305707 0.24652046 0.24706303 0.24537627 0.24232298 0.23824506 0.23271905 0.22522852 0.21616158 0.20535772 0.19317266 0.18098098 0.16991235][0.22161406 0.23186211 0.23620971 0.23837581 0.23807691 0.23610263 0.23300572 0.22924547 0.22435945 0.21784841 0.20988868 0.20030925 0.18932737 0.1781393 0.16796024][0.21704982 0.22550461 0.22737379 0.22765867 0.22600859 0.22323747 0.21981859 0.21615925 0.21170127 0.20601857 0.19899696 0.1903221 0.18031214 0.17026034 0.16103449][0.20997748 0.21615154 0.21541402 0.21374857 0.21098197 0.20751421 0.20381728 0.2006323 0.19663511 0.19157249 0.18540487 0.17774792 0.1689831 0.15977977 0.15135975][0.202826 0.20710249 0.20421842 0.20054203 0.19619593 0.19141641 0.1868688 0.18359557 0.17978294 0.1754138 0.17004585 0.16348886 0.15598966 0.14782713 0.14031413][0.19213609 0.19505323 0.19091295 0.18620811 0.18132313 0.17605817 0.17122847 0.16760582 0.16374433 0.16007572 0.1556824 0.15019022 0.14397898 0.13729925 0.1310709][0.18119867 0.18297572 0.17789622 0.1725297 0.16723157 0.16190839 0.15708329 0.15358253 0.15007006 0.14691906 0.14321187 0.13856007 0.13339311 0.12783407 0.12274054]]...]
INFO - root - 2017-12-09 13:26:40.807112: step 26610, loss = 0.83, batch loss = 0.70 (9.2 examples/sec; 0.868 sec/batch; 73h:47m:17s remains)
INFO - root - 2017-12-09 13:26:49.260478: step 26620, loss = 0.81, batch loss = 0.68 (10.8 examples/sec; 0.739 sec/batch; 62h:47m:58s remains)
INFO - root - 2017-12-09 13:26:57.838585: step 26630, loss = 0.82, batch loss = 0.69 (9.9 examples/sec; 0.812 sec/batch; 68h:57m:01s remains)
INFO - root - 2017-12-09 13:27:06.500529: step 26640, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.856 sec/batch; 72h:44m:36s remains)
INFO - root - 2017-12-09 13:27:15.188484: step 26650, loss = 0.81, batch loss = 0.68 (9.3 examples/sec; 0.863 sec/batch; 73h:19m:13s remains)
INFO - root - 2017-12-09 13:27:23.816266: step 26660, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.883 sec/batch; 75h:02m:23s remains)
INFO - root - 2017-12-09 13:27:32.432637: step 26670, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.902 sec/batch; 76h:35m:17s remains)
INFO - root - 2017-12-09 13:27:40.964117: step 26680, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.839 sec/batch; 71h:18m:36s remains)
INFO - root - 2017-12-09 13:27:49.553430: step 26690, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.847 sec/batch; 71h:54m:28s remains)
INFO - root - 2017-12-09 13:27:58.037853: step 26700, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.877 sec/batch; 74h:30m:59s remains)
2017-12-09 13:27:58.884549: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0018170042 -0.0018078065 -0.0017505654 -0.0015498791 -0.0011203783 -0.00050348358 6.1510131e-05 0.00028983073 6.1016181e-05 -0.00048403419 -0.0010732547 -0.0014976262 -0.0017154546 -0.001795362 -0.0018150777][-0.001815427 -0.001791595 -0.0016673331 -0.0012794597 -0.000504549 0.00056838605 0.0015419117 0.0019411574 0.001563782 0.00063483173 -0.00039589289 -0.0011710889 -0.0015944155 -0.0017631188 -0.0018093627][-0.0018051205 -0.0017293708 -0.0014372364 -0.00065647869 0.00078281213 0.0026904433 0.0044031609 0.0051438571 0.0045582894 0.0029747118 0.0011223512 -0.0003684262 -0.0012606486 -0.0016599804 -0.0017877757][-0.001763957 -0.0015540301 -0.00090557965 0.00058099942 0.003094309 0.0062578907 0.00902951 0.010242821 0.0093479352 0.0068022292 0.0037052738 0.0010797287 -0.00060587237 -0.0014334556 -0.0017328097][-0.001568853 -0.0011432883 6.0369493e-05 0.0025389371 0.0064269491 0.011081236 0.015030245 0.016710958 0.015394938 0.011685807 0.0070802188 0.0030409917 0.00032439327 -0.0010913301 -0.0016435877][-0.0011567264 -0.00044090638 0.0014207627 0.0049726991 0.010237439 0.016274773 0.0212295 0.023216583 0.02139922 0.016529253 0.010456546 0.0050350009 0.0012930861 -0.00072323857 -0.001543153][-0.00062420662 0.00035210524 0.0027747173 0.0071767718 0.01342738 0.020325821 0.02577018 0.027725751 0.02538242 0.01964969 0.0126038 0.0063027879 0.0019136252 -0.00048541126 -0.0014785572][-0.00018802215 0.0009459405 0.0036191787 0.0083360616 0.014855835 0.021839164 0.027117202 0.028697148 0.025923584 0.019855324 0.012636954 0.0062820306 0.0018935638 -0.00049595127 -0.001482165][-8.2690734e-05 0.001038409 0.0035663554 0.0079521565 0.013923931 0.020181004 0.024693081 0.025691921 0.022753177 0.017020889 0.010512335 0.0049661971 0.0012376945 -0.0007486688 -0.0015506975][-0.00034046941 0.00059627916 0.002636495 0.0061717778 0.010979238 0.015947925 0.019351523 0.019773517 0.017025841 0.012231011 0.007095919 0.0029306556 0.00025342836 -0.0011159435 -0.00164704][-0.00081939355 -0.00016054662 0.0012361951 0.0036869021 0.0070617963 0.010538113 0.012802025 0.012827755 0.010600291 0.0070928726 0.0035862485 0.00092476804 -0.00067881343 -0.0014495815 -0.0017310721][-0.0012954539 -0.00091403123 -0.00011491159 0.0013280845 0.00336973 0.0054900665 0.0068120044 0.00667374 0.0051391679 0.0029301299 0.00088447181 -0.00054121809 -0.0013251593 -0.0016685898 -0.001783065][-0.0016145674 -0.0014378501 -0.0010661667 -0.00036080705 0.00068314245 0.0017930066 0.0024662563 0.00232806 0.0014493413 0.00027676427 -0.00072404416 -0.0013523824 -0.00165702 -0.0017728419 -0.0018057885][-0.001770478 -0.0017110077 -0.0015756966 -0.0012962621 -0.00085742632 -0.00037511915 -8.5471547e-05 -0.00016530824 -0.00056831015 -0.0010724019 -0.0014683639 -0.0016893649 -0.0017796168 -0.0018071445 -0.0018129376][-0.0018120533 -0.0018025452 -0.0017686244 -0.0016828451 -0.0015363951 -0.0013686414 -0.0012675112 -0.0012972468 -0.0014384309 -0.0016065782 -0.0017287565 -0.0017895763 -0.0018105255 -0.0018150181 -0.0018150689]]...]
INFO - root - 2017-12-09 13:28:07.601982: step 26710, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.878 sec/batch; 74h:35m:32s remains)
INFO - root - 2017-12-09 13:28:16.354405: step 26720, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.878 sec/batch; 74h:35m:31s remains)
INFO - root - 2017-12-09 13:28:24.793362: step 26730, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.874 sec/batch; 74h:13m:03s remains)
INFO - root - 2017-12-09 13:28:33.590228: step 26740, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.889 sec/batch; 75h:29m:40s remains)
INFO - root - 2017-12-09 13:28:42.187856: step 26750, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.840 sec/batch; 71h:20m:35s remains)
INFO - root - 2017-12-09 13:28:50.924696: step 26760, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.888 sec/batch; 75h:24m:09s remains)
INFO - root - 2017-12-09 13:28:59.644000: step 26770, loss = 0.83, batch loss = 0.70 (9.4 examples/sec; 0.853 sec/batch; 72h:24m:27s remains)
INFO - root - 2017-12-09 13:29:08.145639: step 26780, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.861 sec/batch; 73h:07m:39s remains)
INFO - root - 2017-12-09 13:29:16.830643: step 26790, loss = 0.81, batch loss = 0.68 (9.1 examples/sec; 0.875 sec/batch; 74h:18m:40s remains)
INFO - root - 2017-12-09 13:29:25.441667: step 26800, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 73h:43m:28s remains)
2017-12-09 13:29:26.295320: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.22321209 0.21897776 0.21377923 0.20957656 0.2070072 0.20608203 0.20578827 0.20467386 0.20065652 0.19191082 0.17705458 0.15610689 0.1303257 0.10347873 0.078947976][0.2347541 0.23042083 0.2247875 0.22035445 0.21735825 0.21537998 0.21372792 0.21108982 0.20531848 0.19482839 0.17824191 0.15574338 0.12863402 0.10066005 0.075188421][0.23950908 0.23538972 0.22973627 0.22494575 0.221381 0.21881358 0.21603425 0.21189529 0.20453048 0.19258966 0.17485982 0.15174496 0.1247751 0.09750358 0.072771892][0.24078892 0.23740542 0.23227048 0.22756411 0.22365128 0.22023202 0.21626495 0.21105188 0.20249285 0.18964058 0.17151351 0.14860961 0.12276184 0.0970821 0.074055143][0.23653582 0.23428342 0.22970043 0.22533764 0.221553 0.21784338 0.21329479 0.20720175 0.19795047 0.18494973 0.16733806 0.14549443 0.12152424 0.0982384 0.077679142][0.2267475 0.22548293 0.22119549 0.21704754 0.21331918 0.20962195 0.20491579 0.19858313 0.18945202 0.17707075 0.16072556 0.14089757 0.11961862 0.099222608 0.081385508][0.21131881 0.21077722 0.20645159 0.20224953 0.19845633 0.19486566 0.1903432 0.18438745 0.17615469 0.16505425 0.15076931 0.13362397 0.11550749 0.098340027 0.083426677][0.19107573 0.19084054 0.18642195 0.18201773 0.1780286 0.17451906 0.17026637 0.16487144 0.15773156 0.14846815 0.13672994 0.12260997 0.10791349 0.094144158 0.082292296][0.16789021 0.16782756 0.16327317 0.15881205 0.15486807 0.15159164 0.14785381 0.14332494 0.13749136 0.13001034 0.12066354 0.10932636 0.0976548 0.086735241 0.077427216][0.1448089 0.14458652 0.13988626 0.13549806 0.13177884 0.12880619 0.12558685 0.12181555 0.11698575 0.11078119 0.10306638 0.093890972 0.084535971 0.075931549 0.0687841][0.12432041 0.12399625 0.11937883 0.11511251 0.11161721 0.10911279 0.10655647 0.10345229 0.099335872 0.094026364 0.087443419 0.079641387 0.071640871 0.064561129 0.05886139][0.1067921 0.10636105 0.10203022 0.098063253 0.095009148 0.092882246 0.090958841 0.08852 0.085021995 0.080347575 0.074493878 0.067786142 0.060907315 0.0549304 0.050275803][0.092244573 0.09165623 0.08764787 0.084115155 0.081496581 0.079668455 0.0783237 0.0765666 0.073769584 0.069965221 0.065069206 0.059369583 0.053457603 0.048390742 0.044706609][0.08026842 0.080014773 0.076652415 0.073619545 0.071406595 0.069954023 0.069061384 0.067833222 0.065827355 0.06293454 0.059022911 0.054278906 0.049353503 0.045153912 0.042158306][0.071430437 0.071561806 0.068753466 0.066380329 0.064614773 0.063324288 0.062720396 0.061836857 0.060457878 0.058296453 0.055218946 0.051487394 0.047525436 0.044262372 0.04206093]]...]
INFO - root - 2017-12-09 13:29:34.986126: step 26810, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.872 sec/batch; 74h:01m:15s remains)
INFO - root - 2017-12-09 13:29:43.743006: step 26820, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.856 sec/batch; 72h:41m:01s remains)
INFO - root - 2017-12-09 13:29:52.179944: step 26830, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 73h:46m:18s remains)
INFO - root - 2017-12-09 13:30:00.907797: step 26840, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.856 sec/batch; 72h:40m:40s remains)
INFO - root - 2017-12-09 13:30:09.548470: step 26850, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.874 sec/batch; 74h:10m:06s remains)
INFO - root - 2017-12-09 13:30:18.137004: step 26860, loss = 0.83, batch loss = 0.70 (9.2 examples/sec; 0.866 sec/batch; 73h:32m:06s remains)
INFO - root - 2017-12-09 13:30:26.621249: step 26870, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.837 sec/batch; 71h:01m:22s remains)
INFO - root - 2017-12-09 13:30:35.023920: step 26880, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 73h:19m:06s remains)
INFO - root - 2017-12-09 13:30:43.687558: step 26890, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.872 sec/batch; 74h:03m:49s remains)
INFO - root - 2017-12-09 13:30:52.268511: step 26900, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 73h:54m:33s remains)
2017-12-09 13:30:53.185016: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.0982374 0.098272584 0.096948326 0.096297473 0.095148288 0.094963335 0.094314769 0.093418479 0.092430376 0.091787629 0.091211058 0.090461187 0.089672968 0.088668227 0.087413371][0.13983922 0.14188036 0.14035705 0.13914244 0.13673976 0.13532108 0.13349627 0.1317319 0.13012604 0.12942937 0.12952752 0.12958477 0.12954153 0.12873077 0.12741698][0.17704564 0.18195692 0.18151262 0.18103097 0.178393 0.1748178 0.17059866 0.16710515 0.1640584 0.16242532 0.16294973 0.16504996 0.16691235 0.16629556 0.16470946][0.20529784 0.21502814 0.21744415 0.21766803 0.21436007 0.20915826 0.20223883 0.19581948 0.19044372 0.18733196 0.18800457 0.19119027 0.19517593 0.19604614 0.19535159][0.22708422 0.24102306 0.245297 0.24688366 0.24306552 0.23498489 0.22450393 0.21478298 0.2068637 0.20227879 0.20299563 0.2073525 0.21319936 0.21604791 0.21655336][0.24237888 0.25871417 0.26341876 0.26433218 0.258685 0.24736427 0.23320472 0.21968082 0.20921451 0.20340374 0.20339033 0.20798226 0.21473852 0.21971181 0.22180694][0.2436872 0.26185772 0.26640564 0.26609737 0.257842 0.24299969 0.22486681 0.20775457 0.19464718 0.18729679 0.18679859 0.19186489 0.19976163 0.20586966 0.20863155][0.22619566 0.24440935 0.24797674 0.24614871 0.23578954 0.21834497 0.19757353 0.17829336 0.16364361 0.15514334 0.15391244 0.15884507 0.16717812 0.17397316 0.17734249][0.19079657 0.2062614 0.20791678 0.20364662 0.19143163 0.17331438 0.15250336 0.13348974 0.11917733 0.11096922 0.1097818 0.11438931 0.12245804 0.12955135 0.13360523][0.14518337 0.15542103 0.15478884 0.14874446 0.13615814 0.11852625 0.099366687 0.082954139 0.071163177 0.064805634 0.0643626 0.068787262 0.0762376 0.083047539 0.087529317][0.097862348 0.10297897 0.10043241 0.09366741 0.082245149 0.067653067 0.052774731 0.040479291 0.032198757 0.028502598 0.029466297 0.033623237 0.039478809 0.045055881 0.049163274][0.05891294 0.060853805 0.058028761 0.052262254 0.043750025 0.033595487 0.023698045 0.015887188 0.010996027 0.0093626808 0.010631207 0.013611864 0.017480476 0.021150602 0.0239766][0.029652305 0.030045878 0.028042782 0.024408037 0.019491768 0.013784879 0.0082591 0.0038531346 0.0012583931 0.00064324075 0.0015164814 0.0031159164 0.0050332127 0.0069013955 0.0084635755][0.010733653 0.010699763 0.0097461613 0.00818859 0.0062130187 0.0039921785 0.0018032356 6.392831e-06 -0.0010623123 -0.001370896 -0.0011694385 -0.00074169529 -0.0002104562 0.00033863075 0.00078253215][0.0008076916 0.00075936713 0.00050821668 0.00013520522 -0.00028866751 -0.00074323651 -0.0011893176 -0.0015412368 -0.0017282334 -0.0017833575 -0.0017858854 -0.001779 -0.0017681682 -0.0017601617 -0.0017538079]]...]
INFO - root - 2017-12-09 13:31:01.965948: step 26910, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.865 sec/batch; 73h:27m:22s remains)
INFO - root - 2017-12-09 13:31:10.634963: step 26920, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.847 sec/batch; 71h:52m:35s remains)
INFO - root - 2017-12-09 13:31:19.186882: step 26930, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.886 sec/batch; 75h:13m:27s remains)
INFO - root - 2017-12-09 13:31:27.720179: step 26940, loss = 0.82, batch loss = 0.69 (9.8 examples/sec; 0.816 sec/batch; 69h:14m:52s remains)
INFO - root - 2017-12-09 13:31:36.388478: step 26950, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.881 sec/batch; 74h:44m:03s remains)
INFO - root - 2017-12-09 13:31:45.020158: step 26960, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.861 sec/batch; 73h:05m:06s remains)
INFO - root - 2017-12-09 13:31:53.684890: step 26970, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.857 sec/batch; 72h:44m:06s remains)
INFO - root - 2017-12-09 13:32:02.185969: step 26980, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.858 sec/batch; 72h:46m:43s remains)
INFO - root - 2017-12-09 13:32:10.656292: step 26990, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.892 sec/batch; 75h:39m:52s remains)
INFO - root - 2017-12-09 13:32:19.102796: step 27000, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 73h:42m:00s remains)
2017-12-09 13:32:19.935688: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0015704639 -0.0015121248 -0.0014658011 -0.0014478662 -0.0014622954 -0.0014915812 -0.0015252526 -0.001569467 -0.0016331616 -0.0017031971 -0.0017588201 -0.0017926574 -0.0018099571 -0.0018194675 -0.0018251534][-0.0016278428 -0.0015864759 -0.0015560609 -0.0015491125 -0.0015679057 -0.0015973763 -0.0016265884 -0.0016588651 -0.0017014305 -0.0017470932 -0.0017827997 -0.0018043311 -0.0018157966 -0.0018224636 -0.0018262456][-0.0016986883 -0.0016704554 -0.0016509289 -0.0016495162 -0.0016676019 -0.0016937027 -0.001717482 -0.0017385475 -0.0017618294 -0.0017853318 -0.0018027771 -0.0018137504 -0.0018205527 -0.0018251424 -0.001827696][-0.0017624575 -0.0017441137 -0.0017313395 -0.0017306884 -0.0017432839 -0.0017615813 -0.0017777579 -0.0017895536 -0.001800247 -0.0018100512 -0.0018169516 -0.0018217212 -0.0018254339 -0.0018280675 -0.0018291613][-0.0018055285 -0.0017951125 -0.0017870979 -0.0017852134 -0.0017903713 -0.0017987344 -0.0018060902 -0.0018115336 -0.0018164725 -0.0018209498 -0.0018242481 -0.0018265409 -0.0018288862 -0.0018303598 -0.0018307666][-0.0018260122 -0.0018197815 -0.0018131352 -0.0018085382 -0.0018058182 -0.0018045761 -0.0018043722 -0.0018069831 -0.0018119069 -0.0018176313 -0.0018223444 -0.0018257574 -0.0018286798 -0.0018306378 -0.0018311924][-0.001829945 -0.0018235176 -0.0018143638 -0.0018042292 -0.0017932069 -0.0017840122 -0.0017793062 -0.0017831419 -0.0017937355 -0.0018061851 -0.0018159696 -0.0018223735 -0.0018268835 -0.0018298434 -0.0018311713][-0.0018261654 -0.0018163541 -0.0018018493 -0.0017848868 -0.0017675211 -0.0017546965 -0.0017499032 -0.0017576278 -0.0017752628 -0.0017945413 -0.0018089393 -0.0018179929 -0.0018242766 -0.0018284386 -0.0018305028][-0.0018233582 -0.0018097338 -0.0017896004 -0.0017672525 -0.0017470755 -0.0017347175 -0.0017339948 -0.0017468283 -0.0017686705 -0.0017911159 -0.0018069936 -0.001816676 -0.0018227468 -0.0018269323 -0.0018287938][-0.0018227578 -0.001806841 -0.0017839782 -0.0017602491 -0.0017410315 -0.0017314692 -0.0017349869 -0.001751836 -0.0017754339 -0.00179697 -0.00181077 -0.0018190009 -0.0018240636 -0.0018271084 -0.0018283797][-0.0018234263 -0.0018077623 -0.0017856742 -0.0017637288 -0.0017477178 -0.001741561 -0.0017479656 -0.0017660141 -0.0017884371 -0.0018060823 -0.0018166077 -0.0018228637 -0.0018263039 -0.0018279122 -0.0018286067][-0.0018257223 -0.0018129534 -0.0017953052 -0.0017776495 -0.0017648336 -0.0017604842 -0.0017673201 -0.0017832692 -0.001801461 -0.0018147177 -0.0018221503 -0.0018264835 -0.0018282888 -0.0018286124 -0.0018285649][-0.0018289183 -0.0018209789 -0.0018096837 -0.001798053 -0.0017892831 -0.0017860942 -0.0017909014 -0.0018015209 -0.0018129438 -0.0018210456 -0.0018255541 -0.0018283587 -0.0018291258 -0.00182882 -0.0018286153][-0.0018308625 -0.0018274541 -0.0018221831 -0.0018161176 -0.0018108512 -0.0018085926 -0.001811025 -0.0018164823 -0.0018219309 -0.0018255792 -0.0018277219 -0.0018292119 -0.0018292395 -0.0018286936 -0.0018283698][-0.0018304904 -0.0018296039 -0.0018280641 -0.0018258994 -0.001823663 -0.0018224424 -0.0018229793 -0.0018249442 -0.0018268339 -0.0018279746 -0.0018285953 -0.0018293333 -0.0018293336 -0.0018288989 -0.0018286344]]...]
INFO - root - 2017-12-09 13:32:28.500319: step 27010, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.838 sec/batch; 71h:04m:19s remains)
INFO - root - 2017-12-09 13:32:37.271712: step 27020, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.852 sec/batch; 72h:20m:16s remains)
INFO - root - 2017-12-09 13:32:45.678977: step 27030, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 73h:45m:32s remains)
INFO - root - 2017-12-09 13:32:54.301384: step 27040, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 73h:34m:57s remains)
INFO - root - 2017-12-09 13:33:02.949478: step 27050, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.897 sec/batch; 76h:06m:41s remains)
INFO - root - 2017-12-09 13:33:11.686153: step 27060, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.853 sec/batch; 72h:22m:22s remains)
INFO - root - 2017-12-09 13:33:20.352171: step 27070, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.881 sec/batch; 74h:46m:38s remains)
INFO - root - 2017-12-09 13:33:29.068503: step 27080, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 73h:55m:25s remains)
INFO - root - 2017-12-09 13:33:37.840424: step 27090, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.851 sec/batch; 72h:09m:20s remains)
INFO - root - 2017-12-09 13:33:46.401227: step 27100, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.874 sec/batch; 74h:08m:59s remains)
2017-12-09 13:33:47.313061: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0018160045 -0.0018149031 -0.0018148492 -0.001814891 -0.0018148925 -0.001814907 -0.0018150418 -0.0018149731 -0.0018148588 -0.0018147471 -0.0018146472 -0.0018146309 -0.0018146682 -0.0018148206 -0.0018149921][-0.0018154873 -0.0018144703 -0.0018144263 -0.0018145369 -0.0018145872 -0.001814603 -0.0018146191 -0.001814584 -0.0018145042 -0.0018143738 -0.0018142102 -0.0018141608 -0.0018143145 -0.0018145823 -0.001814886][-0.0018153735 -0.0018144513 -0.0018145072 -0.0018147326 -0.0018148554 -0.0018148863 -0.0018149191 -0.0018149554 -0.0018148662 -0.0018146551 -0.0018143235 -0.0018141528 -0.0018143498 -0.0018147815 -0.001815344][-0.0018153334 -0.0018145404 -0.0018147535 -0.001815162 -0.0018154538 -0.0018155578 -0.0018156874 -0.0018158332 -0.0018157269 -0.001815371 -0.00181479 -0.0018144292 -0.0018145657 -0.0018151096 -0.0018159618][-0.0018152907 -0.0018145842 -0.0018150246 -0.0018156619 -0.0018162068 -0.0018164242 -0.0018165954 -0.001816775 -0.0018166211 -0.0018160996 -0.001815283 -0.0018146957 -0.0018146898 -0.0018152443 -0.0018163309][-0.0018149391 -0.0018142816 -0.0018149421 -0.0018158195 -0.0018166201 -0.001816977 -0.0018171697 -0.0018173279 -0.0018170621 -0.0018163481 -0.0018153524 -0.0018146376 -0.0018145249 -0.0018150611 -0.0018162874][-0.0018142776 -0.0018137266 -0.0018146221 -0.001815682 -0.001816651 -0.0018171914 -0.0018174622 -0.0018175659 -0.0018171828 -0.001816315 -0.0018152047 -0.0018144159 -0.0018142404 -0.0018147506 -0.0018160301][-0.0018137551 -0.001813316 -0.0018144223 -0.0018156596 -0.0018166825 -0.001817283 -0.0018175831 -0.001817635 -0.0018171888 -0.0018162322 -0.001815023 -0.0018141648 -0.0018139072 -0.001814286 -0.0018154397][-0.0018135754 -0.001813271 -0.0018145164 -0.0018158047 -0.0018167514 -0.0018172548 -0.001817433 -0.0018173255 -0.0018168588 -0.0018160001 -0.0018147592 -0.0018137348 -0.0018133139 -0.0018135568 -0.0018145449][-0.0018137111 -0.0018134619 -0.0018146248 -0.0018158078 -0.0018165754 -0.0018168953 -0.0018168538 -0.0018165183 -0.0018159925 -0.0018151944 -0.0018140456 -0.0018129935 -0.0018125026 -0.001812709 -0.0018136441][-0.0018138613 -0.0018135473 -0.0018144747 -0.0018154514 -0.0018160403 -0.0018162091 -0.0018160553 -0.0018156174 -0.0018150259 -0.0018142379 -0.0018131954 -0.0018122866 -0.0018118904 -0.0018123034 -0.0018133908][-0.0018140433 -0.0018136019 -0.0018142568 -0.0018150426 -0.0018155782 -0.001815776 -0.0018156114 -0.001815151 -0.0018145577 -0.0018138515 -0.0018129856 -0.0018122453 -0.0018120516 -0.0018127402 -0.0018141648][-0.0018142631 -0.001813698 -0.0018141824 -0.001814811 -0.0018153338 -0.0018155638 -0.0018154026 -0.0018149953 -0.001814576 -0.0018140979 -0.0018134761 -0.001812987 -0.0018130656 -0.0018140674 -0.0018157613][-0.0018143003 -0.0018136925 -0.0018141663 -0.0018147383 -0.0018151882 -0.0018153617 -0.0018152468 -0.0018149904 -0.0018147286 -0.0018144593 -0.001814074 -0.0018138534 -0.0018143026 -0.0018156421 -0.0018175557][-0.0018142588 -0.0018135369 -0.0018139295 -0.0018145272 -0.0018149454 -0.0018150554 -0.0018149964 -0.0018149344 -0.0018148416 -0.0018146851 -0.0018144343 -0.0018144519 -0.0018153405 -0.0018171646 -0.0018193825]]...]
INFO - root - 2017-12-09 13:33:55.880449: step 27110, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.866 sec/batch; 73h:29m:00s remains)
INFO - root - 2017-12-09 13:34:04.620185: step 27120, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.892 sec/batch; 75h:38m:36s remains)
INFO - root - 2017-12-09 13:34:13.156953: step 27130, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.889 sec/batch; 75h:26m:27s remains)
INFO - root - 2017-12-09 13:34:21.687577: step 27140, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.850 sec/batch; 72h:06m:22s remains)
INFO - root - 2017-12-09 13:34:30.380764: step 27150, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 73h:07m:57s remains)
INFO - root - 2017-12-09 13:34:39.021422: step 27160, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.872 sec/batch; 73h:56m:13s remains)
INFO - root - 2017-12-09 13:34:47.720831: step 27170, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.858 sec/batch; 72h:44m:07s remains)
INFO - root - 2017-12-09 13:34:56.246351: step 27180, loss = 0.82, batch loss = 0.70 (8.7 examples/sec; 0.916 sec/batch; 77h:42m:33s remains)
INFO - root - 2017-12-09 13:35:05.027052: step 27190, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.834 sec/batch; 70h:41m:29s remains)
INFO - root - 2017-12-09 13:35:13.432416: step 27200, loss = 0.82, batch loss = 0.69 (9.7 examples/sec; 0.828 sec/batch; 70h:11m:50s remains)
2017-12-09 13:35:14.287909: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0018026792 -0.0018013816 -0.0018014002 -0.0018015273 -0.0018016322 -0.0018016264 -0.0018016003 -0.0018015242 -0.0018014737 -0.001801469 -0.0018014581 -0.0018014325 -0.0018013433 -0.0018012075 -0.0018010256][-0.001802523 -0.0018010607 -0.0018009364 -0.0018009348 -0.0018009322 -0.0018008921 -0.0018009013 -0.001800967 -0.0018010888 -0.0018012323 -0.0018013169 -0.0018013315 -0.0018012204 -0.001801001 -0.0018007136][-0.0018031326 -0.0018017428 -0.0018015311 -0.001801422 -0.0018013117 -0.001801217 -0.0018012198 -0.0018013439 -0.0018015682 -0.0018017668 -0.0018018435 -0.0018017963 -0.0018016035 -0.001801247 -0.0018008091][-0.0018036887 -0.0018023143 -0.0018019776 -0.0018016963 -0.001801428 -0.0018012417 -0.0018012457 -0.0018014422 -0.0018017944 -0.0018020918 -0.0018021724 -0.0018020388 -0.0018017227 -0.0018012339 -0.001800679][-0.0018040754 -0.0018027163 -0.0018022346 -0.0018017247 -0.0018012166 -0.0018008472 -0.0018008159 -0.0018011385 -0.0018016857 -0.0018021364 -0.0018022581 -0.0018020435 -0.0018015674 -0.0018009507 -0.0018003157][-0.0018044347 -0.0018031318 -0.0018025716 -0.0018018031 -0.0018010132 -0.0018003855 -0.0018002342 -0.001800656 -0.0018014221 -0.0018020622 -0.0018022723 -0.0018020306 -0.0018014429 -0.0018007248 -0.0018000398][-0.0018048149 -0.001803627 -0.0018030279 -0.0018021016 -0.0018010712 -0.0018001538 -0.0017998194 -0.001800321 -0.0018012868 -0.0018020299 -0.0018022675 -0.001801996 -0.0018013495 -0.0018005931 -0.0017999191][-0.0018053895 -0.0018041453 -0.0018035487 -0.0018025873 -0.0018014094 -0.0018002391 -0.0017996879 -0.0018002013 -0.0018013058 -0.0018020526 -0.0018022847 -0.0018020113 -0.0018013903 -0.0018006589 -0.0018000546][-0.0018056175 -0.0018043987 -0.001803841 -0.0018029506 -0.0018018424 -0.0018007497 -0.0018002201 -0.0018006655 -0.001801662 -0.0018022729 -0.0018024002 -0.001802208 -0.001801673 -0.0018009788 -0.001800398][-0.0018057555 -0.0018046057 -0.0018040948 -0.0018033286 -0.0018024241 -0.0018016391 -0.0018013403 -0.0018017793 -0.0018026014 -0.0018030584 -0.001803163 -0.0018031211 -0.0018027346 -0.0018020892 -0.0018014383][-0.0018066012 -0.00180546 -0.0018049313 -0.0018042315 -0.0018035286 -0.0018030282 -0.0018029696 -0.0018035278 -0.0018043002 -0.0018046728 -0.001804894 -0.001805209 -0.0018051355 -0.0018046797 -0.0018040462][-0.0018089757 -0.0018077033 -0.0018069452 -0.0018061164 -0.0018053479 -0.0018048739 -0.0018048441 -0.0018054253 -0.0018062724 -0.0018068001 -0.0018073748 -0.0018082351 -0.0018087132 -0.0018087026 -0.0018081698][-0.0018126838 -0.00181129 -0.0018101626 -0.0018090174 -0.001807949 -0.0018071859 -0.0018069249 -0.0018074014 -0.0018084305 -0.0018092905 -0.0018103725 -0.0018117342 -0.0018128313 -0.0018133151 -0.0018128831][-0.0018172743 -0.0018157667 -0.0018142364 -0.0018126211 -0.0018110754 -0.0018097992 -0.0018090708 -0.0018092465 -0.001810426 -0.0018115474 -0.0018129256 -0.0018146007 -0.0018161434 -0.0018168624 -0.0018162939][-0.0018218565 -0.0018202161 -0.0018184148 -0.001816496 -0.0018143754 -0.0018123301 -0.0018108408 -0.0018105739 -0.0018118335 -0.0018132691 -0.0018149838 -0.0018168964 -0.0018185955 -0.0018193222 -0.0018185654]]...]
INFO - root - 2017-12-09 13:35:22.764830: step 27210, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.863 sec/batch; 73h:10m:30s remains)
INFO - root - 2017-12-09 13:35:31.405246: step 27220, loss = 0.83, batch loss = 0.70 (9.1 examples/sec; 0.877 sec/batch; 74h:23m:53s remains)
INFO - root - 2017-12-09 13:35:39.982322: step 27230, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.853 sec/batch; 72h:19m:46s remains)
INFO - root - 2017-12-09 13:35:48.720132: step 27240, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.844 sec/batch; 71h:35m:42s remains)
INFO - root - 2017-12-09 13:35:57.465920: step 27250, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.854 sec/batch; 72h:26m:27s remains)
INFO - root - 2017-12-09 13:36:06.068741: step 27260, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.829 sec/batch; 70h:18m:12s remains)
INFO - root - 2017-12-09 13:36:14.808325: step 27270, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.860 sec/batch; 72h:52m:50s remains)
INFO - root - 2017-12-09 13:36:23.280485: step 27280, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.843 sec/batch; 71h:27m:19s remains)
INFO - root - 2017-12-09 13:36:31.998331: step 27290, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.860 sec/batch; 72h:55m:17s remains)
INFO - root - 2017-12-09 13:36:40.372985: step 27300, loss = 0.82, batch loss = 0.69 (9.8 examples/sec; 0.812 sec/batch; 68h:52m:23s remains)
2017-12-09 13:36:41.219206: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0018190895 -0.0018153451 -0.0018120838 -0.0018090307 -0.0018063288 -0.0018045533 -0.001804153 -0.0018053931 -0.0018081758 -0.0018120215 -0.0018158762 -0.0018190144 -0.0018214849 -0.0018234709 -0.0018248694][-0.001821398 -0.0018192449 -0.001817349 -0.0018152839 -0.0018134657 -0.001811962 -0.0018110301 -0.001811171 -0.0018125976 -0.0018153535 -0.0018185043 -0.0018211814 -0.0018233479 -0.0018249854 -0.0018261521][-0.001823809 -0.0018235538 -0.0018232717 -0.0018224783 -0.0018215535 -0.0018198552 -0.0018181924 -0.0018172041 -0.0018172361 -0.0018184821 -0.0018203956 -0.0018226547 -0.0018247445 -0.0018264879 -0.001827674][-0.0018248083 -0.0018257833 -0.0018268917 -0.0018270978 -0.0018266855 -0.0018249839 -0.0018228301 -0.0018210799 -0.0018201036 -0.001820191 -0.0018212165 -0.0018228979 -0.0018245482 -0.0018259929 -0.0018270974][-0.001824078 -0.001825552 -0.0018275 -0.001828532 -0.0018285678 -0.0018270923 -0.0018248409 -0.0018224913 -0.0018206688 -0.001819971 -0.0018203186 -0.0018213977 -0.0018225074 -0.0018236424 -0.0018246623][-0.0018224298 -0.0018236336 -0.0018254285 -0.0018267102 -0.0018268813 -0.0018255964 -0.0018233162 -0.001820925 -0.0018188531 -0.001817861 -0.001817991 -0.0018190391 -0.0018200879 -0.0018210465 -0.0018220138][-0.0018207 -0.0018212307 -0.0018224864 -0.0018233891 -0.0018232651 -0.0018222422 -0.001820537 -0.0018186861 -0.0018169543 -0.0018162689 -0.0018165738 -0.0018173308 -0.0018179778 -0.0018184907 -0.0018191735][-0.0018186867 -0.0018185696 -0.0018193247 -0.0018198708 -0.0018195002 -0.0018188076 -0.001817702 -0.00181651 -0.0018152016 -0.0018147646 -0.0018153763 -0.0018158599 -0.0018161751 -0.0018163671 -0.0018166851][-0.0018167782 -0.00181596 -0.0018162105 -0.0018163768 -0.0018159322 -0.0018155019 -0.001814948 -0.0018142671 -0.0018133185 -0.0018131356 -0.0018138262 -0.001814259 -0.0018144385 -0.0018145526 -0.0018147001][-0.0018153482 -0.001814125 -0.0018139093 -0.0018137224 -0.0018132098 -0.0018129613 -0.0018127096 -0.0018123995 -0.0018118615 -0.0018120562 -0.0018129336 -0.0018134918 -0.0018137884 -0.0018138973 -0.0018138697][-0.0018148415 -0.0018135485 -0.0018130888 -0.0018126498 -0.0018120653 -0.001811898 -0.0018117925 -0.0018117864 -0.0018115998 -0.001811999 -0.0018128214 -0.0018132691 -0.0018135603 -0.0018136005 -0.0018135036][-0.001815071 -0.0018138004 -0.0018132697 -0.0018126763 -0.0018119863 -0.0018117346 -0.0018117495 -0.001811936 -0.0018120091 -0.001812382 -0.0018129809 -0.0018132685 -0.001813482 -0.001813572 -0.0018135177][-0.0018154567 -0.0018143274 -0.0018137768 -0.0018132034 -0.0018126302 -0.001812358 -0.0018123833 -0.0018124374 -0.0018125413 -0.0018127548 -0.0018130984 -0.0018135139 -0.001813922 -0.0018141252 -0.0018140932][-0.0018160408 -0.0018150383 -0.0018144577 -0.0018138518 -0.001813314 -0.0018130342 -0.0018129433 -0.0018129012 -0.0018129736 -0.0018130837 -0.0018133139 -0.0018137721 -0.0018142776 -0.0018145675 -0.0018146274][-0.0018167187 -0.0018158292 -0.0018152079 -0.0018145799 -0.0018140194 -0.0018136292 -0.0018134207 -0.0018133572 -0.0018135131 -0.0018137151 -0.001813949 -0.0018143562 -0.0018148276 -0.0018150952 -0.0018151597]]...]
INFO - root - 2017-12-09 13:36:49.721223: step 27310, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 73h:14m:47s remains)
INFO - root - 2017-12-09 13:36:58.376326: step 27320, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.886 sec/batch; 75h:05m:53s remains)
INFO - root - 2017-12-09 13:37:06.875182: step 27330, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.876 sec/batch; 74h:14m:28s remains)
INFO - root - 2017-12-09 13:37:15.401439: step 27340, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.847 sec/batch; 71h:46m:54s remains)
INFO - root - 2017-12-09 13:37:23.931238: step 27350, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 73h:39m:19s remains)
INFO - root - 2017-12-09 13:37:32.473786: step 27360, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.863 sec/batch; 73h:09m:10s remains)
INFO - root - 2017-12-09 13:37:41.038412: step 27370, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.888 sec/batch; 75h:15m:53s remains)
INFO - root - 2017-12-09 13:37:49.645640: step 27380, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.870 sec/batch; 73h:42m:14s remains)
INFO - root - 2017-12-09 13:37:58.390011: step 27390, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.852 sec/batch; 72h:11m:51s remains)
INFO - root - 2017-12-09 13:38:06.965582: step 27400, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.844 sec/batch; 71h:29m:56s remains)
2017-12-09 13:38:07.858270: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0018211672 -0.0018181902 -0.0018160981 -0.0018160134 -0.0018171191 -0.0018191783 -0.0018216668 -0.001824065 -0.0018256659 -0.0018262344 -0.0018262918 -0.0018249972 -0.0018224261 -0.0018193683 -0.0018170445][-0.001818586 -0.0018147152 -0.0018125279 -0.0018127172 -0.0018146043 -0.0018176559 -0.0018210083 -0.0018238322 -0.0018251649 -0.0018253977 -0.0018255673 -0.0018250283 -0.0018235908 -0.0018215714 -0.0018198454][-0.0018166204 -0.0018123389 -0.0018101407 -0.001810758 -0.0018131261 -0.0018167349 -0.0018204307 -0.0018234006 -0.0018245529 -0.0018245457 -0.0018251167 -0.00182524 -0.0018246522 -0.0018234712 -0.0018221374][-0.0018154031 -0.0018110829 -0.0018090006 -0.0018098697 -0.0018123694 -0.0018157157 -0.0018191952 -0.0018220497 -0.0018232758 -0.0018233657 -0.0018241786 -0.0018249237 -0.0018249562 -0.0018241034 -0.0018227567][-0.0018147547 -0.0018108279 -0.0018091135 -0.0018097616 -0.0018118266 -0.0018143107 -0.0018168555 -0.0018193207 -0.0018207128 -0.0018214147 -0.0018225265 -0.0018237974 -0.0018242671 -0.0018234934 -0.0018218177][-0.0018145603 -0.0018113895 -0.0018100804 -0.0018104585 -0.0018118798 -0.0018134211 -0.0018150065 -0.0018166099 -0.0018180201 -0.001819451 -0.0018210287 -0.0018223264 -0.0018226177 -0.0018216015 -0.0018195945][-0.0018147688 -0.0018124944 -0.001811624 -0.0018120682 -0.0018127462 -0.0018130018 -0.0018132145 -0.0018136112 -0.0018145737 -0.0018161124 -0.0018179618 -0.0018193548 -0.0018194632 -0.0018182527 -0.0018162942][-0.0018156805 -0.0018141732 -0.001813598 -0.0018138138 -0.0018136532 -0.0018127833 -0.0018115309 -0.0018107373 -0.0018111224 -0.0018123619 -0.0018139374 -0.0018151155 -0.0018152124 -0.0018141847 -0.0018125989][-0.0018170254 -0.0018157541 -0.0018151384 -0.0018146654 -0.0018135634 -0.0018115697 -0.0018094779 -0.0018078942 -0.0018076344 -0.0018085347 -0.0018097197 -0.0018105874 -0.0018106615 -0.0018100474 -0.0018092151][-0.0018179156 -0.0018165556 -0.0018154969 -0.0018143646 -0.0018125309 -0.001810131 -0.0018078353 -0.0018058093 -0.001804857 -0.0018049775 -0.0018055302 -0.00180607 -0.0018060659 -0.0018059873 -0.0018061779][-0.0018186521 -0.0018167 -0.0018149135 -0.0018131508 -0.0018109035 -0.0018084552 -0.0018061745 -0.0018041648 -0.0018028375 -0.0018023042 -0.0018022074 -0.0018022851 -0.0018023183 -0.0018027994 -0.0018040593][-0.0018191226 -0.0018160203 -0.0018135505 -0.0018113856 -0.0018089196 -0.00180647 -0.001804352 -0.0018023859 -0.0018007874 -0.0017998555 -0.0017995121 -0.0017993997 -0.0017994868 -0.0018005649 -0.0018027117][-0.0018191019 -0.0018149173 -0.001811645 -0.001809022 -0.0018065058 -0.001804342 -0.0018025732 -0.0018008599 -0.0017992901 -0.001798222 -0.001797613 -0.00179744 -0.001797706 -0.0017993189 -0.0018020434][-0.0018181566 -0.0018129002 -0.0018088999 -0.0018060481 -0.0018037109 -0.0018019853 -0.0018006997 -0.0017993028 -0.001797852 -0.0017967385 -0.0017960442 -0.0017959053 -0.0017965125 -0.0017985002 -0.0018016302][-0.0018159252 -0.0018100067 -0.0018055928 -0.0018029007 -0.001800976 -0.001799725 -0.001798708 -0.001797625 -0.0017964414 -0.0017954124 -0.0017947067 -0.0017946291 -0.0017954587 -0.0017976994 -0.001801254]]...]
INFO - root - 2017-12-09 13:38:16.584261: step 27410, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.885 sec/batch; 74h:58m:44s remains)
INFO - root - 2017-12-09 13:38:25.326704: step 27420, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.861 sec/batch; 72h:57m:58s remains)
INFO - root - 2017-12-09 13:38:33.786357: step 27430, loss = 0.82, batch loss = 0.69 (9.8 examples/sec; 0.814 sec/batch; 68h:59m:09s remains)
INFO - root - 2017-12-09 13:38:42.448047: step 27440, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 73h:01m:52s remains)
INFO - root - 2017-12-09 13:38:51.100540: step 27450, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 73h:15m:06s remains)
INFO - root - 2017-12-09 13:38:59.832252: step 27460, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.888 sec/batch; 75h:15m:26s remains)
INFO - root - 2017-12-09 13:39:08.576223: step 27470, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.882 sec/batch; 74h:43m:10s remains)
INFO - root - 2017-12-09 13:39:17.080426: step 27480, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.886 sec/batch; 75h:02m:34s remains)
INFO - root - 2017-12-09 13:39:25.632880: step 27490, loss = 0.82, batch loss = 0.69 (10.8 examples/sec; 0.741 sec/batch; 62h:44m:43s remains)
INFO - root - 2017-12-09 13:39:34.226730: step 27500, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.856 sec/batch; 72h:33m:51s remains)
2017-12-09 13:39:35.112679: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0012496109 -0.00052052981 0.00028741604 0.00090450433 0.0011184033 0.0010928005 0.0010741818 0.0011552001 0.0013773862 0.00161829 0.0018899041 0.0020397594 0.00180825 0.00095066277 -0.00025966286][-0.0013423468 -0.00072024879 -4.6927016e-06 0.0005675588 0.0007886464 0.00079099147 0.00080957718 0.00093514042 0.0012446124 0.0016249128 0.0020672195 0.0023213318 0.0020950772 0.0011314655 -0.00017577794][-0.0014219061 -0.00090482883 -0.00030960166 0.00016819558 0.00036363129 0.00038093573 0.00041797722 0.0005603371 0.000899298 0.0013518062 0.0019421772 0.0023671384 0.00223782 0.0012472145 -0.00013106992][-0.0015175138 -0.0011121284 -0.0006341011 -0.00024589803 -7.46774e-05 -4.5043183e-05 8.2452316e-06 0.00014371134 0.00047643541 0.00098555523 0.0017036357 0.0022637704 0.002208448 0.0012260206 -0.00015164854][-0.0016606401 -0.0014031759 -0.0010774257 -0.00079580827 -0.00064903195 -0.00059749081 -0.00052017951 -0.00038945838 -7.8604906e-05 0.00044744776 0.0012270665 0.0018513649 0.0018328516 0.00091234513 -0.00034867844][-0.001771525 -0.0016616682 -0.0014985647 -0.0013402854 -0.001242368 -0.0011858857 -0.0011006852 -0.00097235 -0.00071339414 -0.00023240305 0.00050867733 0.0011119411 0.0011148847 0.00032244425 -0.00070550491][-0.0018122129 -0.0017900886 -0.0017467525 -0.001692788 -0.0016486879 -0.0016133197 -0.0015530658 -0.0014572045 -0.0012628199 -0.00088667596 -0.0002714087 0.00024817919 0.00027165411 -0.00032888248 -0.0010773045][-0.0018179016 -0.001814765 -0.0018123883 -0.0018084623 -0.001800365 -0.0017872135 -0.001760595 -0.0017124652 -0.0015907736 -0.0013140598 -0.00083869835 -0.00041770504 -0.00037224055 -0.00079369871 -0.0013225728][-0.0018174151 -0.0018150371 -0.0018143494 -0.0018141492 -0.0018139831 -0.0018111402 -0.0017993979 -0.0017805429 -0.0017212215 -0.001542371 -0.0012005409 -0.000881748 -0.000836706 -0.0011226281 -0.0014863248][-0.0018169106 -0.0018147344 -0.0018142877 -0.0018141442 -0.0018143995 -0.0018124478 -0.0018037102 -0.0017903963 -0.0017612586 -0.0016635114 -0.001458752 -0.0012583709 -0.0012296974 -0.0014069736 -0.001626381][-0.0018175181 -0.0018150803 -0.0018145476 -0.0018143354 -0.0018143398 -0.0018147486 -0.0018119185 -0.0018026357 -0.0017906005 -0.0017520518 -0.0016617331 -0.0015700995 -0.0015607757 -0.0016463537 -0.0017425199][-0.0018175837 -0.001815287 -0.0018144695 -0.0018141898 -0.0018141971 -0.0018145893 -0.0018150967 -0.0018142348 -0.0018105069 -0.0018034239 -0.001780912 -0.0017573759 -0.0017571516 -0.0017819288 -0.0018036929][-0.001817929 -0.001815876 -0.0018152418 -0.001814506 -0.0018141904 -0.0018144076 -0.0018149533 -0.0018152948 -0.0018153116 -0.0018144774 -0.0018119949 -0.0018093982 -0.0018094052 -0.0018123592 -0.0018143434][-0.0018181951 -0.0018166808 -0.0018160761 -0.0018154631 -0.0018150922 -0.0018149232 -0.0018150518 -0.0018152068 -0.0018153326 -0.0018155893 -0.0018155784 -0.0018152467 -0.0018149022 -0.0018146669 -0.0018145185][-0.0018183277 -0.0018171754 -0.0018167684 -0.001816263 -0.0018158212 -0.0018153646 -0.0018152129 -0.0018149536 -0.0018147818 -0.0018148426 -0.0018149046 -0.0018149823 -0.0018148014 -0.0018145371 -0.001814369]]...]
INFO - root - 2017-12-09 13:39:43.728507: step 27510, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 73h:02m:19s remains)
INFO - root - 2017-12-09 13:39:52.329269: step 27520, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.848 sec/batch; 71h:49m:50s remains)
INFO - root - 2017-12-09 13:40:00.908820: step 27530, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.879 sec/batch; 74h:25m:53s remains)
INFO - root - 2017-12-09 13:40:09.539253: step 27540, loss = 0.83, batch loss = 0.70 (9.5 examples/sec; 0.838 sec/batch; 71h:01m:02s remains)
INFO - root - 2017-12-09 13:40:18.276626: step 27550, loss = 0.82, batch loss = 0.69 (8.3 examples/sec; 0.966 sec/batch; 81h:50m:46s remains)
INFO - root - 2017-12-09 13:40:26.966184: step 27560, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 73h:09m:54s remains)
INFO - root - 2017-12-09 13:40:35.663317: step 27570, loss = 0.81, batch loss = 0.68 (9.0 examples/sec; 0.893 sec/batch; 75h:40m:43s remains)
INFO - root - 2017-12-09 13:40:44.172893: step 27580, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 73h:36m:34s remains)
INFO - root - 2017-12-09 13:40:52.787732: step 27590, loss = 0.82, batch loss = 0.69 (10.5 examples/sec; 0.762 sec/batch; 64h:32m:13s remains)
INFO - root - 2017-12-09 13:41:01.332317: step 27600, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.876 sec/batch; 74h:09m:48s remains)
2017-12-09 13:41:02.219927: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.00052805291 -0.00052007206 -0.00061579188 -0.0006866256 -0.00075138535 -0.00082332373 -0.00090534647 -0.00099452375 -0.0010851438 -0.0011753587 -0.0012644576 -0.0013443968 -0.001407079 -0.001451066 -0.0014890338][0.0005448634 0.00058347534 0.00041439512 0.00030367926 0.00021645648 0.0001246481 7.5123971e-06 -0.00015203201 -0.000343843 -0.00055324659 -0.00076081546 -0.00094499096 -0.0010867859 -0.001184882 -0.0012662946][0.0014596476 0.001505542 0.0012596635 0.0011290129 0.001060055 0.000994159 0.00087214389 0.00065431197 0.00035478279 4.2736065e-06 -0.00034969463 -0.000661689 -0.00090153 -0.0010645238 -0.001186703][0.0017352049 0.001785193 0.0015469032 0.0014873851 0.0015175996 0.0015355275 0.0014418323 0.0011816785 0.00078354834 0.0003015917 -0.00019272079 -0.00062888605 -0.00096791628 -0.0011905825 -0.001338893][0.0013646105 0.0013961472 0.0012424161 0.0013100497 0.0014705817 0.0015792466 0.0015166333 0.0012337371 0.00078103656 0.00023142842 -0.00032359641 -0.00080161006 -0.001169669 -0.0014023827 -0.0015452852][0.00070702436 0.00074628287 0.00069861754 0.000866789 0.0011077531 0.0012583864 0.0012007238 0.0009096273 0.00045364967 -8.7482622e-05 -0.00061984477 -0.0010624257 -0.0013924977 -0.0015918911 -0.0017064573][-0.00019359402 -0.0001775556 -0.00017171155 -2.1816231e-06 0.00021143223 0.00032901519 0.00026345171 1.7786399e-05 -0.00034106243 -0.00074136583 -0.0011127481 -0.0013982947 -0.0015953018 -0.0017070986 -0.0017699467][-0.0010421989 -0.001044249 -0.0010228053 -0.00090929522 -0.00077892758 -0.0007131662 -0.00076132035 -0.00091485068 -0.001128499 -0.0013488655 -0.0015362636 -0.0016644562 -0.0017446273 -0.0017860588 -0.0018079049][-0.001627937 -0.0016293604 -0.0016202449 -0.0015859599 -0.0015475082 -0.0015286809 -0.0015502822 -0.0016067044 -0.0016792411 -0.001742024 -0.0017848324 -0.0018069571 -0.0018164781 -0.0018193919 -0.0018205487][-0.0018182673 -0.0018157393 -0.0018117486 -0.0018062167 -0.0017993374 -0.0017926421 -0.001790307 -0.001794842 -0.0018037033 -0.001811648 -0.0018165483 -0.0018185237 -0.0018190992 -0.0018192753 -0.0018195562][-0.0018216413 -0.0018211644 -0.001821294 -0.0018213206 -0.001820954 -0.0018194144 -0.0018173731 -0.0018159588 -0.00181575 -0.0018166705 -0.001817589 -0.0018182268 -0.0018183595 -0.0018184005 -0.0018187439][-0.0018205934 -0.0018199713 -0.0018199671 -0.0018200199 -0.0018200369 -0.0018193993 -0.0018182137 -0.001817163 -0.0018165798 -0.0018167609 -0.0018171979 -0.0018177476 -0.0018180278 -0.0018180961 -0.001818346][-0.0018199532 -0.0018192151 -0.0018191409 -0.001819043 -0.0018190498 -0.0018187115 -0.0018180818 -0.0018172634 -0.0018166299 -0.0018164307 -0.0018164418 -0.0018168691 -0.0018171652 -0.0018174534 -0.0018179371][-0.0018199525 -0.0018191314 -0.0018189118 -0.0018187247 -0.0018187114 -0.0018184698 -0.0018179896 -0.0018173626 -0.0018167124 -0.001816294 -0.0018161469 -0.0018164683 -0.0018164872 -0.0018167654 -0.0018174027][-0.0018208483 -0.0018200286 -0.0018196926 -0.0018194049 -0.0018192941 -0.0018190979 -0.0018188279 -0.0018183812 -0.0018178693 -0.0018174405 -0.0018172802 -0.0018173775 -0.0018172764 -0.0018173467 -0.001817772]]...]
INFO - root - 2017-12-09 13:41:10.760491: step 27610, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.853 sec/batch; 72h:12m:37s remains)
INFO - root - 2017-12-09 13:41:19.499783: step 27620, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.880 sec/batch; 74h:33m:58s remains)
INFO - root - 2017-12-09 13:41:27.943230: step 27630, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 73h:29m:50s remains)
INFO - root - 2017-12-09 13:41:36.686835: step 27640, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.878 sec/batch; 74h:20m:03s remains)
INFO - root - 2017-12-09 13:41:45.313549: step 27650, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.875 sec/batch; 74h:07m:28s remains)
INFO - root - 2017-12-09 13:41:53.969682: step 27660, loss = 0.81, batch loss = 0.68 (9.1 examples/sec; 0.878 sec/batch; 74h:22m:33s remains)
INFO - root - 2017-12-09 13:42:02.600636: step 27670, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.888 sec/batch; 75h:11m:40s remains)
INFO - root - 2017-12-09 13:42:11.142305: step 27680, loss = 0.82, batch loss = 0.69 (9.8 examples/sec; 0.817 sec/batch; 69h:10m:58s remains)
INFO - root - 2017-12-09 13:42:19.520051: step 27690, loss = 0.82, batch loss = 0.69 (10.5 examples/sec; 0.761 sec/batch; 64h:24m:01s remains)
INFO - root - 2017-12-09 13:42:28.129791: step 27700, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 73h:35m:18s remains)
2017-12-09 13:42:28.991962: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.31552768 0.33335298 0.35046208 0.36188993 0.365009 0.36006966 0.3500253 0.33350462 0.30938631 0.28312552 0.26030874 0.2463551 0.24275616 0.24966952 0.26637015][0.31653011 0.33863926 0.35867295 0.3731837 0.3781378 0.3726424 0.36040542 0.339996 0.31161624 0.28021333 0.25323915 0.23562221 0.22974961 0.23575246 0.25322452][0.31009865 0.33594194 0.35961294 0.37710702 0.38413766 0.38011032 0.36707243 0.34409541 0.31274119 0.27778488 0.24781446 0.22706726 0.21825598 0.22235522 0.23858017][0.30299032 0.33115259 0.35773885 0.37841618 0.38901731 0.38723922 0.37513894 0.35157207 0.3184211 0.28174794 0.24969542 0.2271481 0.21642551 0.21780768 0.2315816][0.29745865 0.32649428 0.35409388 0.37686858 0.39001992 0.39167708 0.38232937 0.36061579 0.3291972 0.29290932 0.26042965 0.23627703 0.22354217 0.22249176 0.2323876][0.29098997 0.32043695 0.34777066 0.37089035 0.38599852 0.39112225 0.38544816 0.36796886 0.34054112 0.30773827 0.2772941 0.25285751 0.23767032 0.23343924 0.23885329][0.28343737 0.31337658 0.34024456 0.36277115 0.37829897 0.38551915 0.3838931 0.371595 0.35023248 0.32304603 0.2963188 0.27302992 0.25676247 0.24927175 0.25009274][0.27581847 0.30629995 0.33270189 0.35410106 0.36879355 0.37595621 0.37625584 0.36831236 0.35242218 0.33161557 0.31012139 0.28941885 0.27325225 0.26356441 0.26003939][0.2681793 0.29836079 0.32330179 0.34346667 0.35757852 0.36456919 0.36630014 0.36193794 0.35130459 0.33590609 0.31845284 0.30089712 0.28549418 0.27386719 0.26654974][0.26042697 0.28992108 0.31308791 0.3317554 0.34464669 0.35093063 0.35291269 0.35060149 0.34388912 0.33300006 0.319472 0.30436227 0.28955275 0.27700233 0.26675877][0.24943486 0.27787733 0.29960951 0.31627649 0.32746622 0.33336544 0.33543009 0.33377051 0.32881913 0.32098198 0.31045327 0.2976681 0.28370145 0.27060145 0.25903517][0.23888035 0.26590815 0.28568 0.30056667 0.30977169 0.3135674 0.31395552 0.31181002 0.30727956 0.30066955 0.29222128 0.28152928 0.26921016 0.25644934 0.24481289][0.2256577 0.25066012 0.26785764 0.28050441 0.28800076 0.29023153 0.28873891 0.2852805 0.28052568 0.2744844 0.26728031 0.2584818 0.24825752 0.23726851 0.22696777][0.21279424 0.23415983 0.24790403 0.25792068 0.26332802 0.26449564 0.26249832 0.25883034 0.25444028 0.24924332 0.24324441 0.23597546 0.22754793 0.21849976 0.2101589][0.20203023 0.22028552 0.23112093 0.2383655 0.24141507 0.24084668 0.23767246 0.23348498 0.22931843 0.22501244 0.22051373 0.21492811 0.20871904 0.20197195 0.19591402]]...]
INFO - root - 2017-12-09 13:42:37.701961: step 27710, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.877 sec/batch; 74h:16m:27s remains)
INFO - root - 2017-12-09 13:42:46.333080: step 27720, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.851 sec/batch; 72h:03m:57s remains)
INFO - root - 2017-12-09 13:42:54.905680: step 27730, loss = 0.82, batch loss = 0.69 (10.6 examples/sec; 0.758 sec/batch; 64h:08m:36s remains)
INFO - root - 2017-12-09 13:43:03.284039: step 27740, loss = 0.82, batch loss = 0.69 (8.6 examples/sec; 0.931 sec/batch; 78h:50m:13s remains)
INFO - root - 2017-12-09 13:43:11.964791: step 27750, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.865 sec/batch; 73h:14m:13s remains)
INFO - root - 2017-12-09 13:43:20.689108: step 27760, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 73h:21m:34s remains)
INFO - root - 2017-12-09 13:43:29.321145: step 27770, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.877 sec/batch; 74h:14m:50s remains)
INFO - root - 2017-12-09 13:43:37.945729: step 27780, loss = 0.82, batch loss = 0.69 (11.3 examples/sec; 0.706 sec/batch; 59h:45m:04s remains)
INFO - root - 2017-12-09 13:43:46.387397: step 27790, loss = 0.82, batch loss = 0.69 (10.5 examples/sec; 0.766 sec/batch; 64h:47m:47s remains)
INFO - root - 2017-12-09 13:43:54.964701: step 27800, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.843 sec/batch; 71h:21m:20s remains)
2017-12-09 13:43:55.833934: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0018292874 -0.0018283358 -0.001827326 -0.0018261495 -0.0018250041 -0.0018239693 -0.0018230806 -0.0018227499 -0.0018225467 -0.0018222407 -0.0018215922 -0.0018207876 -0.001819663 -0.0018181937 -0.0018164997][-0.0018312339 -0.0018302364 -0.0018292222 -0.0018281192 -0.0018269849 -0.0018259968 -0.0018252629 -0.0018251435 -0.0018251918 -0.0018250273 -0.0018243 -0.0018232558 -0.001821872 -0.0018201066 -0.0018180863][-0.0018317843 -0.0018306964 -0.001829834 -0.0018289688 -0.0018280797 -0.0018274405 -0.001827075 -0.0018272974 -0.0018277032 -0.001827784 -0.0018271125 -0.0018259167 -0.0018243328 -0.0018224551 -0.0018202655][-0.0018312125 -0.0018300291 -0.0018291643 -0.0018283479 -0.0018276044 -0.0018273947 -0.0018276703 -0.0018284786 -0.0018294357 -0.0018299654 -0.0018296342 -0.001828559 -0.0018270253 -0.00182513 -0.0018228856][-0.0018298113 -0.001828572 -0.0018276235 -0.0018266699 -0.0018259506 -0.0018261356 -0.0018270817 -0.0018286605 -0.0018302531 -0.0018313306 -0.0018315621 -0.0018308579 -0.0018296135 -0.0018278841 -0.0018257512][-0.001827572 -0.0018265484 -0.0018257598 -0.001824856 -0.0018240702 -0.0018242706 -0.0018255699 -0.001827846 -0.0018301603 -0.0018318972 -0.0018327077 -0.001832522 -0.0018316385 -0.0018302201 -0.0018283271][-0.0018246411 -0.0018240229 -0.0018236171 -0.0018229231 -0.0018218401 -0.0018215519 -0.0018227237 -0.0018254373 -0.0018285357 -0.0018310676 -0.001832561 -0.0018329028 -0.0018324275 -0.0018312802 -0.001829647][-0.0018219796 -0.0018217319 -0.0018216146 -0.0018210225 -0.0018196821 -0.0018186594 -0.0018191007 -0.0018216203 -0.0018250921 -0.0018281136 -0.0018301822 -0.0018312024 -0.0018312018 -0.0018303315 -0.0018289529][-0.0018197702 -0.0018199051 -0.0018199815 -0.0018195095 -0.0018183423 -0.001817186 -0.0018168846 -0.0018182299 -0.0018209274 -0.0018237372 -0.0018259935 -0.0018274132 -0.0018278271 -0.0018273122 -0.0018262889][-0.0018177333 -0.0018181079 -0.0018184168 -0.0018182086 -0.0018174767 -0.0018167293 -0.00181625 -0.0018164178 -0.0018175405 -0.0018193329 -0.0018212509 -0.0018225758 -0.0018231018 -0.0018228943 -0.0018222461][-0.0018161467 -0.0018165871 -0.0018170242 -0.0018170163 -0.0018165713 -0.001816088 -0.0018156128 -0.0018152146 -0.0018152025 -0.0018158634 -0.0018170399 -0.0018180464 -0.0018185265 -0.0018184631 -0.0018179317][-0.0018149273 -0.001815195 -0.001815617 -0.0018156624 -0.0018152833 -0.0018149106 -0.00181449 -0.0018139636 -0.0018134384 -0.0018132785 -0.0018136662 -0.001814156 -0.0018144572 -0.0018144741 -0.0018141057][-0.0018140553 -0.0018140358 -0.0018142563 -0.0018141489 -0.0018137974 -0.0018135925 -0.0018133365 -0.0018128009 -0.0018121182 -0.0018115838 -0.0018113748 -0.0018113879 -0.0018114365 -0.001811486 -0.0018113543][-0.0018137388 -0.0018132998 -0.0018131789 -0.0018129249 -0.0018126604 -0.0018126034 -0.0018124398 -0.0018119625 -0.0018113421 -0.0018108534 -0.0018105302 -0.0018103784 -0.0018103149 -0.0018102896 -0.0018101884][-0.0018137876 -0.0018129678 -0.0018125967 -0.0018122533 -0.0018119653 -0.001811836 -0.0018116613 -0.0018112827 -0.0018108741 -0.0018106009 -0.0018104772 -0.0018104105 -0.0018103863 -0.0018104131 -0.0018103597]]...]
INFO - root - 2017-12-09 13:44:04.407718: step 27810, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.872 sec/batch; 73h:47m:26s remains)
INFO - root - 2017-12-09 13:44:12.983050: step 27820, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.861 sec/batch; 72h:49m:38s remains)
INFO - root - 2017-12-09 13:44:21.643355: step 27830, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.866 sec/batch; 73h:18m:16s remains)
INFO - root - 2017-12-09 13:44:30.130613: step 27840, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.851 sec/batch; 71h:58m:50s remains)
INFO - root - 2017-12-09 13:44:38.803121: step 27850, loss = 0.81, batch loss = 0.68 (8.8 examples/sec; 0.906 sec/batch; 76h:39m:27s remains)
INFO - root - 2017-12-09 13:44:47.426458: step 27860, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.873 sec/batch; 73h:52m:10s remains)
INFO - root - 2017-12-09 13:44:56.342760: step 27870, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.890 sec/batch; 75h:16m:50s remains)
INFO - root - 2017-12-09 13:45:05.048526: step 27880, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 73h:29m:52s remains)
INFO - root - 2017-12-09 13:45:13.352586: step 27890, loss = 0.82, batch loss = 0.69 (10.7 examples/sec; 0.750 sec/batch; 63h:25m:23s remains)
INFO - root - 2017-12-09 13:45:21.909291: step 27900, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.883 sec/batch; 74h:43m:41s remains)
2017-12-09 13:45:22.738311: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0018221422 -0.0018204757 -0.0018194446 -0.0018190402 -0.0018190527 -0.0018190799 -0.0018189749 -0.0018185722 -0.0018184828 -0.0018184507 -0.0018181207 -0.0018178027 -0.0018176369 -0.0018183077 -0.0018192302][-0.0018241458 -0.0018219784 -0.0018206501 -0.0018199699 -0.0018200184 -0.0018200967 -0.0018200417 -0.001819549 -0.0018192875 -0.0018196637 -0.0018199434 -0.0018203343 -0.0018207027 -0.0018217396 -0.0018229986][-0.0018263311 -0.0018238181 -0.0018220926 -0.0018208741 -0.0018203728 -0.0018201118 -0.0018198091 -0.0018192488 -0.0018190471 -0.0018197561 -0.0018208409 -0.0018221156 -0.0018232801 -0.0018248672 -0.0018264669][-0.0018269194 -0.0018241068 -0.0018220614 -0.0018204519 -0.0018193921 -0.0018186636 -0.0018179113 -0.0018172765 -0.0018172369 -0.0018183413 -0.0018201123 -0.0018222262 -0.0018241928 -0.0018261956 -0.0018282302][-0.0018272691 -0.001824151 -0.0018217191 -0.0018197494 -0.0018182338 -0.0018169709 -0.0018159169 -0.0018153662 -0.001815747 -0.001817456 -0.0018198133 -0.0018223799 -0.0018248097 -0.0018271515 -0.0018294792][-0.0018289119 -0.0018258881 -0.0018231515 -0.0018206853 -0.0018185651 -0.0018166305 -0.0018150419 -0.0018142398 -0.0018146079 -0.0018164658 -0.0018192377 -0.0018223085 -0.0018252063 -0.0018280136 -0.0018307195][-0.0018309226 -0.0018284168 -0.0018257147 -0.0018229131 -0.0018203034 -0.0018178575 -0.0018158169 -0.0018149001 -0.0018154915 -0.0018176384 -0.0018206786 -0.001823988 -0.0018271019 -0.0018300522 -0.0018325858][-0.0018319763 -0.0018301421 -0.0018277254 -0.0018249473 -0.0018220718 -0.0018194431 -0.0018173187 -0.0018164556 -0.0018173971 -0.0018198395 -0.0018229776 -0.0018261551 -0.0018290255 -0.0018315136 -0.001833401][-0.0018324144 -0.0018310138 -0.0018289236 -0.0018262333 -0.0018233645 -0.0018208955 -0.0018190491 -0.0018184538 -0.0018196549 -0.0018221893 -0.0018251812 -0.0018279821 -0.0018302741 -0.0018320595 -0.0018331662][-0.001832445 -0.0018315222 -0.0018298748 -0.0018274707 -0.0018248361 -0.0018226414 -0.0018210985 -0.001820593 -0.0018218894 -0.0018243523 -0.0018271143 -0.0018294713 -0.0018311585 -0.0018322278 -0.0018325673][-0.0018322155 -0.0018316105 -0.0018302988 -0.0018281965 -0.0018259666 -0.0018241439 -0.0018228719 -0.0018224078 -0.0018235759 -0.0018258451 -0.0018282768 -0.001830179 -0.001831154 -0.0018315146 -0.001831286][-0.0018317839 -0.0018313793 -0.0018302862 -0.001828643 -0.0018269831 -0.0018257546 -0.0018248989 -0.0018246202 -0.0018257033 -0.0018276616 -0.0018296998 -0.0018311454 -0.0018314071 -0.001830939 -0.0018299866][-0.0018315812 -0.0018312087 -0.0018302301 -0.0018288993 -0.0018276982 -0.0018270299 -0.00182652 -0.0018262202 -0.0018268562 -0.0018282462 -0.0018295768 -0.0018303684 -0.0018300919 -0.0018290924 -0.0018277452][-0.0018310868 -0.0018304357 -0.0018293345 -0.0018283315 -0.0018276408 -0.0018274387 -0.001827307 -0.0018271698 -0.0018275174 -0.0018283874 -0.0018292086 -0.0018294619 -0.0018287462 -0.0018273634 -0.0018256592][-0.0018300308 -0.0018289951 -0.0018278547 -0.0018271118 -0.0018267424 -0.0018268815 -0.0018271152 -0.0018271576 -0.0018273445 -0.0018279018 -0.0018283812 -0.0018282972 -0.0018273263 -0.0018257374 -0.0018238068]]...]
INFO - root - 2017-12-09 13:45:31.384762: step 27910, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.877 sec/batch; 74h:10m:42s remains)
INFO - root - 2017-12-09 13:45:40.028193: step 27920, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.848 sec/batch; 71h:42m:50s remains)
INFO - root - 2017-12-09 13:45:48.791361: step 27930, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.870 sec/batch; 73h:35m:10s remains)
INFO - root - 2017-12-09 13:45:57.261525: step 27940, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.855 sec/batch; 72h:17m:56s remains)
INFO - root - 2017-12-09 13:46:05.883258: step 27950, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.865 sec/batch; 73h:11m:00s remains)
INFO - root - 2017-12-09 13:46:14.594810: step 27960, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.874 sec/batch; 73h:54m:08s remains)
INFO - root - 2017-12-09 13:46:23.297975: step 27970, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 73h:25m:44s remains)
INFO - root - 2017-12-09 13:46:32.014122: step 27980, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.880 sec/batch; 74h:27m:08s remains)
INFO - root - 2017-12-09 13:46:40.471371: step 27990, loss = 0.82, batch loss = 0.69 (10.7 examples/sec; 0.751 sec/batch; 63h:31m:36s remains)
INFO - root - 2017-12-09 13:46:48.959995: step 28000, loss = 0.82, batch loss = 0.69 (10.0 examples/sec; 0.802 sec/batch; 67h:49m:18s remains)
2017-12-09 13:46:49.786190: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0018309446 -0.0018318694 -0.001832272 -0.001831978 -0.0018309188 -0.0018290486 -0.0018265124 -0.0018241411 -0.0018230387 -0.0018234642 -0.0018251104 -0.0018277575 -0.0018303334 -0.0018318924 -0.0018324818][-0.0018293125 -0.0018302067 -0.0018305618 -0.0018302705 -0.0018292402 -0.0018275053 -0.0018250409 -0.0018228755 -0.0018218425 -0.0018223689 -0.001824204 -0.0018269598 -0.0018297756 -0.0018313829 -0.0018319719][-0.0018277683 -0.0018286696 -0.0018291791 -0.0018290783 -0.0018283349 -0.0018269542 -0.0018247769 -0.0018228247 -0.0018219678 -0.0018226978 -0.0018247416 -0.0018274548 -0.0018301586 -0.0018317195 -0.001832567][-0.0018261357 -0.001826948 -0.0018276423 -0.001828 -0.0018279409 -0.0018272863 -0.0018259017 -0.0018244727 -0.0018238196 -0.0018244179 -0.0018261975 -0.0018284781 -0.0018308315 -0.001832235 -0.0018332095][-0.0018248791 -0.0018254233 -0.0018263807 -0.0018273168 -0.0018279755 -0.00182809 -0.001827686 -0.0018271161 -0.001826679 -0.0018269877 -0.0018283059 -0.0018299863 -0.0018317591 -0.0018329483 -0.0018340133][-0.0018237424 -0.0018240029 -0.0018251815 -0.0018265884 -0.001827802 -0.00182861 -0.0018290256 -0.0018292328 -0.0018292598 -0.0018294879 -0.0018303945 -0.0018315046 -0.0018328439 -0.0018338965 -0.001834873][-0.0018230646 -0.0018230632 -0.0018242132 -0.0018258292 -0.0018274281 -0.0018286252 -0.0018294804 -0.0018303132 -0.0018309915 -0.0018317361 -0.0018325122 -0.0018332136 -0.0018342327 -0.0018351538 -0.001835797][-0.0018226159 -0.0018223737 -0.0018234967 -0.0018251333 -0.0018268052 -0.0018280681 -0.0018290423 -0.0018303191 -0.0018319377 -0.0018333974 -0.0018342332 -0.0018347875 -0.0018354359 -0.0018359479 -0.0018362883][-0.0018223036 -0.001822032 -0.0018230169 -0.0018244652 -0.001826128 -0.0018274522 -0.001828389 -0.0018299127 -0.0018322848 -0.0018343187 -0.0018354582 -0.0018359926 -0.0018362032 -0.0018361356 -0.0018360337][-0.0018219133 -0.0018216451 -0.001822373 -0.0018235504 -0.0018250078 -0.0018264623 -0.0018275909 -0.0018293079 -0.0018319199 -0.0018342981 -0.0018358334 -0.0018365281 -0.0018364516 -0.0018359439 -0.0018355539][-0.0018216528 -0.0018213477 -0.0018217943 -0.001822481 -0.0018235557 -0.0018249954 -0.0018263347 -0.0018281315 -0.0018308009 -0.0018332971 -0.0018349848 -0.0018358747 -0.0018358695 -0.0018353403 -0.001834921][-0.0018213826 -0.0018210612 -0.0018211526 -0.0018214143 -0.0018221746 -0.0018235605 -0.0018249702 -0.0018267011 -0.0018291691 -0.0018315663 -0.001833367 -0.0018343574 -0.0018346077 -0.0018345003 -0.0018344729][-0.0018209771 -0.0018206537 -0.001820593 -0.0018205518 -0.0018210229 -0.0018221844 -0.00182354 -0.0018249712 -0.0018269205 -0.0018290199 -0.001830678 -0.0018316979 -0.0018324526 -0.0018329892 -0.0018335388][-0.0018207716 -0.001820358 -0.001820236 -0.0018200565 -0.0018202444 -0.0018211544 -0.0018223174 -0.0018233561 -0.001824626 -0.001826267 -0.0018276873 -0.0018287707 -0.0018298498 -0.0018309552 -0.0018320684][-0.0018206508 -0.0018201012 -0.0018198963 -0.0018197394 -0.0018197467 -0.0018203482 -0.0018211873 -0.0018219164 -0.0018227784 -0.0018239374 -0.0018250045 -0.0018260089 -0.0018273804 -0.0018289468 -0.0018304732]]...]
INFO - root - 2017-12-09 13:46:58.339954: step 28010, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 73h:19m:12s remains)
INFO - root - 2017-12-09 13:47:06.909695: step 28020, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.848 sec/batch; 71h:44m:28s remains)
INFO - root - 2017-12-09 13:47:15.565145: step 28030, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.858 sec/batch; 72h:34m:13s remains)
INFO - root - 2017-12-09 13:47:24.149367: step 28040, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 73h:17m:17s remains)
INFO - root - 2017-12-09 13:47:32.836457: step 28050, loss = 0.81, batch loss = 0.68 (9.0 examples/sec; 0.888 sec/batch; 75h:06m:39s remains)
INFO - root - 2017-12-09 13:47:41.575349: step 28060, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.844 sec/batch; 71h:23m:47s remains)
INFO - root - 2017-12-09 13:47:50.155736: step 28070, loss = 0.83, batch loss = 0.70 (9.1 examples/sec; 0.879 sec/batch; 74h:22m:21s remains)
INFO - root - 2017-12-09 13:47:58.835203: step 28080, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.889 sec/batch; 75h:11m:43s remains)
INFO - root - 2017-12-09 13:48:07.419044: step 28090, loss = 0.82, batch loss = 0.69 (10.5 examples/sec; 0.762 sec/batch; 64h:27m:19s remains)
INFO - root - 2017-12-09 13:48:16.195469: step 28100, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 73h:30m:11s remains)
2017-12-09 13:48:17.024165: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.038301919 0.037608333 0.038859818 0.042307861 0.047131762 0.052169245 0.056473535 0.059231874 0.060629133 0.060539439 0.05885531 0.056781229 0.054025795 0.052272096 0.050625652][0.035807487 0.034815907 0.036275279 0.040348314 0.046078198 0.052621111 0.058753312 0.063361794 0.066190325 0.067009069 0.065570466 0.062886596 0.059580561 0.057051126 0.054859176][0.034006473 0.033204883 0.035339952 0.040504556 0.0475776 0.055728294 0.063679218 0.070081487 0.074318834 0.075872496 0.074704528 0.07172776 0.068025313 0.064804085 0.0619739][0.035321642 0.034981906 0.037875671 0.044292398 0.05256873 0.062265642 0.071789861 0.079759546 0.085249349 0.087385423 0.086651661 0.083264492 0.079352647 0.075540625 0.071939521][0.04113121 0.041709267 0.045494478 0.05291662 0.062180541 0.07265839 0.082890004 0.091609336 0.097340576 0.099843867 0.099248245 0.095724687 0.091634907 0.087347806 0.083561555][0.051296577 0.053105179 0.057723053 0.06580621 0.075429685 0.086043477 0.096020021 0.10437571 0.1096455 0.11137001 0.11034527 0.10644281 0.10225722 0.097745322 0.093980365][0.063111715 0.0662135 0.071374364 0.079754546 0.0894603 0.099683955 0.10904197 0.11651344 0.12084875 0.12154363 0.11986306 0.11541306 0.11078984 0.10631974 0.10273959][0.074528426 0.078796864 0.084264942 0.092453435 0.10168982 0.11096998 0.11920174 0.12531173 0.12842086 0.1281388 0.12573695 0.12106316 0.11603425 0.11167812 0.10831831][0.084234662 0.089208677 0.094444007 0.10177093 0.10975192 0.11752451 0.12416665 0.12882823 0.13073228 0.12981413 0.12697259 0.12223788 0.11715594 0.11297373 0.10968358][0.090188734 0.095709428 0.10063062 0.10708687 0.11380585 0.1197996 0.12465847 0.12780097 0.12854506 0.12694405 0.12366147 0.1192143 0.11441953 0.11046351 0.10752811][0.092274979 0.097944282 0.10228018 0.107535 0.11281825 0.1171774 0.12048212 0.12223447 0.12224312 0.12034432 0.11720193 0.11344326 0.10946937 0.10621988 0.10349062][0.090963043 0.096275181 0.099858612 0.10408659 0.10812662 0.11136907 0.11376579 0.11491598 0.1146436 0.11304457 0.11036655 0.107226 0.10397911 0.10141382 0.098987363][0.088181093 0.0926786 0.095318131 0.098288983 0.10106612 0.10316592 0.10459221 0.10525344 0.1048485 0.10379115 0.10202051 0.10000823 0.097864658 0.096079439 0.094305128][0.084237665 0.087493278 0.088755451 0.090367176 0.092027731 0.093494132 0.094591893 0.095226943 0.095087163 0.094307557 0.093139023 0.091967881 0.091007367 0.0902539 0.08928705][0.08001215 0.081695393 0.081305258 0.081466369 0.081939735 0.082767054 0.083615556 0.0842176 0.0844324 0.084115259 0.083718948 0.0832699 0.082977608 0.083140962 0.082963713]]...]
INFO - root - 2017-12-09 13:48:25.656104: step 28110, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.876 sec/batch; 74h:02m:12s remains)
INFO - root - 2017-12-09 13:48:34.295676: step 28120, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.852 sec/batch; 72h:01m:44s remains)
INFO - root - 2017-12-09 13:48:42.955983: step 28130, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.850 sec/batch; 71h:53m:54s remains)
INFO - root - 2017-12-09 13:48:51.531885: step 28140, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.889 sec/batch; 75h:11m:48s remains)
INFO - root - 2017-12-09 13:49:00.225413: step 28150, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.899 sec/batch; 76h:02m:06s remains)
INFO - root - 2017-12-09 13:49:08.942403: step 28160, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 73h:29m:10s remains)
INFO - root - 2017-12-09 13:49:17.484089: step 28170, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.832 sec/batch; 70h:22m:06s remains)
INFO - root - 2017-12-09 13:49:26.224625: step 28180, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.887 sec/batch; 75h:01m:12s remains)
INFO - root - 2017-12-09 13:49:34.575901: step 28190, loss = 0.82, batch loss = 0.69 (10.8 examples/sec; 0.738 sec/batch; 62h:25m:00s remains)
INFO - root - 2017-12-09 13:49:43.107147: step 28200, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.866 sec/batch; 73h:14m:26s remains)
2017-12-09 13:49:43.965532: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.22192316 0.22108687 0.21874526 0.215977 0.21357086 0.21287948 0.21189849 0.21171769 0.21194676 0.21098189 0.20692033 0.19874977 0.18920934 0.17791487 0.16766815][0.2284622 0.22883686 0.22845648 0.22787137 0.22709854 0.22713515 0.22614382 0.22501887 0.22403315 0.22194365 0.21752134 0.20906022 0.19898255 0.1872077 0.17606075][0.22850722 0.23129381 0.23374921 0.2360374 0.23805547 0.2397469 0.23849055 0.2360194 0.23274913 0.228842 0.22261782 0.2132722 0.20287308 0.19085032 0.17978092][0.2276399 0.2323795 0.23720577 0.24252795 0.24707583 0.2502985 0.24955063 0.24610558 0.24131243 0.23521498 0.22736038 0.21723694 0.20677973 0.19512407 0.18406337][0.22736742 0.23407686 0.24034615 0.24735902 0.253989 0.25870198 0.25877753 0.25532103 0.24998571 0.24301723 0.23407966 0.2234503 0.21283066 0.20175624 0.19164111][0.22526516 0.23419908 0.24242826 0.25173107 0.25962964 0.26563331 0.2668035 0.26425683 0.25941581 0.25198117 0.24280208 0.23189738 0.22072421 0.21002178 0.20002824][0.22114164 0.2320275 0.24168673 0.2525835 0.26249325 0.26944664 0.27218485 0.27131552 0.26804534 0.26160273 0.25306875 0.24298301 0.23235986 0.22178183 0.21206568][0.21544299 0.22708617 0.2374351 0.24910061 0.26025942 0.26906449 0.27443242 0.27616692 0.2752372 0.27087384 0.26360589 0.25420651 0.24412054 0.2340268 0.22434132][0.20869212 0.22064729 0.23107858 0.24267553 0.25425816 0.2642056 0.27209118 0.27711868 0.27923593 0.27758425 0.2723864 0.26443553 0.25522569 0.24575192 0.23606709][0.1989421 0.21103208 0.22074389 0.23193949 0.24380346 0.25523269 0.26554763 0.27358383 0.27863058 0.27956364 0.276745 0.27068055 0.26261714 0.25390095 0.24462007][0.18657149 0.19828959 0.20776965 0.21859176 0.23009565 0.24236943 0.25468233 0.26512912 0.27256155 0.27604318 0.27600572 0.27235195 0.26612219 0.25823942 0.24927522][0.17152272 0.18199538 0.19011644 0.20007887 0.21148223 0.2239702 0.23742062 0.25009787 0.26032898 0.26675582 0.26971713 0.26901266 0.26510495 0.25865665 0.25079072][0.15645628 0.16559649 0.17240383 0.18045585 0.19002038 0.20138513 0.21456672 0.22800058 0.23982641 0.24906893 0.25533858 0.2577318 0.25650263 0.25232524 0.24611284][0.14178613 0.14961806 0.15476312 0.16097163 0.16860004 0.17842419 0.19049847 0.20357795 0.21596453 0.22662923 0.23521587 0.24032697 0.24163397 0.23969261 0.2352635][0.13009049 0.13635847 0.13958156 0.14413783 0.15012318 0.15816414 0.16857865 0.1803987 0.19240077 0.20321037 0.21270305 0.21906537 0.22169544 0.22091837 0.21754493]]...]
INFO - root - 2017-12-09 13:49:52.511328: step 28210, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.858 sec/batch; 72h:28m:49s remains)
INFO - root - 2017-12-09 13:50:01.140907: step 28220, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.857 sec/batch; 72h:26m:44s remains)
INFO - root - 2017-12-09 13:50:09.728720: step 28230, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.852 sec/batch; 71h:59m:35s remains)
INFO - root - 2017-12-09 13:50:18.049472: step 28240, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.835 sec/batch; 70h:34m:18s remains)
INFO - root - 2017-12-09 13:50:26.564944: step 28250, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.834 sec/batch; 70h:30m:57s remains)
INFO - root - 2017-12-09 13:50:35.043668: step 28260, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.883 sec/batch; 74h:39m:15s remains)
INFO - root - 2017-12-09 13:50:43.618987: step 28270, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 73h:34m:42s remains)
INFO - root - 2017-12-09 13:50:52.141044: step 28280, loss = 0.82, batch loss = 0.69 (9.7 examples/sec; 0.826 sec/batch; 69h:49m:16s remains)
INFO - root - 2017-12-09 13:51:00.548622: step 28290, loss = 0.82, batch loss = 0.69 (10.3 examples/sec; 0.775 sec/batch; 65h:30m:33s remains)
INFO - root - 2017-12-09 13:51:09.317254: step 28300, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 73h:14m:44s remains)
2017-12-09 13:51:10.221893: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.023857029 0.025128929 0.026689127 0.02874101 0.031011641 0.032887634 0.034771983 0.036818497 0.039343871 0.042222559 0.045510024 0.048850648 0.051280364 0.052962583 0.053410452][0.033773534 0.035535179 0.037618272 0.040368862 0.043493174 0.046280876 0.048750978 0.050812878 0.0526863 0.054281119 0.055870261 0.057380117 0.058114618 0.058319189 0.057619076][0.044376403 0.047054343 0.049980108 0.05366952 0.057781328 0.061508473 0.064461231 0.066339239 0.067315653 0.06747368 0.067320526 0.067006975 0.066179037 0.065189719 0.063762218][0.054120302 0.057890493 0.061780367 0.066511691 0.071559384 0.076032534 0.079208389 0.080723494 0.080830693 0.079815239 0.078577623 0.077158064 0.075541735 0.073882312 0.072048053][0.060336992 0.065307543 0.070183977 0.075819984 0.081540957 0.08646071 0.089635 0.090787157 0.090300515 0.088698506 0.086972982 0.085223958 0.083497614 0.0817786 0.079957724][0.0611126 0.067188978 0.073017061 0.079408094 0.085507244 0.090534613 0.093498409 0.094297066 0.093475312 0.091692314 0.089987159 0.088310987 0.086785182 0.085265487 0.083648965][0.056123078 0.0630451 0.069743127 0.076759472 0.083085328 0.088075086 0.090713546 0.091189221 0.090086326 0.088206947 0.086568236 0.085093379 0.083839372 0.082545057 0.081226707][0.046930023 0.054207105 0.061401915 0.068704382 0.075017557 0.079807982 0.082104072 0.08230488 0.080991134 0.079052269 0.077459186 0.076023623 0.074840635 0.073726833 0.072697923][0.035644822 0.042735655 0.049992271 0.057225876 0.0633326 0.067825064 0.069832638 0.069759578 0.06830658 0.06622003 0.064466789 0.06278795 0.061457876 0.060272258 0.059254348][0.023802398 0.0301488 0.036957759 0.043740477 0.049463857 0.053617988 0.055439342 0.055241339 0.053628184 0.051441934 0.049387816 0.047358949 0.04573198 0.044399668 0.043396879][0.013634347 0.018721782 0.024478303 0.030317966 0.03531009 0.038925853 0.040511183 0.040271986 0.038665771 0.036400679 0.034134835 0.031916939 0.03009288 0.028701825 0.027761752][0.0061749844 0.0097664595 0.014032703 0.018482428 0.022353264 0.025174705 0.026418587 0.026177399 0.024814155 0.022805076 0.02066165 0.01856338 0.016871959 0.015629491 0.014880076][0.0016890427 0.0038322215 0.0064895721 0.0093532689 0.01189964 0.013775895 0.014585882 0.014358908 0.013330874 0.011796923 0.010124167 0.0084702345 0.0071350485 0.006212689 0.0057514054][-0.00058010232 0.00042456738 0.0017388207 0.0032217037 0.0045723156 0.0055818846 0.0060026143 0.0058424943 0.0052147754 0.0042755003 0.0032433085 0.0022296337 0.001431999 0.00088721467 0.0006436652][-0.0015606045 -0.0012251713 -0.00074349309 -0.00017273345 0.00037153321 0.00079301908 0.00096849212 0.00089203217 0.00061406638 0.0002117788 -0.0002285043 -0.00064750446 -0.00096394744 -0.0011737491 -0.0012695198]]...]
INFO - root - 2017-12-09 13:51:18.754833: step 28310, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.840 sec/batch; 70h:57m:51s remains)
INFO - root - 2017-12-09 13:51:27.527249: step 28320, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.870 sec/batch; 73h:32m:57s remains)
INFO - root - 2017-12-09 13:51:36.360207: step 28330, loss = 0.82, batch loss = 0.69 (8.2 examples/sec; 0.974 sec/batch; 82h:18m:38s remains)
INFO - root - 2017-12-09 13:51:44.907634: step 28340, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.896 sec/batch; 75h:43m:35s remains)
INFO - root - 2017-12-09 13:51:53.573769: step 28350, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.865 sec/batch; 73h:02m:30s remains)
INFO - root - 2017-12-09 13:52:02.249348: step 28360, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.853 sec/batch; 72h:03m:05s remains)
INFO - root - 2017-12-09 13:52:10.919523: step 28370, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.843 sec/batch; 71h:14m:54s remains)
INFO - root - 2017-12-09 13:52:19.577063: step 28380, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.873 sec/batch; 73h:43m:42s remains)
INFO - root - 2017-12-09 13:52:28.011482: step 28390, loss = 0.82, batch loss = 0.69 (10.4 examples/sec; 0.771 sec/batch; 65h:08m:35s remains)
INFO - root - 2017-12-09 13:52:36.692055: step 28400, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.840 sec/batch; 70h:59m:54s remains)
2017-12-09 13:52:37.653386: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0018077787 -0.001806562 -0.0018062856 -0.0018059369 -0.001805398 -0.0018048623 -0.0018040391 -0.0018032369 -0.0018027262 -0.0018026199 -0.001802656 -0.0018028673 -0.0018032653 -0.0018037612 -0.0018042002][-0.001808366 -0.0018075258 -0.0018077372 -0.0018078891 -0.0018076941 -0.0018071494 -0.0018060381 -0.0018046246 -0.0018033572 -0.0018025141 -0.0018018476 -0.0018015881 -0.0018016986 -0.0018020974 -0.0018025227][-0.0018098295 -0.0018096508 -0.0018106789 -0.0018116137 -0.0018120483 -0.0018116967 -0.0018103704 -0.001808368 -0.0018063238 -0.0018045089 -0.0018029448 -0.0018019684 -0.0018015074 -0.0018015093 -0.001801661][-0.0018116371 -0.0018122066 -0.0018140768 -0.001815924 -0.0018172116 -0.0018172518 -0.0018159192 -0.0018136211 -0.0018110663 -0.0018086497 -0.0018063873 -0.0018046317 -0.0018033987 -0.0018026733 -0.0018021665][-0.0018137958 -0.0018150437 -0.0018175662 -0.0018201435 -0.0018222262 -0.0018227687 -0.0018217787 -0.0018195556 -0.0018168887 -0.0018141728 -0.0018113931 -0.0018088978 -0.001806809 -0.0018051467 -0.0018037166][-0.0018162498 -0.0018178496 -0.0018205552 -0.0018232999 -0.0018258055 -0.0018266805 -0.0018260523 -0.0018242348 -0.0018220497 -0.001819673 -0.0018169492 -0.0018140423 -0.0018112542 -0.0018086656 -0.001806206][-0.0018196194 -0.001820999 -0.0018232709 -0.0018255519 -0.0018278041 -0.001828662 -0.0018283022 -0.0018270906 -0.0018257108 -0.0018242016 -0.0018220373 -0.0018191176 -0.0018157876 -0.0018124247 -0.0018090291][-0.0018233777 -0.0018240919 -0.0018254438 -0.0018267261 -0.0018281675 -0.0018285118 -0.0018281195 -0.001827511 -0.0018270744 -0.0018267429 -0.0018256524 -0.0018232713 -0.0018198377 -0.0018158756 -0.0018116332][-0.0018272346 -0.0018272002 -0.0018274756 -0.0018275301 -0.001827842 -0.0018274271 -0.0018267235 -0.0018264788 -0.0018269451 -0.0018276809 -0.0018277593 -0.0018261875 -0.0018231168 -0.0018189898 -0.0018141704][-0.0018307291 -0.0018302515 -0.0018297583 -0.001828998 -0.0018284435 -0.0018273869 -0.0018262796 -0.0018259245 -0.0018266441 -0.0018279479 -0.0018287129 -0.0018279224 -0.0018254452 -0.0018215771 -0.0018166521][-0.0018335439 -0.0018329936 -0.0018322461 -0.0018311599 -0.0018301487 -0.0018288005 -0.0018273386 -0.0018266257 -0.0018269495 -0.001828097 -0.0018290069 -0.0018285788 -0.0018265515 -0.0018231703 -0.0018185519][-0.0018354434 -0.0018350987 -0.0018345716 -0.0018335768 -0.0018325477 -0.0018311425 -0.001829549 -0.001828561 -0.0018282757 -0.0018289016 -0.0018295356 -0.0018292225 -0.0018275806 -0.001824657 -0.001820396][-0.0018360114 -0.0018361312 -0.0018359696 -0.0018352042 -0.0018342026 -0.0018329235 -0.001831282 -0.0018300989 -0.0018295315 -0.0018297117 -0.001830165 -0.0018300163 -0.0018288726 -0.0018265216 -0.0018227019][-0.0018357751 -0.0018362297 -0.0018363167 -0.001835739 -0.0018347877 -0.0018336548 -0.0018320953 -0.001830825 -0.0018301741 -0.0018301123 -0.001830338 -0.0018304124 -0.0018299076 -0.001828279 -0.0018251773][-0.0018343627 -0.0018346874 -0.0018347127 -0.0018342108 -0.0018333683 -0.0018323984 -0.0018310593 -0.0018299127 -0.0018294151 -0.0018293778 -0.001829673 -0.001830161 -0.0018305037 -0.0018297669 -0.0018275753]]...]
INFO - root - 2017-12-09 13:52:46.346403: step 28410, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.854 sec/batch; 72h:07m:11s remains)
INFO - root - 2017-12-09 13:52:54.910314: step 28420, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.852 sec/batch; 71h:58m:50s remains)
INFO - root - 2017-12-09 13:53:03.600572: step 28430, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.852 sec/batch; 71h:58m:57s remains)
INFO - root - 2017-12-09 13:53:12.137640: step 28440, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.846 sec/batch; 71h:27m:05s remains)
INFO - root - 2017-12-09 13:53:20.765083: step 28450, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.883 sec/batch; 74h:35m:40s remains)
INFO - root - 2017-12-09 13:53:29.552083: step 28460, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.888 sec/batch; 75h:00m:28s remains)
INFO - root - 2017-12-09 13:53:38.323901: step 28470, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.889 sec/batch; 75h:04m:00s remains)
INFO - root - 2017-12-09 13:53:46.996480: step 28480, loss = 0.82, batch loss = 0.69 (8.8 examples/sec; 0.905 sec/batch; 76h:23m:17s remains)
INFO - root - 2017-12-09 13:53:55.401428: step 28490, loss = 0.82, batch loss = 0.69 (10.6 examples/sec; 0.752 sec/batch; 63h:29m:40s remains)
INFO - root - 2017-12-09 13:54:03.832677: step 28500, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.865 sec/batch; 73h:00m:28s remains)
2017-12-09 13:54:04.689640: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0016532246 -0.0015984203 -0.0015299527 -0.0014853841 -0.0014753726 -0.0012746373 -0.00094830408 -0.00040842628 0.00030099286 0.0011521097 0.0017500262 0.0018859893 0.0015916348 0.0016136678 0.002027716][-0.0014872904 -0.0013483921 -0.001180463 -0.0010727514 -0.0009909966 -0.00050742005 0.00026621751 0.0012891331 0.0024907282 0.0037681162 0.0044424115 0.004230096 0.003133778 0.0020540217 0.0012837913][-0.0011588888 -0.00088533957 -0.00059006154 -0.00036863773 -0.00017680693 0.00072949368 0.002122879 0.0037202467 0.0054204846 0.0070145489 0.007521356 0.0067231008 0.0046914676 0.0025471775 0.00092854921][-0.00070415449 -0.00024852075 0.00026803941 0.00060237467 0.00093107054 0.002300649 0.0042628245 0.0062836423 0.0082039554 0.0098467432 0.0099686291 0.0085110739 0.0057448582 0.0030475543 0.0011810776][-0.00028175476 0.0003842864 0.0011031845 0.0015452969 0.002073152 0.003663884 0.0059423768 0.0079312492 0.009734448 0.011155984 0.010843145 0.0090565467 0.0061616306 0.0035435017 0.0019005][-0.00044917758 0.00032897072 0.0011779302 0.0016902963 0.0024709594 0.0040418785 0.0061020493 0.0076248893 0.0089398045 0.0099433167 0.0095982673 0.0082207723 0.0063711018 0.0048772804 0.0041632429][-0.0014471421 -0.00097990222 -0.00054266118 9.5021096e-05 0.0011118109 0.0024150019 0.0042145806 0.0054577757 0.0067169382 0.0076064342 0.0079243919 0.0080378344 0.0083609084 0.0089453282 0.0098953834][-0.0017369194 -0.0016835032 -0.0015359488 -0.0012230885 -0.00064401166 0.00036302803 0.0015257326 0.0029070755 0.00437577 0.0061400416 0.0084359869 0.011070807 0.014626332 0.018159479 0.021477951][-0.0017605057 -0.0017009735 -0.0014537029 -0.0010066886 -0.00028157514 0.00074454036 0.0019142505 0.0035288604 0.0055948896 0.008861918 0.013417414 0.019469386 0.0269686 0.034368332 0.041227221][-0.0017746944 -0.0017476368 -0.0014048005 -0.00074517913 0.00031830941 0.0018248599 0.0035881815 0.0062691947 0.009837864 0.015161078 0.022428494 0.032188486 0.044054363 0.056116637 0.06742996][-0.0017511121 -0.0017117291 -0.0014571591 -0.00077633571 0.0006073151 0.002626833 0.0051250458 0.0091356244 0.014840836 0.022669595 0.032646947 0.045842908 0.061674792 0.078415215 0.094266824][-0.0017053967 -0.001657526 -0.0014693164 -0.00086873048 0.00019680022 0.0020216545 0.0047750515 0.0094650146 0.01629681 0.025625827 0.038166888 0.054444805 0.0735501 0.094210304 0.11348975][-0.0016461659 -0.0015905704 -0.0014796238 -0.0010149803 -0.00010163302 0.0014257451 0.0039941273 0.0082167592 0.014766719 0.024218664 0.0372032 0.054675322 0.075369015 0.098342843 0.1195616][-0.0015814343 -0.0015782528 -0.0015592022 -0.0012675505 -0.00072797865 0.0002582398 0.0023025228 0.0056632119 0.011188739 0.019547205 0.031650674 0.048209559 0.06830807 0.090387389 0.11064422][-0.0015362339 -0.0015657216 -0.0016934853 -0.0016324292 -0.0013622398 -0.00094579119 0.00017230993 0.0025084857 0.0067560696 0.013736768 0.024080273 0.038573049 0.056208286 0.074925944 0.091879927]]...]
INFO - root - 2017-12-09 13:54:13.447751: step 28510, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.874 sec/batch; 73h:46m:29s remains)
INFO - root - 2017-12-09 13:54:22.075161: step 28520, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.854 sec/batch; 72h:04m:32s remains)
INFO - root - 2017-12-09 13:54:30.669183: step 28530, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.844 sec/batch; 71h:14m:15s remains)
INFO - root - 2017-12-09 13:54:39.186684: step 28540, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.896 sec/batch; 75h:41m:10s remains)
INFO - root - 2017-12-09 13:54:47.980823: step 28550, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.851 sec/batch; 71h:50m:58s remains)
INFO - root - 2017-12-09 13:54:56.692410: step 28560, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.876 sec/batch; 73h:58m:20s remains)
INFO - root - 2017-12-09 13:55:05.314365: step 28570, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.839 sec/batch; 70h:51m:39s remains)
INFO - root - 2017-12-09 13:55:14.057169: step 28580, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.865 sec/batch; 72h:59m:24s remains)
INFO - root - 2017-12-09 13:55:22.490974: step 28590, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.865 sec/batch; 73h:00m:54s remains)
INFO - root - 2017-12-09 13:55:30.924670: step 28600, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.852 sec/batch; 71h:57m:17s remains)
2017-12-09 13:55:31.769714: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0017883282 -0.0017914893 -0.001795211 -0.0017985088 -0.0018008683 -0.0018019322 -0.0018020578 -0.0018016314 -0.0018001457 -0.0017977864 -0.0017953067 -0.0017925745 -0.0017887619 -0.0017844086 -0.0017812048][-0.0017839087 -0.001786461 -0.0017903789 -0.0017940961 -0.0017969678 -0.0017988482 -0.001799865 -0.001799832 -0.0017981799 -0.0017955827 -0.0017929529 -0.001789929 -0.0017858925 -0.0017817452 -0.0017788268][-0.0017800988 -0.0017823123 -0.0017865007 -0.0017909068 -0.0017943797 -0.0017971413 -0.0017989929 -0.0017995018 -0.0017978952 -0.001795041 -0.0017919005 -0.0017880447 -0.0017835322 -0.0017796312 -0.0017771634][-0.0017771109 -0.001778618 -0.0017826897 -0.0017874945 -0.0017916748 -0.0017952052 -0.0017978807 -0.0017989455 -0.0017976305 -0.0017946077 -0.0017909039 -0.0017864016 -0.0017817514 -0.00177828 -0.0017762388][-0.0017750626 -0.0017757509 -0.0017793463 -0.0017840625 -0.0017885715 -0.0017926914 -0.0017958772 -0.0017975717 -0.001796948 -0.0017943847 -0.0017906339 -0.00178594 -0.0017814795 -0.0017783891 -0.0017766244][-0.0017741346 -0.0017740978 -0.0017770075 -0.0017812552 -0.0017855888 -0.0017897483 -0.0017929429 -0.0017951768 -0.0017957416 -0.0017944328 -0.0017914765 -0.0017871575 -0.0017830794 -0.001780252 -0.0017784433][-0.0017738149 -0.0017734775 -0.0017758502 -0.0017795784 -0.0017837624 -0.0017879084 -0.0017909938 -0.0017935291 -0.0017953335 -0.0017957424 -0.0017942076 -0.0017908757 -0.001787322 -0.0017844529 -0.0017821521][-0.0017743553 -0.0017736921 -0.0017758107 -0.0017792665 -0.0017834518 -0.0017877697 -0.0017910359 -0.0017939159 -0.0017966998 -0.0017986275 -0.0017985469 -0.0017964053 -0.0017935664 -0.0017905785 -0.0017875175][-0.0017751079 -0.0017740427 -0.0017758901 -0.0017792088 -0.0017834394 -0.0017880669 -0.001791885 -0.0017954234 -0.0017991158 -0.0018022988 -0.0018035453 -0.0018025218 -0.0018002373 -0.0017970367 -0.0017931482][-0.0017752572 -0.0017740668 -0.0017756388 -0.0017786487 -0.0017827135 -0.0017873659 -0.0017915121 -0.0017954969 -0.0017999463 -0.0018042313 -0.0018066043 -0.0018065819 -0.0018048238 -0.0018015624 -0.001797036][-0.0017752828 -0.0017739367 -0.0017751115 -0.0017778298 -0.0017816337 -0.0017859238 -0.0017899234 -0.0017939571 -0.0017986891 -0.0018034339 -0.001806483 -0.0018072812 -0.0018061885 -0.0018032368 -0.0017986633][-0.0017750646 -0.0017736673 -0.0017745399 -0.0017769587 -0.0017803982 -0.0017841953 -0.001787812 -0.0017914275 -0.0017957253 -0.0018001705 -0.001803278 -0.0018045807 -0.0018041444 -0.001801768 -0.0017977133][-0.0017745958 -0.0017733654 -0.0017740313 -0.0017761752 -0.0017791718 -0.0017823618 -0.0017854782 -0.0017884545 -0.0017918283 -0.0017953385 -0.0017978633 -0.0017990859 -0.0017988479 -0.001797013 -0.0017936912][-0.0017739926 -0.001773008 -0.0017735615 -0.0017753319 -0.0017778036 -0.0017802917 -0.001782704 -0.0017848182 -0.0017870192 -0.0017893565 -0.0017909784 -0.0017916863 -0.0017914828 -0.0017901364 -0.001787745][-0.0017733123 -0.0017725159 -0.0017730222 -0.0017743816 -0.0017762089 -0.0017779488 -0.0017795275 -0.0017806981 -0.0017817433 -0.0017829103 -0.0017837152 -0.00178402 -0.0017838832 -0.0017832412 -0.0017818984]]...]
INFO - root - 2017-12-09 13:55:40.380105: step 28610, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.881 sec/batch; 74h:24m:26s remains)
INFO - root - 2017-12-09 13:55:49.129690: step 28620, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.884 sec/batch; 74h:35m:11s remains)
INFO - root - 2017-12-09 13:55:57.828226: step 28630, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.898 sec/batch; 75h:45m:39s remains)
INFO - root - 2017-12-09 13:56:06.444018: step 28640, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.872 sec/batch; 73h:35m:53s remains)
INFO - root - 2017-12-09 13:56:15.197901: step 28650, loss = 0.82, batch loss = 0.69 (8.8 examples/sec; 0.914 sec/batch; 77h:08m:46s remains)
INFO - root - 2017-12-09 13:56:23.900910: step 28660, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.839 sec/batch; 70h:47m:24s remains)
INFO - root - 2017-12-09 13:56:32.823013: step 28670, loss = 0.82, batch loss = 0.69 (8.0 examples/sec; 0.995 sec/batch; 83h:56m:37s remains)
INFO - root - 2017-12-09 13:56:41.431829: step 28680, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.873 sec/batch; 73h:38m:04s remains)
INFO - root - 2017-12-09 13:56:49.835299: step 28690, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.858 sec/batch; 72h:24m:15s remains)
INFO - root - 2017-12-09 13:56:58.356604: step 28700, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.835 sec/batch; 70h:28m:51s remains)
2017-12-09 13:56:59.263007: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.34179088 0.33571577 0.32682139 0.316055 0.3035495 0.29011059 0.2766321 0.2653234 0.25641212 0.24880864 0.24329287 0.23885426 0.23431344 0.22998551 0.22637017][0.34134468 0.33704108 0.32983857 0.32127368 0.3109732 0.29919106 0.28657654 0.27564019 0.26675445 0.25866151 0.25203231 0.24601963 0.24021678 0.23487531 0.23041785][0.33577079 0.33371371 0.32865441 0.32254109 0.31519705 0.30522695 0.29374263 0.28303757 0.27394733 0.26526862 0.2572284 0.24982034 0.24294662 0.23684905 0.23161367][0.32894763 0.329349 0.3266747 0.32306442 0.31824726 0.3105143 0.30076748 0.29101035 0.28199747 0.27310377 0.26406124 0.25528651 0.24695049 0.23971802 0.23361026][0.31882632 0.32121906 0.32047758 0.3198849 0.31839988 0.31316867 0.30600184 0.29814485 0.29013276 0.28108555 0.27111441 0.26137385 0.25193015 0.24326155 0.23600632][0.31030005 0.3130289 0.3126798 0.31380594 0.31432498 0.312445 0.30854851 0.30325353 0.29719281 0.288749 0.27831852 0.26706564 0.25604483 0.24597019 0.23729968][0.30464998 0.30891442 0.30936939 0.31120139 0.3124693 0.31223318 0.3102546 0.30671161 0.30167118 0.29386249 0.2836771 0.27167222 0.2597287 0.24852715 0.23886953][0.29824355 0.30490962 0.30659267 0.308628 0.31010443 0.31029248 0.30890357 0.30575588 0.3008371 0.29351369 0.28360853 0.27186218 0.25980914 0.2480043 0.23780261][0.29348958 0.3015976 0.30360568 0.30570343 0.30707952 0.30747432 0.30639464 0.30360478 0.29882148 0.2917099 0.28197175 0.27021939 0.25776765 0.24557625 0.2350429][0.29042366 0.29960242 0.30160668 0.3035931 0.30483255 0.30491987 0.303498 0.29985985 0.29418269 0.28634253 0.27582118 0.26356009 0.25106454 0.23945718 0.22967859][0.28595674 0.29643229 0.29889289 0.30109119 0.30241403 0.30275705 0.30081666 0.29619208 0.28902909 0.27974281 0.2681652 0.25519952 0.24256361 0.23190176 0.22333555][0.28122407 0.29254952 0.29503044 0.29686978 0.29784375 0.29779795 0.29519957 0.29003027 0.28187862 0.27162293 0.25925452 0.24637757 0.2344517 0.22481164 0.21759643][0.27527553 0.28640795 0.2885344 0.29033554 0.2915163 0.29136044 0.28874442 0.28360438 0.27501857 0.26407611 0.25118491 0.23822197 0.22665149 0.21786399 0.2117981][0.26840007 0.27887934 0.28045046 0.28249627 0.2840887 0.28464285 0.28277028 0.27839217 0.27042082 0.2594738 0.2464747 0.23350476 0.22209913 0.21397988 0.20878048][0.25948104 0.27004591 0.27178705 0.27422583 0.2761178 0.27722818 0.27608231 0.27257878 0.26551163 0.25512144 0.24270165 0.23004456 0.21922229 0.21164748 0.2070533]]...]
INFO - root - 2017-12-09 13:57:07.912743: step 28710, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.840 sec/batch; 70h:52m:34s remains)
INFO - root - 2017-12-09 13:57:16.477907: step 28720, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.856 sec/batch; 72h:15m:29s remains)
INFO - root - 2017-12-09 13:57:25.042835: step 28730, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.848 sec/batch; 71h:35m:00s remains)
INFO - root - 2017-12-09 13:57:33.537525: step 28740, loss = 0.82, batch loss = 0.69 (10.5 examples/sec; 0.763 sec/batch; 64h:21m:18s remains)
INFO - root - 2017-12-09 13:57:42.148798: step 28750, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.891 sec/batch; 75h:11m:55s remains)
INFO - root - 2017-12-09 13:57:50.873738: step 28760, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.882 sec/batch; 74h:25m:33s remains)
INFO - root - 2017-12-09 13:57:59.662777: step 28770, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.888 sec/batch; 74h:54m:31s remains)
INFO - root - 2017-12-09 13:58:08.319235: step 28780, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 72h:56m:04s remains)
INFO - root - 2017-12-09 13:58:16.891333: step 28790, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.881 sec/batch; 74h:21m:44s remains)
INFO - root - 2017-12-09 13:58:25.644691: step 28800, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 73h:30m:08s remains)
2017-12-09 13:58:26.514912: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0018259445 -0.0018256638 -0.001825144 -0.0018241439 -0.0018221674 -0.0018194413 -0.0018161209 -0.0018126158 -0.0018093402 -0.0018062665 -0.0018038679 -0.0018023142 -0.0018015149 -0.0018010206 -0.0018005625][-0.0018241495 -0.0018245163 -0.0018244945 -0.0018235644 -0.0018211472 -0.0018176611 -0.0018136918 -0.0018097546 -0.001806407 -0.0018035782 -0.0018016128 -0.0018004584 -0.0017999379 -0.0017996232 -0.0017992936][-0.0018226755 -0.0018235005 -0.0018239457 -0.0018231932 -0.0018206418 -0.0018168893 -0.00181259 -0.0018083546 -0.0018049125 -0.0018022552 -0.0018006425 -0.0017996931 -0.0017992642 -0.0017990695 -0.0017988683][-0.0018216271 -0.0018225844 -0.0018233238 -0.0018229473 -0.0018207728 -0.0018170734 -0.0018127244 -0.0018082981 -0.001804671 -0.0018020147 -0.0018005172 -0.0017996072 -0.0017991841 -0.0017990312 -0.0017988926][-0.0018205419 -0.0018215392 -0.0018224984 -0.0018225394 -0.0018209325 -0.0018176405 -0.0018135792 -0.001809424 -0.0018058185 -0.0018031572 -0.0018016449 -0.0018006433 -0.0018000505 -0.0017996959 -0.00179938][-0.0018188533 -0.0018196044 -0.0018203394 -0.001820537 -0.0018193333 -0.0018168133 -0.0018135646 -0.0018100938 -0.0018068753 -0.0018044304 -0.0018029606 -0.0018018688 -0.0018010716 -0.0018004611 -0.0017999298][-0.0018166339 -0.0018169545 -0.0018173414 -0.0018174481 -0.0018165188 -0.0018145507 -0.0018119313 -0.0018091007 -0.0018063809 -0.0018043291 -0.0018031338 -0.0018022448 -0.0018015559 -0.0018009122 -0.0018003038][-0.0018140097 -0.0018137548 -0.001813799 -0.0018137662 -0.0018130951 -0.0018116572 -0.0018097411 -0.0018076031 -0.0018055323 -0.0018039523 -0.0018030899 -0.0018023712 -0.0018017443 -0.0018010623 -0.0018003966][-0.0018104105 -0.0018098343 -0.0018098241 -0.0018099169 -0.0018097024 -0.0018090215 -0.0018079677 -0.0018066344 -0.0018051381 -0.0018038738 -0.0018031348 -0.0018025095 -0.0018018722 -0.0018011037 -0.0018004123][-0.0018056702 -0.0018050018 -0.0018051119 -0.0018054442 -0.0018057046 -0.0018057596 -0.0018054751 -0.0018048962 -0.0018040199 -0.0018031104 -0.0018025958 -0.0018021479 -0.0018016452 -0.0018009455 -0.0018002782][-0.0018017233 -0.0018007819 -0.001800919 -0.0018013511 -0.0018017975 -0.0018021077 -0.0018021448 -0.0018019818 -0.0018016316 -0.0018012406 -0.0018011561 -0.0018010613 -0.0018008888 -0.0018004354 -0.001799938][-0.0017991506 -0.0017977474 -0.0017976235 -0.0017979136 -0.001798284 -0.0017985131 -0.0017986187 -0.0017987123 -0.0017987757 -0.0017988355 -0.0017992059 -0.0017995681 -0.0017998205 -0.0017997789 -0.0017995387][-0.0017967707 -0.0017950437 -0.0017946494 -0.001794721 -0.0017950216 -0.001795325 -0.0017956648 -0.0017960608 -0.001796485 -0.0017968978 -0.0017975819 -0.0017983075 -0.0017989152 -0.0017992017 -0.0017991919][-0.001794957 -0.0017930204 -0.0017924601 -0.0017924308 -0.0017927334 -0.0017931707 -0.0017937577 -0.0017943999 -0.001795131 -0.0017958882 -0.0017968187 -0.0017977041 -0.0017984851 -0.0017989686 -0.0017990357][-0.0017940731 -0.0017921121 -0.0017914928 -0.0017914735 -0.0017917664 -0.0017921953 -0.0017928708 -0.0017936027 -0.0017944551 -0.0017954063 -0.0017965105 -0.0017974824 -0.0017982902 -0.0017987879 -0.0017988755]]...]
INFO - root - 2017-12-09 13:58:35.388719: step 28810, loss = 0.82, batch loss = 0.69 (8.6 examples/sec; 0.928 sec/batch; 78h:14m:47s remains)
INFO - root - 2017-12-09 13:58:44.099846: step 28820, loss = 0.82, batch loss = 0.69 (8.8 examples/sec; 0.911 sec/batch; 76h:53m:10s remains)
INFO - root - 2017-12-09 13:58:52.852678: step 28830, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.887 sec/batch; 74h:50m:07s remains)
INFO - root - 2017-12-09 13:59:01.483375: step 28840, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.872 sec/batch; 73h:35m:27s remains)
INFO - root - 2017-12-09 13:59:10.042487: step 28850, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.875 sec/batch; 73h:50m:22s remains)
INFO - root - 2017-12-09 13:59:18.632816: step 28860, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.830 sec/batch; 69h:59m:57s remains)
INFO - root - 2017-12-09 13:59:27.442464: step 28870, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.882 sec/batch; 74h:24m:31s remains)
INFO - root - 2017-12-09 13:59:36.119925: step 28880, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.850 sec/batch; 71h:42m:18s remains)
INFO - root - 2017-12-09 13:59:44.589304: step 28890, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.849 sec/batch; 71h:35m:41s remains)
INFO - root - 2017-12-09 13:59:52.919691: step 28900, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 72h:41m:52s remains)
2017-12-09 13:59:53.761025: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.069532104 0.06733273 0.064894266 0.062175665 0.059546757 0.057156261 0.055060629 0.053389139 0.05185429 0.050438259 0.048880879 0.047184888 0.045236453 0.043065149 0.040815312][0.0744684 0.072249077 0.069947168 0.067492284 0.065168887 0.063032515 0.061023552 0.059246417 0.057371397 0.055342983 0.053044178 0.050537072 0.047782261 0.044957519 0.042179693][0.07885205 0.077072553 0.0753697 0.07368993 0.072166413 0.070699744 0.06910146 0.067292526 0.064972386 0.062061768 0.058617551 0.054857869 0.050905291 0.047076643 0.04354462][0.081968933 0.081223011 0.080694035 0.080321632 0.08004564 0.079587705 0.0785925 0.076779358 0.073903829 0.069893509 0.065044053 0.059685707 0.05422283 0.049171437 0.044762615][0.0830692 0.083653569 0.08456444 0.085796334 0.087068126 0.08780501 0.087531544 0.085756883 0.08229848 0.077193663 0.070941634 0.064059891 0.057098065 0.050859936 0.045630757][0.081880137 0.083715916 0.085896231 0.088587292 0.091271669 0.093129538 0.093571842 0.091951437 0.088165745 0.0822694 0.074973457 0.066900738 0.058815625 0.05165752 0.045794621][0.078759789 0.081458285 0.084383138 0.087956548 0.091500953 0.094050996 0.094962627 0.093520895 0.089646123 0.083437525 0.07568787 0.067115486 0.058587704 0.051075067 0.045057636][0.074539952 0.077475682 0.080374375 0.083977848 0.087587006 0.090193205 0.091124594 0.08973144 0.085973926 0.07997603 0.072497621 0.064228259 0.056092519 0.048908245 0.043261852][0.06980934 0.072273664 0.074313372 0.077076644 0.079943091 0.081967428 0.082523622 0.081071444 0.077615879 0.072276473 0.065725923 0.058547474 0.051596269 0.045423567 0.04061003][0.064982519 0.066394873 0.0669253 0.068142883 0.069576494 0.070484556 0.070363387 0.068801209 0.065820821 0.061511781 0.05639565 0.0508619 0.045586068 0.040906727 0.037257981][0.060093906 0.060293265 0.059135698 0.058511045 0.058190137 0.057724629 0.056738432 0.055006947 0.052577797 0.0494824 0.046039272 0.042377684 0.038980637 0.035949044 0.033522297][0.055460554 0.054490332 0.051790386 0.049459226 0.047461253 0.045600008 0.043687887 0.041698 0.039687086 0.037640519 0.035710111 0.033826571 0.032202955 0.030759916 0.029481865][0.0513154 0.049398273 0.045551229 0.041911528 0.038555004 0.035492446 0.032697942 0.030305211 0.02843502 0.027073517 0.026253369 0.025741562 0.025519414 0.02536815 0.025027713][0.047095615 0.044732753 0.040326208 0.035946108 0.03171457 0.027808115 0.024309948 0.021472815 0.019482326 0.018371474 0.018124487 0.018440751 0.01912646 0.019851839 0.020191155][0.042535387 0.040252548 0.035910733 0.031428747 0.026922431 0.022633744 0.018695546 0.015444216 0.013137565 0.011889861 0.011689619 0.012262769 0.013327532 0.014470921 0.015195333]]...]
INFO - root - 2017-12-09 14:00:02.388914: step 28910, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.880 sec/batch; 74h:11m:19s remains)
INFO - root - 2017-12-09 14:00:10.976012: step 28920, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.865 sec/batch; 72h:58m:09s remains)
INFO - root - 2017-12-09 14:00:19.567814: step 28930, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.898 sec/batch; 75h:45m:07s remains)
INFO - root - 2017-12-09 14:00:28.181240: step 28940, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 72h:48m:55s remains)
INFO - root - 2017-12-09 14:00:36.608344: step 28950, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.857 sec/batch; 72h:13m:36s remains)
INFO - root - 2017-12-09 14:00:45.397121: step 28960, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.875 sec/batch; 73h:44m:48s remains)
INFO - root - 2017-12-09 14:00:54.015260: step 28970, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.879 sec/batch; 74h:09m:01s remains)
INFO - root - 2017-12-09 14:01:02.681744: step 28980, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.885 sec/batch; 74h:37m:52s remains)
INFO - root - 2017-12-09 14:01:11.099547: step 28990, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.877 sec/batch; 73h:57m:40s remains)
INFO - root - 2017-12-09 14:01:19.409824: step 29000, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.884 sec/batch; 74h:31m:59s remains)
2017-12-09 14:01:20.239584: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0018346228 -0.001834488 -0.0018342194 -0.0018334165 -0.0018323781 -0.00183149 -0.0018310067 -0.0018308909 -0.0018309703 -0.0018310813 -0.0018310432 -0.001830784 -0.0018304194 -0.0018299043 -0.0018292612][-0.0018342106 -0.0018341366 -0.0018340143 -0.0018333955 -0.0018326375 -0.0018320415 -0.0018317482 -0.001831732 -0.0018318663 -0.0018319237 -0.0018317684 -0.0018313419 -0.0018308591 -0.0018302149 -0.0018293721][-0.0018337545 -0.0018339468 -0.0018340935 -0.0018338048 -0.0018334594 -0.0018332328 -0.0018331928 -0.0018333141 -0.0018335291 -0.0018335617 -0.0018332753 -0.0018326864 -0.0018320387 -0.0018311671 -0.0018300149][-0.0018338982 -0.0018343051 -0.0018346156 -0.0018346346 -0.0018346033 -0.0018345739 -0.0018346725 -0.0018349957 -0.0018353865 -0.0018354565 -0.0018350601 -0.0018342495 -0.0018333861 -0.0018322101 -0.0018307284][-0.0018341071 -0.00183464 -0.0018351706 -0.0018353633 -0.0018354357 -0.0018354134 -0.0018356022 -0.0018360892 -0.0018366063 -0.0018367725 -0.0018364305 -0.0018355955 -0.0018345775 -0.0018331573 -0.0018314404][-0.0018337306 -0.0018341851 -0.0018347721 -0.0018349955 -0.0018350201 -0.0018350026 -0.001835324 -0.0018360299 -0.0018367611 -0.0018371214 -0.0018370085 -0.0018364135 -0.0018353438 -0.0018337334 -0.0018318309][-0.0018329468 -0.0018331604 -0.0018336496 -0.0018338532 -0.0018338594 -0.0018338148 -0.0018340817 -0.0018348496 -0.0018356695 -0.0018362324 -0.0018364901 -0.0018362693 -0.0018353492 -0.0018337781 -0.0018318442][-0.0018320703 -0.0018320204 -0.0018322512 -0.0018324177 -0.0018325117 -0.0018326479 -0.0018329635 -0.0018335796 -0.0018342767 -0.001834909 -0.0018353683 -0.0018352839 -0.0018345243 -0.0018331754 -0.0018314436][-0.001831257 -0.0018309543 -0.0018310116 -0.0018311272 -0.001831321 -0.0018316702 -0.0018321796 -0.0018329138 -0.0018336066 -0.0018340936 -0.0018342779 -0.0018339939 -0.0018332532 -0.0018320474 -0.0018305368][-0.0018306888 -0.0018301902 -0.0018301683 -0.0018302086 -0.0018303946 -0.0018308298 -0.0018315836 -0.0018325277 -0.0018332452 -0.0018336261 -0.0018335395 -0.0018330088 -0.0018320887 -0.0018308947 -0.0018294826][-0.0018304444 -0.001829817 -0.001829742 -0.0018297049 -0.0018297916 -0.0018301789 -0.0018309922 -0.0018319612 -0.0018326299 -0.0018329324 -0.0018327889 -0.0018321382 -0.0018310483 -0.0018297895 -0.0018285314][-0.0018304889 -0.0018298907 -0.0018298306 -0.0018297648 -0.0018298199 -0.0018301839 -0.0018308132 -0.0018314273 -0.0018318146 -0.0018319815 -0.0018318238 -0.0018311619 -0.001830128 -0.0018290001 -0.0018279683][-0.0018307229 -0.0018303801 -0.0018304765 -0.0018305106 -0.0018306266 -0.0018309234 -0.0018312226 -0.0018314075 -0.0018314191 -0.0018313259 -0.001831077 -0.001830423 -0.0018295075 -0.0018286144 -0.0018278285][-0.0018312813 -0.0018314103 -0.0018318498 -0.0018320549 -0.0018321394 -0.0018321725 -0.0018320385 -0.0018317064 -0.0018312794 -0.0018309335 -0.0018305981 -0.0018300054 -0.001829213 -0.0018284628 -0.0018278594][-0.0018319906 -0.0018325564 -0.0018333592 -0.0018337633 -0.0018338114 -0.001833608 -0.0018330368 -0.0018321895 -0.0018313054 -0.0018306328 -0.0018301202 -0.0018295434 -0.0018288632 -0.0018281989 -0.0018277467]]...]
INFO - root - 2017-12-09 14:01:28.909445: step 29010, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.863 sec/batch; 72h:42m:41s remains)
INFO - root - 2017-12-09 14:01:37.607130: step 29020, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.894 sec/batch; 75h:20m:47s remains)
INFO - root - 2017-12-09 14:01:46.327291: step 29030, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.885 sec/batch; 74h:35m:12s remains)
INFO - root - 2017-12-09 14:01:54.994523: step 29040, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.849 sec/batch; 71h:35m:02s remains)
INFO - root - 2017-12-09 14:02:03.441510: step 29050, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 73h:27m:13s remains)
INFO - root - 2017-12-09 14:02:12.109472: step 29060, loss = 0.83, batch loss = 0.70 (9.8 examples/sec; 0.818 sec/batch; 68h:58m:37s remains)
INFO - root - 2017-12-09 14:02:20.765426: step 29070, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.848 sec/batch; 71h:28m:38s remains)
INFO - root - 2017-12-09 14:02:29.484368: step 29080, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 73h:05m:24s remains)
INFO - root - 2017-12-09 14:02:37.902403: step 29090, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.859 sec/batch; 72h:25m:06s remains)
INFO - root - 2017-12-09 14:02:46.244534: step 29100, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.851 sec/batch; 71h:45m:02s remains)
2017-12-09 14:02:47.140809: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.34657609 0.34560329 0.3424997 0.33766913 0.33214298 0.32303804 0.3124941 0.3003774 0.287519 0.27396274 0.25986943 0.24853589 0.23883055 0.23076169 0.22538069][0.35710812 0.36025476 0.36028972 0.35793245 0.35423878 0.34693155 0.33784822 0.32630497 0.31362084 0.29935274 0.28357247 0.26934165 0.25643957 0.24582875 0.23812668][0.36312354 0.37096402 0.37528169 0.37670359 0.37572151 0.3706041 0.36272183 0.35246488 0.34001604 0.32518002 0.30799684 0.29092905 0.27470997 0.26054567 0.24994002][0.36869606 0.38072807 0.38922834 0.3950958 0.39786053 0.39602238 0.39050561 0.38137168 0.36893213 0.35293445 0.33405843 0.31466821 0.29535806 0.27810073 0.26471797][0.37247336 0.38838252 0.40045512 0.4108313 0.41810441 0.42023465 0.41768974 0.41049564 0.39815992 0.38099772 0.36040995 0.33902669 0.31777841 0.29794279 0.28202364][0.37347525 0.3929773 0.40794718 0.42182702 0.43279135 0.43995795 0.44135004 0.43586102 0.42414907 0.4070366 0.38572556 0.36191469 0.33823991 0.31646958 0.29848987][0.3717446 0.39348111 0.41035134 0.42730334 0.4415516 0.45257226 0.45775777 0.45567751 0.44631425 0.42994571 0.40888068 0.38403231 0.35879377 0.33473092 0.31423351][0.36919069 0.39210138 0.41015208 0.42870384 0.44516984 0.458759 0.46659622 0.46737638 0.46018448 0.44481754 0.42419145 0.39913827 0.3726677 0.34633511 0.32326922][0.36728171 0.39160839 0.40980729 0.42862087 0.44600248 0.46093079 0.47116891 0.47449157 0.4698481 0.45580503 0.43579862 0.41105142 0.38407725 0.35648245 0.33104959][0.36579642 0.39032498 0.40795037 0.42611393 0.44344765 0.45887712 0.47002006 0.47486213 0.47221643 0.46022439 0.44156912 0.41753098 0.3909564 0.36285439 0.33562475][0.3660371 0.39020911 0.40686083 0.42352712 0.43944103 0.4540374 0.465253 0.4705714 0.46907467 0.45837069 0.44086802 0.41719055 0.39027682 0.36174202 0.33324549][0.36285478 0.38690528 0.40221718 0.41758335 0.43174082 0.44486606 0.455306 0.46108785 0.46066955 0.45048898 0.433695 0.41075423 0.38395846 0.35500735 0.32567146][0.36233616 0.38559884 0.39907217 0.41163322 0.42245144 0.43139517 0.43805051 0.44136769 0.43985549 0.43022615 0.41502792 0.39428672 0.36954263 0.34212628 0.31368729][0.36273465 0.38547751 0.3970969 0.40644839 0.41322476 0.41776046 0.42015448 0.41984731 0.41625157 0.40703055 0.39348876 0.37532377 0.35359895 0.32902652 0.30282187][0.36486423 0.38662651 0.39608708 0.40247926 0.40531477 0.40577748 0.40410078 0.40051988 0.39488143 0.38559139 0.37347054 0.357571 0.33839861 0.31618875 0.29206493]]...]
INFO - root - 2017-12-09 14:02:55.726243: step 29110, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.861 sec/batch; 72h:31m:51s remains)
INFO - root - 2017-12-09 14:03:04.351482: step 29120, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.881 sec/batch; 74h:15m:20s remains)
INFO - root - 2017-12-09 14:03:12.923398: step 29130, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.858 sec/batch; 72h:20m:31s remains)
INFO - root - 2017-12-09 14:03:21.553331: step 29140, loss = 0.83, batch loss = 0.70 (9.3 examples/sec; 0.856 sec/batch; 72h:09m:57s remains)
INFO - root - 2017-12-09 14:03:29.998181: step 29150, loss = 0.83, batch loss = 0.70 (9.2 examples/sec; 0.866 sec/batch; 72h:59m:55s remains)
INFO - root - 2017-12-09 14:03:38.641495: step 29160, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.858 sec/batch; 72h:16m:13s remains)
INFO - root - 2017-12-09 14:03:47.085868: step 29170, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 73h:09m:42s remains)
INFO - root - 2017-12-09 14:03:55.721690: step 29180, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.872 sec/batch; 73h:28m:06s remains)
INFO - root - 2017-12-09 14:04:04.183345: step 29190, loss = 0.81, batch loss = 0.68 (9.1 examples/sec; 0.883 sec/batch; 74h:25m:11s remains)
INFO - root - 2017-12-09 14:04:12.622719: step 29200, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.840 sec/batch; 70h:43m:43s remains)
2017-12-09 14:04:13.508876: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.048602093 0.057637796 0.06427367 0.066767931 0.063529044 0.057076771 0.049127109 0.041513506 0.035757605 0.0323665 0.030855455 0.029303635 0.026447179 0.022196487 0.01659916][0.0746755 0.090113655 0.10241319 0.1084763 0.10527916 0.096824452 0.085628361 0.075159043 0.067007035 0.062036332 0.059598751 0.056792192 0.051689032 0.043244939 0.032900751][0.1044739 0.12873538 0.14914989 0.1607611 0.1593897 0.15013872 0.13611335 0.12284455 0.11196649 0.10488322 0.10141814 0.097479343 0.090096548 0.0771168 0.060607631][0.13568333 0.17166269 0.204302 0.22573599 0.22982736 0.22303097 0.20913677 0.19466841 0.18049391 0.16911332 0.1617658 0.15389858 0.14179039 0.12192628 0.097146079][0.15958571 0.20786519 0.2537874 0.28719768 0.30038685 0.29993671 0.28978011 0.27721259 0.26169237 0.24649994 0.23467331 0.22214212 0.20434096 0.17651805 0.14204378][0.17122811 0.22908732 0.28667536 0.33205315 0.35583723 0.36430216 0.36128363 0.35307136 0.33802965 0.31985185 0.30299786 0.2850844 0.26130962 0.22595675 0.18268402][0.17007793 0.23249826 0.29655573 0.35050777 0.38388744 0.40149608 0.40603462 0.40330696 0.3904843 0.37115112 0.35068443 0.32818678 0.29935634 0.25820786 0.20861088][0.15729116 0.21921796 0.28439671 0.342001 0.38172883 0.40635771 0.417392 0.41907111 0.40860042 0.38946298 0.36663955 0.34078667 0.30856 0.26436436 0.21227339][0.13327184 0.18939568 0.25046667 0.30706352 0.34906971 0.37709773 0.39203119 0.39730892 0.38976711 0.37242928 0.34942451 0.32235211 0.28887421 0.24455917 0.19384797][0.10125734 0.14745745 0.1992427 0.2493431 0.28900862 0.3168062 0.33262384 0.3394523 0.33468989 0.32043514 0.29970881 0.27423567 0.24275519 0.20235069 0.15745549][0.0688038 0.10191166 0.14072131 0.18015067 0.21319665 0.23728853 0.25146779 0.25830108 0.25564224 0.24473971 0.22792366 0.20664132 0.18046269 0.14757553 0.11208342][0.040720426 0.061274443 0.086608231 0.11375414 0.13775592 0.1557039 0.16637334 0.17150424 0.1695511 0.16110751 0.14832307 0.13240032 0.11346492 0.09055154 0.066664383][0.020051232 0.031069696 0.045530874 0.061861757 0.0769863 0.088563278 0.09539786 0.098262563 0.09633626 0.089789525 0.0804867 0.069472149 0.057415619 0.043915447 0.030629247][0.0067042904 0.011671737 0.018584458 0.026740853 0.034515295 0.040567171 0.044256505 0.045688093 0.044201095 0.039897796 0.034136571 0.027755231 0.021329148 0.014832228 0.0090278611][0.0002905929 0.0020566413 0.0047108289 0.0079189865 0.011021121 0.013493049 0.015069718 0.015702605 0.015016818 0.013034639 0.010456909 0.0077358279 0.0051833144 0.0027781976 0.00082692504]]...]
INFO - root - 2017-12-09 14:04:22.081495: step 29210, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 72h:49m:24s remains)
INFO - root - 2017-12-09 14:04:30.567464: step 29220, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.838 sec/batch; 70h:33m:19s remains)
INFO - root - 2017-12-09 14:04:39.223705: step 29230, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.886 sec/batch; 74h:37m:26s remains)
INFO - root - 2017-12-09 14:04:47.851633: step 29240, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 72h:46m:56s remains)
INFO - root - 2017-12-09 14:04:56.471219: step 29250, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.843 sec/batch; 71h:03m:04s remains)
INFO - root - 2017-12-09 14:05:05.040510: step 29260, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 72h:36m:15s remains)
INFO - root - 2017-12-09 14:05:13.806977: step 29270, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 73h:11m:35s remains)
INFO - root - 2017-12-09 14:05:22.470175: step 29280, loss = 0.82, batch loss = 0.69 (8.8 examples/sec; 0.907 sec/batch; 76h:21m:22s remains)
INFO - root - 2017-12-09 14:05:30.967230: step 29290, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.853 sec/batch; 71h:51m:10s remains)
INFO - root - 2017-12-09 14:05:39.361377: step 29300, loss = 0.82, batch loss = 0.69 (9.8 examples/sec; 0.818 sec/batch; 68h:52m:19s remains)
2017-12-09 14:05:40.227230: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.0011067517 0.0027659894 0.00451987 0.0059897061 0.0068026278 0.0068645962 0.0062908111 0.0052655186 0.0040040081 0.00278247 0.0017083063 0.00076491 -6.1605708e-05 -0.00073375029 -0.0011933419][0.0030892738 0.0058594039 0.008839881 0.011391534 0.012943766 0.013251583 0.012420141 0.010731769 0.0085712159 0.0064250464 0.0045085181 0.0028596809 0.0014549146 0.00031330634 -0.00052695081][0.0060582245 0.010336782 0.015005177 0.019123541 0.021869972 0.022857994 0.022079313 0.01985273 0.016687959 0.01327257 0.010017369 0.0070601767 0.0044755931 0.0023358976 0.00071614014][0.010763605 0.017162304 0.024087474 0.030194219 0.034289047 0.035815593 0.03481818 0.031757992 0.027310742 0.022375802 0.017561639 0.013105185 0.0091174655 0.0056630075 0.0028742244][0.017038457 0.026137115 0.035877876 0.0444654 0.050303277 0.052629147 0.051495634 0.047443762 0.041359197 0.034346547 0.027369509 0.020861927 0.01499738 0.0098783579 0.0056858202][0.023608919 0.035172042 0.047368929 0.058062039 0.0653361 0.068349913 0.0672276 0.062553413 0.055293787 0.046623461 0.037773088 0.02933605 0.021598514 0.014726356 0.008970757][0.028758442 0.041963004 0.05564848 0.067455009 0.075355507 0.078595318 0.077384874 0.072307855 0.064402066 0.054855786 0.045028314 0.035549071 0.026719872 0.018734585 0.011892162][0.031067487 0.044685226 0.058620498 0.070422128 0.078085184 0.080983639 0.0794372 0.0740848 0.065959074 0.056261677 0.046391502 0.036964107 0.028200306 0.020178014 0.013170223][0.030241415 0.043202359 0.0563262 0.067241967 0.074094951 0.0763663 0.0744774 0.069073573 0.0611276 0.05182511 0.042509921 0.03377647 0.025800453 0.018579734 0.012268603][0.026893176 0.03828378 0.049715422 0.059029911 0.064632855 0.066104688 0.0639667 0.058884244 0.051633719 0.043343592 0.035139017 0.027564924 0.020753911 0.014721099 0.009584452][0.022091957 0.031321496 0.040483635 0.047768172 0.051889442 0.052540287 0.050274979 0.045764815 0.039657611 0.032898232 0.026266808 0.020207219 0.014796911 0.010109922 0.0062338868][0.01666544 0.023527564 0.030160706 0.035187107 0.037701789 0.037526973 0.035228994 0.031451192 0.0266893 0.02169108 0.01689066 0.012572927 0.0087501062 0.0055260034 0.0029718145][0.011060677 0.015688188 0.019971535 0.022957066 0.024089722 0.023328308 0.021192886 0.018254822 0.014896449 0.011613348 0.0085952776 0.00597894 0.0037073768 0.0018461464 0.00043750217][0.0057055685 0.0083819516 0.010707943 0.012120292 0.012348871 0.011448496 0.0098177958 0.0078672133 0.005858053 0.0040628863 0.002525256 0.0012787475 0.00024953776 -0.00054558332 -0.0011068834][0.0014366616 0.0026229774 0.0035846434 0.0040700436 0.0039803842 0.0033881869 0.0025189891 0.0015861419 0.0007141059 4.9301889e-07 -0.00055946806 -0.00097287312 -0.0012911384 -0.0015220011 -0.0016724574]]...]
INFO - root - 2017-12-09 14:05:48.725861: step 29310, loss = 0.83, batch loss = 0.70 (9.5 examples/sec; 0.843 sec/batch; 70h:59m:41s remains)
INFO - root - 2017-12-09 14:05:57.302735: step 29320, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.865 sec/batch; 72h:49m:16s remains)
INFO - root - 2017-12-09 14:06:05.964754: step 29330, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.876 sec/batch; 73h:48m:29s remains)
INFO - root - 2017-12-09 14:06:14.511487: step 29340, loss = 0.82, batch loss = 0.69 (9.7 examples/sec; 0.829 sec/batch; 69h:47m:36s remains)
INFO - root - 2017-12-09 14:06:22.967173: step 29350, loss = 0.82, batch loss = 0.69 (11.4 examples/sec; 0.703 sec/batch; 59h:12m:06s remains)
INFO - root - 2017-12-09 14:06:31.560365: step 29360, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.847 sec/batch; 71h:19m:59s remains)
INFO - root - 2017-12-09 14:06:40.150394: step 29370, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.837 sec/batch; 70h:29m:58s remains)
INFO - root - 2017-12-09 14:06:48.782044: step 29380, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.877 sec/batch; 73h:51m:20s remains)
INFO - root - 2017-12-09 14:06:57.165047: step 29390, loss = 0.82, batch loss = 0.69 (11.0 examples/sec; 0.728 sec/batch; 61h:15m:18s remains)
INFO - root - 2017-12-09 14:07:05.490491: step 29400, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.836 sec/batch; 70h:23m:34s remains)
2017-12-09 14:07:06.316084: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.0049149035 0.005139289 0.0057025547 0.0064809411 0.0071577025 0.0078332461 0.0082890484 0.0082623232 0.0076251789 0.0066456874 0.0055506565 0.0042504058 0.0028848238 0.0015849071 0.00046583288][0.0089884326 0.010677472 0.012711613 0.014790859 0.016256412 0.016814005 0.016388072 0.01488502 0.012484021 0.0097163692 0.0071283006 0.0048352289 0.0028173407 0.0012258801 6.7503424e-05][0.012599759 0.016886091 0.021990964 0.027155263 0.030964024 0.032692246 0.032233071 0.029866278 0.025721187 0.0204863 0.015229313 0.010519581 0.0064944453 0.003221733 0.00086566526][0.017320938 0.024964973 0.03411625 0.043512154 0.051119141 0.055536438 0.05621811 0.053620379 0.048267782 0.040971331 0.032773554 0.024469776 0.016755929 0.010061119 0.0047203111][0.023535732 0.034664642 0.048270341 0.06253922 0.074594028 0.082263879 0.085003451 0.083391145 0.077728815 0.068944782 0.058195278 0.046486896 0.034522086 0.023043452 0.01307727][0.030933764 0.045914561 0.063738316 0.082014874 0.097642966 0.10819332 0.11313885 0.11306405 0.10834976 0.099690422 0.087761678 0.073360138 0.057361636 0.040855806 0.025539026][0.036441822 0.054817934 0.076519586 0.098533452 0.11716364 0.12975416 0.13624118 0.13756217 0.13410325 0.12635453 0.1147159 0.099458285 0.081022024 0.060441468 0.040159296][0.039277103 0.059296202 0.082822435 0.10684261 0.12739709 0.14151713 0.14920993 0.15167083 0.14956047 0.14324288 0.132762 0.11796703 0.0989749 0.07658378 0.053281069][0.038224548 0.057848573 0.080770031 0.10436959 0.12490964 0.1396272 0.14814289 0.15162413 0.15083833 0.14598975 0.13709995 0.12364364 0.10563152 0.083590925 0.059874088][0.032417778 0.049900558 0.070235826 0.091344252 0.11012252 0.12411003 0.13265583 0.13679332 0.13727811 0.13401538 0.12695862 0.1155138 0.099717125 0.079931974 0.058147307][0.022713726 0.036442064 0.052660804 0.069856524 0.085525781 0.097632557 0.10538325 0.10950687 0.11068979 0.10883909 0.10389261 0.09515778 0.082621925 0.066621155 0.048744049][0.012834158 0.021941913 0.033102378 0.045356683 0.056961138 0.066354968 0.072693057 0.076332465 0.077748068 0.076907746 0.073813193 0.067896269 0.059140783 0.047785778 0.034956567][0.0054608667 0.01047387 0.016870735 0.024175441 0.031357534 0.03743995 0.041800767 0.044532061 0.04587099 0.045779228 0.044263247 0.0409274 0.03574647 0.028862847 0.020974576][0.0011333954 0.0034500449 0.0065492853 0.01023308 0.013981504 0.017280612 0.019749623 0.021368096 0.022246441 0.022376569 0.021777203 0.020222817 0.017676245 0.014185561 0.010107899][-0.00089940842 -4.2169471e-05 0.0011942324 0.002750447 0.0044111745 0.0059404764 0.0071391729 0.00795983 0.0084421188 0.0085905511 0.0084238425 0.0078392141 0.0068017379 0.0053108083 0.0035164165]]...]
INFO - root - 2017-12-09 14:07:14.756051: step 29410, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.844 sec/batch; 71h:03m:12s remains)
INFO - root - 2017-12-09 14:07:23.323807: step 29420, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.873 sec/batch; 73h:27m:38s remains)
INFO - root - 2017-12-09 14:07:31.946263: step 29430, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.832 sec/batch; 70h:01m:47s remains)
INFO - root - 2017-12-09 14:07:40.452158: step 29440, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.853 sec/batch; 71h:50m:46s remains)
INFO - root - 2017-12-09 14:07:48.959291: step 29450, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.854 sec/batch; 71h:51m:22s remains)
INFO - root - 2017-12-09 14:07:57.541758: step 29460, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.875 sec/batch; 73h:40m:43s remains)
INFO - root - 2017-12-09 14:08:06.103076: step 29470, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.849 sec/batch; 71h:29m:55s remains)
INFO - root - 2017-12-09 14:08:14.642540: step 29480, loss = 0.82, batch loss = 0.69 (9.9 examples/sec; 0.806 sec/batch; 67h:51m:39s remains)
INFO - root - 2017-12-09 14:08:23.197045: step 29490, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.838 sec/batch; 70h:29m:42s remains)
INFO - root - 2017-12-09 14:08:31.676013: step 29500, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.860 sec/batch; 72h:24m:42s remains)
2017-12-09 14:08:32.555385: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.160297 0.16665038 0.17329574 0.18012661 0.1859111 0.19287433 0.19944006 0.20474719 0.20766461 0.20518638 0.19894834 0.19113272 0.18418121 0.17828096 0.17170945][0.1517415 0.15767546 0.16490783 0.17321391 0.18082473 0.19048676 0.1999478 0.20812485 0.21340251 0.21350662 0.20933801 0.20207922 0.19522698 0.1890333 0.18106653][0.13822126 0.14317527 0.15033333 0.15942885 0.16898094 0.18086129 0.19288839 0.20407762 0.21235415 0.21535949 0.21358953 0.20771469 0.20122409 0.19457826 0.18502007][0.12164026 0.12548196 0.1322421 0.14231762 0.15363778 0.16775015 0.18206127 0.19579916 0.20682083 0.21310271 0.21433017 0.21044014 0.20518334 0.19840081 0.18745102][0.10305698 0.10499542 0.11080208 0.12157599 0.13462751 0.1509764 0.16798577 0.18492453 0.1991062 0.20872958 0.21314071 0.21233657 0.20895132 0.20243932 0.19091122][0.08452943 0.084112294 0.088445112 0.09938129 0.11398821 0.13244647 0.15186727 0.17205566 0.18950768 0.20263745 0.2106335 0.21278802 0.21118943 0.20577139 0.19422518][0.067174815 0.0648945 0.068070725 0.078970164 0.094408788 0.11423655 0.13581543 0.15827055 0.17807309 0.19423796 0.20588219 0.21158682 0.21263994 0.20872101 0.19806854][0.054959096 0.051492903 0.053418163 0.06347961 0.079115242 0.099625088 0.12222873 0.14558694 0.16681343 0.18491822 0.19891624 0.20801149 0.21164292 0.20950784 0.20049891][0.048153449 0.044709485 0.046253603 0.055427503 0.070486911 0.090821885 0.11336612 0.13631105 0.15776709 0.17673339 0.19209738 0.20337811 0.20889492 0.20831677 0.20097052][0.048637442 0.04563766 0.047409911 0.056083903 0.070067286 0.088644072 0.1095487 0.1309873 0.15128623 0.169639 0.18530452 0.19775653 0.20427932 0.20497151 0.1990964][0.05507455 0.053153265 0.055264734 0.06373214 0.076713063 0.093197219 0.11148053 0.12994698 0.14746469 0.1635886 0.17784166 0.18953277 0.19609787 0.19763882 0.19320832][0.065318421 0.064857051 0.067551546 0.075266644 0.086727 0.10079949 0.1157968 0.13063507 0.14470281 0.15795849 0.16979888 0.17969573 0.1855747 0.18723297 0.18412572][0.078040116 0.078944609 0.081846468 0.088269114 0.097382724 0.10830203 0.1195585 0.13032611 0.14067763 0.15060714 0.15981267 0.16750506 0.17204872 0.17354563 0.17136583][0.088036694 0.089433938 0.091785654 0.096743673 0.10352134 0.11132974 0.11925616 0.12687807 0.13415179 0.14133722 0.14832062 0.15419137 0.15780982 0.1591382 0.15761861][0.09507367 0.096704923 0.097957678 0.10113562 0.1054947 0.11024161 0.11493254 0.11995018 0.12487107 0.12970242 0.13465935 0.13885371 0.14169735 0.14299496 0.14222363]]...]
INFO - root - 2017-12-09 14:08:41.195248: step 29510, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.841 sec/batch; 70h:45m:16s remains)
INFO - root - 2017-12-09 14:08:49.910485: step 29520, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.890 sec/batch; 74h:53m:58s remains)
INFO - root - 2017-12-09 14:08:58.566221: step 29530, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.897 sec/batch; 75h:30m:54s remains)
INFO - root - 2017-12-09 14:09:07.212187: step 29540, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.881 sec/batch; 74h:09m:24s remains)
INFO - root - 2017-12-09 14:09:15.934643: step 29550, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.855 sec/batch; 71h:56m:11s remains)
INFO - root - 2017-12-09 14:09:24.419607: step 29560, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.829 sec/batch; 69h:46m:01s remains)
INFO - root - 2017-12-09 14:09:33.089761: step 29570, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.877 sec/batch; 73h:48m:15s remains)
INFO - root - 2017-12-09 14:09:41.795642: step 29580, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.897 sec/batch; 75h:30m:02s remains)
INFO - root - 2017-12-09 14:09:50.637066: step 29590, loss = 0.82, batch loss = 0.69 (8.4 examples/sec; 0.956 sec/batch; 80h:28m:50s remains)
INFO - root - 2017-12-09 14:09:58.924920: step 29600, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.881 sec/batch; 74h:06m:13s remains)
2017-12-09 14:09:59.887731: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0018096991 -0.0018091467 -0.0018094877 -0.0018099868 -0.001810363 -0.00181062 -0.0018107526 -0.001810778 -0.0018107819 -0.0018107281 -0.0018106416 -0.0018106046 -0.0018106349 -0.0018106818 -0.0018106728][-0.0018087567 -0.0018081522 -0.0018084962 -0.0018089804 -0.001809312 -0.0018095175 -0.0018096254 -0.0018096621 -0.0018096906 -0.0018096935 -0.001809686 -0.0018097773 -0.0018099581 -0.0018101264 -0.0018102287][-0.0018085395 -0.0018078998 -0.001808149 -0.0018084776 -0.0018086466 -0.0018087084 -0.0018087262 -0.0018087329 -0.001808728 -0.0018087556 -0.0018088404 -0.0018090864 -0.0018094672 -0.0018098105 -0.0018100981][-0.0018086836 -0.0018079747 -0.0018080224 -0.001808128 -0.0018080688 -0.0018078963 -0.0018077525 -0.001807633 -0.0018074788 -0.0018074508 -0.0018075738 -0.0018079616 -0.00180856 -0.0018091518 -0.0018097338][-0.0018091298 -0.0018083557 -0.0018082088 -0.0018081252 -0.0018078922 -0.0018074777 -0.0018070799 -0.0018067023 -0.0018063338 -0.0018061477 -0.0018062019 -0.0018066494 -0.001807435 -0.0018082898 -0.0018092094][-0.0018096935 -0.0018088756 -0.0018086077 -0.0018083706 -0.0018079901 -0.0018073643 -0.0018067469 -0.0018061206 -0.0018055273 -0.0018051708 -0.0018051143 -0.0018055704 -0.0018064592 -0.0018075267 -0.0018086993][-0.0018102407 -0.0018093707 -0.0018090393 -0.001808705 -0.0018082121 -0.0018074553 -0.0018066941 -0.0018058833 -0.001805126 -0.0018046231 -0.0018044619 -0.0018048725 -0.0018057898 -0.0018069787 -0.0018082859][-0.0018104804 -0.001809608 -0.0018092963 -0.0018089853 -0.0018084785 -0.0018076997 -0.0018068837 -0.0018059831 -0.0018051933 -0.0018046328 -0.0018043704 -0.001804674 -0.0018055266 -0.0018067065 -0.0018080372][-0.0018104325 -0.0018096648 -0.0018094948 -0.0018092876 -0.0018088858 -0.0018082146 -0.0018074608 -0.001806624 -0.001805904 -0.0018053843 -0.0018050979 -0.0018052736 -0.0018059583 -0.0018069833 -0.0018081475][-0.0018104994 -0.0018098393 -0.0018098394 -0.0018097726 -0.0018094943 -0.0018089932 -0.0018084059 -0.0018077448 -0.0018071648 -0.0018067497 -0.0018064979 -0.0018065873 -0.0018070579 -0.0018078055 -0.0018086354][-0.0018105593 -0.0018100002 -0.0018100855 -0.0018100893 -0.0018098792 -0.0018095175 -0.0018091168 -0.0018086924 -0.0018083256 -0.001808082 -0.001807955 -0.0018080272 -0.0018083163 -0.0018087643 -0.0018092388][-0.0018106212 -0.0018101546 -0.0018102574 -0.0018102811 -0.0018101126 -0.0018098572 -0.0018095849 -0.0018093411 -0.0018091457 -0.0018090456 -0.0018090215 -0.001809101 -0.0018092897 -0.0018095241 -0.0018097566][-0.0018106487 -0.0018102659 -0.0018103779 -0.0018104218 -0.0018103425 -0.0018102028 -0.0018100565 -0.001809951 -0.0018098492 -0.0018098 -0.0018098125 -0.0018098714 -0.0018099612 -0.0018100463 -0.0018101148][-0.0018108278 -0.0018104569 -0.0018105172 -0.0018105874 -0.0018105914 -0.0018105549 -0.0018105083 -0.0018104942 -0.0018104456 -0.0018103882 -0.0018103539 -0.0018103459 -0.001810331 -0.0018102952 -0.0018102541][-0.0018110384 -0.0018105545 -0.0018105124 -0.0018105737 -0.0018105946 -0.0018105899 -0.0018105878 -0.0018105985 -0.0018105737 -0.0018105118 -0.0018104538 -0.0018103987 -0.0018103315 -0.0018102577 -0.0018101911]]...]
INFO - root - 2017-12-09 14:10:08.373406: step 29610, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.866 sec/batch; 72h:51m:56s remains)
INFO - root - 2017-12-09 14:10:17.013749: step 29620, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.886 sec/batch; 74h:31m:42s remains)
INFO - root - 2017-12-09 14:10:25.720093: step 29630, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.879 sec/batch; 73h:55m:14s remains)
INFO - root - 2017-12-09 14:10:34.495524: step 29640, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.899 sec/batch; 75h:35m:26s remains)
INFO - root - 2017-12-09 14:10:43.223928: step 29650, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.863 sec/batch; 72h:36m:30s remains)
INFO - root - 2017-12-09 14:10:51.908844: step 29660, loss = 0.83, batch loss = 0.70 (8.9 examples/sec; 0.898 sec/batch; 75h:32m:01s remains)
INFO - root - 2017-12-09 14:11:00.644232: step 29670, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.853 sec/batch; 71h:47m:36s remains)
INFO - root - 2017-12-09 14:11:09.457476: step 29680, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.880 sec/batch; 73h:59m:28s remains)
INFO - root - 2017-12-09 14:11:18.163388: step 29690, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.870 sec/batch; 73h:10m:19s remains)
INFO - root - 2017-12-09 14:11:26.697240: step 29700, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.863 sec/batch; 72h:34m:10s remains)
2017-12-09 14:11:27.593901: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0018144913 -0.0018140138 -0.0018143186 -0.0018147405 -0.0018150592 -0.001815148 -0.0018150229 -0.0018143656 -0.00181314 -0.0018117463 -0.0018105342 -0.0018097474 -0.0018097042 -0.0018104508 -0.0018114835][-0.0018140745 -0.0018135486 -0.0018138692 -0.0018142827 -0.0018145707 -0.0018145703 -0.0018142745 -0.0018133889 -0.0018119561 -0.0018104165 -0.0018091172 -0.0018083591 -0.0018085184 -0.0018095 -0.0018107191][-0.0018141331 -0.0018135961 -0.0018139408 -0.0018143189 -0.0018145072 -0.0018143613 -0.0018139018 -0.0018128516 -0.0018113323 -0.0018098134 -0.0018085497 -0.0018078788 -0.0018081543 -0.001809217 -0.0018104664][-0.0018142145 -0.0018136568 -0.0018139862 -0.0018142867 -0.0018143334 -0.00181402 -0.001813415 -0.0018123095 -0.0018108785 -0.0018095803 -0.0018084719 -0.0018078595 -0.0018081066 -0.0018090642 -0.001810173][-0.0018142287 -0.0018137186 -0.0018140103 -0.0018142115 -0.0018140925 -0.0018135959 -0.0018128405 -0.001811743 -0.0018105231 -0.00180958 -0.0018087167 -0.0018081388 -0.0018082483 -0.0018089906 -0.0018098399][-0.0018142146 -0.0018137603 -0.0018140298 -0.0018141351 -0.0018138527 -0.0018131653 -0.0018122662 -0.0018112041 -0.0018102992 -0.0018097977 -0.0018091992 -0.0018086663 -0.0018085866 -0.0018090144 -0.0018095436][-0.001814149 -0.001813779 -0.0018140167 -0.0018140405 -0.0018135858 -0.0018127139 -0.0018116744 -0.0018106762 -0.0018101422 -0.0018101212 -0.0018098854 -0.0018094758 -0.0018092595 -0.0018093485 -0.0018095381][-0.0018141076 -0.0018137731 -0.0018139577 -0.0018138868 -0.001813293 -0.00181228 -0.0018111386 -0.0018102286 -0.0018100486 -0.0018104617 -0.0018106115 -0.0018104156 -0.0018101626 -0.0018100014 -0.0018098752][-0.0018140093 -0.0018137406 -0.0018138944 -0.0018137447 -0.0018130629 -0.0018119602 -0.0018107552 -0.0018099066 -0.0018099544 -0.0018106541 -0.0018111251 -0.0018111567 -0.0018109106 -0.0018105902 -0.0018102161][-0.0018138966 -0.0018137096 -0.0018138575 -0.0018136706 -0.0018129458 -0.0018117973 -0.001810565 -0.0018097174 -0.0018097829 -0.0018105357 -0.0018111698 -0.0018113806 -0.0018112 -0.0018108061 -0.0018103132][-0.0018139132 -0.0018137066 -0.0018138515 -0.0018136854 -0.0018130128 -0.0018119278 -0.0018107318 -0.0018098337 -0.0018097301 -0.0018103103 -0.001810869 -0.0018111091 -0.001811004 -0.001810673 -0.0018102361][-0.0018139888 -0.0018136845 -0.0018138369 -0.0018137326 -0.0018131997 -0.0018122817 -0.001811186 -0.001810252 -0.0018099296 -0.0018102045 -0.0018105458 -0.0018106994 -0.0018106297 -0.0018104063 -0.001810116][-0.0018140594 -0.0018136773 -0.0018138051 -0.001813784 -0.0018134385 -0.0018127407 -0.0018117942 -0.0018108503 -0.0018103202 -0.0018102636 -0.0018103435 -0.0018103643 -0.0018102874 -0.0018101516 -0.0018099707][-0.0018140727 -0.001813652 -0.0018137567 -0.0018138058 -0.001813616 -0.0018131125 -0.0018123344 -0.0018114486 -0.0018107962 -0.0018104688 -0.0018102764 -0.0018101268 -0.001809981 -0.0018098325 -0.0018096804][-0.0018141698 -0.0018136427 -0.0018137058 -0.0018138144 -0.0018137456 -0.0018134065 -0.0018127959 -0.0018120271 -0.001811337 -0.0018108534 -0.001810504 -0.0018102196 -0.0018100068 -0.0018098034 -0.0018096331]]...]
INFO - root - 2017-12-09 14:11:36.360867: step 29710, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.886 sec/batch; 74h:31m:42s remains)
INFO - root - 2017-12-09 14:11:45.006235: step 29720, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.851 sec/batch; 71h:34m:32s remains)
INFO - root - 2017-12-09 14:11:53.725454: step 29730, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.876 sec/batch; 73h:41m:35s remains)
INFO - root - 2017-12-09 14:12:02.210814: step 29740, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.844 sec/batch; 70h:57m:31s remains)
INFO - root - 2017-12-09 14:12:10.679752: step 29750, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.870 sec/batch; 73h:10m:34s remains)
INFO - root - 2017-12-09 14:12:19.174068: step 29760, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.841 sec/batch; 70h:41m:55s remains)
INFO - root - 2017-12-09 14:12:27.821957: step 29770, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 73h:04m:36s remains)
INFO - root - 2017-12-09 14:12:36.496591: step 29780, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.848 sec/batch; 71h:19m:24s remains)
INFO - root - 2017-12-09 14:12:45.167999: step 29790, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.858 sec/batch; 72h:07m:11s remains)
INFO - root - 2017-12-09 14:12:53.479623: step 29800, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.860 sec/batch; 72h:17m:53s remains)
2017-12-09 14:12:54.367235: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0018159975 -0.0018157437 -0.001815963 -0.0018163419 -0.0018169051 -0.001817539 -0.0018181488 -0.0018185722 -0.0018186993 -0.0018184542 -0.0018179289 -0.0018173164 -0.0018167517 -0.0018162397 -0.001815738][-0.0018162031 -0.0018158163 -0.00181594 -0.0018162575 -0.0018167876 -0.0018174112 -0.0018180122 -0.0018184234 -0.0018185668 -0.0018183915 -0.001817949 -0.0018173956 -0.0018168683 -0.0018163917 -0.0018159146][-0.0018166131 -0.0018160522 -0.0018159976 -0.0018161822 -0.001816607 -0.0018171463 -0.0018176946 -0.0018181033 -0.0018183052 -0.0018182646 -0.0018180115 -0.0018176278 -0.001817206 -0.0018167831 -0.0018163214][-0.0018172442 -0.0018164442 -0.0018161328 -0.0018161128 -0.0018163692 -0.0018167731 -0.0018172439 -0.0018176879 -0.0018180477 -0.0018182708 -0.0018183279 -0.0018182066 -0.0018179266 -0.0018175142 -0.0018169717][-0.0018180294 -0.0018169311 -0.0018163648 -0.0018161502 -0.0018162264 -0.0018164767 -0.0018168516 -0.0018173417 -0.0018179116 -0.0018184637 -0.0018188804 -0.0018190311 -0.0018188537 -0.0018183704 -0.0018176561][-0.001818861 -0.0018173775 -0.0018165953 -0.0018162294 -0.0018161816 -0.0018163169 -0.0018166384 -0.0018171998 -0.0018179925 -0.001818831 -0.0018195362 -0.0018198816 -0.0018197229 -0.0018190927 -0.0018181591][-0.0018195943 -0.0018177194 -0.0018166985 -0.0018162187 -0.0018160834 -0.0018161295 -0.0018163876 -0.001817005 -0.0018179689 -0.0018190487 -0.001820004 -0.0018204988 -0.0018203313 -0.0018195378 -0.0018184131][-0.0018200584 -0.0018178917 -0.0018166951 -0.0018161515 -0.0018159559 -0.0018159202 -0.0018160903 -0.0018166791 -0.0018177096 -0.0018189186 -0.0018200225 -0.0018205929 -0.001820426 -0.0018195625 -0.0018183552][-0.0018201142 -0.0018178151 -0.0018165987 -0.0018160807 -0.0018159142 -0.0018158939 -0.00181606 -0.0018166325 -0.0018176191 -0.001818736 -0.001819718 -0.0018201863 -0.0018199837 -0.0018191155 -0.0018179347][-0.0018198235 -0.0018175363 -0.0018164123 -0.0018159612 -0.0018158694 -0.0018159347 -0.0018161554 -0.0018166705 -0.0018174365 -0.0018182207 -0.0018188502 -0.0018190996 -0.00181888 -0.0018181315 -0.0018171633][-0.001819263 -0.0018171489 -0.0018161281 -0.0018157121 -0.0018156497 -0.0018157554 -0.0018159718 -0.0018163382 -0.0018167885 -0.0018171681 -0.0018174505 -0.0018175375 -0.001817361 -0.001816851 -0.0018162028][-0.0018185287 -0.0018167254 -0.0018158391 -0.0018154437 -0.0018153406 -0.0018153731 -0.0018154751 -0.0018156334 -0.0018157607 -0.0018158047 -0.0018158364 -0.0018158214 -0.0018157456 -0.001815515 -0.0018152196][-0.0018176425 -0.0018162527 -0.0018155855 -0.0018152221 -0.0018150549 -0.0018149606 -0.0018148992 -0.0018148484 -0.0018147534 -0.0018146053 -0.0018144754 -0.0018143764 -0.0018143696 -0.0018143677 -0.0018143544][-0.0018165823 -0.0018155964 -0.0018151421 -0.0018148168 -0.0018145912 -0.0018144008 -0.0018142448 -0.0018141152 -0.0018139462 -0.0018137298 -0.0018135202 -0.0018133856 -0.0018133931 -0.0018134831 -0.0018135998][-0.0018154209 -0.0018147465 -0.0018144484 -0.001814188 -0.0018139554 -0.0018137288 -0.0018135423 -0.0018134135 -0.0018132688 -0.0018130635 -0.0018128721 -0.0018127866 -0.0018128515 -0.0018129895 -0.0018131236]]...]
INFO - root - 2017-12-09 14:13:03.221532: step 29810, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.881 sec/batch; 74h:04m:03s remains)
INFO - root - 2017-12-09 14:13:12.005270: step 29820, loss = 0.81, batch loss = 0.68 (9.3 examples/sec; 0.858 sec/batch; 72h:07m:15s remains)
INFO - root - 2017-12-09 14:13:20.706362: step 29830, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.889 sec/batch; 74h:44m:54s remains)
INFO - root - 2017-12-09 14:13:29.596718: step 29840, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.875 sec/batch; 73h:35m:03s remains)
INFO - root - 2017-12-09 14:13:38.297321: step 29850, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.878 sec/batch; 73h:47m:41s remains)
INFO - root - 2017-12-09 14:13:46.891494: step 29860, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.858 sec/batch; 72h:09m:35s remains)
INFO - root - 2017-12-09 14:13:55.615454: step 29870, loss = 0.83, batch loss = 0.70 (9.2 examples/sec; 0.873 sec/batch; 73h:23m:43s remains)
INFO - root - 2017-12-09 14:14:04.417561: step 29880, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.882 sec/batch; 74h:06m:58s remains)
INFO - root - 2017-12-09 14:14:13.158512: step 29890, loss = 0.81, batch loss = 0.68 (8.9 examples/sec; 0.902 sec/batch; 75h:47m:24s remains)
INFO - root - 2017-12-09 14:14:21.629601: step 29900, loss = 0.83, batch loss = 0.70 (9.0 examples/sec; 0.892 sec/batch; 74h:56m:56s remains)
2017-12-09 14:14:22.594454: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.001817693 -0.0018160744 -0.0018150631 -0.0018144419 -0.0018140281 -0.0018139518 -0.001814156 -0.0018146313 -0.0018155425 -0.001816755 -0.001818148 -0.0018194892 -0.0018210242 -0.001822514 -0.0018236963][-0.0018137638 -0.0018122793 -0.0018114343 -0.0018108959 -0.0018104987 -0.0018104087 -0.0018104432 -0.001810679 -0.0018112743 -0.0018119992 -0.0018129461 -0.0018142995 -0.0018163802 -0.0018186875 -0.001820755][-0.0018100291 -0.0018086648 -0.0018082822 -0.0018082038 -0.0018079975 -0.0018079531 -0.0018078517 -0.0018077294 -0.0018077944 -0.0018079071 -0.0018083579 -0.0018094556 -0.0018118423 -0.0018147614 -0.0018176027][-0.0018067743 -0.0018055597 -0.0018055333 -0.0018058503 -0.0018059083 -0.0018059548 -0.001805795 -0.0018053303 -0.0018048567 -0.0018044057 -0.001804486 -0.0018054011 -0.0018078589 -0.0018110297 -0.001814406][-0.001804776 -0.001803268 -0.0018032634 -0.0018035874 -0.0018036867 -0.0018037759 -0.0018035929 -0.0018028619 -0.0018021587 -0.0018014967 -0.0018014498 -0.0018022369 -0.0018045576 -0.0018076524 -0.0018111429][-0.0018040001 -0.0018017753 -0.001801257 -0.0018012461 -0.0018012596 -0.0018014372 -0.0018012561 -0.0018004102 -0.0017998057 -0.0017992812 -0.0017992199 -0.0017999621 -0.0018020375 -0.0018047247 -0.0018077504][-0.0018039888 -0.0018011484 -0.0018000468 -0.0017994874 -0.0017991908 -0.0017993159 -0.0017989946 -0.0017979203 -0.001797624 -0.001797562 -0.0017976576 -0.001798414 -0.0018003172 -0.0018026364 -0.001805004][-0.0018045014 -0.0018013977 -0.0017999022 -0.0017990317 -0.0017984611 -0.0017984563 -0.0017980343 -0.0017968208 -0.0017965271 -0.0017968034 -0.0017970564 -0.0017978409 -0.0017996011 -0.0018015049 -0.0018032803][-0.0018051621 -0.0018021775 -0.0018005599 -0.0017995866 -0.0017989206 -0.0017988422 -0.0017985187 -0.0017975569 -0.0017971603 -0.0017973981 -0.0017976242 -0.001798225 -0.0017996492 -0.0018010621 -0.0018022667][-0.0018047523 -0.0018022407 -0.0018009336 -0.0018001436 -0.0017995102 -0.0017993717 -0.0017992321 -0.0017985835 -0.0017981518 -0.0017982017 -0.0017982753 -0.0017986052 -0.0017995182 -0.0018004548 -0.0018011309][-0.0018034533 -0.0018015505 -0.0018007801 -0.001800294 -0.0017996881 -0.0017994369 -0.0017992875 -0.0017988256 -0.0017984165 -0.0017983804 -0.0017983622 -0.0017985083 -0.001799055 -0.0017996429 -0.0017999415][-0.001802424 -0.0018009139 -0.0018005634 -0.0018003088 -0.0017998434 -0.0017995316 -0.0017993592 -0.0017990241 -0.0017987053 -0.0017986356 -0.0017985529 -0.0017985546 -0.001798845 -0.001799164 -0.0017992716][-0.0018020533 -0.0018006647 -0.0018004287 -0.0018003345 -0.001800089 -0.0017999137 -0.0017998125 -0.0017995842 -0.0017993632 -0.0017993008 -0.0017991412 -0.0017989352 -0.0017989527 -0.0017990211 -0.0017989967][-0.001802401 -0.0018010275 -0.0018007574 -0.0018006774 -0.0018005669 -0.0018005004 -0.0018004762 -0.0018003458 -0.0018001953 -0.001800162 -0.0018000301 -0.0017997994 -0.0017997109 -0.0017996801 -0.0017995683][-0.0018029949 -0.0018017036 -0.0018014053 -0.0018013127 -0.001801257 -0.0018012554 -0.0018012552 -0.0018011827 -0.0018010949 -0.0018010742 -0.001800986 -0.0018008027 -0.0018006922 -0.0018006423 -0.0018005309]]...]
INFO - root - 2017-12-09 14:14:31.396711: step 29910, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.873 sec/batch; 73h:23m:22s remains)
INFO - root - 2017-12-09 14:14:40.078982: step 29920, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.847 sec/batch; 71h:12m:09s remains)
INFO - root - 2017-12-09 14:14:48.787058: step 29930, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 72h:58m:30s remains)
INFO - root - 2017-12-09 14:14:57.512329: step 29940, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.857 sec/batch; 72h:03m:22s remains)
INFO - root - 2017-12-09 14:15:06.213988: step 29950, loss = 0.81, batch loss = 0.68 (9.2 examples/sec; 0.869 sec/batch; 72h:59m:54s remains)
INFO - root - 2017-12-09 14:15:14.755949: step 29960, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.839 sec/batch; 70h:28m:31s remains)
INFO - root - 2017-12-09 14:15:23.358990: step 29970, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.851 sec/batch; 71h:29m:24s remains)
INFO - root - 2017-12-09 14:15:31.984081: step 29980, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 72h:34m:54s remains)
INFO - root - 2017-12-09 14:15:40.686542: step 29990, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.857 sec/batch; 71h:58m:45s remains)
INFO - root - 2017-12-09 14:15:49.059349: step 30000, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.883 sec/batch; 74h:14m:16s remains)
2017-12-09 14:15:49.928074: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.0049990257 0.0051906733 0.0048859464 0.0039690519 0.0028165712 0.0025237931 0.0028428736 0.0045300946 0.0061142771 0.0070759505 0.0069192462 0.0061676055 0.0052507995 0.0048942985 0.006206017][0.0048329318 0.0042024255 0.0034970979 0.00260415 0.0020615973 0.0027095794 0.0040098019 0.0066576973 0.0088569028 0.0098740058 0.0096236514 0.0087766564 0.0077790888 0.0074237548 0.00941675][0.0066434341 0.0060011642 0.005716213 0.0053272936 0.0056275055 0.0070883371 0.0092721032 0.012483496 0.014925294 0.015710492 0.015119189 0.014166806 0.012745543 0.012380743 0.015059715][0.0089736432 0.0091703469 0.010299315 0.011901444 0.014397688 0.017797628 0.021534629 0.025344597 0.027655998 0.02779348 0.026429921 0.024877911 0.022878543 0.022036774 0.024535438][0.010342335 0.012998783 0.016797885 0.02125871 0.026240213 0.032021228 0.037813127 0.042886756 0.045762129 0.045783613 0.043849453 0.041944589 0.03887324 0.0369247 0.038291041][0.014285587 0.019955114 0.027199229 0.035267737 0.042897873 0.050424848 0.057052694 0.062563792 0.065514088 0.065673329 0.063799269 0.061344739 0.057671018 0.054098133 0.053650204][0.020217404 0.028390925 0.037816875 0.047795307 0.056794621 0.065685526 0.073116742 0.07900086 0.082056656 0.0826918 0.081350088 0.078834459 0.074730352 0.06955038 0.066679686][0.027149279 0.036800213 0.046771158 0.056538511 0.06498152 0.073267639 0.079993926 0.085229672 0.0878719 0.088892378 0.088322714 0.086154759 0.081977606 0.075717941 0.070900895][0.029504986 0.038810492 0.04809428 0.056661204 0.063814729 0.070786014 0.07612133 0.0800974 0.081792064 0.082512088 0.081993073 0.0798259 0.07542856 0.068730868 0.06285689][0.031098461 0.037828226 0.044013411 0.04928444 0.053463016 0.057855036 0.061457325 0.064175442 0.065089643 0.06531667 0.064534418 0.062312994 0.058070183 0.051971465 0.046395931][0.02723467 0.03148789 0.034692787 0.036969889 0.038501102 0.040506281 0.042322848 0.043830648 0.044086702 0.043807559 0.042795885 0.040755391 0.03716743 0.032401644 0.028024571][0.024835538 0.026136609 0.026450593 0.025681138 0.0246192 0.024278546 0.024530709 0.024975495 0.024763236 0.024238797 0.023290051 0.021771235 0.01932154 0.016284257 0.013529871][0.029526522 0.027557332 0.02497269 0.021861477 0.019016292 0.016881308 0.015661659 0.01449126 0.013113664 0.011762912 0.010504118 0.0092560966 0.0077376897 0.00609205 0.0046573877][0.049230255 0.044830494 0.040100757 0.035213251 0.030593609 0.0263969 0.022691296 0.018613327 0.014158392 0.0099115791 0.006498266 0.004060166 0.0023571539 0.0011983062 0.00044890121][0.082683347 0.076132827 0.068813048 0.061496992 0.05453657 0.047634631 0.040665194 0.032738756 0.023896208 0.015373884 0.0084553324 0.0037455282 0.0009639312 -0.00044244295 -0.0010463381]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-momentum-lr0.0001-clip-zeroinit-conv1-3init-from-scratch/model.ckpt-30000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-momentum-lr0.0001-clip-zeroinit-conv1-3init-from-scratch/model.ckpt-30000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-09 14:15:59.427951: step 30010, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 72h:23m:50s remains)
INFO - root - 2017-12-09 14:16:08.058313: step 30020, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.894 sec/batch; 75h:08m:47s remains)
INFO - root - 2017-12-09 14:16:16.837997: step 30030, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.880 sec/batch; 73h:57m:35s remains)
INFO - root - 2017-12-09 14:16:25.409178: step 30040, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 73h:10m:44s remains)
INFO - root - 2017-12-09 14:16:34.091619: step 30050, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 73h:02m:30s remains)
INFO - root - 2017-12-09 14:16:42.518133: step 30060, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.882 sec/batch; 74h:07m:31s remains)
INFO - root - 2017-12-09 14:16:51.213602: step 30070, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.870 sec/batch; 73h:02m:42s remains)
INFO - root - 2017-12-09 14:16:59.892843: step 30080, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.853 sec/batch; 71h:39m:18s remains)
INFO - root - 2017-12-09 14:17:08.547961: step 30090, loss = 0.82, batch loss = 0.69 (10.5 examples/sec; 0.763 sec/batch; 64h:04m:15s remains)
INFO - root - 2017-12-09 14:17:16.937484: step 30100, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.855 sec/batch; 71h:50m:07s remains)
2017-12-09 14:17:17.800738: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.18062286 0.20849286 0.228701 0.23937738 0.24181946 0.23699884 0.22872642 0.21845865 0.20871066 0.20137243 0.1954069 0.19226854 0.18932372 0.18670286 0.18354319][0.18316723 0.21220368 0.23319827 0.24414538 0.2468313 0.24233197 0.23460105 0.22475439 0.21561536 0.2094816 0.2055054 0.20357709 0.20161188 0.19919513 0.19566232][0.18189389 0.21167867 0.23275068 0.2441853 0.24716352 0.24333417 0.23626032 0.2269523 0.21880002 0.21388732 0.21193364 0.21126908 0.21045706 0.20855801 0.2047569][0.17858848 0.20921573 0.2312528 0.24383862 0.24776502 0.24505602 0.23876691 0.22993214 0.22281007 0.21927364 0.21901505 0.21993876 0.22027282 0.21876393 0.21459344][0.17565131 0.20773371 0.23055197 0.24476701 0.25031456 0.24891873 0.24326703 0.23467657 0.22792473 0.2247382 0.22529167 0.22676109 0.22788845 0.22677737 0.22277635][0.17409308 0.20752221 0.23153946 0.24712357 0.25365844 0.25353163 0.24803989 0.23921624 0.23213772 0.22860804 0.22881967 0.23000918 0.23149405 0.23081781 0.22781591][0.17278536 0.20777486 0.23353943 0.250647 0.25832748 0.2585994 0.2525014 0.24270333 0.23374015 0.22876073 0.22766346 0.22809079 0.22963016 0.22959076 0.22817652][0.1718325 0.2088404 0.23682168 0.25533566 0.26362029 0.26341256 0.25560334 0.24354085 0.23193444 0.22421731 0.22060721 0.2197745 0.22097903 0.22168843 0.22185348][0.16912906 0.20737442 0.23706147 0.25707921 0.26612979 0.26559979 0.25622371 0.24146038 0.22648691 0.21555896 0.20912601 0.20615765 0.20648272 0.20764484 0.20907561][0.16385515 0.20209479 0.23244244 0.25329584 0.26326349 0.26307112 0.25288865 0.23604558 0.21814212 0.20347103 0.19345197 0.18760222 0.18619379 0.18718597 0.18944609][0.15480007 0.19191992 0.22228752 0.24369471 0.2542682 0.25445059 0.2439892 0.22609827 0.20617627 0.18830761 0.17481761 0.16587503 0.16224761 0.16259758 0.16531728][0.1437804 0.17934972 0.20912251 0.23047741 0.24151209 0.24190632 0.23125632 0.21278697 0.19137529 0.17132653 0.15512264 0.14355332 0.13798487 0.13738719 0.14008337][0.13278283 0.16616961 0.19475369 0.21561004 0.2264192 0.22699939 0.21685044 0.19877243 0.17705274 0.15566535 0.137669 0.12393252 0.11657032 0.11498496 0.11741074][0.12016355 0.15096129 0.1775346 0.19769555 0.20880309 0.21061024 0.20240825 0.18626776 0.16600774 0.14479803 0.12625967 0.11130299 0.10253927 0.099709183 0.10153405][0.10724369 0.1356189 0.16027667 0.17905511 0.19015957 0.19345871 0.18779746 0.17489862 0.15749201 0.13837945 0.1207551 0.10568795 0.096175127 0.092188984 0.093163304]]...]
INFO - root - 2017-12-09 14:17:26.488867: step 30110, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.890 sec/batch; 74h:46m:04s remains)
INFO - root - 2017-12-09 14:17:35.258442: step 30120, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.857 sec/batch; 71h:57m:39s remains)
INFO - root - 2017-12-09 14:17:44.008853: step 30130, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.887 sec/batch; 74h:27m:46s remains)
INFO - root - 2017-12-09 14:17:52.617590: step 30140, loss = 0.82, batch loss = 0.69 (9.7 examples/sec; 0.824 sec/batch; 69h:10m:05s remains)
INFO - root - 2017-12-09 14:18:01.291476: step 30150, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.866 sec/batch; 72h:45m:01s remains)
INFO - root - 2017-12-09 14:18:09.863701: step 30160, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.877 sec/batch; 73h:36m:45s remains)
INFO - root - 2017-12-09 14:18:18.453530: step 30170, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.884 sec/batch; 74h:16m:48s remains)
INFO - root - 2017-12-09 14:18:27.154839: step 30180, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.881 sec/batch; 73h:57m:36s remains)
INFO - root - 2017-12-09 14:18:35.675237: step 30190, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.840 sec/batch; 70h:34m:17s remains)
INFO - root - 2017-12-09 14:18:44.188464: step 30200, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.861 sec/batch; 72h:15m:58s remains)
2017-12-09 14:18:45.141814: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0017844287 -0.0016835151 -0.0013957997 -0.0008507733 -9.5749157e-05 0.00068661326 0.0012250192 0.0012701029 0.00079982972 1.4753779e-05 -0.00079111627 -0.0013988095 -0.001711195 -0.0017895662 -0.0017944081][-0.0017432207 -0.0014886896 -0.00083365478 0.00033654564 0.0018790319 0.0033834917 0.004309441 0.0042483187 0.0032322095 0.0016752117 0.00013898558 -0.00099110906 -0.0015892229 -0.0017737015 -0.0017928218][-0.0016433381 -0.0010806639 0.00026713044 0.0025709867 0.0055152485 0.0083251344 0.010018612 0.0099002235 0.0080212876 0.0051071411 0.0021802629 -2.9583229e-05 -0.0012634145 -0.0017181843 -0.0017908394][-0.0014652418 -0.00041063409 0.0019843457 0.0059417211 0.010918623 0.015682686 0.018654954 0.018692059 0.015756046 0.010912186 0.005818252 0.001780595 -0.00060881884 -0.0015844788 -0.0017863697][-0.001191264 0.00045382779 0.0040802625 0.0099493871 0.017289227 0.024403188 0.029071251 0.02959107 0.025704099 0.018733107 0.010994521 0.0045156251 0.00044865219 -0.0013447828 -0.0017755579][-0.00078333169 0.0013738825 0.0060657468 0.013609275 0.023105586 0.032520737 0.039085392 0.040489919 0.036090668 0.027282832 0.016912811 0.0077827089 0.0017750965 -0.0010152434 -0.001756728][-0.00035860902 0.0020625554 0.007283641 0.015695723 0.026425965 0.037353814 0.045448221 0.04795076 0.043769527 0.034097578 0.021963999 0.010752496 0.0030436027 -0.00067999819 -0.0017343768][-7.9501886e-05 0.0022578388 0.0072710766 0.015430619 0.02604733 0.037204809 0.045967881 0.049407024 0.046048328 0.036702484 0.024252241 0.012274593 0.0037598968 -0.00046821486 -0.0017167663][-6.7216693e-05 0.0018701818 0.0060170889 0.012891017 0.022077862 0.032087617 0.040400479 0.044271931 0.0420437 0.034103468 0.022899289 0.01175171 0.0036330442 -0.00047523296 -0.0017107667][-0.00032841763 0.0010439755 0.0039689457 0.0089501012 0.015856521 0.023712428 0.030607844 0.034268197 0.03311361 0.027194489 0.01836725 0.0093488218 0.0026898608 -0.000706261 -0.0017211498][-0.000792796 4.4664368e-05 0.0017951644 0.0048795883 0.00936033 0.014716587 0.019676426 0.022576734 0.022144094 0.018260559 0.012196708 0.0059189564 0.0012846292 -0.0010626012 -0.0017419194][-0.0013253201 -0.0008925657 -2.4080509e-06 0.0016243883 0.0041024219 0.0072113518 0.0102207 0.012094785 0.0119876 0.0097645242 0.0062052552 0.0025404571 -0.00010738475 -0.0014117371 -0.0017642528][-0.0016522381 -0.001486813 -0.0011242647 -0.0004260171 0.00069746526 0.0021733283 0.0036494574 0.0045990977 0.0045767147 0.003507772 0.0018014555 9.0926886e-05 -0.001091205 -0.0016443266 -0.0017767245][-0.00176863 -0.0017331708 -0.0016367371 -0.001420824 -0.0010383194 -0.0005094395 3.3482909e-05 0.0003867835 0.00037581532 -1.9691186e-05 -0.00063181797 -0.0012151978 -0.0015897567 -0.0017504572 -0.0017807066][-0.001782699 -0.0017791869 -0.0017671844 -0.0017315535 -0.0016566957 -0.0015419585 -0.0014161018 -0.0013302439 -0.0013299314 -0.0014203453 -0.0015590927 -0.0016832823 -0.0017540519 -0.0017794491 -0.0017821804]]...]
INFO - root - 2017-12-09 14:18:53.749185: step 30210, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 72h:49m:02s remains)
INFO - root - 2017-12-09 14:19:02.429569: step 30220, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.878 sec/batch; 73h:42m:34s remains)
INFO - root - 2017-12-09 14:19:11.120591: step 30230, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.847 sec/batch; 71h:05m:22s remains)
INFO - root - 2017-12-09 14:19:19.570442: step 30240, loss = 0.82, batch loss = 0.69 (9.9 examples/sec; 0.811 sec/batch; 68h:03m:30s remains)
INFO - root - 2017-12-09 14:19:28.221642: step 30250, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.900 sec/batch; 75h:35m:28s remains)
INFO - root - 2017-12-09 14:19:36.649611: step 30260, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.853 sec/batch; 71h:35m:04s remains)
INFO - root - 2017-12-09 14:19:45.285926: step 30270, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.881 sec/batch; 73h:56m:21s remains)
INFO - root - 2017-12-09 14:19:53.947488: step 30280, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.886 sec/batch; 74h:21m:00s remains)
INFO - root - 2017-12-09 14:20:02.461829: step 30290, loss = 0.82, batch loss = 0.69 (9.8 examples/sec; 0.819 sec/batch; 68h:43m:36s remains)
INFO - root - 2017-12-09 14:20:10.936837: step 30300, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 72h:55m:19s remains)
2017-12-09 14:20:11.802887: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0017909259 -0.0017890405 -0.0017890458 -0.0017891743 -0.0017891251 -0.0017888512 -0.0017885383 -0.0017884155 -0.0017884331 -0.0017883321 -0.0017880681 -0.001787748 -0.0017876159 -0.001787635 -0.0017877138][-0.0017904007 -0.0017886156 -0.0017886595 -0.0017887241 -0.0017884499 -0.0017878519 -0.0017873097 -0.0017871604 -0.0017871883 -0.0017870159 -0.0017866974 -0.0017863597 -0.001786246 -0.0017863357 -0.0017865255][-0.0017904373 -0.0017887345 -0.001788646 -0.0017884279 -0.0017878334 -0.0017868388 -0.0017859546 -0.0017857203 -0.0017857307 -0.0017854233 -0.0017850803 -0.0017848654 -0.0017849316 -0.0017851988 -0.0017854887][-0.0017906851 -0.0017890648 -0.0017888352 -0.0017883561 -0.0017874121 -0.0017859904 -0.0017846588 -0.001784071 -0.0017838107 -0.0017834404 -0.0017833158 -0.0017834776 -0.0017837802 -0.0017841023 -0.0017843446][-0.0017908044 -0.0017891154 -0.0017887757 -0.0017880637 -0.0017868705 -0.0017851512 -0.0017834373 -0.0017823558 -0.0017817835 -0.0017815713 -0.0017818945 -0.0017824562 -0.0017828776 -0.0017830557 -0.0017831241][-0.0017906786 -0.0017888736 -0.0017884659 -0.0017877003 -0.0017863746 -0.0017844217 -0.0017823479 -0.0017808807 -0.0017802387 -0.0017803512 -0.0017811302 -0.0017819493 -0.0017822967 -0.0017822286 -0.0017820654][-0.0017903736 -0.00178862 -0.0017882908 -0.0017876535 -0.001786327 -0.0017841671 -0.0017817953 -0.0017801756 -0.0017796791 -0.0017799895 -0.0017808243 -0.0017816407 -0.0017818902 -0.0017816045 -0.001781175][-0.0017902179 -0.0017885845 -0.0017883846 -0.0017878448 -0.0017865456 -0.001784299 -0.0017818967 -0.0017803379 -0.0017800216 -0.0017804861 -0.0017812247 -0.0017818677 -0.0017818516 -0.0017811708 -0.0017803414][-0.0017902225 -0.0017886808 -0.0017886104 -0.0017881574 -0.0017869556 -0.0017848912 -0.0017827603 -0.0017814619 -0.0017813323 -0.0017818394 -0.0017823569 -0.0017825474 -0.0017820211 -0.0017808626 -0.0017796784][-0.0017899934 -0.0017885449 -0.0017884583 -0.0017880428 -0.0017870569 -0.0017854555 -0.0017838943 -0.0017831278 -0.0017832513 -0.0017836815 -0.0017838487 -0.0017835305 -0.0017825824 -0.001781223 -0.0017800309][-0.001789698 -0.00178825 -0.0017880215 -0.0017876845 -0.0017870306 -0.0017859901 -0.0017850292 -0.0017846455 -0.0017848676 -0.0017851723 -0.0017850312 -0.0017843789 -0.0017833732 -0.0017822407 -0.0017813338][-0.001789533 -0.0017879073 -0.0017875746 -0.0017873253 -0.0017869562 -0.0017863656 -0.0017858139 -0.0017855722 -0.0017857425 -0.0017859327 -0.0017856937 -0.0017850437 -0.0017842201 -0.0017834245 -0.0017828386][-0.0017894707 -0.00178776 -0.0017873757 -0.0017872228 -0.0017870049 -0.0017866615 -0.0017863284 -0.0017861637 -0.0017862817 -0.0017864108 -0.0017862814 -0.0017858733 -0.0017853207 -0.0017848065 -0.001784443][-0.0017895296 -0.0017876781 -0.0017872807 -0.0017872088 -0.0017870817 -0.0017868922 -0.0017867421 -0.0017866933 -0.0017867824 -0.0017868998 -0.0017868871 -0.0017866952 -0.001786383 -0.0017860765 -0.001785878][-0.0017894561 -0.001787579 -0.0017871603 -0.0017871277 -0.0017870918 -0.0017870447 -0.0017870192 -0.0017870308 -0.0017871233 -0.0017872314 -0.001787295 -0.0017872729 -0.0017871632 -0.0017870064 -0.0017868818]]...]
INFO - root - 2017-12-09 14:20:20.568725: step 30310, loss = 0.82, batch loss = 0.69 (8.8 examples/sec; 0.911 sec/batch; 76h:27m:59s remains)
INFO - root - 2017-12-09 14:20:29.297696: step 30320, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.880 sec/batch; 73h:53m:17s remains)
INFO - root - 2017-12-09 14:20:37.997091: step 30330, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.853 sec/batch; 71h:34m:17s remains)
INFO - root - 2017-12-09 14:20:46.738409: step 30340, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 72h:57m:51s remains)
INFO - root - 2017-12-09 14:20:55.435204: step 30350, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.853 sec/batch; 71h:37m:53s remains)
INFO - root - 2017-12-09 14:21:04.077213: step 30360, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.841 sec/batch; 70h:37m:06s remains)
INFO - root - 2017-12-09 14:21:12.759380: step 30370, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.859 sec/batch; 72h:04m:30s remains)
INFO - root - 2017-12-09 14:21:21.411696: step 30380, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.854 sec/batch; 71h:38m:31s remains)
INFO - root - 2017-12-09 14:21:30.015661: step 30390, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.865 sec/batch; 72h:36m:48s remains)
INFO - root - 2017-12-09 14:21:38.510621: step 30400, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 72h:50m:19s remains)
2017-12-09 14:21:39.399442: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.14837649 0.1560481 0.1566036 0.14986396 0.13837847 0.12432415 0.10915677 0.093853444 0.080049112 0.070199966 0.066164538 0.068956569 0.077071279 0.08684656 0.093761973][0.15591107 0.1695181 0.17628148 0.17443506 0.16582657 0.15228584 0.13546872 0.11656276 0.097783729 0.082416795 0.073501423 0.0724837 0.07808654 0.086923569 0.094280124][0.16277038 0.18182288 0.1945298 0.19777143 0.19303663 0.18109411 0.16415371 0.14330356 0.12105197 0.10121156 0.087447084 0.081911147 0.083756573 0.090207465 0.096535437][0.16846201 0.19231051 0.21066146 0.21941236 0.21954168 0.21072473 0.19500886 0.17331754 0.14864358 0.12523147 0.10731228 0.097411759 0.095224768 0.09839078 0.10305774][0.17124628 0.19797176 0.22026974 0.23380867 0.23859775 0.23347726 0.22066303 0.2003262 0.1753062 0.15001756 0.1291603 0.11553732 0.10915947 0.10842674 0.11071164][0.16965663 0.19696729 0.22094361 0.23749734 0.24562153 0.2438401 0.2339271 0.21572739 0.19199555 0.1667576 0.14472443 0.12871185 0.11930317 0.115433 0.11542882][0.16234539 0.18812867 0.2112478 0.22870828 0.2384278 0.23879641 0.23097913 0.2152041 0.1935806 0.16956778 0.14779922 0.1307207 0.11974116 0.11378177 0.11225008][0.14853284 0.17117956 0.19144329 0.20754686 0.21698371 0.2182052 0.21179411 0.19832157 0.17938079 0.15805532 0.13810775 0.12157936 0.11029191 0.103115 0.10050577][0.12930481 0.14725043 0.16305764 0.17622422 0.18371409 0.18438441 0.17844693 0.16680883 0.15072222 0.13280801 0.11584716 0.10139088 0.091113187 0.083991081 0.081037648][0.10628542 0.11869638 0.12920812 0.13830459 0.14293018 0.1421455 0.13627535 0.12635106 0.11333485 0.099215381 0.085873187 0.074372463 0.065980464 0.060098857 0.057481527][0.081221625 0.0880071 0.093293548 0.098309085 0.100166 0.097997077 0.092219569 0.083932839 0.074091293 0.06382753 0.054359455 0.046137068 0.040201452 0.036195222 0.034364976][0.057527561 0.059460811 0.060665257 0.0626393 0.062629744 0.059863839 0.054771449 0.048232391 0.041137002 0.03429649 0.028226204 0.02309121 0.019455465 0.017137866 0.016099226][0.038196024 0.035926249 0.034161258 0.034289069 0.033623561 0.031280648 0.027585393 0.023179509 0.018710816 0.014581388 0.011115082 0.0083025806 0.0064225267 0.00522443 0.0047124922][0.025248673 0.019468021 0.015381739 0.014232801 0.013436756 0.011939893 0.0098540587 0.0075574056 0.0053872922 0.0034659877 0.0019524965 0.00083694782 0.0001108424 -0.00032360386 -0.00051189633][0.018704362 0.01070715 0.0052368185 0.0033316384 0.0026827925 0.0020204131 0.0012140545 0.00041077507 -0.00026190875 -0.00078664569 -0.0011538179 -0.0013992117 -0.0015411228 -0.0016180375 -0.0016530803]]...]
INFO - root - 2017-12-09 14:21:48.165763: step 30410, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.854 sec/batch; 71h:39m:45s remains)
INFO - root - 2017-12-09 14:21:56.849456: step 30420, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.894 sec/batch; 75h:00m:59s remains)
INFO - root - 2017-12-09 14:22:05.426658: step 30430, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.882 sec/batch; 73h:59m:44s remains)
INFO - root - 2017-12-09 14:22:14.100038: step 30440, loss = 0.82, batch loss = 0.69 (8.8 examples/sec; 0.909 sec/batch; 76h:17m:59s remains)
INFO - root - 2017-12-09 14:22:22.701883: step 30450, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.856 sec/batch; 71h:49m:45s remains)
INFO - root - 2017-12-09 14:22:31.305016: step 30460, loss = 0.81, batch loss = 0.68 (8.9 examples/sec; 0.895 sec/batch; 75h:05m:44s remains)
INFO - root - 2017-12-09 14:22:40.158285: step 30470, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.851 sec/batch; 71h:23m:39s remains)
INFO - root - 2017-12-09 14:22:48.821289: step 30480, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.875 sec/batch; 73h:25m:36s remains)
INFO - root - 2017-12-09 14:22:57.420584: step 30490, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.872 sec/batch; 73h:10m:07s remains)
INFO - root - 2017-12-09 14:23:05.838103: step 30500, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.844 sec/batch; 70h:46m:28s remains)
2017-12-09 14:23:06.769055: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.50652909 0.49884468 0.49067822 0.48489302 0.47903484 0.47347957 0.46765578 0.46037573 0.45178834 0.44104326 0.43017274 0.42039981 0.41177079 0.40643269 0.40333274][0.50800741 0.50277507 0.497078 0.49329305 0.48891217 0.48484093 0.48005444 0.47483173 0.46736667 0.45697397 0.44479802 0.43278283 0.42183796 0.41381654 0.40861243][0.50091165 0.49962044 0.49803996 0.49733332 0.49579817 0.49365062 0.490018 0.4861854 0.47912103 0.46881121 0.45579425 0.44184288 0.42873925 0.41781816 0.41012311][0.48988107 0.49437547 0.49859434 0.5020991 0.50506949 0.50648248 0.50545108 0.50215209 0.49516237 0.48491684 0.470503 0.45448914 0.43836224 0.42447132 0.41365108][0.47714326 0.48586747 0.49397853 0.50304621 0.51194596 0.51808208 0.52088118 0.5195787 0.51354134 0.50247443 0.48611143 0.46810576 0.44870919 0.43124178 0.41679674][0.46571955 0.47794667 0.48893774 0.50204474 0.5154134 0.52616972 0.53259188 0.53398246 0.52935815 0.518447 0.50053257 0.47983882 0.45736578 0.4362818 0.41845429][0.45426127 0.46916088 0.48172241 0.49727479 0.51358289 0.52810228 0.53761554 0.5413425 0.53788716 0.52812529 0.51044869 0.48859921 0.46469024 0.44118261 0.42096353][0.44008282 0.45755714 0.47039846 0.48647279 0.50356644 0.51960152 0.53069556 0.53533959 0.5332877 0.52448469 0.50797468 0.48672223 0.4629 0.43845618 0.4169974][0.4278062 0.44609806 0.45784569 0.47315368 0.4896026 0.5056085 0.51704663 0.52210027 0.52101821 0.51331419 0.49730179 0.47681475 0.45356157 0.42938179 0.40791938][0.41470146 0.43201563 0.44080389 0.45368841 0.46791869 0.48233238 0.49288464 0.49775454 0.49726465 0.49014413 0.4755801 0.45682395 0.43611872 0.41477856 0.39571723][0.39986238 0.41608906 0.42231894 0.43090746 0.44131318 0.45295668 0.460869 0.46450645 0.46425614 0.45908943 0.44771069 0.43178651 0.41527325 0.39810562 0.38272926][0.386029 0.40096998 0.40457577 0.40943563 0.41572422 0.42320588 0.42805523 0.42981362 0.42898694 0.4251909 0.4172008 0.40563363 0.39374518 0.3812899 0.37020221][0.37293333 0.38601425 0.38752684 0.38973755 0.39301211 0.39664686 0.39865857 0.3986038 0.39720318 0.39415491 0.38827264 0.380287 0.3720918 0.36405981 0.35705703][0.36341795 0.37437576 0.37434533 0.37443429 0.37531778 0.37662295 0.37681311 0.37572229 0.3737697 0.37127733 0.36717084 0.36173081 0.35666135 0.35231566 0.34873459][0.3545064 0.36441565 0.36321643 0.3619411 0.36122388 0.36093256 0.3599073 0.35823449 0.35636187 0.35424671 0.35177952 0.34870341 0.34630567 0.34476328 0.34384128]]...]
INFO - root - 2017-12-09 14:23:15.520484: step 30510, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.856 sec/batch; 71h:46m:36s remains)
INFO - root - 2017-12-09 14:23:24.230187: step 30520, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.859 sec/batch; 72h:05m:36s remains)
INFO - root - 2017-12-09 14:23:32.857503: step 30530, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.883 sec/batch; 74h:03m:33s remains)
INFO - root - 2017-12-09 14:23:41.552878: step 30540, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.874 sec/batch; 73h:16m:30s remains)
INFO - root - 2017-12-09 14:23:50.194329: step 30550, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.866 sec/batch; 72h:38m:34s remains)
INFO - root - 2017-12-09 14:23:58.710635: step 30560, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.858 sec/batch; 71h:57m:46s remains)
INFO - root - 2017-12-09 14:24:07.341410: step 30570, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.832 sec/batch; 69h:45m:34s remains)
INFO - root - 2017-12-09 14:24:16.130002: step 30580, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.859 sec/batch; 72h:00m:32s remains)
INFO - root - 2017-12-09 14:24:24.794368: step 30590, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.886 sec/batch; 74h:20m:34s remains)
INFO - root - 2017-12-09 14:24:33.230121: step 30600, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 73h:02m:53s remains)
2017-12-09 14:24:34.050182: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.25308433 0.25533864 0.25429809 0.25125691 0.24813418 0.24536169 0.24271217 0.23991591 0.23723447 0.23576064 0.234406 0.23316106 0.2314954 0.22893475 0.22499141][0.2551367 0.25611463 0.25413185 0.25053096 0.2463146 0.24262814 0.23941512 0.23644826 0.23381816 0.2326057 0.23185527 0.23096514 0.22928204 0.22732249 0.22417322][0.2463965 0.2459897 0.24309087 0.23848295 0.23366182 0.22886376 0.22410747 0.22032143 0.21692714 0.21518587 0.21448016 0.21398312 0.21309136 0.21195598 0.21053706][0.23495892 0.23327443 0.22911578 0.22345997 0.21745674 0.21128331 0.20539543 0.20049761 0.19620152 0.19386205 0.19315517 0.19321074 0.19329445 0.19314277 0.1934087][0.22394776 0.2217214 0.21665122 0.20997113 0.20315731 0.1954792 0.1879338 0.18129566 0.17549044 0.17213677 0.17077264 0.1708733 0.17158881 0.17261691 0.17444351][0.21171746 0.20872617 0.20244136 0.1954172 0.18790641 0.17947127 0.17087422 0.16274567 0.1552501 0.14967179 0.1464844 0.14555857 0.14595076 0.14812119 0.15205531][0.19835414 0.19495952 0.18754765 0.17954072 0.17132966 0.16226356 0.15268877 0.14315195 0.13399586 0.12650624 0.12177109 0.11971226 0.12059999 0.12433138 0.13055843][0.18397388 0.18016081 0.17159592 0.16216949 0.15342318 0.14375815 0.13335264 0.12296043 0.11285501 0.10439372 0.098335482 0.0963442 0.09860146 0.10427975 0.11278158][0.17267889 0.16822796 0.15856886 0.14862484 0.13994986 0.13063231 0.1206436 0.11010389 0.099575095 0.090531811 0.083788216 0.081734963 0.08483012 0.091917381 0.10215896][0.16462484 0.16023794 0.14948048 0.13920842 0.1306719 0.1226267 0.11406107 0.10431339 0.094707288 0.086005054 0.079397917 0.076666333 0.079656765 0.08762642 0.098546855][0.1552788 0.15230201 0.14194623 0.13231738 0.12412167 0.11751197 0.1102881 0.10235833 0.094653912 0.087241374 0.0817225 0.078810148 0.0813983 0.088842936 0.099415645][0.14750639 0.1469695 0.13880949 0.13057855 0.12374265 0.11863699 0.11284506 0.1061821 0.099385478 0.093213044 0.088606335 0.08609356 0.088602029 0.0952691 0.10462335][0.1416145 0.14323437 0.1371488 0.13092592 0.12557554 0.12145482 0.11725172 0.11157116 0.10623715 0.10154026 0.097874127 0.09594503 0.097633094 0.10308585 0.11057337][0.13681631 0.13943782 0.13558853 0.1314746 0.12795386 0.12549742 0.12296159 0.11893893 0.11490434 0.111563 0.10903765 0.10759524 0.1088249 0.11274318 0.1182695][0.13342634 0.13683113 0.13381192 0.130775 0.12831883 0.12671643 0.12523286 0.12326999 0.12118411 0.11953361 0.11850397 0.11800135 0.11909873 0.12157822 0.12500519]]...]
INFO - root - 2017-12-09 14:24:42.830396: step 30610, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 72h:27m:33s remains)
INFO - root - 2017-12-09 14:24:51.617247: step 30620, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.878 sec/batch; 73h:39m:25s remains)
INFO - root - 2017-12-09 14:25:00.206637: step 30630, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.873 sec/batch; 73h:10m:57s remains)
INFO - root - 2017-12-09 14:25:08.750971: step 30640, loss = 0.82, batch loss = 0.69 (8.8 examples/sec; 0.910 sec/batch; 76h:17m:56s remains)
INFO - root - 2017-12-09 14:25:17.550553: step 30650, loss = 0.81, batch loss = 0.68 (9.1 examples/sec; 0.878 sec/batch; 73h:35m:01s remains)
INFO - root - 2017-12-09 14:25:26.223007: step 30660, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.859 sec/batch; 71h:59m:23s remains)
INFO - root - 2017-12-09 14:25:34.965949: step 30670, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.855 sec/batch; 71h:43m:11s remains)
INFO - root - 2017-12-09 14:25:43.571259: step 30680, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 72h:48m:24s remains)
INFO - root - 2017-12-09 14:25:52.138455: step 30690, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.901 sec/batch; 75h:30m:56s remains)
INFO - root - 2017-12-09 14:26:00.617989: step 30700, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.887 sec/batch; 74h:19m:20s remains)
2017-12-09 14:26:01.498027: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0018060891 -0.0018048574 -0.0018046986 -0.001804863 -0.0018051463 -0.0018056266 -0.0018062376 -0.0018068784 -0.0018074204 -0.0018078474 -0.0018080602 -0.001808021 -0.0018077269 -0.0018072962 -0.0018067722][-0.0018055516 -0.0018043092 -0.0018041779 -0.0018044192 -0.0018049112 -0.0018056458 -0.0018065402 -0.0018074152 -0.001808113 -0.0018085863 -0.0018087563 -0.0018085844 -0.0018080568 -0.0018073573 -0.0018065893][-0.0018054051 -0.0018041038 -0.0018038916 -0.0018041995 -0.0018048338 -0.001805711 -0.0018067054 -0.001807646 -0.0018083943 -0.0018088669 -0.0018089864 -0.0018087216 -0.0018080747 -0.0018072886 -0.0018064436][-0.0018052409 -0.0018037373 -0.0018033964 -0.0018037353 -0.001804468 -0.0018054351 -0.0018064191 -0.001807296 -0.001807973 -0.0018083258 -0.0018083333 -0.001808019 -0.001807365 -0.0018066234 -0.001805854][-0.0018051862 -0.0018033776 -0.0018028622 -0.0018031497 -0.0018038629 -0.0018047908 -0.0018056558 -0.0018063798 -0.0018068765 -0.0018070905 -0.001806939 -0.0018065532 -0.001805934 -0.0018053045 -0.0018047338][-0.0018050433 -0.0018029786 -0.0018023131 -0.0018024696 -0.0018030109 -0.0018037424 -0.0018043674 -0.0018047835 -0.0018050009 -0.0018050163 -0.0018047781 -0.0018044312 -0.0018039808 -0.001803629 -0.001803381][-0.0018048028 -0.0018025244 -0.0018017535 -0.0018017683 -0.0018020712 -0.0018024843 -0.0018027173 -0.0018027232 -0.0018026434 -0.0018025103 -0.0018023048 -0.0018021538 -0.0018020296 -0.0018020425 -0.0018021546][-0.0018044232 -0.0018019992 -0.0018010981 -0.001800917 -0.0018009575 -0.0018010576 -0.0018009547 -0.001800618 -0.0018003671 -0.0018002229 -0.0018001698 -0.0018002806 -0.001800462 -0.0018007992 -0.0018012073][-0.0018041524 -0.0018016122 -0.0018006365 -0.0018002586 -0.001800104 -0.0018000515 -0.0017999476 -0.0017996563 -0.0017993443 -0.0017991692 -0.0017992202 -0.00179942 -0.0017996457 -0.0018000011 -0.0018005174][-0.0018037751 -0.0018013804 -0.0018004009 -0.001799916 -0.0017996703 -0.0017995924 -0.0017995809 -0.0017994455 -0.0017992338 -0.0017990904 -0.0017991354 -0.0017992486 -0.0017993036 -0.001799464 -0.0017999063][-0.0018035553 -0.0018014032 -0.0018005108 -0.0018000463 -0.0017998028 -0.0017997522 -0.0017998338 -0.0017998209 -0.0017997562 -0.0017997273 -0.0017998008 -0.0017998262 -0.0017996966 -0.0017996357 -0.0017999519][-0.0018035597 -0.0018016538 -0.0018008961 -0.0018005874 -0.0018004576 -0.001800471 -0.0018005985 -0.0018006534 -0.0018006471 -0.0018006535 -0.0018007115 -0.0018006625 -0.0018004513 -0.0018002987 -0.0018005053][-0.001803749 -0.0018021127 -0.0018014912 -0.0018013308 -0.0018013159 -0.0018013627 -0.0018014907 -0.0018015521 -0.0018015482 -0.0018015525 -0.0018015718 -0.0018014873 -0.0018012739 -0.0018011121 -0.0018012833][-0.0018040456 -0.001802656 -0.0018021883 -0.0018021578 -0.0018022298 -0.0018023165 -0.0018024129 -0.0018024469 -0.0018024259 -0.0018024243 -0.0018024225 -0.0018023301 -0.0018021364 -0.0018019859 -0.0018021261][-0.0018044394 -0.0018032736 -0.0018029066 -0.0018029499 -0.0018030616 -0.0018031732 -0.0018032503 -0.0018032684 -0.0018032478 -0.0018032528 -0.0018032506 -0.0018031789 -0.0018030233 -0.0018028899 -0.001802963]]...]
INFO - root - 2017-12-09 14:26:10.308778: step 30710, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.898 sec/batch; 75h:14m:21s remains)
INFO - root - 2017-12-09 14:26:19.075641: step 30720, loss = 0.82, batch loss = 0.69 (9.8 examples/sec; 0.814 sec/batch; 68h:15m:42s remains)
INFO - root - 2017-12-09 14:26:27.862805: step 30730, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.887 sec/batch; 74h:20m:14s remains)
INFO - root - 2017-12-09 14:26:36.600266: step 30740, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.873 sec/batch; 73h:08m:22s remains)
INFO - root - 2017-12-09 14:26:45.350945: step 30750, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.886 sec/batch; 74h:14m:47s remains)
INFO - root - 2017-12-09 14:26:54.096693: step 30760, loss = 0.82, batch loss = 0.69 (8.3 examples/sec; 0.962 sec/batch; 80h:37m:39s remains)
INFO - root - 2017-12-09 14:27:02.794082: step 30770, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 72h:23m:14s remains)
INFO - root - 2017-12-09 14:27:11.426946: step 30780, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.839 sec/batch; 70h:18m:37s remains)
INFO - root - 2017-12-09 14:27:19.922714: step 30790, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.852 sec/batch; 71h:22m:08s remains)
INFO - root - 2017-12-09 14:27:28.335971: step 30800, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.861 sec/batch; 72h:08m:35s remains)
2017-12-09 14:27:29.272285: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0018138885 -0.0018157989 -0.0018178644 -0.0018195285 -0.0018203629 -0.0018204972 -0.0018203377 -0.0018200554 -0.0018199289 -0.0018196342 -0.0018187562 -0.0018176034 -0.0018160566 -0.0018142816 -0.0018123473][-0.0018153898 -0.0018177935 -0.0018198466 -0.0018209917 -0.0018212533 -0.0018210349 -0.0018208097 -0.0018209404 -0.0018216433 -0.0018221098 -0.0018214252 -0.0018197396 -0.0018174099 -0.0018148877 -0.0018123717][-0.0018176587 -0.0018201231 -0.00182167 -0.0018221413 -0.0018217227 -0.0018211075 -0.0018208335 -0.0018213551 -0.0018227235 -0.0018237086 -0.001823262 -0.0018214287 -0.0018186525 -0.0018154479 -0.0018123857][-0.0018197122 -0.0018216801 -0.0018223795 -0.0018217313 -0.0018204992 -0.0018194935 -0.0018191363 -0.0018201114 -0.0018222914 -0.0018239713 -0.0018239883 -0.0018225887 -0.0018199512 -0.0018166278 -0.0018133078][-0.0018210512 -0.001822311 -0.0018219594 -0.0018202554 -0.0018182864 -0.0018167545 -0.0018161851 -0.001817447 -0.0018202305 -0.00182253 -0.001823168 -0.0018222095 -0.0018197407 -0.0018164214 -0.0018130679][-0.0018217603 -0.0018224682 -0.0018213675 -0.0018191703 -0.0018167779 -0.0018147174 -0.0018137995 -0.0018149562 -0.0018178344 -0.0018204632 -0.001821638 -0.0018212083 -0.0018191515 -0.0018162644 -0.0018131705][-0.0018219857 -0.0018224349 -0.001821138 -0.0018190774 -0.0018167851 -0.0018144709 -0.0018128833 -0.0018133625 -0.0018156023 -0.0018178022 -0.0018190448 -0.0018191737 -0.0018180241 -0.0018158895 -0.0018135937][-0.0018220957 -0.0018226429 -0.0018218869 -0.0018204956 -0.00181863 -0.001816132 -0.0018137511 -0.0018131246 -0.0018141444 -0.0018155647 -0.00181689 -0.0018176663 -0.0018174399 -0.0018160548 -0.0018144042][-0.0018212575 -0.0018219214 -0.0018215936 -0.001820711 -0.0018194626 -0.0018173583 -0.0018147317 -0.0018134068 -0.0018136359 -0.0018146019 -0.0018158328 -0.0018168705 -0.0018170772 -0.0018160942 -0.0018145751][-0.0018196669 -0.0018206456 -0.0018207771 -0.0018202994 -0.0018194278 -0.0018179645 -0.0018157853 -0.0018144408 -0.0018142924 -0.0018148876 -0.0018159363 -0.0018170008 -0.0018170959 -0.0018156006 -0.0018135192][-0.0018184492 -0.0018195431 -0.0018199942 -0.0018199893 -0.0018195109 -0.0018184893 -0.0018169747 -0.0018159571 -0.0018156964 -0.0018160063 -0.001816781 -0.001817871 -0.0018178404 -0.0018159722 -0.0018132647][-0.0018161169 -0.0018171041 -0.0018178532 -0.0018180499 -0.0018178483 -0.0018172037 -0.001816332 -0.0018159203 -0.0018159429 -0.0018160896 -0.001816415 -0.001817055 -0.0018167833 -0.0018148159 -0.001812037][-0.0018133466 -0.0018138521 -0.0018145232 -0.0018147617 -0.0018147756 -0.0018145555 -0.001814387 -0.0018146291 -0.0018149335 -0.0018151406 -0.0018153082 -0.001815481 -0.0018146295 -0.0018125025 -0.0018099392][-0.001810913 -0.00181068 -0.0018108133 -0.0018108923 -0.0018112073 -0.0018115855 -0.0018121136 -0.0018128075 -0.0018134116 -0.0018139 -0.0018139771 -0.0018137067 -0.0018125664 -0.001810498 -0.0018082336][-0.0018094344 -0.0018087198 -0.0018084757 -0.0018083961 -0.0018086619 -0.0018089799 -0.0018096586 -0.0018104754 -0.0018112629 -0.0018119408 -0.0018121382 -0.0018118162 -0.0018107824 -0.0018091997 -0.0018074759]]...]
INFO - root - 2017-12-09 14:27:37.818138: step 30810, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.857 sec/batch; 71h:47m:04s remains)
INFO - root - 2017-12-09 14:27:46.408470: step 30820, loss = 0.82, batch loss = 0.69 (9.7 examples/sec; 0.821 sec/batch; 68h:46m:52s remains)
INFO - root - 2017-12-09 14:27:55.087408: step 30830, loss = 0.82, batch loss = 0.69 (9.9 examples/sec; 0.812 sec/batch; 68h:02m:25s remains)
INFO - root - 2017-12-09 14:28:03.711614: step 30840, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.847 sec/batch; 70h:58m:43s remains)
INFO - root - 2017-12-09 14:28:12.330504: step 30850, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.834 sec/batch; 69h:51m:42s remains)
INFO - root - 2017-12-09 14:28:20.772078: step 30860, loss = 0.82, batch loss = 0.69 (10.2 examples/sec; 0.781 sec/batch; 65h:28m:14s remains)
INFO - root - 2017-12-09 14:28:29.356178: step 30870, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.844 sec/batch; 70h:41m:55s remains)
INFO - root - 2017-12-09 14:28:38.079203: step 30880, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.873 sec/batch; 73h:10m:15s remains)
INFO - root - 2017-12-09 14:28:46.527575: step 30890, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.853 sec/batch; 71h:26m:31s remains)
INFO - root - 2017-12-09 14:28:54.945949: step 30900, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.858 sec/batch; 71h:51m:29s remains)
2017-12-09 14:28:55.772351: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.00182919 -0.0018287554 -0.0018286452 -0.0018284677 -0.0018283923 -0.0018283052 -0.0018280981 -0.0018279358 -0.0018278656 -0.0018277202 -0.0018274997 -0.0018274738 -0.0018276417 -0.0018280329 -0.0018285573][-0.0018265995 -0.0018262223 -0.0018262155 -0.0018262408 -0.0018264883 -0.0018266772 -0.0018267912 -0.0018271733 -0.0018277436 -0.001828242 -0.0018285363 -0.0018288009 -0.001829044 -0.0018293324 -0.0018295333][-0.0018246259 -0.0018243032 -0.0018243568 -0.0018245537 -0.0018249586 -0.0018252694 -0.0018256005 -0.0018263394 -0.0018273743 -0.0018283726 -0.0018291355 -0.0018296086 -0.0018299295 -0.001830193 -0.0018301961][-0.0018230831 -0.0018228022 -0.0018228912 -0.0018231724 -0.00182363 -0.0018240373 -0.0018245603 -0.0018255222 -0.0018267788 -0.0018279818 -0.0018290159 -0.0018296029 -0.0018299187 -0.0018301446 -0.0018300662][-0.0018217117 -0.0018214558 -0.0018215775 -0.0018219651 -0.0018225006 -0.0018229317 -0.001823495 -0.0018244478 -0.0018257378 -0.0018269544 -0.0018279845 -0.0018286319 -0.0018289663 -0.0018292371 -0.0018291492][-0.001820593 -0.0018204432 -0.0018206913 -0.001821202 -0.0018218025 -0.0018222573 -0.0018227183 -0.0018234124 -0.0018245089 -0.0018256482 -0.0018265834 -0.0018272165 -0.0018275528 -0.0018278831 -0.001827893][-0.0018198956 -0.0018197524 -0.001820071 -0.0018207281 -0.0018214553 -0.0018220205 -0.0018223451 -0.0018226068 -0.0018233279 -0.0018242827 -0.0018251103 -0.001825734 -0.0018260446 -0.0018263835 -0.0018265127][-0.0018194434 -0.001819296 -0.0018196168 -0.0018203394 -0.0018211327 -0.0018217951 -0.0018220972 -0.0018220557 -0.0018224356 -0.0018231287 -0.0018237777 -0.0018243679 -0.0018246901 -0.001824914 -0.0018250567][-0.0018189684 -0.001818761 -0.0018191367 -0.0018198269 -0.0018205785 -0.0018212388 -0.0018215376 -0.0018214156 -0.001821664 -0.0018222133 -0.0018226911 -0.0018231325 -0.0018234919 -0.0018236347 -0.001823661][-0.0018185819 -0.0018184464 -0.0018188802 -0.0018195595 -0.0018202005 -0.0018206857 -0.0018208239 -0.0018206264 -0.0018207582 -0.0018212029 -0.0018215877 -0.0018218919 -0.0018222096 -0.0018223533 -0.0018222989][-0.0018185555 -0.0018184358 -0.0018189275 -0.0018195966 -0.0018201496 -0.001820481 -0.0018204735 -0.0018201894 -0.0018201299 -0.0018203573 -0.0018206784 -0.0018208985 -0.0018211338 -0.0018212653 -0.0018211779][-0.0018186555 -0.0018185347 -0.0018190766 -0.0018197378 -0.0018202346 -0.001820459 -0.0018203385 -0.0018200072 -0.0018198314 -0.0018198813 -0.0018201211 -0.0018202997 -0.0018204701 -0.0018205402 -0.0018203933][-0.0018188386 -0.0018187677 -0.0018193313 -0.0018199207 -0.001820332 -0.0018204676 -0.0018202843 -0.0018199433 -0.0018197199 -0.0018197043 -0.0018199016 -0.0018200687 -0.0018201413 -0.0018200948 -0.0018198948][-0.0018190471 -0.0018190491 -0.0018195676 -0.0018200546 -0.0018203488 -0.0018204098 -0.0018202361 -0.0018199523 -0.0018197488 -0.0018197367 -0.0018199371 -0.001820082 -0.001820071 -0.001819927 -0.0018196893][-0.0018192639 -0.0018192967 -0.0018197193 -0.0018201052 -0.001820317 -0.001820356 -0.0018202398 -0.0018200616 -0.0018199277 -0.0018199511 -0.0018201366 -0.0018202408 -0.0018202012 -0.0018200647 -0.0018198591]]...]
INFO - root - 2017-12-09 14:29:04.320354: step 30910, loss = 0.82, batch loss = 0.69 (9.7 examples/sec; 0.828 sec/batch; 69h:20m:17s remains)
INFO - root - 2017-12-09 14:29:12.972773: step 30920, loss = 0.83, batch loss = 0.70 (9.4 examples/sec; 0.855 sec/batch; 71h:38m:42s remains)
INFO - root - 2017-12-09 14:29:21.716730: step 30930, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.876 sec/batch; 73h:22m:13s remains)
INFO - root - 2017-12-09 14:29:30.546823: step 30940, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.881 sec/batch; 73h:47m:56s remains)
INFO - root - 2017-12-09 14:29:39.329156: step 30950, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 72h:11m:20s remains)
INFO - root - 2017-12-09 14:29:47.881285: step 30960, loss = 0.82, batch loss = 0.69 (10.2 examples/sec; 0.782 sec/batch; 65h:27m:50s remains)
INFO - root - 2017-12-09 14:29:56.561444: step 30970, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.843 sec/batch; 70h:36m:12s remains)
INFO - root - 2017-12-09 14:30:05.164902: step 30980, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.881 sec/batch; 73h:48m:33s remains)
INFO - root - 2017-12-09 14:30:13.691530: step 30990, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.861 sec/batch; 72h:08m:40s remains)
INFO - root - 2017-12-09 14:30:22.101730: step 31000, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.853 sec/batch; 71h:23m:51s remains)
2017-12-09 14:30:23.071194: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0017990047 -0.0017976661 -0.0017976261 -0.001797832 -0.0017980119 -0.0017981514 -0.0017982833 -0.0017983502 -0.0017982873 -0.001798125 -0.0017979509 -0.0017977122 -0.0017973888 -0.0017970509 -0.0017967442][-0.0017987711 -0.0017974804 -0.0017974689 -0.0017977393 -0.0017979994 -0.0017982022 -0.0017983572 -0.0017983906 -0.0017982115 -0.0017979066 -0.001797627 -0.0017973306 -0.0017969626 -0.0017966004 -0.0017963264][-0.0017993376 -0.0017981824 -0.0017982345 -0.0017985816 -0.0017988939 -0.0017991182 -0.0017992591 -0.0017992106 -0.0017989207 -0.0017984923 -0.0017981532 -0.0017978529 -0.0017974598 -0.0017971164 -0.0017968992][-0.0017999795 -0.0017989639 -0.0017990856 -0.0017995285 -0.0017999014 -0.0018001297 -0.0018001979 -0.0017999873 -0.0017994927 -0.0017989519 -0.0017986251 -0.0017984139 -0.0017981277 -0.0017979665 -0.0017978908][-0.001800599 -0.0017997245 -0.0017999113 -0.0018004354 -0.0018008265 -0.0018009648 -0.0018008114 -0.0018003025 -0.0017994951 -0.0017987802 -0.0017984866 -0.0017984988 -0.0017985557 -0.0017987423 -0.0017988966][-0.0018011589 -0.0018004029 -0.0018006504 -0.0018012088 -0.0018015741 -0.0018015507 -0.0018011278 -0.0018002983 -0.0017992395 -0.0017984031 -0.001798204 -0.0017984813 -0.0017988642 -0.0017993186 -0.0017996326][-0.001801615 -0.0018010003 -0.0018012555 -0.0018017626 -0.0018020085 -0.0018017377 -0.0018009956 -0.0017998763 -0.0017987194 -0.001798 -0.0017980405 -0.0017985868 -0.0017991411 -0.0017996967 -0.0018000668][-0.0018019478 -0.0018014294 -0.0018016568 -0.0018020172 -0.0018020122 -0.0018013855 -0.0018002699 -0.0017989213 -0.0017978732 -0.0017974937 -0.0017978792 -0.0017986295 -0.0017992817 -0.0017998818 -0.0018002572][-0.0018019719 -0.0018015906 -0.001801803 -0.0018019703 -0.0018016794 -0.0018007779 -0.0017994958 -0.0017981989 -0.0017973878 -0.0017972896 -0.0017978455 -0.0017986376 -0.0017992964 -0.0017998666 -0.0018002278][-0.00180177 -0.0018014946 -0.001801602 -0.0018015698 -0.0018011001 -0.0018001795 -0.0017990516 -0.0017980168 -0.0017974647 -0.0017974973 -0.0017979927 -0.001798655 -0.0017992178 -0.0017996944 -0.0017999685][-0.0018015708 -0.0018012233 -0.001801184 -0.0018010141 -0.001800522 -0.0017997921 -0.0017989811 -0.0017982506 -0.0017978937 -0.0017979171 -0.0017982278 -0.0017986578 -0.0017990101 -0.0017993539 -0.0017995245][-0.0018013963 -0.0018009589 -0.001800803 -0.0018006102 -0.0018001695 -0.0017996298 -0.0017990564 -0.0017985192 -0.0017982369 -0.0017982292 -0.0017983892 -0.0017986048 -0.0017987776 -0.0017990151 -0.0017991284][-0.0018013493 -0.0018008427 -0.0018006114 -0.0018004717 -0.0018001507 -0.0017997783 -0.0017993478 -0.0017989238 -0.0017986621 -0.0017985697 -0.001798579 -0.0017986214 -0.0017986713 -0.0017988242 -0.0017989115][-0.0018015042 -0.0018009123 -0.0018006461 -0.0018005919 -0.0018004046 -0.0018001753 -0.0017998763 -0.0017995391 -0.0017992678 -0.0017990581 -0.0017989118 -0.0017988011 -0.0017987357 -0.0017987746 -0.0017988329][-0.0018018379 -0.0018011368 -0.0018008178 -0.0018008266 -0.0018007434 -0.0018006217 -0.0018004456 -0.0018001929 -0.0017999222 -0.001799656 -0.0017994042 -0.0017991848 -0.0017989958 -0.0017988959 -0.001798881]]...]
INFO - root - 2017-12-09 14:30:31.630755: step 31010, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.840 sec/batch; 70h:23m:20s remains)
INFO - root - 2017-12-09 14:30:40.173661: step 31020, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.861 sec/batch; 72h:08m:25s remains)
INFO - root - 2017-12-09 14:30:48.553589: step 31030, loss = 0.82, batch loss = 0.69 (10.1 examples/sec; 0.792 sec/batch; 66h:21m:05s remains)
INFO - root - 2017-12-09 14:30:57.191455: step 31040, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.847 sec/batch; 70h:57m:46s remains)
INFO - root - 2017-12-09 14:31:05.847663: step 31050, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.858 sec/batch; 71h:48m:36s remains)
INFO - root - 2017-12-09 14:31:14.413216: step 31060, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.836 sec/batch; 69h:59m:48s remains)
INFO - root - 2017-12-09 14:31:22.803802: step 31070, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.852 sec/batch; 71h:20m:43s remains)
INFO - root - 2017-12-09 14:31:31.414741: step 31080, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.859 sec/batch; 71h:57m:05s remains)
INFO - root - 2017-12-09 14:31:39.862066: step 31090, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.881 sec/batch; 73h:46m:58s remains)
INFO - root - 2017-12-09 14:31:48.242753: step 31100, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.834 sec/batch; 69h:48m:57s remains)
2017-12-09 14:31:49.151439: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0018014266 -0.0017996237 -0.0017990348 -0.0017987486 -0.0017986539 -0.0017987869 -0.0017991234 -0.001799594 -0.0018000508 -0.0018003245 -0.00180037 -0.0018001389 -0.0017998364 -0.0017995997 -0.0017994993][-0.0018009887 -0.0017995544 -0.0017991277 -0.0017989326 -0.0017988987 -0.0017989968 -0.0017992386 -0.0017995454 -0.0017998669 -0.0018001174 -0.001800184 -0.0018000075 -0.0017997349 -0.0017994887 -0.0017993401][-0.001801064 -0.0018001734 -0.0018000472 -0.0018000682 -0.0018001384 -0.00180022 -0.0018002945 -0.0018003482 -0.0018004414 -0.0018005624 -0.0018005872 -0.0018004118 -0.0018001359 -0.0017998916 -0.0017997241][-0.0018012355 -0.0018007566 -0.0018009295 -0.0018011787 -0.0018013235 -0.0018013485 -0.0018012272 -0.0018010094 -0.0018008773 -0.0018008525 -0.0018008557 -0.0018007279 -0.0018004861 -0.0018002265 -0.0018000065][-0.0018013249 -0.0018011905 -0.0018016027 -0.0018020403 -0.0018022211 -0.0018021716 -0.0018018808 -0.0018014285 -0.0018010946 -0.0018009751 -0.0018010079 -0.0018009752 -0.0018008436 -0.0018006426 -0.0018004128][-0.0018013557 -0.0018014017 -0.0018019989 -0.0018025851 -0.0018028078 -0.0018026609 -0.0018021842 -0.0018015278 -0.001801032 -0.0018008442 -0.0018009119 -0.0018010379 -0.001801118 -0.0018010911 -0.0018009314][-0.0018013592 -0.0018014704 -0.0018021668 -0.0018028361 -0.0018030691 -0.0018028021 -0.0018021269 -0.0018012442 -0.0018006045 -0.0018004095 -0.0018005444 -0.0018008389 -0.0018011727 -0.0018013782 -0.0018013616][-0.0018014942 -0.0018015229 -0.0018021894 -0.0018027938 -0.0018029017 -0.001802437 -0.0018015269 -0.0018003983 -0.0017996305 -0.0017994756 -0.0017997617 -0.0018002639 -0.0018008694 -0.0018013363 -0.0018015294][-0.001801715 -0.0018016396 -0.001802237 -0.0018026807 -0.0018025752 -0.001801895 -0.0018008022 -0.0017994815 -0.0017986014 -0.0017984677 -0.0017989018 -0.0017996144 -0.0018004448 -0.0018011375 -0.0018015392][-0.001801881 -0.0018017982 -0.001802305 -0.0018025879 -0.0018023422 -0.0018015552 -0.0018004462 -0.0017991207 -0.0017982123 -0.0017980435 -0.0017985137 -0.0017993116 -0.0018002013 -0.0018009975 -0.0018015524][-0.0018021704 -0.0018019542 -0.0018022989 -0.0018024464 -0.0018021248 -0.0018013342 -0.0018003404 -0.0017991957 -0.0017983904 -0.0017982071 -0.0017986454 -0.0017994141 -0.0018002322 -0.0018009844 -0.0018015858][-0.0018024868 -0.0018021718 -0.0018023489 -0.0018024198 -0.0018020945 -0.0018013839 -0.0018005463 -0.0017996079 -0.0017989231 -0.001798737 -0.0017990757 -0.0017997007 -0.0018003485 -0.0018009804 -0.0018015557][-0.0018029044 -0.001802522 -0.0018025743 -0.0018026368 -0.0018023911 -0.0018018408 -0.0018011953 -0.0018004803 -0.0017999217 -0.0017996954 -0.0017998535 -0.0018002486 -0.0018006852 -0.0018011472 -0.0018016197][-0.0018033575 -0.0018029026 -0.00180286 -0.0018029708 -0.0018028799 -0.0018025611 -0.0018021698 -0.001801741 -0.0018013393 -0.0018010713 -0.0018010354 -0.0018011774 -0.0018013583 -0.0018015925 -0.0018018932][-0.0018038714 -0.0018032609 -0.0018030719 -0.0018031867 -0.001803214 -0.0018031059 -0.0018029489 -0.0018027709 -0.0018025571 -0.0018023337 -0.0018021936 -0.0018021523 -0.0018021486 -0.0018022007 -0.0018023318]]...]
INFO - root - 2017-12-09 14:31:57.896125: step 31110, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 72h:34m:41s remains)
INFO - root - 2017-12-09 14:32:06.478880: step 31120, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.833 sec/batch; 69h:42m:39s remains)
INFO - root - 2017-12-09 14:32:15.304637: step 31130, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.872 sec/batch; 72h:58m:05s remains)
INFO - root - 2017-12-09 14:32:23.907203: step 31140, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.849 sec/batch; 71h:02m:09s remains)
INFO - root - 2017-12-09 14:32:32.638143: step 31150, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 72h:20m:14s remains)
INFO - root - 2017-12-09 14:32:41.287306: step 31160, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.860 sec/batch; 72h:01m:31s remains)
INFO - root - 2017-12-09 14:32:49.651631: step 31170, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 72h:08m:57s remains)
INFO - root - 2017-12-09 14:32:58.199412: step 31180, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.845 sec/batch; 70h:45m:51s remains)
INFO - root - 2017-12-09 14:33:06.901125: step 31190, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.890 sec/batch; 74h:30m:45s remains)
INFO - root - 2017-12-09 14:33:15.398718: step 31200, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.833 sec/batch; 69h:45m:10s remains)
2017-12-09 14:33:16.273468: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0010379895 -0.0010495286 -0.00064871227 0.00025755248 0.0014770989 0.002667787 0.0034090965 0.0035449634 0.0030511781 0.0020932313 0.00095651473 -0.00011005485 -0.00089603959 -0.0013324342 -0.0014954963][-0.0014665439 -0.001066966 -1.4359364e-05 0.001749426 0.0038977447 0.0059113884 0.0071712211 0.0074664615 0.0067447182 0.00526131 0.0033702627 0.0014969254 2.3245811e-05 -0.00085284212 -0.0011952019][-0.0015455085 -0.00066852011 0.0013116793 0.0043949503 0.0080159316 0.011343583 0.013484783 0.014132651 0.013144804 0.010855412 0.0077335043 0.0045171566 0.0018730074 0.00022305653 -0.00043524464][-0.0013584807 2.663536e-05 0.0031389082 0.0079456642 0.013578614 0.018712755 0.022064211 0.023188816 0.021887122 0.018580809 0.013898328 0.00890366 0.0046524103 0.0018837404 0.0007325277][-0.0011054317 0.00073430722 0.0049755336 0.011637337 0.019647861 0.027079148 0.032084618 0.033869822 0.032167088 0.02751971 0.020954624 0.013900226 0.0078389253 0.0037984299 0.0020394805][-0.00077078072 0.001276415 0.006225531 0.014290994 0.024389073 0.034033868 0.040820777 0.043437477 0.041514546 0.035667077 0.027383175 0.018404888 0.010659803 0.0054084756 0.002998794][-0.00035514263 0.0015511337 0.00651896 0.015072493 0.026329059 0.037552435 0.045825988 0.049218666 0.047201734 0.040435012 0.030902991 0.020632271 0.011877894 0.0059400736 0.0031069196][5.1539391e-05 0.0015915813 0.0059396555 0.013936307 0.025057059 0.036695831 0.045703743 0.0496797 0.047808532 0.040761482 0.030787336 0.020167165 0.011264298 0.0052607046 0.0022992222][0.00026442332 0.0013519431 0.0046893847 0.011274186 0.0209736 0.031633437 0.040274866 0.044371933 0.042878821 0.036364108 0.027016593 0.017179225 0.0091003831 0.003740326 0.0010339556][0.00010571664 0.00081194297 0.003094892 0.0078888629 0.015318904 0.023862148 0.031097902 0.03477506 0.033764672 0.028458841 0.020669743 0.012569658 0.0060810251 0.0018974895 -0.00020394241][-0.00041198311 2.4317414e-05 0.0014076085 0.00442941 0.0093192263 0.015189124 0.02037902 0.02320195 0.02267 0.018971816 0.013394942 0.0076480312 0.0031464607 0.00034153776 -0.0010323455][-0.000834735 -0.00072139094 -5.4053264e-05 0.0015463008 0.00425096 0.007623706 0.010741003 0.012568015 0.012397773 0.010249265 0.0068914192 0.00345401 0.00082648068 -0.00074656564 -0.0014770082][-0.00073472026 -0.0010636183 -0.00099184073 -0.00038652064 0.00079648418 0.0023527755 0.0038641691 0.0048227641 0.0048313583 0.0038427203 0.0022246754 0.000575611 -0.00065834867 -0.001372376 -0.0016843232][0.00027157424 -0.000740899 -0.0012689598 -0.0013326518 -0.0010222819 -0.00049669715 6.3632964e-05 0.00046102225 0.00052371307 0.00020113459 -0.00037975912 -0.00097711338 -0.0014200298 -0.0016694129 -0.0017720414][0.002115807 0.00018178171 -0.00099000707 -0.001516458 -0.0016356447 -0.0015608418 -0.0014244207 -0.0013104419 -0.0012759531 -0.001344251 -0.0014825885 -0.0016246054 -0.0017279481 -0.0017834686 -0.0018042623]]...]
INFO - root - 2017-12-09 14:33:24.950268: step 31210, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 72h:20m:09s remains)
INFO - root - 2017-12-09 14:33:33.529553: step 31220, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 72h:35m:01s remains)
INFO - root - 2017-12-09 14:33:42.199894: step 31230, loss = 0.83, batch loss = 0.70 (9.2 examples/sec; 0.865 sec/batch; 72h:24m:31s remains)
INFO - root - 2017-12-09 14:33:50.735482: step 31240, loss = 0.82, batch loss = 0.69 (9.8 examples/sec; 0.819 sec/batch; 68h:32m:32s remains)
INFO - root - 2017-12-09 14:33:59.474710: step 31250, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.866 sec/batch; 72h:28m:05s remains)
INFO - root - 2017-12-09 14:34:08.205568: step 31260, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.883 sec/batch; 73h:53m:30s remains)
INFO - root - 2017-12-09 14:34:16.804119: step 31270, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.856 sec/batch; 71h:36m:30s remains)
INFO - root - 2017-12-09 14:34:25.575869: step 31280, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 72h:43m:25s remains)
INFO - root - 2017-12-09 14:34:34.245266: step 31290, loss = 0.82, batch loss = 0.69 (8.8 examples/sec; 0.906 sec/batch; 75h:48m:04s remains)
INFO - root - 2017-12-09 14:34:42.844205: step 31300, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.838 sec/batch; 70h:07m:28s remains)
2017-12-09 14:34:43.721282: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.21382284 0.20629506 0.19782786 0.18918018 0.181149 0.1738309 0.16719772 0.16219367 0.15811 0.15228102 0.14669725 0.14245735 0.14112887 0.14258873 0.14669281][0.23016086 0.22043672 0.21022911 0.20014076 0.19131307 0.18368018 0.17693155 0.17207576 0.16797473 0.16148351 0.1551429 0.15103233 0.1503202 0.15277168 0.15809359][0.24224573 0.23106118 0.2200173 0.20919356 0.20008324 0.19229418 0.1854372 0.1804041 0.17596862 0.16974396 0.163285 0.15909994 0.15777844 0.15981612 0.16498248][0.25116366 0.23945214 0.22837548 0.21808596 0.20929275 0.20175737 0.19543238 0.19056793 0.18598086 0.1795809 0.17322381 0.16818655 0.16531539 0.1659414 0.16985385][0.25821796 0.246611 0.2356791 0.22636001 0.21890289 0.21250388 0.20737958 0.20334044 0.19901343 0.19276156 0.18609777 0.18016753 0.17556359 0.17384465 0.17556991][0.26568592 0.25458986 0.24384403 0.23542397 0.22942677 0.22502723 0.22186694 0.21862692 0.21445346 0.20865358 0.20164563 0.19363751 0.18639708 0.18213955 0.1814803][0.26993242 0.26008689 0.25061479 0.24386035 0.23961523 0.23723829 0.23619042 0.23421429 0.23085855 0.22533563 0.21786226 0.20829019 0.19891535 0.19208074 0.18883057][0.27479509 0.26701915 0.25846732 0.25319257 0.25019008 0.24900153 0.24896091 0.24781579 0.24496935 0.23962267 0.23205836 0.22179393 0.21137862 0.20202747 0.1961897][0.28329918 0.27734792 0.26948202 0.26500019 0.26270109 0.2618832 0.26217911 0.26127204 0.25867778 0.25335839 0.24568838 0.23529889 0.22470739 0.21413434 0.20678788][0.29095569 0.28727198 0.28026211 0.27607879 0.27415565 0.27346265 0.27359569 0.27252191 0.27011204 0.26528233 0.25751507 0.24737096 0.23694757 0.22604106 0.21787028][0.29853234 0.29707295 0.29086033 0.28694648 0.28472632 0.28353369 0.28280702 0.28108719 0.27809516 0.27316985 0.26571688 0.25604379 0.24613342 0.23569228 0.22734986][0.30238852 0.3031435 0.29781029 0.29407689 0.29157391 0.28957525 0.28780237 0.28560945 0.282406 0.27737647 0.2702001 0.26117644 0.25183359 0.24188577 0.23335627][0.30532575 0.3073836 0.30227283 0.29835132 0.29525182 0.29245287 0.28933394 0.28640884 0.28198504 0.27665398 0.26972574 0.26096505 0.25205994 0.24262512 0.23459573][0.30375528 0.30656534 0.30181247 0.29771769 0.29419914 0.29083833 0.28674108 0.28274167 0.27735594 0.27156195 0.26454785 0.2560406 0.24765109 0.23914868 0.23162809][0.29958856 0.30331776 0.29874322 0.2946986 0.29061598 0.28676471 0.28207067 0.27725324 0.2711423 0.26469311 0.257384 0.24882948 0.24036695 0.23204559 0.22404651]]...]
INFO - root - 2017-12-09 14:34:52.563968: step 31310, loss = 0.82, batch loss = 0.69 (8.8 examples/sec; 0.913 sec/batch; 76h:21m:21s remains)
INFO - root - 2017-12-09 14:35:01.335868: step 31320, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.897 sec/batch; 75h:00m:33s remains)
INFO - root - 2017-12-09 14:35:10.119277: step 31330, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.860 sec/batch; 71h:56m:02s remains)
INFO - root - 2017-12-09 14:35:18.672167: step 31340, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.850 sec/batch; 71h:04m:01s remains)
INFO - root - 2017-12-09 14:35:27.196887: step 31350, loss = 0.83, batch loss = 0.70 (9.1 examples/sec; 0.879 sec/batch; 73h:34m:12s remains)
INFO - root - 2017-12-09 14:35:35.770243: step 31360, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.878 sec/batch; 73h:26m:02s remains)
INFO - root - 2017-12-09 14:35:44.240655: step 31370, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.844 sec/batch; 70h:34m:48s remains)
INFO - root - 2017-12-09 14:35:52.990174: step 31380, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.902 sec/batch; 75h:25m:24s remains)
INFO - root - 2017-12-09 14:36:01.448645: step 31390, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 72h:17m:02s remains)
INFO - root - 2017-12-09 14:36:09.951730: step 31400, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.877 sec/batch; 73h:23m:12s remains)
2017-12-09 14:36:10.735141: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0018009973 -0.0017999356 -0.0018000577 -0.0018002884 -0.001800323 -0.0018000684 -0.0017995946 -0.0017992278 -0.0017990507 -0.0017990582 -0.0017991363 -0.0017993004 -0.0017998662 -0.0018008999 -0.0018022945][-0.001800195 -0.0017991551 -0.0017992508 -0.00179942 -0.0017993753 -0.0017990428 -0.0017985321 -0.0017982011 -0.0017981868 -0.0017984444 -0.0017987941 -0.0017991632 -0.001799746 -0.0018005206 -0.001801373][-0.0018002295 -0.0017992156 -0.0017992115 -0.0017992719 -0.0017991008 -0.0017986953 -0.0017981828 -0.001797953 -0.0017981734 -0.0017987418 -0.0017993731 -0.0017999103 -0.0018004136 -0.0018008574 -0.0018011186][-0.0018002266 -0.0017991868 -0.0017990683 -0.0017989659 -0.0017985948 -0.0017980145 -0.0017974091 -0.0017971991 -0.0017975615 -0.001798325 -0.0017991428 -0.0017997704 -0.0018001975 -0.001800377 -0.0018002286][-0.0018002689 -0.0017990961 -0.0017988378 -0.0017985473 -0.0017979424 -0.0017971342 -0.0017963472 -0.0017960287 -0.0017963734 -0.0017971935 -0.0017980871 -0.0017987664 -0.0017991553 -0.0017992069 -0.0017988168][-0.0018002742 -0.0017989211 -0.0017984894 -0.0017979651 -0.0017970912 -0.0017960466 -0.0017951066 -0.0017947294 -0.0017950936 -0.0017960108 -0.0017969746 -0.0017976754 -0.0017980109 -0.0017979786 -0.0017974743][-0.0018002435 -0.0017987145 -0.0017980756 -0.0017973013 -0.0017961944 -0.0017949704 -0.0017939688 -0.0017936511 -0.0017941267 -0.0017951836 -0.0017962322 -0.001796954 -0.0017972537 -0.0017971918 -0.0017966826][-0.0018001717 -0.0017985025 -0.0017976731 -0.0017967576 -0.0017955547 -0.0017943161 -0.0017934175 -0.0017933274 -0.0017940251 -0.001795257 -0.0017964307 -0.0017972117 -0.0017975159 -0.0017974356 -0.0017969022][-0.0018000576 -0.0017982712 -0.0017974084 -0.0017964072 -0.0017951751 -0.0017939931 -0.0017932921 -0.001793514 -0.0017945304 -0.0017960514 -0.0017974498 -0.0017984021 -0.0017987831 -0.0017986597 -0.0017979919][-0.0017998549 -0.0017981388 -0.0017972809 -0.0017962676 -0.0017950382 -0.0017939026 -0.0017934046 -0.0017939188 -0.001795285 -0.0017971856 -0.0017989005 -0.0018000675 -0.0018005148 -0.0018002687 -0.0017993407][-0.0017998056 -0.0017981556 -0.0017972572 -0.0017963217 -0.0017951709 -0.0017941309 -0.0017937879 -0.0017945197 -0.0017961334 -0.0017982692 -0.0018002175 -0.0018015661 -0.0018020676 -0.0018016995 -0.0018005548][-0.00179978 -0.0017981294 -0.0017972465 -0.001796447 -0.0017954584 -0.0017945576 -0.0017943076 -0.0017951054 -0.0017967877 -0.0017989648 -0.0018009878 -0.0018024081 -0.0018029639 -0.0018025697 -0.0018013101][-0.0017997328 -0.0017980478 -0.0017971961 -0.0017965273 -0.0017957207 -0.0017949941 -0.0017948174 -0.0017956041 -0.0017971741 -0.0017991916 -0.0018011 -0.0018024386 -0.0018030107 -0.0018026851 -0.0018015081][-0.0017997606 -0.0017980804 -0.0017973243 -0.0017968159 -0.0017962076 -0.0017956647 -0.0017955259 -0.0017961751 -0.0017974934 -0.0017991901 -0.0018008018 -0.0018019241 -0.0018023906 -0.0018021298 -0.001801173][-0.001799762 -0.0017981168 -0.0017974412 -0.0017971155 -0.0017966866 -0.0017962724 -0.001796118 -0.0017965512 -0.0017975356 -0.001798786 -0.0017999904 -0.0018008327 -0.0018011696 -0.0018009676 -0.0018002634]]...]
INFO - root - 2017-12-09 14:36:19.332116: step 31410, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.835 sec/batch; 69h:48m:30s remains)
INFO - root - 2017-12-09 14:36:27.941162: step 31420, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.853 sec/batch; 71h:19m:41s remains)
INFO - root - 2017-12-09 14:36:36.561435: step 31430, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.900 sec/batch; 75h:17m:04s remains)
INFO - root - 2017-12-09 14:36:45.223884: step 31440, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.885 sec/batch; 74h:01m:25s remains)
INFO - root - 2017-12-09 14:36:53.897755: step 31450, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.872 sec/batch; 72h:53m:55s remains)
INFO - root - 2017-12-09 14:37:02.614886: step 31460, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.887 sec/batch; 74h:10m:07s remains)
INFO - root - 2017-12-09 14:37:11.130997: step 31470, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.852 sec/batch; 71h:14m:16s remains)
INFO - root - 2017-12-09 14:37:19.774238: step 31480, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.851 sec/batch; 71h:11m:33s remains)
INFO - root - 2017-12-09 14:37:28.421204: step 31490, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 72h:06m:37s remains)
INFO - root - 2017-12-09 14:37:37.168278: step 31500, loss = 0.83, batch loss = 0.70 (9.1 examples/sec; 0.875 sec/batch; 73h:09m:16s remains)
2017-12-09 14:37:38.044961: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0018356773 -0.0018353624 -0.0018355617 -0.0018357701 -0.0018358459 -0.0018358323 -0.0018358877 -0.001835847 -0.0018357148 -0.0018356112 -0.0018354297 -0.0018354019 -0.0018353178 -0.0018349928 -0.0018345666][-0.0018359339 -0.0018356903 -0.0018356923 -0.0018354675 -0.0018347653 -0.0018341357 -0.0018338816 -0.0018339162 -0.0018341548 -0.0018347284 -0.001835305 -0.0018356197 -0.0018357312 -0.0018356405 -0.0018353338][-0.0018360345 -0.0018359097 -0.0018356559 -0.0018342012 -0.0018317682 -0.0018292504 -0.0018273066 -0.0018268502 -0.0018280436 -0.0018301771 -0.0018324796 -0.0018341743 -0.0018353601 -0.0018357664 -0.0018358296][-0.0018369036 -0.0018365458 -0.0018350959 -0.0018315924 -0.001826254 -0.0018205767 -0.0018160108 -0.0018146317 -0.0018167816 -0.0018215699 -0.0018269377 -0.0018310437 -0.0018340042 -0.001835663 -0.0018364895][-0.0018378232 -0.0018367263 -0.001833569 -0.0018271201 -0.0018180307 -0.0018085031 -0.0018012525 -0.0017988541 -0.0018017825 -0.0018092745 -0.0018185562 -0.0018261517 -0.0018315617 -0.0018348121 -0.0018365462][-0.001838828 -0.0018367617 -0.0018316002 -0.0018221142 -0.0018095446 -0.0017968406 -0.0017867816 -0.0017823277 -0.0017847752 -0.0017940212 -0.0018068193 -0.0018183184 -0.0018271897 -0.0018327666 -0.0018360381][-0.0018397786 -0.00183667 -0.0018295984 -0.0018178434 -0.0018027389 -0.0017873492 -0.0017747724 -0.0017681168 -0.0017687788 -0.0017778011 -0.0017926935 -0.0018081904 -0.0018210622 -0.001829243 -0.0018339298][-0.0018405323 -0.0018364047 -0.0018279104 -0.0018144989 -0.0017975371 -0.0017801261 -0.0017655703 -0.0017568169 -0.0017557394 -0.0017636481 -0.0017791389 -0.0017966932 -0.0018126594 -0.0018240872 -0.0018314591][-0.001840653 -0.0018357292 -0.0018265464 -0.0018123779 -0.0017944194 -0.0017757674 -0.0017598076 -0.0017499718 -0.001748046 -0.0017547224 -0.0017690517 -0.0017868126 -0.0018043955 -0.0018184725 -0.0018281282][-0.001839947 -0.0018349509 -0.0018256154 -0.0018111963 -0.0017929844 -0.0017740118 -0.0017581091 -0.0017485521 -0.0017462689 -0.0017519688 -0.001764741 -0.0017814867 -0.0017988485 -0.0018138038 -0.0018249219][-0.0018389638 -0.0018346602 -0.0018264108 -0.0018131767 -0.0017961189 -0.0017781334 -0.0017631351 -0.0017536629 -0.0017510167 -0.0017559975 -0.0017671222 -0.0017822047 -0.0017985171 -0.0018131658 -0.0018240899][-0.0018383941 -0.0018352331 -0.0018286827 -0.001817688 -0.0018033157 -0.0017877865 -0.0017743403 -0.0017658609 -0.0017632747 -0.0017673956 -0.0017771093 -0.0017902544 -0.0018040949 -0.0018165313 -0.001826091][-0.0018387696 -0.001836693 -0.0018319546 -0.0018239021 -0.0018134176 -0.0018018873 -0.0017918457 -0.0017853072 -0.0017833455 -0.0017862888 -0.0017932967 -0.0018032085 -0.0018136299 -0.0018229203 -0.0018301023][-0.0018397836 -0.0018387263 -0.0018358725 -0.0018308272 -0.0018239977 -0.001816135 -0.0018094736 -0.0018051642 -0.0018038758 -0.0018054391 -0.0018095287 -0.001815667 -0.0018227228 -0.0018293264 -0.0018339095][-0.0018409746 -0.0018405556 -0.0018392125 -0.0018366104 -0.0018327298 -0.0018280856 -0.0018237826 -0.0018206063 -0.0018191622 -0.0018197951 -0.0018220118 -0.0018254608 -0.0018296802 -0.0018335955 -0.001836505]]...]
INFO - root - 2017-12-09 14:37:46.718077: step 31510, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.874 sec/batch; 73h:03m:05s remains)
INFO - root - 2017-12-09 14:37:55.460408: step 31520, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.886 sec/batch; 74h:02m:34s remains)
INFO - root - 2017-12-09 14:38:04.142132: step 31530, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.859 sec/batch; 71h:49m:51s remains)
INFO - root - 2017-12-09 14:38:12.829833: step 31540, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 72h:31m:57s remains)
INFO - root - 2017-12-09 14:38:21.465515: step 31550, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.889 sec/batch; 74h:18m:58s remains)
INFO - root - 2017-12-09 14:38:30.379749: step 31560, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.876 sec/batch; 73h:13m:01s remains)
INFO - root - 2017-12-09 14:38:38.853671: step 31570, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.854 sec/batch; 71h:24m:28s remains)
INFO - root - 2017-12-09 14:38:47.597381: step 31580, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.857 sec/batch; 71h:40m:06s remains)
INFO - root - 2017-12-09 14:38:56.079859: step 31590, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.856 sec/batch; 71h:34m:05s remains)
INFO - root - 2017-12-09 14:39:04.875927: step 31600, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.861 sec/batch; 71h:58m:52s remains)
2017-12-09 14:39:05.698490: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0018016449 -0.0017998165 -0.0017989068 -0.0017979078 -0.001796629 -0.001795353 -0.0017936504 -0.0017924797 -0.0017918523 -0.0017926473 -0.0017947687 -0.0017981614 -0.0018017949 -0.0018048952 -0.0018076333][-0.0017996047 -0.0017976703 -0.0017963295 -0.0017949252 -0.0017933288 -0.0017918937 -0.0017904756 -0.0017901507 -0.00179097 -0.0017930839 -0.0017959713 -0.0017995181 -0.0018025291 -0.0018044726 -0.0018058077][-0.0017986149 -0.0017970616 -0.0017956864 -0.0017942367 -0.0017926168 -0.0017911734 -0.0017901311 -0.0017906269 -0.0017925748 -0.001795321 -0.001798245 -0.0018010717 -0.0018027158 -0.0018027528 -0.0018021429][-0.0017978181 -0.001797168 -0.0017963699 -0.0017954982 -0.0017943593 -0.0017932092 -0.0017927979 -0.0017941081 -0.0017966155 -0.0017993591 -0.0018014251 -0.001802619 -0.001802024 -0.0017995391 -0.0017968302][-0.0017979718 -0.0017983426 -0.0017984234 -0.0017984018 -0.0017980458 -0.0017976786 -0.001798241 -0.0018003285 -0.0018031092 -0.0018050773 -0.0018055652 -0.0018044575 -0.0018011559 -0.0017960196 -0.0017910526][-0.0018007606 -0.0018016911 -0.0018024825 -0.0018029824 -0.0018030662 -0.0018032298 -0.0018044749 -0.0018070493 -0.001809789 -0.0018111732 -0.0018102129 -0.0018067412 -0.0018009554 -0.0017934892 -0.0017868106][-0.0018049554 -0.0018060807 -0.0018071983 -0.0018082196 -0.0018086872 -0.0018091616 -0.0018105841 -0.0018131653 -0.0018156212 -0.001816224 -0.0018142125 -0.0018091532 -0.001801642 -0.001792682 -0.0017851145][-0.001809516 -0.0018102961 -0.0018115053 -0.001812698 -0.0018132265 -0.0018139037 -0.0018153861 -0.0018177849 -0.001819517 -0.001819431 -0.001816912 -0.0018109488 -0.0018025588 -0.0017927364 -0.0017846058][-0.001812935 -0.0018133336 -0.001814601 -0.0018159742 -0.0018168008 -0.0018178758 -0.0018192871 -0.0018211607 -0.0018220354 -0.0018213219 -0.0018181871 -0.0018114941 -0.0018023767 -0.0017919492 -0.0017831643][-0.001815449 -0.0018159262 -0.0018175424 -0.0018190823 -0.0018200711 -0.0018211708 -0.0018220809 -0.0018230631 -0.0018230453 -0.0018213278 -0.0018173927 -0.0018100626 -0.0018003559 -0.0017894026 -0.0017799782][-0.0018173221 -0.0018178904 -0.0018195506 -0.0018208198 -0.0018217319 -0.0018224088 -0.0018226251 -0.0018227808 -0.0018216997 -0.0018187759 -0.0018138192 -0.0018058891 -0.0017956164 -0.0017844157 -0.0017752401][-0.0018170932 -0.0018176241 -0.0018190956 -0.0018202467 -0.001821151 -0.001821566 -0.0018211305 -0.0018200349 -0.0018173431 -0.0018127001 -0.0018062207 -0.0017975438 -0.0017871327 -0.0017765135 -0.0017684031][-0.0018149755 -0.0018156467 -0.0018170429 -0.0018183889 -0.0018192641 -0.0018191645 -0.0018176709 -0.0018149939 -0.0018103321 -0.0018038651 -0.0017961718 -0.0017868532 -0.0017768225 -0.0017675182 -0.0017613107][-0.0018110091 -0.0018122905 -0.0018144966 -0.0018164191 -0.0018172596 -0.0018166339 -0.0018140451 -0.0018094734 -0.0018030045 -0.0017949279 -0.0017863425 -0.0017772136 -0.0017682377 -0.0017608422 -0.001757014][-0.0018071424 -0.0018090652 -0.001811704 -0.0018141204 -0.0018149798 -0.0018137989 -0.0018102572 -0.0018045818 -0.0017973012 -0.0017887468 -0.0017802819 -0.0017721017 -0.0017650154 -0.0017599007 -0.0017584417]]...]
INFO - root - 2017-12-09 14:39:14.383004: step 31610, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.848 sec/batch; 70h:51m:29s remains)
INFO - root - 2017-12-09 14:39:22.956766: step 31620, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.852 sec/batch; 71h:11m:53s remains)
INFO - root - 2017-12-09 14:39:31.620167: step 31630, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 72h:27m:26s remains)
INFO - root - 2017-12-09 14:39:40.388707: step 31640, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.873 sec/batch; 72h:55m:08s remains)
INFO - root - 2017-12-09 14:39:49.071751: step 31650, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.886 sec/batch; 74h:02m:06s remains)
INFO - root - 2017-12-09 14:39:58.023348: step 31660, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.872 sec/batch; 72h:50m:02s remains)
INFO - root - 2017-12-09 14:40:06.600371: step 31670, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.860 sec/batch; 71h:52m:24s remains)
INFO - root - 2017-12-09 14:40:15.280913: step 31680, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.870 sec/batch; 72h:40m:03s remains)
INFO - root - 2017-12-09 14:40:23.778603: step 31690, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.846 sec/batch; 70h:39m:08s remains)
INFO - root - 2017-12-09 14:40:32.461995: step 31700, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.891 sec/batch; 74h:27m:41s remains)
2017-12-09 14:40:33.240156: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0017940196 -0.0017931138 -0.0017944206 -0.0017962303 -0.0017982008 -0.0018000023 -0.0018010822 -0.001801601 -0.0018014171 -0.0018004535 -0.0017988562 -0.0017973444 -0.0017963091 -0.0017959881 -0.0017966289][-0.0017933418 -0.0017928758 -0.0017947389 -0.0017971888 -0.0017996894 -0.0018017087 -0.0018028112 -0.0018031874 -0.0018026952 -0.0018012373 -0.0017991133 -0.0017971084 -0.0017955542 -0.0017949064 -0.0017954203][-0.0017936769 -0.0017936778 -0.0017960396 -0.0017990754 -0.0018019762 -0.0018040264 -0.0018050042 -0.0018052093 -0.0018044013 -0.0018025382 -0.0017999496 -0.0017974916 -0.0017954675 -0.0017945045 -0.0017948872][-0.0017939835 -0.0017943827 -0.0017971135 -0.0018004946 -0.0018035648 -0.0018054884 -0.0018062398 -0.0018062226 -0.0018051242 -0.0018030261 -0.0018002645 -0.0017975881 -0.0017953311 -0.0017942002 -0.0017944979][-0.0017941077 -0.0017946698 -0.0017974485 -0.0018008463 -0.0018037881 -0.0018053672 -0.0018057945 -0.0018054987 -0.0018041844 -0.0018020384 -0.0017994833 -0.0017969778 -0.0017948222 -0.0017938055 -0.0017941468][-0.0017937983 -0.001794324 -0.0017969046 -0.0018000893 -0.0018027152 -0.0018039059 -0.0018040064 -0.0018035241 -0.0018021391 -0.001800163 -0.0017980364 -0.001795943 -0.0017940491 -0.0017932581 -0.0017937731][-0.0017932553 -0.0017936161 -0.00179586 -0.0017986567 -0.0018008186 -0.0018015951 -0.0018013915 -0.0018008072 -0.0017995078 -0.0017978142 -0.0017962109 -0.0017945905 -0.0017930806 -0.0017925948 -0.0017932999][-0.0017926692 -0.0017928209 -0.0017946906 -0.0017969781 -0.0017986188 -0.00179903 -0.001798614 -0.0017979773 -0.0017968239 -0.0017954309 -0.0017942841 -0.0017931029 -0.0017919822 -0.0017917594 -0.0017925887][-0.0017920949 -0.0017918977 -0.0017933827 -0.0017951492 -0.0017962342 -0.0017963041 -0.0017957576 -0.0017951519 -0.0017942716 -0.0017932764 -0.0017925977 -0.001791843 -0.0017910884 -0.0017910631 -0.00179192][-0.001791527 -0.0017910973 -0.0017921858 -0.0017934714 -0.0017941494 -0.0017940629 -0.001793522 -0.0017929877 -0.0017924205 -0.0017918583 -0.001791571 -0.0017911843 -0.0017907124 -0.0017907985 -0.0017916047][-0.0017914383 -0.0017908049 -0.0017915352 -0.0017924632 -0.0017929124 -0.0017928192 -0.0017923856 -0.0017919225 -0.0017915588 -0.001791283 -0.0017912209 -0.0017910578 -0.0017908048 -0.0017909647 -0.0017917177][-0.0017917982 -0.0017909521 -0.0017914455 -0.0017920898 -0.001792359 -0.0017922695 -0.001791962 -0.0017915785 -0.0017913032 -0.0017912057 -0.0017912656 -0.0017912295 -0.0017911411 -0.0017913459 -0.0017919873][-0.0017923847 -0.0017913614 -0.0017915796 -0.0017919964 -0.0017921659 -0.0017921248 -0.0017919552 -0.0017916769 -0.0017914529 -0.0017914115 -0.0017914738 -0.001791503 -0.0017915183 -0.0017916978 -0.0017921806][-0.0017932095 -0.0017919657 -0.001791921 -0.0017921415 -0.0017922416 -0.0017922531 -0.0017922103 -0.001792058 -0.0017919129 -0.001791904 -0.001791932 -0.0017919599 -0.0017920063 -0.0017921659 -0.0017925327][-0.0017939421 -0.0017925977 -0.0017923514 -0.0017924616 -0.0017925466 -0.0017926089 -0.0017926372 -0.0017925784 -0.0017924967 -0.0017924774 -0.0017924722 -0.0017924953 -0.0017925691 -0.0017927308 -0.0017930245]]...]
INFO - root - 2017-12-09 14:40:41.931322: step 31710, loss = 0.81, batch loss = 0.68 (9.1 examples/sec; 0.875 sec/batch; 73h:08m:22s remains)
INFO - root - 2017-12-09 14:40:50.647493: step 31720, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.851 sec/batch; 71h:07m:19s remains)
INFO - root - 2017-12-09 14:40:59.252441: step 31730, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.874 sec/batch; 73h:00m:12s remains)
INFO - root - 2017-12-09 14:41:07.912657: step 31740, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.865 sec/batch; 72h:14m:28s remains)
INFO - root - 2017-12-09 14:41:16.735294: step 31750, loss = 0.81, batch loss = 0.68 (9.1 examples/sec; 0.881 sec/batch; 73h:38m:08s remains)
INFO - root - 2017-12-09 14:41:25.367521: step 31760, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.882 sec/batch; 73h:39m:14s remains)
INFO - root - 2017-12-09 14:41:33.919623: step 31770, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.885 sec/batch; 73h:58m:08s remains)
INFO - root - 2017-12-09 14:41:42.586047: step 31780, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.856 sec/batch; 71h:32m:34s remains)
INFO - root - 2017-12-09 14:41:51.243033: step 31790, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 72h:09m:39s remains)
INFO - root - 2017-12-09 14:41:59.848548: step 31800, loss = 0.83, batch loss = 0.70 (9.4 examples/sec; 0.855 sec/batch; 71h:23m:12s remains)
2017-12-09 14:42:00.640912: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.18665671 0.19732188 0.20488845 0.20940934 0.21063186 0.20966066 0.20651965 0.2028372 0.19759142 0.19190237 0.1856989 0.17962985 0.17342994 0.16736044 0.16272914][0.20300747 0.21378058 0.2216979 0.22649871 0.22793618 0.22698729 0.22375366 0.21971354 0.2138309 0.20729169 0.20044319 0.19394106 0.18697961 0.1805252 0.17526366][0.2184324 0.22962925 0.23855533 0.24417521 0.24666651 0.24648859 0.24377069 0.23973364 0.23357671 0.22629434 0.21821713 0.2097152 0.2006817 0.19218621 0.184508][0.23243295 0.24405847 0.25429928 0.26162863 0.26595721 0.26764795 0.2663371 0.26259735 0.25562567 0.24708353 0.23685181 0.22566198 0.2134594 0.20170325 0.19123906][0.24428661 0.25629985 0.26735908 0.27609137 0.28246862 0.28580207 0.28612873 0.2835387 0.276679 0.26685393 0.254436 0.2405331 0.22478949 0.20924668 0.19527623][0.25200653 0.26435119 0.27598122 0.28579322 0.29376069 0.29901314 0.30120316 0.29921189 0.29234266 0.28149581 0.26696986 0.24997047 0.23063342 0.21174413 0.1943529][0.254736 0.26776451 0.27958226 0.28999567 0.29894385 0.3052057 0.3086715 0.30775961 0.30161104 0.29046416 0.27469334 0.25517052 0.23253305 0.20986544 0.18867907][0.25074479 0.26453915 0.27636904 0.28710127 0.29657671 0.30310264 0.30690357 0.30617723 0.30047908 0.28950122 0.27328289 0.25261816 0.22844417 0.20333995 0.17902654][0.24122237 0.255874 0.26745704 0.2781629 0.28750664 0.29366881 0.29734567 0.29655138 0.29139525 0.28083453 0.26486057 0.24426684 0.21968627 0.19364227 0.16765398][0.2274759 0.24200128 0.25292125 0.26326844 0.27215075 0.27755302 0.28065875 0.27992049 0.27550775 0.26580155 0.25091308 0.23147894 0.2080557 0.18271053 0.15623474][0.2103274 0.22376959 0.23337334 0.24306308 0.25113407 0.25617993 0.25916126 0.25842431 0.2546173 0.24628049 0.23341879 0.21575481 0.19402586 0.17027521 0.14485459][0.18984331 0.20194332 0.21021067 0.21880163 0.22593722 0.23012629 0.23272172 0.23295762 0.23065361 0.22425282 0.21392031 0.19945361 0.18069991 0.15927766 0.13590044][0.17025472 0.18032628 0.18664761 0.19370629 0.19957681 0.20307302 0.20507091 0.2053957 0.20346966 0.19898877 0.19153863 0.18053113 0.16598555 0.14880182 0.12978691][0.15404621 0.1621588 0.16640799 0.17167306 0.17595437 0.17891301 0.18071695 0.18146181 0.18047808 0.17732425 0.17201255 0.16383694 0.15300865 0.14028706 0.12629558][0.14267494 0.14899349 0.15150008 0.15501079 0.15768929 0.15990335 0.16141078 0.16231969 0.16174172 0.15980422 0.15646501 0.15078413 0.14309147 0.13426527 0.12487772]]...]
INFO - root - 2017-12-09 14:42:09.511643: step 31810, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 72h:02m:19s remains)
INFO - root - 2017-12-09 14:42:18.205934: step 31820, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.851 sec/batch; 71h:03m:52s remains)
INFO - root - 2017-12-09 14:42:26.891865: step 31830, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.872 sec/batch; 72h:49m:18s remains)
INFO - root - 2017-12-09 14:42:35.560158: step 31840, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.854 sec/batch; 71h:21m:29s remains)
INFO - root - 2017-12-09 14:42:44.359005: step 31850, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.872 sec/batch; 72h:48m:29s remains)
INFO - root - 2017-12-09 14:42:53.093895: step 31860, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.857 sec/batch; 71h:31m:43s remains)
INFO - root - 2017-12-09 14:43:01.561025: step 31870, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.844 sec/batch; 70h:28m:43s remains)
INFO - root - 2017-12-09 14:43:10.270032: step 31880, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.891 sec/batch; 74h:23m:50s remains)
INFO - root - 2017-12-09 14:43:18.803112: step 31890, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.861 sec/batch; 71h:54m:17s remains)
INFO - root - 2017-12-09 14:43:27.598119: step 31900, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 71h:58m:42s remains)
2017-12-09 14:43:28.376279: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.001810155 -0.0018082089 -0.0018076344 -0.0018073519 -0.0018071851 -0.0018070568 -0.0018069366 -0.0018068532 -0.0018067949 -0.0018066114 -0.0018064702 -0.0018066033 -0.0018069946 -0.0018075597 -0.0018081643][-0.0018096451 -0.0018077027 -0.0018071984 -0.0018070261 -0.0018069914 -0.0018069568 -0.0018068836 -0.0018068223 -0.0018068008 -0.0018066671 -0.001806551 -0.0018066646 -0.0018069743 -0.0018074067 -0.0018078868][-0.0018097943 -0.0018079887 -0.0018074942 -0.0018073639 -0.0018074125 -0.001807398 -0.0018072882 -0.0018072056 -0.0018071858 -0.0018071075 -0.0018070391 -0.0018071053 -0.0018073092 -0.0018076051 -0.0018079442][-0.0018099581 -0.0018082212 -0.0018077836 -0.0018076918 -0.0018077751 -0.001807738 -0.0018076213 -0.001807582 -0.0018075255 -0.001807478 -0.0018074632 -0.0018075182 -0.0018076225 -0.0018078123 -0.0018080119][-0.001810136 -0.0018083564 -0.0018079537 -0.0018078643 -0.0018079267 -0.0018078506 -0.0018077183 -0.001807763 -0.0018077965 -0.0018078855 -0.0018080203 -0.0018081431 -0.0018082481 -0.0018084035 -0.0018084757][-0.001810416 -0.0018085407 -0.001808161 -0.0018080741 -0.0018081269 -0.0018079648 -0.0018077438 -0.0018078574 -0.001808095 -0.0018083284 -0.0018086362 -0.0018088699 -0.0018090837 -0.0018092571 -0.0018092429][-0.0018104295 -0.0018085745 -0.0018081758 -0.0018081231 -0.001808156 -0.001807878 -0.0018074564 -0.0018075453 -0.0018080986 -0.0018084975 -0.0018089189 -0.0018092818 -0.0018096959 -0.0018099259 -0.0018098507][-0.0018098716 -0.0018079849 -0.0018075741 -0.0018075443 -0.0018075275 -0.001807122 -0.0018064822 -0.0018065004 -0.0018072916 -0.0018078359 -0.0018082976 -0.0018087855 -0.001809384 -0.0018096938 -0.0018096376][-0.001808629 -0.0018067474 -0.0018064067 -0.0018064022 -0.0018063601 -0.0018058991 -0.0018051331 -0.0018050851 -0.0018059689 -0.0018065725 -0.001807051 -0.0018076266 -0.001808432 -0.0018088791 -0.0018089594][-0.0018073977 -0.0018057151 -0.0018054819 -0.0018054974 -0.0018054277 -0.0018050163 -0.0018043799 -0.0018043347 -0.0018050836 -0.0018055609 -0.0018059391 -0.0018065219 -0.001807378 -0.0018079121 -0.0018081167][-0.0018068277 -0.0018052856 -0.0018050966 -0.0018051456 -0.0018050725 -0.0018047924 -0.0018044361 -0.0018044172 -0.0018048687 -0.0018050502 -0.0018052243 -0.0018056702 -0.001806346 -0.0018068173 -0.0018070788][-0.0018066722 -0.0018051291 -0.0018049974 -0.0018050754 -0.0018050416 -0.0018049243 -0.0018048261 -0.0018048179 -0.0018049689 -0.0018049275 -0.0018049339 -0.0018051632 -0.0018055779 -0.0018058991 -0.0018060873][-0.001806529 -0.0018049488 -0.0018047416 -0.0018048709 -0.0018049238 -0.0018049427 -0.0018049689 -0.0018049631 -0.0018049476 -0.0018048155 -0.0018047498 -0.00180483 -0.0018050366 -0.0018052184 -0.0018053153][-0.0018065746 -0.0018048782 -0.0018046009 -0.0018047644 -0.0018048956 -0.0018050087 -0.0018050713 -0.0018050395 -0.0018049638 -0.0018048222 -0.0018047121 -0.0018047076 -0.0018047504 -0.0018048298 -0.0018048738][-0.0018070143 -0.0018052595 -0.0018048376 -0.0018049497 -0.0018050412 -0.0018051036 -0.0018050947 -0.0018050079 -0.0018048998 -0.0018047652 -0.0018046476 -0.0018045912 -0.0018045658 -0.001804604 -0.0018046682]]...]
INFO - root - 2017-12-09 14:43:37.093723: step 31910, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 72h:34m:22s remains)
INFO - root - 2017-12-09 14:43:45.840018: step 31920, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.854 sec/batch; 71h:17m:47s remains)
INFO - root - 2017-12-09 14:43:54.469915: step 31930, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 72h:45m:29s remains)
INFO - root - 2017-12-09 14:44:03.322759: step 31940, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.892 sec/batch; 74h:26m:22s remains)
INFO - root - 2017-12-09 14:44:12.016621: step 31950, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.836 sec/batch; 69h:49m:03s remains)
INFO - root - 2017-12-09 14:44:20.831660: step 31960, loss = 0.82, batch loss = 0.69 (8.8 examples/sec; 0.905 sec/batch; 75h:31m:21s remains)
INFO - root - 2017-12-09 14:44:29.420259: step 31970, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.873 sec/batch; 72h:54m:50s remains)
INFO - root - 2017-12-09 14:44:37.960910: step 31980, loss = 0.82, batch loss = 0.69 (11.0 examples/sec; 0.726 sec/batch; 60h:37m:11s remains)
INFO - root - 2017-12-09 14:44:46.494857: step 31990, loss = 0.82, batch loss = 0.69 (9.8 examples/sec; 0.815 sec/batch; 67h:59m:44s remains)
INFO - root - 2017-12-09 14:44:55.063120: step 32000, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.878 sec/batch; 73h:18m:44s remains)
2017-12-09 14:44:55.900945: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.46060172 0.45452341 0.44685704 0.43877175 0.43242422 0.42903966 0.42344886 0.41661611 0.41027927 0.40102005 0.39022726 0.38083482 0.3737039 0.36812276 0.36433417][0.46684751 0.46357459 0.45808554 0.45206121 0.44682238 0.44442612 0.44070622 0.4361926 0.43211851 0.42473942 0.41316408 0.40120798 0.39032632 0.38095111 0.37381178][0.46634334 0.4676466 0.46611065 0.46380222 0.46191093 0.46120706 0.45948219 0.45608354 0.45269668 0.44575766 0.43358958 0.41930738 0.40478465 0.39159775 0.3809219][0.4624691 0.47032982 0.47572243 0.47980925 0.48284489 0.48512235 0.48557928 0.48281235 0.47884175 0.47127819 0.45816433 0.44208339 0.42398721 0.40701148 0.39299288][0.45802405 0.47193837 0.483367 0.49407381 0.50325292 0.509928 0.51407909 0.51264441 0.50905573 0.49999335 0.48478815 0.46652892 0.44476476 0.42449307 0.40737438][0.45384115 0.47356039 0.49073675 0.508219 0.52362275 0.53595638 0.54464006 0.54464322 0.54005224 0.52930093 0.51162404 0.48951414 0.4641231 0.44150147 0.42261845][0.4497925 0.47476691 0.49690768 0.52039063 0.54122084 0.557553 0.56888878 0.57024866 0.5650574 0.55311352 0.53452986 0.51104319 0.48333794 0.45880368 0.4385829][0.44466877 0.47440264 0.50055826 0.52753824 0.55185384 0.57133985 0.58487391 0.58701992 0.581011 0.56707686 0.54660231 0.52158451 0.4922452 0.46620136 0.44493985][0.43860286 0.46960086 0.49649265 0.5253666 0.55150634 0.57291043 0.58807 0.592354 0.58721739 0.57307982 0.551897 0.52568668 0.49504021 0.46757567 0.44522202][0.43283412 0.46313551 0.48807877 0.51535451 0.54182523 0.56405717 0.58036393 0.58634925 0.58254141 0.56895417 0.54722905 0.51973408 0.48845917 0.46032572 0.4373773][0.41807455 0.44557926 0.46708554 0.4920623 0.51680875 0.53987157 0.55762917 0.56619 0.56394643 0.55181396 0.53129935 0.50353986 0.47267216 0.44512302 0.42317596][0.40303788 0.42760602 0.44534031 0.46659759 0.488976 0.51052415 0.5276047 0.53815156 0.53750443 0.52713525 0.5084883 0.48276708 0.45413902 0.42745 0.40677738][0.39150447 0.41294712 0.42742562 0.44519421 0.46522415 0.48515895 0.50138742 0.51185632 0.51144761 0.50148278 0.48340085 0.45886439 0.43262863 0.40844107 0.39016038][0.38389158 0.40376151 0.41636875 0.4328571 0.45189708 0.47089791 0.48631632 0.49619755 0.49552962 0.48539555 0.46833226 0.44504541 0.42075303 0.39881438 0.38291654][0.38062462 0.40208378 0.41596186 0.43289095 0.45224693 0.4708553 0.48514274 0.49333885 0.49095035 0.47931132 0.46073321 0.43766776 0.41466728 0.39460763 0.38100994]]...]
INFO - root - 2017-12-09 14:45:04.558212: step 32010, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.873 sec/batch; 72h:53m:18s remains)
INFO - root - 2017-12-09 14:45:13.061357: step 32020, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.844 sec/batch; 70h:24m:39s remains)
INFO - root - 2017-12-09 14:45:21.774946: step 32030, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.870 sec/batch; 72h:38m:37s remains)
INFO - root - 2017-12-09 14:45:30.485706: step 32040, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.901 sec/batch; 75h:14m:11s remains)
INFO - root - 2017-12-09 14:45:39.016891: step 32050, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.863 sec/batch; 72h:02m:26s remains)
INFO - root - 2017-12-09 14:45:47.679642: step 32060, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.893 sec/batch; 74h:32m:19s remains)
INFO - root - 2017-12-09 14:45:56.106325: step 32070, loss = 0.82, batch loss = 0.69 (11.7 examples/sec; 0.685 sec/batch; 57h:11m:53s remains)
INFO - root - 2017-12-09 14:46:04.585730: step 32080, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.855 sec/batch; 71h:22m:12s remains)
INFO - root - 2017-12-09 14:46:13.113761: step 32090, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.856 sec/batch; 71h:28m:13s remains)
INFO - root - 2017-12-09 14:46:21.706126: step 32100, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 72h:22m:02s remains)
2017-12-09 14:46:22.529780: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.14771256 0.15120292 0.15406311 0.15547016 0.15595169 0.15532905 0.15363659 0.15119511 0.14781059 0.14365827 0.1391097 0.13291992 0.12567705 0.11802806 0.11032106][0.15809706 0.16405302 0.16926213 0.17294915 0.17508408 0.17508839 0.17347562 0.17031208 0.16590214 0.16071168 0.1554614 0.14950243 0.14264357 0.13549827 0.12774387][0.16341145 0.17170539 0.17928708 0.18536781 0.18972656 0.19115514 0.1902988 0.18697987 0.18230942 0.17668192 0.17091751 0.16514552 0.15867339 0.15155967 0.14350203][0.16532761 0.17483097 0.18402959 0.19206396 0.19815473 0.20132205 0.20184898 0.19939451 0.19500554 0.1894978 0.18386199 0.17832753 0.17230919 0.16521916 0.15697062][0.16278625 0.17301439 0.18279468 0.19165173 0.19896787 0.20359164 0.2055497 0.20431343 0.20121971 0.1967607 0.19192681 0.18718921 0.18200493 0.17587632 0.16803251][0.15614352 0.16594641 0.17518952 0.18424957 0.19211338 0.19776964 0.20116568 0.2016561 0.20033836 0.19741695 0.19422305 0.19079691 0.18667252 0.1812412 0.17416492][0.14719406 0.15561125 0.16321436 0.17116508 0.17870633 0.18495785 0.18957388 0.19160768 0.19192418 0.19060209 0.18890852 0.1869567 0.18417442 0.17999361 0.17395344][0.13840443 0.14375578 0.14819808 0.15408623 0.16078004 0.16742884 0.17301512 0.17695558 0.17940307 0.17995577 0.17970063 0.17881274 0.17719583 0.17377177 0.16872893][0.13112184 0.13298233 0.13351838 0.1359963 0.14029175 0.14591309 0.15164466 0.15655614 0.1604531 0.16294639 0.16432439 0.16478856 0.1644617 0.16270979 0.15922205][0.12453734 0.12342707 0.1203917 0.11961169 0.12126283 0.12482049 0.12958597 0.13453138 0.13905504 0.14254932 0.14510074 0.14692517 0.14785555 0.14772254 0.14600933][0.11958345 0.11601084 0.10991603 0.10582025 0.10449293 0.1057446 0.10880001 0.11288458 0.11723323 0.12122356 0.12430876 0.12696208 0.12893286 0.12985776 0.12936777][0.11643375 0.11108687 0.10277895 0.0962292 0.092490174 0.091285266 0.092175677 0.094746307 0.098026119 0.10152732 0.10424875 0.10691162 0.10909445 0.11066835 0.11126174][0.11453293 0.10858198 0.09919972 0.091208167 0.08562953 0.0823855 0.081243247 0.081940487 0.08364705 0.085990325 0.0878929 0.089769237 0.091592282 0.092999943 0.094047673][0.11228397 0.10753163 0.098599128 0.090094022 0.083150156 0.077999875 0.074832715 0.073376082 0.073129632 0.073843993 0.074501455 0.075431854 0.076412827 0.077636138 0.078984469][0.10877823 0.10598857 0.098523937 0.090736225 0.083638512 0.077541508 0.072833434 0.069692411 0.067766 0.066835292 0.066216119 0.066023715 0.066154547 0.066528194 0.067409225]]...]
INFO - root - 2017-12-09 14:46:31.030185: step 32110, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.831 sec/batch; 69h:19m:47s remains)
INFO - root - 2017-12-09 14:46:39.665407: step 32120, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.843 sec/batch; 70h:19m:44s remains)
INFO - root - 2017-12-09 14:46:48.415682: step 32130, loss = 0.83, batch loss = 0.70 (9.4 examples/sec; 0.854 sec/batch; 71h:16m:55s remains)
INFO - root - 2017-12-09 14:46:57.112348: step 32140, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.885 sec/batch; 73h:49m:48s remains)
INFO - root - 2017-12-09 14:47:05.799341: step 32150, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.870 sec/batch; 72h:37m:00s remains)
INFO - root - 2017-12-09 14:47:14.479392: step 32160, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.894 sec/batch; 74h:36m:12s remains)
INFO - root - 2017-12-09 14:47:23.150611: step 32170, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.889 sec/batch; 74h:11m:09s remains)
INFO - root - 2017-12-09 14:47:31.712464: step 32180, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.892 sec/batch; 74h:26m:40s remains)
INFO - root - 2017-12-09 14:47:40.385541: step 32190, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.870 sec/batch; 72h:33m:35s remains)
INFO - root - 2017-12-09 14:47:49.072705: step 32200, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.843 sec/batch; 70h:17m:37s remains)
2017-12-09 14:47:49.916869: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.417806 0.41385922 0.40663871 0.39928371 0.39228356 0.38743445 0.38239279 0.37661248 0.37046683 0.36220768 0.35349602 0.345442 0.33740872 0.33013579 0.32551226][0.42012021 0.41825402 0.41279837 0.40764827 0.40294072 0.39960587 0.39587474 0.39147142 0.38668671 0.37876073 0.36891359 0.3587231 0.34847641 0.33911583 0.33240071][0.4171986 0.41794437 0.41500333 0.4125841 0.41086879 0.40954679 0.40719393 0.40338463 0.39912283 0.39087376 0.37997681 0.36784121 0.3555474 0.34423405 0.33550307][0.40980929 0.41379654 0.41497487 0.41646788 0.41799715 0.41960213 0.41910556 0.41586563 0.41130382 0.40272945 0.39089403 0.37754509 0.36343071 0.35047925 0.33988973][0.39959738 0.40634513 0.41086113 0.4169434 0.42245468 0.42705873 0.42892939 0.42698684 0.42239088 0.41337553 0.40028378 0.38614222 0.37049398 0.35594308 0.34406888][0.39016458 0.3988463 0.40537432 0.41445616 0.42289722 0.43047902 0.43508542 0.4347803 0.43069857 0.4216491 0.40766338 0.39219451 0.37525874 0.35981914 0.34693694][0.38055426 0.39087075 0.39842466 0.40912136 0.41968662 0.42924392 0.43565086 0.43668851 0.43312803 0.42446434 0.41106072 0.39539251 0.3785457 0.36302847 0.35013065][0.37065598 0.38271081 0.39092591 0.40167177 0.41197374 0.42202738 0.42900187 0.4300991 0.42642191 0.4181028 0.40600094 0.39176291 0.37638921 0.36179534 0.34955344][0.36106738 0.37374398 0.38127008 0.39195573 0.40177971 0.41120568 0.4181079 0.41975084 0.41657043 0.4088001 0.39790386 0.38502464 0.37082389 0.35738161 0.34611619][0.35663831 0.36964548 0.37538871 0.38391545 0.39190045 0.39987716 0.40559685 0.40684173 0.40386674 0.39665532 0.38687393 0.37487414 0.36230594 0.35054338 0.34073222][0.35095468 0.36354309 0.36729166 0.3733418 0.37935787 0.38591892 0.390465 0.39136419 0.38809928 0.38120958 0.37270817 0.36161464 0.3507612 0.34095281 0.33334488][0.34616345 0.35846668 0.36060214 0.36411807 0.36802152 0.37217084 0.37446308 0.37440675 0.37036324 0.36368057 0.35660365 0.34748533 0.33879271 0.33067971 0.32514089][0.34320885 0.35486051 0.35536462 0.35703152 0.35907069 0.36100903 0.36124402 0.35980913 0.35484818 0.34774795 0.34090811 0.3328658 0.32557136 0.31910717 0.31522167][0.34208018 0.35212049 0.350529 0.35050768 0.35169798 0.35246572 0.35170367 0.3495836 0.34441668 0.33740354 0.33054879 0.32290798 0.31622788 0.31094652 0.30831549][0.33987355 0.34889853 0.34583506 0.34414238 0.34430861 0.34417668 0.34271628 0.34019279 0.33497208 0.32819137 0.32134962 0.31444466 0.30870003 0.3043271 0.30264619]]...]
INFO - root - 2017-12-09 14:47:58.537475: step 32210, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.894 sec/batch; 74h:35m:08s remains)
INFO - root - 2017-12-09 14:48:07.160104: step 32220, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.859 sec/batch; 71h:40m:37s remains)
INFO - root - 2017-12-09 14:48:15.754943: step 32230, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 72h:29m:17s remains)
INFO - root - 2017-12-09 14:48:24.360130: step 32240, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.877 sec/batch; 73h:08m:32s remains)
INFO - root - 2017-12-09 14:48:33.154161: step 32250, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.870 sec/batch; 72h:35m:35s remains)
INFO - root - 2017-12-09 14:48:41.774972: step 32260, loss = 0.82, batch loss = 0.69 (9.8 examples/sec; 0.819 sec/batch; 68h:19m:34s remains)
INFO - root - 2017-12-09 14:48:50.461785: step 32270, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.859 sec/batch; 71h:37m:28s remains)
INFO - root - 2017-12-09 14:48:58.867997: step 32280, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.874 sec/batch; 72h:50m:50s remains)
INFO - root - 2017-12-09 14:49:07.564137: step 32290, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.879 sec/batch; 73h:19m:44s remains)
INFO - root - 2017-12-09 14:49:16.399397: step 32300, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 72h:24m:55s remains)
2017-12-09 14:49:17.302860: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.5358333 0.52294934 0.51150221 0.50533926 0.50199312 0.50012457 0.49771354 0.49335989 0.48447204 0.4696967 0.45133519 0.43336698 0.41405478 0.39146414 0.36563191][0.56681079 0.55542332 0.54345542 0.53616047 0.53150547 0.53003424 0.52797562 0.52457106 0.516301 0.500905 0.48013353 0.45765772 0.43355113 0.40638003 0.37652186][0.59148681 0.58289981 0.5722447 0.56546164 0.560064 0.55792189 0.55497736 0.55071396 0.5412358 0.52501571 0.50252283 0.47648406 0.44855958 0.41763568 0.38439029][0.60987341 0.60606426 0.59870249 0.59355319 0.58864045 0.58658946 0.58326763 0.57775325 0.56597739 0.54844153 0.52417159 0.49576384 0.46504265 0.43147305 0.39595073][0.62153363 0.62346369 0.61958176 0.61589915 0.61186105 0.60966474 0.60572773 0.59864914 0.58484149 0.56611532 0.5409627 0.51161093 0.47939977 0.44429034 0.40743414][0.62086958 0.63029319 0.6308558 0.63013315 0.62753016 0.62542415 0.62089741 0.61275715 0.5982098 0.57880086 0.55375117 0.52521223 0.49360749 0.45883623 0.4214651][0.60979927 0.626943 0.63301361 0.63615048 0.63542449 0.63385797 0.62863052 0.61934322 0.60389221 0.58436114 0.56086642 0.53468794 0.50591922 0.47377306 0.43806329][0.58681309 0.61032492 0.62094319 0.62670332 0.62759817 0.626321 0.62047052 0.61083764 0.59539962 0.57763642 0.55772811 0.53612143 0.51203078 0.48436126 0.4522067][0.55076134 0.5797537 0.59456456 0.60337389 0.60713851 0.60727704 0.60196346 0.59154677 0.57590413 0.55947459 0.54261231 0.52659833 0.508912 0.4878827 0.46120411][0.50630665 0.53760684 0.55383211 0.56477374 0.57109308 0.57320994 0.56985176 0.56036794 0.54694587 0.53341156 0.52090484 0.51062536 0.49938056 0.48560086 0.46461037][0.4543809 0.48519772 0.50182289 0.51462322 0.52276826 0.52636266 0.52477723 0.51806772 0.50844806 0.49905816 0.49218893 0.48904851 0.48540154 0.47896752 0.46387562][0.40634996 0.43536913 0.45126024 0.46411932 0.47357449 0.47854051 0.47942442 0.47496584 0.46831334 0.46324626 0.4614321 0.4640865 0.46631548 0.46628934 0.45719865][0.36757848 0.3931196 0.40723953 0.41968867 0.43007505 0.43647674 0.43934512 0.43699485 0.432608 0.43013853 0.43128428 0.43719095 0.44348565 0.44804835 0.44398949][0.337012 0.35918212 0.37159815 0.38512295 0.39781424 0.40631357 0.41162783 0.41120505 0.40870959 0.40721318 0.40897262 0.41595015 0.42421913 0.4313288 0.43096018][0.3152402 0.33369166 0.34394914 0.3572529 0.37070811 0.38123673 0.38906029 0.39148673 0.39151511 0.39140317 0.39380839 0.40038773 0.40814328 0.41569823 0.41753438]]...]
INFO - root - 2017-12-09 14:49:25.855051: step 32310, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 72h:30m:03s remains)
INFO - root - 2017-12-09 14:49:34.522462: step 32320, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.896 sec/batch; 74h:41m:21s remains)
INFO - root - 2017-12-09 14:49:43.279390: step 32330, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.880 sec/batch; 73h:24m:19s remains)
INFO - root - 2017-12-09 14:49:51.943795: step 32340, loss = 0.82, batch loss = 0.69 (8.8 examples/sec; 0.905 sec/batch; 75h:25m:35s remains)
INFO - root - 2017-12-09 14:50:00.707620: step 32350, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.885 sec/batch; 73h:45m:26s remains)
INFO - root - 2017-12-09 14:50:09.383004: step 32360, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.875 sec/batch; 72h:55m:23s remains)
INFO - root - 2017-12-09 14:50:18.089457: step 32370, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.894 sec/batch; 74h:33m:30s remains)
INFO - root - 2017-12-09 14:50:26.562440: step 32380, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.860 sec/batch; 71h:42m:47s remains)
INFO - root - 2017-12-09 14:50:35.208438: step 32390, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.852 sec/batch; 71h:02m:07s remains)
INFO - root - 2017-12-09 14:50:43.949023: step 32400, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.884 sec/batch; 73h:39m:00s remains)
2017-12-09 14:50:44.778118: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0018272697 -0.0018260988 -0.0018259722 -0.0018265249 -0.0018270748 -0.0018274217 -0.0018269256 -0.0018251925 -0.0018222198 -0.0018182236 -0.0018133422 -0.0018077706 -0.001802333 -0.0017975764 -0.001794096][-0.0018302746 -0.0018293393 -0.0018291381 -0.0018293076 -0.0018292379 -0.0018287539 -0.0018273212 -0.0018246566 -0.0018208058 -0.0018158959 -0.0018103274 -0.0018043747 -0.0017987131 -0.0017939562 -0.0017907335][-0.0018333477 -0.0018328854 -0.0018327426 -0.0018324491 -0.0018318824 -0.0018308932 -0.0018288454 -0.001825484 -0.0018211418 -0.0018158593 -0.0018100226 -0.0018039577 -0.0017982601 -0.0017939314 -0.0017913522][-0.0018354179 -0.0018355096 -0.0018355516 -0.0018350482 -0.0018340531 -0.0018327005 -0.0018304321 -0.0018270452 -0.0018228429 -0.0018179397 -0.001812437 -0.0018064429 -0.0018006298 -0.0017962939 -0.0017939707][-0.0018364579 -0.0018368588 -0.0018371774 -0.0018367108 -0.0018355987 -0.0018340296 -0.0018317049 -0.001828881 -0.0018254229 -0.001821414 -0.0018164429 -0.0018107245 -0.0018049225 -0.0018004891 -0.00179804][-0.0018361498 -0.0018368943 -0.0018375969 -0.0018374278 -0.0018364153 -0.0018349966 -0.0018330025 -0.001830505 -0.0018276748 -0.0018244089 -0.0018201324 -0.0018149184 -0.0018095896 -0.0018055745 -0.0018032257][-0.0018346217 -0.0018357268 -0.0018370316 -0.0018372663 -0.0018365808 -0.0018354553 -0.0018338457 -0.0018315568 -0.0018289228 -0.0018261306 -0.0018227489 -0.0018186655 -0.0018144018 -0.0018110619 -0.0018089663][-0.0018314819 -0.0018326102 -0.001834213 -0.0018348572 -0.0018346321 -0.0018339858 -0.0018328836 -0.0018309803 -0.0018285565 -0.0018261888 -0.0018236539 -0.0018208108 -0.00181796 -0.0018155666 -0.0018137591][-0.001827957 -0.0018289592 -0.001830725 -0.0018316996 -0.0018317987 -0.0018314191 -0.0018306015 -0.001828998 -0.0018269995 -0.001825065 -0.0018230831 -0.001821185 -0.0018196787 -0.0018182769 -0.0018168008][-0.0018239712 -0.0018245908 -0.0018261566 -0.0018271968 -0.0018275438 -0.0018273541 -0.0018266215 -0.0018254395 -0.0018240403 -0.0018226494 -0.0018212888 -0.0018200097 -0.0018193315 -0.0018190182 -0.0018182064][-0.0018202947 -0.0018205381 -0.0018217891 -0.0018226932 -0.0018231391 -0.00182315 -0.0018227327 -0.0018219901 -0.0018210536 -0.0018200962 -0.0018192283 -0.0018183503 -0.0018180395 -0.0018181455 -0.0018179528][-0.0018174882 -0.0018172411 -0.0018181101 -0.0018187471 -0.0018191002 -0.0018191616 -0.0018189839 -0.0018188353 -0.0018182923 -0.001817657 -0.0018172367 -0.0018166485 -0.001816363 -0.0018163532 -0.0018163633][-0.0018160419 -0.0018154131 -0.001815774 -0.0018159916 -0.0018160832 -0.0018160365 -0.0018159883 -0.0018161021 -0.0018158848 -0.0018156172 -0.0018153625 -0.0018148687 -0.0018145073 -0.0018143202 -0.0018143423][-0.0018155143 -0.0018147308 -0.0018148774 -0.0018149724 -0.0018149966 -0.0018149163 -0.0018148714 -0.0018148645 -0.0018146326 -0.0018143054 -0.0018139071 -0.0018134541 -0.0018131403 -0.0018128145 -0.001812786][-0.0018147221 -0.0018140735 -0.0018143024 -0.0018145465 -0.0018146789 -0.0018146751 -0.0018146876 -0.0018146198 -0.0018143279 -0.0018139189 -0.0018134097 -0.0018129515 -0.0018125401 -0.0018120955 -0.0018118877]]...]
INFO - root - 2017-12-09 14:50:52.948013: step 32410, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.852 sec/batch; 71h:01m:17s remains)
INFO - root - 2017-12-09 14:51:01.546464: step 32420, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.863 sec/batch; 71h:57m:48s remains)
INFO - root - 2017-12-09 14:51:10.152909: step 32430, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.882 sec/batch; 73h:32m:00s remains)
INFO - root - 2017-12-09 14:51:18.848856: step 32440, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.847 sec/batch; 70h:37m:25s remains)
INFO - root - 2017-12-09 14:51:27.642336: step 32450, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.898 sec/batch; 74h:52m:44s remains)
INFO - root - 2017-12-09 14:51:36.377121: step 32460, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.879 sec/batch; 73h:14m:03s remains)
INFO - root - 2017-12-09 14:51:44.978888: step 32470, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.841 sec/batch; 70h:05m:58s remains)
INFO - root - 2017-12-09 14:51:53.337135: step 32480, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.875 sec/batch; 72h:56m:33s remains)
INFO - root - 2017-12-09 14:52:02.038796: step 32490, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.898 sec/batch; 74h:49m:34s remains)
INFO - root - 2017-12-09 14:52:10.720933: step 32500, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.892 sec/batch; 74h:22m:00s remains)
2017-12-09 14:52:11.540474: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0018120722 -0.001766466 -0.0016392351 -0.0013871747 -0.0010044584 -0.00055059779 -0.0001289869 0.00014508551 0.00018698687 -7.1702525e-06 -0.00036093453 -0.00076649303 -0.0011294445 -0.0013993047 -0.0015678508][-0.0017888657 -0.0016829293 -0.001426014 -0.00095493515 -0.00026668387 0.00053539861 0.0012805312 0.0017773436 0.0018876832 0.0015938982 0.0010033114 0.00029262935 -0.00037316862 -0.00089120993 -0.0012332657][-0.0017320376 -0.0015027476 -0.00099595217 -0.00013151218 0.0010791981 0.0024630325 0.0037436807 0.0046061156 0.0048236097 0.0043629892 0.0033840542 0.002169217 0.000996881 5.9760176e-05 -0.000576842][-0.0016133578 -0.0011787922 -0.00028797355 0.001154729 0.0031028385 0.0052809557 0.00727425 0.0086242137 0.008998787 0.0083418544 0.0068758675 0.0050039636 0.0031509981 0.0016278048 0.00056032685][-0.0014113202 -0.00069400063 0.00069961173 0.0028722635 0.0057285712 0.0088714007 0.011706304 0.013597358 0.014104709 0.013167987 0.011108231 0.0084763356 0.0058658114 0.0037020682 0.0021641282][-0.001137342 -0.00012254657 0.0017569083 0.0046046469 0.0082741929 0.012264607 0.015825776 0.018190427 0.018840073 0.017697319 0.01515889 0.01188462 0.00861774 0.0058860877 0.0039344593][-0.00087075564 0.00034150656 0.0025124 0.0057467143 0.0098560005 0.01429314 0.018211957 0.020781113 0.021469636 0.020210233 0.017452972 0.01390446 0.010376081 0.0074216328 0.00531345][-0.00071315 0.00051687157 0.002675964 0.0058743763 0.009905451 0.014238004 0.018032789 0.020484785 0.021096563 0.01981885 0.017118676 0.013683837 0.01030835 0.0075209276 0.0055858423][-0.00075447583 0.00029601704 0.0021326528 0.0048708143 0.0083221486 0.012028672 0.015249543 0.017296892 0.017753007 0.016587123 0.014218925 0.011254743 0.0083857235 0.0060671414 0.0045300154][-0.0010062229 -0.00024362269 0.0011003172 0.0031294506 0.0056947451 0.0084422287 0.010799894 0.012256271 0.012503624 0.011536079 0.00968718 0.0074314564 0.0052966345 0.0036210669 0.0025860881][-0.0013580571 -0.00091077038 -0.00010733248 0.0011359464 0.0027271574 0.0044310163 0.0058734971 0.006730109 0.0068073557 0.0061104023 0.0048741805 0.0034177615 0.0020838082 0.001086352 0.00053367007][-0.0016475592 -0.0014458018 -0.0010705697 -0.0004692435 0.000317247 0.0011634968 0.0018733704 0.002272075 0.002261606 0.0018515325 0.0011774384 0.00041841657 -0.00024708244 -0.0007139222 -0.00093330361][-0.0017931249 -0.0017361725 -0.0016182213 -0.0014144642 -0.0011353786 -0.00083241262 -0.00057974283 -0.000446024 -0.0004670138 -0.00063783722 -0.00090084557 -0.001180707 -0.0014140983 -0.0015651773 -0.0016221142][-0.0018217334 -0.001815418 -0.0017972043 -0.0017587431 -0.0016995887 -0.0016314981 -0.0015724777 -0.0015422896 -0.001548865 -0.0015910196 -0.0016532517 -0.0017153008 -0.0017629022 -0.0017903771 -0.0017981271][-0.0018217125 -0.0018210969 -0.0018202736 -0.0018180707 -0.0018143459 -0.0018098349 -0.001805712 -0.0018038602 -0.0018041802 -0.0018067581 -0.0018107209 -0.0018149382 -0.0018182496 -0.0018196516 -0.00182007]]...]
INFO - root - 2017-12-09 14:52:19.861374: step 32510, loss = 0.82, batch loss = 0.69 (9.9 examples/sec; 0.809 sec/batch; 67h:22m:44s remains)
INFO - root - 2017-12-09 14:52:28.408904: step 32520, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.849 sec/batch; 70h:44m:42s remains)
INFO - root - 2017-12-09 14:52:37.083940: step 32530, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.887 sec/batch; 73h:53m:30s remains)
INFO - root - 2017-12-09 14:52:45.760208: step 32540, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.859 sec/batch; 71h:34m:20s remains)
INFO - root - 2017-12-09 14:52:54.482424: step 32550, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.887 sec/batch; 73h:56m:04s remains)
INFO - root - 2017-12-09 14:53:03.065931: step 32560, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.841 sec/batch; 70h:03m:41s remains)
INFO - root - 2017-12-09 14:53:11.733045: step 32570, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.880 sec/batch; 73h:17m:08s remains)
INFO - root - 2017-12-09 14:53:20.180078: step 32580, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.893 sec/batch; 74h:24m:38s remains)
INFO - root - 2017-12-09 14:53:28.906148: step 32590, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.856 sec/batch; 71h:20m:43s remains)
INFO - root - 2017-12-09 14:53:37.443577: step 32600, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 72h:17m:43s remains)
2017-12-09 14:53:38.303502: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.031715337 0.033894237 0.036970582 0.040772 0.044365734 0.047305569 0.049132358 0.049500778 0.049404744 0.048751913 0.048738375 0.048866879 0.048251133 0.045588329 0.040316161][0.027137823 0.030691529 0.035469726 0.040687721 0.044851411 0.047508337 0.048745282 0.048806667 0.048406247 0.047835097 0.047741078 0.047885627 0.047543541 0.045045238 0.039731175][0.019971469 0.024737589 0.03100642 0.037421573 0.042237237 0.045112241 0.046078429 0.045881458 0.045360167 0.044810716 0.044647504 0.044770509 0.044380903 0.042003009 0.03692751][0.01534848 0.021091459 0.028433252 0.03583353 0.041213289 0.044119377 0.044670291 0.043843314 0.04255316 0.04158527 0.041222196 0.041374072 0.041116659 0.039153405 0.034621872][0.013602559 0.02021507 0.028400926 0.036578123 0.042495336 0.045486387 0.045378186 0.043586496 0.041318465 0.039784115 0.039119884 0.039171513 0.038880929 0.037177704 0.033092689][0.013495946 0.020575149 0.029361129 0.038238641 0.0446132 0.047704626 0.0471601 0.044498567 0.041198079 0.03891537 0.037635051 0.037176464 0.036516912 0.03477988 0.030924676][0.013246189 0.020689014 0.029906271 0.039184522 0.045851834 0.048772093 0.047603626 0.0442648 0.040367771 0.037383739 0.035389919 0.033954445 0.032312904 0.029919526 0.025980029][0.011905761 0.01907797 0.027954781 0.036879189 0.043155029 0.045497987 0.04364942 0.039563473 0.0352176 0.031808659 0.029155372 0.026773553 0.024264419 0.021323603 0.017645828][0.0091878017 0.015282878 0.022786228 0.030227909 0.035270285 0.036639042 0.034276456 0.029906319 0.025496865 0.022033207 0.019362172 0.016823741 0.014184122 0.011445529 0.0086376686][0.0055037541 0.0097840065 0.015048907 0.020173855 0.023476327 0.023895366 0.021483889 0.017546322 0.013784035 0.010901436 0.00879645 0.0069042258 0.0050974069 0.0033859226 0.0018677304][0.0020273512 0.0044064363 0.0073737036 0.010241613 0.012020659 0.011993094 0.010182293 0.00743 0.0049140984 0.0031140603 0.0019618333 0.0010497123 0.00026014238 -0.00039618032 -0.00090264931][-0.00045965903 0.00050889375 0.0017668647 0.0030338496 0.0038419904 0.0037914484 0.0028607917 0.0014560162 0.00021481328 -0.00059220532 -0.0010285019 -0.0013185275 -0.0015371398 -0.0016711262 -0.0017302065][-0.0015611723 -0.0013187843 -0.00098617771 -0.00062404457 -0.00036105362 -0.00035245076 -0.00063330075 -0.0010850034 -0.0014738014 -0.0016865757 -0.0017511821 -0.0017632728 -0.0017672005 -0.0017645227 -0.0017598877][-0.0017846439 -0.0017733022 -0.0017557374 -0.0017311862 -0.0017035586 -0.0016869117 -0.0016929735 -0.0017195743 -0.0017458018 -0.0017609699 -0.0017659771 -0.0017680675 -0.001768648 -0.0017646787 -0.0017587021][-0.001794134 -0.0017920713 -0.001789672 -0.0017859999 -0.00178088 -0.0017755035 -0.0017699169 -0.0017664044 -0.0017647761 -0.0017643723 -0.0017658265 -0.0017670543 -0.0017688291 -0.001765984 -0.0017606401]]...]
INFO - root - 2017-12-09 14:53:46.699079: step 32610, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.859 sec/batch; 71h:33m:29s remains)
INFO - root - 2017-12-09 14:53:55.363081: step 32620, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.853 sec/batch; 71h:04m:08s remains)
INFO - root - 2017-12-09 14:54:03.859140: step 32630, loss = 0.82, batch loss = 0.69 (9.7 examples/sec; 0.826 sec/batch; 68h:50m:29s remains)
INFO - root - 2017-12-09 14:54:12.365333: step 32640, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.850 sec/batch; 70h:47m:50s remains)
INFO - root - 2017-12-09 14:54:20.956505: step 32650, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.892 sec/batch; 74h:19m:35s remains)
INFO - root - 2017-12-09 14:54:29.714841: step 32660, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.858 sec/batch; 71h:29m:07s remains)
INFO - root - 2017-12-09 14:54:38.401795: step 32670, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.878 sec/batch; 73h:06m:38s remains)
INFO - root - 2017-12-09 14:54:46.700169: step 32680, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.832 sec/batch; 69h:16m:47s remains)
INFO - root - 2017-12-09 14:54:55.168338: step 32690, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.842 sec/batch; 70h:07m:32s remains)
INFO - root - 2017-12-09 14:55:03.714708: step 32700, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.841 sec/batch; 70h:01m:28s remains)
2017-12-09 14:55:04.563560: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0017904001 -0.0017879191 -0.0017871843 -0.0017868578 -0.0017869184 -0.0017875432 -0.0017887377 -0.0017898839 -0.0017904998 -0.0017903941 -0.0017896753 -0.0017888889 -0.0017887126 -0.0017893175 -0.0017901345][-0.0017886789 -0.0017861086 -0.0017854287 -0.0017853979 -0.0017858944 -0.0017867843 -0.0017879753 -0.0017888676 -0.0017891666 -0.0017888444 -0.0017881757 -0.0017877832 -0.0017880143 -0.0017887021 -0.0017893891][-0.0017881525 -0.0017856704 -0.0017850967 -0.0017853571 -0.0017861953 -0.0017872398 -0.0017882359 -0.0017886532 -0.0017884102 -0.0017878346 -0.0017874743 -0.0017877759 -0.0017884825 -0.0017892504 -0.0017897909][-0.001788042 -0.0017857987 -0.0017853973 -0.001785891 -0.0017867954 -0.0017877152 -0.0017882787 -0.0017880966 -0.0017873255 -0.0017866939 -0.001786859 -0.0017878433 -0.0017889077 -0.0017896547 -0.00179002][-0.0017886409 -0.0017866062 -0.0017863414 -0.0017869376 -0.0017876166 -0.0017880296 -0.0017878616 -0.0017870027 -0.0017859519 -0.0017855921 -0.001786347 -0.0017877389 -0.0017889347 -0.0017896238 -0.0017897973][-0.0017892205 -0.001787386 -0.0017872361 -0.0017877667 -0.0017880511 -0.001787792 -0.0017868662 -0.0017854855 -0.0017845569 -0.001784789 -0.0017860116 -0.0017874958 -0.0017886463 -0.0017892777 -0.0017893459][-0.0017896717 -0.0017880625 -0.0017879508 -0.0017883098 -0.0017880953 -0.0017872654 -0.001785867 -0.0017843512 -0.0017837447 -0.0017845556 -0.0017860908 -0.0017875028 -0.0017885247 -0.0017890835 -0.0017891544][-0.0017901615 -0.001788687 -0.0017885957 -0.0017887403 -0.0017880857 -0.0017868608 -0.0017852949 -0.0017840032 -0.0017838313 -0.0017848705 -0.001786452 -0.001787815 -0.0017887159 -0.0017891693 -0.0017892083][-0.0017904831 -0.0017891089 -0.0017890511 -0.0017889474 -0.0017879278 -0.0017864882 -0.0017849674 -0.0017839856 -0.0017841331 -0.0017852546 -0.0017868753 -0.0017882194 -0.0017890291 -0.0017893522 -0.0017893161][-0.0017904335 -0.0017891704 -0.0017891175 -0.0017888846 -0.0017878053 -0.0017864185 -0.0017850945 -0.0017843597 -0.0017846954 -0.0017858149 -0.0017872966 -0.0017885208 -0.0017892218 -0.00178947 -0.0017893987][-0.0017903829 -0.0017891255 -0.0017890322 -0.0017887841 -0.0017878443 -0.0017866837 -0.0017856008 -0.0017850006 -0.0017853488 -0.0017863942 -0.0017876046 -0.0017885814 -0.0017891623 -0.0017894091 -0.001789373][-0.0017905174 -0.0017891461 -0.0017890364 -0.0017888977 -0.0017882478 -0.0017873894 -0.0017865378 -0.0017860223 -0.0017862881 -0.0017870719 -0.0017879007 -0.0017885847 -0.0017890384 -0.0017893028 -0.0017893201][-0.0017906949 -0.0017892441 -0.0017890877 -0.0017890594 -0.0017887059 -0.0017881088 -0.001787413 -0.001787024 -0.001787212 -0.0017877097 -0.0017882033 -0.0017886431 -0.0017889843 -0.0017892265 -0.0017892505][-0.0017908543 -0.0017893014 -0.001789074 -0.0017890946 -0.001788968 -0.0017886224 -0.0017881537 -0.0017879349 -0.0017880833 -0.0017883708 -0.0017886191 -0.0017888464 -0.0017890469 -0.001789199 -0.0017891986][-0.0017910316 -0.0017894243 -0.0017890992 -0.0017891653 -0.0017891726 -0.0017890306 -0.0017887941 -0.0017887213 -0.0017888297 -0.0017889532 -0.0017890177 -0.0017890669 -0.0017891298 -0.0017891781 -0.0017891573]]...]
INFO - root - 2017-12-09 14:55:13.090716: step 32710, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.883 sec/batch; 73h:34m:11s remains)
INFO - root - 2017-12-09 14:55:21.731465: step 32720, loss = 0.82, batch loss = 0.70 (9.3 examples/sec; 0.856 sec/batch; 71h:18m:40s remains)
INFO - root - 2017-12-09 14:55:30.242074: step 32730, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.852 sec/batch; 70h:57m:11s remains)
INFO - root - 2017-12-09 14:55:38.908583: step 32740, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.854 sec/batch; 71h:07m:38s remains)
INFO - root - 2017-12-09 14:55:47.577771: step 32750, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 71h:56m:20s remains)
INFO - root - 2017-12-09 14:55:56.294671: step 32760, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.888 sec/batch; 73h:55m:23s remains)
INFO - root - 2017-12-09 14:56:05.128695: step 32770, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.890 sec/batch; 74h:04m:41s remains)
INFO - root - 2017-12-09 14:56:13.401098: step 32780, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.857 sec/batch; 71h:19m:40s remains)
INFO - root - 2017-12-09 14:56:22.048686: step 32790, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.860 sec/batch; 71h:37m:15s remains)
INFO - root - 2017-12-09 14:56:30.695938: step 32800, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.852 sec/batch; 70h:54m:24s remains)
2017-12-09 14:56:31.626614: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.025544289 0.02751237 0.029428167 0.031211765 0.032326512 0.034360487 0.038810156 0.045950551 0.054984663 0.065209992 0.074175775 0.082998477 0.089974567 0.096820742 0.1026568][0.025607796 0.028080555 0.030786613 0.034158766 0.037043504 0.041092698 0.047144614 0.054886974 0.063879915 0.072646849 0.080015264 0.086537085 0.0914706 0.09637554 0.10052069][0.025971087 0.02911685 0.033064742 0.038488213 0.044070393 0.050816428 0.058848388 0.06743782 0.075691223 0.082326122 0.087228611 0.090841264 0.093311556 0.096085481 0.098572731][0.030149331 0.034545232 0.040038884 0.047734618 0.055785507 0.064431727 0.073501542 0.081977889 0.088329338 0.092469275 0.094924018 0.095976 0.0964101 0.097003318 0.097878411][0.040002979 0.045711264 0.052815042 0.0620594 0.071388043 0.08067368 0.089306556 0.096291125 0.1004119 0.10211405 0.10229257 0.1015467 0.10041422 0.0996222 0.099001884][0.056067791 0.062580712 0.070376679 0.080153309 0.089064941 0.097411565 0.10428821 0.10947245 0.1118088 0.11121677 0.10981858 0.10734141 0.10481597 0.10237978 0.10035561][0.0753039 0.082411095 0.090196185 0.099200688 0.1068294 0.11318097 0.11766394 0.12033588 0.12042909 0.11825477 0.11592291 0.11284586 0.10987312 0.10660388 0.10344505][0.092826515 0.10077535 0.10805965 0.11578913 0.12161328 0.12563077 0.12776092 0.12853287 0.12730122 0.12419672 0.12136327 0.11832017 0.1147827 0.11091971 0.10656308][0.10484982 0.11322182 0.12007033 0.12688909 0.13147782 0.13404751 0.13467947 0.1336945 0.13140403 0.12756023 0.12434097 0.12138897 0.11812713 0.1143555 0.10946868][0.11151692 0.11925275 0.12503316 0.13079047 0.13431062 0.13604504 0.13591838 0.13449362 0.13198893 0.12818702 0.125178 0.12271993 0.12009447 0.11669821 0.11210696][0.11260785 0.11886574 0.12302111 0.12763599 0.12998417 0.13132629 0.13111548 0.13012108 0.12821224 0.12529489 0.12325247 0.12174544 0.11987185 0.11719459 0.11329742][0.10967464 0.11475706 0.11727242 0.12013468 0.12122634 0.12167674 0.12076032 0.12019614 0.11921079 0.11784688 0.11725014 0.11711865 0.11655471 0.11479317 0.11209583][0.10222054 0.10625704 0.10809924 0.1094903 0.1094663 0.10920192 0.10786978 0.10712854 0.10631998 0.10658422 0.10746358 0.10867657 0.10956633 0.10945516 0.10841378][0.093531184 0.095971972 0.096849538 0.097666919 0.097138017 0.096327417 0.094734579 0.09398485 0.093370333 0.094346315 0.0957703 0.098144963 0.10038652 0.10217832 0.10350282][0.084954493 0.08616361 0.086153589 0.086361654 0.085752569 0.0850753 0.083863705 0.083498307 0.083040774 0.0843035 0.085880741 0.088229209 0.090644129 0.093466707 0.096341617]]...]
INFO - root - 2017-12-09 14:56:40.169769: step 32810, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.858 sec/batch; 71h:24m:15s remains)
INFO - root - 2017-12-09 14:56:48.920053: step 32820, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.892 sec/batch; 74h:15m:41s remains)
INFO - root - 2017-12-09 14:56:57.951324: step 32830, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 72h:28m:14s remains)
INFO - root - 2017-12-09 14:57:06.770762: step 32840, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.863 sec/batch; 71h:48m:16s remains)
INFO - root - 2017-12-09 14:57:15.477244: step 32850, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.870 sec/batch; 72h:23m:59s remains)
INFO - root - 2017-12-09 14:57:24.221368: step 32860, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.874 sec/batch; 72h:43m:43s remains)
INFO - root - 2017-12-09 14:57:33.009233: step 32870, loss = 0.82, batch loss = 0.69 (8.8 examples/sec; 0.913 sec/batch; 76h:00m:42s remains)
INFO - root - 2017-12-09 14:57:41.492831: step 32880, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.897 sec/batch; 74h:40m:50s remains)
INFO - root - 2017-12-09 14:57:50.223717: step 32890, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.876 sec/batch; 72h:56m:08s remains)
INFO - root - 2017-12-09 14:57:58.962958: step 32900, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.877 sec/batch; 72h:57m:13s remains)
2017-12-09 14:57:59.887277: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.059108317 0.058310177 0.058169838 0.059123188 0.061136987 0.063737556 0.066116989 0.068053789 0.069065668 0.068630591 0.066608325 0.063093178 0.058593348 0.05271998 0.045054112][0.066280171 0.066449367 0.066792011 0.067730188 0.069471575 0.07156536 0.073410742 0.07491675 0.075481117 0.0748742 0.07270024 0.068958975 0.064441495 0.058510173 0.050845921][0.067433111 0.068205267 0.068794355 0.0698319 0.071712725 0.073837683 0.075673215 0.077214234 0.077766307 0.07700482 0.074476644 0.070550784 0.066112354 0.060329527 0.052996188][0.065311775 0.066704944 0.067902468 0.069549263 0.071910009 0.074453644 0.076718621 0.078558348 0.079157785 0.078163721 0.07513392 0.07084097 0.066179395 0.060512073 0.053551156][0.060419582 0.063380018 0.066351093 0.069705494 0.073484764 0.077106863 0.080048449 0.082191758 0.0827501 0.081307277 0.077383675 0.072082207 0.066501774 0.060282569 0.053205818][0.052223265 0.057690419 0.063577726 0.069700092 0.07570333 0.080854751 0.084514454 0.086813889 0.086848624 0.084484138 0.079375923 0.072815478 0.065935969 0.058872331 0.051424049][0.041283753 0.0489164 0.057449229 0.066446938 0.074920341 0.08172217 0.086117879 0.088522151 0.087963641 0.084716968 0.078622743 0.071083225 0.063151009 0.055319335 0.047697913][0.028781554 0.037265606 0.047305871 0.058098331 0.06823609 0.076637484 0.082188986 0.085037366 0.084259748 0.080633707 0.074175939 0.066250138 0.0579047 0.049825042 0.042436343][0.017379882 0.025005611 0.034624543 0.045317706 0.05567012 0.064645313 0.0709926 0.0745676 0.07433112 0.071167454 0.065366976 0.058107439 0.050240647 0.042667545 0.036103897][0.0095334258 0.015284047 0.023227906 0.032310884 0.041494172 0.049783837 0.055997912 0.059724815 0.060003869 0.057806525 0.053370867 0.04762027 0.041216757 0.034977615 0.029684916][0.0054611638 0.0093338313 0.015175325 0.022164553 0.029572945 0.036449958 0.0417539 0.044885743 0.045125313 0.043448333 0.040250067 0.036227334 0.031712543 0.027316349 0.023687018][0.003562436 0.0060006459 0.010014461 0.014952018 0.020410746 0.025657116 0.029863987 0.032248061 0.0322458 0.030784797 0.028401831 0.025642591 0.022724815 0.020063467 0.018034661][0.0020974418 0.0036367043 0.0062441989 0.0096138455 0.013499599 0.017423041 0.020775897 0.022780245 0.023019709 0.022126779 0.020662107 0.019014575 0.017243551 0.015598279 0.014416919][0.0014795283 0.0023240861 0.0038717291 0.0059703817 0.0085593695 0.011371856 0.014008434 0.015858848 0.016621485 0.016645366 0.016315334 0.015786339 0.01504122 0.014196952 0.013438772][0.0019429239 0.0023481487 0.0031576892 0.004347818 0.0059224791 0.0077252933 0.0095461132 0.011040025 0.012063624 0.012821801 0.013485027 0.01398597 0.014208823 0.014112732 0.013674669]]...]
INFO - root - 2017-12-09 14:58:08.548104: step 32910, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 72h:20m:38s remains)
INFO - root - 2017-12-09 14:58:17.370980: step 32920, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.852 sec/batch; 70h:56m:09s remains)
INFO - root - 2017-12-09 14:58:26.035475: step 32930, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.853 sec/batch; 70h:56m:39s remains)
INFO - root - 2017-12-09 14:58:34.682475: step 32940, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 72h:12m:18s remains)
INFO - root - 2017-12-09 14:58:43.453813: step 32950, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.872 sec/batch; 72h:32m:55s remains)
INFO - root - 2017-12-09 14:58:52.114246: step 32960, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.858 sec/batch; 71h:24m:31s remains)
INFO - root - 2017-12-09 14:59:00.921764: step 32970, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.878 sec/batch; 73h:05m:26s remains)
INFO - root - 2017-12-09 14:59:09.350370: step 32980, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.833 sec/batch; 69h:17m:14s remains)
INFO - root - 2017-12-09 14:59:17.972606: step 32990, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 71h:43m:39s remains)
INFO - root - 2017-12-09 14:59:26.632384: step 33000, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.856 sec/batch; 71h:15m:01s remains)
2017-12-09 14:59:27.584036: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.001799011 -0.0017707181 -0.0016947442 -0.0015280801 -0.0012519553 -0.00089274143 -0.00051752734 -0.00024576287 -0.0002306588 -0.00050601084 -0.00088938832 -0.0012242669 -0.0015011847 -0.0017082253 -0.0017966395][-0.0017728503 -0.0017060353 -0.0015188918 -0.0011340699 -0.00053073885 0.00021428696 0.00090866175 0.0012721837 0.0010401948 0.00025391451 -0.00062817056 -0.0012244361 -0.001525881 -0.0017165081 -0.0017983747][-0.001474316 -0.0014203963 -0.001037335 -0.00019750278 0.0010979403 0.0026458073 0.0040270584 0.0046006488 0.003902962 0.0021318309 0.00023345009 -0.0010177853 -0.0015488393 -0.001733743 -0.0017990302][-0.00082346273 -0.000557743 0.00029241515 0.0019743564 0.0044910773 0.0074303178 0.0099063087 0.010701152 0.0091480194 0.0057459828 0.0021371404 -0.0003118735 -0.0014093934 -0.0017284882 -0.0017992612][0.00039164477 0.0013450802 0.0032973457 0.0065230867 0.010862689 0.015525303 0.019174615 0.020054223 0.017307911 0.011822013 0.0059461007 0.0016545471 -0.00059282477 -0.0014557121 -0.0017420958][0.0020870958 0.0040210853 0.007526123 0.012814012 0.019444328 0.026161661 0.03105315 0.031949122 0.028020682 0.020514749 0.012237748 0.0056177657 0.0014751052 -0.00061812426 -0.0015149191][0.0033363737 0.0062786518 0.01137006 0.018747579 0.027568534 0.036078945 0.042004913 0.043084446 0.038523328 0.029685451 0.019562501 0.010770876 0.0044704112 0.00069180608 -0.0011498764][0.0041443552 0.0075912094 0.013466886 0.022026567 0.032150786 0.041869372 0.048662867 0.0503013 0.045943547 0.03674072 0.02560853 0.015232268 0.0071574831 0.0018895954 -0.000820503][0.0037875921 0.0072168382 0.012941754 0.021323724 0.031414177 0.041313406 0.048454884 0.050700359 0.047109783 0.038475305 0.027453039 0.016670462 0.0079874108 0.0022206767 -0.00074207573][0.0025494178 0.00542866 0.010281444 0.017558774 0.02649097 0.035573054 0.042516019 0.045306873 0.042785741 0.035316959 0.025228739 0.015082228 0.0069445949 0.0016832411 -0.000913205][0.000955087 0.0029996852 0.0065418845 0.012043941 0.019078167 0.0265678 0.032652471 0.035555117 0.034051895 0.028169194 0.019787181 0.011280637 0.0046714032 0.00064804952 -0.0012040809][-0.00035332516 0.00088446715 0.0031257584 0.0067671752 0.011670753 0.017200062 0.022024812 0.024659444 0.023925511 0.019674066 0.013359323 0.0069960328 0.0022737458 -0.00037435128 -0.0014763521][-0.0011729267 -0.00054791593 0.00063670205 0.0026812758 0.005625423 0.0091657154 0.012490012 0.014522406 0.014294642 0.011587103 0.0074040145 0.0032575694 0.00035065424 -0.0011245606 -0.0016629618][-0.001594597 -0.0013387986 -0.00081549224 0.00015318941 0.0016729875 0.0036402429 0.0056284703 0.0069574006 0.0069593061 0.0054468568 0.0030303588 0.00068761187 -0.00085454667 -0.001548263 -0.0017587178][-0.0017526736 -0.001674918 -0.0014927245 -0.0011131883 -0.00045014883 0.00048452581 0.0015025797 0.0022340082 0.0022925152 0.0015594509 0.00036379078 -0.00076020823 -0.0014526322 -0.001726119 -0.0017925829]]...]
INFO - root - 2017-12-09 14:59:35.984768: step 33010, loss = 0.83, batch loss = 0.70 (9.7 examples/sec; 0.827 sec/batch; 68h:48m:39s remains)
INFO - root - 2017-12-09 14:59:44.494573: step 33020, loss = 0.81, batch loss = 0.68 (9.5 examples/sec; 0.841 sec/batch; 69h:56m:27s remains)
INFO - root - 2017-12-09 14:59:53.032662: step 33030, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.883 sec/batch; 73h:26m:49s remains)
INFO - root - 2017-12-09 15:00:01.687891: step 33040, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.884 sec/batch; 73h:30m:39s remains)
INFO - root - 2017-12-09 15:00:10.475791: step 33050, loss = 0.83, batch loss = 0.70 (9.6 examples/sec; 0.836 sec/batch; 69h:30m:39s remains)
INFO - root - 2017-12-09 15:00:19.208502: step 33060, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.876 sec/batch; 72h:51m:52s remains)
INFO - root - 2017-12-09 15:00:27.915637: step 33070, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.865 sec/batch; 71h:58m:30s remains)
INFO - root - 2017-12-09 15:00:36.482288: step 33080, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.878 sec/batch; 73h:02m:13s remains)
INFO - root - 2017-12-09 15:00:45.234690: step 33090, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.850 sec/batch; 70h:40m:40s remains)
INFO - root - 2017-12-09 15:00:53.829148: step 33100, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.873 sec/batch; 72h:35m:20s remains)
2017-12-09 15:00:54.721004: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.041256797 0.045599941 0.048817616 0.050610371 0.050392844 0.048476879 0.045516197 0.042245496 0.038656212 0.035225108 0.032520581 0.030348053 0.028528739 0.025850775 0.022016441][0.045135505 0.050024886 0.053731613 0.056004606 0.056355558 0.05493195 0.052433304 0.049514305 0.046250109 0.043245662 0.04092522 0.038981028 0.037043419 0.033883832 0.029010084][0.047404751 0.053387772 0.058149107 0.061298102 0.062221929 0.061073065 0.058569532 0.055630535 0.052497953 0.049902149 0.048128854 0.046712711 0.045047361 0.041852806 0.036227729][0.0492189 0.056090176 0.061570227 0.065374553 0.066801563 0.066146046 0.064173236 0.061468996 0.058474842 0.055889413 0.054438543 0.053438194 0.052198645 0.049065989 0.043093134][0.052446108 0.059912812 0.065564014 0.0694169 0.070854358 0.070485756 0.069124408 0.067304417 0.065070488 0.062756263 0.061214425 0.060040008 0.058585789 0.055373259 0.0489868][0.056755859 0.064973451 0.07072749 0.074211463 0.075266585 0.0749912 0.074219316 0.073194467 0.071741141 0.070112862 0.068786822 0.0673588 0.0653038 0.061101779 0.053777408][0.061068162 0.069781929 0.075333223 0.078263208 0.079007477 0.078796253 0.078576073 0.078170128 0.077436075 0.076385394 0.075385235 0.074028574 0.0716085 0.066771828 0.058751248][0.064943828 0.073570006 0.078685604 0.081421718 0.082100168 0.082071416 0.082156211 0.082478248 0.08243975 0.08186952 0.080810688 0.079019018 0.075999632 0.070949785 0.062815242][0.068120286 0.076574221 0.081259452 0.083699837 0.084359124 0.0845466 0.085018545 0.085874282 0.086290829 0.085902825 0.084723629 0.082739949 0.079350844 0.07383655 0.0656317][0.07024198 0.078710556 0.0832322 0.085475169 0.085877873 0.085674964 0.086080767 0.087138325 0.087998219 0.087976627 0.08676815 0.084480077 0.0805897 0.074981764 0.067370892][0.071018316 0.0800279 0.084831424 0.086754635 0.086588018 0.085904583 0.085919723 0.086856484 0.088062465 0.088730536 0.08816459 0.086154856 0.082358971 0.076873638 0.069825508][0.070983581 0.080198325 0.085041367 0.086747609 0.086244546 0.084916785 0.084639944 0.0858462 0.087547608 0.088741876 0.088532194 0.087007761 0.083881415 0.079208747 0.0730315][0.069990531 0.079160191 0.08371532 0.08505784 0.084327191 0.082690544 0.0822799 0.083562829 0.085622579 0.087482564 0.087978356 0.087019034 0.084652044 0.080962338 0.075769879][0.066415 0.07548257 0.079919249 0.081003182 0.0801865 0.07868398 0.078505211 0.079961754 0.082138531 0.084001862 0.0847424 0.084744968 0.0835202 0.080937788 0.076881915][0.061283551 0.069922023 0.074137211 0.075207286 0.074548379 0.073341407 0.073499314 0.075058326 0.077193245 0.079076424 0.080008194 0.080439545 0.080101535 0.078664809 0.075664133]]...]
INFO - root - 2017-12-09 15:01:03.104651: step 33110, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.883 sec/batch; 73h:24m:35s remains)
INFO - root - 2017-12-09 15:01:11.932356: step 33120, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 72h:03m:34s remains)
INFO - root - 2017-12-09 15:01:20.593845: step 33130, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.837 sec/batch; 69h:38m:27s remains)
INFO - root - 2017-12-09 15:01:29.283753: step 33140, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.856 sec/batch; 71h:12m:53s remains)
INFO - root - 2017-12-09 15:01:37.970434: step 33150, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.845 sec/batch; 70h:17m:47s remains)
INFO - root - 2017-12-09 15:01:46.622472: step 33160, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 72h:07m:28s remains)
INFO - root - 2017-12-09 15:01:55.367293: step 33170, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.853 sec/batch; 70h:54m:24s remains)
INFO - root - 2017-12-09 15:02:03.766670: step 33180, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.846 sec/batch; 70h:19m:18s remains)
INFO - root - 2017-12-09 15:02:12.480741: step 33190, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.883 sec/batch; 73h:26m:35s remains)
INFO - root - 2017-12-09 15:02:21.212364: step 33200, loss = 0.82, batch loss = 0.69 (8.8 examples/sec; 0.909 sec/batch; 75h:33m:38s remains)
2017-12-09 15:02:22.100155: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0018403525 -0.0018405324 -0.0018407928 -0.001840823 -0.0018408408 -0.0018409616 -0.0018409863 -0.0018411409 -0.0018414541 -0.0018418276 -0.0018421396 -0.0018423657 -0.0018420686 -0.0018414288 -0.0018401771][-0.0018400952 -0.0018404954 -0.0018410268 -0.0018413831 -0.0018416614 -0.0018419455 -0.0018420024 -0.0018420389 -0.0018422232 -0.0018423439 -0.0018423547 -0.0018423835 -0.0018421341 -0.0018418593 -0.001841108][-0.0018399369 -0.0018405393 -0.0018412928 -0.0018418775 -0.0018423459 -0.0018427196 -0.001842736 -0.0018425987 -0.0018426209 -0.001842447 -0.0018422781 -0.0018422627 -0.0018422286 -0.0018423918 -0.0018421451][-0.0018398177 -0.0018406428 -0.0018415063 -0.0018420859 -0.0018425179 -0.0018427572 -0.0018426899 -0.0018425174 -0.0018424673 -0.0018421904 -0.0018420551 -0.0018422819 -0.0018424414 -0.0018427792 -0.0018427337][-0.0018398343 -0.0018406982 -0.0018415938 -0.0018421825 -0.0018425483 -0.0018425303 -0.0018421907 -0.0018419635 -0.00184202 -0.0018419443 -0.0018420413 -0.0018425032 -0.0018427509 -0.0018430218 -0.0018429186][-0.001840027 -0.0018408551 -0.0018417927 -0.001842381 -0.0018425767 -0.0018420194 -0.0018410956 -0.0018406959 -0.001841007 -0.0018414251 -0.0018419402 -0.0018427043 -0.0018431025 -0.0018433003 -0.0018431656][-0.0018401908 -0.0018408761 -0.0018417966 -0.0018422813 -0.0018421405 -0.00184086 -0.0018390332 -0.001838387 -0.0018393443 -0.0018405305 -0.0018415243 -0.0018426529 -0.0018432864 -0.0018434471 -0.0018432239][-0.0018401241 -0.0018405475 -0.0018412787 -0.0018415905 -0.0018411112 -0.0018392097 -0.0018363885 -0.0018356374 -0.0018376717 -0.0018397861 -0.0018412857 -0.0018426464 -0.0018433844 -0.0018433622 -0.0018427673][-0.0018400002 -0.0018400922 -0.0018406112 -0.0018408273 -0.0018403362 -0.0018385265 -0.0018357888 -0.0018351303 -0.0018376401 -0.0018400956 -0.0018417656 -0.0018430596 -0.001843711 -0.0018432923 -0.0018421011][-0.0018400736 -0.0018399345 -0.0018403038 -0.0018406215 -0.0018405 -0.0018394334 -0.0018378358 -0.001837528 -0.0018393119 -0.0018412094 -0.0018425731 -0.0018436798 -0.0018441316 -0.0018432465 -0.0018410695][-0.001840352 -0.0018400806 -0.0018403569 -0.0018407538 -0.0018409276 -0.0018405078 -0.0018396797 -0.0018395655 -0.0018406827 -0.0018419966 -0.0018429285 -0.0018437338 -0.0018440321 -0.001842957 -0.0018399865][-0.0018407041 -0.0018403148 -0.0018404049 -0.0018406464 -0.0018408572 -0.0018407615 -0.0018404389 -0.0018404939 -0.0018412177 -0.0018420491 -0.0018427632 -0.0018433725 -0.0018435112 -0.0018422523 -0.0018387806][-0.0018413959 -0.0018409209 -0.0018408378 -0.001840808 -0.0018408146 -0.0018407631 -0.0018407189 -0.0018409453 -0.0018415176 -0.0018421818 -0.0018426583 -0.0018429884 -0.0018429917 -0.0018417181 -0.0018382593][-0.0018421949 -0.00184166 -0.0018414712 -0.0018412708 -0.0018410682 -0.0018409385 -0.0018408907 -0.0018410486 -0.0018414508 -0.0018419678 -0.0018422994 -0.0018424934 -0.0018424942 -0.0018416285 -0.0018389256][-0.0018421997 -0.0018417088 -0.0018415721 -0.001841311 -0.0018410987 -0.0018408675 -0.0018407227 -0.0018407761 -0.0018409754 -0.0018412936 -0.001841514 -0.0018417429 -0.0018418602 -0.0018414762 -0.0018399027]]...]
INFO - root - 2017-12-09 15:02:30.586349: step 33210, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.873 sec/batch; 72h:35m:22s remains)
INFO - root - 2017-12-09 15:02:39.220799: step 33220, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.888 sec/batch; 73h:49m:42s remains)
INFO - root - 2017-12-09 15:02:47.932510: step 33230, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.870 sec/batch; 72h:19m:31s remains)
INFO - root - 2017-12-09 15:02:56.693910: step 33240, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.883 sec/batch; 73h:21m:45s remains)
INFO - root - 2017-12-09 15:03:05.281827: step 33250, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.849 sec/batch; 70h:32m:10s remains)
INFO - root - 2017-12-09 15:03:14.008654: step 33260, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.879 sec/batch; 73h:03m:45s remains)
INFO - root - 2017-12-09 15:03:22.682184: step 33270, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.894 sec/batch; 74h:20m:08s remains)
INFO - root - 2017-12-09 15:03:30.963048: step 33280, loss = 0.82, batch loss = 0.69 (10.6 examples/sec; 0.757 sec/batch; 62h:55m:37s remains)
INFO - root - 2017-12-09 15:03:39.735233: step 33290, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.874 sec/batch; 72h:38m:38s remains)
INFO - root - 2017-12-09 15:03:48.423495: step 33300, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 72h:02m:52s remains)
2017-12-09 15:03:49.248221: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0018175779 -0.001817294 -0.00181797 -0.0018186326 -0.0018189278 -0.0018189144 -0.0018186538 -0.0018182264 -0.0018180107 -0.0018181823 -0.0018184573 -0.0018186842 -0.0018187738 -0.0018186684 -0.0018183459][-0.001816555 -0.0018163986 -0.0018172261 -0.001818018 -0.001818419 -0.0018184877 -0.0018182672 -0.0018178738 -0.0018177633 -0.0018180733 -0.0018184495 -0.0018186814 -0.0018187397 -0.0018185389 -0.0018181043][-0.0018158437 -0.0018158569 -0.0018167865 -0.0018176753 -0.0018181523 -0.0018183154 -0.0018181582 -0.0018178221 -0.0018177772 -0.0018181979 -0.0018186689 -0.0018189399 -0.0018189747 -0.0018187109 -0.0018182021][-0.0018154031 -0.0018155518 -0.0018165029 -0.0018174052 -0.0018178987 -0.0018180872 -0.0018179354 -0.0018176163 -0.0018176241 -0.0018181633 -0.0018187425 -0.001819073 -0.0018190874 -0.0018187895 -0.0018182439][-0.0018154887 -0.0018156436 -0.0018164951 -0.0018173127 -0.0018177485 -0.0018179059 -0.0018177377 -0.0018175167 -0.0018176414 -0.0018182496 -0.0018188377 -0.0018191142 -0.001819082 -0.0018187591 -0.0018181963][-0.0018158944 -0.00181602 -0.0018167316 -0.0018174212 -0.0018177886 -0.0018178758 -0.0018176821 -0.0018176427 -0.001817936 -0.0018185107 -0.0018189747 -0.0018191262 -0.001819012 -0.0018186305 -0.0018180786][-0.0018165457 -0.00181661 -0.0018171964 -0.001817727 -0.0018179643 -0.0018178874 -0.0018175944 -0.0018176455 -0.0018180022 -0.0018185015 -0.0018188573 -0.0018189595 -0.0018188022 -0.0018184063 -0.0018179177][-0.0018172278 -0.001817128 -0.0018175893 -0.0018179634 -0.0018180304 -0.001817759 -0.0018173715 -0.0018174398 -0.0018177859 -0.0018182234 -0.0018185711 -0.0018186843 -0.0018185396 -0.0018181868 -0.0018177788][-0.0018176568 -0.0018174716 -0.0018178349 -0.001818095 -0.0018180707 -0.0018177099 -0.0018172605 -0.0018172483 -0.0018175355 -0.001817915 -0.0018182915 -0.0018184131 -0.0018183006 -0.001818006 -0.0018176739][-0.0018177587 -0.0018175887 -0.0018179492 -0.0018181708 -0.0018181239 -0.0018177982 -0.0018173924 -0.0018172913 -0.0018174422 -0.0018177289 -0.001818092 -0.0018181896 -0.0018180995 -0.0018178576 -0.001817593][-0.0018176843 -0.0018174448 -0.0018177883 -0.0018180246 -0.0018180113 -0.0018177486 -0.0018174129 -0.0018172865 -0.0018173519 -0.0018175649 -0.0018178909 -0.0018179886 -0.0018179493 -0.0018177427 -0.0018175447][-0.0018176401 -0.0018173278 -0.0018176381 -0.001817829 -0.0018177781 -0.0018175114 -0.0018172256 -0.0018171245 -0.0018171828 -0.0018173961 -0.0018177216 -0.0018178519 -0.0018178553 -0.0018176857 -0.0018175286][-0.0018177897 -0.0018174218 -0.0018176635 -0.0018177796 -0.0018176326 -0.0018173287 -0.0018171018 -0.0018170752 -0.001817155 -0.0018173652 -0.0018176876 -0.0018178357 -0.0018178331 -0.0018176655 -0.0018175184][-0.0018183478 -0.0018179349 -0.0018180825 -0.0018181199 -0.0018178821 -0.0018174968 -0.0018172765 -0.0018172687 -0.0018173301 -0.0018175078 -0.001817757 -0.0018178887 -0.001817861 -0.0018176667 -0.0018175052][-0.0018199049 -0.0018193439 -0.0018193123 -0.0018192055 -0.0018187857 -0.001818179 -0.0018177798 -0.0018176347 -0.0018176172 -0.0018177198 -0.001817867 -0.001817979 -0.0018179183 -0.0018176902 -0.0018175058]]...]
INFO - root - 2017-12-09 15:03:57.704088: step 33310, loss = 0.81, batch loss = 0.69 (9.5 examples/sec; 0.845 sec/batch; 70h:13m:06s remains)
INFO - root - 2017-12-09 15:04:06.414265: step 33320, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.875 sec/batch; 72h:44m:30s remains)
INFO - root - 2017-12-09 15:04:15.113208: step 33330, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.850 sec/batch; 70h:39m:39s remains)
INFO - root - 2017-12-09 15:04:23.792862: step 33340, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.861 sec/batch; 71h:33m:15s remains)
INFO - root - 2017-12-09 15:04:32.478857: step 33350, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.882 sec/batch; 73h:15m:05s remains)
INFO - root - 2017-12-09 15:04:41.073517: step 33360, loss = 0.83, batch loss = 0.70 (9.5 examples/sec; 0.846 sec/batch; 70h:18m:40s remains)
INFO - root - 2017-12-09 15:04:49.766859: step 33370, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.880 sec/batch; 73h:08m:01s remains)
INFO - root - 2017-12-09 15:04:58.262422: step 33380, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.850 sec/batch; 70h:36m:25s remains)
INFO - root - 2017-12-09 15:05:06.788887: step 33390, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 72h:04m:58s remains)
INFO - root - 2017-12-09 15:05:15.342562: step 33400, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.847 sec/batch; 70h:24m:22s remains)
2017-12-09 15:05:16.242923: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.092171371 0.095767915 0.097976372 0.0988937 0.098815136 0.097460777 0.09509968 0.092595048 0.089918971 0.087237664 0.08413063 0.081387885 0.078799352 0.075832032 0.072977088][0.088857859 0.09268856 0.09503971 0.096570306 0.096949264 0.096157677 0.094801195 0.093474008 0.092214443 0.090721823 0.08870095 0.08655569 0.084271029 0.08099249 0.077450506][0.081152707 0.084887765 0.08766716 0.08961802 0.090389244 0.090608127 0.090662487 0.090803057 0.0912341 0.091308296 0.090976633 0.089649528 0.087558322 0.084480792 0.080536388][0.071771324 0.075786464 0.079170525 0.081622377 0.083058044 0.084163509 0.085285634 0.087033622 0.088850677 0.090473749 0.091754645 0.091589 0.090164132 0.087248944 0.083232582][0.060644004 0.06523411 0.069471106 0.07285399 0.0752266 0.0774059 0.079593405 0.082292765 0.085104093 0.088083468 0.090406276 0.0912671 0.090645783 0.088192865 0.0846244][0.049264066 0.054064523 0.058701064 0.062830761 0.066160165 0.069375359 0.072544664 0.07604374 0.079734705 0.083508126 0.086635694 0.08821322 0.088658728 0.0869981 0.083883442][0.03977254 0.044464312 0.048960395 0.05330836 0.056742392 0.06024494 0.064052761 0.068059631 0.0722696 0.07673081 0.080504268 0.08304067 0.084495127 0.08396443 0.081870161][0.033372384 0.037757613 0.04174022 0.045707773 0.048674695 0.051666692 0.054543208 0.058386113 0.062798254 0.067584462 0.0722813 0.076449 0.079623654 0.080491617 0.079859041][0.02893859 0.033231672 0.036955073 0.040081784 0.042341284 0.044304896 0.045923766 0.049007665 0.052984629 0.058306195 0.0637741 0.069383085 0.074336737 0.076919831 0.077820353][0.027689165 0.031640515 0.034988832 0.037796389 0.039289188 0.040267747 0.040628567 0.042516056 0.045835868 0.050629657 0.056429286 0.063077591 0.069106035 0.073258996 0.075349987][0.028985457 0.032611627 0.035561495 0.037905969 0.03853276 0.03836133 0.037559953 0.03844697 0.040906385 0.045029972 0.050552621 0.057256423 0.064107992 0.069132768 0.072093695][0.032723062 0.035946053 0.03818677 0.039645512 0.039577737 0.0380131 0.036341645 0.03627868 0.037919708 0.041648462 0.046937961 0.053758524 0.06069031 0.06631209 0.069791041][0.038533341 0.041959833 0.043590214 0.04398822 0.042969909 0.040539607 0.037935071 0.03694601 0.037938889 0.04055893 0.044945773 0.05125697 0.057654295 0.063143574 0.066637486][0.04488761 0.048292041 0.04951011 0.048851557 0.046536349 0.043421611 0.040383942 0.038911607 0.039404802 0.04160909 0.045591373 0.050817572 0.056023873 0.060710747 0.064113222][0.052637476 0.05577933 0.056055598 0.053992853 0.050434496 0.046129182 0.042683855 0.041117959 0.041572634 0.043695491 0.047302675 0.051972043 0.056332555 0.060203884 0.063044459]]...]
INFO - root - 2017-12-09 15:05:24.746303: step 33410, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.894 sec/batch; 74h:16m:32s remains)
INFO - root - 2017-12-09 15:05:33.280721: step 33420, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 71h:46m:36s remains)
INFO - root - 2017-12-09 15:05:41.851793: step 33430, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.876 sec/batch; 72h:44m:38s remains)
INFO - root - 2017-12-09 15:05:50.585477: step 33440, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.854 sec/batch; 70h:54m:08s remains)
INFO - root - 2017-12-09 15:05:59.234702: step 33450, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.836 sec/batch; 69h:28m:09s remains)
INFO - root - 2017-12-09 15:06:07.860613: step 33460, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.885 sec/batch; 73h:32m:10s remains)
INFO - root - 2017-12-09 15:06:16.490977: step 33470, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.841 sec/batch; 69h:51m:54s remains)
INFO - root - 2017-12-09 15:06:24.941599: step 33480, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 72h:19m:43s remains)
INFO - root - 2017-12-09 15:06:33.274554: step 33490, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.840 sec/batch; 69h:48m:02s remains)
INFO - root - 2017-12-09 15:06:41.878043: step 33500, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.865 sec/batch; 71h:51m:19s remains)
2017-12-09 15:06:42.737519: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.0020895258 0.0019028981 0.0019463175 0.0018415549 0.0019149631 0.0018848865 0.0023402695 0.0031654884 0.0048892349 0.0073891757 0.010901107 0.014940835 0.019122696 0.023388119 0.027095875][0.0039048472 0.0039920565 0.0042216685 0.0038520552 0.0036943716 0.0033943176 0.0036754052 0.0046805646 0.0072357245 0.011319165 0.016865026 0.022965916 0.028610002 0.033711981 0.037344094][0.0053422907 0.0056841467 0.0061633447 0.0055283774 0.0051481389 0.004745909 0.0052093575 0.0069243433 0.010719411 0.016745709 0.024290204 0.032022625 0.038636114 0.043496642 0.046228804][0.006143189 0.0066612782 0.0070817592 0.0064827986 0.0062932726 0.0064057894 0.0078560663 0.011089415 0.016489811 0.024170943 0.0329394 0.041205414 0.047595441 0.051352203 0.052562289][0.0061276592 0.0070807952 0.0076829139 0.0076207807 0.0082880212 0.0095297191 0.012287864 0.016858574 0.023269892 0.03138439 0.04010373 0.047709238 0.053298175 0.055909637 0.055829115][0.0054548196 0.0068137236 0.0080976272 0.0093417233 0.01156469 0.014503571 0.018819118 0.024283558 0.030792451 0.038137969 0.04531879 0.051225409 0.055202544 0.056700736 0.055621561][0.0045300061 0.0062771533 0.0084803859 0.011567886 0.015660075 0.020298945 0.025546536 0.031178446 0.036998522 0.042998709 0.048529133 0.052816935 0.055501107 0.055959225 0.053943902][0.003812935 0.0058308598 0.0089117792 0.013702462 0.019628763 0.025847418 0.031987563 0.037489776 0.042274829 0.046403553 0.049811125 0.052650418 0.054275338 0.0541035 0.051778071][0.0032722775 0.0054728268 0.0092252465 0.015106426 0.022273421 0.029478924 0.035977509 0.041290626 0.045241781 0.04813166 0.049987502 0.051513534 0.052242719 0.051927034 0.049848143][0.0027879293 0.0049863672 0.0090104612 0.015093581 0.022644361 0.030205918 0.036696829 0.041883454 0.045600992 0.047925558 0.049127951 0.050371069 0.05111827 0.051038269 0.049365647][0.0021165034 0.0039724428 0.0077423826 0.013286387 0.020410169 0.027769269 0.034143243 0.039432645 0.043299608 0.046150226 0.047976747 0.049935721 0.051520646 0.052333653 0.051489349][0.00098301191 0.002492788 0.00552765 0.010239081 0.016691584 0.023476692 0.029856121 0.0356072 0.040277354 0.044155385 0.047320798 0.050713394 0.053611353 0.05550082 0.055324696][0.0004268568 0.0014625529 0.0036350305 0.0072606457 0.012791627 0.019230545 0.025799533 0.032355294 0.038386144 0.043997344 0.048814323 0.053580638 0.0575197 0.060043279 0.059966538][0.00097822747 0.0015605516 0.002924328 0.0055979127 0.010348606 0.016635612 0.023746742 0.031207619 0.038380988 0.0454054 0.051387165 0.057180166 0.06173677 0.064612791 0.064502388][0.0021771637 0.0024844825 0.0033941458 0.0056306245 0.010334129 0.017044915 0.024946511 0.033276144 0.041290473 0.048790462 0.054804467 0.060415026 0.064381272 0.06685181 0.0664181]]...]
INFO - root - 2017-12-09 15:06:51.238015: step 33510, loss = 0.82, batch loss = 0.69 (10.3 examples/sec; 0.780 sec/batch; 64h:47m:14s remains)
INFO - root - 2017-12-09 15:06:59.803054: step 33520, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.894 sec/batch; 74h:13m:23s remains)
INFO - root - 2017-12-09 15:07:08.514198: step 33530, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.858 sec/batch; 71h:16m:09s remains)
INFO - root - 2017-12-09 15:07:17.180807: step 33540, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.883 sec/batch; 73h:20m:52s remains)
INFO - root - 2017-12-09 15:07:25.816489: step 33550, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.860 sec/batch; 71h:27m:14s remains)
INFO - root - 2017-12-09 15:07:34.524271: step 33560, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.847 sec/batch; 70h:18m:10s remains)
INFO - root - 2017-12-09 15:07:43.053876: step 33570, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.874 sec/batch; 72h:36m:05s remains)
INFO - root - 2017-12-09 15:07:51.543656: step 33580, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.880 sec/batch; 73h:03m:23s remains)
INFO - root - 2017-12-09 15:08:00.162035: step 33590, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.872 sec/batch; 72h:23m:17s remains)
INFO - root - 2017-12-09 15:08:08.819122: step 33600, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.838 sec/batch; 69h:32m:26s remains)
2017-12-09 15:08:09.724518: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.25090227 0.25418231 0.25450027 0.2530573 0.24903932 0.24191208 0.23251988 0.22339426 0.21554585 0.20776244 0.20126267 0.19554533 0.19013928 0.18490012 0.18048261][0.2481111 0.252176 0.25325567 0.2530584 0.25127417 0.2460098 0.23820442 0.23074046 0.22490247 0.21873558 0.21354966 0.20850126 0.20299006 0.19770454 0.19301885][0.24152677 0.24548645 0.24688397 0.24770558 0.24719967 0.2438065 0.2385951 0.23336037 0.22976349 0.22559462 0.22212729 0.21864557 0.2134781 0.2083277 0.20370397][0.23506983 0.23783018 0.23898487 0.24027325 0.2405239 0.23873733 0.23608319 0.23395932 0.23301426 0.23126008 0.22997746 0.22882438 0.22515197 0.22049242 0.21610434][0.22955209 0.23130609 0.23080973 0.23070671 0.23012227 0.22994375 0.22968006 0.23017454 0.23244324 0.23393144 0.23571496 0.23662269 0.23490518 0.23131043 0.22740644][0.22361994 0.22373466 0.22133756 0.21903907 0.21743602 0.21774606 0.21920018 0.22236609 0.22754984 0.23252638 0.23751746 0.24032027 0.24067305 0.23858368 0.23575199][0.21523127 0.21346372 0.208847 0.20531324 0.20330766 0.20389313 0.20681147 0.2122851 0.22006714 0.22810112 0.23589665 0.24128042 0.24397741 0.24355558 0.24197641][0.20878504 0.20579074 0.19898671 0.19335446 0.18980896 0.19039276 0.19429299 0.20094398 0.21036267 0.22051109 0.23056383 0.23818159 0.24308217 0.2446375 0.24413325][0.20245026 0.19942199 0.19163711 0.18481241 0.18040015 0.18076555 0.18492079 0.19218902 0.20264825 0.21436711 0.22605179 0.23492946 0.24139576 0.24395946 0.24416012][0.19752932 0.19522327 0.18722495 0.18018423 0.17499344 0.17458664 0.17846259 0.18558908 0.19598116 0.20819323 0.22081345 0.23058033 0.23760036 0.24068442 0.24130763][0.19341663 0.19264777 0.18569078 0.17905341 0.17430708 0.173248 0.1765763 0.18288477 0.19224417 0.20371857 0.21516936 0.22438641 0.23082645 0.23397112 0.23476914][0.18806006 0.1892527 0.18401898 0.17821139 0.1739818 0.17335044 0.17633213 0.18154413 0.18959737 0.19984251 0.20972839 0.21789238 0.22354729 0.22622287 0.22663823][0.18409891 0.18711661 0.18347648 0.17847955 0.17498238 0.17422347 0.176353 0.17987777 0.18612659 0.19478083 0.20246907 0.20888397 0.21321842 0.21543676 0.21553659][0.18015838 0.18463026 0.18246397 0.17878588 0.17583695 0.17470719 0.17585543 0.17836535 0.18313102 0.19006442 0.1960247 0.20097977 0.20419554 0.20542094 0.20528708][0.17741999 0.18224983 0.18054461 0.17745221 0.17474471 0.17331977 0.17359522 0.17529316 0.17877625 0.18403165 0.18831989 0.19183722 0.19406436 0.1948173 0.19452991]]...]
INFO - root - 2017-12-09 15:08:18.232933: step 33610, loss = 0.82, batch loss = 0.69 (11.3 examples/sec; 0.709 sec/batch; 58h:50m:52s remains)
INFO - root - 2017-12-09 15:08:26.878158: step 33620, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.861 sec/batch; 71h:29m:39s remains)
INFO - root - 2017-12-09 15:08:35.559930: step 33630, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.881 sec/batch; 73h:08m:28s remains)
INFO - root - 2017-12-09 15:08:44.442882: step 33640, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 72h:19m:28s remains)
INFO - root - 2017-12-09 15:08:53.139763: step 33650, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.842 sec/batch; 69h:54m:46s remains)
INFO - root - 2017-12-09 15:09:01.860006: step 33660, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.866 sec/batch; 71h:50m:46s remains)
INFO - root - 2017-12-09 15:09:10.450548: step 33670, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 71h:34m:39s remains)
INFO - root - 2017-12-09 15:09:19.087508: step 33680, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.848 sec/batch; 70h:23m:11s remains)
INFO - root - 2017-12-09 15:09:27.636174: step 33690, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.865 sec/batch; 71h:50m:19s remains)
INFO - root - 2017-12-09 15:09:36.350491: step 33700, loss = 0.82, batch loss = 0.69 (8.3 examples/sec; 0.959 sec/batch; 79h:34m:34s remains)
2017-12-09 15:09:37.261626: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0016051044 -0.0011130029 0.0001994021 0.0026383898 0.0060769226 0.0099373246 0.013198221 0.014936545 0.014782288 0.012996892 0.010260873 0.0071616229 0.0042444314 0.0018316965 6.7188172e-05][-0.0011669029 -0.00036495866 0.0018269626 0.005886951 0.01163989 0.018108925 0.023676433 0.026937537 0.027236732 0.024772896 0.020320008 0.014860668 0.0094110826 0.0047703334 0.0014218698][-0.00037996657 0.00093752972 0.0043762783 0.010582307 0.019381491 0.029395722 0.038364496 0.044277381 0.045929253 0.043225009 0.036785536 0.02805814 0.018767703 0.010519362 0.0044028144][0.00067295192 0.0026968885 0.0078966273 0.017066378 0.02986655 0.044373695 0.057588778 0.066674426 0.069789991 0.066631854 0.057821818 0.045285888 0.031465337 0.0188824 0.0092213117][0.0019840272 0.0047674309 0.011805771 0.024120087 0.041411079 0.061362714 0.080162376 0.0937952 0.099316612 0.095852166 0.084040761 0.06665834 0.047245808 0.029422428 0.015516088][0.0032831891 0.0066040913 0.014992994 0.029782876 0.050779771 0.075452752 0.099397324 0.11751191 0.12577784 0.1227111 0.10903196 0.088027187 0.063979335 0.041398332 0.02318304][0.0043782075 0.0079377061 0.016831411 0.032740593 0.055801556 0.083519861 0.11111838 0.13263825 0.14319974 0.14080729 0.12631965 0.10347372 0.076940686 0.051479626 0.030199524][0.0046309619 0.0080814436 0.016585853 0.032004293 0.054888293 0.083098322 0.11194127 0.13507122 0.1470651 0.14557111 0.1316005 0.10902122 0.08249107 0.056581065 0.034308828][0.0042401785 0.0070783421 0.014346653 0.027875213 0.048544202 0.0747148 0.10220527 0.12493663 0.13747659 0.13725527 0.12512927 0.10466288 0.0801739 0.055829529 0.034459267][0.0035790247 0.0056499382 0.010962521 0.021300593 0.03782915 0.059552208 0.083195634 0.10349612 0.11553244 0.11669973 0.10746713 0.090682112 0.070008278 0.049072672 0.030408632][0.002263105 0.00355331 0.0068703396 0.013687151 0.025202949 0.041083939 0.059184898 0.075498514 0.086066984 0.088427775 0.082487829 0.070183024 0.054365374 0.038029578 0.023331173][0.00084210315 0.0016278116 0.0033408911 0.007064132 0.013797908 0.023712119 0.035722304 0.047231063 0.055428978 0.058204081 0.055058505 0.047063887 0.036243271 0.024914354 0.014760627][-0.0006844434 -0.00015569956 0.00071150076 0.0024200357 0.0056572864 0.010791748 0.017463259 0.024348732 0.029775288 0.032177955 0.030907419 0.026408385 0.019954737 0.013140431 0.0071321521][-0.0015856208 -0.0013907067 -0.0010581001 -0.00037930836 0.0009277385 0.0030799382 0.0060526989 0.0093488693 0.012166562 0.013627318 0.013234708 0.011109493 0.007934982 0.0046236613 0.001812348][-0.001779867 -0.0017739413 -0.001732648 -0.0015895085 -0.0012356618 -0.00055576011 0.00047583214 0.0016904565 0.0027935365 0.0034411186 0.003389752 0.0026446772 0.0014751815 0.00026234786 -0.0007260748]]...]
INFO - root - 2017-12-09 15:09:45.715478: step 33710, loss = 0.82, batch loss = 0.69 (11.6 examples/sec; 0.691 sec/batch; 57h:20m:35s remains)
INFO - root - 2017-12-09 15:09:54.459108: step 33720, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.877 sec/batch; 72h:46m:30s remains)
INFO - root - 2017-12-09 15:10:03.101618: step 33730, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.847 sec/batch; 70h:15m:15s remains)
INFO - root - 2017-12-09 15:10:11.768568: step 33740, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 72h:14m:57s remains)
INFO - root - 2017-12-09 15:10:20.413497: step 33750, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.873 sec/batch; 72h:28m:15s remains)
INFO - root - 2017-12-09 15:10:29.096219: step 33760, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.855 sec/batch; 70h:54m:47s remains)
INFO - root - 2017-12-09 15:10:37.740849: step 33770, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.866 sec/batch; 71h:50m:13s remains)
INFO - root - 2017-12-09 15:10:46.279500: step 33780, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.892 sec/batch; 74h:00m:08s remains)
INFO - root - 2017-12-09 15:10:54.992586: step 33790, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.845 sec/batch; 70h:08m:35s remains)
INFO - root - 2017-12-09 15:11:03.745534: step 33800, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.854 sec/batch; 70h:49m:01s remains)
2017-12-09 15:11:04.697632: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0018183691 -0.0018180836 -0.001812008 -0.001778677 -0.0017052349 -0.0016404022 -0.0016329432 -0.0016850369 -0.0017446348 -0.0017884215 -0.0018112124 -0.0018191884 -0.0018203458 -0.001819848 -0.0018190548][-0.0018176453 -0.0018162159 -0.0017765056 -0.0016490365 -0.0014333255 -0.0012592904 -0.0012421955 -0.0013846779 -0.0015659328 -0.0017058932 -0.0017837529 -0.0018136896 -0.0018197814 -0.0018196995 -0.0018186483][-0.0018180506 -0.0018041721 -0.0016643426 -0.0012821865 -0.00069691497 -0.00024461129 -0.00019807916 -0.00055117859 -0.0010445158 -0.0014512464 -0.0016902441 -0.0017901581 -0.0018162944 -0.0018194641 -0.0018186386][-0.0018181551 -0.0017779893 -0.001462872 -0.00065340311 0.000576944 0.0015846571 0.0017886591 0.0011278075 7.1301241e-05 -0.000869598 -0.0014555291 -0.001720698 -0.0018022427 -0.0018173858 -0.001818295][-0.0018184615 -0.0017440047 -0.001222607 0.00011270482 0.002199694 0.0040612016 0.0046679266 0.0037274645 0.001911427 0.00014912605 -0.0010190725 -0.0015837174 -0.0017737502 -0.001813448 -0.0018171207][-0.0018186099 -0.0017242162 -0.0010704837 0.00067529839 0.0035491642 0.0063693738 0.0076255212 0.0066329967 0.0041316366 0.0014736428 -0.00041038904 -0.0013793987 -0.0017284333 -0.0018077999 -0.0018161844][-0.0018180688 -0.0017259869 -0.0010720667 0.0007935426 0.0040702452 0.0076026507 0.0095552774 0.0088181 0.0059940638 0.0026881387 0.00018599827 -0.001168902 -0.0016798779 -0.0018018786 -0.0018157011][-0.0018181516 -0.0017452512 -0.0012143387 0.00042746274 0.0035272711 0.00719114 0.0095860716 0.0093290489 0.0067124749 0.0032951543 0.00053121254 -0.0010352568 -0.0016475328 -0.0017984723 -0.0018159295][-0.0018181127 -0.0017715438 -0.0014210985 -0.00023352983 0.0021824315 0.0052954736 0.0076205321 0.0078093056 0.005831656 0.0029155067 0.00040695292 -0.0010651501 -0.0016528517 -0.0017993991 -0.0018166869][-0.0018180652 -0.0017940727 -0.0016069117 -0.00090475514 0.00064434682 0.0028243959 0.0046590925 0.00508114 0.0038614077 0.00178897 -9.8296441e-05 -0.001234329 -0.0016917734 -0.0018047962 -0.0018178405][-0.0018180377 -0.0018086053 -0.0017333115 -0.0014089904 -0.000622308 0.00059325749 0.001734838 0.0021338123 0.0015371811 0.00035025051 -0.00078214065 -0.0014709603 -0.0017462753 -0.0018120423 -0.0018191272][-0.0018175946 -0.0018147409 -0.0017927291 -0.0016764565 -0.0013606061 -0.00082291465 -0.00026312808 -1.0748976e-05 -0.00023873977 -0.00078465918 -0.0013269191 -0.0016576458 -0.0017875024 -0.0018167665 -0.0018195283][-0.0018172333 -0.0018167465 -0.0018136583 -0.0017859811 -0.0016973028 -0.0015292972 -0.0013368386 -0.0012335399 -0.001294565 -0.0014742007 -0.0016575272 -0.0017678955 -0.0018097921 -0.0018183884 -0.0018188647][-0.0018166244 -0.0018161723 -0.0018163996 -0.0018119827 -0.001793801 -0.0017565415 -0.0017111667 -0.0016842019 -0.0016954846 -0.0017366037 -0.0017799924 -0.0018063602 -0.0018161811 -0.0018180922 -0.0018178615][-0.0018161713 -0.0018154648 -0.0018157652 -0.0018161181 -0.0018154868 -0.0018132502 -0.0018102463 -0.0018077986 -0.001807446 -0.0018097626 -0.0018133613 -0.0018160484 -0.0018170859 -0.0018170668 -0.001816628]]...]
INFO - root - 2017-12-09 15:11:13.163766: step 33810, loss = 0.82, batch loss = 0.69 (11.1 examples/sec; 0.724 sec/batch; 60h:03m:14s remains)
INFO - root - 2017-12-09 15:11:21.872486: step 33820, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.900 sec/batch; 74h:41m:07s remains)
INFO - root - 2017-12-09 15:11:30.558024: step 33830, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.838 sec/batch; 69h:29m:47s remains)
INFO - root - 2017-12-09 15:11:38.991280: step 33840, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.860 sec/batch; 71h:19m:58s remains)
INFO - root - 2017-12-09 15:11:47.644215: step 33850, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.890 sec/batch; 73h:49m:57s remains)
INFO - root - 2017-12-09 15:11:56.309201: step 33860, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.874 sec/batch; 72h:29m:00s remains)
INFO - root - 2017-12-09 15:12:05.000093: step 33870, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.851 sec/batch; 70h:33m:54s remains)
INFO - root - 2017-12-09 15:12:13.631565: step 33880, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.898 sec/batch; 74h:31m:28s remains)
INFO - root - 2017-12-09 15:12:22.313487: step 33890, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.887 sec/batch; 73h:34m:29s remains)
INFO - root - 2017-12-09 15:12:30.911090: step 33900, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 72h:00m:03s remains)
2017-12-09 15:12:31.813242: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.28958121 0.28920698 0.28624204 0.28135008 0.27417859 0.265092 0.2558606 0.24833468 0.24226558 0.23785622 0.23412891 0.23084523 0.22676826 0.21987502 0.21223189][0.29989126 0.30021077 0.29774663 0.29222569 0.2847704 0.27537841 0.26528266 0.25705555 0.25063512 0.24675144 0.24334168 0.24043059 0.23699276 0.23066016 0.22265413][0.30706123 0.30845535 0.30573431 0.29918659 0.29098812 0.280104 0.26887277 0.25967318 0.25247344 0.24849084 0.2451212 0.2426977 0.23990074 0.23480216 0.22778185][0.31558752 0.31758472 0.3145422 0.30649215 0.29637092 0.28311703 0.26954561 0.25900781 0.25040305 0.24553291 0.24224634 0.24110959 0.24000184 0.23631345 0.23071092][0.32078579 0.32399744 0.32005274 0.31058162 0.29885608 0.28321311 0.26706344 0.25403395 0.24391258 0.23829639 0.23510398 0.23525874 0.2363562 0.23552148 0.23188178][0.32557616 0.32916048 0.32338858 0.3116084 0.29768068 0.28072885 0.26317409 0.24802789 0.23710756 0.23111695 0.2279976 0.2285554 0.23019408 0.23180549 0.22994681][0.33214658 0.33695209 0.33018818 0.31679678 0.30045763 0.28169763 0.26215491 0.24533723 0.23314752 0.22686113 0.22424969 0.2250586 0.22731353 0.23016967 0.2291583][0.336315 0.34323341 0.33647713 0.32256806 0.3047888 0.28442341 0.26372328 0.24614517 0.23307738 0.2260135 0.22336808 0.22444934 0.22631916 0.22855604 0.22727436][0.33526376 0.34468582 0.33974606 0.32744986 0.31047237 0.29063642 0.27011013 0.25264895 0.23927711 0.23175359 0.22890766 0.22943443 0.23033375 0.23044932 0.22738829][0.32985359 0.34175959 0.33874243 0.32890144 0.31430227 0.29614571 0.27735537 0.26050574 0.24771069 0.24021934 0.23678507 0.23624006 0.23505107 0.23297004 0.22769392][0.32022828 0.3338815 0.33296442 0.32536384 0.31313577 0.29825953 0.2826826 0.26808631 0.25666288 0.2496838 0.24541259 0.24254929 0.23839739 0.23320776 0.22535712][0.30259815 0.31684926 0.3175469 0.31202307 0.30233005 0.29050556 0.27826887 0.2669473 0.25765705 0.25191489 0.24751352 0.24343644 0.23780227 0.23029746 0.22080614][0.27836812 0.29202989 0.29357988 0.28995132 0.28317875 0.27459487 0.26572034 0.2576026 0.2505019 0.24609549 0.24155684 0.23689012 0.23063083 0.22240123 0.21292734][0.25315168 0.26577339 0.26755959 0.26553309 0.26121452 0.25554743 0.24947923 0.2435293 0.23865904 0.23534828 0.23132478 0.22679208 0.22079773 0.21329242 0.20490395][0.23020448 0.24098867 0.24183512 0.24031037 0.23726326 0.23345131 0.22955185 0.22572285 0.22277741 0.22073165 0.21767364 0.21386467 0.20887738 0.20291542 0.19623955]]...]
INFO - root - 2017-12-09 15:12:40.418675: step 33910, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.859 sec/batch; 71h:17m:00s remains)
INFO - root - 2017-12-09 15:12:48.933197: step 33920, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.852 sec/batch; 70h:40m:54s remains)
INFO - root - 2017-12-09 15:12:57.552477: step 33930, loss = 0.81, batch loss = 0.68 (9.1 examples/sec; 0.880 sec/batch; 73h:01m:13s remains)
INFO - root - 2017-12-09 15:13:06.137559: step 33940, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.855 sec/batch; 70h:55m:30s remains)
INFO - root - 2017-12-09 15:13:14.771950: step 33950, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.861 sec/batch; 71h:25m:13s remains)
INFO - root - 2017-12-09 15:13:23.428037: step 33960, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.861 sec/batch; 71h:23m:02s remains)
INFO - root - 2017-12-09 15:13:32.043494: step 33970, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.849 sec/batch; 70h:21m:46s remains)
INFO - root - 2017-12-09 15:13:40.736736: step 33980, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.880 sec/batch; 73h:00m:42s remains)
INFO - root - 2017-12-09 15:13:49.265974: step 33990, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.865 sec/batch; 71h:43m:44s remains)
INFO - root - 2017-12-09 15:13:57.949942: step 34000, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.861 sec/batch; 71h:23m:39s remains)
2017-12-09 15:13:58.838201: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0018104526 -0.0018096344 -0.0018095747 -0.0018095996 -0.0018095121 -0.0018093789 -0.0018092337 -0.0018091451 -0.0018091716 -0.0018092775 -0.0018094321 -0.001809573 -0.0018097092 -0.0018099661 -0.0018104374][-0.0018097111 -0.0018088985 -0.0018088954 -0.0018089755 -0.0018089445 -0.0018088594 -0.0018087501 -0.0018086841 -0.0018087141 -0.0018088369 -0.001809009 -0.0018091616 -0.0018093043 -0.0018095369 -0.0018100336][-0.0018096429 -0.0018089128 -0.001808911 -0.0018089943 -0.0018089527 -0.0018088432 -0.0018087009 -0.0018086185 -0.0018086585 -0.0018088095 -0.0018090149 -0.0018092053 -0.0018093857 -0.0018096437 -0.0018101797][-0.0018096635 -0.0018089407 -0.001808909 -0.0018089814 -0.0018089173 -0.001808773 -0.0018085856 -0.0018084703 -0.0018085148 -0.001808701 -0.001808943 -0.0018091844 -0.0018094281 -0.00180973 -0.0018102832][-0.0018097122 -0.0018089704 -0.0018089078 -0.0018089639 -0.0018088743 -0.0018086947 -0.001808458 -0.0018082991 -0.0018083396 -0.0018085599 -0.0018088404 -0.0018091404 -0.0018094569 -0.0018097993 -0.0018103275][-0.0018097482 -0.0018089711 -0.0018089032 -0.0018089421 -0.0018088289 -0.0018086206 -0.0018083419 -0.0018081465 -0.0018081811 -0.0018084317 -0.0018087493 -0.0018091069 -0.0018094853 -0.001809846 -0.0018103239][-0.001809767 -0.0018089714 -0.0018088979 -0.0018089254 -0.0018087953 -0.0018085674 -0.001808265 -0.0018080503 -0.0018080797 -0.0018083438 -0.0018086823 -0.0018090761 -0.001809487 -0.0018098389 -0.0018102431][-0.0018098238 -0.0018089706 -0.0018088985 -0.0018089165 -0.001808774 -0.0018085339 -0.0018082249 -0.0018080035 -0.0018080255 -0.0018082848 -0.0018086279 -0.0018090279 -0.0018094295 -0.0018097404 -0.0018100502][-0.0018098541 -0.0018089339 -0.001808905 -0.0018089126 -0.0018087559 -0.0018084991 -0.0018081841 -0.0018079627 -0.0018079736 -0.0018082166 -0.0018085545 -0.0018089481 -0.0018093159 -0.0018095636 -0.0018097759][-0.0018096953 -0.0018088949 -0.0018089021 -0.0018088986 -0.0018087249 -0.0018084486 -0.0018081283 -0.0018079116 -0.0018079189 -0.0018081459 -0.0018084812 -0.0018088692 -0.0018091969 -0.0018093777 -0.0018095][-0.001809613 -0.001808868 -0.0018088901 -0.0018088846 -0.0018087071 -0.0018084255 -0.0018081125 -0.0018079078 -0.0018079149 -0.0018081221 -0.0018084495 -0.001808819 -0.0018090977 -0.0018092208 -0.0018092709][-0.0018094672 -0.0018088357 -0.0018088728 -0.0018088738 -0.0018087109 -0.0018084429 -0.0018081495 -0.0018079657 -0.001807973 -0.0018081539 -0.0018084568 -0.0018087921 -0.0018090227 -0.0018091033 -0.0018090945][-0.0018094191 -0.0018088458 -0.0018088473 -0.0018088613 -0.001808729 -0.0018084972 -0.0018082407 -0.0018080821 -0.0018080825 -0.0018082287 -0.0018084846 -0.0018087681 -0.001808952 -0.0018090081 -0.0018089715][-0.0018094355 -0.0018088422 -0.001808796 -0.0018088174 -0.0018087188 -0.0018085368 -0.0018083309 -0.0018082061 -0.0018082089 -0.0018083223 -0.0018085238 -0.0018087438 -0.0018088821 -0.0018089232 -0.0018088813][-0.0018094965 -0.001808857 -0.0018087301 -0.0018087582 -0.001808696 -0.0018085728 -0.0018084296 -0.001808341 -0.0018083454 -0.0018084281 -0.0018085749 -0.0018087314 -0.001808829 -0.0018088623 -0.001808827]]...]
INFO - root - 2017-12-09 15:14:07.639098: step 34010, loss = 0.82, batch loss = 0.69 (8.7 examples/sec; 0.921 sec/batch; 76h:19m:32s remains)
INFO - root - 2017-12-09 15:14:16.243815: step 34020, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.877 sec/batch; 72h:43m:14s remains)
INFO - root - 2017-12-09 15:14:24.948881: step 34030, loss = 0.81, batch loss = 0.69 (9.0 examples/sec; 0.893 sec/batch; 74h:03m:26s remains)
INFO - root - 2017-12-09 15:14:33.573470: step 34040, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.842 sec/batch; 69h:50m:11s remains)
INFO - root - 2017-12-09 15:14:42.239908: step 34050, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.879 sec/batch; 72h:50m:01s remains)
INFO - root - 2017-12-09 15:14:50.904977: step 34060, loss = 0.83, batch loss = 0.70 (9.3 examples/sec; 0.857 sec/batch; 71h:02m:06s remains)
INFO - root - 2017-12-09 15:14:59.463631: step 34070, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.832 sec/batch; 68h:58m:06s remains)
INFO - root - 2017-12-09 15:15:07.992001: step 34080, loss = 0.82, batch loss = 0.69 (9.8 examples/sec; 0.820 sec/batch; 68h:00m:06s remains)
INFO - root - 2017-12-09 15:15:16.628532: step 34090, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.855 sec/batch; 70h:51m:40s remains)
INFO - root - 2017-12-09 15:15:25.384089: step 34100, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.878 sec/batch; 72h:47m:21s remains)
2017-12-09 15:15:26.232215: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.29966325 0.29559934 0.29133698 0.283578 0.27606264 0.26925495 0.26030397 0.25146782 0.24396537 0.23721403 0.22930987 0.22158812 0.21618675 0.21124902 0.20691454][0.29582465 0.29365867 0.2909846 0.28540736 0.27983022 0.27338687 0.2652922 0.25679615 0.24938279 0.24287494 0.23585689 0.22848454 0.22317342 0.21860422 0.21502107][0.28677619 0.28637594 0.28580222 0.28244856 0.27912387 0.2738649 0.26694152 0.2594744 0.25251886 0.24680658 0.24038258 0.23323458 0.22820762 0.22419956 0.22172792][0.28094929 0.28300855 0.28478217 0.28437093 0.28370661 0.2799527 0.27433258 0.26760256 0.26064557 0.25476715 0.24808161 0.24112242 0.2363655 0.23265393 0.2311697][0.27762744 0.28156593 0.28493339 0.28718036 0.28896075 0.28724056 0.28347504 0.27772734 0.27123889 0.26448706 0.25734395 0.25004116 0.24503808 0.24136698 0.24056517][0.27335989 0.27935177 0.28411329 0.28854552 0.29261142 0.29374245 0.29251313 0.28826919 0.28242356 0.27504939 0.26753178 0.25914875 0.25330904 0.24931329 0.24885428][0.26881686 0.27641848 0.28216973 0.288633 0.29459047 0.29829863 0.29956239 0.29722664 0.29255608 0.28503096 0.27684945 0.26788154 0.26127279 0.25708619 0.2564778][0.26607066 0.27439034 0.28058767 0.2876204 0.29416713 0.29986095 0.30286056 0.3017886 0.2977438 0.29024014 0.2816886 0.27241293 0.26525655 0.26089448 0.2597667][0.26283163 0.27183765 0.27805054 0.28554484 0.292576 0.2989521 0.30296534 0.30265534 0.29901442 0.29169318 0.28292957 0.2736288 0.26613012 0.26104078 0.25891575][0.25667158 0.26564735 0.27133572 0.27827579 0.28504688 0.29147694 0.29555678 0.29581007 0.29244861 0.2859062 0.27793229 0.26901981 0.26159257 0.2562269 0.25334185][0.24841809 0.25780782 0.26268348 0.26870972 0.27463669 0.2804468 0.28407502 0.2841152 0.28108281 0.27529123 0.26759249 0.2592583 0.2521666 0.24716 0.24402401][0.23969182 0.24879262 0.25223443 0.25649875 0.26079535 0.2650117 0.26763466 0.26766008 0.2651659 0.26076612 0.25435546 0.24753298 0.24160096 0.23710445 0.23413424][0.23088387 0.23970853 0.24198048 0.24416472 0.24648926 0.24876216 0.24988666 0.24932724 0.24691865 0.24404739 0.23888615 0.23364876 0.2290664 0.22525388 0.22297628][0.22423407 0.2326594 0.23404908 0.23455042 0.23506148 0.23526886 0.234801 0.23362134 0.2313894 0.22957584 0.22586696 0.22221492 0.21900004 0.21597883 0.2143949][0.2193352 0.22718562 0.22770244 0.22658078 0.22534291 0.22394353 0.22197767 0.220203 0.21803825 0.2171201 0.21471538 0.21246581 0.21057534 0.20868956 0.20787598]]...]
INFO - root - 2017-12-09 15:15:34.847457: step 34110, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.834 sec/batch; 69h:07m:43s remains)
INFO - root - 2017-12-09 15:15:43.455506: step 34120, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.865 sec/batch; 71h:42m:05s remains)
INFO - root - 2017-12-09 15:15:52.188204: step 34130, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.891 sec/batch; 73h:48m:53s remains)
INFO - root - 2017-12-09 15:16:00.838024: step 34140, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.889 sec/batch; 73h:41m:22s remains)
INFO - root - 2017-12-09 15:16:09.488281: step 34150, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.870 sec/batch; 72h:05m:38s remains)
INFO - root - 2017-12-09 15:16:18.131237: step 34160, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.863 sec/batch; 71h:33m:12s remains)
INFO - root - 2017-12-09 15:16:26.852817: step 34170, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.865 sec/batch; 71h:40m:21s remains)
INFO - root - 2017-12-09 15:16:35.520437: step 34180, loss = 0.83, batch loss = 0.70 (9.1 examples/sec; 0.881 sec/batch; 73h:01m:34s remains)
INFO - root - 2017-12-09 15:16:44.190350: step 34190, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.865 sec/batch; 71h:39m:56s remains)
INFO - root - 2017-12-09 15:16:52.945078: step 34200, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.875 sec/batch; 72h:29m:14s remains)
2017-12-09 15:16:53.840116: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.15126272 0.17348546 0.19659017 0.21990471 0.24027617 0.25552046 0.26197672 0.25896415 0.24842367 0.23244591 0.2156294 0.19942853 0.18300451 0.16402636 0.14274928][0.19087958 0.21816407 0.24268644 0.26462653 0.28191572 0.29392391 0.29794642 0.29224578 0.27858478 0.25923595 0.23944181 0.22115535 0.20339082 0.1843054 0.16345529][0.23330885 0.26438257 0.28923634 0.30719855 0.31895477 0.32573837 0.32510465 0.31536356 0.2978608 0.27544904 0.25318244 0.23363304 0.2158256 0.19704838 0.17723319][0.27161038 0.30503458 0.32872713 0.34207457 0.34690097 0.3470462 0.34143025 0.32779557 0.30735183 0.28313249 0.26042068 0.24146673 0.22501555 0.20866694 0.19072527][0.301414 0.3335534 0.35316586 0.36107203 0.36102322 0.35548821 0.34589228 0.32967961 0.30855849 0.2854054 0.26470405 0.24852386 0.23447557 0.2206866 0.20426154][0.31955102 0.35071304 0.36615646 0.36870238 0.36350861 0.35335755 0.34040406 0.32223341 0.30143404 0.28167385 0.26587406 0.25439969 0.24381866 0.23221235 0.21692967][0.32631859 0.35491443 0.36675295 0.36557245 0.35639319 0.34263912 0.32650068 0.30800566 0.28911766 0.27360439 0.26363009 0.25706223 0.24998698 0.24026367 0.22510892][0.32824636 0.35369477 0.36184984 0.35804436 0.34633946 0.33091217 0.31368434 0.29557618 0.27885303 0.26760456 0.26284894 0.26102763 0.25674397 0.24766319 0.23135616][0.32683131 0.35007682 0.35589156 0.34995452 0.33659691 0.3198002 0.30319288 0.28741378 0.27330509 0.26546651 0.26462796 0.26601681 0.26364991 0.25536013 0.23882395][0.3234387 0.34508285 0.34895536 0.3423377 0.3292737 0.31391308 0.2995671 0.2870568 0.27716386 0.27311748 0.2744233 0.27734312 0.27548981 0.26604828 0.24766749][0.32473287 0.34548879 0.34793288 0.34036738 0.32727611 0.31346035 0.30186898 0.29342824 0.28761533 0.28641507 0.29023531 0.29340833 0.29098281 0.27958781 0.25937635][0.32865769 0.34949529 0.35141727 0.34396943 0.33157527 0.31910521 0.30911398 0.30318734 0.30024385 0.30113035 0.306068 0.30916333 0.3059299 0.29302859 0.27126002][0.32394251 0.34687155 0.35069913 0.34539467 0.33566096 0.32564044 0.317686 0.31310242 0.3108542 0.31217751 0.31662744 0.31947973 0.31578839 0.30268949 0.28176421][0.30601588 0.33057618 0.33610624 0.33318332 0.32664767 0.31987017 0.31451461 0.31112355 0.31033811 0.313396 0.31871381 0.32148707 0.31853816 0.30777395 0.28995833][0.27780819 0.30019534 0.30533779 0.30430678 0.30074787 0.29764608 0.29592496 0.29533064 0.29747415 0.30290821 0.31028444 0.31480369 0.31431478 0.30694532 0.2929529]]...]
INFO - root - 2017-12-09 15:17:02.455616: step 34210, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.885 sec/batch; 73h:21m:36s remains)
INFO - root - 2017-12-09 15:17:10.868346: step 34220, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 72h:08m:50s remains)
INFO - root - 2017-12-09 15:17:19.421235: step 34230, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.856 sec/batch; 70h:56m:37s remains)
INFO - root - 2017-12-09 15:17:28.124471: step 34240, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.892 sec/batch; 73h:52m:38s remains)
INFO - root - 2017-12-09 15:17:36.880513: step 34250, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.849 sec/batch; 70h:19m:00s remains)
INFO - root - 2017-12-09 15:17:45.738628: step 34260, loss = 0.82, batch loss = 0.69 (8.0 examples/sec; 0.998 sec/batch; 82h:41m:41s remains)
INFO - root - 2017-12-09 15:17:54.453604: step 34270, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.887 sec/batch; 73h:27m:39s remains)
INFO - root - 2017-12-09 15:18:02.919647: step 34280, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.857 sec/batch; 71h:01m:35s remains)
INFO - root - 2017-12-09 15:18:11.497848: step 34290, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.848 sec/batch; 70h:12m:57s remains)
INFO - root - 2017-12-09 15:18:20.243470: step 34300, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 71h:58m:41s remains)
2017-12-09 15:18:21.090763: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.019872179 0.019642053 0.019508457 0.019143928 0.01843461 0.017289346 0.016101746 0.01526302 0.01518651 0.016138103 0.017486926 0.019085089 0.02045325 0.021232681 0.021238541][0.021879962 0.022142898 0.02224002 0.021772342 0.020666419 0.01880339 0.016657773 0.014887736 0.014073043 0.01452856 0.015712079 0.01737335 0.018914675 0.019928217 0.020059951][0.023815578 0.024472274 0.024637496 0.023937635 0.022329852 0.019659176 0.016501663 0.013774944 0.012217056 0.012154055 0.013207012 0.014822944 0.016464444 0.017585531 0.017862933][0.025404522 0.02641549 0.026569786 0.025552047 0.023295676 0.019723546 0.015543886 0.011913029 0.0097762579 0.0093519315 0.010215442 0.011761053 0.013347979 0.014499926 0.015046268][0.026235804 0.027619628 0.027752044 0.026397713 0.023467377 0.019051233 0.013965976 0.0096419584 0.0070928545 0.0064446512 0.0072164997 0.0085805943 0.0099387942 0.010935125 0.011554407][0.025678795 0.027471157 0.027753128 0.026266819 0.02284744 0.017843116 0.012193886 0.0075173951 0.0047681965 0.0039319331 0.004487874 0.0055868817 0.0066257208 0.0074029337 0.0080733215][0.023696676 0.025777837 0.026304577 0.024979686 0.021465354 0.016315855 0.010507719 0.0057926015 0.0029941211 0.0020353845 0.0022951253 0.0030078464 0.0037480625 0.0043647448 0.00509545][0.020703819 0.022969026 0.023741737 0.02266529 0.019312525 0.014465767 0.009033856 0.00465126 0.0019536014 0.00085003173 0.00074279204 0.0010914855 0.001501955 0.0019406629 0.0027663382][0.017040968 0.01935599 0.020321917 0.019531189 0.016600959 0.012431055 0.0077786432 0.0040747575 0.0016140604 0.00035133364 -0.00016230543 -0.00024549128 -0.00016764668 0.00012992381 0.00097356935][0.013170073 0.0154584 0.01660813 0.016212607 0.013943276 0.010660003 0.0070277941 0.0041088136 0.0020079357 0.00063613208 -0.0002643998 -0.00077484979 -0.0010258532 -0.00096413429 -0.00026709749][0.009273218 0.0114092 0.012704141 0.012809816 0.011401057 0.0092424788 0.0067842905 0.0046988619 0.0029196381 0.0014490943 0.00024186575 -0.00062983448 -0.0011669117 -0.0013312134 -0.00084345159][0.0057986868 0.0076050456 0.0089107146 0.0094317766 0.00891291 0.007931998 0.0066615916 0.0054244674 0.0040317751 0.0025126748 0.0010272317 -0.00015512563 -0.00096423825 -0.0013345061 -0.0010764292][0.0031480449 0.0045230291 0.0056355665 0.0063615064 0.0065294285 0.0065598278 0.006396838 0.0059844241 0.0050119297 0.0035269591 0.0018504531 0.00036961085 -0.00068503083 -0.001233211 -0.0011833499][0.0012996335 0.0022462341 0.0031132395 0.0039236448 0.0045483261 0.0052560177 0.0058548925 0.0060554254 0.00546624 0.0040936437 0.0023541427 0.00072204543 -0.00048640848 -0.0011724855 -0.0013066879][0.00024934008 0.00087137648 0.0015062935 0.0022528213 0.0031164414 0.0041861068 0.0051813638 0.005704287 0.0054019913 0.004199422 0.0025060331 0.0008337592 -0.00043006963 -0.001191453 -0.0014503563]]...]
INFO - root - 2017-12-09 15:18:29.767301: step 34310, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.877 sec/batch; 72h:40m:09s remains)
INFO - root - 2017-12-09 15:18:38.271950: step 34320, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.875 sec/batch; 72h:27m:01s remains)
INFO - root - 2017-12-09 15:18:46.855314: step 34330, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.848 sec/batch; 70h:14m:37s remains)
INFO - root - 2017-12-09 15:18:55.384328: step 34340, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.846 sec/batch; 70h:04m:59s remains)
INFO - root - 2017-12-09 15:19:04.023110: step 34350, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.857 sec/batch; 70h:59m:17s remains)
INFO - root - 2017-12-09 15:19:12.854702: step 34360, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.903 sec/batch; 74h:48m:23s remains)
INFO - root - 2017-12-09 15:19:21.656350: step 34370, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.854 sec/batch; 70h:42m:40s remains)
INFO - root - 2017-12-09 15:19:30.214873: step 34380, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.837 sec/batch; 69h:17m:46s remains)
INFO - root - 2017-12-09 15:19:38.627569: step 34390, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 71h:23m:25s remains)
INFO - root - 2017-12-09 15:19:47.380669: step 34400, loss = 0.81, batch loss = 0.68 (9.2 examples/sec; 0.866 sec/batch; 71h:45m:01s remains)
2017-12-09 15:19:48.295458: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.29965398 0.30001521 0.29595083 0.29030228 0.28260171 0.27408072 0.2654219 0.25686795 0.24891773 0.24175242 0.23556374 0.22908519 0.22351775 0.21973504 0.21657552][0.29143777 0.29410571 0.29142007 0.28692403 0.2806139 0.27273545 0.26486042 0.25723511 0.2499955 0.24317639 0.23742127 0.23164015 0.22616704 0.22194886 0.21742626][0.2804102 0.28531304 0.28461289 0.28140306 0.27644241 0.26975471 0.26301929 0.2561141 0.24968091 0.2433776 0.23800866 0.23290178 0.22776175 0.22314815 0.21690913][0.26922834 0.27718475 0.27886716 0.27724221 0.27347937 0.2679162 0.262323 0.2564815 0.25104383 0.245889 0.24162535 0.23780559 0.23334591 0.22793643 0.21975157][0.26300457 0.27338484 0.27713889 0.27746576 0.27553642 0.2712945 0.26687947 0.26173288 0.25711477 0.25281176 0.24835679 0.24501453 0.24083491 0.23518711 0.22521165][0.26054981 0.27335042 0.27923366 0.28090206 0.27993056 0.27665448 0.27325654 0.26964137 0.26573947 0.26200482 0.25768214 0.25413865 0.2492364 0.24260439 0.23089662][0.25827706 0.27323434 0.280647 0.28395069 0.28429291 0.28189838 0.2793006 0.27627012 0.2730251 0.26979771 0.26536998 0.26183471 0.25692591 0.24975982 0.23658191][0.25746608 0.27423087 0.28269717 0.28634229 0.28662533 0.2844359 0.28191084 0.27932319 0.27645382 0.27347705 0.26908231 0.26541841 0.26039711 0.25278327 0.23863757][0.25365487 0.27183691 0.28094643 0.28517568 0.2860567 0.2842963 0.28181595 0.27954838 0.27669886 0.27411103 0.26983833 0.26590565 0.26063216 0.25227478 0.23771939][0.2496722 0.26822907 0.27739474 0.28186148 0.28329226 0.28149605 0.27868739 0.27604818 0.27294296 0.26987723 0.26506087 0.26058784 0.25491655 0.24657518 0.23239595][0.24079394 0.25944728 0.26865828 0.2725552 0.27365503 0.27212772 0.26950067 0.26664779 0.26322663 0.25989833 0.254957 0.25002563 0.24425125 0.23609592 0.22291528][0.23106688 0.24848425 0.25664371 0.25972742 0.26010069 0.25856918 0.25602907 0.25314376 0.25006324 0.24704207 0.24243742 0.23748827 0.23243144 0.22516933 0.2135798][0.21981467 0.23538356 0.24246153 0.24439755 0.24381182 0.24212523 0.23976065 0.23715213 0.23406738 0.23231365 0.22924525 0.22514212 0.22112623 0.21518391 0.20591323][0.21060742 0.22391389 0.22917429 0.23075266 0.23003587 0.22867677 0.22673042 0.22448592 0.22179209 0.22055592 0.21841474 0.21605909 0.21313764 0.20883189 0.20220192][0.20609792 0.21702784 0.22037002 0.22108059 0.22005844 0.21845083 0.21670155 0.21462888 0.21230897 0.21120003 0.20961425 0.20848136 0.2067851 0.20441993 0.20025311]]...]
INFO - root - 2017-12-09 15:19:56.926599: step 34410, loss = 0.83, batch loss = 0.70 (9.1 examples/sec; 0.879 sec/batch; 72h:49m:26s remains)
INFO - root - 2017-12-09 15:20:05.477179: step 34420, loss = 0.82, batch loss = 0.69 (8.8 examples/sec; 0.912 sec/batch; 75h:29m:17s remains)
INFO - root - 2017-12-09 15:20:14.265360: step 34430, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.873 sec/batch; 72h:16m:09s remains)
INFO - root - 2017-12-09 15:20:22.816355: step 34440, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 71h:50m:32s remains)
INFO - root - 2017-12-09 15:20:31.596385: step 34450, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.865 sec/batch; 71h:37m:55s remains)
INFO - root - 2017-12-09 15:20:40.201587: step 34460, loss = 0.83, batch loss = 0.70 (9.3 examples/sec; 0.862 sec/batch; 71h:22m:46s remains)
INFO - root - 2017-12-09 15:20:48.564905: step 34470, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.836 sec/batch; 69h:14m:33s remains)
INFO - root - 2017-12-09 15:20:57.045934: step 34480, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.849 sec/batch; 70h:15m:13s remains)
INFO - root - 2017-12-09 15:21:05.552480: step 34490, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.863 sec/batch; 71h:24m:04s remains)
INFO - root - 2017-12-09 15:21:14.157427: step 34500, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.857 sec/batch; 70h:57m:16s remains)
2017-12-09 15:21:15.018384: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0017946907 -0.0017441107 -0.0016690851 -0.0015738246 -0.0014664568 -0.0013463246 -0.0012058191 -0.0010661691 -0.0009845047 -0.0010224474 -0.0011883187 -0.0014177572 -0.0016207726 -0.0017472949 -0.0018040757][-0.0017966734 -0.0017372542 -0.0016363286 -0.0014874033 -0.0013040107 -0.0011082925 -0.00091993046 -0.00077241356 -0.00072335743 -0.00082175108 -0.0010586069 -0.001351084 -0.0015967482 -0.001742996 -0.0018044037][-0.0017961424 -0.0017115381 -0.0015469299 -0.0012965094 -0.00099968433 -0.00071420765 -0.00049542857 -0.00038062478 -0.00040943956 -0.00060535443 -0.00093462027 -0.0012975251 -0.0015830507 -0.0017439844 -0.0018073256][-0.0017827825 -0.0016468819 -0.0013732705 -0.00097447122 -0.0005416394 -0.00017967413 2.823933e-05 5.57726e-05 -0.00010002416 -0.00042274583 -0.00085063733 -0.0012737903 -0.0015847076 -0.0017501153 -0.0018110215][-0.001752055 -0.0015452541 -0.0011420462 -0.00058437721 -2.1675834e-05 0.00039197085 0.00055278884 0.00045360322 0.00014016032 -0.00032231596 -0.00083836133 -0.0012962536 -0.0016073757 -0.0017623545 -0.0018160532][-0.0017075418 -0.001434108 -0.00092264329 -0.00023976108 0.00042287586 0.00086867227 0.00097311311 0.00074114394 0.00026604952 -0.00032824615 -0.00090835453 -0.0013665035 -0.0016491619 -0.0017787227 -0.0018195533][-0.0016560168 -0.0013463951 -0.00078143342 -3.6105746e-05 0.00068273046 0.0011464923 0.0012037365 0.00086205848 0.00025622197 -0.00043266837 -0.0010390629 -0.0014649943 -0.0016992451 -0.0017952579 -0.0018220154][-0.0016101706 -0.0013047748 -0.00074754003 -4.9362425e-06 0.00071430556 0.0011649127 0.0011783282 0.000763529 8.8432571e-05 -0.00062657625 -0.0012034308 -0.0015667652 -0.001744069 -0.0018079244 -0.0018234001][-0.0015887852 -0.0013231698 -0.00082587881 -0.00014866889 0.0005103955 0.00090443343 0.00087006483 0.00042692479 -0.00023642066 -0.00089265971 -0.001379453 -0.0016562749 -0.0017767757 -0.0018149548 -0.0018228794][-0.0016099298 -0.0014030822 -0.0010056604 -0.00044934533 9.3511189e-05 0.00039776741 0.00032376009 -9.3106064e-05 -0.00066465477 -0.0011882532 -0.0015445866 -0.001728215 -0.0017997962 -0.0018194276 -0.0018225407][-0.0016711752 -0.0015296409 -0.0012535825 -0.00085639791 -0.00046615 -0.0002612659 -0.00034644827 -0.0006800798 -0.0011005504 -0.0014550316 -0.0016753152 -0.0017781095 -0.0018139776 -0.0018225404 -0.001823105][-0.0017445673 -0.0016627966 -0.0015030885 -0.0012669401 -0.0010328806 -0.00091600895 -0.00098249875 -0.0011978522 -0.0014508195 -0.001648193 -0.00176054 -0.0018077998 -0.0018221901 -0.0018248109 -0.0018244497][-0.0017987946 -0.0017615616 -0.0016884559 -0.001577659 -0.0014666693 -0.0014131139 -0.0014492398 -0.0015552314 -0.0016732117 -0.0017596941 -0.0018049018 -0.0018218018 -0.0018259474 -0.0018261694 -0.0018256244][-0.0018239361 -0.0018116181 -0.0017871684 -0.0017492515 -0.0017104796 -0.0016914456 -0.001704372 -0.0017412312 -0.0017809266 -0.0018089893 -0.001822749 -0.0018273132 -0.0018277636 -0.0018270427 -0.0018262282][-0.0018299688 -0.001827747 -0.0018226443 -0.0018139241 -0.0018049191 -0.0017997667 -0.0018017262 -0.0018094908 -0.0018181643 -0.0018244619 -0.0018275534 -0.0018284438 -0.0018281389 -0.0018274356 -0.0018268503]]...]
INFO - root - 2017-12-09 15:21:23.696745: step 34510, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 72h:06m:56s remains)
INFO - root - 2017-12-09 15:21:32.221412: step 34520, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.883 sec/batch; 73h:06m:46s remains)
INFO - root - 2017-12-09 15:21:40.894257: step 34530, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.881 sec/batch; 72h:53m:16s remains)
INFO - root - 2017-12-09 15:21:49.547803: step 34540, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.847 sec/batch; 70h:06m:16s remains)
INFO - root - 2017-12-09 15:21:58.077053: step 34550, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.851 sec/batch; 70h:26m:50s remains)
INFO - root - 2017-12-09 15:22:06.549743: step 34560, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.865 sec/batch; 71h:37m:43s remains)
INFO - root - 2017-12-09 15:22:15.179254: step 34570, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.860 sec/batch; 71h:11m:38s remains)
INFO - root - 2017-12-09 15:22:23.653841: step 34580, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.873 sec/batch; 72h:15m:43s remains)
INFO - root - 2017-12-09 15:22:32.143759: step 34590, loss = 0.82, batch loss = 0.69 (10.3 examples/sec; 0.775 sec/batch; 64h:08m:17s remains)
INFO - root - 2017-12-09 15:22:40.862658: step 34600, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.882 sec/batch; 73h:00m:26s remains)
2017-12-09 15:22:41.752321: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0014907994 -0.0013745213 -0.0012353864 -0.0010826478 -0.00094099651 -0.00083832105 -0.0007896208 -0.00080271834 -0.00087849819 -0.0010075558 -0.0011722393 -0.0013527266 -0.0015283673 -0.0016746593 -0.0017711279][-0.0014631776 -0.0013415825 -0.001189738 -0.0010154302 -0.00084309711 -0.00070328917 -0.00061950553 -0.00061057624 -0.00068363047 -0.00082937686 -0.0010286326 -0.0012541644 -0.0014732169 -0.0016505037 -0.0017631456][-0.0015399584 -0.0014336989 -0.0012854023 -0.0010959903 -0.00088639749 -0.000694462 -0.00056280789 -0.00052767887 -0.00059987011 -0.00076264271 -0.00098847016 -0.0012407596 -0.0014782943 -0.0016607947 -0.0017702261][-0.0016688323 -0.0015920221 -0.0014618795 -0.0012636302 -0.0010088977 -0.0007473391 -0.00055307278 -0.00049020781 -0.00057328807 -0.00076880294 -0.0010268206 -0.0012958066 -0.0015298498 -0.0016950801 -0.0017862101][-0.0017722878 -0.0017254354 -0.0016206116 -0.0014239836 -0.0011317846 -0.00080240215 -0.000546637 -0.00046448573 -0.00057586911 -0.00082320673 -0.0011208472 -0.0013967698 -0.0016080331 -0.0017397898 -0.0018044563][-0.0018208369 -0.0017940567 -0.0017139139 -0.0015339463 -0.0012338688 -0.00087011163 -0.00057884038 -0.00049297675 -0.00063788041 -0.00093202939 -0.0012541693 -0.0015165511 -0.0016888261 -0.0017803092 -0.0018188157][-0.0018333705 -0.0018183123 -0.0017609668 -0.0016144594 -0.0013480915 -0.0010049231 -0.000723497 -0.00065116317 -0.00081302039 -0.0011145312 -0.0014166699 -0.00163387 -0.0017548953 -0.0018079771 -0.0018264853][-0.0018339256 -0.0018255069 -0.0017880545 -0.0016840909 -0.001481993 -0.0012088292 -0.00098081387 -0.00093027746 -0.0010802344 -0.0013379324 -0.0015750168 -0.0017261355 -0.0017972001 -0.0018219358 -0.0018288164][-0.0018336839 -0.0018299171 -0.0018097685 -0.0017478411 -0.0016201901 -0.0014402749 -0.0012871365 -0.0012564547 -0.0013662546 -0.0015442154 -0.0016968352 -0.0017838796 -0.001817997 -0.0018269973 -0.0018290943][-0.0018333547 -0.0018320953 -0.0018237357 -0.0017939877 -0.0017282101 -0.001631813 -0.0015485281 -0.0015335636 -0.0015969253 -0.0016944389 -0.0017729724 -0.0018133678 -0.0018264082 -0.0018288051 -0.0018293859][-0.0018329327 -0.0018326078 -0.0018301366 -0.0018193247 -0.0017927728 -0.0017523429 -0.0017170948 -0.0017110447 -0.0017380497 -0.0017783607 -0.0018096006 -0.0018245616 -0.0018288749 -0.0018296187 -0.0018300009][-0.0018325191 -0.001832475 -0.0018322264 -0.001829754 -0.0018224051 -0.0018102876 -0.0017994426 -0.0017973472 -0.0018049646 -0.0018164604 -0.0018252088 -0.00182918 -0.0018301792 -0.0018304215 -0.0018307444][-0.0018322907 -0.00183256 -0.0018329967 -0.001833009 -0.0018319703 -0.0018297882 -0.0018277623 -0.0018272157 -0.00182827 -0.0018299288 -0.0018311068 -0.0018314917 -0.0018314997 -0.0018316008 -0.0018319011][-0.001832769 -0.0018332585 -0.0018339086 -0.0018346045 -0.0018351136 -0.0018353539 -0.001835471 -0.0018353016 -0.001834769 -0.0018340347 -0.0018334209 -0.0018329165 -0.0018326339 -0.0018326642 -0.0018329474][-0.0018341247 -0.00183469 -0.0018354747 -0.0018362592 -0.0018368451 -0.0018371787 -0.0018372934 -0.0018370714 -0.0018364791 -0.0018355997 -0.0018346739 -0.0018338568 -0.0018333624 -0.0018332932 -0.0018335186]]...]
INFO - root - 2017-12-09 15:22:50.398250: step 34610, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.875 sec/batch; 72h:25m:02s remains)
INFO - root - 2017-12-09 15:22:59.044374: step 34620, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.866 sec/batch; 71h:41m:33s remains)
INFO - root - 2017-12-09 15:23:07.693983: step 34630, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 71h:54m:26s remains)
INFO - root - 2017-12-09 15:23:16.344022: step 34640, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.874 sec/batch; 72h:17m:11s remains)
INFO - root - 2017-12-09 15:23:25.180489: step 34650, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 71h:53m:05s remains)
INFO - root - 2017-12-09 15:23:33.987515: step 34660, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.889 sec/batch; 73h:33m:28s remains)
INFO - root - 2017-12-09 15:23:42.707508: step 34670, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.890 sec/batch; 73h:36m:22s remains)
INFO - root - 2017-12-09 15:23:51.260253: step 34680, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.866 sec/batch; 71h:40m:51s remains)
INFO - root - 2017-12-09 15:23:59.935493: step 34690, loss = 0.82, batch loss = 0.69 (10.7 examples/sec; 0.745 sec/batch; 61h:38m:31s remains)
INFO - root - 2017-12-09 15:24:08.576596: step 34700, loss = 0.83, batch loss = 0.70 (9.3 examples/sec; 0.857 sec/batch; 70h:55m:32s remains)
2017-12-09 15:24:09.509160: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.26112452 0.27872726 0.291973 0.30278236 0.3114233 0.315242 0.3185198 0.318939 0.31570625 0.31196019 0.3104659 0.31085506 0.31387609 0.31649405 0.32132444][0.25499684 0.27647054 0.29365781 0.31084916 0.32656765 0.33664572 0.34583914 0.35121688 0.35466444 0.35621011 0.35952932 0.36408064 0.37088802 0.37544876 0.38084832][0.24370511 0.26542178 0.28429949 0.3049821 0.32454818 0.34183243 0.35771382 0.37040409 0.38218185 0.39064491 0.40131813 0.41096151 0.42188266 0.42765787 0.43289408][0.22736765 0.24708703 0.26411632 0.28354427 0.30339673 0.3251611 0.34632951 0.36783656 0.38857508 0.40658364 0.42550978 0.44302383 0.45926473 0.4658308 0.470415][0.20827807 0.22444196 0.23849729 0.25431728 0.27260831 0.29573056 0.32235447 0.35095888 0.37997437 0.4083586 0.43636471 0.46111381 0.48267031 0.49160194 0.49623704][0.19072016 0.19994518 0.20891166 0.22032678 0.2359006 0.25883543 0.28840697 0.32296449 0.35890356 0.39513344 0.43034196 0.46136162 0.48592556 0.49799916 0.50236183][0.17869937 0.18114497 0.18260907 0.18901126 0.20076291 0.22344129 0.25459349 0.29375452 0.33518413 0.37708139 0.4174374 0.45233732 0.47919407 0.49307239 0.49697241][0.17092054 0.16825454 0.16365105 0.16391188 0.17057893 0.19110116 0.22170889 0.2626332 0.3071999 0.35197261 0.39492011 0.43161634 0.45937544 0.47426084 0.47778037][0.16482706 0.1598938 0.15207841 0.14683709 0.14869481 0.16494739 0.1933808 0.23268622 0.27597725 0.32126582 0.36521578 0.40289024 0.43050644 0.44589543 0.44923881][0.16358985 0.15786476 0.14820027 0.14057954 0.1386897 0.15044782 0.1746079 0.20967509 0.24959786 0.29130569 0.33231857 0.36821371 0.39408261 0.40879795 0.41143456][0.16389926 0.15950647 0.15008207 0.14158493 0.13771868 0.14634094 0.16600515 0.19530416 0.22933392 0.26545784 0.301043 0.33145115 0.35278749 0.36440805 0.36525154][0.16531786 0.1637315 0.15668923 0.14969207 0.14534122 0.15067147 0.16503249 0.18792044 0.21492963 0.24356656 0.27163565 0.29515225 0.31191638 0.31945798 0.31926784][0.16329837 0.164933 0.16147147 0.1571774 0.15416048 0.15763497 0.16772947 0.18406761 0.20341736 0.22445405 0.24483904 0.26148519 0.27282423 0.27709109 0.27624625][0.15919842 0.16312604 0.16248329 0.16032463 0.15811554 0.16001706 0.1664038 0.17734283 0.19036053 0.2054036 0.22017309 0.23169681 0.2391178 0.24097155 0.2393222][0.15539508 0.15958691 0.15979344 0.15910754 0.15749761 0.1577165 0.16088323 0.16781583 0.17674118 0.18704168 0.19704843 0.20427081 0.20856287 0.20890947 0.20662209]]...]
INFO - root - 2017-12-09 15:24:18.151166: step 34710, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.878 sec/batch; 72h:38m:52s remains)
INFO - root - 2017-12-09 15:24:26.632488: step 34720, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 71h:50m:23s remains)
INFO - root - 2017-12-09 15:24:35.237524: step 34730, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.838 sec/batch; 69h:20m:40s remains)
INFO - root - 2017-12-09 15:24:44.009057: step 34740, loss = 0.81, batch loss = 0.68 (8.9 examples/sec; 0.897 sec/batch; 74h:12m:23s remains)
INFO - root - 2017-12-09 15:24:52.575358: step 34750, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 71h:52m:29s remains)
INFO - root - 2017-12-09 15:25:01.180353: step 34760, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.885 sec/batch; 73h:13m:26s remains)
INFO - root - 2017-12-09 15:25:09.831193: step 34770, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.883 sec/batch; 73h:03m:20s remains)
INFO - root - 2017-12-09 15:25:18.206403: step 34780, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.842 sec/batch; 69h:39m:42s remains)
INFO - root - 2017-12-09 15:25:26.848987: step 34790, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.856 sec/batch; 70h:48m:54s remains)
INFO - root - 2017-12-09 15:25:35.354750: step 34800, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.831 sec/batch; 68h:41m:17s remains)
2017-12-09 15:25:36.279768: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0018016523 -0.0017994881 -0.0017996355 -0.0018011188 -0.0018030941 -0.0018063969 -0.0018112288 -0.0018169214 -0.0018224826 -0.001826711 -0.0018287624 -0.0018288177 -0.0018272264 -0.0018247047 -0.0018220869][-0.0018005938 -0.0017983662 -0.0017981537 -0.0017992929 -0.0018011362 -0.0018043962 -0.0018093238 -0.0018152614 -0.001821087 -0.0018254613 -0.001827665 -0.0018274934 -0.0018258506 -0.0018238754 -0.001822605][-0.0018008129 -0.0017983733 -0.0017976698 -0.0017985068 -0.0018000405 -0.0018028478 -0.0018073957 -0.0018132108 -0.0018189471 -0.0018227387 -0.0018234827 -0.0018206361 -0.0018179243 -0.0018177568 -0.0018206384][-0.001801067 -0.0017987154 -0.001797702 -0.0017980621 -0.0017991636 -0.0018013362 -0.0018050887 -0.0018100797 -0.0018155219 -0.0018169766 -0.0018125271 -0.0018019179 -0.00179748 -0.0018024722 -0.0018141813][-0.0018014496 -0.0017993083 -0.0017982553 -0.0017983192 -0.0017990948 -0.001800605 -0.0018034044 -0.0018071075 -0.0018106552 -0.0018057104 -0.0017898069 -0.0017663593 -0.0017625761 -0.0017785506 -0.0018026804][-0.001801816 -0.0018002585 -0.0017995554 -0.0017996924 -0.0018003415 -0.0018013751 -0.0018031222 -0.0018048224 -0.0018043331 -0.0017891787 -0.0017607633 -0.0017289041 -0.0017305766 -0.0017579347 -0.0017901385][-0.0018026261 -0.0018018063 -0.0018018711 -0.0018025943 -0.0018034956 -0.0018041816 -0.0018049199 -0.0018047652 -0.0017993135 -0.0017749169 -0.0017405058 -0.0017098355 -0.0017153217 -0.0017447727 -0.0017789011][-0.0018036939 -0.001804047 -0.0018053107 -0.0018069231 -0.0018081034 -0.0018084705 -0.0018082471 -0.0018070396 -0.0017997897 -0.0017752118 -0.001746262 -0.0017251746 -0.0017305806 -0.0017512051 -0.0017763909][-0.0018049707 -0.0018061182 -0.0018084248 -0.0018107212 -0.0018121552 -0.0018123832 -0.0018115975 -0.0018097723 -0.0018038389 -0.0017886738 -0.001771431 -0.0017593226 -0.0017615365 -0.00177123 -0.0017834972][-0.0018058834 -0.0018075445 -0.0018100407 -0.0018123562 -0.0018142692 -0.0018152646 -0.0018145763 -0.0018125387 -0.0018084699 -0.0018016107 -0.0017939114 -0.0017875868 -0.0017868729 -0.0017889641 -0.0017926313][-0.0018065822 -0.0018077634 -0.0018071568 -0.001806438 -0.0018087632 -0.0018130462 -0.0018153745 -0.0018141848 -0.0018113926 -0.0018078625 -0.001803613 -0.0017994925 -0.0017970622 -0.0017961966 -0.0017963683][-0.0018058909 -0.0018035249 -0.001792128 -0.0017796982 -0.0017820735 -0.001798159 -0.0018117244 -0.0018148626 -0.0018130626 -0.0018097272 -0.001805491 -0.0018013078 -0.0017982448 -0.0017965888 -0.0017960584][-0.0018014871 -0.0017890681 -0.0017539426 -0.0017181098 -0.0017194579 -0.0017594629 -0.0017990585 -0.0018125915 -0.0018133919 -0.0018110294 -0.0018072602 -0.0018028805 -0.001799506 -0.001797479 -0.0017965252][-0.0017901869 -0.001758017 -0.0016937889 -0.0016308872 -0.0016346131 -0.0017038246 -0.0017786123 -0.0018083114 -0.0018129186 -0.0018117926 -0.001808849 -0.0018046184 -0.0018008238 -0.0017983933 -0.0017972421][-0.0017678343 -0.0017063636 -0.0016165588 -0.0015453142 -0.0015636282 -0.0016535341 -0.001759269 -0.001804062 -0.0018120452 -0.0018117821 -0.0018090695 -0.0018050594 -0.0018013336 -0.0017989364 -0.0017977462]]...]
INFO - root - 2017-12-09 15:25:44.935320: step 34810, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.872 sec/batch; 72h:07m:24s remains)
INFO - root - 2017-12-09 15:25:53.508089: step 34820, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 72h:03m:31s remains)
INFO - root - 2017-12-09 15:26:02.257989: step 34830, loss = 0.82, batch loss = 0.69 (8.8 examples/sec; 0.912 sec/batch; 75h:22m:43s remains)
INFO - root - 2017-12-09 15:26:10.998015: step 34840, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.858 sec/batch; 70h:57m:10s remains)
INFO - root - 2017-12-09 15:26:19.633101: step 34850, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.866 sec/batch; 71h:36m:12s remains)
INFO - root - 2017-12-09 15:26:28.163814: step 34860, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 72h:02m:59s remains)
INFO - root - 2017-12-09 15:26:36.804683: step 34870, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.857 sec/batch; 70h:53m:05s remains)
INFO - root - 2017-12-09 15:26:45.252707: step 34880, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.849 sec/batch; 70h:12m:26s remains)
INFO - root - 2017-12-09 15:26:53.749303: step 34890, loss = 0.82, batch loss = 0.69 (9.7 examples/sec; 0.829 sec/batch; 68h:30m:59s remains)
INFO - root - 2017-12-09 15:27:02.142811: step 34900, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.855 sec/batch; 70h:40m:23s remains)
2017-12-09 15:27:03.017195: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0018281689 -0.0018267966 -0.0018250673 -0.0018227485 -0.0018195298 -0.0018155846 -0.0018111671 -0.0018069113 -0.0018041596 -0.0018025165 -0.0018017534 -0.0018018447 -0.001802557 -0.0018040002 -0.0018063082][-0.0018254521 -0.0018242403 -0.0018227231 -0.001820625 -0.0018176553 -0.0018139658 -0.00180988 -0.0018061568 -0.0018037159 -0.0018020261 -0.0018010486 -0.0018011329 -0.0018019469 -0.0018033915 -0.001805795][-0.0018229277 -0.0018217764 -0.0018204212 -0.0018184752 -0.0018157543 -0.0018124538 -0.0018088372 -0.0018057042 -0.0018036787 -0.0018021123 -0.0018010095 -0.001800956 -0.0018016901 -0.0018030773 -0.001805464][-0.0018199537 -0.001818854 -0.0018176924 -0.0018159779 -0.0018136178 -0.0018107344 -0.0018076949 -0.0018051486 -0.0018034915 -0.0018020897 -0.0018010056 -0.0018007708 -0.0018013354 -0.0018026317 -0.0018048779][-0.0018166291 -0.0018156664 -0.0018147338 -0.0018132478 -0.0018112884 -0.0018090373 -0.0018066603 -0.0018046311 -0.0018032301 -0.0018019613 -0.0018009152 -0.0018005403 -0.0018009406 -0.0018020772 -0.0018040022][-0.0018135629 -0.0018126592 -0.0018119303 -0.0018106706 -0.001809055 -0.0018073916 -0.0018056832 -0.001804139 -0.0018028818 -0.0018017209 -0.001800749 -0.0018002978 -0.0018005623 -0.00180149 -0.0018029986][-0.0018108948 -0.0018099592 -0.0018094075 -0.001808426 -0.001807112 -0.001805876 -0.0018047638 -0.0018035726 -0.0018024669 -0.0018014333 -0.0018005823 -0.001800118 -0.0018002832 -0.001801037 -0.0018021778][-0.0018088494 -0.0018077627 -0.0018072536 -0.0018065464 -0.0018055772 -0.0018046546 -0.0018038984 -0.0018030163 -0.0018021106 -0.0018012305 -0.0018005166 -0.0018001047 -0.0018002038 -0.0018008392 -0.0018017279][-0.0018075203 -0.0018061831 -0.0018056203 -0.0018051002 -0.0018044919 -0.0018038511 -0.0018032603 -0.0018026348 -0.0018019553 -0.0018012623 -0.0018006864 -0.0018003476 -0.0018004102 -0.0018008893 -0.0018015127][-0.0018065935 -0.0018050125 -0.0018043678 -0.0018040392 -0.0018038112 -0.0018034781 -0.0018030333 -0.0018025568 -0.0018020732 -0.0018015362 -0.001801058 -0.0018008065 -0.0018008616 -0.0018011809 -0.001801532][-0.0018059514 -0.0018042437 -0.0018035618 -0.0018034317 -0.0018035048 -0.0018034241 -0.0018030707 -0.0018026704 -0.0018023432 -0.0018019588 -0.001801596 -0.0018014035 -0.0018014224 -0.0018016092 -0.0018017489][-0.0018056139 -0.0018038353 -0.0018032113 -0.001803244 -0.0018034888 -0.001803571 -0.001803245 -0.0018028857 -0.001802658 -0.0018024195 -0.0018021952 -0.0018020642 -0.0018020192 -0.0018020433 -0.0018020023][-0.001805282 -0.0018035914 -0.0018030253 -0.0018031499 -0.001803441 -0.0018035712 -0.0018033105 -0.0018030257 -0.0018028982 -0.0018028056 -0.0018027431 -0.0018026748 -0.0018025829 -0.0018024548 -0.0018022851][-0.0018050021 -0.0018035109 -0.0018030251 -0.0018031891 -0.0018034441 -0.0018035179 -0.0018032747 -0.0018030596 -0.0018029956 -0.0018030033 -0.0018030531 -0.0018030048 -0.0018028861 -0.001802663 -0.0018024408][-0.0018048855 -0.0018035717 -0.001803169 -0.0018032804 -0.0018034389 -0.001803441 -0.0018032136 -0.0018030829 -0.0018030808 -0.0018031416 -0.0018032497 -0.0018032033 -0.0018030441 -0.001802744 -0.0018024726]]...]
INFO - root - 2017-12-09 15:27:11.702784: step 34910, loss = 0.81, batch loss = 0.68 (9.5 examples/sec; 0.846 sec/batch; 69h:57m:51s remains)
INFO - root - 2017-12-09 15:27:20.133244: step 34920, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.857 sec/batch; 70h:49m:40s remains)
INFO - root - 2017-12-09 15:27:28.784084: step 34930, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.857 sec/batch; 70h:52m:00s remains)
INFO - root - 2017-12-09 15:27:37.451266: step 34940, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 72h:01m:45s remains)
INFO - root - 2017-12-09 15:27:46.069494: step 34950, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.866 sec/batch; 71h:36m:47s remains)
INFO - root - 2017-12-09 15:27:54.654371: step 34960, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 71h:59m:44s remains)
INFO - root - 2017-12-09 15:28:03.196822: step 34970, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.856 sec/batch; 70h:46m:04s remains)
INFO - root - 2017-12-09 15:28:11.582037: step 34980, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.873 sec/batch; 72h:10m:28s remains)
INFO - root - 2017-12-09 15:28:20.190480: step 34990, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.873 sec/batch; 72h:08m:51s remains)
INFO - root - 2017-12-09 15:28:28.750413: step 35000, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.859 sec/batch; 70h:57m:47s remains)
2017-12-09 15:28:29.605375: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0018304865 -0.0018301241 -0.0018305572 -0.0018309023 -0.0018309198 -0.0018307697 -0.0018305554 -0.0018302065 -0.0018297408 -0.0018292901 -0.0018289147 -0.001828625 -0.0018285118 -0.0018286164 -0.0018288121][-0.0018297242 -0.0018294699 -0.0018300746 -0.0018306547 -0.0018309306 -0.0018310004 -0.0018310109 -0.0018307414 -0.0018302287 -0.0018295648 -0.0018289497 -0.0018284282 -0.0018281369 -0.0018280691 -0.00182814][-0.0018299165 -0.00183006 -0.0018309831 -0.00183179 -0.0018322899 -0.0018325603 -0.001832641 -0.0018322926 -0.0018316063 -0.0018307114 -0.0018298313 -0.001829074 -0.001828599 -0.0018283824 -0.0018283247][-0.0018302028 -0.0018306104 -0.0018318378 -0.0018329682 -0.001833819 -0.0018343099 -0.0018342907 -0.0018335965 -0.0018326121 -0.0018316059 -0.001830633 -0.001829754 -0.0018291035 -0.001828769 -0.0018286506][-0.0018305811 -0.0018312136 -0.0018326858 -0.001834029 -0.0018349573 -0.0018352829 -0.0018348375 -0.0018337055 -0.0018325754 -0.001831868 -0.001831268 -0.00183046 -0.0018296606 -0.001829144 -0.0018289638][-0.0018306163 -0.0018315592 -0.0018331899 -0.0018346014 -0.001835308 -0.0018349326 -0.0018336669 -0.0018317732 -0.0018304492 -0.0018305242 -0.0018310115 -0.0018307751 -0.0018299855 -0.0018293953 -0.0018291895][-0.0018304645 -0.0018316639 -0.0018332312 -0.0018342844 -0.0018342264 -0.0018327802 -0.0018302733 -0.0018273958 -0.0018260449 -0.001827366 -0.0018294826 -0.0018302952 -0.0018299966 -0.0018295347 -0.0018293605][-0.0018303309 -0.0018314666 -0.001832815 -0.0018331941 -0.0018321624 -0.0018297927 -0.0018264152 -0.0018233866 -0.0018228823 -0.0018254215 -0.001828426 -0.0018296164 -0.0018295817 -0.001829379 -0.0018295245][-0.00183021 -0.0018310363 -0.0018322071 -0.0018322007 -0.0018308217 -0.0018285484 -0.0018259176 -0.0018240316 -0.0018243541 -0.0018267054 -0.0018289255 -0.0018296811 -0.0018296742 -0.0018296986 -0.0018302085][-0.001829765 -0.0018305315 -0.0018315383 -0.0018315613 -0.001830712 -0.0018295945 -0.0018286826 -0.0018280824 -0.0018283456 -0.0018293819 -0.0018303756 -0.0018306673 -0.0018305071 -0.0018306999 -0.0018315939][-0.0018296188 -0.0018301295 -0.0018310905 -0.0018314887 -0.0018314722 -0.0018316198 -0.0018320173 -0.001832171 -0.0018321653 -0.0018322151 -0.0018322354 -0.001832035 -0.00183185 -0.0018321796 -0.0018332509][-0.0018295157 -0.0018298636 -0.001830837 -0.0018317549 -0.0018325452 -0.0018335098 -0.0018345617 -0.001835121 -0.0018350617 -0.0018347427 -0.0018343668 -0.001834019 -0.0018339377 -0.0018344277 -0.0018353529][-0.0018294307 -0.0018297166 -0.0018308415 -0.0018321718 -0.0018334477 -0.0018348109 -0.0018362197 -0.0018370908 -0.0018372322 -0.0018369635 -0.0018365044 -0.0018361229 -0.0018360692 -0.0018364836 -0.0018369611][-0.001829398 -0.0018298698 -0.0018311526 -0.0018327972 -0.0018343041 -0.0018357324 -0.0018371167 -0.001837915 -0.0018379366 -0.0018376529 -0.0018372746 -0.0018370465 -0.0018370505 -0.001837297 -0.0018373469][-0.0018295089 -0.0018302227 -0.0018316491 -0.0018333278 -0.0018347037 -0.0018357418 -0.0018364295 -0.0018361516 -0.0018351317 -0.0018342664 -0.0018340347 -0.0018342638 -0.0018348346 -0.0018355268 -0.0018358894]]...]
INFO - root - 2017-12-09 15:28:38.217066: step 35010, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.836 sec/batch; 69h:07m:17s remains)
INFO - root - 2017-12-09 15:28:46.776256: step 35020, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.874 sec/batch; 72h:14m:58s remains)
INFO - root - 2017-12-09 15:28:55.584170: step 35030, loss = 0.82, batch loss = 0.69 (8.8 examples/sec; 0.909 sec/batch; 75h:07m:31s remains)
INFO - root - 2017-12-09 15:29:04.243617: step 35040, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.877 sec/batch; 72h:25m:52s remains)
INFO - root - 2017-12-09 15:29:12.900083: step 35050, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.842 sec/batch; 69h:32m:37s remains)
INFO - root - 2017-12-09 15:29:21.629055: step 35060, loss = 0.83, batch loss = 0.70 (9.1 examples/sec; 0.880 sec/batch; 72h:41m:41s remains)
INFO - root - 2017-12-09 15:29:30.380092: step 35070, loss = 0.83, batch loss = 0.70 (9.1 examples/sec; 0.877 sec/batch; 72h:27m:16s remains)
INFO - root - 2017-12-09 15:29:38.859952: step 35080, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 71h:45m:26s remains)
INFO - root - 2017-12-09 15:29:47.369636: step 35090, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.861 sec/batch; 71h:08m:58s remains)
INFO - root - 2017-12-09 15:29:55.781910: step 35100, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.849 sec/batch; 70h:10m:27s remains)
2017-12-09 15:29:56.758424: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0010335005 -0.00026839552 0.00052706921 0.001123158 0.0013797747 0.0012560687 0.00079699233 0.00014891895 -0.00050683203 -0.0010441542 -0.0014271584 -0.0016474704 -0.0017566697 -0.0018012227 -0.0018146184][0.00026481017 0.0018051153 0.0032429055 0.0041577825 0.0043607494 0.0038717354 0.0028763011 0.001652908 0.00046336837 -0.00049443822 -0.0011559481 -0.0015312438 -0.0017149443 -0.0017896488 -0.0018124282][0.003055894 0.0059391847 0.0084569883 0.009996024 0.010274626 0.0093823411 0.0076507656 0.0054408135 0.003164876 0.0011842693 -0.00025751686 -0.0011169218 -0.0015537209 -0.0017398638 -0.0018016797][0.0071903197 0.011833813 0.015805436 0.018338675 0.019055508 0.01801512 0.015633794 0.012248465 0.0084065422 0.0047194473 0.0018025301 -9.0983e-05 -0.0011204302 -0.0015918815 -0.0017643856][0.011689411 0.018092485 0.023583136 0.027302438 0.028811194 0.028063232 0.025428882 0.021043839 0.015518578 0.0097683305 0.0049106162 0.0015507699 -0.00037917343 -0.0013185611 -0.001687957][0.015351141 0.023037709 0.029726841 0.034562681 0.037046142 0.036968268 0.034491017 0.029481523 0.022568429 0.01493491 0.0081941215 0.0033427326 0.00045951642 -0.00099622365 -0.0015926324][0.017235646 0.025373757 0.032577481 0.038062 0.041274823 0.041873466 0.039801955 0.034678649 0.027056361 0.018287733 0.01034819 0.0045378581 0.0010305834 -0.00076980982 -0.0015211622][0.016802561 0.024548164 0.031522017 0.037025809 0.040453438 0.041383877 0.039646178 0.034790464 0.027281346 0.01849295 0.010478924 0.0046079885 0.0010624218 -0.0007572748 -0.0015152646][0.014161555 0.020792047 0.026850302 0.031709831 0.034744132 0.035564873 0.034007329 0.029736616 0.023150373 0.015495045 0.008574225 0.0035615547 0.00056844903 -0.0009489471 -0.0015704862][0.010160937 0.015255161 0.01993781 0.023666017 0.025873365 0.026303083 0.02487533 0.021438994 0.016337201 0.010562281 0.0054674717 0.0018740559 -0.00021790271 -0.0012488142 -0.0016555502][0.0059713786 0.0093946215 0.012523266 0.014947973 0.016234044 0.01627088 0.015075641 0.012650064 0.0092427945 0.0055292547 0.0023715945 0.00023654057 -0.00095806277 -0.001521671 -0.0017319371][0.002503216 0.0044684755 0.006223931 0.0075135259 0.0080693662 0.0078798793 0.0070201894 0.0055598365 0.0036483973 0.0016743848 8.3863968e-05 -0.00092662993 -0.0014604048 -0.0016969425 -0.0017788771][0.00011808309 0.0010427956 0.0018405479 0.0023846531 0.0025426664 0.0023420409 0.0018524667 0.0011430536 0.00028579473 -0.00054119283 -0.0011610349 -0.0015256195 -0.0017043822 -0.0017773747 -0.0017999574][-0.0011829899 -0.00085558253 -0.00058466208 -0.00042091205 -0.0004104668 -0.000525921 -0.00072732754 -0.00097913656 -0.0012552361 -0.0014990071 -0.0016640692 -0.0017515664 -0.0017904794 -0.0018045292 -0.0018076834][-0.0016982271 -0.0016198969 -0.0015561122 -0.0015221511 -0.0015268703 -0.0015601965 -0.0016094373 -0.001664673 -0.0017203314 -0.0017649055 -0.0017915585 -0.0018044987 -0.0018095621 -0.0018104839 -0.0018099434]]...]
INFO - root - 2017-12-09 15:30:05.480277: step 35110, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.877 sec/batch; 72h:25m:50s remains)
INFO - root - 2017-12-09 15:30:14.133727: step 35120, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.853 sec/batch; 70h:28m:58s remains)
INFO - root - 2017-12-09 15:30:22.864762: step 35130, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.874 sec/batch; 72h:12m:56s remains)
INFO - root - 2017-12-09 15:30:31.600029: step 35140, loss = 0.82, batch loss = 0.69 (8.8 examples/sec; 0.908 sec/batch; 75h:00m:57s remains)
INFO - root - 2017-12-09 15:30:40.350845: step 35150, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.872 sec/batch; 72h:03m:50s remains)
INFO - root - 2017-12-09 15:30:49.014555: step 35160, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 71h:55m:24s remains)
INFO - root - 2017-12-09 15:30:57.800717: step 35170, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.843 sec/batch; 69h:38m:26s remains)
INFO - root - 2017-12-09 15:31:06.520614: step 35180, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.879 sec/batch; 72h:35m:09s remains)
INFO - root - 2017-12-09 15:31:15.201930: step 35190, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.853 sec/batch; 70h:28m:34s remains)
INFO - root - 2017-12-09 15:31:23.703240: step 35200, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.874 sec/batch; 72h:11m:58s remains)
2017-12-09 15:31:24.642208: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0018219874 -0.0018200765 -0.0018193795 -0.0018202823 -0.0018221454 -0.0018245942 -0.0018279953 -0.0018315512 -0.0018341449 -0.0018359171 -0.0018368338 -0.0018367975 -0.001835659 -0.0018343376 -0.0018329857][-0.0018213034 -0.001819835 -0.0018196234 -0.0018208777 -0.0018229118 -0.0018253227 -0.0018283057 -0.0018311901 -0.0018331715 -0.0018346397 -0.0018353919 -0.0018350846 -0.0018337559 -0.0018324419 -0.0018311702][-0.0018209117 -0.0018199497 -0.0018201685 -0.0018216862 -0.0018237926 -0.0018260647 -0.0018285549 -0.001830718 -0.0018321692 -0.0018333071 -0.0018338241 -0.0018332306 -0.0018316779 -0.0018302787 -0.0018290791][-0.0018207513 -0.0018202171 -0.0018206415 -0.0018221713 -0.0018241276 -0.0018260933 -0.0018279817 -0.0018295441 -0.0018306545 -0.0018315805 -0.0018319108 -0.0018311405 -0.0018295071 -0.0018281196 -0.0018270549][-0.0018207046 -0.001820455 -0.0018208453 -0.0018221682 -0.0018238229 -0.0018253546 -0.0018265944 -0.001827694 -0.0018286729 -0.001829579 -0.0018298675 -0.0018290679 -0.0018275175 -0.0018262941 -0.0018253719][-0.001820766 -0.0018206598 -0.0018209069 -0.0018218821 -0.0018230667 -0.0018240332 -0.0018245932 -0.0018253314 -0.0018263212 -0.0018273273 -0.0018276278 -0.0018269388 -0.0018256489 -0.0018246982 -0.0018239317][-0.0018208462 -0.0018207239 -0.0018207961 -0.0018214408 -0.0018220671 -0.0018223962 -0.0018223678 -0.001822824 -0.0018238492 -0.0018248697 -0.0018252592 -0.0018248606 -0.0018240269 -0.0018233779 -0.0018227748][-0.0018208992 -0.0018206206 -0.0018204802 -0.0018208193 -0.0018209433 -0.001820672 -0.0018201087 -0.0018204589 -0.0018215817 -0.0018224641 -0.0018229985 -0.0018230357 -0.0018226722 -0.0018223765 -0.0018219008][-0.0018207963 -0.0018203282 -0.0018200679 -0.0018201618 -0.0018199183 -0.0018192772 -0.0018184534 -0.0018188044 -0.001819956 -0.0018206831 -0.0018212601 -0.0018216913 -0.0018218062 -0.0018217168 -0.0018212799][-0.0018204721 -0.0018197855 -0.0018194331 -0.0018193678 -0.0018189936 -0.00181846 -0.0018179554 -0.0018183592 -0.0018193792 -0.0018199711 -0.0018204505 -0.0018210388 -0.00182135 -0.0018213227 -0.0018208226][-0.0018198123 -0.0018189426 -0.0018185538 -0.0018184759 -0.001818259 -0.0018182348 -0.0018183789 -0.0018189321 -0.0018197298 -0.00182011 -0.001820325 -0.0018206324 -0.0018208309 -0.0018208008 -0.0018203038][-0.0018188432 -0.0018178429 -0.0018174831 -0.0018175124 -0.001817601 -0.0018181533 -0.0018189833 -0.0018197862 -0.001820478 -0.0018207026 -0.0018206561 -0.001820553 -0.0018204474 -0.0018202988 -0.0018198208][-0.0018177041 -0.0018166642 -0.0018164249 -0.0018166159 -0.0018170273 -0.0018181163 -0.0018194863 -0.0018205371 -0.0018211507 -0.0018213061 -0.0018211589 -0.0018207721 -0.0018203799 -0.0018200674 -0.0018196061][-0.0018167547 -0.0018157664 -0.0018156311 -0.0018160633 -0.0018168035 -0.0018182357 -0.0018198648 -0.0018210177 -0.0018215366 -0.0018216333 -0.0018214218 -0.0018208728 -0.0018203196 -0.0018199381 -0.0018194946][-0.0018159888 -0.0018151712 -0.0018152532 -0.0018159936 -0.0018170205 -0.0018186141 -0.0018202433 -0.0018213844 -0.0018217886 -0.0018217283 -0.001821389 -0.0018206687 -0.0018200316 -0.0018196417 -0.0018192707]]...]
INFO - root - 2017-12-09 15:31:33.312949: step 35210, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.861 sec/batch; 71h:04m:51s remains)
INFO - root - 2017-12-09 15:31:41.762275: step 35220, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.838 sec/batch; 69h:11m:02s remains)
INFO - root - 2017-12-09 15:31:50.452477: step 35230, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.840 sec/batch; 69h:22m:33s remains)
INFO - root - 2017-12-09 15:31:59.217539: step 35240, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.879 sec/batch; 72h:33m:51s remains)
INFO - root - 2017-12-09 15:32:07.777652: step 35250, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.853 sec/batch; 70h:26m:01s remains)
INFO - root - 2017-12-09 15:32:16.410309: step 35260, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.854 sec/batch; 70h:29m:23s remains)
INFO - root - 2017-12-09 15:32:25.109317: step 35270, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.897 sec/batch; 74h:02m:18s remains)
INFO - root - 2017-12-09 15:32:33.662170: step 35280, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.887 sec/batch; 73h:14m:50s remains)
INFO - root - 2017-12-09 15:32:42.281226: step 35290, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.831 sec/batch; 68h:37m:27s remains)
INFO - root - 2017-12-09 15:32:50.804865: step 35300, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.882 sec/batch; 72h:48m:12s remains)
2017-12-09 15:32:51.687456: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0018146769 -0.001811553 -0.0018108841 -0.0018127223 -0.0018160868 -0.0018196807 -0.0018227026 -0.0018246535 -0.0018257352 -0.0018256254 -0.0018244756 -0.0018224483 -0.00181986 -0.0018171015 -0.0018139253][-0.001815663 -0.0018130838 -0.0018127229 -0.0018143804 -0.0018170696 -0.0018198644 -0.0018221706 -0.0018233433 -0.0018237506 -0.0018236196 -0.0018227686 -0.0018213775 -0.0018195944 -0.0018172988 -0.0018143264][-0.0018176086 -0.0018158592 -0.0018156262 -0.0018164552 -0.0018174564 -0.0018182378 -0.0018185076 -0.0018179528 -0.0018175032 -0.0018175498 -0.001817585 -0.0018174527 -0.0018172108 -0.0018163426 -0.0018143974][-0.0018192659 -0.0018184878 -0.0018181878 -0.0018178974 -0.0018170384 -0.0018153902 -0.0018131242 -0.0018105474 -0.0018088827 -0.0018089004 -0.0018097784 -0.0018112332 -0.0018129591 -0.001813938 -0.0018136357][-0.0018205044 -0.0018202808 -0.00181942 -0.0018177107 -0.0018147069 -0.0018105536 -0.0018058933 -0.0018015347 -0.0017988795 -0.001799064 -0.001800877 -0.0018039094 -0.0018076125 -0.0018106785 -0.0018125055][-0.0018211685 -0.0018205537 -0.0018186247 -0.00181508 -0.0018097174 -0.0018033575 -0.00179701 -0.0017916173 -0.0017884143 -0.0017890723 -0.0017920062 -0.0017968433 -0.0018025882 -0.0018072563 -0.0018107665][-0.0018218451 -0.0018200178 -0.0018164762 -0.0018112595 -0.0018044481 -0.0017969534 -0.0017898001 -0.0017840911 -0.0017811097 -0.0017823377 -0.0017862893 -0.0017924666 -0.0017990347 -0.0018050527 -0.001809848][-0.0018223973 -0.0018189462 -0.001813517 -0.0018071589 -0.0017998753 -0.0017920586 -0.0017849903 -0.0017798332 -0.0017775815 -0.0017794907 -0.0017842521 -0.0017913424 -0.0017982365 -0.0018042041 -0.0018090326][-0.0018229156 -0.0018176963 -0.001810813 -0.0018036133 -0.0017963925 -0.0017891824 -0.0017830441 -0.00177923 -0.0017782093 -0.0017805647 -0.001785741 -0.001793014 -0.0017993056 -0.0018046356 -0.0018090213][-0.0018229579 -0.0018165964 -0.0018090185 -0.0018015777 -0.0017946195 -0.0017885022 -0.0017838309 -0.0017813527 -0.0017814595 -0.0017844822 -0.0017900197 -0.0017970649 -0.0018024336 -0.0018066242 -0.0018098993][-0.001822397 -0.0018160014 -0.0018088273 -0.0018020525 -0.0017957392 -0.0017905232 -0.0017870711 -0.0017856978 -0.0017865226 -0.0017900112 -0.0017955861 -0.0018021454 -0.0018066303 -0.0018098732 -0.0018119249][-0.0018216963 -0.0018161274 -0.0018099204 -0.0018041558 -0.001799037 -0.001794752 -0.0017921469 -0.0017915714 -0.0017928433 -0.0017963581 -0.0018015081 -0.0018072645 -0.0018109414 -0.0018131539 -0.0018139944][-0.0018217615 -0.0018172724 -0.0018123705 -0.0018077942 -0.001803915 -0.0018008419 -0.0017987501 -0.0017981582 -0.0017994135 -0.0018025942 -0.0018071901 -0.0018118689 -0.0018145713 -0.0018157003 -0.0018150773][-0.001824279 -0.0018209713 -0.0018175831 -0.0018141263 -0.0018111768 -0.0018087554 -0.001806834 -0.0018057505 -0.0018060822 -0.0018086268 -0.0018121885 -0.0018154943 -0.0018170552 -0.0018173936 -0.0018160332][-0.0018272401 -0.0018250863 -0.0018229873 -0.0018207848 -0.0018187705 -0.0018166809 -0.0018149251 -0.001813349 -0.0018127131 -0.0018136 -0.0018153702 -0.0018170885 -0.0018177771 -0.0018175847 -0.0018159212]]...]
INFO - root - 2017-12-09 15:33:00.300991: step 35310, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.839 sec/batch; 69h:14m:04s remains)
INFO - root - 2017-12-09 15:33:08.621599: step 35320, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.843 sec/batch; 69h:34m:03s remains)
INFO - root - 2017-12-09 15:33:17.112752: step 35330, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.858 sec/batch; 70h:50m:56s remains)
INFO - root - 2017-12-09 15:33:25.837029: step 35340, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.829 sec/batch; 68h:25m:53s remains)
INFO - root - 2017-12-09 15:33:34.525556: step 35350, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.875 sec/batch; 72h:13m:28s remains)
INFO - root - 2017-12-09 15:33:43.126389: step 35360, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.845 sec/batch; 69h:44m:40s remains)
INFO - root - 2017-12-09 15:33:51.793878: step 35370, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 71h:36m:14s remains)
INFO - root - 2017-12-09 15:34:00.291999: step 35380, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.861 sec/batch; 71h:05m:14s remains)
INFO - root - 2017-12-09 15:34:08.883949: step 35390, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.884 sec/batch; 72h:58m:12s remains)
INFO - root - 2017-12-09 15:34:17.317872: step 35400, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.888 sec/batch; 73h:17m:47s remains)
2017-12-09 15:34:18.232521: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.21454737 0.23432823 0.25051895 0.26310235 0.27007666 0.27047178 0.26580903 0.25573388 0.24202649 0.22848386 0.21547705 0.20655446 0.20043486 0.19939241 0.201042][0.2268309 0.25394809 0.2769627 0.2946642 0.30513933 0.30744371 0.30304539 0.29325208 0.27929619 0.26486921 0.25089192 0.24067444 0.23356707 0.23139343 0.23215649][0.23615627 0.27078763 0.30074582 0.32427424 0.33875078 0.34300551 0.33932319 0.32991284 0.31586805 0.3009378 0.28605968 0.27455103 0.26567054 0.26074022 0.25907582][0.24183345 0.28199819 0.31791183 0.34708557 0.36685371 0.37579829 0.37513641 0.3673794 0.35392514 0.33779687 0.32077977 0.30584246 0.29379931 0.28526908 0.27950463][0.24372788 0.28809482 0.32838434 0.36205596 0.38688391 0.40137762 0.4060905 0.40137076 0.38911 0.37217227 0.35232118 0.33334425 0.31721577 0.30442163 0.29444078][0.24568821 0.29195681 0.33408758 0.37038672 0.39857018 0.41748464 0.42651197 0.425862 0.41586259 0.39821938 0.37622163 0.35397539 0.33370459 0.31634626 0.30127203][0.24672195 0.29416731 0.33750984 0.37579429 0.40569878 0.42715657 0.43863049 0.44012353 0.43133143 0.41407683 0.39174849 0.36793196 0.34516597 0.32437566 0.30392277][0.2464598 0.29490674 0.33918819 0.37836975 0.40927634 0.43157959 0.44384804 0.44582868 0.43739405 0.4204872 0.39821446 0.37421748 0.35016066 0.32629874 0.30115286][0.24400167 0.29212236 0.33665952 0.37593907 0.40681797 0.4290736 0.44154415 0.4436419 0.43530092 0.41886979 0.39798251 0.3744483 0.34998125 0.32477614 0.29673955][0.23796032 0.28537291 0.32905397 0.36749202 0.39715868 0.41804683 0.42984173 0.43141353 0.42345312 0.40841308 0.38952461 0.36804298 0.34466296 0.31962761 0.2908228][0.22964625 0.27534187 0.31722173 0.35341737 0.38077661 0.39925659 0.40926638 0.41076812 0.40398023 0.39123854 0.37532541 0.35699981 0.33607596 0.31194073 0.28357163][0.21623003 0.25936395 0.29881832 0.33191702 0.35634845 0.3722229 0.38056457 0.38193318 0.37670729 0.36754444 0.35597917 0.34154058 0.32430762 0.30264473 0.27686045][0.19855785 0.23794751 0.2736077 0.30326906 0.32508746 0.33871979 0.34618068 0.34800944 0.34410089 0.33758986 0.32950348 0.31961212 0.30648285 0.28846082 0.26727125][0.17962441 0.21363792 0.24403515 0.26927444 0.2880924 0.30038512 0.30781746 0.31098965 0.3093982 0.30535 0.29993388 0.29310763 0.28386074 0.27016932 0.25423169][0.16023372 0.18910655 0.21401232 0.23442164 0.2501232 0.26145265 0.26926944 0.27416849 0.27522165 0.27379259 0.27053478 0.26602912 0.25961351 0.24943283 0.23786257]]...]
INFO - root - 2017-12-09 15:34:26.793748: step 35410, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.833 sec/batch; 68h:46m:17s remains)
INFO - root - 2017-12-09 15:34:35.248675: step 35420, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.876 sec/batch; 72h:15m:27s remains)
INFO - root - 2017-12-09 15:34:43.905334: step 35430, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.882 sec/batch; 72h:45m:07s remains)
INFO - root - 2017-12-09 15:34:52.601998: step 35440, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.878 sec/batch; 72h:24m:36s remains)
INFO - root - 2017-12-09 15:35:01.209439: step 35450, loss = 0.82, batch loss = 0.69 (9.7 examples/sec; 0.827 sec/batch; 68h:13m:16s remains)
INFO - root - 2017-12-09 15:35:09.745400: step 35460, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.835 sec/batch; 68h:54m:02s remains)
INFO - root - 2017-12-09 15:35:18.173346: step 35470, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.842 sec/batch; 69h:26m:02s remains)
INFO - root - 2017-12-09 15:35:26.651657: step 35480, loss = 0.82, batch loss = 0.69 (8.2 examples/sec; 0.978 sec/batch; 80h:42m:51s remains)
INFO - root - 2017-12-09 15:35:35.284286: step 35490, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.894 sec/batch; 73h:44m:43s remains)
INFO - root - 2017-12-09 15:35:43.811562: step 35500, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 71h:32m:25s remains)
2017-12-09 15:35:44.701876: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.0086793853 0.012714546 0.017946 0.023861088 0.029184053 0.033399649 0.035893656 0.037003651 0.036456093 0.034765661 0.032259278 0.029420765 0.026679853 0.024145851 0.021451801][0.013542879 0.019453678 0.027033104 0.035669997 0.04364606 0.0501441 0.054165408 0.056103922 0.05569768 0.053559951 0.050352909 0.046715833 0.043200761 0.039508238 0.035469256][0.019418385 0.027903371 0.038756572 0.050931893 0.062307119 0.071603313 0.077506579 0.080248483 0.079555608 0.076394804 0.071920052 0.06717585 0.062784761 0.058377191 0.053609833][0.024777761 0.036131606 0.050500549 0.066298909 0.081128806 0.093187049 0.10096052 0.1044028 0.10346517 0.099438384 0.093945988 0.0883579 0.083427168 0.078607947 0.073338553][0.027808091 0.041611776 0.058820531 0.077451691 0.094758548 0.1085664 0.11743539 0.1213157 0.12032714 0.11607287 0.11069205 0.10579862 0.10191096 0.097982414 0.093365714][0.02814826 0.042973235 0.061391048 0.081463933 0.099987023 0.11436211 0.12339024 0.12714219 0.12610555 0.1221339 0.11761215 0.11430448 0.11246915 0.11072555 0.10812363][0.026239462 0.040858872 0.059051089 0.078970268 0.09732613 0.11158524 0.12029416 0.1236589 0.12244935 0.11883023 0.11528511 0.11363753 0.11400497 0.11459237 0.11427371][0.022919938 0.036209233 0.052886426 0.071386196 0.088415265 0.10146637 0.10924618 0.11236641 0.1114005 0.10845204 0.10582156 0.10564052 0.10770286 0.11001861 0.1115824][0.018506309 0.029716363 0.04399512 0.060002804 0.074952781 0.086500049 0.093277305 0.0960965 0.095526777 0.093546435 0.092024542 0.092978247 0.096130684 0.09955126 0.10218403][0.013337554 0.021809308 0.032802969 0.045370765 0.057412393 0.066988654 0.072928227 0.075764179 0.075767 0.074781835 0.074304692 0.076145329 0.079906426 0.083967261 0.087403439][0.0079877032 0.013554058 0.020926686 0.02953876 0.038113579 0.045411356 0.0503363 0.053189911 0.05396805 0.054097787 0.054662041 0.05708421 0.061053984 0.065253578 0.068944208][0.0033549839 0.0064731045 0.010677527 0.015773021 0.021098524 0.025965169 0.029599784 0.032128885 0.033314094 0.0341772 0.035269 0.037676468 0.041274406 0.045095526 0.048464857][5.2461866e-05 0.0014513868 0.0033914903 0.0057839756 0.0084263515 0.011076838 0.013276974 0.015064426 0.016245514 0.017389523 0.01862135 0.020613408 0.023302574 0.026126094 0.028579753][-0.0014539688 -0.0010375292 -0.00038927479 0.0004659337 0.0014792713 0.0025691353 0.0035556424 0.0044723703 0.0052390844 0.0061425958 0.0070794504 0.0083971638 0.010084451 0.011816125 0.013190596][-0.0017933603 -0.0017618942 -0.0016913536 -0.0015746199 -0.0013956393 -0.0011433626 -0.0008667215 -0.00054813456 -0.00020248245 0.00027121417 0.00080868322 0.0014978272 0.0023537665 0.0032186285 0.0038854382]]...]
INFO - root - 2017-12-09 15:35:53.254364: step 35510, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.836 sec/batch; 68h:59m:52s remains)
INFO - root - 2017-12-09 15:36:01.740289: step 35520, loss = 0.82, batch loss = 0.69 (11.3 examples/sec; 0.710 sec/batch; 58h:36m:15s remains)
INFO - root - 2017-12-09 15:36:10.481452: step 35530, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.875 sec/batch; 72h:11m:24s remains)
INFO - root - 2017-12-09 15:36:19.147279: step 35540, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.876 sec/batch; 72h:14m:57s remains)
INFO - root - 2017-12-09 15:36:27.792762: step 35550, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.874 sec/batch; 72h:07m:56s remains)
INFO - root - 2017-12-09 15:36:36.499334: step 35560, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.882 sec/batch; 72h:46m:56s remains)
INFO - root - 2017-12-09 15:36:45.249571: step 35570, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.861 sec/batch; 71h:03m:14s remains)
INFO - root - 2017-12-09 15:36:53.898786: step 35580, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.858 sec/batch; 70h:44m:22s remains)
INFO - root - 2017-12-09 15:37:02.664518: step 35590, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.863 sec/batch; 71h:10m:28s remains)
INFO - root - 2017-12-09 15:37:11.389370: step 35600, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.857 sec/batch; 70h:40m:27s remains)
2017-12-09 15:37:12.275021: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.16096307 0.17120898 0.18164648 0.19208905 0.19946246 0.20230542 0.20002688 0.19322221 0.18322742 0.17080881 0.15820104 0.14632621 0.13604745 0.12741178 0.12091318][0.17230283 0.18598427 0.19927758 0.21168105 0.22029993 0.22367166 0.22062157 0.21203527 0.19962887 0.18491654 0.16988482 0.15551533 0.14318183 0.13310409 0.12550442][0.1756437 0.19190887 0.20735352 0.22133844 0.23108426 0.23482123 0.23146337 0.2219761 0.20788348 0.19129294 0.17423889 0.15800014 0.14424904 0.13317335 0.12480418][0.17628302 0.19475052 0.21195096 0.22718444 0.23770168 0.24201818 0.23870438 0.22853252 0.21320373 0.19517459 0.1764981 0.1586659 0.14375122 0.13203543 0.12318151][0.17291017 0.19292261 0.2112944 0.22805209 0.24007463 0.24540038 0.2424996 0.23233193 0.21634752 0.19698367 0.17681657 0.15772739 0.14200942 0.12960511 0.12019413][0.16434906 0.1853691 0.20457782 0.22222228 0.23532717 0.24172889 0.23956646 0.22950821 0.21302393 0.19274393 0.171585 0.1517947 0.13552918 0.12292419 0.11338796][0.14968018 0.17047244 0.1895965 0.20769353 0.22153375 0.22881027 0.22733246 0.21746387 0.20093878 0.18050511 0.1591543 0.13906844 0.12274773 0.11017007 0.10078952][0.12830141 0.14736208 0.16498521 0.18240453 0.19593634 0.20345511 0.20247519 0.19316874 0.17740485 0.15768787 0.13734448 0.11804175 0.10235307 0.09034466 0.08134792][0.101577 0.11750177 0.13244946 0.14764465 0.15961297 0.16653265 0.16569656 0.15730685 0.14322244 0.12552619 0.107365 0.090090767 0.076115027 0.0652941 0.057124622][0.07232856 0.0841134 0.095732808 0.10771935 0.1171204 0.1225819 0.12173691 0.11471702 0.10306142 0.088541329 0.073841818 0.059799869 0.048341583 0.039473452 0.032887764][0.044843171 0.053054191 0.061460841 0.070084125 0.076822542 0.080673352 0.079774633 0.074345835 0.065496586 0.054715332 0.043912895 0.033632841 0.025344934 0.019037139 0.014588148][0.023403833 0.028605729 0.034152213 0.039781287 0.044174578 0.046630122 0.045912772 0.042164095 0.036129564 0.028986199 0.021928286 0.015355715 0.010214259 0.0065015415 0.0041366327][0.0090975277 0.011912974 0.015023119 0.018150615 0.020584872 0.02190016 0.021437556 0.01928504 0.015855044 0.011889597 0.0080678621 0.0046283528 0.0020635426 0.00036439707 -0.00053908583][0.0013628545 0.0025927392 0.0039990214 0.0053992374 0.0064786784 0.0070624268 0.0068576904 0.0059242379 0.0044442955 0.0027695624 0.0012260244 -8.4354891e-05 -0.00097634172 -0.001490416 -0.001677577][-0.001337097 -0.0010274674 -0.00064732041 -0.00024635752 6.8090041e-05 0.0002329821 0.00018171559 -5.7502184e-05 -0.00043055532 -0.00084078137 -0.0011951791 -0.0014720309 -0.001633545 -0.0017152212 -0.0017378614]]...]
INFO - root - 2017-12-09 15:37:20.984653: step 35610, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.884 sec/batch; 72h:55m:05s remains)
INFO - root - 2017-12-09 15:37:29.472801: step 35620, loss = 0.82, batch loss = 0.69 (11.1 examples/sec; 0.718 sec/batch; 59h:13m:27s remains)
INFO - root - 2017-12-09 15:37:38.251567: step 35630, loss = 0.83, batch loss = 0.70 (8.9 examples/sec; 0.900 sec/batch; 74h:12m:24s remains)
INFO - root - 2017-12-09 15:37:46.955365: step 35640, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.890 sec/batch; 73h:24m:46s remains)
INFO - root - 2017-12-09 15:37:55.709615: step 35650, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.852 sec/batch; 70h:15m:34s remains)
INFO - root - 2017-12-09 15:38:04.515344: step 35660, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.881 sec/batch; 72h:40m:31s remains)
INFO - root - 2017-12-09 15:38:13.260157: step 35670, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.900 sec/batch; 74h:12m:33s remains)
INFO - root - 2017-12-09 15:38:21.865621: step 35680, loss = 0.83, batch loss = 0.70 (9.2 examples/sec; 0.866 sec/batch; 71h:23m:59s remains)
INFO - root - 2017-12-09 15:38:30.499503: step 35690, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.856 sec/batch; 70h:35m:53s remains)
INFO - root - 2017-12-09 15:38:39.078868: step 35700, loss = 0.82, batch loss = 0.69 (9.9 examples/sec; 0.810 sec/batch; 66h:47m:27s remains)
2017-12-09 15:38:39.907848: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0018047938 -0.0018037917 -0.0018040179 -0.0018054423 -0.0018074999 -0.0018083412 -0.001807067 -0.0018032026 -0.0017965137 -0.0017870704 -0.0017757048 -0.0017637712 -0.001751431 -0.0017396719 -0.0017285747][-0.0018018857 -0.0018005378 -0.0018004177 -0.0018015994 -0.0018038779 -0.0018056488 -0.0018059306 -0.0018044006 -0.001800779 -0.0017947532 -0.0017866438 -0.0017777383 -0.0017678915 -0.00175757 -0.0017474566][-0.0018006964 -0.0017987902 -0.0017981199 -0.0017988393 -0.0018007661 -0.0018029348 -0.0018044153 -0.0018049271 -0.0018040072 -0.0018013852 -0.0017973067 -0.0017921758 -0.0017857418 -0.0017786012 -0.0017709445][-0.0018008528 -0.0017984047 -0.0017971955 -0.0017972918 -0.0017983965 -0.0018002782 -0.0018021665 -0.001803707 -0.0018045662 -0.0018042984 -0.0018030639 -0.0018005738 -0.0017967395 -0.0017919265 -0.0017862887][-0.0018018659 -0.0017989773 -0.0017972598 -0.0017968408 -0.0017971289 -0.0017982562 -0.0017996806 -0.0018012288 -0.0018028487 -0.0018039793 -0.00180469 -0.0018045575 -0.0018030294 -0.0018005475 -0.0017968642][-0.0018033064 -0.0018002479 -0.0017984585 -0.0017977335 -0.0017973487 -0.001797563 -0.0017979045 -0.0017986074 -0.0017998567 -0.001801171 -0.0018024219 -0.0018038371 -0.0018041265 -0.0018035375 -0.0018016004][-0.0018053856 -0.0018023521 -0.0018004862 -0.0017994015 -0.0017985139 -0.0017979819 -0.0017973791 -0.0017970642 -0.0017974807 -0.0017982218 -0.0017990655 -0.001800508 -0.0018015597 -0.0018020481 -0.0018014912][-0.0018063346 -0.0018035484 -0.001801918 -0.0018007183 -0.0017995357 -0.0017986886 -0.0017977955 -0.0017969882 -0.001796857 -0.001797147 -0.0017973843 -0.0017979628 -0.0017986954 -0.0017991433 -0.0017984131][-0.001806864 -0.0018043841 -0.0018030101 -0.0018017458 -0.0018004866 -0.001799521 -0.0017984831 -0.0017975138 -0.0017971937 -0.0017971962 -0.0017968635 -0.0017967031 -0.0017967987 -0.0017965967 -0.001795253][-0.0018069247 -0.0018049041 -0.0018038892 -0.0018025966 -0.0018013829 -0.0018002873 -0.0017991381 -0.001798047 -0.0017976461 -0.0017975887 -0.0017972044 -0.0017970128 -0.0017971478 -0.0017969413 -0.001795728][-0.0018068516 -0.0018052668 -0.0018045204 -0.0018033204 -0.001802206 -0.0018010137 -0.0017998461 -0.0017987847 -0.0017982684 -0.0017980789 -0.00179757 -0.0017970932 -0.0017972087 -0.0017970367 -0.0017962231][-0.0018065214 -0.0018052678 -0.0018047529 -0.0018036895 -0.0018027313 -0.0018016114 -0.0018006419 -0.0017997464 -0.0017991686 -0.0017988064 -0.0017982342 -0.0017975033 -0.001797557 -0.0017975955 -0.0017971417][-0.0018060169 -0.001805054 -0.0018047277 -0.0018038349 -0.0018030917 -0.0018020321 -0.0018012177 -0.0018004141 -0.0017998172 -0.0017994249 -0.0017990585 -0.0017982728 -0.0017980557 -0.0017979511 -0.0017974196][-0.0018056766 -0.0018047504 -0.0018043832 -0.0018035658 -0.0018029517 -0.0018020809 -0.0018015534 -0.0018010022 -0.0018006292 -0.0018001818 -0.0017995688 -0.001799028 -0.0017985504 -0.0017981884 -0.0017977929][-0.0018048321 -0.0018038851 -0.0018034558 -0.0018027624 -0.0018022994 -0.0018016394 -0.0018015003 -0.0018011852 -0.0018010236 -0.0018006281 -0.0017999578 -0.0017992931 -0.0017985335 -0.001798192 -0.0017982931]]...]
INFO - root - 2017-12-09 15:38:48.603201: step 35710, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.866 sec/batch; 71h:22m:31s remains)
INFO - root - 2017-12-09 15:38:57.095377: step 35720, loss = 0.82, batch loss = 0.69 (10.7 examples/sec; 0.745 sec/batch; 61h:25m:21s remains)
INFO - root - 2017-12-09 15:39:05.624640: step 35730, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.863 sec/batch; 71h:06m:33s remains)
INFO - root - 2017-12-09 15:39:14.139703: step 35740, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 71h:31m:19s remains)
INFO - root - 2017-12-09 15:39:22.805571: step 35750, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.844 sec/batch; 69h:32m:13s remains)
INFO - root - 2017-12-09 15:39:31.449825: step 35760, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.849 sec/batch; 69h:56m:42s remains)
INFO - root - 2017-12-09 15:39:40.015816: step 35770, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.889 sec/batch; 73h:15m:12s remains)
INFO - root - 2017-12-09 15:39:48.542858: step 35780, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.902 sec/batch; 74h:22m:48s remains)
INFO - root - 2017-12-09 15:39:57.291699: step 35790, loss = 0.82, batch loss = 0.69 (9.7 examples/sec; 0.828 sec/batch; 68h:13m:21s remains)
INFO - root - 2017-12-09 15:40:05.981681: step 35800, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.891 sec/batch; 73h:24m:11s remains)
2017-12-09 15:40:06.751821: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.42179954 0.39926487 0.38352582 0.37696522 0.38001883 0.39220771 0.40898839 0.4273335 0.44008946 0.44346616 0.43372035 0.41169056 0.38101521 0.34482217 0.30893028][0.46123752 0.43475264 0.41367304 0.40154883 0.3981857 0.40389892 0.41436434 0.42797631 0.43742043 0.43876174 0.42856708 0.40702802 0.37679031 0.34102038 0.30529141][0.48910236 0.46329433 0.44102272 0.42557535 0.41733029 0.41573817 0.417367 0.42362049 0.42738026 0.42549995 0.41355968 0.39241871 0.36417735 0.33078828 0.29762012][0.5007354 0.4813939 0.46411762 0.44995362 0.44069776 0.43486771 0.42924044 0.42722353 0.42316738 0.41631648 0.40140992 0.3794812 0.35249114 0.32097274 0.29031661][0.49479544 0.4847787 0.47569758 0.468297 0.462925 0.4566879 0.44729319 0.43919337 0.42814025 0.4147602 0.39453563 0.36999702 0.34255832 0.31302723 0.28559354][0.47544518 0.47724849 0.4787865 0.48009095 0.4806402 0.47769788 0.46787989 0.45555916 0.43874243 0.419979 0.39519453 0.36622134 0.3361553 0.30648047 0.28063789][0.44368643 0.45773482 0.47066504 0.4829562 0.49147305 0.49295923 0.48508266 0.472056 0.4520064 0.428557 0.39996728 0.36861503 0.33708087 0.30672872 0.281956][0.40645456 0.43142384 0.45562196 0.47775257 0.49416798 0.5011037 0.49656415 0.48408565 0.46274775 0.43716267 0.40620068 0.37245458 0.33944038 0.30926296 0.28527167][0.36768726 0.40041485 0.43201062 0.46180969 0.48567313 0.49961534 0.50112766 0.49228019 0.47254625 0.44691157 0.41479105 0.37936163 0.34423119 0.31302807 0.28922144][0.3324492 0.36768964 0.40175885 0.43573576 0.46447262 0.48286286 0.48965204 0.48511827 0.4694261 0.44568321 0.41523108 0.38065636 0.34600291 0.31603369 0.29328936][0.29595539 0.3309527 0.36407802 0.39868006 0.4294771 0.45166573 0.46353468 0.4638769 0.45377797 0.43450966 0.40820068 0.37719685 0.3452436 0.31734774 0.2958937][0.26466402 0.29571879 0.32497287 0.35678723 0.38643512 0.40948862 0.42511895 0.43099025 0.42765832 0.41435251 0.39381054 0.36827171 0.34104976 0.31689575 0.29791924][0.24010853 0.26534486 0.28852606 0.31435469 0.34049138 0.36259681 0.37998322 0.38967517 0.39226472 0.38597322 0.3724817 0.35338423 0.33156353 0.31200418 0.29600984][0.22163437 0.2412127 0.25762117 0.27718967 0.29891792 0.31878453 0.33660632 0.34895986 0.35614187 0.35566804 0.34877741 0.33661395 0.32114062 0.30614561 0.2933208][0.20787652 0.22211131 0.23278183 0.246825 0.2637473 0.28039172 0.29685578 0.31020576 0.32043138 0.32431135 0.32335046 0.31764266 0.30835462 0.2985456 0.28902778]]...]
INFO - root - 2017-12-09 15:40:15.473582: step 35810, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.897 sec/batch; 73h:54m:21s remains)
INFO - root - 2017-12-09 15:40:24.169412: step 35820, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.833 sec/batch; 68h:37m:30s remains)
INFO - root - 2017-12-09 15:40:32.557639: step 35830, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.850 sec/batch; 70h:05m:01s remains)
INFO - root - 2017-12-09 15:40:41.137857: step 35840, loss = 0.82, batch loss = 0.69 (9.7 examples/sec; 0.827 sec/batch; 68h:07m:52s remains)
INFO - root - 2017-12-09 15:40:49.800213: step 35850, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.855 sec/batch; 70h:27m:49s remains)
INFO - root - 2017-12-09 15:40:58.515896: step 35860, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.891 sec/batch; 73h:23m:37s remains)
INFO - root - 2017-12-09 15:41:07.210273: step 35870, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.874 sec/batch; 72h:03m:07s remains)
INFO - root - 2017-12-09 15:41:15.903538: step 35880, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 71h:38m:23s remains)
INFO - root - 2017-12-09 15:41:24.745244: step 35890, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.854 sec/batch; 70h:20m:31s remains)
INFO - root - 2017-12-09 15:41:33.290902: step 35900, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.855 sec/batch; 70h:26m:40s remains)
2017-12-09 15:41:34.155845: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.058569469 0.05465268 0.050325584 0.044362828 0.039166961 0.036740515 0.034584105 0.032853793 0.031186474 0.029719034 0.027925013 0.025190162 0.021933043 0.018208129 0.014167888][0.069916546 0.067561038 0.06412854 0.058072958 0.052190725 0.0469894 0.042061698 0.037966803 0.034085255 0.031073386 0.028327093 0.026067548 0.023089221 0.019484442 0.015324134][0.087810956 0.088328414 0.087127507 0.082322724 0.076417081 0.068811871 0.060984902 0.053660311 0.046953108 0.042160746 0.038141713 0.035461929 0.032012928 0.028009526 0.022694444][0.12067214 0.12284119 0.12254621 0.1176715 0.11141971 0.10307387 0.094568036 0.086053014 0.07858438 0.073688462 0.069484942 0.065938614 0.060708828 0.054263975 0.045310523][0.16679895 0.16971071 0.16924123 0.16439225 0.15800878 0.14969859 0.14216186 0.13559508 0.1308514 0.12838338 0.12615512 0.12322744 0.11683606 0.10665774 0.091920272][0.22906592 0.23158014 0.22911295 0.2232464 0.21648014 0.20893502 0.20343259 0.19987503 0.19899936 0.1999703 0.20043078 0.19808276 0.19040373 0.17628743 0.15544257][0.30223081 0.30435821 0.29970083 0.29250866 0.2850112 0.27822658 0.27550846 0.27627322 0.28018853 0.28549683 0.28930932 0.28847873 0.27990159 0.2620697 0.2354286][0.36949968 0.37210253 0.36650774 0.35932639 0.35290354 0.34963432 0.35085148 0.35602194 0.36438867 0.37293792 0.37844235 0.37704515 0.36677909 0.3455568 0.3137635][0.42379186 0.42727843 0.42110059 0.41409311 0.40892434 0.40834281 0.41297469 0.42120194 0.43169254 0.4413009 0.44675621 0.44376126 0.43104839 0.40717888 0.37260336][0.45391741 0.45890254 0.45254502 0.44717851 0.44456792 0.44698009 0.45444578 0.46487457 0.4756448 0.48319951 0.48493731 0.47799551 0.46132112 0.43426263 0.39788678][0.46333572 0.47054684 0.46524039 0.4620865 0.46247813 0.4680714 0.47764969 0.48794106 0.49563348 0.49729782 0.49128056 0.47613007 0.45253471 0.421277 0.38367712][0.45328528 0.46374279 0.46034557 0.45990947 0.4630928 0.47068289 0.48046556 0.48836365 0.4904044 0.48298222 0.46616557 0.44036022 0.40797871 0.3715007 0.33288786][0.431393 0.44536227 0.44440335 0.4465428 0.45194671 0.46064293 0.46904236 0.4724099 0.46680281 0.44841668 0.41927609 0.38162866 0.34049439 0.29942408 0.2606414][0.4030427 0.41946775 0.42069736 0.42499706 0.43179923 0.44017786 0.44643047 0.44510955 0.43175709 0.40263322 0.36200389 0.31416523 0.26580247 0.2215061 0.18390688][0.36643663 0.383463 0.38658255 0.3923175 0.3998211 0.40759555 0.41132647 0.40555021 0.38568759 0.34858024 0.29998305 0.24603373 0.19458279 0.15033334 0.11580544]]...]
INFO - root - 2017-12-09 15:41:42.711724: step 35910, loss = 0.83, batch loss = 0.70 (9.0 examples/sec; 0.889 sec/batch; 73h:12m:16s remains)
INFO - root - 2017-12-09 15:41:51.389249: step 35920, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.879 sec/batch; 72h:23m:33s remains)
INFO - root - 2017-12-09 15:41:59.826008: step 35930, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.850 sec/batch; 70h:00m:31s remains)
INFO - root - 2017-12-09 15:42:08.413508: step 35940, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.874 sec/batch; 72h:01m:58s remains)
INFO - root - 2017-12-09 15:42:17.104360: step 35950, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 70h:59m:10s remains)
INFO - root - 2017-12-09 15:42:25.861105: step 35960, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.878 sec/batch; 72h:18m:15s remains)
INFO - root - 2017-12-09 15:42:34.542877: step 35970, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.863 sec/batch; 71h:07m:00s remains)
INFO - root - 2017-12-09 15:42:42.884078: step 35980, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.849 sec/batch; 69h:56m:34s remains)
INFO - root - 2017-12-09 15:42:51.411193: step 35990, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.865 sec/batch; 71h:14m:02s remains)
INFO - root - 2017-12-09 15:43:00.171148: step 36000, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 71h:42m:38s remains)
2017-12-09 15:43:01.040243: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0017922383 -0.0017910179 -0.0017922255 -0.0017943529 -0.0017964025 -0.001798787 -0.0018019121 -0.0018058418 -0.0018096382 -0.0018109917 -0.0018086471 -0.0018025741 -0.0017919174 -0.00177808 -0.0017631471][-0.0017913475 -0.0017898214 -0.001790593 -0.0017925071 -0.0017943588 -0.0017969708 -0.0018007328 -0.0018053126 -0.0018092894 -0.00181014 -0.0018069764 -0.0017998681 -0.0017882581 -0.0017739299 -0.0017595766][-0.0017923823 -0.0017905564 -0.0017905569 -0.0017918169 -0.0017930069 -0.001795704 -0.001799855 -0.0018046928 -0.0018085891 -0.001809266 -0.0018060178 -0.0017988611 -0.0017876417 -0.0017745101 -0.0017621514][-0.0017938836 -0.0017917203 -0.0017910737 -0.0017915531 -0.001792258 -0.0017949928 -0.0017994266 -0.0018041707 -0.001807729 -0.0018084291 -0.0018058907 -0.0017998948 -0.0017906246 -0.0017802828 -0.0017710401][-0.0017955232 -0.0017930764 -0.001791806 -0.0017916227 -0.0017918261 -0.0017945687 -0.0017991199 -0.0018038262 -0.0018071597 -0.0018082631 -0.0018069721 -0.0018024999 -0.0017954424 -0.001788023 -0.0017815551][-0.0017970696 -0.0017943077 -0.0017926317 -0.0017918505 -0.0017916003 -0.0017939917 -0.0017982442 -0.0018029452 -0.0018064547 -0.0018084903 -0.0018087118 -0.0018062264 -0.0018018471 -0.0017969033 -0.0017923136][-0.0017982376 -0.0017953299 -0.001793468 -0.0017923827 -0.0017918836 -0.0017938223 -0.0017975335 -0.0018018822 -0.0018056431 -0.0018086795 -0.0018103324 -0.0018097564 -0.0018078348 -0.0018054489 -0.0018026992][-0.0017987207 -0.0017961441 -0.0017946935 -0.0017934371 -0.0017926892 -0.0017939808 -0.0017967293 -0.0018001519 -0.0018034949 -0.0018069051 -0.0018093545 -0.0018101112 -0.001810089 -0.0018098084 -0.0018087184][-0.0017984598 -0.0017962957 -0.0017954963 -0.0017944527 -0.0017936745 -0.0017942677 -0.001796098 -0.0017986782 -0.001801463 -0.0018047963 -0.0018074076 -0.001808934 -0.0018099634 -0.0018107489 -0.0018106692][-0.0017976854 -0.0017962294 -0.0017961049 -0.0017954701 -0.0017947855 -0.0017948868 -0.0017958594 -0.0017975743 -0.0017999299 -0.0018031077 -0.0018056094 -0.0018073578 -0.0018085839 -0.0018096112 -0.001809833][-0.001796879 -0.0017961038 -0.0017965017 -0.0017962863 -0.0017958708 -0.001795641 -0.0017959489 -0.001796886 -0.0017987097 -0.0018013597 -0.0018033537 -0.0018048075 -0.0018060273 -0.0018074476 -0.0018085146][-0.0017958734 -0.0017957696 -0.0017967235 -0.0017968467 -0.0017966961 -0.001796531 -0.0017965798 -0.001796781 -0.001797737 -0.0017995421 -0.0018008054 -0.0018017337 -0.0018028675 -0.001805 -0.0018076031][-0.0017950457 -0.0017952039 -0.0017965485 -0.001797113 -0.0017974112 -0.0017974571 -0.0017975419 -0.0017973976 -0.0017977287 -0.0017986229 -0.0017989714 -0.0017992808 -0.0018001384 -0.0018024617 -0.0018060029][-0.0017940991 -0.0017943698 -0.0017957835 -0.001796765 -0.0017975547 -0.0017979901 -0.0017981998 -0.0017978851 -0.0017978965 -0.0017982348 -0.0017980034 -0.0017978461 -0.0017984487 -0.001800823 -0.0018047486][-0.0017932437 -0.0017935279 -0.0017948471 -0.0017959822 -0.0017969392 -0.0017975798 -0.0017979337 -0.0017976334 -0.0017976315 -0.0017975644 -0.0017971693 -0.0017967263 -0.0017973041 -0.0017996641 -0.0018037925]]...]
INFO - root - 2017-12-09 15:43:09.486715: step 36010, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.857 sec/batch; 70h:36m:28s remains)
INFO - root - 2017-12-09 15:43:18.084939: step 36020, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.847 sec/batch; 69h:43m:49s remains)
INFO - root - 2017-12-09 15:43:26.569273: step 36030, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.870 sec/batch; 71h:37m:13s remains)
INFO - root - 2017-12-09 15:43:35.171885: step 36040, loss = 0.82, batch loss = 0.69 (8.8 examples/sec; 0.908 sec/batch; 74h:44m:24s remains)
INFO - root - 2017-12-09 15:43:43.915121: step 36050, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.857 sec/batch; 70h:33m:47s remains)
INFO - root - 2017-12-09 15:43:52.603434: step 36060, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 71h:31m:44s remains)
INFO - root - 2017-12-09 15:44:01.255573: step 36070, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.866 sec/batch; 71h:19m:16s remains)
INFO - root - 2017-12-09 15:44:09.739986: step 36080, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.877 sec/batch; 72h:12m:08s remains)
INFO - root - 2017-12-09 15:44:18.344984: step 36090, loss = 0.82, batch loss = 0.69 (9.8 examples/sec; 0.816 sec/batch; 67h:08m:42s remains)
INFO - root - 2017-12-09 15:44:26.963453: step 36100, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.848 sec/batch; 69h:48m:49s remains)
2017-12-09 15:44:27.846862: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0018324106 -0.0018318263 -0.0018318277 -0.0018319855 -0.001832297 -0.0018326431 -0.0018329168 -0.0018330241 -0.0018330362 -0.0018329422 -0.0018327389 -0.0018326311 -0.001832697 -0.0018329631 -0.001833248][-0.0018320773 -0.0018315376 -0.0018317822 -0.0018322644 -0.001832885 -0.001833465 -0.001833922 -0.0018342204 -0.0018344059 -0.0018344289 -0.0018343208 -0.0018342971 -0.0018344511 -0.0018346182 -0.0018347108][-0.0018316882 -0.0018313092 -0.0018317639 -0.00183257 -0.0018336816 -0.0018347697 -0.0018355534 -0.0018360028 -0.0018363246 -0.0018364221 -0.0018364114 -0.0018364661 -0.0018366441 -0.0018368656 -0.0018369297][-0.0018309595 -0.0018306213 -0.0018313137 -0.0018324108 -0.0018339559 -0.0018354487 -0.0018366474 -0.0018373745 -0.0018379183 -0.0018381469 -0.0018381544 -0.0018381501 -0.0018383121 -0.0018385238 -0.0018385759][-0.0018301768 -0.0018298214 -0.0018305731 -0.0018319242 -0.0018336955 -0.0018354793 -0.0018370245 -0.0018381055 -0.0018388643 -0.0018391993 -0.0018391369 -0.0018390235 -0.0018391075 -0.0018392277 -0.0018392707][-0.0018291514 -0.001828893 -0.0018295906 -0.0018310472 -0.0018329977 -0.001835012 -0.0018367767 -0.0018381347 -0.0018389957 -0.0018390372 -0.0018383617 -0.001837948 -0.0018380401 -0.0018383873 -0.0018386878][-0.0018283298 -0.00182815 -0.0018287967 -0.0018302886 -0.0018322276 -0.0018342695 -0.0018361589 -0.0018376105 -0.001837955 -0.0018362107 -0.0018325564 -0.0018308801 -0.0018317866 -0.0018339833 -0.0018359735][-0.0018279142 -0.0018277328 -0.0018282369 -0.0018296534 -0.0018314419 -0.001833292 -0.001835121 -0.0018364133 -0.0018343757 -0.001826429 -0.0018140384 -0.0018088243 -0.0018130729 -0.0018219405 -0.0018295988][-0.0018276813 -0.0018274835 -0.0018279061 -0.0018290894 -0.0018306825 -0.0018323186 -0.0018340122 -0.0018346813 -0.0018265168 -0.0018045618 -0.0017765613 -0.0017660834 -0.0017784445 -0.0018010234 -0.0018195128][-0.0018275751 -0.001827464 -0.0018278891 -0.0018289441 -0.0018303995 -0.00183179 -0.0018331499 -0.0018321377 -0.0018145693 -0.0017746537 -0.0017322258 -0.0017208523 -0.0017454186 -0.0017829108 -0.0018118429][-0.001827625 -0.0018275444 -0.0018281117 -0.0018292331 -0.0018306976 -0.0018320776 -0.0018331154 -0.0018296127 -0.0018027406 -0.0017485394 -0.0016970022 -0.0016887888 -0.0017251989 -0.001773182 -0.0018083166][-0.0018276531 -0.001827657 -0.0018285209 -0.00182993 -0.001831623 -0.0018331839 -0.0018341372 -0.0018288 -0.0017986975 -0.0017406717 -0.0016855268 -0.0016777023 -0.0017173715 -0.0017690923 -0.0018065225][-0.0018276594 -0.0018278275 -0.0018290306 -0.0018308358 -0.0018327683 -0.0018344218 -0.0018352483 -0.0018318527 -0.0018055812 -0.0017507012 -0.0016952532 -0.0016832445 -0.0017171483 -0.0017663848 -0.0018031055][-0.0018278392 -0.0018279274 -0.0018293551 -0.0018315271 -0.0018337216 -0.0018351069 -0.0018344107 -0.0018303243 -0.0018070139 -0.0017567113 -0.0017027551 -0.0016841422 -0.0017091738 -0.001753809 -0.0017908058][-0.0018282763 -0.0018281757 -0.0018294954 -0.0018318152 -0.0018339443 -0.0018333233 -0.0018269458 -0.0018126955 -0.001785065 -0.0017375171 -0.0016898374 -0.0016686667 -0.0016881274 -0.0017274674 -0.0017644253]]...]
INFO - root - 2017-12-09 15:44:36.350058: step 36110, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.887 sec/batch; 73h:02m:44s remains)
INFO - root - 2017-12-09 15:44:45.076039: step 36120, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.865 sec/batch; 71h:11m:00s remains)
INFO - root - 2017-12-09 15:44:53.705838: step 36130, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.854 sec/batch; 70h:18m:21s remains)
INFO - root - 2017-12-09 15:45:02.234032: step 36140, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.848 sec/batch; 69h:46m:05s remains)
INFO - root - 2017-12-09 15:45:10.879226: step 36150, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.855 sec/batch; 70h:25m:02s remains)
INFO - root - 2017-12-09 15:45:19.509099: step 36160, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.844 sec/batch; 69h:28m:14s remains)
INFO - root - 2017-12-09 15:45:28.094132: step 36170, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.857 sec/batch; 70h:30m:49s remains)
INFO - root - 2017-12-09 15:45:36.564198: step 36180, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.855 sec/batch; 70h:22m:26s remains)
INFO - root - 2017-12-09 15:45:45.207456: step 36190, loss = 0.83, batch loss = 0.70 (9.2 examples/sec; 0.868 sec/batch; 71h:27m:01s remains)
INFO - root - 2017-12-09 15:45:54.092034: step 36200, loss = 0.82, batch loss = 0.69 (8.8 examples/sec; 0.908 sec/batch; 74h:43m:07s remains)
2017-12-09 15:45:54.951400: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0018199801 -0.001819254 -0.0018189392 -0.0018185399 -0.0018180441 -0.0018176463 -0.0018173318 -0.0018171929 -0.0018174559 -0.001818227 -0.0018192817 -0.0018204828 -0.0018217257 -0.0018223877 -0.0018224136][-0.0018193794 -0.0018186159 -0.0018182648 -0.0018178329 -0.0018172321 -0.0018166678 -0.0018162562 -0.0018162263 -0.0018168476 -0.0018181838 -0.0018198678 -0.0018219075 -0.0018241311 -0.0018253655 -0.0018254531][-0.0018195 -0.0018187693 -0.0018184778 -0.0018179727 -0.0018171242 -0.0018162429 -0.0018156382 -0.0018156437 -0.0018166496 -0.001818541 -0.001820791 -0.001823557 -0.001826589 -0.0018284305 -0.0018286898][-0.0018196352 -0.0018191523 -0.0018191464 -0.0018187333 -0.0018176649 -0.0018163414 -0.0018152958 -0.001815062 -0.0018162321 -0.0018185661 -0.0018214443 -0.0018250037 -0.0018286625 -0.0018308744 -0.0018311812][-0.0018210162 -0.0018207114 -0.0018209149 -0.0018206274 -0.0018193814 -0.0018174833 -0.0018157051 -0.0018148815 -0.0018158122 -0.0018184432 -0.0018219025 -0.0018260459 -0.0018301238 -0.0018326096 -0.0018328754][-0.0018236578 -0.0018233926 -0.0018236506 -0.0018234097 -0.0018221088 -0.0018197533 -0.0018171892 -0.001815566 -0.001816024 -0.0018186102 -0.0018221812 -0.001826463 -0.0018304422 -0.001832771 -0.0018328924][-0.001826854 -0.0018266155 -0.0018267728 -0.0018264767 -0.0018252158 -0.0018226393 -0.0018194226 -0.0018168635 -0.001816635 -0.0018188474 -0.0018220156 -0.0018257358 -0.0018290401 -0.0018309311 -0.0018308506][-0.0018293563 -0.0018291745 -0.001829438 -0.0018292919 -0.0018282564 -0.0018256353 -0.0018219398 -0.0018184318 -0.0018172318 -0.0018188427 -0.0018214599 -0.001824322 -0.0018267429 -0.0018280169 -0.0018277698][-0.0018318008 -0.0018316936 -0.0018319868 -0.0018319667 -0.0018310271 -0.0018285566 -0.0018249312 -0.0018207986 -0.0018185023 -0.0018190095 -0.0018208339 -0.0018228397 -0.0018243924 -0.0018250786 -0.0018246564][-0.0018340978 -0.0018338102 -0.0018339469 -0.0018338746 -0.0018330219 -0.0018308334 -0.0018276953 -0.0018237666 -0.001820772 -0.0018199875 -0.0018206476 -0.0018215346 -0.0018221397 -0.0018221918 -0.0018215042][-0.0018347591 -0.0018342637 -0.0018341504 -0.0018340399 -0.0018334476 -0.0018318456 -0.0018293578 -0.0018261524 -0.0018231706 -0.0018213902 -0.0018208319 -0.0018205802 -0.0018202405 -0.0018195335 -0.0018185443][-0.001833462 -0.0018329716 -0.0018329018 -0.0018329471 -0.0018326642 -0.0018316079 -0.001829847 -0.0018273854 -0.0018246415 -0.0018223475 -0.0018210639 -0.0018200198 -0.0018189958 -0.0018178014 -0.001816657][-0.0018311118 -0.0018308403 -0.0018310507 -0.0018314498 -0.0018315441 -0.0018308443 -0.0018295841 -0.0018276446 -0.0018251387 -0.0018226103 -0.0018210425 -0.0018197601 -0.0018184272 -0.0018171209 -0.0018160181][-0.0018290315 -0.0018289454 -0.0018293015 -0.0018298182 -0.0018300256 -0.0018296132 -0.0018287359 -0.0018271214 -0.0018248544 -0.0018225808 -0.0018209215 -0.001819508 -0.0018181208 -0.0018169167 -0.0018160153][-0.0018265496 -0.0018266015 -0.0018269562 -0.0018275504 -0.0018278855 -0.0018276743 -0.0018270101 -0.0018256485 -0.0018237623 -0.0018219512 -0.0018204831 -0.0018191326 -0.0018178034 -0.0018166793 -0.0018159731]]...]
INFO - root - 2017-12-09 15:46:03.576493: step 36210, loss = 0.83, batch loss = 0.70 (9.0 examples/sec; 0.892 sec/batch; 73h:25m:43s remains)
INFO - root - 2017-12-09 15:46:12.312771: step 36220, loss = 0.82, batch loss = 0.69 (8.8 examples/sec; 0.910 sec/batch; 74h:51m:17s remains)
INFO - root - 2017-12-09 15:46:20.811615: step 36230, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.859 sec/batch; 70h:40m:12s remains)
INFO - root - 2017-12-09 15:46:29.543824: step 36240, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.858 sec/batch; 70h:34m:55s remains)
INFO - root - 2017-12-09 15:46:38.156244: step 36250, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.842 sec/batch; 69h:19m:31s remains)
INFO - root - 2017-12-09 15:46:46.710284: step 36260, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.861 sec/batch; 70h:51m:06s remains)
INFO - root - 2017-12-09 15:46:55.264666: step 36270, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.872 sec/batch; 71h:44m:58s remains)
INFO - root - 2017-12-09 15:47:03.936957: step 36280, loss = 0.83, batch loss = 0.70 (9.0 examples/sec; 0.892 sec/batch; 73h:23m:42s remains)
INFO - root - 2017-12-09 15:47:12.555822: step 36290, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.857 sec/batch; 70h:29m:53s remains)
INFO - root - 2017-12-09 15:47:21.438369: step 36300, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.885 sec/batch; 72h:49m:48s remains)
2017-12-09 15:47:22.329669: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.36867836 0.36412486 0.36085993 0.35562587 0.34892866 0.34252155 0.3386412 0.33499166 0.32929435 0.32394987 0.31850874 0.31188929 0.30337074 0.29371703 0.28502369][0.37411264 0.37294862 0.37241033 0.3698011 0.36530665 0.36121333 0.35916448 0.35719138 0.35346094 0.34948879 0.34436575 0.33643177 0.32602629 0.31386951 0.30152541][0.37544784 0.37853974 0.38137552 0.38132107 0.37991926 0.37857446 0.37826565 0.37750456 0.37423325 0.37097418 0.36541277 0.3558276 0.34360084 0.32870734 0.31328627][0.37545472 0.38299859 0.38966769 0.39260474 0.39492393 0.39670998 0.39788279 0.39725679 0.39385402 0.39021674 0.3832868 0.37220514 0.35878742 0.34209263 0.32420808][0.37426662 0.38551643 0.3949427 0.40136519 0.40711159 0.41172412 0.414644 0.41431522 0.41075665 0.40568697 0.39689395 0.38412431 0.36959475 0.35146245 0.33169386][0.3727732 0.38713926 0.3982726 0.40690613 0.41475055 0.42185071 0.4260942 0.42597681 0.42208767 0.41572657 0.40528098 0.38985732 0.37381455 0.35487628 0.33451226][0.37145355 0.38867122 0.40110481 0.41116768 0.42029008 0.42881006 0.4335044 0.43345162 0.42914486 0.42164752 0.41026545 0.39341065 0.37638605 0.35660285 0.33592319][0.36802158 0.38808295 0.40188423 0.41247341 0.42231792 0.43129736 0.43576467 0.43507487 0.42968825 0.42124522 0.40908247 0.39184633 0.37463033 0.35423538 0.33364689][0.36307591 0.38465813 0.39870009 0.40996554 0.42009383 0.42894641 0.43294528 0.43198025 0.42653942 0.41777638 0.40550125 0.38864022 0.37135243 0.35092881 0.3303819][0.35709584 0.379641 0.39309877 0.40395716 0.41314614 0.42086786 0.42422056 0.4231922 0.41779432 0.40934908 0.39788166 0.3821668 0.3649537 0.34487483 0.32487696][0.3456867 0.36825114 0.38099629 0.39061874 0.39862537 0.40567389 0.40859625 0.40805009 0.4034673 0.39571112 0.38502926 0.37040392 0.3539643 0.335055 0.31631222][0.33000895 0.35197896 0.3633419 0.37167457 0.37813485 0.38337463 0.38553968 0.3852683 0.38159394 0.37506652 0.366103 0.35416278 0.34008181 0.32306886 0.30619195][0.3126379 0.33243236 0.34191307 0.34847724 0.35346395 0.35702541 0.35808218 0.35741767 0.35424522 0.34864932 0.34124115 0.33187851 0.32060736 0.30691871 0.29305738][0.29589176 0.31236243 0.31869838 0.32323781 0.32668331 0.32930237 0.33035278 0.3300572 0.32810885 0.3239395 0.31832325 0.31135914 0.30248895 0.29224405 0.2815837][0.27914637 0.29234713 0.29581183 0.29835469 0.30049336 0.30231375 0.30353138 0.30397657 0.30322123 0.30041322 0.29650015 0.29160911 0.285357 0.27818444 0.270685]]...]
INFO - root - 2017-12-09 15:47:30.862519: step 36310, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.857 sec/batch; 70h:32m:00s remains)
INFO - root - 2017-12-09 15:47:39.619523: step 36320, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.894 sec/batch; 73h:34m:51s remains)
INFO - root - 2017-12-09 15:47:47.993480: step 36330, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.883 sec/batch; 72h:36m:48s remains)
INFO - root - 2017-12-09 15:47:56.731294: step 36340, loss = 0.83, batch loss = 0.70 (9.7 examples/sec; 0.828 sec/batch; 68h:06m:22s remains)
INFO - root - 2017-12-09 15:48:05.490010: step 36350, loss = 0.82, batch loss = 0.69 (8.7 examples/sec; 0.920 sec/batch; 75h:40m:01s remains)
INFO - root - 2017-12-09 15:48:14.207750: step 36360, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.896 sec/batch; 73h:43m:08s remains)
INFO - root - 2017-12-09 15:48:22.800948: step 36370, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.839 sec/batch; 69h:02m:48s remains)
INFO - root - 2017-12-09 15:48:31.332988: step 36380, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.870 sec/batch; 71h:32m:08s remains)
INFO - root - 2017-12-09 15:48:39.721180: step 36390, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.850 sec/batch; 69h:56m:26s remains)
INFO - root - 2017-12-09 15:48:48.411894: step 36400, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.865 sec/batch; 71h:10m:48s remains)
2017-12-09 15:48:49.238539: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.012221746 0.01279225 0.014544532 0.017971022 0.022226807 0.027828112 0.035134096 0.044567939 0.055207495 0.065697342 0.074640349 0.080271982 0.082450487 0.081886552 0.080019653][0.012559131 0.014117042 0.017588826 0.023205446 0.030071219 0.038390517 0.048225794 0.059044816 0.069506496 0.078845486 0.085990466 0.090024054 0.09120509 0.090760283 0.090169653][0.015342424 0.019285703 0.025775226 0.034824014 0.0454914 0.057332411 0.070000038 0.08227881 0.092555881 0.10047917 0.10538645 0.10724152 0.10684413 0.10585472 0.10589245][0.019392306 0.027228374 0.038266003 0.052044768 0.067468241 0.083452836 0.099054232 0.1125602 0.12263179 0.12947367 0.13272245 0.13316275 0.13192159 0.13043556 0.13059767][0.023092277 0.035580009 0.051967308 0.071308196 0.09181793 0.1118698 0.12991983 0.14421906 0.15399824 0.15983213 0.16192295 0.16155641 0.16010432 0.15861392 0.15899621][0.025566557 0.041927177 0.062848173 0.087097779 0.11187392 0.13507423 0.15446439 0.16899833 0.17836088 0.18338464 0.18492135 0.18454286 0.18372349 0.182534 0.18307187][0.026275851 0.044414449 0.067536175 0.094218016 0.12081964 0.14526477 0.16467637 0.17856029 0.18715952 0.19158445 0.19330983 0.19355313 0.19386411 0.19372022 0.19483426][0.024875835 0.042359304 0.064700179 0.090620294 0.11620906 0.13946961 0.15728512 0.1698617 0.17751458 0.18158011 0.18354598 0.18459904 0.18613788 0.18701237 0.18870293][0.021234019 0.036110792 0.055323832 0.077687331 0.099632 0.11958199 0.13469444 0.14520068 0.15160666 0.15520371 0.15725285 0.15881981 0.16105613 0.16263123 0.16446313][0.016019415 0.027250871 0.041938391 0.059215408 0.07621935 0.091604255 0.10327094 0.11152098 0.11636246 0.11906341 0.12057094 0.12206087 0.12421784 0.12587318 0.12761492][0.010211594 0.017828612 0.027819505 0.039646726 0.051357239 0.062016159 0.070293963 0.076191679 0.079597242 0.081375904 0.082169086 0.083192937 0.084645592 0.085929453 0.087187305][0.0048445645 0.0092744138 0.015149431 0.022122508 0.02911794 0.035716873 0.041178871 0.0452637 0.047663748 0.048970047 0.04934974 0.049774725 0.050340209 0.050995994 0.051677536][0.0010014931 0.0031487872 0.0059866845 0.0093769841 0.01292613 0.016464667 0.019645514 0.022261024 0.023948336 0.024880843 0.025051696 0.025167516 0.02516724 0.025274029 0.025447318][-0.0010518774 -0.00029765908 0.00073752017 0.0020242897 0.0034231432 0.0049234922 0.0064064893 0.00771613 0.008657936 0.0091766827 0.0093061384 0.0092797875 0.0090641491 0.00892403 0.0088269245][-0.0016995119 -0.0015471283 -0.0013133222 -0.001005432 -0.00062919105 -0.00016709464 0.00031984132 0.00078183482 0.0011188034 0.0013026027 0.001348187 0.001314254 0.0012025996 0.001079859 0.00098483055]]...]
INFO - root - 2017-12-09 15:48:57.615828: step 36410, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.832 sec/batch; 68h:25m:02s remains)
INFO - root - 2017-12-09 15:49:06.236666: step 36420, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.858 sec/batch; 70h:33m:17s remains)
INFO - root - 2017-12-09 15:49:14.555925: step 36430, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.845 sec/batch; 69h:27m:25s remains)
INFO - root - 2017-12-09 15:49:22.989823: step 36440, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 71h:16m:36s remains)
INFO - root - 2017-12-09 15:49:31.641449: step 36450, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.844 sec/batch; 69h:26m:20s remains)
INFO - root - 2017-12-09 15:49:40.248950: step 36460, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 71h:04m:25s remains)
INFO - root - 2017-12-09 15:49:48.973156: step 36470, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.857 sec/batch; 70h:26m:29s remains)
INFO - root - 2017-12-09 15:49:57.369360: step 36480, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.849 sec/batch; 69h:48m:56s remains)
INFO - root - 2017-12-09 15:50:05.847028: step 36490, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.851 sec/batch; 69h:59m:24s remains)
INFO - root - 2017-12-09 15:50:14.470487: step 36500, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.859 sec/batch; 70h:35m:47s remains)
2017-12-09 15:50:15.363192: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0017913368 -0.0017792669 -0.0017710297 -0.001768411 -0.0017726448 -0.001780118 -0.0017876 -0.001792195 -0.001793841 -0.0017942339 -0.0017940495 -0.001794033 -0.00179475 -0.0017971648 -0.0018003255][-0.0017888123 -0.0017770258 -0.0017696682 -0.0017687017 -0.001775059 -0.0017843451 -0.0017934567 -0.001799123 -0.0018015277 -0.0018022349 -0.0018012436 -0.001800782 -0.0018004902 -0.0018027108 -0.0018058834][-0.0017878907 -0.0017764338 -0.0017698448 -0.0017702843 -0.0017779008 -0.0017879895 -0.001798164 -0.0018042931 -0.0018073979 -0.0018083091 -0.001807277 -0.0018065837 -0.0018059204 -0.0018081294 -0.0018109278][-0.0017870633 -0.0017757985 -0.0017695081 -0.0017705892 -0.001778669 -0.0017888623 -0.0017995308 -0.001806287 -0.0018100588 -0.0018109878 -0.0018094269 -0.0018084638 -0.0018076901 -0.0018100253 -0.0018128981][-0.0017869608 -0.0017758876 -0.0017697169 -0.001770873 -0.0017788542 -0.0017888662 -0.0017991443 -0.0018056477 -0.0018095033 -0.0018106773 -0.0018090644 -0.0018079851 -0.0018073588 -0.0018095433 -0.0018118082][-0.0017883138 -0.0017775841 -0.0017713046 -0.0017721271 -0.0017793413 -0.0017883552 -0.0017969703 -0.0018022755 -0.0018051631 -0.0018052281 -0.0018028989 -0.0018016886 -0.0018016902 -0.0018041399 -0.0018061887][-0.001791529 -0.0017815154 -0.0017750193 -0.0017751851 -0.0017814193 -0.0017890433 -0.0017956726 -0.0017992182 -0.0018004824 -0.0017990046 -0.0017957233 -0.001794157 -0.0017942562 -0.0017964629 -0.0017981537][-0.0017949843 -0.0017858613 -0.0017794872 -0.0017786052 -0.0017835769 -0.0017900112 -0.0017955644 -0.0017980359 -0.0017978557 -0.0017947596 -0.0017898539 -0.0017870966 -0.0017866809 -0.0017884145 -0.0017893994][-0.0017995818 -0.0017918121 -0.0017860364 -0.0017847985 -0.0017884561 -0.0017936068 -0.0017985379 -0.0018002348 -0.0017986025 -0.0017939579 -0.0017870499 -0.0017817818 -0.0017790432 -0.0017787226 -0.0017783564][-0.0018050065 -0.001798692 -0.0017936818 -0.0017918028 -0.0017939224 -0.0017977034 -0.0018015436 -0.001803135 -0.0018013449 -0.0017959642 -0.0017877042 -0.0017799807 -0.0017743518 -0.0017709456 -0.001767871][-0.0018104285 -0.001805307 -0.0018007288 -0.0017981903 -0.0017992236 -0.0018015795 -0.0018043821 -0.0018056009 -0.0018043721 -0.0017993777 -0.0017907561 -0.0017815224 -0.0017733031 -0.0017662395 -0.0017597958][-0.0018151217 -0.0018111264 -0.0018069394 -0.001803417 -0.0018025369 -0.0018029341 -0.0018048172 -0.001806484 -0.0018068836 -0.0018037619 -0.0017968622 -0.0017876496 -0.0017777601 -0.0017681132 -0.0017584978][-0.0018174032 -0.0018142246 -0.0018105415 -0.001806592 -0.0018039314 -0.0018024914 -0.0018032057 -0.0018049681 -0.0018062436 -0.0018052838 -0.0018012574 -0.0017942933 -0.0017846026 -0.0017741013 -0.0017626794][-0.0018179831 -0.0018159369 -0.0018128416 -0.0018090411 -0.0018052539 -0.0018023174 -0.0018012843 -0.0018020896 -0.0018035152 -0.0018044562 -0.0018033392 -0.0017992798 -0.0017920521 -0.0017827842 -0.0017712538][-0.0018169535 -0.0018153298 -0.0018125385 -0.0018086812 -0.0018046668 -0.0018011727 -0.0017988792 -0.0017990401 -0.0018007171 -0.0018023577 -0.0018032865 -0.0018025333 -0.0017988277 -0.0017923394 -0.0017818129]]...]
INFO - root - 2017-12-09 15:50:23.758104: step 36510, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.870 sec/batch; 71h:33m:10s remains)
INFO - root - 2017-12-09 15:50:32.397565: step 36520, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.855 sec/batch; 70h:16m:16s remains)
INFO - root - 2017-12-09 15:50:40.883646: step 36530, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.846 sec/batch; 69h:34m:36s remains)
INFO - root - 2017-12-09 15:50:49.463368: step 36540, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.856 sec/batch; 70h:23m:02s remains)
INFO - root - 2017-12-09 15:50:58.004695: step 36550, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.850 sec/batch; 69h:52m:54s remains)
INFO - root - 2017-12-09 15:51:06.628297: step 36560, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.881 sec/batch; 72h:26m:33s remains)
INFO - root - 2017-12-09 15:51:15.233357: step 36570, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.842 sec/batch; 69h:14m:33s remains)
INFO - root - 2017-12-09 15:51:23.563854: step 36580, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.856 sec/batch; 70h:21m:53s remains)
INFO - root - 2017-12-09 15:51:32.195646: step 36590, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.881 sec/batch; 72h:26m:19s remains)
INFO - root - 2017-12-09 15:51:40.806674: step 36600, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 71h:02m:01s remains)
2017-12-09 15:51:41.711757: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.23874237 0.23976855 0.23906347 0.23632365 0.23134127 0.22478171 0.21734689 0.20983367 0.2029493 0.19741458 0.19159198 0.18478331 0.17901698 0.17360832 0.16820425][0.24599966 0.25004518 0.25164291 0.25056818 0.24623927 0.23908386 0.23035809 0.22118422 0.21304619 0.2062126 0.19988088 0.19297847 0.18690878 0.1814739 0.17587526][0.2492484 0.25554046 0.25915048 0.25932685 0.25496539 0.24801613 0.23886816 0.22908117 0.22031094 0.2129012 0.20632327 0.19896457 0.19249429 0.1866805 0.18097277][0.25209048 0.26013228 0.26502618 0.26646009 0.26307607 0.2557444 0.2461237 0.23615114 0.22695315 0.21862799 0.21202394 0.2051087 0.19913052 0.19330423 0.18763274][0.25255033 0.26226869 0.26799765 0.27036479 0.26730558 0.2605831 0.25148824 0.24173941 0.23282149 0.22445752 0.21798442 0.21183491 0.20639378 0.20158438 0.19627401][0.25020248 0.26139623 0.26784518 0.2702741 0.26735425 0.26105928 0.25312465 0.24480355 0.23704818 0.23028408 0.22516122 0.22026311 0.2150005 0.21088561 0.20577899][0.24469136 0.2565167 0.262739 0.26504353 0.262597 0.25756517 0.25151572 0.24538308 0.24012817 0.23581789 0.23238605 0.22909273 0.22485331 0.22150502 0.21649902][0.23801686 0.24981193 0.25543389 0.25681448 0.25411096 0.24972506 0.24486083 0.24152358 0.23903811 0.23753643 0.23672551 0.23510812 0.23230368 0.22941081 0.22416145][0.22901633 0.24113008 0.2461102 0.24698879 0.24459743 0.24081822 0.23714316 0.23571633 0.23531298 0.23597619 0.23704827 0.23765838 0.23662001 0.23430102 0.2293027][0.21890621 0.23100689 0.23556374 0.23630486 0.23386727 0.23052096 0.22786085 0.22778833 0.22906046 0.23179568 0.23490083 0.23687321 0.23668382 0.23467897 0.22974908][0.20851876 0.220355 0.22442278 0.22491249 0.22288564 0.2199205 0.21780056 0.21832745 0.22053686 0.22436678 0.22821514 0.23060928 0.2305295 0.22825228 0.22316769][0.19729507 0.20849794 0.21239018 0.21294299 0.21110092 0.20876737 0.20720626 0.20793988 0.21041225 0.21411154 0.21779302 0.2196151 0.219341 0.21675023 0.21170397][0.18652195 0.19684812 0.20028935 0.20057142 0.19853351 0.19598302 0.19411945 0.19467025 0.19647136 0.19965102 0.20309408 0.20450617 0.20407894 0.20139843 0.1967829][0.17477465 0.1841775 0.18717565 0.18734238 0.18559429 0.18328027 0.181494 0.18124172 0.18207991 0.18455629 0.18735978 0.18846902 0.18811764 0.18600737 0.18269025][0.16543138 0.17338283 0.17541546 0.17546508 0.17398575 0.1717207 0.16979699 0.16912968 0.16933218 0.17093414 0.17302075 0.17374459 0.17344855 0.17198919 0.16942811]]...]
INFO - root - 2017-12-09 15:51:50.149832: step 36610, loss = 0.82, batch loss = 0.69 (10.7 examples/sec; 0.748 sec/batch; 61h:29m:47s remains)
INFO - root - 2017-12-09 15:51:58.816602: step 36620, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.872 sec/batch; 71h:40m:07s remains)
INFO - root - 2017-12-09 15:52:07.302835: step 36630, loss = 0.82, batch loss = 0.69 (9.8 examples/sec; 0.812 sec/batch; 66h:46m:13s remains)
INFO - root - 2017-12-09 15:52:16.098906: step 36640, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.860 sec/batch; 70h:41m:10s remains)
INFO - root - 2017-12-09 15:52:24.908148: step 36650, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.894 sec/batch; 73h:25m:49s remains)
INFO - root - 2017-12-09 15:52:33.652242: step 36660, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.878 sec/batch; 72h:10m:06s remains)
INFO - root - 2017-12-09 15:52:42.293135: step 36670, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.833 sec/batch; 68h:28m:28s remains)
INFO - root - 2017-12-09 15:52:50.802469: step 36680, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.863 sec/batch; 70h:54m:19s remains)
INFO - root - 2017-12-09 15:52:59.550974: step 36690, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 71h:24m:18s remains)
INFO - root - 2017-12-09 15:53:08.367717: step 36700, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.904 sec/batch; 74h:15m:40s remains)
2017-12-09 15:53:09.225615: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0018029943 -0.0018015292 -0.0018014149 -0.0018017904 -0.0018025439 -0.0018041643 -0.0018069396 -0.0018107321 -0.001814696 -0.001818567 -0.0018224684 -0.0018253481 -0.0018257018 -0.0018240944 -0.0018213298][-0.0018012306 -0.0017997517 -0.001799698 -0.0018001755 -0.0018010627 -0.0018026489 -0.0018052629 -0.0018086898 -0.0018121303 -0.0018153911 -0.0018186839 -0.0018211235 -0.0018215119 -0.0018204763 -0.0018188937][-0.001800574 -0.0017991099 -0.0017990494 -0.0017995454 -0.001800372 -0.0018016226 -0.001803587 -0.0018062064 -0.0018088213 -0.0018113976 -0.0018141924 -0.0018162619 -0.0018167415 -0.0018163505 -0.0018157441][-0.001800379 -0.0017989358 -0.0017988309 -0.0017992553 -0.0017998638 -0.0018005938 -0.001801738 -0.0018035207 -0.0018054168 -0.0018074339 -0.0018097383 -0.0018115268 -0.0018121476 -0.0018122693 -0.0018122618][-0.0018003153 -0.0017989364 -0.0017989036 -0.0017993448 -0.0017997548 -0.0017999749 -0.0018003811 -0.0018013017 -0.0018025016 -0.0018040055 -0.001805843 -0.0018074112 -0.0018081082 -0.0018084996 -0.0018087428][-0.0018002429 -0.001798908 -0.0017989469 -0.0017994044 -0.0017996625 -0.0017995725 -0.0017994224 -0.0017995861 -0.0017999858 -0.00180084 -0.0018020982 -0.001803341 -0.0018041671 -0.0018048096 -0.0018051957][-0.0018002955 -0.001798988 -0.0017990372 -0.0017994409 -0.0017995582 -0.0017992995 -0.0017988558 -0.0017984479 -0.0017982507 -0.0017985009 -0.0017991633 -0.001800047 -0.001800872 -0.0018016124 -0.0018020679][-0.0018005545 -0.001799088 -0.0017990824 -0.0017994277 -0.0017994869 -0.0017991266 -0.0017985236 -0.001797845 -0.001797391 -0.0017974071 -0.0017977364 -0.0017983214 -0.0017989675 -0.0017996565 -0.0018000387][-0.0018005905 -0.0017991486 -0.0017991371 -0.0017993674 -0.0017993969 -0.0017990863 -0.001798495 -0.0017977455 -0.0017972424 -0.0017971388 -0.0017972268 -0.0017974811 -0.0017978936 -0.0017984427 -0.0017987377][-0.0018005738 -0.0017992789 -0.0017992579 -0.0017993848 -0.0017994072 -0.0017991585 -0.0017986119 -0.0017977846 -0.0017971375 -0.0017968405 -0.0017966718 -0.0017966714 -0.0017970043 -0.0017975642 -0.0017979291][-0.0018004121 -0.0017992383 -0.0017992675 -0.001799404 -0.0017994853 -0.0017993127 -0.0017987675 -0.0017978202 -0.0017969845 -0.0017965434 -0.0017962501 -0.0017961494 -0.001796431 -0.0017969537 -0.0017973934][-0.0018003593 -0.0017992167 -0.0017992284 -0.0017993811 -0.00179949 -0.0017993764 -0.0017988429 -0.0017978341 -0.0017969065 -0.0017964082 -0.0017961354 -0.0017960671 -0.0017963151 -0.0017967556 -0.0017971281][-0.0018001792 -0.0017991164 -0.0017991271 -0.0017993401 -0.0017994901 -0.0017994585 -0.0017990342 -0.0017981259 -0.0017972066 -0.0017966164 -0.001796252 -0.001796141 -0.0017963724 -0.0017967504 -0.0017970001][-0.0018001772 -0.0017990369 -0.0017990423 -0.0017992932 -0.0017995298 -0.0017995855 -0.0017993082 -0.0017986115 -0.0017977605 -0.0017970725 -0.0017965245 -0.0017962145 -0.0017963151 -0.0017966205 -0.0017967727][-0.0018003404 -0.0017990503 -0.0017989905 -0.0017993185 -0.0017996613 -0.0017997896 -0.0017996014 -0.0017991142 -0.001798416 -0.0017977318 -0.0017970988 -0.001796681 -0.001796608 -0.0017967239 -0.0017967252]]...]
INFO - root - 2017-12-09 15:53:17.848023: step 36710, loss = 0.82, batch loss = 0.69 (10.5 examples/sec; 0.760 sec/batch; 62h:27m:28s remains)
INFO - root - 2017-12-09 15:53:26.565403: step 36720, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 71h:21m:37s remains)
INFO - root - 2017-12-09 15:53:35.017448: step 36730, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.861 sec/batch; 70h:44m:15s remains)
INFO - root - 2017-12-09 15:53:43.513920: step 36740, loss = 0.81, batch loss = 0.68 (9.6 examples/sec; 0.833 sec/batch; 68h:25m:38s remains)
INFO - root - 2017-12-09 15:53:52.000897: step 36750, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.849 sec/batch; 69h:46m:59s remains)
INFO - root - 2017-12-09 15:54:00.664752: step 36760, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.874 sec/batch; 71h:46m:44s remains)
INFO - root - 2017-12-09 15:54:09.383346: step 36770, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.888 sec/batch; 72h:58m:09s remains)
INFO - root - 2017-12-09 15:54:17.891143: step 36780, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.856 sec/batch; 70h:19m:21s remains)
INFO - root - 2017-12-09 15:54:26.587862: step 36790, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.884 sec/batch; 72h:35m:32s remains)
INFO - root - 2017-12-09 15:54:35.319940: step 36800, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.834 sec/batch; 68h:29m:48s remains)
2017-12-09 15:54:36.200704: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.001818681 -0.0018126797 -0.0018076216 -0.0018042983 -0.0018041679 -0.001806375 -0.0018100125 -0.0018148051 -0.0018191563 -0.0018212516 -0.0018211119 -0.0018200596 -0.001819812 -0.0018208495 -0.0018229685][-0.0018156493 -0.0018089459 -0.0018037651 -0.0018015404 -0.0018019404 -0.0018046087 -0.001808651 -0.0018140099 -0.0018186459 -0.0018209901 -0.0018208368 -0.0018197305 -0.0018195577 -0.0018206297 -0.0018228893][-0.0018144494 -0.0018076028 -0.0018028658 -0.001801702 -0.0018031572 -0.0018067547 -0.001811415 -0.0018165914 -0.0018203813 -0.0018217376 -0.0018209734 -0.0018197611 -0.0018195263 -0.001820644 -0.0018231017][-0.0018158542 -0.0018090293 -0.0018050852 -0.001805253 -0.001808132 -0.0018122002 -0.0018166066 -0.0018205439 -0.0018226055 -0.0018226985 -0.001821327 -0.0018201709 -0.0018200311 -0.0018211735 -0.0018237359][-0.0018185895 -0.0018119452 -0.001808352 -0.0018091694 -0.0018127857 -0.0018169497 -0.0018209622 -0.0018238126 -0.0018246589 -0.0018237935 -0.0018221645 -0.0018213545 -0.0018217177 -0.0018231677 -0.0018258169][-0.001822697 -0.0018171747 -0.0018144795 -0.0018157277 -0.0018191083 -0.0018224587 -0.0018251808 -0.0018263395 -0.0018259214 -0.0018243461 -0.0018228079 -0.00182254 -0.0018236084 -0.0018255711 -0.0018283093][-0.0018268877 -0.0018227762 -0.0018202385 -0.00182064 -0.0018230851 -0.0018255508 -0.0018274705 -0.0018277334 -0.0018267831 -0.0018249253 -0.0018235433 -0.0018237289 -0.0018252832 -0.0018275202 -0.001830212][-0.0018300983 -0.0018281544 -0.001826657 -0.0018268445 -0.0018282484 -0.0018294365 -0.001829966 -0.0018291561 -0.0018276219 -0.0018257183 -0.0018246017 -0.0018250071 -0.0018267714 -0.0018291126 -0.0018315292][-0.0018312425 -0.0018308645 -0.0018304071 -0.0018307018 -0.0018314165 -0.0018319113 -0.0018315664 -0.0018301783 -0.0018282909 -0.0018263637 -0.0018253893 -0.0018259663 -0.0018279637 -0.0018304103 -0.001832434][-0.0018301788 -0.0018307909 -0.0018312959 -0.0018321438 -0.0018328854 -0.001833189 -0.001832712 -0.0018312868 -0.0018291894 -0.0018269558 -0.0018258623 -0.0018265126 -0.0018286769 -0.0018311208 -0.0018325991][-0.0018285713 -0.0018293471 -0.0018302903 -0.0018317963 -0.001833201 -0.0018338979 -0.0018336462 -0.0018321887 -0.00182985 -0.0018273122 -0.0018258543 -0.0018262703 -0.001828427 -0.001830854 -0.0018320254][-0.0018282051 -0.0018291197 -0.0018303368 -0.0018321355 -0.0018338972 -0.0018348946 -0.0018346205 -0.0018329285 -0.00183018 -0.0018274146 -0.0018256382 -0.0018255671 -0.0018273405 -0.0018297049 -0.0018310905][-0.0018292675 -0.0018304228 -0.0018318207 -0.0018336079 -0.0018352405 -0.0018359313 -0.0018352724 -0.0018330896 -0.0018299071 -0.0018268942 -0.0018248822 -0.0018244624 -0.0018258609 -0.0018282167 -0.0018300128][-0.0018315621 -0.0018327793 -0.001834063 -0.00183549 -0.0018365131 -0.0018365392 -0.0018352545 -0.0018325361 -0.001829033 -0.0018258087 -0.001823665 -0.0018231093 -0.0018244793 -0.0018270123 -0.0018292005][-0.0018340319 -0.0018349568 -0.0018359713 -0.0018367863 -0.0018369774 -0.0018362249 -0.0018343754 -0.0018312853 -0.0018276382 -0.0018243639 -0.0018222417 -0.0018217082 -0.0018230793 -0.0018258556 -0.0018285124]]...]
INFO - root - 2017-12-09 15:54:44.749953: step 36810, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.888 sec/batch; 72h:57m:00s remains)
INFO - root - 2017-12-09 15:54:53.271397: step 36820, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.865 sec/batch; 71h:03m:05s remains)
INFO - root - 2017-12-09 15:55:01.734875: step 36830, loss = 0.82, batch loss = 0.69 (11.0 examples/sec; 0.728 sec/batch; 59h:46m:50s remains)
INFO - root - 2017-12-09 15:55:10.367770: step 36840, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.888 sec/batch; 72h:57m:03s remains)
INFO - root - 2017-12-09 15:55:18.920580: step 36850, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.849 sec/batch; 69h:41m:06s remains)
INFO - root - 2017-12-09 15:55:27.576743: step 36860, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.884 sec/batch; 72h:33m:36s remains)
INFO - root - 2017-12-09 15:55:36.252180: step 36870, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.848 sec/batch; 69h:40m:37s remains)
INFO - root - 2017-12-09 15:55:44.839958: step 36880, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.882 sec/batch; 72h:25m:50s remains)
INFO - root - 2017-12-09 15:55:53.487215: step 36890, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.843 sec/batch; 69h:15m:34s remains)
INFO - root - 2017-12-09 15:56:02.127627: step 36900, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 70h:47m:01s remains)
2017-12-09 15:56:03.000662: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0018076341 -0.001806699 -0.0018068033 -0.0018070942 -0.001807535 -0.0018079177 -0.0018080475 -0.0018078476 -0.001807559 -0.0018074446 -0.0018074147 -0.0018071749 -0.0018066377 -0.0018062615 -0.0018063054][-0.0018079837 -0.0018070685 -0.0018071344 -0.0018073998 -0.0018078915 -0.0018083351 -0.0018085398 -0.0018083804 -0.0018080503 -0.0018078143 -0.0018076386 -0.0018071812 -0.0018063831 -0.0018057661 -0.0018056403][-0.0018086471 -0.001807989 -0.0018082102 -0.0018084655 -0.00180889 -0.0018093149 -0.0018096099 -0.001809597 -0.0018093373 -0.001809091 -0.0018087901 -0.0018080546 -0.0018069459 -0.0018060703 -0.001805793][-0.0018093144 -0.0018089586 -0.0018095052 -0.0018098446 -0.0018101429 -0.0018104273 -0.0018107216 -0.001810797 -0.0018106824 -0.0018105824 -0.0018101546 -0.0018089968 -0.0018075065 -0.0018064204 -0.001806026][-0.0018102436 -0.0018102427 -0.0018110749 -0.0018116324 -0.0018119367 -0.0018120253 -0.0018120342 -0.0018119935 -0.0018120256 -0.0018120997 -0.0018115576 -0.0018100201 -0.0018082204 -0.0018069721 -0.0018064141][-0.0018109499 -0.0018112683 -0.0018124203 -0.0018134413 -0.0018140241 -0.0018140934 -0.0018136849 -0.001813206 -0.0018131541 -0.0018132596 -0.0018126697 -0.0018109692 -0.0018090751 -0.0018077744 -0.0018071426][-0.0018117062 -0.001812218 -0.0018136167 -0.0018150801 -0.0018159957 -0.0018161089 -0.0018152094 -0.0018140095 -0.0018134747 -0.0018135124 -0.001813047 -0.0018114994 -0.0018099131 -0.0018089045 -0.0018084724][-0.0018126133 -0.0018132216 -0.0018147242 -0.0018164536 -0.0018176509 -0.0018178334 -0.0018167637 -0.0018151579 -0.0018140441 -0.0018138688 -0.0018134749 -0.0018122204 -0.001811086 -0.0018105283 -0.0018104243][-0.0018139235 -0.0018146368 -0.0018161524 -0.001817996 -0.0018193672 -0.0018196995 -0.0018187856 -0.0018172127 -0.0018156786 -0.0018149101 -0.0018142767 -0.001813177 -0.0018124927 -0.001812377 -0.0018125795][-0.0018154301 -0.0018161469 -0.0018175257 -0.0018192491 -0.0018206144 -0.0018210397 -0.0018203399 -0.0018189941 -0.0018173375 -0.0018160525 -0.0018151244 -0.0018141477 -0.00181376 -0.0018139135 -0.0018143649][-0.0018163797 -0.0018170092 -0.001818263 -0.0018197672 -0.001820898 -0.0018213594 -0.0018208624 -0.0018197522 -0.0018181873 -0.0018167625 -0.0018157962 -0.0018149815 -0.00181475 -0.001814993 -0.0018155628][-0.0018173339 -0.0018177012 -0.0018187261 -0.0018197714 -0.00182042 -0.0018206333 -0.0018201657 -0.0018193106 -0.0018180843 -0.0018169548 -0.0018162164 -0.0018156798 -0.0018155805 -0.0018157524 -0.0018162275][-0.0018176207 -0.0018176674 -0.0018182815 -0.0018188887 -0.0018191335 -0.0018191427 -0.0018187138 -0.0018181169 -0.0018173471 -0.0018166956 -0.0018163389 -0.0018160766 -0.0018160013 -0.001816022 -0.0018162359][-0.0018169219 -0.0018166656 -0.0018168426 -0.0018171364 -0.0018172237 -0.0018171945 -0.0018167783 -0.0018163059 -0.0018159349 -0.0018157547 -0.0018157333 -0.0018156985 -0.0018157203 -0.0018157342 -0.0018157274][-0.0018157026 -0.0018150811 -0.0018148959 -0.0018149894 -0.0018150774 -0.0018150874 -0.00181472 -0.0018143521 -0.001814222 -0.0018143395 -0.0018145649 -0.0018147088 -0.0018148447 -0.0018148929 -0.0018147874]]...]
INFO - root - 2017-12-09 15:56:11.719912: step 36910, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.858 sec/batch; 70h:28m:59s remains)
INFO - root - 2017-12-09 15:56:20.233042: step 36920, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.847 sec/batch; 69h:32m:53s remains)
INFO - root - 2017-12-09 15:56:28.667586: step 36930, loss = 0.82, batch loss = 0.69 (11.2 examples/sec; 0.715 sec/batch; 58h:43m:08s remains)
INFO - root - 2017-12-09 15:56:37.440436: step 36940, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.884 sec/batch; 72h:36m:27s remains)
INFO - root - 2017-12-09 15:56:46.053334: step 36950, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.858 sec/batch; 70h:28m:26s remains)
INFO - root - 2017-12-09 15:56:54.705922: step 36960, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.884 sec/batch; 72h:34m:03s remains)
INFO - root - 2017-12-09 15:57:03.295330: step 36970, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.859 sec/batch; 70h:31m:59s remains)
INFO - root - 2017-12-09 15:57:11.817367: step 36980, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.873 sec/batch; 71h:40m:30s remains)
INFO - root - 2017-12-09 15:57:20.405799: step 36990, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.845 sec/batch; 69h:23m:00s remains)
INFO - root - 2017-12-09 15:57:29.185387: step 37000, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.901 sec/batch; 73h:57m:24s remains)
2017-12-09 15:57:30.080077: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0018031319 -0.0018015 -0.0018019369 -0.0018035051 -0.0018057108 -0.001807994 -0.0018104763 -0.0018128429 -0.0018152279 -0.0018173583 -0.001818768 -0.0018189044 -0.001817471 -0.0018151379 -0.0018121034][-0.0018012349 -0.0017997242 -0.0017999869 -0.0018013562 -0.0018035601 -0.0018061391 -0.0018093759 -0.0018128996 -0.0018164683 -0.0018200295 -0.0018227239 -0.0018239239 -0.0018228192 -0.0018202128 -0.0018167057][-0.0017996115 -0.0017982541 -0.0017984399 -0.0017996002 -0.00180154 -0.001804163 -0.0018078274 -0.0018122386 -0.0018169526 -0.0018218108 -0.0018258546 -0.0018281451 -0.001827584 -0.0018250792 -0.0018212283][-0.0017989633 -0.0017977009 -0.0017978047 -0.0017988213 -0.0018005479 -0.0018029414 -0.0018063916 -0.0018108428 -0.0018158898 -0.0018213615 -0.0018260635 -0.0018291709 -0.0018293415 -0.0018272605 -0.0018233842][-0.0017986135 -0.0017972892 -0.0017972881 -0.0017982442 -0.0017998646 -0.0018020731 -0.001805243 -0.0018094549 -0.0018143764 -0.0018196953 -0.0018243332 -0.0018275977 -0.0018280764 -0.0018263195 -0.0018226955][-0.0017984521 -0.0017970181 -0.0017970095 -0.0017980026 -0.0017995308 -0.0018015 -0.001804297 -0.00180809 -0.0018124856 -0.0018170824 -0.001821289 -0.0018243775 -0.0018250893 -0.0018236194 -0.0018203967][-0.0017982593 -0.0017966396 -0.0017965329 -0.0017974642 -0.0017988312 -0.001800464 -0.0018027906 -0.001805969 -0.0018094708 -0.0018131202 -0.0018166744 -0.001819436 -0.0018204041 -0.0018195665 -0.001817167][-0.0017979722 -0.0017962038 -0.0017960335 -0.0017968498 -0.0017980153 -0.0017992895 -0.0018010391 -0.0018034494 -0.0018060865 -0.0018087381 -0.0018114733 -0.0018137229 -0.0018147782 -0.0018144199 -0.0018128048][-0.0017980699 -0.0017961876 -0.0017959217 -0.0017965158 -0.0017973437 -0.0017982607 -0.0017994925 -0.0018011577 -0.0018029462 -0.0018045882 -0.0018063942 -0.0018080422 -0.001809024 -0.0018090827 -0.001808224][-0.0017981908 -0.0017963793 -0.0017960797 -0.0017964226 -0.0017968605 -0.0017973972 -0.0017982095 -0.0017992515 -0.0018002652 -0.0018011413 -0.0018022131 -0.0018032425 -0.001804031 -0.0018043991 -0.0018042437][-0.0017983553 -0.0017967828 -0.0017965216 -0.001796608 -0.001796698 -0.0017968544 -0.001797303 -0.0017978745 -0.001798372 -0.0017987925 -0.001799451 -0.0018001611 -0.0018009029 -0.0018015684 -0.0018019881][-0.001798771 -0.0017972621 -0.0017968661 -0.0017967246 -0.0017965956 -0.0017965126 -0.0017967063 -0.0017970846 -0.0017974477 -0.0017978371 -0.0017984263 -0.0017991066 -0.0017999551 -0.0018009528 -0.0018019086][-0.001799244 -0.0017977824 -0.0017972843 -0.0017970537 -0.0017968715 -0.0017967233 -0.0017968224 -0.0017971772 -0.0017976613 -0.0017982289 -0.0017988923 -0.0017996513 -0.001800663 -0.0018019675 -0.0018034749][-0.0018000142 -0.0017986175 -0.0017980713 -0.0017977072 -0.0017974089 -0.0017972015 -0.001797245 -0.001797608 -0.0017982359 -0.0017990526 -0.0017999939 -0.0018010223 -0.001802345 -0.0018041056 -0.0018063616][-0.0018001532 -0.0017987353 -0.0017980359 -0.0017975409 -0.0017971959 -0.0017970139 -0.0017971587 -0.001797749 -0.0017987124 -0.0017999025 -0.0018012288 -0.0018026895 -0.0018044431 -0.0018067562 -0.0018096858]]...]
INFO - root - 2017-12-09 15:57:38.700132: step 37010, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 71h:11m:45s remains)
INFO - root - 2017-12-09 15:57:47.232482: step 37020, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 70h:56m:26s remains)
INFO - root - 2017-12-09 15:57:55.767230: step 37030, loss = 0.82, batch loss = 0.69 (11.2 examples/sec; 0.715 sec/batch; 58h:42m:43s remains)
INFO - root - 2017-12-09 15:58:04.492984: step 37040, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.851 sec/batch; 69h:51m:17s remains)
INFO - root - 2017-12-09 15:58:13.197778: step 37050, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.858 sec/batch; 70h:24m:49s remains)
INFO - root - 2017-12-09 15:58:21.951099: step 37060, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.858 sec/batch; 70h:22m:57s remains)
INFO - root - 2017-12-09 15:58:30.578302: step 37070, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.832 sec/batch; 68h:16m:35s remains)
INFO - root - 2017-12-09 15:58:39.268585: step 37080, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.891 sec/batch; 73h:04m:32s remains)
INFO - root - 2017-12-09 15:58:47.888908: step 37090, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.836 sec/batch; 68h:36m:29s remains)
INFO - root - 2017-12-09 15:58:56.520470: step 37100, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.886 sec/batch; 72h:41m:50s remains)
2017-12-09 15:58:57.439780: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.075844839 0.070227332 0.0656172 0.062140793 0.059945118 0.058593936 0.057560384 0.056892537 0.056197003 0.055584263 0.054816313 0.05374673 0.052320015 0.050734188 0.0488817][0.080951959 0.075913474 0.071634807 0.068524793 0.066722468 0.065625519 0.06489981 0.064360641 0.063756295 0.063433975 0.063033588 0.06233532 0.061193652 0.05983955 0.058212068][0.084336 0.080097869 0.076301485 0.073753566 0.072820619 0.072362721 0.07224223 0.071942993 0.071596839 0.071330659 0.070840314 0.070178382 0.068987757 0.067460448 0.06581907][0.089775175 0.086153209 0.082600057 0.080519006 0.080004625 0.080138728 0.08061973 0.080920883 0.081203826 0.081345171 0.080849558 0.079929061 0.078452773 0.076257676 0.073914915][0.094792008 0.092043594 0.088869646 0.087295309 0.087151833 0.087637037 0.088672914 0.089666478 0.090791337 0.091379538 0.091217056 0.090094127 0.088104263 0.085266717 0.08216837][0.098207593 0.096485272 0.09384308 0.092602089 0.092489854 0.093461163 0.094889738 0.096284054 0.098124541 0.0994983 0.10017418 0.0992021 0.097146161 0.094050564 0.090594932][0.10073352 0.099817552 0.096975505 0.095551357 0.095012173 0.095749006 0.097006343 0.098603569 0.10115836 0.10336866 0.10512129 0.10510457 0.10391696 0.10143817 0.098201111][0.10223569 0.10183498 0.098821186 0.096660614 0.095125519 0.094839565 0.095339067 0.097023651 0.099923 0.10289835 0.10573904 0.10729742 0.1074072 0.10593595 0.10358942][0.100316 0.10051394 0.097381279 0.09439227 0.0917016 0.090192072 0.0898538 0.091183864 0.093933433 0.097679093 0.10165837 0.10453677 0.10599775 0.10605708 0.10509369][0.093244836 0.09428668 0.091163948 0.087695196 0.084390469 0.082077496 0.081171036 0.082182892 0.085033171 0.089159273 0.093577608 0.097273692 0.099956281 0.10115521 0.10123613][0.081389755 0.0827072 0.080012411 0.076610886 0.073066145 0.070256837 0.068806417 0.069426849 0.072094262 0.076317243 0.081133261 0.085494459 0.088982418 0.090933062 0.091797337][0.06586799 0.06730511 0.065238409 0.062378168 0.059138991 0.056352418 0.054750081 0.055088673 0.057452921 0.061337143 0.066028483 0.070650451 0.074630216 0.0772322 0.0788166][0.048685361 0.049857497 0.048597105 0.04655724 0.044133257 0.041933805 0.040810246 0.04129504 0.043382548 0.046820324 0.051091276 0.055445194 0.059430882 0.062337108 0.06443198][0.030894112 0.031723615 0.031006169 0.029796761 0.028613346 0.027476642 0.027125333 0.027931418 0.030032214 0.033099052 0.036643773 0.040375017 0.044094197 0.047239129 0.049835559][0.015789721 0.016297104 0.016037963 0.015541424 0.015147421 0.014914359 0.01534404 0.016430335 0.018342689 0.020941667 0.023841131 0.026762033 0.029798888 0.0328114 0.035688128]]...]
INFO - root - 2017-12-09 15:59:06.102723: step 37110, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.898 sec/batch; 73h:39m:02s remains)
INFO - root - 2017-12-09 15:59:14.645885: step 37120, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.856 sec/batch; 70h:15m:45s remains)
INFO - root - 2017-12-09 15:59:23.239157: step 37130, loss = 0.82, batch loss = 0.69 (10.7 examples/sec; 0.747 sec/batch; 61h:15m:29s remains)
INFO - root - 2017-12-09 15:59:31.832489: step 37140, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.901 sec/batch; 73h:57m:21s remains)
INFO - root - 2017-12-09 15:59:40.448592: step 37150, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.873 sec/batch; 71h:35m:33s remains)
INFO - root - 2017-12-09 15:59:49.026789: step 37160, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.880 sec/batch; 72h:13m:46s remains)
INFO - root - 2017-12-09 15:59:57.664506: step 37170, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.880 sec/batch; 72h:09m:56s remains)
INFO - root - 2017-12-09 16:00:06.254269: step 37180, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.882 sec/batch; 72h:23m:38s remains)
INFO - root - 2017-12-09 16:00:14.978648: step 37190, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 71h:12m:06s remains)
INFO - root - 2017-12-09 16:00:23.700352: step 37200, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.873 sec/batch; 71h:35m:50s remains)
2017-12-09 16:00:24.561565: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0017927443 -0.0017908748 -0.0017906721 -0.0017906674 -0.0017906222 -0.0017905687 -0.0017904806 -0.0017904243 -0.0017904215 -0.0017905377 -0.0017907331 -0.0017909487 -0.0017911136 -0.0017911728 -0.0017911193][-0.0017915147 -0.0017896264 -0.0017894538 -0.0017894811 -0.0017894489 -0.0017893931 -0.0017893158 -0.0017892593 -0.0017892616 -0.0017893683 -0.0017895631 -0.0017897866 -0.0017899681 -0.0017900375 -0.0017900382][-0.0017915609 -0.001789665 -0.0017894511 -0.0017894207 -0.0017893433 -0.0017892472 -0.0017891533 -0.0017891092 -0.0017891348 -0.0017892447 -0.0017894291 -0.0017896334 -0.0017898077 -0.0017899068 -0.0017899525][-0.0017917239 -0.001789856 -0.0017895832 -0.0017894737 -0.0017893135 -0.001789134 -0.0017889744 -0.0017889141 -0.0017889778 -0.0017891244 -0.0017893196 -0.0017895141 -0.0017896787 -0.0017897898 -0.0017898713][-0.001792011 -0.0017900834 -0.0017897316 -0.0017894824 -0.0017891288 -0.0017887572 -0.0017884838 -0.0017884388 -0.0017886037 -0.0017888601 -0.0017891369 -0.0017893661 -0.0017895291 -0.0017896406 -0.0017897459][-0.0017921478 -0.001790144 -0.0017896265 -0.0017891494 -0.0017885239 -0.0017879176 -0.0017875221 -0.0017875361 -0.0017878634 -0.0017883389 -0.0017888043 -0.0017891556 -0.0017893722 -0.0017895169 -0.0017896554][-0.0017920526 -0.0017899872 -0.0017892721 -0.0017885151 -0.0017875534 -0.0017866486 -0.0017861365 -0.0017862493 -0.001786791 -0.0017875547 -0.0017882985 -0.0017888506 -0.0017891799 -0.0017893921 -0.0017895774][-0.0017919748 -0.0017899228 -0.00178909 -0.0017880488 -0.0017867129 -0.0017854628 -0.0017847455 -0.0017848734 -0.0017856155 -0.0017866831 -0.0017877163 -0.0017884923 -0.0017889595 -0.0017892418 -0.0017894735][-0.0017918055 -0.0017897862 -0.0017888626 -0.0017875209 -0.0017858139 -0.0017842421 -0.0017834064 -0.0017835898 -0.0017845161 -0.0017858359 -0.0017870755 -0.0017880111 -0.0017886047 -0.0017889781 -0.0017892622][-0.0017912769 -0.001789304 -0.0017881986 -0.001786591 -0.0017846579 -0.0017829778 -0.0017822103 -0.0017825139 -0.0017836128 -0.0017851185 -0.0017864534 -0.0017874404 -0.0017880864 -0.0017885217 -0.0017888531][-0.0017905657 -0.0017884922 -0.0017872135 -0.0017856003 -0.0017838144 -0.0017823537 -0.0017817585 -0.0017821087 -0.0017832202 -0.0017846988 -0.0017859418 -0.0017868474 -0.001787478 -0.0017879369 -0.0017883028][-0.0017899072 -0.0017877131 -0.0017864814 -0.0017850819 -0.0017836571 -0.001782559 -0.0017821388 -0.0017823994 -0.0017832983 -0.0017845331 -0.001785541 -0.0017862971 -0.0017868807 -0.0017873482 -0.001787734][-0.0017894397 -0.0017872057 -0.0017860663 -0.0017849571 -0.0017839476 -0.0017832373 -0.0017829563 -0.0017830761 -0.0017837039 -0.0017846542 -0.0017854306 -0.0017860527 -0.0017866053 -0.0017870957 -0.0017874894][-0.0017891123 -0.0017867968 -0.0017857358 -0.0017849538 -0.0017843462 -0.0017839258 -0.0017837572 -0.0017838 -0.0017841889 -0.0017848448 -0.0017854813 -0.0017860917 -0.0017866752 -0.0017872015 -0.0017875399][-0.0017890004 -0.0017867391 -0.0017856979 -0.0017851549 -0.0017848115 -0.0017845813 -0.0017844154 -0.0017843297 -0.0017845053 -0.0017849453 -0.0017854943 -0.0017860972 -0.0017867201 -0.0017872176 -0.0017874163]]...]
INFO - root - 2017-12-09 16:00:33.467170: step 37210, loss = 0.82, batch loss = 0.69 (8.8 examples/sec; 0.910 sec/batch; 74h:38m:26s remains)
INFO - root - 2017-12-09 16:00:41.991583: step 37220, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.832 sec/batch; 68h:16m:59s remains)
INFO - root - 2017-12-09 16:00:50.571102: step 37230, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.865 sec/batch; 70h:54m:52s remains)
INFO - root - 2017-12-09 16:00:59.153834: step 37240, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 70h:52m:52s remains)
INFO - root - 2017-12-09 16:01:07.845411: step 37250, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.874 sec/batch; 71h:40m:08s remains)
INFO - root - 2017-12-09 16:01:16.511480: step 37260, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.863 sec/batch; 70h:48m:48s remains)
INFO - root - 2017-12-09 16:01:25.225075: step 37270, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.882 sec/batch; 72h:21m:17s remains)
INFO - root - 2017-12-09 16:01:33.910997: step 37280, loss = 0.82, batch loss = 0.69 (8.8 examples/sec; 0.904 sec/batch; 74h:08m:58s remains)
INFO - root - 2017-12-09 16:01:42.479015: step 37290, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.856 sec/batch; 70h:11m:14s remains)
INFO - root - 2017-12-09 16:01:50.888589: step 37300, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.841 sec/batch; 68h:59m:16s remains)
2017-12-09 16:01:51.738147: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.23526081 0.23337629 0.23217657 0.23129223 0.23165135 0.23357379 0.23585229 0.23737606 0.23699215 0.23487507 0.22952963 0.22248146 0.21327657 0.20372812 0.19483654][0.23591702 0.23671152 0.23836876 0.24052234 0.24385828 0.2479545 0.25206688 0.25534457 0.25606897 0.25430644 0.24809191 0.23957565 0.22854196 0.21678346 0.20584217][0.23601876 0.23938696 0.24382707 0.24911816 0.25486693 0.26120266 0.26726928 0.27110654 0.27243963 0.2706303 0.26379532 0.25364521 0.24071711 0.22735947 0.21446793][0.23760091 0.24327138 0.25005797 0.2573328 0.2651687 0.27275178 0.2794134 0.28339314 0.28451067 0.28257325 0.275031 0.26423112 0.25059095 0.23607005 0.2220491][0.23979816 0.2462921 0.2539393 0.26240146 0.27139217 0.27995729 0.28739974 0.29150695 0.29224145 0.2899192 0.28213638 0.27083415 0.25673226 0.24179374 0.22750117][0.24226029 0.24801144 0.2546567 0.26287526 0.27189746 0.28071076 0.28877941 0.29346731 0.29446176 0.29205382 0.28450409 0.2733359 0.25899094 0.24424075 0.22981077][0.24374859 0.2481183 0.25253716 0.25931358 0.26724973 0.27589318 0.28407261 0.28898266 0.2904633 0.28871176 0.28246024 0.27217045 0.25873974 0.2447094 0.23056623][0.24336059 0.24613608 0.24775778 0.25186414 0.25764349 0.26479933 0.27198711 0.2766149 0.27853239 0.27768284 0.27323735 0.26506007 0.25357273 0.24119276 0.2280488][0.23958144 0.24037085 0.23897381 0.24017718 0.24372113 0.24894886 0.25477767 0.25918856 0.26159582 0.26187798 0.25909355 0.25344062 0.24408914 0.2334213 0.22181329][0.23283666 0.23182712 0.22778751 0.22651087 0.22774334 0.23094812 0.23559181 0.23953919 0.24194391 0.24273174 0.24133283 0.23765801 0.2301141 0.22159518 0.21200819][0.22242679 0.22059655 0.21481018 0.21156833 0.2109935 0.21294017 0.21630797 0.21957983 0.22189622 0.22272418 0.22191478 0.21927829 0.21343264 0.20672543 0.19898628][0.209898 0.20820127 0.20193931 0.19754666 0.19554336 0.19563748 0.19700548 0.19923663 0.20109916 0.20184274 0.20172079 0.20041257 0.19671366 0.19184756 0.18633379][0.19454682 0.19390281 0.18869784 0.18433668 0.18175408 0.18045674 0.18022701 0.18094692 0.18178746 0.18222244 0.18244791 0.18228558 0.18027742 0.1773078 0.17409042][0.17609195 0.17644221 0.17249258 0.1690084 0.16654749 0.16499291 0.1642231 0.16438214 0.16492447 0.1653724 0.16611978 0.16682962 0.16625597 0.16501586 0.16389629][0.15426712 0.15593329 0.15386829 0.15181632 0.15013361 0.14881858 0.14784226 0.14775848 0.14820343 0.14875643 0.14993697 0.1513349 0.1522252 0.15277544 0.15361606]]...]
INFO - root - 2017-12-09 16:02:00.243633: step 37310, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.890 sec/batch; 73h:00m:02s remains)
INFO - root - 2017-12-09 16:02:08.718901: step 37320, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.873 sec/batch; 71h:36m:51s remains)
INFO - root - 2017-12-09 16:02:17.379686: step 37330, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.874 sec/batch; 71h:37m:44s remains)
INFO - root - 2017-12-09 16:02:25.885010: step 37340, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.874 sec/batch; 71h:39m:22s remains)
INFO - root - 2017-12-09 16:02:34.465418: step 37350, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.870 sec/batch; 71h:19m:20s remains)
INFO - root - 2017-12-09 16:02:42.908601: step 37360, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.848 sec/batch; 69h:29m:49s remains)
INFO - root - 2017-12-09 16:02:51.645593: step 37370, loss = 0.83, batch loss = 0.70 (9.1 examples/sec; 0.876 sec/batch; 71h:47m:15s remains)
INFO - root - 2017-12-09 16:03:00.280061: step 37380, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.853 sec/batch; 69h:53m:18s remains)
INFO - root - 2017-12-09 16:03:09.093013: step 37390, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.853 sec/batch; 69h:57m:46s remains)
INFO - root - 2017-12-09 16:03:17.829650: step 37400, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.853 sec/batch; 69h:55m:20s remains)
2017-12-09 16:03:18.745419: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.0089516286 0.010558212 0.01126716 0.011009639 0.0098919729 0.0081908051 0.0063781748 0.0048386054 0.003516742 0.0023244503 0.0011167704 7.80914e-06 -0.00087309966 -0.0014459536 -0.0016481456][0.016014833 0.018957578 0.020563219 0.020543635 0.019120047 0.016737569 0.013858416 0.011114081 0.0084787076 0.0059795678 0.003493533 0.0013422786 -0.00014418561 -0.0010148019 -0.0014417525][0.028409667 0.032857046 0.034834214 0.0343043 0.031835072 0.028253317 0.024246845 0.02050267 0.016880373 0.013063818 0.008944192 0.0049387142 0.0018941226 -2.8241193e-05 -0.001015021][0.04519837 0.051665176 0.05477985 0.0549125 0.052602913 0.04854057 0.043202575 0.03739959 0.031248858 0.024517225 0.017497648 0.010777554 0.0056088059 0.0021511428 7.4355979e-05][0.059898317 0.068324059 0.073205747 0.07535208 0.07513836 0.0728525 0.068228759 0.061492629 0.05291526 0.042551253 0.031380415 0.020405373 0.011599905 0.0055101616 0.0017247676][0.066637412 0.076649316 0.083525091 0.08842881 0.091497496 0.092353575 0.089954883 0.0838952 0.07435827 0.061526872 0.046890449 0.031925745 0.019345313 0.010176433 0.0041478635][0.06339936 0.074589409 0.083488658 0.091026232 0.096890345 0.1005552 0.10036831 0.095618434 0.08628165 0.072636925 0.056476958 0.039489191 0.024768094 0.013646194 0.0060575055][0.054024942 0.065635733 0.075917229 0.085197568 0.092860729 0.098139018 0.099232242 0.095318004 0.086352251 0.072805062 0.056573253 0.039540604 0.024847157 0.013800989 0.006288331][0.042626802 0.0530012 0.062635139 0.071491279 0.0790754 0.084595725 0.0863386 0.083386704 0.075556926 0.063350022 0.048575018 0.033199433 0.020186314 0.010771354 0.0046533444][0.030828068 0.039021656 0.046628844 0.053565942 0.059703071 0.064477235 0.0665161 0.064647347 0.058719374 0.048994 0.036972411 0.024461662 0.014063821 0.0068968982 0.0025176848][0.019190285 0.024559919 0.029487962 0.033931244 0.03804161 0.041593805 0.04363573 0.042981148 0.039366178 0.032840461 0.024477871 0.015655005 0.0083773872 0.00352448 0.000726696][0.00924805 0.012180274 0.014890754 0.017355936 0.019767707 0.022021189 0.023524275 0.023428539 0.021491231 0.017747797 0.012829434 0.0076619852 0.0034918841 0.000811149 -0.0006318728][0.0021084072 0.0032882891 0.0044263606 0.005494487 0.0065760454 0.0076009743 0.008312434 0.0083355326 0.0075027915 0.0058891568 0.003790813 0.0016626077 2.3812754e-05 -0.00097043515 -0.0014529896][-0.00093648228 -0.00062128925 -0.000297499 2.406782e-05 0.00035402819 0.00066312694 0.00087127287 0.00086496596 0.00059688755 0.00011681474 -0.00047484378 -0.0010457895 -0.0014580922 -0.001677888 -0.0017581685][-0.0017949743 -0.0017906164 -0.0017858293 -0.0017796805 -0.0017712164 -0.0017610982 -0.0017500062 -0.0017396619 -0.001735766 -0.0017359933 -0.0017434496 -0.0017530537 -0.0017633602 -0.0017727561 -0.0017780314]]...]
INFO - root - 2017-12-09 16:03:27.307585: step 37410, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.870 sec/batch; 71h:17m:20s remains)
INFO - root - 2017-12-09 16:03:35.706121: step 37420, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.891 sec/batch; 73h:01m:11s remains)
INFO - root - 2017-12-09 16:03:44.311590: step 37430, loss = 0.82, batch loss = 0.69 (9.7 examples/sec; 0.823 sec/batch; 67h:26m:30s remains)
INFO - root - 2017-12-09 16:03:52.889760: step 37440, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.876 sec/batch; 71h:46m:07s remains)
INFO - root - 2017-12-09 16:04:01.525348: step 37450, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.858 sec/batch; 70h:20m:32s remains)
INFO - root - 2017-12-09 16:04:10.076916: step 37460, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.844 sec/batch; 69h:12m:38s remains)
INFO - root - 2017-12-09 16:04:18.594125: step 37470, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.877 sec/batch; 71h:51m:54s remains)
INFO - root - 2017-12-09 16:04:27.190280: step 37480, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.888 sec/batch; 72h:47m:53s remains)
INFO - root - 2017-12-09 16:04:35.879761: step 37490, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.860 sec/batch; 70h:29m:40s remains)
INFO - root - 2017-12-09 16:04:44.544003: step 37500, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.885 sec/batch; 72h:30m:01s remains)
2017-12-09 16:04:45.352896: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.001832116 -0.0018327009 -0.001832868 -0.0018314436 -0.001828343 -0.0018248922 -0.0018218593 -0.0018195525 -0.0018182161 -0.0018175686 -0.0018178825 -0.0018183192 -0.0018183402 -0.0018179413 -0.0018170781][-0.0018307682 -0.001831303 -0.0018317127 -0.001830646 -0.0018280239 -0.0018250438 -0.0018224493 -0.0018203899 -0.0018191905 -0.0018186215 -0.0018190648 -0.0018196058 -0.0018194937 -0.0018187708 -0.0018175226][-0.0018295352 -0.0018300372 -0.0018307255 -0.0018302115 -0.0018283246 -0.0018262231 -0.0018244089 -0.0018228776 -0.0018220767 -0.0018218484 -0.0018224209 -0.0018231274 -0.0018231568 -0.0018224544 -0.0018210261][-0.0018285449 -0.001828942 -0.001829638 -0.001829505 -0.0018283275 -0.0018269755 -0.0018257645 -0.0018247669 -0.0018243508 -0.0018244626 -0.0018252104 -0.0018260012 -0.0018261685 -0.0018256039 -0.0018244898][-0.001827541 -0.0018278733 -0.001828589 -0.0018287436 -0.0018282288 -0.0018276792 -0.0018272472 -0.0018270643 -0.0018273268 -0.0018279779 -0.0018290136 -0.0018301018 -0.0018304911 -0.0018300313 -0.0018290943][-0.0018264877 -0.0018265905 -0.0018272391 -0.0018274011 -0.0018272084 -0.0018272316 -0.001827602 -0.001828269 -0.0018292392 -0.0018304075 -0.0018317173 -0.001833073 -0.0018337095 -0.0018333619 -0.0018326364][-0.0018257025 -0.0018253534 -0.0018257136 -0.0018257597 -0.0018257217 -0.0018260038 -0.0018269194 -0.0018283196 -0.001829849 -0.0018312838 -0.0018326831 -0.0018342045 -0.0018350602 -0.0018349269 -0.0018344929][-0.0018249638 -0.0018242408 -0.0018242971 -0.001824321 -0.0018244173 -0.0018249262 -0.0018260884 -0.0018280308 -0.001829977 -0.0018316411 -0.0018330041 -0.0018343828 -0.0018352264 -0.0018352317 -0.0018349005][-0.001824704 -0.0018239153 -0.0018239492 -0.0018240069 -0.0018242594 -0.0018249049 -0.0018262703 -0.001828398 -0.0018305014 -0.0018321626 -0.0018333405 -0.0018343518 -0.0018349083 -0.001834743 -0.0018343254][-0.0018244607 -0.0018236589 -0.001823823 -0.0018240539 -0.0018245539 -0.0018254218 -0.0018269138 -0.0018291121 -0.0018311874 -0.0018326939 -0.0018337708 -0.0018344767 -0.0018348004 -0.0018343608 -0.0018335918][-0.0018240813 -0.0018233686 -0.0018235722 -0.0018238977 -0.0018245083 -0.0018254311 -0.0018268336 -0.0018287023 -0.0018303844 -0.0018315552 -0.0018323381 -0.0018327048 -0.0018327183 -0.0018322015 -0.0018314118][-0.0018236843 -0.0018231141 -0.0018232815 -0.0018236042 -0.0018241461 -0.0018248577 -0.0018258865 -0.0018273478 -0.0018285494 -0.0018292589 -0.0018297929 -0.0018300015 -0.0018298886 -0.0018294459 -0.0018288898][-0.001823329 -0.0018228871 -0.0018230102 -0.0018232493 -0.001823548 -0.0018239395 -0.0018246134 -0.0018255885 -0.0018263236 -0.0018268306 -0.0018272485 -0.0018273422 -0.0018271951 -0.0018268626 -0.0018265108][-0.0018232062 -0.0018229984 -0.0018231148 -0.0018231905 -0.0018232332 -0.0018232467 -0.0018234296 -0.0018238804 -0.0018242721 -0.001824534 -0.0018247991 -0.0018248863 -0.0018247829 -0.0018246161 -0.0018244038][-0.001823133 -0.0018229713 -0.0018230035 -0.0018229577 -0.0018228269 -0.0018226443 -0.0018225879 -0.0018226425 -0.0018227838 -0.0018228675 -0.0018229492 -0.0018229934 -0.0018229024 -0.0018228535 -0.0018227927]]...]
INFO - root - 2017-12-09 16:04:54.063161: step 37510, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.882 sec/batch; 72h:16m:22s remains)
INFO - root - 2017-12-09 16:05:02.577773: step 37520, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.850 sec/batch; 69h:38m:31s remains)
INFO - root - 2017-12-09 16:05:11.225361: step 37530, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.850 sec/batch; 69h:39m:03s remains)
INFO - root - 2017-12-09 16:05:19.719134: step 37540, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 70h:48m:04s remains)
INFO - root - 2017-12-09 16:05:28.206116: step 37550, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 71h:01m:14s remains)
INFO - root - 2017-12-09 16:05:36.727039: step 37560, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.845 sec/batch; 69h:15m:17s remains)
INFO - root - 2017-12-09 16:05:45.415044: step 37570, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.873 sec/batch; 71h:31m:19s remains)
INFO - root - 2017-12-09 16:05:53.960861: step 37580, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.877 sec/batch; 71h:50m:33s remains)
INFO - root - 2017-12-09 16:06:02.595020: step 37590, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.866 sec/batch; 70h:55m:51s remains)
INFO - root - 2017-12-09 16:06:11.214632: step 37600, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.859 sec/batch; 70h:24m:10s remains)
2017-12-09 16:06:12.133702: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.00096274773 -0.0011110262 -0.0012762285 -0.0014244532 -0.0014992208 -0.0015068643 -0.0014639655 -0.0013833656 -0.0012686145 -0.0011453319 -0.0010556856 -0.0010383306 -0.001053045 -0.0011002462 -0.0011629339][-0.00097548688 -0.00112707 -0.001278834 -0.0014054012 -0.0014556707 -0.0014412785 -0.001379209 -0.0012891081 -0.0011767806 -0.0010672912 -0.00099465344 -0.00099751994 -0.0010354492 -0.0010827824 -0.0011387675][-0.0010481307 -0.0011828858 -0.0013058046 -0.001400809 -0.0014285771 -0.0013964045 -0.001315866 -0.0012143813 -0.0010982945 -0.00099686929 -0.00094335165 -0.000957403 -0.0010044188 -0.0010776322 -0.0011356305][-0.0011679134 -0.0012829038 -0.0013729944 -0.0014305492 -0.0014254603 -0.0013722737 -0.0012757587 -0.0011582655 -0.0010332862 -0.00093740254 -0.00089950836 -0.00092862261 -0.0009934227 -0.0010744524 -0.0011324199][-0.0013050272 -0.0014144641 -0.0014843193 -0.0015073889 -0.0014665233 -0.0013785905 -0.0012501277 -0.001113632 -0.00098469749 -0.00089380529 -0.00086837239 -0.000904254 -0.00096942892 -0.0010437963 -0.0011063288][-0.0014134313 -0.0015329188 -0.0016039186 -0.0016134732 -0.001551931 -0.0014288351 -0.0012584839 -0.0010866912 -0.00094069628 -0.00085694494 -0.00085039064 -0.0008958719 -0.00097408943 -0.0010428885 -0.001100796][-0.0014788343 -0.0016076709 -0.0016767574 -0.0016794137 -0.0016133417 -0.0014791586 -0.0012860692 -0.0010923005 -0.00092827511 -0.00084678421 -0.0008519562 -0.00090106169 -0.00097246288 -0.0010409113 -0.0010896197][-0.0015017209 -0.0016353503 -0.0016980551 -0.0016958589 -0.001631034 -0.0015015521 -0.0013113467 -0.0011164201 -0.00094537123 -0.00086327072 -0.00086324895 -0.00091817277 -0.00097868347 -0.0010498236 -0.0010974277][-0.0014949411 -0.0016397877 -0.0017031092 -0.001694558 -0.001632184 -0.0015135139 -0.0013408817 -0.0011460285 -0.00097712828 -0.0008889591 -0.00087798113 -0.00093749864 -0.0010019897 -0.0010658094 -0.001115452][-0.0014824669 -0.0016344378 -0.0017036182 -0.0017017513 -0.00164829 -0.0015434274 -0.0013884552 -0.0012014724 -0.0010291454 -0.00092759117 -0.00089728908 -0.00094080239 -0.0010029492 -0.001061481 -0.0011126518][-0.001475203 -0.0016324816 -0.0017061682 -0.0017076061 -0.0016626548 -0.0015752048 -0.0014407741 -0.0012681251 -0.0010902754 -0.00096900726 -0.00091356726 -0.00093770685 -0.00099094794 -0.0010443504 -0.001099908][-0.0014650569 -0.0016291416 -0.0017065731 -0.0017102099 -0.0016703061 -0.0015922235 -0.0014718524 -0.0013098905 -0.0011330486 -0.00099767814 -0.00092875538 -0.00094763644 -0.00099611562 -0.0010451297 -0.0011033078][-0.0014422028 -0.0016176107 -0.0017036322 -0.0017079437 -0.0016706219 -0.0015903783 -0.0014707786 -0.0013154128 -0.0011500022 -0.0010221936 -0.00095553353 -0.00096082932 -0.0010030635 -0.0010493896 -0.0011097809][-0.0014204737 -0.0016039817 -0.0016980713 -0.0017038326 -0.0016698671 -0.001593338 -0.001471785 -0.0013134712 -0.001157392 -0.001040248 -0.00097837183 -0.0009695667 -0.000994614 -0.0010465528 -0.0011071998][-0.0014220889 -0.0015975325 -0.0016849412 -0.0016982253 -0.0016701646 -0.0016022739 -0.0014756671 -0.0013181253 -0.0011701823 -0.0010550605 -0.00099612831 -0.00098439842 -0.0010080051 -0.0010585873 -0.0011094256]]...]
INFO - root - 2017-12-09 16:06:20.717367: step 37610, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.877 sec/batch; 71h:52m:29s remains)
INFO - root - 2017-12-09 16:06:29.173474: step 37620, loss = 0.83, batch loss = 0.70 (9.3 examples/sec; 0.863 sec/batch; 70h:41m:44s remains)
INFO - root - 2017-12-09 16:06:37.698485: step 37630, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 71h:07m:00s remains)
INFO - root - 2017-12-09 16:06:46.220746: step 37640, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.892 sec/batch; 73h:05m:39s remains)
INFO - root - 2017-12-09 16:06:54.866249: step 37650, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.879 sec/batch; 72h:01m:53s remains)
INFO - root - 2017-12-09 16:07:03.609656: step 37660, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.859 sec/batch; 70h:22m:06s remains)
INFO - root - 2017-12-09 16:07:12.287477: step 37670, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.870 sec/batch; 71h:15m:08s remains)
INFO - root - 2017-12-09 16:07:20.802406: step 37680, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.856 sec/batch; 70h:07m:43s remains)
INFO - root - 2017-12-09 16:07:29.449061: step 37690, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.857 sec/batch; 70h:11m:27s remains)
INFO - root - 2017-12-09 16:07:38.181048: step 37700, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.890 sec/batch; 72h:52m:18s remains)
2017-12-09 16:07:39.081059: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0018158315 -0.0018146182 -0.0018137104 -0.0018129364 -0.0018123208 -0.001811954 -0.0018116646 -0.0018115491 -0.0018117657 -0.0018122181 -0.0018128302 -0.0018136163 -0.0018144295 -0.001815322 -0.0018160576][-0.0018157144 -0.0018144936 -0.0018135447 -0.0018126916 -0.0018119577 -0.0018115054 -0.001811168 -0.0018110935 -0.001811487 -0.0018124317 -0.0018136983 -0.0018151283 -0.0018164857 -0.0018177003 -0.0018185911][-0.0018153855 -0.0018141625 -0.0018132217 -0.0018124282 -0.0018116382 -0.00181113 -0.0018107936 -0.0018109825 -0.001811882 -0.0018135498 -0.0018154961 -0.0018175166 -0.0018191629 -0.0018203207 -0.001820978][-0.0018148759 -0.0018136394 -0.0018127629 -0.0018121785 -0.0018114534 -0.0018108451 -0.0018104883 -0.0018111193 -0.0018126669 -0.0018149152 -0.0018171533 -0.0018191726 -0.0018204469 -0.0018207901 -0.0018206618][-0.0018143773 -0.0018132617 -0.0018124447 -0.0018119367 -0.0018111684 -0.0018104943 -0.001810157 -0.0018112017 -0.0018132953 -0.0018159995 -0.0018184659 -0.0018203127 -0.0018208139 -0.001819975 -0.0018188141][-0.0018140941 -0.0018130546 -0.0018123554 -0.0018119317 -0.00181113 -0.001810338 -0.0018099614 -0.0018112182 -0.0018136969 -0.001816886 -0.0018195622 -0.001821093 -0.0018206822 -0.0018183342 -0.0018159744][-0.0018137052 -0.001812702 -0.0018121556 -0.0018117133 -0.0018109372 -0.0018100964 -0.0018096413 -0.0018110988 -0.0018140833 -0.0018177696 -0.0018205735 -0.0018215964 -0.0018201161 -0.0018164677 -0.0018130118][-0.001812981 -0.001811894 -0.0018114628 -0.0018111548 -0.0018105431 -0.0018097572 -0.0018093502 -0.0018109403 -0.0018142486 -0.0018180921 -0.0018207987 -0.0018214994 -0.0018193664 -0.0018149825 -0.0018108814][-0.0018122874 -0.0018110764 -0.0018107297 -0.0018104713 -0.001809959 -0.0018093418 -0.0018092289 -0.0018108533 -0.0018138065 -0.0018173822 -0.0018201423 -0.0018207441 -0.0018186367 -0.0018144383 -0.0018101429][-0.0018115047 -0.0018102231 -0.0018098913 -0.0018096782 -0.0018093723 -0.0018090104 -0.0018090568 -0.0018104542 -0.0018128118 -0.0018157102 -0.0018184963 -0.0018199398 -0.0018190884 -0.0018161839 -0.0018127535][-0.0018108704 -0.0018094312 -0.0018090336 -0.0018089247 -0.0018087925 -0.0018086353 -0.0018087297 -0.0018097477 -0.0018115866 -0.0018140569 -0.0018167011 -0.0018189268 -0.0018197594 -0.001818765 -0.0018169244][-0.0018101728 -0.0018086334 -0.0018081851 -0.0018081574 -0.0018081493 -0.0018081169 -0.0018082095 -0.0018088586 -0.0018102125 -0.0018122233 -0.0018146926 -0.0018172866 -0.0018193516 -0.0018204234 -0.0018204248][-0.0018096995 -0.001808121 -0.0018075943 -0.0018076127 -0.0018076849 -0.0018077464 -0.0018078689 -0.0018082751 -0.0018091341 -0.0018105702 -0.001812553 -0.0018151596 -0.0018178755 -0.0018201729 -0.0018215303][-0.0018096109 -0.0018080203 -0.001807432 -0.0018074587 -0.0018075476 -0.0018076443 -0.0018077906 -0.0018080684 -0.0018085364 -0.0018094757 -0.001810986 -0.0018131697 -0.0018157945 -0.0018184521 -0.001820522][-0.0018099066 -0.0018082939 -0.0018075337 -0.0018075745 -0.0018076743 -0.0018077623 -0.0018078587 -0.0018080106 -0.0018082468 -0.0018088095 -0.0018097615 -0.0018114514 -0.0018137412 -0.0018161496 -0.0018183022]]...]
INFO - root - 2017-12-09 16:07:47.846033: step 37710, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.880 sec/batch; 72h:04m:03s remains)
INFO - root - 2017-12-09 16:07:56.501962: step 37720, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.879 sec/batch; 71h:58m:47s remains)
INFO - root - 2017-12-09 16:08:05.128213: step 37730, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.841 sec/batch; 68h:50m:31s remains)
INFO - root - 2017-12-09 16:08:13.658068: step 37740, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 70h:58m:34s remains)
INFO - root - 2017-12-09 16:08:22.335802: step 37750, loss = 0.82, batch loss = 0.69 (9.7 examples/sec; 0.828 sec/batch; 67h:46m:39s remains)
INFO - root - 2017-12-09 16:08:30.967632: step 37760, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.901 sec/batch; 73h:45m:21s remains)
INFO - root - 2017-12-09 16:08:39.706227: step 37770, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.876 sec/batch; 71h:40m:51s remains)
INFO - root - 2017-12-09 16:08:48.168624: step 37780, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.846 sec/batch; 69h:18m:00s remains)
INFO - root - 2017-12-09 16:08:56.864118: step 37790, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.897 sec/batch; 73h:25m:42s remains)
INFO - root - 2017-12-09 16:09:05.479653: step 37800, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.848 sec/batch; 69h:27m:01s remains)
2017-12-09 16:09:06.363394: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.1319768 0.13243635 0.13181177 0.12946701 0.12495179 0.11842772 0.11042558 0.10178047 0.093188234 0.08542338 0.078565665 0.07203234 0.065863423 0.060203616 0.055123936][0.14423783 0.14557385 0.14590548 0.14356598 0.13777824 0.1290635 0.11862591 0.10785925 0.097401589 0.088445321 0.080917574 0.074256636 0.067955293 0.062124003 0.056814566][0.15327059 0.15528843 0.15597098 0.15321118 0.14608333 0.13561144 0.1234973 0.11137407 0.099924758 0.090537973 0.082881704 0.076149672 0.069736488 0.063749656 0.058140732][0.1607248 0.16301678 0.16344358 0.15985942 0.15135039 0.13969338 0.12700102 0.11468201 0.103292 0.094013356 0.086446419 0.079631656 0.072807729 0.066309758 0.060062408][0.16723496 0.16928403 0.16855919 0.16373868 0.15424371 0.14214435 0.1297133 0.11798025 0.10745584 0.0989796 0.091828972 0.085013039 0.07763166 0.070315316 0.063012622][0.17238034 0.17354266 0.17112601 0.16491883 0.1548325 0.14302136 0.13154133 0.12093949 0.11176362 0.10430703 0.097788341 0.091030478 0.083205365 0.075121924 0.066594265][0.17562787 0.17477085 0.16980654 0.16153653 0.15071066 0.13963342 0.12962659 0.12088364 0.1136134 0.10790069 0.10264544 0.09654095 0.088640548 0.079849713 0.070154928][0.17693998 0.17335516 0.16499616 0.15423508 0.14249104 0.13208871 0.12357295 0.11675411 0.1116581 0.10792319 0.10453073 0.099563271 0.092097349 0.082951486 0.072521552][0.17593561 0.16971518 0.15825062 0.1453613 0.13287874 0.12315715 0.11572859 0.11062253 0.10755517 0.10588532 0.10430327 0.10056603 0.09365008 0.084501639 0.073793173][0.17147288 0.16360416 0.1505101 0.13669065 0.12439611 0.11525919 0.10884397 0.10524931 0.10371356 0.10357919 0.10306576 0.10003239 0.0934679 0.084302053 0.0735298][0.16506852 0.15682101 0.14374524 0.13074027 0.11967773 0.11233126 0.10767239 0.10520434 0.10427862 0.1042822 0.1034457 0.099884316 0.09265992 0.08306434 0.07217858][0.15775135 0.15072492 0.13923761 0.128069 0.11895092 0.11381703 0.11106034 0.10971437 0.10914138 0.10883683 0.10700708 0.10178535 0.092931271 0.082172714 0.0707942][0.15055805 0.14683612 0.13924621 0.13199997 0.12616535 0.12284289 0.12097147 0.11972263 0.11845716 0.11665409 0.11297677 0.10553345 0.094755188 0.082430184 0.070112705][0.14199002 0.14321598 0.14086841 0.13838235 0.13599481 0.13470152 0.13381568 0.13217261 0.12988447 0.12653045 0.12072806 0.11093124 0.097908258 0.084072478 0.070717946][0.13188611 0.13865332 0.14222507 0.14491627 0.14634736 0.14695707 0.14685431 0.14478232 0.14128003 0.1360044 0.1279676 0.11607384 0.10124502 0.08599288 0.071637534]]...]
INFO - root - 2017-12-09 16:09:15.000867: step 37810, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.874 sec/batch; 71h:31m:45s remains)
INFO - root - 2017-12-09 16:09:23.299613: step 37820, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.856 sec/batch; 70h:05m:13s remains)
INFO - root - 2017-12-09 16:09:32.014336: step 37830, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.857 sec/batch; 70h:10m:37s remains)
INFO - root - 2017-12-09 16:09:40.445704: step 37840, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.879 sec/batch; 71h:57m:26s remains)
INFO - root - 2017-12-09 16:09:49.157602: step 37850, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.857 sec/batch; 70h:10m:40s remains)
INFO - root - 2017-12-09 16:09:57.876147: step 37860, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.884 sec/batch; 72h:21m:51s remains)
INFO - root - 2017-12-09 16:10:06.608606: step 37870, loss = 0.82, batch loss = 0.69 (8.7 examples/sec; 0.916 sec/batch; 74h:58m:22s remains)
INFO - root - 2017-12-09 16:10:15.206017: step 37880, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.890 sec/batch; 72h:52m:03s remains)
INFO - root - 2017-12-09 16:10:23.954166: step 37890, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.881 sec/batch; 72h:04m:46s remains)
INFO - root - 2017-12-09 16:10:32.745029: step 37900, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.830 sec/batch; 67h:57m:20s remains)
2017-12-09 16:10:33.677430: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.0057074595 0.0056461534 0.0063030622 0.00782964 0.010122835 0.012853408 0.015711835 0.018125506 0.019885218 0.021245807 0.022769565 0.024940176 0.027854573 0.031279318 0.034734074][0.001699014 0.002029784 0.003390484 0.0059695723 0.0096058194 0.013694225 0.017415 0.019872082 0.020613234 0.02008393 0.019247914 0.018977484 0.019672487 0.021220963 0.023164431][-0.00029582588 0.00054857961 0.0027755164 0.00666685 0.011939649 0.017631777 0.022360113 0.024771821 0.024200272 0.021313917 0.017584611 0.014440334 0.012692739 0.012324247 0.012869011][-0.0003972305 0.0011493404 0.0044662 0.009896907 0.017027969 0.024514018 0.030349193 0.032690115 0.030694576 0.02529317 0.018493559 0.012347591 0.0081505673 0.0060564964 0.0055240826][0.0004115164 0.0027280543 0.0072700349 0.014406952 0.023503093 0.03286263 0.039896246 0.042299096 0.039112739 0.031520285 0.021969408 0.013068726 0.0065634339 0.0028395439 0.0013596116][0.0015956365 0.00454852 0.010124967 0.018711796 0.029476589 0.040430211 0.048528936 0.051158369 0.047290113 0.038289443 0.026860595 0.015938168 0.007598883 0.002462286 3.0731317e-05][0.0027051815 0.0061963242 0.012556266 0.022160538 0.034077682 0.046141654 0.055041429 0.058047444 0.054094709 0.044585276 0.032233898 0.020070663 0.010356452 0.0039267112 0.00045489322][0.0034727128 0.0073278607 0.01413693 0.024179481 0.036478132 0.048877586 0.058110323 0.06154304 0.058144223 0.049162723 0.036993913 0.024488032 0.01392872 0.0063528917 0.0017422925][0.0036949329 0.0076026204 0.014322761 0.02407068 0.035903227 0.04784951 0.056930076 0.060789488 0.058428969 0.050794914 0.039780278 0.027816365 0.017072402 0.0087357666 0.0031402027][0.0032497742 0.0068308837 0.012966719 0.021834323 0.032573905 0.043496169 0.05204865 0.056252435 0.055134751 0.049254734 0.039980441 0.029225187 0.01892888 0.010360283 0.0041693947][0.0023452635 0.0053078765 0.010415317 0.01785472 0.026976524 0.036448117 0.044164043 0.048483513 0.048464131 0.044338826 0.036987286 0.027816104 0.018569026 0.010484656 0.0043688463][0.0011934674 0.00351152 0.0075196992 0.01340882 0.020742554 0.028534992 0.0351234 0.039178841 0.039818708 0.037053853 0.031417608 0.023947814 0.016146945 0.0091415169 0.0037453631][0.00012155634 0.0017669232 0.0046629468 0.0089923246 0.014472953 0.020427134 0.025618823 0.029031638 0.02990094 0.028113523 0.023988858 0.018287256 0.012238956 0.0067604282 0.0025409767][-0.00071110378 0.00032471714 0.002205913 0.0051227347 0.0089118015 0.013124383 0.016868858 0.019393602 0.020103697 0.018885806 0.015986636 0.011956323 0.0077300626 0.0039647934 0.0011324441][-0.0013237686 -0.0007635142 0.00032911927 0.0021026321 0.0044732592 0.0071698292 0.0095922 0.011210771 0.011602458 0.01071022 0.0087591754 0.0061672362 0.0035724035 0.001370028 -0.00015306997]]...]
INFO - root - 2017-12-09 16:10:42.520040: step 37910, loss = 0.82, batch loss = 0.69 (8.7 examples/sec; 0.920 sec/batch; 75h:15m:17s remains)
INFO - root - 2017-12-09 16:10:51.185665: step 37920, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.859 sec/batch; 70h:18m:39s remains)
INFO - root - 2017-12-09 16:10:59.990577: step 37930, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 70h:31m:20s remains)
INFO - root - 2017-12-09 16:11:08.545919: step 37940, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.879 sec/batch; 71h:57m:22s remains)
INFO - root - 2017-12-09 16:11:17.205116: step 37950, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 70h:30m:54s remains)
INFO - root - 2017-12-09 16:11:25.772075: step 37960, loss = 0.82, batch loss = 0.69 (9.8 examples/sec; 0.819 sec/batch; 67h:00m:30s remains)
INFO - root - 2017-12-09 16:11:34.112977: step 37970, loss = 0.82, batch loss = 0.69 (11.0 examples/sec; 0.728 sec/batch; 59h:35m:14s remains)
INFO - root - 2017-12-09 16:11:42.709249: step 37980, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 71h:02m:11s remains)
INFO - root - 2017-12-09 16:11:51.335195: step 37990, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.853 sec/batch; 69h:44m:55s remains)
INFO - root - 2017-12-09 16:11:59.917552: step 38000, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.882 sec/batch; 72h:07m:05s remains)
2017-12-09 16:12:00.780747: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.003200396 0.0044864044 0.0056475247 0.0064584538 0.0067420062 0.0064905128 0.00581753 0.0048926692 0.0039109834 0.0029768655 0.0021844571 0.0015907567 0.0011781319 0.00095146406 0.00084914523][0.0063424287 0.0083746351 0.010314533 0.011743852 0.012498498 0.012469545 0.011739809 0.010434925 0.0087953256 0.0070994049 0.0055602165 0.0042794463 0.003344269 0.0027218959 0.0023764796][0.010822265 0.013910758 0.016849555 0.019056868 0.020353742 0.020528577 0.019670021 0.017905932 0.01552323 0.013015797 0.010714693 0.0088861566 0.007532604 0.0065820385 0.0059804311][0.015403184 0.019571308 0.0236747 0.027114157 0.0296624 0.030876245 0.030574236 0.02877333 0.025783792 0.022265993 0.01880509 0.01593237 0.01377665 0.012283661 0.011240725][0.018702287 0.0237936 0.028966095 0.033702169 0.037760109 0.040492013 0.04144695 0.040481273 0.037780095 0.03406997 0.030089051 0.026490312 0.023465708 0.02099327 0.018875014][0.019113168 0.024792988 0.030827321 0.0366986 0.042049903 0.046237089 0.048668526 0.049007993 0.04734347 0.044395957 0.040844496 0.037209995 0.033753239 0.030438345 0.02712246][0.01688597 0.022741925 0.029354207 0.036146104 0.042524822 0.047702227 0.051130537 0.0524729 0.051848304 0.049896225 0.047188867 0.04401185 0.040568452 0.036755104 0.032553621][0.013063504 0.01853101 0.025021594 0.032130525 0.038971733 0.044688642 0.048715465 0.050837945 0.051244918 0.050324216 0.048474815 0.04580161 0.042539991 0.038613111 0.034175329][0.0090194065 0.013693778 0.019537415 0.026211251 0.03274899 0.038325705 0.042392492 0.044919424 0.046164449 0.046187926 0.045142546 0.043018494 0.040145352 0.0365538 0.032459684][0.0052632568 0.0088925324 0.013700571 0.019366719 0.024969641 0.029751629 0.033334922 0.035828475 0.037512247 0.038283683 0.038019292 0.036599968 0.034443222 0.031639691 0.028372694][0.002301469 0.0047290218 0.0081625823 0.012365974 0.016575426 0.020135757 0.022892904 0.025027268 0.026764765 0.027879115 0.028120447 0.027398676 0.026099712 0.024329755 0.022187017][0.00016599311 0.0015423326 0.00361339 0.0062204991 0.008846961 0.011099407 0.012945377 0.014554764 0.016044751 0.017148599 0.017595444 0.017332446 0.016737062 0.015887782 0.014810431][-0.0012307739 -0.00064950052 0.00031000702 0.0015403491 0.0028126743 0.0039444985 0.0049845325 0.0060417941 0.0071309507 0.0079914583 0.0084019294 0.00839664 0.0082327649 0.0080230683 0.0077372314][-0.0017569431 -0.0016228457 -0.0013587535 -0.00097381347 -0.00052505347 -9.3380339e-05 0.00037268875 0.00092143286 0.0015155892 0.0019687477 0.0021820329 0.0022217846 0.0022417284 0.0023031286 0.0023493371][-0.0018200222 -0.0018151023 -0.0017971225 -0.0017652906 -0.0017115116 -0.0016342669 -0.0014989071 -0.0012982911 -0.0010657337 -0.00087429478 -0.00076086225 -0.000702912 -0.00062241813 -0.00051171228 -0.00040316582]]...]
INFO - root - 2017-12-09 16:12:09.420212: step 38010, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.880 sec/batch; 71h:58m:05s remains)
INFO - root - 2017-12-09 16:12:17.967220: step 38020, loss = 0.82, batch loss = 0.69 (10.6 examples/sec; 0.752 sec/batch; 61h:28m:39s remains)
INFO - root - 2017-12-09 16:12:26.614073: step 38030, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.853 sec/batch; 69h:47m:59s remains)
INFO - root - 2017-12-09 16:12:35.158409: step 38040, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.840 sec/batch; 68h:40m:15s remains)
INFO - root - 2017-12-09 16:12:43.881632: step 38050, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 70h:54m:16s remains)
INFO - root - 2017-12-09 16:12:52.374459: step 38060, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.881 sec/batch; 72h:04m:09s remains)
INFO - root - 2017-12-09 16:13:01.182944: step 38070, loss = 0.82, batch loss = 0.69 (8.7 examples/sec; 0.923 sec/batch; 75h:28m:36s remains)
INFO - root - 2017-12-09 16:13:09.895581: step 38080, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.892 sec/batch; 72h:56m:32s remains)
INFO - root - 2017-12-09 16:13:18.589106: step 38090, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.889 sec/batch; 72h:43m:21s remains)
INFO - root - 2017-12-09 16:13:27.217085: step 38100, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 71h:02m:19s remains)
2017-12-09 16:13:28.086854: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0017597276 -0.0016264387 -0.0013447508 -0.00089759758 -0.00037316151 2.6813243e-05 0.00012793345 -0.00010178995 -0.00054784981 -0.0010138929 -0.0013962813 -0.0016472517 -0.0017590612 -0.0018000156 -0.0018077467][-0.0017147715 -0.0014795401 -0.0009795411 -0.00017810927 0.000778337 0.0015493268 0.0017966169 0.0014201649 0.00060906075 -0.00025661907 -0.00095835613 -0.0014144612 -0.0016668125 -0.0017766833 -0.001805308][-0.0016485532 -0.0012825531 -0.00051574269 0.00071385945 0.0022126962 0.0035261321 0.0041505583 0.0038421559 0.002723556 0.0012963009 -1.1238502e-05 -0.00093943946 -0.0014826934 -0.0017244634 -0.0017992771][-0.0015369629 -0.00095954241 0.00021544076 0.0020579412 0.0042809425 0.0062600495 0.0072706584 0.0069298567 0.00537598 0.0032791807 0.0012627505 -0.00024542992 -0.0011873754 -0.001640243 -0.0017895586][-0.001417431 -0.00063087023 0.00094486191 0.0034163985 0.0064592948 0.0093282834 0.011057741 0.010953209 0.00895211 0.0059167268 0.0028755595 0.00057461252 -0.00085616764 -0.0015409032 -0.0017779534][-0.0013128477 -0.00039307144 0.001445692 0.0043536657 0.0080228085 0.011655694 0.014078732 0.014317648 0.012110007 0.0083685517 0.004444229 0.0013926839 -0.00051850663 -0.0014399061 -0.0017637376][-0.0012279097 -0.00029179861 0.0015753331 0.0045659752 0.0084447181 0.012448006 0.015291938 0.015786923 0.013503404 0.0094017619 0.0050703473 0.0017132359 -0.00036998419 -0.0013895654 -0.0017552879][-0.0011762292 -0.00031778193 0.0013870494 0.00415845 0.0078352243 0.011738829 0.014599473 0.01511957 0.012817184 0.0086822454 0.0044356231 0.0012858652 -0.00056610932 -0.0014419472 -0.0017553951][-0.0011854074 -0.00048186176 0.000930747 0.003270593 0.0064302459 0.0098201865 0.01227989 0.012608984 0.010393326 0.0066142082 0.0029131686 0.000347001 -0.001011737 -0.0015847103 -0.0017718475][-0.0012477171 -0.00075617922 0.00027465099 0.0020465786 0.0044933478 0.0071319118 0.0089934785 0.0091026658 0.0071900729 0.004116015 0.0012609283 -0.00056350103 -0.0014084146 -0.0017064182 -0.0017891593][-0.0013858229 -0.0010941175 -0.00046793709 0.00066985795 0.0023078867 0.0041055763 0.0053555351 0.0053532217 0.003943 0.0017811165 -0.00012467301 -0.0012392807 -0.0016669804 -0.0017734448 -0.0017944386][-0.0016012376 -0.0014554489 -0.0011565185 -0.000593066 0.00026100711 0.0012346855 0.0019322443 0.00192776 0.0011375961 -5.2480842e-05 -0.0010551744 -0.0015917845 -0.0017620724 -0.0017919658 -0.0017973736][-0.0017767739 -0.0017222908 -0.0016076233 -0.0013944566 -0.0010699157 -0.00069174729 -0.00040941255 -0.00040737784 -0.0007206084 -0.0011912717 -0.001571561 -0.0017537434 -0.0017954619 -0.0017979934 -0.0017992228][-0.0018094403 -0.0018043201 -0.0017913238 -0.0017609881 -0.0017070776 -0.0016372986 -0.0015794127 -0.0015684685 -0.0016196137 -0.0017057701 -0.0017726716 -0.0018009678 -0.0018041595 -0.0018035802 -0.0018026852][-0.0018094531 -0.0018081161 -0.0018066263 -0.0018045856 -0.0018027689 -0.0018005398 -0.001798613 -0.001798255 -0.0018009505 -0.0018038157 -0.0018052777 -0.0018052546 -0.0018048856 -0.0018046194 -0.0018044978]]...]
INFO - root - 2017-12-09 16:13:36.693997: step 38110, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.876 sec/batch; 71h:37m:04s remains)
INFO - root - 2017-12-09 16:13:45.308974: step 38120, loss = 0.82, batch loss = 0.70 (9.2 examples/sec; 0.868 sec/batch; 70h:58m:28s remains)
INFO - root - 2017-12-09 16:13:53.818887: step 38130, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 71h:13m:19s remains)
INFO - root - 2017-12-09 16:14:02.277851: step 38140, loss = 0.81, batch loss = 0.68 (9.2 examples/sec; 0.868 sec/batch; 70h:57m:30s remains)
INFO - root - 2017-12-09 16:14:10.774720: step 38150, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.859 sec/batch; 70h:16m:17s remains)
INFO - root - 2017-12-09 16:14:19.337551: step 38160, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.877 sec/batch; 71h:44m:30s remains)
INFO - root - 2017-12-09 16:14:27.868765: step 38170, loss = 0.82, batch loss = 0.69 (10.6 examples/sec; 0.753 sec/batch; 61h:34m:07s remains)
INFO - root - 2017-12-09 16:14:36.517910: step 38180, loss = 0.82, batch loss = 0.69 (9.8 examples/sec; 0.818 sec/batch; 66h:54m:54s remains)
INFO - root - 2017-12-09 16:14:45.477625: step 38190, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.896 sec/batch; 73h:12m:41s remains)
INFO - root - 2017-12-09 16:14:54.046427: step 38200, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.853 sec/batch; 69h:43m:55s remains)
2017-12-09 16:14:54.929674: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0018185027 -0.0018188085 -0.0018203272 -0.0018219762 -0.00182354 -0.001824847 -0.0018259134 -0.0018267484 -0.0018273375 -0.0018277757 -0.0018277705 -0.0018271462 -0.0018257647 -0.0018238117 -0.0018214034][-0.0018190471 -0.0018198215 -0.0018219338 -0.0018242523 -0.0018265137 -0.0018284124 -0.0018298806 -0.0018308602 -0.0018313748 -0.0018317421 -0.0018317011 -0.0018310073 -0.0018293967 -0.0018269963 -0.0018241159][-0.0018196623 -0.0018207544 -0.0018232393 -0.0018260792 -0.0018289499 -0.001831394 -0.0018331918 -0.0018342653 -0.0018347123 -0.0018348983 -0.0018347214 -0.0018339616 -0.0018322534 -0.0018296815 -0.0018265306][-0.0018196111 -0.0018204049 -0.0018227287 -0.0018256176 -0.0018287208 -0.0018315103 -0.0018336978 -0.001835105 -0.0018357442 -0.0018359912 -0.0018357863 -0.0018350438 -0.0018333627 -0.0018308538 -0.0018277534][-0.0018187704 -0.0018187929 -0.0018204226 -0.0018228075 -0.001825656 -0.0018284981 -0.0018309529 -0.0018327255 -0.0018337476 -0.0018343843 -0.0018344726 -0.0018339517 -0.0018326844 -0.0018305543 -0.0018277249][-0.0018173902 -0.0018164433 -0.0018171041 -0.001818639 -0.0018208253 -0.0018233339 -0.0018257863 -0.0018277511 -0.0018290868 -0.0018302267 -0.0018307825 -0.0018307617 -0.0018301971 -0.0018288716 -0.001826773][-0.0018158241 -0.0018140833 -0.0018138209 -0.0018144391 -0.0018158529 -0.0018178396 -0.0018199702 -0.0018218146 -0.0018232152 -0.0018245637 -0.0018255722 -0.0018260365 -0.0018260564 -0.0018255998 -0.0018244851][-0.0018145798 -0.0018124258 -0.0018115889 -0.0018114744 -0.0018121068 -0.0018134664 -0.0018151535 -0.001816647 -0.0018178364 -0.0018190539 -0.0018201947 -0.0018209561 -0.001821315 -0.0018214887 -0.0018213647][-0.0018139004 -0.0018116522 -0.0018106791 -0.0018102349 -0.0018104138 -0.0018112591 -0.0018124605 -0.0018136433 -0.0018147101 -0.0018156657 -0.0018165505 -0.0018172779 -0.0018177427 -0.0018181428 -0.0018184459][-0.0018135214 -0.0018116009 -0.0018108326 -0.0018104183 -0.0018104266 -0.0018109134 -0.0018116519 -0.0018124963 -0.0018133766 -0.0018141572 -0.0018148407 -0.0018154199 -0.0018158447 -0.0018162163 -0.0018165109][-0.001813545 -0.0018119799 -0.0018114763 -0.0018112347 -0.0018112052 -0.0018114514 -0.0018118365 -0.0018123528 -0.0018129707 -0.0018135536 -0.0018140734 -0.0018144584 -0.0018146947 -0.0018149553 -0.0018151901][-0.0018136235 -0.0018123079 -0.0018119828 -0.0018118812 -0.0018118729 -0.001812006 -0.0018122175 -0.0018125395 -0.0018129653 -0.0018134031 -0.0018137784 -0.0018139674 -0.0018140118 -0.0018141014 -0.001814166][-0.0018137444 -0.0018124981 -0.0018122084 -0.0018121861 -0.0018121984 -0.0018122691 -0.001812394 -0.0018125831 -0.0018128311 -0.0018131112 -0.0018133874 -0.0018135572 -0.0018135743 -0.001813562 -0.001813508][-0.0018137373 -0.0018125111 -0.001812237 -0.0018122443 -0.0018122562 -0.0018122877 -0.0018123548 -0.0018124648 -0.001812601 -0.0018127628 -0.0018129619 -0.001813131 -0.0018131927 -0.0018131696 -0.0018130663][-0.0018137044 -0.0018124731 -0.0018121835 -0.0018122008 -0.0018122144 -0.0018122344 -0.001812268 -0.0018123196 -0.0018123842 -0.00181247 -0.0018125894 -0.0018127118 -0.0018127982 -0.0018128351 -0.0018127848]]...]
INFO - root - 2017-12-09 16:15:03.702165: step 38210, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.861 sec/batch; 70h:20m:46s remains)
INFO - root - 2017-12-09 16:15:12.528651: step 38220, loss = 0.82, batch loss = 0.69 (8.8 examples/sec; 0.908 sec/batch; 74h:12m:45s remains)
INFO - root - 2017-12-09 16:15:20.992077: step 38230, loss = 0.81, batch loss = 0.68 (9.3 examples/sec; 0.858 sec/batch; 70h:05m:45s remains)
INFO - root - 2017-12-09 16:15:29.411357: step 38240, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.854 sec/batch; 69h:48m:35s remains)
INFO - root - 2017-12-09 16:15:38.013364: step 38250, loss = 0.82, batch loss = 0.69 (9.9 examples/sec; 0.807 sec/batch; 65h:55m:27s remains)
INFO - root - 2017-12-09 16:15:46.580952: step 38260, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.859 sec/batch; 70h:10m:38s remains)
INFO - root - 2017-12-09 16:15:55.287626: step 38270, loss = 0.83, batch loss = 0.70 (10.3 examples/sec; 0.775 sec/batch; 63h:21m:06s remains)
INFO - root - 2017-12-09 16:16:04.086948: step 38280, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.836 sec/batch; 68h:20m:41s remains)
INFO - root - 2017-12-09 16:16:12.783783: step 38290, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.878 sec/batch; 71h:47m:08s remains)
INFO - root - 2017-12-09 16:16:21.336375: step 38300, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.840 sec/batch; 68h:40m:48s remains)
2017-12-09 16:16:22.437390: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.062112447 0.063607574 0.068350837 0.0746497 0.083256833 0.092809759 0.1023679 0.11042562 0.1167073 0.11940376 0.11698441 0.11183752 0.10502571 0.09596958 0.083570063][0.068622977 0.069052383 0.073277168 0.079653725 0.087514728 0.095564261 0.10403066 0.11125677 0.11693763 0.11930117 0.11748977 0.11368681 0.108216 0.10084485 0.090187632][0.080129251 0.080337413 0.084131293 0.090164378 0.097286932 0.10372609 0.11055369 0.11692595 0.12211639 0.12448008 0.12314029 0.12043373 0.1159405 0.10915373 0.099484205][0.097945094 0.098868221 0.10359596 0.11090236 0.11921129 0.12608302 0.13183326 0.13663644 0.14022252 0.14156617 0.13934612 0.13541675 0.12956874 0.12176745 0.11148016][0.12541094 0.12748165 0.13283944 0.14204794 0.15229003 0.16018471 0.1663015 0.17047428 0.17242865 0.17060906 0.16511641 0.15812588 0.14910936 0.13817106 0.12544714][0.16277838 0.16649581 0.17272337 0.18301727 0.19431514 0.2031285 0.21032034 0.21408287 0.21416967 0.20912544 0.19959877 0.18756588 0.17285031 0.1567186 0.13961028][0.2092876 0.21572408 0.22233623 0.23200253 0.2423313 0.2503565 0.25673532 0.25922507 0.257174 0.24940571 0.23675786 0.22061001 0.20030764 0.17837219 0.15571345][0.25514674 0.26329231 0.26849407 0.27614522 0.2849611 0.29122731 0.29598215 0.29673746 0.29290855 0.28293064 0.26762885 0.24742049 0.2222161 0.19501013 0.16740619][0.30094832 0.30835187 0.31041688 0.31465921 0.31968626 0.32276464 0.32491776 0.3237344 0.31897187 0.30821803 0.29205713 0.27004376 0.24219377 0.21142904 0.17972195][0.33843586 0.34593657 0.34559298 0.34580579 0.34649724 0.3461622 0.34555024 0.34271559 0.337326 0.3268109 0.31111404 0.28915456 0.26043698 0.22788009 0.19386224][0.361942 0.37033451 0.36982453 0.36841607 0.36681518 0.36386415 0.36037949 0.35582152 0.34998775 0.3402006 0.32550541 0.30442345 0.2765145 0.24394615 0.20861869][0.37322217 0.38073739 0.37772909 0.37471908 0.37179795 0.36756137 0.36306223 0.35880637 0.3541894 0.34587008 0.33297205 0.31440619 0.28914934 0.25863147 0.22461194][0.37872824 0.38521433 0.3803516 0.37485322 0.36959773 0.36305746 0.35690826 0.35236529 0.34853396 0.34263632 0.33325586 0.31903556 0.29778695 0.27091831 0.23984647][0.38001716 0.38539714 0.37929332 0.37192696 0.36428848 0.35495436 0.34584069 0.33919802 0.33474159 0.33003384 0.32284871 0.3126989 0.29673538 0.27558631 0.24956134][0.36998996 0.3761279 0.37097812 0.36416331 0.35596433 0.34572902 0.33551818 0.32749322 0.32143608 0.31621927 0.31012124 0.3024883 0.29049894 0.27506083 0.2551488]]...]
INFO - root - 2017-12-09 16:16:30.943529: step 38310, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.841 sec/batch; 68h:42m:55s remains)
INFO - root - 2017-12-09 16:16:39.514694: step 38320, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.876 sec/batch; 71h:36m:51s remains)
INFO - root - 2017-12-09 16:16:48.045844: step 38330, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.860 sec/batch; 70h:15m:57s remains)
INFO - root - 2017-12-09 16:16:56.644010: step 38340, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.843 sec/batch; 68h:54m:50s remains)
INFO - root - 2017-12-09 16:17:05.309063: step 38350, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 71h:10m:45s remains)
INFO - root - 2017-12-09 16:17:13.966739: step 38360, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.840 sec/batch; 68h:38m:47s remains)
INFO - root - 2017-12-09 16:17:22.462844: step 38370, loss = 0.82, batch loss = 0.69 (10.6 examples/sec; 0.755 sec/batch; 61h:40m:28s remains)
INFO - root - 2017-12-09 16:17:30.992486: step 38380, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.852 sec/batch; 69h:34m:52s remains)
INFO - root - 2017-12-09 16:17:39.672519: step 38390, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.854 sec/batch; 69h:46m:29s remains)
INFO - root - 2017-12-09 16:17:48.484600: step 38400, loss = 0.82, batch loss = 0.69 (8.8 examples/sec; 0.911 sec/batch; 74h:24m:39s remains)
2017-12-09 16:17:49.329141: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.43088022 0.41732976 0.40663511 0.39738908 0.39098394 0.38734928 0.38612509 0.38706052 0.38776895 0.38789055 0.38720295 0.38444027 0.38070682 0.37490946 0.36712596][0.41362587 0.40278327 0.39365891 0.38613808 0.380665 0.37663296 0.37519035 0.37604386 0.37701672 0.37846696 0.38037497 0.38050044 0.37989601 0.37829968 0.37502459][0.38875663 0.38221622 0.37658015 0.37156075 0.36776829 0.36390981 0.3615042 0.36136898 0.36228189 0.36386493 0.36650211 0.3681381 0.36980242 0.37117189 0.37115741][0.36650297 0.36485153 0.36380476 0.36268207 0.36188585 0.35870945 0.35577065 0.35412681 0.35375661 0.3543433 0.3561427 0.35793704 0.36092415 0.36404252 0.36633348][0.34477702 0.34752178 0.35100776 0.35503286 0.35856265 0.35781434 0.35568258 0.35301262 0.35057905 0.34918362 0.34885859 0.34935445 0.35192287 0.3554025 0.3594844][0.32813185 0.33390686 0.34020969 0.34807748 0.35519662 0.35782748 0.3575308 0.3548882 0.35118347 0.34730563 0.34432968 0.34191227 0.34241512 0.34479555 0.34937328][0.31540588 0.32424751 0.33277032 0.34376112 0.35378659 0.3588841 0.35979363 0.35737029 0.3532061 0.34717607 0.34114683 0.33584598 0.33370876 0.3339833 0.33755305][0.30315411 0.31503621 0.32614106 0.33901736 0.35085341 0.35741538 0.35941434 0.35747567 0.35276103 0.34570408 0.33712655 0.32911503 0.32394651 0.32082856 0.32203078][0.29029447 0.30452463 0.31714475 0.33159548 0.34521842 0.3535096 0.35627183 0.35466477 0.34992543 0.34208578 0.33181497 0.32166845 0.31368598 0.30747285 0.30576307][0.27541834 0.29047123 0.30339983 0.31802681 0.33198389 0.34156546 0.34600815 0.3453891 0.34115824 0.33353236 0.32280293 0.3109636 0.30047497 0.2921398 0.28820205][0.26002365 0.27579197 0.28799316 0.30112571 0.31383577 0.32291695 0.32734275 0.32733536 0.32401571 0.31758413 0.30796218 0.29641467 0.28552842 0.27646992 0.2713035][0.24616727 0.26194206 0.27306113 0.28471634 0.29574245 0.30296826 0.30654708 0.30629265 0.30349061 0.29794541 0.28947207 0.27979162 0.27007112 0.26146907 0.255981][0.23382585 0.24867974 0.25804549 0.26732162 0.2762875 0.28196406 0.2846818 0.28400367 0.28167325 0.27713156 0.27011582 0.26207632 0.25380135 0.24663475 0.24186097][0.2254416 0.23844594 0.24569157 0.25347257 0.26071563 0.26536807 0.2678085 0.26701668 0.26502621 0.26089317 0.25518471 0.24875607 0.24211395 0.23672901 0.23301116][0.22117789 0.23239505 0.23727168 0.24311094 0.24829598 0.251846 0.25388041 0.25364152 0.25218153 0.24914558 0.24497645 0.2400237 0.23494925 0.23102205 0.22861326]]...]
INFO - root - 2017-12-09 16:17:58.072273: step 38410, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.889 sec/batch; 72h:36m:57s remains)
INFO - root - 2017-12-09 16:18:06.765894: step 38420, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.881 sec/batch; 71h:57m:09s remains)
INFO - root - 2017-12-09 16:18:15.326246: step 38430, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.891 sec/batch; 72h:46m:12s remains)
INFO - root - 2017-12-09 16:18:23.858987: step 38440, loss = 0.82, batch loss = 0.69 (10.6 examples/sec; 0.752 sec/batch; 61h:24m:58s remains)
INFO - root - 2017-12-09 16:18:32.361109: step 38450, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 70h:48m:17s remains)
INFO - root - 2017-12-09 16:18:40.899839: step 38460, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.850 sec/batch; 69h:25m:17s remains)
INFO - root - 2017-12-09 16:18:49.405433: step 38470, loss = 0.82, batch loss = 0.69 (10.6 examples/sec; 0.756 sec/batch; 61h:46m:50s remains)
INFO - root - 2017-12-09 16:18:57.863880: step 38480, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.846 sec/batch; 69h:05m:05s remains)
INFO - root - 2017-12-09 16:19:06.676274: step 38490, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.886 sec/batch; 72h:21m:44s remains)
INFO - root - 2017-12-09 16:19:15.359998: step 38500, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.852 sec/batch; 69h:35m:03s remains)
2017-12-09 16:19:16.285056: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0018110722 -0.001810263 -0.0018099746 -0.0018097115 -0.0018094621 -0.0018093097 -0.0018092257 -0.0018091789 -0.0018091318 -0.0018091135 -0.0018091508 -0.0018092454 -0.0018093324 -0.001809396 -0.0018094261][-0.0018105984 -0.0018096665 -0.0018092716 -0.0018089439 -0.0018087025 -0.001808589 -0.0018085552 -0.0018084944 -0.0018083767 -0.0018082784 -0.0018083138 -0.0018084727 -0.0018086644 -0.0018088376 -0.001808943][-0.0018102498 -0.0018090312 -0.0018084591 -0.0018080816 -0.0018078906 -0.0018078947 -0.0018079481 -0.0018079189 -0.001807774 -0.0018076766 -0.0018077644 -0.001808032 -0.0018083784 -0.0018087022 -0.0018089201][-0.0018099395 -0.0018083502 -0.0018074627 -0.0018069434 -0.0018067488 -0.0018068365 -0.0018069486 -0.0018069715 -0.0018068589 -0.0018068374 -0.0018070291 -0.0018074132 -0.0018079136 -0.0018084046 -0.0018087744][-0.0018095634 -0.0018076034 -0.0018063672 -0.001805634 -0.0018053533 -0.0018054813 -0.0018056496 -0.001805693 -0.001805667 -0.0018057702 -0.0018061151 -0.0018066528 -0.0018073282 -0.0018080118 -0.0018085436][-0.00180921 -0.0018070242 -0.0018055463 -0.0018045645 -0.001804082 -0.0018041034 -0.0018041995 -0.0018042062 -0.0018042772 -0.0018045716 -0.0018051497 -0.0018059507 -0.0018068748 -0.0018077522 -0.0018084135][-0.0018091076 -0.0018067611 -0.0018050931 -0.0018038773 -0.0018031945 -0.0018030558 -0.001803 -0.0018029843 -0.0018031771 -0.0018036704 -0.0018044472 -0.0018054509 -0.0018065803 -0.0018076027 -0.0018083145][-0.0018092378 -0.001806861 -0.0018051574 -0.0018038354 -0.0018029966 -0.0018027553 -0.0018026718 -0.0018027022 -0.0018029894 -0.0018036247 -0.0018044731 -0.0018054607 -0.0018065642 -0.0018075816 -0.0018082726][-0.001809355 -0.001807012 -0.0018053673 -0.0018040813 -0.0018033113 -0.0018031909 -0.0018032821 -0.0018034249 -0.0018037815 -0.0018044154 -0.0018051083 -0.0018058363 -0.0018067507 -0.0018076864 -0.001808329][-0.0018092894 -0.0018070416 -0.001805521 -0.0018043906 -0.0018038499 -0.0018039824 -0.0018043269 -0.0018045807 -0.0018049573 -0.0018055476 -0.0018060638 -0.0018065267 -0.0018072147 -0.0018080153 -0.0018085765][-0.0018092679 -0.0018071367 -0.0018057474 -0.0018048163 -0.0018045044 -0.0018047707 -0.001805224 -0.0018055285 -0.0018058369 -0.0018063858 -0.0018068637 -0.001807222 -0.0018077558 -0.0018084085 -0.0018088508][-0.0018093012 -0.0018074093 -0.0018061901 -0.001805417 -0.0018052353 -0.0018055188 -0.0018059785 -0.0018062864 -0.0018065588 -0.0018070636 -0.0018074809 -0.001807768 -0.0018081652 -0.0018086554 -0.0018089834][-0.0018094664 -0.001808016 -0.0018070327 -0.0018063978 -0.0018062526 -0.0018064913 -0.0018068897 -0.001807131 -0.0018073302 -0.0018077139 -0.0018080011 -0.0018081818 -0.0018084487 -0.0018087869 -0.0018090244][-0.001809647 -0.0018085787 -0.001807801 -0.0018073095 -0.0018071808 -0.0018073091 -0.0018075734 -0.0018077077 -0.0018078242 -0.0018081 -0.001808299 -0.0018084255 -0.0018086273 -0.0018088678 -0.0018090219][-0.0018098496 -0.0018089833 -0.0018083759 -0.0018080455 -0.0018079312 -0.0018079312 -0.0018080473 -0.0018081116 -0.0018081869 -0.0018083965 -0.001808545 -0.0018086451 -0.0018087833 -0.0018089343 -0.0018090198]]...]
INFO - root - 2017-12-09 16:19:25.040136: step 38510, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.870 sec/batch; 71h:02m:34s remains)
INFO - root - 2017-12-09 16:19:33.627730: step 38520, loss = 0.83, batch loss = 0.70 (9.3 examples/sec; 0.862 sec/batch; 70h:24m:16s remains)
INFO - root - 2017-12-09 16:19:42.153299: step 38530, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.852 sec/batch; 69h:36m:45s remains)
INFO - root - 2017-12-09 16:19:50.692382: step 38540, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.856 sec/batch; 69h:53m:12s remains)
INFO - root - 2017-12-09 16:19:59.315282: step 38550, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.890 sec/batch; 72h:40m:02s remains)
INFO - root - 2017-12-09 16:20:08.020412: step 38560, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.854 sec/batch; 69h:42m:49s remains)
INFO - root - 2017-12-09 16:20:16.645291: step 38570, loss = 0.82, batch loss = 0.69 (10.5 examples/sec; 0.763 sec/batch; 62h:18m:08s remains)
INFO - root - 2017-12-09 16:20:25.392280: step 38580, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.855 sec/batch; 69h:46m:11s remains)
INFO - root - 2017-12-09 16:20:34.219517: step 38590, loss = 0.81, batch loss = 0.68 (9.3 examples/sec; 0.862 sec/batch; 70h:22m:52s remains)
INFO - root - 2017-12-09 16:20:42.939166: step 38600, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.874 sec/batch; 71h:20m:57s remains)
2017-12-09 16:20:43.793637: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.056336708 0.061227176 0.064751938 0.065319337 0.061918523 0.053321265 0.041699108 0.029192299 0.019101774 0.012107174 0.008249728 0.0069557251 0.0061539765 0.0054827668 0.0051474296][0.065621234 0.070355259 0.073572055 0.07376422 0.069729142 0.060269259 0.047512282 0.033777423 0.022593224 0.014674018 0.010274096 0.008810672 0.0080076447 0.0072711911 0.006692729][0.075633548 0.0801994 0.083104052 0.082733929 0.07789772 0.067672424 0.053717826 0.038790617 0.026484415 0.017718783 0.012341647 0.0097884331 0.0084567629 0.0079645813 0.007598158][0.084543951 0.088950567 0.091651127 0.091286108 0.086229429 0.075714223 0.061045941 0.045063078 0.031276576 0.021047015 0.014729776 0.011549376 0.010177334 0.010078173 0.010231163][0.092003368 0.096183069 0.098598823 0.098425679 0.093658969 0.083503969 0.069172964 0.053257503 0.038925603 0.027595919 0.020221757 0.016217135 0.014627225 0.015163929 0.015996272][0.097882986 0.1022416 0.10466488 0.1048617 0.10081997 0.09135776 0.077743366 0.062812515 0.048678972 0.037224591 0.029732233 0.025614217 0.024130076 0.02510565 0.026662588][0.10283019 0.10764414 0.11004995 0.11076017 0.10788274 0.10007805 0.088263795 0.074999087 0.062313262 0.051301505 0.04370258 0.0393783 0.038168866 0.039633617 0.0415102][0.10586841 0.11118621 0.11386965 0.11507656 0.11346313 0.10745879 0.09790317 0.086871989 0.076214664 0.0670051 0.060372014 0.056583092 0.055566929 0.057005186 0.058392759][0.10625958 0.11208807 0.1151375 0.1171397 0.11689318 0.11310198 0.10645273 0.098424338 0.090097055 0.082725815 0.077270709 0.074101858 0.073104315 0.073967 0.074981786][0.1047558 0.11082511 0.11396275 0.11658207 0.11759717 0.1156856 0.11132193 0.10569905 0.099793136 0.0943207 0.089939915 0.087415069 0.086777732 0.087722316 0.0888548][0.10143975 0.10757613 0.11102039 0.11420991 0.11646347 0.11633206 0.1139932 0.11026081 0.105951 0.1017132 0.097951576 0.095547117 0.094882771 0.095528193 0.09669251][0.098325029 0.10387506 0.10693468 0.11029088 0.11323667 0.11452651 0.11391089 0.11162664 0.10848588 0.10470176 0.10124998 0.098649889 0.097378068 0.097555012 0.098522909][0.094976529 0.10007305 0.10251059 0.10528003 0.10815988 0.11007644 0.11055344 0.109153 0.10656876 0.1030094 0.0995393 0.096880585 0.095142551 0.095019639 0.095961779][0.090329237 0.095079713 0.097203575 0.099828832 0.10259064 0.10455395 0.105427 0.10475335 0.10292852 0.099545255 0.096101671 0.093275137 0.091356434 0.091022409 0.091874324][0.086076342 0.090222657 0.091646791 0.093789913 0.096136674 0.09821967 0.099533245 0.099568546 0.09862078 0.095679566 0.092531279 0.089727066 0.087928921 0.087649994 0.088425234]]...]
INFO - root - 2017-12-09 16:20:52.461123: step 38610, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 70h:54m:34s remains)
INFO - root - 2017-12-09 16:21:01.062549: step 38620, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.856 sec/batch; 69h:51m:13s remains)
INFO - root - 2017-12-09 16:21:09.611201: step 38630, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.873 sec/batch; 71h:16m:37s remains)
INFO - root - 2017-12-09 16:21:18.151244: step 38640, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.851 sec/batch; 69h:28m:32s remains)
INFO - root - 2017-12-09 16:21:26.713183: step 38650, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.870 sec/batch; 71h:01m:52s remains)
INFO - root - 2017-12-09 16:21:35.452539: step 38660, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 70h:53m:59s remains)
INFO - root - 2017-12-09 16:21:44.107023: step 38670, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.859 sec/batch; 70h:06m:21s remains)
INFO - root - 2017-12-09 16:21:52.883242: step 38680, loss = 0.81, batch loss = 0.68 (9.2 examples/sec; 0.867 sec/batch; 70h:45m:06s remains)
INFO - root - 2017-12-09 16:22:01.631727: step 38690, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 70h:31m:33s remains)
INFO - root - 2017-12-09 16:22:10.245958: step 38700, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.892 sec/batch; 72h:46m:13s remains)
2017-12-09 16:22:11.141013: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0018146897 -0.0018214926 -0.001827018 -0.001830793 -0.00183283 -0.0018336452 -0.0018341159 -0.0018337858 -0.0018319914 -0.0018291937 -0.0018267785 -0.0018244759 -0.0018224085 -0.0018212504 -0.0018212919][-0.0018105947 -0.0018205126 -0.001827013 -0.0018310205 -0.00183329 -0.0018344398 -0.0018348364 -0.0018334343 -0.0018302234 -0.0018252509 -0.001820546 -0.0018164682 -0.0018147043 -0.0018154531 -0.0018175028][-0.0018123682 -0.0018234508 -0.0018298483 -0.0018333904 -0.0018351214 -0.0018357438 -0.0018354403 -0.001832569 -0.0018264569 -0.0018175074 -0.0018095203 -0.0018048469 -0.0018055759 -0.0018102473 -0.0018156075][-0.0018188375 -0.0018274749 -0.0018323129 -0.001834668 -0.0018354432 -0.001835164 -0.0018334633 -0.0018283277 -0.0018189576 -0.0018066895 -0.0017973752 -0.0017942224 -0.0017985976 -0.0018074616 -0.0018158803][-0.0018284976 -0.0018322412 -0.0018341612 -0.0018346638 -0.0018345919 -0.0018332592 -0.0018295746 -0.0018215639 -0.0018104929 -0.0017986884 -0.0017923238 -0.001791809 -0.0017986188 -0.0018085227 -0.0018166249][-0.0018351424 -0.0018348814 -0.0018343589 -0.0018332581 -0.0018323234 -0.0018295887 -0.0018238355 -0.0018148721 -0.0018059057 -0.001798242 -0.0017957112 -0.0017965212 -0.0018026376 -0.0018106237 -0.0018164082][-0.0018358261 -0.0018337143 -0.0018319723 -0.001829751 -0.0018277937 -0.0018245811 -0.0018187664 -0.0018115431 -0.0018062901 -0.0018030144 -0.0018025722 -0.0018030395 -0.0018064372 -0.0018109685 -0.00181423][-0.0018319399 -0.0018288353 -0.0018263167 -0.0018231649 -0.0018207341 -0.0018178305 -0.001813448 -0.001809308 -0.0018067763 -0.0018055142 -0.0018053198 -0.0018052465 -0.0018064427 -0.0018086425 -0.0018105609][-0.0018253148 -0.0018222427 -0.001819332 -0.0018161866 -0.0018135782 -0.0018114554 -0.0018089558 -0.0018070695 -0.0018054483 -0.0018044446 -0.0018042304 -0.0018039715 -0.0018040817 -0.001805078 -0.0018065788][-0.001818881 -0.0018165788 -0.0018142093 -0.0018114127 -0.0018094968 -0.0018083073 -0.0018069185 -0.0018056285 -0.0018041549 -0.0018029111 -0.0018023746 -0.0018019185 -0.0018017988 -0.001802374 -0.0018034769][-0.0018144461 -0.0018125912 -0.0018110124 -0.0018091258 -0.0018077471 -0.0018068668 -0.0018058883 -0.0018049729 -0.0018039756 -0.0018028824 -0.0018021755 -0.0018014782 -0.0018011236 -0.0018016452 -0.0018024074][-0.0018111174 -0.0018096971 -0.0018088908 -0.0018080199 -0.001807084 -0.0018063002 -0.0018053865 -0.0018046955 -0.0018039988 -0.001803224 -0.001802649 -0.0018020155 -0.0018016408 -0.0018019334 -0.001802464][-0.0018085602 -0.0018074218 -0.0018070054 -0.0018067734 -0.0018063808 -0.0018059 -0.0018051602 -0.0018046012 -0.0018041021 -0.0018034286 -0.0018030792 -0.001802661 -0.0018023471 -0.0018023993 -0.0018027334][-0.0018075145 -0.0018063416 -0.0018059283 -0.0018058555 -0.0018056552 -0.0018054838 -0.0018051177 -0.001804776 -0.0018043459 -0.001803925 -0.0018035908 -0.0018032183 -0.0018029531 -0.0018028961 -0.0018030796][-0.0018066576 -0.0018054254 -0.0018050796 -0.0018051399 -0.0018050817 -0.0018051217 -0.0018050782 -0.0018049034 -0.0018045447 -0.0018042175 -0.001804005 -0.0018036759 -0.0018034254 -0.0018034186 -0.0018034691]]...]
INFO - root - 2017-12-09 16:22:19.821323: step 38710, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.858 sec/batch; 69h:59m:03s remains)
INFO - root - 2017-12-09 16:22:28.610281: step 38720, loss = 0.81, batch loss = 0.68 (9.0 examples/sec; 0.885 sec/batch; 72h:12m:23s remains)
INFO - root - 2017-12-09 16:22:36.938252: step 38730, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.872 sec/batch; 71h:08m:07s remains)
INFO - root - 2017-12-09 16:22:45.691804: step 38740, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.851 sec/batch; 69h:25m:48s remains)
INFO - root - 2017-12-09 16:22:54.284904: step 38750, loss = 0.82, batch loss = 0.69 (8.5 examples/sec; 0.937 sec/batch; 76h:28m:44s remains)
INFO - root - 2017-12-09 16:23:03.009297: step 38760, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.861 sec/batch; 70h:16m:13s remains)
INFO - root - 2017-12-09 16:23:11.620818: step 38770, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.837 sec/batch; 68h:19m:30s remains)
INFO - root - 2017-12-09 16:23:20.518268: step 38780, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.844 sec/batch; 68h:53m:45s remains)
INFO - root - 2017-12-09 16:23:29.124392: step 38790, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.878 sec/batch; 71h:37m:44s remains)
INFO - root - 2017-12-09 16:23:37.748766: step 38800, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.854 sec/batch; 69h:40m:08s remains)
2017-12-09 16:23:38.744254: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0017981026 -0.0017964884 -0.0017961189 -0.00179575 -0.0017952761 -0.0017948486 -0.0017945515 -0.0017943856 -0.0017943035 -0.0017942745 -0.001794277 -0.0017943126 -0.0017943281 -0.0017943782 -0.0017944163][-0.0017972842 -0.001795745 -0.0017954361 -0.0017951156 -0.0017946914 -0.0017943061 -0.0017940364 -0.0017938765 -0.0017938028 -0.0017937691 -0.001793769 -0.0017938018 -0.0017938206 -0.0017938481 -0.0017939068][-0.001797324 -0.0017959616 -0.0017957282 -0.0017954801 -0.0017951141 -0.0017947681 -0.0017945024 -0.0017943254 -0.0017942382 -0.0017941894 -0.0017941874 -0.0017942288 -0.0017942714 -0.0017943349 -0.001794446][-0.001797458 -0.0017962334 -0.0017960855 -0.0017959508 -0.001795674 -0.0017953607 -0.0017950705 -0.0017948442 -0.0017947016 -0.0017946198 -0.0017946328 -0.0017947168 -0.0017948244 -0.0017949711 -0.0017951874][-0.0017976179 -0.0017964389 -0.0017963713 -0.001796357 -0.0017961784 -0.0017958969 -0.0017955785 -0.0017952783 -0.0017950498 -0.001794913 -0.0017949152 -0.0017950296 -0.0017951985 -0.0017954465 -0.0017957854][-0.0017975955 -0.0017964201 -0.0017964245 -0.0017964911 -0.0017963934 -0.0017961739 -0.0017958746 -0.0017955701 -0.0017953133 -0.0017951683 -0.0017951834 -0.0017953464 -0.0017955845 -0.0017959184 -0.0017963669][-0.0017974675 -0.001796254 -0.0017962708 -0.0017963522 -0.0017962675 -0.0017960733 -0.0017958047 -0.0017955232 -0.0017952733 -0.0017951586 -0.0017952175 -0.0017954337 -0.0017957311 -0.0017961295 -0.0017966409][-0.001797318 -0.0017959785 -0.0017959162 -0.0017959018 -0.0017957372 -0.0017955009 -0.0017952318 -0.0017949401 -0.001794679 -0.0017945713 -0.0017946275 -0.0017948378 -0.0017951349 -0.0017955331 -0.0017960402][-0.0017970795 -0.0017956279 -0.0017954777 -0.0017953204 -0.0017950403 -0.0017947308 -0.0017944403 -0.0017941541 -0.0017939039 -0.0017937885 -0.0017938175 -0.0017939977 -0.001794266 -0.0017946296 -0.0017950891][-0.001796703 -0.0017952694 -0.0017950129 -0.0017947144 -0.0017943357 -0.001793994 -0.0017937451 -0.0017935636 -0.0017934195 -0.0017933534 -0.0017933567 -0.0017934724 -0.0017936515 -0.0017938953 -0.0017942216][-0.0017964893 -0.0017950843 -0.0017947484 -0.0017943684 -0.001793947 -0.0017936161 -0.0017934518 -0.0017934124 -0.001793404 -0.0017934226 -0.001793453 -0.001793543 -0.0017936532 -0.0017937924 -0.0017939958][-0.0017965233 -0.0017950484 -0.0017946106 -0.0017941839 -0.001793741 -0.0017934149 -0.0017933056 -0.0017933388 -0.0017933965 -0.0017934405 -0.0017934701 -0.0017935223 -0.0017935669 -0.0017936241 -0.0017937366][-0.0017965699 -0.00179508 -0.0017945529 -0.001794117 -0.0017936832 -0.0017933634 -0.0017932496 -0.0017932785 -0.0017933364 -0.0017933504 -0.0017933352 -0.0017933272 -0.0017933061 -0.0017933007 -0.0017933482][-0.0017966767 -0.001795091 -0.0017945303 -0.0017941428 -0.0017937677 -0.0017934781 -0.0017933542 -0.0017933853 -0.0017934509 -0.0017934708 -0.001793461 -0.0017934524 -0.0017934319 -0.0017934135 -0.0017934246][-0.0017967842 -0.0017951741 -0.0017945917 -0.0017942978 -0.0017940132 -0.0017937793 -0.0017936663 -0.0017936951 -0.0017937646 -0.0017938096 -0.0017938371 -0.0017938631 -0.0017938759 -0.00179388 -0.0017938936]]...]
INFO - root - 2017-12-09 16:23:47.427887: step 38810, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.855 sec/batch; 69h:47m:01s remains)
INFO - root - 2017-12-09 16:23:56.123392: step 38820, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.874 sec/batch; 71h:18m:00s remains)
INFO - root - 2017-12-09 16:24:04.704207: step 38830, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.879 sec/batch; 71h:44m:17s remains)
INFO - root - 2017-12-09 16:24:13.304921: step 38840, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 71h:04m:18s remains)
INFO - root - 2017-12-09 16:24:21.756136: step 38850, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 70h:18m:03s remains)
INFO - root - 2017-12-09 16:24:30.425216: step 38860, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.884 sec/batch; 72h:08m:30s remains)
INFO - root - 2017-12-09 16:24:38.858520: step 38870, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.859 sec/batch; 70h:02m:28s remains)
INFO - root - 2017-12-09 16:24:47.403629: step 38880, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.848 sec/batch; 69h:08m:40s remains)
INFO - root - 2017-12-09 16:24:55.913317: step 38890, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.858 sec/batch; 69h:56m:39s remains)
INFO - root - 2017-12-09 16:25:04.468959: step 38900, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.861 sec/batch; 70h:10m:48s remains)
2017-12-09 16:25:05.338553: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.00028087851 0.0010325036 0.0017022872 0.0021838488 0.0024065573 0.0023649775 0.002092351 0.0016843681 0.001256638 0.00089711 0.00066068885 0.000564093 0.00059219636 0.00072753942 0.00091796159][-0.00014274486 0.00074618263 0.0016423368 0.002389557 0.0028584192 0.0029773463 0.0027529097 0.0022587429 0.0016722437 0.0011475573 0.00078740739 0.00062621967 0.00064888527 0.00082644378 0.0010958069][-0.00061327091 0.0003049036 0.0013743739 0.0024299037 0.0032915126 0.0037824695 0.00380894 0.0033966687 0.002712711 0.0019650869 0.0013427208 0.00094743678 0.00080021564 0.00087196869 0.0010897976][-0.00071527832 0.00036506937 0.0017466922 0.0032194883 0.0045280466 0.0054250937 0.0057269726 0.0053364495 0.0044317394 0.0033190178 0.0022906999 0.0015061584 0.0010220518 0.0008355584 0.00086511881][-0.00057077 0.00084363297 0.0027327992 0.00484984 0.006827293 0.0082628028 0.0088231238 0.0083502335 0.007035125 0.0052754637 0.0035318614 0.002097022 0.0010938835 0.00051136152 0.0002677897][-0.00030990515 0.001495125 0.00395043 0.0067174584 0.0093189143 0.011242026 0.012008213 0.011370197 0.0095465556 0.0070770252 0.0045890734 0.0024797746 0.00092584849 -5.7167141e-05 -0.00054783258][-7.8614568e-05 0.0019934117 0.0048619723 0.0080918139 0.011098203 0.013291298 0.01412014 0.013306627 0.011044841 0.0079953121 0.0049487008 0.0024012921 0.00053493329 -0.00064807397 -0.0012320601][1.8133083e-05 0.0021123982 0.0050696554 0.00842583 0.011516384 0.013721844 0.014512424 0.013592323 0.011120292 0.0078207646 0.0045691365 0.0019111321 2.123497e-05 -0.0011110494 -0.001612703][-0.00012041884 0.0017410712 0.0044291541 0.0075319726 0.010361792 0.012331123 0.012961765 0.01201872 0.0096300542 0.0064832643 0.003456726 0.0010616202 -0.00054235756 -0.0014152362 -0.0017418441][-0.00040304265 0.0010850593 0.0032950535 0.0058680959 0.0081783859 0.0097064609 0.010072252 0.0091368919 0.007039411 0.0043672733 0.0018765398 2.1286891e-05 -0.001099722 -0.0016264853 -0.0017867219][-0.00081445312 0.00024408544 0.0018639765 0.003756799 0.0053911842 0.0063731708 0.0064615607 0.005630292 0.0040019471 0.0020086886 0.00023906748 -0.0009513665 -0.0015442942 -0.001758175 -0.001805925][-0.0012705014 -0.00062844937 0.00037826365 0.0015534849 0.0025075416 0.002998749 0.0029406834 0.0023657561 0.0013379864 0.00010649324 -0.00091789948 -0.0015173763 -0.0017464801 -0.0018021008 -0.0018108425][-0.0016281581 -0.0013389082 -0.00086702232 -0.00030864892 0.00012185518 0.00031127082 0.00025382871 -2.1607964e-05 -0.0004988322 -0.0010692123 -0.0015138683 -0.0017388498 -0.0018010383 -0.001808753 -0.0018108394][-0.0017994721 -0.0017308039 -0.0016061796 -0.0014469886 -0.0013285058 -0.0012823252 -0.0012945689 -0.0013587719 -0.0014823151 -0.0016426103 -0.0017594458 -0.0018063704 -0.0018133299 -0.0018138145 -0.0018144754][-0.0018240224 -0.0018215781 -0.001815306 -0.0018073939 -0.0018015549 -0.0017999299 -0.0017991191 -0.0018003664 -0.0018055943 -0.0018123109 -0.0018168751 -0.0018178748 -0.001818174 -0.0018182986 -0.0018182929]]...]
INFO - root - 2017-12-09 16:25:14.101606: step 38910, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 70h:50m:15s remains)
INFO - root - 2017-12-09 16:25:22.872098: step 38920, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.899 sec/batch; 73h:20m:20s remains)
INFO - root - 2017-12-09 16:25:31.485836: step 38930, loss = 0.82, batch loss = 0.69 (11.2 examples/sec; 0.715 sec/batch; 58h:16m:25s remains)
INFO - root - 2017-12-09 16:25:40.165378: step 38940, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.848 sec/batch; 69h:10m:36s remains)
INFO - root - 2017-12-09 16:25:48.490500: step 38950, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.845 sec/batch; 68h:54m:34s remains)
INFO - root - 2017-12-09 16:25:57.010252: step 38960, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.873 sec/batch; 71h:12m:01s remains)
INFO - root - 2017-12-09 16:26:05.589580: step 38970, loss = 0.82, batch loss = 0.69 (8.8 examples/sec; 0.906 sec/batch; 73h:50m:05s remains)
INFO - root - 2017-12-09 16:26:14.289668: step 38980, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.879 sec/batch; 71h:40m:11s remains)
INFO - root - 2017-12-09 16:26:22.926882: step 38990, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.886 sec/batch; 72h:13m:30s remains)
INFO - root - 2017-12-09 16:26:31.675979: step 39000, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.846 sec/batch; 68h:55m:55s remains)
2017-12-09 16:26:32.632934: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0018386284 -0.001838343 -0.0018382759 -0.0018382334 -0.001838189 -0.0018381855 -0.0018381352 -0.0018380812 -0.0018380239 -0.0018379424 -0.001837845 -0.0018377171 -0.0018375709 -0.0018374268 -0.0018372592][-0.001838375 -0.0018381381 -0.0018381374 -0.0018381485 -0.0018381405 -0.0018381292 -0.0018380797 -0.0018380218 -0.0018379558 -0.0018378589 -0.0018377666 -0.0018376386 -0.0018374553 -0.0018372475 -0.001837025][-0.0018384776 -0.0018384067 -0.0018385125 -0.0018385854 -0.0018385948 -0.0018385219 -0.0018383835 -0.0018382113 -0.001838046 -0.0018379017 -0.0018377797 -0.0018376521 -0.0018374723 -0.0018372398 -0.0018369857][-0.001839068 -0.001839217 -0.0018394209 -0.0018394964 -0.0018394184 -0.0018391636 -0.0018388253 -0.0018384794 -0.0018381646 -0.0018379304 -0.0018377777 -0.001837688 -0.0018375574 -0.001837359 -0.0018370781][-0.0018402529 -0.0018405138 -0.0018407138 -0.0018406713 -0.0018403291 -0.0018397397 -0.001839096 -0.0018385055 -0.0018380386 -0.0018377108 -0.0018375656 -0.0018375353 -0.0018375044 -0.0018374107 -0.0018372044][-0.0018414757 -0.0018416686 -0.0018417052 -0.0018413278 -0.0018405957 -0.001839649 -0.001838728 -0.0018379798 -0.0018374815 -0.0018372065 -0.0018371295 -0.0018372013 -0.001837288 -0.0018373246 -0.0018372544][-0.0018420827 -0.0018420459 -0.0018418877 -0.0018413246 -0.0018403635 -0.0018392097 -0.0018381232 -0.0018373466 -0.001836961 -0.0018368158 -0.0018368537 -0.0018370675 -0.0018373065 -0.0018374751 -0.0018375166][-0.0018422949 -0.001842075 -0.0018419712 -0.0018415569 -0.0018407787 -0.0018397799 -0.0018388237 -0.0018381005 -0.001837699 -0.0018375254 -0.0018375563 -0.0018377643 -0.0018379786 -0.001838132 -0.0018381746][-0.0018426921 -0.0018423561 -0.0018422368 -0.0018419204 -0.0018413413 -0.0018406401 -0.0018399981 -0.0018394935 -0.0018391733 -0.0018389871 -0.0018389346 -0.0018390199 -0.0018390662 -0.0018390447 -0.0018389833][-0.0018427989 -0.001842333 -0.0018420649 -0.001841687 -0.0018412296 -0.0018407672 -0.0018404659 -0.0018403506 -0.001840368 -0.0018404059 -0.0018403523 -0.0018402415 -0.0018400435 -0.0018397538 -0.001839451][-0.0018422463 -0.0018417643 -0.0018414712 -0.0018411515 -0.0018408387 -0.0018406276 -0.001840674 -0.0018409465 -0.0018413313 -0.0018416055 -0.0018415614 -0.0018412258 -0.0018407295 -0.0018400982 -0.0018394855][-0.0018416534 -0.001841239 -0.0018410158 -0.0018408581 -0.0018407339 -0.0018407283 -0.0018410012 -0.0018414881 -0.0018420463 -0.0018424267 -0.0018424351 -0.0018420184 -0.0018413136 -0.0018404508 -0.0018396131][-0.0018412694 -0.0018410846 -0.0018409955 -0.0018409062 -0.0018408058 -0.0018407867 -0.0018410643 -0.0018415952 -0.0018421753 -0.0018425612 -0.0018426265 -0.0018422692 -0.0018415785 -0.0018407148 -0.0018398336][-0.0018406977 -0.0018406445 -0.0018405555 -0.001840396 -0.0018402369 -0.0018401711 -0.0018404262 -0.0018409775 -0.0018415786 -0.0018419587 -0.0018420602 -0.0018418086 -0.0018412494 -0.0018405053 -0.0018397609][-0.0018397571 -0.0018397298 -0.0018396138 -0.0018394449 -0.0018393295 -0.0018393092 -0.0018395797 -0.0018401272 -0.0018406935 -0.0018410239 -0.0018411174 -0.0018409803 -0.0018406112 -0.0018400876 -0.0018395763]]...]
INFO - root - 2017-12-09 16:26:41.193806: step 39010, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 70h:15m:06s remains)
INFO - root - 2017-12-09 16:26:49.684035: step 39020, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.836 sec/batch; 68h:08m:15s remains)
INFO - root - 2017-12-09 16:26:58.295257: step 39030, loss = 0.83, batch loss = 0.70 (9.1 examples/sec; 0.881 sec/batch; 71h:48m:40s remains)
INFO - root - 2017-12-09 16:27:06.731053: step 39040, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.845 sec/batch; 68h:52m:45s remains)
INFO - root - 2017-12-09 16:27:15.194284: step 39050, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.857 sec/batch; 69h:49m:23s remains)
INFO - root - 2017-12-09 16:27:23.857803: step 39060, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.883 sec/batch; 71h:58m:12s remains)
INFO - root - 2017-12-09 16:27:32.408494: step 39070, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.848 sec/batch; 69h:07m:48s remains)
INFO - root - 2017-12-09 16:27:41.036024: step 39080, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.848 sec/batch; 69h:08m:07s remains)
INFO - root - 2017-12-09 16:27:49.679049: step 39090, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.880 sec/batch; 71h:43m:57s remains)
INFO - root - 2017-12-09 16:27:58.405027: step 39100, loss = 0.83, batch loss = 0.70 (9.4 examples/sec; 0.849 sec/batch; 69h:12m:57s remains)
2017-12-09 16:27:59.304061: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.0092040049 0.017714961 0.028799189 0.040305469 0.050281197 0.057138991 0.060169876 0.05998775 0.057459824 0.054108165 0.050185226 0.04535909 0.038724747 0.030452628 0.021492349][0.013419434 0.024877565 0.040118996 0.05642134 0.071324505 0.082555287 0.088971272 0.091141127 0.089645557 0.086119726 0.080704622 0.073014706 0.062119208 0.048609771 0.034208458][0.017677439 0.032119717 0.051925115 0.073912911 0.094933368 0.1117647 0.12250824 0.12747157 0.12698846 0.12297716 0.11581995 0.10519263 0.089865968 0.070677996 0.050110023][0.020797303 0.0378077 0.062016889 0.090268016 0.11868651 0.14296672 0.15991421 0.16900282 0.16991529 0.16469051 0.15427412 0.1389398 0.11777083 0.092089616 0.065120652][0.022273673 0.040901318 0.068720311 0.10265693 0.13830863 0.17033193 0.1939242 0.20746413 0.2101227 0.2039333 0.19039863 0.17025526 0.14320599 0.11122747 0.078211732][0.021935716 0.040728983 0.070453092 0.10823835 0.14942078 0.18767549 0.21685761 0.2340706 0.23786718 0.23051566 0.21415004 0.19011982 0.15871391 0.1224227 0.085601866][0.020810388 0.038854919 0.068469927 0.1074753 0.15110582 0.19274257 0.22520962 0.24461974 0.24911581 0.24089824 0.22264044 0.19617346 0.16240682 0.124241 0.086212553][0.018829038 0.03512663 0.062579922 0.099631071 0.14202125 0.18332548 0.21640249 0.23659059 0.24178481 0.23355757 0.21479286 0.18764889 0.15353847 0.11595629 0.079345964][0.016306788 0.030200746 0.053832728 0.086234726 0.12398205 0.16148333 0.19214837 0.2114411 0.21705702 0.20975052 0.19198622 0.16589206 0.13363326 0.099009693 0.066221416][0.013420759 0.024480104 0.043183584 0.068958037 0.099374615 0.13006783 0.15567793 0.17197922 0.17706536 0.1711708 0.15601255 0.13338777 0.10561389 0.076574951 0.049839996][0.010065724 0.018249687 0.031630136 0.049961966 0.0717061 0.09394107 0.11278694 0.12488376 0.12875557 0.12435859 0.11282649 0.095465392 0.074318565 0.0527022 0.033357996][0.0061983806 0.011742993 0.020268869 0.031694453 0.04512953 0.058989219 0.070861973 0.078481108 0.080897674 0.078000128 0.070346385 0.058736056 0.04476548 0.030858882 0.018788634][0.0020621605 0.0051058894 0.0096151289 0.015563931 0.022592874 0.030000355 0.036485013 0.040752243 0.042171407 0.04055028 0.036127258 0.029430211 0.021600634 0.014096793 0.0078369388][-0.0007561465 0.00037339644 0.0021213107 0.0045145657 0.0074246996 0.010597232 0.013521055 0.015565073 0.016369173 0.015742481 0.013724344 0.010669605 0.0072122631 0.0040430431 0.0015222566][-0.0017136516 -0.0015420276 -0.0011931049 -0.00062021206 0.00017952721 0.0011651678 0.0021835542 0.002978038 0.0033762837 0.0032719476 0.0026646764 0.0017086145 0.00064233935 -0.00029753346 -0.0010019388]]...]
INFO - root - 2017-12-09 16:28:07.959241: step 39110, loss = 0.82, batch loss = 0.69 (9.7 examples/sec; 0.827 sec/batch; 67h:24m:36s remains)
INFO - root - 2017-12-09 16:28:16.627225: step 39120, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.876 sec/batch; 71h:23m:34s remains)
INFO - root - 2017-12-09 16:28:25.321666: step 39130, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.852 sec/batch; 69h:26m:10s remains)
INFO - root - 2017-12-09 16:28:33.901003: step 39140, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.855 sec/batch; 69h:42m:31s remains)
INFO - root - 2017-12-09 16:28:42.355429: step 39150, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.848 sec/batch; 69h:06m:38s remains)
INFO - root - 2017-12-09 16:28:50.901926: step 39160, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.856 sec/batch; 69h:42m:54s remains)
INFO - root - 2017-12-09 16:28:59.504422: step 39170, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 70h:38m:00s remains)
INFO - root - 2017-12-09 16:29:08.380624: step 39180, loss = 0.82, batch loss = 0.69 (8.5 examples/sec; 0.939 sec/batch; 76h:31m:31s remains)
INFO - root - 2017-12-09 16:29:17.101498: step 39190, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.847 sec/batch; 69h:00m:06s remains)
INFO - root - 2017-12-09 16:29:25.798130: step 39200, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.875 sec/batch; 71h:16m:39s remains)
2017-12-09 16:29:26.720173: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.16041742 0.15616517 0.15181206 0.14710857 0.14243208 0.13804828 0.13465086 0.13138302 0.12837447 0.12541315 0.1211945 0.1154394 0.10834372 0.10062185 0.092883646][0.16778763 0.16637732 0.16417244 0.16115767 0.15750021 0.15387605 0.15042675 0.14702339 0.14362982 0.14032589 0.13612203 0.13013229 0.12295845 0.11479707 0.10649075][0.17328058 0.1752252 0.1757627 0.17501447 0.1729864 0.17012757 0.16682732 0.16310607 0.15909572 0.1548937 0.15013328 0.14392737 0.13626111 0.12763096 0.11869102][0.17662364 0.18196727 0.18529768 0.18666145 0.18642488 0.18482274 0.18188237 0.17804067 0.17365208 0.16893785 0.16333514 0.1566714 0.14859746 0.13934806 0.12964913][0.17760004 0.18564427 0.19151963 0.19507036 0.19637676 0.19615692 0.19391346 0.19035767 0.1855873 0.1802097 0.1739624 0.16627201 0.15769334 0.14794321 0.1379317][0.17486152 0.18523808 0.19300468 0.19843814 0.20147112 0.2023939 0.20102932 0.19810067 0.1934178 0.1876732 0.18049192 0.17202263 0.16284633 0.15291788 0.14293876][0.1694722 0.1811832 0.19012378 0.19694066 0.20111272 0.20312621 0.20244947 0.20035851 0.19609426 0.19027157 0.18285896 0.17399596 0.16471991 0.15484504 0.14513327][0.16326484 0.17506622 0.1841023 0.19126697 0.19594131 0.19847868 0.19852662 0.197268 0.19368659 0.1883518 0.1812101 0.17262851 0.16354464 0.15408218 0.14484474][0.15585737 0.16689345 0.1750131 0.18134674 0.18551838 0.18802091 0.18820857 0.18740438 0.18444075 0.18020973 0.17406465 0.16646031 0.15851225 0.15003502 0.1419363][0.14666128 0.15634969 0.16308758 0.16840927 0.1719497 0.17380865 0.17388955 0.17344987 0.17115457 0.16772258 0.16255143 0.15641192 0.14992274 0.14293677 0.13642007][0.13735996 0.14550954 0.15061793 0.15455808 0.1572932 0.1584522 0.1583119 0.1578337 0.15593626 0.153253 0.14909764 0.14431085 0.139357 0.1339439 0.12874287][0.12813112 0.13465278 0.13816495 0.14081205 0.14250377 0.14277586 0.14226641 0.14184363 0.14068437 0.13879867 0.13591969 0.1326099 0.12907822 0.12495436 0.12067053][0.11880213 0.12419192 0.12651978 0.12800923 0.12902525 0.12840223 0.12730879 0.12643978 0.12561825 0.12462953 0.12282869 0.12092158 0.11863809 0.11579988 0.11245191][0.10900234 0.11349574 0.11498285 0.11583009 0.11649962 0.11556689 0.11439741 0.11325299 0.11251103 0.11186478 0.1106217 0.10990668 0.10859531 0.10710167 0.10500125][0.10082506 0.10473391 0.1057383 0.10654858 0.10732862 0.1066345 0.10574695 0.10486669 0.1043377 0.10374408 0.10277472 0.10233331 0.10157066 0.10067452 0.099257179]]...]
INFO - root - 2017-12-09 16:29:35.353474: step 39210, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.890 sec/batch; 72h:32m:06s remains)
INFO - root - 2017-12-09 16:29:44.022426: step 39220, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.884 sec/batch; 71h:58m:39s remains)
INFO - root - 2017-12-09 16:29:52.791079: step 39230, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.876 sec/batch; 71h:19m:29s remains)
INFO - root - 2017-12-09 16:30:01.257571: step 39240, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.849 sec/batch; 69h:10m:48s remains)
INFO - root - 2017-12-09 16:30:09.850601: step 39250, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.857 sec/batch; 69h:50m:41s remains)
INFO - root - 2017-12-09 16:30:18.562906: step 39260, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.884 sec/batch; 72h:01m:08s remains)
INFO - root - 2017-12-09 16:30:26.993831: step 39270, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.840 sec/batch; 68h:23m:07s remains)
INFO - root - 2017-12-09 16:30:35.749315: step 39280, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.884 sec/batch; 71h:59m:41s remains)
INFO - root - 2017-12-09 16:30:44.452099: step 39290, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.894 sec/batch; 72h:49m:45s remains)
INFO - root - 2017-12-09 16:30:53.171069: step 39300, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.899 sec/batch; 73h:13m:56s remains)
2017-12-09 16:30:54.139654: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.001836819 -0.0018373628 -0.0018374078 -0.0018364713 -0.0018333927 -0.0018310124 -0.0018273786 -0.0018276258 -0.0018281188 -0.0018306121 -0.0018319767 -0.0018326974 -0.0018335276 -0.0018344087 -0.0018355269][-0.0018336603 -0.0018344894 -0.0018347425 -0.001833358 -0.0018289688 -0.001827354 -0.0018242473 -0.0018251854 -0.0018274005 -0.001830112 -0.0018309307 -0.0018309918 -0.0018312685 -0.0018322327 -0.0018332994][-0.001822593 -0.0018226979 -0.0018225011 -0.0018205938 -0.0018168476 -0.0018163207 -0.0018145595 -0.0018153578 -0.0018168514 -0.0018191515 -0.0018212416 -0.0018223714 -0.0018251353 -0.0018290601 -0.0018321776][-0.0017947515 -0.0017886413 -0.0017858775 -0.0017838215 -0.0017808374 -0.0017822983 -0.0017839845 -0.0017860477 -0.0017884518 -0.0017912558 -0.0017953792 -0.001798694 -0.0018060275 -0.001815398 -0.001824189][-0.001744046 -0.0017249121 -0.0017177175 -0.0017145532 -0.0017125964 -0.0017164067 -0.0017224682 -0.001727968 -0.0017353563 -0.0017414852 -0.0017512589 -0.001758257 -0.0017713633 -0.0017877101 -0.0018039249][-0.0016636355 -0.0016246444 -0.0016125295 -0.001609154 -0.001610833 -0.0016172534 -0.0016281282 -0.0016400083 -0.0016581937 -0.0016752317 -0.0016955993 -0.0017080409 -0.0017262114 -0.0017472473 -0.001771457][-0.0015695002 -0.0015091002 -0.0014940967 -0.0014916946 -0.0014964262 -0.0015072855 -0.0015251596 -0.0015476174 -0.0015822363 -0.0016146732 -0.0016470222 -0.0016644766 -0.0016844673 -0.0017061001 -0.0017347845][-0.0014911972 -0.0014112187 -0.0013965176 -0.0013965617 -0.0014052951 -0.0014219462 -0.0014480562 -0.0014828355 -0.0015332114 -0.0015804253 -0.0016230117 -0.0016442591 -0.0016626031 -0.0016813724 -0.0017096769][-0.0014540757 -0.0013616412 -0.0013489036 -0.0013535564 -0.0013687132 -0.0013940185 -0.0014289597 -0.0014733367 -0.0015326347 -0.0015869103 -0.001632434 -0.0016550006 -0.001671496 -0.0016870825 -0.0017119582][-0.001471562 -0.0013839507 -0.0013751012 -0.0013837301 -0.0014040368 -0.0014365793 -0.001478015 -0.0015259378 -0.0015826419 -0.0016323368 -0.0016724258 -0.0016932843 -0.0017077632 -0.0017208233 -0.0017403655][-0.0015401321 -0.0014697717 -0.0014661881 -0.0014774636 -0.0014998771 -0.0015341998 -0.00157472 -0.0016172081 -0.0016617846 -0.0016988458 -0.0017278887 -0.0017441157 -0.001755508 -0.0017654562 -0.0017788023][-0.0016368239 -0.0015908768 -0.0015913554 -0.0016027355 -0.0016228724 -0.0016518428 -0.0016833831 -0.0017134706 -0.0017416881 -0.0017636749 -0.0017805893 -0.0017911098 -0.001798676 -0.0018049596 -0.0018117968][-0.0017260312 -0.0017036314 -0.0017057676 -0.0017142567 -0.0017280708 -0.001747046 -0.0017666563 -0.0017837221 -0.0017976735 -0.0018079034 -0.0018157929 -0.0018210654 -0.0018251196 -0.0018285593 -0.0018313809][-0.0017888311 -0.0017803565 -0.0017822966 -0.0017872223 -0.0017944442 -0.0018036857 -0.0018128207 -0.001820051 -0.0018253817 -0.0018291193 -0.0018321781 -0.0018343853 -0.0018361226 -0.0018373999 -0.001838011][-0.0018214177 -0.0018185873 -0.001819425 -0.0018214969 -0.001824226 -0.0018272731 -0.0018298915 -0.0018317455 -0.0018332618 -0.0018347086 -0.0018362032 -0.0018375173 -0.0018383999 -0.0018388502 -0.001838868]]...]
INFO - root - 2017-12-09 16:31:02.842938: step 39310, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.887 sec/batch; 72h:15m:54s remains)
INFO - root - 2017-12-09 16:31:11.641010: step 39320, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.856 sec/batch; 69h:44m:40s remains)
INFO - root - 2017-12-09 16:31:20.456321: step 39330, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.837 sec/batch; 68h:12m:04s remains)
INFO - root - 2017-12-09 16:31:29.087366: step 39340, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.885 sec/batch; 72h:02m:23s remains)
INFO - root - 2017-12-09 16:31:37.470867: step 39350, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.866 sec/batch; 70h:32m:09s remains)
INFO - root - 2017-12-09 16:31:46.232889: step 39360, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.873 sec/batch; 71h:07m:26s remains)
INFO - root - 2017-12-09 16:31:54.804745: step 39370, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.863 sec/batch; 70h:18m:11s remains)
INFO - root - 2017-12-09 16:32:03.575100: step 39380, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.894 sec/batch; 72h:46m:35s remains)
INFO - root - 2017-12-09 16:32:12.249944: step 39390, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.835 sec/batch; 68h:01m:07s remains)
INFO - root - 2017-12-09 16:32:20.924265: step 39400, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.876 sec/batch; 71h:16m:58s remains)
2017-12-09 16:32:21.829558: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.00032998004 -0.00039478729 -0.0004298439 -0.0003621293 -0.00012292282 0.00023769098 0.00060973619 0.00089767016 0.001091779 0.0011607441 0.0010842031 0.00085424562 0.00058498047 0.00027635368 -0.00015921949][0.00030517997 0.00016744318 3.1703268e-05 1.4210935e-05 0.00025825342 0.00070151873 0.0012099654 0.001620265 0.0018955239 0.0020031082 0.0018685607 0.0015684746 0.0012160232 0.00085680047 0.00034813327][0.00093964557 0.00074728043 0.00052062469 0.00042545539 0.00066994573 0.0011801727 0.0018168001 0.0023339626 0.0026597877 0.0027389417 0.0025129311 0.0021133909 0.001672406 0.0012782773 0.00073129404][0.0015076674 0.0012919325 0.0010211319 0.00089493021 0.0011570372 0.0017406137 0.0024862934 0.0030708222 0.0033950873 0.0033884759 0.0030242123 0.0024917882 0.0019381107 0.0015047044 0.00094094547][0.0019695463 0.0017726177 0.0015184411 0.0014289506 0.0017470992 0.0024002043 0.003212611 0.0038167119 0.004089308 0.0039600218 0.0034241758 0.0027235884 0.0020374984 0.0015617404 0.0009988267][0.0023102157 0.0021839056 0.0020078209 0.0020081755 0.0024048088 0.003114878 0.0039422354 0.0045155538 0.0047026416 0.0044240877 0.0037081323 0.0028311124 0.0020114689 0.0014987697 0.000948427][0.002518309 0.0024967578 0.0024374973 0.0025572071 0.0030264773 0.0037505308 0.0045322115 0.00504377 0.0051379129 0.0047249524 0.0038374744 0.0028124976 0.0018906721 0.0013569386 0.00083413767][0.0025840576 0.0026895816 0.0027505497 0.0029739353 0.0034806544 0.0041647824 0.0048433915 0.0052673956 0.0052757533 0.0047682784 0.0037717854 0.0026668061 0.0017104226 0.0011783736 0.00068610371][0.002501264 0.0027337761 0.0028980456 0.0031797451 0.0036649087 0.0042479159 0.0047805086 0.0050902842 0.0050205328 0.0044761477 0.0034624182 0.0023838712 0.0014727081 0.00097294454 0.00051386328][0.0022403153 0.0025792846 0.0028138992 0.0031087 0.0035218836 0.0039560422 0.004315475 0.0044894069 0.00435107 0.0038216442 0.0028994849 0.0019490689 0.0011605602 0.00071771559 0.00029442459][0.0017843531 0.0021807018 0.0024390707 0.0027037745 0.0030125941 0.0032967527 0.0034928324 0.003535622 0.0033473584 0.0028770233 0.0021239968 0.0013731925 0.00075411168 0.00038312003 1.4398247e-06][0.0011323879 0.0015246945 0.001766002 0.001969975 0.0021679176 0.0023211164 0.0023915654 0.0023456642 0.0021456459 0.0017760901 0.001229299 0.00069607818 0.00025265291 -4.2425352e-05 -0.00037071249][0.0003569629 0.000690243 0.00088194851 0.0010149507 0.0011199082 0.0011835014 0.0011798805 0.0010970745 0.00092533207 0.00067008263 0.00032615568 -8.6970394e-06 -0.00029864663 -0.00052066066 -0.000781831][-0.0004190621 -0.0001776478 -4.70425e-05 2.608716e-05 6.8614725e-05 8.2982704e-05 5.461683e-05 -1.7014914e-05 -0.00013405224 -0.00028066628 -0.00046726642 -0.00065127411 -0.00082464749 -0.00098049291 -0.0011676988][-0.0010604733 -0.00091448775 -0.00083949161 -0.00080613955 -0.00079636055 -0.00079914439 -0.00082408963 -0.00086592603 -0.0009267056 -0.00099106715 -0.0010739376 -0.0011599381 -0.0012525432 -0.0013516278 -0.0014702507]]...]
INFO - root - 2017-12-09 16:32:30.597209: step 39410, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.890 sec/batch; 72h:29m:33s remains)
INFO - root - 2017-12-09 16:32:39.261852: step 39420, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.856 sec/batch; 69h:41m:26s remains)
INFO - root - 2017-12-09 16:32:47.898174: step 39430, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 70h:21m:19s remains)
INFO - root - 2017-12-09 16:32:56.460477: step 39440, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.872 sec/batch; 70h:59m:02s remains)
INFO - root - 2017-12-09 16:33:04.888030: step 39450, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 70h:19m:00s remains)
INFO - root - 2017-12-09 16:33:13.494992: step 39460, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.855 sec/batch; 69h:37m:22s remains)
INFO - root - 2017-12-09 16:33:22.111920: step 39470, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.873 sec/batch; 71h:01m:11s remains)
INFO - root - 2017-12-09 16:33:30.885928: step 39480, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.865 sec/batch; 70h:26m:22s remains)
INFO - root - 2017-12-09 16:33:39.644540: step 39490, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.882 sec/batch; 71h:45m:19s remains)
INFO - root - 2017-12-09 16:33:48.322951: step 39500, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 70h:41m:32s remains)
2017-12-09 16:33:49.176112: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.54925174 0.53380793 0.51453674 0.49403617 0.47276637 0.45085183 0.42631343 0.39784464 0.36371475 0.32266298 0.27343839 0.21798241 0.16081235 0.11032601 0.067835629][0.56214142 0.54895723 0.5324707 0.51497471 0.49535429 0.47437006 0.44915989 0.41840532 0.38061443 0.33608457 0.28338534 0.22430065 0.16455206 0.11242458 0.069395386][0.57253462 0.56177765 0.54750109 0.53210139 0.51354343 0.49223572 0.465184 0.432562 0.39130917 0.34378844 0.2879169 0.22706349 0.16624911 0.11279553 0.069276422][0.5824737 0.57608724 0.56453514 0.55034471 0.53200561 0.50992548 0.48141184 0.44678605 0.40289077 0.35247159 0.29369017 0.23106095 0.16882534 0.11386215 0.069354013][0.58661854 0.58408892 0.57619351 0.56459075 0.54830843 0.5263713 0.49733707 0.4611403 0.41507757 0.3617135 0.30058688 0.23553111 0.17096043 0.11496159 0.070127822][0.58625203 0.58644223 0.58074993 0.57192111 0.55858111 0.53905576 0.51204 0.47641093 0.42949122 0.37441435 0.31028557 0.24216504 0.17487919 0.1169251 0.070556238][0.57355565 0.57851529 0.57602507 0.57027406 0.55967152 0.54423934 0.52081674 0.4877266 0.44268152 0.38794461 0.32380864 0.25399926 0.18345249 0.12234574 0.073639557][0.55310619 0.56259453 0.56331968 0.5606342 0.55415231 0.54183406 0.52075332 0.48979813 0.44740593 0.39484167 0.33227521 0.26325145 0.19257081 0.12960517 0.078285582][0.53116685 0.54497749 0.54896086 0.54981512 0.54707205 0.53827447 0.52065986 0.49263486 0.45279089 0.40247434 0.34155625 0.27304441 0.20195203 0.13786781 0.08500845][0.50931591 0.52654195 0.53358221 0.537253 0.53789866 0.53286886 0.5190134 0.49447179 0.45802489 0.41145766 0.35399106 0.28755319 0.21738337 0.15312281 0.099218108][0.4842954 0.50531578 0.51548731 0.52152181 0.52386546 0.52161187 0.51126361 0.49082068 0.45895609 0.41784707 0.36601031 0.3046028 0.23815149 0.17552863 0.12232505][0.45791242 0.48091605 0.49294555 0.50078326 0.50491631 0.50451207 0.49653515 0.4788214 0.45178154 0.41709471 0.37286612 0.31932431 0.25974485 0.20210889 0.15193225][0.43352428 0.45641088 0.46859509 0.47653341 0.4821966 0.48271924 0.47662389 0.46204382 0.43938017 0.41086236 0.37368321 0.32895127 0.2781302 0.22805396 0.18378791][0.40778923 0.43070936 0.44239959 0.45105684 0.4575195 0.45922339 0.4555048 0.44566491 0.42933741 0.40739879 0.37812626 0.34169912 0.29883203 0.2557098 0.21762303][0.3785463 0.40097788 0.41299981 0.42134696 0.42792058 0.43046349 0.42921403 0.42249486 0.41084325 0.39632073 0.37671795 0.35036668 0.31750354 0.28337806 0.25245363]]...]
INFO - root - 2017-12-09 16:33:57.916266: step 39510, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.874 sec/batch; 71h:06m:26s remains)
INFO - root - 2017-12-09 16:34:06.661072: step 39520, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.894 sec/batch; 72h:45m:05s remains)
INFO - root - 2017-12-09 16:34:15.424549: step 39530, loss = 0.82, batch loss = 0.69 (9.7 examples/sec; 0.824 sec/batch; 67h:04m:12s remains)
INFO - root - 2017-12-09 16:34:23.993761: step 39540, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.861 sec/batch; 70h:03m:37s remains)
INFO - root - 2017-12-09 16:34:32.444876: step 39550, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.838 sec/batch; 68h:13m:50s remains)
INFO - root - 2017-12-09 16:34:41.102772: step 39560, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.849 sec/batch; 69h:05m:54s remains)
INFO - root - 2017-12-09 16:34:49.657308: step 39570, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.858 sec/batch; 69h:49m:54s remains)
INFO - root - 2017-12-09 16:34:58.208030: step 39580, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.853 sec/batch; 69h:25m:40s remains)
INFO - root - 2017-12-09 16:35:06.832241: step 39590, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.878 sec/batch; 71h:27m:07s remains)
INFO - root - 2017-12-09 16:35:15.288844: step 39600, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.878 sec/batch; 71h:27m:47s remains)
2017-12-09 16:35:16.202879: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0018419753 -0.0018439132 -0.0018448434 -0.001844488 -0.0018437775 -0.0018432717 -0.0018435041 -0.0018437653 -0.0018434715 -0.0018428271 -0.00184223 -0.0018418913 -0.0018417567 -0.0018414734 -0.0018412381][-0.0018415743 -0.0018432996 -0.001843525 -0.0018419942 -0.0018404872 -0.0018396908 -0.0018405308 -0.0018414386 -0.0018415689 -0.0018410883 -0.0018405882 -0.0018402049 -0.0018398716 -0.0018394673 -0.0018393066][-0.0018411659 -0.0018422802 -0.0018412258 -0.001837919 -0.0018352702 -0.0018343985 -0.0018365724 -0.0018387684 -0.0018397555 -0.0018396532 -0.0018391259 -0.0018384933 -0.001837721 -0.0018369791 -0.0018366067][-0.0018406138 -0.0018408222 -0.001838097 -0.0018323001 -0.001828488 -0.001827983 -0.0018323436 -0.0018362262 -0.0018378686 -0.0018377545 -0.0018367799 -0.0018356 -0.0018343246 -0.0018332393 -0.0018321747][-0.0018401169 -0.0018392864 -0.0018345395 -0.0018261431 -0.0018212888 -0.00182204 -0.0018285477 -0.0018339549 -0.0018358651 -0.0018351546 -0.0018332705 -0.001831374 -0.0018294207 -0.0018271975 -0.0018239781][-0.0018400439 -0.0018386147 -0.001832099 -0.0018217048 -0.0018166546 -0.0018186587 -0.0018266661 -0.0018319604 -0.0018334371 -0.0018321461 -0.0018295072 -0.0018267721 -0.0018235455 -0.001818369 -0.0018102266][-0.0018398997 -0.0018381755 -0.0018303673 -0.0018195728 -0.0018151801 -0.0018181285 -0.0018262555 -0.0018308026 -0.0018319753 -0.0018302037 -0.0018268089 -0.0018224483 -0.0018163943 -0.0018052513 -0.0017881589][-0.0018396938 -0.0018382317 -0.0018310189 -0.0018217325 -0.0018184263 -0.0018212781 -0.0018281052 -0.0018313116 -0.0018320668 -0.0018295679 -0.0018251763 -0.0018182837 -0.0018078055 -0.0017872552 -0.0017565194][-0.0018395029 -0.0018389643 -0.0018340993 -0.0018276648 -0.0018249936 -0.001827091 -0.0018319482 -0.0018338738 -0.0018341169 -0.0018308749 -0.0018249522 -0.0018144599 -0.0017975859 -0.0017648 -0.0017175809][-0.0018392698 -0.0018399734 -0.0018379187 -0.0018345003 -0.001832829 -0.0018337962 -0.0018364574 -0.0018373532 -0.0018371558 -0.0018333762 -0.0018257254 -0.001811164 -0.0017869433 -0.0017420835 -0.001681103][-0.0018387836 -0.0018405513 -0.0018410042 -0.0018401365 -0.0018395395 -0.0018398166 -0.0018407659 -0.0018410556 -0.0018404376 -0.0018365254 -0.0018274698 -0.0018092126 -0.0017779706 -0.0017257507 -0.0016597438][-0.0018381338 -0.0018403544 -0.0018420587 -0.0018427281 -0.0018429701 -0.0018430506 -0.0018432781 -0.0018433223 -0.0018425008 -0.0018387331 -0.0018293647 -0.0018100943 -0.0017774574 -0.0017270058 -0.0016659941][-0.0018375046 -0.0018397635 -0.0018418434 -0.0018432311 -0.0018439828 -0.0018442816 -0.0018442931 -0.0018440527 -0.0018430597 -0.0018399337 -0.0018321943 -0.0018158028 -0.0017873461 -0.0017458475 -0.0016961567][-0.0018368848 -0.0018388799 -0.0018408465 -0.0018423462 -0.0018432586 -0.0018436587 -0.0018436917 -0.0018433701 -0.0018423712 -0.001840114 -0.001835397 -0.0018250755 -0.0018065349 -0.0017782063 -0.0017428081][-0.0018362009 -0.0018377169 -0.0018392474 -0.001840464 -0.0018412506 -0.001841697 -0.0018417889 -0.0018414932 -0.0018407091 -0.0018392968 -0.0018370294 -0.0018324819 -0.001823468 -0.0018079008 -0.0017863526]]...]
INFO - root - 2017-12-09 16:35:24.958784: step 39610, loss = 0.82, batch loss = 0.69 (8.2 examples/sec; 0.973 sec/batch; 79h:11m:10s remains)
INFO - root - 2017-12-09 16:35:33.695087: step 39620, loss = 0.82, batch loss = 0.69 (8.8 examples/sec; 0.906 sec/batch; 73h:40m:30s remains)
INFO - root - 2017-12-09 16:35:42.627478: step 39630, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.891 sec/batch; 72h:31m:01s remains)
INFO - root - 2017-12-09 16:35:51.182764: step 39640, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.855 sec/batch; 69h:33m:02s remains)
INFO - root - 2017-12-09 16:35:59.771715: step 39650, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.865 sec/batch; 70h:23m:32s remains)
INFO - root - 2017-12-09 16:36:08.392260: step 39660, loss = 0.82, batch loss = 0.69 (8.8 examples/sec; 0.907 sec/batch; 73h:46m:56s remains)
INFO - root - 2017-12-09 16:36:16.823532: step 39670, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.845 sec/batch; 68h:43m:10s remains)
INFO - root - 2017-12-09 16:36:25.450170: step 39680, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.859 sec/batch; 69h:54m:34s remains)
INFO - root - 2017-12-09 16:36:34.022735: step 39690, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.852 sec/batch; 69h:19m:08s remains)
INFO - root - 2017-12-09 16:36:42.514097: step 39700, loss = 0.81, batch loss = 0.68 (9.6 examples/sec; 0.830 sec/batch; 67h:31m:08s remains)
2017-12-09 16:36:43.400769: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0018181498 -0.0018172172 -0.0018169541 -0.0018168875 -0.0018168632 -0.0018168892 -0.0018169619 -0.0018170736 -0.0018171611 -0.001817201 -0.0018172222 -0.0018172093 -0.0018171751 -0.0018171461 -0.0018171342][-0.0018177512 -0.0018167944 -0.001816529 -0.001816516 -0.001816563 -0.0018166641 -0.0018168031 -0.0018169404 -0.0018169991 -0.0018169548 -0.0018168732 -0.0018167545 -0.001816658 -0.0018166178 -0.0018166405][-0.0018176113 -0.0018166685 -0.0018164578 -0.0018165667 -0.0018167597 -0.0018169675 -0.0018171709 -0.001817313 -0.0018172853 -0.0018170974 -0.0018168578 -0.0018166139 -0.0018164637 -0.0018164425 -0.0018165304][-0.0018176289 -0.0018167035 -0.0018165623 -0.0018167862 -0.0018171152 -0.0018173966 -0.0018176154 -0.0018177087 -0.0018175798 -0.0018172399 -0.0018168424 -0.0018164937 -0.0018163066 -0.0018163129 -0.0018164675][-0.0018177114 -0.0018168619 -0.0018167525 -0.0018170073 -0.0018173886 -0.0018177122 -0.0018179346 -0.0018179887 -0.0018177729 -0.0018173172 -0.0018168048 -0.0018163864 -0.001816167 -0.0018161866 -0.0018163924][-0.0018178298 -0.0018170361 -0.0018169094 -0.001817076 -0.0018173677 -0.0018176754 -0.0018178826 -0.0018178986 -0.0018176482 -0.0018171704 -0.0018166488 -0.0018162293 -0.0018160172 -0.0018160659 -0.0018163179][-0.0018183953 -0.00181763 -0.0018174334 -0.001817384 -0.001817422 -0.0018175623 -0.0018176369 -0.0018175598 -0.0018172993 -0.0018168974 -0.0018164821 -0.0018161428 -0.0018159843 -0.0018160725 -0.0018163398][-0.0018201149 -0.0018194318 -0.0018191765 -0.0018189156 -0.0018186172 -0.0018184123 -0.0018181865 -0.0018179129 -0.0018176099 -0.0018172628 -0.0018169284 -0.0018166185 -0.0018164171 -0.0018164143 -0.0018165602][-0.0018230764 -0.0018227026 -0.0018225068 -0.0018220764 -0.0018213947 -0.0018207502 -0.0018201094 -0.0018195589 -0.0018191012 -0.0018186325 -0.0018181705 -0.0018176956 -0.0018172842 -0.0018170278 -0.0018169197][-0.0018262567 -0.001826398 -0.001826385 -0.0018259215 -0.0018249961 -0.0018240145 -0.0018230653 -0.001822223 -0.0018214516 -0.0018206225 -0.0018197999 -0.0018190196 -0.0018183491 -0.0018178122 -0.0018174406][-0.0018289146 -0.0018294968 -0.0018296677 -0.0018292315 -0.0018281464 -0.0018268987 -0.0018256856 -0.0018245453 -0.0018234233 -0.0018222486 -0.0018211133 -0.0018200947 -0.001819232 -0.0018185345 -0.0018180237][-0.00183084 -0.0018317257 -0.0018320329 -0.0018316723 -0.0018305745 -0.0018291692 -0.0018277868 -0.001826447 -0.0018250406 -0.0018235882 -0.0018222096 -0.0018210021 -0.0018199764 -0.0018191535 -0.0018185395][-0.0018314053 -0.0018323915 -0.0018327885 -0.0018325632 -0.0018315772 -0.0018301973 -0.0018287849 -0.0018273297 -0.001825756 -0.0018241395 -0.001822615 -0.0018213057 -0.0018202204 -0.0018193806 -0.0018187521][-0.0018298482 -0.0018307068 -0.0018310716 -0.0018309725 -0.0018302092 -0.0018290443 -0.0018278138 -0.0018264801 -0.0018250189 -0.0018235344 -0.0018221195 -0.0018209134 -0.0018199504 -0.0018192298 -0.0018186811][-0.0018267686 -0.0018272583 -0.0018274679 -0.0018274595 -0.0018269687 -0.0018261471 -0.0018252472 -0.0018242416 -0.0018231401 -0.0018220451 -0.0018209901 -0.0018200905 -0.0018193976 -0.0018188949 -0.0018184761]]...]
INFO - root - 2017-12-09 16:36:52.000794: step 39710, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.883 sec/batch; 71h:48m:14s remains)
INFO - root - 2017-12-09 16:37:00.619615: step 39720, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.836 sec/batch; 67h:58m:19s remains)
INFO - root - 2017-12-09 16:37:09.238162: step 39730, loss = 0.83, batch loss = 0.70 (8.9 examples/sec; 0.901 sec/batch; 73h:17m:18s remains)
INFO - root - 2017-12-09 16:37:17.841777: step 39740, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.861 sec/batch; 70h:02m:24s remains)
INFO - root - 2017-12-09 16:37:26.346704: step 39750, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.850 sec/batch; 69h:07m:57s remains)
INFO - root - 2017-12-09 16:37:34.794117: step 39760, loss = 0.82, batch loss = 0.69 (9.8 examples/sec; 0.816 sec/batch; 66h:22m:36s remains)
INFO - root - 2017-12-09 16:37:43.295897: step 39770, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.843 sec/batch; 68h:31m:07s remains)
INFO - root - 2017-12-09 16:37:51.922558: step 39780, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.882 sec/batch; 71h:44m:01s remains)
INFO - root - 2017-12-09 16:38:00.714015: step 39790, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.872 sec/batch; 70h:53m:13s remains)
INFO - root - 2017-12-09 16:38:09.371863: step 39800, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.857 sec/batch; 69h:39m:19s remains)
2017-12-09 16:38:10.255356: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.001831715 -0.0018309982 -0.0018294557 -0.0018301088 -0.0018300435 -0.0018277122 -0.001822655 -0.0018266338 -0.0018290476 -0.0018298875 -0.001830339 -0.0018304832 -0.001830281 -0.0018074078 -0.0017428967][-0.0018276813 -0.0018265629 -0.0018219125 -0.0018234936 -0.0018202284 -0.0018145366 -0.0017989152 -0.0018069574 -0.0018153797 -0.0018252216 -0.0018295855 -0.0018311328 -0.0018293292 -0.0017761954 -0.0016393116][-0.0018188979 -0.0018168987 -0.0018087204 -0.0018038929 -0.0017851107 -0.0017673839 -0.0017380809 -0.0017424414 -0.001765994 -0.0017968226 -0.0018106154 -0.001824343 -0.0018207959 -0.0017038754 -0.0014341094][-0.0018114841 -0.0018012966 -0.0017892119 -0.001766242 -0.0017054426 -0.0016381221 -0.0015768944 -0.0015883602 -0.0016460065 -0.0017061087 -0.0017367559 -0.0017828614 -0.0017795721 -0.0015764652 -0.0011322103][-0.0018095417 -0.0017914705 -0.0017705115 -0.0017074228 -0.0015633395 -0.0013869889 -0.0012560294 -0.0012620669 -0.0013634848 -0.0014690417 -0.0015304707 -0.0016153522 -0.0016167819 -0.0013559946 -0.0007688445][-0.0018141916 -0.0017908739 -0.0017584423 -0.001645576 -0.0013935214 -0.0010498752 -0.00077761023 -0.00072370842 -0.00082983496 -0.00095254171 -0.00104251 -0.0011623127 -0.0011887655 -0.00094223773 -0.00033582549][-0.0018219881 -0.0017992565 -0.0017561724 -0.0016060051 -0.0012616629 -0.00074424373 -0.000281799 -6.8448018e-05 -6.2067644e-05 -8.4680738e-05 -0.00012472866 -0.00023150968 -0.00030953286 -0.00017661066 0.00025671732][-0.0018282874 -0.0018110407 -0.0017657669 -0.0016077938 -0.0012365343 -0.00062178308 4.3741893e-06 0.0004444184 0.00069794909 0.00095294963 0.0011349282 0.0011907137 0.0011046791 0.0010640699 0.0011539756][-0.0018304917 -0.0018192679 -0.0017811331 -0.0016493228 -0.0013272354 -0.00073992019 -6.3810265e-05 0.00056811457 0.0011250492 0.0017892652 0.0023787622 0.0027728868 0.0028096796 0.0026710741 0.0024082493][-0.001830584 -0.0018241305 -0.0017993941 -0.0017114363 -0.0014865089 -0.0010290088 -0.00043897715 0.00025590917 0.0010399766 0.0020871945 0.0031524422 0.00400438 0.0043372326 0.0042617228 0.0038184631][-0.0018306101 -0.001827193 -0.0018123012 -0.0017674997 -0.0016379675 -0.0013489039 -0.00092627085 -0.00031478459 0.00052909192 0.0017803059 0.0032000542 0.0044873469 0.0051933541 0.0053666574 0.0050050812][-0.0018306234 -0.0018294151 -0.00182066 -0.0018022021 -0.0017395435 -0.0015902484 -0.0013406019 -0.000901006 -0.00016446062 0.0010567083 0.002585859 0.0041370881 0.0051968941 0.0057122256 0.0056547006][-0.0018290916 -0.0018288691 -0.0018232574 -0.0018169169 -0.0017899462 -0.0017290901 -0.0016068126 -0.0013463969 -0.00081390841 0.00018411444 0.0015735341 0.0031581516 0.0044416934 0.0053038048 0.0056453994][-0.0018276407 -0.0018273189 -0.001823507 -0.0018200049 -0.0018092442 -0.0017915282 -0.0017435945 -0.0016175818 -0.0012973598 -0.0006085179 0.00046773057 0.0018663727 0.0032021021 0.0043313173 0.0050615091][-0.0018264945 -0.0018260306 -0.0018244961 -0.0018217221 -0.0018177829 -0.0018155318 -0.0018027019 -0.0017564471 -0.0016019491 -0.0012070378 -0.00049974816 0.00056562305 0.0017893336 0.0030469866 0.0040886779]]...]
INFO - root - 2017-12-09 16:38:18.869206: step 39810, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.839 sec/batch; 68h:12m:38s remains)
INFO - root - 2017-12-09 16:38:27.670822: step 39820, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.879 sec/batch; 71h:26m:26s remains)
INFO - root - 2017-12-09 16:38:36.571611: step 39830, loss = 0.82, batch loss = 0.69 (8.1 examples/sec; 0.983 sec/batch; 79h:52m:52s remains)
INFO - root - 2017-12-09 16:38:45.201513: step 39840, loss = 0.82, batch loss = 0.69 (10.8 examples/sec; 0.738 sec/batch; 60h:00m:00s remains)
INFO - root - 2017-12-09 16:38:53.566499: step 39850, loss = 0.82, batch loss = 0.69 (9.7 examples/sec; 0.824 sec/batch; 67h:00m:23s remains)
INFO - root - 2017-12-09 16:39:02.116080: step 39860, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.854 sec/batch; 69h:26m:18s remains)
INFO - root - 2017-12-09 16:39:10.667075: step 39870, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.877 sec/batch; 71h:17m:47s remains)
INFO - root - 2017-12-09 16:39:19.270975: step 39880, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 70h:26m:42s remains)
INFO - root - 2017-12-09 16:39:27.927634: step 39890, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.880 sec/batch; 71h:30m:03s remains)
INFO - root - 2017-12-09 16:39:36.819910: step 39900, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.844 sec/batch; 68h:38m:02s remains)
2017-12-09 16:39:37.659662: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0017785539 -0.0017769996 -0.0017773883 -0.0017777571 -0.0017778589 -0.0017773869 -0.0017761901 -0.001774763 -0.0017735199 -0.0017726447 -0.0017722623 -0.0017726051 -0.0017733384 -0.0017743461 -0.001775292][-0.0017773589 -0.0017759146 -0.0017764175 -0.0017768703 -0.0017770212 -0.0017766012 -0.0017754978 -0.0017741299 -0.0017729929 -0.0017721064 -0.0017715276 -0.0017715996 -0.0017721191 -0.0017728851 -0.001773699][-0.0017777462 -0.0017763749 -0.001776789 -0.0017772104 -0.0017773246 -0.0017768791 -0.0017758282 -0.0017745669 -0.0017736147 -0.0017728247 -0.0017721431 -0.0017719214 -0.0017720929 -0.0017725764 -0.0017732215][-0.0017788348 -0.0017775189 -0.0017779297 -0.0017784145 -0.001778473 -0.0017779121 -0.0017767771 -0.0017755949 -0.0017748241 -0.0017742186 -0.0017735787 -0.0017732099 -0.0017730765 -0.0017731751 -0.0017734761][-0.0017802747 -0.0017786833 -0.001778877 -0.0017793082 -0.0017792855 -0.001778592 -0.0017775124 -0.0017766776 -0.0017764384 -0.0017762627 -0.0017758476 -0.0017754275 -0.0017750012 -0.001774566 -0.0017743114][-0.0017813111 -0.001779316 -0.0017791567 -0.0017793479 -0.0017790506 -0.0017781325 -0.0017770588 -0.0017766197 -0.0017770602 -0.0017774508 -0.0017774483 -0.0017771289 -0.001776609 -0.0017758042 -0.001775087][-0.0017817686 -0.0017795976 -0.001779126 -0.0017789723 -0.0017782298 -0.0017768565 -0.0017754716 -0.0017751845 -0.0017760013 -0.0017768207 -0.0017772232 -0.0017772483 -0.0017769954 -0.0017762273 -0.0017753664][-0.0017813983 -0.0017790946 -0.0017784609 -0.0017780225 -0.0017768497 -0.0017751156 -0.0017734629 -0.0017730696 -0.0017739483 -0.0017749211 -0.0017756091 -0.0017760682 -0.00177631 -0.0017759331 -0.0017752502][-0.0017803205 -0.001778057 -0.0017774672 -0.0017769184 -0.0017756014 -0.0017738437 -0.0017721677 -0.0017715137 -0.0017720881 -0.0017729313 -0.0017737844 -0.0017746147 -0.0017753119 -0.0017753725 -0.0017749872][-0.0017793404 -0.0017772587 -0.0017766447 -0.0017759097 -0.0017745382 -0.0017729866 -0.0017714808 -0.0017706241 -0.0017707922 -0.0017713635 -0.0017722108 -0.0017732448 -0.0017742625 -0.0017746709 -0.0017746346][-0.0017773274 -0.0017753267 -0.0017745906 -0.0017738716 -0.0017726833 -0.0017713662 -0.0017700196 -0.0017691113 -0.0017689901 -0.0017692449 -0.0017699896 -0.0017712337 -0.0017725893 -0.0017734771 -0.001773985][-0.0017755216 -0.0017732984 -0.001772517 -0.0017719506 -0.001771025 -0.0017699916 -0.0017689593 -0.0017681951 -0.0017679442 -0.0017679881 -0.0017685968 -0.0017698674 -0.0017713328 -0.0017725221 -0.0017734362][-0.0017746434 -0.0017722766 -0.0017715173 -0.0017711727 -0.0017705144 -0.0017697717 -0.0017690766 -0.0017685196 -0.0017682263 -0.0017681529 -0.0017685859 -0.0017696747 -0.0017710081 -0.0017721787 -0.0017731661][-0.0017745144 -0.0017720487 -0.0017713446 -0.0017711759 -0.0017707705 -0.0017703164 -0.0017698592 -0.0017694354 -0.0017691359 -0.0017690101 -0.0017693115 -0.0017701505 -0.0017712195 -0.0017722041 -0.0017731234][-0.0017747887 -0.0017722697 -0.00177156 -0.0017715197 -0.001771335 -0.0017710757 -0.0017707888 -0.0017705134 -0.0017702839 -0.001770151 -0.0017703163 -0.0017708943 -0.0017716521 -0.0017723962 -0.0017731375]]...]
INFO - root - 2017-12-09 16:39:46.419138: step 39910, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.887 sec/batch; 72h:06m:16s remains)
INFO - root - 2017-12-09 16:39:55.138717: step 39920, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.873 sec/batch; 70h:57m:12s remains)
INFO - root - 2017-12-09 16:40:03.856036: step 39930, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.848 sec/batch; 68h:56m:30s remains)
INFO - root - 2017-12-09 16:40:12.638320: step 39940, loss = 0.81, batch loss = 0.68 (8.8 examples/sec; 0.913 sec/batch; 74h:10m:19s remains)
INFO - root - 2017-12-09 16:40:20.891396: step 39950, loss = 0.82, batch loss = 0.69 (11.9 examples/sec; 0.675 sec/batch; 54h:50m:41s remains)
INFO - root - 2017-12-09 16:40:29.570133: step 39960, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.889 sec/batch; 72h:14m:05s remains)
INFO - root - 2017-12-09 16:40:38.103691: step 39970, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.856 sec/batch; 69h:32m:14s remains)
INFO - root - 2017-12-09 16:40:46.773134: step 39980, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.876 sec/batch; 71h:09m:36s remains)
INFO - root - 2017-12-09 16:40:55.411187: step 39990, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.855 sec/batch; 69h:29m:32s remains)
INFO - root - 2017-12-09 16:41:03.961554: step 40000, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.887 sec/batch; 72h:02m:40s remains)
2017-12-09 16:41:04.868411: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0018334838 -0.0018334347 -0.001833169 -0.001832569 -0.001832189 -0.0018328885 -0.0018341849 -0.0018348829 -0.0018346371 -0.0018340972 -0.0018336945 -0.0018332456 -0.0018326252 -0.001831861 -0.0018313747][-0.001833448 -0.0018333201 -0.001833107 -0.0018326907 -0.0018320458 -0.0018322362 -0.0018336959 -0.0018350802 -0.0018351594 -0.0018344221 -0.0018337521 -0.001833257 -0.0018326618 -0.0018318783 -0.0018313068][-0.0018334582 -0.0018332213 -0.0018331306 -0.001833083 -0.0018326123 -0.001832185 -0.0018332333 -0.001835188 -0.0018359479 -0.0018353176 -0.0018343014 -0.0018335824 -0.001832901 -0.0018320037 -0.0018312788][-0.0018334211 -0.00183301 -0.0018329396 -0.0018332108 -0.0018332328 -0.0018325873 -0.0018327287 -0.0018347292 -0.0018363962 -0.0018362995 -0.0018351193 -0.0018340198 -0.0018331194 -0.0018320516 -0.0018311425][-0.0018332945 -0.0018327526 -0.0018326388 -0.0018330517 -0.0018335219 -0.0018331686 -0.0018324696 -0.0018337774 -0.0018361388 -0.0018369686 -0.0018360144 -0.0018346281 -0.0018334409 -0.0018323021 -0.0018312668][-0.0018331329 -0.0018325425 -0.0018324031 -0.0018328005 -0.0018334647 -0.0018335717 -0.0018326971 -0.0018328361 -0.0018351895 -0.0018370884 -0.0018367956 -0.001835393 -0.0018340017 -0.0018328236 -0.0018317185][-0.0018330322 -0.0018324867 -0.0018323726 -0.0018327226 -0.0018333617 -0.0018338282 -0.001833302 -0.0018325148 -0.0018339709 -0.0018364802 -0.0018372178 -0.0018361626 -0.0018347303 -0.0018335058 -0.0018323963][-0.0018329023 -0.0018324498 -0.0018324067 -0.0018327516 -0.001833307 -0.0018339255 -0.001833966 -0.0018330946 -0.0018332099 -0.0018354491 -0.001837206 -0.001836814 -0.0018354926 -0.0018342246 -0.001833147][-0.0018325907 -0.0018322931 -0.001832353 -0.0018327226 -0.0018332081 -0.0018338282 -0.0018343867 -0.0018340676 -0.0018333864 -0.0018345597 -0.0018366715 -0.0018371671 -0.0018360693 -0.0018347488 -0.0018336701][-0.001832234 -0.0018321087 -0.0018322946 -0.0018326766 -0.0018331282 -0.0018337317 -0.0018345506 -0.0018349168 -0.0018342517 -0.0018342099 -0.0018358452 -0.0018369961 -0.0018362819 -0.0018349581 -0.0018338722][-0.0018320134 -0.0018320099 -0.0018323318 -0.0018327709 -0.0018331592 -0.0018336785 -0.0018345434 -0.0018353772 -0.0018351539 -0.0018344332 -0.0018351078 -0.0018363205 -0.0018361511 -0.0018350297 -0.0018340201][-0.0018317592 -0.0018318405 -0.0018322816 -0.001832836 -0.0018332029 -0.0018336151 -0.0018343721 -0.0018352802 -0.0018355396 -0.0018347793 -0.0018345038 -0.0018352576 -0.0018355822 -0.0018349736 -0.0018341864][-0.0018313864 -0.0018315396 -0.0018320003 -0.0018326235 -0.0018330846 -0.0018334426 -0.0018340135 -0.0018347383 -0.001835209 -0.0018347395 -0.0018339317 -0.001834098 -0.0018345376 -0.0018345608 -0.0018341924][-0.0018311352 -0.0018313158 -0.0018316926 -0.0018322904 -0.0018328379 -0.0018331601 -0.0018335136 -0.0018339464 -0.0018342811 -0.0018340485 -0.001833231 -0.0018329577 -0.0018332698 -0.0018336651 -0.0018337723][-0.001830999 -0.0018312294 -0.0018315226 -0.0018319883 -0.0018325241 -0.0018328145 -0.0018329982 -0.0018331285 -0.0018331254 -0.0018329635 -0.0018324648 -0.0018321003 -0.001832196 -0.0018326087 -0.0018330684]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-momentum-lr0.0001-clip-zeroinit-conv1-3init-from-scratch/model.ckpt-40000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-momentum-lr0.0001-clip-zeroinit-conv1-3init-from-scratch/model.ckpt-40000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-09 16:41:14.127865: step 40010, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.896 sec/batch; 72h:48m:22s remains)
INFO - root - 2017-12-09 16:41:22.872102: step 40020, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.850 sec/batch; 69h:03m:25s remains)
INFO - root - 2017-12-09 16:41:31.575847: step 40030, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.883 sec/batch; 71h:45m:54s remains)
INFO - root - 2017-12-09 16:41:40.186915: step 40040, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.840 sec/batch; 68h:15m:26s remains)
INFO - root - 2017-12-09 16:41:48.643398: step 40050, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.855 sec/batch; 69h:26m:53s remains)
INFO - root - 2017-12-09 16:41:57.362554: step 40060, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.889 sec/batch; 72h:12m:03s remains)
INFO - root - 2017-12-09 16:42:05.935476: step 40070, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.865 sec/batch; 70h:16m:34s remains)
INFO - root - 2017-12-09 16:42:14.722572: step 40080, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.872 sec/batch; 70h:51m:28s remains)
INFO - root - 2017-12-09 16:42:23.445961: step 40090, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 70h:45m:19s remains)
INFO - root - 2017-12-09 16:42:32.271098: step 40100, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.884 sec/batch; 71h:47m:20s remains)
2017-12-09 16:42:33.101188: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0018094074 -0.001807965 -0.0018076885 -0.0018081738 -0.0018084813 -0.0018092317 -0.0018096477 -0.001810106 -0.0018104515 -0.0018102339 -0.0018098246 -0.0018092819 -0.0018088121 -0.0018082608 -0.0018077536][-0.0018087478 -0.0018071209 -0.0018068461 -0.0018072282 -0.0018074259 -0.0018080038 -0.0018083578 -0.0018088411 -0.001809174 -0.0018091373 -0.0018088591 -0.0018084698 -0.0018081053 -0.0018076466 -0.0018071561][-0.0018087167 -0.0018072387 -0.0018071035 -0.0018075934 -0.0018079424 -0.0018083821 -0.001808568 -0.0018087482 -0.0018087606 -0.0018086304 -0.0018083442 -0.0018081325 -0.0018079163 -0.0018077637 -0.0018075391][-0.0018087308 -0.0018075759 -0.001807731 -0.0018085286 -0.0018091236 -0.0018095482 -0.0018096491 -0.0018095584 -0.0018092316 -0.0018088056 -0.0018083164 -0.0018080561 -0.0018078964 -0.0018079013 -0.0018078767][-0.0018087943 -0.0018078756 -0.0018083144 -0.0018094036 -0.001810311 -0.0018108522 -0.0018109479 -0.0018106847 -0.0018100983 -0.0018093506 -0.0018085045 -0.0018080187 -0.00180773 -0.0018076566 -0.0018076687][-0.0018089318 -0.0018081641 -0.0018087433 -0.0018099087 -0.0018108963 -0.0018115622 -0.0018117272 -0.0018115164 -0.0018109293 -0.0018100205 -0.0018089617 -0.0018082685 -0.0018077588 -0.00180749 -0.0018074062][-0.0018097769 -0.0018089628 -0.0018093535 -0.0018102591 -0.0018111125 -0.0018117301 -0.0018118953 -0.0018117011 -0.0018111155 -0.0018101555 -0.0018090967 -0.0018082715 -0.0018076543 -0.0018072805 -0.0018070849][-0.0018113429 -0.001810241 -0.0018103012 -0.0018107457 -0.0018111732 -0.0018115063 -0.0018114926 -0.0018112169 -0.0018106896 -0.0018098496 -0.0018088849 -0.0018080404 -0.0018073994 -0.0018069753 -0.001806685][-0.0018137025 -0.0018120959 -0.0018115747 -0.0018112561 -0.0018109808 -0.0018107082 -0.0018102671 -0.0018098089 -0.0018093947 -0.0018088254 -0.0018081961 -0.0018075991 -0.0018070847 -0.0018066714 -0.0018063267][-0.0018163593 -0.001814271 -0.0018131013 -0.0018119245 -0.0018109466 -0.0018100379 -0.0018091034 -0.0018084315 -0.0018081189 -0.0018077836 -0.0018074985 -0.0018071411 -0.0018067923 -0.001806459 -0.001806137][-0.0018183519 -0.0018163488 -0.0018151078 -0.001813359 -0.0018116948 -0.0018100764 -0.0018087449 -0.0018078969 -0.0018075835 -0.0018073788 -0.0018073037 -0.0018070727 -0.0018068195 -0.0018066153 -0.0018063848][-0.0018178571 -0.0018166831 -0.0018163025 -0.0018148855 -0.0018131457 -0.0018112423 -0.001809491 -0.0018083609 -0.0018078116 -0.0018075161 -0.0018074509 -0.0018072138 -0.001806995 -0.0018068095 -0.0018066209][-0.0018140494 -0.0018144038 -0.0018156614 -0.0018155121 -0.0018146369 -0.001812972 -0.0018110354 -0.0018095407 -0.0018086017 -0.0018079553 -0.0018076584 -0.0018072797 -0.0018070327 -0.0018068459 -0.001806697][-0.0018068935 -0.0018093138 -0.001812962 -0.0018149422 -0.0018151406 -0.0018139904 -0.0018121853 -0.0018102875 -0.0018089747 -0.0018079978 -0.0018077111 -0.0018071674 -0.001806881 -0.0018066873 -0.0018065633][-0.0017975763 -0.0018020193 -0.0018083142 -0.0018129621 -0.0018150974 -0.0018150159 -0.0018133249 -0.0018113855 -0.0018095438 -0.0018080294 -0.0018075621 -0.0018068587 -0.0018066387 -0.00180637 -0.0018062763]]...]
INFO - root - 2017-12-09 16:42:41.765997: step 40110, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.890 sec/batch; 72h:17m:42s remains)
INFO - root - 2017-12-09 16:42:50.528660: step 40120, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.895 sec/batch; 72h:39m:33s remains)
INFO - root - 2017-12-09 16:42:59.372394: step 40130, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 70h:23m:24s remains)
INFO - root - 2017-12-09 16:43:08.068836: step 40140, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.892 sec/batch; 72h:26m:48s remains)
INFO - root - 2017-12-09 16:43:16.510638: step 40150, loss = 0.83, batch loss = 0.70 (9.5 examples/sec; 0.841 sec/batch; 68h:16m:14s remains)
INFO - root - 2017-12-09 16:43:25.208805: step 40160, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.865 sec/batch; 70h:12m:22s remains)
INFO - root - 2017-12-09 16:43:33.731794: step 40170, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.852 sec/batch; 69h:11m:26s remains)
INFO - root - 2017-12-09 16:43:42.372050: step 40180, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.886 sec/batch; 71h:54m:43s remains)
INFO - root - 2017-12-09 16:43:51.165036: step 40190, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.856 sec/batch; 69h:32m:07s remains)
INFO - root - 2017-12-09 16:43:59.776352: step 40200, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 70h:35m:22s remains)
2017-12-09 16:44:00.701625: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.0875672e-06 8.0168247e-05 0.00019429158 0.00030072988 0.00041537988 0.00068079191 0.0011962212 0.0019518822 0.0026887732 0.0032168038 0.0033492961 0.0032902628 0.0031364572 0.0030532584 0.0030006443][0.00028950651 0.00049939239 0.00071856193 0.00092856004 0.0012198878 0.0018154583 0.002839885 0.0041810861 0.0054448005 0.0062618041 0.00640253 0.0060665756 0.0054742675 0.0049473951 0.0046766354][0.00031128922 0.00054014311 0.00087703275 0.0013372612 0.0021569191 0.0036384242 0.005811505 0.0082336748 0.010198575 0.011282087 0.011401709 0.010937086 0.010150199 0.0093064681 0.0086211339][0.00060634362 0.00074518658 0.0011565492 0.0019564566 0.0035563626 0.0062966654 0.010101843 0.014130387 0.017168541 0.01855961 0.018402606 0.01752124 0.0165157 0.01559664 0.014697515][0.0017130598 0.0017874511 0.0021458149 0.003234667 0.0057101203 0.0098924972 0.015532845 0.021400919 0.025849456 0.027840549 0.027427765 0.0257681 0.024043227 0.022751387 0.021690376][0.0024066281 0.0028230243 0.00359413 0.0053204326 0.0088030621 0.014405477 0.021709954 0.029192321 0.0349157 0.037553906 0.037165269 0.035017051 0.032611426 0.030723196 0.029234154][0.0028173279 0.0032800029 0.0042563053 0.0066643916 0.011370106 0.018644854 0.027711166 0.036702644 0.043532658 0.046805866 0.046607397 0.044257697 0.041531168 0.039332546 0.037523393][0.0030973561 0.003669142 0.0047192406 0.0074160425 0.012848848 0.021367986 0.031960584 0.04243264 0.050447609 0.054558177 0.054874636 0.052686051 0.049849771 0.047422536 0.04536882][0.0026242614 0.0033947586 0.0046059163 0.0074129868 0.012955729 0.021733718 0.032907959 0.044310991 0.053580604 0.059116989 0.060832724 0.059639074 0.057146154 0.054502692 0.051996797][0.0016383508 0.0025213968 0.0038680939 0.0066704452 0.011986567 0.020355092 0.031133208 0.042496581 0.052399073 0.059347149 0.062892005 0.063334152 0.061776355 0.059328068 0.05660814][0.00054607145 0.0013727774 0.0026607269 0.0052267723 0.010013209 0.017559839 0.027422855 0.038142327 0.048066925 0.055895768 0.060974456 0.063022442 0.062589251 0.060680144 0.058298007][-0.00031371566 0.00035973685 0.0014631266 0.0036114145 0.0076178052 0.014052999 0.022709506 0.032487378 0.042064879 0.050273322 0.056268219 0.059429675 0.0599991 0.058883775 0.057254083][-0.00048200297 -3.5626115e-05 0.00074880244 0.0023234992 0.0053888652 0.010554899 0.017826369 0.026437458 0.035297241 0.043337051 0.049678117 0.053587578 0.055058084 0.054828148 0.054089434][-0.00029825466 1.4619203e-05 0.00054342393 0.001554149 0.0036189074 0.0073951688 0.013131557 0.020382212 0.028181517 0.035549067 0.041639708 0.045873154 0.048182759 0.04902928 0.049257364][-6.8859779e-05 0.00017191819 0.00050474866 0.0010947031 0.0023483138 0.0048642168 0.0090129487 0.014644497 0.020993227 0.027213272 0.032563861 0.036664709 0.039454706 0.041141611 0.0421571]]...]
INFO - root - 2017-12-09 16:44:09.422220: step 40210, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.838 sec/batch; 68h:00m:12s remains)
INFO - root - 2017-12-09 16:44:18.117848: step 40220, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.855 sec/batch; 69h:25m:37s remains)
INFO - root - 2017-12-09 16:44:26.799721: step 40230, loss = 0.83, batch loss = 0.70 (9.2 examples/sec; 0.867 sec/batch; 70h:20m:55s remains)
INFO - root - 2017-12-09 16:44:35.600687: step 40240, loss = 0.82, batch loss = 0.69 (8.1 examples/sec; 0.986 sec/batch; 80h:03m:16s remains)
INFO - root - 2017-12-09 16:44:44.125810: step 40250, loss = 0.82, batch loss = 0.69 (11.0 examples/sec; 0.730 sec/batch; 59h:15m:53s remains)
INFO - root - 2017-12-09 16:44:52.934494: step 40260, loss = 0.82, batch loss = 0.69 (8.7 examples/sec; 0.916 sec/batch; 74h:23m:14s remains)
INFO - root - 2017-12-09 16:45:01.555442: step 40270, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.889 sec/batch; 72h:11m:54s remains)
INFO - root - 2017-12-09 16:45:10.266467: step 40280, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.866 sec/batch; 70h:17m:51s remains)
INFO - root - 2017-12-09 16:45:19.033648: step 40290, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.897 sec/batch; 72h:50m:48s remains)
INFO - root - 2017-12-09 16:45:27.844305: step 40300, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.888 sec/batch; 72h:05m:52s remains)
2017-12-09 16:45:28.725473: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0017910642 -0.0017890088 -0.0017885723 -0.0017881857 -0.0017876918 -0.0017873602 -0.0017872101 -0.0017872914 -0.0017875271 -0.0017879095 -0.0017882986 -0.0017885689 -0.0017886324 -0.0017885581 -0.0017884857][-0.0017904384 -0.0017884683 -0.0017881534 -0.001787845 -0.0017874113 -0.0017872322 -0.0017871764 -0.0017872043 -0.0017873467 -0.0017876509 -0.0017879836 -0.0017881802 -0.0017881939 -0.0017880464 -0.0017879386][-0.0017909331 -0.0017890926 -0.0017889226 -0.0017887906 -0.0017884881 -0.0017884854 -0.0017885214 -0.0017884813 -0.0017884766 -0.0017885681 -0.0017887048 -0.0017887629 -0.0017886988 -0.0017885305 -0.0017883989][-0.0017913525 -0.0017896934 -0.0017897375 -0.0017899119 -0.0017898066 -0.0017899233 -0.0017900588 -0.0017899728 -0.001789791 -0.0017895744 -0.0017894295 -0.0017892922 -0.0017891532 -0.0017889981 -0.0017888708][-0.0017916287 -0.0017900808 -0.0017903103 -0.0017907934 -0.0017908576 -0.0017909287 -0.0017910618 -0.0017909888 -0.0017906879 -0.0017901869 -0.0017897546 -0.0017894442 -0.0017892866 -0.0017891751 -0.0017890705][-0.0017916948 -0.0017902296 -0.0017906392 -0.0017914226 -0.001791677 -0.0017915966 -0.0017916909 -0.0017916798 -0.0017913142 -0.0017906311 -0.0017899597 -0.0017894646 -0.0017892853 -0.0017892265 -0.0017891872][-0.0017916631 -0.0017902609 -0.001790785 -0.0017917953 -0.0017922501 -0.0017920948 -0.0017919688 -0.0017918446 -0.0017914408 -0.0017906502 -0.0017897997 -0.0017891782 -0.001789035 -0.0017891087 -0.001789209][-0.0017916387 -0.0017902363 -0.0017907453 -0.0017918389 -0.0017924719 -0.001792339 -0.0017918651 -0.0017913436 -0.0017908537 -0.001790022 -0.0017890933 -0.0017884488 -0.0017883971 -0.0017887083 -0.0017890288][-0.001791492 -0.0017900734 -0.0017905377 -0.0017915533 -0.0017922524 -0.0017921778 -0.0017914714 -0.0017905543 -0.0017898659 -0.0017890272 -0.0017881234 -0.0017875578 -0.0017875982 -0.0017881 -0.0017886757][-0.0017911777 -0.0017897846 -0.0017901083 -0.0017908908 -0.0017915218 -0.0017915663 -0.0017909028 -0.0017898363 -0.0017889277 -0.0017880682 -0.0017872486 -0.0017867622 -0.0017868556 -0.0017874702 -0.0017882665][-0.0017908883 -0.001789461 -0.0017895501 -0.0017900772 -0.0017905992 -0.0017907892 -0.0017903442 -0.0017893229 -0.0017882653 -0.001787418 -0.0017866925 -0.0017863052 -0.0017864265 -0.0017870847 -0.0017880116][-0.0017906 -0.0017890794 -0.0017889851 -0.0017893026 -0.0017896564 -0.001789888 -0.0017896569 -0.0017888325 -0.0017878505 -0.0017870903 -0.0017865327 -0.0017862808 -0.0017864468 -0.0017870563 -0.0017879505][-0.0017904211 -0.0017888274 -0.0017885849 -0.0017887283 -0.0017889214 -0.001789076 -0.0017889953 -0.0017884684 -0.0017877568 -0.0017871807 -0.0017867826 -0.0017866318 -0.0017867693 -0.0017872615 -0.0017880007][-0.00179031 -0.001788657 -0.0017883454 -0.0017883836 -0.0017884539 -0.0017885489 -0.0017885582 -0.0017883163 -0.0017879033 -0.0017875163 -0.0017872364 -0.0017871433 -0.0017872562 -0.001787619 -0.0017881498][-0.0017902101 -0.0017885959 -0.0017882254 -0.0017882216 -0.0017882155 -0.0017882666 -0.0017883271 -0.001788287 -0.0017881105 -0.001787905 -0.0017877502 -0.0017877137 -0.0017877994 -0.0017880094 -0.001788305]]...]
INFO - root - 2017-12-09 16:45:37.365215: step 40310, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.842 sec/batch; 68h:22m:11s remains)
INFO - root - 2017-12-09 16:45:46.024137: step 40320, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.865 sec/batch; 70h:12m:34s remains)
INFO - root - 2017-12-09 16:45:54.689829: step 40330, loss = 0.83, batch loss = 0.70 (9.3 examples/sec; 0.857 sec/batch; 69h:33m:33s remains)
INFO - root - 2017-12-09 16:46:03.369533: step 40340, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.843 sec/batch; 68h:27m:13s remains)
INFO - root - 2017-12-09 16:46:11.735402: step 40350, loss = 0.82, batch loss = 0.69 (11.2 examples/sec; 0.715 sec/batch; 58h:03m:33s remains)
INFO - root - 2017-12-09 16:46:20.286549: step 40360, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.852 sec/batch; 69h:09m:34s remains)
INFO - root - 2017-12-09 16:46:28.731963: step 40370, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.850 sec/batch; 68h:58m:38s remains)
INFO - root - 2017-12-09 16:46:37.349383: step 40380, loss = 0.83, batch loss = 0.70 (9.2 examples/sec; 0.872 sec/batch; 70h:46m:01s remains)
INFO - root - 2017-12-09 16:46:46.089437: step 40390, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.886 sec/batch; 71h:54m:49s remains)
INFO - root - 2017-12-09 16:46:54.849326: step 40400, loss = 0.83, batch loss = 0.70 (9.2 examples/sec; 0.874 sec/batch; 70h:55m:09s remains)
2017-12-09 16:46:55.813572: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0018024225 -0.0018015566 -0.0018016843 -0.0018015129 -0.0018012146 -0.0018010127 -0.001800879 -0.0018006766 -0.001800363 -0.0018000734 -0.0017998019 -0.0017996173 -0.001799494 -0.0017994465 -0.0017994901][-0.0018086743 -0.0018085961 -0.0018091013 -0.0018091596 -0.0018089975 -0.0018088745 -0.0018086348 -0.0018082357 -0.0018077142 -0.0018070798 -0.0018064621 -0.0018059202 -0.0018055519 -0.0018053009 -0.0018051517][-0.0018146781 -0.0018153816 -0.0018163632 -0.0018167889 -0.0018170223 -0.0018171519 -0.0018168495 -0.0018162866 -0.0018155511 -0.0018145546 -0.0018135214 -0.00181259 -0.0018119032 -0.0018113602 -0.0018109139][-0.0018189156 -0.0018204021 -0.0018220294 -0.0018232842 -0.001824314 -0.0018249275 -0.001824873 -0.0018243808 -0.0018234964 -0.0018221554 -0.0018206281 -0.0018191953 -0.001817998 -0.0018169007 -0.0018158847][-0.0018202438 -0.0018224654 -0.0018247584 -0.0018268926 -0.0018289386 -0.0018303712 -0.0018308862 -0.0018306755 -0.0018298001 -0.0018282552 -0.0018263016 -0.0018243248 -0.0018225198 -0.0018206228 -0.0018186463][-0.0018193276 -0.0018218739 -0.0018244409 -0.0018270166 -0.0018298499 -0.0018320561 -0.0018332255 -0.0018334904 -0.001833052 -0.001831724 -0.0018296671 -0.0018273169 -0.0018248184 -0.0018218621 -0.001818617][-0.0018175021 -0.0018197044 -0.0018220473 -0.0018246054 -0.0018277055 -0.0018303146 -0.0018319625 -0.0018328812 -0.0018331683 -0.0018324776 -0.001830573 -0.0018279852 -0.0018247891 -0.0018207091 -0.0018162107][-0.0018170578 -0.0018188017 -0.0018206378 -0.0018227855 -0.0018255648 -0.0018280568 -0.0018298327 -0.0018313576 -0.0018325428 -0.0018328036 -0.0018313407 -0.0018286882 -0.0018248942 -0.0018198453 -0.0018142123][-0.0018182161 -0.0018195028 -0.0018207142 -0.0018220111 -0.0018238922 -0.0018257095 -0.0018272905 -0.0018291865 -0.0018312393 -0.0018325652 -0.0018318862 -0.0018294144 -0.0018252332 -0.0018194536 -0.0018128525][-0.001821432 -0.0018222645 -0.0018224793 -0.0018224468 -0.0018227962 -0.0018233221 -0.0018242141 -0.0018262806 -0.0018290986 -0.0018314191 -0.0018315411 -0.0018294172 -0.0018252461 -0.0018191339 -0.0018120306][-0.0018246248 -0.0018250304 -0.0018245061 -0.0018232279 -0.0018219064 -0.0018208293 -0.0018206832 -0.0018224916 -0.0018253793 -0.0018279358 -0.0018284715 -0.0018269022 -0.0018232043 -0.0018173857 -0.0018107104][-0.001826099 -0.0018262995 -0.0018255016 -0.0018235241 -0.0018209607 -0.0018184574 -0.0018171059 -0.0018180748 -0.0018202855 -0.001822445 -0.0018230712 -0.0018220238 -0.0018190061 -0.0018139972 -0.0018084669][-0.0018252071 -0.0018255659 -0.0018249616 -0.0018228721 -0.0018196327 -0.0018160265 -0.0018136296 -0.0018135459 -0.001814786 -0.0018162011 -0.0018167985 -0.00181617 -0.001813969 -0.0018100593 -0.0018059169][-0.0018225722 -0.0018231384 -0.0018229658 -0.0018212599 -0.0018180322 -0.0018140355 -0.0018110678 -0.0018100508 -0.0018103309 -0.0018109344 -0.0018113415 -0.0018110208 -0.0018094282 -0.0018065851 -0.0018038441][-0.0018201041 -0.0018208838 -0.0018212704 -0.0018202021 -0.0018173803 -0.0018136852 -0.0018107335 -0.0018089828 -0.0018082578 -0.0018080123 -0.0018080353 -0.0018078316 -0.00180672 -0.0018048697 -0.0018034616]]...]
INFO - root - 2017-12-09 16:47:04.619807: step 40410, loss = 0.83, batch loss = 0.70 (8.9 examples/sec; 0.897 sec/batch; 72h:47m:53s remains)
INFO - root - 2017-12-09 16:47:13.530876: step 40420, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.853 sec/batch; 69h:12m:11s remains)
INFO - root - 2017-12-09 16:47:22.229353: step 40430, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.886 sec/batch; 71h:52m:12s remains)
INFO - root - 2017-12-09 16:47:30.888343: step 40440, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.839 sec/batch; 68h:05m:42s remains)
INFO - root - 2017-12-09 16:47:39.217521: step 40450, loss = 0.82, batch loss = 0.69 (11.0 examples/sec; 0.729 sec/batch; 59h:09m:15s remains)
INFO - root - 2017-12-09 16:47:48.001470: step 40460, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.873 sec/batch; 70h:49m:59s remains)
INFO - root - 2017-12-09 16:47:56.527272: step 40470, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.853 sec/batch; 69h:09m:50s remains)
INFO - root - 2017-12-09 16:48:05.059203: step 40480, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.835 sec/batch; 67h:45m:54s remains)
INFO - root - 2017-12-09 16:48:13.531269: step 40490, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.858 sec/batch; 69h:35m:24s remains)
INFO - root - 2017-12-09 16:48:22.111571: step 40500, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.866 sec/batch; 70h:16m:34s remains)
2017-12-09 16:48:22.975909: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0017901568 -0.0017892178 -0.0017966854 -0.0018097097 -0.001822033 -0.0018299543 -0.0018328351 -0.0018324882 -0.0018309921 -0.0018292317 -0.0018281096 -0.0018277261 -0.0018276292 -0.0018275306 -0.0018277869][-0.0017703335 -0.0017678201 -0.0017773217 -0.0017955533 -0.0018138379 -0.0018260685 -0.001831056 -0.0018314595 -0.0018303443 -0.0018291239 -0.0018286452 -0.0018288422 -0.0018289981 -0.0018292484 -0.0018296319][-0.0017637305 -0.0017597985 -0.0017691213 -0.0017886055 -0.0018089882 -0.0018231919 -0.0018293386 -0.0018304083 -0.0018297327 -0.0018291387 -0.0018295102 -0.0018304761 -0.0018311744 -0.0018318797 -0.0018325418][-0.0017759223 -0.0017719048 -0.0017790719 -0.001794686 -0.0018113611 -0.0018230086 -0.0018282728 -0.0018296008 -0.0018295104 -0.0018296442 -0.0018308223 -0.0018325245 -0.0018338618 -0.0018349814 -0.0018358079][-0.0017967725 -0.0017934546 -0.0017980502 -0.0018078163 -0.0018181406 -0.0018251062 -0.0018284288 -0.0018296199 -0.0018301008 -0.0018308277 -0.0018324873 -0.0018346405 -0.0018363419 -0.0018376997 -0.001838583][-0.0018167635 -0.0018147253 -0.0018166341 -0.001820761 -0.0018251434 -0.0018280545 -0.0018295115 -0.0018303239 -0.0018312011 -0.0018323475 -0.001834201 -0.0018364547 -0.0018382156 -0.0018394826 -0.0018401722][-0.0018276186 -0.00182655 -0.0018270714 -0.0018280136 -0.0018290867 -0.0018300118 -0.0018306841 -0.0018313411 -0.0018324604 -0.0018338066 -0.0018356496 -0.0018377432 -0.0018392778 -0.0018401853 -0.0018405021][-0.0018322871 -0.0018311694 -0.0018308213 -0.0018306657 -0.0018307283 -0.0018311231 -0.0018318023 -0.0018325088 -0.001833744 -0.0018351134 -0.0018366971 -0.0018382939 -0.0018392956 -0.0018397277 -0.0018396372][-0.0018329505 -0.0018314404 -0.0018310327 -0.0018309407 -0.0018311724 -0.0018317723 -0.0018325197 -0.0018332753 -0.0018345179 -0.0018358045 -0.0018370491 -0.0018380728 -0.0018385085 -0.0018383119 -0.001837836][-0.0018329599 -0.0018315088 -0.0018311064 -0.001830993 -0.0018312571 -0.0018318166 -0.0018326005 -0.0018333334 -0.0018344105 -0.0018355235 -0.0018364402 -0.0018371124 -0.001837188 -0.0018367041 -0.0018362843][-0.0018327101 -0.0018313559 -0.0018309986 -0.0018309354 -0.0018311421 -0.0018315843 -0.0018322726 -0.0018328551 -0.0018336393 -0.0018345354 -0.0018352101 -0.0018355818 -0.00183536 -0.001834917 -0.001834988][-0.0018325235 -0.0018310668 -0.0018307063 -0.0018307045 -0.0018309014 -0.0018312703 -0.0018318029 -0.0018322801 -0.0018328312 -0.0018334392 -0.001833934 -0.001834012 -0.0018336226 -0.0018332822 -0.0018337629][-0.0018321698 -0.001830737 -0.0018304131 -0.0018304405 -0.0018306177 -0.0018309078 -0.001831325 -0.0018316871 -0.00183203 -0.0018323488 -0.0018325932 -0.0018324561 -0.0018319633 -0.0018317132 -0.0018325179][-0.0018320297 -0.0018305978 -0.0018302961 -0.0018303724 -0.0018305088 -0.0018306924 -0.0018309974 -0.0018313086 -0.001831538 -0.0018316148 -0.0018315271 -0.0018312288 -0.0018307298 -0.0018305472 -0.0018314052][-0.0018320417 -0.0018306468 -0.0018302625 -0.0018303402 -0.0018304356 -0.0018305519 -0.0018307407 -0.0018309695 -0.001831157 -0.0018311254 -0.001830776 -0.0018303834 -0.001829994 -0.0018299009 -0.0018306557]]...]
INFO - root - 2017-12-09 16:48:31.636475: step 40510, loss = 0.81, batch loss = 0.68 (9.2 examples/sec; 0.867 sec/batch; 70h:17m:11s remains)
INFO - root - 2017-12-09 16:48:40.307325: step 40520, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 70h:04m:12s remains)
INFO - root - 2017-12-09 16:48:48.935955: step 40530, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.849 sec/batch; 68h:51m:32s remains)
INFO - root - 2017-12-09 16:48:57.517472: step 40540, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.846 sec/batch; 68h:37m:56s remains)
INFO - root - 2017-12-09 16:49:05.937052: step 40550, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.873 sec/batch; 70h:48m:01s remains)
INFO - root - 2017-12-09 16:49:14.310030: step 40560, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.866 sec/batch; 70h:14m:24s remains)
INFO - root - 2017-12-09 16:49:22.946810: step 40570, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.877 sec/batch; 71h:08m:05s remains)
INFO - root - 2017-12-09 16:49:31.636445: step 40580, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.876 sec/batch; 71h:01m:01s remains)
INFO - root - 2017-12-09 16:49:40.250522: step 40590, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 69h:51m:23s remains)
INFO - root - 2017-12-09 16:49:48.988882: step 40600, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.876 sec/batch; 71h:03m:42s remains)
2017-12-09 16:49:49.854464: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0018034881 -0.0018016492 -0.0018010938 -0.0018006865 -0.0018002816 -0.0017999427 -0.0017997033 -0.0017995776 -0.0017995297 -0.0017995368 -0.0017995683 -0.0017996209 -0.001799707 -0.0017998404 -0.0017999541][-0.0018025001 -0.0018007056 -0.0018001865 -0.0017998228 -0.001799481 -0.0017992056 -0.0017990158 -0.0017988986 -0.001798843 -0.0017988273 -0.0017988449 -0.0017989109 -0.0017990079 -0.0017991287 -0.0017992477][-0.0018024551 -0.0018007611 -0.0018002758 -0.0017999705 -0.0017996905 -0.001799462 -0.0017992918 -0.0017991607 -0.0017990663 -0.0017990042 -0.0017989876 -0.0017990425 -0.0017991348 -0.0017992764 -0.0017993937][-0.0018023337 -0.0018007032 -0.0018002362 -0.0017999755 -0.0017997356 -0.0017995208 -0.0017993287 -0.0017991809 -0.0017990727 -0.0017989969 -0.0017990124 -0.0017991139 -0.0017992489 -0.0017994076 -0.0017995082][-0.0018021371 -0.0018004752 -0.0018000344 -0.0017998308 -0.0017996271 -0.0017993974 -0.0017991639 -0.0017989728 -0.0017988278 -0.0017987504 -0.001798805 -0.0017989798 -0.0017991838 -0.001799359 -0.0017994293][-0.0018018434 -0.0018001568 -0.0017997625 -0.0017996483 -0.0017995172 -0.0017992962 -0.0017990029 -0.0017987193 -0.0017984936 -0.0017983894 -0.0017984632 -0.0017987025 -0.0017989647 -0.0017991639 -0.0017992271][-0.0018015542 -0.0017998825 -0.0017995628 -0.0017995669 -0.0017995542 -0.001799352 -0.0017989523 -0.001798487 -0.0017981083 -0.0017979465 -0.0017980185 -0.0017983137 -0.001798644 -0.0017988842 -0.0017989599][-0.0018012953 -0.0017996368 -0.0017993764 -0.0017994348 -0.0017994603 -0.0017992362 -0.0017986993 -0.0017980592 -0.0017975883 -0.0017974193 -0.001797491 -0.0017977994 -0.001798193 -0.0017984998 -0.0017986418][-0.0018009082 -0.0017992886 -0.0017990938 -0.0017991508 -0.0017991645 -0.0017989372 -0.001798376 -0.0017977082 -0.0017972124 -0.0017970126 -0.0017970206 -0.001797285 -0.0017977104 -0.0017980929 -0.001798331][-0.0018004546 -0.0017989961 -0.0017988698 -0.0017989299 -0.0017989866 -0.0017988738 -0.0017984514 -0.0017978603 -0.0017973121 -0.0017969892 -0.0017968476 -0.0017969979 -0.0017973817 -0.0017978022 -0.0017981271][-0.0018002542 -0.0017989316 -0.0017988215 -0.0017989471 -0.0017991037 -0.0017991443 -0.0017988848 -0.0017983692 -0.0017978016 -0.0017973544 -0.0017970658 -0.0017970811 -0.001797371 -0.0017977526 -0.0017980925][-0.0018003146 -0.0017990289 -0.001798896 -0.0017990293 -0.0017991958 -0.0017992947 -0.0017991271 -0.0017986862 -0.0017981444 -0.0017976786 -0.0017973661 -0.0017973135 -0.0017975196 -0.0017978486 -0.0017981696][-0.0018004556 -0.0017991407 -0.0017989245 -0.0017989913 -0.0017990858 -0.0017991383 -0.0017989865 -0.0017986259 -0.0017981732 -0.0017977906 -0.0017975506 -0.0017974917 -0.0017976342 -0.0017979054 -0.0017981927][-0.0018005776 -0.0017991377 -0.0017988154 -0.0017987983 -0.0017988047 -0.0017987874 -0.0017986546 -0.0017984051 -0.0017981287 -0.001797903 -0.001797765 -0.0017977261 -0.001797808 -0.0017979884 -0.0017981934][-0.0018006488 -0.0017991327 -0.0017987217 -0.0017986643 -0.0017986101 -0.0017985646 -0.0017984991 -0.0017983977 -0.0017982983 -0.0017982132 -0.0017981505 -0.0017981016 -0.0017980955 -0.0017981556 -0.0017982516]]...]
INFO - root - 2017-12-09 16:49:58.557486: step 40610, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.855 sec/batch; 69h:20m:13s remains)
INFO - root - 2017-12-09 16:50:07.218247: step 40620, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.874 sec/batch; 70h:49m:27s remains)
INFO - root - 2017-12-09 16:50:15.875631: step 40630, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.863 sec/batch; 69h:56m:21s remains)
INFO - root - 2017-12-09 16:50:24.485652: step 40640, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.853 sec/batch; 69h:11m:22s remains)
INFO - root - 2017-12-09 16:50:33.022118: step 40650, loss = 0.82, batch loss = 0.69 (9.7 examples/sec; 0.826 sec/batch; 66h:56m:21s remains)
INFO - root - 2017-12-09 16:50:41.546461: step 40660, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.831 sec/batch; 67h:21m:02s remains)
INFO - root - 2017-12-09 16:50:50.091088: step 40670, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.842 sec/batch; 68h:17m:16s remains)
INFO - root - 2017-12-09 16:50:58.747650: step 40680, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.865 sec/batch; 70h:06m:16s remains)
INFO - root - 2017-12-09 16:51:07.464322: step 40690, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.889 sec/batch; 72h:04m:33s remains)
INFO - root - 2017-12-09 16:51:16.179789: step 40700, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.855 sec/batch; 69h:20m:34s remains)
2017-12-09 16:51:17.038522: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0018217725 -0.0018210891 -0.0018215915 -0.0018220664 -0.0018222926 -0.0018222876 -0.0018222817 -0.0018221455 -0.0018217961 -0.0018211498 -0.0018198479 -0.0018179904 -0.0018154688 -0.0018129604 -0.0018105771][-0.001818874 -0.0018177874 -0.00181763 -0.0018175199 -0.0018173138 -0.0018169709 -0.0018167179 -0.0018164358 -0.0018161824 -0.0018158577 -0.0018150016 -0.0018137501 -0.0018119483 -0.0018102203 -0.0018084657][-0.0018163199 -0.0018149021 -0.0018142704 -0.0018137421 -0.001813184 -0.0018125884 -0.0018121217 -0.0018117497 -0.0018115937 -0.0018115445 -0.0018110867 -0.0018102969 -0.0018091741 -0.0018081513 -0.0018070054][-0.0018127516 -0.0018112616 -0.0018104183 -0.0018097789 -0.0018091853 -0.0018086411 -0.0018082234 -0.0018079053 -0.0018078536 -0.0018080071 -0.0018079027 -0.0018075403 -0.0018069803 -0.0018064298 -0.0018057611][-0.0018091338 -0.0018076209 -0.0018068292 -0.0018062826 -0.0018058147 -0.0018054482 -0.0018051891 -0.0018050183 -0.0018050784 -0.0018054064 -0.0018056493 -0.0018056992 -0.0018055635 -0.0018054015 -0.0018051207][-0.0018063294 -0.0018048215 -0.0018042426 -0.0018039518 -0.0018037552 -0.0018036298 -0.001803547 -0.0018034518 -0.001803544 -0.0018039307 -0.0018043178 -0.0018045411 -0.0018046292 -0.0018047302 -0.0018047283][-0.0018046665 -0.0018032006 -0.0018028005 -0.0018027331 -0.0018027246 -0.0018027367 -0.0018027014 -0.0018026101 -0.0018027234 -0.0018031229 -0.0018035671 -0.0018038845 -0.0018041059 -0.0018043453 -0.0018044972][-0.0018038086 -0.0018023793 -0.0018021064 -0.0018021771 -0.001802295 -0.0018024037 -0.0018023921 -0.001802294 -0.001802414 -0.0018028056 -0.0018032701 -0.0018036144 -0.001803868 -0.0018041577 -0.0018043603][-0.0018031094 -0.0018018205 -0.0018017292 -0.0018018936 -0.0018020902 -0.0018022591 -0.0018022829 -0.0018022375 -0.0018023844 -0.0018027781 -0.0018032416 -0.0018035821 -0.0018038233 -0.0018040959 -0.0018042958][-0.0018024464 -0.0018013472 -0.0018014266 -0.0018016939 -0.0018019401 -0.0018021559 -0.0018022166 -0.0018022072 -0.0018023793 -0.0018027711 -0.0018032121 -0.0018035368 -0.0018037818 -0.0018040553 -0.0018042456][-0.0018019113 -0.0018009524 -0.0018011231 -0.0018014449 -0.0018016988 -0.0018018953 -0.0018019882 -0.0018020261 -0.0018022412 -0.0018026314 -0.001803073 -0.0018034294 -0.0018037163 -0.0018040203 -0.0018042085][-0.0018014786 -0.0018006754 -0.0018008953 -0.0018012201 -0.0018014397 -0.0018016096 -0.0018016931 -0.0018017441 -0.0018019828 -0.0018023894 -0.001802862 -0.0018032857 -0.0018036432 -0.0018039884 -0.0018041831][-0.0018012268 -0.0018006625 -0.0018009242 -0.0018012298 -0.0018014057 -0.0018015342 -0.0018015946 -0.0018016443 -0.0018018926 -0.0018023108 -0.0018027982 -0.001803261 -0.0018036618 -0.0018040141 -0.0018041941][-0.0018016098 -0.0018011265 -0.0018013613 -0.0018016394 -0.001801772 -0.0018018456 -0.0018018677 -0.0018018839 -0.0018020885 -0.0018024703 -0.0018029216 -0.0018033533 -0.0018037249 -0.0018040516 -0.0018042021][-0.0018022279 -0.0018017552 -0.0018019044 -0.0018021723 -0.0018022925 -0.0018023304 -0.0018023289 -0.0018023162 -0.0018024673 -0.0018027739 -0.0018031386 -0.0018035028 -0.0018038149 -0.0018040822 -0.0018041969]]...]
INFO - root - 2017-12-09 16:51:25.750863: step 40710, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.874 sec/batch; 70h:49m:27s remains)
INFO - root - 2017-12-09 16:51:34.320652: step 40720, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.866 sec/batch; 70h:10m:14s remains)
INFO - root - 2017-12-09 16:51:42.951585: step 40730, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.836 sec/batch; 67h:47m:39s remains)
INFO - root - 2017-12-09 16:51:51.455856: step 40740, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.853 sec/batch; 69h:07m:42s remains)
INFO - root - 2017-12-09 16:51:59.947241: step 40750, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.855 sec/batch; 69h:17m:26s remains)
INFO - root - 2017-12-09 16:52:08.305167: step 40760, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.875 sec/batch; 70h:56m:32s remains)
INFO - root - 2017-12-09 16:52:16.903785: step 40770, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.865 sec/batch; 70h:05m:32s remains)
INFO - root - 2017-12-09 16:52:25.383735: step 40780, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.831 sec/batch; 67h:20m:31s remains)
INFO - root - 2017-12-09 16:52:33.776199: step 40790, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.858 sec/batch; 69h:31m:27s remains)
INFO - root - 2017-12-09 16:52:42.423847: step 40800, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.851 sec/batch; 68h:55m:15s remains)
2017-12-09 16:52:43.319061: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0017953403 -0.0017923538 -0.0017908411 -0.0017894714 -0.0017882001 -0.001786935 -0.0017859507 -0.0017853883 -0.0017852754 -0.0017853266 -0.0017854649 -0.0017857428 -0.0017860108 -0.0017861936 -0.0017862472][-0.001797864 -0.0017946339 -0.0017925937 -0.0017906037 -0.0017886787 -0.0017868878 -0.001785398 -0.0017844461 -0.0017840811 -0.0017841059 -0.0017843074 -0.0017846239 -0.001784957 -0.0017851989 -0.0017852493][-0.0018013635 -0.0017979766 -0.0017954813 -0.0017929164 -0.0017904083 -0.0017880392 -0.001785908 -0.001784443 -0.0017837465 -0.0017836805 -0.001783939 -0.0017843369 -0.0017847881 -0.0017851173 -0.0017851711][-0.00180429 -0.0018008491 -0.0017981416 -0.0017952597 -0.0017923488 -0.0017894763 -0.0017868156 -0.0017848017 -0.0017837028 -0.0017834752 -0.0017837303 -0.0017842154 -0.0017848131 -0.0017852746 -0.0017853773][-0.0018062865 -0.0018028166 -0.0018001723 -0.0017972906 -0.0017942752 -0.0017912188 -0.0017883163 -0.0017858754 -0.0017843065 -0.0017837478 -0.0017837903 -0.0017841714 -0.0017847226 -0.0017852163 -0.0017853801][-0.0018065593 -0.0018031769 -0.0018006642 -0.001798048 -0.0017952415 -0.0017923058 -0.0017893449 -0.0017866803 -0.0017847534 -0.0017838842 -0.0017837423 -0.0017839716 -0.001784432 -0.0017849451 -0.0017851959][-0.0018056426 -0.0018025046 -0.001800111 -0.0017977332 -0.0017952839 -0.0017926054 -0.0017897168 -0.0017869138 -0.0017847779 -0.0017837475 -0.001783437 -0.0017834804 -0.0017838813 -0.0017844461 -0.0017848643][-0.0018034306 -0.0018005426 -0.0017985072 -0.0017966074 -0.0017947253 -0.0017924914 -0.001790031 -0.0017875131 -0.0017853291 -0.0017840489 -0.0017834459 -0.0017832395 -0.0017835086 -0.0017840033 -0.0017845324][-0.0018004531 -0.0017979189 -0.0017963679 -0.0017950521 -0.0017938027 -0.0017921694 -0.0017903215 -0.0017883427 -0.0017863344 -0.0017848215 -0.0017838774 -0.0017834586 -0.0017835743 -0.0017838819 -0.0017843533][-0.0017972612 -0.0017951503 -0.0017939461 -0.0017931226 -0.0017923656 -0.0017912608 -0.0017899821 -0.0017885866 -0.0017869025 -0.0017854166 -0.0017844276 -0.0017839284 -0.0017839374 -0.0017840831 -0.0017843924][-0.0017947252 -0.0017928947 -0.0017918369 -0.0017912448 -0.0017907423 -0.0017899729 -0.0017890837 -0.0017881246 -0.0017868346 -0.0017856553 -0.0017848556 -0.0017844526 -0.0017844473 -0.0017844723 -0.0017845916][-0.0017929382 -0.001791151 -0.0017902665 -0.0017898458 -0.0017895437 -0.0017890049 -0.0017883474 -0.0017877136 -0.001786863 -0.0017859926 -0.0017853264 -0.001784999 -0.0017849616 -0.0017848626 -0.001784795][-0.0017916338 -0.0017898896 -0.0017891338 -0.0017889109 -0.0017887165 -0.0017882443 -0.0017877222 -0.0017873065 -0.0017867393 -0.0017860859 -0.0017855793 -0.0017853229 -0.0017852772 -0.0017851196 -0.0017849179][-0.0017903481 -0.0017885767 -0.0017879002 -0.0017877908 -0.0017876532 -0.0017872708 -0.0017868764 -0.001786615 -0.0017862631 -0.0017858332 -0.001785496 -0.0017853357 -0.0017853185 -0.0017851837 -0.0017849478][-0.0017892603 -0.0017874038 -0.0017867169 -0.0017866328 -0.0017865364 -0.0017862873 -0.0017860694 -0.0017859576 -0.001785784 -0.0017855326 -0.001785328 -0.0017852164 -0.001785184 -0.001785078 -0.0017848861]]...]
INFO - root - 2017-12-09 16:52:51.990874: step 40810, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.875 sec/batch; 70h:54m:24s remains)
INFO - root - 2017-12-09 16:53:00.602611: step 40820, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.849 sec/batch; 68h:47m:00s remains)
INFO - root - 2017-12-09 16:53:09.329906: step 40830, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.861 sec/batch; 69h:44m:05s remains)
INFO - root - 2017-12-09 16:53:17.961491: step 40840, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.878 sec/batch; 71h:09m:47s remains)
INFO - root - 2017-12-09 16:53:26.607325: step 40850, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.859 sec/batch; 69h:36m:28s remains)
INFO - root - 2017-12-09 16:53:35.171382: step 40860, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.860 sec/batch; 69h:37m:58s remains)
INFO - root - 2017-12-09 16:53:43.689789: step 40870, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.881 sec/batch; 71h:20m:20s remains)
INFO - root - 2017-12-09 16:53:52.457407: step 40880, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.879 sec/batch; 71h:14m:35s remains)
INFO - root - 2017-12-09 16:54:01.251160: step 40890, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.880 sec/batch; 71h:15m:52s remains)
INFO - root - 2017-12-09 16:54:09.899471: step 40900, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.855 sec/batch; 69h:16m:02s remains)
2017-12-09 16:54:10.765318: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.0052514365 0.0052803727 0.005400307 0.0056683365 0.0061098584 0.0066320072 0.00717638 0.0077466373 0.0081574265 0.0083887288 0.0084133334 0.00835993 0.0080884537 0.0074766246 0.0068089352][0.0058227158 0.0058938623 0.0060913428 0.0064895651 0.0071560624 0.0080043729 0.0089191822 0.0096900854 0.010237315 0.010616691 0.010639297 0.010487663 0.010007309 0.0092249634 0.0083474061][0.0058633075 0.0060150395 0.0063149431 0.0068815057 0.0077845841 0.0088741444 0.010013874 0.010911976 0.011575459 0.011971969 0.012004633 0.011851786 0.011236137 0.010350523 0.0093381964][0.0059378264 0.0061660861 0.0065646362 0.00729747 0.0083843367 0.0096202716 0.010845554 0.011791038 0.012501287 0.012964655 0.013092332 0.013025932 0.012474206 0.011614448 0.010479593][0.0060863751 0.0063311975 0.0067422227 0.0075442246 0.00870959 0.009968238 0.01114562 0.01210495 0.01281546 0.013316949 0.013558913 0.013561937 0.013172944 0.012359297 0.011293736][0.0060027768 0.0063224048 0.0067307637 0.0074222977 0.0084202932 0.0095452871 0.010603248 0.011579362 0.01247158 0.013185005 0.013678921 0.013903074 0.013741854 0.013090751 0.012090187][0.0057735895 0.0059694392 0.0061860611 0.0065755085 0.0072156922 0.0080555174 0.0089595076 0.00995949 0.010997776 0.012085968 0.01297015 0.013620115 0.013827968 0.013424102 0.012600149][0.0056639621 0.0056977258 0.0055618351 0.0054219761 0.0055230684 0.0058794226 0.0065359813 0.0074949036 0.0087000765 0.010078113 0.011471259 0.01263669 0.013325624 0.013377019 0.012876695][0.0055865226 0.0055444613 0.0051139127 0.0044494816 0.0039255475 0.0037235017 0.0039875046 0.0047120308 0.0059135556 0.0074787769 0.0091875931 0.010799616 0.012032985 0.01260187 0.012572414][0.00527007 0.0052455897 0.0047155968 0.003812355 0.00291509 0.0022230628 0.0020007668 0.0023613074 0.0033168779 0.0047932835 0.0065843291 0.0084704217 0.010035846 0.011080911 0.011455691][0.0044682296 0.00444538 0.0038998774 0.0029643886 0.0019971938 0.0012193533 0.0008069406 0.00086703768 0.0014996816 0.0027609887 0.0044433647 0.0063519864 0.00805751 0.0092532719 0.0098900571][0.0030705379 0.0030502733 0.0025559189 0.001762204 0.000960278 0.00031043834 -6.6545676e-05 -6.0046325e-05 0.00039822387 0.0014169657 0.0028677327 0.0045877518 0.0061836732 0.0072712391 0.0078659179][0.0012448967 0.0012847272 0.00097188621 0.00041945151 -0.00014438096 -0.00059143105 -0.00084734033 -0.00080862292 -0.00041096681 0.00044226076 0.0016683609 0.0031428747 0.0044593955 0.005472519 0.0059644463][-0.00044545194 -0.00041862275 -0.00056399277 -0.000807835 -0.0010789278 -0.0013245378 -0.0014700212 -0.0014335178 -0.0011679661 -0.00058383169 0.00030223851 0.0013767831 0.0023352057 0.003121797 0.0035090386][-0.0014746136 -0.0014570903 -0.0014848558 -0.0015482963 -0.0016235465 -0.0016951825 -0.0017472121 -0.0017455535 -0.0016300265 -0.0013455302 -0.00085989095 -0.00024447171 0.00034528424 0.000838115 0.0010542389]]...]
INFO - root - 2017-12-09 16:54:19.456997: step 40910, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.897 sec/batch; 72h:38m:24s remains)
INFO - root - 2017-12-09 16:54:28.133850: step 40920, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 70h:00m:46s remains)
INFO - root - 2017-12-09 16:54:36.879280: step 40930, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.861 sec/batch; 69h:43m:48s remains)
INFO - root - 2017-12-09 16:54:45.639814: step 40940, loss = 0.82, batch loss = 0.69 (8.6 examples/sec; 0.930 sec/batch; 75h:19m:34s remains)
INFO - root - 2017-12-09 16:54:54.040689: step 40950, loss = 0.83, batch loss = 0.70 (9.7 examples/sec; 0.823 sec/batch; 66h:36m:54s remains)
INFO - root - 2017-12-09 16:55:02.533736: step 40960, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.865 sec/batch; 70h:01m:51s remains)
INFO - root - 2017-12-09 16:55:10.999508: step 40970, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.902 sec/batch; 73h:04m:09s remains)
INFO - root - 2017-12-09 16:55:19.702368: step 40980, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.832 sec/batch; 67h:24m:10s remains)
INFO - root - 2017-12-09 16:55:28.330673: step 40990, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.865 sec/batch; 70h:03m:20s remains)
INFO - root - 2017-12-09 16:55:37.160945: step 41000, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.874 sec/batch; 70h:45m:47s remains)
2017-12-09 16:55:38.059069: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0018236187 -0.0018254786 -0.0018277026 -0.0018295217 -0.0018307565 -0.00183179 -0.0018326783 -0.0018336828 -0.0018346505 -0.0018344664 -0.0018337207 -0.0018335652 -0.0018334033 -0.0018337591 -0.0018338669][-0.0018283777 -0.0018298329 -0.0018315583 -0.0018327846 -0.0018336293 -0.001834354 -0.001834807 -0.0018353064 -0.0018356907 -0.0018352825 -0.0018343674 -0.0018337436 -0.0018332117 -0.0018332003 -0.0018330995][-0.0018330448 -0.001834157 -0.0018352907 -0.0018359601 -0.0018362356 -0.0018364425 -0.0018363022 -0.0018361441 -0.0018358225 -0.0018351662 -0.0018342611 -0.0018335779 -0.001832901 -0.0018326588 -0.0018323828][-0.0018369373 -0.0018374881 -0.0018378854 -0.0018378928 -0.001837682 -0.0018374367 -0.0018369349 -0.0018363602 -0.0018357263 -0.0018347461 -0.0018337376 -0.0018330807 -0.0018324816 -0.0018321893 -0.001831769][-0.0018394347 -0.0018391572 -0.0018387546 -0.0018381455 -0.0018375273 -0.0018369638 -0.001836237 -0.0018355928 -0.0018350022 -0.001834144 -0.0018333682 -0.0018328534 -0.0018323624 -0.0018320224 -0.0018315144][-0.0018411959 -0.0018400712 -0.0018387798 -0.0018374742 -0.001836392 -0.001835549 -0.0018347078 -0.0018340702 -0.0018337487 -0.0018332431 -0.0018327817 -0.0018326505 -0.0018322736 -0.0018318214 -0.001831256][-0.0018424185 -0.0018408041 -0.0018389642 -0.0018371004 -0.0018356553 -0.0018346516 -0.0018338986 -0.0018333447 -0.0018331532 -0.0018327758 -0.0018324035 -0.0018323594 -0.0018320975 -0.0018315914 -0.001831067][-0.001843835 -0.0018417727 -0.0018395504 -0.0018372538 -0.0018355511 -0.0018345327 -0.0018337321 -0.0018332051 -0.0018330236 -0.0018327049 -0.0018324065 -0.0018323058 -0.001831958 -0.0018313904 -0.0018308716][-0.0018447686 -0.0018425507 -0.0018401892 -0.0018380111 -0.001836382 -0.0018354413 -0.0018345083 -0.0018338658 -0.0018334057 -0.0018329233 -0.0018326433 -0.0018323481 -0.0018317833 -0.0018311909 -0.0018307613][-0.0018450923 -0.001842991 -0.0018408024 -0.0018387509 -0.0018372101 -0.0018363111 -0.0018353664 -0.0018345462 -0.0018337719 -0.0018330197 -0.0018325396 -0.0018320245 -0.0018313685 -0.0018308216 -0.0018305324][-0.0018453314 -0.0018433125 -0.0018413507 -0.001839561 -0.0018382471 -0.0018374777 -0.0018366792 -0.0018356325 -0.0018344008 -0.0018331956 -0.0018323205 -0.0018317476 -0.0018310713 -0.0018305184 -0.0018302964][-0.001845322 -0.0018435605 -0.001841801 -0.0018402782 -0.0018392173 -0.0018386469 -0.0018380141 -0.0018369186 -0.0018352134 -0.0018333778 -0.0018321801 -0.0018315003 -0.0018308723 -0.0018304134 -0.0018301721][-0.0018450145 -0.0018435083 -0.001842033 -0.0018407032 -0.0018398095 -0.0018394557 -0.0018391153 -0.0018380906 -0.0018360503 -0.0018336907 -0.0018320583 -0.0018312167 -0.001830654 -0.0018303492 -0.0018301047][-0.0018445131 -0.0018433263 -0.0018420378 -0.0018409524 -0.0018402797 -0.001839989 -0.0018396616 -0.001838654 -0.0018365715 -0.001834085 -0.0018321691 -0.0018311865 -0.0018306067 -0.0018303304 -0.0018300141][-0.0018436149 -0.0018426075 -0.0018414385 -0.0018404435 -0.0018398389 -0.0018396826 -0.0018394764 -0.001838582 -0.0018367768 -0.0018344861 -0.0018324868 -0.0018313994 -0.0018308456 -0.0018306014 -0.0018302597]]...]
INFO - root - 2017-12-09 16:55:46.688774: step 41010, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 70h:12m:30s remains)
INFO - root - 2017-12-09 16:55:55.272411: step 41020, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.854 sec/batch; 69h:06m:39s remains)
INFO - root - 2017-12-09 16:56:03.888643: step 41030, loss = 0.82, batch loss = 0.69 (9.7 examples/sec; 0.823 sec/batch; 66h:35m:38s remains)
INFO - root - 2017-12-09 16:56:12.506599: step 41040, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.874 sec/batch; 70h:44m:55s remains)
INFO - root - 2017-12-09 16:56:21.025036: step 41050, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.851 sec/batch; 68h:54m:57s remains)
INFO - root - 2017-12-09 16:56:29.700370: step 41060, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.874 sec/batch; 70h:46m:36s remains)
INFO - root - 2017-12-09 16:56:38.256759: step 41070, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 70h:14m:40s remains)
INFO - root - 2017-12-09 16:56:47.030519: step 41080, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.884 sec/batch; 71h:35m:33s remains)
INFO - root - 2017-12-09 16:56:55.813141: step 41090, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.861 sec/batch; 69h:39m:44s remains)
INFO - root - 2017-12-09 16:57:04.728142: step 41100, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 70h:29m:31s remains)
2017-12-09 16:57:05.637560: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.001787463 -0.0017276887 -0.0016263365 -0.0014948004 -0.0013398756 -0.0011957855 -0.0011169807 -0.0011321806 -0.0012127806 -0.0013161415 -0.0014539812 -0.001605812 -0.001730206 -0.0017923281 -0.0018122728][-0.0017475738 -0.0015993093 -0.0013336671 -0.00096217822 -0.00050642458 -6.9173984e-05 0.00020487886 0.00020968681 -8.1986655e-06 -0.00035570993 -0.000803433 -0.0012481854 -0.0015967474 -0.001760163 -0.0018122175][-0.001670436 -0.0013578041 -0.00076860131 0.000123 0.0012662418 0.0024050409 0.0031885118 0.0033191442 0.0028053615 0.001816947 0.00056683179 -0.00057373696 -0.0013780597 -0.0017182985 -0.0018118584][-0.0015889327 -0.0010874696 -8.220668e-05 0.001551654 0.00375306 0.0060655414 0.0078100716 0.00833765 0.0074692215 0.0054356973 0.0028113718 0.00048091915 -0.0010528658 -0.0016643081 -0.001809802][-0.0015305218 -0.00086773071 0.00053355331 0.0029175961 0.0062369979 0.0098612215 0.012753766 0.013829984 0.012630938 0.0094424244 0.0052724616 0.0016017095 -0.00072724337 -0.0016177904 -0.0018038485][-0.0015330767 -0.0008212406 0.00076550804 0.0035662563 0.007581241 0.012118081 0.015908159 0.017513601 0.016180327 0.012202132 0.0069332155 0.0023174463 -0.000542211 -0.0016007981 -0.0018002212][-0.0015798334 -0.00093955942 0.0005547863 0.0032785148 0.007287378 0.011958105 0.016017457 0.017912909 0.016698381 0.012605492 0.0071139508 0.0023273295 -0.00057612965 -0.0016193349 -0.0017981816][-0.0016502581 -0.0011621313 2.6290189e-05 0.0022606654 0.0056370851 0.0096891969 0.01333712 0.015171183 0.01422709 0.010660851 0.0058406666 0.0016760454 -0.00080107083 -0.0016631298 -0.0017975603][-0.0017207708 -0.0014093915 -0.00061490806 0.00093434961 0.0033474383 0.0063397926 0.0091310889 0.010623181 0.0099968426 0.0073431018 0.0037441039 0.00067412853 -0.0011122089 -0.0017139744 -0.0018003567][-0.0017704448 -0.00160367 -0.0011561703 -0.0002480034 0.001215514 0.003093733 0.0048987539 0.0058977669 0.0055157556 0.0038171313 0.0015407824 -0.00035153038 -0.0014188469 -0.0017609985 -0.0018040356][-0.0017985562 -0.0017241685 -0.0015142285 -0.0010693709 -0.00032430398 0.0006681215 0.0016465131 0.0021961392 0.0019788113 0.0010420342 -0.00017205253 -0.0011309625 -0.0016427318 -0.0017929258 -0.0018085293][-0.0018101957 -0.0017827778 -0.0017018546 -0.0015222558 -0.0012090625 -0.00077793468 -0.00034384266 -0.00010324107 -0.00021758385 -0.00065421546 -0.001188382 -0.0015754899 -0.0017613147 -0.0018076128 -0.0018105413][-0.0018129324 -0.0018049437 -0.0017808806 -0.00172485 -0.0016218021 -0.0014739886 -0.0013221358 -0.0012405525 -0.0012914188 -0.0014551446 -0.0016393382 -0.001755901 -0.0018026063 -0.0018108397 -0.0018107384][-0.0018122779 -0.0018101758 -0.0018051548 -0.0017933444 -0.0017699506 -0.0017344319 -0.0016955821 -0.001674545 -0.0016894076 -0.0017331212 -0.0017789602 -0.0018033 -0.0018102383 -0.0018107187 -0.0018104587][-0.0018110463 -0.0018099095 -0.0018093013 -0.0018079786 -0.0018048924 -0.0018001458 -0.0017950018 -0.0017923034 -0.0017955133 -0.0018022803 -0.001808244 -0.0018103318 -0.001810396 -0.0018103038 -0.001810135]]...]
INFO - root - 2017-12-09 16:57:14.296977: step 41110, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.873 sec/batch; 70h:40m:26s remains)
INFO - root - 2017-12-09 16:57:23.019081: step 41120, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.840 sec/batch; 67h:56m:59s remains)
INFO - root - 2017-12-09 16:57:31.803444: step 41130, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.863 sec/batch; 69h:50m:36s remains)
INFO - root - 2017-12-09 16:57:40.352062: step 41140, loss = 0.82, batch loss = 0.69 (9.9 examples/sec; 0.804 sec/batch; 65h:06m:04s remains)
INFO - root - 2017-12-09 16:57:48.889930: step 41150, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.843 sec/batch; 68h:12m:15s remains)
INFO - root - 2017-12-09 16:57:57.190908: step 41160, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.855 sec/batch; 69h:13m:30s remains)
INFO - root - 2017-12-09 16:58:05.712737: step 41170, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.885 sec/batch; 71h:35m:35s remains)
INFO - root - 2017-12-09 16:58:14.510806: step 41180, loss = 0.83, batch loss = 0.70 (9.4 examples/sec; 0.851 sec/batch; 68h:51m:57s remains)
INFO - root - 2017-12-09 16:58:23.168634: step 41190, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.892 sec/batch; 72h:11m:27s remains)
INFO - root - 2017-12-09 16:58:31.831400: step 41200, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.837 sec/batch; 67h:42m:29s remains)
2017-12-09 16:58:32.707000: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0018275458 -0.0018271052 -0.0018268761 -0.0018266678 -0.0018264508 -0.0018262366 -0.0018260347 -0.0018258746 -0.0018258076 -0.0018258286 -0.0018259072 -0.0018259874 -0.0018260273 -0.0018259892 -0.0018258882][-0.0018275736 -0.0018270971 -0.0018268073 -0.0018265438 -0.0018262926 -0.0018260685 -0.0018258686 -0.0018257217 -0.0018256739 -0.0018257074 -0.0018257791 -0.0018258371 -0.0018258485 -0.001825786 -0.001825668][-0.0018276717 -0.0018272317 -0.0018269349 -0.0018266421 -0.0018263864 -0.0018261689 -0.0018259741 -0.0018258394 -0.0018257995 -0.0018258186 -0.0018258473 -0.0018258555 -0.0018258226 -0.0018257323 -0.0018256047][-0.0018277001 -0.0018273141 -0.0018270256 -0.0018267278 -0.0018264692 -0.0018262499 -0.001826052 -0.0018259237 -0.001825884 -0.0018258794 -0.0018258523 -0.0018257931 -0.0018257073 -0.0018255896 -0.0018254557][-0.0018275968 -0.0018273001 -0.0018270825 -0.0018268561 -0.00182665 -0.0018264505 -0.001826247 -0.0018261025 -0.0018260255 -0.001825948 -0.0018258332 -0.001825685 -0.0018255376 -0.0018253877 -0.0018252453][-0.001827289 -0.0018271004 -0.0018270074 -0.0018268786 -0.0018267456 -0.0018265883 -0.0018263898 -0.0018262063 -0.0018260704 -0.0018259151 -0.0018257075 -0.001825479 -0.0018252868 -0.0018251162 -0.0018249729][-0.001826875 -0.0018268012 -0.0018268755 -0.0018268928 -0.0018268679 -0.001826771 -0.0018265758 -0.0018263233 -0.0018260718 -0.0018258076 -0.0018255088 -0.001825224 -0.0018249997 -0.0018248168 -0.0018246832][-0.0018264933 -0.0018264876 -0.0018267466 -0.0018270013 -0.0018271911 -0.0018272419 -0.001827084 -0.0018267459 -0.0018263322 -0.0018259244 -0.0018255324 -0.0018251979 -0.0018249442 -0.001824743 -0.0018246033][-0.0018261595 -0.0018261439 -0.0018265416 -0.0018270348 -0.0018275076 -0.0018278171 -0.0018278064 -0.0018274576 -0.0018269126 -0.0018263493 -0.001825834 -0.0018254265 -0.0018251296 -0.0018249148 -0.0018247843][-0.0018259888 -0.0018259297 -0.0018263807 -0.0018270232 -0.0018277116 -0.0018282651 -0.001828463 -0.0018282165 -0.0018276632 -0.0018270473 -0.0018264756 -0.0018260224 -0.0018256791 -0.0018254414 -0.0018253045][-0.0018259276 -0.0018258434 -0.0018263091 -0.0018270354 -0.0018278727 -0.0018286237 -0.0018290246 -0.0018289471 -0.0018285046 -0.001827965 -0.0018274392 -0.0018269971 -0.0018266281 -0.0018263556 -0.0018261832][-0.0018261202 -0.0018260119 -0.0018264242 -0.0018271154 -0.0018279644 -0.0018287749 -0.0018292881 -0.0018293782 -0.0018291059 -0.0018287232 -0.0018283251 -0.0018279547 -0.0018276141 -0.0018273408 -0.0018271571][-0.0018264382 -0.0018262991 -0.0018266004 -0.001827146 -0.0018278431 -0.001828545 -0.0018290421 -0.0018292408 -0.0018291631 -0.0018289817 -0.0018287543 -0.001828499 -0.0018282275 -0.0018279974 -0.0018278443][-0.0018266521 -0.0018265338 -0.0018267239 -0.0018270998 -0.0018275775 -0.0018280823 -0.0018284786 -0.0018287194 -0.0018288003 -0.0018287965 -0.0018287154 -0.0018285608 -0.001828352 -0.0018281618 -0.0018280376][-0.0018267628 -0.0018266678 -0.0018267591 -0.0018269798 -0.0018272413 -0.0018275271 -0.0018277841 -0.0018279959 -0.001828141 -0.0018282282 -0.0018282331 -0.0018281396 -0.0018279708 -0.0018278128 -0.001827711]]...]
INFO - root - 2017-12-09 16:58:41.373579: step 41210, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.853 sec/batch; 68h:59m:18s remains)
INFO - root - 2017-12-09 16:58:49.914865: step 41220, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.863 sec/batch; 69h:49m:20s remains)
INFO - root - 2017-12-09 16:58:58.545321: step 41230, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.878 sec/batch; 71h:04m:01s remains)
INFO - root - 2017-12-09 16:59:07.279845: step 41240, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.876 sec/batch; 70h:52m:44s remains)
INFO - root - 2017-12-09 16:59:15.866383: step 41250, loss = 0.81, batch loss = 0.68 (10.9 examples/sec; 0.737 sec/batch; 59h:35m:52s remains)
INFO - root - 2017-12-09 16:59:24.327080: step 41260, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.849 sec/batch; 68h:40m:39s remains)
INFO - root - 2017-12-09 16:59:32.845006: step 41270, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.887 sec/batch; 71h:43m:08s remains)
INFO - root - 2017-12-09 16:59:41.508012: step 41280, loss = 0.81, batch loss = 0.68 (9.0 examples/sec; 0.885 sec/batch; 71h:37m:50s remains)
INFO - root - 2017-12-09 16:59:50.262645: step 41290, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.904 sec/batch; 73h:06m:07s remains)
INFO - root - 2017-12-09 16:59:59.090675: step 41300, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.870 sec/batch; 70h:23m:19s remains)
2017-12-09 16:59:59.953379: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.00048817764 0.0012211765 0.0018064321 0.0022126217 0.0024232129 0.0024589919 0.0024478855 0.00239859 0.0022727032 0.0019999121 0.0015872533 0.0010459272 0.0003575962 -0.00031696202 -0.000894283][0.0014896607 0.0025845566 0.0035923044 0.0044467687 0.0051070983 0.00548201 0.0056847413 0.0057154913 0.0055171279 0.0050616162 0.0043874173 0.0034888238 0.0023492509 0.0011768336 0.00012866524][0.0037477659 0.0055581019 0.0073521673 0.0090255048 0.010434408 0.01142616 0.012076344 0.012354758 0.012135427 0.011445563 0.010311749 0.0086782835 0.0065820483 0.0043193921 0.0022129128][0.0082307328 0.011263026 0.01437987 0.017290467 0.019751245 0.021559874 0.022690814 0.023106711 0.022652984 0.021432983 0.019460443 0.01668294 0.013157884 0.00927238 0.0055732848][0.015838504 0.020576617 0.025213908 0.029411918 0.03285598 0.035379011 0.036916215 0.037420023 0.0366337 0.03469811 0.031547755 0.027127014 0.021618007 0.015551962 0.0097768977][0.025285747 0.03159922 0.03731833 0.042182505 0.046023156 0.048766535 0.050265912 0.050534088 0.049256351 0.046551507 0.042260125 0.036328867 0.029018901 0.021024214 0.013449699][0.033593025 0.040796127 0.046786197 0.051520687 0.055035196 0.057349846 0.058279473 0.057917926 0.055971604 0.052590661 0.047487784 0.040581379 0.03221529 0.023202643 0.014796225][0.03668572 0.043708697 0.0491105 0.053013317 0.055681247 0.057199176 0.057357885 0.056302387 0.0538152 0.050060663 0.044714581 0.037704684 0.029429542 0.020784223 0.012956493][0.032938592 0.038751576 0.042982895 0.045779191 0.047470711 0.048157968 0.047653403 0.046120822 0.043484 0.039907478 0.035068315 0.028914778 0.021884261 0.014844805 0.0087502543][0.023513824 0.027609611 0.030519297 0.032278448 0.03311345 0.033132393 0.032226235 0.030590856 0.028294543 0.025461106 0.021835074 0.017397383 0.012510693 0.0078615509 0.0040521468][0.012711378 0.015179845 0.01692084 0.017855169 0.018078834 0.017672075 0.016653258 0.015263697 0.013647821 0.011870129 0.00975501 0.0072835726 0.0046803094 0.0023346054 0.00054090517][0.0042647249 0.0054973564 0.0063855019 0.0067987745 0.0067482325 0.0062949941 0.0055055018 0.0046003852 0.0037289429 0.0029209806 0.0020585181 0.0011075817 0.00015875767 -0.00064441073 -0.0012083333][-6.7025772e-05 0.0004024196 0.00073018111 0.00083501777 0.00071728462 0.00042948849 1.8780353e-05 -0.00038767944 -0.00070643437 -0.00093331886 -0.0011312033 -0.0013346842 -0.0015185549 -0.0016576494 -0.0017390719][-0.0015758338 -0.0014444665 -0.0013502876 -0.0013250674 -0.001368291 -0.0014613545 -0.001580639 -0.0016855533 -0.0017481645 -0.0017693123 -0.0017744949 -0.0017784502 -0.0017820762 -0.0017830021 -0.0017818317][-0.0017872435 -0.0017801324 -0.0017756948 -0.0017748059 -0.001774148 -0.0017749567 -0.0017772045 -0.0017795302 -0.0017820213 -0.001783033 -0.0017847096 -0.0017856209 -0.0017858007 -0.0017849032 -0.0017832234]]...]
INFO - root - 2017-12-09 17:00:08.708170: step 41310, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.898 sec/batch; 72h:37m:14s remains)
INFO - root - 2017-12-09 17:00:17.239305: step 41320, loss = 0.82, batch loss = 0.69 (9.7 examples/sec; 0.824 sec/batch; 66h:37m:54s remains)
INFO - root - 2017-12-09 17:00:25.975917: step 41330, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.840 sec/batch; 67h:55m:23s remains)
INFO - root - 2017-12-09 17:00:34.508042: step 41340, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.866 sec/batch; 70h:00m:16s remains)
INFO - root - 2017-12-09 17:00:43.013820: step 41350, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.855 sec/batch; 69h:07m:23s remains)
INFO - root - 2017-12-09 17:00:51.291753: step 41360, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.835 sec/batch; 67h:31m:27s remains)
INFO - root - 2017-12-09 17:00:59.884948: step 41370, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.858 sec/batch; 69h:22m:27s remains)
INFO - root - 2017-12-09 17:01:08.543297: step 41380, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.857 sec/batch; 69h:17m:23s remains)
INFO - root - 2017-12-09 17:01:17.205376: step 41390, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.865 sec/batch; 69h:54m:47s remains)
INFO - root - 2017-12-09 17:01:25.936332: step 41400, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.894 sec/batch; 72h:17m:00s remains)
2017-12-09 17:01:26.886213: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0018476401 -0.0018478803 -0.001847717 -0.0018469976 -0.0018460117 -0.0018448023 -0.0018435613 -0.001842736 -0.0018423629 -0.0018424084 -0.0018428355 -0.0018436132 -0.0018446555 -0.0018456036 -0.0018463104][-0.001847575 -0.0018477996 -0.0018475602 -0.0018467373 -0.0018456226 -0.001844358 -0.0018432892 -0.0018427761 -0.0018428041 -0.001843209 -0.0018439372 -0.0018448565 -0.0018458234 -0.0018466815 -0.0018471443][-0.0018469917 -0.0018468654 -0.0018461915 -0.0018449529 -0.0018434169 -0.0018418947 -0.0018408984 -0.0018407217 -0.0018412544 -0.0018420618 -0.0018431892 -0.0018445158 -0.0018457986 -0.0018466627 -0.0018471251][-0.0018459071 -0.0018451262 -0.0018437065 -0.001841532 -0.001839146 -0.001837263 -0.0018361852 -0.0018363133 -0.0018374219 -0.0018391414 -0.0018411326 -0.0018430167 -0.0018447889 -0.0018458796 -0.0018463167][-0.0018447416 -0.0018429245 -0.0018404248 -0.0018371084 -0.001833648 -0.0018311203 -0.0018299137 -0.0018304227 -0.0018322979 -0.0018351763 -0.0018381131 -0.0018408728 -0.0018434484 -0.0018449334 -0.0018455088][-0.0018437516 -0.001841035 -0.0018374919 -0.0018332181 -0.0018288626 -0.001825682 -0.0018241971 -0.0018249941 -0.0018274485 -0.0018310981 -0.0018348652 -0.0018384459 -0.0018415508 -0.0018433611 -0.0018440488][-0.0018433813 -0.0018399982 -0.0018358279 -0.0018308646 -0.0018259244 -0.0018223663 -0.0018207106 -0.0018215248 -0.0018241321 -0.0018281797 -0.0018323751 -0.0018363915 -0.0018397559 -0.0018417876 -0.0018425448][-0.0018429364 -0.0018393883 -0.0018351701 -0.0018301307 -0.0018250982 -0.0018213512 -0.0018195626 -0.0018202103 -0.0018226833 -0.0018264143 -0.0018303386 -0.0018343895 -0.0018379728 -0.001840244 -0.0018410542][-0.0018419089 -0.001838634 -0.0018349889 -0.0018305605 -0.00182611 -0.0018229106 -0.0018212921 -0.0018217297 -0.001823758 -0.0018269233 -0.0018303001 -0.0018336669 -0.0018369682 -0.0018391235 -0.0018398996][-0.0018399701 -0.0018372841 -0.0018344821 -0.0018312379 -0.0018280398 -0.001825695 -0.0018245205 -0.0018250146 -0.0018267969 -0.0018293399 -0.0018320258 -0.0018347829 -0.0018374297 -0.001838904 -0.0018394305][-0.0018370224 -0.0018349905 -0.0018330346 -0.0018309947 -0.0018291196 -0.0018278862 -0.0018277229 -0.0018288385 -0.0018306352 -0.0018328538 -0.001835158 -0.0018372865 -0.0018390385 -0.0018398557 -0.0018399761][-0.0018335663 -0.001831985 -0.0018307129 -0.0018296534 -0.0018289495 -0.0018287943 -0.0018294959 -0.0018310542 -0.0018330363 -0.0018350534 -0.0018370537 -0.0018387865 -0.0018401493 -0.0018408002 -0.001840908][-0.0018299485 -0.0018288317 -0.001828144 -0.0018278762 -0.0018280613 -0.0018287553 -0.0018299611 -0.0018316979 -0.0018337126 -0.0018356185 -0.0018373654 -0.0018387209 -0.0018398911 -0.0018406928 -0.001841128][-0.0018274523 -0.0018265023 -0.0018262529 -0.0018264635 -0.0018269344 -0.0018277208 -0.0018290143 -0.0018306999 -0.0018323926 -0.0018341485 -0.0018357963 -0.0018371904 -0.0018383123 -0.0018392225 -0.0018399401][-0.0018250182 -0.0018242982 -0.0018240991 -0.0018242783 -0.0018249318 -0.0018257892 -0.0018269587 -0.0018282955 -0.0018297624 -0.0018311647 -0.00183254 -0.0018337886 -0.0018349445 -0.0018358991 -0.0018367347]]...]
INFO - root - 2017-12-09 17:01:35.485966: step 41410, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.865 sec/batch; 69h:55m:05s remains)
INFO - root - 2017-12-09 17:01:44.036082: step 41420, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.837 sec/batch; 67h:38m:31s remains)
INFO - root - 2017-12-09 17:01:52.683390: step 41430, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 70h:14m:05s remains)
INFO - root - 2017-12-09 17:02:01.353336: step 41440, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.904 sec/batch; 73h:04m:59s remains)
INFO - root - 2017-12-09 17:02:10.176843: step 41450, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.870 sec/batch; 70h:18m:38s remains)
INFO - root - 2017-12-09 17:02:18.615740: step 41460, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.865 sec/batch; 69h:55m:09s remains)
INFO - root - 2017-12-09 17:02:27.197315: step 41470, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.857 sec/batch; 69h:15m:09s remains)
INFO - root - 2017-12-09 17:02:35.812650: step 41480, loss = 0.81, batch loss = 0.68 (9.1 examples/sec; 0.876 sec/batch; 70h:50m:33s remains)
INFO - root - 2017-12-09 17:02:44.553633: step 41490, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.880 sec/batch; 71h:10m:10s remains)
INFO - root - 2017-12-09 17:02:53.130241: step 41500, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.853 sec/batch; 68h:56m:32s remains)
2017-12-09 17:02:53.967319: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0018257779 -0.0018255429 -0.0018262181 -0.0018272316 -0.0018283252 -0.0018291294 -0.0018296987 -0.0018297475 -0.0018288514 -0.0018272527 -0.0018254765 -0.001824009 -0.0018227505 -0.0018217819 -0.0018213189][-0.0018256396 -0.0018255818 -0.0018266175 -0.0018280536 -0.0018295865 -0.001830867 -0.0018317362 -0.0018318613 -0.0018308258 -0.0018290293 -0.001826892 -0.0018248176 -0.0018230791 -0.0018218779 -0.0018212697][-0.0018257481 -0.0018259002 -0.0018272984 -0.0018291378 -0.0018311468 -0.0018330732 -0.0018344106 -0.0018347101 -0.0018336135 -0.0018317025 -0.0018292342 -0.0018265499 -0.0018242574 -0.0018227415 -0.001821924][-0.0018258246 -0.0018262033 -0.0018278528 -0.0018299562 -0.0018322979 -0.0018346905 -0.0018363218 -0.0018368091 -0.0018358644 -0.0018339603 -0.0018313545 -0.001828335 -0.0018256817 -0.001823875 -0.0018228177][-0.0018258531 -0.001826413 -0.0018282634 -0.001830542 -0.0018331159 -0.0018357832 -0.0018375075 -0.0018381244 -0.0018372907 -0.0018354747 -0.0018327525 -0.0018295054 -0.0018267924 -0.001824843 -0.0018236538][-0.0018258143 -0.0018264769 -0.0018283855 -0.0018307205 -0.0018333546 -0.0018359147 -0.0018375183 -0.0018382374 -0.0018376522 -0.0018360371 -0.0018332388 -0.0018300745 -0.0018274712 -0.0018255449 -0.0018243175][-0.001825665 -0.0018263583 -0.0018281455 -0.001830322 -0.0018326737 -0.0018348029 -0.0018360008 -0.0018366947 -0.0018366238 -0.0018353597 -0.0018328155 -0.0018299888 -0.0018276491 -0.001825858 -0.0018246663][-0.0018255537 -0.001826128 -0.0018276635 -0.0018294599 -0.0018312763 -0.001832725 -0.001833263 -0.0018337718 -0.0018342672 -0.0018335719 -0.0018315603 -0.0018292882 -0.0018273647 -0.0018258338 -0.0018247447][-0.0018254545 -0.0018258634 -0.001827102 -0.0018283903 -0.001829572 -0.0018302412 -0.0018300696 -0.001830318 -0.0018311609 -0.0018310394 -0.0018297334 -0.0018281867 -0.0018268037 -0.0018256138 -0.0018246677][-0.0018253709 -0.0018256316 -0.0018265733 -0.0018274357 -0.0018281052 -0.0018281385 -0.0018275089 -0.0018274955 -0.0018283767 -0.0018286267 -0.0018279529 -0.0018270663 -0.0018261606 -0.0018252893 -0.0018245506][-0.0018253246 -0.0018254546 -0.0018261076 -0.0018266365 -0.0018269087 -0.001826574 -0.0018257617 -0.0018255527 -0.0018262332 -0.0018266439 -0.0018264194 -0.0018260394 -0.0018255415 -0.0018249603 -0.0018244358][-0.0018252357 -0.0018252567 -0.0018256966 -0.0018259452 -0.0018259241 -0.0018254394 -0.0018246603 -0.0018243002 -0.0018247099 -0.0018251041 -0.0018251825 -0.0018252181 -0.0018250459 -0.0018247261 -0.0018243965][-0.0018251482 -0.0018250834 -0.0018253505 -0.0018253662 -0.001825112 -0.001824565 -0.0018238686 -0.0018234199 -0.0018235345 -0.0018238369 -0.001824129 -0.0018244943 -0.0018246074 -0.0018245172 -0.0018243482][-0.0018250948 -0.0018249725 -0.0018250749 -0.0018249369 -0.0018244996 -0.0018238694 -0.0018232182 -0.0018227705 -0.0018226985 -0.0018229333 -0.0018233637 -0.0018239031 -0.0018242312 -0.0018243166 -0.0018242815][-0.0018251684 -0.0018249343 -0.0018248764 -0.00182464 -0.0018241052 -0.0018234324 -0.001822792 -0.0018223834 -0.0018222602 -0.0018224361 -0.0018228809 -0.0018234282 -0.0018238603 -0.0018240769 -0.0018241827]]...]
INFO - root - 2017-12-09 17:03:02.437825: step 41510, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.853 sec/batch; 68h:56m:02s remains)
INFO - root - 2017-12-09 17:03:10.956302: step 41520, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 70h:03m:40s remains)
INFO - root - 2017-12-09 17:03:19.532272: step 41530, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 70h:12m:31s remains)
INFO - root - 2017-12-09 17:03:28.135952: step 41540, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.857 sec/batch; 69h:18m:09s remains)
INFO - root - 2017-12-09 17:03:36.681507: step 41550, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.845 sec/batch; 68h:19m:39s remains)
INFO - root - 2017-12-09 17:03:45.044197: step 41560, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 70h:05m:36s remains)
INFO - root - 2017-12-09 17:03:53.606511: step 41570, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.849 sec/batch; 68h:35m:00s remains)
INFO - root - 2017-12-09 17:04:02.276339: step 41580, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.886 sec/batch; 71h:35m:12s remains)
INFO - root - 2017-12-09 17:04:10.952138: step 41590, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.856 sec/batch; 69h:08m:30s remains)
INFO - root - 2017-12-09 17:04:19.594239: step 41600, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.830 sec/batch; 67h:06m:19s remains)
2017-12-09 17:04:20.611023: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0018153733 -0.0018145971 -0.0018144982 -0.0018144711 -0.0018143951 -0.0018143113 -0.0018142435 -0.0018142315 -0.0018142577 -0.0018142732 -0.0018142735 -0.0018142369 -0.001814187 -0.0018141413 -0.0018141133][-0.0018142578 -0.0018134825 -0.0018133828 -0.0018132784 -0.0018131174 -0.0018129522 -0.001812863 -0.0018128779 -0.0018129506 -0.0018130527 -0.0018131387 -0.0018131771 -0.0018131969 -0.0018132145 -0.0018132281][-0.0018140543 -0.0018132889 -0.0018130779 -0.0018127998 -0.0018124662 -0.0018121608 -0.0018120243 -0.0018120713 -0.0018122179 -0.0018124229 -0.0018126137 -0.0018127471 -0.0018128408 -0.0018129174 -0.001812984][-0.0018140867 -0.0018132216 -0.0018128909 -0.0018124803 -0.0018119633 -0.0018114694 -0.0018111798 -0.0018111606 -0.0018113232 -0.001811613 -0.0018119124 -0.0018121717 -0.001812367 -0.0018125209 -0.0018126626][-0.0018142738 -0.0018134397 -0.001813176 -0.0018127753 -0.00181211 -0.001811318 -0.0018107077 -0.0018104207 -0.0018104677 -0.0018107687 -0.0018111547 -0.0018115176 -0.0018118077 -0.0018120371 -0.0018122654][-0.0018146441 -0.0018141045 -0.0018142723 -0.0018141854 -0.0018135065 -0.0018123888 -0.0018112384 -0.0018103441 -0.001809939 -0.0018100545 -0.0018104208 -0.0018108379 -0.0018112255 -0.001811568 -0.0018119][-0.0018151302 -0.0018153414 -0.0018162562 -0.0018167984 -0.0018162843 -0.0018148741 -0.0018130494 -0.001811216 -0.0018099858 -0.0018096499 -0.00180986 -0.0018102744 -0.0018107772 -0.0018112824 -0.0018117317][-0.0018156585 -0.0018167997 -0.0018186409 -0.0018200821 -0.0018200718 -0.001818749 -0.0018164769 -0.0018136589 -0.0018112179 -0.0018100364 -0.0018097433 -0.001809944 -0.001810467 -0.001811134 -0.001811711][-0.0018160566 -0.0018181668 -0.0018210605 -0.0018235566 -0.00182451 -0.0018238623 -0.0018216671 -0.0018183488 -0.0018147341 -0.0018120732 -0.0018107005 -0.0018101977 -0.0018103812 -0.0018110718 -0.001811757][-0.0018162453 -0.001819131 -0.0018230267 -0.0018266288 -0.0018286748 -0.0018289327 -0.0018271403 -0.0018236436 -0.00181935 -0.0018154717 -0.0018128081 -0.0018112417 -0.0018107094 -0.0018111463 -0.0018118183][-0.0018161546 -0.0018194136 -0.0018239721 -0.0018283472 -0.0018312945 -0.0018323698 -0.0018310631 -0.0018277296 -0.0018232332 -0.001818675 -0.0018150158 -0.0018125067 -0.0018112644 -0.0018113086 -0.0018119183][-0.0018158501 -0.0018190047 -0.0018236078 -0.0018282273 -0.0018315242 -0.0018329478 -0.001831988 -0.0018290596 -0.0018248977 -0.00182041 -0.0018164951 -0.0018136048 -0.0018119224 -0.0018115848 -0.001812065][-0.0018153748 -0.0018179019 -0.00182188 -0.0018259536 -0.001829098 -0.0018306889 -0.0018301845 -0.001827875 -0.0018244429 -0.0018206013 -0.0018169704 -0.0018141482 -0.0018123846 -0.001811865 -0.0018121996][-0.0018149628 -0.0018165959 -0.0018195603 -0.0018227291 -0.0018253741 -0.0018269628 -0.0018269785 -0.0018253606 -0.0018226746 -0.001819563 -0.001816554 -0.0018141516 -0.0018126367 -0.0018121215 -0.0018123137][-0.0018147068 -0.0018154895 -0.001817393 -0.0018195885 -0.0018214884 -0.0018227196 -0.0018229289 -0.001821939 -0.0018200455 -0.001817771 -0.0018156244 -0.0018138998 -0.0018127777 -0.0018123779 -0.0018124911]]...]
INFO - root - 2017-12-09 17:04:29.132683: step 41610, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.843 sec/batch; 68h:07m:06s remains)
INFO - root - 2017-12-09 17:04:37.818145: step 41620, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.884 sec/batch; 71h:24m:59s remains)
INFO - root - 2017-12-09 17:04:46.462176: step 41630, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.856 sec/batch; 69h:07m:21s remains)
INFO - root - 2017-12-09 17:04:55.094914: step 41640, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.838 sec/batch; 67h:41m:32s remains)
INFO - root - 2017-12-09 17:05:03.832123: step 41650, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.885 sec/batch; 71h:31m:39s remains)
INFO - root - 2017-12-09 17:05:12.108298: step 41660, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.834 sec/batch; 67h:22m:10s remains)
INFO - root - 2017-12-09 17:05:20.400863: step 41670, loss = 0.82, batch loss = 0.69 (9.7 examples/sec; 0.829 sec/batch; 66h:57m:14s remains)
INFO - root - 2017-12-09 17:05:29.098171: step 41680, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.900 sec/batch; 72h:43m:07s remains)
INFO - root - 2017-12-09 17:05:37.723438: step 41690, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.863 sec/batch; 69h:42m:10s remains)
INFO - root - 2017-12-09 17:05:46.427228: step 41700, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.881 sec/batch; 71h:07m:57s remains)
2017-12-09 17:05:47.287458: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.13065825 0.12751622 0.1230831 0.11796809 0.11292624 0.10736379 0.10111699 0.092808805 0.081594639 0.0675147 0.051390074 0.035442963 0.020990033 0.0098341014 0.0026205655][0.13766339 0.13575892 0.13259757 0.12857282 0.12429499 0.11899006 0.11246313 0.10323932 0.090503573 0.074593805 0.056466304 0.038762867 0.022867976 0.010737304 0.0029687749][0.1422378 0.14205174 0.14069153 0.13805127 0.134677 0.12948264 0.12244438 0.11219423 0.097912535 0.080148086 0.06020648 0.041064728 0.02407402 0.011268185 0.0031271724][0.14466234 0.14679188 0.14773306 0.146862 0.14465284 0.13983226 0.13229087 0.12074959 0.10470048 0.085142724 0.063440859 0.042812884 0.024738789 0.011402186 0.0031002637][0.14595564 0.15029258 0.15313251 0.15398878 0.15302801 0.14867267 0.14086989 0.12837037 0.11083539 0.0896515 0.06637083 0.044447675 0.025388351 0.011484251 0.0029762744][0.14584531 0.15244828 0.15729272 0.15975007 0.15993686 0.15606016 0.14817528 0.13493195 0.11625619 0.093883753 0.069337808 0.046288364 0.026284583 0.011779399 0.0029962824][0.14418045 0.1526465 0.15909043 0.16321671 0.16484267 0.16188288 0.15443209 0.14092121 0.12152534 0.098144315 0.072460063 0.048389494 0.02746469 0.012316588 0.0031711787][0.14109549 0.150759 0.15806451 0.1631656 0.16578826 0.16371007 0.15698865 0.14412498 0.12502617 0.10143804 0.075127035 0.050235942 0.028552908 0.012820405 0.003334055][0.13773359 0.14805515 0.15567088 0.16126618 0.16462602 0.16346265 0.15767151 0.14554319 0.12704536 0.10398743 0.077766076 0.052521933 0.030217001 0.013820184 0.0038033274][0.13493593 0.14496233 0.15199192 0.15748817 0.16124259 0.16071104 0.15586658 0.14486465 0.12750931 0.10520073 0.079313226 0.054185979 0.031754952 0.014978157 0.0044392832][0.13213876 0.14122008 0.1471948 0.15220654 0.15598354 0.15573783 0.15160099 0.14171931 0.12552123 0.104265 0.079180613 0.054673422 0.032572187 0.015777703 0.0050067957][0.12898923 0.13693278 0.14155111 0.14549007 0.14856881 0.14855452 0.14525057 0.13653614 0.12183438 0.10193478 0.077972822 0.054175954 0.032550521 0.01606939 0.005307531][0.1256358 0.13165614 0.13481969 0.13752943 0.13983637 0.13959958 0.13682881 0.12930912 0.11612425 0.097946234 0.075571209 0.052928351 0.03205692 0.015994223 0.0053670211][0.12146799 0.12586781 0.12721214 0.12871578 0.13039495 0.13027383 0.12835059 0.12214993 0.1107186 0.093978561 0.073039465 0.051647764 0.031662408 0.016068809 0.0055276128][0.11657823 0.11978195 0.11974727 0.12015043 0.1207945 0.12012298 0.11857333 0.11361092 0.10396297 0.08906544 0.070005737 0.050099373 0.031042358 0.015988637 0.0056494614]]...]
INFO - root - 2017-12-09 17:05:55.890375: step 41710, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.835 sec/batch; 67h:27m:06s remains)
INFO - root - 2017-12-09 17:06:04.526718: step 41720, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.893 sec/batch; 72h:07m:31s remains)
INFO - root - 2017-12-09 17:06:13.191247: step 41730, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 70h:04m:20s remains)
INFO - root - 2017-12-09 17:06:22.002250: step 41740, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.878 sec/batch; 70h:55m:04s remains)
INFO - root - 2017-12-09 17:06:30.819337: step 41750, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.873 sec/batch; 70h:30m:51s remains)
INFO - root - 2017-12-09 17:06:39.160022: step 41760, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.874 sec/batch; 70h:33m:40s remains)
INFO - root - 2017-12-09 17:06:47.627187: step 41770, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.834 sec/batch; 67h:19m:10s remains)
INFO - root - 2017-12-09 17:06:56.105304: step 41780, loss = 0.82, batch loss = 0.69 (9.7 examples/sec; 0.824 sec/batch; 66h:30m:08s remains)
INFO - root - 2017-12-09 17:07:04.521090: step 41790, loss = 0.83, batch loss = 0.70 (9.6 examples/sec; 0.834 sec/batch; 67h:20m:25s remains)
INFO - root - 2017-12-09 17:07:13.096385: step 41800, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.840 sec/batch; 67h:50m:05s remains)
2017-12-09 17:07:13.981192: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0017885728 -0.0017859812 -0.0017855689 -0.0017855462 -0.0017855591 -0.0017855058 -0.0017854651 -0.0017854631 -0.0017855321 -0.0017856521 -0.0017857528 -0.001785868 -0.0017859576 -0.0017860025 -0.0017860688][-0.0017870427 -0.001784356 -0.0017839597 -0.001783972 -0.0017840221 -0.0017840501 -0.0017840612 -0.0017840929 -0.0017841859 -0.0017843063 -0.0017843849 -0.0017844706 -0.0017845491 -0.0017846264 -0.0017847309][-0.0017868824 -0.0017842456 -0.0017838299 -0.0017839004 -0.0017840057 -0.0017840965 -0.0017841264 -0.0017841635 -0.0017842205 -0.0017842576 -0.0017842541 -0.0017842553 -0.0017843081 -0.0017844009 -0.0017845177][-0.0017868127 -0.0017842547 -0.0017838816 -0.0017840361 -0.0017842569 -0.0017844301 -0.0017845042 -0.00178456 -0.0017845819 -0.0017845167 -0.001784384 -0.0017842829 -0.0017842849 -0.0017843533 -0.0017844451][-0.0017866449 -0.001783992 -0.0017836168 -0.0017838483 -0.001784166 -0.0017844299 -0.0017845663 -0.00178464 -0.0017846416 -0.0017844918 -0.0017842676 -0.0017841019 -0.0017840691 -0.001784086 -0.0017841043][-0.0017862838 -0.0017834711 -0.0017830273 -0.0017832723 -0.0017836336 -0.0017839173 -0.00178405 -0.0017840964 -0.0017840723 -0.0017838818 -0.0017836303 -0.001783496 -0.0017835232 -0.0017835846 -0.0017836194][-0.0017859883 -0.0017831861 -0.0017827173 -0.0017829692 -0.0017833347 -0.0017836026 -0.001783684 -0.0017836313 -0.0017835338 -0.0017832855 -0.00178299 -0.001782839 -0.0017829186 -0.001783067 -0.0017831931][-0.0017862911 -0.0017835535 -0.0017830309 -0.0017831795 -0.0017834255 -0.0017836002 -0.0017836135 -0.0017834833 -0.0017833066 -0.0017830362 -0.0017827523 -0.0017826175 -0.0017827145 -0.001782872 -0.0017830089][-0.0017865148 -0.0017837766 -0.0017831606 -0.0017830734 -0.0017830604 -0.0017830286 -0.0017829475 -0.0017827642 -0.0017825546 -0.0017823268 -0.001782127 -0.0017820878 -0.0017822653 -0.0017824995 -0.0017827061][-0.0017863144 -0.0017836691 -0.0017828858 -0.0017824664 -0.0017820789 -0.001781746 -0.0017815102 -0.0017813232 -0.00178122 -0.0017811968 -0.0017812267 -0.0017813904 -0.0017817228 -0.001782109 -0.0017824828][-0.0017860202 -0.0017835641 -0.0017827082 -0.0017820846 -0.0017814306 -0.001780899 -0.0017805044 -0.0017802855 -0.00178026 -0.0017804042 -0.0017805801 -0.0017808639 -0.0017813096 -0.0017818069 -0.0017823228][-0.0017861967 -0.001783809 -0.0017829727 -0.0017822455 -0.0017814222 -0.0017807325 -0.0017801919 -0.0017799025 -0.0017798669 -0.0017800475 -0.0017802522 -0.0017805164 -0.0017809455 -0.001781448 -0.0017819969][-0.0017865752 -0.0017841839 -0.0017832817 -0.0017824754 -0.0017815809 -0.0017808253 -0.0017802415 -0.0017799832 -0.0017800375 -0.001780294 -0.0017805093 -0.0017807243 -0.0017810379 -0.0017814059 -0.0017818488][-0.0017869156 -0.0017844242 -0.0017835182 -0.0017827947 -0.0017819508 -0.0017812276 -0.0017806678 -0.0017804385 -0.001780538 -0.0017807785 -0.0017809477 -0.0017810902 -0.001781325 -0.0017815997 -0.0017819239][-0.0017874877 -0.0017850302 -0.0017841435 -0.0017835703 -0.0017828621 -0.0017821972 -0.0017816507 -0.001781367 -0.0017813932 -0.0017815294 -0.0017815632 -0.0017815802 -0.0017817001 -0.0017818585 -0.0017820423]]...]
INFO - root - 2017-12-09 17:07:22.503811: step 41810, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.880 sec/batch; 71h:01m:25s remains)
INFO - root - 2017-12-09 17:07:31.103528: step 41820, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.831 sec/batch; 67h:03m:44s remains)
INFO - root - 2017-12-09 17:07:39.805063: step 41830, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.879 sec/batch; 70h:58m:16s remains)
INFO - root - 2017-12-09 17:07:48.614569: step 41840, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.839 sec/batch; 67h:42m:45s remains)
INFO - root - 2017-12-09 17:07:57.109838: step 41850, loss = 0.82, batch loss = 0.69 (9.8 examples/sec; 0.817 sec/batch; 65h:57m:24s remains)
INFO - root - 2017-12-09 17:08:05.229520: step 41860, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.859 sec/batch; 69h:23m:06s remains)
INFO - root - 2017-12-09 17:08:13.776811: step 41870, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 70h:20m:19s remains)
INFO - root - 2017-12-09 17:08:22.450442: step 41880, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 70h:08m:55s remains)
INFO - root - 2017-12-09 17:08:31.080881: step 41890, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.898 sec/batch; 72h:27m:17s remains)
INFO - root - 2017-12-09 17:08:39.679522: step 41900, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.870 sec/batch; 70h:11m:24s remains)
2017-12-09 17:08:40.582011: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.22084773 0.22399098 0.22614884 0.22473997 0.2206945 0.21829556 0.21726462 0.21559364 0.21063395 0.20404284 0.19917001 0.193665 0.18383309 0.17263292 0.16362935][0.21645147 0.22233196 0.2260731 0.22589731 0.22253586 0.21982192 0.21877636 0.21648747 0.21250911 0.2081503 0.2063913 0.20371453 0.19547993 0.18516119 0.17685619][0.2099112 0.21721171 0.22179289 0.22184852 0.21826783 0.21505092 0.21339768 0.2104301 0.20687607 0.20490564 0.20661479 0.20710292 0.20095496 0.19198616 0.18510975][0.20264632 0.21117191 0.21664661 0.21744341 0.2137256 0.20990556 0.20686127 0.20250322 0.1991578 0.19898053 0.2037303 0.2072497 0.20398384 0.19661161 0.19112998][0.19528261 0.2038116 0.20962168 0.21155781 0.2084271 0.204564 0.20059504 0.19487476 0.19082947 0.19140796 0.19804816 0.20391744 0.20339046 0.19771793 0.19356282][0.18875296 0.19601682 0.20151377 0.20404093 0.2014119 0.19828254 0.19383934 0.18751955 0.18236132 0.18250024 0.18957774 0.19657649 0.19824399 0.19476582 0.19175166][0.18172085 0.18748097 0.19143647 0.19394197 0.19191261 0.18924393 0.18529963 0.17918095 0.17381793 0.1728791 0.17915815 0.18679081 0.1898848 0.18866906 0.18643942][0.17565696 0.18010655 0.18260987 0.18392003 0.18120924 0.17846659 0.17432554 0.16897751 0.16369674 0.16182148 0.16690956 0.17470917 0.17925209 0.18009068 0.1784198][0.16804527 0.17216691 0.17385231 0.17434232 0.17152512 0.16805875 0.16342187 0.15855794 0.15359092 0.15142693 0.15540293 0.16295026 0.16874364 0.17102905 0.16992536][0.15991504 0.16367616 0.1647446 0.16481812 0.16232535 0.15890424 0.15476431 0.15087916 0.14661047 0.14435974 0.14710771 0.15334998 0.15876092 0.16165613 0.1608205][0.1505813 0.15372169 0.15471967 0.15489286 0.15298983 0.15054993 0.14735895 0.14522447 0.14256757 0.14063165 0.14239097 0.14704055 0.15146264 0.15391648 0.15271857][0.14140861 0.1440506 0.1447905 0.14506887 0.14387651 0.14252299 0.14098069 0.14011723 0.13923344 0.13793714 0.13927212 0.14263567 0.14605585 0.14746882 0.14577933][0.13516197 0.136805 0.13667472 0.13666503 0.13579303 0.13493073 0.13422088 0.13441856 0.13479075 0.13458484 0.13607389 0.13867348 0.14102617 0.14163958 0.1400149][0.1306887 0.13076472 0.12934391 0.12836638 0.12743284 0.12726982 0.12751305 0.12860185 0.13014063 0.13120773 0.13326918 0.13564357 0.13759463 0.13812518 0.1371579][0.13091499 0.12889826 0.12536083 0.12263757 0.12087243 0.12063687 0.1210926 0.12262924 0.12484126 0.12660694 0.12897025 0.13126627 0.13359705 0.13485852 0.1349625]]...]
INFO - root - 2017-12-09 17:08:49.315024: step 41910, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 70h:02m:54s remains)
INFO - root - 2017-12-09 17:08:57.935471: step 41920, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.886 sec/batch; 71h:28m:50s remains)
INFO - root - 2017-12-09 17:09:06.581646: step 41930, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.857 sec/batch; 69h:10m:16s remains)
INFO - root - 2017-12-09 17:09:15.248571: step 41940, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.853 sec/batch; 68h:48m:54s remains)
INFO - root - 2017-12-09 17:09:23.899998: step 41950, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.885 sec/batch; 71h:27m:31s remains)
INFO - root - 2017-12-09 17:09:32.252789: step 41960, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.838 sec/batch; 67h:36m:15s remains)
INFO - root - 2017-12-09 17:09:40.672662: step 41970, loss = 0.82, batch loss = 0.69 (9.8 examples/sec; 0.816 sec/batch; 65h:51m:33s remains)
INFO - root - 2017-12-09 17:09:49.351102: step 41980, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.880 sec/batch; 71h:00m:53s remains)
INFO - root - 2017-12-09 17:09:57.927055: step 41990, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.856 sec/batch; 69h:05m:36s remains)
INFO - root - 2017-12-09 17:10:06.518615: step 42000, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.856 sec/batch; 69h:02m:57s remains)
2017-12-09 17:10:07.451451: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0018120618 -0.0018099931 -0.0018086293 -0.0018071797 -0.0018056917 -0.0018040919 -0.001802757 -0.0018016053 -0.0018005174 -0.0017997462 -0.0017992081 -0.0017990785 -0.001799156 -0.0017993919 -0.0017996385][-0.0018141 -0.0018123123 -0.0018114477 -0.0018101033 -0.001808461 -0.0018064837 -0.0018045343 -0.0018027548 -0.0018009447 -0.0017994738 -0.0017984085 -0.0017978852 -0.001797894 -0.0017979827 -0.0017982455][-0.0018158641 -0.0018147847 -0.0018147386 -0.0018140353 -0.0018125533 -0.0018104062 -0.0018078615 -0.0018052246 -0.0018025645 -0.0018002909 -0.0017987079 -0.0017978597 -0.0017976738 -0.0017977197 -0.0017979359][-0.0018163768 -0.0018161947 -0.0018174481 -0.0018177516 -0.0018167072 -0.0018144385 -0.0018114854 -0.0018080665 -0.0018044877 -0.0018012997 -0.0017992503 -0.0017980367 -0.0017977211 -0.0017977412 -0.0017979563][-0.0018151492 -0.0018165462 -0.001819632 -0.0018214326 -0.0018209288 -0.0018181269 -0.0018146839 -0.0018105126 -0.0018060486 -0.0018020235 -0.0017994884 -0.0017982883 -0.0017977747 -0.0017978358 -0.0017981082][-0.0018124486 -0.0018155895 -0.0018204986 -0.0018238216 -0.0018240295 -0.0018212404 -0.0018175726 -0.0018126097 -0.0018073815 -0.0018027729 -0.0017997265 -0.0017982115 -0.0017976218 -0.0017979391 -0.0017983147][-0.0018092667 -0.0018142326 -0.0018202493 -0.001824309 -0.0018247385 -0.0018220774 -0.0018184404 -0.0018133243 -0.0018078338 -0.0018028951 -0.0017998088 -0.0017983033 -0.0017977165 -0.0017981417 -0.0017985315][-0.00180695 -0.001812948 -0.0018192352 -0.0018230405 -0.0018234837 -0.001820969 -0.0018176852 -0.0018130812 -0.0018078134 -0.001803026 -0.0018000918 -0.0017987606 -0.0017980726 -0.0017984688 -0.0017987845][-0.0018059119 -0.001811655 -0.0018171443 -0.0018202767 -0.0018207324 -0.0018187979 -0.0018162174 -0.0018124987 -0.0018080386 -0.00180376 -0.0018008088 -0.0017994026 -0.0017984447 -0.0017987365 -0.0017989984][-0.0018064852 -0.0018106363 -0.0018148022 -0.0018170351 -0.001817279 -0.0018157405 -0.0018136612 -0.001810643 -0.0018070572 -0.0018035746 -0.0018010119 -0.001799703 -0.0017985357 -0.0017987753 -0.0017990826][-0.0018076429 -0.0018096969 -0.0018121412 -0.0018132534 -0.0018131905 -0.0018121779 -0.0018105323 -0.0018081194 -0.0018053448 -0.0018027177 -0.0018006215 -0.0017994179 -0.0017985721 -0.00179883 -0.0017990906][-0.0018086025 -0.0018088447 -0.0018096548 -0.0018094268 -0.0018088243 -0.001807961 -0.0018067132 -0.0018050892 -0.0018031151 -0.0018012 -0.0017996252 -0.0017988724 -0.0017984166 -0.0017986851 -0.0017989392][-0.0018084244 -0.001807154 -0.0018065228 -0.0018055221 -0.0018047174 -0.0018040347 -0.0018031722 -0.0018021625 -0.0018008511 -0.0017995572 -0.0017985837 -0.001798282 -0.0017981225 -0.0017984004 -0.0017987202][-0.0018070319 -0.0018047512 -0.0018032622 -0.0018020686 -0.0018012538 -0.0018006718 -0.0018000043 -0.0017992571 -0.0017983579 -0.0017976171 -0.0017972706 -0.0017974224 -0.0017976724 -0.0017981221 -0.0017984855][-0.0018050352 -0.0018023176 -0.0018007139 -0.0017995243 -0.0017988313 -0.0017981682 -0.0017975656 -0.0017969591 -0.0017963517 -0.0017959771 -0.0017960967 -0.0017966685 -0.0017972775 -0.0017979133 -0.0017983863]]...]
INFO - root - 2017-12-09 17:10:16.148017: step 42010, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.882 sec/batch; 71h:11m:54s remains)
INFO - root - 2017-12-09 17:10:24.914741: step 42020, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 69h:44m:27s remains)
INFO - root - 2017-12-09 17:10:33.749121: step 42030, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.875 sec/batch; 70h:37m:29s remains)
INFO - root - 2017-12-09 17:10:42.389635: step 42040, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.898 sec/batch; 72h:25m:59s remains)
INFO - root - 2017-12-09 17:10:51.062610: step 42050, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.841 sec/batch; 67h:52m:17s remains)
INFO - root - 2017-12-09 17:10:59.513639: step 42060, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.873 sec/batch; 70h:26m:07s remains)
INFO - root - 2017-12-09 17:11:07.890271: step 42070, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.849 sec/batch; 68h:29m:42s remains)
INFO - root - 2017-12-09 17:11:16.563046: step 42080, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.845 sec/batch; 68h:09m:22s remains)
INFO - root - 2017-12-09 17:11:25.254953: step 42090, loss = 0.83, batch loss = 0.70 (9.2 examples/sec; 0.871 sec/batch; 70h:16m:56s remains)
INFO - root - 2017-12-09 17:11:33.939067: step 42100, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.845 sec/batch; 68h:10m:10s remains)
2017-12-09 17:11:34.820771: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0014197772 -0.0014686958 -0.0014696345 -0.0014477166 -0.0012793776 -0.00098996493 -0.00067289989 -0.00046109909 -0.00041965011 -0.00050398859 -0.000752197 -0.001148039 -0.0015425701 -0.0017776835 -0.001833393][-0.0010334367 -0.0010838723 -0.0011700909 -0.001217674 -0.0010985639 -0.00082706672 -0.0005050071 -0.0002901518 -0.00027577754 -0.00040254591 -0.00069130375 -0.001114775 -0.0015251311 -0.0017722574 -0.0018307781][-0.00044919539 -0.00048897695 -0.00067481468 -0.00083453604 -0.0008349712 -0.00064642262 -0.00036860444 -0.00018288835 -0.00020128873 -0.00037221785 -0.00069242774 -0.0011251178 -0.0015283102 -0.0017731476 -0.0018287472][0.00018983753 0.000193618 -7.3805568e-05 -0.00035455008 -0.00052666233 -0.00049450039 -0.00031132321 -0.00016777776 -0.00021041255 -0.00041061628 -0.00074516179 -0.0011624778 -0.0015400448 -0.0017746654 -0.0018271107][0.00074604037 0.00082250475 0.00051573338 0.0001348923 -0.00021989702 -0.00038610934 -0.00032164622 -0.00021505833 -0.00024648546 -0.000443127 -0.00078029279 -0.0011767114 -0.0015316245 -0.0017602631 -0.0018201019][0.0011873327 0.0013549905 0.0010489668 0.00059886673 8.7163527e-05 -0.00028289377 -0.00035480782 -0.00028664072 -0.00027796126 -0.00043781043 -0.00075173308 -0.0011137078 -0.0014505982 -0.0016908722 -0.0017839804][0.0014062058 0.0016566906 0.0013850536 0.00091342116 0.00030420814 -0.00021801272 -0.00039752794 -0.00035045482 -0.00028207968 -0.00037687819 -0.0006447885 -0.00096942781 -0.0012979495 -0.0015586572 -0.0017019432][0.0013233463 0.0016386698 0.0014247114 0.0009829998 0.00036441628 -0.00021340465 -0.00044471631 -0.00040332391 -0.00028378877 -0.0003102927 -0.0005171618 -0.00079935975 -0.0011204663 -0.0014042964 -0.0015994234][0.000925438 0.0012448833 0.0010873338 0.00071097235 0.00016247504 -0.00037259003 -0.00058941753 -0.00053467229 -0.00037406513 -0.00033491012 -0.00047509733 -0.00070800493 -0.001007712 -0.0012948958 -0.0015189503][0.00037865876 0.00064385077 0.00052163214 0.00022345176 -0.00021095295 -0.00063889381 -0.00080092577 -0.00072859193 -0.00055265357 -0.0004722958 -0.00055113912 -0.00073219475 -0.00099941506 -0.0012676304 -0.0014912989][-0.00019557693 -1.0549556e-05 -0.00010986964 -0.00033075956 -0.00064002885 -0.00093332451 -0.0010316144 -0.000951756 -0.00079142663 -0.00069912791 -0.00073232444 -0.00086811953 -0.0010902379 -0.0013189369 -0.0015173136][-0.00072061422 -0.0006261724 -0.0007088416 -0.000857914 -0.0010461621 -0.0012111345 -0.0012463322 -0.001167524 -0.0010412069 -0.000961033 -0.00097334123 -0.0010731552 -0.0012446723 -0.0014218695 -0.001574378][-0.0011829684 -0.0011522295 -0.0012127283 -0.0012953109 -0.0013869114 -0.0014569636 -0.0014560464 -0.0013946349 -0.0013109535 -0.0012549791 -0.0012575116 -0.0013216714 -0.0014362964 -0.0015533437 -0.0016524957][-0.0015391131 -0.0015511216 -0.0015890258 -0.0016237 -0.0016526915 -0.0016679795 -0.0016514418 -0.0016103041 -0.0015630202 -0.0015311086 -0.0015299569 -0.0015643659 -0.0016263861 -0.0016856074 -0.0017331911][-0.0017383419 -0.0017541952 -0.0017723502 -0.0017837052 -0.0017899437 -0.001791055 -0.0017818984 -0.0017634957 -0.0017420327 -0.001725864 -0.0017227014 -0.001735067 -0.0017587392 -0.0017792478 -0.0017939443]]...]
INFO - root - 2017-12-09 17:11:43.478749: step 42110, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.860 sec/batch; 69h:20m:04s remains)
INFO - root - 2017-12-09 17:11:52.074706: step 42120, loss = 0.81, batch loss = 0.68 (9.4 examples/sec; 0.851 sec/batch; 68h:36m:16s remains)
INFO - root - 2017-12-09 17:12:00.694206: step 42130, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 70h:02m:10s remains)
INFO - root - 2017-12-09 17:12:09.428275: step 42140, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.852 sec/batch; 68h:41m:20s remains)
INFO - root - 2017-12-09 17:12:18.211915: step 42150, loss = 0.83, batch loss = 0.70 (9.1 examples/sec; 0.877 sec/batch; 70h:45m:47s remains)
INFO - root - 2017-12-09 17:12:26.628903: step 42160, loss = 0.82, batch loss = 0.69 (9.8 examples/sec; 0.814 sec/batch; 65h:41m:21s remains)
INFO - root - 2017-12-09 17:12:34.960669: step 42170, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.875 sec/batch; 70h:36m:18s remains)
INFO - root - 2017-12-09 17:12:43.554337: step 42180, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.877 sec/batch; 70h:41m:09s remains)
INFO - root - 2017-12-09 17:12:52.356274: step 42190, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.837 sec/batch; 67h:31m:46s remains)
INFO - root - 2017-12-09 17:13:01.139971: step 42200, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.882 sec/batch; 71h:09m:26s remains)
2017-12-09 17:13:01.982138: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0017999643 -0.0018003817 -0.0018034469 -0.0018081609 -0.0018135238 -0.001818424 -0.0018216411 -0.0018223366 -0.0018210432 -0.0018186949 -0.0018159552 -0.0018138164 -0.0018123704 -0.0018112947 -0.0018104257][-0.0017983016 -0.0017983645 -0.0018011741 -0.0018055971 -0.0018105202 -0.0018151085 -0.0018181014 -0.0018187736 -0.001817709 -0.0018155549 -0.0018130288 -0.001811131 -0.0018098911 -0.0018089202 -0.0018081955][-0.0017978294 -0.0017976835 -0.0018000636 -0.00180382 -0.0018078526 -0.0018116989 -0.0018142548 -0.0018148124 -0.0018139743 -0.0018121272 -0.0018098411 -0.0018080506 -0.0018068207 -0.0018059815 -0.0018055298][-0.0017978197 -0.0017974663 -0.0017992174 -0.001801946 -0.0018048208 -0.001807662 -0.0018096243 -0.0018099927 -0.0018093543 -0.0018078985 -0.0018059631 -0.0018043412 -0.0018031562 -0.001802515 -0.0018024577][-0.0017978981 -0.0017973409 -0.0017984094 -0.0018000733 -0.0018018142 -0.0018035754 -0.0018047421 -0.0018049023 -0.001804444 -0.0018033427 -0.0018018549 -0.0018005427 -0.001799611 -0.0017992235 -0.0017995208][-0.0017980579 -0.0017972351 -0.0017977103 -0.0017985111 -0.0017993959 -0.00180042 -0.0018009763 -0.0018009201 -0.0018005874 -0.001799737 -0.0017986404 -0.0017975973 -0.0017969675 -0.0017968529 -0.0017974278][-0.0017980585 -0.0017968904 -0.0017968587 -0.0017970034 -0.0017973116 -0.0017978832 -0.0017982135 -0.0017981664 -0.0017979782 -0.0017974348 -0.0017967499 -0.0017959563 -0.0017954358 -0.0017954762 -0.0017961585][-0.001798003 -0.0017965246 -0.0017961718 -0.001795962 -0.0017959628 -0.0017962442 -0.0017964704 -0.0017964382 -0.0017962385 -0.0017958652 -0.0017955044 -0.0017950108 -0.0017946044 -0.0017948165 -0.0017955841][-0.0017978116 -0.0017961753 -0.0017957386 -0.0017953439 -0.0017951765 -0.0017953289 -0.0017955066 -0.0017955526 -0.0017953292 -0.0017950361 -0.0017948711 -0.0017945368 -0.0017942343 -0.0017945037 -0.0017951983][-0.0017974882 -0.0017959647 -0.0017956041 -0.0017952411 -0.0017950735 -0.0017952472 -0.0017955005 -0.001795665 -0.0017954438 -0.0017950824 -0.0017948699 -0.00179447 -0.0017941148 -0.0017942883 -0.0017948253][-0.0017974032 -0.0017960204 -0.0017957813 -0.0017956026 -0.0017955861 -0.0017958151 -0.0017961408 -0.0017963519 -0.0017960874 -0.0017956111 -0.0017951722 -0.0017946216 -0.0017941322 -0.0017941331 -0.0017944666][-0.0017973105 -0.0017960675 -0.0017959594 -0.00179593 -0.001795982 -0.0017961792 -0.0017964564 -0.0017966443 -0.0017964062 -0.0017959082 -0.001795354 -0.0017947357 -0.0017941756 -0.0017939586 -0.0017940103][-0.0017970526 -0.0017959601 -0.0017960065 -0.0017960607 -0.0017960963 -0.0017961791 -0.0017963257 -0.0017964302 -0.0017962786 -0.0017958902 -0.001795373 -0.0017948047 -0.0017942613 -0.0017938464 -0.0017936158][-0.0017971508 -0.0017960811 -0.0017961559 -0.0017962293 -0.0017961863 -0.0017961235 -0.0017961388 -0.0017961551 -0.0017960343 -0.0017957351 -0.0017952961 -0.0017947846 -0.0017942932 -0.0017938224 -0.0017935017][-0.0017972648 -0.0017962659 -0.0017963046 -0.001796316 -0.0017961564 -0.0017959399 -0.0017958311 -0.0017957619 -0.0017956452 -0.00179543 -0.0017951005 -0.0017947006 -0.0017942882 -0.001793888 -0.0017936304]]...]
INFO - root - 2017-12-09 17:13:10.553618: step 42210, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.832 sec/batch; 67h:05m:51s remains)
INFO - root - 2017-12-09 17:13:19.089023: step 42220, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.842 sec/batch; 67h:54m:42s remains)
INFO - root - 2017-12-09 17:13:27.587914: step 42230, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.860 sec/batch; 69h:18m:48s remains)
INFO - root - 2017-12-09 17:13:36.249969: step 42240, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.886 sec/batch; 71h:26m:18s remains)
INFO - root - 2017-12-09 17:13:44.992436: step 42250, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.850 sec/batch; 68h:32m:33s remains)
INFO - root - 2017-12-09 17:13:53.447225: step 42260, loss = 0.82, batch loss = 0.69 (11.3 examples/sec; 0.709 sec/batch; 57h:07m:24s remains)
INFO - root - 2017-12-09 17:14:02.010519: step 42270, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 69h:53m:53s remains)
INFO - root - 2017-12-09 17:14:10.501454: step 42280, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 69h:57m:07s remains)
INFO - root - 2017-12-09 17:14:19.038883: step 42290, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.830 sec/batch; 66h:55m:23s remains)
INFO - root - 2017-12-09 17:14:27.628063: step 42300, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.861 sec/batch; 69h:24m:39s remains)
2017-12-09 17:14:28.462414: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0018397955 -0.0018390834 -0.0018383568 -0.0018375349 -0.0018365966 -0.0018357219 -0.0018346477 -0.0018332733 -0.0018319339 -0.0018311319 -0.0018306219 -0.0018302194 -0.0018299277 -0.0018298149 -0.0018297356][-0.0018398131 -0.0018388715 -0.0018381777 -0.0018375518 -0.0018368547 -0.001836286 -0.0018354367 -0.0018340173 -0.0018323958 -0.0018312646 -0.001830559 -0.0018299763 -0.0018296128 -0.0018294686 -0.0018293668][-0.0018383771 -0.0018370773 -0.0018363051 -0.00183614 -0.0018359981 -0.001836156 -0.0018358066 -0.0018346901 -0.0018330165 -0.0018316376 -0.0018307276 -0.0018300151 -0.0018296387 -0.0018294902 -0.001829367][-0.001835581 -0.0018339284 -0.0018331378 -0.0018333007 -0.0018336287 -0.0018347087 -0.0018351381 -0.0018347163 -0.001833362 -0.0018319897 -0.0018309088 -0.0018300898 -0.0018297039 -0.0018295746 -0.0018294477][-0.0018323614 -0.0018303316 -0.0018295029 -0.0018299243 -0.0018304837 -0.0018324085 -0.0018338595 -0.0018346548 -0.0018339822 -0.0018328421 -0.0018315407 -0.0018304678 -0.0018299472 -0.0018297549 -0.0018296364][-0.0018290378 -0.0018265821 -0.0018259485 -0.0018267726 -0.0018276044 -0.0018301988 -0.0018324598 -0.0018346305 -0.001834869 -0.0018341407 -0.0018326752 -0.0018312561 -0.0018304486 -0.001830095 -0.0018299187][-0.0018268363 -0.0018239798 -0.0018239901 -0.0018250808 -0.0018260588 -0.0018285525 -0.0018310599 -0.001834127 -0.0018354605 -0.0018354591 -0.0018341488 -0.0018324681 -0.001831271 -0.0018306129 -0.0018302674][-0.0018257481 -0.0018227933 -0.0018235964 -0.0018248374 -0.0018259163 -0.0018280954 -0.0018304483 -0.0018335214 -0.0018355644 -0.0018362368 -0.0018353799 -0.001833684 -0.0018321973 -0.0018312789 -0.001830699][-0.0018260386 -0.0018236456 -0.0018252603 -0.0018267096 -0.0018279764 -0.0018297626 -0.0018314205 -0.0018338697 -0.0018359178 -0.0018366601 -0.0018359773 -0.0018344593 -0.0018329963 -0.0018319456 -0.0018311959][-0.0018274138 -0.0018260769 -0.0018281135 -0.0018296513 -0.0018309922 -0.0018324972 -0.0018335915 -0.0018349842 -0.0018360281 -0.0018363673 -0.0018357163 -0.0018344654 -0.0018332179 -0.0018322419 -0.0018314574][-0.0018294298 -0.0018285986 -0.001830387 -0.0018316617 -0.0018328698 -0.0018340843 -0.001835008 -0.0018358065 -0.0018360543 -0.0018357878 -0.0018350523 -0.0018339748 -0.0018329077 -0.0018320698 -0.0018313917][-0.0018311562 -0.001830199 -0.0018310926 -0.001831894 -0.0018328214 -0.0018337537 -0.0018346041 -0.0018351823 -0.0018353077 -0.0018350733 -0.0018344938 -0.0018335993 -0.0018326838 -0.0018319307 -0.0018313045][-0.0018317603 -0.0018305234 -0.0018306111 -0.0018310922 -0.0018317372 -0.0018323384 -0.0018329495 -0.001833362 -0.0018335897 -0.0018336232 -0.00183338 -0.0018329109 -0.0018323066 -0.0018317575 -0.001831256][-0.0018314658 -0.0018301801 -0.0018298847 -0.0018300872 -0.0018304358 -0.0018307167 -0.0018310089 -0.0018312093 -0.0018313847 -0.0018315627 -0.0018316528 -0.0018316171 -0.0018314297 -0.0018312028 -0.001830876][-0.0018311859 -0.0018300005 -0.0018294897 -0.0018294434 -0.0018295556 -0.0018296585 -0.0018297804 -0.0018298852 -0.0018300068 -0.0018301734 -0.0018303411 -0.0018304699 -0.001830484 -0.0018303937 -0.0018302049]]...]
INFO - root - 2017-12-09 17:14:37.272965: step 42310, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.872 sec/batch; 70h:15m:47s remains)
INFO - root - 2017-12-09 17:14:45.946185: step 42320, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.861 sec/batch; 69h:25m:28s remains)
INFO - root - 2017-12-09 17:14:54.607707: step 42330, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.874 sec/batch; 70h:24m:26s remains)
INFO - root - 2017-12-09 17:15:03.351455: step 42340, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.839 sec/batch; 67h:39m:15s remains)
INFO - root - 2017-12-09 17:15:11.864287: step 42350, loss = 0.83, batch loss = 0.70 (9.3 examples/sec; 0.863 sec/batch; 69h:35m:03s remains)
INFO - root - 2017-12-09 17:15:20.496623: step 42360, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.874 sec/batch; 70h:28m:25s remains)
INFO - root - 2017-12-09 17:15:28.801469: step 42370, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.898 sec/batch; 72h:21m:15s remains)
INFO - root - 2017-12-09 17:15:37.529889: step 42380, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.873 sec/batch; 70h:21m:55s remains)
INFO - root - 2017-12-09 17:15:46.078292: step 42390, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.856 sec/batch; 68h:59m:39s remains)
INFO - root - 2017-12-09 17:15:54.671741: step 42400, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.838 sec/batch; 67h:31m:25s remains)
2017-12-09 17:15:55.565322: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.25812766 0.25379872 0.24883118 0.24335022 0.23918724 0.2351208 0.2313543 0.22767086 0.22186975 0.21538904 0.20751713 0.19840522 0.18865529 0.17764059 0.16812415][0.26709503 0.26608989 0.26403776 0.26137561 0.25946391 0.25739825 0.25549716 0.25283831 0.24758939 0.24073033 0.23170069 0.2207645 0.20802441 0.19416825 0.1812624][0.2733964 0.2771264 0.27926266 0.28031373 0.28112438 0.28176853 0.2813234 0.27964658 0.27499491 0.26766142 0.25708121 0.24345644 0.22745928 0.20982689 0.19284321][0.27743158 0.28554067 0.29167292 0.2969704 0.3018803 0.30564353 0.3073673 0.30695981 0.302849 0.29463977 0.28183898 0.26559952 0.24655496 0.22510937 0.20421772][0.27965519 0.2917026 0.30139154 0.31077552 0.32016009 0.32731697 0.3312166 0.33207464 0.32778049 0.31832859 0.30347964 0.28482053 0.26298088 0.23864067 0.21484059][0.28029779 0.29572651 0.30809647 0.32051566 0.33293882 0.34292844 0.34893358 0.3504611 0.34571505 0.33486187 0.31817952 0.29726598 0.27322888 0.24707875 0.22171369][0.27973932 0.29797751 0.31239292 0.32670605 0.34077165 0.35209575 0.35860747 0.36016974 0.35508257 0.34344584 0.32609606 0.30417636 0.27929422 0.25234425 0.226362][0.27766609 0.29733083 0.3125037 0.32753861 0.34213078 0.35356694 0.35946968 0.360198 0.35430923 0.34221014 0.32459873 0.30265951 0.27842692 0.2519004 0.22622164][0.27304184 0.29387972 0.30943638 0.32442224 0.33901671 0.34969732 0.35473266 0.35420123 0.34719253 0.33469993 0.31717241 0.2963002 0.27339086 0.24828288 0.2236231][0.26561475 0.28680655 0.30240589 0.31684008 0.33028835 0.33967936 0.34352142 0.34159961 0.33353612 0.32096079 0.30423087 0.28473431 0.26348498 0.24074346 0.21780145][0.25489044 0.27581915 0.29044625 0.30329889 0.3142176 0.32120776 0.3231878 0.31995592 0.3116295 0.300009 0.28507331 0.26781332 0.24866009 0.22833547 0.20790306][0.23792078 0.25749913 0.270672 0.28136423 0.28975886 0.294291 0.29438624 0.29068682 0.28330779 0.27356461 0.26119843 0.24676158 0.23063202 0.21327916 0.19552928][0.21746559 0.23426521 0.24486659 0.25321263 0.25918218 0.26135826 0.26010713 0.25641251 0.25012559 0.24246375 0.23316127 0.22253926 0.21009268 0.19635966 0.18204325][0.1952861 0.20898366 0.21679237 0.22260016 0.22659662 0.22762673 0.22637758 0.223522 0.21892506 0.21376918 0.20718776 0.19965705 0.19050616 0.18043047 0.16971467][0.17555033 0.18572663 0.19042057 0.19394505 0.19603929 0.196142 0.19507325 0.19338214 0.19058701 0.18741904 0.18341689 0.1785806 0.17237017 0.16555366 0.15836701]]...]
INFO - root - 2017-12-09 17:16:04.282723: step 42410, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.878 sec/batch; 70h:44m:31s remains)
INFO - root - 2017-12-09 17:16:13.041006: step 42420, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.877 sec/batch; 70h:39m:59s remains)
INFO - root - 2017-12-09 17:16:21.732072: step 42430, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.877 sec/batch; 70h:39m:58s remains)
INFO - root - 2017-12-09 17:16:30.534693: step 42440, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.887 sec/batch; 71h:27m:34s remains)
INFO - root - 2017-12-09 17:16:39.201507: step 42450, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.881 sec/batch; 70h:57m:25s remains)
INFO - root - 2017-12-09 17:16:47.969232: step 42460, loss = 0.82, batch loss = 0.69 (8.8 examples/sec; 0.912 sec/batch; 73h:26m:32s remains)
INFO - root - 2017-12-09 17:16:55.248705: step 42470, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.863 sec/batch; 69h:31m:33s remains)
INFO - root - 2017-12-09 17:17:03.875069: step 42480, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 70h:09m:12s remains)
INFO - root - 2017-12-09 17:17:12.604935: step 42490, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 69h:27m:03s remains)
INFO - root - 2017-12-09 17:17:21.212880: step 42500, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 69h:59m:01s remains)
2017-12-09 17:17:22.121138: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0018301733 -0.0018285114 -0.0018276123 -0.0018270516 -0.001826294 -0.0018252444 -0.0018234885 -0.001821333 -0.0018194013 -0.0018182617 -0.0018177179 -0.0018180915 -0.0018186478 -0.0018187476 -0.0018185636][-0.0018279473 -0.0018259978 -0.0018252057 -0.0018251699 -0.0018251069 -0.0018249623 -0.0018239047 -0.0018219673 -0.001819988 -0.0018185818 -0.0018179558 -0.0018183443 -0.0018193505 -0.0018199525 -0.0018199427][-0.0018258945 -0.0018236268 -0.0018228391 -0.0018231556 -0.0018235908 -0.0018241771 -0.001823767 -0.0018222861 -0.0018205664 -0.0018191617 -0.0018184539 -0.0018187709 -0.0018202646 -0.0018215522 -0.0018220111][-0.0018250841 -0.0018227239 -0.0018219919 -0.0018224611 -0.0018232814 -0.0018241089 -0.0018239982 -0.0018226969 -0.0018209651 -0.0018193974 -0.0018184854 -0.0018187951 -0.0018206093 -0.0018226482 -0.0018238061][-0.0018260189 -0.0018235202 -0.0018225748 -0.0018229348 -0.0018238222 -0.0018247678 -0.0018247311 -0.0018235841 -0.0018216909 -0.0018196786 -0.0018184196 -0.0018186013 -0.001820497 -0.0018230854 -0.0018250837][-0.0018273825 -0.0018249636 -0.0018237145 -0.0018238833 -0.0018247142 -0.0018255223 -0.0018254279 -0.0018242281 -0.0018221536 -0.0018198855 -0.0018182315 -0.0018182071 -0.0018200532 -0.0018229351 -0.0018256695][-0.0018284125 -0.0018258493 -0.0018243419 -0.0018242272 -0.0018247267 -0.0018251513 -0.0018249586 -0.0018237066 -0.0018215919 -0.0018192143 -0.0018175105 -0.0018175542 -0.0018194055 -0.0018224755 -0.0018258333][-0.0018288239 -0.0018261267 -0.0018244263 -0.0018240069 -0.0018242155 -0.0018242402 -0.0018238146 -0.0018225312 -0.0018204689 -0.0018181282 -0.0018164703 -0.0018167169 -0.0018187964 -0.0018220993 -0.0018258906][-0.0018281087 -0.0018254138 -0.0018237317 -0.0018232843 -0.0018234402 -0.0018233315 -0.0018228152 -0.0018216023 -0.0018196587 -0.0018174383 -0.0018159981 -0.001816428 -0.0018186555 -0.0018220926 -0.0018260934][-0.0018262235 -0.0018233698 -0.0018218245 -0.0018214077 -0.001821577 -0.0018216146 -0.0018213756 -0.0018204561 -0.0018188547 -0.0018169733 -0.0018158419 -0.0018164245 -0.001818784 -0.001822219 -0.0018260684][-0.0018236921 -0.0018205817 -0.0018191282 -0.0018188684 -0.00181934 -0.0018196831 -0.0018199 -0.0018195283 -0.0018184393 -0.0018170991 -0.0018164379 -0.0018172037 -0.0018196057 -0.0018228496 -0.0018262658][-0.0018220409 -0.0018184393 -0.0018166988 -0.001816396 -0.0018170523 -0.0018178276 -0.001818707 -0.0018190722 -0.0018186817 -0.001818019 -0.0018179128 -0.0018189464 -0.0018213609 -0.0018244905 -0.0018275615][-0.00182163 -0.0018175095 -0.001815269 -0.0018147138 -0.0018154524 -0.0018167442 -0.0018183674 -0.0018195376 -0.0018198644 -0.0018197937 -0.0018200633 -0.001821182 -0.0018234944 -0.0018265684 -0.0018293703][-0.001822077 -0.0018174808 -0.0018147293 -0.0018138502 -0.0018145702 -0.0018162918 -0.0018185551 -0.0018204086 -0.001821419 -0.0018218769 -0.0018223744 -0.0018234827 -0.0018256865 -0.0018286109 -0.001831141][-0.0018229156 -0.0018178455 -0.0018145964 -0.0018135131 -0.0018142343 -0.0018161875 -0.0018188717 -0.0018211121 -0.0018223879 -0.0018230777 -0.0018236835 -0.0018248837 -0.001827081 -0.0018299466 -0.0018324007]]...]
INFO - root - 2017-12-09 17:17:30.830784: step 42510, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.837 sec/batch; 67h:25m:11s remains)
INFO - root - 2017-12-09 17:17:39.415718: step 42520, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.856 sec/batch; 68h:56m:46s remains)
INFO - root - 2017-12-09 17:17:48.232431: step 42530, loss = 0.82, batch loss = 0.69 (8.7 examples/sec; 0.923 sec/batch; 74h:22m:57s remains)
INFO - root - 2017-12-09 17:17:56.981633: step 42540, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.872 sec/batch; 70h:16m:04s remains)
INFO - root - 2017-12-09 17:18:05.704715: step 42550, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.878 sec/batch; 70h:43m:25s remains)
INFO - root - 2017-12-09 17:18:14.447554: step 42560, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.897 sec/batch; 72h:13m:14s remains)
INFO - root - 2017-12-09 17:18:22.738815: step 42570, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.870 sec/batch; 70h:05m:33s remains)
INFO - root - 2017-12-09 17:18:31.382472: step 42580, loss = 0.82, batch loss = 0.69 (8.8 examples/sec; 0.906 sec/batch; 72h:59m:04s remains)
INFO - root - 2017-12-09 17:18:40.085706: step 42590, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 69h:52m:43s remains)
INFO - root - 2017-12-09 17:18:48.692506: step 42600, loss = 0.82, batch loss = 0.69 (8.8 examples/sec; 0.904 sec/batch; 72h:49m:21s remains)
2017-12-09 17:18:49.587719: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0018394652 -0.001840041 -0.0018402409 -0.0018403108 -0.0018401141 -0.0018400337 -0.0018397245 -0.0018394135 -0.0018391045 -0.001838813 -0.0018385584 -0.0018383256 -0.0018380327 -0.0018376649 -0.0018372579][-0.0018372555 -0.0018391703 -0.0018404211 -0.0018410271 -0.0018410574 -0.001840753 -0.0018399829 -0.0018392394 -0.0018386093 -0.0018382709 -0.0018381877 -0.0018380479 -0.0018378248 -0.0018374389 -0.0018370071][-0.0018347276 -0.0018378732 -0.0018401515 -0.0018410173 -0.001840977 -0.0018400241 -0.0018386954 -0.0018375695 -0.0018369606 -0.0018371481 -0.0018376732 -0.0018380254 -0.0018380113 -0.0018376501 -0.0018372065][-0.0018324635 -0.0018364761 -0.0018393215 -0.0018400513 -0.0018390762 -0.0018369469 -0.0018347302 -0.0018334795 -0.0018335797 -0.0018348788 -0.0018365656 -0.0018377131 -0.0018380629 -0.0018379 -0.0018374458][-0.0018310449 -0.0018354793 -0.0018383777 -0.0018384374 -0.0018359827 -0.0018320652 -0.0018286982 -0.0018274335 -0.0018286126 -0.0018316312 -0.001835221 -0.0018377225 -0.0018385454 -0.0018385332 -0.0018379657][-0.0018319994 -0.0018360525 -0.001837893 -0.0018363369 -0.0018319613 -0.0018261658 -0.0018215813 -0.0018204941 -0.0018228602 -0.0018276853 -0.0018334544 -0.001837441 -0.0018391588 -0.0018393851 -0.0018388254][-0.0018346198 -0.0018372493 -0.0018374173 -0.0018337977 -0.0018271931 -0.0018195979 -0.0018142088 -0.0018135097 -0.0018169995 -0.0018240748 -0.0018318559 -0.0018371788 -0.0018397832 -0.0018403104 -0.0018398341][-0.0018369276 -0.0018380505 -0.0018362773 -0.0018304909 -0.0018220254 -0.0018132962 -0.0018077098 -0.0018078979 -0.0018131918 -0.001822245 -0.0018311451 -0.0018372247 -0.001839973 -0.0018402762 -0.001839624][-0.0018379942 -0.0018376333 -0.0018341789 -0.0018265884 -0.0018166902 -0.0018078816 -0.001803105 -0.0018047135 -0.001811933 -0.0018216786 -0.0018305845 -0.0018364503 -0.0018388338 -0.0018384837 -0.0018373537][-0.0018366401 -0.0018356337 -0.001831255 -0.0018228854 -0.0018125502 -0.0018039924 -0.0018007143 -0.0018040132 -0.001812255 -0.0018217862 -0.0018297673 -0.0018343845 -0.0018356979 -0.0018346199 -0.0018335361][-0.0018332452 -0.0018326183 -0.001828179 -0.0018198988 -0.0018101775 -0.0018031226 -0.0018017975 -0.0018063554 -0.0018145663 -0.0018228443 -0.0018287393 -0.0018312952 -0.0018311124 -0.0018294528 -0.0018291014][-0.0018305565 -0.0018301664 -0.0018266235 -0.0018193275 -0.001811119 -0.0018058649 -0.0018063218 -0.0018115373 -0.0018187482 -0.0018250963 -0.0018282038 -0.0018284644 -0.0018268609 -0.0018250039 -0.0018252626][-0.0018293132 -0.0018297542 -0.0018273718 -0.001821428 -0.0018149685 -0.0018118346 -0.0018136667 -0.0018185211 -0.0018242444 -0.0018280131 -0.0018283838 -0.0018265902 -0.0018244373 -0.0018228379 -0.0018234125][-0.0018297235 -0.0018307569 -0.0018294492 -0.0018254007 -0.0018210855 -0.0018198506 -0.0018221788 -0.0018259274 -0.0018296428 -0.001831047 -0.0018295607 -0.0018268242 -0.0018246419 -0.0018233834 -0.0018238814][-0.0018313165 -0.001832452 -0.0018323022 -0.0018300513 -0.0018275051 -0.0018275576 -0.0018295494 -0.0018319567 -0.0018340108 -0.0018339823 -0.0018316738 -0.0018288607 -0.001826679 -0.0018253983 -0.0018257931]]...]
INFO - root - 2017-12-09 17:18:58.128203: step 42610, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 69h:50m:56s remains)
INFO - root - 2017-12-09 17:19:06.780187: step 42620, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.863 sec/batch; 69h:27m:33s remains)
INFO - root - 2017-12-09 17:19:15.447256: step 42630, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.897 sec/batch; 72h:13m:21s remains)
INFO - root - 2017-12-09 17:19:24.174317: step 42640, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.901 sec/batch; 72h:34m:55s remains)
INFO - root - 2017-12-09 17:19:32.826316: step 42650, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.853 sec/batch; 68h:42m:50s remains)
INFO - root - 2017-12-09 17:19:41.464559: step 42660, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 69h:49m:08s remains)
INFO - root - 2017-12-09 17:19:49.626634: step 42670, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.853 sec/batch; 68h:40m:04s remains)
INFO - root - 2017-12-09 17:19:58.338156: step 42680, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 69h:47m:05s remains)
INFO - root - 2017-12-09 17:20:06.981394: step 42690, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.877 sec/batch; 70h:36m:18s remains)
INFO - root - 2017-12-09 17:20:15.684492: step 42700, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.854 sec/batch; 68h:44m:57s remains)
2017-12-09 17:20:16.591330: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.29102603 0.28031516 0.26767927 0.25461677 0.24127764 0.22807524 0.21523234 0.20301892 0.19166887 0.18322429 0.17787459 0.17482148 0.17207797 0.16961575 0.16721253][0.30226573 0.2946786 0.28407529 0.27182493 0.25816581 0.24366847 0.22886527 0.21414353 0.20020436 0.18911028 0.18101326 0.17479278 0.16863787 0.16294894 0.15715425][0.31111559 0.30934992 0.30296177 0.29289198 0.27995908 0.26478508 0.24804917 0.23100807 0.21406066 0.19971028 0.18789597 0.17771333 0.16729234 0.15706958 0.14682056][0.31995377 0.32492638 0.32439855 0.31851643 0.30794945 0.29318511 0.27570117 0.25682104 0.23704663 0.2192678 0.20307341 0.18775252 0.17182097 0.15600388 0.14027166][0.32662895 0.338147 0.34362578 0.34220037 0.33492488 0.32176042 0.30480516 0.28456274 0.26243332 0.24176939 0.22193788 0.20202844 0.18075883 0.15951793 0.13822418][0.33066431 0.34823269 0.35835552 0.36090922 0.3566767 0.34609312 0.3301886 0.30942693 0.28623614 0.26376465 0.24113269 0.21708494 0.19158719 0.1661417 0.14008154][0.3266421 0.34850249 0.36225629 0.3686015 0.368015 0.36053538 0.34692913 0.32817847 0.30617642 0.28341913 0.25936595 0.23273949 0.20386462 0.1742852 0.14396568][0.31723088 0.34042048 0.35497278 0.36287978 0.36464685 0.3598648 0.34901953 0.33316532 0.31354153 0.29278886 0.26939783 0.24244931 0.21232902 0.18052201 0.14755076][0.30033585 0.32288736 0.33684245 0.34526157 0.34847325 0.34570658 0.3377558 0.32522613 0.3090699 0.29086462 0.26956391 0.24417742 0.21479222 0.18238355 0.14851606][0.27977583 0.29980296 0.31081778 0.31750548 0.32049423 0.31909916 0.31345609 0.3040463 0.29155317 0.27714959 0.25929829 0.23692036 0.21022266 0.17953712 0.14699842][0.25838822 0.27462047 0.28177705 0.28589195 0.28766751 0.28666514 0.2824952 0.27596274 0.2670618 0.25630888 0.24210361 0.22308859 0.20000108 0.17276487 0.14360929][0.2374943 0.24966487 0.25322729 0.25496569 0.25552371 0.25446916 0.2518191 0.24763523 0.24173674 0.23420873 0.22328319 0.20831862 0.1894868 0.1665813 0.14171058][0.221252 0.2295789 0.22972077 0.22886278 0.22815196 0.22696285 0.22529602 0.2225157 0.21886991 0.21421187 0.20639345 0.19530696 0.18094999 0.16318308 0.1435785][0.20898122 0.2148279 0.21292952 0.2105829 0.20888002 0.20737565 0.20610698 0.20357816 0.20079181 0.19834335 0.19392842 0.18659806 0.17696176 0.16493325 0.150947][0.20122638 0.204655 0.2005842 0.19679 0.1941895 0.1925399 0.19127733 0.18894246 0.18686621 0.18600006 0.18417171 0.18064396 0.17573467 0.16945781 0.16135038]]...]
INFO - root - 2017-12-09 17:20:25.180311: step 42710, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.877 sec/batch; 70h:36m:53s remains)
INFO - root - 2017-12-09 17:20:33.875447: step 42720, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.848 sec/batch; 68h:13m:08s remains)
INFO - root - 2017-12-09 17:20:42.553428: step 42730, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.860 sec/batch; 69h:11m:58s remains)
INFO - root - 2017-12-09 17:20:51.202857: step 42740, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.872 sec/batch; 70h:09m:38s remains)
INFO - root - 2017-12-09 17:20:59.820770: step 42750, loss = 0.82, batch loss = 0.69 (9.9 examples/sec; 0.808 sec/batch; 64h:59m:52s remains)
INFO - root - 2017-12-09 17:21:08.519393: step 42760, loss = 0.82, batch loss = 0.69 (9.7 examples/sec; 0.823 sec/batch; 66h:13m:21s remains)
INFO - root - 2017-12-09 17:21:16.668468: step 42770, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.854 sec/batch; 68h:45m:31s remains)
INFO - root - 2017-12-09 17:21:25.206736: step 42780, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.865 sec/batch; 69h:37m:45s remains)
INFO - root - 2017-12-09 17:21:33.813658: step 42790, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.863 sec/batch; 69h:25m:43s remains)
INFO - root - 2017-12-09 17:21:42.448683: step 42800, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 69h:57m:25s remains)
2017-12-09 17:21:43.280550: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.0031217053 0.0024286802 0.0019058597 0.0017654812 0.0026862826 0.0051019108 0.0089359237 0.013831107 0.018827168 0.023189619 0.026194366 0.027008621 0.02518722 0.020875908 0.015348863][0.0052729743 0.0052212649 0.0053567882 0.0060388045 0.0083772745 0.012582744 0.01872289 0.026018789 0.0333721 0.039799672 0.044020679 0.044838138 0.041792396 0.035178374 0.026523909][0.0066429414 0.00711576 0.0085686175 0.011340315 0.016667038 0.024285799 0.033901997 0.04403853 0.053577505 0.061228253 0.065551467 0.065228179 0.060078129 0.050784659 0.038888425][0.0076604895 0.0087283738 0.011586309 0.01698876 0.025985662 0.037633378 0.05092065 0.063797057 0.075179443 0.083526105 0.087220669 0.08500284 0.077247649 0.065082565 0.050169773][0.0080475667 0.01039141 0.015169319 0.023192709 0.035194192 0.050006587 0.066090696 0.081342623 0.094640628 0.10393079 0.10735657 0.10362513 0.093268782 0.0781092 0.0603353][0.0077368179 0.011327695 0.018021982 0.028373122 0.042432718 0.058918882 0.076429106 0.093106091 0.10777686 0.11833628 0.12222152 0.11782377 0.10583087 0.088538267 0.068745159][0.0075371275 0.012164688 0.020167753 0.031741213 0.046494395 0.0632918 0.080684774 0.097434133 0.1125479 0.1239701 0.12860566 0.12462655 0.11251933 0.094893992 0.07464686][0.0073836036 0.012453054 0.021046594 0.032874897 0.047086924 0.062819622 0.078912154 0.09457738 0.10878707 0.1198568 0.12483861 0.12198593 0.11130454 0.095215827 0.076336175][0.0065618837 0.011869032 0.02033872 0.031575572 0.044669993 0.058885068 0.073077984 0.086767323 0.099090338 0.10878652 0.11336014 0.11140771 0.10268872 0.08922901 0.07273411][0.0047999835 0.0096827447 0.0175046 0.027741384 0.039403293 0.051953621 0.064306989 0.075923018 0.085899234 0.0935661 0.09719383 0.095768459 0.088807613 0.077732496 0.063572749][0.002780146 0.0067423461 0.013233509 0.021860603 0.031859592 0.042819567 0.053595155 0.063367166 0.071167625 0.07672897 0.079126291 0.077730834 0.071994811 0.06278009 0.050740048][0.0010310984 0.0038827802 0.0086527308 0.015099198 0.02286548 0.031755522 0.040810365 0.048931528 0.054986026 0.058824398 0.060120534 0.058537722 0.053574763 0.045799028 0.035912182][-0.00018159964 0.0016070604 0.004695002 0.0089315865 0.014290102 0.020795278 0.027697312 0.033807069 0.038074188 0.040507536 0.040946741 0.039285738 0.035193566 0.029101778 0.021742884][-0.00098558911 3.2078824e-05 0.0017611401 0.0041549671 0.0073483186 0.011496805 0.016106458 0.02017683 0.022848366 0.024117121 0.023985054 0.022449758 0.01940419 0.015211547 0.010492595][-0.0014428112 -0.00095158769 -0.00010806823 0.0010484071 0.0026777973 0.0049431487 0.0075689619 0.0098711234 0.01125882 0.011742486 0.01139999 0.01029545 0.0084194811 0.0060051903 0.0034854687]]...]
INFO - root - 2017-12-09 17:21:51.995798: step 42810, loss = 0.83, batch loss = 0.70 (9.4 examples/sec; 0.851 sec/batch; 68h:29m:03s remains)
INFO - root - 2017-12-09 17:22:00.716746: step 42820, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.891 sec/batch; 71h:43m:18s remains)
INFO - root - 2017-12-09 17:22:09.481049: step 42830, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.838 sec/batch; 67h:26m:33s remains)
INFO - root - 2017-12-09 17:22:18.183255: step 42840, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 69h:31m:51s remains)
INFO - root - 2017-12-09 17:22:26.815055: step 42850, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.833 sec/batch; 67h:02m:30s remains)
INFO - root - 2017-12-09 17:22:35.428073: step 42860, loss = 0.82, batch loss = 0.69 (9.7 examples/sec; 0.829 sec/batch; 66h:41m:17s remains)
INFO - root - 2017-12-09 17:22:43.604054: step 42870, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.855 sec/batch; 68h:47m:13s remains)
INFO - root - 2017-12-09 17:22:52.102263: step 42880, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.843 sec/batch; 67h:51m:20s remains)
INFO - root - 2017-12-09 17:23:00.799142: step 42890, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.876 sec/batch; 70h:29m:40s remains)
INFO - root - 2017-12-09 17:23:09.345354: step 42900, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.831 sec/batch; 66h:49m:42s remains)
2017-12-09 17:23:10.186632: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0017913441 -0.0017895489 -0.0017897688 -0.0017902156 -0.0017906298 -0.0017909724 -0.0017917304 -0.00179316 -0.0017954144 -0.0017983455 -0.0018014368 -0.0018042373 -0.0018062268 -0.0018070778 -0.0018066884][-0.0017924763 -0.0017906739 -0.0017907905 -0.00179111 -0.0017912703 -0.0017915678 -0.001792478 -0.001794484 -0.0017975756 -0.001801506 -0.0018054058 -0.0018087724 -0.0018110663 -0.0018118171 -0.0018109659][-0.0017951083 -0.0017933442 -0.0017931295 -0.0017929894 -0.0017927302 -0.0017928878 -0.0017941337 -0.0017968739 -0.0018006256 -0.0018050479 -0.0018092424 -0.0018126306 -0.0018148507 -0.0018153039 -0.0018142161][-0.0017976098 -0.0017956833 -0.0017949815 -0.0017944012 -0.0017937515 -0.0017938233 -0.0017954898 -0.0017988356 -0.0018030785 -0.0018077291 -0.0018120032 -0.0018153647 -0.001817402 -0.0018176754 -0.0018164446][-0.001799155 -0.0017970866 -0.0017960485 -0.0017952488 -0.0017943261 -0.0017943282 -0.0017962186 -0.0017999251 -0.0018044253 -0.0018092167 -0.0018136869 -0.0018169921 -0.0018187808 -0.0018187596 -0.0018174347][-0.0017991873 -0.001797014 -0.0017957719 -0.001794763 -0.0017936314 -0.0017934692 -0.0017951619 -0.001798901 -0.0018036237 -0.0018086022 -0.0018132727 -0.001816672 -0.0018183669 -0.0018182799 -0.0018167697][-0.0017974195 -0.001795248 -0.001793721 -0.0017923575 -0.0017910851 -0.0017908341 -0.0017922681 -0.0017957041 -0.0018004107 -0.0018054784 -0.0018102955 -0.0018137137 -0.0018153677 -0.0018152803 -0.0018136768][-0.001794736 -0.001792388 -0.0017908351 -0.0017894024 -0.0017882689 -0.0017881092 -0.0017892981 -0.0017920622 -0.0017961487 -0.0018006698 -0.0018050708 -0.0018081891 -0.0018097024 -0.0018096626 -0.0018081912][-0.0017923531 -0.0017900355 -0.0017886289 -0.0017875126 -0.0017866519 -0.0017865655 -0.0017873745 -0.0017893885 -0.0017925759 -0.0017962004 -0.001799995 -0.0018028258 -0.0018042021 -0.0018042132 -0.0018031086][-0.0017906163 -0.0017884506 -0.0017873605 -0.001786508 -0.0017858932 -0.0017857045 -0.0017860422 -0.0017873356 -0.0017896274 -0.0017923344 -0.0017954583 -0.0017979075 -0.0017992143 -0.0017994774 -0.0017987923][-0.0017897482 -0.0017876031 -0.0017866269 -0.0017859888 -0.001785542 -0.0017853061 -0.0017853787 -0.0017861201 -0.0017877308 -0.0017897276 -0.0017920709 -0.0017940655 -0.0017953067 -0.001795732 -0.0017953074][-0.0017900714 -0.0017876964 -0.0017868717 -0.001786472 -0.001786211 -0.0017860389 -0.0017859836 -0.0017862864 -0.0017871029 -0.0017882249 -0.0017895518 -0.0017907771 -0.0017916732 -0.0017921265 -0.0017919806][-0.0017906142 -0.00178809 -0.0017872986 -0.0017871113 -0.0017869988 -0.0017869084 -0.0017868129 -0.0017868388 -0.0017870613 -0.0017874634 -0.0017879748 -0.0017885108 -0.0017889618 -0.001789263 -0.0017893702][-0.0017906491 -0.0017880254 -0.0017872424 -0.0017871378 -0.0017870994 -0.0017870404 -0.0017869434 -0.0017868906 -0.0017869103 -0.0017869874 -0.0017871694 -0.0017874005 -0.0017876127 -0.0017877654 -0.0017878569][-0.0017902195 -0.0017876165 -0.0017868215 -0.0017867336 -0.0017866981 -0.0017866582 -0.0017866 -0.0017865559 -0.0017865368 -0.0017865284 -0.0017865739 -0.0017866199 -0.0017866582 -0.0017866824 -0.0017866992]]...]
INFO - root - 2017-12-09 17:23:18.721389: step 42910, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.855 sec/batch; 68h:45m:52s remains)
INFO - root - 2017-12-09 17:23:27.322628: step 42920, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.860 sec/batch; 69h:10m:45s remains)
INFO - root - 2017-12-09 17:23:35.920713: step 42930, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 69h:42m:53s remains)
INFO - root - 2017-12-09 17:23:44.428996: step 42940, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.857 sec/batch; 68h:58m:03s remains)
INFO - root - 2017-12-09 17:23:53.256921: step 42950, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.855 sec/batch; 68h:43m:52s remains)
INFO - root - 2017-12-09 17:24:01.911473: step 42960, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.870 sec/batch; 69h:58m:00s remains)
INFO - root - 2017-12-09 17:24:10.208951: step 42970, loss = 0.82, batch loss = 0.69 (10.7 examples/sec; 0.748 sec/batch; 60h:10m:10s remains)
INFO - root - 2017-12-09 17:24:18.883836: step 42980, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.848 sec/batch; 68h:12m:15s remains)
INFO - root - 2017-12-09 17:24:27.489059: step 42990, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.834 sec/batch; 67h:04m:15s remains)
INFO - root - 2017-12-09 17:24:36.025065: step 43000, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.830 sec/batch; 66h:46m:42s remains)
2017-12-09 17:24:36.939489: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0015522415 -0.0017206008 -0.0018024651 -0.0018265245 -0.0018208859 -0.001790327 -0.00172796 -0.001643935 -0.0015698468 -0.001543071 -0.0015752653 -0.0016508332 -0.0017322503 -0.0017933545 -0.0018227356][-0.0016539635 -0.0017644622 -0.00181402 -0.0018242368 -0.0017997543 -0.001724402 -0.0015852959 -0.0014103941 -0.0012651293 -0.0012189758 -0.0012886082 -0.00144125 -0.0016066046 -0.0017350346 -0.0018035119][-0.00174125 -0.0018007973 -0.0018228752 -0.0018159515 -0.0017576558 -0.0016075346 -0.0013474266 -0.0010326151 -0.00077870139 -0.00070076331 -0.0008268964 -0.0010991186 -0.0013990016 -0.0016368716 -0.0017701344][-0.0017870094 -0.0018175429 -0.0018249312 -0.0017997191 -0.001694373 -0.0014451478 -0.0010333795 -0.0005482079 -0.0001624549 -4.2013242e-05 -0.00023103005 -0.00064661109 -0.0011187557 -0.001501529 -0.0017226934][-0.0017934263 -0.0018158776 -0.0018204665 -0.0017797567 -0.0016279145 -0.0012846491 -0.00073449337 -9.603845e-05 0.00041032082 0.00057699869 0.00034070353 -0.00019930408 -0.00083304371 -0.001359669 -0.0016727714][-0.0017696344 -0.001800017 -0.0018107707 -0.0017619849 -0.0015812619 -0.0011834878 -0.00055610319 0.00016692479 0.00074152474 0.00093830039 0.00068241486 7.8357174e-05 -0.00064806256 -0.001265326 -0.0016397757][-0.0017241755 -0.0017751675 -0.0017996203 -0.0017535804 -0.0015730811 -0.0011812297 -0.00056754693 0.00013880932 0.00069977727 0.00089121552 0.00063942641 5.0411676e-05 -0.00065877615 -0.0012665405 -0.0016390828][-0.0016601218 -0.0017434495 -0.0017899615 -0.0017581398 -0.0016049206 -0.001271859 -0.00075066392 -0.00015160232 0.00031713035 0.00046056148 0.00022403535 -0.0002839946 -0.00087404379 -0.0013710613 -0.0016740962][-0.0015782191 -0.0017039897 -0.0017821924 -0.0017733099 -0.001662989 -0.0014181151 -0.0010341334 -0.00059589255 -0.00026589329 -0.00019213511 -0.00039975741 -0.00078386127 -0.0011987179 -0.0015317269 -0.0017293657][-0.0014708407 -0.0016474988 -0.0017696294 -0.0017905636 -0.0017257653 -0.0015700334 -0.0013256402 -0.0010520197 -0.00086068665 -0.00084569247 -0.0010066896 -0.0012566666 -0.0014994876 -0.0016796144 -0.0017807612][-0.0013440639 -0.0015688761 -0.001742162 -0.0017990494 -0.0017761689 -0.0016935369 -0.0015620573 -0.0014196328 -0.0013309832 -0.0013436903 -0.001446043 -0.0015815567 -0.0016973554 -0.0017744559 -0.0018135884][-0.0012104949 -0.0014721982 -0.001693481 -0.0017895781 -0.0018038445 -0.0017725283 -0.0017153132 -0.0016553637 -0.0016233188 -0.0016376285 -0.0016883806 -0.001747526 -0.0017915813 -0.0018165342 -0.0018269682][-0.0011018233 -0.0013816308 -0.0016363363 -0.0017666626 -0.0018101805 -0.0018105616 -0.0017931141 -0.00177391 -0.0017653985 -0.001772354 -0.0017908388 -0.0018105408 -0.0018232745 -0.0018289752 -0.0018302505][-0.0010429453 -0.0013250385 -0.0015921685 -0.0017433342 -0.0018057422 -0.0018222275 -0.00182131 -0.0018170812 -0.0018155029 -0.0018176908 -0.0018226611 -0.0018273011 -0.0018299802 -0.0018308413 -0.0018304536][-0.0010662994 -0.0013310425 -0.0015845555 -0.0017345328 -0.0018017048 -0.0018236175 -0.0018276719 -0.0018271789 -0.0018268169 -0.001827477 -0.0018289357 -0.0018298937 -0.0018305085 -0.0018305 -0.001830077]]...]
INFO - root - 2017-12-09 17:24:45.385616: step 43010, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 69h:30m:22s remains)
INFO - root - 2017-12-09 17:24:54.082682: step 43020, loss = 0.82, batch loss = 0.69 (8.1 examples/sec; 0.987 sec/batch; 79h:21m:05s remains)
INFO - root - 2017-12-09 17:25:02.671275: step 43030, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.865 sec/batch; 69h:35m:15s remains)
INFO - root - 2017-12-09 17:25:11.401497: step 43040, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.855 sec/batch; 68h:46m:57s remains)
INFO - root - 2017-12-09 17:25:20.040629: step 43050, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.844 sec/batch; 67h:51m:23s remains)
INFO - root - 2017-12-09 17:25:28.724271: step 43060, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.888 sec/batch; 71h:23m:16s remains)
INFO - root - 2017-12-09 17:25:37.092754: step 43070, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.833 sec/batch; 66h:59m:13s remains)
INFO - root - 2017-12-09 17:25:45.591024: step 43080, loss = 0.81, batch loss = 0.68 (9.4 examples/sec; 0.850 sec/batch; 68h:17m:52s remains)
INFO - root - 2017-12-09 17:25:54.348607: step 43090, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.858 sec/batch; 69h:00m:46s remains)
INFO - root - 2017-12-09 17:26:02.955993: step 43100, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.841 sec/batch; 67h:36m:30s remains)
2017-12-09 17:26:03.891639: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0018329925 -0.0018239617 -0.0018076164 -0.0017853635 -0.0017696338 -0.0017718284 -0.0017905878 -0.0018109183 -0.0018240088 -0.0018288497 -0.0018305918 -0.0018311961 -0.0018313165 -0.0018312546 -0.0018306121][-0.0018297767 -0.0018152648 -0.0017858668 -0.001743842 -0.001712114 -0.0017132182 -0.001748247 -0.0017894609 -0.0018170695 -0.0018268961 -0.001829243 -0.0018299709 -0.0018304777 -0.0018308966 -0.0018309691][-0.0018281095 -0.0018090141 -0.0017687518 -0.0017089357 -0.0016599467 -0.0016543283 -0.00170033 -0.0017621001 -0.0018073975 -0.0018251034 -0.0018283173 -0.001828935 -0.0018292078 -0.0018295036 -0.0018295529][-0.0018283599 -0.0018092222 -0.0017670174 -0.0017014663 -0.001642458 -0.0016269966 -0.0016705919 -0.0017409844 -0.0017981027 -0.0018232041 -0.0018279589 -0.0018283391 -0.0018282202 -0.0018281569 -0.0018278328][-0.0018300636 -0.0018157476 -0.0017823795 -0.0017267712 -0.0016700085 -0.0016465858 -0.0016764036 -0.001738898 -0.0017948365 -0.001821602 -0.0018275101 -0.0018279052 -0.0018275337 -0.0018270425 -0.001826452][-0.0018318051 -0.0018239942 -0.0018041679 -0.0017679447 -0.0017254979 -0.00170111 -0.0017142573 -0.0017569673 -0.0017997955 -0.0018214361 -0.0018268682 -0.0018273306 -0.0018268083 -0.0018261597 -0.0018255933][-0.0018322259 -0.0018290182 -0.0018199759 -0.0018021981 -0.001779084 -0.0017620057 -0.00176419 -0.0017855108 -0.001810058 -0.0018229489 -0.0018264175 -0.0018267127 -0.0018261548 -0.0018255877 -0.0018251787][-0.0018316205 -0.0018307598 -0.0018278632 -0.0018212723 -0.001812059 -0.0018040512 -0.0018025331 -0.0018093653 -0.0018193781 -0.0018251865 -0.0018267668 -0.0018267096 -0.0018261856 -0.0018257195 -0.0018253721][-0.0018302355 -0.0018303257 -0.0018300882 -0.00182874 -0.0018260498 -0.0018231033 -0.0018213039 -0.0018219661 -0.0018247046 -0.0018267445 -0.0018274687 -0.0018274814 -0.0018270924 -0.0018266815 -0.0018261982][-0.001828604 -0.001828888 -0.0018294847 -0.0018298682 -0.0018293401 -0.0018279774 -0.0018265061 -0.0018258241 -0.0018265351 -0.0018277839 -0.0018287563 -0.0018290798 -0.0018289166 -0.0018284578 -0.0018276636][-0.0018272151 -0.0018272944 -0.0018278705 -0.0018284214 -0.0018284897 -0.0018281513 -0.0018276847 -0.0018274147 -0.0018278274 -0.0018290415 -0.0018304008 -0.0018311834 -0.0018312178 -0.0018306508 -0.0018294747][-0.0018265158 -0.0018262081 -0.0018265336 -0.0018269873 -0.0018272225 -0.0018272871 -0.0018274253 -0.0018279561 -0.0018289743 -0.0018304579 -0.0018319493 -0.0018330136 -0.0018332959 -0.0018327343 -0.0018312767][-0.0018261006 -0.0018256097 -0.0018257167 -0.0018259428 -0.0018260385 -0.0018262451 -0.0018268084 -0.0018278216 -0.0018293114 -0.0018312829 -0.0018331856 -0.0018346226 -0.0018351548 -0.0018347746 -0.0018332476][-0.0018261127 -0.0018254033 -0.0018251955 -0.0018251226 -0.0018250166 -0.0018251244 -0.0018257829 -0.0018270173 -0.0018287534 -0.0018312109 -0.0018336559 -0.0018355089 -0.0018364384 -0.0018363362 -0.001834934][-0.0018261324 -0.001825369 -0.0018249659 -0.0018246602 -0.001824335 -0.0018243376 -0.001824953 -0.0018261706 -0.0018279855 -0.0018306624 -0.0018334567 -0.0018356887 -0.0018369894 -0.0018371389 -0.0018360928]]...]
INFO - root - 2017-12-09 17:26:12.511449: step 43110, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.870 sec/batch; 69h:58m:20s remains)
INFO - root - 2017-12-09 17:26:21.194887: step 43120, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 69h:18m:05s remains)
INFO - root - 2017-12-09 17:26:30.056577: step 43130, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.892 sec/batch; 71h:40m:06s remains)
INFO - root - 2017-12-09 17:26:38.752499: step 43140, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.865 sec/batch; 69h:30m:38s remains)
INFO - root - 2017-12-09 17:26:47.440763: step 43150, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.874 sec/batch; 70h:15m:06s remains)
INFO - root - 2017-12-09 17:26:56.092670: step 43160, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 69h:17m:50s remains)
INFO - root - 2017-12-09 17:27:04.399962: step 43170, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.851 sec/batch; 68h:22m:01s remains)
INFO - root - 2017-12-09 17:27:12.777506: step 43180, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.854 sec/batch; 68h:36m:00s remains)
INFO - root - 2017-12-09 17:27:21.307791: step 43190, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.872 sec/batch; 70h:06m:32s remains)
INFO - root - 2017-12-09 17:27:29.926804: step 43200, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.879 sec/batch; 70h:37m:10s remains)
2017-12-09 17:27:30.796200: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0018249303 -0.0018242621 -0.0018241802 -0.0018242374 -0.0018243975 -0.0018248643 -0.0018254926 -0.0018261805 -0.0018265963 -0.001826614 -0.0018264214 -0.0018261963 -0.0018259475 -0.0018257435 -0.0018255428][-0.0018247954 -0.0018241509 -0.0018240327 -0.0018240868 -0.0018242596 -0.0018246932 -0.0018252656 -0.0018258664 -0.0018262183 -0.0018261945 -0.0018259941 -0.0018258215 -0.0018256437 -0.0018255241 -0.0018254417][-0.0018248457 -0.001824212 -0.0018240975 -0.0018241523 -0.0018243294 -0.0018247099 -0.0018251688 -0.0018256238 -0.0018258722 -0.0018258191 -0.0018256757 -0.0018255902 -0.0018254948 -0.0018254168 -0.0018253928][-0.0018249273 -0.0018242906 -0.0018241316 -0.0018241043 -0.0018242882 -0.0018246847 -0.0018250628 -0.0018253685 -0.0018255439 -0.00182552 -0.0018254636 -0.0018254932 -0.0018254757 -0.0018254165 -0.0018253933][-0.0018249704 -0.0018243025 -0.0018240251 -0.001823936 -0.001824168 -0.0018246541 -0.0018250757 -0.001825317 -0.001825425 -0.0018254241 -0.0018254291 -0.0018255252 -0.0018256092 -0.0018256516 -0.0018256373][-0.0018250197 -0.001824374 -0.0018240545 -0.001823962 -0.0018242204 -0.0018247667 -0.0018252644 -0.0018254982 -0.0018255956 -0.0018256399 -0.0018256848 -0.0018257796 -0.0018259193 -0.0018260462 -0.0018260691][-0.0018252437 -0.0018247445 -0.0018245411 -0.0018245238 -0.0018247592 -0.0018252461 -0.0018256921 -0.001825941 -0.0018260847 -0.001826168 -0.0018262254 -0.0018262588 -0.0018263465 -0.0018264696 -0.0018264486][-0.001825561 -0.0018251211 -0.0018250219 -0.0018251382 -0.001825384 -0.0018257297 -0.0018260744 -0.0018263645 -0.0018266471 -0.0018268062 -0.0018268313 -0.0018267795 -0.0018267628 -0.0018268069 -0.0018266895][-0.0018258123 -0.0018253177 -0.0018252241 -0.0018253677 -0.0018255929 -0.0018258435 -0.0018260961 -0.0018264288 -0.0018268626 -0.0018271897 -0.0018272542 -0.0018271452 -0.0018270696 -0.0018270946 -0.0018269338][-0.0018262501 -0.0018256709 -0.0018255134 -0.0018255415 -0.0018256637 -0.0018257784 -0.0018259009 -0.0018261849 -0.0018266871 -0.0018271211 -0.001827286 -0.0018272728 -0.0018273266 -0.0018273981 -0.0018272947][-0.0018270639 -0.0018264445 -0.0018262451 -0.0018261593 -0.0018261335 -0.0018261116 -0.001826101 -0.0018262372 -0.0018266493 -0.0018270639 -0.0018273098 -0.0018274318 -0.001827607 -0.0018277006 -0.0018275854][-0.0018281272 -0.0018274876 -0.0018273031 -0.0018271911 -0.0018271076 -0.0018269292 -0.0018267456 -0.001826635 -0.001826814 -0.0018271263 -0.0018273754 -0.0018275853 -0.0018278045 -0.0018278587 -0.0018277492][-0.0018289057 -0.0018283218 -0.0018281183 -0.0018279372 -0.0018277745 -0.0018274847 -0.0018271415 -0.0018268441 -0.0018268918 -0.0018271522 -0.0018274413 -0.0018276797 -0.0018278977 -0.0018279433 -0.0018278721][-0.0018290898 -0.0018285637 -0.0018283834 -0.0018281568 -0.0018278027 -0.0018273476 -0.0018268963 -0.0018265304 -0.0018264817 -0.0018267214 -0.0018271193 -0.0018275053 -0.0018278039 -0.0018279052 -0.0018279165][-0.0018288557 -0.0018283102 -0.0018281119 -0.001827911 -0.0018275506 -0.0018270208 -0.0018265209 -0.0018261413 -0.0018260578 -0.0018262616 -0.0018266751 -0.0018271504 -0.001827546 -0.0018277321 -0.0018278254]]...]
INFO - root - 2017-12-09 17:27:39.420982: step 43210, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.873 sec/batch; 70h:09m:54s remains)
INFO - root - 2017-12-09 17:27:48.145420: step 43220, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.875 sec/batch; 70h:17m:14s remains)
INFO - root - 2017-12-09 17:27:56.789919: step 43230, loss = 0.82, batch loss = 0.70 (9.1 examples/sec; 0.882 sec/batch; 70h:51m:31s remains)
INFO - root - 2017-12-09 17:28:05.540106: step 43240, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.872 sec/batch; 70h:02m:17s remains)
INFO - root - 2017-12-09 17:28:14.229773: step 43250, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 69h:13m:14s remains)
INFO - root - 2017-12-09 17:28:22.808448: step 43260, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 69h:15m:45s remains)
INFO - root - 2017-12-09 17:28:31.322364: step 43270, loss = 0.82, batch loss = 0.69 (9.7 examples/sec; 0.828 sec/batch; 66h:30m:09s remains)
INFO - root - 2017-12-09 17:28:39.748853: step 43280, loss = 0.83, batch loss = 0.70 (9.2 examples/sec; 0.870 sec/batch; 69h:54m:46s remains)
INFO - root - 2017-12-09 17:28:48.311903: step 43290, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.874 sec/batch; 70h:10m:38s remains)
INFO - root - 2017-12-09 17:28:57.035394: step 43300, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.859 sec/batch; 69h:00m:46s remains)
2017-12-09 17:28:57.858389: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0018294664 -0.0018282502 -0.0018273361 -0.0018267885 -0.0018262193 -0.0018257258 -0.0018255728 -0.0018256919 -0.0018254224 -0.0018249337 -0.001824877 -0.0018250683 -0.0018254051 -0.0018253707 -0.0018255224][-0.0018315829 -0.0018305437 -0.0018299037 -0.0018294469 -0.0018289119 -0.0018284374 -0.0018284282 -0.0018286427 -0.0018284647 -0.0018278824 -0.0018275486 -0.0018275868 -0.0018280538 -0.0018283693 -0.0018287335][-0.0018335363 -0.001832781 -0.0018321587 -0.0018315081 -0.0018308028 -0.001830157 -0.0018301036 -0.0018303869 -0.0018305685 -0.0018303017 -0.0018303819 -0.001830599 -0.0018311312 -0.0018318803 -0.001832518][-0.0018350241 -0.0018339695 -0.001832669 -0.0018316322 -0.0018307385 -0.0018299337 -0.0018300356 -0.001830299 -0.0018305989 -0.0018306797 -0.0018310716 -0.0018317119 -0.0018325842 -0.0018334371 -0.0018340825][-0.0018348187 -0.0018330567 -0.0018307812 -0.0018290404 -0.0018280139 -0.0018276734 -0.0018279921 -0.0018286674 -0.0018294986 -0.0018304243 -0.001831106 -0.0018319694 -0.0018329088 -0.0018340431 -0.0018347091][-0.0018335123 -0.001831207 -0.0018284607 -0.0018261748 -0.0018248829 -0.001824619 -0.0018253793 -0.0018268501 -0.0018284578 -0.0018300778 -0.001831308 -0.0018323621 -0.001833348 -0.0018339963 -0.0018339308][-0.0018324824 -0.0018301737 -0.0018273584 -0.0018248073 -0.0018230968 -0.0018224285 -0.0018229757 -0.0018242251 -0.0018260138 -0.0018279477 -0.0018295546 -0.0018309648 -0.0018320007 -0.0018325987 -0.0018324826][-0.0018314027 -0.0018293657 -0.0018269101 -0.0018242882 -0.0018222629 -0.0018214079 -0.0018217028 -0.0018227581 -0.0018242598 -0.0018260883 -0.0018276867 -0.0018289961 -0.0018296953 -0.0018299746 -0.0018297773][-0.0018307408 -0.001828779 -0.0018265272 -0.0018243006 -0.0018227111 -0.0018219453 -0.0018219883 -0.0018226571 -0.001823573 -0.0018247481 -0.0018258643 -0.0018267089 -0.0018273183 -0.0018276809 -0.0018276215][-0.0018317343 -0.0018298368 -0.0018281254 -0.0018264662 -0.0018251787 -0.0018246382 -0.001824767 -0.0018253285 -0.0018258821 -0.0018266282 -0.0018274355 -0.0018281442 -0.0018285745 -0.0018287693 -0.0018288358][-0.0018328676 -0.0018305292 -0.0018286688 -0.0018269847 -0.0018259448 -0.0018258007 -0.0018260831 -0.0018268523 -0.0018276067 -0.0018283285 -0.0018290619 -0.0018296839 -0.0018301281 -0.0018303985 -0.001830468][-0.0018315082 -0.0018278828 -0.0018250194 -0.0018227798 -0.0018215657 -0.0018215603 -0.0018222877 -0.0018235758 -0.0018249727 -0.0018261794 -0.0018275094 -0.0018287284 -0.0018299691 -0.0018310426 -0.0018319294][-0.001827216 -0.001821962 -0.0018178443 -0.0018149372 -0.0018136228 -0.0018136854 -0.0018147809 -0.0018166647 -0.0018189297 -0.0018211801 -0.0018234868 -0.0018260501 -0.0018287044 -0.0018311209 -0.0018330968][-0.0018216775 -0.0018147387 -0.0018093635 -0.0018057274 -0.0018042393 -0.0018046462 -0.0018064233 -0.0018090046 -0.0018119301 -0.0018151437 -0.0018186886 -0.0018226227 -0.0018263746 -0.0018298924 -0.0018329106][-0.0018176165 -0.0018094217 -0.0018031072 -0.0017989003 -0.0017971559 -0.0017975314 -0.0017994952 -0.0018023724 -0.0018058571 -0.0018098232 -0.001814353 -0.0018192602 -0.0018238556 -0.0018280267 -0.0018315557]]...]
INFO - root - 2017-12-09 17:29:06.606675: step 43310, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.858 sec/batch; 68h:54m:28s remains)
INFO - root - 2017-12-09 17:29:15.246379: step 43320, loss = 0.83, batch loss = 0.70 (9.2 examples/sec; 0.870 sec/batch; 69h:52m:06s remains)
INFO - root - 2017-12-09 17:29:23.929195: step 43330, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.872 sec/batch; 70h:03m:40s remains)
INFO - root - 2017-12-09 17:29:32.737816: step 43340, loss = 0.82, batch loss = 0.69 (9.8 examples/sec; 0.819 sec/batch; 65h:47m:39s remains)
INFO - root - 2017-12-09 17:29:41.416896: step 43350, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 69h:46m:32s remains)
INFO - root - 2017-12-09 17:29:50.154847: step 43360, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.859 sec/batch; 69h:00m:38s remains)
INFO - root - 2017-12-09 17:29:58.665697: step 43370, loss = 0.82, batch loss = 0.69 (11.2 examples/sec; 0.716 sec/batch; 57h:32m:33s remains)
INFO - root - 2017-12-09 17:30:07.041702: step 43380, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.875 sec/batch; 70h:15m:58s remains)
INFO - root - 2017-12-09 17:30:15.615931: step 43390, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.891 sec/batch; 71h:31m:02s remains)
INFO - root - 2017-12-09 17:30:24.369599: step 43400, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.882 sec/batch; 70h:48m:50s remains)
2017-12-09 17:30:25.287165: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0017977554 -0.0017966744 -0.0017969641 -0.0017965049 -0.0017954202 -0.0017934393 -0.001791245 -0.0017894905 -0.0017889609 -0.0017895492 -0.0017907488 -0.0017923475 -0.001793724 -0.0017944807 -0.0017946262][-0.0017960797 -0.0017948478 -0.0017951444 -0.0017947396 -0.0017937934 -0.0017919303 -0.0017898302 -0.001788157 -0.0017876112 -0.001788113 -0.0017891944 -0.001790692 -0.0017919929 -0.0017926756 -0.0017928282][-0.0017954385 -0.0017941169 -0.0017942397 -0.0017938295 -0.0017930447 -0.0017914815 -0.0017896763 -0.0017882348 -0.0017877304 -0.0017881301 -0.0017890207 -0.0017902695 -0.0017913585 -0.0017919571 -0.0017921367][-0.0017947255 -0.0017932542 -0.001793111 -0.0017926487 -0.0017919455 -0.001790693 -0.0017893161 -0.0017882962 -0.0017880276 -0.0017883847 -0.0017891355 -0.0017901417 -0.0017909592 -0.0017913413 -0.0017914996][-0.0017939832 -0.0017922957 -0.0017918429 -0.0017913026 -0.0017906269 -0.001789654 -0.0017887485 -0.001788285 -0.0017883966 -0.0017888055 -0.0017894685 -0.0017902532 -0.0017907425 -0.001790821 -0.0017908902][-0.0017932002 -0.0017913457 -0.0017906389 -0.001790055 -0.0017894171 -0.0017886499 -0.0017880494 -0.0017880218 -0.0017884747 -0.0017890309 -0.0017896387 -0.0017902441 -0.0017905192 -0.0017903658 -0.0017903002][-0.0017924965 -0.0017905381 -0.0017896625 -0.0017890909 -0.0017884893 -0.0017878717 -0.0017874915 -0.0017877287 -0.0017884474 -0.0017891468 -0.0017897271 -0.0017901658 -0.0017902285 -0.0017899072 -0.0017896869][-0.0017919882 -0.0017899777 -0.0017891539 -0.0017886737 -0.0017880992 -0.0017875549 -0.001787272 -0.0017876169 -0.0017884149 -0.0017891829 -0.0017897415 -0.0017900409 -0.0017899449 -0.001789516 -0.0017891986][-0.0017917437 -0.0017898717 -0.0017892275 -0.0017888753 -0.0017883256 -0.0017878218 -0.0017875509 -0.0017878793 -0.0017886367 -0.0017893721 -0.0017898651 -0.0017900744 -0.0017898965 -0.0017894149 -0.0017890242][-0.0017918405 -0.0017902411 -0.0017897818 -0.0017895297 -0.0017890402 -0.0017885753 -0.0017882619 -0.0017884676 -0.0017890814 -0.0017896885 -0.001790068 -0.0017901466 -0.0017899341 -0.0017894779 -0.0017890725][-0.0017923804 -0.0017910224 -0.0017907274 -0.0017905699 -0.0017901551 -0.0017897239 -0.0017893724 -0.0017894367 -0.0017898369 -0.0017902441 -0.0017904777 -0.001790511 -0.0017903553 -0.0017899734 -0.0017896023][-0.0017934209 -0.0017921142 -0.0017919319 -0.0017918489 -0.0017915095 -0.0017911466 -0.0017908186 -0.0017907883 -0.001791005 -0.0017912077 -0.0017912973 -0.0017912622 -0.0017911411 -0.0017908182 -0.0017904656][-0.0017946502 -0.0017933388 -0.0017931715 -0.0017931105 -0.0017928532 -0.0017925628 -0.0017922728 -0.0017921792 -0.0017922476 -0.0017922933 -0.0017922792 -0.0017922049 -0.0017921586 -0.0017919552 -0.0017916786][-0.0017957417 -0.0017943437 -0.0017941259 -0.001794075 -0.0017939133 -0.0017937281 -0.0017935236 -0.0017934361 -0.0017934293 -0.001793399 -0.0017933577 -0.0017932887 -0.0017933112 -0.0017932731 -0.0017930957][-0.0017964095 -0.0017949718 -0.0017946638 -0.0017946515 -0.001794604 -0.0017945572 -0.0017944759 -0.0017944283 -0.0017943969 -0.0017943475 -0.0017943045 -0.0017942868 -0.0017943891 -0.0017944737 -0.0017943911]]...]
INFO - root - 2017-12-09 17:30:34.102093: step 43410, loss = 0.82, batch loss = 0.69 (8.1 examples/sec; 0.987 sec/batch; 79h:14m:42s remains)
INFO - root - 2017-12-09 17:30:42.888841: step 43420, loss = 0.82, batch loss = 0.69 (8.8 examples/sec; 0.913 sec/batch; 73h:19m:14s remains)
INFO - root - 2017-12-09 17:30:51.689855: step 43430, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.885 sec/batch; 71h:04m:53s remains)
INFO - root - 2017-12-09 17:31:00.612690: step 43440, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.890 sec/batch; 71h:26m:30s remains)
INFO - root - 2017-12-09 17:31:09.413566: step 43450, loss = 0.82, batch loss = 0.69 (8.8 examples/sec; 0.907 sec/batch; 72h:47m:19s remains)
INFO - root - 2017-12-09 17:31:18.123084: step 43460, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.870 sec/batch; 69h:51m:36s remains)
INFO - root - 2017-12-09 17:31:26.652320: step 43470, loss = 0.82, batch loss = 0.69 (10.1 examples/sec; 0.791 sec/batch; 63h:30m:18s remains)
INFO - root - 2017-12-09 17:31:35.065271: step 43480, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 69h:11m:56s remains)
INFO - root - 2017-12-09 17:31:43.675454: step 43490, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.884 sec/batch; 70h:57m:53s remains)
INFO - root - 2017-12-09 17:31:52.231959: step 43500, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.872 sec/batch; 69h:59m:54s remains)
2017-12-09 17:31:53.117080: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0018215667 -0.0018201327 -0.0018194685 -0.0018192639 -0.0018194937 -0.0018199265 -0.0018206036 -0.0018214864 -0.0018224199 -0.001823161 -0.0018238173 -0.0018241521 -0.0018242253 -0.0018236557 -0.0018224776][-0.0018191714 -0.0018180995 -0.0018177391 -0.0018178318 -0.0018183136 -0.0018190712 -0.0018198917 -0.0018208879 -0.0018218519 -0.001822461 -0.0018227125 -0.0018228763 -0.0018228114 -0.0018220373 -0.0018206803][-0.0018135816 -0.0018091529 -0.0018070106 -0.0018097947 -0.0018127401 -0.0018147393 -0.0018167108 -0.0018188424 -0.0018202431 -0.0018188858 -0.0018168277 -0.0018148307 -0.0018150461 -0.00181615 -0.0018157284][-0.0018055892 -0.0017991108 -0.0017939912 -0.0017862311 -0.0017846143 -0.0017909292 -0.0017934453 -0.0017919987 -0.001789173 -0.0017823543 -0.0017721697 -0.0017685011 -0.0017777617 -0.00179001 -0.0017962286][-0.001788834 -0.0017680676 -0.0017557782 -0.0017538854 -0.0017528622 -0.0017394433 -0.0017134884 -0.001679788 -0.0016494755 -0.0016092069 -0.0015669296 -0.0015574427 -0.0015962657 -0.0016593934 -0.0017067805][-0.0017795719 -0.0017510735 -0.0017149058 -0.0016697717 -0.0016338656 -0.0015869886 -0.0015044899 -0.0014004443 -0.0013030523 -0.0012100511 -0.0011443971 -0.001153867 -0.0012610873 -0.0014159452 -0.0015483433][-0.0017554387 -0.0016975565 -0.0016230656 -0.0015375034 -0.0014518162 -0.0013181404 -0.0011263812 -0.00089683477 -0.00069653511 -0.00053594785 -0.00044617394 -0.00050217437 -0.00073217996 -0.0010397644 -0.0013070807][-0.0017234867 -0.0016138733 -0.0014435102 -0.0012559673 -0.0011049465 -0.00093759311 -0.00069049071 -0.00035009033 -1.2282166e-05 0.00023227651 0.00030957512 0.0001657696 -0.00019700499 -0.00065261417 -0.0010645147][-0.001680475 -0.001504445 -0.0012295878 -0.00093976833 -0.00071570021 -0.00053518126 -0.00029633823 6.8795285e-05 0.00045256689 0.00071359379 0.00075214892 0.00051367353 4.7107693e-05 -0.00049852394 -0.00097641995][-0.0016410648 -0.0014122459 -0.0010823588 -0.0007364183 -0.0004554519 -0.00025931047 -4.8707938e-05 0.00027138693 0.00061224191 0.00080647087 0.00075256615 0.00042586634 -8.22657e-05 -0.00062136888 -0.0010740395][-0.0016583053 -0.001443903 -0.0011442783 -0.00083556317 -0.00056609442 -0.0003703878 -0.00019652653 3.801682e-05 0.00028131832 0.00039335317 0.00027974322 -6.4910389e-05 -0.00051984692 -0.00095193437 -0.0012929791][-0.0017250427 -0.0015758914 -0.0013563532 -0.0011222647 -0.00091857807 -0.000762398 -0.00062860781 -0.00048021798 -0.00035041047 -0.00031920325 -0.00044708245 -0.000725414 -0.0010497363 -0.0013284676 -0.0015319654][-0.0017873602 -0.0017209848 -0.0016134569 -0.0014866748 -0.0013768886 -0.0012925145 -0.0012158873 -0.0011278091 -0.0010593644 -0.0010553736 -0.0011406005 -0.0013007206 -0.0014720784 -0.0016069603 -0.0017004878][-0.0018176559 -0.0017982662 -0.0017628539 -0.0017154822 -0.001673975 -0.001643402 -0.0016164634 -0.0015811996 -0.0015528409 -0.001553533 -0.0015936366 -0.0016585195 -0.0017194364 -0.0017625146 -0.0017916281][-0.001826223 -0.0018228765 -0.0018151293 -0.001803087 -0.0017918871 -0.0017833379 -0.0017761473 -0.001767024 -0.0017590296 -0.0017582783 -0.0017688232 -0.0017852498 -0.0017999879 -0.0018094789 -0.0018158029]]...]
INFO - root - 2017-12-09 17:32:01.823961: step 43510, loss = 0.82, batch loss = 0.70 (9.4 examples/sec; 0.855 sec/batch; 68h:39m:59s remains)
INFO - root - 2017-12-09 17:32:10.417617: step 43520, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.846 sec/batch; 67h:54m:28s remains)
INFO - root - 2017-12-09 17:32:19.061145: step 43530, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.863 sec/batch; 69h:15m:17s remains)
INFO - root - 2017-12-09 17:32:27.655084: step 43540, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.880 sec/batch; 70h:35m:52s remains)
INFO - root - 2017-12-09 17:32:36.314723: step 43550, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.850 sec/batch; 68h:11m:43s remains)
INFO - root - 2017-12-09 17:32:45.092618: step 43560, loss = 0.83, batch loss = 0.70 (9.6 examples/sec; 0.836 sec/batch; 67h:07m:21s remains)
INFO - root - 2017-12-09 17:32:53.740964: step 43570, loss = 0.82, batch loss = 0.69 (8.8 examples/sec; 0.909 sec/batch; 72h:55m:22s remains)
INFO - root - 2017-12-09 17:33:02.155546: step 43580, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.863 sec/batch; 69h:17m:49s remains)
INFO - root - 2017-12-09 17:33:10.833267: step 43590, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.860 sec/batch; 69h:03m:23s remains)
INFO - root - 2017-12-09 17:33:19.495971: step 43600, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.891 sec/batch; 71h:30m:58s remains)
2017-12-09 17:33:20.463077: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0018099253 -0.0018088652 -0.001808651 -0.0018084568 -0.0018082908 -0.001808211 -0.0018082098 -0.0018083462 -0.0018085367 -0.0018087139 -0.001808822 -0.0018088061 -0.0018087146 -0.0018086721 -0.001808658][-0.0018089968 -0.0018080384 -0.0018078184 -0.0018076102 -0.0018074125 -0.0018073079 -0.0018072798 -0.0018073728 -0.0018075719 -0.0018078377 -0.0018080859 -0.0018082301 -0.0018082787 -0.0018083059 -0.0018083474][-0.001808997 -0.0018080925 -0.0018078213 -0.0018075483 -0.0018072625 -0.0018070915 -0.0018070241 -0.0018070955 -0.0018073192 -0.0018076511 -0.0018079933 -0.0018082568 -0.001808419 -0.0018085227 -0.0018086137][-0.0018092765 -0.0018083957 -0.0018080436 -0.0018076915 -0.0018073008 -0.0018070508 -0.0018069232 -0.0018069575 -0.0018071712 -0.0018075273 -0.0018079105 -0.0018082376 -0.0018084851 -0.0018086747 -0.0018088259][-0.0018097094 -0.0018088234 -0.0018084079 -0.0018079786 -0.0018074957 -0.001807176 -0.0018069713 -0.0018069366 -0.0018071124 -0.0018074396 -0.0018078175 -0.0018081625 -0.0018084511 -0.0018087031 -0.0018089074][-0.001810016 -0.0018091613 -0.0018087486 -0.0018082745 -0.001807733 -0.0018073462 -0.0018070784 -0.0018069723 -0.0018070962 -0.0018073781 -0.0018077281 -0.0018080698 -0.0018083655 -0.0018086282 -0.0018088527][-0.0018101527 -0.0018092406 -0.001808835 -0.0018083631 -0.0018078339 -0.0018074386 -0.0018071609 -0.0018070532 -0.0018071557 -0.0018073927 -0.0018076934 -0.0018080054 -0.0018082773 -0.0018085275 -0.0018087479][-0.0018101106 -0.0018090885 -0.001808721 -0.0018083073 -0.0018078655 -0.0018075301 -0.0018073036 -0.0018072332 -0.0018073367 -0.0018075367 -0.0018077712 -0.00180803 -0.0018082567 -0.0018084766 -0.0018086823][-0.0018097183 -0.0018086729 -0.0018084184 -0.0018081138 -0.001807795 -0.0018075501 -0.0018073837 -0.0018073404 -0.0018074232 -0.0018075713 -0.001807737 -0.0018079267 -0.001808112 -0.0018083116 -0.001808524][-0.0018089574 -0.0018080384 -0.0018079262 -0.0018077706 -0.0018075949 -0.0018074461 -0.0018073445 -0.0018073294 -0.0018073911 -0.0018074935 -0.0018076058 -0.0018077444 -0.0018078825 -0.0018080346 -0.0018082237][-0.001808192 -0.001807324 -0.0018072901 -0.0018072356 -0.00180716 -0.0018070887 -0.0018070464 -0.001807083 -0.0018071744 -0.0018072755 -0.0018073784 -0.0018075043 -0.0018076034 -0.0018076894 -0.0018078183][-0.0018075352 -0.0018066209 -0.0018065762 -0.0018065731 -0.0018065559 -0.0018065412 -0.0018065476 -0.0018066422 -0.0018067716 -0.0018068813 -0.0018069817 -0.0018070872 -0.0018071596 -0.0018071907 -0.0018072564][-0.0018068105 -0.0018058537 -0.0018057348 -0.0018057432 -0.0018057439 -0.0018057702 -0.0018058264 -0.0018059569 -0.0018061004 -0.001806205 -0.0018062777 -0.0018063299 -0.0018063611 -0.0018063656 -0.001806395][-0.0018064062 -0.0018053539 -0.0018051498 -0.0018051595 -0.0018051699 -0.0018052211 -0.0018053057 -0.0018054364 -0.001805551 -0.0018056141 -0.0018056402 -0.0018056342 -0.0018056168 -0.0018055968 -0.0018056108][-0.0018065737 -0.001805459 -0.0018051602 -0.0018051704 -0.0018051751 -0.0018052213 -0.0018053066 -0.0018054143 -0.0018054793 -0.0018054919 -0.0018054772 -0.00180543 -0.0018053685 -0.0018053184 -0.0018053155]]...]
INFO - root - 2017-12-09 17:33:29.231389: step 43610, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.875 sec/batch; 70h:12m:28s remains)
INFO - root - 2017-12-09 17:33:37.895974: step 43620, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.885 sec/batch; 71h:02m:03s remains)
INFO - root - 2017-12-09 17:33:46.661384: step 43630, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.881 sec/batch; 70h:43m:39s remains)
INFO - root - 2017-12-09 17:33:55.295686: step 43640, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.856 sec/batch; 68h:40m:21s remains)
INFO - root - 2017-12-09 17:34:03.826649: step 43650, loss = 0.83, batch loss = 0.70 (9.1 examples/sec; 0.875 sec/batch; 70h:13m:08s remains)
INFO - root - 2017-12-09 17:34:12.447126: step 43660, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 69h:50m:53s remains)
INFO - root - 2017-12-09 17:34:20.975769: step 43670, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.879 sec/batch; 70h:31m:52s remains)
INFO - root - 2017-12-09 17:34:29.375164: step 43680, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.884 sec/batch; 70h:56m:55s remains)
INFO - root - 2017-12-09 17:34:38.038944: step 43690, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.861 sec/batch; 69h:03m:34s remains)
INFO - root - 2017-12-09 17:34:46.751982: step 43700, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.878 sec/batch; 70h:24m:56s remains)
2017-12-09 17:34:47.625641: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0018197828 -0.0018180802 -0.0018167342 -0.001815252 -0.0018137228 -0.0018124926 -0.0018115931 -0.0018112542 -0.0018113871 -0.0018117657 -0.0018122339 -0.0018125321 -0.0018128237 -0.0018131172 -0.0018134243][-0.0018193302 -0.0018178459 -0.0018167271 -0.0018155993 -0.0018144429 -0.0018134111 -0.0018125224 -0.0018119585 -0.0018117069 -0.0018116781 -0.0018119185 -0.0018121959 -0.0018125671 -0.0018129286 -0.0018132236][-0.0018193736 -0.001818178 -0.0018175076 -0.0018169718 -0.0018163825 -0.0018157281 -0.001814867 -0.0018139634 -0.0018130671 -0.0018123203 -0.0018121155 -0.0018122223 -0.0018126036 -0.0018130437 -0.0018133926][-0.0018195136 -0.0018187417 -0.0018188221 -0.0018191806 -0.0018193902 -0.0018191081 -0.0018181067 -0.001816586 -0.0018148451 -0.0018134282 -0.0018128139 -0.0018127172 -0.0018128967 -0.0018132447 -0.0018136183][-0.0018198686 -0.0018196903 -0.0018206429 -0.0018219486 -0.0018228535 -0.0018228224 -0.0018216771 -0.001819591 -0.0018171446 -0.0018150448 -0.0018138153 -0.0018133287 -0.001813206 -0.0018133675 -0.0018137055][-0.0018204739 -0.001820963 -0.0018226585 -0.0018247035 -0.0018261289 -0.0018262008 -0.0018248364 -0.0018222883 -0.001819148 -0.0018162833 -0.0018144484 -0.0018135336 -0.0018131746 -0.0018132683 -0.0018136252][-0.0018210766 -0.0018221965 -0.0018243849 -0.0018268554 -0.0018285271 -0.0018287435 -0.0018272065 -0.0018242378 -0.0018204898 -0.001817089 -0.0018148914 -0.0018137757 -0.0018133756 -0.0018135369 -0.0018138607][-0.0018214603 -0.0018229721 -0.0018255261 -0.0018281993 -0.0018299929 -0.0018303629 -0.0018286639 -0.0018253677 -0.0018217576 -0.0018189829 -0.0018170448 -0.0018157783 -0.0018148859 -0.0018146404 -0.0018146025][-0.0018216824 -0.0018234798 -0.0018262455 -0.0018291248 -0.0018310848 -0.0018315706 -0.0018300726 -0.0018271367 -0.0018244281 -0.0018226318 -0.0018211752 -0.0018195958 -0.0018178336 -0.0018166765 -0.0018158708][-0.0018218575 -0.0018239797 -0.0018270619 -0.0018301671 -0.0018322306 -0.0018329389 -0.0018318922 -0.0018299468 -0.0018282098 -0.0018269013 -0.0018255311 -0.0018238324 -0.0018215138 -0.0018193722 -0.0018175405][-0.0018222545 -0.0018247457 -0.0018282203 -0.0018315268 -0.0018337672 -0.0018348924 -0.001834666 -0.0018338262 -0.0018328633 -0.0018316503 -0.0018300476 -0.0018278485 -0.0018247357 -0.0018217767 -0.0018191204][-0.0018229085 -0.0018255527 -0.0018292732 -0.001832823 -0.001835386 -0.0018370234 -0.0018374758 -0.0018371916 -0.0018366177 -0.0018356197 -0.0018338823 -0.0018313169 -0.0018276541 -0.001823997 -0.0018205036][-0.0018234762 -0.0018262138 -0.0018298993 -0.0018334404 -0.0018361167 -0.001837899 -0.0018385173 -0.0018384351 -0.0018381184 -0.0018373869 -0.0018358097 -0.001833037 -0.0018291274 -0.0018250814 -0.0018211876][-0.0018237862 -0.00182638 -0.0018297548 -0.0018329576 -0.0018353974 -0.0018370935 -0.001837567 -0.0018371325 -0.0018365466 -0.0018356584 -0.0018342033 -0.0018317982 -0.0018284 -0.001824701 -0.0018211394][-0.0018235234 -0.0018256938 -0.0018285236 -0.0018311697 -0.0018331454 -0.0018344946 -0.0018347924 -0.0018341716 -0.0018333808 -0.0018324818 -0.0018311332 -0.001829201 -0.0018265302 -0.001823555 -0.0018205284]]...]
INFO - root - 2017-12-09 17:34:56.293042: step 43710, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.876 sec/batch; 70h:16m:31s remains)
INFO - root - 2017-12-09 17:35:04.965871: step 43720, loss = 0.82, batch loss = 0.69 (9.7 examples/sec; 0.826 sec/batch; 66h:15m:44s remains)
INFO - root - 2017-12-09 17:35:13.618700: step 43730, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.870 sec/batch; 69h:49m:20s remains)
INFO - root - 2017-12-09 17:35:22.370124: step 43740, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.851 sec/batch; 68h:16m:47s remains)
INFO - root - 2017-12-09 17:35:31.048482: step 43750, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.860 sec/batch; 68h:58m:40s remains)
INFO - root - 2017-12-09 17:35:39.688378: step 43760, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.852 sec/batch; 68h:17m:54s remains)
INFO - root - 2017-12-09 17:35:48.347878: step 43770, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.876 sec/batch; 70h:16m:57s remains)
INFO - root - 2017-12-09 17:35:56.580420: step 43780, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.870 sec/batch; 69h:48m:24s remains)
INFO - root - 2017-12-09 17:36:05.065668: step 43790, loss = 0.82, batch loss = 0.69 (9.7 examples/sec; 0.821 sec/batch; 65h:52m:34s remains)
INFO - root - 2017-12-09 17:36:13.737978: step 43800, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.875 sec/batch; 70h:12m:32s remains)
2017-12-09 17:36:14.659540: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.12003662 0.12040088 0.11836276 0.11484822 0.10961755 0.10336973 0.096638948 0.090115905 0.08495599 0.081437394 0.079722926 0.079446025 0.079829067 0.080752909 0.08129321][0.1253285 0.12719411 0.12666829 0.12440897 0.12012357 0.11432275 0.10769416 0.10087231 0.095556058 0.092079848 0.09097977 0.092042156 0.094268128 0.097171769 0.09933871][0.12943996 0.13291074 0.13385527 0.13302805 0.13003057 0.12539276 0.11943055 0.11304852 0.10815986 0.10512371 0.10469663 0.10665064 0.11032898 0.11428508 0.11723094][0.13293712 0.13812913 0.14070833 0.14134583 0.13964021 0.13624142 0.13127677 0.12592375 0.12166429 0.11919177 0.11945795 0.12210398 0.12653837 0.13093299 0.1339433][0.1372093 0.14398254 0.14775325 0.14942648 0.14866048 0.14617617 0.14229554 0.13796332 0.1346285 0.13303468 0.1341078 0.13704306 0.14161325 0.14593734 0.14850755][0.14240985 0.15078111 0.15546018 0.15791373 0.15773271 0.15615694 0.15316626 0.14988334 0.1475473 0.14652121 0.14789698 0.15047419 0.15440007 0.15767169 0.15915921][0.14878793 0.15856524 0.16390958 0.166842 0.16725415 0.16619022 0.16395751 0.16147968 0.15995803 0.15948346 0.16055742 0.16228166 0.16480672 0.16654444 0.16661406][0.15523466 0.16629432 0.17229478 0.17583993 0.17671658 0.17617738 0.17448568 0.17247334 0.17106062 0.17035331 0.17066969 0.17107417 0.17177631 0.17160872 0.1699879][0.16068998 0.17307755 0.17982802 0.18397669 0.1853341 0.18510658 0.1836601 0.18155967 0.17952241 0.17794411 0.17682435 0.17534912 0.17399637 0.17218029 0.1692993][0.16385111 0.17701864 0.18452831 0.18942772 0.19149181 0.1915883 0.19002484 0.1871959 0.18405971 0.18107411 0.17810129 0.17485963 0.17158495 0.16814107 0.16422683][0.16247271 0.17567936 0.18353209 0.18918239 0.1920823 0.19291194 0.1915406 0.18846425 0.18432444 0.17960584 0.1748168 0.16958079 0.16457616 0.15966551 0.15483195][0.15629433 0.16933355 0.17742409 0.18351011 0.1869002 0.1880569 0.18685056 0.18360552 0.17912678 0.17376775 0.16823864 0.16201213 0.15580885 0.14973992 0.14394079][0.1464611 0.15885329 0.16688514 0.17336287 0.1773434 0.17890447 0.17786922 0.17467442 0.16983958 0.16406779 0.15812528 0.15153396 0.14507143 0.13847423 0.13208476][0.13350239 0.14484942 0.15230058 0.15859413 0.1628866 0.16502513 0.16460066 0.16178794 0.15747139 0.15203932 0.14623596 0.13968092 0.13306311 0.12652785 0.11999992][0.11914439 0.12914088 0.13589205 0.1419052 0.14631946 0.14880922 0.14899433 0.14680107 0.1432175 0.13841046 0.13308008 0.12706499 0.12087311 0.1146627 0.10809961]]...]
INFO - root - 2017-12-09 17:36:23.200260: step 43810, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.841 sec/batch; 67h:27m:03s remains)
INFO - root - 2017-12-09 17:36:31.765788: step 43820, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.840 sec/batch; 67h:22m:20s remains)
INFO - root - 2017-12-09 17:36:40.631687: step 43830, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.886 sec/batch; 71h:03m:39s remains)
INFO - root - 2017-12-09 17:36:49.280213: step 43840, loss = 0.83, batch loss = 0.70 (9.0 examples/sec; 0.888 sec/batch; 71h:11m:21s remains)
INFO - root - 2017-12-09 17:36:58.057662: step 43850, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 69h:32m:05s remains)
INFO - root - 2017-12-09 17:37:06.731905: step 43860, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.894 sec/batch; 71h:39m:01s remains)
INFO - root - 2017-12-09 17:37:15.305491: step 43870, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.885 sec/batch; 70h:55m:34s remains)
INFO - root - 2017-12-09 17:37:23.807665: step 43880, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 69h:07m:27s remains)
INFO - root - 2017-12-09 17:37:32.577089: step 43890, loss = 0.82, batch loss = 0.69 (8.7 examples/sec; 0.915 sec/batch; 73h:22m:51s remains)
INFO - root - 2017-12-09 17:37:41.439843: step 43900, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.888 sec/batch; 71h:12m:38s remains)
2017-12-09 17:37:42.285375: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0018024223 -0.0018042465 -0.0018079281 -0.0018106867 -0.001811293 -0.0018094633 -0.0018056969 -0.0018013662 -0.0017975505 -0.0017957604 -0.0017953474 -0.0017958427 -0.0017964387 -0.0017969812 -0.0017975658][-0.0018049654 -0.0018057727 -0.0018082903 -0.001809819 -0.0018094855 -0.0018071056 -0.0018032634 -0.001798974 -0.0017953452 -0.0017934649 -0.0017928296 -0.0017931844 -0.0017937233 -0.0017944755 -0.0017954654][-0.0018108421 -0.0018105423 -0.0018112434 -0.001811167 -0.0018093759 -0.0018061526 -0.0018020978 -0.0017977818 -0.0017942891 -0.0017922752 -0.0017914401 -0.0017915793 -0.0017919539 -0.0017927754 -0.0017938904][-0.0018159779 -0.0018143086 -0.0018133487 -0.001811793 -0.0018089738 -0.0018052059 -0.0018010415 -0.0017968819 -0.0017936158 -0.0017916018 -0.0017907143 -0.0017906694 -0.0017909239 -0.0017917656 -0.0017929152][-0.0018190735 -0.0018162832 -0.0018140767 -0.0018115696 -0.0018080226 -0.0018039821 -0.0017999788 -0.0017961637 -0.0017931206 -0.0017912004 -0.0017903346 -0.0017902777 -0.0017905771 -0.0017913972 -0.0017925774][-0.0018195647 -0.0018160697 -0.0018130044 -0.0018098822 -0.0018061197 -0.0018022642 -0.0017986501 -0.0017952763 -0.0017925105 -0.0017907204 -0.0017899227 -0.0017898766 -0.0017902727 -0.0017912355 -0.0017925766][-0.0018173233 -0.0018133989 -0.0018099379 -0.001806737 -0.0018032814 -0.0017999738 -0.0017969743 -0.0017942397 -0.0017918847 -0.0017902623 -0.0017895376 -0.0017895224 -0.0017900206 -0.0017911362 -0.0017926694][-0.0018137107 -0.0018098265 -0.001806461 -0.0018035301 -0.0018005546 -0.0017979061 -0.0017954711 -0.001793334 -0.0017914348 -0.0017900101 -0.0017893423 -0.0017893297 -0.0017898505 -0.001790982 -0.0017925933][-0.0018092558 -0.0018057998 -0.001803059 -0.0018006808 -0.0017983244 -0.0017962297 -0.0017943311 -0.0017927018 -0.0017912037 -0.0017900023 -0.0017893721 -0.0017893236 -0.001789763 -0.0017907994 -0.0017923631][-0.0018046163 -0.0018018459 -0.0017997501 -0.0017979281 -0.0017961706 -0.0017946695 -0.0017933029 -0.001792157 -0.001791055 -0.0017901125 -0.0017895483 -0.0017894629 -0.0017897806 -0.001790642 -0.0017920255][-0.0018010949 -0.0017988447 -0.0017973201 -0.0017959612 -0.0017946888 -0.0017936277 -0.0017926665 -0.0017918407 -0.0017910325 -0.0017903298 -0.0017898404 -0.0017896878 -0.0017898397 -0.0017904869 -0.0017916181][-0.0017988529 -0.0017968586 -0.0017956611 -0.0017946041 -0.0017936336 -0.001792865 -0.001792215 -0.0017916709 -0.0017910841 -0.0017905796 -0.0017901449 -0.0017899334 -0.001789935 -0.0017903371 -0.0017911603][-0.0017976706 -0.0017959633 -0.0017951265 -0.0017943758 -0.0017935862 -0.0017930057 -0.0017924469 -0.0017919575 -0.0017914139 -0.0017909575 -0.0017904826 -0.001790149 -0.0017899984 -0.0017901878 -0.0017907261][-0.0017975704 -0.0017959588 -0.0017953551 -0.0017948444 -0.0017942002 -0.0017936431 -0.0017930499 -0.0017924996 -0.0017919103 -0.0017914061 -0.0017908585 -0.001790404 -0.0017900895 -0.0017901079 -0.0017904444][-0.0017984803 -0.001796936 -0.001796308 -0.0017958221 -0.0017951919 -0.0017946078 -0.0017939584 -0.0017933024 -0.0017925716 -0.0017919447 -0.0017912891 -0.0017907177 -0.001790288 -0.0017901651 -0.0017903494]]...]
INFO - root - 2017-12-09 17:37:50.884253: step 43910, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.857 sec/batch; 68h:43m:23s remains)
INFO - root - 2017-12-09 17:37:59.611694: step 43920, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.886 sec/batch; 71h:01m:48s remains)
INFO - root - 2017-12-09 17:38:08.441118: step 43930, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.849 sec/batch; 68h:01m:05s remains)
INFO - root - 2017-12-09 17:38:17.259346: step 43940, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.877 sec/batch; 70h:16m:17s remains)
INFO - root - 2017-12-09 17:38:26.107115: step 43950, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.887 sec/batch; 71h:07m:46s remains)
INFO - root - 2017-12-09 17:38:34.823648: step 43960, loss = 0.82, batch loss = 0.69 (10.3 examples/sec; 0.777 sec/batch; 62h:18m:21s remains)
INFO - root - 2017-12-09 17:38:43.581562: step 43970, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.881 sec/batch; 70h:37m:13s remains)
INFO - root - 2017-12-09 17:38:52.027885: step 43980, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.843 sec/batch; 67h:31m:47s remains)
INFO - root - 2017-12-09 17:39:00.697417: step 43990, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.860 sec/batch; 68h:55m:43s remains)
INFO - root - 2017-12-09 17:39:09.365212: step 44000, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.863 sec/batch; 69h:09m:40s remains)
2017-12-09 17:39:10.301846: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0018274878 -0.001827189 -0.0018272692 -0.001827332 -0.0018271527 -0.0018266495 -0.0018260216 -0.0018254117 -0.0018249688 -0.001824761 -0.0018247036 -0.0018246564 -0.0018245631 -0.0018244659 -0.0018244301][-0.0018270889 -0.0018268223 -0.0018268963 -0.0018270288 -0.0018269389 -0.0018264743 -0.0018258303 -0.0018251813 -0.0018247151 -0.0018244346 -0.0018243272 -0.0018242635 -0.0018241338 -0.0018240163 -0.0018239813][-0.0018269254 -0.0018266826 -0.0018267474 -0.0018269371 -0.0018269799 -0.0018266172 -0.00182602 -0.0018254074 -0.0018249596 -0.0018246016 -0.001824395 -0.0018242749 -0.0018240946 -0.0018239316 -0.0018238466][-0.0018267804 -0.0018265342 -0.001826609 -0.0018269129 -0.0018270988 -0.0018268334 -0.0018262764 -0.0018256939 -0.0018252707 -0.0018248615 -0.0018246142 -0.0018245348 -0.0018243745 -0.0018241392 -0.0018239141][-0.0018265902 -0.0018263449 -0.0018264494 -0.0018268672 -0.0018271729 -0.0018269907 -0.0018265035 -0.001825975 -0.0018255569 -0.0018250704 -0.0018247676 -0.0018247408 -0.0018246603 -0.0018244043 -0.0018240707][-0.0018263786 -0.0018261372 -0.0018262868 -0.0018267048 -0.0018270417 -0.0018269636 -0.0018265959 -0.001826131 -0.001825711 -0.0018251998 -0.001824872 -0.001824881 -0.0018248671 -0.0018246195 -0.0018242202][-0.001826194 -0.0018259513 -0.0018261267 -0.0018264935 -0.0018267668 -0.0018267113 -0.0018263879 -0.0018259108 -0.0018255051 -0.0018250133 -0.0018247721 -0.0018248464 -0.0018248782 -0.0018246594 -0.0018242376][-0.0018260425 -0.0018257288 -0.0018258879 -0.0018261891 -0.0018263655 -0.0018262753 -0.0018259006 -0.0018253311 -0.0018249446 -0.001824578 -0.0018244756 -0.0018246426 -0.0018247376 -0.0018245835 -0.0018241791][-0.001825853 -0.0018253733 -0.0018254204 -0.0018255996 -0.001825697 -0.001825616 -0.0018253186 -0.0018248091 -0.001824538 -0.0018243212 -0.0018242728 -0.001824441 -0.0018245497 -0.0018244478 -0.0018241064][-0.0018256641 -0.0018250784 -0.0018249666 -0.0018249876 -0.0018250233 -0.0018249957 -0.0018247938 -0.0018244168 -0.0018243102 -0.0018242194 -0.0018241002 -0.0018241782 -0.0018242995 -0.0018242606 -0.001824019][-0.0018256655 -0.0018250231 -0.0018247643 -0.00182466 -0.0018246321 -0.0018245857 -0.0018244012 -0.001824146 -0.0018241709 -0.0018241388 -0.0018239851 -0.0018240002 -0.0018240964 -0.0018240846 -0.0018239123][-0.0018258073 -0.0018251864 -0.0018248338 -0.0018246546 -0.0018245566 -0.0018244502 -0.0018242265 -0.0018240471 -0.0018241478 -0.0018241303 -0.0018239666 -0.0018239361 -0.0018240061 -0.0018239437 -0.0018238045][-0.0018258421 -0.0018252783 -0.0018248875 -0.0018246474 -0.0018245095 -0.0018243662 -0.0018241865 -0.001824148 -0.001824349 -0.0018243769 -0.0018242112 -0.0018240948 -0.0018240093 -0.0018238268 -0.0018236917][-0.0018257113 -0.0018252177 -0.0018248284 -0.0018245755 -0.0018244206 -0.0018242962 -0.0018242269 -0.0018243548 -0.0018246137 -0.0018246532 -0.0018244846 -0.0018242541 -0.0018240118 -0.0018237532 -0.0018236412][-0.001825423 -0.0018249751 -0.00182465 -0.0018244459 -0.0018243011 -0.0018242106 -0.0018242511 -0.0018244651 -0.001824687 -0.0018246522 -0.0018244486 -0.0018241715 -0.001823893 -0.0018236811 -0.0018236468]]...]
INFO - root - 2017-12-09 17:39:18.984684: step 44010, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.859 sec/batch; 68h:47m:48s remains)
INFO - root - 2017-12-09 17:39:27.765981: step 44020, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 69h:29m:02s remains)
INFO - root - 2017-12-09 17:39:36.532866: step 44030, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.884 sec/batch; 70h:52m:19s remains)
INFO - root - 2017-12-09 17:39:45.167744: step 44040, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.880 sec/batch; 70h:28m:38s remains)
INFO - root - 2017-12-09 17:39:53.857083: step 44050, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.873 sec/batch; 69h:55m:40s remains)
INFO - root - 2017-12-09 17:40:02.509685: step 44060, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.884 sec/batch; 70h:51m:42s remains)
INFO - root - 2017-12-09 17:40:11.149044: step 44070, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.881 sec/batch; 70h:35m:36s remains)
INFO - root - 2017-12-09 17:40:19.464459: step 44080, loss = 0.82, batch loss = 0.69 (10.3 examples/sec; 0.777 sec/batch; 62h:15m:45s remains)
INFO - root - 2017-12-09 17:40:28.052362: step 44090, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.857 sec/batch; 68h:41m:09s remains)
INFO - root - 2017-12-09 17:40:36.757572: step 44100, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.834 sec/batch; 66h:49m:05s remains)
2017-12-09 17:40:37.634876: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0018349529 -0.0018354218 -0.0018357605 -0.0018353639 -0.0018338757 -0.0018318173 -0.0018300837 -0.0018295438 -0.0018301879 -0.0018320202 -0.0018343225 -0.0018362894 -0.0018375675 -0.0018379533 -0.001837473][-0.0018367313 -0.0018375915 -0.0018382337 -0.0018378963 -0.0018362397 -0.001833844 -0.0018317678 -0.0018309653 -0.0018311166 -0.0018324249 -0.0018341906 -0.0018355692 -0.0018362622 -0.001836086 -0.0018351411][-0.0018375826 -0.001838697 -0.0018396561 -0.0018395232 -0.0018380553 -0.0018358171 -0.0018337137 -0.0018327899 -0.0018325759 -0.0018333069 -0.0018344191 -0.0018352235 -0.0018351369 -0.0018341539 -0.0018326716][-0.0018379167 -0.0018389649 -0.0018398809 -0.0018398347 -0.0018387277 -0.0018369278 -0.0018350614 -0.0018341806 -0.0018337816 -0.0018340177 -0.0018345071 -0.001834669 -0.0018339104 -0.0018322625 -0.0018305557][-0.0018382547 -0.0018388984 -0.0018393223 -0.0018390295 -0.0018380577 -0.0018366254 -0.0018350105 -0.0018341416 -0.0018338178 -0.001833987 -0.0018341393 -0.0018339956 -0.0018329578 -0.0018310277 -0.0018290863][-0.0018386551 -0.0018388077 -0.0018386322 -0.0018378753 -0.001836654 -0.0018353399 -0.0018338498 -0.0018327937 -0.0018325052 -0.0018329441 -0.0018330693 -0.0018329635 -0.0018321152 -0.0018303858 -0.0018287266][-0.001839179 -0.0018387507 -0.0018379996 -0.0018367619 -0.0018352161 -0.0018336666 -0.0018321149 -0.0018309569 -0.0018306061 -0.001831249 -0.0018317658 -0.001832171 -0.0018319675 -0.0018310227 -0.0018300567][-0.0018399559 -0.0018389681 -0.001837588 -0.0018358394 -0.0018338183 -0.0018318687 -0.0018302717 -0.0018290251 -0.0018284809 -0.0018290011 -0.0018297387 -0.0018305534 -0.0018310201 -0.0018309404 -0.0018306964][-0.001840143 -0.0018388754 -0.001837056 -0.0018348197 -0.0018324621 -0.0018301953 -0.0018284658 -0.0018272758 -0.0018265877 -0.0018267758 -0.0018274568 -0.0018284774 -0.001829451 -0.001830031 -0.0018302734][-0.0018394607 -0.0018381523 -0.0018362381 -0.0018339143 -0.0018315804 -0.0018292186 -0.0018273863 -0.0018262075 -0.001825504 -0.0018254149 -0.0018258361 -0.0018268005 -0.0018280022 -0.0018290085 -0.0018296364][-0.0018377351 -0.0018367524 -0.0018352424 -0.0018331095 -0.0018309428 -0.0018287109 -0.0018269613 -0.0018257888 -0.0018250857 -0.0018248517 -0.001825055 -0.0018259739 -0.0018271767 -0.0018282877 -0.0018290472][-0.0018353696 -0.0018347262 -0.0018335248 -0.0018317105 -0.0018298336 -0.0018279008 -0.0018264788 -0.0018255229 -0.0018249098 -0.0018246768 -0.0018247812 -0.0018254486 -0.001826438 -0.001827414 -0.0018281261][-0.0018327398 -0.0018322958 -0.0018313596 -0.0018298987 -0.0018284483 -0.0018269657 -0.001825919 -0.0018252115 -0.0018247482 -0.0018245756 -0.0018246307 -0.0018250113 -0.0018255648 -0.0018261734 -0.0018266393][-0.0018301366 -0.0018298379 -0.0018291931 -0.0018281842 -0.0018271642 -0.0018261671 -0.0018254607 -0.0018250273 -0.0018247884 -0.0018247233 -0.0018247248 -0.00182489 -0.0018251485 -0.0018254388 -0.0018256624][-0.0018281311 -0.0018279116 -0.0018275189 -0.0018269194 -0.0018262709 -0.0018256538 -0.0018251997 -0.0018249752 -0.0018249062 -0.0018249181 -0.0018249569 -0.0018250479 -0.0018251704 -0.0018252733 -0.0018253354]]...]
INFO - root - 2017-12-09 17:40:46.287483: step 44110, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.895 sec/batch; 71h:40m:19s remains)
INFO - root - 2017-12-09 17:40:55.025236: step 44120, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.839 sec/batch; 67h:13m:30s remains)
INFO - root - 2017-12-09 17:41:03.727229: step 44130, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.896 sec/batch; 71h:45m:49s remains)
INFO - root - 2017-12-09 17:41:12.542853: step 44140, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.850 sec/batch; 68h:06m:40s remains)
INFO - root - 2017-12-09 17:41:21.218649: step 44150, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.887 sec/batch; 71h:02m:17s remains)
INFO - root - 2017-12-09 17:41:29.713597: step 44160, loss = 0.82, batch loss = 0.69 (11.1 examples/sec; 0.721 sec/batch; 57h:45m:52s remains)
INFO - root - 2017-12-09 17:41:38.228000: step 44170, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.873 sec/batch; 69h:56m:53s remains)
INFO - root - 2017-12-09 17:41:46.861958: step 44180, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.866 sec/batch; 69h:22m:06s remains)
INFO - root - 2017-12-09 17:41:55.443719: step 44190, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 69h:46m:37s remains)
INFO - root - 2017-12-09 17:42:04.184458: step 44200, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.892 sec/batch; 71h:24m:06s remains)
2017-12-09 17:42:05.008414: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0018139891 -0.0018130494 -0.0018132923 -0.001813866 -0.0018145132 -0.0018152321 -0.0018159518 -0.00181653 -0.0018168034 -0.0018167973 -0.0018165254 -0.0018159453 -0.0018151545 -0.0018143432 -0.0018136386][-0.0018136292 -0.001813035 -0.0018136876 -0.0018147755 -0.0018160592 -0.00181736 -0.0018185523 -0.0018193908 -0.0018197073 -0.0018194909 -0.0018188234 -0.0018177667 -0.0018164546 -0.0018151106 -0.0018139755][-0.001813535 -0.0018133575 -0.0018144982 -0.001816267 -0.0018183226 -0.0018202801 -0.0018219339 -0.0018230632 -0.0018233685 -0.0018228089 -0.0018216326 -0.0018200021 -0.0018181828 -0.0018163281 -0.0018147042][-0.0018136796 -0.0018139892 -0.0018156843 -0.0018181811 -0.0018209972 -0.0018236303 -0.0018258159 -0.0018272173 -0.0018273955 -0.0018263117 -0.0018243933 -0.001822027 -0.0018196394 -0.0018173205 -0.001815299][-0.0018145145 -0.0018151937 -0.0018173689 -0.0018204828 -0.0018239262 -0.0018271053 -0.001829643 -0.0018312145 -0.0018311203 -0.001829405 -0.0018266598 -0.0018234694 -0.0018205058 -0.001817768 -0.0018154865][-0.001816523 -0.0018175161 -0.0018199139 -0.0018231978 -0.0018267601 -0.0018298947 -0.0018321957 -0.0018335237 -0.0018330244 -0.0018307586 -0.0018273751 -0.0018236157 -0.0018202318 -0.0018173049 -0.0018150704][-0.0018200362 -0.0018212313 -0.0018234821 -0.0018263326 -0.001829265 -0.0018316194 -0.0018330714 -0.0018336688 -0.0018327147 -0.001830145 -0.0018265443 -0.0018226909 -0.0018192541 -0.0018164045 -0.001814399][-0.0018244759 -0.0018256059 -0.0018273969 -0.0018293933 -0.0018311887 -0.0018323566 -0.0018326927 -0.0018323805 -0.0018309554 -0.0018282582 -0.0018247706 -0.0018212184 -0.0018180357 -0.0018154093 -0.0018137291][-0.0018282823 -0.001829227 -0.0018302897 -0.0018311745 -0.0018317236 -0.0018316724 -0.0018309921 -0.0018299337 -0.0018280377 -0.0018253265 -0.0018221874 -0.0018191884 -0.0018164637 -0.0018142487 -0.0018129689][-0.0018313688 -0.0018318251 -0.0018318138 -0.0018313878 -0.0018306067 -0.0018294008 -0.0018279806 -0.0018264611 -0.0018244542 -0.0018219878 -0.0018193456 -0.0018169745 -0.0018148998 -0.0018132717 -0.0018124128][-0.0018329452 -0.0018329369 -0.0018320223 -0.0018303912 -0.0018284897 -0.0018264891 -0.0018245147 -0.0018227822 -0.0018209652 -0.0018189299 -0.0018169048 -0.0018152123 -0.0018138043 -0.0018127756 -0.0018122848][-0.0018320848 -0.0018316378 -0.0018303063 -0.0018281871 -0.0018257916 -0.0018234353 -0.0018212886 -0.0018195832 -0.0018180982 -0.0018165616 -0.001815147 -0.0018140745 -0.0018132521 -0.0018126515 -0.0018123377][-0.0018286328 -0.001827924 -0.0018266346 -0.0018247785 -0.0018225757 -0.0018203714 -0.001818417 -0.0018169592 -0.0018158538 -0.0018148343 -0.0018139338 -0.0018132756 -0.0018128133 -0.0018125 -0.0018123018][-0.0018245124 -0.0018236073 -0.0018225382 -0.0018211664 -0.0018194495 -0.0018176225 -0.0018160561 -0.0018149529 -0.0018142371 -0.0018136565 -0.0018131317 -0.0018127242 -0.0018124566 -0.0018123018 -0.0018121873][-0.0018208807 -0.0018198424 -0.0018190027 -0.001818089 -0.0018168809 -0.0018156131 -0.001814526 -0.001813792 -0.0018133706 -0.0018130603 -0.0018127502 -0.0018124813 -0.0018123062 -0.0018122045 -0.0018121249]]...]
INFO - root - 2017-12-09 17:42:13.699508: step 44210, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.853 sec/batch; 68h:18m:29s remains)
INFO - root - 2017-12-09 17:42:22.352865: step 44220, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.897 sec/batch; 71h:47m:50s remains)
INFO - root - 2017-12-09 17:42:31.047400: step 44230, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.847 sec/batch; 67h:51m:31s remains)
INFO - root - 2017-12-09 17:42:39.745754: step 44240, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.870 sec/batch; 69h:38m:52s remains)
INFO - root - 2017-12-09 17:42:48.487193: step 44250, loss = 0.83, batch loss = 0.70 (9.6 examples/sec; 0.831 sec/batch; 66h:31m:44s remains)
INFO - root - 2017-12-09 17:42:56.962896: step 44260, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.830 sec/batch; 66h:25m:17s remains)
INFO - root - 2017-12-09 17:43:05.853288: step 44270, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.875 sec/batch; 70h:01m:53s remains)
INFO - root - 2017-12-09 17:43:14.317688: step 44280, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.857 sec/batch; 68h:35m:16s remains)
INFO - root - 2017-12-09 17:43:22.860196: step 44290, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.877 sec/batch; 70h:12m:05s remains)
INFO - root - 2017-12-09 17:43:31.531334: step 44300, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.852 sec/batch; 68h:10m:55s remains)
2017-12-09 17:43:32.504255: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0018142127 -0.0018141362 -0.0018150688 -0.0018158313 -0.0018161217 -0.0018158422 -0.0018150881 -0.0018140991 -0.0018130722 -0.0018122218 -0.0018115701 -0.0018112264 -0.0018111011 -0.0018110661 -0.0018110259][-0.0018158999 -0.0018166164 -0.0018182938 -0.0018196033 -0.0018200156 -0.0018194485 -0.0018181299 -0.0018164178 -0.0018146058 -0.001813024 -0.0018118238 -0.0018111852 -0.0018109587 -0.0018109092 -0.001810894][-0.0018185474 -0.001820099 -0.0018225214 -0.001824391 -0.0018250098 -0.0018241741 -0.0018221933 -0.0018197173 -0.0018170476 -0.0018146374 -0.0018127743 -0.0018117911 -0.0018114916 -0.0018114643 -0.0018114604][-0.0018209666 -0.0018233722 -0.0018264683 -0.0018288303 -0.0018296181 -0.0018285002 -0.0018259309 -0.0018227104 -0.0018192258 -0.0018160329 -0.0018135338 -0.0018122643 -0.0018119505 -0.0018120134 -0.0018120877][-0.0018232836 -0.001826425 -0.0018300749 -0.0018327383 -0.0018335405 -0.0018320864 -0.0018289568 -0.001825092 -0.0018209551 -0.0018172045 -0.001814286 -0.0018128832 -0.0018126116 -0.0018127774 -0.0018128906][-0.0018246715 -0.0018281747 -0.0018319767 -0.0018345987 -0.0018352484 -0.0018336423 -0.0018303518 -0.0018261983 -0.001821829 -0.0018178858 -0.0018148925 -0.0018135435 -0.0018133547 -0.001813596 -0.0018136716][-0.001824587 -0.0018280452 -0.0018316446 -0.0018339463 -0.0018344099 -0.0018328243 -0.0018297363 -0.0018257195 -0.0018215323 -0.0018178227 -0.0018150281 -0.0018138045 -0.0018136761 -0.0018139733 -0.0018140159][-0.0018228993 -0.0018258479 -0.0018289256 -0.0018308524 -0.0018311685 -0.0018297849 -0.0018271586 -0.0018236686 -0.0018201293 -0.001817059 -0.0018148 -0.0018138128 -0.0018137717 -0.0018140549 -0.0018140116][-0.0018205799 -0.0018229543 -0.0018255479 -0.0018271507 -0.0018274375 -0.0018263459 -0.0018242777 -0.001821506 -0.0018187945 -0.0018165318 -0.0018149432 -0.0018142182 -0.0018141634 -0.0018142593 -0.0018139863][-0.0018185835 -0.0018203587 -0.0018224377 -0.0018237366 -0.001824035 -0.0018233166 -0.0018218966 -0.0018198976 -0.0018179881 -0.001816469 -0.0018154451 -0.0018148993 -0.0018147385 -0.0018145674 -0.0018140196][-0.0018170136 -0.0018181445 -0.0018196723 -0.0018207047 -0.0018210175 -0.0018206551 -0.0018197665 -0.0018184495 -0.0018172192 -0.0018163037 -0.0018157408 -0.0018153312 -0.0018150753 -0.001814701 -0.0018139289][-0.0018161695 -0.0018165531 -0.0018174893 -0.0018181744 -0.0018183974 -0.0018182112 -0.0018176958 -0.0018169213 -0.0018162399 -0.0018158091 -0.00181557 -0.0018152896 -0.0018150203 -0.0018145359 -0.0018136642][-0.0018156895 -0.0018154851 -0.0018158171 -0.0018161444 -0.0018162419 -0.001816124 -0.0018158352 -0.001815501 -0.0018152801 -0.0018152249 -0.0018152647 -0.0018151306 -0.0018148536 -0.001814325 -0.0018134394][-0.0018154434 -0.0018149369 -0.0018149539 -0.0018151108 -0.0018151581 -0.001815088 -0.0018149496 -0.0018148361 -0.0018148217 -0.0018149283 -0.0018150668 -0.0018149585 -0.0018146248 -0.0018140798 -0.0018132243][-0.0018155717 -0.0018148914 -0.0018147612 -0.0018148534 -0.0018148775 -0.0018148315 -0.0018147453 -0.0018146685 -0.0018146748 -0.0018147747 -0.0018148669 -0.0018147082 -0.0018143242 -0.0018138139 -0.0018130081]]...]
INFO - root - 2017-12-09 17:43:41.130756: step 44310, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.880 sec/batch; 70h:26m:18s remains)
INFO - root - 2017-12-09 17:43:49.854639: step 44320, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.891 sec/batch; 71h:21m:04s remains)
INFO - root - 2017-12-09 17:43:58.543319: step 44330, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 69h:35m:13s remains)
INFO - root - 2017-12-09 17:44:07.222973: step 44340, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 69h:44m:32s remains)
INFO - root - 2017-12-09 17:44:15.919737: step 44350, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.857 sec/batch; 68h:36m:19s remains)
INFO - root - 2017-12-09 17:44:24.568145: step 44360, loss = 0.82, batch loss = 0.69 (8.8 examples/sec; 0.905 sec/batch; 72h:25m:46s remains)
INFO - root - 2017-12-09 17:44:33.250226: step 44370, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.834 sec/batch; 66h:45m:57s remains)
INFO - root - 2017-12-09 17:44:41.874627: step 44380, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.885 sec/batch; 70h:48m:09s remains)
INFO - root - 2017-12-09 17:44:50.419941: step 44390, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.844 sec/batch; 67h:34m:43s remains)
INFO - root - 2017-12-09 17:44:59.189185: step 44400, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.853 sec/batch; 68h:13m:53s remains)
2017-12-09 17:45:00.088752: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0018155923 -0.0018144683 -0.0018145441 -0.001815037 -0.0018155795 -0.0018159912 -0.0018162193 -0.0018161535 -0.001815816 -0.001815243 -0.001814727 -0.0018146221 -0.001814935 -0.00181553 -0.0018163342][-0.0018167874 -0.0018152132 -0.0018147358 -0.0018145825 -0.0018145081 -0.0018144517 -0.0018144294 -0.0018143258 -0.001814116 -0.0018138267 -0.0018136305 -0.0018137994 -0.001814238 -0.001814838 -0.0018156308][-0.001819578 -0.0018173993 -0.0018161536 -0.0018153116 -0.0018147069 -0.0018143846 -0.0018143518 -0.0018143979 -0.00181433 -0.0018141913 -0.0018141168 -0.0018142707 -0.0018145728 -0.0018149858 -0.0018156043][-0.0018229769 -0.001820075 -0.0018180602 -0.0018165481 -0.001815582 -0.0018152372 -0.0018154042 -0.0018157471 -0.001815938 -0.0018159308 -0.0018158247 -0.0018157441 -0.0018156802 -0.0018157085 -0.0018159868][-0.0018267161 -0.0018233329 -0.0018206686 -0.0018186256 -0.0018174801 -0.0018172738 -0.0018177676 -0.0018184616 -0.0018189041 -0.0018189714 -0.0018186779 -0.0018181568 -0.0018175063 -0.0018169365 -0.0018167042][-0.0018306079 -0.0018270786 -0.0018241835 -0.0018220019 -0.0018209992 -0.0018211143 -0.0018219718 -0.001822917 -0.0018233659 -0.0018232538 -0.001822529 -0.0018213631 -0.0018199598 -0.0018186485 -0.0018177741][-0.0018344342 -0.0018311309 -0.0018284424 -0.0018265658 -0.0018259988 -0.0018265466 -0.0018276935 -0.0018286958 -0.0018289528 -0.0018284223 -0.0018270877 -0.0018251715 -0.0018230001 -0.0018209374 -0.0018192776][-0.0018377231 -0.0018349367 -0.0018328039 -0.0018314865 -0.0018314066 -0.0018322523 -0.0018334816 -0.0018343703 -0.0018343792 -0.0018334575 -0.0018315566 -0.0018289908 -0.0018262247 -0.0018235357 -0.0018211165][-0.0018401782 -0.0018381159 -0.0018366582 -0.0018358884 -0.0018361183 -0.001837031 -0.0018380729 -0.0018387059 -0.0018385061 -0.0018373767 -0.0018351952 -0.0018322982 -0.0018291408 -0.0018260085 -0.001822988][-0.0018418785 -0.0018405244 -0.0018396841 -0.00183934 -0.0018396648 -0.0018404466 -0.0018412018 -0.0018415373 -0.0018412009 -0.001840006 -0.0018378473 -0.001834832 -0.0018314623 -0.001827987 -0.0018244748][-0.0018422565 -0.0018412789 -0.0018408196 -0.00184068 -0.0018409279 -0.0018413893 -0.0018417909 -0.0018418815 -0.0018415698 -0.0018405414 -0.0018386889 -0.0018359205 -0.0018326658 -0.0018291258 -0.0018253356][-0.0018410684 -0.0018399146 -0.0018393318 -0.0018391077 -0.0018391655 -0.0018393957 -0.0018395853 -0.0018395864 -0.001839485 -0.0018388425 -0.0018374615 -0.0018352055 -0.0018323991 -0.0018291326 -0.0018254382][-0.0018385117 -0.0018370108 -0.0018360736 -0.0018355465 -0.0018353559 -0.0018353865 -0.0018354886 -0.0018355119 -0.0018355935 -0.0018353809 -0.0018345044 -0.0018328215 -0.0018305328 -0.0018277786 -0.0018246186][-0.0018351109 -0.001833424 -0.0018322474 -0.0018315129 -0.0018310724 -0.0018308939 -0.0018309009 -0.0018309576 -0.0018311312 -0.001831105 -0.0018305868 -0.0018293834 -0.0018276419 -0.0018255024 -0.0018230387][-0.0018310074 -0.0018292218 -0.001827908 -0.0018270798 -0.0018265664 -0.0018263306 -0.0018263792 -0.001826541 -0.0018268315 -0.0018269367 -0.0018266211 -0.0018258013 -0.001824597 -0.001823029 -0.0018212292]]...]
INFO - root - 2017-12-09 17:45:08.691801: step 44410, loss = 0.82, batch loss = 0.69 (9.7 examples/sec; 0.829 sec/batch; 66h:18m:58s remains)
INFO - root - 2017-12-09 17:45:17.321107: step 44420, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.834 sec/batch; 66h:44m:34s remains)
INFO - root - 2017-12-09 17:45:26.061368: step 44430, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.885 sec/batch; 70h:50m:08s remains)
INFO - root - 2017-12-09 17:45:34.766713: step 44440, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.861 sec/batch; 68h:53m:20s remains)
INFO - root - 2017-12-09 17:45:43.427795: step 44450, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.854 sec/batch; 68h:22m:10s remains)
INFO - root - 2017-12-09 17:45:52.036344: step 44460, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.874 sec/batch; 69h:54m:50s remains)
INFO - root - 2017-12-09 17:46:00.805086: step 44470, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.841 sec/batch; 67h:19m:17s remains)
INFO - root - 2017-12-09 17:46:09.316188: step 44480, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 69h:31m:16s remains)
INFO - root - 2017-12-09 17:46:17.959798: step 44490, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 69h:28m:43s remains)
INFO - root - 2017-12-09 17:46:26.572132: step 44500, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.853 sec/batch; 68h:13m:31s remains)
2017-12-09 17:46:27.483650: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0018091942 -0.0017623074 -0.0016226408 -0.0013453036 -0.00094367226 -0.00049818726 -0.00013692782 -1.5078112e-06 -0.0001586451 -0.00055810809 -0.0010587128 -0.0014847559 -0.0017287172 -0.0018031708 -0.0018101047][-0.0017922693 -0.001667642 -0.0013310128 -0.00070366496 0.00015785254 0.0010522065 0.0017013649 0.0018628078 0.0014700546 0.00065099157 -0.00031853304 -0.0011325425 -0.0016147641 -0.0017859093 -0.001811134][-0.0017482579 -0.0014662617 -0.00074739254 0.00054395723 0.0022672093 0.0040035159 0.0052105431 0.0054713148 0.0046924786 0.00311493 0.0012526998 -0.00033860479 -0.0013246182 -0.00172666 -0.0018093978][-0.0016719389 -0.0011408329 0.00016794901 0.0024640337 0.0054925722 0.0085413922 0.010703459 0.01128533 0.010072491 0.0073994044 0.0041072587 0.001173834 -0.00073837908 -0.0015893565 -0.0018021583][-0.0015647457 -0.00073720445 0.0012722175 0.0047624353 0.0093590412 0.01402982 0.017463818 0.018628037 0.017102187 0.013238661 0.0081845382 0.0034512263 0.00019577786 -0.0013495206 -0.0017861576][-0.0014152839 -0.00033898803 0.0022633257 0.0067938603 0.012813364 0.019052062 0.023861954 0.025867488 0.024354894 0.019555474 0.01279674 0.006139311 0.0013486833 -0.0010308721 -0.0017603666][-0.001256839 -7.7101868e-05 0.0027755871 0.0077917092 0.014559141 0.021749165 0.027569873 0.030434236 0.029342186 0.024281856 0.016519386 0.0084625157 0.0024040625 -0.0007206389 -0.0017314656][-0.0011380641 -3.8313563e-05 0.0026197429 0.0073746396 0.013938204 0.021137862 0.027275901 0.030731872 0.030290443 0.025669331 0.017922802 0.0095003378 0.0029411144 -0.00054066966 -0.0017089655][-0.0011213862 -0.00024191965 0.0018681226 0.0057298187 0.011229715 0.017510956 0.023165211 0.026723284 0.026912343 0.023251439 0.016516225 0.0088791726 0.0027580359 -0.00056537648 -0.0017044517][-0.0012264368 -0.00062044524 0.00080602837 0.0034838342 0.0074584745 0.01222922 0.016779544 0.019915778 0.020488007 0.017954877 0.0128355 0.0068266038 0.0019153404 -0.00078951381 -0.001720782][-0.0014285562 -0.0010640665 -0.00023957982 0.0013466695 0.0038230354 0.0069716321 0.010162574 0.012525218 0.013162061 0.011604353 0.008189775 0.00410207 0.00073919527 -0.0011172229 -0.0017488196][-0.0016547602 -0.001472512 -0.0010661298 -0.00026189873 0.0010589442 0.0028332849 0.0047307587 0.0062052011 0.0066730324 0.0058142729 0.0038472181 0.0014980311 -0.0004066471 -0.0014402386 -0.0017778274][-0.0017684256 -0.0017057441 -0.001548123 -0.0012136031 -0.00063089747 0.00019674154 0.0011226301 0.0018631931 0.0021089972 0.0016933259 0.0007489546 -0.00034961547 -0.0012079112 -0.0016572548 -0.0017937963][-0.0017987571 -0.0017874134 -0.0017491996 -0.0016517508 -0.0014614864 -0.0011714865 -0.00083224196 -0.00055581122 -0.00046626478 -0.00062465796 -0.00096826314 -0.0013483375 -0.0016265882 -0.0017630454 -0.0017991997][-0.0017989703 -0.0017977175 -0.001793238 -0.0017773797 -0.0017404649 -0.0016781688 -0.001599869 -0.0015325977 -0.0015101334 -0.0015490585 -0.0016310909 -0.001715359 -0.0017703755 -0.0017938718 -0.0017990179]]...]
INFO - root - 2017-12-09 17:46:36.300122: step 44510, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.857 sec/batch; 68h:34m:01s remains)
INFO - root - 2017-12-09 17:46:44.903149: step 44520, loss = 0.83, batch loss = 0.70 (9.2 examples/sec; 0.874 sec/batch; 69h:53m:48s remains)
INFO - root - 2017-12-09 17:46:53.559855: step 44530, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 69h:05m:33s remains)
INFO - root - 2017-12-09 17:47:02.385726: step 44540, loss = 0.82, batch loss = 0.69 (9.7 examples/sec; 0.821 sec/batch; 65h:38m:32s remains)
INFO - root - 2017-12-09 17:47:10.977580: step 44550, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.888 sec/batch; 71h:02m:06s remains)
INFO - root - 2017-12-09 17:47:19.441783: step 44560, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.838 sec/batch; 67h:02m:07s remains)
INFO - root - 2017-12-09 17:47:28.196723: step 44570, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.861 sec/batch; 68h:50m:10s remains)
INFO - root - 2017-12-09 17:47:36.707991: step 44580, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.893 sec/batch; 71h:26m:32s remains)
INFO - root - 2017-12-09 17:47:45.221184: step 44590, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.831 sec/batch; 66h:28m:47s remains)
INFO - root - 2017-12-09 17:47:53.877567: step 44600, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 69h:30m:57s remains)
2017-12-09 17:47:54.812659: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0018348116 -0.0018341688 -0.0018332067 -0.0018319798 -0.0018305946 -0.0018293326 -0.0018282867 -0.0018282974 -0.0018299721 -0.0018323412 -0.0018342399 -0.0018347929 -0.0018346378 -0.001834414 -0.0018338346][-0.0018348158 -0.0018317522 -0.0018274961 -0.0018227702 -0.0018186026 -0.001816421 -0.0018157068 -0.0018169193 -0.001820777 -0.0018260919 -0.0018307209 -0.0018327519 -0.0018337329 -0.0018346077 -0.0018344697][-0.0018317635 -0.0018218721 -0.0018083622 -0.0017945566 -0.0017849044 -0.0017824557 -0.0017852073 -0.0017911216 -0.0018012126 -0.0018129908 -0.0018228211 -0.0018277179 -0.0018306555 -0.0018336955 -0.0018350799][-0.0018163226 -0.0017875906 -0.0017505607 -0.0017164163 -0.0016979917 -0.001700633 -0.0017179812 -0.0017408888 -0.0017661866 -0.0017900034 -0.0018070763 -0.0018164332 -0.0018227337 -0.0018293024 -0.0018338851][-0.0017701008 -0.0016938741 -0.0016008217 -0.0015218431 -0.0014862973 -0.0015011723 -0.0015507746 -0.0016149075 -0.0016803782 -0.0017377358 -0.0017751139 -0.0017957827 -0.0018076486 -0.001819376 -0.0018288869][-0.0016758634 -0.0015102252 -0.0013147469 -0.001156523 -0.0010901634 -0.0011248277 -0.001226749 -0.0013587831 -0.0014921171 -0.0016121138 -0.0016925171 -0.0017417711 -0.0017686334 -0.0017932127 -0.0018141731][-0.0015365775 -0.0012472651 -0.00091411464 -0.00064942846 -0.00053955661 -0.00059129752 -0.00074597227 -0.00095257896 -0.0011727239 -0.0013849656 -0.0015375054 -0.0016355015 -0.0016881713 -0.0017359949 -0.0017798959][-0.0014008648 -0.0010078561 -0.00056594622 -0.00021946221 -7.588719e-05 -0.00013020262 -0.00030468591 -0.00054437225 -0.00081927457 -0.0011071051 -0.0013304891 -0.0014808063 -0.00156765 -0.0016489089 -0.001727627][-0.0013258411 -0.000896984 -0.00042468158 -6.018288e-05 8.8147586e-05 3.9922772e-05 -0.0001221922 -0.00034829706 -0.00062532676 -0.00093365018 -0.001187293 -0.0013612257 -0.0014665076 -0.0015691287 -0.001676677][-0.0013503691 -0.00096768042 -0.00055118848 -0.00023414788 -0.00011184684 -0.0001559892 -0.00029223191 -0.0004758396 -0.00070750678 -0.00097408792 -0.001203143 -0.0013618219 -0.0014616101 -0.0015595958 -0.0016685424][-0.0014598339 -0.001172014 -0.00085644634 -0.00061695511 -0.00053032127 -0.00057629857 -0.00069618691 -0.0008437857 -0.0010189661 -0.001211526 -0.0013770817 -0.0014879568 -0.0015565065 -0.001624016 -0.0017051592][-0.0016102751 -0.0014322413 -0.0012328611 -0.0010798879 -0.0010285744 -0.0010710531 -0.0011675929 -0.001274919 -0.0013900613 -0.0015041353 -0.001598575 -0.0016583975 -0.0016930355 -0.0017257256 -0.0017670668][-0.001735423 -0.0016484761 -0.0015479184 -0.0014692122 -0.0014448455 -0.0014759627 -0.0015397219 -0.0016043831 -0.0016653035 -0.0017162468 -0.0017545562 -0.0017760667 -0.0017876009 -0.0017978171 -0.0018110253][-0.0018066244 -0.0017769893 -0.0017415518 -0.0017131246 -0.0017051219 -0.0017198457 -0.0017480049 -0.0017744191 -0.0017961611 -0.0018109803 -0.001820927 -0.0018254776 -0.0018273462 -0.0018284033 -0.001829486][-0.0018291267 -0.0018228895 -0.0018151911 -0.0018085483 -0.0018064841 -0.0018101013 -0.0018177105 -0.0018246155 -0.0018293665 -0.0018314593 -0.0018320719 -0.0018320521 -0.0018318547 -0.0018315496 -0.0018311688]]...]
INFO - root - 2017-12-09 17:48:03.356674: step 44610, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 69h:18m:13s remains)
INFO - root - 2017-12-09 17:48:11.928387: step 44620, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.893 sec/batch; 71h:22m:50s remains)
INFO - root - 2017-12-09 17:48:20.661768: step 44630, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.858 sec/batch; 68h:34m:42s remains)
INFO - root - 2017-12-09 17:48:29.359365: step 44640, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.873 sec/batch; 69h:48m:01s remains)
INFO - root - 2017-12-09 17:48:38.303291: step 44650, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.891 sec/batch; 71h:13m:16s remains)
INFO - root - 2017-12-09 17:48:46.930297: step 44660, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.878 sec/batch; 70h:11m:22s remains)
INFO - root - 2017-12-09 17:48:55.804207: step 44670, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.883 sec/batch; 70h:35m:59s remains)
INFO - root - 2017-12-09 17:49:04.111635: step 44680, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.840 sec/batch; 67h:08m:56s remains)
INFO - root - 2017-12-09 17:49:12.560704: step 44690, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.853 sec/batch; 68h:11m:30s remains)
INFO - root - 2017-12-09 17:49:21.157129: step 44700, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.831 sec/batch; 66h:24m:01s remains)
2017-12-09 17:49:22.034678: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.001817571 -0.0018170463 -0.0018170805 -0.0018172032 -0.0018173086 -0.0018173982 -0.0018174244 -0.0018173484 -0.0018172457 -0.001817135 -0.0018169641 -0.0018167201 -0.0018164829 -0.0018162637 -0.0018160443][-0.0018176013 -0.0018172127 -0.00181731 -0.0018174512 -0.0018175719 -0.0018176299 -0.001817558 -0.0018173514 -0.0018171732 -0.0018170417 -0.0018168293 -0.0018165583 -0.0018163157 -0.0018160862 -0.0018158295][-0.0018180509 -0.0018178545 -0.0018180257 -0.0018181982 -0.0018183014 -0.0018182796 -0.001818044 -0.0018176321 -0.0018172876 -0.001817047 -0.0018168201 -0.0018165588 -0.0018163472 -0.001816128 -0.0018158766][-0.0018183148 -0.0018183743 -0.001818652 -0.0018188952 -0.0018190445 -0.0018190327 -0.0018187254 -0.0018181254 -0.001817562 -0.0018171394 -0.0018167738 -0.0018163974 -0.0018161114 -0.0018158797 -0.0018157233][-0.0018179972 -0.0018183183 -0.0018187555 -0.0018190999 -0.0018193217 -0.0018193991 -0.0018191589 -0.001818507 -0.001817809 -0.0018171988 -0.0018165922 -0.001815947 -0.0018154768 -0.0018152705 -0.0018152953][-0.0018172145 -0.0018177554 -0.0018182914 -0.0018185918 -0.0018187847 -0.0018189261 -0.0018187795 -0.0018182634 -0.0018176364 -0.0018169141 -0.0018160179 -0.0018150606 -0.001814422 -0.0018142707 -0.0018145435][-0.0018163866 -0.0018169223 -0.0018174585 -0.0018177064 -0.0018178636 -0.0018180304 -0.0018179348 -0.0018175251 -0.0018169401 -0.0018160947 -0.0018149201 -0.0018137245 -0.0018129806 -0.0018129296 -0.0018134217][-0.0018154047 -0.0018157082 -0.0018162135 -0.0018166007 -0.0018169614 -0.0018172973 -0.0018172817 -0.0018169131 -0.0018162352 -0.0018151418 -0.0018136811 -0.0018122954 -0.0018115101 -0.001811531 -0.0018122122][-0.0018141433 -0.0018140498 -0.0018145487 -0.0018152524 -0.0018160677 -0.0018167408 -0.0018169006 -0.0018166408 -0.0018158971 -0.0018145817 -0.0018129231 -0.0018114627 -0.0018106273 -0.0018105997 -0.0018113327][-0.0018130287 -0.0018125757 -0.0018129856 -0.001813861 -0.0018149739 -0.0018159212 -0.0018163428 -0.0018163492 -0.0018156568 -0.0018142783 -0.0018126373 -0.0018111925 -0.0018102428 -0.0018101081 -0.0018108451][-0.0018123521 -0.0018115694 -0.0018118646 -0.0018127265 -0.001813931 -0.0018150094 -0.0018156634 -0.0018158897 -0.001815299 -0.0018139924 -0.0018124202 -0.0018110651 -0.0018101361 -0.0018100218 -0.0018107656][-0.0018119278 -0.0018108434 -0.0018110952 -0.0018119307 -0.0018131206 -0.0018142329 -0.0018150329 -0.0018153552 -0.0018149026 -0.0018138299 -0.0018124974 -0.0018112934 -0.0018104878 -0.0018104527 -0.0018111272][-0.0018116733 -0.0018105075 -0.0018107033 -0.0018115379 -0.0018127003 -0.001813796 -0.0018145986 -0.0018149542 -0.0018146996 -0.0018139133 -0.0018128682 -0.0018118919 -0.0018112423 -0.0018112618 -0.0018118225][-0.0018124821 -0.0018113016 -0.0018113462 -0.0018119692 -0.0018129123 -0.001813853 -0.0018145745 -0.001814918 -0.0018147961 -0.0018142576 -0.0018134966 -0.0018127742 -0.001812276 -0.0018122799 -0.0018127038][-0.0018138539 -0.0018127534 -0.0018126337 -0.0018129988 -0.0018136319 -0.0018143074 -0.0018148554 -0.0018151151 -0.001815074 -0.0018147369 -0.0018142319 -0.0018137643 -0.0018134598 -0.0018134401 -0.0018136951]]...]
INFO - root - 2017-12-09 17:49:30.584667: step 44710, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.844 sec/batch; 67h:26m:44s remains)
INFO - root - 2017-12-09 17:49:39.195494: step 44720, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.851 sec/batch; 68h:03m:56s remains)
INFO - root - 2017-12-09 17:49:47.903830: step 44730, loss = 0.83, batch loss = 0.70 (9.2 examples/sec; 0.871 sec/batch; 69h:37m:02s remains)
INFO - root - 2017-12-09 17:49:56.407436: step 44740, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.878 sec/batch; 70h:10m:56s remains)
INFO - root - 2017-12-09 17:50:05.133791: step 44750, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.855 sec/batch; 68h:18m:49s remains)
INFO - root - 2017-12-09 17:50:13.684206: step 44760, loss = 0.83, batch loss = 0.70 (9.6 examples/sec; 0.832 sec/batch; 66h:30m:00s remains)
INFO - root - 2017-12-09 17:50:22.403031: step 44770, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.890 sec/batch; 71h:05m:59s remains)
INFO - root - 2017-12-09 17:50:30.935689: step 44780, loss = 0.81, batch loss = 0.68 (9.1 examples/sec; 0.883 sec/batch; 70h:36m:04s remains)
INFO - root - 2017-12-09 17:50:39.509949: step 44790, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.858 sec/batch; 68h:32m:41s remains)
INFO - root - 2017-12-09 17:50:48.214245: step 44800, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.874 sec/batch; 69h:49m:02s remains)
2017-12-09 17:50:49.026110: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0018316801 -0.0018317478 -0.0018323851 -0.0018331169 -0.0018338477 -0.0018339467 -0.0018330722 -0.0018313501 -0.0018289865 -0.0018267243 -0.0018249989 -0.0018239591 -0.0018239381 -0.0018242603 -0.0018249622][-0.0018327771 -0.001833251 -0.0018338125 -0.0018342764 -0.0018349694 -0.0018350431 -0.0018341825 -0.0018326874 -0.0018307968 -0.0018287527 -0.0018270372 -0.001825831 -0.0018252931 -0.0018250615 -0.0018250245][-0.0018335083 -0.0018337098 -0.001834173 -0.0018344301 -0.0018350645 -0.0018353516 -0.0018351295 -0.0018345622 -0.0018337052 -0.0018324313 -0.0018309847 -0.0018295055 -0.0018282282 -0.0018270523 -0.0018258246][-0.0018317596 -0.0018312265 -0.0018312788 -0.0018314674 -0.00183248 -0.0018334412 -0.0018342355 -0.0018349481 -0.0018351955 -0.0018348122 -0.0018337278 -0.001832414 -0.0018308623 -0.0018291534 -0.0018272602][-0.0018293386 -0.0018277805 -0.0018272566 -0.0018272833 -0.0018278871 -0.0018287071 -0.0018299667 -0.0018313961 -0.0018324901 -0.0018331516 -0.0018331413 -0.0018326367 -0.0018316667 -0.001830208 -0.0018284394][-0.0018281209 -0.0018255784 -0.0018243019 -0.0018239237 -0.001824149 -0.0018245884 -0.0018254212 -0.0018264849 -0.0018276419 -0.00182872 -0.0018294079 -0.0018297024 -0.0018297863 -0.0018293726 -0.0018282519][-0.0018269497 -0.0018238409 -0.0018219609 -0.0018212087 -0.0018211933 -0.0018218316 -0.0018228153 -0.0018236276 -0.0018245416 -0.0018259097 -0.0018270814 -0.0018277434 -0.0018279568 -0.0018278065 -0.00182718][-0.0018273675 -0.0018236565 -0.0018213979 -0.0018202455 -0.0018198688 -0.0018202028 -0.0018210175 -0.0018219789 -0.0018230962 -0.0018246466 -0.0018258684 -0.001826364 -0.001826476 -0.0018263167 -0.0018259377][-0.0018272739 -0.0018238937 -0.0018217139 -0.0018202601 -0.0018197265 -0.0018198063 -0.0018201795 -0.0018209984 -0.0018219852 -0.0018234701 -0.0018246458 -0.0018253832 -0.0018255669 -0.0018255176 -0.0018253555][-0.0018274568 -0.0018246849 -0.0018227942 -0.0018214086 -0.0018208803 -0.0018205337 -0.0018206136 -0.0018209028 -0.0018212601 -0.0018219298 -0.0018226427 -0.0018232638 -0.0018235596 -0.001823913 -0.0018242592][-0.0018275257 -0.001825146 -0.0018234984 -0.0018224671 -0.0018221553 -0.0018220283 -0.0018221714 -0.0018223361 -0.0018224859 -0.0018226343 -0.0018226239 -0.0018226677 -0.0018224174 -0.0018222592 -0.0018223919][-0.0018274109 -0.0018253298 -0.0018241376 -0.001823462 -0.0018234891 -0.0018235896 -0.0018238598 -0.0018240298 -0.001824036 -0.001823946 -0.0018236337 -0.0018235048 -0.0018229667 -0.0018225125 -0.0018221631][-0.0018267946 -0.001825177 -0.0018243799 -0.0018240572 -0.0018242325 -0.0018244927 -0.0018247335 -0.0018248074 -0.0018245861 -0.001824019 -0.0018233117 -0.0018229821 -0.0018224715 -0.0018222155 -0.0018219536][-0.001824687 -0.0018235273 -0.0018231675 -0.0018231622 -0.0018233897 -0.0018237151 -0.0018241757 -0.001824434 -0.0018242146 -0.0018234857 -0.0018227373 -0.0018221927 -0.0018215779 -0.0018213325 -0.0018212171][-0.0018231544 -0.001821971 -0.0018216091 -0.0018216032 -0.0018218013 -0.0018219876 -0.0018224075 -0.0018226122 -0.0018224149 -0.001822087 -0.0018217309 -0.001821305 -0.0018207661 -0.0018205256 -0.0018204297]]...]
INFO - root - 2017-12-09 17:50:57.690959: step 44810, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.849 sec/batch; 67h:48m:52s remains)
INFO - root - 2017-12-09 17:51:06.321307: step 44820, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.840 sec/batch; 67h:06m:39s remains)
INFO - root - 2017-12-09 17:51:15.022644: step 44830, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.853 sec/batch; 68h:08m:23s remains)
INFO - root - 2017-12-09 17:51:23.565209: step 44840, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.888 sec/batch; 70h:56m:16s remains)
INFO - root - 2017-12-09 17:51:32.147177: step 44850, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.880 sec/batch; 70h:18m:40s remains)
INFO - root - 2017-12-09 17:51:40.708676: step 44860, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.865 sec/batch; 69h:07m:36s remains)
INFO - root - 2017-12-09 17:51:49.451232: step 44870, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.852 sec/batch; 68h:03m:36s remains)
INFO - root - 2017-12-09 17:51:58.160159: step 44880, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.892 sec/batch; 71h:14m:42s remains)
INFO - root - 2017-12-09 17:52:06.653645: step 44890, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.858 sec/batch; 68h:33m:56s remains)
INFO - root - 2017-12-09 17:52:15.368120: step 44900, loss = 0.82, batch loss = 0.69 (8.8 examples/sec; 0.905 sec/batch; 72h:17m:00s remains)
2017-12-09 17:52:16.233594: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.11471808 0.11324231 0.10703604 0.097617984 0.085680977 0.0721967 0.058262814 0.045458872 0.034384008 0.025143988 0.017798541 0.012087779 0.0078337844 0.0046041533 0.0022482723][0.14979103 0.1499206 0.14376418 0.13352211 0.12008311 0.10451679 0.087751895 0.07165245 0.05687901 0.043701481 0.032274626 0.022625053 0.014919073 0.0087953229 0.0042984816][0.18033925 0.18288624 0.1777046 0.16744365 0.15346119 0.13709523 0.11925392 0.10189091 0.085231304 0.06941738 0.054481193 0.040611405 0.028404163 0.017973313 0.0099215023][0.20196399 0.20735133 0.20422041 0.19551268 0.18283461 0.16754942 0.15049446 0.13367784 0.11687467 0.10012576 0.082948916 0.0655504 0.048774388 0.033262961 0.020361714][0.21124575 0.21982642 0.22027257 0.21509111 0.20616215 0.19462591 0.18096033 0.16683187 0.15137032 0.1347374 0.11591646 0.095436677 0.074293934 0.053566482 0.035352398][0.20902535 0.22087257 0.22539565 0.22519091 0.22194786 0.21604735 0.20769092 0.19796431 0.18539162 0.17011967 0.15066478 0.12787764 0.10276952 0.077061944 0.053399015][0.19699034 0.21150008 0.21974707 0.22409672 0.2262169 0.22626911 0.22363859 0.21860397 0.2095495 0.19660568 0.17795774 0.15450411 0.12720469 0.098228365 0.070529155][0.17740373 0.19327761 0.20436613 0.2126703 0.21919756 0.22374117 0.22523159 0.22368568 0.21739098 0.20650057 0.18944307 0.1668632 0.13957417 0.10987528 0.080728985][0.15174355 0.168078 0.18106431 0.19219682 0.20185915 0.20946385 0.21347411 0.2134774 0.20813692 0.19805284 0.18211649 0.16089247 0.1352143 0.10731909 0.079805888][0.12132403 0.13773282 0.1521233 0.16528092 0.17692204 0.185998 0.190843 0.19077554 0.18505479 0.17465001 0.15923169 0.13961363 0.11668297 0.092493191 0.068998761][0.089467794 0.10480107 0.11924634 0.13308153 0.14536259 0.15465678 0.15919881 0.15811367 0.15148722 0.14061649 0.12602393 0.10860541 0.089324571 0.070073321 0.051977202][0.060144331 0.07292328 0.085705966 0.098292895 0.10949944 0.11767802 0.12109052 0.1188871 0.11176743 0.10137665 0.08864557 0.074492477 0.059819512 0.046023861 0.033578876][0.035673723 0.04526956 0.055532735 0.065790087 0.074882939 0.081202105 0.083343834 0.080582313 0.073847547 0.064796574 0.054504439 0.04396626 0.033824321 0.024995262 0.017487872][0.017403731 0.024010975 0.03158531 0.0392632 0.046110556 0.050698418 0.052001134 0.049426053 0.043893483 0.036833372 0.029125722 0.021794815 0.015286068 0.010223455 0.0063231234][0.0058298116 0.0097893607 0.01467358 0.019835159 0.024578378 0.027819239 0.028800325 0.02705838 0.023320187 0.01855503 0.013446042 0.0088673439 0.0051176017 0.0025634151 0.00082869118]]...]
INFO - root - 2017-12-09 17:52:24.824069: step 44910, loss = 0.83, batch loss = 0.70 (9.2 examples/sec; 0.873 sec/batch; 69h:46m:10s remains)
INFO - root - 2017-12-09 17:52:33.524002: step 44920, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.852 sec/batch; 68h:01m:14s remains)
INFO - root - 2017-12-09 17:52:42.107576: step 44930, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.830 sec/batch; 66h:17m:09s remains)
INFO - root - 2017-12-09 17:52:50.763180: step 44940, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.860 sec/batch; 68h:41m:39s remains)
INFO - root - 2017-12-09 17:52:59.472660: step 44950, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.874 sec/batch; 69h:49m:41s remains)
INFO - root - 2017-12-09 17:53:08.139775: step 44960, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 69h:17m:15s remains)
INFO - root - 2017-12-09 17:53:16.849049: step 44970, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.873 sec/batch; 69h:41m:33s remains)
INFO - root - 2017-12-09 17:53:25.291524: step 44980, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.853 sec/batch; 68h:05m:53s remains)
INFO - root - 2017-12-09 17:53:33.789978: step 44990, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.848 sec/batch; 67h:42m:05s remains)
INFO - root - 2017-12-09 17:53:42.582406: step 45000, loss = 0.82, batch loss = 0.69 (8.6 examples/sec; 0.932 sec/batch; 74h:25m:36s remains)
2017-12-09 17:53:43.454369: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0018132147 -0.0018133875 -0.0018145213 -0.0018156815 -0.0018165604 -0.0018172293 -0.0018176092 -0.0018174247 -0.0018167404 -0.0018156485 -0.0018143619 -0.0018132287 -0.0018124243 -0.0018117286 -0.0018111959][-0.0018115921 -0.0018116545 -0.0018128025 -0.0018139862 -0.0018149954 -0.0018158591 -0.0018164156 -0.0018163465 -0.0018157276 -0.0018146778 -0.0018133587 -0.0018121966 -0.00181139 -0.0018107455 -0.0018103066][-0.0018117593 -0.0018117618 -0.001812879 -0.0018139873 -0.0018148911 -0.0018156337 -0.001816076 -0.0018158853 -0.0018151647 -0.0018141726 -0.0018130394 -0.0018120185 -0.0018113366 -0.0018108116 -0.0018104805][-0.0018128777 -0.0018127881 -0.001813744 -0.0018147309 -0.0018154854 -0.0018159739 -0.0018161542 -0.0018157661 -0.0018149253 -0.0018138885 -0.0018128267 -0.0018120245 -0.0018115189 -0.0018110943 -0.0018108198][-0.001815 -0.0018148249 -0.001815563 -0.0018164023 -0.0018170375 -0.0018172744 -0.001817097 -0.0018163779 -0.0018153221 -0.0018140472 -0.0018127739 -0.0018118682 -0.0018113725 -0.0018110123 -0.0018107415][-0.0018189659 -0.0018187887 -0.0018193177 -0.0018199779 -0.001820449 -0.0018203409 -0.0018196438 -0.0018183754 -0.0018168221 -0.0018149979 -0.0018131898 -0.0018118563 -0.0018111044 -0.0018107094 -0.0018103718][-0.0018243236 -0.0018242167 -0.0018245896 -0.001825053 -0.0018253339 -0.0018249257 -0.0018237504 -0.0018217994 -0.0018195738 -0.0018170947 -0.0018146504 -0.0018126883 -0.0018114423 -0.001810798 -0.0018102668][-0.0018300705 -0.0018300511 -0.0018302506 -0.0018304234 -0.0018305344 -0.0018299727 -0.0018284568 -0.0018259825 -0.0018231686 -0.0018201466 -0.0018170833 -0.0018144592 -0.0018126488 -0.0018116663 -0.0018109196][-0.0018352643 -0.0018354016 -0.0018355491 -0.0018355597 -0.0018355276 -0.001834846 -0.0018331667 -0.0018304647 -0.0018273558 -0.0018239858 -0.0018204504 -0.0018173131 -0.0018150315 -0.0018137179 -0.0018128231][-0.0018391906 -0.0018394827 -0.0018395844 -0.0018394489 -0.0018392791 -0.001838688 -0.0018371729 -0.0018346276 -0.0018315983 -0.0018281997 -0.0018245007 -0.0018210926 -0.0018184858 -0.0018168924 -0.0018158873][-0.0018407662 -0.0018410435 -0.001841078 -0.0018407839 -0.0018406007 -0.0018403496 -0.0018394954 -0.0018377467 -0.0018353539 -0.0018324038 -0.0018289731 -0.0018256637 -0.0018229738 -0.001821209 -0.0018201714][-0.0018391417 -0.0018393784 -0.0018393341 -0.0018390039 -0.0018389443 -0.0018393156 -0.0018395316 -0.001839 -0.0018376186 -0.0018355318 -0.0018327384 -0.0018297968 -0.0018271982 -0.0018255062 -0.0018246368][-0.0018354427 -0.0018357044 -0.0018357624 -0.0018355484 -0.0018357832 -0.0018367813 -0.0018379347 -0.0018385294 -0.0018381733 -0.001836956 -0.0018349132 -0.0018324059 -0.0018300358 -0.0018285562 -0.0018280277][-0.0018316454 -0.0018321385 -0.0018324394 -0.0018325038 -0.0018329246 -0.0018342425 -0.0018359213 -0.0018370552 -0.0018372943 -0.0018366165 -0.0018350611 -0.00183285 -0.0018308412 -0.001829872 -0.0018299337][-0.0018285542 -0.001829408 -0.0018301156 -0.00183048 -0.0018310499 -0.0018324952 -0.0018343175 -0.0018354999 -0.0018357934 -0.0018352994 -0.0018340491 -0.0018321779 -0.0018306547 -0.0018303713 -0.0018310584]]...]
INFO - root - 2017-12-09 17:53:52.231963: step 45010, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.891 sec/batch; 71h:10m:39s remains)
INFO - root - 2017-12-09 17:54:00.991471: step 45020, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.859 sec/batch; 68h:34m:48s remains)
INFO - root - 2017-12-09 17:54:09.549834: step 45030, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.850 sec/batch; 67h:51m:12s remains)
INFO - root - 2017-12-09 17:54:18.031199: step 45040, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.851 sec/batch; 67h:55m:51s remains)
INFO - root - 2017-12-09 17:54:26.623843: step 45050, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.883 sec/batch; 70h:30m:56s remains)
INFO - root - 2017-12-09 17:54:35.265604: step 45060, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.841 sec/batch; 67h:10m:37s remains)
INFO - root - 2017-12-09 17:54:44.107199: step 45070, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.853 sec/batch; 68h:04m:53s remains)
INFO - root - 2017-12-09 17:54:52.594798: step 45080, loss = 0.81, batch loss = 0.68 (9.1 examples/sec; 0.875 sec/batch; 69h:52m:19s remains)
INFO - root - 2017-12-09 17:55:01.149841: step 45090, loss = 0.82, batch loss = 0.69 (9.9 examples/sec; 0.810 sec/batch; 64h:38m:09s remains)
INFO - root - 2017-12-09 17:55:09.841129: step 45100, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.863 sec/batch; 68h:53m:40s remains)
2017-12-09 17:55:10.724710: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0018106846 -0.0018101196 -0.0018107843 -0.0018117581 -0.0018128307 -0.0018137349 -0.001814393 -0.0018150072 -0.001815564 -0.0018158383 -0.0018158659 -0.0018153101 -0.0018142803 -0.0018132075 -0.0018123061][-0.0018106914 -0.0018101113 -0.0018108611 -0.0018122159 -0.0018140028 -0.0018157529 -0.0018173583 -0.0018186582 -0.0018195575 -0.0018196909 -0.0018192305 -0.0018178914 -0.00181593 -0.0018140636 -0.0018125912][-0.001811346 -0.001810808 -0.0018116353 -0.0018133408 -0.0018157199 -0.0018183067 -0.0018208917 -0.0018228716 -0.0018240559 -0.00182392 -0.0018227873 -0.0018205625 -0.0018176381 -0.0018150668 -0.0018130972][-0.0018120366 -0.0018116137 -0.0018125108 -0.0018145097 -0.0018173859 -0.0018206625 -0.0018241358 -0.001826646 -0.001827996 -0.0018276386 -0.0018259087 -0.0018229207 -0.0018191596 -0.0018160038 -0.0018136029][-0.0018127557 -0.0018124223 -0.0018133334 -0.0018154621 -0.0018185823 -0.0018223089 -0.0018263732 -0.0018292808 -0.0018307259 -0.0018302728 -0.0018282213 -0.0018247846 -0.0018205486 -0.0018169554 -0.0018142409][-0.0018134273 -0.0018132401 -0.0018141305 -0.0018162415 -0.0018192591 -0.0018229701 -0.0018271258 -0.0018301408 -0.0018316837 -0.0018312407 -0.0018291326 -0.0018256962 -0.0018214483 -0.0018177894 -0.0018150096][-0.001813962 -0.0018139716 -0.0018148092 -0.001816763 -0.0018195042 -0.0018227719 -0.0018264276 -0.00182916 -0.0018306995 -0.0018303628 -0.001828476 -0.0018254314 -0.0018215753 -0.001818232 -0.0018156476][-0.0018144541 -0.0018144998 -0.0018152611 -0.0018169209 -0.0018192291 -0.0018218448 -0.0018247676 -0.0018269497 -0.0018282339 -0.0018280866 -0.0018266758 -0.0018242133 -0.0018209846 -0.0018181195 -0.0018159166][-0.0018146589 -0.0018146959 -0.0018154222 -0.0018167406 -0.0018185787 -0.001820597 -0.0018228068 -0.0018244273 -0.0018252947 -0.0018252203 -0.0018242496 -0.001822429 -0.0018199054 -0.0018176174 -0.0018159223][-0.0018145851 -0.0018146371 -0.001815374 -0.0018164286 -0.0018178914 -0.0018193867 -0.0018208881 -0.0018219396 -0.0018224032 -0.0018222668 -0.0018215084 -0.0018202323 -0.0018185113 -0.0018169143 -0.0018157416][-0.0018145505 -0.0018145669 -0.0018153079 -0.0018162136 -0.0018173525 -0.0018183059 -0.0018191088 -0.0018195716 -0.0018196403 -0.0018193313 -0.0018186851 -0.0018178674 -0.001816874 -0.0018159429 -0.0018152711][-0.0018146911 -0.001814605 -0.0018153959 -0.0018162627 -0.0018171493 -0.0018176287 -0.0018177412 -0.0018176687 -0.0018173611 -0.0018168117 -0.0018162158 -0.0018157754 -0.0018153552 -0.001814932 -0.0018146478][-0.0018148385 -0.0018146719 -0.0018154359 -0.0018162417 -0.0018168885 -0.0018170315 -0.0018167361 -0.0018163814 -0.0018158123 -0.0018150656 -0.0018144638 -0.0018142409 -0.0018141898 -0.0018140757 -0.0018140305][-0.0018148132 -0.00181457 -0.001815225 -0.0018159213 -0.0018163566 -0.0018162949 -0.0018158405 -0.0018154189 -0.0018147563 -0.0018139252 -0.0018133625 -0.0018132357 -0.0018133792 -0.001813478 -0.0018135766][-0.0018146293 -0.0018142872 -0.0018147678 -0.0018153392 -0.0018155989 -0.0018154681 -0.0018150001 -0.0018145075 -0.0018138265 -0.0018131012 -0.0018126748 -0.0018126412 -0.0018128585 -0.0018130803 -0.0018132937]]...]
INFO - root - 2017-12-09 17:55:19.319927: step 45110, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.875 sec/batch; 69h:51m:30s remains)
INFO - root - 2017-12-09 17:55:28.139347: step 45120, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.889 sec/batch; 70h:57m:16s remains)
INFO - root - 2017-12-09 17:55:36.690157: step 45130, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.860 sec/batch; 68h:40m:47s remains)
INFO - root - 2017-12-09 17:55:45.483125: step 45140, loss = 0.83, batch loss = 0.70 (9.1 examples/sec; 0.877 sec/batch; 69h:58m:07s remains)
INFO - root - 2017-12-09 17:55:54.280081: step 45150, loss = 0.82, batch loss = 0.69 (8.8 examples/sec; 0.912 sec/batch; 72h:46m:57s remains)
INFO - root - 2017-12-09 17:56:02.921291: step 45160, loss = 0.82, batch loss = 0.69 (8.6 examples/sec; 0.928 sec/batch; 74h:05m:13s remains)
INFO - root - 2017-12-09 17:56:11.574584: step 45170, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 68h:49m:34s remains)
INFO - root - 2017-12-09 17:56:20.179889: step 45180, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.893 sec/batch; 71h:15m:29s remains)
INFO - root - 2017-12-09 17:56:28.732148: step 45190, loss = 0.82, batch loss = 0.69 (10.4 examples/sec; 0.773 sec/batch; 61h:39m:35s remains)
INFO - root - 2017-12-09 17:56:37.203145: step 45200, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.847 sec/batch; 67h:34m:26s remains)
2017-12-09 17:56:38.143767: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0018124691 -0.001810694 -0.0018102615 -0.0018101388 -0.0018100479 -0.0018098647 -0.0018095613 -0.001809123 -0.0018086617 -0.0018083035 -0.0018081468 -0.0018082979 -0.001808708 -0.0018091996 -0.0018096268][-0.0018118424 -0.0018100732 -0.0018097024 -0.0018096465 -0.0018095556 -0.0018092415 -0.0018086878 -0.0018078947 -0.0018071306 -0.0018065622 -0.0018063521 -0.0018065862 -0.00180723 -0.001808032 -0.0018087657][-0.0018121306 -0.0018103835 -0.0018100179 -0.0018099685 -0.0018098161 -0.0018093414 -0.0018085525 -0.0018075021 -0.0018065618 -0.0018059142 -0.0018057289 -0.0018060674 -0.0018069402 -0.0018080164 -0.001808983][-0.0018122196 -0.0018105144 -0.0018101472 -0.0018101094 -0.0018099316 -0.0018093765 -0.0018084778 -0.0018073302 -0.001806346 -0.0018057622 -0.001805702 -0.0018061711 -0.0018071752 -0.0018083842 -0.0018094362][-0.0018120605 -0.0018103499 -0.0018099636 -0.0018099451 -0.0018097952 -0.0018092857 -0.0018084835 -0.0018074538 -0.0018065786 -0.0018061025 -0.0018061677 -0.0018067091 -0.0018077157 -0.0018089042 -0.0018099024][-0.0018117346 -0.0018099735 -0.0018095557 -0.0018095251 -0.0018094332 -0.0018090701 -0.0018085131 -0.0018077847 -0.0018071628 -0.0018068429 -0.0018069965 -0.0018075156 -0.0018083778 -0.0018093865 -0.0018102215][-0.0018113718 -0.0018096149 -0.0018091534 -0.0018090733 -0.0018090182 -0.0018088181 -0.0018085516 -0.0018081863 -0.0018078357 -0.0018076871 -0.0018078701 -0.0018082927 -0.0018089431 -0.0018097033 -0.0018103528][-0.0018110391 -0.0018092868 -0.001808766 -0.0018086317 -0.0018085897 -0.0018085157 -0.0018084475 -0.0018083465 -0.0018082398 -0.0018082478 -0.0018084522 -0.0018087807 -0.0018092488 -0.0018097957 -0.0018102727][-0.0018104499 -0.0018087835 -0.0018083189 -0.0018081901 -0.0018081865 -0.0018082003 -0.0018082504 -0.0018083273 -0.0018083989 -0.0018085116 -0.0018086941 -0.001808931 -0.001809258 -0.0018096283 -0.0018099514][-0.0018098209 -0.0018083844 -0.001808 -0.0018078893 -0.0018079159 -0.001807979 -0.0018080743 -0.0018082288 -0.0018083958 -0.0018085405 -0.0018086664 -0.0018088127 -0.0018090281 -0.0018092567 -0.0018094514][-0.001809533 -0.0018082529 -0.0018078891 -0.0018077582 -0.0018077698 -0.0018078382 -0.0018079451 -0.0018081199 -0.0018083146 -0.0018084459 -0.0018085302 -0.0018086047 -0.0018087152 -0.0018088265 -0.0018089043][-0.0018093935 -0.0018081353 -0.0018078259 -0.0018077041 -0.0018077081 -0.0018077987 -0.0018079298 -0.0018080965 -0.0018082606 -0.0018083556 -0.0018083969 -0.0018084056 -0.0018084315 -0.0018084513 -0.0018084404][-0.0018093532 -0.0018081036 -0.0018078196 -0.0018077361 -0.0018077379 -0.0018078234 -0.0018079438 -0.0018081003 -0.0018082731 -0.0018083713 -0.001808391 -0.0018083542 -0.0018083162 -0.0018082667 -0.0018081869][-0.0018095035 -0.0018081915 -0.0018079081 -0.0018078424 -0.0018078348 -0.0018079032 -0.0018080055 -0.0018081524 -0.0018083275 -0.0018084355 -0.0018084545 -0.0018083868 -0.0018082998 -0.0018082064 -0.0018080969][-0.0018097342 -0.0018083821 -0.0018080459 -0.0018080191 -0.0018080468 -0.0018081106 -0.0018081954 -0.0018083135 -0.0018084485 -0.0018085307 -0.0018085458 -0.0018084735 -0.0018083707 -0.0018082636 -0.0018081514]]...]
INFO - root - 2017-12-09 17:56:46.727587: step 45210, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.882 sec/batch; 70h:24m:54s remains)
INFO - root - 2017-12-09 17:56:55.440405: step 45220, loss = 0.81, batch loss = 0.69 (9.1 examples/sec; 0.879 sec/batch; 70h:06m:30s remains)
INFO - root - 2017-12-09 17:57:04.122677: step 45230, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.874 sec/batch; 69h:44m:34s remains)
INFO - root - 2017-12-09 17:57:12.864170: step 45240, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.863 sec/batch; 68h:51m:55s remains)
INFO - root - 2017-12-09 17:57:21.711252: step 45250, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.870 sec/batch; 69h:27m:12s remains)
INFO - root - 2017-12-09 17:57:30.388803: step 45260, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.876 sec/batch; 69h:53m:14s remains)
INFO - root - 2017-12-09 17:57:39.020294: step 45270, loss = 0.82, batch loss = 0.69 (8.8 examples/sec; 0.913 sec/batch; 72h:51m:17s remains)
INFO - root - 2017-12-09 17:57:47.448429: step 45280, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.855 sec/batch; 68h:12m:41s remains)
INFO - root - 2017-12-09 17:57:55.995539: step 45290, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.873 sec/batch; 69h:39m:39s remains)
INFO - root - 2017-12-09 17:58:04.543718: step 45300, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.863 sec/batch; 68h:50m:07s remains)
2017-12-09 17:58:05.467843: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0018209032 -0.0018202547 -0.001819953 -0.001819656 -0.0018193094 -0.0018188859 -0.0018182355 -0.001817575 -0.0018171696 -0.0018171452 -0.0018174608 -0.0018176632 -0.0018176885 -0.0018175882 -0.0018173362][-0.001821338 -0.0018207774 -0.0018204511 -0.0018200425 -0.0018195459 -0.001819002 -0.0018182391 -0.0018174661 -0.0018169339 -0.0018168528 -0.0018172152 -0.0018174552 -0.0018174533 -0.0018173248 -0.0018170092][-0.0018219767 -0.0018215402 -0.0018212727 -0.0018208041 -0.0018201874 -0.0018195631 -0.0018187821 -0.0018179566 -0.0018172971 -0.0018170767 -0.0018173513 -0.0018175452 -0.0018175349 -0.0018174199 -0.0018170959][-0.0018220468 -0.0018218299 -0.0018217291 -0.0018212965 -0.0018205317 -0.0018196964 -0.0018187953 -0.001817918 -0.0018172592 -0.0018170642 -0.0018172479 -0.0018174072 -0.0018174322 -0.0018173936 -0.0018170973][-0.0018212871 -0.0018212268 -0.0018212704 -0.0018209711 -0.0018201955 -0.0018192248 -0.0018182201 -0.0018173578 -0.0018168667 -0.0018167886 -0.0018169351 -0.0018170932 -0.0018172074 -0.0018172704 -0.0018170266][-0.0018202724 -0.0018201895 -0.0018203063 -0.0018202183 -0.0018196395 -0.0018187641 -0.0018177265 -0.0018167733 -0.001816335 -0.0018163354 -0.0018164821 -0.0018167028 -0.001816958 -0.001817157 -0.001816963][-0.0018188704 -0.0018185914 -0.0018188028 -0.0018190282 -0.0018188483 -0.001818183 -0.0018170921 -0.0018159993 -0.0018155579 -0.0018157145 -0.0018159696 -0.0018163192 -0.0018168065 -0.0018171521 -0.0018169847][-0.0018169075 -0.0018165178 -0.0018169372 -0.0018175322 -0.0018177199 -0.0018172862 -0.001816191 -0.0018150228 -0.0018146359 -0.0018149683 -0.0018154188 -0.0018159345 -0.0018166661 -0.0018171837 -0.0018170231][-0.0018147147 -0.0018143104 -0.001814979 -0.0018158369 -0.0018162639 -0.0018160049 -0.0018151441 -0.0018141543 -0.0018137839 -0.0018141178 -0.0018147482 -0.0018155358 -0.0018165053 -0.001817151 -0.0018170249][-0.0018129913 -0.0018125911 -0.001813355 -0.0018143349 -0.0018149173 -0.0018149853 -0.0018145731 -0.0018139962 -0.001813701 -0.0018138051 -0.0018143344 -0.0018152216 -0.0018162815 -0.0018169949 -0.0018169263][-0.0018123076 -0.0018117441 -0.0018124589 -0.0018134937 -0.0018142657 -0.0018146596 -0.0018145791 -0.0018143248 -0.0018140697 -0.0018139476 -0.0018142725 -0.0018150765 -0.0018160859 -0.0018167763 -0.0018167808][-0.0018127974 -0.0018121434 -0.0018127435 -0.0018137454 -0.001814553 -0.0018150598 -0.0018152073 -0.0018151546 -0.0018148816 -0.0018145944 -0.0018146891 -0.0018152563 -0.001816015 -0.0018165931 -0.0018166348][-0.0018135766 -0.0018130303 -0.0018134677 -0.0018142713 -0.0018149224 -0.001815426 -0.001815741 -0.00181579 -0.0018155288 -0.001815239 -0.0018152408 -0.0018155678 -0.001816045 -0.001816445 -0.0018164824][-0.0018146363 -0.0018141795 -0.0018144129 -0.0018149526 -0.0018154324 -0.0018158756 -0.0018162027 -0.0018162993 -0.0018160812 -0.0018158501 -0.001815776 -0.0018159107 -0.0018161517 -0.0018163514 -0.0018163506][-0.0018157368 -0.0018152598 -0.0018153281 -0.0018156487 -0.0018159712 -0.0018162973 -0.0018165314 -0.0018165959 -0.0018164056 -0.0018162349 -0.0018161326 -0.0018161248 -0.0018162115 -0.0018162885 -0.0018162631]]...]
INFO - root - 2017-12-09 17:58:14.061677: step 45310, loss = 0.82, batch loss = 0.69 (9.7 examples/sec; 0.828 sec/batch; 66h:01m:54s remains)
INFO - root - 2017-12-09 17:58:22.635023: step 45320, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.863 sec/batch; 68h:50m:12s remains)
INFO - root - 2017-12-09 17:58:31.233553: step 45330, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 69h:14m:48s remains)
INFO - root - 2017-12-09 17:58:39.824974: step 45340, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.846 sec/batch; 67h:26m:36s remains)
INFO - root - 2017-12-09 17:58:48.414041: step 45350, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 68h:44m:18s remains)
INFO - root - 2017-12-09 17:58:56.948382: step 45360, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.870 sec/batch; 69h:23m:37s remains)
INFO - root - 2017-12-09 17:59:05.580821: step 45370, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.858 sec/batch; 68h:24m:43s remains)
INFO - root - 2017-12-09 17:59:13.981429: step 45380, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.873 sec/batch; 69h:35m:22s remains)
INFO - root - 2017-12-09 17:59:22.672024: step 45390, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.863 sec/batch; 68h:49m:27s remains)
INFO - root - 2017-12-09 17:59:31.179815: step 45400, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.847 sec/batch; 67h:31m:57s remains)
2017-12-09 17:59:32.031198: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0018219652 -0.0018213486 -0.0018211281 -0.001821024 -0.001821124 -0.001821346 -0.0018215681 -0.0018216363 -0.0018216639 -0.0018216387 -0.0018214537 -0.0018209604 -0.0018206595 -0.0018210403 -0.0018218674][-0.0018215817 -0.0018210574 -0.0018208565 -0.0018207594 -0.0018209257 -0.0018212544 -0.0018215319 -0.0018216281 -0.0018217354 -0.0018217977 -0.0018216368 -0.0018211694 -0.0018209433 -0.0018212347 -0.0018217906][-0.0018213817 -0.0018210345 -0.0018209569 -0.0018210065 -0.0018213617 -0.0018218559 -0.0018222294 -0.0018223706 -0.0018225423 -0.0018226651 -0.0018224925 -0.0018219377 -0.0018215847 -0.0018216413 -0.0018219082][-0.0018212406 -0.0018210552 -0.0018211824 -0.0018214836 -0.0018220471 -0.0018226888 -0.0018231196 -0.0018232755 -0.0018234559 -0.0018236033 -0.0018233689 -0.0018226118 -0.0018219698 -0.0018217344 -0.001821709][-0.0018210321 -0.0018209346 -0.0018212372 -0.0018217638 -0.0018224663 -0.0018231614 -0.0018236029 -0.0018238445 -0.0018241009 -0.0018242543 -0.0018238669 -0.0018228672 -0.0018219672 -0.0018215135 -0.0018212593][-0.0018206573 -0.0018205673 -0.0018209643 -0.0018216118 -0.0018222827 -0.0018228287 -0.0018232377 -0.0018237005 -0.0018241516 -0.0018242031 -0.0018235543 -0.0018224025 -0.0018215097 -0.0018210739 -0.0018207474][-0.0018201587 -0.0018199726 -0.0018203892 -0.0018210654 -0.0018215325 -0.0018218029 -0.001822326 -0.0018232014 -0.0018238461 -0.0018237747 -0.0018228721 -0.0018217717 -0.001821178 -0.0018209422 -0.0018206626][-0.0018198399 -0.0018195648 -0.0018199576 -0.0018205495 -0.0018208772 -0.0018210149 -0.0018216451 -0.0018227799 -0.0018235126 -0.0018234468 -0.0018225824 -0.0018217214 -0.0018213982 -0.0018212999 -0.0018210982][-0.0018197468 -0.00181944 -0.0018197933 -0.0018202524 -0.0018205704 -0.0018208483 -0.0018215295 -0.0018225869 -0.0018232294 -0.0018232345 -0.0018226354 -0.0018220773 -0.0018218901 -0.0018218119 -0.0018215972][-0.0018200371 -0.0018197325 -0.0018199515 -0.0018202381 -0.0018206025 -0.0018209873 -0.0018215807 -0.0018223599 -0.0018228519 -0.0018229261 -0.0018225888 -0.0018222237 -0.0018220578 -0.0018219734 -0.0018217643][-0.0018206795 -0.0018203011 -0.0018203476 -0.0018205035 -0.0018208234 -0.0018211915 -0.0018216388 -0.0018221427 -0.0018224749 -0.0018225551 -0.0018223523 -0.0018220951 -0.0018219325 -0.0018218549 -0.0018217238][-0.0018212659 -0.0018208056 -0.0018207348 -0.0018207969 -0.0018210163 -0.0018213196 -0.0018216691 -0.0018219798 -0.0018221756 -0.0018222337 -0.0018221249 -0.0018219601 -0.0018218341 -0.0018217531 -0.001821676][-0.0018216242 -0.0018211104 -0.001821002 -0.0018210605 -0.0018212253 -0.0018214446 -0.0018216757 -0.0018218505 -0.0018219459 -0.0018219692 -0.0018219213 -0.0018218213 -0.0018216834 -0.0018215508 -0.001821455][-0.0018218164 -0.0018213328 -0.001821193 -0.0018212401 -0.0018213305 -0.0018214448 -0.0018215595 -0.0018216296 -0.0018216382 -0.0018216221 -0.0018215893 -0.0018215138 -0.0018213737 -0.0018212307 -0.0018211572][-0.0018219074 -0.0018214206 -0.0018212404 -0.001821285 -0.0018213304 -0.001821371 -0.0018214243 -0.0018214406 -0.0018214056 -0.0018213546 -0.001821295 -0.0018212048 -0.0018210864 -0.0018209906 -0.0018209718]]...]
INFO - root - 2017-12-09 17:59:40.751054: step 45410, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 69h:29m:21s remains)
INFO - root - 2017-12-09 17:59:49.369396: step 45420, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.836 sec/batch; 66h:39m:57s remains)
INFO - root - 2017-12-09 17:59:58.049104: step 45430, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.886 sec/batch; 70h:39m:08s remains)
INFO - root - 2017-12-09 18:00:06.827868: step 45440, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.837 sec/batch; 66h:45m:27s remains)
INFO - root - 2017-12-09 18:00:15.572521: step 45450, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.866 sec/batch; 69h:04m:42s remains)
INFO - root - 2017-12-09 18:00:24.118477: step 45460, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.888 sec/batch; 70h:50m:34s remains)
INFO - root - 2017-12-09 18:00:32.976340: step 45470, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.887 sec/batch; 70h:44m:18s remains)
INFO - root - 2017-12-09 18:00:41.500409: step 45480, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.846 sec/batch; 67h:25m:52s remains)
INFO - root - 2017-12-09 18:00:50.088011: step 45490, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 69h:07m:40s remains)
INFO - root - 2017-12-09 18:00:58.555712: step 45500, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.853 sec/batch; 68h:00m:27s remains)
2017-12-09 18:00:59.404410: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0018339531 -0.001834586 -0.0018352884 -0.0018359984 -0.0018365992 -0.0018370978 -0.0018377694 -0.0018389681 -0.0018403087 -0.001841396 -0.0018419911 -0.0018419024 -0.0018411516 -0.0018397112 -0.0018379156][-0.0018341949 -0.0018350811 -0.0018360678 -0.0018370318 -0.0018377922 -0.0018383418 -0.0018389686 -0.0018400128 -0.0018411519 -0.0018421059 -0.0018425949 -0.0018424188 -0.0018415075 -0.001839906 -0.0018379791][-0.0018341861 -0.0018352049 -0.0018363518 -0.001837466 -0.0018383603 -0.0018390496 -0.0018396925 -0.0018405612 -0.0018414714 -0.0018422484 -0.0018426725 -0.001842502 -0.0018415996 -0.0018400407 -0.0018381428][-0.0018337452 -0.0018347141 -0.0018358312 -0.0018369935 -0.0018379933 -0.0018387991 -0.0018394635 -0.001840191 -0.0018409203 -0.0018415352 -0.0018419366 -0.001841882 -0.0018411763 -0.0018398482 -0.0018381869][-0.0018329294 -0.0018336908 -0.0018346665 -0.0018357909 -0.001836847 -0.0018377856 -0.0018385315 -0.0018391982 -0.0018397645 -0.0018402647 -0.0018406671 -0.0018407435 -0.0018403117 -0.0018393095 -0.0018379758][-0.0018319674 -0.0018324448 -0.0018332107 -0.0018341849 -0.0018351964 -0.0018362323 -0.0018370459 -0.0018376656 -0.0018381327 -0.0018385528 -0.0018389332 -0.0018390698 -0.0018388656 -0.0018382091 -0.0018372291][-0.0018311528 -0.0018313312 -0.0018318124 -0.0018325249 -0.0018333684 -0.0018343371 -0.0018350765 -0.0018355986 -0.0018360119 -0.0018363986 -0.0018367318 -0.0018368923 -0.0018368238 -0.0018364604 -0.0018358262][-0.0018306951 -0.0018306009 -0.0018307829 -0.0018312109 -0.0018317822 -0.0018325226 -0.0018330974 -0.00183346 -0.0018337733 -0.0018340813 -0.001834339 -0.0018344984 -0.0018345312 -0.0018343726 -0.0018340228][-0.0018304732 -0.0018301628 -0.0018301636 -0.0018304057 -0.0018307897 -0.0018313774 -0.0018318667 -0.0018321148 -0.001832232 -0.0018323343 -0.0018324521 -0.0018325635 -0.0018326315 -0.0018325875 -0.0018324277][-0.0018302744 -0.0018298884 -0.0018298323 -0.0018300193 -0.001830351 -0.0018308894 -0.0018313195 -0.0018314807 -0.0018314106 -0.0018312989 -0.0018312405 -0.0018312545 -0.0018313195 -0.0018313644 -0.0018313631][-0.0018302632 -0.0018298946 -0.0018298401 -0.0018300039 -0.0018302737 -0.0018306852 -0.0018309646 -0.0018309887 -0.0018307884 -0.0018305577 -0.0018304165 -0.0018303581 -0.0018303925 -0.0018304714 -0.0018305505][-0.0018304961 -0.0018301473 -0.0018300975 -0.0018302046 -0.001830369 -0.0018306081 -0.0018307145 -0.0018306193 -0.0018303433 -0.0018300624 -0.0018298796 -0.0018297702 -0.0018297321 -0.0018297753 -0.0018298848][-0.0018307322 -0.001830421 -0.001830366 -0.0018303865 -0.0018304063 -0.0018304698 -0.0018304358 -0.0018302589 -0.0018299503 -0.0018296447 -0.0018294463 -0.0018293082 -0.0018292342 -0.0018292521 -0.0018293954][-0.0018308484 -0.0018305727 -0.0018305229 -0.0018304517 -0.0018303486 -0.0018302974 -0.0018301624 -0.001829894 -0.0018295425 -0.0018292511 -0.0018290866 -0.0018289441 -0.001828869 -0.0018289111 -0.0018290981][-0.0018308513 -0.0018305234 -0.001830437 -0.0018303082 -0.0018301792 -0.001830111 -0.0018299477 -0.0018296257 -0.0018292653 -0.001829056 -0.001829015 -0.0018289404 -0.0018288918 -0.0018289414 -0.0018291021]]...]
INFO - root - 2017-12-09 18:01:08.134340: step 45510, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.901 sec/batch; 71h:51m:26s remains)
INFO - root - 2017-12-09 18:01:16.631189: step 45520, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.846 sec/batch; 67h:27m:37s remains)
INFO - root - 2017-12-09 18:01:25.219267: step 45530, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.863 sec/batch; 68h:46m:07s remains)
INFO - root - 2017-12-09 18:01:33.835797: step 45540, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.838 sec/batch; 66h:50m:11s remains)
INFO - root - 2017-12-09 18:01:42.560899: step 45550, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.861 sec/batch; 68h:39m:28s remains)
INFO - root - 2017-12-09 18:01:51.084027: step 45560, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.886 sec/batch; 70h:37m:36s remains)
INFO - root - 2017-12-09 18:01:59.818966: step 45570, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.879 sec/batch; 70h:04m:11s remains)
INFO - root - 2017-12-09 18:02:08.328923: step 45580, loss = 0.82, batch loss = 0.69 (9.7 examples/sec; 0.828 sec/batch; 66h:01m:49s remains)
INFO - root - 2017-12-09 18:02:16.822807: step 45590, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.878 sec/batch; 69h:59m:05s remains)
INFO - root - 2017-12-09 18:02:25.273271: step 45600, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.874 sec/batch; 69h:39m:00s remains)
2017-12-09 18:02:26.116517: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0018001248 -0.0017970445 -0.0017961654 -0.0017957352 -0.0017955296 -0.001795416 -0.001795268 -0.0017951025 -0.0017949375 -0.0017948524 -0.0017947864 -0.0017947251 -0.0017947343 -0.0017946674 -0.00179458][-0.0017989523 -0.0017955657 -0.0017944274 -0.0017939075 -0.0017937344 -0.0017937891 -0.001793729 -0.001793512 -0.0017932453 -0.0017930921 -0.0017929414 -0.0017927915 -0.0017926393 -0.00179247 -0.0017923116][-0.0017993314 -0.0017957257 -0.0017942273 -0.0017935954 -0.0017935808 -0.0017938216 -0.001793863 -0.0017936496 -0.0017933699 -0.0017931702 -0.0017929948 -0.0017928301 -0.0017925468 -0.0017922204 -0.0017918877][-0.0017997375 -0.001795935 -0.0017942195 -0.0017935438 -0.0017939307 -0.0017945024 -0.0017946126 -0.0017944941 -0.001794304 -0.0017942187 -0.0017940678 -0.0017938423 -0.0017934751 -0.001792984 -0.0017924996][-0.0017998462 -0.0017958393 -0.001793941 -0.0017932836 -0.0017939938 -0.0017947399 -0.0017948241 -0.0017948418 -0.0017949193 -0.0017951427 -0.0017951872 -0.0017950025 -0.0017945177 -0.0017938123 -0.001793191][-0.0018001115 -0.0017961306 -0.0017943816 -0.0017939653 -0.0017949019 -0.0017955423 -0.0017953969 -0.0017953441 -0.0017957675 -0.0017964885 -0.0017968694 -0.0017967519 -0.0017960514 -0.0017951285 -0.0017943862][-0.0018005602 -0.0017968741 -0.0017955871 -0.0017955641 -0.0017964846 -0.001796714 -0.0017960491 -0.0017957918 -0.0017967169 -0.0017980514 -0.0017988522 -0.0017988351 -0.0017979551 -0.0017969146 -0.0017961366][-0.001800424 -0.0017969701 -0.0017960418 -0.0017963168 -0.0017971215 -0.0017969956 -0.0017959827 -0.0017956846 -0.0017971956 -0.0017990568 -0.001800055 -0.0018000149 -0.0017990994 -0.0017980976 -0.0017974159][-0.001799312 -0.0017960049 -0.0017954026 -0.0017958747 -0.0017965276 -0.0017964429 -0.0017959097 -0.0017961683 -0.0017977746 -0.0017993513 -0.0018000182 -0.00179974 -0.0017988806 -0.001798045 -0.0017976346][-0.0017976591 -0.0017944269 -0.001794018 -0.0017945875 -0.001795185 -0.0017954203 -0.0017957805 -0.0017965998 -0.0017977813 -0.001798448 -0.001798459 -0.0017978665 -0.0017972179 -0.0017968204 -0.0017968353][-0.0017957329 -0.0017923572 -0.0017919083 -0.0017926067 -0.0017933791 -0.0017941803 -0.0017953825 -0.0017965286 -0.0017971245 -0.001796918 -0.0017963077 -0.001795547 -0.0017952692 -0.0017954913 -0.0017959293][-0.001794246 -0.0017905892 -0.0017899055 -0.0017904958 -0.0017913555 -0.0017926812 -0.0017944844 -0.0017958708 -0.0017961286 -0.0017954272 -0.001794518 -0.0017938013 -0.0017939236 -0.0017947016 -0.0017955123][-0.0017933258 -0.0017894952 -0.0017884155 -0.0017885866 -0.0017892628 -0.0017907652 -0.0017929341 -0.0017944485 -0.0017945574 -0.0017937018 -0.0017928304 -0.0017923819 -0.0017928391 -0.0017938932 -0.0017948623][-0.0017932446 -0.0017892626 -0.0017876386 -0.0017871977 -0.0017875099 -0.0017889463 -0.0017910963 -0.0017924481 -0.0017923746 -0.0017915524 -0.0017909476 -0.0017908098 -0.0017914202 -0.0017924334 -0.0017932961][-0.0017937316 -0.0017894992 -0.0017873435 -0.0017864457 -0.0017865659 -0.0017878049 -0.0017895526 -0.0017904788 -0.0017902539 -0.0017895846 -0.0017892731 -0.0017894007 -0.0017899082 -0.0017905599 -0.0017911259]]...]
INFO - root - 2017-12-09 18:02:34.683012: step 45610, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.846 sec/batch; 67h:24m:21s remains)
INFO - root - 2017-12-09 18:02:43.248731: step 45620, loss = 0.83, batch loss = 0.70 (9.0 examples/sec; 0.885 sec/batch; 70h:31m:12s remains)
INFO - root - 2017-12-09 18:02:51.868424: step 45630, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 69h:26m:34s remains)
INFO - root - 2017-12-09 18:03:00.501559: step 45640, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.838 sec/batch; 66h:47m:00s remains)
INFO - root - 2017-12-09 18:03:09.237872: step 45650, loss = 0.82, batch loss = 0.69 (8.7 examples/sec; 0.918 sec/batch; 73h:10m:14s remains)
INFO - root - 2017-12-09 18:03:17.848192: step 45660, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.877 sec/batch; 69h:51m:43s remains)
INFO - root - 2017-12-09 18:03:26.537291: step 45670, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.874 sec/batch; 69h:39m:37s remains)
INFO - root - 2017-12-09 18:03:35.139913: step 45680, loss = 0.83, batch loss = 0.70 (11.0 examples/sec; 0.729 sec/batch; 58h:05m:06s remains)
INFO - root - 2017-12-09 18:03:43.679533: step 45690, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.842 sec/batch; 67h:04m:13s remains)
INFO - root - 2017-12-09 18:03:52.253193: step 45700, loss = 0.83, batch loss = 0.70 (9.2 examples/sec; 0.870 sec/batch; 69h:17m:11s remains)
2017-12-09 18:03:53.194484: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.00012938201 0.00098609424 0.0023507364 0.00367029 0.0047968049 0.005483747 0.0058091693 0.0057932222 0.0056897607 0.00573826 0.0058972058 0.0057143439 0.0054845717 0.0051486618 0.004339485][0.0017629919 0.0026971479 0.004179413 0.0058050286 0.0070818691 0.0078556268 0.0082032382 0.0083058961 0.0084037241 0.0083391406 0.008679186 0.0088339895 0.0087946262 0.0084652044 0.0072497269][0.0047049737 0.005937329 0.0075630378 0.00880958 0.0094536534 0.0094443858 0.0090875225 0.0093462327 0.0098940618 0.010669314 0.012069422 0.013334963 0.014234467 0.013986285 0.012142483][0.0070748981 0.0089675058 0.010532637 0.011531248 0.011712786 0.010860564 0.00967033 0.0092582852 0.0096789282 0.011452332 0.014147396 0.016647846 0.018448098 0.01877095 0.017230013][0.0086670974 0.011117147 0.012990009 0.013458691 0.012773989 0.011022625 0.0091263475 0.0081533045 0.0086428262 0.011122814 0.014623432 0.018590869 0.021889567 0.023079135 0.022232428][0.0097388644 0.012647636 0.014915113 0.015566449 0.014764453 0.012536404 0.0099549331 0.0084083807 0.0085451948 0.01111253 0.015044888 0.019696256 0.023669189 0.02632831 0.027066844][0.011518362 0.015391367 0.018634157 0.020266971 0.019976245 0.017945249 0.015424854 0.013443289 0.013056573 0.014832634 0.01821729 0.022615954 0.026818065 0.030355981 0.032275386][0.014359782 0.019509437 0.024090927 0.027083149 0.028118465 0.027212441 0.025519336 0.023555169 0.022868061 0.023866363 0.026396118 0.030475559 0.034354169 0.038368028 0.040241022][0.017057559 0.023732902 0.029906815 0.034492448 0.036946714 0.037122697 0.036266554 0.034953285 0.034547195 0.035483316 0.037359968 0.040969972 0.04482941 0.048786644 0.0503072][0.018390002 0.026084095 0.033431437 0.039078608 0.042652622 0.043941438 0.043865632 0.043301791 0.043182533 0.044068996 0.045781568 0.049095903 0.052928839 0.056858242 0.058764279][0.017521532 0.025706254 0.03373735 0.040296089 0.044584244 0.046776623 0.047529589 0.047455035 0.047469661 0.048103854 0.049430087 0.052002061 0.055127 0.058625057 0.060569152][0.014518841 0.022143738 0.029866988 0.036698796 0.041743502 0.044601072 0.045973361 0.046543423 0.046880916 0.047214288 0.047961198 0.049540851 0.051573388 0.054069605 0.055599187][0.011053495 0.017327979 0.023923319 0.030045556 0.034942593 0.038190678 0.040002756 0.041066743 0.041464753 0.04172913 0.042141184 0.042906556 0.044009574 0.045462038 0.046457559][0.0074714273 0.01193974 0.016857054 0.02173027 0.025813324 0.028692806 0.030472009 0.031597909 0.032096785 0.032299791 0.032324724 0.032626055 0.033130236 0.033870868 0.034391578][0.0041488344 0.0069051413 0.010094418 0.013400358 0.016260162 0.018357005 0.019714532 0.020569725 0.020944379 0.02103447 0.020966429 0.020981468 0.020944027 0.021126326 0.021218181]]...]
INFO - root - 2017-12-09 18:04:01.823795: step 45710, loss = 0.82, batch loss = 0.69 (9.9 examples/sec; 0.806 sec/batch; 64h:12m:07s remains)
INFO - root - 2017-12-09 18:04:10.538723: step 45720, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 69h:07m:34s remains)
INFO - root - 2017-12-09 18:04:19.216257: step 45730, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.886 sec/batch; 70h:35m:44s remains)
INFO - root - 2017-12-09 18:04:28.006903: step 45740, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 69h:02m:25s remains)
INFO - root - 2017-12-09 18:04:36.725182: step 45750, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.897 sec/batch; 71h:24m:56s remains)
INFO - root - 2017-12-09 18:04:45.243580: step 45760, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.853 sec/batch; 67h:58m:20s remains)
INFO - root - 2017-12-09 18:04:53.967179: step 45770, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.834 sec/batch; 66h:26m:36s remains)
INFO - root - 2017-12-09 18:05:02.642553: step 45780, loss = 0.82, batch loss = 0.69 (10.6 examples/sec; 0.752 sec/batch; 59h:55m:02s remains)
INFO - root - 2017-12-09 18:05:11.087785: step 45790, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.839 sec/batch; 66h:49m:22s remains)
INFO - root - 2017-12-09 18:05:19.518782: step 45800, loss = 0.83, batch loss = 0.70 (9.2 examples/sec; 0.871 sec/batch; 69h:21m:06s remains)
2017-12-09 18:05:20.382645: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0018164713 -0.0018194779 -0.0018230532 -0.0018262737 -0.0018279855 -0.0018283036 -0.0018279675 -0.0018274185 -0.0018274297 -0.0018274732 -0.001827396 -0.001826612 -0.0018244962 -0.0018215436 -0.0018182808][-0.0018157513 -0.0018190652 -0.001822939 -0.0018264626 -0.001828486 -0.0018291064 -0.0018288424 -0.0018283339 -0.0018283976 -0.0018285909 -0.0018285714 -0.0018279044 -0.0018256197 -0.0018225773 -0.0018192943][-0.0018158435 -0.0018189593 -0.0018226554 -0.0018260722 -0.0018281946 -0.0018290817 -0.0018289591 -0.001828641 -0.0018290401 -0.0018294451 -0.0018296017 -0.0018292259 -0.0018271172 -0.0018239944 -0.0018207061][-0.0018167336 -0.0018195054 -0.0018229696 -0.0018263496 -0.0018283731 -0.0018293152 -0.0018291996 -0.0018287736 -0.0018289466 -0.0018289391 -0.0018288448 -0.0018287173 -0.0018271217 -0.0018242504 -0.0018212316][-0.0018177693 -0.0018204026 -0.0018235364 -0.0018268613 -0.0018289126 -0.0018297188 -0.0018293122 -0.0018284121 -0.0018280919 -0.0018274097 -0.0018268195 -0.001826758 -0.0018259081 -0.00182379 -0.0018212954][-0.001818006 -0.0018205 -0.0018234128 -0.0018266257 -0.0018286597 -0.0018294151 -0.0018287932 -0.0018272975 -0.0018262349 -0.0018248819 -0.0018239251 -0.0018238216 -0.0018237076 -0.0018227086 -0.001821341][-0.0018171137 -0.0018191105 -0.0018215164 -0.0018242731 -0.0018259679 -0.0018266003 -0.0018259182 -0.0018242266 -0.0018229046 -0.0018214798 -0.0018206357 -0.0018206344 -0.0018212312 -0.0018212873 -0.0018208988][-0.0018153192 -0.0018166039 -0.0018183421 -0.0018206313 -0.0018220518 -0.0018225079 -0.0018218636 -0.0018199797 -0.0018183661 -0.0018170718 -0.0018164961 -0.0018168526 -0.0018180696 -0.0018190098 -0.001819295][-0.0018131464 -0.0018137586 -0.001815112 -0.0018170445 -0.0018181757 -0.0018185219 -0.0018179022 -0.0018159561 -0.0018143818 -0.0018132573 -0.0018129417 -0.0018134692 -0.0018149952 -0.0018165731 -0.0018173645][-0.0018110097 -0.0018109613 -0.0018119056 -0.0018133686 -0.0018143118 -0.0018146889 -0.0018142688 -0.0018125081 -0.0018110439 -0.0018100404 -0.0018099493 -0.0018104775 -0.0018119219 -0.0018137112 -0.0018148434][-0.0018092763 -0.0018085273 -0.001808908 -0.0018099558 -0.0018109167 -0.0018114478 -0.0018114933 -0.0018103245 -0.0018090991 -0.0018080499 -0.0018079069 -0.0018082085 -0.0018093258 -0.0018109239 -0.0018120521][-0.0018081537 -0.001806766 -0.0018066044 -0.0018073388 -0.0018083531 -0.0018091768 -0.001809756 -0.0018091603 -0.0018082043 -0.0018071142 -0.0018066645 -0.0018066027 -0.0018072546 -0.0018083785 -0.0018092471][-0.0018074589 -0.0018056461 -0.0018052277 -0.0018057993 -0.0018068262 -0.001807784 -0.0018085621 -0.0018083601 -0.0018076359 -0.0018064915 -0.001805755 -0.0018054288 -0.0018056869 -0.0018064288 -0.0018070282][-0.0018072319 -0.0018052406 -0.0018047792 -0.0018052709 -0.0018062826 -0.0018073035 -0.0018080821 -0.0018079758 -0.0018073217 -0.001806261 -0.0018054877 -0.0018050534 -0.0018051128 -0.001805581 -0.0018060128][-0.0018069551 -0.001804982 -0.0018044787 -0.0018049846 -0.0018060281 -0.0018069773 -0.001807722 -0.0018076507 -0.0018070383 -0.0018061138 -0.0018053954 -0.0018050156 -0.0018050162 -0.0018053415 -0.0018056529]]...]
INFO - root - 2017-12-09 18:05:28.990781: step 45810, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 69h:08m:42s remains)
INFO - root - 2017-12-09 18:05:37.593178: step 45820, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.866 sec/batch; 68h:59m:40s remains)
INFO - root - 2017-12-09 18:05:46.472583: step 45830, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.889 sec/batch; 70h:46m:33s remains)
INFO - root - 2017-12-09 18:05:55.232764: step 45840, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.875 sec/batch; 69h:39m:02s remains)
INFO - root - 2017-12-09 18:06:03.785415: step 45850, loss = 0.81, batch loss = 0.68 (9.3 examples/sec; 0.863 sec/batch; 68h:43m:54s remains)
INFO - root - 2017-12-09 18:06:12.384019: step 45860, loss = 0.83, batch loss = 0.70 (9.2 examples/sec; 0.874 sec/batch; 69h:35m:53s remains)
INFO - root - 2017-12-09 18:06:20.972023: step 45870, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.874 sec/batch; 69h:33m:19s remains)
INFO - root - 2017-12-09 18:06:29.575182: step 45880, loss = 0.82, batch loss = 0.69 (10.9 examples/sec; 0.735 sec/batch; 58h:29m:11s remains)
INFO - root - 2017-12-09 18:06:38.182796: step 45890, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.859 sec/batch; 68h:23m:34s remains)
INFO - root - 2017-12-09 18:06:46.435179: step 45900, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.850 sec/batch; 67h:38m:19s remains)
2017-12-09 18:06:47.319315: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.001810947 -0.001810295 -0.0018106189 -0.0018111444 -0.0018113777 -0.0018113718 -0.0018111354 -0.0018105848 -0.0018097234 -0.0018085735 -0.0018074272 -0.0018065142 -0.0018059111 -0.0018056798 -0.0018057446][-0.0018112434 -0.0018112421 -0.0018124217 -0.001813935 -0.0018151418 -0.0018157791 -0.0018157455 -0.0018149969 -0.0018135592 -0.0018115925 -0.001809535 -0.0018078161 -0.0018066565 -0.0018061183 -0.0018059949][-0.0018121408 -0.0018129969 -0.0018151304 -0.0018176205 -0.0018198219 -0.0018212026 -0.0018214235 -0.0018204886 -0.0018184854 -0.0018157114 -0.0018126705 -0.0018100232 -0.0018081624 -0.0018071749 -0.0018067189][-0.0018132325 -0.0018148597 -0.0018177328 -0.0018210581 -0.0018241735 -0.0018262016 -0.0018266373 -0.0018255678 -0.0018231027 -0.0018196441 -0.001815738 -0.0018122232 -0.0018096244 -0.0018080936 -0.0018072707][-0.001814126 -0.0018162923 -0.00181969 -0.001823636 -0.0018272742 -0.0018296887 -0.0018301677 -0.0018288285 -0.001825946 -0.0018220618 -0.0018177143 -0.0018136461 -0.0018105218 -0.001808609 -0.0018075318][-0.0018145295 -0.0018168796 -0.001820547 -0.0018248727 -0.0018288086 -0.0018312308 -0.0018314295 -0.0018297378 -0.0018266106 -0.0018225989 -0.0018181297 -0.0018139736 -0.0018107967 -0.0018088217 -0.0018076957][-0.0018143436 -0.001816445 -0.0018198645 -0.0018239517 -0.0018276328 -0.0018296087 -0.0018293808 -0.0018275236 -0.0018245708 -0.0018208991 -0.0018167961 -0.0018130522 -0.0018102311 -0.0018084818 -0.0018074769][-0.0018137491 -0.0018152307 -0.0018179022 -0.0018212732 -0.0018241975 -0.0018256023 -0.0018251757 -0.0018234412 -0.0018209436 -0.0018178566 -0.0018144542 -0.001811423 -0.001809223 -0.0018079473 -0.0018072042][-0.0018128345 -0.0018137083 -0.0018155669 -0.0018179887 -0.0018200307 -0.0018209076 -0.0018203162 -0.0018186957 -0.0018165876 -0.0018141319 -0.00181161 -0.0018095159 -0.0018081333 -0.0018075047 -0.0018071497][-0.0018118655 -0.0018122811 -0.0018135358 -0.0018150862 -0.0018162947 -0.0018165783 -0.0018156865 -0.0018140734 -0.0018123174 -0.0018105056 -0.001808909 -0.0018078034 -0.0018073239 -0.0018073325 -0.0018073719][-0.0018110293 -0.00181102 -0.0018117941 -0.0018126827 -0.0018131986 -0.0018129827 -0.0018119973 -0.0018105881 -0.0018092048 -0.0018079039 -0.0018069856 -0.0018066741 -0.0018068379 -0.0018072705 -0.0018076001][-0.0018104531 -0.0018100942 -0.0018104317 -0.0018108764 -0.0018109346 -0.0018105265 -0.0018096374 -0.0018084659 -0.0018073784 -0.0018065155 -0.0018061701 -0.0018063656 -0.0018068723 -0.0018075042 -0.0018079764][-0.0018101266 -0.0018094439 -0.0018094889 -0.0018097077 -0.0018095887 -0.001809166 -0.0018084211 -0.001807441 -0.0018065895 -0.0018060327 -0.0018059839 -0.0018063745 -0.0018069576 -0.0018076178 -0.0018081286][-0.0018098711 -0.0018090265 -0.0018089147 -0.0018090261 -0.0018088424 -0.0018084433 -0.001807818 -0.0018070193 -0.0018063206 -0.0018059022 -0.0018059314 -0.0018062977 -0.0018068306 -0.0018074085 -0.0018078866][-0.0018098071 -0.0018088047 -0.0018085862 -0.0018086956 -0.0018085446 -0.0018082466 -0.0018077974 -0.0018071915 -0.0018065968 -0.0018061353 -0.001805994 -0.0018061248 -0.0018064559 -0.001806892 -0.0018073254]]...]
INFO - root - 2017-12-09 18:06:55.892669: step 45910, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.865 sec/batch; 68h:52m:00s remains)
INFO - root - 2017-12-09 18:07:04.635724: step 45920, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.875 sec/batch; 69h:38m:56s remains)
INFO - root - 2017-12-09 18:07:13.246877: step 45930, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.873 sec/batch; 69h:28m:14s remains)
INFO - root - 2017-12-09 18:07:21.911799: step 45940, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.848 sec/batch; 67h:29m:59s remains)
INFO - root - 2017-12-09 18:07:30.653535: step 45950, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.861 sec/batch; 68h:32m:07s remains)
INFO - root - 2017-12-09 18:07:39.297370: step 45960, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.849 sec/batch; 67h:35m:05s remains)
INFO - root - 2017-12-09 18:07:48.133928: step 45970, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.884 sec/batch; 70h:21m:11s remains)
INFO - root - 2017-12-09 18:07:56.769573: step 45980, loss = 0.82, batch loss = 0.69 (11.1 examples/sec; 0.723 sec/batch; 57h:32m:24s remains)
INFO - root - 2017-12-09 18:08:05.302463: step 45990, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.851 sec/batch; 67h:43m:25s remains)
INFO - root - 2017-12-09 18:08:13.831752: step 46000, loss = 0.82, batch loss = 0.69 (9.8 examples/sec; 0.819 sec/batch; 65h:11m:26s remains)
2017-12-09 18:08:14.706489: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0017990968 -0.0017972657 -0.0017970871 -0.0017970467 -0.0017972016 -0.0017972674 -0.0017970942 -0.00179687 -0.0017965597 -0.0017962395 -0.001796077 -0.0017963576 -0.0017966109 -0.0017970942 -0.0017977933][-0.0017973382 -0.0017955465 -0.0017955116 -0.0017955578 -0.0017957041 -0.0017957753 -0.001795587 -0.0017954744 -0.0017954464 -0.0017954776 -0.0017955682 -0.0017959285 -0.0017961026 -0.0017963679 -0.001796792][-0.0017973189 -0.0017956475 -0.0017956446 -0.0017956368 -0.0017955879 -0.0017954614 -0.0017951725 -0.0017951021 -0.0017953047 -0.001795657 -0.0017960249 -0.0017964976 -0.0017967144 -0.0017968613 -0.0017970474][-0.0017977655 -0.0017962004 -0.0017961581 -0.0017959637 -0.0017956023 -0.0017952722 -0.0017948749 -0.00179484 -0.0017952116 -0.0017958625 -0.0017965111 -0.0017971561 -0.0017975027 -0.0017976139 -0.0017976762][-0.0017983181 -0.0017967211 -0.001796601 -0.0017962267 -0.0017955558 -0.0017950148 -0.0017944972 -0.0017944603 -0.001794916 -0.0017957574 -0.0017966571 -0.0017974749 -0.0017979901 -0.0017981397 -0.0017981326][-0.0017987416 -0.001796985 -0.0017967155 -0.0017962279 -0.0017953034 -0.0017945284 -0.0017939169 -0.0017938938 -0.0017944113 -0.0017953395 -0.0017964171 -0.0017973728 -0.0017980408 -0.0017982988 -0.0017983203][-0.0017988274 -0.0017969699 -0.0017965913 -0.0017960558 -0.0017950581 -0.0017940889 -0.0017933461 -0.001793279 -0.0017938262 -0.0017947918 -0.0017959746 -0.0017970522 -0.0017978473 -0.0017982473 -0.0017983371][-0.0017988068 -0.0017969533 -0.0017965556 -0.0017961197 -0.0017952377 -0.0017942273 -0.0017933735 -0.0017931855 -0.0017936481 -0.0017945779 -0.0017957615 -0.0017968584 -0.0017976885 -0.0017981596 -0.0017982828][-0.0017988043 -0.0017970576 -0.0017967442 -0.0017964416 -0.0017957384 -0.0017947992 -0.0017938955 -0.0017935452 -0.0017938488 -0.0017946358 -0.0017957082 -0.0017967463 -0.0017975621 -0.0017980525 -0.0017982111][-0.0017988684 -0.0017972527 -0.0017970223 -0.0017968225 -0.0017962421 -0.0017953727 -0.0017944974 -0.0017940534 -0.0017941878 -0.0017948095 -0.0017957183 -0.001796666 -0.0017974448 -0.0017979358 -0.0017981397][-0.001798948 -0.0017974377 -0.0017972391 -0.0017971516 -0.0017967449 -0.0017960381 -0.0017953273 -0.0017949232 -0.0017949421 -0.0017953402 -0.0017959946 -0.0017967535 -0.0017974015 -0.0017978442 -0.001798092][-0.0017990174 -0.0017974986 -0.0017973233 -0.001797318 -0.0017970803 -0.001796577 -0.0017960331 -0.0017956984 -0.0017956644 -0.0017959062 -0.0017963401 -0.0017969045 -0.001797442 -0.0017978466 -0.0017981036][-0.0017990445 -0.001797527 -0.0017973225 -0.0017973608 -0.0017972194 -0.0017968849 -0.0017964685 -0.0017961924 -0.0017961381 -0.0017962871 -0.0017965947 -0.0017970455 -0.0017975173 -0.0017978766 -0.0017981114][-0.0017990714 -0.0017974764 -0.0017972233 -0.0017972846 -0.0017972264 -0.0017970481 -0.0017967875 -0.0017965728 -0.0017965145 -0.0017965984 -0.001796824 -0.0017972036 -0.0017976129 -0.001797923 -0.0017980965][-0.001799026 -0.0017974064 -0.0017970393 -0.0017970938 -0.0017970743 -0.001796987 -0.0017968669 -0.0017967797 -0.0017968061 -0.0017969196 -0.0017971399 -0.0017974732 -0.0017977939 -0.0017979798 -0.0017980097]]...]
INFO - root - 2017-12-09 18:08:23.192120: step 46010, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.858 sec/batch; 68h:17m:27s remains)
INFO - root - 2017-12-09 18:08:31.724195: step 46020, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.870 sec/batch; 69h:15m:46s remains)
INFO - root - 2017-12-09 18:08:40.332625: step 46030, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 69h:10m:22s remains)
INFO - root - 2017-12-09 18:08:48.987939: step 46040, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 69h:10m:00s remains)
INFO - root - 2017-12-09 18:08:57.641675: step 46050, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.854 sec/batch; 67h:57m:42s remains)
INFO - root - 2017-12-09 18:09:06.135257: step 46060, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 69h:01m:03s remains)
INFO - root - 2017-12-09 18:09:14.720423: step 46070, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.880 sec/batch; 70h:01m:39s remains)
INFO - root - 2017-12-09 18:09:23.265969: step 46080, loss = 0.82, batch loss = 0.69 (10.9 examples/sec; 0.737 sec/batch; 58h:37m:16s remains)
INFO - root - 2017-12-09 18:09:31.686848: step 46090, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.840 sec/batch; 66h:49m:46s remains)
INFO - root - 2017-12-09 18:09:40.100783: step 46100, loss = 0.82, batch loss = 0.69 (10.9 examples/sec; 0.734 sec/batch; 58h:24m:39s remains)
2017-12-09 18:09:41.002978: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.13513859 0.13705239 0.1380675 0.13895814 0.14011434 0.14121512 0.14269251 0.14373221 0.14420141 0.14315964 0.13971403 0.13341831 0.12466162 0.11429226 0.10355801][0.13314664 0.13511558 0.13644613 0.1378229 0.13968696 0.14135373 0.14339475 0.14494628 0.14572784 0.14434473 0.14015034 0.13269971 0.12228644 0.11004081 0.097256556][0.1278227 0.1296197 0.13114724 0.1330518 0.13565472 0.13806966 0.1408917 0.14298849 0.14413303 0.14255504 0.1376417 0.12915002 0.1172934 0.10322751 0.088332184][0.12210361 0.12397007 0.12571822 0.12814449 0.13146333 0.13465357 0.1382277 0.14092372 0.14250576 0.140907 0.13550995 0.12603706 0.11267728 0.0966628 0.079688579][0.1165904 0.11860394 0.12058098 0.12343398 0.12729163 0.13108052 0.13530689 0.13867542 0.14081517 0.13941084 0.13382278 0.12358084 0.10886723 0.090965465 0.071936637][0.11219568 0.11446367 0.11646146 0.11939318 0.12349886 0.12759481 0.13217212 0.13594213 0.13845918 0.13724662 0.1315186 0.12071173 0.1049184 0.085638434 0.065013796][0.10878108 0.11110254 0.11268288 0.11505955 0.11856943 0.12228914 0.12675135 0.13078023 0.13376607 0.13301268 0.12756632 0.11663049 0.10031365 0.080296554 0.058821645][0.10556632 0.10786235 0.10878337 0.1101828 0.11252782 0.11523829 0.11899745 0.12268142 0.12571828 0.12541841 0.12058558 0.11009431 0.093992144 0.074131474 0.052796528][0.10157805 0.10379141 0.10389396 0.10403789 0.10492527 0.10627057 0.10883404 0.11178154 0.11457462 0.11462506 0.11051171 0.1008507 0.085625738 0.066723034 0.046505511][0.097865358 0.09956079 0.098645292 0.097441986 0.096647389 0.096264318 0.097223289 0.098921388 0.10084432 0.10072514 0.097006306 0.088219091 0.074358255 0.057233691 0.039151333][0.094588593 0.095651075 0.093568861 0.09098468 0.088514514 0.086394973 0.085400559 0.085263945 0.085611545 0.084576555 0.080670729 0.072630972 0.060474008 0.045845721 0.03072916][0.092208542 0.092199482 0.088881351 0.085105561 0.081175804 0.077474855 0.074609809 0.072508126 0.070895948 0.068376355 0.063852586 0.056372069 0.046045695 0.0341333 0.022233974][0.090009265 0.088948913 0.084774539 0.080376282 0.075748108 0.071105994 0.066708431 0.062606744 0.058716953 0.054294046 0.048708942 0.041474402 0.032804981 0.023525923 0.014732997][0.087223813 0.085407458 0.080956981 0.076538466 0.071870379 0.0669491 0.061542686 0.055807061 0.04975057 0.043280832 0.036328875 0.029007159 0.021610975 0.014574737 0.0084897615][0.083970673 0.08187665 0.077602051 0.073915705 0.069941618 0.065269276 0.059392814 0.052451894 0.044463065 0.035885606 0.027377497 0.019621387 0.012974088 0.0075996155 0.0036152932]]...]
INFO - root - 2017-12-09 18:09:49.659658: step 46110, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.890 sec/batch; 70h:46m:18s remains)
INFO - root - 2017-12-09 18:09:58.315780: step 46120, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.876 sec/batch; 69h:40m:00s remains)
INFO - root - 2017-12-09 18:10:06.842631: step 46130, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.840 sec/batch; 66h:51m:20s remains)
INFO - root - 2017-12-09 18:10:15.385372: step 46140, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.870 sec/batch; 69h:11m:37s remains)
INFO - root - 2017-12-09 18:10:24.014376: step 46150, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.852 sec/batch; 67h:44m:58s remains)
INFO - root - 2017-12-09 18:10:32.531214: step 46160, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.839 sec/batch; 66h:46m:01s remains)
INFO - root - 2017-12-09 18:10:41.204828: step 46170, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.859 sec/batch; 68h:19m:34s remains)
INFO - root - 2017-12-09 18:10:49.947068: step 46180, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.876 sec/batch; 69h:40m:00s remains)
INFO - root - 2017-12-09 18:10:58.397164: step 46190, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.874 sec/batch; 69h:30m:54s remains)
INFO - root - 2017-12-09 18:11:07.148684: step 46200, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.875 sec/batch; 69h:34m:29s remains)
2017-12-09 18:11:07.980529: I tensorflow/core/kernels/logging_ops.cc:79] [[[-3.6822166e-07 0.00015861983 0.00018158276 6.8440335e-05 -0.00016330625 -0.00039295806 -0.0005190965 -0.00047108857 -0.0002104775 0.00037187384 0.0016396388 0.0042262515 0.008389567 0.013887577 0.019805634][0.0017725534 0.0020517663 0.0020972737 0.0018798192 0.0014235633 0.00087968586 0.00043077581 0.00017753267 0.00016532792 0.00053456263 0.0016361177 0.0041156346 0.0081542125 0.013561295 0.019493993][0.0045428365 0.0050257905 0.005146041 0.0048307604 0.0040786364 0.0030888547 0.0021161959 0.0014057739 0.0010051345 0.0010568525 0.0018529198 0.0039858981 0.0077524884 0.012854124 0.018546855][0.0081140976 0.0089262361 0.0092754541 0.0089770136 0.0079733739 0.0064725857 0.0048797727 0.0035391045 0.0025639627 0.002107441 0.0024689706 0.0041414164 0.0074222255 0.012095721 0.017457178][0.012151213 0.013350583 0.013979787 0.013815288 0.012704013 0.010760148 0.0085430807 0.0065549444 0.0049347258 0.0037931511 0.0034710532 0.0044431649 0.0070301816 0.010989485 0.015829692][0.015718268 0.017383015 0.018410427 0.018511018 0.017423667 0.015188793 0.012499128 0.0098772971 0.0075465944 0.0056384 0.004510113 0.0046255686 0.0063832393 0.0095905494 0.013834771][0.017813036 0.019856447 0.021228727 0.021652281 0.02076734 0.018510077 0.015649714 0.012653466 0.0097627034 0.0071229888 0.0051958039 0.0044650631 0.0053949808 0.0078156693 0.011430459][0.017811095 0.020058947 0.021623444 0.022205723 0.021493487 0.019441003 0.016735978 0.013664148 0.010525286 0.0074646659 0.0050094733 0.0036666719 0.0040016519 0.0058269724 0.0088225929][0.015880756 0.01802353 0.01954197 0.02013493 0.019549405 0.017794615 0.01543333 0.012612644 0.0096103456 0.0065642828 0.0040067765 0.0024277349 0.0024026479 0.0037523974 0.0061383564][0.012325494 0.014086148 0.015354402 0.015872004 0.015436728 0.014068805 0.01219226 0.0098755928 0.0073783463 0.004776658 0.0025201258 0.0010744685 0.00092389621 0.0018955944 0.0036344025][0.007875382 0.0091163041 0.010034309 0.010449183 0.010209193 0.0093156016 0.00804186 0.0063875858 0.004577084 0.0026667998 0.00095943618 -0.00013452326 -0.00028582849 0.00035852706 0.001487762][0.0036140829 0.0042982311 0.0048226477 0.0051153209 0.0050494545 0.0046067284 0.0039231884 0.0029706666 0.0019147566 0.0007426762 -0.00033499638 -0.0010047543 -0.0011015483 -0.00073212094 -0.00011048163][0.00072074262 0.00095070107 0.0011215203 0.0012381428 0.0012094274 0.0010356598 0.00078190956 0.00036360673 -9.9417404e-05 -0.00066263741 -0.001193649 -0.0014949357 -0.0015248333 -0.0013536953 -0.0010760576][-0.00031580147 -0.00025103195 -0.00025888858 -0.00032113877 -0.00042686902 -0.00056441943 -0.0007153668 -0.00088299805 -0.0010314148 -0.0012774715 -0.001526867 -0.0016393658 -0.0016581949 -0.0016188687 -0.0015376777][1.5046215e-05 0.00011807401 9.7167329e-05 2.4004839e-06 -0.00015136239 -0.00037055404 -0.000579637 -0.00076983788 -0.00091047672 -0.0011452425 -0.0013830436 -0.0014948356 -0.0015544216 -0.0016047414 -0.001631855]]...]
INFO - root - 2017-12-09 18:11:16.636406: step 46210, loss = 0.83, batch loss = 0.70 (9.3 examples/sec; 0.860 sec/batch; 68h:24m:57s remains)
INFO - root - 2017-12-09 18:11:25.322852: step 46220, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 68h:33m:28s remains)
INFO - root - 2017-12-09 18:11:33.996576: step 46230, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.839 sec/batch; 66h:40m:48s remains)
INFO - root - 2017-12-09 18:11:42.717034: step 46240, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.878 sec/batch; 69h:47m:50s remains)
INFO - root - 2017-12-09 18:11:51.453750: step 46250, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.877 sec/batch; 69h:46m:23s remains)
INFO - root - 2017-12-09 18:11:59.957156: step 46260, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 68h:41m:24s remains)
INFO - root - 2017-12-09 18:12:08.552680: step 46270, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.865 sec/batch; 68h:48m:18s remains)
INFO - root - 2017-12-09 18:12:17.327914: step 46280, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.870 sec/batch; 69h:08m:23s remains)
INFO - root - 2017-12-09 18:12:25.853330: step 46290, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.882 sec/batch; 70h:07m:48s remains)
INFO - root - 2017-12-09 18:12:34.524587: step 46300, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.860 sec/batch; 68h:22m:59s remains)
2017-12-09 18:12:35.438012: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0017932949 -0.0017917084 -0.0017919334 -0.0017922698 -0.0017923588 -0.0017921438 -0.0017918494 -0.0017915131 -0.0017912738 -0.0017917826 -0.0017925738 -0.001793514 -0.0017945125 -0.0017952614 -0.0017954494][-0.0017920081 -0.0017905061 -0.0017908511 -0.0017913773 -0.0017915674 -0.0017913709 -0.0017910915 -0.0017908664 -0.001790741 -0.001791167 -0.0017916962 -0.0017922462 -0.0017927978 -0.0017932064 -0.0017930681][-0.0017919737 -0.0017908439 -0.0017915568 -0.0017925671 -0.0017930937 -0.0017932002 -0.0017931937 -0.0017932323 -0.0017932305 -0.0017933568 -0.0017934607 -0.0017934161 -0.001793201 -0.0017929594 -0.0017923351][-0.0017916716 -0.0017910983 -0.0017923445 -0.001793948 -0.0017950537 -0.0017957152 -0.0017963349 -0.0017968873 -0.0017970034 -0.0017968024 -0.001796309 -0.0017955091 -0.0017944175 -0.0017933787 -0.0017922206][-0.0017910124 -0.0017907878 -0.0017924625 -0.0017946523 -0.0017964996 -0.0017980541 -0.0017996355 -0.0018010049 -0.0018014241 -0.0018009659 -0.0017998359 -0.0017981685 -0.001796119 -0.0017942442 -0.0017926006][-0.0017903941 -0.0017904174 -0.0017924508 -0.0017952456 -0.0017978859 -0.0018003784 -0.0018029676 -0.0018051746 -0.0018058916 -0.0018051664 -0.0018035162 -0.001801052 -0.001798032 -0.0017953826 -0.0017933609][-0.0017908434 -0.0017909104 -0.0017930365 -0.0017962973 -0.0017997423 -0.0018031034 -0.0018065583 -0.0018093217 -0.0018101414 -0.0018090239 -0.0018067879 -0.0018035312 -0.0017995645 -0.0017962833 -0.0017939915][-0.001792299 -0.0017921844 -0.0017941884 -0.001797635 -0.0018017805 -0.0018060395 -0.0018101719 -0.001813238 -0.0018139952 -0.0018125281 -0.0018096106 -0.0018054234 -0.0018006136 -0.0017967853 -0.0017942274][-0.001794344 -0.001793805 -0.0017955323 -0.0017991002 -0.0018038463 -0.0018089269 -0.0018136814 -0.0018168769 -0.001817396 -0.001815535 -0.0018119313 -0.0018068202 -0.001801316 -0.0017970748 -0.0017943613][-0.0017967721 -0.0017956588 -0.0017972426 -0.0018010557 -0.0018062448 -0.0018120036 -0.0018173144 -0.0018204763 -0.0018207405 -0.0018184609 -0.0018142493 -0.0018084344 -0.0018023949 -0.001797866 -0.0017949244][-0.0017997613 -0.0017983185 -0.0017998826 -0.0018038844 -0.0018093159 -0.001815198 -0.0018205183 -0.0018234565 -0.0018234166 -0.0018207743 -0.0018161014 -0.0018099798 -0.0018037043 -0.0017990383 -0.0017958051][-0.0018036093 -0.0018021029 -0.0018036596 -0.0018076719 -0.001813025 -0.0018185319 -0.0018232478 -0.0018257059 -0.0018252574 -0.0018222362 -0.0018173053 -0.0018111768 -0.001804995 -0.0018002222 -0.0017967733][-0.0018081171 -0.0018068731 -0.0018083612 -0.0018120731 -0.0018169724 -0.001821723 -0.0018255705 -0.0018275304 -0.0018267457 -0.0018235561 -0.0018185156 -0.0018125178 -0.0018064365 -0.001801396 -0.0017977193][-0.0018124048 -0.001811631 -0.0018129725 -0.0018162233 -0.0018204362 -0.001824256 -0.0018272067 -0.0018286211 -0.0018276401 -0.0018243683 -0.0018194326 -0.0018137401 -0.0018079089 -0.0018027233 -0.0017987348][-0.0018156529 -0.0018152934 -0.0018165617 -0.0018192717 -0.0018224723 -0.001825082 -0.0018270405 -0.0018278802 -0.0018267137 -0.0018235813 -0.0018190764 -0.0018138541 -0.0018082769 -0.0018030579 -0.0017989016]]...]
INFO - root - 2017-12-09 18:12:43.862953: step 46310, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.863 sec/batch; 68h:35m:14s remains)
INFO - root - 2017-12-09 18:12:52.553777: step 46320, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.861 sec/batch; 68h:28m:11s remains)
INFO - root - 2017-12-09 18:13:01.268301: step 46330, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.870 sec/batch; 69h:10m:53s remains)
INFO - root - 2017-12-09 18:13:10.074562: step 46340, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.870 sec/batch; 69h:08m:16s remains)
INFO - root - 2017-12-09 18:13:18.768843: step 46350, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.861 sec/batch; 68h:25m:46s remains)
INFO - root - 2017-12-09 18:13:27.331538: step 46360, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.891 sec/batch; 70h:49m:54s remains)
INFO - root - 2017-12-09 18:13:35.902513: step 46370, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.834 sec/batch; 66h:16m:46s remains)
INFO - root - 2017-12-09 18:13:44.545422: step 46380, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.881 sec/batch; 69h:59m:21s remains)
INFO - root - 2017-12-09 18:13:52.956276: step 46390, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.851 sec/batch; 67h:39m:09s remains)
INFO - root - 2017-12-09 18:14:01.527677: step 46400, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 68h:40m:51s remains)
2017-12-09 18:14:02.354971: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0018148653 -0.0018149756 -0.0018172505 -0.0018200784 -0.0018227393 -0.001824895 -0.001826298 -0.0018269868 -0.0018270572 -0.0018266779 -0.0018258606 -0.0018246566 -0.0018231749 -0.0018217337 -0.0018206472][-0.0018153781 -0.0018163387 -0.0018195878 -0.0018232609 -0.0018263519 -0.0018286386 -0.0018300132 -0.0018303448 -0.0018299909 -0.0018295714 -0.0018290892 -0.0018281954 -0.0018267926 -0.0018254458 -0.0018243819][-0.0018163803 -0.0018181796 -0.0018220465 -0.0018258066 -0.0018284859 -0.0018301487 -0.0018308439 -0.0018306993 -0.0018300618 -0.0018299912 -0.0018302647 -0.0018301456 -0.0018292946 -0.001828067 -0.0018271682][-0.0018174155 -0.0018197752 -0.0018235953 -0.0018270118 -0.0018290777 -0.0018298901 -0.0018296662 -0.0018287947 -0.0018279362 -0.001828228 -0.0018293537 -0.0018300947 -0.001830037 -0.0018291833 -0.0018283935][-0.0018177143 -0.0018200628 -0.0018235454 -0.0018265405 -0.0018279926 -0.0018280573 -0.0018269331 -0.0018254309 -0.0018245012 -0.0018251671 -0.0018267806 -0.001828115 -0.0018287714 -0.0018284366 -0.0018277295][-0.0018173327 -0.0018191829 -0.0018223072 -0.0018250487 -0.0018258973 -0.001825082 -0.0018229082 -0.0018204807 -0.0018191033 -0.0018200624 -0.0018221199 -0.0018239418 -0.0018251506 -0.0018254759 -0.0018253621][-0.0018165365 -0.0018176764 -0.0018202318 -0.0018225394 -0.0018226384 -0.001820473 -0.001817102 -0.001814125 -0.0018130356 -0.0018143479 -0.0018164823 -0.001818295 -0.0018197164 -0.0018208234 -0.0018214497][-0.0018158003 -0.0018160464 -0.0018178234 -0.0018193786 -0.0018190136 -0.0018162287 -0.0018122287 -0.0018091755 -0.0018085752 -0.0018103124 -0.0018125427 -0.0018142569 -0.0018157406 -0.0018171513 -0.0018179731][-0.0018153168 -0.0018146294 -0.0018154802 -0.001816533 -0.0018162847 -0.0018139736 -0.0018105152 -0.0018078644 -0.0018075042 -0.0018091879 -0.0018111601 -0.0018126522 -0.0018139157 -0.0018149008 -0.0018153767][-0.0018147635 -0.0018135073 -0.0018137282 -0.0018143889 -0.0018143181 -0.0018130624 -0.0018110398 -0.0018094308 -0.0018093307 -0.0018103633 -0.0018116658 -0.0018126805 -0.0018133695 -0.0018136642 -0.0018135631][-0.0018142062 -0.00181269 -0.0018125082 -0.0018128534 -0.001813044 -0.0018127915 -0.0018121189 -0.0018114641 -0.0018114166 -0.0018119888 -0.0018125612 -0.0018128349 -0.0018129109 -0.001812782 -0.0018124467][-0.0018136868 -0.0018119526 -0.0018114945 -0.0018115281 -0.0018117503 -0.0018118725 -0.0018117541 -0.0018115329 -0.0018115605 -0.0018118138 -0.0018120288 -0.0018120562 -0.0018120279 -0.0018119296 -0.0018117123][-0.0018131231 -0.001811268 -0.0018107124 -0.0018105889 -0.0018106257 -0.0018107074 -0.0018107004 -0.00181073 -0.001810914 -0.0018111817 -0.001811363 -0.0018114224 -0.0018114322 -0.0018113751 -0.0018112807][-0.001812977 -0.001811029 -0.0018104389 -0.0018103044 -0.0018102534 -0.0018102353 -0.0018102694 -0.0018103777 -0.0018105865 -0.0018108293 -0.0018109622 -0.0018109924 -0.0018110124 -0.0018110395 -0.0018110513][-0.0018128772 -0.0018110274 -0.0018103942 -0.0018103255 -0.0018103237 -0.0018103202 -0.0018103509 -0.0018104468 -0.00181058 -0.0018107005 -0.0018107818 -0.0018108205 -0.0018108636 -0.0018109238 -0.0018109749]]...]
INFO - root - 2017-12-09 18:14:10.839059: step 46410, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.872 sec/batch; 69h:16m:11s remains)
INFO - root - 2017-12-09 18:14:19.450197: step 46420, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.860 sec/batch; 68h:22m:20s remains)
INFO - root - 2017-12-09 18:14:28.238259: step 46430, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.891 sec/batch; 70h:50m:16s remains)
INFO - root - 2017-12-09 18:14:36.902164: step 46440, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 69h:13m:34s remains)
INFO - root - 2017-12-09 18:14:45.634501: step 46450, loss = 0.82, batch loss = 0.69 (8.8 examples/sec; 0.909 sec/batch; 72h:13m:00s remains)
INFO - root - 2017-12-09 18:14:54.193504: step 46460, loss = 0.81, batch loss = 0.68 (9.3 examples/sec; 0.860 sec/batch; 68h:20m:18s remains)
INFO - root - 2017-12-09 18:15:02.846176: step 46470, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.883 sec/batch; 70h:09m:47s remains)
INFO - root - 2017-12-09 18:15:11.611394: step 46480, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.893 sec/batch; 70h:57m:18s remains)
INFO - root - 2017-12-09 18:15:20.148459: step 46490, loss = 0.82, batch loss = 0.70 (9.5 examples/sec; 0.839 sec/batch; 66h:41m:36s remains)
INFO - root - 2017-12-09 18:15:28.706418: step 46500, loss = 0.81, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 68h:38m:44s remains)
2017-12-09 18:15:29.539590: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0017998383 -0.0017976374 -0.0017969804 -0.0017968717 -0.0017971389 -0.0017974131 -0.0017976842 -0.0017978533 -0.0017977585 -0.0017973615 -0.0017968215 -0.0017964414 -0.0017962022 -0.0017960502 -0.0017959458][-0.001798743 -0.0017962703 -0.0017954702 -0.0017953981 -0.0017959117 -0.0017965336 -0.0017971211 -0.001797448 -0.0017973641 -0.0017967955 -0.0017960069 -0.0017954784 -0.0017951633 -0.0017949472 -0.0017948423][-0.0017989599 -0.0017963309 -0.0017954779 -0.0017955445 -0.001796336 -0.0017972315 -0.0017979557 -0.0017983033 -0.0017980682 -0.001797262 -0.0017962729 -0.001795673 -0.0017953687 -0.0017951941 -0.0017951048][-0.0017992681 -0.0017965265 -0.001795698 -0.0017959101 -0.0017969243 -0.0017980089 -0.0017987478 -0.0017990428 -0.0017986426 -0.0017976961 -0.0017966668 -0.0017961635 -0.0017960602 -0.001796095 -0.0017961045][-0.001799088 -0.0017963072 -0.001795597 -0.00179602 -0.001797171 -0.0017983301 -0.0017990459 -0.0017992826 -0.0017987929 -0.0017977804 -0.0017968644 -0.0017967348 -0.0017971173 -0.0017975911 -0.0017978394][-0.0017982997 -0.0017955685 -0.001795217 -0.0017960034 -0.001797321 -0.0017985438 -0.0017992263 -0.0017993817 -0.0017987686 -0.0017977013 -0.0017970577 -0.0017974356 -0.0017983904 -0.0017993434 -0.0017998094][-0.0017973712 -0.0017949094 -0.001795069 -0.0017963295 -0.0017978724 -0.001799086 -0.0017995872 -0.0017994916 -0.0017986579 -0.0017975911 -0.0017973761 -0.0017983073 -0.0017996882 -0.0018009536 -0.0018016202][-0.0017968401 -0.0017946885 -0.0017952909 -0.0017968874 -0.0017985057 -0.001799576 -0.0017997473 -0.0017992996 -0.0017982998 -0.0017974403 -0.0017977471 -0.001799116 -0.0018006579 -0.0018020157 -0.0018028173][-0.0017963181 -0.0017944431 -0.0017953098 -0.0017969988 -0.0017985359 -0.0017994429 -0.001799351 -0.0017986244 -0.0017976799 -0.0017971641 -0.0017978924 -0.0017994504 -0.0018009942 -0.0018023258 -0.0018032037][-0.0017955914 -0.001793919 -0.0017948373 -0.0017965112 -0.0017980116 -0.0017988473 -0.0017987264 -0.0017979542 -0.0017971612 -0.0017969831 -0.00179789 -0.0017993322 -0.001800605 -0.0018017624 -0.0018025873][-0.0017951252 -0.0017935358 -0.0017943137 -0.001795804 -0.0017971931 -0.0017980317 -0.0017980144 -0.0017973327 -0.0017967479 -0.0017968222 -0.0017976766 -0.0017987394 -0.001799598 -0.0018004166 -0.0018009912][-0.0017952591 -0.0017936029 -0.0017941715 -0.001795324 -0.0017964715 -0.0017972388 -0.0017973329 -0.0017968032 -0.0017964154 -0.001796613 -0.0017972771 -0.0017978974 -0.0017982943 -0.0017987081 -0.0017990504][-0.0017956076 -0.0017938887 -0.0017941851 -0.001794957 -0.0017957719 -0.0017963914 -0.001796576 -0.0017962604 -0.001796042 -0.0017962867 -0.0017967797 -0.0017970604 -0.0017970671 -0.0017971568 -0.0017972825][-0.0017962062 -0.0017943602 -0.0017943576 -0.0017948014 -0.001795309 -0.0017957822 -0.0017960165 -0.0017958746 -0.0017957862 -0.0017960207 -0.0017963554 -0.0017964039 -0.0017962437 -0.0017961878 -0.0017961661][-0.0017968236 -0.0017949747 -0.0017947082 -0.0017949488 -0.0017952331 -0.0017955651 -0.0017958003 -0.001795746 -0.0017957299 -0.0017959063 -0.0017961148 -0.0017960723 -0.0017958645 -0.0017957616 -0.0017956664]]...]
INFO - root - 2017-12-09 18:15:37.922447: step 46510, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.839 sec/batch; 66h:40m:58s remains)
INFO - root - 2017-12-09 18:15:46.544951: step 46520, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.876 sec/batch; 69h:34m:21s remains)
INFO - root - 2017-12-09 18:15:55.337532: step 46530, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.849 sec/batch; 67h:25m:00s remains)
INFO - root - 2017-12-09 18:16:04.075367: step 46540, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.835 sec/batch; 66h:18m:19s remains)
INFO - root - 2017-12-09 18:16:12.791156: step 46550, loss = 0.81, batch loss = 0.68 (9.0 examples/sec; 0.886 sec/batch; 70h:20m:40s remains)
INFO - root - 2017-12-09 18:16:21.255365: step 46560, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.863 sec/batch; 68h:31m:12s remains)
INFO - root - 2017-12-09 18:16:29.874182: step 46570, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 68h:59m:27s remains)
INFO - root - 2017-12-09 18:16:38.640505: step 46580, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.859 sec/batch; 68h:11m:19s remains)
INFO - root - 2017-12-09 18:16:47.143279: step 46590, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.865 sec/batch; 68h:39m:58s remains)
INFO - root - 2017-12-09 18:16:55.757042: step 46600, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.859 sec/batch; 68h:13m:20s remains)
2017-12-09 18:16:56.638197: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0018337524 -0.0018327851 -0.0018324787 -0.0018323075 -0.0018323639 -0.0018324505 -0.0018324305 -0.0018324093 -0.0018324086 -0.001832408 -0.001832436 -0.0018325866 -0.0018327105 -0.0018327741 -0.0018329315][-0.0018335491 -0.0018328632 -0.0018327862 -0.0018329312 -0.0018332908 -0.0018337099 -0.0018339768 -0.0018340673 -0.0018340626 -0.0018339878 -0.0018339766 -0.0018340597 -0.0018340937 -0.0018340927 -0.0018342769][-0.001834081 -0.0018338171 -0.0018342199 -0.0018348086 -0.0018355296 -0.0018362605 -0.0018367735 -0.0018369516 -0.0018368163 -0.001836615 -0.0018364647 -0.0018364473 -0.0018363803 -0.0018363753 -0.0018365877][-0.0018348204 -0.0018350299 -0.0018359278 -0.001836978 -0.0018380017 -0.0018389224 -0.00183956 -0.0018397609 -0.0018394615 -0.0018391238 -0.0018389539 -0.0018389456 -0.0018389043 -0.0018390378 -0.0018394232][-0.001835829 -0.0018362966 -0.0018373672 -0.0018384774 -0.0018395053 -0.0018402222 -0.0018406425 -0.001840778 -0.0018405659 -0.0018404557 -0.0018404942 -0.0018407436 -0.0018409885 -0.0018413877 -0.0018418513][-0.001836525 -0.0018370947 -0.0018382226 -0.0018390826 -0.0018395764 -0.0018393158 -0.0018389979 -0.0018388192 -0.0018389237 -0.0018394273 -0.0018401287 -0.0018409868 -0.0018418618 -0.0018427214 -0.0018432984][-0.0018369501 -0.0018375145 -0.001838462 -0.0018390185 -0.0018386608 -0.0018367827 -0.001834676 -0.0018336116 -0.0018340602 -0.0018355778 -0.001837468 -0.0018395153 -0.0018414017 -0.001842955 -0.0018437832][-0.0018370635 -0.0018375519 -0.0018383248 -0.0018387098 -0.0018378182 -0.0018347368 -0.0018304936 -0.0018273998 -0.0018275699 -0.0018300777 -0.0018331136 -0.0018364195 -0.0018395862 -0.0018421999 -0.001843476][-0.0018369547 -0.0018372817 -0.0018380557 -0.0018384643 -0.001837807 -0.0018352001 -0.0018306933 -0.001825868 -0.0018240029 -0.0018258654 -0.0018291138 -0.0018330318 -0.0018370433 -0.0018405202 -0.0018423357][-0.0018363715 -0.0018366156 -0.0018373772 -0.001837943 -0.0018379424 -0.0018367004 -0.0018337675 -0.0018294774 -0.0018263881 -0.0018264187 -0.0018282662 -0.0018313577 -0.0018350033 -0.0018382572 -0.0018400507][-0.0018356703 -0.0018356927 -0.0018362417 -0.0018368291 -0.0018372759 -0.0018371005 -0.0018358929 -0.0018336795 -0.0018317724 -0.0018310118 -0.0018312861 -0.001832515 -0.0018343303 -0.0018363515 -0.0018377008][-0.001835038 -0.0018347597 -0.0018350279 -0.0018355108 -0.0018360317 -0.0018363898 -0.0018363893 -0.0018358161 -0.0018352225 -0.0018347471 -0.0018344448 -0.001834381 -0.0018346012 -0.0018352653 -0.0018358482][-0.0018344248 -0.0018339241 -0.0018340081 -0.0018344283 -0.0018349474 -0.0018355042 -0.0018359859 -0.0018361338 -0.0018361333 -0.0018359418 -0.0018356045 -0.0018351185 -0.0018345808 -0.0018343973 -0.0018343957][-0.0018338964 -0.0018333002 -0.0018332773 -0.0018335496 -0.0018339889 -0.0018346047 -0.0018351942 -0.0018355497 -0.0018357094 -0.0018356036 -0.0018352451 -0.0018346522 -0.0018339565 -0.0018334935 -0.001833254][-0.0018335168 -0.001832851 -0.0018327077 -0.0018328881 -0.0018331475 -0.0018335483 -0.0018339861 -0.0018342863 -0.0018344619 -0.0018343775 -0.0018341114 -0.0018336879 -0.0018331555 -0.0018327386 -0.0018324631]]...]
INFO - root - 2017-12-09 18:17:05.017548: step 46610, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.859 sec/batch; 68h:11m:20s remains)
INFO - root - 2017-12-09 18:17:13.609166: step 46620, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.890 sec/batch; 70h:42m:02s remains)
INFO - root - 2017-12-09 18:17:22.337232: step 46630, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.896 sec/batch; 71h:07m:41s remains)
INFO - root - 2017-12-09 18:17:31.128211: step 46640, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.855 sec/batch; 67h:53m:26s remains)
INFO - root - 2017-12-09 18:17:39.887204: step 46650, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.833 sec/batch; 66h:08m:56s remains)
INFO - root - 2017-12-09 18:17:48.496957: step 46660, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.870 sec/batch; 69h:05m:31s remains)
INFO - root - 2017-12-09 18:17:57.230558: step 46670, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 68h:54m:49s remains)
INFO - root - 2017-12-09 18:18:05.893011: step 46680, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.878 sec/batch; 69h:40m:29s remains)
INFO - root - 2017-12-09 18:18:14.391485: step 46690, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.872 sec/batch; 69h:14m:01s remains)
INFO - root - 2017-12-09 18:18:23.132558: step 46700, loss = 0.83, batch loss = 0.70 (9.0 examples/sec; 0.888 sec/batch; 70h:30m:01s remains)
2017-12-09 18:18:24.013943: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.001816296 -0.0018160301 -0.0018167367 -0.0018169702 -0.0018167637 -0.0018165064 -0.0018159696 -0.0018152646 -0.0018146553 -0.0018143798 -0.0018141437 -0.0018133562 -0.0018123104 -0.0018114154 -0.0018108885][-0.0018163663 -0.0018164458 -0.0018176339 -0.0018185228 -0.0018191519 -0.0018196338 -0.0018195459 -0.0018188661 -0.0018179378 -0.0018171 -0.0018160822 -0.0018145825 -0.0018130525 -0.0018118036 -0.0018110737][-0.0018170001 -0.0018174567 -0.0018192638 -0.0018211048 -0.0018228376 -0.001824313 -0.0018248498 -0.0018243116 -0.0018231024 -0.0018215979 -0.0018196217 -0.0018172202 -0.0018151207 -0.0018135527 -0.0018125185][-0.0018179135 -0.0018187383 -0.0018211069 -0.0018237926 -0.0018264489 -0.0018288785 -0.0018301203 -0.0018298319 -0.0018285209 -0.0018266338 -0.0018240244 -0.0018209066 -0.0018183151 -0.0018163288 -0.0018147739][-0.0018191772 -0.001820271 -0.0018229103 -0.0018259462 -0.0018290504 -0.0018319688 -0.0018337248 -0.0018338173 -0.0018326752 -0.0018308036 -0.0018282743 -0.0018250308 -0.0018221034 -0.0018196454 -0.0018174909][-0.0018213454 -0.0018224504 -0.0018250113 -0.0018278336 -0.0018309082 -0.0018339989 -0.0018360652 -0.0018363909 -0.0018354709 -0.00183379 -0.0018315319 -0.0018285291 -0.0018256406 -0.0018229233 -0.001820275][-0.0018232861 -0.0018243331 -0.0018266669 -0.001829131 -0.0018319017 -0.0018349655 -0.0018372106 -0.0018377181 -0.0018369117 -0.0018354205 -0.0018334166 -0.0018307416 -0.0018279509 -0.0018250974 -0.001822155][-0.0018241644 -0.0018250889 -0.0018272017 -0.0018294874 -0.0018319419 -0.0018348178 -0.0018373245 -0.0018380703 -0.001837216 -0.0018356005 -0.001833747 -0.0018313731 -0.001828645 -0.0018256878 -0.0018225833][-0.0018236139 -0.0018245046 -0.0018262977 -0.0018282523 -0.0018303136 -0.0018327622 -0.0018352387 -0.0018363646 -0.0018357927 -0.0018340133 -0.0018321527 -0.0018301064 -0.0018275948 -0.0018247042 -0.0018216691][-0.0018220691 -0.0018228295 -0.0018243066 -0.0018259021 -0.0018274913 -0.0018293001 -0.0018312896 -0.0018324386 -0.0018321855 -0.0018306519 -0.001829045 -0.0018272158 -0.0018250677 -0.0018224536 -0.0018198169][-0.0018205151 -0.0018209296 -0.0018221057 -0.0018233652 -0.0018245783 -0.0018257367 -0.001826893 -0.0018276597 -0.0018275535 -0.0018265103 -0.001825242 -0.0018237915 -0.0018219887 -0.0018197859 -0.0018177015][-0.0018191813 -0.001819135 -0.0018200245 -0.0018210802 -0.0018220843 -0.0018227851 -0.0018233465 -0.0018237494 -0.0018236246 -0.0018229048 -0.0018218752 -0.001820626 -0.0018190432 -0.0018172055 -0.0018156336][-0.0018177583 -0.0018175775 -0.0018183448 -0.0018193827 -0.0018202208 -0.0018206675 -0.001820828 -0.0018207866 -0.0018205137 -0.0018199212 -0.0018189092 -0.0018176037 -0.0018161768 -0.0018147331 -0.001813653][-0.001816835 -0.0018167888 -0.0018176972 -0.0018187951 -0.0018195638 -0.0018198818 -0.001819726 -0.0018190646 -0.0018182922 -0.0018175648 -0.0018165563 -0.0018152684 -0.001813993 -0.0018127979 -0.0018121239][-0.0018169186 -0.0018172936 -0.0018185776 -0.0018199307 -0.0018208239 -0.0018210585 -0.0018204754 -0.0018191558 -0.0018176511 -0.0018164612 -0.001815151 -0.0018137395 -0.0018125267 -0.0018115976 -0.0018113028]]...]
INFO - root - 2017-12-09 18:18:32.505941: step 46710, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.848 sec/batch; 67h:19m:59s remains)
INFO - root - 2017-12-09 18:18:41.237470: step 46720, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.872 sec/batch; 69h:13m:20s remains)
INFO - root - 2017-12-09 18:18:49.810653: step 46730, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.838 sec/batch; 66h:32m:11s remains)
INFO - root - 2017-12-09 18:18:58.460338: step 46740, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.886 sec/batch; 70h:20m:43s remains)
INFO - root - 2017-12-09 18:19:07.169128: step 46750, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.852 sec/batch; 67h:38m:52s remains)
INFO - root - 2017-12-09 18:19:15.840727: step 46760, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.870 sec/batch; 69h:02m:23s remains)
INFO - root - 2017-12-09 18:19:24.533194: step 46770, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 68h:36m:15s remains)
INFO - root - 2017-12-09 18:19:33.188215: step 46780, loss = 0.83, batch loss = 0.70 (9.1 examples/sec; 0.876 sec/batch; 69h:30m:07s remains)
INFO - root - 2017-12-09 18:19:41.889902: step 46790, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.894 sec/batch; 70h:57m:27s remains)
INFO - root - 2017-12-09 18:19:50.663355: step 46800, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.832 sec/batch; 66h:01m:36s remains)
2017-12-09 18:19:51.578571: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0018338632 -0.0018324677 -0.0018312111 -0.0018307999 -0.0018311093 -0.0018316855 -0.0018322677 -0.0018329031 -0.0018335556 -0.001833848 -0.0018336953 -0.0018330545 -0.001832069 -0.0018308575 -0.0018297677][-0.0018357458 -0.0018342166 -0.0018327824 -0.0018320974 -0.0018321967 -0.0018326422 -0.0018332512 -0.0018339825 -0.0018347286 -0.0018350378 -0.0018348137 -0.0018340393 -0.0018327867 -0.0018312492 -0.0018299411][-0.0018375546 -0.001835963 -0.0018344374 -0.0018336087 -0.0018335654 -0.0018338584 -0.0018343423 -0.0018349669 -0.001835688 -0.001835992 -0.0018357672 -0.0018349533 -0.0018336079 -0.0018319396 -0.0018304976][-0.0018385221 -0.0018369791 -0.0018355305 -0.0018346547 -0.0018345274 -0.0018346354 -0.0018348662 -0.0018351319 -0.0018355512 -0.0018357773 -0.0018355791 -0.0018349082 -0.0018337888 -0.0018323553 -0.0018310328][-0.0018385155 -0.001836907 -0.0018355623 -0.0018347546 -0.0018345058 -0.0018343159 -0.0018342413 -0.0018342137 -0.0018343242 -0.0018344314 -0.0018343427 -0.0018339957 -0.0018333126 -0.0018323592 -0.0018313762][-0.0018377363 -0.0018359172 -0.0018346417 -0.0018338669 -0.0018335325 -0.0018331598 -0.0018328801 -0.0018326651 -0.0018325337 -0.0018325073 -0.0018325046 -0.0018324485 -0.0018322194 -0.0018318072 -0.0018313034][-0.0018366106 -0.0018344385 -0.001833064 -0.0018323021 -0.0018320417 -0.0018317192 -0.0018314497 -0.0018312177 -0.0018309347 -0.001830722 -0.00183064 -0.0018307487 -0.0018308714 -0.0018309471 -0.0018309061][-0.0018356888 -0.0018331794 -0.0018315692 -0.0018307505 -0.0018305788 -0.0018304342 -0.0018303405 -0.0018302248 -0.0018299356 -0.0018296198 -0.0018293769 -0.0018294071 -0.0018297168 -0.001830106 -0.0018303443][-0.0018352701 -0.0018324278 -0.0018304902 -0.0018294198 -0.00182915 -0.0018291613 -0.0018292888 -0.0018294534 -0.0018293989 -0.0018291306 -0.0018287732 -0.0018286617 -0.0018289536 -0.0018294491 -0.0018298212][-0.0018351106 -0.0018321881 -0.0018299019 -0.0018284464 -0.0018279419 -0.0018280174 -0.0018283217 -0.0018287767 -0.0018290605 -0.0018290182 -0.0018287442 -0.0018285721 -0.0018287997 -0.0018292615 -0.0018297245][-0.0018352547 -0.0018325201 -0.0018300909 -0.0018282545 -0.0018274189 -0.0018273731 -0.0018276738 -0.0018282749 -0.0018288826 -0.0018292291 -0.0018292674 -0.0018292235 -0.0018294173 -0.0018298001 -0.0018302001][-0.0018358178 -0.0018333065 -0.0018309021 -0.0018287757 -0.001827591 -0.001827297 -0.001827489 -0.0018280827 -0.0018288685 -0.001829596 -0.0018300967 -0.0018304493 -0.0018308023 -0.0018311745 -0.0018314576][-0.0018364821 -0.0018342064 -0.001831834 -0.0018294923 -0.0018279472 -0.0018273933 -0.0018274459 -0.0018280391 -0.0018289745 -0.0018300577 -0.0018310515 -0.0018319458 -0.0018327134 -0.0018332312 -0.0018334071][-0.0018373471 -0.0018352484 -0.0018329383 -0.0018304291 -0.0018285728 -0.0018277362 -0.0018275839 -0.0018280608 -0.0018290621 -0.0018304434 -0.0018319559 -0.0018334581 -0.0018347908 -0.0018356704 -0.0018359559][-0.0018380311 -0.001836061 -0.0018338955 -0.0018313946 -0.0018293412 -0.0018282105 -0.0018277612 -0.0018279541 -0.0018287906 -0.0018302636 -0.0018320832 -0.001834057 -0.0018359319 -0.0018372774 -0.001837814]]...]
INFO - root - 2017-12-09 18:20:00.215302: step 46810, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.894 sec/batch; 70h:58m:30s remains)
INFO - root - 2017-12-09 18:20:08.897751: step 46820, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.873 sec/batch; 69h:15m:11s remains)
INFO - root - 2017-12-09 18:20:17.574493: step 46830, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.877 sec/batch; 69h:33m:48s remains)
INFO - root - 2017-12-09 18:20:26.143961: step 46840, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.855 sec/batch; 67h:52m:06s remains)
INFO - root - 2017-12-09 18:20:34.756219: step 46850, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.897 sec/batch; 71h:12m:14s remains)
INFO - root - 2017-12-09 18:20:43.228478: step 46860, loss = 0.83, batch loss = 0.70 (9.4 examples/sec; 0.848 sec/batch; 67h:17m:05s remains)
INFO - root - 2017-12-09 18:20:51.865318: step 46870, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.858 sec/batch; 68h:06m:23s remains)
INFO - root - 2017-12-09 18:21:00.595116: step 46880, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.882 sec/batch; 69h:56m:36s remains)
INFO - root - 2017-12-09 18:21:09.155636: step 46890, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.863 sec/batch; 68h:27m:15s remains)
INFO - root - 2017-12-09 18:21:17.738099: step 46900, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.857 sec/batch; 68h:01m:27s remains)
2017-12-09 18:21:18.554052: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0018002826 -0.0017999033 -0.0018007107 -0.0018020109 -0.0018029669 -0.0018031223 -0.0018024518 -0.0018012226 -0.0017996273 -0.0017978221 -0.0017962876 -0.001795313 -0.0017948699 -0.001795043 -0.0017960153][-0.0018055037 -0.0018054261 -0.00180643 -0.0018077513 -0.001808619 -0.0018086835 -0.0018079809 -0.0018066261 -0.0018047066 -0.00180241 -0.0018001476 -0.0017982514 -0.0017967721 -0.00179587 -0.0017957799][-0.0018136755 -0.0018134068 -0.0018139716 -0.0018146405 -0.0018150123 -0.0018149391 -0.0018144493 -0.0018134508 -0.0018118325 -0.0018096267 -0.0018069276 -0.0018040468 -0.001801318 -0.0017989636 -0.0017973991][-0.0018214834 -0.0018205807 -0.001820282 -0.0018201729 -0.00182003 -0.0018198983 -0.001819766 -0.0018194818 -0.0018186268 -0.0018169454 -0.0018142019 -0.0018106158 -0.0018067291 -0.0018029201 -0.001799795][-0.0018279308 -0.0018265556 -0.0018256474 -0.0018250826 -0.0018247217 -0.0018246073 -0.0018247849 -0.0018249959 -0.001824686 -0.0018233438 -0.0018205686 -0.0018164846 -0.0018117197 -0.0018068135 -0.0018024412][-0.0018313373 -0.0018298829 -0.0018287196 -0.0018279549 -0.0018275096 -0.0018276277 -0.0018281847 -0.0018289115 -0.0018291133 -0.0018281083 -0.0018253479 -0.0018209949 -0.0018156575 -0.0018100373 -0.0018048277][-0.0018301401 -0.0018286975 -0.001827596 -0.0018268978 -0.0018265055 -0.0018267832 -0.001827752 -0.001828909 -0.0018296519 -0.00182911 -0.0018268726 -0.0018228733 -0.0018175972 -0.0018118365 -0.0018062353][-0.0018244522 -0.0018229386 -0.0018218326 -0.0018212418 -0.0018211425 -0.0018216381 -0.0018228876 -0.0018245053 -0.0018258042 -0.0018258515 -0.0018243702 -0.0018214056 -0.0018169591 -0.001811699 -0.0018063014][-0.0018170096 -0.0018156758 -0.0018150338 -0.0018149103 -0.0018151397 -0.0018157836 -0.0018169603 -0.0018182941 -0.0018193374 -0.001819487 -0.001818608 -0.001816654 -0.0018133745 -0.0018092368 -0.0018047927][-0.0018108865 -0.0018097492 -0.0018097534 -0.0018101009 -0.0018105556 -0.0018110955 -0.0018117811 -0.0018124008 -0.0018127845 -0.0018126681 -0.0018120351 -0.0018107081 -0.0018084147 -0.0018055101 -0.0018023646][-0.0018060864 -0.0018050775 -0.0018053312 -0.001805815 -0.0018062113 -0.0018064536 -0.0018066958 -0.001806792 -0.0018068405 -0.0018066529 -0.0018061352 -0.0018052183 -0.0018037113 -0.0018019101 -0.0017999469][-0.0018050048 -0.0018037586 -0.0018037014 -0.0018037726 -0.0018037625 -0.0018035708 -0.0018033786 -0.0018030965 -0.0018028732 -0.0018025519 -0.001802088 -0.0018014733 -0.0018005674 -0.0017995047 -0.0017982976][-0.0018083131 -0.001806993 -0.0018064666 -0.0018059303 -0.0018053178 -0.0018046336 -0.0018039431 -0.0018032928 -0.0018026661 -0.0018019057 -0.0018010509 -0.0018002595 -0.0017994066 -0.0017985019 -0.0017976204][-0.001813819 -0.0018127101 -0.0018121355 -0.0018115102 -0.0018108559 -0.0018100429 -0.001809079 -0.0018079876 -0.0018066905 -0.0018051356 -0.0018034552 -0.0018019215 -0.0018005652 -0.0017993541 -0.0017983643][-0.0018198377 -0.0018189353 -0.0018184374 -0.0018179949 -0.0018175945 -0.0018170011 -0.0018160989 -0.0018149199 -0.0018132695 -0.001811123 -0.0018086644 -0.0018062663 -0.0018041085 -0.0018022631 -0.0018009139]]...]
INFO - root - 2017-12-09 18:21:26.988146: step 46910, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 68h:21m:59s remains)
INFO - root - 2017-12-09 18:21:35.692001: step 46920, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.870 sec/batch; 69h:03m:09s remains)
INFO - root - 2017-12-09 18:21:44.217442: step 46930, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.847 sec/batch; 67h:09m:37s remains)
INFO - root - 2017-12-09 18:21:52.785725: step 46940, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.857 sec/batch; 68h:00m:47s remains)
INFO - root - 2017-12-09 18:22:01.313913: step 46950, loss = 0.82, batch loss = 0.69 (8.8 examples/sec; 0.905 sec/batch; 71h:47m:27s remains)
INFO - root - 2017-12-09 18:22:09.890374: step 46960, loss = 0.82, batch loss = 0.69 (8.8 examples/sec; 0.905 sec/batch; 71h:45m:13s remains)
INFO - root - 2017-12-09 18:22:18.529138: step 46970, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.859 sec/batch; 68h:08m:58s remains)
INFO - root - 2017-12-09 18:22:27.189751: step 46980, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.883 sec/batch; 70h:02m:02s remains)
INFO - root - 2017-12-09 18:22:35.654184: step 46990, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.875 sec/batch; 69h:24m:24s remains)
INFO - root - 2017-12-09 18:22:44.257335: step 47000, loss = 0.83, batch loss = 0.70 (9.4 examples/sec; 0.848 sec/batch; 67h:12m:55s remains)
2017-12-09 18:22:45.118585: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.0049954578 0.0041602389 0.0039759595 0.0039299065 0.00496162 0.00683854 0.0095223291 0.012273349 0.015125939 0.017419215 0.019827882 0.02241653 0.025774503 0.029910617 0.034504686][0.0057624313 0.0046661547 0.0040754355 0.00392592 0.0051358449 0.0070026596 0.009736672 0.012189652 0.01478389 0.017321292 0.020112826 0.022898188 0.026403408 0.030368537 0.034218721][0.0065472783 0.0056378893 0.0049210461 0.0045938431 0.0055860146 0.0075182361 0.0098426044 0.01189135 0.014307882 0.017596053 0.021818068 0.02661355 0.031903561 0.036453962 0.039970063][0.00847774 0.0082166037 0.0077813626 0.0075128176 0.0079964912 0.0092280507 0.010750511 0.01251013 0.014866418 0.019584602 0.02622194 0.034104548 0.042190529 0.048362575 0.051984958][0.011118495 0.011877963 0.012176935 0.012055536 0.011994882 0.012231946 0.012990952 0.014821307 0.01795215 0.024847219 0.0343062 0.045850452 0.056531068 0.06418328 0.067781106][0.014802667 0.016886843 0.017938379 0.018116495 0.017761273 0.017209953 0.017325353 0.019835634 0.024881152 0.034097169 0.045873966 0.05993608 0.072377764 0.081029721 0.084851034][0.01978654 0.023834353 0.026338967 0.026773177 0.025955977 0.024778808 0.024543801 0.027539013 0.033804659 0.045201808 0.058590841 0.073879242 0.086692862 0.095735319 0.10026024][0.026817834 0.032855511 0.036821339 0.037640873 0.036058508 0.03380309 0.032808997 0.035953265 0.042269222 0.054134436 0.068048142 0.083750881 0.09620139 0.10550346 0.11037818][0.035928853 0.043239634 0.047948509 0.048747662 0.046230432 0.04269607 0.040428493 0.042414129 0.047814105 0.05875257 0.071743786 0.087202445 0.09961047 0.10942023 0.11499836][0.045488127 0.053349689 0.057555087 0.057596166 0.053795196 0.048645649 0.044824261 0.045244396 0.049018186 0.058340412 0.070252962 0.085297249 0.098041728 0.10871041 0.11508723][0.054464571 0.061212379 0.063466296 0.06187154 0.05635906 0.049714144 0.044472668 0.043352827 0.045661129 0.053853843 0.065294109 0.080213211 0.094059736 0.10553551 0.11240319][0.061648559 0.065831169 0.065560356 0.061706189 0.054554082 0.046539113 0.040467456 0.038681928 0.040868293 0.0489796 0.060615543 0.0759801 0.0905521 0.10293841 0.10985063][0.066350728 0.067411147 0.06396801 0.057845656 0.049567144 0.040960208 0.034913193 0.033209752 0.036366832 0.04535003 0.057981845 0.073767848 0.088708185 0.10135382 0.10797564][0.068039067 0.06610065 0.059952181 0.0518569 0.043218229 0.034680966 0.029214699 0.028545229 0.033472132 0.043783795 0.057627667 0.073696829 0.088275075 0.1003177 0.10665827][0.06680534 0.062166873 0.053798452 0.044261467 0.035251584 0.027352935 0.023373712 0.024434026 0.031400248 0.043131169 0.057925038 0.07401292 0.087890409 0.098933078 0.10438662]]...]
INFO - root - 2017-12-09 18:22:53.586640: step 47010, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.861 sec/batch; 68h:16m:57s remains)
INFO - root - 2017-12-09 18:23:02.291891: step 47020, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.863 sec/batch; 68h:27m:12s remains)
INFO - root - 2017-12-09 18:23:11.047901: step 47030, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.886 sec/batch; 70h:16m:05s remains)
INFO - root - 2017-12-09 18:23:19.760663: step 47040, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.866 sec/batch; 68h:40m:52s remains)
INFO - root - 2017-12-09 18:23:28.609535: step 47050, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.889 sec/batch; 70h:29m:05s remains)
INFO - root - 2017-12-09 18:23:37.197119: step 47060, loss = 0.82, batch loss = 0.69 (8.5 examples/sec; 0.944 sec/batch; 74h:52m:57s remains)
INFO - root - 2017-12-09 18:23:45.946017: step 47070, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 68h:22m:44s remains)
INFO - root - 2017-12-09 18:23:54.545907: step 47080, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 68h:18m:49s remains)
INFO - root - 2017-12-09 18:24:02.950431: step 47090, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.843 sec/batch; 66h:51m:26s remains)
INFO - root - 2017-12-09 18:24:11.461784: step 47100, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.839 sec/batch; 66h:30m:30s remains)
2017-12-09 18:24:12.353834: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.3417021 0.33611298 0.33106911 0.32499903 0.31774476 0.31114492 0.30548614 0.30023226 0.29320374 0.2866444 0.28048643 0.27507523 0.26912743 0.26330161 0.25969777][0.34117475 0.33801538 0.33499885 0.33072835 0.32474008 0.31934839 0.31444076 0.31007779 0.30434611 0.29900464 0.29294178 0.2864725 0.27913243 0.27186108 0.26609677][0.33679205 0.33738571 0.33735934 0.33542472 0.33181766 0.32805514 0.32381678 0.31976172 0.31461334 0.309929 0.30376509 0.29621789 0.2873455 0.27813274 0.26994911][0.33420369 0.33828881 0.34164414 0.34274381 0.34233755 0.34042123 0.33774781 0.33411309 0.32920358 0.32468233 0.31786442 0.30937308 0.2988832 0.28738517 0.27645865][0.33249366 0.33910367 0.34503281 0.34955677 0.35257125 0.3528026 0.352014 0.34955794 0.34524629 0.34055394 0.33290511 0.32354596 0.31147003 0.29732582 0.28362805][0.33236858 0.34072566 0.34777862 0.35467556 0.3601664 0.36311874 0.36503097 0.36414102 0.36104849 0.35664949 0.34855074 0.33735684 0.32314119 0.30656418 0.29006043][0.33276442 0.34270737 0.35038838 0.358486 0.36557683 0.37077138 0.37458286 0.37553442 0.37361079 0.36933139 0.36136928 0.34919724 0.33362854 0.31496722 0.29651386][0.33233529 0.34310257 0.35055831 0.35841495 0.36617911 0.37228367 0.37709188 0.37900361 0.37750745 0.37320074 0.36528334 0.35323322 0.33740404 0.31766334 0.29846817][0.32904673 0.33958539 0.3455092 0.35300717 0.36080018 0.3669664 0.37216428 0.374777 0.37420577 0.37012631 0.36232388 0.350867 0.33517587 0.3157919 0.2968052][0.32254726 0.33243123 0.33634344 0.34217045 0.34855166 0.35407242 0.35870326 0.36161149 0.36157498 0.3581728 0.35134581 0.34106937 0.32639873 0.30852035 0.29114205][0.3102757 0.31925058 0.32100448 0.32463193 0.32913721 0.33451822 0.33914888 0.34242696 0.34264117 0.33989784 0.334098 0.32486671 0.3120864 0.29692292 0.28260303][0.29588816 0.30417055 0.30466586 0.30613282 0.30857691 0.31241897 0.31588718 0.31909749 0.31930974 0.31734866 0.31298935 0.30621451 0.29675111 0.28480884 0.27399209][0.28110215 0.28810927 0.28720409 0.28735533 0.28849685 0.29066071 0.29300675 0.29539359 0.29544985 0.29388565 0.29065368 0.28616169 0.27973819 0.27155492 0.26417235][0.2669656 0.272655 0.27044195 0.26960945 0.2696788 0.27105856 0.27311826 0.27515444 0.27549824 0.27462494 0.27270204 0.26986557 0.26576841 0.26078671 0.25627905][0.25225022 0.25710493 0.254347 0.25289607 0.25227118 0.25328228 0.25515428 0.2572518 0.25792608 0.25776729 0.25703087 0.25550348 0.25364795 0.251023 0.24881662]]...]
INFO - root - 2017-12-09 18:24:20.761058: step 47110, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.873 sec/batch; 69h:12m:51s remains)
INFO - root - 2017-12-09 18:24:29.402309: step 47120, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.860 sec/batch; 68h:11m:30s remains)
INFO - root - 2017-12-09 18:24:37.873274: step 47130, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.832 sec/batch; 65h:59m:20s remains)
INFO - root - 2017-12-09 18:24:46.564134: step 47140, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.890 sec/batch; 70h:34m:35s remains)
INFO - root - 2017-12-09 18:24:55.137326: step 47150, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.853 sec/batch; 67h:36m:21s remains)
INFO - root - 2017-12-09 18:25:03.575238: step 47160, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.855 sec/batch; 67h:46m:54s remains)
INFO - root - 2017-12-09 18:25:12.225823: step 47170, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.865 sec/batch; 68h:31m:53s remains)
INFO - root - 2017-12-09 18:25:20.958920: step 47180, loss = 0.83, batch loss = 0.70 (8.9 examples/sec; 0.897 sec/batch; 71h:06m:44s remains)
INFO - root - 2017-12-09 18:25:29.477505: step 47190, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.875 sec/batch; 69h:22m:18s remains)
INFO - root - 2017-12-09 18:25:38.164126: step 47200, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.837 sec/batch; 66h:19m:28s remains)
2017-12-09 18:25:39.081235: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.19933638 0.19843635 0.19603407 0.1920172 0.187038 0.18280628 0.17952353 0.17767736 0.17680109 0.17689145 0.17600524 0.17489141 0.17174315 0.16742575 0.16269788][0.2062901 0.20525491 0.20156696 0.19634753 0.19043675 0.18591046 0.18242028 0.18087032 0.18143713 0.18329233 0.18476564 0.18450347 0.18204036 0.1783002 0.17362934][0.21104869 0.20866379 0.20363411 0.196989 0.19029564 0.18459716 0.180183 0.17863128 0.1799252 0.18382126 0.1873754 0.18889412 0.18787742 0.18544748 0.18163714][0.21398509 0.21020651 0.20335442 0.19448882 0.18629706 0.17923284 0.17359111 0.17162892 0.17369664 0.17989537 0.18613604 0.19012928 0.19118735 0.19063345 0.18826085][0.21468842 0.20879787 0.19923553 0.18789846 0.17817488 0.16956946 0.16290545 0.16081032 0.16403341 0.1719538 0.18065596 0.18786955 0.19130322 0.19316852 0.19253221][0.21523616 0.20696849 0.19448659 0.18027201 0.16811869 0.15770105 0.15037185 0.14855677 0.15307841 0.16320902 0.17442802 0.18418196 0.19025803 0.19437817 0.1953952][0.21649155 0.20611864 0.19045451 0.17314354 0.15824756 0.14560941 0.13760239 0.13642856 0.14300328 0.15572637 0.16967168 0.18227696 0.19046521 0.19632506 0.19826004][0.21665779 0.20598304 0.18840611 0.16837478 0.1505976 0.13615504 0.12779048 0.12698683 0.13506246 0.14991319 0.16597566 0.18091378 0.19094592 0.19756448 0.19959247][0.2161682 0.20593265 0.18757996 0.16640991 0.1469045 0.131174 0.12272901 0.12233722 0.1316787 0.14782763 0.16517267 0.18143329 0.19183412 0.1985403 0.20007846][0.21495572 0.20586896 0.18794423 0.1674093 0.14792067 0.13152987 0.12311769 0.12328655 0.13323504 0.14937156 0.16660172 0.18273437 0.19270793 0.19867533 0.1992189][0.21198124 0.20424587 0.187852 0.16929507 0.15126798 0.13659433 0.12866232 0.12911424 0.13868654 0.15355274 0.16940191 0.18368694 0.19261277 0.1975373 0.19707841][0.20670068 0.20079046 0.186292 0.17003486 0.1541546 0.14156932 0.13477954 0.13581112 0.1447432 0.15748346 0.17145017 0.1837737 0.19135138 0.19507512 0.19374371][0.20001695 0.19591472 0.18399572 0.17067018 0.15732735 0.14694858 0.14129117 0.14215368 0.14954647 0.15990451 0.17127296 0.18129611 0.18759555 0.19050574 0.18887821][0.19381246 0.19051242 0.1802865 0.1693652 0.15849692 0.14979202 0.1450837 0.14612433 0.15222767 0.16046098 0.16961831 0.17767444 0.18284538 0.18532678 0.18386143][0.18551052 0.18361628 0.17534664 0.16654888 0.157733 0.15085566 0.14689228 0.14778098 0.15251121 0.15882212 0.16600132 0.17240039 0.17685142 0.17894459 0.17773095]]...]
INFO - root - 2017-12-09 18:25:47.635318: step 47210, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.843 sec/batch; 66h:46m:30s remains)
INFO - root - 2017-12-09 18:25:56.300317: step 47220, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.870 sec/batch; 68h:57m:21s remains)
INFO - root - 2017-12-09 18:26:04.877748: step 47230, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.845 sec/batch; 66h:58m:45s remains)
INFO - root - 2017-12-09 18:26:13.411393: step 47240, loss = 0.82, batch loss = 0.69 (9.7 examples/sec; 0.823 sec/batch; 65h:13m:10s remains)
INFO - root - 2017-12-09 18:26:22.015140: step 47250, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.886 sec/batch; 70h:10m:07s remains)
INFO - root - 2017-12-09 18:26:30.537070: step 47260, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.870 sec/batch; 68h:53m:43s remains)
INFO - root - 2017-12-09 18:26:39.183886: step 47270, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 68h:53m:11s remains)
INFO - root - 2017-12-09 18:26:47.882532: step 47280, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.880 sec/batch; 69h:41m:10s remains)
INFO - root - 2017-12-09 18:26:56.416159: step 47290, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.878 sec/batch; 69h:32m:32s remains)
INFO - root - 2017-12-09 18:27:05.135935: step 47300, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 68h:44m:26s remains)
2017-12-09 18:27:06.028179: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0018183469 -0.001817974 -0.0018182471 -0.0018186794 -0.0018189913 -0.0018190367 -0.0018188561 -0.0018185608 -0.0018181815 -0.0018177051 -0.0018172235 -0.0018168833 -0.0018166787 -0.001816584 -0.0018165479][-0.001818441 -0.0018184817 -0.0018191847 -0.0018200956 -0.0018208686 -0.0018211963 -0.0018210381 -0.0018205782 -0.0018198604 -0.0018188804 -0.0018178374 -0.0018170172 -0.001816522 -0.0018163015 -0.0018162127][-0.0018189156 -0.0018195368 -0.0018207914 -0.0018222831 -0.0018236088 -0.0018243236 -0.0018242357 -0.0018235651 -0.0018224749 -0.0018210231 -0.0018194229 -0.0018180351 -0.0018171081 -0.0018166262 -0.0018163997][-0.0018194508 -0.0018206228 -0.0018223225 -0.0018243439 -0.0018261845 -0.0018273192 -0.0018274762 -0.001826735 -0.0018252593 -0.0018233293 -0.0018212496 -0.0018193685 -0.0018179653 -0.0018171276 -0.0018166801][-0.0018202151 -0.0018218725 -0.0018239379 -0.0018262446 -0.0018284695 -0.0018299746 -0.0018302156 -0.0018292924 -0.0018275189 -0.0018252205 -0.0018227465 -0.0018204863 -0.0018187525 -0.0018176616 -0.0018170062][-0.0018206619 -0.0018227218 -0.0018251685 -0.001827674 -0.0018300787 -0.0018316084 -0.0018317384 -0.0018307209 -0.0018288684 -0.0018264524 -0.0018237341 -0.0018212197 -0.0018193487 -0.0018181327 -0.0018173649][-0.0018205416 -0.0018226206 -0.001825291 -0.0018280316 -0.0018306008 -0.0018320194 -0.0018317875 -0.0018305287 -0.0018287809 -0.0018264927 -0.0018238602 -0.0018213007 -0.0018194939 -0.0018183383 -0.0018175781][-0.001819839 -0.0018216508 -0.0018241624 -0.0018268822 -0.0018294445 -0.0018308882 -0.0018304811 -0.0018288058 -0.0018269438 -0.0018249425 -0.0018227325 -0.0018205083 -0.0018189558 -0.0018180811 -0.0018174878][-0.0018188732 -0.0018202174 -0.0018223374 -0.0018246273 -0.0018269019 -0.001828238 -0.0018279741 -0.001826324 -0.0018244559 -0.0018227064 -0.0018209281 -0.0018192708 -0.0018181917 -0.0018176733 -0.0018172779][-0.0018180492 -0.0018188689 -0.0018205332 -0.001822219 -0.0018240226 -0.0018252399 -0.0018252111 -0.0018240326 -0.0018225742 -0.0018211921 -0.0018196863 -0.0018182461 -0.0018174932 -0.0018172606 -0.0018170548][-0.0018174832 -0.0018177537 -0.001818895 -0.0018200811 -0.0018213388 -0.001822314 -0.0018225007 -0.001821868 -0.0018209701 -0.0018200129 -0.0018189349 -0.0018177626 -0.0018170689 -0.00181689 -0.0018167935][-0.0018171574 -0.0018169321 -0.001817533 -0.0018183147 -0.0018191632 -0.0018198793 -0.0018201304 -0.0018198639 -0.0018193704 -0.0018187953 -0.0018181272 -0.0018173658 -0.0018168187 -0.0018166158 -0.0018165346][-0.0018169634 -0.0018164713 -0.0018166903 -0.0018171235 -0.0018176256 -0.001818083 -0.0018183111 -0.0018182814 -0.0018181105 -0.0018178269 -0.0018174566 -0.0018170193 -0.0018166325 -0.001816446 -0.0018163602][-0.0018169465 -0.0018164058 -0.0018163848 -0.0018165574 -0.0018168159 -0.0018170571 -0.001817205 -0.0018172757 -0.0018172939 -0.0018172131 -0.0018170283 -0.0018168117 -0.0018165935 -0.0018164524 -0.0018163682][-0.0018171279 -0.0018165034 -0.0018163461 -0.0018164011 -0.0018165117 -0.0018166326 -0.0018167191 -0.0018167851 -0.0018168377 -0.0018168296 -0.0018167597 -0.0018166696 -0.0018165424 -0.0018164427 -0.0018163648]]...]
INFO - root - 2017-12-09 18:27:14.687380: step 47310, loss = 0.82, batch loss = 0.69 (10.3 examples/sec; 0.780 sec/batch; 61h:48m:35s remains)
INFO - root - 2017-12-09 18:27:23.394771: step 47320, loss = 0.83, batch loss = 0.70 (9.1 examples/sec; 0.879 sec/batch; 69h:36m:23s remains)
INFO - root - 2017-12-09 18:27:32.060187: step 47330, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.866 sec/batch; 68h:36m:54s remains)
INFO - root - 2017-12-09 18:27:40.679618: step 47340, loss = 0.81, batch loss = 0.68 (9.2 examples/sec; 0.871 sec/batch; 69h:00m:04s remains)
INFO - root - 2017-12-09 18:27:49.422043: step 47350, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.841 sec/batch; 66h:34m:56s remains)
INFO - root - 2017-12-09 18:27:57.812186: step 47360, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.877 sec/batch; 69h:27m:01s remains)
INFO - root - 2017-12-09 18:28:06.542285: step 47370, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.882 sec/batch; 69h:52m:02s remains)
INFO - root - 2017-12-09 18:28:15.197839: step 47380, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 68h:50m:36s remains)
INFO - root - 2017-12-09 18:28:23.565535: step 47390, loss = 0.81, batch loss = 0.68 (9.2 examples/sec; 0.872 sec/batch; 69h:02m:16s remains)
INFO - root - 2017-12-09 18:28:32.039090: step 47400, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.856 sec/batch; 67h:45m:21s remains)
2017-12-09 18:28:32.885029: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.00082795159 0.00074340333 0.00056929677 0.00036727218 0.00016543665 -1.3859011e-05 -0.00016512536 -0.00030443212 -0.00043508294 -0.00056147983 -0.00066491368 -0.0007295484 -0.00074103707 -0.00065131835 -0.00048513408][0.00053747 0.00044118962 0.00023397314 -1.5311176e-05 -0.0002626928 -0.00047800585 -0.0006525279 -0.00079920667 -0.00093605922 -0.0010708647 -0.0011864179 -0.0012707233 -0.0013041657 -0.0012613853 -0.0011351986][0.00010857894 1.6281148e-05 -0.0002037267 -0.00047808106 -0.0007455477 -0.00096352294 -0.0011217875 -0.0012365404 -0.0013343121 -0.0014291601 -0.0015126471 -0.001573273 -0.0016008271 -0.0015887336 -0.0015170834][-0.00039302988 -0.00045383384 -0.0006484919 -0.000904308 -0.0011519857 -0.0013412023 -0.001462594 -0.0015329149 -0.0015821897 -0.0016279327 -0.0016705545 -0.0017043467 -0.0017236542 -0.001724349 -0.0016902495][-0.00091945636 -0.0009440952 -0.0010733576 -0.0012536924 -0.0014281274 -0.0015558854 -0.0016322915 -0.0016701946 -0.0016906733 -0.0017088325 -0.0017273764 -0.0017426671 -0.0017521086 -0.0017526834 -0.0017378215][-0.0013394734 -0.0013382578 -0.0013980065 -0.0014890207 -0.0015794975 -0.00164246 -0.0016760763 -0.0016892615 -0.001694302 -0.0017006826 -0.00170581 -0.0017057407 -0.0016995855 -0.0016866148 -0.0016763633][-0.0016032746 -0.0015886411 -0.0016008752 -0.0016275912 -0.0016556829 -0.001673623 -0.0016783497 -0.0016747508 -0.0016670912 -0.0016623004 -0.0016518931 -0.0016301043 -0.0015983955 -0.0015683681 -0.001561504][-0.0017272667 -0.0017069508 -0.0016967141 -0.0016913188 -0.0016888888 -0.0016833303 -0.0016697473 -0.0016530827 -0.001636895 -0.0016237674 -0.0015997186 -0.0015561146 -0.001498129 -0.001450914 -0.0014439495][-0.0017849442 -0.0017679064 -0.0017540378 -0.0017426829 -0.0017341354 -0.0017239536 -0.0017083517 -0.0016906748 -0.0016722928 -0.0016522219 -0.0016195297 -0.0015674577 -0.0015021971 -0.0014500482 -0.0014386887][-0.0018156439 -0.0018082031 -0.0017996962 -0.0017905468 -0.0017819953 -0.0017723977 -0.0017605369 -0.0017489934 -0.0017367495 -0.0017216988 -0.0016955755 -0.001653271 -0.0016008106 -0.0015566841 -0.0015417197][-0.0018311982 -0.0018299938 -0.001827112 -0.0018232586 -0.0018189771 -0.0018131353 -0.0018056466 -0.0017989118 -0.001792776 -0.0017848418 -0.0017706694 -0.0017471234 -0.001717027 -0.0016897438 -0.0016763629][-0.0018370205 -0.0018387659 -0.0018383163 -0.0018369522 -0.0018346574 -0.0018309186 -0.0018261923 -0.0018222169 -0.0018197336 -0.0018171764 -0.001812544 -0.0018029397 -0.0017887669 -0.0017742466 -0.0017647913][-0.0018387708 -0.001839765 -0.0018396619 -0.0018394982 -0.0018384689 -0.0018358602 -0.0018323113 -0.0018293707 -0.00182845 -0.0018283442 -0.0018274221 -0.0018235223 -0.0018166287 -0.0018086993 -0.0018022194][-0.0018401251 -0.0018404773 -0.001840264 -0.0018401253 -0.0018391385 -0.0018376247 -0.0018358072 -0.001834253 -0.0018339555 -0.0018341732 -0.0018340156 -0.0018319687 -0.0018282983 -0.0018235015 -0.0018191129][-0.001840255 -0.00184015 -0.0018399548 -0.0018398406 -0.0018392133 -0.0018386248 -0.0018379539 -0.0018373409 -0.0018372423 -0.0018372879 -0.0018371434 -0.0018360507 -0.0018344219 -0.001832372 -0.001830068]]...]
INFO - root - 2017-12-09 18:28:41.475831: step 47410, loss = 0.82, batch loss = 0.69 (9.7 examples/sec; 0.829 sec/batch; 65h:37m:04s remains)
INFO - root - 2017-12-09 18:28:49.954559: step 47420, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.877 sec/batch; 69h:25m:21s remains)
INFO - root - 2017-12-09 18:28:58.540467: step 47430, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.831 sec/batch; 65h:49m:17s remains)
INFO - root - 2017-12-09 18:29:07.055673: step 47440, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 68h:23m:10s remains)
INFO - root - 2017-12-09 18:29:15.528697: step 47450, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.865 sec/batch; 68h:29m:38s remains)
INFO - root - 2017-12-09 18:29:23.893014: step 47460, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 68h:23m:01s remains)
INFO - root - 2017-12-09 18:29:32.580170: step 47470, loss = 0.82, batch loss = 0.69 (8.8 examples/sec; 0.913 sec/batch; 72h:15m:33s remains)
INFO - root - 2017-12-09 18:29:41.126477: step 47480, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 68h:15m:46s remains)
INFO - root - 2017-12-09 18:29:49.666323: step 47490, loss = 0.81, batch loss = 0.68 (9.4 examples/sec; 0.854 sec/batch; 67h:35m:02s remains)
INFO - root - 2017-12-09 18:29:58.337566: step 47500, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.844 sec/batch; 66h:48m:32s remains)
2017-12-09 18:29:59.218478: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0018293853 -0.0018320498 -0.0018346149 -0.0018377727 -0.0018400454 -0.001840967 -0.001840802 -0.0018399782 -0.0018390575 -0.0018385344 -0.0018378942 -0.0018374076 -0.0018371404 -0.0018370321 -0.001836932][-0.0018295544 -0.0018320817 -0.0018349281 -0.0018381523 -0.0018406856 -0.001842086 -0.0018424323 -0.0018419384 -0.0018410782 -0.0018404629 -0.0018397196 -0.0018391624 -0.0018388546 -0.0018388059 -0.0018387588][-0.0018339943 -0.0018358312 -0.0018381192 -0.001840356 -0.0018422463 -0.0018434103 -0.0018438657 -0.0018435649 -0.0018428486 -0.0018421718 -0.0018415418 -0.0018411058 -0.001840946 -0.0018409499 -0.0018407759][-0.0018392922 -0.0018403926 -0.0018416987 -0.0018426341 -0.0018434123 -0.0018438381 -0.001843983 -0.0018437896 -0.0018433225 -0.0018428619 -0.0018425597 -0.0018425096 -0.0018426811 -0.0018428972 -0.0018426684][-0.00184301 -0.0018430488 -0.0018432315 -0.0018433066 -0.0018431754 -0.0018429493 -0.001842841 -0.0018425873 -0.0018422733 -0.0018422678 -0.0018426375 -0.0018430435 -0.0018433119 -0.0018435809 -0.0018433151][-0.0018442639 -0.001843265 -0.0018423075 -0.0018413882 -0.0018404943 -0.0018399234 -0.0018394617 -0.0018390984 -0.0018392187 -0.001840112 -0.0018412531 -0.0018422763 -0.0018425791 -0.0018426233 -0.0018424825][-0.0018434194 -0.0018412544 -0.0018392892 -0.0018374788 -0.0018360903 -0.0018350518 -0.0018340937 -0.0018335959 -0.0018344756 -0.001836883 -0.0018391561 -0.0018409044 -0.0018412486 -0.0018409044 -0.0018406806][-0.001842179 -0.0018392851 -0.0018366033 -0.0018342923 -0.001832511 -0.0018309532 -0.0018293265 -0.0018281469 -0.0018297087 -0.0018339304 -0.0018374305 -0.0018395765 -0.0018398204 -0.001838936 -0.0018384184][-0.0018413994 -0.0018385057 -0.0018358969 -0.0018336563 -0.0018316831 -0.0018298111 -0.0018277913 -0.0018260382 -0.0018278213 -0.0018328157 -0.0018370284 -0.0018391655 -0.0018389885 -0.0018374373 -0.00183646][-0.0018413746 -0.001838995 -0.0018368149 -0.0018349973 -0.0018333544 -0.0018315484 -0.0018299284 -0.001828715 -0.001830225 -0.0018338524 -0.0018372583 -0.0018389804 -0.0018386031 -0.001836885 -0.0018355661][-0.0018418913 -0.0018402587 -0.0018386996 -0.0018373334 -0.0018360221 -0.0018345583 -0.0018335967 -0.0018328906 -0.0018337115 -0.0018358195 -0.0018379175 -0.001838831 -0.001838332 -0.0018368162 -0.001835563][-0.0018427111 -0.0018415492 -0.0018404037 -0.001839409 -0.0018383028 -0.0018371291 -0.0018365216 -0.0018360368 -0.0018362461 -0.0018370855 -0.0018380694 -0.0018384747 -0.0018379634 -0.0018368989 -0.001835816][-0.0018438161 -0.0018428714 -0.0018419172 -0.0018410558 -0.0018400431 -0.001839065 -0.0018384084 -0.0018377778 -0.0018375324 -0.0018375235 -0.001837801 -0.0018378692 -0.0018374864 -0.0018366874 -0.0018358742][-0.0018450518 -0.0018443239 -0.0018435066 -0.0018425591 -0.0018415757 -0.0018407273 -0.0018398981 -0.0018390529 -0.0018382822 -0.0018377444 -0.0018376333 -0.0018374773 -0.0018370983 -0.0018364672 -0.0018358303][-0.0018458016 -0.0018453316 -0.0018447032 -0.0018437682 -0.0018427547 -0.0018419595 -0.0018410905 -0.0018401587 -0.001839257 -0.0018385064 -0.0018380733 -0.0018377115 -0.0018372576 -0.0018367537 -0.0018361941]]...]
INFO - root - 2017-12-09 18:30:07.789980: step 47510, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.875 sec/batch; 69h:18m:13s remains)
INFO - root - 2017-12-09 18:30:16.331655: step 47520, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.886 sec/batch; 70h:07m:11s remains)
INFO - root - 2017-12-09 18:30:25.012957: step 47530, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.833 sec/batch; 65h:54m:27s remains)
INFO - root - 2017-12-09 18:30:33.748386: step 47540, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.886 sec/batch; 70h:05m:36s remains)
INFO - root - 2017-12-09 18:30:42.389259: step 47550, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.878 sec/batch; 69h:29m:31s remains)
INFO - root - 2017-12-09 18:30:50.952681: step 47560, loss = 0.82, batch loss = 0.69 (8.8 examples/sec; 0.911 sec/batch; 72h:06m:55s remains)
INFO - root - 2017-12-09 18:30:59.634008: step 47570, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.854 sec/batch; 67h:37m:05s remains)
INFO - root - 2017-12-09 18:31:08.393329: step 47580, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.882 sec/batch; 69h:50m:25s remains)
INFO - root - 2017-12-09 18:31:16.797697: step 47590, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.889 sec/batch; 70h:20m:33s remains)
INFO - root - 2017-12-09 18:31:25.351919: step 47600, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 68h:13m:36s remains)
2017-12-09 18:31:26.285173: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0018002224 -0.0018042072 -0.0018110913 -0.0018156373 -0.0018173759 -0.0018159862 -0.0018125369 -0.0018072701 -0.0018019683 -0.0017985536 -0.0017976748 -0.001799383 -0.0018030877 -0.0018077667 -0.0018127548][-0.0017940744 -0.001798331 -0.0018058543 -0.0018111519 -0.0018138777 -0.0018133178 -0.0018106323 -0.0018063126 -0.0018017204 -0.0017984181 -0.0017971687 -0.0017984628 -0.0018018234 -0.0018060372 -0.0018108198][-0.0017925386 -0.0017965952 -0.0018041893 -0.0018097627 -0.0018128789 -0.0018126214 -0.0018105769 -0.0018070415 -0.0018032946 -0.0018006355 -0.0017994451 -0.0018001249 -0.0018026136 -0.0018055046 -0.0018091697][-0.0017944069 -0.0017975649 -0.0018044821 -0.0018099009 -0.0018130183 -0.0018129013 -0.0018112886 -0.0018086275 -0.0018058158 -0.0018038482 -0.0018028282 -0.0018031161 -0.0018045882 -0.0018059638 -0.0018083255][-0.0017982015 -0.0018005923 -0.0018067029 -0.0018117158 -0.0018149177 -0.0018150186 -0.0018136684 -0.0018116353 -0.0018096193 -0.0018080617 -0.0018070703 -0.0018067801 -0.0018071177 -0.0018072113 -0.0018080978][-0.0018050128 -0.0018056622 -0.0018100815 -0.0018146279 -0.0018178038 -0.0018179857 -0.0018170394 -0.0018157514 -0.0018140109 -0.0018127749 -0.0018117686 -0.0018108123 -0.0018098804 -0.0018086343 -0.0018080545][-0.0018124337 -0.0018120651 -0.001814495 -0.0018179239 -0.0018206773 -0.0018212586 -0.0018208915 -0.0018200986 -0.0018187667 -0.0018175619 -0.0018164768 -0.0018146947 -0.001812547 -0.001809989 -0.0018081031][-0.0018187689 -0.0018179455 -0.0018193987 -0.0018218746 -0.0018241423 -0.0018245293 -0.0018245461 -0.0018241236 -0.0018227173 -0.0018215324 -0.0018196545 -0.0018173294 -0.0018145407 -0.0018114672 -0.0018088382][-0.0018220864 -0.0018212311 -0.0018219531 -0.0018247069 -0.0018267441 -0.0018272644 -0.0018273046 -0.0018269365 -0.0018255627 -0.0018236791 -0.0018213782 -0.0018186826 -0.0018156397 -0.0018126288 -0.0018097727][-0.0018234522 -0.0018229723 -0.00182401 -0.0018271902 -0.0018295574 -0.0018302313 -0.0018300971 -0.001828809 -0.0018267427 -0.0018239889 -0.0018211522 -0.001818595 -0.0018158918 -0.001813205 -0.0018106136][-0.0018234539 -0.0018226503 -0.0018240969 -0.0018275799 -0.0018301533 -0.0018309985 -0.0018308006 -0.0018290445 -0.0018261209 -0.0018224949 -0.001819129 -0.0018167953 -0.0018146248 -0.0018127253 -0.0018108515][-0.0018214668 -0.0018205141 -0.0018221072 -0.0018259345 -0.001828728 -0.0018297975 -0.001829489 -0.0018276599 -0.0018242022 -0.0018196061 -0.0018157563 -0.0018135309 -0.0018121165 -0.0018111165 -0.0018099549][-0.0018176553 -0.0018167195 -0.0018185857 -0.0018225141 -0.0018258332 -0.0018272839 -0.0018270084 -0.0018250489 -0.0018214884 -0.0018166088 -0.0018123018 -0.0018100586 -0.0018089311 -0.001808557 -0.001808075][-0.0018132436 -0.0018124047 -0.0018143235 -0.0018183452 -0.0018219923 -0.0018241574 -0.0018242395 -0.0018225856 -0.0018189161 -0.0018140441 -0.0018097269 -0.0018070801 -0.0018057558 -0.0018055611 -0.0018056544][-0.0018095822 -0.0018088825 -0.0018105402 -0.0018140938 -0.0018178463 -0.0018205963 -0.0018211298 -0.0018197851 -0.0018164681 -0.0018119772 -0.0018077461 -0.0018049364 -0.0018034455 -0.0018030423 -0.0018033426]]...]
INFO - root - 2017-12-09 18:31:34.904907: step 47610, loss = 0.81, batch loss = 0.68 (8.9 examples/sec; 0.901 sec/batch; 71h:17m:05s remains)
INFO - root - 2017-12-09 18:31:43.424917: step 47620, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.854 sec/batch; 67h:34m:44s remains)
INFO - root - 2017-12-09 18:31:51.949573: step 47630, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.857 sec/batch; 67h:46m:45s remains)
INFO - root - 2017-12-09 18:32:00.451583: step 47640, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.846 sec/batch; 66h:58m:23s remains)
INFO - root - 2017-12-09 18:32:09.026320: step 47650, loss = 0.82, batch loss = 0.69 (9.8 examples/sec; 0.820 sec/batch; 64h:53m:17s remains)
INFO - root - 2017-12-09 18:32:17.507858: step 47660, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.891 sec/batch; 70h:30m:36s remains)
INFO - root - 2017-12-09 18:32:26.112642: step 47670, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.833 sec/batch; 65h:55m:31s remains)
INFO - root - 2017-12-09 18:32:34.717904: step 47680, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.902 sec/batch; 71h:22m:52s remains)
INFO - root - 2017-12-09 18:32:43.331551: step 47690, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.831 sec/batch; 65h:46m:58s remains)
INFO - root - 2017-12-09 18:32:51.997736: step 47700, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.863 sec/batch; 68h:14m:20s remains)
2017-12-09 18:32:52.860542: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.001800996 -0.0017994746 -0.0017994827 -0.001799886 -0.0018003092 -0.0018006407 -0.0018008857 -0.0018011539 -0.0018014924 -0.0018017907 -0.0018019236 -0.001801793 -0.0018015066 -0.0018012164 -0.0018010153][-0.0017996501 -0.0017980426 -0.0017981264 -0.0017987057 -0.0017993465 -0.0017999291 -0.0018003845 -0.0018007047 -0.0018009511 -0.0018010929 -0.0018010632 -0.0018008428 -0.0018005908 -0.0018003895 -0.0018002908][-0.0017995212 -0.0017978332 -0.0017978547 -0.0017984314 -0.0017991081 -0.0017998142 -0.0018004143 -0.0018008098 -0.0018010286 -0.001801074 -0.00180096 -0.0018007553 -0.0018006115 -0.0018005753 -0.001800606][-0.0017996109 -0.001797849 -0.0017976984 -0.0017980718 -0.0017986214 -0.0017993008 -0.0017999714 -0.001800468 -0.0018007957 -0.0018009568 -0.0018009697 -0.0018009378 -0.0018010095 -0.0018011765 -0.0018013248][-0.0018000659 -0.0017980998 -0.0017976352 -0.0017976953 -0.0017979633 -0.001798474 -0.0017991625 -0.0017998167 -0.0018003961 -0.0018008184 -0.0018011201 -0.0018013527 -0.0018016568 -0.0018019998 -0.0018022187][-0.0018007274 -0.0017985887 -0.0017977823 -0.0017974547 -0.001797308 -0.0017974941 -0.0017980954 -0.0017988719 -0.0017996947 -0.0018004153 -0.0018010722 -0.0018016552 -0.0018022128 -0.0018027057 -0.001802981][-0.0018012543 -0.0017991022 -0.0017981081 -0.0017974706 -0.0017969722 -0.0017968408 -0.0017973006 -0.0017980811 -0.0017989659 -0.0017998705 -0.001800828 -0.001801678 -0.0018023889 -0.0018029693 -0.0018032874][-0.0018016027 -0.0017994939 -0.001798486 -0.0017977254 -0.0017970664 -0.0017967547 -0.0017970782 -0.0017977759 -0.0017986154 -0.0017995153 -0.0018005534 -0.001801467 -0.0018021626 -0.0018027081 -0.0018030321][-0.0018019879 -0.0018000252 -0.0017991539 -0.001798421 -0.001797737 -0.0017973499 -0.0017975518 -0.0017981094 -0.0017988342 -0.0017996391 -0.0018005732 -0.001801375 -0.0018019351 -0.0018023466 -0.0018025939][-0.0018025268 -0.0018008568 -0.0018002364 -0.0017996704 -0.0017990727 -0.0017986221 -0.0017986179 -0.0017989533 -0.0017994723 -0.0018000568 -0.0018007625 -0.001801376 -0.0018017729 -0.0018020306 -0.001802153][-0.0018030306 -0.0018016009 -0.001801176 -0.0018008281 -0.0018004091 -0.0018000146 -0.0017998759 -0.0017999648 -0.0018002033 -0.0018005485 -0.0018010134 -0.0018014206 -0.0018016587 -0.0018017701 -0.0018018022][-0.0018033274 -0.0018019552 -0.0018016584 -0.0018015127 -0.0018012803 -0.0018010257 -0.0018008798 -0.0018008333 -0.0018008786 -0.0018010321 -0.0018012933 -0.0018014918 -0.0018015591 -0.0018015443 -0.0018015035][-0.0018035594 -0.0018021665 -0.0018018842 -0.0018018617 -0.0018017722 -0.0018016523 -0.0018015637 -0.0018015 -0.0018014837 -0.0018015248 -0.0018016306 -0.0018016878 -0.0018016453 -0.0018015277 -0.0018014194][-0.0018038738 -0.0018023795 -0.0018020511 -0.0018020769 -0.0018020665 -0.0018020262 -0.00180198 -0.0018019393 -0.0018019198 -0.0018019115 -0.001801934 -0.001801928 -0.0018018655 -0.0018017438 -0.0018016228][-0.001804095 -0.0018025303 -0.0018020917 -0.001802124 -0.0018021346 -0.0018021186 -0.0018020932 -0.0018020745 -0.0018020619 -0.0018020426 -0.0018020426 -0.0018020381 -0.0018020029 -0.0018019215 -0.0018018294]]...]
INFO - root - 2017-12-09 18:33:01.482004: step 47710, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.882 sec/batch; 69h:46m:46s remains)
INFO - root - 2017-12-09 18:33:10.061122: step 47720, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.887 sec/batch; 70h:08m:45s remains)
INFO - root - 2017-12-09 18:33:18.708740: step 47730, loss = 0.82, batch loss = 0.70 (9.4 examples/sec; 0.854 sec/batch; 67h:33m:22s remains)
INFO - root - 2017-12-09 18:33:27.317243: step 47740, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.872 sec/batch; 68h:57m:57s remains)
INFO - root - 2017-12-09 18:33:35.925761: step 47750, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 68h:13m:13s remains)
INFO - root - 2017-12-09 18:33:44.412588: step 47760, loss = 0.83, batch loss = 0.70 (9.2 examples/sec; 0.871 sec/batch; 68h:53m:03s remains)
INFO - root - 2017-12-09 18:33:53.025862: step 47770, loss = 0.82, batch loss = 0.69 (9.7 examples/sec; 0.821 sec/batch; 64h:54m:08s remains)
INFO - root - 2017-12-09 18:34:01.398954: step 47780, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.878 sec/batch; 69h:28m:12s remains)
INFO - root - 2017-12-09 18:34:09.895495: step 47790, loss = 0.82, batch loss = 0.69 (8.1 examples/sec; 0.983 sec/batch; 77h:42m:24s remains)
INFO - root - 2017-12-09 18:34:18.699316: step 47800, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.872 sec/batch; 68h:57m:52s remains)
2017-12-09 18:34:19.714697: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0018259458 -0.0018279875 -0.0018300517 -0.0018312563 -0.0018311834 -0.0018306851 -0.0018299847 -0.001829781 -0.0018302319 -0.0018303298 -0.0018304641 -0.0018307806 -0.0018308379 -0.0018294913 -0.0018270515][-0.0018240662 -0.0018256311 -0.0018271582 -0.0018280076 -0.0018278209 -0.0018272346 -0.0018264606 -0.0018261895 -0.0018266046 -0.0018267967 -0.0018271962 -0.0018280913 -0.0018287284 -0.0018278734 -0.0018257146][-0.001822573 -0.0018236657 -0.0018248301 -0.0018255857 -0.001825431 -0.001824962 -0.0018243089 -0.0018240782 -0.0018242771 -0.001824238 -0.00182457 -0.0018256259 -0.0018264808 -0.0018261668 -0.001824591][-0.0018212207 -0.0018217141 -0.0018223693 -0.0018227933 -0.0018225532 -0.001822087 -0.0018215736 -0.001821488 -0.0018215985 -0.0018214936 -0.0018217175 -0.0018227262 -0.0018236676 -0.0018238965 -0.0018229851][-0.0018200605 -0.001819827 -0.0018198558 -0.0018198367 -0.0018194343 -0.0018190324 -0.00181883 -0.0018190761 -0.001819427 -0.0018195383 -0.0018198175 -0.0018206934 -0.0018215958 -0.0018222284 -0.0018221788][-0.001818959 -0.0018181608 -0.0018176648 -0.0018171696 -0.0018165339 -0.0018162025 -0.0018163723 -0.0018172615 -0.0018182561 -0.0018189539 -0.0018194236 -0.001820141 -0.0018208009 -0.001821447 -0.0018218253][-0.0018183023 -0.0018172382 -0.0018164914 -0.0018157524 -0.0018149789 -0.0018146975 -0.0018152207 -0.0018166235 -0.0018181307 -0.0018193356 -0.00181999 -0.0018204806 -0.0018206558 -0.0018207987 -0.0018210338][-0.0018180035 -0.0018168886 -0.0018161428 -0.0018153744 -0.0018145927 -0.0018143964 -0.0018151448 -0.0018168576 -0.0018186505 -0.0018201495 -0.0018208756 -0.0018210441 -0.0018206375 -0.0018201045 -0.0018198459][-0.0018176141 -0.0018165506 -0.0018159334 -0.0018152305 -0.0018145402 -0.0018144746 -0.0018153831 -0.0018172985 -0.0018192905 -0.0018210238 -0.0018218461 -0.0018217603 -0.0018208193 -0.0018195144 -0.0018186017][-0.0018171396 -0.0018162061 -0.0018156803 -0.0018150762 -0.0018145186 -0.0018146054 -0.0018156439 -0.0018176577 -0.0018197525 -0.0018216292 -0.0018225351 -0.0018222814 -0.0018210063 -0.001819185 -0.0018176611][-0.0018166552 -0.0018158923 -0.001815486 -0.0018149624 -0.0018145092 -0.0018146731 -0.0018157177 -0.0018176369 -0.001819614 -0.0018214639 -0.0018223643 -0.0018220665 -0.0018207225 -0.0018187605 -0.0018169679][-0.0018164086 -0.001815688 -0.0018153152 -0.0018148505 -0.0018144761 -0.0018146103 -0.0018154758 -0.0018170425 -0.0018186563 -0.0018202313 -0.0018210284 -0.0018207778 -0.0018196729 -0.0018180163 -0.0018163804][-0.0018162075 -0.0018154623 -0.0018150577 -0.0018146223 -0.001814301 -0.0018143603 -0.0018149494 -0.0018160602 -0.0018172316 -0.0018183837 -0.0018189808 -0.0018187953 -0.0018180518 -0.0018168697 -0.0018156086][-0.0018161639 -0.0018153291 -0.0018147725 -0.0018143443 -0.0018140695 -0.0018140762 -0.0018144565 -0.0018152051 -0.0018160559 -0.0018169044 -0.0018173528 -0.0018172605 -0.0018168408 -0.0018161035 -0.0018151766][-0.001816219 -0.0018152624 -0.001814611 -0.0018142035 -0.0018139996 -0.0018140124 -0.0018142988 -0.0018148287 -0.0018154475 -0.0018160472 -0.001816368 -0.0018163493 -0.0018161254 -0.0018157215 -0.0018151602]]...]
INFO - root - 2017-12-09 18:34:28.301651: step 47810, loss = 0.81, batch loss = 0.68 (9.4 examples/sec; 0.855 sec/batch; 67h:35m:25s remains)
INFO - root - 2017-12-09 18:34:36.882881: step 47820, loss = 0.82, batch loss = 0.69 (8.5 examples/sec; 0.944 sec/batch; 74h:40m:03s remains)
INFO - root - 2017-12-09 18:34:45.438503: step 47830, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.876 sec/batch; 69h:14m:19s remains)
INFO - root - 2017-12-09 18:34:54.098362: step 47840, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.861 sec/batch; 68h:06m:31s remains)
INFO - root - 2017-12-09 18:35:02.779791: step 47850, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.856 sec/batch; 67h:40m:45s remains)
INFO - root - 2017-12-09 18:35:11.203656: step 47860, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.855 sec/batch; 67h:34m:16s remains)
INFO - root - 2017-12-09 18:35:19.762092: step 47870, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.880 sec/batch; 69h:36m:10s remains)
INFO - root - 2017-12-09 18:35:28.419038: step 47880, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.874 sec/batch; 69h:07m:27s remains)
INFO - root - 2017-12-09 18:35:36.971443: step 47890, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.847 sec/batch; 67h:00m:00s remains)
INFO - root - 2017-12-09 18:35:45.730974: step 47900, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.876 sec/batch; 69h:13m:05s remains)
2017-12-09 18:35:46.584301: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0013686856 -0.0014057891 -0.0014435144 -0.0015013013 -0.0015532488 -0.0016408187 -0.0017010977 -0.0017433885 -0.0017592956 -0.0017468721 -0.0017270665 -0.0017087179 -0.0016902727 -0.001721717 -0.0017499402][-0.0013709979 -0.0013959262 -0.0013804666 -0.0013965493 -0.001419347 -0.0014837064 -0.0015356648 -0.0015813936 -0.0016200754 -0.0016299111 -0.0016390892 -0.0016370716 -0.0016532893 -0.001711019 -0.0017703025][-0.0011594498 -0.0013269775 -0.0013805367 -0.0014147176 -0.0014366986 -0.0014487891 -0.0014577853 -0.001449325 -0.0014641921 -0.0014758424 -0.0015002236 -0.0015218987 -0.0016319812 -0.0017258206 -0.0017615139][-0.00092020881 -0.0010177958 -0.0011395117 -0.0013183728 -0.0014802897 -0.0015392158 -0.0015683474 -0.001512709 -0.0014849801 -0.001443612 -0.0014211064 -0.0014459258 -0.0015774767 -0.0016455448 -0.0016896921][-0.00062709383 -0.0007592116 -0.00092493364 -0.0010516813 -0.001251736 -0.0014203788 -0.001537787 -0.0015987996 -0.0016467345 -0.001602277 -0.0015324713 -0.0015547799 -0.0015493849 -0.0015503666 -0.0015887804][-4.7303038e-06 -0.00027034979 -0.00049239281 -0.00066354859 -0.00085795065 -0.0010353685 -0.0012348052 -0.0013457433 -0.0014581854 -0.0015281418 -0.0014906282 -0.0014559613 -0.0014114652 -0.0014118322 -0.0014351937][0.0008830626 0.00054678693 0.00018451712 -0.00018433377 -0.00045665493 -0.00078912824 -0.00098717026 -0.0011037802 -0.0012494066 -0.0012655768 -0.0011867802 -0.0011424921 -0.0010680251 -0.0010694072 -0.0011177924][0.0017399399 0.0013203269 0.0008485592 0.00031745411 -7.8720273e-05 -0.00048842747 -0.00078740588 -0.0010596326 -0.0012201827 -0.0011471685 -0.0009150902 -0.00073043536 -0.000615394 -0.00065276993 -0.00077392615][0.00234369 0.0018692205 0.0013183695 0.0006757942 0.00015428709 -0.00027532538 -0.00059088855 -0.0007486816 -0.00091248937 -0.00083700835 -0.00065377576 -0.00049888657 -0.00037108129 -0.00035990961 -0.00044056645][0.0026038727 0.0020770929 0.0014523412 0.00072106719 0.00015688641 -0.00022467889 -0.00047812425 -0.00065960421 -0.00076192361 -0.00063743826 -0.00044826779 -0.0002679209 -0.00021292595 -0.00024810375 -0.00025771582][0.0025768704 0.0020019854 0.0013264669 0.00056751329 5.0402246e-05 -0.00023184053 -0.00036661106 -0.00037670147 -0.00044076284 -0.00039247784 -0.00029910624 -0.00015148264 -0.00014375837 -7.5275311e-05 -4.2764121e-05][0.0025706165 0.002012769 0.0013347061 0.00060861185 8.5783657e-05 -0.00014957169 -0.00026598712 -0.00021950563 -0.00013453851 -3.1360891e-05 1.527369e-06 1.8981402e-05 4.5016641e-05 0.00014180434 0.00019733887][0.0025217044 0.0020433636 0.0014284132 0.00078396383 0.00027932716 6.8951049e-05 9.5088384e-05 0.00020604883 0.00034049107 0.00040325918 0.00042179646 0.00046547805 0.000427593 0.00043612975 0.00047786254][0.002434602 0.0020514848 0.0015507236 0.0010437905 0.00066721044 0.00053937174 0.00061387825 0.00064012548 0.00077931746 0.00080215954 0.00085292896 0.00079632038 0.0008051286 0.0008427212 0.00094951852][0.0022843443 0.0020403096 0.0016543122 0.0012390227 0.00092926784 0.00085694343 0.00094514526 0.00099309 0.0011027707 0.0011550586 0.0012256452 0.0012432165 0.001298367 0.0013579424 0.0014225701]]...]
INFO - root - 2017-12-09 18:35:55.212531: step 47910, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.852 sec/batch; 67h:20m:35s remains)
INFO - root - 2017-12-09 18:36:03.377683: step 47920, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.851 sec/batch; 67h:15m:29s remains)
INFO - root - 2017-12-09 18:36:11.956986: step 47930, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.851 sec/batch; 67h:18m:19s remains)
INFO - root - 2017-12-09 18:36:20.556781: step 47940, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.856 sec/batch; 67h:41m:53s remains)
INFO - root - 2017-12-09 18:36:29.140155: step 47950, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.870 sec/batch; 68h:45m:09s remains)
INFO - root - 2017-12-09 18:36:37.668581: step 47960, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.832 sec/batch; 65h:45m:48s remains)
INFO - root - 2017-12-09 18:36:46.341727: step 47970, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 68h:29m:08s remains)
INFO - root - 2017-12-09 18:36:55.071874: step 47980, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.870 sec/batch; 68h:47m:52s remains)
INFO - root - 2017-12-09 18:37:03.570740: step 47990, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.897 sec/batch; 70h:55m:37s remains)
INFO - root - 2017-12-09 18:37:12.243213: step 48000, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.847 sec/batch; 66h:54m:36s remains)
2017-12-09 18:37:13.116526: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.012674633 0.012479529 0.012095913 0.011658232 0.011217472 0.010814661 0.010441225 0.010027379 0.0095744729 0.0092288312 0.0087936893 0.0083659627 0.0079541178 0.0075242347 0.0071771662][0.012774596 0.012649641 0.012323015 0.011893107 0.011437275 0.010954822 0.010557229 0.010154169 0.00962575 0.0091877766 0.0087304246 0.0083275242 0.007911345 0.0075200903 0.0072275545][0.013070222 0.013103966 0.012855482 0.01247699 0.012084448 0.011624063 0.011151037 0.010633477 0.0099995025 0.0094881374 0.009054848 0.0086681293 0.0082246559 0.007781331 0.00746349][0.013556369 0.013963026 0.013871894 0.0135583 0.013309631 0.013002934 0.012606196 0.012017422 0.011256857 0.01064995 0.010251968 0.0098746354 0.0092936922 0.0086352173 0.0081362491][0.014286012 0.015102192 0.015262797 0.015150485 0.015151597 0.015109767 0.014881059 0.014270005 0.013419167 0.012752127 0.012375283 0.011948408 0.011059623 0.00999573 0.0091930479][0.015056006 0.0162181 0.016600385 0.016714964 0.016990008 0.017263429 0.017272063 0.016686708 0.015783137 0.015075947 0.014728063 0.014168403 0.012898157 0.011372481 0.010271677][0.015851505 0.017273257 0.017874176 0.018251447 0.018778909 0.019266855 0.019337397 0.018668555 0.017671915 0.016875798 0.016450841 0.015701795 0.014093372 0.012222527 0.010966313][0.016303239 0.017938463 0.018758506 0.019400356 0.020083709 0.020595502 0.020503011 0.019668799 0.018553071 0.017685955 0.017147107 0.016169041 0.014350406 0.012422994 0.011257605][0.016570717 0.018332215 0.019336434 0.020193163 0.020941878 0.021334363 0.020966902 0.019914392 0.018640585 0.017657606 0.016982084 0.015885042 0.014027356 0.012170938 0.011164084][0.016360154 0.018227145 0.019405399 0.020463068 0.021260815 0.021525705 0.020977728 0.019809622 0.018459404 0.017397722 0.016572351 0.015387463 0.013601121 0.011858244 0.010956658][0.015514695 0.017426271 0.018681984 0.019836405 0.020655973 0.020898031 0.020381507 0.01929643 0.018076321 0.017052175 0.016148001 0.014937147 0.013259319 0.011625116 0.010671965][0.014067837 0.015927954 0.017167054 0.018307177 0.019089773 0.019415125 0.01912945 0.018324781 0.017314218 0.016418958 0.015560693 0.014421708 0.012896544 0.011345744 0.010278736][0.012203316 0.013821236 0.014888373 0.015874334 0.016567068 0.016952539 0.016895898 0.016413728 0.015728215 0.015052472 0.014322552 0.013310416 0.012029075 0.010701852 0.0096276011][0.010168385 0.011452937 0.012243531 0.012999269 0.013504694 0.0138431 0.013949369 0.013755809 0.013366734 0.012906463 0.012372673 0.011619717 0.010670855 0.0096699549 0.0087505737][0.0083491569 0.0092860628 0.0097661875 0.010248508 0.010566605 0.01079893 0.010928134 0.010885878 0.010703359 0.010444252 0.010089867 0.0096275862 0.0090147546 0.0084054628 0.0077427714]]...]
INFO - root - 2017-12-09 18:37:21.739406: step 48010, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 68h:39m:43s remains)
INFO - root - 2017-12-09 18:37:30.177348: step 48020, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.855 sec/batch; 67h:33m:13s remains)
INFO - root - 2017-12-09 18:37:38.844111: step 48030, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.872 sec/batch; 68h:52m:12s remains)
INFO - root - 2017-12-09 18:37:47.473127: step 48040, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 68h:05m:39s remains)
INFO - root - 2017-12-09 18:37:56.061201: step 48050, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.835 sec/batch; 65h:58m:53s remains)
INFO - root - 2017-12-09 18:38:04.540610: step 48060, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 68h:07m:06s remains)
INFO - root - 2017-12-09 18:38:13.224713: step 48070, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.887 sec/batch; 70h:05m:23s remains)
INFO - root - 2017-12-09 18:38:21.949238: step 48080, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.886 sec/batch; 70h:01m:50s remains)
INFO - root - 2017-12-09 18:38:30.477566: step 48090, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.858 sec/batch; 67h:45m:34s remains)
INFO - root - 2017-12-09 18:38:39.238006: step 48100, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.882 sec/batch; 69h:39m:45s remains)
2017-12-09 18:38:40.131562: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0016212234 -0.0013294832 -0.00089430885 -0.00042409752 -0.00010827067 -0.00012801006 -0.00049468642 -0.0010219265 -0.0014602401 -0.0017094597 -0.0018057146 -0.0018291184 -0.0018317555 -0.0018308519 -0.0018302032][-0.0014704446 -0.0009363516 -0.00013854902 0.0007166689 0.0012986833 0.0012792601 0.00063091714 -0.00031546562 -0.0011192689 -0.0015864567 -0.0017735157 -0.0018218431 -0.0018283962 -0.0018276554 -0.0018265923][-0.0012513283 -0.00036232523 0.00095687143 0.0023401608 0.0032514003 0.0032167975 0.0022116988 0.00072372786 -0.00058032596 -0.0013729584 -0.0017116403 -0.0018083039 -0.0018242506 -0.0018245365 -0.0018237953][-0.00096371246 0.00036888814 0.0023124884 0.0042620003 0.0054623149 0.0053442647 0.0039566695 0.0019265177 0.00010087097 -0.0010689639 -0.001610572 -0.0017854477 -0.0018211404 -0.0018240138 -0.001822963][-0.00050087261 0.0014013179 0.00401764 0.0064885239 0.007845181 0.0075225285 0.0057185637 0.0031886506 0.00087654369 -0.00068176771 -0.0014616635 -0.0017451864 -0.0018139199 -0.0018220106 -0.0018214133][0.00025365653 0.0027445117 0.005974256 0.008904865 0.010433348 0.0099584088 0.0077610672 0.004694141 0.0018159653 -0.00021004444 -0.001279827 -0.0016967002 -0.0018058937 -0.0018198983 -0.001818926][0.0012970775 0.0043086065 0.0079529323 0.011136856 0.012711401 0.012043362 0.0094511462 0.0058741192 0.0025000093 0.00010726659 -0.0011675106 -0.0016696464 -0.0018020367 -0.0018193687 -0.0018180913][0.0022509987 0.0055494886 0.0092850588 0.012441256 0.013927865 0.01308887 0.010207682 0.0062798695 0.002623464 0.00010118063 -0.0011967988 -0.0016835023 -0.0018028902 -0.0018167337 -0.0018161131][0.0024169786 0.0055158339 0.0088750208 0.011660925 0.012949011 0.012123371 0.0093528694 0.0055732895 0.0021070484 -0.00020046579 -0.0013302377 -0.0017215902 -0.0018067001 -0.0018138024 -0.0018134239][0.0015901454 0.0040011229 0.0066134366 0.0088335285 0.0099809291 0.0094703808 0.0072813188 0.0041548163 0.0012765626 -0.000597114 -0.0014753203 -0.0017557794 -0.0018082908 -0.0018100811 -0.0018100103][0.00017639098 0.0016570702 0.0033726715 0.0049817511 0.006010124 0.0059287562 0.0045533595 0.0023741908 0.00032028218 -0.001004861 -0.0016056105 -0.0017824498 -0.0018101324 -0.0018086707 -0.0018085298][-0.00096638518 -0.00023939775 0.000704914 0.0017186092 0.0025029141 0.0026460192 0.0019399695 0.0006680045 -0.00055879948 -0.0013489951 -0.0017002071 -0.0017987028 -0.0018121451 -0.0018098489 -0.0018087486][-0.0016113883 -0.0013381591 -0.00091769977 -0.00039421604 7.2494033e-05 0.00023646734 -4.85942e-05 -0.00064458617 -0.0012293335 -0.0016036223 -0.0017661601 -0.0018107335 -0.00181425 -0.0018062483 -0.0017978235][-0.0017907727 -0.0017171092 -0.0015843019 -0.0013976332 -0.0012215325 -0.001152822 -0.0012489471 -0.0014525058 -0.0016435194 -0.0017594228 -0.0018056306 -0.0018136477 -0.0017967909 -0.0017581586 -0.0017306913][-0.0018226732 -0.0018116786 -0.0017909994 -0.0017543852 -0.0017124352 -0.0016890394 -0.0017029509 -0.0017423005 -0.0017811467 -0.0018048527 -0.0018110207 -0.0017914958 -0.0017233259 -0.0016078906 -0.0015377078]]...]
INFO - root - 2017-12-09 18:38:48.744985: step 48110, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.853 sec/batch; 67h:21m:46s remains)
INFO - root - 2017-12-09 18:38:57.191954: step 48120, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.859 sec/batch; 67h:50m:00s remains)
INFO - root - 2017-12-09 18:39:05.827681: step 48130, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 68h:45m:58s remains)
INFO - root - 2017-12-09 18:39:14.557423: step 48140, loss = 0.82, batch loss = 0.69 (8.2 examples/sec; 0.978 sec/batch; 77h:12m:59s remains)
INFO - root - 2017-12-09 18:39:23.217938: step 48150, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.845 sec/batch; 66h:46m:14s remains)
INFO - root - 2017-12-09 18:39:31.672021: step 48160, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.874 sec/batch; 69h:04m:07s remains)
INFO - root - 2017-12-09 18:39:40.345733: step 48170, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.880 sec/batch; 69h:29m:21s remains)
INFO - root - 2017-12-09 18:39:48.885757: step 48180, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.865 sec/batch; 68h:16m:41s remains)
INFO - root - 2017-12-09 18:39:57.496249: step 48190, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.866 sec/batch; 68h:21m:58s remains)
INFO - root - 2017-12-09 18:40:05.964828: step 48200, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.856 sec/batch; 67h:37m:35s remains)
2017-12-09 18:40:06.911301: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0018191705 -0.0018177526 -0.0018175481 -0.0018174647 -0.0018174363 -0.001817207 -0.0018168684 -0.0018165343 -0.001816438 -0.0018167775 -0.0018172732 -0.0018178762 -0.0018183694 -0.0018186342 -0.0018185601][-0.0018189937 -0.0018181616 -0.0018181538 -0.001818349 -0.001818251 -0.0018179701 -0.0018174312 -0.0018169222 -0.0018167711 -0.0018172021 -0.0018179424 -0.0018188516 -0.0018196035 -0.0018199463 -0.0018197725][-0.001820192 -0.0018199509 -0.0018205307 -0.0018209746 -0.0018209839 -0.0018206853 -0.0018199262 -0.0018189461 -0.0018184012 -0.001818539 -0.001819339 -0.0018207298 -0.0018218909 -0.0018223445 -0.0018218894][-0.0018213284 -0.0018218971 -0.0018232716 -0.0018240998 -0.0018241374 -0.0018236832 -0.0018222047 -0.001820105 -0.0018184385 -0.0018181249 -0.0018192881 -0.0018215704 -0.0018237929 -0.0018247153 -0.001824205][-0.0018222225 -0.001823506 -0.001825424 -0.0018266776 -0.0018267768 -0.00182548 -0.0018219531 -0.001817412 -0.0018135199 -0.0018120769 -0.0018140069 -0.0018182 -0.0018228788 -0.0018255359 -0.0018257101][-0.0018234426 -0.0018253227 -0.0018272472 -0.0018284686 -0.001828243 -0.0018249732 -0.0018178499 -0.0018095784 -0.0018033781 -0.001801359 -0.0018045485 -0.0018117257 -0.0018199421 -0.0018255862 -0.0018271578][-0.0018244844 -0.0018265673 -0.001828322 -0.001829258 -0.0018281699 -0.0018222221 -0.0018110907 -0.0017989217 -0.0017910947 -0.0017888573 -0.0017933444 -0.0018035375 -0.0018156567 -0.0018249492 -0.001828341][-0.0018253898 -0.0018271942 -0.0018283878 -0.0018286637 -0.0018263442 -0.0018183326 -0.0018048226 -0.0017909427 -0.0017824408 -0.0017800053 -0.0017851068 -0.0017967257 -0.0018113529 -0.0018230806 -0.0018279393][-0.0018259331 -0.0018279736 -0.0018288824 -0.0018286887 -0.0018259055 -0.0018179527 -0.0018053328 -0.0017917622 -0.0017831963 -0.0017803086 -0.0017847773 -0.0017959445 -0.001810051 -0.0018214507 -0.0018265019][-0.0018255998 -0.0018281713 -0.0018296265 -0.0018297681 -0.0018275609 -0.0018214142 -0.0018118657 -0.0018011861 -0.0017939981 -0.0017911256 -0.0017941567 -0.0018025315 -0.0018129501 -0.0018215503 -0.001825623][-0.0018250684 -0.0018275271 -0.0018295168 -0.0018303316 -0.0018291947 -0.001825624 -0.0018198065 -0.0018124758 -0.0018073444 -0.0018052945 -0.0018070779 -0.0018119389 -0.0018181095 -0.0018233 -0.00182576][-0.0018236135 -0.0018254563 -0.001827589 -0.0018287998 -0.00182869 -0.0018271961 -0.0018245379 -0.0018206132 -0.0018176053 -0.0018159103 -0.0018163179 -0.0018184842 -0.001821341 -0.0018241354 -0.0018254593][-0.0018212195 -0.0018223354 -0.0018242591 -0.0018258967 -0.0018265151 -0.0018264098 -0.001825747 -0.0018241991 -0.0018228025 -0.0018217603 -0.0018215714 -0.0018220274 -0.001822873 -0.0018240992 -0.0018244582][-0.0018188553 -0.0018189683 -0.0018203907 -0.0018219551 -0.0018230863 -0.001823913 -0.0018245616 -0.0018244855 -0.0018243311 -0.0018241483 -0.0018240194 -0.0018239432 -0.0018238467 -0.0018238227 -0.0018232513][-0.0018171738 -0.0018166044 -0.0018172202 -0.0018181875 -0.0018190199 -0.0018198977 -0.0018209185 -0.0018217032 -0.0018223933 -0.0018228889 -0.0018231191 -0.0018231556 -0.001822904 -0.0018223125 -0.0018213753]]...]
INFO - root - 2017-12-09 18:40:15.535451: step 48210, loss = 0.81, batch loss = 0.68 (9.1 examples/sec; 0.877 sec/batch; 69h:16m:13s remains)
INFO - root - 2017-12-09 18:40:23.990816: step 48220, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.879 sec/batch; 69h:23m:12s remains)
INFO - root - 2017-12-09 18:40:32.597106: step 48230, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.859 sec/batch; 67h:49m:56s remains)
INFO - root - 2017-12-09 18:40:41.354134: step 48240, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.882 sec/batch; 69h:36m:41s remains)
INFO - root - 2017-12-09 18:40:50.017803: step 48250, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.873 sec/batch; 68h:57m:57s remains)
INFO - root - 2017-12-09 18:40:58.651256: step 48260, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.866 sec/batch; 68h:24m:18s remains)
INFO - root - 2017-12-09 18:41:07.329013: step 48270, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.876 sec/batch; 69h:07m:35s remains)
INFO - root - 2017-12-09 18:41:16.082181: step 48280, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.887 sec/batch; 70h:00m:57s remains)
INFO - root - 2017-12-09 18:41:24.614376: step 48290, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 68h:30m:33s remains)
INFO - root - 2017-12-09 18:41:33.111761: step 48300, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.851 sec/batch; 67h:09m:12s remains)
2017-12-09 18:41:33.930296: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0018016499 -0.0018007772 -0.0018012526 -0.0018016838 -0.001801719 -0.0018015131 -0.0018012334 -0.0018009836 -0.0018007992 -0.0018008029 -0.0018010344 -0.0018013678 -0.0018017009 -0.00180194 -0.0018020475][-0.0018002708 -0.0017993138 -0.0017998911 -0.0018004739 -0.0018006261 -0.00180042 -0.0018000221 -0.0017995973 -0.0017992657 -0.0017992278 -0.0017995063 -0.0017999553 -0.0018004375 -0.0018007987 -0.0018010107][-0.0017994887 -0.0017985393 -0.0017991484 -0.0017998081 -0.0018000153 -0.0017998199 -0.0017993328 -0.001798782 -0.001798374 -0.0017983324 -0.001798632 -0.0017991451 -0.0017997737 -0.0018002898 -0.0018006024][-0.0017988648 -0.0017979266 -0.0017984716 -0.0017991014 -0.0017993363 -0.0017991567 -0.0017986106 -0.0017979788 -0.001797555 -0.001797526 -0.0017978207 -0.0017983635 -0.0017990887 -0.0017997099 -0.0018000889][-0.001798727 -0.0017976739 -0.0017980988 -0.0017986059 -0.0017987984 -0.0017986313 -0.0017980371 -0.001797328 -0.0017969325 -0.001796958 -0.0017972531 -0.0017978143 -0.0017985911 -0.0017992838 -0.0017996909][-0.0017985546 -0.0017973839 -0.0017976454 -0.0017980017 -0.00179818 -0.0017980842 -0.0017975355 -0.0017968703 -0.0017965683 -0.0017966622 -0.0017969719 -0.0017974791 -0.0017981969 -0.0017988564 -0.0017992449][-0.0017978352 -0.0017966251 -0.0017966731 -0.001796858 -0.001796981 -0.0017969584 -0.0017965424 -0.0017960083 -0.0017958813 -0.0017961323 -0.0017965129 -0.0017970423 -0.001797761 -0.0017984111 -0.0017988009][-0.0017972175 -0.0017958253 -0.0017955998 -0.00179559 -0.0017956056 -0.0017955927 -0.0017953047 -0.0017949601 -0.0017950376 -0.001795404 -0.0017958005 -0.0017963259 -0.0017970346 -0.0017976505 -0.0017980563][-0.0017969927 -0.001795313 -0.0017948472 -0.0017945647 -0.0017944619 -0.001794483 -0.0017944162 -0.0017944267 -0.0017947725 -0.0017952087 -0.0017955072 -0.0017958729 -0.0017963932 -0.0017968131 -0.0017971125][-0.001796886 -0.0017951303 -0.0017945135 -0.00179406 -0.0017938613 -0.0017938686 -0.0017939853 -0.001794295 -0.0017948208 -0.0017952495 -0.0017954276 -0.0017956077 -0.0017958996 -0.0017961236 -0.001796336][-0.0017969152 -0.0017952216 -0.0017945424 -0.0017941062 -0.0017938595 -0.0017937629 -0.0017938352 -0.0017941943 -0.0017947732 -0.0017951907 -0.0017953304 -0.001795435 -0.0017956357 -0.0017958151 -0.0017960155][-0.0017970775 -0.0017954321 -0.0017948784 -0.0017946011 -0.0017943779 -0.001794145 -0.0017940246 -0.0017942181 -0.0017946567 -0.0017949961 -0.0017951139 -0.0017952367 -0.0017954473 -0.0017956438 -0.0017958594][-0.0017975672 -0.0017959335 -0.0017954193 -0.0017952136 -0.0017950113 -0.0017947115 -0.0017944659 -0.001794521 -0.0017947685 -0.0017949076 -0.0017949088 -0.001794943 -0.0017950824 -0.0017952194 -0.0017953948][-0.0017984963 -0.0017966942 -0.0017961026 -0.0017958636 -0.001795637 -0.0017953095 -0.0017950311 -0.001794968 -0.001795019 -0.0017949861 -0.0017948687 -0.0017948166 -0.0017948608 -0.0017949413 -0.0017950783][-0.0017997979 -0.0017979168 -0.001797223 -0.0017969417 -0.00179662 -0.0017961768 -0.0017957772 -0.0017955288 -0.0017953775 -0.0017951807 -0.0017949861 -0.0017948784 -0.0017948734 -0.0017949735 -0.0017951522]]...]
INFO - root - 2017-12-09 18:41:42.547781: step 48310, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.857 sec/batch; 67h:40m:23s remains)
INFO - root - 2017-12-09 18:41:51.109942: step 48320, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.849 sec/batch; 67h:03m:06s remains)
INFO - root - 2017-12-09 18:41:59.833356: step 48330, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 68h:43m:21s remains)
INFO - root - 2017-12-09 18:42:08.600428: step 48340, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.885 sec/batch; 69h:51m:47s remains)
INFO - root - 2017-12-09 18:42:17.258927: step 48350, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.853 sec/batch; 67h:17m:31s remains)
INFO - root - 2017-12-09 18:42:25.957488: step 48360, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 68h:02m:37s remains)
INFO - root - 2017-12-09 18:42:34.610669: step 48370, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.860 sec/batch; 67h:50m:38s remains)
INFO - root - 2017-12-09 18:42:43.295990: step 48380, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.875 sec/batch; 69h:02m:22s remains)
INFO - root - 2017-12-09 18:42:51.894166: step 48390, loss = 0.81, batch loss = 0.68 (10.8 examples/sec; 0.738 sec/batch; 58h:14m:22s remains)
INFO - root - 2017-12-09 18:43:00.398417: step 48400, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.844 sec/batch; 66h:36m:54s remains)
2017-12-09 18:43:01.219354: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0018097289 -0.0018113476 -0.0018159688 -0.0018206208 -0.0018241296 -0.0018261569 -0.001826268 -0.0018250566 -0.0018232695 -0.0018212942 -0.0018197981 -0.0018192707 -0.0018197547 -0.00182019 -0.001820069][-0.001809002 -0.0018103981 -0.0018149369 -0.0018197588 -0.001823345 -0.0018252265 -0.0018249805 -0.0018232184 -0.001820666 -0.0018177931 -0.001815465 -0.0018142067 -0.0018142072 -0.0018144659 -0.0018145335][-0.0018087641 -0.0018100704 -0.0018144208 -0.001819082 -0.0018225409 -0.0018241019 -0.0018235004 -0.0018215025 -0.0018185729 -0.0018150763 -0.0018121717 -0.0018104299 -0.0018099268 -0.0018098393 -0.001809869][-0.0018086 -0.0018098226 -0.0018139791 -0.0018184262 -0.0018216623 -0.0018230042 -0.0018221661 -0.0018200105 -0.0018168022 -0.0018129559 -0.0018097562 -0.0018077107 -0.0018068978 -0.0018065865 -0.0018065497][-0.0018084393 -0.0018095379 -0.001813398 -0.0018175691 -0.001820613 -0.0018217762 -0.0018207944 -0.0018185732 -0.0018152457 -0.001811295 -0.0018080056 -0.0018058447 -0.0018048849 -0.0018045084 -0.0018044497][-0.001808301 -0.0018092201 -0.0018128137 -0.0018166738 -0.0018194396 -0.001820409 -0.0018192825 -0.00181708 -0.0018137994 -0.0018099924 -0.0018068304 -0.0018047071 -0.0018037074 -0.0018032817 -0.0018032063][-0.0018081319 -0.0018090465 -0.0018124215 -0.0018159873 -0.0018185043 -0.001819257 -0.0018179583 -0.001815805 -0.0018126713 -0.0018091201 -0.0018061619 -0.0018040901 -0.0018030743 -0.0018026195 -0.0018025419][-0.0018080451 -0.0018090123 -0.0018122487 -0.0018157137 -0.0018181849 -0.0018188888 -0.0018175155 -0.0018153953 -0.0018124235 -0.0018090556 -0.0018061434 -0.001804007 -0.0018029048 -0.0018023844 -0.0018022609][-0.001807979 -0.0018091885 -0.0018125765 -0.0018161562 -0.0018188683 -0.0018198041 -0.001818505 -0.0018162971 -0.0018131732 -0.0018097064 -0.0018066121 -0.0018042608 -0.0018029696 -0.0018023377 -0.0018022037][-0.0018079651 -0.0018095517 -0.0018132437 -0.0018171023 -0.001820296 -0.0018218695 -0.0018210433 -0.0018188343 -0.0018153377 -0.0018114337 -0.0018078916 -0.0018051476 -0.0018034794 -0.0018026289 -0.0018024248][-0.0018082244 -0.0018101669 -0.0018142937 -0.001818558 -0.0018223178 -0.0018246809 -0.0018245252 -0.0018223271 -0.0018183714 -0.0018138794 -0.0018097593 -0.0018064412 -0.0018042424 -0.0018030634 -0.0018027417][-0.0018085734 -0.0018107491 -0.0018153817 -0.0018200903 -0.0018243898 -0.0018274754 -0.0018279365 -0.0018257544 -0.001821392 -0.001816418 -0.0018117417 -0.0018077843 -0.0018050277 -0.0018035334 -0.0018030806][-0.0018088118 -0.001811151 -0.0018162229 -0.0018213916 -0.0018260926 -0.001829565 -0.0018304638 -0.0018283468 -0.0018238148 -0.0018185421 -0.0018134771 -0.0018090636 -0.0018059019 -0.0018041997 -0.0018036504][-0.0018088587 -0.0018113726 -0.0018166627 -0.0018220368 -0.0018267686 -0.0018302716 -0.0018313874 -0.0018294496 -0.0018250503 -0.0018198019 -0.0018147196 -0.0018102641 -0.0018070232 -0.0018052513 -0.0018046402][-0.0018089148 -0.0018112726 -0.0018163838 -0.0018217344 -0.0018261861 -0.0018294622 -0.0018306195 -0.0018289635 -0.0018250329 -0.0018201965 -0.0018155013 -0.001811333 -0.0018082884 -0.0018065561 -0.0018058965]]...]
INFO - root - 2017-12-09 18:43:09.727634: step 48410, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.876 sec/batch; 69h:05m:21s remains)
INFO - root - 2017-12-09 18:43:18.451482: step 48420, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.860 sec/batch; 67h:49m:51s remains)
INFO - root - 2017-12-09 18:43:26.920019: step 48430, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.853 sec/batch; 67h:18m:14s remains)
INFO - root - 2017-12-09 18:43:35.558353: step 48440, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 68h:41m:27s remains)
INFO - root - 2017-12-09 18:43:44.345081: step 48450, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.858 sec/batch; 67h:39m:36s remains)
INFO - root - 2017-12-09 18:43:52.840879: step 48460, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 68h:27m:12s remains)
INFO - root - 2017-12-09 18:44:01.555746: step 48470, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.895 sec/batch; 70h:36m:25s remains)
INFO - root - 2017-12-09 18:44:10.231813: step 48480, loss = 0.83, batch loss = 0.70 (9.1 examples/sec; 0.878 sec/batch; 69h:15m:36s remains)
INFO - root - 2017-12-09 18:44:18.989481: step 48490, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.837 sec/batch; 66h:01m:58s remains)
INFO - root - 2017-12-09 18:44:27.379433: step 48500, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.841 sec/batch; 66h:20m:37s remains)
2017-12-09 18:44:28.314528: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0018311546 -0.0018307809 -0.0018308625 -0.0018310293 -0.0018311361 -0.0018311561 -0.0018307312 -0.0018295778 -0.0018282434 -0.0018276882 -0.0018283885 -0.0018295672 -0.0018304657 -0.0018307362 -0.0018305563][-0.0018317989 -0.0018316258 -0.0018316297 -0.001831355 -0.0018313105 -0.0018305379 -0.0018279088 -0.0018231494 -0.0018184156 -0.0018173988 -0.0018212481 -0.0018265463 -0.0018303188 -0.0018317252 -0.0018314748][-0.0018305579 -0.0018274458 -0.0018242226 -0.0018216502 -0.0018211348 -0.0018186071 -0.001810637 -0.0017976308 -0.0017863356 -0.001786167 -0.0017984305 -0.0018140577 -0.0018254094 -0.0018307262 -0.0018318299][-0.0018178179 -0.0017993738 -0.0017800762 -0.0017677639 -0.0017648233 -0.0017597971 -0.001741958 -0.0017136224 -0.0016933425 -0.0017011261 -0.0017356589 -0.0017771919 -0.0018082123 -0.0018242759 -0.0018298344][-0.001775873 -0.0017142505 -0.0016497382 -0.0016080169 -0.0015940322 -0.0015827148 -0.0015472542 -0.0014926633 -0.0014611408 -0.0014921044 -0.0015794879 -0.0016824403 -0.0017617823 -0.0018058652 -0.00182405][-0.0016914732 -0.0015516047 -0.0014043258 -0.0013036645 -0.0012610918 -0.0012339496 -0.001172051 -0.0010815726 -0.0010383357 -0.0011106709 -0.0012889536 -0.001500105 -0.0016685996 -0.0017670423 -0.001810904][-0.0015826697 -0.0013515229 -0.0011052859 -0.00092795724 -0.000840975 -0.00078927609 -0.00069885864 -0.00057105767 -0.00051275454 -0.00062729814 -0.000910232 -0.0012552433 -0.0015400592 -0.0017123639 -0.0017918583][-0.0015044422 -0.0012152895 -0.00090231415 -0.00066657155 -0.00053996174 -0.00046665443 -0.00035963114 -0.00020836014 -0.00013113755 -0.00026022014 -0.00061021768 -0.0010552023 -0.0014334396 -0.0016666199 -0.0017755249][-0.0015097059 -0.0012292 -0.000919879 -0.00067911507 -0.00054515258 -0.00046910567 -0.00036406924 -0.00020887284 -0.00011481112 -0.00022371765 -0.00056814216 -0.0010227272 -0.0014150002 -0.001657981 -0.0017715766][-0.0015960465 -0.0013855023 -0.0011475958 -0.00095758215 -0.00084956712 -0.000786535 -0.00069757481 -0.000561227 -0.00046878739 -0.00054384291 -0.00081862113 -0.0011859258 -0.001500056 -0.0016926293 -0.0017821287][-0.0017068953 -0.0015869592 -0.0014483808 -0.0013363918 -0.0012722951 -0.0012304008 -0.0011653404 -0.0010644896 -0.00099442434 -0.0010377744 -0.0012120253 -0.0014416829 -0.0016327542 -0.0017473798 -0.0017999712][-0.0017872326 -0.0017385095 -0.0016808793 -0.0016336824 -0.0016059056 -0.0015830938 -0.0015447779 -0.0014875438 -0.0014494038 -0.0014703048 -0.0015545383 -0.0016606633 -0.0017449211 -0.0017937192 -0.001815783][-0.0018220844 -0.001809417 -0.0017941425 -0.001781552 -0.0017732563 -0.0017637794 -0.0017469046 -0.0017229316 -0.0017079309 -0.0017148113 -0.0017433929 -0.0017778232 -0.001803762 -0.0018182348 -0.0018249009][-0.0018298951 -0.0018285497 -0.0018271133 -0.0018258445 -0.0018243699 -0.0018217223 -0.0018165441 -0.0018096538 -0.0018054628 -0.0018066076 -0.0018126336 -0.0018193109 -0.0018240854 -0.0018267268 -0.0018280084][-0.0018294473 -0.0018293191 -0.0018295461 -0.001830003 -0.0018300524 -0.0018295143 -0.0018282962 -0.001826715 -0.0018258151 -0.0018260293 -0.001826914 -0.0018277678 -0.0018284578 -0.0018287037 -0.0018286176]]...]
INFO - root - 2017-12-09 18:44:36.845896: step 48510, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.842 sec/batch; 66h:24m:24s remains)
INFO - root - 2017-12-09 18:44:45.378309: step 48520, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.858 sec/batch; 67h:41m:25s remains)
INFO - root - 2017-12-09 18:44:53.848211: step 48530, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.860 sec/batch; 67h:49m:54s remains)
INFO - root - 2017-12-09 18:45:02.542776: step 48540, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.872 sec/batch; 68h:48m:41s remains)
INFO - root - 2017-12-09 18:45:11.134069: step 48550, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.851 sec/batch; 67h:05m:19s remains)
INFO - root - 2017-12-09 18:45:19.724230: step 48560, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.875 sec/batch; 68h:59m:25s remains)
INFO - root - 2017-12-09 18:45:28.341637: step 48570, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.863 sec/batch; 68h:03m:09s remains)
INFO - root - 2017-12-09 18:45:36.987957: step 48580, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.886 sec/batch; 69h:50m:27s remains)
INFO - root - 2017-12-09 18:45:45.727362: step 48590, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.900 sec/batch; 71h:00m:51s remains)
INFO - root - 2017-12-09 18:45:54.168124: step 48600, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.889 sec/batch; 70h:06m:02s remains)
2017-12-09 18:45:54.988826: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0018109371 -0.0018100155 -0.0018100153 -0.0018101498 -0.0018102706 -0.0018103016 -0.0018102446 -0.0018102117 -0.0018101556 -0.0018100799 -0.0018099942 -0.0018099067 -0.001809725 -0.0018095315 -0.0018092801][-0.0018114815 -0.001810654 -0.0018107084 -0.001810912 -0.0018110734 -0.0018110933 -0.0018109435 -0.0018107535 -0.0018105928 -0.0018104771 -0.0018103051 -0.0018101226 -0.0018098513 -0.0018095576 -0.001809188][-0.0018124414 -0.0018118115 -0.0018119218 -0.0018121937 -0.0018123714 -0.0018123181 -0.0018120251 -0.0018116577 -0.0018113833 -0.0018111852 -0.0018109106 -0.0018106103 -0.0018102698 -0.0018099183 -0.0018094884][-0.0018132582 -0.0018129416 -0.0018131137 -0.0018135337 -0.0018138009 -0.0018136464 -0.0018131549 -0.0018125088 -0.0018120243 -0.0018116313 -0.0018111961 -0.0018107451 -0.0018102636 -0.0018098233 -0.0018094049][-0.0018140641 -0.0018141083 -0.0018146795 -0.0018154511 -0.0018159125 -0.0018156356 -0.0018148088 -0.0018136569 -0.0018126571 -0.0018118352 -0.0018110826 -0.0018104288 -0.0018098594 -0.0018094034 -0.0018090482][-0.0018150185 -0.0018158488 -0.0018172055 -0.0018184214 -0.0018189872 -0.0018184128 -0.0018169509 -0.0018149375 -0.0018130947 -0.0018116368 -0.0018104555 -0.0018096045 -0.0018090112 -0.0018086645 -0.0018084622][-0.0018165333 -0.0018182199 -0.001820311 -0.0018220594 -0.0018225183 -0.0018214078 -0.0018191821 -0.0018162223 -0.0018134224 -0.0018111536 -0.0018094989 -0.0018085105 -0.0018079617 -0.0018077868 -0.001807787][-0.0018186948 -0.0018209148 -0.0018235365 -0.0018254253 -0.0018258073 -0.0018241849 -0.0018212822 -0.0018175836 -0.0018139787 -0.0018109067 -0.0018087078 -0.0018075081 -0.0018069758 -0.001806956 -0.0018071821][-0.0018213789 -0.0018235609 -0.0018262999 -0.0018281814 -0.0018284017 -0.0018264035 -0.0018230864 -0.0018189095 -0.0018148074 -0.0018111347 -0.0018084304 -0.0018069855 -0.0018064235 -0.0018065309 -0.0018069054][-0.0018246125 -0.0018262363 -0.0018284825 -0.0018299805 -0.0018299251 -0.0018278419 -0.0018245856 -0.0018203406 -0.0018159971 -0.0018118829 -0.0018086396 -0.0018068453 -0.0018061569 -0.0018062838 -0.0018067631][-0.0018275005 -0.0018279888 -0.001829064 -0.0018297585 -0.0018295241 -0.0018276834 -0.0018250309 -0.0018213105 -0.001817102 -0.00181276 -0.0018091205 -0.0018069948 -0.0018061644 -0.0018062894 -0.0018068298][-0.0018294982 -0.0018283511 -0.0018278781 -0.0018275811 -0.0018272256 -0.0018259707 -0.0018242619 -0.0018215441 -0.0018177928 -0.0018135193 -0.0018096722 -0.0018073262 -0.0018063552 -0.0018064653 -0.0018070129][-0.0018305132 -0.0018279534 -0.0018262035 -0.0018252562 -0.0018248615 -0.0018241731 -0.0018233893 -0.0018214668 -0.0018181732 -0.0018139505 -0.0018100137 -0.0018075572 -0.0018064806 -0.00180653 -0.001807051][-0.0018312098 -0.0018276051 -0.0018248647 -0.0018234003 -0.0018229383 -0.0018226672 -0.0018224758 -0.0018209768 -0.0018179854 -0.0018139549 -0.0018101152 -0.0018077139 -0.0018066028 -0.0018065613 -0.0018070159][-0.0018319647 -0.0018278966 -0.0018247843 -0.0018230259 -0.0018225302 -0.0018223883 -0.0018222069 -0.0018206344 -0.0018176804 -0.0018139121 -0.0018103292 -0.0018079389 -0.0018067992 -0.0018066562 -0.001807034]]...]
INFO - root - 2017-12-09 18:46:03.643236: step 48610, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 67h:56m:37s remains)
INFO - root - 2017-12-09 18:46:12.209556: step 48620, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 67h:56m:37s remains)
INFO - root - 2017-12-09 18:46:20.695706: step 48630, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.841 sec/batch; 66h:20m:44s remains)
INFO - root - 2017-12-09 18:46:29.315023: step 48640, loss = 0.81, batch loss = 0.68 (9.3 examples/sec; 0.863 sec/batch; 68h:03m:18s remains)
INFO - root - 2017-12-09 18:46:37.958807: step 48650, loss = 0.83, batch loss = 0.70 (9.1 examples/sec; 0.875 sec/batch; 68h:57m:48s remains)
INFO - root - 2017-12-09 18:46:46.553637: step 48660, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 68h:28m:41s remains)
INFO - root - 2017-12-09 18:46:55.146135: step 48670, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 67h:55m:22s remains)
INFO - root - 2017-12-09 18:47:03.550318: step 48680, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.839 sec/batch; 66h:06m:35s remains)
INFO - root - 2017-12-09 18:47:12.193636: step 48690, loss = 0.82, batch loss = 0.69 (8.8 examples/sec; 0.906 sec/batch; 71h:24m:51s remains)
INFO - root - 2017-12-09 18:47:20.657182: step 48700, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 68h:19m:15s remains)
2017-12-09 18:47:21.597188: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0018068242 -0.0018054119 -0.0018036976 -0.0017750458 -0.0017649216 -0.0017830902 -0.0018072652 -0.0018061672 -0.0017564199 -0.0016459213 -0.0014777326 -0.001307315 -0.0011984098 -0.0012191292 -0.0013135502][-0.0018002888 -0.001812315 -0.0018257453 -0.0018139138 -0.0018068576 -0.0018079721 -0.0018101992 -0.0017819938 -0.0016855315 -0.0014844087 -0.0011900734 -0.00089495227 -0.0006964046 -0.00070190232 -0.00082100136][-0.0017848626 -0.0018100522 -0.0018323652 -0.0018324995 -0.001827594 -0.0018209849 -0.0018109734 -0.0017578733 -0.0016169291 -0.0013307421 -0.00092048506 -0.00049944746 -0.00018577406 -0.00011736585 -0.00019022007][-0.0017308017 -0.0017765274 -0.0018215374 -0.0018328564 -0.0018288505 -0.001821012 -0.0018074864 -0.0017439172 -0.0015867766 -0.0012694853 -0.00081763894 -0.00033161638 8.2696439e-05 0.0002922389 0.00038151292][-0.0016157903 -0.0016988633 -0.0017975239 -0.0018312698 -0.0018290604 -0.0018219454 -0.0018093694 -0.0017546432 -0.0016228085 -0.0013573095 -0.000978736 -0.00053286424 -8.2207262e-05 0.00030332885 0.0006577085][-0.0014424508 -0.0015794158 -0.0017586898 -0.0018294048 -0.0018314616 -0.0018267088 -0.0018166609 -0.0017830097 -0.0017063827 -0.0015502975 -0.0013196436 -0.0010043915 -0.00060222659 -9.2517817e-05 0.00053317577][-0.0012661074 -0.0014576436 -0.0017161211 -0.0018260927 -0.0018337248 -0.0018332627 -0.0018281032 -0.0018130956 -0.0017793748 -0.0017141114 -0.0016126962 -0.0014410105 -0.0011611645 -0.00065184408 0.00013483211][-0.0011711104 -0.0013894847 -0.0016889912 -0.001821703 -0.0018318169 -0.0018327296 -0.0018318007 -0.0018285357 -0.0018200693 -0.0017984622 -0.0017535205 -0.0016605383 -0.0014888868 -0.0010672691 -0.00026550284][-0.0012391272 -0.0014346913 -0.001700343 -0.0018201288 -0.0018297326 -0.0018307324 -0.0018299593 -0.00182752 -0.0018204565 -0.0018003145 -0.0017620814 -0.0016859222 -0.0015789273 -0.0012458301 -0.00050730235][-0.0014398969 -0.0015684069 -0.0017415336 -0.0018214904 -0.0018286404 -0.0018291865 -0.0018280194 -0.0018247939 -0.0018149188 -0.0017884123 -0.0017387819 -0.0016497526 -0.0015565389 -0.0012735552 -0.00062528357][-0.0016584744 -0.0017117568 -0.0017873612 -0.0018241903 -0.0018282561 -0.0018287011 -0.0018273033 -0.0018238137 -0.0018129221 -0.0017825763 -0.0017279665 -0.001627807 -0.0015336043 -0.0012824505 -0.00074044184][-0.001784934 -0.0017964338 -0.0018130229 -0.0018265848 -0.0018280663 -0.0018280103 -0.0018268859 -0.0018244715 -0.0018166014 -0.001795608 -0.0017521442 -0.0016644257 -0.0015706592 -0.001355878 -0.00093353732][-0.0018265639 -0.0018231563 -0.0018227808 -0.0018276824 -0.0018277406 -0.0018272739 -0.0018263634 -0.0018246067 -0.0018192857 -0.0018071722 -0.0017843575 -0.0017308493 -0.0016586839 -0.0014956713 -0.0011971875][-0.0018284988 -0.0018254017 -0.001823951 -0.0018273484 -0.0018270178 -0.0018264198 -0.0018260134 -0.0018258725 -0.0018245311 -0.0018199764 -0.0018075668 -0.0017824408 -0.0017475205 -0.0016584877 -0.0014799752][-0.0018293074 -0.0018281478 -0.0018275554 -0.0018270243 -0.0018264712 -0.001825978 -0.0018257321 -0.0018255824 -0.0018255216 -0.0018255233 -0.001823699 -0.0018166415 -0.0018005585 -0.0017681886 -0.0016928163]]...]
INFO - root - 2017-12-09 18:47:30.319908: step 48710, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.880 sec/batch; 69h:22m:29s remains)
INFO - root - 2017-12-09 18:47:38.880822: step 48720, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.838 sec/batch; 66h:01m:33s remains)
INFO - root - 2017-12-09 18:47:47.222677: step 48730, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 67h:55m:50s remains)
INFO - root - 2017-12-09 18:47:55.732526: step 48740, loss = 0.82, batch loss = 0.69 (9.7 examples/sec; 0.826 sec/batch; 65h:08m:03s remains)
INFO - root - 2017-12-09 18:48:04.368109: step 48750, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.866 sec/batch; 68h:17m:25s remains)
INFO - root - 2017-12-09 18:48:12.901263: step 48760, loss = 0.83, batch loss = 0.70 (9.0 examples/sec; 0.885 sec/batch; 69h:44m:03s remains)
INFO - root - 2017-12-09 18:48:21.648375: step 48770, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.882 sec/batch; 69h:32m:46s remains)
INFO - root - 2017-12-09 18:48:30.260591: step 48780, loss = 0.81, batch loss = 0.68 (9.7 examples/sec; 0.821 sec/batch; 64h:40m:37s remains)
INFO - root - 2017-12-09 18:48:38.932120: step 48790, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.850 sec/batch; 66h:59m:53s remains)
INFO - root - 2017-12-09 18:48:47.387307: step 48800, loss = 0.83, batch loss = 0.70 (9.5 examples/sec; 0.842 sec/batch; 66h:22m:20s remains)
2017-12-09 18:48:48.247440: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.088356152 0.10258944 0.11226417 0.11741255 0.11798881 0.11508711 0.1096244 0.10353294 0.097486719 0.092468731 0.088120975 0.084854655 0.08209262 0.079776272 0.0777445][0.084851556 0.10124859 0.11337282 0.12086116 0.1228973 0.12077466 0.11561636 0.10910676 0.10243933 0.096668117 0.091786407 0.088237934 0.085602194 0.083485179 0.081694447][0.077006318 0.094012111 0.10771856 0.11730009 0.12146753 0.12116407 0.11709535 0.11100988 0.10413231 0.097742096 0.09241128 0.08844348 0.085648738 0.083554529 0.082002632][0.068585813 0.086089715 0.10123744 0.1126091 0.11888542 0.12058608 0.11812945 0.11301252 0.1062695 0.099554837 0.093680993 0.08909972 0.085997313 0.083716467 0.082266428][0.059771378 0.077693127 0.094200641 0.10742476 0.1158787 0.1195153 0.11887376 0.1150805 0.10903044 0.10235635 0.0959386 0.090604439 0.086719021 0.083842896 0.08213371][0.05142292 0.069174647 0.08644022 0.10128994 0.11172891 0.11720788 0.11825294 0.11576436 0.11077778 0.10465348 0.098244414 0.092468962 0.087726556 0.084178537 0.081864648][0.042723466 0.059928756 0.0775305 0.093378216 0.10525894 0.11252657 0.11519182 0.1139902 0.11010858 0.10474283 0.098694906 0.09290617 0.087729126 0.083607443 0.080535136][0.034676008 0.0506165 0.067633018 0.083523691 0.0960065 0.10422116 0.10800181 0.10819185 0.10567266 0.10149696 0.096322805 0.091005549 0.085750364 0.0812927 0.077730022][0.027163159 0.041336421 0.057116859 0.072316505 0.084595934 0.093002237 0.097281232 0.098373763 0.097021528 0.094274685 0.090332046 0.085815251 0.080827557 0.076274179 0.0722738][0.020620223 0.032389112 0.046115972 0.05973307 0.071077995 0.078983456 0.083188944 0.084769741 0.084412046 0.083033733 0.080467023 0.07719107 0.073014952 0.0686507 0.0643361][0.015450245 0.024521558 0.03572116 0.04729633 0.057315361 0.064389728 0.06833528 0.069928154 0.0698907 0.069303766 0.067832716 0.06577377 0.0626212 0.058982421 0.054993913][0.011003144 0.017472839 0.025863755 0.035056446 0.04354237 0.04997969 0.053961873 0.055833265 0.056277379 0.056202345 0.055362564 0.054078814 0.05173419 0.048771132 0.045255534][0.0068011507 0.011115297 0.016955517 0.023765279 0.030553471 0.03626363 0.0403186 0.042786233 0.044024397 0.044641204 0.044456482 0.043662917 0.041812804 0.039273161 0.036138996][0.0029109439 0.0052527171 0.0086484831 0.013091406 0.018055525 0.022892497 0.027073644 0.030442633 0.032797642 0.034297522 0.034947887 0.034707807 0.033302214 0.030992858 0.028032171][-3.9230566e-05 0.00099972123 0.0026727116 0.005029954 0.0079823919 0.011357768 0.014856301 0.018296273 0.02137078 0.02396109 0.025662923 0.026184574 0.025438491 0.023584425 0.020987341]]...]
INFO - root - 2017-12-09 18:48:56.768611: step 48810, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.853 sec/batch; 67h:11m:01s remains)
INFO - root - 2017-12-09 18:49:05.420225: step 48820, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 68h:18m:06s remains)
INFO - root - 2017-12-09 18:49:13.872743: step 48830, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 68h:39m:37s remains)
INFO - root - 2017-12-09 18:49:22.444578: step 48840, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.859 sec/batch; 67h:42m:20s remains)
INFO - root - 2017-12-09 18:49:31.052296: step 48850, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.872 sec/batch; 68h:44m:20s remains)
INFO - root - 2017-12-09 18:49:39.435973: step 48860, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.870 sec/batch; 68h:35m:04s remains)
INFO - root - 2017-12-09 18:49:47.945564: step 48870, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.880 sec/batch; 69h:22m:10s remains)
INFO - root - 2017-12-09 18:49:56.638592: step 48880, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.860 sec/batch; 67h:47m:24s remains)
INFO - root - 2017-12-09 18:50:05.300845: step 48890, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.879 sec/batch; 69h:14m:30s remains)
INFO - root - 2017-12-09 18:50:13.807765: step 48900, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.870 sec/batch; 68h:31m:41s remains)
2017-12-09 18:50:14.678667: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0018071719 -0.0018059808 -0.0018058779 -0.0018058448 -0.0018057902 -0.001805776 -0.0018059611 -0.0018066656 -0.0018085727 -0.0018120479 -0.0018171336 -0.0018235494 -0.0018300769 -0.0018359139 -0.001840597][-0.0018063192 -0.0018052221 -0.0018051313 -0.0018050949 -0.0018050445 -0.0018050402 -0.0018052305 -0.0018059581 -0.0018078478 -0.0018113239 -0.0018164354 -0.0018228679 -0.0018294723 -0.001835421 -0.0018402222][-0.0018060718 -0.0018051767 -0.0018050515 -0.0018049919 -0.0018049154 -0.001804888 -0.0018050543 -0.0018057746 -0.0018076401 -0.0018111208 -0.0018162535 -0.0018226937 -0.001829309 -0.0018352823 -0.0018400972][-0.0018058937 -0.0018051438 -0.0018049801 -0.0018048857 -0.0018047729 -0.0018047153 -0.0018048555 -0.0018055646 -0.0018074111 -0.0018108845 -0.0018160238 -0.0018224779 -0.001829128 -0.0018351602 -0.001840021][-0.001805881 -0.0018051334 -0.0018048963 -0.0018047622 -0.0018046091 -0.0018045207 -0.0018046411 -0.0018053442 -0.0018071725 -0.0018106197 -0.0018157468 -0.0018222202 -0.0018289236 -0.0018350291 -0.0018399504][-0.0018059573 -0.001805108 -0.0018047936 -0.0018046293 -0.0018044495 -0.001804339 -0.0018044392 -0.0018051225 -0.0018069358 -0.0018103486 -0.0018154713 -0.0018219767 -0.00182876 -0.0018349515 -0.0018399173][-0.0018061056 -0.0018050795 -0.0018047104 -0.0018045362 -0.0018043485 -0.0018042236 -0.0018043075 -0.0018049527 -0.0018067391 -0.0018101244 -0.0018152553 -0.0018218185 -0.0018286847 -0.0018349417 -0.0018399452][-0.0018062871 -0.0018050602 -0.0018046598 -0.0018044915 -0.001804317 -0.001804191 -0.0018042598 -0.0018048679 -0.0018066427 -0.00181002 -0.0018151564 -0.0018217572 -0.0018286684 -0.0018349515 -0.0018399605][-0.0018063661 -0.0018049862 -0.0018046452 -0.00180449 -0.0018043207 -0.0018041909 -0.0018042438 -0.0018048207 -0.0018065941 -0.0018099818 -0.0018151469 -0.0018217789 -0.0018287011 -0.0018349787 -0.0018399775][-0.0018062887 -0.0018049396 -0.0018046732 -0.001804546 -0.0018043887 -0.0018042579 -0.001804302 -0.00180486 -0.0018066377 -0.0018100461 -0.0018152328 -0.0018218782 -0.0018287776 -0.0018350118 -0.0018399718][-0.0018062296 -0.0018049376 -0.0018047201 -0.0018046245 -0.0018044852 -0.0018043697 -0.0018044257 -0.0018049829 -0.0018067609 -0.0018101829 -0.0018153766 -0.0018220081 -0.0018288564 -0.0018350339 -0.0018399566][-0.0018060622 -0.0018049254 -0.0018047503 -0.0018046744 -0.0018045525 -0.0018044595 -0.0018045438 -0.0018051347 -0.0018069346 -0.0018103705 -0.0018155483 -0.0018221326 -0.0018289003 -0.0018350022 -0.0018398867][-0.0018059032 -0.0018049229 -0.0018047614 -0.0018046957 -0.0018045868 -0.0018045154 -0.0018046366 -0.0018052752 -0.0018071058 -0.0018105529 -0.0018156981 -0.0018222067 -0.0018288813 -0.0018349132 -0.0018397685][-0.0018058331 -0.0018049626 -0.001804749 -0.001804693 -0.0018045998 -0.0018045506 -0.0018046976 -0.0018053758 -0.001807224 -0.0018106637 -0.0018157756 -0.0018222169 -0.0018288263 -0.0018348079 -0.0018396517][-0.0018058016 -0.0018050042 -0.0018047062 -0.0018046589 -0.0018045873 -0.00180456 -0.0018047336 -0.0018054382 -0.001807294 -0.0018107082 -0.0018157745 -0.0018221708 -0.0018287532 -0.0018347239 -0.0018395749]]...]
INFO - root - 2017-12-09 18:50:23.427805: step 48910, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.894 sec/batch; 70h:26m:40s remains)
INFO - root - 2017-12-09 18:50:32.147540: step 48920, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.880 sec/batch; 69h:17m:58s remains)
INFO - root - 2017-12-09 18:50:40.587715: step 48930, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.865 sec/batch; 68h:10m:03s remains)
INFO - root - 2017-12-09 18:50:49.268469: step 48940, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 68h:17m:47s remains)
INFO - root - 2017-12-09 18:50:58.147260: step 48950, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 68h:22m:59s remains)
INFO - root - 2017-12-09 18:51:06.703348: step 48960, loss = 0.83, batch loss = 0.70 (9.2 examples/sec; 0.867 sec/batch; 68h:15m:57s remains)
INFO - root - 2017-12-09 18:51:15.339703: step 48970, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.866 sec/batch; 68h:12m:10s remains)
INFO - root - 2017-12-09 18:51:24.133919: step 48980, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.854 sec/batch; 67h:15m:04s remains)
INFO - root - 2017-12-09 18:51:32.748876: step 48990, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.848 sec/batch; 66h:48m:39s remains)
INFO - root - 2017-12-09 18:51:41.085299: step 49000, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.834 sec/batch; 65h:42m:00s remains)
2017-12-09 18:51:41.989060: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.22957939 0.23870698 0.24336475 0.2431138 0.23662928 0.22600046 0.21258807 0.19698285 0.18056637 0.16061731 0.13891587 0.11505825 0.090890579 0.067425661 0.047339633][0.23308583 0.24359657 0.24906702 0.24918322 0.2427178 0.23148824 0.21690981 0.1993719 0.18040703 0.1579556 0.13424078 0.10916794 0.085730933 0.064891569 0.048852552][0.23269957 0.24434522 0.25045019 0.25167632 0.24634427 0.2359581 0.22195059 0.20416814 0.18419088 0.16042054 0.13567425 0.11086448 0.089233086 0.071563162 0.059652038][0.23062539 0.2435732 0.25111395 0.25413337 0.2505444 0.24217123 0.22908869 0.21268561 0.19355235 0.17049788 0.14686558 0.12336989 0.10424743 0.089032046 0.08039742][0.2292615 0.2433781 0.2521525 0.25694156 0.256512 0.25139672 0.24077222 0.22603065 0.20804746 0.18723279 0.16469051 0.14299795 0.12643211 0.11460859 0.10963659][0.22825083 0.24347475 0.25385213 0.26067573 0.26316255 0.26189667 0.25490889 0.24377131 0.22856003 0.21004544 0.18949616 0.16996299 0.15520518 0.14555833 0.14341499][0.22478667 0.24141437 0.25362271 0.26315147 0.26902658 0.27147317 0.26837355 0.26089528 0.24877225 0.23308627 0.21496594 0.19792534 0.18531412 0.17757167 0.17704919][0.2211756 0.23828885 0.25150493 0.26280329 0.27129984 0.27686983 0.27702627 0.27295887 0.2634404 0.25018638 0.23430653 0.21931531 0.2082385 0.20158207 0.2016675][0.21852903 0.23605561 0.25007328 0.26188478 0.27150688 0.27870724 0.28068733 0.27858415 0.27062404 0.25944611 0.24564998 0.23246826 0.22241691 0.21589586 0.21531393][0.21483207 0.23307766 0.24777222 0.25959069 0.2694349 0.2764484 0.27864674 0.27668312 0.26920021 0.259023 0.24660654 0.23500124 0.22583468 0.21943301 0.21759546][0.20850214 0.22714554 0.24229862 0.25400537 0.26333255 0.26890951 0.2702297 0.26756716 0.26006651 0.2499613 0.23831421 0.22752473 0.21863189 0.21172428 0.20812385][0.19896008 0.21736468 0.23198415 0.24278055 0.2509841 0.254923 0.25493151 0.25113139 0.24351181 0.23374289 0.22274072 0.21231902 0.20346738 0.19597884 0.19064569][0.18537615 0.20197389 0.21494341 0.22386448 0.23059785 0.23281422 0.23178561 0.22759762 0.22028144 0.21135516 0.2011975 0.19149317 0.18294927 0.17510946 0.16854824][0.1688128 0.1832054 0.193854 0.20058024 0.20537013 0.20590119 0.20415485 0.19962075 0.19302018 0.18528444 0.17647013 0.16792496 0.15991028 0.15227197 0.14526327][0.15209658 0.16363698 0.17143171 0.17597516 0.17886752 0.17796335 0.17549932 0.17073475 0.16474591 0.15773547 0.1499176 0.1423512 0.13514014 0.1282253 0.12159827]]...]
INFO - root - 2017-12-09 18:51:50.634102: step 49010, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.859 sec/batch; 67h:38m:57s remains)
INFO - root - 2017-12-09 18:51:59.208797: step 49020, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.870 sec/batch; 68h:29m:04s remains)
INFO - root - 2017-12-09 18:52:07.533691: step 49030, loss = 0.82, batch loss = 0.69 (10.4 examples/sec; 0.769 sec/batch; 60h:34m:07s remains)
INFO - root - 2017-12-09 18:52:16.107601: step 49040, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.840 sec/batch; 66h:09m:39s remains)
INFO - root - 2017-12-09 18:52:24.563948: step 49050, loss = 0.82, batch loss = 0.69 (9.7 examples/sec; 0.823 sec/batch; 64h:45m:40s remains)
INFO - root - 2017-12-09 18:52:33.012195: step 49060, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.880 sec/batch; 69h:14m:53s remains)
INFO - root - 2017-12-09 18:52:41.627018: step 49070, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.860 sec/batch; 67h:41m:00s remains)
INFO - root - 2017-12-09 18:52:50.326227: step 49080, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.849 sec/batch; 66h:50m:25s remains)
INFO - root - 2017-12-09 18:52:58.919473: step 49090, loss = 0.83, batch loss = 0.70 (9.1 examples/sec; 0.879 sec/batch; 69h:09m:38s remains)
INFO - root - 2017-12-09 18:53:07.491430: step 49100, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.865 sec/batch; 68h:04m:50s remains)
2017-12-09 18:53:08.395093: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0018344553 -0.0018347799 -0.0018350234 -0.0018350546 -0.0018349882 -0.0018348166 -0.001834865 -0.0018351607 -0.0018352552 -0.0018352222 -0.0018349101 -0.0018342952 -0.0018332994 -0.0018318272 -0.0018301144][-0.0018369909 -0.0018376423 -0.0018381726 -0.0018383225 -0.0018382093 -0.0018379218 -0.0018377915 -0.0018379531 -0.0018379083 -0.0018377478 -0.0018372554 -0.0018363447 -0.001834955 -0.0018330753 -0.0018309116][-0.001838924 -0.0018396676 -0.0018402794 -0.001840361 -0.0018401799 -0.0018399564 -0.0018398388 -0.0018399261 -0.0018398408 -0.0018394368 -0.0018387494 -0.0018376604 -0.0018361371 -0.0018340034 -0.0018315632][-0.0018398053 -0.0018402283 -0.0018406929 -0.0018407324 -0.0018406454 -0.0018407403 -0.0018409311 -0.0018412201 -0.0018411701 -0.0018405176 -0.0018396981 -0.0018383616 -0.0018365715 -0.0018342963 -0.0018318326][-0.0018394768 -0.0018390424 -0.0018387629 -0.001838367 -0.0018382907 -0.0018389019 -0.001839834 -0.001840641 -0.0018408907 -0.0018403562 -0.0018396376 -0.0018383452 -0.0018365198 -0.0018343297 -0.0018320475][-0.0018379843 -0.0018366146 -0.0018355905 -0.0018346432 -0.0018344303 -0.0018354605 -0.0018371402 -0.0018387998 -0.0018396825 -0.0018395094 -0.0018390649 -0.0018379459 -0.0018363155 -0.0018343348 -0.0018321691][-0.0018360247 -0.0018337346 -0.0018319926 -0.0018305009 -0.0018302432 -0.0018317093 -0.0018340232 -0.001836372 -0.0018379324 -0.0018382807 -0.0018382425 -0.0018374664 -0.0018361089 -0.0018343938 -0.0018322958][-0.0018342434 -0.0018313657 -0.0018289569 -0.0018270963 -0.0018269543 -0.0018286958 -0.0018312919 -0.0018339467 -0.0018361798 -0.0018371529 -0.0018373409 -0.0018368763 -0.001835869 -0.0018344654 -0.0018324991][-0.0018333334 -0.0018304616 -0.0018280006 -0.0018260772 -0.0018261197 -0.0018278477 -0.0018301825 -0.0018325617 -0.0018349336 -0.0018361986 -0.0018365146 -0.0018363745 -0.0018356116 -0.0018344232 -0.0018325947][-0.0018329946 -0.0018304538 -0.0018282362 -0.0018264443 -0.0018263806 -0.0018277648 -0.0018295995 -0.0018314676 -0.00183343 -0.0018347282 -0.0018351168 -0.0018351384 -0.0018346625 -0.0018336407 -0.0018321609][-0.0018325492 -0.0018304677 -0.0018287536 -0.0018275561 -0.0018275161 -0.0018283244 -0.0018293918 -0.0018305541 -0.0018318644 -0.0018327411 -0.0018331455 -0.0018332563 -0.0018328273 -0.0018319531 -0.0018308864][-0.0018318812 -0.0018302546 -0.0018291093 -0.0018284128 -0.0018284619 -0.001829043 -0.001829516 -0.0018299273 -0.0018305406 -0.0018308719 -0.0018310306 -0.0018310882 -0.0018307811 -0.0018301752 -0.0018293546][-0.0018310489 -0.0018298869 -0.0018292116 -0.0018288392 -0.0018289152 -0.0018293135 -0.0018293597 -0.0018292009 -0.0018292386 -0.0018291199 -0.0018289511 -0.0018289059 -0.0018287498 -0.0018284367 -0.0018279027][-0.0018299618 -0.0018290561 -0.0018286061 -0.00182831 -0.0018283012 -0.0018285281 -0.0018283996 -0.0018280393 -0.0018278206 -0.0018275536 -0.0018272393 -0.001827053 -0.0018269803 -0.001826828 -0.0018265087][-0.001828184 -0.0018274004 -0.0018269926 -0.0018267115 -0.0018266288 -0.0018267905 -0.0018267554 -0.0018265733 -0.0018263606 -0.0018260966 -0.0018257925 -0.0018255685 -0.00182551 -0.0018254211 -0.0018252111]]...]
INFO - root - 2017-12-09 18:53:17.022538: step 49110, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 67h:52m:10s remains)
INFO - root - 2017-12-09 18:53:25.682435: step 49120, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 68h:13m:01s remains)
INFO - root - 2017-12-09 18:53:34.379971: step 49130, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.896 sec/batch; 70h:30m:25s remains)
INFO - root - 2017-12-09 18:53:42.888793: step 49140, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.847 sec/batch; 66h:38m:29s remains)
INFO - root - 2017-12-09 18:53:51.580067: step 49150, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.878 sec/batch; 69h:06m:30s remains)
INFO - root - 2017-12-09 18:54:00.158275: step 49160, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.854 sec/batch; 67h:11m:40s remains)
INFO - root - 2017-12-09 18:54:08.748024: step 49170, loss = 0.81, batch loss = 0.68 (9.3 examples/sec; 0.865 sec/batch; 68h:02m:54s remains)
INFO - root - 2017-12-09 18:54:17.223965: step 49180, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.848 sec/batch; 66h:44m:36s remains)
INFO - root - 2017-12-09 18:54:25.678351: step 49190, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.866 sec/batch; 68h:10m:54s remains)
INFO - root - 2017-12-09 18:54:33.984943: step 49200, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.840 sec/batch; 66h:06m:57s remains)
2017-12-09 18:54:34.970779: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0018233778 -0.0018227542 -0.0018226429 -0.0018225365 -0.0018223413 -0.0018221752 -0.001822007 -0.001821872 -0.0018217688 -0.0018216447 -0.0018215133 -0.001821351 -0.0018211884 -0.0018210338 -0.0018208987][-0.0018229932 -0.0018222998 -0.0018221245 -0.0018219196 -0.0018216893 -0.00182161 -0.001821585 -0.0018215607 -0.0018215912 -0.0018216254 -0.0018215989 -0.0018214624 -0.0018212501 -0.0018210278 -0.0018208099][-0.0018222668 -0.0018215794 -0.0018213675 -0.0018211622 -0.0018209809 -0.0018210001 -0.0018211245 -0.0018213161 -0.0018216035 -0.0018219112 -0.0018221356 -0.0018221209 -0.0018218847 -0.0018215813 -0.0018212391][-0.001821439 -0.0018208725 -0.0018206893 -0.0018205838 -0.0018205362 -0.0018205717 -0.0018206983 -0.0018209568 -0.0018214228 -0.0018219922 -0.001822509 -0.0018226751 -0.0018224773 -0.0018221044 -0.0018216413][-0.0018208857 -0.0018204714 -0.0018204139 -0.001820464 -0.0018205554 -0.0018205805 -0.0018206316 -0.0018208555 -0.0018214018 -0.0018221468 -0.0018229298 -0.0018233197 -0.0018231701 -0.0018227585 -0.0018222112][-0.0018205448 -0.0018202245 -0.0018202899 -0.0018205087 -0.001820739 -0.0018208062 -0.0018208433 -0.0018210534 -0.001821694 -0.0018226325 -0.0018235962 -0.0018240911 -0.0018239577 -0.0018235474 -0.0018229296][-0.0018205619 -0.001820263 -0.0018203527 -0.0018206411 -0.0018209233 -0.0018210417 -0.0018210924 -0.0018213224 -0.0018220763 -0.0018231368 -0.0018241674 -0.0018247085 -0.0018246073 -0.0018242165 -0.0018235028][-0.0018211645 -0.0018207621 -0.0018207625 -0.0018209929 -0.0018211977 -0.0018212763 -0.0018213263 -0.0018215468 -0.0018222149 -0.00182319 -0.0018241375 -0.0018246904 -0.0018247403 -0.0018244331 -0.0018236928][-0.0018219212 -0.0018213807 -0.0018212862 -0.0018214432 -0.0018215848 -0.0018216611 -0.001821729 -0.0018218721 -0.0018223065 -0.0018229228 -0.0018235652 -0.0018240232 -0.0018241396 -0.001823955 -0.0018233873][-0.0018223723 -0.0018216907 -0.0018214718 -0.0018215256 -0.001821664 -0.0018218528 -0.0018220479 -0.0018221517 -0.0018223156 -0.0018225538 -0.0018228368 -0.0018230322 -0.001823006 -0.001822894 -0.0018226065][-0.0018223793 -0.0018215604 -0.0018212585 -0.0018213353 -0.0018216089 -0.0018219441 -0.001822188 -0.0018221952 -0.0018220601 -0.001821894 -0.0018217332 -0.0018215689 -0.0018213919 -0.0018213848 -0.0018214027][-0.0018221138 -0.0018212802 -0.0018210745 -0.0018212664 -0.0018216764 -0.0018220594 -0.001822239 -0.0018221394 -0.0018217477 -0.001821217 -0.0018206472 -0.0018202224 -0.0018200272 -0.0018201114 -0.0018203226][-0.0018214947 -0.001820839 -0.0018208373 -0.001821237 -0.0018217892 -0.0018222296 -0.0018223582 -0.0018221723 -0.001821626 -0.0018208828 -0.0018201106 -0.0018195708 -0.0018193973 -0.0018195554 -0.0018198873][-0.0018207431 -0.0018202581 -0.0018203834 -0.0018209069 -0.001821525 -0.0018219511 -0.0018220163 -0.0018217893 -0.0018212606 -0.0018205625 -0.0018198505 -0.0018193658 -0.0018192345 -0.0018194155 -0.001819785][-0.0018202984 -0.0018198785 -0.0018200084 -0.0018204516 -0.0018209351 -0.00182121 -0.0018211892 -0.001820993 -0.0018206123 -0.0018201047 -0.0018195732 -0.0018192511 -0.0018192038 -0.0018193792 -0.0018196855]]...]
INFO - root - 2017-12-09 18:54:43.574947: step 49210, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.880 sec/batch; 69h:12m:58s remains)
INFO - root - 2017-12-09 18:54:52.286800: step 49220, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.865 sec/batch; 68h:05m:08s remains)
INFO - root - 2017-12-09 18:55:00.916587: step 49230, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.850 sec/batch; 66h:52m:31s remains)
INFO - root - 2017-12-09 18:55:09.586357: step 49240, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.879 sec/batch; 69h:11m:32s remains)
INFO - root - 2017-12-09 18:55:18.308322: step 49250, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.859 sec/batch; 67h:33m:44s remains)
INFO - root - 2017-12-09 18:55:26.917565: step 49260, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.886 sec/batch; 69h:42m:45s remains)
INFO - root - 2017-12-09 18:55:35.643002: step 49270, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.878 sec/batch; 69h:03m:14s remains)
INFO - root - 2017-12-09 18:55:44.349059: step 49280, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.858 sec/batch; 67h:28m:50s remains)
INFO - root - 2017-12-09 18:55:53.167558: step 49290, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.891 sec/batch; 70h:06m:12s remains)
INFO - root - 2017-12-09 18:56:01.721186: step 49300, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.880 sec/batch; 69h:12m:23s remains)
2017-12-09 18:56:02.615975: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0018126173 -0.0018112934 -0.001811184 -0.0018113109 -0.0018114005 -0.0018113219 -0.0018111197 -0.0018109373 -0.0018109973 -0.0018115323 -0.0018123008 -0.0018130157 -0.0018135215 -0.0018137848 -0.0018137418][-0.0018119101 -0.0018106295 -0.0018106152 -0.0018108092 -0.0018108829 -0.0018106957 -0.0018103741 -0.0018100744 -0.0018100948 -0.001810668 -0.0018115161 -0.001812343 -0.0018129024 -0.0018131218 -0.0018130337][-0.001812113 -0.0018109308 -0.0018109583 -0.0018111109 -0.0018110344 -0.0018106489 -0.0018100978 -0.001809655 -0.00180963 -0.00181026 -0.0018112187 -0.0018121572 -0.0018127906 -0.0018129768 -0.0018127719][-0.0018126207 -0.0018115197 -0.0018115522 -0.0018116285 -0.0018113566 -0.001810715 -0.0018098629 -0.0018091742 -0.0018090218 -0.0018096603 -0.0018107235 -0.0018117933 -0.001812555 -0.0018127977 -0.0018125775][-0.001813061 -0.0018119555 -0.0018119852 -0.0018119953 -0.0018115174 -0.001810592 -0.0018094105 -0.0018084267 -0.0018081019 -0.001808716 -0.0018098769 -0.0018110781 -0.0018120565 -0.0018125429 -0.001812474][-0.0018132029 -0.0018120344 -0.0018120019 -0.0018119237 -0.0018112954 -0.0018101486 -0.0018087438 -0.001807566 -0.0018071121 -0.0018076883 -0.0018089134 -0.0018102038 -0.0018113549 -0.0018121559 -0.0018123826][-0.0018132607 -0.0018120393 -0.00181196 -0.0018118887 -0.0018113459 -0.0018102736 -0.0018088777 -0.0018075656 -0.0018068531 -0.0018071798 -0.0018082694 -0.0018094816 -0.0018106415 -0.0018116094 -0.0018120619][-0.0018133849 -0.0018121147 -0.0018120211 -0.0018120139 -0.0018116763 -0.0018108586 -0.0018096884 -0.001808421 -0.0018074887 -0.0018074461 -0.0018082417 -0.0018092669 -0.0018102615 -0.0018111374 -0.0018116153][-0.0018133592 -0.0018120906 -0.001811974 -0.0018119676 -0.0018117662 -0.0018112207 -0.0018103938 -0.0018094141 -0.0018085528 -0.0018082459 -0.001808667 -0.0018094117 -0.0018101642 -0.0018108132 -0.0018111994][-0.0018132322 -0.0018120442 -0.0018118657 -0.0018117996 -0.0018116249 -0.001811236 -0.0018106811 -0.0018100294 -0.001809422 -0.0018091173 -0.0018093169 -0.0018098083 -0.0018103336 -0.0018107783 -0.0018110402][-0.0018131451 -0.0018120478 -0.0018118403 -0.0018117387 -0.0018115856 -0.0018112891 -0.0018108883 -0.0018104534 -0.0018100698 -0.0018098591 -0.0018099453 -0.0018102579 -0.0018105877 -0.0018108672 -0.0018110214][-0.0018132261 -0.0018121115 -0.0018118846 -0.0018117833 -0.00181168 -0.0018114881 -0.0018112313 -0.0018109737 -0.0018107431 -0.0018105864 -0.0018105833 -0.001810716 -0.0018108765 -0.001811014 -0.0018110842][-0.0018133092 -0.0018122025 -0.0018119341 -0.001811862 -0.0018118037 -0.0018116942 -0.0018115576 -0.0018114388 -0.0018112997 -0.0018111607 -0.0018110613 -0.001811045 -0.0018110669 -0.001811111 -0.0018111597][-0.0018133669 -0.0018121898 -0.0018118537 -0.001811792 -0.0018117591 -0.0018117141 -0.0018116729 -0.0018116462 -0.0018115724 -0.0018114477 -0.0018112863 -0.0018111665 -0.0018111159 -0.0018111339 -0.0018112076][-0.0018134082 -0.001812124 -0.0018116734 -0.0018116006 -0.0018115818 -0.0018115599 -0.0018115557 -0.0018115777 -0.0018115524 -0.0018114587 -0.0018113023 -0.00181116 -0.0018110853 -0.0018111 -0.0018111882]]...]
INFO - root - 2017-12-09 18:56:11.324995: step 49310, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.863 sec/batch; 67h:55m:02s remains)
INFO - root - 2017-12-09 18:56:20.041475: step 49320, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.841 sec/batch; 66h:09m:47s remains)
INFO - root - 2017-12-09 18:56:28.780666: step 49330, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.843 sec/batch; 66h:20m:47s remains)
INFO - root - 2017-12-09 18:56:37.218616: step 49340, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.838 sec/batch; 65h:54m:29s remains)
INFO - root - 2017-12-09 18:56:45.908217: step 49350, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.832 sec/batch; 65h:26m:31s remains)
INFO - root - 2017-12-09 18:56:54.513102: step 49360, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.847 sec/batch; 66h:36m:54s remains)
INFO - root - 2017-12-09 18:57:03.096000: step 49370, loss = 0.82, batch loss = 0.69 (9.9 examples/sec; 0.812 sec/batch; 63h:51m:46s remains)
INFO - root - 2017-12-09 18:57:11.591109: step 49380, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.841 sec/batch; 66h:07m:46s remains)
INFO - root - 2017-12-09 18:57:20.337830: step 49390, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 68h:13m:27s remains)
INFO - root - 2017-12-09 18:57:28.873133: step 49400, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.848 sec/batch; 66h:40m:05s remains)
2017-12-09 18:57:29.874376: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.001829127 -0.0018286846 -0.0018286392 -0.0018288826 -0.0018293022 -0.0018297618 -0.0018301422 -0.0018303134 -0.0018300639 -0.0018293053 -0.0018281607 -0.0018268549 -0.0018256289 -0.0018244348 -0.0018234036][-0.0018304955 -0.0018299489 -0.0018298642 -0.0018301129 -0.00183059 -0.0018311287 -0.0018315387 -0.0018316819 -0.001831291 -0.0018303215 -0.0018289343 -0.0018274048 -0.0018259618 -0.0018245614 -0.0018233885][-0.0018312474 -0.0018307471 -0.0018306866 -0.0018309376 -0.0018313983 -0.0018319138 -0.0018322777 -0.0018323906 -0.0018319151 -0.0018308443 -0.0018293645 -0.001827742 -0.0018262195 -0.0018247383 -0.0018235648][-0.0018308423 -0.001830446 -0.0018304302 -0.0018306439 -0.0018309985 -0.0018313816 -0.0018316036 -0.0018316582 -0.0018312047 -0.0018301787 -0.0018287504 -0.0018272146 -0.0018257786 -0.0018244057 -0.0018233615][-0.0018289014 -0.0018286016 -0.0018285998 -0.0018287022 -0.0018288763 -0.0018290973 -0.0018291774 -0.0018291469 -0.0018287812 -0.0018279847 -0.001826877 -0.0018256922 -0.0018245963 -0.0018235932 -0.0018228702][-0.0018263324 -0.0018261586 -0.0018261938 -0.0018261956 -0.0018261776 -0.0018262338 -0.0018261741 -0.0018260513 -0.0018257591 -0.0018252322 -0.0018245224 -0.0018237712 -0.0018231019 -0.0018225184 -0.0018221468][-0.0018238188 -0.0018237283 -0.0018237642 -0.0018236396 -0.0018234561 -0.0018233565 -0.0018231404 -0.0018228668 -0.0018226262 -0.0018223751 -0.00182202 -0.0018216334 -0.001821388 -0.0018212644 -0.0018212741][-0.0018221781 -0.0018221058 -0.0018220814 -0.0018218313 -0.0018214933 -0.0018212309 -0.0018208905 -0.0018205347 -0.0018203922 -0.001820377 -0.0018202483 -0.0018201056 -0.0018201454 -0.0018203356 -0.0018205793][-0.0018209794 -0.0018208853 -0.0018208504 -0.0018205922 -0.0018202533 -0.0018200091 -0.0018197064 -0.0018193953 -0.0018192987 -0.0018193685 -0.0018192921 -0.0018192134 -0.0018193709 -0.0018197244 -0.0018201041][-0.00182028 -0.0018201713 -0.0018201449 -0.001819941 -0.0018196788 -0.0018195157 -0.0018193258 -0.0018191341 -0.0018190844 -0.0018191325 -0.0018190281 -0.0018189326 -0.0018191086 -0.0018194786 -0.0018198829][-0.0018201475 -0.0018199837 -0.001819927 -0.0018197588 -0.0018195719 -0.0018194679 -0.0018193451 -0.0018192258 -0.0018191963 -0.0018192094 -0.0018190634 -0.0018189673 -0.0018191395 -0.001819476 -0.0018198583][-0.001820182 -0.0018199377 -0.0018198594 -0.0018197288 -0.0018196023 -0.0018195288 -0.0018194644 -0.001819395 -0.0018193705 -0.001819361 -0.0018192178 -0.0018191341 -0.0018192808 -0.0018195792 -0.0018199339][-0.0018203844 -0.0018201628 -0.0018200994 -0.0018200086 -0.0018198914 -0.0018198073 -0.0018197584 -0.001819683 -0.0018196293 -0.0018195872 -0.0018194631 -0.0018193962 -0.0018194921 -0.0018197484 -0.001820073][-0.0018205991 -0.0018203917 -0.0018203316 -0.0018202639 -0.0018201796 -0.0018201318 -0.0018201106 -0.0018200438 -0.0018199843 -0.0018199149 -0.0018198052 -0.0018197392 -0.0018197979 -0.0018200075 -0.0018202756][-0.0018207879 -0.001820541 -0.0018205059 -0.0018204644 -0.0018204148 -0.0018204127 -0.0018204229 -0.0018203669 -0.0018202921 -0.0018202022 -0.0018200933 -0.0018200313 -0.001820071 -0.0018202367 -0.0018204537]]...]
INFO - root - 2017-12-09 18:57:38.585229: step 49410, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.873 sec/batch; 68h:37m:29s remains)
INFO - root - 2017-12-09 18:57:47.452876: step 49420, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.896 sec/batch; 70h:28m:03s remains)
INFO - root - 2017-12-09 18:57:56.273109: step 49430, loss = 0.82, batch loss = 0.69 (8.2 examples/sec; 0.980 sec/batch; 77h:02m:46s remains)
INFO - root - 2017-12-09 18:58:04.900579: step 49440, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.879 sec/batch; 69h:06m:03s remains)
INFO - root - 2017-12-09 18:58:13.625781: step 49450, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.836 sec/batch; 65h:42m:09s remains)
INFO - root - 2017-12-09 18:58:22.298422: step 49460, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.874 sec/batch; 68h:41m:38s remains)
INFO - root - 2017-12-09 18:58:30.941453: step 49470, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.886 sec/batch; 69h:38m:20s remains)
INFO - root - 2017-12-09 18:58:39.535380: step 49480, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 67h:56m:16s remains)
INFO - root - 2017-12-09 18:58:48.135412: step 49490, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.881 sec/batch; 69h:17m:22s remains)
INFO - root - 2017-12-09 18:58:56.590468: step 49500, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.857 sec/batch; 67h:24m:19s remains)
2017-12-09 18:58:57.454393: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.52010089 0.51592469 0.50685149 0.49657086 0.48104072 0.46329796 0.44194561 0.41861314 0.39227325 0.36681241 0.34592217 0.32999122 0.31754777 0.30887809 0.30558249][0.52286214 0.52045959 0.51327437 0.50453287 0.49045387 0.47481951 0.45519572 0.43338525 0.408632 0.38309667 0.36076745 0.34290734 0.32870638 0.31804255 0.31242833][0.51724482 0.51601356 0.51061368 0.50380307 0.49235582 0.47882208 0.46149909 0.44223627 0.42000213 0.39596429 0.3730742 0.35369805 0.33753118 0.32449281 0.31624663][0.50768465 0.50904304 0.50721443 0.50325018 0.49518073 0.48496675 0.4708541 0.45340574 0.43263328 0.41058549 0.38835359 0.36757335 0.34919041 0.33388481 0.32288337][0.49087325 0.49460086 0.49562263 0.49630353 0.49341902 0.48781872 0.4781417 0.46344775 0.44494408 0.42438689 0.40236583 0.38064098 0.36042345 0.34343934 0.32998094][0.4772881 0.48346803 0.48596093 0.48938861 0.48999128 0.48820186 0.48252037 0.47105604 0.4560371 0.43791839 0.41682851 0.39507893 0.3737061 0.35505182 0.33881694][0.46371943 0.47273359 0.47722223 0.48315004 0.4868817 0.48739824 0.48477498 0.47710577 0.46568289 0.4509367 0.43262371 0.41172245 0.38984483 0.36895105 0.34976611][0.45490009 0.46461192 0.46841377 0.47349861 0.47824568 0.48030162 0.47970554 0.47466424 0.46589008 0.45517457 0.44047287 0.42154086 0.400046 0.3783583 0.35771564][0.44796327 0.45909041 0.4630011 0.46770126 0.47294524 0.47601017 0.47720015 0.47378907 0.46653906 0.45790467 0.44538864 0.42833763 0.40759906 0.38573161 0.36430123][0.4443492 0.45656946 0.4587782 0.4622848 0.46718234 0.47063673 0.47277474 0.47086591 0.46614861 0.4591634 0.44820362 0.4328742 0.41344553 0.39260578 0.37089628][0.44446588 0.45602092 0.4561893 0.45734516 0.46056637 0.46243909 0.46420264 0.46439686 0.46169126 0.45720723 0.44899169 0.43697119 0.420396 0.40124005 0.38047081][0.44446456 0.45550764 0.45303932 0.45068413 0.45059153 0.45098591 0.45211968 0.45349491 0.45302832 0.45200402 0.44742712 0.438745 0.42591241 0.40933019 0.39029106][0.44358984 0.45403531 0.44963539 0.44476992 0.44224498 0.44075513 0.44091433 0.44235611 0.4428443 0.44370285 0.44174933 0.43650323 0.42733496 0.41424489 0.39805228][0.44077414 0.45152503 0.44645879 0.44169575 0.43902424 0.4372274 0.43749925 0.43942803 0.44124949 0.44275513 0.44185024 0.43895587 0.432488 0.42195264 0.4078086][0.43873096 0.45019931 0.44477478 0.43941158 0.43560359 0.43310431 0.43306234 0.43530133 0.43802887 0.44063213 0.44160157 0.44024244 0.43579721 0.42738652 0.41502994]]...]
INFO - root - 2017-12-09 18:59:06.000250: step 49510, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.856 sec/batch; 67h:19m:15s remains)
INFO - root - 2017-12-09 18:59:14.616252: step 49520, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.873 sec/batch; 68h:36m:22s remains)
INFO - root - 2017-12-09 18:59:23.387324: step 49530, loss = 0.83, batch loss = 0.70 (9.3 examples/sec; 0.865 sec/batch; 67h:57m:19s remains)
INFO - root - 2017-12-09 18:59:31.924018: step 49540, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.856 sec/batch; 67h:14m:37s remains)
INFO - root - 2017-12-09 18:59:40.602592: step 49550, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.856 sec/batch; 67h:17m:05s remains)
INFO - root - 2017-12-09 18:59:49.118302: step 49560, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.855 sec/batch; 67h:10m:08s remains)
INFO - root - 2017-12-09 18:59:57.754798: step 49570, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.863 sec/batch; 67h:47m:53s remains)
INFO - root - 2017-12-09 19:00:06.475637: step 49580, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.878 sec/batch; 69h:00m:43s remains)
INFO - root - 2017-12-09 19:00:15.043090: step 49590, loss = 0.82, batch loss = 0.69 (9.9 examples/sec; 0.809 sec/batch; 63h:35m:57s remains)
INFO - root - 2017-12-09 19:00:23.631818: step 49600, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.874 sec/batch; 68h:41m:00s remains)
2017-12-09 19:00:24.548925: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.084805362 0.0815271 0.075598583 0.067088343 0.057537787 0.047566466 0.03815446 0.030774966 0.02652075 0.025576927 0.026903711 0.02966392 0.033128832 0.036073968 0.037345961][0.09078411 0.089987569 0.086708538 0.080735743 0.073100016 0.064212807 0.054901622 0.04679288 0.040950369 0.037796021 0.036484357 0.036482517 0.037324544 0.037972484 0.037605379][0.096633829 0.098488271 0.098233044 0.095485739 0.090749487 0.084101371 0.075972788 0.067783415 0.060528412 0.054940846 0.050534252 0.047174748 0.04469195 0.042295773 0.039474197][0.10174222 0.10612347 0.10904649 0.10990386 0.10854616 0.10467995 0.098327845 0.090560712 0.082289778 0.074498057 0.067143887 0.060367502 0.05432044 0.048565213 0.042922352][0.10543058 0.11176192 0.11739174 0.121743 0.12402306 0.12337263 0.11930253 0.11241644 0.10356041 0.093949109 0.08400058 0.074173473 0.06484431 0.055869542 0.047438212][0.10704283 0.11455821 0.12193762 0.12887004 0.13412397 0.13642274 0.13478252 0.12928919 0.12064368 0.11004783 0.09836328 0.086259149 0.074297555 0.062738232 0.051991709][0.1062369 0.11419006 0.12230109 0.13069524 0.13790499 0.14241029 0.14283626 0.13885747 0.13100547 0.1204289 0.10819178 0.094948359 0.081360884 0.0680995 0.055774018][0.10399608 0.11176654 0.11953492 0.12804347 0.13581581 0.14126991 0.14284283 0.13995421 0.13309328 0.12338643 0.11174896 0.098618515 0.084697552 0.0709078 0.05804687][0.10062049 0.10791739 0.11474462 0.12243295 0.1296722 0.13500501 0.13678834 0.13447267 0.12854087 0.12001915 0.10957666 0.09748885 0.084324472 0.071094878 0.05866595][0.095844947 0.10253799 0.1081463 0.11448747 0.12051493 0.12498122 0.12629509 0.12412147 0.11886089 0.11147971 0.10247599 0.0919465 0.080359384 0.06858214 0.057418764][0.089394271 0.095313124 0.099750228 0.10468097 0.10931009 0.11260257 0.11315953 0.11072456 0.10583819 0.099438079 0.091858551 0.083106473 0.073531918 0.063823223 0.054518532][0.081082746 0.086201735 0.089704037 0.093556732 0.097051345 0.099234052 0.098975457 0.096261583 0.09170606 0.086147696 0.079866849 0.072901972 0.0653921 0.05773975 0.050291743][0.071293347 0.07550633 0.078260086 0.081282549 0.083881021 0.085156843 0.084343478 0.081573233 0.077466078 0.072744653 0.067745566 0.062505715 0.057022814 0.05136475 0.045721192][0.060455568 0.063724265 0.065777294 0.0680438 0.069971979 0.0707866 0.069973886 0.067566007 0.064142123 0.060400624 0.056655861 0.05290005 0.04904677 0.045138881 0.041080568][0.049991235 0.052374005 0.053693641 0.055262011 0.056634303 0.057248536 0.056692939 0.054909132 0.052395672 0.049620524 0.04699032 0.044486057 0.041995622 0.039473627 0.036701694]]...]
INFO - root - 2017-12-09 19:00:33.270111: step 49610, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.887 sec/batch; 69h:44m:06s remains)
INFO - root - 2017-12-09 19:00:41.915590: step 49620, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.854 sec/batch; 67h:08m:17s remains)
INFO - root - 2017-12-09 19:00:50.588647: step 49630, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.873 sec/batch; 68h:35m:39s remains)
INFO - root - 2017-12-09 19:00:59.172739: step 49640, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.870 sec/batch; 68h:20m:47s remains)
INFO - root - 2017-12-09 19:01:07.766394: step 49650, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.859 sec/batch; 67h:31m:49s remains)
INFO - root - 2017-12-09 19:01:16.270847: step 49660, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.876 sec/batch; 68h:47m:43s remains)
INFO - root - 2017-12-09 19:01:25.018185: step 49670, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.861 sec/batch; 67h:39m:58s remains)
INFO - root - 2017-12-09 19:01:33.756120: step 49680, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.883 sec/batch; 69h:24m:10s remains)
INFO - root - 2017-12-09 19:01:42.477718: step 49690, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.882 sec/batch; 69h:15m:08s remains)
INFO - root - 2017-12-09 19:01:51.040723: step 49700, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.852 sec/batch; 66h:56m:17s remains)
2017-12-09 19:01:51.878322: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0018276697 -0.0018273134 -0.0018275953 -0.0018270556 -0.0018260193 -0.0018252998 -0.0018253473 -0.0018257332 -0.0018255753 -0.0018245331 -0.001822637 -0.0018196619 -0.0018165496 -0.0018138177 -0.001812287][-0.0018283218 -0.0018277309 -0.0018271843 -0.0018258605 -0.0018243998 -0.0018238421 -0.0018244062 -0.0018257652 -0.0018265722 -0.0018260167 -0.0018241297 -0.0018208909 -0.0018172474 -0.0018140255 -0.0018121379][-0.0018287216 -0.0018283023 -0.0018272715 -0.0018252551 -0.0018231623 -0.0018226359 -0.0018234149 -0.0018251266 -0.0018266216 -0.001826835 -0.0018253921 -0.0018221528 -0.0018182099 -0.0018146035 -0.0018124565][-0.0018287958 -0.0018288817 -0.0018276403 -0.0018252244 -0.0018228717 -0.0018220203 -0.0018225174 -0.0018242247 -0.0018260289 -0.0018268056 -0.001825772 -0.0018226951 -0.0018186514 -0.0018149393 -0.0018126711][-0.0018281887 -0.0018289563 -0.0018278625 -0.0018254425 -0.0018229675 -0.0018217633 -0.0018221149 -0.0018238781 -0.0018260198 -0.0018270551 -0.0018263926 -0.001823374 -0.0018191519 -0.0018151427 -0.0018125887][-0.0018267565 -0.0018280565 -0.001827501 -0.0018256254 -0.001823354 -0.0018220744 -0.0018225693 -0.0018244665 -0.0018267711 -0.0018278793 -0.0018272714 -0.0018241808 -0.001819859 -0.0018156366 -0.0018127607][-0.001825269 -0.0018269221 -0.0018269734 -0.0018255271 -0.0018233134 -0.0018215472 -0.001821902 -0.0018241295 -0.0018265885 -0.001827818 -0.001827433 -0.0018247528 -0.0018208307 -0.0018166354 -0.0018135156][-0.0018243811 -0.0018258407 -0.001825982 -0.0018246566 -0.0018223006 -0.0018196584 -0.0018193473 -0.0018215355 -0.0018243749 -0.0018260438 -0.0018263375 -0.0018244054 -0.0018212444 -0.0018173777 -0.0018140721][-0.0018244978 -0.0018249867 -0.0018245217 -0.0018226086 -0.0018194923 -0.0018158082 -0.0018145468 -0.0018166674 -0.0018202215 -0.0018228451 -0.0018243821 -0.0018234674 -0.0018208849 -0.0018170876 -0.0018138202][-0.0018251025 -0.0018241947 -0.0018226716 -0.0018197161 -0.0018154347 -0.001810689 -0.0018087761 -0.0018108756 -0.0018152406 -0.0018191679 -0.0018220679 -0.0018223452 -0.0018203509 -0.0018166567 -0.0018132478][-0.0018255233 -0.0018234842 -0.0018209344 -0.0018169496 -0.0018117675 -0.0018061071 -0.0018033705 -0.0018053144 -0.0018102627 -0.0018155719 -0.00181986 -0.0018212564 -0.0018198428 -0.0018162131 -0.0018127635][-0.0018256776 -0.0018230594 -0.0018198905 -0.0018155838 -0.0018098897 -0.0018036669 -0.0018000931 -0.001801315 -0.0018060202 -0.001812227 -0.0018177438 -0.0018201309 -0.0018193079 -0.0018158673 -0.0018125267][-0.001824763 -0.0018219959 -0.0018187944 -0.0018148619 -0.0018097926 -0.0018042901 -0.00180087 -0.001801424 -0.0018054968 -0.0018113148 -0.0018169838 -0.0018195843 -0.0018189391 -0.0018155313 -0.0018121916][-0.0018233657 -0.0018205544 -0.001817758 -0.0018147039 -0.001811012 -0.0018070922 -0.0018046258 -0.001804716 -0.0018074423 -0.0018119446 -0.0018167431 -0.0018188762 -0.0018180724 -0.0018149368 -0.0018118318][-0.0018216342 -0.0018191005 -0.0018166784 -0.0018144973 -0.0018121097 -0.00180978 -0.001808289 -0.0018084032 -0.0018102393 -0.0018134093 -0.001816835 -0.0018182097 -0.0018172576 -0.0018144525 -0.001811782]]...]
INFO - root - 2017-12-09 19:02:00.508709: step 49710, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.880 sec/batch; 69h:05m:16s remains)
INFO - root - 2017-12-09 19:02:09.105665: step 49720, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 67h:52m:05s remains)
INFO - root - 2017-12-09 19:02:17.652029: step 49730, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.845 sec/batch; 66h:21m:47s remains)
INFO - root - 2017-12-09 19:02:26.021808: step 49740, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.857 sec/batch; 67h:20m:07s remains)
INFO - root - 2017-12-09 19:02:34.560244: step 49750, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.849 sec/batch; 66h:42m:37s remains)
INFO - root - 2017-12-09 19:02:43.067567: step 49760, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 67h:40m:06s remains)
INFO - root - 2017-12-09 19:02:51.687089: step 49770, loss = 0.83, batch loss = 0.70 (9.5 examples/sec; 0.846 sec/batch; 66h:26m:01s remains)
INFO - root - 2017-12-09 19:03:00.450154: step 49780, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.835 sec/batch; 65h:34m:10s remains)
INFO - root - 2017-12-09 19:03:09.089446: step 49790, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.865 sec/batch; 67h:57m:00s remains)
INFO - root - 2017-12-09 19:03:17.563977: step 49800, loss = 0.82, batch loss = 0.69 (10.0 examples/sec; 0.801 sec/batch; 62h:54m:26s remains)
2017-12-09 19:03:18.478294: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0018246803 -0.0018246553 -0.0018256672 -0.0018249674 -0.0018184432 -0.0018071053 -0.0017981253 -0.0017963233 -0.0017999129 -0.0018062412 -0.0018145581 -0.001821212 -0.0018240135 -0.0018240849 -0.0018234227][-0.0018251452 -0.0018251374 -0.0018243019 -0.0018137135 -0.0017829891 -0.0017399455 -0.0017083883 -0.0017036728 -0.0017220273 -0.0017520522 -0.0017846503 -0.0018095099 -0.0018207954 -0.0018230216 -0.0018225282][-0.0018254332 -0.0018250436 -0.0018152511 -0.0017701898 -0.0016697338 -0.0015445203 -0.0014612703 -0.0014590991 -0.0015247676 -0.0016198952 -0.0017132037 -0.0017801403 -0.0018121391 -0.0018210806 -0.0018216721][-0.0018024617 -0.0017940175 -0.0017542475 -0.0016249273 -0.0013754201 -0.0010884436 -0.00091753207 -0.00094453822 -0.0011279779 -0.0013661719 -0.0015800729 -0.0017237575 -0.0017938457 -0.0018172297 -0.0018208681][-0.0016820773 -0.0016408884 -0.0015341585 -0.0012581858 -0.00077444734 -0.00025353732 1.9549741e-05 -9.326532e-05 -0.00049698574 -0.00097946369 -0.0013838251 -0.0016414765 -0.0017661013 -0.0018108658 -0.0018195478][-0.0013501816 -0.0012310931 -0.0010160867 -0.0005565288 0.00018772762 0.00094742887 0.0012948783 0.001031545 0.00031773816 -0.00049298245 -0.0011434759 -0.0015439076 -0.0017340344 -0.0018035418 -0.0018173553][-0.00071801303 -0.00046464556 -0.00012045843 0.00048993784 0.0014166895 0.0023255222 0.0026718881 0.0022033784 0.0011416504 -1.8384191e-05 -0.0009210069 -0.0014611019 -0.0017097211 -0.0017977913 -0.0018147822][0.00015571713 0.0005885025 0.0010431649 0.0016861295 0.0025943811 0.0034537655 0.0036919545 0.0030261055 0.0016989799 0.00028600986 -0.00079249428 -0.0014228296 -0.0017021046 -0.0017958855 -0.0018131792][0.0010423406 0.0016655615 0.0021806518 0.0027047675 0.0033569625 0.0039387732 0.00396099 0.0031594844 0.0017441248 0.0002760503 -0.00082471641 -0.0014507188 -0.0017152445 -0.0017980722 -0.0018128369][0.0015423368 0.0022914759 0.002796195 0.0030975398 0.0033512204 0.0035294448 0.0033105006 0.0024937233 0.0012165448 -6.930274e-05 -0.0010146343 -0.0015349428 -0.0017433641 -0.0018035219 -0.0018138858][0.0013746163 0.0021441397 0.0025977171 0.0026983423 0.0026108241 0.0024444121 0.002057618 0.0013323906 0.00034253951 -0.00060840836 -0.0012853009 -0.0016412506 -0.0017750758 -0.0018101534 -0.0018158224][0.00059294864 0.001251257 0.0016249113 0.0016219886 0.0013684344 0.0010247203 0.00059143757 3.5633682e-05 -0.0006033075 -0.0011686978 -0.0015482494 -0.0017354541 -0.0018010483 -0.0018162151 -0.0018184896][-0.00041770493 4.1148392e-05 0.00031022169 0.00028657052 3.9792503e-05 -0.00029120792 -0.00063866773 -0.00098338292 -0.0013082746 -0.0015633842 -0.0017210452 -0.0017932331 -0.0018164224 -0.0018205839 -0.0018207496][-0.0012354392 -0.00099902868 -0.00085235131 -0.00086630345 -0.0010164034 -0.0012209776 -0.001415997 -0.0015737928 -0.0016895877 -0.0017635552 -0.0018024344 -0.0018178836 -0.0018220128 -0.001821956 -0.0018213878][-0.0016769884 -0.0015983604 -0.0015461836 -0.0015497244 -0.0016032266 -0.0016756103 -0.0017383314 -0.001780601 -0.0018044461 -0.001816264 -0.001821139 -0.0018225597 -0.0018224951 -0.0018217691 -0.0018211604]]...]
INFO - root - 2017-12-09 19:03:27.237655: step 49810, loss = 0.83, batch loss = 0.70 (9.1 examples/sec; 0.880 sec/batch; 69h:04m:10s remains)
INFO - root - 2017-12-09 19:03:36.005068: step 49820, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.887 sec/batch; 69h:39m:18s remains)
INFO - root - 2017-12-09 19:03:44.732422: step 49830, loss = 0.82, batch loss = 0.69 (8.7 examples/sec; 0.915 sec/batch; 71h:52m:45s remains)
INFO - root - 2017-12-09 19:03:53.291309: step 49840, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.880 sec/batch; 69h:04m:45s remains)
INFO - root - 2017-12-09 19:04:01.908862: step 49850, loss = 0.81, batch loss = 0.68 (9.1 examples/sec; 0.878 sec/batch; 68h:53m:55s remains)
INFO - root - 2017-12-09 19:04:10.457588: step 49860, loss = 0.82, batch loss = 0.69 (9.7 examples/sec; 0.821 sec/batch; 64h:29m:08s remains)
INFO - root - 2017-12-09 19:04:19.109778: step 49870, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.860 sec/batch; 67h:28m:55s remains)
INFO - root - 2017-12-09 19:04:27.754659: step 49880, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.883 sec/batch; 69h:18m:46s remains)
INFO - root - 2017-12-09 19:04:36.478109: step 49890, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.859 sec/batch; 67h:25m:18s remains)
INFO - root - 2017-12-09 19:04:45.030114: step 49900, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.849 sec/batch; 66h:40m:02s remains)
2017-12-09 19:04:45.909400: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.025237381 0.025589069 0.025879685 0.026139919 0.026188675 0.026081406 0.02576359 0.025312625 0.02466256 0.023815563 0.022853786 0.021714693 0.020289795 0.018639326 0.016953012][0.024856986 0.025267916 0.025637737 0.025983376 0.026192615 0.026289146 0.026228318 0.025934329 0.025357556 0.024528839 0.023582947 0.022427453 0.02107135 0.019473994 0.017705688][0.024393225 0.024818664 0.025213245 0.025648022 0.02601086 0.026274588 0.026410041 0.026375141 0.025990089 0.025302239 0.024410522 0.023201019 0.021800231 0.020160172 0.018344482][0.02413556 0.024600508 0.024936931 0.025333738 0.025724079 0.026150409 0.026492029 0.02676787 0.026719006 0.026359318 0.025614643 0.024437023 0.022923874 0.021172317 0.019256825][0.024019845 0.024601292 0.024995966 0.025376491 0.025698666 0.026111845 0.026537925 0.027096188 0.027339337 0.027383137 0.026960064 0.025968021 0.024501702 0.022630261 0.020563988][0.02360337 0.024389217 0.024963284 0.025410647 0.025733527 0.026142722 0.026559316 0.027267871 0.027823014 0.02819442 0.028069561 0.027381314 0.026109677 0.024291163 0.022156887][0.022788815 0.023677723 0.024385242 0.024954149 0.025362575 0.025792234 0.026244007 0.02705705 0.027848605 0.028539108 0.028797908 0.028421093 0.027390623 0.025730824 0.023716003][0.021813564 0.022726677 0.023493908 0.024113549 0.024578841 0.025056016 0.025560403 0.026398607 0.02733537 0.028166924 0.028684147 0.028619455 0.027958628 0.026591169 0.024837436][0.020549677 0.021499015 0.022305189 0.023009285 0.023548843 0.024045205 0.024570456 0.025340311 0.026301125 0.027255112 0.027955074 0.028134756 0.027805783 0.026841795 0.025428684][0.018978763 0.019932032 0.020709492 0.021474846 0.022127204 0.022718551 0.023229575 0.023883102 0.024712294 0.025630157 0.026434017 0.026847256 0.026827579 0.026253471 0.025225356][0.017194638 0.018128432 0.018833498 0.019576535 0.020252533 0.020893786 0.021453811 0.021999789 0.022695502 0.023519058 0.024312122 0.024862263 0.025068535 0.024794968 0.024070019][0.015233349 0.016151359 0.016796909 0.017465111 0.018086003 0.01868399 0.019217465 0.019716213 0.020341443 0.021010609 0.0217135 0.022307483 0.022632163 0.02260722 0.022217805][0.013152847 0.01403041 0.014613582 0.015166633 0.015674697 0.016160794 0.016635781 0.017080942 0.017612481 0.018162716 0.018746227 0.019264657 0.019605998 0.019746471 0.019633507][0.010953438 0.011723627 0.012240862 0.012688969 0.013080137 0.013442249 0.01382567 0.014175944 0.014581818 0.014998227 0.015472498 0.015951168 0.016319552 0.016595358 0.016717963][0.0089966105 0.0095937569 0.010000704 0.010358096 0.01065327 0.010870412 0.011098186 0.011323468 0.011610392 0.011926636 0.012284118 0.012668899 0.013026247 0.013371783 0.013649618]]...]
INFO - root - 2017-12-09 19:04:54.720128: step 49910, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.903 sec/batch; 70h:52m:08s remains)
INFO - root - 2017-12-09 19:05:03.431742: step 49920, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.893 sec/batch; 70h:04m:39s remains)
INFO - root - 2017-12-09 19:05:12.046873: step 49930, loss = 0.82, batch loss = 0.69 (9.7 examples/sec; 0.829 sec/batch; 65h:02m:43s remains)
INFO - root - 2017-12-09 19:05:20.522110: step 49940, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.856 sec/batch; 67h:09m:31s remains)
INFO - root - 2017-12-09 19:05:29.213811: step 49950, loss = 0.82, batch loss = 0.69 (8.7 examples/sec; 0.919 sec/batch; 72h:09m:33s remains)
INFO - root - 2017-12-09 19:05:37.946545: step 49960, loss = 0.82, batch loss = 0.69 (8.8 examples/sec; 0.912 sec/batch; 71h:33m:49s remains)
INFO - root - 2017-12-09 19:05:46.596833: step 49970, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.885 sec/batch; 69h:29m:38s remains)
INFO - root - 2017-12-09 19:05:55.332643: step 49980, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.875 sec/batch; 68h:38m:27s remains)
INFO - root - 2017-12-09 19:06:04.015832: step 49990, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.872 sec/batch; 68h:23m:30s remains)
INFO - root - 2017-12-09 19:06:12.423150: step 50000, loss = 0.82, batch loss = 0.69 (10.0 examples/sec; 0.804 sec/batch; 63h:04m:40s remains)
2017-12-09 19:06:13.312557: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.32738566 0.32392386 0.31730068 0.30971947 0.30316851 0.29827872 0.29707557 0.29679379 0.29598877 0.29178226 0.28295675 0.27047312 0.255261 0.2386298 0.22379959][0.3245995 0.32474193 0.32170203 0.31825766 0.31546336 0.31489754 0.31756997 0.32147029 0.32385847 0.32156727 0.31345695 0.29916936 0.28072461 0.2595787 0.24003285][0.31636757 0.32028693 0.32178766 0.32342592 0.32547432 0.33029419 0.33735433 0.34486282 0.350044 0.348661 0.34076294 0.32485256 0.30323026 0.27771536 0.25342882][0.30640158 0.31400478 0.32047695 0.32700279 0.3338798 0.3437137 0.35518405 0.36664426 0.37442958 0.37457395 0.36684269 0.35002995 0.32612979 0.29672951 0.26802379][0.29679576 0.30813226 0.31832865 0.32919714 0.34107164 0.35626072 0.37304777 0.38850492 0.39922526 0.40061662 0.39259306 0.37432224 0.34803513 0.31605309 0.28396469][0.28954872 0.30468017 0.31916222 0.33396763 0.35023639 0.37011623 0.3918629 0.41146269 0.42440245 0.42684782 0.41835567 0.39897287 0.37038681 0.33661473 0.30181539][0.28539816 0.30501097 0.32317048 0.3417705 0.3616617 0.38483089 0.40956229 0.43164673 0.44630408 0.44968972 0.44122347 0.42158261 0.39249021 0.35799754 0.32133937][0.28494662 0.30835161 0.32965323 0.35087362 0.37336943 0.39886829 0.42535707 0.4485189 0.4634847 0.46665689 0.45744196 0.43742263 0.40807584 0.37326464 0.33533627][0.28630984 0.3136279 0.33825541 0.36224636 0.38747182 0.41435763 0.44144574 0.46439797 0.47852021 0.48096561 0.47064102 0.44984311 0.41995224 0.3844173 0.3457056][0.28733003 0.31710675 0.34388474 0.37056905 0.39818931 0.42643315 0.45364195 0.47554109 0.488311 0.4892782 0.47804382 0.45649731 0.42605925 0.3904573 0.35157782][0.28394765 0.31541502 0.34403598 0.37321985 0.40256462 0.43151078 0.45791122 0.47787029 0.48853239 0.48793313 0.47574911 0.45348161 0.42312387 0.38804108 0.35016683][0.27726769 0.30923909 0.33810237 0.36757407 0.39660695 0.42395139 0.4477486 0.46486998 0.47353974 0.47173622 0.45949569 0.4376587 0.40887696 0.37568185 0.34016052][0.26420212 0.29411852 0.32133749 0.34879586 0.3754966 0.39954367 0.41944578 0.43344927 0.43978715 0.43725002 0.42589137 0.40609387 0.38037905 0.35080394 0.3195425][0.24747072 0.27318364 0.29604024 0.31931955 0.34193796 0.36196843 0.37834293 0.38937482 0.39400154 0.39149693 0.38196978 0.36541435 0.34410238 0.31980881 0.29439521][0.23032504 0.25024608 0.26695076 0.28457683 0.3016496 0.31658414 0.32876059 0.33728054 0.34099379 0.33926216 0.33230808 0.31994289 0.30397362 0.28597173 0.26698628]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-momentum-lr0.0001-clip-zeroinit-conv1-3init-from-scratch/model.ckpt-50000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-momentum-lr0.0001-clip-zeroinit-conv1-3init-from-scratch/model.ckpt-50000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-09 19:06:22.879474: step 50010, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.865 sec/batch; 67h:54m:49s remains)
INFO - root - 2017-12-09 19:06:31.599020: step 50020, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.888 sec/batch; 69h:40m:55s remains)
INFO - root - 2017-12-09 19:06:40.407005: step 50030, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.885 sec/batch; 69h:26m:38s remains)
INFO - root - 2017-12-09 19:06:48.921552: step 50040, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.890 sec/batch; 69h:51m:37s remains)
INFO - root - 2017-12-09 19:06:57.375342: step 50050, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.846 sec/batch; 66h:22m:47s remains)
INFO - root - 2017-12-09 19:07:06.054011: step 50060, loss = 0.82, batch loss = 0.69 (9.9 examples/sec; 0.808 sec/batch; 63h:23m:02s remains)
INFO - root - 2017-12-09 19:07:14.676099: step 50070, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.854 sec/batch; 67h:00m:59s remains)
INFO - root - 2017-12-09 19:07:23.382143: step 50080, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.855 sec/batch; 67h:06m:24s remains)
INFO - root - 2017-12-09 19:07:31.962807: step 50090, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.886 sec/batch; 69h:32m:15s remains)
INFO - root - 2017-12-09 19:07:40.413283: step 50100, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.861 sec/batch; 67h:33m:40s remains)
2017-12-09 19:07:41.276431: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0018279335 -0.0018262576 -0.0018261531 -0.0018266754 -0.0018269387 -0.0018271036 -0.001826969 -0.0018259516 -0.00182393 -0.0018220625 -0.0018212887 -0.0018217572 -0.0018235387 -0.0018260097 -0.001828632][-0.0018272698 -0.0018263825 -0.0018266251 -0.0018273804 -0.0018277373 -0.0018276032 -0.0018270084 -0.0018257245 -0.0018236267 -0.0018216834 -0.0018209124 -0.0018214916 -0.0018232708 -0.0018254574 -0.0018278583][-0.0018271731 -0.0018268415 -0.0018273768 -0.0018281991 -0.0018286553 -0.0018284294 -0.0018275769 -0.001826189 -0.0018242394 -0.0018224404 -0.0018215944 -0.001822081 -0.0018237924 -0.0018257269 -0.0018278837][-0.0018272093 -0.0018272843 -0.0018278866 -0.0018288195 -0.0018293671 -0.0018291023 -0.0018281795 -0.0018269854 -0.0018254437 -0.0018237978 -0.0018228804 -0.0018231493 -0.0018245864 -0.001826213 -0.001828221][-0.0018269949 -0.0018274144 -0.0018281821 -0.0018291551 -0.0018297849 -0.0018297698 -0.0018290599 -0.0018280942 -0.0018268652 -0.0018254223 -0.0018245414 -0.001824614 -0.0018256421 -0.0018270084 -0.0018288683][-0.0018267086 -0.0018273093 -0.0018281725 -0.0018292366 -0.001830021 -0.0018302522 -0.001829994 -0.0018292988 -0.0018284152 -0.0018272148 -0.0018263306 -0.0018262691 -0.0018270368 -0.0018282132 -0.0018301347][-0.0018273407 -0.0018277647 -0.001828461 -0.0018294452 -0.0018303542 -0.0018308146 -0.0018309767 -0.0018306697 -0.0018300718 -0.0018290229 -0.0018280738 -0.0018279256 -0.0018286105 -0.0018297222 -0.001831632][-0.0018300498 -0.0018301847 -0.0018304036 -0.0018308571 -0.001831367 -0.0018317275 -0.0018319718 -0.0018319348 -0.0018314912 -0.0018305782 -0.0018297639 -0.0018295866 -0.0018301727 -0.0018311988 -0.0018329275][-0.0018330603 -0.0018332258 -0.0018330442 -0.0018328117 -0.0018326498 -0.001832476 -0.0018323818 -0.0018323096 -0.0018320503 -0.0018313869 -0.0018309384 -0.001831028 -0.0018316937 -0.0018326835 -0.0018341546][-0.0018353408 -0.0018356956 -0.0018352636 -0.0018344731 -0.0018335275 -0.001832662 -0.0018320844 -0.0018317467 -0.001831633 -0.0018313342 -0.0018312488 -0.0018316569 -0.0018325067 -0.0018336945 -0.0018351057][-0.001836761 -0.001837237 -0.0018365094 -0.0018351384 -0.0018334104 -0.0018319014 -0.0018308993 -0.0018302633 -0.001830217 -0.0018302362 -0.0018304994 -0.0018311496 -0.0018321936 -0.0018335971 -0.0018350873][-0.0018370666 -0.0018377261 -0.0018368408 -0.0018350076 -0.0018327642 -0.0018309052 -0.0018296984 -0.0018288009 -0.0018286795 -0.0018289 -0.0018293993 -0.0018300348 -0.0018308888 -0.0018321013 -0.0018334133][-0.0018357686 -0.0018367426 -0.0018359717 -0.0018340854 -0.0018317357 -0.0018298427 -0.0018285515 -0.0018274479 -0.0018271765 -0.0018274486 -0.0018281062 -0.0018287199 -0.0018292911 -0.0018302336 -0.0018312903][-0.0018331213 -0.0018339978 -0.0018333293 -0.0018317887 -0.0018296638 -0.0018278729 -0.0018265442 -0.0018254389 -0.0018250409 -0.0018252549 -0.0018259016 -0.0018265676 -0.0018270854 -0.0018278994 -0.0018288932][-0.0018302052 -0.0018308344 -0.001830243 -0.0018290272 -0.001827345 -0.0018257649 -0.0018244261 -0.001823349 -0.0018228778 -0.0018229975 -0.0018235412 -0.0018241766 -0.0018247258 -0.0018254751 -0.0018264176]]...]
INFO - root - 2017-12-09 19:07:49.847886: step 50110, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.845 sec/batch; 66h:18m:55s remains)
INFO - root - 2017-12-09 19:07:58.431493: step 50120, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.856 sec/batch; 67h:08m:18s remains)
INFO - root - 2017-12-09 19:08:07.095565: step 50130, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.891 sec/batch; 69h:53m:28s remains)
INFO - root - 2017-12-09 19:08:15.716457: step 50140, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.872 sec/batch; 68h:22m:44s remains)
INFO - root - 2017-12-09 19:08:24.267772: step 50150, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.863 sec/batch; 67h:41m:22s remains)
INFO - root - 2017-12-09 19:08:32.981288: step 50160, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 67h:34m:20s remains)
INFO - root - 2017-12-09 19:08:41.592041: step 50170, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.872 sec/batch; 68h:21m:19s remains)
INFO - root - 2017-12-09 19:08:50.128101: step 50180, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 68h:03m:39s remains)
INFO - root - 2017-12-09 19:08:58.784757: step 50190, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.883 sec/batch; 69h:15m:45s remains)
INFO - root - 2017-12-09 19:09:07.300727: step 50200, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 67h:34m:49s remains)
2017-12-09 19:09:08.175827: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.17811877 0.17472337 0.17097466 0.1660931 0.1613539 0.15623143 0.15111497 0.14612858 0.14042577 0.13355272 0.12482909 0.11518177 0.10541973 0.095176339 0.085012786][0.20908633 0.20825191 0.20701914 0.20434642 0.20121542 0.19720168 0.19238725 0.18707335 0.18055062 0.17246519 0.16214353 0.15067014 0.13886945 0.12631162 0.11312002][0.23498572 0.23735151 0.23943609 0.23997442 0.23965789 0.23708574 0.23270445 0.22692043 0.21966341 0.21078312 0.19935904 0.18648887 0.17289837 0.15822965 0.14188655][0.253839 0.25917324 0.2646077 0.26881027 0.27179053 0.2714071 0.26797193 0.26229796 0.25463864 0.24503848 0.2326833 0.21884844 0.20364276 0.18668284 0.16699803][0.26375577 0.27252313 0.28118134 0.28850326 0.29475838 0.29664737 0.29444924 0.28912619 0.28158507 0.271712 0.25909743 0.244963 0.22907758 0.2104626 0.18788771][0.26394933 0.27554679 0.28680724 0.2974039 0.307043 0.3115572 0.31117624 0.3067748 0.29975212 0.29035789 0.27811849 0.26424018 0.24819234 0.2288741 0.20458154][0.25898427 0.27175727 0.28419641 0.29693696 0.30905253 0.31589803 0.31785569 0.31551179 0.31049466 0.3024137 0.29148084 0.27871782 0.26331171 0.24390434 0.21855141][0.25243711 0.26552993 0.27819693 0.29182804 0.30534247 0.31401113 0.31822664 0.31808695 0.31516853 0.30912358 0.29995614 0.28863153 0.27415803 0.25523791 0.22974162][0.25006947 0.2625955 0.2740753 0.2870737 0.30036283 0.30987546 0.31549332 0.31704602 0.31572151 0.31114852 0.30339515 0.29378471 0.28091419 0.2633166 0.23866346][0.25016519 0.26209426 0.27207336 0.28356931 0.29553357 0.30437186 0.31006062 0.31229082 0.31169808 0.30831745 0.30200794 0.29409829 0.28305745 0.26717347 0.24381156][0.25251827 0.26374754 0.27218056 0.28176802 0.29143667 0.29839957 0.30297238 0.30432278 0.30310425 0.29976007 0.29420841 0.287765 0.27842206 0.26444459 0.2430909][0.25518546 0.26478758 0.27073681 0.27732131 0.28365779 0.28837594 0.29124776 0.29109427 0.28883886 0.28498003 0.27982065 0.27429262 0.26654226 0.25461027 0.23544863][0.25918764 0.26721588 0.27024531 0.27295291 0.27471176 0.27526566 0.27444991 0.27187145 0.26783711 0.263293 0.25850245 0.25397727 0.2479001 0.23781267 0.22098857][0.26081613 0.26779008 0.26812968 0.26700285 0.26446903 0.26071489 0.25584105 0.24980646 0.24308212 0.23725502 0.23234192 0.22857681 0.22419655 0.21626332 0.20207286][0.25835085 0.2642816 0.26258552 0.258487 0.25247121 0.244999 0.23651989 0.2274489 0.21844956 0.21125981 0.20605604 0.20275892 0.19945344 0.19312072 0.18116528]]...]
INFO - root - 2017-12-09 19:09:16.872371: step 50210, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.884 sec/batch; 69h:20m:36s remains)
INFO - root - 2017-12-09 19:09:25.474473: step 50220, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.888 sec/batch; 69h:36m:25s remains)
INFO - root - 2017-12-09 19:09:34.247107: step 50230, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.888 sec/batch; 69h:39m:29s remains)
INFO - root - 2017-12-09 19:09:42.796267: step 50240, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.886 sec/batch; 69h:26m:58s remains)
INFO - root - 2017-12-09 19:09:51.244706: step 50250, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 67h:57m:22s remains)
INFO - root - 2017-12-09 19:09:59.885641: step 50260, loss = 0.82, batch loss = 0.69 (9.7 examples/sec; 0.826 sec/batch; 64h:45m:59s remains)
INFO - root - 2017-12-09 19:10:08.570462: step 50270, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.859 sec/batch; 67h:19m:42s remains)
INFO - root - 2017-12-09 19:10:17.210587: step 50280, loss = 0.82, batch loss = 0.69 (9.7 examples/sec; 0.824 sec/batch; 64h:33m:32s remains)
INFO - root - 2017-12-09 19:10:25.854435: step 50290, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.851 sec/batch; 66h:40m:49s remains)
INFO - root - 2017-12-09 19:10:34.358734: step 50300, loss = 0.82, batch loss = 0.69 (9.8 examples/sec; 0.813 sec/batch; 63h:45m:38s remains)
2017-12-09 19:10:35.261237: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.34509379 0.35222098 0.3556141 0.35815275 0.35856125 0.35754964 0.35519272 0.35202783 0.34895164 0.34593537 0.34232906 0.33848158 0.33179167 0.32367203 0.31283879][0.37600356 0.3833456 0.38705036 0.39032233 0.39126426 0.3906309 0.38785136 0.3843253 0.38090572 0.377134 0.37326854 0.36847481 0.36078984 0.35211134 0.34052548][0.39019933 0.39704639 0.40069315 0.40426651 0.4059647 0.40542161 0.4020032 0.39813125 0.39417055 0.38978073 0.38470572 0.37865344 0.37059122 0.36174434 0.3504287][0.39628848 0.40311652 0.40716445 0.41081631 0.41302097 0.41276643 0.40866974 0.40410432 0.39896479 0.39355007 0.38664004 0.37849215 0.36872634 0.35870832 0.3483355][0.3941637 0.40152523 0.40617141 0.41069564 0.41378972 0.41406468 0.4100076 0.40457594 0.39762411 0.38984311 0.38037533 0.36975214 0.35834837 0.34733516 0.33748978][0.38828513 0.39650545 0.40100828 0.40510917 0.40790397 0.40771911 0.40265119 0.3957285 0.38671347 0.37621993 0.36408234 0.35098872 0.338783 0.32790175 0.31965384][0.3770178 0.3844783 0.38626748 0.38796285 0.38810703 0.38515428 0.37776187 0.36779556 0.35596764 0.34310707 0.32958806 0.3160091 0.30524167 0.29708758 0.29330963][0.35509923 0.35980707 0.35705239 0.35460469 0.35042197 0.34348673 0.33292669 0.32050082 0.30668572 0.29174826 0.27796474 0.26599669 0.25921065 0.25596085 0.25812739][0.32440373 0.32568595 0.317353 0.30939996 0.29911825 0.28871992 0.2752777 0.26042205 0.24538395 0.23057207 0.21895152 0.2105049 0.2089739 0.21133544 0.21966566][0.29211536 0.28864437 0.27458584 0.26102117 0.24514858 0.23081973 0.21532498 0.199761 0.18476769 0.17068143 0.16159362 0.15680666 0.15959688 0.16771767 0.18159227][0.25537404 0.24862233 0.2303445 0.21124086 0.19156708 0.17399383 0.15656075 0.14136358 0.12858707 0.11817116 0.11224429 0.11171558 0.11899608 0.13179047 0.14960174][0.22494139 0.21527876 0.19477299 0.17224865 0.14990033 0.13149679 0.11514535 0.101434 0.090144873 0.082148187 0.078883827 0.081299596 0.091088049 0.10661342 0.12659559][0.19995083 0.19156209 0.17178108 0.1495637 0.12816912 0.11120065 0.096937247 0.085310519 0.076181449 0.06987571 0.067228466 0.0694511 0.078428321 0.093035541 0.11261811][0.1825279 0.17638038 0.15912721 0.14068477 0.12340984 0.11029854 0.10065812 0.092543215 0.086479977 0.081677146 0.079205953 0.080334388 0.086939394 0.098967776 0.11551077][0.17698868 0.1738293 0.16033885 0.14669596 0.13450992 0.12676409 0.12140493 0.11674895 0.11290192 0.10909887 0.10620666 0.10505888 0.10856787 0.11699973 0.12953649]]...]
INFO - root - 2017-12-09 19:10:43.999794: step 50310, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.874 sec/batch; 68h:31m:50s remains)
INFO - root - 2017-12-09 19:10:52.749190: step 50320, loss = 0.81, batch loss = 0.69 (9.4 examples/sec; 0.853 sec/batch; 66h:49m:19s remains)
INFO - root - 2017-12-09 19:11:01.543427: step 50330, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.891 sec/batch; 69h:49m:48s remains)
INFO - root - 2017-12-09 19:11:10.260554: step 50340, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 68h:02m:14s remains)
INFO - root - 2017-12-09 19:11:18.765012: step 50350, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.870 sec/batch; 68h:09m:20s remains)
INFO - root - 2017-12-09 19:11:27.439157: step 50360, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.837 sec/batch; 65h:33m:54s remains)
INFO - root - 2017-12-09 19:11:35.827635: step 50370, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.834 sec/batch; 65h:23m:52s remains)
INFO - root - 2017-12-09 19:11:44.395508: step 50380, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.870 sec/batch; 68h:12m:10s remains)
INFO - root - 2017-12-09 19:11:53.017003: step 50390, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.870 sec/batch; 68h:10m:02s remains)
INFO - root - 2017-12-09 19:12:01.361156: step 50400, loss = 0.82, batch loss = 0.69 (11.1 examples/sec; 0.720 sec/batch; 56h:25m:43s remains)
2017-12-09 19:12:02.252922: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.14433651 0.15164649 0.15734597 0.16085024 0.16302268 0.16323903 0.16207466 0.16029474 0.15728772 0.15418141 0.15174355 0.14996295 0.14890935 0.14763294 0.14624687][0.1453653 0.15397403 0.16087721 0.16556798 0.16867357 0.16951659 0.16867602 0.16680473 0.16418572 0.16163284 0.15984581 0.15914753 0.15907617 0.158862 0.15800226][0.14155471 0.15132113 0.15930611 0.16510728 0.16888335 0.17014241 0.16975197 0.16817074 0.16623165 0.16444759 0.16373867 0.16423756 0.16508766 0.16580456 0.16524351][0.13697547 0.14748259 0.15596816 0.1623272 0.1665518 0.1682726 0.1685762 0.16763258 0.16661915 0.16592498 0.16647911 0.16818248 0.16978909 0.17090082 0.17013188][0.13332468 0.14416845 0.15282013 0.15951799 0.16411519 0.16656429 0.16785246 0.16800097 0.16814111 0.16866982 0.17040356 0.17268659 0.17439388 0.17496406 0.17302211][0.13086633 0.14138676 0.14949077 0.15609255 0.16103509 0.164351 0.16680324 0.1682774 0.16957632 0.17091747 0.17307697 0.17503588 0.17593715 0.17502882 0.1713528][0.1281732 0.13803864 0.14544559 0.15173723 0.15683436 0.1610269 0.16453603 0.16708946 0.1690454 0.17048286 0.17196621 0.17261618 0.1719244 0.16901213 0.1634602][0.12455521 0.13399962 0.14101849 0.14725691 0.15267479 0.15732098 0.1612078 0.16398919 0.16556102 0.1660493 0.16587727 0.16445202 0.16172169 0.15673809 0.14955989][0.12031388 0.12924129 0.13569407 0.14163283 0.14707296 0.15168791 0.15524581 0.1573171 0.1576525 0.15641336 0.15404399 0.1504651 0.14581373 0.13925152 0.13116059][0.11481284 0.12314685 0.12901931 0.13451274 0.13945302 0.14327699 0.14562294 0.1461321 0.14457178 0.14117381 0.13661055 0.13128322 0.12536794 0.11820959 0.11007258][0.10713305 0.11476918 0.11993996 0.1246795 0.12855531 0.13085921 0.13129683 0.12964064 0.12584406 0.1204472 0.11436246 0.1082416 0.1021748 0.095581852 0.08846084][0.096622884 0.10343129 0.10782993 0.11158317 0.11400577 0.11442011 0.11264637 0.10874856 0.10303527 0.096307851 0.089752048 0.084007926 0.078979753 0.073924094 0.068544894][0.082264334 0.08797843 0.09152317 0.094144136 0.095019355 0.093655065 0.090063512 0.084636331 0.077931695 0.070995912 0.065102443 0.060731202 0.057519462 0.054496039 0.05114124][0.064032577 0.068438776 0.071089573 0.072680391 0.072383255 0.069920339 0.06548547 0.059664644 0.053196706 0.047145702 0.042713873 0.0401427 0.038838129 0.037716623 0.03616662][0.04446958 0.047622997 0.049497817 0.050317146 0.049413413 0.046684846 0.042434726 0.037287775 0.031984814 0.027479002 0.024719039 0.023718433 0.023777192 0.023903487 0.023545042]]...]
INFO - root - 2017-12-09 19:12:10.936928: step 50410, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.858 sec/batch; 67h:12m:28s remains)
INFO - root - 2017-12-09 19:12:19.474245: step 50420, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.846 sec/batch; 66h:17m:11s remains)
INFO - root - 2017-12-09 19:12:28.032569: step 50430, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.865 sec/batch; 67h:48m:06s remains)
INFO - root - 2017-12-09 19:12:36.465869: step 50440, loss = 0.82, batch loss = 0.69 (10.9 examples/sec; 0.736 sec/batch; 57h:37m:59s remains)
INFO - root - 2017-12-09 19:12:45.133641: step 50450, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.870 sec/batch; 68h:07m:30s remains)
INFO - root - 2017-12-09 19:12:53.783258: step 50460, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 68h:15m:36s remains)
INFO - root - 2017-12-09 19:13:02.286947: step 50470, loss = 0.82, batch loss = 0.69 (9.7 examples/sec; 0.824 sec/batch; 64h:33m:05s remains)
INFO - root - 2017-12-09 19:13:10.897680: step 50480, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.836 sec/batch; 65h:30m:21s remains)
INFO - root - 2017-12-09 19:13:19.517032: step 50490, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 68h:15m:45s remains)
INFO - root - 2017-12-09 19:13:27.927656: step 50500, loss = 0.82, batch loss = 0.69 (11.5 examples/sec; 0.693 sec/batch; 54h:17m:34s remains)
2017-12-09 19:13:28.836696: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0018296589 -0.0018286632 -0.001828454 -0.0018282934 -0.0018280119 -0.0018278457 -0.0018277313 -0.0018277352 -0.0018277199 -0.0018277228 -0.001827703 -0.0018276536 -0.0018276774 -0.0018278268 -0.0018279075][-0.0018272066 -0.0018234872 -0.0018201178 -0.0018181398 -0.0018187356 -0.0018214758 -0.0018245183 -0.001826364 -0.0018270659 -0.0018272668 -0.0018273077 -0.0018273372 -0.0018274065 -0.0018274528 -0.0018274644][-0.0018170088 -0.0018072253 -0.0017970976 -0.0017903681 -0.0017914805 -0.0018006898 -0.0018126341 -0.0018216306 -0.0018258709 -0.0018271647 -0.0018273821 -0.0018272852 -0.0018271988 -0.0018271409 -0.0018271398][-0.0018012837 -0.0017829709 -0.0017622411 -0.0017457802 -0.001744159 -0.0017604299 -0.0017858081 -0.0018080788 -0.0018210902 -0.0018259409 -0.0018261567 -0.0018238765 -0.0018209645 -0.0018196342 -0.0018210781][-0.0017764044 -0.0017391628 -0.0016897129 -0.0016458069 -0.0016356694 -0.0016682073 -0.0017228273 -0.0017714071 -0.0018008198 -0.0018142691 -0.0018182427 -0.0018148418 -0.0018069672 -0.001801385 -0.0018038619][-0.0017321954 -0.0016428923 -0.0015169631 -0.0013993601 -0.0013589831 -0.0014217342 -0.001545792 -0.0016648012 -0.0017414514 -0.0017772455 -0.0017874591 -0.0017820036 -0.0017694498 -0.0017630464 -0.0017722552][-0.0016516974 -0.0014526157 -0.0011675655 -0.00089591491 -0.00078179815 -0.00088407006 -0.0011240039 -0.0013671957 -0.0015338976 -0.0016209275 -0.001653498 -0.0016530505 -0.0016408442 -0.0016464699 -0.0016854212][-0.0015182635 -0.0011427908 -0.00060010934 -6.5453583e-05 0.00020529667 8.7318942e-05 -0.00030395144 -0.00073253328 -0.0010380152 -0.0011988077 -0.001257245 -0.0012635507 -0.0012672108 -0.0013239032 -0.0014519361][-0.0013376464 -0.00073265482 0.0001470641 0.0010433671 0.0015661878 0.0015001016 0.00098256429 0.00035674789 -0.00011236477 -0.00036301534 -0.00044463354 -0.00044522551 -0.00046580704 -0.0006169749 -0.00092641223][-0.001152738 -0.00032083644 0.00090263865 0.002193525 0.0030378471 0.0031152847 0.0025631948 0.0018154295 0.0012386477 0.00093696092 0.00085524318 0.00087129662 0.00082456355 0.00053442491 -4.4741319e-05][-0.0010503626 -7.6398836e-05 0.0013837056 0.0029845932 0.0041403775 0.004445048 0.0040030312 0.0032851947 0.0027249544 0.0024635061 0.0024436028 0.0025127712 0.002448197 0.0019942098 0.0010865704][-0.0010808981 -0.00011185545 0.0013837958 0.0030945088 0.0044378466 0.0049729608 0.00476023 0.004245373 0.0038604187 0.0037471615 0.0038411296 0.0039767781 0.0039010462 0.0033100084 0.0021206108][-0.0012384118 -0.00042516738 0.00087870879 0.0024414766 0.0037674871 0.00444421 0.0044861212 0.004261483 0.0041394783 0.0042323866 0.0044638868 0.0046637096 0.0045792041 0.003913444 0.0025940309][-0.0014581369 -0.00088862778 7.0780749e-05 0.0012776827 0.0023751073 0.0030411179 0.0032635247 0.003304407 0.0034272512 0.0036937492 0.0040188869 0.0042465627 0.0041598412 0.0035175509 0.002279141][-0.0016544574 -0.0013357839 -0.00075800414 1.6174628e-05 0.00077354035 0.0013016224 0.0015775113 0.0017585944 0.0020056069 0.0023390488 0.0026758001 0.002885011 0.0028042207 0.0022760751 0.0013004631]]...]
INFO - root - 2017-12-09 19:13:37.551837: step 50510, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.875 sec/batch; 68h:30m:48s remains)
INFO - root - 2017-12-09 19:13:46.257296: step 50520, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.881 sec/batch; 69h:02m:37s remains)
INFO - root - 2017-12-09 19:13:54.908116: step 50530, loss = 0.82, batch loss = 0.69 (8.6 examples/sec; 0.929 sec/batch; 72h:46m:15s remains)
INFO - root - 2017-12-09 19:14:03.486691: step 50540, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.860 sec/batch; 67h:21m:24s remains)
INFO - root - 2017-12-09 19:14:11.714191: step 50550, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.860 sec/batch; 67h:19m:01s remains)
INFO - root - 2017-12-09 19:14:20.243180: step 50560, loss = 0.82, batch loss = 0.69 (9.7 examples/sec; 0.825 sec/batch; 64h:38m:14s remains)
INFO - root - 2017-12-09 19:14:28.782661: step 50570, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.875 sec/batch; 68h:31m:16s remains)
INFO - root - 2017-12-09 19:14:37.540501: step 50580, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.897 sec/batch; 70h:16m:23s remains)
INFO - root - 2017-12-09 19:14:46.208015: step 50590, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.874 sec/batch; 68h:28m:34s remains)
INFO - root - 2017-12-09 19:14:54.892372: step 50600, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.876 sec/batch; 68h:33m:31s remains)
2017-12-09 19:14:55.654516: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0018332275 -0.0018327245 -0.0018326857 -0.0018326815 -0.0018316588 -0.001827996 -0.001820925 -0.0018132371 -0.0018096294 -0.0018123194 -0.0018194213 -0.0018264849 -0.001830634 -0.0018322028 -0.0018326209][-0.0018335119 -0.0018330327 -0.0018327609 -0.0018305684 -0.0018209373 -0.0017982946 -0.0017653998 -0.0017372115 -0.0017309471 -0.0017501578 -0.0017824671 -0.0018106997 -0.001826469 -0.0018319825 -0.0018329115][-0.0018342366 -0.0018337817 -0.0018321804 -0.0018208059 -0.0017813141 -0.0016983481 -0.0015863434 -0.00149671 -0.0014818849 -0.0015477994 -0.0016533526 -0.0017467847 -0.0018034106 -0.0018268673 -0.0018330362][-0.0018350052 -0.0018344296 -0.0018283364 -0.0017936609 -0.0016855298 -0.0014691612 -0.0011816811 -0.00094752526 -0.00089625933 -0.0010509717 -0.0013191152 -0.0015714458 -0.0017354513 -0.0018091856 -0.0018313247][-0.001835499 -0.0018334209 -0.0018156396 -0.0017327562 -0.0014969771 -0.0010447772 -0.00045268773 3.5904581e-05 0.00015943882 -0.00014535757 -0.00070407311 -0.0012486833 -0.0016112456 -0.0017769051 -0.0018269435][-0.0018340583 -0.0018262066 -0.0017841238 -0.0016214876 -0.0011948359 -0.00041127065 0.00059168891 0.0014060371 0.0015951748 0.0010526412 8.1200153e-05 -0.00085556938 -0.0014691171 -0.0017427248 -0.0018219494][-0.0018272707 -0.0018045657 -0.0017215733 -0.0014538382 -0.00080520473 0.00033620431 0.0017513126 0.0028516003 0.003032635 0.0021791393 0.00076344714 -0.00054700708 -0.0013717267 -0.0017234884 -0.0018193483][-0.001817901 -0.0017742292 -0.0016424647 -0.0012745827 -0.00044909376 0.00093952694 0.0025955541 0.0038042273 0.003877826 0.0027473285 0.0010347675 -0.00046804023 -0.0013658786 -0.0017278889 -0.0018197595][-0.0018124741 -0.0017554972 -0.0015960557 -0.0011849192 -0.00031404092 0.0010911577 0.0026983507 0.0037862747 0.0037139086 0.002490378 0.0007832 -0.00063786423 -0.0014429856 -0.0017496522 -0.0018222327][-0.0018150953 -0.0017615336 -0.0016143208 -0.0012469515 -0.00049622927 0.00067544437 0.0019624936 0.0027639796 0.0025930116 0.0015242113 0.00013466494 -0.00096784969 -0.00156288 -0.001777849 -0.0018254874][-0.0018235033 -0.0017866099 -0.0016823201 -0.0014218269 -0.00089830317 -0.00010130566 0.00074316433 0.0012268861 0.0010493296 0.00030154537 -0.00061627 -0.0013151162 -0.0016777902 -0.0018028797 -0.0018289854][-0.0018318463 -0.0018136775 -0.0017578863 -0.0016135037 -0.0013226799 -0.00088707136 -0.00043967064 -0.00020298432 -0.00032509828 -0.00073834381 -0.0012227686 -0.001579605 -0.0017597119 -0.0018197105 -0.001831686][-0.0018364216 -0.0018303778 -0.0018084446 -0.0017474894 -0.0016218716 -0.0014346889 -0.0012464162 -0.0011524056 -0.0012116502 -0.0013889105 -0.0015897581 -0.0017341075 -0.0018055087 -0.0018284516 -0.0018326619][-0.0018373942 -0.0018362427 -0.0018300576 -0.0018111035 -0.0017705844 -0.0017099259 -0.0016494102 -0.0016198228 -0.0016392625 -0.0016956013 -0.0017586481 -0.0018034065 -0.0018254371 -0.0018320695 -0.0018329576][-0.0018368713 -0.0018367394 -0.0018353525 -0.0018308897 -0.0018212574 -0.0018068817 -0.0017927436 -0.0017857836 -0.0017895971 -0.0018018585 -0.0018161191 -0.001826572 -0.001831762 -0.001833022 -0.0018328852]]...]
INFO - root - 2017-12-09 19:15:04.325789: step 50610, loss = 0.82, batch loss = 0.69 (8.8 examples/sec; 0.907 sec/batch; 70h:59m:42s remains)
INFO - root - 2017-12-09 19:15:13.149964: step 50620, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.893 sec/batch; 69h:56m:26s remains)
INFO - root - 2017-12-09 19:15:21.886034: step 50630, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 67h:55m:01s remains)
INFO - root - 2017-12-09 19:15:30.632149: step 50640, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.886 sec/batch; 69h:23m:39s remains)
INFO - root - 2017-12-09 19:15:38.927569: step 50650, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.859 sec/batch; 67h:14m:24s remains)
INFO - root - 2017-12-09 19:15:47.601920: step 50660, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 67h:27m:34s remains)
INFO - root - 2017-12-09 19:15:56.198629: step 50670, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.889 sec/batch; 69h:35m:51s remains)
INFO - root - 2017-12-09 19:16:04.785022: step 50680, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.882 sec/batch; 69h:01m:47s remains)
INFO - root - 2017-12-09 19:16:13.477718: step 50690, loss = 0.81, batch loss = 0.68 (9.4 examples/sec; 0.852 sec/batch; 66h:43m:07s remains)
INFO - root - 2017-12-09 19:16:22.179810: step 50700, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.877 sec/batch; 68h:41m:18s remains)
2017-12-09 19:16:22.986377: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0018130995 -0.0018122463 -0.0018124182 -0.0018125835 -0.0018125637 -0.0018124006 -0.0018121819 -0.0018120415 -0.0018120415 -0.0018121364 -0.0018121824 -0.0018121063 -0.0018119363 -0.0018117686 -0.0018116768][-0.0018124494 -0.0018116275 -0.0018118065 -0.0018119285 -0.0018118485 -0.0018116341 -0.0018113873 -0.0018112309 -0.0018112652 -0.0018114473 -0.0018115707 -0.001811531 -0.0018113733 -0.0018112062 -0.0018111463][-0.0018124614 -0.0018117379 -0.0018119465 -0.0018120253 -0.0018118692 -0.0018115713 -0.0018112618 -0.0018110515 -0.0018110995 -0.0018113483 -0.0018115639 -0.0018115917 -0.0018114857 -0.0018113803 -0.0018113989][-0.001812558 -0.001811953 -0.0018122031 -0.0018122661 -0.0018120462 -0.0018116437 -0.0018112405 -0.0018109453 -0.0018109846 -0.0018112987 -0.0018116226 -0.0018117528 -0.0018117449 -0.0018117501 -0.0018118656][-0.0018126598 -0.0018121377 -0.0018124405 -0.0018125316 -0.0018122867 -0.0018117924 -0.0018112888 -0.0018109127 -0.0018109367 -0.001811297 -0.0018117003 -0.0018119231 -0.0018120428 -0.0018122031 -0.0018124661][-0.001812579 -0.001812126 -0.001812502 -0.0018126464 -0.0018124249 -0.0018118938 -0.0018113224 -0.0018108838 -0.0018108701 -0.0018112335 -0.0018116691 -0.0018119769 -0.0018122506 -0.0018126137 -0.0018130579][-0.0018123493 -0.0018119114 -0.0018123481 -0.0018125784 -0.0018124379 -0.0018119308 -0.001811348 -0.0018108431 -0.0018107627 -0.0018110876 -0.0018115313 -0.0018119126 -0.0018123509 -0.0018129472 -0.0018135756][-0.0018121087 -0.001811596 -0.0018120324 -0.001812332 -0.0018123072 -0.0018118903 -0.0018113323 -0.001810775 -0.001810628 -0.0018108978 -0.0018113208 -0.0018117711 -0.0018123611 -0.0018131664 -0.0018139514][-0.001811998 -0.0018113643 -0.0018117268 -0.0018120043 -0.0018120338 -0.0018117179 -0.0018112286 -0.0018106913 -0.0018105023 -0.0018107115 -0.0018110885 -0.001811559 -0.0018122352 -0.0018131684 -0.0018140521][-0.0018120066 -0.0018113389 -0.0018115721 -0.0018117307 -0.0018117469 -0.0018115033 -0.0018110845 -0.0018105712 -0.0018103333 -0.0018104694 -0.0018107732 -0.001811226 -0.0018119494 -0.0018129626 -0.0018138987][-0.0018122302 -0.0018116006 -0.0018117647 -0.0018118124 -0.0018117677 -0.0018115343 -0.0018111189 -0.0018105959 -0.0018102898 -0.0018103212 -0.0018105253 -0.0018109225 -0.0018116459 -0.001812653 -0.0018135472][-0.0018124148 -0.0018119473 -0.0018121299 -0.0018121839 -0.0018121302 -0.0018119133 -0.0018115132 -0.0018109927 -0.0018106279 -0.0018105335 -0.0018106081 -0.001810887 -0.0018114947 -0.0018123795 -0.0018131474][-0.0018124734 -0.0018122094 -0.0018124774 -0.0018126384 -0.0018126641 -0.0018125203 -0.0018121788 -0.0018117202 -0.0018113266 -0.001811108 -0.001811032 -0.0018111434 -0.0018115676 -0.0018122364 -0.0018128143][-0.0018124091 -0.0018122778 -0.0018126252 -0.0018129376 -0.0018131008 -0.0018130841 -0.0018128526 -0.0018124869 -0.0018121144 -0.0018118402 -0.0018116458 -0.0018115905 -0.0018118145 -0.001812271 -0.0018126781][-0.0018122875 -0.0018121728 -0.0018125791 -0.0018130738 -0.001813423 -0.0018135858 -0.0018135132 -0.0018132742 -0.0018129764 -0.0018126853 -0.0018124111 -0.0018122272 -0.0018122805 -0.0018125391 -0.0018127777]]...]
INFO - root - 2017-12-09 19:16:31.618647: step 50710, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 67h:27m:18s remains)
INFO - root - 2017-12-09 19:16:40.421829: step 50720, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.870 sec/batch; 68h:06m:08s remains)
INFO - root - 2017-12-09 19:16:49.091061: step 50730, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.859 sec/batch; 67h:16m:00s remains)
INFO - root - 2017-12-09 19:16:57.773903: step 50740, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.884 sec/batch; 69h:13m:30s remains)
INFO - root - 2017-12-09 19:17:06.171227: step 50750, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.841 sec/batch; 65h:49m:14s remains)
INFO - root - 2017-12-09 19:17:14.933294: step 50760, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.872 sec/batch; 68h:13m:10s remains)
INFO - root - 2017-12-09 19:17:23.652432: step 50770, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.870 sec/batch; 68h:04m:04s remains)
INFO - root - 2017-12-09 19:17:32.402758: step 50780, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.882 sec/batch; 69h:00m:13s remains)
INFO - root - 2017-12-09 19:17:41.123047: step 50790, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.854 sec/batch; 66h:51m:38s remains)
INFO - root - 2017-12-09 19:17:49.870977: step 50800, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.883 sec/batch; 69h:05m:07s remains)
2017-12-09 19:17:50.746775: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0018335253 -0.0018326519 -0.0018329531 -0.0018337677 -0.001834631 -0.0018348754 -0.0018338633 -0.0018317743 -0.0018287989 -0.001825303 -0.0018217065 -0.0018183747 -0.0018157638 -0.001814308 -0.0018138846][-0.0018344126 -0.0018334414 -0.0018335528 -0.0018342241 -0.0018348695 -0.0018348052 -0.0018335464 -0.0018312603 -0.001828182 -0.0018246358 -0.0018209696 -0.001817575 -0.0018149989 -0.0018135626 -0.0018131364][-0.0018340612 -0.0018329774 -0.0018328971 -0.0018331924 -0.0018333677 -0.001832953 -0.0018315455 -0.0018292943 -0.0018263408 -0.0018231254 -0.0018199146 -0.0018169144 -0.0018146017 -0.0018133747 -0.0018130548][-0.0018320396 -0.0018309593 -0.0018306938 -0.0018306305 -0.0018304557 -0.0018298062 -0.0018283388 -0.0018262747 -0.0018237004 -0.001821047 -0.0018183908 -0.0018159891 -0.0018142018 -0.0018133097 -0.0018131724][-0.0018293504 -0.0018283994 -0.0018281186 -0.0018279904 -0.0018278254 -0.0018272016 -0.0018257658 -0.0018237501 -0.0018213922 -0.0018190553 -0.0018168619 -0.0018150359 -0.0018138543 -0.0018134046 -0.0018135075][-0.0018264081 -0.0018255339 -0.0018253224 -0.0018253054 -0.0018251461 -0.0018245196 -0.0018231828 -0.0018213394 -0.0018192137 -0.0018171537 -0.0018154464 -0.0018142689 -0.0018136489 -0.0018135621 -0.00181385][-0.0018236173 -0.0018226479 -0.001822415 -0.0018223191 -0.0018220498 -0.0018214041 -0.0018202782 -0.0018186835 -0.0018169301 -0.001815379 -0.0018142577 -0.0018136653 -0.0018135139 -0.0018137504 -0.0018141875][-0.0018211768 -0.0018201392 -0.0018198803 -0.0018197939 -0.001819518 -0.0018189393 -0.0018180172 -0.001816702 -0.0018153262 -0.001814206 -0.0018135573 -0.0018133806 -0.0018135208 -0.001813935 -0.0018144159][-0.0018191064 -0.0018181881 -0.001818 -0.0018180511 -0.0018179376 -0.0018175261 -0.0018168541 -0.0018158609 -0.001814795 -0.0018139632 -0.0018135324 -0.0018135128 -0.0018137419 -0.0018141515 -0.0018145517][-0.0018172794 -0.001816545 -0.001816534 -0.0018166883 -0.0018166812 -0.0018163753 -0.0018158699 -0.0018151426 -0.0018143703 -0.001813818 -0.0018135938 -0.0018136636 -0.0018138795 -0.0018141915 -0.0018144888][-0.001815906 -0.0018152787 -0.0018153474 -0.0018155411 -0.0018155564 -0.0018153299 -0.0018149875 -0.001814528 -0.0018140828 -0.0018137703 -0.001813659 -0.0018137277 -0.0018138522 -0.0018140462 -0.0018142401][-0.0018153297 -0.0018147371 -0.0018148141 -0.0018150094 -0.0018150356 -0.0018148763 -0.001814655 -0.0018143803 -0.0018141352 -0.001813966 -0.0018139009 -0.001813926 -0.0018139655 -0.0018140391 -0.0018141449][-0.001815047 -0.0018144832 -0.0018145457 -0.0018147214 -0.0018147806 -0.0018146851 -0.0018145354 -0.0018143842 -0.0018142648 -0.0018141655 -0.0018141113 -0.0018141152 -0.0018141086 -0.0018141189 -0.0018141936][-0.0018150774 -0.0018145462 -0.0018144724 -0.0018145569 -0.0018146087 -0.0018145544 -0.0018144535 -0.0018143763 -0.0018143229 -0.0018142688 -0.0018142281 -0.0018142172 -0.0018141993 -0.0018142046 -0.0018142799][-0.0018152606 -0.0018146499 -0.0018144792 -0.0018144943 -0.001814509 -0.0018144681 -0.0018144221 -0.0018143944 -0.0018143671 -0.0018143376 -0.0018143111 -0.0018143028 -0.0018142936 -0.0018143089 -0.0018143875]]...]
INFO - root - 2017-12-09 19:17:59.464514: step 50810, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.874 sec/batch; 68h:24m:47s remains)
INFO - root - 2017-12-09 19:18:08.234965: step 50820, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.876 sec/batch; 68h:32m:30s remains)
INFO - root - 2017-12-09 19:18:16.952140: step 50830, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.879 sec/batch; 68h:45m:29s remains)
INFO - root - 2017-12-09 19:18:25.824078: step 50840, loss = 0.82, batch loss = 0.69 (8.2 examples/sec; 0.979 sec/batch; 76h:33m:26s remains)
INFO - root - 2017-12-09 19:18:34.267262: step 50850, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.880 sec/batch; 68h:48m:38s remains)
INFO - root - 2017-12-09 19:18:42.937187: step 50860, loss = 0.82, batch loss = 0.69 (9.8 examples/sec; 0.819 sec/batch; 64h:03m:02s remains)
INFO - root - 2017-12-09 19:18:51.736691: step 50870, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 67h:56m:45s remains)
INFO - root - 2017-12-09 19:19:00.465138: step 50880, loss = 0.81, batch loss = 0.68 (8.8 examples/sec; 0.908 sec/batch; 71h:00m:45s remains)
INFO - root - 2017-12-09 19:19:09.061846: step 50890, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.846 sec/batch; 66h:10m:27s remains)
INFO - root - 2017-12-09 19:19:17.640185: step 50900, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.832 sec/batch; 65h:06m:55s remains)
2017-12-09 19:19:18.513432: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0018270442 -0.0018306874 -0.0018346945 -0.0018370905 -0.0018382125 -0.0018378004 -0.0018361999 -0.0018342218 -0.0018324159 -0.0018309021 -0.0018296129 -0.0018289296 -0.0018281619 -0.0018273952 -0.001826398][-0.0018297782 -0.0018331651 -0.0018368013 -0.0018388013 -0.001839727 -0.0018393673 -0.0018378661 -0.0018359566 -0.0018340749 -0.0018323674 -0.0018306867 -0.0018295976 -0.0018284817 -0.0018273888 -0.0018261566][-0.0018330836 -0.0018357866 -0.0018388781 -0.0018406152 -0.001841478 -0.0018413167 -0.0018400947 -0.0018384125 -0.0018365205 -0.0018345016 -0.0018325229 -0.0018309654 -0.0018293853 -0.0018279179 -0.0018263712][-0.0018352714 -0.0018371888 -0.0018396486 -0.0018410158 -0.0018418861 -0.001841924 -0.0018411754 -0.0018398018 -0.0018380666 -0.0018359916 -0.0018338736 -0.0018319461 -0.0018300146 -0.001828262 -0.0018264097][-0.0018360014 -0.001837043 -0.0018386884 -0.0018397281 -0.0018406539 -0.0018410031 -0.0018405253 -0.0018393196 -0.0018377742 -0.0018358619 -0.0018338215 -0.0018317784 -0.0018297398 -0.0018277763 -0.0018258009][-0.0018357976 -0.0018359608 -0.0018368788 -0.0018375729 -0.0018383407 -0.0018386582 -0.0018381218 -0.0018369268 -0.0018354699 -0.001833943 -0.0018322298 -0.0018302139 -0.0018281788 -0.0018262054 -0.0018243527][-0.0018351272 -0.0018345767 -0.0018348288 -0.001835076 -0.0018353114 -0.0018351901 -0.0018343298 -0.0018331504 -0.0018320825 -0.0018311194 -0.0018297677 -0.0018279087 -0.0018261145 -0.0018243322 -0.0018227904][-0.0018347339 -0.0018334787 -0.0018329193 -0.0018325031 -0.0018319782 -0.0018310938 -0.0018297957 -0.0018289215 -0.0018286152 -0.0018283845 -0.0018274755 -0.0018260162 -0.0018246284 -0.001823124 -0.0018219184][-0.0018346375 -0.0018329981 -0.0018317869 -0.0018307169 -0.0018294393 -0.0018279575 -0.0018266045 -0.0018260574 -0.0018261702 -0.0018263572 -0.0018259767 -0.0018250499 -0.0018241124 -0.0018230441 -0.0018221257][-0.001834317 -0.0018327957 -0.0018315453 -0.0018300178 -0.0018282206 -0.001826534 -0.0018252652 -0.0018249379 -0.0018251385 -0.0018253956 -0.0018253009 -0.001825028 -0.0018246189 -0.0018240559 -0.0018235573][-0.0018340334 -0.0018332173 -0.0018324093 -0.0018308554 -0.001828917 -0.0018270664 -0.0018256658 -0.0018250919 -0.0018251525 -0.0018254075 -0.0018255712 -0.0018256907 -0.0018258346 -0.0018259196 -0.0018259729][-0.001833431 -0.0018334223 -0.0018333062 -0.0018320759 -0.0018302968 -0.0018285066 -0.0018269726 -0.0018259984 -0.0018257354 -0.0018259231 -0.0018262906 -0.0018267881 -0.0018275464 -0.0018282478 -0.0018288177][-0.0018317637 -0.001832557 -0.0018336019 -0.0018332818 -0.0018321094 -0.0018304255 -0.0018286931 -0.0018272355 -0.0018266086 -0.0018264914 -0.0018269186 -0.001827819 -0.0018290688 -0.0018303848 -0.0018313001][-0.0018294301 -0.0018310967 -0.0018332803 -0.0018342425 -0.0018340888 -0.0018326464 -0.0018306025 -0.0018285625 -0.0018276374 -0.0018271341 -0.0018275186 -0.0018286351 -0.0018302352 -0.0018319245 -0.0018331241][-0.0018279125 -0.0018297256 -0.0018323638 -0.0018341219 -0.0018344788 -0.0018332651 -0.0018311581 -0.0018290418 -0.0018277871 -0.0018272669 -0.0018277247 -0.00182897 -0.0018308101 -0.0018325944 -0.0018338184]]...]
INFO - root - 2017-12-09 19:19:27.039673: step 50910, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 68h:00m:41s remains)
INFO - root - 2017-12-09 19:19:35.561588: step 50920, loss = 0.81, batch loss = 0.68 (9.3 examples/sec; 0.864 sec/batch; 67h:34m:34s remains)
INFO - root - 2017-12-09 19:19:44.222265: step 50930, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 67h:46m:33s remains)
INFO - root - 2017-12-09 19:19:52.862427: step 50940, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.888 sec/batch; 69h:26m:42s remains)
INFO - root - 2017-12-09 19:20:01.216827: step 50950, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.876 sec/batch; 68h:29m:14s remains)
INFO - root - 2017-12-09 19:20:09.921304: step 50960, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.881 sec/batch; 68h:54m:21s remains)
INFO - root - 2017-12-09 19:20:18.601644: step 50970, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.858 sec/batch; 67h:04m:08s remains)
INFO - root - 2017-12-09 19:20:27.255383: step 50980, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 67h:55m:24s remains)
INFO - root - 2017-12-09 19:20:35.975006: step 50990, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.863 sec/batch; 67h:29m:08s remains)
INFO - root - 2017-12-09 19:20:44.640695: step 51000, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.892 sec/batch; 69h:46m:14s remains)
2017-12-09 19:20:45.587985: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0017900718 -0.0017887158 -0.0017894779 -0.0017906935 -0.0017917985 -0.0017925992 -0.0017928786 -0.0017926884 -0.0017920602 -0.0017912122 -0.00179046 -0.0017897859 -0.0017890565 -0.0017884917 -0.0017880561][-0.0017886335 -0.0017879436 -0.0017895625 -0.0017916934 -0.0017936232 -0.0017949716 -0.001795331 -0.0017948293 -0.0017936691 -0.001792168 -0.0017908533 -0.0017898087 -0.0017887815 -0.0017880092 -0.001787409][-0.0017883071 -0.0017883716 -0.0017908673 -0.0017939209 -0.0017965728 -0.0017982528 -0.0017984724 -0.0017974678 -0.001795585 -0.0017933762 -0.0017915473 -0.0017901942 -0.0017889909 -0.0017882037 -0.0017875772][-0.0017883341 -0.0017889314 -0.0017921093 -0.0017958526 -0.0017989327 -0.0018007922 -0.0018008255 -0.0017992581 -0.0017966309 -0.0017937549 -0.0017915589 -0.001790074 -0.0017888494 -0.0017881653 -0.0017876369][-0.0017886668 -0.0017894141 -0.0017928436 -0.0017967715 -0.0017998475 -0.0018015574 -0.001801235 -0.0017991581 -0.0017959866 -0.0017927559 -0.0017904401 -0.0017890416 -0.0017880368 -0.0017876701 -0.0017874413][-0.0017889147 -0.001789503 -0.0017927249 -0.0017963238 -0.001798917 -0.0018000816 -0.00179926 -0.0017967653 -0.0017933601 -0.0017901796 -0.0017881729 -0.001787249 -0.0017867299 -0.0017868563 -0.0017870875][-0.0017892062 -0.0017893949 -0.001792 -0.0017948505 -0.0017966842 -0.0017971457 -0.0017957558 -0.0017930241 -0.0017897506 -0.001787093 -0.00178574 -0.0017854626 -0.0017855313 -0.0017861064 -0.0017867303][-0.0017896283 -0.0017891587 -0.0017908552 -0.001792738 -0.0017937397 -0.001793591 -0.001791863 -0.001789209 -0.0017865153 -0.0017846603 -0.0017840241 -0.00178425 -0.0017847517 -0.0017856223 -0.0017864561][-0.0017898689 -0.0017887061 -0.0017894437 -0.0017903846 -0.0017907992 -0.001790432 -0.0017888147 -0.0017865087 -0.0017844795 -0.0017833791 -0.0017832557 -0.0017836601 -0.001784423 -0.001785463 -0.0017863165][-0.001789747 -0.0017882939 -0.0017883077 -0.0017885125 -0.0017884581 -0.0017879718 -0.0017866221 -0.001784816 -0.0017833542 -0.001782767 -0.0017829315 -0.0017834235 -0.0017842789 -0.0017853735 -0.0017862055][-0.0017898191 -0.0017881365 -0.0017876008 -0.0017873346 -0.0017870185 -0.0017865116 -0.0017855254 -0.0017843626 -0.0017835051 -0.0017832491 -0.001783479 -0.0017839044 -0.0017846073 -0.0017854791 -0.0017861179][-0.0017897654 -0.001787907 -0.0017871133 -0.001786613 -0.0017862199 -0.00178587 -0.0017852946 -0.0017847229 -0.0017843362 -0.0017842759 -0.0017844394 -0.001784657 -0.0017850596 -0.0017856047 -0.0017860011][-0.001789583 -0.0017876925 -0.0017868093 -0.0017863124 -0.0017859697 -0.0017857427 -0.0017854144 -0.0017851939 -0.0017851582 -0.001785195 -0.0017852777 -0.0017853406 -0.0017855038 -0.0017857564 -0.0017859152][-0.0017894362 -0.0017874873 -0.0017866958 -0.0017863424 -0.0017860675 -0.0017858901 -0.0017856533 -0.0017855354 -0.0017856352 -0.001785722 -0.001785755 -0.0017857971 -0.0017858313 -0.0017858548 -0.0017858462][-0.0017891806 -0.0017872375 -0.0017865661 -0.0017863599 -0.0017861732 -0.0017860434 -0.0017858718 -0.0017857844 -0.001785877 -0.0017859355 -0.0017859327 -0.0017859273 -0.0017858956 -0.001785832 -0.001785752]]...]
INFO - root - 2017-12-09 19:20:54.122402: step 51010, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.857 sec/batch; 67h:01m:26s remains)
INFO - root - 2017-12-09 19:21:02.982919: step 51020, loss = 0.81, batch loss = 0.68 (9.1 examples/sec; 0.878 sec/batch; 68h:38m:53s remains)
INFO - root - 2017-12-09 19:21:11.514640: step 51030, loss = 0.82, batch loss = 0.69 (9.7 examples/sec; 0.824 sec/batch; 64h:24m:35s remains)
INFO - root - 2017-12-09 19:21:20.044667: step 51040, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.848 sec/batch; 66h:15m:40s remains)
INFO - root - 2017-12-09 19:21:28.401358: step 51050, loss = 0.82, batch loss = 0.70 (9.2 examples/sec; 0.867 sec/batch; 67h:48m:41s remains)
INFO - root - 2017-12-09 19:21:36.943858: step 51060, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.850 sec/batch; 66h:24m:46s remains)
INFO - root - 2017-12-09 19:21:45.503142: step 51070, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.893 sec/batch; 69h:49m:14s remains)
INFO - root - 2017-12-09 19:21:54.237237: step 51080, loss = 0.82, batch loss = 0.69 (8.3 examples/sec; 0.966 sec/batch; 75h:32m:05s remains)
INFO - root - 2017-12-09 19:22:02.890179: step 51090, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.882 sec/batch; 68h:54m:36s remains)
INFO - root - 2017-12-09 19:22:11.628492: step 51100, loss = 0.82, batch loss = 0.69 (9.7 examples/sec; 0.821 sec/batch; 64h:08m:59s remains)
2017-12-09 19:22:12.560997: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0016733073 -0.0013902879 -0.0007895903 0.00023228873 0.001681056 0.0034450367 0.00532434 0.0070889541 0.0084358025 0.0089100972 0.0081051122 0.006092628 0.0034981244 0.0011323112 -0.00049792684][-0.0016863759 -0.0014030779 -0.00068971748 0.00071144325 0.0028407024 0.0054374249 0.0079641966 0.0098478962 0.010713816 0.010314493 0.0086237183 0.0060054311 0.0031657643 0.00081584335 -0.00070032571][-0.0016476517 -0.0012269388 -0.00015943765 0.0019090039 0.0049567991 0.0084524481 0.011484649 0.013203032 0.013270105 0.011763614 0.0090761054 0.0058420221 0.0027989489 0.00051424361 -0.00085769966][-0.0013946984 -0.00056297204 0.0012275482 0.0042939004 0.0084289974 0.012768041 0.016050374 0.017262543 0.016195128 0.013335928 0.0095328391 0.0056673731 0.0024446212 0.00023456651 -0.0010042703][-0.00062079879 0.000942208 0.0037971218 0.008080788 0.013328673 0.018345537 0.021603473 0.022045981 0.019664958 0.015343464 0.010341723 0.0057569579 0.0022635357 4.4956454e-05 -0.0011167896][0.00061303552 0.0030933102 0.0071150591 0.012563785 0.018712403 0.024117149 0.027086742 0.026612831 0.022924662 0.017243875 0.011161458 0.0059324405 0.0021916446 -5.1371753e-05 -0.0011649397][0.0019385676 0.0052714832 0.010250186 0.016522367 0.023144571 0.028569892 0.03108111 0.0297473 0.024992727 0.018279083 0.011435079 0.0058087162 0.0019721072 -0.00021358172 -0.0012384298][0.0029809694 0.0068774098 0.012350623 0.018883092 0.025418971 0.030473176 0.032426745 0.030444585 0.02506892 0.01788461 0.01081987 0.00522127 0.0015711955 -0.00039307331 -0.0012452138][0.0034025782 0.0074274265 0.012827616 0.018997101 0.024880914 0.029194813 0.030562617 0.028271794 0.022885937 0.015955158 0.0093423631 0.004269992 0.0010974134 -0.00052509992 -0.0011956198][0.0030487096 0.0067227953 0.011507791 0.016783912 0.021587435 0.024906946 0.025693044 0.023415361 0.018624024 0.012694975 0.0072231418 0.0031848876 0.00078781054 -0.00035543798 -0.0008071668][0.0020115161 0.0049327114 0.0086879125 0.012737842 0.016284784 0.018583784 0.01891298 0.016952626 0.013213742 0.0088044647 0.004917427 0.0022000042 0.0007157732 9.07667e-05 -0.00015247939][0.00077878439 0.0028146673 0.0054139104 0.0081813857 0.010533949 0.011972693 0.0120626 0.010661988 0.008208205 0.0055178371 0.0033626244 0.0020467532 0.0014982651 0.001392065 0.0013228721][-0.00030171988 0.0009746108 0.0025961949 0.0043022265 0.0057102353 0.0065146913 0.0064998562 0.005704436 0.0045340229 0.0035739434 0.0031980262 0.0033656643 0.0038502798 0.0043473043 0.0044648419][-0.0010966139 -0.000379727 0.00054336607 0.0015093187 0.0022966745 0.0027244934 0.0027294653 0.0025049569 0.0025000139 0.0031809276 0.0045796544 0.0063098916 0.0079611577 0.0091856392 0.009534806][-0.0015548685 -0.0011964957 -0.00070734741 -0.00018200255 0.0002699279 0.00054831395 0.00066900172 0.0009233217 0.0018685843 0.0039338842 0.0069110994 0.010127909 0.012978881 0.014994892 0.015609463]]...]
INFO - root - 2017-12-09 19:22:21.271550: step 51110, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.874 sec/batch; 68h:20m:56s remains)
INFO - root - 2017-12-09 19:22:29.782488: step 51120, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.837 sec/batch; 65h:24m:28s remains)
INFO - root - 2017-12-09 19:22:38.585595: step 51130, loss = 0.83, batch loss = 0.70 (8.9 examples/sec; 0.897 sec/batch; 70h:08m:30s remains)
INFO - root - 2017-12-09 19:22:47.420985: step 51140, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.885 sec/batch; 69h:09m:32s remains)
INFO - root - 2017-12-09 19:22:55.744037: step 51150, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.853 sec/batch; 66h:40m:48s remains)
INFO - root - 2017-12-09 19:23:04.332250: step 51160, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.881 sec/batch; 68h:51m:43s remains)
INFO - root - 2017-12-09 19:23:12.975659: step 51170, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.847 sec/batch; 66h:13m:05s remains)
INFO - root - 2017-12-09 19:23:21.594168: step 51180, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.831 sec/batch; 64h:55m:22s remains)
INFO - root - 2017-12-09 19:23:30.066631: step 51190, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.870 sec/batch; 67h:58m:55s remains)
INFO - root - 2017-12-09 19:23:38.685245: step 51200, loss = 0.82, batch loss = 0.69 (8.8 examples/sec; 0.904 sec/batch; 70h:38m:27s remains)
2017-12-09 19:23:39.607139: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0018048834 -0.0018029436 -0.001802396 -0.0018026348 -0.0018029934 -0.0018035097 -0.0018040355 -0.001804183 -0.0018042264 -0.0018039573 -0.0018036306 -0.0018032523 -0.0018028131 -0.0018024003 -0.0018018928][-0.0018102435 -0.0018081649 -0.0018072485 -0.00180736 -0.0018075348 -0.0018078334 -0.0018081476 -0.001808178 -0.001808145 -0.0018079125 -0.0018077206 -0.0018074316 -0.0018070872 -0.0018067306 -0.0018062586][-0.0018141704 -0.0018121917 -0.0018112224 -0.0018112144 -0.0018111762 -0.0018111384 -0.0018111129 -0.0018109443 -0.0018107241 -0.0018105416 -0.0018104287 -0.0018102534 -0.0018101918 -0.0018100431 -0.0018097308][-0.0018173154 -0.0018155291 -0.0018145976 -0.00181449 -0.0018140309 -0.0018134714 -0.0018129372 -0.0018124835 -0.001812003 -0.0018115409 -0.0018115005 -0.0018113623 -0.0018113252 -0.0018111171 -0.0018107081][-0.0018166136 -0.0018148753 -0.001814089 -0.0018138043 -0.0018130253 -0.0018122566 -0.0018114335 -0.001810733 -0.0018099318 -0.0018093189 -0.0018091574 -0.001808933 -0.0018087701 -0.0018084722 -0.0018080057][-0.0018124401 -0.0018107183 -0.0018100456 -0.0018098087 -0.0018090584 -0.0018084381 -0.0018076797 -0.0018069186 -0.0018061569 -0.0018056128 -0.0018054467 -0.0018051912 -0.0018050196 -0.0018046428 -0.0018040707][-0.0018065949 -0.0018045608 -0.0018037932 -0.001803431 -0.0018026771 -0.0018020719 -0.0018012266 -0.001800371 -0.0017995884 -0.0017990394 -0.001798839 -0.0017986384 -0.0017985086 -0.0017981223 -0.0017975571][-0.0018007613 -0.0017984502 -0.0017977396 -0.0017972395 -0.0017962263 -0.0017952871 -0.0017942297 -0.0017932305 -0.0017924376 -0.0017920184 -0.0017919713 -0.0017920058 -0.0017920077 -0.0017916996 -0.0017911493][-0.0017965832 -0.0017938394 -0.001793177 -0.0017924919 -0.0017913706 -0.0017904264 -0.0017894823 -0.0017885352 -0.0017877955 -0.0017874909 -0.0017875897 -0.0017877697 -0.0017878595 -0.0017876881 -0.0017872828][-0.0017931665 -0.0017904626 -0.0017897842 -0.0017890573 -0.0017881308 -0.0017873691 -0.0017866173 -0.0017857824 -0.0017851751 -0.0017850688 -0.0017851671 -0.0017854756 -0.0017857664 -0.0017858573 -0.001785732][-0.0017914404 -0.0017890439 -0.001788535 -0.0017879372 -0.0017871242 -0.0017863431 -0.001785644 -0.0017848415 -0.0017842674 -0.0017842487 -0.0017844889 -0.0017849391 -0.0017853735 -0.0017856952 -0.001785794][-0.0017906273 -0.0017884559 -0.0017879871 -0.0017874732 -0.0017867334 -0.0017858151 -0.0017849999 -0.0017841565 -0.0017835587 -0.0017834922 -0.0017838035 -0.0017843068 -0.0017848224 -0.0017853188 -0.0017855894][-0.0017901014 -0.0017882208 -0.0017878897 -0.0017875066 -0.001786697 -0.0017858184 -0.001785054 -0.0017841889 -0.0017835065 -0.001783503 -0.0017838634 -0.0017842916 -0.0017847754 -0.0017853398 -0.0017857201][-0.0017899631 -0.0017882466 -0.0017877227 -0.0017872546 -0.0017864401 -0.001785572 -0.0017847462 -0.0017838868 -0.0017831469 -0.0017831248 -0.0017834484 -0.0017838329 -0.0017843291 -0.0017849602 -0.0017854475][-0.0017900229 -0.0017882653 -0.0017875085 -0.0017869475 -0.00178616 -0.0017852318 -0.0017844614 -0.001783688 -0.0017829082 -0.0017827604 -0.0017829316 -0.0017832816 -0.0017838024 -0.0017844493 -0.0017850243]]...]
INFO - root - 2017-12-09 19:23:48.185735: step 51210, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.872 sec/batch; 68h:08m:44s remains)
INFO - root - 2017-12-09 19:23:56.926807: step 51220, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.861 sec/batch; 67h:15m:04s remains)
INFO - root - 2017-12-09 19:24:05.575129: step 51230, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.835 sec/batch; 65h:13m:14s remains)
INFO - root - 2017-12-09 19:24:14.252140: step 51240, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.866 sec/batch; 67h:37m:42s remains)
INFO - root - 2017-12-09 19:24:22.752973: step 51250, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.882 sec/batch; 68h:53m:52s remains)
INFO - root - 2017-12-09 19:24:31.437633: step 51260, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.881 sec/batch; 68h:51m:14s remains)
INFO - root - 2017-12-09 19:24:40.051316: step 51270, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.870 sec/batch; 67h:55m:42s remains)
INFO - root - 2017-12-09 19:24:48.700543: step 51280, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.876 sec/batch; 68h:25m:52s remains)
INFO - root - 2017-12-09 19:24:57.237112: step 51290, loss = 0.82, batch loss = 0.69 (9.8 examples/sec; 0.817 sec/batch; 63h:47m:08s remains)
INFO - root - 2017-12-09 19:25:05.770002: step 51300, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.841 sec/batch; 65h:43m:42s remains)
2017-12-09 19:25:06.655330: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.00073926593 -0.00029573916 0.00039877219 0.0016758166 0.0034900056 0.0058700405 0.0085922033 0.011471938 0.013916141 0.015803358 0.016784769 0.017183663 0.016807932 0.016105093 0.014649284][-0.00095855095 -0.00056209683 -2.2156863e-05 0.0010842098 0.0031887875 0.0061377636 0.0094432915 0.012747396 0.015703751 0.017866569 0.019070031 0.019543223 0.019303294 0.018704556 0.017477559][-0.0012546848 -0.000945863 -0.00042565179 0.00072597677 0.0028464668 0.0057678409 0.0092385281 0.012871653 0.016261118 0.018973585 0.020819267 0.021298662 0.02097977 0.020263419 0.019083][-0.0014503684 -0.0012174599 -0.00073753996 0.00033940526 0.0024317354 0.0054805949 0.0089048576 0.012453402 0.015884334 0.019113505 0.021541571 0.02249964 0.022343528 0.02129324 0.019848375][-0.0014277811 -0.0013487432 -0.00096520165 3.7508784e-05 0.0020460258 0.0050754091 0.0084392875 0.011701625 0.014965123 0.018165683 0.02080249 0.02214908 0.022202916 0.021276936 0.019673487][-0.0012591718 -0.0012436169 -0.00099300267 -0.00016966835 0.0016384242 0.0042785974 0.0071780724 0.010083207 0.013255437 0.016445285 0.019225042 0.020602595 0.020685203 0.019783171 0.018201327][-0.0011578937 -0.0011286151 -0.00092899631 -0.00030888722 0.0010541609 0.0030980906 0.0054494292 0.007924825 0.011053383 0.014450944 0.017321089 0.018599775 0.018651696 0.017589552 0.01593451][-0.0010987169 -0.0011141973 -0.0010062299 -0.00062948919 0.00029125728 0.0016969725 0.003504504 0.0057681627 0.0088544283 0.012313874 0.015247867 0.016595842 0.016598063 0.015409776 0.013535512][-0.0011708976 -0.0011269078 -0.0010627271 -0.0009228618 -0.00043801335 0.00042146689 0.0018021866 0.0038554636 0.0068697589 0.01033816 0.01313394 0.014429017 0.01432665 0.013134172 0.011133065][-0.0013024288 -0.0012201299 -0.001167662 -0.0010778802 -0.00084899087 -0.00041977898 0.00057480962 0.0023671826 0.0051329038 0.00826479 0.010720264 0.011824801 0.011612399 0.010444676 0.0085348058][-0.0015523732 -0.0014140918 -0.001318635 -0.0012346322 -0.0011301148 -0.00089487078 -0.00017998938 0.0013174241 0.0036951052 0.0063530155 0.0083250394 0.0091735423 0.0088669248 0.0076825772 0.0057951431][-0.0017050798 -0.0016582232 -0.0015470844 -0.0014356816 -0.0013167859 -0.0011290025 -0.0005751258 0.00068940653 0.0027003339 0.004869618 0.0063938941 0.0069724075 0.0065587875 0.0053376313 0.0035373666][-0.0017268567 -0.0017200647 -0.0017102626 -0.0016556254 -0.0015356927 -0.0013328189 -0.00080153754 0.00028468494 0.0019275878 0.0036298544 0.0047508981 0.0051757749 0.0047272672 0.0035242466 0.0018552599][-0.0017679576 -0.0017640148 -0.0017601344 -0.0017585161 -0.0017107606 -0.0015470441 -0.0010519046 -0.00014507095 0.0011120538 0.0023014806 0.0030373177 0.0032745805 0.0028501661 0.0018424432 0.00052009861][-0.0017678543 -0.0017817059 -0.0017896371 -0.0017930444 -0.0017814349 -0.0016995218 -0.001362975 -0.00074590347 8.2237762e-05 0.00078104867 0.0011832785 0.001298841 0.0010185157 0.00032187824 -0.00054429949]]...]
INFO - root - 2017-12-09 19:25:15.222033: step 51310, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.882 sec/batch; 68h:55m:04s remains)
INFO - root - 2017-12-09 19:25:23.979915: step 51320, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.859 sec/batch; 67h:05m:52s remains)
INFO - root - 2017-12-09 19:25:32.670271: step 51330, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.872 sec/batch; 68h:06m:53s remains)
INFO - root - 2017-12-09 19:25:41.463641: step 51340, loss = 0.83, batch loss = 0.70 (8.9 examples/sec; 0.900 sec/batch; 70h:19m:31s remains)
INFO - root - 2017-12-09 19:25:49.879059: step 51350, loss = 0.82, batch loss = 0.69 (10.0 examples/sec; 0.796 sec/batch; 62h:11m:16s remains)
INFO - root - 2017-12-09 19:25:58.546307: step 51360, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.893 sec/batch; 69h:44m:51s remains)
INFO - root - 2017-12-09 19:26:07.240397: step 51370, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.854 sec/batch; 66h:43m:27s remains)
INFO - root - 2017-12-09 19:26:15.932306: step 51380, loss = 0.81, batch loss = 0.68 (9.2 examples/sec; 0.868 sec/batch; 67h:46m:33s remains)
INFO - root - 2017-12-09 19:26:24.654319: step 51390, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.849 sec/batch; 66h:18m:24s remains)
INFO - root - 2017-12-09 19:26:33.352572: step 51400, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.843 sec/batch; 65h:51m:38s remains)
2017-12-09 19:26:34.192509: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0018274703 -0.0018266252 -0.001826653 -0.0018266471 -0.0018266373 -0.0018266252 -0.0018265609 -0.0018264492 -0.0018263129 -0.0018261665 -0.0018259845 -0.0018258558 -0.0018257427 -0.0018256885 -0.0018256707][-0.0018269019 -0.0018262949 -0.0018264096 -0.001826467 -0.0018265061 -0.0018265373 -0.0018264836 -0.0018263398 -0.0018261444 -0.0018259423 -0.0018257217 -0.0018255293 -0.0018253229 -0.0018251663 -0.0018250644][-0.0018272486 -0.0018268072 -0.0018270597 -0.0018272325 -0.001827337 -0.001827436 -0.0018274274 -0.0018273426 -0.0018271697 -0.0018268698 -0.0018265301 -0.0018261588 -0.0018257378 -0.0018254181 -0.0018251807][-0.0018278531 -0.0018276214 -0.0018280516 -0.0018283364 -0.0018285596 -0.0018287807 -0.0018288501 -0.0018288075 -0.0018286274 -0.0018282462 -0.0018277529 -0.0018271618 -0.0018265074 -0.0018260027 -0.0018255948][-0.0018287051 -0.0018287512 -0.0018292272 -0.0018295094 -0.0018299033 -0.0018303419 -0.0018304755 -0.0018304553 -0.001830171 -0.0018296103 -0.0018290223 -0.0018282612 -0.0018274819 -0.0018267827 -0.0018261112][-0.0018293799 -0.0018297866 -0.0018304554 -0.0018309182 -0.0018317074 -0.0018323803 -0.0018324873 -0.0018322687 -0.0018317609 -0.0018309341 -0.0018302085 -0.0018292555 -0.001828332 -0.0018274575 -0.0018265639][-0.0018296584 -0.001830329 -0.0018312642 -0.0018321273 -0.0018332558 -0.001834176 -0.0018342742 -0.0018337875 -0.0018328934 -0.0018318027 -0.0018309953 -0.0018300253 -0.0018289845 -0.0018280259 -0.001827003][-0.0018292853 -0.001829945 -0.0018310042 -0.0018321066 -0.0018333928 -0.0018345371 -0.0018346787 -0.001834051 -0.0018331015 -0.0018322261 -0.001831749 -0.0018309341 -0.0018297406 -0.001828558 -0.0018273252][-0.0018286244 -0.0018292458 -0.0018302869 -0.0018315195 -0.0018328294 -0.0018339456 -0.0018341027 -0.0018335463 -0.0018327358 -0.0018321916 -0.0018320406 -0.0018313227 -0.0018300893 -0.0018287554 -0.0018273531][-0.0018282119 -0.0018288202 -0.0018297815 -0.0018308703 -0.0018320346 -0.0018330084 -0.001833125 -0.0018327467 -0.0018322285 -0.0018319737 -0.0018317568 -0.001831064 -0.001829938 -0.0018285856 -0.0018271593][-0.0018277512 -0.0018281738 -0.0018289526 -0.0018298111 -0.001830773 -0.0018315951 -0.0018318398 -0.0018317822 -0.001831661 -0.0018315837 -0.0018312806 -0.0018305669 -0.0018294898 -0.0018282407 -0.0018269611][-0.0018272067 -0.0018274107 -0.0018279462 -0.0018285397 -0.0018292045 -0.0018297945 -0.0018301075 -0.0018303054 -0.0018305233 -0.0018306178 -0.00183028 -0.0018295435 -0.0018285504 -0.0018274767 -0.0018264789][-0.0018266793 -0.001826638 -0.0018269601 -0.0018272676 -0.0018275959 -0.0018279362 -0.0018282124 -0.0018284788 -0.0018287685 -0.0018289007 -0.0018286933 -0.0018281899 -0.0018274368 -0.0018266316 -0.0018259439][-0.0018262808 -0.0018260238 -0.0018261644 -0.0018263031 -0.0018264489 -0.0018266339 -0.0018268147 -0.0018270029 -0.0018272041 -0.0018273464 -0.0018272669 -0.0018269565 -0.0018264448 -0.0018259111 -0.0018254841][-0.0018260676 -0.0018256468 -0.0018256461 -0.0018256997 -0.0018257619 -0.0018258651 -0.001825957 -0.0018260425 -0.0018261462 -0.0018262131 -0.0018261515 -0.0018259505 -0.0018256656 -0.0018253881 -0.0018251663]]...]
INFO - root - 2017-12-09 19:26:42.608463: step 51410, loss = 0.82, batch loss = 0.69 (9.7 examples/sec; 0.827 sec/batch; 64h:33m:17s remains)
INFO - root - 2017-12-09 19:26:51.222847: step 51420, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.865 sec/batch; 67h:31m:54s remains)
INFO - root - 2017-12-09 19:26:59.854677: step 51430, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.851 sec/batch; 66h:28m:21s remains)
INFO - root - 2017-12-09 19:27:08.428440: step 51440, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 68h:02m:08s remains)
INFO - root - 2017-12-09 19:27:16.709418: step 51450, loss = 0.82, batch loss = 0.69 (11.4 examples/sec; 0.704 sec/batch; 54h:56m:19s remains)
INFO - root - 2017-12-09 19:27:25.370427: step 51460, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 67h:48m:30s remains)
INFO - root - 2017-12-09 19:27:33.921286: step 51470, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.844 sec/batch; 65h:53m:41s remains)
INFO - root - 2017-12-09 19:27:42.458227: step 51480, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.889 sec/batch; 69h:21m:43s remains)
INFO - root - 2017-12-09 19:27:51.081513: step 51490, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.872 sec/batch; 68h:05m:40s remains)
INFO - root - 2017-12-09 19:27:59.781400: step 51500, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.902 sec/batch; 70h:22m:41s remains)
2017-12-09 19:28:00.661182: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.013978572 0.015027343 0.015962206 0.016620424 0.016612897 0.015908839 0.014819277 0.013475212 0.012239603 0.010924184 0.0097694043 0.00867698 0.0074983132 0.0064240382 0.0053029587][0.018954493 0.020593986 0.022029435 0.02291231 0.022873919 0.021892091 0.02046876 0.018853571 0.017411264 0.016019437 0.014767871 0.0136071 0.012262142 0.0109362 0.0093479846][0.025844973 0.028225405 0.030332588 0.031530019 0.0315599 0.030279923 0.028503729 0.026589172 0.024856841 0.02345204 0.02213815 0.020909864 0.01911856 0.017173681 0.014894123][0.034881081 0.038107812 0.040930506 0.042539064 0.042602811 0.041044604 0.0389659 0.03681637 0.034821529 0.033187333 0.031596277 0.030031282 0.027527552 0.024698282 0.021460429][0.046105012 0.050004244 0.053345285 0.055141956 0.055079408 0.053178743 0.050732121 0.048290372 0.045957748 0.04393661 0.041935064 0.039960694 0.036901474 0.0332143 0.029166467][0.059193853 0.063463464 0.066786334 0.068372123 0.067839652 0.065277472 0.062075913 0.059001293 0.056146014 0.053721689 0.051414989 0.049068537 0.045740642 0.041658025 0.037359193][0.072420478 0.076834381 0.079565778 0.080411352 0.078994021 0.075559817 0.0713924 0.067447364 0.064097382 0.061334625 0.059043311 0.056815464 0.0537531 0.049900416 0.045437258][0.083656795 0.087847017 0.089555718 0.089362979 0.086878881 0.0825586 0.0774809 0.072709173 0.06887307 0.065972373 0.063936286 0.062006563 0.059512578 0.056163348 0.051935315][0.091106124 0.094966955 0.095447242 0.09421923 0.090912268 0.085981652 0.0802775 0.074820638 0.070663281 0.067780249 0.066057771 0.064544074 0.062636659 0.059983112 0.056044482][0.093898438 0.097432211 0.096689276 0.094443791 0.090449862 0.085030317 0.078924887 0.073141381 0.068902262 0.066358984 0.065083608 0.064175345 0.062999293 0.061086252 0.05765656][0.092694521 0.095976017 0.094119541 0.091060355 0.086526975 0.080703251 0.074309967 0.068355419 0.064159684 0.061972141 0.061253116 0.061069719 0.060752489 0.059784107 0.057441846][0.089361437 0.09227214 0.089327537 0.08533778 0.080145635 0.073898457 0.067354627 0.061557233 0.05773415 0.056158181 0.056214571 0.0571507 0.058173556 0.058548581 0.057669766][0.085860051 0.088208675 0.084266029 0.079391435 0.073501311 0.066876531 0.060324427 0.054762371 0.051363286 0.050488912 0.051419154 0.053545948 0.055941343 0.057778005 0.058664881][0.082424305 0.084056392 0.079353958 0.073784873 0.067395605 0.060678035 0.054386925 0.049396191 0.046769686 0.04678512 0.048767511 0.052087359 0.055760864 0.058777604 0.061026357][0.079562373 0.080533192 0.075469121 0.0696201 0.063135609 0.056739833 0.051067233 0.046931382 0.045132395 0.045849293 0.04847331 0.052326165 0.056504428 0.06005032 0.063155241]]...]
INFO - root - 2017-12-09 19:28:09.142801: step 51510, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.894 sec/batch; 69h:48m:10s remains)
INFO - root - 2017-12-09 19:28:17.864686: step 51520, loss = 0.82, batch loss = 0.69 (9.8 examples/sec; 0.816 sec/batch; 63h:41m:39s remains)
INFO - root - 2017-12-09 19:28:26.512715: step 51530, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 67h:28m:06s remains)
INFO - root - 2017-12-09 19:28:35.109008: step 51540, loss = 0.81, batch loss = 0.68 (9.0 examples/sec; 0.888 sec/batch; 69h:16m:08s remains)
INFO - root - 2017-12-09 19:28:43.678927: step 51550, loss = 0.81, batch loss = 0.68 (9.3 examples/sec; 0.857 sec/batch; 66h:53m:09s remains)
INFO - root - 2017-12-09 19:28:52.193719: step 51560, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.881 sec/batch; 68h:44m:09s remains)
INFO - root - 2017-12-09 19:29:00.949827: step 51570, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.886 sec/batch; 69h:08m:31s remains)
INFO - root - 2017-12-09 19:29:09.672064: step 51580, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.877 sec/batch; 68h:25m:30s remains)
INFO - root - 2017-12-09 19:29:18.395032: step 51590, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.884 sec/batch; 69h:00m:01s remains)
INFO - root - 2017-12-09 19:29:27.073037: step 51600, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.860 sec/batch; 67h:05m:39s remains)
2017-12-09 19:29:27.943710: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0018239997 -0.0018243126 -0.001824048 -0.0018195317 -0.0018064958 -0.0017646336 -0.0016825756 -0.0015778106 -0.0015104669 -0.0015283606 -0.0016173432 -0.0017201209 -0.0017885406 -0.0018169276 -0.0018232563][-0.0018239371 -0.0018245305 -0.0018254855 -0.0018191444 -0.0017850669 -0.0016777434 -0.0014818995 -0.0012501555 -0.0011124846 -0.001160898 -0.0013575994 -0.0015827147 -0.001737058 -0.0018049423 -0.0018223226][-0.0018239835 -0.0018248128 -0.001823017 -0.0017923932 -0.0016789967 -0.0014080624 -0.00099185389 -0.00056651328 -0.00036591862 -0.0005133762 -0.00090978091 -0.0013402236 -0.0016381957 -0.0017770879 -0.0018178311][-0.0018232221 -0.0018250658 -0.0018148976 -0.0017249277 -0.001432527 -0.00082302711 7.6879514e-06 0.000748997 0.0010102381 0.00065298646 -0.00010681362 -0.00089550437 -0.0014446807 -0.001713622 -0.0018029313][-0.0017735003 -0.0017959257 -0.001785894 -0.0016022661 -0.0010133298 0.00014307525 0.0016330971 0.0028706584 0.0032328661 0.0025586579 0.0012332975 -0.00012962881 -0.0010960206 -0.0015914466 -0.00177072][-0.0015992321 -0.0016813528 -0.0017064741 -0.0014325933 -0.00049410481 0.0013244984 0.0036341865 0.0055190539 0.00606324 0.0050421339 0.0030283895 0.00092873129 -0.00059806078 -0.0014124 -0.00172376][-0.0012694376 -0.0014490864 -0.0015602655 -0.0012587052 -6.1048893e-05 0.0022983346 0.00533114 0.0078563383 0.0086695049 0.007416612 0.0047993609 0.0020017936 -8.1663136e-05 -0.0012234133 -0.0016746892][-0.00088338589 -0.0011615728 -0.0013806351 -0.0011391342 9.1864611e-05 0.0026069283 0.0059189093 0.0087655066 0.0097963987 0.0085261157 0.005670873 0.0025484813 0.00018517755 -0.0011260253 -0.0016507235][-0.00051610521 -0.00087548408 -0.0012075172 -0.0011116476 -0.00010669965 0.0020808345 0.0050573694 0.0077035646 0.0087601636 0.0076948046 0.0051098014 0.0022300058 3.3980119e-05 -0.0011835692 -0.0016673529][-0.00010679243 -0.00053232547 -0.0009894995 -0.0011127896 -0.00052287744 0.00099017879 0.0031636776 0.0051667676 0.006022139 0.0052752537 0.0033480059 0.0011801735 -0.00046691205 -0.001366643 -0.0017148571][0.00061942416 0.00012003153 -0.00053396553 -0.00097497716 -0.00089797424 -0.0001635697 0.0010683603 0.00227365 0.0028194496 0.0023918021 0.0012315029 -7.7016768e-05 -0.0010571405 -0.0015759727 -0.0017672491][0.0021819021 0.0015489635 0.00051698915 -0.00043455174 -0.00095901772 -0.00094327005 -0.00050915859 2.1750689e-05 0.00028596644 9.777362e-05 -0.00044008018 -0.0010477316 -0.0014947812 -0.0017221726 -0.0018005612][0.0050521335 0.004233731 0.0025771288 0.00080811826 -0.00050647371 -0.0011731968 -0.0013213051 -0.0012279801 -0.0011473875 -0.0012034355 -0.0013771881 -0.0015752581 -0.0017195093 -0.0017903398 -0.0018133994][0.0094408933 0.0083926907 0.0058611813 0.0029339627 0.00055498036 -0.00088154641 -0.0015082133 -0.0016778359 -0.0016921062 -0.0016988006 -0.0017294633 -0.0017683182 -0.0017978938 -0.0018121538 -0.0018164268][0.015023869 0.013721492 0.010155155 0.0058246646 0.0021487046 -0.00019702676 -0.00132286 -0.0017109702 -0.0017941773 -0.0018012669 -0.0018025725 -0.0018069063 -0.001812331 -0.0018150778 -0.001815995]]...]
INFO - root - 2017-12-09 19:29:36.425369: step 51610, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.873 sec/batch; 68h:09m:12s remains)
INFO - root - 2017-12-09 19:29:45.219729: step 51620, loss = 0.82, batch loss = 0.69 (9.7 examples/sec; 0.827 sec/batch; 64h:33m:36s remains)
INFO - root - 2017-12-09 19:29:53.950866: step 51630, loss = 0.82, batch loss = 0.69 (8.4 examples/sec; 0.951 sec/batch; 74h:11m:18s remains)
INFO - root - 2017-12-09 19:30:02.721604: step 51640, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.891 sec/batch; 69h:29m:13s remains)
INFO - root - 2017-12-09 19:30:11.179945: step 51650, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 67h:48m:45s remains)
INFO - root - 2017-12-09 19:30:19.686921: step 51660, loss = 0.83, batch loss = 0.70 (9.7 examples/sec; 0.823 sec/batch; 64h:14m:07s remains)
INFO - root - 2017-12-09 19:30:28.463176: step 51670, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.870 sec/batch; 67h:51m:23s remains)
INFO - root - 2017-12-09 19:30:37.240267: step 51680, loss = 0.83, batch loss = 0.70 (9.0 examples/sec; 0.888 sec/batch; 69h:16m:13s remains)
INFO - root - 2017-12-09 19:30:45.952273: step 51690, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 67h:56m:44s remains)
INFO - root - 2017-12-09 19:30:54.694147: step 51700, loss = 0.83, batch loss = 0.70 (9.3 examples/sec; 0.859 sec/batch; 67h:00m:04s remains)
2017-12-09 19:30:55.560142: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.00095811009 -0.00093613612 -0.000943864 -0.00095856335 -0.0009734293 -0.00097745925 -0.00096276432 -0.00093738531 -0.00091312645 -0.00091054046 -0.00095188024 -0.0010532178 -0.0012073976 -0.0013857234 -0.001556161][-0.00088365865 -0.00085131358 -0.00086051039 -0.0008818201 -0.00090982224 -0.00093150709 -0.00093503081 -0.00091629766 -0.00088403688 -0.00086287991 -0.00088568265 -0.000976924 -0.0011342776 -0.0013290927 -0.001519167][-0.000990512 -0.00095440343 -0.00096432469 -0.00099398685 -0.0010355806 -0.0010730531 -0.0010915403 -0.0010839445 -0.0010558034 -0.0010286474 -0.0010340877 -0.0011006757 -0.0012306315 -0.0013980763 -0.001562832][-0.0011695264 -0.0011414199 -0.0011494077 -0.0011751256 -0.0012129062 -0.0012489662 -0.0012706602 -0.0012700586 -0.0012495718 -0.0012255465 -0.0012247497 -0.0012719221 -0.0013713415 -0.0015018317 -0.0016297441][-0.0013806539 -0.0013700138 -0.0013865835 -0.0014143459 -0.0014444302 -0.0014656088 -0.0014723607 -0.0014624185 -0.0014397879 -0.0014171622 -0.0014144485 -0.0014483954 -0.0015198694 -0.0016120893 -0.0017000523][-0.0015735326 -0.0015729782 -0.0015902951 -0.0016160164 -0.00164098 -0.0016550627 -0.0016545991 -0.0016398471 -0.0016161039 -0.0015932472 -0.001585721 -0.0016044818 -0.0016482283 -0.0017037173 -0.0017551245][-0.0017223557 -0.001725592 -0.0017381366 -0.0017549302 -0.0017698889 -0.0017773043 -0.0017752977 -0.0017641033 -0.0017469692 -0.0017299124 -0.0017213152 -0.0017268689 -0.0017457487 -0.0017706462 -0.0017936041][-0.0017964437 -0.0017972729 -0.0018020296 -0.0018081643 -0.0018138062 -0.0018170998 -0.0018174005 -0.0018143969 -0.001808748 -0.0018020807 -0.0017972273 -0.001796891 -0.0018004122 -0.0018063235 -0.0018127904][-0.0018186875 -0.0018176077 -0.0018176999 -0.0018185576 -0.001819586 -0.0018200201 -0.0018198772 -0.0018194278 -0.0018188646 -0.0018183573 -0.0018179042 -0.0018173556 -0.0018166254 -0.0018166126 -0.0018173257][-0.001818092 -0.0018174108 -0.0018174753 -0.001818219 -0.001819363 -0.0018198224 -0.0018196678 -0.0018194321 -0.0018191537 -0.0018188535 -0.001818401 -0.0018173342 -0.0018159561 -0.0018154606 -0.001815823][-0.0018180622 -0.001817256 -0.0018174861 -0.0018180988 -0.0018189859 -0.001819315 -0.0018191038 -0.0018189158 -0.0018188361 -0.0018185838 -0.0018180294 -0.0018171477 -0.0018162733 -0.0018161433 -0.0018165152][-0.0018185204 -0.0018178098 -0.0018180738 -0.0018185312 -0.0018191587 -0.0018192157 -0.0018187953 -0.0018184696 -0.0018184183 -0.0018182836 -0.0018178469 -0.0018170949 -0.0018168241 -0.0018171335 -0.001817523][-0.0018181435 -0.0018175947 -0.0018178192 -0.0018181606 -0.0018185804 -0.001818931 -0.0018188292 -0.0018187171 -0.0018188221 -0.0018189846 -0.0018186411 -0.0018178013 -0.0018174486 -0.0018176341 -0.0018178161][-0.0018185321 -0.0018180293 -0.001817878 -0.0018179312 -0.0018182076 -0.0018183186 -0.0018180001 -0.0018179067 -0.0018178521 -0.0018177925 -0.0018175149 -0.0018169221 -0.0018167425 -0.0018171759 -0.0018175446][-0.0018192467 -0.0018187301 -0.0018184527 -0.00181825 -0.001818244 -0.0018182949 -0.0018182141 -0.0018184404 -0.0018185322 -0.001818179 -0.0018178674 -0.0018176411 -0.0018176582 -0.0018179732 -0.0018185213]]...]
INFO - root - 2017-12-09 19:31:04.189285: step 51710, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.846 sec/batch; 66h:01m:07s remains)
INFO - root - 2017-12-09 19:31:12.892143: step 51720, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 67h:39m:53s remains)
INFO - root - 2017-12-09 19:31:21.659814: step 51730, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.902 sec/batch; 70h:21m:59s remains)
INFO - root - 2017-12-09 19:31:30.370393: step 51740, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.872 sec/batch; 67h:58m:51s remains)
INFO - root - 2017-12-09 19:31:38.918582: step 51750, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.878 sec/batch; 68h:27m:11s remains)
INFO - root - 2017-12-09 19:31:47.690451: step 51760, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.880 sec/batch; 68h:35m:41s remains)
INFO - root - 2017-12-09 19:31:56.220492: step 51770, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.863 sec/batch; 67h:18m:52s remains)
INFO - root - 2017-12-09 19:32:04.876259: step 51780, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.870 sec/batch; 67h:48m:08s remains)
INFO - root - 2017-12-09 19:32:13.678381: step 51790, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.860 sec/batch; 67h:04m:15s remains)
INFO - root - 2017-12-09 19:32:22.235723: step 51800, loss = 0.82, batch loss = 0.69 (9.8 examples/sec; 0.816 sec/batch; 63h:38m:14s remains)
2017-12-09 19:32:23.164249: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.18917608 0.19303298 0.19583271 0.19804171 0.19998482 0.203199 0.20649743 0.2102578 0.21393171 0.21649666 0.21841182 0.21921255 0.21916305 0.21797407 0.21618019][0.19731791 0.20383011 0.20919481 0.21396798 0.21807589 0.2224151 0.22609478 0.2293501 0.23193549 0.232302 0.23200801 0.23097517 0.22939809 0.22741307 0.22478449][0.20340656 0.21277104 0.22081093 0.22814165 0.23462847 0.24032021 0.24482961 0.24763632 0.24913093 0.24798219 0.24559081 0.24222732 0.2386871 0.23525178 0.23148036][0.20831361 0.22090805 0.2320634 0.24238393 0.25149295 0.25896394 0.26402447 0.26671529 0.26720762 0.26469865 0.26021951 0.25437507 0.24858445 0.24326602 0.23777522][0.21352062 0.22946715 0.24310231 0.25639543 0.26790896 0.27697098 0.28264931 0.28477639 0.28422311 0.27996859 0.27296773 0.26449555 0.25629136 0.24902153 0.24212351][0.21928713 0.2377228 0.25362033 0.26915354 0.28273869 0.29318297 0.29936075 0.30098325 0.29892489 0.29237911 0.28210607 0.27040604 0.2593464 0.24998474 0.24178068][0.226072 0.24661076 0.26392093 0.28076002 0.29509535 0.30558237 0.31136703 0.31169894 0.30776921 0.29879203 0.28525722 0.27061921 0.25696394 0.24591637 0.23687445][0.23153821 0.25457126 0.27351934 0.29101139 0.30490425 0.31407419 0.31753102 0.31545216 0.30882162 0.29707527 0.28069028 0.26349604 0.24814296 0.2359293 0.22640251][0.23387994 0.25887167 0.27915317 0.29759219 0.31127709 0.31899604 0.31970015 0.3139371 0.303452 0.28852606 0.26977053 0.25058082 0.23409471 0.22139798 0.21205595][0.23343386 0.25940341 0.28036329 0.29888877 0.311743 0.3177222 0.31580332 0.30633536 0.2919389 0.27368867 0.25248018 0.23206811 0.21500759 0.20252043 0.19411294][0.22846951 0.25451782 0.27555025 0.29387867 0.30556723 0.30966029 0.30509806 0.29261202 0.27511537 0.2543644 0.23203273 0.21103141 0.19429472 0.18252422 0.17498437][0.217851 0.24406523 0.26507235 0.28248432 0.29274517 0.29493716 0.28798145 0.2731882 0.25370073 0.2320758 0.21029675 0.19016872 0.17465647 0.16362426 0.1569619][0.20181862 0.2274503 0.24814615 0.26525769 0.2748923 0.27595228 0.26767632 0.25191036 0.23162135 0.21004508 0.18930922 0.17064662 0.15676451 0.14679484 0.14105891][0.18213758 0.20569618 0.22493984 0.24133708 0.25096005 0.25299776 0.24587078 0.23104286 0.21183261 0.19143896 0.17212863 0.15486293 0.14208528 0.13303678 0.12797914][0.16242912 0.18310484 0.20005824 0.21480174 0.22383857 0.22668907 0.22134551 0.2092323 0.19284566 0.17505996 0.15784185 0.14230479 0.13061219 0.12221726 0.11744135]]...]
INFO - root - 2017-12-09 19:32:31.631554: step 51810, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.853 sec/batch; 66h:32m:14s remains)
INFO - root - 2017-12-09 19:32:40.248525: step 51820, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.878 sec/batch; 68h:26m:41s remains)
INFO - root - 2017-12-09 19:32:48.876071: step 51830, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.830 sec/batch; 64h:41m:56s remains)
INFO - root - 2017-12-09 19:32:57.314436: step 51840, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.830 sec/batch; 64h:44m:06s remains)
INFO - root - 2017-12-09 19:33:06.069867: step 51850, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.847 sec/batch; 66h:04m:02s remains)
INFO - root - 2017-12-09 19:33:14.613480: step 51860, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 67h:12m:46s remains)
INFO - root - 2017-12-09 19:33:23.307710: step 51870, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.857 sec/batch; 66h:46m:39s remains)
INFO - root - 2017-12-09 19:33:31.963513: step 51880, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.834 sec/batch; 65h:00m:25s remains)
INFO - root - 2017-12-09 19:33:40.611854: step 51890, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.882 sec/batch; 68h:43m:35s remains)
INFO - root - 2017-12-09 19:33:49.326022: step 51900, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.886 sec/batch; 69h:05m:01s remains)
2017-12-09 19:33:50.249673: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.0051098685 0.0073282919 0.010578371 0.014900575 0.019730749 0.024000753 0.026423322 0.026046643 0.022693077 0.017269079 0.011223065 0.0059396341 0.0021079993 -0.00019666355 -0.001310561][0.0082191853 0.010909342 0.014432174 0.018928766 0.023968838 0.02855413 0.031340905 0.03118854 0.027756596 0.021825537 0.014903068 0.0085720811 0.0037492379 0.00067219872 -0.00093388773][0.010663679 0.013741991 0.017335178 0.021575361 0.026087541 0.030072045 0.032461207 0.032165073 0.028759176 0.022901906 0.015986674 0.0095419288 0.0044902661 0.0011559614 -0.00067171594][0.010903337 0.014150784 0.017714147 0.021597344 0.025493238 0.028821567 0.03076978 0.030314039 0.027039357 0.021554373 0.015089043 0.0090395492 0.0042707259 0.001106717 -0.00065268774][0.0082912138 0.011301079 0.014656057 0.018121693 0.021482561 0.024259146 0.025833154 0.025342986 0.022417711 0.017646402 0.01210084 0.0069931555 0.0030156276 0.00041861844 -0.00097393442][0.0044166096 0.0066481116 0.0093548428 0.012181815 0.014994523 0.017396826 0.01885578 0.018583521 0.01629564 0.012568168 0.0082928948 0.0044259103 0.0014740661 -0.00039577554 -0.0013381564][0.00079747674 0.002023506 0.0036934172 0.0056122076 0.0077253697 0.0097112507 0.011072539 0.011158588 0.0097266044 0.0072461269 0.0044061849 0.0018759578 -1.0208576e-05 -0.0011399661 -0.0016433285][-0.0011838283 -0.0007439442 -3.4237397e-05 0.00088359707 0.0020592576 0.00334625 0.00436386 0.0046352646 0.0039944053 0.00274933 0.0012858481 -3.8141967e-05 -0.0010166951 -0.0015660452 -0.0017754927][-0.0017658451 -0.0016753408 -0.0014976505 -0.0012187551 -0.00079107925 -0.00024427916 0.0002447929 0.00044262491 0.00026601122 -0.00016070821 -0.000688354 -0.0011868516 -0.00155738 -0.0017533081 -0.0018173639][-0.0018336728 -0.0018316342 -0.0018267208 -0.0018089081 -0.0017559044 -0.0016632293 -0.0015727881 -0.0015331488 -0.0015519426 -0.0016062626 -0.0016789231 -0.0017519747 -0.0018020144 -0.0018224379 -0.0018259578][-0.0018349276 -0.0018340525 -0.0018326648 -0.0018302732 -0.0018239992 -0.0018133362 -0.0018018659 -0.0017963034 -0.0017973895 -0.0018024683 -0.001809918 -0.0018176311 -0.0018230864 -0.0018249627 -0.0018251939][-0.0018329821 -0.0018322968 -0.0018322106 -0.0018312229 -0.0018288919 -0.0018247578 -0.0018211122 -0.0018187293 -0.0018182768 -0.0018195469 -0.0018218749 -0.0018247702 -0.0018269081 -0.0018280739 -0.0018278696][-0.0018319717 -0.0018313221 -0.0018311436 -0.0018306242 -0.0018294224 -0.0018269516 -0.0018242195 -0.0018216184 -0.0018204607 -0.0018204944 -0.0018218785 -0.0018243378 -0.0018262255 -0.001827782 -0.0018282143][-0.0018300725 -0.0018297759 -0.0018303846 -0.0018306605 -0.0018305779 -0.001829874 -0.0018286777 -0.0018271107 -0.0018262267 -0.0018256732 -0.0018256644 -0.001826518 -0.0018272923 -0.0018278633 -0.0018281644][-0.0018287711 -0.0018283755 -0.0018288082 -0.0018294412 -0.0018299811 -0.0018301129 -0.0018300086 -0.0018294754 -0.0018288167 -0.0018283649 -0.0018281608 -0.0018282755 -0.0018279614 -0.0018277506 -0.0018277261]]...]
INFO - root - 2017-12-09 19:33:58.658587: step 51910, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.855 sec/batch; 66h:39m:45s remains)
INFO - root - 2017-12-09 19:34:07.212876: step 51920, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 67h:19m:34s remains)
INFO - root - 2017-12-09 19:34:15.694241: step 51930, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.837 sec/batch; 65h:15m:11s remains)
INFO - root - 2017-12-09 19:34:24.404373: step 51940, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.851 sec/batch; 66h:18m:25s remains)
INFO - root - 2017-12-09 19:34:32.955070: step 51950, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.853 sec/batch; 66h:27m:15s remains)
INFO - root - 2017-12-09 19:34:41.365390: step 51960, loss = 0.81, batch loss = 0.68 (9.4 examples/sec; 0.853 sec/batch; 66h:28m:56s remains)
INFO - root - 2017-12-09 19:34:50.096185: step 51970, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.851 sec/batch; 66h:17m:27s remains)
INFO - root - 2017-12-09 19:34:58.881688: step 51980, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.855 sec/batch; 66h:38m:18s remains)
INFO - root - 2017-12-09 19:35:07.593693: step 51990, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.883 sec/batch; 68h:49m:00s remains)
INFO - root - 2017-12-09 19:35:16.266965: step 52000, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.880 sec/batch; 68h:33m:08s remains)
2017-12-09 19:35:17.099334: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.24694973 0.24358672 0.2398186 0.23501666 0.22904174 0.2220342 0.21532173 0.2094781 0.20396031 0.197546 0.19162208 0.18671632 0.18236226 0.17747056 0.1729188][0.25721839 0.25625867 0.253866 0.24956715 0.2430042 0.23490338 0.22677982 0.21929049 0.21297544 0.20658815 0.20175692 0.19808644 0.19476344 0.19120824 0.18743712][0.26350114 0.26527259 0.26502547 0.26156539 0.25494796 0.24634272 0.23721492 0.22872153 0.22184072 0.21581261 0.21234274 0.2094934 0.20700869 0.2041861 0.20092681][0.26940262 0.27450523 0.2765362 0.27461869 0.26841244 0.25960809 0.24935371 0.23954378 0.231713 0.22557235 0.22270747 0.22085404 0.21909763 0.21697474 0.21449734][0.27351102 0.28140518 0.28524458 0.28477204 0.27940315 0.27046418 0.25977728 0.24930996 0.24058701 0.23418209 0.23121639 0.23002966 0.22934407 0.22796677 0.22640032][0.27528474 0.28564149 0.29060519 0.29096091 0.28585455 0.27710605 0.26702416 0.256164 0.2467505 0.24003571 0.23693986 0.23568077 0.23496623 0.23460335 0.23402987][0.27531624 0.28759575 0.29297903 0.29341078 0.28850076 0.28071603 0.27146807 0.26107451 0.25181684 0.24508519 0.24169709 0.23990978 0.23881659 0.23862247 0.23832205][0.27350613 0.28685257 0.29228234 0.29301891 0.28883058 0.28210935 0.2739138 0.26462495 0.25596562 0.24960664 0.24546899 0.24251485 0.24042821 0.23939548 0.23873837][0.27112997 0.28472441 0.28963017 0.29061556 0.28750062 0.28185982 0.27485046 0.26687965 0.25900131 0.25225079 0.24699515 0.24309811 0.23986585 0.23762023 0.23593731][0.26801676 0.2801851 0.28379077 0.2844972 0.28225246 0.27824923 0.27271062 0.26605925 0.2588093 0.25212169 0.2460638 0.24071071 0.23648933 0.23332845 0.23074397][0.26570943 0.27617961 0.27813336 0.2780624 0.2760908 0.27295423 0.26857907 0.26278231 0.25600639 0.24925512 0.24264272 0.23612231 0.2304721 0.22631009 0.22291327][0.26216856 0.27104592 0.27139625 0.270467 0.26814541 0.26528788 0.26123634 0.25638375 0.24991655 0.24307068 0.23608044 0.22897786 0.22262304 0.21749213 0.21370646][0.25931805 0.26614496 0.26473111 0.26281929 0.25998229 0.25652736 0.25216717 0.24724554 0.24033456 0.2328952 0.22538778 0.21787384 0.21136335 0.20605323 0.20258127][0.25643843 0.26198196 0.2592991 0.25624734 0.25260991 0.2485078 0.24360758 0.23769546 0.22969985 0.22103702 0.2127395 0.20480327 0.19819497 0.19318862 0.19013369][0.25363767 0.25843492 0.25469524 0.25077087 0.24615541 0.24094701 0.23478688 0.22751659 0.2180832 0.20815884 0.19885357 0.1902274 0.18330438 0.1782252 0.17558675]]...]
INFO - root - 2017-12-09 19:35:25.623437: step 52010, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.844 sec/batch; 65h:45m:47s remains)
INFO - root - 2017-12-09 19:35:34.215961: step 52020, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.881 sec/batch; 68h:39m:46s remains)
INFO - root - 2017-12-09 19:35:42.810380: step 52030, loss = 0.83, batch loss = 0.70 (9.2 examples/sec; 0.867 sec/batch; 67h:30m:52s remains)
INFO - root - 2017-12-09 19:35:51.425510: step 52040, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.859 sec/batch; 66h:54m:37s remains)
INFO - root - 2017-12-09 19:36:00.033630: step 52050, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.887 sec/batch; 69h:07m:00s remains)
INFO - root - 2017-12-09 19:36:08.527467: step 52060, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.845 sec/batch; 65h:49m:18s remains)
INFO - root - 2017-12-09 19:36:17.186795: step 52070, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.831 sec/batch; 64h:43m:22s remains)
INFO - root - 2017-12-09 19:36:25.830618: step 52080, loss = 0.81, batch loss = 0.68 (9.5 examples/sec; 0.845 sec/batch; 65h:50m:40s remains)
INFO - root - 2017-12-09 19:36:34.464085: step 52090, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.883 sec/batch; 68h:45m:06s remains)
INFO - root - 2017-12-09 19:36:43.051995: step 52100, loss = 0.81, batch loss = 0.68 (9.5 examples/sec; 0.839 sec/batch; 65h:22m:50s remains)
2017-12-09 19:36:44.008527: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0018338989 -0.0018324779 -0.001831237 -0.0018298086 -0.0018286749 -0.0018276434 -0.0018265012 -0.0018256251 -0.0018249061 -0.0018248049 -0.0018248777 -0.0018252272 -0.0018251571 -0.0018240341 -0.001821678][-0.0018332951 -0.0018321954 -0.0018311358 -0.0018297182 -0.0018281138 -0.0018263638 -0.001824577 -0.00182316 -0.0018221509 -0.0018222962 -0.0018230078 -0.0018241898 -0.0018248704 -0.0018245089 -0.0018229336][-0.001830226 -0.0018291452 -0.0018283786 -0.0018271601 -0.0018252161 -0.0018229701 -0.0018203149 -0.0018177342 -0.0018157441 -0.0018154574 -0.0018166908 -0.0018193054 -0.001821546 -0.0018224072 -0.0018220292][-0.0018241027 -0.0018230228 -0.0018224341 -0.0018214862 -0.0018191159 -0.0018158896 -0.001811507 -0.0018066382 -0.0018022821 -0.0018007176 -0.0018034677 -0.001809686 -0.0018155609 -0.0018189958 -0.0018200855][-0.0018167726 -0.001815209 -0.0018147334 -0.0018139173 -0.001810639 -0.0018049795 -0.0017964621 -0.0017869205 -0.0017782762 -0.0017751893 -0.0017804605 -0.0017925929 -0.0018050949 -0.0018130862 -0.0018170342][-0.0018116191 -0.0018095799 -0.0018091986 -0.0018086432 -0.0018048127 -0.0017966278 -0.0017833486 -0.0017686896 -0.0017557773 -0.001751532 -0.0017591129 -0.0017760459 -0.001793375 -0.0018047534 -0.0018111067][-0.0018088511 -0.0018067909 -0.0018066774 -0.0018066835 -0.0018031551 -0.0017937885 -0.0017779045 -0.0017606698 -0.0017474656 -0.0017442668 -0.0017526395 -0.0017698052 -0.0017869142 -0.0017983562 -0.0018050683][-0.0018068545 -0.0018048129 -0.001805152 -0.0018058349 -0.0018039923 -0.0017966407 -0.0017833905 -0.001768847 -0.0017600844 -0.0017597878 -0.0017667178 -0.001778639 -0.0017896193 -0.0017968485 -0.0018016592][-0.0018042451 -0.0018021682 -0.0018025517 -0.0018035362 -0.0018031829 -0.0017995189 -0.0017927086 -0.001784982 -0.0017807405 -0.0017815997 -0.0017854896 -0.0017910935 -0.0017958675 -0.0017990193 -0.0018013032][-0.0018020896 -0.0017999411 -0.0018001634 -0.0018009297 -0.0018011988 -0.0018001918 -0.0017982569 -0.0017960174 -0.0017948452 -0.0017956441 -0.0017970166 -0.0017986662 -0.0017999188 -0.0018007092 -0.0018015616][-0.0018015578 -0.0017993385 -0.0017993164 -0.0017996628 -0.0017997762 -0.0017993554 -0.0017988327 -0.0017983341 -0.0017981266 -0.001798593 -0.0017992895 -0.0017999547 -0.0018004943 -0.0018010233 -0.0018014052][-0.0018019194 -0.0017998383 -0.0017996032 -0.0017997074 -0.0017996595 -0.0017994372 -0.0017992886 -0.0017991259 -0.0017989993 -0.001799138 -0.0017995242 -0.0018000602 -0.0018005251 -0.001800949 -0.0018012036][-0.001802511 -0.0018005928 -0.001800227 -0.0018002045 -0.0018000947 -0.0017999936 -0.0017999933 -0.0017999662 -0.0017998866 -0.001799923 -0.001800069 -0.0018003455 -0.0018006098 -0.0018008254 -0.0018009791][-0.0018033457 -0.00180146 -0.0018009722 -0.001800772 -0.001800682 -0.0018006468 -0.0018007229 -0.0018007108 -0.001800607 -0.0018005861 -0.0018006053 -0.0018006852 -0.0018007729 -0.0018008025 -0.0018008229][-0.001803903 -0.0018021049 -0.0018014284 -0.0018011092 -0.0018009973 -0.001801008 -0.0018010765 -0.0018010916 -0.0018010105 -0.0018009503 -0.0018009226 -0.0018009102 -0.0018008972 -0.0018008506 -0.0018007876]]...]
INFO - root - 2017-12-09 19:36:52.432138: step 52110, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.875 sec/batch; 68h:10m:53s remains)
INFO - root - 2017-12-09 19:37:01.197935: step 52120, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.877 sec/batch; 68h:16m:05s remains)
INFO - root - 2017-12-09 19:37:09.864479: step 52130, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.866 sec/batch; 67h:26m:44s remains)
INFO - root - 2017-12-09 19:37:18.485207: step 52140, loss = 0.81, batch loss = 0.68 (9.4 examples/sec; 0.853 sec/batch; 66h:26m:57s remains)
INFO - root - 2017-12-09 19:37:27.066156: step 52150, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.840 sec/batch; 65h:26m:24s remains)
INFO - root - 2017-12-09 19:37:35.517359: step 52160, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 67h:17m:04s remains)
INFO - root - 2017-12-09 19:37:44.202265: step 52170, loss = 0.82, batch loss = 0.69 (8.8 examples/sec; 0.908 sec/batch; 70h:43m:47s remains)
INFO - root - 2017-12-09 19:37:52.925022: step 52180, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 67h:06m:43s remains)
INFO - root - 2017-12-09 19:38:01.558954: step 52190, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.847 sec/batch; 65h:57m:22s remains)
INFO - root - 2017-12-09 19:38:10.292404: step 52200, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.899 sec/batch; 70h:00m:26s remains)
2017-12-09 19:38:11.151815: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0018069234 -0.001805558 -0.0018056469 -0.0018057389 -0.0018059657 -0.0018063851 -0.0018065375 -0.0018062409 -0.0018056515 -0.0018050893 -0.001804503 -0.0018038646 -0.0018032243 -0.0018028454 -0.0018027725][-0.0018066436 -0.0018052552 -0.0018055381 -0.0018059993 -0.0018065845 -0.0018074672 -0.0018080357 -0.0018080149 -0.0018076458 -0.0018071447 -0.001806239 -0.001804873 -0.0018034345 -0.0018025563 -0.0018022069][-0.0018060675 -0.0018048277 -0.0018052472 -0.0018060487 -0.0018070735 -0.0018085728 -0.0018097654 -0.0018103068 -0.0018105049 -0.001810342 -0.0018092577 -0.0018071122 -0.0018046864 -0.0018030049 -0.0018021595][-0.001805068 -0.0018040632 -0.0018046517 -0.0018058536 -0.0018073063 -0.0018092369 -0.0018109041 -0.0018120897 -0.0018130979 -0.0018134585 -0.0018124135 -0.0018096961 -0.0018063983 -0.0018038709 -0.0018023974][-0.001803992 -0.0018031794 -0.0018039907 -0.0018056089 -0.0018074433 -0.0018095167 -0.0018114935 -0.0018131549 -0.0018147534 -0.0018155188 -0.0018146279 -0.0018117444 -0.0018079621 -0.0018046716 -0.0018025042][-0.0018027269 -0.0018020535 -0.0018031962 -0.0018052921 -0.0018074054 -0.0018093676 -0.0018112259 -0.0018129578 -0.0018149072 -0.0018161398 -0.001815489 -0.0018127207 -0.001808853 -0.0018050665 -0.0018023235][-0.0018012463 -0.0018008418 -0.0018022184 -0.0018045517 -0.0018066964 -0.0018082826 -0.0018095474 -0.0018108133 -0.0018130456 -0.0018146728 -0.0018143572 -0.0018120331 -0.0018085507 -0.0018046976 -0.0018017319][-0.0017998931 -0.0017996182 -0.0018012205 -0.0018035126 -0.0018054547 -0.0018065537 -0.0018070237 -0.0018075057 -0.001809372 -0.001811115 -0.0018112306 -0.001809567 -0.0018068798 -0.0018035943 -0.0018009311][-0.0017990839 -0.0017987828 -0.0018004603 -0.00180254 -0.0018040882 -0.0018047582 -0.0018047652 -0.0018046177 -0.0018055147 -0.0018068912 -0.0018073134 -0.0018062588 -0.0018043644 -0.0018020426 -0.0018001422][-0.0017989267 -0.0017986048 -0.0018001967 -0.0018019452 -0.00180308 -0.0018033927 -0.0018031453 -0.0018026388 -0.0018027483 -0.0018035867 -0.0018039898 -0.0018032923 -0.0018019932 -0.001800585 -0.0017996765][-0.0017996975 -0.0017992485 -0.0018004664 -0.0018017881 -0.0018025887 -0.0018026897 -0.0018022369 -0.0018015373 -0.0018012756 -0.001801732 -0.0018020505 -0.0018015864 -0.0018008117 -0.0018001408 -0.0017998818][-0.0018009148 -0.0018001662 -0.0018009206 -0.001801788 -0.0018023074 -0.0018022894 -0.0018017571 -0.0018009833 -0.0018005011 -0.001800667 -0.0018008855 -0.0018006775 -0.0018004004 -0.0018002758 -0.0018003278][-0.0018021999 -0.0018010652 -0.0018013417 -0.0018018661 -0.0018021854 -0.0018021433 -0.0018017038 -0.0018010137 -0.0018004735 -0.0018004656 -0.0018005852 -0.0018005461 -0.0018005496 -0.0018006888 -0.0018009458][-0.0018032693 -0.0018018871 -0.0018017907 -0.0018020598 -0.0018022353 -0.0018022375 -0.0018019665 -0.0018014767 -0.0018010462 -0.0018009437 -0.0018009912 -0.0018010419 -0.0018011248 -0.0018013227 -0.0018015782][-0.0018040197 -0.0018025461 -0.0018022128 -0.0018023459 -0.0018024622 -0.0018024916 -0.0018023619 -0.0018020533 -0.0018017559 -0.0018016406 -0.0018016495 -0.001801716 -0.0018018127 -0.0018019773 -0.0018021377]]...]
INFO - root - 2017-12-09 19:38:19.731494: step 52210, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.865 sec/batch; 67h:19m:10s remains)
INFO - root - 2017-12-09 19:38:28.429147: step 52220, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.887 sec/batch; 69h:02m:55s remains)
INFO - root - 2017-12-09 19:38:37.114394: step 52230, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 67h:05m:43s remains)
INFO - root - 2017-12-09 19:38:45.951215: step 52240, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.861 sec/batch; 67h:02m:11s remains)
INFO - root - 2017-12-09 19:38:54.583918: step 52250, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.877 sec/batch; 68h:15m:36s remains)
INFO - root - 2017-12-09 19:39:03.049095: step 52260, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.852 sec/batch; 66h:20m:05s remains)
INFO - root - 2017-12-09 19:39:11.522238: step 52270, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.859 sec/batch; 66h:53m:55s remains)
INFO - root - 2017-12-09 19:39:19.985640: step 52280, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.848 sec/batch; 66h:02m:37s remains)
INFO - root - 2017-12-09 19:39:28.493162: step 52290, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 67h:30m:33s remains)
INFO - root - 2017-12-09 19:39:36.978626: step 52300, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.839 sec/batch; 65h:16m:21s remains)
2017-12-09 19:39:37.929228: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.001820967 -0.001820719 -0.0018208169 -0.0018208957 -0.0018209228 -0.0018209261 -0.0018209288 -0.0018209015 -0.0018208561 -0.0018208257 -0.001820814 -0.0018207848 -0.0018207129 -0.0018206332 -0.0018205564][-0.0018210093 -0.0018208524 -0.001821053 -0.0018212186 -0.0018212962 -0.0018213224 -0.0018213452 -0.0018213213 -0.0018212458 -0.0018211645 -0.0018210983 -0.0018210022 -0.0018208238 -0.0018206266 -0.0018204511][-0.0018216518 -0.00182165 -0.001821941 -0.0018221616 -0.0018222532 -0.0018223216 -0.0018224276 -0.0018224934 -0.0018224665 -0.0018223964 -0.0018222765 -0.0018220552 -0.0018216814 -0.0018212599 -0.0018208923][-0.0018225198 -0.0018227235 -0.0018231566 -0.0018234779 -0.0018236677 -0.001823908 -0.0018241841 -0.0018244233 -0.0018244834 -0.0018243685 -0.0018240617 -0.0018235511 -0.0018228464 -0.0018220716 -0.0018214227][-0.0018234412 -0.0018238926 -0.0018245258 -0.0018250674 -0.001825511 -0.0018260641 -0.0018266473 -0.0018271357 -0.0018272963 -0.0018270274 -0.0018263649 -0.0018253939 -0.0018242159 -0.0018229719 -0.0018219895][-0.0018242183 -0.0018249344 -0.0018257399 -0.0018265025 -0.0018272442 -0.0018281526 -0.0018290301 -0.001829726 -0.0018300301 -0.0018296184 -0.001828545 -0.0018270704 -0.0018254484 -0.0018237643 -0.0018224479][-0.0018248225 -0.0018257019 -0.0018265417 -0.001827408 -0.0018283364 -0.0018294908 -0.0018305433 -0.0018314571 -0.0018319506 -0.0018314465 -0.0018300528 -0.0018281826 -0.0018262372 -0.0018242436 -0.00182268][-0.0018249466 -0.0018258577 -0.0018267111 -0.0018275777 -0.0018286272 -0.001829978 -0.0018312967 -0.001832498 -0.0018331619 -0.0018327336 -0.0018311613 -0.0018289929 -0.0018267891 -0.0018245503 -0.0018228022][-0.0018247761 -0.001825637 -0.0018264946 -0.0018273884 -0.0018285479 -0.0018300775 -0.0018315923 -0.0018329279 -0.001833606 -0.0018331854 -0.0018315782 -0.001829232 -0.0018268265 -0.0018245286 -0.0018227583][-0.00182429 -0.0018250516 -0.0018258588 -0.0018267409 -0.0018279072 -0.0018294298 -0.001830932 -0.0018322052 -0.001832817 -0.0018323954 -0.0018309877 -0.0018288136 -0.0018265207 -0.0018243488 -0.0018226509][-0.0018234968 -0.0018240734 -0.0018247428 -0.0018254679 -0.0018264387 -0.0018277401 -0.001829031 -0.0018301944 -0.0018307416 -0.0018304563 -0.0018294083 -0.0018276876 -0.0018257666 -0.0018238738 -0.0018223783][-0.0018226106 -0.001822928 -0.0018234103 -0.0018239119 -0.0018245676 -0.0018255442 -0.0018266229 -0.0018276499 -0.0018281484 -0.00182807 -0.0018273994 -0.0018261675 -0.001824686 -0.0018231538 -0.0018219576][-0.0018217856 -0.0018219419 -0.0018222447 -0.0018225088 -0.0018228665 -0.0018235539 -0.0018243421 -0.0018251174 -0.0018255331 -0.0018256013 -0.0018252163 -0.0018243829 -0.0018233341 -0.0018222725 -0.0018214413][-0.0018211119 -0.0018211778 -0.0018213429 -0.0018214961 -0.0018217056 -0.0018221509 -0.0018226606 -0.0018231493 -0.0018234621 -0.0018235159 -0.0018232897 -0.0018227537 -0.0018220821 -0.00182142 -0.001820904][-0.0018205895 -0.001820548 -0.0018206284 -0.001820741 -0.0018208923 -0.0018211546 -0.0018214654 -0.0018217507 -0.0018219359 -0.0018219727 -0.0018218487 -0.0018215645 -0.0018211917 -0.0018208174 -0.0018205162]]...]
INFO - root - 2017-12-09 19:39:46.371632: step 52310, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.850 sec/batch; 66h:10m:54s remains)
INFO - root - 2017-12-09 19:39:55.022538: step 52320, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.858 sec/batch; 66h:48m:39s remains)
INFO - root - 2017-12-09 19:40:03.627030: step 52330, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.887 sec/batch; 69h:00m:40s remains)
INFO - root - 2017-12-09 19:40:12.282369: step 52340, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.861 sec/batch; 67h:00m:12s remains)
INFO - root - 2017-12-09 19:40:20.773445: step 52350, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.875 sec/batch; 68h:05m:08s remains)
INFO - root - 2017-12-09 19:40:29.377263: step 52360, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.883 sec/batch; 68h:43m:24s remains)
INFO - root - 2017-12-09 19:40:38.186842: step 52370, loss = 0.82, batch loss = 0.69 (8.5 examples/sec; 0.942 sec/batch; 73h:18m:55s remains)
INFO - root - 2017-12-09 19:40:46.972375: step 52380, loss = 0.83, batch loss = 0.70 (9.3 examples/sec; 0.863 sec/batch; 67h:07m:42s remains)
INFO - root - 2017-12-09 19:40:55.596961: step 52390, loss = 0.82, batch loss = 0.69 (9.7 examples/sec; 0.825 sec/batch; 64h:12m:38s remains)
INFO - root - 2017-12-09 19:41:04.320326: step 52400, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.903 sec/batch; 70h:14m:06s remains)
2017-12-09 19:41:05.216684: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0016301263 -0.0017207201 -0.0017681431 -0.0017323967 -0.001584932 -0.0012770307 -0.00080477574 -0.00019202125 0.00052195869 0.0013314338 0.0023208493 0.0036254986 0.0052821594 0.0070545417 0.0084186308][-0.0015000532 -0.001614496 -0.0016911128 -0.0016903022 -0.0015878282 -0.0013096395 -0.00079612853 2.6204623e-05 0.0012027699 0.0027413354 0.0046066474 0.0067217215 0.0089257015 0.010920389 0.012241608][-0.001436241 -0.0015161874 -0.0015659246 -0.00153981 -0.001393667 -0.0010155602 -0.00029799982 0.00088274 0.0025960612 0.0048274714 0.0074407128 0.01019273 0.012743638 0.014724427 0.015771693][-0.0014514568 -0.0014554155 -0.0014055565 -0.0012394472 -0.00087187078 -0.00016064185 0.0010149564 0.0027319198 0.00496564 0.0076271486 0.010552449 0.013488762 0.016041193 0.017776357 0.018402332][-0.0014533871 -0.0013838981 -0.0011755102 -0.0007351808 5.7288446e-05 0.0013349369 0.0031621377 0.0054701036 0.00807215 0.010789437 0.01352912 0.016159059 0.018351104 0.019653697 0.019798644][-0.0013510678 -0.0012122538 -0.00080710906 3.1569507e-06 0.0013380897 0.0032466105 0.0056737596 0.0083823288 0.011062155 0.013485502 0.015683969 0.017676713 0.0192723 0.020050047 0.019751664][-0.0011522535 -0.00092927244 -0.00031849626 0.00085532072 0.0026597134 0.0050163139 0.0077353413 0.010466967 0.01288381 0.014777956 0.016279921 0.017515825 0.018441953 0.018714117 0.018088991][-0.00098979927 -0.00067092851 9.2504313e-05 0.001484695 0.0035111872 0.0059717693 0.0085759778 0.010961933 0.012860319 0.014116961 0.014905467 0.015410081 0.015710637 0.015572884 0.01479507][-0.0009973282 -0.00062481884 0.00016220694 0.0015244299 0.0034289011 0.0056293551 0.0078074639 0.0096450727 0.010940083 0.011593965 0.011770983 0.011681884 0.011493333 0.01109209 0.010330607][-0.001195689 -0.00085424376 -0.00019647344 0.00089495128 0.0023854282 0.0040651374 0.0056637772 0.0069280784 0.0076966253 0.007897431 0.007665277 0.0072162733 0.0067577944 0.0062743197 0.0056684739][-0.0014697461 -0.0012321498 -0.00079623528 -9.2191622e-05 0.00085833634 0.0019265254 0.0029343143 0.0037001385 0.0040928703 0.0040533883 0.0036817067 0.0031625773 0.0026856665 0.0022860444 0.0019029645][-0.0016835761 -0.0015624317 -0.0013417176 -0.00098776026 -0.00050810864 3.9284234e-05 0.0005617995 0.00094974937 0.0011152664 0.0010164889 0.00071539532 0.00033539173 8.0158934e-06 -0.00023001432 -0.00041301711][-0.0017882508 -0.0017469272 -0.001666785 -0.0015348318 -0.0013527959 -0.0011399424 -0.0009322298 -0.00077906321 -0.00072449807 -0.00079072686 -0.00094715809 -0.0011328487 -0.0012842177 -0.0013835474 -0.0014463898][-0.0018161084 -0.001808427 -0.0017907921 -0.001758805 -0.0017118638 -0.0016540969 -0.0015957769 -0.0015520363 -0.0015388827 -0.0015635053 -0.0016149512 -0.0016721122 -0.0017151958 -0.0017414284 -0.0017548051][-0.0018184227 -0.0018180154 -0.0018166377 -0.0018133182 -0.0018073014 -0.0017987763 -0.0017891476 -0.0017812854 -0.0017786613 -0.0017820189 -0.0017890695 -0.0017956712 -0.001798286 -0.0017985982 -0.0017985898]]...]
INFO - root - 2017-12-09 19:41:13.755859: step 52410, loss = 0.81, batch loss = 0.68 (9.1 examples/sec; 0.878 sec/batch; 68h:20m:57s remains)
INFO - root - 2017-12-09 19:41:22.524372: step 52420, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.898 sec/batch; 69h:53m:28s remains)
INFO - root - 2017-12-09 19:41:31.336492: step 52430, loss = 0.82, batch loss = 0.69 (8.5 examples/sec; 0.939 sec/batch; 73h:05m:00s remains)
INFO - root - 2017-12-09 19:41:40.083643: step 52440, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.886 sec/batch; 68h:57m:27s remains)
INFO - root - 2017-12-09 19:41:48.836726: step 52450, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.872 sec/batch; 67h:51m:12s remains)
INFO - root - 2017-12-09 19:41:57.353841: step 52460, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.832 sec/batch; 64h:43m:54s remains)
INFO - root - 2017-12-09 19:42:06.173849: step 52470, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.881 sec/batch; 68h:32m:16s remains)
INFO - root - 2017-12-09 19:42:14.872763: step 52480, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 67h:32m:21s remains)
INFO - root - 2017-12-09 19:42:23.608940: step 52490, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.878 sec/batch; 68h:15m:54s remains)
INFO - root - 2017-12-09 19:42:32.235108: step 52500, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.851 sec/batch; 66h:12m:49s remains)
2017-12-09 19:42:33.071695: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0018254747 -0.0018253045 -0.0018176155 -0.0017620345 -0.001598867 -0.001288258 -0.00087503728 -0.0004094484 -1.3695913e-05 0.00021884881 0.00022255571 4.6599889e-06 -0.00033492525 -0.000698328 -0.0010141565][-0.0018259934 -0.0018256577 -0.0018139342 -0.00172827 -0.0014689474 -0.0009580896 -0.00025756645 0.00054480962 0.0012429984 0.0016958994 0.0017559455 0.0014506778 0.00091265573 0.0003210787 -0.00021091383][-0.0018243836 -0.0018234786 -0.001803917 -0.0016764917 -0.0012971252 -0.000547315 0.00052627304 0.0017716483 0.0028925044 0.0036651511 0.0038472144 0.0034816172 0.0027386993 0.0018833169 0.0010860889][-0.0018142165 -0.0018094006 -0.0017758977 -0.0016030315 -0.0010978619 -0.0001185002 0.0013207392 0.0029935786 0.0045379582 0.0056302552 0.005968383 0.005602614 0.0047443784 0.0037236819 0.0027280739][-0.0017784165 -0.0017607554 -0.0016951544 -0.0014731004 -0.00086066662 0.00027051487 0.0019314721 0.0038404339 0.0056161792 0.0068793842 0.007328562 0.007023863 0.0062048589 0.0052035633 0.0041887183][-0.001703882 -0.0016520927 -0.0015184373 -0.0012186811 -0.00050711294 0.00069978286 0.0024035182 0.004284637 0.0059945486 0.0071648941 0.0075745308 0.007310417 0.0066075218 0.0057707955 0.0049093408][-0.0015976196 -0.0014848227 -0.0012388593 -0.00080692023 2.1192944e-05 0.0012622705 0.0028832546 0.0045415675 0.0059398971 0.0067913109 0.0069998675 0.0067009493 0.0061070542 0.0054657524 0.0048253769][-0.0015002554 -0.0013014426 -0.00091373554 -0.0002900121 0.00070277473 0.0020006024 0.0035181204 0.0048998278 0.0058996151 0.0063361 0.0062592891 0.0058509163 0.0053042267 0.0048057786 0.0043478995][-0.0014575485 -0.001177165 -0.000653852 0.00017169921 0.0013372182 0.0027116151 0.0041499343 0.0052942405 0.0059454511 0.006014043 0.0056731272 0.0051366379 0.0045972657 0.0041785603 0.0038356814][-0.0014790711 -0.0011536113 -0.000552431 0.00040380226 0.0016655383 0.0030602729 0.0044011059 0.005338049 0.0057210913 0.0055214688 0.0049892082 0.0043533295 0.0038070446 0.0034347493 0.0031706635][-0.0015526181 -0.001242605 -0.00066071004 0.00027442502 0.0014637624 0.0027318411 0.0038826251 0.0046019373 0.0047869147 0.0044363597 0.0038150023 0.0031392402 0.0026013474 0.002260759 0.0020551318][-0.0016484407 -0.001408882 -0.00094675372 -0.00019520661 0.00074884773 0.0017321968 0.0025874525 0.0030667996 0.0031096074 0.0027247318 0.0021370891 0.0015255961 0.0010561348 0.00077329308 0.00062334572][-0.0017408861 -0.0015992731 -0.0013124027 -0.00083608914 -0.00023068639 0.00039619755 0.00092580693 0.0011932229 0.0011666083 0.00085185037 0.0004061925 -4.6767993e-05 -0.00038787385 -0.00058690051 -0.00067985489][-0.0017996583 -0.0017406594 -0.0016092426 -0.0013827016 -0.0010834839 -0.00076992868 -0.00051029108 -0.00039107562 -0.00042792875 -0.00061425753 -0.00086532487 -0.0011143781 -0.0012987098 -0.0014039653 -0.0014505102][-0.0018221762 -0.0018080175 -0.0017694219 -0.0016966716 -0.0015926809 -0.0014793003 -0.0013844542 -0.0013424617 -0.0013602984 -0.0014346323 -0.0015326065 -0.0016294001 -0.0017004823 -0.0017398064 -0.0017545375]]...]
INFO - root - 2017-12-09 19:42:41.430868: step 52510, loss = 0.82, batch loss = 0.69 (11.6 examples/sec; 0.692 sec/batch; 53h:48m:47s remains)
INFO - root - 2017-12-09 19:42:49.659364: step 52520, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.846 sec/batch; 65h:46m:51s remains)
INFO - root - 2017-12-09 19:42:58.239194: step 52530, loss = 0.82, batch loss = 0.69 (8.8 examples/sec; 0.908 sec/batch; 70h:38m:44s remains)
INFO - root - 2017-12-09 19:43:06.969586: step 52540, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.860 sec/batch; 66h:51m:47s remains)
INFO - root - 2017-12-09 19:43:15.622800: step 52550, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.894 sec/batch; 69h:31m:07s remains)
INFO - root - 2017-12-09 19:43:24.261194: step 52560, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.859 sec/batch; 66h:47m:08s remains)
INFO - root - 2017-12-09 19:43:32.914198: step 52570, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.889 sec/batch; 69h:07m:59s remains)
INFO - root - 2017-12-09 19:43:41.645790: step 52580, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.861 sec/batch; 66h:58m:34s remains)
INFO - root - 2017-12-09 19:43:50.380716: step 52590, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.851 sec/batch; 66h:11m:18s remains)
INFO - root - 2017-12-09 19:43:59.108099: step 52600, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 67h:41m:55s remains)
2017-12-09 19:43:59.992938: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0018264133 -0.0018261478 -0.0018258594 -0.001825469 -0.0018250325 -0.0018245854 -0.0018242204 -0.0018240295 -0.0018239769 -0.0018239169 -0.0018238083 -0.0018236741 -0.0018235666 -0.0018235245 -0.0018235205][-0.0018270969 -0.0018267243 -0.0018262418 -0.0018256345 -0.001824997 -0.0018243702 -0.0018239249 -0.0018237402 -0.0018237194 -0.0018236381 -0.0018234508 -0.0018232479 -0.0018231069 -0.0018230793 -0.001823117][-0.0018278483 -0.0018274948 -0.0018268988 -0.0018261508 -0.0018253531 -0.0018245899 -0.0018240614 -0.0018238322 -0.0018238176 -0.0018237015 -0.0018234563 -0.0018232074 -0.0018230526 -0.001823045 -0.0018231297][-0.0018281867 -0.0018277809 -0.0018270456 -0.0018261984 -0.0018253506 -0.0018245592 -0.0018240111 -0.0018237999 -0.0018238315 -0.0018237092 -0.0018234501 -0.0018232027 -0.001823054 -0.0018230525 -0.0018231552][-0.0018279627 -0.0018273841 -0.0018265406 -0.0018257033 -0.0018249217 -0.0018241768 -0.0018237069 -0.0018235609 -0.0018236437 -0.0018235652 -0.0018233621 -0.0018231797 -0.0018230713 -0.0018230752 -0.0018231842][-0.0018273192 -0.0018265824 -0.0018257342 -0.0018249978 -0.0018243243 -0.0018236713 -0.001823266 -0.0018231451 -0.0018232387 -0.001823211 -0.0018231084 -0.0018230261 -0.0018230028 -0.0018230518 -0.0018231779][-0.0018265328 -0.0018258118 -0.0018251045 -0.0018245082 -0.0018239527 -0.0018234081 -0.0018230105 -0.0018227932 -0.0018228081 -0.0018228077 -0.0018228086 -0.0018228474 -0.00182293 -0.0018230358 -0.0018231706][-0.0018257363 -0.0018250587 -0.0018244829 -0.0018240059 -0.0018235996 -0.0018231872 -0.0018227849 -0.0018224951 -0.0018224602 -0.0018225154 -0.0018226322 -0.0018227898 -0.0018229638 -0.0018231092 -0.0018232205][-0.0018250766 -0.0018243883 -0.0018238912 -0.0018235032 -0.0018232239 -0.0018229277 -0.0018225925 -0.001822336 -0.0018223233 -0.0018224412 -0.0018226333 -0.0018228415 -0.001823046 -0.0018231866 -0.0018232631][-0.0018246314 -0.0018239963 -0.0018236111 -0.0018233032 -0.0018231003 -0.001822878 -0.0018225976 -0.0018223745 -0.0018223573 -0.0018224787 -0.0018226624 -0.00182286 -0.0018230568 -0.0018231887 -0.0018232443][-0.0018243624 -0.0018238468 -0.0018235848 -0.0018233507 -0.0018231814 -0.0018230055 -0.0018227713 -0.0018225798 -0.0018225401 -0.0018226056 -0.0018227373 -0.0018228913 -0.0018230546 -0.0018231669 -0.0018232091][-0.001824016 -0.0018236064 -0.001823467 -0.0018233164 -0.0018232085 -0.0018231076 -0.0018229736 -0.0018228461 -0.0018227951 -0.0018228119 -0.00182289 -0.0018229886 -0.0018230948 -0.0018231693 -0.0018231906][-0.0018236751 -0.0018233644 -0.0018233098 -0.0018232407 -0.00182321 -0.0018231926 -0.0018231455 -0.0018230864 -0.0018230568 -0.0018230606 -0.0018230922 -0.0018231313 -0.00182317 -0.0018231933 -0.0018231849][-0.0018234863 -0.0018232295 -0.0018231999 -0.0018231866 -0.0018231972 -0.0018232096 -0.0018232011 -0.0018231815 -0.0018231747 -0.0018231742 -0.0018231765 -0.0018231752 -0.0018231751 -0.0018231735 -0.0018231538][-0.0018234543 -0.0018231588 -0.0018231138 -0.0018231196 -0.0018231343 -0.0018231451 -0.0018231447 -0.0018231384 -0.0018231384 -0.0018231367 -0.0018231289 -0.0018231203 -0.0018231146 -0.0018231121 -0.0018231008]]...]
INFO - root - 2017-12-09 19:44:08.661259: step 52610, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.873 sec/batch; 67h:53m:07s remains)
INFO - root - 2017-12-09 19:44:17.299948: step 52620, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.887 sec/batch; 68h:58m:25s remains)
INFO - root - 2017-12-09 19:44:26.088326: step 52630, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.866 sec/batch; 67h:20m:24s remains)
INFO - root - 2017-12-09 19:44:34.708716: step 52640, loss = 0.82, batch loss = 0.69 (10.5 examples/sec; 0.764 sec/batch; 59h:22m:29s remains)
INFO - root - 2017-12-09 19:44:43.430386: step 52650, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.890 sec/batch; 69h:09m:05s remains)
INFO - root - 2017-12-09 19:44:51.937485: step 52660, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.838 sec/batch; 65h:07m:21s remains)
INFO - root - 2017-12-09 19:45:00.579724: step 52670, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.865 sec/batch; 67h:13m:44s remains)
INFO - root - 2017-12-09 19:45:09.196506: step 52680, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.833 sec/batch; 64h:43m:26s remains)
INFO - root - 2017-12-09 19:45:17.719201: step 52690, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 67h:20m:57s remains)
INFO - root - 2017-12-09 19:45:26.325501: step 52700, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.888 sec/batch; 68h:59m:24s remains)
2017-12-09 19:45:27.235355: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0018056182 -0.0018039886 -0.0018034533 -0.0018028854 -0.0018022817 -0.0018019346 -0.0018018304 -0.0018018164 -0.0018018301 -0.0018019759 -0.0018021968 -0.0018022836 -0.001802113 -0.0018019819 -0.0018018801][-0.0018046747 -0.0018031999 -0.0018027637 -0.0018022637 -0.0018016603 -0.0018012194 -0.0018010144 -0.0018009448 -0.0018010457 -0.0018014001 -0.0018018431 -0.0018020597 -0.0018019192 -0.0018017521 -0.0018016428][-0.0018047456 -0.0018034541 -0.0018031083 -0.0018026968 -0.0018021263 -0.0018016181 -0.001801316 -0.0018011789 -0.0018013355 -0.0018018342 -0.0018024358 -0.0018027907 -0.0018027693 -0.0018026574 -0.0018025627][-0.001804973 -0.0018039315 -0.0018037747 -0.0018035412 -0.0018030807 -0.0018025616 -0.0018021606 -0.0018018881 -0.0018019681 -0.0018024894 -0.0018031931 -0.0018037172 -0.0018038553 -0.0018038204 -0.0018037567][-0.0018054856 -0.0018046518 -0.0018047305 -0.0018047593 -0.0018045428 -0.0018041005 -0.0018035731 -0.0018030782 -0.0018029604 -0.0018033176 -0.0018039224 -0.0018044995 -0.0018047751 -0.0018048426 -0.0018048151][-0.0018058683 -0.0018052728 -0.0018056785 -0.0018060704 -0.0018061919 -0.0018058942 -0.0018052782 -0.0018045416 -0.0018041583 -0.0018042349 -0.0018046093 -0.0018051011 -0.0018054213 -0.0018055702 -0.0018055885][-0.0018061707 -0.0018057062 -0.001806388 -0.0018071563 -0.0018076245 -0.0018074936 -0.0018068298 -0.0018058895 -0.0018052279 -0.0018049928 -0.0018050662 -0.0018053742 -0.0018056696 -0.0018059205 -0.0018060784][-0.0018063219 -0.0018059565 -0.0018068382 -0.0018079334 -0.0018087119 -0.0018088063 -0.0018081802 -0.0018071634 -0.0018062418 -0.001805654 -0.0018053491 -0.0018053706 -0.0018055309 -0.0018058596 -0.0018062455][-0.0018066206 -0.0018062501 -0.0018072253 -0.0018084709 -0.0018094298 -0.001809682 -0.0018090677 -0.0018080067 -0.0018068735 -0.0018059167 -0.0018052666 -0.0018050047 -0.0018050184 -0.001805371 -0.0018059758][-0.0018070553 -0.0018066436 -0.0018075593 -0.0018087677 -0.0018096975 -0.0018100063 -0.0018094247 -0.0018083496 -0.0018070837 -0.0018058874 -0.0018049993 -0.0018045095 -0.0018043469 -0.001804643 -0.0018053642][-0.0018075809 -0.0018071114 -0.0018078808 -0.0018089408 -0.0018096935 -0.0018099539 -0.0018094425 -0.0018084833 -0.0018072751 -0.001806047 -0.0018050935 -0.0018044811 -0.0018041481 -0.001804321 -0.00180505][-0.00180842 -0.0018077171 -0.0018082191 -0.0018090082 -0.0018095077 -0.0018096642 -0.0018092401 -0.0018084594 -0.0018074217 -0.0018063247 -0.0018054841 -0.0018048909 -0.001804525 -0.0018045938 -0.0018052063][-0.0018091932 -0.0018083975 -0.0018085812 -0.001809025 -0.0018092656 -0.0018093383 -0.0018090103 -0.0018084415 -0.0018076567 -0.0018068239 -0.0018061872 -0.0018056863 -0.0018053784 -0.001805349 -0.0018057526][-0.001809853 -0.0018089839 -0.0018089181 -0.0018090737 -0.001809089 -0.0018090355 -0.0018087818 -0.0018084163 -0.0018079209 -0.0018074327 -0.0018070597 -0.0018067122 -0.001806484 -0.0018063998 -0.0018065937][-0.0018103898 -0.0018094381 -0.0018092393 -0.0018092708 -0.0018091662 -0.0018090466 -0.0018088766 -0.0018086799 -0.0018084345 -0.0018082222 -0.0018080521 -0.0018078302 -0.0018076503 -0.0018075297 -0.0018075627]]...]
INFO - root - 2017-12-09 19:45:35.951079: step 52710, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.880 sec/batch; 68h:23m:26s remains)
INFO - root - 2017-12-09 19:45:44.323004: step 52720, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.835 sec/batch; 64h:55m:20s remains)
INFO - root - 2017-12-09 19:45:52.819470: step 52730, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.863 sec/batch; 67h:02m:46s remains)
INFO - root - 2017-12-09 19:46:01.244147: step 52740, loss = 0.82, batch loss = 0.69 (11.1 examples/sec; 0.719 sec/batch; 55h:51m:42s remains)
INFO - root - 2017-12-09 19:46:09.814462: step 52750, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.858 sec/batch; 66h:38m:26s remains)
INFO - root - 2017-12-09 19:46:18.577300: step 52760, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.878 sec/batch; 68h:11m:25s remains)
INFO - root - 2017-12-09 19:46:27.085046: step 52770, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.870 sec/batch; 67h:37m:41s remains)
INFO - root - 2017-12-09 19:46:35.674637: step 52780, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.876 sec/batch; 68h:02m:11s remains)
INFO - root - 2017-12-09 19:46:44.294639: step 52790, loss = 0.82, batch loss = 0.69 (9.7 examples/sec; 0.822 sec/batch; 63h:51m:47s remains)
INFO - root - 2017-12-09 19:46:53.004845: step 52800, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.870 sec/batch; 67h:34m:49s remains)
2017-12-09 19:46:53.858854: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0018209073 -0.0018206406 -0.0018207638 -0.0018208744 -0.0018209006 -0.0018209894 -0.0018209917 -0.0018210303 -0.0018208688 -0.0018206391 -0.0018201901 -0.0018197352 -0.0018192937 -0.0018188672 -0.0018186194][-0.0018207174 -0.0018203736 -0.0018204106 -0.0018191081 -0.0018168843 -0.0018188765 -0.0018197473 -0.0018209373 -0.0018207323 -0.0018204942 -0.0018199491 -0.001819444 -0.0018188662 -0.0018183299 -0.0018179885][-0.0018210621 -0.0018205267 -0.0018210565 -0.001820876 -0.0018202687 -0.0018199361 -0.0018189626 -0.00182086 -0.001821158 -0.0018209976 -0.0018204397 -0.0018199096 -0.0018191956 -0.0018186172 -0.0018181885][-0.0017911632 -0.0017982003 -0.0018062852 -0.0018123057 -0.0018172835 -0.0018204192 -0.001821095 -0.0018216769 -0.0018216883 -0.0018215569 -0.0018211473 -0.0018207446 -0.0018200864 -0.0018195239 -0.0018190664][-0.0017254259 -0.0017281441 -0.0017425492 -0.001758281 -0.0017774863 -0.0017976586 -0.0018134029 -0.0018212688 -0.0018218234 -0.0018219948 -0.0018219091 -0.0018217982 -0.0018214254 -0.0018210228 -0.0018207692][-0.0016286835 -0.0016238042 -0.0016455891 -0.0016726365 -0.0017101869 -0.0017547935 -0.001792799 -0.0018154946 -0.0018209644 -0.0018214465 -0.0018215929 -0.0018220074 -0.0018223622 -0.0018225871 -0.0018229223][-0.00154101 -0.001509805 -0.0015336974 -0.0015725824 -0.0016293988 -0.0017004879 -0.0017646656 -0.001805953 -0.0018193102 -0.0018208134 -0.0018208482 -0.0018216105 -0.0018224042 -0.001823485 -0.0018247484][-0.0014948519 -0.0014494526 -0.0014771525 -0.0015175528 -0.0015784933 -0.0016595511 -0.0017387884 -0.0017942998 -0.0018166961 -0.0018202743 -0.0018205281 -0.0018213416 -0.0018223205 -0.0018242442 -0.0018262553][-0.001502516 -0.0014497829 -0.0014811291 -0.0015214439 -0.0015782518 -0.0016526919 -0.0017286203 -0.0017870949 -0.0018145078 -0.0018200119 -0.0018204154 -0.0018211921 -0.0018225819 -0.0018247731 -0.0018269602][-0.0015291583 -0.0014772193 -0.0015130264 -0.0015531058 -0.0016035034 -0.0016672271 -0.0017324667 -0.0017855716 -0.0018133477 -0.0018200985 -0.0018201983 -0.0018208439 -0.0018222862 -0.0018245565 -0.0018269653][-0.0015517758 -0.0015187114 -0.0015613671 -0.0016017755 -0.0016441382 -0.0016966475 -0.0017495847 -0.0017922762 -0.0018149307 -0.0018202583 -0.0018199455 -0.0018201378 -0.0018211419 -0.0018230136 -0.001825174][-0.0015798969 -0.0015494742 -0.0016024045 -0.0016466982 -0.0016887782 -0.0017298828 -0.0017687792 -0.00180072 -0.0018174262 -0.0018206044 -0.0018199607 -0.0018197128 -0.0018201629 -0.0018214415 -0.0018230429][-0.0016088694 -0.0015901257 -0.0016513162 -0.0016981749 -0.0017347464 -0.0017614694 -0.0017873832 -0.0018070787 -0.0018178063 -0.0018200228 -0.001820087 -0.0018196221 -0.0018195707 -0.0018202877 -0.0018212376][-0.0016475798 -0.0016486293 -0.0017064218 -0.0017481898 -0.0017730488 -0.0017859164 -0.0017978993 -0.0018077014 -0.0018149719 -0.0018173456 -0.0018193151 -0.0018196731 -0.0018195122 -0.0018197668 -0.0018202171][-0.0017094329 -0.0017240545 -0.001768069 -0.0017949555 -0.0018040842 -0.0018034759 -0.0018026606 -0.0018038766 -0.001807774 -0.0018115049 -0.0018171403 -0.0018195141 -0.001819444 -0.0018194369 -0.0018196574]]...]
INFO - root - 2017-12-09 19:47:02.614630: step 52810, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 67h:27m:34s remains)
INFO - root - 2017-12-09 19:47:11.305876: step 52820, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.896 sec/batch; 69h:38m:22s remains)
INFO - root - 2017-12-09 19:47:20.014599: step 52830, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.866 sec/batch; 67h:17m:41s remains)
INFO - root - 2017-12-09 19:47:28.481801: step 52840, loss = 0.81, batch loss = 0.68 (10.3 examples/sec; 0.777 sec/batch; 60h:22m:24s remains)
INFO - root - 2017-12-09 19:47:37.278266: step 52850, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.878 sec/batch; 68h:14m:27s remains)
INFO - root - 2017-12-09 19:47:45.931349: step 52860, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.859 sec/batch; 66h:43m:21s remains)
INFO - root - 2017-12-09 19:47:54.542065: step 52870, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.878 sec/batch; 68h:11m:42s remains)
INFO - root - 2017-12-09 19:48:03.209523: step 52880, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.846 sec/batch; 65h:42m:01s remains)
INFO - root - 2017-12-09 19:48:11.965329: step 52890, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.877 sec/batch; 68h:05m:54s remains)
INFO - root - 2017-12-09 19:48:20.589213: step 52900, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.880 sec/batch; 68h:19m:00s remains)
2017-12-09 19:48:21.453685: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0018065006 -0.0018068507 -0.0018085641 -0.0018106751 -0.0018128748 -0.0018147398 -0.0018161199 -0.001817144 -0.0018177058 -0.0018175273 -0.001816634 -0.0018151279 -0.0018130073 -0.0018104245 -0.0018077164][-0.0018066665 -0.0018076532 -0.0018101042 -0.0018129735 -0.0018157959 -0.0018181513 -0.0018198694 -0.00182103 -0.0018216808 -0.0018217139 -0.0018208066 -0.0018189797 -0.0018161325 -0.0018125327 -0.0018087346][-0.0018073876 -0.0018091812 -0.0018123692 -0.0018158497 -0.0018190115 -0.0018215191 -0.0018232015 -0.0018242021 -0.0018246934 -0.0018248691 -0.0018241032 -0.001822248 -0.0018190405 -0.001814843 -0.0018103634][-0.0018081556 -0.0018106948 -0.0018144596 -0.0018183693 -0.0018217446 -0.0018241724 -0.0018256259 -0.0018262906 -0.0018264151 -0.0018265559 -0.0018258929 -0.0018241524 -0.0018208565 -0.0018164295 -0.0018116337][-0.0018086911 -0.0018117314 -0.0018159773 -0.0018203151 -0.0018239027 -0.0018263009 -0.001827568 -0.0018278654 -0.0018274568 -0.0018273958 -0.0018267198 -0.0018250294 -0.0018217611 -0.0018172745 -0.0018123291][-0.0018091006 -0.0018122687 -0.0018167644 -0.0018214368 -0.001825212 -0.0018276109 -0.0018287741 -0.0018288704 -0.001828035 -0.001827544 -0.0018266455 -0.0018248864 -0.0018216955 -0.0018172794 -0.0018123932][-0.0018091397 -0.0018122067 -0.0018166424 -0.0018215094 -0.0018253612 -0.0018278889 -0.0018291927 -0.0018293121 -0.0018283749 -0.0018274863 -0.0018262744 -0.001824275 -0.001821103 -0.001816828 -0.0018119928][-0.0018089518 -0.0018117938 -0.0018161504 -0.0018212096 -0.0018253361 -0.0018282169 -0.001829832 -0.001830066 -0.0018290831 -0.0018277875 -0.0018261074 -0.0018236492 -0.0018203087 -0.0018161819 -0.0018114615][-0.0018087156 -0.0018112009 -0.001815468 -0.0018207987 -0.0018253479 -0.0018288335 -0.0018310954 -0.0018318694 -0.0018310917 -0.0018294607 -0.0018272452 -0.0018241958 -0.0018204836 -0.0018162042 -0.0018114976][-0.0018083473 -0.0018105828 -0.0018148615 -0.0018204739 -0.0018253719 -0.0018295031 -0.0018326094 -0.0018340383 -0.0018334858 -0.0018317833 -0.0018293043 -0.001825778 -0.0018215439 -0.0018168786 -0.0018121464][-0.0018080445 -0.001810007 -0.0018141193 -0.0018196468 -0.001824578 -0.0018289973 -0.0018326232 -0.0018345797 -0.0018345099 -0.0018330448 -0.0018305447 -0.0018268455 -0.0018223468 -0.0018174744 -0.0018126772][-0.0018075734 -0.001809406 -0.0018132441 -0.0018182932 -0.0018228837 -0.0018271664 -0.0018307595 -0.0018328724 -0.0018331818 -0.0018320777 -0.0018298439 -0.0018263744 -0.0018220347 -0.0018173134 -0.0018127956][-0.0018072389 -0.0018087872 -0.0018121937 -0.0018165252 -0.0018204575 -0.0018241357 -0.0018271928 -0.0018290173 -0.0018293812 -0.0018286692 -0.0018270016 -0.0018242174 -0.0018205926 -0.0018166327 -0.0018128433][-0.0018068762 -0.0018082127 -0.0018110509 -0.0018145177 -0.0018176363 -0.0018205191 -0.0018228431 -0.0018242231 -0.001824492 -0.0018239671 -0.0018227899 -0.0018207482 -0.0018180199 -0.0018149766 -0.0018120785][-0.0018065028 -0.0018076088 -0.0018097452 -0.0018122446 -0.0018144397 -0.0018164088 -0.0018179348 -0.0018188352 -0.0018189492 -0.0018185447 -0.0018177833 -0.001816452 -0.0018146308 -0.0018125378 -0.0018105472]]...]
INFO - root - 2017-12-09 19:48:30.207810: step 52910, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 67h:38m:00s remains)
INFO - root - 2017-12-09 19:48:38.986588: step 52920, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 67h:40m:19s remains)
INFO - root - 2017-12-09 19:48:47.689436: step 52930, loss = 0.81, batch loss = 0.68 (9.0 examples/sec; 0.891 sec/batch; 69h:13m:09s remains)
INFO - root - 2017-12-09 19:48:56.277793: step 52940, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.872 sec/batch; 67h:44m:09s remains)
INFO - root - 2017-12-09 19:49:04.988387: step 52950, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.896 sec/batch; 69h:36m:20s remains)
INFO - root - 2017-12-09 19:49:13.690703: step 52960, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 66h:55m:43s remains)
INFO - root - 2017-12-09 19:49:22.203466: step 52970, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.853 sec/batch; 66h:15m:53s remains)
INFO - root - 2017-12-09 19:49:30.946892: step 52980, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.870 sec/batch; 67h:30m:56s remains)
INFO - root - 2017-12-09 19:49:39.631392: step 52990, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.861 sec/batch; 66h:48m:47s remains)
INFO - root - 2017-12-09 19:49:48.375402: step 53000, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 67h:38m:56s remains)
2017-12-09 19:49:49.235621: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0018266808 -0.0018274459 -0.0018287172 -0.001830391 -0.0018319598 -0.0018329488 -0.001833408 -0.0018334578 -0.0018331592 -0.0018327332 -0.0018326479 -0.0018333263 -0.0018347186 -0.0018363941 -0.001837807][-0.0018258053 -0.0018264663 -0.0018275252 -0.0018289587 -0.0018302776 -0.0018310496 -0.0018313146 -0.0018311105 -0.0018305528 -0.0018299869 -0.0018299166 -0.0018306691 -0.0018322204 -0.0018340094 -0.0018354887][-0.0018252408 -0.0018258081 -0.0018266662 -0.0018278599 -0.0018289286 -0.001829528 -0.0018296625 -0.0018292834 -0.0018285973 -0.0018279717 -0.0018278912 -0.0018286061 -0.0018301738 -0.0018319504 -0.0018333615][-0.0018249737 -0.0018255361 -0.0018262691 -0.0018272614 -0.0018280918 -0.0018284593 -0.0018283309 -0.0018277655 -0.001827067 -0.0018265329 -0.0018265634 -0.0018273641 -0.0018289177 -0.001830591 -0.0018317845][-0.0018247598 -0.0018253067 -0.0018259584 -0.0018268299 -0.0018274989 -0.0018276474 -0.0018273052 -0.0018266144 -0.0018258992 -0.0018254252 -0.0018255712 -0.0018265366 -0.0018280378 -0.0018295094 -0.0018304036][-0.0018244975 -0.0018249991 -0.0018255602 -0.0018263073 -0.0018268473 -0.0018268563 -0.0018264058 -0.00182567 -0.0018249777 -0.0018245983 -0.0018248395 -0.0018258147 -0.0018270961 -0.0018281777 -0.0018287642][-0.0018242607 -0.0018247254 -0.0018251908 -0.0018257708 -0.0018261408 -0.0018260331 -0.0018255283 -0.0018248449 -0.0018242944 -0.0018241154 -0.0018245059 -0.0018254523 -0.0018264558 -0.0018271635 -0.0018274933][-0.0018240951 -0.0018244947 -0.0018248553 -0.0018252593 -0.0018254555 -0.0018252525 -0.0018247666 -0.0018242486 -0.0018239531 -0.0018240184 -0.0018245789 -0.0018255478 -0.0018264106 -0.0018269098 -0.0018270734][-0.0018240471 -0.0018243059 -0.0018245474 -0.0018247598 -0.0018247907 -0.0018245241 -0.0018241309 -0.0018238826 -0.0018238566 -0.0018241388 -0.0018247962 -0.0018257604 -0.0018265765 -0.0018270054 -0.0018271082][-0.0018239897 -0.0018240678 -0.0018241866 -0.0018242474 -0.0018241534 -0.0018239259 -0.001823702 -0.0018237189 -0.0018239011 -0.0018242605 -0.001824868 -0.0018256783 -0.0018264089 -0.0018267783 -0.0018268347][-0.0018238991 -0.0018238439 -0.0018238515 -0.0018238003 -0.0018236601 -0.001823533 -0.0018234804 -0.0018236607 -0.0018239872 -0.0018244144 -0.0018249701 -0.0018256077 -0.0018262486 -0.0018265607 -0.0018265059][-0.0018237635 -0.0018236896 -0.0018236876 -0.0018236446 -0.0018235054 -0.0018234063 -0.0018234208 -0.0018236478 -0.0018240535 -0.0018245321 -0.0018250901 -0.0018256317 -0.0018261755 -0.0018264229 -0.0018262417][-0.001824083 -0.0018239814 -0.0018239985 -0.0018239836 -0.001823851 -0.0018237334 -0.0018237438 -0.0018239764 -0.001824406 -0.001824911 -0.0018254676 -0.0018259251 -0.0018263367 -0.0018264841 -0.0018262489][-0.0018246694 -0.0018245266 -0.0018245323 -0.0018245461 -0.0018244309 -0.0018242858 -0.0018242704 -0.0018244748 -0.0018248545 -0.0018252945 -0.0018257691 -0.001826093 -0.0018263595 -0.0018264349 -0.0018262047][-0.001825236 -0.0018249954 -0.0018249716 -0.001825008 -0.0018249289 -0.0018248066 -0.001824779 -0.0018249121 -0.0018251921 -0.0018255111 -0.0018258658 -0.0018260791 -0.0018262328 -0.0018262354 -0.001826024]]...]
INFO - root - 2017-12-09 19:49:57.858585: step 53010, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 67h:27m:24s remains)
INFO - root - 2017-12-09 19:50:06.356995: step 53020, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.833 sec/batch; 64h:40m:16s remains)
INFO - root - 2017-12-09 19:50:15.165420: step 53030, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.866 sec/batch; 67h:14m:34s remains)
INFO - root - 2017-12-09 19:50:23.743071: step 53040, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.881 sec/batch; 68h:23m:23s remains)
INFO - root - 2017-12-09 19:50:32.469158: step 53050, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.876 sec/batch; 68h:00m:04s remains)
INFO - root - 2017-12-09 19:50:40.992321: step 53060, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.839 sec/batch; 65h:09m:04s remains)
INFO - root - 2017-12-09 19:50:49.505891: step 53070, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.876 sec/batch; 68h:01m:37s remains)
INFO - root - 2017-12-09 19:50:58.146866: step 53080, loss = 0.82, batch loss = 0.69 (9.7 examples/sec; 0.823 sec/batch; 63h:52m:51s remains)
INFO - root - 2017-12-09 19:51:06.790780: step 53090, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.847 sec/batch; 65h:46m:05s remains)
INFO - root - 2017-12-09 19:51:15.535874: step 53100, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.904 sec/batch; 70h:07m:52s remains)
2017-12-09 19:51:16.493824: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.00058509258 0.00066962896 0.00076903927 0.00087280513 0.0009538309 0.00099501072 0.00098626467 0.00093759235 0.00085825764 0.00074624375 0.000617758 0.00049182435 0.00037044834 0.0002517536 0.00013286772][0.00058122224 0.00071598531 0.0008866085 0.0010643484 0.0012061266 0.0012795102 0.0012806141 0.0012157563 0.001110186 0.00096663891 0.0008006721 0.00062959467 0.0004792331 0.0003653568 0.00026370317][0.00047257415 0.00063471019 0.00083416433 0.0010477925 0.0012241007 0.0013197487 0.0013324831 0.0012602386 0.0011362861 0.00097772724 0.00080725655 0.00064305437 0.00047710782 0.00035384309 0.00026620657][0.00041172036 0.00058617548 0.00079675403 0.0010079039 0.0011690018 0.0012584914 0.0012656505 0.001195304 0.0010609726 0.00089243019 0.00072348292 0.000552839 0.00038932834 0.00025722862 0.00017469202][0.00020691997 0.00037721719 0.0005943781 0.00079631794 0.00094013696 0.0010066625 0.00098478247 0.00089223741 0.00075341866 0.00060569344 0.00046513148 0.00032135856 0.00020390947 0.00010912318 4.1416963e-05][-0.00011028477 1.8395716e-05 0.00021231931 0.00042381312 0.00059052731 0.00066973583 0.00065765216 0.00055610028 0.00041020813 0.00025148049 0.00010821235 -1.1218712e-05 -0.00010680675 -0.00016771047 -0.00018348114][-0.00046613952 -0.00036753237 -0.00020094879 -1.581572e-05 0.00014472997 0.00024709885 0.00026773394 0.00019676203 6.2010949e-05 -9.7728218e-05 -0.00023705617 -0.00034281437 -0.00042703538 -0.00046059256 -0.00042811607][-0.00077653385 -0.00072939752 -0.0006020508 -0.00042683294 -0.00026654382 -0.00015297148 -0.00010813342 -0.00014387036 -0.00025193626 -0.00040334242 -0.00054261542 -0.00066076277 -0.00072146626 -0.00071081251 -0.00062935753][-0.00096094608 -0.000946638 -0.00085804437 -0.00071846205 -0.00057536736 -0.00046106649 -0.00040388282 -0.00041568966 -0.00049018336 -0.00060637738 -0.00071518158 -0.00079372316 -0.00082318985 -0.00079007237 -0.00069709972][-0.000955804 -0.0009574666 -0.00089135242 -0.00078092958 -0.00065836392 -0.00055173517 -0.00049391412 -0.00049841439 -0.00055303762 -0.000627063 -0.0007072608 -0.0007701685 -0.00078024634 -0.00072949135 -0.00062661397][-0.00087524578 -0.00087915384 -0.00083677156 -0.00073949597 -0.00063243054 -0.00054048561 -0.00048998976 -0.00048348971 -0.00052226114 -0.0005803986 -0.00064215809 -0.00068944134 -0.00068310241 -0.00063538458 -0.0005450591][-0.00067441631 -0.00066637155 -0.00064572738 -0.00058086682 -0.00050439732 -0.00043070887 -0.00039629266 -0.00039747637 -0.00042996043 -0.0004756219 -0.00052817608 -0.00057267433 -0.00056514377 -0.00052455452 -0.0004657621][-0.00036912039 -0.00032312342 -0.00029450061 -0.00024096575 -0.0001867566 -0.00014306256 -0.00013664877 -0.00015762483 -0.00020753278 -0.00026283809 -0.00032573531 -0.00038378057 -0.00042067817 -0.00041748711 -0.00038766616][-0.00011173054 -2.0094798e-05 3.2524345e-05 8.2212384e-05 0.00013369729 0.00017280749 0.00017735327 0.00014698796 9.5761032e-05 4.1777152e-05 -2.381776e-05 -9.4975228e-05 -0.00016582326 -0.0002274384 -0.000267984][3.3291406e-05 0.00012594403 0.00016479252 0.00020527921 0.00024492817 0.00027188647 0.00027839805 0.00025450496 0.00022017944 0.00018093374 0.00012426951 5.5922777e-05 -2.793863e-05 -0.00010197086 -0.00017001608]]...]
INFO - root - 2017-12-09 19:51:25.123932: step 53110, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.878 sec/batch; 68h:10m:37s remains)
INFO - root - 2017-12-09 19:51:33.493965: step 53120, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 67h:24m:32s remains)
INFO - root - 2017-12-09 19:51:42.067238: step 53130, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.850 sec/batch; 65h:59m:06s remains)
INFO - root - 2017-12-09 19:51:50.501352: step 53140, loss = 0.82, batch loss = 0.69 (9.7 examples/sec; 0.826 sec/batch; 64h:07m:13s remains)
INFO - root - 2017-12-09 19:51:58.975988: step 53150, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.854 sec/batch; 66h:16m:37s remains)
INFO - root - 2017-12-09 19:52:07.583891: step 53160, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.866 sec/batch; 67h:11m:09s remains)
INFO - root - 2017-12-09 19:52:16.023569: step 53170, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.879 sec/batch; 68h:13m:07s remains)
INFO - root - 2017-12-09 19:52:24.625989: step 53180, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.896 sec/batch; 69h:31m:26s remains)
INFO - root - 2017-12-09 19:52:33.349181: step 53190, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.881 sec/batch; 68h:20m:33s remains)
INFO - root - 2017-12-09 19:52:41.976928: step 53200, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.874 sec/batch; 67h:47m:37s remains)
2017-12-09 19:52:42.837709: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0018027173 -0.0017990191 -0.0017979024 -0.0017978989 -0.0017988413 -0.001800064 -0.0018011959 -0.0018015929 -0.0018009546 -0.0017996313 -0.0017979706 -0.0017967726 -0.0017962072 -0.0017959055 -0.001795592][-0.0018030418 -0.0017987121 -0.0017968635 -0.0017963079 -0.0017967985 -0.0017978422 -0.001798966 -0.0017996436 -0.0017995833 -0.001798809 -0.0017975293 -0.0017964327 -0.0017958317 -0.0017953712 -0.0017949392][-0.0018041735 -0.0017995664 -0.0017972358 -0.0017962363 -0.0017963636 -0.0017970732 -0.0017978578 -0.0017983236 -0.0017984423 -0.0017981556 -0.0017973945 -0.0017965349 -0.0017959822 -0.0017955395 -0.0017950539][-0.0018051843 -0.0018007105 -0.0017980486 -0.0017967669 -0.0017966821 -0.0017969789 -0.0017972041 -0.0017973301 -0.0017974997 -0.0017975005 -0.0017970975 -0.0017965087 -0.0017961989 -0.0017958469 -0.0017953039][-0.0018061391 -0.001802069 -0.0017993377 -0.0017978153 -0.0017973933 -0.0017971304 -0.0017968152 -0.0017966367 -0.0017967802 -0.0017969009 -0.0017966712 -0.0017963416 -0.0017963087 -0.0017961061 -0.0017955326][-0.0018069851 -0.0018034278 -0.0018009202 -0.0017993394 -0.0017984433 -0.0017974563 -0.0017964405 -0.0017959302 -0.0017959618 -0.0017960311 -0.0017958846 -0.0017958138 -0.0017960826 -0.0017960947 -0.001795626][-0.0018081652 -0.0018051205 -0.0018028002 -0.0018010923 -0.0017996781 -0.001798013 -0.0017963524 -0.0017954507 -0.0017953 -0.0017952918 -0.0017952102 -0.0017952536 -0.0017956864 -0.0017959097 -0.0017955755][-0.0018103422 -0.0018076174 -0.0018053704 -0.0018033008 -0.0018013045 -0.0017991982 -0.0017972584 -0.0017959878 -0.0017955329 -0.0017954177 -0.0017952627 -0.0017952991 -0.0017957084 -0.0017959625 -0.001795614][-0.0018141123 -0.0018113945 -0.0018087127 -0.001805931 -0.0018032478 -0.0018007706 -0.0017986546 -0.0017971728 -0.0017964288 -0.0017960762 -0.0017958052 -0.0017957347 -0.0017960035 -0.0017960467 -0.0017955472][-0.0018186241 -0.0018154127 -0.0018119344 -0.0018083027 -0.0018048382 -0.0018019073 -0.0017996154 -0.0017981784 -0.0017973545 -0.0017968233 -0.0017964557 -0.0017962139 -0.0017963159 -0.0017961413 -0.0017954883][-0.0018229978 -0.0018191036 -0.0018148096 -0.0018103322 -0.0018060878 -0.001802589 -0.0017999926 -0.0017985335 -0.0017977197 -0.0017971912 -0.0017967937 -0.0017964473 -0.0017964424 -0.0017961377 -0.0017954053][-0.0018268186 -0.001822346 -0.0018173741 -0.0018121847 -0.0018072346 -0.0018031124 -0.0018000604 -0.0017984003 -0.0017975208 -0.0017969533 -0.0017966544 -0.0017964544 -0.0017964408 -0.0017960869 -0.0017953455][-0.0018294792 -0.0018246004 -0.0018191074 -0.0018134102 -0.0018080315 -0.0018034838 -0.0017998877 -0.0017977811 -0.0017966077 -0.0017959785 -0.0017958009 -0.0017958106 -0.0017959671 -0.0017957198 -0.0017951438][-0.0018299905 -0.0018247371 -0.0018188667 -0.0018130484 -0.0018076792 -0.0018030792 -0.0017992326 -0.0017968175 -0.0017954544 -0.0017948202 -0.001794812 -0.0017950308 -0.001795325 -0.0017952288 -0.0017948258][-0.0018282615 -0.0018225575 -0.0018166756 -0.0018112122 -0.0018063259 -0.0018020563 -0.0017984388 -0.0017960551 -0.0017946559 -0.0017940239 -0.0017940733 -0.0017944177 -0.0017947915 -0.0017948136 -0.0017945638]]...]
INFO - root - 2017-12-09 19:52:51.536126: step 53210, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 67h:17m:40s remains)
INFO - root - 2017-12-09 19:53:00.003072: step 53220, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.848 sec/batch; 65h:48m:07s remains)
INFO - root - 2017-12-09 19:53:08.500678: step 53230, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.878 sec/batch; 68h:05m:01s remains)
INFO - root - 2017-12-09 19:53:17.082680: step 53240, loss = 0.82, batch loss = 0.69 (8.3 examples/sec; 0.969 sec/batch; 75h:08m:03s remains)
INFO - root - 2017-12-09 19:53:25.712522: step 53250, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.852 sec/batch; 66h:05m:14s remains)
INFO - root - 2017-12-09 19:53:34.373188: step 53260, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.858 sec/batch; 66h:31m:19s remains)
INFO - root - 2017-12-09 19:53:42.792897: step 53270, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.848 sec/batch; 65h:46m:35s remains)
INFO - root - 2017-12-09 19:53:51.344822: step 53280, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.872 sec/batch; 67h:38m:42s remains)
INFO - root - 2017-12-09 19:54:00.009297: step 53290, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.865 sec/batch; 67h:07m:00s remains)
INFO - root - 2017-12-09 19:54:08.624908: step 53300, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 67h:33m:50s remains)
2017-12-09 19:54:09.639198: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0018346413 -0.0018297819 -0.0018277545 -0.0018295248 -0.0018349643 -0.0018365941 -0.0018373389 -0.0018372288 -0.0018371386 -0.0018369601 -0.0018368269 -0.0018369 -0.0018371512 -0.0018371696 -0.0018371503][-0.0018345789 -0.0018293483 -0.001826158 -0.0018281238 -0.0018336041 -0.0018364366 -0.0018373888 -0.0018370653 -0.0018368149 -0.0018364412 -0.0018368982 -0.001836937 -0.0018373595 -0.0018373324 -0.0018374078][-0.0018365543 -0.0018321144 -0.0018291998 -0.0018302868 -0.0018338112 -0.0018364431 -0.0018366918 -0.0018345844 -0.0018330695 -0.0018320386 -0.0018357167 -0.0018359012 -0.0018379147 -0.0018372268 -0.0018371466][-0.001839189 -0.0018359278 -0.0018335311 -0.001833556 -0.0018348092 -0.0018358271 -0.0018337747 -0.0018261726 -0.0018199532 -0.0018152399 -0.0018241777 -0.0018252438 -0.0018339391 -0.0018299497 -0.0018296663][-0.0018369847 -0.0018328527 -0.0018315462 -0.0018327566 -0.0018344499 -0.001833618 -0.0018275314 -0.0018105332 -0.0017932541 -0.0017780681 -0.0017917369 -0.0017947005 -0.0018108755 -0.001801386 -0.0017977706][-0.0018183407 -0.001809228 -0.0018101562 -0.0018188296 -0.0018279786 -0.0018300117 -0.0018213489 -0.0017936915 -0.00176149 -0.0017296545 -0.0017386351 -0.0017382111 -0.001758227 -0.0017388832 -0.001717577][-0.0017617468 -0.0017448748 -0.0017549779 -0.0017822782 -0.0018099695 -0.0018241639 -0.0018200608 -0.0017906781 -0.0017472792 -0.0016998511 -0.0016900739 -0.001671252 -0.0016738621 -0.0016253772 -0.0015690299][-0.0016404933 -0.0016184243 -0.0016516674 -0.001714568 -0.0017764205 -0.0018143563 -0.0018241962 -0.0018038384 -0.0017629851 -0.0017111109 -0.0016711744 -0.0016254392 -0.0015641676 -0.0014693703 -0.0013343537][-0.0014525527 -0.001437044 -0.0015123519 -0.0016280834 -0.0017350384 -0.0018032488 -0.0018312898 -0.0018255298 -0.0017972997 -0.0017492098 -0.001683533 -0.0015843913 -0.0014507285 -0.0012654241 -0.0010372165][-0.0012460591 -0.0012524357 -0.0013814149 -0.0015553497 -0.0017041466 -0.0017959811 -0.0018346413 -0.0018391397 -0.001824341 -0.0017835043 -0.0017015354 -0.0015581951 -0.0013434374 -0.0010522886 -0.00070444657][-0.0010986147 -0.001133845 -0.0013108881 -0.0015287286 -0.0017006329 -0.0017999626 -0.0018367845 -0.0018418838 -0.0018305583 -0.0017918638 -0.0017035643 -0.0015379428 -0.001282329 -0.00089754781 -0.00041398453][-0.0010572231 -0.0011111281 -0.0013131518 -0.001549267 -0.0017222178 -0.0018134667 -0.0018403534 -0.0018429825 -0.0018330407 -0.0017978451 -0.0017132965 -0.0015515976 -0.0012790835 -0.00083659054 -0.00021920737][-0.0010887496 -0.0011433563 -0.0013519628 -0.0015869462 -0.0017503195 -0.0018274016 -0.0018437305 -0.0018443106 -0.0018362971 -0.0018082181 -0.0017410553 -0.0016037866 -0.00134048 -0.00086320669 -0.00013460102][-0.0011338007 -0.0011773335 -0.0013874029 -0.0016165868 -0.0017707491 -0.0018360399 -0.0018454272 -0.0018451425 -0.0018395602 -0.0018197456 -0.0017693451 -0.0016509246 -0.0013863556 -0.00087079185 -7.4599986e-05][-0.001161397 -0.0011963358 -0.0014113017 -0.0016348839 -0.0017814428 -0.00183719 -0.0018442514 -0.0018427905 -0.0018386932 -0.0018241046 -0.0017860089 -0.0016766338 -0.0013981455 -0.00083662616 2.1498767e-05]]...]
INFO - root - 2017-12-09 19:54:18.256393: step 53310, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.860 sec/batch; 66h:40m:00s remains)
INFO - root - 2017-12-09 19:54:26.846575: step 53320, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.890 sec/batch; 69h:02m:16s remains)
INFO - root - 2017-12-09 19:54:35.300201: step 53330, loss = 0.82, batch loss = 0.69 (9.8 examples/sec; 0.818 sec/batch; 63h:24m:59s remains)
INFO - root - 2017-12-09 19:54:43.577323: step 53340, loss = 0.82, batch loss = 0.69 (9.8 examples/sec; 0.819 sec/batch; 63h:30m:28s remains)
INFO - root - 2017-12-09 19:54:52.086411: step 53350, loss = 0.81, batch loss = 0.68 (9.3 examples/sec; 0.859 sec/batch; 66h:36m:05s remains)
INFO - root - 2017-12-09 19:55:00.756852: step 53360, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.852 sec/batch; 66h:03m:06s remains)
INFO - root - 2017-12-09 19:55:09.262837: step 53370, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.879 sec/batch; 68h:08m:36s remains)
INFO - root - 2017-12-09 19:55:17.830397: step 53380, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.859 sec/batch; 66h:33m:46s remains)
INFO - root - 2017-12-09 19:55:26.381670: step 53390, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.845 sec/batch; 65h:29m:57s remains)
INFO - root - 2017-12-09 19:55:35.096175: step 53400, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 67h:30m:24s remains)
2017-12-09 19:55:36.002835: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0018273361 -0.0018298361 -0.0018321257 -0.0018333539 -0.0018334201 -0.0018329727 -0.0018322843 -0.0018319072 -0.0018316566 -0.0018316208 -0.0018319541 -0.0018322625 -0.0018322091 -0.0018313843 -0.0018293046][-0.0018280018 -0.0018300494 -0.0018319167 -0.0018325049 -0.0018316462 -0.0018300877 -0.0018283911 -0.0018273671 -0.0018271913 -0.0018276918 -0.001828579 -0.0018292287 -0.0018293951 -0.0018286481 -0.0018267678][-0.0018280012 -0.0018296015 -0.0018306376 -0.0018301265 -0.0018276391 -0.0018241383 -0.001820594 -0.0018185378 -0.0018187636 -0.0018204997 -0.0018224954 -0.0018237989 -0.0018242955 -0.0018237594 -0.0018222951][-0.0018273871 -0.0018283869 -0.0018284403 -0.0018263775 -0.0018214417 -0.0018144445 -0.0018069554 -0.0018024216 -0.0018033863 -0.0018086381 -0.0018142841 -0.0018174706 -0.0018183675 -0.0018178764 -0.0018167469][-0.0018263883 -0.0018268579 -0.0018260236 -0.0018224008 -0.0018137961 -0.0017998435 -0.0017835355 -0.0017731233 -0.0017758097 -0.0017896731 -0.0018044406 -0.0018125434 -0.0018145911 -0.0018142466 -0.0018132307][-0.0018251245 -0.0018257046 -0.0018247059 -0.0018196787 -0.0018060511 -0.0017813114 -0.0017506268 -0.0017307596 -0.0017360166 -0.0017628872 -0.0017916154 -0.0018080897 -0.0018129367 -0.0018132011 -0.0018127665][-0.0018232369 -0.0018241863 -0.0018236489 -0.0018182023 -0.0018013399 -0.0017679757 -0.0017248766 -0.0016960964 -0.0017027431 -0.0017402228 -0.0017807472 -0.0018043941 -0.0018116031 -0.0018128162 -0.0018129763][-0.0018211035 -0.0018220345 -0.001822075 -0.001817678 -0.0018019258 -0.0017689277 -0.0017249514 -0.0016941129 -0.0016990978 -0.0017370646 -0.0017792109 -0.0018037778 -0.0018112027 -0.0018127129 -0.0018128671][-0.0018187887 -0.0018195229 -0.0018200802 -0.0018175808 -0.0018064686 -0.001781762 -0.0017473071 -0.0017217436 -0.0017246711 -0.0017541117 -0.0017867167 -0.0018051931 -0.0018107304 -0.0018119569 -0.0018118847][-0.0018163561 -0.0018168993 -0.00181789 -0.0018168644 -0.0018099871 -0.0017936549 -0.0017706057 -0.0017532816 -0.0017555063 -0.0017749752 -0.0017960928 -0.0018075712 -0.0018107588 -0.001811286 -0.0018108105][-0.0018142781 -0.0018144927 -0.0018156049 -0.0018156896 -0.0018124043 -0.0018033924 -0.0017900488 -0.0017799482 -0.0017812927 -0.0017924869 -0.0018038943 -0.0018095162 -0.0018106468 -0.0018104044 -0.0018097549][-0.0018125824 -0.0018122385 -0.0018131257 -0.0018135902 -0.0018124345 -0.0018083079 -0.001801712 -0.001796771 -0.0017977078 -0.0018031711 -0.0018081369 -0.0018100709 -0.0018100321 -0.0018095705 -0.0018089478][-0.0018112313 -0.0018104399 -0.0018108342 -0.0018113579 -0.0018118455 -0.0018118119 -0.0018110855 -0.0018101202 -0.0018097609 -0.0018099614 -0.0018100495 -0.0018098516 -0.0018094836 -0.0018090246 -0.0018085][-0.0018105127 -0.0018094839 -0.0018094179 -0.0018096187 -0.0018100085 -0.0018102867 -0.0018102869 -0.0018100876 -0.0018099487 -0.0018097953 -0.0018095842 -0.0018093009 -0.0018089712 -0.0018085734 -0.0018081624][-0.0018104154 -0.0018091695 -0.0018087497 -0.0018086525 -0.0018089211 -0.0018091176 -0.0018091595 -0.0018091016 -0.0018090702 -0.0018090477 -0.0018089833 -0.0018088004 -0.0018085535 -0.0018082364 -0.0018079489]]...]
INFO - root - 2017-12-09 19:55:44.566377: step 53410, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.858 sec/batch; 66h:29m:12s remains)
INFO - root - 2017-12-09 19:55:53.006989: step 53420, loss = 0.82, batch loss = 0.69 (9.7 examples/sec; 0.826 sec/batch; 63h:59m:56s remains)
INFO - root - 2017-12-09 19:56:01.717609: step 53430, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.884 sec/batch; 68h:31m:01s remains)
INFO - root - 2017-12-09 19:56:10.268061: step 53440, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.857 sec/batch; 66h:26m:31s remains)
INFO - root - 2017-12-09 19:56:18.938545: step 53450, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.872 sec/batch; 67h:34m:54s remains)
INFO - root - 2017-12-09 19:56:27.702544: step 53460, loss = 0.83, batch loss = 0.70 (9.1 examples/sec; 0.875 sec/batch; 67h:48m:10s remains)
INFO - root - 2017-12-09 19:56:36.178460: step 53470, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.898 sec/batch; 69h:34m:25s remains)
INFO - root - 2017-12-09 19:56:44.772051: step 53480, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.856 sec/batch; 66h:20m:22s remains)
INFO - root - 2017-12-09 19:56:53.415200: step 53490, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.858 sec/batch; 66h:29m:24s remains)
INFO - root - 2017-12-09 19:57:02.093112: step 53500, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 67h:12m:12s remains)
2017-12-09 19:57:02.977218: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0018361025 -0.0018353737 -0.0018349735 -0.0018347567 -0.0018347139 -0.0018347929 -0.0018349477 -0.0018350774 -0.0018351021 -0.0018350482 -0.0018349233 -0.0018347222 -0.0018345059 -0.0018343379 -0.0018342215][-0.0018370091 -0.0018361479 -0.0018356369 -0.0018353381 -0.0018352984 -0.0018354435 -0.0018356871 -0.0018358664 -0.0018358844 -0.0018357835 -0.0018355619 -0.0018352113 -0.0018348582 -0.001834634 -0.0018345596][-0.0018379822 -0.0018372365 -0.0018367927 -0.0018365489 -0.0018365924 -0.0018368554 -0.0018371961 -0.0018373774 -0.0018373386 -0.0018371275 -0.0018366887 -0.0018360696 -0.0018354693 -0.001835105 -0.0018349951][-0.0018385024 -0.0018380889 -0.0018378854 -0.001837885 -0.0018381518 -0.0018385993 -0.0018390521 -0.0018391951 -0.0018390608 -0.0018387113 -0.0018380352 -0.0018371261 -0.001836254 -0.0018356814 -0.0018353998][-0.0018382649 -0.0018381784 -0.0018382943 -0.0018386005 -0.0018390889 -0.0018396685 -0.0018402041 -0.0018403513 -0.0018401403 -0.0018397645 -0.0018390543 -0.0018380387 -0.0018370183 -0.0018362341 -0.0018357213][-0.0018377525 -0.0018378766 -0.0018382323 -0.0018387854 -0.0018393975 -0.0018400092 -0.0018405436 -0.0018407147 -0.0018404975 -0.0018401487 -0.0018395173 -0.0018385526 -0.0018374788 -0.0018365525 -0.001835841][-0.001837409 -0.001837659 -0.0018381537 -0.0018387067 -0.0018392102 -0.0018396648 -0.0018400707 -0.001840165 -0.0018398997 -0.0018395953 -0.0018390848 -0.001838272 -0.0018372686 -0.0018363403 -0.0018355959][-0.0018373028 -0.0018375162 -0.0018379615 -0.0018383251 -0.0018385329 -0.0018386726 -0.0018388499 -0.0018388324 -0.0018385327 -0.0018382985 -0.0018379226 -0.0018373046 -0.0018364983 -0.0018357044 -0.001835073][-0.0018370269 -0.001837127 -0.0018374857 -0.0018376781 -0.0018375762 -0.0018373759 -0.0018373058 -0.0018371829 -0.0018368415 -0.0018366755 -0.0018365078 -0.0018361415 -0.0018355689 -0.0018349816 -0.0018345464][-0.0018365184 -0.0018366133 -0.0018369278 -0.0018370346 -0.001836788 -0.0018363874 -0.0018361558 -0.001835941 -0.001835549 -0.0018353732 -0.0018353084 -0.001835053 -0.0018346173 -0.0018342518 -0.001834058][-0.0018361188 -0.0018362141 -0.0018364502 -0.0018365079 -0.0018362843 -0.0018359446 -0.0018357185 -0.0018354486 -0.0018350484 -0.0018348097 -0.0018346466 -0.0018343596 -0.0018339779 -0.0018337712 -0.0018337792][-0.0018361382 -0.0018361737 -0.0018362676 -0.0018362332 -0.001836072 -0.0018358496 -0.0018356208 -0.0018353012 -0.0018349082 -0.001834647 -0.0018343993 -0.0018340661 -0.0018337594 -0.0018336836 -0.0018338212][-0.0018361812 -0.0018361333 -0.0018361524 -0.0018361114 -0.0018360204 -0.00183589 -0.001835677 -0.001835346 -0.0018349728 -0.0018347009 -0.00183442 -0.0018341029 -0.0018338923 -0.001833884 -0.0018340244][-0.0018362623 -0.0018361172 -0.0018361078 -0.0018360828 -0.0018360087 -0.0018359066 -0.0018357011 -0.0018353682 -0.0018349964 -0.0018346795 -0.0018344083 -0.0018341666 -0.0018340442 -0.0018340526 -0.0018341349][-0.0018363857 -0.0018361076 -0.0018360068 -0.0018359507 -0.0018358561 -0.0018357232 -0.0018355018 -0.0018351817 -0.0018348286 -0.0018344974 -0.0018342303 -0.0018340519 -0.0018339852 -0.0018340264 -0.0018341013]]...]
INFO - root - 2017-12-09 19:57:11.566546: step 53510, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.839 sec/batch; 65h:03m:21s remains)
INFO - root - 2017-12-09 19:57:20.016521: step 53520, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.886 sec/batch; 68h:39m:45s remains)
INFO - root - 2017-12-09 19:57:28.784040: step 53530, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.861 sec/batch; 66h:41m:52s remains)
INFO - root - 2017-12-09 19:57:37.305598: step 53540, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 66h:48m:22s remains)
INFO - root - 2017-12-09 19:57:46.008883: step 53550, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 67h:30m:51s remains)
INFO - root - 2017-12-09 19:57:55.016689: step 53560, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.885 sec/batch; 68h:33m:11s remains)
INFO - root - 2017-12-09 19:58:03.549595: step 53570, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.842 sec/batch; 65h:15m:08s remains)
INFO - root - 2017-12-09 19:58:12.253994: step 53580, loss = 0.81, batch loss = 0.68 (9.2 examples/sec; 0.869 sec/batch; 67h:18m:58s remains)
INFO - root - 2017-12-09 19:58:20.872862: step 53590, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.865 sec/batch; 67h:01m:42s remains)
INFO - root - 2017-12-09 19:58:29.626349: step 53600, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.882 sec/batch; 68h:17m:52s remains)
2017-12-09 19:58:30.467952: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.080401123 0.091243155 0.10269298 0.1136448 0.12113209 0.1236956 0.12069762 0.11277028 0.10290998 0.091544807 0.079728812 0.0690214 0.060168918 0.053524338 0.047550861][0.084127709 0.09795554 0.11387403 0.13055044 0.14409654 0.15217178 0.15398544 0.14904183 0.13929097 0.12527838 0.10904415 0.092791371 0.078124896 0.066734254 0.057369865][0.089417472 0.10775291 0.12822755 0.14954808 0.16782066 0.18008144 0.18543817 0.18338928 0.17468607 0.16015474 0.14169475 0.12145109 0.10191614 0.085723042 0.072515763][0.09496215 0.11702135 0.14155146 0.16696081 0.18939921 0.20487379 0.21272875 0.21345598 0.20640343 0.19227034 0.17303148 0.15065284 0.1279041 0.10753559 0.090483189][0.10803713 0.13201988 0.15821624 0.18542433 0.2090787 0.22608794 0.23579516 0.2388394 0.2330979 0.21966866 0.20023857 0.17694239 0.15239082 0.12978849 0.11018792][0.13023114 0.15565659 0.18250957 0.20943671 0.23303747 0.2502346 0.26055494 0.26514205 0.26113185 0.24853987 0.22914305 0.20511037 0.17881969 0.15338328 0.13091955][0.15849592 0.18589309 0.21321981 0.23965628 0.26241603 0.27849734 0.2876313 0.29140219 0.28787589 0.27574673 0.25681725 0.23304872 0.20625947 0.17950433 0.15498552][0.19236094 0.2201532 0.24574102 0.26976919 0.29020393 0.30418387 0.31172207 0.31438449 0.31106454 0.30015907 0.28227738 0.25935274 0.23288085 0.20590147 0.18046737][0.22888798 0.25402388 0.2753678 0.29496711 0.31117862 0.32219455 0.32749563 0.32898659 0.32610103 0.31736124 0.30215746 0.28201696 0.25821123 0.23263685 0.2076676][0.26347658 0.28490421 0.30107185 0.31566662 0.32756582 0.33595559 0.33991536 0.34085533 0.33861497 0.33193612 0.31943125 0.30229211 0.28152907 0.25830871 0.23480549][0.29213288 0.31061298 0.32286152 0.33357295 0.34221634 0.34823453 0.35091597 0.35135704 0.349655 0.34492126 0.335258 0.32065353 0.30216298 0.280764 0.25729591][0.30861714 0.32579532 0.33579695 0.34408844 0.35054374 0.35474303 0.35662612 0.35683405 0.35557368 0.35220757 0.34499493 0.33277297 0.31696045 0.29811367 0.27559063][0.31572959 0.33047658 0.33714181 0.34208748 0.34584469 0.34898052 0.35153502 0.3531819 0.35391885 0.35358164 0.35002455 0.34143353 0.32839957 0.31233656 0.29181832][0.31560308 0.32767665 0.33118796 0.33364534 0.33587253 0.33729208 0.33881819 0.34099647 0.34266219 0.34507778 0.34543455 0.34202948 0.33455613 0.32302219 0.30617186][0.31495351 0.32532373 0.32711068 0.32725754 0.32801735 0.32768998 0.32779208 0.32801735 0.32819089 0.33065084 0.33181754 0.33134606 0.32838851 0.32228383 0.311394]]...]
INFO - root - 2017-12-09 19:58:39.086060: step 53610, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.887 sec/batch; 68h:41m:34s remains)
INFO - root - 2017-12-09 19:58:47.631394: step 53620, loss = 0.82, batch loss = 0.69 (8.8 examples/sec; 0.908 sec/batch; 70h:18m:06s remains)
INFO - root - 2017-12-09 19:58:56.376463: step 53630, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.875 sec/batch; 67h:46m:13s remains)
INFO - root - 2017-12-09 19:59:04.725116: step 53640, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.843 sec/batch; 65h:18m:17s remains)
INFO - root - 2017-12-09 19:59:13.352215: step 53650, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.857 sec/batch; 66h:21m:36s remains)
INFO - root - 2017-12-09 19:59:22.079208: step 53660, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 67h:29m:18s remains)
INFO - root - 2017-12-09 19:59:30.716974: step 53670, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.872 sec/batch; 67h:31m:59s remains)
INFO - root - 2017-12-09 19:59:39.447832: step 53680, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.887 sec/batch; 68h:41m:56s remains)
INFO - root - 2017-12-09 19:59:48.178320: step 53690, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.883 sec/batch; 68h:23m:37s remains)
INFO - root - 2017-12-09 19:59:56.960588: step 53700, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.872 sec/batch; 67h:33m:42s remains)
2017-12-09 19:59:57.825732: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.3817178 0.38848576 0.39058283 0.38499805 0.37516034 0.36185074 0.34703016 0.3301287 0.30966744 0.28495318 0.25656271 0.22559637 0.19525121 0.16769469 0.14482927][0.38108596 0.39012191 0.3956427 0.3945933 0.38798115 0.37637213 0.36354452 0.35001051 0.33312383 0.31177595 0.28632396 0.25732094 0.22756106 0.20011318 0.17731294][0.37546635 0.38704479 0.39636341 0.40049359 0.39899719 0.39150015 0.3817834 0.36971679 0.35436964 0.33601919 0.31343293 0.28749588 0.25985774 0.233587 0.21129213][0.36667618 0.38085717 0.39406425 0.40393329 0.40864864 0.4070403 0.4015834 0.39248917 0.37824684 0.36083603 0.339467 0.31581312 0.29063797 0.26740712 0.24750109][0.35828695 0.37674573 0.39365444 0.40756845 0.41677785 0.42031685 0.41962111 0.41403255 0.40264621 0.38684836 0.36689824 0.34528688 0.32203668 0.30081475 0.28248677][0.3504177 0.373183 0.39335689 0.41074711 0.423762 0.43210527 0.43666136 0.43549061 0.42752126 0.41406509 0.39560655 0.37548366 0.35452232 0.33597866 0.3191829][0.34657085 0.37290123 0.3948997 0.41432381 0.42949206 0.44080716 0.44868889 0.45135543 0.44769853 0.43823555 0.42297482 0.40464523 0.38506806 0.36817545 0.35196131][0.34798947 0.37678221 0.3987999 0.41714397 0.43103904 0.44285691 0.45207524 0.45690238 0.45571169 0.44922659 0.43759379 0.42178208 0.40394861 0.38836721 0.37316558][0.35602623 0.38511306 0.40450621 0.41995603 0.43074784 0.43986082 0.44781721 0.454243 0.45596862 0.45215872 0.44371065 0.43101206 0.41572782 0.40242881 0.38969362][0.36400321 0.39485818 0.413078 0.42503908 0.43079892 0.43503696 0.43947062 0.4441897 0.44692627 0.4457067 0.44029331 0.43058193 0.41772822 0.40554038 0.39429659][0.36391145 0.39473403 0.41095957 0.42191151 0.42664722 0.42886248 0.4310689 0.43388003 0.4362801 0.43506315 0.43081906 0.42332104 0.41312408 0.40261742 0.39373392][0.36317408 0.39269879 0.40594515 0.41411614 0.41659591 0.41718638 0.41805479 0.4200767 0.42226595 0.42159384 0.41865286 0.41312265 0.40460023 0.3957597 0.38916361][0.36267421 0.39144143 0.40332633 0.40940157 0.40990517 0.40809748 0.40705734 0.40734756 0.40842965 0.40769893 0.40557858 0.40212491 0.39592966 0.38888431 0.38398117][0.35488114 0.38325438 0.39514774 0.40078643 0.40154415 0.39980468 0.39931729 0.39942497 0.40107125 0.40201974 0.40114185 0.39923331 0.39481169 0.38951302 0.38656422][0.34248754 0.37170717 0.384797 0.39238182 0.39506525 0.39471531 0.39502037 0.39467946 0.39596286 0.39677402 0.39643922 0.3957904 0.39299911 0.38938707 0.38774356]]...]
INFO - root - 2017-12-09 20:00:06.536197: step 53710, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.883 sec/batch; 68h:21m:15s remains)
INFO - root - 2017-12-09 20:00:14.914201: step 53720, loss = 0.82, batch loss = 0.69 (9.7 examples/sec; 0.827 sec/batch; 64h:00m:55s remains)
INFO - root - 2017-12-09 20:00:23.766540: step 53730, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.857 sec/batch; 66h:22m:14s remains)
INFO - root - 2017-12-09 20:00:32.450999: step 53740, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 67h:06m:34s remains)
INFO - root - 2017-12-09 20:00:41.146508: step 53750, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.872 sec/batch; 67h:28m:57s remains)
INFO - root - 2017-12-09 20:00:49.926025: step 53760, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 67h:27m:18s remains)
INFO - root - 2017-12-09 20:00:58.649653: step 53770, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.860 sec/batch; 66h:36m:21s remains)
INFO - root - 2017-12-09 20:01:07.415954: step 53780, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.866 sec/batch; 67h:02m:29s remains)
INFO - root - 2017-12-09 20:01:16.215721: step 53790, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.884 sec/batch; 68h:26m:03s remains)
INFO - root - 2017-12-09 20:01:24.868216: step 53800, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 66h:44m:13s remains)
2017-12-09 20:01:25.757305: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0018176892 -0.0018193307 -0.0018163567 -0.0017833019 -0.0016955375 -0.0015613013 -0.0014262326 -0.0013489651 -0.0013605113 -0.0014467975 -0.0015705547 -0.0016870531 -0.0017657792 -0.0018027473 -0.0018126367][-0.0018167787 -0.0018177374 -0.0018051384 -0.0017319032 -0.0015516357 -0.0012755791 -0.00099408033 -0.00082012732 -0.00081997365 -0.00097830652 -0.0012296608 -0.001484987 -0.0016737371 -0.0017736529 -0.0018081325][-0.0018164674 -0.0018155691 -0.00178125 -0.0016343095 -0.0012960583 -0.00077267888 -0.00022159482 0.00014161796 0.00018101058 -9.6056843e-05 -0.00057656819 -0.0010900234 -0.0014869883 -0.0017096351 -0.00179465][-0.0018161311 -0.0018120493 -0.0017455369 -0.0014921788 -0.0009267088 -6.9780508e-05 0.00083888124 0.0014616904 0.001586425 0.0011955294 0.00043544674 -0.00042986672 -0.001142934 -0.0015741291 -0.0017592144][-0.0018160375 -0.001811379 -0.0017241819 -0.0013962196 -0.00065849989 0.00047129195 0.0017009802 0.0025914432 0.0028383662 0.002388671 0.0014092958 0.00023937423 -0.00076595438 -0.001407113 -0.0017049643][-0.0018151766 -0.0018121449 -0.0017395101 -0.0014352896 -0.00070293166 0.00047110382 0.0018085282 0.0028508189 0.0032317932 0.0028416254 0.0018317698 0.00056497043 -0.00055840588 -0.0013016068 -0.0016639845][-0.0018138613 -0.0018116623 -0.0017711932 -0.0015733879 -0.0010463102 -0.00013442652 0.000981118 0.0019287228 0.0023534461 0.0021076733 0.0013007416 0.00024645205 -0.00070493808 -0.0013461667 -0.0016673406][-0.0018134437 -0.0018118375 -0.0017928875 -0.0016913535 -0.0013956031 -0.00084016845 -0.00010891457 0.0005557033 0.00088613282 0.00074594317 0.0002107959 -0.00048603769 -0.0011026105 -0.0015106166 -0.0017129915][-0.001813342 -0.0018122526 -0.0018077671 -0.0017742504 -0.0016561671 -0.0014054279 -0.0010416391 -0.00068887568 -0.00050384586 -0.00057278189 -0.00085062208 -0.0012000476 -0.001494333 -0.0016778726 -0.0017639326][-0.0018128245 -0.0018122834 -0.0018117492 -0.0018027299 -0.0017649933 -0.0016779661 -0.0015440634 -0.0014110722 -0.0013451157 -0.0013793046 -0.0014902649 -0.0016182114 -0.0017165064 -0.0017709896 -0.0017936387][-0.0018118832 -0.0018112113 -0.0018115431 -0.0018104137 -0.0018029872 -0.001784382 -0.0017549756 -0.0017259356 -0.0017120048 -0.0017198102 -0.0017435045 -0.0017699306 -0.0017897036 -0.001800105 -0.0018043056][-0.0018113654 -0.0018103168 -0.0018103289 -0.0018105856 -0.0018106127 -0.001810164 -0.0018092145 -0.0018082352 -0.0018077188 -0.0018074537 -0.0018075743 -0.0018077614 -0.0018080766 -0.0018082508 -0.0018082206][-0.0018112597 -0.0018099243 -0.0018096595 -0.0018096551 -0.0018097513 -0.001809831 -0.001809822 -0.0018097919 -0.0018098791 -0.0018100088 -0.0018101878 -0.0018102621 -0.001810356 -0.00181027 -0.0018097354][-0.0018116315 -0.0018103498 -0.0018097826 -0.0018095899 -0.0018095442 -0.0018096194 -0.0018096762 -0.0018097487 -0.001809779 -0.0018098524 -0.0018099975 -0.0018102606 -0.0018105445 -0.0018106326 -0.0018101411][-0.001811724 -0.0018107865 -0.0018103328 -0.0018100955 -0.0018100149 -0.0018100026 -0.0018100131 -0.0018100415 -0.0018099665 -0.0018098726 -0.0018098394 -0.001809969 -0.0018101453 -0.0018101955 -0.0018098676]]...]
INFO - root - 2017-12-09 20:01:34.395079: step 53810, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.874 sec/batch; 67h:40m:25s remains)
INFO - root - 2017-12-09 20:01:42.777220: step 53820, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.865 sec/batch; 66h:58m:13s remains)
INFO - root - 2017-12-09 20:01:51.387814: step 53830, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.895 sec/batch; 69h:15m:58s remains)
INFO - root - 2017-12-09 20:01:59.986013: step 53840, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.858 sec/batch; 66h:27m:01s remains)
INFO - root - 2017-12-09 20:02:08.532960: step 53850, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.841 sec/batch; 65h:05m:01s remains)
INFO - root - 2017-12-09 20:02:17.137793: step 53860, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.845 sec/batch; 65h:25m:18s remains)
INFO - root - 2017-12-09 20:02:25.663293: step 53870, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.854 sec/batch; 66h:06m:05s remains)
INFO - root - 2017-12-09 20:02:34.162832: step 53880, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.875 sec/batch; 67h:43m:27s remains)
INFO - root - 2017-12-09 20:02:42.753657: step 53890, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.844 sec/batch; 65h:20m:08s remains)
INFO - root - 2017-12-09 20:02:51.507805: step 53900, loss = 0.82, batch loss = 0.69 (10.0 examples/sec; 0.804 sec/batch; 62h:12m:03s remains)
2017-12-09 20:02:52.423810: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.321317 0.31665915 0.31116691 0.30550307 0.29899305 0.29402006 0.28826222 0.2796042 0.26957521 0.25937989 0.24941418 0.23968694 0.23149581 0.2253468 0.21978123][0.35553309 0.35162368 0.34596127 0.33993047 0.33296791 0.32699794 0.31997451 0.3122333 0.30361688 0.29322726 0.28409508 0.27578303 0.26891652 0.26325342 0.25794947][0.39574465 0.39372128 0.38878846 0.38329703 0.37679905 0.37091124 0.36400878 0.35781491 0.35140488 0.34281465 0.33380514 0.3258684 0.318541 0.31111586 0.30393338][0.43984661 0.43997225 0.436077 0.43053377 0.42406029 0.4178642 0.41146868 0.40588912 0.40064719 0.39441106 0.38589177 0.37824315 0.37044144 0.36260986 0.35440004][0.47869682 0.48143753 0.47843367 0.47370183 0.46926916 0.46419996 0.45999593 0.45527413 0.45157823 0.44675925 0.43831062 0.43043423 0.42125955 0.41311365 0.40366182][0.512038 0.51734358 0.51525646 0.51184064 0.50902325 0.50547123 0.50334466 0.50019604 0.49747226 0.49330223 0.48506179 0.47621825 0.46566656 0.45656961 0.44612789][0.53434211 0.54278016 0.54106748 0.53869772 0.5372591 0.53507262 0.534679 0.53333932 0.53260922 0.5303157 0.52352643 0.5145095 0.50310951 0.49230015 0.48036441][0.55050743 0.56023711 0.5576871 0.55488265 0.55291218 0.55117244 0.552248 0.55277079 0.553464 0.55345482 0.54913485 0.54005176 0.52807575 0.51617384 0.50326407][0.55655813 0.56859511 0.56561446 0.56250435 0.56001854 0.55769527 0.55845225 0.5595004 0.56125665 0.56259871 0.56090128 0.55450708 0.54425 0.53382212 0.52121252][0.554216 0.56853467 0.56633323 0.56485879 0.56383967 0.56199312 0.5630393 0.56443161 0.5670194 0.567415 0.56629735 0.56127834 0.55231953 0.54222649 0.53024238][0.54891151 0.56373721 0.56145751 0.55993527 0.55858463 0.55696416 0.55784029 0.56017625 0.56334549 0.56455415 0.56450033 0.560383 0.55253357 0.54232883 0.53069383][0.53696311 0.55243713 0.55069941 0.54972363 0.54872078 0.54738635 0.54799032 0.55135179 0.5557723 0.5579865 0.5592609 0.55562335 0.54835385 0.5375008 0.52580404][0.52510494 0.54080707 0.53993475 0.53979075 0.53926516 0.53801829 0.53725219 0.53982592 0.54343677 0.54434425 0.54456455 0.54130989 0.53537571 0.52566308 0.51535577][0.50856739 0.52545965 0.52640921 0.52725583 0.527711 0.52674389 0.52525949 0.52581871 0.52752256 0.52751619 0.52686292 0.52453572 0.52028209 0.51265413 0.50498021][0.48974821 0.50567412 0.50768125 0.50974762 0.511401 0.51176363 0.51077241 0.51088667 0.51101106 0.50958037 0.50765139 0.50451463 0.50073135 0.49414417 0.48849523]]...]
INFO - root - 2017-12-09 20:03:01.118446: step 53910, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.858 sec/batch; 66h:23m:53s remains)
INFO - root - 2017-12-09 20:03:09.584827: step 53920, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.870 sec/batch; 67h:19m:51s remains)
INFO - root - 2017-12-09 20:03:18.178867: step 53930, loss = 0.82, batch loss = 0.70 (9.2 examples/sec; 0.873 sec/batch; 67h:33m:29s remains)
INFO - root - 2017-12-09 20:03:26.537991: step 53940, loss = 0.82, batch loss = 0.69 (10.6 examples/sec; 0.756 sec/batch; 58h:27m:44s remains)
INFO - root - 2017-12-09 20:03:35.140948: step 53950, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.851 sec/batch; 65h:49m:32s remains)
INFO - root - 2017-12-09 20:03:43.881623: step 53960, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.876 sec/batch; 67h:46m:41s remains)
INFO - root - 2017-12-09 20:03:52.802078: step 53970, loss = 0.82, batch loss = 0.69 (8.2 examples/sec; 0.981 sec/batch; 75h:56m:12s remains)
INFO - root - 2017-12-09 20:04:01.341312: step 53980, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 67h:12m:28s remains)
INFO - root - 2017-12-09 20:04:09.932076: step 53990, loss = 0.83, batch loss = 0.70 (9.3 examples/sec; 0.864 sec/batch; 66h:50m:24s remains)
INFO - root - 2017-12-09 20:04:18.450081: step 54000, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.843 sec/batch; 65h:13m:02s remains)
2017-12-09 20:04:19.282089: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.0996524 0.10114248 0.10254967 0.10367237 0.10458423 0.10469208 0.10401009 0.10220151 0.099394545 0.095380984 0.090763845 0.086640008 0.0827175 0.07867004 0.075343058][0.1033552 0.10474169 0.10602933 0.10692504 0.1074273 0.10714829 0.10614622 0.10445699 0.10169953 0.0977541 0.0934222 0.08958099 0.08579766 0.081511527 0.077980995][0.10485693 0.10581324 0.10611678 0.10610453 0.10569645 0.10488657 0.10382424 0.10230118 0.10048937 0.097701631 0.094398141 0.091161489 0.087545887 0.083376028 0.079660319][0.10639342 0.1067884 0.1063624 0.10578956 0.10494553 0.10339034 0.10169126 0.10032976 0.098655507 0.09668041 0.094176441 0.09186469 0.089126363 0.08524292 0.081846014][0.10897008 0.10908599 0.10773397 0.10617491 0.10411644 0.10171654 0.099325716 0.097641923 0.096380696 0.095158525 0.093755409 0.092396334 0.090715826 0.087601662 0.084595136][0.1119715 0.11181902 0.10939793 0.10678477 0.1033104 0.0999567 0.096959032 0.095185891 0.09420751 0.093801446 0.093892373 0.093338229 0.092308283 0.089905635 0.087507255][0.11452996 0.11406294 0.1108574 0.10711185 0.1024135 0.098108456 0.094467647 0.09257032 0.09172006 0.092011862 0.093253866 0.093777515 0.093891919 0.092385 0.090881921][0.11596048 0.11548039 0.11164122 0.1073209 0.10223229 0.097418785 0.093399741 0.091105886 0.0903189 0.091048665 0.092924871 0.093944982 0.094816513 0.0942351 0.09339074][0.1167188 0.11722674 0.11394618 0.10931616 0.10395681 0.098466 0.0937042 0.090939991 0.089584671 0.090346836 0.0925416 0.094019175 0.095668547 0.095981874 0.095810108][0.11393382 0.11528959 0.11274613 0.10886547 0.10428488 0.099507481 0.09516909 0.092145465 0.090487786 0.090646222 0.092479289 0.093845613 0.09555281 0.096515566 0.096743658][0.10934469 0.11124219 0.10928803 0.1062158 0.1028319 0.099334039 0.096165515 0.093801059 0.092382662 0.092296176 0.093519837 0.094469689 0.095899083 0.0967647 0.097142771][0.1029672 0.10524688 0.10423844 0.10278014 0.10157953 0.099591926 0.097723626 0.096103676 0.094940655 0.094303623 0.09448418 0.095123708 0.096332915 0.097097464 0.097487][0.097437866 0.10004462 0.0998543 0.1000141 0.10079075 0.10080056 0.10069279 0.09990444 0.099173613 0.0982508 0.097732745 0.097566232 0.098081015 0.098413669 0.098481357][0.093595639 0.096598044 0.097270876 0.098617017 0.10064117 0.10196157 0.10302223 0.10281088 0.10219488 0.10129251 0.10027237 0.099967122 0.10027061 0.10049909 0.10058367][0.091735773 0.094497353 0.0953122 0.097707205 0.10106198 0.10413609 0.10685665 0.10777453 0.10788019 0.10687871 0.10531756 0.104362 0.1039484 0.10362296 0.10340004]]...]
INFO - root - 2017-12-09 20:04:27.774667: step 54010, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.847 sec/batch; 65h:30m:16s remains)
INFO - root - 2017-12-09 20:04:36.280270: step 54020, loss = 0.82, batch loss = 0.69 (11.1 examples/sec; 0.724 sec/batch; 55h:58m:26s remains)
INFO - root - 2017-12-09 20:04:44.957965: step 54030, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.866 sec/batch; 67h:01m:21s remains)
INFO - root - 2017-12-09 20:04:53.532399: step 54040, loss = 0.81, batch loss = 0.68 (9.6 examples/sec; 0.834 sec/batch; 64h:29m:39s remains)
INFO - root - 2017-12-09 20:05:02.231987: step 54050, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.880 sec/batch; 68h:03m:12s remains)
INFO - root - 2017-12-09 20:05:10.813498: step 54060, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.874 sec/batch; 67h:37m:46s remains)
INFO - root - 2017-12-09 20:05:19.465889: step 54070, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.861 sec/batch; 66h:35m:51s remains)
INFO - root - 2017-12-09 20:05:27.987757: step 54080, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.873 sec/batch; 67h:32m:58s remains)
INFO - root - 2017-12-09 20:05:36.683913: step 54090, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.889 sec/batch; 68h:44m:05s remains)
INFO - root - 2017-12-09 20:05:45.333593: step 54100, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 66h:41m:43s remains)
2017-12-09 20:05:46.289947: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0018007525 -0.001798905 -0.0017985755 -0.0017984465 -0.00179833 -0.0017982265 -0.0017980717 -0.0017978505 -0.0017974897 -0.0017972502 -0.0017971632 -0.0017973332 -0.0017977941 -0.0017987231 -0.0018001231][-0.0017993962 -0.0017972344 -0.0017967046 -0.0017965474 -0.0017966137 -0.0017968451 -0.0017970584 -0.0017971618 -0.0017970469 -0.0017969463 -0.0017969396 -0.0017971084 -0.0017974913 -0.0017982472 -0.0017994245][-0.001799435 -0.0017970358 -0.0017962703 -0.0017960434 -0.0017961939 -0.0017966337 -0.0017971103 -0.0017974705 -0.0017975285 -0.0017974885 -0.0017974952 -0.0017975931 -0.0017978234 -0.0017983293 -0.0017991571][-0.0017996901 -0.0017970945 -0.0017961167 -0.0017957859 -0.001795953 -0.0017965018 -0.0017971257 -0.001797599 -0.0017977593 -0.0017978009 -0.0017978362 -0.0017979034 -0.0017980326 -0.0017983119 -0.0017987727][-0.0017999944 -0.0017972523 -0.0017961458 -0.0017956817 -0.0017957665 -0.0017962778 -0.0017968962 -0.0017973664 -0.0017975653 -0.0017976535 -0.0017977339 -0.0017978302 -0.0017979308 -0.0017980814 -0.0017982585][-0.0018003901 -0.0017975728 -0.0017963892 -0.0017958032 -0.0017957096 -0.001796013 -0.0017964857 -0.0017968827 -0.0017970903 -0.0017972236 -0.0017973824 -0.0017975484 -0.001797657 -0.0017977317 -0.0017976911][-0.0018005932 -0.0017978121 -0.0017965627 -0.0017958462 -0.0017955449 -0.00179563 -0.0017959578 -0.0017962897 -0.0017965271 -0.0017967576 -0.0017970204 -0.0017972499 -0.0017973674 -0.0017973848 -0.0017971975][-0.0018005599 -0.0017978452 -0.0017965989 -0.0017957909 -0.0017953231 -0.0017952492 -0.0017954455 -0.0017957166 -0.001796002 -0.00179633 -0.001796691 -0.0017969637 -0.0017971103 -0.0017971377 -0.0017969056][-0.001800187 -0.0017976217 -0.0017965087 -0.0017957005 -0.0017951587 -0.0017949681 -0.0017950396 -0.0017952684 -0.0017956038 -0.0017960141 -0.0017964788 -0.0017968224 -0.0017970037 -0.0017970538 -0.0017968445][-0.0017994447 -0.001797143 -0.0017962235 -0.0017955017 -0.0017949695 -0.0017947223 -0.0017947371 -0.0017949693 -0.0017953715 -0.0017958826 -0.0017964677 -0.0017969285 -0.0017971743 -0.0017972334 -0.0017970451][-0.0017985143 -0.0017964228 -0.0017956168 -0.0017950187 -0.0017945606 -0.0017943447 -0.0017943918 -0.0017946794 -0.0017952051 -0.0017958815 -0.0017966438 -0.0017972525 -0.0017975701 -0.0017975993 -0.0017973523][-0.0017976611 -0.0017956333 -0.0017949147 -0.0017944138 -0.0017940535 -0.0017939359 -0.0017940587 -0.0017944539 -0.001795161 -0.0017960551 -0.0017970029 -0.0017977243 -0.0017980575 -0.0017980167 -0.0017976806][-0.0017969458 -0.0017949531 -0.0017943032 -0.0017939331 -0.0017936814 -0.0017936503 -0.0017938673 -0.0017943905 -0.0017952443 -0.0017962784 -0.0017973274 -0.0017980824 -0.0017983855 -0.0017982317 -0.001797763][-0.0017965998 -0.0017946389 -0.001794086 -0.0017938367 -0.0017936742 -0.0017936873 -0.0017939307 -0.0017944769 -0.0017953243 -0.0017963392 -0.0017973718 -0.0017980945 -0.0017983435 -0.0017981025 -0.001797576][-0.0017967749 -0.0017949089 -0.0017944011 -0.0017942498 -0.0017941404 -0.001794182 -0.0017944588 -0.001795009 -0.0017958229 -0.0017967924 -0.0017977755 -0.0017984456 -0.0017986195 -0.0017983142 -0.0017977637]]...]
INFO - root - 2017-12-09 20:05:54.828318: step 54110, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.839 sec/batch; 64h:51m:50s remains)
INFO - root - 2017-12-09 20:06:03.452223: step 54120, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.858 sec/batch; 66h:19m:42s remains)
INFO - root - 2017-12-09 20:06:11.934652: step 54130, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 66h:50m:36s remains)
INFO - root - 2017-12-09 20:06:20.296323: step 54140, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.854 sec/batch; 66h:00m:09s remains)
INFO - root - 2017-12-09 20:06:28.925193: step 54150, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.849 sec/batch; 65h:36m:38s remains)
INFO - root - 2017-12-09 20:06:37.523096: step 54160, loss = 0.83, batch loss = 0.70 (9.0 examples/sec; 0.888 sec/batch; 68h:40m:53s remains)
INFO - root - 2017-12-09 20:06:46.150177: step 54170, loss = 0.82, batch loss = 0.69 (9.8 examples/sec; 0.819 sec/batch; 63h:19m:03s remains)
INFO - root - 2017-12-09 20:06:54.676519: step 54180, loss = 0.82, batch loss = 0.69 (9.7 examples/sec; 0.827 sec/batch; 63h:57m:38s remains)
INFO - root - 2017-12-09 20:07:03.398339: step 54190, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.860 sec/batch; 66h:27m:29s remains)
INFO - root - 2017-12-09 20:07:11.991883: step 54200, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.876 sec/batch; 67h:45m:02s remains)
2017-12-09 20:07:12.894949: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.21759899 0.23906441 0.26337165 0.28773516 0.30947495 0.326715 0.33654207 0.33844638 0.33248633 0.3198342 0.30183 0.28066054 0.26026452 0.24282847 0.22884735][0.22874199 0.250728 0.27466795 0.29852787 0.31987208 0.33654159 0.34606355 0.34788433 0.34128132 0.32648057 0.30496925 0.27884942 0.25204289 0.22763853 0.20599681][0.23175469 0.25480717 0.27890712 0.30274209 0.32414582 0.34120402 0.35156053 0.3541486 0.34787378 0.3324517 0.30847633 0.27817571 0.245117 0.21307074 0.18275711][0.23127171 0.25632438 0.28225073 0.30791596 0.33124843 0.3500016 0.36190897 0.36586308 0.36020532 0.34442347 0.31844071 0.28421688 0.24507323 0.20518275 0.16641919][0.22549352 0.25246412 0.27994066 0.30760831 0.33311319 0.35377848 0.36749882 0.37284043 0.36842582 0.35306388 0.32623708 0.2896041 0.24596807 0.19994245 0.15403737][0.21513365 0.24316956 0.27118278 0.29914096 0.32472157 0.34573421 0.35980785 0.36561251 0.36203542 0.34754598 0.32120755 0.28377175 0.23796038 0.18893148 0.13943443][0.19957434 0.2274804 0.25436637 0.28059018 0.30392727 0.32248893 0.33463544 0.33964247 0.33687624 0.32403734 0.300181 0.26489353 0.22040984 0.17210868 0.12305769][0.18179587 0.2070739 0.229981 0.25150415 0.26974666 0.28385472 0.29263744 0.29587272 0.29359013 0.28341019 0.26387545 0.2333183 0.19365962 0.14974453 0.10491624][0.16455305 0.1849847 0.20124352 0.21503456 0.22523363 0.23244408 0.23608202 0.23694463 0.23525479 0.22827126 0.21457179 0.19108932 0.15914424 0.12278961 0.085227244][0.148719 0.16347824 0.17166224 0.1763317 0.17755029 0.17660995 0.17421873 0.17168139 0.16959746 0.16540422 0.15689406 0.14086656 0.11801055 0.091045395 0.062929608][0.1345285 0.14373125 0.14440338 0.14091615 0.13379897 0.12580433 0.11826263 0.11272265 0.1099333 0.10724544 0.10241035 0.092270754 0.077111557 0.059047572 0.040329259][0.12382215 0.12796053 0.12245699 0.1123737 0.098835289 0.085208133 0.073476434 0.065453433 0.061736479 0.059714351 0.057203997 0.051492982 0.042627461 0.031882934 0.021032523][0.11693868 0.11683797 0.10663157 0.091676578 0.073600851 0.056127794 0.041527521 0.031670436 0.027083671 0.02538728 0.024315652 0.021703083 0.017579528 0.012587731 0.0077169272][0.11309379 0.11009864 0.096936993 0.079260729 0.058777392 0.039166655 0.022950785 0.012265219 0.0074022762 0.0059177615 0.0055306344 0.0047204234 0.0034345821 0.0019457497 0.00060218468][0.11224993 0.10794909 0.093409337 0.074491583 0.053123988 0.032955684 0.016343344 0.0054515847 0.000535437 -0.00092269876 -0.0011442183 -0.0012734382 -0.0014091528 -0.0015337694 -0.0016127394]]...]
INFO - root - 2017-12-09 20:07:21.518010: step 54210, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.897 sec/batch; 69h:18m:08s remains)
INFO - root - 2017-12-09 20:07:30.101634: step 54220, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.866 sec/batch; 66h:55m:46s remains)
INFO - root - 2017-12-09 20:07:38.671468: step 54230, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.865 sec/batch; 66h:52m:30s remains)
INFO - root - 2017-12-09 20:07:47.260924: step 54240, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.865 sec/batch; 66h:49m:29s remains)
INFO - root - 2017-12-09 20:07:56.062009: step 54250, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.886 sec/batch; 68h:29m:18s remains)
INFO - root - 2017-12-09 20:08:04.639072: step 54260, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.841 sec/batch; 64h:59m:42s remains)
INFO - root - 2017-12-09 20:08:13.162015: step 54270, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.830 sec/batch; 64h:10m:41s remains)
INFO - root - 2017-12-09 20:08:21.595021: step 54280, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.840 sec/batch; 64h:56m:27s remains)
INFO - root - 2017-12-09 20:08:30.220572: step 54290, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.849 sec/batch; 65h:36m:46s remains)
INFO - root - 2017-12-09 20:08:38.641000: step 54300, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.835 sec/batch; 64h:29m:19s remains)
2017-12-09 20:08:39.617954: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.14025801 0.16481099 0.18891795 0.20930205 0.22262162 0.2293485 0.23072089 0.22838755 0.22318803 0.21688688 0.20843166 0.19994631 0.19106518 0.18365678 0.17969598][0.13887949 0.16392137 0.18783602 0.20862451 0.2220888 0.22902706 0.2304153 0.22879268 0.22509187 0.22043209 0.21466419 0.20825216 0.20085628 0.19300224 0.18690659][0.13439304 0.15887919 0.18213964 0.20258746 0.2162763 0.2242174 0.22639033 0.22541992 0.22334366 0.22081915 0.21752553 0.21285775 0.20685261 0.19940612 0.19107497][0.12842549 0.15171306 0.17392407 0.19392233 0.20837808 0.21727021 0.22077854 0.22117198 0.22018687 0.21983033 0.21891993 0.21620658 0.21161614 0.20426995 0.19442242][0.12077191 0.14289123 0.16400114 0.18342583 0.19817056 0.20863527 0.21407931 0.21610014 0.21673195 0.21786092 0.21823874 0.21723609 0.2134399 0.20627718 0.19516122][0.11414156 0.1349041 0.15499465 0.17401537 0.18886967 0.20006493 0.20673928 0.21032174 0.21249691 0.21496089 0.21662204 0.21645665 0.21320421 0.20619686 0.19416545][0.10803864 0.12757325 0.14722665 0.1662465 0.1817589 0.19349535 0.2006617 0.20522399 0.20800267 0.21128535 0.2136413 0.21429279 0.2120042 0.20549865 0.19323272][0.10339106 0.12236747 0.14196149 0.16074495 0.17655663 0.18867986 0.19607526 0.20075323 0.2034391 0.20661962 0.20882842 0.20949797 0.20761979 0.20209819 0.19054793][0.099768057 0.11836486 0.13760352 0.15609895 0.17198236 0.18407747 0.19200706 0.19701619 0.19962569 0.20238402 0.20415504 0.20471647 0.2026307 0.19765678 0.18761113][0.097022675 0.11531609 0.13420659 0.15273429 0.168586 0.18089788 0.1891548 0.19440214 0.19711316 0.19901265 0.19995019 0.19955543 0.19691177 0.19246553 0.18404125][0.0937267 0.11209711 0.13074474 0.14944975 0.16576274 0.17794128 0.18602642 0.19104742 0.19343597 0.19433253 0.19435941 0.19323342 0.19083242 0.18707451 0.18049206][0.090128928 0.10834181 0.1267529 0.14553383 0.16194738 0.17418069 0.18165028 0.18601042 0.18775566 0.18787484 0.1875319 0.18602572 0.18432914 0.18168913 0.17703636][0.08539816 0.10311486 0.12093455 0.13922498 0.15559514 0.16777788 0.17481267 0.17833537 0.17924483 0.17885908 0.17817937 0.17693156 0.17588744 0.17448458 0.17178039][0.079588428 0.095620848 0.11209447 0.12951492 0.14529049 0.1575208 0.16504225 0.16880375 0.16968834 0.16912396 0.16827929 0.16724576 0.16668175 0.16595067 0.16466643][0.073971257 0.088240527 0.1029559 0.11895403 0.13364838 0.14535341 0.15252294 0.15667956 0.15808405 0.15766561 0.15705305 0.156368 0.15644872 0.15627477 0.15579955]]...]
INFO - root - 2017-12-09 20:08:48.265761: step 54310, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 66h:45m:06s remains)
INFO - root - 2017-12-09 20:08:56.876215: step 54320, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.847 sec/batch; 65h:25m:26s remains)
INFO - root - 2017-12-09 20:09:05.260481: step 54330, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.830 sec/batch; 64h:09m:08s remains)
INFO - root - 2017-12-09 20:09:13.900335: step 54340, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.891 sec/batch; 68h:48m:41s remains)
INFO - root - 2017-12-09 20:09:22.652222: step 54350, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.848 sec/batch; 65h:32m:52s remains)
INFO - root - 2017-12-09 20:09:31.432975: step 54360, loss = 0.82, batch loss = 0.69 (8.0 examples/sec; 1.003 sec/batch; 77h:27m:19s remains)
INFO - root - 2017-12-09 20:09:40.216707: step 54370, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.875 sec/batch; 67h:35m:01s remains)
INFO - root - 2017-12-09 20:09:48.743937: step 54380, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.887 sec/batch; 68h:31m:59s remains)
INFO - root - 2017-12-09 20:09:57.347415: step 54390, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.838 sec/batch; 64h:45m:36s remains)
INFO - root - 2017-12-09 20:10:05.780211: step 54400, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.846 sec/batch; 65h:19m:27s remains)
2017-12-09 20:10:06.657012: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0018108709 -0.0018108544 -0.0018127844 -0.0018154321 -0.0018180032 -0.0018201462 -0.0018211722 -0.0018208853 -0.0018181873 -0.0018117558 -0.0018025278 -0.0017898135 -0.0017757158 -0.0017584234 -0.0017389501][-0.0018095718 -0.0018100347 -0.0018125366 -0.0018161004 -0.0018196652 -0.0018225461 -0.0018244665 -0.0018251545 -0.0018249239 -0.0018237743 -0.0018220787 -0.0018195487 -0.0018159195 -0.00181122 -0.0018032667][-0.0018095239 -0.0018102943 -0.0018131292 -0.0018170133 -0.0018205539 -0.0018231792 -0.0018249492 -0.0018258215 -0.0018260516 -0.0018253903 -0.0018238816 -0.0018217772 -0.0018192417 -0.0018169116 -0.0018150012][-0.0018094151 -0.0018103691 -0.0018134133 -0.001817229 -0.0018199403 -0.0018213227 -0.001822231 -0.0018232431 -0.0018243702 -0.001824741 -0.0018239639 -0.0018220994 -0.0018195293 -0.0018171869 -0.0018156493][-0.0018093293 -0.0018100261 -0.0018127355 -0.0018156196 -0.0018157872 -0.0018136742 -0.001812437 -0.0018144882 -0.0018186198 -0.0018216057 -0.0018225536 -0.001821637 -0.0018195816 -0.0018177475 -0.0018169987][-0.0018092891 -0.0018093917 -0.0018111016 -0.0018115576 -0.0018072479 -0.0017995579 -0.0017949392 -0.0017987443 -0.0018077927 -0.0018153524 -0.0018189324 -0.0018192946 -0.0018182177 -0.0018174123 -0.0018177758][-0.0018092097 -0.0018087815 -0.0018093048 -0.0018066262 -0.0017952716 -0.0017779588 -0.0017663295 -0.0017712543 -0.0017877257 -0.0018028937 -0.0018116358 -0.0018148873 -0.0018154379 -0.0018158259 -0.0018172623][-0.0018091324 -0.0018084366 -0.0018084676 -0.0018048475 -0.0017911871 -0.0017694747 -0.0017536493 -0.00175754 -0.0017759647 -0.0017939933 -0.0018050235 -0.0018097692 -0.001811602 -0.0018131215 -0.0018152603][-0.0018091648 -0.0018078841 -0.0018081415 -0.0018057792 -0.001795481 -0.0017777424 -0.0017628442 -0.0017632573 -0.0017770656 -0.0017922744 -0.0018019392 -0.0018065507 -0.0018086486 -0.0018103857 -0.0018123039][-0.0018089443 -0.0018073532 -0.0018078053 -0.0018074198 -0.001804163 -0.001797146 -0.00179027 -0.0017884952 -0.0017925837 -0.0017984838 -0.0018028064 -0.0018052853 -0.001806739 -0.0018080814 -0.0018091081][-0.001808885 -0.0018071492 -0.0018074765 -0.0018079266 -0.0018076348 -0.0018058463 -0.0018033294 -0.0018017432 -0.0018017417 -0.0018026819 -0.0018038133 -0.0018048978 -0.001805776 -0.0018066353 -0.001807105][-0.0018085279 -0.0018068833 -0.0018071552 -0.0018076599 -0.0018077794 -0.0018072659 -0.0018062082 -0.0018050998 -0.0018043984 -0.0018041165 -0.0018042364 -0.0018047498 -0.0018053349 -0.001806034 -0.0018065092][-0.001808165 -0.0018064351 -0.0018068712 -0.0018073892 -0.0018073324 -0.0018067642 -0.0018060054 -0.0018053546 -0.0018048302 -0.0018045496 -0.0018045102 -0.0018047929 -0.0018051637 -0.0018056596 -0.0018061739][-0.0018080137 -0.001806307 -0.0018066026 -0.0018070454 -0.0018069651 -0.001806567 -0.001806108 -0.0018057461 -0.0018053723 -0.0018050688 -0.0018049241 -0.0018049446 -0.0018050383 -0.0018052564 -0.001805824][-0.0018079604 -0.0018063128 -0.0018063587 -0.0018066638 -0.0018066238 -0.0018064291 -0.0018061782 -0.0018060005 -0.0018056761 -0.0018053516 -0.0018051126 -0.0018050029 -0.0018049905 -0.001805091 -0.001805482]]...]
INFO - root - 2017-12-09 20:10:15.236203: step 54410, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.888 sec/batch; 68h:37m:52s remains)
INFO - root - 2017-12-09 20:10:23.892159: step 54420, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.874 sec/batch; 67h:31m:34s remains)
INFO - root - 2017-12-09 20:10:32.536862: step 54430, loss = 0.83, batch loss = 0.70 (9.1 examples/sec; 0.883 sec/batch; 68h:11m:40s remains)
INFO - root - 2017-12-09 20:10:41.311168: step 54440, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.858 sec/batch; 66h:16m:04s remains)
INFO - root - 2017-12-09 20:10:49.987350: step 54450, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.875 sec/batch; 67h:32m:45s remains)
INFO - root - 2017-12-09 20:10:58.775686: step 54460, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.860 sec/batch; 66h:25m:32s remains)
INFO - root - 2017-12-09 20:11:07.485980: step 54470, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.865 sec/batch; 66h:46m:40s remains)
INFO - root - 2017-12-09 20:11:16.177833: step 54480, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.897 sec/batch; 69h:14m:53s remains)
INFO - root - 2017-12-09 20:11:24.918345: step 54490, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 67h:03m:40s remains)
INFO - root - 2017-12-09 20:11:33.647319: step 54500, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 66h:34m:07s remains)
2017-12-09 20:11:34.530293: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.0018115718 0.0010623686 0.00021057867 -0.00055974594 -0.001068271 -0.0013540466 -0.0014551908 -0.0014742301 -0.0014678086 -0.0014217931 -0.0013675122 -0.0013045798 -0.0011920263 -0.0010436964 -0.00084812765][0.0016705374 0.0010782912 0.00036652538 -0.00030453422 -0.00076650409 -0.0010522308 -0.0011605143 -0.0011907781 -0.0011745907 -0.0011312151 -0.0011149812 -0.0011193957 -0.0010557206 -0.00087946956 -0.00058390805][0.0011275647 0.00074260181 0.00025476317 -0.00021432922 -0.0005372751 -0.00072666432 -0.00077287958 -0.00075221085 -0.00067672494 -0.00062111474 -0.00063319888 -0.00070932496 -0.00072337734 -0.00055838632 -0.00021506776][0.00041257881 0.00023297046 -2.8673094e-07 -0.0002130022 -0.00031495327 -0.000319995 -0.00023361272 -0.00011095451 4.1120918e-05 0.00011159258 7.63837e-05 -7.4819778e-05 -0.00019184046 -0.0001103651 0.00015586789][-0.00015105773 -0.00015222572 -0.0001455046 -9.7621931e-05 5.9553422e-05 0.00027676474 0.00051230576 0.00072733 0.00088914519 0.00093994022 0.0008420184 0.00059579487 0.0003410842 0.00026465056 0.00034068][-0.0004644026 -0.00032889075 -0.00011790986 0.00018642854 0.00061654311 0.0010729703 0.0014469289 0.0017106232 0.0018186116 0.0017716993 0.0015372754 0.0011510485 0.00073028926 0.00046366884 0.000350583][-0.00054956076 -0.0003148804 6.6537294e-05 0.0006033458 0.001273895 0.0019320803 0.0024067927 0.0026607728 0.0026729228 0.0025013653 0.0021117856 0.0015798787 0.0010201215 0.00061903859 0.0003798831][-0.00040496432 -5.6944089e-05 0.00050101 0.001252385 0.0021043778 0.002869782 0.0033691232 0.0035693371 0.0034930683 0.0032170829 0.0027197376 0.0020878869 0.001460191 0.0010301151 0.00079225458][-4.6475325e-05 0.00045102148 0.001228012 0.0022012973 0.0031984649 0.0040047509 0.0044814828 0.0046172845 0.0044853212 0.0041264989 0.0035756626 0.0029208218 0.0023252582 0.0019784379 0.0018479062][0.00067423575 0.001305187 0.0022987258 0.0034794402 0.00458775 0.0053742137 0.005772389 0.0058232038 0.0056199147 0.00521609 0.00468393 0.0041151252 0.0036601415 0.0034878012 0.0035335347][0.0017871893 0.0025255932 0.0036703567 0.0049862564 0.0061498876 0.0068605454 0.0071170437 0.0070179985 0.0066904607 0.0062313867 0.00573054 0.0053122356 0.0050762347 0.0051378342 0.0053972737][0.0029355902 0.003727105 0.004904225 0.0062408526 0.0073752631 0.0079948166 0.0081109069 0.0078291055 0.0073407884 0.0067953612 0.0063004843 0.0060198619 0.00599583 0.0062741828 0.00673115][0.0035497649 0.0043480275 0.0054363525 0.0066463766 0.007637532 0.0081343977 0.0081396587 0.0077353162 0.0071221418 0.00651364 0.006035903 0.0058603408 0.0059935045 0.006414887 0.0069901687][0.0032388689 0.0039903875 0.0049089403 0.0058849561 0.0066500758 0.0069892225 0.0069133383 0.0064812149 0.0058591431 0.005265642 0.0048420378 0.0047505386 0.004968104 0.0054359259 0.0060199029][0.0021043275 0.0027304455 0.003423471 0.0041256808 0.0046427124 0.0048276843 0.0047126603 0.0043311394 0.0038132132 0.0033323914 0.0030087754 0.0029753088 0.0031994218 0.0036273189 0.0041334331]]...]
INFO - root - 2017-12-09 20:11:43.199654: step 54510, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.841 sec/batch; 64h:57m:04s remains)
INFO - root - 2017-12-09 20:11:51.969916: step 54520, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.861 sec/batch; 66h:29m:21s remains)
INFO - root - 2017-12-09 20:12:00.518936: step 54530, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 66h:54m:34s remains)
INFO - root - 2017-12-09 20:12:09.130811: step 54540, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.872 sec/batch; 67h:18m:11s remains)
INFO - root - 2017-12-09 20:12:17.773381: step 54550, loss = 0.81, batch loss = 0.68 (9.4 examples/sec; 0.855 sec/batch; 66h:01m:27s remains)
INFO - root - 2017-12-09 20:12:26.262127: step 54560, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.833 sec/batch; 64h:19m:09s remains)
INFO - root - 2017-12-09 20:12:34.874336: step 54570, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.856 sec/batch; 66h:05m:49s remains)
INFO - root - 2017-12-09 20:12:43.449040: step 54580, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.878 sec/batch; 67h:45m:30s remains)
INFO - root - 2017-12-09 20:12:52.095086: step 54590, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.870 sec/batch; 67h:10m:52s remains)
INFO - root - 2017-12-09 20:13:00.780642: step 54600, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.846 sec/batch; 65h:16m:04s remains)
2017-12-09 20:13:01.598596: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0018191421 -0.001819013 -0.001818401 -0.0018173829 -0.0018169043 -0.0018174628 -0.0018183986 -0.001818839 -0.0018184884 -0.0018173493 -0.0018151116 -0.0018120942 -0.0018091166 -0.001806727 -0.0018051243][-0.0018169082 -0.001814413 -0.0018111632 -0.0018084062 -0.001807478 -0.0018088556 -0.0018118459 -0.0018146753 -0.0018165703 -0.0018171481 -0.0018156974 -0.001813047 -0.0018098127 -0.0018069581 -0.0018048726][-0.0018124956 -0.0018066402 -0.0018001338 -0.0017953677 -0.0017941496 -0.0017971484 -0.0018028083 -0.0018090704 -0.0018137351 -0.0018164678 -0.0018163361 -0.0018142938 -0.0018111967 -0.0018081455 -0.0018057968][-0.0018067277 -0.0017973394 -0.001787324 -0.0017800061 -0.0017784908 -0.0017835863 -0.0017926942 -0.0018027127 -0.0018109283 -0.0018162376 -0.00181739 -0.0018156403 -0.0018124405 -0.0018088671 -0.0018062522][-0.0018015513 -0.0017890248 -0.0017757952 -0.001766655 -0.0017647285 -0.0017705126 -0.0017814717 -0.0017944593 -0.0018058927 -0.0018139404 -0.0018171577 -0.0018169467 -0.0018142504 -0.0018109131 -0.0018081804][-0.0017988698 -0.00178471 -0.0017696919 -0.0017596505 -0.0017577764 -0.0017638324 -0.0017753501 -0.0017894729 -0.0018022535 -0.0018112428 -0.0018154099 -0.0018161125 -0.0018142344 -0.0018111625 -0.0018088401][-0.0017991707 -0.0017848501 -0.001769571 -0.0017593111 -0.0017575418 -0.0017634095 -0.0017746789 -0.0017885077 -0.0018013007 -0.0018102411 -0.0018145343 -0.001815501 -0.0018138955 -0.0018112365 -0.0018091913][-0.0018026825 -0.0017900092 -0.0017763979 -0.001767139 -0.0017653977 -0.0017701221 -0.0017791393 -0.0017906929 -0.0018015689 -0.0018090791 -0.0018127369 -0.0018137002 -0.0018128087 -0.0018109111 -0.0018094084][-0.0018073227 -0.0017976441 -0.0017870157 -0.0017796181 -0.0017780765 -0.0017813938 -0.0017877137 -0.0017956778 -0.0018030645 -0.0018082995 -0.0018108491 -0.0018114706 -0.0018107352 -0.0018094237 -0.0018086317][-0.0018104503 -0.0018040288 -0.0017970897 -0.0017920071 -0.0017907713 -0.0017925679 -0.0017962558 -0.0018007691 -0.0018047612 -0.0018076407 -0.0018089837 -0.0018093245 -0.0018087864 -0.0018080171 -0.0018078127][-0.0018125728 -0.0018088536 -0.0018049178 -0.0018018741 -0.0018009386 -0.0018017162 -0.001803314 -0.001805073 -0.0018065695 -0.0018075757 -0.0018076075 -0.0018070905 -0.001806303 -0.00180564 -0.0018055596][-0.0018130869 -0.0018109228 -0.0018088601 -0.0018073632 -0.0018069326 -0.0018070479 -0.0018072672 -0.0018073544 -0.0018072544 -0.0018071306 -0.0018066155 -0.0018059197 -0.001805149 -0.0018047006 -0.001804617][-0.0018127776 -0.0018115762 -0.0018105151 -0.001809907 -0.0018096765 -0.0018094785 -0.0018090698 -0.0018082203 -0.0018072312 -0.0018063958 -0.0018053954 -0.0018043376 -0.0018035707 -0.0018033757 -0.0018034856][-0.0018108643 -0.0018096394 -0.0018092503 -0.0018089369 -0.0018088598 -0.0018086607 -0.00180845 -0.0018077097 -0.0018066766 -0.0018057686 -0.0018046774 -0.0018035829 -0.0018027676 -0.0018024761 -0.0018023758][-0.0018093375 -0.0018077026 -0.0018068744 -0.0018062998 -0.0018060678 -0.0018059063 -0.0018056892 -0.0018055022 -0.0018049309 -0.0018044135 -0.0018036441 -0.0018028984 -0.0018024303 -0.001802149 -0.0018019509]]...]
INFO - root - 2017-12-09 20:13:10.332555: step 54610, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.887 sec/batch; 68h:29m:53s remains)
INFO - root - 2017-12-09 20:13:19.049695: step 54620, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.878 sec/batch; 67h:45m:45s remains)
INFO - root - 2017-12-09 20:13:27.569256: step 54630, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.849 sec/batch; 65h:30m:49s remains)
INFO - root - 2017-12-09 20:13:36.100416: step 54640, loss = 0.82, batch loss = 0.69 (9.7 examples/sec; 0.828 sec/batch; 63h:56m:10s remains)
INFO - root - 2017-12-09 20:13:44.751227: step 54650, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 66h:58m:20s remains)
INFO - root - 2017-12-09 20:13:53.400855: step 54660, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.885 sec/batch; 68h:18m:57s remains)
INFO - root - 2017-12-09 20:14:02.152236: step 54670, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.901 sec/batch; 69h:31m:10s remains)
INFO - root - 2017-12-09 20:14:10.653468: step 54680, loss = 0.81, batch loss = 0.68 (9.5 examples/sec; 0.845 sec/batch; 65h:13m:50s remains)
INFO - root - 2017-12-09 20:14:19.233393: step 54690, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.841 sec/batch; 64h:53m:57s remains)
INFO - root - 2017-12-09 20:14:27.867008: step 54700, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.852 sec/batch; 65h:44m:28s remains)
2017-12-09 20:14:28.747661: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.0034554079 0.0038403519 0.0043702726 0.00497799 0.0055294968 0.0059537771 0.006233023 0.0064213821 0.0064834817 0.0065019974 0.0063913264 0.0063724075 0.0064082653 0.0063580158 0.00599293][0.0020885067 0.0022042082 0.0025772443 0.0031367489 0.0037270235 0.0042603603 0.0046894872 0.0050175888 0.0052199839 0.0053612031 0.005311978 0.0052745556 0.0052158087 0.0050440328 0.0045544095][0.0011307196 0.0009478206 0.0011047133 0.0015601133 0.0021469113 0.0027485546 0.0032863868 0.0037183003 0.0040203966 0.0042217555 0.0041798037 0.0040824255 0.003927859 0.0036702198 0.0030887802][0.00074755319 0.0002927793 0.00019991805 0.00049187348 0.0010038695 0.0016035232 0.0021770042 0.0026410175 0.0029513147 0.0031042504 0.0029995618 0.002826184 0.0026186425 0.0023611817 0.0018029952][0.0010539863 0.00042417308 7.1669929e-05 0.00014523708 0.000494514 0.000978697 0.001462448 0.0018449639 0.0020478242 0.0020615608 0.0018560257 0.0016095742 0.0013964203 0.0012117141 0.00078470854][0.0018995776 0.0012279976 0.00066831533 0.00052936829 0.00068272476 0.00099977653 0.001309814 0.0015070738 0.001508006 0.0013321709 0.0010151909 0.00070753566 0.00049832568 0.00038316322 9.5173018e-05][0.0029764036 0.0024125092 0.00173034 0.0014439096 0.0014037421 0.0015070661 0.0015644754 0.0014974895 0.0012544439 0.00089509145 0.00049208628 0.000144455 -7.1514864e-05 -0.0001646867 -0.0003658561][0.0038406858 0.0034705848 0.0027406928 0.0023605097 0.0021609864 0.0020630029 0.0018549989 0.0015000891 0.0010221867 0.00053070381 9.2869275e-05 -0.00025659951 -0.00047423842 -0.00057503011 -0.00073630386][0.0042466749 0.0040036775 0.0032623103 0.0028240732 0.0024991352 0.0022137323 0.0017676255 0.0011777406 0.00054375583 3.8791914e-06 -0.00039763341 -0.00069040526 -0.0008740687 -0.0009716035 -0.0010992514][0.0041399505 0.0039354968 0.0031788563 0.0026802197 0.0022615246 0.0018273104 0.0012189083 0.0005070233 -0.00015803601 -0.00064220454 -0.00094512274 -0.0011397057 -0.001257633 -0.001326584 -0.0014105375][0.0036599911 0.0034082392 0.002636313 0.0020926734 0.0016078561 0.0010766437 0.00041144236 -0.00028250646 -0.00085178611 -0.0012079333 -0.0013933587 -0.0014939052 -0.0015514432 -0.0015876573 -0.0016292464][0.0030032727 0.0027039619 0.0019325867 0.0013510225 0.00080885447 0.00023062679 -0.00039947545 -0.00096357934 -0.0013589171 -0.0015675818 -0.0016583556 -0.0016977148 -0.0017174574 -0.0017292636 -0.001743565][0.0023486051 0.0020227684 0.0012923497 0.00069763639 0.00011339074 -0.00046789844 -0.0010064424 -0.0014048194 -0.0016339044 -0.0017328768 -0.0017702916 -0.0017840302 -0.0017884725 -0.0017896342 -0.0017921819][0.0017474642 0.0014528484 0.00079886278 0.00021608744 -0.00038533809 -0.00094151165 -0.0013804572 -0.001638539 -0.0017502601 -0.0017876317 -0.0018009732 -0.0018069878 -0.0018085351 -0.0018068429 -0.0018059899][0.0012499617 0.000983826 0.00040822208 -0.00014368934 -0.00072338863 -0.0012255388 -0.0015723411 -0.0017350098 -0.0017853924 -0.0017995531 -0.0018054164 -0.0018098344 -0.0018109253 -0.0018102783 -0.0018091957]]...]
INFO - root - 2017-12-09 20:14:37.384174: step 54710, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.880 sec/batch; 67h:54m:22s remains)
INFO - root - 2017-12-09 20:14:46.120467: step 54720, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.861 sec/batch; 66h:25m:28s remains)
INFO - root - 2017-12-09 20:14:54.852142: step 54730, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.881 sec/batch; 67h:59m:50s remains)
INFO - root - 2017-12-09 20:15:03.445128: step 54740, loss = 0.82, batch loss = 0.69 (8.8 examples/sec; 0.909 sec/batch; 70h:06m:31s remains)
INFO - root - 2017-12-09 20:15:12.081054: step 54750, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.839 sec/batch; 64h:44m:13s remains)
INFO - root - 2017-12-09 20:15:20.794996: step 54760, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.889 sec/batch; 68h:33m:24s remains)
INFO - root - 2017-12-09 20:15:29.428540: step 54770, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.851 sec/batch; 65h:39m:34s remains)
INFO - root - 2017-12-09 20:15:37.841145: step 54780, loss = 0.81, batch loss = 0.68 (10.2 examples/sec; 0.783 sec/batch; 60h:22m:39s remains)
INFO - root - 2017-12-09 20:15:46.484249: step 54790, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.856 sec/batch; 66h:00m:37s remains)
INFO - root - 2017-12-09 20:15:55.319857: step 54800, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.888 sec/batch; 68h:29m:09s remains)
2017-12-09 20:15:56.244460: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.22227843 0.2219763 0.21850029 0.21423046 0.20878495 0.20172268 0.19189727 0.18049115 0.16872714 0.1579521 0.14769815 0.13804989 0.12916185 0.11986213 0.1126899][0.23204653 0.23259722 0.22932708 0.22508061 0.22049114 0.21600685 0.20953289 0.20282491 0.19648391 0.19174093 0.18688169 0.18052089 0.17385438 0.16519947 0.15735359][0.23169091 0.2319582 0.22875187 0.22599162 0.2239628 0.22280641 0.22071446 0.21947064 0.21962145 0.22140339 0.222529 0.2208983 0.21781836 0.21092969 0.20339407][0.22419035 0.22346964 0.22047226 0.21924785 0.22002698 0.22285882 0.2260343 0.23102765 0.23745519 0.24543516 0.25210959 0.25496376 0.25492761 0.24990416 0.24294147][0.21223821 0.21083046 0.20795822 0.20737371 0.21024615 0.21643184 0.22410882 0.23469631 0.24707226 0.26081806 0.27176765 0.27806929 0.28062212 0.27728415 0.27095106][0.19632278 0.19418363 0.19137856 0.19229788 0.19700862 0.20556749 0.21662901 0.23162444 0.2484501 0.26626021 0.28044984 0.28955427 0.29389063 0.29227468 0.28724545][0.17589268 0.17432006 0.17216124 0.17434533 0.18097742 0.1918693 0.20556207 0.22299175 0.242359 0.26230288 0.27763954 0.28824598 0.29343414 0.29357085 0.28940034][0.15258554 0.1518731 0.15129705 0.15549615 0.1641167 0.17672513 0.19213136 0.21091954 0.23042364 0.24976635 0.26402596 0.27451047 0.27958328 0.28082108 0.2778239][0.12403432 0.12577277 0.12760366 0.13393535 0.14460124 0.15865703 0.17516088 0.19448951 0.21418917 0.23236217 0.24519067 0.25429818 0.25820208 0.25951439 0.25668913][0.092868179 0.096519768 0.10038126 0.1087568 0.12121592 0.13634263 0.15376146 0.17355485 0.19358082 0.21119098 0.22347112 0.23165438 0.23493862 0.23595311 0.23357795][0.066492088 0.07052613 0.074937105 0.083774835 0.096807644 0.11249838 0.13042921 0.15004387 0.16999555 0.18738638 0.19991829 0.20812136 0.21205671 0.21367244 0.21186391][0.046034362 0.05039908 0.055621728 0.064001165 0.076589242 0.091358893 0.10835579 0.12665653 0.14529313 0.16197567 0.17447938 0.18365924 0.18944591 0.19278991 0.19258611][0.03098559 0.035554118 0.041783709 0.050542761 0.062706746 0.075779088 0.090620652 0.10649356 0.12254398 0.13723829 0.14892806 0.15875767 0.16663618 0.17211011 0.17384683][0.019405684 0.024092302 0.030525893 0.039053395 0.050497137 0.062298439 0.075073466 0.087872207 0.10137375 0.11435885 0.12549075 0.13594493 0.14523585 0.15277407 0.15611802][0.013330685 0.017670428 0.024054864 0.032475442 0.042684808 0.052515134 0.063282594 0.073730543 0.084527723 0.094945684 0.10468768 0.11498155 0.1246353 0.13308619 0.13735881]]...]
INFO - root - 2017-12-09 20:16:04.896300: step 54810, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 66h:59m:06s remains)
INFO - root - 2017-12-09 20:16:13.611310: step 54820, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.859 sec/batch; 66h:15m:06s remains)
INFO - root - 2017-12-09 20:16:22.213225: step 54830, loss = 0.82, batch loss = 0.69 (8.7 examples/sec; 0.918 sec/batch; 70h:49m:43s remains)
INFO - root - 2017-12-09 20:16:30.781702: step 54840, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.857 sec/batch; 66h:04m:19s remains)
INFO - root - 2017-12-09 20:16:39.507989: step 54850, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.830 sec/batch; 64h:02m:19s remains)
INFO - root - 2017-12-09 20:16:48.092031: step 54860, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.873 sec/batch; 67h:20m:35s remains)
INFO - root - 2017-12-09 20:16:56.827362: step 54870, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.896 sec/batch; 69h:04m:05s remains)
INFO - root - 2017-12-09 20:17:05.352215: step 54880, loss = 0.82, batch loss = 0.69 (10.5 examples/sec; 0.763 sec/batch; 58h:52m:35s remains)
INFO - root - 2017-12-09 20:17:14.089598: step 54890, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.863 sec/batch; 66h:32m:09s remains)
INFO - root - 2017-12-09 20:17:22.612692: step 54900, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.851 sec/batch; 65h:36m:46s remains)
2017-12-09 20:17:23.533141: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.38543165 0.37937477 0.37105605 0.36110422 0.35256955 0.34525293 0.34265926 0.34391904 0.34774527 0.35188445 0.35456616 0.35626748 0.35574219 0.35226145 0.34638563][0.40077743 0.3958309 0.38802621 0.37887257 0.37114784 0.36554122 0.36407831 0.3658483 0.37047267 0.37505722 0.3785975 0.38090286 0.38080779 0.37787661 0.37230819][0.3987678 0.39658496 0.39131364 0.38508028 0.38016206 0.37762654 0.37852114 0.38126796 0.38609144 0.39153802 0.39605629 0.39904135 0.39977854 0.39735687 0.39190146][0.38651657 0.38798675 0.38662314 0.38532475 0.3849434 0.38618112 0.38974985 0.39408529 0.39909789 0.40406787 0.40864024 0.41248032 0.41404808 0.41238645 0.40766335][0.36530942 0.37235865 0.37631533 0.37990418 0.38422513 0.389713 0.395772 0.40141469 0.40691587 0.41199982 0.41588509 0.41902992 0.42050627 0.41910288 0.41433218][0.33649373 0.34872255 0.35775223 0.36634454 0.37426424 0.38195014 0.38937843 0.3960501 0.40173107 0.4064858 0.4099901 0.4124625 0.41316274 0.41172689 0.40746921][0.30109549 0.31868076 0.33195123 0.3434346 0.35280269 0.3614732 0.36869738 0.37567815 0.3814624 0.38650113 0.39025643 0.39206544 0.3924883 0.39062402 0.38731733][0.26362953 0.28543812 0.30224058 0.3154617 0.32537356 0.33292407 0.33821091 0.3438226 0.34890845 0.35399556 0.35765523 0.35948738 0.35952926 0.35765156 0.35483259][0.22760867 0.25111648 0.26891536 0.28198981 0.29055381 0.29591054 0.29911402 0.3025862 0.30618569 0.31031722 0.31340367 0.31513229 0.31485128 0.31329069 0.31130764][0.19555068 0.21834488 0.23476833 0.24520464 0.25026318 0.25185505 0.25133806 0.251586 0.253055 0.25530776 0.25732693 0.25803035 0.25710329 0.2561982 0.25578931][0.16707964 0.18697546 0.19985376 0.20644799 0.20708223 0.20394728 0.19938879 0.19603363 0.19429746 0.19415452 0.19494066 0.19494377 0.19425637 0.19431338 0.19676335][0.14212172 0.15709552 0.16485247 0.16664678 0.16289696 0.15570049 0.14789687 0.14161287 0.13785475 0.13623555 0.13589083 0.13556486 0.13532041 0.13720547 0.14252895][0.11803839 0.1275277 0.13031651 0.12811135 0.12153286 0.11234313 0.10315937 0.095646143 0.090860784 0.088245668 0.087157033 0.086856894 0.087242194 0.091009185 0.099374883][0.092609212 0.096875653 0.0954108 0.090826847 0.083026312 0.073745973 0.064866476 0.0577868 0.053189471 0.050352521 0.049043939 0.048907679 0.050849102 0.057286177 0.069316775][0.068448044 0.069167957 0.065533072 0.059672505 0.051854219 0.043416765 0.035506777 0.029496398 0.025369402 0.022570109 0.021241173 0.021600055 0.025307795 0.034725994 0.05061442]]...]
INFO - root - 2017-12-09 20:17:32.172016: step 54910, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.840 sec/batch; 64h:45m:53s remains)
INFO - root - 2017-12-09 20:17:40.740963: step 54920, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.849 sec/batch; 65h:26m:06s remains)
INFO - root - 2017-12-09 20:17:49.157128: step 54930, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.839 sec/batch; 64h:42m:45s remains)
INFO - root - 2017-12-09 20:17:57.520862: step 54940, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.833 sec/batch; 64h:14m:29s remains)
INFO - root - 2017-12-09 20:18:06.132103: step 54950, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.870 sec/batch; 67h:05m:54s remains)
INFO - root - 2017-12-09 20:18:14.809818: step 54960, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.850 sec/batch; 65h:30m:51s remains)
INFO - root - 2017-12-09 20:18:23.368393: step 54970, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.863 sec/batch; 66h:30m:25s remains)
INFO - root - 2017-12-09 20:18:31.825894: step 54980, loss = 0.82, batch loss = 0.69 (9.8 examples/sec; 0.817 sec/batch; 62h:57m:40s remains)
INFO - root - 2017-12-09 20:18:40.329076: step 54990, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 66h:52m:54s remains)
INFO - root - 2017-12-09 20:18:49.050236: step 55000, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.880 sec/batch; 67h:51m:17s remains)
2017-12-09 20:18:49.932713: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0018200708 -0.0018194558 -0.001819051 -0.0018183879 -0.0018173992 -0.0018165164 -0.0018154418 -0.0018144121 -0.0018135866 -0.0018133231 -0.0018134196 -0.001813723 -0.00181437 -0.0018153589 -0.0018163645][-0.0018191398 -0.0018184314 -0.0018179594 -0.001817257 -0.0018162755 -0.0018153853 -0.0018144738 -0.0018135938 -0.0018129122 -0.0018127194 -0.0018128102 -0.0018131163 -0.0018137989 -0.0018148341 -0.0018158955][-0.001818539 -0.0018177569 -0.0018172467 -0.0018165767 -0.001815676 -0.0018148741 -0.0018141449 -0.0018134164 -0.0018128969 -0.001812733 -0.0018127712 -0.0018129995 -0.0018136038 -0.0018145808 -0.0018155698][-0.0018177732 -0.0018169892 -0.0018164628 -0.0018158647 -0.0018151067 -0.0018144274 -0.0018138516 -0.0018132807 -0.0018129115 -0.0018127061 -0.0018125939 -0.0018127166 -0.0018132654 -0.0018141369 -0.0018150198][-0.0018170935 -0.0018163236 -0.0018158267 -0.0018153477 -0.0018147761 -0.0018142323 -0.0018137256 -0.0018132306 -0.0018129438 -0.0018127069 -0.0018124635 -0.0018124786 -0.0018129452 -0.0018137196 -0.0018145036][-0.0018164722 -0.0018157511 -0.0018153193 -0.0018149852 -0.0018145698 -0.0018141043 -0.0018136509 -0.0018131923 -0.0018129505 -0.0018128357 -0.0018127132 -0.0018127474 -0.0018131052 -0.0018137229 -0.0018143284][-0.0018158888 -0.0018152324 -0.0018149028 -0.0018147342 -0.001814462 -0.0018140717 -0.0018136613 -0.0018132254 -0.0018130516 -0.0018130887 -0.0018131349 -0.0018132605 -0.0018135655 -0.0018139947 -0.0018143649][-0.001815368 -0.001814799 -0.0018146253 -0.0018146506 -0.0018145543 -0.0018143018 -0.0018138917 -0.001813421 -0.0018132576 -0.0018134096 -0.0018136303 -0.0018138449 -0.0018141569 -0.0018144955 -0.0018147276][-0.001814766 -0.0018142845 -0.0018142945 -0.0018145087 -0.001814599 -0.0018144966 -0.0018141173 -0.0018136328 -0.0018134667 -0.0018137161 -0.001814139 -0.0018145409 -0.0018149182 -0.0018152274 -0.0018153719][-0.0018141015 -0.0018137131 -0.0018138484 -0.0018141626 -0.0018143795 -0.001814426 -0.0018141469 -0.0018137527 -0.0018136424 -0.0018139544 -0.0018145173 -0.001815061 -0.0018155287 -0.0018158788 -0.0018160106][-0.0018134869 -0.0018131814 -0.0018133869 -0.0018137115 -0.0018139551 -0.0018141096 -0.0018139816 -0.0018137262 -0.0018136955 -0.0018140916 -0.0018147462 -0.0018154335 -0.0018160434 -0.001816489 -0.0018166745][-0.0018127597 -0.0018125781 -0.0018129096 -0.0018132769 -0.0018135411 -0.0018137817 -0.0018137877 -0.0018136741 -0.0018137384 -0.0018141686 -0.0018148443 -0.0018156446 -0.0018163918 -0.0018169421 -0.0018172341][-0.001811962 -0.0018119668 -0.001812505 -0.0018130436 -0.0018134095 -0.0018136705 -0.0018137083 -0.0018136672 -0.0018137568 -0.001814203 -0.001814921 -0.0018157918 -0.0018166052 -0.0018172031 -0.0018175929][-0.0018115252 -0.0018116174 -0.0018122757 -0.0018129748 -0.0018134553 -0.0018137471 -0.001813843 -0.0018138849 -0.0018140281 -0.001814508 -0.0018152372 -0.001816089 -0.0018168385 -0.0018173951 -0.0018177987][-0.0018117921 -0.0018118576 -0.0018124582 -0.0018131805 -0.0018136908 -0.0018140321 -0.0018142434 -0.0018144215 -0.0018146728 -0.0018151876 -0.001815866 -0.0018166187 -0.0018172531 -0.0018177174 -0.001818067]]...]
INFO - root - 2017-12-09 20:18:58.589906: step 55010, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 66h:36m:32s remains)
INFO - root - 2017-12-09 20:19:07.258463: step 55020, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.860 sec/batch; 66h:16m:20s remains)
INFO - root - 2017-12-09 20:19:15.787160: step 55030, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.872 sec/batch; 67h:12m:16s remains)
INFO - root - 2017-12-09 20:19:24.328116: step 55040, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 67h:00m:48s remains)
INFO - root - 2017-12-09 20:19:32.987012: step 55050, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.882 sec/batch; 68h:00m:27s remains)
INFO - root - 2017-12-09 20:19:41.633665: step 55060, loss = 0.82, batch loss = 0.69 (9.8 examples/sec; 0.816 sec/batch; 62h:54m:25s remains)
INFO - root - 2017-12-09 20:19:50.280084: step 55070, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.853 sec/batch; 65h:42m:52s remains)
INFO - root - 2017-12-09 20:19:58.979372: step 55080, loss = 0.81, batch loss = 0.68 (9.3 examples/sec; 0.862 sec/batch; 66h:26m:52s remains)
INFO - root - 2017-12-09 20:20:07.456137: step 55090, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.891 sec/batch; 68h:38m:44s remains)
INFO - root - 2017-12-09 20:20:16.050656: step 55100, loss = 0.82, batch loss = 0.69 (9.7 examples/sec; 0.821 sec/batch; 63h:17m:30s remains)
2017-12-09 20:20:16.984666: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0018368983 -0.0018360172 -0.0018359306 -0.0018359559 -0.0018360293 -0.0018361754 -0.0018363412 -0.0018364962 -0.0018365779 -0.0018365844 -0.0018364879 -0.0018363682 -0.0018362842 -0.0018363356 -0.0018366554][-0.0018362204 -0.0018353309 -0.0018353815 -0.0018355811 -0.0018358725 -0.0018362261 -0.0018365898 -0.0018368713 -0.0018370186 -0.0018370061 -0.0018368447 -0.0018365666 -0.0018362511 -0.0018360252 -0.0018360161][-0.0018362562 -0.0018355369 -0.0018357727 -0.0018361864 -0.0018367238 -0.0018373231 -0.0018379161 -0.0018384318 -0.0018387522 -0.0018387747 -0.0018385181 -0.0018380114 -0.0018373939 -0.0018368247 -0.0018364256][-0.0018365298 -0.0018359877 -0.001836432 -0.0018370812 -0.001837857 -0.0018386972 -0.0018395043 -0.0018401861 -0.0018405718 -0.001840528 -0.0018401279 -0.0018394395 -0.0018386057 -0.0018377614 -0.0018370432][-0.0018368732 -0.0018365523 -0.001837219 -0.0018381497 -0.0018391575 -0.001840096 -0.0018408825 -0.0018414081 -0.0018415742 -0.0018414352 -0.0018409861 -0.0018403247 -0.0018394972 -0.0018385598 -0.0018376607][-0.0018371461 -0.0018370756 -0.0018379958 -0.0018391653 -0.0018402524 -0.001841017 -0.0018413869 -0.0018412693 -0.0018408349 -0.0018404211 -0.0018402027 -0.0018401215 -0.00183978 -0.0018390958 -0.0018381793][-0.0018374312 -0.001837494 -0.0018386047 -0.0018398731 -0.0018408404 -0.0018412339 -0.0018407307 -0.0018392415 -0.0018375777 -0.0018368911 -0.0018376256 -0.0018389688 -0.0018397386 -0.0018395161 -0.0018386099][-0.0018376251 -0.0018377547 -0.0018388558 -0.0018401085 -0.0018409803 -0.0018408425 -0.0018391297 -0.0018360578 -0.0018332617 -0.0018331888 -0.0018356642 -0.0018384376 -0.0018399148 -0.0018398775 -0.0018389488][-0.0018377616 -0.0018378058 -0.0018388294 -0.0018399602 -0.0018405828 -0.0018399941 -0.0018378955 -0.0018350616 -0.0018332396 -0.0018339447 -0.0018365444 -0.0018390224 -0.0018402043 -0.0018400318 -0.0018391248][-0.0018378138 -0.001837724 -0.0018385912 -0.0018395119 -0.0018400144 -0.0018396594 -0.0018384474 -0.0018370264 -0.001836383 -0.0018370649 -0.0018385135 -0.0018397735 -0.001840267 -0.001839968 -0.0018391546][-0.0018377617 -0.0018375607 -0.0018382167 -0.0018389736 -0.0018395105 -0.0018396176 -0.0018393768 -0.0018390863 -0.0018390163 -0.0018393025 -0.0018397624 -0.0018401371 -0.0018401725 -0.0018398004 -0.0018390926][-0.0018376992 -0.0018373652 -0.0018377888 -0.001838363 -0.0018389097 -0.0018393319 -0.0018396324 -0.0018398347 -0.0018399896 -0.0018400728 -0.0018401446 -0.0018401225 -0.0018398936 -0.0018394653 -0.0018388799][-0.0018375894 -0.0018371997 -0.0018373943 -0.0018377787 -0.0018382268 -0.001838704 -0.001839158 -0.0018395715 -0.0018398283 -0.0018399601 -0.001839935 -0.0018397353 -0.0018393829 -0.0018389493 -0.0018385659][-0.0018375973 -0.0018371396 -0.0018371923 -0.0018374061 -0.0018376904 -0.0018380501 -0.0018384206 -0.0018387467 -0.001838979 -0.0018390991 -0.0018390737 -0.001838931 -0.0018387082 -0.0018384707 -0.0018383113][-0.001837783 -0.001837261 -0.0018372191 -0.00183733 -0.0018374758 -0.0018376503 -0.0018378255 -0.0018379588 -0.0018380466 -0.0018380895 -0.0018380899 -0.0018380759 -0.0018380773 -0.0018381081 -0.0018381588]]...]
INFO - root - 2017-12-09 20:20:25.468781: step 55110, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.861 sec/batch; 66h:20m:26s remains)
INFO - root - 2017-12-09 20:20:34.023973: step 55120, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.886 sec/batch; 68h:17m:43s remains)
INFO - root - 2017-12-09 20:20:42.730484: step 55130, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.838 sec/batch; 64h:31m:55s remains)
INFO - root - 2017-12-09 20:20:51.246149: step 55140, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.852 sec/batch; 65h:37m:20s remains)
INFO - root - 2017-12-09 20:20:59.844456: step 55150, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 66h:51m:23s remains)
INFO - root - 2017-12-09 20:21:08.471557: step 55160, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 66h:51m:40s remains)
INFO - root - 2017-12-09 20:21:17.193830: step 55170, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.848 sec/batch; 65h:20m:10s remains)
INFO - root - 2017-12-09 20:21:25.844031: step 55180, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.838 sec/batch; 64h:34m:26s remains)
INFO - root - 2017-12-09 20:21:34.386421: step 55190, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.896 sec/batch; 69h:00m:01s remains)
INFO - root - 2017-12-09 20:21:43.044467: step 55200, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.866 sec/batch; 66h:42m:24s remains)
2017-12-09 20:21:43.965528: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.0021251496 0.0010515488 0.00024712889 -0.00025686168 -0.00052567665 -0.00066658086 -0.00077388564 -0.0008985321 -0.0010508545 -0.0012093829 -0.0013578411 -0.0014837324 -0.0015872137 -0.0016619053 -0.0017115236][0.0040248036 0.0026460318 0.0015561337 0.0008178429 0.000385353 0.00012905139 -7.6976023e-05 -0.00031390833 -0.00060256093 -0.00090170471 -0.0011740872 -0.0013890964 -0.0015437098 -0.0016438256 -0.0017046896][0.0061455411 0.0046399371 0.0034064748 0.0025341935 0.0020038332 0.001665389 0.0013565164 0.0009587825 0.00044038019 -0.00012057705 -0.00064969517 -0.0010789973 -0.0013837571 -0.0015680396 -0.001668442][0.0080910753 0.0066978135 0.0055544204 0.0047463314 0.0042659161 0.0039475453 0.0035953894 0.0030515036 0.0022649714 0.0013387102 0.0003981198 -0.00041622133 -0.0010188948 -0.0013895166 -0.0015886032][0.0094848592 0.0083915358 0.0075286026 0.0069633736 0.0066864979 0.0065186084 0.0062387227 0.0056436826 0.0046559968 0.0033752169 0.0019782917 0.00068548752 -0.00033532828 -0.0010107134 -0.0013978765][0.010095565 0.0093640387 0.0088561745 0.0086074444 0.008597834 0.0086440239 0.0085277483 0.0080142021 0.0070002642 0.0055422704 0.0038321833 0.0021301513 0.00068563328 -0.00035306718 -0.00099830073][0.009969716 0.0095645906 0.0093738483 0.0093979118 0.0096112313 0.0098424889 0.0098988656 0.0095373252 0.0086476747 0.0072383336 0.0054796254 0.0036092415 0.0019048251 0.00056562142 -0.00034354709][0.0092251021 0.0090794107 0.0091339555 0.0093555516 0.0097148763 0.010062081 0.010242829 0.010031397 0.00933252 0.0081239454 0.0065342742 0.0047458978 0.0030115061 0.0015410158 0.00046581833][0.00795633 0.008005335 0.0082511939 0.0086245043 0.00907632 0.0094759529 0.009710486 0.009604332 0.0090862243 0.0081209242 0.0067981076 0.0052497555 0.0036784862 0.0022660634 0.0011641128][0.0062036919 0.0064093391 0.0068012211 0.0072885086 0.0078008031 0.0082131252 0.0084520178 0.0083997771 0.0080112359 0.0072571295 0.0062013767 0.0049456125 0.0036476906 0.0024420535 0.0014490528][0.0041355961 0.004471397 0.004979067 0.0055534039 0.006104141 0.0064908857 0.0066794478 0.0066170241 0.006299037 0.0057109962 0.004893024 0.0039215572 0.0029210248 0.0019845353 0.0011825761][0.0021372251 0.0025536194 0.0031278795 0.0037469035 0.0043055424 0.0046382491 0.0047442759 0.0046225316 0.0043096836 0.0038170265 0.003183648 0.0024602287 0.0017404555 0.0010793259 0.00049970008][0.00073222036 0.0011460715 0.0017019577 0.0022850814 0.0027872939 0.0030323188 0.003026526 0.0028054169 0.0024550413 0.0020153942 0.0015303568 0.0010120695 0.000524748 9.3026552e-05 -0.0002793672][0.00028381275 0.00059990713 0.001048464 0.0015186748 0.001912611 0.0020557716 0.0019356495 0.0015989559 0.0011694011 0.00072172878 0.00031373685 -6.9592847e-05 -0.00038668257 -0.00064582296 -0.0008580063][0.00087690109 0.0010546913 0.0013471012 0.0016471586 0.0018887647 0.0019164738 0.0016968121 0.0012630286 0.0007509276 0.00024908164 -0.00016919139 -0.00051811943 -0.00076724286 -0.00094838516 -0.0010834621]]...]
INFO - root - 2017-12-09 20:21:52.624538: step 55210, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.861 sec/batch; 66h:20m:15s remains)
INFO - root - 2017-12-09 20:22:01.325529: step 55220, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.866 sec/batch; 66h:42m:33s remains)
INFO - root - 2017-12-09 20:22:09.924278: step 55230, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.856 sec/batch; 65h:57m:52s remains)
INFO - root - 2017-12-09 20:22:18.491912: step 55240, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.874 sec/batch; 67h:20m:59s remains)
INFO - root - 2017-12-09 20:22:27.105240: step 55250, loss = 0.82, batch loss = 0.69 (9.7 examples/sec; 0.828 sec/batch; 63h:44m:17s remains)
INFO - root - 2017-12-09 20:22:35.843238: step 55260, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.863 sec/batch; 66h:26m:13s remains)
INFO - root - 2017-12-09 20:22:44.560795: step 55270, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.880 sec/batch; 67h:48m:19s remains)
INFO - root - 2017-12-09 20:22:53.288045: step 55280, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.861 sec/batch; 66h:19m:35s remains)
INFO - root - 2017-12-09 20:23:01.806491: step 55290, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.853 sec/batch; 65h:41m:22s remains)
INFO - root - 2017-12-09 20:23:10.472510: step 55300, loss = 0.83, batch loss = 0.70 (9.3 examples/sec; 0.863 sec/batch; 66h:25m:36s remains)
2017-12-09 20:23:11.359225: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.019928576 0.023082495 0.025919192 0.027867809 0.029219577 0.029718257 0.029946562 0.030198146 0.030350316 0.030201562 0.029636338 0.028718982 0.027234334 0.025229963 0.022669546][0.027720625 0.032208677 0.035943471 0.038225412 0.039735749 0.040472589 0.041304134 0.042447235 0.043697096 0.044739444 0.045199353 0.045102965 0.044092018 0.042009424 0.038618702][0.0367397 0.0424441 0.046785831 0.049226042 0.050688542 0.0515983 0.053152837 0.055608422 0.058506474 0.061363883 0.063570328 0.065180533 0.065298207 0.063468568 0.059337094][0.045981795 0.052900244 0.057423741 0.05956256 0.060465246 0.061401717 0.063758276 0.067732349 0.072885029 0.078257427 0.082818367 0.0862337 0.087625235 0.0860809 0.081072144][0.05393713 0.06172622 0.06627012 0.068107978 0.068437181 0.069308758 0.072276793 0.077728674 0.084907949 0.092551067 0.099512592 0.10497285 0.10777813 0.10655153 0.10074759][0.059469447 0.067879274 0.07251513 0.074077472 0.074060112 0.075142063 0.078504644 0.084892645 0.093256906 0.10253966 0.11106632 0.11790671 0.12164072 0.12073688 0.11439525][0.062099859 0.070907414 0.075652577 0.077210255 0.077025011 0.078110643 0.0814939 0.088259071 0.097082771 0.10726346 0.11661048 0.12412717 0.12812792 0.12723649 0.12062708][0.062266469 0.071382113 0.07638368 0.078062966 0.07790447 0.078958116 0.082019508 0.088488042 0.097072855 0.10749143 0.11722254 0.12494151 0.12884487 0.12755571 0.12060935][0.061033368 0.070357017 0.075689264 0.077624574 0.07763347 0.078506783 0.080997549 0.086610012 0.094348058 0.10430002 0.11374814 0.1212579 0.12475988 0.12336428 0.11630203][0.058851544 0.068287827 0.074064925 0.076447621 0.076837525 0.077506408 0.079165809 0.083537884 0.08997605 0.098796345 0.10729664 0.11399931 0.11682829 0.11526343 0.10858724][0.055367786 0.064735554 0.070717148 0.073377527 0.07409133 0.07468845 0.0757827 0.078938439 0.084025934 0.0915799 0.098873869 0.10429846 0.10617091 0.10439223 0.098405026][0.050506253 0.059316937 0.065051638 0.067706838 0.06842754 0.068778738 0.069416724 0.071731031 0.075873524 0.082194611 0.088371605 0.092700012 0.093857825 0.092023328 0.086861439][0.044444546 0.052355982 0.057509422 0.059885241 0.060443789 0.060483936 0.060649641 0.062228553 0.065508075 0.070765659 0.075917453 0.07944233 0.080396816 0.078939058 0.074772924][0.037047543 0.043775998 0.048078172 0.05001568 0.050349515 0.050178766 0.050135989 0.051211394 0.053764492 0.057881892 0.061996251 0.0648905 0.065805711 0.065009691 0.061949][0.02869595 0.034006428 0.037237093 0.038533472 0.038550243 0.03825435 0.038165867 0.038989782 0.041007295 0.044147585 0.047369059 0.049686037 0.050626583 0.050271321 0.048115581]]...]
INFO - root - 2017-12-09 20:23:19.877069: step 55310, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.854 sec/batch; 65h:44m:18s remains)
INFO - root - 2017-12-09 20:23:28.556671: step 55320, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.841 sec/batch; 64h:44m:49s remains)
INFO - root - 2017-12-09 20:23:37.042903: step 55330, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.874 sec/batch; 67h:15m:13s remains)
INFO - root - 2017-12-09 20:23:45.585039: step 55340, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.866 sec/batch; 66h:39m:59s remains)
INFO - root - 2017-12-09 20:23:54.077162: step 55350, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.857 sec/batch; 65h:57m:16s remains)
INFO - root - 2017-12-09 20:24:02.522844: step 55360, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.851 sec/batch; 65h:28m:34s remains)
INFO - root - 2017-12-09 20:24:11.253116: step 55370, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.876 sec/batch; 67h:23m:55s remains)
INFO - root - 2017-12-09 20:24:19.989876: step 55380, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.876 sec/batch; 67h:26m:11s remains)
INFO - root - 2017-12-09 20:24:28.634375: step 55390, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.884 sec/batch; 68h:01m:53s remains)
INFO - root - 2017-12-09 20:24:37.206820: step 55400, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.892 sec/batch; 68h:39m:19s remains)
2017-12-09 20:24:38.269248: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.1340411 0.14101502 0.14616032 0.15048155 0.15458441 0.15766263 0.15956187 0.15972294 0.15911019 0.15699708 0.15243162 0.14603111 0.13738529 0.12882517 0.12035467][0.13929044 0.14727376 0.15377526 0.16021363 0.16655037 0.17214733 0.17574126 0.17681921 0.17635918 0.17324214 0.16739 0.15908909 0.14922509 0.13944134 0.12971306][0.14310259 0.15203987 0.16022184 0.16865294 0.17722781 0.1849584 0.18997452 0.1914154 0.19062924 0.18629833 0.17871657 0.16848674 0.15678886 0.14548199 0.13432811][0.14686894 0.15658353 0.1661454 0.17658444 0.1874222 0.19692995 0.20293953 0.20434682 0.20255794 0.19689818 0.18789604 0.17565663 0.16218103 0.14948094 0.13716622][0.15213472 0.16194518 0.17219901 0.18380129 0.19596423 0.2069031 0.2141422 0.21572451 0.21308629 0.20625097 0.19598681 0.18195677 0.1668061 0.15273598 0.13978527][0.15829097 0.16811962 0.17836961 0.1903646 0.20310897 0.21450716 0.22225203 0.22447634 0.22219118 0.21502312 0.20402192 0.18891944 0.17263038 0.1573644 0.1437844][0.16284922 0.17232506 0.1823459 0.1941134 0.20676723 0.21824923 0.22621901 0.22911063 0.22743288 0.22100151 0.21029009 0.19514397 0.17871273 0.16318239 0.14930059][0.16514057 0.17437294 0.1839128 0.19526465 0.20758545 0.21878998 0.22726662 0.2312925 0.2307356 0.225692 0.21629992 0.20234911 0.18654381 0.17127346 0.15802114][0.16388445 0.17281786 0.1815006 0.19207105 0.20392537 0.21511477 0.22411065 0.2296412 0.23100667 0.22795354 0.22039041 0.20859057 0.19450258 0.18037832 0.16818218][0.15859817 0.1674739 0.17574973 0.18561837 0.19648814 0.20746328 0.21716887 0.22447731 0.22812359 0.22773769 0.22284377 0.21353859 0.20170027 0.18918373 0.17815822][0.14904732 0.15767784 0.16526647 0.1747137 0.18546456 0.19644085 0.20683068 0.21596892 0.22227645 0.22436902 0.22223262 0.21592042 0.20678514 0.19609787 0.18568236][0.13705926 0.14516863 0.15218745 0.16092826 0.17100465 0.18189448 0.19300319 0.2035147 0.21175925 0.21660146 0.21771248 0.21450599 0.20819329 0.19974291 0.19049473][0.12410163 0.13156737 0.13761143 0.14521238 0.15429924 0.16453496 0.17572582 0.18690786 0.19663866 0.20370589 0.20781738 0.20797873 0.20448634 0.19829889 0.19055344][0.11103542 0.11750028 0.1224793 0.12883852 0.13666452 0.14582552 0.15635037 0.1670851 0.17715356 0.18549678 0.19151808 0.19428152 0.19333193 0.18952975 0.18320854][0.10006604 0.10542822 0.10914809 0.11416807 0.12058066 0.1282189 0.13761668 0.147631 0.15732919 0.16581118 0.17258911 0.17644686 0.17678104 0.17404342 0.16825308]]...]
INFO - root - 2017-12-09 20:24:46.849794: step 55410, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.877 sec/batch; 67h:31m:21s remains)
INFO - root - 2017-12-09 20:24:55.374651: step 55420, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.851 sec/batch; 65h:31m:51s remains)
INFO - root - 2017-12-09 20:25:03.754945: step 55430, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.849 sec/batch; 65h:22m:35s remains)
INFO - root - 2017-12-09 20:25:12.183508: step 55440, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.870 sec/batch; 66h:56m:21s remains)
INFO - root - 2017-12-09 20:25:20.909233: step 55450, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.893 sec/batch; 68h:42m:35s remains)
INFO - root - 2017-12-09 20:25:29.497327: step 55460, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.848 sec/batch; 65h:13m:16s remains)
INFO - root - 2017-12-09 20:25:38.057417: step 55470, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.859 sec/batch; 66h:05m:46s remains)
INFO - root - 2017-12-09 20:25:46.635820: step 55480, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.861 sec/batch; 66h:15m:48s remains)
INFO - root - 2017-12-09 20:25:54.937141: step 55490, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.849 sec/batch; 65h:19m:21s remains)
INFO - root - 2017-12-09 20:26:03.460974: step 55500, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.859 sec/batch; 66h:05m:41s remains)
2017-12-09 20:26:04.383634: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.001819021 -0.0018198683 -0.0018217241 -0.0018245721 -0.0018277036 -0.0018309896 -0.0018342003 -0.0018363073 -0.0018368628 -0.0018355853 -0.0018330046 -0.0018300421 -0.0018277385 -0.001826315 -0.0018262087][-0.0018207883 -0.0018218011 -0.0018235283 -0.0018259904 -0.0018287323 -0.0018316007 -0.0018343109 -0.0018357246 -0.0018355361 -0.0018335129 -0.0018300996 -0.0018261501 -0.001822914 -0.0018208461 -0.0018205832][-0.0018239823 -0.0018250662 -0.0018265877 -0.0018285941 -0.0018307234 -0.0018328326 -0.0018347163 -0.0018352828 -0.0018342726 -0.0018316256 -0.0018274792 -0.0018225102 -0.0018183491 -0.001815886 -0.0018156383][-0.0018274972 -0.0018283273 -0.0018295627 -0.0018311488 -0.0018325963 -0.0018338206 -0.0018346574 -0.0018345696 -0.0018331184 -0.0018302114 -0.0018256044 -0.0018199249 -0.0018150712 -0.0018121921 -0.0018118898][-0.0018311312 -0.0018314446 -0.0018321913 -0.0018332355 -0.0018339468 -0.0018342878 -0.0018342271 -0.001833713 -0.0018322553 -0.0018295997 -0.0018250177 -0.0018190028 -0.0018137189 -0.0018105924 -0.0018100918][-0.0018349336 -0.0018345249 -0.0018345332 -0.0018345548 -0.0018341605 -0.0018336496 -0.0018330317 -0.0018324616 -0.0018315313 -0.0018295532 -0.0018253955 -0.0018193242 -0.0018138334 -0.0018104846 -0.0018097946][-0.001838 -0.0018366469 -0.0018354927 -0.0018342697 -0.001832785 -0.0018313942 -0.0018305824 -0.001830522 -0.001830456 -0.0018292054 -0.0018254581 -0.0018195725 -0.0018141679 -0.0018109382 -0.0018102778][-0.0018393556 -0.0018370489 -0.0018348293 -0.0018326141 -0.0018303331 -0.0018284109 -0.0018276025 -0.0018282877 -0.001829244 -0.0018286922 -0.0018253438 -0.0018197901 -0.0018146555 -0.0018117492 -0.001811411][-0.001839462 -0.001836547 -0.0018335576 -0.0018306569 -0.0018278081 -0.0018258397 -0.0018255601 -0.0018269884 -0.0018286362 -0.0018286216 -0.0018260038 -0.0018212214 -0.0018167023 -0.0018140237 -0.00181364][-0.0018386657 -0.0018354144 -0.0018317561 -0.0018282095 -0.0018250985 -0.0018234239 -0.0018237815 -0.0018257928 -0.0018278267 -0.0018283444 -0.0018267126 -0.001823295 -0.0018198923 -0.0018177917 -0.0018174569][-0.0018371633 -0.0018341667 -0.0018302501 -0.0018262756 -0.0018230369 -0.001821607 -0.0018223234 -0.0018245757 -0.0018266669 -0.0018273997 -0.0018265255 -0.0018244421 -0.0018222122 -0.0018208881 -0.0018209087][-0.0018347895 -0.0018319098 -0.0018282448 -0.0018245439 -0.0018215214 -0.0018203047 -0.0018212235 -0.0018234048 -0.0018253195 -0.0018261002 -0.0018257347 -0.0018244821 -0.0018232042 -0.0018225505 -0.0018227492][-0.0018314184 -0.0018288386 -0.0018256499 -0.0018225457 -0.0018201793 -0.0018193608 -0.00182017 -0.0018219858 -0.001823693 -0.0018245656 -0.0018247752 -0.0018242837 -0.001823621 -0.0018232225 -0.0018234138][-0.0018281442 -0.0018258892 -0.0018233265 -0.0018209739 -0.0018193154 -0.0018188572 -0.0018195689 -0.0018208169 -0.0018220634 -0.001822861 -0.0018232991 -0.0018233996 -0.0018232157 -0.0018231154 -0.0018233041][-0.001825172 -0.0018232099 -0.0018214269 -0.0018198837 -0.0018188911 -0.0018186639 -0.0018191196 -0.0018198288 -0.0018205462 -0.0018209763 -0.0018211982 -0.0018212944 -0.0018213358 -0.0018215361 -0.0018217671]]...]
INFO - root - 2017-12-09 20:26:12.935604: step 55510, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.875 sec/batch; 67h:17m:10s remains)
INFO - root - 2017-12-09 20:26:21.495335: step 55520, loss = 0.81, batch loss = 0.68 (9.1 examples/sec; 0.877 sec/batch; 67h:28m:18s remains)
INFO - root - 2017-12-09 20:26:30.023712: step 55530, loss = 0.82, batch loss = 0.69 (9.9 examples/sec; 0.807 sec/batch; 62h:02m:59s remains)
INFO - root - 2017-12-09 20:26:38.566405: step 55540, loss = 0.83, batch loss = 0.70 (9.3 examples/sec; 0.857 sec/batch; 65h:56m:52s remains)
INFO - root - 2017-12-09 20:26:47.035745: step 55550, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.834 sec/batch; 64h:07m:59s remains)
INFO - root - 2017-12-09 20:26:55.653279: step 55560, loss = 0.83, batch loss = 0.70 (9.0 examples/sec; 0.888 sec/batch; 68h:16m:46s remains)
INFO - root - 2017-12-09 20:27:04.302588: step 55570, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.891 sec/batch; 68h:31m:16s remains)
INFO - root - 2017-12-09 20:27:13.153796: step 55580, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.865 sec/batch; 66h:30m:13s remains)
INFO - root - 2017-12-09 20:27:21.803777: step 55590, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.837 sec/batch; 64h:22m:45s remains)
INFO - root - 2017-12-09 20:27:30.528732: step 55600, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.873 sec/batch; 67h:10m:15s remains)
2017-12-09 20:27:31.401687: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0016999235 -0.0017067316 -0.0017285096 -0.0017565981 -0.0017837537 -0.0018065901 -0.0018199587 -0.001824806 -0.0018246772 -0.0018228504 -0.001821016 -0.0018192463 -0.0018176723 -0.0018171954 -0.001817615][-0.0016642989 -0.0016711554 -0.0016988359 -0.0017344723 -0.0017691597 -0.0017972026 -0.0018144134 -0.0018220914 -0.0018235908 -0.0018230397 -0.0018218256 -0.0018205621 -0.0018189583 -0.001817998 -0.0018181384][-0.0016367655 -0.0016457818 -0.0016793988 -0.0017201043 -0.001757785 -0.001787335 -0.0018065372 -0.0018164916 -0.001819912 -0.0018210493 -0.0018221311 -0.0018223514 -0.0018215533 -0.0018208738 -0.001821088][-0.0016194804 -0.0016286414 -0.0016670689 -0.0017112322 -0.0017502072 -0.0017784839 -0.0017969673 -0.0018079274 -0.0018134934 -0.0018167262 -0.0018200482 -0.0018225034 -0.0018232516 -0.0018234081 -0.0018244997][-0.0016163776 -0.0016260894 -0.0016644656 -0.0017058684 -0.0017402228 -0.0017645195 -0.001782198 -0.0017949964 -0.0018037073 -0.0018103441 -0.0018165533 -0.001821799 -0.0018244645 -0.0018254999 -0.0018269218][-0.0016245119 -0.001635823 -0.001672343 -0.0017083419 -0.001735386 -0.0017529469 -0.0017675159 -0.0017807372 -0.0017927417 -0.0018032746 -0.0018126336 -0.0018198153 -0.0018240282 -0.0018257682 -0.0018273675][-0.0016479004 -0.001658632 -0.0016910345 -0.0017214858 -0.0017417031 -0.0017525435 -0.0017621394 -0.0017738155 -0.0017873567 -0.0018001299 -0.0018112581 -0.0018185853 -0.0018226474 -0.0018247464 -0.001826226][-0.0016861393 -0.0016956425 -0.0017222613 -0.0017465182 -0.0017603817 -0.0017654011 -0.0017691754 -0.0017769071 -0.0017885164 -0.0018009153 -0.0018116947 -0.0018181652 -0.0018219996 -0.0018236237 -0.0018248084][-0.0017332269 -0.0017408361 -0.0017612017 -0.0017781988 -0.0017860018 -0.0017856839 -0.0017842404 -0.0017871953 -0.0017950353 -0.001804832 -0.0018134825 -0.0018183462 -0.0018213644 -0.001822623 -0.0018233896][-0.0017782261 -0.0017838256 -0.001796823 -0.0018061294 -0.0018091083 -0.0018057985 -0.0018018044 -0.0018010384 -0.0018048544 -0.0018107776 -0.0018159465 -0.0018189431 -0.0018208766 -0.0018216317 -0.0018219344][-0.0018082792 -0.0018106112 -0.0018167547 -0.001820588 -0.0018217135 -0.0018191396 -0.0018165474 -0.0018144082 -0.0018146635 -0.0018164195 -0.0018178837 -0.0018188875 -0.0018196048 -0.0018199621 -0.0018200751][-0.0018215558 -0.0018212942 -0.001823041 -0.0018243634 -0.0018251779 -0.0018241936 -0.0018233922 -0.001821653 -0.0018208158 -0.0018198119 -0.0018185897 -0.0018177679 -0.0018175506 -0.001817962 -0.0018180684][-0.001825329 -0.0018239284 -0.0018239386 -0.0018243737 -0.0018249613 -0.0018245614 -0.0018243535 -0.0018228129 -0.0018217595 -0.001820171 -0.0018188681 -0.0018171464 -0.0018165949 -0.0018167949 -0.0018169731][-0.0018252726 -0.0018238729 -0.0018236641 -0.0018234826 -0.0018236957 -0.0018231602 -0.0018229032 -0.0018216314 -0.0018210008 -0.0018196803 -0.0018187223 -0.001817381 -0.0018168561 -0.0018169665 -0.0018170123][-0.0018245934 -0.0018232343 -0.0018226816 -0.0018223701 -0.0018221361 -0.0018213462 -0.0018212812 -0.0018205802 -0.001820185 -0.0018191151 -0.0018187682 -0.0018178595 -0.001817659 -0.0018174028 -0.0018172968]]...]
INFO - root - 2017-12-09 20:27:39.975490: step 55610, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.882 sec/batch; 67h:51m:37s remains)
INFO - root - 2017-12-09 20:27:48.563959: step 55620, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.854 sec/batch; 65h:41m:49s remains)
INFO - root - 2017-12-09 20:27:57.025656: step 55630, loss = 0.82, batch loss = 0.69 (11.1 examples/sec; 0.718 sec/batch; 55h:14m:42s remains)
INFO - root - 2017-12-09 20:28:05.472274: step 55640, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.853 sec/batch; 65h:36m:01s remains)
INFO - root - 2017-12-09 20:28:14.028334: step 55650, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.850 sec/batch; 65h:22m:20s remains)
INFO - root - 2017-12-09 20:28:22.624888: step 55660, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 66h:39m:18s remains)
INFO - root - 2017-12-09 20:28:31.365238: step 55670, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.857 sec/batch; 65h:51m:49s remains)
INFO - root - 2017-12-09 20:28:39.964900: step 55680, loss = 0.81, batch loss = 0.68 (9.3 examples/sec; 0.863 sec/batch; 66h:23m:31s remains)
INFO - root - 2017-12-09 20:28:48.584131: step 55690, loss = 0.82, batch loss = 0.69 (10.5 examples/sec; 0.760 sec/batch; 58h:27m:45s remains)
INFO - root - 2017-12-09 20:28:57.186498: step 55700, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.886 sec/batch; 68h:06m:09s remains)
2017-12-09 20:28:58.093783: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.0045031263 0.0088870851 0.013840689 0.018364234 0.021479968 0.022402441 0.021219635 0.018466394 0.014932019 0.01104102 0.0073966873 0.0044937166 0.0025525615 0.0014883078 0.00082987186][0.0069854385 0.01355252 0.021515582 0.029235762 0.035090983 0.037774865 0.036828335 0.032862268 0.027068622 0.020246612 0.013687977 0.0083641112 0.0048720143 0.0030152956 0.002105746][0.010938857 0.019965181 0.031337339 0.042895675 0.052112002 0.057189114 0.057204425 0.052692272 0.044797357 0.034578666 0.024081392 0.014996658 0.0085998178 0.0049613854 0.0033858297][0.01658228 0.028626885 0.044076625 0.060392451 0.073715113 0.081472561 0.082136057 0.076505922 0.065945946 0.051826164 0.037188634 0.024190297 0.014529171 0.0084500313 0.0055023408][0.023505991 0.0388222 0.05839584 0.079154223 0.096221305 0.10655754 0.1078366 0.10133219 0.088118263 0.069999315 0.050887454 0.033590719 0.02051294 0.011903739 0.0076839491][0.030070737 0.04801837 0.071031809 0.095435947 0.11562778 0.12773672 0.12939465 0.12183393 0.10618643 0.085148856 0.062558949 0.041730229 0.025452562 0.014319939 0.0086752474][0.03544933 0.054454129 0.079011373 0.10534073 0.12722668 0.14037661 0.14215121 0.13343273 0.11584484 0.092926718 0.068723045 0.046456914 0.02885537 0.016612494 0.010146811][0.037412822 0.056544829 0.081369705 0.10817096 0.13079445 0.14438552 0.14630944 0.13697451 0.11791971 0.093894631 0.069079973 0.046853624 0.029501513 0.017776119 0.0119782][0.037136469 0.055143673 0.078809351 0.10454781 0.12653224 0.13985439 0.14151716 0.13178535 0.11274482 0.089184821 0.065690458 0.045428421 0.030367017 0.020926286 0.016886625][0.034964703 0.0506477 0.071631655 0.094563685 0.11423448 0.12593167 0.1265374 0.11678599 0.099095523 0.077857025 0.057446733 0.040666286 0.029174816 0.023218244 0.022575142][0.034949232 0.047247272 0.063614979 0.081329256 0.096255668 0.10444852 0.10343593 0.093862541 0.078445762 0.060701244 0.044715781 0.032802142 0.025999954 0.025027538 0.029347189][0.042788524 0.051315047 0.061467014 0.071625389 0.078967139 0.081040278 0.076740451 0.066926688 0.053944983 0.040556069 0.029663265 0.023232307 0.022327546 0.027742358 0.038622241][0.060403425 0.064907186 0.069008976 0.071787909 0.071235113 0.0660831 0.056821484 0.045238871 0.033369776 0.023454148 0.016828334 0.015096194 0.019272301 0.030859776 0.048152197][0.088469647 0.090104714 0.088502161 0.084105156 0.075991966 0.063925415 0.0494148 0.034891758 0.022598837 0.014100623 0.010005243 0.011377955 0.019323604 0.035752703 0.058162704][0.11932451 0.11979807 0.11498629 0.10565976 0.091437116 0.0728402 0.052324686 0.033365056 0.018727908 0.0098346043 0.006574539 0.0096764816 0.02093414 0.04167936 0.068641923]]...]
INFO - root - 2017-12-09 20:29:06.820267: step 55710, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.850 sec/batch; 65h:22m:09s remains)
INFO - root - 2017-12-09 20:29:15.563021: step 55720, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.888 sec/batch; 68h:17m:33s remains)
INFO - root - 2017-12-09 20:29:24.163948: step 55730, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.876 sec/batch; 67h:21m:17s remains)
INFO - root - 2017-12-09 20:29:32.761593: step 55740, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.877 sec/batch; 67h:26m:59s remains)
INFO - root - 2017-12-09 20:29:41.470262: step 55750, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.859 sec/batch; 66h:00m:57s remains)
INFO - root - 2017-12-09 20:29:50.430095: step 55760, loss = 0.82, batch loss = 0.69 (8.7 examples/sec; 0.919 sec/batch; 70h:37m:37s remains)
INFO - root - 2017-12-09 20:29:59.130126: step 55770, loss = 0.82, batch loss = 0.69 (8.8 examples/sec; 0.912 sec/batch; 70h:06m:54s remains)
INFO - root - 2017-12-09 20:30:07.746627: step 55780, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.878 sec/batch; 67h:28m:39s remains)
INFO - root - 2017-12-09 20:30:16.291349: step 55790, loss = 0.82, batch loss = 0.69 (10.9 examples/sec; 0.737 sec/batch; 56h:37m:38s remains)
INFO - root - 2017-12-09 20:30:25.176107: step 55800, loss = 0.82, batch loss = 0.69 (8.1 examples/sec; 0.986 sec/batch; 75h:46m:08s remains)
2017-12-09 20:30:26.051918: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0017504346 -0.0017466792 -0.0017431909 -0.0017398744 -0.0017360492 -0.0017333244 -0.00173207 -0.0017318921 -0.0017326603 -0.0017337627 -0.0017340035 -0.0017340865 -0.0017332409 -0.0017303535 -0.0017262216][-0.0017562124 -0.0017530873 -0.0017503886 -0.0017472582 -0.0017434843 -0.0017405869 -0.0017391563 -0.0017381161 -0.0017381663 -0.0017393966 -0.001740295 -0.0017411631 -0.0017410776 -0.0017391768 -0.0017353465][-0.0017714802 -0.0017685525 -0.00176556 -0.0017624181 -0.001758656 -0.0017556171 -0.0017539782 -0.0017527957 -0.0017532127 -0.0017548073 -0.0017569251 -0.0017587035 -0.0017590924 -0.0017576959 -0.0017543989][-0.0017905242 -0.0017876595 -0.001784327 -0.0017802819 -0.001775804 -0.0017720615 -0.0017701096 -0.0017692831 -0.0017702981 -0.0017725321 -0.0017757043 -0.0017780667 -0.0017792035 -0.0017788048 -0.0017765288][-0.0018064182 -0.0018027843 -0.0017987672 -0.0017942182 -0.0017892292 -0.0017856981 -0.001784722 -0.0017855804 -0.001787728 -0.0017909135 -0.0017947052 -0.0017974272 -0.0017984769 -0.0017986507 -0.0017978568][-0.0018190873 -0.001814349 -0.0018090211 -0.0018033703 -0.0017980161 -0.0017951042 -0.001795736 -0.0017985071 -0.001801881 -0.0018053413 -0.0018082727 -0.0018103438 -0.0018112474 -0.0018118324 -0.0018118769][-0.0018272165 -0.0018223465 -0.0018164301 -0.0018105318 -0.00180556 -0.0018035759 -0.0018059006 -0.0018099134 -0.0018136185 -0.0018166128 -0.001818571 -0.0018193393 -0.0018190918 -0.0018189942 -0.0018189393][-0.0018317607 -0.0018274678 -0.0018219026 -0.001816339 -0.0018118041 -0.0018103505 -0.0018135664 -0.001818026 -0.0018216542 -0.0018235814 -0.0018238683 -0.001823149 -0.0018216796 -0.0018206477 -0.0018200772][-0.0018332999 -0.0018295257 -0.0018249775 -0.0018212241 -0.001818122 -0.0018169959 -0.0018191827 -0.0018224653 -0.0018245185 -0.0018246962 -0.0018235535 -0.0018219204 -0.0018202077 -0.0018190425 -0.0018185213][-0.0018326844 -0.0018298241 -0.0018265519 -0.0018248762 -0.0018237346 -0.0018233848 -0.0018246947 -0.0018260191 -0.0018258601 -0.0018241705 -0.0018221607 -0.0018200976 -0.0018182399 -0.001817005 -0.0018164264][-0.0018311769 -0.0018290041 -0.0018272389 -0.0018272891 -0.0018282215 -0.0018289225 -0.0018292812 -0.001828576 -0.0018265807 -0.0018234965 -0.0018209938 -0.001818943 -0.0018171904 -0.0018163281 -0.0018157858][-0.0018295349 -0.0018285296 -0.0018281614 -0.0018295546 -0.0018311387 -0.0018319084 -0.0018314103 -0.0018294777 -0.0018264494 -0.0018227566 -0.0018202647 -0.0018187445 -0.0018174787 -0.0018168176 -0.0018164102][-0.0018272897 -0.0018270914 -0.0018279245 -0.001830288 -0.0018320775 -0.0018327957 -0.0018316545 -0.0018293615 -0.0018262845 -0.0018228709 -0.0018207238 -0.0018194859 -0.0018185581 -0.0018181409 -0.0018178669][-0.0018241175 -0.001824851 -0.0018264788 -0.0018297308 -0.0018321049 -0.0018323181 -0.0018306747 -0.001828147 -0.0018252203 -0.0018220848 -0.0018200529 -0.0018189048 -0.0018181533 -0.0018179684 -0.0018179866][-0.001821346 -0.0018215729 -0.0018235896 -0.0018271651 -0.0018297186 -0.0018304036 -0.0018291011 -0.0018268019 -0.0018242947 -0.0018217757 -0.0018201263 -0.0018191144 -0.0018187223 -0.0018188301 -0.0018186952]]...]
INFO - root - 2017-12-09 20:30:34.703450: step 55810, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.846 sec/batch; 65h:00m:51s remains)
INFO - root - 2017-12-09 20:30:43.277307: step 55820, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.885 sec/batch; 68h:00m:28s remains)
INFO - root - 2017-12-09 20:30:51.945594: step 55830, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.874 sec/batch; 67h:11m:57s remains)
INFO - root - 2017-12-09 20:31:00.403932: step 55840, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.851 sec/batch; 65h:23m:44s remains)
INFO - root - 2017-12-09 20:31:09.044032: step 55850, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.859 sec/batch; 66h:02m:27s remains)
INFO - root - 2017-12-09 20:31:17.681179: step 55860, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.829 sec/batch; 63h:42m:31s remains)
INFO - root - 2017-12-09 20:31:26.268820: step 55870, loss = 0.83, batch loss = 0.70 (9.3 examples/sec; 0.864 sec/batch; 66h:24m:49s remains)
INFO - root - 2017-12-09 20:31:34.971831: step 55880, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.837 sec/batch; 64h:19m:59s remains)
INFO - root - 2017-12-09 20:31:43.648792: step 55890, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.861 sec/batch; 66h:10m:44s remains)
INFO - root - 2017-12-09 20:31:51.978537: step 55900, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.847 sec/batch; 65h:05m:45s remains)
2017-12-09 20:31:52.840282: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.58636719 0.563005 0.53563565 0.51051772 0.48754832 0.47030491 0.45920125 0.45093778 0.44459075 0.43745023 0.43166783 0.42784095 0.42597049 0.42518166 0.42532054][0.57420063 0.55786496 0.53757167 0.5170716 0.49731821 0.4810718 0.47146407 0.46476248 0.45907387 0.45140016 0.44423044 0.43851504 0.43386674 0.430455 0.4287383][0.550895 0.54441786 0.53463244 0.52332181 0.51030356 0.49690595 0.48795906 0.48015594 0.47205368 0.46227047 0.45272094 0.44427451 0.4367134 0.43047076 0.42635402][0.52559656 0.53050554 0.53291708 0.53285933 0.52882552 0.52117133 0.51393193 0.50373614 0.49130872 0.47772285 0.464491 0.45241457 0.44125921 0.43252 0.426473][0.50205886 0.51669753 0.5295313 0.54059738 0.54694515 0.54629439 0.54171324 0.53023738 0.51477474 0.49647135 0.47809553 0.46165633 0.44674966 0.43534085 0.42667866][0.48422709 0.50672 0.52756149 0.54737705 0.562851 0.56933242 0.56876874 0.55769974 0.53944153 0.51737756 0.49417391 0.47348088 0.45488125 0.44084123 0.43048364][0.47156662 0.49786514 0.52265143 0.548617 0.57002389 0.58122677 0.58372265 0.57512128 0.55718708 0.53428781 0.50982952 0.48671353 0.46581894 0.45009989 0.43812197][0.45734495 0.48600069 0.51234078 0.54064077 0.56507075 0.57971305 0.58524048 0.57898951 0.56242621 0.54072332 0.51716268 0.49343383 0.47175705 0.45544851 0.44289467][0.44413638 0.47239017 0.49720278 0.5250867 0.55036449 0.56665987 0.57488614 0.57103711 0.55671763 0.53632653 0.51458997 0.49276093 0.47261009 0.45751536 0.44610754][0.43005469 0.45545721 0.4756147 0.49984783 0.5236274 0.54036069 0.55064917 0.55061209 0.54099059 0.52429086 0.50573218 0.48666391 0.46942207 0.45656934 0.44683558][0.41489461 0.43611619 0.45066848 0.4695608 0.48890632 0.50338238 0.5133884 0.51643956 0.51176244 0.50140953 0.48980284 0.4770267 0.46509385 0.45576867 0.4485203][0.40015322 0.41744831 0.42653286 0.439673 0.45385873 0.46516421 0.474072 0.47835895 0.47803485 0.47452956 0.47055179 0.46492881 0.4592979 0.45480904 0.45021647][0.38579574 0.40042245 0.40600973 0.4142631 0.42393419 0.43186864 0.43903694 0.44337502 0.44565767 0.44758791 0.45025596 0.45225587 0.45324382 0.45385075 0.45254374][0.37404808 0.38634261 0.38862151 0.39398786 0.40093064 0.40739685 0.41390571 0.41920733 0.42470983 0.43120492 0.43945217 0.44809937 0.45567393 0.46101868 0.46239263][0.36261481 0.37361184 0.37371522 0.37631473 0.3803862 0.38555947 0.39236403 0.40037373 0.41076064 0.42338741 0.43796274 0.45279586 0.46543467 0.47391152 0.4762651]]...]
INFO - root - 2017-12-09 20:32:01.379067: step 55910, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.831 sec/batch; 63h:49m:54s remains)
INFO - root - 2017-12-09 20:32:10.142094: step 55920, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.870 sec/batch; 66h:52m:37s remains)
INFO - root - 2017-12-09 20:32:18.837282: step 55930, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.853 sec/batch; 65h:33m:11s remains)
INFO - root - 2017-12-09 20:32:27.295507: step 55940, loss = 0.83, batch loss = 0.70 (9.1 examples/sec; 0.877 sec/batch; 67h:22m:46s remains)
INFO - root - 2017-12-09 20:32:36.090274: step 55950, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.863 sec/batch; 66h:18m:58s remains)
INFO - root - 2017-12-09 20:32:44.795984: step 55960, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.879 sec/batch; 67h:31m:26s remains)
INFO - root - 2017-12-09 20:32:53.472727: step 55970, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.847 sec/batch; 65h:01m:50s remains)
INFO - root - 2017-12-09 20:33:02.218322: step 55980, loss = 0.81, batch loss = 0.68 (9.1 examples/sec; 0.883 sec/batch; 67h:48m:28s remains)
INFO - root - 2017-12-09 20:33:10.966855: step 55990, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.846 sec/batch; 64h:58m:37s remains)
INFO - root - 2017-12-09 20:33:19.431558: step 56000, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.831 sec/batch; 63h:51m:20s remains)
2017-12-09 20:33:20.429844: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0017984099 -0.0017990057 -0.0018015143 -0.0018046149 -0.0018077018 -0.0018104465 -0.001812615 -0.0018139275 -0.0018142872 -0.0018142968 -0.0018145413 -0.0018142033 -0.0018131034 -0.0018123904 -0.0018115439][-0.0017985359 -0.0017996001 -0.0018026261 -0.0018060911 -0.0018092641 -0.0018117926 -0.0018135452 -0.00181424 -0.0018138876 -0.00181313 -0.0018125689 -0.0018117427 -0.0018104623 -0.0018100197 -0.0018097438][-0.0017999854 -0.0018018238 -0.0018057831 -0.0018098343 -0.0018130203 -0.001815256 -0.0018164831 -0.0018163687 -0.0018150066 -0.0018133746 -0.0018119842 -0.0018105613 -0.0018090283 -0.0018086898 -0.0018088499][-0.0018018889 -0.0018045283 -0.0018090786 -0.001813306 -0.0018162874 -0.0018178819 -0.0018184254 -0.0018177022 -0.0018159612 -0.0018140321 -0.0018119719 -0.0018099807 -0.001808016 -0.0018071819 -0.001807143][-0.0018039432 -0.0018070688 -0.0018117268 -0.0018158435 -0.0018184814 -0.0018193806 -0.0018192981 -0.0018183372 -0.0018167349 -0.00181502 -0.0018126045 -0.0018098347 -0.0018068849 -0.0018050481 -0.0018040407][-0.0018055054 -0.0018091539 -0.0018137749 -0.0018175478 -0.0018196198 -0.00181991 -0.001819359 -0.001818304 -0.001817056 -0.0018157221 -0.0018132009 -0.0018097217 -0.001805658 -0.0018024306 -0.0018002188][-0.001806507 -0.0018104357 -0.0018150236 -0.0018186123 -0.0018203858 -0.0018202579 -0.0018194078 -0.0018181944 -0.0018170266 -0.0018157003 -0.0018130617 -0.0018093506 -0.0018047843 -0.0018008357 -0.0017980004][-0.0018062969 -0.001810136 -0.0018145351 -0.0018178745 -0.0018192492 -0.0018187324 -0.0018175999 -0.0018162248 -0.0018149681 -0.001813652 -0.0018112558 -0.0018079367 -0.0018037031 -0.001799773 -0.0017968552][-0.001805553 -0.0018088182 -0.0018124812 -0.0018152234 -0.001816217 -0.001815408 -0.0018140054 -0.0018125261 -0.0018113076 -0.0018100836 -0.0018079173 -0.0018049991 -0.0018013818 -0.0017979813 -0.0017954496][-0.0018042603 -0.0018065826 -0.0018095797 -0.0018118689 -0.0018126711 -0.0018119336 -0.0018106832 -0.0018093464 -0.0018080714 -0.0018066441 -0.0018045218 -0.0018018465 -0.0017987541 -0.0017959743 -0.001793936][-0.0018022169 -0.0018039831 -0.0018066615 -0.0018087175 -0.001809633 -0.0018093043 -0.0018084272 -0.0018071319 -0.0018056887 -0.00180397 -0.0018017968 -0.0017993356 -0.0017968107 -0.0017946423 -0.0017931116][-0.0018004389 -0.0018014781 -0.0018035523 -0.0018052525 -0.0018062444 -0.0018062557 -0.0018058162 -0.0018048913 -0.0018035443 -0.0018018277 -0.0017997382 -0.0017976124 -0.0017956267 -0.0017941117 -0.0017930859][-0.001799016 -0.0017992008 -0.0018005244 -0.0018015301 -0.0018021584 -0.0018021284 -0.0018019301 -0.001801485 -0.0018006372 -0.0017994348 -0.0017978611 -0.0017963144 -0.0017948874 -0.0017937896 -0.0017930504][-0.0017976981 -0.0017971764 -0.0017978838 -0.0017982812 -0.0017985123 -0.0017983838 -0.0017983431 -0.0017981931 -0.0017978097 -0.0017971823 -0.0017962209 -0.0017952032 -0.0017942587 -0.0017935631 -0.0017930592][-0.0017964428 -0.001795633 -0.0017959387 -0.0017960892 -0.0017961744 -0.0017960471 -0.0017960764 -0.0017960523 -0.0017959349 -0.0017956853 -0.0017951231 -0.0017945328 -0.001793955 -0.001793521 -0.0017931869]]...]
INFO - root - 2017-12-09 20:33:29.203016: step 56010, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.870 sec/batch; 66h:49m:29s remains)
INFO - root - 2017-12-09 20:33:37.921236: step 56020, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.880 sec/batch; 67h:36m:29s remains)
INFO - root - 2017-12-09 20:33:46.652576: step 56030, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.838 sec/batch; 64h:19m:36s remains)
INFO - root - 2017-12-09 20:33:55.017959: step 56040, loss = 0.82, batch loss = 0.69 (9.9 examples/sec; 0.806 sec/batch; 61h:55m:49s remains)
INFO - root - 2017-12-09 20:34:03.511317: step 56050, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.858 sec/batch; 65h:53m:27s remains)
INFO - root - 2017-12-09 20:34:12.070282: step 56060, loss = 0.83, batch loss = 0.70 (9.4 examples/sec; 0.848 sec/batch; 65h:07m:21s remains)
INFO - root - 2017-12-09 20:34:20.704828: step 56070, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 66h:51m:28s remains)
INFO - root - 2017-12-09 20:34:29.419161: step 56080, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.863 sec/batch; 66h:14m:02s remains)
INFO - root - 2017-12-09 20:34:38.162870: step 56090, loss = 0.83, batch loss = 0.70 (9.2 examples/sec; 0.871 sec/batch; 66h:51m:34s remains)
INFO - root - 2017-12-09 20:34:46.770837: step 56100, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.892 sec/batch; 68h:28m:12s remains)
2017-12-09 20:34:47.787886: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0017935876 -0.001791839 -0.0017914114 -0.0017911858 -0.001790975 -0.0017907753 -0.0017906355 -0.0017906019 -0.0017906785 -0.0017907948 -0.0017908151 -0.0017907661 -0.0017907274 -0.0017907943 -0.0017909701][-0.0017922013 -0.0017905615 -0.0017902966 -0.0017902913 -0.001790324 -0.0017903234 -0.0017903124 -0.0017903175 -0.0017903476 -0.0017903238 -0.0017902147 -0.0017901056 -0.0017900767 -0.0017901618 -0.001790415][-0.0017918042 -0.0017903356 -0.0017902753 -0.0017905285 -0.0017908369 -0.0017910769 -0.0017911871 -0.0017911926 -0.0017911514 -0.0017909944 -0.0017908055 -0.0017906794 -0.0017907075 -0.0017908979 -0.001791212][-0.001791622 -0.0017903454 -0.0017904952 -0.001790987 -0.0017915096 -0.0017918777 -0.0017919983 -0.0017919464 -0.0017918283 -0.0017916207 -0.0017915062 -0.0017915312 -0.0017917223 -0.0017920247 -0.0017923829][-0.0017918537 -0.0017907128 -0.0017910381 -0.0017916986 -0.0017923064 -0.0017926417 -0.0017926351 -0.0017924303 -0.0017922438 -0.0017920516 -0.0017921056 -0.0017924038 -0.0017928246 -0.0017932446 -0.001793623][-0.001792482 -0.0017914211 -0.0017918231 -0.0017925084 -0.0017930571 -0.0017933168 -0.0017932102 -0.0017929114 -0.0017927298 -0.0017926656 -0.0017929649 -0.001793518 -0.0017941056 -0.0017945856 -0.0017949587][-0.0017933842 -0.0017923517 -0.0017927287 -0.0017933266 -0.0017937466 -0.001793887 -0.0017936886 -0.0017933678 -0.0017933204 -0.0017935085 -0.0017940861 -0.0017948485 -0.0017955488 -0.0017960325 -0.0017963538][-0.0017947499 -0.0017936889 -0.001793988 -0.0017944247 -0.001794636 -0.0017946132 -0.0017943525 -0.0017940681 -0.0017941631 -0.001794552 -0.0017952832 -0.0017961143 -0.0017968076 -0.0017972366 -0.0017975033][-0.0017964452 -0.001795317 -0.0017955385 -0.0017958366 -0.0017958693 -0.001795713 -0.0017954407 -0.0017952197 -0.0017953379 -0.0017957356 -0.0017964363 -0.0017971782 -0.0017977568 -0.0017980902 -0.0017983007][-0.0017980647 -0.0017969208 -0.0017970551 -0.0017972367 -0.001797193 -0.0017970218 -0.0017967958 -0.001796594 -0.0017966517 -0.0017969336 -0.0017974477 -0.0017979904 -0.0017984078 -0.0017986464 -0.0017988125][-0.0017993129 -0.0017981364 -0.0017981344 -0.0017982572 -0.0017982363 -0.0017981414 -0.001797992 -0.0017978278 -0.0017978295 -0.0017979895 -0.0017983107 -0.0017986657 -0.0017989598 -0.0017991265 -0.0017992607][-0.0018000597 -0.0017987761 -0.0017986817 -0.0017987906 -0.0017988149 -0.001798777 -0.0017986631 -0.0017985388 -0.0017985326 -0.0017986501 -0.0017988557 -0.0017990778 -0.0017992854 -0.0017994009 -0.0017994834][-0.0018004236 -0.0017990329 -0.0017988328 -0.0017989139 -0.0017989251 -0.0017988773 -0.0017987771 -0.0017986927 -0.0017987109 -0.0017988151 -0.0017989709 -0.0017991301 -0.0017992685 -0.0017993323 -0.0017993706][-0.0018006025 -0.0017990973 -0.0017987947 -0.0017988246 -0.0017987916 -0.0017986922 -0.0017985683 -0.0017984916 -0.0017985285 -0.0017986327 -0.0017987657 -0.0017988991 -0.0017989934 -0.0017990228 -0.0017990243][-0.0018004985 -0.0017989543 -0.0017985804 -0.0017985781 -0.0017985279 -0.0017984137 -0.0017982771 -0.0017981975 -0.0017982268 -0.0017983136 -0.0017984076 -0.0017984982 -0.0017985561 -0.001798569 -0.0017985513]]...]
INFO - root - 2017-12-09 20:34:56.411214: step 56110, loss = 0.83, batch loss = 0.70 (9.3 examples/sec; 0.856 sec/batch; 65h:44m:22s remains)
INFO - root - 2017-12-09 20:35:05.086162: step 56120, loss = 0.81, batch loss = 0.68 (9.0 examples/sec; 0.886 sec/batch; 68h:02m:07s remains)
INFO - root - 2017-12-09 20:35:13.801866: step 56130, loss = 0.83, batch loss = 0.70 (8.9 examples/sec; 0.901 sec/batch; 69h:09m:04s remains)
INFO - root - 2017-12-09 20:35:22.255823: step 56140, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.856 sec/batch; 65h:41m:33s remains)
INFO - root - 2017-12-09 20:35:31.039990: step 56150, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.857 sec/batch; 65h:47m:22s remains)
INFO - root - 2017-12-09 20:35:39.814343: step 56160, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.875 sec/batch; 67h:10m:53s remains)
INFO - root - 2017-12-09 20:35:48.451951: step 56170, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 66h:37m:09s remains)
INFO - root - 2017-12-09 20:35:57.259480: step 56180, loss = 0.82, batch loss = 0.69 (8.8 examples/sec; 0.908 sec/batch; 69h:41m:45s remains)
INFO - root - 2017-12-09 20:36:05.943101: step 56190, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.881 sec/batch; 67h:34m:51s remains)
INFO - root - 2017-12-09 20:36:14.576388: step 56200, loss = 0.82, batch loss = 0.69 (8.8 examples/sec; 0.908 sec/batch; 69h:42m:36s remains)
2017-12-09 20:36:15.468623: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.24603283 0.24285054 0.23888914 0.23388016 0.22890918 0.22369544 0.2183525 0.2123514 0.20669387 0.20090809 0.19447371 0.18931699 0.18562606 0.18264329 0.18094817][0.25080848 0.24995086 0.24795339 0.24448092 0.24107966 0.23721378 0.23358434 0.22950217 0.22554952 0.22127095 0.21569909 0.2102944 0.20539986 0.20034215 0.19598506][0.2521728 0.2532534 0.25335428 0.25155678 0.24987465 0.24785891 0.24622464 0.2447736 0.24315472 0.24107552 0.23675953 0.23107257 0.22486675 0.21724539 0.20955025][0.25267226 0.25634217 0.25872898 0.25890419 0.25950837 0.25928438 0.25971666 0.26065767 0.26099274 0.26037541 0.25677869 0.2509644 0.24309337 0.23273024 0.22149532][0.24982694 0.25647759 0.2604675 0.26302615 0.26611179 0.26832852 0.27071509 0.2735537 0.27595681 0.27672157 0.27358741 0.26711959 0.25781772 0.24502921 0.23053527][0.24449486 0.25354439 0.25853822 0.26265457 0.26770052 0.272105 0.27677107 0.28129122 0.28525707 0.28685346 0.28391573 0.27645764 0.26542586 0.25073203 0.23386572][0.24115486 0.25092781 0.25543028 0.25979954 0.26548177 0.271015 0.27678415 0.28253627 0.28793588 0.29022083 0.28744829 0.27981648 0.26821744 0.25264975 0.23459679][0.23829672 0.24765731 0.25091124 0.25481209 0.25989082 0.26561293 0.27147129 0.27736905 0.28287911 0.28511006 0.28243074 0.27472958 0.26340678 0.24834749 0.23040144][0.2336151 0.24277744 0.24553448 0.24905849 0.25367197 0.25898698 0.26439223 0.26952305 0.27387166 0.27495781 0.27182862 0.26459256 0.25418216 0.24053909 0.224087][0.23102973 0.2395241 0.24097484 0.24337809 0.24684469 0.25162086 0.25593758 0.25958425 0.26235378 0.26218158 0.25874409 0.25179052 0.24300618 0.23173803 0.21764301][0.23224413 0.23945168 0.23975988 0.24036941 0.24198647 0.24495171 0.24807958 0.25042012 0.25166118 0.25034809 0.24674284 0.24059753 0.23282513 0.22355269 0.21201555][0.23276751 0.23873436 0.2378315 0.23744975 0.23738265 0.23885965 0.24055453 0.24192552 0.24217053 0.24016869 0.23685066 0.23190832 0.22547494 0.21794289 0.20860381][0.23232368 0.23731099 0.23560515 0.23419411 0.232824 0.23278534 0.23294881 0.23320335 0.23201185 0.22964263 0.22660875 0.22219914 0.21686858 0.21106872 0.20419359][0.2307184 0.23539621 0.23311365 0.23047584 0.22809859 0.22698809 0.22596912 0.22470944 0.22223201 0.21960351 0.21687227 0.21319455 0.20886162 0.20454839 0.20006372][0.22931416 0.23324323 0.22937787 0.22545514 0.2218115 0.21948007 0.21728319 0.2151735 0.21259362 0.21006826 0.20781311 0.20481454 0.20141354 0.19824919 0.19555017]]...]
INFO - root - 2017-12-09 20:36:24.208655: step 56210, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.877 sec/batch; 67h:18m:21s remains)
INFO - root - 2017-12-09 20:36:32.956715: step 56220, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.898 sec/batch; 68h:54m:54s remains)
INFO - root - 2017-12-09 20:36:41.724746: step 56230, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.865 sec/batch; 66h:22m:39s remains)
INFO - root - 2017-12-09 20:36:50.115608: step 56240, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.866 sec/batch; 66h:28m:38s remains)
INFO - root - 2017-12-09 20:36:58.805057: step 56250, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.881 sec/batch; 67h:35m:47s remains)
INFO - root - 2017-12-09 20:37:07.567711: step 56260, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.878 sec/batch; 67h:21m:53s remains)
INFO - root - 2017-12-09 20:37:16.239468: step 56270, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.884 sec/batch; 67h:50m:55s remains)
INFO - root - 2017-12-09 20:37:24.955799: step 56280, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.877 sec/batch; 67h:18m:10s remains)
INFO - root - 2017-12-09 20:37:33.716044: step 56290, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 66h:33m:13s remains)
INFO - root - 2017-12-09 20:37:42.352840: step 56300, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.877 sec/batch; 67h:16m:11s remains)
2017-12-09 20:37:43.185442: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0018283556 -0.0018274102 -0.0018277046 -0.0018285314 -0.0018294437 -0.001830389 -0.0018309435 -0.0018309058 -0.0018303341 -0.0018297354 -0.0018290597 -0.0018284471 -0.0018281133 -0.0018280897 -0.0018282183][-0.0018281995 -0.0018273529 -0.0018280632 -0.0018294513 -0.0018309875 -0.0018324902 -0.0018334123 -0.0018333317 -0.0018323136 -0.001831165 -0.001830092 -0.0018291192 -0.0018284904 -0.0018281235 -0.0018280223][-0.0018285736 -0.0018280096 -0.0018292142 -0.0018310919 -0.0018329973 -0.0018346787 -0.0018356628 -0.0018354857 -0.0018339928 -0.0018325157 -0.0018314051 -0.0018302419 -0.0018292193 -0.0018284351 -0.0018280694][-0.0018290264 -0.0018289796 -0.0018307235 -0.0018327276 -0.0018345573 -0.0018358347 -0.0018363759 -0.0018356215 -0.0018334633 -0.001832096 -0.0018316058 -0.0018309408 -0.0018298898 -0.0018288565 -0.0018283871][-0.0018293805 -0.0018299469 -0.001832108 -0.001834034 -0.0018355194 -0.0018361015 -0.0018356957 -0.0018338219 -0.0018309604 -0.0018302101 -0.0018308418 -0.0018310461 -0.0018303058 -0.0018292567 -0.0018288372][-0.0018298024 -0.0018309249 -0.0018333683 -0.0018349576 -0.0018357348 -0.0018353808 -0.0018335806 -0.0018297806 -0.0018263446 -0.0018270122 -0.0018295475 -0.0018308093 -0.00183053 -0.001829679 -0.0018294008][-0.001830126 -0.0018317446 -0.0018343013 -0.001835457 -0.0018354697 -0.0018339374 -0.0018302036 -0.0018239836 -0.0018203065 -0.0018233096 -0.0018280373 -0.001830308 -0.0018305081 -0.0018298957 -0.00182972][-0.0018304281 -0.0018323258 -0.001834814 -0.0018355033 -0.001834905 -0.0018322935 -0.0018263828 -0.0018178329 -0.0018148836 -0.0018203376 -0.0018265845 -0.001829618 -0.0018303152 -0.0018299578 -0.001829661][-0.0018307272 -0.0018327128 -0.0018349531 -0.0018352329 -0.001834454 -0.0018311588 -0.0018237879 -0.0018146622 -0.0018131251 -0.0018196525 -0.0018260769 -0.0018293067 -0.0018302872 -0.0018299938 -0.0018296065][-0.0018307545 -0.0018325186 -0.0018344707 -0.0018346925 -0.0018339286 -0.0018305441 -0.001823218 -0.0018157838 -0.0018157318 -0.0018214096 -0.0018267224 -0.0018295818 -0.0018303568 -0.0018299104 -0.0018294356][-0.0018306305 -0.00183189 -0.0018334978 -0.0018337778 -0.0018332101 -0.0018302301 -0.0018243418 -0.0018195725 -0.0018203628 -0.001824377 -0.0018280714 -0.0018299897 -0.0018302189 -0.0018296906 -0.0018291234][-0.0018302689 -0.0018308826 -0.0018321187 -0.0018325355 -0.0018322062 -0.001830066 -0.001826331 -0.0018239423 -0.0018247806 -0.0018272434 -0.0018293349 -0.0018301295 -0.0018299046 -0.0018292862 -0.0018286067][-0.001829639 -0.0018297698 -0.0018307128 -0.0018312265 -0.0018312141 -0.0018301232 -0.0018282974 -0.0018272601 -0.0018277931 -0.0018290369 -0.0018298487 -0.001829901 -0.0018294837 -0.0018288353 -0.001828136][-0.0018290289 -0.0018287762 -0.001829387 -0.0018299498 -0.0018301558 -0.0018298541 -0.0018292447 -0.0018289384 -0.0018292343 -0.0018297009 -0.0018297565 -0.0018295135 -0.0018290158 -0.0018284178 -0.0018278516][-0.0018285383 -0.0018280117 -0.001828271 -0.0018287812 -0.0018290883 -0.0018291913 -0.0018291775 -0.0018291859 -0.0018293047 -0.0018293007 -0.0018291341 -0.0018289196 -0.0018285242 -0.0018280835 -0.0018276656]]...]
INFO - root - 2017-12-09 20:37:51.886216: step 56310, loss = 0.83, batch loss = 0.70 (8.6 examples/sec; 0.927 sec/batch; 71h:05m:39s remains)
INFO - root - 2017-12-09 20:38:00.487771: step 56320, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.854 sec/batch; 65h:30m:37s remains)
INFO - root - 2017-12-09 20:38:09.313295: step 56330, loss = 0.82, batch loss = 0.69 (8.8 examples/sec; 0.909 sec/batch; 69h:44m:55s remains)
INFO - root - 2017-12-09 20:38:17.647632: step 56340, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.855 sec/batch; 65h:36m:11s remains)
INFO - root - 2017-12-09 20:38:26.082102: step 56350, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.870 sec/batch; 66h:43m:16s remains)
INFO - root - 2017-12-09 20:38:34.750688: step 56360, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.879 sec/batch; 67h:24m:23s remains)
INFO - root - 2017-12-09 20:38:43.396683: step 56370, loss = 0.83, batch loss = 0.70 (9.0 examples/sec; 0.886 sec/batch; 67h:55m:59s remains)
INFO - root - 2017-12-09 20:38:52.183306: step 56380, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.886 sec/batch; 67h:59m:13s remains)
INFO - root - 2017-12-09 20:39:00.921277: step 56390, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.879 sec/batch; 67h:26m:59s remains)
INFO - root - 2017-12-09 20:39:09.431975: step 56400, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.879 sec/batch; 67h:24m:13s remains)
2017-12-09 20:39:10.362470: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.2813811 0.2825841 0.28318879 0.28333518 0.28204784 0.27872932 0.27387062 0.26636991 0.25681645 0.24403219 0.22899635 0.21409994 0.20026894 0.18850097 0.17944138][0.29139063 0.29563484 0.29940087 0.30151671 0.30173689 0.30040774 0.29635823 0.28877383 0.27832291 0.26448345 0.24780662 0.23017867 0.21374993 0.19991152 0.18917952][0.29677993 0.30386549 0.31046069 0.31463704 0.316763 0.31649247 0.31316409 0.3061198 0.29500264 0.28016505 0.26197231 0.24246453 0.22403677 0.20807992 0.19582848][0.29824987 0.30826288 0.31764975 0.32414711 0.32842308 0.32942626 0.32658622 0.32023531 0.30881765 0.29329348 0.27407983 0.25325394 0.23295316 0.21492022 0.20120057][0.29531142 0.30746242 0.3190912 0.32784373 0.33446777 0.33673093 0.33459848 0.32881159 0.31699911 0.30101714 0.28114033 0.25963935 0.23853713 0.21930286 0.2045721][0.28882813 0.3023155 0.31507993 0.32541507 0.33365011 0.33821616 0.33797061 0.33235598 0.32036841 0.30423918 0.284153 0.26166993 0.23934545 0.21987163 0.20462932][0.28324765 0.29698685 0.30922502 0.31951702 0.32864034 0.33477432 0.33615595 0.33175525 0.32060933 0.30476636 0.28479385 0.26223841 0.23958349 0.21933231 0.20342919][0.27977869 0.29331923 0.30463856 0.31483173 0.32436672 0.33041909 0.33183974 0.32816917 0.317468 0.3016656 0.28186694 0.25979787 0.23702098 0.21619292 0.19981551][0.27820235 0.29166275 0.30211514 0.31177807 0.32097483 0.32722121 0.32879391 0.3252914 0.31470731 0.29909542 0.27962685 0.25809294 0.23554061 0.21441446 0.19739519][0.27849782 0.29166916 0.30072856 0.3091799 0.31735444 0.32298017 0.32453659 0.32077238 0.31022286 0.2951059 0.2763744 0.25546926 0.23362185 0.21275873 0.19541863][0.27905655 0.29204357 0.30015656 0.30760947 0.31495655 0.32021347 0.3218528 0.31816059 0.30806962 0.2933532 0.27486703 0.25407624 0.23198092 0.21100716 0.19317465][0.27886432 0.29146644 0.29826638 0.30452424 0.31146926 0.31661612 0.31873044 0.31660733 0.3078351 0.2941176 0.27553967 0.25495189 0.23296075 0.21109779 0.19260576][0.27659479 0.288609 0.29423329 0.299554 0.30569583 0.31077665 0.31334731 0.31231314 0.30517843 0.29313663 0.2758117 0.25593892 0.2341606 0.21223357 0.19314989][0.26992121 0.2813201 0.28585306 0.29060131 0.29692754 0.30305311 0.30759802 0.30855334 0.30390123 0.29404393 0.27827469 0.25924411 0.23795123 0.21614021 0.19651158][0.26117867 0.27156967 0.2749218 0.27929759 0.28586245 0.29321492 0.29984331 0.30335402 0.30178657 0.29464796 0.28090566 0.26298237 0.24204168 0.22019896 0.19988543]]...]
INFO - root - 2017-12-09 20:39:18.986317: step 56410, loss = 0.82, batch loss = 0.69 (8.8 examples/sec; 0.904 sec/batch; 69h:20m:58s remains)
INFO - root - 2017-12-09 20:39:27.637895: step 56420, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.843 sec/batch; 64h:38m:55s remains)
INFO - root - 2017-12-09 20:39:36.239773: step 56430, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.851 sec/batch; 65h:16m:02s remains)
INFO - root - 2017-12-09 20:39:44.491964: step 56440, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 66h:27m:08s remains)
INFO - root - 2017-12-09 20:39:52.981443: step 56450, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.860 sec/batch; 65h:56m:54s remains)
INFO - root - 2017-12-09 20:40:01.576363: step 56460, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 66h:35m:02s remains)
INFO - root - 2017-12-09 20:40:10.306950: step 56470, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.891 sec/batch; 68h:19m:46s remains)
INFO - root - 2017-12-09 20:40:19.075181: step 56480, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.902 sec/batch; 69h:09m:42s remains)
INFO - root - 2017-12-09 20:40:27.743929: step 56490, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.853 sec/batch; 65h:24m:49s remains)
INFO - root - 2017-12-09 20:40:36.189003: step 56500, loss = 0.82, batch loss = 0.69 (9.8 examples/sec; 0.817 sec/batch; 62h:37m:12s remains)
2017-12-09 20:40:37.165491: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0018336311 -0.0018326248 -0.0018311688 -0.0018295577 -0.0018281734 -0.0018273462 -0.0018272415 -0.0018277125 -0.0018285128 -0.0018287837 -0.0018281806 -0.0018266937 -0.0018245317 -0.0018221302 -0.0018199595][-0.0018355914 -0.0018343413 -0.0018325084 -0.0018303945 -0.0018286058 -0.0018274492 -0.0018272246 -0.0018277698 -0.0018287473 -0.0018292292 -0.0018287994 -0.0018274094 -0.0018251599 -0.0018225232 -0.0018200955][-0.0018367051 -0.0018354136 -0.0018334282 -0.0018309985 -0.0018288974 -0.0018274764 -0.0018270379 -0.0018275046 -0.0018285043 -0.0018290675 -0.0018287495 -0.0018274562 -0.0018253159 -0.0018226893 -0.0018202914][-0.0018358629 -0.0018347081 -0.0018328928 -0.0018306753 -0.0018287867 -0.0018274061 -0.0018268676 -0.001827143 -0.0018279508 -0.0018283784 -0.0018280467 -0.001826837 -0.0018248153 -0.0018224055 -0.0018202668][-0.0018331507 -0.0018322893 -0.0018309734 -0.0018294002 -0.0018280406 -0.0018269296 -0.001826358 -0.001826409 -0.0018268317 -0.0018269445 -0.0018265183 -0.0018254266 -0.0018237438 -0.0018217795 -0.0018201217][-0.0018299896 -0.0018295648 -0.0018289487 -0.0018282392 -0.0018276309 -0.0018270286 -0.0018266402 -0.0018265747 -0.0018266123 -0.0018263237 -0.0018255886 -0.0018244314 -0.0018229225 -0.0018213437 -0.0018200965][-0.0018265763 -0.0018265294 -0.0018265957 -0.0018266889 -0.0018268394 -0.001826818 -0.0018266691 -0.0018265439 -0.0018263097 -0.0018257283 -0.0018248236 -0.0018237 -0.0018224006 -0.0018211408 -0.0018201764][-0.0018233984 -0.0018234567 -0.0018239826 -0.0018246266 -0.0018252952 -0.0018257301 -0.0018258558 -0.0018257861 -0.0018254849 -0.0018248669 -0.0018240223 -0.001823043 -0.0018220263 -0.0018211005 -0.0018204028][-0.0018209571 -0.0018207823 -0.0018214056 -0.0018222722 -0.0018232071 -0.0018239421 -0.0018243329 -0.0018244541 -0.001824301 -0.0018238709 -0.0018232529 -0.0018225462 -0.001821851 -0.001821219 -0.001820731][-0.0018196173 -0.0018191301 -0.0018195927 -0.0018203105 -0.0018211185 -0.0018218524 -0.0018223695 -0.0018226629 -0.0018227367 -0.0018225934 -0.0018223075 -0.0018219554 -0.001821621 -0.0018213223 -0.001821087][-0.001819204 -0.0018185503 -0.0018188136 -0.0018193384 -0.0018199303 -0.0018205184 -0.0018209829 -0.0018212672 -0.0018214309 -0.0018214913 -0.0018214767 -0.0018214277 -0.0018213938 -0.0018213721 -0.0018213604][-0.0018192513 -0.0018184944 -0.0018185914 -0.0018189192 -0.0018192733 -0.0018196297 -0.0018199291 -0.0018201357 -0.0018203389 -0.0018205295 -0.00182072 -0.0018209057 -0.0018210991 -0.0018212792 -0.0018214374][-0.0018194392 -0.0018186151 -0.0018185553 -0.0018186719 -0.0018188267 -0.0018189994 -0.0018191511 -0.0018192853 -0.0018194938 -0.0018197567 -0.0018200539 -0.001820375 -0.0018207113 -0.001821039 -0.0018213349][-0.0018196519 -0.0018188421 -0.0018185977 -0.0018185036 -0.0018184895 -0.0018185363 -0.0018186034 -0.0018186958 -0.0018188952 -0.0018191707 -0.0018194917 -0.0018198538 -0.0018202453 -0.0018206326 -0.0018210006][-0.0018198948 -0.0018190831 -0.0018186441 -0.0018183475 -0.0018181588 -0.0018180708 -0.0018180602 -0.0018181094 -0.00181827 -0.0018185166 -0.001818813 -0.0018191595 -0.0018195417 -0.0018199307 -0.0018203028]]...]
INFO - root - 2017-12-09 20:40:45.663429: step 56510, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.853 sec/batch; 65h:22m:59s remains)
INFO - root - 2017-12-09 20:40:54.409905: step 56520, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.882 sec/batch; 67h:37m:05s remains)
INFO - root - 2017-12-09 20:41:03.074333: step 56530, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.860 sec/batch; 65h:57m:19s remains)
INFO - root - 2017-12-09 20:41:11.558780: step 56540, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.884 sec/batch; 67h:43m:31s remains)
INFO - root - 2017-12-09 20:41:20.268256: step 56550, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.899 sec/batch; 68h:56m:23s remains)
INFO - root - 2017-12-09 20:41:29.103931: step 56560, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.872 sec/batch; 66h:52m:35s remains)
INFO - root - 2017-12-09 20:41:37.782970: step 56570, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.841 sec/batch; 64h:29m:36s remains)
INFO - root - 2017-12-09 20:41:46.453467: step 56580, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.878 sec/batch; 67h:15m:36s remains)
INFO - root - 2017-12-09 20:41:55.178399: step 56590, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.883 sec/batch; 67h:38m:43s remains)
INFO - root - 2017-12-09 20:42:03.828201: step 56600, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.879 sec/batch; 67h:22m:56s remains)
2017-12-09 20:42:04.671598: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.21364824 0.21189034 0.210205 0.20850152 0.20634624 0.20359203 0.20056932 0.19678167 0.19339538 0.19112444 0.18823352 0.18509181 0.18228637 0.17955823 0.1768337][0.21693173 0.21487558 0.21269189 0.21057531 0.20764531 0.20468496 0.20103545 0.19785753 0.1952357 0.19341882 0.19141418 0.1886764 0.18629786 0.18388471 0.18125594][0.21495223 0.21360463 0.21119441 0.2079265 0.20392706 0.19917037 0.19423291 0.19055291 0.18803063 0.18695058 0.18607192 0.18472587 0.18354225 0.18178554 0.17955886][0.21056551 0.21019411 0.20776045 0.20313106 0.19758034 0.19123553 0.18488881 0.18028438 0.17761965 0.17763641 0.17809373 0.178764 0.17920505 0.17848466 0.17696424][0.20448773 0.20467742 0.20151785 0.19588728 0.1890855 0.18125395 0.17390354 0.16846356 0.16585469 0.16671723 0.16907579 0.17195565 0.17433608 0.17466097 0.1739824][0.19842577 0.19846891 0.19442546 0.18784887 0.1795696 0.17075695 0.16248217 0.15687977 0.15499426 0.15679458 0.16057116 0.16461483 0.16836081 0.16974257 0.16961068][0.19350509 0.19382927 0.1887282 0.18103629 0.1713727 0.16139048 0.15206848 0.146635 0.14541292 0.14807685 0.15316686 0.1581537 0.16282995 0.16464503 0.1647975][0.18510629 0.18646234 0.18178491 0.17380187 0.16395842 0.15350321 0.14385596 0.13821739 0.13685361 0.13995793 0.14518757 0.15054643 0.1553521 0.15685026 0.15676481][0.17773558 0.18003331 0.17587471 0.16903107 0.15995865 0.15096915 0.14248468 0.13739693 0.13605975 0.13871071 0.14312652 0.14720497 0.15066263 0.1509224 0.14977562][0.16930018 0.17317156 0.17046013 0.16581328 0.15935984 0.15277791 0.14608401 0.14191338 0.14074348 0.14227369 0.1445782 0.14635365 0.147655 0.14661121 0.14447157][0.16231614 0.16788235 0.16699214 0.16487572 0.16102642 0.15753375 0.15298888 0.14984782 0.14847557 0.14894035 0.14959465 0.14859256 0.14745194 0.14494444 0.14175257][0.1565014 0.1636969 0.16485922 0.16492613 0.16361897 0.16221708 0.15975526 0.15750997 0.15576394 0.15497582 0.15414895 0.15169536 0.14880195 0.1449507 0.14097735][0.14937104 0.15746954 0.16029266 0.16251691 0.16377401 0.16440667 0.16384108 0.16225322 0.16012497 0.15818867 0.15573248 0.15186375 0.14747941 0.14297743 0.13878717][0.14368312 0.15183595 0.15532039 0.15880778 0.16154973 0.16396467 0.16507643 0.16422246 0.16221794 0.15951966 0.15618408 0.15142551 0.1463564 0.14196785 0.138186][0.13921276 0.14700432 0.15027301 0.15438133 0.15781778 0.16091082 0.1627465 0.16240311 0.16058208 0.15755674 0.15390301 0.14939879 0.14478645 0.14116286 0.13831885]]...]
INFO - root - 2017-12-09 20:42:13.324348: step 56610, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.855 sec/batch; 65h:29m:54s remains)
INFO - root - 2017-12-09 20:42:22.092120: step 56620, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 66h:30m:32s remains)
INFO - root - 2017-12-09 20:42:30.696428: step 56630, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.885 sec/batch; 67h:51m:20s remains)
INFO - root - 2017-12-09 20:42:39.249653: step 56640, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.886 sec/batch; 67h:52m:46s remains)
INFO - root - 2017-12-09 20:42:48.057567: step 56650, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.885 sec/batch; 67h:49m:27s remains)
INFO - root - 2017-12-09 20:42:56.767799: step 56660, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.877 sec/batch; 67h:11m:31s remains)
INFO - root - 2017-12-09 20:43:05.523585: step 56670, loss = 0.82, batch loss = 0.69 (9.7 examples/sec; 0.828 sec/batch; 63h:25m:39s remains)
INFO - root - 2017-12-09 20:43:14.312822: step 56680, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.873 sec/batch; 66h:55m:23s remains)
INFO - root - 2017-12-09 20:43:22.870701: step 56690, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 66h:00m:32s remains)
INFO - root - 2017-12-09 20:43:31.404107: step 56700, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.857 sec/batch; 65h:37m:54s remains)
2017-12-09 20:43:32.293746: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.001797458 -0.0017933623 -0.0017909267 -0.001788865 -0.0017875315 -0.0017867475 -0.0017862016 -0.0017860152 -0.0017859774 -0.0017860911 -0.0017860737 -0.0017854575 -0.001784378 -0.0017824604 -0.0017803228][-0.0018045299 -0.0018009392 -0.0017988986 -0.0017973628 -0.0017964344 -0.0017959471 -0.0017956278 -0.0017955555 -0.0017954146 -0.0017955019 -0.0017956281 -0.001795168 -0.0017941855 -0.0017921727 -0.0017902855][-0.0018104945 -0.0018079811 -0.0018063689 -0.0018054692 -0.001805027 -0.001804874 -0.0018047934 -0.0018048591 -0.001804782 -0.0018048875 -0.0018051695 -0.0018051962 -0.0018048213 -0.0018038085 -0.0018029038][-0.0018138746 -0.0018122493 -0.0018111229 -0.0018106906 -0.0018105628 -0.0018105111 -0.0018104957 -0.0018105187 -0.0018105514 -0.0018107819 -0.0018111656 -0.001811231 -0.0018113143 -0.0018109556 -0.0018106205][-0.0018131572 -0.0018119683 -0.0018112828 -0.0018113372 -0.0018115176 -0.001811884 -0.0018123182 -0.0018128144 -0.0018131211 -0.0018134268 -0.0018139395 -0.0018141628 -0.0018146109 -0.0018148089 -0.0018151075][-0.0018096344 -0.001808618 -0.0018082872 -0.0018086148 -0.0018090078 -0.0018095097 -0.0018101184 -0.0018107494 -0.0018111813 -0.001811492 -0.0018119772 -0.0018123906 -0.0018132235 -0.0018140457 -0.001814937][-0.0018064786 -0.0018047066 -0.0018041237 -0.0018040789 -0.001804119 -0.0018045111 -0.001805169 -0.001805887 -0.0018063121 -0.0018065905 -0.0018069557 -0.0018075068 -0.0018087245 -0.0018102907 -0.0018122017][-0.0018039626 -0.0018014904 -0.0018007094 -0.001800533 -0.0018004081 -0.0018007224 -0.0018013435 -0.0018020731 -0.0018023581 -0.0018023829 -0.0018024759 -0.001802862 -0.001804119 -0.001805964 -0.0018081778][-0.0018029087 -0.0018001263 -0.0017992203 -0.0017988654 -0.0017985727 -0.0017987711 -0.0017993059 -0.0017997934 -0.0017997419 -0.0017994246 -0.0017992028 -0.0017993394 -0.0018003081 -0.0018019602 -0.0018040995][-0.0018018712 -0.0017991621 -0.001798449 -0.0017981743 -0.0017978024 -0.0017980239 -0.0017985654 -0.0017990518 -0.0017988114 -0.0017984528 -0.0017982676 -0.0017984294 -0.0017992792 -0.0018009077 -0.0018030955][-0.0018017831 -0.0017994505 -0.0017989263 -0.0017987629 -0.0017984294 -0.001798411 -0.0017985828 -0.0017987347 -0.0017982402 -0.001797627 -0.0017973392 -0.0017975955 -0.0017985706 -0.0018004471 -0.0018029119][-0.0018012599 -0.0017996422 -0.0017994068 -0.0017995173 -0.0017994407 -0.0017994173 -0.001799409 -0.0017993228 -0.0017987817 -0.0017980782 -0.0017978519 -0.0017982333 -0.0017993884 -0.0018014407 -0.0018040877][-0.0018009413 -0.0017999625 -0.0018001571 -0.0018006582 -0.0018009351 -0.0018009915 -0.001800858 -0.0018005086 -0.0017998714 -0.0017991828 -0.0017990855 -0.0017996023 -0.001800843 -0.0018029009 -0.0018053463][-0.0018011584 -0.0018007107 -0.0018011181 -0.0018014589 -0.0018015846 -0.0018014337 -0.0018010705 -0.0018003342 -0.0017995475 -0.0017990195 -0.0017991177 -0.0017998976 -0.0018012709 -0.0018033669 -0.001805831][-0.0018015996 -0.001801216 -0.0018014719 -0.0018016641 -0.001801733 -0.0018015187 -0.0018012878 -0.0018007202 -0.0018001411 -0.0017996869 -0.0017998093 -0.0018006364 -0.0018020733 -0.0018042291 -0.0018067649]]...]
INFO - root - 2017-12-09 20:43:41.004317: step 56710, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.841 sec/batch; 64h:23m:36s remains)
INFO - root - 2017-12-09 20:43:49.591112: step 56720, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.863 sec/batch; 66h:07m:55s remains)
INFO - root - 2017-12-09 20:43:58.252851: step 56730, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.875 sec/batch; 67h:02m:28s remains)
INFO - root - 2017-12-09 20:44:06.435519: step 56740, loss = 0.82, batch loss = 0.69 (9.7 examples/sec; 0.826 sec/batch; 63h:16m:01s remains)
INFO - root - 2017-12-09 20:44:14.925344: step 56750, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.879 sec/batch; 67h:19m:40s remains)
INFO - root - 2017-12-09 20:44:23.664040: step 56760, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.883 sec/batch; 67h:38m:27s remains)
INFO - root - 2017-12-09 20:44:32.430231: step 56770, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.861 sec/batch; 65h:56m:35s remains)
INFO - root - 2017-12-09 20:44:41.188833: step 56780, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.892 sec/batch; 68h:19m:02s remains)
INFO - root - 2017-12-09 20:44:49.905396: step 56790, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.866 sec/batch; 66h:19m:34s remains)
INFO - root - 2017-12-09 20:44:58.499415: step 56800, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.848 sec/batch; 64h:55m:17s remains)
2017-12-09 20:44:59.376568: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.078260347 0.084814176 0.08898177 0.090679251 0.089835167 0.086619511 0.081537724 0.07531403 0.06909366 0.062674 0.057612669 0.054452848 0.052750584 0.052325822 0.052260544][0.097564779 0.10680636 0.11286783 0.11515246 0.11395165 0.10913892 0.10158186 0.092494741 0.083009616 0.074114673 0.067077093 0.06277696 0.061166361 0.062317505 0.065004155][0.1160439 0.12785134 0.13552141 0.13862778 0.13706833 0.13082436 0.12085525 0.10832169 0.095099606 0.083028466 0.073811345 0.068551287 0.067659162 0.071213916 0.077221625][0.13012598 0.14383098 0.15268295 0.15646544 0.15462944 0.14717086 0.13495821 0.11949665 0.10325368 0.088511288 0.077889353 0.072421983 0.07292179 0.078894705 0.088025428][0.13726518 0.15196967 0.16160163 0.16590507 0.1639481 0.15589435 0.1424295 0.12524831 0.10714145 0.090834372 0.079763204 0.07482373 0.07702104 0.085276753 0.097282313][0.13755514 0.15280806 0.16296935 0.1678012 0.16620696 0.15827607 0.14447321 0.12696657 0.10860154 0.092217721 0.08156319 0.077547133 0.0811027 0.090805642 0.10444447][0.13186635 0.14694713 0.15735462 0.1627605 0.16189991 0.1545323 0.14146651 0.1247314 0.10711684 0.091931075 0.082541548 0.079880312 0.084497012 0.094954349 0.10911433][0.12142462 0.13567993 0.14541675 0.15112463 0.15104924 0.14514466 0.13374797 0.11876234 0.10318974 0.089685477 0.081623033 0.080030806 0.08525347 0.095934942 0.10989813][0.10668975 0.11950812 0.12835936 0.13377705 0.13438037 0.13024059 0.12133789 0.10938003 0.096665308 0.085513271 0.079103932 0.078065991 0.083015285 0.093068235 0.10630409][0.089544311 0.10039061 0.10795319 0.1128855 0.11400449 0.11159483 0.10545164 0.096950412 0.087666832 0.079397544 0.074674964 0.074088946 0.078542612 0.087196343 0.098883532][0.072286218 0.080987088 0.08717557 0.091292441 0.092732824 0.091761284 0.088131793 0.082804173 0.07674741 0.071368374 0.068405323 0.068303056 0.071858779 0.078588344 0.087782137][0.056495577 0.0633321 0.068127543 0.071348824 0.0728971 0.072909221 0.071368381 0.068603873 0.06518314 0.062063821 0.060327172 0.060433831 0.062875345 0.067552134 0.073935524][0.042420648 0.047659561 0.051294927 0.053740185 0.055135269 0.055893455 0.055840369 0.054972276 0.053505458 0.051831692 0.050731543 0.05049219 0.051584251 0.054235924 0.058071092][0.030136928 0.034112174 0.036880281 0.038814016 0.040145092 0.041255232 0.042017125 0.042328697 0.042057369 0.041308392 0.040509243 0.039711788 0.039481279 0.040293239 0.042074975][0.020092834 0.023021039 0.025105117 0.026598157 0.027755149 0.028937671 0.029962078 0.030722618 0.031023342 0.03076417 0.03008727 0.028991636 0.027972803 0.02753951 0.027969791]]...]
INFO - root - 2017-12-09 20:45:07.980753: step 56810, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.880 sec/batch; 67h:22m:40s remains)
INFO - root - 2017-12-09 20:45:16.588177: step 56820, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.839 sec/batch; 64h:12m:56s remains)
INFO - root - 2017-12-09 20:45:25.222852: step 56830, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.899 sec/batch; 68h:48m:49s remains)
INFO - root - 2017-12-09 20:45:33.577276: step 56840, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.875 sec/batch; 67h:01m:52s remains)
INFO - root - 2017-12-09 20:45:42.394091: step 56850, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 66h:29m:20s remains)
INFO - root - 2017-12-09 20:45:51.217661: step 56860, loss = 0.82, batch loss = 0.69 (8.8 examples/sec; 0.907 sec/batch; 69h:24m:37s remains)
INFO - root - 2017-12-09 20:45:59.852808: step 56870, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.836 sec/batch; 63h:59m:05s remains)
INFO - root - 2017-12-09 20:46:08.510964: step 56880, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.849 sec/batch; 65h:01m:09s remains)
INFO - root - 2017-12-09 20:46:17.136467: step 56890, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.866 sec/batch; 66h:17m:17s remains)
INFO - root - 2017-12-09 20:46:25.684275: step 56900, loss = 0.82, batch loss = 0.69 (10.3 examples/sec; 0.779 sec/batch; 59h:37m:32s remains)
2017-12-09 20:46:26.504153: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0018277784 -0.0018272564 -0.0018269991 -0.0018266002 -0.0018259397 -0.0018250601 -0.0018239389 -0.0018227968 -0.0018217139 -0.0018211766 -0.0018212014 -0.0018215936 -0.0018220715 -0.0018224174 -0.001822473][-0.0018260517 -0.0018257794 -0.0018258445 -0.0018256175 -0.0018249204 -0.0018239579 -0.0018228141 -0.0018216436 -0.0018205494 -0.0018198701 -0.0018195121 -0.0018195751 -0.0018198572 -0.0018201254 -0.001820358][-0.0018247645 -0.0018247075 -0.0018250558 -0.0018249006 -0.0018240073 -0.001822715 -0.0018214108 -0.0018204991 -0.0018197603 -0.0018193178 -0.0018189758 -0.0018188508 -0.001818966 -0.0018190928 -0.001819266][-0.0018235905 -0.0018236348 -0.0018241613 -0.0018239658 -0.0018227758 -0.0018210521 -0.0018195773 -0.001818919 -0.0018187317 -0.0018188857 -0.0018188 -0.0018186565 -0.0018186539 -0.0018186898 -0.0018187658][-0.0018234927 -0.0018234926 -0.0018237644 -0.0018232887 -0.0018218053 -0.0018196832 -0.0018179574 -0.0018174049 -0.0018176906 -0.0018183679 -0.0018186119 -0.0018184885 -0.0018183851 -0.0018183191 -0.0018183537][-0.0018241293 -0.0018240827 -0.0018240177 -0.0018231245 -0.0018214175 -0.0018192139 -0.0018174194 -0.0018168108 -0.0018172909 -0.0018182569 -0.0018187757 -0.0018187282 -0.001818529 -0.0018183753 -0.0018183583][-0.0018256434 -0.0018254919 -0.0018251041 -0.0018237933 -0.0018219046 -0.0018197423 -0.001818053 -0.0018174616 -0.0018179574 -0.001818952 -0.0018195808 -0.0018196272 -0.0018193648 -0.0018190749 -0.0018189822][-0.0018275286 -0.0018271507 -0.0018265424 -0.0018250496 -0.0018231291 -0.0018211248 -0.001819636 -0.0018191786 -0.0018196902 -0.0018206089 -0.001821233 -0.0018213184 -0.0018210148 -0.0018206764 -0.0018205453][-0.0018300001 -0.0018291742 -0.0018282542 -0.0018267718 -0.0018251067 -0.001823529 -0.0018225344 -0.0018223403 -0.0018228372 -0.0018235374 -0.001823899 -0.0018236833 -0.0018231845 -0.0018227991 -0.0018227623][-0.0018325369 -0.0018314822 -0.0018304624 -0.0018291177 -0.0018278722 -0.0018269368 -0.0018264714 -0.0018265662 -0.0018271219 -0.0018275096 -0.0018274259 -0.0018268611 -0.0018261719 -0.0018257468 -0.0018257559][-0.0018347772 -0.001833661 -0.0018326909 -0.0018316555 -0.001830959 -0.0018306228 -0.001830634 -0.0018310511 -0.0018317975 -0.0018320273 -0.0018315346 -0.0018306279 -0.001829649 -0.001829064 -0.0018289529][-0.0018364472 -0.0018353648 -0.001834622 -0.001834042 -0.0018338499 -0.0018340044 -0.0018344604 -0.0018351711 -0.0018361706 -0.0018365637 -0.001836011 -0.0018347991 -0.0018335007 -0.0018326227 -0.0018322289][-0.0018370129 -0.0018360867 -0.0018357503 -0.0018356084 -0.0018360248 -0.0018367486 -0.0018376729 -0.0018385622 -0.0018394901 -0.0018399645 -0.0018395026 -0.001838266 -0.0018368928 -0.001835886 -0.0018353859][-0.0018363019 -0.0018357839 -0.0018358419 -0.0018361334 -0.0018369417 -0.0018379752 -0.0018391258 -0.0018400395 -0.0018408647 -0.0018413687 -0.0018411154 -0.0018401948 -0.0018390052 -0.001838121 -0.0018376854][-0.0018328775 -0.0018329791 -0.0018338703 -0.0018348887 -0.0018360512 -0.0018373561 -0.0018385911 -0.0018395146 -0.0018401326 -0.0018404275 -0.0018402684 -0.001839758 -0.0018391153 -0.0018386715 -0.0018385713]]...]
INFO - root - 2017-12-09 20:46:35.175854: step 56910, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.877 sec/batch; 67h:08m:13s remains)
INFO - root - 2017-12-09 20:46:43.815156: step 56920, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.870 sec/batch; 66h:36m:10s remains)
INFO - root - 2017-12-09 20:46:52.314508: step 56930, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.852 sec/batch; 65h:14m:45s remains)
INFO - root - 2017-12-09 20:47:00.673165: step 56940, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.872 sec/batch; 66h:42m:32s remains)
INFO - root - 2017-12-09 20:47:09.493603: step 56950, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 66h:32m:13s remains)
INFO - root - 2017-12-09 20:47:18.140016: step 56960, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.857 sec/batch; 65h:36m:05s remains)
INFO - root - 2017-12-09 20:47:26.889052: step 56970, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.875 sec/batch; 66h:58m:06s remains)
INFO - root - 2017-12-09 20:47:35.592637: step 56980, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.861 sec/batch; 65h:55m:26s remains)
INFO - root - 2017-12-09 20:47:44.264963: step 56990, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.885 sec/batch; 67h:41m:32s remains)
INFO - root - 2017-12-09 20:47:52.833726: step 57000, loss = 0.82, batch loss = 0.69 (9.7 examples/sec; 0.822 sec/batch; 62h:52m:03s remains)
2017-12-09 20:47:53.814998: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0014459786 -0.001577926 -0.0017114236 -0.0017837804 -0.0018022432 -0.0018063356 -0.0018046816 -0.001802813 -0.0018002492 -0.0017991166 -0.0018012776 -0.0018004305 -0.0018022915 -0.001802637 -0.0018032228][-0.0011858754 -0.0014299594 -0.0016509163 -0.001764333 -0.001799294 -0.0018026868 -0.0017991831 -0.0017996444 -0.0017961421 -0.0017969641 -0.0018004966 -0.0018019662 -0.001801527 -0.0018016723 -0.0018020787][-0.0010789981 -0.0013491573 -0.001617901 -0.0017634579 -0.0018108401 -0.0018097333 -0.001806 -0.0018031794 -0.0017985284 -0.0017958415 -0.0017977399 -0.0018008668 -0.0018017862 -0.0018018299 -0.001802041][-0.0010548122 -0.0013240036 -0.0016056857 -0.0017632112 -0.0018142256 -0.0018146245 -0.0018097039 -0.0018039251 -0.0018014346 -0.0018013794 -0.0018039892 -0.0018032755 -0.0018023932 -0.001802291 -0.0018023119][-0.0010789946 -0.0013444538 -0.0016169939 -0.0017685889 -0.0018168443 -0.0018171844 -0.0018096883 -0.0017982953 -0.0017859202 -0.0017795083 -0.0017885967 -0.0017985477 -0.001803124 -0.001803171 -0.0018029044][-0.0011453427 -0.0013730561 -0.0016228113 -0.0017630978 -0.0018143955 -0.0018152181 -0.0017991667 -0.0017717044 -0.0017392251 -0.0017249312 -0.0017472543 -0.0017807853 -0.0018002347 -0.001804415 -0.0018037765][-0.0012146817 -0.0014132607 -0.0016421103 -0.0017728542 -0.0018169339 -0.0018130157 -0.001787478 -0.0017364894 -0.0016766812 -0.0016521427 -0.0016925937 -0.0017558474 -0.0017949679 -0.0018057417 -0.0018047609][-0.0011920875 -0.0014034694 -0.0016417004 -0.0017572695 -0.0017933866 -0.0017964602 -0.0017678486 -0.0017041737 -0.0016318562 -0.0016062948 -0.001656518 -0.0017368282 -0.0017900292 -0.0018062529 -0.001805121][-0.0011954206 -0.0013509246 -0.0016023986 -0.0017496708 -0.0017998589 -0.0018014495 -0.0017804364 -0.0017304702 -0.0016773634 -0.0016577395 -0.0016940155 -0.0017548301 -0.0017949373 -0.0018060383 -0.0018045984][-0.0011738618 -0.0012796102 -0.0015453671 -0.0016975411 -0.0017711457 -0.0018045867 -0.0018052927 -0.0017784133 -0.0017453843 -0.0017348097 -0.0017544555 -0.00178449 -0.0018037362 -0.0018072597 -0.0018050718][-0.0011218132 -0.0011657862 -0.001427151 -0.0016297171 -0.0017502964 -0.0018005334 -0.0018080105 -0.0018050071 -0.0017977264 -0.0017930346 -0.0017953573 -0.0018039055 -0.0018088266 -0.0018076862 -0.0018052449][-0.001083897 -0.0010960212 -0.0013254316 -0.0015280312 -0.0016743545 -0.0017668869 -0.0018067397 -0.0018157798 -0.0018146196 -0.0018133037 -0.0018115198 -0.0018106346 -0.0018091804 -0.0018067304 -0.0018045059][-0.00104186 -0.0010500698 -0.0012441182 -0.0014289875 -0.0015859404 -0.001700592 -0.0017694911 -0.0018026193 -0.0018140489 -0.0018146596 -0.0018126245 -0.0018108679 -0.0018086628 -0.0018059867 -0.0018034488][-0.0010705383 -0.0010729671 -0.0012013878 -0.0013422159 -0.0014787517 -0.0015967224 -0.001694422 -0.0017647264 -0.0018024443 -0.0018136235 -0.0018120253 -0.0018097025 -0.0018075833 -0.0018050756 -0.0018027065][-0.0012156675 -0.0011711668 -0.0011979835 -0.0012690735 -0.0013648412 -0.0014668753 -0.001582651 -0.0016885541 -0.0017654212 -0.0018032918 -0.0018104238 -0.0018086511 -0.0018066374 -0.0018039871 -0.0018020895]]...]
INFO - root - 2017-12-09 20:48:02.460719: step 57010, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.861 sec/batch; 65h:51m:59s remains)
INFO - root - 2017-12-09 20:48:11.158182: step 57020, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.855 sec/batch; 65h:27m:12s remains)
INFO - root - 2017-12-09 20:48:19.786441: step 57030, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.875 sec/batch; 66h:56m:19s remains)
INFO - root - 2017-12-09 20:48:27.929128: step 57040, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.849 sec/batch; 64h:57m:11s remains)
INFO - root - 2017-12-09 20:48:36.591280: step 57050, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.883 sec/batch; 67h:32m:49s remains)
INFO - root - 2017-12-09 20:48:45.328176: step 57060, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.840 sec/batch; 64h:15m:56s remains)
INFO - root - 2017-12-09 20:48:54.169619: step 57070, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.890 sec/batch; 68h:03m:26s remains)
INFO - root - 2017-12-09 20:49:02.768628: step 57080, loss = 0.83, batch loss = 0.70 (9.4 examples/sec; 0.849 sec/batch; 64h:59m:27s remains)
INFO - root - 2017-12-09 20:49:11.611716: step 57090, loss = 0.82, batch loss = 0.69 (8.5 examples/sec; 0.944 sec/batch; 72h:13m:51s remains)
INFO - root - 2017-12-09 20:49:20.165773: step 57100, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 66h:23m:20s remains)
2017-12-09 20:49:21.035977: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0018293401 -0.0018294343 -0.0018302548 -0.0018312982 -0.0018324349 -0.0018331482 -0.0018336105 -0.001833613 -0.001833005 -0.0018322269 -0.0018313581 -0.0018302932 -0.0018289224 -0.0018278632 -0.0018270826][-0.0018291301 -0.001829513 -0.0018307435 -0.0018320966 -0.0018334724 -0.0018344481 -0.0018350732 -0.001834985 -0.0018341022 -0.0018330629 -0.001831988 -0.0018306902 -0.0018290245 -0.0018275417 -0.0018266104][-0.0018294323 -0.001830133 -0.0018318335 -0.0018335985 -0.0018352094 -0.0018363999 -0.0018369196 -0.0018364898 -0.0018352274 -0.0018339778 -0.0018326272 -0.0018310884 -0.0018292648 -0.0018276284 -0.0018264944][-0.0018302631 -0.0018314382 -0.001833381 -0.0018354007 -0.0018371751 -0.0018383203 -0.0018383355 -0.0018372692 -0.0018356179 -0.0018342653 -0.0018327567 -0.0018310319 -0.0018292433 -0.0018276873 -0.0018264629][-0.0018315112 -0.0018329947 -0.0018350129 -0.0018369273 -0.0018382715 -0.0018388615 -0.0018379674 -0.0018360459 -0.0018344542 -0.0018336042 -0.0018325523 -0.001830916 -0.0018291766 -0.0018276728 -0.0018264306][-0.0018334676 -0.0018349683 -0.0018367671 -0.0018381045 -0.0018385412 -0.0018382079 -0.0018361292 -0.0018328165 -0.0018314058 -0.0018319446 -0.0018318886 -0.0018307872 -0.0018291456 -0.0018275996 -0.0018262238][-0.0018359084 -0.0018372608 -0.0018385819 -0.0018390148 -0.001838195 -0.0018365206 -0.001833191 -0.0018284277 -0.0018274358 -0.0018295192 -0.0018307607 -0.0018303487 -0.0018289788 -0.0018273594 -0.0018260032][-0.0018377686 -0.0018386266 -0.0018390553 -0.0018384489 -0.0018367511 -0.0018343485 -0.0018308415 -0.0018263895 -0.0018252225 -0.0018277155 -0.0018296754 -0.001829869 -0.0018287546 -0.0018272076 -0.0018259394][-0.0018381553 -0.001837955 -0.0018371581 -0.0018356147 -0.0018337634 -0.0018321952 -0.001830062 -0.0018271114 -0.0018258679 -0.0018274748 -0.0018291001 -0.0018294258 -0.0018284791 -0.0018270615 -0.0018258953][-0.0018358979 -0.0018336941 -0.0018314082 -0.0018295436 -0.0018287727 -0.0018292574 -0.0018294179 -0.0018283668 -0.0018276917 -0.0018285377 -0.0018293443 -0.001829355 -0.0018283472 -0.0018269547 -0.0018258672][-0.0018311809 -0.0018263774 -0.0018225886 -0.0018212195 -0.0018228114 -0.0018263039 -0.0018289471 -0.0018295972 -0.0018295067 -0.0018298907 -0.0018298863 -0.0018293137 -0.0018281271 -0.0018267997 -0.0018257661][-0.0018258028 -0.0018184262 -0.0018135965 -0.0018128774 -0.0018165854 -0.0018228043 -0.0018274832 -0.0018296854 -0.0018305129 -0.0018307819 -0.0018302092 -0.001829143 -0.0018278203 -0.0018265954 -0.0018256702][-0.0018221334 -0.0018131415 -0.0018080308 -0.0018077698 -0.0018126051 -0.0018202383 -0.0018259422 -0.0018292543 -0.0018306094 -0.0018309096 -0.0018300358 -0.0018286781 -0.001827391 -0.0018263526 -0.0018255361][-0.0018219885 -0.0018123268 -0.0018068627 -0.0018064089 -0.0018114222 -0.0018192446 -0.0018251353 -0.0018287639 -0.001830145 -0.0018302816 -0.001829394 -0.001827979 -0.0018268471 -0.0018260012 -0.0018253167][-0.0018250031 -0.0018158947 -0.0018100257 -0.0018086145 -0.001812991 -0.0018201899 -0.0018256778 -0.0018288834 -0.0018298124 -0.0018296216 -0.0018287029 -0.0018273487 -0.0018263794 -0.0018257155 -0.0018251735]]...]
INFO - root - 2017-12-09 20:49:29.614738: step 57110, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.835 sec/batch; 63h:50m:56s remains)
INFO - root - 2017-12-09 20:49:38.536179: step 57120, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.854 sec/batch; 65h:20m:43s remains)
INFO - root - 2017-12-09 20:49:47.273956: step 57130, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.899 sec/batch; 68h:45m:16s remains)
INFO - root - 2017-12-09 20:49:55.697374: step 57140, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.880 sec/batch; 67h:19m:03s remains)
INFO - root - 2017-12-09 20:50:04.161840: step 57150, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.855 sec/batch; 65h:25m:48s remains)
INFO - root - 2017-12-09 20:50:12.687920: step 57160, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.859 sec/batch; 65h:41m:47s remains)
INFO - root - 2017-12-09 20:50:21.386101: step 57170, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.875 sec/batch; 66h:53m:17s remains)
INFO - root - 2017-12-09 20:50:30.235692: step 57180, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.895 sec/batch; 68h:25m:00s remains)
INFO - root - 2017-12-09 20:50:38.875859: step 57190, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.866 sec/batch; 66h:13m:42s remains)
INFO - root - 2017-12-09 20:50:47.533842: step 57200, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 66h:27m:24s remains)
2017-12-09 20:50:48.450622: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0018219381 -0.001820936 -0.001820827 -0.0018207931 -0.0018207884 -0.0018207676 -0.0018207544 -0.0018207929 -0.0018208585 -0.0018207176 -0.0018204439 -0.0018199969 -0.0018193683 -0.0018187657 -0.0018182502][-0.0018247936 -0.0018238688 -0.0018236606 -0.0018234062 -0.0018231844 -0.0018231063 -0.0018231369 -0.0018232265 -0.0018233571 -0.0018231827 -0.001822668 -0.0018218337 -0.0018207673 -0.0018197105 -0.0018187616][-0.0018291122 -0.0018281576 -0.001827784 -0.0018272892 -0.001826892 -0.0018266776 -0.001826619 -0.0018267602 -0.0018270703 -0.0018268167 -0.0018259283 -0.0018246039 -0.0018230908 -0.0018214427 -0.0018199065][-0.0018333843 -0.0018322343 -0.0018317129 -0.0018310441 -0.001830401 -0.001829821 -0.0018294973 -0.0018295834 -0.0018298487 -0.0018295334 -0.0018286604 -0.001827396 -0.00182582 -0.0018237354 -0.0018216451][-0.0018367913 -0.0018354513 -0.0018347858 -0.0018339989 -0.0018330556 -0.0018320607 -0.0018313823 -0.0018310654 -0.0018309916 -0.0018305921 -0.0018300265 -0.0018291089 -0.0018277833 -0.0018259451 -0.0018237191][-0.0018385249 -0.0018369636 -0.0018361497 -0.0018353941 -0.0018344582 -0.0018332284 -0.0018323433 -0.0018317241 -0.0018312648 -0.0018306561 -0.0018302513 -0.0018297599 -0.0018288622 -0.0018275223 -0.0018253861][-0.0018379082 -0.0018362323 -0.0018354089 -0.0018348405 -0.0018341267 -0.0018330013 -0.0018322333 -0.0018316946 -0.001831121 -0.0018303478 -0.0018297435 -0.0018292771 -0.0018286983 -0.0018278316 -0.0018259519][-0.0018353496 -0.0018337233 -0.0018330569 -0.0018328028 -0.0018322825 -0.0018315051 -0.001831213 -0.0018310254 -0.0018305207 -0.0018296753 -0.0018289916 -0.0018283672 -0.0018277172 -0.0018269912 -0.0018253246][-0.0018317578 -0.0018302932 -0.001829786 -0.0018297872 -0.0018295411 -0.0018291242 -0.0018292724 -0.0018295704 -0.0018293352 -0.0018286183 -0.0018279055 -0.0018271232 -0.0018263089 -0.0018255614 -0.0018240496][-0.0018283307 -0.0018270283 -0.0018265354 -0.0018266086 -0.0018266855 -0.0018266066 -0.0018268696 -0.0018273931 -0.0018277284 -0.0018275193 -0.0018268866 -0.0018258083 -0.0018246698 -0.0018237784 -0.0018224018][-0.0018257946 -0.0018243835 -0.0018237517 -0.0018238595 -0.0018243031 -0.0018246761 -0.0018250388 -0.0018256491 -0.0018264559 -0.0018266306 -0.0018260233 -0.0018248589 -0.0018235408 -0.0018223136 -0.001820828][-0.0018245112 -0.0018226547 -0.0018217359 -0.0018217403 -0.0018223842 -0.0018230957 -0.001823645 -0.0018243144 -0.0018252648 -0.0018256227 -0.0018251572 -0.001823965 -0.0018225991 -0.0018211238 -0.0018195956][-0.0018242049 -0.0018217616 -0.0018204999 -0.0018202439 -0.0018207902 -0.0018215897 -0.0018222049 -0.0018229241 -0.0018237438 -0.0018241681 -0.0018239155 -0.0018229502 -0.0018216596 -0.0018202402 -0.0018188909][-0.0018248791 -0.0018218041 -0.0018200898 -0.001819452 -0.0018197038 -0.0018202874 -0.001820802 -0.0018214388 -0.0018220977 -0.0018224252 -0.001822288 -0.0018216477 -0.0018206569 -0.0018195467 -0.0018184595][-0.0018259457 -0.0018224403 -0.0018202701 -0.0018191702 -0.001819047 -0.0018193721 -0.0018197881 -0.0018203473 -0.0018209199 -0.0018211562 -0.0018210559 -0.0018206626 -0.001819981 -0.0018191201 -0.0018182194]]...]
INFO - root - 2017-12-09 20:50:56.934492: step 57210, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.852 sec/batch; 65h:08m:11s remains)
INFO - root - 2017-12-09 20:51:05.744049: step 57220, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.856 sec/batch; 65h:26m:13s remains)
INFO - root - 2017-12-09 20:51:14.500590: step 57230, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.894 sec/batch; 68h:20m:38s remains)
INFO - root - 2017-12-09 20:51:22.861018: step 57240, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.888 sec/batch; 67h:52m:27s remains)
INFO - root - 2017-12-09 20:51:31.600468: step 57250, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.890 sec/batch; 68h:04m:15s remains)
INFO - root - 2017-12-09 20:51:40.255518: step 57260, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 66h:22m:13s remains)
INFO - root - 2017-12-09 20:51:48.903034: step 57270, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 66h:25m:53s remains)
INFO - root - 2017-12-09 20:51:57.548823: step 57280, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.881 sec/batch; 67h:20m:24s remains)
INFO - root - 2017-12-09 20:52:06.108029: step 57290, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 65h:55m:04s remains)
INFO - root - 2017-12-09 20:52:14.712173: step 57300, loss = 0.82, batch loss = 0.69 (8.6 examples/sec; 0.931 sec/batch; 71h:10m:59s remains)
2017-12-09 20:52:15.582948: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.0021369802 0.0036265417 0.0049436931 0.0057574683 0.0060446518 0.0057368991 0.0049566096 0.0038217381 0.0025032004 0.0011956753 0.00014206755 -0.00062047259 -0.0011087159 -0.0014266889 -0.0016496766][0.0048472467 0.0075488179 0.010010584 0.011571762 0.012142556 0.011595818 0.010215634 0.008225603 0.0059240367 0.0036806501 0.0018591975 0.00055003294 -0.00032171153 -0.00095716811 -0.0014140187][0.009121052 0.01331575 0.017084841 0.019503681 0.020461131 0.019772172 0.017891359 0.015031044 0.011556286 0.0080241337 0.0049597318 0.0026456956 0.0010049365 -0.00022292917 -0.0010571945][0.015956724 0.021987526 0.027102338 0.030208468 0.031219851 0.030053739 0.027378075 0.023412557 0.01858975 0.013642003 0.0091485251 0.0055600912 0.0028327904 0.00075734209 -0.0006131155][0.024063054 0.031719431 0.037926536 0.041575357 0.042711452 0.041283913 0.038027946 0.033038076 0.026761198 0.020116724 0.013829851 0.0086629055 0.0046417625 0.0016570963 -0.00022732327][0.031043336 0.039626185 0.046279155 0.050116908 0.051371004 0.050012179 0.046700019 0.041283365 0.034077346 0.026094519 0.018148921 0.011402895 0.0060729315 0.0022710231 -2.3106346e-05][0.034267463 0.04282742 0.049369827 0.053254128 0.054794528 0.053871356 0.050954878 0.04562664 0.038085941 0.029367123 0.020370683 0.012595305 0.0064376346 0.0022573895 -0.00013728021][0.032432765 0.040247813 0.046446383 0.050562542 0.052814893 0.052806281 0.05066755 0.045779824 0.03831356 0.02937072 0.019984335 0.011884641 0.0055985246 0.0015968584 -0.00054196909][0.026043456 0.032792035 0.038643986 0.043153644 0.046329357 0.047461834 0.046329863 0.042189296 0.035242256 0.026638079 0.017579425 0.0098778605 0.0041004387 0.000673263 -0.0010172565][0.017820843 0.023196941 0.028452663 0.03317846 0.037079122 0.039217953 0.039089367 0.035898626 0.029878344 0.022160627 0.014093692 0.0073958351 0.0025730836 -0.00011432241 -0.0013527686][0.0099966414 0.013962942 0.01831151 0.022684205 0.026638024 0.029204825 0.02969978 0.027419878 0.022597162 0.016316334 0.0098742135 0.0046995888 0.0011593279 -0.00070639397 -0.0015329554][0.0042828862 0.0068027326 0.0099073965 0.013338088 0.016645545 0.019001985 0.019711165 0.018219607 0.014734353 0.010172113 0.0056203906 0.0021435106 -7.3010451e-05 -0.0011759894 -0.0016525981][0.0007110798 0.0020522685 0.0038516917 0.0059954417 0.0081818225 0.0098316772 0.010421351 0.0095954547 0.007478639 0.0046911309 0.0020001465 8.5418345e-05 -0.0010173362 -0.0015191674 -0.0017331791][-0.0010681087 -0.00050011394 0.00031889614 0.0013523364 0.0024499702 0.003321779 0.003675316 0.0033186432 0.0022888477 0.0009243387 -0.00034092495 -0.0011576223 -0.0015616007 -0.0017192506 -0.0017835428][-0.001722544 -0.0015876932 -0.0013496819 -0.0010055145 -0.0006084044 -0.00026898447 -0.00010581838 -0.00019141147 -0.00052451587 -0.0009834849 -0.001399994 -0.0016511341 -0.0017551482 -0.0017868085 -0.0017992337]]...]
INFO - root - 2017-12-09 20:52:24.013501: step 57310, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.879 sec/batch; 67h:09m:35s remains)
INFO - root - 2017-12-09 20:52:32.607548: step 57320, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.849 sec/batch; 64h:52m:31s remains)
INFO - root - 2017-12-09 20:52:41.116892: step 57330, loss = 0.83, batch loss = 0.70 (9.2 examples/sec; 0.867 sec/batch; 66h:14m:03s remains)
INFO - root - 2017-12-09 20:52:49.335700: step 57340, loss = 0.82, batch loss = 0.69 (9.8 examples/sec; 0.816 sec/batch; 62h:22m:39s remains)
INFO - root - 2017-12-09 20:52:57.902082: step 57350, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.854 sec/batch; 65h:14m:31s remains)
INFO - root - 2017-12-09 20:53:06.735686: step 57360, loss = 0.82, batch loss = 0.69 (8.8 examples/sec; 0.908 sec/batch; 69h:23m:21s remains)
INFO - root - 2017-12-09 20:53:15.453286: step 57370, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.883 sec/batch; 67h:30m:36s remains)
INFO - root - 2017-12-09 20:53:24.262194: step 57380, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.900 sec/batch; 68h:47m:43s remains)
INFO - root - 2017-12-09 20:53:32.989284: step 57390, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.866 sec/batch; 66h:10m:44s remains)
INFO - root - 2017-12-09 20:53:41.898856: step 57400, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.865 sec/batch; 66h:04m:22s remains)
2017-12-09 20:53:42.767474: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.12152167 0.11969332 0.12057132 0.12273527 0.1273149 0.13080572 0.13265282 0.12936491 0.11903472 0.10239117 0.0808469 0.060523078 0.043050807 0.029913057 0.021326043][0.14274667 0.14120391 0.14232442 0.1445774 0.14826924 0.15000395 0.14880495 0.14204305 0.12821074 0.10839521 0.084364943 0.061950736 0.042954329 0.028861113 0.019631675][0.16806005 0.1677444 0.17036672 0.17302085 0.17516421 0.173883 0.16837963 0.15702853 0.13895158 0.11656277 0.091340393 0.067920119 0.048327979 0.03343131 0.023436824][0.19115622 0.19314721 0.19735655 0.20047939 0.20170222 0.19801854 0.18867612 0.1730725 0.15171424 0.12801176 0.10275067 0.079952307 0.061105505 0.046402674 0.035862498][0.21264341 0.21637097 0.22133721 0.22449349 0.2245754 0.21920271 0.20693532 0.18867715 0.16594905 0.14227484 0.11843506 0.097344048 0.080290213 0.067063011 0.056769285][0.23256387 0.23868378 0.24457186 0.24784957 0.24758938 0.24081209 0.22652437 0.20634663 0.18310134 0.16054852 0.13920452 0.1206667 0.10561512 0.094145946 0.084299542][0.24777985 0.25700805 0.26405674 0.26817036 0.26773423 0.26027536 0.24564677 0.2253667 0.20329437 0.18280357 0.16483657 0.14957131 0.13715374 0.12742414 0.11763831][0.25507352 0.26699331 0.27545258 0.28136086 0.28309846 0.27739379 0.26437837 0.2456712 0.22575474 0.20797822 0.19301681 0.18105321 0.17139326 0.16375726 0.1545599][0.25584477 0.26958433 0.27892289 0.2863262 0.29043242 0.28826889 0.27872229 0.26350102 0.24703941 0.23225363 0.2197234 0.21025006 0.20291357 0.19667347 0.18824548][0.25255883 0.26689449 0.27669096 0.2854515 0.29175413 0.29297921 0.28790343 0.27757582 0.26523519 0.2536824 0.24365914 0.23579882 0.22862712 0.22236438 0.21411975][0.24517825 0.25969404 0.26959813 0.27903402 0.28721675 0.29121026 0.29006919 0.28410402 0.27540168 0.266387 0.25786877 0.25098032 0.24401742 0.23716283 0.2285251][0.23279476 0.24678597 0.25668529 0.26682949 0.27631894 0.28269225 0.2850033 0.28281412 0.27746779 0.27043849 0.26265761 0.25555849 0.24821106 0.24065442 0.23168461][0.21701333 0.22943164 0.23868646 0.24894717 0.25923052 0.26755592 0.27273813 0.27341062 0.27077648 0.26559088 0.25871012 0.25101516 0.24261688 0.2339716 0.22481646][0.20124626 0.21183053 0.21912484 0.22842546 0.23776221 0.24650885 0.25317648 0.25643814 0.25651824 0.25305727 0.24753623 0.23986627 0.23139425 0.22257982 0.21349688][0.18599336 0.19517322 0.20048061 0.2078355 0.2157231 0.2231639 0.22953643 0.23333357 0.23451395 0.23282115 0.22904222 0.22301304 0.21566661 0.20790319 0.2004516]]...]
INFO - root - 2017-12-09 20:53:51.256939: step 57410, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 66h:34m:48s remains)
INFO - root - 2017-12-09 20:53:59.833643: step 57420, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.870 sec/batch; 66h:29m:46s remains)
INFO - root - 2017-12-09 20:54:08.397686: step 57430, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.847 sec/batch; 64h:41m:37s remains)
INFO - root - 2017-12-09 20:54:16.829110: step 57440, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.840 sec/batch; 64h:12m:49s remains)
INFO - root - 2017-12-09 20:54:25.584731: step 57450, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.885 sec/batch; 67h:35m:02s remains)
INFO - root - 2017-12-09 20:54:34.204661: step 57460, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.859 sec/batch; 65h:36m:23s remains)
INFO - root - 2017-12-09 20:54:42.984009: step 57470, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.881 sec/batch; 67h:20m:33s remains)
INFO - root - 2017-12-09 20:54:51.683757: step 57480, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.873 sec/batch; 66h:42m:11s remains)
INFO - root - 2017-12-09 20:55:00.416648: step 57490, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.882 sec/batch; 67h:24m:33s remains)
INFO - root - 2017-12-09 20:55:09.121390: step 57500, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.860 sec/batch; 65h:40m:01s remains)
2017-12-09 20:55:10.021309: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0018205966 -0.0018203733 -0.0018204938 -0.0018201396 -0.0018189991 -0.001817398 -0.0018157687 -0.0018149967 -0.0018152749 -0.0018163027 -0.0018176333 -0.0018188809 -0.0018197205 -0.0018199817 -0.0018198384][-0.0018205316 -0.0018202905 -0.0018203274 -0.0018199018 -0.0018187499 -0.0018170296 -0.0018151844 -0.0018142664 -0.0018144918 -0.001815615 -0.0018171201 -0.0018185551 -0.0018195321 -0.0018198658 -0.0018197][-0.0018208044 -0.0018205534 -0.0018204422 -0.0018198981 -0.0018187267 -0.0018168733 -0.001814788 -0.0018137057 -0.0018138705 -0.0018150848 -0.0018167867 -0.0018184218 -0.0018195817 -0.0018200316 -0.001819895][-0.0018210479 -0.0018207359 -0.0018204694 -0.0018198183 -0.0018185433 -0.0018164758 -0.001814154 -0.001812926 -0.0018130697 -0.0018143469 -0.0018161853 -0.0018180049 -0.0018193864 -0.0018200042 -0.0018199828][-0.0018211739 -0.0018207822 -0.001820332 -0.0018194683 -0.0018178991 -0.0018155607 -0.0018130864 -0.0018118061 -0.0018119747 -0.0018133919 -0.0018153688 -0.0018173049 -0.0018188916 -0.0018197426 -0.0018199208][-0.0018212294 -0.0018206825 -0.0018200174 -0.0018188126 -0.0018168179 -0.0018142224 -0.0018116409 -0.0018103461 -0.0018106466 -0.0018122448 -0.0018143454 -0.0018163745 -0.0018181766 -0.0018193127 -0.0018197817][-0.0018211324 -0.0018204284 -0.0018195519 -0.0018179925 -0.0018156552 -0.001812948 -0.0018104522 -0.0018092033 -0.0018096048 -0.0018112608 -0.0018133681 -0.0018154774 -0.001817411 -0.0018187517 -0.0018194095][-0.001820958 -0.0018200647 -0.0018190066 -0.0018172706 -0.0018148825 -0.0018123228 -0.0018100286 -0.0018087907 -0.0018091369 -0.001810652 -0.0018126451 -0.0018146522 -0.0018165581 -0.0018179647 -0.0018186468][-0.0018206281 -0.0018196468 -0.0018185771 -0.0018168894 -0.0018146997 -0.0018124358 -0.0018104285 -0.0018092509 -0.0018094261 -0.0018106573 -0.0018124145 -0.00181413 -0.0018157084 -0.0018168919 -0.0018173874][-0.0018201512 -0.0018192461 -0.0018183041 -0.0018168282 -0.0018149847 -0.0018131068 -0.001811469 -0.0018104347 -0.0018104783 -0.0018114741 -0.0018129828 -0.0018143413 -0.0018153491 -0.0018160034 -0.0018160748][-0.0018196821 -0.0018189061 -0.0018181979 -0.0018170708 -0.0018155966 -0.0018141012 -0.0018128445 -0.0018120252 -0.001812023 -0.0018128911 -0.0018142278 -0.0018152562 -0.0018156323 -0.0018155029 -0.0018149117][-0.0018194329 -0.0018188382 -0.0018183965 -0.0018176438 -0.0018165045 -0.0018152292 -0.0018142144 -0.0018135936 -0.0018136246 -0.0018144563 -0.001815667 -0.0018163732 -0.0018161898 -0.0018154188 -0.001814178][-0.0018192123 -0.001818902 -0.0018187423 -0.0018183365 -0.0018173913 -0.0018162121 -0.0018152862 -0.0018148241 -0.001814991 -0.0018159021 -0.0018169954 -0.0018174646 -0.0018170584 -0.0018160108 -0.0018143906][-0.0018187828 -0.001818893 -0.0018190588 -0.0018188863 -0.0018180034 -0.0018167694 -0.0018157638 -0.0018153095 -0.0018155748 -0.0018166068 -0.0018177136 -0.001818175 -0.0018177844 -0.0018167155 -0.0018149797][-0.001818443 -0.0018191303 -0.0018197058 -0.0018196949 -0.0018187535 -0.001817324 -0.0018159988 -0.0018153153 -0.0018155003 -0.0018165715 -0.0018177814 -0.0018183988 -0.001818128 -0.0018170899 -0.0018153184]]...]
INFO - root - 2017-12-09 20:55:18.464182: step 57510, loss = 0.83, batch loss = 0.70 (9.6 examples/sec; 0.834 sec/batch; 63h:44m:35s remains)
INFO - root - 2017-12-09 20:55:27.132244: step 57520, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 66h:11m:45s remains)
INFO - root - 2017-12-09 20:55:35.900888: step 57530, loss = 0.83, batch loss = 0.70 (9.3 examples/sec; 0.860 sec/batch; 65h:42m:37s remains)
INFO - root - 2017-12-09 20:55:44.345265: step 57540, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.839 sec/batch; 64h:03m:57s remains)
INFO - root - 2017-12-09 20:55:52.821000: step 57550, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.852 sec/batch; 65h:03m:03s remains)
INFO - root - 2017-12-09 20:56:01.246616: step 57560, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.854 sec/batch; 65h:11m:31s remains)
INFO - root - 2017-12-09 20:56:09.803968: step 57570, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.847 sec/batch; 64h:39m:16s remains)
INFO - root - 2017-12-09 20:56:18.516985: step 57580, loss = 0.82, batch loss = 0.69 (8.7 examples/sec; 0.925 sec/batch; 70h:36m:59s remains)
INFO - root - 2017-12-09 20:56:27.117523: step 57590, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.857 sec/batch; 65h:27m:33s remains)
INFO - root - 2017-12-09 20:56:35.759088: step 57600, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.861 sec/batch; 65h:43m:30s remains)
2017-12-09 20:56:36.711040: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0018295688 -0.0018283848 -0.0018277143 -0.0018273927 -0.0018273594 -0.0018275736 -0.0018279681 -0.001828444 -0.0018289746 -0.0018293982 -0.0018296011 -0.0018296269 -0.0018294659 -0.001829244 -0.0018291493][-0.0018296773 -0.0018285939 -0.0018280178 -0.0018277785 -0.0018278682 -0.001828188 -0.0018286326 -0.0018291171 -0.0018295927 -0.0018298693 -0.0018299123 -0.0018298205 -0.00182957 -0.001829233 -0.0018290168][-0.0018299747 -0.0018290343 -0.0018285968 -0.0018284785 -0.0018286818 -0.0018290449 -0.0018294854 -0.0018299478 -0.0018303751 -0.0018305246 -0.0018303951 -0.0018301524 -0.0018298062 -0.0018293835 -0.0018290703][-0.001830305 -0.001829499 -0.0018291676 -0.0018291816 -0.0018295373 -0.0018299402 -0.001830317 -0.0018306997 -0.0018310623 -0.001831088 -0.0018307744 -0.001830377 -0.0018299575 -0.0018295004 -0.0018291407][-0.0018307088 -0.0018299817 -0.0018296643 -0.001829745 -0.001830247 -0.0018307238 -0.0018310545 -0.0018314016 -0.0018317965 -0.0018317758 -0.0018312978 -0.001830717 -0.0018301976 -0.0018296964 -0.0018292841][-0.0018311075 -0.0018304625 -0.0018301652 -0.0018302536 -0.0018308609 -0.0018314901 -0.0018318236 -0.0018321146 -0.0018325491 -0.0018325582 -0.001831938 -0.001831101 -0.0018304012 -0.0018298217 -0.0018293712][-0.0018315041 -0.0018308529 -0.001830537 -0.0018306264 -0.0018313278 -0.0018321064 -0.001832524 -0.0018328398 -0.0018332575 -0.0018332406 -0.0018324619 -0.0018313471 -0.0018303769 -0.0018296627 -0.0018292117][-0.0018317696 -0.0018309942 -0.0018305532 -0.0018305774 -0.001831339 -0.0018322449 -0.0018328371 -0.0018332726 -0.0018337462 -0.0018336866 -0.0018327357 -0.001831378 -0.0018301517 -0.001829288 -0.0018288821][-0.0018319977 -0.0018311647 -0.001830599 -0.0018305401 -0.0018313287 -0.0018323609 -0.0018331705 -0.0018336737 -0.0018341152 -0.0018340034 -0.0018329386 -0.0018313947 -0.0018299253 -0.0018288606 -0.0018284686][-0.00183213 -0.0018312937 -0.0018307095 -0.0018306688 -0.0018314847 -0.0018325869 -0.0018335122 -0.0018340038 -0.0018343113 -0.0018340396 -0.0018328639 -0.0018312018 -0.0018295092 -0.001828183 -0.0018276727][-0.0018320217 -0.0018311826 -0.0018306115 -0.001830625 -0.0018314369 -0.0018324736 -0.0018333041 -0.0018336067 -0.0018336794 -0.0018332355 -0.0018320641 -0.0018304912 -0.0018287908 -0.0018274144 -0.0018268578][-0.0018315832 -0.0018306809 -0.0018301081 -0.0018301409 -0.0018308323 -0.0018316684 -0.0018322874 -0.0018323557 -0.0018321739 -0.0018316178 -0.0018305882 -0.0018293408 -0.0018279391 -0.0018267749 -0.0018262228][-0.0018309724 -0.0018299541 -0.0018293018 -0.0018293003 -0.0018298107 -0.0018304263 -0.00183084 -0.0018307951 -0.0018304868 -0.0018298849 -0.0018290359 -0.0018280455 -0.0018269722 -0.0018261303 -0.0018256598][-0.001830522 -0.0018293512 -0.0018285393 -0.0018284363 -0.001828796 -0.001829234 -0.0018295243 -0.0018294884 -0.0018292045 -0.0018286797 -0.0018280554 -0.0018273281 -0.0018266005 -0.0018260708 -0.0018257456][-0.0018304284 -0.0018291366 -0.0018281902 -0.0018279412 -0.0018281513 -0.0018284385 -0.0018286217 -0.0018285954 -0.0018284051 -0.0018280372 -0.0018276208 -0.0018271799 -0.0018267896 -0.0018265598 -0.0018264011]]...]
INFO - root - 2017-12-09 20:56:45.159119: step 57610, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.845 sec/batch; 64h:31m:08s remains)
INFO - root - 2017-12-09 20:56:53.850344: step 57620, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.887 sec/batch; 67h:44m:19s remains)
INFO - root - 2017-12-09 20:57:02.479356: step 57630, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 65h:56m:24s remains)
INFO - root - 2017-12-09 20:57:10.888221: step 57640, loss = 0.82, batch loss = 0.69 (10.7 examples/sec; 0.746 sec/batch; 56h:56m:02s remains)
INFO - root - 2017-12-09 20:57:19.609455: step 57650, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.885 sec/batch; 67h:36m:19s remains)
INFO - root - 2017-12-09 20:57:28.325583: step 57660, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.882 sec/batch; 67h:18m:18s remains)
INFO - root - 2017-12-09 20:57:37.041478: step 57670, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.861 sec/batch; 65h:41m:45s remains)
INFO - root - 2017-12-09 20:57:45.753544: step 57680, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.874 sec/batch; 66h:44m:07s remains)
INFO - root - 2017-12-09 20:57:54.444168: step 57690, loss = 0.81, batch loss = 0.68 (9.3 examples/sec; 0.860 sec/batch; 65h:40m:11s remains)
INFO - root - 2017-12-09 20:58:03.068564: step 57700, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.863 sec/batch; 65h:54m:06s remains)
2017-12-09 20:58:03.900645: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0017931171 -0.0017914632 -0.0017914595 -0.0017915675 -0.0017915119 -0.0017913547 -0.0017910862 -0.0017909795 -0.0017910633 -0.001791331 -0.0017916696 -0.0017918522 -0.001791787 -0.0017915282 -0.0017911616][-0.0017921393 -0.0017905246 -0.0017906374 -0.001790829 -0.001790863 -0.0017908139 -0.0017905802 -0.0017904608 -0.0017905725 -0.0017908781 -0.0017912056 -0.0017913446 -0.0017912594 -0.0017910219 -0.0017907526][-0.0017921913 -0.0017906683 -0.001790739 -0.0017909269 -0.0017909658 -0.00179101 -0.0017907934 -0.0017906675 -0.0017908346 -0.0017911886 -0.0017914944 -0.001791549 -0.001791433 -0.0017912103 -0.0017910172][-0.0017923109 -0.0017909092 -0.0017909665 -0.001791158 -0.0017912213 -0.0017913459 -0.0017911219 -0.0017909168 -0.0017910828 -0.0017914539 -0.0017917262 -0.0017917255 -0.0017915976 -0.0017914176 -0.001791351][-0.0017925941 -0.0017912172 -0.0017913429 -0.001791585 -0.0017916991 -0.0017918668 -0.0017915836 -0.0017912143 -0.0017913028 -0.001791631 -0.0017917899 -0.00179171 -0.001791585 -0.0017914664 -0.0017914975][-0.0017929841 -0.0017915871 -0.0017918035 -0.0017921644 -0.0017923319 -0.0017925276 -0.0017922118 -0.0017916652 -0.0017916133 -0.0017918835 -0.0017919226 -0.0017917415 -0.0017916019 -0.0017915026 -0.0017915611][-0.0017934281 -0.0017920282 -0.0017922538 -0.0017927076 -0.0017929265 -0.0017931383 -0.001792811 -0.0017921615 -0.0017920078 -0.0017922665 -0.0017922508 -0.0017919476 -0.0017917167 -0.0017915731 -0.0017915384][-0.0017939578 -0.0017925404 -0.0017927639 -0.0017932444 -0.0017935336 -0.0017937477 -0.0017934584 -0.0017927811 -0.001792527 -0.0017927893 -0.0017928127 -0.0017925061 -0.0017921941 -0.0017919939 -0.0017918363][-0.0017946238 -0.0017931336 -0.0017933919 -0.0017939 -0.0017942601 -0.0017945059 -0.001794336 -0.001793741 -0.0017934089 -0.0017936163 -0.0017936591 -0.0017933545 -0.0017929961 -0.0017927199 -0.0017923921][-0.001795208 -0.0017937723 -0.0017940322 -0.001794517 -0.0017949298 -0.0017952336 -0.001795295 -0.0017949868 -0.0017947159 -0.0017948439 -0.00179487 -0.0017945605 -0.0017941246 -0.0017937165 -0.0017931924][-0.0017959117 -0.001794553 -0.0017947211 -0.0017951623 -0.0017955791 -0.0017959577 -0.0017962728 -0.0017963677 -0.0017963407 -0.0017964736 -0.0017964859 -0.001796134 -0.001795581 -0.0017949751 -0.0017942184][-0.0017969937 -0.001795537 -0.0017955251 -0.0017958587 -0.0017962925 -0.001796775 -0.0017973058 -0.0017977639 -0.0017980566 -0.0017982712 -0.0017982718 -0.0017978827 -0.0017972013 -0.0017963832 -0.001795389][-0.001798213 -0.0017967273 -0.0017965592 -0.0017968314 -0.0017973037 -0.0017979209 -0.0017986103 -0.0017992971 -0.0017998283 -0.0018001237 -0.001800135 -0.001799742 -0.0017989968 -0.0017980506 -0.0017969201][-0.0017995387 -0.0017981457 -0.0017980221 -0.0017983699 -0.0017989282 -0.0017996036 -0.0018002872 -0.0018009598 -0.0018014539 -0.0018016313 -0.0018015422 -0.0018011278 -0.0018003534 -0.0017993732 -0.0017982468][-0.0018007099 -0.0017993944 -0.0017993237 -0.0017997619 -0.0018003515 -0.0018009907 -0.001801563 -0.0018020638 -0.0018023422 -0.0018023005 -0.0018020665 -0.0018015839 -0.0018008158 -0.0017998851 -0.0017988724]]...]
INFO - root - 2017-12-09 20:58:12.520384: step 57710, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.888 sec/batch; 67h:48m:42s remains)
INFO - root - 2017-12-09 20:58:20.992640: step 57720, loss = 0.82, batch loss = 0.69 (9.8 examples/sec; 0.816 sec/batch; 62h:16m:23s remains)
INFO - root - 2017-12-09 20:58:29.506779: step 57730, loss = 0.82, batch loss = 0.69 (11.0 examples/sec; 0.729 sec/batch; 55h:40m:21s remains)
INFO - root - 2017-12-09 20:58:38.046147: step 57740, loss = 0.82, batch loss = 0.69 (10.9 examples/sec; 0.737 sec/batch; 56h:16m:15s remains)
INFO - root - 2017-12-09 20:58:46.698617: step 57750, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.886 sec/batch; 67h:36m:21s remains)
INFO - root - 2017-12-09 20:58:55.375400: step 57760, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.849 sec/batch; 64h:47m:40s remains)
INFO - root - 2017-12-09 20:59:03.970498: step 57770, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.880 sec/batch; 67h:11m:12s remains)
INFO - root - 2017-12-09 20:59:12.760776: step 57780, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.899 sec/batch; 68h:36m:50s remains)
INFO - root - 2017-12-09 20:59:21.400849: step 57790, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 66h:07m:27s remains)
INFO - root - 2017-12-09 20:59:30.030313: step 57800, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 66h:28m:27s remains)
2017-12-09 20:59:30.920225: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.00063488318 0.0021626344 0.0053084865 0.0097191017 0.014451032 0.018553561 0.021461478 0.023117991 0.0240619 0.025279293 0.027673125 0.031169567 0.035124324 0.038434498 0.039832931][0.0017754257 0.0040924535 0.0087492755 0.015207916 0.021933123 0.027232431 0.029747002 0.029062636 0.026033042 0.022429397 0.020160047 0.019859655 0.021162428 0.022893492 0.023729915][0.0044083409 0.0083023356 0.015757211 0.02578491 0.036048677 0.043867551 0.046905328 0.044141978 0.036694821 0.027183253 0.018789385 0.01329685 0.010930324 0.010504656 0.010499422][0.0085029677 0.014802617 0.026204005 0.041200668 0.056475751 0.068149753 0.072709814 0.06835483 0.056385875 0.040402334 0.025002256 0.013429244 0.0067571881 0.0038310946 0.0028123613][0.014057358 0.023286207 0.039470408 0.060504925 0.081950717 0.0984323 0.10507972 0.099322252 0.082594074 0.059686244 0.036757648 0.018774452 0.0075871996 0.0020786882 -5.9755053e-05][0.020156773 0.032225154 0.052955307 0.079817832 0.10742272 0.12882714 0.13777609 0.13084997 0.10955916 0.07996653 0.049743436 0.025733929 0.010406626 0.0026537674 -0.00044605229][0.024624642 0.038594797 0.062488273 0.093513936 0.12568006 0.15077721 0.16149108 0.15374979 0.12914269 0.094677925 0.059168439 0.030921265 0.012750998 0.0035447013 -0.00017412542][0.026194295 0.040487468 0.0650132 0.097207785 0.13110678 0.15789703 0.1696595 0.16184542 0.13605015 0.099703252 0.062092252 0.032328382 0.013240883 0.0036948593 -0.00012629677][0.024079563 0.036899529 0.059143744 0.08884161 0.12073215 0.14644757 0.15815148 0.15121718 0.12703758 0.09271346 0.057147577 0.029216252 0.011511496 0.0029309965 -0.00037362648][0.018984156 0.02901314 0.046901889 0.071241945 0.097958364 0.12000408 0.13045827 0.12507936 0.10492039 0.076056443 0.046161458 0.022844149 0.0083255963 0.0016062491 -0.00080688647][0.012211628 0.019185161 0.031793833 0.049188644 0.068717666 0.085242882 0.093439192 0.0898641 0.075188145 0.05398605 0.0320793 0.015093726 0.0047537275 0.00022591429 -0.001232126][0.0057445113 0.0099138245 0.017556585 0.028256148 0.040557422 0.051237732 0.056819659 0.054919641 0.045852322 0.032564264 0.018827157 0.0081995148 0.0018545516 -0.00077195524 -0.0015064332][0.0014354357 0.0034753853 0.0072115413 0.012609256 0.019036843 0.024846749 0.028131265 0.027442934 0.022847934 0.015924722 0.0087233987 0.0031618471 -9.549188e-05 -0.0013561151 -0.0016472042][-0.00087133213 -7.4942596e-05 0.0013663358 0.0034691719 0.0060276543 0.0084235817 0.009856673 0.009677859 0.0078858854 0.0051261871 0.0022555003 5.8628386e-05 -0.0011942061 -0.0016361425 -0.001712318][-0.0016764284 -0.001469233 -0.0010636995 -0.00044184236 0.00033088808 0.0010563853 0.0014919013 0.0014495688 0.00093761191 0.00014736585 -0.00067026541 -0.0012843315 -0.0016190177 -0.0017228368 -0.001736601]]...]
INFO - root - 2017-12-09 20:59:39.343838: step 57810, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.856 sec/batch; 65h:18m:32s remains)
INFO - root - 2017-12-09 20:59:47.956219: step 57820, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 66h:27m:28s remains)
INFO - root - 2017-12-09 20:59:56.635951: step 57830, loss = 0.82, batch loss = 0.69 (10.4 examples/sec; 0.766 sec/batch; 58h:26m:30s remains)
INFO - root - 2017-12-09 21:00:05.167897: step 57840, loss = 0.82, batch loss = 0.69 (11.2 examples/sec; 0.712 sec/batch; 54h:19m:04s remains)
INFO - root - 2017-12-09 21:00:13.984038: step 57850, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.901 sec/batch; 68h:45m:35s remains)
INFO - root - 2017-12-09 21:00:22.728750: step 57860, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.879 sec/batch; 67h:03m:32s remains)
INFO - root - 2017-12-09 21:00:31.304762: step 57870, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 65h:54m:45s remains)
INFO - root - 2017-12-09 21:00:39.870853: step 57880, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.863 sec/batch; 65h:51m:28s remains)
INFO - root - 2017-12-09 21:00:48.322307: step 57890, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.831 sec/batch; 63h:21m:31s remains)
INFO - root - 2017-12-09 21:00:56.920064: step 57900, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.872 sec/batch; 66h:28m:36s remains)
2017-12-09 21:00:57.837948: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0018179057 -0.0018178549 -0.0018183943 -0.0018189597 -0.0018197303 -0.0018205708 -0.0018211422 -0.0018213146 -0.0018211983 -0.0018206643 -0.0018196554 -0.0018186281 -0.0018180041 -0.001817719 -0.0018177336][-0.0018172953 -0.0018173538 -0.001818037 -0.0018187743 -0.0018197875 -0.0018208649 -0.0018216143 -0.0018218686 -0.0018216678 -0.0018209101 -0.0018197169 -0.0018185546 -0.0018178164 -0.0018173498 -0.0018171937][-0.0018168729 -0.0018169872 -0.0018177379 -0.0018186297 -0.0018198249 -0.0018210828 -0.00182191 -0.0018222267 -0.0018220812 -0.0018213487 -0.0018202089 -0.0018189867 -0.0018179467 -0.0018171167 -0.0018167613][-0.0018165403 -0.0018165872 -0.0018173925 -0.0018184767 -0.001819875 -0.001821278 -0.0018221411 -0.001822493 -0.0018224057 -0.0018218174 -0.0018207622 -0.0018194655 -0.0018180589 -0.0018169326 -0.0018164618][-0.0018162013 -0.0018161709 -0.0018170255 -0.001818206 -0.0018197082 -0.0018211579 -0.0018220081 -0.0018222532 -0.0018220965 -0.0018216567 -0.0018207283 -0.0018194242 -0.0018178719 -0.0018167067 -0.0018162786][-0.0018156936 -0.0018155942 -0.0018164915 -0.0018176833 -0.0018191644 -0.0018205141 -0.00182118 -0.0018211899 -0.0018209926 -0.00182073 -0.0018200398 -0.0018189569 -0.0018175229 -0.0018164562 -0.0018161504][-0.0018155725 -0.0018152853 -0.0018160124 -0.0018170014 -0.0018182423 -0.0018193256 -0.0018196546 -0.0018194306 -0.0018192278 -0.0018191934 -0.0018188332 -0.0018180244 -0.0018168044 -0.0018159585 -0.0018158539][-0.0018156823 -0.0018150591 -0.0018154208 -0.0018160518 -0.0018169306 -0.0018176351 -0.0018177306 -0.0018174615 -0.0018173736 -0.0018175325 -0.0018174525 -0.0018168357 -0.0018157776 -0.0018151723 -0.0018153758][-0.0018155446 -0.0018147031 -0.0018148323 -0.001815281 -0.0018159448 -0.0018163826 -0.0018163647 -0.0018161086 -0.0018160262 -0.0018161379 -0.0018160975 -0.001815602 -0.0018147677 -0.0018144653 -0.0018150008][-0.0018159145 -0.0018150586 -0.0018150357 -0.0018153148 -0.0018157628 -0.0018159281 -0.0018157426 -0.0018154695 -0.0018153393 -0.0018152718 -0.0018151008 -0.0018146783 -0.0018141617 -0.0018141329 -0.0018148149][-0.0018164875 -0.0018155831 -0.0018153867 -0.0018154383 -0.0018156322 -0.0018155642 -0.0018152486 -0.0018150632 -0.001814976 -0.0018147682 -0.0018145124 -0.0018142471 -0.0018140737 -0.0018142313 -0.0018148588][-0.0018169599 -0.001815961 -0.0018156157 -0.0018155277 -0.0018155579 -0.0018153848 -0.0018150873 -0.0018150306 -0.0018149986 -0.001814734 -0.0018144426 -0.0018142819 -0.0018143284 -0.0018146082 -0.0018151295][-0.0018169108 -0.0018160213 -0.0018156227 -0.0018154964 -0.0018154682 -0.0018153336 -0.0018151593 -0.0018152016 -0.001815205 -0.001814961 -0.0018146649 -0.0018145402 -0.0018146962 -0.001815025 -0.0018154127][-0.0018170966 -0.0018163078 -0.0018159685 -0.0018159671 -0.0018159747 -0.0018158909 -0.0018157751 -0.0018158186 -0.0018157779 -0.0018155016 -0.0018151858 -0.0018150675 -0.0018152155 -0.0018154857 -0.0018157432][-0.0018175204 -0.0018167795 -0.0018165105 -0.0018165713 -0.0018165973 -0.0018165181 -0.001816416 -0.0018164518 -0.0018164143 -0.001816166 -0.0018158823 -0.0018157698 -0.0018158441 -0.0018159816 -0.0018160978]]...]
INFO - root - 2017-12-09 21:01:06.257734: step 57910, loss = 0.82, batch loss = 0.69 (9.7 examples/sec; 0.821 sec/batch; 62h:36m:44s remains)
INFO - root - 2017-12-09 21:01:14.948371: step 57920, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.870 sec/batch; 66h:21m:17s remains)
INFO - root - 2017-12-09 21:01:23.470863: step 57930, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.883 sec/batch; 67h:21m:54s remains)
INFO - root - 2017-12-09 21:01:32.180302: step 57940, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.890 sec/batch; 67h:52m:00s remains)
INFO - root - 2017-12-09 21:01:40.756145: step 57950, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.854 sec/batch; 65h:06m:42s remains)
INFO - root - 2017-12-09 21:01:49.517581: step 57960, loss = 0.82, batch loss = 0.70 (9.0 examples/sec; 0.886 sec/batch; 67h:31m:46s remains)
INFO - root - 2017-12-09 21:01:58.246393: step 57970, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 66h:09m:20s remains)
INFO - root - 2017-12-09 21:02:06.897763: step 57980, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 66h:12m:24s remains)
INFO - root - 2017-12-09 21:02:15.695743: step 57990, loss = 0.82, batch loss = 0.69 (8.1 examples/sec; 0.982 sec/batch; 74h:51m:01s remains)
INFO - root - 2017-12-09 21:02:24.330614: step 58000, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.861 sec/batch; 65h:37m:34s remains)
2017-12-09 21:02:25.178731: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.21893722 0.20854293 0.19587083 0.18256383 0.16848576 0.15562505 0.143118 0.13141738 0.1193216 0.1066497 0.093063429 0.078259371 0.0628051 0.047561884 0.03409414][0.27548975 0.26390657 0.24935526 0.23325478 0.21629277 0.20064092 0.18556765 0.17141595 0.15662849 0.14086805 0.12363323 0.1046043 0.084142938 0.063648321 0.045130044][0.337681 0.32370809 0.30606717 0.28586739 0.26571077 0.247072 0.22996426 0.2139513 0.19698766 0.17838226 0.15742016 0.13402657 0.10833517 0.082308955 0.058348957][0.39469147 0.37981552 0.35923159 0.33469856 0.31084517 0.28924897 0.27082738 0.25387344 0.23620044 0.21604052 0.19221172 0.16467176 0.13370845 0.10195876 0.07232926][0.43737236 0.423074 0.40106338 0.37358388 0.34684131 0.32241446 0.30270359 0.28530595 0.26798031 0.24768354 0.22299249 0.19320832 0.15830584 0.1214409 0.086472109][0.46541125 0.45245436 0.42988688 0.40175074 0.37494552 0.35002482 0.33029157 0.31261611 0.29563677 0.27561221 0.25048798 0.21882387 0.18081386 0.1398204 0.1000967][0.47893894 0.46793437 0.445767 0.4179365 0.39136642 0.36804563 0.35066828 0.33483413 0.31955722 0.30030245 0.27512434 0.24179029 0.20075358 0.15580724 0.11190769][0.48233739 0.47304097 0.45165458 0.42501089 0.40003353 0.37881821 0.36358982 0.34994912 0.33696297 0.31919032 0.29439744 0.25984925 0.21652322 0.16834261 0.12090375][0.47871408 0.47120789 0.45089725 0.42652178 0.4038609 0.38569152 0.37340364 0.3621324 0.35105318 0.3348279 0.3108342 0.27551928 0.23014332 0.17918672 0.1288462][0.47126359 0.4654161 0.44732913 0.42710918 0.40898946 0.39434281 0.38419282 0.37409854 0.36315268 0.34673551 0.32208282 0.28598943 0.2392889 0.18638863 0.13402134][0.46681297 0.46327969 0.44795769 0.43099925 0.41638991 0.40463212 0.39632627 0.38629156 0.37402079 0.35575643 0.32842806 0.28978091 0.24085039 0.18668449 0.13365205][0.45833555 0.4575834 0.44595596 0.43361926 0.42336228 0.41450137 0.4079763 0.39769349 0.383577 0.36261302 0.33196381 0.29013151 0.23843479 0.18301873 0.12981853][0.44905427 0.44971851 0.43970859 0.42992106 0.4232623 0.41781574 0.41367197 0.40453216 0.39030218 0.36694205 0.33286604 0.28794953 0.23396841 0.17768191 0.12458723][0.44108471 0.4422946 0.43246359 0.42283222 0.41604254 0.4105438 0.40709731 0.39931172 0.38614216 0.36294979 0.32868788 0.28307834 0.22827484 0.17183404 0.11936027][0.42865476 0.43014771 0.42044336 0.41038632 0.40324172 0.3970522 0.39261353 0.38471019 0.37162295 0.3488591 0.31568706 0.27129257 0.21821584 0.16358696 0.1130764]]...]
INFO - root - 2017-12-09 21:02:33.324025: step 58010, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.866 sec/batch; 66h:00m:15s remains)
INFO - root - 2017-12-09 21:02:42.251339: step 58020, loss = 0.82, batch loss = 0.69 (8.5 examples/sec; 0.941 sec/batch; 71h:44m:03s remains)
INFO - root - 2017-12-09 21:02:50.739420: step 58030, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.840 sec/batch; 64h:01m:22s remains)
INFO - root - 2017-12-09 21:02:59.592167: step 58040, loss = 0.82, batch loss = 0.69 (8.1 examples/sec; 0.982 sec/batch; 74h:53m:31s remains)
INFO - root - 2017-12-09 21:03:08.002364: step 58050, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.883 sec/batch; 67h:17m:19s remains)
INFO - root - 2017-12-09 21:03:16.630987: step 58060, loss = 0.82, batch loss = 0.69 (9.7 examples/sec; 0.829 sec/batch; 63h:11m:34s remains)
INFO - root - 2017-12-09 21:03:25.094880: step 58070, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.841 sec/batch; 64h:05m:11s remains)
INFO - root - 2017-12-09 21:03:33.580379: step 58080, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.852 sec/batch; 64h:56m:37s remains)
INFO - root - 2017-12-09 21:03:42.221084: step 58090, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.852 sec/batch; 64h:56m:38s remains)
INFO - root - 2017-12-09 21:03:50.798554: step 58100, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.858 sec/batch; 65h:25m:20s remains)
2017-12-09 21:03:51.680956: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.0014154214 0.0021537594 0.0027833385 0.0032297266 0.0034815641 0.0035838634 0.0036092047 0.0036173197 0.0036403835 0.0036514411 0.0035893884 0.0033952864 0.0030203131 0.0024517816 0.0016926486][0.0028595617 0.0039803255 0.0049087163 0.0055674245 0.0059461207 0.0060753464 0.0060552391 0.0059885527 0.00595167 0.0059190434 0.0058223009 0.0055778129 0.00510231 0.0043809609 0.0033784369][0.00368331 0.0049621235 0.0060037184 0.0067712278 0.0072506233 0.0074351965 0.0074232058 0.0073276828 0.0072448454 0.0071587549 0.0070056538 0.0066977567 0.00615227 0.0053523951 0.004246919][0.0037428411 0.0049886522 0.0059880265 0.0067515564 0.0072683566 0.0074981009 0.0075139259 0.0074302317 0.0073535484 0.0072583719 0.0070792395 0.0067433179 0.0061799749 0.005386149 0.0042924015][0.0031748223 0.0042844932 0.0051669823 0.0058489838 0.0063275103 0.0065505668 0.0065728421 0.0064939647 0.0064143906 0.0063183787 0.0061475397 0.0058266758 0.0052967239 0.004572934 0.0035978779][0.0024448493 0.0034041437 0.0041676238 0.0047498932 0.0051384666 0.0053021265 0.0052868016 0.0051808194 0.0050677783 0.0049399678 0.0047586192 0.0044475049 0.003963131 0.0033218828 0.0024905545][0.0016175049 0.0024449988 0.0031026597 0.0035712509 0.0038403794 0.003913979 0.0038207322 0.0036418242 0.0034602829 0.0032870793 0.0030943132 0.0028011864 0.0023789713 0.0018367536 0.0011720991][0.00060657633 0.0012606973 0.00180028 0.00217563 0.0023509087 0.0023635328 0.0022382839 0.0020357445 0.0018200987 0.0016233156 0.001437896 0.0011850692 0.00084043841 0.0004150212 -6.5288739e-05][-0.00049968308 -8.6935936e-05 0.00026458513 0.000499176 0.00058619108 0.00056342466 0.00044314668 0.00026783894 8.3762454e-05 -7.2196824e-05 -0.00020002585 -0.00035510992 -0.0005553097 -0.00079516752 -0.0010488273][-0.0013936798 -0.0012266901 -0.0010799705 -0.00098264636 -0.00094896584 -0.00096145144 -0.0010166966 -0.0010989257 -0.0011840535 -0.0012505604 -0.0013026264 -0.0013655339 -0.0014428545 -0.0015276729 -0.0016103578][-0.0017652157 -0.0017332643 -0.0017079223 -0.0016962731 -0.0017026099 -0.0017202829 -0.0017448177 -0.0017719362 -0.001794143 -0.0018055016 -0.0018078851 -0.0018094877 -0.0018125847 -0.0018155304 -0.001817379][-0.0018231994 -0.0018186147 -0.001815328 -0.0018147478 -0.0018178478 -0.0018234084 -0.0018280003 -0.0018303385 -0.001831493 -0.0018317386 -0.0018316553 -0.0018312992 -0.0018307082 -0.0018298671 -0.0018288894][-0.0018228362 -0.0018198158 -0.0018176589 -0.0018176036 -0.001820747 -0.0018260074 -0.0018298486 -0.001831395 -0.0018322251 -0.0018326063 -0.00183274 -0.0018327062 -0.0018323954 -0.0018318754 -0.0018312145][-0.0018256448 -0.001823362 -0.0018219682 -0.0018219738 -0.0018239005 -0.0018268282 -0.0018293932 -0.0018304881 -0.0018309749 -0.0018311825 -0.0018314959 -0.0018315905 -0.0018314295 -0.0018311918 -0.0018308243][-0.001831386 -0.0018302326 -0.0018292734 -0.0018286826 -0.0018287055 -0.0018291557 -0.0018296303 -0.0018299065 -0.0018300887 -0.0018300244 -0.0018300742 -0.0018299758 -0.0018299216 -0.0018297789 -0.001829612]]...]
INFO - root - 2017-12-09 21:04:00.149589: step 58110, loss = 0.82, batch loss = 0.69 (10.5 examples/sec; 0.763 sec/batch; 58h:10m:49s remains)
INFO - root - 2017-12-09 21:04:08.781459: step 58120, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.883 sec/batch; 67h:19m:31s remains)
INFO - root - 2017-12-09 21:04:17.229916: step 58130, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.859 sec/batch; 65h:28m:36s remains)
INFO - root - 2017-12-09 21:04:25.811775: step 58140, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 66h:13m:47s remains)
INFO - root - 2017-12-09 21:04:34.452622: step 58150, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.883 sec/batch; 67h:18m:35s remains)
INFO - root - 2017-12-09 21:04:43.084633: step 58160, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.836 sec/batch; 63h:43m:07s remains)
INFO - root - 2017-12-09 21:04:51.910888: step 58170, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.865 sec/batch; 65h:56m:10s remains)
INFO - root - 2017-12-09 21:05:00.451934: step 58180, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 66h:10m:09s remains)
INFO - root - 2017-12-09 21:05:09.109254: step 58190, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 66h:09m:05s remains)
INFO - root - 2017-12-09 21:05:17.809664: step 58200, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 66h:11m:38s remains)
2017-12-09 21:05:18.666976: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.001747536 -0.001745184 -0.0017410894 -0.0017656939 -0.0017848485 -0.0017849084 -0.0017448775 -0.0016722139 -0.0016007998 -0.001542887 -0.001501164 -0.0014650053 -0.0014546175 -0.0014363988 -0.0014342908][-0.0016383864 -0.0016388471 -0.0016358275 -0.0016659705 -0.0016892591 -0.0017027312 -0.0016692344 -0.001601508 -0.0015371649 -0.0014854497 -0.0014305167 -0.0013237533 -0.0011935767 -0.0010718014 -0.0010312172][-0.0013942518 -0.0013512374 -0.0013308399 -0.001405356 -0.0014760932 -0.0015246656 -0.0015111841 -0.0014719823 -0.0014213881 -0.0013516652 -0.001222844 -0.00097116944 -0.00064593588 -0.00037388422 -0.00032009254][-0.00095639587 -0.00080304523 -0.00073990226 -0.00083806633 -0.00097199011 -0.0010963462 -0.0011292538 -0.001139597 -0.0011101179 -0.0010029337 -0.00076672167 -0.0003291585 0.00021071581 0.00065801211 0.00073645392][-0.00026770367 0.00013324444 0.00033304247 0.00022827426 -2.000737e-05 -0.00024272094 -0.00033040112 -0.00037377398 -0.00035879482 -0.00023471448 7.01485e-05 0.000649487 0.0013602226 0.001945082 0.0020938152][0.00062623771 0.0013900775 0.0018364104 0.0018061468 0.0014703764 0.0011373198 0.00098394847 0.00093072152 0.00092510821 0.0010135645 0.0012962512 0.0018794826 0.0026138062 0.0032767998 0.0035388926][0.0016137665 0.0027519222 0.0034898478 0.0036041359 0.0032549589 0.0028721839 0.002678243 0.0026099966 0.0025384948 0.0024837605 0.0026142411 0.0030471217 0.0037085931 0.004390311 0.0047778306][0.00247173 0.0038627009 0.004819246 0.0051045711 0.0048388126 0.0045026815 0.0043197097 0.0042231432 0.0040269857 0.0037228866 0.003582743 0.0037580552 0.0042704046 0.0049496419 0.005473752][0.002872806 0.0043359627 0.0053884885 0.0058262334 0.0057136524 0.0055106743 0.0053813765 0.0052353297 0.0048710066 0.0042729373 0.0038218782 0.0037041674 0.004033403 0.0047049825 0.0053741476][0.0025669551 0.0039256876 0.0049589775 0.0055085216 0.00558096 0.0055476166 0.005490114 0.00529115 0.0047684521 0.0039259261 0.003218452 0.0028698929 0.0030452418 0.0036831265 0.0044321][0.0015976847 0.0027111885 0.0036276923 0.0042295586 0.0044583576 0.0045651672 0.0045633321 0.0043303897 0.0037299325 0.0028013731 0.0019934042 0.0015353587 0.001600162 0.0021655206 0.0028978181][0.00036034908 0.0011424917 0.0018530752 0.0024054721 0.0027053915 0.0028879247 0.0029166979 0.0026910957 0.0021329671 0.0013109896 0.0005919562 0.00017057837 0.00017452391 0.00060157466 0.0011987925][-0.00070177962 -0.00023921113 0.00022378622 0.00063045823 0.0008926458 0.0010655745 0.0010976439 0.00092238176 0.0005059432 -7.6277182e-05 -0.0005783079 -0.00087074575 -0.00088302768 -0.00060756423 -0.00020962558][-0.0013783589 -0.0011534065 -0.00090512465 -0.00066658936 -0.00049720251 -0.00038127752 -0.0003607471 -0.00047098415 -0.00071999908 -0.0010479174 -0.0013215886 -0.0014748843 -0.0014807497 -0.0013373474 -0.0011276009][-0.0017002734 -0.0016136682 -0.001508576 -0.0014003578 -0.001318526 -0.001261519 -0.0012532144 -0.001307924 -0.0014228101 -0.001562888 -0.0016746155 -0.0017327886 -0.0017341463 -0.0016778193 -0.0015933238]]...]
INFO - root - 2017-12-09 21:05:27.226239: step 58210, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.890 sec/batch; 67h:47m:21s remains)
INFO - root - 2017-12-09 21:05:35.636655: step 58220, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.865 sec/batch; 65h:55m:13s remains)
INFO - root - 2017-12-09 21:05:44.230426: step 58230, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.885 sec/batch; 67h:25m:21s remains)
INFO - root - 2017-12-09 21:05:52.911744: step 58240, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 66h:11m:17s remains)
INFO - root - 2017-12-09 21:06:01.262957: step 58250, loss = 0.82, batch loss = 0.69 (9.8 examples/sec; 0.820 sec/batch; 62h:26m:59s remains)
INFO - root - 2017-12-09 21:06:09.862516: step 58260, loss = 0.82, batch loss = 0.70 (9.5 examples/sec; 0.841 sec/batch; 64h:02m:51s remains)
INFO - root - 2017-12-09 21:06:18.355189: step 58270, loss = 0.82, batch loss = 0.69 (9.7 examples/sec; 0.823 sec/batch; 62h:41m:08s remains)
INFO - root - 2017-12-09 21:06:26.941136: step 58280, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.890 sec/batch; 67h:46m:12s remains)
INFO - root - 2017-12-09 21:06:35.493203: step 58290, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 66h:00m:48s remains)
INFO - root - 2017-12-09 21:06:43.971914: step 58300, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 66h:00m:59s remains)
2017-12-09 21:06:44.930721: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.0020929421 0.0022220039 0.00223728 0.0021821265 0.002091466 0.0019780779 0.001858991 0.001759888 0.0016450485 0.0014982576 0.0013036713 0.0010546054 0.00068137457 0.00035637722 7.523899e-05][0.0024939571 0.0026805694 0.0027358737 0.0026811445 0.0025640256 0.0024285009 0.0022684606 0.0021282556 0.0019701421 0.0017898475 0.001586411 0.0013464011 0.000994817 0.00066877517 0.00034802372][0.0027064988 0.0029186043 0.0029816045 0.0029319539 0.0028066766 0.0026690117 0.0025122818 0.00237347 0.0022064396 0.0020179297 0.0018011493 0.0015570157 0.0012507912 0.00093915325 0.00057686877][0.00278184 0.002983293 0.0030446639 0.0029949872 0.0028888946 0.00279625 0.0026812451 0.0025861403 0.0024700104 0.0023012836 0.002073145 0.0018025931 0.001502856 0.0011996176 0.00080504816][0.0027281004 0.0028939326 0.0029611527 0.0029464681 0.0028841109 0.0028442778 0.0028075329 0.0027678944 0.00271055 0.0025859773 0.0023554689 0.0020429185 0.0017396876 0.0014377766 0.0010217036][0.0024667606 0.0026259497 0.0027295882 0.0027814335 0.0027913987 0.0028207386 0.0028419765 0.0028495882 0.0028457241 0.0027637733 0.0025535882 0.0022313162 0.0019158729 0.001583598 0.001168792][0.0020186873 0.0021840911 0.0023295702 0.0024565114 0.002560555 0.0026704096 0.0027430705 0.0027838722 0.0027991687 0.0027799667 0.0026080464 0.0023115268 0.0019905283 0.0016413979 0.0012403509][0.0014960902 0.0016426671 0.0017998941 0.0019864193 0.0021753036 0.0023747138 0.002503688 0.0025744881 0.0026048748 0.0026422064 0.0025425493 0.0023046075 0.0019961861 0.0016364054 0.001233318][0.0010273323 0.0011225942 0.0012451386 0.0014432539 0.0016930363 0.0019645491 0.0021456364 0.0022709453 0.0023351158 0.0024172952 0.0023918692 0.0022174534 0.0019400214 0.0015794808 0.001177258][0.000675548 0.00072214112 0.000803538 0.00097876543 0.0012466573 0.0015391196 0.0017509843 0.0018991622 0.0020034751 0.0021177828 0.0021453635 0.0020352793 0.0017815136 0.0014388174 0.0010485266][0.00044971926 0.00047686754 0.00053659652 0.00067991728 0.00091415143 0.0011679485 0.0013654557 0.0015008476 0.0016053159 0.0017280664 0.0017925968 0.0017151072 0.0014971666 0.0011963899 0.00083208655][0.00032337161 0.0003694935 0.00043326186 0.00055299408 0.000729625 0.00090313714 0.0010317814 0.0011087774 0.0011825151 0.001305139 0.0013634217 0.0013099153 0.0011195111 0.00086633291 0.00053633854][0.00025585492 0.0003423841 0.00042208692 0.00050907594 0.00061676966 0.00071536389 0.0007753399 0.00078225264 0.00079548208 0.00088023569 0.00090876513 0.00085668883 0.0007008597 0.000498308 0.00022229564][0.00016360299 0.00027691538 0.000360801 0.00042811513 0.00049130607 0.00053005607 0.00054683106 0.00052291283 0.00049506559 0.00050723937 0.00050394994 0.00046258245 0.00033692305 0.00017447129 -4.3228385e-05][1.0451768e-06 0.000113099 0.00019037712 0.00023601425 0.00026403868 0.00026557723 0.00025923376 0.00023312552 0.00019402208 0.00016149378 0.00013476505 9.887584e-05 -1.2200559e-05 -0.00014174788 -0.00030240347]]...]
INFO - root - 2017-12-09 21:06:53.486239: step 58310, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.852 sec/batch; 64h:53m:59s remains)
INFO - root - 2017-12-09 21:07:02.072306: step 58320, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.897 sec/batch; 68h:19m:49s remains)
INFO - root - 2017-12-09 21:07:10.649400: step 58330, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.861 sec/batch; 65h:36m:08s remains)
INFO - root - 2017-12-09 21:07:19.121597: step 58340, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.889 sec/batch; 67h:43m:27s remains)
INFO - root - 2017-12-09 21:07:27.695779: step 58350, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.865 sec/batch; 65h:53m:50s remains)
INFO - root - 2017-12-09 21:07:36.348001: step 58360, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.842 sec/batch; 64h:07m:13s remains)
INFO - root - 2017-12-09 21:07:45.104966: step 58370, loss = 0.81, batch loss = 0.68 (8.8 examples/sec; 0.905 sec/batch; 68h:52m:32s remains)
INFO - root - 2017-12-09 21:07:53.798970: step 58380, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.854 sec/batch; 64h:59m:22s remains)
INFO - root - 2017-12-09 21:08:02.500178: step 58390, loss = 0.82, batch loss = 0.69 (8.7 examples/sec; 0.918 sec/batch; 69h:53m:41s remains)
INFO - root - 2017-12-09 21:08:11.193403: step 58400, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.886 sec/batch; 67h:28m:50s remains)
2017-12-09 21:08:12.044108: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.1908474 0.18803881 0.18484588 0.18122144 0.1770924 0.17297184 0.16896212 0.16583402 0.16257976 0.1600606 0.15711977 0.15456495 0.15191351 0.14863315 0.14603233][0.20556441 0.20378861 0.20165032 0.19856904 0.19454622 0.19100368 0.18792437 0.18567735 0.18353795 0.18211055 0.18057701 0.17856011 0.17558002 0.17193615 0.16850366][0.22088099 0.22118123 0.22098257 0.21927305 0.21587093 0.21290532 0.21024661 0.20892207 0.20804381 0.20796032 0.20725185 0.2053005 0.20167874 0.1965453 0.19106978][0.23615679 0.23928325 0.24102679 0.24042906 0.23851983 0.23621935 0.23419765 0.23352462 0.23378667 0.23457314 0.23423547 0.23218194 0.22752459 0.22063963 0.21281119][0.2490229 0.25448778 0.25768119 0.25896689 0.25846964 0.25664523 0.25524592 0.25506973 0.25624618 0.25753 0.25747287 0.25545588 0.24990368 0.24126479 0.23107591][0.258906 0.26680875 0.2714529 0.27380905 0.27417353 0.27331373 0.2727181 0.27241513 0.2735782 0.27486593 0.27480274 0.27184349 0.26493296 0.2549842 0.24306716][0.2654573 0.27521268 0.28037021 0.28328195 0.2839722 0.28353462 0.28305098 0.28269356 0.28343126 0.28448692 0.28405958 0.28060982 0.27291167 0.26178628 0.2490488][0.26879951 0.27949771 0.2844274 0.28712863 0.28753567 0.28673506 0.28587416 0.28465366 0.28457558 0.28474545 0.28391635 0.28016669 0.27267355 0.26157144 0.24891618][0.26929423 0.28039992 0.28485271 0.28669211 0.28618127 0.28469065 0.28315309 0.28127852 0.28015298 0.27926266 0.27761367 0.27367914 0.2665292 0.25616685 0.24425012][0.26710123 0.27745563 0.28063649 0.28130493 0.27983531 0.27741146 0.27498525 0.2725077 0.27048367 0.26889715 0.26659793 0.26241207 0.25600889 0.24674693 0.2362157][0.262308 0.27187234 0.27375162 0.27278489 0.27015817 0.267014 0.2639319 0.26092058 0.25827628 0.25616223 0.25373858 0.24927238 0.24328862 0.23523815 0.22628604][0.25169271 0.2610971 0.26285431 0.26149127 0.25849181 0.25464979 0.25103155 0.2478469 0.24486779 0.24242169 0.23987968 0.23589151 0.23078942 0.22379977 0.21628493][0.24095502 0.25003514 0.25190246 0.25070262 0.24793521 0.24379735 0.23966497 0.23590283 0.23209453 0.22878765 0.22552876 0.2213942 0.21683267 0.21126842 0.20550124][0.2291058 0.23798281 0.23979099 0.23882228 0.23652102 0.23320358 0.22955364 0.22579354 0.22154608 0.21750353 0.21355213 0.20921953 0.20512234 0.2010588 0.19721505][0.21740831 0.22575639 0.22719902 0.22721314 0.22606628 0.22374922 0.22070803 0.21709746 0.21256608 0.20769611 0.20308751 0.19883092 0.1953259 0.19218551 0.18992789]]...]
INFO - root - 2017-12-09 21:08:20.886060: step 58410, loss = 0.82, batch loss = 0.69 (8.7 examples/sec; 0.923 sec/batch; 70h:14m:34s remains)
INFO - root - 2017-12-09 21:08:29.459773: step 58420, loss = 0.83, batch loss = 0.70 (9.5 examples/sec; 0.842 sec/batch; 64h:07m:02s remains)
INFO - root - 2017-12-09 21:08:37.877931: step 58430, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.836 sec/batch; 63h:40m:55s remains)
INFO - root - 2017-12-09 21:08:46.630266: step 58440, loss = 0.82, batch loss = 0.69 (9.7 examples/sec; 0.824 sec/batch; 62h:43m:25s remains)
INFO - root - 2017-12-09 21:08:55.270479: step 58450, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 65h:45m:29s remains)
INFO - root - 2017-12-09 21:09:03.943920: step 58460, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.884 sec/batch; 67h:16m:41s remains)
INFO - root - 2017-12-09 21:09:12.759953: step 58470, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 66h:06m:14s remains)
INFO - root - 2017-12-09 21:09:21.464023: step 58480, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.904 sec/batch; 68h:46m:59s remains)
INFO - root - 2017-12-09 21:09:30.367078: step 58490, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.873 sec/batch; 66h:27m:12s remains)
INFO - root - 2017-12-09 21:09:39.090171: step 58500, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 66h:09m:00s remains)
2017-12-09 21:09:39.989275: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.032838546 0.034148481 0.0368622 0.039922558 0.044501003 0.049328066 0.054739818 0.058945216 0.061823018 0.0623712 0.0607467 0.0561873 0.048377804 0.038376119 0.02731117][0.036133241 0.038070444 0.043220114 0.049994532 0.059474926 0.069296978 0.0790385 0.086112037 0.089858755 0.089306064 0.083782189 0.073515616 0.059389714 0.043898903 0.028793747][0.045728192 0.047335688 0.054353673 0.06488 0.07947395 0.094802529 0.10925055 0.11890691 0.1228057 0.11979784 0.10919132 0.092143364 0.070919432 0.049582608 0.0302305][0.061971284 0.064682394 0.0741086 0.088012226 0.10670358 0.12653896 0.14439134 0.15528303 0.15817389 0.15180056 0.13556977 0.11097626 0.082467958 0.055367511 0.031911075][0.081763126 0.086856321 0.099319786 0.11681595 0.13869874 0.16028596 0.17859854 0.18808311 0.18825682 0.17782652 0.15653262 0.1265012 0.092578925 0.060952857 0.033961389][0.10263056 0.11039313 0.12601377 0.14700767 0.17156908 0.19417617 0.21134682 0.21784522 0.21318829 0.19761768 0.17133757 0.13702889 0.099623404 0.065132543 0.036138222][0.1247116 0.13566737 0.15365449 0.1768188 0.20239583 0.2243771 0.23926751 0.2421798 0.23339023 0.21321553 0.18283403 0.1451989 0.1053752 0.069066867 0.038681485][0.14862935 0.16249527 0.18164375 0.20486876 0.2288262 0.24796045 0.25912195 0.25848794 0.24636093 0.22354795 0.19150896 0.15286608 0.11197078 0.0741825 0.042328384][0.17072161 0.18638082 0.20526339 0.22700408 0.24835801 0.26429456 0.27216458 0.26898044 0.2551657 0.23145024 0.19914672 0.16059545 0.11962233 0.081120811 0.048172135][0.18750013 0.20417708 0.22243987 0.24244283 0.26131368 0.27476209 0.28038618 0.27603957 0.26196551 0.23900926 0.2079301 0.17038849 0.1295992 0.089997388 0.055343833][0.20165144 0.21793769 0.23476066 0.25289214 0.26995713 0.28220621 0.28714406 0.28302705 0.26995251 0.24840033 0.21864454 0.18209548 0.1413738 0.10071027 0.064020514][0.20999385 0.22597404 0.24135387 0.25786251 0.27342427 0.28480878 0.28984767 0.28719795 0.27647337 0.2571221 0.22930333 0.1937781 0.15284929 0.11080172 0.072079226][0.21537279 0.22990136 0.24321947 0.25756294 0.27169466 0.28312549 0.28976521 0.29000184 0.28251517 0.26594728 0.23995914 0.20507814 0.1636889 0.12020516 0.079436071][0.2190178 0.23190524 0.24267705 0.25434077 0.26621634 0.27673727 0.28486362 0.28833687 0.28478664 0.27164528 0.24836126 0.21454866 0.17212333 0.12646317 0.083427757][0.22134759 0.23321882 0.24193367 0.25132957 0.26119095 0.27054659 0.27867633 0.28348115 0.28215346 0.27211335 0.25167185 0.21957308 0.17787547 0.13194312 0.087588772]]...]
INFO - root - 2017-12-09 21:09:48.801125: step 58510, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 65h:35m:04s remains)
INFO - root - 2017-12-09 21:09:57.326711: step 58520, loss = 0.83, batch loss = 0.70 (9.2 examples/sec; 0.870 sec/batch; 66h:13m:26s remains)
INFO - root - 2017-12-09 21:10:05.928165: step 58530, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.883 sec/batch; 67h:11m:03s remains)
INFO - root - 2017-12-09 21:10:14.582338: step 58540, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 65h:46m:10s remains)
INFO - root - 2017-12-09 21:10:23.061788: step 58550, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.853 sec/batch; 64h:54m:01s remains)
INFO - root - 2017-12-09 21:10:31.837039: step 58560, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.892 sec/batch; 67h:53m:21s remains)
INFO - root - 2017-12-09 21:10:40.443050: step 58570, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.897 sec/batch; 68h:15m:27s remains)
INFO - root - 2017-12-09 21:10:49.199377: step 58580, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.880 sec/batch; 66h:59m:44s remains)
INFO - root - 2017-12-09 21:10:57.916194: step 58590, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.884 sec/batch; 67h:13m:23s remains)
INFO - root - 2017-12-09 21:11:06.773430: step 58600, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.890 sec/batch; 67h:40m:44s remains)
2017-12-09 21:11:07.753656: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0018326159 -0.0018314443 -0.0018272826 -0.0018169605 -0.0017954574 -0.0017609968 -0.0017284526 -0.0017224816 -0.0017528436 -0.0017901368 -0.001811785 -0.0018181184 -0.0018215443 -0.0018213158 -0.0018201824][-0.0018342119 -0.0018280686 -0.0018098466 -0.0017756172 -0.0017211717 -0.0016518941 -0.0015993036 -0.001604579 -0.0016794864 -0.0017557717 -0.0018014628 -0.0018151966 -0.0018209136 -0.0018208815 -0.001819614][-0.0018246728 -0.00178565 -0.0017087142 -0.0016060699 -0.0014901182 -0.0013695711 -0.0012935246 -0.001336019 -0.0015075282 -0.001677843 -0.0017826987 -0.001812959 -0.0018205248 -0.0018205032 -0.0018193262][-0.0017825974 -0.0016544784 -0.0014462878 -0.0012188219 -0.001010352 -0.00081832905 -0.00071815227 -0.00083825376 -0.0011810218 -0.0015266642 -0.0017452985 -0.0018114538 -0.0018207488 -0.0018208838 -0.0018196226][-0.0016765278 -0.0013897349 -0.00097700418 -0.00057511206 -0.00024347869 3.5700737e-05 0.00014760101 -0.00010416866 -0.00070322328 -0.0013120412 -0.001693623 -0.0018110526 -0.0018212816 -0.0018211517 -0.001819932][-0.001501061 -0.001015611 -0.00036610081 0.0002377684 0.00071434875 0.0010809208 0.0011709203 0.00073102734 -0.00017712335 -0.0010864125 -0.0016435762 -0.0018108959 -0.001821212 -0.0018209041 -0.001819814][-0.0012840868 -0.00060916203 0.00025843142 0.0010555981 0.0016690925 0.0021037408 0.0021311645 0.0014725957 0.00025577343 -0.000921711 -0.001613266 -0.0018116104 -0.0018206019 -0.001819891 -0.0018189789][-0.0010794703 -0.000268723 0.00075527385 0.0016995458 0.0024153572 0.0028861845 0.0028315983 0.0019783755 0.000523724 -0.00083868485 -0.0016068757 -0.0018137128 -0.0018206052 -0.0018198749 -0.0018188432][-0.00095487142 -8.9911395e-05 0.00098595687 0.0019838465 0.0027324045 0.0032063271 0.0031027095 0.0021594986 0.00060275395 -0.00083235465 -0.0016179119 -0.0018160392 -0.0018207709 -0.0018200442 -0.0018189761][-0.00095689169 -0.00012389082 0.00089946913 0.0018483853 0.0025465591 0.0029830127 0.0028779227 0.0019726427 0.00047983055 -0.00089995039 -0.0016412941 -0.0018179239 -0.0018205344 -0.0018199678 -0.001819063][-0.001089596 -0.00037424755 0.000498125 0.0013004692 0.0018781222 0.0022396292 0.0021695932 0.0014171376 0.00014929462 -0.0010412076 -0.0016742526 -0.001819285 -0.0018203473 -0.0018198704 -0.0018191094][-0.0012969815 -0.00075136637 -8.9236652e-05 0.000509245 0.000927803 0.0011887887 0.0011571759 0.00061047112 -0.00033305329 -0.00123713 -0.0017148873 -0.0018208005 -0.0018207331 -0.0018199518 -0.001819121][-0.0015089744 -0.0011489484 -0.00071028376 -0.0003238779 -6.3573476e-05 9.4615272e-05 8.8035595e-05 -0.00025453244 -0.00085494737 -0.0014441697 -0.0017540767 -0.0018215333 -0.0018208806 -0.0018200962 -0.0018192994][-0.0016719881 -0.0014706836 -0.001223685 -0.001010289 -0.00087186007 -0.00079336169 -0.00079334306 -0.00097733538 -0.0012997673 -0.0016214151 -0.001787942 -0.0018215453 -0.0018205784 -0.0018198433 -0.0018191016][-0.0017676143 -0.0016759284 -0.0015599647 -0.0014620412 -0.0014012103 -0.0013708328 -0.0013716695 -0.0014563476 -0.001598619 -0.0017396886 -0.0018090451 -0.001821149 -0.0018202488 -0.0018195634 -0.0018188654]]...]
INFO - root - 2017-12-09 21:11:16.415473: step 58610, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.880 sec/batch; 66h:58m:31s remains)
INFO - root - 2017-12-09 21:11:24.898283: step 58620, loss = 0.81, batch loss = 0.68 (9.1 examples/sec; 0.876 sec/batch; 66h:39m:19s remains)
INFO - root - 2017-12-09 21:11:33.345966: step 58630, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 66h:16m:22s remains)
INFO - root - 2017-12-09 21:11:41.918005: step 58640, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.829 sec/batch; 63h:04m:44s remains)
INFO - root - 2017-12-09 21:11:50.402115: step 58650, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.843 sec/batch; 64h:08m:45s remains)
INFO - root - 2017-12-09 21:11:58.903352: step 58660, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.866 sec/batch; 65h:52m:56s remains)
INFO - root - 2017-12-09 21:12:07.519655: step 58670, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.858 sec/batch; 65h:15m:16s remains)
INFO - root - 2017-12-09 21:12:16.002813: step 58680, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.847 sec/batch; 64h:23m:21s remains)
INFO - root - 2017-12-09 21:12:24.533906: step 58690, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.861 sec/batch; 65h:27m:42s remains)
INFO - root - 2017-12-09 21:12:33.237616: step 58700, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.900 sec/batch; 68h:27m:47s remains)
2017-12-09 21:12:34.081704: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.36304006 0.36724839 0.36986098 0.37355411 0.37844026 0.38781065 0.39913362 0.41283706 0.42465347 0.43232316 0.43470129 0.43300655 0.42814437 0.42252579 0.4182584][0.37255555 0.37768239 0.3788878 0.37989274 0.38241065 0.3890512 0.39943245 0.41316581 0.42514384 0.43359706 0.43606669 0.43377379 0.42805263 0.42123374 0.41604483][0.37676188 0.38151109 0.37947157 0.37491304 0.3717159 0.37354854 0.38210091 0.3951467 0.40696374 0.41586488 0.41956657 0.41913426 0.41468447 0.40869248 0.404646][0.37806508 0.38116813 0.37522048 0.36525509 0.35620874 0.35247159 0.35738114 0.36894351 0.38030952 0.39002174 0.3958874 0.39848152 0.39706555 0.3936691 0.39141515][0.37671706 0.37789205 0.36733019 0.35219693 0.33845803 0.3301518 0.33208862 0.34157225 0.35272837 0.36304489 0.3706691 0.37611568 0.37772933 0.37634209 0.37545571][0.37213603 0.3721123 0.35871676 0.34029585 0.32283106 0.3111921 0.31085703 0.3181476 0.32862186 0.3386946 0.34682578 0.35336453 0.35616875 0.35569254 0.3546094][0.36423081 0.36560854 0.35300356 0.3339498 0.3150878 0.3019523 0.30014583 0.30574098 0.31435373 0.32307407 0.3311311 0.33727095 0.33912691 0.33743125 0.33542642][0.35269502 0.3584114 0.34963202 0.33329034 0.31554294 0.3020452 0.29799277 0.3003841 0.30577007 0.31195572 0.31813374 0.32246396 0.32247844 0.31875429 0.31506827][0.34126607 0.35297838 0.35049483 0.34013948 0.32693276 0.31518075 0.30972096 0.30850536 0.30944771 0.31115627 0.31260478 0.31229514 0.30795664 0.30074531 0.29436424][0.32839888 0.3475793 0.35350722 0.3511439 0.34450069 0.33613765 0.33063236 0.32646468 0.32251835 0.31785226 0.31230634 0.30513108 0.29424843 0.28169796 0.27195132][0.30904403 0.33912781 0.35643661 0.36354303 0.36404818 0.3603152 0.35531247 0.34916538 0.34078923 0.3305009 0.31813571 0.30304587 0.28479767 0.26611337 0.25195041][0.2839126 0.32451963 0.35423261 0.37252551 0.38147926 0.38251826 0.37895173 0.37122756 0.35915834 0.34400302 0.32576409 0.30451393 0.28049189 0.25709966 0.23948027][0.25497022 0.30402389 0.34322983 0.37127137 0.38865197 0.39527026 0.39380798 0.38494736 0.37050006 0.35218179 0.32989052 0.30462405 0.277561 0.25191644 0.23231053][0.22847115 0.28128573 0.32658416 0.36093086 0.38378111 0.39529687 0.39698544 0.38952017 0.37512746 0.35586622 0.33202186 0.30518267 0.27704036 0.250684 0.23029323][0.20381665 0.25762969 0.30529165 0.34256765 0.36818218 0.38261086 0.38675216 0.3820146 0.36973813 0.35190505 0.32923111 0.30296767 0.27536562 0.24950561 0.22921403]]...]
INFO - root - 2017-12-09 21:12:42.887213: step 58710, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 65h:57m:17s remains)
INFO - root - 2017-12-09 21:12:51.445061: step 58720, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.885 sec/batch; 67h:20m:12s remains)
INFO - root - 2017-12-09 21:13:00.153040: step 58730, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.875 sec/batch; 66h:32m:32s remains)
INFO - root - 2017-12-09 21:13:08.792915: step 58740, loss = 0.82, batch loss = 0.69 (8.8 examples/sec; 0.904 sec/batch; 68h:44m:48s remains)
INFO - root - 2017-12-09 21:13:17.153881: step 58750, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.865 sec/batch; 65h:44m:21s remains)
INFO - root - 2017-12-09 21:13:25.672123: step 58760, loss = 0.82, batch loss = 0.69 (9.8 examples/sec; 0.819 sec/batch; 62h:16m:10s remains)
INFO - root - 2017-12-09 21:13:34.188813: step 58770, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.872 sec/batch; 66h:18m:10s remains)
INFO - root - 2017-12-09 21:13:42.850463: step 58780, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.887 sec/batch; 67h:27m:26s remains)
INFO - root - 2017-12-09 21:13:51.439579: step 58790, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 65h:58m:46s remains)
INFO - root - 2017-12-09 21:14:00.082479: step 58800, loss = 0.83, batch loss = 0.70 (9.3 examples/sec; 0.858 sec/batch; 65h:12m:57s remains)
2017-12-09 21:14:00.953707: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0018395962 -0.0018390124 -0.0018389601 -0.0018389145 -0.0018388315 -0.0018387173 -0.0018385332 -0.0018383126 -0.0018380592 -0.0018378206 -0.0018377558 -0.0018377426 -0.0018376362 -0.0018369032 -0.0018356969][-0.00183952 -0.0018389225 -0.0018388167 -0.0018387494 -0.0018386417 -0.0018385284 -0.00183835 -0.0018380939 -0.0018377539 -0.001837432 -0.0018372803 -0.0018371866 -0.001837011 -0.0018362458 -0.0018350325][-0.001837578 -0.0018370299 -0.0018369188 -0.0018369429 -0.0018369588 -0.0018369989 -0.0018369656 -0.0018368381 -0.0018366043 -0.0018363406 -0.0018361046 -0.0018358892 -0.0018355907 -0.0018348319 -0.0018337584][-0.0018341625 -0.0018335563 -0.0018333909 -0.00183343 -0.0018335326 -0.0018337523 -0.0018339567 -0.0018341027 -0.0018341439 -0.0018340988 -0.0018339616 -0.001833743 -0.0018333161 -0.0018325774 -0.0018316512][-0.0018305564 -0.0018298452 -0.0018296062 -0.0018296202 -0.0018297783 -0.001830147 -0.0018305724 -0.0018309739 -0.0018312997 -0.0018315312 -0.0018315183 -0.001831228 -0.0018305997 -0.0018298073 -0.0018289831][-0.0018272161 -0.0018264005 -0.0018260732 -0.0018259746 -0.0018260797 -0.0018265044 -0.0018270697 -0.0018276658 -0.0018282346 -0.0018287067 -0.0018288462 -0.0018285256 -0.001827843 -0.0018270881 -0.0018264138][-0.0018243956 -0.0018234415 -0.0018229657 -0.0018227118 -0.0018227164 -0.0018230695 -0.0018236021 -0.0018242588 -0.0018249992 -0.0018257154 -0.0018260351 -0.0018258094 -0.0018252505 -0.0018246687 -0.0018242088][-0.0018222805 -0.0018212226 -0.0018206543 -0.0018202361 -0.001820086 -0.0018202863 -0.0018206527 -0.0018212034 -0.0018219723 -0.0018228056 -0.0018232565 -0.0018231543 -0.0018227954 -0.0018224436 -0.0018222459][-0.0018211225 -0.0018200096 -0.001819453 -0.0018189512 -0.001818684 -0.0018187786 -0.0018190349 -0.0018194766 -0.0018200903 -0.001820773 -0.0018212233 -0.0018212253 -0.0018210381 -0.0018209126 -0.0018209375][-0.0018207533 -0.0018197158 -0.0018192271 -0.001818768 -0.0018184885 -0.0018185287 -0.0018187308 -0.0018191092 -0.00181956 -0.0018199965 -0.0018202658 -0.0018202601 -0.0018201788 -0.0018201951 -0.0018203529][-0.0018207786 -0.0018198093 -0.0018193695 -0.0018189759 -0.0018187071 -0.0018187008 -0.0018188477 -0.0018191355 -0.001819453 -0.0018196962 -0.0018198416 -0.0018198553 -0.0018198597 -0.0018199726 -0.0018201713][-0.0018209063 -0.0018200515 -0.0018196768 -0.0018193501 -0.0018191094 -0.0018190499 -0.0018191191 -0.0018192745 -0.0018194544 -0.0018195629 -0.0018196203 -0.0018196596 -0.0018197265 -0.0018198993 -0.0018201184][-0.0018211445 -0.0018203947 -0.001820081 -0.0018198692 -0.0018196794 -0.0018195788 -0.001819576 -0.0018196243 -0.00181972 -0.0018197609 -0.001819782 -0.0018198256 -0.0018198922 -0.0018200555 -0.0018202439][-0.0018213207 -0.0018206704 -0.0018204494 -0.0018203725 -0.0018202784 -0.0018202101 -0.0018201877 -0.0018201653 -0.0018201866 -0.0018202015 -0.0018202151 -0.0018202475 -0.0018202794 -0.0018203839 -0.001820494][-0.0018215276 -0.001820886 -0.001820689 -0.0018206998 -0.0018206929 -0.001820673 -0.001820656 -0.0018206199 -0.0018206119 -0.0018206091 -0.0018206143 -0.0018206205 -0.0018206028 -0.0018206364 -0.0018206792]]...]
INFO - root - 2017-12-09 21:14:09.816019: step 58810, loss = 0.82, batch loss = 0.69 (8.0 examples/sec; 0.999 sec/batch; 75h:58m:00s remains)
INFO - root - 2017-12-09 21:14:18.503986: step 58820, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.836 sec/batch; 63h:32m:58s remains)
INFO - root - 2017-12-09 21:14:26.994726: step 58830, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.873 sec/batch; 66h:20m:47s remains)
INFO - root - 2017-12-09 21:14:35.670021: step 58840, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.888 sec/batch; 67h:30m:45s remains)
INFO - root - 2017-12-09 21:14:44.149309: step 58850, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.893 sec/batch; 67h:52m:47s remains)
INFO - root - 2017-12-09 21:14:52.838368: step 58860, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.850 sec/batch; 64h:36m:44s remains)
INFO - root - 2017-12-09 21:15:01.597998: step 58870, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.903 sec/batch; 68h:36m:08s remains)
INFO - root - 2017-12-09 21:15:10.353513: step 58880, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.873 sec/batch; 66h:20m:22s remains)
INFO - root - 2017-12-09 21:15:19.027668: step 58890, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.849 sec/batch; 64h:29m:22s remains)
INFO - root - 2017-12-09 21:15:27.735887: step 58900, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.876 sec/batch; 66h:33m:03s remains)
2017-12-09 21:15:28.695840: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.013232054 0.01383941 0.015064909 0.016660277 0.018433562 0.020266676 0.022495927 0.025843741 0.030481335 0.036495477 0.042594675 0.047597345 0.050272144 0.050421428 0.048408661][0.020834759 0.021454401 0.022672925 0.024231911 0.026079355 0.028006973 0.030256648 0.033343211 0.037336595 0.042290483 0.047094982 0.050556894 0.051599294 0.050198261 0.046899378][0.029841125 0.030490343 0.03157229 0.032799132 0.034355219 0.035976537 0.037860606 0.040200733 0.043092448 0.046626855 0.049897563 0.0517197 0.051194109 0.048439864 0.044149294][0.038118526 0.038698521 0.039444491 0.04014197 0.041154757 0.042195842 0.043451916 0.044800311 0.046387393 0.04834773 0.050008267 0.050260913 0.048297726 0.04436003 0.039430123][0.044204332 0.044464115 0.044627588 0.04462849 0.044995304 0.045368392 0.045955036 0.046323314 0.046672106 0.047117412 0.04721735 0.045960672 0.042664018 0.037757736 0.032466255][0.046788905 0.046559859 0.046118375 0.045565251 0.045523979 0.045569859 0.045857888 0.045653012 0.045153324 0.044305857 0.042883456 0.040143 0.035593398 0.029922508 0.024547102][0.045021228 0.044589505 0.043918259 0.043251425 0.043287378 0.043546326 0.044056941 0.04375897 0.042774931 0.040962871 0.038267791 0.034170728 0.028560167 0.022363944 0.017076328][0.039097887 0.038970415 0.03873169 0.0386201 0.039233111 0.040079616 0.041149531 0.041227475 0.04028165 0.037996195 0.034370352 0.029161021 0.022718927 0.01618528 0.011047826][0.030660592 0.031005602 0.031439107 0.032132667 0.033494342 0.035064019 0.036767107 0.037424639 0.036846552 0.034582637 0.030596331 0.024875443 0.018104173 0.011617674 0.00677227][0.021243665 0.022123925 0.023201486 0.024505908 0.026315656 0.028300781 0.0303666 0.031480595 0.031359885 0.029480845 0.025743704 0.020282745 0.013962382 0.0081250435 0.0038957871][0.012562136 0.013677048 0.015022133 0.01655676 0.018450418 0.020485779 0.022558363 0.023868265 0.02409796 0.022734273 0.019657707 0.015078451 0.0098677445 0.0051967711 0.0019001992][0.0059992052 0.0070369155 0.0082477536 0.0095694289 0.011113082 0.012775052 0.014488858 0.015699789 0.016116759 0.015306463 0.013130255 0.0097880028 0.0060166861 0.002720939 0.00045137794][0.001783152 0.0025447966 0.0034157094 0.0043364931 0.0053713387 0.0064881924 0.0076548294 0.0085485578 0.0089504132 0.0085386634 0.0071979337 0.0050964304 0.0027478118 0.00075019628 -0.00058593159][-0.00049018115 -7.0275855e-05 0.00040793826 0.00090853765 0.0014734789 0.0021029329 0.0027766563 0.0033314936 0.0036299373 0.0034698946 0.0027751285 0.0016623329 0.00043109839 -0.00058372237 -0.001237606][-0.0014805662 -0.0013079052 -0.0011112359 -0.00090953015 -0.00067368557 -0.00039437553 -7.5774384e-05 0.00021192629 0.00039006968 0.00034909986 5.8539095e-05 -0.00041524903 -0.00092693965 -0.0013326074 -0.0015834029]]...]
INFO - root - 2017-12-09 21:15:37.364199: step 58910, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.891 sec/batch; 67h:43m:00s remains)
INFO - root - 2017-12-09 21:15:45.930135: step 58920, loss = 0.83, batch loss = 0.70 (9.2 examples/sec; 0.873 sec/batch; 66h:22m:19s remains)
INFO - root - 2017-12-09 21:15:54.566578: step 58930, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.877 sec/batch; 66h:39m:19s remains)
INFO - root - 2017-12-09 21:16:03.317204: step 58940, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.884 sec/batch; 67h:09m:20s remains)
INFO - root - 2017-12-09 21:16:11.820552: step 58950, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.840 sec/batch; 63h:50m:55s remains)
INFO - root - 2017-12-09 21:16:20.292595: step 58960, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.877 sec/batch; 66h:36m:35s remains)
INFO - root - 2017-12-09 21:16:28.904341: step 58970, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 66h:03m:01s remains)
INFO - root - 2017-12-09 21:16:37.563884: step 58980, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.863 sec/batch; 65h:33m:27s remains)
INFO - root - 2017-12-09 21:16:46.328551: step 58990, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.899 sec/batch; 68h:18m:17s remains)
INFO - root - 2017-12-09 21:16:55.015458: step 59000, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.850 sec/batch; 64h:36m:06s remains)
2017-12-09 21:16:55.898791: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.06712281 0.0739815 0.077817716 0.079053566 0.078766912 0.077684075 0.075892687 0.073118977 0.069766894 0.066180654 0.062574178 0.058610171 0.053841271 0.048119567 0.04147014][0.079149649 0.087922826 0.092665449 0.093930729 0.093066335 0.090945214 0.087779395 0.083426774 0.078394853 0.073220834 0.068186514 0.063091628 0.057518937 0.05129404 0.04429305][0.088707976 0.0992467 0.10488205 0.10606846 0.10444298 0.10090297 0.095921792 0.089552239 0.082464546 0.075363658 0.068690747 0.062530689 0.056454837 0.050233532 0.043518063][0.09441071 0.10637227 0.1128858 0.11418062 0.11183004 0.10684991 0.099961281 0.091478914 0.082354315 0.073508732 0.065452 0.058463417 0.052193951 0.046391848 0.0403947][0.096444845 0.10955318 0.11676703 0.11812718 0.11519257 0.10904051 0.10068093 0.090743855 0.0802711 0.070290916 0.061418109 0.054074787 0.047932509 0.042645678 0.03736499][0.094709374 0.10861399 0.11648037 0.11798631 0.11459782 0.10766404 0.098393783 0.087661728 0.076607116 0.066293769 0.057266586 0.050015122 0.044261552 0.039611258 0.035065789][0.089889072 0.10427215 0.11269713 0.11456913 0.11121602 0.10398512 0.094371647 0.083392672 0.072276309 0.062127292 0.053450558 0.0467072 0.041592404 0.037695814 0.033892054][0.082676187 0.097124226 0.10590795 0.10820141 0.10513055 0.097992495 0.088451482 0.07771492 0.067122228 0.05765494 0.049798764 0.043985892 0.039793305 0.036726113 0.03360961][0.074241608 0.088375032 0.0974119 0.10032499 0.097972296 0.091588669 0.082899712 0.073136553 0.06367287 0.055404607 0.048706807 0.043828849 0.040311795 0.037651498 0.034702126][0.064988419 0.078110658 0.086857654 0.090273075 0.088926777 0.08390253 0.076833546 0.068842925 0.061149478 0.054447565 0.049076676 0.045177929 0.042237964 0.039727461 0.036575023][0.05531377 0.066983752 0.074997351 0.078546695 0.0780492 0.074420571 0.069093131 0.063030712 0.057274211 0.052316312 0.048377536 0.045481488 0.043064859 0.040588852 0.037142985][0.045187432 0.054797437 0.061505549 0.06469477 0.064628839 0.062042743 0.058118027 0.053677082 0.049574535 0.046218332 0.04375435 0.042065393 0.040456906 0.038320057 0.034891445][0.035472006 0.04278883 0.047861014 0.050265718 0.050227534 0.048289675 0.045364875 0.042141568 0.039330363 0.037282992 0.036051232 0.035399958 0.034580238 0.032947604 0.029890344][0.026159933 0.031277649 0.034780681 0.036419697 0.036350641 0.034992687 0.032991365 0.030856602 0.029146541 0.02813082 0.027810281 0.027861098 0.02759506 0.02641164 0.02388373][0.017900908 0.021161631 0.023304624 0.024235617 0.024075566 0.023131955 0.021871991 0.020677535 0.019906316 0.019703321 0.020011285 0.020469693 0.020490179 0.019631509 0.017674822]]...]
INFO - root - 2017-12-09 21:17:04.421420: step 59010, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.830 sec/batch; 63h:01m:36s remains)
INFO - root - 2017-12-09 21:17:13.093984: step 59020, loss = 0.82, batch loss = 0.69 (10.5 examples/sec; 0.764 sec/batch; 58h:03m:54s remains)
INFO - root - 2017-12-09 21:17:21.552475: step 59030, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.878 sec/batch; 66h:42m:06s remains)
INFO - root - 2017-12-09 21:17:30.060821: step 59040, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.847 sec/batch; 64h:20m:58s remains)
INFO - root - 2017-12-09 21:17:38.456937: step 59050, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.878 sec/batch; 66h:39m:18s remains)
INFO - root - 2017-12-09 21:17:46.949620: step 59060, loss = 0.82, batch loss = 0.69 (9.7 examples/sec; 0.826 sec/batch; 62h:42m:17s remains)
INFO - root - 2017-12-09 21:17:55.643943: step 59070, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.854 sec/batch; 64h:53m:26s remains)
INFO - root - 2017-12-09 21:18:04.210146: step 59080, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.851 sec/batch; 64h:36m:00s remains)
INFO - root - 2017-12-09 21:18:12.769151: step 59090, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.880 sec/batch; 66h:49m:42s remains)
INFO - root - 2017-12-09 21:18:21.372068: step 59100, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.852 sec/batch; 64h:40m:30s remains)
2017-12-09 21:18:22.288291: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.00067530188 0.00070026692 0.0006926473 0.00068399694 0.000682589 0.00069352041 0.00071931782 0.0007343645 0.00075202819 0.0007713883 0.00077087 0.00074287748 0.00070833124 0.00065938209 0.00061776524][0.00042551255 0.00045309763 0.00045030063 0.00044526358 0.00044588244 0.00045747415 0.00048532279 0.00050900166 0.00053682749 0.00056395016 0.00058663741 0.00060508621 0.000600058 0.00056437904 0.00052603323][-0.00024222967 -0.0002196416 -0.00021729921 -0.00021624693 -0.0002082698 -0.00019054743 -0.00015909888 -0.00012749457 -9.5167081e-05 -6.6162786e-05 -4.2792759e-05 -2.5431043e-05 -1.5454716e-05 -2.4983892e-05 -4.4418266e-05][-0.00091513974 -0.00089981116 -0.00089511718 -0.00088961661 -0.00087566196 -0.00085282011 -0.00081996445 -0.00078467012 -0.00075352227 -0.00072892057 -0.00071186211 -0.0007002179 -0.00069408875 -0.000693421 -0.000712301][-0.0014521496 -0.0014449138 -0.0014410418 -0.0014349706 -0.0014218201 -0.0014026673 -0.001378503 -0.0013552897 -0.0013372847 -0.0013261305 -0.0013205181 -0.0013176769 -0.0013168234 -0.001317526 -0.0013198934][-0.0017501425 -0.0017476411 -0.0017469818 -0.0017450547 -0.0017394236 -0.0017306895 -0.0017194503 -0.0017089777 -0.001701397 -0.0016973239 -0.0016958333 -0.00169563 -0.0016960145 -0.001696659 -0.0016977645][-0.0018292246 -0.0018283173 -0.0018283573 -0.0018280443 -0.0018273726 -0.0018266244 -0.001825665 -0.001824721 -0.0018242555 -0.0018244062 -0.0018249488 -0.0018257316 -0.001826452 -0.0018269265 -0.0018270081][-0.0018284683 -0.0018274293 -0.0018276713 -0.0018274472 -0.0018270466 -0.0018268928 -0.0018267022 -0.0018260593 -0.0018257187 -0.0018255942 -0.0018256847 -0.0018259654 -0.001826385 -0.0018267905 -0.0018268712][-0.0018275998 -0.0018265249 -0.001826726 -0.0018265968 -0.0018264186 -0.0018264342 -0.0018263399 -0.0018256311 -0.0018252005 -0.0018249559 -0.001825068 -0.0018254002 -0.001825896 -0.0018264435 -0.0018266354][-0.0018266871 -0.0018256354 -0.0018259387 -0.0018261055 -0.0018261961 -0.0018262636 -0.001826123 -0.0018255831 -0.0018253549 -0.0018252769 -0.0018254447 -0.0018258146 -0.0018262053 -0.0018266347 -0.0018267384][-0.001826034 -0.0018250466 -0.0018253733 -0.0018256495 -0.0018256244 -0.0018254222 -0.0018252088 -0.0018247247 -0.0018244002 -0.001824165 -0.0018242501 -0.0018246394 -0.0018250667 -0.0018253599 -0.0018253835][-0.0018260247 -0.0018255082 -0.0018260182 -0.0018262693 -0.0018260655 -0.0018256149 -0.0018252805 -0.001824825 -0.0018246413 -0.0018245055 -0.001824633 -0.0018247019 -0.0018247186 -0.0018246535 -0.0018245362][-0.0018260865 -0.0018258474 -0.0018260903 -0.0018260269 -0.0018259316 -0.0018257502 -0.0018255028 -0.0018251054 -0.0018247564 -0.0018245345 -0.0018244389 -0.0018242829 -0.0018241449 -0.0018239743 -0.0018238476][-0.0018271504 -0.0018271079 -0.00182742 -0.0018274422 -0.0018273575 -0.0018270148 -0.0018266088 -0.0018262196 -0.0018258537 -0.0018256407 -0.0018255704 -0.0018255202 -0.0018253758 -0.0018251974 -0.0018250931][-0.0018282741 -0.0018280994 -0.0018281892 -0.001828 -0.0018276825 -0.0018272693 -0.0018269176 -0.0018266586 -0.0018263481 -0.0018262644 -0.0018262522 -0.0018262254 -0.0018261584 -0.001826191 -0.0018262721]]...]
INFO - root - 2017-12-09 21:18:30.886476: step 59110, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 65h:35m:06s remains)
INFO - root - 2017-12-09 21:18:39.582739: step 59120, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 65h:51m:29s remains)
INFO - root - 2017-12-09 21:18:47.932783: step 59130, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.856 sec/batch; 64h:58m:09s remains)
INFO - root - 2017-12-09 21:18:56.634861: step 59140, loss = 0.82, batch loss = 0.69 (8.7 examples/sec; 0.917 sec/batch; 69h:35m:54s remains)
INFO - root - 2017-12-09 21:19:05.200624: step 59150, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.878 sec/batch; 66h:41m:02s remains)
INFO - root - 2017-12-09 21:19:13.988045: step 59160, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.899 sec/batch; 68h:13m:37s remains)
INFO - root - 2017-12-09 21:19:22.732986: step 59170, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.863 sec/batch; 65h:30m:21s remains)
INFO - root - 2017-12-09 21:19:31.433104: step 59180, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 65h:25m:17s remains)
INFO - root - 2017-12-09 21:19:40.184913: step 59190, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.848 sec/batch; 64h:20m:50s remains)
INFO - root - 2017-12-09 21:19:48.956624: step 59200, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.872 sec/batch; 66h:11m:34s remains)
2017-12-09 21:19:49.893700: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0018047971 -0.0018034241 -0.0018031488 -0.0018029785 -0.0018027116 -0.0018023936 -0.0018020729 -0.001801957 -0.0018021093 -0.0018023817 -0.0018026524 -0.0018028611 -0.0018030173 -0.0018031187 -0.0018031624][-0.0018041102 -0.0018027879 -0.0018025407 -0.0018023216 -0.0018019478 -0.0018014696 -0.0018009495 -0.0018006929 -0.0018008221 -0.0018011305 -0.0018014712 -0.0018017526 -0.001801939 -0.0018020486 -0.0018021771][-0.0018042012 -0.0018030521 -0.0018028639 -0.0018026173 -0.0018021647 -0.001801528 -0.0018008342 -0.0018004394 -0.0018005518 -0.0018009244 -0.0018013458 -0.0018016823 -0.0018018505 -0.0018019078 -0.0018020192][-0.0018042907 -0.0018033144 -0.0018032014 -0.0018029372 -0.0018024088 -0.0018016478 -0.001800816 -0.0018003244 -0.0018004387 -0.0018009245 -0.0018014609 -0.001801836 -0.0018019695 -0.0018019709 -0.0018021031][-0.0018044407 -0.0018035341 -0.0018034917 -0.0018032022 -0.0018026008 -0.0018017316 -0.001800813 -0.0018002804 -0.0018004434 -0.0018010431 -0.0018016492 -0.0018020103 -0.0018020584 -0.0018020141 -0.0018021837][-0.0018045297 -0.0018036423 -0.0018036548 -0.0018033646 -0.0018027493 -0.0018018616 -0.0018009713 -0.001800489 -0.0018007326 -0.0018014534 -0.0018020654 -0.0018023462 -0.0018022904 -0.0018022034 -0.0018023654][-0.001804537 -0.0018036454 -0.0018037162 -0.0018034793 -0.0018029532 -0.0018021918 -0.0018014892 -0.0018011739 -0.0018015426 -0.0018023715 -0.0018029511 -0.0018031134 -0.0018029618 -0.001802841 -0.001802947][-0.0018045073 -0.0018035338 -0.0018036227 -0.0018034304 -0.001803005 -0.0018024294 -0.0018019581 -0.0018018412 -0.0018022951 -0.0018031338 -0.0018036286 -0.0018037041 -0.0018035129 -0.0018033896 -0.0018034328][-0.0018043341 -0.0018033012 -0.001803418 -0.001803244 -0.0018028887 -0.0018024846 -0.001802195 -0.0018022282 -0.0018026766 -0.0018034214 -0.0018037882 -0.0018037796 -0.001803586 -0.0018034813 -0.0018034977][-0.0018040368 -0.0018030447 -0.0018031596 -0.0018030111 -0.0018027161 -0.0018024154 -0.0018022244 -0.0018023219 -0.0018027019 -0.0018032664 -0.0018034705 -0.0018033984 -0.0018032152 -0.001803145 -0.0018031645][-0.0018039094 -0.0018029079 -0.0018029438 -0.0018028255 -0.0018025849 -0.0018023493 -0.0018021978 -0.0018022776 -0.0018025421 -0.0018028992 -0.0018029332 -0.0018028048 -0.0018026546 -0.0018026255 -0.0018026679][-0.00180393 -0.0018028085 -0.0018027755 -0.0018027021 -0.0018025219 -0.0018023757 -0.0018022662 -0.0018023155 -0.0018024412 -0.0018025873 -0.001802492 -0.001802325 -0.0018022024 -0.0018022005 -0.0018022587][-0.0018039481 -0.0018027673 -0.0018026513 -0.0018026201 -0.0018025032 -0.0018024065 -0.0018022873 -0.0018022656 -0.0018022429 -0.0018021922 -0.001802003 -0.0018018153 -0.0018016959 -0.0018016679 -0.0018016754][-0.0018040551 -0.00180275 -0.0018025613 -0.0018025835 -0.0018025372 -0.0018024696 -0.0018023505 -0.0018022691 -0.0018021396 -0.0018019646 -0.0018017322 -0.0018015321 -0.0018014099 -0.0018013447 -0.0018013038][-0.0018041538 -0.0018027583 -0.001802478 -0.0018025453 -0.0018025561 -0.0018025105 -0.0018024022 -0.001802279 -0.0018020923 -0.0018018747 -0.001801651 -0.0018014718 -0.0018013574 -0.0018012836 -0.0018012335]]...]
INFO - root - 2017-12-09 21:19:58.543047: step 59210, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.860 sec/batch; 65h:16m:33s remains)
INFO - root - 2017-12-09 21:20:07.238613: step 59220, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 65h:27m:50s remains)
INFO - root - 2017-12-09 21:20:15.682518: step 59230, loss = 0.82, batch loss = 0.69 (8.5 examples/sec; 0.944 sec/batch; 71h:37m:57s remains)
INFO - root - 2017-12-09 21:20:24.434802: step 59240, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.853 sec/batch; 64h:42m:49s remains)
INFO - root - 2017-12-09 21:20:32.947801: step 59250, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.879 sec/batch; 66h:41m:30s remains)
INFO - root - 2017-12-09 21:20:41.541312: step 59260, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 65h:26m:51s remains)
INFO - root - 2017-12-09 21:20:50.127454: step 59270, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.861 sec/batch; 65h:20m:26s remains)
INFO - root - 2017-12-09 21:20:58.776890: step 59280, loss = 0.82, batch loss = 0.69 (9.7 examples/sec; 0.829 sec/batch; 62h:54m:04s remains)
INFO - root - 2017-12-09 21:21:07.410061: step 59290, loss = 0.81, batch loss = 0.68 (9.4 examples/sec; 0.850 sec/batch; 64h:28m:28s remains)
INFO - root - 2017-12-09 21:21:16.034009: step 59300, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.874 sec/batch; 66h:20m:27s remains)
2017-12-09 21:21:16.957828: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.001807318 -0.0017373607 -0.0015046011 -0.00094946037 -8.2881073e-05 0.00078950159 0.0012482886 0.001232758 0.00090358581 0.00037872524 -0.00026404671 -0.00087605824 -0.0013482093 -0.0016365651 -0.0017655935][-0.0017759542 -0.0016247367 -0.001171286 -0.0002320424 0.0011126417 0.0024495693 0.0032448936 0.0033519729 0.0029476257 0.0021660621 0.0011261272 4.9413531e-05 -0.00083588483 -0.0014110198 -0.0016915078][-0.0016269756 -0.0012348939 -0.00031357224 0.0013303774 0.0035179467 0.0056732576 0.0071390313 0.0075840973 0.0071543097 0.0059316214 0.0041123694 0.0020873016 0.00032739819 -0.00087690353 -0.00150608][-0.001101558 -0.0001509547 0.0016442278 0.0044438909 0.0079387836 0.011317016 0.013779071 0.01473054 0.014208752 0.012230463 0.0091004753 0.0055074785 0.0023058648 5.9531769e-05 -0.0011647521][0.00010042067 0.0020388071 0.0051408717 0.0094683617 0.014604873 0.01946808 0.023040872 0.02445069 0.023555174 0.020382619 0.015435106 0.0097924089 0.0047615552 0.0012252043 -0.00073118135][0.0019928124 0.0052755037 0.0099440757 0.015897639 0.02266133 0.028942287 0.033429839 0.035001453 0.033337068 0.028606474 0.021592189 0.013795762 0.0069544245 0.0022222968 -0.0003724253][0.0041894363 0.0088267038 0.014908046 0.022136832 0.030015334 0.037161216 0.0419433 0.043082468 0.040220827 0.033873178 0.025180019 0.015898339 0.007964815 0.0026144874 -0.00025430461][0.0059407735 0.011481883 0.018368119 0.026144398 0.034303866 0.041455392 0.045726031 0.045829121 0.041615184 0.034064487 0.024651174 0.015131603 0.0073079024 0.0022088811 -0.00043547514][0.0065223626 0.01219727 0.019023694 0.026464198 0.033987481 0.040255442 0.043349285 0.04218109 0.03696283 0.029087339 0.020191366 0.011795691 0.005267886 0.0012016596 -0.00081686373][0.0056593488 0.01065593 0.016571745 0.022880878 0.029043693 0.0338323 0.035491046 0.033301745 0.027866242 0.020785952 0.013548394 0.0072485148 0.0026936056 2.1045911e-05 -0.0012315452][0.0037840102 0.0075364434 0.011961529 0.0166406 0.021082342 0.024260258 0.024731951 0.022190314 0.017446531 0.012029054 0.0070435493 0.0030985302 0.00049146486 -0.00092591927 -0.0015432527][0.0016763246 0.0040316042 0.0068106563 0.0097599281 0.012509732 0.014319134 0.01415339 0.011977268 0.0085645448 0.00510262 0.002265627 0.00027340779 -0.00090118148 -0.0014802039 -0.0017107654][-7.3879375e-05 0.0011435036 0.0025775209 0.0041143908 0.0055359243 0.0064032278 0.0060958522 0.004683028 0.0027154558 0.00092195848 -0.000374311 -0.0011560603 -0.001550218 -0.0017191591 -0.0017779688][-0.0011777496 -0.0006875965 -0.00010307995 0.0005421947 0.0011371392 0.0014732975 0.0012659704 0.00058697851 -0.00027424388 -0.00098547782 -0.0014362151 -0.0016615745 -0.0017534535 -0.0017863351 -0.0017961978][-0.0016783074 -0.0015406942 -0.0013624278 -0.0011507547 -0.00095550204 -0.00085467147 -0.00093560881 -0.0011606712 -0.0014237533 -0.0016230426 -0.0017340882 -0.0017800835 -0.0017947581 -0.0017993479 -0.0018001224]]...]
INFO - root - 2017-12-09 21:21:25.440376: step 59310, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.882 sec/batch; 66h:57m:25s remains)
INFO - root - 2017-12-09 21:21:34.116589: step 59320, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.855 sec/batch; 64h:52m:58s remains)
INFO - root - 2017-12-09 21:21:42.498104: step 59330, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.894 sec/batch; 67h:50m:27s remains)
INFO - root - 2017-12-09 21:21:51.140794: step 59340, loss = 0.82, batch loss = 0.69 (9.7 examples/sec; 0.824 sec/batch; 62h:31m:45s remains)
INFO - root - 2017-12-09 21:21:59.654838: step 59350, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.891 sec/batch; 67h:34m:44s remains)
INFO - root - 2017-12-09 21:22:08.250678: step 59360, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.873 sec/batch; 66h:13m:15s remains)
INFO - root - 2017-12-09 21:22:16.962895: step 59370, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 65h:52m:17s remains)
INFO - root - 2017-12-09 21:22:25.641172: step 59380, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.848 sec/batch; 64h:19m:22s remains)
INFO - root - 2017-12-09 21:22:34.179601: step 59390, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.856 sec/batch; 64h:56m:52s remains)
INFO - root - 2017-12-09 21:22:42.885939: step 59400, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.835 sec/batch; 63h:19m:02s remains)
2017-12-09 21:22:43.777141: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0017908718 -0.00178791 -0.0017862538 -0.0017848909 -0.0017836289 -0.0017823112 -0.0017808969 -0.0017794812 -0.0017784478 -0.0017778873 -0.0017779671 -0.0017788613 -0.001780377 -0.0017820898 -0.0017835749][-0.001789685 -0.0017862612 -0.0017842127 -0.0017826939 -0.0017814189 -0.0017800448 -0.001778433 -0.0017766961 -0.0017753773 -0.0017746262 -0.0017747512 -0.0017759108 -0.001777892 -0.0017800977 -0.0017819703][-0.0017892916 -0.0017856357 -0.0017833788 -0.0017819147 -0.0017807846 -0.0017794739 -0.0017777007 -0.0017755786 -0.0017739397 -0.0017729747 -0.0017732017 -0.0017746923 -0.0017771072 -0.0017797441 -0.0017818477][-0.0017884962 -0.0017847767 -0.0017825468 -0.0017812806 -0.0017804056 -0.0017792472 -0.0017773663 -0.0017749275 -0.0017729506 -0.0017718032 -0.0017722576 -0.0017741825 -0.0017770824 -0.0017800311 -0.0017822311][-0.0017877625 -0.0017840379 -0.0017820813 -0.0017811401 -0.0017805612 -0.0017795588 -0.0017775605 -0.0017748 -0.0017724998 -0.0017713734 -0.001772236 -0.0017746293 -0.0017778615 -0.0017808179 -0.0017828501][-0.0017874056 -0.0017838749 -0.001782406 -0.0017818086 -0.0017813892 -0.0017803735 -0.0017781381 -0.0017750326 -0.0017725092 -0.0017716384 -0.0017730487 -0.0017758929 -0.0017791835 -0.0017818277 -0.0017834356][-0.0017874333 -0.001784364 -0.0017833925 -0.0017830384 -0.0017825343 -0.0017813154 -0.0017786997 -0.0017752618 -0.0017725575 -0.0017720292 -0.001773985 -0.0017772708 -0.0017804772 -0.0017826381 -0.0017836913][-0.0017873698 -0.0017846419 -0.0017840363 -0.0017838186 -0.001783189 -0.0017818047 -0.0017789601 -0.0017753243 -0.0017725637 -0.0017723201 -0.0017746105 -0.0017780457 -0.0017809811 -0.0017826394 -0.0017832392][-0.0017869176 -0.0017843291 -0.0017838586 -0.0017835842 -0.001782848 -0.0017814891 -0.0017787613 -0.0017753495 -0.0017728413 -0.0017728491 -0.0017751524 -0.0017783158 -0.0017808022 -0.0017819796 -0.0017821856][-0.0017862783 -0.0017838136 -0.0017832912 -0.0017828967 -0.0017821424 -0.0017810095 -0.0017787146 -0.0017758802 -0.001773882 -0.0017740013 -0.0017759462 -0.0017784724 -0.0017803172 -0.0017810228 -0.0017809536][-0.0017858718 -0.0017834296 -0.0017827036 -0.0017822217 -0.0017815575 -0.0017808102 -0.0017791358 -0.001777046 -0.0017755905 -0.0017757046 -0.0017770834 -0.0017787246 -0.001779779 -0.001779972 -0.0017797872][-0.0017858227 -0.0017832206 -0.0017823484 -0.0017818279 -0.0017813329 -0.0017809373 -0.0017799513 -0.0017786387 -0.0017776939 -0.0017777439 -0.0017785262 -0.0017793401 -0.0017796773 -0.0017794627 -0.0017792039][-0.0017858017 -0.0017831231 -0.0017821102 -0.0017815841 -0.0017812488 -0.0017811275 -0.0017807551 -0.001780121 -0.0017796464 -0.0017796946 -0.0017800389 -0.0017802415 -0.0017801328 -0.0017797092 -0.001779418][-0.0017859731 -0.0017832648 -0.001782204 -0.0017817231 -0.0017814887 -0.0017815003 -0.0017814758 -0.0017813229 -0.0017812034 -0.0017813027 -0.0017814183 -0.0017813389 -0.0017810498 -0.0017805913 -0.0017803092][-0.0017863679 -0.0017838035 -0.0017827861 -0.0017824365 -0.001782248 -0.0017822323 -0.0017823139 -0.0017823912 -0.0017824244 -0.0017825385 -0.00178258 -0.0017824289 -0.0017821071 -0.001781711 -0.0017814927]]...]
INFO - root - 2017-12-09 21:22:52.465785: step 59410, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.882 sec/batch; 66h:52m:12s remains)
INFO - root - 2017-12-09 21:23:01.263921: step 59420, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.894 sec/batch; 67h:49m:44s remains)
INFO - root - 2017-12-09 21:23:09.703460: step 59430, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.890 sec/batch; 67h:30m:27s remains)
INFO - root - 2017-12-09 21:23:18.320655: step 59440, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.854 sec/batch; 64h:44m:40s remains)
INFO - root - 2017-12-09 21:23:26.685562: step 59450, loss = 0.82, batch loss = 0.69 (9.8 examples/sec; 0.818 sec/batch; 62h:02m:01s remains)
INFO - root - 2017-12-09 21:23:35.398484: step 59460, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.900 sec/batch; 68h:14m:44s remains)
INFO - root - 2017-12-09 21:23:44.106685: step 59470, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.882 sec/batch; 66h:54m:03s remains)
INFO - root - 2017-12-09 21:23:52.834597: step 59480, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.842 sec/batch; 63h:51m:19s remains)
INFO - root - 2017-12-09 21:24:01.610274: step 59490, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.886 sec/batch; 67h:13m:09s remains)
INFO - root - 2017-12-09 21:24:10.269402: step 59500, loss = 0.82, batch loss = 0.69 (8.0 examples/sec; 0.996 sec/batch; 75h:30m:09s remains)
2017-12-09 21:24:11.197790: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0018347788 -0.0018358101 -0.0018363289 -0.0018345576 -0.0018278804 -0.0018105893 -0.0017682392 -0.0016824554 -0.0015527759 -0.0014149002 -0.0013068387 -0.001260493 -0.0012878878 -0.0013865089 -0.0015259972][-0.0018341391 -0.0018348185 -0.0018344357 -0.0018298147 -0.0018157023 -0.0017792105 -0.0016916546 -0.0015238137 -0.0012831551 -0.0010264812 -0.00082733016 -0.00073947385 -0.00077982352 -0.00094627781 -0.0011860086][-0.0018336149 -0.0018337581 -0.0018319747 -0.0018231429 -0.0017983632 -0.0017335658 -0.0015794887 -0.0012872911 -0.00088107074 -0.00045880163 -0.00014263904 9.7230077e-07 -3.8207974e-05 -0.000268511 -0.00062065548][-0.0018329869 -0.0018325287 -0.0018292294 -0.0018162375 -0.0017811765 -0.0016892863 -0.0014704694 -0.0010489535 -0.00046370737 0.00014452694 0.00060249015 0.00082776591 0.00080823933 0.00053329451 9.2093134e-05][-0.0018324725 -0.0018317441 -0.0018280718 -0.0018139991 -0.0017761525 -0.0016766317 -0.0014341855 -0.00095300976 -0.00026279991 0.00048339448 0.0010896326 0.0014452751 0.0015094954 0.0012543859 0.0007723273][-0.0018320442 -0.0018313564 -0.00182869 -0.00181765 -0.0017869507 -0.0017038586 -0.0014949313 -0.0010634824 -0.00040445593 0.00037159969 0.0010894072 0.001594623 0.0017766944 0.0015821158 0.0011057538][-0.0018318685 -0.0018310142 -0.0018295097 -0.0018231432 -0.001804079 -0.0017499354 -0.0016086232 -0.0013019547 -0.00078759622 -0.0001020428 0.00063487457 0.0012435742 0.0015344647 0.0014053568 0.00094652118][-0.0018319517 -0.0018309081 -0.0018300845 -0.0018275836 -0.001819192 -0.0017924717 -0.001718078 -0.0015454609 -0.0012219765 -0.00072008488 -8.9517795e-05 0.00050890457 0.00084219372 0.000769253 0.00035581773][-0.0018319831 -0.0018308009 -0.001830262 -0.0018295796 -0.0018271421 -0.0018175832 -0.0017875383 -0.001710415 -0.0015459395 -0.0012497115 -0.00081996026 -0.00035670248 -6.24496e-05 -8.914771e-05 -0.00041106495][-0.0018318401 -0.0018307966 -0.001830356 -0.001830252 -0.0018298348 -0.0018278561 -0.0018194115 -0.0017939373 -0.0017303013 -0.0015965963 -0.0013733455 -0.0010994312 -0.000902541 -0.00090104877 -0.0011008122][-0.0018315708 -0.0018306666 -0.0018301619 -0.0018299272 -0.0018297904 -0.0018297574 -0.0018280277 -0.0018212562 -0.0018028469 -0.0017596645 -0.001676559 -0.0015590818 -0.0014626696 -0.0014545373 -0.0015469248][-0.0018311951 -0.0018303917 -0.0018299849 -0.0018296351 -0.0018294092 -0.0018297833 -0.0018298679 -0.0018287482 -0.0018248226 -0.0018153435 -0.0017944924 -0.0017596446 -0.0017267691 -0.0017218364 -0.0017519136][-0.0018306792 -0.0018302691 -0.0018300137 -0.0018296253 -0.0018293338 -0.0018295688 -0.0018299489 -0.0018300673 -0.001829367 -0.0018275866 -0.0018237218 -0.0018170539 -0.0018101768 -0.0018093152 -0.0018150705][-0.0018303215 -0.0018302058 -0.0018302404 -0.0018300096 -0.0018296782 -0.0018296335 -0.0018297697 -0.0018299632 -0.0018300182 -0.0018299352 -0.0018291527 -0.0018273492 -0.0018255025 -0.001825129 -0.001826147][-0.001830117 -0.0018300145 -0.0018301983 -0.0018301695 -0.0018299639 -0.0018298018 -0.0018297014 -0.0018297135 -0.001829912 -0.0018303117 -0.0018305779 -0.0018303769 -0.0018299144 -0.0018296689 -0.0018295527]]...]
INFO - root - 2017-12-09 21:24:19.918263: step 59510, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 65h:47m:44s remains)
INFO - root - 2017-12-09 21:24:28.671999: step 59520, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.857 sec/batch; 64h:57m:33s remains)
INFO - root - 2017-12-09 21:24:37.132047: step 59530, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.874 sec/batch; 66h:17m:05s remains)
INFO - root - 2017-12-09 21:24:45.860957: step 59540, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 66h:01m:38s remains)
INFO - root - 2017-12-09 21:24:54.261705: step 59550, loss = 0.82, batch loss = 0.69 (9.9 examples/sec; 0.808 sec/batch; 61h:16m:06s remains)
INFO - root - 2017-12-09 21:25:02.827237: step 59560, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.850 sec/batch; 64h:25m:24s remains)
INFO - root - 2017-12-09 21:25:11.394005: step 59570, loss = 0.81, batch loss = 0.68 (9.3 examples/sec; 0.859 sec/batch; 65h:06m:33s remains)
INFO - root - 2017-12-09 21:25:20.101555: step 59580, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.872 sec/batch; 66h:08m:38s remains)
INFO - root - 2017-12-09 21:25:28.872273: step 59590, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.886 sec/batch; 67h:09m:59s remains)
INFO - root - 2017-12-09 21:25:37.560050: step 59600, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.887 sec/batch; 67h:12m:27s remains)
2017-12-09 21:25:38.414649: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0018212729 -0.0018206536 -0.0018208883 -0.0018215084 -0.0018221354 -0.0018225011 -0.0018223709 -0.0018218925 -0.0018213195 -0.0018207971 -0.0018203104 -0.0018199047 -0.0018197382 -0.0018197851 -0.0018201034][-0.0018208954 -0.001820204 -0.0018206342 -0.0018216667 -0.0018227301 -0.0018232747 -0.0018231436 -0.0018225127 -0.0018216494 -0.0018206893 -0.0018196173 -0.0018187222 -0.0018182143 -0.0018182392 -0.0018187395][-0.0018209127 -0.0018202811 -0.0018208628 -0.0018222089 -0.0018237365 -0.0018245751 -0.0018244304 -0.0018236862 -0.001822548 -0.0018210709 -0.0018192602 -0.0018177062 -0.0018168126 -0.0018168089 -0.001817498][-0.0018209354 -0.0018204609 -0.0018211885 -0.0018227126 -0.0018244927 -0.0018255888 -0.0018255289 -0.0018247218 -0.001823252 -0.001821265 -0.0018188974 -0.001816806 -0.0018155945 -0.0018155561 -0.0018163859][-0.0018210603 -0.0018208236 -0.0018216881 -0.0018232807 -0.001825063 -0.0018261727 -0.0018262032 -0.0018253529 -0.0018236298 -0.0018213417 -0.0018186957 -0.0018162971 -0.0018148619 -0.0018147002 -0.001815503][-0.0018213181 -0.0018212424 -0.0018222239 -0.0018238005 -0.00182546 -0.0018263359 -0.0018263691 -0.0018255387 -0.0018237624 -0.0018214619 -0.0018188269 -0.0018163803 -0.0018148157 -0.0018144158 -0.0018149355][-0.0018214207 -0.0018214522 -0.0018223987 -0.0018238017 -0.0018251528 -0.0018256067 -0.0018253463 -0.0018246552 -0.0018232632 -0.0018212931 -0.0018188856 -0.0018165731 -0.0018149985 -0.0018143627 -0.0018145452][-0.0018211007 -0.0018210136 -0.001821814 -0.0018229787 -0.0018239795 -0.0018240523 -0.0018232584 -0.0018225296 -0.0018217105 -0.0018203511 -0.0018184974 -0.0018166014 -0.0018151937 -0.0018144182 -0.0018143705][-0.0018205984 -0.0018203618 -0.0018210716 -0.0018221347 -0.0018229106 -0.0018228918 -0.0018219174 -0.0018209118 -0.0018202172 -0.0018193712 -0.0018179744 -0.0018165123 -0.0018154597 -0.0018147966 -0.0018146794][-0.0018202931 -0.0018198818 -0.0018204603 -0.0018214453 -0.0018221767 -0.0018222879 -0.0018216625 -0.0018206986 -0.0018198711 -0.0018191761 -0.0018179864 -0.0018167896 -0.001816103 -0.0018157713 -0.0018157358][-0.001820381 -0.0018197631 -0.0018200558 -0.0018208132 -0.0018214295 -0.0018216383 -0.0018214448 -0.001820878 -0.0018201994 -0.0018195768 -0.001818487 -0.0018174897 -0.0018170314 -0.0018168959 -0.0018169769][-0.0018206104 -0.0018199625 -0.0018199889 -0.0018204181 -0.0018208629 -0.0018211288 -0.0018211564 -0.0018208636 -0.0018203845 -0.0018198936 -0.0018190951 -0.0018183987 -0.0018180736 -0.0018179803 -0.0018180899][-0.0018208415 -0.0018203232 -0.0018202685 -0.0018204906 -0.0018207639 -0.0018210107 -0.0018210667 -0.0018208494 -0.0018205058 -0.0018201601 -0.0018197144 -0.0018193312 -0.0018191516 -0.0018190931 -0.0018191435][-0.0018211222 -0.0018207044 -0.0018206503 -0.0018207892 -0.0018209624 -0.001821139 -0.0018211941 -0.0018210495 -0.0018208056 -0.0018205945 -0.0018204211 -0.001820276 -0.0018201785 -0.0018200799 -0.0018200438][-0.001821277 -0.0018208669 -0.0018207679 -0.0018208483 -0.001820976 -0.0018211335 -0.0018212289 -0.0018211732 -0.0018210387 -0.0018209154 -0.0018208255 -0.0018207371 -0.0018206553 -0.0018205536 -0.0018204761]]...]
INFO - root - 2017-12-09 21:25:47.025214: step 59610, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.858 sec/batch; 65h:02m:33s remains)
INFO - root - 2017-12-09 21:25:55.815968: step 59620, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 65h:29m:05s remains)
INFO - root - 2017-12-09 21:26:04.216609: step 59630, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.894 sec/batch; 67h:44m:38s remains)
INFO - root - 2017-12-09 21:26:12.764510: step 59640, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.866 sec/batch; 65h:36m:50s remains)
INFO - root - 2017-12-09 21:26:21.538151: step 59650, loss = 0.82, batch loss = 0.69 (10.6 examples/sec; 0.757 sec/batch; 57h:22m:34s remains)
INFO - root - 2017-12-09 21:26:30.319567: step 59660, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.879 sec/batch; 66h:35m:45s remains)
INFO - root - 2017-12-09 21:26:39.070941: step 59670, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.881 sec/batch; 66h:47m:46s remains)
INFO - root - 2017-12-09 21:26:47.922232: step 59680, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.870 sec/batch; 65h:54m:54s remains)
INFO - root - 2017-12-09 21:26:56.731970: step 59690, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.859 sec/batch; 65h:03m:51s remains)
INFO - root - 2017-12-09 21:27:05.390111: step 59700, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 65h:19m:05s remains)
2017-12-09 21:27:06.311087: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0018223678 -0.0018208384 -0.0018195976 -0.0018180062 -0.0018166607 -0.0018156844 -0.0018148748 -0.0018151281 -0.0018165389 -0.0018189333 -0.0018209456 -0.0018223565 -0.0018232669 -0.0018240559 -0.0018244721][-0.0018221932 -0.0018206795 -0.0018197418 -0.0018186658 -0.0018176336 -0.0018169293 -0.0018161277 -0.001816521 -0.0018182223 -0.0018203502 -0.0018210568 -0.0018202625 -0.0018190896 -0.0018179787 -0.0018166057][-0.0018221252 -0.0018209468 -0.0018205482 -0.001820397 -0.0018203265 -0.0018201832 -0.0018195848 -0.0018201683 -0.0018219895 -0.0018233838 -0.0018221861 -0.0018187813 -0.0018149707 -0.0018115101 -0.0018082883][-0.0018219972 -0.0018213477 -0.0018216507 -0.0018224769 -0.0018232644 -0.0018237671 -0.0018234145 -0.0018242034 -0.0018258644 -0.0018263288 -0.001823332 -0.0018174311 -0.0018110882 -0.0018052278 -0.0017997079][-0.0018215512 -0.0018213205 -0.0018222235 -0.0018237877 -0.0018252762 -0.0018262669 -0.00182616 -0.0018271655 -0.0018286328 -0.0018281726 -0.0018238248 -0.0018163541 -0.0018080296 -0.0018001122 -0.0017927199][-0.0018210716 -0.0018210866 -0.0018222723 -0.0018242027 -0.0018259711 -0.0018274876 -0.0018278226 -0.001828869 -0.0018301355 -0.0018291079 -0.001824175 -0.0018161624 -0.0018070566 -0.0017977045 -0.0017891628][-0.0018204337 -0.0018203596 -0.0018217739 -0.0018239218 -0.0018258162 -0.0018276207 -0.0018285265 -0.0018299003 -0.0018312846 -0.00182984 -0.0018248232 -0.0018172497 -0.0018085643 -0.0017993755 -0.0017908608][-0.0018188711 -0.0018188792 -0.0018204257 -0.0018224798 -0.0018243574 -0.0018263248 -0.0018276483 -0.001829196 -0.0018302447 -0.0018290605 -0.0018247701 -0.0018186389 -0.001811697 -0.0018042574 -0.001797486][-0.0018165503 -0.0018165181 -0.0018182314 -0.0018202669 -0.0018221365 -0.0018239445 -0.0018254539 -0.0018268902 -0.0018275965 -0.0018268282 -0.0018237589 -0.0018196158 -0.0018148673 -0.0018098406 -0.0018051348][-0.0018136741 -0.0018136867 -0.0018153349 -0.0018172035 -0.0018189951 -0.0018206083 -0.0018218614 -0.001822933 -0.0018238121 -0.0018236231 -0.0018220859 -0.0018199509 -0.0018174798 -0.0018147397 -0.0018120348][-0.0018109551 -0.0018108085 -0.0018122355 -0.0018137874 -0.0018154572 -0.00181696 -0.0018181466 -0.0018189768 -0.0018197912 -0.0018203588 -0.0018200995 -0.0018191587 -0.001818232 -0.001817573 -0.0018166229][-0.0018084326 -0.0018079499 -0.0018091541 -0.0018104111 -0.0018120153 -0.0018133138 -0.0018145483 -0.0018154443 -0.0018162387 -0.001817069 -0.0018171994 -0.001817191 -0.0018170991 -0.0018175445 -0.0018179035][-0.0018063748 -0.0018056014 -0.0018064708 -0.0018075376 -0.0018090178 -0.0018102179 -0.0018112716 -0.0018121182 -0.0018128143 -0.0018136003 -0.0018137896 -0.0018142471 -0.0018146383 -0.0018156403 -0.0018165364][-0.0018049042 -0.0018038576 -0.0018044233 -0.0018054359 -0.0018067119 -0.0018077407 -0.0018087195 -0.0018095816 -0.0018102615 -0.0018108666 -0.0018110839 -0.0018115717 -0.001811936 -0.0018128722 -0.0018139861][-0.0018043307 -0.0018029363 -0.0018029736 -0.0018037582 -0.0018047289 -0.0018057071 -0.0018066135 -0.001807446 -0.0018081439 -0.0018088103 -0.0018090144 -0.0018094968 -0.0018096925 -0.001810409 -0.0018112212]]...]
INFO - root - 2017-12-09 21:27:14.933345: step 59710, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.885 sec/batch; 67h:03m:13s remains)
INFO - root - 2017-12-09 21:27:23.825523: step 59720, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.876 sec/batch; 66h:24m:03s remains)
INFO - root - 2017-12-09 21:27:32.266420: step 59730, loss = 0.82, batch loss = 0.69 (9.8 examples/sec; 0.817 sec/batch; 61h:56m:14s remains)
INFO - root - 2017-12-09 21:27:40.939915: step 59740, loss = 0.82, batch loss = 0.69 (9.7 examples/sec; 0.828 sec/batch; 62h:43m:28s remains)
INFO - root - 2017-12-09 21:27:49.503045: step 59750, loss = 0.82, batch loss = 0.69 (10.9 examples/sec; 0.736 sec/batch; 55h:43m:54s remains)
INFO - root - 2017-12-09 21:27:58.003466: step 59760, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.876 sec/batch; 66h:20m:12s remains)
INFO - root - 2017-12-09 21:28:06.645370: step 59770, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.873 sec/batch; 66h:06m:25s remains)
INFO - root - 2017-12-09 21:28:15.277329: step 59780, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.863 sec/batch; 65h:21m:51s remains)
INFO - root - 2017-12-09 21:28:24.002065: step 59790, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.893 sec/batch; 67h:38m:15s remains)
INFO - root - 2017-12-09 21:28:32.755095: step 59800, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.855 sec/batch; 64h:44m:12s remains)
2017-12-09 21:28:33.644237: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0013863393 -0.0010604584 -0.00065063406 -0.00020514044 0.00019626168 0.000464104 0.0005412678 0.00042060495 0.00015006808 -0.00022845878 -0.00065859442 -0.0010548797 -0.0013713294 -0.0015899782 -0.0017302412][-0.00071115035 -0.00018693297 0.00041118928 0.00099157647 0.0014580906 0.0017030312 0.0016790353 0.0014035368 0.00092737575 0.00033749745 -0.00029493961 -0.00085546589 -0.0012703359 -0.0015403838 -0.0017061286][0.00091582665 0.0019272323 0.0029671285 0.0038515395 0.0044724457 0.0046836012 0.0044787088 0.0039185314 0.0030538593 0.0020180061 0.00090166193 -9.8909019e-05 -0.00085230207 -0.0013470483 -0.0016354603][0.003899192 0.0058217496 0.0076746563 0.0091242446 0.010043076 0.0102323 0.009764187 0.0087652244 0.0072591868 0.0054457337 0.0034452435 0.001608102 0.00016723957 -0.00082928536 -0.0014188664][0.008145893 0.011430852 0.014502144 0.016832281 0.018249847 0.018504316 0.017767962 0.01623863 0.013909466 0.011029847 0.0077466522 0.004627944 0.0020561446 0.00018329418 -0.00097099115][0.01302155 0.017992105 0.022598771 0.02609949 0.028239327 0.028715314 0.027785525 0.025716543 0.022476424 0.018348409 0.01352455 0.0088182334 0.0047917976 0.0017376182 -0.00023104111][0.017260902 0.023858342 0.029974189 0.034693506 0.037665315 0.038564675 0.037697725 0.035357393 0.031502504 0.026346484 0.02010438 0.013776598 0.0081586745 0.0037389235 0.00077115942][0.019556234 0.02724486 0.034426659 0.040092349 0.0437902 0.045196947 0.044595923 0.042303089 0.0383072 0.032694262 0.025646606 0.018196428 0.011335206 0.0057555842 0.0018524498][0.019159975 0.027031837 0.034487616 0.040535152 0.044641476 0.046503 0.046338104 0.044446919 0.040861584 0.035524406 0.028522069 0.02077492 0.013382573 0.0071872077 0.0026852326][0.016197167 0.023297159 0.030159842 0.035925522 0.040031664 0.042193796 0.042527806 0.041289866 0.0385405 0.034082256 0.027897527 0.020720541 0.013634119 0.0075330776 0.0029588514][0.011698033 0.017346933 0.022947531 0.027838986 0.031511903 0.033716559 0.034474589 0.033968415 0.032235485 0.028988531 0.024125941 0.018178618 0.012099715 0.0067261704 0.0025962656][0.0069154804 0.010842371 0.014847609 0.018498642 0.021404583 0.02335573 0.024311572 0.024376962 0.023541406 0.021499386 0.018115431 0.013740971 0.0091149313 0.0049331309 0.0016705325][0.0029005585 0.0052355239 0.0076969494 0.010038987 0.012018998 0.013483032 0.01437444 0.014717991 0.014458374 0.013358586 0.011298792 0.0084979711 0.0054494543 0.0026511094 0.00046287465][0.00015833729 0.0012787775 0.0025056768 0.0037256838 0.0048256232 0.0057224268 0.0063758041 0.0067605334 0.0068009538 0.0063395719 0.0052918871 0.003785505 0.0021026968 0.00054991839 -0.00064617768][-0.0012555462 -0.00086104742 -0.00040910533 6.0132123e-05 0.000515432 0.000929308 0.0012859005 0.0015539372 0.0016711355 0.0015482047 0.0011401767 0.00050893484 -0.00021673902 -0.0008832076 -0.0013791884]]...]
INFO - root - 2017-12-09 21:28:42.372457: step 59810, loss = 0.82, batch loss = 0.69 (8.8 examples/sec; 0.906 sec/batch; 68h:36m:53s remains)
INFO - root - 2017-12-09 21:28:51.069504: step 59820, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.881 sec/batch; 66h:45m:42s remains)
INFO - root - 2017-12-09 21:28:59.525711: step 59830, loss = 0.82, batch loss = 0.69 (10.6 examples/sec; 0.751 sec/batch; 56h:54m:36s remains)
INFO - root - 2017-12-09 21:29:08.233040: step 59840, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.874 sec/batch; 66h:11m:27s remains)
INFO - root - 2017-12-09 21:29:16.887126: step 59850, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.855 sec/batch; 64h:45m:36s remains)
INFO - root - 2017-12-09 21:29:25.222928: step 59860, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.831 sec/batch; 62h:57m:44s remains)
INFO - root - 2017-12-09 21:29:33.809881: step 59870, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.834 sec/batch; 63h:10m:56s remains)
INFO - root - 2017-12-09 21:29:42.399522: step 59880, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.861 sec/batch; 65h:11m:49s remains)
INFO - root - 2017-12-09 21:29:51.068224: step 59890, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 65h:17m:17s remains)
INFO - root - 2017-12-09 21:29:59.757090: step 59900, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.872 sec/batch; 66h:00m:50s remains)
2017-12-09 21:30:00.610137: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.040454622 0.038598277 0.035420679 0.030169688 0.024287855 0.018529192 0.013727883 0.010628607 0.0094055887 0.010332051 0.012551876 0.015473233 0.017923726 0.019787982 0.020123469][0.060392316 0.059778333 0.056682549 0.049999055 0.041692521 0.032409977 0.02407353 0.018006805 0.014890869 0.015544702 0.018768175 0.023676453 0.028312502 0.032052454 0.033613786][0.0860069 0.086750492 0.083780758 0.076096989 0.065664537 0.053313725 0.041818947 0.032798793 0.028000345 0.028486392 0.032824669 0.039542746 0.046319932 0.052003931 0.054630302][0.11486082 0.11699102 0.11438591 0.10638735 0.094722904 0.080321111 0.066543415 0.055559643 0.049726687 0.05012789 0.055079754 0.063049279 0.071428642 0.078460306 0.08145117][0.14293782 0.14640722 0.14459693 0.13715667 0.12539154 0.11032739 0.095568657 0.083667137 0.077273242 0.077464454 0.082649976 0.091179721 0.10033351 0.10764316 0.11042113][0.1663318 0.17118241 0.170643 0.16460842 0.15387952 0.13959816 0.12517835 0.11335842 0.10673639 0.10641968 0.1110759 0.1190267 0.12776145 0.13461013 0.13684064][0.18143946 0.18789117 0.18875971 0.1845627 0.17565092 0.16319147 0.15016021 0.13919231 0.13262884 0.13156819 0.13497245 0.14135082 0.14857262 0.1540793 0.15539092][0.18625838 0.19444342 0.1966314 0.19433779 0.18756284 0.17739296 0.16624671 0.156381 0.14994498 0.14788371 0.14957656 0.15382744 0.15891497 0.16269402 0.16294703][0.17884819 0.18833357 0.19164257 0.19113716 0.18646571 0.17858893 0.16942376 0.16076882 0.15443465 0.15132928 0.15128517 0.15344767 0.15646458 0.15865715 0.15805593][0.15924144 0.16910017 0.17307445 0.17391638 0.17111097 0.16533236 0.15806733 0.15050502 0.14420784 0.14017704 0.13866235 0.13913339 0.14057067 0.14175767 0.14088972][0.12926547 0.13818534 0.14209452 0.14356 0.14206913 0.13808809 0.13268739 0.12649381 0.12086851 0.11668868 0.11447649 0.11388838 0.11429543 0.11488426 0.11397327][0.093734212 0.10077205 0.10409972 0.10562754 0.10493467 0.10233242 0.098547444 0.093880557 0.089395039 0.085813895 0.083731025 0.08294408 0.083061352 0.083466388 0.082812622][0.059583861 0.064350113 0.066767417 0.06796027 0.067652255 0.066039249 0.063568577 0.060364503 0.05718736 0.054593068 0.053070225 0.052515209 0.052702747 0.053178795 0.052944034][0.032261632 0.034998365 0.036502168 0.037243571 0.037092321 0.036161486 0.034701977 0.032760508 0.03082199 0.029251022 0.028386898 0.028155241 0.028441386 0.0289376 0.02898181][0.013932013 0.015243211 0.016009297 0.016367311 0.016275575 0.015784303 0.015020206 0.01401005 0.013032326 0.012276062 0.011930401 0.011935834 0.01224788 0.012669304 0.012832109]]...]
INFO - root - 2017-12-09 21:30:09.271844: step 59910, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 65h:50m:07s remains)
INFO - root - 2017-12-09 21:30:17.849792: step 59920, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.833 sec/batch; 63h:06m:11s remains)
INFO - root - 2017-12-09 21:30:26.405645: step 59930, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.844 sec/batch; 63h:53m:32s remains)
INFO - root - 2017-12-09 21:30:34.827924: step 59940, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.853 sec/batch; 64h:33m:41s remains)
INFO - root - 2017-12-09 21:30:43.332861: step 59950, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.894 sec/batch; 67h:41m:30s remains)
INFO - root - 2017-12-09 21:30:51.908434: step 59960, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.879 sec/batch; 66h:32m:39s remains)
INFO - root - 2017-12-09 21:31:00.539070: step 59970, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.842 sec/batch; 63h:43m:11s remains)
INFO - root - 2017-12-09 21:31:09.206887: step 59980, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 65h:24m:44s remains)
INFO - root - 2017-12-09 21:31:17.984613: step 59990, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.870 sec/batch; 65h:49m:34s remains)
INFO - root - 2017-12-09 21:31:26.686721: step 60000, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.881 sec/batch; 66h:39m:40s remains)
2017-12-09 21:31:27.501847: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.00024199497 0.00026440073 0.0009751179 0.0015716451 0.0019222297 0.0019378824 0.0016910374 0.0012675746 0.00077913643 0.00040774618 0.00020891184 0.00012472167 -3.9755832e-06 -0.00024740119 -0.00058935024][0.00064894452 0.0016296994 0.0029106 0.0040159933 0.004707037 0.0048179431 0.0044057784 0.0036290959 0.0027151578 0.001949129 0.0014529555 0.0011844732 0.00092152448 0.00052417966 7.49412e-06][0.0015531603 0.0031574643 0.0051169069 0.00684672 0.0080279205 0.008404674 0.0079364032 0.0068330732 0.0054580178 0.0042134975 0.0032781647 0.0026640561 0.0021299203 0.0014940466 0.0007507453][0.0026002107 0.0049222028 0.0075862841 0.0099487193 0.011624747 0.012276468 0.011786533 0.010419106 0.0086662434 0.0070467838 0.0057935719 0.004906883 0.0041044895 0.0031838738 0.0021113693][0.0037339381 0.006726624 0.010009876 0.012898823 0.014975133 0.015852664 0.015385359 0.013879604 0.011955309 0.0102279 0.0089215795 0.0079656877 0.0070009292 0.0057883975 0.0042648111][0.004848707 0.0084598549 0.012290232 0.015592727 0.017911918 0.018860094 0.018394904 0.01687192 0.01500835 0.013451596 0.012376055 0.011557813 0.010525876 0.0090112286 0.0069438969][0.0057349722 0.0098348772 0.014107128 0.017723782 0.02017949 0.021108557 0.020612845 0.019139618 0.017503278 0.016299127 0.015599794 0.015035239 0.014013168 0.012238212 0.0096372534][0.0063510952 0.010699227 0.015157348 0.018853273 0.021271886 0.022102524 0.021583822 0.02024954 0.018954035 0.018202826 0.017961327 0.017738558 0.016834868 0.014915963 0.011919213][0.0068511912 0.011202957 0.015525744 0.01897455 0.021085044 0.021687185 0.021135744 0.020026643 0.019151146 0.018896284 0.019134063 0.019287536 0.018586446 0.016658522 0.013447527][0.0072053275 0.011281974 0.015120273 0.017976297 0.01951753 0.019789884 0.019243488 0.018464182 0.018082814 0.018345606 0.019025303 0.019495629 0.018978713 0.017102022 0.013834222][0.00699669 0.010474114 0.013528794 0.015575723 0.016454129 0.016430708 0.016003866 0.015664347 0.015805341 0.016528266 0.01752891 0.0181777 0.017749673 0.01592027 0.012744647][0.0057467553 0.0083431667 0.010450488 0.011679272 0.012017326 0.01185192 0.011655116 0.011784027 0.012397584 0.0134579 0.014611463 0.015290889 0.014885011 0.013151595 0.010267453][0.0035945368 0.0052116783 0.0064127394 0.0069907256 0.0070201294 0.0068672667 0.0069018942 0.0073504741 0.0082228146 0.0093964729 0.010515635 0.011129785 0.010760385 0.009275631 0.0069399313][0.0012610162 0.0020688344 0.0026086466 0.002800378 0.0027323207 0.0026561925 0.0028079841 0.0033182139 0.0041405312 0.0051441505 0.0060347761 0.0065042558 0.0062164278 0.0051338547 0.0035259509][-0.00049260829 -0.00018305809 -3.6902493e-06 2.6519061e-05 -2.9994408e-05 -4.52745e-05 9.910448e-05 0.00047387939 0.0010372075 0.0016975574 0.0022595068 0.0025477759 0.0023597465 0.0017175645 0.00081812788]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-momentum-lr0.0001-clip-zeroinit-conv1-3init-from-scratch/model.ckpt-60000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-momentum-lr0.0001-clip-zeroinit-conv1-3init-from-scratch/model.ckpt-60000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-09 21:31:36.899722: step 60010, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.865 sec/batch; 65h:27m:35s remains)
INFO - root - 2017-12-09 21:31:45.630488: step 60020, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.839 sec/batch; 63h:31m:56s remains)
INFO - root - 2017-12-09 21:31:54.110253: step 60030, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 65h:14m:11s remains)
INFO - root - 2017-12-09 21:32:02.509644: step 60040, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.859 sec/batch; 65h:02m:18s remains)
INFO - root - 2017-12-09 21:32:11.296380: step 60050, loss = 0.83, batch loss = 0.70 (8.7 examples/sec; 0.918 sec/batch; 69h:29m:38s remains)
INFO - root - 2017-12-09 21:32:19.622719: step 60060, loss = 0.82, batch loss = 0.69 (9.7 examples/sec; 0.829 sec/batch; 62h:42m:46s remains)
INFO - root - 2017-12-09 21:32:28.295203: step 60070, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 65h:41m:59s remains)
INFO - root - 2017-12-09 21:32:37.194488: step 60080, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.887 sec/batch; 67h:09m:04s remains)
INFO - root - 2017-12-09 21:32:45.956279: step 60090, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.885 sec/batch; 66h:58m:44s remains)
INFO - root - 2017-12-09 21:32:54.702748: step 60100, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.902 sec/batch; 68h:17m:06s remains)
2017-12-09 21:32:55.595372: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.51384485 0.506676 0.49956769 0.4931365 0.48732767 0.48288342 0.47779346 0.47227278 0.46562961 0.45655611 0.4465571 0.43658626 0.42872658 0.42198136 0.41341385][0.517657 0.51276082 0.50699687 0.5018791 0.49774837 0.49532357 0.49242783 0.48940328 0.48442733 0.47597018 0.46464685 0.4519954 0.44080916 0.431097 0.41995105][0.51242912 0.51126659 0.509006 0.5072909 0.5064618 0.50635839 0.50516409 0.50366569 0.49929178 0.49038595 0.47747841 0.4621028 0.44776604 0.43488306 0.42111245][0.5023399 0.50762153 0.51153469 0.51566088 0.52013743 0.52331823 0.52413523 0.52253377 0.51670313 0.50610137 0.49074179 0.47262529 0.45478588 0.43896791 0.42268488][0.49117029 0.50203741 0.51200044 0.5236035 0.53497118 0.5418241 0.54485726 0.54310769 0.53528893 0.52125078 0.50162172 0.48060438 0.46020475 0.44141847 0.42305043][0.48340404 0.49970973 0.51471215 0.53207886 0.54844177 0.5592832 0.56361884 0.56074011 0.550501 0.53284872 0.50880969 0.48401368 0.46106362 0.44015631 0.41998893][0.4780491 0.50020105 0.51973319 0.54149044 0.561244 0.57432812 0.57798052 0.57233775 0.55794418 0.5367173 0.50961995 0.48221713 0.45754752 0.43586275 0.41556117][0.46949935 0.49680364 0.51999021 0.54482967 0.56591177 0.57961136 0.58213121 0.57349646 0.5555976 0.53055179 0.50105107 0.47250181 0.44694918 0.42511362 0.40504929][0.46033561 0.49100247 0.51611954 0.54221 0.56324571 0.57600027 0.57648271 0.56492245 0.54403126 0.51677316 0.48542067 0.45647645 0.43129691 0.41023865 0.39117306][0.44790617 0.47876817 0.50235564 0.527565 0.54691213 0.55794859 0.55663222 0.54338121 0.52156514 0.49353471 0.46259043 0.43478814 0.41171238 0.39338806 0.37675789][0.43116724 0.46079242 0.48169902 0.50238985 0.51734126 0.52582359 0.52225888 0.5084421 0.48785666 0.46290749 0.43635929 0.41172475 0.39217922 0.37735996 0.36350417][0.4109174 0.43831325 0.45595139 0.4716568 0.48199993 0.48674929 0.4816125 0.46822295 0.44998327 0.42946085 0.40863132 0.38960746 0.3747496 0.3635653 0.35231814][0.39012158 0.41342232 0.42668766 0.43806073 0.44474736 0.44631466 0.44052458 0.42868137 0.41392684 0.39814541 0.38246694 0.36866263 0.35818574 0.35028791 0.34136221][0.37371224 0.39239135 0.40158603 0.40893605 0.41323343 0.41362277 0.40865895 0.39947402 0.3884272 0.37730783 0.36642146 0.35658085 0.34981179 0.34457371 0.33761466][0.36138967 0.3761391 0.3811996 0.38553417 0.38810694 0.3884722 0.3852621 0.37934709 0.37245068 0.36535758 0.35884616 0.35280737 0.34883505 0.34571418 0.34066549]]...]
INFO - root - 2017-12-09 21:33:04.422010: step 60110, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.878 sec/batch; 66h:25m:44s remains)
INFO - root - 2017-12-09 21:33:13.080499: step 60120, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.882 sec/batch; 66h:43m:27s remains)
INFO - root - 2017-12-09 21:33:21.728707: step 60130, loss = 0.82, batch loss = 0.69 (8.2 examples/sec; 0.973 sec/batch; 73h:36m:54s remains)
INFO - root - 2017-12-09 21:33:30.253034: step 60140, loss = 0.82, batch loss = 0.69 (8.8 examples/sec; 0.911 sec/batch; 68h:56m:04s remains)
INFO - root - 2017-12-09 21:33:38.984804: step 60150, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.856 sec/batch; 64h:44m:58s remains)
INFO - root - 2017-12-09 21:33:47.599695: step 60160, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.863 sec/batch; 65h:15m:10s remains)
INFO - root - 2017-12-09 21:33:56.273140: step 60170, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.886 sec/batch; 66h:59m:50s remains)
INFO - root - 2017-12-09 21:34:04.960036: step 60180, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.863 sec/batch; 65h:18m:52s remains)
INFO - root - 2017-12-09 21:34:13.614066: step 60190, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.845 sec/batch; 63h:54m:50s remains)
INFO - root - 2017-12-09 21:34:22.424864: step 60200, loss = 0.82, batch loss = 0.69 (8.3 examples/sec; 0.961 sec/batch; 72h:41m:23s remains)
2017-12-09 21:34:23.322032: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.00031003787 0.00039869139 0.00037608307 0.00031524675 0.00025621441 0.00012059149 -8.8694273e-05 -0.00033822178 -0.000552202 -0.00074225874 -0.00095624331 -0.0012015358 -0.0014385022 -0.0016245564 -0.0017501159][0.0011348481 0.0012804427 0.0012386603 0.0011572087 0.0010745748 0.0009089309 0.00065280951 0.00033978012 6.5672677e-05 -0.00018559978 -0.00049083726 -0.00084789714 -0.0012008278 -0.0014867922 -0.0016853774][0.0020031696 0.0021896949 0.0021282872 0.0020309794 0.001913759 0.0017020277 0.0013745836 0.00099945825 0.00067677291 0.00038216577 -1.9406434e-07 -0.00046281668 -0.000932976 -0.0013258136 -0.0016031507][0.0027208449 0.00295335 0.0028883587 0.0027852161 0.0026253667 0.0023600659 0.0019538733 0.0015058717 0.0011133641 0.00076732005 0.0003212759 -0.00019976927 -0.00072751148 -0.0011821921 -0.0015185704][0.003131018 0.00340387 0.0033570216 0.0032821358 0.0031269412 0.0028423513 0.0023728269 0.0018430856 0.0013482958 0.00091699546 0.000406867 -0.00013757008 -0.00066881219 -0.0011252731 -0.0014728794][0.0031631007 0.0034889281 0.0034927544 0.0034873341 0.0033913353 0.0031490778 0.0026598107 0.0020424919 0.0014115375 0.00085107784 0.00026159978 -0.00028053904 -0.00076886627 -0.0011766992 -0.0014899374][0.0028535384 0.0032189218 0.0032900223 0.0034094127 0.0034610508 0.0033372925 0.0028906045 0.0022252412 0.001462889 0.00074264745 4.5708264e-05 -0.00052169187 -0.00097591855 -0.0013170696 -0.0015652206][0.0022780094 0.0026546447 0.0027819802 0.00304073 0.0032806569 0.0033475999 0.0030413731 0.0024230131 0.0016043411 0.00075599842 -6.8634632e-05 -0.00071658345 -0.001188828 -0.0014900309 -0.001668627][0.0015421199 0.0018956735 0.0020467956 0.0024238918 0.0028469767 0.0031299354 0.0030396329 0.0025811181 0.0018254615 0.0009397323 1.9065104e-05 -0.00074447796 -0.0012983491 -0.0016134211 -0.0017514873][0.00077724189 0.0010599202 0.0011756314 0.0016201903 0.00219385 0.002691763 0.0028513693 0.0026231962 0.0020275638 0.0011977464 0.00024089601 -0.0006250221 -0.0012873772 -0.0016611289 -0.0017967563][9.5616793e-05 0.00027014187 0.00030892633 0.00073943322 0.001380131 0.0020403913 0.0024306094 0.0024403948 0.00204378 0.0013444527 0.00043916341 -0.00046328397 -0.0012075917 -0.0016493272 -0.0018080068][-0.00049515197 -0.00042581698 -0.00046064914 -9.9304831e-05 0.000516723 0.0012256681 0.0017547492 0.0019566799 0.0017540617 0.0012178716 0.00043086696 -0.00042002578 -0.0011693896 -0.0016361936 -0.0018084748][-0.00098803849 -0.00099714473 -0.0010625607 -0.00080019305 -0.00030142 0.00032991206 0.00088248763 0.0011906599 0.0011443669 0.00078184309 0.00016173872 -0.000560057 -0.0012256476 -0.0016488296 -0.0018082256][-0.0013746596 -0.001417228 -0.0014699983 -0.0013020311 -0.00096098147 -0.00049342506 -2.2501452e-05 0.00030325342 0.00036250998 0.00015104993 -0.00029563019 -0.00084746268 -0.0013676415 -0.0016922201 -0.0018119621][-0.0016390767 -0.0016802531 -0.0017049405 -0.0016157882 -0.0014294479 -0.0011422234 -0.00080248213 -0.00051876425 -0.00040273077 -0.00050370127 -0.000797822 -0.0011799142 -0.0015347379 -0.0017433359 -0.0018170179]]...]
INFO - root - 2017-12-09 21:34:32.018793: step 60210, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.881 sec/batch; 66h:36m:47s remains)
INFO - root - 2017-12-09 21:34:40.789742: step 60220, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.865 sec/batch; 65h:24m:40s remains)
INFO - root - 2017-12-09 21:34:49.425047: step 60230, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.884 sec/batch; 66h:52m:12s remains)
INFO - root - 2017-12-09 21:34:58.057964: step 60240, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.892 sec/batch; 67h:28m:18s remains)
INFO - root - 2017-12-09 21:35:06.785216: step 60250, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.883 sec/batch; 66h:45m:46s remains)
INFO - root - 2017-12-09 21:35:15.329184: step 60260, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 65h:12m:30s remains)
INFO - root - 2017-12-09 21:35:23.925632: step 60270, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.874 sec/batch; 66h:05m:15s remains)
INFO - root - 2017-12-09 21:35:32.652505: step 60280, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 65h:51m:06s remains)
INFO - root - 2017-12-09 21:35:41.423752: step 60290, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 65h:41m:31s remains)
INFO - root - 2017-12-09 21:35:50.126959: step 60300, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.886 sec/batch; 67h:00m:13s remains)
2017-12-09 21:35:50.956110: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.016543189 0.015704563 0.014541405 0.013385366 0.012772795 0.012671212 0.012940577 0.013042378 0.012701498 0.011765894 0.010469403 0.0086958027 0.0065275687 0.0039033033 0.0015469607][0.018904231 0.01774147 0.016269358 0.014923339 0.014176338 0.01410794 0.014595634 0.014976637 0.01478996 0.014031923 0.012777865 0.010989897 0.0086270729 0.0056186691 0.0027462449][0.01895923 0.017668065 0.016135935 0.014713599 0.013933252 0.014032056 0.014821035 0.01555515 0.015560105 0.015000659 0.013859808 0.01227593 0.0099741481 0.0067994795 0.0036044572][0.01786029 0.016393304 0.014838993 0.013418339 0.012570835 0.012823801 0.014032616 0.015352304 0.015806317 0.015540312 0.01453035 0.013115273 0.010919578 0.0077194874 0.0043082605][0.015923209 0.014246262 0.012523696 0.011033387 0.01031056 0.010785061 0.012468735 0.014390641 0.01541967 0.015490173 0.014717449 0.013483495 0.011408135 0.0082891872 0.0047709169][0.013834317 0.011803737 0.0097947149 0.0081758332 0.0075031877 0.00831833 0.010537182 0.012970028 0.014462749 0.01483911 0.014330263 0.013367463 0.011496026 0.008504333 0.0049874303][0.011950205 0.009558293 0.0072318888 0.0055518565 0.0049707964 0.0060010604 0.008582267 0.011421728 0.013267307 0.013800859 0.01351626 0.012817118 0.011193133 0.0083909826 0.004962197][0.010461069 0.0080064414 0.0057232426 0.004051331 0.0035485495 0.0045505976 0.0070764641 0.0098521449 0.011745268 0.01238881 0.012252465 0.011780985 0.010482772 0.0079682376 0.0047451369][0.0092605613 0.0069978568 0.0049985959 0.0036185118 0.0031948285 0.0039473386 0.0061317119 0.0085630724 0.010259643 0.01077081 0.010682622 0.010371041 0.0093602566 0.0071714283 0.0042592944][0.0081305252 0.0064097117 0.0049236314 0.0037302384 0.0032153171 0.0036882767 0.0054377723 0.0074411896 0.0088042952 0.0090818182 0.0088945217 0.0085928272 0.0078018722 0.0059611858 0.0034517054][0.007098529 0.0058203442 0.0047458517 0.003766011 0.003109029 0.0032614744 0.0045970292 0.0062543373 0.0073413015 0.0074307811 0.0071136034 0.0067070392 0.0059641558 0.0044212 0.0023834226][0.0060853432 0.0052054874 0.0044702878 0.0036161342 0.0028972705 0.002822211 0.0037431992 0.0050740517 0.0058954232 0.0058258749 0.0054032477 0.0048855338 0.0041381251 0.0028009219 0.0011896879][0.0048182067 0.0043122862 0.0039125639 0.0032804515 0.0026564365 0.0024693706 0.0030379971 0.0039356379 0.0044318531 0.004287417 0.0038322015 0.003284202 0.0025747158 0.0014709086 0.0002736802][0.0031667822 0.0028746915 0.0027056453 0.0023659198 0.0020327414 0.0018787807 0.0021662163 0.0026566843 0.0028500771 0.0026406287 0.0022085328 0.0017253006 0.0011192727 0.00030593446 -0.00048958813][0.0012223365 0.001125072 0.001092957 0.00098241365 0.000892718 0.00081822334 0.0009480865 0.0011368616 0.0011482585 0.00092813431 0.0005691516 0.00021139893 -0.00021243002 -0.00069315569 -0.0011243116]]...]
INFO - root - 2017-12-09 21:35:59.814602: step 60310, loss = 0.83, batch loss = 0.70 (8.2 examples/sec; 0.972 sec/batch; 73h:28m:24s remains)
INFO - root - 2017-12-09 21:36:08.523927: step 60320, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.885 sec/batch; 66h:56m:19s remains)
INFO - root - 2017-12-09 21:36:17.030709: step 60330, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.876 sec/batch; 66h:14m:57s remains)
INFO - root - 2017-12-09 21:36:25.689505: step 60340, loss = 0.83, batch loss = 0.70 (9.1 examples/sec; 0.883 sec/batch; 66h:43m:42s remains)
INFO - root - 2017-12-09 21:36:34.227128: step 60350, loss = 0.82, batch loss = 0.69 (9.7 examples/sec; 0.823 sec/batch; 62h:13m:17s remains)
INFO - root - 2017-12-09 21:36:42.706967: step 60360, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.854 sec/batch; 64h:31m:32s remains)
INFO - root - 2017-12-09 21:36:51.395884: step 60370, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.860 sec/batch; 64h:59m:28s remains)
INFO - root - 2017-12-09 21:37:00.075875: step 60380, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.842 sec/batch; 63h:39m:15s remains)
INFO - root - 2017-12-09 21:37:08.955930: step 60390, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.882 sec/batch; 66h:38m:47s remains)
INFO - root - 2017-12-09 21:37:17.770854: step 60400, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.882 sec/batch; 66h:39m:07s remains)
2017-12-09 21:37:18.647078: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.0067473981 0.0047698 0.0027969666 0.0011533814 -9.6091535e-06 -0.00069875072 -0.0010348941 -0.0011952471 -0.0012861311 -0.0013631129 -0.0014268797 -0.0014600985 -0.0014854178 -0.0015334318 -0.0016102688][0.0065331277 0.004055596 0.0019080561 0.00040569215 -0.00042757893 -0.00076655042 -0.00084388175 -0.00087661936 -0.00094845961 -0.0010856178 -0.0012735897 -0.00145686 -0.0016049588 -0.0017040332 -0.0017657016][0.007143253 0.0040743323 0.0016498404 0.00016400788 -0.00046226208 -0.00054682943 -0.00042322 -0.00034118735 -0.00040181435 -0.00062432874 -0.0009570609 -0.0012979486 -0.0015650216 -0.0017203749 -0.0017913535][0.0084871138 0.0049084672 0.0021937015 0.00065706263 0.00019518926 0.00040366186 0.00083092053 0.001090191 0.0010111422 0.00055668957 -0.00013371347 -0.00082938978 -0.0013550322 -0.0016480959 -0.0017742069][0.010071179 0.0063328664 0.0035608169 0.0020933859 0.0018781294 0.0024657608 0.0032813344 0.003769259 0.003631474 0.0027880175 0.0015219931 0.00022491126 -0.00078388711 -0.00139312 -0.0016868823][0.011847083 0.0082319668 0.0056231171 0.0044200798 0.0045779184 0.0055785393 0.0067639644 0.0073804916 0.0070371595 0.00567656 0.0037474926 0.0017623423 0.0001579182 -0.00090093637 -0.0014840276][0.01367154 0.010229712 0.0077492571 0.006807866 0.0073186788 0.00869121 0.010152739 0.010797049 0.010225611 0.0084309541 0.0059633725 0.0034054928 0.0012529901 -0.00027809781 -0.0012068346][0.015198975 0.011779036 0.0091878306 0.0082271621 0.0088190418 0.01034174 0.011888975 0.012530629 0.011903428 0.010021038 0.0074135684 0.0046216096 0.0021456932 0.000260505 -0.00095925404][0.016373552 0.012734422 0.0098022027 0.0084977038 0.0087841228 0.010064616 0.01142886 0.012003648 0.011467454 0.0098368609 0.0075069028 0.0049007689 0.0024572844 0.00048557983 -0.00084556942][0.017106932 0.013276941 0.0099690557 0.0081396755 0.00784125 0.0085651781 0.0095068384 0.0098878592 0.0094308211 0.0081510106 0.0063017015 0.0041599185 0.0020552604 0.00029725151 -0.00091076444][0.017602338 0.013771511 0.010219493 0.0078660659 0.0068409392 0.006795634 0.0071146111 0.0071627167 0.0067066108 0.0057461904 0.0044161342 0.0028567938 0.0012719148 -8.0249039e-05 -0.0010267075][0.017919665 0.014297078 0.010717246 0.0079579912 0.0062160729 0.0053230659 0.00489378 0.0045039849 0.0039761863 0.0032836604 0.0024386859 0.0014732451 0.00046170095 -0.00043943955 -0.0011098164][0.017830532 0.014550584 0.011157322 0.0082000168 0.0059033888 0.0042609083 0.0031219786 0.0023020436 0.0016947692 0.0012411346 0.00084195903 0.00040432822 -0.00010793586 -0.00064330932 -0.0011200102][0.01679055 0.014026853 0.010998313 0.0080985371 0.0055483044 0.003464527 0.0018792053 0.00079821108 0.00016998185 -8.8141183e-05 -0.00015552994 -0.0002228393 -0.00040082855 -0.00071741082 -0.0010997131][0.014540615 0.012491877 0.010012771 0.0074345428 0.0049748425 0.002823601 0.001130361 3.0687079e-07 -0.000602287 -0.00076583074 -0.00068332744 -0.0005798057 -0.00060270762 -0.00081455067 -0.0011448276]]...]
INFO - root - 2017-12-09 21:37:27.232029: step 60410, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.863 sec/batch; 65h:13m:37s remains)
INFO - root - 2017-12-09 21:37:35.902708: step 60420, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.877 sec/batch; 66h:15m:06s remains)
INFO - root - 2017-12-09 21:37:44.545157: step 60430, loss = 0.83, batch loss = 0.70 (8.4 examples/sec; 0.955 sec/batch; 72h:08m:50s remains)
INFO - root - 2017-12-09 21:37:53.193428: step 60440, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.880 sec/batch; 66h:31m:22s remains)
INFO - root - 2017-12-09 21:38:01.916985: step 60450, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.866 sec/batch; 65h:26m:01s remains)
INFO - root - 2017-12-09 21:38:10.382509: step 60460, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.859 sec/batch; 64h:55m:47s remains)
INFO - root - 2017-12-09 21:38:18.972779: step 60470, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.850 sec/batch; 64h:14m:43s remains)
INFO - root - 2017-12-09 21:38:27.633062: step 60480, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.876 sec/batch; 66h:12m:03s remains)
INFO - root - 2017-12-09 21:38:36.217846: step 60490, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.861 sec/batch; 65h:02m:32s remains)
INFO - root - 2017-12-09 21:38:44.808793: step 60500, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.865 sec/batch; 65h:20m:47s remains)
2017-12-09 21:38:45.701961: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0018207934 -0.0018218431 -0.0018244477 -0.0018278473 -0.0018311919 -0.0018334676 -0.0018337404 -0.0018319556 -0.0018289651 -0.0018251665 -0.0018211631 -0.0018180693 -0.0018159735 -0.0018147388 -0.0018141354][-0.001818623 -0.001820021 -0.0018230029 -0.0018268517 -0.0018306809 -0.0018333032 -0.0018336916 -0.0018317073 -0.0018283367 -0.0018240991 -0.0018198149 -0.0018166573 -0.0018146229 -0.0018135121 -0.0018130258][-0.0018168457 -0.0018185554 -0.0018217097 -0.0018256295 -0.00182941 -0.0018319531 -0.0018322931 -0.0018302413 -0.0018267599 -0.0018224991 -0.0018184392 -0.0018155299 -0.0018136876 -0.0018127793 -0.0018124264][-0.0018157903 -0.0018175174 -0.0018204456 -0.0018240587 -0.0018274493 -0.0018295661 -0.0018296672 -0.0018277541 -0.0018245013 -0.0018205669 -0.0018169768 -0.0018145026 -0.001813028 -0.0018123484 -0.0018120472][-0.0018156243 -0.0018171459 -0.001819568 -0.0018224817 -0.0018250914 -0.0018265123 -0.0018263421 -0.0018247205 -0.0018220663 -0.0018188199 -0.0018158545 -0.0018138901 -0.0018128599 -0.0018123722 -0.0018120287][-0.001816079 -0.0018172842 -0.0018190708 -0.0018210271 -0.001822549 -0.0018231623 -0.0018228574 -0.0018216536 -0.0018198104 -0.0018174888 -0.0018153323 -0.0018139842 -0.0018133005 -0.0018128715 -0.0018123973][-0.0018167121 -0.0018175707 -0.001818631 -0.0018195647 -0.0018199098 -0.001819719 -0.0018193033 -0.0018186375 -0.0018178159 -0.0018167389 -0.0018157614 -0.0018151798 -0.0018147458 -0.0018141849 -0.0018134046][-0.0018176744 -0.00181792 -0.0018181126 -0.0018181299 -0.0018175899 -0.0018167496 -0.001816254 -0.0018161352 -0.0018163335 -0.0018166109 -0.0018170098 -0.0018173632 -0.0018171952 -0.0018164013 -0.001815198][-0.0018181819 -0.0018177336 -0.0018172328 -0.0018166809 -0.0018157326 -0.0018145466 -0.0018140523 -0.001814452 -0.0018155688 -0.0018170599 -0.0018185952 -0.0018197526 -0.0018198432 -0.0018188547 -0.0018171862][-0.0018181071 -0.0018170851 -0.0018162512 -0.0018155208 -0.0018145245 -0.0018132783 -0.0018128677 -0.0018135766 -0.0018151806 -0.0018173452 -0.0018195049 -0.0018212163 -0.001821607 -0.0018206525 -0.0018187964][-0.001817405 -0.0018160357 -0.0018151343 -0.0018145319 -0.0018136962 -0.0018125654 -0.001812227 -0.0018130715 -0.0018148227 -0.0018171642 -0.0018195395 -0.0018215037 -0.001822165 -0.0018214467 -0.0018197602][-0.0018163236 -0.0018147964 -0.001814094 -0.0018138035 -0.0018133444 -0.001812552 -0.0018123068 -0.001812961 -0.0018144493 -0.001816595 -0.0018188282 -0.0018207026 -0.0018215408 -0.0018212519 -0.0018200069][-0.0018155605 -0.0018139606 -0.0018134287 -0.0018134086 -0.0018133337 -0.0018129564 -0.0018128091 -0.0018131767 -0.0018141952 -0.0018159238 -0.0018177893 -0.0018193329 -0.0018202136 -0.0018204027 -0.0018196731][-0.0018151395 -0.0018136149 -0.0018132161 -0.0018133923 -0.0018136521 -0.0018136097 -0.0018134665 -0.001813475 -0.0018138958 -0.0018149812 -0.0018162827 -0.0018174358 -0.0018182781 -0.001818783 -0.0018186125][-0.0018151928 -0.0018137252 -0.0018134498 -0.0018137511 -0.0018141872 -0.0018142998 -0.0018141062 -0.0018137683 -0.001813603 -0.0018140269 -0.0018147806 -0.0018156089 -0.0018163638 -0.001817005 -0.0018173404]]...]
INFO - root - 2017-12-09 21:38:54.464279: step 60510, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 65h:06m:06s remains)
INFO - root - 2017-12-09 21:39:03.227488: step 60520, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.889 sec/batch; 67h:08m:05s remains)
INFO - root - 2017-12-09 21:39:11.720161: step 60530, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.857 sec/batch; 64h:45m:35s remains)
INFO - root - 2017-12-09 21:39:20.265371: step 60540, loss = 0.82, batch loss = 0.69 (9.8 examples/sec; 0.815 sec/batch; 61h:35m:57s remains)
INFO - root - 2017-12-09 21:39:28.794744: step 60550, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.838 sec/batch; 63h:18m:59s remains)
INFO - root - 2017-12-09 21:39:37.290325: step 60560, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.855 sec/batch; 64h:32m:57s remains)
INFO - root - 2017-12-09 21:39:45.850329: step 60570, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.886 sec/batch; 66h:55m:30s remains)
INFO - root - 2017-12-09 21:39:54.403366: step 60580, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.847 sec/batch; 63h:57m:01s remains)
INFO - root - 2017-12-09 21:40:03.136011: step 60590, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.885 sec/batch; 66h:51m:24s remains)
INFO - root - 2017-12-09 21:40:11.795439: step 60600, loss = 0.81, batch loss = 0.68 (8.9 examples/sec; 0.899 sec/batch; 67h:55m:21s remains)
2017-12-09 21:40:12.664072: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.001816795 -0.0018160639 -0.0018162341 -0.0018166242 -0.0018170563 -0.0018175426 -0.0018181871 -0.0018190807 -0.0018204399 -0.0018221769 -0.0018240374 -0.0018257364 -0.0018269623 -0.0018273956 -0.0018271205][-0.0018158498 -0.0018152248 -0.0018157308 -0.0018164606 -0.0018171944 -0.0018179296 -0.0018186425 -0.0018193731 -0.0018203057 -0.0018214986 -0.001822745 -0.0018238561 -0.0018246192 -0.0018247748 -0.0018243482][-0.0018156498 -0.0018153302 -0.0018161808 -0.001817306 -0.0018184159 -0.0018193788 -0.0018201105 -0.0018206154 -0.0018210424 -0.0018215531 -0.001822124 -0.0018226621 -0.0018230157 -0.0018229742 -0.0018224992][-0.0018155334 -0.0018155851 -0.0018168187 -0.0018183911 -0.001819975 -0.0018213139 -0.0018222115 -0.001822523 -0.0018224564 -0.0018222511 -0.0018220971 -0.0018220114 -0.0018218777 -0.0018215382 -0.0018210218][-0.0018153967 -0.0018157928 -0.0018173399 -0.0018193931 -0.0018215623 -0.0018234388 -0.0018246233 -0.0018249196 -0.001824603 -0.0018239691 -0.00182325 -0.0018225806 -0.0018219543 -0.001821294 -0.0018206043][-0.0018152669 -0.0018158504 -0.0018176261 -0.0018200008 -0.0018226458 -0.0018249963 -0.0018264175 -0.0018267623 -0.001826353 -0.0018255957 -0.0018246872 -0.0018237702 -0.0018229235 -0.0018221413 -0.0018213365][-0.0018152861 -0.0018158223 -0.001817627 -0.0018200553 -0.0018226997 -0.0018250499 -0.0018264644 -0.0018267406 -0.0018262295 -0.001825551 -0.0018249038 -0.0018243103 -0.0018237606 -0.0018231643 -0.0018224651][-0.0018152146 -0.0018156273 -0.0018172809 -0.0018194924 -0.0018218004 -0.001823821 -0.0018249727 -0.0018250003 -0.0018242707 -0.0018236452 -0.0018235258 -0.0018237191 -0.0018239381 -0.0018239209 -0.0018235988][-0.0018150095 -0.001815239 -0.0018167346 -0.0018187235 -0.0018207971 -0.0018226741 -0.0018237748 -0.001823866 -0.00182325 -0.0018226427 -0.0018226475 -0.0018231849 -0.0018237926 -0.0018241137 -0.0018240704][-0.0018145774 -0.0018147196 -0.0018160827 -0.0018179518 -0.0018199515 -0.0018218379 -0.0018231113 -0.0018236559 -0.0018235876 -0.0018233409 -0.0018232991 -0.0018235334 -0.0018238582 -0.0018239695 -0.0018237645][-0.0018143312 -0.0018142327 -0.0018153376 -0.0018169445 -0.0018186931 -0.0018204266 -0.0018217803 -0.0018226595 -0.0018230529 -0.0018232156 -0.0018233721 -0.001823407 -0.0018233371 -0.0018230786 -0.0018226017][-0.001814149 -0.0018137258 -0.0018145369 -0.0018157997 -0.0018172206 -0.0018187982 -0.0018202099 -0.0018213551 -0.0018221638 -0.001822705 -0.0018231103 -0.0018231593 -0.0018228916 -0.0018222982 -0.0018214721][-0.0018142411 -0.001813458 -0.0018139299 -0.0018148938 -0.0018161467 -0.0018176627 -0.0018191615 -0.0018205467 -0.001821629 -0.0018223273 -0.0018227231 -0.0018226677 -0.0018222565 -0.0018214824 -0.0018204136][-0.0018147266 -0.0018136225 -0.001813701 -0.001814356 -0.0018154 -0.001816739 -0.0018181755 -0.0018196302 -0.0018208759 -0.0018215909 -0.0018218462 -0.0018216621 -0.0018210963 -0.0018202073 -0.0018190544][-0.0018155668 -0.0018141828 -0.0018138139 -0.0018140625 -0.0018147929 -0.0018158935 -0.0018171751 -0.0018185483 -0.0018197939 -0.0018205534 -0.0018207703 -0.0018204831 -0.0018197655 -0.0018187562 -0.0018175605]]...]
INFO - root - 2017-12-09 21:40:21.232465: step 60610, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 65h:27m:24s remains)
INFO - root - 2017-12-09 21:40:29.837112: step 60620, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.840 sec/batch; 63h:24m:16s remains)
INFO - root - 2017-12-09 21:40:38.517576: step 60630, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.895 sec/batch; 67h:36m:15s remains)
INFO - root - 2017-12-09 21:40:46.995757: step 60640, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.849 sec/batch; 64h:07m:16s remains)
INFO - root - 2017-12-09 21:40:55.774755: step 60650, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.881 sec/batch; 66h:33m:09s remains)
INFO - root - 2017-12-09 21:41:04.466994: step 60660, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.873 sec/batch; 65h:55m:32s remains)
INFO - root - 2017-12-09 21:41:13.200412: step 60670, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 65h:34m:10s remains)
INFO - root - 2017-12-09 21:41:21.845640: step 60680, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.829 sec/batch; 62h:35m:48s remains)
INFO - root - 2017-12-09 21:41:30.488580: step 60690, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.885 sec/batch; 66h:48m:18s remains)
INFO - root - 2017-12-09 21:41:39.220720: step 60700, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.881 sec/batch; 66h:32m:29s remains)
2017-12-09 21:41:40.125292: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0017380011 -0.0017372448 -0.0017368421 -0.0017373301 -0.0017375874 -0.0017377051 -0.0017378699 -0.0017383364 -0.0017390761 -0.001739846 -0.0017407342 -0.0017409767 -0.00174097 -0.0017398561 -0.0017391921][-0.0017399231 -0.0017385236 -0.0017375944 -0.001737775 -0.0017376851 -0.0017375335 -0.0017376164 -0.0017380244 -0.0017387891 -0.0017398295 -0.0017411908 -0.0017417448 -0.0017418499 -0.0017410202 -0.001740973][-0.0017459892 -0.001743716 -0.0017418545 -0.0017410335 -0.0017402242 -0.0017395837 -0.00173931 -0.0017399675 -0.0017408289 -0.0017425655 -0.0017442785 -0.0017455685 -0.0017462084 -0.0017460858 -0.0017464752][-0.001755634 -0.0017525925 -0.001749873 -0.0017478133 -0.0017463397 -0.0017453235 -0.001744938 -0.0017459152 -0.0017470246 -0.0017488059 -0.0017506636 -0.0017520358 -0.0017529281 -0.0017532378 -0.0017541338][-0.0017685867 -0.0017645701 -0.001760971 -0.0017574554 -0.0017551773 -0.0017541691 -0.0017541739 -0.0017558974 -0.0017577022 -0.0017600061 -0.0017625386 -0.0017640984 -0.0017654246 -0.0017666236 -0.0017680693][-0.0017820428 -0.0017772452 -0.0017729112 -0.0017685167 -0.0017659875 -0.0017653675 -0.0017662585 -0.0017691398 -0.0017723404 -0.0017752456 -0.0017779302 -0.0017791719 -0.0017802968 -0.0017812661 -0.0017820825][-0.0017945993 -0.0017891185 -0.0017844404 -0.0017798281 -0.0017773421 -0.0017775925 -0.0017795586 -0.0017831437 -0.0017868998 -0.0017900085 -0.0017925007 -0.0017932635 -0.0017938203 -0.0017943298 -0.0017949276][-0.0018034199 -0.0017977125 -0.0017933488 -0.0017889685 -0.0017872395 -0.0017885498 -0.0017912665 -0.0017953679 -0.0017994182 -0.0018024918 -0.0018045799 -0.0018047396 -0.001804859 -0.0018047836 -0.0018045687][-0.0018093465 -0.0018042952 -0.0018006598 -0.0017969663 -0.0017963371 -0.0017984445 -0.0018017744 -0.001806078 -0.0018101408 -0.0018129953 -0.0018143911 -0.0018141705 -0.001813996 -0.0018135406 -0.0018128713][-0.0018130975 -0.001809051 -0.0018063349 -0.0018038746 -0.001803869 -0.0018064001 -0.0018099888 -0.0018139705 -0.0018177499 -0.0018201678 -0.0018209544 -0.0018201735 -0.0018194837 -0.001818843 -0.0018183087][-0.0018153973 -0.0018123813 -0.0018107855 -0.0018096578 -0.0018101993 -0.0018129132 -0.0018161694 -0.0018194908 -0.0018225333 -0.0018238927 -0.0018235616 -0.0018220961 -0.001820766 -0.0018195207 -0.0018185681][-0.0018175023 -0.001815246 -0.0018144635 -0.0018137571 -0.0018143712 -0.0018168014 -0.0018191347 -0.0018212516 -0.0018231947 -0.0018238735 -0.0018230322 -0.0018213455 -0.0018199815 -0.0018189448 -0.0018182865][-0.0018205597 -0.0018192515 -0.0018192679 -0.0018186697 -0.0018189764 -0.0018201311 -0.0018209489 -0.0018219671 -0.001822839 -0.0018226997 -0.0018218635 -0.0018206881 -0.0018197385 -0.0018189002 -0.0018184533][-0.0018217987 -0.0018207396 -0.0018212028 -0.0018207502 -0.0018204697 -0.001821048 -0.0018215547 -0.0018218656 -0.001821743 -0.0018212873 -0.0018205751 -0.001819385 -0.0018187515 -0.0018185998 -0.0018186236][-0.0018229381 -0.0018214012 -0.001821592 -0.0018210458 -0.0018203791 -0.0018204898 -0.0018205377 -0.0018198391 -0.0018188886 -0.001817988 -0.0018167959 -0.0018152678 -0.0018146069 -0.0018147853 -0.001815088]]...]
INFO - root - 2017-12-09 21:41:48.669372: step 60710, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.839 sec/batch; 63h:18m:45s remains)
INFO - root - 2017-12-09 21:41:57.392205: step 60720, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 65h:31m:15s remains)
INFO - root - 2017-12-09 21:42:05.858183: step 60730, loss = 0.82, batch loss = 0.69 (9.9 examples/sec; 0.811 sec/batch; 61h:14m:12s remains)
INFO - root - 2017-12-09 21:42:14.158269: step 60740, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.884 sec/batch; 66h:45m:04s remains)
INFO - root - 2017-12-09 21:42:22.715300: step 60750, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 65h:26m:13s remains)
INFO - root - 2017-12-09 21:42:31.147782: step 60760, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.888 sec/batch; 66h:59m:30s remains)
INFO - root - 2017-12-09 21:42:39.762686: step 60770, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.883 sec/batch; 66h:38m:37s remains)
INFO - root - 2017-12-09 21:42:48.466403: step 60780, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.844 sec/batch; 63h:40m:30s remains)
INFO - root - 2017-12-09 21:42:57.052412: step 60790, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 65h:04m:40s remains)
INFO - root - 2017-12-09 21:43:05.954844: step 60800, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.883 sec/batch; 66h:39m:22s remains)
2017-12-09 21:43:06.949292: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0017508994 -0.0017493452 -0.0017491378 -0.0017488591 -0.0017482697 -0.0017473234 -0.0017464069 -0.0017465641 -0.0017469005 -0.0017477871 -0.0017492796 -0.001751315 -0.0017535633 -0.0017554011 -0.0017572008][-0.0017522576 -0.0017502786 -0.0017498641 -0.0017491265 -0.0017481933 -0.0017471238 -0.0017459467 -0.0017459258 -0.0017460872 -0.001746904 -0.0017482938 -0.0017503183 -0.0017526502 -0.0017546435 -0.001756637][-0.0017545796 -0.0017516545 -0.0017507356 -0.0017495987 -0.0017483494 -0.0017473254 -0.0017463578 -0.0017464987 -0.001746826 -0.0017478077 -0.0017492489 -0.0017511953 -0.0017534763 -0.0017553931 -0.0017573194][-0.0017561119 -0.0017522168 -0.001750623 -0.0017490946 -0.0017475886 -0.0017465756 -0.0017458611 -0.0017462471 -0.0017469027 -0.0017482402 -0.0017498336 -0.0017517539 -0.0017540689 -0.0017561264 -0.0017583045][-0.0017574487 -0.0017528302 -0.0017506973 -0.0017488434 -0.0017473297 -0.0017469737 -0.0017472241 -0.0017479219 -0.0017491737 -0.0017509403 -0.0017527383 -0.0017543146 -0.0017563485 -0.0017583095 -0.0017602767][-0.001758064 -0.0017529108 -0.0017505427 -0.001748521 -0.0017470882 -0.0017472064 -0.0017483262 -0.001749634 -0.0017515192 -0.0017538504 -0.0017558888 -0.001757576 -0.0017594285 -0.0017612772 -0.0017630768][-0.0017616171 -0.0017558166 -0.0017530344 -0.0017508384 -0.0017495088 -0.0017496122 -0.0017511081 -0.0017528719 -0.0017551911 -0.0017580057 -0.0017603032 -0.0017621907 -0.0017639409 -0.0017655282 -0.0017669958][-0.0017696279 -0.0017634437 -0.001760266 -0.0017575991 -0.0017561888 -0.0017562107 -0.0017577098 -0.0017596753 -0.0017622473 -0.0017652735 -0.0017675319 -0.0017692349 -0.0017706753 -0.0017717187 -0.0017727276][-0.0017814125 -0.00177581 -0.0017726502 -0.0017696606 -0.0017678621 -0.0017676841 -0.0017689572 -0.001770679 -0.0017732127 -0.0017759934 -0.0017780112 -0.0017796186 -0.0017809115 -0.0017816351 -0.001782181][-0.0017929837 -0.0017886373 -0.0017862141 -0.0017838166 -0.0017823889 -0.0017819994 -0.0017829197 -0.0017842456 -0.0017861633 -0.0017882539 -0.0017897865 -0.0017912794 -0.001792335 -0.0017929531 -0.0017932547][-0.0018028044 -0.0017996089 -0.0017977969 -0.0017961313 -0.0017951576 -0.0017946014 -0.0017949867 -0.0017956193 -0.0017966786 -0.0017977471 -0.0017984547 -0.0017995636 -0.001800299 -0.0018008022 -0.0018011959][-0.0018104019 -0.0018082728 -0.0018071776 -0.0018060358 -0.0018053127 -0.0018048082 -0.0018047018 -0.0018047058 -0.0018051123 -0.0018054196 -0.0018054511 -0.0018059285 -0.0018062684 -0.0018064274 -0.0018066554][-0.0018145557 -0.0018134511 -0.0018129016 -0.001812421 -0.0018121777 -0.001812219 -0.0018119521 -0.0018118451 -0.0018114733 -0.0018110258 -0.0018104026 -0.0018101773 -0.001810257 -0.0018100286 -0.0018101274][-0.0018163195 -0.0018157226 -0.0018157096 -0.0018154548 -0.0018152894 -0.0018149819 -0.0018143222 -0.0018135261 -0.0018123097 -0.0018113061 -0.0018102264 -0.0018098672 -0.001809814 -0.0018097882 -0.0018099921][-0.0018136448 -0.0018129833 -0.0018131005 -0.0018134144 -0.0018136689 -0.0018136003 -0.0018131527 -0.0018122487 -0.0018108642 -0.0018092915 -0.0018073559 -0.0018063426 -0.0018056236 -0.0018055299 -0.0018058054]]...]
INFO - root - 2017-12-09 21:43:15.553513: step 60810, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.874 sec/batch; 65h:55m:22s remains)
INFO - root - 2017-12-09 21:43:24.276415: step 60820, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.865 sec/batch; 65h:16m:48s remains)
INFO - root - 2017-12-09 21:43:33.136054: step 60830, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.890 sec/batch; 67h:08m:04s remains)
INFO - root - 2017-12-09 21:43:41.780964: step 60840, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 65h:02m:50s remains)
INFO - root - 2017-12-09 21:43:50.371397: step 60850, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.879 sec/batch; 66h:20m:38s remains)
INFO - root - 2017-12-09 21:43:58.918550: step 60860, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 65h:01m:58s remains)
INFO - root - 2017-12-09 21:44:07.546630: step 60870, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.879 sec/batch; 66h:17m:35s remains)
INFO - root - 2017-12-09 21:44:16.287077: step 60880, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.856 sec/batch; 64h:33m:09s remains)
INFO - root - 2017-12-09 21:44:25.053379: step 60890, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.865 sec/batch; 65h:15m:20s remains)
INFO - root - 2017-12-09 21:44:33.716691: step 60900, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.870 sec/batch; 65h:38m:20s remains)
2017-12-09 21:44:34.537769: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.0071175792 0.01106535 0.016406316 0.021892885 0.026734175 0.029727209 0.030357361 0.028716838 0.025094064 0.020447522 0.015389302 0.010751633 0.0068049631 0.0038234261 0.0016882514][0.012716498 0.019089399 0.027636016 0.036774334 0.045228891 0.051084436 0.05333196 0.051643122 0.046361294 0.038608424 0.029566471 0.020726278 0.013021017 0.0071836761 0.0031300294][0.02017444 0.029756989 0.042212296 0.055713817 0.068384141 0.077725343 0.082222924 0.081075326 0.074533485 0.063733645 0.050445765 0.036761057 0.024247676 0.014162484 0.0068594445][0.029574245 0.043038152 0.059883855 0.078050666 0.0950406 0.10781519 0.11435044 0.11350799 0.1054742 0.091565356 0.074120507 0.055657245 0.038289979 0.023700679 0.012616834][0.039412152 0.056910414 0.077955008 0.10058336 0.12185923 0.13835047 0.14742464 0.14739503 0.13819988 0.12126625 0.0996059 0.07628838 0.053960994 0.03471354 0.019631179][0.047257815 0.068354249 0.092764445 0.11867504 0.14286891 0.16198012 0.17309561 0.17434737 0.16527653 0.1471405 0.12311988 0.09640418 0.070119381 0.046622507 0.027524][0.051601544 0.074967414 0.10140035 0.12907796 0.15466259 0.17491147 0.18691477 0.18899605 0.18061513 0.16302571 0.13929872 0.11225731 0.084609605 0.058538981 0.036201157][0.053286236 0.07703355 0.10346814 0.13099879 0.15640759 0.17653996 0.18863063 0.19125107 0.18392152 0.16793628 0.14623043 0.12118957 0.094779253 0.068655185 0.045072902][0.051910531 0.07494466 0.10022087 0.12631883 0.15031758 0.16923943 0.18064436 0.18333845 0.17719266 0.16351514 0.14492634 0.12330178 0.099882588 0.075867958 0.053159282][0.046976924 0.068345033 0.091618471 0.11551281 0.13736933 0.15449385 0.16484967 0.16759796 0.1629993 0.15215768 0.13724071 0.11961343 0.10007108 0.07939864 0.059110988][0.039008837 0.057596084 0.077943519 0.0987823 0.11770986 0.13240047 0.14130259 0.144021 0.14117441 0.13357717 0.12283346 0.10976876 0.094866611 0.07856378 0.061923094][0.030023988 0.044843838 0.061260037 0.078139544 0.093531504 0.10545139 0.11275527 0.1153715 0.11402026 0.10938909 0.10261011 0.094129264 0.084115312 0.07267148 0.060439214][0.021343654 0.032371078 0.044594008 0.057081342 0.068457231 0.077288933 0.082873993 0.085236534 0.084986031 0.082618371 0.078855276 0.074042924 0.068270393 0.061455674 0.053801365][0.012968523 0.020540278 0.028956018 0.037440307 0.045096468 0.051046651 0.054964054 0.056935068 0.057402968 0.056652948 0.055012438 0.052750554 0.04994522 0.046561982 0.042593][0.0056691631 0.010055775 0.015027939 0.020075412 0.024644721 0.028268179 0.030823261 0.032413006 0.033283625 0.03356348 0.033353504 0.032827266 0.031988576 0.0308231 0.029268183]]...]
INFO - root - 2017-12-09 21:44:43.158972: step 60910, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.875 sec/batch; 66h:01m:41s remains)
INFO - root - 2017-12-09 21:44:51.808260: step 60920, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.836 sec/batch; 63h:05m:39s remains)
INFO - root - 2017-12-09 21:45:00.594968: step 60930, loss = 0.82, batch loss = 0.69 (8.2 examples/sec; 0.976 sec/batch; 73h:36m:33s remains)
INFO - root - 2017-12-09 21:45:09.167850: step 60940, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.866 sec/batch; 65h:17m:35s remains)
INFO - root - 2017-12-09 21:45:17.945505: step 60950, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.873 sec/batch; 65h:50m:35s remains)
INFO - root - 2017-12-09 21:45:26.324662: step 60960, loss = 0.82, batch loss = 0.69 (9.9 examples/sec; 0.810 sec/batch; 61h:03m:33s remains)
INFO - root - 2017-12-09 21:45:34.830771: step 60970, loss = 0.82, batch loss = 0.69 (9.9 examples/sec; 0.811 sec/batch; 61h:10m:26s remains)
INFO - root - 2017-12-09 21:45:43.618697: step 60980, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.878 sec/batch; 66h:11m:49s remains)
INFO - root - 2017-12-09 21:45:52.353255: step 60990, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 65h:11m:32s remains)
INFO - root - 2017-12-09 21:46:01.050615: step 61000, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.860 sec/batch; 64h:50m:37s remains)
2017-12-09 21:46:01.934764: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0018287977 -0.0018289766 -0.0018296727 -0.0018303331 -0.0018308378 -0.0018309071 -0.0018304953 -0.0018297061 -0.0018286889 -0.0018276621 -0.0018265168 -0.0018256779 -0.0018250074 -0.0018243903 -0.0018237646][-0.0018231021 -0.0018233712 -0.0018246946 -0.0018262332 -0.0018277833 -0.0018290234 -0.0018296781 -0.0018297454 -0.0018293689 -0.0018286181 -0.001827516 -0.0018266262 -0.0018258366 -0.0018250346 -0.0018241396][-0.0018150101 -0.001815237 -0.001816994 -0.0018193737 -0.0018223334 -0.001825025 -0.0018272125 -0.0018287252 -0.0018294437 -0.0018293815 -0.0018286625 -0.0018277456 -0.0018268229 -0.001825868 -0.001824743][-0.0018071842 -0.0018072313 -0.0018094527 -0.0018124603 -0.0018164117 -0.001820455 -0.0018240602 -0.0018268947 -0.0018287126 -0.0018295337 -0.001829265 -0.0018283371 -0.0018272899 -0.001826142 -0.0018248649][-0.0017993156 -0.0017995823 -0.0018024475 -0.0018061495 -0.00181102 -0.0018160664 -0.0018208342 -0.0018247729 -0.0018276254 -0.0018291232 -0.0018292626 -0.0018282258 -0.0018269594 -0.0018257261 -0.0018244406][-0.001792206 -0.0017927084 -0.0017958373 -0.0018000214 -0.0018055186 -0.0018113556 -0.0018167305 -0.0018218308 -0.0018259513 -0.0018282479 -0.0018289814 -0.001828253 -0.0018273961 -0.0018261145 -0.0018250274][-0.0017880831 -0.0017886361 -0.0017918108 -0.0017958501 -0.0018010385 -0.0018071063 -0.0018131124 -0.0018186769 -0.0018233156 -0.0018262732 -0.0018277066 -0.0018273478 -0.0018264584 -0.0018255978 -0.001825062][-0.0017880823 -0.0017883159 -0.0017910993 -0.0017942897 -0.001798687 -0.0018045263 -0.0018105903 -0.0018166141 -0.0018217663 -0.0018249997 -0.0018268146 -0.001826955 -0.0018260799 -0.0018251879 -0.00182461][-0.0017921835 -0.0017921391 -0.0017941751 -0.0017960736 -0.0017992434 -0.0018041446 -0.0018099496 -0.0018158238 -0.0018209222 -0.0018244053 -0.0018265012 -0.0018267845 -0.0018261077 -0.0018252576 -0.0018247728][-0.0017989654 -0.0017988243 -0.0018000256 -0.0018009685 -0.0018030361 -0.001806922 -0.0018119649 -0.0018171414 -0.0018216255 -0.001824683 -0.0018264377 -0.0018268954 -0.0018262081 -0.0018254739 -0.0018251205][-0.0018073864 -0.0018068773 -0.0018076906 -0.0018079994 -0.001809111 -0.0018118856 -0.0018157222 -0.0018198506 -0.0018232713 -0.0018255502 -0.0018267392 -0.0018268308 -0.0018262469 -0.0018255798 -0.0018251906][-0.0018150966 -0.0018142223 -0.0018147473 -0.0018146922 -0.0018153312 -0.0018171954 -0.001820225 -0.0018229824 -0.0018248935 -0.0018260949 -0.0018262794 -0.0018257711 -0.0018248892 -0.0018242348 -0.0018239428][-0.0018203661 -0.0018191916 -0.00181959 -0.0018195622 -0.0018198638 -0.0018211005 -0.0018231793 -0.0018252253 -0.0018262686 -0.0018265959 -0.0018261512 -0.0018253213 -0.001824391 -0.0018236203 -0.0018232322][-0.0018238245 -0.0018225373 -0.0018228039 -0.0018229443 -0.0018232951 -0.0018240167 -0.0018254028 -0.0018263817 -0.0018264297 -0.0018264392 -0.0018258626 -0.0018249765 -0.0018241117 -0.0018234061 -0.0018230323][-0.001826231 -0.0018249003 -0.0018248674 -0.0018248876 -0.0018249927 -0.0018255799 -0.0018264995 -0.0018267749 -0.0018263982 -0.0018260344 -0.0018253545 -0.0018245747 -0.001823712 -0.0018233457 -0.0018231408]]...]
INFO - root - 2017-12-09 21:46:10.641708: step 61010, loss = 0.81, batch loss = 0.68 (9.2 examples/sec; 0.866 sec/batch; 65h:19m:23s remains)
INFO - root - 2017-12-09 21:46:19.449056: step 61020, loss = 0.83, batch loss = 0.70 (8.1 examples/sec; 0.989 sec/batch; 74h:34m:20s remains)
INFO - root - 2017-12-09 21:46:27.980261: step 61030, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 65h:30m:53s remains)
INFO - root - 2017-12-09 21:46:36.537051: step 61040, loss = 0.82, batch loss = 0.69 (10.9 examples/sec; 0.731 sec/batch; 55h:08m:59s remains)
INFO - root - 2017-12-09 21:46:45.234246: step 61050, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.876 sec/batch; 66h:01m:37s remains)
INFO - root - 2017-12-09 21:46:53.685664: step 61060, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.851 sec/batch; 64h:10m:23s remains)
INFO - root - 2017-12-09 21:47:02.279708: step 61070, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.838 sec/batch; 63h:10m:58s remains)
INFO - root - 2017-12-09 21:47:10.792979: step 61080, loss = 0.82, batch loss = 0.69 (9.7 examples/sec; 0.821 sec/batch; 61h:54m:44s remains)
INFO - root - 2017-12-09 21:47:19.208140: step 61090, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.863 sec/batch; 65h:03m:17s remains)
INFO - root - 2017-12-09 21:47:27.856102: step 61100, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.897 sec/batch; 67h:35m:58s remains)
2017-12-09 21:47:28.742341: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.45477006 0.44583541 0.43529171 0.42540663 0.41855451 0.41223061 0.40530726 0.39853165 0.39200687 0.38346818 0.37563059 0.37056789 0.3681469 0.36735281 0.36828122][0.45986098 0.45419461 0.44611594 0.4378112 0.43085718 0.42347345 0.41453257 0.40748182 0.40113008 0.39175424 0.38340226 0.37753633 0.37454236 0.37303549 0.37298405][0.45911005 0.4588497 0.45482174 0.44886202 0.44226369 0.43358004 0.42290807 0.41402304 0.4062756 0.39624202 0.38796535 0.38178167 0.37830147 0.37591347 0.37491879][0.46040067 0.46565172 0.46682557 0.46475625 0.45980483 0.45093143 0.43920428 0.42758805 0.41640019 0.40496537 0.39603594 0.38950208 0.38481286 0.3818768 0.38085842][0.46520811 0.47521839 0.48015919 0.4817051 0.47896203 0.47056973 0.45810208 0.44474596 0.43052736 0.4169232 0.406283 0.39855963 0.393014 0.38928983 0.38806617][0.47340712 0.48714429 0.49495938 0.49946272 0.49846739 0.49108461 0.47918802 0.46447545 0.44843593 0.43283644 0.42039749 0.41105911 0.40423259 0.40012619 0.39846724][0.48177245 0.49852681 0.50734067 0.51361924 0.51405138 0.5071584 0.49551097 0.47995663 0.46349886 0.44796029 0.43580329 0.42598417 0.41861713 0.41425362 0.41173419][0.484664 0.50446486 0.51336586 0.5196262 0.52020454 0.51396519 0.5031271 0.48758295 0.47146335 0.45704797 0.44562319 0.43566 0.42798117 0.42323044 0.42017409][0.4846774 0.50509107 0.51370293 0.51966089 0.52082932 0.51585078 0.50660175 0.49261418 0.4773964 0.46387246 0.45261258 0.44222215 0.4338502 0.42846903 0.42470819][0.47474277 0.49504855 0.50216132 0.50782168 0.50990534 0.50713551 0.5006395 0.48909524 0.47618121 0.46334925 0.45190656 0.44076285 0.43222234 0.42707205 0.42305326][0.45760015 0.4771156 0.48287281 0.48777571 0.49066162 0.4907009 0.4873994 0.47861698 0.46873984 0.4582577 0.44829273 0.43706411 0.42830384 0.42355227 0.41949582][0.43655619 0.4561795 0.4616527 0.46677491 0.47139663 0.47374839 0.47276184 0.4668746 0.45905009 0.45031381 0.4414432 0.430981 0.42278779 0.41779825 0.41379705][0.41543284 0.43464339 0.44088462 0.44735727 0.45466855 0.46015629 0.46208611 0.45842943 0.4518896 0.44349802 0.43425643 0.42380297 0.41547242 0.41056892 0.40702268][0.39778054 0.41642162 0.42385095 0.43282378 0.44334409 0.45234433 0.45744327 0.45616135 0.450486 0.44206706 0.43220863 0.42144039 0.41266066 0.40751711 0.40414467][0.38458994 0.40288493 0.41071543 0.42176467 0.43476331 0.44645852 0.45394766 0.45458078 0.4500007 0.4414289 0.43085313 0.41975397 0.41057503 0.40514952 0.40187362]]...]
INFO - root - 2017-12-09 21:47:37.275006: step 61110, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.858 sec/batch; 64h:39m:10s remains)
INFO - root - 2017-12-09 21:47:45.788460: step 61120, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.850 sec/batch; 64h:03m:42s remains)
INFO - root - 2017-12-09 21:47:54.304803: step 61130, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.858 sec/batch; 64h:39m:55s remains)
INFO - root - 2017-12-09 21:48:02.805561: step 61140, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.877 sec/batch; 66h:07m:27s remains)
INFO - root - 2017-12-09 21:48:11.310726: step 61150, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.873 sec/batch; 65h:47m:03s remains)
INFO - root - 2017-12-09 21:48:19.653595: step 61160, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.853 sec/batch; 64h:16m:11s remains)
INFO - root - 2017-12-09 21:48:28.173673: step 61170, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.849 sec/batch; 64h:00m:23s remains)
INFO - root - 2017-12-09 21:48:36.751964: step 61180, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 65h:07m:33s remains)
INFO - root - 2017-12-09 21:48:45.396966: step 61190, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.894 sec/batch; 67h:22m:24s remains)
INFO - root - 2017-12-09 21:48:53.996285: step 61200, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.840 sec/batch; 63h:19m:14s remains)
2017-12-09 21:48:54.829115: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0017637905 -0.0017578371 -0.0017557355 -0.0017566123 -0.0017599392 -0.0017659292 -0.0017748125 -0.0017848689 -0.0017941264 -0.0018015532 -0.0018059795 -0.0018075232 -0.0018063958 -0.0018051696 -0.0018041367][-0.0017681472 -0.0017625197 -0.0017608914 -0.0017623114 -0.0017660336 -0.0017719357 -0.0017802019 -0.0017888373 -0.0017963373 -0.0018022513 -0.0018053115 -0.0018057243 -0.0018039519 -0.0018025712 -0.0018016329][-0.0017791365 -0.0017739632 -0.0017722888 -0.0017737778 -0.0017771977 -0.0017823346 -0.0017892485 -0.0017959967 -0.0018013081 -0.0018052177 -0.0018067575 -0.0018059815 -0.0018034974 -0.0018013071 -0.0017998915][-0.0017913731 -0.0017867472 -0.0017850171 -0.001786042 -0.0017884939 -0.001792189 -0.0017973003 -0.0018020335 -0.0018052715 -0.0018071862 -0.0018072997 -0.0018057789 -0.0018028296 -0.001800327 -0.0017987662][-0.0018012729 -0.001797488 -0.0017958579 -0.0017965084 -0.0017979632 -0.0017997244 -0.0018026214 -0.0018053968 -0.0018070656 -0.0018076004 -0.0018068824 -0.001805125 -0.0018022822 -0.0017995849 -0.0017978165][-0.001807102 -0.001804092 -0.0018027188 -0.0018031984 -0.0018039652 -0.001804018 -0.0018048009 -0.0018056949 -0.0018061446 -0.0018059541 -0.0018051011 -0.001803535 -0.0018013205 -0.00179897 -0.0017972135][-0.0018089144 -0.0018068991 -0.0018061943 -0.0018065685 -0.0018068211 -0.0018059443 -0.0018052688 -0.001804826 -0.0018046586 -0.0018044348 -0.0018037067 -0.0018024247 -0.0018007839 -0.0017987645 -0.0017971169][-0.001806804 -0.0018051506 -0.0018047022 -0.001805232 -0.0018054869 -0.0018044418 -0.0018035326 -0.0018028581 -0.0018029769 -0.0018027851 -0.0018023158 -0.0018014972 -0.0018001858 -0.0017985282 -0.0017970416][-0.0018024467 -0.0018010968 -0.0018010173 -0.0018020957 -0.0018028186 -0.0018021803 -0.0018012164 -0.0018007713 -0.0018014758 -0.0018017815 -0.0018016625 -0.0018010944 -0.0018002469 -0.0017988298 -0.0017973947][-0.0017984774 -0.0017973428 -0.001797776 -0.0017992784 -0.0018004067 -0.0018004405 -0.0017998833 -0.0017993854 -0.0017999703 -0.0018005215 -0.0018006983 -0.0017999844 -0.0017992776 -0.0017982202 -0.0017970559][-0.0017963609 -0.0017952046 -0.0017957364 -0.0017972502 -0.0017986286 -0.0017991153 -0.0017989762 -0.0017988315 -0.0017993283 -0.0017999529 -0.0018003167 -0.0017995997 -0.0017986908 -0.001797564 -0.0017965791][-0.0017956231 -0.0017944288 -0.0017950046 -0.0017964742 -0.0017979045 -0.0017988663 -0.0017989635 -0.0017989208 -0.0017993877 -0.001800142 -0.0018006744 -0.0018001336 -0.0017993138 -0.0017981235 -0.0017971138][-0.0017956485 -0.0017939932 -0.001794041 -0.0017952013 -0.001796389 -0.0017973969 -0.0017979345 -0.0017983631 -0.0017991349 -0.0018000398 -0.0018007762 -0.0018003761 -0.0017994615 -0.0017983214 -0.0017972427][-0.0017961212 -0.0017941128 -0.0017936761 -0.0017941308 -0.0017949599 -0.0017958647 -0.0017964651 -0.0017971682 -0.0017982088 -0.0017995231 -0.001800463 -0.0018002961 -0.0017996663 -0.0017985173 -0.0017973295][-0.0017965186 -0.0017945542 -0.0017938345 -0.001793712 -0.0017940706 -0.0017947716 -0.0017952959 -0.0017958703 -0.0017968826 -0.0017984032 -0.001799517 -0.0017996428 -0.0017993141 -0.001798481 -0.0017974812]]...]
INFO - root - 2017-12-09 21:49:03.336797: step 61210, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.852 sec/batch; 64h:14m:27s remains)
INFO - root - 2017-12-09 21:49:11.942048: step 61220, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.856 sec/batch; 64h:30m:23s remains)
INFO - root - 2017-12-09 21:49:20.459007: step 61230, loss = 0.81, batch loss = 0.68 (9.3 examples/sec; 0.857 sec/batch; 64h:33m:36s remains)
INFO - root - 2017-12-09 21:49:29.149662: step 61240, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.853 sec/batch; 64h:17m:19s remains)
INFO - root - 2017-12-09 21:49:37.559029: step 61250, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 64h:56m:17s remains)
INFO - root - 2017-12-09 21:49:46.017065: step 61260, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.876 sec/batch; 66h:00m:20s remains)
INFO - root - 2017-12-09 21:49:54.589892: step 61270, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.866 sec/batch; 65h:12m:41s remains)
INFO - root - 2017-12-09 21:50:03.215568: step 61280, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.879 sec/batch; 66h:13m:16s remains)
INFO - root - 2017-12-09 21:50:11.917853: step 61290, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.886 sec/batch; 66h:45m:24s remains)
INFO - root - 2017-12-09 21:50:20.634200: step 61300, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.875 sec/batch; 65h:54m:44s remains)
2017-12-09 21:50:21.519901: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.0018567567 0.0023715878 0.0025972351 0.0025644372 0.0023499052 0.002008786 0.001607407 0.0012008491 0.00084105262 0.00053931877 0.00024784438 -0.00010248681 -0.00053577416 -0.00099627138 -0.0013938892][0.0034088073 0.004244349 0.0046703178 0.00472589 0.0045065838 0.0041184938 0.0036388095 0.0031255186 0.0026082462 0.002095961 0.0015452235 0.00090436067 0.000166376 -0.00056009763 -0.0011542554][0.0059235836 0.0072894064 0.0080398722 0.0081674093 0.0078004566 0.0071391533 0.0063344887 0.0055354508 0.0048106122 0.0041554249 0.0034558768 0.0025888877 0.0015150901 0.00035757909 -0.00066328817][0.0090080332 0.010863735 0.011898329 0.01216084 0.011820213 0.011102171 0.010146879 0.0091294758 0.008121998 0.0071425042 0.00607957 0.0047882427 0.0032320442 0.001562446 7.8679877e-05][0.011389288 0.013353234 0.014371063 0.014633012 0.014413423 0.013974317 0.013408079 0.012781612 0.012058868 0.011137405 0.0098493965 0.0080416966 0.0057464112 0.0032612253 0.0010512801][0.011863816 0.013712531 0.014637222 0.014947255 0.015000138 0.015051263 0.015111172 0.015114713 0.014895237 0.014249916 0.012992726 0.01095283 0.0082134344 0.0051120003 0.0022464315][0.010037514 0.011743432 0.012674172 0.013175595 0.013600517 0.014178232 0.014871719 0.015473261 0.01567203 0.015235023 0.01402182 0.011921094 0.0090709534 0.0058231116 0.0027967286][0.0066183168 0.0080996314 0.00913242 0.0099898875 0.010935592 0.01215427 0.013508251 0.014674566 0.015240838 0.01492142 0.013629343 0.011410926 0.00851661 0.0053441087 0.0024710288][0.0028781332 0.00393615 0.0049059922 0.0059857029 0.0073863552 0.0092013916 0.011187861 0.012889865 0.013783921 0.013552693 0.012142793 0.0097725876 0.0068648993 0.0039253449 0.0014401724][0.00026082026 0.00081651087 0.00146949 0.0024173302 0.0038730674 0.0059030582 0.0082386965 0.010308376 0.011452774 0.011301197 0.0098491656 0.007477486 0.0047470885 0.002211866 0.00025742978][-0.0011387815 -0.00086464477 -0.00045890477 0.00025381183 0.0015012393 0.0033749584 0.0056379214 0.0077284533 0.0089770742 0.0089315809 0.00755066 0.0053134151 0.002895487 0.00082562293 -0.00062383409][-0.0015800234 -0.0014492942 -0.0011889093 -0.00066148257 0.00033239496 0.0018965135 0.003829719 0.0056537832 0.0067681815 0.0067572328 0.005578917 0.0036603864 0.0016554693 9.2826085e-06 -0.0010755616][-0.001695452 -0.0016057414 -0.0014283204 -0.0010280409 -0.00024797244 0.00096774858 0.0024455469 0.0038434062 0.0046970877 0.0046885372 0.0037910244 0.0023321803 0.00082040334 -0.00042368867 -0.0012508046][-0.0017936545 -0.0017590271 -0.0016623932 -0.0014136888 -0.00090387429 -9.03802e-05 0.00088878151 0.0017968105 0.002341785 0.0023130029 0.0017008957 0.00075260655 -0.00020054786 -0.00098029186 -0.0014906586][-0.0018167846 -0.0018177034 -0.0017944119 -0.0016943921 -0.0014596701 -0.0010645548 -0.00056805764 -8.2569779e-05 0.00022926263 0.00024342851 -4.5583467e-05 -0.00051912887 -0.0010019362 -0.0013972819 -0.0016555359]]...]
INFO - root - 2017-12-09 21:50:30.031114: step 61310, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.855 sec/batch; 64h:22m:53s remains)
INFO - root - 2017-12-09 21:50:38.704839: step 61320, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.857 sec/batch; 64h:34m:05s remains)
INFO - root - 2017-12-09 21:50:47.240524: step 61330, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.870 sec/batch; 65h:32m:05s remains)
INFO - root - 2017-12-09 21:50:55.915687: step 61340, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.843 sec/batch; 63h:29m:07s remains)
INFO - root - 2017-12-09 21:51:04.354676: step 61350, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.861 sec/batch; 64h:52m:35s remains)
INFO - root - 2017-12-09 21:51:12.874728: step 61360, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.854 sec/batch; 64h:17m:21s remains)
INFO - root - 2017-12-09 21:51:21.433584: step 61370, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.866 sec/batch; 65h:15m:15s remains)
INFO - root - 2017-12-09 21:51:30.129300: step 61380, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.875 sec/batch; 65h:55m:22s remains)
INFO - root - 2017-12-09 21:51:38.931157: step 61390, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.880 sec/batch; 66h:17m:27s remains)
INFO - root - 2017-12-09 21:51:47.579548: step 61400, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 65h:21m:24s remains)
2017-12-09 21:51:48.526974: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.056346871 0.059858631 0.062449366 0.064085364 0.064551368 0.064056814 0.0627364 0.0610255 0.059139945 0.057143904 0.0555105 0.053972118 0.052403215 0.050556179 0.048961636][0.057095055 0.060827762 0.063565053 0.065162122 0.065525182 0.064792804 0.063308433 0.06146951 0.059467882 0.057456043 0.055669546 0.054060657 0.052416138 0.050748352 0.049300592][0.057975177 0.061798915 0.064554341 0.066109248 0.066384763 0.065489769 0.06395974 0.062201932 0.060288619 0.058187228 0.056263909 0.054604158 0.052868504 0.051205277 0.049810383][0.059060194 0.063074738 0.065867655 0.067372233 0.067540348 0.066645592 0.064993776 0.06328097 0.06153395 0.059654407 0.05779288 0.056028202 0.054425005 0.052764434 0.051299162][0.059988867 0.064368255 0.067405872 0.0690997 0.069349848 0.068447754 0.066759318 0.06501963 0.063089937 0.061318725 0.059675176 0.058145802 0.056719787 0.055201273 0.053966474][0.059709437 0.064496405 0.067911208 0.069977641 0.070548154 0.069936849 0.06834276 0.066653267 0.064808428 0.0631417 0.061520785 0.060208097 0.059127882 0.057875469 0.056834262][0.05755702 0.062674984 0.066428505 0.068957984 0.070063464 0.069834873 0.068584979 0.06708356 0.065444157 0.064109564 0.062861212 0.061834924 0.061139904 0.060279828 0.05946463][0.053528331 0.058954451 0.063093543 0.066126846 0.067811713 0.068152569 0.067417838 0.066300325 0.065024085 0.064025477 0.06293951 0.062167086 0.06172996 0.06113676 0.060504939][0.047887035 0.053496152 0.057979006 0.061629139 0.064010784 0.06505008 0.065001823 0.064378738 0.063541584 0.062910408 0.06210795 0.061413467 0.061097369 0.060735472 0.060077831][0.041156664 0.046717163 0.0513708 0.055509988 0.058587186 0.060383588 0.061137959 0.061206251 0.060850427 0.060428135 0.059791837 0.059316639 0.059041239 0.058762815 0.058167361][0.034650788 0.039771356 0.044282313 0.048575897 0.052068975 0.054462697 0.055914085 0.056596864 0.056795545 0.056700438 0.056338053 0.055968858 0.055761248 0.055545673 0.05489371][0.029164502 0.033519756 0.037513163 0.041527644 0.04502866 0.047673855 0.049525362 0.050664227 0.051201131 0.05128333 0.05115746 0.051008847 0.050892182 0.050815493 0.050499894][0.024982924 0.02838151 0.031540424 0.034857526 0.037911795 0.040419295 0.042267121 0.043547254 0.044281293 0.044558521 0.044614177 0.044577274 0.044576388 0.044611979 0.044606511][0.021667674 0.024091674 0.026263639 0.028657675 0.030961506 0.032975305 0.03452 0.035679925 0.036409624 0.03677386 0.036926951 0.037024256 0.037134115 0.037338197 0.037628304][0.018955624 0.020563655 0.021823116 0.023322873 0.024814591 0.026175676 0.027218103 0.028032213 0.028593479 0.028952211 0.029097095 0.029218649 0.029408764 0.029629331 0.03004786]]...]
INFO - root - 2017-12-09 21:51:57.221314: step 61410, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.854 sec/batch; 64h:17m:05s remains)
INFO - root - 2017-12-09 21:52:05.940652: step 61420, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.886 sec/batch; 66h:44m:21s remains)
INFO - root - 2017-12-09 21:52:14.547964: step 61430, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.844 sec/batch; 63h:33m:56s remains)
INFO - root - 2017-12-09 21:52:23.216067: step 61440, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.847 sec/batch; 63h:45m:10s remains)
INFO - root - 2017-12-09 21:52:31.660934: step 61450, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.873 sec/batch; 65h:42m:35s remains)
INFO - root - 2017-12-09 21:52:40.260635: step 61460, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.866 sec/batch; 65h:13m:50s remains)
INFO - root - 2017-12-09 21:52:48.780395: step 61470, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 64h:55m:08s remains)
INFO - root - 2017-12-09 21:52:57.383942: step 61480, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.881 sec/batch; 66h:21m:14s remains)
INFO - root - 2017-12-09 21:53:06.006091: step 61490, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 65h:26m:17s remains)
INFO - root - 2017-12-09 21:53:14.584948: step 61500, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.875 sec/batch; 65h:52m:19s remains)
2017-12-09 21:53:15.425751: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.001824885 -0.0018228457 -0.0018216795 -0.0018179731 -0.00180604 -0.0017891916 -0.0017774311 -0.0017773526 -0.0017884271 -0.0018034813 -0.001815193 -0.0018201626 -0.001820959 -0.0018208631 -0.0018208077][-0.0018242579 -0.0018190647 -0.0018048694 -0.0017655612 -0.001691859 -0.0016049028 -0.0015496574 -0.0015580195 -0.0016216417 -0.0017058692 -0.001774099 -0.0018091542 -0.0018193393 -0.0018204408 -0.0018204985][-0.0018188885 -0.0017976587 -0.0017393772 -0.0016044598 -0.0013755708 -0.0011172559 -0.00095437089 -0.00097909116 -0.0011745221 -0.0014372523 -0.0016553927 -0.0017750041 -0.0018150611 -0.0018208877 -0.0018210978][-0.0018030757 -0.0017418845 -0.0015890379 -0.0012715769 -0.00075717643 -0.00017555279 0.00020631577 0.00017854513 -0.00025089504 -0.00085857045 -0.0013845481 -0.0016892212 -0.0018006484 -0.0018207526 -0.0018214877][-0.0017785374 -0.0016580732 -0.001374206 -0.00081667211 6.9988775e-05 0.0010977046 0.0018341254 0.0018909363 0.0012092168 0.00013647194 -0.00085980305 -0.0014848961 -0.0017450035 -0.0018099883 -0.0018190552][-0.0017569328 -0.0015835663 -0.0011753633 -0.00037363777 0.00091578125 0.0024754344 0.0037165419 0.004019774 0.003179275 0.0016182916 3.0892435e-05 -0.0010642515 -0.0015895442 -0.0017643393 -0.001807418][-0.0017466554 -0.0015488896 -0.0010693445 -9.6010859e-05 0.0015215302 0.0035817744 0.0053930213 0.006107368 0.0053024376 0.0033838851 0.0012241878 -0.00041423482 -0.0013064067 -0.00166781 -0.0017820338][-0.0017540911 -0.0015682195 -0.0010969593 -9.5659168e-05 0.0016365676 0.0039611286 0.0061906823 0.0073513514 0.0068039461 0.0048257569 0.0023400751 0.00027836941 -0.00096643472 -0.0015403354 -0.0017476653][-0.0017729135 -0.0016278049 -0.0012404774 -0.00037326454 0.0012018 0.0034349388 0.0057518724 0.0072060493 0.0070515354 0.0053597172 0.0029399749 0.00075054204 -0.00069261459 -0.0014247725 -0.0017152122][-0.0017932209 -0.0016990083 -0.0014367332 -0.000815763 0.00038184354 0.0021926113 0.004226381 0.005707196 0.0059154066 0.004743054 0.0027643824 0.00080352917 -0.0005988176 -0.0013689334 -0.0016979247][-0.0018094247 -0.0017597935 -0.0016156342 -0.0012480372 -0.00048117852 0.00076948141 0.0022925041 0.0035444619 0.0039387308 0.0032810951 0.0019062987 0.00041534391 -0.00073119719 -0.0014022676 -0.0017052393][-0.0018192515 -0.0017986519 -0.0017363769 -0.0015607688 -0.0011579938 -0.00044523971 0.00049174519 0.0013420357 0.0017162705 0.0014318851 0.0006413887 -0.00029165018 -0.0010520848 -0.0015179273 -0.0017356023][-0.001823584 -0.0018173467 -0.0017980539 -0.0017350912 -0.0015725131 -0.0012587761 -0.00081424043 -0.00037585851 -0.00014116068 -0.0002268831 -0.00058298139 -0.0010352943 -0.001418185 -0.0016581105 -0.0017724897][-0.0018231594 -0.0018211864 -0.0018177993 -0.001803882 -0.0017608418 -0.0016670087 -0.0015207544 -0.0013632305 -0.0012654045 -0.0012775421 -0.0013901972 -0.0015424653 -0.001674067 -0.0017576822 -0.0017992835][-0.0018221565 -0.0018205636 -0.0018199952 -0.0018191822 -0.0018152699 -0.0018028511 -0.0017780006 -0.0017460055 -0.001721709 -0.0017186339 -0.0017369975 -0.0017646755 -0.0017891232 -0.0018052057 -0.0018144441]]...]
INFO - root - 2017-12-09 21:53:24.190296: step 61510, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.861 sec/batch; 64h:47m:38s remains)
INFO - root - 2017-12-09 21:53:32.870558: step 61520, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.865 sec/batch; 65h:06m:11s remains)
INFO - root - 2017-12-09 21:53:41.397993: step 61530, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.874 sec/batch; 65h:46m:23s remains)
INFO - root - 2017-12-09 21:53:50.034135: step 61540, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 65h:21m:28s remains)
INFO - root - 2017-12-09 21:53:58.603549: step 61550, loss = 0.82, batch loss = 0.69 (10.0 examples/sec; 0.801 sec/batch; 60h:18m:46s remains)
INFO - root - 2017-12-09 21:54:07.318625: step 61560, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.872 sec/batch; 65h:37m:55s remains)
INFO - root - 2017-12-09 21:54:16.079952: step 61570, loss = 0.82, batch loss = 0.69 (9.7 examples/sec; 0.825 sec/batch; 62h:05m:14s remains)
INFO - root - 2017-12-09 21:54:24.761487: step 61580, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.861 sec/batch; 64h:45m:47s remains)
INFO - root - 2017-12-09 21:54:33.522784: step 61590, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.860 sec/batch; 64h:43m:27s remains)
INFO - root - 2017-12-09 21:54:42.373939: step 61600, loss = 0.82, batch loss = 0.69 (8.8 examples/sec; 0.912 sec/batch; 68h:37m:45s remains)
2017-12-09 21:54:43.283185: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0018303646 -0.0018321644 -0.0018343533 -0.0018359724 -0.0018368893 -0.0018375467 -0.0018382535 -0.0018390437 -0.0018397925 -0.0018400088 -0.0018392341 -0.0018374298 -0.0018346329 -0.001831234 -0.0018277864][-0.0018324958 -0.001833589 -0.0018347944 -0.0018353636 -0.0018354099 -0.001835298 -0.0018354264 -0.0018358584 -0.0018365062 -0.0018368586 -0.0018364561 -0.0018351949 -0.0018330823 -0.0018303829 -0.0018274897][-0.0018349455 -0.0018354342 -0.0018357048 -0.0018353144 -0.0018344524 -0.0018334645 -0.001832925 -0.0018328464 -0.0018331987 -0.001833625 -0.0018336242 -0.0018330611 -0.0018318834 -0.0018302375 -0.0018283071][-0.0018366779 -0.0018366495 -0.0018362064 -0.0018350132 -0.00183336 -0.0018317992 -0.0018308251 -0.0018304409 -0.0018306223 -0.0018311753 -0.0018316882 -0.0018319607 -0.0018317539 -0.0018312137 -0.0018302095][-0.0018375948 -0.0018370334 -0.001836175 -0.0018345844 -0.0018325364 -0.0018306618 -0.0018295333 -0.0018290784 -0.0018292234 -0.001829861 -0.0018307995 -0.0018317082 -0.0018323261 -0.0018326567 -0.0018324025][-0.0018384163 -0.0018376083 -0.0018364264 -0.0018347229 -0.0018327975 -0.0018310117 -0.0018299944 -0.0018296038 -0.0018296551 -0.001830176 -0.0018311418 -0.0018322321 -0.0018331755 -0.0018339141 -0.0018341477][-0.0018388611 -0.0018381072 -0.0018369916 -0.0018356087 -0.0018341417 -0.0018328273 -0.0018321287 -0.0018318897 -0.0018319391 -0.0018322276 -0.0018328243 -0.0018335918 -0.0018343111 -0.0018348492 -0.0018349272][-0.001838814 -0.0018384708 -0.00183801 -0.0018374213 -0.0018367327 -0.0018359964 -0.0018355268 -0.0018354022 -0.0018353987 -0.001835437 -0.0018356638 -0.0018360406 -0.0018362383 -0.0018361888 -0.001835714][-0.001838092 -0.0018381689 -0.0018382424 -0.0018383209 -0.0018384734 -0.0018385643 -0.0018386963 -0.0018389388 -0.001839058 -0.0018389944 -0.0018388893 -0.0018387147 -0.001838324 -0.0018376801 -0.001836689][-0.0018364354 -0.0018364193 -0.0018366213 -0.0018370118 -0.0018376278 -0.0018382953 -0.0018390062 -0.001839708 -0.0018401329 -0.0018401787 -0.0018400009 -0.0018396052 -0.00183902 -0.0018382337 -0.0018371514][-0.0018343417 -0.0018345015 -0.0018349 -0.0018354398 -0.0018361306 -0.0018369617 -0.0018378537 -0.001838565 -0.0018389995 -0.0018390226 -0.0018388 -0.0018383384 -0.0018377245 -0.0018369994 -0.001836073][-0.0018315242 -0.0018318739 -0.001832467 -0.0018331221 -0.0018339405 -0.0018347766 -0.0018355208 -0.001836104 -0.0018364564 -0.0018362802 -0.0018359135 -0.00183541 -0.0018348739 -0.0018342276 -0.0018334892][-0.0018283169 -0.0018288963 -0.0018297982 -0.0018306813 -0.0018314605 -0.0018320575 -0.0018324339 -0.001832705 -0.0018327183 -0.0018323665 -0.0018319879 -0.0018315507 -0.0018311474 -0.0018306611 -0.0018301286][-0.0018252743 -0.0018257801 -0.0018265961 -0.0018274109 -0.001828183 -0.0018287025 -0.0018289568 -0.0018290482 -0.0018289469 -0.0018286069 -0.0018282229 -0.0018278317 -0.0018274904 -0.0018271067 -0.0018267767][-0.0018230925 -0.0018231656 -0.001823712 -0.0018243784 -0.0018250525 -0.0018255168 -0.0018257499 -0.0018258566 -0.0018258397 -0.0018256861 -0.0018255095 -0.0018252622 -0.0018250078 -0.0018247163 -0.0018244256]]...]
INFO - root - 2017-12-09 21:54:51.946098: step 61610, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.881 sec/batch; 66h:16m:24s remains)
INFO - root - 2017-12-09 21:55:00.567794: step 61620, loss = 0.83, batch loss = 0.70 (9.1 examples/sec; 0.876 sec/batch; 65h:52m:44s remains)
INFO - root - 2017-12-09 21:55:09.159701: step 61630, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.882 sec/batch; 66h:22m:30s remains)
INFO - root - 2017-12-09 21:55:17.879801: step 61640, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.883 sec/batch; 66h:25m:52s remains)
INFO - root - 2017-12-09 21:55:26.486553: step 61650, loss = 0.82, batch loss = 0.69 (9.7 examples/sec; 0.821 sec/batch; 61h:45m:29s remains)
INFO - root - 2017-12-09 21:55:35.063916: step 61660, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.849 sec/batch; 63h:52m:33s remains)
INFO - root - 2017-12-09 21:55:43.723868: step 61670, loss = 0.82, batch loss = 0.69 (9.7 examples/sec; 0.826 sec/batch; 62h:10m:07s remains)
INFO - root - 2017-12-09 21:55:52.202211: step 61680, loss = 0.82, batch loss = 0.69 (9.8 examples/sec; 0.819 sec/batch; 61h:35m:53s remains)
INFO - root - 2017-12-09 21:56:00.888171: step 61690, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.875 sec/batch; 65h:48m:52s remains)
INFO - root - 2017-12-09 21:56:09.528419: step 61700, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.872 sec/batch; 65h:37m:17s remains)
2017-12-09 21:56:10.384346: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.11146893 0.13169041 0.14863728 0.16121042 0.16662133 0.16349947 0.15243948 0.13502073 0.11511629 0.096077144 0.081249073 0.071060568 0.065029316 0.060870774 0.05641561][0.10868859 0.12849897 0.14491856 0.157518 0.16363272 0.16207908 0.15331124 0.13803679 0.11960779 0.10079242 0.085121967 0.073439211 0.065829642 0.060289681 0.054761834][0.098306827 0.11624894 0.13077486 0.141891 0.14731358 0.14647844 0.13960533 0.12701955 0.11143584 0.094447531 0.07939592 0.067343295 0.059012126 0.052643903 0.046534579][0.082406804 0.0977331 0.10985423 0.11866715 0.12245429 0.12119459 0.11529487 0.10527253 0.092919566 0.078970268 0.0661025 0.055178232 0.047228735 0.041045245 0.035331972][0.062389404 0.074522041 0.083932318 0.090361893 0.092782728 0.091215447 0.086225152 0.07853388 0.069462053 0.059283528 0.049645532 0.041139178 0.0347323 0.029683255 0.024849083][0.0427977 0.051560078 0.05835348 0.062813863 0.064395554 0.0630943 0.059519108 0.05427045 0.048504036 0.042044695 0.035802662 0.030140545 0.025712714 0.022302648 0.018738685][0.027437214 0.033231139 0.037727471 0.040749628 0.042226952 0.042070314 0.040477447 0.037978627 0.035385348 0.032376233 0.029308025 0.026410719 0.023955802 0.022201564 0.019957958][0.019627446 0.023624375 0.026850544 0.029274181 0.031076234 0.032111663 0.03244964 0.032353215 0.032409213 0.032396205 0.032060482 0.031671792 0.031056441 0.030919526 0.029831421][0.019922225 0.024027823 0.027405038 0.030251302 0.033064 0.035389613 0.037177302 0.038570028 0.040328342 0.042361684 0.043820359 0.045196708 0.045879282 0.047181197 0.047019266][0.024829492 0.030383423 0.035139449 0.03948475 0.04381451 0.047616292 0.050759777 0.053305157 0.056101877 0.059082132 0.061361387 0.063577458 0.065037 0.067117631 0.067237861][0.0303001 0.037714366 0.044291425 0.050439127 0.056525502 0.061991252 0.0665331 0.070188068 0.073714927 0.077122316 0.079683967 0.082030833 0.083632685 0.085900031 0.08598952][0.032140214 0.040671673 0.048505146 0.056035917 0.063582167 0.070553184 0.076419741 0.081213638 0.085614949 0.089507744 0.092176445 0.094318226 0.0959787 0.098136768 0.097938679][0.02909426 0.037410002 0.045311734 0.053151987 0.061269149 0.06907329 0.075884126 0.081710339 0.087229922 0.0918642 0.094977409 0.097077563 0.098768726 0.10063972 0.1001119][0.021903021 0.028694034 0.035346918 0.042167097 0.049567517 0.057069536 0.063849866 0.07003922 0.076173283 0.08157032 0.085623935 0.088453539 0.090648986 0.092408195 0.091890447][0.013213023 0.017789133 0.022463366 0.027495032 0.033259723 0.0393684 0.045142252 0.0506804 0.056470096 0.062028371 0.066622652 0.070128955 0.072964214 0.0750118 0.07492321]]...]
INFO - root - 2017-12-09 21:56:18.965751: step 61710, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.858 sec/batch; 64h:31m:34s remains)
INFO - root - 2017-12-09 21:56:27.711616: step 61720, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.850 sec/batch; 63h:57m:13s remains)
INFO - root - 2017-12-09 21:56:36.191436: step 61730, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.836 sec/batch; 62h:52m:52s remains)
INFO - root - 2017-12-09 21:56:44.756322: step 61740, loss = 0.82, batch loss = 0.69 (9.7 examples/sec; 0.828 sec/batch; 62h:17m:02s remains)
INFO - root - 2017-12-09 21:56:53.353172: step 61750, loss = 0.82, batch loss = 0.69 (9.7 examples/sec; 0.828 sec/batch; 62h:14m:33s remains)
INFO - root - 2017-12-09 21:57:01.823814: step 61760, loss = 0.82, batch loss = 0.69 (10.2 examples/sec; 0.783 sec/batch; 58h:54m:26s remains)
INFO - root - 2017-12-09 21:57:10.338815: step 61770, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.894 sec/batch; 67h:11m:48s remains)
INFO - root - 2017-12-09 21:57:19.006426: step 61780, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.876 sec/batch; 65h:54m:33s remains)
INFO - root - 2017-12-09 21:57:27.751459: step 61790, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.866 sec/batch; 65h:05m:34s remains)
INFO - root - 2017-12-09 21:57:36.419789: step 61800, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.847 sec/batch; 63h:39m:23s remains)
2017-12-09 21:57:37.327792: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0018299919 -0.0018325708 -0.001834481 -0.0018353852 -0.0018352849 -0.001834398 -0.0018335339 -0.001832358 -0.0018310731 -0.0018296369 -0.0018282298 -0.0018272287 -0.0018269019 -0.0018272174 -0.0018282485][-0.0018295854 -0.0018323071 -0.0018344882 -0.0018354984 -0.0018353657 -0.0018345409 -0.0018337417 -0.0018324439 -0.0018309031 -0.0018291095 -0.001827254 -0.0018256907 -0.0018249 -0.0018250658 -0.0018262491][-0.0018290487 -0.0018319977 -0.0018344178 -0.00183581 -0.0018359838 -0.0018352953 -0.0018345686 -0.0018333147 -0.0018318576 -0.0018300328 -0.001828093 -0.0018262344 -0.0018249489 -0.0018247103 -0.0018255154][-0.0018287217 -0.0018317611 -0.0018344339 -0.0018361268 -0.0018366417 -0.0018362489 -0.0018356963 -0.0018345884 -0.0018333038 -0.0018316197 -0.001829781 -0.0018277575 -0.0018259945 -0.001825164 -0.0018252337][-0.0018280813 -0.0018312124 -0.0018340875 -0.0018362701 -0.0018373274 -0.0018375396 -0.0018374975 -0.0018367879 -0.0018357856 -0.0018343984 -0.0018327576 -0.0018307054 -0.0018284919 -0.0018268136 -0.0018258131][-0.0018272308 -0.0018301789 -0.0018330738 -0.001835441 -0.0018370164 -0.0018378909 -0.0018385767 -0.0018387102 -0.0018383535 -0.0018374969 -0.001836164 -0.0018340306 -0.0018313284 -0.0018287497 -0.001826611][-0.0018253435 -0.0018277655 -0.0018303152 -0.0018327733 -0.0018347345 -0.0018363449 -0.0018377786 -0.0018386751 -0.0018389991 -0.0018385838 -0.0018375253 -0.001835588 -0.0018328623 -0.0018300958 -0.0018273896][-0.0018228443 -0.001824407 -0.0018264044 -0.0018285287 -0.0018308065 -0.0018330629 -0.0018351191 -0.0018367342 -0.0018377033 -0.0018376089 -0.0018366196 -0.0018348853 -0.0018324055 -0.0018298511 -0.0018271977][-0.0018196639 -0.0018204206 -0.0018219098 -0.0018236819 -0.0018259292 -0.001828516 -0.0018310661 -0.0018332133 -0.0018346425 -0.0018348129 -0.001833927 -0.0018322617 -0.0018299817 -0.0018276218 -0.0018252812][-0.0018163234 -0.0018161997 -0.0018172495 -0.0018188093 -0.0018210097 -0.001823645 -0.001826269 -0.0018284684 -0.0018299404 -0.0018301947 -0.001829498 -0.0018279543 -0.0018260414 -0.0018241776 -0.0018223403][-0.0018132526 -0.0018124275 -0.0018130558 -0.0018141918 -0.0018160712 -0.0018184693 -0.001820888 -0.0018228287 -0.0018241189 -0.0018243464 -0.0018237863 -0.0018224884 -0.0018210727 -0.0018198903 -0.0018188164][-0.0018112585 -0.0018099888 -0.0018100415 -0.0018106401 -0.0018119432 -0.0018138214 -0.0018157471 -0.0018172855 -0.0018182982 -0.0018184413 -0.0018179861 -0.0018170143 -0.00181619 -0.0018156198 -0.0018152687][-0.0018106429 -0.0018090684 -0.0018087031 -0.0018087535 -0.0018093749 -0.0018105364 -0.0018118203 -0.0018128675 -0.0018135773 -0.0018137201 -0.0018134851 -0.0018128614 -0.0018123835 -0.0018120313 -0.0018119351][-0.0018102669 -0.0018084972 -0.0018078136 -0.0018074977 -0.0018075362 -0.0018080381 -0.0018086901 -0.001809199 -0.0018096293 -0.0018098759 -0.0018100296 -0.0018098421 -0.0018096995 -0.0018094216 -0.0018091575][-0.0018100117 -0.0018080221 -0.0018070699 -0.0018064436 -0.0018061416 -0.0018061319 -0.0018062954 -0.0018064531 -0.0018066223 -0.001806854 -0.0018071736 -0.0018072861 -0.0018073774 -0.0018072309 -0.0018069416]]...]
INFO - root - 2017-12-09 21:57:45.982439: step 61810, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.901 sec/batch; 67h:45m:48s remains)
INFO - root - 2017-12-09 21:57:54.671437: step 61820, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.877 sec/batch; 65h:57m:25s remains)
INFO - root - 2017-12-09 21:58:03.186460: step 61830, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.843 sec/batch; 63h:21m:19s remains)
INFO - root - 2017-12-09 21:58:11.838823: step 61840, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.877 sec/batch; 65h:57m:35s remains)
INFO - root - 2017-12-09 21:58:20.457394: step 61850, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.842 sec/batch; 63h:18m:41s remains)
INFO - root - 2017-12-09 21:58:28.902677: step 61860, loss = 0.82, batch loss = 0.69 (10.7 examples/sec; 0.747 sec/batch; 56h:10m:11s remains)
INFO - root - 2017-12-09 21:58:37.531215: step 61870, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.882 sec/batch; 66h:18m:51s remains)
INFO - root - 2017-12-09 21:58:46.198527: step 61880, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 64h:59m:09s remains)
INFO - root - 2017-12-09 21:58:54.809289: step 61890, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.840 sec/batch; 63h:07m:24s remains)
INFO - root - 2017-12-09 21:59:03.378170: step 61900, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.875 sec/batch; 65h:48m:10s remains)
2017-12-09 21:59:04.263041: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.001796241 -0.0017831634 -0.0017626642 -0.0017118409 -0.0016053643 -0.0014421387 -0.0012581983 -0.001102628 -0.00099441269 -0.00093028031 -0.00092742074 -0.0010228357 -0.0012246952 -0.001471865 -0.0016744974][-0.0018010944 -0.0017892657 -0.0017721477 -0.001733248 -0.0016487456 -0.0015072996 -0.0013311859 -0.0011664042 -0.0010429239 -0.000970437 -0.00096991687 -0.0010726453 -0.0012753988 -0.001512449 -0.0016986182][-0.0018119698 -0.0018054079 -0.0017962856 -0.0017714683 -0.0017113078 -0.0016017386 -0.0014524311 -0.0012983117 -0.001171867 -0.0010947799 -0.0010929422 -0.001191552 -0.0013768728 -0.0015822127 -0.0017346489][-0.0018205565 -0.0018160202 -0.0018109976 -0.001797176 -0.0017570814 -0.0016729499 -0.0015445765 -0.0013977035 -0.0012692006 -0.0011916193 -0.0011933113 -0.0012904154 -0.0014596095 -0.0016365 -0.0017608571][-0.0018225816 -0.0018212756 -0.0018202807 -0.0018136309 -0.0017897609 -0.0017328692 -0.0016334178 -0.0015019374 -0.0013722252 -0.0012885437 -0.0012874799 -0.0013787786 -0.0015309935 -0.001681366 -0.0017818483][-0.0018228351 -0.0018210409 -0.0018203181 -0.0018182852 -0.0018074079 -0.0017749772 -0.0017070998 -0.0015995844 -0.0014748819 -0.0013835543 -0.0013737733 -0.0014557597 -0.0015919044 -0.0017193467 -0.001799113][-0.0018234686 -0.0018207334 -0.0018181922 -0.0018172155 -0.0018157666 -0.001804453 -0.0017672966 -0.0016899693 -0.0015805687 -0.0014858358 -0.0014635966 -0.001528318 -0.0016433191 -0.0017486974 -0.0018114515][-0.0018244763 -0.0018223104 -0.0018206333 -0.0018195946 -0.0018186595 -0.0018133096 -0.001793293 -0.0017412052 -0.0016533002 -0.0015654601 -0.0015359696 -0.0015844062 -0.0016791892 -0.0017669055 -0.0018183726][-0.001825363 -0.0018231787 -0.0018224641 -0.0018227185 -0.0018227358 -0.0018202561 -0.0018081797 -0.0017724209 -0.0017061565 -0.0016343222 -0.0016060219 -0.0016411213 -0.0017146629 -0.0017834412 -0.0018235839][-0.0018267573 -0.0018241403 -0.0018229277 -0.0018231209 -0.0018241828 -0.0018238365 -0.0018172095 -0.0017938373 -0.0017486702 -0.0016987056 -0.0016783793 -0.0017030375 -0.0017545538 -0.0018019135 -0.0018289771][-0.0018281246 -0.0018253461 -0.0018236581 -0.0018231161 -0.001824004 -0.0018251658 -0.0018237192 -0.0018129005 -0.0017891154 -0.0017611668 -0.0017498432 -0.0017647137 -0.0017944431 -0.0018208878 -0.0018352406][-0.0018291428 -0.0018263169 -0.0018242243 -0.001823107 -0.00182332 -0.0018247495 -0.0018264119 -0.00182482 -0.0018173748 -0.001806603 -0.0018016578 -0.0018083599 -0.0018216874 -0.0018332286 -0.0018391467][-0.0018295191 -0.0018269229 -0.0018248699 -0.0018234267 -0.0018230804 -0.0018241877 -0.0018267154 -0.0018292379 -0.0018305104 -0.0018298849 -0.0018298562 -0.0018321659 -0.0018362631 -0.0018397671 -0.0018411936][-0.0018295429 -0.0018272953 -0.0018254331 -0.0018239371 -0.0018233204 -0.001823996 -0.0018261393 -0.0018290487 -0.0018321092 -0.0018344057 -0.0018364182 -0.0018382404 -0.0018401626 -0.0018416763 -0.0018419758][-0.0018288261 -0.0018271732 -0.0018257439 -0.0018244502 -0.0018236873 -0.0018240862 -0.0018258455 -0.0018284065 -0.0018313458 -0.0018339966 -0.0018364653 -0.0018384288 -0.0018402225 -0.0018417826 -0.0018423406]]...]
INFO - root - 2017-12-09 21:59:12.840063: step 61910, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.890 sec/batch; 66h:54m:06s remains)
INFO - root - 2017-12-09 21:59:21.627275: step 61920, loss = 0.82, batch loss = 0.69 (8.8 examples/sec; 0.907 sec/batch; 68h:08m:16s remains)
INFO - root - 2017-12-09 21:59:30.122006: step 61930, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.840 sec/batch; 63h:09m:48s remains)
INFO - root - 2017-12-09 21:59:38.882997: step 61940, loss = 0.82, batch loss = 0.69 (8.2 examples/sec; 0.977 sec/batch; 73h:26m:08s remains)
INFO - root - 2017-12-09 21:59:47.631857: step 61950, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 65h:18m:51s remains)
INFO - root - 2017-12-09 21:59:56.247559: step 61960, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.872 sec/batch; 65h:30m:32s remains)
INFO - root - 2017-12-09 22:00:04.784269: step 61970, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.874 sec/batch; 65h:42m:07s remains)
INFO - root - 2017-12-09 22:00:13.583708: step 61980, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.889 sec/batch; 66h:48m:59s remains)
INFO - root - 2017-12-09 22:00:22.387092: step 61990, loss = 0.82, batch loss = 0.69 (8.8 examples/sec; 0.914 sec/batch; 68h:40m:36s remains)
INFO - root - 2017-12-09 22:00:30.998900: step 62000, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 65h:14m:26s remains)
2017-12-09 22:00:31.922826: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0018247067 -0.001824168 -0.0018242712 -0.0018248077 -0.0018254302 -0.0018258261 -0.0018257669 -0.0018253245 -0.0018247139 -0.0018244846 -0.0018245081 -0.0018244236 -0.0018242793 -0.0018242638 -0.0018242151][-0.0018242855 -0.0018236613 -0.0018237031 -0.0018242546 -0.0018249518 -0.0018254491 -0.0018255182 -0.0018252138 -0.0018246776 -0.0018244586 -0.0018244316 -0.0018242934 -0.0018241408 -0.0018241287 -0.0018240439][-0.0018241503 -0.001823428 -0.0018233294 -0.001823843 -0.0018245428 -0.0018250794 -0.0018252826 -0.0018251027 -0.0018246362 -0.0018245147 -0.0018245891 -0.0018245251 -0.0018244253 -0.001824388 -0.001824219][-0.0018240359 -0.0018233302 -0.0018231451 -0.0018235461 -0.001824185 -0.0018247303 -0.0018250078 -0.001824983 -0.0018245874 -0.0018245869 -0.0018247884 -0.0018248182 -0.0018247176 -0.0018246026 -0.001824328][-0.0018240542 -0.001823583 -0.0018235559 -0.0018240334 -0.0018247678 -0.0018253931 -0.0018256916 -0.001825712 -0.0018253081 -0.0018251307 -0.001825162 -0.0018250915 -0.0018248415 -0.0018246588 -0.0018243102][-0.0018245231 -0.0018243211 -0.0018245898 -0.0018253926 -0.001826386 -0.0018269811 -0.0018271009 -0.0018269996 -0.0018265592 -0.0018261013 -0.0018257726 -0.001825431 -0.001824938 -0.0018246411 -0.0018242135][-0.0018254907 -0.0018255638 -0.0018261317 -0.0018272924 -0.0018285355 -0.0018290168 -0.0018287996 -0.0018284468 -0.0018279001 -0.0018271226 -0.0018264113 -0.0018257375 -0.0018250324 -0.0018246148 -0.0018241028][-0.0018273613 -0.0018276358 -0.0018284952 -0.0018299476 -0.001831254 -0.0018315957 -0.0018310491 -0.0018302522 -0.0018293073 -0.0018280762 -0.0018268441 -0.0018257943 -0.0018249992 -0.0018245419 -0.0018239814][-0.0018299144 -0.0018304844 -0.0018315872 -0.0018330762 -0.0018342214 -0.001834447 -0.0018336227 -0.0018321368 -0.0018304422 -0.0018286825 -0.0018269722 -0.0018255934 -0.001824741 -0.0018243708 -0.0018238726][-0.0018332996 -0.0018341057 -0.0018351648 -0.0018363923 -0.0018371197 -0.0018369706 -0.0018357034 -0.0018335141 -0.001831069 -0.0018288714 -0.0018268612 -0.0018253196 -0.0018244834 -0.0018242426 -0.0018238258][-0.0018370928 -0.0018379705 -0.0018387118 -0.001839326 -0.0018393628 -0.0018385808 -0.0018367324 -0.0018339903 -0.0018310401 -0.0018286116 -0.001826592 -0.0018251368 -0.001824413 -0.0018242314 -0.0018238442][-0.0018405037 -0.0018412301 -0.0018415434 -0.0018414096 -0.0018407358 -0.0018393181 -0.0018369448 -0.0018338738 -0.0018306441 -0.0018281202 -0.0018262126 -0.0018249498 -0.0018244097 -0.0018242558 -0.0018238636][-0.0018428864 -0.0018432151 -0.0018428988 -0.001842187 -0.0018411052 -0.0018392981 -0.0018366439 -0.0018334177 -0.0018300686 -0.0018274798 -0.001825751 -0.0018247728 -0.001824409 -0.0018242521 -0.0018238472][-0.0018437429 -0.0018434433 -0.0018425571 -0.0018416092 -0.001840429 -0.001838518 -0.0018358082 -0.0018325953 -0.0018292594 -0.001826682 -0.0018251735 -0.0018245317 -0.0018243709 -0.0018241934 -0.0018237932][-0.0018430001 -0.0018420191 -0.0018407702 -0.001839734 -0.0018385694 -0.001836836 -0.0018343369 -0.0018312953 -0.0018282033 -0.0018259113 -0.001824687 -0.0018243162 -0.0018243537 -0.0018241596 -0.0018237487]]...]
INFO - root - 2017-12-09 22:00:40.526201: step 62010, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.892 sec/batch; 67h:01m:26s remains)
INFO - root - 2017-12-09 22:00:49.140563: step 62020, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.847 sec/batch; 63h:37m:56s remains)
INFO - root - 2017-12-09 22:00:57.819740: step 62030, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.889 sec/batch; 66h:47m:39s remains)
INFO - root - 2017-12-09 22:01:06.604973: step 62040, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.889 sec/batch; 66h:47m:21s remains)
INFO - root - 2017-12-09 22:01:15.349768: step 62050, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.878 sec/batch; 65h:57m:11s remains)
INFO - root - 2017-12-09 22:01:23.913053: step 62060, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 64h:54m:03s remains)
INFO - root - 2017-12-09 22:01:32.482720: step 62070, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.898 sec/batch; 67h:28m:22s remains)
INFO - root - 2017-12-09 22:01:41.363822: step 62080, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.865 sec/batch; 64h:59m:31s remains)
INFO - root - 2017-12-09 22:01:50.108467: step 62090, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.894 sec/batch; 67h:09m:53s remains)
INFO - root - 2017-12-09 22:01:58.820239: step 62100, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.859 sec/batch; 64h:31m:57s remains)
2017-12-09 22:01:59.682988: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0018385024 -0.0018377691 -0.0018371457 -0.0018366477 -0.0018362635 -0.0018360238 -0.0018359834 -0.0018359951 -0.0018361532 -0.0018364363 -0.0018367644 -0.0018370935 -0.0018372789 -0.0018373569 -0.0018373462][-0.001839961 -0.0018391988 -0.0018385679 -0.0018380657 -0.0018376646 -0.001837501 -0.0018375054 -0.0018375719 -0.0018376844 -0.0018379845 -0.0018383005 -0.0018385636 -0.0018386876 -0.0018387268 -0.0018386396][-0.0018413353 -0.0018404986 -0.0018399283 -0.0018394159 -0.0018388162 -0.001838176 -0.0018376462 -0.0018372757 -0.0018369784 -0.0018369327 -0.0018371502 -0.0018374297 -0.001837817 -0.0018382354 -0.0018386042][-0.0018403158 -0.001839834 -0.0018397792 -0.0018396428 -0.0018392096 -0.0018385303 -0.0018376827 -0.0018366873 -0.0018356257 -0.0018344636 -0.0018334764 -0.0018328772 -0.0018330334 -0.0018340561 -0.0018355998][-0.0018378444 -0.0018376439 -0.0018380817 -0.0018384914 -0.0018384815 -0.0018378677 -0.001836777 -0.0018351205 -0.0018333232 -0.0018315301 -0.0018299813 -0.0018286485 -0.0018279013 -0.0018284548 -0.0018302214][-0.0018364282 -0.001836411 -0.0018367536 -0.0018369263 -0.0018363806 -0.0018350836 -0.0018330063 -0.0018302583 -0.0018274141 -0.0018249002 -0.0018228559 -0.0018214949 -0.0018214005 -0.0018228493 -0.0018254617][-0.0018294741 -0.0018299856 -0.001831444 -0.0018325774 -0.001832305 -0.0018303393 -0.0018268708 -0.001822404 -0.0018175871 -0.0018132813 -0.0018097226 -0.0018076898 -0.0018078733 -0.0018105346 -0.0018152739][-0.0018183135 -0.0018192456 -0.0018216129 -0.0018240497 -0.0018247757 -0.0018231093 -0.0018192704 -0.001813805 -0.0018074003 -0.0018007184 -0.0017945196 -0.0017900731 -0.0017893004 -0.0017931312 -0.0018005533][-0.0018060358 -0.0018066396 -0.0018104493 -0.0018150312 -0.0018173422 -0.001816233 -0.0018121242 -0.0018057552 -0.0017979841 -0.0017895023 -0.0017811449 -0.0017747328 -0.0017725363 -0.0017764311 -0.0017857493][-0.0018016447 -0.0018005578 -0.0018037618 -0.0018087616 -0.0018123302 -0.0018124924 -0.0018090289 -0.0018025142 -0.0017939907 -0.0017843061 -0.0017746589 -0.0017673475 -0.0017648046 -0.0017685654 -0.0017780182][-0.0018019496 -0.0017994831 -0.0018019436 -0.0018066334 -0.001810412 -0.0018115169 -0.0018093474 -0.0018039759 -0.0017957803 -0.0017854302 -0.0017743108 -0.0017656612 -0.0017621614 -0.0017652605 -0.0017744149][-0.0017964744 -0.0017941453 -0.0017974728 -0.0018034495 -0.0018088443 -0.0018117048 -0.0018112911 -0.0018077549 -0.0018010641 -0.0017917082 -0.0017814072 -0.0017728531 -0.0017689531 -0.0017711571 -0.0017791775][-0.001781665 -0.0017781747 -0.001781772 -0.0017897498 -0.0017981506 -0.0018048326 -0.0018084372 -0.0018085836 -0.0018046221 -0.001797131 -0.001788564 -0.0017817293 -0.0017791102 -0.0017817143 -0.0017890593][-0.001761302 -0.0017566842 -0.0017600692 -0.001769111 -0.0017802049 -0.0017904531 -0.0017979621 -0.0018019237 -0.0018014426 -0.0017969056 -0.0017909993 -0.001786295 -0.0017852866 -0.0017885739 -0.0017954825][-0.0017386151 -0.0017330481 -0.0017360512 -0.00174566 -0.0017584125 -0.0017712326 -0.0017818029 -0.0017887136 -0.0017907294 -0.0017882823 -0.0017842266 -0.0017812606 -0.0017814309 -0.0017847024 -0.0017908221]]...]
INFO - root - 2017-12-09 22:02:08.526035: step 62110, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.844 sec/batch; 63h:23m:44s remains)
INFO - root - 2017-12-09 22:02:17.263766: step 62120, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.899 sec/batch; 67h:31m:47s remains)
INFO - root - 2017-12-09 22:02:25.984004: step 62130, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.883 sec/batch; 66h:19m:52s remains)
INFO - root - 2017-12-09 22:02:34.726567: step 62140, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.895 sec/batch; 67h:11m:03s remains)
INFO - root - 2017-12-09 22:02:43.423185: step 62150, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.835 sec/batch; 62h:42m:30s remains)
INFO - root - 2017-12-09 22:02:51.946031: step 62160, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.839 sec/batch; 62h:59m:34s remains)
INFO - root - 2017-12-09 22:03:00.516967: step 62170, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.878 sec/batch; 65h:55m:35s remains)
INFO - root - 2017-12-09 22:03:09.205334: step 62180, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.878 sec/batch; 65h:56m:56s remains)
INFO - root - 2017-12-09 22:03:17.905445: step 62190, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 65h:15m:53s remains)
INFO - root - 2017-12-09 22:03:26.607056: step 62200, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.881 sec/batch; 66h:07m:53s remains)
2017-12-09 22:03:27.503541: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0017971552 -0.0017950595 -0.0017944928 -0.0017942322 -0.0017942643 -0.0017945239 -0.0017951044 -0.0017958597 -0.0017964953 -0.0017969145 -0.0017969948 -0.0017969375 -0.0017968335 -0.0017967235 -0.0017966254][-0.0017954799 -0.0017931963 -0.0017925492 -0.00179239 -0.0017927289 -0.00179337 -0.0017942622 -0.0017951917 -0.001795868 -0.0017962712 -0.0017962857 -0.0017961935 -0.0017960542 -0.0017958953 -0.0017957583][-0.0017948917 -0.0017925734 -0.0017919181 -0.0017919247 -0.0017924862 -0.0017933412 -0.0017943322 -0.0017952786 -0.0017958952 -0.0017962079 -0.0017961786 -0.0017960952 -0.0017959781 -0.0017958416 -0.0017956689][-0.0017948311 -0.0017926982 -0.0017922212 -0.0017924479 -0.0017931563 -0.0017940516 -0.0017949053 -0.0017956314 -0.0017960261 -0.0017961756 -0.0017960998 -0.0017960595 -0.00179601 -0.0017959028 -0.0017957087][-0.0017954899 -0.0017935913 -0.0017933265 -0.001793731 -0.001794461 -0.0017951983 -0.0017957068 -0.0017960542 -0.0017961647 -0.001796113 -0.0017959807 -0.0017960059 -0.0017960297 -0.0017959425 -0.0017957399][-0.0017967202 -0.0017950449 -0.0017949227 -0.0017953794 -0.0017959753 -0.0017963822 -0.0017964209 -0.0017963304 -0.0017961774 -0.0017959676 -0.0017958125 -0.0017959037 -0.0017959973 -0.0017959408 -0.0017957394][-0.0017982033 -0.0017967024 -0.001796606 -0.0017969845 -0.001797343 -0.0017973768 -0.0017969657 -0.0017964694 -0.0017961006 -0.0017957827 -0.0017956344 -0.0017957749 -0.0017959283 -0.0017959109 -0.0017957175][-0.0017994819 -0.0017981071 -0.0017980096 -0.0017982771 -0.0017984175 -0.001798147 -0.001797398 -0.0017966237 -0.0017961162 -0.0017957281 -0.0017955708 -0.0017957108 -0.0017958848 -0.001795881 -0.001795692][-0.0018003123 -0.0017990137 -0.0017989593 -0.0017991607 -0.0017991666 -0.0017987007 -0.0017978034 -0.0017969235 -0.0017963271 -0.0017958652 -0.0017956544 -0.0017957432 -0.0017958776 -0.0017958465 -0.0017956637][-0.0018007754 -0.0017995938 -0.001799566 -0.0017997125 -0.0017996531 -0.0017990903 -0.0017981484 -0.0017972392 -0.001796594 -0.0017960607 -0.0017957862 -0.0017958081 -0.0017958818 -0.0017958144 -0.0017956315][-0.0018012563 -0.0018001017 -0.0018000107 -0.0018000944 -0.0017999636 -0.0017993553 -0.0017984072 -0.001797508 -0.001796829 -0.001796249 -0.0017959285 -0.0017958838 -0.0017958888 -0.0017957871 -0.0017955997][-0.0018017463 -0.0018005029 -0.0018003597 -0.0018003806 -0.0018001836 -0.0017995622 -0.0017986563 -0.0017977778 -0.0017970577 -0.0017964425 -0.0017960749 -0.0017959496 -0.0017958871 -0.0017957562 -0.0017955757][-0.0018020098 -0.0018007461 -0.0018005391 -0.001800537 -0.0018003249 -0.0017997314 -0.0017989031 -0.0017980625 -0.0017973114 -0.0017966591 -0.0017962238 -0.0017960066 -0.0017958803 -0.0017957202 -0.0017955535][-0.0018021788 -0.0018008355 -0.0018005853 -0.0018005825 -0.00180039 -0.0017998511 -0.0017990893 -0.0017982792 -0.0017975128 -0.0017968308 -0.0017963231 -0.0017960286 -0.0017958492 -0.0017956748 -0.0017955272][-0.0018023134 -0.0018009155 -0.0018005886 -0.0018005732 -0.001800377 -0.0017998792 -0.0017991703 -0.0017983795 -0.0017976084 -0.0017969134 -0.0017963637 -0.0017960127 -0.0017957985 -0.0017956255 -0.0017955007]]...]
INFO - root - 2017-12-09 22:03:36.146286: step 62210, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 65h:15m:58s remains)
INFO - root - 2017-12-09 22:03:44.913703: step 62220, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.847 sec/batch; 63h:35m:49s remains)
INFO - root - 2017-12-09 22:03:53.541902: step 62230, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.853 sec/batch; 64h:04m:19s remains)
INFO - root - 2017-12-09 22:04:02.184305: step 62240, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.851 sec/batch; 63h:52m:48s remains)
INFO - root - 2017-12-09 22:04:10.967948: step 62250, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.882 sec/batch; 66h:13m:40s remains)
INFO - root - 2017-12-09 22:04:19.462007: step 62260, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.878 sec/batch; 65h:54m:07s remains)
INFO - root - 2017-12-09 22:04:27.996351: step 62270, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.896 sec/batch; 67h:13m:17s remains)
INFO - root - 2017-12-09 22:04:36.776940: step 62280, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.882 sec/batch; 66h:13m:36s remains)
INFO - root - 2017-12-09 22:04:45.673027: step 62290, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.897 sec/batch; 67h:19m:11s remains)
INFO - root - 2017-12-09 22:04:54.379449: step 62300, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.872 sec/batch; 65h:28m:39s remains)
2017-12-09 22:04:55.296016: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0018062219 -0.0018049084 -0.0018048177 -0.0018050661 -0.0018053701 -0.0018056156 -0.001805754 -0.0018058288 -0.0018058468 -0.0018058184 -0.0018058236 -0.0018059569 -0.0018062056 -0.0018065221 -0.0018068918][-0.0018053387 -0.0018040888 -0.0018040757 -0.0018043621 -0.0018046717 -0.0018048832 -0.0018049577 -0.0018049889 -0.001805003 -0.0018050894 -0.0018052198 -0.0018054214 -0.0018056788 -0.0018058858 -0.0018061][-0.001805321 -0.001804168 -0.0018042293 -0.0018045593 -0.0018048318 -0.0018049639 -0.0018049314 -0.0018048858 -0.0018049171 -0.0018051025 -0.0018053603 -0.0018056371 -0.0018058658 -0.0018059381 -0.0018059429][-0.0018054905 -0.001804408 -0.0018045193 -0.0018048977 -0.0018051474 -0.0018052182 -0.0018050935 -0.0018049587 -0.0018049625 -0.0018051715 -0.0018054948 -0.0018058106 -0.0018060206 -0.0018059788 -0.0018057942][-0.0018056424 -0.0018046019 -0.0018047832 -0.0018052168 -0.0018054742 -0.0018055025 -0.0018053093 -0.0018051054 -0.0018050869 -0.0018052801 -0.0018056252 -0.0018059643 -0.0018061439 -0.0018059897 -0.0018056252][-0.001805626 -0.0018046834 -0.0018049321 -0.0018054585 -0.0018058113 -0.0018059021 -0.0018057306 -0.0018054732 -0.0018053931 -0.0018054916 -0.0018057356 -0.0018060161 -0.0018061176 -0.0018058583 -0.0018053475][-0.0018057165 -0.0018047972 -0.0018051172 -0.0018057488 -0.0018062551 -0.0018065005 -0.0018064047 -0.0018061158 -0.0018059328 -0.0018058353 -0.0018058732 -0.0018059743 -0.0018059381 -0.0018055727 -0.0018049653][-0.0018056541 -0.0018047368 -0.0018051082 -0.0018058048 -0.0018064484 -0.0018068572 -0.0018068998 -0.0018066746 -0.0018064561 -0.0018061947 -0.0018060001 -0.0018058734 -0.0018056583 -0.0018051974 -0.0018045383][-0.0018056843 -0.0018047997 -0.0018052395 -0.0018059553 -0.001806682 -0.0018071973 -0.0018073301 -0.0018071777 -0.0018069214 -0.0018065255 -0.0018060921 -0.001805746 -0.0018053763 -0.0018048327 -0.0018041664][-0.0018057835 -0.0018050704 -0.001805593 -0.0018062941 -0.0018070231 -0.0018075621 -0.0018076907 -0.0018075142 -0.0018071762 -0.0018066835 -0.0018060867 -0.0018055978 -0.0018051461 -0.0018045779 -0.0018039314][-0.0018058767 -0.0018052898 -0.0018058728 -0.0018065799 -0.0018072971 -0.0018078255 -0.0018079036 -0.0018076476 -0.0018072035 -0.0018066312 -0.001805931 -0.0018053433 -0.0018048685 -0.0018043371 -0.0018037624][-0.0018058893 -0.0018052653 -0.001805853 -0.0018065537 -0.0018072284 -0.0018077331 -0.0018077666 -0.0018074577 -0.0018069629 -0.0018063926 -0.0018056802 -0.0018050359 -0.0018045567 -0.001804086 -0.0018036152][-0.0018057512 -0.0018051576 -0.0018056652 -0.0018063155 -0.0018068998 -0.0018073219 -0.0018073132 -0.0018069841 -0.0018064793 -0.001805937 -0.0018052588 -0.001804649 -0.0018042388 -0.0018038978 -0.0018036009][-0.0018058316 -0.0018051632 -0.001805512 -0.0018060248 -0.0018064517 -0.0018067396 -0.0018066521 -0.0018062945 -0.0018058022 -0.0018052894 -0.001804699 -0.0018042111 -0.0018039648 -0.0018038159 -0.0018037327][-0.0018059568 -0.0018051809 -0.0018052631 -0.0018055484 -0.0018057538 -0.0018058884 -0.0018057439 -0.0018053898 -0.0018049539 -0.0018045412 -0.0018041161 -0.0018038217 -0.0018037814 -0.0018038554 -0.0018039942]]...]
INFO - root - 2017-12-09 22:05:04.062584: step 62310, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.881 sec/batch; 66h:07m:28s remains)
INFO - root - 2017-12-09 22:05:12.829431: step 62320, loss = 0.82, batch loss = 0.69 (8.0 examples/sec; 0.998 sec/batch; 74h:53m:28s remains)
INFO - root - 2017-12-09 22:05:21.381427: step 62330, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.878 sec/batch; 65h:53m:41s remains)
INFO - root - 2017-12-09 22:05:30.013893: step 62340, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.872 sec/batch; 65h:28m:09s remains)
INFO - root - 2017-12-09 22:05:38.816256: step 62350, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.891 sec/batch; 66h:53m:35s remains)
INFO - root - 2017-12-09 22:05:47.369476: step 62360, loss = 0.82, batch loss = 0.69 (8.7 examples/sec; 0.916 sec/batch; 68h:46m:09s remains)
INFO - root - 2017-12-09 22:05:55.710334: step 62370, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.878 sec/batch; 65h:51m:26s remains)
INFO - root - 2017-12-09 22:06:04.332939: step 62380, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.840 sec/batch; 63h:02m:21s remains)
INFO - root - 2017-12-09 22:06:13.023253: step 62390, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.849 sec/batch; 63h:41m:24s remains)
INFO - root - 2017-12-09 22:06:21.831245: step 62400, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.901 sec/batch; 67h:36m:39s remains)
2017-12-09 22:06:22.796457: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.0029493566 0.0037843329 0.0043645762 0.0045799431 0.00456696 0.0041655316 0.0034892871 0.0026241275 0.0017448176 0.00071947149 -0.00032344053 -0.0011532648 -0.0016289167 -0.0017885673 -0.0018114372][0.0057001244 0.0068841972 0.0075547765 0.00769826 0.0075928434 0.0070693041 0.006139128 0.0048986808 0.0035427641 0.0020728931 0.00053620327 -0.00074435619 -0.0015097528 -0.0017760167 -0.0018105627][0.0091311419 0.010787856 0.011632235 0.011741432 0.01152487 0.010855192 0.0096355369 0.0079524172 0.0060236347 0.0039303889 0.0017105906 -0.00017222262 -0.001336863 -0.0017576697 -0.0018102446][0.013112449 0.015389775 0.016503187 0.016616445 0.016225541 0.015259565 0.01352997 0.011251318 0.008640728 0.005923396 0.0030143857 0.00046932336 -0.0011386096 -0.0017372065 -0.0018090733][0.01600671 0.018769097 0.020230696 0.020578524 0.020291194 0.019213641 0.01710248 0.014209601 0.010877619 0.00748067 0.0039549908 0.00092978321 -0.000990741 -0.0017197545 -0.0018069933][0.018834017 0.021784257 0.023209827 0.023465445 0.022959225 0.02155667 0.019035013 0.015747335 0.012081365 0.00836448 0.00449427 0.0011735995 -0.00092206313 -0.0017141779 -0.0018053273][0.019866992 0.023222908 0.024993971 0.025526946 0.025076238 0.023444746 0.020438002 0.016631816 0.012552312 0.0084938286 0.0045086951 0.0011886529 -0.00088748289 -0.001700619 -0.001803109][0.020270159 0.023613978 0.025500581 0.026206246 0.025883377 0.024332773 0.02131873 0.017468713 0.013341742 0.0091402857 0.0049374993 0.0014268406 -0.00075976492 -0.0016360653 -0.0017848013][0.020340219 0.023639539 0.0255617 0.026469961 0.026404329 0.025089564 0.02227011 0.018606637 0.014648584 0.010534757 0.00626583 0.002529514 -3.3474178e-05 -0.0012864311 -0.0016921819][0.020988096 0.02414245 0.025732003 0.026585102 0.02683224 0.026206166 0.024221208 0.021312045 0.017833047 0.013775907 0.0091238869 0.0047495826 0.0014364122 -0.00052560214 -0.0014308813][0.023797216 0.026774047 0.027628381 0.028004495 0.028157812 0.027910406 0.02678068 0.024938524 0.022339845 0.018640796 0.013639455 0.0082560582 0.0036223303 0.00054582336 -0.0010628951][0.027731473 0.030454852 0.030559428 0.030541658 0.030803639 0.031259473 0.031166207 0.030347414 0.028362466 0.02462348 0.018956434 0.012392714 0.0062899995 0.0018539351 -0.0006301424][0.031790469 0.034153506 0.033438712 0.032963883 0.033226635 0.034282338 0.035182413 0.035385087 0.034078665 0.030500766 0.02428115 0.01648587 0.0088844988 0.0031647906 -0.00015909888][0.034912348 0.036537863 0.034933772 0.034122214 0.034507137 0.036055826 0.037591122 0.03837597 0.0373752 0.033850167 0.027425654 0.019141886 0.010762083 0.0041559478 0.00018564018][0.035739105 0.03642299 0.03420848 0.032946929 0.03302101 0.034514304 0.036139932 0.037118159 0.036344964 0.033144087 0.027054006 0.019028459 0.010804714 0.0042235893 0.00021579873]]...]
INFO - root - 2017-12-09 22:06:31.412926: step 62410, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.874 sec/batch; 65h:35m:07s remains)
INFO - root - 2017-12-09 22:06:40.096424: step 62420, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.866 sec/batch; 64h:58m:18s remains)
INFO - root - 2017-12-09 22:06:48.829877: step 62430, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.881 sec/batch; 66h:07m:02s remains)
INFO - root - 2017-12-09 22:06:57.720593: step 62440, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.874 sec/batch; 65h:32m:53s remains)
INFO - root - 2017-12-09 22:07:06.426589: step 62450, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.873 sec/batch; 65h:30m:12s remains)
INFO - root - 2017-12-09 22:07:14.980002: step 62460, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.856 sec/batch; 64h:13m:41s remains)
INFO - root - 2017-12-09 22:07:23.501950: step 62470, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.883 sec/batch; 66h:15m:07s remains)
INFO - root - 2017-12-09 22:07:32.283387: step 62480, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.873 sec/batch; 65h:28m:45s remains)
INFO - root - 2017-12-09 22:07:41.044097: step 62490, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.842 sec/batch; 63h:10m:36s remains)
INFO - root - 2017-12-09 22:07:49.703296: step 62500, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 64h:46m:29s remains)
2017-12-09 22:07:50.543666: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0018371744 -0.0018376017 -0.0018387306 -0.001839618 -0.0018401836 -0.0018401471 -0.0018389686 -0.0018364479 -0.00183398 -0.0018318383 -0.0018305677 -0.0018302317 -0.0018311765 -0.0018334272 -0.0018356157][-0.0018313832 -0.001832368 -0.001834868 -0.0018368644 -0.0018381595 -0.0018387006 -0.0018374188 -0.0018335115 -0.0018291961 -0.0018253183 -0.0018227275 -0.0018220447 -0.0018235927 -0.0018274286 -0.0018313965][-0.0018247315 -0.0018263989 -0.0018304265 -0.0018336778 -0.0018358515 -0.0018372157 -0.0018360283 -0.0018309104 -0.0018246273 -0.0018189907 -0.0018150469 -0.0018138677 -0.0018162134 -0.0018219571 -0.0018276016][-0.0018189758 -0.0018214975 -0.0018265742 -0.0018309123 -0.0018339028 -0.0018356977 -0.0018344995 -0.0018285216 -0.001820761 -0.001813519 -0.0018085397 -0.0018072572 -0.0018102964 -0.0018172488 -0.0018244548][-0.0018144096 -0.0018178793 -0.0018240643 -0.0018293653 -0.0018329017 -0.0018350352 -0.00183396 -0.0018274036 -0.0018187448 -0.0018107122 -0.0018052892 -0.0018039326 -0.0018072437 -0.0018149067 -0.0018231167][-0.0018117027 -0.0018157336 -0.0018225145 -0.0018283036 -0.0018323549 -0.0018347884 -0.0018340172 -0.0018280819 -0.0018193658 -0.0018115004 -0.0018063113 -0.0018048567 -0.001807738 -0.0018152407 -0.001823466][-0.0018110255 -0.0018151124 -0.0018217722 -0.001827781 -0.0018319876 -0.0018345204 -0.0018341683 -0.0018291959 -0.0018213305 -0.0018142536 -0.0018095955 -0.0018085785 -0.0018110676 -0.0018176187 -0.0018251741][-0.0018126537 -0.0018162522 -0.0018223245 -0.0018280618 -0.0018320816 -0.0018345302 -0.0018344341 -0.0018302967 -0.0018236644 -0.0018175264 -0.0018136089 -0.0018127227 -0.0018148815 -0.001820504 -0.0018268083][-0.0018162148 -0.0018194707 -0.001824466 -0.001829245 -0.001832606 -0.0018344526 -0.0018344322 -0.0018315603 -0.0018265276 -0.0018213867 -0.0018181269 -0.0018174907 -0.0018191772 -0.0018232736 -0.0018278689][-0.0018204793 -0.0018228448 -0.0018266668 -0.0018303759 -0.0018329117 -0.0018341388 -0.001834119 -0.0018320961 -0.0018288892 -0.0018254515 -0.0018229801 -0.0018222519 -0.0018234733 -0.0018261744 -0.0018290041][-0.0018251749 -0.0018260776 -0.0018284991 -0.0018310632 -0.001832685 -0.0018332659 -0.0018331261 -0.0018319092 -0.001829911 -0.001827969 -0.0018266302 -0.0018262441 -0.00182698 -0.0018284345 -0.0018298352][-0.0018291681 -0.0018290282 -0.0018300344 -0.001831327 -0.0018322634 -0.0018323184 -0.001832062 -0.0018314846 -0.0018303959 -0.0018291733 -0.0018283393 -0.0018281946 -0.0018289249 -0.0018298668 -0.0018304789][-0.0018316709 -0.0018306953 -0.0018309514 -0.0018313351 -0.0018315966 -0.0018314415 -0.0018312115 -0.001830765 -0.0018302103 -0.0018295289 -0.0018289372 -0.0018287217 -0.001828989 -0.0018295684 -0.0018300781][-0.0018325782 -0.0018310046 -0.0018305499 -0.0018306496 -0.0018308723 -0.0018308748 -0.001830754 -0.0018304953 -0.0018301856 -0.0018298421 -0.0018293731 -0.0018291458 -0.0018291008 -0.0018292664 -0.0018294859][-0.001832808 -0.0018310677 -0.0018302675 -0.0018298595 -0.001829913 -0.0018300723 -0.0018303065 -0.0018303973 -0.001830237 -0.0018301154 -0.0018298752 -0.0018296946 -0.0018295008 -0.0018293898 -0.0018293741]]...]
INFO - root - 2017-12-09 22:07:59.194625: step 62510, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.874 sec/batch; 65h:33m:48s remains)
INFO - root - 2017-12-09 22:08:07.775381: step 62520, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.855 sec/batch; 64h:05m:54s remains)
INFO - root - 2017-12-09 22:08:16.215337: step 62530, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 64h:40m:44s remains)
INFO - root - 2017-12-09 22:08:24.858335: step 62540, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.876 sec/batch; 65h:41m:02s remains)
INFO - root - 2017-12-09 22:08:33.465009: step 62550, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.875 sec/batch; 65h:35m:33s remains)
INFO - root - 2017-12-09 22:08:41.938438: step 62560, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 64h:37m:28s remains)
INFO - root - 2017-12-09 22:08:50.202291: step 62570, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 65h:06m:19s remains)
INFO - root - 2017-12-09 22:08:58.743771: step 62580, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.842 sec/batch; 63h:10m:04s remains)
INFO - root - 2017-12-09 22:09:07.383211: step 62590, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.865 sec/batch; 64h:52m:55s remains)
INFO - root - 2017-12-09 22:09:16.038071: step 62600, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.893 sec/batch; 66h:57m:11s remains)
2017-12-09 22:09:16.991833: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.011800961 0.012064881 0.012038083 0.011628445 0.010895699 0.0099052666 0.00880964 0.0077613345 0.0068319649 0.0060257982 0.00529392 0.0046953158 0.0042490452 0.0039725387 0.0039134971][0.014630304 0.015396959 0.015706174 0.015438867 0.014624265 0.013373849 0.011931067 0.01056122 0.0093999486 0.008424541 0.007551312 0.006791635 0.0061623207 0.0056316257 0.0052455869][0.017415665 0.018708605 0.019410718 0.019335182 0.018503785 0.017027307 0.015236427 0.013519225 0.012104463 0.010988988 0.010018949 0.009195352 0.0084763644 0.0077584158 0.0070275408][0.019865168 0.021635206 0.022699114 0.022840174 0.022027113 0.020375211 0.018297426 0.016312204 0.014711293 0.013505572 0.012511871 0.011689624 0.010956866 0.010114369 0.0091047157][0.021767702 0.02391942 0.02526531 0.025593527 0.02484276 0.023132566 0.020921012 0.018799402 0.017088776 0.015823184 0.014818066 0.014000597 0.013252422 0.012356342 0.011171328][0.023019986 0.025478853 0.027052801 0.027541112 0.026864193 0.025198339 0.02301285 0.020929284 0.019238016 0.017964903 0.016965963 0.016147939 0.015393835 0.014449569 0.013134285][0.023598166 0.026198179 0.027886366 0.0285111 0.027965728 0.026469925 0.024486866 0.022622947 0.021106832 0.019936724 0.018987097 0.018181739 0.017385092 0.016367236 0.014938832][0.023538468 0.026118947 0.027832806 0.028577581 0.028249646 0.027055459 0.025419634 0.023903713 0.022682847 0.021740276 0.020925539 0.020119321 0.019250702 0.018092752 0.01656376][0.022836141 0.02529454 0.026964176 0.027833082 0.027777899 0.026979391 0.025779745 0.024674231 0.023806991 0.023142571 0.022519656 0.021745179 0.020780988 0.01949504 0.017871086][0.021651203 0.0238944 0.025463514 0.026426295 0.026650917 0.026259264 0.025505647 0.02479326 0.024251107 0.023838514 0.023371447 0.022631826 0.021618757 0.020265773 0.018636391][0.020067096 0.022009369 0.023416845 0.024426149 0.024896478 0.024886061 0.024556348 0.024197198 0.023927938 0.023706533 0.023366477 0.022698274 0.021710016 0.020382376 0.018810628][0.018235611 0.019784605 0.020930883 0.021888357 0.022517385 0.02280646 0.02282235 0.022754136 0.022690015 0.022604108 0.022361863 0.021804685 0.020929178 0.019758387 0.018347712][0.016396169 0.01757808 0.01843765 0.019229844 0.019874932 0.020308813 0.020515207 0.020595515 0.020621536 0.020575522 0.020383058 0.019931948 0.019244129 0.018268153 0.017095121][0.014540852 0.015422002 0.01599996 0.016568838 0.017125556 0.017567331 0.017837388 0.017995466 0.018069906 0.018033579 0.017869631 0.017536234 0.017042367 0.016338073 0.015436004][0.01302823 0.013661483 0.01398153 0.014310629 0.014691557 0.015018948 0.015241223 0.015390043 0.015473579 0.015453207 0.015333162 0.015144111 0.014879937 0.014461984 0.013861003]]...]
INFO - root - 2017-12-09 22:09:25.621944: step 62610, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 64h:58m:16s remains)
INFO - root - 2017-12-09 22:09:34.209398: step 62620, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.850 sec/batch; 63h:41m:23s remains)
INFO - root - 2017-12-09 22:09:42.724291: step 62630, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.877 sec/batch; 65h:46m:42s remains)
INFO - root - 2017-12-09 22:09:51.390205: step 62640, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.856 sec/batch; 64h:12m:14s remains)
INFO - root - 2017-12-09 22:10:00.183888: step 62650, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.883 sec/batch; 66h:13m:05s remains)
INFO - root - 2017-12-09 22:10:08.807553: step 62660, loss = 0.82, batch loss = 0.69 (9.8 examples/sec; 0.818 sec/batch; 61h:17m:54s remains)
INFO - root - 2017-12-09 22:10:17.518700: step 62670, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 64h:59m:10s remains)
INFO - root - 2017-12-09 22:10:26.253584: step 62680, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.838 sec/batch; 62h:49m:41s remains)
INFO - root - 2017-12-09 22:10:34.781362: step 62690, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 64h:45m:58s remains)
INFO - root - 2017-12-09 22:10:43.257783: step 62700, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.877 sec/batch; 65h:42m:47s remains)
2017-12-09 22:10:44.151839: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0018058331 -0.0017722226 -0.0017233193 -0.0016653729 -0.0016127768 -0.0015840442 -0.0015893958 -0.0016257775 -0.001679976 -0.0017339534 -0.001774949 -0.0018006574 -0.0018139409 -0.0018201084 -0.0018226251][-0.0018300635 -0.0018163286 -0.0017891331 -0.0017458256 -0.0016969354 -0.0016625994 -0.0016572883 -0.0016821765 -0.0017252078 -0.0017689068 -0.0018016506 -0.0018210143 -0.0018302082 -0.0018338163 -0.0018347437][-0.0018330319 -0.0018159439 -0.0017709837 -0.0016902033 -0.0015923294 -0.0015167334 -0.0014944464 -0.0015321316 -0.0016095156 -0.0016946095 -0.0017625394 -0.001804785 -0.0018254516 -0.0018329993 -0.00183474][-0.0018269457 -0.0017875207 -0.0016834046 -0.0015016181 -0.0012844397 -0.0011133617 -0.0010551943 -0.0011272302 -0.0012912136 -0.0014820092 -0.0016437162 -0.0017503249 -0.001805416 -0.0018270805 -0.0018328878][-0.0018168928 -0.0017446032 -0.0015551113 -0.0012266811 -0.00083130226 -0.00050650619 -0.00037446211 -0.00047600269 -0.00075825385 -0.0011101842 -0.0014261416 -0.0016448835 -0.001764077 -0.0018138812 -0.0018289301][-0.0018053921 -0.0017019811 -0.0014301194 -0.00095177267 -0.00035851239 0.00015911425 0.00040928822 0.00030924624 -8.7898923e-05 -0.00062274176 -0.0011294285 -0.0014952843 -0.0017033641 -0.0017943253 -0.0018237622][-0.001797559 -0.0016798845 -0.0013645875 -0.00078943651 -4.3450505e-05 0.00065247796 0.0010421361 0.00098920672 0.00052560831 -0.00015660608 -0.00083557854 -0.001342979 -0.0016405394 -0.0017743934 -0.0018190712][-0.0017973809 -0.0016880571 -0.0013841764 -0.00079967431 8.9930836e-07 0.00079921854 0.0013014482 0.0013232654 0.00086093519 0.00011458213 -0.00065871479 -0.0012503434 -0.0016030953 -0.0017632201 -0.0018166206][-0.0018045182 -0.0017178395 -0.0014651896 -0.00094800029 -0.00020034122 0.000590355 0.0011316017 0.0012151439 0.00080559042 8.9281355e-05 -0.00067158369 -0.0012592936 -0.0016095235 -0.0017666458 -0.0018176736][-0.0018149809 -0.0017543385 -0.00156772 -0.0011615019 -0.00054658414 0.00013437949 0.00062636251 0.000732096 0.000401863 -0.00020643522 -0.00085763074 -0.0013588618 -0.0016529483 -0.0017811905 -0.0018207513][-0.0018231762 -0.0017857002 -0.0016609213 -0.0013757379 -0.00093202735 -0.00042741268 -5.4115895e-05 3.3234828e-05 -0.00020874594 -0.00066029187 -0.001140262 -0.0015045925 -0.0017127634 -0.0017997639 -0.0018245947][-0.001828176 -0.0018085475 -0.0017375787 -0.0015695492 -0.0013039522 -0.000997248 -0.0007689395 -0.00071564247 -0.00086540822 -0.0011419235 -0.0014317431 -0.0016474167 -0.0017667918 -0.0018144831 -0.0018270854][-0.0018290034 -0.0018206121 -0.0017888875 -0.0017113845 -0.00158703 -0.0014418776 -0.0013343766 -0.0013111074 -0.0013850781 -0.0015176013 -0.0016533879 -0.0017516507 -0.0018038852 -0.0018234905 -0.0018280033][-0.0018291874 -0.0018267761 -0.0018179045 -0.0017945807 -0.0017555377 -0.0017082724 -0.0016730546 -0.001665625 -0.0016904935 -0.0017337495 -0.0017769914 -0.0018071259 -0.0018222553 -0.0018273527 -0.0018281646][-0.0018293643 -0.0018285138 -0.0018272612 -0.0018237656 -0.0018173328 -0.0018088346 -0.0018022328 -0.0018007804 -0.0018055502 -0.0018136292 -0.0018213406 -0.0018262633 -0.0018283221 -0.0018287256 -0.001828557]]...]
INFO - root - 2017-12-09 22:10:52.683377: step 62710, loss = 0.83, batch loss = 0.70 (9.8 examples/sec; 0.818 sec/batch; 61h:17m:04s remains)
INFO - root - 2017-12-09 22:11:01.253228: step 62720, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.839 sec/batch; 62h:50m:55s remains)
INFO - root - 2017-12-09 22:11:09.755367: step 62730, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.860 sec/batch; 64h:27m:20s remains)
INFO - root - 2017-12-09 22:11:18.343185: step 62740, loss = 0.83, batch loss = 0.70 (9.4 examples/sec; 0.853 sec/batch; 63h:53m:48s remains)
INFO - root - 2017-12-09 22:11:26.946835: step 62750, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 65h:17m:38s remains)
INFO - root - 2017-12-09 22:11:35.467754: step 62760, loss = 0.82, batch loss = 0.69 (10.6 examples/sec; 0.754 sec/batch; 56h:30m:47s remains)
INFO - root - 2017-12-09 22:11:43.847923: step 62770, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.875 sec/batch; 65h:31m:54s remains)
INFO - root - 2017-12-09 22:11:52.550704: step 62780, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.847 sec/batch; 63h:27m:20s remains)
INFO - root - 2017-12-09 22:12:01.356142: step 62790, loss = 0.82, batch loss = 0.69 (8.8 examples/sec; 0.913 sec/batch; 68h:22m:47s remains)
INFO - root - 2017-12-09 22:12:10.114598: step 62800, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.863 sec/batch; 64h:37m:45s remains)
2017-12-09 22:12:10.942792: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0018111739 -0.0018099726 -0.0018099358 -0.0018099982 -0.0018099975 -0.0018099596 -0.0018098803 -0.0018097969 -0.0018097069 -0.0018095991 -0.0018095109 -0.0018094562 -0.0018094392 -0.0018094557 -0.0018094768][-0.0018103686 -0.0018091634 -0.0018091585 -0.0018092614 -0.0018093025 -0.0018093046 -0.0018092514 -0.001809144 -0.0018090028 -0.0018088617 -0.0018087543 -0.0018087013 -0.0018087045 -0.0018087476 -0.001808813][-0.0018102567 -0.0018090768 -0.0018090797 -0.0018092006 -0.0018092627 -0.0018092841 -0.0018092676 -0.0018091646 -0.0018089809 -0.001808779 -0.001808624 -0.001808556 -0.0018085727 -0.0018086493 -0.0018087565][-0.0018102238 -0.0018090213 -0.0018089433 -0.0018089684 -0.001808955 -0.0018089771 -0.0018090345 -0.0018089684 -0.00180878 -0.0018085791 -0.0018084231 -0.0018083701 -0.0018084364 -0.0018085653 -0.0018087128][-0.0018103013 -0.0018089795 -0.00180877 -0.0018086064 -0.0018084085 -0.0018083747 -0.0018085022 -0.0018085015 -0.0018083344 -0.0018081838 -0.0018080944 -0.001808121 -0.0018082776 -0.0018084802 -0.0018086752][-0.0018104151 -0.0018090304 -0.0018086962 -0.0018082904 -0.0018078152 -0.0018076408 -0.0018077441 -0.0018077954 -0.0018077291 -0.0018077 -0.001807741 -0.0018078971 -0.001808158 -0.0018084331 -0.0018086644][-0.0018104697 -0.001809055 -0.001808594 -0.001807956 -0.0018071878 -0.001806816 -0.0018068283 -0.0018069005 -0.0018069625 -0.0018071447 -0.001807401 -0.001807704 -0.0018080621 -0.0018083976 -0.0018086565][-0.0018105268 -0.0018090344 -0.0018084901 -0.0018077032 -0.001806728 -0.0018061785 -0.0018060784 -0.0018061444 -0.0018063275 -0.0018067263 -0.0018072089 -0.0018076318 -0.0018080435 -0.0018084162 -0.0018086712][-0.0018105378 -0.0018090772 -0.0018085899 -0.0018078008 -0.0018067554 -0.0018060538 -0.0018057739 -0.0018057685 -0.0018060168 -0.0018065575 -0.0018071771 -0.0018076834 -0.0018081175 -0.0018084856 -0.0018087126][-0.0018104551 -0.0018091924 -0.0018088297 -0.0018081788 -0.0018072505 -0.0018065305 -0.0018060745 -0.0018059458 -0.0018061695 -0.0018067084 -0.0018073372 -0.0018078496 -0.0018082588 -0.0018085781 -0.0018087649][-0.0018103668 -0.0018092021 -0.0018089204 -0.0018084341 -0.0018077521 -0.0018071819 -0.0018067022 -0.0018064843 -0.0018066368 -0.0018070914 -0.0018076391 -0.0018080749 -0.0018084159 -0.0018086818 -0.0018088301][-0.0018103523 -0.0018091211 -0.0018088913 -0.0018085965 -0.0018082095 -0.0018078826 -0.0018075496 -0.0018073291 -0.001807357 -0.0018076379 -0.0018080135 -0.0018083175 -0.001808571 -0.0018087787 -0.0018088854][-0.0018103266 -0.0018091286 -0.001808931 -0.0018088279 -0.0018087041 -0.0018086006 -0.0018084343 -0.0018082791 -0.0018082134 -0.0018082774 -0.0018084291 -0.0018085741 -0.0018087217 -0.0018088534 -0.0018089167][-0.0018103591 -0.0018091812 -0.0018089915 -0.0018089849 -0.0018089979 -0.0018090208 -0.0018089735 -0.0018088944 -0.0018088229 -0.0018087917 -0.0018087941 -0.0018088057 -0.0018088551 -0.0018089138 -0.0018089395][-0.0018104621 -0.0018092825 -0.0018090204 -0.0018090329 -0.0018090686 -0.0018091083 -0.0018091156 -0.0018091114 -0.0018090934 -0.0018090532 -0.0018090106 -0.0018089672 -0.0018089522 -0.0018089492 -0.0018089459]]...]
INFO - root - 2017-12-09 22:12:19.602156: step 62810, loss = 0.83, batch loss = 0.70 (9.1 examples/sec; 0.878 sec/batch; 65h:44m:42s remains)
INFO - root - 2017-12-09 22:12:28.312450: step 62820, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.877 sec/batch; 65h:42m:28s remains)
INFO - root - 2017-12-09 22:12:36.882095: step 62830, loss = 0.83, batch loss = 0.70 (9.2 examples/sec; 0.870 sec/batch; 65h:08m:23s remains)
INFO - root - 2017-12-09 22:12:45.598799: step 62840, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.892 sec/batch; 66h:48m:29s remains)
INFO - root - 2017-12-09 22:12:54.334215: step 62850, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.878 sec/batch; 65h:43m:38s remains)
INFO - root - 2017-12-09 22:13:02.909446: step 62860, loss = 0.82, batch loss = 0.69 (11.4 examples/sec; 0.701 sec/batch; 52h:28m:56s remains)
INFO - root - 2017-12-09 22:13:11.333262: step 62870, loss = 0.82, batch loss = 0.69 (9.7 examples/sec; 0.822 sec/batch; 61h:32m:41s remains)
INFO - root - 2017-12-09 22:13:19.820263: step 62880, loss = 0.82, batch loss = 0.69 (9.9 examples/sec; 0.808 sec/batch; 60h:29m:55s remains)
INFO - root - 2017-12-09 22:13:28.319534: step 62890, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.849 sec/batch; 63h:33m:51s remains)
INFO - root - 2017-12-09 22:13:37.005295: step 62900, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.884 sec/batch; 66h:11m:45s remains)
2017-12-09 22:13:37.932477: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.011657759 0.012353532 0.011457131 0.0091843307 0.0061659426 0.0031247498 0.00066145335 -0.0008882985 -0.0015916371 -0.0017876287 -0.0018135417 -0.0018151925 -0.0018084428 -0.0018016725 -0.0017912929][0.010333535 0.010776274 0.0098395795 0.00775409 0.0051157 0.0025502 0.00053508894 -0.0007417195 -0.0013654364 -0.001600106 -0.0017028828 -0.0017568833 -0.0017902725 -0.0017897377 -0.0017837012][0.008564665 0.008889203 0.0080538718 0.0063267122 0.0042214203 0.0022454332 0.00075543427 -0.00017570646 -0.0006856909 -0.0010052838 -0.0012598156 -0.0014587853 -0.0016192528 -0.0017019226 -0.0017387255][0.0060479217 0.0062613771 0.0056270007 0.0043890341 0.0029556714 0.0017009835 0.000829365 0.00030962646 -1.6964739e-05 -0.00032375439 -0.00063596235 -0.00094712386 -0.0012201067 -0.0014127231 -0.0015260181][0.0037207166 0.0039130016 0.0035389382 0.0027893507 0.0019496767 0.0012679399 0.00083706493 0.00059670385 0.00042271626 0.00020382053 -6.3003274e-05 -0.00035772892 -0.00062620768 -0.0008527498 -0.001019438][0.0023120716 0.0025732694 0.0025056526 0.0022550719 0.0019637067 0.0017362266 0.0015775714 0.0014430861 0.0012822641 0.0010411158 0.00076459383 0.00045902852 0.00015178032 -0.0001671362 -0.0004354181][0.0015452512 0.0019097776 0.0021829754 0.0024149683 0.0026614564 0.0029543964 0.0031786705 0.0032682484 0.0031989878 0.0029831277 0.0026468472 0.0022128173 0.0017284654 0.0012148508 0.00077166117][0.0019081071 0.0024520359 0.0029626386 0.0034942203 0.0040829619 0.0046586283 0.0051974384 0.0055554542 0.0057359426 0.0056916177 0.0054048928 0.0049136844 0.0042354972 0.0034883143 0.0028064628][0.0030068 0.0037772483 0.0045450814 0.0053865346 0.0062283627 0.0070011946 0.0076218578 0.0079980893 0.0082217753 0.00823338 0.0080634635 0.0076698666 0.0070356722 0.0062502553 0.0053798016][0.004554586 0.005524307 0.0064976173 0.0075192093 0.0085344389 0.0094472431 0.010088745 0.010389026 0.010444024 0.010277915 0.0099629769 0.0094540352 0.0087643582 0.00804356 0.0072396672][0.00584961 0.00685742 0.0078595206 0.0089804959 0.010066895 0.011023611 0.011689419 0.011962559 0.011915918 0.011614311 0.011136189 0.010439305 0.0096751181 0.0089477105 0.0082176207][0.0064937263 0.0074715591 0.00841739 0.00944386 0.010479661 0.011411728 0.012062682 0.012350913 0.012308259 0.012017366 0.011492059 0.010808377 0.010031163 0.0092484234 0.0085798018][0.0069696861 0.0077871908 0.0085667893 0.0094021633 0.010201333 0.010875203 0.011332444 0.011537028 0.011495562 0.011230021 0.010823799 0.010270634 0.0096065141 0.0089095794 0.0082703233][0.0068463883 0.0075981705 0.00824795 0.0088780019 0.009449644 0.0098884627 0.010213817 0.01036945 0.010363537 0.010212547 0.0099499077 0.0096054254 0.0091670277 0.0086742276 0.0081956042][0.0066711358 0.0074592675 0.0081093181 0.0086772926 0.0091113839 0.0093792239 0.0095840842 0.0097572235 0.0098599549 0.0098348046 0.0097152246 0.0095140459 0.00921303 0.0088456348 0.0084261568]]...]
INFO - root - 2017-12-09 22:13:46.592614: step 62910, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.878 sec/batch; 65h:42m:53s remains)
INFO - root - 2017-12-09 22:13:55.206713: step 62920, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 65h:01m:21s remains)
INFO - root - 2017-12-09 22:14:03.568718: step 62930, loss = 0.82, batch loss = 0.69 (9.7 examples/sec; 0.826 sec/batch; 61h:49m:18s remains)
INFO - root - 2017-12-09 22:14:12.173946: step 62940, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.895 sec/batch; 66h:59m:22s remains)
INFO - root - 2017-12-09 22:14:20.837904: step 62950, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.847 sec/batch; 63h:26m:27s remains)
INFO - root - 2017-12-09 22:14:29.597463: step 62960, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.844 sec/batch; 63h:12m:50s remains)
INFO - root - 2017-12-09 22:14:37.919321: step 62970, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.863 sec/batch; 64h:35m:57s remains)
INFO - root - 2017-12-09 22:14:46.679058: step 62980, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 64h:57m:51s remains)
INFO - root - 2017-12-09 22:14:55.195349: step 62990, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.866 sec/batch; 64h:51m:28s remains)
INFO - root - 2017-12-09 22:15:03.804289: step 63000, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.873 sec/batch; 65h:20m:02s remains)
2017-12-09 22:15:04.600345: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0018127742 -0.0018113862 -0.0018110352 -0.0018110584 -0.0018110428 -0.0018108338 -0.0018105888 -0.0018103929 -0.0018102548 -0.0018101372 -0.0018099974 -0.0018099139 -0.0018099315 -0.0018100773 -0.0018101983][-0.001812574 -0.001811263 -0.0018110126 -0.0018110961 -0.0018111276 -0.0018109039 -0.0018105478 -0.0018101806 -0.0018098993 -0.0018096804 -0.0018094486 -0.0018092871 -0.0018092648 -0.0018094343 -0.0018096355][-0.001812265 -0.0018111204 -0.0018110019 -0.0018111443 -0.0018112578 -0.0018110888 -0.0018106974 -0.0018102644 -0.001809921 -0.0018096408 -0.0018093294 -0.0018090995 -0.0018090376 -0.0018092209 -0.0018094885][-0.0018116278 -0.0018105897 -0.0018105546 -0.001810739 -0.0018109146 -0.0018108082 -0.0018104309 -0.001809986 -0.0018096194 -0.0018092991 -0.0018089429 -0.0018086487 -0.001808557 -0.0018087737 -0.0018091331][-0.0018108801 -0.0018099222 -0.0018099147 -0.0018100792 -0.0018102312 -0.001810157 -0.0018098126 -0.0018093804 -0.0018090191 -0.001808711 -0.0018083664 -0.0018080479 -0.0018079265 -0.0018081387 -0.0018085379][-0.0018100926 -0.001809228 -0.0018093142 -0.0018094684 -0.001809551 -0.0018094288 -0.0018090478 -0.001808565 -0.001808165 -0.0018078692 -0.0018075824 -0.0018073111 -0.001807207 -0.0018074051 -0.0018077982][-0.0018093971 -0.0018085096 -0.0018086819 -0.0018088736 -0.0018089249 -0.0018087344 -0.0018082849 -0.0018077369 -0.0018072804 -0.0018069791 -0.0018067564 -0.0018065876 -0.0018065345 -0.0018067065 -0.0018070649][-0.0018088868 -0.0018078971 -0.0018081 -0.0018083625 -0.0018084411 -0.0018082428 -0.0018077805 -0.0018071927 -0.0018066829 -0.0018063508 -0.0018061592 -0.0018060439 -0.0018060134 -0.0018061497 -0.0018064422][-0.0018085828 -0.0018075662 -0.0018078664 -0.0018082326 -0.0018084005 -0.0018082658 -0.0018078351 -0.0018072319 -0.0018066728 -0.0018062755 -0.0018060436 -0.0018058992 -0.00180582 -0.0018058573 -0.0018060136][-0.0018083756 -0.0018075565 -0.0018079856 -0.0018084801 -0.0018087784 -0.0018087336 -0.0018083466 -0.0018077686 -0.0018072021 -0.0018067383 -0.0018063965 -0.0018061581 -0.0018059833 -0.0018058593 -0.0018058106][-0.0018085499 -0.0018078682 -0.0018083173 -0.0018088089 -0.001809133 -0.0018091329 -0.0018087872 -0.0018082662 -0.0018077402 -0.0018072693 -0.001806881 -0.0018065969 -0.0018063505 -0.001806125 -0.0018059476][-0.0018090613 -0.0018083921 -0.0018087653 -0.0018091602 -0.0018094328 -0.0018094445 -0.0018091579 -0.0018087182 -0.0018082532 -0.0018078055 -0.0018074003 -0.0018070805 -0.0018068014 -0.0018065391 -0.0018063321][-0.0018097023 -0.0018090206 -0.0018092565 -0.0018095373 -0.0018097332 -0.0018097351 -0.0018095231 -0.0018091765 -0.0018087729 -0.0018083394 -0.0018079202 -0.0018075584 -0.0018072525 -0.0018069992 -0.0018068348][-0.001810407 -0.0018096907 -0.0018097368 -0.0018098794 -0.0018099693 -0.0018099358 -0.001809762 -0.0018094793 -0.0018091166 -0.0018087159 -0.0018083169 -0.0018079676 -0.0018076849 -0.0018074756 -0.0018073847][-0.0018110879 -0.0018102392 -0.0018100579 -0.001810052 -0.0018100142 -0.0018099032 -0.0018097267 -0.0018094943 -0.0018091975 -0.0018088776 -0.0018085801 -0.0018083432 -0.0018081695 -0.0018080405 -0.0018080034]]...]
INFO - root - 2017-12-09 22:15:13.304927: step 63010, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.884 sec/batch; 66h:12m:29s remains)
INFO - root - 2017-12-09 22:15:21.823686: step 63020, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.843 sec/batch; 63h:05m:49s remains)
INFO - root - 2017-12-09 22:15:30.211285: step 63030, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 64h:29m:59s remains)
INFO - root - 2017-12-09 22:15:38.778127: step 63040, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.855 sec/batch; 64h:00m:10s remains)
INFO - root - 2017-12-09 22:15:47.427171: step 63050, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 64h:29m:18s remains)
INFO - root - 2017-12-09 22:15:56.140728: step 63060, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 65h:03m:38s remains)
INFO - root - 2017-12-09 22:16:04.458957: step 63070, loss = 0.82, batch loss = 0.69 (10.0 examples/sec; 0.800 sec/batch; 59h:53m:47s remains)
INFO - root - 2017-12-09 22:16:13.146507: step 63080, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.865 sec/batch; 64h:43m:24s remains)
INFO - root - 2017-12-09 22:16:21.634029: step 63090, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.877 sec/batch; 65h:38m:19s remains)
INFO - root - 2017-12-09 22:16:30.281060: step 63100, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.856 sec/batch; 64h:04m:32s remains)
2017-12-09 22:16:31.157582: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.037092682 0.036932141 0.036138486 0.034746788 0.032748751 0.030723695 0.028971959 0.027912801 0.027920261 0.029074408 0.031328972 0.033984065 0.036773585 0.039684068 0.04183659][0.036536135 0.035720568 0.034167096 0.0319985 0.029411379 0.026991861 0.02501142 0.023675287 0.023637442 0.024866911 0.027506789 0.030887548 0.034592569 0.038528755 0.041601881][0.034611583 0.03319547 0.03092698 0.028063593 0.025058653 0.022449216 0.02044061 0.019222857 0.019276168 0.020822113 0.023961414 0.028096577 0.032447044 0.03700681 0.040646013][0.032782331 0.030562632 0.027514471 0.024074262 0.020808758 0.018081518 0.016177282 0.015189321 0.015614032 0.01752796 0.021020612 0.025708079 0.030712377 0.035672259 0.039573945][0.031822931 0.028917948 0.024971526 0.020903863 0.017397184 0.014724762 0.013099115 0.012447743 0.013325053 0.015756175 0.019771684 0.0249551 0.030166876 0.035150651 0.038845293][0.032172836 0.028900616 0.024511427 0.020026075 0.016240831 0.01363942 0.012272667 0.012068 0.013400624 0.016316755 0.020745944 0.0261053 0.031379178 0.036027558 0.039143179][0.033339843 0.030060224 0.02552674 0.020973934 0.017194932 0.014545209 0.013086256 0.013089721 0.014689126 0.017914163 0.022482531 0.027752761 0.032727096 0.036785528 0.03920687][0.034515522 0.031807628 0.027718088 0.023481557 0.019854732 0.017268227 0.015785219 0.015845705 0.017459504 0.020623613 0.024888445 0.029596634 0.033896193 0.037055593 0.038608022][0.035326611 0.033534881 0.030166868 0.026617514 0.023493281 0.021060832 0.019547956 0.019472061 0.020816065 0.023522878 0.027074961 0.030812707 0.033994958 0.036035586 0.036623146][0.034961209 0.03403046 0.031499833 0.028729618 0.026124246 0.024116136 0.022801783 0.022524141 0.023408739 0.025289387 0.027831407 0.0303417 0.032273859 0.03321109 0.032913275][0.032729913 0.0324743 0.03076905 0.028777022 0.026802709 0.025172705 0.024009909 0.023593172 0.023931624 0.024917761 0.026385363 0.027741412 0.028664693 0.028735865 0.027832292][0.02798696 0.0281762 0.027118003 0.025765719 0.024344038 0.023071477 0.022016279 0.021377634 0.021179687 0.02138027 0.021918172 0.022468584 0.022778241 0.02247376 0.021443365][0.021169782 0.021570982 0.020975951 0.020083113 0.019105235 0.018130522 0.017183335 0.016384507 0.015843134 0.015592908 0.015656143 0.01580607 0.015853632 0.015521909 0.014669619][0.013767842 0.014172359 0.013845134 0.01331454 0.012710021 0.012062047 0.011352235 0.010616497 0.01000994 0.0096130222 0.00949163 0.0095126648 0.009512078 0.0092774695 0.008687227][0.00753836 0.0078285029 0.0076701748 0.0073851352 0.0070426506 0.0066544753 0.0061870678 0.0056679449 0.005214151 0.0048866244 0.0047704587 0.0047440734 0.0047192732 0.0045635905 0.0041812574]]...]
INFO - root - 2017-12-09 22:16:39.949168: step 63110, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.850 sec/batch; 63h:36m:39s remains)
INFO - root - 2017-12-09 22:16:48.754884: step 63120, loss = 0.83, batch loss = 0.70 (9.1 examples/sec; 0.879 sec/batch; 65h:45m:22s remains)
INFO - root - 2017-12-09 22:16:57.278377: step 63130, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.894 sec/batch; 66h:52m:51s remains)
INFO - root - 2017-12-09 22:17:06.042535: step 63140, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 64h:31m:49s remains)
INFO - root - 2017-12-09 22:17:14.728431: step 63150, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.850 sec/batch; 63h:34m:25s remains)
INFO - root - 2017-12-09 22:17:23.379207: step 63160, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.835 sec/batch; 62h:28m:11s remains)
INFO - root - 2017-12-09 22:17:31.755215: step 63170, loss = 0.82, batch loss = 0.69 (10.7 examples/sec; 0.747 sec/batch; 55h:53m:05s remains)
INFO - root - 2017-12-09 22:17:40.387568: step 63180, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 64h:55m:38s remains)
INFO - root - 2017-12-09 22:17:49.226247: step 63190, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.900 sec/batch; 67h:18m:44s remains)
INFO - root - 2017-12-09 22:17:58.077961: step 63200, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.852 sec/batch; 63h:42m:37s remains)
2017-12-09 22:17:58.928869: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0017661595 -0.0017455363 -0.0017223877 -0.0017091741 -0.0016859955 -0.0016287212 -0.0015238998 -0.0013468915 -0.0010845948 -0.000713155 -0.00029360119 0.00012793706 0.00046372612 0.00066454255 0.00067455077][-0.0017604388 -0.0017342172 -0.0017033059 -0.0016775317 -0.001643594 -0.0015706214 -0.0014262238 -0.0011866263 -0.00081642275 -0.00029581261 0.00029579282 0.00086790475 0.0012990983 0.0015222315 0.0014829106][-0.0017259703 -0.0017022276 -0.001677527 -0.0016546679 -0.001619665 -0.001545027 -0.0013909728 -0.0010963305 -0.00062696787 1.8246006e-05 0.00075055764 0.0014698001 0.0020034644 0.002252995 0.0021674028][-0.0016535034 -0.0016271736 -0.0016128849 -0.0016037135 -0.0015821534 -0.0015241751 -0.0013796359 -0.0010586079 -0.00050257344 0.00027747604 0.0011566599 0.0020222943 0.002647025 0.0029016691 0.0027393671][-0.0015566908 -0.001523765 -0.001511628 -0.0015102015 -0.0014981873 -0.0014527425 -0.0013064746 -0.00095166289 -0.00031188095 0.0006020969 0.0016245031 0.0026149163 0.0033123326 0.0035563787 0.0033105225][-0.0015423218 -0.0014637839 -0.0014034649 -0.0013787184 -0.0013428836 -0.001279878 -0.0010851362 -0.00065188936 9.6752425e-05 0.0011478093 0.0022815787 0.0033423021 0.0040420578 0.0042428961 0.0039175693][-0.0015829654 -0.001471811 -0.0013581457 -0.0012743421 -0.0011683467 -0.0010143779 -0.00069433881 -0.00011241483 0.00077486306 0.0019317913 0.0031391969 0.0042104186 0.0048882235 0.0050727618 0.0047524543][-0.0016024649 -0.0014923874 -0.0013621945 -0.0012279644 -0.0010302118 -0.00072391855 -0.00020381401 0.00056731107 0.0015863598 0.0027938751 0.0039807875 0.0049729659 0.0055928426 0.0057657757 0.0054917913][-0.0016174637 -0.0015094934 -0.0013722371 -0.0011926161 -0.00090207742 -0.00042583863 0.00029587734 0.001261301 0.002402897 0.0036071786 0.0046907393 0.00553678 0.0060353964 0.0061674318 0.0059665707][-0.0016423964 -0.0015357871 -0.0013828296 -0.0011475382 -0.0007580485 -0.00014717504 0.00069231295 0.0017538591 0.00290662 0.0040260623 0.004962238 0.0056485124 0.0060638771 0.0062006633 0.0061056013][-0.0016645787 -0.0015502071 -0.0013711224 -0.0010717134 -0.00059604854 9.6241827e-05 0.0010049342 0.0020301412 0.003072734 0.004008085 0.0047415462 0.005291352 0.005651793 0.0058994158 0.0059880516][-0.0017004805 -0.0015766259 -0.001375379 -0.0010250764 -0.0005097352 0.00017595373 0.0010036881 0.0019027746 0.0027732751 0.0035541337 0.0041941544 0.0047343229 0.0051683211 0.0055419211 0.0058141113][-0.0017666775 -0.0016666396 -0.0014720835 -0.0011349362 -0.000663443 -7.4377866e-05 0.00058315124 0.0012650384 0.00194638 0.002608079 0.0032388214 0.0039117504 0.0045586713 0.0051662927 0.0056403861][-0.0018133682 -0.0017535702 -0.0016176074 -0.0013778338 -0.001036783 -0.00061459618 -0.00015743077 0.00033279543 0.00086873781 0.0014844482 0.0022074743 0.0030392986 0.0039056693 0.00474393 0.0054190708][-0.0018282421 -0.0018038657 -0.0017311292 -0.0016047695 -0.001423985 -0.0011877788 -0.00091002474 -0.0005741358 -0.00011501217 0.0005033185 0.0013183967 0.0022926843 0.0033614603 0.0043569328 0.0051384084]]...]
INFO - root - 2017-12-09 22:18:07.576985: step 63210, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.863 sec/batch; 64h:32m:50s remains)
INFO - root - 2017-12-09 22:18:16.377822: step 63220, loss = 0.82, batch loss = 0.69 (8.2 examples/sec; 0.976 sec/batch; 72h:58m:58s remains)
INFO - root - 2017-12-09 22:18:24.968678: step 63230, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.870 sec/batch; 65h:05m:54s remains)
INFO - root - 2017-12-09 22:18:33.582124: step 63240, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.861 sec/batch; 64h:26m:01s remains)
INFO - root - 2017-12-09 22:18:42.176732: step 63250, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.858 sec/batch; 64h:10m:33s remains)
INFO - root - 2017-12-09 22:18:50.874961: step 63260, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.874 sec/batch; 65h:22m:04s remains)
INFO - root - 2017-12-09 22:18:59.412936: step 63270, loss = 0.82, batch loss = 0.69 (10.5 examples/sec; 0.765 sec/batch; 57h:12m:37s remains)
INFO - root - 2017-12-09 22:19:07.982692: step 63280, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.857 sec/batch; 64h:05m:28s remains)
INFO - root - 2017-12-09 22:19:16.534123: step 63290, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.854 sec/batch; 63h:53m:14s remains)
INFO - root - 2017-12-09 22:19:25.248355: step 63300, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.874 sec/batch; 65h:23m:12s remains)
2017-12-09 22:19:26.134226: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.33262557 0.33020225 0.32622024 0.32221895 0.31783786 0.31184262 0.30588272 0.30092812 0.29692075 0.29084638 0.28450179 0.27920747 0.27354702 0.26685914 0.25997603][0.32918453 0.32719252 0.32416183 0.32110023 0.31755176 0.31237704 0.30728725 0.30301768 0.29941648 0.29501539 0.29035467 0.28645229 0.28208193 0.27648708 0.26986203][0.32924026 0.32700166 0.3239733 0.32084057 0.31752506 0.31311819 0.30847982 0.3041954 0.30008671 0.29595524 0.29203719 0.28869924 0.28500077 0.28049976 0.27486518][0.33496207 0.33216158 0.32890195 0.3258633 0.32258704 0.31853163 0.31440529 0.31053516 0.30626619 0.30219856 0.298649 0.29582125 0.29265118 0.28872091 0.28384998][0.3442584 0.34101963 0.33682254 0.33325952 0.32999647 0.32668439 0.32321605 0.31952956 0.31508291 0.31118855 0.30826747 0.305902 0.30330873 0.30060649 0.29705814][0.35382721 0.35032591 0.34479308 0.34031856 0.3367573 0.33416972 0.33178741 0.32936752 0.32592282 0.32253742 0.31996718 0.31812784 0.31592068 0.3148399 0.31199715][0.3573713 0.35434607 0.34755197 0.34226611 0.33892518 0.33773604 0.33717582 0.33647981 0.33488584 0.33320364 0.33181137 0.33055717 0.32888269 0.32882467 0.32674429][0.35486245 0.35247371 0.34517708 0.33972853 0.33721271 0.33778816 0.33902174 0.3398132 0.33956724 0.33904818 0.33844271 0.33826816 0.3377445 0.33872223 0.33753502][0.34329611 0.34213912 0.33465376 0.33014625 0.32951468 0.33234334 0.33532539 0.33820575 0.3399744 0.34109822 0.34180227 0.34274703 0.34291929 0.3445605 0.3438049][0.31994191 0.32052571 0.31446469 0.31178275 0.31410319 0.31992063 0.32528612 0.32995209 0.33350849 0.33601582 0.33773983 0.33963671 0.34062368 0.34277272 0.34237385][0.28388497 0.28640786 0.28295174 0.28425404 0.29050764 0.30036542 0.30925253 0.31662449 0.32235759 0.32564846 0.32819891 0.33027187 0.33124143 0.33303925 0.33226168][0.23827742 0.24263749 0.24262615 0.24803984 0.25878969 0.27296656 0.28500837 0.29472211 0.3027643 0.30787832 0.31183732 0.31451669 0.31675193 0.31820071 0.31762764][0.18909706 0.19544782 0.19957702 0.20980082 0.22481027 0.24232671 0.25717759 0.26862743 0.27831578 0.28525954 0.29057238 0.29440781 0.29754707 0.29952011 0.29918337][0.14015824 0.14785466 0.15540391 0.16955519 0.18811682 0.20857054 0.22636235 0.24054143 0.25313717 0.26218194 0.26925242 0.27434763 0.27792081 0.27971166 0.2792148][0.093719259 0.10166053 0.11163282 0.12859295 0.14978358 0.17262451 0.19336239 0.21056239 0.226048 0.23765428 0.24652112 0.25258711 0.25650913 0.25795797 0.25688064]]...]
INFO - root - 2017-12-09 22:19:34.891075: step 63310, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.904 sec/batch; 67h:35m:06s remains)
INFO - root - 2017-12-09 22:19:43.688438: step 63320, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.852 sec/batch; 63h:41m:57s remains)
INFO - root - 2017-12-09 22:19:52.300715: step 63330, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 64h:47m:37s remains)
INFO - root - 2017-12-09 22:20:00.965340: step 63340, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 65h:00m:17s remains)
INFO - root - 2017-12-09 22:20:09.675465: step 63350, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.880 sec/batch; 65h:46m:20s remains)
INFO - root - 2017-12-09 22:20:18.415351: step 63360, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.885 sec/batch; 66h:11m:44s remains)
INFO - root - 2017-12-09 22:20:27.027043: step 63370, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.875 sec/batch; 65h:24m:51s remains)
INFO - root - 2017-12-09 22:20:35.572385: step 63380, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.878 sec/batch; 65h:38m:43s remains)
INFO - root - 2017-12-09 22:20:44.233039: step 63390, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.843 sec/batch; 63h:02m:45s remains)
INFO - root - 2017-12-09 22:20:52.950139: step 63400, loss = 0.82, batch loss = 0.70 (9.1 examples/sec; 0.882 sec/batch; 65h:54m:51s remains)
2017-12-09 22:20:53.876637: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0018292966 -0.0018284005 -0.0018250287 -0.0018167665 -0.0018041263 -0.0017888776 -0.0017771665 -0.0017739746 -0.0017867612 -0.0018063053 -0.0018221256 -0.0018281876 -0.0018291214 -0.0018289635 -0.0018280062][-0.0018268694 -0.0018200999 -0.0018053724 -0.0017793684 -0.0017435242 -0.0017033599 -0.0016761221 -0.0016781859 -0.0017152539 -0.0017667019 -0.0018061171 -0.0018254315 -0.0018291537 -0.0018290053 -0.0018282725][-0.0018097272 -0.0017901104 -0.0017612602 -0.001717834 -0.0016551137 -0.0015823756 -0.0015327325 -0.0015440072 -0.0016185218 -0.0017160709 -0.001787457 -0.0018218172 -0.0018286796 -0.0018278147 -0.0018266649][-0.00178804 -0.0017621893 -0.0017261035 -0.0016682227 -0.0015749439 -0.0014631241 -0.0013878898 -0.0014078827 -0.0015208572 -0.0016637406 -0.0017659466 -0.0018130469 -0.0018246828 -0.0018247641 -0.001822494][-0.0017849777 -0.0017603587 -0.0017239477 -0.001653853 -0.0015329249 -0.0013848487 -0.00128301 -0.0013003089 -0.0014332299 -0.0016058083 -0.001732648 -0.0017928665 -0.0018105258 -0.0018129938 -0.001811887][-0.0017975458 -0.001775326 -0.0017368458 -0.001656881 -0.0015150902 -0.0013393916 -0.0012109955 -0.0012118192 -0.0013413508 -0.0015223117 -0.0016666928 -0.0017456328 -0.001778863 -0.0017891168 -0.0017900808][-0.0018144074 -0.0017939155 -0.0017484443 -0.0016533124 -0.0014916891 -0.001297069 -0.0011496661 -0.0011309246 -0.0012406523 -0.0014087774 -0.0015517984 -0.0016382892 -0.0016824293 -0.0016997864 -0.0017004859][-0.0018255009 -0.0018056674 -0.0017560824 -0.0016539885 -0.0014854674 -0.0012814249 -0.0011151107 -0.0010623857 -0.001126949 -0.0012528168 -0.0013668692 -0.0014402346 -0.0014787226 -0.0014920492 -0.0014843664][-0.0018273594 -0.0018066816 -0.0017572294 -0.0016585188 -0.0014944714 -0.001283864 -0.0010855061 -0.00097358366 -0.0009671289 -0.0010295785 -0.0010978931 -0.001145413 -0.001171165 -0.0011791101 -0.0011685113][-0.0018256621 -0.0018027712 -0.0017529139 -0.0016565283 -0.0014903669 -0.0012574391 -0.0010060173 -0.00081789494 -0.00073267461 -0.00073482841 -0.00076711015 -0.00079969887 -0.00082505646 -0.0008457182 -0.00085950841][-0.0018243745 -0.0018022201 -0.0017477149 -0.0016376621 -0.0014432038 -0.0011612112 -0.00084314635 -0.00058634568 -0.00044677022 -0.00041944289 -0.00044561818 -0.00049065659 -0.00054230366 -0.00060412008 -0.00067328266][-0.0018258637 -0.0018039483 -0.0017420313 -0.0016093377 -0.0013727159 -0.0010364002 -0.00066723383 -0.00037375349 -0.00021804823 -0.00019559031 -0.00024601596 -0.00033018971 -0.00043170282 -0.00055355683 -0.00068968127][-0.0018277895 -0.0018053092 -0.0017392361 -0.0015923864 -0.001330549 -0.00096496439 -0.00057699729 -0.00028072624 -0.000140029 -0.00014752243 -0.00024302723 -0.0003821213 -0.00054029759 -0.000716144 -0.00090123195][-0.0018293282 -0.0018082507 -0.001746327 -0.0016056716 -0.0013530178 -0.0010004202 -0.00062966277 -0.000355514 -0.00024631876 -0.00029573916 -0.00044222979 -0.00063322333 -0.00083377911 -0.0010331727 -0.0012214072][-0.0018308435 -0.0018143085 -0.0017656946 -0.0016511906 -0.0014396794 -0.0011383491 -0.00081841357 -0.000587799 -0.0005168342 -0.00060346804 -0.0007841296 -0.00099715113 -0.0011998976 -0.0013769345 -0.0015220416]]...]
INFO - root - 2017-12-09 22:21:02.651548: step 63410, loss = 0.81, batch loss = 0.68 (9.1 examples/sec; 0.883 sec/batch; 66h:02m:07s remains)
INFO - root - 2017-12-09 22:21:11.278334: step 63420, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.844 sec/batch; 63h:03m:29s remains)
INFO - root - 2017-12-09 22:21:19.957679: step 63430, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.878 sec/batch; 65h:37m:09s remains)
INFO - root - 2017-12-09 22:21:28.938674: step 63440, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.892 sec/batch; 66h:41m:09s remains)
INFO - root - 2017-12-09 22:21:37.573212: step 63450, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.873 sec/batch; 65h:15m:10s remains)
INFO - root - 2017-12-09 22:21:46.163437: step 63460, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.861 sec/batch; 64h:22m:20s remains)
INFO - root - 2017-12-09 22:21:54.668291: step 63470, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.849 sec/batch; 63h:24m:48s remains)
INFO - root - 2017-12-09 22:22:03.176520: step 63480, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.848 sec/batch; 63h:23m:18s remains)
INFO - root - 2017-12-09 22:22:11.802565: step 63490, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.879 sec/batch; 65h:43m:12s remains)
INFO - root - 2017-12-09 22:22:20.707269: step 63500, loss = 0.82, batch loss = 0.69 (8.8 examples/sec; 0.913 sec/batch; 68h:15m:07s remains)
2017-12-09 22:22:21.601624: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.0026525161 0.0026008533 0.0024239062 0.0019950755 0.0014879777 0.0013006345 0.0017315898 0.0029840008 0.0049838722 0.007390847 0.00978356 0.011942023 0.013767067 0.01570091 0.017921841][0.0037950729 0.0034754798 0.0032115239 0.0027971067 0.0023153033 0.0020082602 0.0023462097 0.0036501382 0.0060367514 0.0092411675 0.012753358 0.016118051 0.019062748 0.021840282 0.024621958][0.005870509 0.00501451 0.004388555 0.003864998 0.0033883802 0.0031183558 0.0035150824 0.0050108572 0.00773833 0.011596041 0.01598423 0.020338899 0.024222253 0.02766379 0.03070312][0.0093398923 0.0079020467 0.0067775068 0.0059223468 0.0053044045 0.005047217 0.0054834634 0.00705896 0.00998378 0.014234486 0.019153688 0.024151662 0.028650139 0.032374751 0.035345294][0.014457741 0.012795411 0.011222468 0.00996275 0.0091019347 0.0086689191 0.0088801272 0.010190858 0.012836781 0.01687531 0.021765413 0.026803898 0.031306669 0.034890354 0.037516382][0.020685241 0.019296078 0.017507691 0.015889131 0.014643145 0.013737009 0.013429566 0.014084657 0.016031783 0.01928385 0.023450818 0.02786286 0.03193089 0.035134882 0.0372494][0.027623329 0.026870454 0.025055543 0.023227632 0.0215212 0.019912131 0.018750791 0.01835398 0.019164374 0.021208864 0.024132593 0.027358359 0.030460354 0.03293879 0.034526043][0.033772998 0.034130953 0.032621738 0.030584965 0.02825935 0.025793631 0.023547113 0.02190748 0.021406054 0.022060582 0.023496976 0.025314696 0.02720941 0.028788324 0.029814146][0.037957042 0.039282206 0.038069956 0.035869151 0.032921016 0.029528715 0.026182825 0.023396868 0.021708066 0.021091137 0.021181673 0.021640589 0.022335891 0.023119802 0.023726238][0.03978616 0.041650087 0.040493935 0.037935872 0.034259547 0.029947594 0.025649302 0.022029355 0.019625654 0.018216362 0.017380491 0.016790505 0.016585618 0.016736086 0.017050648][0.03922502 0.0410731 0.039663445 0.036615282 0.032275897 0.027189892 0.022192081 0.018046539 0.015332597 0.013620527 0.012462746 0.011426211 0.010772295 0.010576867 0.01078529][0.036628116 0.038103029 0.036241487 0.032679543 0.02786959 0.022357149 0.017072426 0.012764662 0.01008568 0.0084691057 0.00744243 0.00646063 0.0058004376 0.0055275019 0.0057161492][0.032307316 0.03322845 0.030891165 0.026950387 0.022006989 0.016629701 0.011609265 0.0075642173 0.0051546041 0.0037993486 0.0030721505 0.0023949556 0.0019885898 0.0018818317 0.0021350011][0.02679019 0.027249115 0.024621377 0.020560363 0.015852885 0.011115792 0.0068532438 0.0034917668 0.0015469718 0.00055532774 0.00014880172 -0.00018312619 -0.00034216908 -0.00032654649 -6.9539296e-05][0.020700961 0.02095487 0.018389164 0.01454221 0.010372492 0.0065680719 0.0033444217 0.00090296345 -0.00049149268 -0.0011292512 -0.0012970879 -0.0013998421 -0.001420331 -0.0013666406 -0.0011992652]]...]
INFO - root - 2017-12-09 22:22:30.237124: step 63510, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.900 sec/batch; 67h:15m:49s remains)
INFO - root - 2017-12-09 22:22:38.881241: step 63520, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.863 sec/batch; 64h:27m:50s remains)
INFO - root - 2017-12-09 22:22:47.334921: step 63530, loss = 0.82, batch loss = 0.69 (9.9 examples/sec; 0.812 sec/batch; 60h:40m:45s remains)
INFO - root - 2017-12-09 22:22:55.962149: step 63540, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.887 sec/batch; 66h:15m:39s remains)
INFO - root - 2017-12-09 22:23:04.578830: step 63550, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 64h:34m:47s remains)
INFO - root - 2017-12-09 22:23:13.224177: step 63560, loss = 0.83, batch loss = 0.70 (9.3 examples/sec; 0.857 sec/batch; 64h:03m:29s remains)
INFO - root - 2017-12-09 22:23:21.771554: step 63570, loss = 0.81, batch loss = 0.68 (9.3 examples/sec; 0.861 sec/batch; 64h:19m:40s remains)
INFO - root - 2017-12-09 22:23:30.105670: step 63580, loss = 0.81, batch loss = 0.68 (9.1 examples/sec; 0.876 sec/batch; 65h:28m:06s remains)
INFO - root - 2017-12-09 22:23:38.733400: step 63590, loss = 0.83, batch loss = 0.70 (9.4 examples/sec; 0.851 sec/batch; 63h:32m:01s remains)
INFO - root - 2017-12-09 22:23:47.479854: step 63600, loss = 0.82, batch loss = 0.69 (9.7 examples/sec; 0.829 sec/batch; 61h:53m:16s remains)
2017-12-09 22:23:48.392382: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.14998986 0.14318901 0.13843209 0.13688457 0.13800608 0.14162855 0.14526132 0.14749275 0.14590788 0.13806795 0.12279063 0.10164356 0.075849324 0.050045498 0.027735315][0.14199342 0.13395551 0.1292274 0.12885286 0.13079323 0.13379516 0.13584159 0.1369677 0.13531515 0.12782799 0.11296955 0.092682116 0.068155341 0.044265561 0.023983704][0.12933026 0.12018985 0.11541598 0.11590538 0.11916027 0.12276273 0.12444871 0.12486746 0.1227194 0.11567967 0.10194506 0.08315473 0.060615636 0.038737364 0.020875057][0.11655068 0.10633079 0.10152828 0.10305347 0.10802581 0.11283335 0.11492123 0.11480794 0.11218142 0.10508125 0.091430195 0.073610663 0.052974835 0.033654414 0.018088045][0.10457996 0.094038613 0.08950524 0.092377663 0.099303827 0.10572769 0.10867535 0.10845591 0.10477277 0.096581094 0.082677983 0.0653681 0.045931797 0.028103845 0.014585041][0.092637688 0.083006196 0.079525396 0.0840388 0.09281794 0.10063475 0.10402849 0.10338733 0.098232746 0.088441171 0.073983967 0.057252429 0.039847545 0.02382078 0.012211026][0.083013237 0.074878685 0.072335884 0.078189492 0.088265777 0.097063392 0.10088767 0.099274009 0.092219964 0.080401354 0.065023795 0.048513278 0.032572381 0.018945582 0.0096300049][0.079039522 0.072160833 0.070210576 0.076268554 0.085938938 0.094443545 0.0976116 0.09482269 0.085354678 0.071195975 0.054977655 0.039088894 0.025515094 0.014720756 0.0086196922][0.0794352 0.074263789 0.072113432 0.077361315 0.085595667 0.092148431 0.093707629 0.089152135 0.0780205 0.062452793 0.045907281 0.031352151 0.020034323 0.012312718 0.008515141][0.084649056 0.080856428 0.077643305 0.080785744 0.086395189 0.090544149 0.089911 0.083283477 0.07012783 0.054005779 0.037972234 0.025003131 0.016582847 0.012006974 0.011106108][0.09212 0.089298926 0.084386528 0.084638506 0.086859554 0.087813839 0.084625892 0.076579183 0.062881038 0.046784878 0.031510439 0.02004526 0.013924834 0.012557976 0.014689717][0.10025401 0.097513385 0.0903415 0.087176412 0.086210378 0.084547095 0.079003 0.069256887 0.054944683 0.039524272 0.025513459 0.016097885 0.012089762 0.013638494 0.019087728][0.10779547 0.10395031 0.094403975 0.08801467 0.083652087 0.078897715 0.071711749 0.061664358 0.048132874 0.034372494 0.022400087 0.014804686 0.01221715 0.015672227 0.023826407][0.11125998 0.10765325 0.097174168 0.088651024 0.081975952 0.074870415 0.06610468 0.055563271 0.043206241 0.031601548 0.021506567 0.015295972 0.013610071 0.018237524 0.027942261][0.11151571 0.10823724 0.09768714 0.088216633 0.080166936 0.071850009 0.062350914 0.051604483 0.039966479 0.029855955 0.021118602 0.015979527 0.015253625 0.020387979 0.030981191]]...]
INFO - root - 2017-12-09 22:23:57.153275: step 63610, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.878 sec/batch; 65h:35m:21s remains)
INFO - root - 2017-12-09 22:24:05.742379: step 63620, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.857 sec/batch; 64h:02m:35s remains)
INFO - root - 2017-12-09 22:24:14.266889: step 63630, loss = 0.82, batch loss = 0.69 (9.7 examples/sec; 0.822 sec/batch; 61h:23m:52s remains)
INFO - root - 2017-12-09 22:24:22.725243: step 63640, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.839 sec/batch; 62h:37m:28s remains)
INFO - root - 2017-12-09 22:24:31.273264: step 63650, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.844 sec/batch; 63h:00m:52s remains)
INFO - root - 2017-12-09 22:24:39.801178: step 63660, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.852 sec/batch; 63h:39m:10s remains)
INFO - root - 2017-12-09 22:24:48.291361: step 63670, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.851 sec/batch; 63h:33m:37s remains)
INFO - root - 2017-12-09 22:24:56.564086: step 63680, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.872 sec/batch; 65h:08m:28s remains)
INFO - root - 2017-12-09 22:25:05.196650: step 63690, loss = 0.81, batch loss = 0.68 (9.3 examples/sec; 0.856 sec/batch; 63h:53m:46s remains)
INFO - root - 2017-12-09 22:25:13.774490: step 63700, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.866 sec/batch; 64h:39m:03s remains)
2017-12-09 22:25:14.622250: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.002907929 0.003289145 0.0034816414 0.0035501798 0.0033258493 0.0026173615 0.001476609 0.00020861614 -0.00079243514 -0.0013999491 -0.0017013932 -0.0017955626 -0.001819792 -0.0018217204 -0.0018237154][0.0028069806 0.003417498 0.0038135247 0.0040232311 0.0038271616 0.0030109137 0.0016975292 0.00024861272 -0.00087101234 -0.0014732269 -0.0017312266 -0.0018003596 -0.0018180554 -0.001820234 -0.0018223481][0.002598966 0.0033541485 0.0039000353 0.0042477287 0.004132384 0.0033274638 0.0019211957 0.00032553461 -0.00092076708 -0.0015426991 -0.0017626722 -0.0018046942 -0.0018166784 -0.0018186048 -0.0018205196][0.0022320063 0.0030470239 0.0037224917 0.0042684316 0.0043157269 0.0035900143 0.0021374086 0.00043853174 -0.00092796196 -0.0015912217 -0.0017905842 -0.0018096311 -0.0018159876 -0.0018177349 -0.0018199108][0.0017248784 0.0025551142 0.0033285422 0.0040163686 0.004254682 0.0037066187 0.0023082434 0.00056646846 -0.00088818511 -0.0016092065 -0.0018054076 -0.0018130133 -0.0018159645 -0.0018173212 -0.0018190147][0.0010883404 0.0019484324 0.0027987668 0.0035840455 0.0039660153 0.0036056177 0.0023752581 0.00068684074 -0.00081895827 -0.0015980978 -0.0018037006 -0.001808901 -0.0018120179 -0.0018150951 -0.0018168263][0.00043477479 0.0012882754 0.0022121719 0.0030512926 0.0035262192 0.0033432655 0.0023017596 0.00073156494 -0.00075673533 -0.0015749549 -0.0017917798 -0.001796504 -0.0017999124 -0.0018054058 -0.0018076082][-0.0001862162 0.000663638 0.0015725197 0.0024073338 0.0029534269 0.002933451 0.0021113683 0.00069258816 -0.00074103882 -0.001557858 -0.0017741064 -0.0017749798 -0.0017787083 -0.0017879853 -0.0017893094][-0.00065641908 0.0001225333 0.00097033067 0.0017368017 0.0022584102 0.0023648939 0.0017628529 0.00053889479 -0.00077858358 -0.0015615495 -0.0017693661 -0.0017627147 -0.0017594004 -0.001765575 -0.0017625618][-0.00098785758 -0.00033340137 0.00038652832 0.0010520051 0.0015238294 0.0016814702 0.0012427353 0.00023789413 -0.00089740788 -0.0015902938 -0.001773703 -0.0017618097 -0.0017558128 -0.0017566661 -0.0017444275][-0.0012235742 -0.000716483 -0.00013467832 0.00041370012 0.00081457116 0.00096573017 0.00062979723 -0.00016247761 -0.0010801102 -0.0016522296 -0.0018119699 -0.0017938636 -0.0017734182 -0.0017550458 -0.0017254644][-0.0014147498 -0.0010616574 -0.00062117795 -0.00017708971 0.00016601302 0.00028599857 1.7409679e-05 -0.00059338519 -0.001277802 -0.0016902582 -0.0018098825 -0.0018149557 -0.0018138351 -0.0017861256 -0.0017283012][-0.001581419 -0.00136842 -0.0010702729 -0.00073297974 -0.00044341281 -0.00034714537 -0.00054184697 -0.00098084833 -0.0014506704 -0.001713172 -0.0017862172 -0.0017932036 -0.001799008 -0.0017671043 -0.0016865229][-0.0017136763 -0.0016130405 -0.0014421911 -0.0012195333 -0.00099372165 -0.00090787973 -0.0010244531 -0.0012996465 -0.001578778 -0.001723443 -0.0017491251 -0.0017510147 -0.0017648553 -0.0017250119 -0.0015864206][-0.0018053831 -0.0017721197 -0.0016951613 -0.0015830543 -0.0014441723 -0.0013660435 -0.0013893989 -0.0015193655 -0.0016541549 -0.0017153024 -0.0017152183 -0.0017027584 -0.0016857395 -0.0016226107 -0.0013887223]]...]
INFO - root - 2017-12-09 22:25:23.192397: step 63710, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.837 sec/batch; 62h:27m:59s remains)
INFO - root - 2017-12-09 22:25:31.799493: step 63720, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.860 sec/batch; 64h:11m:56s remains)
INFO - root - 2017-12-09 22:25:40.193651: step 63730, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.876 sec/batch; 65h:23m:31s remains)
INFO - root - 2017-12-09 22:25:48.834002: step 63740, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.873 sec/batch; 65h:10m:18s remains)
INFO - root - 2017-12-09 22:25:57.271415: step 63750, loss = 0.82, batch loss = 0.69 (9.9 examples/sec; 0.809 sec/batch; 60h:21m:44s remains)
INFO - root - 2017-12-09 22:26:05.796645: step 63760, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.846 sec/batch; 63h:07m:36s remains)
INFO - root - 2017-12-09 22:26:14.432952: step 63770, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.859 sec/batch; 64h:06m:05s remains)
INFO - root - 2017-12-09 22:26:22.828590: step 63780, loss = 0.82, batch loss = 0.69 (8.8 examples/sec; 0.904 sec/batch; 67h:28m:42s remains)
INFO - root - 2017-12-09 22:26:31.282583: step 63790, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.848 sec/batch; 63h:16m:46s remains)
INFO - root - 2017-12-09 22:26:39.986018: step 63800, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 64h:28m:04s remains)
2017-12-09 22:26:40.922837: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.051465604 0.065422438 0.082522631 0.10152066 0.11952867 0.13467085 0.14354411 0.14665709 0.14379658 0.13557531 0.12412908 0.11198716 0.10146158 0.094088323 0.090408087][0.056576781 0.073551394 0.095028743 0.11976322 0.14370848 0.16489583 0.1784927 0.18359287 0.18024749 0.16945416 0.15455237 0.13872457 0.12508583 0.11526483 0.10987412][0.061854109 0.082810596 0.10905184 0.13946483 0.16881087 0.19509359 0.21251021 0.21954446 0.21622488 0.20347917 0.18600649 0.16690913 0.1501122 0.13741572 0.12902959][0.07035207 0.095522843 0.12601897 0.16087453 0.19390047 0.22277123 0.24150476 0.24982852 0.24570093 0.23222472 0.21314843 0.19244643 0.17363772 0.15850721 0.14784466][0.082611986 0.11209781 0.14672619 0.18516946 0.21984413 0.24968937 0.26870674 0.27680057 0.27184176 0.25782815 0.23850371 0.21802853 0.19841538 0.18148857 0.16820297][0.099601686 0.13366526 0.17196582 0.21229009 0.24722622 0.27625388 0.29470217 0.30265984 0.29717526 0.28355733 0.26485184 0.24523653 0.22570446 0.20829186 0.19325303][0.11745409 0.15671773 0.19899978 0.24067289 0.274919 0.30197757 0.31836334 0.32502982 0.3193807 0.30671644 0.28981405 0.2719132 0.25314391 0.23587289 0.22008462][0.13417293 0.17774786 0.22282191 0.2653504 0.29904673 0.32433572 0.33874118 0.3442159 0.33855495 0.32687935 0.31129307 0.29521248 0.2777366 0.26065713 0.24441393][0.14978907 0.19540057 0.24104595 0.28252596 0.31494272 0.33878133 0.35216659 0.35698807 0.35169026 0.34135836 0.32695436 0.31225711 0.29597995 0.279747 0.26449817][0.15999824 0.20531435 0.24957967 0.28902522 0.31976455 0.34260628 0.3561998 0.3613238 0.35760605 0.34842765 0.33531013 0.32112324 0.30502766 0.28925112 0.27406418][0.16172367 0.20483406 0.24577644 0.28168893 0.31013638 0.33200142 0.34578273 0.3522037 0.35065952 0.34350437 0.33192924 0.31819329 0.3024888 0.28705806 0.27247575][0.15481542 0.19365364 0.2299321 0.26161876 0.28689083 0.30667782 0.31967339 0.32678932 0.32737386 0.32259384 0.31347531 0.30159163 0.28743771 0.27284777 0.25908768][0.14125782 0.17427425 0.20458388 0.23110983 0.25272164 0.26981959 0.28148025 0.28835714 0.28983957 0.28708822 0.28019857 0.27088076 0.25949484 0.24726778 0.23550692][0.12058264 0.14717066 0.1712667 0.19232184 0.20966148 0.22362794 0.23354483 0.23979957 0.24175029 0.24052407 0.23581426 0.22898005 0.22018723 0.21098277 0.2018993][0.094368145 0.11478414 0.13310803 0.14914006 0.16231184 0.1728875 0.18057244 0.18581697 0.18801756 0.18789928 0.18520607 0.18072473 0.17440984 0.16761427 0.16057877]]...]
INFO - root - 2017-12-09 22:26:49.437233: step 63810, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.834 sec/batch; 62h:14m:52s remains)
INFO - root - 2017-12-09 22:26:58.107160: step 63820, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.860 sec/batch; 64h:11m:17s remains)
INFO - root - 2017-12-09 22:27:06.778551: step 63830, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.876 sec/batch; 65h:22m:38s remains)
INFO - root - 2017-12-09 22:27:15.535662: step 63840, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.853 sec/batch; 63h:40m:49s remains)
INFO - root - 2017-12-09 22:27:24.325742: step 63850, loss = 0.82, batch loss = 0.69 (7.9 examples/sec; 1.016 sec/batch; 75h:49m:12s remains)
INFO - root - 2017-12-09 22:27:33.187715: step 63860, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 64h:18m:15s remains)
INFO - root - 2017-12-09 22:27:41.841537: step 63870, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.860 sec/batch; 64h:11m:17s remains)
INFO - root - 2017-12-09 22:27:50.153978: step 63880, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.848 sec/batch; 63h:15m:06s remains)
INFO - root - 2017-12-09 22:27:58.876244: step 63890, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.860 sec/batch; 64h:10m:54s remains)
INFO - root - 2017-12-09 22:28:07.728307: step 63900, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.861 sec/batch; 64h:12m:33s remains)
2017-12-09 22:28:08.733165: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0018340187 -0.0018280909 -0.0018217199 -0.0018160236 -0.0018127402 -0.0018131444 -0.0018163442 -0.0018218058 -0.001827747 -0.0018328974 -0.0018367666 -0.0018387219 -0.0018396202 -0.0018398904 -0.0018403349][-0.0018326641 -0.0018267169 -0.0018203005 -0.0018147094 -0.0018113328 -0.0018116655 -0.0018150349 -0.0018208049 -0.0018270756 -0.0018320483 -0.0018355235 -0.0018373933 -0.0018386056 -0.0018389798 -0.001839287][-0.0018329058 -0.0018271279 -0.0018209489 -0.0018155974 -0.0018123287 -0.0018129307 -0.0018163419 -0.0018219834 -0.0018278207 -0.0018324725 -0.0018357412 -0.0018373874 -0.0018386291 -0.001839139 -0.00183972][-0.0018336781 -0.0018283038 -0.001822227 -0.0018170143 -0.0018140414 -0.001814686 -0.0018179672 -0.0018230901 -0.0018285202 -0.0018333024 -0.0018367125 -0.001838558 -0.0018394965 -0.0018396991 -0.0018399011][-0.0018348374 -0.0018299242 -0.0018242771 -0.0018193847 -0.0018163088 -0.0018166089 -0.0018192929 -0.0018240737 -0.0018291974 -0.0018336871 -0.0018370424 -0.0018391102 -0.0018403614 -0.001840691 -0.0018402714][-0.0018358652 -0.0018313437 -0.0018260638 -0.0018213203 -0.0018182026 -0.0018181072 -0.0018200991 -0.0018243263 -0.0018291585 -0.0018338179 -0.00183723 -0.0018391507 -0.0018402747 -0.00184043 -0.0018397977][-0.0018361976 -0.0018317208 -0.0018265235 -0.001821798 -0.0018184636 -0.0018180921 -0.0018198579 -0.001823865 -0.0018287054 -0.0018333446 -0.001836754 -0.0018387786 -0.0018393766 -0.0018389805 -0.0018378642][-0.0018364791 -0.0018316999 -0.001826039 -0.0018205618 -0.0018166084 -0.0018156931 -0.0018171365 -0.00182097 -0.001825937 -0.001830756 -0.0018343939 -0.0018367971 -0.0018375019 -0.0018367682 -0.0018351488][-0.0018361858 -0.0018308157 -0.0018244291 -0.0018180024 -0.0018131657 -0.0018113684 -0.0018125209 -0.0018162971 -0.0018215093 -0.0018264687 -0.0018301294 -0.0018325446 -0.0018333023 -0.0018327493 -0.0018313624][-0.0018350655 -0.0018288415 -0.0018215036 -0.0018143526 -0.0018090187 -0.0018067663 -0.0018076928 -0.00181131 -0.0018162583 -0.0018210445 -0.0018247906 -0.0018272988 -0.0018283635 -0.0018281019 -0.0018271265][-0.0018335732 -0.0018265296 -0.0018180396 -0.0018099782 -0.001804205 -0.0018016816 -0.0018025544 -0.0018059794 -0.0018110134 -0.0018156299 -0.0018191035 -0.0018215369 -0.0018228918 -0.0018232234 -0.0018228506][-0.0018320369 -0.0018242749 -0.0018151229 -0.0018066171 -0.0018005426 -0.0017977925 -0.0017983575 -0.0018014594 -0.0018060926 -0.0018102089 -0.0018137826 -0.0018164567 -0.0018181339 -0.0018189139 -0.0018190946][-0.001831108 -0.0018232744 -0.001814423 -0.0018064426 -0.001800447 -0.0017973989 -0.0017971869 -0.0017993369 -0.0018028583 -0.0018063048 -0.0018094459 -0.0018117375 -0.0018134684 -0.0018146775 -0.0018154765][-0.0018310051 -0.0018235084 -0.0018153072 -0.0018083195 -0.0018030424 -0.0018001058 -0.0017993458 -0.0018006066 -0.0018029187 -0.0018055625 -0.0018081636 -0.0018100096 -0.0018114463 -0.0018121576 -0.0018129728][-0.0018321254 -0.0018255835 -0.0018187547 -0.0018129201 -0.00180832 -0.0018052841 -0.0018039563 -0.0018044424 -0.0018055671 -0.001807141 -0.0018089458 -0.0018100135 -0.0018109696 -0.0018112044 -0.0018117815]]...]
INFO - root - 2017-12-09 22:28:17.252137: step 63910, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 64h:49m:03s remains)
INFO - root - 2017-12-09 22:28:25.877729: step 63920, loss = 0.81, batch loss = 0.68 (8.9 examples/sec; 0.901 sec/batch; 67h:15m:19s remains)
INFO - root - 2017-12-09 22:28:34.454930: step 63930, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.858 sec/batch; 64h:02m:33s remains)
INFO - root - 2017-12-09 22:28:43.225874: step 63940, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.883 sec/batch; 65h:50m:20s remains)
INFO - root - 2017-12-09 22:28:51.822988: step 63950, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.853 sec/batch; 63h:39m:53s remains)
INFO - root - 2017-12-09 22:29:00.467018: step 63960, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.859 sec/batch; 64h:04m:24s remains)
INFO - root - 2017-12-09 22:29:09.294645: step 63970, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.888 sec/batch; 66h:13m:33s remains)
INFO - root - 2017-12-09 22:29:17.510036: step 63980, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.845 sec/batch; 63h:01m:35s remains)
INFO - root - 2017-12-09 22:29:26.104704: step 63990, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.859 sec/batch; 64h:05m:41s remains)
INFO - root - 2017-12-09 22:29:34.972194: step 64000, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.877 sec/batch; 65h:26m:22s remains)
2017-12-09 22:29:35.915706: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0018173897 -0.0018166958 -0.001816717 -0.0018170244 -0.0018175115 -0.0018181209 -0.0018187629 -0.0018193839 -0.0018198868 -0.0018203049 -0.0018205974 -0.0018204683 -0.0018200121 -0.0018192377 -0.0018182584][-0.0018167496 -0.0018162261 -0.0018164804 -0.0018169875 -0.001817684 -0.0018184865 -0.0018192896 -0.0018200113 -0.0018205763 -0.0018209267 -0.0018210065 -0.0018205773 -0.0018197175 -0.0018185048 -0.0018170987][-0.0018167716 -0.0018165217 -0.0018170283 -0.0018177209 -0.0018185428 -0.0018193979 -0.0018202031 -0.0018209086 -0.0018214679 -0.0018216769 -0.0018214541 -0.0018207009 -0.0018194816 -0.0018179108 -0.0018162332][-0.0018169855 -0.0018170798 -0.0018178258 -0.001818704 -0.0018195391 -0.0018202851 -0.0018208702 -0.0018213864 -0.0018218455 -0.0018218644 -0.0018213518 -0.0018203092 -0.0018188341 -0.0018170684 -0.0018153224][-0.0018173468 -0.0018177261 -0.0018186604 -0.0018195959 -0.0018203055 -0.0018208012 -0.0018210423 -0.0018212121 -0.0018214451 -0.0018213077 -0.0018206417 -0.0018195455 -0.0018180574 -0.0018163566 -0.0018147934][-0.0018178785 -0.00181854 -0.0018195683 -0.0018203803 -0.0018207737 -0.0018208002 -0.0018204937 -0.0018202021 -0.0018202147 -0.001820069 -0.0018195324 -0.0018187361 -0.0018175805 -0.0018161447 -0.0018147969][-0.0018187424 -0.0018197598 -0.0018208438 -0.0018214294 -0.0018212751 -0.0018205215 -0.0018194546 -0.0018186424 -0.0018184949 -0.0018185267 -0.0018184892 -0.0018183636 -0.0018178673 -0.0018168942 -0.0018157833][-0.0018199874 -0.0018213297 -0.0018225026 -0.0018228751 -0.0018221443 -0.0018205448 -0.0018186234 -0.0018172541 -0.0018170027 -0.0018173767 -0.0018180443 -0.0018187466 -0.0018190813 -0.0018187359 -0.0018178498][-0.0018212331 -0.0018231182 -0.0018246392 -0.001825009 -0.0018238972 -0.0018216401 -0.0018190097 -0.0018170985 -0.0018167354 -0.0018174287 -0.0018187915 -0.0018203694 -0.0018215801 -0.0018219403 -0.0018213493][-0.0018224854 -0.0018249057 -0.0018269306 -0.0018275883 -0.0018264635 -0.0018240383 -0.0018212358 -0.0018190509 -0.0018183726 -0.0018191009 -0.0018208465 -0.0018230402 -0.0018248439 -0.0018256741 -0.0018252784][-0.001823549 -0.0018264335 -0.0018289293 -0.0018299806 -0.0018290426 -0.0018267662 -0.0018241158 -0.0018219493 -0.0018210786 -0.0018217424 -0.0018236358 -0.001826152 -0.0018282593 -0.0018293001 -0.0018289199][-0.0018239617 -0.0018271005 -0.0018299117 -0.001831347 -0.0018307548 -0.0018289214 -0.0018266871 -0.0018248075 -0.0018239513 -0.0018245913 -0.0018263969 -0.0018287285 -0.0018306719 -0.0018316115 -0.0018312064][-0.0018236696 -0.0018266976 -0.0018294976 -0.0018310816 -0.0018308496 -0.001829586 -0.0018278251 -0.0018261995 -0.0018254261 -0.0018260257 -0.0018276197 -0.0018295909 -0.0018312326 -0.0018320128 -0.0018316386][-0.0018230549 -0.0018255947 -0.0018280334 -0.0018295179 -0.001829535 -0.0018287797 -0.0018274983 -0.0018261067 -0.0018254301 -0.0018259167 -0.0018271732 -0.0018287259 -0.0018300518 -0.0018306602 -0.0018303731][-0.0018223077 -0.0018241015 -0.0018258723 -0.0018269969 -0.0018270134 -0.0018265382 -0.001825664 -0.0018245747 -0.0018240217 -0.0018243722 -0.0018253049 -0.0018264749 -0.0018274804 -0.0018278895 -0.0018276655]]...]
INFO - root - 2017-12-09 22:29:44.611936: step 64010, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.855 sec/batch; 63h:47m:46s remains)
INFO - root - 2017-12-09 22:29:53.160516: step 64020, loss = 0.82, batch loss = 0.69 (10.7 examples/sec; 0.749 sec/batch; 55h:50m:48s remains)
INFO - root - 2017-12-09 22:30:01.812363: step 64030, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.882 sec/batch; 65h:47m:36s remains)
INFO - root - 2017-12-09 22:30:10.529517: step 64040, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 64h:59m:08s remains)
INFO - root - 2017-12-09 22:30:19.252868: step 64050, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 64h:57m:20s remains)
INFO - root - 2017-12-09 22:30:27.761389: step 64060, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.855 sec/batch; 63h:46m:53s remains)
INFO - root - 2017-12-09 22:30:36.452228: step 64070, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.882 sec/batch; 65h:47m:38s remains)
INFO - root - 2017-12-09 22:30:44.759176: step 64080, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.873 sec/batch; 65h:07m:30s remains)
INFO - root - 2017-12-09 22:30:53.398762: step 64090, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.859 sec/batch; 64h:02m:47s remains)
INFO - root - 2017-12-09 22:31:02.030656: step 64100, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 64h:56m:52s remains)
2017-12-09 22:31:02.913623: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.075277559 0.075302064 0.074024491 0.072070442 0.068678305 0.063750871 0.057336479 0.050093006 0.04293504 0.036618996 0.031568728 0.027854506 0.025219655 0.023192802 0.020992197][0.084938452 0.084988341 0.083166242 0.080619968 0.076604351 0.071200691 0.064217679 0.056316976 0.048442751 0.041515507 0.036005341 0.031963624 0.029132005 0.026970396 0.024532491][0.090106063 0.090577573 0.088491946 0.085630454 0.081248619 0.075447395 0.068036646 0.059877139 0.051718213 0.044435505 0.038488496 0.034042809 0.030875873 0.028406961 0.02566326][0.092232607 0.093238115 0.091197476 0.088302314 0.083889 0.078081541 0.070563629 0.062204689 0.053711992 0.046097714 0.039776478 0.034939915 0.03132705 0.028373051 0.025068527][0.09228196 0.094069019 0.09233503 0.089718372 0.085689582 0.080273271 0.07317967 0.065176621 0.056810744 0.04905894 0.042306736 0.036883149 0.032570638 0.028897893 0.0248714][0.091177084 0.093404889 0.091872774 0.089712366 0.086352237 0.081705704 0.075444572 0.068218708 0.060550023 0.053284381 0.046678983 0.041060235 0.0361879 0.031718258 0.026777541][0.087925643 0.0906026 0.0893021 0.087488793 0.084799886 0.081216849 0.076241508 0.070279621 0.063692629 0.057195004 0.0510203 0.045545824 0.040535219 0.035658702 0.030129064][0.081256874 0.0844876 0.0837452 0.082543157 0.080699913 0.078161873 0.074428529 0.069923878 0.064804 0.05956392 0.054327898 0.04944047 0.04464601 0.039639551 0.033775833][0.070442155 0.073935583 0.073676296 0.073177554 0.0722862 0.070941441 0.068642884 0.06561847 0.062015757 0.058325063 0.054524221 0.050792903 0.046854518 0.042362303 0.036708321][0.056664344 0.059793591 0.059781276 0.059753634 0.059565358 0.059218135 0.058173738 0.056647152 0.054747459 0.052700549 0.050467197 0.048123885 0.045347188 0.041715246 0.036723439][0.041760951 0.044018209 0.044001941 0.044125382 0.044232547 0.044358373 0.044081341 0.043674622 0.043112848 0.042476512 0.041663196 0.040537022 0.038857218 0.036223933 0.03226386][0.028022643 0.029417545 0.0294775 0.029664803 0.029881813 0.030135073 0.030152582 0.030207835 0.030239768 0.030265361 0.030113723 0.029625058 0.028656736 0.026804984 0.02390909][0.015840983 0.016667418 0.016947027 0.01728422 0.017651727 0.017962934 0.018147051 0.018397016 0.018683748 0.018988872 0.01911127 0.01894285 0.018384783 0.017137339 0.015146787][0.0061892555 0.0065824487 0.0068780952 0.0072262096 0.00761239 0.0079637552 0.0082742991 0.0085939467 0.0089725954 0.0093862014 0.0096658478 0.0097357742 0.0095194653 0.0088378852 0.0076725013][0.00067214656 0.00081281259 0.00094680057 0.0011232517 0.0013350857 0.0015488396 0.0017809357 0.0020744428 0.0024458319 0.0027976995 0.0030789017 0.003248788 0.0032408531 0.0029556681 0.0023851739]]...]
INFO - root - 2017-12-09 22:31:11.593777: step 64110, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.881 sec/batch; 65h:41m:40s remains)
INFO - root - 2017-12-09 22:31:20.303889: step 64120, loss = 0.82, batch loss = 0.69 (10.0 examples/sec; 0.801 sec/batch; 59h:44m:12s remains)
INFO - root - 2017-12-09 22:31:28.958443: step 64130, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.847 sec/batch; 63h:06m:57s remains)
INFO - root - 2017-12-09 22:31:37.674654: step 64140, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.870 sec/batch; 64h:49m:06s remains)
INFO - root - 2017-12-09 22:31:46.315723: step 64150, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.859 sec/batch; 64h:01m:16s remains)
INFO - root - 2017-12-09 22:31:55.044488: step 64160, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.876 sec/batch; 65h:19m:06s remains)
INFO - root - 2017-12-09 22:32:03.797588: step 64170, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.881 sec/batch; 65h:41m:31s remains)
INFO - root - 2017-12-09 22:32:12.149685: step 64180, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.851 sec/batch; 63h:25m:30s remains)
INFO - root - 2017-12-09 22:32:20.727457: step 64190, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.860 sec/batch; 64h:07m:59s remains)
INFO - root - 2017-12-09 22:32:29.489484: step 64200, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.894 sec/batch; 66h:36m:59s remains)
2017-12-09 22:32:30.353665: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0018224233 -0.0018261666 -0.0018306095 -0.0018334414 -0.0018347143 -0.0018346993 -0.0018341433 -0.0018336036 -0.0018333624 -0.0018332782 -0.001833171 -0.00183314 -0.0018331982 -0.0018333262 -0.0018334304][-0.0018198465 -0.0018233304 -0.0018274944 -0.0018304196 -0.0018320999 -0.0018327548 -0.0018326774 -0.0018323945 -0.0018323269 -0.0018324304 -0.0018323456 -0.0018321211 -0.0018318749 -0.0018316574 -0.0018313985][-0.0018171126 -0.0018200227 -0.0018237815 -0.0018267373 -0.001829035 -0.0018306421 -0.0018312818 -0.0018315002 -0.0018318649 -0.0018323826 -0.0018324198 -0.0018319872 -0.0018313753 -0.0018306617 -0.0018298131][-0.0018143915 -0.0018164236 -0.0018196256 -0.0018226206 -0.0018254208 -0.0018281032 -0.0018298472 -0.0018308606 -0.0018317141 -0.0018325326 -0.0018327165 -0.0018321783 -0.0018314753 -0.001830469 -0.0018290889][-0.0018118265 -0.0018127474 -0.0018153177 -0.0018184507 -0.0018219538 -0.0018257133 -0.0018285749 -0.001830289 -0.0018313845 -0.0018322469 -0.0018324143 -0.0018317875 -0.0018310632 -0.0018301478 -0.0018287092][-0.001809863 -0.0018095636 -0.0018115216 -0.0018145539 -0.0018186336 -0.0018232755 -0.0018270132 -0.0018294204 -0.0018307455 -0.001831591 -0.0018316471 -0.0018308741 -0.0018302589 -0.0018296981 -0.0018283961][-0.0018084826 -0.0018073112 -0.0018087014 -0.0018117661 -0.0018162311 -0.0018212965 -0.0018254898 -0.0018283355 -0.0018298883 -0.001830754 -0.0018307914 -0.0018302674 -0.0018299147 -0.0018296839 -0.0018284885][-0.0018075858 -0.0018056381 -0.0018065873 -0.0018094062 -0.0018139309 -0.0018192098 -0.0018237963 -0.0018270613 -0.0018287501 -0.0018297151 -0.0018298998 -0.001829799 -0.001829769 -0.0018297281 -0.0018286832][-0.0018065363 -0.0018041442 -0.0018045589 -0.0018068524 -0.0018108592 -0.0018157702 -0.0018204207 -0.0018239514 -0.0018258551 -0.0018269416 -0.0018275389 -0.0018279136 -0.0018279903 -0.0018277526 -0.0018267344][-0.0018056094 -0.0018029899 -0.0018028646 -0.0018043666 -0.0018075188 -0.0018114991 -0.0018154895 -0.0018187419 -0.0018206651 -0.0018218742 -0.0018228275 -0.0018237376 -0.0018240078 -0.0018237393 -0.0018228369][-0.0018050981 -0.0018024518 -0.0018016866 -0.0018023408 -0.0018042907 -0.0018070943 -0.0018101835 -0.0018130705 -0.0018150391 -0.0018163241 -0.0018175141 -0.0018186941 -0.001819109 -0.0018187987 -0.0018181199][-0.0018047567 -0.0018021818 -0.0018010361 -0.0018008824 -0.001801746 -0.001803427 -0.0018057424 -0.0018082929 -0.0018104425 -0.0018120277 -0.0018133481 -0.0018144058 -0.0018146909 -0.0018142438 -0.0018135114][-0.001804854 -0.00180241 -0.0018010449 -0.0018003238 -0.0018004809 -0.0018013339 -0.0018028631 -0.0018048416 -0.0018067977 -0.0018082536 -0.0018093693 -0.0018100954 -0.0018101551 -0.001809668 -0.0018090856][-0.0018053434 -0.0018029364 -0.0018015335 -0.0018006072 -0.0018003716 -0.0018007207 -0.0018015186 -0.0018027986 -0.0018042325 -0.0018053404 -0.0018060592 -0.0018064864 -0.0018064018 -0.0018059905 -0.0018055605][-0.0018058142 -0.0018035364 -0.0018021859 -0.0018012503 -0.0018008332 -0.0018008964 -0.0018012391 -0.0018018374 -0.0018025712 -0.0018032144 -0.0018036697 -0.0018039623 -0.0018039709 -0.0018037581 -0.0018035407]]...]
INFO - root - 2017-12-09 22:32:38.939006: step 64210, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 64h:44m:25s remains)
INFO - root - 2017-12-09 22:32:47.648243: step 64220, loss = 0.82, batch loss = 0.69 (10.0 examples/sec; 0.802 sec/batch; 59h:45m:29s remains)
INFO - root - 2017-12-09 22:32:56.434125: step 64230, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 64h:34m:48s remains)
INFO - root - 2017-12-09 22:33:05.074620: step 64240, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.831 sec/batch; 61h:53m:19s remains)
INFO - root - 2017-12-09 22:33:13.752871: step 64250, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 64h:56m:09s remains)
INFO - root - 2017-12-09 22:33:22.466106: step 64260, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.861 sec/batch; 64h:08m:54s remains)
INFO - root - 2017-12-09 22:33:31.051556: step 64270, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.893 sec/batch; 66h:33m:37s remains)
INFO - root - 2017-12-09 22:33:39.416115: step 64280, loss = 0.83, batch loss = 0.70 (9.2 examples/sec; 0.865 sec/batch; 64h:26m:44s remains)
INFO - root - 2017-12-09 22:33:47.934782: step 64290, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.849 sec/batch; 63h:13m:30s remains)
INFO - root - 2017-12-09 22:33:56.663796: step 64300, loss = 0.82, batch loss = 0.69 (8.8 examples/sec; 0.905 sec/batch; 67h:23m:34s remains)
2017-12-09 22:33:57.530746: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.026538553 0.027987402 0.028992012 0.02957337 0.030057169 0.030589998 0.031289931 0.031673785 0.031242549 0.029290788 0.026058955 0.021633223 0.016400455 0.010730047 0.00571217][0.034460932 0.036494143 0.037576783 0.037820812 0.037617158 0.037308816 0.037309375 0.037375383 0.036892079 0.035095386 0.031872515 0.027299998 0.021475814 0.014876415 0.0086160786][0.038886648 0.041477442 0.042931706 0.043448403 0.043356109 0.043013114 0.042825151 0.042513993 0.041531879 0.039388988 0.035863183 0.031203639 0.025215149 0.018168237 0.01111549][0.040315341 0.043358408 0.045350641 0.046478894 0.047003619 0.047218449 0.047413588 0.047227286 0.046195034 0.043715924 0.039710127 0.034603205 0.028156858 0.020665029 0.013005788][0.039525073 0.043515392 0.046597179 0.048916463 0.050576977 0.051704425 0.052448962 0.052428409 0.051341161 0.048413951 0.043752078 0.037824109 0.030649967 0.022565028 0.014336395][0.036063138 0.041583788 0.046420343 0.050452404 0.053601559 0.0557087 0.057031251 0.057212852 0.055954937 0.052432254 0.046987545 0.04013345 0.032060891 0.023351103 0.01474316][0.029721152 0.036676127 0.043366559 0.049259987 0.053946618 0.057102591 0.059055716 0.059498537 0.058202174 0.054433588 0.04844781 0.040858556 0.032061916 0.022889052 0.014163223][0.021580411 0.029082067 0.036723316 0.043664683 0.049376346 0.053337682 0.055941045 0.056957994 0.056001421 0.052513834 0.046585277 0.038941707 0.029953487 0.020812711 0.012473967][0.013523013 0.020166218 0.027424866 0.034241855 0.040146906 0.044515178 0.047671605 0.04934109 0.049019326 0.046406623 0.041252524 0.034280971 0.025808664 0.017347516 0.0099260323][0.0074277134 0.012279037 0.018078629 0.023873275 0.029184675 0.033266984 0.036465306 0.03847234 0.038728669 0.037107214 0.033164561 0.027451254 0.020203825 0.01303907 0.0070015467][0.0034102034 0.0064844065 0.010535612 0.014848267 0.01904186 0.022450326 0.02517575 0.026877565 0.027236687 0.026232691 0.023430865 0.019128775 0.013609407 0.0082748989 0.00398731][0.0010250917 0.00287676 0.005554385 0.0084606986 0.011348397 0.013774398 0.015727153 0.016926451 0.017204894 0.016562974 0.014705925 0.011703239 0.0079177078 0.0043808846 0.001666596][-0.00064085482 0.00035150081 0.0019288148 0.0036680112 0.005456985 0.0069661788 0.0081833676 0.0089104958 0.0090536093 0.0086549418 0.00751006 0.00568477 0.0034635025 0.0014702744 -4.1875755e-06][-0.0015280207 -0.0011361254 -0.00042564375 0.00036936312 0.0012488408 0.0019917032 0.0026236209 0.0030249571 0.003128428 0.0029470581 0.0023674788 0.0014802777 0.00043734384 -0.00045143138 -0.0010863218][-0.001816967 -0.0017599318 -0.001618493 -0.0014092966 -0.0011174041 -0.00086921873 -0.00063569308 -0.00047361234 -0.00040479272 -0.0004478544 -0.00064021628 -0.0009177287 -0.0012258743 -0.0014729535 -0.0016445708]]...]
INFO - root - 2017-12-09 22:34:06.194862: step 64310, loss = 0.82, batch loss = 0.69 (8.4 examples/sec; 0.950 sec/batch; 70h:46m:56s remains)
INFO - root - 2017-12-09 22:34:14.832580: step 64320, loss = 0.82, batch loss = 0.69 (10.7 examples/sec; 0.745 sec/batch; 55h:29m:00s remains)
INFO - root - 2017-12-09 22:34:23.477832: step 64330, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.853 sec/batch; 63h:34m:22s remains)
INFO - root - 2017-12-09 22:34:32.171636: step 64340, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.872 sec/batch; 64h:59m:25s remains)
INFO - root - 2017-12-09 22:34:40.879520: step 64350, loss = 0.82, batch loss = 0.69 (8.8 examples/sec; 0.912 sec/batch; 67h:57m:16s remains)
INFO - root - 2017-12-09 22:34:49.539263: step 64360, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.866 sec/batch; 64h:29m:36s remains)
INFO - root - 2017-12-09 22:34:58.222145: step 64370, loss = 0.81, batch loss = 0.68 (9.3 examples/sec; 0.862 sec/batch; 64h:11m:02s remains)
INFO - root - 2017-12-09 22:35:06.648586: step 64380, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.851 sec/batch; 63h:21m:51s remains)
INFO - root - 2017-12-09 22:35:15.523509: step 64390, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.873 sec/batch; 65h:02m:07s remains)
INFO - root - 2017-12-09 22:35:24.120818: step 64400, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.850 sec/batch; 63h:17m:58s remains)
2017-12-09 22:35:25.076265: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0018211259 -0.001821792 -0.0018234598 -0.0018254753 -0.0018272118 -0.001828248 -0.0018281788 -0.0018272261 -0.001825167 -0.0018224001 -0.0018199037 -0.0018185341 -0.0018183119 -0.0018190324 -0.0018204034][-0.0018227499 -0.0018239918 -0.001825884 -0.0018278451 -0.0018291399 -0.0018293459 -0.0018281144 -0.0018256829 -0.0018221451 -0.0018181866 -0.0018148804 -0.0018132281 -0.0018130419 -0.001814062 -0.0018158634][-0.001825552 -0.0018272577 -0.0018291943 -0.0018309521 -0.0018316901 -0.0018307494 -0.0018281356 -0.0018242806 -0.0018195353 -0.0018145384 -0.0018106343 -0.0018089575 -0.0018089993 -0.0018102411 -0.0018123172][-0.0018282679 -0.0018302691 -0.0018320845 -0.0018333329 -0.0018333386 -0.0018315694 -0.0018281812 -0.0018237174 -0.0018184588 -0.001813204 -0.0018093534 -0.0018078083 -0.0018079949 -0.0018090704 -0.0018109831][-0.0018299983 -0.0018321864 -0.0018338469 -0.0018346512 -0.0018341823 -0.0018322453 -0.0018289561 -0.001824628 -0.0018197851 -0.0018151589 -0.0018118357 -0.0018105123 -0.0018105509 -0.0018114094 -0.0018129633][-0.0018301477 -0.0018323538 -0.0018341263 -0.0018349936 -0.0018345168 -0.0018328934 -0.0018301699 -0.0018265046 -0.0018223445 -0.0018183985 -0.0018156533 -0.0018146285 -0.0018148085 -0.0018157938 -0.0018171917][-0.0018286264 -0.001830783 -0.0018329072 -0.0018343229 -0.001834331 -0.0018333958 -0.00183167 -0.0018289831 -0.0018257648 -0.0018226007 -0.001820466 -0.0018198001 -0.0018204034 -0.001821497 -0.0018229327][-0.0018262618 -0.0018281291 -0.0018304961 -0.0018326415 -0.0018333151 -0.0018329122 -0.0018317896 -0.0018298442 -0.0018275707 -0.0018253857 -0.0018242612 -0.0018243731 -0.0018254058 -0.001826629 -0.0018281984][-0.001823749 -0.0018253876 -0.0018280678 -0.0018304785 -0.001831535 -0.0018316183 -0.0018308067 -0.0018291717 -0.0018273829 -0.0018260271 -0.0018258094 -0.0018262817 -0.0018275226 -0.0018292358 -0.0018312093][-0.0018215049 -0.0018228716 -0.001825498 -0.0018279372 -0.0018290853 -0.0018292725 -0.0018285615 -0.0018271814 -0.0018255962 -0.0018246769 -0.0018248766 -0.0018256626 -0.0018270859 -0.001829156 -0.001831378][-0.0018198533 -0.0018207844 -0.001822865 -0.0018247297 -0.0018256502 -0.0018258966 -0.0018253083 -0.001824039 -0.0018226535 -0.001821895 -0.0018222155 -0.0018231901 -0.0018247882 -0.0018269349 -0.0018293705][-0.0018190332 -0.0018193383 -0.0018205498 -0.0018214805 -0.00182195 -0.0018220746 -0.00182162 -0.0018207041 -0.0018196299 -0.001819057 -0.0018194584 -0.001820519 -0.0018220865 -0.0018240963 -0.0018264456][-0.0018187105 -0.0018184127 -0.0018186645 -0.0018185962 -0.0018184184 -0.0018182558 -0.0018178611 -0.0018173094 -0.0018167722 -0.0018165973 -0.0018171169 -0.0018181454 -0.0018194653 -0.001821024 -0.0018228359][-0.0018189052 -0.0018180828 -0.0018175811 -0.0018167708 -0.0018159573 -0.0018153765 -0.0018149153 -0.0018146795 -0.0018145691 -0.0018147519 -0.0018152847 -0.0018160618 -0.001816938 -0.0018179246 -0.0018191335][-0.0018196028 -0.0018185988 -0.0018176103 -0.0018163302 -0.0018151972 -0.0018143218 -0.0018135778 -0.0018132443 -0.0018133074 -0.0018135319 -0.0018138534 -0.0018142294 -0.0018146429 -0.001815055 -0.0018158666]]...]
INFO - root - 2017-12-09 22:35:33.831047: step 64410, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.857 sec/batch; 63h:49m:05s remains)
INFO - root - 2017-12-09 22:35:42.394527: step 64420, loss = 0.82, batch loss = 0.69 (11.1 examples/sec; 0.724 sec/batch; 53h:54m:38s remains)
INFO - root - 2017-12-09 22:35:50.982729: step 64430, loss = 0.82, batch loss = 0.69 (9.7 examples/sec; 0.828 sec/batch; 61h:39m:16s remains)
INFO - root - 2017-12-09 22:35:59.651679: step 64440, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.852 sec/batch; 63h:26m:20s remains)
INFO - root - 2017-12-09 22:36:08.131921: step 64450, loss = 0.82, batch loss = 0.69 (9.7 examples/sec; 0.825 sec/batch; 61h:25m:28s remains)
INFO - root - 2017-12-09 22:36:16.635617: step 64460, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.870 sec/batch; 64h:45m:28s remains)
INFO - root - 2017-12-09 22:36:25.320950: step 64470, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.881 sec/batch; 65h:34m:03s remains)
INFO - root - 2017-12-09 22:36:33.648801: step 64480, loss = 0.82, batch loss = 0.69 (10.5 examples/sec; 0.765 sec/batch; 56h:58m:52s remains)
INFO - root - 2017-12-09 22:36:42.320756: step 64490, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.858 sec/batch; 63h:51m:48s remains)
INFO - root - 2017-12-09 22:36:51.067766: step 64500, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.850 sec/batch; 63h:18m:07s remains)
2017-12-09 22:36:51.928344: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.1335776 0.13405164 0.13681999 0.14203951 0.14794932 0.15343918 0.15684715 0.15714309 0.15358739 0.14609779 0.136177 0.12587665 0.11677497 0.11045048 0.10668431][0.12400671 0.12514573 0.12843624 0.13380919 0.1401031 0.14608531 0.15013498 0.15105262 0.14800371 0.14105529 0.13109225 0.12004413 0.10972502 0.10201599 0.097322054][0.11209461 0.11425127 0.11806133 0.12364965 0.13000675 0.13604423 0.1403842 0.14184275 0.13956136 0.13310109 0.12335952 0.1121569 0.10113213 0.092177242 0.086040348][0.10178893 0.10517053 0.10958358 0.11518137 0.12122881 0.12689121 0.13108908 0.13270912 0.13092196 0.12518862 0.11609723 0.10512475 0.0938997 0.08400213 0.076820068][0.0923955 0.097547553 0.10290684 0.1086554 0.11430433 0.11929725 0.12300696 0.12444174 0.12292838 0.11793608 0.10963946 0.09912131 0.087838449 0.077143729 0.068625718][0.083985113 0.091042072 0.097533233 0.10356678 0.10886847 0.11317273 0.11616843 0.11714434 0.11565131 0.11106943 0.1034314 0.093345515 0.081954494 0.070402659 0.060503833][0.0752573 0.084160648 0.091930039 0.098463818 0.10360343 0.10736641 0.10968108 0.11002872 0.10827454 0.10380376 0.096609533 0.086792991 0.075200118 0.063008882 0.05195966][0.066033907 0.076408692 0.08539217 0.092546806 0.0977368 0.10100269 0.10254493 0.10208984 0.099736653 0.095090613 0.088025048 0.078410074 0.066887669 0.05457481 0.043088187][0.056164693 0.0674386 0.077288806 0.084950686 0.090283878 0.093201958 0.0940977 0.09286429 0.089870356 0.084936343 0.077954173 0.068628289 0.057426155 0.045437824 0.034119844][0.045972858 0.057154864 0.067113072 0.074868344 0.080112062 0.082741775 0.083160549 0.081408739 0.077996388 0.07289502 0.066123389 0.05730401 0.046854556 0.035783984 0.025398824][0.036682904 0.046803337 0.055986628 0.063102618 0.067729585 0.069803961 0.069717966 0.067633405 0.064045966 0.059118334 0.052904334 0.045120984 0.036067195 0.026652504 0.017932834][0.02842186 0.037001424 0.044926539 0.051004045 0.054713383 0.055992097 0.055281758 0.052890103 0.049258027 0.04472208 0.039373253 0.033005152 0.025811188 0.018481927 0.011838354][0.021257458 0.028219989 0.034693331 0.039531656 0.04217178 0.042537279 0.041189719 0.038505718 0.034966826 0.031017039 0.026756044 0.022034701 0.016881326 0.011736449 0.0071435985][0.014838425 0.020163078 0.025146535 0.028776841 0.030443475 0.030157525 0.02844483 0.025689986 0.022415247 0.019077132 0.015844569 0.012619587 0.0093303379 0.006163836 0.0034042229][0.0098716132 0.013558038 0.017017037 0.019446759 0.020220837 0.019445742 0.017589349 0.015033026 0.012254026 0.0096684527 0.00749014 0.0055997921 0.0038751569 0.0023026853 0.00097138446]]...]
INFO - root - 2017-12-09 22:37:00.574711: step 64510, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.863 sec/batch; 64h:16m:11s remains)
INFO - root - 2017-12-09 22:37:09.162825: step 64520, loss = 0.82, batch loss = 0.69 (10.4 examples/sec; 0.770 sec/batch; 57h:19m:39s remains)
INFO - root - 2017-12-09 22:37:17.887249: step 64530, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.847 sec/batch; 63h:02m:05s remains)
INFO - root - 2017-12-09 22:37:26.688025: step 64540, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.853 sec/batch; 63h:30m:36s remains)
INFO - root - 2017-12-09 22:37:35.495068: step 64550, loss = 0.82, batch loss = 0.69 (8.2 examples/sec; 0.972 sec/batch; 72h:20m:35s remains)
INFO - root - 2017-12-09 22:37:44.367769: step 64560, loss = 0.82, batch loss = 0.69 (8.8 examples/sec; 0.906 sec/batch; 67h:28m:06s remains)
INFO - root - 2017-12-09 22:37:53.112063: step 64570, loss = 0.83, batch loss = 0.70 (9.0 examples/sec; 0.887 sec/batch; 65h:58m:59s remains)
INFO - root - 2017-12-09 22:38:01.504370: step 64580, loss = 0.82, batch loss = 0.69 (11.4 examples/sec; 0.704 sec/batch; 52h:23m:54s remains)
INFO - root - 2017-12-09 22:38:10.170396: step 64590, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.875 sec/batch; 65h:05m:47s remains)
INFO - root - 2017-12-09 22:38:19.016162: step 64600, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.900 sec/batch; 66h:59m:16s remains)
2017-12-09 22:38:19.968669: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0018247769 -0.0018240332 -0.0018240151 -0.0018241205 -0.0018241931 -0.0018242346 -0.0018242848 -0.0018243466 -0.0018243674 -0.0018243343 -0.0018242912 -0.0018242613 -0.0018242144 -0.0018241752 -0.0018241371][-0.001824406 -0.0018237142 -0.0018237275 -0.0018239568 -0.0018242575 -0.0018245743 -0.0018248606 -0.0018250541 -0.0018251006 -0.0018250066 -0.0018247965 -0.0018245443 -0.0018243099 -0.0018241308 -0.0018240225][-0.0018243723 -0.0018238617 -0.0018240217 -0.0018245532 -0.0018252851 -0.0018260801 -0.0018267499 -0.0018271218 -0.0018271148 -0.0018267796 -0.0018262429 -0.0018256223 -0.0018250159 -0.0018245488 -0.0018242893][-0.0018245011 -0.0018241185 -0.0018245633 -0.001825586 -0.0018268781 -0.001828232 -0.0018292742 -0.0018297414 -0.00182961 -0.0018290327 -0.0018281648 -0.001827079 -0.0018259989 -0.0018251092 -0.0018245721][-0.0018247229 -0.0018244593 -0.0018253487 -0.0018269643 -0.0018287967 -0.0018305645 -0.0018317861 -0.001832215 -0.0018318188 -0.0018308626 -0.0018296709 -0.0018282773 -0.0018268815 -0.0018256736 -0.0018248754][-0.0018248991 -0.0018248794 -0.00182624 -0.001828324 -0.0018303912 -0.00183214 -0.0018334213 -0.0018338428 -0.0018331689 -0.0018318139 -0.0018304919 -0.0018290953 -0.0018275575 -0.0018261039 -0.0018250807][-0.0018251233 -0.0018253196 -0.0018270082 -0.001829423 -0.001831545 -0.0018333131 -0.0018345887 -0.001834783 -0.0018335658 -0.001831834 -0.0018305273 -0.0018292025 -0.0018277285 -0.0018262386 -0.0018251047][-0.0018252749 -0.0018255864 -0.001827457 -0.0018300574 -0.0018322595 -0.0018339229 -0.0018349114 -0.0018345214 -0.0018324476 -0.001830457 -0.0018294273 -0.0018285095 -0.0018272928 -0.0018259453 -0.0018249004][-0.0018252531 -0.0018255773 -0.0018274719 -0.0018299626 -0.0018320736 -0.0018334318 -0.0018339383 -0.0018328946 -0.0018303987 -0.0018285527 -0.0018279211 -0.0018274399 -0.0018265013 -0.0018254784 -0.0018246663][-0.0018250999 -0.0018253825 -0.0018270329 -0.0018291734 -0.0018309002 -0.0018318363 -0.0018319887 -0.0018310276 -0.0018289589 -0.0018274005 -0.0018269127 -0.0018265785 -0.0018259092 -0.001825121 -0.0018245031][-0.0018249525 -0.001825108 -0.0018263243 -0.0018279155 -0.0018292007 -0.0018299286 -0.0018301345 -0.0018295805 -0.0018283317 -0.0018270549 -0.0018264528 -0.0018260269 -0.0018254349 -0.0018248174 -0.0018243145][-0.0018248657 -0.0018247867 -0.001825531 -0.0018266165 -0.0018275547 -0.0018281868 -0.0018283817 -0.001828114 -0.0018274721 -0.0018266778 -0.0018261152 -0.0018255938 -0.0018250325 -0.0018244761 -0.0018240971][-0.0018247034 -0.001824448 -0.0018247594 -0.0018253989 -0.0018260895 -0.0018265746 -0.0018267689 -0.0018266599 -0.0018263132 -0.0018259016 -0.001825508 -0.0018250466 -0.001824556 -0.0018241486 -0.0018239042][-0.0018245373 -0.0018241502 -0.0018241747 -0.0018244549 -0.001824855 -0.0018252172 -0.0018253945 -0.0018253297 -0.0018251878 -0.0018250429 -0.0018248151 -0.0018244889 -0.0018241684 -0.0018239338 -0.0018238008][-0.001824478 -0.0018239148 -0.0018237534 -0.0018238309 -0.0018239778 -0.0018241623 -0.0018242911 -0.0018243111 -0.0018243018 -0.0018242943 -0.0018242177 -0.0018240735 -0.0018239312 -0.001823817 -0.0018237549]]...]
INFO - root - 2017-12-09 22:38:28.591699: step 64610, loss = 0.83, batch loss = 0.70 (9.4 examples/sec; 0.847 sec/batch; 63h:02m:56s remains)
INFO - root - 2017-12-09 22:38:37.134237: step 64620, loss = 0.82, batch loss = 0.69 (10.7 examples/sec; 0.746 sec/batch; 55h:30m:55s remains)
INFO - root - 2017-12-09 22:38:45.771380: step 64630, loss = 0.82, batch loss = 0.69 (9.8 examples/sec; 0.815 sec/batch; 60h:40m:08s remains)
INFO - root - 2017-12-09 22:38:54.411242: step 64640, loss = 0.82, batch loss = 0.69 (9.7 examples/sec; 0.825 sec/batch; 61h:23m:40s remains)
INFO - root - 2017-12-09 22:39:03.154270: step 64650, loss = 0.82, batch loss = 0.69 (8.7 examples/sec; 0.915 sec/batch; 68h:05m:08s remains)
INFO - root - 2017-12-09 22:39:12.009365: step 64660, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.892 sec/batch; 66h:22m:02s remains)
INFO - root - 2017-12-09 22:39:20.921677: step 64670, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.863 sec/batch; 64h:12m:34s remains)
INFO - root - 2017-12-09 22:39:29.348290: step 64680, loss = 0.82, batch loss = 0.69 (11.0 examples/sec; 0.725 sec/batch; 53h:57m:50s remains)
INFO - root - 2017-12-09 22:39:37.840102: step 64690, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.855 sec/batch; 63h:37m:55s remains)
INFO - root - 2017-12-09 22:39:46.475807: step 64700, loss = 0.82, batch loss = 0.69 (9.8 examples/sec; 0.819 sec/batch; 60h:54m:29s remains)
2017-12-09 22:39:47.368242: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0018392456 -0.001838816 -0.0018375758 -0.0018347556 -0.0018309794 -0.0018292028 -0.0018298848 -0.0018323659 -0.0018349366 -0.0018368548 -0.0018381699 -0.0018388801 -0.0018390813 -0.0018390279 -0.0018390575][-0.0018394754 -0.0018378516 -0.0018323321 -0.0018223501 -0.0018104649 -0.0018041738 -0.0018048596 -0.0018117643 -0.0018210736 -0.0018294894 -0.0018352348 -0.0018383346 -0.001839765 -0.0018400361 -0.0018397906][-0.001838189 -0.0018347549 -0.0018212895 -0.0017974208 -0.0017692641 -0.0017522627 -0.0017511762 -0.001765738 -0.0017887619 -0.0018108352 -0.0018266926 -0.0018353382 -0.0018394182 -0.0018406989 -0.0018406593][-0.0018355412 -0.0018300518 -0.001804469 -0.0017595134 -0.0017058383 -0.0016691496 -0.0016615461 -0.0016842941 -0.0017267166 -0.0017704483 -0.0018046423 -0.0018246672 -0.0018347752 -0.0018389883 -0.0018401871][-0.0018304019 -0.0018237542 -0.0017861228 -0.0017171267 -0.0016334258 -0.0015713155 -0.0015517555 -0.0015786614 -0.0016382948 -0.0017044254 -0.0017617145 -0.0017998873 -0.0018218689 -0.0018325927 -0.0018369297][-0.0018219674 -0.001817905 -0.0017733896 -0.0016853304 -0.0015739178 -0.001485441 -0.0014497258 -0.0014730578 -0.001540389 -0.0016205349 -0.0016970258 -0.0017547895 -0.0017942209 -0.0018173956 -0.0018292364][-0.0018123505 -0.0018147948 -0.0017730524 -0.0016800049 -0.0015539351 -0.0014449033 -0.0013891815 -0.0013966026 -0.0014533381 -0.0015292146 -0.0016105826 -0.0016825763 -0.0017426893 -0.0017856845 -0.0018120212][-0.0018050467 -0.0018142049 -0.0017817877 -0.0016979388 -0.0015736958 -0.0014538591 -0.001377885 -0.0013607864 -0.00139076 -0.0014415855 -0.001507733 -0.0015817736 -0.0016610468 -0.0017308862 -0.0017812838][-0.0018043973 -0.0018166899 -0.0017957203 -0.0017319093 -0.001627711 -0.0015150181 -0.0014284262 -0.001387057 -0.0013826975 -0.0013959158 -0.0014296275 -0.0014889326 -0.0015743035 -0.0016652137 -0.0017409546][-0.0018142995 -0.0018229256 -0.0018101858 -0.0017689278 -0.0016961041 -0.0016089744 -0.0015294463 -0.0014767316 -0.0014461463 -0.0014255326 -0.0014225235 -0.0014543522 -0.0015284705 -0.0016242352 -0.001713087][-0.0018254581 -0.0018286479 -0.0018212182 -0.0017994613 -0.0017594751 -0.0017076154 -0.0016535691 -0.0016105509 -0.0015755451 -0.0015403623 -0.0015100067 -0.0015057549 -0.0015461366 -0.0016224388 -0.0017056583][-0.0018327837 -0.0018324081 -0.0018292859 -0.0018207176 -0.0018039276 -0.001780741 -0.001754224 -0.0017305941 -0.0017074693 -0.0016763014 -0.0016374155 -0.0016072447 -0.0016093859 -0.0016514539 -0.0017138191][-0.0018331136 -0.0018323113 -0.0018314614 -0.0018293385 -0.0018246332 -0.0018174007 -0.0018086247 -0.0018004054 -0.0017905789 -0.0017709499 -0.00173761 -0.0016978642 -0.0016730898 -0.0016819978 -0.0017194776][-0.0018315789 -0.0018308779 -0.0018306811 -0.0018304547 -0.0018295634 -0.0018279302 -0.0018259773 -0.0018243 -0.0018213036 -0.0018106162 -0.0017855243 -0.0017460301 -0.0017077159 -0.0016914022 -0.0017059146][-0.0018302512 -0.0018295668 -0.0018293255 -0.0018292053 -0.0018286995 -0.0018277518 -0.0018267033 -0.0018261631 -0.0018253557 -0.0018197802 -0.0018017769 -0.0017644714 -0.0017172409 -0.001679205 -0.0016709814]]...]
INFO - root - 2017-12-09 22:39:55.935068: step 64710, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 64h:28m:45s remains)
INFO - root - 2017-12-09 22:40:04.479800: step 64720, loss = 0.83, batch loss = 0.70 (9.2 examples/sec; 0.865 sec/batch; 64h:20m:05s remains)
INFO - root - 2017-12-09 22:40:13.121233: step 64730, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.848 sec/batch; 63h:04m:42s remains)
INFO - root - 2017-12-09 22:40:21.736432: step 64740, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.870 sec/batch; 64h:40m:45s remains)
INFO - root - 2017-12-09 22:40:30.605174: step 64750, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.872 sec/batch; 64h:52m:24s remains)
INFO - root - 2017-12-09 22:40:39.266334: step 64760, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.880 sec/batch; 65h:25m:20s remains)
INFO - root - 2017-12-09 22:40:47.896149: step 64770, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.875 sec/batch; 65h:02m:21s remains)
INFO - root - 2017-12-09 22:40:56.413247: step 64780, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.873 sec/batch; 64h:53m:06s remains)
INFO - root - 2017-12-09 22:41:04.813085: step 64790, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.835 sec/batch; 62h:07m:42s remains)
INFO - root - 2017-12-09 22:41:13.279122: step 64800, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.861 sec/batch; 64h:02m:54s remains)
2017-12-09 22:41:14.124672: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.056489438 0.055369314 0.053967632 0.052765042 0.051659442 0.050988492 0.050547298 0.050537221 0.050728358 0.051068392 0.050555758 0.048992269 0.045421254 0.040070046 0.032447875][0.066386573 0.065444462 0.063634492 0.061995998 0.060296748 0.059239041 0.058240585 0.057515565 0.056741629 0.0561111 0.054919709 0.053066991 0.049514432 0.044179272 0.036296289][0.073152058 0.073047288 0.071319208 0.069395706 0.067201376 0.0656659 0.064067908 0.062638491 0.060791306 0.058880504 0.056451693 0.053732444 0.049871936 0.044557929 0.036925353][0.077489287 0.078338385 0.076821126 0.074665777 0.072070055 0.070064135 0.068116039 0.066313647 0.063872412 0.061055891 0.057530887 0.05381377 0.049231544 0.043522954 0.035832789][0.079636388 0.0808834 0.079045206 0.076273307 0.073162906 0.070769183 0.0688637 0.067266986 0.065051593 0.062134586 0.058113571 0.053619936 0.0481708 0.041682243 0.033614304][0.08003559 0.080897756 0.077969134 0.073991127 0.070067845 0.0673208 0.065741017 0.064929157 0.063679211 0.0613731 0.057532728 0.052774612 0.046667345 0.039454691 0.030919515][0.077517092 0.077412151 0.073117882 0.067888491 0.063195087 0.060204882 0.059199516 0.059577446 0.059782512 0.058667656 0.055487983 0.050672971 0.044080183 0.036405522 0.02768516][0.070289589 0.068840459 0.063596614 0.057938877 0.053504132 0.051091425 0.051090315 0.052795865 0.054518681 0.054732934 0.052377347 0.047753613 0.041018754 0.033174936 0.024582386][0.059392568 0.056308147 0.049949542 0.044356443 0.040928829 0.040093932 0.041846294 0.045021512 0.048212115 0.049734849 0.048344158 0.044266008 0.037848879 0.030290987 0.022217588][0.046644624 0.042381439 0.03568447 0.030839898 0.028786784 0.0296444 0.032946367 0.037470989 0.041934595 0.044453755 0.043963976 0.040691424 0.035011813 0.028202273 0.020989072][0.034940336 0.030036546 0.02351778 0.019767189 0.01916464 0.0214669 0.025862193 0.0311497 0.036302567 0.039642185 0.040086411 0.037732482 0.033048838 0.027251702 0.020959444][0.025722012 0.021013808 0.015328881 0.012639063 0.012998095 0.015978402 0.02073366 0.026236586 0.03160451 0.035340343 0.036431164 0.035013787 0.031308513 0.026353853 0.020842543][0.018677004 0.014618891 0.010063578 0.0085165445 0.0097781625 0.013226508 0.018031379 0.023192441 0.028127441 0.031691305 0.0329607 0.032184124 0.02933166 0.025274673 0.020499717][0.012889298 0.0097413389 0.0063865115 0.0055177747 0.0070993672 0.010588821 0.015225642 0.020061597 0.024512403 0.027629897 0.028763847 0.028282559 0.026010616 0.022767102 0.018850496][0.0086991386 0.0062521412 0.0038105072 0.0033091768 0.0047252355 0.0076498282 0.011648379 0.015971204 0.01990719 0.022582682 0.02358181 0.023252338 0.021485837 0.018941468 0.01575882]]...]
INFO - root - 2017-12-09 22:41:22.671094: step 64810, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.847 sec/batch; 62h:57m:44s remains)
INFO - root - 2017-12-09 22:41:31.269339: step 64820, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.855 sec/batch; 63h:33m:04s remains)
INFO - root - 2017-12-09 22:41:40.039368: step 64830, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.878 sec/batch; 65h:15m:04s remains)
INFO - root - 2017-12-09 22:41:48.834029: step 64840, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.848 sec/batch; 63h:02m:42s remains)
INFO - root - 2017-12-09 22:41:57.647699: step 64850, loss = 0.82, batch loss = 0.69 (8.4 examples/sec; 0.949 sec/batch; 70h:32m:27s remains)
INFO - root - 2017-12-09 22:42:06.364315: step 64860, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.883 sec/batch; 65h:37m:45s remains)
INFO - root - 2017-12-09 22:42:15.159941: step 64870, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.894 sec/batch; 66h:29m:32s remains)
INFO - root - 2017-12-09 22:42:23.782674: step 64880, loss = 0.82, batch loss = 0.69 (10.7 examples/sec; 0.749 sec/batch; 55h:42m:21s remains)
INFO - root - 2017-12-09 22:42:32.129989: step 64890, loss = 0.83, batch loss = 0.70 (9.4 examples/sec; 0.849 sec/batch; 63h:05m:54s remains)
INFO - root - 2017-12-09 22:42:40.731263: step 64900, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.875 sec/batch; 65h:01m:15s remains)
2017-12-09 22:42:41.631067: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0018378363 -0.0018363883 -0.0018355368 -0.0018348257 -0.0018342 -0.0018332894 -0.0018320895 -0.0018309705 -0.00183022 -0.0018300695 -0.0018305815 -0.0018316986 -0.0018329407 -0.001833825 -0.0018341941][-0.0018364534 -0.0018358288 -0.001835544 -0.0018352844 -0.001834948 -0.0018344084 -0.0018337355 -0.001833064 -0.0018324254 -0.0018322138 -0.001832401 -0.0018328895 -0.0018333464 -0.0018335242 -0.0018331794][-0.0018349314 -0.0018354161 -0.0018359782 -0.0018362695 -0.0018361385 -0.0018357395 -0.0018351163 -0.0018345683 -0.0018342666 -0.0018345098 -0.0018347597 -0.0018348304 -0.0018346756 -0.0018342227 -0.0018331872][-0.0018335795 -0.0018346739 -0.0018358674 -0.0018363969 -0.0018355643 -0.0018334826 -0.0018310816 -0.0018297483 -0.0018302781 -0.0018321414 -0.0018337633 -0.0018345969 -0.0018348783 -0.0018343617 -0.0018332212][-0.0018326745 -0.0018338306 -0.0018349809 -0.0018345654 -0.0018312301 -0.0018247999 -0.001818383 -0.0018154598 -0.0018177 -0.0018230759 -0.0018280543 -0.0018313836 -0.0018332255 -0.0018336053 -0.0018328828][-0.0018318109 -0.0018326432 -0.0018332768 -0.0018312399 -0.0018241105 -0.0018117845 -0.0017999539 -0.0017945453 -0.0017985718 -0.0018081062 -0.0018176871 -0.0018249975 -0.0018296833 -0.0018318851 -0.0018321663][-0.0018306733 -0.0018312981 -0.0018314763 -0.0018279057 -0.0018173394 -0.0018001384 -0.0017840508 -0.001775809 -0.0017799401 -0.0017919759 -0.0018054891 -0.0018168378 -0.0018246234 -0.0018290962 -0.001830904][-0.0018300184 -0.0018305355 -0.0018306946 -0.0018270863 -0.0018165417 -0.0017989959 -0.0017816774 -0.0017709984 -0.0017725247 -0.0017836138 -0.0017980289 -0.0018110563 -0.0018208071 -0.0018270516 -0.0018301024][-0.0018296036 -0.0018299661 -0.0018304479 -0.0018283649 -0.0018214521 -0.0018089212 -0.001794796 -0.0017840462 -0.001781977 -0.0017883675 -0.0017999504 -0.00181169 -0.0018210569 -0.0018273529 -0.0018305483][-0.001828931 -0.0018289818 -0.0018297432 -0.0018292433 -0.0018265384 -0.0018207108 -0.0018130803 -0.0018058494 -0.001802448 -0.0018042387 -0.0018104889 -0.0018179935 -0.001824489 -0.0018290327 -0.0018314919][-0.00182843 -0.0018281655 -0.0018289404 -0.0018292227 -0.0018287388 -0.0018270651 -0.0018243563 -0.0018210566 -0.0018189558 -0.0018189972 -0.0018212424 -0.0018246762 -0.0018280565 -0.0018305595 -0.0018321718][-0.0018280142 -0.0018275414 -0.001828153 -0.0018286569 -0.0018289897 -0.0018289658 -0.0018285095 -0.0018275368 -0.0018267104 -0.0018265058 -0.0018271513 -0.0018284023 -0.0018297619 -0.0018309915 -0.0018320794][-0.0018281701 -0.0018273541 -0.0018277204 -0.0018281972 -0.0018286826 -0.0018290153 -0.0018292372 -0.0018292549 -0.0018290251 -0.0018289209 -0.0018291164 -0.0018294395 -0.0018299553 -0.0018306863 -0.001831488][-0.0018281228 -0.0018273056 -0.0018274811 -0.0018278562 -0.0018281846 -0.0018284728 -0.0018286692 -0.0018288182 -0.001828786 -0.0018286903 -0.0018287919 -0.0018290074 -0.0018293355 -0.0018298915 -0.0018306552][-0.0018277648 -0.0018270197 -0.0018271112 -0.0018274037 -0.0018276472 -0.0018278537 -0.0018279806 -0.0018280656 -0.0018280278 -0.0018278741 -0.001827883 -0.0018280551 -0.0018282866 -0.0018287733 -0.00182935]]...]
INFO - root - 2017-12-09 22:42:50.340811: step 64910, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.878 sec/batch; 65h:17m:38s remains)
INFO - root - 2017-12-09 22:42:58.886676: step 64920, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.889 sec/batch; 66h:06m:23s remains)
INFO - root - 2017-12-09 22:43:07.643476: step 64930, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.866 sec/batch; 64h:22m:29s remains)
INFO - root - 2017-12-09 22:43:16.567324: step 64940, loss = 0.82, batch loss = 0.69 (8.7 examples/sec; 0.919 sec/batch; 68h:17m:01s remains)
INFO - root - 2017-12-09 22:43:25.294226: step 64950, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.879 sec/batch; 65h:21m:14s remains)
INFO - root - 2017-12-09 22:43:34.070731: step 64960, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.885 sec/batch; 65h:45m:00s remains)
INFO - root - 2017-12-09 22:43:42.818151: step 64970, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.865 sec/batch; 64h:16m:34s remains)
INFO - root - 2017-12-09 22:43:51.527723: step 64980, loss = 0.82, batch loss = 0.69 (11.0 examples/sec; 0.730 sec/batch; 54h:14m:37s remains)
INFO - root - 2017-12-09 22:44:00.040672: step 64990, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 64h:30m:37s remains)
INFO - root - 2017-12-09 22:44:08.606113: step 65000, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.833 sec/batch; 61h:55m:29s remains)
2017-12-09 22:44:09.540950: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0017790116 -0.0017729443 -0.0017584767 -0.0017385854 -0.0017205864 -0.0017114845 -0.0017127977 -0.0017204089 -0.0017293568 -0.0017342109 -0.0017330473 -0.0017277476 -0.0017246974 -0.001730945 -0.0017496464][-0.0017859294 -0.0017784928 -0.0017622589 -0.0017402337 -0.0017192874 -0.0017056374 -0.0017021376 -0.0017067551 -0.0017161182 -0.0017252918 -0.0017307966 -0.0017313884 -0.0017319146 -0.0017389848 -0.0017564802][-0.0018032375 -0.0017948415 -0.0017770089 -0.0017514589 -0.0017236405 -0.0017008169 -0.0016886224 -0.0016889812 -0.0017003064 -0.0017173776 -0.0017340675 -0.001745867 -0.0017546434 -0.0017648621 -0.0017797285][-0.001817614 -0.0018079608 -0.0017873237 -0.0017550916 -0.0017156882 -0.0016793628 -0.0016560766 -0.0016524089 -0.0016680986 -0.0016970438 -0.0017298816 -0.0017578165 -0.0017778792 -0.0017917389 -0.0018033562][-0.0018266423 -0.0018155595 -0.0017911276 -0.0017493747 -0.0016945538 -0.0016404842 -0.0016029458 -0.0015938369 -0.0016136642 -0.0016563055 -0.0017083896 -0.0017559598 -0.0017900274 -0.0018095603 -0.0018196314][-0.0018268412 -0.0018120992 -0.001779722 -0.001723054 -0.0016477052 -0.0015721065 -0.0015193648 -0.00150607 -0.0015331493 -0.0015921451 -0.0016657984 -0.0017348737 -0.0017853972 -0.0018134924 -0.0018250508][-0.0018247083 -0.0018048682 -0.0017620588 -0.0016891266 -0.0015945206 -0.0015020334 -0.0014390749 -0.0014234944 -0.0014562337 -0.0015274757 -0.0016177129 -0.0017050918 -0.0017707517 -0.0018082922 -0.0018239957][-0.0018228223 -0.0018002591 -0.0017521417 -0.0016719882 -0.0015710222 -0.0014732176 -0.0014048187 -0.0013841287 -0.0014147768 -0.0014892588 -0.0015874198 -0.0016843519 -0.0017583655 -0.001801661 -0.0018208447][-0.001822588 -0.001802776 -0.00176036 -0.0016891136 -0.0015980888 -0.0015047567 -0.0014337309 -0.001405976 -0.0014289903 -0.0014970498 -0.0015913746 -0.0016859293 -0.0017581254 -0.0018005582 -0.0018196924][-0.0018245807 -0.0018109662 -0.0017809309 -0.0017262693 -0.0016500759 -0.001566036 -0.0014980579 -0.001467873 -0.001484822 -0.0015433135 -0.0016249708 -0.0017068686 -0.0017686412 -0.0018044024 -0.001820495][-0.001827098 -0.0018182247 -0.001797671 -0.0017572388 -0.0016970098 -0.0016280361 -0.0015713989 -0.0015462827 -0.0015609523 -0.0016089377 -0.001674038 -0.0017375024 -0.0017846413 -0.0018113948 -0.0018229486][-0.0018281597 -0.0018222472 -0.0018081098 -0.001780104 -0.0017384208 -0.0016913792 -0.0016538742 -0.0016380509 -0.0016484503 -0.0016809056 -0.001724383 -0.0017668855 -0.001798749 -0.0018171105 -0.0018249121][-0.0018282762 -0.0018242818 -0.0018157825 -0.001799721 -0.001776871 -0.0017516867 -0.0017317426 -0.0017232494 -0.0017287174 -0.0017461664 -0.0017697258 -0.0017931439 -0.0018109863 -0.001821299 -0.0018256914][-0.0018277046 -0.0018254184 -0.00182182 -0.0018156337 -0.0018071622 -0.0017978533 -0.0017901934 -0.0017863994 -0.001787674 -0.0017937336 -0.0018026856 -0.0018121561 -0.0018196985 -0.0018241109 -0.0018258628][-0.001827355 -0.0018259679 -0.0018247598 -0.0018230653 -0.0018209126 -0.0018186998 -0.0018168638 -0.0018158404 -0.0018160839 -0.0018175291 -0.0018197913 -0.0018222254 -0.0018242417 -0.0018254372 -0.0018258105]]...]
INFO - root - 2017-12-09 22:44:18.316315: step 65010, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.872 sec/batch; 64h:46m:27s remains)
INFO - root - 2017-12-09 22:44:27.135060: step 65020, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 64h:35m:44s remains)
INFO - root - 2017-12-09 22:44:35.963031: step 65030, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.877 sec/batch; 65h:10m:33s remains)
INFO - root - 2017-12-09 22:44:44.692613: step 65040, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.882 sec/batch; 65h:32m:25s remains)
INFO - root - 2017-12-09 22:44:53.403299: step 65050, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.849 sec/batch; 63h:04m:58s remains)
INFO - root - 2017-12-09 22:45:02.039769: step 65060, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.872 sec/batch; 64h:47m:14s remains)
INFO - root - 2017-12-09 22:45:10.773049: step 65070, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.890 sec/batch; 66h:06m:34s remains)
INFO - root - 2017-12-09 22:45:19.398793: step 65080, loss = 0.82, batch loss = 0.69 (10.7 examples/sec; 0.751 sec/batch; 55h:45m:44s remains)
INFO - root - 2017-12-09 22:45:27.800296: step 65090, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.848 sec/batch; 63h:01m:24s remains)
INFO - root - 2017-12-09 22:45:36.479077: step 65100, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.861 sec/batch; 63h:55m:10s remains)
2017-12-09 22:45:37.376679: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0018332178 -0.0018340758 -0.0018351646 -0.0018357693 -0.0018355689 -0.0018347553 -0.001833909 -0.0018335318 -0.0018336145 -0.0018341852 -0.0018354247 -0.0018367645 -0.0018376026 -0.0018379125 -0.0018377043][-0.0018339661 -0.0018348483 -0.0018354359 -0.0018348121 -0.0018329796 -0.0018303647 -0.0018281256 -0.0018269926 -0.0018275749 -0.0018296143 -0.0018324964 -0.0018353586 -0.0018371418 -0.0018376835 -0.0018370382][-0.0018352313 -0.0018356174 -0.0018351628 -0.0018328711 -0.0018287499 -0.0018237121 -0.0018193009 -0.0018171711 -0.0018184271 -0.0018223189 -0.0018277144 -0.00183302 -0.0018363469 -0.0018372977 -0.0018364063][-0.0018361483 -0.001835777 -0.0018339006 -0.0018294142 -0.0018222844 -0.0018136898 -0.0018064214 -0.0018030849 -0.0018049025 -0.0018111101 -0.0018197191 -0.0018283403 -0.0018340779 -0.0018359355 -0.0018349995][-0.001836507 -0.0018357114 -0.0018327374 -0.0018259959 -0.001815756 -0.0018033243 -0.0017928295 -0.0017880199 -0.001789993 -0.0017978109 -0.0018093257 -0.0018218344 -0.0018305979 -0.0018338198 -0.0018331802][-0.0018366021 -0.0018355567 -0.0018316478 -0.0018232941 -0.0018108246 -0.0017958082 -0.0017832819 -0.0017777481 -0.001779295 -0.0017872567 -0.0018001376 -0.0018155281 -0.0018271083 -0.0018321233 -0.0018322413][-0.0018363376 -0.0018351697 -0.0018309276 -0.0018222654 -0.0018094862 -0.0017945801 -0.0017822152 -0.0017765876 -0.0017773629 -0.001784153 -0.0017964562 -0.001812111 -0.0018243368 -0.0018300831 -0.0018309585][-0.001836112 -0.0018349959 -0.0018310646 -0.001823478 -0.0018122032 -0.0017990199 -0.0017875882 -0.0017814368 -0.0017810729 -0.0017868533 -0.0017984342 -0.0018129238 -0.0018238495 -0.0018293743 -0.0018306581][-0.0018357919 -0.0018352178 -0.0018321805 -0.0018262311 -0.0018174296 -0.0018069015 -0.001796931 -0.0017902277 -0.001788683 -0.0017933184 -0.0018032935 -0.0018151338 -0.0018241223 -0.0018288103 -0.0018302816][-0.0018352005 -0.00183522 -0.0018336619 -0.0018296923 -0.0018235364 -0.0018154697 -0.0018066437 -0.0017999317 -0.0017981216 -0.0018022511 -0.0018105906 -0.001819204 -0.0018256835 -0.0018289243 -0.0018301384][-0.0018344577 -0.0018346319 -0.0018341009 -0.0018320207 -0.0018281942 -0.0018222269 -0.0018147831 -0.0018089224 -0.0018078544 -0.0018117551 -0.0018179392 -0.0018237669 -0.001827753 -0.0018296329 -0.0018305702][-0.0018333676 -0.0018333899 -0.0018332764 -0.001832236 -0.0018299819 -0.0018260335 -0.0018210276 -0.0018171431 -0.0018168078 -0.0018196468 -0.0018235367 -0.0018268178 -0.0018287447 -0.0018295385 -0.0018300263][-0.0018323384 -0.0018322369 -0.0018322683 -0.0018318209 -0.0018306873 -0.001828561 -0.0018257579 -0.0018238043 -0.0018237941 -0.0018252512 -0.0018272189 -0.0018285392 -0.0018291345 -0.00182929 -0.0018293872][-0.001831551 -0.0018313302 -0.0018313371 -0.0018311645 -0.0018307342 -0.0018299554 -0.0018289065 -0.0018280249 -0.0018277115 -0.0018280948 -0.00182871 -0.0018289706 -0.0018290683 -0.00182899 -0.0018289284][-0.0018310674 -0.0018307765 -0.0018306127 -0.0018305219 -0.0018304411 -0.0018302094 -0.0018300066 -0.0018298028 -0.0018294827 -0.0018292065 -0.0018291211 -0.0018290585 -0.0018290586 -0.001828952 -0.0018288159]]...]
INFO - root - 2017-12-09 22:45:46.079720: step 65110, loss = 0.82, batch loss = 0.69 (9.7 examples/sec; 0.826 sec/batch; 61h:20m:30s remains)
INFO - root - 2017-12-09 22:45:54.581687: step 65120, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.847 sec/batch; 62h:54m:27s remains)
INFO - root - 2017-12-09 22:46:03.097379: step 65130, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.847 sec/batch; 62h:56m:00s remains)
INFO - root - 2017-12-09 22:46:11.701488: step 65140, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.861 sec/batch; 63h:58m:42s remains)
INFO - root - 2017-12-09 22:46:20.330918: step 65150, loss = 0.83, batch loss = 0.70 (9.4 examples/sec; 0.851 sec/batch; 63h:10m:36s remains)
INFO - root - 2017-12-09 22:46:29.022304: step 65160, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.841 sec/batch; 62h:24m:59s remains)
INFO - root - 2017-12-09 22:46:37.695991: step 65170, loss = 0.82, batch loss = 0.69 (9.7 examples/sec; 0.828 sec/batch; 61h:30m:03s remains)
INFO - root - 2017-12-09 22:46:46.403772: step 65180, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.863 sec/batch; 64h:05m:55s remains)
INFO - root - 2017-12-09 22:46:54.594676: step 65190, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.859 sec/batch; 63h:47m:45s remains)
INFO - root - 2017-12-09 22:47:03.247619: step 65200, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.902 sec/batch; 66h:57m:08s remains)
2017-12-09 22:47:04.244340: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.0027711014 0.0056781867 0.0088930475 0.011808269 0.014160956 0.015921598 0.017185926 0.018414861 0.019895682 0.021588461 0.02347848 0.025302066 0.026459813 0.026584053 0.026195172][0.0044956305 0.0084905419 0.013273099 0.017766913 0.021579318 0.024660265 0.026942343 0.028945712 0.030595776 0.031946138 0.032813229 0.032842241 0.031986132 0.030290056 0.028665539][0.0086102085 0.014766426 0.022280257 0.029565863 0.035922218 0.041088358 0.04477315 0.0473583 0.048661571 0.048722021 0.047561206 0.04508717 0.041641049 0.037543666 0.034023914][0.014764563 0.024394119 0.03596361 0.04744694 0.057573386 0.0657719 0.071500286 0.075104766 0.076221146 0.074935079 0.071303457 0.065171659 0.057675533 0.049722247 0.042910725][0.021185251 0.034859449 0.051139027 0.067428991 0.08181905 0.093365438 0.10106935 0.10537948 0.10598194 0.10332745 0.09734042 0.088060506 0.076906681 0.065092824 0.054554813][0.025180362 0.042311262 0.062543474 0.083104543 0.10146532 0.11596749 0.12539957 0.13025509 0.13058041 0.12695101 0.11920166 0.1076422 0.093612388 0.0785489 0.064556129][0.024992246 0.043320302 0.065438181 0.088491812 0.10945592 0.12628877 0.13732946 0.14305478 0.14361568 0.13977641 0.13126989 0.11855754 0.10283808 0.085570328 0.069064505][0.021086164 0.0379877 0.059150424 0.0823049 0.10415846 0.12243252 0.134942 0.14201683 0.14342645 0.13991646 0.13126063 0.11803446 0.10155748 0.083363459 0.065674357][0.015644455 0.028954001 0.046542719 0.0669854 0.0872918 0.10527146 0.1184085 0.1266039 0.12914152 0.12640435 0.11824038 0.10530425 0.089159884 0.071405306 0.054183025][0.010447604 0.01937913 0.031977072 0.047592212 0.064026661 0.079610415 0.09175811 0.099950172 0.10308948 0.10112873 0.094013356 0.082397461 0.068008073 0.0524572 0.037686136][0.0059954296 0.011072795 0.018785926 0.02894349 0.040206645 0.051564209 0.061018318 0.06783925 0.070723161 0.069441855 0.063936219 0.054825172 0.043651681 0.031902872 0.021191277][0.0024337238 0.0049193436 0.00892899 0.014509111 0.021052506 0.027947757 0.033956923 0.038529832 0.040531576 0.039649554 0.035880629 0.029810455 0.022587473 0.015310223 0.0090460135][-0.00023963954 0.00086154777 0.00265727 0.0052116839 0.0083211791 0.011668463 0.014679578 0.01701281 0.017989833 0.017378069 0.015196004 0.01190725 0.0082115531 0.0047145365 0.0019476697][-0.0015600147 -0.0012349623 -0.00064221711 0.00027935591 0.0014373587 0.0027057952 0.0038575027 0.00471275 0.0050129737 0.0046393587 0.0036695008 0.0023638862 0.0010315626 -0.00011301204 -0.00093192625][-0.001788691 -0.0017602684 -0.0016715899 -0.0015129582 -0.0012908659 -0.0010049001 -0.00072662474 -0.00051015476 -0.00042829604 -0.00052004866 -0.00074708345 -0.0010369613 -0.0013078561 -0.0015215316 -0.0016580304]]...]
INFO - root - 2017-12-09 22:47:12.989595: step 65210, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.879 sec/batch; 65h:16m:19s remains)
INFO - root - 2017-12-09 22:47:21.699252: step 65220, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.894 sec/batch; 66h:20m:44s remains)
INFO - root - 2017-12-09 22:47:30.454964: step 65230, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.873 sec/batch; 64h:46m:49s remains)
INFO - root - 2017-12-09 22:47:39.566239: step 65240, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.887 sec/batch; 65h:50m:12s remains)
INFO - root - 2017-12-09 22:47:48.287042: step 65250, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.851 sec/batch; 63h:10m:10s remains)
INFO - root - 2017-12-09 22:47:56.963440: step 65260, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.884 sec/batch; 65h:37m:48s remains)
INFO - root - 2017-12-09 22:48:05.760893: step 65270, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.890 sec/batch; 66h:02m:23s remains)
INFO - root - 2017-12-09 22:48:14.522987: step 65280, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.884 sec/batch; 65h:38m:04s remains)
INFO - root - 2017-12-09 22:48:22.871575: step 65290, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.830 sec/batch; 61h:34m:51s remains)
INFO - root - 2017-12-09 22:48:31.568012: step 65300, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.878 sec/batch; 65h:08m:01s remains)
2017-12-09 22:48:32.508303: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.001797404 -0.0018027439 -0.0018041233 -0.0017981435 -0.0017635485 -0.0016757497 -0.0015372868 -0.0013896796 -0.0012920505 -0.0012835246 -0.0013653757 -0.0015011525 -0.0016322853 -0.0017236125 -0.0017716801][-0.0017612888 -0.0017902822 -0.0017987586 -0.0017861129 -0.0017311611 -0.0016132186 -0.0014464075 -0.0012851208 -0.0011941965 -0.0012013016 -0.0013079778 -0.0014687037 -0.0016176325 -0.0017161171 -0.0017653941][-0.0013694512 -0.0015122981 -0.0015852842 -0.0015291254 -0.0013195376 -0.00099641341 -0.00066902093 -0.00046190852 -0.00046605221 -0.00066800497 -0.00099547289 -0.0013231293 -0.0015673621 -0.0017054102 -0.001766587][-0.00062881352 -0.00071404409 -0.00059827848 -0.00014742138 0.0006462686 0.0015712908 0.0022566668 0.0023971815 0.0018985505 0.0009548167 -0.00010707637 -0.00095686846 -0.0014663659 -0.0016955147 -0.0017702859][0.00050593924 0.00085118331 0.0017291735 0.0033550607 0.0056095873 0.0078970687 0.009319447 0.0092284624 0.0075615277 0.0049075535 0.0021435083 3.3855205e-05 -0.0011574754 -0.0016431769 -0.0017726441][0.0020406139 0.0032814965 0.0056716651 0.0094225165 0.014167572 0.018702982 0.021365194 0.020999858 0.01757353 0.012233128 0.0066591729 0.0022994806 -0.00027949468 -0.0014038432 -0.0017348182][0.0035280711 0.0060205306 0.010428754 0.016915705 0.024776982 0.032152057 0.036571223 0.036269862 0.031132724 0.022789536 0.013771343 0.0063466649 0.0016108631 -0.0007121308 -0.0015511335][0.0048146225 0.0086003859 0.014945949 0.023931246 0.034570515 0.044539891 0.050814588 0.051113345 0.045092948 0.034526773 0.022486649 0.011936688 0.0046350076 0.00061387091 -0.001111201][0.0058434084 0.010717522 0.018452698 0.028972227 0.041125689 0.052506648 0.059980247 0.061105691 0.055273045 0.043990511 0.030351739 0.017634425 0.0081438962 0.0023761839 -0.00044013478][0.00646943 0.012054821 0.020378938 0.031176742 0.0432421 0.0544156 0.061965361 0.063679919 0.058766231 0.048230533 0.034740161 0.02143074 0.010846636 0.0039162631 0.00021916616][0.0064379522 0.012199036 0.020272804 0.030192889 0.040799607 0.0503726 0.0568782 0.058610771 0.054799441 0.045972284 0.034122895 0.02187931 0.011638817 0.0045562638 0.00055441877][0.0055878949 0.010875679 0.017920606 0.026139043 0.034490742 0.041728839 0.046529315 0.047772069 0.044864383 0.038060434 0.028714817 0.018775459 0.010171742 0.0040082051 0.00040754548][0.0039968817 0.0082232673 0.013646341 0.019702898 0.025567492 0.030394498 0.033427615 0.034037936 0.03186699 0.027059268 0.020461878 0.013370406 0.007118911 0.0025628177 -0.00013186096][0.0020548459 0.0049395473 0.0085500954 0.012448709 0.016071973 0.018892236 0.020517394 0.020643003 0.019116014 0.01605515 0.01195213 0.0075694318 0.0036990102 0.00087469758 -0.00078782532][0.00033010368 0.0019728839 0.0040045171 0.0061478526 0.00807688 0.0095000584 0.010227902 0.010136452 0.0091885878 0.0074795079 0.0052701975 0.002960274 0.00095135521 -0.00048994424 -0.0013194936]]...]
INFO - root - 2017-12-09 22:48:41.216218: step 65310, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.888 sec/batch; 65h:52m:39s remains)
INFO - root - 2017-12-09 22:48:49.819807: step 65320, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.866 sec/batch; 64h:14m:15s remains)
INFO - root - 2017-12-09 22:48:58.588379: step 65330, loss = 0.82, batch loss = 0.70 (9.0 examples/sec; 0.889 sec/batch; 65h:56m:26s remains)
INFO - root - 2017-12-09 22:49:07.449653: step 65340, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 64h:30m:51s remains)
INFO - root - 2017-12-09 22:49:16.141544: step 65350, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.887 sec/batch; 65h:47m:57s remains)
INFO - root - 2017-12-09 22:49:24.734453: step 65360, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.877 sec/batch; 65h:04m:06s remains)
INFO - root - 2017-12-09 22:49:33.128793: step 65370, loss = 0.82, batch loss = 0.69 (9.8 examples/sec; 0.814 sec/batch; 60h:24m:02s remains)
INFO - root - 2017-12-09 22:49:41.631289: step 65380, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.835 sec/batch; 61h:56m:12s remains)
INFO - root - 2017-12-09 22:49:50.166814: step 65390, loss = 0.83, batch loss = 0.70 (9.1 examples/sec; 0.883 sec/batch; 65h:29m:01s remains)
INFO - root - 2017-12-09 22:49:58.806556: step 65400, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.852 sec/batch; 63h:15m:02s remains)
2017-12-09 22:49:59.691327: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0018131598 -0.0018128043 -0.001813348 -0.0018141356 -0.0018146664 -0.0018147776 -0.0018144834 -0.0018139509 -0.001813333 -0.0018128178 -0.0018126585 -0.0018128197 -0.0018131655 -0.0018132633 -0.0018129455][-0.0018131167 -0.0018124945 -0.0018127721 -0.0018132845 -0.0018136038 -0.0018135704 -0.0018132301 -0.0018127771 -0.0018124076 -0.0018122066 -0.0018123572 -0.0018128151 -0.0018133462 -0.0018134569 -0.0018130371][-0.001813866 -0.0018130753 -0.0018130556 -0.0018132454 -0.0018132776 -0.0018130289 -0.0018125728 -0.0018120497 -0.0018117414 -0.0018116916 -0.0018120386 -0.0018126742 -0.0018133131 -0.0018134685 -0.0018130583][-0.001815038 -0.0018140773 -0.001813784 -0.0018136986 -0.0018134326 -0.0018129052 -0.0018122569 -0.0018115294 -0.001811059 -0.0018109607 -0.0018113465 -0.0018120403 -0.0018127298 -0.0018129285 -0.0018125956][-0.0018160737 -0.0018151534 -0.0018146948 -0.0018144012 -0.0018138652 -0.0018130214 -0.0018120744 -0.0018110792 -0.0018104069 -0.0018101428 -0.0018104182 -0.0018110699 -0.0018118017 -0.0018120392 -0.0018117834][-0.0018168312 -0.0018161034 -0.0018156531 -0.0018151834 -0.0018143464 -0.0018131442 -0.0018118633 -0.0018105956 -0.0018097323 -0.0018092792 -0.0018094066 -0.001809947 -0.0018106567 -0.0018109418 -0.0018108027][-0.0018172818 -0.0018168475 -0.0018164995 -0.0018159205 -0.0018148533 -0.0018133557 -0.0018118034 -0.0018103444 -0.0018093444 -0.0018087653 -0.0018088025 -0.0018092225 -0.0018097922 -0.0018099775 -0.0018098537][-0.0018175512 -0.0018173051 -0.0018170485 -0.0018164157 -0.0018151812 -0.0018135114 -0.0018118535 -0.0018103881 -0.0018094074 -0.0018088503 -0.0018088755 -0.0018091981 -0.0018095614 -0.0018094975 -0.0018092075][-0.0018176226 -0.0018175181 -0.0018173283 -0.0018166275 -0.0018152696 -0.0018135466 -0.0018119562 -0.0018106452 -0.0018098574 -0.0018094967 -0.0018096211 -0.0018098745 -0.0018100051 -0.0018096631 -0.001809075][-0.001817628 -0.0018175889 -0.0018173879 -0.0018166822 -0.0018153964 -0.0018138249 -0.0018124032 -0.0018113096 -0.0018107624 -0.001810613 -0.0018108315 -0.0018110221 -0.0018109631 -0.0018104366 -0.0018096061][-0.0018176985 -0.0018175615 -0.0018173418 -0.0018166982 -0.0018156193 -0.0018143592 -0.0018132449 -0.0018124154 -0.0018120982 -0.0018121338 -0.0018124181 -0.001812559 -0.0018123738 -0.0018117129 -0.0018107546][-0.0018177102 -0.0018174724 -0.0018172708 -0.0018167648 -0.0018159773 -0.0018151402 -0.0018144272 -0.0018139128 -0.0018137916 -0.0018139099 -0.0018141369 -0.0018141833 -0.0018138979 -0.0018131738 -0.0018121653][-0.0018176697 -0.0018173649 -0.0018171994 -0.0018168794 -0.0018164013 -0.0018159199 -0.0018155322 -0.0018152434 -0.0018151958 -0.0018152893 -0.0018154106 -0.0018153725 -0.0018150635 -0.0018144321 -0.0018135086][-0.0018175825 -0.0018172432 -0.001817115 -0.0018169729 -0.0018167428 -0.0018164966 -0.0018162896 -0.001816095 -0.001816016 -0.0018160116 -0.0018160451 -0.0018160075 -0.001815793 -0.0018153416 -0.0018146182][-0.0018174843 -0.0018170875 -0.0018169621 -0.001816944 -0.0018168603 -0.0018167456 -0.0018166251 -0.001816459 -0.0018163278 -0.0018162611 -0.0018162822 -0.0018163081 -0.0018162299 -0.001815962 -0.0018154415]]...]
INFO - root - 2017-12-09 22:50:08.361919: step 65410, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.876 sec/batch; 65h:00m:12s remains)
INFO - root - 2017-12-09 22:50:16.839638: step 65420, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.860 sec/batch; 63h:46m:45s remains)
INFO - root - 2017-12-09 22:50:25.706419: step 65430, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.861 sec/batch; 63h:54m:15s remains)
INFO - root - 2017-12-09 22:50:34.267869: step 65440, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.830 sec/batch; 61h:35m:49s remains)
INFO - root - 2017-12-09 22:50:42.709600: step 65450, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.866 sec/batch; 64h:12m:55s remains)
INFO - root - 2017-12-09 22:50:51.378943: step 65460, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.882 sec/batch; 65h:24m:40s remains)
INFO - root - 2017-12-09 22:51:00.119331: step 65470, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.892 sec/batch; 66h:11m:20s remains)
INFO - root - 2017-12-09 22:51:08.894806: step 65480, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.851 sec/batch; 63h:07m:31s remains)
INFO - root - 2017-12-09 22:51:17.441403: step 65490, loss = 0.82, batch loss = 0.69 (9.8 examples/sec; 0.814 sec/batch; 60h:22m:01s remains)
INFO - root - 2017-12-09 22:51:26.040974: step 65500, loss = 0.82, batch loss = 0.69 (9.8 examples/sec; 0.818 sec/batch; 60h:40m:27s remains)
2017-12-09 22:51:26.992350: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.041506611 0.057762794 0.079550229 0.10482757 0.12937129 0.15154818 0.16758537 0.17686175 0.17947707 0.17817834 0.17411968 0.1706367 0.16746913 0.16608448 0.16469741][0.041888811 0.06061868 0.0843381 0.11158628 0.13741118 0.1608713 0.177485 0.18701628 0.19021462 0.1890814 0.18555874 0.18234874 0.17966907 0.17908846 0.17843047][0.041835796 0.061553285 0.086475924 0.11427445 0.14075258 0.16445665 0.18130492 0.19103691 0.1943945 0.1938954 0.19076027 0.18793069 0.18534118 0.18526696 0.18473791][0.041474674 0.061956346 0.087083638 0.11456744 0.14083108 0.16428138 0.18129298 0.19127631 0.19531447 0.19574422 0.19305494 0.19045195 0.1879341 0.18716107 0.18646617][0.040254846 0.060795814 0.085963637 0.1128365 0.13811064 0.16079557 0.17775798 0.18810897 0.19303831 0.19476348 0.19291706 0.19067305 0.18752328 0.18602382 0.18444246][0.037776303 0.0576844 0.081625827 0.10734897 0.13168788 0.15360311 0.17006728 0.18123305 0.18744615 0.19066072 0.19026248 0.18899235 0.18573071 0.18308839 0.18090871][0.034096807 0.052821297 0.075024657 0.098747812 0.12160692 0.14275737 0.15874384 0.17000549 0.17672393 0.18109825 0.18196043 0.18131548 0.17882662 0.17618279 0.17390627][0.029540289 0.046500236 0.066586033 0.088163808 0.1091626 0.1285616 0.14373738 0.15488517 0.16158324 0.16648112 0.16794771 0.16790561 0.16596146 0.16361736 0.16221617][0.024598382 0.039339803 0.056957033 0.0758579 0.094413429 0.1122098 0.12630039 0.13740495 0.14481251 0.15047576 0.1528313 0.15322982 0.15124257 0.14829387 0.14665553][0.019550761 0.031438615 0.045756128 0.061485291 0.077384315 0.0930951 0.10636933 0.1174565 0.12541065 0.13196838 0.13541973 0.13641518 0.13454925 0.13163491 0.12975554][0.015100942 0.023794947 0.034294512 0.046180498 0.059085429 0.072613649 0.08482004 0.09591303 0.1049389 0.11282298 0.1175273 0.11931222 0.11803066 0.11535236 0.1133238][0.011068142 0.017086023 0.024379021 0.032686684 0.042380616 0.053351309 0.064483143 0.075123109 0.084339485 0.092887394 0.098518193 0.10079771 0.099953726 0.097865425 0.096129522][0.0075205197 0.011274386 0.01607037 0.021677308 0.02901352 0.03785795 0.047613926 0.057734128 0.066938356 0.075501874 0.081237413 0.0841493 0.083664939 0.082044587 0.080431081][0.0048610219 0.006831462 0.0095540769 0.013225035 0.018633429 0.025964141 0.034749661 0.044282887 0.053817511 0.0626967 0.068710528 0.071708366 0.071398988 0.069912836 0.067971192][0.0027874438 0.0037670168 0.0053564715 0.0077482709 0.01179555 0.017797548 0.025736373 0.034782954 0.044339165 0.053538144 0.059944738 0.063425861 0.063681506 0.062490873 0.06058538]]...]
INFO - root - 2017-12-09 22:51:35.854516: step 65510, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 64h:17m:54s remains)
INFO - root - 2017-12-09 22:51:44.558810: step 65520, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.880 sec/batch; 65h:16m:42s remains)
INFO - root - 2017-12-09 22:51:53.252726: step 65530, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.876 sec/batch; 64h:57m:32s remains)
INFO - root - 2017-12-09 22:52:01.928410: step 65540, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.850 sec/batch; 63h:00m:58s remains)
INFO - root - 2017-12-09 22:52:10.615054: step 65550, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.883 sec/batch; 65h:29m:35s remains)
INFO - root - 2017-12-09 22:52:19.432524: step 65560, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.872 sec/batch; 64h:37m:22s remains)
INFO - root - 2017-12-09 22:52:28.283947: step 65570, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.843 sec/batch; 62h:29m:30s remains)
INFO - root - 2017-12-09 22:52:36.988482: step 65580, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 64h:26m:24s remains)
INFO - root - 2017-12-09 22:52:45.567264: step 65590, loss = 0.82, batch loss = 0.69 (10.7 examples/sec; 0.748 sec/batch; 55h:25m:16s remains)
INFO - root - 2017-12-09 22:52:54.167976: step 65600, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.874 sec/batch; 64h:49m:33s remains)
2017-12-09 22:52:55.020926: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0010215321 -0.0010717136 -0.00087359524 -0.00050484459 0.00030558824 0.0015454496 0.0032726461 0.0052382457 0.0073232367 0.0093981642 0.010887277 0.011973444 0.01235347 0.01269254 0.012939846][-0.00078777503 -0.00085012557 -0.0006802514 -0.00046538038 0.00024522643 0.00143952 0.003133724 0.0051495023 0.0073879291 0.0096554151 0.011425818 0.012592914 0.012969669 0.013125891 0.013145541][-0.00060082297 -0.00066024496 -0.00045290589 -0.00017583964 0.00058702647 0.0017373647 0.0034174165 0.0054726093 0.0077904803 0.010258313 0.01217891 0.013364188 0.013634241 0.013566347 0.01339016][-0.00054608192 -0.00051698519 -0.0002105271 0.00020806573 0.0011120135 0.0023614494 0.0041129938 0.0061780089 0.0085525131 0.011103837 0.013166864 0.014279184 0.014445844 0.014138707 0.013805901][-0.00056305912 -0.00042438437 -4.70652e-05 0.00058786559 0.0016663411 0.0030385405 0.0048340084 0.0069503183 0.00933964 0.011925774 0.01400053 0.01511803 0.015219587 0.01479715 0.01443342][-0.00063808844 -0.00034660578 0.00016803306 0.00096440909 0.0021280283 0.003614767 0.0054027527 0.0074805417 0.0098041184 0.012359939 0.014382538 0.015478973 0.015640806 0.015259027 0.014925887][-0.00073739362 -0.00027615903 0.00038899959 0.0013661952 0.0025031925 0.0039633769 0.0056178807 0.0076205186 0.0097776642 0.012237628 0.014152496 0.015241082 0.015545906 0.015291062 0.01503642][-0.00084764557 -0.00027120439 0.00053540862 0.0016584698 0.0027801157 0.0041618291 0.005650009 0.007540124 0.009500565 0.011721347 0.013388541 0.014507235 0.014870557 0.01479663 0.014631805][-0.000992951 -0.00036792771 0.00049648143 0.0016269326 0.0026929034 0.0040178723 0.0054103886 0.0071545043 0.0089082206 0.010863622 0.012275666 0.013260754 0.013626608 0.0136721 0.013493191][-0.001175723 -0.0006121156 0.00017913443 0.0012047436 0.0021487321 0.0033348105 0.0046371426 0.0062825712 0.0078546107 0.0095342472 0.010624939 0.01148016 0.01174543 0.01185665 0.011667788][-0.0013793849 -0.00094432675 -0.00030991028 0.00047362258 0.0012171507 0.0022006077 0.0033300556 0.0047572404 0.0061109015 0.0074931984 0.0083396286 0.0090186466 0.009216221 0.0093420167 0.0091491453][-0.0015583427 -0.0012733468 -0.00085403642 -0.00030961842 0.00020314881 0.00090321142 0.0017684366 0.0028754668 0.0039188354 0.0049615791 0.005587385 0.006119058 0.0062617245 0.0064006331 0.0062248069][-0.0017008252 -0.001539933 -0.0012997037 -0.00098163588 -0.00065652514 -0.00019822258 0.00037824258 0.0011146661 0.0018150314 0.0024874234 0.0028958232 0.0032686568 0.0033693509 0.003492516 0.0033787712][-0.0017837845 -0.00171828 -0.0016100557 -0.0014550239 -0.0012859277 -0.0010162516 -0.00068246294 -0.00025625469 0.00014673441 0.00054303056 0.0007820843 0.00099096925 0.0010600785 0.001144786 0.0010653142][-0.0018244555 -0.0018037435 -0.0017680108 -0.0017143423 -0.0016453994 -0.0015206019 -0.0013615265 -0.0011381666 -0.00093978021 -0.00074501825 -0.00062259962 -0.00050461595 -0.0004660706 -0.00042367761 -0.0004546335]]...]
INFO - root - 2017-12-09 22:53:03.683528: step 65610, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 64h:04m:52s remains)
INFO - root - 2017-12-09 22:53:12.191589: step 65620, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.830 sec/batch; 61h:31m:33s remains)
INFO - root - 2017-12-09 22:53:20.900270: step 65630, loss = 0.82, batch loss = 0.69 (8.8 examples/sec; 0.913 sec/batch; 67h:41m:28s remains)
INFO - root - 2017-12-09 22:53:29.680422: step 65640, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.856 sec/batch; 63h:28m:33s remains)
INFO - root - 2017-12-09 22:53:38.466745: step 65650, loss = 0.82, batch loss = 0.69 (8.8 examples/sec; 0.907 sec/batch; 67h:12m:33s remains)
INFO - root - 2017-12-09 22:53:47.083713: step 65660, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.856 sec/batch; 63h:25m:23s remains)
INFO - root - 2017-12-09 22:53:55.980109: step 65670, loss = 0.83, batch loss = 0.70 (8.2 examples/sec; 0.980 sec/batch; 72h:36m:05s remains)
INFO - root - 2017-12-09 22:54:04.708293: step 65680, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.891 sec/batch; 66h:01m:37s remains)
INFO - root - 2017-12-09 22:54:13.220949: step 65690, loss = 0.82, batch loss = 0.69 (10.3 examples/sec; 0.780 sec/batch; 57h:49m:42s remains)
INFO - root - 2017-12-09 22:54:21.851720: step 65700, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.894 sec/batch; 66h:15m:33s remains)
2017-12-09 22:54:22.793284: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.022924753 0.022223698 0.021140264 0.020059507 0.019152597 0.018297348 0.017482881 0.016363991 0.014947719 0.013485389 0.011896085 0.0097348709 0.006972603 0.0040503461 0.0017939551][0.026406741 0.025848925 0.024906311 0.023982033 0.023313412 0.022816373 0.022112876 0.020916807 0.019271767 0.017598657 0.015712842 0.013114894 0.0096522924 0.0059219408 0.002941547][0.028919084 0.028407995 0.027548602 0.026850145 0.026553323 0.026332907 0.025923308 0.024863405 0.023129884 0.021423113 0.019435667 0.016512979 0.012384183 0.0078372639 0.0041535595][0.030663993 0.030349851 0.029596299 0.028992582 0.02894628 0.029059365 0.029028116 0.028091352 0.026424019 0.024694327 0.022535846 0.019345773 0.014605946 0.0093655493 0.0051146611][0.032846119 0.032780357 0.032095533 0.031516984 0.031717952 0.032257397 0.03270933 0.032168146 0.030683562 0.028734311 0.026133863 0.022277856 0.016725797 0.010709589 0.0058981283][0.036244221 0.036237925 0.035433702 0.034830038 0.035171479 0.036029726 0.037094418 0.037055068 0.035875142 0.033727929 0.030525506 0.025715156 0.0191355 0.012230895 0.006818566][0.039995063 0.039549854 0.038248058 0.037174229 0.037412681 0.03857984 0.040244643 0.04066316 0.039691653 0.037377466 0.033675741 0.028085867 0.020698676 0.013169419 0.00739288][0.044218559 0.042873003 0.040455 0.038476393 0.038197216 0.039277788 0.04136819 0.042273734 0.0415994 0.039201304 0.035214756 0.029175062 0.021318009 0.013495023 0.0075863772][0.048046015 0.045521889 0.041938372 0.038863234 0.03784043 0.038784105 0.040995155 0.042227045 0.04188139 0.039698832 0.035770807 0.02955544 0.021477155 0.013558248 0.0076246005][0.050252277 0.047024596 0.042751186 0.039048366 0.037572231 0.038257319 0.040327266 0.041661885 0.041450009 0.03941961 0.035556391 0.029294431 0.021213643 0.013369198 0.0075236368][0.04863609 0.045568842 0.041507106 0.03800666 0.036666293 0.037377283 0.039403919 0.040618498 0.040267605 0.038217559 0.034309551 0.028074691 0.020130651 0.012613401 0.00708117][0.043731797 0.040919337 0.037170488 0.034031529 0.033075817 0.034157671 0.036367819 0.037601665 0.037246846 0.035243668 0.031422708 0.0254341 0.017981092 0.011106707 0.00613704][0.03728335 0.034750588 0.031305041 0.028428733 0.027561152 0.028654644 0.030889872 0.032363068 0.032317467 0.030577123 0.027115457 0.021746183 0.015199632 0.0092651406 0.0050152061][0.02950646 0.027372599 0.024476252 0.022008006 0.021256465 0.022204287 0.024157951 0.02552761 0.025628034 0.024300322 0.0214525 0.017057046 0.011794326 0.00711061 0.0037707705][0.020752538 0.019406721 0.017430197 0.015806273 0.015507142 0.016448019 0.018123953 0.019222982 0.019287134 0.018180303 0.015845146 0.012412749 0.008435213 0.0049708621 0.0024824012]]...]
INFO - root - 2017-12-09 22:54:31.436809: step 65710, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.845 sec/batch; 62h:37m:32s remains)
INFO - root - 2017-12-09 22:54:39.971071: step 65720, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.882 sec/batch; 65h:23m:10s remains)
INFO - root - 2017-12-09 22:54:48.674447: step 65730, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.861 sec/batch; 63h:49m:09s remains)
INFO - root - 2017-12-09 22:54:57.473775: step 65740, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.873 sec/batch; 64h:43m:04s remains)
INFO - root - 2017-12-09 22:55:06.137833: step 65750, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.841 sec/batch; 62h:17m:33s remains)
INFO - root - 2017-12-09 22:55:14.942581: step 65760, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 64h:19m:31s remains)
INFO - root - 2017-12-09 22:55:23.626855: step 65770, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.850 sec/batch; 62h:57m:08s remains)
INFO - root - 2017-12-09 22:55:32.447251: step 65780, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.879 sec/batch; 65h:05m:15s remains)
INFO - root - 2017-12-09 22:55:40.741730: step 65790, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.834 sec/batch; 61h:46m:47s remains)
INFO - root - 2017-12-09 22:55:49.172462: step 65800, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.845 sec/batch; 62h:34m:47s remains)
2017-12-09 22:55:50.051791: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.25439784 0.25196996 0.24810384 0.24323516 0.23636684 0.22935046 0.22221667 0.21589515 0.20987758 0.20430996 0.19940245 0.1941615 0.18874858 0.18242623 0.17619385][0.25160742 0.25094447 0.248667 0.24518529 0.23968098 0.23377508 0.22830768 0.22417943 0.22066511 0.21716349 0.21416442 0.20946331 0.20320532 0.19507894 0.1863585][0.24796736 0.24979457 0.24956283 0.24754485 0.24366583 0.23937105 0.23498236 0.23287129 0.23178557 0.23063338 0.22899827 0.22475909 0.21759155 0.20700692 0.19528332][0.2472291 0.25184792 0.25411955 0.25456992 0.2530089 0.25052243 0.24838084 0.24863932 0.2491788 0.24936435 0.24818844 0.24371912 0.23510626 0.22154444 0.20694195][0.25066984 0.25818083 0.26309446 0.26618302 0.26746467 0.26727986 0.26724386 0.26901615 0.27083468 0.27176738 0.27016884 0.26471037 0.25407419 0.23827788 0.22078736][0.25723892 0.26748422 0.27460614 0.27896702 0.28179261 0.28389749 0.28630641 0.2891849 0.2917532 0.29303238 0.29111978 0.28387073 0.27070269 0.25260538 0.2327892][0.26493832 0.27790171 0.28690517 0.29249892 0.2959103 0.29863119 0.30186653 0.30562198 0.3087551 0.31010202 0.3077271 0.299444 0.28475165 0.26518866 0.24380755][0.26907605 0.28418222 0.29427963 0.30084881 0.30495095 0.30844253 0.3120842 0.31608963 0.31920686 0.32032126 0.31720456 0.30802816 0.29241413 0.27153707 0.24919587][0.26685342 0.28377646 0.29531178 0.30306965 0.30791751 0.31163815 0.31577176 0.31999984 0.32233366 0.32271528 0.3190068 0.30931556 0.29347613 0.27294204 0.25106806][0.25930786 0.27647153 0.28782141 0.2955977 0.30072522 0.30492508 0.30902129 0.31291714 0.31516248 0.31548089 0.3118943 0.30239722 0.28752729 0.26841366 0.24788441][0.247387 0.26360869 0.27366191 0.28038323 0.28484064 0.28878185 0.29282081 0.29681125 0.29903352 0.29958302 0.29669246 0.28799221 0.27461067 0.25750262 0.23929621][0.23299056 0.24701653 0.254646 0.25983793 0.26358524 0.26718208 0.27104175 0.27539423 0.27832 0.27933097 0.27724338 0.270001 0.25871524 0.2440801 0.22865292][0.21538983 0.227285 0.23298678 0.23697673 0.23999202 0.24311452 0.24639779 0.25035083 0.25326195 0.2550748 0.2544111 0.2490986 0.24046355 0.22875684 0.21653603][0.20051858 0.21035652 0.21433696 0.21686013 0.21904077 0.22160584 0.22450691 0.22784935 0.23048148 0.23263612 0.23296343 0.22939581 0.2232417 0.21497484 0.20634933][0.18945101 0.19694757 0.19860655 0.19957247 0.20050251 0.20251577 0.20535038 0.20859347 0.21170026 0.21432051 0.2155275 0.21352106 0.20954382 0.20386989 0.19783428]]...]
INFO - root - 2017-12-09 22:55:58.691997: step 65810, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.852 sec/batch; 63h:06m:47s remains)
INFO - root - 2017-12-09 22:56:07.297481: step 65820, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.890 sec/batch; 65h:56m:26s remains)
INFO - root - 2017-12-09 22:56:15.905658: step 65830, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.859 sec/batch; 63h:38m:47s remains)
INFO - root - 2017-12-09 22:56:24.633754: step 65840, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.888 sec/batch; 65h:48m:04s remains)
INFO - root - 2017-12-09 22:56:33.288864: step 65850, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.863 sec/batch; 63h:57m:11s remains)
INFO - root - 2017-12-09 22:56:41.943913: step 65860, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.860 sec/batch; 63h:40m:33s remains)
INFO - root - 2017-12-09 22:56:50.699765: step 65870, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.896 sec/batch; 66h:23m:10s remains)
INFO - root - 2017-12-09 22:56:59.408838: step 65880, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 64h:20m:38s remains)
INFO - root - 2017-12-09 22:57:07.995389: step 65890, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.899 sec/batch; 66h:34m:24s remains)
INFO - root - 2017-12-09 22:57:16.479369: step 65900, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.854 sec/batch; 63h:13m:11s remains)
2017-12-09 22:57:17.348408: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0018143971 -0.001813267 -0.0018133001 -0.0018136544 -0.0018142638 -0.0018151075 -0.0018161042 -0.0018170993 -0.0018179852 -0.0018183752 -0.001818086 -0.001817242 -0.0018160866 -0.0018150872 -0.0018144291][-0.0018136175 -0.0018126908 -0.0018130469 -0.0018138156 -0.0018149927 -0.0018164624 -0.0018181087 -0.0018196913 -0.0018208482 -0.0018212255 -0.0018205403 -0.001819041 -0.0018171226 -0.0018155071 -0.0018146567][-0.0018136164 -0.0018129626 -0.0018137225 -0.0018151545 -0.0018170973 -0.0018193955 -0.0018217752 -0.0018237847 -0.0018250241 -0.0018251623 -0.0018239619 -0.001821734 -0.001819162 -0.0018171798 -0.0018162399][-0.0018135108 -0.0018131966 -0.0018144503 -0.0018166937 -0.0018195147 -0.0018226746 -0.0018256523 -0.0018278696 -0.0018291535 -0.0018291678 -0.0018275467 -0.0018248663 -0.0018219389 -0.0018198276 -0.0018189289][-0.0018136298 -0.0018136356 -0.0018154138 -0.0018184792 -0.0018220554 -0.0018257804 -0.0018289909 -0.0018312022 -0.0018324261 -0.0018322547 -0.0018304416 -0.0018275965 -0.0018246465 -0.0018226269 -0.0018218518][-0.0018139047 -0.0018142099 -0.0018165035 -0.0018203221 -0.0018243763 -0.0018282527 -0.0018314584 -0.0018336342 -0.0018347994 -0.001834405 -0.0018324387 -0.0018297188 -0.0018269671 -0.0018251592 -0.0018246125][-0.0018143058 -0.0018149797 -0.0018177234 -0.0018220848 -0.0018264008 -0.0018302974 -0.0018333176 -0.0018351045 -0.001835822 -0.001835116 -0.0018332731 -0.0018308475 -0.0018284934 -0.0018271797 -0.0018269481][-0.0018145735 -0.0018156404 -0.0018187754 -0.0018234438 -0.0018278705 -0.0018317821 -0.0018345406 -0.001835715 -0.0018360001 -0.0018353846 -0.0018338773 -0.0018315521 -0.0018293479 -0.0018282934 -0.001828045][-0.0018152358 -0.0018166361 -0.0018200353 -0.0018247236 -0.0018289805 -0.001832693 -0.0018351702 -0.0018358314 -0.0018357799 -0.0018351752 -0.0018337471 -0.0018314458 -0.0018292613 -0.001828058 -0.0018274974][-0.0018162322 -0.0018178755 -0.0018212708 -0.0018256815 -0.0018294852 -0.0018326552 -0.0018346657 -0.0018350149 -0.0018347087 -0.0018337519 -0.0018320662 -0.0018298576 -0.0018278376 -0.0018265592 -0.0018257588][-0.0018170594 -0.001818686 -0.0018218623 -0.0018257028 -0.0018288406 -0.0018313524 -0.001832989 -0.0018332673 -0.0018327695 -0.0018314502 -0.0018296733 -0.0018277284 -0.0018259542 -0.0018246352 -0.0018235951][-0.0018173199 -0.0018186081 -0.0018213311 -0.0018244328 -0.0018269713 -0.0018291108 -0.0018304548 -0.0018305569 -0.0018298659 -0.0018284569 -0.001826855 -0.0018252342 -0.0018236897 -0.0018223778 -0.0018212389][-0.0018172634 -0.0018181358 -0.0018202736 -0.0018225849 -0.0018244075 -0.0018259624 -0.0018269379 -0.0018270211 -0.0018264417 -0.0018252948 -0.0018239832 -0.0018226283 -0.001821333 -0.0018201455 -0.0018190596][-0.0018172596 -0.0018175171 -0.0018189041 -0.001820364 -0.0018215033 -0.0018224515 -0.0018230134 -0.00182297 -0.001822441 -0.0018215019 -0.0018205632 -0.0018196639 -0.0018188019 -0.0018179816 -0.0018171703][-0.0018171491 -0.0018168107 -0.0018174214 -0.0018181859 -0.0018187157 -0.0018190701 -0.0018191895 -0.0018189817 -0.0018186121 -0.0018180974 -0.001817639 -0.0018172237 -0.0018167852 -0.0018163279 -0.0018158115]]...]
INFO - root - 2017-12-09 22:57:25.868877: step 65910, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.852 sec/batch; 63h:06m:47s remains)
INFO - root - 2017-12-09 22:57:34.337948: step 65920, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.865 sec/batch; 64h:01m:49s remains)
INFO - root - 2017-12-09 22:57:42.973664: step 65930, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.848 sec/batch; 62h:46m:04s remains)
INFO - root - 2017-12-09 22:57:51.588681: step 65940, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.866 sec/batch; 64h:06m:04s remains)
INFO - root - 2017-12-09 22:58:00.321905: step 65950, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.863 sec/batch; 63h:53m:35s remains)
INFO - root - 2017-12-09 22:58:09.053729: step 65960, loss = 0.82, batch loss = 0.69 (8.6 examples/sec; 0.933 sec/batch; 69h:06m:46s remains)
INFO - root - 2017-12-09 22:58:17.800221: step 65970, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.842 sec/batch; 62h:19m:15s remains)
INFO - root - 2017-12-09 22:58:26.423459: step 65980, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.874 sec/batch; 64h:42m:29s remains)
INFO - root - 2017-12-09 22:58:34.839036: step 65990, loss = 0.81, batch loss = 0.68 (9.6 examples/sec; 0.836 sec/batch; 61h:54m:50s remains)
INFO - root - 2017-12-09 22:58:43.318945: step 66000, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.879 sec/batch; 65h:02m:44s remains)
2017-12-09 22:58:44.272301: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.01139344 0.014477528 0.020191934 0.027950337 0.036486477 0.044093613 0.049289554 0.051189981 0.049537487 0.044588391 0.036932979 0.027643425 0.018199854 0.010072066 0.0043141027][0.012892725 0.017540272 0.025903031 0.037112929 0.049223475 0.059762951 0.066782311 0.069317654 0.0671684 0.060628954 0.050335258 0.037697628 0.024810296 0.013769137 0.0060112015][0.014333921 0.021188829 0.032665804 0.047461726 0.06289085 0.075805254 0.084007174 0.086726964 0.0839534 0.076003514 0.063405626 0.047737353 0.031578381 0.017657612 0.0078410646][0.015793482 0.025073571 0.039656546 0.05776665 0.076061569 0.090910055 0.09996625 0.10263497 0.099083275 0.089702144 0.0749751 0.056642361 0.037658975 0.021249224 0.009604536][0.017466195 0.028838469 0.04589431 0.066412233 0.086617589 0.10272115 0.11242854 0.11526032 0.11140334 0.10109249 0.08470355 0.064161919 0.042810988 0.024333391 0.011163817][0.018912971 0.031926617 0.050787859 0.072848156 0.094124243 0.11085033 0.12092368 0.12392836 0.11998175 0.10917065 0.091749519 0.06972722 0.046703409 0.026709361 0.012390196][0.019358695 0.033605546 0.053694312 0.076690987 0.098476931 0.11537769 0.12551086 0.12853935 0.12454673 0.11348261 0.095556222 0.072804771 0.048912872 0.028095972 0.01312632][0.018555138 0.033456646 0.054266617 0.077804036 0.0998195 0.11665209 0.12651454 0.12922014 0.12493024 0.11367754 0.095694289 0.072961293 0.049068231 0.028216161 0.013194684][0.017397685 0.032256216 0.0529924 0.076398619 0.098119713 0.11445136 0.12366673 0.12568723 0.12087677 0.10946752 0.091820352 0.069850482 0.046883713 0.026891034 0.012511431][0.015896572 0.030180689 0.050134975 0.0725953 0.093276627 0.10850997 0.11664649 0.1177551 0.11243784 0.10113832 0.084324405 0.063803121 0.042581826 0.024246825 0.011147524][0.013920714 0.026912294 0.04506506 0.065510184 0.084321581 0.097987 0.10493664 0.10527287 0.099783689 0.089089587 0.073754482 0.055414408 0.036691539 0.020680927 0.0093507273][0.011365715 0.022266405 0.037649635 0.055070121 0.07120356 0.082911611 0.088736668 0.088729911 0.083658054 0.074202888 0.060975276 0.045434177 0.029804077 0.016604565 0.0073651741][0.0082667228 0.016428711 0.028167859 0.041781504 0.054717261 0.064334281 0.069297887 0.069455482 0.065388754 0.057731647 0.047091916 0.034754124 0.022540556 0.012386002 0.0053701997][0.0048147696 0.010245977 0.018181527 0.027655289 0.037061911 0.044474803 0.048710875 0.049365483 0.046691783 0.041180477 0.033408497 0.02444954 0.015694277 0.0085071772 0.0035855165][0.001615273 0.0047172643 0.0093989158 0.015218484 0.021331053 0.02653897 0.029937606 0.030985273 0.029659823 0.026260477 0.021258218 0.015464452 0.0098386677 0.0052536642 0.0021153605]]...]
INFO - root - 2017-12-09 22:58:52.830148: step 66010, loss = 0.82, batch loss = 0.69 (8.8 examples/sec; 0.909 sec/batch; 67h:16m:19s remains)
INFO - root - 2017-12-09 22:59:01.426732: step 66020, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.873 sec/batch; 64h:38m:37s remains)
INFO - root - 2017-12-09 22:59:10.146294: step 66030, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.853 sec/batch; 63h:07m:29s remains)
INFO - root - 2017-12-09 22:59:18.883958: step 66040, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.875 sec/batch; 64h:45m:14s remains)
INFO - root - 2017-12-09 22:59:27.560320: step 66050, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.891 sec/batch; 65h:56m:27s remains)
INFO - root - 2017-12-09 22:59:36.197634: step 66060, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.859 sec/batch; 63h:32m:48s remains)
INFO - root - 2017-12-09 22:59:44.788120: step 66070, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 63h:57m:11s remains)
INFO - root - 2017-12-09 22:59:53.615300: step 66080, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.876 sec/batch; 64h:51m:46s remains)
INFO - root - 2017-12-09 23:00:02.065834: step 66090, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.855 sec/batch; 63h:17m:22s remains)
INFO - root - 2017-12-09 23:00:10.508120: step 66100, loss = 0.83, batch loss = 0.70 (9.0 examples/sec; 0.890 sec/batch; 65h:53m:20s remains)
2017-12-09 23:00:11.448885: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.0080028912 0.010199066 0.011651974 0.012081803 0.01175101 0.010766831 0.0093412064 0.0076268683 0.00572407 0.0037626862 0.0018770703 0.00026114623 -0.00088377192 -0.0015073689 -0.0017518728][0.013612109 0.016991422 0.019246241 0.01997138 0.019455034 0.017917031 0.015743451 0.01314905 0.010307532 0.0073479167 0.00443639 0.0018713706 -3.1622592e-05 -0.0011482863 -0.0016425784][0.020311588 0.024980288 0.028109359 0.029189501 0.028505934 0.026435221 0.023528166 0.020048836 0.016219864 0.012122434 0.0079722879 0.0041729654 0.0012400745 -0.00057258771 -0.0014522999][0.027656812 0.033504367 0.037359115 0.03869028 0.037785277 0.035189044 0.031623758 0.027376655 0.022666834 0.017487772 0.012104476 0.0070184232 0.0029098545 0.00021682109 -0.0011884053][0.034310862 0.041136563 0.045589752 0.047169153 0.046168011 0.043259937 0.039260063 0.0344244 0.028951561 0.022758177 0.016163602 0.0098180911 0.0045756958 0.0010221741 -0.00092340063][0.038743649 0.046155624 0.050978672 0.052805018 0.051940348 0.049071696 0.044975743 0.039887439 0.033915713 0.026931146 0.019352997 0.011985259 0.0058238991 0.0015946812 -0.00074990629][0.040425666 0.048058186 0.05303767 0.055066749 0.054486163 0.051892772 0.047936931 0.042769343 0.036498804 0.029036248 0.02088503 0.012980217 0.0063635451 0.0018231025 -0.00069625548][0.03911439 0.046496764 0.051372878 0.053517964 0.053256746 0.051049337 0.047338132 0.042230166 0.035852779 0.02825775 0.020085772 0.012302401 0.005886544 0.0015535263 -0.00081230619][0.035317443 0.042121265 0.046710588 0.048940528 0.049025793 0.047250986 0.043842938 0.038851678 0.032530665 0.025095657 0.017348468 0.0102264 0.0045663333 0.00088343758 -0.0010551164][0.029146448 0.035126124 0.0393361 0.041600853 0.042006258 0.040659811 0.03764011 0.03296078 0.026996663 0.020157382 0.01331664 0.0072977026 0.0027620262 -1.7795246e-05 -0.0013645194][0.021532563 0.026492083 0.030198589 0.032458607 0.033128895 0.032210115 0.029655445 0.025509188 0.020264572 0.01442949 0.0089041693 0.0042668679 0.0010084485 -0.000830282 -0.0016042197][0.013790079 0.01759791 0.020684354 0.022792745 0.02366055 0.023187561 0.021265211 0.017946849 0.013760026 0.0092148483 0.0051109195 0.0018256506 -0.00029434985 -0.0013691132 -0.0017343259][0.0070474218 0.0096950112 0.012065073 0.013885626 0.014807236 0.014689223 0.013416345 0.011068855 0.0081131086 0.0049622306 0.0022186106 0.00011150318 -0.0011142199 -0.0016544866 -0.0017802553][0.0024521649 0.00408796 0.0056901425 0.0070486446 0.0078501739 0.0079324013 0.0072081252 0.00576372 0.0039447858 0.0020117736 0.00036433118 -0.00085534423 -0.0015006693 -0.0017501907 -0.0017902895][-0.00028033659 0.00054016721 0.0014125552 0.0022212514 0.0027492489 0.0028759353 0.0025592018 0.001844674 0.00093612785 -3.3387914e-05 -0.00083789765 -0.001409355 -0.0016868844 -0.0017830522 -0.0017951364]]...]
INFO - root - 2017-12-09 23:00:20.193360: step 66110, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.866 sec/batch; 64h:03m:59s remains)
INFO - root - 2017-12-09 23:00:28.781752: step 66120, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.891 sec/batch; 65h:55m:57s remains)
INFO - root - 2017-12-09 23:00:37.462480: step 66130, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.884 sec/batch; 65h:24m:36s remains)
INFO - root - 2017-12-09 23:00:46.218979: step 66140, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 64h:14m:35s remains)
INFO - root - 2017-12-09 23:00:54.936005: step 66150, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.860 sec/batch; 63h:38m:19s remains)
INFO - root - 2017-12-09 23:01:03.510881: step 66160, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.870 sec/batch; 64h:22m:22s remains)
INFO - root - 2017-12-09 23:01:12.248054: step 66170, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.858 sec/batch; 63h:27m:17s remains)
INFO - root - 2017-12-09 23:01:20.703026: step 66180, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.858 sec/batch; 63h:27m:58s remains)
INFO - root - 2017-12-09 23:01:29.215681: step 66190, loss = 0.82, batch loss = 0.69 (8.8 examples/sec; 0.906 sec/batch; 66h:59m:32s remains)
INFO - root - 2017-12-09 23:01:37.705570: step 66200, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.841 sec/batch; 62h:13m:11s remains)
2017-12-09 23:01:38.617063: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.23709519 0.24171439 0.24306189 0.24115096 0.23683001 0.23175265 0.22714752 0.22433919 0.2218741 0.22072072 0.22030319 0.21960296 0.21753162 0.21300301 0.20723951][0.23546326 0.24430871 0.25024056 0.25345045 0.25470039 0.25482589 0.25416422 0.25362414 0.25212884 0.25046363 0.24888739 0.24707012 0.24469818 0.24118415 0.23632216][0.22893502 0.24084572 0.2506116 0.258683 0.26470712 0.26969576 0.27325359 0.27475747 0.2741324 0.27225462 0.2697756 0.26682323 0.26411125 0.26101163 0.25689659][0.22387731 0.23672406 0.24838391 0.25940073 0.26904094 0.27766892 0.28427136 0.28822836 0.28875965 0.28701434 0.28416941 0.28068542 0.278105 0.27560377 0.27240759][0.22017342 0.2330299 0.24470113 0.25646853 0.26794669 0.278973 0.28804269 0.29406402 0.29622698 0.29543924 0.29293025 0.2895143 0.28736991 0.28555435 0.28327003][0.21557991 0.2278198 0.23873706 0.25063694 0.26244533 0.27412367 0.28413928 0.29204902 0.29608718 0.29660439 0.29527774 0.29279435 0.29168385 0.29063779 0.289074][0.2064922 0.21899442 0.22942731 0.24031582 0.25171193 0.26363972 0.27443519 0.28333384 0.28888094 0.29163158 0.29225385 0.29125765 0.29075202 0.28984794 0.28824973][0.19228882 0.20537654 0.21641035 0.22736154 0.23864931 0.25036821 0.26104507 0.270399 0.27701354 0.28133789 0.28349185 0.28398207 0.28415939 0.28319043 0.28069612][0.17043625 0.18503809 0.19724749 0.20900387 0.22116876 0.2334642 0.24465621 0.25444919 0.26155019 0.26662996 0.26928702 0.27031696 0.2701118 0.26841071 0.26481164][0.14240076 0.15813658 0.17188244 0.18508197 0.19819936 0.21128692 0.22332032 0.23385316 0.24168095 0.24735454 0.25015131 0.25065821 0.24894309 0.24537569 0.23961087][0.11071832 0.12620252 0.14076363 0.15504116 0.16939391 0.18366316 0.19684571 0.20822875 0.21685269 0.2231172 0.22601105 0.22584715 0.22230645 0.21606274 0.20735042][0.080339387 0.094340324 0.10837512 0.12270265 0.13770971 0.15300176 0.167382 0.18015034 0.19010393 0.19722816 0.20011091 0.19897749 0.19362201 0.1846257 0.17291301][0.053654984 0.065542027 0.078185126 0.091776542 0.10635287 0.12178436 0.13688233 0.15068139 0.16172938 0.16952968 0.17268661 0.17098367 0.16419435 0.15307459 0.13916564][0.031197824 0.040134244 0.050292529 0.061916649 0.075058058 0.089911856 0.10533657 0.12009398 0.13234288 0.14096498 0.1445981 0.14248589 0.13468137 0.12218737 0.10702913][0.014254582 0.019909341 0.026988963 0.035875637 0.046662197 0.059771731 0.074380971 0.089261331 0.10236461 0.11192305 0.11633309 0.11436787 0.10629615 0.093348473 0.077944867]]...]
INFO - root - 2017-12-09 23:01:47.215373: step 66210, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.873 sec/batch; 64h:35m:27s remains)
INFO - root - 2017-12-09 23:01:55.847606: step 66220, loss = 0.83, batch loss = 0.70 (9.4 examples/sec; 0.848 sec/batch; 62h:41m:16s remains)
INFO - root - 2017-12-09 23:02:04.660646: step 66230, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.855 sec/batch; 63h:16m:30s remains)
INFO - root - 2017-12-09 23:02:13.411565: step 66240, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.897 sec/batch; 66h:21m:44s remains)
INFO - root - 2017-12-09 23:02:22.218701: step 66250, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.885 sec/batch; 65h:26m:49s remains)
INFO - root - 2017-12-09 23:02:31.084437: step 66260, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 64h:24m:19s remains)
INFO - root - 2017-12-09 23:02:39.825746: step 66270, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 64h:24m:11s remains)
INFO - root - 2017-12-09 23:02:48.451636: step 66280, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 64h:04m:54s remains)
INFO - root - 2017-12-09 23:02:56.809456: step 66290, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.845 sec/batch; 62h:29m:59s remains)
INFO - root - 2017-12-09 23:03:05.163339: step 66300, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.850 sec/batch; 62h:51m:56s remains)
2017-12-09 23:03:06.030200: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.2459778 0.24243252 0.23846844 0.23605405 0.23527965 0.23577709 0.2396353 0.24645397 0.25166658 0.25629517 0.25870213 0.25935 0.2566376 0.25010967 0.24346673][0.2475674 0.24582314 0.24339511 0.24226707 0.24188179 0.24264461 0.24616304 0.2522983 0.25665438 0.25987417 0.26101494 0.25980711 0.25545609 0.24776702 0.24027202][0.245489 0.24718535 0.24747579 0.24749914 0.24834621 0.24955183 0.25263312 0.25694206 0.25958255 0.26163906 0.26139617 0.25821143 0.25207442 0.24294586 0.23413186][0.24067216 0.24571975 0.24952328 0.25231475 0.25573713 0.2587193 0.2622827 0.26512483 0.26598889 0.26624718 0.26388797 0.25883263 0.25041372 0.24003609 0.23033005][0.23646489 0.24384683 0.24943677 0.25480598 0.260379 0.26515102 0.26957881 0.27189735 0.27137944 0.26947498 0.26531234 0.2588174 0.24872431 0.23756467 0.22767386][0.23407729 0.24325135 0.25069582 0.25804642 0.26523459 0.27156588 0.27694851 0.27892032 0.277487 0.27403283 0.2681604 0.25946638 0.24808885 0.23676109 0.22700441][0.2355186 0.24567778 0.25325707 0.26133838 0.26923978 0.27607498 0.28123465 0.28311118 0.28184578 0.27814919 0.27199429 0.26240554 0.25035983 0.23888762 0.22901233][0.23800538 0.24995852 0.25767866 0.26534927 0.27246755 0.27794626 0.28191447 0.28242666 0.28025451 0.27618027 0.26990515 0.26055181 0.24906838 0.23821795 0.22882144][0.23994274 0.25300482 0.26038551 0.26832634 0.27513352 0.27915853 0.28150105 0.28100926 0.27815321 0.27319393 0.26595348 0.257009 0.24590807 0.23571421 0.22680566][0.24366461 0.25643989 0.26247847 0.26936075 0.27460682 0.27720425 0.27836946 0.27764755 0.274722 0.26924995 0.26203251 0.25310823 0.2428697 0.23308925 0.22441652][0.24440187 0.25713077 0.26236922 0.26766044 0.27133244 0.27348757 0.27387258 0.27314988 0.27104911 0.26630822 0.25944883 0.24996835 0.24009627 0.23062825 0.22165413][0.24333611 0.25624678 0.26075464 0.26513344 0.26770315 0.2687743 0.26821807 0.26802754 0.26664159 0.26264581 0.25691855 0.24872881 0.2400395 0.2302262 0.22042255][0.23994517 0.25273356 0.25708884 0.26100791 0.26341611 0.26390189 0.26292855 0.26256093 0.26163581 0.25820529 0.25299144 0.24629219 0.23897813 0.23001553 0.22011107][0.23449223 0.24712464 0.25143516 0.25530306 0.2581909 0.25961062 0.25945869 0.25994843 0.25988948 0.25746781 0.25333324 0.24773388 0.2414114 0.23312356 0.22338413][0.22962634 0.24183877 0.24597256 0.25009745 0.25319195 0.25528079 0.25616804 0.25763559 0.25859845 0.25711814 0.25418743 0.24970102 0.24453449 0.23727633 0.22840849]]...]
INFO - root - 2017-12-09 23:03:14.702784: step 66310, loss = 0.82, batch loss = 0.69 (9.7 examples/sec; 0.821 sec/batch; 60h:42m:13s remains)
INFO - root - 2017-12-09 23:03:23.207946: step 66320, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.892 sec/batch; 65h:56m:23s remains)
INFO - root - 2017-12-09 23:03:31.745307: step 66330, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.851 sec/batch; 62h:55m:09s remains)
INFO - root - 2017-12-09 23:03:40.359468: step 66340, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.852 sec/batch; 62h:58m:35s remains)
INFO - root - 2017-12-09 23:03:48.946816: step 66350, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 64h:07m:03s remains)
INFO - root - 2017-12-09 23:03:57.702916: step 66360, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.875 sec/batch; 64h:43m:13s remains)
INFO - root - 2017-12-09 23:04:06.390238: step 66370, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.877 sec/batch; 64h:48m:29s remains)
INFO - root - 2017-12-09 23:04:15.023409: step 66380, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.860 sec/batch; 63h:33m:16s remains)
INFO - root - 2017-12-09 23:04:23.509490: step 66390, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 64h:20m:55s remains)
INFO - root - 2017-12-09 23:04:31.964243: step 66400, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.880 sec/batch; 65h:04m:22s remains)
2017-12-09 23:04:32.940109: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0018128322 -0.0018120888 -0.0018121036 -0.0018122754 -0.0018124302 -0.0018125167 -0.0018125615 -0.0018125572 -0.0018124849 -0.0018123834 -0.0018122823 -0.0018121629 -0.0018119894 -0.0018118098 -0.0018116796][-0.0018131143 -0.00181261 -0.0018128208 -0.001813222 -0.0018135735 -0.0018138263 -0.001813989 -0.0018140431 -0.0018139782 -0.0018138048 -0.0018135686 -0.0018132584 -0.0018128252 -0.00181234 -0.0018119047][-0.0018143746 -0.0018142908 -0.0018149548 -0.0018157281 -0.00181633 -0.001816733 -0.0018169457 -0.0018170481 -0.0018170346 -0.0018168216 -0.0018164556 -0.0018158796 -0.0018150493 -0.0018140769 -0.0018131065][-0.0018161394 -0.0018167857 -0.001818163 -0.0018195355 -0.0018204751 -0.0018210942 -0.0018214347 -0.0018216139 -0.0018216372 -0.0018214246 -0.0018208687 -0.0018198789 -0.0018184581 -0.0018167774 -0.0018150491][-0.0018180697 -0.001819585 -0.0018217741 -0.0018237705 -0.0018250652 -0.0018258874 -0.0018262421 -0.0018263701 -0.0018263318 -0.0018261855 -0.0018255316 -0.0018241566 -0.0018222237 -0.0018198527 -0.0018173832][-0.0018197655 -0.0018221807 -0.0018251435 -0.0018276214 -0.001829193 -0.0018301373 -0.0018303322 -0.0018300447 -0.0018297532 -0.0018295631 -0.0018289906 -0.0018275013 -0.0018253046 -0.0018225602 -0.0018196472][-0.0018210107 -0.0018240309 -0.0018276001 -0.0018304401 -0.0018321902 -0.0018330893 -0.001832984 -0.0018323133 -0.001831881 -0.0018317472 -0.0018312795 -0.0018298506 -0.0018275627 -0.0018246507 -0.0018214092][-0.0018214638 -0.0018247417 -0.0018285695 -0.0018316044 -0.0018334838 -0.0018344385 -0.00183437 -0.0018337152 -0.0018333317 -0.0018333459 -0.001833023 -0.0018315833 -0.0018291731 -0.0018259987 -0.0018224312][-0.0018210661 -0.0018242509 -0.0018279459 -0.0018308727 -0.0018327691 -0.0018337945 -0.0018339822 -0.0018335071 -0.0018331448 -0.0018331549 -0.0018329173 -0.0018315599 -0.0018291296 -0.0018258997 -0.0018223759][-0.0018197823 -0.0018225288 -0.0018257557 -0.0018283047 -0.0018300463 -0.0018310561 -0.0018314307 -0.0018311546 -0.0018308019 -0.001830666 -0.0018304187 -0.0018292549 -0.0018270878 -0.0018242615 -0.0018211334][-0.0018179764 -0.0018200319 -0.0018225088 -0.0018245723 -0.001826035 -0.0018268995 -0.0018274067 -0.0018274059 -0.0018272228 -0.0018270969 -0.0018268459 -0.00182592 -0.0018241401 -0.0018218338 -0.001819202][-0.0018160782 -0.0018173452 -0.0018190757 -0.0018206306 -0.0018217745 -0.0018224621 -0.0018230625 -0.0018234077 -0.0018235015 -0.0018234488 -0.0018232059 -0.0018224395 -0.0018210075 -0.0018190844 -0.0018169387][-0.0018143354 -0.0018149443 -0.001816012 -0.0018170357 -0.00181784 -0.0018184141 -0.0018190352 -0.0018195765 -0.0018198189 -0.00181979 -0.0018195346 -0.0018188762 -0.0018177149 -0.0018162179 -0.0018147265][-0.0018130401 -0.0018131004 -0.0018136222 -0.0018142062 -0.0018147044 -0.0018151012 -0.001815584 -0.0018160014 -0.0018162068 -0.0018162236 -0.001816013 -0.0018155137 -0.0018147469 -0.0018138429 -0.001812987][-0.0018123237 -0.0018119402 -0.0018120467 -0.0018123691 -0.001812662 -0.0018129185 -0.0018132138 -0.0018134888 -0.001813621 -0.0018136397 -0.0018134884 -0.0018132264 -0.0018128506 -0.0018123986 -0.0018119577]]...]
INFO - root - 2017-12-09 23:04:41.634511: step 66410, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.872 sec/batch; 64h:25m:53s remains)
INFO - root - 2017-12-09 23:04:50.323151: step 66420, loss = 0.81, batch loss = 0.68 (9.1 examples/sec; 0.884 sec/batch; 65h:18m:57s remains)
INFO - root - 2017-12-09 23:04:59.094713: step 66430, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.856 sec/batch; 63h:14m:25s remains)
INFO - root - 2017-12-09 23:05:07.809453: step 66440, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 64h:14m:13s remains)
INFO - root - 2017-12-09 23:05:16.503458: step 66450, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 63h:40m:18s remains)
INFO - root - 2017-12-09 23:05:25.097783: step 66460, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.872 sec/batch; 64h:24m:36s remains)
INFO - root - 2017-12-09 23:05:33.618421: step 66470, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 63h:43m:43s remains)
INFO - root - 2017-12-09 23:05:42.207558: step 66480, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 63h:49m:38s remains)
INFO - root - 2017-12-09 23:05:50.731907: step 66490, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 63h:51m:11s remains)
INFO - root - 2017-12-09 23:05:59.188002: step 66500, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.875 sec/batch; 64h:40m:50s remains)
2017-12-09 23:06:00.041238: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0017212813 -0.0017189407 -0.0017189373 -0.0017193373 -0.0017194649 -0.0017197565 -0.001719558 -0.0017194368 -0.0017195225 -0.0017197912 -0.001720876 -0.0017226444 -0.0017241391 -0.0017248582 -0.0017247319][-0.0017198516 -0.0017165922 -0.0017160464 -0.0017162638 -0.0017164683 -0.0017160214 -0.0017133935 -0.0017021307 -0.001687505 -0.0016769908 -0.0016761171 -0.0016824169 -0.0016942529 -0.0017086779 -0.0017186347][-0.0017213247 -0.0017178823 -0.001716809 -0.0017114483 -0.0016916695 -0.0016578895 -0.0016169122 -0.0015697826 -0.0015341116 -0.0015228717 -0.0015479489 -0.001589561 -0.0016361176 -0.0016793369 -0.0017072217][-0.0017230442 -0.0017174762 -0.0016910575 -0.0015728755 -0.0012971812 -0.00086570857 -0.00039929932 -4.183338e-05 8.10941e-05 -7.5442949e-05 -0.00045874447 -0.00090925168 -0.0012934918 -0.0015489813 -0.0016759394][-0.0017227228 -0.001681637 -0.0014522837 -0.00073648908 0.00067159755 0.0026562866 0.0046954341 0.006162493 0.006539634 0.0056903 0.0039277324 0.001910963 0.00020682754 -0.00092428754 -0.0014955972][-0.0016383756 -0.0014115178 -0.0005239374 0.0017735387 0.0058765616 0.011325772 0.016740723 0.020501193 0.021309678 0.018846113 0.014009164 0.00850852 0.0038295952 0.00066154113 -0.0009994607][-0.0013148773 -0.00059530232 0.0015805458 0.0066831815 0.015290673 0.026272891 0.036899839 0.044040643 0.045272704 0.040044434 0.030223258 0.019161426 0.0097486861 0.0033110199 -0.00013979117][-0.00081784988 0.00062816555 0.0045280969 0.013155575 0.027293293 0.044948943 0.061730251 0.072683126 0.074141487 0.065379322 0.049457334 0.031712227 0.016701758 0.0064448393 0.00090135692][-0.00031491637 0.0018978027 0.0074952911 0.019471088 0.038791448 0.062621824 0.084974043 0.099191755 0.1005498 0.088266142 0.066577859 0.042688686 0.022672337 0.0091098445 0.001791756][-2.2130669e-05 0.0026828772 0.0093724215 0.023407804 0.045901682 0.073436148 0.098990634 0.11482122 0.11571829 0.1010378 0.075779594 0.048305362 0.025557892 0.010353664 0.0022172397][1.9186293e-05 0.0027891281 0.0095157344 0.023490995 0.045836862 0.073069446 0.098087594 0.11311857 0.11326565 0.098179147 0.072953224 0.045909975 0.023885066 0.0094888043 0.0019252532][-0.00024401641 0.002122154 0.0077777454 0.019492421 0.038336556 0.061331123 0.082273886 0.094430879 0.093858279 0.080547869 0.058952056 0.036220714 0.018137041 0.0067421827 0.00098065974][-0.00080090493 0.0007674935 0.0046207262 0.012715407 0.02599174 0.042362683 0.057228297 0.06556946 0.064665452 0.054766968 0.039162118 0.023073081 0.010671266 0.0032850825 -0.00019799778][-0.0013336132 -0.0005665581 0.0014816389 0.0060179844 0.013753108 0.023487639 0.032358356 0.037200402 0.036415018 0.030280434 0.020869704 0.011394873 0.0043724878 0.00050947734 -0.0011127016][-0.0016535633 -0.0014472733 -0.00069500494 0.0012254136 0.0047797631 0.0094423546 0.013785272 0.016150776 0.015755575 0.01277369 0.0082423147 0.0037625968 0.00058359804 -0.00099984976 -0.0015611937]]...]
INFO - root - 2017-12-09 23:06:08.701213: step 66510, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.897 sec/batch; 66h:15m:08s remains)
INFO - root - 2017-12-09 23:06:17.402286: step 66520, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.839 sec/batch; 62h:01m:13s remains)
INFO - root - 2017-12-09 23:06:26.167900: step 66530, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 63h:40m:16s remains)
INFO - root - 2017-12-09 23:06:34.845020: step 66540, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.839 sec/batch; 61h:58m:13s remains)
INFO - root - 2017-12-09 23:06:43.588729: step 66550, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.873 sec/batch; 64h:29m:25s remains)
INFO - root - 2017-12-09 23:06:52.462922: step 66560, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.850 sec/batch; 62h:45m:24s remains)
INFO - root - 2017-12-09 23:07:01.077734: step 66570, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.873 sec/batch; 64h:29m:08s remains)
INFO - root - 2017-12-09 23:07:09.668613: step 66580, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.857 sec/batch; 63h:19m:20s remains)
INFO - root - 2017-12-09 23:07:18.177772: step 66590, loss = 0.82, batch loss = 0.69 (9.7 examples/sec; 0.821 sec/batch; 60h:37m:29s remains)
INFO - root - 2017-12-09 23:07:26.500151: step 66600, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.872 sec/batch; 64h:24m:06s remains)
2017-12-09 23:07:27.417673: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0018325964 -0.0018319702 -0.0018312741 -0.0018303607 -0.0018287683 -0.0018266821 -0.0018243726 -0.0018227962 -0.0018224133 -0.0018224366 -0.001822571 -0.0018224283 -0.0018220908 -0.0018219108 -0.0018221283][-0.0018302668 -0.0018303817 -0.0018306366 -0.0018306014 -0.001829911 -0.0018283348 -0.001826343 -0.0018248609 -0.0018244026 -0.0018244351 -0.0018244323 -0.0018241436 -0.0018236628 -0.0018234435 -0.0018237884][-0.0018284862 -0.0018291887 -0.0018302184 -0.0018310611 -0.0018313441 -0.0018304152 -0.0018288104 -0.0018275628 -0.0018271104 -0.0018270678 -0.0018270307 -0.0018267207 -0.0018261719 -0.0018259358 -0.0018263515][-0.0018276261 -0.0018287182 -0.0018304667 -0.0018319425 -0.001832788 -0.0018324465 -0.0018313481 -0.0018304174 -0.0018300076 -0.0018298957 -0.0018297853 -0.0018295655 -0.0018290124 -0.0018287605 -0.0018291278][-0.001827732 -0.0018291653 -0.0018315309 -0.0018334403 -0.0018345284 -0.0018346065 -0.001833963 -0.0018332558 -0.0018327442 -0.0018325018 -0.0018322199 -0.001831888 -0.0018313053 -0.00183097 -0.0018311893][-0.0018283257 -0.0018298916 -0.0018327534 -0.0018350994 -0.001836382 -0.0018365785 -0.0018362469 -0.0018354995 -0.0018347719 -0.0018342661 -0.0018338592 -0.0018334925 -0.0018329094 -0.0018324273 -0.0018323328][-0.0018289401 -0.0018305461 -0.0018336851 -0.001836132 -0.001837453 -0.0018376934 -0.0018376037 -0.0018368853 -0.0018359039 -0.0018352388 -0.0018348811 -0.0018346107 -0.0018339778 -0.0018331611 -0.0018325242][-0.0018290571 -0.0018307718 -0.001833863 -0.0018363297 -0.0018373787 -0.0018376955 -0.0018377919 -0.0018372469 -0.0018363575 -0.0018355495 -0.0018350141 -0.0018345952 -0.0018337022 -0.0018324917 -0.0018313032][-0.0018285571 -0.0018302622 -0.0018333329 -0.0018356421 -0.0018364855 -0.0018368043 -0.0018368049 -0.0018361348 -0.0018351636 -0.0018342362 -0.0018335024 -0.0018327333 -0.0018316313 -0.0018301605 -0.0018287111][-0.0018274517 -0.001829111 -0.0018321481 -0.0018343908 -0.0018353863 -0.0018357137 -0.001835459 -0.0018343953 -0.0018329356 -0.0018316627 -0.0018306741 -0.0018295327 -0.0018281102 -0.0018264941 -0.0018248959][-0.0018259429 -0.0018274864 -0.0018305071 -0.0018328283 -0.0018341244 -0.0018346619 -0.0018341152 -0.0018324425 -0.0018304042 -0.0018287044 -0.0018273075 -0.0018257126 -0.0018239847 -0.0018222916 -0.0018208751][-0.0018244578 -0.0018258448 -0.0018289195 -0.0018313262 -0.001833091 -0.0018337633 -0.0018328893 -0.0018307545 -0.001828062 -0.0018258252 -0.0018238351 -0.0018218217 -0.0018200356 -0.001818614 -0.0018175475][-0.0018234127 -0.0018247768 -0.0018278192 -0.0018302103 -0.0018320993 -0.0018326219 -0.0018313302 -0.0018287358 -0.0018256514 -0.001823143 -0.0018209402 -0.0018189264 -0.0018173492 -0.0018164481 -0.001815889][-0.0018229976 -0.0018242964 -0.0018274328 -0.0018298519 -0.0018315462 -0.0018317545 -0.0018300185 -0.001826935 -0.0018236628 -0.0018211602 -0.0018191452 -0.0018174053 -0.0018160768 -0.0018153718 -0.0018150267][-0.0018235708 -0.001824595 -0.0018274955 -0.0018297853 -0.0018313344 -0.0018312213 -0.0018292218 -0.0018261251 -0.0018228537 -0.0018205165 -0.0018186809 -0.0018172015 -0.0018159603 -0.0018151941 -0.0018146798]]...]
INFO - root - 2017-12-09 23:07:36.075058: step 66610, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.863 sec/batch; 63h:45m:33s remains)
INFO - root - 2017-12-09 23:07:44.727913: step 66620, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.863 sec/batch; 63h:43m:13s remains)
INFO - root - 2017-12-09 23:07:53.464050: step 66630, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.895 sec/batch; 66h:03m:47s remains)
INFO - root - 2017-12-09 23:08:02.172156: step 66640, loss = 0.83, batch loss = 0.70 (9.2 examples/sec; 0.868 sec/batch; 64h:06m:54s remains)
INFO - root - 2017-12-09 23:08:11.004310: step 66650, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.852 sec/batch; 62h:56m:38s remains)
INFO - root - 2017-12-09 23:08:19.733056: step 66660, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.887 sec/batch; 65h:30m:07s remains)
INFO - root - 2017-12-09 23:08:28.404104: step 66670, loss = 0.82, batch loss = 0.70 (9.3 examples/sec; 0.862 sec/batch; 63h:39m:12s remains)
INFO - root - 2017-12-09 23:08:37.111350: step 66680, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.861 sec/batch; 63h:33m:37s remains)
INFO - root - 2017-12-09 23:08:45.640919: step 66690, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.876 sec/batch; 64h:39m:12s remains)
INFO - root - 2017-12-09 23:08:53.955113: step 66700, loss = 0.82, batch loss = 0.69 (10.8 examples/sec; 0.744 sec/batch; 54h:54m:40s remains)
2017-12-09 23:08:54.870535: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0016687429 -0.00168764 -0.0017068106 -0.0017273594 -0.0017386646 -0.0017444415 -0.0017396292 -0.0017310539 -0.0017137312 -0.0016907965 -0.0016703729 -0.001657792 -0.0016572046 -0.0016607398 -0.0016537246][-0.0016501234 -0.0016722807 -0.0016958918 -0.0017212429 -0.0017376572 -0.0017480974 -0.0017475137 -0.0017417471 -0.0017256476 -0.0017017508 -0.0016766693 -0.0016554598 -0.0016492234 -0.0016502817 -0.0016452175][-0.0016384898 -0.001659965 -0.0016858375 -0.0017162225 -0.0017384035 -0.0017536153 -0.0017571005 -0.0017535842 -0.0017391468 -0.0017150922 -0.0016869443 -0.0016572465 -0.0016447149 -0.0016400776 -0.0016395277][-0.0016313888 -0.001650044 -0.0016756746 -0.0017087739 -0.001735879 -0.0017550969 -0.0017612358 -0.0017605955 -0.0017478805 -0.0017233676 -0.0016915244 -0.0016573106 -0.0016371601 -0.0016291272 -0.0016333174][-0.0016236923 -0.0016382019 -0.001660565 -0.0016935723 -0.0017231168 -0.0017450145 -0.0017536526 -0.001754691 -0.0017433943 -0.0017200912 -0.0016864756 -0.0016506204 -0.0016262685 -0.0016154136 -0.001623896][-0.0016147401 -0.0016241048 -0.0016419494 -0.0016711239 -0.0016995789 -0.0017221487 -0.001732543 -0.001735092 -0.0017265889 -0.0017062646 -0.0016744798 -0.0016401256 -0.0016148553 -0.0016027971 -0.0016111621][-0.0016092137 -0.0016131049 -0.0016250187 -0.0016479231 -0.0016726791 -0.0016921961 -0.0017021053 -0.0017057307 -0.0017013004 -0.0016850682 -0.0016572648 -0.0016292129 -0.0016039308 -0.0015915064 -0.0015991497][-0.0016150514 -0.0016147025 -0.0016199037 -0.0016344448 -0.0016520232 -0.0016659563 -0.0016734973 -0.0016763802 -0.001674991 -0.0016645971 -0.0016445073 -0.0016239177 -0.0016010123 -0.0015896526 -0.0015954498][-0.0016321408 -0.001629476 -0.0016295423 -0.0016353474 -0.0016441911 -0.0016509766 -0.0016548651 -0.0016574694 -0.0016594161 -0.0016547154 -0.0016428488 -0.0016292498 -0.001613669 -0.0016031103 -0.0016032341][-0.0016555317 -0.0016517572 -0.0016475733 -0.0016462667 -0.0016471521 -0.0016490823 -0.0016509034 -0.0016550325 -0.0016614726 -0.0016616547 -0.0016562683 -0.0016474646 -0.0016383908 -0.0016244777 -0.0016181925][-0.0016811222 -0.0016754285 -0.0016686333 -0.0016625222 -0.0016591111 -0.0016584382 -0.0016596093 -0.0016659448 -0.0016758452 -0.0016816683 -0.0016828597 -0.0016758512 -0.0016705472 -0.0016530112 -0.0016385727][-0.0017040952 -0.0016978597 -0.0016900329 -0.0016811845 -0.0016762085 -0.0016750755 -0.0016770029 -0.0016853828 -0.0016985203 -0.0017070833 -0.0017131264 -0.00171079 -0.0017086501 -0.0016879452 -0.0016618596][-0.0017192216 -0.0017133727 -0.0017063243 -0.0016987787 -0.0016962917 -0.0016980066 -0.0017026986 -0.0017125502 -0.0017267835 -0.0017367303 -0.0017438789 -0.0017432244 -0.0017392737 -0.0017163492 -0.0016825007][-0.0017258973 -0.0017208871 -0.0017158728 -0.0017112554 -0.0017129736 -0.001719194 -0.0017276471 -0.001738407 -0.0017516571 -0.0017618416 -0.0017680858 -0.0017680327 -0.0017594564 -0.0017334096 -0.0016939571][-0.0017271079 -0.0017225614 -0.0017197089 -0.0017196176 -0.0017266772 -0.0017376755 -0.0017489131 -0.0017601368 -0.0017699237 -0.0017772118 -0.0017801104 -0.0017791999 -0.0017687054 -0.001740195 -0.0016981049]]...]
INFO - root - 2017-12-09 23:09:03.367399: step 66710, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.840 sec/batch; 61h:59m:05s remains)
INFO - root - 2017-12-09 23:09:11.991691: step 66720, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.884 sec/batch; 65h:17m:14s remains)
INFO - root - 2017-12-09 23:09:20.812245: step 66730, loss = 0.82, batch loss = 0.69 (8.7 examples/sec; 0.915 sec/batch; 67h:33m:08s remains)
INFO - root - 2017-12-09 23:09:29.597179: step 66740, loss = 0.83, batch loss = 0.70 (9.2 examples/sec; 0.867 sec/batch; 63h:58m:29s remains)
INFO - root - 2017-12-09 23:09:38.330349: step 66750, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.876 sec/batch; 64h:38m:38s remains)
INFO - root - 2017-12-09 23:09:47.158916: step 66760, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.874 sec/batch; 64h:31m:29s remains)
INFO - root - 2017-12-09 23:09:55.874783: step 66770, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.875 sec/batch; 64h:37m:14s remains)
INFO - root - 2017-12-09 23:10:04.567404: step 66780, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.888 sec/batch; 65h:33m:44s remains)
INFO - root - 2017-12-09 23:10:13.147966: step 66790, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.872 sec/batch; 64h:21m:47s remains)
INFO - root - 2017-12-09 23:10:21.788302: step 66800, loss = 0.82, batch loss = 0.69 (10.6 examples/sec; 0.752 sec/batch; 55h:30m:56s remains)
2017-12-09 23:10:22.723296: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.00053662225 -0.00048983982 -0.00035015889 -0.00014480134 0.00014202541 0.0003391559 0.00053518836 0.00073998387 0.00099997187 0.0013835855 0.0019171176 0.0027003055 0.0034588091 0.0039210552 0.0038374886][-0.00032496441 -0.00026787631 -4.2892527e-05 0.00031420856 0.00070400175 0.00094941689 0.0011174603 0.001254825 0.0014742134 0.0018655242 0.0024434733 0.0032366775 0.0039873417 0.0044280295 0.0043907347][-0.00019448297 -8.8195549e-05 0.00024521409 0.00073328277 0.001193971 0.0014282685 0.0015098174 0.0015938467 0.0017958652 0.0021595326 0.0027115368 0.0034319302 0.0040543904 0.0044042533 0.0044325995][-0.00011390308 1.566112e-05 0.00041938818 0.0010030436 0.00148455 0.0017495988 0.0018178365 0.0018864562 0.0020600911 0.0023707976 0.0028310078 0.0034020087 0.0038962127 0.00424829 0.0043225456][-8.7645487e-05 4.6679052e-05 0.00045602664 0.00099745381 0.0014579325 0.0017459238 0.0019343508 0.0020884927 0.0022830497 0.0025276304 0.0028405278 0.0032399716 0.0036260784 0.0039183595 0.0040278812][-0.00014313404 3.45218e-06 0.0003741273 0.00083852594 0.0012837778 0.0016116226 0.0018755415 0.0021584448 0.002367971 0.002535874 0.0026826053 0.0029345751 0.0031572473 0.0033527412 0.0033873622][-0.00025945518 -0.00012411759 0.00017437909 0.00056019856 0.0009711493 0.0013367584 0.0016746206 0.0019617043 0.002146055 0.0022219252 0.0022547664 0.0023562117 0.0024687219 0.0025480264 0.0025152406][-0.00051268435 -0.00039298367 -0.00013458787 0.00021997618 0.00061166903 0.00099195947 0.001325576 0.001567229 0.0016539154 0.0016461961 0.001593087 0.0016086871 0.0016224677 0.0015926732 0.0014646907][-0.00085863855 -0.00074578263 -0.000502001 -0.00015445391 0.00021320616 0.00054776424 0.00080065324 0.00098722486 0.0010503501 0.0010173655 0.0010009249 0.00097876915 0.00096889527 0.000901756 0.0007327731][-0.0010838456 -0.0010027447 -0.00079987792 -0.00049631309 -0.00018990063 9.0755522e-05 0.00028664677 0.00043625582 0.00051832327 0.00054922618 0.000571485 0.00061421946 0.00065514015 0.00062872295 0.00046894012][-0.00099035306 -0.00096365967 -0.00083715946 -0.00062500651 -0.00040302088 -0.00019963982 -2.7211383e-05 0.00015227019 0.00031410449 0.00041146309 0.00050582096 0.00060404919 0.00064310886 0.00065212941 0.00054267875][-0.00061478105 -0.00060701033 -0.00053832319 -0.00043350621 -0.00031797169 -0.00017960789 -1.0642223e-05 0.00018638337 0.00034859439 0.00047354738 0.00055402459 0.00070295075 0.00078005821 0.000749593 0.00069034251][-0.00031997275 -0.00025780569 -0.00017804664 -0.00011560344 -7.0754671e-05 -2.0037405e-06 0.00013027934 0.00032094785 0.00045267877 0.00055360084 0.00059063861 0.00072015019 0.00082871562 0.00082640687 0.00080180622][-0.00018384994 -0.00010879675 -2.6904629e-05 1.9713887e-05 2.890802e-05 6.24361e-05 0.00015859108 0.00030637684 0.00042378285 0.00048077048 0.000520507 0.00058739155 0.00066136441 0.00069720217 0.00074457412][-0.00017488911 -0.00011445605 -5.39436e-05 -1.7234008e-05 -3.1646807e-05 -4.10676e-05 -9.4510615e-06 8.3121355e-05 0.00016792852 0.00021986954 0.00025221787 0.00032451947 0.00038281863 0.0004459637 0.00052516547]]...]
INFO - root - 2017-12-09 23:10:31.353588: step 66810, loss = 0.83, batch loss = 0.70 (9.6 examples/sec; 0.831 sec/batch; 61h:21m:09s remains)
INFO - root - 2017-12-09 23:10:39.745762: step 66820, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.872 sec/batch; 64h:23m:02s remains)
INFO - root - 2017-12-09 23:10:48.376773: step 66830, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.851 sec/batch; 62h:47m:55s remains)
INFO - root - 2017-12-09 23:10:57.083820: step 66840, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 63h:38m:15s remains)
INFO - root - 2017-12-09 23:11:05.841723: step 66850, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.896 sec/batch; 66h:08m:21s remains)
INFO - root - 2017-12-09 23:11:14.536311: step 66860, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 64h:08m:15s remains)
INFO - root - 2017-12-09 23:11:23.193919: step 66870, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.902 sec/batch; 66h:34m:34s remains)
INFO - root - 2017-12-09 23:11:31.839381: step 66880, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.855 sec/batch; 63h:04m:25s remains)
INFO - root - 2017-12-09 23:11:40.585189: step 66890, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.875 sec/batch; 64h:33m:29s remains)
INFO - root - 2017-12-09 23:11:49.251916: step 66900, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.899 sec/batch; 66h:21m:45s remains)
2017-12-09 23:11:50.059090: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0018231552 -0.0018145619 -0.0018044098 -0.0017929611 -0.0017829851 -0.0017808742 -0.0017846588 -0.0017963467 -0.0018080656 -0.001819422 -0.0018275888 -0.0018336074 -0.001836966 -0.001838145 -0.0018373957][-0.00179621 -0.0017777788 -0.0017597765 -0.0017436941 -0.0017291035 -0.0017202641 -0.0017206862 -0.0017374413 -0.0017610588 -0.0017865429 -0.0018059664 -0.0018199302 -0.0018290851 -0.0018345603 -0.0018359961][-0.0017391965 -0.0016974899 -0.0016599889 -0.0016324212 -0.0016133862 -0.0016058399 -0.0016120535 -0.0016377617 -0.0016785072 -0.0017240003 -0.0017619345 -0.001790693 -0.0018122895 -0.0018276647 -0.0018340084][-0.0016516405 -0.001567112 -0.0014939618 -0.001444626 -0.0014152192 -0.0014016776 -0.0014114433 -0.0014534844 -0.0015259575 -0.0016071292 -0.0016765812 -0.0017333501 -0.0017792323 -0.0018130083 -0.0018301966][-0.0015187786 -0.0013438384 -0.0011804006 -0.0010612358 -0.0010007436 -0.00099110568 -0.0010304896 -0.0011253275 -0.0012541621 -0.0013960339 -0.0015162718 -0.0016185672 -0.0017084633 -0.0017782839 -0.0018203795][-0.0013320534 -0.001005857 -0.00069066568 -0.00046501984 -0.00035718712 -0.000339263 -0.00040378456 -0.000559803 -0.0007802475 -0.0010386128 -0.0012603115 -0.0014413539 -0.001596145 -0.0017134984 -0.0017965591][-0.0011783708 -0.00071476842 -0.00025739672 6.154587e-05 0.00020338909 0.00022882165 0.00017644034 1.7797458e-05 -0.00025854132 -0.0006323969 -0.000972559 -0.0012440793 -0.0014609231 -0.0016251155 -0.0017569913][-0.0011448837 -0.00064974371 -0.00015846663 0.00017875165 0.00032331224 0.00036978314 0.00039111066 0.00031551637 6.4900611e-05 -0.00035586802 -0.00077077455 -0.0011051485 -0.0013616439 -0.0015540977 -0.001718828][-0.001304494 -0.0009260251 -0.00054013322 -0.00027290895 -0.00014911511 -6.4636464e-05 7.5381831e-05 0.00016402698 5.0141825e-05 -0.00029776 -0.00070801156 -0.001062446 -0.001334043 -0.0015370952 -0.0017067702][-0.0015025316 -0.0012712551 -0.0010253179 -0.00084893859 -0.00075165881 -0.00065116235 -0.00045115792 -0.00023793499 -0.00018649455 -0.0003858807 -0.00071991747 -0.0010589315 -0.0013423414 -0.0015567697 -0.0017177163][-0.001672778 -0.001548543 -0.0014059957 -0.0012961961 -0.0012294431 -0.0011530862 -0.00098153763 -0.00075146812 -0.00059386925 -0.00062383479 -0.00081972207 -0.0010937195 -0.0013640234 -0.0015837366 -0.0017364486][-0.0017554938 -0.0016875529 -0.0016085313 -0.0015470528 -0.001511571 -0.0014747545 -0.0013774944 -0.0012165769 -0.0010597404 -0.00099510513 -0.0010605449 -0.001231736 -0.001443482 -0.0016334761 -0.001760289][-0.0018028833 -0.0017719647 -0.0017356108 -0.0017088631 -0.0016977453 -0.0016909809 -0.0016568952 -0.0015795947 -0.0014801063 -0.0014064973 -0.0013989 -0.0014681467 -0.0015855593 -0.0017063327 -0.0017869833][-0.0018240513 -0.0018164429 -0.0018071145 -0.0017999862 -0.0017981686 -0.0017992972 -0.0017937889 -0.0017718506 -0.0017353834 -0.0016986887 -0.0016812554 -0.0016973264 -0.0017395184 -0.001785354 -0.001814866][-0.0018275829 -0.0018257087 -0.0018239315 -0.001822729 -0.0018226807 -0.001823823 -0.0018250841 -0.0018246525 -0.0018207566 -0.0018137095 -0.0018072843 -0.0018064732 -0.0018128741 -0.0018210446 -0.0018264003]]...]
INFO - root - 2017-12-09 23:11:58.557496: step 66910, loss = 0.81, batch loss = 0.68 (9.3 examples/sec; 0.857 sec/batch; 63h:15m:19s remains)
INFO - root - 2017-12-09 23:12:07.043922: step 66920, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 64h:08m:23s remains)
INFO - root - 2017-12-09 23:12:15.751636: step 66930, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 63h:42m:44s remains)
INFO - root - 2017-12-09 23:12:24.375877: step 66940, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.853 sec/batch; 62h:53m:19s remains)
INFO - root - 2017-12-09 23:12:33.045108: step 66950, loss = 0.81, batch loss = 0.68 (9.5 examples/sec; 0.841 sec/batch; 62h:04m:00s remains)
INFO - root - 2017-12-09 23:12:41.732086: step 66960, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.879 sec/batch; 64h:51m:43s remains)
INFO - root - 2017-12-09 23:12:50.414513: step 66970, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.858 sec/batch; 63h:18m:59s remains)
INFO - root - 2017-12-09 23:12:59.020377: step 66980, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.874 sec/batch; 64h:26m:53s remains)
INFO - root - 2017-12-09 23:13:07.433871: step 66990, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.851 sec/batch; 62h:44m:41s remains)
INFO - root - 2017-12-09 23:13:15.900587: step 67000, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.865 sec/batch; 63h:49m:28s remains)
2017-12-09 23:13:16.818000: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0018286685 -0.0018310846 -0.0018340244 -0.0018364951 -0.0018385211 -0.0018401633 -0.0018411233 -0.0018412052 -0.0018408188 -0.0018402267 -0.0018395521 -0.001838539 -0.0018374616 -0.0018365637 -0.0018360683][-0.0018286437 -0.0018309838 -0.0018338587 -0.001836509 -0.0018387444 -0.0018404106 -0.0018411607 -0.0018409502 -0.0018402371 -0.0018391974 -0.001838058 -0.0018367345 -0.0018354027 -0.0018344739 -0.0018339904][-0.0018287436 -0.0018309451 -0.0018337135 -0.0018365313 -0.001838848 -0.0018405342 -0.0018411726 -0.0018407545 -0.0018396662 -0.0018383417 -0.0018369172 -0.0018353474 -0.0018338128 -0.0018328866 -0.0018324442][-0.001828626 -0.0018304561 -0.0018328974 -0.0018357014 -0.0018381053 -0.0018398802 -0.0018406815 -0.0018403704 -0.0018393277 -0.0018380248 -0.0018365812 -0.0018349625 -0.0018334573 -0.0018326978 -0.0018321697][-0.0018282675 -0.0018295496 -0.0018315224 -0.0018341832 -0.0018366812 -0.0018384962 -0.0018395014 -0.0018396681 -0.0018390238 -0.0018378762 -0.0018365447 -0.0018351146 -0.0018338453 -0.0018331403 -0.0018326102][-0.0018276513 -0.0018285039 -0.0018301281 -0.0018326307 -0.0018351432 -0.0018370908 -0.0018382975 -0.0018388036 -0.0018385717 -0.0018377651 -0.0018365571 -0.0018351435 -0.0018340073 -0.0018334285 -0.0018330498][-0.0018270046 -0.001827405 -0.0018286339 -0.0018308654 -0.0018332716 -0.001835387 -0.0018369579 -0.0018379182 -0.0018380262 -0.001837493 -0.0018364723 -0.0018352552 -0.0018342372 -0.0018337823 -0.0018334872][-0.0018260189 -0.0018260654 -0.0018269802 -0.0018287549 -0.0018308428 -0.0018328405 -0.0018346481 -0.0018359566 -0.0018363466 -0.0018360956 -0.0018353539 -0.0018343312 -0.0018334482 -0.0018328957 -0.0018324896][-0.0018249503 -0.0018246812 -0.0018253041 -0.0018267159 -0.0018285604 -0.0018304108 -0.0018322835 -0.0018338516 -0.0018344596 -0.0018342565 -0.0018335682 -0.0018326922 -0.0018318922 -0.0018311555 -0.0018305712][-0.0018241464 -0.0018237188 -0.0018240816 -0.0018251515 -0.0018266798 -0.0018283213 -0.0018299768 -0.0018315125 -0.0018322237 -0.0018320554 -0.0018313573 -0.0018305404 -0.0018297584 -0.0018288464 -0.001828061][-0.0018234664 -0.0018228977 -0.0018231107 -0.0018239485 -0.0018251871 -0.0018265799 -0.0018279954 -0.0018292635 -0.0018298848 -0.0018297487 -0.0018291337 -0.0018283256 -0.0018274648 -0.0018265314 -0.0018257468][-0.0018227741 -0.0018221873 -0.0018222905 -0.0018228375 -0.0018237183 -0.0018247786 -0.0018259269 -0.0018269885 -0.0018275537 -0.0018274572 -0.0018269501 -0.0018261545 -0.0018252453 -0.0018243152 -0.0018235306][-0.0018222246 -0.0018217349 -0.0018217819 -0.0018220481 -0.0018225656 -0.0018231977 -0.0018239301 -0.00182468 -0.0018251513 -0.0018250926 -0.0018246677 -0.0018240307 -0.0018232258 -0.001822377 -0.001821684][-0.0018219618 -0.0018214741 -0.0018215153 -0.0018217341 -0.0018220742 -0.0018224634 -0.0018228556 -0.0018232929 -0.0018235701 -0.0018234703 -0.0018231341 -0.001822695 -0.0018221822 -0.0018216432 -0.0018212111][-0.0018218567 -0.0018213409 -0.0018213806 -0.001821608 -0.0018218948 -0.0018222044 -0.0018224855 -0.0018227341 -0.0018228895 -0.0018227973 -0.0018225462 -0.001822249 -0.0018219507 -0.0018216721 -0.0018214708]]...]
INFO - root - 2017-12-09 23:13:25.233545: step 67010, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.883 sec/batch; 65h:05m:56s remains)
INFO - root - 2017-12-09 23:13:33.752900: step 67020, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.866 sec/batch; 63h:50m:05s remains)
INFO - root - 2017-12-09 23:13:42.434175: step 67030, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.879 sec/batch; 64h:49m:49s remains)
INFO - root - 2017-12-09 23:13:51.170288: step 67040, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.879 sec/batch; 64h:46m:57s remains)
INFO - root - 2017-12-09 23:13:59.932037: step 67050, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.870 sec/batch; 64h:10m:59s remains)
INFO - root - 2017-12-09 23:14:08.535393: step 67060, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.884 sec/batch; 65h:12m:39s remains)
INFO - root - 2017-12-09 23:14:17.243704: step 67070, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.840 sec/batch; 61h:53m:53s remains)
INFO - root - 2017-12-09 23:14:26.061107: step 67080, loss = 0.81, batch loss = 0.68 (9.2 examples/sec; 0.869 sec/batch; 64h:02m:25s remains)
INFO - root - 2017-12-09 23:14:34.523239: step 67090, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 63h:54m:11s remains)
INFO - root - 2017-12-09 23:14:43.354299: step 67100, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 63h:34m:37s remains)
2017-12-09 23:14:44.365014: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.010537795 0.011310633 0.011594324 0.011312037 0.010455298 0.0096546942 0.0093446951 0.0093553206 0.0094366781 0.00947663 0.0092399418 0.0088923005 0.0086654695 0.0086524859 0.0085303029][0.011055935 0.011909657 0.012163804 0.011892573 0.011108261 0.010302789 0.0099829547 0.009952195 0.0099796634 0.0097627053 0.0094453394 0.0092667872 0.0090589616 0.0090212841 0.0089537865][0.010935972 0.011640648 0.01169108 0.011427309 0.010781888 0.010113114 0.0098663708 0.0098227579 0.0098393187 0.0096298167 0.0092927981 0.0091121634 0.0089433808 0.0089190081 0.0089346124][0.010735766 0.011183897 0.010926996 0.010507962 0.010102688 0.009702405 0.0096145775 0.0096462341 0.0097633861 0.00968187 0.0094311032 0.0092257624 0.0090788258 0.0090350006 0.0090453206][0.010554279 0.010700523 0.010276386 0.0097460086 0.0094064968 0.0093345484 0.0094662039 0.0097231129 0.010034918 0.010146776 0.010035601 0.0097317863 0.009435907 0.009311487 0.0093023013][0.010527011 0.010466942 0.0098016821 0.009116671 0.0089502186 0.009064272 0.0093341656 0.0099096633 0.010496306 0.010817064 0.010758387 0.01035891 0.0099316044 0.0095286295 0.0094853509][0.010367291 0.01031831 0.00956888 0.0087716561 0.0085314242 0.0086999582 0.0091970218 0.0099605434 0.010704787 0.011285174 0.011283488 0.010779497 0.010213732 0.009599478 0.0094497073][0.010009882 0.0099669434 0.00915476 0.0084574725 0.0081303781 0.0082245618 0.0087184953 0.0095076477 0.010440414 0.01107209 0.011031187 0.01060005 0.010047293 0.0093807932 0.0091772312][0.0092284493 0.009329007 0.0086342171 0.007917474 0.0074566673 0.0075611896 0.0079549942 0.008639046 0.0094175059 0.0099287778 0.00997205 0.0095884828 0.0091116158 0.0086394893 0.0085338866][0.0080806948 0.00829899 0.0077775014 0.0071934597 0.0067240088 0.006628837 0.0069133383 0.0074038464 0.0078900279 0.00816741 0.0081043644 0.00776854 0.0074053011 0.007165683 0.0071599162][0.006649869 0.0070274663 0.0067444681 0.0062727467 0.0058298893 0.0056481864 0.0057508866 0.0060190684 0.0061596534 0.0061543132 0.0059643951 0.0056735864 0.0054306635 0.0052700117 0.005272171][0.005233712 0.0056189941 0.0055403956 0.0052619078 0.0049040648 0.0046347557 0.0045892303 0.004602192 0.0044519114 0.0042621638 0.00392328 0.0036569322 0.0034782235 0.0033790101 0.0033767642][0.0037346282 0.0041052653 0.0041229529 0.0039817472 0.0037237043 0.0035282797 0.0034380336 0.0032443977 0.0028761066 0.0024976539 0.002063924 0.0018504489 0.0017023782 0.0016639355 0.0016605662][0.0021367176 0.0023948578 0.0024617785 0.0024279235 0.0022613308 0.0021411809 0.0020227525 0.0017815036 0.0014101317 0.00096401863 0.00055582693 0.00033280125 0.00018950284 0.00021244597 0.0002238591][0.0005232367 0.00070985581 0.00079948304 0.00077776553 0.000692441 0.00063535536 0.00052566559 0.00031588727 -6.2917825e-06 -0.00035749527 -0.00063369318 -0.00081504928 -0.00089061627 -0.00088215317 -0.00084912649]]...]
INFO - root - 2017-12-09 23:14:52.728962: step 67110, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.858 sec/batch; 63h:14m:41s remains)
INFO - root - 2017-12-09 23:15:01.222744: step 67120, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.841 sec/batch; 62h:00m:46s remains)
INFO - root - 2017-12-09 23:15:09.883788: step 67130, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.881 sec/batch; 64h:57m:02s remains)
INFO - root - 2017-12-09 23:15:18.632319: step 67140, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.881 sec/batch; 64h:55m:36s remains)
INFO - root - 2017-12-09 23:15:27.469635: step 67150, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.885 sec/batch; 65h:12m:15s remains)
INFO - root - 2017-12-09 23:15:36.258505: step 67160, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.891 sec/batch; 65h:38m:47s remains)
INFO - root - 2017-12-09 23:15:45.003995: step 67170, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 63h:54m:50s remains)
INFO - root - 2017-12-09 23:15:53.628471: step 67180, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 64h:13m:03s remains)
INFO - root - 2017-12-09 23:16:02.156034: step 67190, loss = 0.82, batch loss = 0.69 (9.7 examples/sec; 0.824 sec/batch; 60h:42m:55s remains)
INFO - root - 2017-12-09 23:16:10.632334: step 67200, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.834 sec/batch; 61h:27m:40s remains)
2017-12-09 23:16:11.569497: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.30911097 0.30389977 0.29836598 0.29218638 0.28586119 0.27910861 0.27258229 0.26720247 0.26157805 0.25519344 0.24893963 0.24338958 0.23670794 0.23025736 0.22550775][0.30624568 0.30310768 0.29897031 0.29428995 0.28889036 0.28396377 0.27915108 0.27487993 0.27052271 0.26505736 0.25909704 0.2525529 0.24460259 0.23678981 0.2304762][0.29973722 0.29879937 0.29668379 0.29409641 0.29088473 0.28819337 0.28531355 0.28224593 0.2788187 0.27369839 0.26722285 0.25913891 0.24955267 0.24009202 0.23205006][0.29328659 0.2947374 0.29452804 0.2946502 0.29427451 0.29469547 0.29453903 0.29303452 0.29000515 0.28454745 0.2765314 0.26680002 0.25513986 0.24377264 0.23434368][0.28701106 0.29060602 0.29226515 0.29536998 0.29838729 0.30145058 0.30377403 0.30426162 0.30196416 0.29588869 0.28610611 0.27472645 0.26129231 0.24785174 0.2369363][0.28300166 0.28742233 0.28998506 0.29511294 0.30082375 0.30708325 0.31238121 0.31496304 0.31373587 0.30738252 0.29601443 0.28212839 0.26670927 0.25165677 0.23922387][0.28316489 0.28841549 0.2919502 0.29813471 0.30515873 0.31316292 0.31996915 0.32407692 0.32309708 0.31634146 0.30429015 0.28901851 0.27244422 0.25606591 0.24280758][0.28364348 0.29064584 0.29503214 0.30168563 0.30954173 0.31829152 0.32511422 0.32919803 0.32741463 0.31985348 0.30691233 0.2911734 0.27450496 0.25750443 0.24405159][0.28523996 0.29311788 0.29704103 0.30366695 0.31139302 0.32012251 0.32653147 0.32993174 0.32775688 0.31970745 0.30649942 0.290812 0.27429524 0.25758335 0.24417016][0.28628 0.29460245 0.29793862 0.3034215 0.31057277 0.31798235 0.32307193 0.32522953 0.32241103 0.31443954 0.30166015 0.28684372 0.27121261 0.25543737 0.242644][0.28428474 0.29273212 0.2953144 0.29983461 0.30600083 0.31221217 0.31580997 0.31674519 0.31340581 0.30507907 0.29291576 0.27879786 0.26447183 0.25031012 0.23878781][0.27959633 0.28806126 0.28944331 0.292189 0.29634309 0.30047685 0.30190676 0.30220217 0.29891995 0.29130012 0.28094667 0.26910654 0.25731429 0.24508375 0.23524855][0.27402645 0.28212881 0.28213996 0.28322396 0.28540075 0.28706369 0.28650516 0.2856814 0.28218895 0.27526969 0.26642984 0.25691587 0.2477355 0.23836328 0.23079072][0.26884109 0.27590451 0.27453709 0.2741107 0.27453744 0.27429208 0.27276325 0.27127951 0.26815683 0.26236507 0.25534812 0.24805681 0.241072 0.23440002 0.22891372][0.26382709 0.27063382 0.26859432 0.2668829 0.26569486 0.26387307 0.26135981 0.25956964 0.25670537 0.25190616 0.24656707 0.24126597 0.23659176 0.23211277 0.22844921]]...]
INFO - root - 2017-12-09 23:16:19.914066: step 67210, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.854 sec/batch; 62h:57m:42s remains)
INFO - root - 2017-12-09 23:16:28.408382: step 67220, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 63h:51m:57s remains)
INFO - root - 2017-12-09 23:16:37.077614: step 67230, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.872 sec/batch; 64h:16m:37s remains)
INFO - root - 2017-12-09 23:16:45.753559: step 67240, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.838 sec/batch; 61h:42m:47s remains)
INFO - root - 2017-12-09 23:16:54.330114: step 67250, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.845 sec/batch; 62h:17m:26s remains)
INFO - root - 2017-12-09 23:17:02.841518: step 67260, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.865 sec/batch; 63h:44m:59s remains)
INFO - root - 2017-12-09 23:17:11.537457: step 67270, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.857 sec/batch; 63h:07m:22s remains)
INFO - root - 2017-12-09 23:17:20.188764: step 67280, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.860 sec/batch; 63h:19m:57s remains)
INFO - root - 2017-12-09 23:17:28.805956: step 67290, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.881 sec/batch; 64h:52m:23s remains)
INFO - root - 2017-12-09 23:17:37.380596: step 67300, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.857 sec/batch; 63h:08m:30s remains)
2017-12-09 23:17:38.261872: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.45692244 0.45703232 0.455571 0.45568234 0.45307308 0.44487911 0.4261772 0.39519083 0.3510426 0.29846537 0.23862384 0.17757824 0.11867301 0.069039591 0.032239337][0.49406984 0.49296543 0.48746184 0.48231205 0.4732329 0.45911971 0.4352673 0.40103683 0.35461473 0.29953137 0.23694064 0.17339987 0.11326666 0.064341031 0.029419526][0.51204807 0.51034296 0.50387025 0.49678805 0.48453397 0.46667367 0.43923271 0.40219766 0.35281259 0.29497665 0.23001274 0.16495013 0.10468251 0.057459261 0.025295794][0.52101696 0.52269846 0.51719034 0.50900632 0.4950968 0.47571129 0.44660503 0.40746287 0.35549721 0.29500985 0.2269434 0.15930527 0.097977288 0.051765453 0.021659259][0.52251846 0.52873325 0.52644372 0.51996785 0.50657541 0.48635459 0.45590332 0.41460112 0.35974434 0.29622141 0.22523697 0.15546513 0.092931829 0.047064364 0.018443011][0.51807469 0.52902561 0.5293268 0.52488816 0.51284379 0.49291629 0.46209562 0.419154 0.36226434 0.29610354 0.22261517 0.15120782 0.088012651 0.042802509 0.015490984][0.50782615 0.52260226 0.52486885 0.52171206 0.50997394 0.49037215 0.4597556 0.41647038 0.35922837 0.29209921 0.21812187 0.14638227 0.08330135 0.038916491 0.012830094][0.49027312 0.50806153 0.51021713 0.5063991 0.49346554 0.47339123 0.44302288 0.40123877 0.34618285 0.28106204 0.20949095 0.13988848 0.078765079 0.035602015 0.010671006][0.46874353 0.4887692 0.489574 0.4829284 0.46683913 0.44469351 0.41388687 0.37423074 0.32252944 0.26171371 0.19520354 0.13036919 0.073257141 0.032524303 0.0091759795][0.44487089 0.46392363 0.4626129 0.45312458 0.43348819 0.4088456 0.37714875 0.33909437 0.29097736 0.23547475 0.175356 0.11717519 0.065889291 0.028923465 0.0077899029][0.41810715 0.43633595 0.43311146 0.42044771 0.39771277 0.37109995 0.338901 0.30240455 0.25745562 0.20696923 0.15334044 0.10212693 0.057172861 0.024825403 0.0064552543][0.38788229 0.40574276 0.40140352 0.38639143 0.36140606 0.33298945 0.30057612 0.26624575 0.22512127 0.17936923 0.13166691 0.0871591 0.048524015 0.020816842 0.0051163412][0.35175097 0.36897066 0.36544263 0.34946334 0.3233642 0.29403427 0.26206261 0.2295294 0.19162108 0.15083236 0.10926867 0.071449764 0.039353792 0.016653623 0.0038174442][0.3085748 0.32570267 0.32388729 0.30924669 0.28483835 0.25636306 0.22562321 0.19453217 0.15929674 0.12253777 0.086348727 0.0550731 0.029593982 0.012164689 0.0024278113][0.25766912 0.27285525 0.27194521 0.25925758 0.23779188 0.21286568 0.18623894 0.15813997 0.12664723 0.094734795 0.064315192 0.03934582 0.020043505 0.00760299 0.00095694012]]...]
INFO - root - 2017-12-09 23:17:46.661290: step 67310, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.851 sec/batch; 62h:42m:43s remains)
INFO - root - 2017-12-09 23:17:55.134251: step 67320, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.876 sec/batch; 64h:33m:40s remains)
INFO - root - 2017-12-09 23:18:03.914091: step 67330, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.894 sec/batch; 65h:52m:06s remains)
INFO - root - 2017-12-09 23:18:12.592882: step 67340, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 64h:00m:55s remains)
INFO - root - 2017-12-09 23:18:21.209033: step 67350, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.853 sec/batch; 62h:48m:39s remains)
INFO - root - 2017-12-09 23:18:29.960038: step 67360, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.873 sec/batch; 64h:19m:45s remains)
INFO - root - 2017-12-09 23:18:38.605679: step 67370, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.883 sec/batch; 64h:59m:40s remains)
INFO - root - 2017-12-09 23:18:47.341994: step 67380, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.896 sec/batch; 66h:00m:27s remains)
INFO - root - 2017-12-09 23:18:55.831135: step 67390, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.866 sec/batch; 63h:46m:06s remains)
INFO - root - 2017-12-09 23:19:04.404570: step 67400, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.881 sec/batch; 64h:53m:24s remains)
2017-12-09 23:19:05.233895: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.00058304565 -0.00057484233 -0.00057640695 -0.00057368923 -0.00058518071 -0.00060524337 -0.00062258646 -0.00061524776 -0.00061005645 -0.00057912921 -0.00057625887 -0.00059820584 -0.0006178621 -0.00063613686 -0.00065321114][-0.00045836146 -0.00043991324 -0.00043700857 -0.00044378336 -0.00046387524 -0.00049194635 -0.00051795947 -0.00052085274 -0.00051534409 -0.00050443341 -0.00049668958 -0.00051405933 -0.00055043271 -0.00058570073 -0.00061738025][-0.00053168938 -0.00049999612 -0.00048729102 -0.00048423687 -0.00049644581 -0.0005192284 -0.00054320239 -0.00055256172 -0.00054429343 -0.00053359929 -0.00052840891 -0.00055340293 -0.00059286691 -0.00064579817 -0.0006938033][-0.00072455662 -0.000678284 -0.00065565249 -0.00064042304 -0.00063484616 -0.00063356257 -0.00063738588 -0.00063455396 -0.00061016181 -0.00060241716 -0.00060148165 -0.00063531031 -0.00069939776 -0.00077824667 -0.00085234188][-0.0010235328 -0.00095916342 -0.00091478683 -0.00087506138 -0.000836828 -0.0007936625 -0.0007568436 -0.00072977832 -0.00070034293 -0.00068538717 -0.00068484235 -0.0007351347 -0.00082967104 -0.00095237885 -0.0010664953][-0.0013732068 -0.0012982122 -0.0012288378 -0.0011462281 -0.0010518529 -0.00095060252 -0.000865342 -0.0008148239 -0.00078192458 -0.00076425914 -0.00076908397 -0.00084074493 -0.00097126176 -0.0011418309 -0.0012980221][-0.0016491223 -0.0015854371 -0.0015034993 -0.0013793953 -0.0012199713 -0.0010559284 -0.00093112065 -0.000867364 -0.00083665224 -0.00082506251 -0.00083474 -0.0009345478 -0.0011018927 -0.001310227 -0.0014894337][-0.001786811 -0.001750289 -0.0016749944 -0.001539072 -0.0013441624 -0.0011399912 -0.00099657988 -0.00093207526 -0.00090631208 -0.00089714705 -0.0009181372 -0.0010350149 -0.0012246622 -0.0014436713 -0.0016155731][-0.0018113881 -0.0017958074 -0.0017463152 -0.0016341332 -0.0014577901 -0.001269616 -0.0011339881 -0.0010656676 -0.0010391339 -0.0010328377 -0.0010671841 -0.0011911136 -0.0013757057 -0.0015650459 -0.0016973544][-0.0018130981 -0.001804229 -0.0017789587 -0.0017118787 -0.0016006327 -0.0014734579 -0.001371255 -0.0013119912 -0.0012862505 -0.0012860172 -0.0013269724 -0.0014277289 -0.001562754 -0.0016840387 -0.0017569526][-0.0018158393 -0.0018094038 -0.001798136 -0.0017701272 -0.0017221044 -0.0016600703 -0.0016039984 -0.001567692 -0.0015530998 -0.0015589783 -0.0015908527 -0.0016486469 -0.0017148274 -0.0017658576 -0.001791434][-0.0018157294 -0.0018130755 -0.0018106083 -0.0018048862 -0.0017927462 -0.0017730265 -0.0017517245 -0.0017359733 -0.0017308915 -0.0017360839 -0.0017509776 -0.001771703 -0.0017910013 -0.0018029544 -0.001806812][-0.0018151308 -0.0018133742 -0.001813001 -0.0018123146 -0.0018117805 -0.0018102668 -0.0018069893 -0.0018037421 -0.0018027663 -0.0018043066 -0.0018073806 -0.0018096507 -0.0018106913 -0.0018108418 -0.0018107188][-0.0018144835 -0.0018126294 -0.0018124207 -0.0018122026 -0.0018126235 -0.0018129626 -0.0018127769 -0.0018125706 -0.0018120895 -0.0018118551 -0.0018118578 -0.0018118554 -0.0018119178 -0.0018115727 -0.0018111086][-0.0018144357 -0.0018132122 -0.0018128561 -0.0018126613 -0.001813004 -0.0018132082 -0.0018131541 -0.0018132632 -0.0018129204 -0.0018126923 -0.0018126982 -0.0018125578 -0.0018124817 -0.0018121823 -0.0018118431]]...]
INFO - root - 2017-12-09 23:19:13.596878: step 67410, loss = 0.81, batch loss = 0.68 (9.3 examples/sec; 0.858 sec/batch; 63h:09m:23s remains)
INFO - root - 2017-12-09 23:19:22.116496: step 67420, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 64h:09m:07s remains)
INFO - root - 2017-12-09 23:19:30.705275: step 67430, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 63h:30m:01s remains)
INFO - root - 2017-12-09 23:19:39.378220: step 67440, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 63h:30m:03s remains)
INFO - root - 2017-12-09 23:19:47.957453: step 67450, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.878 sec/batch; 64h:40m:11s remains)
INFO - root - 2017-12-09 23:19:56.660250: step 67460, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.888 sec/batch; 65h:20m:29s remains)
INFO - root - 2017-12-09 23:20:05.533262: step 67470, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.866 sec/batch; 63h:43m:32s remains)
INFO - root - 2017-12-09 23:20:14.143147: step 67480, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.860 sec/batch; 63h:19m:15s remains)
INFO - root - 2017-12-09 23:20:22.619125: step 67490, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.888 sec/batch; 65h:20m:18s remains)
INFO - root - 2017-12-09 23:20:31.254578: step 67500, loss = 0.81, batch loss = 0.68 (9.4 examples/sec; 0.853 sec/batch; 62h:49m:31s remains)
2017-12-09 23:20:32.121652: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.25970951 0.25878668 0.25739631 0.25557768 0.25245333 0.24685891 0.23851174 0.22852585 0.21922538 0.21071108 0.20172097 0.19444133 0.18876493 0.18491721 0.18158711][0.2604416 0.26215029 0.26320356 0.26353264 0.26138991 0.25666097 0.24837402 0.23805639 0.2275704 0.21756786 0.20706961 0.19813842 0.19121832 0.18625955 0.18229219][0.25969946 0.26416618 0.26762795 0.27005294 0.26923531 0.26479554 0.256447 0.24584481 0.2346345 0.22298348 0.21129051 0.20072918 0.19238475 0.18603642 0.18116409][0.25917557 0.26687375 0.27317172 0.27804032 0.2792314 0.27584612 0.26802805 0.25771061 0.24590619 0.23257746 0.21923321 0.20705892 0.19659744 0.18854876 0.18240002][0.25552577 0.26676816 0.27535054 0.28226948 0.28522089 0.28368607 0.2772918 0.26760659 0.25638619 0.24247536 0.22796947 0.21416253 0.20166968 0.19185334 0.18431486][0.25328052 0.2676079 0.27873167 0.28632668 0.28998724 0.28964126 0.28472355 0.2760101 0.265339 0.25224125 0.23744729 0.22214544 0.20775561 0.19589002 0.1867702][0.25106052 0.26622695 0.27788627 0.28632095 0.29111397 0.29198092 0.28836146 0.28115594 0.27169269 0.25919357 0.24435036 0.22876948 0.21345451 0.20033151 0.18994483][0.24741516 0.26277304 0.27422592 0.28252566 0.28747562 0.28902063 0.286399 0.28045589 0.2724703 0.26110029 0.24746737 0.23280767 0.21787164 0.20432006 0.19312917][0.24082567 0.25574306 0.26651534 0.27489293 0.28045422 0.283065 0.28199342 0.27777824 0.27101055 0.26110616 0.24887094 0.23591614 0.22220683 0.20892075 0.19767669][0.23648915 0.24974653 0.25821972 0.26537 0.27031955 0.27327371 0.27336106 0.2703357 0.26509497 0.25733754 0.24762844 0.23703368 0.22575738 0.21378571 0.2031249][0.23552029 0.2464879 0.25251979 0.25785261 0.2614772 0.2635676 0.26412159 0.26247486 0.25854334 0.25277478 0.24539025 0.23736037 0.22855727 0.21857525 0.20908697][0.2333831 0.24256811 0.24584761 0.24933028 0.25140712 0.25266576 0.25356928 0.25310388 0.25126788 0.24760604 0.24292721 0.23789401 0.23204027 0.22448578 0.21648553][0.23375121 0.24151476 0.24242899 0.24299999 0.24208051 0.24084075 0.24038203 0.24024175 0.23948981 0.2385591 0.23747246 0.23591906 0.23378009 0.22932574 0.22378193][0.23588057 0.24250719 0.24082701 0.23871239 0.23494273 0.23114489 0.22918749 0.22854893 0.22865674 0.22991347 0.23184541 0.23361318 0.23468274 0.23339349 0.23047662][0.2397348 0.2452711 0.24147439 0.23623309 0.22910909 0.22240856 0.21852665 0.2173983 0.21850999 0.22183672 0.22656575 0.23118113 0.23499654 0.23651733 0.23562343]]...]
INFO - root - 2017-12-09 23:20:40.650871: step 67510, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.884 sec/batch; 65h:05m:43s remains)
INFO - root - 2017-12-09 23:20:49.098530: step 67520, loss = 0.83, batch loss = 0.70 (9.7 examples/sec; 0.826 sec/batch; 60h:47m:15s remains)
INFO - root - 2017-12-09 23:20:57.771766: step 67530, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.886 sec/batch; 65h:13m:18s remains)
INFO - root - 2017-12-09 23:21:06.306816: step 67540, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 63h:50m:36s remains)
INFO - root - 2017-12-09 23:21:14.988010: step 67550, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.843 sec/batch; 62h:04m:01s remains)
INFO - root - 2017-12-09 23:21:23.630887: step 67560, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.854 sec/batch; 62h:52m:37s remains)
INFO - root - 2017-12-09 23:21:32.142234: step 67570, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.839 sec/batch; 61h:46m:19s remains)
INFO - root - 2017-12-09 23:21:40.805228: step 67580, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.844 sec/batch; 62h:05m:18s remains)
INFO - root - 2017-12-09 23:21:49.110969: step 67590, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 63h:34m:31s remains)
INFO - root - 2017-12-09 23:21:57.989229: step 67600, loss = 0.82, batch loss = 0.69 (8.6 examples/sec; 0.933 sec/batch; 68h:37m:03s remains)
2017-12-09 23:21:59.002710: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.0039315782 0.0072355526 0.011904747 0.017478094 0.023046685 0.027723925 0.030930132 0.032798182 0.033522867 0.033736132 0.033817187 0.034162115 0.034703556 0.035332575 0.03585355][0.003325745 0.0065179053 0.011155384 0.016865404 0.022730205 0.027738709 0.031197611 0.033014715 0.033499468 0.03329723 0.032907259 0.032784384 0.03301027 0.033625886 0.034411944][0.0022634179 0.004973039 0.0090397783 0.014262932 0.01986469 0.024887651 0.028517086 0.030499883 0.031001229 0.030603204 0.029851362 0.029258905 0.029037016 0.029295515 0.029981203][0.0011044581 0.0032970617 0.0067204502 0.011292927 0.0164354 0.021252604 0.024883987 0.026938638 0.027491061 0.026986936 0.025959661 0.024954919 0.024255605 0.02407152 0.024451192][0.00022065488 0.0017919393 0.0043646581 0.0080241058 0.012360743 0.016645906 0.020092009 0.022160843 0.022748213 0.022144575 0.020899732 0.01956962 0.018485075 0.017913878 0.017995775][-0.00033316587 0.00063614 0.0023203664 0.0048868204 0.0081240153 0.011611723 0.014651715 0.016643647 0.017310897 0.016733678 0.015332059 0.013674646 0.012277775 0.011447234 0.011361465][-0.00079369615 -0.000281387 0.00064082595 0.0021948223 0.0043834569 0.0070034969 0.0095261345 0.011399864 0.012114241 0.011552768 0.0100546 0.0082182633 0.0066537079 0.0057038418 0.0056176241][-0.0012503952 -0.00096985552 -0.00046292751 0.00044932298 0.0018744044 0.0038180617 0.0059119454 0.007590638 0.0082374122 0.0076131788 0.0060162679 0.004075232 0.0025245375 0.0016991688 0.001730329][-0.0016076788 -0.0014795256 -0.001210806 -0.00066058582 0.00031584722 0.0017806507 0.0034689917 0.00488272 0.0054219854 0.0048007304 0.0032886127 0.0015274031 0.00019386515 -0.00044590468 -0.00032023538][-0.001781304 -0.0017259753 -0.0015925406 -0.0012707871 -0.000647314 0.00034191355 0.001511156 0.0024938816 0.00285842 0.0023801094 0.0012761859 9.4263814e-06 -0.00092495058 -0.0013419325 -0.0012443042][-0.0018260153 -0.0018164526 -0.0017801228 -0.0016424537 -0.001330514 -0.00081534043 -0.00020007847 0.00031364232 0.00049601623 0.00022617483 -0.00036120124 -0.0010072959 -0.0014659853 -0.0016662312 -0.0016218107][-0.0018264904 -0.0018249543 -0.0018192588 -0.001784927 -0.0016882904 -0.0015167904 -0.00130621 -0.0011249784 -0.0010565931 -0.0011452492 -0.001348821 -0.0015710382 -0.0017253022 -0.0017941826 -0.0017912395][-0.0018268465 -0.0018257467 -0.0018247896 -0.0018201559 -0.001807084 -0.0017844152 -0.0017555982 -0.0017324894 -0.0017245614 -0.0017383007 -0.0017675761 -0.0017986177 -0.0018193283 -0.001826923 -0.0018278155][-0.0018267313 -0.0018257467 -0.0018249208 -0.0018227221 -0.0018205012 -0.0018176624 -0.0018146654 -0.0018131163 -0.0018130988 -0.0018156427 -0.0018181447 -0.0018212051 -0.0018243761 -0.001825399 -0.0018254087][-0.0018266186 -0.0018257214 -0.0018247718 -0.0018222086 -0.0018202818 -0.0018176701 -0.0018151966 -0.0018142771 -0.0018142022 -0.0018160659 -0.0018177574 -0.0018203644 -0.0018224984 -0.0018233287 -0.0018231866]]...]
INFO - root - 2017-12-09 23:22:07.574187: step 67610, loss = 0.82, batch loss = 0.69 (9.9 examples/sec; 0.806 sec/batch; 59h:18m:25s remains)
INFO - root - 2017-12-09 23:22:16.177972: step 67620, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.841 sec/batch; 61h:52m:22s remains)
INFO - root - 2017-12-09 23:22:24.861495: step 67630, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.854 sec/batch; 62h:49m:38s remains)
INFO - root - 2017-12-09 23:22:33.548080: step 67640, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 63h:27m:13s remains)
INFO - root - 2017-12-09 23:22:42.230453: step 67650, loss = 0.82, batch loss = 0.69 (9.7 examples/sec; 0.828 sec/batch; 60h:53m:08s remains)
INFO - root - 2017-12-09 23:22:50.887711: step 67660, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.849 sec/batch; 62h:25m:43s remains)
INFO - root - 2017-12-09 23:22:59.564577: step 67670, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.902 sec/batch; 66h:23m:12s remains)
INFO - root - 2017-12-09 23:23:08.440449: step 67680, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.903 sec/batch; 66h:23m:42s remains)
INFO - root - 2017-12-09 23:23:16.795181: step 67690, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.830 sec/batch; 61h:01m:35s remains)
INFO - root - 2017-12-09 23:23:25.331925: step 67700, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.856 sec/batch; 62h:59m:56s remains)
2017-12-09 23:23:26.223382: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.10394997 0.096196584 0.083505929 0.069339022 0.05513975 0.043178152 0.033678971 0.027709516 0.02388777 0.021006316 0.01808612 0.014695493 0.011089948 0.0073131756 0.0039403769][0.13809368 0.13043652 0.11612228 0.0997784 0.083248176 0.069218315 0.05755 0.049388293 0.043278378 0.038046639 0.032678548 0.026814349 0.020705065 0.014538371 0.0089794351][0.1677338 0.16157971 0.1474905 0.13077486 0.1136521 0.099048086 0.08665657 0.077379383 0.06947691 0.061959364 0.053785548 0.04483559 0.035429165 0.025984678 0.017291505][0.19094494 0.18673 0.17371483 0.15787782 0.14205307 0.1286718 0.11722034 0.10833289 0.099850684 0.090660512 0.079556979 0.067005746 0.053513598 0.04000935 0.027507674][0.20248406 0.20108502 0.19075747 0.17748778 0.16475452 0.1543995 0.14592767 0.1390695 0.13124029 0.12116718 0.10735206 0.09097597 0.072871648 0.054735873 0.0379405][0.20069689 0.20171407 0.19447622 0.18500374 0.17693312 0.17108519 0.16699113 0.16344805 0.15740468 0.14716569 0.13118044 0.11127359 0.088806368 0.066490389 0.045953043][0.18659875 0.18970959 0.18522933 0.17948769 0.17590117 0.17473629 0.17511769 0.17503437 0.17132373 0.16183691 0.14515035 0.12321917 0.097847432 0.072627135 0.049610995][0.16429651 0.16867399 0.16634722 0.16362368 0.16327724 0.16519864 0.1682186 0.17030171 0.16838087 0.16035096 0.14486808 0.12345686 0.0979732 0.072201118 0.048624091][0.13857582 0.14345954 0.14236592 0.14115153 0.14212178 0.14493369 0.14820234 0.15032314 0.14885354 0.14231069 0.12937129 0.1108627 0.088204764 0.06477651 0.043139279][0.11071822 0.11583408 0.11588892 0.11546373 0.11653475 0.11864252 0.1206996 0.12161718 0.11970685 0.11424418 0.10399777 0.089353427 0.071108542 0.05193191 0.034080546][0.0811664 0.085903093 0.086657323 0.086648032 0.087383665 0.0885344 0.089324556 0.089062929 0.086900733 0.082638644 0.075224295 0.064698577 0.051442817 0.037399784 0.024168698][0.052985821 0.056758769 0.057574756 0.057488658 0.057651713 0.057950731 0.057959482 0.057277478 0.055494133 0.052632306 0.047922712 0.041276425 0.03276993 0.023649078 0.014944864][0.029879835 0.032371372 0.03305608 0.032983754 0.032912564 0.032797556 0.032493837 0.031810723 0.030583613 0.028909437 0.026269006 0.022611827 0.017869473 0.012714 0.0076934914][0.014031683 0.015446185 0.01593408 0.015917445 0.0158103 0.015601403 0.015293679 0.014794267 0.014103946 0.013276439 0.012025698 0.010289275 0.0079649044 0.0054159774 0.0028804163][0.0047130021 0.0053758295 0.0056633195 0.0056871423 0.0056122527 0.0054491935 0.0052595767 0.00501214 0.004739366 0.0044422727 0.0039852178 0.0033223871 0.002371632 0.001279795 0.00015736755]]...]
INFO - root - 2017-12-09 23:23:34.480014: step 67710, loss = 0.82, batch loss = 0.69 (13.0 examples/sec; 0.616 sec/batch; 45h:19m:44s remains)
INFO - root - 2017-12-09 23:23:42.917700: step 67720, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.856 sec/batch; 62h:57m:36s remains)
INFO - root - 2017-12-09 23:23:51.657570: step 67730, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.887 sec/batch; 65h:14m:55s remains)
INFO - root - 2017-12-09 23:24:00.597561: step 67740, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.879 sec/batch; 64h:37m:33s remains)
INFO - root - 2017-12-09 23:24:09.363846: step 67750, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.866 sec/batch; 63h:39m:28s remains)
INFO - root - 2017-12-09 23:24:18.215689: step 67760, loss = 0.82, batch loss = 0.70 (9.1 examples/sec; 0.875 sec/batch; 64h:20m:27s remains)
INFO - root - 2017-12-09 23:24:27.028088: step 67770, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.863 sec/batch; 63h:29m:00s remains)
INFO - root - 2017-12-09 23:24:35.673626: step 67780, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.853 sec/batch; 62h:45m:26s remains)
INFO - root - 2017-12-09 23:24:44.245511: step 67790, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.839 sec/batch; 61h:41m:33s remains)
INFO - root - 2017-12-09 23:24:53.066621: step 67800, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.872 sec/batch; 64h:05m:20s remains)
2017-12-09 23:24:53.955217: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0018075467 -0.0018071914 -0.0018074582 -0.0018079424 -0.0018081963 -0.001808102 -0.0018078116 -0.0018076242 -0.0018074245 -0.0018071171 -0.0018070504 -0.0018073716 -0.0018078689 -0.0018082564 -0.0018083972][-0.0018063032 -0.0018055631 -0.0018048948 -0.0018040797 -0.0018026849 -0.001801078 -0.0017999385 -0.0017996357 -0.0018002605 -0.001801359 -0.0018030775 -0.001805327 -0.0018072338 -0.0018085316 -0.0018088938][-0.0018111529 -0.001810259 -0.0018086149 -0.0018061654 -0.0018026342 -0.0017988528 -0.0017962805 -0.0017955394 -0.0017967048 -0.00179912 -0.0018032452 -0.0018078659 -0.0018113061 -0.0018131993 -0.0018137284][-0.0018182724 -0.0018172866 -0.0018153408 -0.0018116884 -0.0018066146 -0.0018008637 -0.0017964414 -0.0017947573 -0.0017960626 -0.0017997954 -0.0018062242 -0.0018125107 -0.0018170404 -0.001819565 -0.0018205044][-0.0018259626 -0.0018253271 -0.0018229366 -0.0018182696 -0.0018121713 -0.0018047806 -0.0017987895 -0.0017963691 -0.001797747 -0.0018023393 -0.0018105106 -0.0018177367 -0.0018224305 -0.001824727 -0.0018254522][-0.0018320398 -0.0018317703 -0.0018293265 -0.0018240619 -0.0018172751 -0.0018087983 -0.0018018245 -0.001798744 -0.0017999373 -0.0018050127 -0.0018138518 -0.001821707 -0.0018264432 -0.0018284607 -0.0018283991][-0.0018358298 -0.0018357798 -0.0018328574 -0.0018271965 -0.0018199249 -0.0018110612 -0.0018035966 -0.0018001416 -0.0018014362 -0.0018069169 -0.0018162732 -0.0018245809 -0.0018294458 -0.0018311172 -0.0018304233][-0.0018371869 -0.0018366971 -0.0018329689 -0.0018262691 -0.0018181432 -0.0018092502 -0.0018018562 -0.0017982483 -0.0017998553 -0.0018063957 -0.0018166677 -0.0018257138 -0.0018312108 -0.0018327343 -0.0018318746][-0.0018364512 -0.0018350943 -0.0018307972 -0.0018231784 -0.0018142952 -0.0018050432 -0.0017976246 -0.0017939907 -0.0017959146 -0.0018035532 -0.0018146358 -0.0018247144 -0.0018308983 -0.0018331205 -0.0018328395][-0.0018361175 -0.0018335469 -0.0018282331 -0.0018197865 -0.0018103175 -0.0018009967 -0.0017939106 -0.0017907206 -0.0017930509 -0.0018009848 -0.0018125564 -0.0018231246 -0.0018301018 -0.0018329928 -0.0018331644][-0.0018365326 -0.0018326914 -0.0018266877 -0.0018178197 -0.0018080134 -0.0017992269 -0.0017932276 -0.0017911118 -0.0017937161 -0.001801015 -0.0018117257 -0.0018220245 -0.0018294159 -0.0018330886 -0.0018337553][-0.0018374143 -0.0018328779 -0.0018266037 -0.0018181683 -0.001809263 -0.0018018011 -0.0017971592 -0.0017960044 -0.0017987849 -0.0018049583 -0.0018136193 -0.0018225091 -0.0018291922 -0.0018329155 -0.0018341955][-0.0018382089 -0.001833819 -0.001828162 -0.0018210041 -0.0018136484 -0.0018079783 -0.0018047268 -0.001804048 -0.0018063484 -0.0018110607 -0.001817484 -0.0018241396 -0.0018291551 -0.0018320571 -0.0018330714][-0.001838583 -0.0018348814 -0.001830471 -0.0018252173 -0.0018200515 -0.0018161205 -0.0018137016 -0.0018132478 -0.0018147368 -0.0018177503 -0.0018217982 -0.0018264871 -0.001830101 -0.0018320824 -0.0018327478][-0.0018382076 -0.0018357164 -0.0018330215 -0.0018298178 -0.001826674 -0.0018241701 -0.0018225209 -0.0018220969 -0.0018228802 -0.0018244167 -0.0018264316 -0.0018289107 -0.0018310051 -0.0018321417 -0.0018324408]]...]
INFO - root - 2017-12-09 23:25:02.415012: step 67810, loss = 0.82, batch loss = 0.69 (11.0 examples/sec; 0.727 sec/batch; 53h:28m:08s remains)
INFO - root - 2017-12-09 23:25:10.784740: step 67820, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.863 sec/batch; 63h:25m:17s remains)
INFO - root - 2017-12-09 23:25:19.601662: step 67830, loss = 0.81, batch loss = 0.68 (9.5 examples/sec; 0.841 sec/batch; 61h:48m:03s remains)
INFO - root - 2017-12-09 23:25:28.284256: step 67840, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 64h:00m:27s remains)
INFO - root - 2017-12-09 23:25:36.881369: step 67850, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.860 sec/batch; 63h:12m:55s remains)
INFO - root - 2017-12-09 23:25:45.601260: step 67860, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.883 sec/batch; 64h:54m:19s remains)
INFO - root - 2017-12-09 23:25:54.320122: step 67870, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.872 sec/batch; 64h:06m:11s remains)
INFO - root - 2017-12-09 23:26:03.050163: step 67880, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.882 sec/batch; 64h:48m:58s remains)
INFO - root - 2017-12-09 23:26:11.484110: step 67890, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.838 sec/batch; 61h:35m:53s remains)
INFO - root - 2017-12-09 23:26:20.179415: step 67900, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.849 sec/batch; 62h:23m:35s remains)
2017-12-09 23:26:21.062557: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.0025178916 0.0036623296 0.0040812842 0.0036423057 0.0025597806 0.0012301005 1.956441e-05 -0.0008717872 -0.0014181559 -0.001694554 -0.0017889886 -0.0018062128 -0.0017934361 -0.0017268725 -0.0015779397][0.0053954618 0.0074525387 0.008349481 0.0077803466 0.0060245567 0.003717517 0.0015379522 -9.1060996e-05 -0.0010718243 -0.001553292 -0.0017309291 -0.0017715528 -0.0017452713 -0.001625999 -0.0013704535][0.0087385373 0.011869803 0.01348827 0.013133867 0.010993895 0.00778136 0.0044147819 0.0016068438 -0.00026438187 -0.0012485939 -0.0016276352 -0.0017172125 -0.0016672037 -0.0014509858 -0.0010048498][0.012887489 0.017346485 0.0198081 0.019618662 0.016956324 0.012709273 0.0080111641 0.0038701454 0.00093058229 -0.00074272847 -0.001454202 -0.0016382673 -0.0015591099 -0.0012052667 -0.00049331994][0.016559822 0.02254807 0.026342886 0.026985025 0.024376104 0.019267488 0.012982737 0.0070176362 0.0025458597 -8.4987842e-05 -0.0012314303 -0.0015435717 -0.0014278729 -0.0009121506 0.00010114454][0.018657187 0.025785036 0.030755632 0.032413639 0.030413982 0.025227616 0.018035846 0.010596005 0.0045720087 0.00078846107 -0.00096375443 -0.0014740885 -0.0013309743 -0.00066989975 0.00057541777][0.018747279 0.026371816 0.032065645 0.034547381 0.033252895 0.028405074 0.021037795 0.012942794 0.0060371323 0.0014727073 -0.00077152892 -0.0014775712 -0.0013632143 -0.000682184 0.0006115326][0.016664967 0.02385821 0.029542256 0.032448675 0.031841174 0.027737798 0.020949438 0.013172593 0.0063359705 0.0016677998 -0.00071116665 -0.001524103 -0.0014847124 -0.00093206909 0.00014293904][0.012916786 0.018977277 0.024063151 0.027024707 0.027045485 0.023931704 0.018258708 0.01152114 0.0054833665 0.0013131154 -0.0008180053 -0.0015553468 -0.001585213 -0.0012545206 -0.00059877348][0.0085046794 0.013069013 0.017205462 0.019942228 0.020445393 0.018368244 0.014076636 0.0087763332 0.0039589056 0.00062609825 -0.0010658575 -0.0016393359 -0.001683141 -0.0015322488 -0.0012382227][0.0044797463 0.0075720162 0.010696578 0.013093431 0.013942803 0.012787901 0.00983058 0.005991017 0.002433781 -4.1221967e-05 -0.0013038928 -0.00173276 -0.0017871531 -0.0017481539 -0.0016742271][0.0014838445 0.0034302855 0.0056776721 0.0076491986 0.0086084353 0.008061436 0.0061219265 0.0034897439 0.0010419566 -0.00063721952 -0.0014845845 -0.0017681702 -0.0018132294 -0.0018094076 -0.001796827][-0.00035106589 0.00079842366 0.0023224377 0.0038041989 0.004639646 0.0044062105 0.0031757322 0.0014658141 -9.8092249e-05 -0.0011304362 -0.0016278573 -0.0017865977 -0.001814594 -0.0018168902 -0.0018162347][-0.0012753453 -0.00065028016 0.0002755163 0.0012438755 0.0018417271 0.0017600282 0.0010383426 1.975894e-05 -0.00089201872 -0.0014634347 -0.0017216908 -0.0017998343 -0.0018143426 -0.0018164959 -0.0018173767][-0.0016552277 -0.0013695981 -0.00091789494 -0.00042818766 -0.00011993409 -0.0001581473 -0.00051876996 -0.0010168079 -0.0014404152 -0.0016833114 -0.0017800367 -0.0018086506 -0.0018153194 -0.0018163568 -0.001816789]]...]
INFO - root - 2017-12-09 23:26:29.472536: step 67910, loss = 0.82, batch loss = 0.69 (11.9 examples/sec; 0.671 sec/batch; 49h:19m:52s remains)
INFO - root - 2017-12-09 23:26:37.813331: step 67920, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.874 sec/batch; 64h:13m:45s remains)
INFO - root - 2017-12-09 23:26:46.470831: step 67930, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 63h:47m:03s remains)
INFO - root - 2017-12-09 23:26:54.959304: step 67940, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.856 sec/batch; 62h:53m:01s remains)
INFO - root - 2017-12-09 23:27:03.505719: step 67950, loss = 0.83, batch loss = 0.70 (9.4 examples/sec; 0.852 sec/batch; 62h:34m:25s remains)
INFO - root - 2017-12-09 23:27:12.189603: step 67960, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.880 sec/batch; 64h:40m:55s remains)
INFO - root - 2017-12-09 23:27:20.853595: step 67970, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.835 sec/batch; 61h:21m:13s remains)
INFO - root - 2017-12-09 23:27:29.530115: step 67980, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.881 sec/batch; 64h:44m:30s remains)
INFO - root - 2017-12-09 23:27:38.150479: step 67990, loss = 0.82, batch loss = 0.69 (10.5 examples/sec; 0.758 sec/batch; 55h:43m:05s remains)
INFO - root - 2017-12-09 23:27:47.059795: step 68000, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 63h:51m:58s remains)
2017-12-09 23:27:47.917078: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.0013264226 0.0015004064 0.0020580925 0.0031272057 0.0046633687 0.0064604231 0.0081451293 0.0093367314 0.00984268 0.0096295495 0.0088078091 0.0075456086 0.00614619 0.0049141338 0.0040325439][0.00013679301 0.00033197727 0.00095820043 0.0022453163 0.0042655123 0.0067624613 0.0091616344 0.010903974 0.011680653 0.011443763 0.010378251 0.0087181479 0.0069002877 0.0053204838 0.0042071706][-0.00094714737 -0.00076514622 -0.00011185312 0.001380412 0.0039403317 0.007231385 0.010446423 0.012788741 0.013847264 0.013557806 0.012196894 0.010067557 0.0077511794 0.0057399254 0.0043292902][-0.0015626857 -0.0014128359 -0.00073437789 0.00099474855 0.0041463305 0.0082553895 0.012259531 0.015113351 0.016359109 0.015931992 0.01420823 0.01156442 0.0087198187 0.006253053 0.0045236717][-0.0017484344 -0.0016063908 -0.00088455557 0.0010853185 0.0048228027 0.0097124223 0.014427426 0.017702593 0.019022159 0.018361732 0.016205261 0.013021763 0.0096630026 0.0067707975 0.0047468785][-0.0016732152 -0.0015255488 -0.00077174779 0.0013629046 0.0055092359 0.010990003 0.016271612 0.019901643 0.021272188 0.020403029 0.017838113 0.014188779 0.010399885 0.0071672886 0.0049262471][-0.0015348385 -0.001368738 -0.00063679961 0.0014982898 0.0057217642 0.011436695 0.017036088 0.020976515 0.022493783 0.021582684 0.018784616 0.014848557 0.010792827 0.0073760939 0.00501832][-0.0013944472 -0.0012104757 -0.00056284503 0.0013507298 0.0052261795 0.010695423 0.01625886 0.020392839 0.022167046 0.021467116 0.018731298 0.014803528 0.010750093 0.0073576174 0.0050247274][-0.0013010108 -0.0011145775 -0.00062000635 0.00087724987 0.0040240521 0.008753309 0.013840341 0.01793105 0.019983502 0.019723104 0.01741893 0.013903509 0.010196534 0.007081727 0.0049299127][-0.0012871642 -0.0011006573 -0.0007590499 0.00026346126 0.0025000158 0.0061141965 0.01028008 0.013958479 0.016160747 0.016450057 0.014909894 0.012194859 0.0091794739 0.0065842955 0.0047589871][-0.0014624131 -0.0012883282 -0.001046581 -0.00041319919 0.00097292161 0.0033502821 0.00630751 0.0092078438 0.011290907 0.012080096 0.011480422 0.0098529374 0.0078063491 0.0059257839 0.0045395982][-0.0016630506 -0.0015459954 -0.0013929 -0.0010470182 -0.000306522 0.0010198791 0.0028136247 0.0047728741 0.0064670932 0.00752294 0.0077705248 0.0072705527 0.0062880036 0.0052003181 0.0042866045][-0.0017862042 -0.0017359923 -0.0016538436 -0.0014917153 -0.0011656559 -0.00056958105 0.00032592507 0.0014519867 0.0026430641 0.0036880923 0.0044709137 0.0048659598 0.0048264973 0.0044750832 0.004019096][-0.0018125845 -0.0018015282 -0.0017769546 -0.001718578 -0.0016067876 -0.0014098585 -0.0010686682 -0.00055534882 0.00014133926 0.0010076339 0.0020396537 0.0030228985 0.003675885 0.0038997857 0.0038036378][-0.0018129051 -0.0018070349 -0.0017907538 -0.001761958 -0.001725527 -0.0016758032 -0.0015692667 -0.0013742242 -0.0010237653 -0.00039241766 0.00064887514 0.001899473 0.002939302 0.0035150051 0.0036435397]]...]
INFO - root - 2017-12-09 23:27:56.345575: step 68010, loss = 0.82, batch loss = 0.69 (11.1 examples/sec; 0.721 sec/batch; 52h:59m:47s remains)
INFO - root - 2017-12-09 23:28:04.779557: step 68020, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.859 sec/batch; 63h:07m:48s remains)
INFO - root - 2017-12-09 23:28:13.204428: step 68030, loss = 0.83, batch loss = 0.70 (9.5 examples/sec; 0.841 sec/batch; 61h:47m:22s remains)
INFO - root - 2017-12-09 23:28:21.718925: step 68040, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.851 sec/batch; 62h:29m:05s remains)
INFO - root - 2017-12-09 23:28:30.318196: step 68050, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.842 sec/batch; 61h:51m:02s remains)
INFO - root - 2017-12-09 23:28:38.947020: step 68060, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.855 sec/batch; 62h:48m:48s remains)
INFO - root - 2017-12-09 23:28:47.439133: step 68070, loss = 0.82, batch loss = 0.69 (9.9 examples/sec; 0.807 sec/batch; 59h:15m:50s remains)
INFO - root - 2017-12-09 23:28:56.016432: step 68080, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.863 sec/batch; 63h:24m:55s remains)
INFO - root - 2017-12-09 23:29:04.660706: step 68090, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 63h:17m:36s remains)
INFO - root - 2017-12-09 23:29:13.163124: step 68100, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 63h:51m:21s remains)
2017-12-09 23:29:14.111002: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0018034142 -0.0018028915 -0.0018041527 -0.0018056441 -0.0018064759 -0.001806737 -0.001806124 -0.0018049263 -0.0018034198 -0.001801634 -0.0018002326 -0.0017992624 -0.0017987101 -0.0017985204 -0.0017989371][-0.0018024271 -0.0018019043 -0.0018030538 -0.0018045255 -0.0018054271 -0.001805928 -0.0018056724 -0.0018048639 -0.0018038285 -0.0018024491 -0.0018013159 -0.0018005784 -0.0018002613 -0.0018003181 -0.0018008301][-0.0018022495 -0.0018016935 -0.0018026563 -0.0018040602 -0.0018050573 -0.0018056933 -0.0018056878 -0.0018051837 -0.0018045697 -0.0018036277 -0.0018027971 -0.0018023899 -0.0018024627 -0.0018028021 -0.0018033801][-0.0018020861 -0.0018014807 -0.001802244 -0.0018035322 -0.0018045707 -0.0018053295 -0.001805526 -0.0018052757 -0.0018050215 -0.0018045701 -0.0018041572 -0.0018040603 -0.0018045015 -0.0018051931 -0.0018059155][-0.0018021277 -0.0018013016 -0.0018018266 -0.0018028894 -0.0018038438 -0.0018046523 -0.0018050162 -0.0018049676 -0.0018050197 -0.0018050573 -0.0018050545 -0.0018052304 -0.0018059321 -0.0018069316 -0.0018077671][-0.001802212 -0.0018011441 -0.0018014758 -0.0018023625 -0.0018032109 -0.0018040003 -0.0018044645 -0.0018045525 -0.0018047785 -0.0018051272 -0.0018053799 -0.0018057283 -0.0018065885 -0.0018077582 -0.0018086969][-0.0018022581 -0.0018010364 -0.0018012479 -0.0018020484 -0.0018028711 -0.0018036138 -0.0018040714 -0.0018042236 -0.0018044345 -0.0018047865 -0.0018050708 -0.0018055094 -0.0018065318 -0.0018078543 -0.0018089528][-0.0018023239 -0.001800964 -0.0018011113 -0.0018018129 -0.0018025972 -0.0018032562 -0.0018036277 -0.0018037783 -0.0018039545 -0.0018042006 -0.0018044356 -0.0018049064 -0.0018059964 -0.0018073802 -0.0018085456][-0.0018023766 -0.0018009337 -0.0018010983 -0.0018017285 -0.0018024166 -0.0018029779 -0.001803219 -0.0018032799 -0.0018033488 -0.0018034932 -0.0018036611 -0.0018040847 -0.0018051197 -0.0018064678 -0.0018076077][-0.0018023148 -0.0018010221 -0.0018012678 -0.0018018605 -0.0018024541 -0.0018029043 -0.0018030109 -0.0018029104 -0.0018027761 -0.0018027704 -0.0018028832 -0.0018033207 -0.0018042775 -0.0018055243 -0.0018065848][-0.0018023083 -0.0018012932 -0.0018016137 -0.0018022552 -0.0018027964 -0.0018031669 -0.0018031219 -0.0018028022 -0.0018024378 -0.0018022474 -0.0018022826 -0.0018027001 -0.0018036012 -0.0018047466 -0.0018057203][-0.0018024101 -0.0018015534 -0.0018020109 -0.0018027724 -0.0018033147 -0.0018036158 -0.0018034236 -0.0018028938 -0.001802311 -0.001801925 -0.0018018531 -0.0018022056 -0.0018030464 -0.0018040725 -0.0018049374][-0.0018025989 -0.0018017953 -0.0018023507 -0.0018032141 -0.0018037349 -0.0018039573 -0.0018036733 -0.0018029931 -0.0018022712 -0.0018017651 -0.0018016271 -0.0018019204 -0.0018026794 -0.0018035505 -0.0018042797][-0.0018026745 -0.0018019845 -0.0018025734 -0.0018034836 -0.0018039282 -0.0018040747 -0.0018037392 -0.0018029821 -0.0018022205 -0.00180168 -0.0018015092 -0.001801717 -0.001802325 -0.0018030334 -0.001803663][-0.0018027013 -0.0018021169 -0.0018026831 -0.0018035808 -0.0018039249 -0.0018039658 -0.0018035744 -0.0018028202 -0.0018021087 -0.0018016368 -0.0018014745 -0.0018015866 -0.0018020577 -0.0018026152 -0.001803144]]...]
INFO - root - 2017-12-09 23:29:22.811835: step 68110, loss = 0.82, batch loss = 0.69 (10.5 examples/sec; 0.759 sec/batch; 55h:44m:12s remains)
INFO - root - 2017-12-09 23:29:31.084783: step 68120, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 63h:48m:50s remains)
INFO - root - 2017-12-09 23:29:39.688527: step 68130, loss = 0.82, batch loss = 0.69 (9.7 examples/sec; 0.828 sec/batch; 60h:48m:51s remains)
INFO - root - 2017-12-09 23:29:48.330537: step 68140, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.848 sec/batch; 62h:17m:17s remains)
INFO - root - 2017-12-09 23:29:57.092143: step 68150, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.895 sec/batch; 65h:44m:45s remains)
INFO - root - 2017-12-09 23:30:05.854305: step 68160, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.859 sec/batch; 63h:05m:50s remains)
INFO - root - 2017-12-09 23:30:14.497926: step 68170, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.837 sec/batch; 61h:28m:10s remains)
INFO - root - 2017-12-09 23:30:23.217878: step 68180, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.878 sec/batch; 64h:26m:43s remains)
INFO - root - 2017-12-09 23:30:32.039943: step 68190, loss = 0.82, batch loss = 0.69 (8.7 examples/sec; 0.917 sec/batch; 67h:20m:20s remains)
INFO - root - 2017-12-09 23:30:40.585888: step 68200, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 63h:48m:03s remains)
2017-12-09 23:30:41.488397: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.001800973 -0.0017995129 -0.001799322 -0.0017993436 -0.0017994399 -0.0017996887 -0.0018000804 -0.0018007153 -0.0018015419 -0.0018025881 -0.0018036352 -0.001804496 -0.0018051596 -0.0018056192 -0.0018060089][-0.0018002051 -0.0017987192 -0.0017985541 -0.0017986504 -0.0017988619 -0.0017992619 -0.0017997851 -0.0018005134 -0.001801367 -0.0018024133 -0.0018033803 -0.0018041122 -0.0018045894 -0.0018048985 -0.0018051966][-0.0018001375 -0.0017987298 -0.0017985594 -0.0017986916 -0.0017989819 -0.0017994851 -0.0018001017 -0.0018008812 -0.0018017031 -0.0018027101 -0.0018035751 -0.0018041675 -0.0018044905 -0.0018047343 -0.0018049604][-0.001800037 -0.0017986702 -0.0017985054 -0.0017986733 -0.0017990298 -0.001799633 -0.0018003491 -0.0018011598 -0.0018019291 -0.0018028581 -0.0018035454 -0.0018039503 -0.0018041811 -0.0018044247 -0.0018046239][-0.0018000415 -0.0017985963 -0.0017984121 -0.0017986228 -0.0017990501 -0.0017997519 -0.0018005479 -0.0018013922 -0.0018020809 -0.0018028313 -0.0018033092 -0.0018035515 -0.0018037367 -0.0018039573 -0.0018040969][-0.0017999839 -0.0017984818 -0.0017983008 -0.0017985427 -0.0017990068 -0.0017997405 -0.0018005559 -0.0018013713 -0.0018019286 -0.0018024177 -0.0018026939 -0.0018028606 -0.0018030751 -0.0018033163 -0.001803443][-0.0017999246 -0.0017983998 -0.0017982161 -0.0017984585 -0.0017989238 -0.0017996553 -0.0018004529 -0.0018011818 -0.0018015751 -0.0018017994 -0.0018018691 -0.0018019221 -0.0018021251 -0.0018023819 -0.0018025589][-0.0017999189 -0.0017983702 -0.0017981826 -0.0017984133 -0.0017988731 -0.0017995968 -0.0018004128 -0.0018011285 -0.0018014527 -0.0018014652 -0.0018012731 -0.0018010893 -0.0018011284 -0.0018012633 -0.001801373][-0.0017999194 -0.0017983646 -0.0017982264 -0.0017984521 -0.0017989018 -0.001799604 -0.0018004262 -0.0018011535 -0.0018014557 -0.0018013034 -0.0018008768 -0.0018004054 -0.0018001939 -0.0018001364 -0.0018001534][-0.0017998581 -0.0017984236 -0.0017982989 -0.0017984682 -0.0017988497 -0.0017994769 -0.0018002382 -0.0018009092 -0.0018011804 -0.0018009432 -0.0018003935 -0.0017997319 -0.0017993067 -0.0017991045 -0.0017990614][-0.0017998329 -0.0017984767 -0.0017982587 -0.0017983487 -0.0017986504 -0.0017992019 -0.0017998907 -0.0018004847 -0.0018007248 -0.0018004765 -0.0017998978 -0.0017991964 -0.0017986764 -0.0017983842 -0.001798245][-0.0017995611 -0.0017982386 -0.0017979764 -0.0017980094 -0.0017982873 -0.0017988078 -0.0017994428 -0.0017999932 -0.001800221 -0.0018000139 -0.0017994861 -0.0017988476 -0.0017983444 -0.001798032 -0.0017978331][-0.0017992508 -0.0017978881 -0.0017975385 -0.0017975721 -0.0017978777 -0.0017984199 -0.00179902 -0.0017995301 -0.0017997489 -0.0017996035 -0.0017991987 -0.0017987104 -0.0017983018 -0.0017980151 -0.0017978121][-0.001799161 -0.0017976582 -0.0017972175 -0.0017972756 -0.0017976266 -0.0017982026 -0.001798781 -0.0017992449 -0.0017994462 -0.0017993521 -0.0017990523 -0.0017986944 -0.0017984009 -0.0017981606 -0.0017979546][-0.0017994897 -0.0017978295 -0.00179725 -0.0017973306 -0.001797741 -0.0017983377 -0.001798903 -0.0017993004 -0.001799448 -0.0017993472 -0.0017990983 -0.0017988099 -0.0017985869 -0.0017983844 -0.0017982]]...]
INFO - root - 2017-12-09 23:30:50.120777: step 68210, loss = 0.82, batch loss = 0.69 (10.4 examples/sec; 0.771 sec/batch; 56h:37m:04s remains)
INFO - root - 2017-12-09 23:30:58.612558: step 68220, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.859 sec/batch; 63h:05m:24s remains)
INFO - root - 2017-12-09 23:31:07.231338: step 68230, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.831 sec/batch; 61h:01m:50s remains)
INFO - root - 2017-12-09 23:31:15.751237: step 68240, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.856 sec/batch; 62h:50m:43s remains)
INFO - root - 2017-12-09 23:31:24.295283: step 68250, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.846 sec/batch; 62h:06m:23s remains)
INFO - root - 2017-12-09 23:31:32.809916: step 68260, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.845 sec/batch; 62h:00m:54s remains)
INFO - root - 2017-12-09 23:31:41.368473: step 68270, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.874 sec/batch; 64h:08m:56s remains)
INFO - root - 2017-12-09 23:31:49.979880: step 68280, loss = 0.83, batch loss = 0.70 (9.3 examples/sec; 0.857 sec/batch; 62h:54m:00s remains)
INFO - root - 2017-12-09 23:31:58.607202: step 68290, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.884 sec/batch; 64h:50m:38s remains)
INFO - root - 2017-12-09 23:32:07.097925: step 68300, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 63h:45m:01s remains)
2017-12-09 23:32:08.063582: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.20158426 0.2224063 0.24047032 0.25589913 0.26818374 0.27561945 0.27637386 0.27166784 0.26233476 0.24898314 0.23100069 0.21008831 0.18881832 0.16820295 0.1477465][0.19693148 0.21917291 0.23887496 0.25559688 0.26926175 0.27805567 0.2803359 0.27709126 0.26873866 0.25591755 0.23791997 0.21662709 0.19477554 0.1735114 0.15240754][0.18838526 0.21099642 0.23117852 0.24813998 0.26234677 0.2717545 0.2750473 0.27288565 0.26555282 0.2532106 0.23527248 0.21411613 0.19247214 0.17149742 0.15066499][0.17938948 0.20154163 0.22166397 0.23887166 0.25330982 0.26324454 0.26750508 0.26625872 0.25977412 0.24794643 0.23056868 0.20997536 0.18888649 0.16853905 0.14831427][0.16987735 0.19071673 0.20999242 0.22673711 0.24086602 0.25091833 0.25574276 0.25507486 0.24936792 0.23847435 0.22249851 0.20326662 0.18335968 0.16399397 0.14460927][0.15835285 0.17687841 0.1943745 0.20994705 0.22334367 0.23332256 0.23887795 0.23928356 0.23475587 0.22543292 0.21140508 0.19410391 0.17555493 0.1573365 0.13907441][0.1432171 0.15882435 0.17375292 0.18775496 0.2002936 0.21021445 0.21666721 0.21847917 0.21577868 0.20841259 0.19674122 0.18177104 0.16491613 0.14794679 0.13098012][0.12585874 0.13825335 0.15039179 0.16264768 0.1746496 0.18513335 0.19293343 0.19656332 0.19582251 0.19042222 0.18075699 0.16755342 0.15218055 0.13638884 0.12066658][0.1078319 0.11724359 0.12689269 0.13772419 0.14945677 0.16078094 0.17020237 0.17556688 0.17640306 0.1725249 0.16415803 0.15191643 0.13729192 0.12227742 0.10773992][0.090530753 0.0971045 0.10441782 0.11405228 0.12573622 0.13802129 0.14882523 0.1557239 0.1578259 0.15479951 0.14694437 0.13499443 0.12071385 0.10619087 0.092574649][0.074820735 0.079467915 0.085260265 0.094121769 0.10570405 0.11851968 0.13001707 0.13768202 0.14022063 0.13737763 0.12939584 0.11728132 0.10325254 0.089337088 0.076754622][0.060769439 0.064535 0.06978602 0.078475624 0.090166695 0.10317463 0.114668 0.12232107 0.12465156 0.12144079 0.11315261 0.10097686 0.087351464 0.074151784 0.062581815][0.048133239 0.051645175 0.05699154 0.065874256 0.077675514 0.0905466 0.10169061 0.10884219 0.11060761 0.10684744 0.098131709 0.086093172 0.073116086 0.060936287 0.050545521][0.03578626 0.038641497 0.043821361 0.052502714 0.063937776 0.076411553 0.087173559 0.09397117 0.095439255 0.091510743 0.082901165 0.071435258 0.059481662 0.048616961 0.039606676][0.023949077 0.025960313 0.030328561 0.037980784 0.048182867 0.059283566 0.068981744 0.075290717 0.07678771 0.073259734 0.065554261 0.055523641 0.04528603 0.036219202 0.028948195]]...]
INFO - root - 2017-12-09 23:32:16.590457: step 68310, loss = 0.82, batch loss = 0.69 (11.3 examples/sec; 0.709 sec/batch; 52h:03m:19s remains)
INFO - root - 2017-12-09 23:32:25.212836: step 68320, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.872 sec/batch; 63h:57m:30s remains)
INFO - root - 2017-12-09 23:32:33.866479: step 68330, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.838 sec/batch; 61h:29m:05s remains)
INFO - root - 2017-12-09 23:32:42.387898: step 68340, loss = 0.82, batch loss = 0.69 (9.7 examples/sec; 0.824 sec/batch; 60h:29m:03s remains)
INFO - root - 2017-12-09 23:32:51.054110: step 68350, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.872 sec/batch; 63h:59m:50s remains)
INFO - root - 2017-12-09 23:32:59.783421: step 68360, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.876 sec/batch; 64h:14m:49s remains)
INFO - root - 2017-12-09 23:33:08.431709: step 68370, loss = 0.82, batch loss = 0.69 (8.8 examples/sec; 0.906 sec/batch; 66h:26m:13s remains)
INFO - root - 2017-12-09 23:33:17.117907: step 68380, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.870 sec/batch; 63h:49m:13s remains)
INFO - root - 2017-12-09 23:33:25.886805: step 68390, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.897 sec/batch; 65h:47m:50s remains)
INFO - root - 2017-12-09 23:33:34.547182: step 68400, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 63h:40m:43s remains)
2017-12-09 23:33:35.505994: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.29567584 0.29344109 0.28886211 0.28337678 0.27718332 0.2709645 0.2668649 0.2641432 0.26195014 0.25974545 0.25859413 0.25718576 0.25199574 0.24300526 0.2325411][0.29826224 0.29644728 0.29183519 0.28621498 0.27919546 0.27216208 0.26687929 0.26300597 0.260232 0.25771397 0.25690085 0.25632575 0.25269717 0.24533018 0.23604815][0.29712638 0.29614687 0.29170787 0.28537846 0.27652457 0.26790476 0.26096436 0.25541306 0.25145656 0.24878582 0.24866912 0.24900797 0.24714661 0.24183413 0.23439367][0.29439393 0.29490894 0.29128695 0.28473055 0.27456668 0.26403755 0.25530052 0.24853978 0.24292599 0.24003321 0.24126275 0.2431484 0.24289845 0.2389071 0.23306638][0.29053265 0.29323134 0.29020783 0.2830005 0.2716127 0.25986618 0.24972931 0.24128598 0.23504072 0.23264025 0.23428515 0.23696716 0.23791519 0.23565222 0.23104115][0.28434584 0.28958929 0.28726849 0.2799013 0.26757437 0.25476047 0.24347356 0.23421893 0.22750004 0.22537115 0.22812843 0.23155054 0.23294537 0.23140195 0.22767003][0.27178773 0.27974904 0.27806392 0.27055863 0.25768343 0.24444622 0.23245363 0.22319317 0.21694161 0.21619281 0.22049372 0.22533496 0.22842357 0.22813155 0.22551575][0.25470158 0.26474574 0.26439443 0.25766677 0.24542658 0.23239346 0.22025332 0.21149498 0.20577994 0.20606056 0.21178058 0.21811464 0.22225012 0.22310564 0.22126062][0.2334227 0.24521656 0.24630074 0.24156693 0.2315229 0.21994421 0.20890771 0.20186523 0.19791716 0.19925822 0.20560299 0.212544 0.21707964 0.21797609 0.21619695][0.21294649 0.2252384 0.22709027 0.22402123 0.21613307 0.20610599 0.19656786 0.19096774 0.18943472 0.19224566 0.19882812 0.20570989 0.21006668 0.21145374 0.20979366][0.19225362 0.20433985 0.20649973 0.20459725 0.19879848 0.19113159 0.18374582 0.18042195 0.18087848 0.18442574 0.19099383 0.19789986 0.20240805 0.20349969 0.20203914][0.17421028 0.18564235 0.18822865 0.18699352 0.1825659 0.1764081 0.17077276 0.1689934 0.1707689 0.17522219 0.18188013 0.18895338 0.19403109 0.19584179 0.19486456][0.16264811 0.17334323 0.17639276 0.17619495 0.17320414 0.1683168 0.16404109 0.16277757 0.16472793 0.16874956 0.17468724 0.18136318 0.18631405 0.18825212 0.18753898][0.15754732 0.16727583 0.16996403 0.17012523 0.16775633 0.16367109 0.16038372 0.15987062 0.16233192 0.16602017 0.17123301 0.1769107 0.18139641 0.18346983 0.18305387][0.15722805 0.16585012 0.16763535 0.16746373 0.16544664 0.16225965 0.15976003 0.15971978 0.1622995 0.16560027 0.16985972 0.17464224 0.17864336 0.18071221 0.18063453]]...]
INFO - root - 2017-12-09 23:33:44.118418: step 68410, loss = 0.82, batch loss = 0.69 (11.0 examples/sec; 0.730 sec/batch; 53h:32m:27s remains)
INFO - root - 2017-12-09 23:33:52.574013: step 68420, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.853 sec/batch; 62h:35m:48s remains)
INFO - root - 2017-12-09 23:34:01.273491: step 68430, loss = 0.81, batch loss = 0.68 (9.3 examples/sec; 0.860 sec/batch; 63h:07m:02s remains)
INFO - root - 2017-12-09 23:34:10.004098: step 68440, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.883 sec/batch; 64h:44m:46s remains)
INFO - root - 2017-12-09 23:34:18.830050: step 68450, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.860 sec/batch; 63h:02m:38s remains)
INFO - root - 2017-12-09 23:34:27.754812: step 68460, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.870 sec/batch; 63h:50m:40s remains)
INFO - root - 2017-12-09 23:34:36.489474: step 68470, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.894 sec/batch; 65h:35m:38s remains)
INFO - root - 2017-12-09 23:34:45.156378: step 68480, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.890 sec/batch; 65h:14m:32s remains)
INFO - root - 2017-12-09 23:34:53.963502: step 68490, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.854 sec/batch; 62h:36m:01s remains)
INFO - root - 2017-12-09 23:35:02.515667: step 68500, loss = 0.82, batch loss = 0.69 (9.7 examples/sec; 0.827 sec/batch; 60h:39m:59s remains)
2017-12-09 23:35:03.392113: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.42166537 0.41258529 0.40664795 0.40457943 0.4028556 0.40063292 0.39785898 0.39409611 0.3886086 0.38070416 0.37341174 0.36519703 0.35624483 0.34733918 0.34115788][0.43571684 0.42612913 0.41899565 0.4160555 0.41362116 0.4112944 0.4082858 0.40432265 0.39909586 0.39174545 0.38563636 0.37834007 0.37042904 0.36213425 0.35598284][0.44393319 0.43538976 0.42886826 0.42583248 0.42289683 0.41996163 0.41593212 0.41104439 0.40562233 0.3984136 0.39285597 0.38640365 0.37978482 0.37267116 0.366377][0.44905168 0.44317758 0.43760633 0.43536869 0.43270177 0.42958325 0.42494103 0.41887045 0.41260397 0.40580565 0.40097538 0.39592147 0.39098427 0.38581192 0.3806608][0.44547382 0.44462118 0.44166648 0.44119626 0.43978411 0.43762925 0.43274814 0.42618722 0.41992792 0.4137806 0.40985715 0.40655109 0.40302363 0.39975667 0.39633167][0.4293915 0.43554854 0.437793 0.44024378 0.44036439 0.43868417 0.43406808 0.42822421 0.42261672 0.41796577 0.41579068 0.41469583 0.41293356 0.41111124 0.40849531][0.39794025 0.41327873 0.4223536 0.42921653 0.43241403 0.43189773 0.42772496 0.42270726 0.41841915 0.41631803 0.41662008 0.41788608 0.41825682 0.41799131 0.41620848][0.35746005 0.38095191 0.396687 0.40864071 0.41539758 0.41631165 0.41291791 0.40820047 0.40515354 0.40539145 0.40783015 0.41169572 0.41384909 0.41523817 0.41425002][0.31122386 0.34096828 0.36243337 0.37874636 0.38885587 0.39230451 0.39054152 0.38639143 0.38409519 0.38554081 0.38962618 0.39509282 0.39844882 0.40152436 0.40199977][0.2637943 0.29729408 0.32319728 0.34282929 0.35603929 0.36216453 0.36290804 0.36060622 0.36003467 0.3622767 0.36702329 0.37276456 0.37676466 0.38042608 0.38131627][0.21951531 0.25379825 0.28192338 0.30381757 0.31933993 0.32820573 0.33160317 0.33187962 0.33315852 0.33644363 0.34154645 0.34665677 0.35068968 0.35457462 0.35573667][0.18346733 0.21662852 0.24508068 0.26801088 0.28557205 0.2966859 0.30234382 0.30442545 0.30678836 0.31051496 0.31522271 0.31959274 0.32336122 0.32729116 0.32935542][0.15815146 0.18826051 0.21504427 0.23704055 0.25480995 0.2669501 0.27408397 0.27766782 0.28103647 0.28483793 0.28868055 0.29219002 0.29524565 0.29870152 0.30111259][0.13683395 0.16345128 0.18811761 0.2086468 0.22565907 0.23849297 0.24717449 0.25270659 0.25705227 0.26120886 0.26470619 0.26729757 0.26965982 0.27254912 0.27519003][0.11876614 0.14179878 0.16348825 0.18257028 0.19920649 0.21218963 0.22175333 0.22877477 0.23432466 0.23884028 0.24186987 0.24385841 0.2455385 0.24757856 0.24990585]]...]
INFO - root - 2017-12-09 23:35:12.023405: step 68510, loss = 0.82, batch loss = 0.69 (9.9 examples/sec; 0.804 sec/batch; 58h:58m:12s remains)
INFO - root - 2017-12-09 23:35:20.701016: step 68520, loss = 0.82, batch loss = 0.69 (8.8 examples/sec; 0.904 sec/batch; 66h:18m:14s remains)
INFO - root - 2017-12-09 23:35:29.428720: step 68530, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.895 sec/batch; 65h:39m:25s remains)
INFO - root - 2017-12-09 23:35:38.162440: step 68540, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.843 sec/batch; 61h:50m:24s remains)
INFO - root - 2017-12-09 23:35:46.905292: step 68550, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.887 sec/batch; 65h:03m:18s remains)
INFO - root - 2017-12-09 23:35:55.541532: step 68560, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 63h:42m:21s remains)
INFO - root - 2017-12-09 23:36:04.157400: step 68570, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.859 sec/batch; 62h:57m:04s remains)
INFO - root - 2017-12-09 23:36:12.851305: step 68580, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.902 sec/batch; 66h:06m:35s remains)
INFO - root - 2017-12-09 23:36:21.697511: step 68590, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.860 sec/batch; 63h:00m:55s remains)
INFO - root - 2017-12-09 23:36:30.281665: step 68600, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.890 sec/batch; 65h:14m:41s remains)
2017-12-09 23:36:31.154310: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0017973735 -0.0017947641 -0.001793535 -0.0017922211 -0.001790925 -0.0017898506 -0.0017893317 -0.0017903831 -0.0017923114 -0.0017948769 -0.0017979869 -0.0018013123 -0.0018047156 -0.0018076057 -0.0018100133][-0.0017960409 -0.0017934879 -0.0017920963 -0.0017910568 -0.0017900015 -0.0017891768 -0.0017888756 -0.0017903014 -0.0017926523 -0.0017956776 -0.0017992234 -0.0018030325 -0.0018066512 -0.0018096534 -0.0018123215][-0.001798118 -0.0017957687 -0.0017944732 -0.001793439 -0.0017925523 -0.0017917776 -0.0017916794 -0.001793463 -0.0017963799 -0.0017999341 -0.0018038655 -0.0018081293 -0.0018119448 -0.0018146628 -0.0018169312][-0.0018028972 -0.0018006628 -0.0017994428 -0.0017983424 -0.001797315 -0.0017965962 -0.0017965052 -0.0017981167 -0.0018008627 -0.00180424 -0.0018081355 -0.0018119715 -0.0018155674 -0.0018182192 -0.001820442][-0.0018092081 -0.0018070755 -0.001805863 -0.0018047224 -0.0018036284 -0.0018028432 -0.001802556 -0.001803762 -0.0018058654 -0.0018085152 -0.0018116914 -0.0018148208 -0.0018179223 -0.0018203168 -0.0018224961][-0.0018146108 -0.0018124913 -0.0018113098 -0.0018102659 -0.0018093231 -0.0018086046 -0.0018081045 -0.0018086463 -0.0018098848 -0.0018115612 -0.0018137625 -0.001816062 -0.0018186904 -0.0018208942 -0.0018228972][-0.0018189627 -0.001816869 -0.0018159143 -0.0018150031 -0.0018142175 -0.0018136802 -0.0018131797 -0.0018131617 -0.0018136111 -0.0018142749 -0.0018153725 -0.0018165953 -0.0018184598 -0.0018202096 -0.0018220483][-0.0018212887 -0.0018192802 -0.0018185563 -0.0018178783 -0.0018173085 -0.0018169953 -0.0018166899 -0.0018164485 -0.0018165183 -0.0018164694 -0.0018168049 -0.0018171942 -0.0018184382 -0.001819732 -0.0018213115][-0.0018217212 -0.0018196173 -0.0018190022 -0.0018185596 -0.0018180733 -0.0018179241 -0.0018176906 -0.0018174531 -0.0018174053 -0.001817102 -0.0018171024 -0.0018170438 -0.0018177698 -0.0018188687 -0.0018202781][-0.0018209824 -0.0018189991 -0.0018183888 -0.0018181371 -0.0018178986 -0.0018177738 -0.0018177497 -0.0018176812 -0.0018177221 -0.0018177502 -0.0018180392 -0.0018184111 -0.0018190419 -0.0018197795 -0.0018206123][-0.0018196069 -0.0018178014 -0.0018171837 -0.0018170616 -0.0018171299 -0.0018173228 -0.0018179073 -0.0018182844 -0.0018189189 -0.0018194609 -0.0018198323 -0.0018200875 -0.0018202175 -0.0018202906 -0.0018202929][-0.0018191491 -0.0018176486 -0.0018172187 -0.0018173 -0.0018176168 -0.0018182787 -0.0018193244 -0.0018202018 -0.0018211583 -0.0018220056 -0.001822467 -0.001822466 -0.0018221822 -0.0018215772 -0.0018208744][-0.0018201943 -0.0018191256 -0.0018188708 -0.00181918 -0.0018197991 -0.0018207086 -0.0018216938 -0.0018222516 -0.0018225431 -0.0018228313 -0.001823027 -0.0018229106 -0.0018226175 -0.0018220394 -0.0018213326][-0.0018229226 -0.0018222749 -0.001822131 -0.0018225117 -0.0018229372 -0.0018233 -0.0018233235 -0.0018224437 -0.0018210402 -0.0018199872 -0.0018192725 -0.0018192274 -0.001819527 -0.0018198991 -0.0018201874][-0.0018266067 -0.0018264946 -0.0018266699 -0.0018271047 -0.0018271443 -0.0018266762 -0.0018251521 -0.001822302 -0.0018187648 -0.0018161072 -0.0018145607 -0.0018148466 -0.0018161746 -0.0018177428 -0.0018194519]]...]
INFO - root - 2017-12-09 23:36:39.747684: step 68610, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.873 sec/batch; 64h:01m:04s remains)
INFO - root - 2017-12-09 23:36:48.283958: step 68620, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.857 sec/batch; 62h:48m:15s remains)
INFO - root - 2017-12-09 23:36:56.981171: step 68630, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.853 sec/batch; 62h:33m:18s remains)
INFO - root - 2017-12-09 23:37:05.629966: step 68640, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.863 sec/batch; 63h:13m:06s remains)
INFO - root - 2017-12-09 23:37:14.288516: step 68650, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.891 sec/batch; 65h:17m:59s remains)
INFO - root - 2017-12-09 23:37:23.035808: step 68660, loss = 0.82, batch loss = 0.70 (9.2 examples/sec; 0.871 sec/batch; 63h:49m:38s remains)
INFO - root - 2017-12-09 23:37:31.796267: step 68670, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.861 sec/batch; 63h:05m:24s remains)
INFO - root - 2017-12-09 23:37:40.678627: step 68680, loss = 0.82, batch loss = 0.69 (8.1 examples/sec; 0.990 sec/batch; 72h:32m:04s remains)
INFO - root - 2017-12-09 23:37:49.362688: step 68690, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.857 sec/batch; 62h:48m:20s remains)
INFO - root - 2017-12-09 23:37:57.714047: step 68700, loss = 0.81, batch loss = 0.69 (9.5 examples/sec; 0.840 sec/batch; 61h:31m:39s remains)
2017-12-09 23:37:58.629441: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.049506214 0.06293498 0.075373858 0.086001672 0.091744468 0.093240589 0.090646237 0.085117877 0.077930778 0.070983358 0.066195346 0.06456545 0.065635696 0.069552757 0.074103564][0.05984927 0.077052467 0.09348 0.10767071 0.11608934 0.11880691 0.11588635 0.10933092 0.10052325 0.091646105 0.0851804 0.082744189 0.083854817 0.088414907 0.094286375][0.068646334 0.089602292 0.11036725 0.12844737 0.13973057 0.14387663 0.14104994 0.13335094 0.12278739 0.11187717 0.10337899 0.099502459 0.099949621 0.10438546 0.11082389][0.073599018 0.09775129 0.12250205 0.14422041 0.15857862 0.16439541 0.16210839 0.15389498 0.14191362 0.12936845 0.11894879 0.11332287 0.11244906 0.11591098 0.12197892][0.073914021 0.10022612 0.12788543 0.1525193 0.16973503 0.17732079 0.17597453 0.16783641 0.15545185 0.14181249 0.12969337 0.12216563 0.11929511 0.12077058 0.12542464][0.070801958 0.097993433 0.12726161 0.15409733 0.17349365 0.18289156 0.18269198 0.1750133 0.16267213 0.14857079 0.13544536 0.1256807 0.12031415 0.11902224 0.12100098][0.06488765 0.092020437 0.12198589 0.15032445 0.17165373 0.18290281 0.18398854 0.17728132 0.16542108 0.15121272 0.13724461 0.12546302 0.11727978 0.11223031 0.1109632][0.057065073 0.083024368 0.11274949 0.14181776 0.16478181 0.1781868 0.1813332 0.17621113 0.16531283 0.15159729 0.13722533 0.12363424 0.11223256 0.10305079 0.097600885][0.049013883 0.072981745 0.10137882 0.13023961 0.15428247 0.16980498 0.17553505 0.1730184 0.16409291 0.1516073 0.13728169 0.12230086 0.10781071 0.094530895 0.084916644][0.04131531 0.062882647 0.089274965 0.11713459 0.14153178 0.15890804 0.16720895 0.16749404 0.16107231 0.15048714 0.13687666 0.12086265 0.10394913 0.087347917 0.073887877][0.033503834 0.05247166 0.076522209 0.102718 0.12694626 0.145857 0.15676998 0.16006051 0.15638714 0.14809905 0.13569552 0.11929822 0.10063438 0.0814283 0.065084405][0.025755243 0.041460432 0.062437356 0.0863624 0.10990416 0.12990682 0.14330234 0.14964116 0.14905259 0.14333665 0.13265795 0.11675849 0.097537436 0.076961175 0.058893472][0.019022129 0.031245753 0.048316516 0.068946086 0.090709955 0.11091626 0.12627792 0.13539784 0.13786216 0.13471495 0.1261691 0.11183247 0.093522482 0.073337473 0.054998353][0.013288676 0.022475192 0.035741 0.052635591 0.071553558 0.090646692 0.10663135 0.11761575 0.12260303 0.12196227 0.115762 0.10364222 0.087529294 0.069301568 0.05230118][0.0087227542 0.015335591 0.025392823 0.038805407 0.054515004 0.071249656 0.086160347 0.097522758 0.10376149 0.10488237 0.10092812 0.091568977 0.078559458 0.063314274 0.04884221]]...]
INFO - root - 2017-12-09 23:38:07.093001: step 68710, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.844 sec/batch; 61h:52m:31s remains)
INFO - root - 2017-12-09 23:38:15.611352: step 68720, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.859 sec/batch; 62h:54m:31s remains)
INFO - root - 2017-12-09 23:38:24.383625: step 68730, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.865 sec/batch; 63h:23m:38s remains)
INFO - root - 2017-12-09 23:38:33.071002: step 68740, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.853 sec/batch; 62h:28m:32s remains)
INFO - root - 2017-12-09 23:38:41.666825: step 68750, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.845 sec/batch; 61h:56m:17s remains)
INFO - root - 2017-12-09 23:38:50.309937: step 68760, loss = 0.83, batch loss = 0.70 (9.2 examples/sec; 0.865 sec/batch; 63h:21m:43s remains)
INFO - root - 2017-12-09 23:38:58.993350: step 68770, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.865 sec/batch; 63h:23m:47s remains)
INFO - root - 2017-12-09 23:39:07.672768: step 68780, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 63h:30m:48s remains)
INFO - root - 2017-12-09 23:39:16.507374: step 68790, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.879 sec/batch; 64h:22m:54s remains)
INFO - root - 2017-12-09 23:39:25.028103: step 68800, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.893 sec/batch; 65h:24m:25s remains)
2017-12-09 23:39:25.959701: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.26962706 0.26358613 0.25497931 0.24371535 0.22912988 0.2116 0.19230404 0.17202514 0.15068263 0.12878816 0.10586788 0.084227711 0.065536223 0.051291771 0.041468654][0.27876797 0.27315459 0.26521888 0.25519142 0.24176104 0.22509746 0.20577091 0.18452422 0.16153534 0.13751903 0.1124437 0.088562258 0.068269119 0.053485297 0.044100061][0.28335124 0.27923927 0.27277714 0.26414344 0.25226846 0.23697658 0.218096 0.19672588 0.17310123 0.147964 0.12144788 0.096226111 0.074861668 0.059408654 0.050165012][0.28558683 0.28351644 0.2790516 0.27231979 0.26198786 0.24752007 0.22890514 0.20743278 0.18323757 0.15781361 0.13122208 0.1058707 0.084118582 0.06795115 0.058114193][0.28336817 0.28349105 0.28112826 0.27672 0.26869294 0.25609195 0.23833737 0.21737544 0.1932018 0.16802785 0.14151907 0.11637578 0.094655722 0.07822565 0.068123862][0.27927446 0.28165486 0.28094175 0.2781986 0.27179 0.26096931 0.24561596 0.22645171 0.20426711 0.18090796 0.15598744 0.13157159 0.10956069 0.09273 0.081738912][0.27192608 0.27659413 0.27697995 0.27507973 0.26965821 0.26052982 0.24732766 0.23144491 0.21285656 0.19317019 0.1718315 0.1502725 0.12992013 0.11327084 0.10200632][0.26235259 0.26875252 0.26982602 0.26843673 0.26369488 0.25579163 0.24462032 0.23161651 0.21674243 0.20152491 0.18468815 0.16697462 0.14954437 0.13458192 0.12394867][0.25106379 0.25894484 0.26058927 0.25952464 0.2552557 0.24812321 0.23869777 0.22857811 0.21705456 0.20567293 0.19296846 0.17930318 0.16473126 0.15168077 0.14185625][0.2371098 0.24613105 0.24851578 0.24821007 0.2449057 0.23896423 0.23139611 0.22355528 0.21510357 0.20693576 0.19732933 0.1868127 0.17499541 0.16413799 0.15548247][0.22044942 0.23079671 0.23434973 0.23468557 0.23233086 0.2274164 0.22145079 0.21544883 0.20918147 0.20346245 0.19662935 0.1890742 0.17950171 0.17042285 0.16290316][0.20038687 0.21154974 0.21623491 0.21796109 0.21704997 0.21317244 0.20802256 0.20332113 0.19832487 0.19395319 0.18867601 0.18344691 0.1765673 0.16938451 0.16349275][0.18060097 0.19165885 0.19694684 0.19943139 0.19968639 0.19682689 0.19273613 0.18853773 0.18405612 0.1800776 0.17564635 0.1718203 0.16666174 0.161482 0.15721951][0.16063924 0.17152445 0.1772352 0.18014589 0.18117879 0.17929885 0.17630154 0.1727998 0.16891997 0.16544935 0.1617524 0.15871327 0.15480646 0.15106288 0.14801078][0.14242107 0.15208387 0.15732506 0.16047142 0.16238716 0.16174175 0.15983883 0.15698835 0.15380922 0.15087344 0.14791255 0.14556687 0.14298135 0.14072886 0.13929917]]...]
INFO - root - 2017-12-09 23:39:34.514012: step 68810, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.885 sec/batch; 64h:47m:43s remains)
INFO - root - 2017-12-09 23:39:42.984721: step 68820, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 63h:40m:58s remains)
INFO - root - 2017-12-09 23:39:51.920519: step 68830, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.888 sec/batch; 65h:00m:19s remains)
INFO - root - 2017-12-09 23:40:00.507874: step 68840, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.829 sec/batch; 60h:44m:56s remains)
INFO - root - 2017-12-09 23:40:09.168507: step 68850, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.856 sec/batch; 62h:41m:23s remains)
INFO - root - 2017-12-09 23:40:17.793218: step 68860, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.874 sec/batch; 64h:02m:11s remains)
INFO - root - 2017-12-09 23:40:26.381935: step 68870, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.831 sec/batch; 60h:50m:56s remains)
INFO - root - 2017-12-09 23:40:34.988916: step 68880, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.858 sec/batch; 62h:51m:13s remains)
INFO - root - 2017-12-09 23:40:43.805880: step 68890, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.883 sec/batch; 64h:41m:10s remains)
INFO - root - 2017-12-09 23:40:52.360762: step 68900, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.885 sec/batch; 64h:46m:08s remains)
2017-12-09 23:40:53.311911: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.001818749 -0.0018184879 -0.0018183693 -0.0018183215 -0.0018183279 -0.001818467 -0.0018187105 -0.001819012 -0.0018193709 -0.0018196381 -0.0018198189 -0.001819953 -0.001820082 -0.0018202501 -0.0018204228][-0.0018174885 -0.001817016 -0.0018166828 -0.0018164365 -0.0018163185 -0.001816416 -0.0018166975 -0.0018170752 -0.0018175334 -0.0018179008 -0.0018181145 -0.0018183085 -0.0018185397 -0.0018188257 -0.0018191254][-0.0018161829 -0.001815294 -0.0018146384 -0.0018141707 -0.0018138625 -0.0018139028 -0.001814253 -0.0018147605 -0.0018153657 -0.0018158754 -0.0018161769 -0.001816403 -0.0018167124 -0.0018171192 -0.001817558][-0.0018149085 -0.0018134742 -0.0018124363 -0.0018116476 -0.0018111665 -0.0018111458 -0.0018115841 -0.0018122587 -0.0018130682 -0.0018137313 -0.001814155 -0.0018145086 -0.0018149246 -0.0018154365 -0.0018159802][-0.0018137672 -0.001811849 -0.0018104458 -0.0018092935 -0.0018085664 -0.0018083887 -0.0018088192 -0.0018095908 -0.0018105349 -0.0018114016 -0.001811976 -0.0018124824 -0.0018130168 -0.0018136398 -0.0018143386][-0.0018130068 -0.001810785 -0.0018090428 -0.0018075734 -0.0018065289 -0.0018061397 -0.0018064599 -0.0018072607 -0.0018082765 -0.0018092933 -0.0018101049 -0.0018107672 -0.0018114963 -0.0018122959 -0.0018130617][-0.0018129306 -0.0018104126 -0.0018084123 -0.0018065911 -0.00180523 -0.0018046299 -0.0018048279 -0.0018055817 -0.0018066718 -0.0018078126 -0.0018088073 -0.0018096427 -0.0018106222 -0.0018115293 -0.0018123378][-0.001813678 -0.0018110062 -0.0018088648 -0.0018067621 -0.0018051149 -0.001804168 -0.0018041065 -0.0018046772 -0.0018057101 -0.0018069535 -0.0018081308 -0.0018091395 -0.0018102564 -0.0018113391 -0.0018122909][-0.0018149748 -0.0018123781 -0.0018102402 -0.0018080284 -0.0018061614 -0.0018048962 -0.0018044773 -0.0018047479 -0.0018056257 -0.0018068429 -0.0018080787 -0.0018092943 -0.0018105793 -0.0018117109 -0.0018126506][-0.0018163469 -0.0018140401 -0.0018120941 -0.0018099668 -0.0018080164 -0.0018065571 -0.0018057943 -0.0018057354 -0.0018063596 -0.001807426 -0.0018086755 -0.0018099431 -0.0018112543 -0.0018123755 -0.0018132004][-0.0018175468 -0.0018155646 -0.0018139483 -0.001812093 -0.0018103007 -0.0018088646 -0.0018079753 -0.0018076837 -0.0018080435 -0.0018088779 -0.0018099885 -0.0018112011 -0.00181237 -0.0018133828 -0.0018142876][-0.0018183324 -0.0018166468 -0.0018154398 -0.0018140143 -0.0018125908 -0.0018113644 -0.0018104819 -0.0018100746 -0.0018102055 -0.0018107241 -0.0018113767 -0.0018122407 -0.0018131161 -0.0018138023 -0.0018146734][-0.0018185603 -0.0018171674 -0.0018163839 -0.0018154529 -0.0018145217 -0.0018136937 -0.0018129632 -0.0018125693 -0.0018124867 -0.0018126373 -0.0018128931 -0.0018132231 -0.0018136731 -0.0018141771 -0.0018148717][-0.0018182849 -0.0018171462 -0.0018167498 -0.0018162419 -0.001815795 -0.0018154929 -0.0018151428 -0.001814784 -0.0018145498 -0.0018143972 -0.0018141448 -0.0018138664 -0.0018140028 -0.0018142504 -0.0018146192][-0.0018178462 -0.0018168872 -0.0018168129 -0.0018166966 -0.0018167484 -0.001816895 -0.0018168008 -0.0018165931 -0.001816242 -0.0018159328 -0.0018153795 -0.0018146671 -0.0018143179 -0.0018142267 -0.0018144103]]...]
INFO - root - 2017-12-09 23:41:01.848303: step 68910, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.845 sec/batch; 61h:52m:45s remains)
INFO - root - 2017-12-09 23:41:10.335368: step 68920, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 63h:34m:15s remains)
INFO - root - 2017-12-09 23:41:18.968737: step 68930, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.885 sec/batch; 64h:45m:50s remains)
INFO - root - 2017-12-09 23:41:27.657452: step 68940, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 63h:06m:15s remains)
INFO - root - 2017-12-09 23:41:36.351881: step 68950, loss = 0.81, batch loss = 0.68 (8.9 examples/sec; 0.898 sec/batch; 65h:45m:04s remains)
INFO - root - 2017-12-09 23:41:45.148367: step 68960, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.858 sec/batch; 62h:49m:51s remains)
INFO - root - 2017-12-09 23:41:53.858551: step 68970, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 63h:07m:35s remains)
INFO - root - 2017-12-09 23:42:02.493342: step 68980, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 63h:37m:29s remains)
INFO - root - 2017-12-09 23:42:11.240260: step 68990, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.892 sec/batch; 65h:18m:20s remains)
INFO - root - 2017-12-09 23:42:19.915126: step 69000, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.879 sec/batch; 64h:20m:14s remains)
2017-12-09 23:42:20.819488: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.00057116279 -0.0006309225 -0.00078728015 -0.00098398724 -0.0011626136 -0.001289296 -0.0013507328 -0.0013445914 -0.001279835 -0.0011908973 -0.0011385349 -0.0011695133 -0.0012727056 -0.001418134 -0.0015708121][0.00040269119 0.00037124951 0.00014238467 -0.00020534953 -0.00055809948 -0.0008256156 -0.00097564294 -0.000995385 -0.00090489187 -0.00076322514 -0.00066770648 -0.00069572369 -0.00084280537 -0.0010731725 -0.0013343503][0.0014964856 0.0015064495 0.0012053755 0.00069429248 0.00014089502 -0.00030769745 -0.00058180874 -0.00065147376 -0.00055111886 -0.00035778829 -0.00020736048 -0.00021788629 -0.0003925116 -0.00069408643 -0.0010539866][0.0024960497 0.0025256425 0.0021328414 0.0014527271 0.00071584678 0.00010674389 -0.0002788303 -0.00040665304 -0.00031212624 -8.6716725e-05 0.00010395597 0.00011470087 -6.8465946e-05 -0.00041079347 -0.00083115429][0.00294459 0.0029602135 0.002488242 0.0017021064 0.00087016716 0.00018015841 -0.00025342987 -0.00040407537 -0.00030090415 -5.2737421e-05 0.00015897222 0.00018800108 1.63574e-05 -0.00032574171 -0.000759316][0.0026417952 0.002615415 0.0021075155 0.0013142132 0.0005041376 -0.00015606754 -0.00055444322 -0.00067704509 -0.00054957124 -0.00029355218 -8.5161533e-05 -4.7896057e-05 -0.00018986594 -0.00048751605 -0.00087742356][0.0017560947 0.001684988 0.0012093385 0.0005039348 -0.0001857006 -0.00072102307 -0.0010116664 -0.0010665283 -0.00092199136 -0.00069989508 -0.00054170389 -0.00052575034 -0.00064164039 -0.000868442 -0.0011640497][0.00051124406 0.00042112276 5.4601696e-05 -0.00047180278 -0.00095023523 -0.0012820592 -0.0014090465 -0.0013683785 -0.0012117114 -0.0010438233 -0.00095370674 -0.00097602821 -0.0010874876 -0.0012621391 -0.0014651518][-0.00061630085 -0.0006877057 -0.00090261322 -0.0012080928 -0.0014559708 -0.0015887457 -0.001573741 -0.0014500878 -0.0012780558 -0.0011442183 -0.0011026969 -0.0011554072 -0.0012799054 -0.0014531784 -0.0016261094][-0.001417755 -0.0014541677 -0.0015389464 -0.0016526 -0.0017234681 -0.0017186457 -0.0016130202 -0.001430562 -0.001234927 -0.0011037069 -0.0010743296 -0.0011418893 -0.0012869479 -0.0014870707 -0.0016732903][-0.001727387 -0.0017319092 -0.0017497041 -0.0017744569 -0.001782121 -0.001744563 -0.0016362695 -0.0014632463 -0.0012845115 -0.0011624598 -0.0011271485 -0.0011807254 -0.0013163532 -0.0015118555 -0.0016915915][-0.0018279835 -0.0018258669 -0.001822788 -0.0018183517 -0.001810642 -0.0017828359 -0.0017162628 -0.0016131857 -0.0015102259 -0.0014401625 -0.00141572 -0.0014432892 -0.0015257674 -0.0016459335 -0.0017530841][-0.0018297147 -0.0018282952 -0.0018277913 -0.0018265052 -0.0018234316 -0.0018137137 -0.0017909259 -0.0017569908 -0.0017225789 -0.0016992602 -0.0016911882 -0.0017000374 -0.001728434 -0.0017683061 -0.0018035866][-0.0018301151 -0.00182898 -0.0018287108 -0.0018286968 -0.0018286376 -0.0018278204 -0.0018263038 -0.0018241614 -0.0018220877 -0.0018206775 -0.0018201973 -0.00182113 -0.0018232057 -0.0018258482 -0.0018278998][-0.0018302391 -0.0018292544 -0.0018289702 -0.0018289068 -0.0018289393 -0.0018289649 -0.0018290044 -0.0018290115 -0.0018288726 -0.0018286394 -0.0018284566 -0.0018284102 -0.0018284238 -0.0018284704 -0.0018286078]]...]
INFO - root - 2017-12-09 23:42:29.486158: step 69010, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.866 sec/batch; 63h:24m:01s remains)
INFO - root - 2017-12-09 23:42:37.996621: step 69020, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.836 sec/batch; 61h:10m:16s remains)
INFO - root - 2017-12-09 23:42:46.511878: step 69030, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.866 sec/batch; 63h:20m:33s remains)
INFO - root - 2017-12-09 23:42:55.147743: step 69040, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.847 sec/batch; 62h:00m:58s remains)
INFO - root - 2017-12-09 23:43:03.691699: step 69050, loss = 0.81, batch loss = 0.68 (9.5 examples/sec; 0.846 sec/batch; 61h:56m:35s remains)
INFO - root - 2017-12-09 23:43:12.259735: step 69060, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 63h:36m:03s remains)
INFO - root - 2017-12-09 23:43:20.956559: step 69070, loss = 0.82, batch loss = 0.69 (9.9 examples/sec; 0.812 sec/batch; 59h:23m:14s remains)
INFO - root - 2017-12-09 23:43:29.482676: step 69080, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.846 sec/batch; 61h:52m:40s remains)
INFO - root - 2017-12-09 23:43:38.058583: step 69090, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.885 sec/batch; 64h:43m:26s remains)
INFO - root - 2017-12-09 23:43:46.528339: step 69100, loss = 0.82, batch loss = 0.69 (9.7 examples/sec; 0.827 sec/batch; 60h:29m:38s remains)
2017-12-09 23:43:47.480510: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0017879705 -0.0017791581 -0.0017717774 -0.0017649423 -0.0017596119 -0.0017570382 -0.0017565218 -0.0017577718 -0.0017599883 -0.0017628318 -0.0017654367 -0.0017675265 -0.0017690781 -0.0017699995 -0.0017704086][-0.0017934084 -0.0017845221 -0.0017767211 -0.0017693612 -0.0017634375 -0.001760434 -0.0017597986 -0.0017612373 -0.0017637843 -0.0017671486 -0.0017702582 -0.001772741 -0.0017745825 -0.0017755022 -0.0017759626][-0.0018021272 -0.0017944382 -0.0017873025 -0.0017805237 -0.0017748101 -0.0017717785 -0.0017709119 -0.0017720828 -0.0017745518 -0.0017778989 -0.0017809246 -0.0017833977 -0.0017853387 -0.0017866538 -0.001787383][-0.0018101186 -0.0018041668 -0.0017985397 -0.0017930943 -0.0017882804 -0.0017855761 -0.0017846081 -0.0017854876 -0.0017876409 -0.0017906843 -0.0017934282 -0.0017954985 -0.0017971208 -0.0017980902 -0.001798525][-0.0018146767 -0.0018102165 -0.0018058687 -0.0018017337 -0.001797944 -0.0017957153 -0.0017950014 -0.0017957239 -0.0017975792 -0.0018002597 -0.001802642 -0.0018043034 -0.00180544 -0.0018060568 -0.0018061234][-0.0018149919 -0.0018119002 -0.0018092241 -0.0018066104 -0.0018040012 -0.0018024174 -0.0018017637 -0.0018021439 -0.0018033931 -0.0018053609 -0.0018071043 -0.0018081857 -0.0018088502 -0.0018090598 -0.0018088171][-0.0018128289 -0.0018102418 -0.0018087529 -0.0018075446 -0.0018062519 -0.001805498 -0.0018051225 -0.0018055659 -0.0018064234 -0.0018075254 -0.0018085333 -0.0018091076 -0.0018094659 -0.0018095246 -0.0018091878][-0.0018095378 -0.0018069743 -0.0018058228 -0.0018050664 -0.0018041488 -0.0018036824 -0.0018035015 -0.0018037644 -0.0018044219 -0.0018053399 -0.0018061616 -0.0018064977 -0.0018065989 -0.0018065948 -0.0018062561][-0.0018074261 -0.0018044739 -0.0018029439 -0.0018019088 -0.0018005524 -0.001800004 -0.0018000136 -0.0018002039 -0.0018005215 -0.0018011826 -0.0018021538 -0.0018026893 -0.0018027633 -0.0018028847 -0.0018028831][-0.00180626 -0.0018028972 -0.0018011066 -0.0017997787 -0.0017981596 -0.0017975138 -0.0017975853 -0.0017979003 -0.0017981739 -0.0017986591 -0.0017996237 -0.0018001668 -0.001800174 -0.0018002714 -0.0018003109][-0.0018056369 -0.0018026471 -0.0018010616 -0.0018000625 -0.0017988766 -0.0017982959 -0.0017982753 -0.0017985975 -0.0017986542 -0.0017986664 -0.0017993262 -0.0017996801 -0.0017996771 -0.001799821 -0.0017999511][-0.001805127 -0.0018028914 -0.0018017134 -0.0018010662 -0.0018003257 -0.0017999067 -0.0017999915 -0.0018000493 -0.0017998177 -0.0017996024 -0.0018000695 -0.0018002602 -0.0018000891 -0.0018002065 -0.0018004323][-0.0018040332 -0.0018023966 -0.0018016428 -0.0018012872 -0.0018006511 -0.0018002294 -0.0018001462 -0.0017997109 -0.0017991298 -0.0017988386 -0.0017991675 -0.0017992072 -0.0017989744 -0.001799016 -0.0017991032][-0.0018035712 -0.0018022849 -0.0018016718 -0.0018014718 -0.0018012503 -0.0018008997 -0.0018004973 -0.0017996392 -0.0017987897 -0.0017980259 -0.0017979047 -0.0017977577 -0.0017977052 -0.0017978922 -0.001797996][-0.001803259 -0.0018024694 -0.0018021893 -0.0018022549 -0.0018022557 -0.0018018453 -0.0018013074 -0.0018002175 -0.0017992186 -0.0017981867 -0.0017975979 -0.0017972434 -0.001797232 -0.0017976668 -0.0017979537]]...]
INFO - root - 2017-12-09 23:43:55.984748: step 69110, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.851 sec/batch; 62h:15m:34s remains)
INFO - root - 2017-12-09 23:44:04.476398: step 69120, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.876 sec/batch; 64h:04m:48s remains)
INFO - root - 2017-12-09 23:44:13.171392: step 69130, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.865 sec/batch; 63h:16m:45s remains)
INFO - root - 2017-12-09 23:44:21.832416: step 69140, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.845 sec/batch; 61h:49m:47s remains)
INFO - root - 2017-12-09 23:44:30.518568: step 69150, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.876 sec/batch; 64h:03m:06s remains)
INFO - root - 2017-12-09 23:44:39.280906: step 69160, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.895 sec/batch; 65h:28m:53s remains)
INFO - root - 2017-12-09 23:44:48.056258: step 69170, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.883 sec/batch; 64h:36m:08s remains)
INFO - root - 2017-12-09 23:44:56.767757: step 69180, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.861 sec/batch; 62h:57m:05s remains)
INFO - root - 2017-12-09 23:45:05.351885: step 69190, loss = 0.82, batch loss = 0.69 (9.8 examples/sec; 0.815 sec/batch; 59h:36m:36s remains)
INFO - root - 2017-12-09 23:45:13.696770: step 69200, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.833 sec/batch; 60h:56m:23s remains)
2017-12-09 23:45:14.599337: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.13337065 0.13235296 0.13076515 0.12778383 0.12387919 0.11944131 0.11454949 0.10947202 0.10454718 0.10039145 0.09683419 0.093443625 0.090844385 0.088905491 0.087611154][0.13549764 0.13497075 0.13347083 0.13033532 0.12602384 0.12087971 0.11546461 0.10995563 0.10468046 0.10023565 0.096392758 0.092439137 0.08915782 0.086830363 0.085385814][0.13663402 0.13655104 0.13520618 0.13211392 0.12760027 0.12203762 0.1161476 0.11034095 0.10477689 0.10002875 0.095885776 0.091341957 0.087616146 0.084851906 0.083105251][0.13779466 0.13843504 0.1373091 0.13448195 0.13001162 0.12446038 0.11846417 0.11249916 0.10698679 0.1020906 0.097754844 0.0928809 0.088819414 0.085567527 0.08338362][0.13859525 0.1400909 0.13937777 0.13698222 0.13300338 0.1277028 0.12177116 0.11609329 0.11087818 0.10619257 0.10180087 0.096956283 0.092771158 0.088947482 0.086164914][0.13927722 0.14167871 0.14145415 0.13958076 0.13622801 0.13145594 0.12619036 0.12077548 0.11573115 0.11118929 0.10672247 0.10187089 0.097409539 0.093452 0.090474673][0.13840444 0.14137921 0.1413181 0.13994029 0.13722979 0.1332068 0.12881231 0.12445042 0.1203087 0.11616052 0.11201728 0.10749076 0.10301684 0.0987377 0.095379479][0.13726939 0.14045671 0.14018875 0.13864008 0.13607521 0.13266158 0.12899424 0.12547505 0.12216355 0.11861256 0.11490626 0.11082692 0.106726 0.10248418 0.098919861][0.13535713 0.13907368 0.13891843 0.13737236 0.13492802 0.13187203 0.12856422 0.12537616 0.12235476 0.11917314 0.11584571 0.11230624 0.10856064 0.10436287 0.10059469][0.13265902 0.13655333 0.13626313 0.13459197 0.13216479 0.12900747 0.12562376 0.12238593 0.11926378 0.11613093 0.11296807 0.10987844 0.10663445 0.10295536 0.099348754][0.13019444 0.1339108 0.13317989 0.13113838 0.12830538 0.12483493 0.1211511 0.11746212 0.11396217 0.11062725 0.10745582 0.10439884 0.10134397 0.098209746 0.0950027][0.12687363 0.1302458 0.12887757 0.12633842 0.12299692 0.11907756 0.11485078 0.11079445 0.10709164 0.10351119 0.10036486 0.097371712 0.094659567 0.091906719 0.089053512][0.12333495 0.12639338 0.12477634 0.12190519 0.11822969 0.11370097 0.10869301 0.10390863 0.099595077 0.09571778 0.092409186 0.089430422 0.087131649 0.085018836 0.082819209][0.11801394 0.12073934 0.11886397 0.11595455 0.11227935 0.10758138 0.10232075 0.0970866 0.092312269 0.088270552 0.08483921 0.082002625 0.080150872 0.078815125 0.077448368][0.11229609 0.11452555 0.1122958 0.10930628 0.10566877 0.10104418 0.09593083 0.090695575 0.085919529 0.081841528 0.078463256 0.075823046 0.074342549 0.073725671 0.073177747]]...]
INFO - root - 2017-12-09 23:45:23.027364: step 69210, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.892 sec/batch; 65h:16m:14s remains)
INFO - root - 2017-12-09 23:45:31.463067: step 69220, loss = 0.82, batch loss = 0.69 (10.7 examples/sec; 0.751 sec/batch; 54h:56m:00s remains)
INFO - root - 2017-12-09 23:45:39.919938: step 69230, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.859 sec/batch; 62h:49m:55s remains)
INFO - root - 2017-12-09 23:45:48.367521: step 69240, loss = 0.82, batch loss = 0.69 (9.9 examples/sec; 0.810 sec/batch; 59h:12m:34s remains)
INFO - root - 2017-12-09 23:45:56.907271: step 69250, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.840 sec/batch; 61h:25m:46s remains)
INFO - root - 2017-12-09 23:46:05.402346: step 69260, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.846 sec/batch; 61h:52m:40s remains)
INFO - root - 2017-12-09 23:46:14.098882: step 69270, loss = 0.82, batch loss = 0.69 (9.7 examples/sec; 0.829 sec/batch; 60h:36m:42s remains)
INFO - root - 2017-12-09 23:46:22.764167: step 69280, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.878 sec/batch; 64h:10m:56s remains)
INFO - root - 2017-12-09 23:46:31.364964: step 69290, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 63h:39m:30s remains)
INFO - root - 2017-12-09 23:46:39.727512: step 69300, loss = 0.83, batch loss = 0.70 (9.4 examples/sec; 0.854 sec/batch; 62h:27m:56s remains)
2017-12-09 23:46:40.592413: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.23315187 0.23099147 0.22706993 0.22320051 0.22209626 0.22150286 0.22069478 0.21951067 0.21755913 0.21388341 0.20712771 0.19995192 0.19385852 0.18909979 0.18527238][0.25660771 0.25826284 0.25599194 0.25285921 0.25101593 0.24829954 0.24429321 0.23902926 0.23353285 0.22594328 0.21613336 0.20646894 0.19902931 0.19403604 0.19116983][0.27062428 0.27709821 0.2780686 0.27660671 0.27383944 0.26897395 0.26107502 0.25165689 0.24194734 0.23039064 0.2175127 0.20499709 0.19576189 0.19001582 0.18801695][0.2777797 0.28958732 0.29470652 0.29563418 0.29308313 0.28636819 0.27551043 0.26211277 0.24809267 0.2319361 0.21549672 0.20069255 0.19020808 0.18357296 0.18198664][0.28015244 0.2965771 0.30549619 0.3094714 0.3082647 0.30168259 0.28984225 0.27381241 0.25569946 0.23547632 0.21561742 0.19837722 0.18627876 0.17995891 0.17946278][0.28147188 0.30241764 0.31451076 0.32038987 0.32071003 0.31544086 0.30345315 0.28604245 0.26536453 0.24259785 0.22048864 0.20098586 0.1879198 0.1813591 0.18170461][0.28233406 0.30581605 0.31992555 0.32747307 0.32897857 0.32475489 0.31375766 0.29643297 0.27473617 0.25119767 0.22840422 0.20825846 0.19486824 0.18855806 0.18970424][0.28437597 0.30903164 0.32360724 0.33148977 0.33348069 0.32991284 0.31989145 0.30358931 0.28244179 0.2597045 0.23819157 0.21897645 0.20599274 0.19988506 0.20099722][0.28543872 0.30962813 0.32313102 0.3302756 0.3324905 0.32952824 0.32076865 0.30587107 0.28645134 0.26612011 0.247126 0.23024338 0.21889895 0.2136817 0.21426243][0.2841067 0.30612221 0.31681412 0.3220576 0.32357949 0.3214567 0.31453383 0.30201894 0.28555363 0.26807538 0.25227764 0.23773517 0.22776088 0.22316328 0.22320887][0.2781989 0.29791072 0.30588388 0.30845398 0.30863363 0.30651328 0.30067796 0.29029369 0.27656394 0.26226324 0.24953115 0.237773 0.22969952 0.22558542 0.22560489][0.26568168 0.28276649 0.28834006 0.28922224 0.2883029 0.28571761 0.28055641 0.27198941 0.26076859 0.24885899 0.23865975 0.22954708 0.2230833 0.21998809 0.22039704][0.24831143 0.26296419 0.26646987 0.26535597 0.26319313 0.26000276 0.25475991 0.24675271 0.2369972 0.2272833 0.21934018 0.21259376 0.20838071 0.20718786 0.20873694][0.22633159 0.23859939 0.24072723 0.23906875 0.2367174 0.2336394 0.22899334 0.22179705 0.21284549 0.20411259 0.19710796 0.19179106 0.18906303 0.18931609 0.19214235][0.20232053 0.21260977 0.21399944 0.21245775 0.21045214 0.2081117 0.20430908 0.19808474 0.19009227 0.18195309 0.17530614 0.17047743 0.16816241 0.16868034 0.17181554]]...]
INFO - root - 2017-12-09 23:46:49.060069: step 69310, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 63h:30m:12s remains)
INFO - root - 2017-12-09 23:46:57.588300: step 69320, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.860 sec/batch; 62h:52m:01s remains)
INFO - root - 2017-12-09 23:47:05.980198: step 69330, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 63h:33m:43s remains)
INFO - root - 2017-12-09 23:47:14.561665: step 69340, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.853 sec/batch; 62h:19m:14s remains)
INFO - root - 2017-12-09 23:47:23.191915: step 69350, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.858 sec/batch; 62h:43m:45s remains)
INFO - root - 2017-12-09 23:47:31.812339: step 69360, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.855 sec/batch; 62h:31m:10s remains)
INFO - root - 2017-12-09 23:47:40.372264: step 69370, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.863 sec/batch; 63h:03m:00s remains)
INFO - root - 2017-12-09 23:47:48.995954: step 69380, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.876 sec/batch; 64h:02m:36s remains)
INFO - root - 2017-12-09 23:47:57.748193: step 69390, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.849 sec/batch; 62h:02m:30s remains)
INFO - root - 2017-12-09 23:48:06.302931: step 69400, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.855 sec/batch; 62h:31m:17s remains)
2017-12-09 23:48:07.256984: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0018262907 -0.0018256868 -0.0018257188 -0.0018259268 -0.0018263975 -0.0018270167 -0.0018274619 -0.0018275875 -0.0018278986 -0.0018279647 -0.0018277415 -0.001827613 -0.0018279125 -0.0018283654 -0.0018287533][-0.001826159 -0.0018257421 -0.0018257271 -0.0018260506 -0.0018267422 -0.0018274524 -0.0018279688 -0.0018282974 -0.0018285868 -0.0018284337 -0.0018280608 -0.0018277486 -0.0018275749 -0.0018276213 -0.0018276332][-0.001826535 -0.0018263465 -0.001826631 -0.0018272111 -0.0018279725 -0.0018286093 -0.0018289929 -0.0018289761 -0.0018289241 -0.001828666 -0.0018283775 -0.0018280558 -0.0018276104 -0.0018271785 -0.0018268436][-0.0018270378 -0.0018273041 -0.0018280934 -0.0018286711 -0.0018287571 -0.0018272515 -0.0018238288 -0.001819832 -0.0018178152 -0.0018191476 -0.0018226973 -0.0018258496 -0.0018274039 -0.001827407 -0.0018267261][-0.0018277194 -0.00182846 -0.0018294468 -0.001828954 -0.0018239671 -0.0018099595 -0.0017867322 -0.0017636773 -0.0017543621 -0.0017655932 -0.0017896477 -0.0018117571 -0.0018239978 -0.0018275398 -0.0018271903][-0.0018283667 -0.0018294002 -0.0018294536 -0.0018236926 -0.0018021916 -0.0017541808 -0.0016822099 -0.0016151238 -0.0015919229 -0.0016290857 -0.0017030837 -0.001772115 -0.0018121907 -0.0018263891 -0.0018280318][-0.0018293452 -0.0018302634 -0.0018278159 -0.0018103777 -0.0017564505 -0.0016475156 -0.001493945 -0.0013579511 -0.0013172362 -0.0013996281 -0.0015556547 -0.0017020784 -0.0017892295 -0.0018226212 -0.0018288571][-0.0018309539 -0.001830821 -0.0018228588 -0.0017859826 -0.001685897 -0.0014994354 -0.001252248 -0.0010431665 -0.00098797912 -0.0011228061 -0.0013717538 -0.0016090288 -0.001755188 -0.0018150026 -0.0018289192][-0.0018328556 -0.0018306772 -0.0018137246 -0.0017520256 -0.001602415 -0.001343475 -0.0010196818 -0.00075619831 -0.00068939046 -0.00086096814 -0.0011819268 -0.0014996923 -0.0017072519 -0.0018004129 -0.001826418][-0.0018346244 -0.0018289146 -0.0017996106 -0.0017087633 -0.0015110481 -0.0011955081 -0.00082514342 -0.00053569244 -0.00046179385 -0.00064615963 -0.0010041348 -0.0013784572 -0.0016432975 -0.0017749523 -0.0018151791][-0.00183566 -0.0018249316 -0.0017809136 -0.0016603427 -0.0014216967 -0.0010685015 -0.0006780827 -0.00038268161 -0.00030223047 -0.00047791598 -0.00084100652 -0.0012475508 -0.0015627269 -0.001736243 -0.0017911518][-0.0018320781 -0.0018110233 -0.0017462613 -0.0015936743 -0.0013208846 -0.00094716245 -0.00055915944 -0.00027758081 -0.00020155462 -0.00036569277 -0.00071855355 -0.0011366406 -0.0014855717 -0.0016921753 -0.0017573894][-0.0018078563 -0.0017618045 -0.0016641569 -0.0014785808 -0.0011870074 -0.00082127121 -0.00046786398 -0.00022612372 -0.00017140468 -0.00032773695 -0.00065864588 -0.0010634905 -0.0014190152 -0.0016397961 -0.0017094992][-0.001742974 -0.0016491862 -0.001503616 -0.0012890643 -0.0010038652 -0.00068221381 -0.00039596227 -0.00021718256 -0.00019757717 -0.00035076344 -0.000651108 -0.0010204206 -0.001354452 -0.001567366 -0.0016331234][-0.0016368462 -0.0014749167 -0.0012709668 -0.0010329254 -0.0007711146 -0.00051315164 -0.00030616031 -0.00019697146 -0.00021614041 -0.00036823843 -0.00063156127 -0.00094720878 -0.0012333486 -0.001414985 -0.0014660819]]...]
INFO - root - 2017-12-09 23:48:15.696012: step 69410, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.846 sec/batch; 61h:51m:22s remains)
INFO - root - 2017-12-09 23:48:24.287563: step 69420, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.873 sec/batch; 63h:49m:55s remains)
INFO - root - 2017-12-09 23:48:32.872951: step 69430, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.872 sec/batch; 63h:41m:33s remains)
INFO - root - 2017-12-09 23:48:41.531409: step 69440, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.833 sec/batch; 60h:51m:34s remains)
INFO - root - 2017-12-09 23:48:50.072184: step 69450, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.852 sec/batch; 62h:15m:40s remains)
INFO - root - 2017-12-09 23:48:58.843015: step 69460, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.882 sec/batch; 64h:24m:38s remains)
INFO - root - 2017-12-09 23:49:07.626169: step 69470, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 63h:06m:33s remains)
INFO - root - 2017-12-09 23:49:16.333276: step 69480, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.865 sec/batch; 63h:10m:13s remains)
INFO - root - 2017-12-09 23:49:25.043935: step 69490, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.856 sec/batch; 62h:30m:20s remains)
INFO - root - 2017-12-09 23:49:33.599118: step 69500, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 63h:24m:08s remains)
2017-12-09 23:49:34.539134: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.001832442 -0.0018321391 -0.00183229 -0.0018323847 -0.0018323279 -0.0018321947 -0.0018322122 -0.0018325783 -0.001833166 -0.001833649 -0.0018336893 -0.0018332946 -0.0018326169 -0.0018318404 -0.001831168][-0.0018321094 -0.00183177 -0.0018319421 -0.0018321036 -0.0018322372 -0.0018324353 -0.0018327595 -0.0018334355 -0.0018341277 -0.0018346845 -0.0018347043 -0.0018342022 -0.0018332383 -0.0018321008 -0.0018310754][-0.0018323244 -0.001831964 -0.0018321815 -0.0018325382 -0.0018330914 -0.0018336624 -0.0018340822 -0.0018346184 -0.0018351679 -0.0018357132 -0.0018358845 -0.0018355148 -0.0018345152 -0.0018330439 -0.0018315695][-0.0018328041 -0.0018325294 -0.0018329554 -0.00183362 -0.0018345244 -0.0018351896 -0.0018353758 -0.0018354524 -0.001835633 -0.0018360467 -0.0018363013 -0.0018361587 -0.0018353619 -0.0018338631 -0.0018320347][-0.0018334589 -0.0018334651 -0.0018342861 -0.0018352715 -0.0018363486 -0.0018368254 -0.0018364871 -0.0018359093 -0.0018357547 -0.0018360442 -0.0018362829 -0.001836354 -0.001835824 -0.0018344185 -0.0018324694][-0.001834173 -0.0018345246 -0.0018357353 -0.0018369579 -0.0018380713 -0.0018382943 -0.0018373557 -0.0018361831 -0.0018358808 -0.0018361878 -0.0018363334 -0.0018364149 -0.0018359493 -0.0018345315 -0.0018326734][-0.0018347851 -0.0018353463 -0.0018367453 -0.0018380942 -0.0018392124 -0.0018391127 -0.0018374141 -0.0018356877 -0.0018353277 -0.0018359226 -0.0018361304 -0.0018362264 -0.0018357013 -0.0018343362 -0.0018326015][-0.001835087 -0.0018356805 -0.0018371325 -0.0018386543 -0.001839805 -0.0018394827 -0.0018371716 -0.0018345673 -0.0018341502 -0.0018350896 -0.0018356767 -0.0018358525 -0.0018353735 -0.0018340452 -0.0018323521][-0.00183502 -0.0018355631 -0.0018371501 -0.0018387564 -0.0018398716 -0.0018395131 -0.0018373062 -0.0018344958 -0.0018338945 -0.0018348546 -0.0018353454 -0.0018353086 -0.0018347178 -0.0018334348 -0.0018318872][-0.0018346836 -0.0018351225 -0.0018366989 -0.0018382909 -0.001839352 -0.0018392334 -0.0018378727 -0.0018359089 -0.0018349955 -0.001835199 -0.0018351214 -0.0018346177 -0.0018337702 -0.001832512 -0.0018311241][-0.0018342644 -0.0018343471 -0.0018354427 -0.0018368855 -0.0018381124 -0.0018385086 -0.0018379708 -0.0018369331 -0.0018360628 -0.0018355288 -0.0018348363 -0.0018339794 -0.0018329337 -0.0018316476 -0.0018304486][-0.001833874 -0.0018335355 -0.0018340668 -0.0018352675 -0.0018364805 -0.001837125 -0.0018369273 -0.0018362398 -0.001835375 -0.0018345672 -0.0018336693 -0.0018327401 -0.0018317316 -0.0018305785 -0.001829633][-0.0018334736 -0.0018331305 -0.0018335113 -0.0018343021 -0.0018351858 -0.001835605 -0.0018353711 -0.0018346533 -0.0018336478 -0.0018326031 -0.0018316463 -0.0018307924 -0.0018299037 -0.0018289946 -0.0018283802][-0.0018332392 -0.0018330199 -0.001833467 -0.0018339256 -0.0018343072 -0.0018342868 -0.0018337934 -0.0018328802 -0.0018317916 -0.0018307096 -0.001829718 -0.0018289046 -0.0018281369 -0.0018275208 -0.0018273279][-0.0018329854 -0.0018328347 -0.0018333006 -0.0018336561 -0.0018336048 -0.0018332192 -0.001832629 -0.0018317117 -0.0018306719 -0.001829721 -0.0018288493 -0.0018280837 -0.0018273939 -0.0018269565 -0.0018269562]]...]
INFO - root - 2017-12-09 23:49:43.133570: step 69510, loss = 0.83, batch loss = 0.70 (9.4 examples/sec; 0.851 sec/batch; 62h:09m:26s remains)
INFO - root - 2017-12-09 23:49:51.836457: step 69520, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.885 sec/batch; 64h:38m:42s remains)
INFO - root - 2017-12-09 23:50:00.299763: step 69530, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 63h:18m:31s remains)
INFO - root - 2017-12-09 23:50:09.057455: step 69540, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 62h:58m:13s remains)
INFO - root - 2017-12-09 23:50:17.599712: step 69550, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 63h:35m:24s remains)
INFO - root - 2017-12-09 23:50:26.252373: step 69560, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.874 sec/batch; 63h:51m:07s remains)
INFO - root - 2017-12-09 23:50:35.003729: step 69570, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.880 sec/batch; 64h:16m:12s remains)
INFO - root - 2017-12-09 23:50:43.611644: step 69580, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.851 sec/batch; 62h:09m:53s remains)
INFO - root - 2017-12-09 23:50:52.382422: step 69590, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 62h:55m:50s remains)
INFO - root - 2017-12-09 23:51:00.843706: step 69600, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.834 sec/batch; 60h:54m:29s remains)
2017-12-09 23:51:01.769036: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0017030088 -0.001396028 -0.00076278427 5.9127342e-05 0.00076150603 0.0010602962 0.00087091571 0.00033992098 -0.00031869626 -0.00089213788 -0.0012953408 -0.0015322582 -0.0016535509 -0.0017146914 -0.0017462957][-0.0013484744 -0.00065591338 0.00057832932 0.0020754109 0.0033017597 0.0037823436 0.003379432 0.0023359731 0.0010635942 -5.1808776e-05 -0.00083823781 -0.0012950037 -0.0015276133 -0.0016426714 -0.0017068986][-0.00044286484 0.0008661981 0.0029667774 0.0053475061 0.0071806726 0.0078037796 0.0070893844 0.005433776 0.0034149662 0.0015574101 0.00013637135 -0.00077072263 -0.0012704023 -0.0015046659 -0.001605358][0.00080418808 0.0029205414 0.0059692073 0.009189168 0.011539543 0.012252394 0.011245044 0.0090330737 0.006315934 0.0036905962 0.0015219162 5.9051672e-06 -0.00090807769 -0.0013690629 -0.0015693052][0.002261993 0.0052216304 0.0091716237 0.013125691 0.015897168 0.016680842 0.015450523 0.012785966 0.0094203437 0.0060135038 0.0030524307 0.00088552607 -0.00047455577 -0.0011834828 -0.0014947887][0.0038224417 0.007601554 0.012307434 0.016791003 0.019824816 0.020664264 0.019336144 0.016381269 0.012484773 0.0083290432 0.0045757126 0.001759797 -2.9822579e-05 -0.00096961187 -0.0013809025][0.0051396848 0.0096449358 0.014995237 0.019883288 0.023081562 0.023964578 0.022622986 0.019528002 0.0152478 0.010465353 0.0060042725 0.0025917199 0.00040788681 -0.0007344858 -0.0012245604][0.0059829871 0.011067716 0.016878596 0.021984607 0.025228327 0.026142241 0.024866745 0.02178142 0.017311655 0.012113874 0.0071498551 0.0033019315 0.00085044454 -0.0003975766 -0.00089040783][0.0063845995 0.011801119 0.017850164 0.023041116 0.026267461 0.027186291 0.025967581 0.022936422 0.018434523 0.013098317 0.0079505378 0.003957314 0.0014658383 0.00026974047 -0.00013843842][0.00643471 0.011970616 0.018077398 0.023252374 0.026420258 0.027320005 0.026142538 0.023203464 0.01882186 0.013630566 0.0086468467 0.0048186411 0.0024982812 0.0014761848 0.0012119279][0.0059843292 0.011413344 0.017355483 0.022365762 0.025414104 0.02627765 0.025159534 0.02240306 0.018363532 0.013647284 0.0091753565 0.0057834443 0.0037741438 0.0029476788 0.0027739978][0.0049935165 0.010024651 0.015566292 0.020253226 0.023101434 0.023905402 0.022874989 0.020402731 0.01689445 0.012909943 0.0092034116 0.0064280923 0.004802925 0.0041525904 0.0040173619][0.0036935494 0.00792507 0.012628546 0.016648851 0.019126641 0.019855695 0.019009676 0.016971635 0.014166229 0.011080153 0.0082775624 0.0062199659 0.0050350348 0.0045750681 0.004473235][0.002331865 0.0055793524 0.0092060985 0.012326552 0.014257149 0.014829141 0.014185474 0.012657374 0.010628432 0.0084807668 0.0065904274 0.005243341 0.0044815573 0.0041874978 0.0040999847][0.000972662 0.0032187151 0.0057374644 0.0079171471 0.0092834551 0.0097029777 0.0092801619 0.0082654627 0.0069631389 0.0056377151 0.0045109279 0.0037313448 0.0032908483 0.003108786 0.003018185]]...]
INFO - root - 2017-12-09 23:51:10.447164: step 69610, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.898 sec/batch; 65h:35m:08s remains)
INFO - root - 2017-12-09 23:51:19.321873: step 69620, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.877 sec/batch; 64h:00m:44s remains)
INFO - root - 2017-12-09 23:51:27.856861: step 69630, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.874 sec/batch; 63h:49m:40s remains)
INFO - root - 2017-12-09 23:51:36.502027: step 69640, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.850 sec/batch; 62h:02m:20s remains)
INFO - root - 2017-12-09 23:51:45.049368: step 69650, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 63h:26m:41s remains)
INFO - root - 2017-12-09 23:51:53.651644: step 69660, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.856 sec/batch; 62h:29m:21s remains)
INFO - root - 2017-12-09 23:52:02.363909: step 69670, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.861 sec/batch; 62h:51m:33s remains)
INFO - root - 2017-12-09 23:52:11.152116: step 69680, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.901 sec/batch; 65h:47m:21s remains)
INFO - root - 2017-12-09 23:52:19.908885: step 69690, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.903 sec/batch; 65h:55m:08s remains)
INFO - root - 2017-12-09 23:52:28.554244: step 69700, loss = 0.82, batch loss = 0.69 (8.7 examples/sec; 0.916 sec/batch; 66h:53m:11s remains)
2017-12-09 23:52:29.457346: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.001977216 0.0020911256 0.0021832325 0.0022501163 0.0022928873 0.0023026527 0.0023248745 0.0023023495 0.0022912184 0.0022752774 0.0023186561 0.0023725205 0.0023864806 0.0023539104 0.0021530367][0.0023127622 0.0024174084 0.0024814988 0.0025419267 0.0026104716 0.0026693637 0.0027342439 0.0027651703 0.0028278795 0.0029117786 0.0030062338 0.0030925348 0.0031351503 0.0030990853 0.0028774096][0.0022791531 0.0023138304 0.0022937488 0.0023014098 0.0023435722 0.0024206797 0.002543312 0.0026697633 0.0028473949 0.0030517359 0.0032457928 0.0033977111 0.00348743 0.0034528556 0.0032384028][0.0020927871 0.001971283 0.0018058229 0.001706917 0.0016679781 0.0017216242 0.0018738528 0.0021127826 0.0024184622 0.002724343 0.0030316827 0.0032740263 0.0034057624 0.0034075435 0.0032182052][0.0018594406 0.0015800778 0.0012891699 0.0010801767 0.00097126269 0.00098815223 0.0011393296 0.0014330073 0.0018426388 0.0022601322 0.0026563373 0.0029481696 0.0031134253 0.0031235241 0.0029429155][0.0014180954 0.0010676509 0.00073537987 0.00049108511 0.00036918197 0.00037160132 0.00052375777 0.0008161132 0.0012429972 0.0017315975 0.0021803081 0.0025196248 0.0027169795 0.0027700989 0.002597224][0.00080433942 0.00048578822 0.00018634449 -1.1996948e-05 -8.7577151e-05 -5.4796226e-05 0.00012282573 0.00042481243 0.00085166388 0.00132662 0.0017654478 0.0021214243 0.0023246729 0.0023604552 0.002193274][9.020674e-05 -0.0001829759 -0.00041678292 -0.000526433 -0.00053258205 -0.00044871983 -0.00023534929 9.52516e-05 0.00052736688 0.00096850179 0.0013746415 0.0017024555 0.0018912778 0.0019045389 0.0017091831][-0.00065667322 -0.00084706733 -0.000991202 -0.0010186909 -0.00096927187 -0.00083100586 -0.00057665736 -0.0002230932 0.00019749964 0.00061905512 0.000990566 0.0012538912 0.0013772148 0.0013361851 0.0011097613][-0.0012568573 -0.0013775922 -0.0014429173 -0.0014028708 -0.0013025642 -0.0011313246 -0.00085716555 -0.0004808919 -4.6683243e-05 0.00034718111 0.00069677958 0.00093797094 0.0010309477 0.00095797435 0.0007234771][-0.0015602936 -0.0016126228 -0.0016148197 -0.0015362807 -0.0014048382 -0.0011959316 -0.00089743041 -0.00050579139 -5.8286823e-05 0.00032236183 0.00063449156 0.00085836777 0.00094729604 0.00090179534 0.00071993528][-0.0016468796 -0.0016654816 -0.0016349402 -0.0015378176 -0.001386907 -0.00115365 -0.00084379432 -0.00044705765 -4.6628993e-06 0.00038698839 0.00068232964 0.0009069232 0.00098715222 0.0010037344 0.00091240241][-0.0016659544 -0.0016705259 -0.0016293325 -0.0015137043 -0.0013392458 -0.0010986303 -0.00079102663 -0.00039654947 4.5560882e-05 0.00047478906 0.00078584871 0.0010321076 0.001175773 0.0012572053 0.0012596805][-0.0016835216 -0.0016845217 -0.0016497425 -0.0015442602 -0.0013706138 -0.0011265164 -0.00082033488 -0.00043401378 1.6765203e-05 0.00045784202 0.00079779036 0.0010803059 0.0012396859 0.001410066 0.0014866182][-0.0016860173 -0.0016887669 -0.0016609299 -0.0015830292 -0.0014399705 -0.0012095459 -0.00089444005 -0.00051380112 -6.7718094e-05 0.00040351076 0.00079726626 0.0010991009 0.0013153659 0.0014888138 0.0015525297]]...]
INFO - root - 2017-12-09 23:52:38.005548: step 69710, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.888 sec/batch; 64h:47m:55s remains)
INFO - root - 2017-12-09 23:52:46.816263: step 69720, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.888 sec/batch; 64h:51m:16s remains)
INFO - root - 2017-12-09 23:52:55.282132: step 69730, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.874 sec/batch; 63h:48m:22s remains)
INFO - root - 2017-12-09 23:53:04.021070: step 69740, loss = 0.82, batch loss = 0.69 (9.8 examples/sec; 0.817 sec/batch; 59h:37m:40s remains)
INFO - root - 2017-12-09 23:53:12.644066: step 69750, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.873 sec/batch; 63h:41m:11s remains)
INFO - root - 2017-12-09 23:53:21.322701: step 69760, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.856 sec/batch; 62h:26m:56s remains)
INFO - root - 2017-12-09 23:53:30.051384: step 69770, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.875 sec/batch; 63h:52m:01s remains)
INFO - root - 2017-12-09 23:53:38.649053: step 69780, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.856 sec/batch; 62h:28m:31s remains)
INFO - root - 2017-12-09 23:53:47.438093: step 69790, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.872 sec/batch; 63h:36m:52s remains)
INFO - root - 2017-12-09 23:53:56.055080: step 69800, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 63h:35m:27s remains)
2017-12-09 23:53:57.010356: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0018083263 -0.0018127841 -0.0018192538 -0.0018255588 -0.0018302109 -0.0018329903 -0.0018343063 -0.0018346886 -0.0018344669 -0.0018342313 -0.001834158 -0.0018339901 -0.0018334155 -0.0018315245 -0.0018282544][-0.0018069485 -0.0018110423 -0.0018172235 -0.0018232435 -0.0018276609 -0.001830467 -0.0018319536 -0.0018326632 -0.0018327874 -0.0018328086 -0.0018328181 -0.0018325804 -0.0018318623 -0.0018297906 -0.0018264547][-0.0018054417 -0.0018088863 -0.0018140418 -0.0018192168 -0.0018231029 -0.0018257297 -0.0018273903 -0.0018286152 -0.0018294565 -0.0018302312 -0.0018308205 -0.0018309589 -0.0018303491 -0.0018281783 -0.0018246527][-0.0018036499 -0.0018060951 -0.0018099643 -0.0018139175 -0.0018169293 -0.0018191194 -0.0018208618 -0.0018227161 -0.0018245887 -0.0018264997 -0.0018281147 -0.0018289182 -0.0018285681 -0.001826323 -0.0018224946][-0.0018020612 -0.0018032265 -0.0018057962 -0.0018085004 -0.0018104832 -0.0018119364 -0.0018135047 -0.0018158461 -0.0018186737 -0.001821717 -0.0018244903 -0.0018260892 -0.0018260467 -0.0018237833 -0.0018197675][-0.0018006088 -0.0018006179 -0.0018018198 -0.001803261 -0.0018042739 -0.001804995 -0.0018063132 -0.0018090258 -0.0018127466 -0.0018170032 -0.0018209176 -0.001823292 -0.0018235707 -0.001821324 -0.0018170788][-0.0017995564 -0.0017985599 -0.0017986509 -0.0017990385 -0.0017991565 -0.0017990938 -0.0017999792 -0.0018027467 -0.0018070525 -0.0018122437 -0.0018170992 -0.0018202007 -0.0018207774 -0.0018186041 -0.0018142541][-0.0017987773 -0.0017970291 -0.0017963075 -0.0017959408 -0.0017954593 -0.0017949386 -0.0017955698 -0.0017982391 -0.0018026704 -0.0018081161 -0.0018133001 -0.0018167235 -0.0018173021 -0.0018151925 -0.0018109782][-0.0017982378 -0.0017960662 -0.0017950583 -0.001794296 -0.0017934757 -0.0017927859 -0.0017933558 -0.0017957871 -0.0017997809 -0.0018047838 -0.0018096901 -0.0018129084 -0.0018132312 -0.0018112004 -0.0018074574][-0.0017977869 -0.0017956984 -0.0017947225 -0.0017938525 -0.0017929769 -0.0017923857 -0.0017929997 -0.0017951165 -0.0017984648 -0.0018025953 -0.0018066358 -0.0018092595 -0.0018093338 -0.0018074675 -0.0018042986][-0.0017976267 -0.0017956855 -0.0017948456 -0.0017940828 -0.0017932999 -0.0017928631 -0.0017934017 -0.0017950714 -0.0017976803 -0.0018008369 -0.0018039399 -0.0018058954 -0.0018058283 -0.0018042588 -0.0018017521][-0.0017975396 -0.0017956367 -0.001795055 -0.0017944911 -0.0017938481 -0.001793506 -0.0017938996 -0.0017950657 -0.0017968502 -0.0017989348 -0.0018009951 -0.0018023478 -0.0018023264 -0.0018012529 -0.0017995208][-0.0017976119 -0.0017956996 -0.0017952265 -0.0017948688 -0.0017944204 -0.0017942039 -0.0017944876 -0.0017952478 -0.00179632 -0.0017974986 -0.0017986259 -0.0017994269 -0.0017994461 -0.0017988679 -0.0017978811][-0.0017978202 -0.0017958442 -0.0017954315 -0.0017952695 -0.0017950392 -0.0017949636 -0.0017951648 -0.0017956321 -0.0017962437 -0.0017968907 -0.001797471 -0.0017979085 -0.0017979587 -0.0017976792 -0.0017971455][-0.0017978904 -0.0017960063 -0.0017955669 -0.0017955494 -0.001795498 -0.0017955231 -0.00179566 -0.0017959272 -0.0017962843 -0.0017966414 -0.0017969431 -0.0017971746 -0.0017972512 -0.0017971547 -0.0017969004]]...]
INFO - root - 2017-12-09 23:54:05.681489: step 69810, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.886 sec/batch; 64h:38m:19s remains)
INFO - root - 2017-12-09 23:54:14.541975: step 69820, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.854 sec/batch; 62h:18m:52s remains)
INFO - root - 2017-12-09 23:54:22.997175: step 69830, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.881 sec/batch; 64h:16m:43s remains)
INFO - root - 2017-12-09 23:54:31.604150: step 69840, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.856 sec/batch; 62h:25m:26s remains)
INFO - root - 2017-12-09 23:54:40.247320: step 69850, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.873 sec/batch; 63h:42m:28s remains)
INFO - root - 2017-12-09 23:54:48.904736: step 69860, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.859 sec/batch; 62h:41m:13s remains)
INFO - root - 2017-12-09 23:54:57.652971: step 69870, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.876 sec/batch; 63h:55m:22s remains)
INFO - root - 2017-12-09 23:55:06.382989: step 69880, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.852 sec/batch; 62h:09m:06s remains)
INFO - root - 2017-12-09 23:55:15.024563: step 69890, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 63h:19m:56s remains)
INFO - root - 2017-12-09 23:55:23.495270: step 69900, loss = 0.82, batch loss = 0.70 (9.3 examples/sec; 0.856 sec/batch; 62h:27m:03s remains)
2017-12-09 23:55:24.448770: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0017151523 -0.0016981994 -0.0016871487 -0.001684044 -0.0016916363 -0.001711054 -0.0017400347 -0.0017705978 -0.0017931763 -0.0017986054 -0.0017800994 -0.0017336467 -0.0016673402 -0.0016001685 -0.0015568135][-0.0017682998 -0.0017682163 -0.0017695178 -0.0017718746 -0.0017769982 -0.001785986 -0.0017994836 -0.0018137554 -0.0018236615 -0.0018241528 -0.0018097868 -0.0017745526 -0.0017211053 -0.0016630177 -0.0016207064][-0.0017883906 -0.0018006518 -0.0018107807 -0.0018174665 -0.0018217111 -0.0018255742 -0.0018305111 -0.0018355378 -0.0018391407 -0.0018387988 -0.0018309352 -0.001809982 -0.001775431 -0.0017337794 -0.0016989773][-0.0017991284 -0.0018148855 -0.0018266761 -0.0018337645 -0.0018373193 -0.0018390021 -0.0018404617 -0.0018422782 -0.001843789 -0.0018442755 -0.0018417063 -0.0018327475 -0.0018160141 -0.0017935344 -0.0017718623][-0.0018107431 -0.0018234018 -0.0018320484 -0.0018370547 -0.0018393476 -0.0018399268 -0.0018399159 -0.0018404581 -0.0018416853 -0.0018432038 -0.0018438912 -0.0018420807 -0.0018368638 -0.0018285669 -0.0018187932][-0.001824255 -0.001830946 -0.0018353112 -0.0018373668 -0.0018373865 -0.0018356815 -0.0018339432 -0.0018333918 -0.0018349679 -0.0018379587 -0.0018410311 -0.0018428789 -0.0018428537 -0.0018413294 -0.0018386471][-0.0018351836 -0.0018372637 -0.0018386331 -0.0018386875 -0.0018370665 -0.0018334057 -0.0018288539 -0.0018259074 -0.0018276174 -0.001832393 -0.0018375638 -0.0018411512 -0.0018428891 -0.0018436024 -0.0018434285][-0.0018401638 -0.0018400415 -0.0018402472 -0.0018400762 -0.0018387794 -0.0018361405 -0.001832379 -0.0018292447 -0.0018290736 -0.0018325206 -0.0018368649 -0.0018402468 -0.0018421635 -0.0018432325 -0.00184359][-0.0018414536 -0.0018410101 -0.0018411946 -0.0018411548 -0.0018405417 -0.0018393071 -0.0018376112 -0.001836202 -0.0018357015 -0.0018368745 -0.001839066 -0.0018407858 -0.001841689 -0.0018421393 -0.0018423522][-0.0018414083 -0.0018409386 -0.0018412949 -0.0018415506 -0.0018413913 -0.0018410106 -0.0018405535 -0.001840153 -0.0018401353 -0.0018405478 -0.0018413153 -0.0018418684 -0.0018420346 -0.0018417296 -0.0018413985][-0.0018409507 -0.0018403665 -0.0018404871 -0.0018405626 -0.0018405813 -0.0018406537 -0.0018407633 -0.0018410288 -0.0018413371 -0.0018417533 -0.0018422303 -0.0018423022 -0.0018420316 -0.0018415083 -0.0018408027][-0.0018403521 -0.0018398145 -0.0018399301 -0.001840034 -0.0018400969 -0.001840323 -0.0018406314 -0.0018410994 -0.0018415595 -0.0018420373 -0.0018423556 -0.0018422005 -0.0018416941 -0.0018411009 -0.0018403935][-0.0018399142 -0.001839387 -0.0018394041 -0.0018395158 -0.0018397464 -0.00184008 -0.0018403427 -0.0018406685 -0.0018408762 -0.0018411624 -0.0018412864 -0.0018412473 -0.0018409936 -0.0018405103 -0.0018399304][-0.0018400466 -0.0018393237 -0.0018392256 -0.001839262 -0.0018393503 -0.001839709 -0.0018400187 -0.0018402489 -0.0018403312 -0.0018402314 -0.0018401009 -0.0018399756 -0.0018398204 -0.0018395656 -0.0018392687][-0.0018400435 -0.001839205 -0.0018389378 -0.0018388591 -0.0018387958 -0.0018390273 -0.0018393156 -0.0018395256 -0.0018395742 -0.0018394742 -0.0018392735 -0.0018391566 -0.0018391391 -0.0018390584 -0.0018389309]]...]
INFO - root - 2017-12-09 23:55:33.070272: step 69910, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.841 sec/batch; 61h:21m:31s remains)
INFO - root - 2017-12-09 23:55:41.711412: step 69920, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.861 sec/batch; 62h:48m:14s remains)
INFO - root - 2017-12-09 23:55:50.130427: step 69930, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.879 sec/batch; 64h:04m:35s remains)
INFO - root - 2017-12-09 23:55:58.700702: step 69940, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.855 sec/batch; 62h:20m:49s remains)
INFO - root - 2017-12-09 23:56:07.268145: step 69950, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.843 sec/batch; 61h:30m:43s remains)
INFO - root - 2017-12-09 23:56:15.971508: step 69960, loss = 0.82, batch loss = 0.69 (9.7 examples/sec; 0.828 sec/batch; 60h:22m:39s remains)
INFO - root - 2017-12-09 23:56:24.590239: step 69970, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.866 sec/batch; 63h:09m:06s remains)
INFO - root - 2017-12-09 23:56:33.229840: step 69980, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.842 sec/batch; 61h:22m:11s remains)
INFO - root - 2017-12-09 23:56:41.865975: step 69990, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.858 sec/batch; 62h:34m:07s remains)
INFO - root - 2017-12-09 23:56:50.485448: step 70000, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.894 sec/batch; 65h:12m:39s remains)
2017-12-09 23:56:51.402891: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0015988199 -0.0015250202 -0.0014470529 -0.0013592702 -0.0012632608 -0.0011700679 -0.0010980773 -0.0010643732 -0.0010546017 -0.0010595298 -0.0010794161 -0.001123289 -0.0011978582 -0.0013072502 -0.0014520857][-0.0015196027 -0.0014431509 -0.0013731524 -0.0012902713 -0.0011927714 -0.0011001929 -0.0010361238 -0.0010085545 -0.0010065788 -0.0010157535 -0.0010321399 -0.0010628365 -0.0011270272 -0.0012310696 -0.0013828211][-0.0014329951 -0.0013681897 -0.0013296329 -0.0012788163 -0.0012066794 -0.0011383151 -0.0010953411 -0.0010838156 -0.0010938827 -0.0011074666 -0.0011213894 -0.0011420371 -0.0011866172 -0.0012663605 -0.0013985266][-0.0013596893 -0.001296229 -0.0012802517 -0.0012680074 -0.0012368805 -0.0012086181 -0.0011993777 -0.0012096865 -0.0012297808 -0.0012401527 -0.00124435 -0.0012462515 -0.0012652895 -0.0013187267 -0.0014250004][-0.0013142797 -0.001243926 -0.0012435284 -0.0012682197 -0.0012852971 -0.0013069024 -0.0013343459 -0.0013623312 -0.0013834573 -0.0013824453 -0.0013663021 -0.0013426763 -0.0013355628 -0.0013662883 -0.001451941][-0.0013359542 -0.0012593117 -0.001263128 -0.0013108341 -0.0013662369 -0.0014257437 -0.0014785868 -0.0015179452 -0.0015385853 -0.0015298435 -0.001500917 -0.001460038 -0.0014332175 -0.0014446054 -0.0015073456][-0.0014170487 -0.0013470741 -0.001350989 -0.0014084375 -0.0014796528 -0.0015521824 -0.0016085454 -0.0016439571 -0.0016577581 -0.0016459082 -0.0016169877 -0.0015781694 -0.0015489381 -0.0015506187 -0.0015934135][-0.0015397691 -0.0014814408 -0.0014827574 -0.0015365379 -0.0016050006 -0.0016712602 -0.0017183099 -0.0017457283 -0.001755651 -0.0017478125 -0.0017293692 -0.0017053883 -0.0016858624 -0.0016841246 -0.0017050874][-0.0016586634 -0.0016206421 -0.0016247018 -0.0016682231 -0.0017176985 -0.0017600432 -0.0017856517 -0.001799048 -0.0018033871 -0.0017995915 -0.0017910118 -0.0017807968 -0.0017721537 -0.0017710224 -0.0017786298][-0.0017483291 -0.0017296619 -0.0017365254 -0.0017640411 -0.0017905816 -0.0018079538 -0.0018148873 -0.0018176307 -0.0018183063 -0.0018172343 -0.0018154394 -0.0018136125 -0.0018122921 -0.00181222 -0.0018133323][-0.0017983658 -0.0017912849 -0.0017954989 -0.0018069582 -0.001816595 -0.001820869 -0.0018209071 -0.0018200051 -0.0018188744 -0.0018175374 -0.0018163065 -0.0018154603 -0.0018151593 -0.001815608 -0.0018165453][-0.0018212813 -0.0018185576 -0.0018181576 -0.001818993 -0.0018199327 -0.0018198586 -0.0018193049 -0.0018186198 -0.0018175265 -0.0018167486 -0.0018161577 -0.0018157181 -0.0018157543 -0.0018164732 -0.0018174133][-0.0018237543 -0.0018221095 -0.0018210462 -0.0018202205 -0.0018198447 -0.0018195204 -0.0018189923 -0.0018183065 -0.0018175227 -0.0018171527 -0.0018168173 -0.0018165867 -0.00181672 -0.0018173307 -0.0018182689][-0.0018228434 -0.0018215067 -0.0018207029 -0.0018199708 -0.0018194255 -0.0018190363 -0.0018185974 -0.0018178808 -0.0018171581 -0.0018167961 -0.0018165995 -0.0018166474 -0.0018168652 -0.0018176831 -0.0018187099][-0.0018227112 -0.0018214504 -0.0018204938 -0.0018197903 -0.0018192763 -0.0018188074 -0.0018183948 -0.0018180044 -0.0018174413 -0.0018169562 -0.0018167341 -0.0018168068 -0.0018171644 -0.001818174 -0.0018194186]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-momentum-lr0.0001-clip-zeroinit-conv1-3init-from-scratch/model.ckpt-70000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-momentum-lr0.0001-clip-zeroinit-conv1-3init-from-scratch/model.ckpt-70000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-09 23:57:00.847807: step 70010, loss = 0.82, batch loss = 0.69 (8.2 examples/sec; 0.970 sec/batch; 70h:45m:22s remains)
INFO - root - 2017-12-09 23:57:09.395188: step 70020, loss = 0.81, batch loss = 0.68 (9.5 examples/sec; 0.840 sec/batch; 61h:14m:02s remains)
INFO - root - 2017-12-09 23:57:17.924903: step 70030, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.889 sec/batch; 64h:48m:02s remains)
INFO - root - 2017-12-09 23:57:26.562724: step 70040, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.865 sec/batch; 63h:04m:13s remains)
INFO - root - 2017-12-09 23:57:35.124467: step 70050, loss = 0.82, batch loss = 0.69 (9.8 examples/sec; 0.813 sec/batch; 59h:15m:09s remains)
INFO - root - 2017-12-09 23:57:43.829183: step 70060, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.851 sec/batch; 62h:02m:00s remains)
INFO - root - 2017-12-09 23:57:52.388492: step 70070, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.861 sec/batch; 62h:45m:19s remains)
INFO - root - 2017-12-09 23:58:00.766414: step 70080, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.844 sec/batch; 61h:33m:19s remains)
INFO - root - 2017-12-09 23:58:09.338163: step 70090, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.861 sec/batch; 62h:46m:22s remains)
INFO - root - 2017-12-09 23:58:17.824531: step 70100, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.894 sec/batch; 65h:08m:58s remains)
2017-12-09 23:58:18.814151: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0018314859 -0.0018308806 -0.0018292993 -0.0018246182 -0.001815144 -0.001802736 -0.0017903472 -0.0017807182 -0.0017747617 -0.0017714073 -0.0017692985 -0.0017679579 -0.0017678976 -0.0017709107 -0.0017783069][-0.0018301654 -0.0018295931 -0.0018280249 -0.0018232004 -0.0018125487 -0.0017969472 -0.0017797839 -0.0017655696 -0.0017570329 -0.0017533717 -0.0017524522 -0.00175265 -0.0017537796 -0.0017571537 -0.0017638751][-0.0018298073 -0.0018294147 -0.0018280984 -0.0018237163 -0.0018131211 -0.0017960885 -0.0017758956 -0.0017578469 -0.0017470464 -0.0017431861 -0.0017441449 -0.0017475213 -0.0017518986 -0.0017575932 -0.0017647615][-0.0018293289 -0.0018288816 -0.00182809 -0.0018245843 -0.0018150946 -0.0017987445 -0.0017782276 -0.0017587822 -0.0017468198 -0.0017435597 -0.0017466705 -0.001753362 -0.0017611204 -0.0017694193 -0.0017775927][-0.0018289557 -0.0018283349 -0.0018279974 -0.0018258851 -0.001818575 -0.0018046545 -0.0017858839 -0.0017672103 -0.0017552054 -0.0017528083 -0.0017580538 -0.001767278 -0.0017773146 -0.0017869879 -0.0017951033][-0.0018286773 -0.0018277479 -0.0018277179 -0.0018269365 -0.0018225196 -0.001812191 -0.0017967321 -0.0017803077 -0.001769113 -0.0017669484 -0.0017726879 -0.0017828151 -0.0017936699 -0.0018034216 -0.0018101822][-0.0018285086 -0.0018271883 -0.0018272037 -0.0018271047 -0.0018250728 -0.0018188545 -0.0018080203 -0.0017953294 -0.0017857276 -0.0017834328 -0.0017882374 -0.0017971267 -0.0018065419 -0.0018144784 -0.0018195711][-0.0018283915 -0.0018268002 -0.001826747 -0.001827075 -0.0018264398 -0.0018230812 -0.0018162978 -0.0018076292 -0.0018005468 -0.0017984444 -0.001801873 -0.0018084756 -0.0018152432 -0.0018201709 -0.0018227547][-0.0018282782 -0.0018264477 -0.0018264428 -0.0018270196 -0.0018273733 -0.0018261187 -0.0018225176 -0.0018173843 -0.0018126094 -0.0018107912 -0.0018132783 -0.0018179418 -0.0018219284 -0.0018240674 -0.001824251][-0.0018284216 -0.0018263422 -0.0018262089 -0.0018268797 -0.001827899 -0.001828261 -0.0018272274 -0.0018248215 -0.001822083 -0.0018202922 -0.0018208446 -0.0018234904 -0.0018257747 -0.0018267783 -0.0018261686][-0.0018285543 -0.0018266093 -0.0018261723 -0.0018267517 -0.0018279169 -0.0018292071 -0.0018300422 -0.0018298558 -0.0018288691 -0.0018276088 -0.0018271361 -0.0018277725 -0.0018284082 -0.0018282852 -0.0018274748][-0.0018286484 -0.0018270541 -0.0018265221 -0.0018267328 -0.0018276233 -0.0018289104 -0.0018303357 -0.0018312224 -0.0018313896 -0.0018306448 -0.0018298669 -0.0018295831 -0.0018294457 -0.0018290513 -0.0018283564][-0.0018287277 -0.0018274101 -0.0018269679 -0.001827011 -0.0018274838 -0.0018282172 -0.0018294201 -0.0018304214 -0.0018310419 -0.0018309435 -0.0018303989 -0.0018296896 -0.0018291144 -0.0018286182 -0.0018281355][-0.0018291516 -0.0018279067 -0.0018275385 -0.0018274583 -0.0018275556 -0.0018279171 -0.0018286043 -0.0018291989 -0.0018296988 -0.0018298618 -0.0018296526 -0.0018290568 -0.0018283963 -0.0018278835 -0.0018274463][-0.0018292334 -0.0018282149 -0.001827911 -0.0018278037 -0.0018277389 -0.0018277323 -0.0018280299 -0.0018281136 -0.0018282819 -0.0018283627 -0.0018283012 -0.0018279749 -0.0018274967 -0.001827131 -0.0018267299]]...]
INFO - root - 2017-12-09 23:58:27.245334: step 70110, loss = 0.83, batch loss = 0.70 (9.3 examples/sec; 0.860 sec/batch; 62h:42m:12s remains)
INFO - root - 2017-12-09 23:58:35.877400: step 70120, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.879 sec/batch; 64h:05m:11s remains)
INFO - root - 2017-12-09 23:58:44.420935: step 70130, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.865 sec/batch; 63h:01m:48s remains)
INFO - root - 2017-12-09 23:58:53.073117: step 70140, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 63h:30m:12s remains)
INFO - root - 2017-12-09 23:59:01.819674: step 70150, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.856 sec/batch; 62h:20m:47s remains)
INFO - root - 2017-12-09 23:59:10.437348: step 70160, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.865 sec/batch; 63h:02m:41s remains)
INFO - root - 2017-12-09 23:59:19.336660: step 70170, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 63h:12m:21s remains)
INFO - root - 2017-12-09 23:59:28.078424: step 70180, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.865 sec/batch; 63h:00m:46s remains)
INFO - root - 2017-12-09 23:59:36.744146: step 70190, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.851 sec/batch; 61h:59m:51s remains)
INFO - root - 2017-12-09 23:59:45.352619: step 70200, loss = 0.83, batch loss = 0.70 (9.0 examples/sec; 0.886 sec/batch; 64h:34m:58s remains)
2017-12-09 23:59:46.309432: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0018301258 -0.0018273415 -0.0017954428 -0.0017363491 -0.0016670052 -0.0016159981 -0.0015021358 -0.0011932373 -0.0007940823 -0.00047595496 -0.00046407792 -0.00073089125 -0.0011199564 -0.0014432963 -0.0016536157][-0.0018260278 -0.0017804655 -0.0016460777 -0.0014520605 -0.0012688055 -0.0011503835 -0.00099124957 -0.00060922792 -0.00014042784 0.00025574665 0.00025985937 -8.8903354e-05 -0.00064647931 -0.00114313 -0.001490979][-0.0018005043 -0.0016506243 -0.0012953702 -0.00081819843 -0.00040602835 -0.00018642726 -6.6239038e-05 0.00019674574 0.00048867485 0.00075771764 0.000680792 0.00030929653 -0.00029795989 -0.00087508943 -0.001318974][-0.0017453677 -0.0014287096 -0.00076210662 8.7803928e-05 0.00079696171 0.0011360621 0.0011419466 0.001096713 0.00098103459 0.00092418946 0.00068514782 0.00031969964 -0.00022407621 -0.00075875525 -0.0012160164][-0.0016951126 -0.0012307961 -0.00030142232 0.00086540065 0.001823428 0.0022369553 0.0020920625 0.0016921248 0.0011366504 0.00072316115 0.0003377524 1.3921293e-05 -0.00040186686 -0.00082109543 -0.0012201206][-0.0016786377 -0.0011695204 -0.0001658618 0.0010791434 0.0020794882 0.0024606483 0.002204244 0.0016021881 0.00082654238 0.0002348345 -0.00019572116 -0.00045726227 -0.00074004882 -0.0010339257 -0.0013367616][-0.0017025793 -0.0012666516 -0.00040053343 0.0006690271 0.0015044716 0.0017672438 0.00145972 0.00083899766 9.9361292e-05 -0.00044482818 -0.00078827189 -0.00095217425 -0.0011133879 -0.0012970616 -0.0015031882][-0.001740003 -0.0014321592 -0.000821606 -7.6987664e-05 0.00048513396 0.000613612 0.0003346774 -0.00015488314 -0.00069505372 -0.0010692634 -0.001274826 -0.0013494263 -0.0014252677 -0.001529065 -0.0016549756][-0.0017745213 -0.0015875023 -0.0012197618 -0.00078159093 -0.0004691272 -0.0004351458 -0.00064341491 -0.00096333516 -0.0012879217 -0.0014953308 -0.0015894254 -0.0016079043 -0.0016324023 -0.0016833734 -0.0017517946][-0.001802566 -0.0017084613 -0.0015199338 -0.0012983815 -0.0011512747 -0.0011602887 -0.0012895862 -0.0014626232 -0.0016214951 -0.0017142635 -0.0017474546 -0.0017461212 -0.0017500288 -0.0017712369 -0.0017995101][-0.0018191463 -0.0017834181 -0.001707613 -0.0016175358 -0.0015597485 -0.0015721472 -0.0016327178 -0.0017058016 -0.0017667504 -0.0017992296 -0.0018090112 -0.0018072224 -0.0018066724 -0.0018120744 -0.0018116439][-0.0018248768 -0.0018159233 -0.0017951363 -0.0017687854 -0.0017512448 -0.0017567518 -0.0017767316 -0.001798891 -0.0018155325 -0.0018236401 -0.0018263224 -0.0018267877 -0.001827226 -0.0018255231 -0.0018038076][-0.0018246912 -0.0018231338 -0.001820505 -0.0018169922 -0.0018140883 -0.0018150122 -0.0018184719 -0.0018226542 -0.0018257083 -0.0018275026 -0.0018289274 -0.001830286 -0.0018311082 -0.0018250655 -0.0017872831][-0.0018237239 -0.001822865 -0.0018228103 -0.001823056 -0.0018234336 -0.0018239565 -0.0018245322 -0.001825479 -0.0018263911 -0.0018271445 -0.0018280884 -0.0018291931 -0.0018299004 -0.0018222713 -0.0017788375][-0.0018234471 -0.0018225174 -0.0018224696 -0.0018226261 -0.0018229074 -0.0018232509 -0.0018234458 -0.0018239637 -0.0018245833 -0.0018252586 -0.0018262455 -0.0018274168 -0.0018283131 -0.001820783 -0.001781307]]...]
INFO - root - 2017-12-09 23:59:54.812216: step 70210, loss = 0.82, batch loss = 0.69 (8.8 examples/sec; 0.911 sec/batch; 66h:21m:36s remains)
INFO - root - 2017-12-10 00:00:03.500829: step 70220, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.901 sec/batch; 65h:37m:38s remains)
INFO - root - 2017-12-10 00:00:11.948912: step 70230, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.861 sec/batch; 62h:44m:06s remains)
INFO - root - 2017-12-10 00:00:20.775895: step 70240, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.860 sec/batch; 62h:37m:58s remains)
INFO - root - 2017-12-10 00:00:29.426546: step 70250, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.880 sec/batch; 64h:06m:43s remains)
INFO - root - 2017-12-10 00:00:38.195444: step 70260, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.863 sec/batch; 62h:53m:31s remains)
INFO - root - 2017-12-10 00:00:46.864224: step 70270, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.880 sec/batch; 64h:05m:53s remains)
INFO - root - 2017-12-10 00:00:55.558534: step 70280, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.839 sec/batch; 61h:08m:08s remains)
INFO - root - 2017-12-10 00:01:03.966022: step 70290, loss = 0.81, batch loss = 0.68 (9.5 examples/sec; 0.845 sec/batch; 61h:32m:38s remains)
INFO - root - 2017-12-10 00:01:12.517117: step 70300, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.883 sec/batch; 64h:17m:33s remains)
2017-12-10 00:01:13.344981: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0018242523 -0.0018243938 -0.0018256427 -0.0018271087 -0.0018284647 -0.00182964 -0.0018303466 -0.0018308908 -0.0018311412 -0.001830786 -0.001829678 -0.0018281268 -0.0018255134 -0.0018223905 -0.0018187405][-0.001822045 -0.001821915 -0.0018227543 -0.0018238025 -0.001824672 -0.001825356 -0.0018256435 -0.0018258273 -0.0018258819 -0.0018255343 -0.001824696 -0.0018235674 -0.0018214697 -0.001819008 -0.0018159966][-0.0018205477 -0.001820005 -0.0018202257 -0.0018205584 -0.0018207649 -0.0018208569 -0.0018207208 -0.0018205758 -0.0018204773 -0.0018201466 -0.001819637 -0.0018189129 -0.0018173862 -0.0018155563 -0.0018133069][-0.0018174407 -0.0018165918 -0.0018165961 -0.0018165584 -0.0018163648 -0.0018161199 -0.0018157185 -0.0018153293 -0.0018150504 -0.0018148188 -0.0018146499 -0.0018142986 -0.0018133244 -0.0018121296 -0.0018106014][-0.0018138282 -0.0018127887 -0.0018125159 -0.0018122699 -0.0018119176 -0.001811512 -0.0018110025 -0.0018105847 -0.0018103004 -0.0018101963 -0.0018103082 -0.0018103275 -0.0018098919 -0.001809292 -0.0018084742][-0.0018095785 -0.001808423 -0.0018082532 -0.0018080941 -0.0018077911 -0.0018074102 -0.0018069508 -0.0018065242 -0.0018062341 -0.0018062067 -0.0018064888 -0.0018068142 -0.0018068895 -0.0018068473 -0.0018066189][-0.0018059665 -0.0018046679 -0.0018045807 -0.0018045764 -0.001804465 -0.0018042463 -0.0018038938 -0.0018035177 -0.0018032419 -0.0018032361 -0.0018035963 -0.0018041345 -0.0018045793 -0.0018049304 -0.0018051475][-0.0018035695 -0.0018021076 -0.0018019854 -0.00180204 -0.0018020533 -0.0018019883 -0.0018017734 -0.0018015343 -0.0018013387 -0.0018013982 -0.0018018401 -0.0018025365 -0.0018032031 -0.0018037721 -0.0018042674][-0.0018018788 -0.0018004188 -0.0018003946 -0.0018005688 -0.0018006832 -0.0018007077 -0.0018005662 -0.0018003802 -0.0018002463 -0.0018003894 -0.0018009224 -0.0018017512 -0.0018025526 -0.0018032655 -0.0018039015][-0.001800725 -0.00179939 -0.0017994209 -0.0017996081 -0.0017997493 -0.0017997862 -0.0017996698 -0.001799526 -0.0017994639 -0.0017997141 -0.0018003704 -0.0018013264 -0.0018022596 -0.0018030913 -0.0018037948][-0.0017996974 -0.0017984429 -0.0017985143 -0.0017987337 -0.0017989085 -0.0017989893 -0.0017989409 -0.0017989047 -0.0017989548 -0.0017993377 -0.0018000926 -0.0018011212 -0.0018021635 -0.001803088 -0.0018038156][-0.0017990557 -0.0017977696 -0.0017978395 -0.0017980821 -0.0017983203 -0.0017984814 -0.0017985674 -0.0017986367 -0.0017987855 -0.0017992458 -0.0018000265 -0.0018010506 -0.0018021134 -0.0018030745 -0.0018038247][-0.0017990969 -0.0017978136 -0.0017979024 -0.0017981665 -0.001798438 -0.0017986337 -0.0017987732 -0.0017989047 -0.0017990912 -0.001799532 -0.0018002501 -0.0018012285 -0.0018022454 -0.0018031769 -0.0018038964][-0.0017999788 -0.0017986404 -0.0017986818 -0.0017989314 -0.0017991806 -0.0017993691 -0.0017994987 -0.001799608 -0.0017997719 -0.0018001547 -0.0018007891 -0.0018016595 -0.001802558 -0.001803385 -0.0018040111][-0.0018012642 -0.0017998775 -0.0017998035 -0.0018000324 -0.0018002647 -0.00180046 -0.0018005765 -0.0018006597 -0.0018007869 -0.0018010752 -0.0018015596 -0.0018022486 -0.0018029676 -0.0018036221 -0.0018040971]]...]
INFO - root - 2017-12-10 00:01:21.933942: step 70310, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.880 sec/batch; 64h:05m:57s remains)
INFO - root - 2017-12-10 00:01:30.573254: step 70320, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.852 sec/batch; 62h:03m:05s remains)
INFO - root - 2017-12-10 00:01:39.241779: step 70330, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 62h:48m:34s remains)
INFO - root - 2017-12-10 00:01:47.825410: step 70340, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.863 sec/batch; 62h:50m:05s remains)
INFO - root - 2017-12-10 00:01:56.527429: step 70350, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 63h:26m:38s remains)
INFO - root - 2017-12-10 00:02:05.398377: step 70360, loss = 0.81, batch loss = 0.68 (8.3 examples/sec; 0.964 sec/batch; 70h:13m:46s remains)
INFO - root - 2017-12-10 00:02:14.128340: step 70370, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.881 sec/batch; 64h:10m:50s remains)
INFO - root - 2017-12-10 00:02:22.885165: step 70380, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.889 sec/batch; 64h:41m:56s remains)
INFO - root - 2017-12-10 00:02:31.501237: step 70390, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.865 sec/batch; 62h:59m:05s remains)
INFO - root - 2017-12-10 00:02:39.836324: step 70400, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.849 sec/batch; 61h:47m:52s remains)
2017-12-10 00:02:40.695653: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0018279306 -0.0018292211 -0.0018313334 -0.0018335131 -0.0018356207 -0.0018374784 -0.0018388889 -0.0018397252 -0.0018400936 -0.0018397791 -0.0018385184 -0.001836647 -0.0018345959 -0.0018327563 -0.0018310567][-0.001828869 -0.001830469 -0.0018328361 -0.0018350788 -0.0018368697 -0.0018381439 -0.0018388388 -0.0018390702 -0.0018390741 -0.0018387198 -0.0018375475 -0.0018358491 -0.0018340801 -0.0018327072 -0.0018313969][-0.0018307756 -0.0018324159 -0.0018344801 -0.0018361104 -0.0018370177 -0.0018374014 -0.0018372417 -0.0018367649 -0.00183638 -0.0018361708 -0.0018355219 -0.001834559 -0.0018336782 -0.001833081 -0.001832291][-0.0018332861 -0.0018346289 -0.0018359796 -0.0018365249 -0.0018362469 -0.0018356183 -0.0018345539 -0.0018335538 -0.0018329351 -0.0018328936 -0.0018329702 -0.0018330044 -0.0018330166 -0.0018331767 -0.0018330321][-0.0018355271 -0.0018362352 -0.001836647 -0.0018359335 -0.0018343959 -0.0018325679 -0.0018305567 -0.0018290495 -0.0018283154 -0.0018287894 -0.001829675 -0.0018306182 -0.0018315441 -0.0018324867 -0.0018329168][-0.0018371855 -0.0018373328 -0.0018368706 -0.0018350565 -0.0018322705 -0.0018290594 -0.0018260966 -0.0018241701 -0.0018236255 -0.0018246572 -0.0018260269 -0.0018274439 -0.0018289371 -0.0018306294 -0.0018318088][-0.0018378554 -0.001837395 -0.0018361456 -0.00183345 -0.0018296088 -0.001825338 -0.0018217044 -0.0018199236 -0.0018197863 -0.0018209278 -0.0018223537 -0.0018237914 -0.0018255177 -0.0018275349 -0.001829388][-0.0018379233 -0.0018370327 -0.001835307 -0.00183226 -0.0018279516 -0.0018229889 -0.0018191049 -0.001817534 -0.0018175934 -0.0018185244 -0.0018194844 -0.0018205334 -0.001821688 -0.0018236239 -0.0018260828][-0.0018374528 -0.0018365602 -0.0018348664 -0.001831747 -0.0018274009 -0.0018226515 -0.0018191534 -0.0018179837 -0.0018181865 -0.0018189184 -0.0018191671 -0.0018191434 -0.0018193761 -0.0018210575 -0.0018241106][-0.0018367886 -0.0018359079 -0.0018344017 -0.0018314392 -0.0018273483 -0.0018233811 -0.001820842 -0.0018204502 -0.0018213384 -0.0018223333 -0.0018221241 -0.0018214075 -0.0018212353 -0.0018226964 -0.0018253194][-0.0018365899 -0.0018357025 -0.0018343459 -0.0018318736 -0.0018285111 -0.0018253861 -0.0018237905 -0.0018240666 -0.0018254181 -0.0018263493 -0.0018261494 -0.0018255074 -0.0018253425 -0.0018264382 -0.0018280408][-0.0018361061 -0.001835363 -0.0018341966 -0.001832262 -0.0018296768 -0.0018273939 -0.0018264739 -0.0018270172 -0.0018281908 -0.0018288817 -0.0018290629 -0.0018289255 -0.0018288031 -0.0018293047 -0.0018299244][-0.0018349161 -0.0018343624 -0.0018334466 -0.0018321199 -0.0018303262 -0.0018288515 -0.0018282086 -0.001828355 -0.0018288882 -0.001829423 -0.0018298584 -0.0018299865 -0.001830067 -0.0018303646 -0.0018305844][-0.0018333283 -0.0018328672 -0.0018322387 -0.0018315051 -0.0018304063 -0.0018294153 -0.0018285591 -0.0018280962 -0.0018281663 -0.0018286695 -0.0018293224 -0.0018297503 -0.0018302387 -0.001830669 -0.0018307186][-0.001831593 -0.0018311476 -0.001830783 -0.0018303472 -0.0018295556 -0.0018287546 -0.0018278634 -0.0018273179 -0.0018272515 -0.0018277441 -0.00182847 -0.0018290416 -0.0018296726 -0.001830147 -0.001830184]]...]
INFO - root - 2017-12-10 00:02:49.161144: step 70410, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.842 sec/batch; 61h:20m:04s remains)
INFO - root - 2017-12-10 00:02:57.781894: step 70420, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.829 sec/batch; 60h:23m:11s remains)
INFO - root - 2017-12-10 00:03:06.312740: step 70430, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.897 sec/batch; 65h:16m:26s remains)
INFO - root - 2017-12-10 00:03:14.909150: step 70440, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 62h:42m:59s remains)
INFO - root - 2017-12-10 00:03:23.579579: step 70450, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.865 sec/batch; 62h:56m:18s remains)
INFO - root - 2017-12-10 00:03:32.301933: step 70460, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.898 sec/batch; 65h:23m:02s remains)
INFO - root - 2017-12-10 00:03:41.032992: step 70470, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.872 sec/batch; 63h:27m:07s remains)
INFO - root - 2017-12-10 00:03:49.608151: step 70480, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.852 sec/batch; 62h:00m:00s remains)
INFO - root - 2017-12-10 00:03:58.363456: step 70490, loss = 0.83, batch loss = 0.70 (9.0 examples/sec; 0.885 sec/batch; 64h:24m:24s remains)
INFO - root - 2017-12-10 00:04:06.786504: step 70500, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.841 sec/batch; 61h:14m:23s remains)
2017-12-10 00:04:07.732703: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.087972231 0.092213534 0.096036412 0.098116145 0.098297663 0.096228808 0.092529088 0.087586 0.082099035 0.076795705 0.071309306 0.065563411 0.059586369 0.053246554 0.046400577][0.088302709 0.093703352 0.099055193 0.10266148 0.10424352 0.10313927 0.099920571 0.094912544 0.0890602 0.083354831 0.077500388 0.071480982 0.065153673 0.0583424 0.050830066][0.084311031 0.090737566 0.097256824 0.10229496 0.10538809 0.10542434 0.10308335 0.098523855 0.092985876 0.087428987 0.0816134 0.075586513 0.069159858 0.062253557 0.054300442][0.078709841 0.085787825 0.092986129 0.098824896 0.1028336 0.10395584 0.10280187 0.099101014 0.094603583 0.089977421 0.084923021 0.079279989 0.072802283 0.065568633 0.056983691][0.077193409 0.0839861 0.090805508 0.096472204 0.10056011 0.1020617 0.10179065 0.0994533 0.096545145 0.0931546 0.089237645 0.084245719 0.077762261 0.069869936 0.060282253][0.082594849 0.08908806 0.094969735 0.09961427 0.10282359 0.10403056 0.10416164 0.1027578 0.10120632 0.099236794 0.096476704 0.09184739 0.085024342 0.07615684 0.065237321][0.0935993 0.10026265 0.10539156 0.10908713 0.11127959 0.11177428 0.11160637 0.11056691 0.10977905 0.10834101 0.10592258 0.10127587 0.093859591 0.08369118 0.071149141][0.1064551 0.11364423 0.11832059 0.12129067 0.12250763 0.12219969 0.12143434 0.1203943 0.11981231 0.11848701 0.11579657 0.11043624 0.10198948 0.090423085 0.076346762][0.11947031 0.12680221 0.13091157 0.1330419 0.13329247 0.1322156 0.13074079 0.12936094 0.12839344 0.12674749 0.12352061 0.11732817 0.10782687 0.095066883 0.079803623][0.12958987 0.13688147 0.14045879 0.14182292 0.14135419 0.13955061 0.1374236 0.13556397 0.13387921 0.13152681 0.12732981 0.12023965 0.10997071 0.0964894 0.080668263][0.13374947 0.14056103 0.14342192 0.14429061 0.14368254 0.14195961 0.13987049 0.13780493 0.1356373 0.13263875 0.12756306 0.11965784 0.10884058 0.095149077 0.0793644][0.13054211 0.13671254 0.13884382 0.13937908 0.13884521 0.13748904 0.13583358 0.13404192 0.13185218 0.12841065 0.12280863 0.11463478 0.10381572 0.090448186 0.075235993][0.11982019 0.12536521 0.12710139 0.12752633 0.12711345 0.12603992 0.12464803 0.12312002 0.12105002 0.11771839 0.11231939 0.10454204 0.094465628 0.082130507 0.068247736][0.10273121 0.10763474 0.10928256 0.10990193 0.10985674 0.10916526 0.10805877 0.10668518 0.10465083 0.10149781 0.096659325 0.089842379 0.081142351 0.070612125 0.058850195][0.081857271 0.085988164 0.08755412 0.088351 0.088659905 0.088384733 0.08767619 0.086503163 0.084682405 0.081950746 0.077911742 0.072376221 0.065475017 0.057248343 0.048044905]]...]
INFO - root - 2017-12-10 00:04:16.177463: step 70510, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.877 sec/batch; 63h:50m:55s remains)
INFO - root - 2017-12-10 00:04:24.956095: step 70520, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.879 sec/batch; 63h:56m:44s remains)
INFO - root - 2017-12-10 00:04:33.317369: step 70530, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.837 sec/batch; 60h:55m:11s remains)
INFO - root - 2017-12-10 00:04:41.767290: step 70540, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.855 sec/batch; 62h:14m:16s remains)
INFO - root - 2017-12-10 00:04:50.419917: step 70550, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 62h:43m:00s remains)
INFO - root - 2017-12-10 00:04:58.864384: step 70560, loss = 0.83, batch loss = 0.70 (9.3 examples/sec; 0.865 sec/batch; 62h:55m:29s remains)
INFO - root - 2017-12-10 00:05:07.511923: step 70570, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 63h:23m:24s remains)
INFO - root - 2017-12-10 00:05:16.225226: step 70580, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.855 sec/batch; 62h:13m:11s remains)
INFO - root - 2017-12-10 00:05:25.073853: step 70590, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.885 sec/batch; 64h:24m:09s remains)
INFO - root - 2017-12-10 00:05:33.762843: step 70600, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.899 sec/batch; 65h:23m:24s remains)
2017-12-10 00:05:34.625157: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0018175847 -0.0018154129 -0.0018146578 -0.0018149775 -0.0018164432 -0.0018181101 -0.0018175988 -0.0018122829 -0.0017971847 -0.0017724764 -0.0017534341 -0.0017565719 -0.0017796446 -0.0018000196 -0.0018114347][-0.0018171532 -0.0018137726 -0.0018123216 -0.0018133372 -0.0018163119 -0.0018184849 -0.001813154 -0.00179012 -0.0017238896 -0.0016231785 -0.0015444453 -0.001547012 -0.0016298685 -0.0017232156 -0.001781154][-0.001816402 -0.0018116587 -0.0018098899 -0.0018122991 -0.0018173346 -0.0018193646 -0.0018043724 -0.0017454892 -0.0015678785 -0.0012975171 -0.0010657242 -0.0010570895 -0.0012618657 -0.0015273948 -0.0017008749][-0.0018154739 -0.0018099202 -0.0018092218 -0.0018130431 -0.0018194455 -0.0018206914 -0.0017933246 -0.0016762682 -0.0013101771 -0.00073759863 -0.00021480012 -0.00017040991 -0.00058747455 -0.0011670829 -0.0015522217][-0.0018144057 -0.0018087826 -0.0018096345 -0.0018147054 -0.0018213214 -0.0018204444 -0.0017788934 -0.0015608105 -0.00089626765 0.00014535256 0.0010901674 0.001169525 0.00042268459 -0.00062967651 -0.0013313118][-0.0018127791 -0.0018071618 -0.0018093524 -0.0018152309 -0.0018211976 -0.0018155019 -0.0017472366 -0.0013532197 -0.00025744864 0.0014041384 0.0028148256 0.0028533312 0.0016429707 2.1007145e-06 -0.0010766482][-0.0018106976 -0.0018051364 -0.0018084716 -0.0018150659 -0.0018198326 -0.001805155 -0.0016917222 -0.0010535761 0.00053733482 0.0028341725 0.0046003489 0.0044663902 0.0027344925 0.00053214154 -0.00087410538][-0.0018081317 -0.0018029991 -0.0018070632 -0.0018139955 -0.0018176785 -0.0017903077 -0.0016244012 -0.00075978239 0.0011977974 0.003881569 0.00572315 0.0053137271 0.0031859167 0.00068869942 -0.0008366789][-0.0018065771 -0.0018026115 -0.0018079354 -0.0018152079 -0.0018177198 -0.0017836256 -0.0015932654 -0.00062859792 0.0013882067 0.004014018 0.0055906787 0.0049242363 0.0027214512 0.00035487243 -0.0010016419][-0.0018073663 -0.0018049747 -0.0018108914 -0.0018176598 -0.0018191385 -0.0017873966 -0.0016152486 -0.00073289557 0.001009704 0.0031640925 0.0042551113 0.0034665307 0.0015576474 -0.00030317635 -0.0012829521][-0.001809517 -0.0018088747 -0.0018150113 -0.0018208683 -0.0018221574 -0.0017986951 -0.0016692553 -0.00099530292 0.00027401687 0.0017579378 0.0023534126 0.0016221007 0.00023360259 -0.00097807287 -0.0015465594][-0.0018113039 -0.0018117543 -0.0018176725 -0.0018232082 -0.0018249969 -0.0018109169 -0.0017298348 -0.0012963731 -0.00051517645 0.00034498388 0.00058948074 4.4065411e-05 -0.00079982472 -0.0014490476 -0.0017126561][-0.0018122793 -0.001812949 -0.0018183151 -0.0018233334 -0.0018255821 -0.0018193817 -0.0017784408 -0.0015488931 -0.0011475072 -0.00073341129 -0.000665282 -0.00098904222 -0.0014111617 -0.0016907987 -0.0017869159][-0.0018133742 -0.0018138889 -0.0018182158 -0.0018225083 -0.0018250796 -0.0018246403 -0.0018112762 -0.0017150979 -0.0015449314 -0.0013835358 -0.0013764276 -0.0015245012 -0.0016902572 -0.0017831154 -0.0018108233][-0.0018148301 -0.0018144852 -0.0018172349 -0.0018202943 -0.0018224441 -0.0018235691 -0.0018218311 -0.0017948012 -0.0017369121 -0.0016860215 -0.0016873347 -0.0017385005 -0.0017867808 -0.0018089478 -0.0018161901]]...]
INFO - root - 2017-12-10 00:05:43.133552: step 70610, loss = 0.83, batch loss = 0.70 (9.1 examples/sec; 0.881 sec/batch; 64h:06m:09s remains)
INFO - root - 2017-12-10 00:05:51.878032: step 70620, loss = 0.81, batch loss = 0.68 (9.2 examples/sec; 0.874 sec/batch; 63h:34m:18s remains)
INFO - root - 2017-12-10 00:06:00.375561: step 70630, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.895 sec/batch; 65h:07m:39s remains)
INFO - root - 2017-12-10 00:06:09.034427: step 70640, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.877 sec/batch; 63h:48m:36s remains)
INFO - root - 2017-12-10 00:06:17.698676: step 70650, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.856 sec/batch; 62h:13m:40s remains)
INFO - root - 2017-12-10 00:06:26.368487: step 70660, loss = 0.83, batch loss = 0.70 (9.0 examples/sec; 0.893 sec/batch; 64h:55m:37s remains)
INFO - root - 2017-12-10 00:06:35.099572: step 70670, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.858 sec/batch; 62h:22m:32s remains)
INFO - root - 2017-12-10 00:06:43.795029: step 70680, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.879 sec/batch; 63h:56m:50s remains)
INFO - root - 2017-12-10 00:06:52.459311: step 70690, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.901 sec/batch; 65h:30m:19s remains)
INFO - root - 2017-12-10 00:07:01.124555: step 70700, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 62h:40m:23s remains)
2017-12-10 00:07:02.064593: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0018346385 -0.0018337416 -0.0018333833 -0.0018329676 -0.0018326237 -0.0018322178 -0.0018319525 -0.0018316421 -0.0018313543 -0.0018311976 -0.0018311109 -0.0018309753 -0.0018308618 -0.0018307798 -0.0018306954][-0.0018359056 -0.0018348087 -0.0018341704 -0.0018334626 -0.0018327651 -0.001832081 -0.0018315478 -0.0018312767 -0.0018311437 -0.0018312236 -0.0018313335 -0.0018313284 -0.0018312718 -0.0018311418 -0.0018309577][-0.0018378249 -0.0018366749 -0.0018358961 -0.0018350598 -0.0018342511 -0.0018334063 -0.0018326894 -0.001832315 -0.0018322535 -0.0018324652 -0.00183268 -0.0018327025 -0.0018325108 -0.0018321108 -0.001831705][-0.0018393678 -0.0018381544 -0.0018372985 -0.0018364222 -0.0018355702 -0.0018347031 -0.0018339414 -0.0018335731 -0.0018336577 -0.0018338864 -0.0018340988 -0.001834074 -0.0018337561 -0.0018330172 -0.0018322278][-0.0018403496 -0.0018389727 -0.0018380295 -0.0018371276 -0.0018361739 -0.001835246 -0.0018345006 -0.0018341943 -0.0018343184 -0.0018346334 -0.0018349197 -0.0018348964 -0.0018345331 -0.0018338521 -0.001832998][-0.0018416324 -0.0018399284 -0.0018387444 -0.0018376574 -0.001836515 -0.0018353262 -0.0018342385 -0.00183376 -0.0018339931 -0.0018343629 -0.0018347484 -0.0018348855 -0.0018347675 -0.0018343853 -0.0018337561][-0.0018428423 -0.0018411507 -0.0018398589 -0.0018386997 -0.0018373551 -0.0018358512 -0.0018344201 -0.0018337697 -0.0018339708 -0.0018344368 -0.0018349658 -0.00183536 -0.001835485 -0.0018354341 -0.0018350211][-0.00184351 -0.0018421494 -0.0018410924 -0.0018400633 -0.0018388287 -0.0018373332 -0.0018358616 -0.001834864 -0.0018345647 -0.0018347452 -0.0018350997 -0.001835512 -0.0018359441 -0.0018363013 -0.0018362219][-0.0018431706 -0.0018422672 -0.0018417034 -0.0018410427 -0.0018400491 -0.0018386273 -0.0018371074 -0.0018355908 -0.0018344988 -0.0018340427 -0.0018340871 -0.0018345878 -0.001835335 -0.0018364423 -0.0018372186][-0.0018416265 -0.0018410858 -0.0018408939 -0.0018407359 -0.0018400039 -0.0018386894 -0.0018370308 -0.0018351033 -0.0018333695 -0.0018323021 -0.001832172 -0.0018328044 -0.0018341272 -0.0018362049 -0.001838049][-0.0018397644 -0.0018393242 -0.001839297 -0.0018394933 -0.0018390269 -0.0018378127 -0.0018361744 -0.0018343201 -0.0018324 -0.0018308292 -0.0018304837 -0.0018313184 -0.0018330164 -0.0018354871 -0.0018378557][-0.0018372092 -0.0018369785 -0.001837093 -0.0018376044 -0.0018375294 -0.0018367233 -0.0018352877 -0.0018336503 -0.001831912 -0.0018304454 -0.0018301226 -0.0018309235 -0.0018327356 -0.0018350842 -0.0018372993][-0.0018350685 -0.0018351631 -0.0018356277 -0.0018363721 -0.0018366923 -0.0018363027 -0.0018350917 -0.0018335808 -0.0018316308 -0.0018298995 -0.0018292333 -0.0018296558 -0.0018313535 -0.0018334201 -0.0018352668][-0.0018338086 -0.0018341175 -0.00183474 -0.0018355877 -0.0018361135 -0.0018360098 -0.0018349623 -0.0018333289 -0.0018312336 -0.0018291719 -0.0018282295 -0.0018282571 -0.0018293733 -0.0018307966 -0.0018320598][-0.0018338622 -0.0018338866 -0.0018343918 -0.001835029 -0.0018354781 -0.0018354114 -0.0018343343 -0.0018325639 -0.0018302503 -0.0018280693 -0.0018268488 -0.0018265599 -0.0018272157 -0.0018280533 -0.0018288023]]...]
INFO - root - 2017-12-10 00:07:10.558563: step 70710, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.849 sec/batch; 61h:42m:23s remains)
INFO - root - 2017-12-10 00:07:19.423577: step 70720, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.887 sec/batch; 64h:29m:30s remains)
INFO - root - 2017-12-10 00:07:28.007299: step 70730, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.879 sec/batch; 63h:54m:32s remains)
INFO - root - 2017-12-10 00:07:36.744807: step 70740, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.902 sec/batch; 65h:34m:36s remains)
INFO - root - 2017-12-10 00:07:45.463903: step 70750, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.860 sec/batch; 62h:32m:00s remains)
INFO - root - 2017-12-10 00:07:54.301362: step 70760, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.875 sec/batch; 63h:36m:33s remains)
INFO - root - 2017-12-10 00:08:02.925558: step 70770, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.847 sec/batch; 61h:36m:15s remains)
INFO - root - 2017-12-10 00:08:11.584445: step 70780, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.889 sec/batch; 64h:37m:33s remains)
INFO - root - 2017-12-10 00:08:20.200946: step 70790, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 62h:49m:19s remains)
INFO - root - 2017-12-10 00:08:28.682681: step 70800, loss = 0.83, batch loss = 0.70 (9.3 examples/sec; 0.863 sec/batch; 62h:44m:00s remains)
2017-12-10 00:08:29.666636: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.0092746876 0.0095922826 0.0095940195 0.0090833213 0.0084755933 0.0073061422 0.0061451965 0.00490229 0.0037799594 0.0029521571 0.0021512075 0.00178145 0.0013713239 0.0010477108 0.00069186569][0.0089032138 0.0088167731 0.0085333344 0.0081332037 0.0078319488 0.0074825897 0.0071018948 0.0067295902 0.0063712015 0.0058634956 0.0051660603 0.0044612624 0.0036887117 0.0029194877 0.0019498075][0.0084944135 0.0083775222 0.0081231268 0.008268618 0.0086966427 0.00943348 0.010035496 0.010668425 0.011143949 0.010836897 0.0099847317 0.0087059736 0.0073138303 0.0058200061 0.0042534014][0.0094725806 0.008963041 0.0087902285 0.0097789336 0.011439375 0.01377515 0.015801588 0.01752599 0.018647954 0.018592229 0.017485138 0.015316029 0.012634577 0.0098656854 0.0068538818][0.014334748 0.014047937 0.013919865 0.014960797 0.016835639 0.019471915 0.021587111 0.023042243 0.023566291 0.022798797 0.020726005 0.017538656 0.01377117 0.010148595 0.0066086124][0.019394983 0.020110423 0.021169603 0.023439439 0.026500216 0.029738527 0.031646624 0.031887896 0.030586872 0.027555399 0.023308013 0.018376047 0.013435639 0.0091767106 0.0055589178][0.026994579 0.028429532 0.030099776 0.032743137 0.035987295 0.039046854 0.040713824 0.040526956 0.038495068 0.034273028 0.028973883 0.023018217 0.017210536 0.012083526 0.0079844054][0.037744142 0.039565861 0.041362096 0.043766536 0.046407361 0.048479296 0.049039647 0.048061933 0.045269161 0.040757373 0.035461761 0.029659931 0.024223939 0.019223791 0.015051886][0.052112933 0.054184191 0.055630345 0.057475582 0.059274405 0.060366031 0.059931941 0.058015749 0.054558169 0.0499611 0.044902332 0.039880954 0.035292648 0.031000733 0.026958236][0.069714308 0.071651742 0.072435744 0.073427372 0.074480414 0.074893616 0.073705062 0.070920661 0.066756934 0.061961569 0.056962658 0.052603088 0.04873798 0.045179494 0.041421503][0.089516781 0.09208066 0.092654273 0.093028627 0.092990994 0.091794878 0.089183308 0.084981129 0.079701521 0.074609041 0.069886833 0.066309795 0.0633922 0.060805172 0.057594564][0.10845193 0.11135229 0.1107977 0.10977808 0.10849699 0.10611578 0.10249255 0.097383149 0.091437809 0.08578971 0.080902807 0.077497855 0.075345151 0.073742442 0.071654834][0.12393945 0.12685841 0.12460976 0.12199917 0.11918602 0.11567664 0.11140322 0.10612106 0.10046485 0.095197789 0.090875067 0.08830943 0.087135032 0.086553238 0.085755177][0.13298653 0.13537386 0.1323448 0.12901674 0.12573768 0.12200316 0.11818187 0.11381683 0.10927343 0.10508494 0.10185186 0.10029592 0.099936269 0.10007224 0.10003722][0.13514537 0.13702686 0.13293503 0.1284107 0.12484493 0.12231282 0.12012219 0.1176019 0.1155078 0.11363661 0.11189581 0.11088753 0.11066864 0.11077055 0.11065985]]...]
INFO - root - 2017-12-10 00:08:38.166228: step 70810, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 63h:12m:08s remains)
INFO - root - 2017-12-10 00:08:46.845781: step 70820, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.852 sec/batch; 61h:55m:19s remains)
INFO - root - 2017-12-10 00:08:55.554421: step 70830, loss = 0.83, batch loss = 0.70 (9.1 examples/sec; 0.878 sec/batch; 63h:48m:57s remains)
INFO - root - 2017-12-10 00:09:04.254209: step 70840, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.888 sec/batch; 64h:30m:49s remains)
INFO - root - 2017-12-10 00:09:12.913267: step 70850, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.866 sec/batch; 62h:56m:52s remains)
INFO - root - 2017-12-10 00:09:21.684344: step 70860, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.880 sec/batch; 63h:56m:15s remains)
INFO - root - 2017-12-10 00:09:30.329943: step 70870, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.865 sec/batch; 62h:52m:32s remains)
INFO - root - 2017-12-10 00:09:39.021577: step 70880, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.865 sec/batch; 62h:53m:37s remains)
INFO - root - 2017-12-10 00:09:47.762514: step 70890, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 62h:45m:38s remains)
INFO - root - 2017-12-10 00:09:56.332161: step 70900, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.888 sec/batch; 64h:30m:54s remains)
2017-12-10 00:09:57.272356: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0018241944 -0.0018231454 -0.001817868 -0.0017930483 -0.0017306317 -0.001633208 -0.0015433116 -0.0015193645 -0.0015796314 -0.00168348 -0.0017698059 -0.0018124813 -0.0018235816 -0.0018241253 -0.0018239333][-0.0018236553 -0.0018192524 -0.0017960055 -0.0017179317 -0.0015523154 -0.0013241922 -0.0011392913 -0.0011145575 -0.0012674942 -0.0015001578 -0.0016901845 -0.0017882931 -0.001819192 -0.0018236934 -0.0018236468][-0.0018228599 -0.0018066305 -0.0017320843 -0.0015173252 -0.0011033381 -0.00057301647 -0.00017225824 -0.00014312577 -0.00050258951 -0.0010318095 -0.0014729672 -0.0017166485 -0.0018046258 -0.001822418 -0.0018233641][-0.001820365 -0.0017780808 -0.00159567 -0.0011005251 -0.00018762902 0.00094378449 0.0017779659 0.0018253421 0.0010624678 -6.401795e-05 -0.0010203021 -0.0015668572 -0.0017740713 -0.0018202672 -0.0018230431][-0.0018158614 -0.0017318518 -0.0013790269 -0.00044368336 0.0012486257 0.0033207289 0.0048385379 0.0049205711 0.0035251454 0.0014541029 -0.000316876 -0.001338697 -0.0017281666 -0.0018162139 -0.001821608][-0.0018088096 -0.0016743983 -0.0011167221 0.0003466598 0.0029669898 0.0061482093 0.0084562842 0.0085489275 0.0063762623 0.0031806552 0.00046349654 -0.0010947424 -0.0016817966 -0.0018125838 -0.0018198562][-0.0017919934 -0.0016085141 -0.00087199348 0.0010370562 0.0044192513 0.0084825866 0.011380686 0.011418379 0.0085712085 0.0044650426 0.0010180768 -0.00093226717 -0.0016541679 -0.0018112226 -0.0018189276][-0.0017663813 -0.0015511436 -0.00073355844 0.0013431447 0.0049709133 0.0092671588 0.012249382 0.012156159 0.009034479 0.0046643773 0.0010642264 -0.00093446876 -0.0016588354 -0.001811409 -0.0018178779][-0.0017437449 -0.0015289601 -0.00077169144 0.0011001314 0.0043129879 0.0080456948 0.010541654 0.010308439 0.0074760071 0.0036520013 0.00057147059 -0.0011009683 -0.0016935321 -0.0018141313 -0.0018183752][-0.0017412561 -0.0015642191 -0.00098980393 0.00038420164 0.0026970049 0.0053258506 0.0070051751 0.0067156688 0.00462261 0.0019078659 -0.0002234882 -0.0013513289 -0.0017411169 -0.0018173732 -0.0018195931][-0.0017641581 -0.0016480982 -0.0013010351 -0.00050002558 0.00082021568 0.0022859988 0.0031738649 0.002934067 0.0017044252 0.00017742498 -0.00098613766 -0.0015835164 -0.0017836506 -0.001820287 -0.0018205683][-0.0017956073 -0.0017390209 -0.001580599 -0.0012270247 -0.00065639638 -3.7916703e-05 0.00031512103 0.00017896679 -0.00036791712 -0.0010181761 -0.001497463 -0.0017345153 -0.0018103356 -0.0018227167 -0.001822223][-0.0018191912 -0.0018016318 -0.0017515745 -0.0016416766 -0.0014684957 -0.0012860398 -0.0011885853 -0.0012389774 -0.0014081526 -0.001601367 -0.0017383364 -0.0018026553 -0.0018213245 -0.0018232572 -0.0018225521][-0.0018241466 -0.0018220465 -0.0018146979 -0.0017955854 -0.0017644987 -0.0017320036 -0.0017153465 -0.0017251595 -0.0017555683 -0.0017893023 -0.001811955 -0.0018216213 -0.0018235147 -0.0018228764 -0.0018222822][-0.0018232878 -0.0018228448 -0.0018224635 -0.0018211233 -0.0018190274 -0.0018166284 -0.0018150123 -0.0018149328 -0.0018171681 -0.0018205851 -0.0018229096 -0.0018235878 -0.0018231767 -0.001822549 -0.0018221413]]...]
INFO - root - 2017-12-10 00:10:05.756086: step 70910, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.870 sec/batch; 63h:11m:10s remains)
INFO - root - 2017-12-10 00:10:14.470270: step 70920, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.839 sec/batch; 60h:59m:30s remains)
INFO - root - 2017-12-10 00:10:23.062400: step 70930, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.861 sec/batch; 62h:32m:05s remains)
INFO - root - 2017-12-10 00:10:31.751096: step 70940, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.856 sec/batch; 62h:10m:32s remains)
INFO - root - 2017-12-10 00:10:40.304050: step 70950, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.857 sec/batch; 62h:17m:27s remains)
INFO - root - 2017-12-10 00:10:48.999774: step 70960, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.876 sec/batch; 63h:39m:48s remains)
INFO - root - 2017-12-10 00:10:57.576589: step 70970, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.840 sec/batch; 61h:03m:06s remains)
INFO - root - 2017-12-10 00:11:06.242874: step 70980, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.894 sec/batch; 64h:58m:19s remains)
INFO - root - 2017-12-10 00:11:14.912632: step 70990, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.866 sec/batch; 62h:52m:45s remains)
INFO - root - 2017-12-10 00:11:23.479709: step 71000, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.860 sec/batch; 62h:29m:17s remains)
2017-12-10 00:11:24.388253: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0018384439 -0.0018382581 -0.001836714 -0.0018315538 -0.0018212767 -0.0018052901 -0.0017854783 -0.0017670435 -0.0017566971 -0.0017576397 -0.0017687674 -0.0017858987 -0.0018042978 -0.0018197367 -0.0018301713][-0.0018383564 -0.0018369443 -0.001831327 -0.0018192878 -0.0018020307 -0.0017817686 -0.0017601495 -0.001740591 -0.0017295341 -0.001730946 -0.0017444069 -0.0017661267 -0.0017913138 -0.0018141565 -0.0018303312][-0.0018342312 -0.0018277966 -0.0018097876 -0.0017779662 -0.001738537 -0.0017004428 -0.0016683311 -0.001644281 -0.001633825 -0.0016417013 -0.0016684775 -0.0017085754 -0.0017533406 -0.0017930269 -0.0018208205][-0.0018174122 -0.0017955712 -0.0017469965 -0.0016724024 -0.0015885402 -0.0015164246 -0.0014664597 -0.0014382233 -0.0014340195 -0.0014579254 -0.0015111213 -0.0015862389 -0.0016690413 -0.0017427695 -0.0017954486][-0.0017760519 -0.00171995 -0.0016119648 -0.0014617076 -0.0013048563 -0.0011795991 -0.0011027465 -0.0010694333 -0.0010759733 -0.0011256463 -0.0012212852 -0.0013543848 -0.0015034438 -0.0016400359 -0.0017417005][-0.0017035275 -0.0015883965 -0.0013880058 -0.0011288826 -0.00087273272 -0.000676156 -0.00056062138 -0.00051763374 -0.00053887593 -0.00062672445 -0.00078356953 -0.00099848048 -0.0012423747 -0.0014724745 -0.0016508702][-0.0016084462 -0.0014167574 -0.0011057247 -0.00072502706 -0.00036258192 -8.8365283e-05 7.4995332e-05 0.00013626751 0.00010321487 -2.5895308e-05 -0.00025002076 -0.00055572006 -0.00090873981 -0.0012524105 -0.0015287616][-0.0015175469 -0.0012517588 -0.00084267929 -0.00036182092 8.4670843e-05 0.00042276166 0.00063209364 0.00071493082 0.00067245367 0.00050441839 0.00022215059 -0.00015612901 -0.00059706043 -0.0010384906 -0.0014053988][-0.0014562783 -0.0011393127 -0.00066843722 -0.00013165874 0.0003576387 0.00072925596 0.00096735533 0.0010655067 0.001015169 0.00081633858 0.00049398828 7.5074728e-05 -0.00041025947 -0.00090367364 -0.0013239083][-0.0014265741 -0.0010852099 -0.00059365807 -4.8704329e-05 0.00043682673 0.00080273987 0.0010417843 0.0011434091 0.0010893125 0.00087698584 0.00053981587 0.0001113218 -0.00037838286 -0.00087732065 -0.0013063499][-0.0014251778 -0.0010753238 -0.000588872 -6.7460467e-05 0.00037927588 0.00070389581 0.00091089716 0.00099466078 0.00093365426 0.00072187942 0.00039285503 -1.7752871e-05 -0.00048076513 -0.00094938721 -0.0013499453][-0.0014483 -0.0011055798 -0.00064680958 -0.00017422764 0.00021042617 0.00047135132 0.00062316249 0.00067009113 0.0005944994 0.0003902592 8.6710206e-05 -0.00028310367 -0.0006915438 -0.0010976798 -0.0014380509][-0.0015094473 -0.0011997614 -0.00079427985 -0.00038936827 -7.7852979e-05 0.00010991225 0.0001937818 0.00018974801 9.2106406e-05 -9.9090277e-05 -0.00036076398 -0.00066558504 -0.00099012768 -0.0013026227 -0.0015561143][-0.0016060013 -0.0013632649 -0.0010418955 -0.00072210142 -0.00048471394 -0.0003623378 -0.000338407 -0.00038859935 -0.00050286984 -0.000671305 -0.00087523623 -0.0010950823 -0.0013155784 -0.0015171779 -0.0016737612][-0.0017097688 -0.0015547787 -0.0013388135 -0.0011166972 -0.00095290085 -0.00088169216 -0.00089394487 -0.00096272695 -0.0010680179 -0.0011958682 -0.0013330695 -0.0014667697 -0.0015889709 -0.0016917302 -0.0017665009]]...]
INFO - root - 2017-12-10 00:11:32.976579: step 71010, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.879 sec/batch; 63h:51m:26s remains)
INFO - root - 2017-12-10 00:11:41.620216: step 71020, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.876 sec/batch; 63h:38m:53s remains)
INFO - root - 2017-12-10 00:11:50.172297: step 71030, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.850 sec/batch; 61h:43m:10s remains)
INFO - root - 2017-12-10 00:11:58.833393: step 71040, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.857 sec/batch; 62h:15m:31s remains)
INFO - root - 2017-12-10 00:12:07.763712: step 71050, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.860 sec/batch; 62h:26m:27s remains)
INFO - root - 2017-12-10 00:12:16.576922: step 71060, loss = 0.82, batch loss = 0.69 (8.2 examples/sec; 0.974 sec/batch; 70h:45m:55s remains)
INFO - root - 2017-12-10 00:12:25.356323: step 71070, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 63h:07m:54s remains)
INFO - root - 2017-12-10 00:12:33.987162: step 71080, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.876 sec/batch; 63h:36m:35s remains)
INFO - root - 2017-12-10 00:12:42.696432: step 71090, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.848 sec/batch; 61h:36m:15s remains)
INFO - root - 2017-12-10 00:12:51.280045: step 71100, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.857 sec/batch; 62h:12m:48s remains)
2017-12-10 00:12:52.273928: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.00073500676 -0.00041999028 6.5537752e-05 0.0006579099 0.0013441237 0.0021914723 0.0031960811 0.0043962654 0.0056539821 0.0068999124 0.0079363557 0.0084618535 0.0083813649 0.007745719 0.0065337834][-0.00050001428 -0.00024572865 0.00023593998 0.00099066074 0.0019711195 0.0033730934 0.0050811451 0.0070672403 0.0090588676 0.010908225 0.012320116 0.013007662 0.012823921 0.011737261 0.009945754][-0.00021005655 5.0453236e-05 0.00069249875 0.0017746488 0.0033260197 0.0054895263 0.0080536492 0.010786954 0.013295386 0.01540242 0.01684944 0.017479502 0.017114606 0.015778001 0.013638155][0.0001514185 0.00054592069 0.0014799499 0.0030358168 0.0052896151 0.0082364231 0.011517359 0.0147748 0.017572617 0.019750617 0.021156847 0.021646688 0.021049432 0.019440811 0.017036041][0.00046225672 0.0011909289 0.0025889305 0.0046975375 0.0075849216 0.011079165 0.014786336 0.018220173 0.020985736 0.022992197 0.024271689 0.024699083 0.024117945 0.022545151 0.020163741][0.00066139863 0.0018298953 0.0038226438 0.0065284655 0.0099025639 0.01364499 0.017380053 0.020676378 0.023178354 0.024824034 0.025850045 0.026171818 0.0257192 0.024427477 0.022470364][0.00084553089 0.0024388502 0.0049398718 0.0081858449 0.01191259 0.01572996 0.019200081 0.022040093 0.02400808 0.02509821 0.025710247 0.025882086 0.025668249 0.0248678 0.023565196][0.0012511857 0.0031259111 0.0059206015 0.009538996 0.013372752 0.017050531 0.020111766 0.02236532 0.023606764 0.023967646 0.024058897 0.024120301 0.024252675 0.024106637 0.023557719][0.0017896007 0.0039007966 0.0068491329 0.010511568 0.014150011 0.017418612 0.019834299 0.021303575 0.021706037 0.021390794 0.020962851 0.0209134 0.021327866 0.021773526 0.021865552][0.0020699073 0.0043064095 0.0072887796 0.010753701 0.013963447 0.016589941 0.018178282 0.018777922 0.0183984 0.017554902 0.016888 0.016859056 0.017492788 0.01825957 0.018685805][0.0019947267 0.0041280249 0.0068348716 0.00981692 0.012412675 0.014321067 0.015126118 0.014969234 0.014036489 0.012955233 0.012244835 0.012321151 0.012996054 0.013820508 0.014349415][0.001500511 0.0032965476 0.00551789 0.0078571346 0.009800341 0.010987372 0.011157847 0.010549031 0.0093725761 0.0082927784 0.0076933359 0.0077791824 0.008330672 0.0090120509 0.0095009869][0.00060662476 0.0019126575 0.0034924 0.0051079961 0.0063701165 0.0070137172 0.0068856427 0.0061604795 0.0050893435 0.0042347419 0.0037966897 0.0038476894 0.004165255 0.0046751932 0.0050787893][-0.00044071337 0.00037490774 0.0013528465 0.0023158668 0.0030048811 0.0032497486 0.0029895166 0.0023787492 0.0016499314 0.0011005021 0.00085825042 0.00090617931 0.0010910524 0.0014188335 0.0017004752][-0.0012709254 -0.00087325682 -0.00038758328 6.6536944e-05 0.0003481129 0.0003901493 0.0001781442 -0.00017991371 -0.00056635239 -0.0008378803 -0.00094084488 -0.0008896084 -0.00075314613 -0.00057851512 -0.00043279666]]...]
INFO - root - 2017-12-10 00:13:00.836698: step 71110, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.829 sec/batch; 60h:12m:05s remains)
INFO - root - 2017-12-10 00:13:09.434394: step 71120, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.875 sec/batch; 63h:29m:47s remains)
INFO - root - 2017-12-10 00:13:18.244253: step 71130, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.866 sec/batch; 62h:50m:16s remains)
INFO - root - 2017-12-10 00:13:26.998584: step 71140, loss = 0.82, batch loss = 0.69 (8.3 examples/sec; 0.966 sec/batch; 70h:09m:53s remains)
INFO - root - 2017-12-10 00:13:35.638065: step 71150, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.878 sec/batch; 63h:42m:34s remains)
INFO - root - 2017-12-10 00:13:44.217885: step 71160, loss = 0.82, batch loss = 0.69 (8.8 examples/sec; 0.909 sec/batch; 66h:00m:47s remains)
INFO - root - 2017-12-10 00:13:52.965650: step 71170, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 63h:04m:26s remains)
INFO - root - 2017-12-10 00:14:01.767111: step 71180, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.856 sec/batch; 62h:07m:46s remains)
INFO - root - 2017-12-10 00:14:10.475367: step 71190, loss = 0.82, batch loss = 0.70 (9.4 examples/sec; 0.850 sec/batch; 61h:39m:54s remains)
INFO - root - 2017-12-10 00:14:19.098161: step 71200, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.888 sec/batch; 64h:27m:38s remains)
2017-12-10 00:14:19.936075: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.01225159 0.011082117 0.0097868359 0.0086756935 0.0081452494 0.0078231664 0.0076759937 0.0076051704 0.007708251 0.0080707762 0.0089500379 0.011108082 0.013799052 0.017302129 0.021046575][0.014704212 0.013604609 0.012200202 0.010716092 0.0097404057 0.0089373766 0.0084717311 0.00808853 0.0077928952 0.00783517 0.0083135255 0.010016054 0.012446102 0.015902145 0.019708071][0.018229451 0.0177285 0.01639438 0.014762192 0.013440769 0.012250262 0.01161504 0.011193087 0.010888384 0.010513475 0.010424203 0.011435601 0.01316395 0.016071374 0.019570189][0.022486465 0.022899022 0.021944765 0.020457866 0.018874753 0.01766081 0.017206296 0.017054545 0.016893251 0.016349122 0.015820188 0.015998006 0.016906764 0.018965622 0.021712698][0.027018957 0.028611574 0.02839943 0.027354894 0.025916828 0.025028169 0.024962196 0.025219429 0.025362846 0.024632793 0.023685832 0.023248492 0.023509994 0.024769787 0.026805891][0.031268436 0.034163456 0.034956127 0.034668587 0.033648998 0.033276755 0.03365488 0.034174055 0.034369856 0.033584941 0.032291152 0.031401619 0.031234216 0.031986736 0.033499826][0.034318846 0.038438022 0.040378496 0.041011356 0.040645588 0.040827066 0.041527689 0.042143907 0.042188313 0.041260842 0.039723858 0.038640097 0.038230877 0.03867295 0.039793536][0.035985403 0.041222997 0.044317823 0.045861125 0.04613737 0.046652913 0.047399178 0.047907375 0.047681872 0.046637073 0.044994313 0.043895617 0.043298453 0.043427974 0.044193987][0.036091827 0.042120136 0.046146482 0.048420005 0.049159136 0.049709156 0.050209805 0.050414715 0.049890693 0.048799098 0.0472372 0.04627962 0.045715015 0.04588123 0.04636085][0.034641657 0.040931638 0.045381121 0.048093528 0.049107037 0.049514841 0.049554829 0.049279239 0.048438739 0.047289446 0.045937184 0.045198329 0.044837207 0.0451292 0.045520976][0.031605009 0.0375169 0.0417968 0.04444154 0.045369305 0.04549415 0.045062251 0.044382066 0.043352306 0.042138595 0.040975526 0.040413044 0.040315129 0.040836319 0.041421384][0.027185468 0.032153644 0.035721015 0.037838265 0.038402528 0.038120385 0.037242405 0.036235429 0.035097424 0.033931538 0.03301632 0.032566693 0.032664072 0.033317909 0.034078281][0.02174565 0.025424819 0.027944993 0.029242879 0.029285947 0.028599475 0.027415659 0.026211018 0.025054576 0.024008552 0.023298386 0.022966515 0.023173979 0.023835303 0.024681196][0.01586864 0.018198749 0.019613909 0.020074792 0.019650334 0.018714095 0.017474722 0.016316716 0.015301662 0.014456761 0.013929923 0.013676626 0.013861855 0.014392479 0.01512958][0.010367386 0.011532082 0.012031924 0.011871871 0.011191565 0.010237184 0.0091776354 0.0082535762 0.007496146 0.0069205086 0.0065766121 0.006404107 0.0065087196 0.0068328143 0.0073464061]]...]
INFO - root - 2017-12-10 00:14:28.522688: step 71210, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.891 sec/batch; 64h:39m:58s remains)
INFO - root - 2017-12-10 00:14:37.215124: step 71220, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.858 sec/batch; 62h:18m:21s remains)
INFO - root - 2017-12-10 00:14:45.902303: step 71230, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.870 sec/batch; 63h:09m:59s remains)
INFO - root - 2017-12-10 00:14:54.680006: step 71240, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.899 sec/batch; 65h:14m:34s remains)
INFO - root - 2017-12-10 00:15:03.296105: step 71250, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.855 sec/batch; 62h:01m:28s remains)
INFO - root - 2017-12-10 00:15:11.951833: step 71260, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.848 sec/batch; 61h:33m:06s remains)
INFO - root - 2017-12-10 00:15:20.612185: step 71270, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.870 sec/batch; 63h:06m:25s remains)
INFO - root - 2017-12-10 00:15:29.407932: step 71280, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.879 sec/batch; 63h:45m:32s remains)
INFO - root - 2017-12-10 00:15:38.200346: step 71290, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.878 sec/batch; 63h:43m:29s remains)
INFO - root - 2017-12-10 00:15:46.675203: step 71300, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.866 sec/batch; 62h:51m:47s remains)
2017-12-10 00:15:47.574213: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.020479437 0.024514483 0.028399035 0.032067027 0.034900293 0.037196491 0.03888496 0.039772294 0.039708484 0.039257571 0.038490228 0.038329579 0.037864957 0.038079061 0.037641287][0.022761352 0.027375456 0.0317323 0.035767809 0.039453279 0.042621385 0.044975944 0.046482649 0.046909153 0.046472702 0.045778953 0.045676149 0.045711558 0.046145316 0.045530371][0.022676699 0.02721737 0.031354126 0.035444584 0.039255388 0.042702336 0.045358725 0.047225766 0.048067316 0.04835397 0.048111796 0.048271861 0.048479922 0.049196664 0.049404629][0.020142805 0.024687812 0.028778983 0.032852069 0.036865991 0.04065562 0.04367432 0.045823827 0.047065504 0.047626559 0.047454767 0.047377177 0.047334455 0.048004474 0.048806213][0.015695512 0.019651489 0.02373771 0.028153406 0.0323791 0.036331836 0.039691258 0.042089749 0.043487642 0.044042286 0.043784626 0.043418437 0.043038648 0.043331712 0.043984707][0.010793841 0.013840958 0.01729331 0.021380605 0.025990386 0.030313723 0.033823963 0.036279451 0.037891857 0.038437586 0.037864611 0.036982045 0.036081236 0.035839777 0.036049921][0.0061703757 0.0080393432 0.010570973 0.013888166 0.017887937 0.021798864 0.02526528 0.028151371 0.030013643 0.03050999 0.029815324 0.028642992 0.027239796 0.026294688 0.025941685][0.0024066023 0.0032643937 0.0046997643 0.0068196845 0.0097555118 0.012864936 0.015779192 0.018341469 0.020201722 0.021016072 0.02045182 0.01922885 0.017724274 0.016552439 0.015896006][-0.00021368999 0.00011789834 0.00073764462 0.0017605786 0.0034457669 0.0054187463 0.0074961889 0.00943846 0.01085381 0.011412649 0.010921618 0.0098222448 0.008583636 0.0075129545 0.0068648648][-0.0016245692 -0.0015014682 -0.0013009266 -0.00094601163 -0.00027961261 0.00058748515 0.0015986633 0.002575275 0.0033081146 0.0035838094 0.0032652118 0.0026000189 0.001885601 0.0013395319 0.00098688586][-0.0018178985 -0.0018068167 -0.0017825601 -0.0016986377 -0.0015294339 -0.0012728126 -0.0009592789 -0.00065826252 -0.00043534918 -0.00037752918 -0.00051614025 -0.00077423325 -0.0010464897 -0.0012589314 -0.0013984098][-0.0018188584 -0.0018146482 -0.0018109933 -0.0018059289 -0.0017993397 -0.0017876161 -0.0017704602 -0.001752191 -0.0017386307 -0.001739722 -0.0017553949 -0.0017725371 -0.0017861747 -0.0017973451 -0.0018059821][-0.0018199437 -0.0018174143 -0.0018147357 -0.0018107331 -0.0018072537 -0.0018052283 -0.0018063786 -0.0018089653 -0.0018106353 -0.0018119046 -0.0018130904 -0.0018141597 -0.0018143886 -0.0018148692 -0.0018150456][-0.0018201924 -0.0018193841 -0.0018188448 -0.0018171184 -0.0018154258 -0.00181311 -0.0018124075 -0.0018128393 -0.0018130256 -0.0018133833 -0.0018132421 -0.0018131213 -0.0018128891 -0.001813403 -0.0018137795][-0.0018204725 -0.0018200869 -0.0018200529 -0.0018196217 -0.0018189268 -0.0018165959 -0.0018153604 -0.0018144611 -0.0018138177 -0.0018139988 -0.0018134266 -0.0018135255 -0.001813309 -0.0018138625 -0.0018141012]]...]
INFO - root - 2017-12-10 00:15:56.117463: step 71310, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.873 sec/batch; 63h:22m:21s remains)
INFO - root - 2017-12-10 00:16:04.849236: step 71320, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 62h:54m:35s remains)
INFO - root - 2017-12-10 00:16:13.203152: step 71330, loss = 0.82, batch loss = 0.69 (10.7 examples/sec; 0.750 sec/batch; 54h:25m:49s remains)
INFO - root - 2017-12-10 00:16:21.853482: step 71340, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 62h:54m:02s remains)
INFO - root - 2017-12-10 00:16:30.491717: step 71350, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 62h:42m:16s remains)
INFO - root - 2017-12-10 00:16:38.943920: step 71360, loss = 0.81, batch loss = 0.68 (9.7 examples/sec; 0.822 sec/batch; 59h:37m:25s remains)
INFO - root - 2017-12-10 00:16:47.651220: step 71370, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.875 sec/batch; 63h:28m:05s remains)
INFO - root - 2017-12-10 00:16:56.430481: step 71380, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 63h:08m:47s remains)
INFO - root - 2017-12-10 00:17:05.280138: step 71390, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.884 sec/batch; 64h:05m:51s remains)
INFO - root - 2017-12-10 00:17:13.886020: step 71400, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.873 sec/batch; 63h:19m:00s remains)
2017-12-10 00:17:14.825413: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.001817535 -0.0018168731 -0.0018168322 -0.0018172261 -0.001818167 -0.0018198285 -0.0018220493 -0.0018247261 -0.0018279763 -0.0018312174 -0.0018340467 -0.0018362099 -0.0018372523 -0.0018368498 -0.0018349781][-0.0018174046 -0.0018167815 -0.0018169782 -0.0018178241 -0.001819443 -0.0018217703 -0.0018245373 -0.0018274508 -0.0018306166 -0.0018336177 -0.0018359461 -0.0018374281 -0.0018377198 -0.001836533 -0.0018340474][-0.0018176135 -0.0018170689 -0.0018175766 -0.0018189946 -0.00182124 -0.0018241266 -0.0018272067 -0.0018302064 -0.0018333459 -0.00183597 -0.0018377177 -0.0018385766 -0.0018384159 -0.0018369472 -0.0018341487][-0.0018178187 -0.0018174077 -0.0018182141 -0.0018201187 -0.0018228758 -0.0018261073 -0.0018293711 -0.0018324548 -0.0018354848 -0.00183767 -0.0018388216 -0.0018391957 -0.0018388267 -0.0018374406 -0.0018347836][-0.0018182929 -0.0018177668 -0.0018186207 -0.0018209252 -0.0018240794 -0.001827594 -0.0018310569 -0.001834262 -0.0018370548 -0.0018387439 -0.0018393599 -0.0018393978 -0.0018390613 -0.0018378141 -0.0018354174][-0.0018182611 -0.0018179117 -0.0018189399 -0.0018215257 -0.0018249045 -0.0018285597 -0.0018321627 -0.0018353194 -0.0018376316 -0.0018387252 -0.001838855 -0.0018387815 -0.0018384126 -0.001837259 -0.0018352345][-0.0018182425 -0.0018178234 -0.0018187859 -0.0018213768 -0.0018247574 -0.0018284967 -0.0018321715 -0.0018351297 -0.0018370879 -0.0018378403 -0.0018378571 -0.001837571 -0.0018369622 -0.0018358832 -0.001834118][-0.0018179478 -0.0018174627 -0.0018182376 -0.0018205224 -0.0018236096 -0.0018270661 -0.0018304408 -0.0018332046 -0.0018350022 -0.0018356334 -0.0018357325 -0.0018352695 -0.0018343959 -0.0018331584 -0.0018315284][-0.0018175496 -0.0018169407 -0.001817535 -0.0018194341 -0.0018220292 -0.001824855 -0.0018276478 -0.0018300756 -0.0018316617 -0.0018323254 -0.0018325095 -0.0018320045 -0.0018309255 -0.0018295787 -0.0018280323][-0.0018171577 -0.0018166223 -0.0018171737 -0.0018186209 -0.0018205426 -0.0018225956 -0.0018248033 -0.0018267642 -0.0018281089 -0.0018286883 -0.0018289298 -0.0018284644 -0.0018273115 -0.0018259317 -0.0018244832][-0.0018166804 -0.0018163061 -0.0018168633 -0.0018178821 -0.0018191031 -0.0018204777 -0.0018221313 -0.0018236146 -0.0018247012 -0.0018251193 -0.0018252669 -0.0018248355 -0.0018237031 -0.0018224014 -0.0018211545][-0.0018163347 -0.0018158946 -0.0018162297 -0.0018168781 -0.0018176408 -0.0018185249 -0.0018196882 -0.0018207739 -0.0018216975 -0.0018220637 -0.0018221566 -0.001821702 -0.0018207574 -0.0018197324 -0.0018188079][-0.001816057 -0.0018154829 -0.0018156266 -0.001815951 -0.0018163545 -0.0018168687 -0.0018175396 -0.0018181466 -0.0018187186 -0.0018190353 -0.0018191821 -0.0018189745 -0.0018184378 -0.0018179 -0.0018174157][-0.0018161596 -0.0018154453 -0.0018154716 -0.0018156361 -0.0018158432 -0.0018160659 -0.0018163617 -0.0018166049 -0.0018168224 -0.0018169455 -0.0018170278 -0.0018170256 -0.0018168172 -0.0018166471 -0.0018164745][-0.0018163265 -0.001815604 -0.0018155545 -0.0018156808 -0.0018158404 -0.001815978 -0.0018161532 -0.0018163078 -0.0018164443 -0.0018165341 -0.0018165504 -0.0018165035 -0.0018163741 -0.0018162667 -0.0018161816]]...]
INFO - root - 2017-12-10 00:17:23.471728: step 71410, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 63h:00m:45s remains)
INFO - root - 2017-12-10 00:17:32.114354: step 71420, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.835 sec/batch; 60h:32m:09s remains)
INFO - root - 2017-12-10 00:17:40.759356: step 71430, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.839 sec/batch; 60h:49m:43s remains)
INFO - root - 2017-12-10 00:17:49.355872: step 71440, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.855 sec/batch; 62h:02m:08s remains)
INFO - root - 2017-12-10 00:17:58.122512: step 71450, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.891 sec/batch; 64h:34m:59s remains)
INFO - root - 2017-12-10 00:18:06.844934: step 71460, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.872 sec/batch; 63h:12m:32s remains)
INFO - root - 2017-12-10 00:18:15.537198: step 71470, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.883 sec/batch; 64h:00m:35s remains)
INFO - root - 2017-12-10 00:18:24.234413: step 71480, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.891 sec/batch; 64h:37m:15s remains)
INFO - root - 2017-12-10 00:18:33.064438: step 71490, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 63h:02m:13s remains)
INFO - root - 2017-12-10 00:18:41.602022: step 71500, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.857 sec/batch; 62h:07m:40s remains)
2017-12-10 00:18:42.552197: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0018274169 -0.0018269266 -0.0018273352 -0.0018278926 -0.0018284974 -0.0018291703 -0.0018298824 -0.0018304739 -0.0018308242 -0.001830781 -0.001830391 -0.0018298498 -0.0018291706 -0.0018284105 -0.0018276938][-0.001827031 -0.001826963 -0.0018278438 -0.001828919 -0.0018300426 -0.0018312207 -0.0018322015 -0.0018328521 -0.0018330918 -0.0018328378 -0.0018322296 -0.0018313318 -0.0018302728 -0.0018291239 -0.0018281656][-0.0018269171 -0.0018273983 -0.0018288292 -0.001830537 -0.0018322158 -0.0018338204 -0.0018349705 -0.0018356306 -0.0018357366 -0.0018351272 -0.001834185 -0.0018329207 -0.001831486 -0.0018299758 -0.0018287115][-0.0018270665 -0.0018280402 -0.0018301138 -0.0018323906 -0.0018345193 -0.0018363417 -0.0018374054 -0.001837894 -0.0018376337 -0.0018366359 -0.0018353307 -0.0018337911 -0.0018322179 -0.0018305557 -0.001829143][-0.0018270914 -0.0018284988 -0.0018310844 -0.0018337446 -0.0018361112 -0.0018379155 -0.0018387282 -0.0018389728 -0.0018383663 -0.0018370702 -0.0018354902 -0.0018339076 -0.001832335 -0.0018306328 -0.0018292392][-0.0018267736 -0.001828539 -0.0018313548 -0.001834144 -0.0018365594 -0.0018382794 -0.0018388558 -0.0018388957 -0.0018381863 -0.0018367924 -0.001835081 -0.0018334324 -0.0018318925 -0.001830238 -0.0018289081][-0.0018262947 -0.0018281783 -0.001830944 -0.0018336182 -0.0018359162 -0.0018374504 -0.001837859 -0.0018378999 -0.0018373684 -0.0018361446 -0.0018344788 -0.0018326973 -0.0018311338 -0.0018294745 -0.0018282386][-0.0018260973 -0.0018276714 -0.0018301269 -0.0018324996 -0.0018346033 -0.0018359667 -0.0018364177 -0.0018366751 -0.0018363702 -0.001835341 -0.001833873 -0.00183211 -0.0018305144 -0.0018288407 -0.0018276606][-0.001825533 -0.0018267342 -0.0018287658 -0.0018307124 -0.0018325611 -0.0018339003 -0.0018346949 -0.0018353906 -0.0018353745 -0.001834453 -0.0018330096 -0.0018313352 -0.0018297883 -0.0018281946 -0.0018272082][-0.0018245425 -0.0018253099 -0.0018267974 -0.0018283474 -0.0018299575 -0.0018315169 -0.0018327659 -0.0018336934 -0.0018338156 -0.0018330131 -0.0018316673 -0.0018301213 -0.0018287267 -0.0018273524 -0.0018266373][-0.0018236317 -0.001823841 -0.0018247847 -0.0018259233 -0.0018272247 -0.00182879 -0.0018302851 -0.0018313669 -0.0018315173 -0.0018307592 -0.0018295232 -0.001828274 -0.0018271707 -0.0018262425 -0.0018259094][-0.0018231682 -0.0018227979 -0.0018233653 -0.0018241405 -0.0018250957 -0.0018264799 -0.0018279017 -0.0018288841 -0.0018290443 -0.0018283104 -0.0018271777 -0.0018262781 -0.0018256241 -0.0018252713 -0.0018253948][-0.00182319 -0.0018225686 -0.0018228707 -0.0018234357 -0.0018241671 -0.0018252727 -0.0018263016 -0.0018270105 -0.0018271379 -0.0018264889 -0.0018254988 -0.0018248404 -0.0018246009 -0.0018247011 -0.0018251567][-0.0018234623 -0.0018227361 -0.0018228506 -0.0018232673 -0.0018238447 -0.0018246425 -0.001825282 -0.0018256052 -0.0018256938 -0.0018252642 -0.0018246142 -0.0018242424 -0.0018242468 -0.001824592 -0.0018251509][-0.0018239198 -0.001823061 -0.0018230246 -0.0018232826 -0.0018237059 -0.0018242605 -0.0018246204 -0.0018246768 -0.0018246807 -0.0018244993 -0.0018242092 -0.0018241248 -0.0018242862 -0.001824711 -0.0018252551]]...]
INFO - root - 2017-12-10 00:18:51.100559: step 71510, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.855 sec/batch; 61h:59m:50s remains)
INFO - root - 2017-12-10 00:18:59.829523: step 71520, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.884 sec/batch; 64h:03m:34s remains)
INFO - root - 2017-12-10 00:19:08.658668: step 71530, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.898 sec/batch; 65h:04m:20s remains)
INFO - root - 2017-12-10 00:19:17.199610: step 71540, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.859 sec/batch; 62h:17m:44s remains)
INFO - root - 2017-12-10 00:19:25.757697: step 71550, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.853 sec/batch; 61h:49m:33s remains)
INFO - root - 2017-12-10 00:19:34.385592: step 71560, loss = 0.81, batch loss = 0.68 (9.1 examples/sec; 0.878 sec/batch; 63h:38m:43s remains)
INFO - root - 2017-12-10 00:19:43.059527: step 71570, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 62h:56m:52s remains)
INFO - root - 2017-12-10 00:19:51.727286: step 71580, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.846 sec/batch; 61h:18m:28s remains)
INFO - root - 2017-12-10 00:20:00.461582: step 71590, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.878 sec/batch; 63h:37m:55s remains)
INFO - root - 2017-12-10 00:20:08.838910: step 71600, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.850 sec/batch; 61h:36m:48s remains)
2017-12-10 00:20:09.733231: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0018116968 -0.0018107742 -0.0018120776 -0.001814487 -0.0018172426 -0.001819734 -0.0018212865 -0.0018221404 -0.0018225055 -0.0018228556 -0.001823443 -0.0018242616 -0.0018254415 -0.0018267011 -0.0018277597][-0.0018098766 -0.0018087499 -0.0018100376 -0.0018125525 -0.0018155403 -0.0018181631 -0.0018197391 -0.001820486 -0.001820597 -0.0018207739 -0.0018212611 -0.0018220982 -0.0018233523 -0.0018248974 -0.0018263585][-0.0018121846 -0.0018109366 -0.001811904 -0.0018141028 -0.00181691 -0.0018193006 -0.0018207257 -0.0018213819 -0.0018213253 -0.0018212165 -0.001821429 -0.0018220557 -0.0018231302 -0.001824706 -0.001826342][-0.0018167 -0.0018150603 -0.00181535 -0.0018167187 -0.0018188334 -0.0018208289 -0.0018222259 -0.0018229989 -0.0018229613 -0.0018227252 -0.0018226706 -0.0018228835 -0.0018235078 -0.001824878 -0.0018265325][-0.0018200889 -0.0018180228 -0.0018176028 -0.0018181393 -0.0018196082 -0.0018213446 -0.0018228898 -0.0018240161 -0.0018242495 -0.0018240471 -0.0018236825 -0.0018232473 -0.001823131 -0.0018240068 -0.0018254382][-0.0018219468 -0.0018197122 -0.001818802 -0.0018186359 -0.0018195719 -0.001821086 -0.0018228042 -0.0018241911 -0.0018247581 -0.0018248156 -0.0018243825 -0.001823515 -0.0018227781 -0.0018229565 -0.0018237127][-0.0018223297 -0.0018199236 -0.0018185895 -0.0018177526 -0.0018181439 -0.0018193878 -0.0018212064 -0.0018228577 -0.0018238019 -0.0018242055 -0.0018239841 -0.0018229466 -0.0018217904 -0.0018214014 -0.0018215589][-0.0018204388 -0.0018179404 -0.0018163349 -0.0018151997 -0.0018153534 -0.0018163794 -0.0018180411 -0.0018196822 -0.0018208397 -0.0018215146 -0.0018214532 -0.0018204644 -0.0018192612 -0.0018186255 -0.0018183994][-0.0018175731 -0.0018150475 -0.0018135161 -0.0018124075 -0.0018124141 -0.0018131388 -0.0018143984 -0.0018157038 -0.0018166393 -0.0018173819 -0.0018175463 -0.0018169302 -0.001816021 -0.0018154763 -0.001815159][-0.0018143629 -0.0018120405 -0.0018107645 -0.0018098301 -0.0018098253 -0.0018102936 -0.0018111629 -0.0018120315 -0.0018127125 -0.0018133367 -0.0018135913 -0.001813397 -0.0018129555 -0.0018126601 -0.0018123785][-0.0018123558 -0.0018101564 -0.0018090364 -0.0018083494 -0.0018082485 -0.001808445 -0.0018088898 -0.0018093484 -0.0018096678 -0.0018100822 -0.0018103218 -0.0018104916 -0.0018105402 -0.0018105307 -0.0018104067][-0.0018111977 -0.0018091771 -0.0018081404 -0.0018074282 -0.0018072178 -0.0018071748 -0.0018073743 -0.0018076362 -0.0018077927 -0.0018080064 -0.0018081961 -0.0018084447 -0.0018087057 -0.0018088965 -0.001808951][-0.0018107186 -0.0018089665 -0.0018079459 -0.0018071685 -0.0018067157 -0.0018064989 -0.0018065056 -0.0018066231 -0.001806639 -0.0018067427 -0.0018068652 -0.0018071226 -0.0018074575 -0.0018077003 -0.0018078553][-0.0018107838 -0.0018091905 -0.0018081912 -0.0018073986 -0.0018067806 -0.0018063405 -0.0018061688 -0.0018062085 -0.001806203 -0.0018062718 -0.0018063526 -0.0018065345 -0.0018068206 -0.0018070326 -0.001807177][-0.0018111966 -0.001809805 -0.001808762 -0.0018078662 -0.0018071134 -0.0018065744 -0.001806307 -0.0018062901 -0.0018063025 -0.001806341 -0.0018063633 -0.0018064349 -0.0018066432 -0.0018068061 -0.0018069388]]...]
INFO - root - 2017-12-10 00:20:18.257528: step 71610, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.856 sec/batch; 62h:00m:38s remains)
INFO - root - 2017-12-10 00:20:26.801850: step 71620, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.831 sec/batch; 60h:12m:32s remains)
INFO - root - 2017-12-10 00:20:35.321644: step 71630, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.839 sec/batch; 60h:49m:54s remains)
INFO - root - 2017-12-10 00:20:43.775767: step 71640, loss = 0.81, batch loss = 0.68 (9.4 examples/sec; 0.851 sec/batch; 61h:40m:52s remains)
INFO - root - 2017-12-10 00:20:52.320638: step 71650, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.837 sec/batch; 60h:39m:17s remains)
INFO - root - 2017-12-10 00:21:00.877734: step 71660, loss = 0.82, batch loss = 0.69 (9.7 examples/sec; 0.828 sec/batch; 59h:57m:44s remains)
INFO - root - 2017-12-10 00:21:09.668664: step 71670, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.855 sec/batch; 61h:56m:47s remains)
INFO - root - 2017-12-10 00:21:18.323404: step 71680, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.870 sec/batch; 63h:01m:26s remains)
INFO - root - 2017-12-10 00:21:26.968014: step 71690, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 63h:07m:08s remains)
INFO - root - 2017-12-10 00:21:35.278234: step 71700, loss = 0.82, batch loss = 0.69 (10.3 examples/sec; 0.774 sec/batch; 56h:06m:19s remains)
2017-12-10 00:21:36.173277: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0015679754 -0.0010208247 0.00023601425 0.002092889 0.0042272438 0.0064083007 0.0084925042 0.0098949475 0.0099494988 0.0085551981 0.0061405986 0.0034125764 0.0010251269 -0.00060366164 -0.0014381793][-0.0014848602 -0.00072870275 0.0010969645 0.0040778136 0.0078544756 0.011949524 0.015801203 0.018356467 0.018495217 0.016131757 0.011982592 0.0073121744 0.0032448592 0.00043368957 -0.0010597856][-0.0013943064 -0.00030139473 0.0023575034 0.0068786093 0.012905561 0.019580152 0.025710974 0.029782098 0.030177981 0.026707275 0.020356288 0.01308869 0.0066794888 0.0021329694 -0.00039341953][-0.0011580293 0.00055956689 0.0045974031 0.011444954 0.020548446 0.030489817 0.039101206 0.044422496 0.04454254 0.039412133 0.030399881 0.020134443 0.011028764 0.0044214521 0.00058371236][-0.00056007644 0.0020225071 0.0078134444 0.017229512 0.029497519 0.042611882 0.053629026 0.060137797 0.05995167 0.053110134 0.041330058 0.027890518 0.015859326 0.0070064249 0.0017199347][0.00055186881 0.0040680119 0.011512011 0.023095535 0.037682667 0.052803483 0.065122232 0.072016209 0.071326084 0.063205421 0.049519017 0.033846185 0.019672407 0.0091217533 0.0026833671][0.0019755405 0.0062586949 0.014676384 0.027209176 0.042475734 0.057867534 0.070064895 0.076464936 0.075163 0.066389121 0.052046131 0.035689112 0.020870032 0.0098063471 0.002997173][0.0031179739 0.007617584 0.015887151 0.027655698 0.041543748 0.055195127 0.06579712 0.071031988 0.069315538 0.060839847 0.047410358 0.032267541 0.018658377 0.0085773729 0.0024270397][0.0032606036 0.0071483343 0.013990133 0.023408022 0.034255702 0.044747014 0.052802097 0.056618504 0.054911748 0.047806509 0.036818851 0.024601385 0.013790083 0.0059125079 0.0012270826][0.0021822485 0.0049286126 0.0095951017 0.015869314 0.022970272 0.029768158 0.034988169 0.037402008 0.036084533 0.031081311 0.023486452 0.015181488 0.0079753809 0.0028512878 -7.7237375e-05][0.00037870149 0.0018439043 0.0043030242 0.0076064044 0.011366862 0.015004677 0.01784292 0.019165514 0.018369911 0.015506173 0.01125673 0.0067367447 0.0029293839 0.00032553088 -0.001071926][-0.001060878 -0.000493011 0.00045096513 0.0017394359 0.0032494492 0.0047626854 0.0059915674 0.0065888581 0.0062419125 0.0049916026 0.0031858743 0.0013385735 -0.00015543366 -0.0011176768 -0.0015872901][-0.0017335537 -0.0016281877 -0.001430089 -0.0011235249 -0.00072052504 -0.00027657242 0.00011435209 0.00032320817 0.00023493415 -0.00012359954 -0.00062082289 -0.0011017642 -0.0014658956 -0.001677083 -0.0017634442][-0.0018043845 -0.0018016619 -0.0017933513 -0.0017713009 -0.0017269998 -0.0016595941 -0.0015878554 -0.0015430483 -0.0015481446 -0.0015959246 -0.0016619902 -0.0017240064 -0.0017662426 -0.0017864793 -0.00179242][-0.0018027119 -0.0018016793 -0.0018006953 -0.0017986252 -0.0017958652 -0.0017925471 -0.0017895979 -0.0017884173 -0.0017889838 -0.001790894 -0.0017933424 -0.0017959274 -0.0017975976 -0.0017980525 -0.0017979199]]...]
INFO - root - 2017-12-10 00:21:44.539850: step 71710, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.854 sec/batch; 61h:51m:37s remains)
INFO - root - 2017-12-10 00:21:53.103662: step 71720, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 62h:52m:15s remains)
INFO - root - 2017-12-10 00:22:01.685250: step 71730, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.832 sec/batch; 60h:15m:59s remains)
INFO - root - 2017-12-10 00:22:10.063528: step 71740, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.857 sec/batch; 62h:04m:51s remains)
INFO - root - 2017-12-10 00:22:18.505018: step 71750, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.877 sec/batch; 63h:32m:28s remains)
INFO - root - 2017-12-10 00:22:27.047721: step 71760, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.878 sec/batch; 63h:35m:29s remains)
INFO - root - 2017-12-10 00:22:35.543690: step 71770, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.858 sec/batch; 62h:07m:41s remains)
INFO - root - 2017-12-10 00:22:44.196584: step 71780, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.904 sec/batch; 65h:26m:01s remains)
INFO - root - 2017-12-10 00:22:52.920108: step 71790, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 62h:57m:20s remains)
INFO - root - 2017-12-10 00:23:01.653597: step 71800, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.870 sec/batch; 62h:58m:34s remains)
2017-12-10 00:23:02.547518: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0018177785 -0.0018165 -0.0018163176 -0.0018166393 -0.0018173429 -0.0018181344 -0.0018184317 -0.0018180702 -0.0018172193 -0.0018159605 -0.0018148184 -0.001814172 -0.0018140828 -0.0018142586 -0.0018146731][-0.0018171455 -0.0018159087 -0.0018157037 -0.0018158349 -0.0018162038 -0.001816668 -0.0018168287 -0.0018164462 -0.0018157862 -0.0018147909 -0.0018140244 -0.001813718 -0.0018138798 -0.0018141746 -0.0018146487][-0.0018164736 -0.0018155242 -0.0018155919 -0.0018158731 -0.0018162404 -0.0018166424 -0.0018167299 -0.0018162522 -0.0018155681 -0.001814645 -0.0018140011 -0.0018138621 -0.0018142437 -0.0018146798 -0.0018152334][-0.0018161079 -0.0018153313 -0.0018155897 -0.0018160581 -0.0018165357 -0.0018170109 -0.001817088 -0.0018165888 -0.0018159 -0.0018150058 -0.001814299 -0.0018140734 -0.0018144579 -0.0018149688 -0.0018155359][-0.0018158368 -0.001815257 -0.0018156186 -0.0018162197 -0.0018166511 -0.0018170057 -0.0018170147 -0.001816446 -0.001815744 -0.0018148891 -0.0018141478 -0.0018138693 -0.0018142753 -0.0018149019 -0.0018155295][-0.0018158747 -0.0018153007 -0.0018155581 -0.0018160887 -0.0018164356 -0.0018167135 -0.0018167367 -0.0018161652 -0.0018154006 -0.0018144075 -0.0018134316 -0.0018131296 -0.0018135551 -0.0018143315 -0.0018150838][-0.0018167316 -0.001816107 -0.001816074 -0.0018162236 -0.0018162257 -0.0018163004 -0.0018162079 -0.0018156135 -0.0018148507 -0.0018139416 -0.0018128795 -0.0018124047 -0.0018127722 -0.0018135341 -0.0018142849][-0.0018186641 -0.0018178272 -0.0018173882 -0.0018170446 -0.0018166135 -0.0018163339 -0.0018160146 -0.0018154873 -0.0018149148 -0.0018143168 -0.0018133 -0.0018127435 -0.0018129473 -0.0018134298 -0.0018139966][-0.0018213292 -0.0018202467 -0.001819357 -0.001818481 -0.0018176209 -0.0018169935 -0.0018165016 -0.0018161009 -0.0018157883 -0.0018153646 -0.0018145093 -0.0018140135 -0.0018140828 -0.0018143206 -0.0018146932][-0.0018247626 -0.0018235443 -0.0018224316 -0.0018212809 -0.0018202278 -0.0018193811 -0.0018188679 -0.0018187717 -0.0018187193 -0.0018183884 -0.0018176669 -0.0018171009 -0.0018168786 -0.0018167322 -0.0018167137][-0.0018286181 -0.0018271293 -0.001825834 -0.0018245818 -0.0018234804 -0.0018226028 -0.0018220945 -0.00182207 -0.0018219936 -0.0018216791 -0.0018209823 -0.0018204277 -0.0018202389 -0.0018200224 -0.0018197345][-0.0018317782 -0.0018300333 -0.0018285351 -0.0018270927 -0.0018258212 -0.0018248332 -0.0018241111 -0.0018238977 -0.0018238039 -0.0018235012 -0.0018228922 -0.0018224788 -0.0018224342 -0.0018223772 -0.0018220542][-0.0018339299 -0.0018317042 -0.001829598 -0.0018275545 -0.001825738 -0.0018242118 -0.0018229557 -0.0018223575 -0.0018220822 -0.0018217746 -0.0018213794 -0.0018212942 -0.0018216477 -0.001822088 -0.0018222426][-0.0018351609 -0.0018327031 -0.0018302243 -0.0018275358 -0.0018249424 -0.0018224537 -0.001820087 -0.0018183158 -0.0018170725 -0.0018162302 -0.0018157981 -0.0018160449 -0.001816945 -0.0018181595 -0.0018191728][-0.0018354368 -0.0018330883 -0.0018305068 -0.001827477 -0.0018241411 -0.0018205253 -0.0018168269 -0.0018136979 -0.0018111875 -0.0018095141 -0.0018087602 -0.001809145 -0.0018105839 -0.0018126772 -0.0018148726]]...]
INFO - root - 2017-12-10 00:23:11.119315: step 71810, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.865 sec/batch; 62h:37m:03s remains)
INFO - root - 2017-12-10 00:23:19.923227: step 71820, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.887 sec/batch; 64h:14m:03s remains)
INFO - root - 2017-12-10 00:23:28.781205: step 71830, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.889 sec/batch; 64h:22m:28s remains)
INFO - root - 2017-12-10 00:23:37.342761: step 71840, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 62h:49m:35s remains)
INFO - root - 2017-12-10 00:23:46.002142: step 71850, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.865 sec/batch; 62h:36m:57s remains)
INFO - root - 2017-12-10 00:23:54.752041: step 71860, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.887 sec/batch; 64h:11m:00s remains)
INFO - root - 2017-12-10 00:24:03.232309: step 71870, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.865 sec/batch; 62h:39m:05s remains)
INFO - root - 2017-12-10 00:24:11.895966: step 71880, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.863 sec/batch; 62h:28m:23s remains)
INFO - root - 2017-12-10 00:24:20.504015: step 71890, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.852 sec/batch; 61h:40m:00s remains)
INFO - root - 2017-12-10 00:24:29.125409: step 71900, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.859 sec/batch; 62h:11m:42s remains)
2017-12-10 00:24:30.040951: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.00977386 0.010301948 0.010976878 0.01158739 0.011758857 0.011142086 0.0096980073 0.0076352493 0.005587467 0.0039388747 0.0028416086 0.0020729662 0.0014757559 0.00088229345 0.00024733285][0.00820962 0.00916475 0.010439582 0.011675371 0.012355349 0.012008967 0.010547323 0.008079296 0.005420879 0.0031878892 0.0017732404 0.00095643906 0.00048590463 0.00010527449 -0.00030567718][0.0059511228 0.0073926891 0.0093377735 0.011286356 0.012585007 0.01266033 0.011385954 0.0087279668 0.0055875345 0.0027515865 0.0009084848 -6.5160915e-05 -0.00049194694 -0.00071549939 -0.000933445][0.003720718 0.005572001 0.0081099747 0.010735563 0.012677072 0.01327965 0.012389429 0.00981432 0.0063953693 0.0030204034 0.00066080561 -0.00060965971 -0.0011364176 -0.0013224118 -0.0014264042][0.0019336949 0.0040009897 0.006934932 0.010100584 0.012650609 0.013889325 0.01361103 0.011441882 0.0079981657 0.0041824891 0.0012159204 -0.0005225878 -0.0013165099 -0.0015989796 -0.0016875225][0.00078706734 0.0028415113 0.0059129391 0.00942957 0.01253332 0.014500364 0.015018021 0.01351919 0.010268155 0.0061104684 0.0024739518 0.00012232165 -0.0010846307 -0.001579351 -0.0017411702][0.0001703006 0.0020485506 0.0050415313 0.0087131392 0.012240303 0.014917656 0.016269786 0.015551842 0.012643739 0.0083031328 0.004089688 0.0011263768 -0.00055573485 -0.0013440397 -0.0016538398][-0.0001071546 0.0015121211 0.0042650294 0.0078917025 0.011658145 0.014883577 0.016910039 0.016851613 0.014315045 0.010015734 0.0055237776 0.0021484704 6.3820626e-05 -0.001024761 -0.0015134809][-0.00031609752 0.0010277823 0.0034200768 0.006767184 0.010476593 0.013893261 0.016253931 0.016615039 0.014486098 0.01054575 0.0062522949 0.0028540534 0.00058870285 -0.0007109337 -0.0013577726][-0.00059930061 0.00044746662 0.002373239 0.0052077314 0.0085140914 0.011680746 0.013956578 0.014463699 0.012787706 0.0095390892 0.0059237792 0.0029323855 0.00078528177 -0.00055087113 -0.001267119][-0.00096174743 -0.00021959096 0.0011779483 0.0033205315 0.0059299045 0.0084779095 0.010320751 0.010767591 0.0095683727 0.0072281198 0.0045964965 0.002320495 0.00055827445 -0.00061984686 -0.0012862254][-0.0013419921 -0.00088268373 1.2077624e-05 0.0014358115 0.0032353252 0.00499963 0.0062619331 0.0065593361 0.00580195 0.00435892 0.0027221232 0.0012328591 -1.2160279e-05 -0.00089366653 -0.0014071125][-0.0016241801 -0.0013961546 -0.00091942138 -0.0001158173 0.000948022 0.0019947514 0.0027340017 0.0029051825 0.0025003687 0.0017492058 0.00088472955 5.038816e-05 -0.00069819053 -0.0012488272 -0.0015694379][-0.0017790736 -0.0016970206 -0.0015033572 -0.0011500119 -0.00064954534 -0.00014364405 0.00022172986 0.0003205383 0.00016114453 -0.00014282844 -0.00051147584 -0.00089491 -0.0012630357 -0.0015403046 -0.0016992725][-0.0018230742 -0.0018091821 -0.0017604068 -0.0016546405 -0.0014837616 -0.0012988554 -0.001157222 -0.0011087363 -0.0011479813 -0.0012331416 -0.0013499333 -0.0014820695 -0.0016175609 -0.0017201588 -0.0017771623]]...]
INFO - root - 2017-12-10 00:24:38.642791: step 71910, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.879 sec/batch; 63h:37m:39s remains)
INFO - root - 2017-12-10 00:24:47.319703: step 71920, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.876 sec/batch; 63h:22m:33s remains)
INFO - root - 2017-12-10 00:24:56.211795: step 71930, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.892 sec/batch; 64h:35m:10s remains)
INFO - root - 2017-12-10 00:25:04.837580: step 71940, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.842 sec/batch; 60h:58m:10s remains)
INFO - root - 2017-12-10 00:25:13.551565: step 71950, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 62h:48m:36s remains)
INFO - root - 2017-12-10 00:25:22.394804: step 71960, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 62h:21m:01s remains)
INFO - root - 2017-12-10 00:25:30.962076: step 71970, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.874 sec/batch; 63h:14m:38s remains)
INFO - root - 2017-12-10 00:25:39.674758: step 71980, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.877 sec/batch; 63h:27m:50s remains)
INFO - root - 2017-12-10 00:25:48.400161: step 71990, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.882 sec/batch; 63h:47m:40s remains)
INFO - root - 2017-12-10 00:25:56.890080: step 72000, loss = 0.82, batch loss = 0.69 (9.9 examples/sec; 0.807 sec/batch; 58h:22m:49s remains)
2017-12-10 00:25:57.770623: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.37476638 0.36544842 0.35345861 0.3407034 0.32704857 0.3155199 0.30385223 0.29222438 0.28131527 0.273323 0.27107137 0.27836373 0.29741421 0.32592517 0.35820261][0.38611075 0.37851939 0.36832613 0.3573398 0.34616774 0.33598995 0.32584712 0.31632289 0.30670986 0.29999363 0.2972295 0.30243084 0.31829736 0.34300077 0.37182364][0.39450651 0.38958737 0.38140565 0.37177703 0.36232668 0.35365906 0.3452374 0.33785769 0.33065662 0.32529205 0.3227087 0.32643294 0.33825159 0.3571 0.3797363][0.401736 0.39962706 0.3941 0.3866595 0.37901247 0.37185442 0.36557555 0.35986257 0.35434696 0.350286 0.34815511 0.35050422 0.35829103 0.370315 0.38569263][0.41068926 0.41168174 0.4083688 0.4033016 0.3978022 0.39205533 0.38710287 0.38246769 0.37846279 0.37466592 0.37156034 0.37223092 0.37528759 0.38148171 0.38892275][0.41729155 0.42205286 0.42140031 0.41889864 0.41559407 0.41126394 0.40768978 0.40438521 0.40151396 0.39855349 0.39459285 0.39234653 0.39081815 0.39035836 0.39008307][0.41849309 0.42780447 0.43003622 0.42979509 0.42856047 0.42596415 0.42378324 0.4213331 0.41907221 0.41651174 0.412402 0.40869331 0.40337038 0.397655 0.39108223][0.41224977 0.4255552 0.4307189 0.43337604 0.43480641 0.43414047 0.43303153 0.43047825 0.42807773 0.42472115 0.41952372 0.41431764 0.40685526 0.39763308 0.3866016][0.39935589 0.41575474 0.42388472 0.42986724 0.43438724 0.43592855 0.43607149 0.43379161 0.43070018 0.42632058 0.42003408 0.41390687 0.40563038 0.3946099 0.38141978][0.38138 0.39990893 0.40997329 0.41907632 0.42665645 0.43015096 0.43201596 0.43053958 0.42748624 0.42174906 0.41449636 0.40772822 0.39928308 0.38827667 0.37514731][0.35605523 0.37733385 0.38951695 0.40000477 0.40927303 0.41420272 0.41757676 0.41764668 0.41583225 0.41085884 0.40416828 0.39734092 0.38961464 0.37882021 0.36675173][0.32947072 0.35224324 0.366403 0.37855783 0.38892862 0.3940759 0.39739913 0.39843574 0.3973259 0.39272469 0.38676932 0.38088223 0.37468454 0.365915 0.35624063][0.30410114 0.3281337 0.34301841 0.35614327 0.36721206 0.37236729 0.37535751 0.37605232 0.37537009 0.37107015 0.36549434 0.36026981 0.35569772 0.34927255 0.34253445][0.28191891 0.30603275 0.32153302 0.33485085 0.34525716 0.35012722 0.35300893 0.35424179 0.35411268 0.3503398 0.34561744 0.34161934 0.3387723 0.33458844 0.3305425][0.25919524 0.28384688 0.3004517 0.31500345 0.32577676 0.33069074 0.33305705 0.3342728 0.33394796 0.33032778 0.32624954 0.3229661 0.3213504 0.31936207 0.3180142]]...]
INFO - root - 2017-12-10 00:26:06.195701: step 72010, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.858 sec/batch; 62h:05m:35s remains)
INFO - root - 2017-12-10 00:26:14.759490: step 72020, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.861 sec/batch; 62h:18m:48s remains)
INFO - root - 2017-12-10 00:26:23.499837: step 72030, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 62h:32m:05s remains)
INFO - root - 2017-12-10 00:26:31.961172: step 72040, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.870 sec/batch; 62h:56m:25s remains)
INFO - root - 2017-12-10 00:26:40.620348: step 72050, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.849 sec/batch; 61h:25m:58s remains)
INFO - root - 2017-12-10 00:26:49.261448: step 72060, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.836 sec/batch; 60h:28m:40s remains)
INFO - root - 2017-12-10 00:26:58.071042: step 72070, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.865 sec/batch; 62h:34m:28s remains)
INFO - root - 2017-12-10 00:27:06.734372: step 72080, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.890 sec/batch; 64h:23m:45s remains)
INFO - root - 2017-12-10 00:27:15.382271: step 72090, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.881 sec/batch; 63h:44m:14s remains)
INFO - root - 2017-12-10 00:27:24.096380: step 72100, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.852 sec/batch; 61h:37m:17s remains)
2017-12-10 00:27:25.055671: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.059114706 0.075007334 0.090161718 0.10173222 0.10714806 0.10762522 0.10354038 0.097456321 0.091092773 0.08621081 0.082777835 0.079645135 0.075543344 0.069360562 0.059732769][0.081999309 0.1057995 0.12903641 0.14759122 0.15763094 0.16009471 0.15564577 0.14789143 0.13945158 0.13264365 0.12773778 0.12302532 0.11673346 0.10720541 0.0925564][0.1040609 0.13662022 0.16916242 0.19602054 0.21247384 0.21876231 0.21545686 0.20684499 0.19643919 0.18717961 0.17969924 0.17226394 0.16282515 0.14936942 0.12953597][0.12123186 0.16195032 0.20348254 0.2392398 0.26329568 0.27534392 0.27580959 0.26886147 0.25816366 0.24711557 0.2370449 0.22599584 0.21212325 0.19340529 0.16745269][0.12877986 0.17556792 0.22396639 0.26683131 0.29788479 0.3163107 0.32213089 0.31908113 0.31031522 0.29921883 0.28756705 0.27360049 0.25588724 0.23248383 0.20127539][0.12652455 0.17588191 0.22787791 0.27517506 0.31122035 0.33492085 0.34616074 0.34786409 0.34254247 0.33328909 0.32165533 0.30612588 0.28586984 0.25908747 0.22419253][0.11738587 0.16590853 0.21807301 0.26681912 0.3056671 0.33305666 0.34835714 0.35416764 0.352491 0.34583694 0.33572239 0.32053337 0.2996307 0.2714133 0.23474656][0.10422223 0.14938845 0.1990499 0.24652664 0.28577265 0.31482503 0.33261114 0.341238 0.34218487 0.33811802 0.33018929 0.31671894 0.29696044 0.26921314 0.23268192][0.088190667 0.12862806 0.17406005 0.2184042 0.25591555 0.28430519 0.30236965 0.31193778 0.3140147 0.31136099 0.30511817 0.29359049 0.27576619 0.24999547 0.21558782][0.070216261 0.10454172 0.14417794 0.18364479 0.21773073 0.24375317 0.26031151 0.26900423 0.27084059 0.26864856 0.26347169 0.2538738 0.23860192 0.21598674 0.18543024][0.052429531 0.079584986 0.1119271 0.14494029 0.17398354 0.19636193 0.21045177 0.21747179 0.21833773 0.215894 0.21128567 0.20327064 0.19067448 0.17199971 0.14673045][0.035936896 0.055713084 0.080021754 0.10556223 0.12863594 0.14674956 0.15807961 0.16330424 0.16313946 0.1602065 0.15568021 0.14880329 0.13873273 0.1243116 0.10517575][0.021513011 0.034652431 0.051228855 0.069158234 0.085860491 0.099381045 0.10803243 0.11190812 0.11138143 0.10847058 0.10418011 0.0982148 0.0902302 0.07970167 0.066428125][0.010033677 0.017694902 0.027681524 0.0388583 0.049593609 0.058621638 0.064650923 0.067436263 0.067104936 0.064887688 0.061505221 0.056839839 0.050965324 0.043790277 0.035377253][0.0025883922 0.0061447979 0.011047171 0.016770292 0.022512289 0.027617591 0.031284183 0.033171333 0.033272184 0.032205295 0.030301675 0.027504286 0.024016554 0.01991895 0.015366578]]...]
INFO - root - 2017-12-10 00:27:33.467746: step 72110, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.887 sec/batch; 64h:10m:22s remains)
INFO - root - 2017-12-10 00:27:42.258845: step 72120, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.854 sec/batch; 61h:46m:46s remains)
INFO - root - 2017-12-10 00:27:50.903789: step 72130, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 62h:51m:41s remains)
INFO - root - 2017-12-10 00:27:59.365178: step 72140, loss = 0.83, batch loss = 0.70 (9.2 examples/sec; 0.869 sec/batch; 62h:50m:55s remains)
INFO - root - 2017-12-10 00:28:08.074549: step 72150, loss = 0.82, batch loss = 0.69 (8.8 examples/sec; 0.912 sec/batch; 65h:55m:10s remains)
INFO - root - 2017-12-10 00:28:16.699347: step 72160, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.888 sec/batch; 64h:14m:11s remains)
INFO - root - 2017-12-10 00:28:25.492129: step 72170, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.896 sec/batch; 64h:45m:52s remains)
INFO - root - 2017-12-10 00:28:34.270899: step 72180, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.889 sec/batch; 64h:15m:41s remains)
INFO - root - 2017-12-10 00:28:43.071589: step 72190, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.848 sec/batch; 61h:18m:28s remains)
INFO - root - 2017-12-10 00:28:51.693406: step 72200, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.858 sec/batch; 62h:00m:08s remains)
2017-12-10 00:28:52.636797: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0017822378 -0.0017719308 -0.001772531 -0.0016330569 -0.0015723457 -0.0015096486 -0.0014951626 -0.0015068504 -0.0015377793 -0.0015508669 -0.0015735562 -0.0016021703 -0.0016398674 -0.0016867812 -0.001740523][-0.0016989594 -0.0016773437 -0.0016804603 -0.0015890878 -0.0015644047 -0.0014886796 -0.001443044 -0.001446223 -0.0014506114 -0.0014528658 -0.0014848036 -0.0015411987 -0.0016081971 -0.0016766621 -0.0017393624][-0.0015848051 -0.0015889609 -0.0016021384 -0.0015247886 -0.0014961723 -0.0014248396 -0.0013725373 -0.0013433208 -0.0013173418 -0.0013136422 -0.0013580839 -0.0014406596 -0.0015470857 -0.0016545287 -0.0017391234][-0.0012060015 -0.0012043216 -0.001226539 -0.0012123435 -0.0011960653 -0.0010898676 -0.00097960467 -0.00089574576 -0.00083384244 -0.00084197661 -0.000901075 -0.0010118327 -0.0011789476 -0.0013831785 -0.0015739845][-0.0005055489 -0.00050042314 -0.00048686005 -0.00038572703 -0.00027433084 -0.00014366512 -1.6586855e-06 0.00011374836 0.00023993 0.00026723079 0.00019518647 -2.2512046e-05 -0.00034261961 -0.00074599718 -0.0011564989][0.00063050294 0.00067644438 0.00072209851 0.00087322423 0.001065974 0.001290897 0.0015082845 0.0016634424 0.0018279055 0.0018353647 0.001710646 0.0013182684 0.00078908552 0.00011455116 -0.00057511881][0.0017071144 0.0017973421 0.0018782819 0.0020431331 0.0022416026 0.0024779714 0.0027263016 0.0029057311 0.003084348 0.0030216635 0.0027670967 0.0022217152 0.0015208492 0.00066264847 -0.00019538682][0.0023851972 0.0024297871 0.002463276 0.0025597569 0.0026906813 0.0028718123 0.0031047151 0.003324551 0.0035337596 0.0034106006 0.0030334406 0.0023343721 0.0014959894 0.0005773938 -0.00028870907][0.0025589806 0.0023782989 0.0022124853 0.0021418417 0.0021513337 0.0022655618 0.0024714232 0.0027158973 0.0029206984 0.0027762903 0.0023391126 0.0016018163 0.00076253118 -6.9520669e-05 -0.00077468366][0.0023316988 0.0017597455 0.0012904623 0.0010293644 0.00094332348 0.0010113626 0.0011817074 0.0014081773 0.001571081 0.0014471289 0.001052071 0.00042962877 -0.00024441234 -0.00084028288 -0.0012883518][0.0016660151 0.00078850251 0.00013632153 -0.00021493458 -0.00033721898 -0.00029984198 -0.00017829228 -2.1284795e-05 7.1171788e-05 -2.3355475e-05 -0.00030107063 -0.00070137763 -0.0011012311 -0.0014137462 -0.0016199587][0.00055409351 -0.00029552216 -0.00085807138 -0.0011363068 -0.0012299367 -0.0012189802 -0.0011597639 -0.0010818215 -0.0010411926 -0.0010892327 -0.0012245553 -0.0014066703 -0.0015752366 -0.0016926238 -0.0017612494][-0.00053088833 -0.0011224526 -0.0014512208 -0.0015945589 -0.0016396218 -0.001637727 -0.0016161583 -0.001588089 -0.0015759366 -0.0015952336 -0.0016443477 -0.001704846 -0.0017563388 -0.0017875237 -0.0018035774][-0.0012766975 -0.001573747 -0.0017093429 -0.0017607246 -0.0017767589 -0.0017779188 -0.0017721007 -0.0017643071 -0.0017615365 -0.0017669562 -0.0017796725 -0.0017935456 -0.0018042012 -0.001809602 -0.0018130967][-0.0016321359 -0.0017492216 -0.0017910246 -0.001806165 -0.0018119547 -0.0018132165 -0.0018111224 -0.0018084398 -0.0018071912 -0.0018079273 -0.0018098855 -0.0018116628 -0.0018132356 -0.0018142853 -0.001815942]]...]
INFO - root - 2017-12-10 00:29:01.051976: step 72210, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 62h:59m:14s remains)
INFO - root - 2017-12-10 00:29:09.710851: step 72220, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.876 sec/batch; 63h:18m:55s remains)
INFO - root - 2017-12-10 00:29:18.434769: step 72230, loss = 0.83, batch loss = 0.70 (9.0 examples/sec; 0.884 sec/batch; 63h:56m:32s remains)
INFO - root - 2017-12-10 00:29:26.855607: step 72240, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 62h:39m:15s remains)
INFO - root - 2017-12-10 00:29:35.550327: step 72250, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.891 sec/batch; 64h:25m:26s remains)
INFO - root - 2017-12-10 00:29:44.187176: step 72260, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.859 sec/batch; 62h:03m:49s remains)
INFO - root - 2017-12-10 00:29:52.753784: step 72270, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.843 sec/batch; 60h:55m:03s remains)
INFO - root - 2017-12-10 00:30:01.396641: step 72280, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.876 sec/batch; 63h:19m:04s remains)
INFO - root - 2017-12-10 00:30:10.091904: step 72290, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.870 sec/batch; 62h:50m:58s remains)
INFO - root - 2017-12-10 00:30:18.806968: step 72300, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.860 sec/batch; 62h:08m:33s remains)
2017-12-10 00:30:19.660455: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.00099800515 -0.00087154307 -0.00082933495 -0.0009282484 -0.00095650548 -0.00081222528 -0.00064589141 -0.00055315322 -0.00024303456 7.4475305e-05 0.00024285784 0.00026386662 -0.00012576743 -0.00073821936 -0.0011357681][-0.00058573845 -0.00061719224 -0.00069581973 -0.00079948723 -0.00074890233 -0.00054717471 -0.00043986959 -0.00049664546 -0.00030269555 -4.2636879e-05 9.9308207e-05 7.0377835e-05 -0.00027650688 -0.0007698118 -0.0010607413][0.00010187377 -0.00016637414 -0.00041687966 -0.00055512064 -0.000413939 -0.00017336057 -0.00022263953 -0.00047776231 -0.00032350211 -0.00015342934 -6.9213333e-05 -0.00014678447 -0.00040035765 -0.00076007575 -0.0010398503][0.0010650688 0.00060520449 0.00020910229 2.6052934e-05 0.00016587216 0.00034476921 0.00022560579 -8.5225329e-05 2.4318229e-05 8.16026e-05 2.8701616e-05 -0.00019042823 -0.00041982788 -0.00058006996 -0.00077581231][0.0021613459 0.0017070837 0.0012419714 0.00095784815 0.0010164451 0.0011559437 0.0010479979 0.0007331575 0.00059953064 0.00031687517 -4.1435705e-06 -0.00026719738 -0.00043278886 -0.0006520889 -0.00094515953][0.0035458095 0.0031395801 0.0026191915 0.002307015 0.0021843836 0.0021805237 0.0019917437 0.0015737895 0.0011583095 0.00055415661 5.3087133e-05 -0.00033798476 -0.00048671896 -0.00065807742 -0.00093234039][0.0053429068 0.0050532036 0.0044175982 0.0038728062 0.0034646299 0.0032021319 0.00282186 0.0021984191 0.0015090489 0.00074534968 0.00018779503 -0.00017123541 -0.00022273581 -0.00033693691 -0.00059316133][0.0070452834 0.0069684046 0.0063568992 0.0056084618 0.0048444346 0.0041723391 0.0033920435 0.0024344707 0.0014432267 0.00059634133 0.00015068112 -7.77751e-05 5.4286909e-05 0.00019558438 0.00017208548][0.0090641817 0.0090080267 0.0082170572 0.00726667 0.0062596337 0.005215208 0.0039457609 0.0025740243 0.0013253124 0.00047294295 0.00018806185 0.00014401681 0.00047869526 0.00096255483 0.0012515431][0.011125885 0.011215218 0.010268258 0.008989118 0.0075228517 0.0059754322 0.0042663747 0.0025995537 0.0012547272 0.00045280636 0.00027734938 0.00054408878 0.0013047474 0.0021471637 0.0026316652][0.012818844 0.013096676 0.012090844 0.010643399 0.0087948116 0.0067845951 0.0046432186 0.0027384511 0.0012679565 0.00045947579 0.0005369134 0.0013604263 0.0026916433 0.0039682519 0.0046807975][0.0141071 0.014542636 0.01337756 0.011599557 0.0094373729 0.0071659465 0.0049201008 0.00300213 0.0015590145 0.00087151316 0.001297556 0.0027424404 0.0046793181 0.0063280663 0.0071725403][0.015104941 0.015503115 0.014122824 0.012067244 0.00968568 0.0073596318 0.0052167536 0.0033753822 0.0020910143 0.0017686681 0.0026314715 0.0045815618 0.0068534729 0.0086737117 0.0095665175][0.015667906 0.0160193 0.014423796 0.012214543 0.00981548 0.00759549 0.0056666858 0.004161038 0.0033650082 0.0036174082 0.00482306 0.0069452827 0.00923269 0.01100733 0.01186723][0.015907608 0.01619257 0.014301254 0.011999937 0.0097814752 0.007948461 0.006573359 0.0056211879 0.0053588729 0.0060565309 0.00739003 0.0093351044 0.011291625 0.012842213 0.013663958]]...]
INFO - root - 2017-12-10 00:30:27.904410: step 72310, loss = 0.82, batch loss = 0.69 (9.7 examples/sec; 0.829 sec/batch; 59h:53m:23s remains)
INFO - root - 2017-12-10 00:30:36.424779: step 72320, loss = 0.81, batch loss = 0.68 (9.4 examples/sec; 0.850 sec/batch; 61h:26m:49s remains)
INFO - root - 2017-12-10 00:30:45.169515: step 72330, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.874 sec/batch; 63h:10m:37s remains)
INFO - root - 2017-12-10 00:30:53.644901: step 72340, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.856 sec/batch; 61h:52m:05s remains)
INFO - root - 2017-12-10 00:31:02.357941: step 72350, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.837 sec/batch; 60h:30m:43s remains)
INFO - root - 2017-12-10 00:31:10.959070: step 72360, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.865 sec/batch; 62h:28m:58s remains)
INFO - root - 2017-12-10 00:31:19.545809: step 72370, loss = 0.82, batch loss = 0.70 (9.1 examples/sec; 0.879 sec/batch; 63h:33m:02s remains)
INFO - root - 2017-12-10 00:31:28.261977: step 72380, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.892 sec/batch; 64h:25m:01s remains)
INFO - root - 2017-12-10 00:31:36.932540: step 72390, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.889 sec/batch; 64h:14m:24s remains)
INFO - root - 2017-12-10 00:31:45.621374: step 72400, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.872 sec/batch; 63h:00m:44s remains)
2017-12-10 00:31:46.552824: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.0031793918 0.003176067 0.0031603971 0.003175165 0.0031977659 0.0032235235 0.0032556183 0.0032518171 0.0032122121 0.0030578962 0.0028210869 0.0025152392 0.0021584532 0.0017058606 0.0010917109][0.0036000363 0.0036045024 0.0035805753 0.0035754456 0.0035904832 0.0036214958 0.0036648517 0.0036856248 0.0036709704 0.0035437238 0.0033219215 0.003000427 0.0026128362 0.0021232134 0.0014630001][0.00303241 0.0030394779 0.0030178609 0.0030206125 0.00305084 0.0031118188 0.0031991317 0.0032810732 0.0033274116 0.0032606102 0.003081446 0.0027934345 0.00243092 0.0019631363 0.0013305472][0.0021357569 0.0021971348 0.0022195019 0.0022660261 0.0023401398 0.0024420754 0.0025582756 0.002669353 0.002742541 0.0027162614 0.0025665634 0.0023127696 0.001990749 0.0015740831 0.0010144055][0.0010740556 0.0011867107 0.0012967197 0.0014438707 0.0016061134 0.0017645477 0.0019022034 0.0020079012 0.0020591468 0.0020167837 0.0018573616 0.0016062307 0.0012993362 0.00092924957 0.0004650437][0.00018901203 0.00033145596 0.00050419697 0.00072868669 0.00097843877 0.0012083562 0.0013757519 0.0014650243 0.0014687147 0.0013640368 0.0011460864 0.00084929389 0.00051965506 0.00016403024 -0.00022502488][-0.00056396821 -0.00042882818 -0.00025152275 -9.5670111e-06 0.00026811298 0.00052599341 0.00072325568 0.00083594595 0.00085630652 0.00076294912 0.00055880111 0.00026457047 -7.3040719e-05 -0.00043157337 -0.00078286591][-0.0011692645 -0.0010951143 -0.00097619428 -0.00079186109 -0.00056066271 -0.00032737455 -0.00013139867 -3.9469451e-06 5.2734045e-05 2.6374357e-05 -8.8269007e-05 -0.00029829238 -0.00057358027 -0.00088474288 -0.0011812164][-0.0016083267 -0.0015881415 -0.0015461989 -0.0014647194 -0.0013444782 -0.0012070502 -0.0010757386 -0.00096816494 -0.00088290422 -0.00083358074 -0.00083586271 -0.00091546116 -0.0010670718 -0.0012642003 -0.0014612409][-0.0018070347 -0.0018055144 -0.0017981634 -0.0017789732 -0.0017458535 -0.0017039342 -0.0016606664 -0.0016181035 -0.0015704326 -0.0015207396 -0.0014835823 -0.0014846723 -0.0015293558 -0.0016032625 -0.0016845801][-0.0018263212 -0.0018265732 -0.0018271211 -0.0018279939 -0.0018288135 -0.001828756 -0.0018281632 -0.0018248974 -0.001815607 -0.001799028 -0.0017808359 -0.0017720514 -0.0017754023 -0.0017871136 -0.0018026674][-0.0018261212 -0.0018261534 -0.00182623 -0.0018263394 -0.0018270089 -0.0018273796 -0.0018282315 -0.0018294145 -0.0018310602 -0.001832177 -0.0018327533 -0.001833136 -0.0018335223 -0.0018336998 -0.001833461][-0.0018250663 -0.0018254763 -0.0018256127 -0.001826072 -0.0018271977 -0.0018280225 -0.0018289906 -0.0018298806 -0.0018312474 -0.0018324564 -0.0018332109 -0.0018336892 -0.0018339131 -0.0018337226 -0.0018329213][-0.0018262975 -0.0018264485 -0.0018259201 -0.0018258925 -0.0018263262 -0.0018266628 -0.0018272175 -0.0018278215 -0.0018287768 -0.0018297144 -0.0018300688 -0.0018304408 -0.0018308049 -0.0018310269 -0.0018308724][-0.0018257924 -0.0018265102 -0.0018264678 -0.0018264535 -0.0018264755 -0.001826537 -0.0018263472 -0.001826206 -0.0018264774 -0.0018263609 -0.0018258586 -0.0018258055 -0.0018258351 -0.0018259366 -0.0018263062]]...]
INFO - root - 2017-12-10 00:31:54.893662: step 72410, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.897 sec/batch; 64h:46m:37s remains)
INFO - root - 2017-12-10 00:32:03.711777: step 72420, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.876 sec/batch; 63h:16m:18s remains)
INFO - root - 2017-12-10 00:32:12.473090: step 72430, loss = 0.83, batch loss = 0.70 (9.9 examples/sec; 0.805 sec/batch; 58h:08m:16s remains)
INFO - root - 2017-12-10 00:32:20.975133: step 72440, loss = 0.82, batch loss = 0.69 (10.0 examples/sec; 0.803 sec/batch; 58h:00m:26s remains)
INFO - root - 2017-12-10 00:32:29.580321: step 72450, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.902 sec/batch; 65h:08m:23s remains)
INFO - root - 2017-12-10 00:32:38.233482: step 72460, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.863 sec/batch; 62h:19m:50s remains)
INFO - root - 2017-12-10 00:32:46.899693: step 72470, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.859 sec/batch; 62h:02m:59s remains)
INFO - root - 2017-12-10 00:32:55.572261: step 72480, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.886 sec/batch; 64h:00m:56s remains)
INFO - root - 2017-12-10 00:33:04.313444: step 72490, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.860 sec/batch; 62h:05m:09s remains)
INFO - root - 2017-12-10 00:33:12.982155: step 72500, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.874 sec/batch; 63h:07m:47s remains)
2017-12-10 00:33:13.859228: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.025976751 0.028660733 0.030482873 0.03107626 0.030634869 0.029198214 0.027143195 0.024707003 0.022253497 0.019850085 0.017583143 0.015160547 0.012738488 0.0099723069 0.0073150066][0.023268167 0.026012102 0.028196028 0.029305328 0.029201897 0.027846411 0.025696978 0.023001703 0.020315539 0.017792901 0.015608032 0.013438433 0.011388068 0.0090224724 0.0067415922][0.024538435 0.027669737 0.030519385 0.032470096 0.03314513 0.032319903 0.030326594 0.027298015 0.024047699 0.020903746 0.018224031 0.015577667 0.013088388 0.010339141 0.0077371416][0.029240526 0.033091374 0.037088551 0.040378366 0.042340793 0.042452447 0.040880971 0.03762304 0.033697281 0.029436421 0.025527369 0.021562645 0.017728424 0.013758318 0.01010896][0.035732742 0.040597264 0.045973092 0.050814006 0.05436882 0.055793539 0.055020697 0.05185103 0.047382746 0.041944757 0.036490366 0.030585803 0.024654562 0.01873569 0.013425754][0.042038232 0.047847059 0.054532297 0.060919307 0.066123538 0.06896425 0.06923905 0.066563942 0.062028438 0.055833198 0.048979938 0.041095097 0.032864664 0.024663037 0.017340222][0.045977652 0.052485295 0.06008343 0.067630291 0.074206091 0.078429118 0.079951033 0.078204989 0.074165419 0.067846254 0.060261142 0.05101341 0.040936328 0.030691355 0.021408221][0.046399493 0.053219873 0.061264604 0.069507957 0.07699541 0.08222644 0.084828705 0.084148012 0.08094991 0.075180657 0.067669071 0.057969119 0.046966203 0.035455953 0.024825238][0.043258518 0.050120678 0.058273874 0.066850789 0.074808791 0.080668211 0.084071212 0.084289685 0.081935704 0.076991647 0.07007125 0.060717277 0.049715556 0.037878014 0.0267616][0.038028371 0.04468723 0.052727889 0.061389633 0.069452316 0.075507022 0.079292744 0.080084771 0.078391314 0.074214585 0.068022951 0.059443332 0.049079947 0.037696086 0.026876058][0.032548767 0.038730875 0.046437945 0.054945171 0.062806539 0.068852253 0.07286144 0.074047565 0.072789162 0.0691066 0.063487388 0.055710636 0.046217714 0.035685729 0.025599804][0.02915399 0.03450859 0.041449714 0.049327422 0.056579195 0.06238094 0.066406131 0.067898847 0.067033619 0.063799381 0.058689557 0.051573992 0.042839359 0.03312625 0.023790801][0.029174428 0.033337869 0.038983457 0.045672704 0.051840618 0.056981977 0.060669176 0.062316656 0.061764657 0.058972195 0.05435716 0.047831278 0.039742995 0.030705724 0.022009222][0.032243088 0.035015162 0.039042346 0.0440197 0.048577771 0.052693829 0.055670757 0.05719018 0.056723464 0.054241855 0.050110988 0.044140026 0.036702748 0.028300323 0.02021509][0.036662143 0.03833868 0.040752836 0.043880817 0.046753816 0.049474131 0.0512908 0.052124947 0.051292721 0.048855182 0.04515427 0.039809383 0.033161305 0.025543258 0.018232629]]...]
INFO - root - 2017-12-10 00:33:22.192255: step 72510, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 62h:42m:19s remains)
INFO - root - 2017-12-10 00:33:30.992802: step 72520, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.850 sec/batch; 61h:21m:41s remains)
INFO - root - 2017-12-10 00:33:39.658591: step 72530, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 62h:25m:32s remains)
INFO - root - 2017-12-10 00:33:48.209719: step 72540, loss = 0.82, batch loss = 0.69 (10.2 examples/sec; 0.783 sec/batch; 56h:33m:04s remains)
INFO - root - 2017-12-10 00:33:56.806963: step 72550, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.863 sec/batch; 62h:16m:56s remains)
INFO - root - 2017-12-10 00:34:05.519322: step 72560, loss = 0.83, batch loss = 0.70 (9.1 examples/sec; 0.878 sec/batch; 63h:23m:16s remains)
INFO - root - 2017-12-10 00:34:14.196302: step 72570, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.876 sec/batch; 63h:14m:03s remains)
INFO - root - 2017-12-10 00:34:22.941330: step 72580, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.863 sec/batch; 62h:16m:52s remains)
INFO - root - 2017-12-10 00:34:31.600400: step 72590, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.842 sec/batch; 60h:47m:42s remains)
INFO - root - 2017-12-10 00:34:40.293936: step 72600, loss = 0.81, batch loss = 0.68 (9.3 examples/sec; 0.864 sec/batch; 62h:22m:40s remains)
2017-12-10 00:34:41.111364: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.34189132 0.34840402 0.35213003 0.35157344 0.34706056 0.33896461 0.32909745 0.31803897 0.30719692 0.29296827 0.27975333 0.26932949 0.25996596 0.25120547 0.24220581][0.344378 0.35043007 0.35394832 0.35337606 0.34888864 0.34059522 0.33066994 0.31971419 0.30868691 0.29525307 0.28199714 0.27136245 0.26240903 0.2546609 0.24672811][0.33707175 0.3414185 0.34412992 0.34351188 0.33934134 0.33103275 0.321575 0.31145817 0.30092686 0.28892991 0.27730355 0.26770565 0.25996968 0.2538729 0.24763149][0.32536814 0.3298324 0.33264056 0.33256665 0.32956848 0.32259867 0.3139382 0.30418807 0.29406759 0.28367451 0.27409792 0.26700932 0.26135582 0.2576111 0.25361589][0.31230173 0.31714714 0.3194426 0.32019117 0.31865719 0.31349224 0.30655682 0.29843765 0.29000375 0.28201431 0.27437538 0.27005708 0.26727781 0.26572987 0.26410598][0.30385926 0.30843136 0.309421 0.30956638 0.3083733 0.30435586 0.29952726 0.29345414 0.287063 0.28109938 0.27591074 0.27403745 0.27330098 0.27397808 0.27490026][0.29486525 0.29957667 0.29984903 0.30011961 0.29972231 0.2968449 0.29320583 0.28864393 0.28390944 0.28055862 0.27843326 0.2787379 0.28003609 0.28274977 0.28579369][0.28427684 0.29068276 0.29065397 0.29021561 0.28968385 0.28810298 0.28587624 0.2819798 0.27854609 0.27701291 0.27755922 0.28001496 0.28257692 0.28658745 0.29066986][0.27513939 0.28331879 0.28417698 0.28417972 0.28408027 0.28357577 0.28195038 0.27851498 0.27583823 0.27519235 0.27659672 0.28017718 0.28366745 0.28800136 0.29229274][0.26713631 0.27657613 0.27817386 0.27931517 0.28013164 0.28064436 0.27963728 0.27660802 0.2744908 0.27436286 0.27602851 0.27924722 0.28222212 0.285717 0.28887025][0.26229939 0.27179554 0.27250457 0.27257937 0.27271786 0.27359197 0.2727693 0.27070737 0.26974431 0.27025604 0.27249667 0.27485222 0.27630746 0.27823207 0.27954561][0.25555423 0.26544809 0.26583865 0.26497141 0.26411849 0.26435331 0.26330861 0.26152062 0.26106119 0.26281956 0.26546827 0.26714516 0.26709956 0.26696256 0.2658681][0.25353384 0.26340577 0.26330632 0.26150209 0.25973451 0.25917494 0.2574262 0.25527593 0.25457388 0.25610146 0.25779772 0.25806466 0.25639504 0.2544224 0.25131255][0.25431716 0.26408103 0.26407391 0.26264733 0.26122364 0.26047963 0.25843027 0.25648952 0.2555922 0.25630978 0.25653666 0.25498569 0.25190517 0.24806853 0.2431377][0.25907928 0.26924875 0.26908064 0.26761448 0.2662355 0.2654222 0.26356861 0.26165304 0.26049942 0.2603229 0.25920486 0.25618288 0.25135311 0.24603456 0.23981768]]...]
INFO - root - 2017-12-10 00:34:48.969556: step 72610, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.844 sec/batch; 60h:54m:17s remains)
INFO - root - 2017-12-10 00:34:57.526204: step 72620, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.844 sec/batch; 60h:55m:16s remains)
INFO - root - 2017-12-10 00:35:06.041536: step 72630, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.848 sec/batch; 61h:11m:29s remains)
INFO - root - 2017-12-10 00:35:14.781103: step 72640, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.877 sec/batch; 63h:16m:32s remains)
INFO - root - 2017-12-10 00:35:23.176337: step 72650, loss = 0.82, batch loss = 0.69 (10.0 examples/sec; 0.802 sec/batch; 57h:54m:44s remains)
INFO - root - 2017-12-10 00:35:31.750312: step 72660, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.896 sec/batch; 64h:40m:35s remains)
INFO - root - 2017-12-10 00:35:40.275776: step 72670, loss = 0.83, batch loss = 0.70 (9.0 examples/sec; 0.891 sec/batch; 64h:19m:28s remains)
INFO - root - 2017-12-10 00:35:48.794429: step 72680, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.879 sec/batch; 63h:27m:09s remains)
INFO - root - 2017-12-10 00:35:57.481083: step 72690, loss = 0.82, batch loss = 0.69 (9.7 examples/sec; 0.824 sec/batch; 59h:27m:33s remains)
INFO - root - 2017-12-10 00:36:06.193809: step 72700, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.861 sec/batch; 62h:08m:51s remains)
2017-12-10 00:36:07.155820: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0017567697 -0.0017541698 -0.0017541931 -0.0017557442 -0.0017561818 -0.0017557937 -0.0017540277 -0.0017522343 -0.001751155 -0.0017525736 -0.001757109 -0.0017649223 -0.0017757163 -0.0017869109 -0.001797643][-0.0017650792 -0.0017620192 -0.0017614153 -0.0017622703 -0.0017621729 -0.0017610185 -0.0017584614 -0.0017557016 -0.0017532334 -0.0017528473 -0.0017555638 -0.0017617714 -0.0017710194 -0.0017813838 -0.0017918507][-0.0017791507 -0.0017760752 -0.0017751862 -0.0017752928 -0.0017748827 -0.0017731753 -0.0017700787 -0.0017666842 -0.0017633345 -0.0017613928 -0.0017618279 -0.0017653246 -0.0017718682 -0.0017801938 -0.0017895092][-0.0017941445 -0.001791598 -0.0017908893 -0.0017909259 -0.00179042 -0.0017887123 -0.0017855002 -0.0017815905 -0.0017773748 -0.001773985 -0.0017722724 -0.0017728343 -0.0017762739 -0.0017817908 -0.0017892355][-0.0018091743 -0.0018067273 -0.001806299 -0.0018064453 -0.001806222 -0.0018048822 -0.0018022432 -0.0017985633 -0.0017941573 -0.0017895894 -0.0017857528 -0.0017837461 -0.0017843102 -0.0017871825 -0.0017920458][-0.0018204956 -0.0018185433 -0.0018182505 -0.0018187411 -0.001819085 -0.0018183033 -0.0018164908 -0.001813542 -0.00180939 -0.0018046073 -0.0018000719 -0.0017965637 -0.0017949402 -0.001795234 -0.0017974603][-0.0018275056 -0.0018264323 -0.0018266026 -0.0018269604 -0.0018275521 -0.0018270807 -0.0018259861 -0.0018237131 -0.0018200311 -0.0018153188 -0.0018103541 -0.0018061423 -0.0018035569 -0.0018028475 -0.001803553][-0.0018301633 -0.0018290835 -0.0018293972 -0.001829994 -0.0018314143 -0.0018318989 -0.001832093 -0.0018313335 -0.0018286988 -0.0018247125 -0.0018198751 -0.0018150621 -0.0018112091 -0.0018087922 -0.0018078272][-0.001830368 -0.0018288676 -0.0018290174 -0.0018298232 -0.0018314032 -0.0018328244 -0.0018342921 -0.0018348569 -0.0018333313 -0.0018302873 -0.0018259914 -0.0018213011 -0.0018170412 -0.0018138135 -0.0018116185][-0.0018300351 -0.0018280237 -0.0018275201 -0.0018280236 -0.0018297154 -0.0018318128 -0.0018340283 -0.0018355494 -0.0018349565 -0.0018325037 -0.0018285705 -0.0018240403 -0.0018195487 -0.0018158215 -0.0018132238][-0.0018301252 -0.0018273222 -0.0018262473 -0.0018263741 -0.0018277017 -0.0018298848 -0.0018323966 -0.0018343979 -0.0018343204 -0.001832217 -0.001828635 -0.0018241275 -0.0018194448 -0.0018153504 -0.0018124083][-0.0018304323 -0.0018268946 -0.0018248655 -0.0018244118 -0.0018255183 -0.0018277054 -0.0018302103 -0.0018327506 -0.0018334104 -0.0018317777 -0.0018284685 -0.0018241466 -0.0018195697 -0.0018152549 -0.0018120309][-0.0018311354 -0.0018272485 -0.0018248232 -0.0018235551 -0.001823891 -0.00182603 -0.0018284752 -0.0018312265 -0.0018324353 -0.0018317194 -0.0018290672 -0.0018249035 -0.0018201702 -0.0018157688 -0.0018126054][-0.0018321438 -0.0018285704 -0.0018259262 -0.0018241099 -0.0018238892 -0.0018251828 -0.0018274172 -0.0018297982 -0.0018310242 -0.0018307437 -0.0018286803 -0.0018250576 -0.0018205332 -0.0018162392 -0.0018130722][-0.0018335328 -0.0018302358 -0.0018277577 -0.0018257136 -0.0018249153 -0.0018254394 -0.001827321 -0.0018293995 -0.0018305286 -0.0018300653 -0.0018279071 -0.0018243947 -0.0018199098 -0.001815802 -0.0018129143]]...]
INFO - root - 2017-12-10 00:36:15.507021: step 72710, loss = 0.83, batch loss = 0.70 (9.3 examples/sec; 0.859 sec/batch; 62h:00m:07s remains)
INFO - root - 2017-12-10 00:36:24.265936: step 72720, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.850 sec/batch; 61h:19m:09s remains)
INFO - root - 2017-12-10 00:36:32.967229: step 72730, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.870 sec/batch; 62h:45m:18s remains)
INFO - root - 2017-12-10 00:36:41.732153: step 72740, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.893 sec/batch; 64h:24m:34s remains)
INFO - root - 2017-12-10 00:36:50.234527: step 72750, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.865 sec/batch; 62h:25m:49s remains)
INFO - root - 2017-12-10 00:36:58.926992: step 72760, loss = 0.81, batch loss = 0.68 (8.9 examples/sec; 0.896 sec/batch; 64h:39m:47s remains)
INFO - root - 2017-12-10 00:37:07.505526: step 72770, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 62h:37m:27s remains)
INFO - root - 2017-12-10 00:37:16.119625: step 72780, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.835 sec/batch; 60h:12m:53s remains)
INFO - root - 2017-12-10 00:37:24.750353: step 72790, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.879 sec/batch; 63h:23m:01s remains)
INFO - root - 2017-12-10 00:37:33.500154: step 72800, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.890 sec/batch; 64h:14m:18s remains)
2017-12-10 00:37:34.493128: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0017855436 -0.0017842291 -0.0017848565 -0.0017858014 -0.0017864712 -0.0017866563 -0.0017866471 -0.0017865831 -0.0017863268 -0.0017859627 -0.0017854313 -0.001784834 -0.0017840749 -0.0017835692 -0.0017834462][-0.0017851756 -0.0017835601 -0.0017839584 -0.001784734 -0.0017853384 -0.001785694 -0.0017859234 -0.0017861049 -0.0017860958 -0.0017858034 -0.0017851738 -0.0017844262 -0.0017834954 -0.0017827071 -0.0017823592][-0.0017864583 -0.0017845869 -0.001784616 -0.0017850772 -0.0017854867 -0.0017857932 -0.0017860627 -0.0017863142 -0.0017864284 -0.0017862368 -0.0017856661 -0.0017848869 -0.0017838986 -0.001782967 -0.0017824][-0.0017880873 -0.0017860167 -0.0017856712 -0.0017857227 -0.0017857965 -0.0017858851 -0.0017860385 -0.0017862453 -0.0017863642 -0.001786259 -0.0017858237 -0.0017851815 -0.001784298 -0.0017833743 -0.0017827072][-0.0017891171 -0.0017868276 -0.001786188 -0.0017858581 -0.0017855352 -0.0017853364 -0.0017853524 -0.0017855322 -0.0017856695 -0.0017856733 -0.0017854203 -0.0017849711 -0.0017842659 -0.0017834647 -0.0017828428][-0.0017890744 -0.0017866711 -0.0017858993 -0.0017853355 -0.0017847019 -0.0017842185 -0.0017840796 -0.0017842223 -0.0017843872 -0.001784525 -0.0017844933 -0.0017842791 -0.0017838145 -0.0017832265 -0.0017827726][-0.0017882035 -0.0017858482 -0.001785065 -0.0017844208 -0.0017836199 -0.0017829495 -0.0017826833 -0.0017827511 -0.0017828994 -0.0017831018 -0.0017832327 -0.0017831861 -0.0017829161 -0.001782565 -0.001782355][-0.0017870999 -0.0017848313 -0.0017841101 -0.0017835005 -0.0017826777 -0.0017819246 -0.0017815426 -0.0017814647 -0.0017814679 -0.0017815968 -0.0017817816 -0.0017818466 -0.0017817366 -0.0017816334 -0.0017817082][-0.0017856988 -0.0017834804 -0.0017828788 -0.0017823685 -0.0017816207 -0.0017808715 -0.0017803746 -0.0017800707 -0.0017798161 -0.0017797316 -0.0017798711 -0.0017800878 -0.0017802374 -0.0017804827 -0.0017809345][-0.0017839884 -0.0017819635 -0.0017814721 -0.0017810927 -0.0017804871 -0.0017798467 -0.0017793586 -0.0017789085 -0.0017784194 -0.0017780826 -0.0017781096 -0.0017784132 -0.0017788329 -0.0017794441 -0.0017802566][-0.0017825809 -0.001780681 -0.0017801907 -0.0017798971 -0.0017794536 -0.0017789654 -0.0017785437 -0.0017780514 -0.0017774485 -0.0017769218 -0.0017767895 -0.001777096 -0.0017777156 -0.0017786263 -0.0017797154][-0.0017819054 -0.0017799007 -0.0017793977 -0.0017791649 -0.0017788531 -0.0017785224 -0.0017782005 -0.0017777265 -0.0017770795 -0.0017764185 -0.0017761078 -0.0017763221 -0.0017770177 -0.0017781018 -0.0017793551][-0.0017818736 -0.0017798373 -0.0017792962 -0.0017791254 -0.001778919 -0.0017787045 -0.0017784704 -0.0017780677 -0.0017774835 -0.0017768375 -0.0017764638 -0.0017765743 -0.0017771964 -0.0017782389 -0.0017794196][-0.0017821739 -0.0017800811 -0.0017795059 -0.0017793914 -0.0017792585 -0.0017791201 -0.0017789613 -0.0017786683 -0.0017782494 -0.0017777835 -0.0017775 -0.0017775885 -0.0017781123 -0.001778985 -0.0017799345][-0.0017824786 -0.0017804434 -0.001779843 -0.0017797739 -0.0017797027 -0.0017796212 -0.0017795269 -0.0017793548 -0.0017791229 -0.0017789021 -0.001778794 -0.0017789318 -0.0017793835 -0.001780047 -0.001780703]]...]
INFO - root - 2017-12-10 00:37:42.787230: step 72810, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.851 sec/batch; 61h:25m:02s remains)
INFO - root - 2017-12-10 00:37:51.577054: step 72820, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.875 sec/batch; 63h:07m:41s remains)
INFO - root - 2017-12-10 00:38:00.233100: step 72830, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.861 sec/batch; 62h:06m:39s remains)
INFO - root - 2017-12-10 00:38:08.961876: step 72840, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.858 sec/batch; 61h:51m:46s remains)
INFO - root - 2017-12-10 00:38:17.362708: step 72850, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.852 sec/batch; 61h:25m:52s remains)
INFO - root - 2017-12-10 00:38:25.888265: step 72860, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.873 sec/batch; 62h:57m:16s remains)
INFO - root - 2017-12-10 00:38:34.511773: step 72870, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.881 sec/batch; 63h:31m:18s remains)
INFO - root - 2017-12-10 00:38:43.159576: step 72880, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.866 sec/batch; 62h:25m:29s remains)
INFO - root - 2017-12-10 00:38:51.846430: step 72890, loss = 0.82, batch loss = 0.70 (9.2 examples/sec; 0.869 sec/batch; 62h:38m:34s remains)
INFO - root - 2017-12-10 00:39:00.471754: step 72900, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.891 sec/batch; 64h:16m:55s remains)
2017-12-10 00:39:01.342425: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.010410211 0.011201865 0.012755128 0.015209512 0.018654158 0.023073202 0.028562931 0.034952968 0.041716203 0.048935451 0.055938173 0.062194109 0.066390365 0.067367047 0.065430179][0.013670959 0.014032246 0.015201134 0.017336316 0.02070743 0.025208289 0.030937165 0.0375348 0.044469338 0.05188119 0.058984455 0.065189049 0.069235414 0.070044383 0.067685343][0.018021349 0.017574057 0.018022917 0.019643476 0.022754 0.027340189 0.033429448 0.040348727 0.047527425 0.054846548 0.061619103 0.067185827 0.070376486 0.070483632 0.067420289][0.023331111 0.022301596 0.022144156 0.023233714 0.025998781 0.030380111 0.036387295 0.043353468 0.050533537 0.057664312 0.064004414 0.068729714 0.070784688 0.069874957 0.066013627][0.029009985 0.027722701 0.027258392 0.028066227 0.03057285 0.034651842 0.04033339 0.046866 0.053629056 0.060188677 0.065743476 0.069475278 0.070478156 0.068543062 0.063932896][0.035200734 0.034105122 0.033658069 0.034390941 0.036689393 0.040445879 0.045583103 0.051317744 0.057213847 0.062733047 0.067180589 0.069719411 0.069481142 0.066603445 0.061209872][0.042134292 0.041189205 0.040788211 0.041486096 0.043579355 0.0469969 0.051540896 0.056388777 0.061166033 0.065345883 0.06846 0.069632277 0.068148457 0.064350113 0.058219105][0.049554482 0.049020067 0.048731133 0.049308371 0.050971102 0.053888518 0.057765022 0.061745971 0.065474369 0.068486549 0.070405707 0.070315622 0.067730635 0.062937245 0.056046333][0.055619266 0.056228228 0.05675292 0.057641227 0.059175838 0.061516292 0.064478375 0.067307226 0.069691367 0.071337238 0.072023459 0.070916206 0.0675101 0.062000856 0.054547139][0.059663095 0.061266951 0.062562019 0.064174458 0.066119373 0.068311244 0.070750751 0.07257086 0.073613457 0.073611364 0.072782338 0.070658863 0.066728674 0.0608704 0.053147174][0.061936818 0.064411677 0.0662343 0.068226308 0.070135549 0.072355457 0.074469812 0.075645193 0.075677939 0.074391849 0.07238429 0.069318466 0.065024436 0.059283711 0.051978581][0.061784118 0.064982109 0.067371987 0.069724567 0.07164716 0.073614307 0.075050309 0.075506411 0.074610785 0.072426684 0.0698017 0.06648954 0.062628508 0.057637073 0.051299762][0.058864392 0.062396809 0.065181032 0.067944631 0.070063651 0.071825355 0.07258568 0.072242066 0.070479557 0.067788042 0.064969167 0.062026739 0.059184004 0.055433135 0.050605487][0.0523882 0.055973858 0.058936056 0.061976209 0.064363442 0.066076778 0.066450596 0.065656759 0.0634657 0.060442947 0.057653826 0.055366155 0.053524993 0.051110338 0.047814112][0.043232553 0.046420559 0.049131483 0.052075226 0.0544256 0.056129593 0.056348551 0.055594821 0.05363673 0.051094729 0.048877705 0.047236163 0.046284236 0.045000728 0.042861439]]...]
INFO - root - 2017-12-10 00:39:09.581758: step 72910, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.845 sec/batch; 60h:53m:52s remains)
INFO - root - 2017-12-10 00:39:18.061423: step 72920, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.839 sec/batch; 60h:27m:52s remains)
INFO - root - 2017-12-10 00:39:26.564175: step 72930, loss = 0.82, batch loss = 0.69 (9.7 examples/sec; 0.826 sec/batch; 59h:32m:57s remains)
INFO - root - 2017-12-10 00:39:35.111209: step 72940, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.870 sec/batch; 62h:42m:38s remains)
INFO - root - 2017-12-10 00:39:43.567542: step 72950, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.889 sec/batch; 64h:07m:36s remains)
INFO - root - 2017-12-10 00:39:52.287456: step 72960, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.877 sec/batch; 63h:12m:11s remains)
INFO - root - 2017-12-10 00:40:00.843846: step 72970, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.851 sec/batch; 61h:19m:47s remains)
INFO - root - 2017-12-10 00:40:09.438264: step 72980, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.858 sec/batch; 61h:52m:47s remains)
INFO - root - 2017-12-10 00:40:18.003261: step 72990, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.850 sec/batch; 61h:15m:13s remains)
INFO - root - 2017-12-10 00:40:26.687535: step 73000, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.853 sec/batch; 61h:28m:48s remains)
2017-12-10 00:40:27.561835: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.29075724 0.28372741 0.27814013 0.27401772 0.26996821 0.26547277 0.25923243 0.2520822 0.24400476 0.23576988 0.22731902 0.21935965 0.21240929 0.20623243 0.20112975][0.30620864 0.29954481 0.29427716 0.29060668 0.28676176 0.28181371 0.27460274 0.26592392 0.25588179 0.24576944 0.23535646 0.2252882 0.21629602 0.20859505 0.20229723][0.31851467 0.31291449 0.308453 0.30517805 0.3013356 0.29586565 0.28753909 0.27727112 0.26506117 0.25264502 0.23970333 0.22734185 0.21617903 0.20672487 0.19891302][0.32594758 0.32249114 0.3193064 0.31636986 0.31265518 0.30684522 0.29807141 0.286687 0.27316579 0.25896066 0.24399506 0.2291443 0.21529582 0.20353429 0.19386145][0.32695413 0.32562006 0.32346255 0.32144791 0.31842452 0.31257659 0.30360332 0.29198006 0.27811044 0.26277897 0.24608977 0.22921035 0.21294624 0.1986479 0.18677443][0.319535 0.32037839 0.3195006 0.31864706 0.31662545 0.31177858 0.30374667 0.29246062 0.27874991 0.26311165 0.24536887 0.22678201 0.20840545 0.19208916 0.17820205][0.30356833 0.30650344 0.30717102 0.30798525 0.30773214 0.30461887 0.29817691 0.28815848 0.27535415 0.25983703 0.24156605 0.22169228 0.20170932 0.18356101 0.16793318][0.27925596 0.28414902 0.28629622 0.28871146 0.29058674 0.28978181 0.2855674 0.27747157 0.26639059 0.25200334 0.23401931 0.21360207 0.19248302 0.17312551 0.15605938][0.2517589 0.25855947 0.26217845 0.26622286 0.27006304 0.27157718 0.26955044 0.26347572 0.25413918 0.24116051 0.22424883 0.20417312 0.18266067 0.16231756 0.14432326][0.22451182 0.23294297 0.23788403 0.24338447 0.24884874 0.25222784 0.25221473 0.24789709 0.24004626 0.2285133 0.2128029 0.19366263 0.17244664 0.15185797 0.13331331][0.19816266 0.20819393 0.21476021 0.22154433 0.2281684 0.23305607 0.23450652 0.23175856 0.22525075 0.21501623 0.20060045 0.18255502 0.16205835 0.14185858 0.12336304][0.17362034 0.18534602 0.19336104 0.2012562 0.20901899 0.21495742 0.21753228 0.2160386 0.21091305 0.20214041 0.18935055 0.1727639 0.15336308 0.13359083 0.11522285][0.15363227 0.16608971 0.17531706 0.18383232 0.19185941 0.19804895 0.20109288 0.2005792 0.1965895 0.18961205 0.17877102 0.16415374 0.14629936 0.12728499 0.10918827][0.13590156 0.14890204 0.15922117 0.16855308 0.17726299 0.18424305 0.1882928 0.18858178 0.18554369 0.17988887 0.17076696 0.15781066 0.14131133 0.12308162 0.10526583][0.11955184 0.13270372 0.14375021 0.15408891 0.16380282 0.17186473 0.17716561 0.17870665 0.17684737 0.17237075 0.16447791 0.15273327 0.13728099 0.11975911 0.10221491]]...]
INFO - root - 2017-12-10 00:40:35.795495: step 73010, loss = 0.82, batch loss = 0.69 (9.7 examples/sec; 0.825 sec/batch; 59h:28m:58s remains)
INFO - root - 2017-12-10 00:40:44.473775: step 73020, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.870 sec/batch; 62h:41m:12s remains)
INFO - root - 2017-12-10 00:40:53.043710: step 73030, loss = 0.82, batch loss = 0.69 (9.8 examples/sec; 0.817 sec/batch; 58h:54m:14s remains)
INFO - root - 2017-12-10 00:41:01.590386: step 73040, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.854 sec/batch; 61h:34m:32s remains)
INFO - root - 2017-12-10 00:41:10.097568: step 73050, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.843 sec/batch; 60h:44m:57s remains)
INFO - root - 2017-12-10 00:41:18.785705: step 73060, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 62h:46m:52s remains)
INFO - root - 2017-12-10 00:41:27.658306: step 73070, loss = 0.82, batch loss = 0.69 (8.8 examples/sec; 0.905 sec/batch; 65h:12m:53s remains)
INFO - root - 2017-12-10 00:41:36.302934: step 73080, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.849 sec/batch; 61h:11m:26s remains)
INFO - root - 2017-12-10 00:41:45.018560: step 73090, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.866 sec/batch; 62h:23m:58s remains)
INFO - root - 2017-12-10 00:41:53.691960: step 73100, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.878 sec/batch; 63h:17m:57s remains)
2017-12-10 00:41:54.612088: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0017805255 -0.0017792213 -0.0017799132 -0.0017807499 -0.0017813726 -0.0017816935 -0.0017816854 -0.0017812968 -0.0017806201 -0.0017800387 -0.0017798685 -0.0017799161 -0.0017798488 -0.0017797006 -0.001779498][-0.0017781692 -0.0017768787 -0.001777912 -0.0017792173 -0.0017804216 -0.0017813534 -0.0017818934 -0.0017818218 -0.0017813157 -0.0017810236 -0.0017811779 -0.0017816036 -0.0017817869 -0.0017816736 -0.0017812349][-0.0017770649 -0.0017759512 -0.00177712 -0.001778867 -0.0017807761 -0.001782489 -0.0017838483 -0.0017845823 -0.0017847796 -0.0017852309 -0.0017861014 -0.0017871294 -0.0017875878 -0.0017874356 -0.001786519][-0.0017763058 -0.001775099 -0.0017761693 -0.0017781501 -0.0017807914 -0.0017835202 -0.0017860257 -0.0017878626 -0.0017892288 -0.0017909096 -0.0017928062 -0.0017945268 -0.0017952452 -0.0017950357 -0.0017936161][-0.0017759404 -0.0017744546 -0.0017752307 -0.0017771628 -0.0017801754 -0.0017837771 -0.0017874478 -0.0017907209 -0.0017936653 -0.0017970121 -0.0018002806 -0.0018028425 -0.0018039994 -0.0018035898 -0.0018014986][-0.0017759561 -0.0017741598 -0.001774692 -0.0017763703 -0.001779361 -0.0017833578 -0.001787875 -0.0017924209 -0.0017969965 -0.0018019461 -0.0018065181 -0.001809873 -0.0018113739 -0.0018108115 -0.0018081638][-0.0017761439 -0.0017742827 -0.0017745888 -0.001775872 -0.0017785303 -0.0017824875 -0.0017873946 -0.0017928183 -0.0017987837 -0.0018048049 -0.001810225 -0.00181406 -0.001815846 -0.0018151152 -0.0018120096][-0.0017766677 -0.0017749121 -0.0017750916 -0.0017759656 -0.0017781092 -0.0017816564 -0.0017862828 -0.001791728 -0.0017981252 -0.0018043652 -0.0018097023 -0.0018135059 -0.0018153783 -0.0018147053 -0.0018118059][-0.0017777054 -0.0017760171 -0.001776275 -0.0017768517 -0.0017782833 -0.0017809598 -0.0017847089 -0.0017894346 -0.0017951267 -0.0018004929 -0.0018049825 -0.0018082103 -0.0018098031 -0.0018092981 -0.0018071339][-0.0017787096 -0.0017773719 -0.0017777677 -0.0017780726 -0.0017786345 -0.0017800849 -0.0017824406 -0.0017858513 -0.0017900288 -0.0017937972 -0.0017970396 -0.0017994532 -0.0018007467 -0.0018005259 -0.0017991269][-0.0017793669 -0.001778269 -0.0017787353 -0.0017789397 -0.0017789033 -0.0017792896 -0.0017803906 -0.0017823627 -0.0017847343 -0.0017865318 -0.0017880667 -0.0017893878 -0.0017901466 -0.0017899823 -0.0017893289][-0.0017792225 -0.0017781543 -0.0017786904 -0.001778953 -0.0017787562 -0.0017784776 -0.001778713 -0.0017796975 -0.0017805074 -0.0017805618 -0.0017804676 -0.001780667 -0.0017807166 -0.0017804352 -0.0017802562][-0.0017785367 -0.001777288 -0.00177779 -0.00177822 -0.0017780752 -0.0017775593 -0.0017772778 -0.0017776192 -0.0017773854 -0.0017762959 -0.0017750001 -0.0017740995 -0.0017732987 -0.0017727833 -0.0017728087][-0.0017770808 -0.0017756547 -0.0017760954 -0.0017767064 -0.0017767544 -0.0017762959 -0.0017758991 -0.001775938 -0.0017753701 -0.0017738652 -0.0017719335 -0.0017701659 -0.0017686186 -0.0017676719 -0.0017675201][-0.0017748392 -0.0017733085 -0.0017736158 -0.0017743305 -0.0017746352 -0.0017744962 -0.0017742362 -0.0017743659 -0.0017739906 -0.0017725661 -0.001770413 -0.0017681951 -0.0017662245 -0.0017649517 -0.0017645264]]...]
INFO - root - 2017-12-10 00:42:02.998172: step 73110, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.870 sec/batch; 62h:42m:55s remains)
INFO - root - 2017-12-10 00:42:11.666244: step 73120, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 62h:47m:18s remains)
INFO - root - 2017-12-10 00:42:20.467983: step 73130, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.876 sec/batch; 63h:07m:29s remains)
INFO - root - 2017-12-10 00:42:29.023128: step 73140, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.840 sec/batch; 60h:28m:59s remains)
INFO - root - 2017-12-10 00:42:37.454934: step 73150, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.888 sec/batch; 64h:00m:25s remains)
INFO - root - 2017-12-10 00:42:46.050184: step 73160, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.846 sec/batch; 60h:55m:39s remains)
INFO - root - 2017-12-10 00:42:54.595814: step 73170, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.876 sec/batch; 63h:08m:16s remains)
INFO - root - 2017-12-10 00:43:03.332960: step 73180, loss = 0.82, batch loss = 0.69 (8.8 examples/sec; 0.912 sec/batch; 65h:41m:39s remains)
INFO - root - 2017-12-10 00:43:12.157726: step 73190, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 62h:07m:16s remains)
INFO - root - 2017-12-10 00:43:21.003915: step 73200, loss = 0.82, batch loss = 0.69 (8.7 examples/sec; 0.916 sec/batch; 65h:59m:15s remains)
2017-12-10 00:43:21.917521: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0017973098 -0.0017951218 -0.0017947464 -0.001794606 -0.0017940582 -0.0017931471 -0.0017920941 -0.0017912276 -0.0017906298 -0.0017901479 -0.0017897277 -0.0017895044 -0.0017893802 -0.0017893275 -0.0017892763][-0.001795655 -0.0017934826 -0.001793301 -0.0017933435 -0.0017930091 -0.001792326 -0.0017914081 -0.0017905405 -0.0017899179 -0.0017894061 -0.0017889276 -0.0017886109 -0.0017884516 -0.0017883801 -0.0017883739][-0.0017953959 -0.0017933931 -0.0017934201 -0.0017937558 -0.0017937049 -0.0017932662 -0.0017924944 -0.0017916388 -0.0017909275 -0.0017903215 -0.0017897796 -0.0017893838 -0.0017891689 -0.0017890783 -0.0017890708][-0.0017952713 -0.0017934628 -0.0017936807 -0.0017942876 -0.0017945584 -0.0017944091 -0.0017938698 -0.0017931118 -0.0017923461 -0.0017916264 -0.0017910177 -0.0017906786 -0.0017905213 -0.0017904558 -0.0017904815][-0.001795158 -0.0017934105 -0.0017936891 -0.0017943914 -0.001794822 -0.0017948472 -0.0017944823 -0.0017938411 -0.0017931393 -0.0017924112 -0.0017918714 -0.0017916943 -0.0017916567 -0.0017916632 -0.0017917167][-0.0017951303 -0.0017934298 -0.0017937385 -0.0017944799 -0.0017950118 -0.0017951195 -0.0017947892 -0.0017941499 -0.001793496 -0.0017928482 -0.0017924018 -0.001792338 -0.0017924375 -0.0017925685 -0.0017926865][-0.0017952346 -0.0017937557 -0.0017941752 -0.0017949531 -0.0017955638 -0.001795731 -0.0017953479 -0.0017945761 -0.0017938692 -0.0017932187 -0.0017927376 -0.0017926327 -0.0017927446 -0.0017928978 -0.0017930985][-0.001795443 -0.0017942084 -0.0017947609 -0.0017955846 -0.0017961692 -0.0017962742 -0.0017957841 -0.0017948917 -0.0017941145 -0.0017933712 -0.0017928178 -0.0017926168 -0.0017926551 -0.0017928021 -0.0017930595][-0.0017956797 -0.0017946268 -0.0017952524 -0.0017960595 -0.001796474 -0.0017963642 -0.0017957104 -0.001794746 -0.001793922 -0.0017931485 -0.0017925586 -0.0017922992 -0.0017922437 -0.0017923553 -0.0017926311][-0.0017957253 -0.0017949824 -0.0017957337 -0.0017965452 -0.0017967949 -0.0017964352 -0.0017955865 -0.0017944956 -0.0017935189 -0.0017925929 -0.0017919131 -0.0017915508 -0.0017913785 -0.0017913985 -0.0017916625][-0.0017959691 -0.0017955151 -0.0017963503 -0.0017971987 -0.0017973186 -0.0017967195 -0.0017955943 -0.0017942457 -0.0017930039 -0.0017918675 -0.001791059 -0.0017906129 -0.0017904176 -0.0017904508 -0.0017907652][-0.0017963914 -0.0017959643 -0.0017968068 -0.0017976457 -0.0017975841 -0.0017966736 -0.0017951626 -0.0017935474 -0.0017920453 -0.0017906927 -0.0017897709 -0.0017893219 -0.0017892027 -0.0017893207 -0.0017897264][-0.001796706 -0.0017962308 -0.0017969579 -0.0017977192 -0.0017974895 -0.0017963365 -0.0017945843 -0.0017928212 -0.0017912237 -0.0017898091 -0.0017888418 -0.0017883755 -0.0017883001 -0.0017885024 -0.0017889269][-0.0017968626 -0.0017962194 -0.0017967613 -0.0017973636 -0.00179702 -0.001795756 -0.0017938936 -0.0017920845 -0.0017905704 -0.0017893018 -0.0017884381 -0.001788018 -0.0017879744 -0.0017881879 -0.0017885767][-0.0017966843 -0.001795899 -0.0017961948 -0.0017965876 -0.0017961888 -0.0017949985 -0.001793236 -0.0017915383 -0.0017902236 -0.001789186 -0.0017884757 -0.0017881108 -0.0017880852 -0.001788286 -0.0017885987]]...]
INFO - root - 2017-12-10 00:43:30.230069: step 73210, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.884 sec/batch; 63h:40m:11s remains)
INFO - root - 2017-12-10 00:43:38.922329: step 73220, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 62h:26m:00s remains)
INFO - root - 2017-12-10 00:43:47.645004: step 73230, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.895 sec/batch; 64h:26m:17s remains)
INFO - root - 2017-12-10 00:43:56.410562: step 73240, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 62h:24m:23s remains)
INFO - root - 2017-12-10 00:44:04.920277: step 73250, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.831 sec/batch; 59h:48m:41s remains)
INFO - root - 2017-12-10 00:44:13.573254: step 73260, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.877 sec/batch; 63h:08m:35s remains)
INFO - root - 2017-12-10 00:44:22.269070: step 73270, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.893 sec/batch; 64h:18m:53s remains)
INFO - root - 2017-12-10 00:44:30.945424: step 73280, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 62h:43m:33s remains)
INFO - root - 2017-12-10 00:44:39.681785: step 73290, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.889 sec/batch; 63h:59m:02s remains)
INFO - root - 2017-12-10 00:44:48.370902: step 73300, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.891 sec/batch; 64h:06m:59s remains)
2017-12-10 00:44:49.265863: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.00033179892 0.00026600284 0.00047927455 0.00088361197 0.0013961926 0.0019579693 0.0027106479 0.0033425014 0.0035057534 0.0028873668 0.0017969453 0.00055282435 -0.00057199167 -0.0013218416 -0.0016819208][0.0027041882 0.0017024827 0.0011826284 0.0014757622 0.0025077462 0.0039239624 0.0054550818 0.0064713177 0.0066355276 0.0056514796 0.0039768913 0.0020439452 0.00027871213 -0.00095642236 -0.0015842458][0.0070306254 0.0059125954 0.0051145372 0.0051680095 0.0063017583 0.0083449977 0.010769252 0.012426657 0.012766166 0.011316665 0.0086980443 0.0055283941 0.0024183537 7.8382087e-05 -0.0012348967][0.013439343 0.013039505 0.012915264 0.013587504 0.015185521 0.017836427 0.020944949 0.023079602 0.023301337 0.020996008 0.016805738 0.011624817 0.0063609914 0.002168349 -0.000430012][0.021760274 0.022850053 0.023995336 0.026066724 0.029155133 0.033099592 0.037055772 0.0393598 0.038866848 0.034899466 0.028253691 0.020241903 0.012108439 0.0053566815 0.00089620531][0.032800112 0.036012713 0.039070595 0.042854171 0.047351483 0.052512594 0.057200093 0.059375204 0.057664938 0.051373873 0.041574329 0.030052554 0.01855504 0.0090434561 0.0025805221][0.04699523 0.052082565 0.056499615 0.061530054 0.067161821 0.073078677 0.077858493 0.079304218 0.075747728 0.066528544 0.053101379 0.037875846 0.023225259 0.01150577 0.003617208][0.064097896 0.0705183 0.075156108 0.079963535 0.08498928 0.089785375 0.093025684 0.092533894 0.086621977 0.07469324 0.058428537 0.040648881 0.024318239 0.011795264 0.0036526751][0.080749229 0.087446444 0.09124361 0.094991893 0.098396271 0.10076126 0.10066018 0.096551791 0.087268956 0.072735153 0.054882947 0.0366647 0.020993164 0.009635834 0.0026517035][0.094142765 0.10139294 0.10429638 0.10652439 0.10757605 0.10652396 0.10222954 0.093798712 0.080901876 0.064200945 0.045883115 0.028800683 0.015337435 0.0063597034 0.001258572][0.10433388 0.11210022 0.11419393 0.11484188 0.11318924 0.10823034 0.099232785 0.086184129 0.069890767 0.051826634 0.034252405 0.019568818 0.009261298 0.0031340015 -2.0774547e-05][0.11127167 0.11959817 0.12122081 0.12029321 0.11565681 0.10651546 0.092726693 0.075483866 0.056721907 0.038498525 0.022824721 0.011290872 0.0042730281 0.00068066048 -0.00094927952][0.11655053 0.12533803 0.12659805 0.12393338 0.11594021 0.10232718 0.083929725 0.063379459 0.043408167 0.026285104 0.013366462 0.0051584556 0.00096471829 -0.00079522876 -0.0014688222][0.12109667 0.1305131 0.13116334 0.12623176 0.11436868 0.096199758 0.073833771 0.051251728 0.031474877 0.016500415 0.0066712708 0.0014014897 -0.00078459329 -0.0014852483 -0.0016902883][0.12486585 0.13326643 0.13177699 0.12326265 0.10741577 0.085746937 0.061431482 0.039061558 0.021310551 0.0094277747 0.0026341863 -0.0004381215 -0.0014666212 -0.0017007655 -0.0017493208]]...]
INFO - root - 2017-12-10 00:44:57.552948: step 73310, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.876 sec/batch; 63h:04m:12s remains)
INFO - root - 2017-12-10 00:45:06.173444: step 73320, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.881 sec/batch; 63h:25m:05s remains)
INFO - root - 2017-12-10 00:45:14.715041: step 73330, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.856 sec/batch; 61h:35m:57s remains)
INFO - root - 2017-12-10 00:45:23.299799: step 73340, loss = 0.81, batch loss = 0.68 (9.1 examples/sec; 0.876 sec/batch; 63h:03m:18s remains)
INFO - root - 2017-12-10 00:45:31.772806: step 73350, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.866 sec/batch; 62h:21m:18s remains)
INFO - root - 2017-12-10 00:45:40.442499: step 73360, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.863 sec/batch; 62h:06m:57s remains)
INFO - root - 2017-12-10 00:45:49.105666: step 73370, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.883 sec/batch; 63h:32m:09s remains)
INFO - root - 2017-12-10 00:45:57.676343: step 73380, loss = 0.81, batch loss = 0.68 (9.4 examples/sec; 0.853 sec/batch; 61h:25m:40s remains)
INFO - root - 2017-12-10 00:46:06.291958: step 73390, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.861 sec/batch; 61h:56m:08s remains)
INFO - root - 2017-12-10 00:46:15.099741: step 73400, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.875 sec/batch; 62h:59m:12s remains)
2017-12-10 00:46:16.007995: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0018049555 -0.0018037691 -0.0018046452 -0.0018057635 -0.0018068948 -0.0018080429 -0.0018094785 -0.0018111944 -0.0018129298 -0.0018145173 -0.0018152925 -0.0018150805 -0.0018141046 -0.0018125743 -0.001810591][-0.0018056416 -0.0018049184 -0.0018062049 -0.0018076316 -0.0018090783 -0.0018105434 -0.0018122724 -0.0018143443 -0.0018166226 -0.0018187043 -0.0018198454 -0.0018198816 -0.0018187484 -0.0018167484 -0.0018140402][-0.0018069424 -0.0018069606 -0.0018087602 -0.0018107553 -0.0018125969 -0.0018143099 -0.0018162752 -0.0018186286 -0.001821121 -0.0018231925 -0.0018243353 -0.0018243941 -0.0018230844 -0.001820734 -0.0018174914][-0.001808079 -0.0018089494 -0.0018113785 -0.0018142154 -0.0018166346 -0.00181878 -0.0018210171 -0.0018234643 -0.001825734 -0.00182731 -0.0018280592 -0.0018279763 -0.0018265869 -0.0018241899 -0.0018207764][-0.0018091508 -0.0018107784 -0.0018138658 -0.0018176368 -0.0018208082 -0.0018233937 -0.0018258236 -0.0018280789 -0.0018298958 -0.0018308024 -0.001831043 -0.001830661 -0.0018291585 -0.0018267963 -0.0018234362][-0.0018100876 -0.0018123828 -0.0018160391 -0.0018205098 -0.0018242121 -0.0018271762 -0.001829517 -0.0018313588 -0.0018328723 -0.0018333417 -0.0018330882 -0.0018321576 -0.0018304001 -0.00182819 -0.0018250415][-0.0018108942 -0.0018135542 -0.0018176119 -0.0018223267 -0.001826218 -0.0018290804 -0.0018310809 -0.0018324873 -0.0018335531 -0.0018338138 -0.0018332661 -0.0018318935 -0.0018299718 -0.0018280416 -0.0018255127][-0.0018113105 -0.0018143654 -0.0018185516 -0.0018232206 -0.0018269032 -0.0018292163 -0.0018306727 -0.001831776 -0.0018325654 -0.001832854 -0.0018323847 -0.0018309094 -0.0018291178 -0.0018274221 -0.0018254719][-0.0018114161 -0.0018146146 -0.0018186002 -0.0018226518 -0.0018252789 -0.0018265959 -0.0018272427 -0.0018280721 -0.0018292303 -0.0018301419 -0.0018304706 -0.0018297696 -0.0018286632 -0.0018275816 -0.0018262][-0.001811435 -0.0018147081 -0.0018182211 -0.0018213245 -0.0018226298 -0.0018225772 -0.0018221777 -0.0018229214 -0.0018248811 -0.0018271706 -0.0018289823 -0.001829648 -0.0018296348 -0.0018292726 -0.0018284175][-0.0018113489 -0.0018146194 -0.0018177817 -0.0018198296 -0.0018197478 -0.0018182595 -0.0018166292 -0.0018171784 -0.0018200222 -0.0018238776 -0.0018276278 -0.0018301181 -0.001831389 -0.0018318354 -0.0018314944][-0.001810908 -0.0018141792 -0.0018171653 -0.0018186539 -0.0018177526 -0.0018150645 -0.0018123232 -0.0018123308 -0.0018154279 -0.001820412 -0.0018257332 -0.0018298632 -0.0018323624 -0.0018336735 -0.0018340842][-0.0018101485 -0.0018135473 -0.0018166762 -0.0018184015 -0.0018174379 -0.001814491 -0.0018113831 -0.0018112483 -0.001814648 -0.0018197768 -0.0018255478 -0.0018304472 -0.0018337015 -0.0018354643 -0.0018361661][-0.0018093239 -0.0018126046 -0.0018161093 -0.0018184176 -0.0018179956 -0.0018152959 -0.0018124586 -0.0018121823 -0.0018152855 -0.0018199831 -0.001825577 -0.0018305877 -0.0018341786 -0.0018362069 -0.001837004][-0.0018082891 -0.001811286 -0.0018148869 -0.0018178163 -0.0018184525 -0.0018168401 -0.0018145494 -0.001814188 -0.0018164949 -0.0018201858 -0.0018248172 -0.001829189 -0.0018326703 -0.00183477 -0.0018353534]]...]
INFO - root - 2017-12-10 00:46:24.351502: step 73410, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.872 sec/batch; 62h:47m:04s remains)
INFO - root - 2017-12-10 00:46:33.115300: step 73420, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.849 sec/batch; 61h:04m:32s remains)
INFO - root - 2017-12-10 00:46:41.587729: step 73430, loss = 0.83, batch loss = 0.70 (9.3 examples/sec; 0.860 sec/batch; 61h:51m:58s remains)
INFO - root - 2017-12-10 00:46:50.145620: step 73440, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.876 sec/batch; 63h:01m:38s remains)
INFO - root - 2017-12-10 00:46:58.666017: step 73450, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.873 sec/batch; 62h:50m:14s remains)
INFO - root - 2017-12-10 00:47:07.404236: step 73460, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.863 sec/batch; 62h:03m:58s remains)
INFO - root - 2017-12-10 00:47:16.038952: step 73470, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.883 sec/batch; 63h:33m:46s remains)
INFO - root - 2017-12-10 00:47:24.664246: step 73480, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.865 sec/batch; 62h:13m:01s remains)
INFO - root - 2017-12-10 00:47:33.274280: step 73490, loss = 0.82, batch loss = 0.69 (8.6 examples/sec; 0.926 sec/batch; 66h:36m:48s remains)
INFO - root - 2017-12-10 00:47:41.923454: step 73500, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.848 sec/batch; 60h:59m:35s remains)
2017-12-10 00:47:42.828086: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0018333546 -0.0018340169 -0.00183454 -0.001834553 -0.0018341845 -0.0018333946 -0.0018322464 -0.0018311082 -0.001830287 -0.0018299922 -0.0018302823 -0.0018310335 -0.0018319156 -0.0018327091 -0.0018332832][-0.0018331187 -0.0018338133 -0.0018344349 -0.0018345927 -0.0018343665 -0.0018337651 -0.0018327595 -0.0018317141 -0.0018309274 -0.0018306182 -0.0018309037 -0.0018317265 -0.0018327839 -0.0018338013 -0.0018345957][-0.0018332684 -0.0018341045 -0.0018348434 -0.0018351431 -0.0018349611 -0.0018343292 -0.001833279 -0.0018321511 -0.0018312953 -0.0018308773 -0.0018310344 -0.0018318344 -0.0018330194 -0.0018342515 -0.0018352828][-0.0018332552 -0.0018342332 -0.0018350886 -0.0018354405 -0.0018352047 -0.0018344923 -0.0018333751 -0.0018321235 -0.0018311681 -0.0018307023 -0.0018308278 -0.0018316172 -0.0018327768 -0.0018340175 -0.001835153][-0.0018330887 -0.0018341237 -0.0018349586 -0.0018352704 -0.0018349747 -0.0018342187 -0.001833063 -0.0018317999 -0.0018309349 -0.001830623 -0.0018309262 -0.0018318517 -0.0018330348 -0.0018342339 -0.0018353078][-0.0018325381 -0.0018335638 -0.0018343522 -0.0018346087 -0.0018343076 -0.0018335702 -0.0018324313 -0.0018311437 -0.001830371 -0.001830259 -0.0018308149 -0.001832003 -0.0018333021 -0.0018344822 -0.0018354957][-0.0018320825 -0.00183304 -0.0018337738 -0.0018340087 -0.0018336715 -0.0018328929 -0.0018317577 -0.0018304711 -0.0018297414 -0.0018297646 -0.0018305471 -0.001831955 -0.0018334319 -0.0018347355 -0.0018358208][-0.00183181 -0.0018326906 -0.0018334158 -0.001833737 -0.0018334965 -0.0018328211 -0.0018318731 -0.0018307238 -0.0018300806 -0.0018301778 -0.0018309972 -0.0018323966 -0.001833875 -0.0018352537 -0.0018364625][-0.0018317378 -0.0018326698 -0.0018334851 -0.0018340021 -0.0018340189 -0.0018336226 -0.0018329059 -0.0018319186 -0.0018313003 -0.0018313695 -0.0018321021 -0.0018333851 -0.0018347735 -0.0018361574 -0.0018374843][-0.0018318718 -0.0018329001 -0.0018338843 -0.0018346225 -0.0018348788 -0.0018347107 -0.001834219 -0.0018334116 -0.0018327867 -0.0018326777 -0.001833124 -0.0018340645 -0.0018352011 -0.0018364934 -0.0018378668][-0.001832133 -0.0018332794 -0.0018343935 -0.0018353335 -0.0018358104 -0.0018358383 -0.0018354921 -0.0018347384 -0.0018339482 -0.0018334577 -0.0018333646 -0.0018337463 -0.0018344803 -0.0018355639 -0.0018369319][-0.0018321442 -0.0018332835 -0.0018343893 -0.0018353313 -0.0018358293 -0.0018359323 -0.0018356304 -0.001834842 -0.001833886 -0.0018331052 -0.0018326549 -0.0018326277 -0.0018329832 -0.0018338115 -0.0018350153][-0.0018317722 -0.0018327091 -0.0018336091 -0.0018344104 -0.0018349213 -0.0018351682 -0.0018350462 -0.0018344335 -0.0018335592 -0.0018327103 -0.0018320639 -0.0018317268 -0.00183169 -0.0018321191 -0.0018329899][-0.0018311206 -0.0018316793 -0.0018321224 -0.0018326374 -0.0018330591 -0.0018334099 -0.0018335176 -0.0018332116 -0.0018326255 -0.0018319448 -0.0018312782 -0.0018307444 -0.0018304005 -0.0018304357 -0.0018308694][-0.0018299498 -0.001829976 -0.0018299086 -0.0018300093 -0.0018302209 -0.0018305179 -0.0018307548 -0.0018306893 -0.0018303897 -0.0018299288 -0.0018293967 -0.0018288948 -0.0018284473 -0.0018282057 -0.0018282342]]...]
INFO - root - 2017-12-10 00:47:51.103262: step 73510, loss = 0.83, batch loss = 0.70 (9.4 examples/sec; 0.848 sec/batch; 61h:01m:20s remains)
INFO - root - 2017-12-10 00:47:59.559672: step 73520, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.845 sec/batch; 60h:45m:35s remains)
INFO - root - 2017-12-10 00:48:08.154146: step 73530, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.885 sec/batch; 63h:39m:57s remains)
INFO - root - 2017-12-10 00:48:16.771847: step 73540, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.861 sec/batch; 61h:54m:44s remains)
INFO - root - 2017-12-10 00:48:25.306629: step 73550, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.832 sec/batch; 59h:51m:57s remains)
INFO - root - 2017-12-10 00:48:34.001644: step 73560, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 61h:58m:13s remains)
INFO - root - 2017-12-10 00:48:42.698875: step 73570, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.844 sec/batch; 60h:43m:51s remains)
INFO - root - 2017-12-10 00:48:51.349115: step 73580, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.849 sec/batch; 61h:05m:47s remains)
INFO - root - 2017-12-10 00:49:00.028885: step 73590, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 62h:30m:51s remains)
INFO - root - 2017-12-10 00:49:08.584656: step 73600, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.856 sec/batch; 61h:31m:54s remains)
2017-12-10 00:49:09.474339: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0018229906 -0.0018239158 -0.0018247595 -0.0018254224 -0.0018259849 -0.0018263609 -0.0018263892 -0.0018263023 -0.00182587 -0.0018250596 -0.0018241567 -0.0018231627 -0.0018223776 -0.0018227316 -0.0018244472][-0.0018234409 -0.0018244217 -0.0018252359 -0.0018257159 -0.0018260989 -0.0018264279 -0.0018265343 -0.0018265388 -0.0018262522 -0.0018256488 -0.001824968 -0.0018240615 -0.0018234489 -0.0018236906 -0.0018252207][-0.001822554 -0.001823648 -0.0018245255 -0.0018250801 -0.0018254661 -0.0018258018 -0.0018259409 -0.0018259445 -0.0018258016 -0.0018253839 -0.0018249156 -0.0018240396 -0.001823745 -0.0018240784 -0.0018255549][-0.0018202733 -0.001821411 -0.0018223635 -0.0018230139 -0.0018235199 -0.0018239018 -0.0018241449 -0.0018243359 -0.0018243695 -0.0018242714 -0.0018239804 -0.0018235046 -0.0018234832 -0.0018240624 -0.0018255272][-0.0018156854 -0.0018168747 -0.0018181072 -0.0018190941 -0.0018198458 -0.001820469 -0.0018209949 -0.0018215467 -0.00182175 -0.0018218512 -0.0018216164 -0.0018214672 -0.0018217277 -0.0018226895 -0.0018244614][-0.0018089811 -0.0018101068 -0.0018115509 -0.0018128088 -0.0018139011 -0.0018149111 -0.0018158761 -0.0018168264 -0.0018173858 -0.0018178632 -0.0018178555 -0.0018180936 -0.0018186537 -0.001820124 -0.0018225107][-0.0018027694 -0.0018035969 -0.0018051051 -0.0018066169 -0.001807892 -0.0018092986 -0.0018104579 -0.0018115443 -0.0018122783 -0.0018130767 -0.0018132827 -0.0018137746 -0.0018148145 -0.0018167621 -0.0018198716][-0.0017995918 -0.0018000038 -0.001801379 -0.0018028519 -0.0018040134 -0.0018053604 -0.0018063249 -0.0018072461 -0.0018078554 -0.001808669 -0.0018090766 -0.0018098584 -0.0018113438 -0.0018137153 -0.0018174188][-0.0018010895 -0.0018013344 -0.0018024727 -0.0018037074 -0.0018045387 -0.0018052222 -0.00180548 -0.0018059387 -0.0018061885 -0.0018067154 -0.0018070866 -0.0018079635 -0.0018097523 -0.0018124095 -0.0018164126][-0.00180717 -0.0018073632 -0.0018080486 -0.0018086464 -0.0018088955 -0.0018089194 -0.001808563 -0.0018083976 -0.0018081438 -0.0018081012 -0.0018081805 -0.0018086355 -0.0018100333 -0.0018125424 -0.0018163681][-0.0018157421 -0.0018156099 -0.0018157669 -0.0018156131 -0.0018151677 -0.0018145787 -0.0018137532 -0.0018130129 -0.0018122415 -0.0018117321 -0.0018112173 -0.0018111747 -0.00181207 -0.0018141762 -0.0018175184][-0.0018247003 -0.0018242624 -0.0018238735 -0.001823227 -0.001822489 -0.0018217133 -0.0018208034 -0.0018196719 -0.001818415 -0.0018172506 -0.0018159926 -0.0018150023 -0.0018151666 -0.0018165178 -0.0018191597][-0.0018319718 -0.0018313539 -0.0018309354 -0.0018302797 -0.0018295737 -0.0018288813 -0.0018280157 -0.001826768 -0.0018252868 -0.0018235702 -0.0018217195 -0.00182005 -0.0018194735 -0.0018199476 -0.001821733][-0.0018372135 -0.0018366356 -0.001836288 -0.0018357832 -0.0018352304 -0.001834478 -0.0018334069 -0.0018321332 -0.0018304788 -0.0018283585 -0.0018262038 -0.0018241238 -0.0018230365 -0.0018228507 -0.0018240244][-0.0018396318 -0.0018390673 -0.0018387599 -0.0018384763 -0.0018380679 -0.0018374418 -0.0018363474 -0.001835213 -0.0018336824 -0.001831616 -0.0018293648 -0.0018273363 -0.0018259785 -0.0018253394 -0.0018259047]]...]
INFO - root - 2017-12-10 00:49:17.797734: step 73610, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.848 sec/batch; 60h:58m:46s remains)
INFO - root - 2017-12-10 00:49:26.379219: step 73620, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.895 sec/batch; 64h:23m:15s remains)
INFO - root - 2017-12-10 00:49:35.069200: step 73630, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.874 sec/batch; 62h:50m:34s remains)
INFO - root - 2017-12-10 00:49:43.731175: step 73640, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.863 sec/batch; 62h:03m:13s remains)
INFO - root - 2017-12-10 00:49:52.258428: step 73650, loss = 0.82, batch loss = 0.69 (9.8 examples/sec; 0.819 sec/batch; 58h:51m:42s remains)
INFO - root - 2017-12-10 00:50:00.952370: step 73660, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.843 sec/batch; 60h:34m:52s remains)
INFO - root - 2017-12-10 00:50:09.683728: step 73670, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.876 sec/batch; 63h:00m:16s remains)
INFO - root - 2017-12-10 00:50:18.374128: step 73680, loss = 0.83, batch loss = 0.70 (9.1 examples/sec; 0.882 sec/batch; 63h:23m:30s remains)
INFO - root - 2017-12-10 00:50:27.021855: step 73690, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.855 sec/batch; 61h:27m:56s remains)
INFO - root - 2017-12-10 00:50:35.847847: step 73700, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.876 sec/batch; 62h:59m:43s remains)
2017-12-10 00:50:36.741107: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0018323172 -0.0018315658 -0.0018314666 -0.0018315169 -0.0018314499 -0.0018315425 -0.0018316605 -0.0018318718 -0.0018321341 -0.0018318812 -0.0018244222 -0.0018033634 -0.0017643279 -0.0017241917 -0.0017063993][-0.0018323826 -0.0018315845 -0.0018314655 -0.0018314382 -0.0018313871 -0.001831373 -0.001831559 -0.0018319268 -0.0018319803 -0.0018288505 -0.0018035943 -0.0017417722 -0.0016424935 -0.00155015 -0.001516163][-0.0018328979 -0.0018322648 -0.0018322545 -0.0018324543 -0.0018325194 -0.0018325552 -0.0018328074 -0.0018330716 -0.001831102 -0.0018166262 -0.0017455879 -0.0015887402 -0.0013616843 -0.001168825 -0.0011137226][-0.0018330073 -0.0018325833 -0.0018328454 -0.0018333109 -0.0018335081 -0.0018336779 -0.0018340556 -0.0018334721 -0.0018240688 -0.0017771316 -0.0016072351 -0.0012571444 -0.000765153 -0.00033980864 -0.00020646711][-0.0018328862 -0.0018325713 -0.0018330321 -0.001833817 -0.0018340944 -0.0018343051 -0.0018346745 -0.0018317663 -0.0018045253 -0.001689467 -0.0013499926 -0.00069015287 0.00020701054 0.00098846189 0.001281754][-0.0018326469 -0.0018322821 -0.00183259 -0.0018334404 -0.0018339092 -0.0018341949 -0.0018343955 -0.0018273129 -0.0017690958 -0.0015469276 -0.00097193482 8.8721979e-05 0.0015130405 0.0028027985 0.0033794646][-0.0018322977 -0.0018319276 -0.0018321979 -0.0018329568 -0.0018334816 -0.0018338525 -0.0018338539 -0.0018216525 -0.0017252603 -0.0013851926 -0.00057697378 0.00084200862 0.0027258657 0.0044964436 0.0054218643][-0.0018319137 -0.0018313965 -0.0018314522 -0.0018320003 -0.001832419 -0.0018326103 -0.0018325069 -0.0018182703 -0.0017034992 -0.0013133823 -0.00042178703 0.001106439 0.0031482964 0.0051480695 0.0063218237][-0.0018314178 -0.0018306935 -0.0018304898 -0.0018307042 -0.0018307976 -0.0018307057 -0.0018304847 -0.0018188949 -0.0017194325 -0.0013812948 -0.00060659926 0.00073089672 0.002548947 0.0044020703 0.00558675][-0.0018307446 -0.0018300236 -0.0018296357 -0.0018294939 -0.001829366 -0.0018290209 -0.0018285181 -0.0018222515 -0.0017671307 -0.0015698062 -0.0010887593 -0.0002006978 0.0010755317 0.0024366709 0.0033586556][-0.0018300109 -0.0018291884 -0.001828761 -0.0018284762 -0.0018283138 -0.0018278231 -0.001827065 -0.0018252643 -0.0018063206 -0.0017319792 -0.0015243306 -0.0010891561 -0.00040653034 0.00036597473 0.00091181172][-0.00182969 -0.001828691 -0.0018281874 -0.0018277757 -0.0018274756 -0.0018272402 -0.0018270714 -0.0018269311 -0.0018250622 -0.0018161421 -0.0017723028 -0.0016433557 -0.0014040554 -0.0011076805 -0.00088964903][-0.0018295335 -0.0018284972 -0.0018280493 -0.0018276352 -0.0018273246 -0.0018271763 -0.001827036 -0.0018270015 -0.0018270207 -0.0018272114 -0.0018241972 -0.0018066706 -0.0017635145 -0.0017024729 -0.0016560775][-0.0018294161 -0.0018285653 -0.0018282251 -0.0018279325 -0.001827799 -0.0018277848 -0.0018277061 -0.0018276861 -0.0018276145 -0.0018276391 -0.0018276505 -0.0018276154 -0.0018273143 -0.0018270777 -0.0018269975][-0.0018293939 -0.001828584 -0.0018284008 -0.0018283003 -0.0018283336 -0.0018283752 -0.0018283768 -0.0018284338 -0.0018284728 -0.0018284728 -0.0018284699 -0.0018284752 -0.001828621 -0.00182901 -0.0018294883]]...]
INFO - root - 2017-12-10 00:50:45.278762: step 73710, loss = 0.83, batch loss = 0.70 (9.3 examples/sec; 0.862 sec/batch; 61h:55m:50s remains)
INFO - root - 2017-12-10 00:50:53.736491: step 73720, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.901 sec/batch; 64h:44m:30s remains)
INFO - root - 2017-12-10 00:51:02.276083: step 73730, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.850 sec/batch; 61h:05m:24s remains)
INFO - root - 2017-12-10 00:51:10.886449: step 73740, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.860 sec/batch; 61h:50m:04s remains)
INFO - root - 2017-12-10 00:51:19.418652: step 73750, loss = 0.82, batch loss = 0.69 (11.1 examples/sec; 0.721 sec/batch; 51h:47m:52s remains)
INFO - root - 2017-12-10 00:51:28.078682: step 73760, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.892 sec/batch; 64h:06m:36s remains)
INFO - root - 2017-12-10 00:51:36.781742: step 73770, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.876 sec/batch; 62h:59m:12s remains)
INFO - root - 2017-12-10 00:51:45.516536: step 73780, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 62h:36m:25s remains)
INFO - root - 2017-12-10 00:51:54.064493: step 73790, loss = 0.83, batch loss = 0.70 (9.2 examples/sec; 0.868 sec/batch; 62h:24m:16s remains)
INFO - root - 2017-12-10 00:52:02.741891: step 73800, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.838 sec/batch; 60h:14m:06s remains)
2017-12-10 00:52:03.606902: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0018302602 -0.0018310713 -0.0018327836 -0.0018338357 -0.0018334463 -0.0018316169 -0.0018291831 -0.0018268105 -0.0018251607 -0.0018250641 -0.0018260517 -0.0018269573 -0.0018272874 -0.0018267978 -0.0018250556][-0.0018269035 -0.0018278955 -0.0018301366 -0.0018319403 -0.0018321455 -0.0018307222 -0.0018284976 -0.0018262836 -0.0018247998 -0.0018246755 -0.0018254573 -0.001826235 -0.0018264945 -0.0018257854 -0.0018236975][-0.0018238049 -0.0018247952 -0.001827182 -0.0018294986 -0.0018303661 -0.0018296522 -0.0018279209 -0.001826116 -0.0018249247 -0.0018247091 -0.0018251808 -0.0018256312 -0.0018256807 -0.0018247603 -0.0018226455][-0.0018224865 -0.0018230085 -0.0018249857 -0.0018272385 -0.0018284916 -0.0018284977 -0.0018275054 -0.0018262932 -0.0018255371 -0.0018254084 -0.0018256084 -0.0018256272 -0.0018253007 -0.0018241144 -0.0018219674][-0.0018221737 -0.0018223738 -0.0018237936 -0.001825597 -0.0018268304 -0.0018271614 -0.0018267534 -0.0018259574 -0.0018254437 -0.0018253035 -0.0018252375 -0.0018249321 -0.0018243899 -0.0018232003 -0.0018212482][-0.0018219857 -0.0018218561 -0.0018226255 -0.0018237797 -0.0018247495 -0.0018252191 -0.0018251955 -0.0018250045 -0.0018249806 -0.0018248647 -0.0018244958 -0.0018239454 -0.0018232584 -0.0018221397 -0.0018204717][-0.0018214786 -0.0018210218 -0.0018212347 -0.0018217524 -0.0018224058 -0.0018230246 -0.0018234738 -0.0018238162 -0.0018240552 -0.001823951 -0.001823412 -0.0018226395 -0.0018218076 -0.0018207852 -0.0018195667][-0.0018207297 -0.0018199318 -0.0018196432 -0.0018196658 -0.0018200632 -0.0018206809 -0.0018213396 -0.0018218784 -0.0018221388 -0.00182207 -0.0018215526 -0.0018207791 -0.0018200526 -0.0018193302 -0.0018186448][-0.001820048 -0.0018189007 -0.0018182669 -0.0018179876 -0.0018181164 -0.0018186225 -0.0018192986 -0.0018198341 -0.0018200341 -0.0018200375 -0.0018195943 -0.0018189629 -0.0018184074 -0.0018180555 -0.0018178037][-0.0018191853 -0.0018179318 -0.0018172087 -0.001816747 -0.0018165598 -0.0018166916 -0.0018170639 -0.0018174669 -0.0018176342 -0.0018176173 -0.0018172922 -0.0018169433 -0.0018167156 -0.001816703 -0.0018168197][-0.0018188704 -0.0018175797 -0.0018168809 -0.0018163855 -0.0018161183 -0.0018160851 -0.0018162328 -0.0018165086 -0.0018166885 -0.001816729 -0.0018164902 -0.001816237 -0.0018161462 -0.0018162704 -0.0018164619][-0.0018190606 -0.0018177994 -0.0018171116 -0.0018167071 -0.001816452 -0.0018163663 -0.0018164006 -0.0018165298 -0.0018166078 -0.0018166094 -0.0018164027 -0.0018161481 -0.0018160486 -0.0018161442 -0.0018163061][-0.0018196702 -0.0018184813 -0.001817769 -0.0018174024 -0.0018171823 -0.0018170517 -0.0018169606 -0.001816913 -0.0018168289 -0.0018167082 -0.0018164562 -0.00181623 -0.0018161199 -0.0018161605 -0.0018162278][-0.0018204708 -0.0018194966 -0.0018188162 -0.0018183654 -0.0018180829 -0.0018179242 -0.0018177875 -0.0018176278 -0.0018174015 -0.0018171663 -0.001816854 -0.0018165815 -0.0018164029 -0.0018163341 -0.0018162737][-0.0018208928 -0.0018200976 -0.0018195197 -0.0018190248 -0.0018186775 -0.0018184551 -0.0018182812 -0.0018180556 -0.0018177727 -0.0018174758 -0.0018171034 -0.0018167796 -0.0018165506 -0.0018164245 -0.0018162888]]...]
INFO - root - 2017-12-10 00:52:12.146458: step 73810, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.863 sec/batch; 61h:59m:56s remains)
INFO - root - 2017-12-10 00:52:20.626835: step 73820, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.878 sec/batch; 63h:06m:51s remains)
INFO - root - 2017-12-10 00:52:29.275672: step 73830, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.861 sec/batch; 61h:53m:40s remains)
INFO - root - 2017-12-10 00:52:37.798637: step 73840, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.847 sec/batch; 60h:50m:16s remains)
INFO - root - 2017-12-10 00:52:46.357936: step 73850, loss = 0.82, batch loss = 0.69 (9.7 examples/sec; 0.823 sec/batch; 59h:07m:43s remains)
INFO - root - 2017-12-10 00:52:54.911796: step 73860, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.857 sec/batch; 61h:34m:51s remains)
INFO - root - 2017-12-10 00:53:03.411737: step 73870, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.847 sec/batch; 60h:49m:50s remains)
INFO - root - 2017-12-10 00:53:11.989903: step 73880, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.860 sec/batch; 61h:47m:44s remains)
INFO - root - 2017-12-10 00:53:20.625533: step 73890, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.899 sec/batch; 64h:33m:40s remains)
INFO - root - 2017-12-10 00:53:29.278754: step 73900, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.878 sec/batch; 63h:03m:59s remains)
2017-12-10 00:53:30.144969: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0017984182 -0.0017968656 -0.0017970157 -0.00179745 -0.0017979102 -0.0017982807 -0.0017985026 -0.0017985896 -0.0017984555 -0.0017981399 -0.0017976435 -0.0017971129 -0.0017965488 -0.001796041 -0.0017956296][-0.0017980967 -0.0017967706 -0.0017972374 -0.0017979953 -0.0017987809 -0.0017994904 -0.0018001202 -0.0018004463 -0.0018003937 -0.0017999962 -0.0017992816 -0.0017983844 -0.0017973681 -0.0017964127 -0.001795645][-0.0017983698 -0.0017973509 -0.0017979827 -0.0017989303 -0.001799966 -0.0018009811 -0.0018019498 -0.0018025887 -0.0018027285 -0.0018023452 -0.0018014373 -0.0018001838 -0.001798738 -0.0017973495 -0.0017962462][-0.0017983031 -0.001797436 -0.0017981436 -0.0017992607 -0.0018005118 -0.0018018266 -0.0018031362 -0.0018041866 -0.001804592 -0.0018042848 -0.0018032801 -0.0018017587 -0.0017999809 -0.0017981795 -0.0017967653][-0.0017980525 -0.0017971353 -0.0017978813 -0.0017991832 -0.0018006639 -0.0018022546 -0.001803983 -0.0018055292 -0.0018062009 -0.0018059313 -0.0018048125 -0.0018029955 -0.0018008045 -0.0017986087 -0.0017969455][-0.0017974133 -0.0017963069 -0.0017969109 -0.0017981402 -0.0017996866 -0.0018013801 -0.0018033407 -0.0018052796 -0.0018062284 -0.0018060126 -0.0018047785 -0.0018028829 -0.0018005399 -0.0017981805 -0.0017965176][-0.0017966024 -0.0017951881 -0.0017955033 -0.0017964731 -0.0017977817 -0.001799214 -0.0018009387 -0.001802789 -0.0018038227 -0.0018038228 -0.0018027988 -0.0018010958 -0.00179907 -0.0017969779 -0.0017955774][-0.0017960269 -0.0017943737 -0.0017944487 -0.0017952271 -0.0017962588 -0.0017972689 -0.0017985485 -0.001800005 -0.0018008746 -0.0018010401 -0.0018003712 -0.0017991501 -0.0017975739 -0.0017958257 -0.0017947278][-0.001795653 -0.00179389 -0.0017939581 -0.0017946773 -0.0017955579 -0.0017963276 -0.00179743 -0.0017986374 -0.0017991655 -0.0017991139 -0.0017985962 -0.0017977749 -0.0017965722 -0.0017951728 -0.0017943151][-0.0017954353 -0.0017937754 -0.0017938326 -0.0017943588 -0.0017950278 -0.0017956231 -0.0017965032 -0.001797415 -0.0017976968 -0.0017974071 -0.0017968519 -0.0017962668 -0.0017954436 -0.0017945135 -0.0017940112][-0.0017956011 -0.0017940921 -0.0017940523 -0.0017942999 -0.0017946054 -0.001794851 -0.0017952821 -0.0017957377 -0.0017957877 -0.0017955402 -0.0017952169 -0.0017949333 -0.001794505 -0.001794011 -0.0017938474][-0.0017958502 -0.0017943074 -0.0017942409 -0.0017944201 -0.0017945454 -0.001794589 -0.001794747 -0.0017949376 -0.0017948491 -0.0017946335 -0.0017945356 -0.0017944799 -0.0017942848 -0.0017940277 -0.0017939403][-0.0017959191 -0.0017943677 -0.0017942548 -0.0017944871 -0.0017946865 -0.0017947889 -0.0017949161 -0.0017950335 -0.0017949508 -0.0017947539 -0.0017946061 -0.0017945134 -0.0017943637 -0.001794166 -0.0017940308][-0.0017959905 -0.0017943415 -0.0017941506 -0.0017943685 -0.0017945839 -0.0017947431 -0.0017948569 -0.0017949177 -0.0017948716 -0.0017947396 -0.0017945704 -0.001794412 -0.0017942588 -0.0017941005 -0.0017939603][-0.0017960782 -0.0017943849 -0.0017940389 -0.0017941887 -0.0017943396 -0.0017944676 -0.0017945296 -0.0017945543 -0.0017945125 -0.0017944118 -0.0017942762 -0.0017941312 -0.0017940293 -0.0017939266 -0.0017938388]]...]
INFO - root - 2017-12-10 00:53:38.709139: step 73910, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.891 sec/batch; 63h:59m:37s remains)
INFO - root - 2017-12-10 00:53:47.233600: step 73920, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.839 sec/batch; 60h:14m:45s remains)
INFO - root - 2017-12-10 00:53:55.944218: step 73930, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.875 sec/batch; 62h:49m:52s remains)
INFO - root - 2017-12-10 00:54:04.606231: step 73940, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.878 sec/batch; 63h:04m:06s remains)
INFO - root - 2017-12-10 00:54:13.442283: step 73950, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.866 sec/batch; 62h:13m:07s remains)
INFO - root - 2017-12-10 00:54:21.871308: step 73960, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.848 sec/batch; 60h:54m:51s remains)
INFO - root - 2017-12-10 00:54:30.561643: step 73970, loss = 0.82, batch loss = 0.70 (9.3 examples/sec; 0.861 sec/batch; 61h:49m:39s remains)
INFO - root - 2017-12-10 00:54:39.142405: step 73980, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.830 sec/batch; 59h:35m:05s remains)
INFO - root - 2017-12-10 00:54:47.678241: step 73990, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 62h:19m:17s remains)
INFO - root - 2017-12-10 00:54:56.355618: step 74000, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.858 sec/batch; 61h:34m:59s remains)
2017-12-10 00:54:57.274977: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0018313591 -0.0018304294 -0.0018305349 -0.0018310071 -0.0018319675 -0.001833118 -0.0018343221 -0.0018356041 -0.0018365824 -0.0018369936 -0.0018366062 -0.0018357205 -0.0018345932 -0.0018333516 -0.0018321681][-0.001830808 -0.0018299595 -0.0018303627 -0.001831252 -0.0018326595 -0.0018341432 -0.0018356278 -0.0018369901 -0.0018379929 -0.001838389 -0.0018380317 -0.0018371177 -0.0018357971 -0.0018344182 -0.0018329752][-0.0018312406 -0.0018306407 -0.0018313433 -0.0018326886 -0.0018343453 -0.0018359295 -0.0018372788 -0.0018382142 -0.0018387934 -0.0018389791 -0.0018386132 -0.0018377512 -0.0018366689 -0.0018355813 -0.0018341442][-0.0018319008 -0.0018316125 -0.0018326544 -0.0018343158 -0.0018359307 -0.0018371821 -0.0018379141 -0.0018380656 -0.0018379847 -0.0018378946 -0.0018378827 -0.0018375003 -0.0018368957 -0.0018362158 -0.0018350929][-0.0018324336 -0.0018323489 -0.0018335487 -0.0018352072 -0.001836505 -0.0018371529 -0.0018369623 -0.001836209 -0.0018355069 -0.0018355168 -0.0018362071 -0.0018367179 -0.0018367771 -0.0018364994 -0.0018357149][-0.0018325931 -0.0018325186 -0.0018336251 -0.0018349814 -0.0018357243 -0.0018356348 -0.0018345588 -0.0018330412 -0.001832101 -0.0018324408 -0.0018339573 -0.0018354399 -0.0018362853 -0.0018363812 -0.0018358267][-0.0018328329 -0.0018326786 -0.0018334693 -0.0018343147 -0.0018342432 -0.0018333253 -0.0018318448 -0.0018302852 -0.0018294753 -0.0018300398 -0.0018319188 -0.0018339106 -0.0018353459 -0.0018358476 -0.0018354574][-0.0018333683 -0.0018331279 -0.001833637 -0.0018340708 -0.0018335759 -0.0018322611 -0.0018308017 -0.0018295199 -0.0018290004 -0.0018297418 -0.0018314831 -0.0018334145 -0.0018348085 -0.0018352838 -0.0018349803][-0.0018337958 -0.001833501 -0.0018340774 -0.0018346569 -0.0018347012 -0.001834043 -0.0018329151 -0.0018317902 -0.0018312345 -0.0018316892 -0.0018327934 -0.0018340437 -0.0018348777 -0.0018350434 -0.0018347361][-0.0018341644 -0.0018338329 -0.0018343318 -0.0018350329 -0.0018355412 -0.0018356199 -0.0018352998 -0.0018347398 -0.0018343368 -0.0018344271 -0.0018348425 -0.0018352516 -0.001835452 -0.001835251 -0.0018348444][-0.0018343158 -0.001833863 -0.0018342948 -0.0018348993 -0.0018354614 -0.0018358403 -0.0018360214 -0.0018360392 -0.00183595 -0.0018359981 -0.0018359852 -0.0018358984 -0.0018357396 -0.0018353966 -0.0018348808][-0.0018345148 -0.0018338966 -0.0018341389 -0.0018345285 -0.0018349428 -0.0018353249 -0.0018356608 -0.0018359144 -0.0018361005 -0.0018362362 -0.0018361592 -0.0018359222 -0.001835596 -0.0018351874 -0.0018347016][-0.0018347154 -0.0018340013 -0.0018340497 -0.0018342895 -0.0018345381 -0.0018348057 -0.0018350771 -0.0018353106 -0.001835492 -0.0018355849 -0.0018355041 -0.001835282 -0.0018350095 -0.0018347341 -0.0018344414][-0.0018348277 -0.0018340377 -0.0018339949 -0.0018341497 -0.0018342758 -0.0018343996 -0.0018344964 -0.001834563 -0.0018345916 -0.0018345978 -0.0018345377 -0.0018344359 -0.00183433 -0.0018342314 -0.0018341193][-0.0018348666 -0.0018339972 -0.001833886 -0.0018339781 -0.0018340575 -0.0018341095 -0.0018341165 -0.0018341126 -0.0018341121 -0.0018340941 -0.0018340432 -0.0018339846 -0.0018339399 -0.0018339037 -0.0018338673]]...]
INFO - root - 2017-12-10 00:55:05.721040: step 74010, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.866 sec/batch; 62h:11m:16s remains)
INFO - root - 2017-12-10 00:55:14.233772: step 74020, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 62h:20m:09s remains)
INFO - root - 2017-12-10 00:55:22.807587: step 74030, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.865 sec/batch; 62h:07m:10s remains)
INFO - root - 2017-12-10 00:55:31.313435: step 74040, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 62h:15m:00s remains)
INFO - root - 2017-12-10 00:55:39.798041: step 74050, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.837 sec/batch; 60h:04m:23s remains)
INFO - root - 2017-12-10 00:55:48.188028: step 74060, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.849 sec/batch; 60h:56m:32s remains)
INFO - root - 2017-12-10 00:55:56.848377: step 74070, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.831 sec/batch; 59h:41m:04s remains)
INFO - root - 2017-12-10 00:56:05.617014: step 74080, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.852 sec/batch; 61h:08m:54s remains)
INFO - root - 2017-12-10 00:56:14.315380: step 74090, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.859 sec/batch; 61h:37m:58s remains)
INFO - root - 2017-12-10 00:56:22.985157: step 74100, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.851 sec/batch; 61h:05m:30s remains)
2017-12-10 00:56:23.879521: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0018087284 -0.0018073532 -0.0018071749 -0.0018074135 -0.0018078339 -0.0018082391 -0.0018085951 -0.0018087217 -0.0018085586 -0.0018081794 -0.0018075509 -0.0018072114 -0.0018071044 -0.0018072177 -0.0018077416][-0.0018079258 -0.0018066482 -0.0018066602 -0.00180725 -0.0018082071 -0.0018090567 -0.0018098617 -0.0018101662 -0.0018099361 -0.0018093925 -0.0018085132 -0.0018078377 -0.0018074521 -0.001807523 -0.0018080408][-0.0018075238 -0.0018065042 -0.0018068522 -0.0018079475 -0.0018095638 -0.0018110559 -0.001812477 -0.0018131523 -0.0018129103 -0.0018121739 -0.0018108577 -0.0018095019 -0.0018085206 -0.0018083603 -0.0018087664][-0.0018069776 -0.0018061942 -0.0018069352 -0.0018086093 -0.0018109198 -0.0018132149 -0.0018154058 -0.0018165052 -0.0018163481 -0.0018152706 -0.0018133734 -0.0018112033 -0.0018094464 -0.0018088759 -0.0018091014][-0.0018063661 -0.001805876 -0.0018069528 -0.0018090464 -0.0018118902 -0.0018148085 -0.0018175398 -0.001818981 -0.0018188901 -0.0018175554 -0.0018151578 -0.0018122987 -0.0018098359 -0.0018087893 -0.0018088287][-0.0018057966 -0.0018055106 -0.001806831 -0.0018091005 -0.0018121523 -0.001815422 -0.0018184035 -0.0018200123 -0.0018200238 -0.001818578 -0.0018158673 -0.0018125823 -0.0018096874 -0.0018082882 -0.0018082213][-0.0018056525 -0.0018053944 -0.0018067007 -0.0018088345 -0.0018117448 -0.001814971 -0.0018178079 -0.0018192648 -0.0018193101 -0.0018180078 -0.0018153221 -0.0018120132 -0.0018090211 -0.0018074821 -0.0018074565][-0.0018058051 -0.0018054263 -0.0018064647 -0.0018081933 -0.0018106267 -0.00181334 -0.0018155568 -0.0018164943 -0.0018163589 -0.0018153787 -0.0018132445 -0.0018105063 -0.0018079982 -0.0018067288 -0.0018069204][-0.0018060744 -0.0018055172 -0.0018063106 -0.0018075707 -0.0018093489 -0.0018111932 -0.0018125097 -0.0018127675 -0.001812462 -0.0018118523 -0.0018104916 -0.0018086164 -0.0018068701 -0.0018061069 -0.0018065519][-0.0018063338 -0.0018057587 -0.0018063581 -0.001807139 -0.0018082256 -0.0018092148 -0.0018097698 -0.0018096045 -0.0018091456 -0.0018087578 -0.0018079843 -0.0018068461 -0.0018057754 -0.0018054602 -0.001806119][-0.0018068833 -0.0018062706 -0.0018066877 -0.0018070893 -0.0018075601 -0.0018078849 -0.0018078712 -0.0018075022 -0.0018070447 -0.0018067549 -0.00180624 -0.0018054766 -0.0018047957 -0.0018047646 -0.0018055724][-0.0018075664 -0.0018068483 -0.0018071274 -0.0018073109 -0.0018074116 -0.0018073202 -0.0018070181 -0.001806644 -0.0018062931 -0.0018059496 -0.0018054183 -0.0018047357 -0.001804156 -0.0018041785 -0.0018049818][-0.0018080559 -0.001807243 -0.0018074038 -0.0018075252 -0.0018074965 -0.0018073143 -0.001807021 -0.0018067426 -0.0018065076 -0.0018061409 -0.0018054578 -0.0018046455 -0.0018039816 -0.0018039206 -0.001804639][-0.0018080997 -0.0018071644 -0.001807228 -0.0018073838 -0.0018073833 -0.0018072901 -0.0018071345 -0.0018070444 -0.0018069791 -0.0018067096 -0.0018059696 -0.0018050454 -0.0018043036 -0.0018041646 -0.0018047289][-0.0018080726 -0.0018070649 -0.0018069727 -0.0018070848 -0.0018070765 -0.0018070467 -0.001807003 -0.0018070658 -0.0018071024 -0.0018069466 -0.0018063148 -0.0018055104 -0.0018048405 -0.0018046717 -0.0018051425]]...]
INFO - root - 2017-12-10 00:56:32.332911: step 74110, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.872 sec/batch; 62h:33m:14s remains)
INFO - root - 2017-12-10 00:56:40.816684: step 74120, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.866 sec/batch; 62h:07m:09s remains)
INFO - root - 2017-12-10 00:56:49.461314: step 74130, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.888 sec/batch; 63h:45m:26s remains)
INFO - root - 2017-12-10 00:56:58.129959: step 74140, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.865 sec/batch; 62h:06m:41s remains)
INFO - root - 2017-12-10 00:57:06.801074: step 74150, loss = 0.83, batch loss = 0.70 (9.0 examples/sec; 0.892 sec/batch; 63h:59m:33s remains)
INFO - root - 2017-12-10 00:57:15.258231: step 74160, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.863 sec/batch; 61h:56m:49s remains)
INFO - root - 2017-12-10 00:57:23.875878: step 74170, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 62h:22m:47s remains)
INFO - root - 2017-12-10 00:57:32.468721: step 74180, loss = 0.82, batch loss = 0.69 (9.7 examples/sec; 0.824 sec/batch; 59h:06m:26s remains)
INFO - root - 2017-12-10 00:57:41.063704: step 74190, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.890 sec/batch; 63h:51m:37s remains)
INFO - root - 2017-12-10 00:57:49.750164: step 74200, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.891 sec/batch; 63h:57m:46s remains)
2017-12-10 00:57:50.613273: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0018122294 -0.0018110082 -0.0018109413 -0.0018110229 -0.0018110661 -0.0018111191 -0.001811204 -0.0018112777 -0.0018113035 -0.0018112746 -0.0018112198 -0.0018111484 -0.001811141 -0.0018112923 -0.0018115846][-0.0018106751 -0.0018094376 -0.0018093664 -0.0018094502 -0.001809477 -0.0018094464 -0.0018094205 -0.0018093643 -0.0018093172 -0.0018092908 -0.0018092882 -0.0018092999 -0.0018094197 -0.0018097651 -0.0018103049][-0.0018096674 -0.0018083868 -0.0018082936 -0.0018083794 -0.0018084064 -0.0018082996 -0.0018081291 -0.0018079086 -0.0018077778 -0.0018077594 -0.0018078457 -0.001807979 -0.0018082636 -0.00180884 -0.001809643][-0.0018090651 -0.0018076887 -0.0018074951 -0.0018075267 -0.0018075386 -0.0018074247 -0.0018071907 -0.0018068792 -0.0018067032 -0.0018067398 -0.0018069666 -0.0018072597 -0.001807696 -0.0018084097 -0.0018093557][-0.0018093818 -0.0018078453 -0.0018074417 -0.0018072713 -0.0018071752 -0.0018069752 -0.0018066709 -0.0018063342 -0.0018062332 -0.0018064465 -0.001806861 -0.0018073134 -0.0018078571 -0.0018085941 -0.0018095187][-0.0018104225 -0.0018088375 -0.0018082617 -0.0018077834 -0.0018073678 -0.0018068622 -0.0018063422 -0.0018059214 -0.0018059125 -0.0018064019 -0.0018070594 -0.0018076975 -0.0018083658 -0.0018091142 -0.001809947][-0.0018117722 -0.0018102339 -0.0018095094 -0.0018087699 -0.0018080341 -0.0018072128 -0.0018064203 -0.0018059041 -0.0018059865 -0.0018067353 -0.0018076572 -0.0018084605 -0.0018092191 -0.0018099328 -0.0018106214][-0.0018131889 -0.0018117095 -0.0018109528 -0.0018101566 -0.0018093007 -0.0018083111 -0.0018073412 -0.0018067345 -0.001806795 -0.0018076288 -0.001808683 -0.001809596 -0.0018103831 -0.0018109586 -0.0018114003][-0.0018142055 -0.0018129497 -0.0018123317 -0.0018116265 -0.001810835 -0.0018098957 -0.0018090111 -0.0018084924 -0.0018085063 -0.0018092068 -0.0018101273 -0.0018109096 -0.0018115514 -0.0018118962 -0.0018120512][-0.0018147791 -0.0018138218 -0.001813415 -0.0018129046 -0.0018123039 -0.0018115521 -0.0018108641 -0.0018104819 -0.0018104564 -0.001810894 -0.0018114611 -0.0018119527 -0.0018123684 -0.0018125236 -0.0018124895][-0.0018150541 -0.0018143139 -0.0018141046 -0.0018138376 -0.0018134565 -0.0018129249 -0.0018124415 -0.0018121902 -0.0018121119 -0.0018122813 -0.0018125231 -0.0018127302 -0.0018129259 -0.0018129388 -0.0018128102][-0.0018150513 -0.0018144422 -0.0018142941 -0.0018141937 -0.0018140145 -0.0018137195 -0.0018134256 -0.0018132679 -0.0018132083 -0.0018132275 -0.0018132456 -0.0018132223 -0.0018132448 -0.0018131818 -0.0018130121][-0.0018149141 -0.0018143497 -0.0018141887 -0.0018141465 -0.0018140634 -0.0018139114 -0.001813742 -0.0018136428 -0.0018136032 -0.0018135851 -0.0018135121 -0.0018133898 -0.0018133263 -0.001813239 -0.0018130798][-0.0018147225 -0.0018141457 -0.0018139652 -0.0018139484 -0.0018139001 -0.0018138187 -0.001813722 -0.0018136631 -0.0018136339 -0.0018136106 -0.0018135292 -0.0018133963 -0.0018133139 -0.0018132296 -0.0018130895][-0.0018146527 -0.0018139959 -0.0018137798 -0.0018137955 -0.0018137773 -0.0018137367 -0.0018136821 -0.0018136337 -0.0018135965 -0.0018135603 -0.0018134759 -0.0018133459 -0.0018132546 -0.0018131818 -0.0018130674]]...]
INFO - root - 2017-12-10 00:57:59.196727: step 74210, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.872 sec/batch; 62h:32m:39s remains)
INFO - root - 2017-12-10 00:58:07.614455: step 74220, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 62h:13m:05s remains)
INFO - root - 2017-12-10 00:58:16.269084: step 74230, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.857 sec/batch; 61h:30m:07s remains)
INFO - root - 2017-12-10 00:58:24.837756: step 74240, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.844 sec/batch; 60h:30m:58s remains)
INFO - root - 2017-12-10 00:58:33.301804: step 74250, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.839 sec/batch; 60h:11m:36s remains)
INFO - root - 2017-12-10 00:58:41.797580: step 74260, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.860 sec/batch; 61h:40m:37s remains)
INFO - root - 2017-12-10 00:58:50.438311: step 74270, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.882 sec/batch; 63h:14m:46s remains)
INFO - root - 2017-12-10 00:58:58.920631: step 74280, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 62h:27m:47s remains)
INFO - root - 2017-12-10 00:59:07.443152: step 74290, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.841 sec/batch; 60h:19m:38s remains)
INFO - root - 2017-12-10 00:59:16.280066: step 74300, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.894 sec/batch; 64h:07m:16s remains)
2017-12-10 00:59:17.199672: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.001813716 -0.0018132859 -0.0018133426 -0.001813175 -0.0018127335 -0.0018121634 -0.0018114921 -0.0018107353 -0.0018098658 -0.0018089713 -0.0018081164 -0.0018073181 -0.0018065205 -0.0018057413 -0.0018050642][-0.0018179097 -0.0018178876 -0.0018180134 -0.0018176853 -0.0018169389 -0.001815917 -0.0018146733 -0.0018132653 -0.0018118205 -0.0018104897 -0.0018093011 -0.0018082295 -0.0018071648 -0.0018061474 -0.001805226][-0.0018233432 -0.0018235435 -0.0018237465 -0.0018232174 -0.0018220297 -0.0018204431 -0.0018185482 -0.0018165526 -0.001814617 -0.0018129314 -0.0018115287 -0.0018102811 -0.0018089896 -0.0018077015 -0.0018063803][-0.0018278832 -0.0018282891 -0.0018284362 -0.0018276437 -0.0018261007 -0.0018241014 -0.0018217443 -0.0018194618 -0.0018173744 -0.0018156677 -0.0018142553 -0.0018129508 -0.0018114765 -0.0018098174 -0.0018079653][-0.0018298143 -0.0018302958 -0.0018304021 -0.0018294015 -0.0018276167 -0.0018254166 -0.0018228698 -0.0018205488 -0.0018186115 -0.0018172364 -0.0018162437 -0.0018151936 -0.0018137516 -0.0018118114 -0.0018094492][-0.0018287663 -0.0018290236 -0.0018290565 -0.0018280051 -0.0018261663 -0.0018239445 -0.0018214578 -0.0018194042 -0.0018179426 -0.0018172212 -0.0018168868 -0.0018163194 -0.0018151129 -0.0018130607 -0.0018103417][-0.001824848 -0.0018246681 -0.0018245525 -0.0018235772 -0.0018218957 -0.0018199416 -0.0018178285 -0.0018163775 -0.001815696 -0.0018158564 -0.0018162969 -0.0018162768 -0.0018153674 -0.0018133023 -0.0018104641][-0.0018187452 -0.0018179318 -0.0018176052 -0.0018168982 -0.00181575 -0.001814531 -0.0018133358 -0.0018129087 -0.0018132153 -0.0018141594 -0.0018151117 -0.0018153767 -0.0018146696 -0.0018127038 -0.0018099754][-0.0018120855 -0.0018107772 -0.001810439 -0.0018101739 -0.0018098229 -0.0018095732 -0.0018094147 -0.0018099414 -0.0018109712 -0.0018122561 -0.0018133034 -0.001813628 -0.0018131025 -0.0018114094 -0.0018090394][-0.0018072757 -0.00180569 -0.0018053662 -0.0018054784 -0.001805686 -0.0018060836 -0.0018065671 -0.0018075434 -0.0018088176 -0.0018101144 -0.0018110807 -0.0018113801 -0.0018110002 -0.0018096844 -0.0018078447][-0.0018047857 -0.0018030839 -0.0018026957 -0.0018029297 -0.0018033958 -0.0018040892 -0.0018048406 -0.001805842 -0.0018069563 -0.0018079338 -0.0018086516 -0.0018088896 -0.0018086687 -0.0018077862 -0.0018065391][-0.0018039239 -0.0018021574 -0.0018017731 -0.0018020548 -0.0018026151 -0.0018033669 -0.0018040671 -0.001804829 -0.0018055667 -0.0018061623 -0.0018066164 -0.001806783 -0.0018066955 -0.0018061715 -0.001805403][-0.0018042333 -0.0018025611 -0.0018021935 -0.0018024386 -0.0018028858 -0.0018034582 -0.0018039439 -0.0018043868 -0.0018047653 -0.0018050433 -0.0018052772 -0.0018053777 -0.0018053328 -0.001805026 -0.0018045581][-0.0018049638 -0.0018034698 -0.0018030748 -0.0018031786 -0.0018033993 -0.0018036811 -0.0018039083 -0.0018040994 -0.0018042495 -0.0018043432 -0.0018044321 -0.0018044726 -0.0018044208 -0.0018042105 -0.0018039158][-0.0018052896 -0.0018040303 -0.001803615 -0.0018035891 -0.0018036045 -0.0018036456 -0.0018036793 -0.0018037037 -0.0018037283 -0.0018037364 -0.0018037584 -0.0018037632 -0.0018037099 -0.001803584 -0.0018034145]]...]
INFO - root - 2017-12-10 00:59:25.605109: step 74310, loss = 0.83, batch loss = 0.70 (9.4 examples/sec; 0.850 sec/batch; 60h:56m:02s remains)
INFO - root - 2017-12-10 00:59:34.082325: step 74320, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.902 sec/batch; 64h:42m:23s remains)
INFO - root - 2017-12-10 00:59:42.776727: step 74330, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.889 sec/batch; 63h:44m:14s remains)
INFO - root - 2017-12-10 00:59:51.539827: step 74340, loss = 0.82, batch loss = 0.69 (8.7 examples/sec; 0.924 sec/batch; 66h:17m:19s remains)
INFO - root - 2017-12-10 01:00:00.318793: step 74350, loss = 0.81, batch loss = 0.68 (8.9 examples/sec; 0.895 sec/batch; 64h:10m:25s remains)
INFO - root - 2017-12-10 01:00:08.910484: step 74360, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.885 sec/batch; 63h:26m:19s remains)
INFO - root - 2017-12-10 01:00:17.717414: step 74370, loss = 0.82, batch loss = 0.69 (8.4 examples/sec; 0.947 sec/batch; 67h:55m:50s remains)
INFO - root - 2017-12-10 01:00:26.417280: step 74380, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.891 sec/batch; 63h:54m:27s remains)
INFO - root - 2017-12-10 01:00:35.167685: step 74390, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 62h:18m:19s remains)
INFO - root - 2017-12-10 01:00:43.776375: step 74400, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.866 sec/batch; 62h:05m:03s remains)
2017-12-10 01:00:44.642382: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0018028026 -0.0017998783 -0.0018029182 -0.0018107493 -0.0018147712 -0.0018113136 -0.0017912838 -0.0017656921 -0.0017251736 -0.0016659636 -0.0016113746 -0.0015621502 -0.0015376285 -0.0015365022 -0.0015735215][-0.0017545124 -0.0017834331 -0.0018042903 -0.0018067455 -0.0017856192 -0.0017400539 -0.0016595087 -0.0015570845 -0.0014423414 -0.00132968 -0.0012466684 -0.0011957169 -0.0011956231 -0.0012383489 -0.001325665][-0.0012123537 -0.0014373444 -0.0016358651 -0.0017266746 -0.0016906501 -0.0015364855 -0.0012887614 -0.000983893 -0.00066328305 -0.00039357506 -0.00022103346 -0.00016028946 -0.00020439364 -0.00034532091 -0.00057245616][0.00015553401 -0.00047512772 -0.0010992524 -0.0014688451 -0.0014947511 -0.0012044205 -0.00070291886 -9.1858907e-05 0.0005354021 0.0010423186 0.0013582773 0.0014411734 0.0013255548 0.0010319749 0.00059821818][0.0023525939 0.0010624061 -0.00022158283 -0.0010454728 -0.0012569767 -0.00092242239 -0.00023895164 0.00061546511 0.0014973424 0.0022135302 0.00267644 0.0028167521 0.0026793531 0.0022785636 0.0016677048][0.0052060736 0.0030409619 0.00087707129 -0.00056801678 -0.0010966435 -0.00087832677 -0.00021990936 0.00064493727 0.0015517791 0.0023074844 0.0028262991 0.0030310457 0.0029474949 0.0025633895 0.001927985][0.0087159444 0.0054886728 0.0021963632 -4.89885e-05 -0.0010248412 -0.0010579611 -0.00058604858 9.6458709e-05 0.00082848768 0.0014576212 0.0019131493 0.0021309787 0.0021165414 0.0018416116 0.0013392834][0.012673181 0.0083871884 0.0038883816 0.00070186413 -0.00085771037 -0.0012473898 -0.0010507642 -0.00063958124 -0.00017942907 0.00023288152 0.00054916611 0.00072777283 0.00076083269 0.00061550213 0.00030601153][0.01641826 0.01137957 0.005826625 0.0016799624 -0.00054733467 -0.0013193164 -0.0013695552 -0.0012009109 -0.0010086873 -0.00081491668 -0.0006460119 -0.0005326079 -0.00048953225 -0.00054055091 -0.00068303756][0.019524513 0.014264836 0.00800767 0.0030549057 0.00012600061 -0.0011159049 -0.0014418598 -0.0014768475 -0.0014733088 -0.0014347434 -0.0013783408 -0.0013274462 -0.0012983775 -0.0013045709 -0.0013475072][0.021310166 0.016397769 0.0099359686 0.0044386052 0.00093208242 -0.00070209 -0.0012506358 -0.0014423569 -0.0016013377 -0.0016745279 -0.0016807988 -0.0016634873 -0.0016527048 -0.0016541021 -0.0016638067][0.021391977 0.017309293 0.011258389 0.0057698735 0.0019832267 3.124238e-06 -0.00084582833 -0.0012469285 -0.0015418775 -0.0016976693 -0.0017487875 -0.001757429 -0.001765267 -0.0017702893 -0.0017717417][0.020072168 0.016998759 0.011783962 0.0067749373 0.0030665211 0.00090466917 -0.00023834873 -0.00091588381 -0.0014005458 -0.0016608825 -0.0017357627 -0.00174468 -0.0017588907 -0.0017775538 -0.0017920593][0.01849146 0.01646544 0.012337348 0.0081419665 0.0048046578 0.0025770133 0.0010765569 -3.9919047e-05 -0.00088493561 -0.0013929828 -0.0016081599 -0.0016860989 -0.0017374443 -0.0017697185 -0.0017894252][0.017491557 0.016415771 0.013423011 0.010150354 0.0072891652 0.0050323033 0.0031527593 0.0015480028 0.00024439895 -0.00062097004 -0.0010694771 -0.001282527 -0.0014187176 -0.0015069528 -0.0015648911]]...]
INFO - root - 2017-12-10 01:00:53.205040: step 74410, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.878 sec/batch; 62h:54m:41s remains)
INFO - root - 2017-12-10 01:01:01.653208: step 74420, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.860 sec/batch; 61h:37m:18s remains)
INFO - root - 2017-12-10 01:01:10.341965: step 74430, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.860 sec/batch; 61h:39m:40s remains)
INFO - root - 2017-12-10 01:01:19.025834: step 74440, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.853 sec/batch; 61h:10m:26s remains)
INFO - root - 2017-12-10 01:01:27.723770: step 74450, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.856 sec/batch; 61h:22m:29s remains)
INFO - root - 2017-12-10 01:01:36.296174: step 74460, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.848 sec/batch; 60h:49m:02s remains)
INFO - root - 2017-12-10 01:01:44.977714: step 74470, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.883 sec/batch; 63h:18m:11s remains)
INFO - root - 2017-12-10 01:01:53.853294: step 74480, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.889 sec/batch; 63h:41m:15s remains)
INFO - root - 2017-12-10 01:02:02.683800: step 74490, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.898 sec/batch; 64h:23m:39s remains)
INFO - root - 2017-12-10 01:02:11.387381: step 74500, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.883 sec/batch; 63h:16m:17s remains)
2017-12-10 01:02:12.437983: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0018110964 -0.001810226 -0.0018102142 -0.0018103155 -0.0018103013 -0.0018102189 -0.0018100414 -0.0018098665 -0.0018096749 -0.0018094663 -0.0018093131 -0.0018092154 -0.001809162 -0.0018091459 -0.0018091083][-0.0018109797 -0.0018100212 -0.00181001 -0.0018101445 -0.0018100984 -0.0018099205 -0.001809689 -0.0018094423 -0.0018091722 -0.0018089382 -0.0018087577 -0.0018086506 -0.0018086148 -0.001808638 -0.0018086369][-0.0018112262 -0.0018101805 -0.0018101375 -0.0018102964 -0.0018102167 -0.0018099709 -0.0018096598 -0.0018093463 -0.0018090145 -0.0018087267 -0.0018084887 -0.0018083545 -0.0018083696 -0.0018084719 -0.0018085324][-0.0018115477 -0.0018103387 -0.0018101606 -0.0018102669 -0.0018101364 -0.0018098421 -0.0018094807 -0.0018091121 -0.0018087856 -0.0018084307 -0.0018080936 -0.0018078989 -0.0018079592 -0.00180817 -0.0018083064][-0.0018119578 -0.0018105874 -0.0018102202 -0.0018102051 -0.0018100445 -0.0018097657 -0.0018094077 -0.0018090176 -0.0018087386 -0.001808366 -0.0018079202 -0.0018075915 -0.00180763 -0.0018078741 -0.0018080568][-0.0018121571 -0.0018107035 -0.0018101285 -0.0018098647 -0.0018096551 -0.0018094049 -0.0018090261 -0.0018085589 -0.0018084107 -0.0018082126 -0.0018077489 -0.0018073651 -0.0018073742 -0.0018076536 -0.0018078395][-0.0018120045 -0.0018105229 -0.0018097514 -0.0018092529 -0.0018089784 -0.0018087649 -0.0018082851 -0.001807675 -0.0018075804 -0.0018076241 -0.001807288 -0.0018069671 -0.0018070005 -0.0018073529 -0.0018076348][-0.0018118607 -0.0018103806 -0.001809551 -0.001808934 -0.0018086534 -0.001808482 -0.0018079199 -0.0018071862 -0.0018070465 -0.0018072348 -0.0018070277 -0.0018067026 -0.0018066862 -0.0018070458 -0.0018074298][-0.0018116309 -0.0018102693 -0.001809608 -0.0018090714 -0.0018088585 -0.0018087982 -0.0018083632 -0.0018077284 -0.0018074636 -0.0018075212 -0.0018072944 -0.0018069043 -0.0018067828 -0.0018070434 -0.0018073962][-0.0018111808 -0.0018101007 -0.0018097274 -0.0018093961 -0.0018092946 -0.0018093259 -0.0018090629 -0.0018086267 -0.0018082863 -0.0018081624 -0.0018078146 -0.0018073749 -0.0018071977 -0.0018073209 -0.0018075666][-0.0018107068 -0.0018097647 -0.001809636 -0.0018095578 -0.0018095541 -0.001809612 -0.0018094791 -0.0018092163 -0.0018088361 -0.0018085259 -0.0018080887 -0.0018076857 -0.001807532 -0.0018075852 -0.001807757][-0.0018103565 -0.0018094425 -0.001809419 -0.0018095073 -0.00180957 -0.0018096482 -0.0018095861 -0.0018094617 -0.0018091184 -0.001808756 -0.0018083025 -0.0018079083 -0.0018077744 -0.0018077972 -0.0018079345][-0.0018101181 -0.0018092212 -0.0018092417 -0.0018094468 -0.0018095906 -0.0018097056 -0.001809705 -0.0018096539 -0.0018093933 -0.0018090436 -0.0018086158 -0.0018082233 -0.0018080552 -0.001808038 -0.0018081146][-0.0018100108 -0.0018090502 -0.0018090616 -0.0018093116 -0.001809497 -0.0018096353 -0.0018096948 -0.0018096834 -0.0018095296 -0.0018092743 -0.0018089304 -0.0018085937 -0.0018083922 -0.0018083192 -0.0018083265][-0.0018100149 -0.0018089596 -0.0018088252 -0.001809005 -0.0018091562 -0.0018092688 -0.0018093388 -0.0018093575 -0.0018092896 -0.0018091452 -0.0018089267 -0.0018086985 -0.001808544 -0.0018084747 -0.001808458]]...]
INFO - root - 2017-12-10 01:02:20.886032: step 74510, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.866 sec/batch; 62h:03m:34s remains)
INFO - root - 2017-12-10 01:02:29.395867: step 74520, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.876 sec/batch; 62h:47m:08s remains)
INFO - root - 2017-12-10 01:02:38.223475: step 74530, loss = 0.83, batch loss = 0.70 (9.1 examples/sec; 0.881 sec/batch; 63h:06m:10s remains)
INFO - root - 2017-12-10 01:02:46.958791: step 74540, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.874 sec/batch; 62h:38m:11s remains)
INFO - root - 2017-12-10 01:02:55.715247: step 74550, loss = 0.81, batch loss = 0.68 (9.4 examples/sec; 0.854 sec/batch; 61h:11m:17s remains)
INFO - root - 2017-12-10 01:03:04.170968: step 74560, loss = 0.81, batch loss = 0.68 (10.0 examples/sec; 0.803 sec/batch; 57h:31m:48s remains)
INFO - root - 2017-12-10 01:03:12.888499: step 74570, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.837 sec/batch; 59h:57m:50s remains)
INFO - root - 2017-12-10 01:03:21.540091: step 74580, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 61h:54m:25s remains)
INFO - root - 2017-12-10 01:03:30.251497: step 74590, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.849 sec/batch; 60h:49m:24s remains)
INFO - root - 2017-12-10 01:03:38.977816: step 74600, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.872 sec/batch; 62h:28m:51s remains)
2017-12-10 01:03:39.898641: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.023588775 0.024330586 0.023507541 0.021448933 0.018726103 0.015664468 0.012367901 0.0092141991 0.00652328 0.004366742 0.0025730403 0.0010792358 -8.9554931e-05 -0.00089012884 -0.0013832948][0.029968053 0.031082995 0.0302389 0.027823443 0.024554342 0.020897193 0.017005339 0.013248324 0.0099585876 0.0072160386 0.0048161373 0.0026981616 0.00095633243 -0.00029879261 -0.0011172511][0.034965608 0.036320604 0.035362672 0.032667026 0.029048251 0.02511823 0.021112619 0.017317031 0.013946862 0.010961367 0.0081555936 0.0054349592 0.0029878276 0.001031998 -0.00039560487][0.038340624 0.039860509 0.038956851 0.036216471 0.032562625 0.028765867 0.025133349 0.021807119 0.018758124 0.015804354 0.012752976 0.009446484 0.0061544734 0.0032248609 0.00088394864][0.040350467 0.042043597 0.041423686 0.0390949 0.035963215 0.032772921 0.029924195 0.027325708 0.024718512 0.021793939 0.018436095 0.014483948 0.010215185 0.0061302944 0.0026862691][0.041278582 0.043332972 0.04322996 0.041568752 0.039187916 0.036824755 0.034859911 0.032935459 0.03062195 0.027571283 0.023833698 0.019235913 0.01407005 0.00896803 0.0045505348][0.04115092 0.043728709 0.044161156 0.043143451 0.041414537 0.039667331 0.0383139 0.036834329 0.034709752 0.031570978 0.027601475 0.02263019 0.016919164 0.011146802 0.0060701012][0.03992667 0.042969897 0.043716893 0.043025751 0.041530162 0.03998502 0.038838733 0.037562404 0.035673253 0.032727029 0.028941143 0.024058724 0.018295592 0.012330692 0.0069893436][0.037537642 0.040752482 0.04151291 0.04082007 0.0392186 0.037486546 0.036163885 0.034904581 0.033261005 0.03071763 0.027407931 0.023000125 0.017666444 0.01203949 0.0069227233][0.033712391 0.036792606 0.037452418 0.036693782 0.034993775 0.033061728 0.031506687 0.030156165 0.028613096 0.026392123 0.023487238 0.019628914 0.014976311 0.010106773 0.0056877034][0.027783036 0.030478396 0.031106476 0.030465968 0.028961299 0.027118029 0.02555993 0.024200823 0.022719152 0.020715898 0.018151158 0.014886566 0.011067412 0.0072108661 0.0037781289][0.019977743 0.022093061 0.02269483 0.022318501 0.021274244 0.019882394 0.018645579 0.017487582 0.01618682 0.014471359 0.012329018 0.00976675 0.006908406 0.004151321 0.0017760404][0.011860932 0.013291913 0.0137921 0.013647492 0.013061281 0.012193171 0.011389229 0.010577154 0.0096160434 0.00836051 0.00684965 0.0051422375 0.0033014435 0.0015888313 0.00015973474][0.0052763554 0.0060804994 0.0064188507 0.0064149559 0.0061708703 0.0057443813 0.0053289453 0.0048631425 0.0042824484 0.0035311664 0.0026631006 0.0017371177 0.00075711368 -0.00014263473 -0.00088083779][0.0011227486 0.0014943053 0.0016802762 0.0017193597 0.0016483908 0.0014859081 0.0013175729 0.0011101415 0.000837169 0.0004865838 0.00010505307 -0.00028523523 -0.00070913055 -0.0011077237 -0.00143852]]...]
INFO - root - 2017-12-10 01:03:48.427288: step 74610, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 62h:11m:29s remains)
INFO - root - 2017-12-10 01:03:56.893774: step 74620, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.879 sec/batch; 62h:57m:09s remains)
INFO - root - 2017-12-10 01:04:05.626578: step 74630, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.899 sec/batch; 64h:24m:27s remains)
INFO - root - 2017-12-10 01:04:14.349933: step 74640, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 62h:05m:04s remains)
INFO - root - 2017-12-10 01:04:23.024262: step 74650, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.884 sec/batch; 63h:18m:08s remains)
INFO - root - 2017-12-10 01:04:31.612337: step 74660, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.856 sec/batch; 61h:17m:04s remains)
INFO - root - 2017-12-10 01:04:40.320833: step 74670, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 62h:22m:18s remains)
INFO - root - 2017-12-10 01:04:48.977409: step 74680, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.872 sec/batch; 62h:24m:57s remains)
INFO - root - 2017-12-10 01:04:57.667813: step 74690, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.865 sec/batch; 61h:58m:23s remains)
INFO - root - 2017-12-10 01:05:06.296077: step 74700, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.879 sec/batch; 62h:55m:52s remains)
2017-12-10 01:05:07.063934: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.00053468824 0.00012977479 0.00066114415 0.00093087938 0.00097416819 0.00067462993 0.00017359981 -0.00038677838 -0.000978202 -0.0014479472 -0.0017159958 -0.0017978146 -0.001803834 -0.0018031605 -0.0018019057][0.00038303935 0.0014195085 0.0022961157 0.0028161244 0.0029781195 0.0025775679 0.0017802782 0.00078777911 -0.00025199691 -0.0010962491 -0.0015936494 -0.0017749193 -0.0018036152 -0.0018041085 -0.0018018675][0.0025971476 0.0043500285 0.0057505211 0.0066416538 0.0070511526 0.0066112163 0.0053715692 0.0035034618 0.0014547055 -0.00023934478 -0.0012540468 -0.0016827346 -0.0017919815 -0.0018054276 -0.0018026814][0.006487872 0.0092744594 0.011276676 0.01259657 0.013413427 0.01309515 0.01134798 0.0082491329 0.0046355054 0.0015048614 -0.00050751539 -0.0014640365 -0.0017565596 -0.0018059158 -0.0018038974][0.011817069 0.015845049 0.018484211 0.020282671 0.021554422 0.021466382 0.019213229 0.014644417 0.00905737 0.004042895 0.00064252166 -0.0010964873 -0.001689515 -0.0018040135 -0.0018039072][0.017055543 0.022211822 0.025378108 0.027616326 0.029414099 0.029696405 0.027131377 0.021337263 0.013885674 0.0069241435 0.0019704197 -0.00067012664 -0.0016085876 -0.0018012641 -0.0018039611][0.020343786 0.0261783 0.029649831 0.032110013 0.034155741 0.034648761 0.031979453 0.025555154 0.017031683 0.0088840295 0.0029262998 -0.00032856723 -0.0015345095 -0.0017940748 -0.0018005541][0.020837938 0.026825808 0.03036765 0.032741509 0.03455241 0.034791686 0.031972185 0.025548723 0.017079081 0.008983572 0.0030104062 -0.00027341908 -0.0015112589 -0.0017872723 -0.0017960699][0.018852433 0.024491163 0.027780348 0.029605962 0.030563097 0.030036561 0.027035613 0.021186352 0.01384424 0.0070295953 0.0021027429 -0.00055781344 -0.0015578177 -0.0017837327 -0.001792239][0.015570096 0.020504901 0.023204491 0.024081232 0.023763178 0.02222814 0.019140989 0.014357463 0.008861416 0.0040103085 0.00067453121 -0.0010340621 -0.0016470292 -0.0017852728 -0.0017913587][0.011714768 0.015641676 0.017521758 0.017438192 0.016035875 0.013762635 0.010863087 0.007386412 0.0039160275 0.0011128617 -0.00065217179 -0.0014727328 -0.0017350334 -0.0017911411 -0.0017944658][0.0075904629 0.010302998 0.011349117 0.010676805 0.0088788318 0.0065939338 0.0043271063 0.0022012144 0.0004540981 -0.0007642922 -0.0014298044 -0.0017011211 -0.0017768329 -0.0017935961 -0.0017956261][0.0037024515 0.005249212 0.0056849131 0.0049528149 0.0035016285 0.0018711406 0.00049266883 -0.00055129128 -0.0012248615 -0.0015922777 -0.0017465183 -0.0017887271 -0.0017932701 -0.0017945983 -0.0017950515][0.0006641798 0.0013331879 0.0014344031 0.000948141 0.00015199592 -0.00064497232 -0.0012158065 -0.0015660867 -0.0017323029 -0.0017872022 -0.0017970147 -0.001796807 -0.001795876 -0.0017960179 -0.0017954981][-0.0010405614 -0.00083006988 -0.00082487008 -0.0010200999 -0.0012984424 -0.0015462682 -0.0016951896 -0.0017695889 -0.0017959669 -0.0017998714 -0.0017992726 -0.0017982041 -0.0017970758 -0.0017970374 -0.0017966211]]...]
INFO - root - 2017-12-10 01:05:15.749586: step 74710, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.894 sec/batch; 63h:59m:32s remains)
INFO - root - 2017-12-10 01:05:24.144524: step 74720, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.855 sec/batch; 61h:11m:35s remains)
INFO - root - 2017-12-10 01:05:32.790120: step 74730, loss = 0.83, batch loss = 0.70 (9.5 examples/sec; 0.846 sec/batch; 60h:35m:42s remains)
INFO - root - 2017-12-10 01:05:41.318079: step 74740, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.853 sec/batch; 61h:05m:37s remains)
INFO - root - 2017-12-10 01:05:49.917261: step 74750, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.870 sec/batch; 62h:18m:32s remains)
INFO - root - 2017-12-10 01:05:58.574955: step 74760, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.856 sec/batch; 61h:15m:08s remains)
INFO - root - 2017-12-10 01:06:07.154030: step 74770, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.880 sec/batch; 63h:00m:33s remains)
INFO - root - 2017-12-10 01:06:15.754287: step 74780, loss = 0.82, batch loss = 0.69 (9.7 examples/sec; 0.828 sec/batch; 59h:16m:52s remains)
INFO - root - 2017-12-10 01:06:24.415204: step 74790, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.840 sec/batch; 60h:06m:24s remains)
INFO - root - 2017-12-10 01:06:33.102542: step 74800, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.842 sec/batch; 60h:14m:33s remains)
2017-12-10 01:06:33.860499: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.001819298 -0.0018177216 -0.0018155756 -0.0018079784 -0.0017848018 -0.0017375569 -0.001672314 -0.0016214582 -0.0016208552 -0.0016712344 -0.0017384191 -0.0017868641 -0.0018080366 -0.0018133919 -0.0018139626][-0.0018182271 -0.0018163284 -0.0018107134 -0.0017891604 -0.0017326972 -0.001629448 -0.0015039514 -0.0014158398 -0.0014192511 -0.001513695 -0.001641652 -0.0017421388 -0.0017937893 -0.0018108237 -0.0018137076][-0.0018182077 -0.0018150473 -0.0017992847 -0.0017415951 -0.0016056586 -0.0013803835 -0.0011262258 -0.0009595083 -0.00097126566 -0.0011566598 -0.0014147789 -0.0016319416 -0.0017559829 -0.0018033111 -0.0018136174][-0.00181791 -0.0018102131 -0.0017701341 -0.0016423047 -0.0013644024 -0.00093186105 -0.00046167662 -0.00016239029 -0.00019077223 -0.000534081 -0.0010176837 -0.0014373356 -0.0016874926 -0.0017883778 -0.0018128167][-0.0018158939 -0.0017974137 -0.0017156915 -0.0014825156 -0.0010098547 -0.00031387212 0.0004106377 0.00085702597 0.00079952634 0.00025638414 -0.00050986267 -0.0011832782 -0.0015933807 -0.0017646611 -0.0018097012][-0.0018078517 -0.001771717 -0.0016284733 -0.001253808 -0.00054341194 0.00044486637 0.0014257232 0.0019976292 0.0018709038 0.0010891965 1.4984049e-05 -0.00092073967 -0.0014912984 -0.0017340665 -0.0018023112][-0.0017901282 -0.001731699 -0.0015255953 -0.0010201109 -0.0001101807 0.0011017564 0.002257234 0.0028942944 0.0026888042 0.0017102478 0.00039947068 -0.00072590995 -0.0014095688 -0.0017043576 -0.0017919525][-0.001769486 -0.0016856136 -0.0014255352 -0.00083193684 0.00017827388 0.0014692637 0.0026575886 0.0032724082 0.0029969802 0.0019239105 0.00052200106 -0.00066283916 -0.0013759623 -0.0016854282 -0.0017819055][-0.0017555289 -0.00165865 -0.0013878802 -0.00080843095 0.00013303186 0.0013035795 0.002361238 0.0028882851 0.0026036524 0.0015993576 0.0003097622 -0.00076800969 -0.0014104695 -0.001688535 -0.0017774806][-0.0017514471 -0.0016604572 -0.0014214248 -0.00093645882 -0.00017474697 0.00075489923 0.0015891696 0.0019972124 0.0017515357 0.00092599809 -0.0001188803 -0.00098265661 -0.0014919119 -0.0017110108 -0.0017815641][-0.001754495 -0.0016833516 -0.0015082884 -0.0011673041 -0.000638713 7.8609446e-06 0.00059522584 0.000886903 0.00070941669 0.00011491182 -0.000630795 -0.0012391985 -0.001593284 -0.001743392 -0.0017911713][-0.0017754524 -0.0017291371 -0.001625434 -0.0014311289 -0.001126283 -0.00074114907 -0.0003777087 -0.00018853182 -0.00029347301 -0.00065812853 -0.0011127989 -0.0014786996 -0.0016882578 -0.0017750181 -0.0018017532][-0.0018036743 -0.001780112 -0.0017299068 -0.0016393856 -0.0014954364 -0.0013054369 -0.0011187494 -0.0010177093 -0.0010701679 -0.0012548163 -0.0014806286 -0.001658385 -0.0017575576 -0.0017976399 -0.0018094495][-0.0018147703 -0.0018101044 -0.0017994825 -0.0017765786 -0.0017325317 -0.0016662015 -0.0015958601 -0.0015544998 -0.0015702336 -0.0016333308 -0.0017089806 -0.0017665782 -0.0017975012 -0.0018092834 -0.0018122034][-0.001813681 -0.0018121864 -0.0018106933 -0.0018071547 -0.001798061 -0.0017824061 -0.001765385 -0.0017558469 -0.0017599559 -0.0017747085 -0.0017912482 -0.0018032482 -0.0018093408 -0.0018114478 -0.0018118016]]...]
INFO - root - 2017-12-10 01:06:42.537936: step 74810, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.849 sec/batch; 60h:44m:20s remains)
INFO - root - 2017-12-10 01:06:51.143178: step 74820, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.883 sec/batch; 63h:12m:05s remains)
INFO - root - 2017-12-10 01:06:59.877961: step 74830, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.897 sec/batch; 64h:11m:45s remains)
INFO - root - 2017-12-10 01:07:08.674109: step 74840, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.898 sec/batch; 64h:14m:20s remains)
INFO - root - 2017-12-10 01:07:17.454593: step 74850, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.843 sec/batch; 60h:21m:45s remains)
INFO - root - 2017-12-10 01:07:26.181806: step 74860, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.852 sec/batch; 61h:00m:22s remains)
INFO - root - 2017-12-10 01:07:34.711203: step 74870, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.858 sec/batch; 61h:24m:44s remains)
INFO - root - 2017-12-10 01:07:43.361272: step 74880, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.853 sec/batch; 61h:03m:17s remains)
INFO - root - 2017-12-10 01:07:51.952171: step 74890, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.897 sec/batch; 64h:13m:16s remains)
INFO - root - 2017-12-10 01:08:00.490348: step 74900, loss = 0.82, batch loss = 0.69 (10.8 examples/sec; 0.744 sec/batch; 53h:14m:24s remains)
2017-12-10 01:08:01.388826: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0018023081 -0.0017979735 -0.0017956612 -0.001794366 -0.0017937016 -0.0017934941 -0.0017934832 -0.0017934686 -0.0017932839 -0.0017930057 -0.0017927111 -0.0017924655 -0.0017924909 -0.0017928558 -0.0017935985][-0.0018041651 -0.0017989441 -0.0017957179 -0.0017936567 -0.0017925996 -0.0017924077 -0.0017926506 -0.0017929964 -0.0017931593 -0.0017929861 -0.0017925909 -0.0017921242 -0.0017919217 -0.0017920653 -0.0017926934][-0.0018073402 -0.001801408 -0.0017974303 -0.0017948495 -0.0017936138 -0.0017934454 -0.0017939451 -0.0017946251 -0.0017950797 -0.0017950119 -0.0017946106 -0.0017940899 -0.0017937905 -0.0017938442 -0.0017944259][-0.0018106027 -0.0018043786 -0.0018000674 -0.0017973182 -0.0017959257 -0.001795728 -0.0017963983 -0.0017973538 -0.0017980554 -0.0017981294 -0.0017979349 -0.0017976371 -0.0017975696 -0.0017977156 -0.0017982462][-0.0018139371 -0.001807701 -0.0018032684 -0.0018004827 -0.0017990118 -0.0017987419 -0.001799569 -0.0018009588 -0.0018021334 -0.0018027092 -0.0018030397 -0.0018031854 -0.0018033116 -0.0018034469 -0.0018036442][-0.0018167589 -0.0018106343 -0.0018062049 -0.0018035048 -0.0018021645 -0.001802145 -0.0018033939 -0.0018052808 -0.0018070803 -0.0018083632 -0.0018093725 -0.0018099656 -0.0018103108 -0.0018103988 -0.0018100633][-0.0018183746 -0.0018126151 -0.0018085926 -0.0018064859 -0.0018058235 -0.0018065025 -0.0018083217 -0.00181057 -0.0018127771 -0.0018146413 -0.0018161922 -0.001817166 -0.0018176623 -0.0018175869 -0.0018168107][-0.0018188034 -0.0018137384 -0.0018106012 -0.0018095948 -0.0018100566 -0.0018116635 -0.0018140613 -0.0018166804 -0.0018192414 -0.0018214209 -0.0018231105 -0.0018241877 -0.001824679 -0.0018243748 -0.0018230993][-0.0018183975 -0.0018142285 -0.00181201 -0.0018121211 -0.0018136784 -0.0018162108 -0.0018190637 -0.0018218132 -0.0018243176 -0.0018263247 -0.0018280026 -0.0018291509 -0.001829657 -0.0018291228 -0.0018272415][-0.0018171396 -0.0018136811 -0.0018122202 -0.0018129837 -0.0018151765 -0.0018181659 -0.0018212189 -0.0018238238 -0.0018260195 -0.0018278309 -0.0018295 -0.0018306288 -0.0018310411 -0.0018302016 -0.0018277745][-0.001814816 -0.001811913 -0.0018108982 -0.0018118592 -0.0018140677 -0.0018170835 -0.0018200914 -0.0018224102 -0.0018240924 -0.0018255456 -0.0018270384 -0.0018281024 -0.0018283855 -0.0018272685 -0.0018247155][-0.001811669 -0.0018091486 -0.0018085092 -0.0018095943 -0.0018116955 -0.0018144449 -0.0018170959 -0.0018189305 -0.0018199558 -0.0018208706 -0.0018219532 -0.0018227811 -0.001822976 -0.0018218432 -0.0018194899][-0.0018080409 -0.0018058027 -0.0018054385 -0.0018064535 -0.001808231 -0.0018104711 -0.0018126335 -0.001813936 -0.0018144388 -0.0018149378 -0.0018157193 -0.0018165257 -0.0018168846 -0.0018159332 -0.0018138974][-0.0018038539 -0.0018014256 -0.0018009532 -0.0018017502 -0.0018031001 -0.0018048037 -0.0018064084 -0.001807329 -0.0018077034 -0.0018081435 -0.0018089319 -0.0018098877 -0.0018104592 -0.0018098133 -0.0018081138][-0.001799253 -0.0017966592 -0.0017960642 -0.0017965724 -0.0017974392 -0.0017985635 -0.0017996505 -0.0018003338 -0.0018007891 -0.0018015106 -0.0018024831 -0.0018034853 -0.0018039808 -0.0018035468 -0.0018023255]]...]
INFO - root - 2017-12-10 01:08:09.976068: step 74910, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.860 sec/batch; 61h:31m:37s remains)
INFO - root - 2017-12-10 01:08:18.491481: step 74920, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.877 sec/batch; 62h:47m:05s remains)
INFO - root - 2017-12-10 01:08:27.027107: step 74930, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.839 sec/batch; 60h:03m:23s remains)
INFO - root - 2017-12-10 01:08:35.618112: step 74940, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 62h:04m:20s remains)
INFO - root - 2017-12-10 01:08:44.265593: step 74950, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.879 sec/batch; 62h:54m:30s remains)
INFO - root - 2017-12-10 01:08:52.977600: step 74960, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.885 sec/batch; 63h:20m:04s remains)
INFO - root - 2017-12-10 01:09:01.484064: step 74970, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 62h:07m:23s remains)
INFO - root - 2017-12-10 01:09:10.117883: step 74980, loss = 0.81, batch loss = 0.68 (9.4 examples/sec; 0.853 sec/batch; 61h:00m:15s remains)
INFO - root - 2017-12-10 01:09:18.744863: step 74990, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.833 sec/batch; 59h:34m:00s remains)
INFO - root - 2017-12-10 01:09:27.299406: step 75000, loss = 0.82, batch loss = 0.69 (11.4 examples/sec; 0.700 sec/batch; 50h:03m:37s remains)
2017-12-10 01:09:28.260342: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.2913262 0.308478 0.32167205 0.33089852 0.33579198 0.33712721 0.33439338 0.32671025 0.314528 0.29878664 0.28061473 0.26170352 0.2459227 0.23484124 0.22697031][0.31361651 0.33834991 0.3573705 0.37058568 0.37791923 0.38060185 0.37903446 0.3722479 0.35957149 0.34284267 0.32322145 0.30251312 0.28403986 0.27002442 0.26010737][0.32934237 0.3607094 0.38504469 0.40250236 0.4124777 0.41785359 0.41792488 0.412602 0.40066093 0.38317516 0.36269993 0.34016451 0.31957117 0.30186754 0.28856295][0.34148207 0.37858346 0.40763894 0.42929071 0.44230959 0.44951493 0.45108834 0.44729134 0.43658721 0.41935119 0.39900473 0.37527376 0.3530755 0.33226669 0.31482986][0.35074633 0.39282659 0.4256728 0.45070255 0.46555868 0.47438151 0.47680771 0.47364059 0.46366528 0.44731185 0.42765036 0.404356 0.38166103 0.35897258 0.33877081][0.356874 0.40265003 0.43824929 0.46486634 0.48040324 0.48989004 0.49333903 0.49127269 0.48259833 0.46754679 0.44944891 0.42744187 0.40460724 0.38152388 0.35995826][0.3578417 0.4058519 0.44313484 0.47065514 0.48647532 0.49576133 0.49962163 0.4985674 0.49137765 0.47909731 0.4634763 0.44423684 0.42285514 0.400106 0.37846586][0.35322732 0.401702 0.43906924 0.46673241 0.48235959 0.49142489 0.49520895 0.49477634 0.4896796 0.47990134 0.46764615 0.45167664 0.43291768 0.41237685 0.39156196][0.34065577 0.38828486 0.424991 0.45224941 0.46829358 0.4771412 0.48113608 0.48138821 0.47793838 0.47124425 0.46154496 0.44872829 0.43254477 0.41471002 0.39586034][0.32059413 0.36547229 0.40009922 0.42641667 0.4423942 0.45205021 0.45730892 0.45861238 0.45648742 0.45116532 0.44334158 0.43270138 0.41893038 0.40385202 0.38790622][0.29380882 0.33517063 0.36709127 0.39142388 0.40712434 0.41695544 0.42250344 0.42515248 0.4246625 0.42100668 0.41461247 0.4055621 0.39363116 0.38067827 0.3670471][0.26084206 0.2978591 0.32695696 0.34980026 0.36514649 0.37483039 0.38009962 0.38315386 0.3830336 0.38003531 0.37486246 0.3671076 0.35727197 0.34590262 0.33439511][0.22624516 0.25797179 0.28329402 0.30380276 0.318176 0.32741871 0.33240145 0.33504897 0.33451778 0.33161193 0.32682016 0.32000473 0.31161952 0.30219626 0.29268903][0.19107926 0.21693441 0.23743972 0.25464815 0.26718527 0.27566487 0.28025258 0.28246656 0.28170142 0.27874726 0.27421737 0.26795349 0.26093864 0.25326228 0.24578139][0.15903671 0.17901853 0.19454987 0.20769845 0.21734166 0.22386618 0.22734126 0.22900331 0.22813234 0.22546008 0.22178656 0.21682525 0.21126334 0.20516378 0.19942503]]...]
INFO - root - 2017-12-10 01:09:36.929445: step 75010, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.903 sec/batch; 64h:35m:32s remains)
INFO - root - 2017-12-10 01:09:45.535120: step 75020, loss = 0.83, batch loss = 0.70 (9.2 examples/sec; 0.865 sec/batch; 61h:53m:50s remains)
INFO - root - 2017-12-10 01:09:54.061346: step 75030, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.888 sec/batch; 63h:29m:00s remains)
INFO - root - 2017-12-10 01:10:02.555903: step 75040, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.852 sec/batch; 60h:55m:46s remains)
INFO - root - 2017-12-10 01:10:11.204541: step 75050, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.887 sec/batch; 63h:27m:18s remains)
INFO - root - 2017-12-10 01:10:19.794082: step 75060, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.888 sec/batch; 63h:29m:42s remains)
INFO - root - 2017-12-10 01:10:28.441857: step 75070, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.877 sec/batch; 62h:44m:22s remains)
INFO - root - 2017-12-10 01:10:37.131093: step 75080, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.888 sec/batch; 63h:27m:43s remains)
INFO - root - 2017-12-10 01:10:45.756531: step 75090, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.891 sec/batch; 63h:41m:38s remains)
INFO - root - 2017-12-10 01:10:54.345626: step 75100, loss = 0.82, batch loss = 0.69 (10.3 examples/sec; 0.773 sec/batch; 55h:18m:16s remains)
2017-12-10 01:10:55.199255: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.48766902 0.46645841 0.44513682 0.42632169 0.41072527 0.39870545 0.3878549 0.3726849 0.35107011 0.31745645 0.26983583 0.21316096 0.15505823 0.10404196 0.061875187][0.48348075 0.46332389 0.44299385 0.42516646 0.41109335 0.40179157 0.39308646 0.3802188 0.35947579 0.32624453 0.27782598 0.21881816 0.15849635 0.10543342 0.062686972][0.4718096 0.45532614 0.43864134 0.42446882 0.41355926 0.40649331 0.39894006 0.3866204 0.36470449 0.32998848 0.27996352 0.21953353 0.15836242 0.10419297 0.061306275][0.46120182 0.45152023 0.44154134 0.43253568 0.42589486 0.42174998 0.41442159 0.39992934 0.374434 0.3363868 0.28302363 0.22010542 0.15785147 0.10308156 0.060008761][0.45100895 0.44938871 0.44691604 0.44610721 0.44631046 0.44553041 0.43911028 0.42235953 0.39216712 0.34730068 0.28768271 0.22086047 0.15598823 0.10026021 0.057527479][0.44668576 0.45161697 0.45479026 0.46026066 0.466365 0.46872225 0.46253964 0.44424269 0.41109785 0.36168739 0.2963452 0.22538862 0.15776792 0.10092635 0.0576317][0.44570768 0.45717496 0.46617368 0.47564647 0.48427662 0.4883489 0.48163551 0.46242654 0.42690745 0.37511274 0.30822051 0.23643802 0.16804922 0.11014603 0.066509664][0.44510305 0.46093524 0.47318381 0.48485124 0.49482554 0.49841595 0.49081317 0.470463 0.43413737 0.38264754 0.31734711 0.24811921 0.18244936 0.12668186 0.084176637][0.44344863 0.46244523 0.47616521 0.48795956 0.49750671 0.50060076 0.49284774 0.47301212 0.43803927 0.38914341 0.32777303 0.26313993 0.20164244 0.14933342 0.10953768][0.4383224 0.45768154 0.4704763 0.48132724 0.49006376 0.49199343 0.48475608 0.46716788 0.43620658 0.392511 0.33778238 0.28036448 0.22550681 0.17902358 0.14335448][0.42663011 0.44617498 0.45792726 0.46600196 0.47181273 0.4720695 0.46495339 0.4498519 0.42387643 0.38857871 0.34427088 0.29728761 0.2518428 0.21301155 0.18299273][0.4081752 0.42671043 0.43677604 0.44270897 0.44651541 0.44618717 0.4404048 0.42785072 0.40722641 0.37998068 0.34621736 0.31007704 0.27496824 0.24517028 0.22192608][0.38647333 0.40284145 0.41045964 0.41419369 0.41760075 0.41689518 0.41238883 0.40278283 0.38793838 0.36803523 0.342754 0.31636673 0.29026955 0.26882219 0.25198248][0.36079097 0.37603739 0.38254377 0.3857787 0.38957319 0.38990304 0.38698339 0.38066441 0.37090719 0.35699922 0.33858854 0.31958756 0.30091152 0.28569102 0.27379167][0.3332988 0.3477366 0.35427904 0.35749844 0.3617278 0.36277968 0.36176223 0.3578639 0.3512857 0.34342372 0.33227834 0.32027942 0.30795655 0.2978943 0.2897653]]...]
INFO - root - 2017-12-10 01:11:03.877043: step 75110, loss = 0.83, batch loss = 0.70 (8.9 examples/sec; 0.901 sec/batch; 64h:27m:09s remains)
INFO - root - 2017-12-10 01:11:12.386587: step 75120, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.855 sec/batch; 61h:08m:19s remains)
INFO - root - 2017-12-10 01:11:20.963408: step 75130, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.873 sec/batch; 62h:24m:25s remains)
INFO - root - 2017-12-10 01:11:29.538551: step 75140, loss = 0.82, batch loss = 0.69 (9.7 examples/sec; 0.828 sec/batch; 59h:13m:09s remains)
INFO - root - 2017-12-10 01:11:38.156102: step 75150, loss = 0.81, batch loss = 0.68 (9.2 examples/sec; 0.868 sec/batch; 62h:01m:57s remains)
INFO - root - 2017-12-10 01:11:46.739174: step 75160, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 62h:03m:43s remains)
INFO - root - 2017-12-10 01:11:55.345280: step 75170, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.881 sec/batch; 62h:59m:20s remains)
INFO - root - 2017-12-10 01:12:04.003048: step 75180, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.878 sec/batch; 62h:44m:34s remains)
INFO - root - 2017-12-10 01:12:12.859870: step 75190, loss = 0.82, batch loss = 0.69 (8.8 examples/sec; 0.909 sec/batch; 64h:58m:17s remains)
INFO - root - 2017-12-10 01:12:21.432060: step 75200, loss = 0.82, batch loss = 0.69 (10.7 examples/sec; 0.746 sec/batch; 53h:17m:53s remains)
2017-12-10 01:12:22.404617: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0018084307 -0.0018011415 -0.0017967743 -0.0017989958 -0.0018033646 -0.0017465133 -0.0013616749 -0.0002960508 0.00168722 0.0038569639 0.0052700341 0.0053822943 0.0043330933 0.0026630834 0.00088291417][-0.0018134867 -0.0018129259 -0.001812696 -0.0018129599 -0.001801496 -0.0016076538 -0.00069553987 0.0015534464 0.0053057848 0.009255223 0.011850529 0.012154842 0.010385797 0.0073742927 0.0040285131][-0.001813123 -0.0018138917 -0.0018150551 -0.0018135388 -0.0017684954 -0.0013410283 0.00036937615 0.0042947764 0.010452702 0.016826233 0.021096982 0.02181909 0.019181537 0.014380439 0.0088485954][-0.0018120199 -0.0018136376 -0.0018155153 -0.0018119243 -0.0017283008 -0.0010652626 0.0014481036 0.0070344764 0.015684972 0.024758581 0.031220399 0.032922018 0.029793791 0.023163334 0.015069165][-0.0017821799 -0.0017776678 -0.0017842015 -0.0017890051 -0.0016811007 -0.00085403991 0.0023164996 0.0094279 0.020507757 0.032389641 0.04121979 0.04407654 0.040545985 0.032171391 0.021557799][-0.0017782991 -0.0017567117 -0.0017290561 -0.0017060965 -0.001576556 -0.0006839917 0.0028130063 0.010782477 0.023467394 0.037490055 0.048383936 0.052587882 0.049198624 0.039662492 0.027077861][-0.0017746742 -0.0017490623 -0.0017154673 -0.0016712626 -0.0014948585 -0.00058196275 0.0028705942 0.010767243 0.023571698 0.038195893 0.050121393 0.055419598 0.052626602 0.04295728 0.029704578][-0.0017715729 -0.0017426842 -0.0017038067 -0.0016530436 -0.0014804171 -0.0007053545 0.0022185002 0.0089882705 0.020245489 0.03364154 0.045161456 0.050956812 0.049096741 0.040465556 0.028140726][-0.0017650104 -0.0017352122 -0.0016946062 -0.001643275 -0.0015089272 -0.0010150721 0.00098711054 0.0058911568 0.014515336 0.025413353 0.035375111 0.040967237 0.04012024 0.033315174 0.0231533][-0.0017665451 -0.0017356256 -0.001692683 -0.0016411631 -0.0015469787 -0.0013094442 -0.00020189211 0.0028455714 0.00867017 0.016542738 0.024116172 0.02873701 0.028495837 0.023681795 0.016253063][-0.0018057142 -0.001783876 -0.0017389336 -0.0016793916 -0.0016051696 -0.0015007854 -0.0010169323 0.00053932739 0.0038446481 0.0087294225 0.013718781 0.016964214 0.016991122 0.013976133 0.0092542693][-0.0018038069 -0.0018044574 -0.0018063993 -0.0017848783 -0.0017302645 -0.0016439162 -0.0014371405 -0.00077530788 0.00071261905 0.0031392742 0.0057973024 0.0076375972 0.007695396 0.0060988697 0.0036237193][-0.0018021003 -0.0018029108 -0.0018047828 -0.0018062885 -0.0018050253 -0.0017810562 -0.0017264845 -0.0015396699 -0.0010837144 -0.00022176281 0.00080865214 0.0015851787 0.0016388464 0.0010277942 8.66208e-05][-0.0018004928 -0.0018016858 -0.0018038151 -0.0018056389 -0.0018045795 -0.0017988583 -0.0017893051 -0.0017495974 -0.0016085764 -0.0013760095 -0.0011492987 -0.00099990889 -0.0010180619 -0.0011840654 -0.0014138225][-0.0018044363 -0.0017981695 -0.0018000921 -0.0018028537 -0.0018030206 -0.0017801003 -0.001722788 -0.0015857005 -0.0013574788 -0.0012626443 -0.0013137897 -0.0014839894 -0.001638006 -0.0017384731 -0.0017906264]]...]
INFO - root - 2017-12-10 01:12:30.971841: step 75210, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.881 sec/batch; 62h:57m:02s remains)
INFO - root - 2017-12-10 01:12:39.482377: step 75220, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.859 sec/batch; 61h:24m:23s remains)
INFO - root - 2017-12-10 01:12:48.220738: step 75230, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.866 sec/batch; 61h:53m:29s remains)
INFO - root - 2017-12-10 01:12:56.903061: step 75240, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.880 sec/batch; 62h:52m:25s remains)
INFO - root - 2017-12-10 01:13:05.506105: step 75250, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.897 sec/batch; 64h:05m:42s remains)
INFO - root - 2017-12-10 01:13:14.050853: step 75260, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 61h:58m:56s remains)
INFO - root - 2017-12-10 01:13:22.535052: step 75270, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.865 sec/batch; 61h:46m:41s remains)
INFO - root - 2017-12-10 01:13:31.129939: step 75280, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.889 sec/batch; 63h:31m:55s remains)
INFO - root - 2017-12-10 01:13:39.751646: step 75290, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.857 sec/batch; 61h:11m:54s remains)
INFO - root - 2017-12-10 01:13:48.305634: step 75300, loss = 0.82, batch loss = 0.69 (10.8 examples/sec; 0.740 sec/batch; 52h:53m:36s remains)
2017-12-10 01:13:49.231225: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.34533048 0.34122345 0.33707827 0.33378837 0.32945988 0.32548428 0.32168868 0.31691664 0.31022567 0.30372524 0.29837891 0.29474512 0.29174805 0.29188406 0.29453814][0.35894293 0.35754806 0.3549957 0.35258242 0.3491624 0.3464438 0.343433 0.33954519 0.33326486 0.32663363 0.32037568 0.31493351 0.31003845 0.30817989 0.30898115][0.37333503 0.37398997 0.37318236 0.37216604 0.37029305 0.36874369 0.36622325 0.36300191 0.35679963 0.3494378 0.3414495 0.33346587 0.32594791 0.32086134 0.31855181][0.39087611 0.39420527 0.39527255 0.3954097 0.39478189 0.39335725 0.39057603 0.38685325 0.3797029 0.37157309 0.36185473 0.35148892 0.34131673 0.33243155 0.32648665][0.41362467 0.41843402 0.41999117 0.42099887 0.4213188 0.41946381 0.41553876 0.41086063 0.40271944 0.39252648 0.38028744 0.36765853 0.35474902 0.34175575 0.33174][0.43736 0.44303849 0.4436489 0.44398043 0.44364527 0.44144517 0.4367955 0.43078616 0.42192245 0.41014346 0.39635423 0.38088197 0.36500889 0.34864929 0.33468428][0.45467 0.46141627 0.46140224 0.46145388 0.46011728 0.45722106 0.452054 0.44468331 0.43480232 0.421879 0.40727136 0.39000583 0.37225828 0.35356206 0.33681208][0.46248621 0.46987161 0.46809772 0.46624759 0.46381685 0.46050149 0.45493674 0.44657373 0.4359031 0.42251736 0.40757313 0.39013377 0.37225443 0.3531765 0.33554485][0.45886719 0.46688375 0.46358389 0.46023044 0.45657384 0.45249847 0.4469443 0.43826327 0.42785165 0.41480184 0.40035018 0.38389033 0.36649582 0.34823722 0.33094659][0.44582373 0.4533895 0.44788176 0.44306007 0.43823126 0.43349448 0.4279559 0.41984695 0.41054535 0.39844546 0.38547316 0.37113631 0.35528156 0.33912724 0.32371646][0.42136008 0.42823941 0.42118934 0.41463304 0.40817493 0.40325856 0.39834809 0.39186409 0.38436717 0.37427172 0.36365607 0.3512722 0.3378734 0.32499295 0.31268042][0.38969722 0.39531106 0.38680223 0.37888235 0.37155747 0.36585951 0.36080012 0.3558856 0.35075152 0.34398305 0.33646837 0.3279084 0.31846592 0.30936217 0.30054659][0.3550202 0.35901928 0.34974781 0.34071031 0.33275002 0.32635269 0.32140085 0.3177233 0.31449097 0.31042105 0.30568308 0.30071205 0.2952131 0.29020214 0.28535637][0.32191446 0.32419857 0.3142947 0.30547175 0.29745626 0.29100233 0.28660178 0.28405234 0.28263935 0.28094986 0.27910694 0.27728775 0.27469727 0.27276534 0.27128947][0.29207662 0.29335776 0.28351 0.27511913 0.26752913 0.26150569 0.2573525 0.25565654 0.25549117 0.25562173 0.25588307 0.2564216 0.25688052 0.25763527 0.25886756]]...]
INFO - root - 2017-12-10 01:13:57.893156: step 75310, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.874 sec/batch; 62h:28m:28s remains)
INFO - root - 2017-12-10 01:14:06.483849: step 75320, loss = 0.83, batch loss = 0.70 (9.4 examples/sec; 0.849 sec/batch; 60h:40m:36s remains)
INFO - root - 2017-12-10 01:14:15.190862: step 75330, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.861 sec/batch; 61h:31m:18s remains)
INFO - root - 2017-12-10 01:14:23.838944: step 75340, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.903 sec/batch; 64h:30m:56s remains)
INFO - root - 2017-12-10 01:14:32.506450: step 75350, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.903 sec/batch; 64h:28m:45s remains)
INFO - root - 2017-12-10 01:14:41.160941: step 75360, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 61h:42m:21s remains)
INFO - root - 2017-12-10 01:14:49.779670: step 75370, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.882 sec/batch; 62h:59m:44s remains)
INFO - root - 2017-12-10 01:14:58.309855: step 75380, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.856 sec/batch; 61h:07m:09s remains)
INFO - root - 2017-12-10 01:15:06.983923: step 75390, loss = 0.82, batch loss = 0.69 (9.7 examples/sec; 0.824 sec/batch; 58h:48m:57s remains)
INFO - root - 2017-12-10 01:15:15.491795: step 75400, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.850 sec/batch; 60h:43m:42s remains)
2017-12-10 01:15:16.405492: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0017942963 -0.001791826 -0.0017915014 -0.001791623 -0.0017917948 -0.001791866 -0.001791842 -0.001791737 -0.0017915332 -0.001791212 -0.0017908638 -0.0017905997 -0.0017904598 -0.0017904181 -0.0017904021][-0.0017933269 -0.0017908638 -0.0017906742 -0.0017909809 -0.0017913808 -0.0017916402 -0.001791681 -0.0017914725 -0.0017910827 -0.0017905612 -0.0017900205 -0.0017896506 -0.0017895062 -0.0017894754 -0.0017894575][-0.0017934091 -0.0017910149 -0.0017909283 -0.0017914322 -0.0017920423 -0.0017924241 -0.0017924145 -0.001791955 -0.0017912118 -0.0017903745 -0.0017896855 -0.0017893565 -0.0017893707 -0.0017894754 -0.0017894958][-0.0017935769 -0.0017912363 -0.0017912345 -0.0017919469 -0.0017928128 -0.0017932878 -0.0017931506 -0.0017923495 -0.0017911871 -0.0017899993 -0.0017891794 -0.001788951 -0.0017892073 -0.0017895364 -0.0017896593][-0.0017937956 -0.0017914721 -0.0017915911 -0.001792504 -0.0017935035 -0.0017939598 -0.0017936688 -0.0017925926 -0.0017911224 -0.0017897099 -0.0017888099 -0.0017886104 -0.0017890247 -0.0017895427 -0.0017897965][-0.0017936068 -0.0017912963 -0.00179155 -0.0017926522 -0.0017937639 -0.0017941634 -0.001793777 -0.0017925638 -0.0017909681 -0.0017894682 -0.0017885134 -0.0017882823 -0.0017887144 -0.0017893591 -0.0017897659][-0.0017929625 -0.0017907078 -0.0017910304 -0.0017922203 -0.0017934151 -0.001793785 -0.0017932928 -0.0017920003 -0.0017904142 -0.001788877 -0.0017879597 -0.0017877425 -0.0017882386 -0.0017889941 -0.0017895737][-0.0017922765 -0.0017900269 -0.0017903324 -0.0017914488 -0.0017926084 -0.0017929516 -0.0017924915 -0.0017912397 -0.0017897256 -0.0017882239 -0.0017873851 -0.0017872364 -0.001787804 -0.0017886402 -0.0017893704][-0.0017917552 -0.0017895289 -0.0017898267 -0.0017907446 -0.0017917266 -0.0017920549 -0.0017917409 -0.0017906277 -0.0017892426 -0.0017878108 -0.0017870744 -0.0017870155 -0.0017875906 -0.0017884304 -0.0017892108][-0.0017912149 -0.0017891095 -0.0017893316 -0.0017899985 -0.0017907413 -0.0017910575 -0.0017908949 -0.0017900342 -0.0017888711 -0.0017876734 -0.0017871348 -0.0017871675 -0.0017876831 -0.0017884381 -0.0017891547][-0.001790934 -0.0017888326 -0.0017888169 -0.0017892607 -0.0017897844 -0.0017900497 -0.0017900066 -0.0017893882 -0.0017884924 -0.0017876253 -0.0017873221 -0.0017874829 -0.00178796 -0.0017885803 -0.0017891598][-0.0017910836 -0.0017888793 -0.0017886671 -0.0017889114 -0.0017892156 -0.0017894227 -0.0017894099 -0.0017889368 -0.0017882928 -0.0017877539 -0.0017876488 -0.001787862 -0.0017882844 -0.0017887583 -0.0017891669][-0.0017914929 -0.0017892807 -0.0017889219 -0.0017890473 -0.0017892108 -0.0017893087 -0.0017892864 -0.0017889813 -0.0017885754 -0.0017882773 -0.0017882396 -0.0017883951 -0.0017886464 -0.0017889418 -0.0017891779][-0.0017919204 -0.0017896919 -0.0017892818 -0.0017893208 -0.0017893807 -0.001789414 -0.0017893902 -0.0017892283 -0.0017890348 -0.0017889303 -0.0017889214 -0.001788981 -0.0017890661 -0.001789158 -0.0017892189][-0.0017921388 -0.0017899332 -0.0017894791 -0.0017894926 -0.0017895038 -0.0017895186 -0.0017895148 -0.0017894588 -0.0017893843 -0.0017893626 -0.0017893416 -0.0017893234 -0.0017892894 -0.0017892469 -0.0017892101]]...]
INFO - root - 2017-12-10 01:15:25.009342: step 75410, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.852 sec/batch; 60h:52m:12s remains)
INFO - root - 2017-12-10 01:15:33.524494: step 75420, loss = 0.82, batch loss = 0.69 (9.7 examples/sec; 0.824 sec/batch; 58h:50m:26s remains)
INFO - root - 2017-12-10 01:15:42.194075: step 75430, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 61h:53m:26s remains)
INFO - root - 2017-12-10 01:15:50.960665: step 75440, loss = 0.83, batch loss = 0.70 (9.0 examples/sec; 0.886 sec/batch; 63h:16m:04s remains)
INFO - root - 2017-12-10 01:15:59.740154: step 75450, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.848 sec/batch; 60h:31m:20s remains)
INFO - root - 2017-12-10 01:16:08.518690: step 75460, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.899 sec/batch; 64h:10m:20s remains)
INFO - root - 2017-12-10 01:16:17.114136: step 75470, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.878 sec/batch; 62h:39m:18s remains)
INFO - root - 2017-12-10 01:16:25.649044: step 75480, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.854 sec/batch; 60h:57m:42s remains)
INFO - root - 2017-12-10 01:16:34.273438: step 75490, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.865 sec/batch; 61h:47m:04s remains)
INFO - root - 2017-12-10 01:16:42.685824: step 75500, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.844 sec/batch; 60h:17m:14s remains)
2017-12-10 01:16:43.583056: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.10462296 0.10236568 0.099952608 0.097504236 0.094634384 0.091257043 0.086722918 0.080566622 0.072557777 0.063122407 0.052917752 0.042397153 0.032143235 0.023042858 0.015611728][0.11795417 0.11601493 0.11389614 0.11170056 0.10898829 0.10591949 0.1015486 0.095340841 0.086832464 0.076743335 0.065691017 0.053931322 0.041915547 0.030673943 0.021248991][0.12767366 0.12632613 0.12473104 0.1227067 0.12002766 0.11702645 0.11245734 0.10587776 0.096740879 0.086219765 0.074807055 0.0625757 0.049573425 0.036822595 0.025747996][0.13389848 0.13365811 0.13285762 0.13103725 0.12828319 0.12511368 0.12015135 0.1128286 0.10264184 0.0914717 0.079691879 0.067147128 0.053430047 0.039714 0.027694009][0.13554259 0.13628627 0.135938 0.13430053 0.13150734 0.12804478 0.12270342 0.11467985 0.10361547 0.091880091 0.079919808 0.067410424 0.053404491 0.039237142 0.026863117][0.13233207 0.13389818 0.13375819 0.13206114 0.12901506 0.12529929 0.11959292 0.11093713 0.099191882 0.087069646 0.075112365 0.062797852 0.04893031 0.034980338 0.023013717][0.1251985 0.12729165 0.12703545 0.12521952 0.12193305 0.11779513 0.11156411 0.10244364 0.090402745 0.078276291 0.066638872 0.054862726 0.041700024 0.028596409 0.017671078][0.11611576 0.11827329 0.11755229 0.11532495 0.11165604 0.10713983 0.10052916 0.091301121 0.07945545 0.067699961 0.05666798 0.0457791 0.033893213 0.022341583 0.013099832][0.10612149 0.10829733 0.10704388 0.10439127 0.10034436 0.09545356 0.088597335 0.079507276 0.068189837 0.057190418 0.047051407 0.037325326 0.027149817 0.017591938 0.010268822][0.096149355 0.097715877 0.09580382 0.092748374 0.088438295 0.083233669 0.076345384 0.067768745 0.057439465 0.047551967 0.038647063 0.030600045 0.022658825 0.015438731 0.010098816][0.086311325 0.08704748 0.084334664 0.080686845 0.075907566 0.070396714 0.063577779 0.055743963 0.046803053 0.038466569 0.031200912 0.025112947 0.019620217 0.014905338 0.011446408][0.07697998 0.077116609 0.073809132 0.069553114 0.064192966 0.058159322 0.05115452 0.043939136 0.036374643 0.029687405 0.024264287 0.020343598 0.01733386 0.014951413 0.013164044][0.068517767 0.068314627 0.064865358 0.060093913 0.054045971 0.047350015 0.040155765 0.033402551 0.027017701 0.021892816 0.018184071 0.01613473 0.015083638 0.014485913 0.013944718][0.062227227 0.062304385 0.058816805 0.053617116 0.046886258 0.039468754 0.031944003 0.025345674 0.019806845 0.015829057 0.013293423 0.012378421 0.012385097 0.012714338 0.012753285][0.057799768 0.058369387 0.054973125 0.049356267 0.041946754 0.033889148 0.02598526 0.019333292 0.014271581 0.011086508 0.0093740588 0.0090237819 0.0094437627 0.010135382 0.010424875]]...]
INFO - root - 2017-12-10 01:16:52.134132: step 75510, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.861 sec/batch; 61h:27m:20s remains)
INFO - root - 2017-12-10 01:17:00.656616: step 75520, loss = 0.82, batch loss = 0.69 (10.6 examples/sec; 0.754 sec/batch; 53h:50m:01s remains)
INFO - root - 2017-12-10 01:17:09.249800: step 75530, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.902 sec/batch; 64h:23m:03s remains)
INFO - root - 2017-12-10 01:17:17.906980: step 75540, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.877 sec/batch; 62h:36m:12s remains)
INFO - root - 2017-12-10 01:17:26.540280: step 75550, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.830 sec/batch; 59h:15m:44s remains)
INFO - root - 2017-12-10 01:17:35.119892: step 75560, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.860 sec/batch; 61h:21m:52s remains)
INFO - root - 2017-12-10 01:17:43.665132: step 75570, loss = 0.81, batch loss = 0.68 (9.0 examples/sec; 0.890 sec/batch; 63h:29m:46s remains)
INFO - root - 2017-12-10 01:17:52.393421: step 75580, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.848 sec/batch; 60h:29m:56s remains)
INFO - root - 2017-12-10 01:18:00.935512: step 75590, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.891 sec/batch; 63h:33m:35s remains)
INFO - root - 2017-12-10 01:18:09.524774: step 75600, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.887 sec/batch; 63h:15m:59s remains)
2017-12-10 01:18:10.423075: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0018355926 -0.0018358383 -0.0018364261 -0.0018370857 -0.0018375099 -0.0018376225 -0.0018374786 -0.0018373204 -0.0018372297 -0.0018371745 -0.0018369723 -0.0018366162 -0.0018361127 -0.001835592 -0.0018351707][-0.0018359766 -0.0018364521 -0.0018372333 -0.0018377841 -0.0018378489 -0.0018372843 -0.0018363135 -0.0018357148 -0.0018357608 -0.0018363383 -0.0018368847 -0.0018371562 -0.0018370152 -0.0018366236 -0.0018361917][-0.0018367907 -0.0018373423 -0.0018379804 -0.0018379909 -0.0018366415 -0.0018336412 -0.0018297917 -0.0018272429 -0.0018274166 -0.0018299927 -0.0018333615 -0.0018359555 -0.0018372283 -0.0018374462 -0.0018369614][-0.0018377565 -0.0018378745 -0.0018373312 -0.0018349229 -0.0018288828 -0.0018189691 -0.0018078035 -0.0018007334 -0.0018020276 -0.0018108876 -0.0018221588 -0.0018310479 -0.0018359998 -0.0018379093 -0.0018379715][-0.0018383026 -0.0018366582 -0.0018317191 -0.0018206449 -0.0018008091 -0.0017729986 -0.001744037 -0.0017265759 -0.0017304904 -0.0017545858 -0.0017861142 -0.0018124657 -0.0018284573 -0.0018357119 -0.0018378632][-0.0018372482 -0.0018310078 -0.0018146102 -0.0017824752 -0.0017324882 -0.0016701575 -0.0016109421 -0.0015781206 -0.001588603 -0.0016402442 -0.0017093336 -0.0017698789 -0.0018088629 -0.0018281788 -0.0018353616][-0.0018338949 -0.0018175612 -0.0017769779 -0.0017019687 -0.0015959203 -0.0014772937 -0.0013761495 -0.0013264444 -0.0013509287 -0.0014444913 -0.0015711532 -0.0016874678 -0.0017676991 -0.0018110931 -0.0018294054][-0.0018297243 -0.0018003419 -0.0017265569 -0.0015915963 -0.0014069772 -0.0012111685 -0.0010544203 -0.00098138675 -0.0010199202 -0.0011634115 -0.0013647636 -0.0015589779 -0.0017006865 -0.001781764 -0.0018185819][-0.0018271402 -0.0017902016 -0.0016950596 -0.0015174039 -0.0012708632 -0.0010099001 -0.00080331042 -0.00070638035 -0.00074947905 -0.00092643982 -0.001184115 -0.0014419669 -0.001636538 -0.0017512325 -0.0018055178][-0.00182697 -0.0017930857 -0.0017033005 -0.0015292978 -0.001277104 -0.0010006542 -0.00077539007 -0.00066370773 -0.00069594255 -0.00086779485 -0.0011289001 -0.0013992912 -0.0016089213 -0.0017358433 -0.0017976022][-0.0018278882 -0.0018040792 -0.001741828 -0.001617057 -0.0014258181 -0.0012043379 -0.0010140659 -0.00091043458 -0.00092091452 -0.0010455274 -0.001247732 -0.0014656187 -0.0016393079 -0.0017471761 -0.0018007081][-0.001829752 -0.0018156721 -0.0017811578 -0.0017117197 -0.0016020797 -0.0014705606 -0.0013506692 -0.0012757406 -0.0012634214 -0.0013215587 -0.0014367122 -0.001574568 -0.00169234 -0.0017693979 -0.0018087114][-0.0018328362 -0.0018264859 -0.0018110827 -0.0017797011 -0.0017292027 -0.001666915 -0.0016048348 -0.0015556933 -0.0015298979 -0.0015399088 -0.0015898418 -0.0016656129 -0.0017386838 -0.0017899864 -0.0018167775][-0.0018334052 -0.0018303511 -0.0018245962 -0.0018139401 -0.0017973931 -0.0017766389 -0.0017523239 -0.0017255207 -0.0017006756 -0.0016896303 -0.0017028591 -0.0017375029 -0.0017769764 -0.0018067874 -0.0018229092][-0.0018335413 -0.0018321311 -0.0018301314 -0.0018268191 -0.001821164 -0.0018132203 -0.0018021944 -0.0017875432 -0.0017715571 -0.0017616041 -0.001765089 -0.0017819265 -0.0018031475 -0.001819482 -0.0018281272]]...]
INFO - root - 2017-12-10 01:18:19.047111: step 75610, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.879 sec/batch; 62h:43m:11s remains)
INFO - root - 2017-12-10 01:18:27.612355: step 75620, loss = 0.83, batch loss = 0.70 (10.7 examples/sec; 0.750 sec/batch; 53h:32m:31s remains)
INFO - root - 2017-12-10 01:18:36.336830: step 75630, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.888 sec/batch; 63h:19m:34s remains)
INFO - root - 2017-12-10 01:18:45.042351: step 75640, loss = 0.83, batch loss = 0.70 (9.3 examples/sec; 0.859 sec/batch; 61h:18m:19s remains)
INFO - root - 2017-12-10 01:18:53.803355: step 75650, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.874 sec/batch; 62h:20m:39s remains)
INFO - root - 2017-12-10 01:19:02.504579: step 75660, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.886 sec/batch; 63h:14m:18s remains)
INFO - root - 2017-12-10 01:19:10.993534: step 75670, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.866 sec/batch; 61h:45m:45s remains)
INFO - root - 2017-12-10 01:19:19.783213: step 75680, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.888 sec/batch; 63h:20m:58s remains)
INFO - root - 2017-12-10 01:19:28.422776: step 75690, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.880 sec/batch; 62h:48m:17s remains)
INFO - root - 2017-12-10 01:19:36.951944: step 75700, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 62h:07m:02s remains)
2017-12-10 01:19:37.937097: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.037473816 0.047415558 0.058230102 0.072152793 0.0877022 0.10430653 0.12117626 0.13666023 0.15153712 0.16464174 0.17503236 0.18429936 0.1908177 0.19618703 0.19707033][0.039145939 0.049597055 0.061963134 0.077531256 0.094720431 0.11238686 0.12968016 0.14501421 0.15911154 0.17128035 0.18099795 0.1895391 0.19569579 0.20069344 0.2018684][0.043557964 0.05510249 0.069077015 0.086062476 0.10441259 0.12245537 0.1388261 0.15331496 0.16577145 0.17616694 0.18407777 0.19130787 0.19681935 0.20133525 0.20280236][0.049885035 0.063420244 0.078918636 0.096944131 0.11585268 0.1335085 0.14852659 0.16133127 0.17199394 0.18130296 0.18809904 0.19436178 0.1991733 0.20294292 0.20409514][0.057277419 0.073333822 0.090777136 0.10937812 0.1270605 0.14381297 0.15743461 0.16877203 0.17807381 0.18632771 0.19168179 0.19691786 0.20142807 0.20471984 0.20588398][0.064310893 0.082642995 0.10166419 0.12148054 0.13861342 0.15351403 0.16537113 0.17615403 0.18469189 0.19189644 0.19649847 0.2001013 0.20278901 0.20523922 0.20599285][0.067118324 0.087191507 0.10810813 0.12964857 0.14734063 0.1621742 0.17265172 0.1817469 0.18845955 0.19461168 0.19834936 0.20177373 0.20453773 0.20586078 0.20551808][0.064096391 0.084490195 0.10615722 0.12896354 0.14805746 0.16366212 0.17439735 0.18330699 0.18904759 0.19434044 0.1972926 0.20035402 0.2024955 0.20370112 0.20317748][0.057752457 0.076340064 0.096868649 0.11914904 0.13864329 0.15583731 0.16881356 0.17930411 0.18637151 0.19245361 0.19547066 0.19825792 0.19954048 0.19992806 0.19840471][0.048671026 0.064238511 0.082412466 0.10319398 0.1222923 0.14025736 0.15573031 0.16954158 0.18013155 0.18822908 0.19272216 0.19592583 0.19632231 0.19545461 0.19234712][0.038066372 0.049522582 0.063851736 0.082220308 0.10131393 0.12069771 0.13897218 0.15690422 0.17179365 0.18353856 0.19078456 0.1954598 0.1957023 0.19357373 0.18836574][0.028756548 0.035786889 0.046220623 0.060867943 0.078436695 0.098695621 0.12007969 0.14242275 0.16190736 0.17842747 0.18942808 0.19620183 0.19740462 0.19450195 0.18793559][0.022023432 0.026013078 0.032112576 0.042535845 0.057789277 0.077771254 0.10117676 0.12740444 0.15176757 0.17306654 0.1877443 0.19688796 0.19835716 0.19515663 0.1876556][0.017796533 0.020238951 0.02371507 0.030448303 0.042295843 0.061055444 0.0852897 0.1140657 0.14203392 0.16732809 0.18543541 0.19710268 0.20006594 0.1972986 0.18943195][0.013979382 0.016176607 0.018622098 0.023479361 0.032970216 0.049884748 0.073478371 0.10282297 0.13281316 0.16058901 0.18131995 0.19520305 0.19954665 0.19737332 0.18977463]]...]
INFO - root - 2017-12-10 01:19:46.586365: step 75710, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.899 sec/batch; 64h:07m:27s remains)
INFO - root - 2017-12-10 01:19:55.155579: step 75720, loss = 0.82, batch loss = 0.69 (10.6 examples/sec; 0.756 sec/batch; 53h:54m:55s remains)
INFO - root - 2017-12-10 01:20:03.656471: step 75730, loss = 0.81, batch loss = 0.68 (9.3 examples/sec; 0.862 sec/batch; 61h:30m:01s remains)
INFO - root - 2017-12-10 01:20:12.209929: step 75740, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.841 sec/batch; 59h:57m:21s remains)
INFO - root - 2017-12-10 01:20:20.732997: step 75750, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.870 sec/batch; 62h:02m:48s remains)
INFO - root - 2017-12-10 01:20:29.174738: step 75760, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.876 sec/batch; 62h:28m:44s remains)
INFO - root - 2017-12-10 01:20:37.555559: step 75770, loss = 0.83, batch loss = 0.70 (9.3 examples/sec; 0.858 sec/batch; 61h:12m:59s remains)
INFO - root - 2017-12-10 01:20:46.157188: step 75780, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 61h:54m:25s remains)
INFO - root - 2017-12-10 01:20:54.841494: step 75790, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.834 sec/batch; 59h:29m:42s remains)
INFO - root - 2017-12-10 01:21:03.333196: step 75800, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 61h:59m:20s remains)
2017-12-10 01:21:04.231299: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.43073374 0.42260477 0.41184649 0.40001369 0.38733968 0.37485403 0.36242333 0.3501817 0.33870986 0.32782441 0.31943044 0.31311116 0.30739439 0.30123472 0.29482529][0.48743472 0.48155874 0.47065902 0.45745614 0.44358754 0.43076485 0.41755325 0.40407494 0.39172664 0.37965408 0.36942089 0.36029294 0.35224894 0.34397539 0.33436882][0.54573315 0.54525375 0.53678143 0.52374244 0.50861657 0.49355894 0.47805452 0.46274188 0.44851243 0.43455389 0.42188084 0.40978733 0.39847818 0.38661662 0.37319839][0.5985083 0.606574 0.60298747 0.59241748 0.57816511 0.56167859 0.54339451 0.52431226 0.50659978 0.4895328 0.47373751 0.45800668 0.44305563 0.42727152 0.40982917][0.63842362 0.65530652 0.65756428 0.65054077 0.63738328 0.62040859 0.59987879 0.57690585 0.55487692 0.533518 0.51370031 0.49428096 0.4759351 0.45616573 0.43505716][0.65687072 0.68158817 0.68995243 0.68795586 0.67751747 0.65987712 0.63695812 0.61072427 0.58456331 0.55860996 0.53456271 0.51097661 0.48884553 0.46586847 0.44228593][0.65066272 0.68097079 0.69392937 0.6965974 0.68862134 0.67133814 0.64727014 0.61818886 0.5878967 0.55833447 0.53083849 0.50462234 0.48034263 0.45671064 0.4332324][0.62744886 0.66080236 0.67611468 0.6813885 0.67564726 0.6591633 0.63436693 0.60421932 0.57162255 0.53977722 0.51048464 0.48333326 0.45849603 0.43517947 0.41290304][0.58671331 0.62058789 0.6359055 0.64238894 0.63838637 0.62419194 0.60145897 0.57213521 0.53969336 0.50800258 0.479033 0.452928 0.42948216 0.4079777 0.38812953][0.54202592 0.57281178 0.58529395 0.59087646 0.5873968 0.57459933 0.55420214 0.52788365 0.49785367 0.46839267 0.44184628 0.41853094 0.39791206 0.38005885 0.36380357][0.48951679 0.51637417 0.52495325 0.52787447 0.523692 0.51234138 0.49474728 0.47263888 0.44740435 0.42282161 0.40085921 0.38208035 0.36610016 0.35312605 0.34157655][0.43815023 0.45962963 0.46309334 0.46174577 0.45571512 0.44530863 0.4303948 0.41250697 0.39306772 0.37512848 0.3597526 0.34741279 0.33777958 0.330396 0.32379889][0.39082938 0.40685356 0.40606368 0.40133286 0.39352858 0.38360715 0.37129173 0.35727403 0.34288841 0.33065081 0.32141033 0.31494471 0.31103626 0.30882081 0.30707735][0.34824958 0.36041275 0.35741979 0.35145265 0.34366396 0.3344886 0.32438844 0.31363729 0.3037065 0.29620498 0.29205304 0.29058003 0.29162791 0.2936264 0.29548445][0.31414133 0.32318336 0.31852347 0.31169829 0.30378047 0.29544744 0.28712729 0.278889 0.27240413 0.26844421 0.26790029 0.26993525 0.2742236 0.27940208 0.28348339]]...]
INFO - root - 2017-12-10 01:21:12.823830: step 75810, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.851 sec/batch; 60h:40m:07s remains)
INFO - root - 2017-12-10 01:21:21.582065: step 75820, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.861 sec/batch; 61h:23m:18s remains)
INFO - root - 2017-12-10 01:21:30.102485: step 75830, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.834 sec/batch; 59h:26m:46s remains)
INFO - root - 2017-12-10 01:21:38.840345: step 75840, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.858 sec/batch; 61h:08m:43s remains)
INFO - root - 2017-12-10 01:21:47.514249: step 75850, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 62h:05m:40s remains)
INFO - root - 2017-12-10 01:21:56.145079: step 75860, loss = 0.83, batch loss = 0.70 (9.1 examples/sec; 0.881 sec/batch; 62h:46m:53s remains)
INFO - root - 2017-12-10 01:22:04.628220: step 75870, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.901 sec/batch; 64h:12m:26s remains)
INFO - root - 2017-12-10 01:22:13.353441: step 75880, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.872 sec/batch; 62h:11m:18s remains)
INFO - root - 2017-12-10 01:22:22.113261: step 75890, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.853 sec/batch; 60h:47m:15s remains)
INFO - root - 2017-12-10 01:22:30.672467: step 75900, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.880 sec/batch; 62h:41m:27s remains)
2017-12-10 01:22:31.581017: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0018043036 -0.0018050827 -0.0017986099 -0.001788659 -0.0017757622 -0.0017613175 -0.0017492201 -0.0017476366 -0.0017616184 -0.0017847858 -0.0018057754 -0.0018176037 -0.0018216184 -0.0018220562 -0.0018217356][-0.0017705816 -0.0017748503 -0.0017642173 -0.0017475154 -0.0017293395 -0.0017129671 -0.0017029981 -0.0017079849 -0.0017320933 -0.0017664847 -0.0017963693 -0.0018137241 -0.0018203545 -0.0018216319 -0.0018212143][-0.0017107951 -0.0017282973 -0.0017179231 -0.0016934229 -0.0016653816 -0.0016408457 -0.0016289686 -0.0016413121 -0.0016806625 -0.0017333091 -0.0017793293 -0.0018076123 -0.0018192978 -0.0018220512 -0.0018215406][-0.0015903942 -0.0016408594 -0.0016435555 -0.001617339 -0.0015797919 -0.0015425737 -0.0015216258 -0.0015343409 -0.0015855518 -0.0016604461 -0.0017340545 -0.0017864348 -0.0018127812 -0.0018213947 -0.0018222755][-0.0013245586 -0.0014145565 -0.0014371205 -0.0014129876 -0.0013655073 -0.0013128181 -0.0012750116 -0.0012771334 -0.0013404776 -0.0014606387 -0.0016020888 -0.0017191105 -0.0017878363 -0.0018155933 -0.0018220486][-0.00086221413 -0.00096764922 -0.00099536206 -0.00096174621 -0.00089565263 -0.00082194421 -0.000763324 -0.00075693917 -0.00085391663 -0.0010678757 -0.001344206 -0.0015875873 -0.0017382604 -0.0018028863 -0.0018202158][-0.0002847485 -0.00033098308 -0.00031467527 -0.00024606485 -0.00015269394 -6.173458e-05 3.3226097e-06 -8.0518657e-06 -0.00017567817 -0.00052951754 -0.00099274027 -0.0014083497 -0.0016698483 -0.0017843121 -0.0018162692][0.00013584399 0.00020729855 0.00031221507 0.00044138322 0.00057442824 0.00068591826 0.00074608217 0.00068943144 0.00042358914 -7.6405006e-05 -0.00070623634 -0.0012638632 -0.0016138587 -0.0017678002 -0.0018112215][0.00016678858 0.00035780075 0.00055527443 0.00075034925 0.00093123189 0.0010727219 0.00113565 0.0010415724 0.0007010902 0.00010694924 -0.00060865982 -0.0012221724 -0.0015983745 -0.0017609589 -0.0018068574][-0.00019253301 6.0372404e-05 0.00031210564 0.00055315963 0.00077858393 0.00096093665 0.0010475948 0.00095607748 0.00060466363 3.2777898e-06 -0.00069953967 -0.0012805617 -0.0016228952 -0.0017645892 -0.001803639][-0.00075903744 -0.000513552 -0.00026586908 -2.27493e-05 0.00021681015 0.000420696 0.00053353806 0.00047623913 0.00018071302 -0.00033375307 -0.0009248514 -0.0013995722 -0.0016684424 -0.0017750108 -0.0018033809][-0.0012776167 -0.0010916129 -0.00089479482 -0.00069716934 -0.00049864524 -0.00032237591 -0.00021266914 -0.00023490447 -0.00043999765 -0.00080640416 -0.0012200993 -0.0015440291 -0.0017214073 -0.001788768 -0.0018054538][-0.0016211323 -0.0015193395 -0.0014031176 -0.0012808889 -0.0011549448 -0.0010413716 -0.00096735213 -0.00097186444 -0.0010834733 -0.0012838738 -0.0015055202 -0.0016756934 -0.0017671422 -0.0018014981 -0.0018098166][-0.0017723821 -0.0017340697 -0.0016863605 -0.0016322846 -0.0015753191 -0.0015231044 -0.001489815 -0.0014911471 -0.001536953 -0.0016152058 -0.0016990023 -0.0017625081 -0.001796364 -0.0018092931 -0.0018123719][-0.0018168703 -0.0018095637 -0.0017980804 -0.0017824724 -0.0017648272 -0.001747753 -0.0017371476 -0.0017364582 -0.0017484706 -0.0017663391 -0.0017844593 -0.0017987003 -0.0018072597 -0.0018116293 -0.0018135152]]...]
INFO - root - 2017-12-10 01:22:40.220648: step 75910, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.885 sec/batch; 63h:03m:51s remains)
INFO - root - 2017-12-10 01:22:48.877617: step 75920, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.874 sec/batch; 62h:19m:36s remains)
INFO - root - 2017-12-10 01:22:57.445368: step 75930, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.872 sec/batch; 62h:10m:44s remains)
INFO - root - 2017-12-10 01:23:06.138358: step 75940, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.842 sec/batch; 59h:58m:20s remains)
INFO - root - 2017-12-10 01:23:14.836102: step 75950, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.857 sec/batch; 61h:04m:08s remains)
INFO - root - 2017-12-10 01:23:23.615701: step 75960, loss = 0.83, batch loss = 0.70 (9.2 examples/sec; 0.867 sec/batch; 61h:46m:31s remains)
INFO - root - 2017-12-10 01:23:32.293866: step 75970, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.881 sec/batch; 62h:46m:51s remains)
INFO - root - 2017-12-10 01:23:40.924310: step 75980, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 62h:03m:32s remains)
INFO - root - 2017-12-10 01:23:49.652244: step 75990, loss = 0.82, batch loss = 0.69 (8.2 examples/sec; 0.981 sec/batch; 69h:52m:54s remains)
INFO - root - 2017-12-10 01:23:58.285384: step 76000, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.870 sec/batch; 62h:00m:28s remains)
2017-12-10 01:23:59.217341: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.082595713 0.074002795 0.069228426 0.067539223 0.06894248 0.071943432 0.074579574 0.077175453 0.077990726 0.079148561 0.077984959 0.075214177 0.0698269 0.063570186 0.057167184][0.078186944 0.069757268 0.064112887 0.060862429 0.060378686 0.061065048 0.061950438 0.064029329 0.065743931 0.067832537 0.068448231 0.065998308 0.060677752 0.05324972 0.04477321][0.081048146 0.0730571 0.066980653 0.063123859 0.06163954 0.06066896 0.059122507 0.060497902 0.062446184 0.065051042 0.067229949 0.065770671 0.06172286 0.054042742 0.044646874][0.092338167 0.085474052 0.079530567 0.075130858 0.072496936 0.071503066 0.070707142 0.071627565 0.074320257 0.079245165 0.082121864 0.0821088 0.0783469 0.071523815 0.062065572][0.11114976 0.10746729 0.10333847 0.10005452 0.098210916 0.097608209 0.09749648 0.099335611 0.10241073 0.10756067 0.11082727 0.11162677 0.10799035 0.10151583 0.092895776][0.13367099 0.13337 0.13158914 0.13076982 0.13104671 0.13203621 0.13383938 0.13670161 0.14090608 0.14518996 0.14767404 0.1481362 0.14487374 0.13872659 0.13065408][0.15424994 0.15701756 0.1572993 0.15873082 0.16144863 0.16438705 0.16821852 0.17252599 0.17776601 0.18138386 0.18364491 0.18387225 0.18075612 0.17564291 0.1685157][0.1686528 0.17559423 0.17915086 0.18278942 0.18709666 0.19148138 0.19606508 0.20111966 0.20565037 0.20851791 0.21015452 0.20920679 0.20604733 0.20079216 0.19471416][0.17715508 0.18669872 0.19290072 0.19869673 0.20455132 0.21000996 0.21524504 0.22009359 0.22366853 0.22466898 0.22462791 0.22271314 0.21863022 0.21339867 0.20811962][0.17950751 0.1903016 0.19740489 0.20422734 0.21131447 0.21622743 0.22048298 0.22452089 0.22699727 0.226428 0.22493689 0.22314425 0.2201035 0.21533073 0.21090685][0.17756957 0.18818413 0.19480965 0.20167385 0.20846571 0.21287748 0.2170617 0.2192279 0.21982977 0.21825454 0.21546002 0.21218623 0.20849721 0.20496781 0.20183589][0.17323638 0.18223587 0.18754579 0.19283895 0.19783629 0.20167309 0.20532423 0.20670442 0.2068747 0.20473757 0.20179608 0.19823553 0.19488092 0.19167995 0.18905017][0.1685892 0.17564155 0.17915769 0.18227199 0.18497628 0.1874409 0.189804 0.19107929 0.19155754 0.19003096 0.18814971 0.18513222 0.18248893 0.17961241 0.17662984][0.16038103 0.16627204 0.16876268 0.17047587 0.17166552 0.17259891 0.17343426 0.17368174 0.17335239 0.1723066 0.17180218 0.16984248 0.16826569 0.16650869 0.16459343][0.14504841 0.15069549 0.15301386 0.15445586 0.15526858 0.15578711 0.15615335 0.15569918 0.15482198 0.15361235 0.15285724 0.15106288 0.14961855 0.1484641 0.14726666]]...]
INFO - root - 2017-12-10 01:24:07.804202: step 76010, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.859 sec/batch; 61h:13m:50s remains)
INFO - root - 2017-12-10 01:24:16.514381: step 76020, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.881 sec/batch; 62h:46m:51s remains)
INFO - root - 2017-12-10 01:24:24.963131: step 76030, loss = 0.81, batch loss = 0.68 (9.3 examples/sec; 0.860 sec/batch; 61h:15m:09s remains)
INFO - root - 2017-12-10 01:24:33.595879: step 76040, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.858 sec/batch; 61h:05m:45s remains)
INFO - root - 2017-12-10 01:24:42.214973: step 76050, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.878 sec/batch; 62h:33m:26s remains)
INFO - root - 2017-12-10 01:24:50.695396: step 76060, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.840 sec/batch; 59h:48m:09s remains)
INFO - root - 2017-12-10 01:24:59.153075: step 76070, loss = 0.82, batch loss = 0.69 (10.5 examples/sec; 0.762 sec/batch; 54h:18m:01s remains)
INFO - root - 2017-12-10 01:25:07.789370: step 76080, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.835 sec/batch; 59h:30m:37s remains)
INFO - root - 2017-12-10 01:25:16.498087: step 76090, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.882 sec/batch; 62h:48m:12s remains)
INFO - root - 2017-12-10 01:25:25.023002: step 76100, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.855 sec/batch; 60h:54m:45s remains)
2017-12-10 01:25:25.942567: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.030172672 0.031054188 0.031451922 0.031242363 0.030309 0.028265273 0.025860792 0.023638865 0.021579735 0.020139802 0.019145815 0.018982524 0.019138323 0.019180279 0.018952][0.03074662 0.03190124 0.03252263 0.03233758 0.031505518 0.029606756 0.027137935 0.024682876 0.022272872 0.020763384 0.019758565 0.019630294 0.01956805 0.019697977 0.019683426][0.029660642 0.030737441 0.031208817 0.031209957 0.030556735 0.028918147 0.026842829 0.024544947 0.022246655 0.020668622 0.019865366 0.01981584 0.019701229 0.01990293 0.020072905][0.028324658 0.029291075 0.029613413 0.029678132 0.029134434 0.027891718 0.026290048 0.024438374 0.02280823 0.021727059 0.021200521 0.021194784 0.020914983 0.021177469 0.021360997][0.027055319 0.0278773 0.02826127 0.028490858 0.028200166 0.02739317 0.026133962 0.024767078 0.023677696 0.023033563 0.02317602 0.023498813 0.023216616 0.023309968 0.023547076][0.025801662 0.026516475 0.0267856 0.027043413 0.026910003 0.026566684 0.025800673 0.024926947 0.024437437 0.02466587 0.02543989 0.025991447 0.025974322 0.026034461 0.026153693][0.024515215 0.024804834 0.024794105 0.024842937 0.024719816 0.024647197 0.024370287 0.024250316 0.024419796 0.025307262 0.026677763 0.02778215 0.028065559 0.028353684 0.028545][0.023234671 0.023105748 0.022586621 0.022197528 0.021856837 0.022021944 0.022147143 0.022751128 0.023618152 0.025229834 0.027203184 0.028923076 0.02963933 0.029979773 0.030287914][0.021182137 0.020702599 0.01977603 0.018918965 0.018472292 0.018633218 0.018913273 0.019952677 0.021441948 0.023754612 0.026382254 0.028730281 0.030333515 0.03111146 0.031564858][0.018550482 0.017758247 0.016395044 0.015103654 0.014388635 0.014418273 0.015205664 0.016483499 0.01837411 0.02121824 0.024315588 0.027486801 0.029931508 0.031335123 0.032089721][0.015522693 0.014574124 0.012983687 0.01149713 0.010576808 0.010482562 0.011299578 0.01312407 0.015606561 0.018530533 0.021667177 0.024989039 0.02789068 0.029802125 0.030880984][0.012368003 0.011519725 0.01010091 0.0087542152 0.0079057077 0.0079094581 0.0088374792 0.010765618 0.013380101 0.016438654 0.019624919 0.022636225 0.025300875 0.02730366 0.028459303][0.010302677 0.0096330363 0.0086915875 0.0077067134 0.0070723812 0.0071736951 0.0080396729 0.0097169215 0.0119905 0.014555364 0.01720407 0.019741904 0.021998623 0.023714503 0.024706021][0.0092417346 0.0086865453 0.0079394458 0.0071749049 0.0067822482 0.0068778354 0.0076993 0.0092029693 0.011163727 0.013322541 0.015436006 0.017348113 0.018957302 0.020200219 0.0208203][0.0090071959 0.0088166716 0.0083514042 0.0077429316 0.007354991 0.0074135005 0.0081224022 0.0093191573 0.010862914 0.012549397 0.014123273 0.015410173 0.016345261 0.016987493 0.017214907]]...]
INFO - root - 2017-12-10 01:25:34.656987: step 76110, loss = 0.83, batch loss = 0.70 (8.9 examples/sec; 0.896 sec/batch; 63h:48m:50s remains)
INFO - root - 2017-12-10 01:25:43.490280: step 76120, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.876 sec/batch; 62h:21m:59s remains)
INFO - root - 2017-12-10 01:25:51.998922: step 76130, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.877 sec/batch; 62h:27m:23s remains)
INFO - root - 2017-12-10 01:26:00.733506: step 76140, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.886 sec/batch; 63h:07m:04s remains)
INFO - root - 2017-12-10 01:26:09.567178: step 76150, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.881 sec/batch; 62h:44m:54s remains)
INFO - root - 2017-12-10 01:26:18.252430: step 76160, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.887 sec/batch; 63h:08m:12s remains)
INFO - root - 2017-12-10 01:26:27.063020: step 76170, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.873 sec/batch; 62h:10m:17s remains)
INFO - root - 2017-12-10 01:26:35.603010: step 76180, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.859 sec/batch; 61h:10m:15s remains)
INFO - root - 2017-12-10 01:26:44.407036: step 76190, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.886 sec/batch; 63h:05m:04s remains)
INFO - root - 2017-12-10 01:26:52.805187: step 76200, loss = 0.82, batch loss = 0.69 (8.8 examples/sec; 0.910 sec/batch; 64h:46m:56s remains)
2017-12-10 01:26:53.702920: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.14900418 0.15337105 0.15663877 0.15856385 0.16056022 0.16159312 0.16184819 0.16077529 0.15879183 0.15684061 0.15598577 0.15783182 0.16136721 0.16544746 0.16938649][0.17856924 0.1865463 0.19433588 0.19905715 0.20339657 0.20660816 0.20748071 0.20821385 0.20808651 0.20924321 0.21037364 0.2142282 0.2197068 0.22499792 0.23025088][0.21688764 0.22906336 0.24093743 0.2497884 0.25729036 0.26246977 0.26466751 0.2668598 0.26824111 0.27124417 0.27463704 0.27939239 0.28550643 0.2908828 0.2947349][0.26106471 0.27490225 0.28885075 0.3010242 0.31134123 0.31915537 0.32342488 0.32700065 0.32923356 0.33342716 0.33845177 0.34415457 0.35057938 0.35458925 0.35658473][0.3028518 0.31829536 0.33355722 0.34734958 0.35911572 0.36838275 0.37417263 0.379074 0.38235527 0.38684157 0.39224848 0.39807621 0.40364268 0.40558267 0.40471554][0.33731094 0.35313302 0.36823231 0.38285497 0.39619678 0.40725413 0.41508651 0.42112923 0.42510551 0.42955336 0.43447974 0.43851218 0.44159341 0.44081381 0.43678543][0.36257452 0.37772927 0.39190102 0.40739268 0.4212184 0.43317467 0.44206783 0.44888803 0.45320845 0.45644954 0.4606404 0.46355054 0.46414536 0.46014792 0.45285621][0.37996009 0.39321715 0.40474758 0.41933191 0.43317491 0.44562814 0.45527804 0.46295887 0.46713087 0.4688746 0.47196433 0.47338155 0.47240898 0.46583909 0.456625][0.38996327 0.40119696 0.41069195 0.42366654 0.43656287 0.44857749 0.45752993 0.46446985 0.46811149 0.468154 0.46912527 0.46937832 0.46781576 0.4607318 0.45210513][0.38993308 0.40100446 0.40946183 0.42071256 0.43228725 0.44249144 0.44993487 0.45491648 0.45778629 0.45709518 0.457578 0.45804831 0.45733523 0.45282033 0.4466905][0.37915197 0.389841 0.39700365 0.40670431 0.41632375 0.4248372 0.43095064 0.43488497 0.43800789 0.43860075 0.4403353 0.44261965 0.44403481 0.44208476 0.43769339][0.36349198 0.37355313 0.3791697 0.38703209 0.39423636 0.39998496 0.40361568 0.4063257 0.40878403 0.41048533 0.41468221 0.4207482 0.42666513 0.42823079 0.42705363][0.34753254 0.35749885 0.36151794 0.366799 0.37136403 0.37383854 0.37465933 0.37507463 0.37655497 0.379252 0.3852466 0.39520949 0.40536153 0.41088864 0.413334][0.32878745 0.34036514 0.34448525 0.34769759 0.34811518 0.34590554 0.34274644 0.33999184 0.34021953 0.34422088 0.35327771 0.36708316 0.38133776 0.39140964 0.39773235][0.30766279 0.31855789 0.32037649 0.32095522 0.31777772 0.3122355 0.30635351 0.3021006 0.30243862 0.30866367 0.32158113 0.33982518 0.35874537 0.37359872 0.38418242]]...]
INFO - root - 2017-12-10 01:27:02.379549: step 76210, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.877 sec/batch; 62h:25m:32s remains)
INFO - root - 2017-12-10 01:27:11.005016: step 76220, loss = 0.82, batch loss = 0.69 (8.8 examples/sec; 0.906 sec/batch; 64h:28m:18s remains)
INFO - root - 2017-12-10 01:27:19.494791: step 76230, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.889 sec/batch; 63h:18m:35s remains)
INFO - root - 2017-12-10 01:27:28.522550: step 76240, loss = 0.82, batch loss = 0.69 (8.0 examples/sec; 0.996 sec/batch; 70h:55m:59s remains)
INFO - root - 2017-12-10 01:27:37.344461: step 76250, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.885 sec/batch; 62h:57m:53s remains)
INFO - root - 2017-12-10 01:27:45.979180: step 76260, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 61h:29m:11s remains)
INFO - root - 2017-12-10 01:27:54.683678: step 76270, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.890 sec/batch; 63h:18m:59s remains)
INFO - root - 2017-12-10 01:28:03.287522: step 76280, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.878 sec/batch; 62h:30m:20s remains)
INFO - root - 2017-12-10 01:28:12.043069: step 76290, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.856 sec/batch; 60h:56m:32s remains)
INFO - root - 2017-12-10 01:28:20.504513: step 76300, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 61h:40m:33s remains)
2017-12-10 01:28:21.405481: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0014181945 -0.0014734244 -0.0014495371 -0.0013641824 -0.0011811277 -0.00097003923 -0.00074366096 -0.00063516328 -0.00061365822 -0.00078914082 -0.0010799142 -0.0014117984 -0.0016469746 -0.0017681936 -0.0018044823][-0.00067785452 -0.00084601261 -0.00086530077 -0.00071383314 -0.00039714621 -3.1073228e-05 0.00032998261 0.00043664838 0.00035165751 -7.9496065e-05 -0.00061496254 -0.0011605073 -0.0015418876 -0.001739859 -0.0018014028][0.0010626767 0.0006959344 0.00054078817 0.00078050944 0.0013477969 0.0020188456 0.0026403158 0.0027918629 0.0025071562 0.0015880599 0.00050000206 -0.00053892285 -0.0012643205 -0.001649467 -0.0017865479][0.0041494919 0.0035413075 0.003228724 0.0037105405 0.0048276796 0.0061464184 0.0072991722 0.0075653312 0.0068579908 0.0050000343 0.0027990327 0.00077227585 -0.00064773147 -0.0014247433 -0.0017394454][0.0082280654 0.0074241054 0.007065183 0.0080868648 0.010229162 0.012726666 0.014839894 0.015389207 0.014047393 0.010798192 0.00681602 0.0031698253 0.00053312012 -0.00097750849 -0.0016379108][0.01213075 0.011247756 0.011077151 0.013014511 0.016653458 0.020805251 0.024243308 0.025274515 0.023211462 0.01834169 0.012170436 0.0064989156 0.0022503315 -0.00029310631 -0.0014708424][0.014364368 0.013564509 0.013828718 0.016845042 0.022088625 0.028025763 0.032962948 0.034688756 0.032155033 0.025954148 0.017813791 0.010199609 0.0042320075 0.00051082729 -0.0012677924][0.013733312 0.013133802 0.01392353 0.017817361 0.024151886 0.031341244 0.03743092 0.039937615 0.037497368 0.030875817 0.02179789 0.013048018 0.005846417 0.0011793216 -0.0010954353][0.010508304 0.010115714 0.011283197 0.015398531 0.021809004 0.029181316 0.035569873 0.03862115 0.036863249 0.030976493 0.022437487 0.013854888 0.0064117643 0.0014271626 -0.001030009][0.0062023853 0.0059571569 0.0071373205 0.01066961 0.016047629 0.022378301 0.02806869 0.031227812 0.030465476 0.026195684 0.019464567 0.012275801 0.0056882715 0.0011397955 -0.0011080244][0.0023542969 0.0022790148 0.0032488089 0.0057359659 0.009436598 0.013900206 0.018129004 0.020844858 0.020918874 0.018457506 0.014048702 0.0089299493 0.0039588078 0.000428578 -0.0012954721][-0.00019782758 -0.00016035314 0.00048463687 0.0019298833 0.0040373523 0.0066414005 0.0092344731 0.011098092 0.011519569 0.010444527 0.0080657965 0.0049881628 0.0018385089 -0.00044387334 -0.0015167824][-0.0014191282 -0.001358358 -0.0010239379 -0.00035808666 0.00060379959 0.0018384448 0.0031396355 0.0041647051 0.0045399629 0.0041881716 0.0031300634 0.0016109456 -1.7253915e-06 -0.0011689272 -0.0016817752][-0.0017273556 -0.0017149951 -0.0016147649 -0.0014047084 -0.0010782421 -0.00063810812 -0.00015583844 0.00024878595 0.0004297971 0.00034402206 -2.2879569e-05 -0.00058776094 -0.0011851523 -0.001602708 -0.0017672773][-0.0018044114 -0.0017957387 -0.0017743242 -0.0017332246 -0.0016634781 -0.0015616728 -0.0014448988 -0.0013449797 -0.0012971655 -0.0013120498 -0.0013999527 -0.0015403104 -0.0016782997 -0.0017651221 -0.0017929591]]...]
INFO - root - 2017-12-10 01:28:30.046006: step 76310, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.837 sec/batch; 59h:35m:55s remains)
INFO - root - 2017-12-10 01:28:38.794530: step 76320, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.865 sec/batch; 61h:35m:09s remains)
INFO - root - 2017-12-10 01:28:47.316714: step 76330, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.863 sec/batch; 61h:26m:22s remains)
INFO - root - 2017-12-10 01:28:55.892415: step 76340, loss = 0.82, batch loss = 0.69 (9.7 examples/sec; 0.828 sec/batch; 58h:56m:11s remains)
INFO - root - 2017-12-10 01:29:04.548422: step 76350, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.863 sec/batch; 61h:25m:53s remains)
INFO - root - 2017-12-10 01:29:13.019718: step 76360, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.849 sec/batch; 60h:25m:03s remains)
INFO - root - 2017-12-10 01:29:21.639905: step 76370, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.857 sec/batch; 60h:58m:54s remains)
INFO - root - 2017-12-10 01:29:30.153734: step 76380, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 61h:42m:03s remains)
INFO - root - 2017-12-10 01:29:38.828896: step 76390, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.875 sec/batch; 62h:13m:10s remains)
INFO - root - 2017-12-10 01:29:47.252818: step 76400, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.848 sec/batch; 60h:19m:08s remains)
2017-12-10 01:29:48.162486: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.61971611 0.62677979 0.625846 0.62039149 0.6117385 0.60053879 0.58697933 0.57291943 0.55860168 0.54276484 0.52618051 0.50959015 0.49400163 0.47762424 0.46072465][0.6158433 0.62391382 0.62386119 0.61834788 0.61076891 0.6023789 0.59242064 0.58234686 0.571393 0.55840594 0.54235351 0.52385718 0.50471258 0.48454905 0.46472707][0.596845 0.60724372 0.60949332 0.60658348 0.60166568 0.59590244 0.58928579 0.5825249 0.57429236 0.56374335 0.54875857 0.52909678 0.50725287 0.48400643 0.4616791][0.57230908 0.58560777 0.59198296 0.59342271 0.59308255 0.59126878 0.58796704 0.58344173 0.57654136 0.56696796 0.55212218 0.53182429 0.5079506 0.48280162 0.45928872][0.54577464 0.56203878 0.57167214 0.577681 0.58193284 0.58412 0.58428484 0.58141232 0.5754475 0.56589669 0.550462 0.52972549 0.5044322 0.47876763 0.45531312][0.51783633 0.53791195 0.55215353 0.56357259 0.57310712 0.58011192 0.58356589 0.58260119 0.57611006 0.56545812 0.54863268 0.52658641 0.500348 0.47452009 0.45199874][0.49230203 0.51518381 0.53221893 0.54802537 0.56163436 0.57171267 0.57763916 0.57839805 0.57223678 0.56160694 0.54510415 0.52304858 0.49709615 0.47192249 0.45032609][0.46740547 0.49295542 0.51284456 0.53212 0.54940438 0.56197822 0.56960803 0.57103366 0.564428 0.55341119 0.53689635 0.51522118 0.48994994 0.46569315 0.44519171][0.44472095 0.47157234 0.49266574 0.51479417 0.53553689 0.55119264 0.56116682 0.56307685 0.55629247 0.54466832 0.52779049 0.50718135 0.48311147 0.46003607 0.4405342][0.42675221 0.45320296 0.47254816 0.49462736 0.51673484 0.53476268 0.54721278 0.55109394 0.54598236 0.53544551 0.51929426 0.49939907 0.47665092 0.45501491 0.43647516][0.40857819 0.43346775 0.4505997 0.47132748 0.49309388 0.51172507 0.52547419 0.53166318 0.52978921 0.52235246 0.50947279 0.49204978 0.47187918 0.45200849 0.4337486][0.39113116 0.41386989 0.42798093 0.4458 0.4661347 0.48462677 0.49953496 0.50786519 0.509507 0.50639379 0.49731266 0.48343772 0.46623817 0.44841024 0.43071681][0.37409028 0.39378017 0.40522438 0.42025059 0.43898878 0.45744836 0.47389132 0.48464176 0.48964572 0.48957992 0.48354807 0.47285932 0.45809245 0.44197267 0.42526063][0.35829008 0.37452284 0.38207456 0.39431876 0.41131729 0.43007481 0.44869223 0.46271881 0.47203603 0.47607651 0.47404674 0.46682492 0.45481661 0.44062302 0.42506295][0.34552944 0.35821438 0.36166677 0.36992741 0.38369477 0.40151149 0.42105228 0.43849617 0.45242319 0.46122178 0.46399406 0.46053687 0.45167765 0.4399471 0.42608297]]...]
INFO - root - 2017-12-10 01:29:56.747948: step 76410, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.844 sec/batch; 60h:00m:32s remains)
INFO - root - 2017-12-10 01:30:05.335414: step 76420, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 61h:57m:27s remains)
INFO - root - 2017-12-10 01:30:13.889316: step 76430, loss = 0.82, batch loss = 0.70 (9.2 examples/sec; 0.867 sec/batch; 61h:42m:01s remains)
INFO - root - 2017-12-10 01:30:22.528361: step 76440, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.896 sec/batch; 63h:43m:59s remains)
INFO - root - 2017-12-10 01:30:31.280794: step 76450, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.872 sec/batch; 62h:02m:30s remains)
INFO - root - 2017-12-10 01:30:40.053030: step 76460, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 61h:25m:34s remains)
INFO - root - 2017-12-10 01:30:48.634391: step 76470, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.859 sec/batch; 61h:07m:03s remains)
INFO - root - 2017-12-10 01:30:57.241052: step 76480, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.877 sec/batch; 62h:20m:19s remains)
INFO - root - 2017-12-10 01:31:05.865796: step 76490, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.872 sec/batch; 61h:58m:33s remains)
INFO - root - 2017-12-10 01:31:14.417553: step 76500, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.874 sec/batch; 62h:09m:27s remains)
2017-12-10 01:31:15.288117: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.00061570376 -0.00012981053 -0.00034284068 -1.6737496e-05 0.00071690965 0.0016948275 0.0027733599 0.0038244408 0.0047643231 0.0054604919 0.0057137916 0.0052782269 0.0040948316 0.00239343 0.00064640108][0.0007848338 1.4891848e-05 -0.00024611095 1.1478784e-05 0.00064188673 0.0014877628 0.0024050446 0.0032674023 0.0039986093 0.0044959378 0.0046062283 0.0041415812 0.0030672862 0.0015967254 0.00012796733][0.00037490961 -0.00028222566 -0.00050683669 -0.00032660656 0.00011598936 0.00070247462 0.0013358515 0.0019297487 0.0024283603 0.0027555539 0.0027959277 0.0024164482 0.0016008586 0.00051927648 -0.00053105818][-0.00034824607 -0.00078392331 -0.00089703739 -0.00075911835 -0.0004941345 -0.00017960172 0.00014317955 0.000439942 0.00069111667 0.00085656682 0.000861484 0.00062420953 0.0001318265 -0.00050913205 -0.0011195022][-0.00099535333 -0.0011673855 -0.0011088268 -0.00092276937 -0.00071875914 -0.00056567672 -0.00047869398 -0.00045663922 -0.0004681868 -0.00049841474 -0.0005662028 -0.00071018573 -0.00095048535 -0.0012475012 -0.0015231656][-0.0013186457 -0.0012033184 -0.00090240705 -0.00052612252 -0.00019661116 -2.4984241e-05 -5.2451156e-05 -0.00027220079 -0.00059543375 -0.0009270903 -0.0011964457 -0.0013887915 -0.0015285186 -0.0016394942 -0.0017282562][-0.0013390249 -0.00095648121 -0.00035809306 0.00034568144 0.00099102908 0.0013796563 0.0013887679 0.000990444 0.00031976902 -0.0004288 -0.0010600237 -0.0014747244 -0.0016837494 -0.0017662271 -0.0017973832][-0.0012156968 -0.00062355038 0.00025643513 0.0013173447 0.0023470675 0.0030535571 0.0032131583 0.0027335384 0.0017632508 0.00058344274 -0.00048085931 -0.0012216163 -0.0016107562 -0.0017627421 -0.0018059794][-0.0010880189 -0.00037021015 0.00069803454 0.0020168214 0.0033458797 0.0043377955 0.00469531 0.0042632348 0.0031541344 0.0016844677 0.00025818672 -0.0008105539 -0.0014279244 -0.0017028856 -0.001793848][-0.0010701229 -0.00034721568 0.00074082112 0.0021076403 0.0035160803 0.0046202703 0.00510991 0.0048061623 0.0037762178 0.0022994941 0.00076220313 -0.00047438522 -0.0012555434 -0.0016419846 -0.0017812875][-0.0011627693 -0.00051579287 0.00047199184 0.0017298186 0.0030423487 0.0040993141 0.0046211528 0.0044505792 0.0036174902 0.0023381105 0.0009168036 -0.00030819327 -0.0011494202 -0.0016000805 -0.0017720211][-0.0013572375 -0.00086375151 -7.9546357e-05 0.00095252891 0.0020542042 0.0029630847 0.00343533 0.0033495673 0.0027322872 0.0017441047 0.00060211017 -0.00042923226 -0.0011819499 -0.0016072127 -0.0017740291][-0.0015592154 -0.0012314108 -0.0006748609 9.4654737e-05 0.00094636146 0.0016688096 0.0020564552 0.002010291 0.0015619468 0.00084401562 1.1403114e-05 -0.00075206265 -0.0013261861 -0.0016568555 -0.0017867796][-0.0017142295 -0.0015416191 -0.0012159117 -0.00072746235 -0.0001525312 0.0003588387 0.00064539223 0.000623254 0.00032101863 -0.00015535345 -0.00068968919 -0.0011679207 -0.0015241613 -0.0017261964 -0.0018030143][-0.0017917779 -0.0017199104 -0.00156551 -0.0013124333 -0.00099319546 -0.00069208373 -0.00051240716 -0.00051741954 -0.00069241342 -0.00096561987 -0.0012587577 -0.0015084895 -0.0016863161 -0.0017815287 -0.0018148806]]...]
INFO - root - 2017-12-10 01:31:23.855766: step 76510, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.856 sec/batch; 60h:53m:58s remains)
INFO - root - 2017-12-10 01:31:32.560317: step 76520, loss = 0.81, batch loss = 0.68 (9.2 examples/sec; 0.869 sec/batch; 61h:46m:27s remains)
INFO - root - 2017-12-10 01:31:40.820400: step 76530, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.841 sec/batch; 59h:46m:39s remains)
INFO - root - 2017-12-10 01:31:49.370356: step 76540, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.882 sec/batch; 62h:42m:43s remains)
INFO - root - 2017-12-10 01:31:58.060262: step 76550, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.872 sec/batch; 61h:58m:38s remains)
INFO - root - 2017-12-10 01:32:06.763586: step 76560, loss = 0.82, batch loss = 0.69 (9.7 examples/sec; 0.825 sec/batch; 58h:38m:13s remains)
INFO - root - 2017-12-10 01:32:15.590993: step 76570, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.873 sec/batch; 62h:02m:57s remains)
INFO - root - 2017-12-10 01:32:24.241391: step 76580, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.902 sec/batch; 64h:07m:12s remains)
INFO - root - 2017-12-10 01:32:32.876820: step 76590, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.847 sec/batch; 60h:12m:52s remains)
INFO - root - 2017-12-10 01:32:41.440421: step 76600, loss = 0.82, batch loss = 0.69 (8.8 examples/sec; 0.907 sec/batch; 64h:26m:51s remains)
2017-12-10 01:32:42.321828: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.37806457 0.38655135 0.39014035 0.38976792 0.38528597 0.37584934 0.36154884 0.34564388 0.3289659 0.31106067 0.28952107 0.26359695 0.2362068 0.20920756 0.1865806][0.37632495 0.38383019 0.38572785 0.38335392 0.37659946 0.36480677 0.35004422 0.33541059 0.32160369 0.30725148 0.28949594 0.26635885 0.24044234 0.21386944 0.19101655][0.35674232 0.36290658 0.36224294 0.35615674 0.34545237 0.33162507 0.31645444 0.30345586 0.29382777 0.28496149 0.27320883 0.25532985 0.23345876 0.21028103 0.18942998][0.32888082 0.33405527 0.33104753 0.32132107 0.30667123 0.28955892 0.27297196 0.26108047 0.25505134 0.25176194 0.24704999 0.236377 0.22113046 0.20325275 0.18605317][0.2971307 0.29981539 0.29384682 0.28085539 0.26270595 0.24341726 0.226228 0.21510385 0.21168111 0.21295102 0.21453436 0.21133305 0.20375863 0.19258919 0.18035854][0.26373672 0.26321378 0.2542887 0.23864532 0.21842416 0.19838595 0.18223397 0.17335604 0.17257395 0.17784625 0.18502633 0.18864271 0.18763603 0.18256365 0.17501214][0.23420018 0.23055461 0.21842681 0.20020021 0.17803988 0.15710908 0.14201033 0.13543561 0.13748012 0.14671981 0.15833202 0.16773781 0.17254761 0.17304148 0.16968884][0.21097463 0.20545037 0.19164233 0.17180127 0.14868048 0.12787843 0.11324108 0.10811413 0.11186237 0.12352499 0.13826936 0.15145022 0.16021539 0.16438535 0.16420941][0.19110087 0.18703707 0.17415953 0.15545504 0.13344264 0.11323436 0.099518359 0.095304146 0.09959352 0.11162793 0.12710203 0.14190282 0.15265329 0.15853143 0.16015527][0.17849532 0.17633098 0.16584431 0.14984852 0.13049084 0.11256617 0.10030877 0.096521012 0.10073007 0.1118157 0.12592736 0.13935001 0.14921989 0.15532771 0.15735969][0.17004745 0.17140007 0.16447535 0.15258041 0.13718465 0.12195715 0.11147599 0.10834302 0.11159225 0.12054959 0.13198493 0.14248359 0.14988029 0.15397395 0.15518655][0.17125328 0.17629592 0.17304109 0.16488644 0.15307951 0.14058235 0.13152342 0.12814674 0.12990522 0.13557759 0.14264005 0.1487502 0.15270773 0.15410377 0.15380767][0.17329526 0.18231066 0.18352956 0.17986317 0.17272033 0.16407835 0.15678899 0.15304652 0.15248641 0.15422933 0.15640056 0.15756935 0.15695821 0.15478159 0.15244272][0.16924983 0.18124551 0.18655907 0.18796092 0.18598364 0.18151535 0.17668425 0.17314276 0.17099294 0.16945784 0.16770165 0.16510081 0.161467 0.15713055 0.15341383][0.16455877 0.1769539 0.18450737 0.18917979 0.19101582 0.19021057 0.18763329 0.18468384 0.18132633 0.17769124 0.17333663 0.16830705 0.16301489 0.15778582 0.15357807]]...]
INFO - root - 2017-12-10 01:32:50.881690: step 76610, loss = 0.83, batch loss = 0.70 (9.3 examples/sec; 0.865 sec/batch; 61h:26m:58s remains)
INFO - root - 2017-12-10 01:32:59.466333: step 76620, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.854 sec/batch; 60h:42m:59s remains)
INFO - root - 2017-12-10 01:33:07.827895: step 76630, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 61h:14m:51s remains)
INFO - root - 2017-12-10 01:33:16.282840: step 76640, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.844 sec/batch; 59h:57m:41s remains)
INFO - root - 2017-12-10 01:33:24.894480: step 76650, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.872 sec/batch; 61h:59m:15s remains)
INFO - root - 2017-12-10 01:33:33.611532: step 76660, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.857 sec/batch; 60h:55m:22s remains)
INFO - root - 2017-12-10 01:33:42.273984: step 76670, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 61h:35m:24s remains)
INFO - root - 2017-12-10 01:33:50.725281: step 76680, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.875 sec/batch; 62h:11m:44s remains)
INFO - root - 2017-12-10 01:33:59.378802: step 76690, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.835 sec/batch; 59h:19m:26s remains)
INFO - root - 2017-12-10 01:34:08.023277: step 76700, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.885 sec/batch; 62h:51m:09s remains)
2017-12-10 01:34:08.902476: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0018204544 -0.0018201284 -0.0018200683 -0.0018198417 -0.0018194618 -0.0018191388 -0.0018189581 -0.0018189038 -0.0018188363 -0.0018187123 -0.0018185839 -0.0018184292 -0.001818149 -0.0018178625 -0.0018176902][-0.0018213509 -0.0018211219 -0.0018210218 -0.0018206553 -0.0018200873 -0.0018195509 -0.001819141 -0.0018188864 -0.0018187064 -0.0018185321 -0.0018183884 -0.0018182199 -0.0018179356 -0.0018176024 -0.0018173904][-0.0018223452 -0.0018223132 -0.0018222722 -0.0018218687 -0.0018212066 -0.0018204832 -0.0018198042 -0.0018192652 -0.0018188542 -0.0018185952 -0.0018184069 -0.0018181856 -0.0018179296 -0.0018176434 -0.001817478][-0.001823575 -0.0018239369 -0.0018241083 -0.0018237617 -0.0018230665 -0.0018222133 -0.0018213195 -0.0018204954 -0.0018198139 -0.001819343 -0.001818945 -0.001818503 -0.0018181192 -0.0018177929 -0.0018176361][-0.0018248502 -0.0018258924 -0.0018265505 -0.0018265183 -0.0018260119 -0.0018251734 -0.001824066 -0.0018228403 -0.0018216851 -0.0018207332 -0.0018198539 -0.0018190084 -0.0018183662 -0.0018179429 -0.0018177618][-0.0018256586 -0.0018273602 -0.0018285889 -0.0018290513 -0.0018289294 -0.0018282837 -0.0018270599 -0.0018253888 -0.0018236308 -0.0018220682 -0.0018206352 -0.0018193446 -0.0018184631 -0.0018179755 -0.0018178183][-0.001825824 -0.0018280047 -0.0018297139 -0.0018307369 -0.00183111 -0.0018307401 -0.0018294705 -0.0018274658 -0.0018251976 -0.0018231536 -0.0018212497 -0.0018195555 -0.0018184688 -0.0018178805 -0.0018177459][-0.0018258297 -0.0018283642 -0.0018305293 -0.0018320642 -0.0018329351 -0.0018328759 -0.0018316754 -0.0018295049 -0.0018269171 -0.0018245068 -0.0018222582 -0.0018201971 -0.0018187793 -0.0018179291 -0.0018176662][-0.0018257299 -0.001828653 -0.0018314136 -0.0018335962 -0.0018350136 -0.0018353554 -0.0018344509 -0.00183246 -0.0018297775 -0.0018269905 -0.0018242684 -0.0018217422 -0.0018198247 -0.0018184746 -0.0018178519][-0.0018249997 -0.0018279974 -0.0018310155 -0.0018335497 -0.0018352637 -0.0018359025 -0.0018353608 -0.0018337492 -0.0018312471 -0.0018283577 -0.0018254037 -0.0018226554 -0.0018205235 -0.0018188995 -0.0018180205][-0.0018236835 -0.0018261559 -0.001828876 -0.001831282 -0.0018329585 -0.0018336576 -0.0018333667 -0.0018321113 -0.0018300228 -0.0018274691 -0.0018248587 -0.0018224395 -0.0018205274 -0.0018190385 -0.0018181172][-0.0018220487 -0.0018237167 -0.0018257337 -0.0018276484 -0.0018290115 -0.0018295709 -0.0018293652 -0.001828459 -0.0018269642 -0.0018250805 -0.0018231897 -0.0018214505 -0.0018200171 -0.0018188579 -0.0018180683][-0.0018205392 -0.0018215162 -0.0018228951 -0.0018242837 -0.0018252542 -0.0018256677 -0.001825551 -0.0018249592 -0.0018240032 -0.0018228014 -0.001821616 -0.0018205023 -0.0018195051 -0.0018186287 -0.0018179519][-0.0018193017 -0.0018197675 -0.0018205604 -0.0018214559 -0.0018220991 -0.0018223835 -0.0018223776 -0.0018220879 -0.001821563 -0.0018208531 -0.001820144 -0.0018194836 -0.0018188418 -0.0018182311 -0.0018177239][-0.0018183228 -0.0018182686 -0.0018184845 -0.0018188816 -0.0018191785 -0.0018193172 -0.0018193444 -0.001819243 -0.0018190528 -0.0018187675 -0.0018184669 -0.0018181723 -0.0018178798 -0.0018175921 -0.001817335]]...]
INFO - root - 2017-12-10 01:34:17.521719: step 76710, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 61h:45m:26s remains)
INFO - root - 2017-12-10 01:34:25.991596: step 76720, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.866 sec/batch; 61h:31m:38s remains)
INFO - root - 2017-12-10 01:34:34.482735: step 76730, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.855 sec/batch; 60h:43m:01s remains)
INFO - root - 2017-12-10 01:34:43.250424: step 76740, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.863 sec/batch; 61h:19m:27s remains)
INFO - root - 2017-12-10 01:34:51.897695: step 76750, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.852 sec/batch; 60h:31m:30s remains)
INFO - root - 2017-12-10 01:35:00.575147: step 76760, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.870 sec/batch; 61h:49m:44s remains)
INFO - root - 2017-12-10 01:35:09.198594: step 76770, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.863 sec/batch; 61h:17m:15s remains)
INFO - root - 2017-12-10 01:35:17.689855: step 76780, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 61h:35m:28s remains)
INFO - root - 2017-12-10 01:35:26.463052: step 76790, loss = 0.81, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 61h:44m:16s remains)
INFO - root - 2017-12-10 01:35:34.958827: step 76800, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.877 sec/batch; 62h:16m:36s remains)
2017-12-10 01:35:35.861638: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.001242795 -0.0012578561 -0.0012713729 -0.0012981873 -0.0013431949 -0.0014029786 -0.0014605561 -0.0014944976 -0.001487509 -0.0014304178 -0.0013382051 -0.0012463322 -0.0011952972 -0.0012032778 -0.001257919][-0.0016018974 -0.0016021521 -0.0015985852 -0.0015985001 -0.0016067135 -0.0016241156 -0.0016451345 -0.0016618643 -0.0016650198 -0.0016467556 -0.0016094965 -0.0015685634 -0.0015423766 -0.001539071 -0.001553732][-0.0017503739 -0.0017401009 -0.0017234005 -0.0017022463 -0.0016789219 -0.0016582925 -0.0016466276 -0.0016484454 -0.0016595791 -0.001667599 -0.0016649778 -0.0016533176 -0.0016398586 -0.0016288562 -0.0016198958][-0.0017641488 -0.0017345143 -0.0016903866 -0.0016299583 -0.0015550157 -0.0014777245 -0.0014222523 -0.0014086002 -0.0014357136 -0.0014783171 -0.0015126993 -0.0015284157 -0.0015302486 -0.001523635 -0.0015107796][-0.0016921851 -0.0016229384 -0.0015233806 -0.001390089 -0.0012283755 -0.0010639329 -0.00094403245 -0.00090913475 -0.00096102251 -0.0010569744 -0.0011483678 -0.0012092798 -0.0012405929 -0.0012506568 -0.0012422744][-0.0015440944 -0.0014135158 -0.0012311451 -0.00099896314 -0.00073225272 -0.00047371129 -0.00029062747 -0.00023804966 -0.0003196235 -0.00047888758 -0.00064734812 -0.0007773143 -0.00085488148 -0.0008818396 -0.00086461171][-0.0013076284 -0.0011201433 -0.00087198056 -0.00056645856 -0.00022739521 9.0864487e-05 0.00030956685 0.00036851631 0.00026151363 5.1154639e-05 -0.00018528954 -0.00037859671 -0.00049599865 -0.00052806991 -0.0004925814][-0.0010017452 -0.00078348687 -0.00051896728 -0.00020612567 0.00012951868 0.00043692754 0.000641412 0.000690821 0.00057300075 0.00034494058 7.8012e-05 -0.00014640065 -0.00028260157 -0.00031484268 -0.00027303712][-0.00075866189 -0.00054280029 -0.00030940713 -5.1918672e-05 0.00021133886 0.00044586894 0.00059566332 0.00062310335 0.00050697883 0.00029078696 3.2538897e-05 -0.00018744031 -0.00032285589 -0.00035933137 -0.00033430208][-0.00075601554 -0.000573655 -0.00039712852 -0.00021868967 -4.7791167e-05 9.8009477e-05 0.00018391979 0.00018627301 8.1366161e-05 -0.0001025839 -0.00032390759 -0.00051562267 -0.00063891348 -0.00068398553 -0.0006851973][-0.00099681551 -0.00086823979 -0.00075465976 -0.00065064919 -0.00055843545 -0.00048460776 -0.00044807489 -0.00046274753 -0.00054652791 -0.000683757 -0.00084810855 -0.00099251745 -0.0010900856 -0.0011350643 -0.0011518436][-0.0013446213 -0.0012705824 -0.001208022 -0.0011543386 -0.0011099466 -0.0010772909 -0.001065982 -0.001083479 -0.001139052 -0.0012239162 -0.0013235562 -0.0014113253 -0.0014726513 -0.0015048631 -0.001521559][-0.0016205289 -0.0015859455 -0.0015568878 -0.0015324352 -0.0015127887 -0.0014995264 -0.0014970907 -0.0015085803 -0.0015378046 -0.0015800041 -0.0016282346 -0.0016706713 -0.0017009177 -0.0017180727 -0.0017279135][-0.0017701793 -0.0017575753 -0.0017462091 -0.0017361642 -0.0017279512 -0.0017226691 -0.0017218286 -0.0017265818 -0.0017377951 -0.0017541152 -0.0017723106 -0.0017881505 -0.001799515 -0.0018062325 -0.0018103328][-0.0018264339 -0.0018226905 -0.0018190241 -0.001815322 -0.0018121621 -0.0018098718 -0.001808972 -0.0018098761 -0.0018124349 -0.0018167834 -0.0018215432 -0.0018259623 -0.0018292298 -0.001831262 -0.0018325475]]...]
INFO - root - 2017-12-10 01:35:44.406320: step 76810, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.881 sec/batch; 62h:35m:33s remains)
INFO - root - 2017-12-10 01:35:53.085209: step 76820, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.877 sec/batch; 62h:19m:01s remains)
INFO - root - 2017-12-10 01:36:01.631244: step 76830, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.838 sec/batch; 59h:28m:50s remains)
INFO - root - 2017-12-10 01:36:10.328201: step 76840, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.870 sec/batch; 61h:45m:31s remains)
INFO - root - 2017-12-10 01:36:19.063323: step 76850, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.875 sec/batch; 62h:07m:33s remains)
INFO - root - 2017-12-10 01:36:27.732085: step 76860, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.865 sec/batch; 61h:24m:47s remains)
INFO - root - 2017-12-10 01:36:36.453679: step 76870, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.849 sec/batch; 60h:17m:53s remains)
INFO - root - 2017-12-10 01:36:45.009498: step 76880, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.870 sec/batch; 61h:48m:15s remains)
INFO - root - 2017-12-10 01:36:53.586547: step 76890, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 61h:48m:33s remains)
INFO - root - 2017-12-10 01:37:02.031408: step 76900, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 61h:37m:51s remains)
2017-12-10 01:37:02.920923: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.25785568 0.25598964 0.25172332 0.24562627 0.2371235 0.22751254 0.21894494 0.21326871 0.20930105 0.20621504 0.20443712 0.20280656 0.20161128 0.19848704 0.19447052][0.28456128 0.28258 0.27686712 0.26830703 0.25766844 0.24642015 0.23699315 0.2307031 0.2275622 0.225572 0.22559544 0.22550867 0.22553612 0.22364807 0.21994068][0.30727282 0.30422789 0.29645157 0.28550526 0.27230194 0.25924298 0.24892816 0.24309243 0.24125959 0.24085404 0.2426378 0.24441174 0.24632445 0.2458595 0.24268983][0.32058316 0.31649265 0.30745754 0.2951526 0.28105128 0.26732132 0.25690922 0.25119191 0.24944612 0.25037768 0.25417268 0.25819296 0.26198021 0.26321244 0.26133484][0.32482344 0.31981045 0.30966938 0.2961759 0.28203487 0.26876396 0.25909585 0.25369412 0.25199664 0.25351128 0.25812563 0.26437247 0.27036947 0.27313238 0.27235392][0.32054681 0.31373644 0.30204502 0.28871775 0.27547145 0.26286587 0.25365245 0.24816829 0.24638468 0.24873784 0.25456196 0.262144 0.26932231 0.27399269 0.27477208][0.30642226 0.29944563 0.28726417 0.27533677 0.26409456 0.25319889 0.24458131 0.2387425 0.23693281 0.23928894 0.24560152 0.25423005 0.26269612 0.26900718 0.27069795][0.29217756 0.28698981 0.27631789 0.26591158 0.25529739 0.24480948 0.23554939 0.22839305 0.22488159 0.2267033 0.23325187 0.24228297 0.25140509 0.25865495 0.261352][0.28125498 0.27944416 0.2706435 0.26138553 0.25136739 0.24048056 0.22933982 0.21976729 0.21370742 0.21344648 0.21891434 0.22797991 0.23750441 0.24522981 0.24860248][0.27796778 0.27857694 0.27026308 0.26029888 0.24885812 0.23619249 0.22291973 0.21092932 0.20274715 0.20027305 0.20378423 0.21151817 0.22075054 0.22902147 0.23314424][0.27673513 0.27819741 0.26883498 0.25743285 0.24394381 0.22960523 0.21480009 0.20146224 0.19208094 0.18811357 0.19018936 0.19599959 0.20373224 0.2117714 0.21617737][0.27372929 0.27521041 0.26419076 0.250377 0.23443875 0.21883778 0.20337279 0.18962432 0.18007119 0.17539646 0.17661767 0.18121894 0.18827379 0.19569704 0.20040758][0.2676338 0.26840076 0.25633606 0.24103262 0.22387536 0.20741597 0.1920314 0.1787875 0.169536 0.16442019 0.16450466 0.16783369 0.17358641 0.18027467 0.18520208][0.25447175 0.25481877 0.24263744 0.22762099 0.21115431 0.19542097 0.18113312 0.16906264 0.16051805 0.15530232 0.15448922 0.15667625 0.16143605 0.16759358 0.17281984][0.23698558 0.23691033 0.22538011 0.21169239 0.19713058 0.18325643 0.17088585 0.16059752 0.15314968 0.14827058 0.14659102 0.1474712 0.15084746 0.15592936 0.16086806]]...]
INFO - root - 2017-12-10 01:37:11.648181: step 76910, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.887 sec/batch; 62h:56m:44s remains)
INFO - root - 2017-12-10 01:37:20.166663: step 76920, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.831 sec/batch; 58h:58m:13s remains)
INFO - root - 2017-12-10 01:37:28.693465: step 76930, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.860 sec/batch; 61h:05m:02s remains)
INFO - root - 2017-12-10 01:37:37.161770: step 76940, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.844 sec/batch; 59h:55m:48s remains)
INFO - root - 2017-12-10 01:37:45.812927: step 76950, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.852 sec/batch; 60h:29m:20s remains)
INFO - root - 2017-12-10 01:37:54.405493: step 76960, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.886 sec/batch; 62h:51m:33s remains)
INFO - root - 2017-12-10 01:38:03.088442: step 76970, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.845 sec/batch; 60h:00m:15s remains)
INFO - root - 2017-12-10 01:38:11.859444: step 76980, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.856 sec/batch; 60h:46m:08s remains)
INFO - root - 2017-12-10 01:38:20.496134: step 76990, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 61h:40m:40s remains)
INFO - root - 2017-12-10 01:38:28.994278: step 77000, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.845 sec/batch; 59h:59m:28s remains)
2017-12-10 01:38:29.934400: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0018333496 -0.0018333773 -0.001833768 -0.0018342228 -0.0018346814 -0.0018350896 -0.001835465 -0.0018356813 -0.0018354793 -0.0018348856 -0.0018341716 -0.0018334317 -0.0018327534 -0.0018321315 -0.0018315524][-0.0018334743 -0.0018337078 -0.0018343766 -0.001835156 -0.0018359418 -0.0018366709 -0.0018373142 -0.0018377341 -0.0018376678 -0.0018371639 -0.0018364589 -0.0018356226 -0.0018347003 -0.0018337368 -0.0018327214][-0.0018341596 -0.0018347867 -0.0018358877 -0.0018369918 -0.0018379386 -0.0018387705 -0.0018394766 -0.0018399402 -0.0018399556 -0.0018395561 -0.0018388991 -0.0018380465 -0.0018370235 -0.0018357927 -0.0018344119][-0.0018354245 -0.0018363424 -0.0018376791 -0.0018388662 -0.0018398011 -0.0018405376 -0.0018410609 -0.0018414072 -0.0018414924 -0.0018411778 -0.0018405354 -0.0018397262 -0.001838792 -0.00183754 -0.0018359773][-0.001836582 -0.0018376157 -0.0018389721 -0.0018401378 -0.0018410536 -0.0018417337 -0.0018420171 -0.0018420849 -0.0018420391 -0.0018417548 -0.0018411697 -0.0018404494 -0.0018396354 -0.0018385203 -0.001836986][-0.0018370018 -0.0018380581 -0.0018393777 -0.0018405897 -0.0018415352 -0.0018421129 -0.0018420112 -0.0018416785 -0.0018414955 -0.0018412739 -0.0018408451 -0.0018402898 -0.0018397853 -0.0018389999 -0.0018377772][-0.0018365544 -0.0018374381 -0.0018385776 -0.0018399323 -0.001841028 -0.0018416323 -0.001841342 -0.001840773 -0.0018406213 -0.0018405824 -0.0018403641 -0.0018399683 -0.0018397486 -0.0018394416 -0.0018386184][-0.0018357048 -0.0018362887 -0.0018372085 -0.0018384703 -0.0018396159 -0.0018401995 -0.001839851 -0.0018391692 -0.0018389556 -0.0018390591 -0.0018391135 -0.0018389932 -0.0018391378 -0.0018392584 -0.0018388144][-0.0018348441 -0.0018351634 -0.0018358602 -0.0018367349 -0.0018375464 -0.0018379368 -0.0018376603 -0.0018371047 -0.0018368823 -0.0018370269 -0.0018372686 -0.0018374239 -0.0018377742 -0.001838309 -0.0018383691][-0.001833885 -0.001834068 -0.0018346194 -0.0018351971 -0.0018356665 -0.00183592 -0.0018358133 -0.0018355591 -0.0018354603 -0.0018356431 -0.0018359824 -0.0018362377 -0.0018366528 -0.0018373564 -0.0018378488][-0.0018330042 -0.0018330674 -0.001833486 -0.0018339389 -0.001834293 -0.0018345631 -0.0018346271 -0.0018344864 -0.0018344097 -0.0018345726 -0.0018350406 -0.0018355134 -0.0018360515 -0.0018368304 -0.0018374968][-0.0018325613 -0.0018324205 -0.0018326556 -0.001832962 -0.0018332625 -0.0018335527 -0.0018337409 -0.0018336504 -0.0018335521 -0.0018337636 -0.001834337 -0.0018349596 -0.0018355778 -0.0018363545 -0.0018369944][-0.0018326624 -0.0018324348 -0.0018325257 -0.0018326833 -0.0018328653 -0.0018330748 -0.0018331906 -0.0018331476 -0.0018330745 -0.0018332793 -0.001833809 -0.001834435 -0.0018350229 -0.0018356855 -0.0018363094][-0.001832962 -0.0018327427 -0.0018327516 -0.0018328924 -0.0018329897 -0.001833076 -0.0018330852 -0.0018329709 -0.0018328744 -0.0018330232 -0.0018334716 -0.0018339889 -0.001834446 -0.0018349969 -0.0018355605][-0.0018331939 -0.0018329265 -0.0018328967 -0.0018330081 -0.0018331044 -0.0018331866 -0.0018331851 -0.0018330818 -0.0018329952 -0.0018330856 -0.0018333902 -0.001833778 -0.0018341389 -0.001834519 -0.0018348889]]...]
INFO - root - 2017-12-10 01:38:38.482453: step 77010, loss = 0.83, batch loss = 0.70 (9.4 examples/sec; 0.849 sec/batch; 60h:13m:13s remains)
INFO - root - 2017-12-10 01:38:47.149226: step 77020, loss = 0.83, batch loss = 0.70 (9.2 examples/sec; 0.871 sec/batch; 61h:50m:17s remains)
INFO - root - 2017-12-10 01:38:55.536652: step 77030, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.852 sec/batch; 60h:26m:44s remains)
INFO - root - 2017-12-10 01:39:04.083782: step 77040, loss = 0.83, batch loss = 0.70 (9.1 examples/sec; 0.880 sec/batch; 62h:26m:38s remains)
INFO - root - 2017-12-10 01:39:12.716896: step 77050, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 61h:34m:33s remains)
INFO - root - 2017-12-10 01:39:21.491151: step 77060, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.896 sec/batch; 63h:33m:32s remains)
INFO - root - 2017-12-10 01:39:30.165750: step 77070, loss = 0.83, batch loss = 0.70 (9.0 examples/sec; 0.888 sec/batch; 62h:59m:54s remains)
INFO - root - 2017-12-10 01:39:38.792141: step 77080, loss = 0.82, batch loss = 0.69 (9.8 examples/sec; 0.815 sec/batch; 57h:51m:25s remains)
INFO - root - 2017-12-10 01:39:47.408734: step 77090, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.888 sec/batch; 63h:00m:01s remains)
INFO - root - 2017-12-10 01:39:56.040990: step 77100, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.889 sec/batch; 63h:05m:54s remains)
2017-12-10 01:39:56.937313: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.0001643392 0.0011255931 0.0026314273 0.0045638531 0.006719193 0.0086986274 0.010030106 0.010345818 0.0095417583 0.0077843047 0.0054668467 0.0031105713 0.0011069324 -0.0003231616 -0.0011780262][0.0010054879 0.0024404675 0.0044817138 0.0070533133 0.0097781308 0.012169716 0.013753044 0.014128636 0.013141398 0.011015095 0.0082204323 0.0053014276 0.0026866449 0.00068835763 -0.00061297417][0.0019499554 0.0036212201 0.0058511877 0.0086238617 0.011676123 0.014454312 0.016314268 0.016782366 0.015684824 0.013342997 0.010270079 0.007037383 0.004090535 0.0017264913 6.1797211e-05][0.0026142085 0.0044773845 0.0067451731 0.0094145816 0.012301484 0.015014798 0.016978947 0.017631814 0.016676424 0.014379441 0.01124042 0.0078903548 0.0047728089 0.0022300915 0.0004116263][0.0025725323 0.0046514967 0.00707766 0.0097467368 0.012489811 0.014961706 0.016737333 0.017329939 0.016418904 0.014220306 0.011171002 0.0078853117 0.0047913459 0.0022544302 0.00044072384][0.0017052478 0.0037585064 0.00615665 0.0086580021 0.011096197 0.013219873 0.014683976 0.015077782 0.014157533 0.012109403 0.0092982184 0.0063089193 0.0035561887 0.0013815892 -0.00010464666][0.00053871318 0.0021733614 0.0041512358 0.006250177 0.00831097 0.010121596 0.011389752 0.011759059 0.010975058 0.00919247 0.0067521934 0.0042014765 0.0019501148 0.00027678546 -0.00078002829][-0.00064656 0.00035133853 0.0016132829 0.0029987264 0.0044679595 0.0058878758 0.006988727 0.0074329511 0.006963423 0.0056705857 0.0038323915 0.0019302176 0.00033048668 -0.00076604192 -0.0013755386][-0.0013903629 -0.00096417969 -0.00038938283 0.00030960434 0.0011505621 0.0020903018 0.0029180665 0.0033445545 0.0031607011 0.0024104081 0.0013019779 0.00015261921 -0.00078093645 -0.0013740221 -0.0016626621][-0.00174114 -0.0016339275 -0.0014714898 -0.0012447474 -0.00090882229 -0.00047096936 -4.4898246e-05 0.00019875925 0.00015137868 -0.00016035128 -0.00063070317 -0.0011126122 -0.0014863217 -0.0017009532 -0.0017852768][-0.0018142953 -0.0018088544 -0.0017913635 -0.0017542944 -0.0016756073 -0.0015477733 -0.0014102022 -0.0013273971 -0.0013326854 -0.001409975 -0.001530842 -0.0016557632 -0.0017481855 -0.0017934426 -0.0018057757][-0.0018124054 -0.0018115453 -0.0018117381 -0.0018101232 -0.0018035286 -0.00178816 -0.001769057 -0.0017563291 -0.0017548831 -0.0017624174 -0.0017763923 -0.0017917844 -0.0018032971 -0.0018079932 -0.0018080411][-0.0018104847 -0.0018092815 -0.0018090592 -0.0018094686 -0.0018097893 -0.0018095397 -0.001809612 -0.0018097621 -0.0018099182 -0.0018092761 -0.0018086811 -0.0018082939 -0.0018080828 -0.0018076519 -0.001807285][-0.0018088673 -0.0018074783 -0.0018069458 -0.0018069117 -0.0018071292 -0.0018071836 -0.0018070687 -0.0018071932 -0.0018070895 -0.0018064596 -0.0018063757 -0.0018065203 -0.001806823 -0.0018066869 -0.0018067692][-0.0018082904 -0.0018068707 -0.0018061498 -0.0018059084 -0.0018058304 -0.0018056855 -0.001805608 -0.0018052461 -0.0018052013 -0.0018049019 -0.0018049199 -0.001805055 -0.0018053218 -0.0018056822 -0.0018060378]]...]
INFO - root - 2017-12-10 01:40:05.698498: step 77110, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.854 sec/batch; 60h:35m:46s remains)
INFO - root - 2017-12-10 01:40:14.432182: step 77120, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.866 sec/batch; 61h:25m:09s remains)
INFO - root - 2017-12-10 01:40:22.993217: step 77130, loss = 0.82, batch loss = 0.69 (8.6 examples/sec; 0.930 sec/batch; 65h:56m:14s remains)
INFO - root - 2017-12-10 01:40:31.883890: step 77140, loss = 0.82, batch loss = 0.69 (8.8 examples/sec; 0.908 sec/batch; 64h:24m:06s remains)
INFO - root - 2017-12-10 01:40:40.619442: step 77150, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.873 sec/batch; 61h:54m:09s remains)
INFO - root - 2017-12-10 01:40:49.321600: step 77160, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.847 sec/batch; 60h:02m:45s remains)
INFO - root - 2017-12-10 01:40:57.948103: step 77170, loss = 0.82, batch loss = 0.69 (9.7 examples/sec; 0.828 sec/batch; 58h:44m:07s remains)
INFO - root - 2017-12-10 01:41:06.731346: step 77180, loss = 0.82, batch loss = 0.69 (9.8 examples/sec; 0.813 sec/batch; 57h:38m:44s remains)
INFO - root - 2017-12-10 01:41:15.267581: step 77190, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.887 sec/batch; 62h:56m:08s remains)
INFO - root - 2017-12-10 01:41:23.738466: step 77200, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.875 sec/batch; 62h:04m:46s remains)
2017-12-10 01:41:24.656389: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.32583466 0.33102235 0.33478481 0.3386226 0.33995309 0.33582798 0.32399461 0.30556172 0.28348634 0.25688654 0.22940795 0.2047603 0.18601546 0.17272617 0.16509144][0.33187315 0.33757713 0.34189937 0.34587613 0.34751621 0.34441239 0.33356702 0.31553397 0.293531 0.26699555 0.23883553 0.21209511 0.19064093 0.17369872 0.16252641][0.32430959 0.33105826 0.33690763 0.34277871 0.34627891 0.34527153 0.33624277 0.32001507 0.29905787 0.27363741 0.24604283 0.21908557 0.19649845 0.17754386 0.16374557][0.30518827 0.31360903 0.32166073 0.33048749 0.33742574 0.34065902 0.33536369 0.32239035 0.30435202 0.28127754 0.25571954 0.22980529 0.20772822 0.18808261 0.17278893][0.27678171 0.28789651 0.29890013 0.31152841 0.32245022 0.32996821 0.32912654 0.32091829 0.30689785 0.28785238 0.26604873 0.24316576 0.22323655 0.20474815 0.18913981][0.243309 0.2580719 0.27335089 0.29020521 0.30534121 0.31705531 0.32046622 0.31698316 0.30758333 0.29383388 0.2765063 0.25761572 0.24088472 0.22441195 0.20947823][0.20752585 0.22461478 0.2437197 0.26527858 0.28544497 0.30176431 0.31018281 0.31165528 0.30652398 0.29762116 0.28473476 0.26950955 0.25591522 0.24188682 0.22817466][0.17651191 0.1953214 0.21755381 0.24277824 0.26677254 0.28648937 0.29836321 0.303809 0.30223367 0.29643553 0.28634089 0.27441153 0.26334882 0.2513901 0.23910461][0.15258305 0.17232941 0.19611025 0.2230747 0.24928281 0.27110267 0.28537416 0.29341137 0.29445517 0.2914913 0.28363317 0.27380031 0.26411226 0.25366098 0.24240084][0.13845588 0.15768974 0.18108268 0.20792101 0.23406893 0.25504339 0.26944566 0.27824032 0.28049 0.27902466 0.27282277 0.2649332 0.25631335 0.24703917 0.23693979][0.13272396 0.1505104 0.17194092 0.19655094 0.22059333 0.23954955 0.25250062 0.26038876 0.26257095 0.26113284 0.25531697 0.24822545 0.24040291 0.23199691 0.22276615][0.1320183 0.14784679 0.16587442 0.18716127 0.20829475 0.22474499 0.23581742 0.24205814 0.24333943 0.24057701 0.2340809 0.22657202 0.21860337 0.21047811 0.20193031][0.138606 0.15201625 0.1660313 0.18227978 0.19841148 0.21051589 0.21822277 0.22230986 0.22250481 0.21866228 0.21168977 0.20396161 0.19573107 0.1872434 0.17889959][0.14516827 0.15560921 0.16493741 0.17654365 0.18799813 0.19647925 0.20144257 0.20355806 0.20270222 0.19850351 0.1916469 0.18388581 0.17595354 0.1680844 0.16034934][0.15420265 0.16194804 0.16707903 0.17385551 0.18068933 0.18542382 0.1877687 0.1880772 0.18634194 0.18198377 0.17576244 0.16880557 0.16178466 0.15490159 0.14830028]]...]
INFO - root - 2017-12-10 01:41:33.338208: step 77210, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.861 sec/batch; 61h:03m:52s remains)
INFO - root - 2017-12-10 01:41:41.951218: step 77220, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.884 sec/batch; 62h:42m:48s remains)
INFO - root - 2017-12-10 01:41:50.605939: step 77230, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.870 sec/batch; 61h:41m:31s remains)
INFO - root - 2017-12-10 01:41:59.150554: step 77240, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.836 sec/batch; 59h:17m:29s remains)
INFO - root - 2017-12-10 01:42:07.778660: step 77250, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.860 sec/batch; 60h:58m:20s remains)
INFO - root - 2017-12-10 01:42:16.478544: step 77260, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.861 sec/batch; 61h:02m:02s remains)
INFO - root - 2017-12-10 01:42:25.300836: step 77270, loss = 0.81, batch loss = 0.68 (8.8 examples/sec; 0.907 sec/batch; 64h:20m:04s remains)
INFO - root - 2017-12-10 01:42:33.932602: step 77280, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.874 sec/batch; 61h:58m:23s remains)
INFO - root - 2017-12-10 01:42:42.559513: step 77290, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.889 sec/batch; 63h:02m:20s remains)
INFO - root - 2017-12-10 01:42:51.122363: step 77300, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.872 sec/batch; 61h:47m:00s remains)
2017-12-10 01:42:52.130296: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.077841178 0.071937531 0.066801794 0.063675761 0.06285838 0.064046748 0.067046933 0.071841434 0.077769965 0.084122844 0.089774117 0.093919776 0.095777355 0.093772359 0.08800371][0.085607976 0.079376616 0.073910117 0.0705827 0.0699321 0.071326666 0.074331 0.07872884 0.083870381 0.089314185 0.093878724 0.09729778 0.098991752 0.097530141 0.092672274][0.0939112 0.088312142 0.0833153 0.080497786 0.080435105 0.082308844 0.085479237 0.089366704 0.093485765 0.097347878 0.10020433 0.1019246 0.10240937 0.10066219 0.096126534][0.10305165 0.098830447 0.094898626 0.092902295 0.093533181 0.09593524 0.099100977 0.1024977 0.10560795 0.10814063 0.10928441 0.10926387 0.10822608 0.10547613 0.10057682][0.11140293 0.10943691 0.10722736 0.10651327 0.10795699 0.11073142 0.11364649 0.11605575 0.11779685 0.11875889 0.1183663 0.1167827 0.11429054 0.11056484 0.1052409][0.1172727 0.11776389 0.11729241 0.1176534 0.11956867 0.12228451 0.12460668 0.12598427 0.12642024 0.12601182 0.12432805 0.12152282 0.11794199 0.11349128 0.10783857][0.11906756 0.1217675 0.12284537 0.12417334 0.12637559 0.12888908 0.13056178 0.1309637 0.13036615 0.12894133 0.1263409 0.12269733 0.11835372 0.11333048 0.10742611][0.11706394 0.12143951 0.12351989 0.12529495 0.1274133 0.12948298 0.1306016 0.13035823 0.12906089 0.12701994 0.12400823 0.11995289 0.11511436 0.10969866 0.10359117][0.11170848 0.11713186 0.11976416 0.12166087 0.12344574 0.12483425 0.12520127 0.12429968 0.12249266 0.12013847 0.1170224 0.11314489 0.10839555 0.10303015 0.096956655][0.10335935 0.10909017 0.1117647 0.11346029 0.11471715 0.11531212 0.11492062 0.1133456 0.11106228 0.10839195 0.10524634 0.10162412 0.0972966 0.092403874 0.086847432][0.091984555 0.097285591 0.099496834 0.10059043 0.10105266 0.10077634 0.099634774 0.097612441 0.095137961 0.092469625 0.089588121 0.086531863 0.082920246 0.078845061 0.074092217][0.07764411 0.082172319 0.083863869 0.084409446 0.084234096 0.08327242 0.081564307 0.079225585 0.076695226 0.074205257 0.071770325 0.069465123 0.066853866 0.063806646 0.060131524][0.061104171 0.064607367 0.065742895 0.065798476 0.065127596 0.063718148 0.06170395 0.059288159 0.056878537 0.054724917 0.052872624 0.051357884 0.049800847 0.047956731 0.045555156][0.0439007 0.046370868 0.047099657 0.046938159 0.046101216 0.044594884 0.0425933 0.040347837 0.038241755 0.03646794 0.035119485 0.034269013 0.033602279 0.032830402 0.031635959][0.028177237 0.029697668 0.030059341 0.029760282 0.028935427 0.027616357 0.025950411 0.024155077 0.022501914 0.021180471 0.020269839 0.019830046 0.019719152 0.019641055 0.019334557]]...]
INFO - root - 2017-12-10 01:43:00.818055: step 77310, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.891 sec/batch; 63h:10m:17s remains)
INFO - root - 2017-12-10 01:43:09.525705: step 77320, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 61h:25m:45s remains)
INFO - root - 2017-12-10 01:43:18.147737: step 77330, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.866 sec/batch; 61h:24m:59s remains)
INFO - root - 2017-12-10 01:43:27.033577: step 77340, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.884 sec/batch; 62h:39m:56s remains)
INFO - root - 2017-12-10 01:43:35.763602: step 77350, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.881 sec/batch; 62h:26m:57s remains)
INFO - root - 2017-12-10 01:43:44.389825: step 77360, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.844 sec/batch; 59h:47m:20s remains)
INFO - root - 2017-12-10 01:43:53.021158: step 77370, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.874 sec/batch; 61h:56m:30s remains)
INFO - root - 2017-12-10 01:44:01.726525: step 77380, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.897 sec/batch; 63h:35m:08s remains)
INFO - root - 2017-12-10 01:44:10.276135: step 77390, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.873 sec/batch; 61h:53m:24s remains)
INFO - root - 2017-12-10 01:44:18.840235: step 77400, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.836 sec/batch; 59h:12m:23s remains)
2017-12-10 01:44:19.802246: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.49850696 0.4785445 0.45333657 0.42978817 0.41039261 0.39673421 0.38883054 0.38567272 0.38386521 0.37891284 0.36792782 0.35277066 0.33425397 0.31594649 0.30359158][0.50103784 0.48321542 0.46057931 0.44008645 0.42296842 0.4110201 0.40381426 0.40169889 0.40053764 0.39502504 0.38237569 0.36445314 0.34442234 0.32577878 0.31363082][0.48926026 0.47589305 0.45898893 0.44423044 0.43272641 0.42333034 0.41684157 0.41448087 0.41164559 0.40480161 0.39063495 0.371305 0.35095409 0.33269852 0.32123768][0.47246975 0.46382505 0.45332927 0.44556305 0.44073495 0.43648365 0.43341362 0.43036824 0.42481953 0.41559017 0.39936271 0.37944704 0.35981005 0.34340629 0.33338886][0.45074314 0.44777715 0.44432798 0.44385344 0.44550252 0.44623995 0.44674912 0.44428489 0.43675357 0.42545706 0.40762585 0.38731247 0.36828384 0.35347626 0.34493867][0.42708927 0.43032053 0.43331233 0.44008827 0.4480665 0.45338845 0.45669654 0.4551625 0.44717035 0.43413877 0.41631034 0.39764604 0.38068643 0.36736917 0.3597331][0.40283141 0.41242114 0.42109975 0.43401974 0.44700107 0.45555192 0.460931 0.45997831 0.45213816 0.43975681 0.4247236 0.40949371 0.39532918 0.38442135 0.37788889][0.38084221 0.39518398 0.40792426 0.42424119 0.43974984 0.4505665 0.45720187 0.45734328 0.45073044 0.44075075 0.42982292 0.41963524 0.40985811 0.40088192 0.39465714][0.36063185 0.37855917 0.39360639 0.41154623 0.42871895 0.44140297 0.4498398 0.45147473 0.44720584 0.4409425 0.43430489 0.42876869 0.42251241 0.41563049 0.40952274][0.341736 0.36177239 0.37786508 0.39543581 0.41179204 0.4249894 0.43493319 0.43943545 0.43926567 0.43764919 0.43579969 0.43415561 0.43050724 0.4250364 0.41880837][0.32464534 0.3441225 0.35799277 0.37370914 0.38915989 0.40183622 0.41200069 0.41914317 0.42353404 0.42714614 0.4309271 0.43424582 0.43441549 0.43139055 0.42550716][0.30884844 0.32745379 0.33891103 0.35074595 0.36359876 0.37546378 0.38557827 0.39392829 0.40156743 0.4105984 0.41958675 0.42683849 0.4307285 0.43061495 0.42672616][0.29649103 0.31206411 0.3199974 0.32806647 0.33811224 0.34800863 0.35743159 0.366593 0.3764385 0.38852018 0.40076473 0.41089162 0.41814566 0.42130417 0.42030844][0.28265134 0.2956143 0.30084085 0.30648628 0.31470683 0.32299989 0.33183634 0.34116286 0.35252354 0.36620083 0.38008949 0.3926622 0.40259063 0.40935507 0.41204932][0.2692908 0.280016 0.282521 0.28582281 0.29118332 0.29746455 0.30505002 0.31444785 0.32698387 0.34131002 0.35631606 0.3704465 0.3827903 0.39264345 0.39885262]]...]
INFO - root - 2017-12-10 01:44:28.453076: step 77410, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 61h:13m:52s remains)
INFO - root - 2017-12-10 01:44:37.088751: step 77420, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.842 sec/batch; 59h:37m:45s remains)
INFO - root - 2017-12-10 01:44:45.486835: step 77430, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.847 sec/batch; 60h:00m:26s remains)
INFO - root - 2017-12-10 01:44:54.159059: step 77440, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.878 sec/batch; 62h:13m:30s remains)
INFO - root - 2017-12-10 01:45:02.717963: step 77450, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.848 sec/batch; 60h:04m:59s remains)
INFO - root - 2017-12-10 01:45:11.425643: step 77460, loss = 0.82, batch loss = 0.69 (8.4 examples/sec; 0.948 sec/batch; 67h:08m:01s remains)
INFO - root - 2017-12-10 01:45:20.162158: step 77470, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 61h:25m:13s remains)
INFO - root - 2017-12-10 01:45:28.802232: step 77480, loss = 0.82, batch loss = 0.69 (9.8 examples/sec; 0.815 sec/batch; 57h:44m:41s remains)
INFO - root - 2017-12-10 01:45:37.191429: step 77490, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.843 sec/batch; 59h:42m:58s remains)
INFO - root - 2017-12-10 01:45:45.617607: step 77500, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.850 sec/batch; 60h:11m:34s remains)
2017-12-10 01:45:46.583641: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0018237518 -0.0018235265 -0.0018237582 -0.0018240038 -0.0018240542 -0.0018239741 -0.0018237877 -0.001823474 -0.0018230785 -0.0018227905 -0.0018226209 -0.0018224906 -0.001822339 -0.0018222693 -0.0018222935][-0.001824014 -0.001823877 -0.0018241389 -0.0018243872 -0.0018243213 -0.0018240261 -0.0018236572 -0.0018232656 -0.0018228531 -0.0018226223 -0.0018225595 -0.0018225026 -0.00182234 -0.00182219 -0.001822121][-0.0018245716 -0.0018245509 -0.0018248389 -0.0018250386 -0.0018247539 -0.0018242018 -0.0018236313 -0.0018231635 -0.0018227206 -0.0018224985 -0.0018224866 -0.0018225011 -0.0018224353 -0.0018223388 -0.0018222365][-0.0018252044 -0.0018252023 -0.001825451 -0.0018255946 -0.0018251499 -0.0018244132 -0.0018236778 -0.0018231351 -0.0018226387 -0.0018223922 -0.001822361 -0.0018224081 -0.0018224658 -0.0018224764 -0.001822339][-0.0018257912 -0.0018257647 -0.0018259345 -0.0018260168 -0.0018255305 -0.0018247253 -0.0018237986 -0.0018231719 -0.0018227436 -0.0018225588 -0.0018225266 -0.0018225592 -0.0018226729 -0.0018226797 -0.0018224658][-0.0018259076 -0.0018258493 -0.0018259485 -0.001825925 -0.0018253435 -0.0018244195 -0.001823284 -0.0018225873 -0.0018223531 -0.0018223174 -0.0018223283 -0.0018223699 -0.0018225244 -0.0018225595 -0.0018223376][-0.0018256592 -0.001825535 -0.0018255711 -0.0018254846 -0.0018248387 -0.001823813 -0.0018225791 -0.001821843 -0.001821673 -0.0018216907 -0.0018217355 -0.0018217826 -0.0018219833 -0.0018221403 -0.0018220238][-0.0018252755 -0.0018250584 -0.0018250524 -0.0018249876 -0.0018243865 -0.0018234296 -0.0018222806 -0.0018214912 -0.0018211567 -0.0018210548 -0.0018210773 -0.0018211463 -0.0018213816 -0.0018216092 -0.0018215715][-0.0018245843 -0.0018243331 -0.0018243687 -0.0018243406 -0.0018238126 -0.0018230336 -0.0018221274 -0.0018212569 -0.0018207454 -0.001820543 -0.0018206114 -0.0018207575 -0.0018209956 -0.0018211855 -0.0018211679][-0.0018237438 -0.0018235088 -0.0018236112 -0.0018235324 -0.0018229956 -0.0018224026 -0.0018217213 -0.0018208402 -0.0018202954 -0.0018201207 -0.0018202905 -0.0018205254 -0.001820748 -0.0018208991 -0.0018209314][-0.0018231381 -0.0018228991 -0.0018230795 -0.0018229877 -0.0018224785 -0.001821997 -0.0018214178 -0.0018205565 -0.0018199862 -0.0018198097 -0.0018199766 -0.0018202369 -0.0018204569 -0.0018206261 -0.0018207364][-0.0018225708 -0.001822285 -0.0018224757 -0.0018224115 -0.0018220748 -0.001821726 -0.0018211666 -0.0018203611 -0.001819771 -0.0018195695 -0.0018196804 -0.0018199089 -0.0018201056 -0.0018202757 -0.0018204552][-0.0018217922 -0.0018214623 -0.0018216256 -0.0018216107 -0.0018215058 -0.0018213951 -0.001820973 -0.0018202643 -0.0018196366 -0.0018194086 -0.0018195078 -0.0018197031 -0.0018198618 -0.0018200007 -0.0018202409][-0.0018210054 -0.001820583 -0.0018207543 -0.0018208869 -0.0018209766 -0.0018210493 -0.0018208206 -0.0018202915 -0.001819652 -0.0018194165 -0.0018195625 -0.0018197768 -0.0018198546 -0.0018199328 -0.0018201688][-0.0018204267 -0.0018198539 -0.0018199525 -0.0018201905 -0.001820416 -0.001820643 -0.0018206289 -0.0018203408 -0.0018198427 -0.0018197048 -0.0018198922 -0.0018200598 -0.00182005 -0.0018200767 -0.0018202675]]...]
INFO - root - 2017-12-10 01:45:55.251789: step 77510, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.866 sec/batch; 61h:19m:37s remains)
INFO - root - 2017-12-10 01:46:03.893435: step 77520, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.866 sec/batch; 61h:22m:14s remains)
INFO - root - 2017-12-10 01:46:12.648001: step 77530, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.879 sec/batch; 62h:16m:12s remains)
INFO - root - 2017-12-10 01:46:21.318199: step 77540, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 61h:23m:02s remains)
INFO - root - 2017-12-10 01:46:29.972144: step 77550, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.877 sec/batch; 62h:05m:28s remains)
INFO - root - 2017-12-10 01:46:38.760102: step 77560, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.903 sec/batch; 63h:56m:50s remains)
INFO - root - 2017-12-10 01:46:47.500463: step 77570, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.847 sec/batch; 59h:57m:56s remains)
INFO - root - 2017-12-10 01:46:56.147225: step 77580, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.877 sec/batch; 62h:05m:56s remains)
INFO - root - 2017-12-10 01:47:04.786867: step 77590, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.878 sec/batch; 62h:11m:56s remains)
INFO - root - 2017-12-10 01:47:13.323869: step 77600, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.870 sec/batch; 61h:37m:40s remains)
2017-12-10 01:47:14.217465: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0018111243 -0.0018097112 -0.001809431 -0.0018094796 -0.0018095383 -0.0018098941 -0.0018103563 -0.0018109407 -0.0018114932 -0.0018121736 -0.0018126464 -0.0018127979 -0.0018127442 -0.0018126287 -0.0018124363][-0.0018100471 -0.0018087416 -0.0018084436 -0.0018083467 -0.00180823 -0.0018084852 -0.0018088362 -0.0018093054 -0.0018098858 -0.0018106523 -0.0018112038 -0.0018113808 -0.0018113417 -0.0018112509 -0.0018111743][-0.0018097138 -0.0018085802 -0.0018082241 -0.0018079978 -0.0018076542 -0.0018077453 -0.0018080223 -0.0018085083 -0.0018091645 -0.0018100226 -0.0018106446 -0.0018108798 -0.0018109104 -0.0018109189 -0.0018109739][-0.0018095496 -0.0018084324 -0.0018080517 -0.0018077294 -0.0018072802 -0.0018072296 -0.001807395 -0.001807826 -0.0018084086 -0.0018092102 -0.0018098683 -0.0018102375 -0.0018104434 -0.0018106764 -0.0018110005][-0.0018095233 -0.0018083621 -0.0018078795 -0.0018075098 -0.0018070288 -0.0018067951 -0.0018066926 -0.0018068572 -0.0018071827 -0.0018078246 -0.0018084815 -0.0018089589 -0.0018093985 -0.0018099244 -0.0018105573][-0.0018092544 -0.0018081994 -0.0018077597 -0.0018074065 -0.0018069533 -0.0018065327 -0.0018060472 -0.0018058313 -0.0018058744 -0.001806324 -0.001806968 -0.0018074918 -0.0018080922 -0.0018088291 -0.001809706][-0.0018094094 -0.0018083362 -0.0018079821 -0.001807639 -0.0018071441 -0.0018065645 -0.0018057422 -0.0018051307 -0.0018048539 -0.0018050999 -0.0018056437 -0.0018061171 -0.0018067901 -0.0018076975 -0.0018086547][-0.0018096716 -0.0018085566 -0.0018081358 -0.0018077681 -0.0018072772 -0.0018065918 -0.0018056541 -0.0018048696 -0.0018043077 -0.0018043512 -0.0018047456 -0.0018051524 -0.0018057802 -0.0018066644 -0.0018076053][-0.0018098378 -0.0018086469 -0.0018079662 -0.0018073641 -0.0018067829 -0.0018061211 -0.0018052965 -0.0018045724 -0.0018040331 -0.0018041119 -0.0018044481 -0.0018047745 -0.0018052156 -0.001805953 -0.0018067423][-0.0018100075 -0.0018088805 -0.0018080291 -0.0018070994 -0.0018063252 -0.001805616 -0.0018048515 -0.0018043349 -0.0018041359 -0.0018044654 -0.001804863 -0.0018051728 -0.0018054853 -0.0018060091 -0.0018065274][-0.0018111906 -0.0018100715 -0.0018090153 -0.0018078275 -0.0018068422 -0.0018059186 -0.001805124 -0.0018048211 -0.0018050182 -0.0018056791 -0.0018062379 -0.0018066163 -0.0018069262 -0.0018072388 -0.0018074324][-0.0018142031 -0.0018131142 -0.0018120189 -0.001810736 -0.0018095203 -0.0018083395 -0.0018074764 -0.0018072752 -0.0018077041 -0.0018085601 -0.0018092686 -0.0018097871 -0.0018100907 -0.0018100891 -0.0018097647][-0.0018180957 -0.0018169377 -0.0018159469 -0.0018147882 -0.0018136364 -0.0018124365 -0.0018116196 -0.0018115782 -0.0018121408 -0.0018129739 -0.0018136657 -0.0018141671 -0.0018143137 -0.0018139476 -0.0018130445][-0.0018201591 -0.0018191346 -0.0018184665 -0.0018176478 -0.0018167546 -0.0018159064 -0.0018155125 -0.0018158684 -0.0018167556 -0.0018178353 -0.0018188261 -0.0018195229 -0.0018196743 -0.0018190852 -0.0018177441][-0.0018201424 -0.0018192193 -0.0018188148 -0.0018184205 -0.0018180312 -0.0018178294 -0.001818086 -0.0018190114 -0.0018203288 -0.0018217526 -0.001823119 -0.0018241968 -0.001824672 -0.0018242118 -0.0018229916]]...]
INFO - root - 2017-12-10 01:47:22.706923: step 77610, loss = 0.82, batch loss = 0.69 (9.8 examples/sec; 0.813 sec/batch; 57h:33m:38s remains)
INFO - root - 2017-12-10 01:47:31.335605: step 77620, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.841 sec/batch; 59h:31m:56s remains)
INFO - root - 2017-12-10 01:47:39.685255: step 77630, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 61h:22m:55s remains)
INFO - root - 2017-12-10 01:47:48.236391: step 77640, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.841 sec/batch; 59h:32m:23s remains)
INFO - root - 2017-12-10 01:47:56.886390: step 77650, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 61h:31m:05s remains)
INFO - root - 2017-12-10 01:48:05.618491: step 77660, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.892 sec/batch; 63h:08m:49s remains)
INFO - root - 2017-12-10 01:48:14.336983: step 77670, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.891 sec/batch; 63h:02m:41s remains)
INFO - root - 2017-12-10 01:48:23.073345: step 77680, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.885 sec/batch; 62h:38m:45s remains)
INFO - root - 2017-12-10 01:48:31.711272: step 77690, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.854 sec/batch; 60h:28m:05s remains)
INFO - root - 2017-12-10 01:48:40.252493: step 77700, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.902 sec/batch; 63h:51m:27s remains)
2017-12-10 01:48:41.134539: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.039708339 0.049229134 0.05674696 0.061999418 0.063805588 0.062813826 0.059516475 0.05483653 0.049915534 0.045642182 0.042288162 0.039805315 0.038090359 0.03724793 0.03694357][0.039639253 0.050438732 0.059829582 0.067171134 0.0707066 0.071204 0.068417206 0.063559562 0.057896987 0.052552044 0.048147112 0.044912249 0.042710003 0.041851282 0.041607328][0.037022486 0.048423495 0.059021849 0.068211153 0.073777653 0.075925358 0.074268453 0.070025 0.064183749 0.058092251 0.052729219 0.048585746 0.045746364 0.044695795 0.044626556][0.032457598 0.04392061 0.055314809 0.065806553 0.073042437 0.076967165 0.076824039 0.073658451 0.068371147 0.062099494 0.056283556 0.051536165 0.04842722 0.047358118 0.047795758][0.026657449 0.037727013 0.049437664 0.060853429 0.069554724 0.075032 0.076776378 0.075330369 0.071154512 0.065366834 0.059315313 0.054270852 0.050985254 0.05002895 0.050892338][0.020664787 0.030714627 0.04212359 0.05395142 0.063925892 0.070805356 0.074101 0.074207604 0.071247637 0.06632562 0.060430843 0.05585942 0.052996956 0.052587155 0.054215457][0.015109773 0.023848947 0.034748781 0.046593357 0.05737029 0.06547758 0.070438154 0.071848504 0.06977199 0.065531678 0.059821654 0.05556576 0.052750681 0.052850645 0.055181194][0.010492119 0.017707163 0.027419876 0.038665071 0.049498219 0.058067266 0.064070739 0.066749536 0.065795675 0.06232778 0.056820706 0.053077884 0.050408594 0.050709777 0.053600851][0.0071708853 0.012820451 0.021074984 0.031069275 0.040958621 0.049323916 0.055570271 0.058865093 0.058854427 0.05636967 0.051347133 0.048049632 0.045663483 0.046564084 0.050218567][0.0051922072 0.009500159 0.016228642 0.024598835 0.033007909 0.04032626 0.045928255 0.049170755 0.049390141 0.047566183 0.043514796 0.041355874 0.040077258 0.041887265 0.046373326][0.0040205941 0.007247129 0.012624455 0.019378427 0.02624039 0.032199793 0.036640152 0.039092578 0.039200045 0.037772883 0.034586009 0.033650566 0.033752833 0.036761303 0.042115457][0.00302879 0.0052833627 0.0093454691 0.014561032 0.019756956 0.02432557 0.027686149 0.029401083 0.029329047 0.028211551 0.026249075 0.026313761 0.027647233 0.031656582 0.03757992][0.0018914436 0.0032720175 0.0060441974 0.0097434372 0.013352002 0.016487885 0.018671347 0.019826857 0.019700525 0.019003637 0.018290749 0.019652663 0.022593442 0.027420714 0.033696529][0.00090005517 0.001350805 0.0028949943 0.0051716752 0.0073782508 0.0092249243 0.010457533 0.01115276 0.011109183 0.011060565 0.011390883 0.013830693 0.017877923 0.023502206 0.029987372][0.0003432458 4.4986955e-05 0.00054562965 0.0015781847 0.0025957655 0.0034463285 0.00404949 0.0044191093 0.0044784583 0.00491118 0.0062980889 0.0096753221 0.014531351 0.020647677 0.027107906]]...]
INFO - root - 2017-12-10 01:48:49.841061: step 77710, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.895 sec/batch; 63h:20m:44s remains)
INFO - root - 2017-12-10 01:48:58.486734: step 77720, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 61h:25m:45s remains)
INFO - root - 2017-12-10 01:49:06.967441: step 77730, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.875 sec/batch; 61h:55m:49s remains)
INFO - root - 2017-12-10 01:49:15.553625: step 77740, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.858 sec/batch; 60h:41m:32s remains)
INFO - root - 2017-12-10 01:49:24.003724: step 77750, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.844 sec/batch; 59h:44m:39s remains)
INFO - root - 2017-12-10 01:49:32.572361: step 77760, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.837 sec/batch; 59h:12m:07s remains)
INFO - root - 2017-12-10 01:49:41.223080: step 77770, loss = 0.82, batch loss = 0.69 (8.8 examples/sec; 0.907 sec/batch; 64h:08m:54s remains)
INFO - root - 2017-12-10 01:49:49.959334: step 77780, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.847 sec/batch; 59h:54m:44s remains)
INFO - root - 2017-12-10 01:49:58.439375: step 77790, loss = 0.82, batch loss = 0.69 (9.8 examples/sec; 0.818 sec/batch; 57h:52m:29s remains)
INFO - root - 2017-12-10 01:50:06.996670: step 77800, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.886 sec/batch; 62h:39m:43s remains)
2017-12-10 01:50:07.917760: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.034087785 0.036352538 0.036771458 0.035831191 0.034611918 0.033388089 0.031827275 0.029390652 0.025610762 0.020465739 0.014532301 0.0088222567 0.0043004947 0.0013050368 -0.00043337606][0.048050061 0.050761782 0.050569881 0.048168309 0.045319341 0.04267602 0.040032633 0.036737636 0.032227557 0.02630195 0.019285928 0.012269061 0.0064621144 0.002509587 0.00016581][0.064087808 0.0665905 0.065034278 0.06041792 0.055161852 0.050243478 0.045869753 0.041476943 0.036475077 0.030295391 0.022829577 0.015034514 0.008275656 0.0035377182 0.000688763][0.08020331 0.08168871 0.078101769 0.0707884 0.062589318 0.054969486 0.048527777 0.042891096 0.037477784 0.031520296 0.024397986 0.01658893 0.009432504 0.0042194012 0.0010406537][0.093790531 0.093985133 0.088217027 0.078166686 0.067018457 0.056607287 0.048051607 0.041292582 0.035832256 0.030599743 0.024373764 0.017164012 0.010060328 0.0046415543 0.0012644428][0.10352179 0.10252813 0.094938427 0.082657516 0.069004372 0.056239054 0.045978602 0.038359437 0.032988008 0.028573435 0.023439925 0.017088277 0.010288636 0.0048408285 0.0013576016][0.10878894 0.10714614 0.098446265 0.084795415 0.06951265 0.055169847 0.04378818 0.035605982 0.030283052 0.026348928 0.021979196 0.016354777 0.0099458471 0.0046539847 0.0012239231][0.10890627 0.10734077 0.098509133 0.084539704 0.068722993 0.053813968 0.042062368 0.033670984 0.028231796 0.024285132 0.020132948 0.01489325 0.0088982414 0.0039742496 0.00081410247][0.10401618 0.10307606 0.094907142 0.081521757 0.066154696 0.051659815 0.040230025 0.031906292 0.02623385 0.021914981 0.017580817 0.012569507 0.0071553285 0.0028691678 0.00020910299][0.094563678 0.09462171 0.087846152 0.07590767 0.06196101 0.048704743 0.038160961 0.03013568 0.024205171 0.019342314 0.014683102 0.0098671978 0.0051451204 0.0016419986 -0.00041375321][0.0807246 0.081706651 0.076726995 0.066998482 0.055278674 0.043944664 0.03470213 0.027240856 0.021228738 0.016022585 0.011277539 0.0068887589 0.0030578836 0.00045114348 -0.0009640081][0.064002112 0.065462835 0.062111266 0.054801319 0.045688633 0.03665014 0.029012259 0.022471411 0.016850153 0.011890567 0.0075863567 0.0039713695 0.0011832536 -0.00052798819 -0.0013708684][0.046259005 0.047826607 0.045846518 0.04081367 0.034250263 0.027519675 0.021563031 0.016241217 0.011528277 0.0074274656 0.0040708147 0.0014967093 -0.00024789642 -0.0012015579 -0.0016193083][0.029908616 0.031260468 0.030270863 0.027165713 0.022859368 0.018261729 0.014010041 0.010113339 0.0066228169 0.0036605406 0.001387813 -0.0001811058 -0.001111253 -0.0015599297 -0.0017339018][0.016716821 0.017746817 0.017418049 0.015774507 0.013280676 0.010480587 0.0077905911 0.0052698553 0.002992887 0.001106481 -0.00025036454 -0.0010916727 -0.0015295735 -0.0017134101 -0.0017774166]]...]
INFO - root - 2017-12-10 01:50:16.600492: step 77810, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.886 sec/batch; 62h:40m:51s remains)
INFO - root - 2017-12-10 01:50:25.212675: step 77820, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.858 sec/batch; 60h:40m:51s remains)
INFO - root - 2017-12-10 01:50:33.707895: step 77830, loss = 0.82, batch loss = 0.69 (10.1 examples/sec; 0.790 sec/batch; 55h:54m:57s remains)
INFO - root - 2017-12-10 01:50:42.189282: step 77840, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.849 sec/batch; 60h:03m:28s remains)
INFO - root - 2017-12-10 01:50:50.754907: step 77850, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.857 sec/batch; 60h:36m:32s remains)
INFO - root - 2017-12-10 01:50:59.380728: step 77860, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.873 sec/batch; 61h:45m:37s remains)
INFO - root - 2017-12-10 01:51:08.214203: step 77870, loss = 0.82, batch loss = 0.69 (8.7 examples/sec; 0.924 sec/batch; 65h:21m:44s remains)
INFO - root - 2017-12-10 01:51:16.771423: step 77880, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.854 sec/batch; 60h:24m:23s remains)
INFO - root - 2017-12-10 01:51:25.285915: step 77890, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.879 sec/batch; 62h:09m:19s remains)
INFO - root - 2017-12-10 01:51:33.838416: step 77900, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.858 sec/batch; 60h:39m:47s remains)
2017-12-10 01:51:34.733568: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.35267392 0.34947556 0.34445882 0.33665672 0.32927769 0.32257405 0.31641281 0.31342539 0.31194735 0.31446198 0.31695232 0.32026848 0.32387421 0.32609868 0.32832155][0.35190114 0.35176837 0.34981057 0.34450883 0.33935103 0.33481294 0.32954171 0.32770157 0.32839677 0.33326486 0.3375853 0.34204775 0.34679559 0.34997311 0.35151291][0.34318495 0.34770432 0.35123187 0.35088226 0.34962115 0.34790525 0.34481221 0.3436597 0.34395975 0.34866902 0.353065 0.3572951 0.36182749 0.364616 0.36512509][0.33413446 0.34276888 0.35106894 0.35670516 0.36085749 0.36280361 0.363194 0.36338106 0.36274269 0.36515835 0.36719891 0.36966 0.37215391 0.37322268 0.37274194][0.32762134 0.3388682 0.35029709 0.36045083 0.36910272 0.37493014 0.37830997 0.38040781 0.37956473 0.37890536 0.3773568 0.37573734 0.37416175 0.37213326 0.36966559][0.32081732 0.33419651 0.34803006 0.36143076 0.373976 0.38338023 0.38987187 0.39373207 0.39281502 0.39011323 0.38494417 0.37847659 0.37224174 0.36630493 0.36130869][0.31573677 0.32919589 0.34263125 0.35777962 0.3725493 0.3844485 0.39303276 0.39853397 0.39781502 0.39350882 0.38562772 0.37603879 0.36594987 0.35651883 0.3495799][0.30985475 0.32290524 0.33489361 0.34920597 0.36403376 0.37735781 0.38702244 0.39318094 0.39275929 0.38753617 0.37855721 0.36737055 0.35565573 0.3440367 0.33563969][0.303671 0.31478962 0.32328159 0.33515653 0.34863839 0.36188364 0.3717823 0.37907186 0.38037613 0.3761524 0.36719877 0.35606679 0.34377903 0.33165127 0.32280037][0.29443875 0.30350339 0.30856287 0.31703958 0.32815713 0.34028158 0.34975788 0.35742044 0.36009359 0.35770196 0.3505269 0.34103402 0.32988602 0.31928194 0.31167993][0.28360617 0.29160482 0.29454231 0.30077606 0.30907938 0.31948209 0.32763442 0.33482888 0.33823678 0.33698812 0.33213273 0.32547784 0.31753713 0.31015828 0.30535284][0.27398157 0.28132826 0.28208321 0.28601071 0.29154798 0.29993778 0.30642459 0.31238028 0.31543228 0.31467184 0.31247976 0.30939138 0.30592331 0.30281371 0.30221832][0.26632553 0.27367613 0.27392483 0.27596474 0.27932784 0.28533545 0.29015157 0.29446283 0.2969757 0.29643935 0.29542416 0.29466897 0.29512548 0.29684535 0.30079821][0.26117042 0.26790968 0.26724809 0.2684356 0.27014193 0.274124 0.27793863 0.28157586 0.28395846 0.28383109 0.28441489 0.28595808 0.28926143 0.29501453 0.3030881][0.25627992 0.26293898 0.26199111 0.26230133 0.26278147 0.26576504 0.26867878 0.27151278 0.27349034 0.27367124 0.27534148 0.27882168 0.2849656 0.29391977 0.3053081]]...]
INFO - root - 2017-12-10 01:51:43.345852: step 77910, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.859 sec/batch; 60h:46m:45s remains)
INFO - root - 2017-12-10 01:51:51.886142: step 77920, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.848 sec/batch; 59h:58m:35s remains)
INFO - root - 2017-12-10 01:52:00.422343: step 77930, loss = 0.82, batch loss = 0.69 (10.5 examples/sec; 0.759 sec/batch; 53h:41m:44s remains)
INFO - root - 2017-12-10 01:52:09.012844: step 77940, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.859 sec/batch; 60h:44m:07s remains)
INFO - root - 2017-12-10 01:52:17.567585: step 77950, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.855 sec/batch; 60h:26m:28s remains)
INFO - root - 2017-12-10 01:52:26.267024: step 77960, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.877 sec/batch; 62h:01m:53s remains)
INFO - root - 2017-12-10 01:52:34.792436: step 77970, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.853 sec/batch; 60h:19m:25s remains)
INFO - root - 2017-12-10 01:52:43.372625: step 77980, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.860 sec/batch; 60h:47m:04s remains)
INFO - root - 2017-12-10 01:52:51.645457: step 77990, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.842 sec/batch; 59h:30m:17s remains)
INFO - root - 2017-12-10 01:53:00.067409: step 78000, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.839 sec/batch; 59h:18m:13s remains)
2017-12-10 01:53:00.976210: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0018318769 -0.0018317199 -0.0018318385 -0.0018321298 -0.0018323505 -0.0018324092 -0.0018322228 -0.0018318456 -0.0018313624 -0.0018310616 -0.001830967 -0.001831062 -0.0018308121 -0.0018298448 -0.0018286356][-0.0018321715 -0.0018322255 -0.0018326435 -0.0018331546 -0.0018334088 -0.0018332364 -0.0018327192 -0.0018321135 -0.0018317713 -0.001831686 -0.0018312847 -0.0018298032 -0.001825842 -0.0018194903 -0.0018133605][-0.0018324953 -0.0018323783 -0.0018325961 -0.0018324002 -0.0018314479 -0.0018296274 -0.0018275236 -0.0018266019 -0.0018272436 -0.0018280331 -0.0018266581 -0.0018200575 -0.0018058225 -0.001786995 -0.0017716498][-0.0018324467 -0.0018319056 -0.0018305354 -0.0018269535 -0.0018203388 -0.0018116535 -0.0018039771 -0.0018015928 -0.0018037141 -0.0018058462 -0.0018013096 -0.0017834967 -0.0017515634 -0.0017144533 -0.0016892171][-0.0018320648 -0.001830403 -0.0018254707 -0.0018135784 -0.0017933531 -0.001769687 -0.0017514019 -0.0017458837 -0.0017493756 -0.0017513877 -0.0017391128 -0.0017030785 -0.0016469239 -0.0015901063 -0.0015618962][-0.0018314599 -0.0018281499 -0.0018174931 -0.0017918311 -0.0017478287 -0.0016973718 -0.0016600726 -0.0016473869 -0.0016491635 -0.0016477281 -0.0016249102 -0.0015702096 -0.0014946809 -0.0014299726 -0.0014157444][-0.0018304964 -0.0018252257 -0.001808072 -0.0017669998 -0.0016962157 -0.0016142277 -0.0015517235 -0.0015249935 -0.001519613 -0.0015123182 -0.0014800589 -0.001412604 -0.0013307903 -0.0012776847 -0.0012957682][-0.0018292355 -0.0018227808 -0.0018008882 -0.0017486399 -0.00165902 -0.0015528831 -0.001464819 -0.0014167832 -0.0013979935 -0.0013841552 -0.0013505301 -0.0012856859 -0.0012162388 -0.0011905918 -0.0012487569][-0.0018278578 -0.0018212041 -0.001798289 -0.0017439948 -0.0016512752 -0.0015392834 -0.001441826 -0.0013819549 -0.0013561591 -0.0013444296 -0.001321153 -0.0012724402 -0.0012231492 -0.0012222119 -0.0013043843][-0.0018266734 -0.0018211319 -0.0018019945 -0.0017568794 -0.0016797107 -0.0015836672 -0.0014956682 -0.001439088 -0.0014196541 -0.0014219598 -0.0014184387 -0.0013911959 -0.0013603223 -0.0013688391 -0.0014447554][-0.001825524 -0.001821737 -0.0018095444 -0.0017801058 -0.00172828 -0.0016601152 -0.0015938955 -0.0015506453 -0.001543162 -0.0015609101 -0.0015773403 -0.0015726505 -0.0015590844 -0.0015673174 -0.0016154665][-0.0018246874 -0.001822532 -0.0018167894 -0.0018027442 -0.0017765922 -0.0017388768 -0.0016991541 -0.0016731916 -0.0016735788 -0.0016941841 -0.0017157806 -0.0017231691 -0.0017214788 -0.0017262796 -0.0017464729][-0.0018242524 -0.0018231034 -0.0018214124 -0.0018168218 -0.0018069929 -0.0017909509 -0.0017724595 -0.0017600571 -0.0017615759 -0.0017752725 -0.0017908622 -0.0017997135 -0.0018026771 -0.0018049873 -0.0018098783][-0.0018242768 -0.0018233602 -0.0018230225 -0.0018224772 -0.0018205397 -0.0018161006 -0.0018101484 -0.0018059564 -0.0018064375 -0.0018116619 -0.0018183335 -0.001823232 -0.0018257992 -0.001826628 -0.0018267754][-0.0018250583 -0.0018241388 -0.0018239748 -0.0018242395 -0.0018243784 -0.0018238705 -0.0018228194 -0.0018217206 -0.0018214702 -0.0018226245 -0.001824268 -0.0018255856 -0.0018263632 -0.001826575 -0.001826606]]...]
INFO - root - 2017-12-10 01:53:09.659848: step 78010, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.892 sec/batch; 63h:01m:45s remains)
INFO - root - 2017-12-10 01:53:18.303005: step 78020, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.857 sec/batch; 60h:36m:13s remains)
INFO - root - 2017-12-10 01:53:26.873107: step 78030, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.879 sec/batch; 62h:09m:42s remains)
INFO - root - 2017-12-10 01:53:35.403073: step 78040, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 61h:21m:04s remains)
INFO - root - 2017-12-10 01:53:44.113564: step 78050, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 61h:26m:57s remains)
INFO - root - 2017-12-10 01:53:52.713312: step 78060, loss = 0.83, batch loss = 0.70 (9.3 examples/sec; 0.857 sec/batch; 60h:34m:59s remains)
INFO - root - 2017-12-10 01:54:01.507053: step 78070, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.857 sec/batch; 60h:33m:32s remains)
INFO - root - 2017-12-10 01:54:10.164031: step 78080, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.856 sec/batch; 60h:29m:40s remains)
INFO - root - 2017-12-10 01:54:18.896116: step 78090, loss = 0.82, batch loss = 0.69 (10.4 examples/sec; 0.768 sec/batch; 54h:15m:42s remains)
INFO - root - 2017-12-10 01:54:27.507257: step 78100, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.891 sec/batch; 62h:57m:13s remains)
2017-12-10 01:54:28.418701: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0018308994 -0.0018297714 -0.001829332 -0.0018289768 -0.0018286972 -0.0018284861 -0.0018283521 -0.0018283674 -0.0018284679 -0.0018287414 -0.0018292745 -0.0018300842 -0.0018312458 -0.0018323723 -0.0018331894][-0.0018318758 -0.0018308256 -0.0018303617 -0.0018299177 -0.001829552 -0.0018290835 -0.0018287981 -0.0018287257 -0.001828805 -0.0018292618 -0.0018300634 -0.0018310059 -0.0018318591 -0.0018324665 -0.001832905][-0.0018323234 -0.0018315333 -0.0018313826 -0.0018311756 -0.0018309446 -0.0018302848 -0.0018296503 -0.0018292904 -0.0018293994 -0.0018300691 -0.0018310794 -0.0018320222 -0.0018324931 -0.0018327138 -0.001832997][-0.001831571 -0.0018317555 -0.0018319885 -0.0018322292 -0.0018322578 -0.0018316869 -0.0018306035 -0.0018297426 -0.0018297561 -0.0018303647 -0.0018312854 -0.0018320824 -0.0018323468 -0.0018324072 -0.0018329851][-0.0018297514 -0.0018313563 -0.0018323637 -0.0018330383 -0.0018334254 -0.0018328369 -0.001831143 -0.0018294927 -0.0018292535 -0.0018296539 -0.0018303473 -0.0018308822 -0.0018310352 -0.0018314794 -0.0018326798][-0.0018273502 -0.0018305376 -0.0018325202 -0.0018338077 -0.0018344986 -0.0018336797 -0.0018310942 -0.0018283299 -0.0018275748 -0.0018276402 -0.0018278295 -0.0018280984 -0.0018282175 -0.0018293025 -0.0018312474][-0.0018246822 -0.0018291097 -0.001831873 -0.0018338512 -0.0018349997 -0.0018339925 -0.0018305308 -0.0018270822 -0.0018256403 -0.0018250835 -0.0018242863 -0.0018242989 -0.0018246297 -0.0018262125 -0.0018288482][-0.0018224232 -0.0018272965 -0.0018307423 -0.0018335592 -0.0018352177 -0.0018340577 -0.001830423 -0.00182676 -0.0018243169 -0.0018227774 -0.0018210018 -0.0018209077 -0.0018213958 -0.001823456 -0.0018263776][-0.0018212424 -0.0018260181 -0.001829877 -0.001833127 -0.0018351659 -0.001834459 -0.001831584 -0.0018282662 -0.0018249095 -0.0018224685 -0.001819987 -0.001819663 -0.0018202673 -0.0018224694 -0.0018252749][-0.0018222706 -0.0018263133 -0.0018300297 -0.0018334511 -0.0018356966 -0.0018355898 -0.0018337036 -0.0018311084 -0.0018276157 -0.0018247324 -0.0018216638 -0.0018213693 -0.0018219838 -0.0018242169 -0.001826808][-0.0018255661 -0.0018286953 -0.0018319794 -0.001834879 -0.0018370419 -0.0018373592 -0.0018363203 -0.0018345613 -0.0018314923 -0.001828578 -0.0018256881 -0.0018251154 -0.0018255138 -0.0018272202 -0.0018293419][-0.0018297561 -0.0018318919 -0.0018345461 -0.0018368231 -0.001838884 -0.001839531 -0.0018389346 -0.0018375973 -0.0018351477 -0.0018326894 -0.0018304497 -0.0018299319 -0.0018303775 -0.0018315747 -0.0018330949][-0.0018335157 -0.0018349013 -0.0018369734 -0.0018390275 -0.0018409255 -0.0018417026 -0.0018412485 -0.0018400622 -0.001838261 -0.0018365945 -0.0018350732 -0.0018347411 -0.0018351815 -0.0018360278 -0.0018369657][-0.001836609 -0.0018371973 -0.0018385357 -0.0018401691 -0.0018419579 -0.0018429077 -0.0018429243 -0.001842336 -0.0018413219 -0.0018402803 -0.0018393341 -0.0018391655 -0.001839446 -0.0018398573 -0.0018403757][-0.0018389274 -0.001839065 -0.0018397206 -0.0018407609 -0.0018420733 -0.0018430297 -0.0018435759 -0.0018436316 -0.0018432515 -0.0018426591 -0.0018420838 -0.0018420817 -0.0018422395 -0.0018424408 -0.0018427575]]...]
INFO - root - 2017-12-10 01:54:37.033994: step 78110, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.838 sec/batch; 59h:14m:19s remains)
INFO - root - 2017-12-10 01:54:45.913205: step 78120, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.891 sec/batch; 62h:57m:13s remains)
INFO - root - 2017-12-10 01:54:54.624105: step 78130, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.874 sec/batch; 61h:47m:25s remains)
INFO - root - 2017-12-10 01:55:03.033705: step 78140, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.866 sec/batch; 61h:10m:36s remains)
INFO - root - 2017-12-10 01:55:11.520556: step 78150, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.856 sec/batch; 60h:30m:26s remains)
INFO - root - 2017-12-10 01:55:20.162323: step 78160, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.850 sec/batch; 60h:05m:09s remains)
INFO - root - 2017-12-10 01:55:28.859019: step 78170, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.856 sec/batch; 60h:27m:36s remains)
INFO - root - 2017-12-10 01:55:37.651729: step 78180, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.896 sec/batch; 63h:17m:33s remains)
INFO - root - 2017-12-10 01:55:46.280993: step 78190, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.855 sec/batch; 60h:21m:49s remains)
INFO - root - 2017-12-10 01:55:54.376020: step 78200, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.875 sec/batch; 61h:49m:18s remains)
2017-12-10 01:55:55.249917: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.001828493 -0.001829779 -0.0018307685 -0.0018306965 -0.0018293407 -0.0018270441 -0.0018244976 -0.0018217642 -0.0018188868 -0.001815908 -0.00181305 -0.0018105295 -0.001808572 -0.0018074232 -0.0018072039][-0.0018304948 -0.0018323225 -0.0018335811 -0.001833295 -0.0018314732 -0.0018286187 -0.001825552 -0.0018222934 -0.0018190558 -0.0018158851 -0.0018128022 -0.001810052 -0.0018078682 -0.0018065162 -0.0018060692][-0.001832336 -0.0018346448 -0.0018359498 -0.0018353033 -0.001832931 -0.0018295541 -0.0018259955 -0.001822358 -0.0018190055 -0.0018159179 -0.0018128841 -0.0018101196 -0.0018078457 -0.0018063575 -0.0018057004][-0.001833241 -0.0018358871 -0.0018370514 -0.0018359568 -0.0018330321 -0.0018291122 -0.0018250201 -0.0018210832 -0.0018178507 -0.0018151238 -0.0018124714 -0.0018099542 -0.0018078118 -0.001806315 -0.0018055302][-0.0018325176 -0.0018352169 -0.0018361837 -0.0018346994 -0.001831303 -0.0018269754 -0.0018224724 -0.0018184364 -0.0018154655 -0.0018132417 -0.0018111827 -0.0018092003 -0.0018074318 -0.0018061218 -0.0018053552][-0.0018301746 -0.0018326581 -0.0018334984 -0.0018318755 -0.0018283052 -0.0018238582 -0.001819327 -0.0018153789 -0.0018126795 -0.0018109272 -0.0018095035 -0.0018081338 -0.0018068305 -0.0018058019 -0.0018051531][-0.0018267265 -0.0018288321 -0.0018295302 -0.0018280026 -0.0018245996 -0.0018203775 -0.0018161179 -0.0018124785 -0.0018101273 -0.0018087934 -0.0018079266 -0.0018071164 -0.0018062675 -0.0018055291 -0.0018050259][-0.0018224585 -0.0018240436 -0.001824617 -0.0018233417 -0.0018203756 -0.0018166882 -0.0018130365 -0.0018099912 -0.0018080743 -0.0018071244 -0.0018066971 -0.0018063529 -0.0018058842 -0.0018053711 -0.0018049853][-0.0018179643 -0.0018189407 -0.0018194731 -0.0018185135 -0.0018161677 -0.0018132009 -0.0018103096 -0.0018079462 -0.0018064588 -0.0018058029 -0.0018056685 -0.0018056624 -0.0018054901 -0.0018051667 -0.0018048795][-0.0018136255 -0.0018139592 -0.0018144327 -0.00181385 -0.0018122558 -0.001810173 -0.0018081828 -0.0018064971 -0.0018053675 -0.0018049278 -0.001804952 -0.0018051141 -0.0018051048 -0.001804896 -0.001804682][-0.0018101943 -0.0018100154 -0.0018103179 -0.0018100183 -0.0018091099 -0.0018078497 -0.0018066319 -0.001805512 -0.0018047094 -0.001804432 -0.001804518 -0.0018047399 -0.0018048111 -0.0018046759 -0.0018045271][-0.0018079464 -0.0018074581 -0.0018076489 -0.0018075076 -0.0018070854 -0.0018064337 -0.0018057119 -0.0018049838 -0.0018044153 -0.0018042445 -0.0018043348 -0.0018045383 -0.0018046277 -0.0018045404 -0.0018044378][-0.0018067305 -0.001806139 -0.0018062689 -0.0018062154 -0.0018060748 -0.0018057585 -0.0018053009 -0.0018047959 -0.0018043786 -0.0018042547 -0.0018043064 -0.001804455 -0.001804534 -0.0018044845 -0.0018044204][-0.0018062076 -0.0018055458 -0.0018056083 -0.0018056132 -0.0018056026 -0.0018054598 -0.0018051603 -0.0018047879 -0.0018044746 -0.001804363 -0.0018043646 -0.0018044385 -0.0018044861 -0.0018044624 -0.0018044292][-0.0018061234 -0.0018054027 -0.0018053966 -0.0018054224 -0.0018054417 -0.0018053681 -0.0018051693 -0.0018049005 -0.0018046731 -0.0018045624 -0.0018045116 -0.0018045104 -0.0018045163 -0.0018045042 -0.0018044942]]...]
INFO - root - 2017-12-10 01:56:03.963934: step 78210, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 61h:19m:11s remains)
INFO - root - 2017-12-10 01:56:12.625473: step 78220, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.874 sec/batch; 61h:42m:15s remains)
INFO - root - 2017-12-10 01:56:21.309801: step 78230, loss = 0.83, batch loss = 0.70 (9.3 examples/sec; 0.860 sec/batch; 60h:46m:30s remains)
INFO - root - 2017-12-10 01:56:29.775827: step 78240, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.852 sec/batch; 60h:11m:22s remains)
INFO - root - 2017-12-10 01:56:38.456482: step 78250, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.898 sec/batch; 63h:26m:42s remains)
INFO - root - 2017-12-10 01:56:47.061215: step 78260, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.886 sec/batch; 62h:36m:08s remains)
INFO - root - 2017-12-10 01:56:55.757480: step 78270, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.887 sec/batch; 62h:37m:58s remains)
INFO - root - 2017-12-10 01:57:04.377992: step 78280, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.848 sec/batch; 59h:51m:06s remains)
INFO - root - 2017-12-10 01:57:12.850388: step 78290, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.863 sec/batch; 60h:55m:42s remains)
INFO - root - 2017-12-10 01:57:21.203925: step 78300, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.866 sec/batch; 61h:07m:13s remains)
2017-12-10 01:57:22.069580: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.0019494632 0.0017938066 0.0021673036 0.0033418695 0.0056838826 0.0088929208 0.012246052 0.014310162 0.014666355 0.013281087 0.010813284 0.0083721867 0.0064862804 0.0054759225 0.0046140878][0.003805778 0.0038819471 0.004452317 0.0060113962 0.009075501 0.013384333 0.017896665 0.02101735 0.021812413 0.01985063 0.016077505 0.01189279 0.0083579849 0.0060125194 0.0045202337][0.0038874554 0.0046334113 0.0062741884 0.0092865983 0.014336777 0.0210194 0.027714327 0.032200325 0.033082504 0.030161846 0.02453592 0.017931936 0.012103113 0.0079239216 0.0050747055][0.0042540221 0.0060861227 0.0098141572 0.015936345 0.025187934 0.036713596 0.04763576 0.054382563 0.05507683 0.049947146 0.040671427 0.030026428 0.020652285 0.013646188 0.00860253][0.0074462513 0.010929089 0.017329127 0.02752452 0.042065471 0.059429958 0.075637631 0.085816517 0.0870635 0.079525806 0.065864563 0.05019182 0.0362663 0.025460446 0.01744994][0.012511464 0.019551352 0.030948717 0.047111973 0.06785509 0.090740986 0.11092838 0.12271448 0.1231024 0.1126662 0.0950332 0.075236835 0.057367161 0.042920027 0.031519819][0.015983384 0.026706327 0.043623034 0.0666346 0.0943593 0.12299819 0.14663407 0.15915114 0.15807164 0.14494111 0.12442047 0.1020724 0.081784762 0.06491936 0.0507543][0.019042265 0.03217189 0.052865427 0.080692783 0.11314831 0.14531302 0.17068934 0.18330145 0.18154711 0.16769415 0.14701293 0.12515441 0.10496628 0.087539025 0.071720049][0.019868368 0.033928741 0.055995129 0.085563518 0.11950283 0.15243213 0.1779124 0.19060305 0.18947539 0.17733431 0.15930337 0.14028004 0.12237585 0.1064232 0.091023125][0.018303331 0.031758774 0.052800111 0.080799662 0.11254336 0.14290422 0.16627686 0.17838265 0.17891563 0.17070819 0.15804549 0.14466284 0.13181838 0.11966109 0.10676853][0.013895418 0.025190564 0.043029368 0.066853218 0.093780093 0.11936035 0.1392758 0.15028663 0.15266912 0.14904855 0.14278662 0.13638368 0.13020909 0.12392496 0.11586494][0.0086638909 0.016831551 0.029944982 0.047573127 0.067585066 0.086752295 0.10208396 0.11150948 0.11573568 0.11677938 0.11698847 0.11777007 0.11879246 0.11897361 0.11664333][0.0043889587 0.0094899014 0.017781246 0.029083626 0.041958358 0.054423336 0.0647764 0.072089083 0.07720504 0.081804067 0.087425321 0.094409645 0.10165224 0.10747262 0.11004625][0.0015158238 0.004355683 0.0089210328 0.015163991 0.022284396 0.029226933 0.035229746 0.040367015 0.045655396 0.052402556 0.061655641 0.073078655 0.084854461 0.094671279 0.10057383][-7.7464734e-05 0.001411694 0.0037549492 0.0068183686 0.010265852 0.01366902 0.016923621 0.020646812 0.026062686 0.034373958 0.045923341 0.059877682 0.073849067 0.085332252 0.092355691]]...]
INFO - root - 2017-12-10 01:57:30.697120: step 78310, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.884 sec/batch; 62h:24m:55s remains)
INFO - root - 2017-12-10 01:57:39.306897: step 78320, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 60h:58m:23s remains)
INFO - root - 2017-12-10 01:57:48.307810: step 78330, loss = 0.82, batch loss = 0.69 (8.0 examples/sec; 0.999 sec/batch; 70h:34m:01s remains)
INFO - root - 2017-12-10 01:57:56.718624: step 78340, loss = 0.83, batch loss = 0.70 (9.2 examples/sec; 0.868 sec/batch; 61h:14m:54s remains)
INFO - root - 2017-12-10 01:58:05.342592: step 78350, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.881 sec/batch; 62h:13m:49s remains)
INFO - root - 2017-12-10 01:58:13.992505: step 78360, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.870 sec/batch; 61h:26m:06s remains)
INFO - root - 2017-12-10 01:58:22.604444: step 78370, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.857 sec/batch; 60h:27m:47s remains)
INFO - root - 2017-12-10 01:58:31.200241: step 78380, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.854 sec/batch; 60h:16m:11s remains)
INFO - root - 2017-12-10 01:58:39.845126: step 78390, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.843 sec/batch; 59h:28m:53s remains)
INFO - root - 2017-12-10 01:58:47.927179: step 78400, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.891 sec/batch; 62h:54m:13s remains)
2017-12-10 01:58:48.838922: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.10568014 0.10425841 0.10236139 0.10013048 0.0978724 0.095547743 0.093769446 0.092796482 0.092369124 0.092808694 0.094181538 0.096606269 0.099525742 0.10271811 0.10581514][0.10855036 0.1076787 0.1063 0.10447533 0.10250199 0.10026255 0.098125145 0.096649781 0.0956377 0.095580004 0.096649073 0.098870151 0.10177259 0.10489582 0.10795779][0.10930118 0.1090138 0.10832179 0.10733224 0.10612885 0.10440153 0.10234672 0.10042635 0.098771051 0.098009065 0.098326854 0.099811517 0.10216025 0.10478052 0.10738727][0.10844411 0.10884772 0.10907117 0.10911637 0.10911893 0.10839991 0.10696638 0.10504846 0.10287106 0.10110737 0.10015299 0.10023804 0.10136809 0.10285632 0.10461993][0.10593048 0.10697556 0.10816538 0.10959809 0.11121497 0.11199086 0.11156754 0.11009146 0.10757194 0.10471451 0.10207758 0.10022423 0.099538513 0.09940286 0.10001391][0.10299317 0.10463233 0.10677359 0.10964343 0.11306123 0.11576754 0.11683422 0.11605912 0.1133861 0.10943031 0.10486968 0.10049534 0.097309269 0.094946876 0.093874343][0.10083968 0.10274297 0.10541898 0.10944054 0.1144857 0.11899189 0.12173289 0.12194394 0.11943226 0.11457863 0.10801575 0.10094585 0.0947078 0.08985009 0.086780511][0.099348582 0.10154322 0.10452043 0.10946522 0.11574829 0.12172848 0.12585048 0.12698089 0.12480735 0.11935311 0.11125225 0.10164212 0.092425689 0.084822156 0.079484381][0.097088538 0.099725634 0.10291088 0.10826008 0.11538946 0.12258787 0.12788954 0.13009582 0.12855427 0.12315332 0.11408079 0.10258336 0.090822853 0.080661766 0.073095173][0.095397234 0.0981975 0.10121429 0.10673374 0.1141808 0.12185096 0.12780167 0.13073692 0.12972106 0.12436952 0.1146979 0.10215153 0.088839382 0.076987758 0.067788251][0.094132833 0.0969196 0.099400595 0.10437629 0.11141697 0.11918951 0.12560529 0.12916261 0.12874946 0.12370741 0.11402466 0.10102134 0.086949781 0.074089512 0.063771233][0.092617385 0.095588677 0.097548053 0.10168403 0.10780763 0.11496537 0.12113062 0.1249347 0.12500013 0.12049922 0.11129321 0.098563656 0.084536761 0.071438432 0.060624141][0.09139248 0.094210774 0.095507607 0.098624744 0.10357083 0.10960659 0.11502726 0.11867075 0.11889689 0.11496314 0.10670169 0.0950416 0.081985995 0.069282711 0.05855456][0.089222595 0.09172491 0.092306383 0.09418577 0.097731732 0.10256842 0.10734236 0.11076585 0.11126639 0.10806657 0.10084686 0.09056779 0.078847669 0.067220271 0.057168711][0.088056065 0.09021166 0.090075977 0.090947211 0.093134083 0.096601859 0.10032234 0.10309292 0.10340089 0.10049541 0.094262257 0.085362054 0.075254127 0.0650384 0.056072064]]...]
INFO - root - 2017-12-10 01:58:57.502865: step 78410, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.876 sec/batch; 61h:48m:32s remains)
INFO - root - 2017-12-10 01:59:06.095540: step 78420, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.836 sec/batch; 58h:59m:19s remains)
INFO - root - 2017-12-10 01:59:14.643446: step 78430, loss = 0.82, batch loss = 0.69 (9.7 examples/sec; 0.823 sec/batch; 58h:05m:09s remains)
INFO - root - 2017-12-10 01:59:23.093250: step 78440, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.879 sec/batch; 62h:01m:38s remains)
INFO - root - 2017-12-10 01:59:31.536688: step 78450, loss = 0.82, batch loss = 0.69 (9.7 examples/sec; 0.825 sec/batch; 58h:13m:56s remains)
INFO - root - 2017-12-10 01:59:40.191920: step 78460, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.884 sec/batch; 62h:21m:32s remains)
INFO - root - 2017-12-10 01:59:48.862572: step 78470, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.890 sec/batch; 62h:49m:46s remains)
INFO - root - 2017-12-10 01:59:57.510023: step 78480, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.857 sec/batch; 60h:26m:54s remains)
INFO - root - 2017-12-10 02:00:06.185060: step 78490, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.849 sec/batch; 59h:55m:02s remains)
INFO - root - 2017-12-10 02:00:14.624514: step 78500, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.898 sec/batch; 63h:20m:00s remains)
2017-12-10 02:00:15.532975: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.001818574 -0.001819861 -0.0018222006 -0.0018238091 -0.0018244055 -0.0018241259 -0.0018232045 -0.0018223546 -0.0018220944 -0.0018221501 -0.0018224621 -0.0018231428 -0.0018238419 -0.001824197 -0.0018238797][-0.0018204286 -0.0018214118 -0.0018226639 -0.0018223586 -0.0018203752 -0.0018168234 -0.0018124114 -0.0018089841 -0.0018081251 -0.0018093218 -0.0018119543 -0.0018157858 -0.0018195959 -0.001822527 -0.0018238905][-0.0018223267 -0.001822289 -0.001820021 -0.0018132632 -0.0018022209 -0.0017871327 -0.0017705004 -0.0017576754 -0.0017535231 -0.001757379 -0.0017677624 -0.0017825699 -0.0017977476 -0.0018101957 -0.0018180967][-0.0018221493 -0.0018176301 -0.0018042407 -0.001778578 -0.0017412915 -0.0016940776 -0.001645313 -0.0016089638 -0.001597513 -0.0016112219 -0.0016445743 -0.0016903441 -0.0017367997 -0.0017747823 -0.0017999164][-0.0018182427 -0.001801683 -0.0017613069 -0.0016930717 -0.0016007759 -0.0014913511 -0.0013839833 -0.0013088203 -0.0012902927 -0.0013294709 -0.0014107542 -0.0015154949 -0.0016204566 -0.0017057271 -0.001762656][-0.0018083073 -0.0017686283 -0.0016796505 -0.0015393974 -0.001364436 -0.0011731246 -0.00099742063 -0.00088747212 -0.00087553484 -0.00095835124 -0.0011053409 -0.0012848161 -0.0014624414 -0.0016073147 -0.0017065564][-0.001795651 -0.0017303617 -0.0015920266 -0.0013859643 -0.0011469729 -0.00090706127 -0.00070281525 -0.00059111405 -0.00060096267 -0.00072147965 -0.00091055559 -0.0011311051 -0.0013489262 -0.0015296006 -0.0016571388][-0.0017876178 -0.001706993 -0.0015458892 -0.0013198724 -0.0010773245 -0.00085677567 -0.00068834738 -0.00061460491 -0.00065121625 -0.00077938044 -0.00095989642 -0.0011622256 -0.0013616786 -0.0015294934 -0.0016513007][-0.0017920132 -0.001727963 -0.001607025 -0.0014441762 -0.0012811542 -0.0011475345 -0.0010584312 -0.0010322416 -0.001071685 -0.0011621321 -0.0012788347 -0.0014044817 -0.0015269351 -0.0016305672 -0.0017069796][-0.0018017325 -0.0017675912 -0.001706448 -0.0016270746 -0.0015535182 -0.0015006075 -0.0014726883 -0.0014732319 -0.0014994555 -0.0015427975 -0.0015926261 -0.0016439359 -0.0016932271 -0.0017344764 -0.0017652251][-0.0018095307 -0.0017994164 -0.0017841805 -0.0017654098 -0.0017498164 -0.0017412021 -0.0017391247 -0.0017430772 -0.0017516225 -0.0017622132 -0.0017718927 -0.0017802247 -0.0017876631 -0.0017935382 -0.0017980156][-0.00181079 -0.0018075075 -0.0018051259 -0.0018030768 -0.0018020307 -0.0018021192 -0.0018031111 -0.0018044541 -0.0018055975 -0.0018060951 -0.0018059141 -0.0018054995 -0.0018050699 -0.0018047031 -0.0018045348][-0.0018100945 -0.0018079316 -0.0018070916 -0.0018067955 -0.0018066353 -0.0018066894 -0.0018068942 -0.0018071766 -0.0018074043 -0.0018073912 -0.0018071933 -0.0018070164 -0.0018067932 -0.0018063807 -0.0018059976][-0.0018093803 -0.00180754 -0.0018068257 -0.0018067524 -0.0018067523 -0.0018068535 -0.0018069403 -0.0018070696 -0.0018071955 -0.0018071525 -0.001807019 -0.0018068793 -0.0018067494 -0.0018066135 -0.0018065027][-0.0018092019 -0.0018073666 -0.001806507 -0.0018063296 -0.001806312 -0.0018064578 -0.0018065473 -0.0018066142 -0.001806734 -0.0018067092 -0.0018066149 -0.001806514 -0.0018064366 -0.0018063795 -0.001806338]]...]
INFO - root - 2017-12-10 02:00:24.165973: step 78510, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.858 sec/batch; 60h:30m:10s remains)
INFO - root - 2017-12-10 02:00:32.797361: step 78520, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.858 sec/batch; 60h:30m:20s remains)
INFO - root - 2017-12-10 02:00:41.577160: step 78530, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.858 sec/batch; 60h:30m:01s remains)
INFO - root - 2017-12-10 02:00:50.009780: step 78540, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.861 sec/batch; 60h:43m:53s remains)
INFO - root - 2017-12-10 02:00:58.527851: step 78550, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.866 sec/batch; 61h:03m:43s remains)
INFO - root - 2017-12-10 02:01:07.314337: step 78560, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.874 sec/batch; 61h:40m:48s remains)
INFO - root - 2017-12-10 02:01:15.956199: step 78570, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.863 sec/batch; 60h:52m:18s remains)
INFO - root - 2017-12-10 02:01:24.975163: step 78580, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.898 sec/batch; 63h:19m:09s remains)
INFO - root - 2017-12-10 02:01:33.759237: step 78590, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.877 sec/batch; 61h:50m:52s remains)
INFO - root - 2017-12-10 02:01:42.177391: step 78600, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.853 sec/batch; 60h:08m:04s remains)
2017-12-10 02:01:43.112231: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0018218781 -0.0018206269 -0.0018200567 -0.0018212023 -0.0018222443 -0.0018235034 -0.0018250025 -0.0018261803 -0.0018272399 -0.0018274079 -0.0018270452 -0.0018270348 -0.0018274238 -0.0018279813 -0.0018282265][-0.0017992738 -0.0017964796 -0.0017952003 -0.0017973052 -0.0018013518 -0.0018071694 -0.0018139703 -0.0018207345 -0.0018257637 -0.0018286717 -0.0018296443 -0.0018297791 -0.001829232 -0.0018281346 -0.0018255637][-0.0017669986 -0.0017645333 -0.0017634621 -0.0017642804 -0.0017687053 -0.0017786527 -0.0017926752 -0.0018073203 -0.0018189765 -0.0018273372 -0.0018309373 -0.0018312191 -0.0018294082 -0.0018257552 -0.0018192165][-0.0017171931 -0.0017014156 -0.0016911922 -0.0016829304 -0.0016810461 -0.0016914924 -0.0017168773 -0.00175148 -0.0017859787 -0.001811684 -0.0018252109 -0.0018290083 -0.0018260063 -0.001818237 -0.0018047307][-0.0015647141 -0.0015243901 -0.0015071606 -0.0014987703 -0.0015032114 -0.0015271041 -0.0015778735 -0.0016474568 -0.0017195055 -0.0017761093 -0.0018100117 -0.0018220907 -0.0018169663 -0.0018000398 -0.0017680222][-0.0013456203 -0.0012845942 -0.0012772413 -0.0012850168 -0.001309572 -0.001360893 -0.0014417336 -0.0015442581 -0.0016497911 -0.0017350429 -0.0017882232 -0.0018082282 -0.0017993842 -0.0017657755 -0.0016999061][-0.001183913 -0.001131373 -0.001164114 -0.0012058972 -0.0012569012 -0.0013258683 -0.001411939 -0.0015136267 -0.0016171581 -0.0017057372 -0.0017663777 -0.0017905748 -0.0017779912 -0.0017287582 -0.0016327904][-0.0012267157 -0.0012135345 -0.0012810179 -0.0013463874 -0.0014059017 -0.0014660994 -0.0015268457 -0.0015933549 -0.0016594437 -0.0017196697 -0.0017635481 -0.0017801974 -0.001765925 -0.0017161689 -0.001620322][-0.0014330752 -0.0014582409 -0.0015215523 -0.001571585 -0.0016086195 -0.0016411531 -0.0016700524 -0.0017001206 -0.0017292686 -0.0017573212 -0.0017784223 -0.0017850866 -0.0017733633 -0.001738757 -0.0016738989][-0.0016580804 -0.0016890175 -0.0017257867 -0.0017498042 -0.0017635203 -0.0017736831 -0.0017810549 -0.0017878015 -0.0017937372 -0.0017992357 -0.0018027652 -0.0018019994 -0.0017959686 -0.001781921 -0.0017561815][-0.0017851951 -0.0017945556 -0.0018041602 -0.0018086255 -0.0018099358 -0.001810874 -0.0018114874 -0.0018120043 -0.0018124352 -0.0018127885 -0.0018126835 -0.0018117802 -0.0018097444 -0.0018057778 -0.0017997589][-0.0018151945 -0.0018136192 -0.0018136282 -0.0018137754 -0.0018136572 -0.0018132889 -0.0018129769 -0.001813114 -0.0018133398 -0.0018138093 -0.0018141217 -0.0018143128 -0.0018143638 -0.0018139124 -0.0018133058][-0.0018184888 -0.0018167519 -0.0018161972 -0.0018157947 -0.0018154166 -0.0018149085 -0.0018142973 -0.0018136778 -0.0018131291 -0.0018131133 -0.0018134471 -0.0018138634 -0.0018141986 -0.0018142634 -0.0018141717][-0.0018210885 -0.0018196424 -0.0018190652 -0.0018186359 -0.0018180325 -0.00181741 -0.001816757 -0.0018161053 -0.0018156394 -0.0018151514 -0.0018149783 -0.0018146713 -0.0018144134 -0.0018140768 -0.0018137443][-0.0018222247 -0.001821031 -0.0018205367 -0.0018203601 -0.0018201659 -0.0018198541 -0.0018193969 -0.0018188078 -0.0018181127 -0.0018173724 -0.0018168518 -0.0018162385 -0.0018156273 -0.0018148576 -0.0018140643]]...]
INFO - root - 2017-12-10 02:01:51.706805: step 78610, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.863 sec/batch; 60h:52m:54s remains)
INFO - root - 2017-12-10 02:02:00.194974: step 78620, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 61h:12m:24s remains)
INFO - root - 2017-12-10 02:02:08.820284: step 78630, loss = 0.81, batch loss = 0.68 (9.5 examples/sec; 0.845 sec/batch; 59h:37m:10s remains)
INFO - root - 2017-12-10 02:02:17.300096: step 78640, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.852 sec/batch; 60h:04m:45s remains)
INFO - root - 2017-12-10 02:02:25.946699: step 78650, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.855 sec/batch; 60h:15m:40s remains)
INFO - root - 2017-12-10 02:02:34.447909: step 78660, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 61h:26m:54s remains)
INFO - root - 2017-12-10 02:02:43.019993: step 78670, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 61h:11m:35s remains)
INFO - root - 2017-12-10 02:02:51.672479: step 78680, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 61h:14m:45s remains)
INFO - root - 2017-12-10 02:03:00.359943: step 78690, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.835 sec/batch; 58h:53m:06s remains)
INFO - root - 2017-12-10 02:03:08.948364: step 78700, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.895 sec/batch; 63h:07m:14s remains)
2017-12-10 02:03:09.779184: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.0014080055 0.0024009836 0.0042374539 0.0069797686 0.011969492 0.018601125 0.025502898 0.030984558 0.034657266 0.036024764 0.034847382 0.032651294 0.030009594 0.029121291 0.029444888][0.0014046916 0.0021545538 0.0040924214 0.0073995632 0.013419914 0.021148141 0.028826982 0.034620989 0.038357407 0.039224941 0.037788488 0.035455193 0.033015124 0.032484975 0.03317403][0.001719047 0.0022963728 0.0042454791 0.0080047343 0.014762838 0.023326578 0.031485021 0.037548203 0.041238345 0.0418575 0.040172648 0.037393678 0.034961335 0.034748383 0.035580061][0.0021744771 0.0025614761 0.0046533961 0.008858881 0.016222078 0.025072787 0.033501912 0.039610252 0.043200225 0.04361508 0.041728649 0.038721956 0.036318317 0.036023371 0.036786094][0.0026043956 0.0029380806 0.0051358668 0.0098696845 0.017617505 0.026687957 0.034982868 0.041071709 0.044229414 0.04445938 0.042413726 0.039228659 0.0365846 0.0362268 0.036909617][0.0027456908 0.0032110056 0.0056598242 0.010819712 0.018914042 0.028018171 0.035922281 0.041667886 0.044142991 0.044164207 0.041826624 0.038689245 0.035810083 0.035144564 0.035685021][0.0024878164 0.0031653438 0.0058085229 0.011478309 0.019653512 0.028774386 0.036201779 0.041438196 0.043263275 0.0427239 0.039977372 0.036770497 0.033667255 0.032857109 0.033220854][0.0020697038 0.0027945363 0.0055197952 0.011493383 0.019716088 0.028705314 0.035647593 0.040287185 0.041267451 0.040240768 0.037110828 0.03371064 0.030472502 0.029433556 0.029882628][0.0014464859 0.0021976135 0.0048779366 0.010733907 0.018847333 0.027651414 0.034241337 0.038338847 0.038626481 0.036872949 0.033325985 0.029746924 0.026381662 0.025104687 0.025652491][0.00088981178 0.0015866536 0.0040704221 0.0093700867 0.016983066 0.025449594 0.03193422 0.035806108 0.035705872 0.033414692 0.02943594 0.025542 0.021972036 0.020464018 0.021027151][0.00050663704 0.0010668811 0.0031566233 0.0077194576 0.014558584 0.02235401 0.028790234 0.032752268 0.032735806 0.030018214 0.025583887 0.021288326 0.017469801 0.015802881 0.016450398][0.00017651345 0.00062784704 0.0022197631 0.0057814843 0.011580086 0.018695796 0.025114637 0.02937085 0.029625569 0.02684688 0.022260893 0.017532425 0.013547613 0.011577737 0.012173372][-0.00010238006 0.00022902561 0.0013120797 0.0037684096 0.0083153881 0.014601317 0.020975037 0.025603143 0.026230948 0.023678564 0.019203395 0.014449441 0.010338069 0.0081450529 0.0085582761][-0.00045125932 -0.00017196289 0.00049173238 0.0019162506 0.0051015834 0.010325496 0.016442165 0.021391889 0.022532029 0.020529443 0.016445994 0.012054075 0.0082300156 0.0058272877 0.0059171543][-0.00079924939 -0.00056728 -0.00015417812 0.00052043761 0.0025205896 0.0065289489 0.011947022 0.016988622 0.018648891 0.017359849 0.014037794 0.010377407 0.0069110557 0.0045016333 0.0042570494]]...]
INFO - root - 2017-12-10 02:03:18.356616: step 78710, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 60h:56m:21s remains)
INFO - root - 2017-12-10 02:03:27.007900: step 78720, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.850 sec/batch; 59h:54m:43s remains)
INFO - root - 2017-12-10 02:03:35.632063: step 78730, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.891 sec/batch; 62h:49m:03s remains)
INFO - root - 2017-12-10 02:03:44.067118: step 78740, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.834 sec/batch; 58h:45m:34s remains)
INFO - root - 2017-12-10 02:03:52.653107: step 78750, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.842 sec/batch; 59h:20m:23s remains)
INFO - root - 2017-12-10 02:04:01.256800: step 78760, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.879 sec/batch; 61h:56m:36s remains)
INFO - root - 2017-12-10 02:04:09.980911: step 78770, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.859 sec/batch; 60h:30m:55s remains)
INFO - root - 2017-12-10 02:04:18.766774: step 78780, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.875 sec/batch; 61h:40m:44s remains)
INFO - root - 2017-12-10 02:04:27.458683: step 78790, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.842 sec/batch; 59h:19m:30s remains)
INFO - root - 2017-12-10 02:04:36.173807: step 78800, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.853 sec/batch; 60h:06m:46s remains)
2017-12-10 02:04:37.095039: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.17078713 0.16691691 0.15974468 0.15139176 0.14351757 0.13664909 0.13086873 0.12706119 0.12487146 0.12321695 0.12105174 0.11758292 0.11251445 0.10620883 0.099017777][0.17603463 0.17381914 0.16836098 0.16160508 0.15503405 0.14898717 0.14355907 0.13964577 0.13681585 0.1347679 0.13273299 0.13023865 0.12671654 0.12219582 0.116685][0.17794101 0.17736238 0.17364281 0.16821818 0.16261177 0.15726726 0.15184829 0.14741789 0.14401597 0.14156884 0.13977763 0.13838966 0.13692176 0.13447104 0.13082418][0.18050039 0.18147424 0.17939302 0.1751281 0.17020427 0.16480859 0.1588112 0.15339217 0.14848073 0.14496572 0.14316155 0.14258301 0.14284718 0.14223951 0.14059263][0.18125053 0.18352647 0.18265726 0.17962715 0.17564155 0.17002219 0.16314931 0.15644783 0.15003566 0.14533177 0.14257289 0.14266324 0.14430265 0.14544475 0.14566459][0.1792195 0.18225831 0.18198961 0.17967878 0.17617428 0.17098317 0.1640543 0.15644558 0.14872935 0.14262849 0.13905674 0.13912156 0.14128199 0.14371711 0.14556994][0.17585561 0.17947769 0.17969123 0.17809676 0.17491408 0.16993864 0.16286375 0.15462498 0.14609395 0.13914445 0.13507785 0.13504118 0.13726407 0.14021923 0.14266384][0.17306788 0.17680186 0.1768567 0.17518055 0.17177792 0.16681345 0.15953498 0.15108244 0.14208598 0.13484703 0.13067649 0.13029209 0.13230701 0.13469262 0.13682516][0.17018941 0.17440994 0.17426732 0.17283757 0.16953218 0.16480219 0.15780553 0.14956592 0.14079368 0.13338658 0.12888381 0.12770976 0.12875187 0.1301966 0.13117182][0.16492204 0.16953383 0.16969176 0.16888118 0.16634752 0.16238731 0.15623458 0.14884315 0.1408449 0.13401985 0.12930629 0.12710945 0.12660365 0.12657702 0.12601788][0.15859988 0.16338536 0.16394429 0.16369422 0.16210298 0.1595736 0.15509224 0.14951943 0.14296377 0.1368688 0.13213915 0.12879957 0.12643871 0.12430119 0.12176992][0.15080644 0.15642177 0.15783356 0.15850446 0.15781522 0.15603773 0.15286522 0.14956637 0.14495414 0.14028549 0.13601121 0.13202292 0.12811977 0.12370146 0.11896644][0.14136176 0.14755578 0.14977382 0.15139809 0.15215923 0.15164471 0.14976738 0.14824827 0.14539717 0.14215791 0.13826784 0.13404591 0.12923726 0.12328114 0.11704763][0.13122724 0.13821894 0.1413364 0.14406608 0.146026 0.1466088 0.14604351 0.14575616 0.1444089 0.14238206 0.13925155 0.13513196 0.12984851 0.12330947 0.11613602][0.12198839 0.12929595 0.13286796 0.13615751 0.13896035 0.14030431 0.14078099 0.14157559 0.14147459 0.14065132 0.1384232 0.13484767 0.12959994 0.12268426 0.11486676]]...]
INFO - root - 2017-12-10 02:04:45.507171: step 78810, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 61h:20m:54s remains)
INFO - root - 2017-12-10 02:04:54.185743: step 78820, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.854 sec/batch; 60h:10m:13s remains)
INFO - root - 2017-12-10 02:05:02.928976: step 78830, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 61h:24m:16s remains)
INFO - root - 2017-12-10 02:05:11.401450: step 78840, loss = 0.82, batch loss = 0.69 (10.0 examples/sec; 0.802 sec/batch; 56h:30m:59s remains)
INFO - root - 2017-12-10 02:05:20.047539: step 78850, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.874 sec/batch; 61h:34m:59s remains)
INFO - root - 2017-12-10 02:05:28.705846: step 78860, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.850 sec/batch; 59h:51m:20s remains)
INFO - root - 2017-12-10 02:05:37.188359: step 78870, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.854 sec/batch; 60h:09m:42s remains)
INFO - root - 2017-12-10 02:05:45.712232: step 78880, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.842 sec/batch; 59h:18m:16s remains)
INFO - root - 2017-12-10 02:05:54.384267: step 78890, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.882 sec/batch; 62h:08m:16s remains)
INFO - root - 2017-12-10 02:06:02.818690: step 78900, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.886 sec/batch; 62h:24m:01s remains)
2017-12-10 02:06:03.691912: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.070932791 0.070913732 0.070082359 0.06883958 0.0672684 0.065370761 0.063343838 0.061402403 0.0596843 0.057730723 0.0555341 0.053558514 0.051928159 0.050943166 0.050471134][0.072002918 0.071647346 0.070837945 0.069642246 0.068178289 0.066510871 0.064782016 0.06315 0.061356843 0.059491254 0.057488963 0.055625338 0.054049883 0.052893277 0.052361835][0.069538608 0.069004007 0.068113089 0.0671337 0.065951027 0.064537 0.063032128 0.061570428 0.059944361 0.058105588 0.056147914 0.05443719 0.052890413 0.05166389 0.050986864][0.065952361 0.0653378 0.064411774 0.0634886 0.062406272 0.061290622 0.060085338 0.058843721 0.057536766 0.056044694 0.054295581 0.052517481 0.050866846 0.049573742 0.048498653][0.062077649 0.06158755 0.060609419 0.05968437 0.058625147 0.057597063 0.056509234 0.055556122 0.054587629 0.053455539 0.051929265 0.050194308 0.048360631 0.046763513 0.045365475][0.058364376 0.058220148 0.057345621 0.056300122 0.05516424 0.054162256 0.053272221 0.052652203 0.052118663 0.051406872 0.050051555 0.048247006 0.046167523 0.044232491 0.042389337][0.054043628 0.054294828 0.053317722 0.052099954 0.050830662 0.049820915 0.049093891 0.048743486 0.048668314 0.048477698 0.047534216 0.045900784 0.043778788 0.041526295 0.03932875][0.048967257 0.049342792 0.048244305 0.0468874 0.045534123 0.044548292 0.04400805 0.044070479 0.044523526 0.044883225 0.044565514 0.0433139 0.041281786 0.038833361 0.036426105][0.042061467 0.042559374 0.041367095 0.040030882 0.038802065 0.038009439 0.037807886 0.038291369 0.039255984 0.04017872 0.040537316 0.039950579 0.038260043 0.035817351 0.033275262][0.035195339 0.0357901 0.034661762 0.033511918 0.0324967 0.032055184 0.03229567 0.033251468 0.034640048 0.036053058 0.03693923 0.036799107 0.035365205 0.033116564 0.030726116][0.029192943 0.029933184 0.029058887 0.028293058 0.027650151 0.027498847 0.028009228 0.029320907 0.03100327 0.032640453 0.033714533 0.033815835 0.032620687 0.03058343 0.028403169][0.02524285 0.026371436 0.025946593 0.025580641 0.025287373 0.025318643 0.025830772 0.027081313 0.028707255 0.030330189 0.031539891 0.031850971 0.031002231 0.02916782 0.027156597][0.023674145 0.025092723 0.024987176 0.02489667 0.024853759 0.024976579 0.025438029 0.026396804 0.027679611 0.028895335 0.02983997 0.03010742 0.029347021 0.027731717 0.026020352][0.023340125 0.02484606 0.02481823 0.024879379 0.024920767 0.025058605 0.025457529 0.026298502 0.027409406 0.0282561 0.028884543 0.028966745 0.028233625 0.026806856 0.025353296][0.023470266 0.024914457 0.024755338 0.024756121 0.024711926 0.024707217 0.024884626 0.025461977 0.026265815 0.026916662 0.027376903 0.027203361 0.026591439 0.02559163 0.024752494]]...]
INFO - root - 2017-12-10 02:06:12.127172: step 78910, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.865 sec/batch; 60h:55m:36s remains)
INFO - root - 2017-12-10 02:06:20.797264: step 78920, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.872 sec/batch; 61h:27m:00s remains)
INFO - root - 2017-12-10 02:06:29.449444: step 78930, loss = 0.83, batch loss = 0.70 (9.5 examples/sec; 0.845 sec/batch; 59h:31m:48s remains)
INFO - root - 2017-12-10 02:06:37.904286: step 78940, loss = 0.82, batch loss = 0.69 (11.4 examples/sec; 0.701 sec/batch; 49h:23m:12s remains)
INFO - root - 2017-12-10 02:06:46.332382: step 78950, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.854 sec/batch; 60h:09m:55s remains)
INFO - root - 2017-12-10 02:06:54.996531: step 78960, loss = 0.82, batch loss = 0.69 (8.2 examples/sec; 0.970 sec/batch; 68h:17m:55s remains)
INFO - root - 2017-12-10 02:07:03.731791: step 78970, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.895 sec/batch; 63h:01m:45s remains)
INFO - root - 2017-12-10 02:07:12.438381: step 78980, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.863 sec/batch; 60h:44m:44s remains)
INFO - root - 2017-12-10 02:07:21.065310: step 78990, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 61h:19m:40s remains)
INFO - root - 2017-12-10 02:07:29.479894: step 79000, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.879 sec/batch; 61h:54m:26s remains)
2017-12-10 02:07:30.415532: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.006809386 0.0069744443 0.0069754622 0.0071239094 0.0072486359 0.0071490672 0.0066272072 0.0056514307 0.0043055941 0.0027266243 0.0011094516 -0.00025433395 -0.0011754842 -0.0016342965 -0.001791314][0.011177301 0.011420821 0.011299555 0.011330463 0.011250026 0.010898817 0.0099260611 0.0084096 0.0063955034 0.0041794316 0.0020297416 0.00027747161 -0.00090369367 -0.001529705 -0.0017627104][0.016694773 0.016914431 0.016508542 0.016167017 0.015599047 0.014729814 0.013107254 0.010963217 0.0082661025 0.0054740389 0.0028438042 0.0007435804 -0.000673515 -0.0014475536 -0.0017378833][0.022525214 0.022631092 0.021882722 0.021084502 0.019902389 0.018364886 0.015958516 0.013067555 0.009636756 0.0063055181 0.0033192285 0.0010261043 -0.00051100843 -0.0013820188 -0.0017199839][0.027221702 0.02709711 0.026053768 0.024835521 0.023105437 0.020979151 0.01791377 0.014383144 0.010325155 0.0065524275 0.0033135246 0.00095076824 -0.00056891411 -0.0014036014 -0.0017203558][0.030034836 0.029593095 0.028274916 0.026743306 0.024646258 0.022113023 0.018635841 0.014687051 0.010230732 0.0061952691 0.0028637941 0.00057665945 -0.00079948781 -0.0014971355 -0.0017433155][0.030626621 0.029948074 0.028392432 0.026643328 0.024343345 0.021632686 0.018040193 0.013950524 0.0093992967 0.005323838 0.0020766668 -1.0817195e-05 -0.0011413686 -0.0016323541 -0.001776922][0.029313121 0.028530777 0.026771976 0.024905898 0.022603516 0.019927599 0.01647146 0.012564145 0.0082650809 0.0043956363 0.0013673933 -0.00051142951 -0.0014286912 -0.0017400873 -0.0018022927][0.026361117 0.025630275 0.023767142 0.021891145 0.019773614 0.017362943 0.014305084 0.010814465 0.0070013716 0.0035570436 0.00084227219 -0.00083569554 -0.0016040481 -0.0017973359 -0.0018136614][0.022242777 0.021706602 0.019776104 0.01801019 0.016223751 0.014263247 0.011816015 0.0089455834 0.0058108144 0.0028970973 0.00052137219 -0.000975745 -0.0016643412 -0.0018077538 -0.0018137839][0.017856658 0.017393481 0.015464251 0.013839214 0.012384125 0.010919191 0.0091198906 0.0069468296 0.004549752 0.0022374298 0.00025864353 -0.0010508192 -0.0016790512 -0.0018075627 -0.0018136671][0.0135036 0.013014941 0.011221783 0.0098265633 0.0087334467 0.007744649 0.0065393928 0.0050202357 0.0033060042 0.0015785181 4.2996835e-06 -0.0011122565 -0.001678907 -0.00180872 -0.0018154004][0.0094284751 0.00889129 0.0073952931 0.0063304505 0.005625505 0.0050537288 0.0043335645 0.0033305767 0.002168891 0.00093646033 -0.00027878629 -0.0011980529 -0.0016868457 -0.0018117464 -0.0018190925][0.0055914493 0.0051087542 0.0040418892 0.0033670436 0.003019955 0.0027747885 0.0024172007 0.0018156503 0.0010849916 0.00025313406 -0.00063600962 -0.0013380174 -0.0017158196 -0.0018134704 -0.0018198749][0.0023719305 0.0019822745 0.0013698029 0.0010580077 0.00098478643 0.00095565605 0.00083612732 0.00052379246 0.00011153787 -0.0004104391 -0.001006433 -0.0014853624 -0.0017460341 -0.001813999 -0.0018190031]]...]
INFO - root - 2017-12-10 02:07:38.928241: step 79010, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.876 sec/batch; 61h:42m:54s remains)
INFO - root - 2017-12-10 02:07:47.640993: step 79020, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 61h:10m:30s remains)
INFO - root - 2017-12-10 02:07:56.357260: step 79030, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 61h:11m:09s remains)
INFO - root - 2017-12-10 02:08:05.032282: step 79040, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.883 sec/batch; 62h:11m:42s remains)
INFO - root - 2017-12-10 02:08:13.448228: step 79050, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.844 sec/batch; 59h:26m:48s remains)
INFO - root - 2017-12-10 02:08:21.990945: step 79060, loss = 0.81, batch loss = 0.68 (9.2 examples/sec; 0.868 sec/batch; 61h:06m:08s remains)
INFO - root - 2017-12-10 02:08:30.494175: step 79070, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.850 sec/batch; 59h:51m:46s remains)
INFO - root - 2017-12-10 02:08:39.152779: step 79080, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.876 sec/batch; 61h:39m:24s remains)
INFO - root - 2017-12-10 02:08:47.913057: step 79090, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.848 sec/batch; 59h:40m:42s remains)
INFO - root - 2017-12-10 02:08:56.456217: step 79100, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.881 sec/batch; 62h:00m:11s remains)
2017-12-10 02:08:57.297195: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.0046071755 0.0083290152 0.013839892 0.020690717 0.028047178 0.035244044 0.041529216 0.046076845 0.048022054 0.046796348 0.042416427 0.03533015 0.026571939 0.017629616 0.0099349692][0.0057473951 0.01099023 0.018542197 0.027857449 0.037958127 0.047653411 0.055611815 0.060909368 0.062952854 0.061219759 0.055617969 0.046829894 0.03604177 0.024870772 0.014880654][0.0061956025 0.01199932 0.02040091 0.030661039 0.041868206 0.052784491 0.062059641 0.068407834 0.070776381 0.0687478 0.062415026 0.052633639 0.040563282 0.028156962 0.01719141][0.0056955991 0.011777234 0.020423274 0.030897327 0.042109411 0.052903589 0.062238179 0.068664081 0.071246125 0.069452487 0.063229829 0.053437617 0.041252811 0.028680492 0.017472975][0.0039722458 0.0097216992 0.017802786 0.027600439 0.038212132 0.048260663 0.056699894 0.062293433 0.06429413 0.062274143 0.056383234 0.0474023 0.036392216 0.025146935 0.015187046][0.0020440449 0.0065960991 0.013459084 0.022044158 0.0314017 0.040471319 0.048146773 0.052893519 0.054109525 0.051706273 0.046082966 0.038030159 0.028584907 0.019271582 0.011281039][0.00061309373 0.0036458112 0.0084842434 0.014880977 0.022294901 0.029769465 0.036158912 0.040058438 0.040808581 0.038390856 0.0333643 0.026613221 0.019143829 0.012208947 0.0066028549][-0.00056968757 0.0010790575 0.0040282472 0.0081455121 0.013201742 0.018623494 0.023495309 0.026519129 0.027046613 0.025175475 0.021346098 0.016298128 0.010958159 0.0063264049 0.0028346162][-0.0013113436 -0.00060413766 0.00072806061 0.0027524028 0.00546621 0.0085528446 0.011421179 0.013281369 0.013697801 0.01263932 0.010365761 0.0073786168 0.0043358286 0.0018572347 0.00012003013][-0.0017220504 -0.0014934507 -0.0010769904 -0.00042341242 0.00054242637 0.0017541872 0.0029229717 0.0036791735 0.0038683657 0.0034666169 0.0025543994 0.0013388199 0.00015072839 -0.00074967835 -0.0013186102][-0.0018065679 -0.0017844522 -0.0017331606 -0.0016229858 -0.0014338674 -0.0011621449 -0.00087417976 -0.000663009 -0.00059006037 -0.00066866749 -0.00087234064 -0.0011572789 -0.0014340105 -0.0016326422 -0.0017430322][-0.0018127612 -0.0018091701 -0.0018061181 -0.0018039457 -0.001800817 -0.0017974072 -0.0017959839 -0.0017949642 -0.0017962862 -0.0017959111 -0.0017967334 -0.0017994294 -0.0018020116 -0.0018046102 -0.0018061029][-0.0018110652 -0.0018098396 -0.0018091175 -0.0018084665 -0.0018072019 -0.001804378 -0.0018025516 -0.0017999494 -0.001800446 -0.0018012924 -0.0018026328 -0.0018047412 -0.0018067815 -0.0018087126 -0.0018089833][-0.0018106253 -0.0018094551 -0.0018088531 -0.0018085506 -0.0018080818 -0.001806649 -0.0018050255 -0.0018025512 -0.0018030494 -0.0018035612 -0.0018039476 -0.001805305 -0.0018073531 -0.0018093449 -0.0018096543][-0.0018128034 -0.0018120513 -0.0018106956 -0.001809869 -0.0018089827 -0.0018081275 -0.00180732 -0.0018053402 -0.0018054859 -0.0018055069 -0.0018060223 -0.0018064078 -0.0018069663 -0.0018085425 -0.0018089938]]...]
INFO - root - 2017-12-10 02:09:05.828228: step 79110, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.884 sec/batch; 62h:14m:35s remains)
INFO - root - 2017-12-10 02:09:14.337225: step 79120, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.888 sec/batch; 62h:30m:36s remains)
INFO - root - 2017-12-10 02:09:23.003427: step 79130, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 61h:08m:41s remains)
INFO - root - 2017-12-10 02:09:31.597495: step 79140, loss = 0.83, batch loss = 0.70 (9.3 examples/sec; 0.857 sec/batch; 60h:19m:11s remains)
INFO - root - 2017-12-10 02:09:40.028139: step 79150, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.847 sec/batch; 59h:35m:20s remains)
INFO - root - 2017-12-10 02:09:48.545638: step 79160, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.878 sec/batch; 61h:47m:32s remains)
INFO - root - 2017-12-10 02:09:57.061758: step 79170, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.858 sec/batch; 60h:21m:04s remains)
INFO - root - 2017-12-10 02:10:05.630444: step 79180, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.858 sec/batch; 60h:23m:39s remains)
INFO - root - 2017-12-10 02:10:14.275366: step 79190, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.850 sec/batch; 59h:49m:13s remains)
INFO - root - 2017-12-10 02:10:22.798867: step 79200, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.894 sec/batch; 62h:54m:01s remains)
2017-12-10 02:10:23.688786: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.00020654057 -5.4751756e-05 5.8449572e-05 0.00018568162 0.00028441392 0.00032275857 0.00028382184 0.00018486369 5.6909397e-05 -5.1400391e-05 -9.9453726e-05 -6.7090034e-05 4.5059249e-05 0.0001713495 0.00025646377][5.5365963e-06 0.00017259491 0.00023424497 0.00029290945 0.00031846145 0.00028866983 0.00019254477 5.1351846e-05 -9.1997441e-05 -0.00018377742 -0.00019377784 -0.00010967185 5.6437915e-05 0.00026727968 0.0004459779][4.3522683e-05 0.0002057465 0.00020107126 0.00017346104 0.00010272989 -1.7489656e-05 -0.00018266647 -0.000356892 -0.00049516174 -0.00055183668 -0.00050171081 -0.00033906929 -8.9021632e-05 0.00020147406 0.00044976536][2.5094952e-05 0.00017071434 9.84784e-05 -9.2765549e-06 -0.00017340202 -0.00038870785 -0.0006304892 -0.00084374764 -0.00097355619 -0.000980434 -0.00085969327 -0.00062148354 -0.00029815524 6.499188e-05 0.00039092812][-7.9732854e-05 3.8201571e-05 -8.4818923e-05 -0.00024519628 -0.00047489046 -0.00076115341 -0.0010572826 -0.0012876152 -0.0013994707 -0.0013718118 -0.0012188567 -0.00095957355 -0.00061286497 -0.00021875789 0.00015919644][-0.00029565522 -0.00024560851 -0.00040067197 -0.00058730878 -0.000840893 -0.0011421279 -0.0014276664 -0.0016178687 -0.0016836767 -0.0016391641 -0.0015078103 -0.0012994469 -0.0010114483 -0.00066188048 -0.000309287][-0.00073208951 -0.00074050867 -0.00087662076 -0.0010322013 -0.001234178 -0.0014591413 -0.0016478054 -0.0017524516 -0.001780154 -0.0017486714 -0.0016703672 -0.0015489252 -0.0013808563 -0.0011657167 -0.00091849588][-0.0012817545 -0.0013073763 -0.0013894967 -0.0014765513 -0.0015831712 -0.0016913218 -0.0017678961 -0.0018011072 -0.0018045574 -0.001792489 -0.0017628424 -0.0017126385 -0.0016423329 -0.0015536338 -0.0014456401][-0.0016747653 -0.0016850492 -0.0017097183 -0.0017338601 -0.00176255 -0.0017879579 -0.0018031856 -0.0018077379 -0.0018082476 -0.0018070856 -0.001802376 -0.0017924098 -0.0017770185 -0.0017584988 -0.0017354244][-0.0018066751 -0.0018065727 -0.0018072891 -0.0018071512 -0.0018067448 -0.001806724 -0.0018066408 -0.0018066822 -0.0018067317 -0.0018064729 -0.0018063522 -0.0018068061 -0.0018069178 -0.0018064115 -0.001805598][-0.0018097061 -0.0018070517 -0.0018067262 -0.0018061673 -0.0018053291 -0.0018049715 -0.0018049376 -0.0018052497 -0.0018056791 -0.0018057526 -0.0018059314 -0.0018063964 -0.0018065727 -0.0018062735 -0.0018056284][-0.0018093397 -0.0018071013 -0.0018068178 -0.0018060351 -0.0018053276 -0.0018048388 -0.0018047016 -0.0018049724 -0.0018053281 -0.0018055312 -0.0018056554 -0.0018059695 -0.001806083 -0.00180596 -0.0018055218][-0.0018093489 -0.0018076648 -0.001807305 -0.0018067184 -0.0018060097 -0.0018054724 -0.0018053385 -0.0018053353 -0.0018054162 -0.0018054905 -0.0018053899 -0.0018055839 -0.0018056805 -0.0018058204 -0.0018056976][-0.0018094074 -0.0018080488 -0.0018075521 -0.0018068455 -0.001805783 -0.0018051794 -0.001804833 -0.0018045654 -0.0018048132 -0.0018049434 -0.001804973 -0.001805021 -0.0018049509 -0.0018052617 -0.0018053916][-0.001809445 -0.00180822 -0.0018075687 -0.0018067493 -0.0018059523 -0.0018056709 -0.0018053491 -0.001804942 -0.0018048716 -0.0018048293 -0.0018050458 -0.0018049588 -0.0018049347 -0.0018051484 -0.0018051832]]...]
INFO - root - 2017-12-10 02:10:32.111838: step 79210, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.866 sec/batch; 60h:55m:23s remains)
INFO - root - 2017-12-10 02:10:40.689300: step 79220, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.853 sec/batch; 60h:00m:39s remains)
INFO - root - 2017-12-10 02:10:49.266139: step 79230, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.843 sec/batch; 59h:18m:26s remains)
INFO - root - 2017-12-10 02:10:57.867825: step 79240, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.865 sec/batch; 60h:50m:40s remains)
INFO - root - 2017-12-10 02:11:06.413936: step 79250, loss = 0.82, batch loss = 0.69 (8.8 examples/sec; 0.908 sec/batch; 63h:51m:56s remains)
INFO - root - 2017-12-10 02:11:15.056441: step 79260, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.874 sec/batch; 61h:29m:31s remains)
INFO - root - 2017-12-10 02:11:23.788532: step 79270, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.870 sec/batch; 61h:10m:19s remains)
INFO - root - 2017-12-10 02:11:32.504203: step 79280, loss = 0.81, batch loss = 0.68 (9.1 examples/sec; 0.878 sec/batch; 61h:44m:13s remains)
INFO - root - 2017-12-10 02:11:41.129087: step 79290, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.854 sec/batch; 60h:03m:47s remains)
INFO - root - 2017-12-10 02:11:49.724389: step 79300, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.872 sec/batch; 61h:19m:57s remains)
2017-12-10 02:11:50.691159: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0017979916 -0.0017960066 -0.00179573 -0.0017957896 -0.0017958691 -0.001795946 -0.001795991 -0.001795999 -0.0017959444 -0.0017958523 -0.001795751 -0.0017956904 -0.0017957032 -0.0017957619 -0.0017958368][-0.0017968457 -0.0017948475 -0.0017946197 -0.0017947218 -0.0017948168 -0.0017948954 -0.0017949427 -0.0017949356 -0.0017948837 -0.0017947879 -0.0017946808 -0.0017946098 -0.0017946155 -0.0017946387 -0.0017946769][-0.0017968644 -0.0017948273 -0.0017945823 -0.0017946836 -0.0017947631 -0.0017948438 -0.0017949317 -0.0017950152 -0.0017950248 -0.0017949067 -0.0017947421 -0.0017946193 -0.0017945576 -0.0017945103 -0.0017944719][-0.0017967207 -0.0017945588 -0.0017942098 -0.0017942569 -0.0017943478 -0.0017945345 -0.0017948017 -0.0017950659 -0.0017951773 -0.0017950434 -0.0017948137 -0.0017946019 -0.0017944837 -0.0017943855 -0.0017943216][-0.0017963842 -0.0017939307 -0.0017933507 -0.001793323 -0.0017934591 -0.0017937585 -0.0017942035 -0.0017946317 -0.0017948112 -0.0017946193 -0.0017943123 -0.0017940943 -0.0017940386 -0.0017940005 -0.0017940227][-0.0017959019 -0.0017932527 -0.0017924708 -0.0017923755 -0.0017925123 -0.0017927785 -0.0017931464 -0.0017934723 -0.0017935452 -0.0017932284 -0.0017929148 -0.0017928929 -0.0017931259 -0.0017933889 -0.0017936536][-0.0017952918 -0.0017925117 -0.0017915526 -0.0017913964 -0.0017915684 -0.0017917742 -0.0017919469 -0.0017920283 -0.0017920095 -0.0017916675 -0.0017914653 -0.0017917703 -0.0017924474 -0.0017930893 -0.0017935886][-0.001794696 -0.0017919029 -0.0017908765 -0.0017906985 -0.0017908702 -0.0017909934 -0.001790971 -0.0017908366 -0.0017907735 -0.0017905699 -0.0017906327 -0.0017913076 -0.0017923614 -0.0017932613 -0.0017938334][-0.0017941151 -0.0017915685 -0.0017907227 -0.0017905842 -0.0017906945 -0.0017906121 -0.0017902624 -0.0017898455 -0.0017897282 -0.0017897574 -0.0017901899 -0.0017912297 -0.0017925295 -0.0017935175 -0.0017940759][-0.0017936089 -0.0017915514 -0.0017910283 -0.001790966 -0.0017909814 -0.0017906576 -0.0017900056 -0.0017894126 -0.0017893038 -0.0017895715 -0.0017903555 -0.0017916659 -0.001793002 -0.0017938832 -0.0017942914][-0.0017935444 -0.0017918138 -0.0017914574 -0.0017913848 -0.00179127 -0.0017908587 -0.0017902005 -0.0017897428 -0.0017898744 -0.0017904784 -0.0017915044 -0.0017928021 -0.001793892 -0.0017944766 -0.0017946237][-0.0017936941 -0.0017920232 -0.0017917095 -0.0017915842 -0.0017914374 -0.0017911565 -0.0017908345 -0.0017908087 -0.0017912854 -0.0017921161 -0.0017931492 -0.0017941683 -0.0017948375 -0.0017950376 -0.0017949172][-0.001793956 -0.0017922324 -0.0017918264 -0.0017915977 -0.0017914588 -0.0017914466 -0.0017915674 -0.0017919992 -0.0017927111 -0.0017936132 -0.0017944935 -0.0017951106 -0.0017953074 -0.0017951956 -0.0017949695][-0.0017944458 -0.0017925946 -0.0017920823 -0.0017917807 -0.001791709 -0.001791951 -0.0017924003 -0.0017930424 -0.0017937738 -0.0017945432 -0.0017951147 -0.0017953557 -0.0017952668 -0.0017950388 -0.0017948349][-0.0017950645 -0.0017930783 -0.0017923766 -0.0017920353 -0.0017920426 -0.0017924636 -0.001793093 -0.0017937956 -0.0017944509 -0.0017949992 -0.0017952628 -0.0017952392 -0.0017950614 -0.0017948593 -0.0017947208]]...]
INFO - root - 2017-12-10 02:11:59.137001: step 79310, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.850 sec/batch; 59h:46m:20s remains)
INFO - root - 2017-12-10 02:12:07.706746: step 79320, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.887 sec/batch; 62h:22m:02s remains)
INFO - root - 2017-12-10 02:12:16.383133: step 79330, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.880 sec/batch; 61h:51m:46s remains)
INFO - root - 2017-12-10 02:12:24.973554: step 79340, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.851 sec/batch; 59h:50m:08s remains)
INFO - root - 2017-12-10 02:12:33.436379: step 79350, loss = 0.82, batch loss = 0.69 (9.7 examples/sec; 0.821 sec/batch; 57h:42m:44s remains)
INFO - root - 2017-12-10 02:12:41.946457: step 79360, loss = 0.82, batch loss = 0.69 (9.7 examples/sec; 0.827 sec/batch; 58h:11m:08s remains)
INFO - root - 2017-12-10 02:12:50.479385: step 79370, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.877 sec/batch; 61h:39m:26s remains)
INFO - root - 2017-12-10 02:12:59.095843: step 79380, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.844 sec/batch; 59h:20m:53s remains)
INFO - root - 2017-12-10 02:13:07.855625: step 79390, loss = 0.82, batch loss = 0.69 (8.3 examples/sec; 0.966 sec/batch; 67h:56m:54s remains)
INFO - root - 2017-12-10 02:13:16.277971: step 79400, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.863 sec/batch; 60h:42m:31s remains)
2017-12-10 02:13:17.215199: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0018167937 -0.0018163291 -0.0018164363 -0.0018166022 -0.0018166973 -0.0018166754 -0.0018165436 -0.0018163598 -0.0018161653 -0.0018159711 -0.0018157889 -0.0018156742 -0.0018157356 -0.0018162394 -0.0018174736][-0.001816573 -0.001816183 -0.0018163784 -0.0018166148 -0.0018167293 -0.0018166597 -0.0018164415 -0.0018161684 -0.0018159056 -0.0018156866 -0.0018155164 -0.0018154484 -0.0018155562 -0.0018160546 -0.0018172057][-0.0018166893 -0.0018164288 -0.0018167261 -0.0018170208 -0.0018171108 -0.0018169568 -0.0018166575 -0.0018163067 -0.001815983 -0.0018157572 -0.0018156455 -0.001815676 -0.0018158428 -0.0018163308 -0.0018173326][-0.0018169775 -0.0018167392 -0.0018170858 -0.0018173736 -0.0018173882 -0.0018171201 -0.0018166974 -0.0018162312 -0.0018158332 -0.0018156348 -0.0018156483 -0.0018158376 -0.0018160915 -0.0018165706 -0.0018174263][-0.0018173585 -0.0018171381 -0.001817483 -0.0018177208 -0.0018176356 -0.0018172163 -0.0018165929 -0.0018159801 -0.0018154997 -0.0018153437 -0.0018155263 -0.0018159164 -0.00181629 -0.0018167662 -0.0018174845][-0.0018175435 -0.0018173746 -0.0018177289 -0.0018179227 -0.0018177698 -0.0018172329 -0.0018164221 -0.0018156363 -0.0018150975 -0.0018150167 -0.0018153847 -0.0018159454 -0.0018164329 -0.0018169266 -0.0018175546][-0.0018175829 -0.0018173688 -0.0018176712 -0.00181778 -0.0018175225 -0.0018168744 -0.0018159563 -0.0018151077 -0.0018145863 -0.0018146352 -0.0018152059 -0.0018159094 -0.0018165021 -0.0018170431 -0.0018176162][-0.0018173906 -0.0018171228 -0.0018173289 -0.0018173058 -0.0018168982 -0.0018161213 -0.001815166 -0.001814354 -0.0018139527 -0.0018142289 -0.0018150462 -0.0018159148 -0.0018165745 -0.0018171462 -0.0018176693][-0.0018170198 -0.0018167223 -0.001816898 -0.0018167978 -0.0018163219 -0.0018155142 -0.0018146061 -0.0018138957 -0.0018136428 -0.001814126 -0.0018151173 -0.0018160448 -0.0018166841 -0.0018171869 -0.0018176214][-0.0018165318 -0.0018162674 -0.0018164663 -0.0018164016 -0.0018159624 -0.0018152309 -0.0018144901 -0.0018139837 -0.0018139315 -0.0018145478 -0.001815559 -0.001816386 -0.0018168689 -0.0018171996 -0.0018174911][-0.001816305 -0.0018160595 -0.0018162625 -0.0018162449 -0.0018158599 -0.0018152426 -0.0018146738 -0.0018143719 -0.0018144822 -0.0018151753 -0.0018160838 -0.0018167145 -0.0018169843 -0.0018171491 -0.0018173178][-0.0018163894 -0.001816138 -0.0018163241 -0.0018163138 -0.0018159577 -0.0018154155 -0.0018149748 -0.0018147925 -0.0018149951 -0.0018156556 -0.0018164123 -0.0018168607 -0.0018169724 -0.0018170439 -0.0018171304][-0.001816622 -0.0018163869 -0.0018165099 -0.0018165077 -0.0018161982 -0.0018157236 -0.001815377 -0.0018152398 -0.0018154186 -0.0018159314 -0.001816502 -0.0018168435 -0.0018169071 -0.0018169404 -0.0018169635][-0.0018167291 -0.0018164344 -0.0018164485 -0.0018164489 -0.0018162176 -0.0018158797 -0.0018156422 -0.0018155093 -0.0018156031 -0.0018159237 -0.0018163216 -0.0018166025 -0.0018166802 -0.0018167186 -0.0018166903][-0.0018166965 -0.0018163078 -0.0018161724 -0.0018161219 -0.0018159782 -0.0018157769 -0.0018156283 -0.0018154978 -0.0018155072 -0.0018156592 -0.0018158949 -0.0018161376 -0.0018162458 -0.0018163077 -0.0018162667]]...]
INFO - root - 2017-12-10 02:13:25.736330: step 79410, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.876 sec/batch; 61h:33m:53s remains)
INFO - root - 2017-12-10 02:13:34.488926: step 79420, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.832 sec/batch; 58h:28m:18s remains)
INFO - root - 2017-12-10 02:13:43.239391: step 79430, loss = 0.83, batch loss = 0.70 (9.2 examples/sec; 0.869 sec/batch; 61h:03m:49s remains)
INFO - root - 2017-12-10 02:13:51.927268: step 79440, loss = 0.82, batch loss = 0.69 (8.8 examples/sec; 0.906 sec/batch; 63h:39m:33s remains)
INFO - root - 2017-12-10 02:14:00.424272: step 79450, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.890 sec/batch; 62h:33m:59s remains)
INFO - root - 2017-12-10 02:14:09.199613: step 79460, loss = 0.83, batch loss = 0.70 (9.2 examples/sec; 0.865 sec/batch; 60h:48m:07s remains)
INFO - root - 2017-12-10 02:14:17.851477: step 79470, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.847 sec/batch; 59h:33m:28s remains)
INFO - root - 2017-12-10 02:14:26.716900: step 79480, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.858 sec/batch; 60h:19m:43s remains)
INFO - root - 2017-12-10 02:14:35.495970: step 79490, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.888 sec/batch; 62h:26m:17s remains)
INFO - root - 2017-12-10 02:14:44.030049: step 79500, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.836 sec/batch; 58h:47m:13s remains)
2017-12-10 02:14:44.972752: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.47731876 0.47224143 0.4682081 0.46499723 0.46122685 0.45521212 0.44706991 0.43631017 0.42340109 0.40905619 0.39581162 0.3850536 0.37678725 0.37110049 0.36727989][0.48015502 0.48086452 0.48146939 0.48286974 0.48350063 0.48064023 0.47405031 0.46274871 0.44832695 0.43095043 0.41377011 0.398469 0.38588196 0.37665191 0.36985463][0.47952196 0.48650169 0.49255204 0.49934173 0.50470811 0.504878 0.50028932 0.4894436 0.47391641 0.45379046 0.43281639 0.41263449 0.39496735 0.381017 0.37015182][0.47957718 0.49508497 0.5084095 0.52096051 0.53074908 0.53344345 0.52994531 0.51860368 0.50166076 0.47945651 0.45534074 0.43101755 0.4080573 0.38879329 0.37309164][0.47706884 0.4993853 0.51933289 0.53880525 0.55415517 0.56008691 0.55835247 0.54732323 0.52926326 0.50455558 0.47701621 0.44912285 0.42208362 0.39777169 0.37737533][0.47562826 0.50300062 0.527134 0.55144382 0.57083738 0.58005977 0.57984006 0.5694055 0.55127633 0.52492857 0.49472135 0.46357954 0.43266597 0.40461755 0.37997195][0.47452664 0.50600433 0.53276742 0.55895054 0.579998 0.59153712 0.59238017 0.58207405 0.56321669 0.53638804 0.50483388 0.47142887 0.43825012 0.40765432 0.38013437][0.46892276 0.50246537 0.5294143 0.5556705 0.576371 0.58792037 0.58836895 0.57827389 0.55940086 0.53239441 0.50044149 0.46644962 0.432526 0.40102834 0.37214708][0.45764023 0.49199504 0.51832968 0.54324883 0.56223315 0.57277727 0.57188582 0.56058139 0.54073232 0.5132423 0.48117355 0.44738165 0.41427821 0.38336334 0.35507461][0.43813467 0.4702822 0.49332833 0.51588786 0.53291065 0.54239058 0.54104328 0.52920431 0.50887913 0.48121461 0.44981462 0.41733176 0.38648844 0.35861591 0.3331][0.41625443 0.44484079 0.46312669 0.48094657 0.49400261 0.50135124 0.49852782 0.4862293 0.46647713 0.44063863 0.41186535 0.38200954 0.3546102 0.33047611 0.3083972][0.39274514 0.41644302 0.42923453 0.44128516 0.44967544 0.45374802 0.44934702 0.436756 0.41794997 0.3946102 0.3693893 0.34375533 0.32087919 0.30079097 0.28257021][0.36763784 0.3857297 0.39275077 0.39957461 0.40379855 0.40441865 0.39858627 0.38615856 0.36904666 0.34862715 0.32723856 0.30632338 0.28786013 0.27192605 0.25771683][0.34688485 0.35982996 0.36166111 0.36357847 0.36391747 0.36197183 0.35532188 0.343764 0.32869619 0.31152749 0.29408687 0.27733251 0.26295465 0.25070044 0.24005193][0.33053133 0.33927357 0.33654591 0.33437565 0.33143231 0.327517 0.32064077 0.31060508 0.29843354 0.2851266 0.27189803 0.25946888 0.24905458 0.24031301 0.23269823]]...]
INFO - root - 2017-12-10 02:14:53.616338: step 79510, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.883 sec/batch; 62h:04m:18s remains)
INFO - root - 2017-12-10 02:15:02.241146: step 79520, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.838 sec/batch; 58h:54m:03s remains)
INFO - root - 2017-12-10 02:15:10.899831: step 79530, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.861 sec/batch; 60h:32m:01s remains)
INFO - root - 2017-12-10 02:15:19.604891: step 79540, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.876 sec/batch; 61h:31m:19s remains)
INFO - root - 2017-12-10 02:15:28.212661: step 79550, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.857 sec/batch; 60h:12m:42s remains)
INFO - root - 2017-12-10 02:15:36.801684: step 79560, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.846 sec/batch; 59h:24m:53s remains)
INFO - root - 2017-12-10 02:15:45.414230: step 79570, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.846 sec/batch; 59h:25m:14s remains)
INFO - root - 2017-12-10 02:15:54.029389: step 79580, loss = 0.82, batch loss = 0.69 (9.7 examples/sec; 0.827 sec/batch; 58h:06m:31s remains)
INFO - root - 2017-12-10 02:16:02.717477: step 79590, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.875 sec/batch; 61h:29m:44s remains)
INFO - root - 2017-12-10 02:16:11.216243: step 79600, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 60h:58m:48s remains)
2017-12-10 02:16:12.172217: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0018388199 -0.0018391882 -0.0018401578 -0.0018410617 -0.0018414003 -0.0018411326 -0.001840318 -0.0018393158 -0.0018388426 -0.0018390183 -0.0018392664 -0.0018388183 -0.0018378838 -0.0018372631 -0.0018376331][-0.0018385472 -0.0018390373 -0.0018398301 -0.0018400184 -0.0018387687 -0.0018361956 -0.0018328771 -0.0018301401 -0.0018294608 -0.0018307454 -0.0018323377 -0.0018320869 -0.0018300638 -0.0018280045 -0.0018280024][-0.0018383513 -0.0018388472 -0.0018390395 -0.0018374203 -0.0018325272 -0.0018251087 -0.0018169578 -0.0018113429 -0.0018112122 -0.0018155543 -0.0018203643 -0.0018208532 -0.0018170252 -0.001812342 -0.0018111796][-0.0018382911 -0.0018386132 -0.0018374772 -0.0018324661 -0.0018212169 -0.0018055167 -0.0017899841 -0.0017811771 -0.0017833319 -0.0017934474 -0.0018039311 -0.0018063338 -0.0018007283 -0.0017925379 -0.0017890285][-0.0018386745 -0.0018386933 -0.0018358009 -0.0018261686 -0.0018065951 -0.0017807243 -0.0017565907 -0.0017447957 -0.0017505478 -0.0017682937 -0.0017861619 -0.0017921476 -0.0017862011 -0.0017749877 -0.0017679916][-0.0018393244 -0.0018390521 -0.0018348993 -0.0018216685 -0.0017957356 -0.001762057 -0.0017317783 -0.0017187939 -0.0017282817 -0.0017524287 -0.0017764207 -0.0017864972 -0.0017819874 -0.0017696442 -0.0017592304][-0.0018399307 -0.0018395443 -0.0018352042 -0.0018215507 -0.001794982 -0.0017606051 -0.0017300542 -0.0017176528 -0.0017285949 -0.0017545986 -0.0017804975 -0.0017932333 -0.0017915305 -0.0017805849 -0.001768794][-0.0018403621 -0.0018399379 -0.0018364082 -0.0018254964 -0.0018044286 -0.0017773153 -0.001753422 -0.00174381 -0.0017527245 -0.0017740893 -0.0017961626 -0.0018087187 -0.0018097961 -0.0018023853 -0.0017921282][-0.001840596 -0.001840304 -0.0018381496 -0.0018310408 -0.0018172582 -0.0017996556 -0.0017843803 -0.0017785622 -0.0017847418 -0.0017993003 -0.001814842 -0.0018246969 -0.0018269734 -0.0018233331 -0.0018164812][-0.0018406154 -0.001840462 -0.0018394868 -0.0018354706 -0.001827353 -0.0018169483 -0.0018082636 -0.0018055444 -0.0018096505 -0.0018183532 -0.0018276216 -0.0018339345 -0.0018361867 -0.0018351373 -0.0018318094][-0.0018404702 -0.0018404381 -0.0018401814 -0.0018382417 -0.0018339746 -0.0018286668 -0.0018249189 -0.0018246209 -0.0018275424 -0.0018319143 -0.0018359983 -0.0018386999 -0.0018396428 -0.0018393502 -0.0018381231][-0.0018401528 -0.0018401587 -0.001840296 -0.001839488 -0.0018374698 -0.0018350787 -0.0018338425 -0.0018344109 -0.0018361023 -0.0018379505 -0.0018394517 -0.0018401145 -0.001840072 -0.0018397099 -0.0018392061][-0.0018398247 -0.0018398487 -0.0018401818 -0.0018401268 -0.0018395458 -0.0018388507 -0.001838775 -0.0018392705 -0.0018398899 -0.0018403302 -0.0018405886 -0.0018404265 -0.0018399591 -0.0018395612 -0.0018393026][-0.0018395479 -0.0018395828 -0.0018399046 -0.0018401074 -0.001840038 -0.0018399144 -0.0018400266 -0.0018402247 -0.0018403663 -0.0018403927 -0.0018403688 -0.0018400726 -0.0018396465 -0.0018392908 -0.0018390979][-0.0018394688 -0.0018394279 -0.0018396068 -0.0018398289 -0.0018399669 -0.0018400452 -0.0018401463 -0.0018402116 -0.0018402025 -0.0018401134 -0.0018399672 -0.0018397288 -0.0018394404 -0.0018391609 -0.0018389837]]...]
INFO - root - 2017-12-10 02:16:20.737617: step 79610, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.829 sec/batch; 58h:14m:32s remains)
INFO - root - 2017-12-10 02:16:29.379059: step 79620, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 60h:32m:14s remains)
INFO - root - 2017-12-10 02:16:38.031327: step 79630, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.866 sec/batch; 60h:50m:26s remains)
INFO - root - 2017-12-10 02:16:46.672109: step 79640, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.859 sec/batch; 60h:20m:54s remains)
INFO - root - 2017-12-10 02:16:55.170622: step 79650, loss = 0.82, batch loss = 0.69 (9.9 examples/sec; 0.810 sec/batch; 56h:53m:45s remains)
INFO - root - 2017-12-10 02:17:03.816351: step 79660, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.885 sec/batch; 62h:09m:37s remains)
INFO - root - 2017-12-10 02:17:12.465314: step 79670, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 61h:08m:21s remains)
INFO - root - 2017-12-10 02:17:21.054982: step 79680, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 60h:41m:09s remains)
INFO - root - 2017-12-10 02:17:29.644515: step 79690, loss = 0.81, batch loss = 0.68 (9.3 examples/sec; 0.863 sec/batch; 60h:34m:14s remains)
INFO - root - 2017-12-10 02:17:38.244741: step 79700, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 60h:56m:17s remains)
2017-12-10 02:17:39.100573: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.014934733 0.013390956 0.012003598 0.01079576 0.0095975325 0.0084209731 0.0071814465 0.0061443863 0.0054682884 0.0053144996 0.0054350793 0.005632265 0.0056165052 0.005043067 0.0040912325][0.024154829 0.021893697 0.019739959 0.01803072 0.016364077 0.014726958 0.013039375 0.011685335 0.010919306 0.010837191 0.011098483 0.011255688 0.010866432 0.009545452 0.0075381617][0.036412593 0.033498373 0.030614395 0.028499877 0.026494637 0.024466472 0.02222796 0.020322219 0.019147664 0.01904946 0.0195343 0.019955685 0.019569064 0.01779514 0.014930367][0.052051023 0.049332019 0.046453618 0.0445243 0.042547464 0.040212825 0.03729675 0.034467731 0.032399543 0.031651121 0.031730182 0.03188663 0.03107509 0.028713752 0.024972454][0.066953346 0.06545569 0.063375928 0.062431894 0.06111075 0.058892217 0.055491034 0.051602539 0.04826311 0.046414491 0.045758747 0.045483448 0.044358782 0.041685443 0.03747103][0.077085391 0.077372409 0.0764676 0.07636141 0.075558916 0.073545031 0.069909751 0.065188296 0.060707681 0.057723477 0.056367982 0.055884939 0.054974187 0.052783255 0.049061727][0.080151714 0.08155039 0.081395477 0.081729151 0.081118934 0.079032183 0.075094163 0.069808632 0.064512223 0.060617473 0.058707442 0.058188334 0.057887267 0.056758009 0.05419885][0.074214265 0.076181412 0.076557674 0.076971486 0.076347731 0.074160814 0.070176624 0.064866304 0.059500463 0.055496845 0.053588681 0.053384852 0.053802181 0.053605158 0.052079439][0.060721725 0.062792741 0.063700117 0.064347178 0.063800991 0.061652303 0.057816464 0.052964412 0.048167396 0.044717938 0.043403644 0.043702107 0.044757381 0.045254368 0.044309583][0.043415204 0.045382719 0.046551585 0.047306843 0.046855029 0.044921428 0.041604828 0.037604824 0.033823416 0.031351037 0.030812297 0.031613071 0.032913666 0.033523258 0.032727383][0.026789656 0.028510535 0.029517461 0.030066136 0.029580574 0.02796502 0.025368666 0.022510305 0.020043237 0.018632021 0.018617166 0.019468568 0.020591585 0.021006484 0.020124126][0.012989865 0.01402576 0.014621052 0.014837229 0.01432985 0.013132674 0.011434505 0.0098140305 0.0085521052 0.007958645 0.0081337206 0.0087681655 0.009463734 0.0096031222 0.008915253][0.0037801769 0.0042724949 0.0045636115 0.0046026371 0.0042577428 0.0035842299 0.0027323384 0.0020414824 0.0015759483 0.0014401505 0.0015888006 0.0018704649 0.0021214569 0.0021147965 0.0017543872][-0.00024907687 3.3217832e-05 0.00022008049 0.00018219033 -7.6828408e-05 -0.00046112551 -0.00083498505 -0.0011056142 -0.0012460167 -0.0012714188 -0.0012283045 -0.001163032 -0.0011159404 -0.0011350948 -0.0012315104][-0.00034199294 5.9176353e-05 0.00020997378 -2.1084561e-05 -0.00052018696 -0.0010617612 -0.0014628512 -0.0016746605 -0.0017290987 -0.0017358419 -0.0017343101 -0.0017367962 -0.0017373774 -0.0017419333 -0.0017476343]]...]
INFO - root - 2017-12-10 02:17:47.563416: step 79710, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.878 sec/batch; 61h:37m:49s remains)
INFO - root - 2017-12-10 02:17:56.173639: step 79720, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.863 sec/batch; 60h:36m:01s remains)
INFO - root - 2017-12-10 02:18:04.816781: step 79730, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.866 sec/batch; 60h:47m:59s remains)
INFO - root - 2017-12-10 02:18:13.461710: step 79740, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.835 sec/batch; 58h:36m:25s remains)
INFO - root - 2017-12-10 02:18:22.001704: step 79750, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.843 sec/batch; 59h:12m:32s remains)
INFO - root - 2017-12-10 02:18:30.711024: step 79760, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.873 sec/batch; 61h:19m:18s remains)
INFO - root - 2017-12-10 02:18:39.207125: step 79770, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.854 sec/batch; 59h:57m:07s remains)
INFO - root - 2017-12-10 02:18:47.793142: step 79780, loss = 0.82, batch loss = 0.69 (9.7 examples/sec; 0.822 sec/batch; 57h:41m:50s remains)
INFO - root - 2017-12-10 02:18:56.481891: step 79790, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.873 sec/batch; 61h:14m:51s remains)
INFO - root - 2017-12-10 02:19:05.180511: step 79800, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.884 sec/batch; 62h:02m:20s remains)
2017-12-10 02:19:06.080531: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0018211558 -0.001818385 -0.0018149833 -0.0018107316 -0.001805471 -0.0017999579 -0.0017948919 -0.0017911168 -0.0017892574 -0.001788982 -0.0017905089 -0.0017924939 -0.0017941705 -0.0017946758 -0.001794088][-0.0018205456 -0.0018189556 -0.0018170236 -0.0018143726 -0.0018105223 -0.0018063819 -0.0018024183 -0.0017994149 -0.001797847 -0.0017975327 -0.0017988702 -0.0018008493 -0.0018025654 -0.0018032892 -0.0018030799][-0.0018192533 -0.0018190134 -0.0018186877 -0.0018179504 -0.0018158637 -0.001813242 -0.0018106699 -0.0018084693 -0.0018073014 -0.0018070987 -0.0018084327 -0.0018102074 -0.001811782 -0.0018126544 -0.0018127126][-0.0018164525 -0.0018166305 -0.0018173209 -0.0018181315 -0.001817707 -0.0018168418 -0.0018158254 -0.0018147123 -0.0018140486 -0.0018138346 -0.0018149557 -0.0018163256 -0.0018176241 -0.0018182783 -0.0018183361][-0.0018138122 -0.0018135253 -0.0018142603 -0.0018156171 -0.0018162526 -0.0018168566 -0.0018173198 -0.0018175427 -0.0018176767 -0.0018175306 -0.0018183256 -0.0018192342 -0.0018202661 -0.0018208958 -0.0018211339][-0.0018121916 -0.0018110701 -0.0018111182 -0.0018122471 -0.0018131464 -0.0018142903 -0.0018154056 -0.0018163222 -0.0018169677 -0.0018168435 -0.0018172562 -0.0018178644 -0.0018187611 -0.0018195845 -0.001820276][-0.0018115697 -0.0018100127 -0.0018094954 -0.0018100359 -0.0018108631 -0.0018117242 -0.0018124075 -0.001813267 -0.0018138875 -0.0018137947 -0.001814106 -0.0018148462 -0.0018158728 -0.0018167882 -0.0018177438][-0.0018119395 -0.0018099652 -0.0018089266 -0.0018088262 -0.0018091279 -0.0018094303 -0.0018095572 -0.0018099822 -0.0018104487 -0.0018102189 -0.0018103229 -0.0018109821 -0.0018121342 -0.0018131927 -0.0018138919][-0.001812409 -0.0018101322 -0.0018087863 -0.00180822 -0.0018081679 -0.0018081099 -0.001807777 -0.0018078863 -0.0018080618 -0.0018076033 -0.0018075 -0.0018080577 -0.0018090979 -0.0018102085 -0.0018108111][-0.0018130518 -0.0018106765 -0.0018092085 -0.0018083465 -0.0018080398 -0.0018076763 -0.0018070322 -0.0018067481 -0.0018066997 -0.0018062672 -0.0018060661 -0.0018066955 -0.0018078218 -0.0018090176 -0.0018097546][-0.0018134598 -0.0018112933 -0.0018098993 -0.001809053 -0.0018086314 -0.0018078776 -0.0018069487 -0.0018065106 -0.0018063448 -0.0018060037 -0.0018058054 -0.0018063415 -0.0018072597 -0.0018082176 -0.0018088336][-0.0018140114 -0.0018123187 -0.0018112643 -0.0018103224 -0.0018096763 -0.0018086247 -0.0018075921 -0.0018070742 -0.0018068362 -0.0018065932 -0.0018065098 -0.001806954 -0.0018074991 -0.0018081616 -0.001808482][-0.0018144016 -0.0018130874 -0.0018122337 -0.0018112537 -0.0018104541 -0.0018094342 -0.0018084268 -0.001807568 -0.0018070538 -0.0018066521 -0.0018064834 -0.0018067381 -0.00180726 -0.0018080224 -0.0018083339][-0.0018147683 -0.0018138922 -0.0018133614 -0.0018123422 -0.0018113828 -0.001810191 -0.0018090104 -0.0018079717 -0.0018072119 -0.0018067843 -0.0018066742 -0.0018069971 -0.0018073359 -0.0018078549 -0.0018080057][-0.0018147689 -0.0018142848 -0.001813966 -0.0018131055 -0.0018121706 -0.0018109544 -0.0018095598 -0.0018083215 -0.0018075383 -0.0018071963 -0.0018071546 -0.0018074895 -0.0018078797 -0.001808214 -0.0018083467]]...]
INFO - root - 2017-12-10 02:19:14.557321: step 79810, loss = 0.82, batch loss = 0.69 (10.1 examples/sec; 0.793 sec/batch; 55h:37m:44s remains)
INFO - root - 2017-12-10 02:19:23.184252: step 79820, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.897 sec/batch; 62h:56m:55s remains)
INFO - root - 2017-12-10 02:19:31.814126: step 79830, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.872 sec/batch; 61h:12m:18s remains)
INFO - root - 2017-12-10 02:19:40.420054: step 79840, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.858 sec/batch; 60h:12m:58s remains)
INFO - root - 2017-12-10 02:19:48.999736: step 79850, loss = 0.82, batch loss = 0.69 (8.8 examples/sec; 0.904 sec/batch; 63h:28m:38s remains)
INFO - root - 2017-12-10 02:19:57.677943: step 79860, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.848 sec/batch; 59h:29m:27s remains)
INFO - root - 2017-12-10 02:20:06.292021: step 79870, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.853 sec/batch; 59h:52m:01s remains)
INFO - root - 2017-12-10 02:20:14.889922: step 79880, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.855 sec/batch; 59h:58m:57s remains)
INFO - root - 2017-12-10 02:20:23.357856: step 79890, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.854 sec/batch; 59h:56m:54s remains)
INFO - root - 2017-12-10 02:20:31.868007: step 79900, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 60h:50m:58s remains)
2017-12-10 02:20:32.724401: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0018109813 -0.0018114836 -0.0018137844 -0.0018167717 -0.0018199249 -0.0018229826 -0.0018252018 -0.0018266963 -0.0018276143 -0.001828188 -0.0018285892 -0.0018289143 -0.0018292897 -0.0018295059 -0.0018293745][-0.0018134197 -0.0018137182 -0.0018156306 -0.0018180395 -0.0018205451 -0.0018228478 -0.0018241846 -0.0018247223 -0.0018248691 -0.0018248932 -0.0018252202 -0.0018258895 -0.0018267516 -0.001827445 -0.0018278596][-0.0018171135 -0.001816983 -0.0018179513 -0.0018191603 -0.0018204596 -0.0018216158 -0.001822065 -0.0018219181 -0.0018216802 -0.0018216389 -0.0018222787 -0.0018234786 -0.0018248734 -0.0018259576 -0.0018267893][-0.0018213984 -0.0018207744 -0.0018206567 -0.0018204632 -0.0018203054 -0.0018200487 -0.0018195097 -0.0018187826 -0.0018183355 -0.0018185414 -0.0018196754 -0.0018214806 -0.0018232568 -0.0018245995 -0.0018256297][-0.001825355 -0.001824491 -0.0018236369 -0.0018222465 -0.0018205702 -0.0018187935 -0.0018171233 -0.0018157926 -0.0018152948 -0.0018157973 -0.0018173201 -0.0018193775 -0.0018212025 -0.0018226134 -0.0018237976][-0.0018282185 -0.0018275526 -0.001826409 -0.0018241494 -0.0018211557 -0.0018179895 -0.0018152423 -0.0018134338 -0.0018129647 -0.0018137483 -0.0018154839 -0.0018175191 -0.0018190837 -0.0018202781 -0.0018213682][-0.0018296851 -0.001829584 -0.0018285516 -0.0018258879 -0.0018219438 -0.0018176846 -0.0018140735 -0.0018117301 -0.0018111219 -0.0018119817 -0.0018136078 -0.001815327 -0.0018165581 -0.0018174888 -0.0018183622][-0.0018296066 -0.0018301461 -0.0018294322 -0.0018267895 -0.001822414 -0.0018174809 -0.0018132137 -0.0018104307 -0.0018095736 -0.0018102622 -0.0018115318 -0.0018127799 -0.0018136665 -0.001814392 -0.0018150476][-0.0018280717 -0.0018289273 -0.0018285069 -0.0018261074 -0.0018218349 -0.0018167048 -0.001812249 -0.0018093563 -0.0018083504 -0.0018087146 -0.0018095649 -0.0018104549 -0.0018110747 -0.0018116684 -0.0018123076][-0.0018252979 -0.0018259694 -0.0018256253 -0.0018236036 -0.0018198624 -0.0018152202 -0.0018110987 -0.0018084915 -0.0018076629 -0.0018078426 -0.0018083564 -0.001808936 -0.0018094099 -0.0018099744 -0.0018106129][-0.0018220934 -0.0018221136 -0.0018215879 -0.0018199336 -0.0018170404 -0.0018134118 -0.00181018 -0.0018080717 -0.0018074471 -0.0018075278 -0.0018078265 -0.0018082219 -0.0018086506 -0.0018091711 -0.0018097942][-0.0018193403 -0.0018183382 -0.0018174328 -0.0018161881 -0.0018142782 -0.001811833 -0.0018096115 -0.001808061 -0.001807522 -0.0018074767 -0.0018076105 -0.0018078523 -0.0018081624 -0.0018085534 -0.001809055][-0.0018173198 -0.0018151292 -0.0018136797 -0.0018126352 -0.0018115683 -0.0018102832 -0.0018091096 -0.0018081192 -0.0018076248 -0.0018074217 -0.0018073722 -0.0018073948 -0.0018074911 -0.0018077078 -0.0018081071][-0.0018163227 -0.0018131692 -0.0018111814 -0.0018100308 -0.0018094288 -0.0018088741 -0.0018084443 -0.0018079208 -0.0018074554 -0.0018070517 -0.0018067736 -0.0018066395 -0.001806653 -0.0018068462 -0.0018072496][-0.0018160633 -0.0018123537 -0.0018100762 -0.0018087727 -0.0018083069 -0.0018080492 -0.0018079464 -0.0018076188 -0.0018071488 -0.0018066318 -0.0018062907 -0.0018061752 -0.0018062451 -0.0018064833 -0.0018068856]]...]
INFO - root - 2017-12-10 02:20:41.366983: step 79910, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.884 sec/batch; 62h:02m:24s remains)
INFO - root - 2017-12-10 02:20:49.866767: step 79920, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.855 sec/batch; 59h:58m:59s remains)
INFO - root - 2017-12-10 02:20:58.543232: step 79930, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 61h:05m:03s remains)
INFO - root - 2017-12-10 02:21:07.225462: step 79940, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.870 sec/batch; 61h:01m:36s remains)
INFO - root - 2017-12-10 02:21:15.782160: step 79950, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.836 sec/batch; 58h:37m:11s remains)
INFO - root - 2017-12-10 02:21:24.570198: step 79960, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.894 sec/batch; 62h:43m:53s remains)
INFO - root - 2017-12-10 02:21:33.199678: step 79970, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.848 sec/batch; 59h:28m:22s remains)
INFO - root - 2017-12-10 02:21:41.865746: step 79980, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.856 sec/batch; 60h:02m:28s remains)
INFO - root - 2017-12-10 02:21:50.572416: step 79990, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 60h:52m:53s remains)
INFO - root - 2017-12-10 02:21:59.266247: step 80000, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.894 sec/batch; 62h:44m:03s remains)
2017-12-10 02:22:00.203810: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0018296082 -0.0018291647 -0.0018291497 -0.0018293057 -0.0018295436 -0.0018298205 -0.0018299621 -0.0018299187 -0.0018297264 -0.0018294543 -0.0018291004 -0.0018287541 -0.0018285321 -0.0018284671 -0.0018284941][-0.0018295493 -0.001829225 -0.0018293597 -0.0018297592 -0.0018303007 -0.0018308598 -0.0018311559 -0.0018310554 -0.0018306571 -0.0018301207 -0.0018295345 -0.0018289892 -0.0018285657 -0.0018283762 -0.001828364][-0.0018294143 -0.0018292548 -0.0018296732 -0.0018305277 -0.0018314826 -0.001832269 -0.0018326661 -0.0018324874 -0.001831886 -0.001831094 -0.001830338 -0.0018296873 -0.0018291672 -0.0018288689 -0.0018287281][-0.0018292096 -0.0018293514 -0.0018302342 -0.0018316719 -0.0018331573 -0.0018343015 -0.0018349151 -0.0018347573 -0.0018339668 -0.001832868 -0.001831776 -0.0018308319 -0.0018300812 -0.0018295884 -0.0018293104][-0.0018297414 -0.001830269 -0.0018316132 -0.0018335044 -0.0018353534 -0.0018367034 -0.0018372998 -0.0018369226 -0.0018357222 -0.0018341419 -0.0018326609 -0.0018315071 -0.001830629 -0.001829979 -0.0018296465][-0.0018306172 -0.0018312566 -0.0018327426 -0.0018347297 -0.0018365616 -0.001837693 -0.0018378056 -0.0018369927 -0.0018354646 -0.001833567 -0.001832062 -0.001831133 -0.0018304986 -0.0018299269 -0.0018296131][-0.0018307491 -0.0018313422 -0.0018326457 -0.0018343605 -0.0018358239 -0.0018364838 -0.001836165 -0.0018351906 -0.0018338212 -0.0018320195 -0.0018306696 -0.0018300639 -0.0018298312 -0.0018295256 -0.0018293288][-0.0018303425 -0.0018307059 -0.0018316509 -0.0018328076 -0.0018335616 -0.0018336611 -0.001833245 -0.0018325705 -0.0018317428 -0.001830454 -0.0018295299 -0.0018292029 -0.0018292257 -0.001829114 -0.0018290773][-0.0018298199 -0.001829996 -0.0018307081 -0.0018313999 -0.0018316566 -0.0018314677 -0.0018311135 -0.0018307763 -0.0018302874 -0.0018294839 -0.0018289248 -0.0018287464 -0.0018287917 -0.0018288064 -0.0018289224][-0.0018295059 -0.0018295778 -0.00183015 -0.0018305931 -0.0018307323 -0.0018305329 -0.001830186 -0.0018297634 -0.0018292727 -0.0018286485 -0.0018283162 -0.001828275 -0.0018283351 -0.0018284688 -0.001828697][-0.0018292806 -0.0018292218 -0.0018295792 -0.0018297458 -0.001829657 -0.0018294665 -0.0018292086 -0.0018287671 -0.001828353 -0.0018279159 -0.0018277185 -0.0018277909 -0.0018279683 -0.0018281854 -0.0018284455][-0.0018291179 -0.0018288759 -0.0018289796 -0.0018289336 -0.0018287562 -0.0018286027 -0.0018284719 -0.0018283018 -0.0018281104 -0.001827794 -0.0018276282 -0.0018277118 -0.0018279006 -0.0018281047 -0.0018283085][-0.0018290601 -0.0018287314 -0.001828732 -0.0018286658 -0.0018285323 -0.0018284591 -0.0018283862 -0.0018283081 -0.0018281894 -0.0018279781 -0.0018278843 -0.0018279199 -0.0018280465 -0.0018281792 -0.0018283009][-0.0018291003 -0.001828779 -0.0018287491 -0.0018287298 -0.0018286596 -0.0018285855 -0.0018284989 -0.0018283771 -0.0018282247 -0.0018280831 -0.0018280313 -0.0018280766 -0.0018281726 -0.0018282676 -0.0018283257][-0.0018291313 -0.001828723 -0.001828632 -0.0018286473 -0.0018285983 -0.0018285414 -0.0018284696 -0.0018283771 -0.0018282734 -0.0018281708 -0.0018281349 -0.0018281692 -0.0018282384 -0.0018282817 -0.0018283136]]...]
INFO:tensorflow:/home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-momentum-lr0.0001-clip-zeroinit-conv1-3init-from-scratch/model.ckpt-80000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - tensorflow - /home/v-chaoqw/MYSFC-ORI/track_model_checkpoints/SFC-2-momentum-lr0.0001-clip-zeroinit-conv1-3init-from-scratch/model.ckpt-80000 is not in all_model_checkpoint_paths. Manually adding it.
INFO - root - 2017-12-10 02:22:09.492278: step 80010, loss = 0.81, batch loss = 0.68 (11.1 examples/sec; 0.720 sec/batch; 50h:29m:58s remains)
INFO - root - 2017-12-10 02:22:18.186189: step 80020, loss = 0.81, batch loss = 0.68 (9.2 examples/sec; 0.865 sec/batch; 60h:41m:43s remains)
INFO - root - 2017-12-10 02:22:26.839309: step 80030, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.890 sec/batch; 62h:24m:53s remains)
INFO - root - 2017-12-10 02:22:35.509717: step 80040, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.875 sec/batch; 61h:23m:09s remains)
INFO - root - 2017-12-10 02:22:44.021106: step 80050, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 60h:26m:25s remains)
INFO - root - 2017-12-10 02:22:52.726201: step 80060, loss = 0.82, batch loss = 0.70 (9.3 examples/sec; 0.857 sec/batch; 60h:07m:32s remains)
INFO - root - 2017-12-10 02:23:01.386512: step 80070, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.901 sec/batch; 63h:12m:39s remains)
INFO - root - 2017-12-10 02:23:10.063797: step 80080, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.849 sec/batch; 59h:30m:29s remains)
INFO - root - 2017-12-10 02:23:18.703541: step 80090, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.860 sec/batch; 60h:16m:29s remains)
INFO - root - 2017-12-10 02:23:26.986933: step 80100, loss = 0.82, batch loss = 0.69 (10.1 examples/sec; 0.795 sec/batch; 55h:44m:07s remains)
2017-12-10 02:23:27.890031: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.30293611 0.30102751 0.30046535 0.30234736 0.30701 0.3138018 0.321058 0.32732543 0.33076605 0.32864717 0.3209258 0.30899134 0.29321286 0.27460724 0.25450149][0.30626369 0.30726764 0.30911782 0.3130447 0.31988534 0.32898396 0.33837759 0.34648764 0.3513999 0.35078821 0.34364989 0.33081332 0.31301975 0.29185364 0.26880485][0.30243206 0.30680329 0.31197503 0.31880879 0.32798398 0.33890828 0.3499274 0.35860384 0.36340794 0.36255825 0.35484383 0.34063289 0.32054636 0.29698539 0.27139166][0.29838377 0.30572212 0.31404412 0.32349935 0.33469281 0.346687 0.35834539 0.36697873 0.3708525 0.36864364 0.35956502 0.34355125 0.3212828 0.29542708 0.2676574][0.29377332 0.30326793 0.3134405 0.32490498 0.33771595 0.35042632 0.36202639 0.37005112 0.37282944 0.36893466 0.35787094 0.3402 0.31606412 0.28843585 0.25925997][0.28626621 0.29650098 0.30727747 0.31958693 0.33313045 0.34647533 0.35841635 0.36581779 0.36749178 0.36232466 0.34986648 0.3307943 0.30541843 0.27707162 0.24755682][0.27642581 0.28609285 0.29576275 0.30728474 0.32018656 0.33334062 0.34536439 0.35302666 0.3547518 0.34919995 0.33625296 0.31659847 0.29076326 0.26239952 0.23343478][0.26633626 0.27451539 0.28187057 0.29105693 0.30167422 0.31325889 0.32416764 0.33187568 0.33379462 0.32895303 0.31682321 0.29794881 0.27297723 0.24561115 0.21827947][0.25440741 0.26109639 0.26550263 0.27169752 0.27960446 0.28903183 0.2983118 0.30548021 0.3076261 0.30395424 0.29348692 0.27649531 0.25350085 0.22813641 0.20353021][0.23941466 0.24461827 0.24640472 0.24973452 0.25481665 0.26153892 0.26885557 0.2751545 0.27747059 0.27513143 0.26671815 0.25250539 0.23277526 0.21067293 0.18973327][0.22120291 0.22541092 0.22494371 0.22571842 0.22820848 0.23245208 0.23784935 0.24312124 0.24565484 0.24443124 0.23835593 0.22717956 0.21132852 0.19323644 0.17621684][0.20032927 0.20433594 0.20301653 0.20212646 0.20244157 0.2043442 0.20741722 0.21149786 0.2140957 0.2137441 0.20981611 0.20168594 0.18978974 0.17533302 0.16176955][0.17727998 0.18125249 0.17994152 0.17869021 0.1781566 0.17848751 0.17976497 0.18197456 0.18370658 0.18367578 0.18124172 0.17571437 0.16738756 0.15692024 0.14685114][0.15280208 0.1564807 0.15571001 0.1547707 0.15430993 0.15447399 0.15514413 0.1563382 0.15752785 0.15754689 0.15609889 0.1523173 0.14669105 0.13958423 0.13258412][0.13077728 0.13408273 0.13368699 0.13301896 0.13277324 0.13282558 0.13325898 0.13401428 0.13483265 0.13495113 0.13408564 0.13181347 0.12847804 0.12418085 0.11994991]]...]
INFO - root - 2017-12-10 02:23:36.429430: step 80110, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.842 sec/batch; 59h:01m:03s remains)
INFO - root - 2017-12-10 02:23:44.822383: step 80120, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.854 sec/batch; 59h:52m:31s remains)
INFO - root - 2017-12-10 02:23:53.479961: step 80130, loss = 0.81, batch loss = 0.68 (9.2 examples/sec; 0.865 sec/batch; 60h:38m:02s remains)
INFO - root - 2017-12-10 02:24:02.123796: step 80140, loss = 0.83, batch loss = 0.70 (9.1 examples/sec; 0.881 sec/batch; 61h:43m:51s remains)
INFO - root - 2017-12-10 02:24:10.586869: step 80150, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.852 sec/batch; 59h:44m:40s remains)
INFO - root - 2017-12-10 02:24:19.207727: step 80160, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.849 sec/batch; 59h:29m:43s remains)
INFO - root - 2017-12-10 02:24:27.730183: step 80170, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.866 sec/batch; 60h:40m:56s remains)
INFO - root - 2017-12-10 02:24:36.375848: step 80180, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.866 sec/batch; 60h:40m:24s remains)
INFO - root - 2017-12-10 02:24:45.210494: step 80190, loss = 0.81, batch loss = 0.68 (9.1 examples/sec; 0.883 sec/batch; 61h:54m:22s remains)
INFO - root - 2017-12-10 02:24:53.818858: step 80200, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.856 sec/batch; 59h:59m:50s remains)
2017-12-10 02:24:54.710441: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.0059245862 0.0059246281 0.0058892514 0.0060734679 0.0065561947 0.0071900566 0.0077078962 0.0078669284 0.0076666842 0.0071935444 0.006256198 0.0047826651 0.0028543239 0.0010077524 -0.00040333532][0.0054167132 0.0055824923 0.0055899634 0.0057086535 0.0060310229 0.0064379429 0.0066817715 0.0065956977 0.0062334067 0.0057117306 0.0048412043 0.0035235453 0.0018378581 0.00028772012 -0.00081975944][0.0045081405 0.0049302354 0.0051285485 0.0053141369 0.0055691116 0.0057971017 0.0057837158 0.0054434515 0.0048994939 0.0042856494 0.0034214715 0.0022264561 0.00079109252 -0.00043569435 -0.0012253639][0.0033446671 0.0040213182 0.0044955742 0.004882846 0.0052284296 0.0054230723 0.0052862135 0.0047900523 0.0040991791 0.0033476865 0.0024111103 0.0012536779 -6.2189065e-06 -0.000977231 -0.0015208867][0.0019739629 0.0027895556 0.0035011149 0.0041425405 0.004716442 0.0050879125 0.005067681 0.0046120621 0.0038587344 0.002952741 0.0018713091 0.000673839 -0.00047866255 -0.0012782397 -0.0016703381][0.00061356521 0.0013870924 0.0021980288 0.0030508055 0.0039129988 0.0046140831 0.0049148463 0.0046838783 0.00397525 0.0029379381 0.001684521 0.0004122284 -0.00068449485 -0.0013849379 -0.0017083038][-0.00049735256 0.00011183915 0.00087291107 0.001812148 0.0028942656 0.0039159297 0.0045745145 0.0046369531 0.0040526008 0.0029678252 0.0016132045 0.0003012846 -0.00074204663 -0.0013811504 -0.0016843764][-0.0011940864 -0.000769006 -0.00012764707 0.00077994179 0.001927836 0.0031068446 0.0039810948 0.0042564184 0.0038170246 0.0027979906 0.0014782083 0.00022349379 -0.00074068946 -0.0013349802 -0.0016420113][-0.0015083592 -0.0012124727 -0.00069954677 9.5779193e-05 0.0011496268 0.0022664429 0.0031378302 0.0034778477 0.0031585479 0.0023024995 0.0011647394 7.5980905e-05 -0.00076691166 -0.0013072352 -0.0016118709][-0.0016283763 -0.0013963797 -0.00097795972 -0.00032367278 0.00053679047 0.0014387231 0.0021383818 0.0024170186 0.0021801875 0.0015280083 0.0006508379 -0.00019799185 -0.00087546383 -0.0013357515 -0.0016141608][-0.0016935939 -0.001513961 -0.0011858869 -0.00067820749 -3.2042386e-05 0.00062161323 0.0011086398 0.0012862372 0.0011040404 0.00064164132 1.99707e-05 -0.0005856707 -0.0010818385 -0.0014326531 -0.0016528692][-0.0017536215 -0.0016371101 -0.0014160763 -0.001067822 -0.00062646868 -0.00018533727 0.00013578928 0.00024728046 0.00012089883 -0.00018643285 -0.00060089014 -0.0010042116 -0.0013351933 -0.0015683641 -0.001714556][-0.0017994591 -0.0017414311 -0.001621559 -0.0014213431 -0.00115564 -0.000877055 -0.00066281867 -0.00057803246 -0.00064726942 -0.00083590287 -0.0010986445 -0.0013529684 -0.0015553784 -0.0016895997 -0.001768289][-0.0018238745 -0.0018047242 -0.0017581034 -0.0016700848 -0.0015402171 -0.0013863619 -0.0012497578 -0.0011787147 -0.0011974373 -0.0012960893 -0.0014475209 -0.0015951698 -0.0017073648 -0.0017735995 -0.001806266][-0.0018306822 -0.0018272507 -0.00181629 -0.0017897261 -0.0017417541 -0.0016733623 -0.001600222 -0.0015489765 -0.0015404072 -0.0015796542 -0.0016527832 -0.0017278132 -0.0017838726 -0.0018129926 -0.0018238517]]...]
INFO - root - 2017-12-10 02:25:03.432803: step 80210, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.875 sec/batch; 61h:17m:24s remains)
INFO - root - 2017-12-10 02:25:11.935967: step 80220, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.863 sec/batch; 60h:29m:22s remains)
INFO - root - 2017-12-10 02:25:20.491076: step 80230, loss = 0.82, batch loss = 0.69 (9.9 examples/sec; 0.810 sec/batch; 56h:46m:36s remains)
INFO - root - 2017-12-10 02:25:29.140388: step 80240, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.856 sec/batch; 59h:58m:16s remains)
INFO - root - 2017-12-10 02:25:37.682557: step 80250, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.879 sec/batch; 61h:35m:52s remains)
INFO - root - 2017-12-10 02:25:46.376545: step 80260, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.856 sec/batch; 59h:58m:35s remains)
INFO - root - 2017-12-10 02:25:55.081659: step 80270, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.873 sec/batch; 61h:11m:49s remains)
INFO - root - 2017-12-10 02:26:03.746820: step 80280, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.854 sec/batch; 59h:49m:39s remains)
INFO - root - 2017-12-10 02:26:12.424718: step 80290, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.873 sec/batch; 61h:08m:53s remains)
INFO - root - 2017-12-10 02:26:20.939027: step 80300, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.865 sec/batch; 60h:35m:16s remains)
2017-12-10 02:26:21.869540: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0018390813 -0.0018379039 -0.001836881 -0.0018363781 -0.001836738 -0.0018378769 -0.0018391586 -0.0018405649 -0.0018415891 -0.0018422598 -0.0018425134 -0.0018422189 -0.0018409882 -0.0018389871 -0.0018364355][-0.0018381154 -0.0018371233 -0.0018361429 -0.0018356618 -0.0018358908 -0.001837033 -0.0018383531 -0.0018399147 -0.0018413246 -0.001842341 -0.0018426789 -0.001842287 -0.001840975 -0.0018388237 -0.0018361282][-0.0018378702 -0.0018366998 -0.0018355838 -0.0018349108 -0.0018352065 -0.0018363983 -0.0018378291 -0.0018394795 -0.0018411386 -0.001842325 -0.0018426863 -0.0018423168 -0.0018410305 -0.0018388563 -0.0018360765][-0.0018382379 -0.0018365488 -0.0018351433 -0.0018342466 -0.001834463 -0.0018357626 -0.001837472 -0.0018391769 -0.0018409748 -0.0018422597 -0.0018426479 -0.0018422388 -0.0018408751 -0.0018388091 -0.0018361788][-0.0018380887 -0.0018359052 -0.0018339898 -0.0018327741 -0.0018328545 -0.0018342255 -0.0018362763 -0.0018382798 -0.0018401985 -0.0018416239 -0.0018422815 -0.0018420737 -0.0018407654 -0.0018387168 -0.0018362418][-0.0018373431 -0.0018347853 -0.0018324113 -0.0018308257 -0.0018307711 -0.0018320443 -0.0018341829 -0.0018363412 -0.0018385261 -0.0018403169 -0.0018415054 -0.001841751 -0.0018406988 -0.0018388585 -0.0018365121][-0.0018356777 -0.0018327831 -0.001830345 -0.0018288605 -0.0018289551 -0.001830331 -0.00183259 -0.0018348313 -0.0018372944 -0.001839333 -0.0018408564 -0.0018413855 -0.0018405436 -0.0018387679 -0.0018364481][-0.0018336776 -0.0018305626 -0.0018281246 -0.0018270349 -0.0018274657 -0.0018291284 -0.0018315256 -0.0018339589 -0.0018366378 -0.0018389773 -0.0018406512 -0.0018412916 -0.0018406705 -0.0018390798 -0.0018367838][-0.0018318356 -0.0018286803 -0.0018263522 -0.0018256184 -0.0018264573 -0.0018283864 -0.001830982 -0.0018335721 -0.0018362883 -0.001838823 -0.0018405562 -0.0018413789 -0.0018407066 -0.0018390974 -0.00183686][-0.0018304912 -0.0018276725 -0.0018256285 -0.0018251577 -0.0018263879 -0.0018285718 -0.0018310947 -0.0018336066 -0.0018361359 -0.0018385261 -0.0018402135 -0.00184112 -0.0018405772 -0.0018390808 -0.0018368734][-0.0018300676 -0.0018277533 -0.0018262173 -0.0018258711 -0.00182721 -0.0018294675 -0.0018317677 -0.0018341107 -0.0018364862 -0.0018386452 -0.0018401553 -0.0018409095 -0.0018404488 -0.0018389581 -0.0018368163][-0.0018309891 -0.0018290292 -0.0018278903 -0.0018276727 -0.0018288103 -0.0018308158 -0.001832927 -0.0018350362 -0.0018371553 -0.0018391093 -0.0018403352 -0.0018409315 -0.001840424 -0.0018388574 -0.0018366133][-0.0018330619 -0.0018314661 -0.0018304522 -0.0018302064 -0.0018310483 -0.0018326066 -0.0018342871 -0.0018357837 -0.0018373512 -0.0018388794 -0.0018398148 -0.0018402473 -0.001839702 -0.0018383734 -0.0018363063][-0.001835641 -0.001834207 -0.0018331573 -0.0018327889 -0.0018333662 -0.0018344254 -0.0018356781 -0.0018367587 -0.0018378753 -0.001838911 -0.0018395495 -0.0018398224 -0.0018391061 -0.0018378027 -0.0018358229][-0.0018370639 -0.0018357714 -0.0018347043 -0.0018341946 -0.0018345615 -0.0018354002 -0.0018363808 -0.0018372565 -0.0018380138 -0.0018386687 -0.0018391043 -0.0018392388 -0.0018384558 -0.00183706 -0.001835116]]...]
INFO - root - 2017-12-10 02:26:30.586032: step 80310, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.881 sec/batch; 61h:42m:10s remains)
INFO - root - 2017-12-10 02:26:39.134398: step 80320, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.852 sec/batch; 59h:41m:36s remains)
INFO - root - 2017-12-10 02:26:47.968189: step 80330, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.870 sec/batch; 60h:58m:14s remains)
INFO - root - 2017-12-10 02:26:56.520023: step 80340, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.854 sec/batch; 59h:49m:43s remains)
INFO - root - 2017-12-10 02:27:04.994457: step 80350, loss = 0.82, batch loss = 0.69 (9.7 examples/sec; 0.828 sec/batch; 57h:59m:44s remains)
INFO - root - 2017-12-10 02:27:13.641645: step 80360, loss = 0.82, batch loss = 0.69 (9.9 examples/sec; 0.807 sec/batch; 56h:32m:04s remains)
INFO - root - 2017-12-10 02:27:22.204094: step 80370, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.842 sec/batch; 58h:58m:19s remains)
INFO - root - 2017-12-10 02:27:30.822636: step 80380, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 60h:21m:51s remains)
INFO - root - 2017-12-10 02:27:39.442771: step 80390, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.842 sec/batch; 59h:00m:02s remains)
INFO - root - 2017-12-10 02:27:48.077122: step 80400, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 60h:47m:20s remains)
2017-12-10 02:27:48.963200: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0014017504 -0.0010243806 -0.00039087352 0.00041183352 0.0012046072 0.0017011318 0.0017169908 0.0012589815 0.00048920175 -0.00028802676 -0.00089789153 -0.0012763765 -0.0014678036 -0.00153858 -0.0015606259][-0.00088026415 2.0059524e-06 0.0012940123 0.0028407625 0.0042755068 0.0051217666 0.0050683068 0.0041224468 0.0026278156 0.0010956476 -0.000109007 -0.0008457416 -0.0011872343 -0.0012733482 -0.0012716807][-0.00010213081 0.0016294034 0.004064241 0.006845254 0.0092753256 0.01063994 0.010499897 0.0089040669 0.0063963379 0.0037171389 0.0015047501 4.5193941e-05 -0.00071449054 -0.00097412657 -0.00099842146][0.00076373282 0.0035449169 0.0074328217 0.011833768 0.015633374 0.0177707 0.017642982 0.015348704 0.011648965 0.0075485469 0.0039907089 0.0014688357 1.1401949e-05 -0.00059035944 -0.0006849902][0.0014427261 0.0051399823 0.0104443 0.016539438 0.021901095 0.025106439 0.025299598 0.022524269 0.017696638 0.012097646 0.0070361472 0.0032782047 0.0010299567 0.00012110441 0.00010181835][0.0019067138 0.0060570841 0.012301818 0.019747932 0.026602615 0.031056061 0.03194667 0.029163066 0.023648193 0.016880536 0.010487572 0.0055760923 0.0026281434 0.001578617 0.0018697126][0.0020027878 0.0060257656 0.012473924 0.02058677 0.02852351 0.034180742 0.036075823 0.033837061 0.028318722 0.021064825 0.013908432 0.0082815159 0.0049808682 0.0040873429 0.0049222829][0.0017786726 0.00521212 0.01108438 0.018949743 0.027189417 0.03364446 0.036594763 0.035373498 0.0306108 0.023747379 0.01665136 0.010970081 0.0077731661 0.0072728354 0.0087276511][0.001182709 0.0038025961 0.0085643139 0.015362301 0.022979913 0.029487729 0.033147 0.033062283 0.029569931 0.023866186 0.017670389 0.012642885 0.0099549042 0.0099611934 0.011960509][0.00036710047 0.0021466459 0.0055757235 0.010809656 0.017068449 0.022831537 0.026562743 0.027322637 0.025190046 0.021044379 0.016302297 0.0124257 0.010513833 0.010993768 0.013310603][-0.00045588089 0.0006247774 0.002804813 0.0063522724 0.010869351 0.015321321 0.018514948 0.019615645 0.018549776 0.015917338 0.012770739 0.010207307 0.0090892855 0.0098657114 0.012169468][-0.0011382544 -0.00056173012 0.000650918 0.0027459459 0.0055739721 0.0085382843 0.010844191 0.011861882 0.011457663 0.01000102 0.00819012 0.0067469361 0.0062472583 0.0070869033 0.0090912478][-0.0015618585 -0.0012970801 -0.00071892026 0.00033499522 0.0018430062 0.0035154042 0.0049026068 0.0056037153 0.0055068871 0.0048151421 0.0039334162 0.0032645888 0.0031294585 0.0038158176 0.0052953181][-0.0017615135 -0.0016717922 -0.0014471583 -0.001007623 -0.00034023006 0.00043921208 0.0011176291 0.0014811869 0.0014657102 0.0011752072 0.00081187452 0.00056824938 0.00059229892 0.0010507762 0.0019784723][-0.0018128261 -0.001794699 -0.001734365 -0.0015969226 -0.0013677783 -0.0010839265 -0.00082771515 -0.00068796892 -0.00069352717 -0.00080088445 -0.00092651363 -0.000995127 -0.00094954087 -0.00070854207 -0.00022634619]]...]
INFO - root - 2017-12-10 02:27:57.518863: step 80410, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.850 sec/batch; 59h:29m:42s remains)
INFO - root - 2017-12-10 02:28:06.018702: step 80420, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.881 sec/batch; 61h:39m:25s remains)
INFO - root - 2017-12-10 02:28:14.778315: step 80430, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 60h:43m:59s remains)
INFO - root - 2017-12-10 02:28:23.453078: step 80440, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.872 sec/batch; 61h:04m:20s remains)
INFO - root - 2017-12-10 02:28:31.964132: step 80450, loss = 0.82, batch loss = 0.69 (10.8 examples/sec; 0.739 sec/batch; 51h:44m:14s remains)
INFO - root - 2017-12-10 02:28:40.700209: step 80460, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.836 sec/batch; 58h:32m:12s remains)
INFO - root - 2017-12-10 02:28:49.436969: step 80470, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.881 sec/batch; 61h:39m:19s remains)
INFO - root - 2017-12-10 02:28:58.051358: step 80480, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.866 sec/batch; 60h:37m:39s remains)
INFO - root - 2017-12-10 02:29:06.834254: step 80490, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.874 sec/batch; 61h:10m:36s remains)
INFO - root - 2017-12-10 02:29:15.436409: step 80500, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.882 sec/batch; 61h:42m:31s remains)
2017-12-10 02:29:16.312694: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.00044484646 -0.00018241513 8.4769563e-05 0.00035186077 0.00061178335 0.0008541547 0.0010819981 0.0013002212 0.0014603481 0.0015836988 0.001623463 0.0016428792 0.001670862 0.0017842989 0.001922615][-0.00020204787 4.6447734e-05 0.00030261173 0.00056030287 0.000810695 0.0010430893 0.0012604845 0.0014733303 0.0016572861 0.0018089876 0.0018575458 0.0018569132 0.0018398283 0.0019277086 0.0021031369][6.5319822e-05 0.00026664475 0.00048540288 0.00072151015 0.00096533878 0.0012016877 0.0014323414 0.0016681886 0.0018986015 0.0020902283 0.0021553622 0.0021317583 0.0020801411 0.0021513677 0.0023554768][0.00033754052 0.00045295188 0.00060180796 0.00079572422 0.0010327696 0.0012989143 0.0015867437 0.0018913787 0.0021907031 0.0024369815 0.0025321958 0.002498141 0.0024378877 0.0025050165 0.0027316869][0.00064360735 0.000647033 0.00069890579 0.00083500415 0.0010720909 0.0013968024 0.001781002 0.002182459 0.0025470541 0.0028258753 0.0029320288 0.0028845361 0.0028163036 0.002886516 0.00312316][0.00098682393 0.0008832569 0.00084226753 0.00093572855 0.0011965529 0.0016024826 0.0020866557 0.002565254 0.0029564155 0.0032123094 0.0032877168 0.0032119984 0.0031123855 0.0031582052 0.0033835331][0.0012539962 0.0010822663 0.0009897534 0.0010808035 0.0013884796 0.0018732451 0.0024229269 0.0029219165 0.0032761521 0.0034570205 0.0034537627 0.0033279732 0.0031961813 0.0032051587 0.0034099668][0.0012280369 0.0010403056 0.00094805413 0.0010745752 0.0014331198 0.0019629356 0.0025222716 0.0029858681 0.0032615229 0.0033439798 0.0032508005 0.0030759578 0.0029201 0.002911821 0.0031106737][0.00076633052 0.00060969859 0.00056144164 0.00073337473 0.0011203877 0.0016510872 0.0021763672 0.002574244 0.0027636513 0.0027523846 0.0025770613 0.0023617684 0.0022014133 0.0022084462 0.0024063215][-2.8284849e-05 -0.00012371293 -0.00010951352 9.8263146e-05 0.00048676028 0.00097448856 0.0014298033 0.0017427987 0.0018472074 0.00175479 0.0015232625 0.0012753241 0.0011228736 0.0011481916 0.0013462309][-0.00085857883 -0.00089499826 -0.00083298341 -0.00061541365 -0.00026030233 0.00015128858 0.00051170343 0.00072401215 0.00074468472 0.00059296971 0.00034099969 9.8390738e-05 -3.9860955e-05 -1.4874269e-05 0.00016122393][-0.0014523605 -0.0014511714 -0.0013733215 -0.0011837715 -0.00090104947 -0.00059488451 -0.0003506667 -0.00024103164 -0.00028487225 -0.00045389053 -0.00067716162 -0.00087196054 -0.0009784787 -0.00096475735 -0.00083740777][-0.0017409257 -0.0017293184 -0.0016683273 -0.0015377641 -0.0013545055 -0.001167675 -0.0010365213 -0.0010032785 -0.0010692073 -0.0012034865 -0.001353404 -0.0014739414 -0.0015361317 -0.0015342296 -0.0014662482][-0.001830879 -0.0018235936 -0.0017914311 -0.0017244659 -0.0016345221 -0.0015480306 -0.0014976158 -0.0014997476 -0.0015488303 -0.0016226004 -0.0016935237 -0.0017444538 -0.0017689025 -0.0017699746 -0.0017453308][-0.0018434317 -0.0018409085 -0.0018297863 -0.0018063164 -0.0017760023 -0.0017487376 -0.0017370385 -0.0017438278 -0.0017645464 -0.001789405 -0.0018094892 -0.0018216156 -0.0018262442 -0.0018260991 -0.0018204082]]...]
INFO - root - 2017-12-10 02:29:24.853848: step 80510, loss = 0.82, batch loss = 0.69 (9.8 examples/sec; 0.817 sec/batch; 57h:11m:03s remains)
INFO - root - 2017-12-10 02:29:33.340103: step 80520, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.878 sec/batch; 61h:26m:22s remains)
INFO - root - 2017-12-10 02:29:41.883209: step 80530, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.865 sec/batch; 60h:34m:36s remains)
INFO - root - 2017-12-10 02:29:50.520831: step 80540, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.883 sec/batch; 61h:46m:55s remains)
INFO - root - 2017-12-10 02:29:59.052295: step 80550, loss = 0.82, batch loss = 0.69 (11.0 examples/sec; 0.728 sec/batch; 50h:55m:36s remains)
INFO - root - 2017-12-10 02:30:07.713617: step 80560, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.839 sec/batch; 58h:43m:42s remains)
INFO - root - 2017-12-10 02:30:16.409142: step 80570, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.861 sec/batch; 60h:13m:18s remains)
INFO - root - 2017-12-10 02:30:25.037409: step 80580, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.886 sec/batch; 61h:59m:25s remains)
INFO - root - 2017-12-10 02:30:33.613468: step 80590, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.846 sec/batch; 59h:12m:40s remains)
INFO - root - 2017-12-10 02:30:42.209772: step 80600, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.861 sec/batch; 60h:12m:43s remains)
2017-12-10 02:30:43.169437: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0018145097 -0.0018128443 -0.0018122172 -0.0018120023 -0.0018121426 -0.001812537 -0.0018131272 -0.001813411 -0.0018133105 -0.0018130009 -0.0018125243 -0.0018119605 -0.0018114598 -0.0018109536 -0.0018108091][-0.0018140258 -0.0018124736 -0.0018119476 -0.0018118491 -0.0018121037 -0.0018125935 -0.0018131542 -0.0018133947 -0.001813193 -0.0018128093 -0.0018122392 -0.0018116065 -0.0018110009 -0.0018104198 -0.0018101392][-0.0018140242 -0.0018126398 -0.0018122524 -0.0018123529 -0.0018127429 -0.0018132895 -0.0018137764 -0.001813926 -0.0018136193 -0.0018131642 -0.0018125798 -0.0018119493 -0.0018113465 -0.0018107443 -0.0018103403][-0.0018140656 -0.001812796 -0.0018125221 -0.0018127678 -0.001813241 -0.0018138133 -0.0018142008 -0.0018142089 -0.0018138093 -0.0018133291 -0.0018128095 -0.0018122464 -0.0018117233 -0.0018111625 -0.0018106773][-0.0018140565 -0.0018129239 -0.0018127623 -0.0018131278 -0.0018136471 -0.0018141993 -0.0018144626 -0.0018143117 -0.0018137733 -0.0018132274 -0.0018127479 -0.0018122951 -0.0018118815 -0.0018114144 -0.0018109756][-0.0018141154 -0.0018130635 -0.001812993 -0.0018134015 -0.001813865 -0.0018142845 -0.0018143804 -0.0018140924 -0.0018134888 -0.0018129548 -0.0018125582 -0.001812207 -0.0018118955 -0.0018115557 -0.0018111867][-0.001814199 -0.0018132337 -0.0018132211 -0.001813621 -0.0018139679 -0.0018142011 -0.0018141092 -0.0018136842 -0.0018130463 -0.0018125389 -0.0018122221 -0.0018119607 -0.0018117845 -0.0018115659 -0.0018112668][-0.0018143028 -0.0018133789 -0.00181335 -0.0018136809 -0.0018138706 -0.001813934 -0.0018136885 -0.0018131188 -0.0018124582 -0.001812004 -0.0018117538 -0.0018115602 -0.0018114431 -0.0018113182 -0.0018111154][-0.0018142667 -0.0018133554 -0.0018132803 -0.0018134669 -0.0018134823 -0.0018133728 -0.0018130454 -0.0018124864 -0.0018119417 -0.0018116042 -0.001811429 -0.0018112368 -0.0018110628 -0.0018108933 -0.0018107521][-0.0018140564 -0.0018131582 -0.0018129722 -0.0018129692 -0.0018127763 -0.0018124779 -0.0018121124 -0.0018117182 -0.0018114377 -0.0018112885 -0.0018111891 -0.0018109754 -0.0018107464 -0.0018105285 -0.0018104325][-0.0018137705 -0.0018128259 -0.0018125366 -0.0018123817 -0.0018120144 -0.0018115457 -0.001811153 -0.0018109131 -0.0018108902 -0.001810942 -0.001810954 -0.0018107694 -0.001810557 -0.0018103726 -0.0018102808][-0.0018135397 -0.0018124612 -0.0018120564 -0.001811788 -0.0018113294 -0.0018108223 -0.0018104893 -0.0018103683 -0.0018105124 -0.001810675 -0.0018107678 -0.0018106411 -0.0018104211 -0.001810209 -0.0018101238][-0.0018134388 -0.0018123019 -0.001811812 -0.0018114914 -0.0018110295 -0.0018105728 -0.0018103176 -0.0018102904 -0.0018105125 -0.0018107493 -0.001810934 -0.001810863 -0.0018106439 -0.0018103682 -0.0018102737][-0.0018136044 -0.0018124423 -0.0018119091 -0.001811592 -0.0018111895 -0.0018108115 -0.0018106289 -0.0018106687 -0.0018109303 -0.0018111957 -0.0018114154 -0.0018113912 -0.0018112004 -0.0018109211 -0.0018108469][-0.001814045 -0.001812861 -0.0018122967 -0.0018120438 -0.0018117605 -0.0018115157 -0.0018113982 -0.0018114573 -0.0018117109 -0.0018119694 -0.0018121637 -0.0018121577 -0.0018120242 -0.0018118218 -0.0018117578]]...]
INFO - root - 2017-12-10 02:30:51.834626: step 80610, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.848 sec/batch; 59h:18m:40s remains)
INFO - root - 2017-12-10 02:31:00.411658: step 80620, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.880 sec/batch; 61h:33m:08s remains)
INFO - root - 2017-12-10 02:31:09.183888: step 80630, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.882 sec/batch; 61h:42m:03s remains)
INFO - root - 2017-12-10 02:31:17.874424: step 80640, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 60h:58m:01s remains)
INFO - root - 2017-12-10 02:31:26.374072: step 80650, loss = 0.82, batch loss = 0.69 (11.3 examples/sec; 0.706 sec/batch; 49h:23m:57s remains)
INFO - root - 2017-12-10 02:31:34.922640: step 80660, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 60h:38m:05s remains)
INFO - root - 2017-12-10 02:31:43.517802: step 80670, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.834 sec/batch; 58h:20m:06s remains)
INFO - root - 2017-12-10 02:31:52.099210: step 80680, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.870 sec/batch; 60h:49m:56s remains)
INFO - root - 2017-12-10 02:32:00.728998: step 80690, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.859 sec/batch; 60h:06m:35s remains)
INFO - root - 2017-12-10 02:32:09.137078: step 80700, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.846 sec/batch; 59h:09m:02s remains)
2017-12-10 02:32:10.069556: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.00087966857 -0.0010648059 -0.0010385903 -0.00079407007 -0.00030128006 0.00054265757 0.00164337 0.0027448926 0.0036690468 0.0043326155 0.0050471937 0.0055769761 0.0058992985 0.0057362984 0.005190745][0.00013314735 -0.00020420051 -0.00023416511 -3.6514248e-05 0.00057148386 0.0017658399 0.0034839665 0.0053803287 0.0071648569 0.0086808819 0.010019802 0.011265473 0.012006455 0.011952165 0.011038367][0.0017165906 0.0012491859 0.0012021553 0.0014361666 0.0022388482 0.0039222455 0.0064752842 0.0096327728 0.012894941 0.01558743 0.017674873 0.019405017 0.020413937 0.020514237 0.019247][0.003701949 0.0032433742 0.0032497067 0.0035674856 0.0046826554 0.0069191963 0.010436716 0.015092534 0.02006774 0.024342524 0.027410109 0.02962162 0.030849222 0.030763354 0.028939394][0.00582902 0.005513262 0.0056160213 0.0060762237 0.007508134 0.010289716 0.014701624 0.020636141 0.027126847 0.032836061 0.036856998 0.03941112 0.040700823 0.040428717 0.038171031][0.0076202224 0.0074747861 0.0075984416 0.0081868274 0.0097841518 0.012915103 0.01790343 0.024585253 0.031938896 0.038592938 0.043251038 0.046019297 0.047332082 0.046860375 0.044290271][0.0082216514 0.0083482545 0.0084947357 0.0091386531 0.01068805 0.013760181 0.018637525 0.025317721 0.032742366 0.039740071 0.04488907 0.048099138 0.049542177 0.048985522 0.046294093][0.0075789397 0.0077801873 0.0078016459 0.0084586814 0.0097913509 0.012431765 0.016583787 0.022523243 0.029353555 0.036114179 0.041662302 0.045485068 0.047435109 0.047154114 0.044557068][0.0057064281 0.0059745717 0.0059786807 0.0065157865 0.0074833683 0.0095119281 0.012640392 0.017311815 0.022921607 0.028956262 0.034325153 0.038700871 0.041169003 0.041459285 0.039392229][0.0033018882 0.0035599018 0.0036400496 0.0041506435 0.004869462 0.0061559877 0.0081164408 0.011357744 0.015453172 0.020198435 0.024829794 0.028994085 0.031632457 0.032462005 0.031239837][0.00095579249 0.0011995166 0.0013456811 0.0017078455 0.0021973392 0.0029692147 0.0040635187 0.0059477948 0.0084849307 0.011731111 0.015137375 0.018484859 0.020735452 0.021755045 0.021296032][-0.00083605666 -0.00064602308 -0.00041813636 -0.00016761466 0.00012654869 0.00045021356 0.00094573374 0.0019026332 0.0032354258 0.0050448235 0.0070844772 0.0092418808 0.010799928 0.011592957 0.011549161][-0.0016477911 -0.001620434 -0.0015446318 -0.0014212527 -0.0012688611 -0.0010857882 -0.00086851662 -0.00049703009 5.0019706e-05 0.00083919743 0.0017846654 0.0028344253 0.0036385581 0.0041301637 0.0042152628][-0.0018068105 -0.0018061702 -0.0018051267 -0.0018018837 -0.0017959661 -0.0017629826 -0.0016978551 -0.0015962938 -0.0014476123 -0.0011882405 -0.00086812803 -0.0004985081 -0.00019873737 4.046713e-06 7.3972391e-05][-0.0018108749 -0.0018095312 -0.0018095179 -0.0018100854 -0.0018113781 -0.001812333 -0.0018129433 -0.0018141783 -0.0018097546 -0.0017823783 -0.0017289255 -0.0016394445 -0.0015567868 -0.0014962321 -0.0014624146]]...]
INFO - root - 2017-12-10 02:32:18.658452: step 80710, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.843 sec/batch; 58h:56m:25s remains)
INFO - root - 2017-12-10 02:32:27.060209: step 80720, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.848 sec/batch; 59h:18m:55s remains)
INFO - root - 2017-12-10 02:32:35.534286: step 80730, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.835 sec/batch; 58h:23m:22s remains)
INFO - root - 2017-12-10 02:32:44.031427: step 80740, loss = 0.83, batch loss = 0.70 (9.8 examples/sec; 0.814 sec/batch; 56h:57m:23s remains)
INFO - root - 2017-12-10 02:32:52.511995: step 80750, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.859 sec/batch; 60h:04m:16s remains)
INFO - root - 2017-12-10 02:33:00.900999: step 80760, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.861 sec/batch; 60h:11m:38s remains)
INFO - root - 2017-12-10 02:33:09.559522: step 80770, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.883 sec/batch; 61h:45m:10s remains)
INFO - root - 2017-12-10 02:33:18.182948: step 80780, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.851 sec/batch; 59h:32m:18s remains)
INFO - root - 2017-12-10 02:33:26.859669: step 80790, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.841 sec/batch; 58h:48m:13s remains)
INFO - root - 2017-12-10 02:33:35.433089: step 80800, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.843 sec/batch; 58h:55m:01s remains)
2017-12-10 02:33:36.376976: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0018017614 -0.0018000868 -0.0017997539 -0.0017996519 -0.0017995604 -0.0017995225 -0.0017995035 -0.001799505 -0.0017995222 -0.0017995713 -0.0017996313 -0.0017996881 -0.0017997649 -0.0017998915 -0.001800017][-0.0018003485 -0.0017986464 -0.0017982969 -0.0017981796 -0.0017980677 -0.0017980357 -0.001798025 -0.0017980388 -0.0017981105 -0.0017982215 -0.0017983499 -0.0017984938 -0.0017986617 -0.0017988508 -0.0017990165][-0.0017995692 -0.0017979289 -0.0017975782 -0.0017974434 -0.0017972706 -0.0017971498 -0.0017970445 -0.0017970128 -0.0017971074 -0.0017973009 -0.0017975633 -0.0017978982 -0.0017982738 -0.001798638 -0.0017989002][-0.0017986308 -0.0017971123 -0.0017967589 -0.0017966036 -0.0017963824 -0.0017961568 -0.0017959328 -0.0017958218 -0.0017959179 -0.001796236 -0.0017966965 -0.001797298 -0.0017979362 -0.0017985009 -0.0017988821][-0.0017976433 -0.0017961895 -0.0017958131 -0.0017955871 -0.0017952879 -0.0017949156 -0.0017945839 -0.0017944408 -0.0017946137 -0.0017951144 -0.0017958037 -0.0017966973 -0.0017976183 -0.0017983764 -0.0017988569][-0.0017967519 -0.0017954044 -0.0017950833 -0.0017948127 -0.0017943633 -0.0017937655 -0.0017932669 -0.0017930721 -0.0017933438 -0.0017940346 -0.0017949942 -0.0017962103 -0.0017974325 -0.0017983788 -0.001798927][-0.0017958917 -0.0017947558 -0.0017945761 -0.0017943395 -0.0017937606 -0.0017928891 -0.0017921572 -0.0017918685 -0.0017922278 -0.0017931348 -0.0017943963 -0.0017959322 -0.0017974089 -0.0017984799 -0.0017990681][-0.0017955007 -0.0017945821 -0.0017945754 -0.0017943464 -0.0017936251 -0.0017925146 -0.0017916156 -0.0017912753 -0.0017917803 -0.0017929628 -0.0017944609 -0.0017961316 -0.0017976474 -0.0017986796 -0.0017991989][-0.0017961666 -0.0017953314 -0.0017954579 -0.0017952425 -0.0017944548 -0.0017932873 -0.0017923632 -0.0017920206 -0.0017926084 -0.0017938649 -0.0017953865 -0.0017969375 -0.0017982107 -0.0017990158 -0.0017993689][-0.0017977179 -0.0017968348 -0.0017969713 -0.0017968112 -0.0017961788 -0.0017952898 -0.0017946106 -0.0017943304 -0.0017947599 -0.0017957116 -0.0017968694 -0.001797988 -0.0017988667 -0.0017993605 -0.0017995004][-0.0017992996 -0.0017982707 -0.0017983174 -0.0017982759 -0.0017979222 -0.0017974214 -0.0017970389 -0.0017968536 -0.001797067 -0.0017975599 -0.0017981969 -0.0017988014 -0.0017992515 -0.0017994622 -0.0017994569][-0.0018000077 -0.0017987993 -0.0017987685 -0.0017988186 -0.0017986933 -0.0017985009 -0.0017983835 -0.00179833 -0.0017984174 -0.0017986065 -0.0017988675 -0.0017991211 -0.0017992717 -0.0017993218 -0.0017992639][-0.0018000579 -0.0017987365 -0.0017985939 -0.0017986685 -0.0017986656 -0.0017986754 -0.0017987281 -0.00179876 -0.0017988202 -0.0017989002 -0.001799012 -0.0017990995 -0.0017991209 -0.0017991151 -0.001799053][-0.001800066 -0.0017986258 -0.0017983832 -0.001798453 -0.0017985116 -0.0017986085 -0.0017987033 -0.0017987697 -0.0017988449 -0.0017989207 -0.0017989909 -0.0017990168 -0.0017989938 -0.0017989568 -0.0017988716][-0.0018000978 -0.001798559 -0.0017982046 -0.001798266 -0.0017983537 -0.0017984782 -0.0017985993 -0.0017987068 -0.0017988145 -0.0017989108 -0.0017989718 -0.0017989723 -0.0017989181 -0.0017988313 -0.0017987181]]...]
INFO - root - 2017-12-10 02:33:45.037930: step 80810, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.853 sec/batch; 59h:39m:21s remains)
INFO - root - 2017-12-10 02:33:53.510275: step 80820, loss = 0.82, batch loss = 0.69 (11.2 examples/sec; 0.715 sec/batch; 49h:57m:56s remains)
INFO - root - 2017-12-10 02:34:02.143372: step 80830, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.876 sec/batch; 61h:14m:43s remains)
INFO - root - 2017-12-10 02:34:10.942411: step 80840, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.877 sec/batch; 61h:16m:34s remains)
INFO - root - 2017-12-10 02:34:19.630430: step 80850, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.866 sec/batch; 60h:31m:54s remains)
INFO - root - 2017-12-10 02:34:28.349637: step 80860, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.894 sec/batch; 62h:30m:55s remains)
INFO - root - 2017-12-10 02:34:37.021100: step 80870, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.879 sec/batch; 61h:26m:10s remains)
INFO - root - 2017-12-10 02:34:45.720097: step 80880, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.866 sec/batch; 60h:33m:33s remains)
INFO - root - 2017-12-10 02:34:54.615384: step 80890, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.884 sec/batch; 61h:47m:58s remains)
INFO - root - 2017-12-10 02:35:03.218070: step 80900, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 60h:22m:25s remains)
2017-12-10 02:35:04.084342: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.027606938 0.028012529 0.0280447 0.02831378 0.028610781 0.028696137 0.02859534 0.028612491 0.028580891 0.028124876 0.027417339 0.026645573 0.0259575 0.025400793 0.024704318][0.030054197 0.03087702 0.031277712 0.031809278 0.031890359 0.031643845 0.03135911 0.03127449 0.031169854 0.030873325 0.030331329 0.02973124 0.02920619 0.0284423 0.027387241][0.03022144 0.031293675 0.031943347 0.032715186 0.032792293 0.032393366 0.0318831 0.031583216 0.031343415 0.030979946 0.030559625 0.03023706 0.029794928 0.028995054 0.027820405][0.030019686 0.032039925 0.033289447 0.034271248 0.034330837 0.033755641 0.032929216 0.032175798 0.0314283 0.03079829 0.030286204 0.030171033 0.030026451 0.029417951 0.02824489][0.029368334 0.032338277 0.034065504 0.035468526 0.035750065 0.034978565 0.033521555 0.032045834 0.030544534 0.029492464 0.029066807 0.029286174 0.029524025 0.029172488 0.028083276][0.029140292 0.032599773 0.034671314 0.036157273 0.0361432 0.034627154 0.032114193 0.029532094 0.027297517 0.026185844 0.026187709 0.027086021 0.027949629 0.028004607 0.027117781][0.029112607 0.03256521 0.034587305 0.035938412 0.0353416 0.03263244 0.028599655 0.024549453 0.021481734 0.020391682 0.0210411 0.022916261 0.024752416 0.02560552 0.025230493][0.028095938 0.031789206 0.033511326 0.034320369 0.032959826 0.029204646 0.023894502 0.018661562 0.014884186 0.013509559 0.014394988 0.016863631 0.019458013 0.021127276 0.021359552][0.025540495 0.028832249 0.029942609 0.030061763 0.028025057 0.023719417 0.018054415 0.012623688 0.0088526541 0.0073829428 0.0080016293 0.010066801 0.012501057 0.0144264 0.015053072][0.020875394 0.023075186 0.023552334 0.023006598 0.020589573 0.01639816 0.011352755 0.0068412353 0.0038971822 0.0027289432 0.0030870191 0.0044234958 0.0061140605 0.0075658173 0.00805551][0.01499764 0.016435143 0.016388804 0.01537303 0.012976061 0.009446214 0.00561409 0.0024680994 0.00059844658 -5.6570978e-05 0.0001759337 0.00093003677 0.0019206383 0.0027542077 0.0029583774][0.0089504039 0.0099022081 0.0097908741 0.0088634165 0.0069049313 0.0042597079 0.0016840635 -0.00016975473 -0.0010927781 -0.0013533273 -0.0012052616 -0.00083935645 -0.00036364957 2.4472363e-05 5.9297425e-05][0.0041305884 0.0045603509 0.0044270274 0.0037662755 0.002475475 0.00088108249 -0.00050224061 -0.0013576705 -0.0016951213 -0.001761363 -0.0017078179 -0.001600551 -0.0014526509 -0.0013287017 -0.0013300168][0.00064808794 0.00076389161 0.00066817168 0.00033429975 -0.00026757352 -0.00095134112 -0.0014732102 -0.0017326998 -0.0018009199 -0.0018071027 -0.001800146 -0.0017878861 -0.0017685215 -0.0017496725 -0.0017484142][-0.0012010327 -0.0011887902 -0.0012287837 -0.0013317736 -0.0014977654 -0.0016649792 -0.0017692116 -0.0018060175 -0.0018081213 -0.0018074408 -0.0018068106 -0.0018059425 -0.0018048619 -0.0018059423 -0.0018074695]]...]
INFO - root - 2017-12-10 02:35:12.922073: step 80910, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.895 sec/batch; 62h:31m:48s remains)
INFO - root - 2017-12-10 02:35:21.722109: step 80920, loss = 0.82, batch loss = 0.69 (9.7 examples/sec; 0.825 sec/batch; 57h:41m:09s remains)
INFO - root - 2017-12-10 02:35:30.448426: step 80930, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.851 sec/batch; 59h:26m:32s remains)
INFO - root - 2017-12-10 02:35:39.065196: step 80940, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.883 sec/batch; 61h:42m:06s remains)
INFO - root - 2017-12-10 02:35:47.648206: step 80950, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.874 sec/batch; 61h:03m:03s remains)
INFO - root - 2017-12-10 02:35:56.211966: step 80960, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.899 sec/batch; 62h:48m:09s remains)
INFO - root - 2017-12-10 02:36:04.788458: step 80970, loss = 0.83, batch loss = 0.70 (9.3 examples/sec; 0.862 sec/batch; 60h:13m:43s remains)
INFO - root - 2017-12-10 02:36:13.542137: step 80980, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.861 sec/batch; 60h:09m:52s remains)
INFO - root - 2017-12-10 02:36:22.178992: step 80990, loss = 0.82, batch loss = 0.69 (8.7 examples/sec; 0.918 sec/batch; 64h:07m:53s remains)
INFO - root - 2017-12-10 02:36:30.796234: step 81000, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.878 sec/batch; 61h:18m:46s remains)
2017-12-10 02:36:31.747399: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.07740055 0.07770972 0.076691911 0.074383616 0.071141832 0.067735553 0.064088948 0.061261214 0.059020579 0.057145488 0.055651408 0.053204969 0.050102245 0.046800252 0.044111736][0.07778433 0.078423515 0.077649452 0.075233631 0.071680345 0.068099946 0.064816974 0.06312672 0.062171072 0.061659671 0.061428487 0.060041636 0.057586543 0.05403645 0.050549377][0.07694561 0.077943839 0.07720273 0.07464347 0.070835717 0.067225531 0.064573243 0.0641155 0.064646773 0.065843038 0.067465425 0.067641415 0.066209331 0.062741883 0.05853124][0.076472856 0.07735198 0.076090112 0.07289993 0.068626337 0.065064318 0.063260958 0.06415505 0.066137135 0.068766288 0.072001792 0.074143745 0.074392766 0.071457729 0.066967547][0.076237887 0.076489463 0.074474894 0.070739828 0.066003658 0.062395453 0.061183281 0.063127674 0.066227421 0.070271254 0.075167067 0.079008505 0.080762535 0.078694686 0.074180484][0.0764249 0.0762471 0.073490031 0.06918592 0.064071208 0.06042017 0.059664004 0.062371358 0.0662687 0.071298189 0.0776034 0.082989931 0.085905895 0.084244207 0.079627633][0.076954171 0.076936178 0.074096933 0.069734752 0.064545378 0.061022267 0.06039821 0.063221589 0.067289159 0.072854675 0.079953678 0.086438544 0.090402074 0.089416005 0.085096583][0.078083053 0.078721233 0.076235235 0.072456166 0.067526057 0.064177081 0.063561738 0.066354223 0.070510566 0.0761747 0.083555505 0.090251669 0.094338931 0.093639664 0.089555949][0.079940639 0.08178661 0.080290824 0.077322431 0.073150977 0.070352972 0.069838777 0.072461993 0.076404944 0.081869483 0.08874274 0.094952956 0.09855786 0.0975675 0.093606524][0.080811717 0.084449954 0.084688209 0.0836202 0.0809189 0.078728274 0.078496188 0.081044823 0.084756553 0.08955396 0.09504234 0.099591821 0.10145551 0.0995526 0.09555883][0.08032947 0.085994639 0.088392466 0.089197084 0.0882062 0.087170862 0.087496147 0.089897938 0.093194351 0.09666054 0.10011029 0.10255257 0.10244915 0.099390663 0.095280193][0.078811772 0.086108834 0.0904715 0.093239166 0.094035134 0.0940306 0.094721608 0.096757606 0.099097416 0.1010469 0.10235699 0.10268285 0.10101784 0.097310416 0.09328796][0.075981162 0.084234126 0.08986403 0.094151482 0.096521765 0.097555138 0.098578312 0.099971712 0.10133372 0.10165939 0.10119192 0.099986516 0.097316928 0.093386039 0.089552648][0.072199188 0.080886558 0.08725591 0.092398934 0.095627308 0.097283177 0.098386921 0.099135563 0.099478059 0.098509975 0.096996009 0.095147781 0.092373185 0.088855349 0.085607693][0.067535006 0.076162137 0.082498416 0.087979496 0.091704682 0.093799882 0.09495873 0.095181786 0.094807267 0.093291007 0.091409206 0.0893982 0.086791269 0.083853774 0.081229866]]...]
INFO - root - 2017-12-10 02:36:40.334117: step 81010, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.834 sec/batch; 58h:17m:08s remains)
INFO - root - 2017-12-10 02:36:48.924876: step 81020, loss = 0.82, batch loss = 0.69 (9.7 examples/sec; 0.828 sec/batch; 57h:51m:54s remains)
INFO - root - 2017-12-10 02:36:57.497110: step 81030, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.876 sec/batch; 61h:11m:25s remains)
INFO - root - 2017-12-10 02:37:06.213682: step 81040, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 60h:19m:32s remains)
INFO - root - 2017-12-10 02:37:14.878244: step 81050, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.876 sec/batch; 61h:09m:29s remains)
INFO - root - 2017-12-10 02:37:23.234888: step 81060, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 60h:10m:29s remains)
INFO - root - 2017-12-10 02:37:31.895738: step 81070, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.834 sec/batch; 58h:13m:18s remains)
INFO - root - 2017-12-10 02:37:40.519154: step 81080, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.878 sec/batch; 61h:18m:58s remains)
INFO - root - 2017-12-10 02:37:49.233113: step 81090, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.865 sec/batch; 60h:25m:30s remains)
INFO - root - 2017-12-10 02:37:57.944632: step 81100, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 60h:35m:10s remains)
2017-12-10 02:37:58.792561: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0018188827 -0.0018193061 -0.0018214157 -0.0018238863 -0.0018257144 -0.0018266541 -0.0018271215 -0.0018273655 -0.0018275597 -0.0018274442 -0.0018271774 -0.0018263498 -0.0018256491 -0.0018249397 -0.0018245898][-0.0018196559 -0.0018199468 -0.0018218766 -0.0018241745 -0.001825923 -0.0018269614 -0.0018273124 -0.0018272033 -0.0018271098 -0.001826814 -0.0018264522 -0.0018254163 -0.0018245982 -0.0018241314 -0.0018243593][-0.0018219599 -0.0018223783 -0.0018241552 -0.001826382 -0.0018280818 -0.0018289984 -0.0018289417 -0.0018282217 -0.0018274611 -0.0018266512 -0.0018258316 -0.0018245688 -0.0018233951 -0.0018228358 -0.0018234034][-0.0018249376 -0.0018259023 -0.0018275976 -0.0018295982 -0.0018312032 -0.001831788 -0.0018312343 -0.0018294971 -0.0018274889 -0.0018258073 -0.0018243705 -0.0018229766 -0.0018216289 -0.0018209193 -0.0018215552][-0.0018274381 -0.0018288987 -0.0018306404 -0.0018321238 -0.0018331382 -0.0018332874 -0.0018323448 -0.0018300129 -0.0018270951 -0.0018250711 -0.0018235398 -0.0018221055 -0.0018206511 -0.0018198682 -0.0018204049][-0.0018289181 -0.0018309038 -0.0018327364 -0.0018335525 -0.0018336835 -0.0018333842 -0.0018323296 -0.0018297876 -0.0018261585 -0.001823841 -0.0018226033 -0.0018213786 -0.0018201313 -0.0018192922 -0.0018194588][-0.0018296271 -0.0018319361 -0.0018338736 -0.0018342148 -0.0018336148 -0.0018327786 -0.0018316708 -0.0018289835 -0.0018249692 -0.0018225288 -0.001821686 -0.0018209695 -0.0018199667 -0.0018191225 -0.0018190483][-0.0018290631 -0.0018317418 -0.0018336554 -0.0018339782 -0.001833047 -0.0018317275 -0.0018304966 -0.0018280142 -0.0018239634 -0.0018212107 -0.0018206672 -0.0018205063 -0.0018198162 -0.0018191313 -0.0018189322][-0.0018281033 -0.0018307206 -0.0018326238 -0.0018330502 -0.0018324257 -0.0018312094 -0.0018297747 -0.0018274487 -0.0018233302 -0.0018199689 -0.0018191776 -0.0018194537 -0.0018192081 -0.0018188598 -0.0018187391][-0.001826616 -0.0018286996 -0.0018303997 -0.0018311597 -0.0018310691 -0.0018303244 -0.0018291861 -0.0018269817 -0.0018231291 -0.0018194468 -0.0018181729 -0.0018185127 -0.001818527 -0.001818342 -0.0018182962][-0.001825017 -0.0018265109 -0.0018279281 -0.0018288704 -0.0018290181 -0.0018285587 -0.0018278792 -0.0018261439 -0.0018230029 -0.0018197409 -0.0018181033 -0.0018182591 -0.0018183152 -0.0018181046 -0.0018180188][-0.0018237068 -0.0018246403 -0.001825696 -0.0018265758 -0.0018268385 -0.0018266006 -0.0018260862 -0.0018249112 -0.001822648 -0.0018202527 -0.0018186495 -0.0018183715 -0.0018183619 -0.0018181637 -0.0018179663][-0.0018225857 -0.0018229648 -0.0018237698 -0.001824599 -0.0018250659 -0.0018249413 -0.0018243963 -0.0018234976 -0.0018219985 -0.0018202405 -0.0018188143 -0.0018182604 -0.0018181843 -0.0018179878 -0.0018178021][-0.0018218977 -0.0018215796 -0.0018220245 -0.0018225444 -0.0018229372 -0.0018227702 -0.0018223558 -0.0018218552 -0.0018208942 -0.0018196534 -0.0018186008 -0.0018180882 -0.001817905 -0.001817772 -0.0018176505][-0.0018211219 -0.0018203937 -0.0018204781 -0.0018205653 -0.0018207181 -0.0018205917 -0.0018204873 -0.0018203725 -0.001819917 -0.0018191639 -0.001818364 -0.0018178757 -0.0018176902 -0.001817628 -0.0018175569]]...]
INFO - root - 2017-12-10 02:38:07.640941: step 81110, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.865 sec/batch; 60h:24m:12s remains)
INFO - root - 2017-12-10 02:38:16.450469: step 81120, loss = 0.82, batch loss = 0.69 (8.3 examples/sec; 0.961 sec/batch; 67h:05m:12s remains)
INFO - root - 2017-12-10 02:38:24.988353: step 81130, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.884 sec/batch; 61h:43m:52s remains)
INFO - root - 2017-12-10 02:38:33.654797: step 81140, loss = 0.82, batch loss = 0.69 (8.8 examples/sec; 0.909 sec/batch; 63h:27m:14s remains)
INFO - root - 2017-12-10 02:38:42.455006: step 81150, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.885 sec/batch; 61h:46m:47s remains)
INFO - root - 2017-12-10 02:38:51.026250: step 81160, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.850 sec/batch; 59h:22m:11s remains)
INFO - root - 2017-12-10 02:38:59.742728: step 81170, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.879 sec/batch; 61h:23m:34s remains)
INFO - root - 2017-12-10 02:39:08.524574: step 81180, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.879 sec/batch; 61h:22m:25s remains)
INFO - root - 2017-12-10 02:39:17.139342: step 81190, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.890 sec/batch; 62h:07m:48s remains)
INFO - root - 2017-12-10 02:39:25.761783: step 81200, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.852 sec/batch; 59h:28m:26s remains)
2017-12-10 02:39:26.652277: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.19124633 0.17579639 0.15550518 0.1338435 0.11402657 0.097941145 0.086867288 0.080043569 0.0773794 0.077052034 0.077439778 0.077502094 0.077259466 0.077835448 0.078534596][0.19451466 0.18416564 0.16864705 0.15128069 0.13487613 0.12147989 0.11257684 0.10747223 0.10546307 0.10462596 0.10345189 0.1008938 0.097136222 0.093532689 0.090260163][0.19224282 0.18895589 0.18047124 0.16927499 0.1581071 0.14838627 0.1414696 0.13769267 0.1357896 0.13364688 0.13023734 0.12458911 0.11712072 0.10894821 0.10130078][0.18580835 0.18978386 0.18877819 0.18443708 0.17935191 0.17383753 0.16960609 0.16693483 0.16514097 0.16179992 0.15594023 0.14728411 0.13630274 0.12411454 0.11236655][0.1752103 0.18616234 0.19243513 0.19483925 0.19554269 0.19429399 0.19262765 0.1911508 0.18965524 0.18551919 0.17808174 0.16743918 0.15393291 0.138596 0.12333521][0.16242093 0.17926921 0.19177394 0.19998349 0.20567426 0.20804653 0.20867234 0.20847833 0.20723267 0.20302694 0.19487362 0.18321791 0.16830254 0.15096276 0.13331258][0.15074873 0.17205308 0.1889185 0.20116396 0.21013346 0.21531361 0.21768571 0.21851009 0.21748866 0.21310161 0.20454356 0.19235137 0.17698826 0.15887269 0.14005087][0.14197484 0.16582723 0.18564488 0.20071898 0.21164308 0.21839447 0.22131483 0.22189182 0.21958014 0.21420145 0.20531255 0.19309562 0.17840734 0.1609351 0.1426592][0.13669063 0.16100912 0.18154925 0.19762605 0.20942852 0.21694066 0.21990404 0.21965922 0.21560439 0.20850356 0.1986071 0.18660763 0.17300189 0.15728135 0.14082386][0.13627043 0.15931055 0.17839488 0.19340646 0.20413072 0.21082991 0.21272145 0.21065636 0.20469536 0.19616017 0.18569298 0.17400396 0.16168933 0.14863259 0.1348258][0.1413473 0.16210249 0.17853528 0.19057341 0.19822004 0.20196387 0.20085783 0.19626811 0.18818322 0.17887765 0.16883309 0.15860501 0.14858162 0.13818501 0.12708926][0.14808366 0.16636837 0.17968023 0.18850869 0.19313644 0.19355695 0.18932165 0.18211247 0.17243531 0.16264927 0.15304518 0.14431718 0.13660108 0.12864892 0.11996006][0.15325123 0.16873705 0.17892641 0.18499479 0.18723409 0.18529207 0.17924193 0.1708452 0.16067015 0.15088034 0.14184874 0.13423359 0.12779026 0.12108373 0.11386645][0.15405446 0.16755217 0.17549324 0.17961611 0.18032561 0.17712893 0.17064403 0.16198438 0.15214436 0.14283811 0.13440777 0.12742209 0.12154049 0.11587134 0.10975587][0.15022169 0.1612791 0.16683693 0.16969554 0.16974074 0.1665359 0.16087581 0.15289742 0.14386091 0.13488315 0.12694556 0.1204792 0.11497047 0.11012414 0.10495552]]...]
INFO - root - 2017-12-10 02:39:35.341197: step 81210, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.882 sec/batch; 61h:33m:39s remains)
INFO - root - 2017-12-10 02:39:44.050403: step 81220, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.863 sec/batch; 60h:14m:16s remains)
INFO - root - 2017-12-10 02:39:52.555780: step 81230, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.854 sec/batch; 59h:35m:09s remains)
INFO - root - 2017-12-10 02:40:01.142574: step 81240, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.900 sec/batch; 62h:50m:47s remains)
INFO - root - 2017-12-10 02:40:09.907921: step 81250, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.875 sec/batch; 61h:02m:14s remains)
INFO - root - 2017-12-10 02:40:18.387072: step 81260, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.845 sec/batch; 59h:00m:17s remains)
INFO - root - 2017-12-10 02:40:27.084926: step 81270, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.873 sec/batch; 60h:56m:35s remains)
INFO - root - 2017-12-10 02:40:35.912825: step 81280, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.882 sec/batch; 61h:34m:21s remains)
INFO - root - 2017-12-10 02:40:44.667858: step 81290, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.877 sec/batch; 61h:11m:02s remains)
INFO - root - 2017-12-10 02:40:53.258250: step 81300, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.903 sec/batch; 63h:00m:30s remains)
2017-12-10 02:40:54.144059: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.20055787 0.20171373 0.20266438 0.20359671 0.20407464 0.20431101 0.2041218 0.20442183 0.20515446 0.20582832 0.20502569 0.20313668 0.20063238 0.19658446 0.19215308][0.20233591 0.20604925 0.20991206 0.21384671 0.21732892 0.21954405 0.21987183 0.21971756 0.21917829 0.21799441 0.21628256 0.21392184 0.21193022 0.20851751 0.20476897][0.19988488 0.20615235 0.21283586 0.22037822 0.22731031 0.23280685 0.23552647 0.23589657 0.2348315 0.23222141 0.22906475 0.22550562 0.22292951 0.21995348 0.21678871][0.19612978 0.2046051 0.21434225 0.22550829 0.23627989 0.24528588 0.25071415 0.25286609 0.25188586 0.24805216 0.2434106 0.23850968 0.2350281 0.23107897 0.22771198][0.1911916 0.20138802 0.21341203 0.228019 0.24224657 0.25467011 0.2633675 0.26765952 0.26713368 0.26261488 0.25630873 0.24986383 0.24495891 0.24034084 0.23694177][0.18535221 0.19641778 0.20957692 0.22648372 0.24352132 0.25924459 0.27084857 0.27775595 0.27862552 0.2740024 0.26660275 0.25813606 0.25133976 0.24550323 0.24146572][0.17981102 0.19085208 0.20398405 0.22191371 0.24035363 0.25861895 0.27269796 0.28146634 0.28371015 0.2795803 0.27153251 0.26172987 0.25324872 0.24643153 0.24162574][0.17512442 0.18537022 0.19774076 0.21536501 0.23406093 0.25351641 0.2691282 0.27909493 0.28212625 0.27866465 0.27055952 0.25976282 0.24968246 0.24163109 0.23589976][0.16867396 0.17824019 0.189512 0.2056798 0.22412905 0.24371281 0.2597715 0.27053463 0.27426559 0.2717976 0.26407167 0.25302204 0.2419789 0.2324601 0.22568114][0.16285644 0.17090102 0.18017961 0.19464619 0.21216783 0.23059012 0.24613336 0.25690296 0.26098928 0.25928262 0.2520318 0.2412564 0.22996022 0.22025475 0.2129081][0.15683286 0.16358633 0.17073995 0.18280087 0.19873266 0.21565135 0.23059459 0.24115355 0.24595645 0.24490547 0.23843636 0.22855023 0.21735007 0.20708522 0.19874538][0.15087016 0.156135 0.16121462 0.17076519 0.18425429 0.19913125 0.21291377 0.22334425 0.22873525 0.22912267 0.22429466 0.21580145 0.205594 0.19506261 0.18614659][0.14376986 0.1478198 0.15142509 0.15886842 0.17015819 0.18254817 0.19491866 0.20491372 0.21082008 0.212561 0.20982237 0.20358557 0.19487047 0.18468648 0.17537548][0.13714938 0.14025751 0.14242305 0.14785226 0.15703328 0.16724382 0.17827895 0.18803999 0.19504985 0.19849046 0.19795302 0.19416711 0.18707015 0.17786071 0.16842766][0.13496295 0.13717403 0.13802162 0.14203019 0.14919032 0.157056 0.16651113 0.17550354 0.18296032 0.18764985 0.18918046 0.18782289 0.18274437 0.17476201 0.16541997]]...]
INFO - root - 2017-12-10 02:41:02.893395: step 81310, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.853 sec/batch; 59h:32m:35s remains)
INFO - root - 2017-12-10 02:41:11.620128: step 81320, loss = 0.83, batch loss = 0.70 (8.9 examples/sec; 0.902 sec/batch; 62h:56m:33s remains)
INFO - root - 2017-12-10 02:41:20.161226: step 81330, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 60h:39m:34s remains)
INFO - root - 2017-12-10 02:41:28.817985: step 81340, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 60h:44m:42s remains)
INFO - root - 2017-12-10 02:41:37.455290: step 81350, loss = 0.81, batch loss = 0.68 (8.9 examples/sec; 0.895 sec/batch; 62h:26m:05s remains)
INFO - root - 2017-12-10 02:41:45.922286: step 81360, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 60h:18m:19s remains)
INFO - root - 2017-12-10 02:41:54.558537: step 81370, loss = 0.83, batch loss = 0.70 (9.2 examples/sec; 0.865 sec/batch; 60h:21m:21s remains)
INFO - root - 2017-12-10 02:42:03.135784: step 81380, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.879 sec/batch; 61h:20m:59s remains)
INFO - root - 2017-12-10 02:42:11.659068: step 81390, loss = 0.82, batch loss = 0.69 (9.7 examples/sec; 0.829 sec/batch; 57h:48m:30s remains)
INFO - root - 2017-12-10 02:42:20.167891: step 81400, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.863 sec/batch; 60h:12m:45s remains)
2017-12-10 02:42:21.121875: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0018282178 -0.0018263885 -0.0018250769 -0.0018239674 -0.0018233882 -0.0018233641 -0.0018236464 -0.0018239936 -0.0018242212 -0.0018240876 -0.0018234331 -0.001822446 -0.0018213679 -0.001820305 -0.0018193451][-0.0018313127 -0.0018295359 -0.0018280959 -0.001826838 -0.0018261155 -0.0018259989 -0.0018262346 -0.0018265483 -0.001826796 -0.001826497 -0.0018255273 -0.0018241253 -0.0018224979 -0.0018209254 -0.0018195561][-0.0018343427 -0.0018328247 -0.0018315874 -0.0018304049 -0.0018297308 -0.0018295974 -0.0018298368 -0.0018300626 -0.0018301632 -0.0018295937 -0.0018281889 -0.001826229 -0.0018239296 -0.0018217628 -0.0018199356][-0.001836382 -0.0018353324 -0.0018344448 -0.0018336792 -0.0018332979 -0.0018332984 -0.0018333999 -0.0018333832 -0.001833276 -0.0018323766 -0.0018304392 -0.001827859 -0.0018249432 -0.0018222327 -0.001820022][-0.0018373741 -0.0018368111 -0.0018364083 -0.0018362213 -0.0018362674 -0.001836258 -0.0018360101 -0.0018356728 -0.0018353377 -0.0018340776 -0.0018314997 -0.0018283626 -0.0018250589 -0.0018220417 -0.0018196716][-0.0018371703 -0.0018371188 -0.0018373383 -0.0018377787 -0.0018381095 -0.0018379511 -0.0018373107 -0.0018366768 -0.0018360378 -0.0018342322 -0.0018310933 -0.0018275779 -0.0018240929 -0.0018210472 -0.0018187785][-0.0018363795 -0.0018368121 -0.0018374855 -0.0018383313 -0.0018389812 -0.0018386946 -0.0018375844 -0.0018364072 -0.0018351913 -0.0018328947 -0.0018293456 -0.0018256354 -0.0018222725 -0.0018194776 -0.0018175209][-0.001834925 -0.0018354019 -0.0018362484 -0.0018372318 -0.0018379479 -0.001837634 -0.0018364408 -0.0018349275 -0.0018332169 -0.0018304888 -0.0018267799 -0.0018232225 -0.0018202183 -0.0018178772 -0.0018163754][-0.0018323341 -0.0018327812 -0.0018337554 -0.0018347678 -0.0018353312 -0.0018350169 -0.0018338592 -0.0018322454 -0.0018301546 -0.0018272121 -0.0018237378 -0.00182063 -0.0018182155 -0.0018164983 -0.001815535][-0.001829545 -0.0018299853 -0.0018309037 -0.001831557 -0.0018318261 -0.0018314607 -0.0018305006 -0.0018289255 -0.0018267588 -0.0018240063 -0.0018209376 -0.0018183647 -0.0018165715 -0.001815491 -0.0018150739][-0.0018269 -0.001827176 -0.0018281115 -0.0018286015 -0.0018285169 -0.0018280847 -0.0018272621 -0.0018258869 -0.0018239133 -0.0018214999 -0.0018189718 -0.0018169892 -0.001815728 -0.0018151448 -0.0018150635][-0.0018245383 -0.0018243776 -0.0018250381 -0.0018252197 -0.0018250474 -0.0018247161 -0.0018242027 -0.0018232499 -0.0018217609 -0.0018198346 -0.0018178553 -0.0018164269 -0.001815617 -0.0018153179 -0.0018154423][-0.0018221481 -0.0018215067 -0.0018217555 -0.0018218587 -0.0018217209 -0.0018215078 -0.0018213812 -0.0018209632 -0.0018200645 -0.0018186731 -0.0018173245 -0.0018163818 -0.001815832 -0.0018157091 -0.0018159336][-0.0018203321 -0.0018193324 -0.0018193455 -0.0018194889 -0.0018195543 -0.0018195175 -0.0018195224 -0.0018193821 -0.0018189092 -0.0018180674 -0.0018172765 -0.0018166775 -0.001816323 -0.0018162713 -0.0018164888][-0.0018196104 -0.0018183312 -0.0018182066 -0.0018183995 -0.0018185654 -0.0018186639 -0.0018187409 -0.0018186868 -0.0018184524 -0.0018180265 -0.0018175872 -0.0018172119 -0.001816994 -0.0018169474 -0.0018170704]]...]
INFO - root - 2017-12-10 02:42:29.679691: step 81410, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.835 sec/batch; 58h:12m:51s remains)
INFO - root - 2017-12-10 02:42:38.335064: step 81420, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.881 sec/batch; 61h:28m:43s remains)
INFO - root - 2017-12-10 02:42:46.867962: step 81430, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 60h:28m:49s remains)
INFO - root - 2017-12-10 02:42:55.479157: step 81440, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.902 sec/batch; 62h:55m:01s remains)
INFO - root - 2017-12-10 02:43:04.056301: step 81450, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.845 sec/batch; 58h:56m:36s remains)
INFO - root - 2017-12-10 02:43:12.389470: step 81460, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.843 sec/batch; 58h:47m:48s remains)
INFO - root - 2017-12-10 02:43:20.917034: step 81470, loss = 0.83, batch loss = 0.70 (9.3 examples/sec; 0.857 sec/batch; 59h:47m:06s remains)
INFO - root - 2017-12-10 02:43:29.525350: step 81480, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.853 sec/batch; 59h:27m:39s remains)
INFO - root - 2017-12-10 02:43:38.037351: step 81490, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 60h:07m:47s remains)
INFO - root - 2017-12-10 02:43:46.508528: step 81500, loss = 0.81, batch loss = 0.68 (9.0 examples/sec; 0.886 sec/batch; 61h:45m:38s remains)
2017-12-10 02:43:47.366831: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0018181461 -0.0018187352 -0.0018170257 -0.0018045255 -0.0017730095 -0.0017279405 -0.0016887648 -0.001687683 -0.001723289 -0.0017697001 -0.001801144 -0.0018121611 -0.0018154663 -0.0018144245 -0.0018111626][-0.0018221973 -0.001821234 -0.0018119088 -0.0017755246 -0.0016987742 -0.0016022769 -0.0015297692 -0.0015348333 -0.0016069114 -0.0016999062 -0.0017688987 -0.0018001179 -0.0018124261 -0.0018149685 -0.0018128183][-0.0018238659 -0.0018174887 -0.0017866059 -0.0016945096 -0.0015269396 -0.0013359574 -0.0012122389 -0.0012349189 -0.0013742931 -0.0015504451 -0.0016899508 -0.0017642026 -0.0017977937 -0.0018102573 -0.0018126166][-0.0018231019 -0.001803731 -0.0017261447 -0.0015273597 -0.0012026085 -0.00086008897 -0.0006604729 -0.0007197063 -0.000972549 -0.0012933121 -0.0015543103 -0.0017032512 -0.00177231 -0.0018000775 -0.0018094822][-0.0018193854 -0.001777651 -0.0016228352 -0.0012628951 -0.00072210841 -0.00019334478 7.9715042e-05 -4.0117884e-05 -0.00044658349 -0.00095774815 -0.0013772574 -0.0016256182 -0.0017396373 -0.0017863795 -0.0018039987][-0.0018124733 -0.0017403568 -0.0014890539 -0.00094181445 -0.00017013773 0.00053583959 0.00085602084 0.000650125 7.0630573e-05 -0.00064605637 -0.0012259468 -0.0015678625 -0.0017181749 -0.0017768942 -0.0017990463][-0.0018044963 -0.0017042689 -0.0013710157 -0.00067775708 0.00025412848 0.0010539313 0.0013627239 0.0010663584 0.00035337044 -0.00049971987 -0.0011722032 -0.0015602409 -0.0017188218 -0.0017767694 -0.0017972682][-0.0017986855 -0.0016868678 -0.0013271859 -0.00060038187 0.00034686585 0.0011196543 0.00136361 0.0010017058 0.000247775 -0.000613041 -0.00126106 -0.0016160329 -0.0017466462 -0.0017878073 -0.0017999628][-0.0017981097 -0.0016979484 -0.0013823872 -0.00075312937 5.2278046e-05 0.00068147329 0.00083216734 0.00046175893 -0.00021329138 -0.00093498413 -0.0014423607 -0.0017008749 -0.0017827109 -0.0018026771 -0.0018057737][-0.0018020406 -0.0017314317 -0.0015109393 -0.0010699632 -0.00050770317 -8.5672713e-05 -2.3433939e-05 -0.00033415796 -0.00082768826 -0.0013117343 -0.0016241294 -0.0017687192 -0.0018062169 -0.0018119245 -0.0018102741][-0.0018077877 -0.0017695858 -0.001649847 -0.0014056062 -0.0010936847 -0.00087094121 -0.00086496852 -0.0010717197 -0.0013553909 -0.0016040185 -0.0017457838 -0.0018028166 -0.0018140662 -0.001813953 -0.0018113328][-0.001812967 -0.0017976895 -0.0017509328 -0.0016522574 -0.0015260904 -0.0014420177 -0.0014528122 -0.0015511225 -0.0016669304 -0.0017548413 -0.0017972075 -0.0018108777 -0.0018127898 -0.0018122066 -0.0018105827][-0.0018140769 -0.001809585 -0.0017969433 -0.0017701848 -0.0017360601 -0.001715511 -0.0017223901 -0.0017527473 -0.0017828479 -0.0018014957 -0.0018075553 -0.0018087147 -0.0018090738 -0.0018087452 -0.0018078343][-0.0018117368 -0.0018102813 -0.0018084269 -0.0018053952 -0.0018011808 -0.0017988605 -0.0017993983 -0.0018021346 -0.0018042518 -0.001805093 -0.0018055752 -0.001805885 -0.0018060348 -0.0018058521 -0.0018053683][-0.0018092238 -0.0018077967 -0.0018068003 -0.0018061788 -0.0018051085 -0.0018040588 -0.001803527 -0.0018034168 -0.0018036229 -0.0018038381 -0.0018039228 -0.0018039477 -0.0018039078 -0.0018037806 -0.0018036299]]...]
INFO - root - 2017-12-10 02:43:55.976444: step 81510, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.833 sec/batch; 58h:06m:27s remains)
INFO - root - 2017-12-10 02:44:04.690688: step 81520, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.863 sec/batch; 60h:11m:45s remains)
INFO - root - 2017-12-10 02:44:13.248629: step 81530, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.870 sec/batch; 60h:40m:18s remains)
INFO - root - 2017-12-10 02:44:21.867168: step 81540, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 60h:26m:03s remains)
INFO - root - 2017-12-10 02:44:30.460753: step 81550, loss = 0.81, batch loss = 0.68 (9.4 examples/sec; 0.850 sec/batch; 59h:15m:01s remains)
INFO - root - 2017-12-10 02:44:38.965691: step 81560, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.888 sec/batch; 61h:55m:56s remains)
INFO - root - 2017-12-10 02:44:47.604260: step 81570, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 60h:06m:40s remains)
INFO - root - 2017-12-10 02:44:56.269224: step 81580, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.877 sec/batch; 61h:06m:43s remains)
INFO - root - 2017-12-10 02:45:05.039078: step 81590, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.877 sec/batch; 61h:08m:46s remains)
INFO - root - 2017-12-10 02:45:13.579639: step 81600, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.851 sec/batch; 59h:17m:02s remains)
2017-12-10 02:45:14.408787: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0018378701 -0.0018391396 -0.0018398777 -0.0018400524 -0.0018401482 -0.001840443 -0.001840909 -0.0018411318 -0.0018402179 -0.0018379188 -0.0018349627 -0.0018309931 -0.001825084 -0.0018185343 -0.001812604][-0.0018396518 -0.001840939 -0.0018415875 -0.0018414495 -0.0018410202 -0.0018408765 -0.0018409783 -0.0018411186 -0.001839967 -0.0018371337 -0.0018333935 -0.0018285615 -0.0018217753 -0.0018143306 -0.0018078892][-0.0018405701 -0.0018418843 -0.0018424621 -0.001842247 -0.0018413529 -0.0018406144 -0.0018402203 -0.001840225 -0.0018390856 -0.0018361598 -0.0018321237 -0.0018270005 -0.0018201293 -0.0018125392 -0.0018060952][-0.0018410207 -0.0018421868 -0.0018426673 -0.0018422562 -0.001840811 -0.0018395014 -0.001838605 -0.001838479 -0.0018376441 -0.0018351164 -0.0018313316 -0.0018264576 -0.0018201191 -0.0018130494 -0.0018068446][-0.0018412544 -0.0018422208 -0.0018423226 -0.0018414733 -0.0018394535 -0.00183757 -0.0018361841 -0.001836034 -0.0018356445 -0.0018338772 -0.0018310682 -0.0018272405 -0.0018219568 -0.0018155383 -0.0018097212][-0.0018411988 -0.0018419154 -0.0018417484 -0.0018404012 -0.0018379329 -0.001835647 -0.00183411 -0.0018338958 -0.0018338204 -0.0018328214 -0.0018312521 -0.0018289137 -0.0018251715 -0.0018195617 -0.001814035][-0.0018403165 -0.0018408742 -0.001840798 -0.0018392832 -0.001836627 -0.0018341197 -0.0018325372 -0.0018323304 -0.0018326028 -0.0018324386 -0.0018321691 -0.0018312479 -0.0018288663 -0.0018245052 -0.0018196756][-0.001838665 -0.0018392476 -0.0018394757 -0.0018381192 -0.0018356022 -0.0018334656 -0.0018321861 -0.0018321457 -0.0018327816 -0.0018331172 -0.0018335839 -0.0018337097 -0.00183242 -0.0018293078 -0.0018252966][-0.0018358921 -0.0018368111 -0.0018377615 -0.0018368553 -0.0018349448 -0.0018334369 -0.0018327683 -0.0018328876 -0.0018335949 -0.0018341477 -0.0018351022 -0.0018359754 -0.0018354844 -0.0018334182 -0.0018302213][-0.0018324733 -0.0018338584 -0.001835571 -0.0018354168 -0.001834297 -0.0018335216 -0.0018334474 -0.0018336597 -0.0018342871 -0.0018350628 -0.0018364789 -0.0018377984 -0.0018378141 -0.0018363348 -0.0018333839][-0.0018287448 -0.0018307526 -0.0018333665 -0.0018340326 -0.0018333403 -0.0018327523 -0.0018330336 -0.001833539 -0.0018342767 -0.0018352441 -0.001837011 -0.0018386006 -0.0018387621 -0.0018374884 -0.0018348554][-0.0018259673 -0.0018284647 -0.0018316786 -0.0018327733 -0.0018320786 -0.0018311091 -0.0018313717 -0.0018318666 -0.0018328241 -0.0018340804 -0.0018360438 -0.0018376808 -0.001838024 -0.00183708 -0.0018348208][-0.0018237453 -0.0018262659 -0.0018297429 -0.0018310819 -0.0018305115 -0.0018295284 -0.0018294479 -0.001829831 -0.0018308639 -0.001832373 -0.001834514 -0.0018360897 -0.0018367482 -0.0018360551 -0.0018342915][-0.0018239745 -0.0018255832 -0.0018283137 -0.001829289 -0.0018287795 -0.0018276975 -0.001827523 -0.0018276334 -0.0018283175 -0.0018295858 -0.0018316719 -0.0018334405 -0.0018342686 -0.0018339873 -0.0018329977][-0.0018249727 -0.0018253791 -0.0018267848 -0.0018271899 -0.0018267154 -0.0018256775 -0.0018254201 -0.0018255101 -0.0018260032 -0.0018270924 -0.0018289288 -0.0018307448 -0.0018317073 -0.0018317786 -0.0018312947]]...]
INFO - root - 2017-12-10 02:45:23.128159: step 81610, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.881 sec/batch; 61h:25m:31s remains)
INFO - root - 2017-12-10 02:45:31.760287: step 81620, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.836 sec/batch; 58h:15m:46s remains)
INFO - root - 2017-12-10 02:45:40.234601: step 81630, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 60h:26m:34s remains)
INFO - root - 2017-12-10 02:45:48.918971: step 81640, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.887 sec/batch; 61h:47m:50s remains)
INFO - root - 2017-12-10 02:45:57.517219: step 81650, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.874 sec/batch; 60h:52m:59s remains)
INFO - root - 2017-12-10 02:46:06.142944: step 81660, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.883 sec/batch; 61h:30m:27s remains)
INFO - root - 2017-12-10 02:46:14.902364: step 81670, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 60h:23m:40s remains)
INFO - root - 2017-12-10 02:46:23.612075: step 81680, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.866 sec/batch; 60h:20m:19s remains)
INFO - root - 2017-12-10 02:46:32.372302: step 81690, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.889 sec/batch; 61h:56m:54s remains)
INFO - root - 2017-12-10 02:46:40.968545: step 81700, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.863 sec/batch; 60h:08m:59s remains)
2017-12-10 02:46:41.860613: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0018356824 -0.0018344366 -0.0018338086 -0.0018334233 -0.0018331277 -0.0018329271 -0.0018330938 -0.0018333163 -0.0018335707 -0.0018339213 -0.0018342357 -0.0018343608 -0.0018340462 -0.0018333169 -0.0018325335][-0.0018361169 -0.0018352466 -0.0018350072 -0.0018347703 -0.0018344277 -0.0018340057 -0.001833676 -0.0018332961 -0.0018330868 -0.0018331819 -0.0018332909 -0.0018333254 -0.0018330354 -0.0018324445 -0.0018317612][-0.0018370388 -0.0018365718 -0.0018366409 -0.0018364349 -0.0018361569 -0.0018356889 -0.0018350275 -0.0018341806 -0.0018335269 -0.0018332129 -0.0018330495 -0.0018329544 -0.0018326769 -0.0018321649 -0.0018315603][-0.0018380354 -0.0018378752 -0.0018381064 -0.0018379956 -0.0018377968 -0.00183741 -0.0018366376 -0.0018355198 -0.0018345282 -0.0018338114 -0.0018333239 -0.0018330034 -0.0018325853 -0.0018320595 -0.0018315061][-0.00183879 -0.0018387097 -0.0018389915 -0.0018390281 -0.0018389475 -0.0018387503 -0.001837979 -0.001836783 -0.0018355674 -0.0018344779 -0.0018336562 -0.0018330927 -0.0018326149 -0.0018320669 -0.00183154][-0.0018392402 -0.0018389244 -0.0018389872 -0.0018389864 -0.0018390442 -0.0018390595 -0.0018383594 -0.001837275 -0.0018359793 -0.0018346393 -0.0018336115 -0.0018329688 -0.0018326092 -0.001832202 -0.0018317139][-0.0018392695 -0.0018385226 -0.0018381588 -0.001838012 -0.0018380593 -0.001838055 -0.0018374681 -0.0018365309 -0.0018353984 -0.0018342251 -0.0018333143 -0.001832752 -0.0018324722 -0.001832219 -0.0018318308][-0.0018389219 -0.0018378485 -0.0018369951 -0.0018365239 -0.0018364963 -0.0018364794 -0.0018361665 -0.0018354735 -0.0018346728 -0.0018338184 -0.0018331271 -0.00183262 -0.0018323815 -0.0018321241 -0.0018317215][-0.0018379965 -0.0018367897 -0.0018358345 -0.00183536 -0.0018351504 -0.0018349725 -0.0018347729 -0.001834423 -0.0018339886 -0.0018334597 -0.0018329971 -0.0018325892 -0.0018323091 -0.0018319588 -0.001831507][-0.0018367812 -0.0018355367 -0.0018346286 -0.0018342653 -0.0018339405 -0.0018335039 -0.0018333209 -0.0018332355 -0.0018330329 -0.0018327564 -0.0018325563 -0.0018323404 -0.0018320492 -0.0018316604 -0.0018312231][-0.0018353454 -0.0018341958 -0.00183341 -0.001832978 -0.0018325293 -0.0018320677 -0.0018319401 -0.0018320299 -0.0018320478 -0.0018320405 -0.0018320525 -0.0018319759 -0.001831712 -0.0018313394 -0.0018309682][-0.0018340643 -0.0018330279 -0.0018322965 -0.0018318073 -0.0018313616 -0.0018310173 -0.0018309888 -0.0018311234 -0.0018313365 -0.0018314878 -0.001831545 -0.0018314982 -0.001831324 -0.0018310689 -0.001830812][-0.0018331344 -0.0018321013 -0.0018314183 -0.0018310081 -0.0018307237 -0.0018305478 -0.0018306206 -0.0018308153 -0.0018310805 -0.001831234 -0.0018312328 -0.0018311768 -0.0018310684 -0.0018309273 -0.00183075][-0.0018325594 -0.0018315441 -0.0018309882 -0.0018307165 -0.0018305604 -0.0018305281 -0.0018306529 -0.0018308575 -0.0018310626 -0.001831147 -0.0018311191 -0.0018310512 -0.001830972 -0.0018308922 -0.0018307409][-0.0018322099 -0.0018312725 -0.0018308642 -0.0018307194 -0.0018306641 -0.0018306822 -0.0018307838 -0.0018309316 -0.001831079 -0.0018311325 -0.0018311006 -0.0018310095 -0.0018309088 -0.0018308287 -0.0018306822]]...]
INFO - root - 2017-12-10 02:46:50.445895: step 81710, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.830 sec/batch; 57h:51m:17s remains)
INFO - root - 2017-12-10 02:46:59.197076: step 81720, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.863 sec/batch; 60h:06m:18s remains)
INFO - root - 2017-12-10 02:47:07.720531: step 81730, loss = 0.82, batch loss = 0.69 (9.7 examples/sec; 0.825 sec/batch; 57h:27m:42s remains)
INFO - root - 2017-12-10 02:47:16.372993: step 81740, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 60h:22m:07s remains)
INFO - root - 2017-12-10 02:47:25.031859: step 81750, loss = 0.82, batch loss = 0.69 (9.7 examples/sec; 0.821 sec/batch; 57h:10m:14s remains)
INFO - root - 2017-12-10 02:47:33.535104: step 81760, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.856 sec/batch; 59h:37m:22s remains)
INFO - root - 2017-12-10 02:47:42.139585: step 81770, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.853 sec/batch; 59h:24m:00s remains)
INFO - root - 2017-12-10 02:47:50.833594: step 81780, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.873 sec/batch; 60h:49m:52s remains)
INFO - root - 2017-12-10 02:47:59.576095: step 81790, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.856 sec/batch; 59h:36m:07s remains)
INFO - root - 2017-12-10 02:48:08.128516: step 81800, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.872 sec/batch; 60h:43m:38s remains)
2017-12-10 02:48:09.013090: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.048245233 0.049625769 0.050036885 0.04983595 0.049059171 0.047943495 0.047111325 0.047226831 0.049204912 0.054103397 0.060499221 0.066746049 0.071494259 0.073308557 0.070658378][0.059164692 0.06191691 0.063323945 0.063895337 0.06330011 0.062289909 0.061446115 0.061820515 0.063847981 0.0689856 0.075812623 0.082008965 0.086240523 0.087013565 0.083354779][0.07411667 0.078608274 0.081267431 0.082659513 0.082156025 0.081107378 0.079352967 0.079387635 0.081171542 0.08604531 0.092690006 0.098627858 0.10252911 0.10179622 0.09684886][0.091066338 0.097575881 0.10099949 0.10298811 0.10251454 0.10088888 0.098541833 0.097613335 0.098847017 0.10311775 0.109519 0.1153651 0.11908714 0.11804593 0.11243585][0.10714601 0.11543728 0.12015411 0.12226775 0.1214192 0.1192143 0.11612897 0.1147605 0.11496598 0.11891404 0.12479084 0.13105306 0.13440113 0.13335101 0.12829465][0.12010033 0.12999523 0.13603652 0.13907784 0.13875316 0.1363257 0.13268836 0.13080618 0.13014354 0.13295387 0.13806646 0.1443789 0.14752178 0.1471584 0.14263447][0.12949464 0.14099528 0.14841992 0.15215379 0.15270837 0.15063708 0.14714408 0.14492022 0.1434039 0.14560401 0.149779 0.15529658 0.15780097 0.15730664 0.15358651][0.13485518 0.14748833 0.15592644 0.16079201 0.16255653 0.16150746 0.15842521 0.15636204 0.15473472 0.1565351 0.15959664 0.16405061 0.16611838 0.16539104 0.16209647][0.13681237 0.14974312 0.1585326 0.16396095 0.1666794 0.16670989 0.16493081 0.16390514 0.16320597 0.16476081 0.16727 0.17109838 0.17211333 0.17097086 0.16791378][0.13485053 0.14756331 0.15615779 0.1619525 0.16554733 0.16677098 0.16641004 0.16661334 0.16702248 0.16908589 0.17163137 0.17460816 0.17520028 0.17383812 0.17116308][0.12945683 0.14155456 0.14958309 0.15515591 0.15920708 0.16144377 0.16265383 0.16406615 0.16572668 0.16822726 0.17078753 0.17312758 0.17367119 0.17253977 0.17034875][0.12178145 0.13297883 0.14007692 0.14522842 0.14896639 0.15152667 0.15372762 0.15613188 0.15855709 0.16138695 0.16403048 0.16586278 0.16612953 0.16482669 0.1628454][0.1117174 0.12189896 0.12805407 0.13266091 0.1360981 0.13881926 0.14115678 0.14362887 0.14623608 0.14894724 0.15123908 0.15264253 0.15297073 0.15195009 0.15024099][0.0989989 0.10786947 0.11296964 0.11696496 0.11992735 0.12259305 0.12513965 0.12771553 0.12994552 0.13209324 0.13389479 0.13497803 0.13500081 0.13421458 0.13290115][0.084411949 0.091598585 0.095545657 0.098665938 0.1010294 0.10328341 0.10556825 0.10801038 0.10994218 0.1116887 0.11293371 0.11354303 0.11354922 0.11281401 0.11165735]]...]
INFO - root - 2017-12-10 02:48:17.604421: step 81810, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.863 sec/batch; 60h:03m:55s remains)
INFO - root - 2017-12-10 02:48:26.331455: step 81820, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.879 sec/batch; 61h:11m:27s remains)
INFO - root - 2017-12-10 02:48:34.851368: step 81830, loss = 0.82, batch loss = 0.69 (10.2 examples/sec; 0.782 sec/batch; 54h:27m:19s remains)
INFO - root - 2017-12-10 02:48:43.507760: step 81840, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.851 sec/batch; 59h:16m:50s remains)
INFO - root - 2017-12-10 02:48:52.067365: step 81850, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.845 sec/batch; 58h:50m:26s remains)
INFO - root - 2017-12-10 02:49:00.512084: step 81860, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.875 sec/batch; 60h:53m:09s remains)
INFO - root - 2017-12-10 02:49:09.142911: step 81870, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.874 sec/batch; 60h:51m:43s remains)
INFO - root - 2017-12-10 02:49:17.835546: step 81880, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 60h:28m:50s remains)
INFO - root - 2017-12-10 02:49:26.436753: step 81890, loss = 0.83, batch loss = 0.70 (9.3 examples/sec; 0.860 sec/batch; 59h:50m:33s remains)
INFO - root - 2017-12-10 02:49:34.946735: step 81900, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.873 sec/batch; 60h:46m:12s remains)
2017-12-10 02:49:35.805692: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0017962351 -0.0017939379 -0.0017937463 -0.0017938841 -0.0017941076 -0.0017943692 -0.0017946558 -0.0017949683 -0.0017952321 -0.0017954374 -0.0017954527 -0.0017953338 -0.0017950592 -0.0017946444 -0.0017942694][-0.0017944857 -0.0017923894 -0.001792524 -0.0017931056 -0.0017938972 -0.001794846 -0.0017958344 -0.0017967032 -0.0017973715 -0.0017976295 -0.0017974064 -0.0017968297 -0.0017959413 -0.001794961 -0.0017941854][-0.0017942464 -0.0017927457 -0.0017935473 -0.0017949491 -0.0017966616 -0.0017985959 -0.0018003813 -0.0018017926 -0.0018026375 -0.0018026491 -0.0018018766 -0.0018004546 -0.001798559 -0.0017966145 -0.0017951088][-0.0017946848 -0.0017939673 -0.0017958212 -0.0017985022 -0.0018015461 -0.0018045693 -0.0018072244 -0.0018091683 -0.0018099746 -0.0018094941 -0.001807867 -0.0018053122 -0.0018020622 -0.0017987622 -0.0017963351][-0.0017952094 -0.0017953749 -0.0017984891 -0.0018024917 -0.0018066565 -0.0018104622 -0.0018135614 -0.0018157305 -0.0018163703 -0.0018154489 -0.0018131315 -0.0018096315 -0.0018053107 -0.0018007872 -0.0017974295][-0.0017956003 -0.0017965238 -0.0018006562 -0.0018057593 -0.0018107095 -0.0018150422 -0.0018183219 -0.0018204466 -0.001820693 -0.0018193076 -0.0018164976 -0.0018123781 -0.0018074078 -0.0018021429 -0.0017981853][-0.0017961622 -0.0017976859 -0.0018024324 -0.0018082375 -0.0018137681 -0.0018183829 -0.0018216466 -0.0018235139 -0.0018232985 -0.001821466 -0.0018182314 -0.0018137683 -0.0018084849 -0.0018029436 -0.0017987789][-0.001797193 -0.0017987222 -0.0018036435 -0.0018097456 -0.0018155193 -0.0018200976 -0.0018232147 -0.0018247919 -0.0018241041 -0.0018218482 -0.0018183945 -0.0018138124 -0.0018085574 -0.0018031603 -0.0017991472][-0.0017973275 -0.0017986781 -0.0018033264 -0.0018091146 -0.0018144585 -0.0018185677 -0.0018213603 -0.0018225866 -0.0018216268 -0.0018194206 -0.0018162658 -0.0018120853 -0.0018074175 -0.0018026434 -0.0017990174][-0.0017966653 -0.0017976247 -0.0018014468 -0.0018062983 -0.0018107378 -0.0018140222 -0.0018161968 -0.0018170031 -0.0018161004 -0.0018142631 -0.001811711 -0.0018083513 -0.0018047986 -0.0018011889 -0.0017983253][-0.0017961576 -0.001796463 -0.001799138 -0.0018028365 -0.0018062219 -0.0018086397 -0.0018101447 -0.0018105692 -0.0018097645 -0.0018082551 -0.0018064207 -0.0018040134 -0.0018015748 -0.0017992529 -0.0017973342][-0.0017948544 -0.0017943801 -0.0017960352 -0.0017985289 -0.0018008928 -0.0018024963 -0.0018034909 -0.0018036903 -0.0018031114 -0.001802086 -0.0018010545 -0.0017996937 -0.0017983732 -0.0017972622 -0.0017962572][-0.0017933064 -0.0017921367 -0.001792966 -0.0017944642 -0.0017959316 -0.0017969647 -0.0017975918 -0.0017976193 -0.0017972459 -0.0017968121 -0.0017966147 -0.0017961948 -0.0017958434 -0.0017956621 -0.0017953379][-0.0017928485 -0.0017911764 -0.0017914219 -0.0017922026 -0.0017929141 -0.0017934395 -0.0017937388 -0.0017936972 -0.0017935054 -0.0017934848 -0.0017938841 -0.001794087 -0.0017943073 -0.0017946057 -0.0017946621][-0.0017934752 -0.0017914579 -0.001791229 -0.0017915488 -0.0017918437 -0.0017921106 -0.0017922319 -0.0017921726 -0.001792088 -0.0017921673 -0.0017927049 -0.0017930788 -0.0017934929 -0.0017939397 -0.0017941346]]...]
INFO - root - 2017-12-10 02:49:44.426221: step 81910, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.874 sec/batch; 60h:52m:06s remains)
INFO - root - 2017-12-10 02:49:53.080931: step 81920, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.865 sec/batch; 60h:11m:50s remains)
INFO - root - 2017-12-10 02:50:01.735976: step 81930, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.885 sec/batch; 61h:35m:10s remains)
INFO - root - 2017-12-10 02:50:10.255260: step 81940, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.858 sec/batch; 59h:44m:21s remains)
INFO - root - 2017-12-10 02:50:18.948922: step 81950, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.859 sec/batch; 59h:49m:07s remains)
INFO - root - 2017-12-10 02:50:27.390394: step 81960, loss = 0.82, batch loss = 0.69 (9.7 examples/sec; 0.827 sec/batch; 57h:31m:16s remains)
INFO - root - 2017-12-10 02:50:35.997681: step 81970, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.874 sec/batch; 60h:51m:23s remains)
INFO - root - 2017-12-10 02:50:44.564480: step 81980, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 60h:36m:28s remains)
INFO - root - 2017-12-10 02:50:53.063775: step 81990, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.847 sec/batch; 58h:54m:47s remains)
INFO - root - 2017-12-10 02:51:01.354532: step 82000, loss = 0.83, batch loss = 0.70 (9.5 examples/sec; 0.845 sec/batch; 58h:48m:28s remains)
2017-12-10 02:51:02.195732: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.0028955168 0.0028148736 0.0027906904 0.0028549 0.0029898658 0.0030977665 0.0031408081 0.0031514727 0.0032361876 0.00321464 0.0031503444 0.0029411702 0.0027152854 0.0025855415 0.0024536066][0.0031405496 0.0030712262 0.0030173119 0.0030304426 0.0031404118 0.00321993 0.0032449453 0.0032430119 0.0032918723 0.0033194823 0.003298243 0.003125981 0.0029564565 0.0029315115 0.0028541749][0.0033119712 0.0032487256 0.0031969394 0.0032094377 0.0032693897 0.0032694545 0.0032107905 0.0031490857 0.0032282053 0.0032653501 0.0033996245 0.0032716314 0.0031526559 0.003123099 0.0031319261][0.0033976524 0.0033514476 0.0032670065 0.0032444336 0.0032859081 0.0032560485 0.0031419806 0.003009133 0.0030621924 0.0031939894 0.0033631912 0.0033077714 0.0032260548 0.0032191155 0.003330423][0.0035109613 0.0033865655 0.0032164766 0.003166677 0.0031850496 0.0031128647 0.0030162684 0.002863674 0.002891087 0.0031032218 0.0032920577 0.003339217 0.0032448592 0.0032657199 0.0034425585][0.0036686845 0.0034675202 0.0032010782 0.0030093072 0.00292229 0.0028463169 0.0027929433 0.0027209017 0.0027850461 0.0030063726 0.0032134606 0.003377331 0.0033194157 0.0033761906 0.0035382281][0.003796835 0.0036036614 0.0032901797 0.003007215 0.0027915565 0.0026817513 0.0025784979 0.0025564088 0.002619327 0.0028445837 0.0031620041 0.0033751628 0.0034046858 0.0034825969 0.0036490955][0.0040029972 0.0038224654 0.0034815781 0.0031731473 0.0028687329 0.0026781275 0.0025865226 0.0024914439 0.0024846615 0.0026300615 0.0029739882 0.0032168245 0.0032906188 0.003327182 0.0035036295][0.0042841518 0.004194967 0.0038717459 0.00349046 0.0031107566 0.0028518862 0.0026307758 0.0024867626 0.0023638816 0.0024502752 0.0027003759 0.0028702919 0.0028754161 0.0029400163 0.0031318492][0.0044855848 0.0044763535 0.0042216419 0.0039262637 0.00355442 0.0032082764 0.0029113744 0.0026814658 0.0024866788 0.0024269512 0.0025114217 0.002595088 0.0025296216 0.0024826615 0.0026619518][0.0045020413 0.0045665707 0.004376295 0.0040971763 0.0037291618 0.0034207902 0.00315905 0.0029166127 0.0027453438 0.002567423 0.0024839481 0.0024037757 0.002262651 0.0021287315 0.002159331][0.0046323589 0.004695971 0.0045026615 0.00420211 0.003807656 0.0034759585 0.00321555 0.002977652 0.0027425732 0.0025799875 0.0023953035 0.0022884784 0.0020992276 0.0018750491 0.0017906268][0.0047846334 0.0049237926 0.0047998531 0.0044990936 0.0040603778 0.0036951643 0.0033522234 0.0030713324 0.0027268152 0.0024915519 0.0022736308 0.0021018018 0.0018635927 0.0016050384 0.0014994008][0.0048684645 0.0050884965 0.0049838475 0.0047265012 0.0043216036 0.0039152494 0.0035134242 0.0031443229 0.0027314918 0.0024405522 0.0021390039 0.001906704 0.0015835079 0.0013468693 0.0012458129][0.0046316232 0.0049996632 0.0050146775 0.0048566395 0.0045298371 0.0041328189 0.0037037679 0.0032412964 0.0028007627 0.0024202857 0.0020614672 0.0017519788 0.0014125571 0.0011367343 0.0010140188]]...]
INFO - root - 2017-12-10 02:51:10.775658: step 82010, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.875 sec/batch; 60h:54m:27s remains)
INFO - root - 2017-12-10 02:51:19.498542: step 82020, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 60h:07m:22s remains)
INFO - root - 2017-12-10 02:51:28.257693: step 82030, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 59h:58m:17s remains)
INFO - root - 2017-12-10 02:51:36.702356: step 82040, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.849 sec/batch; 59h:03m:11s remains)
INFO - root - 2017-12-10 02:51:45.280115: step 82050, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.846 sec/batch; 58h:52m:34s remains)
INFO - root - 2017-12-10 02:51:53.676615: step 82060, loss = 0.83, batch loss = 0.70 (9.4 examples/sec; 0.849 sec/batch; 59h:04m:44s remains)
INFO - root - 2017-12-10 02:52:02.144701: step 82070, loss = 0.82, batch loss = 0.69 (9.8 examples/sec; 0.818 sec/batch; 56h:55m:03s remains)
INFO - root - 2017-12-10 02:52:10.920911: step 82080, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.896 sec/batch; 62h:21m:20s remains)
INFO - root - 2017-12-10 02:52:19.783370: step 82090, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.885 sec/batch; 61h:31m:59s remains)
INFO - root - 2017-12-10 02:52:28.420385: step 82100, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.859 sec/batch; 59h:45m:37s remains)
2017-12-10 02:52:29.255029: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0018376233 -0.001835599 -0.0018334009 -0.0018312203 -0.0018294458 -0.001828202 -0.0018279634 -0.0018286238 -0.0018302386 -0.0018320205 -0.001833234 -0.0018335866 -0.0018330291 -0.0018318237 -0.0018301253][-0.0018363792 -0.0018346455 -0.001832828 -0.0018310029 -0.0018294909 -0.0018284197 -0.0018281562 -0.0018286781 -0.0018300598 -0.0018315922 -0.0018325887 -0.0018328498 -0.0018323113 -0.0018310677 -0.0018294158][-0.0018340189 -0.0018327854 -0.0018315484 -0.0018302759 -0.001829232 -0.001828609 -0.001828615 -0.0018292003 -0.0018304323 -0.0018316621 -0.0018323631 -0.0018324488 -0.0018318654 -0.0018306511 -0.0018291796][-0.0018325029 -0.0018319828 -0.0018315122 -0.0018309427 -0.00183043 -0.0018301173 -0.0018302014 -0.0018307323 -0.0018315931 -0.0018322857 -0.0018325915 -0.0018324874 -0.0018319581 -0.0018309409 -0.0018297118][-0.0018310362 -0.0018311013 -0.0018313392 -0.0018315106 -0.0018316722 -0.0018318746 -0.0018322915 -0.0018328975 -0.0018335604 -0.0018338565 -0.0018338233 -0.0018335386 -0.001832927 -0.0018319651 -0.0018308533][-0.0018302345 -0.0018304413 -0.0018308873 -0.0018314203 -0.0018320142 -0.001832652 -0.0018334067 -0.0018342745 -0.0018350878 -0.0018354018 -0.0018353066 -0.0018349162 -0.001834254 -0.0018332432 -0.0018320815][-0.0018305146 -0.0018307141 -0.0018312151 -0.0018318221 -0.0018325882 -0.0018335084 -0.0018345406 -0.001835613 -0.0018365557 -0.0018369034 -0.001836747 -0.0018362668 -0.0018354882 -0.0018343864 -0.0018331239][-0.0018317273 -0.0018318312 -0.0018322535 -0.0018328215 -0.001833629 -0.0018346327 -0.0018357381 -0.0018368515 -0.0018377457 -0.0018379969 -0.0018376763 -0.0018370957 -0.0018363146 -0.0018352047 -0.0018339783][-0.0018332941 -0.0018335147 -0.0018340269 -0.0018346751 -0.0018355696 -0.0018366184 -0.0018376068 -0.0018384573 -0.0018390056 -0.0018389157 -0.0018382245 -0.0018374074 -0.0018365218 -0.0018354902 -0.0018343787][-0.0018347258 -0.0018350333 -0.0018356269 -0.0018363573 -0.0018372553 -0.001838234 -0.0018389937 -0.0018395159 -0.0018396955 -0.001839285 -0.001838338 -0.0018373849 -0.0018363954 -0.0018354021 -0.0018344225][-0.0018356175 -0.0018359306 -0.0018365307 -0.0018372327 -0.0018380088 -0.0018388025 -0.0018392617 -0.0018394713 -0.0018394282 -0.0018389248 -0.0018380255 -0.0018371118 -0.0018361262 -0.0018351544 -0.001834237][-0.0018357165 -0.0018359771 -0.0018365296 -0.0018371892 -0.0018378465 -0.0018384749 -0.0018386376 -0.001838549 -0.0018383096 -0.0018377881 -0.0018370411 -0.0018362881 -0.0018354678 -0.0018345987 -0.0018337646][-0.0018352937 -0.0018353824 -0.0018357685 -0.0018362743 -0.0018367708 -0.0018372079 -0.0018371455 -0.0018367968 -0.0018363643 -0.0018357966 -0.001835194 -0.0018346416 -0.0018340915 -0.0018334825 -0.0018328538][-0.0018345031 -0.0018344283 -0.0018345618 -0.001834834 -0.0018351715 -0.0018354763 -0.001835348 -0.0018349483 -0.0018344927 -0.0018339735 -0.001833447 -0.0018329754 -0.0018325779 -0.0018321825 -0.0018317573][-0.0018334507 -0.0018331896 -0.0018330507 -0.0018330972 -0.001833232 -0.0018333872 -0.0018332172 -0.0018328757 -0.0018325098 -0.0018320685 -0.0018316432 -0.0018312671 -0.0018309378 -0.0018306447 -0.001830381]]...]
INFO - root - 2017-12-10 02:52:37.926288: step 82110, loss = 0.82, batch loss = 0.69 (8.6 examples/sec; 0.931 sec/batch; 64h:46m:45s remains)
INFO - root - 2017-12-10 02:52:46.661953: step 82120, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.866 sec/batch; 60h:13m:56s remains)
INFO - root - 2017-12-10 02:52:55.526301: step 82130, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.873 sec/batch; 60h:43m:30s remains)
INFO - root - 2017-12-10 02:53:03.988121: step 82140, loss = 0.83, batch loss = 0.70 (9.6 examples/sec; 0.833 sec/batch; 57h:53m:54s remains)
INFO - root - 2017-12-10 02:53:12.676699: step 82150, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 60h:23m:35s remains)
INFO - root - 2017-12-10 02:53:21.204332: step 82160, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.865 sec/batch; 60h:08m:02s remains)
INFO - root - 2017-12-10 02:53:29.756380: step 82170, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.852 sec/batch; 59h:15m:25s remains)
INFO - root - 2017-12-10 02:53:38.427867: step 82180, loss = 0.82, batch loss = 0.69 (8.8 examples/sec; 0.904 sec/batch; 62h:53m:02s remains)
INFO - root - 2017-12-10 02:53:47.104856: step 82190, loss = 0.82, batch loss = 0.69 (8.8 examples/sec; 0.912 sec/batch; 63h:26m:10s remains)
INFO - root - 2017-12-10 02:53:55.751941: step 82200, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 60h:03m:45s remains)
2017-12-10 02:53:56.597029: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.001811172 -0.0018102711 -0.0018100498 -0.0018099674 -0.0018098503 -0.0018097804 -0.0018097577 -0.0018097991 -0.0018098046 -0.0018097184 -0.0018094789 -0.0018090871 -0.0018086816 -0.0018083701 -0.0018082133][-0.0018113886 -0.0018104644 -0.001810194 -0.0018100233 -0.001809828 -0.0018097453 -0.0018097986 -0.0018099278 -0.0018099337 -0.001809697 -0.0018091393 -0.0018083539 -0.0018076762 -0.0018073218 -0.0018072397][-0.0018117011 -0.001810751 -0.0018104579 -0.0018102234 -0.0018099092 -0.0018098067 -0.0018099293 -0.0018101442 -0.0018101401 -0.0018097367 -0.0018088751 -0.0018077359 -0.0018068744 -0.0018065204 -0.0018065376][-0.0018117045 -0.00181077 -0.0018104841 -0.0018102282 -0.0018097822 -0.0018095393 -0.0018095636 -0.0018097342 -0.0018096957 -0.0018091808 -0.0018081446 -0.0018068714 -0.0018060151 -0.0018057565 -0.0018058728][-0.0018112779 -0.0018103733 -0.0018101417 -0.0018098834 -0.0018092999 -0.0018088068 -0.0018085219 -0.0018084891 -0.0018083651 -0.0018078508 -0.0018068458 -0.0018056312 -0.0018049643 -0.0018049153 -0.0018051987][-0.0018105262 -0.0018096619 -0.0018095293 -0.0018092723 -0.0018085272 -0.0018077578 -0.0018070906 -0.0018067624 -0.0018065751 -0.0018061816 -0.0018053785 -0.0018043532 -0.0018038965 -0.0018040667 -0.0018045634][-0.0018096841 -0.0018089901 -0.0018090445 -0.0018088323 -0.0018079816 -0.0018069841 -0.0018060067 -0.001805341 -0.001805052 -0.0018047494 -0.0018041462 -0.0018033346 -0.0018030233 -0.0018033764 -0.001804115][-0.0018090875 -0.0018084564 -0.0018087052 -0.0018086117 -0.0018078095 -0.0018067613 -0.001805629 -0.0018046682 -0.0018041159 -0.0018037331 -0.0018031851 -0.0018025035 -0.0018023072 -0.0018027718 -0.0018037066][-0.0018083453 -0.0018078377 -0.0018082533 -0.0018082721 -0.0018076416 -0.0018067235 -0.0018056807 -0.0018047183 -0.0018039272 -0.0018033219 -0.0018026977 -0.0018020447 -0.0018018818 -0.0018023448 -0.0018033549][-0.0018075615 -0.0018071509 -0.0018076359 -0.0018077643 -0.0018073831 -0.0018067571 -0.0018059737 -0.0018051928 -0.0018043795 -0.0018036134 -0.0018028915 -0.0018021968 -0.001801966 -0.0018022981 -0.0018032118][-0.0018071734 -0.001806694 -0.0018071642 -0.001807382 -0.0018072302 -0.0018069067 -0.0018064516 -0.001805942 -0.0018053123 -0.0018045574 -0.0018037974 -0.0018030612 -0.0018026764 -0.00180277 -0.0018034331][-0.0018072238 -0.0018066359 -0.0018070152 -0.001807253 -0.0018072474 -0.0018071303 -0.0018069263 -0.0018066793 -0.0018062575 -0.0018056477 -0.0018049588 -0.0018042581 -0.0018038047 -0.001803727 -0.0018041375][-0.0018075798 -0.0018068923 -0.0018071012 -0.0018072855 -0.0018073259 -0.0018073269 -0.0018072757 -0.001807222 -0.0018070097 -0.0018066115 -0.0018061148 -0.0018055546 -0.0018051603 -0.0018050319 -0.001805235][-0.0018080301 -0.001807331 -0.0018073251 -0.0018074233 -0.0018074497 -0.0018074848 -0.0018075185 -0.0018075991 -0.0018075578 -0.0018073717 -0.0018071148 -0.0018067405 -0.0018064325 -0.001806275 -0.0018063223][-0.0018085463 -0.001807794 -0.0018075977 -0.001807631 -0.0018076393 -0.0018076719 -0.0018077348 -0.0018078721 -0.0018079182 -0.001807862 -0.0018077565 -0.0018075283 -0.00180731 -0.0018071578 -0.0018071027]]...]
INFO - root - 2017-12-10 02:54:05.314951: step 82210, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.866 sec/batch; 60h:11m:59s remains)
INFO - root - 2017-12-10 02:54:14.057914: step 82220, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.849 sec/batch; 59h:02m:02s remains)
INFO - root - 2017-12-10 02:54:22.748127: step 82230, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.865 sec/batch; 60h:09m:58s remains)
INFO - root - 2017-12-10 02:54:31.302520: step 82240, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.877 sec/batch; 60h:59m:51s remains)
INFO - root - 2017-12-10 02:54:39.838652: step 82250, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.848 sec/batch; 58h:57m:54s remains)
INFO - root - 2017-12-10 02:54:48.455877: step 82260, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.856 sec/batch; 59h:32m:05s remains)
INFO - root - 2017-12-10 02:54:57.175259: step 82270, loss = 0.82, batch loss = 0.69 (8.7 examples/sec; 0.914 sec/batch; 63h:33m:46s remains)
INFO - root - 2017-12-10 02:55:05.984107: step 82280, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.881 sec/batch; 61h:15m:13s remains)
INFO - root - 2017-12-10 02:55:14.649658: step 82290, loss = 0.82, batch loss = 0.69 (9.9 examples/sec; 0.807 sec/batch; 56h:03m:57s remains)
INFO - root - 2017-12-10 02:55:23.463978: step 82300, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.896 sec/batch; 62h:15m:44s remains)
2017-12-10 02:55:24.283868: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0018248531 -0.0018238368 -0.0018236506 -0.0018234652 -0.0018231025 -0.0018226164 -0.0018220771 -0.0018215609 -0.0018209765 -0.0018203118 -0.0018195538 -0.0018185978 -0.0018174666 -0.0018162007 -0.0018150208][-0.0018266265 -0.0018251038 -0.0018245755 -0.0018242656 -0.0018239414 -0.0018235372 -0.0018231012 -0.0018225713 -0.0018220332 -0.0018217409 -0.0018213355 -0.0018205021 -0.0018193237 -0.0018179046 -0.0018163125][-0.0018263033 -0.0018241376 -0.0018231582 -0.0018227508 -0.0018223892 -0.0018218263 -0.0018211954 -0.001820574 -0.0018205597 -0.0018213276 -0.0018222721 -0.0018225516 -0.0018219791 -0.0018206947 -0.0018187015][-0.0018232393 -0.0018202168 -0.0018187505 -0.0018182697 -0.0018179968 -0.0018173642 -0.0018164029 -0.0018156136 -0.0018161634 -0.0018183866 -0.0018212375 -0.0018231984 -0.0018237844 -0.0018229309 -0.0018207105][-0.0018164631 -0.0018126122 -0.0018110857 -0.0018114024 -0.001812046 -0.0018117694 -0.0018106281 -0.0018096864 -0.0018107035 -0.0018142614 -0.0018190176 -0.0018227856 -0.0018246463 -0.0018242984 -0.0018220181][-0.001805731 -0.0018006614 -0.0017993218 -0.0018012073 -0.0018040078 -0.0018055731 -0.0018055439 -0.0018051706 -0.0018066628 -0.0018110039 -0.0018168469 -0.0018217079 -0.0018243948 -0.0018244501 -0.0018223408][-0.0017922162 -0.0017869429 -0.0017869505 -0.0017913617 -0.0017971235 -0.0018012163 -0.0018030575 -0.001803901 -0.0018059178 -0.0018102418 -0.0018158277 -0.0018205538 -0.0018232798 -0.0018235164 -0.0018216461][-0.0017761582 -0.0017725457 -0.0017758927 -0.0017837699 -0.0017922348 -0.00179816 -0.0018012784 -0.0018028673 -0.0018050284 -0.0018090064 -0.0018140812 -0.0018184166 -0.0018210361 -0.0018215531 -0.0018202394][-0.0017680357 -0.0017677685 -0.0017748823 -0.0017850974 -0.0017943557 -0.0018000503 -0.0018026417 -0.001803803 -0.0018053261 -0.0018083898 -0.0018123781 -0.0018158747 -0.0018181124 -0.0018187804 -0.0018180825][-0.0017728824 -0.001775385 -0.0017841028 -0.0017942457 -0.0018019142 -0.0018055005 -0.001806309 -0.0018062601 -0.0018067686 -0.001808531 -0.0018111627 -0.0018136972 -0.0018155202 -0.0018163237 -0.0018161651][-0.0017896071 -0.0017922908 -0.0017991045 -0.0018061042 -0.0018104812 -0.0018114952 -0.0018107466 -0.0018097498 -0.0018093251 -0.0018098607 -0.0018111326 -0.0018126345 -0.0018139258 -0.0018146865 -0.0018148458][-0.0018081791 -0.0018088932 -0.0018116195 -0.0018140599 -0.0018148301 -0.0018139153 -0.0018125538 -0.0018114517 -0.0018107733 -0.001810673 -0.0018110623 -0.0018117568 -0.001812592 -0.0018133777 -0.001813797][-0.0018180588 -0.001816649 -0.001816123 -0.0018156654 -0.0018148198 -0.0018135078 -0.0018123556 -0.00181157 -0.0018110577 -0.0018109311 -0.0018111183 -0.0018114838 -0.0018119625 -0.0018124724 -0.0018128479][-0.0018203378 -0.0018180771 -0.0018163509 -0.0018149951 -0.0018139434 -0.0018128953 -0.0018121998 -0.0018117792 -0.001811494 -0.0018114105 -0.0018114795 -0.0018116509 -0.0018119033 -0.0018121717 -0.0018123896][-0.0018184432 -0.0018164599 -0.0018150125 -0.0018139888 -0.0018133195 -0.0018127344 -0.0018123658 -0.0018121416 -0.0018120176 -0.0018119963 -0.0018120422 -0.001812114 -0.0018122144 -0.0018123036 -0.0018123586]]...]
INFO - root - 2017-12-10 02:55:33.026541: step 82310, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 60h:31m:03s remains)
INFO - root - 2017-12-10 02:55:41.727934: step 82320, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.834 sec/batch; 57h:56m:35s remains)
INFO - root - 2017-12-10 02:55:50.398222: step 82330, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.847 sec/batch; 58h:50m:38s remains)
INFO - root - 2017-12-10 02:55:58.939641: step 82340, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.892 sec/batch; 61h:58m:29s remains)
INFO - root - 2017-12-10 02:56:07.687281: step 82350, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.858 sec/batch; 59h:35m:06s remains)
INFO - root - 2017-12-10 02:56:16.280301: step 82360, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.831 sec/batch; 57h:45m:57s remains)
INFO - root - 2017-12-10 02:56:24.966680: step 82370, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.876 sec/batch; 60h:51m:12s remains)
INFO - root - 2017-12-10 02:56:33.717168: step 82380, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.876 sec/batch; 60h:50m:59s remains)
INFO - root - 2017-12-10 02:56:42.457142: step 82390, loss = 0.82, batch loss = 0.69 (10.4 examples/sec; 0.772 sec/batch; 53h:37m:50s remains)
INFO - root - 2017-12-10 02:56:51.180134: step 82400, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.877 sec/batch; 60h:54m:48s remains)
2017-12-10 02:56:52.131573: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.30794147 0.30285031 0.29715592 0.29097605 0.28212667 0.27289009 0.26588431 0.26003504 0.25472507 0.24804631 0.24238576 0.23828728 0.23346417 0.22876193 0.22488652][0.31128621 0.30752364 0.30250669 0.29651558 0.2878544 0.27907571 0.27191153 0.26593667 0.26074332 0.2548359 0.24904653 0.24417275 0.23856051 0.23311156 0.22820155][0.30943832 0.30788919 0.30432555 0.29882711 0.2909269 0.2830193 0.27564204 0.26875567 0.26278189 0.25724906 0.25165009 0.24605213 0.23989384 0.23398547 0.22844437][0.31008741 0.30973175 0.30715939 0.30233413 0.29534465 0.28798112 0.28079548 0.27354646 0.26651174 0.2606799 0.25489861 0.24917027 0.24302895 0.23659804 0.2306982][0.31140924 0.31098506 0.307797 0.30317867 0.29715255 0.29035148 0.28358778 0.27675685 0.26990926 0.26362252 0.25745514 0.2522245 0.24588242 0.23910809 0.23303381][0.31098768 0.31050292 0.30643767 0.30178598 0.29672751 0.29144338 0.28655893 0.28138149 0.27588066 0.27007863 0.26369441 0.25774834 0.25088844 0.24351132 0.23696715][0.31120688 0.3110567 0.30626321 0.30115724 0.29678705 0.29320788 0.28996143 0.28638336 0.28231391 0.27763376 0.27197957 0.26588073 0.258496 0.25090089 0.24421215][0.30956319 0.31073084 0.30601376 0.30110377 0.29752284 0.29551059 0.29353291 0.29130146 0.28810513 0.28354058 0.27832529 0.272475 0.26494625 0.25701177 0.25026774][0.30747068 0.30957249 0.30477446 0.30079031 0.29836634 0.29691279 0.295889 0.29497463 0.29311952 0.28894082 0.28393963 0.2787967 0.27132624 0.26341242 0.25651023][0.30333883 0.30689022 0.30260965 0.29950443 0.29808292 0.29706788 0.29672304 0.29658666 0.29569271 0.2922042 0.28750461 0.28300035 0.27554181 0.26803887 0.26120764][0.29661942 0.30174333 0.2984482 0.29647017 0.29602432 0.29609457 0.29624727 0.29688528 0.29671451 0.29379696 0.28960371 0.28475493 0.27795139 0.27107197 0.26478577][0.28654945 0.29305598 0.29074711 0.28939813 0.28922325 0.28994998 0.2898522 0.29148746 0.29252589 0.29109505 0.28828079 0.28459203 0.27933878 0.27339685 0.26784366][0.27471635 0.28175941 0.28028885 0.27971217 0.28003979 0.28068373 0.28075686 0.28302389 0.28488284 0.2844747 0.28271762 0.28040892 0.27708119 0.27264646 0.26826346][0.26305345 0.26960319 0.26804587 0.26790604 0.26853853 0.26930565 0.27014583 0.27336836 0.27666754 0.27780211 0.2775735 0.27677521 0.27497685 0.27214077 0.26862034][0.24941903 0.25540763 0.25376946 0.253668 0.25455594 0.25616848 0.2580862 0.262578 0.26729122 0.27002156 0.27139723 0.27170116 0.27133083 0.26959682 0.26668251]]...]
INFO - root - 2017-12-10 02:57:00.864550: step 82410, loss = 0.83, batch loss = 0.70 (9.1 examples/sec; 0.883 sec/batch; 61h:21m:22s remains)
INFO - root - 2017-12-10 02:57:09.678247: step 82420, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.851 sec/batch; 59h:06m:22s remains)
INFO - root - 2017-12-10 02:57:18.540604: step 82430, loss = 0.81, batch loss = 0.68 (9.3 examples/sec; 0.865 sec/batch; 60h:03m:43s remains)
INFO - root - 2017-12-10 02:57:27.193353: step 82440, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.892 sec/batch; 61h:57m:48s remains)
INFO - root - 2017-12-10 02:57:35.857540: step 82450, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.865 sec/batch; 60h:03m:08s remains)
INFO - root - 2017-12-10 02:57:44.422146: step 82460, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.845 sec/batch; 58h:40m:18s remains)
INFO - root - 2017-12-10 02:57:53.072126: step 82470, loss = 0.82, batch loss = 0.69 (9.7 examples/sec; 0.824 sec/batch; 57h:14m:07s remains)
INFO - root - 2017-12-10 02:58:01.567337: step 82480, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.845 sec/batch; 58h:40m:07s remains)
INFO - root - 2017-12-10 02:58:10.191431: step 82490, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.889 sec/batch; 61h:46m:09s remains)
INFO - root - 2017-12-10 02:58:18.875253: step 82500, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.841 sec/batch; 58h:22m:12s remains)
2017-12-10 02:58:19.894928: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.27599511 0.27640665 0.27505606 0.27263227 0.26877335 0.26356491 0.25618938 0.24724984 0.23875558 0.22959539 0.2210097 0.21290416 0.20704393 0.20120022 0.196403][0.2760818 0.27929321 0.28054363 0.28071287 0.27800393 0.27315077 0.2660169 0.25746676 0.24932238 0.2407487 0.23323663 0.22497384 0.21825173 0.21123385 0.20502995][0.27195334 0.27812824 0.28261733 0.28534886 0.28437576 0.28046042 0.27424362 0.26595533 0.25806686 0.25043398 0.24398839 0.23606278 0.22874099 0.22069415 0.21316992][0.2702626 0.27865848 0.28542286 0.29080942 0.29255006 0.28991285 0.28544784 0.27833089 0.27120039 0.26423806 0.25800422 0.25075069 0.24328238 0.23417141 0.22509177][0.27026466 0.28024614 0.28815606 0.2955364 0.29905307 0.29825118 0.2951155 0.28962868 0.28429702 0.27864107 0.27280942 0.26599166 0.25786781 0.24791142 0.23740132][0.26881734 0.28089279 0.28981143 0.29799733 0.30294558 0.30366305 0.302071 0.29829982 0.29549959 0.29181394 0.2873022 0.28072685 0.27210274 0.26143727 0.24955875][0.26686168 0.28036562 0.29009405 0.29876375 0.30486068 0.30683082 0.30663079 0.30467227 0.30396646 0.30250257 0.29914621 0.29351631 0.28538349 0.27463067 0.26230943][0.2649107 0.27894929 0.28833872 0.296658 0.30341277 0.30680197 0.3079825 0.3081663 0.30911046 0.30885667 0.30600145 0.30133882 0.29402396 0.28338283 0.27125388][0.26072049 0.27506658 0.28411287 0.29276112 0.30012196 0.30459929 0.30706322 0.30897364 0.31114683 0.3117573 0.30942982 0.3054817 0.29903966 0.28886789 0.27753335][0.25374472 0.26832759 0.27714008 0.28590441 0.29361144 0.29886365 0.30182785 0.30456394 0.30728841 0.308717 0.30737481 0.30422041 0.29894385 0.28999382 0.27996361][0.24610439 0.2607834 0.26974809 0.27807412 0.28578255 0.29109284 0.29462671 0.29775137 0.30058059 0.30231249 0.30157512 0.29923731 0.29543215 0.28810269 0.27978289][0.23643735 0.25088838 0.25956768 0.26708758 0.27389479 0.27891183 0.28246516 0.28628078 0.28909531 0.29164052 0.29220486 0.29149133 0.28960088 0.2841135 0.27795058][0.22637154 0.240032 0.24789563 0.25433406 0.26014167 0.26421919 0.26712167 0.27054036 0.27280569 0.27587593 0.27725449 0.278328 0.27858692 0.27563992 0.27217659][0.21729408 0.22968733 0.23584981 0.24090351 0.24532856 0.2482647 0.25035155 0.25269228 0.25469849 0.2581915 0.2606737 0.26351857 0.265944 0.26608598 0.26540852][0.21044695 0.22115918 0.22529784 0.22812255 0.23047957 0.23154457 0.23227803 0.23377644 0.23565505 0.23946215 0.24282329 0.24699014 0.25117877 0.25378779 0.25537789]]...]
INFO - root - 2017-12-10 02:58:28.436779: step 82510, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.870 sec/batch; 60h:22m:54s remains)
INFO - root - 2017-12-10 02:58:37.129298: step 82520, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.876 sec/batch; 60h:48m:33s remains)
INFO - root - 2017-12-10 02:58:45.871473: step 82530, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.897 sec/batch; 62h:15m:19s remains)
INFO - root - 2017-12-10 02:58:54.456215: step 82540, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.859 sec/batch; 59h:37m:46s remains)
INFO - root - 2017-12-10 02:59:03.191893: step 82550, loss = 0.83, batch loss = 0.70 (8.9 examples/sec; 0.898 sec/batch; 62h:19m:08s remains)
INFO - root - 2017-12-10 02:59:11.693370: step 82560, loss = 0.82, batch loss = 0.69 (11.2 examples/sec; 0.714 sec/batch; 49h:34m:34s remains)
INFO - root - 2017-12-10 02:59:20.401466: step 82570, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.896 sec/batch; 62h:10m:50s remains)
INFO - root - 2017-12-10 02:59:29.202481: step 82580, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.886 sec/batch; 61h:31m:33s remains)
INFO - root - 2017-12-10 02:59:37.942503: step 82590, loss = 0.82, batch loss = 0.69 (8.8 examples/sec; 0.911 sec/batch; 63h:12m:39s remains)
INFO - root - 2017-12-10 02:59:46.692878: step 82600, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.864 sec/batch; 59h:57m:59s remains)
2017-12-10 02:59:47.656910: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0018231402 -0.001822368 -0.0018224386 -0.0018229277 -0.0018237064 -0.0018248369 -0.001825368 -0.0018254116 -0.0018249998 -0.0018242737 -0.0018235727 -0.0018229125 -0.0018223065 -0.0018219332 -0.0018219722][-0.0018234726 -0.0018230332 -0.0018236208 -0.0018245833 -0.0018257209 -0.0018270871 -0.0018275816 -0.0018274428 -0.0018265832 -0.0018254807 -0.0018241516 -0.001822595 -0.0018200786 -0.0018173502 -0.0018155907][-0.0018241843 -0.0018243213 -0.0018253374 -0.0018265792 -0.0018276945 -0.0018284615 -0.0018281329 -0.0018270367 -0.0018254333 -0.0018234923 -0.0018206722 -0.0018174172 -0.0018125108 -0.0018063466 -0.0018012542][-0.0018242607 -0.0018247262 -0.0018259116 -0.0018268968 -0.0018269194 -0.0018258431 -0.0018232715 -0.001820242 -0.0018168993 -0.0018133464 -0.0018092809 -0.0018046136 -0.0017975535 -0.0017876534 -0.0017783706][-0.0018239035 -0.0018242816 -0.0018250181 -0.0018249647 -0.0018228753 -0.0018188974 -0.0018127537 -0.0018062565 -0.0018000053 -0.0017943233 -0.0017886123 -0.0017819462 -0.0017719014 -0.0017577675 -0.0017437803][-0.0018232977 -0.0018234359 -0.0018234673 -0.0018219718 -0.001816946 -0.0018084388 -0.0017967668 -0.0017854145 -0.0017753164 -0.0017667325 -0.0017585943 -0.0017491487 -0.0017360287 -0.0017186131 -0.0017024447][-0.0018223347 -0.0018219607 -0.0018212538 -0.0018183612 -0.0018101431 -0.0017960676 -0.0017778643 -0.0017613873 -0.0017474293 -0.0017355275 -0.0017245379 -0.0017123208 -0.0016977747 -0.0016809094 -0.0016677945][-0.0018216386 -0.0018206066 -0.0018191375 -0.0018150794 -0.0018047615 -0.0017876833 -0.0017661056 -0.0017467114 -0.0017297561 -0.0017145444 -0.0017006972 -0.0016862184 -0.0016713317 -0.0016572952 -0.0016495199][-0.0018216057 -0.0018200887 -0.0018184544 -0.0018145455 -0.001805558 -0.0017906886 -0.0017716867 -0.0017533946 -0.0017353453 -0.0017174218 -0.0017002414 -0.0016836098 -0.0016686972 -0.0016572323 -0.0016537367][-0.0018214349 -0.00181994 -0.0018188849 -0.0018165578 -0.0018109841 -0.0018018673 -0.0017899834 -0.0017765581 -0.0017609838 -0.0017429575 -0.0017237727 -0.0017051012 -0.0016904685 -0.0016815652 -0.0016808059][-0.0018206228 -0.0018193951 -0.0018189711 -0.0018181618 -0.0018157412 -0.0018117737 -0.0018063589 -0.0017989866 -0.0017884322 -0.001774698 -0.0017583031 -0.0017413027 -0.0017279689 -0.0017215273 -0.0017223572][-0.0018197857 -0.001818481 -0.0018183003 -0.001818287 -0.0018177112 -0.0018166747 -0.0018151306 -0.0018123953 -0.001807297 -0.001799613 -0.0017895496 -0.0017781237 -0.0017682307 -0.0017636927 -0.0017643169][-0.0018190894 -0.0018173874 -0.0018168752 -0.0018169431 -0.0018170221 -0.001817289 -0.0018175109 -0.0018173023 -0.0018159244 -0.0018131852 -0.0018085623 -0.001802575 -0.0017976033 -0.0017952167 -0.0017952429][-0.0018187021 -0.0018169199 -0.0018162386 -0.0018161136 -0.0018160961 -0.0018162236 -0.0018164723 -0.0018168131 -0.0018168911 -0.0018165441 -0.0018153723 -0.0018136912 -0.0018122258 -0.0018113371 -0.0018116408][-0.0018185095 -0.0018167553 -0.0018160651 -0.0018159697 -0.0018159053 -0.0018158536 -0.0018157358 -0.0018156716 -0.0018159192 -0.0018161754 -0.0018161488 -0.0018158808 -0.0018153065 -0.0018149198 -0.0018148773]]...]
INFO - root - 2017-12-10 02:59:56.348792: step 82610, loss = 0.82, batch loss = 0.69 (8.9 examples/sec; 0.894 sec/batch; 62h:05m:10s remains)
INFO - root - 2017-12-10 03:00:05.186603: step 82620, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.853 sec/batch; 59h:14m:11s remains)
INFO - root - 2017-12-10 03:00:13.949152: step 82630, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.879 sec/batch; 61h:00m:08s remains)
INFO - root - 2017-12-10 03:00:22.402671: step 82640, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.860 sec/batch; 59h:42m:48s remains)
INFO - root - 2017-12-10 03:00:31.138709: step 82650, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.885 sec/batch; 61h:24m:04s remains)
INFO - root - 2017-12-10 03:00:39.803776: step 82660, loss = 0.82, batch loss = 0.69 (10.5 examples/sec; 0.762 sec/batch; 52h:54m:45s remains)
INFO - root - 2017-12-10 03:00:48.166827: step 82670, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.832 sec/batch; 57h:42m:31s remains)
INFO - root - 2017-12-10 03:00:56.850626: step 82680, loss = 0.81, batch loss = 0.68 (9.4 examples/sec; 0.850 sec/batch; 58h:57m:12s remains)
INFO - root - 2017-12-10 03:01:05.418264: step 82690, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.870 sec/batch; 60h:23m:04s remains)
INFO - root - 2017-12-10 03:01:14.315104: step 82700, loss = 0.83, batch loss = 0.70 (9.2 examples/sec; 0.874 sec/batch; 60h:40m:01s remains)
2017-12-10 03:01:15.224363: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.0017934277 -0.0017914228 -0.0017911828 -0.0017911447 -0.0017908536 -0.0017904984 -0.0017903402 -0.0017904248 -0.0017907429 -0.0017913696 -0.0017923646 -0.0017934502 -0.0017941658 -0.0017943889 -0.0017944168][-0.0017916196 -0.0017896303 -0.0017894692 -0.0017894625 -0.0017891426 -0.0017887438 -0.0017885909 -0.0017886977 -0.0017891447 -0.0017900503 -0.0017913721 -0.0017927152 -0.0017935577 -0.0017937915 -0.001793773][-0.0017906715 -0.0017887346 -0.0017885565 -0.001788544 -0.0017882067 -0.0017877158 -0.0017874456 -0.0017874609 -0.0017879426 -0.0017889574 -0.0017904183 -0.0017918622 -0.001792811 -0.0017931288 -0.0017931176][-0.0017896639 -0.0017877761 -0.0017875697 -0.0017875093 -0.0017871434 -0.0017865733 -0.0017862249 -0.0017861837 -0.00178668 -0.001787758 -0.0017892621 -0.0017907472 -0.0017917225 -0.0017921422 -0.0017922254][-0.0017885746 -0.0017866826 -0.0017864751 -0.0017863704 -0.001785964 -0.0017853283 -0.0017849061 -0.0017848096 -0.0017852939 -0.0017863904 -0.0017879159 -0.0017894235 -0.0017904093 -0.0017908905 -0.0017910309][-0.001787741 -0.0017858275 -0.0017856304 -0.0017855098 -0.001785121 -0.0017844874 -0.0017839987 -0.0017838211 -0.0017842079 -0.0017851661 -0.0017865031 -0.001787845 -0.0017887234 -0.0017891657 -0.0017893068][-0.0017874073 -0.0017855506 -0.0017853389 -0.0017852042 -0.0017848378 -0.0017842358 -0.001783723 -0.0017835099 -0.001783789 -0.0017845221 -0.0017855297 -0.0017865278 -0.0017871622 -0.0017874573 -0.0017875639][-0.0017874561 -0.0017856644 -0.0017854247 -0.001785241 -0.0017848484 -0.0017842836 -0.0017838082 -0.0017836122 -0.0017838621 -0.0017844664 -0.0017852688 -0.0017860011 -0.0017864575 -0.0017866516 -0.0017867472][-0.0017873755 -0.0017856746 -0.0017854593 -0.0017852464 -0.0017848469 -0.0017843295 -0.0017839151 -0.0017837704 -0.0017840117 -0.001784529 -0.0017852024 -0.0017857713 -0.0017861017 -0.0017862213 -0.0017862902][-0.0017870994 -0.0017856074 -0.0017854369 -0.0017852241 -0.001784833 -0.0017843673 -0.0017839958 -0.0017838773 -0.0017840781 -0.0017845088 -0.0017850716 -0.0017855196 -0.0017857598 -0.0017858053 -0.0017858207][-0.0017869363 -0.0017855514 -0.0017853662 -0.0017851957 -0.0017848418 -0.0017844251 -0.0017841074 -0.0017839953 -0.0017841277 -0.0017844526 -0.001784882 -0.0017851841 -0.001785312 -0.0017852979 -0.0017852882][-0.0017869924 -0.0017855074 -0.0017853134 -0.0017851854 -0.0017848834 -0.0017845357 -0.0017842721 -0.001784181 -0.0017842766 -0.0017845149 -0.0017848326 -0.0017850288 -0.0017850809 -0.0017850413 -0.0017850283][-0.0017872022 -0.0017856046 -0.001785337 -0.0017852401 -0.0017850014 -0.0017847357 -0.0017845425 -0.0017844824 -0.0017845532 -0.0017847257 -0.0017849617 -0.0017850987 -0.0017851208 -0.0017850847 -0.0017850868][-0.0017875364 -0.0017857783 -0.0017854484 -0.001785395 -0.0017852358 -0.0017850518 -0.0017849238 -0.0017848772 -0.0017849052 -0.0017849972 -0.0017851398 -0.0017852297 -0.001785245 -0.001785229 -0.0017852444][-0.0017878952 -0.0017860484 -0.0017856344 -0.0017856311 -0.0017855589 -0.0017854528 -0.0017853669 -0.0017853206 -0.0017853032 -0.0017853056 -0.0017853434 -0.0017853777 -0.0017853888 -0.001785389 -0.0017854123]]...]
INFO - root - 2017-12-10 03:01:23.905768: step 82710, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.857 sec/batch; 59h:27m:11s remains)
INFO - root - 2017-12-10 03:01:32.521530: step 82720, loss = 0.81, batch loss = 0.68 (9.0 examples/sec; 0.886 sec/batch; 61h:27m:05s remains)
INFO - root - 2017-12-10 03:01:41.175469: step 82730, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.869 sec/batch; 60h:15m:25s remains)
INFO - root - 2017-12-10 03:01:49.760045: step 82740, loss = 0.83, batch loss = 0.70 (9.3 examples/sec; 0.856 sec/batch; 59h:24m:06s remains)
INFO - root - 2017-12-10 03:01:58.408477: step 82750, loss = 0.82, batch loss = 0.69 (9.6 examples/sec; 0.831 sec/batch; 57h:39m:53s remains)
INFO - root - 2017-12-10 03:02:06.809843: step 82760, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.841 sec/batch; 58h:20m:13s remains)
INFO - root - 2017-12-10 03:02:15.324846: step 82770, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.881 sec/batch; 61h:05m:18s remains)
INFO - root - 2017-12-10 03:02:24.150452: step 82780, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.858 sec/batch; 59h:30m:48s remains)
INFO - root - 2017-12-10 03:02:32.685916: step 82790, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.857 sec/batch; 59h:28m:29s remains)
INFO - root - 2017-12-10 03:02:41.413324: step 82800, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.866 sec/batch; 60h:02m:11s remains)
2017-12-10 03:02:42.316677: I tensorflow/core/kernels/logging_ops.cc:79] [[[-0.001490999 -0.0012676655 -0.00095606543 -0.00056126923 -0.00010122848 0.00033655216 0.00063849485 0.0007145718 0.00051753118 9.55658e-05 -0.00044453621 -0.0009618653 -0.0013665473 -0.0016305624 -0.0017679861][-0.001288153 -0.00094640418 -0.00046307687 0.00017500354 0.00095192029 0.0017233466 0.0023139273 0.002537373 0.0023013814 0.0016436504 0.00074990222 -0.0001516242 -0.00089528121 -0.0013998842 -0.0016769136][-0.00090078055 -0.0004285801 0.0002071046 0.0010402253 0.0020111538 0.0029615448 0.003706207 0.0040284661 0.0037827785 0.0029920731 0.0018619717 0.00067362364 -0.0003578274 -0.0011005006 -0.0015386567][-0.00038879819 0.00019910128 0.00098610122 0.0019683032 0.0030647665 0.004122436 0.0049121967 0.005214341 0.0048982855 0.0040046996 0.0027159206 0.0013267343 9.7705983e-05 -0.00081144704 -0.0013797464][-1.9560452e-05 0.00059172616 0.0014404029 0.0024766191 0.0035863644 0.0046250103 0.0053892313 0.0056799697 0.0053365012 0.0044148536 0.0030824915 0.0016396955 0.00033031229 -0.00066078326 -0.0012911453][-2.3316126e-05 0.00055386906 0.0014180905 0.0024931161 0.0036101164 0.004626594 0.0053478237 0.0055990005 0.0052242381 0.0042961678 0.0029861866 0.0015758598 0.00029447686 -0.00067895337 -0.0012984702][-0.00053155154 -3.2519805e-05 0.00073560851 0.0016984997 0.002713799 0.0036526 0.0043240408 0.0045672329 0.0042415163 0.0034180228 0.0022596037 0.0010201734 -8.7150722e-05 -0.00091443688 -0.0014252664][-0.0011926221 -0.00085472246 -0.00030055107 0.00042565877 0.0012166462 0.0020038681 0.0026183827 0.0028882925 0.0026756003 0.0020438959 0.0011478286 0.00019057782 -0.00064800784 -0.0012525206 -0.0016002093][-0.0016754938 -0.0015217282 -0.0012461804 -0.00086436211 -0.00040238479 0.00012130837 0.00059186586 0.0008549093 0.00078297837 0.0004203812 -0.0001200001 -0.00070390035 -0.00121086 -0.0015614545 -0.0017422086][-0.0018173318 -0.0017778502 -0.0016942824 -0.0015627224 -0.0013720212 -0.0011118455 -0.00084284996 -0.00066937006 -0.00067369721 -0.00083131192 -0.0010791597 -0.0013536741 -0.0015907453 -0.0017441716 -0.0018114272][-0.0018393613 -0.0018378516 -0.0018324447 -0.001817151 -0.0017732133 -0.0016907897 -0.001592102 -0.001525521 -0.0015173153 -0.0015557276 -0.0016243441 -0.0017063633 -0.0017766226 -0.0018162869 -0.0018290606][-0.0018368647 -0.0018369408 -0.0018372249 -0.0018357963 -0.0018306 -0.0018168886 -0.0017977169 -0.0017849522 -0.0017829507 -0.0017880365 -0.0017978726 -0.0018112389 -0.0018232517 -0.001829019 -0.00183007][-0.0018345373 -0.0018345135 -0.0018350667 -0.001835863 -0.0018364449 -0.0018369537 -0.0018372069 -0.001837131 -0.0018363014 -0.0018350161 -0.0018337242 -0.0018326286 -0.0018317517 -0.0018310093 -0.0018308037][-0.0018327835 -0.0018324568 -0.001832752 -0.0018332722 -0.0018335775 -0.001833925 -0.001834214 -0.001834457 -0.0018341774 -0.0018334376 -0.0018323748 -0.0018315281 -0.0018308193 -0.0018303785 -0.0018303214][-0.0018311536 -0.0018307718 -0.0018308624 -0.001831271 -0.0018315214 -0.0018318542 -0.0018320387 -0.0018323642 -0.0018324262 -0.0018322731 -0.0018319048 -0.001831591 -0.0018310515 -0.0018304704 -0.0018302378]]...]
INFO - root - 2017-12-10 03:02:50.993004: step 82810, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.875 sec/batch; 60h:39m:24s remains)
INFO - root - 2017-12-10 03:02:59.699584: step 82820, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 59h:48m:55s remains)
INFO - root - 2017-12-10 03:03:08.544904: step 82830, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 60h:08m:11s remains)
INFO - root - 2017-12-10 03:03:17.016780: step 82840, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.882 sec/batch; 61h:08m:53s remains)
INFO - root - 2017-12-10 03:03:25.718403: step 82850, loss = 0.82, batch loss = 0.69 (9.7 examples/sec; 0.823 sec/batch; 57h:02m:54s remains)
INFO - root - 2017-12-10 03:03:34.324986: step 82860, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.880 sec/batch; 60h:59m:23s remains)
INFO - root - 2017-12-10 03:03:42.772961: step 82870, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.859 sec/batch; 59h:32m:11s remains)
INFO - root - 2017-12-10 03:03:51.310467: step 82880, loss = 0.83, batch loss = 0.70 (9.3 examples/sec; 0.859 sec/batch; 59h:33m:01s remains)
INFO - root - 2017-12-10 03:03:59.750809: step 82890, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.870 sec/batch; 60h:18m:36s remains)
INFO - root - 2017-12-10 03:04:08.324169: step 82900, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.860 sec/batch; 59h:38m:36s remains)
2017-12-10 03:04:09.154924: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.0087171942 0.0088127861 0.0092142047 0.0095868064 0.010573244 0.011574598 0.012309519 0.012139406 0.010862641 0.0089270389 0.0063449 0.0038596122 0.0018417629 0.00041654229 -0.00060611323][0.010993633 0.011533902 0.012684738 0.01415141 0.016317111 0.018278208 0.01976666 0.019454293 0.017293157 0.013878168 0.0095225573 0.00569314 0.0027399166 0.00096809294 -0.00013263221][0.015023923 0.015437684 0.01679844 0.019110819 0.022570193 0.02595151 0.028574396 0.028425859 0.025223015 0.019879347 0.013402336 0.0079295561 0.0036470657 0.0010460686 -0.00035727909][0.023599513 0.023769323 0.024967577 0.02741608 0.031444084 0.035818167 0.039311979 0.039097492 0.034785893 0.027681923 0.019345112 0.01210964 0.0060484442 0.0021154583 -0.00019792723][0.035891406 0.036293443 0.03743415 0.039596714 0.043258488 0.047444638 0.050743394 0.0499662 0.044431344 0.03574989 0.026096204 0.017672434 0.010082729 0.0044129882 0.00062501931][0.050259676 0.051336627 0.052708279 0.054783192 0.057903163 0.06126713 0.063378267 0.061186865 0.053960051 0.043641221 0.032828767 0.023524646 0.014965122 0.0078099896 0.0023610415][0.065371938 0.066934161 0.068148538 0.069808081 0.07201954 0.074176326 0.074912705 0.071543083 0.063225567 0.051895387 0.040395211 0.030435231 0.020904897 0.012134026 0.0048382594][0.079583518 0.081794083 0.082745917 0.084067509 0.085470021 0.086425371 0.085747167 0.081383795 0.07251174 0.060799606 0.048997942 0.038504206 0.028234927 0.018009307 0.0087319361][0.092097051 0.09429314 0.0940971 0.094593257 0.095075734 0.095045768 0.093631811 0.089330129 0.081221245 0.07041093 0.059130576 0.048484065 0.037521563 0.02595569 0.014813722][0.10280515 0.10453344 0.10248718 0.10151403 0.10071398 0.099596933 0.097666159 0.0939801 0.087757818 0.079366796 0.070115492 0.060564984 0.049744826 0.037308726 0.024269123][0.11338472 0.11379206 0.10898308 0.10576591 0.10290969 0.10014249 0.097316287 0.094073847 0.089874819 0.084563412 0.078546546 0.07156086 0.062319957 0.050272539 0.036228351][0.12104772 0.12068956 0.1136426 0.10784412 0.10242866 0.097603656 0.093344748 0.089905083 0.087042741 0.08444228 0.081723593 0.077836558 0.071014456 0.060460582 0.046643138][0.12349192 0.12140507 0.11304717 0.10536324 0.097836129 0.091064386 0.085259989 0.081139334 0.078681454 0.077724531 0.077365853 0.076027267 0.071678288 0.063295662 0.050945681][0.11630953 0.11323087 0.10413013 0.095122211 0.086242124 0.078178808 0.071230531 0.066495076 0.064157367 0.064139426 0.0652216 0.065625116 0.063347831 0.057292622 0.047315922][0.10158258 0.098332122 0.089487053 0.080026172 0.070479 0.061723448 0.054224525 0.04915021 0.04667642 0.046846949 0.048444413 0.049842656 0.049168423 0.045424059 0.038259991]]...]
INFO - root - 2017-12-10 03:04:17.704399: step 82910, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.878 sec/batch; 60h:50m:36s remains)
INFO - root - 2017-12-10 03:04:26.323048: step 82920, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.862 sec/batch; 59h:46m:24s remains)
INFO - root - 2017-12-10 03:04:34.866314: step 82930, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.858 sec/batch; 59h:27m:00s remains)
INFO - root - 2017-12-10 03:04:43.385725: step 82940, loss = 0.82, batch loss = 0.69 (9.9 examples/sec; 0.808 sec/batch; 55h:59m:07s remains)
INFO - root - 2017-12-10 03:04:52.001884: step 82950, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.887 sec/batch; 61h:28m:56s remains)
INFO - root - 2017-12-10 03:05:00.618253: step 82960, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.867 sec/batch; 60h:06m:48s remains)
INFO - root - 2017-12-10 03:05:08.946255: step 82970, loss = 0.82, batch loss = 0.69 (10.0 examples/sec; 0.804 sec/batch; 55h:43m:46s remains)
INFO - root - 2017-12-10 03:05:17.569571: step 82980, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.880 sec/batch; 61h:01m:05s remains)
INFO - root - 2017-12-10 03:05:26.030518: step 82990, loss = 0.82, batch loss = 0.69 (9.3 examples/sec; 0.859 sec/batch; 59h:30m:45s remains)
INFO - root - 2017-12-10 03:05:34.527677: step 83000, loss = 0.82, batch loss = 0.69 (9.0 examples/sec; 0.890 sec/batch; 61h:39m:24s remains)
2017-12-10 03:05:35.428638: I tensorflow/core/kernels/logging_ops.cc:79] [[[0.041155577 0.060155954 0.086155586 0.1157256 0.144672 0.1709702 0.19002217 0.20269057 0.20805959 0.20913103 0.20643392 0.20316915 0.19925135 0.19619957 0.19328259][0.042948887 0.064630285 0.093459532 0.1256306 0.1567411 0.18485878 0.20423497 0.21531738 0.21834514 0.21660435 0.21164824 0.20521168 0.19913445 0.19412757 0.19004975][0.044273395 0.067933455 0.098418251 0.13283122 0.16622002 0.19554128 0.21525361 0.22560182 0.22657482 0.22179657 0.21417555 0.20559055 0.19790335 0.19159444 0.18709722][0.044481937 0.069767065 0.10184818 0.13767277 0.17249285 0.20290691 0.22313161 0.23262633 0.23205276 0.22490568 0.21523754 0.2054438 0.19772844 0.19176197 0.18795425][0.04285071 0.0692802 0.10232412 0.13889314 0.17429765 0.20463488 0.22394699 0.23240098 0.23041853 0.22238716 0.21279238 0.20443185 0.19901766 0.19528945 0.19358361][0.039596938 0.066262767 0.099624313 0.13563803 0.16977128 0.19866887 0.21644777 0.22290571 0.21953773 0.21172686 0.203786 0.19856428 0.19708203 0.1972205 0.19866642][0.0346732 0.060500018 0.093106069 0.12792927 0.16025212 0.18630534 0.20140931 0.20538116 0.2001259 0.19250193 0.18692432 0.18589881 0.18923746 0.19410367 0.19919908][0.02972905 0.053535145 0.0840998 0.11676936 0.14648847 0.16944273 0.18102032 0.18277332 0.17684102 0.16994736 0.16614123 0.16926621 0.17723855 0.18636528 0.1951704][0.025162745 0.046072677 0.073680907 0.10341994 0.12970735 0.14911245 0.1574938 0.15660487 0.14962041 0.14419158 0.14328137 0.14959224 0.16095366 0.17366152 0.18493214][0.02004288 0.03762354 0.061375469 0.087359294 0.10987937 0.12567946 0.13097529 0.12827292 0.12103634 0.11704005 0.11867362 0.12803873 0.14222021 0.15698224 0.1696716][0.015280212 0.029483479 0.049074586 0.070770711 0.0890765 0.10127437 0.10425997 0.10029338 0.093050741 0.090132177 0.094015293 0.10590722 0.12179469 0.13790566 0.15104526][0.010666993 0.021301256 0.036520313 0.053789109 0.068168432 0.077353761 0.078843743 0.074861333 0.06851916 0.066406414 0.071538933 0.084737055 0.10157207 0.11799142 0.13081463][0.0067623793 0.014184584 0.025168454 0.038036779 0.04866473 0.055338267 0.056141071 0.052849364 0.048019726 0.046851452 0.052266665 0.064952619 0.081048481 0.096633069 0.10843403][0.0034803757 0.008223 0.015533249 0.024337679 0.03162295 0.03617841 0.03675966 0.034401149 0.030966975 0.030463144 0.035514332 0.04630499 0.059842374 0.073384568 0.083671957][0.00096083095 0.0037117437 0.008042682 0.013471773 0.018004268 0.020853613 0.021193022 0.019717084 0.017564274 0.017370785 0.021357534 0.029809672 0.040475823 0.05106771 0.059228476]]...]
INFO - root - 2017-12-10 03:05:43.964313: step 83010, loss = 0.82, batch loss = 0.69 (9.5 examples/sec; 0.845 sec/batch; 58h:31m:52s remains)
INFO - root - 2017-12-10 03:05:52.561658: step 83020, loss = 0.82, batch loss = 0.69 (9.4 examples/sec; 0.853 sec/batch; 59h:08m:49s remains)
INFO - root - 2017-12-10 03:06:01.208674: step 83030, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.875 sec/batch; 60h:40m:08s remains)
INFO - root - 2017-12-10 03:06:09.894206: step 83040, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.871 sec/batch; 60h:23m:10s remains)
INFO - root - 2017-12-10 03:06:18.346591: step 83050, loss = 0.82, batch loss = 0.69 (9.1 examples/sec; 0.875 sec/batch; 60h:39m:00s remains)
INFO - root - 2017-12-10 03:06:27.100223: step 83060, loss = 0.82, batch loss = 0.69 (9.2 examples/sec; 0.868 sec/batch; 60h:10m:14s remains)
